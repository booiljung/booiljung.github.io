<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:AMD ROCm</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>AMD ROCm</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">컴퓨터 (Computers)</a> / <a href="index.html">AMD</a> / <span>AMD ROCm</span></nav>
                </div>
            </header>
            <article>
                <h1>AMD ROCm</h1>
<p>2025-10-18, G25DR</p>
<h2>1. 서론</h2>
<h3>1.1 ROCm의 정의와 GPGPU 시장에서의 전략적 위상</h3>
<p>ROCm(Radeon Open Compute)은 AMD(Advanced Micro Devices)가 개발한 GPU(Graphics Processing Unit) 프로그래밍을 위한 포괄적인 개방형 소프트웨어 스택(stack)이다.1 이는 단순히 라이브러리의 집합이 아닌, 하드웨어와 직접 상호작용하는 커널 드라이버부터 최종 사용자 애플리케이션에 이르기까지 모든 계층을 아우르는 드라이버, 런타임, 컴파일러, 개발 도구 및 API(Application Programming Interface)를 포함하는 하나의 통합된 생태계로 정의된다.2 ROCm은 범용 GPU 컴퓨팅(GPGPU), 고성능 컴퓨팅(HPC), 그리고 인공지능(AI)과 같은 여러 영역에 걸쳐 AMD GPU의 병렬 처리 능력을 최대한 활용할 수 있도록 설계되었다.1</p>
<p>GPGPU 시장에서 ROCm의 전략적 위상은 시장의 지배적 플랫폼인 NVIDIA의 CUDA(Compute Unified Device Architecture)에 대한 개방형 대안을 제공하는 데 있다.6 CUDA가 강력한 성능과 성숙한 생태계를 바탕으로 사실상의 산업 표준으로 자리 잡은 반면, 이는 특정 벤더에 대한 기술적 종속성(vendor lock-in)이라는 문제를 야기하였다. ROCm은 이러한 CUDA의 폐쇄성에 대응하여, 오픈소스 라이선스를 기반으로 코드 이식성, 유연성, 그리고 개발자의 선택권을 극대화하는 것을 핵심 가치로 삼는다.9 이러한 개방형 철학은 특정 벤더에 종속되기를 꺼리는 대규모 연구 기관, 학계, 그리고 비용 효율성과 시스템 제어권을 중시하는 기업들에게 중요한 소구점으로 작용한다.</p>
<p>초기 ROCm은 ’Radeon Open Compute platform’의 약어였으나, ’Open Compute’가 등록 상표임에 따라 현재 ’ROCm’은 더 이상 약어가 아닌 플랫폼을 지칭하는 고유 명사로 사용된다.1 이 플랫폼은 개발자들이 자유롭게 소프트웨어를 수정하고, 협업하며, 자신의 특정 요구에 맞게 GPU 소프트웨어를 맞춤화할 수 있는 환경을 제공함으로써, 혁신을 가속하고 GPU 컴퓨팅의 접근성을 넓히는 것을 목표로 한다.2</p>
<h3>1.2 볼츠만 이니셔티브(Boltzmann Initiative)부터 현재까지의 발전 과정 개괄</h3>
<p>ROCm의 등장은 AMD의 GPGPU 전략이 오랜 진화 과정을 거친 결과물이다. 과거 ATI 시절의 저수준 하드웨어 접근 API인 ’Close to Metal’과 이를 발전시킨 ‘ATI Stream’ SDK를 거쳐, 업계 표준인 OpenCL을 지원하는 방향으로 선회하였으나, OpenCL 생태계의 더딘 성장과 CUDA의 시장 지배력 강화라는 현실에 직면하게 되었다.6 이러한 배경 속에서 AMD는 2015년 SC15(Supercomputing Conference 2015)에서 ’볼츠만 이니셔티브(Boltzmann Initiative)’를 발표하였고, 그 결과물로 2016년에 ROCm이 공식적으로 출시되었다.6 이는 CUDA와 본격적으로 경쟁하기 위한 AMD의 새로운 전략적 출발점이었다.</p>
<p>초기 ROCm 스택은 CPU, GPU, DSP 등 다양한 처리 장치를 통합적으로 활용하는 것을 목표로 하는 HSA(이기종 시스템 아키텍처, Heterogeneous System Architecture)에 깊이 뿌리를 두고 있었다.8 그러나 시장의 주류 기술과의 호환성 및 개발 편의성을 확보하기 위해, 현재는 업계 표준으로 자리 잡은 LLVM 컴파일러 인프라와 업스트림된 AMDGPU 백엔드를 사용하는 방향으로 아키텍처가 발전하였다.11</p>
<p>ROCm의 발전 과정은 버전별 주요 업데이트를 통해 명확히 확인할 수 있다. 이는 시장의 요구에 기민하게 대응하며 경쟁력을 강화해 온 AMD의 노력을 보여준다.</p>
<ul>
<li>
<p><strong>ROCm 1.0 (2016년경):</strong> 최초 버전이 개발되었으며, <code>HIPIFY</code> 도구를 통해 당시 주요 딥러닝 프레임워크였던 Caffe와 Torch7을 CUDA에서 HIP으로 이식하는 것을 시연하며 코드 이식성을 강조했다.12</p>
</li>
<li>
<p><strong>ROCm 2.0:</strong> Linux 커널에 AMDGPU 드라이버가 업스트림으로 지원되기 시작했으며, 딥러닝 연산을 위한 핵심 라이브러리인 MIOpen이 포함되었다.12</p>
</li>
<li>
<p><strong>ROCm 3.0:</strong> PyTorch 프레임워크에 대한 업스트림 지원이 시작되었고, 다중 GPU 통신을 위한 RCCL(Radeon C++ Collective Library)이 릴리스되었다. 또한 Docker, Kubernetes, SLURM과 같은 클러스터 관리 및 배포 도구 지원이 확장되며 엔터프라이즈 환경으로의 진입을 본격화했다.3</p>
</li>
<li>
<p><strong>ROCm 4.0 &amp; 5.0:</strong> AMD의 데이터센터용 GPU 아키텍처인 CDNA 및 CDNA 2를 지원하기 시작했으며, PyTorch 공식 패키지가 제공되어 사용 편의성이 크게 향상되었다.12 이 시기 AMD GPU 기반의 Frontier 슈퍼컴퓨터가 세계 최초의 엑사스케일 시스템으로 등극하며 ROCm의 성능과 안정성을 입증했다.12</p>
</li>
<li>
<p><strong>ROCm 6.0 (2023년):</strong> Hugging Face와의 협력을 통해 수천 개의 최신 AI 모델에 대한 지원을 확보했으며, PyTorch 2.0에 대한 제로데이(zero-day) 지원을 포함하여 최신 AI 생태계와의 호환성을 강화했다.3</p>
</li>
<li>
<p><strong>ROCm 7.0 (2025년):</strong> 최신 버전으로, 차세대 CDNA 4 아키텍처 기반의 AMD Instinct™ MI350 시리즈 가속기를 완벽하게 지원한다. AI 추론 성능 극대화를 위해 FP4, FP6와 같은 새로운 저정밀도 데이터 타입을 도입하고, vLLM-d, SGLang 등과의 통합을 통해 분산 추론 기능을 강화하는 등 AI 워크로드에 대한 지원을 전례 없는 수준으로 끌어올렸다.3</p>
</li>
</ul>
<p>ROCm의 발전사는 시장의 ’추격자’가 어떻게 전략적으로 진화하는지를 명확히 보여준다. 초기에는 HSA와 같은 독자적인 기술 아키텍처를 통해 차별화를 시도했으나, 이는 시장의 주류인 CUDA 생태계와의 간극을 넓히는 결과를 낳았다. 이후 AMD는 시장의 현실을 직시하고 전략의 방향을 수정했다. PyTorch와 같은 핵심 프레임워크를 적극적으로 지원하고(ROCm 3.0) 3, <code>HIPIFY</code>를 통해 기존 CUDA 코드 자산을 ROCm으로 쉽게 가져올 수 있는 경로를 제공하며(ROCm 1.0) 6, Hugging Face와 같은 AI 생태계의 중심과 협력하는(ROCm 6.0) 3 일련의 과정은, 기술적 독창성만으로는 시장을 바꿀 수 없으며 생태계 호환성이 GPGPU 플랫폼의 성패를 가르는 핵심 요인임을 AMD가 깊이 인지했음을 시사한다.</p>
<p>더 나아가, ROCm의 핵심 철학인 ’개방성(Openness)’은 단순히 라이선스 정책을 넘어, CUDA의 강력한 생태계적 ’해자(moat)’를 공략하기 위한 핵심 무기로 작용한다. CUDA의 가장 큰 강점은 기술 자체가 아니라 수십 년간 축적된 방대한 라이브러리, 잘 정리된 문서, 그리고 거대한 개발자 커뮤니티라는 성숙한 생태계에 있다.10 AMD는 이를 정면으로 상대하기보다, CUDA의 약점인 ’폐쇄성’과 ’벤더 종속성’을 파고드는 비대칭적 전략을 선택했다. ROCm을 오픈소스로 공개하고 7, HIP을 통해 코드 이식성을 제공하며 18, Triton, OpenXLA와 같은 개방형 표준에 적극적으로 참여하는 것 9은 개발자들에게 ’선택의 자유’라는 중요한 가치를 제공한다. 이는 장기적으로 특정 벤더에 대한 기술적 종속을 경계하는 대규모 연구 기관이나 클라우드 서비스 제공업체, 그리고 비용 효율성을 중시하는 기업들에게 매우 매력적인 제안이 될 수 있으며, 이를 통해 CUDA의 견고한 생태계적 장벽을 우회하려는 전략적 의도가 담겨 있다.</p>
<h2>2.  ROCm 소프트웨어 스택 아키텍처 심층 분석</h2>
<p>ROCm은 50개가 넘는 개별 프로젝트로 구성된 복잡하고 거대한 소프트웨어 생태계이다.19 이 복잡성을 효과적으로 관리하고 개발자에게 일관된 경험을 제공하기 위해, ROCm 스택은 명확한 계층적 구조를 채택하고 있다. 이 구조는 하드웨어와 직접 상호작용하는 저수준의 커널 인터페이스부터, 컴파일러와 핵심 라이브러리를 포함하는 중간 계층, 그리고 개발자가 직접 사용하는 고수준의 프로그래밍 모델과 도구에 이르기까지 체계적으로 구성되어 있다. 이러한 모듈성은 사용자가 자신의 필요에 따라 특정 구성 요소만을 선택적으로 설치할 수 있게 해주며 4, 각 계층의 독립적인 개발과 유지보수를 용이하게 한다.</p>
<h3>2.1 하위 계층: 하드웨어 추상화 및 커널 인터페이스</h3>
<p>이 계층은 운영체제 커널 수준에서 작동하며, 사용자 공간의 애플리케이션이 GPU 하드웨어와 통신할 수 있는 기반을 마련한다.</p>
<ul>
<li>
<p><strong>커널 모드 드라이버 (ROCk - Radeon Open Compute kernel):</strong> ROCm 스택의 가장 낮은 수준에 위치한 핵심 구성 요소로, AMD GPU 하드웨어와 Linux 커널 간의 통신을 담당하는 커널 공간 드라이버이다.1 GPU 초기화, 메모리 관리, 명령어 제출과 같은 기본적인 하드웨어 제어 기능을 수행한다. GPU 펌웨어 Blob을 제외한 모든 코드가 오픈소스로 제공되어 투명성과 확장성을 보장한다.1</p>
</li>
<li>
<p><strong>ROCt (Thunk Interface):</strong> 사용자 공간(user-space)의 런타임 라이브러리와 커널 모드 드라이버(ROCk) 사이를 중계하는 얇은 인터페이스 계층이다.4 Thunk는 사용자 공간에서 발생한 API 호출을 커널 드라이버가 이해할 수 있는 저수준의 명령으로 변환하고, 스택에서 발생하는 모든 요청을 처리하여 명령어 큐에 넣는 중요한 역할을 담당한다.1</p>
</li>
<li>
<p><strong>ROCr (Runtime):</strong> 호스트(CPU) 애플리케이션이 GPU에서 컴퓨팅 커널을 실행할 수 있도록 하는 핵심 런타임 API 및 라이브러리 세트이다. 이는 HSA(이기종 시스템 아키텍처) 런타임 API의 AMD 구현체로서 1, 사용자 모드 큐(user-mode queues), 직접 메모리 접근(DMA), 프로세스 동시성 및 선점(preemption), 시그널 및 아토믹 연산과 같은 저수준 하드웨어 제어 기능을 프로그래밍 언어에 독립적인 방식으로 제공한다.20 이를 통해 커널 디스패치와 데이터 전송의 지연 시간을 최소화하여 HPC 애플리케이션의 성능을 극대화할 수 있다.</p>
</li>
</ul>
<h3>2.2 중위 계층: 컴파일러, 라이브러리 및 런타임</h3>
<p>이 계층은 하위 계층의 추상화를 기반으로, 실제 애플리케이션 개발에 필요한 컴파일러, 핵심 라이브러리, 그리고 다양한 프로그래밍 모델을 지원하는 런타임을 제공한다.</p>
<ul>
<li>
<p><strong>LLVM 기반 컴파일러 툴체인:</strong> ROCm은 독자적인 컴파일러 대신 업계 표준으로 널리 사용되는 Clang/LLVM을 컴파일러 인프라의 근간으로 채택했다.11 AMD GPU 백엔드는 LLVM 프로젝트에 공식적으로 업스트림되어 있어, LLVM 커뮤니티의 지속적인 발전과 최적화 혜택을 직접적으로 받을 수 있다. 이 컴파일러 툴체인은 HIP, OpenCL, OpenMP 등 다양한 프로그래밍 모델로 작성된 소스 코드를 AMD GPU의 특정 ISA(Instruction Set Architecture, 예: GCN, CDNA)에 맞는 기계어로 변환하는 역할을 수행한다.19 특히 <code>hipcc</code>는 컴파일 대상 플랫폼(AMD 또는 NVIDIA)에 따라 적절한 백엔드 컴파일러(HIP-Clang 또는 NVCC)를 자동으로 호출하는 편리한 컴파일러 드라이버 스크립트이다.18 LLVM의 채택은 ROCm의 미래 확장성과 개방성을 담보하는 핵심적인 결정이다. 이를 통해 Julia, Rust와 같은 새로운 프로그래밍 언어나, LLVM의 차세대 중간 표현(IR)인 MLIR(Multi-Level Intermediate Representation)과 같은 신기술을 보다 용이하게 통합할 수 있는 기반을 마련했다.1</p>
</li>
<li>
<p><strong>핵심 라이브러리:</strong> ROCm은 HPC 및 AI 워크로드에 필수적인 다양한 가속 라이브러리를 제공한다. 이 라이브러리들은 <code>roc*</code>와 <code>hip*</code>의 이원화된 구조를 가지는 것이 특징이다.</p>
</li>
<li>
<p><strong>수학 라이브러리:</strong></p>
</li>
<li>
<p><code>rocBLAS</code> / <code>hipBLAS</code>: 행렬-행렬 곱셈, 행렬-벡터 곱셈 등 기본 선형대수 연산을 위한 BLAS(Basic Linear Algebra Subprograms) 라이브러리.1</p>
</li>
<li>
<p><code>rocFFT</code> / <code>hipFFT</code>: 고속 푸리에 변환(Fast Fourier Transform) 라이브러리.6</p>
</li>
<li>
<p><code>rocSOLVER</code> / <code>hipSOLVER</code>: 선형 방정식 시스템 및 고유값 문제 등을 해결하기 위한 LAPACK(Linear Algebra PACKage) 구현체.1</p>
</li>
<li>
<p><code>rocSPARSE</code> / <code>hipSPARSE</code>: 희소 행렬(sparse matrix) 연산을 위한 라이브러리.24</p>
</li>
<li>
<p><code>rocRAND</code> / <code>hipRAND</code>: 고성능 병렬 난수 생성을 위한 라이브러리.6</p>
</li>
<li>
<p><strong>머신러닝 라이브러리:</strong></p>
</li>
<li>
<p><code>MIOpen</code>: 컨볼루션, 풀링, 정규화 등 딥러닝에 필수적인 기본 연산(primitive)들을 고도로 최적화하여 제공하는 라이브러리.3</p>
</li>
<li>
<p><code>MIVisionX</code>: 컴퓨터 비전 및 이미지 처리 워크로드를 위한 라이브러리.3</p>
</li>
<li>
<p><code>RPP</code> (ROCm Performance Primitives): 이미지, 비디오, 오디오 처리를 위한 포괄적인 고성능 컴퓨터 비전 라이브러리.26</p>
</li>
<li>
<p><strong>통신 라이브러리:</strong></p>
</li>
<li>
<p><code>RCCL</code> (Radeon C++ Collective Library): 다중 GPU 및 다중 노드 환경에서 <code>AllReduce</code>, <code>Broadcast</code>와 같은 집합 통신(collective communication) 연산을 효율적으로 수행하기 위한 라이브러리로, NVIDIA의 NCCL에 대응된다.3</p>
</li>
</ul>
<p><code>roc*</code>와 <code>hip*</code> 라이브러리의 이원화 구조는 ’이식성’과 ’최적화’라는 두 가지 목표를 동시에 달성하기 위한 전략적 선택이다. <code>hip*</code> 라이브러리는 CUDA의 <code>cu*</code> 라이브러리와 API 수준의 호환성을 제공하여, 기존 CUDA 기반 애플리케이션의 마이그레이션 비용을 최소화하는 ’이식성’에 초점을 맞춘다.6 반면, <code>roc*</code> 라이브러리는 AMD 하드웨어의 고유한 아키텍처 특성을 최대한 활용하여 최고의 성능을 끌어내기 위한 ’최적화’에 중점을 둔다.24 이 구조는 개발자에게 유연한 선택권을 제공한다. 빠른 포팅이 필요할 때는 <code>hip*</code>를 사용하고, AMD 플랫폼에서 최고의 성능을 추구해야 할 때는 <code>roc*</code>의 추가적인 API나 최적화된 구현을 활용할 수 있다.</p>
<ul>
<li><strong>ROCm Device Libraries (ROCm-Device-Libs):</strong> 수학 함수(<code>sin</code>, <code>cos</code> 등), 아토믹 연산, 커널 런치 파라미터 쿼리 등 GPU 디바이스에서 직접 실행되는 다양한 유틸리티 함수들을 LLVM 비트코드(bitcode) 형태로 제공하는 라이브러리이다.1 컴파일러는 커널 코드 컴파일 시 이 라이브러리를 링크하여 최종 실행 코드를 생성한다.</li>
</ul>
<h3>2.3 상위 계층: 프로그래밍 모델 및 개발 도구</h3>
<p>이 계층은 개발자가 실제로 코드를 작성하고, 디버깅하며, 성능을 분석하는 데 사용하는 인터페이스와 도구들로 구성된다.</p>
<ul>
<li>
<p><strong>프로그래밍 모델:</strong> ROCm은 다양한 개발자 배경과 애플리케이션 요구사항을 만족시키기 위해 여러 프로그래밍 모델을 지원한다.</p>
</li>
<li>
<p><strong>HIP (Heterogeneous-compute Interface for Portability):</strong> ROCm 생태계의 핵심이자 가장 권장되는 프로그래밍 모델이다. CUDA와 매우 유사한 C++ 기반 프로그래밍 환경을 제공하여 CUDA 개발자들이 쉽게 적응할 수 있도록 설계되었다.1</p>
</li>
<li>
<p><strong>OpenCL (Open Computing Language):</strong> 크로노스 그룹(Khronos Group)에서 제정한 개방형 표준 병렬 컴퓨팅 프레임워크로, ROCm에서 공식적으로 지원된다. 다양한 하드웨어 플랫폼에 대한 이식성을 중시하는 애플리케이션에 적합하다.1</p>
</li>
<li>
<p><strong>OpenMP:</strong> C, C++, Fortran에서 널리 사용되는 지시어(directive) 기반 병렬 프로그래밍 모델이다. 기존의 순차적인 코드를 최소한의 수정으로 GPU 오프로딩(offloading) 할 수 있어 HPC 워크로드에서 많이 활용된다.1</p>
</li>
<li>
<p><strong>개발 및 디버깅 도구:</strong> 애플리케이션 개발의 전 과정을 지원하는 포괄적인 도구 세트를 제공한다.</p>
</li>
<li>
<p><strong>프로파일러:</strong> <code>rocprof</code>는 커맨드 라인 기반의 강력한 프로파일링 도구로, 커널 실행 시간, API 호출 추적, 메모리 대역폭, 하드웨어 성능 카운터 등 상세한 성능 데이터를 수집할 수 있다.24 최신 <code>ROCm Compute Profiler</code>는 텍스트 기반 사용자 인터페이스(TUI)를 제공하여 상호작용적인 분석을 가능하게 한다.15</p>
</li>
<li>
<p><strong>디버거:</strong> <code>ROCgdb</code>는 널리 사용되는 GDB(GNU Debugger)를 확장하여 GPU에서 실행되는 커널 코드에 대한 중단점(breakpoint) 설정, 변수 검사, 단계별 실행 등 호스트 코드와 유사한 디버깅 경험을 제공한다.19</p>
</li>
<li>
<p><strong>시스템 모니터링:</strong> <code>rocm-smi</code>(ROCm System Management Interface)는 NVIDIA의 <code>nvidia-smi</code>에 해당하는 커맨드 라인 유틸리티로, GPU의 온도, 클럭 속도, 메모리 사용량, 전력 소비 등 실시간 상태를 모니터링하고 일부 설정을 관리하는 기능을 제공한다.3</p>
</li>
</ul>
<h2>3.  핵심 프로그래밍 모델: HIP (Heterogeneous-compute Interface for Portability)</h2>
<p>ROCm 생태계의 중심에는 HIP(Heterogeneous-compute Interface for Portability)이 있다. HIP은 단순한 API를 넘어, AMD의 GPGPU 전략을 관통하는 핵심 철학을 담고 있는 C++ 기반 프로그래밍 모델이다. 후발주자로서 거대한 CUDA 생태계와 경쟁해야 하는 AMD의 현실적 고민이 HIP의 설계에 깊이 반영되어 있다. 이는 새로운 생태계를 밑바닥부터 구축하는 대신, 가장 큰 경쟁자의 생태계에 ’편승(piggybacking)’하여 진입 장벽을 극적으로 낮추고 기존 개발자 자산을 흡수하려는 비대칭적 경쟁 전략의 핵심 도구이다.</p>
<h3>3.1 HIP의 설계 철학: CUDA와의 관계 및 이식성 극대화 전략</h3>
<p>HIP의 가장 중요한 설계 철학은 ’이식성(Portability)’이다. 구체적으로는, 단일 소스 코드를 최소한의 수정으로 AMD GPU와 NVIDIA GPU 양쪽에서 모두 컴파일하고 실행할 수 있도록 하는 것을 목표로 한다.18 이를 달성하기 위한 핵심 전략은 CUDA와의 구문적, 기능적 유사성을 극대화하는 것이다.</p>
<p>CUDA에 익숙한 개발자라면 HIP의 API와 커널 언어가 매우 친숙하게 느껴질 것이다. 대부분의 CUDA API 호출은 네임스페이스나 함수 접두사 <code>cuda</code>를 <code>hip</code>으로 단순히 치환하는 것만으로 변환이 가능하다.6 예를 들어, 디바이스 메모리 할당을 위한 <code>cudaMalloc</code>은 <code>hipMalloc</code>으로, 데이터 전송을 위한 <code>cudaMemcpy</code>는 <code>hipMemcpy</code>로 일대일 대응된다. 이러한 설계는 CUDA 개발자들이 새로운 프로그래밍 모델을 학습하는 데 드는 인지적 부하와 시간을 최소화하여 ROCm 생태계로의 전환을 유도하는 강력한 유인책으로 작용한다.</p>
<p>기능적으로 HIP은 CUDA 런타임 기능의 ’강력한 서브셋(strong subset)’을 지원하는 것을 목표로 한다.31 이는 가장 널리 사용되는 핵심적인 CUDA 기능들을 대부분 지원함으로써, 대다수의 CUDA 애플리케이션이 큰 어려움 없이 HIP으로 이식될 수 있도록 보장한다.</p>
<p>NVIDIA 플랫폼에서 HIP의 동작 방식은 이러한 이식성 전략의 영리함을 보여준다. NVIDIA GPU를 대상으로 컴파일할 때, HIP은 CUDA 런타임 API 호출로 변환되는 인라인 함수들로 구성된 얇은 헤더 파일 계층으로만 동작한다.18 이 과정에서 추가적인 런타임 오버헤드가 거의 발생하지 않기 때문에, HIP으로 작성된 코드는 NVIDIA GPU에서 네이티브 CUDA 코드와 동등한 성능을 낼 수 있다.18 이는 개발자들이 기존의 검증된 NVIDIA 개발 환경(프로파일러, 디버거 등)을 그대로 사용하면서 점진적으로 코드를 HIP으로 마이그레이션하고, 성능 및 기능 검증을 수행할 수 있게 해주는 매우 중요한 특징이다.32 개발자는 먼저 NVIDIA 플랫폼에서 CUDA 코드를 HIP으로 성공적으로 이식한 후, 동일한 HIP 코드를 AMD 플랫폼에서 컴파일하여 실행하는 2단계 접근 방식을 통해 안정적으로 멀티 플랫폼을 지원할 수 있다.</p>
<h3>3.2 <code>hipify</code> 도구를 활용한 CUDA 코드의 자동 변환 및 이식 과정</h3>
<p>AMD는 기존의 방대한 CUDA 코드 자산을 HIP으로 전환하는 과정을 자동화하기 위해 <code>hipify</code>라는 강력한 소스-투-소스(source-to-source) 변환 도구를 제공한다.1 수동으로 대규모 코드를 변환하는 것은 시간이 많이 소요되고 오류가 발생하기 쉬운 작업이므로, <code>hipify</code>는 ROCm 생태계 확산에 있어 필수적인 도구이다.</p>
<p><code>hipify</code>는 두 가지 주요 버전으로 제공된다:</p>
<ul>
<li>
<p><strong><code>hipify-perl</code>:</strong> 이 버전은 펄(Perl) 스크립트를 기반으로 하며, 정규 표현식을 사용하여 소스 코드 내의 CUDA 관련 문자열(API 함수명, 타입, 매크로 등)을 해당하는 HIP 문자열로 치환하는 방식으로 동작한다.1 사용이 간편하고 별도의 의존성이 적다는 장점이 있지만, 코드의 문법적 구조나 의미를 이해하지 못하고 단순 텍스트 치환을 수행하기 때문에 복잡한 매크로나 코드 구조에서는 부정확한 변환을 유발할 수 있다.</p>
</li>
<li>
<p><strong><code>hipify-clang</code>:</strong> 이 버전은 Clang 컴파일러 프론트엔드를 사용하여 CUDA 소스 코드를 파싱하고, 이를 추상 구문 트리(AST, Abstract Syntax Tree)로 변환한다.1 변환 작업은 이 AST 상에서 이루어지므로, 코드의 문법적 구조와 의미를 정확하게 이해한 상태에서 변환을 수행한다. 따라서 <code>hipify-perl</code>에 비해 훨씬 더 정확하고 안정적인 변환 결과를 제공하며, 복잡한 C++ 템플릿이나 매크로가 포함된 코드도 효과적으로 처리할 수 있다.</p>
</li>
</ul>
<p>실제 이식 과정은 일반적으로 <code>hipify</code> 도구를 사용하여 코드의 80-90%를 자동으로 변환하는 것으로 시작된다. 그 후, 개발자는 변환된 코드를 컴파일하면서 발생하는 오류들을 수정하고, <code>hipify</code>가 자동으로 변환하지 못한 부분(예: 일부 라이브러리 호출이나 특정 CUDA 기능)을 수동으로 수정하는 반복적인 과정을 거치게 된다.32 이 과정이 완료되면 기능 검증과 함께, AMD GPU 아키텍처에 맞는 성능 튜닝을 진행하게 된다.</p>
<p>그러나 ’소스 코드 이식성’이 곧 ’성능 이식성’을 보장하는 것은 아니다. HIP을 통해 CUDA 코드를 AMD GPU에서 실행할 수 있게 되더라도, 이것이 반드시 최적의 성능을 의미하지는 않는다. NVIDIA의 SIMT(Single Instruction, Multiple Threads) 아키텍처와 AMD의 아키텍처(웨이브프론트 기반)는 스레드 스케줄링, 메모리 서브시스템, 캐시 구조 등에서 미묘하지만 중요한 차이를 보인다.33 CUDA 코드에 깊숙이 녹아있는 NVIDIA 아키텍처에 특화된 최적화 기법들(예: 워프 크기(32)에 의존하는 알고리즘, 특정 공유 메모리 뱅크 충돌 회피 패턴)은 HIP으로 변환된 후 AMD GPU(웨이브프론트 크기 32 또는 64)에서 오히려 성능 저하의 원인이 될 수 있다.23 따라서 <code>hipify</code>는 이식 작업의 효율적인 시작점을 제공하는 도구이며, 진정한 성능 이식성을 달성하기 위해서는 AMD GPU 아키텍처에 대한 깊은 이해를 바탕으로 한 프로파일링 기반의 수동 튜닝 과정이 필수적이다.</p>
<p>한편, ZLUDA(소스 코드 수정 없이 CUDA 애플리케이션을 AMD GPU에서 실행)나 SCALE(바이너리 수준 호환성 제공)과 같은 제3자 프로젝트의 등장은 HIP의 한계와 ROCm의 가능성을 동시에 보여준다.6 이는 소스 코드 접근이 불가능한 상용 소프트웨어나 복잡한 빌드 시스템을 가진 프로젝트의 경우 소스 코드 변환을 요구하는 HIP만으로는 모든 시장의 요구를 충족시키기 어렵다는 한계를 드러낸다. 동시에, ROCm의 개방적인 아키텍처와 표준 기반(LLVM) 컴파일러가 이러한 제3자 호환성 계층 개발을 가능하게 했다는 점에서 그 생태계적 확장성의 잠재력을 보여준다. 이는 ROCm 생태계가 AMD의 직접적인 노력뿐만 아니라, 커뮤니티와 제3자 기업들에 의해 다양한 방식으로 풍성해질 수 있음을 시사한다.</p>
<h3>3.3 HIP 프로그래밍 모델: 호스트-디바이스 패러다임</h3>
<p>HIP은 CUDA와 마찬가지로, 하나의 프로그램 내에 두 개의 서로 다른 실행 공간이 공존하는 이기종 프로그래밍 모델을 기반으로 한다.30</p>
<ul>
<li>
<p><strong>호스트(Host):</strong> 시스템의 주 프로세서인 CPU를 의미한다. 호스트 코드는 일반적인 C++ 문법을 따르며, 프로그램의 전체적인 흐름을 제어한다. 주요 역할은 데이터 준비, GPU 메모리 할당 및 데이터 전송, GPU에서 실행될 커널(kernel)의 런치(launch), 그리고 GPU로부터 결과 데이터를 다시 가져와 후처리하는 것이다. <code>__host__</code> 데코레이터는 해당 함수가 호스트에서 실행됨을 명시한다.38</p>
</li>
<li>
<p><strong>디바이스(Device):</strong> 병렬 연산을 수행하는 가속기, 즉 GPU를 의미한다. 디바이스 코드는 GPU의 수많은 코어에서 병렬로 실행되며, 대규모 데이터 병렬(data-parallel) 연산을 처리하는 데 특화되어 있다. C++의 일부 기능(템플릿, 클래스 등)을 지원하는 C++ 유사 구문을 사용한다.</p>
</li>
</ul>
<p>이 두 공간의 상호작용은 HIP 런타임 API를 통해 이루어진다.</p>
<ul>
<li>
<p><strong>커널 런치(Kernel Launch):</strong> GPU에서 병렬로 실행될 함수를 ’커널’이라고 하며, <code>__global__</code> 키워드로 선언된다. 커널은 반드시 <code>void</code> 반환 타입을 가져야 한다.31 호스트 코드에서는 <code>hipLaunchKernelGGL</code> 매크로를 사용하여 이 커널을 실행시킨다.18 커널을 런치할 때는 실행될 스레드의 전체 구성을 지정해야 하는데, 이는 **그리드(Grid) &gt; 블록(Block) &gt; 스레드(Thread)**의 3단계 계층 구조로 정의된다.31</p>
</li>
<li>
<p><strong>스레드(Thread):</strong> 가장 작은 실행 단위로, 커널 코드의 한 인스턴스를 실행한다. 각 스레드는 고유한 ID(<code>threadIdx</code>)를 가진다.</p>
</li>
<li>
<p><strong>블록(Block):</strong> 스레드들의 그룹이다. 한 블록 내의 스레드들은 고속의 공유 메모리(shared memory)를 통해 데이터를 공유하고, 배리어(barrier) 동기화를 통해 협력할 수 있다. 각 블록은 고유한 ID(<code>blockIdx</code>)를 가진다.</p>
</li>
<li>
<p><strong>그리드(Grid):</strong> 블록들의 집합으로, 하나의 커널 런치는 하나의 그리드를 구성한다.</p>
</li>
<li>
<p><strong>메모리 관리:</strong> 호스트와 디바이스는 각자 독립적인 메모리 공간을 가지고 있다. 따라서 GPU에서 연산을 수행하기 위해서는 먼저 데이터를 호스트 메모리에서 디바이스 메모리로 복사해야 한다.</p>
</li>
<li>
<p><code>hipMalloc()</code>: GPU 디바이스의 글로벌 메모리(global memory)에 공간을 할당한다.</p>
</li>
<li>
<p><code>hipMemcpy()</code>: 호스트와 디바이스 메모리 간, 또는 디바이스 메모리 간에 데이터를 복사한다. 전송 방향은 <code>hipMemcpyHostToDevice</code>, <code>hipMemcpyDeviceToHost</code> 등의 열거형 상수로 지정한다.</p>
</li>
<li>
<p>hipFree(): hipMalloc으로 할당했던 디바이스 메모리를 해제한다.</p>
</li>
</ul>
<p>HIP은 CUDA와 유사하게, 각 스레드만 접근 가능한 로컬 메모리(레지스터), 블록 내 스레드들이 공유하는 공유 메모리, 모든 스레드가 접근 가능한 글로벌 메모리의 계층적 메모리 구조를 제공하여, 데이터 지역성(data locality)을 활용한 성능 최적화를 가능하게 한다.34</p>
<ul>
<li><strong>스트림(Stream)을 통한 비동기 실행:</strong> 성능을 극대화하기 위해 HIP은 스트림을 이용한 비동기(asynchronous) 실행 모델을 지원한다. 스트림은 순서대로 실행되어야 하는 작업들의 큐(queue)이다. 서로 다른 스트림에 속한 작업들(예: 데이터 전송, 커널 실행)은 GPU 하드웨어가 허용하는 한 동시에 실행될 수 있다. 이를 통해 CPU 연산, 데이터 전송, GPU 커널 실행을 서로 중첩(overlap)시켜 전체 시스템의 유휴 시간을 줄이고 처리량을 높일 수 있다.38</li>
</ul>
<h2>4.  ROCm과 CUDA의 다각적 비교 분석</h2>
<p>ROCm과 CUDA는 현대 GPU 컴퓨팅 생태계를 양분하는 핵심 플랫폼이다. 두 플랫폼의 선택은 단순히 하드웨어의 성능을 넘어, 개발 생산성, 장기적인 유지보수 비용, 그리고 미래 기술에 대한 확장성까지 영향을 미치는 중요한 전략적 결정이다. 따라서 성능, 생태계, 그리고 플랫폼의 근본적인 전략이라는 세 가지 차원에서 두 플랫폼을 다각적으로 비교 분석하는 것은 필수적이다.</p>
<h3>4.1 성능 벤치마크: 최신 AI 및 HPC 워크로드 기반 비교</h3>
<p>과거에는 NVIDIA GPU와 CUDA 플랫폼이 GPGPU 성능의 절대적인 기준으로 여겨졌으나, 최근 AMD 하드웨어의 발전과 ROCm 소프트웨어 스택의 성숙으로 인해 그 격차가 크게 줄어들고 있으며, 특정 워크로드에서는 ROCm이 우위를 보이기도 한다.</p>
<ul>
<li>
<p><strong>AI 워크로드 성능:</strong> 최신 AI 워크로드, 특히 대규모 언어 모델(LLM)의 등장으로 GPU의 메모리 대역폭과 용량이 성능의 핵심 병목으로 부상했다. AMD Instinct MI325X와 같은 최신 가속기는 256GB에 달하는 대용량 HBM3e 메모리를 탑재하여, NVIDIA H100과 같은 경쟁 제품 대비 메모리 용량에서 우위를 점한다.39 이 덕분에 대규모 모델을 여러 GPU에 분할(model parallelism)하지 않고 단일 GPU 내에서 처리할 수 있어, 통신 오버헤드가 줄고 추론 지연 시간이 감소하는 효과를 얻을 수 있다. TensorWave의 내부 벤치마크에 따르면, 이러한 메모리 이점을 바탕으로 ROCm은 메모리 집약적인(memory-bound) LLM 학습 및 대규모 추론 작업에서 CUDA 플랫폼 대비 경쟁력 있는 성능을 보여준다.39</p>
</li>
<li>
<p><strong>성능 격차와 최적화:</strong> 전통적으로 AMD의 최상위 GPU는 동급의 NVIDIA GPU에 비해 원시 연산 성능(raw performance) 면에서 10-30% 가량의 격차가 존재한다는 인식이 있었다.10 그러나 이러한 격차는 하드웨어 자체의 문제라기보다는 소프트웨어 최적화 수준의 차이에서 기인하는 경우가 많았다. ROCm 6.x 버전 이후, AMD는 MIOpen 및 핵심 수학 라이브러리에서 어텐션(attention) 메커니즘과 같은 트랜스포머(transformer) 워크로드에 특화된 최적화를 대거 적용했다.39 또한 <code>hipRTC</code>와 <code>MIGraphX</code>를 통한 컴파일러 스택의 지속적인 개선으로 행렬 곱셈(matmul) 연산 및 커널 융합(fused kernels) 등에서 CUDA와의 성능 격차를 빠르게 좁히고 있다.39</p>
</li>
<li>
<p><strong>ROCm 7.0의 성능 도약:</strong> 최신 ROCm 7.0 릴리스는 소프트웨어 최적화를 통한 성능 향상의 중요성을 명확히 보여준다. AMD의 발표에 따르면, ROCm 7.0은 이전 버전인 ROCm 6.0 대비 Llama 3.1 70B 모델에서 3.2배, DeepSeek R1 모델에서 최대 3.8배의 추론 성능 향상을 달성했다.16 특히, DeepSeek R1 모델의 FP8 정밀도 추론 워크로드에서는 AMD Instinct MI355X와 ROCm 7.0 조합이 NVIDIA Blackwell B200 플랫폼보다 30% 더 높은 처리량(throughput)을 기록했다고 보고되었다.16 학습 성능 또한 Llama 2 70B와 같은 모델에서 평균 3배의 향상을 보였다.16 이는 하드웨어 발전과 더불어 소프트웨어 스택의 성숙이 성능 향상에 얼마나 결정적인지를 입증하는 사례이다.</p>
</li>
</ul>
<h3>4.2 생태계 및 개발 환경: 성숙도와 지원 범위</h3>
<p>소프트웨어 플랫폼의 가치는 단순히 기술적 성능만으로 결정되지 않는다. 개발자 커뮤니티의 규모, 문서화의 질, 서드파티 도구의 다양성, 그리고 클라우드 서비스의 지원 범위 등이 종합된 생태계의 성숙도가 플랫폼의 실질적인 활용성을 좌우한다.</p>
<ul>
<li>
<p><strong>CUDA의 성숙한 생태계:</strong> CUDA는 10년 이상 GPGPU 시장을 선도하며 매우 성숙하고 방대한 생태계를 구축했다.10 거의 모든 HPC 및 AI 프레임워크(TensorFlow, PyTorch 등)가 CUDA를 최우선으로 지원하며, 문제 발생 시 참고할 수 있는 풍부한 공식 문서, 튜토리얼, 포럼, 그리고 블로그 게시물이 존재한다.17 NVIDIA Nsight와 같은 강력한 프로파일링 및 디버깅 도구, cuDNN, cuBLAS, NCCL과 같은 고도로 최적화된 라이브러리, 그리고 TensorRT와 같은 추론 최적화 엔진은 개발 생산성을 크게 향상시킨다. 또한, AWS, Azure, Google Cloud Platform 등 모든 주요 클라우드 제공업체들이 NVIDIA GPU와 최적화된 CUDA 환경을 폭넓게 제공하여, 확장성 있는 솔루션 구축에 유리하다.10 이러한 강력한 생태계는 CUDA를 ’안전한 선택’으로 만드는 가장 큰 이유이다.</p>
</li>
<li>
<p><strong>ROCm의 성장하는 생태계:</strong> ROCm의 생태계는 CUDA에 비해 상대적으로 역사가 짧고 성숙도가 낮다. 과거에는 문서화가 부족하고, 특정 하드웨어나 소프트웨어 조합에서 안정성 문제를 겪는 경우가 있었으며, 커뮤니티 지원이 제한적이라는 평가를 받았다.10 특히, 일부 특수한(niche) 라이브러리나 레거시 도구에 대한 지원이 부족하여 수동 튜닝이 필요한 경우가 여전히 존재한다.39 하지만 AMD는 이러한 약점을 극복하기 위해 공격적인 투자를 진행하고 있다. PyTorch와 TensorFlow와 같은 핵심 프레임워크에 대한 지원을 업스트림에 완전히 통합했으며 3, Hugging Face와의 긴밀한 협력을 통해 수많은 최신 AI 모델에 대한 접근성을 확보했다.3 또한, Docker, Kubernetes와 같은 컨테이너화 도구를 적극적으로 지원하여 배포의 복잡성을 줄이고 최신 DevOps 트렌드에 부합하고 있다.3 생태계는 여전히 성장 단계에 있지만, 그 확장 속도는 매우 빠르다.</p>
</li>
</ul>
<h3>4.3 플랫폼 전략: 개방성, 벤더 종속성, 비용 효율성 분석</h3>
<p>두 플랫폼은 기술적 차이를 넘어 근본적인 비즈니스 및 플랫폼 전략에서 뚜렷한 대조를 보인다.</p>
<ul>
<li>
<p><strong>개방성 vs. 폐쇄성:</strong> ROCm의 가장 큰 차별점은 개방성이다. GPU 펌웨어를 제외한 대부분의 소프트웨어 스택이 오픈소스 라이선스로 제공되어, 누구나 소스 코드를 확인하고, 수정하며, 기여할 수 있다.2 이는 개발자에게 높은 수준의 유연성과 시스템 제어권을 부여하며, 특정 요구에 맞게 플랫폼을 맞춤화할 수 있는 가능성을 열어준다.10 반면, CUDA는 NVIDIA의 독점(proprietary) 플랫폼으로, 내부 구현에 대한 접근이 제한적이다.10</p>
</li>
<li>
<p><strong>벤더 종속성 완화:</strong> CUDA는 NVIDIA GPU에서만 작동하므로, CUDA 기반으로 개발된 소프트웨어 자산은 NVIDIA 하드웨어에 종속된다. ROCm의 HIP은 AMD와 NVIDIA GPU 모두에서 코드를 실행할 수 있는 이식성을 제공함으로써 이러한 벤더 종속성을 완화하는 것을 목표로 한다.9 이는 하드웨어 선택의 유연성을 확보하고, 특정 벤더의 가격 정책이나 기술 로드맵에 종속될 위험을 줄이고자 하는 조직에게 중요한 장점이다.</p>
</li>
<li>
<p><strong>비용 효율성:</strong> 하드웨어 비용 측면에서, 일반적으로 AMD GPU는 동급 성능의 NVIDIA GPU에 비해 더 경쟁력 있는 가격에 제공된다.9 따라서 ROCm을 기반으로 시스템을 구축할 경우, 동일한 예산으로 더 많은 컴퓨팅 자원을 확보하거나, 전체 시스템 구축 비용(TCO, Total Cost of Ownership)을 절감할 수 있다.39 성능 대비 비용(price-performance)을 중시하는 조직에게 ROCm은 매우 매력적인 대안이 될 수 있다.</p>
</li>
</ul>
<p>이러한 다각적인 비교를 통해 각 플랫폼의 장단점을 명확히 할 수 있다. CUDA는 최고의 성능, 안정성, 그리고 방대한 생태계를 바탕으로 한 생산성을 중시하는 경우에 적합한 선택이다. 반면, ROCm은 오픈소스 기반의 유연성, 벤더 종속성 탈피, 그리고 높은 비용 효율성을 우선시하는 경우에 더 나은 선택이 될 수 있다.</p>
<h3>4.4 표 1: ROCm과 CUDA의 핵심 특징 비교</h3>
<p>다음 표는 ROCm과 CUDA의 핵심적인 특징과 전략적 차이점을 요약하여 보여준다.</p>
<table><thead><tr><th><strong>특징</strong></th><th><strong>AMD ROCm</strong></th><th><strong>NVIDIA CUDA</strong></th></tr></thead><tbody>
<tr><td><strong>라이선스</strong></td><td>오픈소스 (GPU 펌웨어 제외) 11</td><td>독점 (Proprietary) 10</td></tr>
<tr><td><strong>주요 프로그래밍 모델</strong></td><td>HIP (C++ 기반), OpenCL, OpenMP 1</td><td>CUDA C/C++, OpenCL, OpenACC 20</td></tr>
<tr><td><strong>하드웨어 종속성</strong></td><td>AMD GPU에 최적화, HIP을 통해 NVIDIA GPU 지원 18</td><td>NVIDIA GPU 전용 10</td></tr>
<tr><td><strong>코드 이식성</strong></td><td>높음 (HIP을 통한 크로스 플랫폼 지원) 18</td><td>낮음 (벤더 종속적)</td></tr>
<tr><td><strong>생태계 성숙도</strong></td><td>성장 중, 빠르게 확장 10</td><td>매우 성숙하고 방대함 17</td></tr>
<tr><td><strong>핵심 라이브러리</strong></td><td>rocBLAS, MIOpen, RCCL 등 3</td><td>cuBLAS, cuDNN, NCCL 등 17</td></tr>
<tr><td><strong>비용 효율성</strong></td><td>높음 (하드웨어 가격 경쟁력) 9</td><td>낮음 (프리미엄 가격 정책)</td></tr>
<tr><td><strong>클라우드 지원</strong></td><td>제한적에서 확장 중 10</td><td>압도적으로 넓음 17</td></tr>
</tbody></table>
<h2>5.  하드웨어 및 운영체제 호환성</h2>
<p>ROCm 플랫폼의 활용 가능성은 지원되는 하드웨어 및 소프트웨어 환경에 의해 결정된다. AMD는 공식 문서를 통해 지원되는 GPU 목록과 운영체제 매트릭스를 상세히 제공하며, 최신 릴리스마다 지원 범위를 지속적으로 확대하고 있다.</p>
<h3>5.1 공식 지원 GPU 목록</h3>
<p>ROCm은 주로 데이터센터 및 전문가용 GPU를 타겟으로 개발되었지만, 최근에는 소비자용 하드웨어에 대한 지원도 크게 강화되고 있다.</p>
<ul>
<li>
<p><strong>데이터센터/HPC용 가속기:</strong> AMD Instinct™ 시리즈 가속기는 ROCm의 최우선 지원 대상이며, 현재 출시된 모든 모델을 공식적으로 지원한다.12 여기에는 엑사스케일 슈퍼컴퓨터에 탑재된 MI200 시리즈, 최신 칩렛 아키텍처 기반의 MI300 시리즈, 그리고 차세대 MI350 시리즈가 포함된다.15 이들 가속기는 ROCm의 모든 기능과 최상의 성능을 제공하도록 설계되었다.</p>
</li>
<li>
<p><strong>워크스테이션용 GPU:</strong> 전문가용 그래픽 및 컴퓨팅 작업을 위한 AMD Radeon™ PRO 시리즈 역시 공식 지원 대상이다. Radeon™ PRO W7900, W7800, W6800 등 RDNA 3 및 RDNA 2 아키텍처 기반의 워크스테이션 GPU들이 지원 목록에 포함되어, AI 모델 개발, 데이터 시각화, 고해상도 렌더링 등 전문적인 워크로드에서 ROCm을 활용할 수 있다.45</p>
</li>
<li>
<p><strong>소비자용 GPU 및 APU:</strong> 공식적으로는 전문가용 제품군을 중심으로 지원이 이루어지지만, 동일한 아키텍처를 공유하는 다수의 소비자용 Radeon™ RX 시리즈 GPU도 ROCm을 사용할 수 있다.1 특히 ROCm 5.x 버전 이후로 RDNA 2 아키텍처(예: Radeon™ RX 6000 시리즈)에 대한 지원이 공식화되었고, 최신 버전에서는 RDNA 3 아키텍처(예: Radeon™ RX 7900 XTX, RX 7800 XT) 및 RDNA 4 아키텍처(예: Radeon™ RX 9000 시리즈)에 대한 지원이 확대되고 있다.45 또한, Ryzen™ AI APU에 대한 초기 지원이 시작되어, 개발자들이 개인용 노트북이나 데스크톱에서 AI 개발 및 추론을 수행할 수 있는 길이 열렸다.47</p>
</li>
<li>
<p><strong>지원 아키텍처:</strong> ROCm은 여러 세대의 AMD GPU 아키텍처를 지원한다. 데이터센터용으로는 CDNA, CDNA 2, CDNA 3, 그리고 최신 CDNA 4 아키텍처를 지원하며, 소비자 및 워크스테이션용으로는 RDNA 2, RDNA 3, 그리고 차세대 RDNA 4 아키텍처를 지원한다.48</p>
</li>
</ul>
<h3>5.2 운영체제 지원 매트릭스</h3>
<p>ROCm은 전통적으로 Linux 환경을 중심으로 개발되었으나, 최근 Windows 및 WSL에 대한 지원을 강화하며 플랫폼 저변을 넓히고 있다.</p>
<ul>
<li>
<p><strong>Linux:</strong> Linux는 ROCm의 가장 안정적이고 포괄적인 기능을 제공하는 주력 지원 운영체제이다. 주요 엔터프라이즈 배포판을 중심으로 공식 지원이 이루어진다.48</p>
</li>
<li>
<p><strong>Ubuntu:</strong> 24.04 LTS, 22.04 LTS 등 최신 LTS 버전을 중심으로 지원된다.</p>
</li>
<li>
<p><strong>Red Hat Enterprise Linux (RHEL):</strong> RHEL 10, 9.x, 8.x 등 기업 환경에서 널리 사용되는 버전을 지원한다.</p>
</li>
<li>
<p><strong>SUSE Linux Enterprise Server (SLES):</strong> SLES 15 SPx 버전을 지원한다.</p>
</li>
<li>
<p><strong>기타 배포판:</strong> 최신 ROCm 릴리스에서는 Debian, Rocky Linux, Oracle Linux, 그리고 Microsoft의 Azure Linux 등 지원하는 배포판의 범위가 크게 확대되어 다양한 클라우드 및 서버 환경에 대한 호환성을 높였다.50</p>
</li>
<li>
<p><strong>Windows:</strong> 과거 ROCm은 Windows를 거의 지원하지 않았으나, ROCm 5.5에서 HIP SDK의 Windows 지원이 공식적으로 추가되면서 중요한 전환점을 맞았다.6 이후 ROCm 7.0부터는 Windows를 1등급(first-class) 지원 운영체제로 격상시켜, Ryzen APU 및 Radeon GPU가 탑재된 노트북과 워크스테이션에서 네이티브 개발 환경을 제공하는 것을 목표로 하고 있다.40 이는 AI 개발의 접근성을 대폭 향상시키는 중요한 전략적 변화이다.</p>
</li>
<li>
<p><strong>WSL (Windows Subsystem for Linux):</strong> Windows 사용자들이 Linux 환경의 ROCm을 쉽게 사용할 수 있도록, WSL2 환경에 대한 지원도 제공된다. 특정 지원 GPU 목록에 포함된 하드웨어를 사용하는 경우, Windows 10 또는 11에서 WSL2를 설치하고 그 안에 Ubuntu와 같은 Linux 배포판을 구성한 뒤, Linux용 ROCm 설치 절차를 따르면 된다.47 이는 Windows 환경을 벗어나지 않고도 완전한 기능의 ROCm 개발 환경을 구축할 수 있는 편리한 방법을 제공한다.</p>
</li>
</ul>
<h3>5.3 표 2: 주요 AMD GPU 아키텍처별 ROCm 지원 현황</h3>
<p>다음 표는 주요 AMD GPU 아키텍처의 세대별 발전과 그에 따른 ROCm의 지원 수준 및 핵심 특징을 요약한 것이다. 이를 통해 사용자는 자신의 하드웨어에 적합한 ROCm 버전을 선택하고 해당 아키텍처의 기능적 특징을 이해하는 데 도움을 받을 수 있다.</p>
<table><thead><tr><th><strong>아키텍처</strong></th><th><strong>주요 GPU 모델</strong></th><th><strong>LLVM Target</strong></th><th><strong>ROCm 지원 상태</strong></th><th><strong>주요 특징</strong></th></tr></thead><tbody>
<tr><td><strong>CDNA 4</strong></td><td>Instinct MI350 Series</td><td><code>gfx950</code></td><td>ROCm 7.0+ 부터 공식 지원 50</td><td>FP4/FP6 데이터 타입 지원, AI 추론 성능 극대화 16</td></tr>
<tr><td><strong>CDNA 3</strong></td><td>Instinct MI300 Series</td><td><code>gfx942</code></td><td>ROCm 6.0+ 부터 공식 지원 49</td><td>칩렛(Chiplet) 아키텍처, 대용량 HBM 메모리 28</td></tr>
<tr><td><strong>CDNA 2</strong></td><td>Instinct MI200 Series</td><td><code>gfx90a</code></td><td>ROCm 5.0+ 부터 공식 지원 12</td><td>엑사스케일 슈퍼컴퓨터(Frontier) 채택 아키텍처 27</td></tr>
<tr><td><strong>RDNA 4</strong></td><td>Radeon RX 9000 Series</td><td><code>gfx1201</code>, <code>gfx1200</code></td><td>ROCm 7.0+ 부터 공식 지원 50</td><td>차세대 소비자 및 워크스테이션 AI 개발 지원</td></tr>
<tr><td><strong>RDNA 3</strong></td><td>Radeon RX 7000 Series, PRO W7000 Series</td><td><code>gfx1100</code>, <code>gfx1101</code></td><td>공식 지원 확대 중 45</td><td>메인스트림 AI 개발 및 추론 지원 강화 47</td></tr>
<tr><td><strong>RDNA 2</strong></td><td>Radeon RX 6000 Series, PRO W6000 Series</td><td><code>gfx1030</code></td><td>ROCm 5.x 부터 공식 지원 1</td><td>Blender Cycles 렌더러 등 그래픽스 연산 가속 1</td></tr>
</tbody></table>
<h2>6.  주요 머신러닝 프레임워크 지원 현황</h2>
<p>현대 AI 개발은 PyTorch, TensorFlow, JAX와 같은 고수준 머신러닝 프레임워크를 중심으로 이루어진다. 따라서 GPGPU 플랫폼의 성공은 이들 핵심 프레임워크와의 통합 수준과 안정성에 크게 좌우된다. AMD는 ROCm 생태계의 확산을 위해 이들 프레임워크를 지원하는 데 막대한 노력을 기울여 왔으며, 단순한 호환성 제공을 넘어 성능 최적화 단계로 나아가고 있다.</p>
<p>AMD의 프레임워크 지원 전략에서 두드러지는 특징은 ‘Docker 우선(Docker-first)’ 접근 방식이다. ROCm 스택, GPU 드라이버, 프레임워크 버전 간의 복잡한 의존성 문제를 해결하고 사용자에게 안정적이고 재현 가능한 개발 환경을 신속하게 제공하기 위해, 사전 구성 및 테스트가 완료된 Docker 이미지를 사용하는 것을 최우선으로 권장한다.53 이는 사용자가 복잡한 설치 과정에서 겪는 어려움을 최소화하고 “의존성 지옥(dependency hell)“을 피하게 함으로써, 생태계 초기 단계에서 사용자 이탈을 막고 긍정적인 첫인상을 심어주기 위한 매우 현실적이고 효과적인 전략이다.</p>
<h3>6.1 PyTorch 지원</h3>
<ul>
<li>
<p><strong>현황:</strong> AMD는 PyTorch 재단(PyTorch Foundation)의 창립 멤버로서 PyTorch 생태계에 깊이 관여하고 있다.12 ROCm 지원 코드는 PyTorch의 공식 메인 저장소에 완전히 업스트림(upstream)되어 있어, PyTorch의 새로운 릴리스와 긴밀하게 통합된다.3 이는 ROCm이 PyTorch 커뮤니티에서 1등급(first-class) 플랫폼으로 인정받고 있음을 의미한다. AMD는 공식 PyTorch 릴리스와는 별개로, 최신 ROCm 버전에 대한 지원이나 긴급 버그 수정을 더 빠르게 제공하기 위해 자체적인 <code>ROCm/pytorch</code> 저장소도 운영한다.56</p>
</li>
<li>
<p><strong>설치 방법:</strong></p>
</li>
</ul>
<ol>
<li>
<p><strong>Docker 이미지 (권장):</strong> AMD가 공식적으로 빌드하고 테스트하여 Docker Hub의 <code>rocm/pytorch</code> 저장소에 게시하는 이미지를 사용하는 것이 가장 안정적이고 간편하다.53 이 이미지들에는 특정 ROCm 버전, PyTorch 버전, 운영체제, 그리고 torchvision, Apex와 같은 관련 라이브러리들이 최적의 조합으로 사전 설치되어 있다.</p>
</li>
<li>
<p><strong>Wheels 패키지:</strong> PyTorch의 공식 웹사이트(<a href="https://pytorch.org/get-started/locally/">pytorch.org</a>)에서 자신의 환경(Linux, pip, ROCm)에 맞는 설치 명령어를 생성하여 <code>pip</code>으로 직접 설치할 수 있다.53 최신 ROCm 버전에 대한 지원은 주로 Nightly 빌드를 통해 제공된다.</p>
</li>
<li>
<p><strong>소스 코드 빌드:</strong> 특정 GPU 아키텍처에 대한 최적화(예: <code>PYTORCH_ROCM_ARCH</code> 환경 변수 설정)나 사용자 정의 기능 추가가 필요한 경우, PyTorch 소스 코드를 직접 클론하여 빌드할 수 있다.53</p>
</li>
</ol>
<ul>
<li><strong>호환성 및 최적화:</strong> ROCm 버전과 PyTorch 버전 간의 상세한 호환성 매트릭스는 공식 문서를 통해 제공된다.48 ROCm 7.0은 PyTorch 2.7 및 2.8을 지원한다.15 최근에는 단순한 기능 호환성을 넘어, FlashAttention-2의 역전파(backward pass) 활성화 47, APEX 라이브러리에 Fused Rope 커널 통합 15 등 AMD 하드웨어의 특성을 활용한 저수준 성능 최적화가 활발히 이루어지고 있다.</li>
</ul>
<h3>6.2 TensorFlow 지원</h3>
<ul>
<li>
<p><strong>현황:</strong> TensorFlow 역시 ROCm을 공식적으로 지원하며, 관련 지원 코드가 TensorFlow의 메인 저장소에 통합되어 있다.54 PyTorch와 마찬가지로, AMD는 신속한 버그 수정 및 최신 ROCm 버전 지원을 위해 별도의 <code>ROCm/tensorflow-upstream</code> 포크(fork) 저장소를 유지 및 관리하고 있다.54</p>
</li>
<li>
<p><strong>설치 방법:</strong></p>
</li>
</ul>
<ol>
<li>
<p><strong>Docker 이미지 (권장):</strong> <code>rocm/tensorflow</code> Docker 이미지를 사용하는 것이 가장 권장되는 방법이다. 이 이미지에는 TensorFlow와 ROCm 백엔드가 사전 설치되어 있어 즉시 사용 가능하다.54</p>
</li>
<li>
<p><strong>Wheels 패키지:</strong> <code>tensorflow-rocm</code>이라는 이름으로 제공되는 pip 패키지를 통해 설치할 수 있다.52 AMD의 공식 저장소에서 제공되는 wheels 파일을 <code>pip</code>으로 설치한다.</p>
</li>
</ol>
<ul>
<li><strong>호환성 및 최적화:</strong> ROCm 7.0은 TensorFlow 2.19.1을 지원한다.15 TensorFlow는 ROCm 환경에서 MIOpen 라이브러리를 통해 컨볼루션과 같은 딥러닝 핵심 연산을 가속하고, <code>hipBLAS</code>, <code>rocThrust</code> 등의 수학 라이브러리를 활용하여 다양한 텐서 연산을 효율적으로 처리한다.54</li>
</ul>
<h3>6.3 JAX 지원</h3>
<ul>
<li>
<p><strong>현황:</strong> 고성능 머신러닝 연구에서 인기를 얻고 있는 JAX는 AMD가 직접 지원하는 ROCm 플러그인을 통해 AMD GPU를 지원한다.55 JAX의 컴파일러 백엔드인 XLA(Accelerated Linear Algebra)가 ROCm을 타겟으로 코드를 생성하는 방식이다. 현재 Linux 환경에서 공식적으로 지원되며, Windows WSL2 환경에서는 실험적으로 지원되고 있다.58</p>
</li>
<li>
<p><strong>설치 방법:</strong> JAX의 ROCm 지원 설치는 다른 프레임워크에 비해 다소 복잡할 수 있어, AMD가 제공하는 <code>rocm/jax</code> Docker 이미지를 사용하는 것이 가장 간단하고 확실한 방법으로 권장된다.55 소스에서 직접 빌드할 경우, JAX, XLA, 그리고 <code>jax-rocm-plugin</code>을 특정 순서와 버전에 맞춰 빌드해야 한다.</p>
</li>
<li>
<p><strong>호환성:</strong> ROCm 7.0은 JAX 0.6.0 버전을 지원한다.15 JAX 지원은 비교적 최근에 본격화되었지만, LUMI와 같은 대규모 슈퍼컴퓨터에서 JAX를 활용하려는 수요가 증가함에 따라 AMD와 HPC 커뮤니티의 지원이 빠르게 강화되고 있다.59</p>
</li>
</ul>
<h2>7.  최신 동향: ROCm 7.0의 혁신</h2>
<p>2025년 발표된 ROCm 7.0은 AMD의 개방형 GPU 컴퓨팅 전략이 새로운 단계로 진입했음을 알리는 중요한 이정표이다. 이 릴리스는 단순한 성능 향상을 넘어, AI, 특히 생성 AI(Generative AI) 시대의 요구에 부응하기 위한 근본적인 아키텍처 개선과 플랫폼 확장을 포함하고 있다. ROCm 7.0의 혁신은 AMD가 ‘학습(Training)’ 시장에서의 경쟁을 넘어, 폭발적으로 성장하는 ‘추론(Inference)’ 시장에서 기술적 우위를 확보하려는 전략적 무게 중심의 이동을 명확히 보여준다.</p>
<h3>7.1 주요 기능 및 아키텍처 개선 사항</h3>
<p>ROCm 7.0은 알고리즘부터 인프라까지 AI 스택의 모든 계층에 걸쳐 혁신을 가속화하는 것을 목표로 한다.40</p>
<ul>
<li>
<p><strong>차세대 하드웨어 완벽 지원:</strong> ROCm 7.0의 가장 핵심적인 변화는 AMD의 차세대 데이터센터 가속기인 Instinct MI350 시리즈(MI355X, MI350X)와 이를 구동하는 CDNA 4 아키텍처를 완벽하게 지원한다는 점이다.3 이는 최신 하드웨어의 성능을 최대한 끌어낼 수 있는 소프트웨어 기반을 마련했음을 의미한다.</p>
</li>
<li>
<p><strong>저정밀도 데이터 타입(FP4, FP6, FP8) 지원:</strong> 대규모 언어 모델(LLM)의 추론 과정에서 메모리 사용량과 연산량을 줄이는 것은 비용 효율성과 직결된다. ROCm 7.0은 HIP 런타임 수준에서 FP4, FP6, FP8과 같은 새로운 저정밀도 데이터 타입을 공식적으로 지원한다.3 이를 통해 약간의 정확도 손실을 감수하는 대신, 모델의 메모리 점유율을 대폭 줄이고 추론 처리량을 극대화할 수 있다. 이는 추론 서비스의 비용을 절감하고 더 많은 사용자를 수용할 수 있게 하는 핵심 기술이다.</p>
</li>
<li>
<p><strong>분산 추론 및 스케일 아웃 기능 강화:</strong> 단일 GPU의 성능을 넘어, 수백, 수천 개의 GPU를 묶어 거대한 모델을 서비스하는 분산 추론의 중요성이 커지고 있다. ROCm 7.0은 vLLM-d, DeepEP, SGLang과 같은 최신 오픈소스 분산 서빙 프레임워크와의 긴밀한 통합을 통해 다중 노드, 다중 GPU 환경에서의 효율적인 추론 오케스트레이션을 지원한다.13 특히 DeepEP 추론 엔진은 연산과 데이터 전송을 지능적으로 파이프라이닝하여 GPU 노드 간의 통신 오버헤드를 최소화하고 전체 시스템의 활용률을 높인다.60</p>
</li>
<li>
<p><strong>엔터프라이즈급 AI 도구 도입:</strong> 기업 환경에서 AI 모델을 안정적으로 운영하고 관리(MLOps)하기 위한 도구들이 도입되었다. AMD Resource Manager와 새로운 AI Workbench는 클러스터 수준의 자원 관리, 워크로드 오케스트레이션, 모니터링 기능을 제공하여 대규모 AI 배포 및 관리를 용이하게 한다.13</p>
</li>
<li>
<p><strong>‘클라우드에서 엣지까지’ 플랫폼 확장:</strong> ROCm 7.0의 가장 중요한 전략적 변화 중 하나는 지원 플랫폼의 확장이다. 기존의 데이터센터용 Instinct 가속기 중심에서 벗어나, Ryzen AI 프로세서가 탑재된 노트북과 Radeon GPU 기반의 워크스테이션까지 공식 지원 범위에 포함시켰다.13 이는 개발자가 자신의 개인용 컴퓨터에서 개발하고 테스트한 AI 모델을 코드 변경 없이 클라우드의 대규모 클러스터에 배포할 수 있는 일관된 개발 경험을 제공한다. 이러한 전략은 CPU, GPU, APU, FPGA(Xilinx 인수)를 모두 아우르는 AMD의 포괄적인 반도체 포트폴리오의 강점을 극대화하는 것이다. ROCm이라는 단일 소프트웨어 플랫폼으로 이 모든 이기종 하드웨어를 묶음으로써, NVIDIA가 제공하기 어려운 ’통합 이기종 컴퓨팅 생태계’를 구축하려는 장기적인 비전을 보여준다.</p>
</li>
</ul>
<h3>7.2 성능 향상 분석</h3>
<p>ROCm 7.0은 소프트웨어 최적화를 통해 상당한 성능 향상을 이루었다.</p>
<ul>
<li>
<p><strong>추론 성능:</strong> AMD의 내부 벤치마크에 따르면, ROCm 7.0은 이전 버전인 ROCm 6.0 대비 다양한 LLM 추론 워크로드에서 평균 3.5배, 최대 4.6배의 성능 향상을 기록했다.16 이는 저정밀도 데이터 타입 지원, 최적화된 어텐션 커널, 그리고 향상된 분산 처리 기술이 복합적으로 작용한 결과이다.</p>
</li>
<li>
<p><strong>학습 성능:</strong> 추론에 초점이 맞춰졌음에도 불구하고, 학습 성능 또한 크게 개선되었다. Llama 2 70B, Llama 3.1 8B 등의 모델 학습에서 ROCm 6.0 대비 평균 3배의 성능 향상을 달성했다.16 이는 새로운 FP8 정밀도 지원과 최적화된 통신 라이브러리(RCCL) 덕분이다.</p>
</li>
</ul>
<p>이러한 성능 향상은 ROCm이 더 이상 CUDA의 기능을 따라가는 수준을 넘어, 특정 워크로드에서는 CUDA를 능가하는 성능을 제공할 수 있는 잠재력을 갖추었음을 보여준다.</p>
<h2>8.  활용 사례 및 연구 동향</h2>
<p>ROCm은 이론적인 가능성을 넘어, 세계에서 가장 까다로운 컴퓨팅 환경인 고성능 컴퓨팅(HPC)과 빠르게 변화하는 인공지능(AI) 분야에서 실질적인 영향력을 입증하고 있다. 대규모 슈퍼컴퓨터부터 최신 AI 모델 개발에 이르기까지 ROCm의 활용 사례는 그 기술적 성숙도와 신뢰성을 보여준다. 또한, 최신 학술 연구에서도 ROCm은 새로운 알고리즘 구현, 성능 분석, 그리고 이기종 컴퓨팅 패러다임 탐구의 중요한 도구로 사용되고 있다.</p>
<h3>8.1 HPC (고성능 컴퓨팅) 분야</h3>
<p>HPC는 ROCm이 태동한 근간이자 가장 중요한 적용 분야 중 하나이다.</p>
<ul>
<li>
<p><strong>엑사스케일 슈퍼컴퓨터:</strong> ROCm의 성능과 확장성을 가장 극적으로 보여주는 사례는 세계 최고 수준의 슈퍼컴퓨터들이다. 미국 오크리지 국립연구소(ORNL)의 <strong>Frontier</strong>와 로렌스 리버모어 국립연구소(LLNL)의 <strong>El Capitan</strong>은 각각 세계 최초와 두 번째 엑사플롭스(초당 100경 번 연산) 성능을 달성한 시스템으로, 두 시스템 모두 AMD EPYC™ CPU와 AMD Instinct™ GPU 가속기를 기반으로 구축되었다.3 이 거대한 시스템들의 수만 개에 달하는 GPU를 효율적으로 프로그래밍하고 관리하는 핵심 소프트웨어 스택이 바로 ROCm이다.1 이는 ROCm이 극단적인 규모의 병렬 컴퓨팅 환경에서도 안정적으로 동작하며 최고의 성능을 이끌어낼 수 있음을 증명한다.</p>
</li>
<li>
<p><strong>과학 컴퓨팅 애플리케이션:</strong> ROCm은 다양한 과학 및 공학 분야의 시뮬레이션 애플리케이션을 가속하는 데 널리 사용된다. 유럽의 LUMI 슈퍼컴퓨터는 ROCm을 활용하여 기후 변화 연구, 암 연구, 신소재 개발 등 복잡한 과학적 난제를 해결하고 있다.27 대표적인 분자 동역학 시뮬레이션 코드인 <strong>GROMACS</strong>, 계산 화학, 전산 유체 역학(CFD), 유전체학, 물리학 등 전통적인 HPC 애플리케이션들이 ROCm 환경에 맞게 이식되거나 개발되고 있다.5 AMD는 이러한 주요 HPC 애플리케이션들을 컨테이너화하여 AMD Infinity Hub를 통해 제공함으로써, 연구자들이 복잡한 설치 과정 없이 즉시 연구에 활용할 수 있도록 지원하고 있다.9</p>
</li>
</ul>
<h3>8.2 AI (인공지능) 분야</h3>
<p>HPC에서의 성공을 바탕으로, ROCm은 AI, 특히 생성 AI 분야로 빠르게 영향력을 확장하고 있다.</p>
<ul>
<li>
<p><strong>대규모 언어 모델(LLM) 학습 및 추론:</strong> ROCm은 LLM의 전체 수명 주기를 지원한다. Frontier 슈퍼컴퓨터에서는 ROCm을 사용하여 1조 개 이상의 파라미터를 가진 거대 모델을 성공적으로 학습시킨 사례가 있으며 3, Microsoft의 GPT-4와 같은 최첨단 상용 모델을 대규모로 실행하는 데에도 ROCm이 활용되고 있다.12</p>
</li>
<li>
<p><strong>개방형 AI 생태계와의 통합:</strong> AMD는 개방형 AI 생태계의 핵심 플레이어들과 긴밀하게 협력하고 있다. <strong>Hugging Face</strong>와의 파트너십을 통해 플랫폼에서 제공되는 수많은 최신 모델들이 ROCm에서 원활하게 작동하도록 지원하며 3, <strong>PyTorch, TensorFlow, JAX, ONNX, Triton</strong>과 같은 주요 AI 프레임워크 및 컴파일러와 완벽하게 통합되어 있다.3 이는 개발자들이 기존에 사용하던 도구와 워크플로우를 그대로 유지하면서 AMD 하드웨어의 이점을 누릴 수 있게 해준다.</p>
</li>
<li>
<p><strong>컨테이너 기반의 유연한 배포:</strong> AI 모델의 개발, 테스트, 배포 과정을 효율적으로 관리하기 위해 컨테이너 기술의 중요성이 부각되고 있다. ROCm은 <strong>Docker, Singularity, Kubernetes, Slurm</strong>과 같은 업계 표준 컨테이너 및 클러스터 관리 도구를 완벽하게 지원한다.3 이를 통해 개발자들은 재현 가능하고 이식성 높은 환경에서 AI 모델을 개발하고, 이를 수천 개의 GPU로 구성된 대규모 클러스터에 원활하게 배포 및 확장할 수 있다.</p>
</li>
</ul>
<h3>8.3 ROCm 기반 최신 학술 연구 분석</h3>
<p>ROCm의 개방성과 확장성은 학술 연구자들에게 새로운 탐구의 기회를 제공하고 있다. 최신 연구 논문들은 ROCm을 활용하여 GPU 아키텍처의 깊은 곳을 탐구하고, 새로운 병렬 알고리즘을 구현하며, 차세대 컴퓨팅 패러다임을 모색하고 있다.</p>
<ul>
<li>
<p><strong>성능 모델링 및 분석:</strong></p>
</li>
<li>
<p>논문: “Metrics and Design of an Instruction Roofline Model for AMD GPUs” 62</p>
</li>
</ul>
<p>이 연구는 AMD GPU의 성능 병목 현상을 시각적으로 분석하는 도구인 ’명령어 루프라인 모델(Instruction Roofline Model)’을 설계하고 구현하는 방법을 제시했다. 연구진은 AMD의 프로파일링 도구가 제공하는 메트릭을 기반으로 이론적인 최대 성능과 실제 애플리케이션의 성능을 비교 분석함으로써, 코드 최적화 방향을 제시했다. 이는 ROCm 생태계의 성능 분석 도구와 방법론이 학술적으로도 활발히 연구되고 있음을 보여준다.</p>
<ul>
<li>논문: “LLM-based Performance Prediction for GPU Kernels” 63</li>
</ul>
<p>이 연구는 LLM을 사용하여 주어진 GPU 커널 코드의 성능을 예측하는 혁신적인 접근 방식을 제안했다. 연구에서는 AMD MI250 및 MI300X 아키텍처를 대상으로 실험을 진행했으며, ROCm Compute Profiler와 같은 기존 프로파일링 도구의 높은 오버헤드를 지적하며, LLM 기반 예측 모델의 효용성을 주장했다.</p>
<ul>
<li>
<p><strong>알고리즘 구현 및 최적화:</strong></p>
</li>
<li>
<p>논문: “An Efficient Implementation of Subsequence Dynamic Time Warping on AMD GPUs” 64</p>
</li>
</ul>
<p>시계열 데이터 분석에 널리 사용되는 sDTW(subsequence Dynamic Time Warping) 알고리즘을 AMD GPU에 효율적으로 구현한 사례이다. 이 연구는 HIP 프로그래밍 모델을 사용했을 뿐만 아니라, 웨이브프론트 내 스레드 간 통신을 위한 셔플(shuffle) 명령어와 블록 내 스레드 간 데이터 공유를 위한 공유 메모리(shared memory) 등 ROCm이 제공하는 저수준 하드웨어 제어 기능을 적극적으로 활용하여 높은 성능을 달성했다.</p>
<ul>
<li>
<p><strong>이기종 컴퓨팅 및 이식성 연구:</strong></p>
</li>
<li>
<p>논문: “GROMACS on AMD GPU-Based HPC Platforms: Using SYCL for Performance and Portability” 61</p>
</li>
</ul>
<p>이 연구는 HIP 대신 또 다른 개방형 표준인 SYCL을 사용하여 GROMACS를 AMD GPU에서 실행하려는 시도를 다루었다. 이는 단일 소스 코드로 AMD, NVIDIA, Intel GPU를 모두 지원하려는 ’성능 이식성(performance portability)’에 대한 HPC 커뮤니티의 높은 관심을 보여준다. ROCm/HIP이 AMD의 주력 모델이지만, SYCL과 같은 벤더 중립적인 대안적 프로그래밍 모델에 대한 탐색과 연구가 활발히 이루어지고 있음을 시사하는 중요한 사례이다.</p>
<ul>
<li>
<p><strong>시스템 수준 최적화:</strong></p>
</li>
<li>
<p>논문: “Concurrent Computation and Communication on GPUs for ML” 65</p>
</li>
</ul>
<p>이 연구는 AMD Instinct MI300X GPU에서 대규모 ML 학습 시 발생하는 연산(computation)과 통신(communication)을 중첩(concurrency)시킬 때 발생하는 성능 간섭 현상을 심도 있게 분석했다. 연구진은 GPU의 DMA(Direct Memory Access) 엔진을 직접 활용하여 통신 작업을 오프로드하는 새로운 통신 라이브러리(ConCCL)의 프로토타입을 구현함으로써, 연산과 통신 간의 간섭을 줄이고 시스템 전체의 효율성을 크게 향상시킬 수 있음을 보였다. 이는 ROCm 플랫폼을 기반으로 하드웨어의 저수준 기능까지 제어하여 시스템 전체를 최적화하려는 연구가 진행되고 있음을 보여준다.</p>
<h2>9.  결론 및 미래 전망</h2>
<h3>9.1 ROCm의 현재 위상 요약</h3>
<p>ROCm은 지난 몇 년간 괄목할 만한 발전을 거듭하며, GPGPU 시장에서 NVIDIA CUDA의 독점적 지위에 도전하는 가장 강력한 대안으로 부상했다. 특히 ‘개방성’, ‘비용 효율성’, 그리고 HIP을 통한 ’코드 이식성’이라는 세 가지 핵심 가치를 무기로 HPC 및 AI 시장에서 그 영향력을 빠르게 확대하고 있다. Frontier와 El Capitan과 같은 세계 최고 수준의 엑사스케일 슈퍼컴퓨터에 채택됨으로써 그 성능과 안정성을 입증했으며, PyTorch와 TensorFlow 등 주요 AI 프레임워크와의 완전한 통합을 통해 AI 개발자 생태계로의 진입에도 성공했다.</p>
<p>물론, 생태계의 성숙도, 문서화의 깊이, 그리고 커뮤니티 지원의 폭 측면에서는 수십 년간 시장을 지배해 온 CUDA에 비해 여전히 열세에 있는 것이 사실이다. 그러나 AMD의 지속적인 투자와 개방형 커뮤니티와의 협력을 통해 그 격차는 빠르게 좁혀지고 있으며, ROCm 7.0에서 보여준 혁신은 ROCm이 더 이상 단순한 추격자가 아닌, 특정 영역에서는 시장을 선도할 수 있는 잠재력을 갖춘 플랫폼으로 성장했음을 시사한다.</p>
<h3>9.2 GPU 컴퓨팅 시장에서의 미래 전망</h3>
<p>향후 GPU 컴퓨팅 시장의 지형을 바꿀 몇 가지 중요한 기술적, 산업적 트렌드가 있으며, ROCm은 이러한 변화의 흐름 속에서 중요한 기회를 맞이할 것으로 전망된다.</p>
<ul>
<li>
<p><strong>개방형 표준의 부상:</strong> 개발자 커뮤니티와 산업계 전반에서 특정 벤더에 대한 종속성을 줄이려는 움직임이 강화되고 있다. Triton, OpenXLA, SYCL과 같은 벤더 중립적인 고수준 프로그래밍 모델 및 컴파일러 기술의 등장은 이러한 흐름을 가속화할 것이다.9 이러한 기술들은 개발자가 특정 하드웨어 API(CUDA C++ 또는 HIP C++)에 직접 의존하지 않고도 고성능 코드를 작성할 수 있게 해준다. 근본적으로 개방형 철학에 기반을 둔 ROCm은 이러한 표준 기술들과의 통합에 매우 유리한 위치에 있으며, 장기적으로는 개발자들이 하드웨어를 자유롭게 선택할 수 있는 개방형 생태계의 중심축 역할을 할 수 있다.</p>
</li>
<li>
<p><strong>추론(Inference) 시장의 중요성 증대:</strong> AI 시장이 성숙 단계로 접어들면서, 거대 모델을 한 번 ’학습’하는 비용만큼이나, 학습된 모델을 수백만 사용자에게 효율적으로 ’서빙’하는 추론의 비용과 성능이 중요해지고 있다. 추론 시장은 학습 시장보다 규모가 크고 지속적인 수요를 창출할 잠재력을 가지고 있다. ROCm 7.0에서 보여준 저정밀도 데이터 타입 지원, 분산 추론 기능 강화 등 추론 워크로드에 대한 집중적인 최적화는 이러한 시장 변화에 선제적으로 대응하기 위한 명확한 전략적 행보로 평가된다. AMD가 추론 시장에서 가격 대비 성능 우위를 확보한다면, 전체 AI 시장에서의 점유율을 극적으로 확대할 수 있을 것이다.</p>
</li>
<li>
<p><strong>이기종 컴퓨팅(Heterogeneous Computing) 시대의 도래:</strong> 미래의 복잡한 AI 및 HPC 워크로드는 더 이상 단일 종류의 프로세서만으로 해결할 수 없다. CPU, GPU, NPU(신경망 처리 장치), FPGA 등 각기 다른 장점을 가진 프로세서들을 하나의 시스템 안에서 유기적으로 통합하여 활용하는 이기종 컴퓨팅이 차세대 컴퓨팅의 핵심 패러다임으로 부상하고 있다. AMD는 EPYC CPU, Instinct 및 Radeon GPU, Ryzen AI APU, 그리고 Xilinx 인수를 통해 확보한 FPGA 및 적응형 SoC에 이르기까지, 업계에서 가장 폭넓은 고성용 컴퓨팅 하드웨어 포트폴리오를 보유한 유일한 기업이다. ROCm을 이 모든 이기종 하드웨어를 아우르는 단일 통합 소프트웨어 플랫폼으로 발전시키는 것은 AMD의 장기적인 비전이다. 이 비전이 성공적으로 실현된다면, AMD는 NVIDIA가 제공하기 어려운 강력한 ’통합 이기종 컴퓨팅 솔루션’을 시장에 제공하며, 차세대 컴퓨팅 시대의 리더로 발돋움할 수 있는 독보적인 경쟁력을 확보하게 될 것이다. 따라서 ROCm의 성공은 단순히 GPU 시장에서의 점유율 경쟁을 넘어, AMD가 이기종 컴퓨팅 시대의 미래를 주도할 수 있는지를 가늠하는 중요한 척도가 될 것이다.</p>
</li>
</ul>
<h2>10.  부록: ROCm 설치 가이드</h2>
<p>이 부록에서는 주요 Linux 배포판 및 Windows 환경에서 ROCm을 설치하는 구체적인 절차를 제공한다. 설치 전, 반드시 공식 ROCm 문서에서 자신의 하드웨어(GPU)와 운영체제 버전이 지원 목록에 포함되어 있는지 확인해야 한다.</p>
<h3>10.1 Linux 환경 설치</h3>
<p>Linux 환경에서는 각 배포판의 네이티브 패키지 관리자를 사용하는 것이 가장 권장되는 설치 방법이다. 아래는 ROCm 7.0.2 버전을 기준으로 한 주요 배포판별 설치 명령어 요약이다.51</p>
<h4>10.1.1 사전 요구사항</h4>
<ol>
<li>
<p>지원되는 Linux 배포판 및 커널 버전.</p>
</li>
<li>
<p>공식적으로 지원되는 AMD GPU.</p>
</li>
<li>
<p>시스템 재부팅을 위한 <code>sudo</code> 권한.</p>
</li>
</ol>
<h4>10.1.2 배포판별 설치 명령어</h4>
<table><thead><tr><th><strong>운영체제</strong></th><th><strong>버전</strong></th><th><strong>ROCm 설치 명령어</strong></th></tr></thead><tbody>
<tr><td><strong>Ubuntu</strong></td><td><strong>22.04</strong></td><td><code>bash # 설치 유틸리티 다운로드 및 설치 wget https://repo.radeon.com/amdgpu-install/7.0.2/ubuntu/jammy/amdgpu-install_7.0.2.70002-1_all.deb sudo apt install./amdgpu-install_7.0.2.70002-1_all.deb # 저장소 업데이트 및 필수 패키지 설치 sudo apt update sudo apt install python3-setuptools python3-wheel # 사용자 그룹 추가 sudo usermod -a -G render,video $LOGNAME # ROCm 메타 패키지 설치 sudo apt install rocm</code></td></tr>
<tr><td><strong>RHEL</strong></td><td><strong>9.6</strong></td><td><code>bash # Enterprise Linux 등록 및 업데이트 (사전 필요) # 설치 유틸리티 설치 sudo dnf install https://repo.radeon.com/amdgpu-install/7.0.2/rhel/9.6/amdgpu-install-7.0.2.70002-1.el9.noarch.rpm sudo dnf clean all # EPEL 저장소 및 필수 패키지 설치 wget https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm sudo rpm -ivh epel-release-latest-9.noarch.rpm sudo dnf config-manager --enable codeready-builder-for-rhel-9-x86_64-rpms sudo dnf install python3-setuptools python3-wheel # 사용자 그룹 추가 sudo usermod -a -G render,video $LOGNAME # ROCm 메타 패키지 설치 sudo dnf install rocm</code></td></tr>
<tr><td><strong>SLES</strong></td><td><strong>15.7</strong></td><td><code>bash # Enterprise Linux 등록 및 업데이트 (사전 필요) # 필수 모듈 활성화 sudo SUSEConnect -p sle-module-desktop-applications/15.7/x86_64 sudo SUSEConnect -p sle-module-development-tools/15.7/x86_64 sudo SUSEConnect -p PackageHub/15.7/x86_64 # 설치 유틸리티 설치 sudo zypper --no-gpg-checks install https://repo.radeon.com/amdgpu-install/7.0.2/sle/15.7/amdgpu-install-7.0.2.70002-1.noarch.rpm sudo zypper --gpg-auto-import-keys refresh # 필수 패키지 설치 sudo zypper addrepo https://download.opensuse.org/repositories/science/SLE_15_SP5/science.repo sudo zypper install python3-setuptools python3-wheel # 사용자 그룹 추가 sudo usermod -a -G render,video $LOGNAME # ROCm 메타 패키지 설치 sudo zypper install rocm</code></td></tr>
</tbody></table>
<h4>10.1.3 설치 후 절차</h4>
<ol>
<li>
<p>위 명령어를 실행하여 ROCm 패키지 설치를 완료한다.</p>
</li>
<li>
<p>모든 설정이 적용되도록 시스템을 <strong>반드시 재부팅</strong>한다.</p>
</li>
<li>
<p>재부팅 후 터미널에서 다음 명령어를 실행하여 설치가 정상적으로 완료되었는지 확인한다.</p>
</li>
</ol>
<ul>
<li>
<p><code>rocminfo</code>: 시스템에 설치된 GPU 정보와 ROCm 관련 세부 사항을 출력한다.</p>
</li>
<li>
<p><code>rocm-smi</code>: GPU의 현재 상태(온도, 클럭, 메모리 사용량 등)를 실시간으로 보여준다.</p>
</li>
</ul>
<h3>10.2 Windows 및 WSL 환경 설치</h3>
<p>최신 ROCm은 Windows 네이티브 및 WSL(Windows Subsystem for Linux) 환경을 지원한다.</p>
<h4>10.2.1 Windows 네이티브 (HIP SDK)</h4>
<ol>
<li>
<p><strong>사전 요구사항:</strong> 지원되는 Windows 10/11 버전, 지원되는 Radeon PRO 또는 Radeon RX GPU.46</p>
</li>
<li>
<p><strong>드라이버 설치:</strong> AMD 공식 웹사이트에서 최신 버전의 Adrenalin Edition 또는 PRO Edition 드라이버를 다운로드하여 설치한다. 설치 과정에서 ‘HIP SDK’ 구성 요소를 선택적으로 설치할 수 있다.</p>
</li>
<li>
<p><strong>개발 환경:</strong> Visual Studio와 같은 C++ 개발 환경에서 HIP SDK를 사용하여 Windows 네이티브 애플리케이션을 개발할 수 있다.</p>
</li>
</ol>
<h4>10.2.2 WSL (Windows Subsystem for Linux)</h4>
<p>WSL을 사용하면 Windows 환경 내에서 완전한 기능의 Linux용 ROCm을 사용할 수 있다.</p>
<ol>
<li>
<p><strong>사전 요구사항:</strong> 지원되는 Windows 버전, WSL2, 그리고 WSL 환경에서 ROCm이 지원하는 특정 GPU 모델.52</p>
</li>
<li>
<p><strong>WSL2 설치:</strong> Microsoft의 공식 가이드에 따라 WSL2를 설치하고, 지원되는 Linux 배포판(예: Ubuntu 22.04)을 Microsoft Store에서 설치한다.52</p>
</li>
<li>
<p><strong>드라이버 설치:</strong> Windows 호스트 시스템에 최신 AMD 드라이버를 설치한다. 이 드라이버는 WSL2가 GPU에 접근하는 데 필요한 커널 드라이버를 포함한다.</p>
</li>
<li>
<p><strong>WSL 내부에서 ROCm 설치:</strong> WSL 터미널을 실행한 후, 위 <strong>Linux 환경 설치</strong> 섹션의 Ubuntu용 설치 절차를 그대로 따른다.52</p>
</li>
<li>
<p><strong>설치 확인:</strong> WSL 터미널 내에서 <code>rocminfo</code>와 <code>rocm-smi</code>를 실행하여 GPU가 정상적으로 인식되는지 확인한다.</p>
</li>
</ol>
<h2>11. 참고 자료</h2>
<ol>
<li>ROCm - Wikipedia, https://en.wikipedia.org/wiki/ROCm</li>
<li>AMD ROCm™ Software - GitHub Home, https://github.com/ROCm/ROCm</li>
<li>AMD ROCm™ 소프트웨어, https://www.amd.com/ko/products/software/rocm.html</li>
<li>Software Stack for AMD GPU — ROCm 4.5.0 documentation - Read the Docs, https://cgmb-rocm-docs.readthedocs.io/en/latest/Installation_Guide/Software-Stack-for-AMD-GPU.html</li>
<li>Introduction to ROCm - Scientific Programming School, https://scientificprogramming.io/course/AMD-GPU-ROCm-and-HIP-Programming/lessons/2960/read</li>
<li>ROCm - 나무위키, https://namu.wiki/w/ROCm</li>
<li>What is AMD ROCm, why was it invented and what can one do with it? : r/AskProgramming - Reddit, https://www.reddit.com/r/AskProgramming/comments/1n4avki/what_is_amd_rocm_why_was_it_invented_and_what_can/</li>
<li>ROCm - 오늘의AI위키, AI가 만드는 백과사전, https://wiki.onul.works/w/ROCm</li>
<li>5 REASONS TO CHOOSE THE AMD ROCm™ PLATFORM, https://www.amd.com/content/dam/amd/en/documents/partner-hub/instinct/why-choose-rocm-platform.pdf</li>
<li>ROCm vs CUDA: A Practical Comparison for AI Developers - SCIMUS, https://thescimus.com/blog/rocm-vs-cuda-a-practical-comparison-for-ai-developers/</li>
<li>ROCm - 위키백과, 우리 모두의 백과사전, https://ko.wikipedia.org/wiki/ROCm</li>
<li>AMD ROCm™ Software, https://www.amd.com/en/products/software/rocm.html</li>
<li>AMD ROCm™ 7 소프트웨어, https://www.amd.com/ko/products/software/rocm/whats-new.html</li>
<li>AMD Launches ROCm 7.0, Up to 3.8x Performance Uplift Over ROCm 6.0 | TechPowerUp, https://www.techpowerup.com/341074/amd-launches-rocm-7-0-up-to-3-8x-performance-uplift-over-rocm-6-0</li>
<li>AMD ROCm 7.0 Officially Released With Many Significant … - Phoronix, https://www.phoronix.com/news/AMD-ROCm-7.0-Released</li>
<li>AMD ROCm 7 Announced: MI350 Support, New Algorithms, Models &amp; Advanced Features For AI Added, Focus on Inference With 3.5x Uplfit - Wccftech, https://wccftech.com/amd-rocm-7-mi350-support-new-algorithms-models-advanced-features-ai-3-5x-uplift/</li>
<li>What are the main advantages of using NVIDIA’s CUDA over AMD’s ROCm for high-performance computing?, <a href="https://massedcompute.com/faq-answers/?question=What+are+the+main+advantages+of+using+NVIDIA&#x27;s+CUDA+over+AMD&#x27;s+ROCm+for+high-performance+computing?">https://massedcompute.com/faq-answers/?question=What%20are%20the%20main%20advantages%20of%20using%20NVIDIA%27s%20CUDA%20over%20AMD%27s%20ROCm%20for%20high-performance%20computing?</a></li>
<li>CHIP-SPV/HIP: HIP: C++ Heterogeneous-Compute Interface for Portability - GitHub, https://github.com/CHIP-SPV/HIP</li>
<li>Building AMD ROCm from Source on a Supercomputer - Cray User Group, https://cug.org/proceedings/cug2023_proceedings/includes/files/pap104s2-file1.pdf</li>
<li>What’s the Difference Between CUDA and ROCm for GPGPU Apps? | Electronic Design, https://www.electronicdesign.com/technologies/embedded/article/21254328/electronic-design-whats-the-difference-between-cuda-and-rocm-for-gpgpu-apps</li>
<li>amd’s rocm software helps accelerate hpc &amp; deep learning - Moor Insights &amp; Strategy, https://www.moorinsightsstrategy.com/wp-content/uploads/2016/11/AMDs-ROCm-Software-Helps-Accelerate-HPC-and-Deep-Learning-by-Moor-Insights-and-Strategy.pdf</li>
<li>HIP Support — Clang 22.0.0git documentation, https://clang.llvm.org/docs/HIPSupport.html</li>
<li>Experiences with the Heterogeneous- compute Interface for Portability (HIP) on OLCF Summit, https://www.olcf.ornl.gov/wp-content/uploads/2019/10/Roth-HIP-on-Summit-20191009.pdf</li>
<li>ROCm™ Library Support &amp; Profiling Tools, https://www.olcf.ornl.gov/wp-content/uploads/2021/04/SPOCK_Libraries_profiling_JMaia.pdf</li>
<li>Exploring AMD’s Ambitious ROCm Initiative - Page: 1.4 - ADMIN Magazine, https://www.admin-magazine.com/HPC/Articles/Discovering-ROCm/(offset)/4</li>
<li>AMD ROCm Performance Primitives (RPP) library is a comprehensive high-performance computer vision library for AMD processors with HIP/OpenCL/CPU back-ends. - GitHub, https://github.com/ROCm/rpp</li>
<li>HPC용 AMD ROCm™ 소프트웨어, https://www.amd.com/ko/products/software/rocm/hpc.html</li>
<li>Deep dive into the MI300 compute and memory partition modes - AMD ROCm™ Blogs, https://rocm.blogs.amd.com/software-tools-optimization/compute-memory-modes/README.html</li>
<li>ROCm/hip: HIP: C++ Heterogeneous-Compute Interface for Portability - GitHub, https://github.com/ROCm/hip</li>
<li>What is HIP? - AMD ROCm documentation, https://rocm.docs.amd.com/projects/HIP/en/latest/what_is_hip.html</li>
<li>Introduction to HIP Programming - GitHub Pages, https://enccs.github.io/amd-rocm-development/_downloads/25e8b33cc5a6ff33a79b872a5281bded/intro_hip_programming.pdf</li>
<li>Application portability with HIP - AMD GPUOpen, https://gpuopen.com/learn/amd-lab-notes/amd-lab-notes-hipify-readme/</li>
<li>Components of the ROCm software stack | Download Scientific Diagram - ResearchGate, https://www.researchgate.net/figure/Components-of-the-ROCm-software-stack_fig2_359434105</li>
<li>ROCm Revisited: Getting Started with HIP, https://rocm.blogs.amd.com/ecosystems-and-partners/rocm-revisited-hip/README.html</li>
<li>[D] 왜 CUDA가 ROCm보다 훨씬 빠를까요? : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/1fa8vq5/d_why_is_cuda_so_much_faster_than_rocm/?tl=ko</li>
<li>AMD가 자금을 지원한 ROCm 기반 드롭인 CUDA 구현체, 이제 오픈소스로 공개 | GeekNews, https://news.hada.io/topic?id=13328</li>
<li>CUDA v ROCm : r/AMD_Stock - Reddit, https://www.reddit.com/r/AMD_Stock/comments/1hdqu6m/cuda_v_rocm/</li>
<li>Introduction to the HIP programming model - AMD ROCm documentation, https://rocm.docs.amd.com/projects/HIP/en/latest/understand/programming_model.html</li>
<li>ROCm vs CUDA: A Performance Showdown for Modern AI Workloads, https://tensorwave.com/blog/rocm-vs-cuda-a-performance-showdown-for-modern-ai-workloads</li>
<li>Enabling the Future of AI: Introducing AMD ROCm 7 and AMD Developer Cloud, https://www.amd.com/en/blogs/2025/enabling-the-future-of-ai-introducing-amd-rocm-7-and-the-amd-developer-cloud.html</li>
<li>Hardware Noob: is AMD ROCm as usable as NVIDA Cuda : r/learnmachinelearning - Reddit, https://www.reddit.com/r/learnmachinelearning/comments/1jf95af/hardware_noob_is_amd_rocm_as_usable_as_nvida_cuda/</li>
<li>Spelunking the HPC and AI GPU Software Stacks - HPCwire, https://www.hpcwire.com/2024/06/21/spelunking-the-hpc-and-ai-gpu-software-stacks/</li>
<li>A Comprehensive Guide: Switching from CUDA to ROCm - TensorWave, https://tensorwave.com/blog/transitioning-to-high-performance-a-comprehensive-guide-to-switching-from-cuda-to-rocm</li>
<li>Choosing the Right AI Platform for Your Company | by Scimus - Medium, https://medium.com/@thescimus.com/choosing-the-right-ai-platform-for-your-company-4b7923a4b6ff</li>
<li>System requirements (Linux) - AMD ROCm documentation, https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html</li>
<li>System requirements (Windows) — HIP SDK installation (Windows) - AMD ROCm documentation, https://rocm.docs.amd.com/projects/install-on-windows/en/latest/reference/system-requirements.html</li>
<li>Use ROCm on Radeon and Ryzen, https://rocm.docs.amd.com/projects/radeon-ryzen/en/latest/index.html</li>
<li>Compatibility matrix — ROCm Documentation, https://rocm.docs.amd.com/en/develop/compatibility/compatibility-matrix.html</li>
<li>Compatibility matrix - AMD ROCm documentation, https://rocm.docs.amd.com/en/docs-6.2.4/compatibility/compatibility-matrix.html</li>
<li>Compatibility matrix - AMD ROCm documentation, https://rocm.docs.amd.com/en/latest/compatibility/compatibility-matrix.html</li>
<li>Quick start installation guide — ROCm installation (Linux), https://rocm.docs.amd.com/projects/install-on-linux/en/develop/install/quick-start.html</li>
<li>How To guide : PyTorch/Tensorflow on AMD (ROCm) in Windows PC - Reddit, https://www.reddit.com/r/learnmachinelearning/comments/1je91m0/how_to_guide_pytorchtensorflow_on_amd_rocm_in/</li>
<li>PyTorch on ROCm — ROCm installation (Linux), https://rocm.docs.amd.com/projects/install-on-linux/en/develop/install/3rd-party/pytorch-install.html</li>
<li>TensorFlow compatibility — ROCm Documentation, https://rocm.docs.amd.com/en/latest/compatibility/ml-compatibility/tensorflow-compatibility.html</li>
<li>rocm-jax - GitHub, https://github.com/ROCm/rocm-jax</li>
<li>PyTorch compatibility - AMD ROCm documentation, https://rocm.docs.amd.com/en/latest/compatibility/ml-compatibility/pytorch-compatibility.html</li>
<li>tensorflow-rocm - PyPI, https://pypi.org/project/tensorflow-rocm/</li>
<li>Installation — JAX documentation, https://docs.jax.dev/en/latest/installation.html</li>
<li>jax - LUMI Software Library, https://lumi-supercomputer.github.io/LUMI-EasyBuild-docs/j/jax/</li>
<li>AMD ROCm 7.0 Software: Supercharging AI and HPC Infrastructure with AMD Instinct Series GPUs and Open Innovation, https://www.amd.com/en/blogs/2025/rocm7-supercharging-ai-and-hpc-infrastructure.html</li>
<li>GROMACS on AMD GPU-Based HPC Platforms: Using SYCL for Performance and Portability - arXiv, https://arxiv.org/html/2405.01420v1</li>
<li>Metrics and Design of an Instruction Roofline Model for AMD GPUs - arXiv, https://arxiv.org/pdf/2110.08221</li>
<li>Omniwise: Predicting GPU Kernels Performance with LLMs - arXiv, https://arxiv.org/pdf/2506.20886</li>
<li>[2403.06931] Optimizing sDTW for AMD GPUs - arXiv, https://arxiv.org/abs/2403.06931</li>
<li>Optimizing ML Concurrent Computation and Communication with GPU DMA Engines - arXiv, https://arxiv.org/pdf/2412.14335</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>