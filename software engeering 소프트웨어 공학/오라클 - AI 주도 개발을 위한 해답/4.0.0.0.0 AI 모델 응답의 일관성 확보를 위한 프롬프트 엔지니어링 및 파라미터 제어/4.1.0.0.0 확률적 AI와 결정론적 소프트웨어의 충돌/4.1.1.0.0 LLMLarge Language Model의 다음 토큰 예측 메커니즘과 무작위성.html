<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:4.1.1 LLM(Large Language Model)의 다음 토큰 예측 메커니즘과 무작위성</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>4.1.1 LLM(Large Language Model)의 다음 토큰 예측 메커니즘과 무작위성</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 4. AI 모델 응답의 일관성 확보를 위한 프롬프트 엔지니어링 및 파라미터 제어</a> / <a href="index.html">4.1 확률적 AI와 결정론적 소프트웨어의 충돌</a> / <span>4.1.1 LLM(Large Language Model)의 다음 토큰 예측 메커니즘과 무작위성</span></nav>
                </div>
            </header>
            <article>
                <h1>4.1.1 LLM(Large Language Model)의 다음 토큰 예측 메커니즘과 무작위성</h1>
<p>인공지능을 기반으로 한 소프트웨어 개발 패러다임에서 가장 핵심적인 충돌은 결정론적(Deterministic) 시스템과 확률적(Probabilistic) 모델의 만남에서 발생한다. 전통적인 소프트웨어 엔지니어링은 동일한 입력에 대해 항상 동일한 출력을 보장하는 멱등성과 엄격한 결정론을 기반으로 발전해 왔다. 코드의 실행 경로는 분기문과 반복문에 의해 명확히 추적 가능하며, 상태 기계(State Machine)는 예측 가능한 궤도를 따라 작동한다. 그러나 트랜스포머(Transformer) 아키텍처를 근간으로 하는 대형 언어 모델(LLM, Large Language Model)은 근본적으로 확률 기반의 생성 엔진이다. 이러한 대형 언어 모델의 확률적 본질은 소프트웨어 테스트 영역에서 확정적인 오라클(Oracle)을 구축하고 검증 시스템을 자동화하는 데 전례 없는 난제를 던진다.</p>
<p>이 절에서는 대형 언어 모델이 텍스트를 생성하는 기저 메커니즘인 다음 토큰 예측(Next Token Prediction)의 수학적 원리를 심층적으로 해부하고, 모델의 추론 과정에 개입하는 샘플링 알고리즘들이 어떻게 텍스트의 무작위성(Randomness)을 제어하는지 분석한다. 나아가, 알고리즘 수준의 확률적 제어를 넘어 하드웨어의 부동소수점 연산 구조에서 기인하는 불가피한 물리적 비결정성까지 고찰함으로써, 실전 AI 소프트웨어 개발에서 확정적 정답지를 다루기 위해 엔지니어가 반드시 이해해야 할 모델의 근원적 한계와 동작 방식을 명확히 규명한다.</p>
<h2>1. 자기회귀적(Autoregressive) 텍스트 생성의 수학적 기반과 아키텍처</h2>
<p>현대의 대형 언어 모델은 방대한 코퍼스를 학습하여 인간의 언어 패턴을 모사하지만, 추론(Inference) 시점에서의 동작 원리는 놀랍도록 단순한 반복적 규칙으로 귀결된다. 즉, 주어진 이전 문맥을 바탕으로 가장 등장할 법한 다음 단위의 텍스트 조각을 예측하는 과정을 반복하는 것이다. 이를 자기회귀적(Autoregressive) 텍스트 생성 모델이라고 명명한다.</p>
<p>이러한 생성 과정을 이해하기 위해서는 먼저 모델이 텍스트를 인식하는 단위인 토큰(Token)의 개념을 명확히 해야 한다. 사용자가 입력한 프롬프트 문자열은 토크나이저(Tokenizer)에 의해 모델의 어휘 사전(Vocabulary)에 정의된 이산적인 하위 단어(Subword) 단위로 분할된다. 분할된 각 토큰은 임베딩(Embedding) 레이어를 거쳐 수천 차원의 고차원 실수 벡터 공간의 한 좌표로 매핑되며, 이때 텍스트의 순서 정보를 보존하기 위한 위치 인코딩(Positional Encoding)이 결합된다.</p>
<p>토큰 벡터들의 시퀀스는 트랜스포머의 핵심인 잔차 스트림(Residual Stream)을 타고 다수의 트랜스포머 블록을 통과한다. 각 블록 내의 자기 주의(Self-Attention) 메커니즘은 전체 문맥 내에서 현재 토큰이 다른 어떤 토큰의 정보에 주의를 기울여야 하는지, 즉 쿼리(Query), 키(Key), 값(Value) 행렬을 통한 정보의 라우팅 비중을 계산한다. 이 과정에서 특정 패턴을 인식하고 이전 문맥을 복사해오는 귀납 헤드(Induction Heads)와 같은 특화된 어텐션 구조가 작동하여 모델의 문맥 내 학습(In-Context Learning) 능력을 발현시킨다. 어텐션 레이어를 통과한 정보는 다층 퍼셉트론(MLP) 레이어를 거치며 모델이 사전 학습 과정에서 파라미터 내부에 저장한 방대한 지식과 결합하여 은닉 상태(Hidden State)를 고도화한다.</p>
<p>수학적 관점에서, 이 모든 거대한 행렬 연산의 목표는 단 하나, 길이가 <span class="math math-inline">T</span>인 토큰 시퀀스 <span class="math math-inline">x_{1:T} = (x_1, x_2, \dots, x_T)</span>가 생성될 결합 확률(Joint Probability)을 계산하는 것이다. 이 결합 확률은 확률론의 연쇄 법칙(Chain Rule)에 의해 각 시점 <span class="math math-inline">t</span>에서의 조건부 확률(Conditional Probability)의 곱으로 완벽하게 분해된다.<br />
<span class="math math-display">
p(x_1, x_2, \dots, x_T) = \prod_{t=1}^T p(x_t \vert x_{&lt;t})
</span><br />
여기서 <span class="math math-inline">x_t</span>는 시점 <span class="math math-inline">t</span>에서 생성될 토큰을 의미하며, <span class="math math-inline">x_{&lt;t}</span>는 해당 시점 이전까지 주어지거나 생성된 모든 토큰의 시퀀스, 즉 프롬프트와 모델이 방금 전까지 스스로 생성한 출력의 결합을 나타낸다. 이 수식은 대형 언어 모델이 전체 문장의 구조나 결론을 미리 계획하고 한 번에 텍스트를 쏟아내는 것이 아니라, 매 단계마다 오직 지금까지의 상태만을 바탕으로 다음 단일 토큰에 대한 확률 분포를 추정하는 지극히 근시안적인(Myopic) 탐색을 반복함을 의미한다. 단순한 다음 토큰 예측이라는 목적 함수가 모델의 규모 확장(Scaling)과 결합될 때 보편적인 추론 능력으로 창발한다는 사실은 기계학습 분야의 놀라운 발견 중 하나이다.</p>
<p>하지만 이러한 조건부 확률의 연쇄적 결합 구조는 소프트웨어 테스트의 관점에서 치명적인 취약점을 내포하고 있다. 하나의 시퀀스가 길게 생성되는 과정에서 초기 시점인 <span class="math math-inline">t=1</span>에 내부 연산의 미세한 오차나 샘플링의 무작위성으로 인해 원래 예측과 다른 토큰이 우연히 출력되었다고 가정해보자. 이는 단순히 단어 하나가 틀린 것으로 끝나지 않는다. <span class="math math-inline">t=2</span> 시점에서 모델이 받아들이는 조건부 입력 <span class="math math-inline">x_{&lt;2}</span> 자체가 변경되었음을 의미하므로, 후속 토큰들을 생성하기 위해 모델이 계산하는 확률 공간의 궤적은 완전히 발산하게 된다. 입력 데이터의 아주 작은 변화나 생성 과정의 초기 확률적 요동이 최종 출력물의 구조와 논리적 결론을 완전히 뒤바꿀 수 있는 전형적인 나비 효과(Butterfly Effect)가 내재되어 있는 것이다.</p>
<h2>2. 로짓(Logits) 원시 점수와 소프트맥스(Softmax) 병목의 기하학</h2>
<p>트랜스포머의 수많은 층을 통과하여 문맥적 의미가 극한으로 농축된 마지막 레이어의 잔차 스트림 벡터는 언임베딩(Unembedding) 레이어를 거치며 최종적인 변환을 겪는다. 언임베딩 레이어는 이 벡터를 모델이 알고 있는 전체 어휘 사전(Vocabulary)의 크기인 <span class="math math-inline">V</span> 차원 공간으로 투영한다. 투영의 결과물로 산출되는 <span class="math math-inline">V</span>개의 실수 벡터 요소를 로짓(Logits)이라고 부른다.</p>
<p>특정 시점에서의 로짓 벡터 <span class="math math-inline">z = (z_1, z_2, \dots, z_V)</span>의 각 요소 <span class="math math-inline">z_i</span>는 모델이 어휘 사전 내의 <span class="math math-inline">i</span>번째 토큰이 다음 위치에 등장할 가능성을 평가한 원시 점수(Raw Score)이다. 이 값은 <span class="math math-inline">-\infty</span>부터 <span class="math math-inline">+\infty</span>까지 정규화되지 않은 임의의 실수값을 가지며, 모델이 해당 토큰의 출현을 강력하게 지지할수록 큰 양수 값을, 배제할수록 큰 음수 값을 띠게 된다.</p>
<p>결정론적 논리를 다루는 소프트웨어 시스템이 이러한 원시 점수를 해석하고 최종적으로 단 하나의 토큰을 확정하기 위해서는, 무한대의 범위를 갖는 로짓을 총합이 <span class="math math-inline">1</span>이고 모든 요소가 <span class="math math-inline">0</span> 이상의 값을 갖는 유효한 확률 분포(Probability Distribution)로 변환하는 수학적 교량 역할이 필요하다. 이 역할을 수행하는 것이 바로 소프트맥스(Softmax) 함수이다. 소프트맥스 함수는 토큰 <span class="math math-inline">i</span>가 선택될 확률 <span class="math math-inline">q_i</span>를 다음과 같이 계산한다.<br />
<span class="math math-display">
q_i = \frac{e^{z_i}}{\sum_{j=1}^V e^{z_j}}
</span><br />
소프트맥스 함수는 자연상수 <span class="math math-inline">e</span>를 밑으로 하는 지수 함수를 분자와 분모에 모두 적용한다. 지수 함수의 특성상 로짓 벡터 내에 존재하는 수치들의 미세한 선형적 차이는 지수적으로 폭발하여 증폭된다. 이는 모델이 특정 토큰에 대해 다른 토큰들보다 아주 약간의 추가적인 확신만 가져도, 소프트맥스 변환을 거친 후의 확률 공간에서는 그 토큰이 매우 압도적인 확률을 점유하는 승자독식(Winner-takes-all) 현상을 유도한다.</p>
<p>그러나 소프트맥스 함수는 확률 분포를 생성하는 우수한 수학적 도구임과 동시에 모델의 표현력을 제약하는 심각한 병목(Bottleneck)으로 작용하기도 한다. 어휘 사전의 크기가 수만에서 십만 개를 넘어가는 현대의 대형 언어 모델에서, 극소수의 정답 후보 토큰을 제외한 절대 다수의 수만 개의 토큰들은 소프트맥스를 거치면서 <span class="math math-inline">0</span>에 한없이 수렴하는 극소 확률로 압착된다. 이들을 확률 분포의 신뢰할 수 없는 꼬리(Unreliable Tail)라고 부른다.</p>
<p>연구자들은 대형 언어 모델의 각 레이어에서 생성되는 로짓 분포의 기하학적 특성을 파악하기 위해 소프트맥스 엔트로피(Softmax Entropy)의 누적량(Cumulants)을 분석하는 정보이론적 접근법을 사용한다. 이러한 섭동적 전개(Perturbative Expansion)를 통한 분석에 따르면, 아무리 모델의 확신도가 높고 프롬프트가 명확하여 로짓의 중심 분포가 뾰족하더라도, 꼬리에 위치한 수만 개의 토큰들이 가지는 미세한 확률 질량(Probability Mass)을 모두 합산하면 결코 무시할 수 없는 수준의 통계적 비중을 차지하게 된다.</p>
<p>소프트웨어 시스템이 API를 호출하여 대형 언어 모델의 응답을 받을 때 간헐적으로 맥락에 전혀 맞지 않는 환각(Hallucination) 발언이나 논리적 비약이 발생하는 근본적인 이유가 바로 여기에 있다. 모델은 다음 토큰을 무작위로 추출(Sampling)하는 과정에서 이 광범위한 꼬리 영역의 확률 질량에 걸려들어, 스스로의 로짓 평가 체계에서는 매우 점수가 낮았던 엉뚱한 토큰을 출력 경로에 편입시키고 마는 것이다.</p>
<h2>3. 무작위성의 첫 번째 제어: 온도(Temperature) 매개변수의 스케일링</h2>
<p>모델의 가중치 네트워크 연산이 완료되어 튀어나온 로짓 벡터는 결정론적인 고정값이다. 프롬프트가 동일하다면 모델 아키텍처 내부의 순전파(Forward Pass) 결과물인 로짓 자체는 변하지 않는다. 그러나 사용자가 실제로 마주하는 최종 응답 텍스트의 가변성과 무작위성을 획기적으로 조작할 수 있는 강력한 외부 제어 변수가 존재한다. 그것이 바로 온도(Temperature, <span class="math math-inline">T</span>) 하이퍼파라미터이다.</p>
<p>온도는 대형 언어 모델의 가중치 파라미터가 아니다. 이는 모델의 추론(Inference) 단계에서 소프트맥스 함수에 개입하여 확률 분포의 평탄도와 날카로움이라는 형태(Shape)를 근본적으로 뒤틀어버리는 디코딩 알고리즘의 매개변수이다. 열역학(Thermodynamics)에서 온도가 입자들의 무질서도인 엔트로피(Entropy)를 결정짓듯, 이 변수는 언어 모델의 토큰 분포의 엔트로피를 직접적으로 제어한다.</p>
<p>온도 <span class="math math-inline">T</span>가 적용된 소프트맥스 함수의 확장 수식은 다음과 같다.<br />
<span class="math math-display">
q_i = \frac{\exp(z_i / T)}{\sum_{j=1}^V \exp(z_j / T)}
</span><br />
이 수식을 통해 온도 <span class="math math-inline">T</span>의 변화가 원시 로짓 값들 간의 차이를 어떻게 조작하는지 분석하면, 소프트웨어 개발자가 AI 응답의 결정론적 일관성을 어떤 원리로 통제할 수 있는지 수학적 직관을 얻을 수 있다. 소프트맥스 변환 이전의 유효 로짓 값을 <span class="math math-inline">k_i = z_i / T</span>라고 정의할 때, 두 토큰 간의 유효 로짓 차이 <span class="math math-inline">\delta_k</span>는 원래 로짓의 차이 <span class="math math-inline">\delta_z</span>를 온도 <span class="math math-inline">T</span>로 나눈 값(<span class="math math-inline">\delta_k = \frac{\delta_z}{T}</span>)이 된다.</p>
<ol>
<li><strong>극한의 결정론 (<span class="math math-inline">T \to 0</span>):</strong> 온도가 0에 수렴하도록 극도로 작아지면, 원래 로짓 간의 미세한 차이 <span class="math math-inline">\delta_z</span>가 <span class="math math-inline">T</span>로 나뉘면서 무한대로 발산하게 된다. 그 결과, 어휘 사전 내에서 로짓 값이 가장 큰 단 하나의 토큰에 부여되는 소프트맥스 확률은 <span class="math math-inline">1.0</span>에 한없이 수렴하고, 2위 이하의 나머지 모든 토큰들의 확률은 <span class="math math-inline">0.0</span>으로 소멸한다. 이는 모델의 동작을 수학적으로 순수한 결정론적 탐욕 탐색(Greedy Decoding)으로 강제 변환시킨다. 오라클을 구축할 때 테스트의 재현성을 확보하기 위해 가장 보편적으로 권장되는 설정이다.</li>
<li><strong>원형 분포 (<span class="math math-inline">T = 1.0</span>):</strong> <span class="math math-inline">T</span>가 <span class="math math-inline">1</span>일 경우 로짓 값은 아무런 스케일링을 받지 않으며, 모델이 학습한 원래의 확률 분포가 그대로 보존되어 출력된다.</li>
<li><strong>분포의 예리화 (<span class="math math-inline">0 &lt; T &lt; 1.0</span>, Sharpening):</strong> <span class="math math-inline">T</span>가 <span class="math math-inline">1</span>보다 작아지면 로짓 간의 격차가 상대적으로 확대된다. 원래 확률이 높았던 주류 토큰들은 확률 질량을 더욱 흡수하여 그 위상이 강화되고, 확률이 낮았던 비주류 토큰들의 확률은 바닥으로 납작하게 눌리게 된다. 확률 분포 차트의 봉우리가 매우 뾰족한 첨점(Sharp Peak)의 형태를 띠게 되며, 모델은 보수적이고 예측 가능하며 정형화된 안전한 답변을 내놓게 된다.</li>
<li><strong>분포의 평탄화 (<span class="math math-inline">T &gt; 1.0</span>, Flattening):</strong> 반대로 <span class="math math-inline">T</span>가 <span class="math math-inline">1</span>을 초과하면 로짓 값들의 수치적 스케일이 전반적으로 축소되면서 상호 간의 격차가 희석된다. 이는 확률 분포를 평탄하게(Flat) 만들어 가장 유력했던 토큰의 지배력을 크게 끌어내리고, 신뢰할 수 없는 꼬리에 위치한 비주류 토큰들의 확률을 인위적으로 상승시킨다. 텍스트의 수사적 다양성과 창의성은 비약적으로 증가하지만, 모델이 사실과 다른 내용을 지어내는 환각 발생 빈도와 문맥 이탈의 위험성 역시 이와 비례하여 급증하게 된다.</li>
</ol>
<p><img src="./4.1.1.0.0%20LLMLarge%20Language%20Model%EC%9D%98%20%EB%8B%A4%EC%9D%8C%20%ED%86%A0%ED%81%B0%20%EC%98%88%EC%B8%A1%20%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC%20%EB%AC%B4%EC%9E%91%EC%9C%84%EC%84%B1.assets/image-20260222220953325.jpg" alt="image-20260222220953325" /></p>
<p>이러한 기본적인 온도 스케일링 외에도, 모델 API는 로짓 단계에서 특수한 형태의 벌점(Penalty) 매개변수를 추가적으로 적용하여 텍스트의 흐름을 강제하기도 한다. 대표적으로 반복 벌점(Repetition Penalty)과 빈도 벌점(Frequency Penalty)이 있다. 이는 모델이 이전 시퀀스에서 이미 생성한 토큰을 또다시 선택하는 것을 방지하기 위해 고안된 제어 장치이다. 이전에 생성된 횟수 정보를 바탕으로 소프트맥스 변환 이전의 해당 토큰 로짓 값 자체를 수식적으로 차감하거나 나누어 확률 순위를 강제로 끌어내린다.</p>
<p>소프트웨어 엔지니어가 자동화된 테스트 파이프라인과 결정론적 오라클을 구축할 때 가장 먼저 범하는 실수가 바로 온도를 0으로 설정하면 모든 문제가 해결될 것이라는 맹신이다. <span class="math math-inline">T=0</span>이 확률 분포를 결정론적으로 만든다는 것은 수식적으로 참이지만, 이는 모델이 복잡한 논리 구조를 완성하기 위해 여러 가능성을 탐색하는 능력을 원천적으로 박탈하는 양날의 검이다. 탐욕 알고리즘의 본질적 한계로 인해, 모델이 초기 시점에서 근시안적으로 최선의 토큰을 선택했다가 이후 문맥 전개에서 논리적 함정에 빠져버렸을 때, 더 나은 우회 경로를 탐색하지 못하고 끝없는 반복(Loop)에 빠지거나 어색한 결론을 도출하는 텍스트 퇴화 현상이 발생하기 때문이다. 따라서 온도의 조절만으로는 텍스트의 고품질과 검증 가능성을 동시에 확보할 수 없으며, 이를 보완하기 위한 고도화된 디코딩 샘플링 알고리즘들이 발전하게 되었다.</p>
<h2>4. 디코딩 샘플링 알고리즘의 진화: 무작위성과 일관성의 균형</h2>
<p>소프트맥스를 통해 최종 확률 분포가 계산된 후, 모델이 텍스트를 출력하기 위해 전체 어휘 사전 중에서 단 하나의 토큰을 골라내는 과정을 디코딩(Decoding) 또는 샘플링(Sampling)이라고 부른다. 소프트웨어의 테스트 오라클을 설계하는 엔지니어는 자신이 호출하는 언어 모델 API가 내부적으로 어떠한 샘플링 알고리즘의 파이프라인을 거치는지 정확히 파악해야 한다. 샘플링 알고리즘의 수학적 작동 원리에 따라 모델의 응답이 결정론적 검증의 경계 내에 머물 수도 있고, 완전히 예측 불가능한 통계적 발산의 영역으로 넘어갈 수도 있기 때문이다.</p>
<h3>4.1 탐욕적 탐색(Greedy Decoding)과 빔 서치(Beam Search)의 한계</h3>
<p>가장 단순하고 직관적인 디코딩 전략은 확률 분포에서 값이 가장 큰 1순위 토큰만을 무조건 선택하는 탐욕적 탐색(Greedy Decoding)이다. 이는 온도를 0으로 설정한 것과 동일한 효과를 내며, 이론적으로 오직 단 하나의 결정론적 경로만을 산출한다. 입력과 출력이 엄격하게 일치해야 하는 소프트웨어 유닛 테스트나 오라클 구축 관점에서는 가장 이상적인 방식처럼 보인다.</p>
<p>이러한 단일 토큰 단위의 맹목적인 탐색을 보완하기 위해 고안된 기법이 빔 서치(Beam Search)이다. 빔 서치는 매 시점마다 최상위 확률을 가진 단일 토큰만 추적하는 것이 아니라, 여러 개(Beam Size)의 유력한 문장 시퀀스 후보군을 동시에 추적하여 최종적으로 전체 시퀀스의 결합 확률이 가장 높은 문장을 채택한다. 입력 문장의 의미를 훼손하지 않고 정확한 대응쌍을 찾아야 하는 기계 번역(Machine Translation)과 같은 닫힌 도메인에서는 빔 서치가 매우 뛰어난 성능을 발휘한다.</p>
<p>그러나 개방형 텍스트 생성(Open-ended Text Generation) 영역에서 빔 서치나 탐욕적 탐색과 같이 확률 극대화(Probability Maximization)만을 추구하는 디코딩 전략은 치명적인 부작용을 동반한다. ICLR 2020 학회에서 발표되어 학계에 큰 파장을 일으킨 논문 “The Curious Case of Neural Text Degeneration”(Holtzman et al.)은 확률 최대화 기반의 디코딩 방식이 대형 언어 모델의 텍스트를 심각하게 퇴화시킨다는 사실을 수학적 지표와 통계적 분석을 통해 증명하였다.</p>
<p>이 연구에 따르면, 실제 인간이 발화하는 텍스트의 확률 분포를 분석해보면 인간은 매 순간 가장 등장 확률이 높은 뻔한 단어만을 연속해서 선택하지 않는다. 때로는 정보량이 높고 출현 확률이 다소 낮더라도 창의적인 단어를 의도적으로 선택하여 문맥에 변주를 주고 의미를 풍부하게 만든다. 반면, 탐욕적 탐색과 빔 서치는 예측 가능성을 극도로 추구한 나머지 자연스러운 문맥의 변동성을 상실하고, 결과적으로 지루하고, 진부하며(Bland), 동일한 구문이 무한정 반복되는 논리적 퇴화(Degeneration) 루프에 빠지게 된다.</p>
<p>따라서 모델의 창의성과 맥락 유지 능력을 살리면서도 논리적 타당성을 잃지 않기 위해서는 확률 분포에 기반하여 토큰을 무작위로 추출하는 스토캐스틱(Stochastic) 샘플링 기법의 도입이 필수적이다. 문제는 단순히 무작위 추출을 수행할 경우, 앞서 언급한 소프트맥스의 신뢰할 수 없는 꼬리(Unreliable Tail)에 분포하는 노이즈 토큰들이 튀어나와 텍스트를 지리멸렬한 헛소리(Incoherent Gibberish)로 만들어버린다는 점이다. 이를 방지하기 위해 확률 공간에서 꼬리를 안전하게 잘라내는 절단(Truncation) 알고리즘들이 연이어 고안되었다.</p>
<h3>4.2 Top-K 샘플링: 고정된 하드 컷오프(Hard Cutoff)의 경직성</h3>
<p>꼬리를 절단하는 가장 고전적이고 직관적인 방식은 Top-K 샘플링이다. 이 알고리즘은 모델이 산출한 확률 분포에서 값이 가장 높은 순서대로 토큰을 정렬한 뒤, 사용자가 설정한 상위 <span class="math math-inline">K</span>개의 토큰만을 후보군으로 보존하고 나머지 하위 토큰들의 확률을 모두 <span class="math math-inline">0</span>으로 강제 할당(Masking)하여 삭제한다. 살아남은 <span class="math math-inline">K</span>개의 토큰들은 총 확률의 합이 1이 되도록 다시 재정규화(Renormalization) 연산을 거친 후, 그 새로운 확률에 비례하여 최종 토큰이 무작위로 샘플링된다.</p>
<p>예를 들어 <span class="math math-inline">K=50</span>으로 설정하면, 모델의 전체 어휘 사전이 10만 개라 하더라도 매 생성 시점마다 오직 1위부터 50위까지의 토큰만이 선택 대상 테이블에 오르게 된다. 이는 확률 분포의 기저에 깔려 있는 비문법적이거나 문맥에 전혀 맞지 않는 기괴한 토큰들이 무작위로 샘플링될 가능성을 원천적으로 차단하여 텍스트의 유창성(Fluency)을 보장하는 훌륭한 안전장치 역할을 한다. 소프트웨어 개발자는 <span class="math math-inline">K</span> 값을 조절함으로써 출력의 다양성과 정확성 사이의 균형점을 설정할 수 있다.</p>
<p>그러나 Top-K 샘플링은 모델이 처한 문맥적 환경의 변화를 전혀 반영하지 못한다는 구조적인 결함(Fundamental Flaw)을 지니고 있다. 대형 언어 모델의 다음 토큰 예측 불확실성은 시시각각 급격하게 요동친다. 첫째, 문맥의 확신도(Certainty)가 극도로 높은 상황을 가정해보자. “대한민국의 수도는 [서울]“과 같이 모델이 다음 단어를 강력하게 확신하여 1위 토큰의 확률이 99%에 육박하는 경우, 2위부터 <span class="math math-inline">K</span>위까지의 토큰들은 사실상 아무런 의미가 없는 노이즈임에도 불구하고 고정된 수치 <span class="math math-inline">K</span>를 채우기 위해 억지로 후보군에 포함되는 모순이 발생한다. 둘째, 불확실성(Uncertainty)이 매우 높은 상황이다. “내가 주말에 가장 즐겨하는 취미는 […]“과 같이 수십, 수백 가지의 문맥상 타당한 답변이 존재하는 평탄한 분포(Flat Distribution)에서는 상위 <span class="math math-inline">K</span>개라는 고정된 장벽이 유효하고 창의적인 선택지들을 너무 많이 잘라내어 표현의 다양성을 심각하게 훼손하게 된다.</p>
<p>소프트웨어 시스템의 입력값은 예측 불가능하며 문맥의 복잡도는 계속 변화한다. 따라서 이러한 고정 길이 기반의 정적인 절단 방식은 응답의 일관성과 창의성을 동시에 달성해야 하는 최신 AI 애플리케이션의 테스트 오라클 전략으로 사용하기에는 지나치게 경직되어 있다.</p>
<h3>4.3 Top-P (Nucleus) 샘플링: 동적 누적 확률 기반 절단</h3>
<p>Top-K의 정적인 한계를 근본적으로 해결하기 위해 앞서 언급한 “The Curious Case of Neural Text Degeneration” 논문에서 새롭게 제안된 혁신적인 방법론이 바로 Top-P 샘플링, 일명 뉴클리어스 샘플링(Nucleus Sampling)이다. 이 기법은 고정된 후보의 ’개수’가 아니라, 토큰들이 지닌 ’누적 확률 질량(Cumulative Probability Mass)’을 기준으로 후보군의 크기를 유동적으로 결정한다.</p>
<p>작동 원리는 다음과 같다. 모델이 예측한 토큰들을 확률이 높은 순으로 내림차순 정렬한 뒤, 1위 토큰부터 차례대로 확률을 더해 나간다. 이 누적된 합계가 사용자가 사전에 정의한 임계값 <span class="math math-inline">P</span> (일반적으로 <span class="math math-inline">0.9</span>에서 <span class="math math-inline">0.95</span> 사이)에 도달하거나 이를 초과하는 순간 연산을 멈추고, 그때까지 합산에 참여한 토큰들만을 최종 후보군으로 확정한다. 연구자들은 이 좁고 신뢰할 수 있는 상위 확률 집합을 ’핵(Nucleus)’이라고 명명했으며, 꼬리에 해당하는 나머지 하위 토큰들은 모두 버려진다.</p>
<p>수학적으로 뉴클리어스 집합 <span class="math math-inline">V&#39;</span>는 다음의 부등식을 만족하는 가장 작은 토큰의 부분집합으로 정의된다.<br />
<span class="math math-display">
V&#39; \subset V \text{ such that } \sum_{x \in V&#39;} p(x) \ge P
</span><br />
Top-P 샘플링이 소프트웨어 엔지니어링 생태계에서 표준 디코딩 전략으로 빠르게 자리 잡은 이유는 모델의 ’확신도’에 호응하여 후보군의 크기가 마치 생물처럼 숨쉬듯 팽창하고 수축하는 적응성(Adaptability) 때문이다. 모델의 예측 확신이 매우 강하여 1, 2위 토큰의 확률만 더해도 <span class="math math-inline">0.95</span>에 도달한다면 후보군은 단 2개로 좁혀져 환각을 억제하고 안전성을 보장한다. 반대로 확률 분포가 수많은 단어로 잘게 쪼개져 평탄하게 분산되어 있다면, 누적 확률 <span class="math math-inline">0.95</span>를 달성하기 위해 수십, 수백 개의 다양한 토큰들이 후보군에 자연스럽게 합류하여 문맥의 창의성을 확보한다.</p>
<p>하지만 완벽해 보이는 Top-P 샘플링 역시 높은 온도(High-temperature) 매개변수와 결합되거나 모델이 극심한 추론의 혼란을 겪을 때 ’평탄한 분포의 결함(Flat-distribution Flaw)’이라는 치명적인 약점을 노출한다. 앞서 설명했듯이 온도가 상승하면 로짓의 차이가 줄어들어 확률 분포가 극단적으로 평탄해진다. 1위 토큰부터 1000위 토큰까지 확률의 차이가 거의 없는 평탄한 혼돈의 상태에서 누적 확률 <span class="math math-inline">0.95</span>라는 높은 임계값을 채우려면, 모델은 필연적으로 질이 낮고 문맥에 어긋나는 수천 개의 쓰레기(Garbage) 노이즈 토큰들까지 무더기로 뉴클리어스 안에 끌어모을 수밖에 없다. 확신이 없을 때 너무 많은 관용을 베푼 나머지, 논리의 일관성이 붕괴되고 응답 텍스트가 방향성을 상실하는 것이다.</p>
<h3>4.4 Min-P 샘플링: 최고 확률 비례 동적 절단 알고리즘</h3>
<p>Top-P 샘플링이 평탄한 확률 분포의 늪에 빠져 고온 환경에서 논리적 파탄을 일으키는 현상을 극복하기 위해, 가장 최신의 연구 동향인 ICLR 2025 학술대회의 구두 발표(Oral Presentation) 논문 “Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs”(Nguyen et al.)에서 완전히 새로운 수학적 접근법인 Min-P 샘플링이 제안되었다.</p>
<p>Min-P 기법의 핵심 철학은 절대적인 개수(Top-K)나 고정된 누적 합계(Top-P)를 쫓는 것이 아니라, 매 시점 예측되는 ’가장 확률이 높은 1위 토큰의 절대적 확률값(<span class="math math-inline">p_{max}</span>)’을 기준으로 상대적이고 동적인 절단 임계값을 생성한다는 점이다. 사용자가 <span class="math math-inline">\text{min\_p}</span>라는 단일 매개변수(통상적으로 <span class="math math-inline">0.05</span>에서 <span class="math math-inline">0.1</span> 사이 권장)를 설정했을 때, 특정 토큰 <span class="math math-inline">i</span>가 최종 샘플링 후보군에 포함되기 위한 수학적 최소 필요조건은 다음과 같이 정의된다.<br />
<span class="math math-display">
p_i \ge \text{min\_p} \times p_{max}
</span><br />
이 직관적인 방정식은 모델의 상태에 따라 극적인 변화를 만들어낸다. 첫째, 모델의 확신이 강한 경우이다. 1위 토큰의 확률(<span class="math math-inline">p_{max}</span>)이 <span class="math math-inline">80%(0.8)</span>에 달할 정도로 지배적이고 <span class="math math-inline">\text{min\_p}</span>가 <span class="math math-inline">0.1</span>이라면, 절단 임계값은 <span class="math math-inline">0.08(8%)</span>로 높게 설정된다. 따라서 <span class="math math-inline">8\%</span> 미만의 확률을 가진 모든 자잘한 토큰들은 가차 없이 버려지며 텍스트의 논리적 일관성(Coherence)이 강력하게 유지된다. 둘째, 모델이 확신을 가지지 못하고 분포가 평탄화된 고온의 상황이다. 가장 유력한 1위 토큰조차 그 확률이 <span class="math math-inline">10%(0.1)</span>에 불과하다면, 임계값은 그에 비례하여 <span class="math math-inline">0.01(1%)</span>이라는 매우 낮은 수치로 대폭 완화된다. 기준선이 크게 낮아졌으므로 <span class="math math-inline">1\%</span> 이상의 확률만 얻은 다수의 토큰들이 후보군에 폭넓게 포진하게 되어 자연스럽게 풍부한 창의성(Creativity)을 발휘할 수 있는 환경이 조성된다.</p>
<p>이 방식은 마치 인간이 의사결정을 내릴 때 “내가 현재 1순위 대안에 대해 확신이 아주 강하다면 나머지 허접한 대안들은 쳐다보지도 않겠지만, 1순위 대안조차 확신이 서지 않고 아리송하다면 최대한 다양한 아이디어들의 문턱을 낮춰 열린 마음으로 수용하겠다“라고 판단하는 인지적 메커니즘과 정확히 일치한다. 논문의 광범위한 벤치마크 테스트 결과에 따르면, Min-P 샘플링은 기존의 Top-P가 환각으로 무너져 내리던 높은 온도 범위(<span class="math math-inline">T&gt;1.0</span>)에서도 창의성과 정합성의 아슬아슬한 균형을 완벽하게 통제하며 압도적인 성능 우위를 입증했다. 그 실용성과 알고리즘적 우수성이 즉각적으로 인정받아 Hugging Face Transformers, vLLM을 비롯한 주요 오픈소스 LLM 인프라 프레임워크에 표준 샘플링 옵션으로 신속하게 채택되었다.</p>
<p>소프트웨어 시스템 내에 하이브리드 오라클을 구축하거나 재현성을 담보해야 하는 엔지니어의 관점에서, 완벽한 텍스트 결정론이 아니라 비즈니스 로직상 ’허용 가능한 범위 내의 유효한 가변성’을 철저하게 통제해야 할 때, Min-P는 기존의 낡은 절단 기법들을 대체할 가장 신뢰할 수 있고 수학적으로 유연한 제어 기준선을 제공한다.</p>
<p><img src="./4.1.1.0.0%20LLMLarge%20Language%20Model%EC%9D%98%20%EB%8B%A4%EC%9D%8C%20%ED%86%A0%ED%81%B0%20%EC%98%88%EC%B8%A1%20%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC%20%EB%AC%B4%EC%9E%91%EC%9C%84%EC%84%B1.assets/image-20260222221052610.jpg" alt="image-20260222221052610" /></p>
<p>위에서 논의한 언어 모델의 대표적인 샘플링 파라미터들이 모델의 무작위성 제어에 미치는 수학적 특성과 결정론에 미치는 영향을 요약하면 다음 표와 같다.</p>
<table><thead><tr><th><strong>샘플링 기법</strong></th><th><strong>수학적 제어 임계값 기준</strong></th><th><strong>핵심 메커니즘 및 철학</strong></th><th><strong>결정론적 오라클 관점에서의 취약점 및 한계</strong></th></tr></thead><tbody>
<tr><td><strong>Temperature (<span class="math math-inline">T</span>)</strong></td><td><span class="math math-inline">q_i = \frac{\exp(z_i/T)}{\sum \exp(z_j/T)}</span></td><td>확률 분포의 엔트로피와 전반적인 평탄도(무작위성)를 수치적으로 조절</td><td>고온 설정 시 무작위성이 기하급수적으로 폭발하여 텍스트의 논리성이 붕괴되며 완전 발산함.</td></tr>
<tr><td><strong>Top-K</strong></td><td><span class="math math-inline">\text{Rank}(q_i) \le K</span></td><td>예측 불확실성과 무관하게 고정된 상위 K개의 토큰만을 일괄 허용</td><td>분포가 평탄하여 다양한 유효 토큰이 존재할 때도 이를 기계적으로 잘라내어 문맥의 유연성을 심각하게 훼손함.</td></tr>
<tr><td><strong>Top-P (Nucleus)</strong></td><td><span class="math math-inline">\sum_{x \in V&#39;} q(x) \ge P</span></td><td>누적 확률 질량이 목표치 P에 도달할 때까지 후보군을 동적으로 팽창시킴</td><td>분포가 평탄화될 경우 P를 채우기 위해 수천 개의 무의미한 노이즈 토큰까지 핵(Nucleus)에 대거 포함시켜 환각을 유발함.</td></tr>
<tr><td><strong>Min-P</strong></td><td><span class="math math-inline">q_i \ge \text{min\_p} \times q_{max}</span></td><td>1위 토큰의 절대 확률에 정비례하는 동적인 수평 최소 임계값(Threshold) 적용</td><td>극단적인 하이퍼파라미터 조율 오류 시 Top-P와 유사하게 노이즈가 유입될 수 있으나 현재까지 가장 안정적인 균형을 보임.</td></tr>
</tbody></table>
<h2>5. 하드웨어 수준의 근원적 비결정성: 온도(Temperature) 0의 환상</h2>
<p>대형 언어 모델이 토큰을 확률적으로 뱉어내는 로짓의 기하학과 샘플링 디코딩 이론을 완벽하게 숙지한 소프트웨어 엔지니어라면 자연스럽게 다음과 같은 기술적 논리적 결론에 도달하게 될 것이다.</p>
<p><em>“만약 API 파라미터에서 온도를 0으로 설정(<span class="math math-inline">T=0</span>)하여 무작위성을 완전히 제거하고 수학적으로 순수한 탐욕적 탐색(Greedy Decoding)을 강제한다면, 내부 로짓 계산이 끝나는 즉시 오직 단 하나의 토큰만이 확률 1.0을 차지하게 될 것이다. 따라서 우리는 어떠한 확률적 요동도 없는 완벽한 결정론적(Deterministic) 소프트웨어 테스트 오라클을 구축할 수 있을 것이다.”</em></p>
<p>그러나 참담하게도, 현대의 거대한 상용 LLM 추론 인프라를 상대하는 실무 환경에서 이러한 전제는 이론적인 환상(Illusion)에 불과하다. 실제로 클라우드 API를 통해 동일한 프롬프트를, 시드(Seed) 값을 고정하고, 온도를 0으로 둔 채 수십 번 반복 호출해보면 어느 순간 응답 텍스트에 미세한 단어 변형이 발생하거나 심지어 완전히 다른 논리 전개가 튀어나와 테스트가 실패로 끝나는 현상을 뼈저리게 목격할 수 있다.</p>
<p>이해하기 힘든 이러한 무작위성과 비결정성은 샘플링 알고리즘이나 모델의 학습 데이터에서 기인하는 것이 아니다. 이는 모델을 실제로 구동하는 실물 ’하드웨어’의 병렬 연산 구조와 대용량 트래픽을 처리하기 위한 ’분산 시스템 아키텍처’의 본질적인 제약 조건에서 기인한다.</p>
<h3>5.1 부동소수점 비결합 법칙(Floating-Point Non-Associativity)과 오차의 누적</h3>
<p>대형 언어 모델을 초고속으로 구동하는 GPU나 TPU는 거대한 텐서(Tensor) 행렬 연산을 수천 개의 코어에 쪼개어 병렬로 처리함으로써 극한의 연산 처리량(Throughput)을 달성한다. 이때 가중치 곱셈, 어텐션 스코어, 활성화 함수 등 수십억 번에 달하는 신경망 연산은 모두 IEEE 754 표준을 따르는 부동소수점(Floating-Point) 데이터 형식으로 처리된다.</p>
<p>수학적 공리계에서 순수한 실수의 덧셈 연산은 결합 법칙(Associative Property)이 완벽하게 성립하므로 덧셈의 괄호 순서에 상관없이 <span class="math math-inline">(a+b)+c = a+(b+c)</span>를 항상 만족한다. 하지만 컴퓨터 공학의 영역으로 내려오면 이야기가 달라진다. 제한된 메모리 비트 안에 소수점 이하의 유효숫자(Precision)를 구겨 넣어야 하는 부동소수점 체계에서는, 연산 중간 단계마다 범위를 초과하는 미세한 숫자들을 필연적으로 잘라내거나 반올림하는 오차(Rounding Error)가 발생한다. 결과적으로 부동소수점의 세계에서는 숫자들을 어느 ’순서’로 더하느냐에 따라 최종 누적 합계의 최하위 비트 단위(ULP, Unit in the Last Place) 값이 미세하게 어긋나게 되며 결합 법칙이 성립하지 않는다.<br />
<span class="math math-display">
(a + b) + c \neq a + (b + c)
</span><br />
단일 CPU 아키텍처에서 직렬로 코드를 실행한다면 덧셈의 순서는 항상 동일하게 유지되므로 이 오차 역시 결정론적으로 고정될 것이다. 그러나 GPU는 수천 개의 스트리밍 멀티프로세서(SM) 코어에 스레드(Thread) 뭉치를 동시다발적으로 할당하여 덧셈 연산을 산발적으로 수행한다. 그리고 각각의 스레드가 연산을 완료하고 결과값을 메모리에 반환하는 도착 순서는 운영체제의 스케줄러 상태, 글로벌 메모리 대역폭의 병목, 코어의 순간적인 온도 등 인간이 결코 통제할 수 없는 미시적인 물리적 하드웨어의 요인들에 의해 매 밀리초마다 요동치는 완전히 비결정적(Non-deterministic)인 특성을 띤다.</p>
<h3>5.2 GPU 병렬 축소 연산(Reduction Operations)과 원자적 덧셈(Atomic Adds)</h3>
<p>LLM의 입력부터 출력까지 이어지는 순전파(Forward Pass) 연산 커널 내부에는 분산된 수많은 값들을 하나로 합산하는 축소(Reduction) 연산이 곳곳에 지뢰처럼 포진해 있다. 레이어 정규화를 수행하는 RMSNorm의 분산 계산, 거대한 행렬 곱셈(Matrix Multiplication) 과정에서의 내적 합산, 그리고 어텐션(Attention) 점수를 확률로 변환하는 소프트맥스 함수 내부의 총합 계산 등이 모두 거대한 축소 연산에 해당한다.</p>
<p>수많은 GPU 스레드가 자신이 맡은 일부분의 계산을 끝내고, 최종 합계를 내기 위해 전역 메모리의 단일한 주소 공간에 일제히 접근하여 값을 덮어쓰려 한다고 상상해보라. 심각한 데이터 충돌(Race Condition)이 발생할 것이다. 이를 방지하기 위해 GPU 프로그래밍 모델은 원자적 덧셈(Atomic Add)이라는 하드웨어 락(Lock) 기능을 제공한다. 이 기능은 한 스레드가 메모리에 값을 더하는 찰나의 순간 동안 다른 스레드의 접근을 차단하여 덧셈 연산 자체의 수학적 무결성을 훌륭하게 보장한다.</p>
<p>그러나 여기서 치명적인 모순이 발생한다. 원자적 덧셈은 값들이 하나도 누락되지 않고 안전하게 더해진다는 사실은 보장하지만, 수백 개의 스레드 중 ‘누구의 값이 먼저’ 메모리에 도착하여 더해질 것인지에 대한 절대적인 순서는 보장하지 않는다. 앞서 설명한 부동소수점 비결합 법칙이라는 화약고에 스레드 도착 순서의 무작위성이라는 불씨가 결합되는 순간 폭발적인 나비 효과가 시작된다. 매 API 호출마다, 매 텍스트 생성 시점마다 어텐션 스코어나 RMSNorm의 결과값은 부동소수점의 저 아래 심연에 위치한 소수점 단위에서 아주 미세하게 요동치며 달라지게 되며, 이는 런-투-런(Run-to-Run) 비결정성이라는 거대한 기술적 장벽으로 소프트웨어 엔지니어를 가로막게 된다.</p>
<h3>5.3 동적 배치(Dynamic Batching)와 혼합 전문가(MoE) 라우팅의 교란 현상</h3>
<p>하드웨어 수준의 미세한 오차가 최종적인 응답 텍스트를 완전히 뒤바꿀 만큼 증폭되는 데에는 현대 클라우드 LLM 인프라의 아키텍처 구조가 결정적인 역할을 한다. API를 서비스하는 기업들은 값비싼 GPU의 유휴 자원을 최소화하고 연산 처리량을 극대화하기 위해 전 세계에서 쏟아지는 수백 명의 사용자 요청을 메모리 상에서 하나의 거대한 덩어리로 묶어 동시에 처리하는 동적 배치(Dynamic Batching) 기술을 구사한다.</p>
<p>내가 <span class="math math-inline">T=0</span>으로 전송한 프롬프트가 GPU에 올라가는 순간, 옆자리에 함께 묶인 타인들의 프롬프트 배치 크기(Batch Size)나 텍스트의 패딩(Padding) 길이는 매번 다를 수밖에 없다. 배치 형태가 변경되면 GPU 컴파일러는 커널을 할당하고 연산 타일(Tiling)을 쪼개는 전략을 가장 효율적인 방식으로 실시간 변경하게 되며, 이는 곧 내부에서 일어나는 모든 덧셈 연산의 스레드 그룹핑과 합산 순서가 통째로 재구성됨을 의미한다. 즉, 타인의 무작위적인 요청 형태가 나의 독립적인 프롬프트 연산 궤적에 간섭을 일으키는 것이다.</p>
<p>이러한 상호 간섭은 최신 대형 언어 모델의 대세로 자리 잡은 혼합 전문가(MoE, Mixture of Experts) 아키텍처에서 정점을 찍는다. MoE 아키텍처는 모델 내부에 여러 개의 작은 ’전문가(Expert) 서브 네트워크’들을 병렬로 배치해두고, 입력되는 토큰의 성격에 따라 라우터(Router) 네트워크가 가장 적합한 소수의 전문가만을 동적으로 활성화하여 연산을 수행하는 구조이다. 그런데 전체 GPU 클러스터의 부하를 고르게 분산시키기 위해, 이 라우팅 과정은 단일 프롬프트가 아니라 현재 배치에 묶인 ’전체 토큰의 앙상블 분포’를 고려하여 할당량을 동적으로 조율하도록 설계되는 경우가 많다.</p>
<p>내가 전송한 프롬프트는 1초 전의 호출과 글자 하나 다르지 않게 완벽히 동일하더라도, 그 순간 동시대의 누군가가 보낸 프롬프트가 나와 동일한 배치 공간에 묶이는 우연이 발생하면 MoE의 라우팅 결정이 뒤틀리고 GPU 스레드의 스케줄링이 변동된다. 이로 인해 신경망의 수십 개 층을 통과하며 축적된 부동소수점 오차는 언임베딩 레이어를 통과하며 로짓 벡터에 저장되는 수치들에 미세한 진동을 유발한다.</p>
<p>소프트맥스의 치명적인 병목을 지나 최종 1위 토큰과 2위 토큰을 확정 짓는 탐욕적 탐색의 순간, 만약 이 두 토큰이 모델 내부에서 그 우선순위를 다투며 0.0001%의 확률 차이만으로 치열하게 경합하던 상황이었다면 어떻게 될까? 방금 전까지 축적된 아주 미세한 ULP(Unit in the Last Place) 단위의 부동소수점 오차가 그 근소한 확률의 격차를 뒤집어버리는 대형 사고를 일으킨다. 가장 유력했던 단어 대신 전혀 다른 단어가 <span class="math math-inline">t=1</span> 시점에 화면에 출력되는 순간, 이후 텍스트를 생성하기 위해 모델이 계산해야 하는 조건부 확률 <span class="math math-inline">p(x_t \vert x_{&lt;t})</span>의 문맥 기반은 완전히 틀어지게 되고, 결국 응답 전체의 논리적 궤도와 결론이 영구적으로 변경되는 치명적인 나비 효과가 완성된다.</p>
<p>이것이 소프트웨어 개발자가 온도를 0으로 낮추고 완벽한 통제를 기대하더라도, 시스템은 결코 절대적인 결정론적 오라클을 뱉어내지 않는 근본적인 하드웨어적 이유이다.</p>
<h2>6. 결언: 다음 토큰 예측 메커니즘이 소프트웨어 오라클에 던지는 질문</h2>
<p>지금까지 분석한 대형 언어 모델의 수학적 예측 메커니즘과 그로 인해 파생되는 다중적인 비결정성(알고리즘적 무작위성 및 물리적 하드웨어 오차)은, 소프트웨어 엔지니어링의 근간을 이루는 테스트 주도 개발(TDD)과 자동화 검증 시스템 아키텍처에 근본적인 인식의 전환을 촉구한다.</p>
<p>전통적인 소프트웨어 테스트의 정답지(Oracle)는 닫힌 세계관에서의 인과율과 결정론을 전제로 한다. “특정 로직을 포함한 함수 모듈에 정확한 매개변수 집합을 주입하면, 반환되는 결과값의 상태는 언제나 1비트의 오차도 없이 동일해야 한다“는 명확한 단언(Assertion)을 통해 시스템의 무결성을 검증한다. 그러나 확률 분포에 몸을 싣고 끝없는 조건부 탐색의 바다를 부유하는 대형 언어 모델 기반의 컴포넌트들을 테스트할 때, 이러한 문자열 1:1 매칭 기반의 엄격한 원자적 오라클(Atomic Oracle) 방식을 적용하면 테스트 파이프라인은 즉시 붕괴한다. 아무리 온도를 낮추고 프롬프트를 정교하게 다듬어 <span class="math math-inline">T=0</span>의 환상 속에 숨으려 해도, 의미론적으로는 완벽하게 올바른 응답이 토큰 하나의 미세한 변동으로 인해 텍스트 비교 알고리즘에서 거짓 음성(False Negative)으로 처리되어 빌드를 실패로 이끄는 대참사가 쉴 새 없이 발생하기 때문이다. 동일한 테스트를 수백 번 반복 실행했을 때 일치율이 100%에 고정되는 것이 아니라 85%에서 100% 사이를 오가며 진동하는 극심한 성능 불안정성(Performance Instability)은 이제 버그(Bug)가 아니라 모델의 본질적 기저 스펙(Feature)으로 받아들여져야 한다.</p>
<p>따라서 AI 시대의 선도적인 소프트웨어 개발 조직은 테스트 대상 소프트웨어(SUT, Software Under Test)의 무결성과 ’정확성’을 단일한 이분법적 수치(Pass or Fail)가 아니라, 확률적 통계 분포(Distribution of Outcomes)의 스펙트럼으로 이해하는 패러다임의 극적인 도약을 이뤄내야 한다.</p>
<p>이러한 난관을 돌파하기 위해 소프트웨어 아키텍트와 품질 보증(QA) 엔지니어는 단일 실행의 일치 여부를 묻는 편협한 원자적 오라클 체계에서 과감히 탈피해야 한다. 대신, 동일한 환경 조건에서 확률적 시스템을 다중 실행(Multi-sample)하여 결과의 분산 통계를 측정하고, 코사인 유사도를 통한 의미론적 정합성(Semantic Similarity)을 평가하며, 복수의 응답 모델 간의 다수결 투표(Majority Voting)를 집계하거나, 혹은 역으로 LLM 그 자체를 평가 및 판별자로 사용하는 강력한 집계 오라클(Aggregated Oracle) 체계로 인프라를 전면 재설계해야만 한다.</p>
<p>결론적으로, 대형 언어 모델의 심장부에서 고동치는 다음 토큰 예측 메커니즘과 로짓의 기하학, 그리고 확률론적 무작위성을 뼛속 깊이 이해하는 것은 단순한 AI 공학 이론의 지적 유희를 넘어선다. 이는 통제 불가능한 하드웨어 비결정성과 평탄한 확률 분포의 혼돈이 몰아치는 실전 비즈니스 환경 한가운데에서, 흔들리지 않고 강건하게 작동하는 신뢰할 수 있는 AI 소프트웨어 아키텍처를 세우기 위한 가장 최우선적이고 치열한 선결 조건이다. 예측할 수 없는 것을 통제하려 들지 말고, 텍스트가 발산하는 통계적 분포의 경계를 포용하고 관리함으로써 오라클의 개념을 확률의 영역으로 승격시켜야만 우리는 진정한 AI 소프트웨어 공학의 시대로 진입할 수 있다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Token Sampling Methods - aman.ai, https://aman.ai/primers/ai/token-sampling/</li>
<li>Understanding LLMs: Insights from Mechanistic Interpretability, https://www.lesswrong.com/posts/XGHf7EY3CK4KorBpw/understanding-llms-insights-from-mechanistic</li>
<li>Challenges in Testing Large Language Model Based Software, https://arxiv.org/html/2503.00481v2</li>
<li>[D] Non-deterministic behavior of LLMs when temperature is 0 - Reddit, https://www.reddit.com/r/MachineLearning/comments/1ie15ev/d_nondeterministic_behavior_of_llms_when/</li>
<li>Why is deterministic output from LLMs nearly impossible? - Unstract, https://unstract.com/blog/understanding-why-deterministic-output-from-llms-is-nearly-impossible/</li>
<li>How LLMs Choose Their Words: A Practical Walk-Through of Logits, https://machinelearningmastery.com/how-llms-choose-their-words-a-practical-walk-through-of-logits-softmax-and-sampling/</li>
<li>From Logits to Tokens. Introduction | by Aditya Modi | Medium, https://medium.com/@adimodi96/from-logits-to-tokens-9a36feab9cab</li>
<li>Probing Geometry of Next Token Prediction Using Cumulant, https://openreview.net/pdf/755c1242edccbf78ea2563085a35e18bb9dce521.pdf</li>
<li>Closing the Curious Case of Neural Text Degeneration, https://www.semanticscholar.org/paper/Closing-the-Curious-Case-of-Neural-Text-Finlayson-Hewitt/678a3cc761024b6daaf41ac4333f695358447a2f</li>
<li>ICLR Poster Closing the Curious Case of Neural Text Degeneration, https://iclr.cc/virtual/2024/poster/18260</li>
<li>[1904.09751] The Curious Case of Neural Text Degeneration - ar5iv, https://ar5iv.labs.arxiv.org/html/1904.09751</li>
<li>The curious case of neural text degeneration - CEUR-WS.org, https://ceur-ws.org/Vol-2540/FAIR2019_paper_15.pdf</li>
<li>THE CURIOUS CASE OF NEURAL TEXT DeGENERATION, https://openreview.net/pdf?id=rygGQyrFvH</li>
<li>LLM Sampling Demystified: How to Stop Hallucinations in Your Stack, https://mikulskibartosz.name/llm-sampling</li>
<li>Determinism in LLMs: Order of Operations, Precision and Why It, https://medium.com/aimonks/determinism-in-llms-order-of-operations-precision-and-why-it-breaks-3192c69eaec4</li>
<li>LLM Sampling: Temperature, Top-K, Top-P, and Min-P Explained, https://www.letsdatascience.com/blog/llm-sampling-temperature-top-k-top-p-and-min-p-explained</li>
<li>Understanding Temperature, Top-k, and Top-p Sampling in, https://codefinity.com/blog/Understanding-Temperature%2C-Top-k%2C-and-Top-p-Sampling-in-Generative-Models</li>
<li>Does Temperature 0 Guarantee Deterministic LLM Outputs?, https://www.vincentschmalbach.com/does-temperature-0-guarantee-deterministic-llm-outputs/</li>
<li>Non-Determinism of “Deterministic” LLM System Settings in Hosted, https://aclanthology.org/2025.eval4nlp-1.12.pdf</li>
<li>Deterministic Software Testing vs Non-deterministic LLM Agent, https://medium.com/@promptedmind28/deterministic-software-testing-vs-non-deterministic-llm-agent-testing-what-you-need-to-know-f3abd5f9009d</li>
<li>(PDF) Challenges in Testing Large Language Model Based Software, https://www.researchgate.net/publication/389547844_Challenges_in_Testing_Large_Language_Model_Based_Software_A_Faceted_Taxonomy</li>
<li>The Curious Case of Neural Text Degeneration - ResearchGate, https://www.researchgate.net/publication/332590110_The_Curious_Case_of_Neural_Text_Degeneration</li>
<li>arXiv:1904.09751v2 [cs.CL] 14 Feb 2020, https://arxiv.org/pdf/1904.09751</li>
<li>Nucleus Sampling for Natural Language Processing, https://jamesmccaffrey.wordpress.com/2021/10/14/nucleus-sampling-for-natural-language-processing/</li>
<li>Top-p sampling - Wikipedia, https://en.wikipedia.org/wiki/Top-p_sampling</li>
<li>Temperature, Top-K and Top-P Sampling in LLMs - GeeksforGeeks, https://www.geeksforgeeks.org/artificial-intelligence/graph-based-semi-supervised-learning/</li>
<li>Foundation model parameters: decoding and stopping criteria - IBM, https://www.ibm.com/docs/en/watsonx/saas?topic=prompts-model-parameters-prompting</li>
<li>Min-p Sampling for Creative and Coherent LLM Outputs - arXiv, https://arxiv.org/pdf/2407.1082</li>
<li>MIN-P SAMPLING FOR CREATIVE AND COHERENT LLM OUTPUTS, https://openreview.net/notes/edits/attachment?id=LnzBe4fymH&amp;name=pdf</li>
<li>Min-p Sampling for Creative and Coherent LLM Outputs - arXiv.org, https://arxiv.org/html/2407.01082v2</li>
<li>Min-p Sampling for Creative and Coherent LLM Outputs - arXiv.org, https://arxiv.org/abs/2407.01082</li>
<li>ICLR 2025 Orals, https://iclr.cc/virtual/2025/events/oral</li>
<li>A Critical Analysis of Min-p Sampling in Language Models - arXiv, https://arxiv.org/abs/2506.13681</li>
<li>Apart News: ICLR Awards &amp; Women in AI Safety, https://apartresearch.com/news/apart-news-iclr-awards-women-in-ai-safety</li>
<li>Min-p sampling for LLMs - Thoughtworks, https://www.thoughtworks.com/insights/blog/generative-ai/Min-p-sampling-for-LLMs</li>
<li>(PDF) Turning Down the Heat: A Critical Analysis of Min-p Sampling, https://www.researchgate.net/publication/392735665_Turning_Down_the_Heat_A_Critical_Analysis_of_Min-p_Sampling_in_Language_Models</li>
<li>Defeating Nondeterminism in LLM Inference - Thinking Machines Lab, https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/</li>
<li>Defeating Non-Determinism in LLMs: Solving AI’s Reproducibility, https://www.flowhunt.io/blog/defeating-non-determinism-in-llms/</li>
<li>Nit: in practice, even at temperature 0, production LLM, https://news.ycombinator.com/item?id=44461746</li>
<li>Challenges in Testing Large Language Model Based Software - arXiv, https://arxiv.org/html/2503.00481v1</li>
<li>The Challenges of Testing in a Non-Deterministic World, https://www.sei.cmu.edu/blog/the-challenges-of-testing-in-a-non-deterministic-world/</li>
<li>(PDF) Verifiable LLM-Generated Test Oracles - ResearchGate, https://www.researchgate.net/publication/398511554_Verifiable_LLM-Generated_Test_Oracles_Ensuring_Consistency_Correctness_and_Explainability_in_AI-_Assisted_Testing</li>
<li>Reliability for unreliable LLMs - The Stack Overflow Blog, https://stackoverflow.blog/2025/06/30/reliability-for-unreliable-llms/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>