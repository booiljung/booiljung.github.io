<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:4.1.3 비결정적 응답이 자동화 테스트 및 오라클 구축에 미치는 악영향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>4.1.3 비결정적 응답이 자동화 테스트 및 오라클 구축에 미치는 악영향</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 4. AI 모델 응답의 일관성 확보를 위한 프롬프트 엔지니어링 및 파라미터 제어</a> / <a href="index.html">4.1 확률적 AI와 결정론적 소프트웨어의 충돌</a> / <span>4.1.3 비결정적 응답이 자동화 테스트 및 오라클 구축에 미치는 악영향</span></nav>
                </div>
            </header>
            <article>
                <h1>4.1.3 비결정적 응답이 자동화 테스트 및 오라클 구축에 미치는 악영향</h1>
<p>소프트웨어 엔지니어링의 역사는 예측 가능성(Predictability)과 결정론(Determinism)을 확보하기 위한 투쟁의 과정이었다. 동일한 입력(Input)과 초기 상태(State)가 주어지면 언제나 동일한 출력(Output)과 상태 변화를 반환해야 한다는 튜링 기계(Turing Machine)의 근본 원리는 현대 소프트웨어 아키텍처와 자동화 테스트 파이프라인을 지탱하는 핵심 철학이다. 이러한 결정론적 시스템에서는 결함의 재현이 용이하며, 예상되는 결과를 하드코딩된 규칙으로 검증하는 것이 당연한 절차로 여겨졌다. 그러나 거대 언어 모델(LLM)을 비롯한 생성형 AI(Generative AI)가 소프트웨어 시스템의 핵심 컴포넌트로 편입되면서, 이러한 결정론적 가정은 근본적인 붕괴를 맞이했다.</p>
<p>AI 모델의 응답이 지니는 본질적인 비결정성(Nondeterminism)은 단순히 출력이 매번 달라질 수 있다는 단순한 가변성을 넘어선다. 이는 소프트웨어의 품질을 검증하고 보증하는 ’테스트 오라클(Test Oracle)’의 구축을 극도로 난해하게 만들며, 정답을 판별하는 기준 자체를 모호하게 만든다. 결과적으로 이 비결정성은 CI/CD(지속적 통합/지속적 배포) 파이프라인의 신뢰성을 무너뜨리고, 유지보수 비용을 기하급수적으로 증가시키며, 종국에는 개발 조직 전체의 생산성과 소프트웨어 품질 보증 체계를 심각하게 저해하는 악영향을 초래한다. 본 절에서는 확률적 AI의 비결정적 응답이 자동화 테스트 환경과 오라클 설계에 미치는 구조적, 경제적, 심리적 악영향을 학술적 연구와 산업계 사례를 바탕으로 심층적으로 분석한다.</p>
<h2>1.  소프트웨어 검증의 근본 원리 붕괴와 테스트 오라클 문제의 심화</h2>
<p>전통적인 소프트웨어 테스트의 가장 중요한 전제는 테스트 대상 시스템(System Under Test, SUT)의 동작이 올바른지 판별할 수 있는 명확한 기준, 즉 ’결정론적 정답지(Deterministic Ground Truth)’가 존재한다는 것이다. 그러나 AI 시스템의 비결정성은 이러한 정답지의 존재 자체를 부정하며, 검증의 패러다임을 근본적으로 뒤흔든다.</p>
<h3>1.1  오라클 문제(Test Oracle Problem)의 학술적 정의와 AI 환경에서의 진화</h3>
<p>소프트웨어 공학에서 ’오라클(Oracle)’이란 주어진 입력에 대해 시스템이 산출한 실제 결과(Actual Result)가 기대되는 올바른 동작(Expected Behavior)과 일치하는지 판별하는 메커니즘을 의미한다. 기념비적인 논문 “The Oracle Problem in Software Testing: A Survey” 에서 상세히 정의한 바와 같이, 올바른 동작과 잠재적으로 잘못된 동작을 구별하는 과제 자체가 ’테스트 오라클 문제(Test Oracle Problem)’로 규정된다. 전통적인 소프트웨어 테스트에서는 이 오라클을 자동화하는 것이 인간의 개입을 줄이고 테스트 커버리지를 높이는 핵심 병목으로 지적되어 왔으나, 시스템의 동작이 결정론적이었기 때문에 사양서(Specification)나 계약 기반 프로그래밍(Contract-driven Development)을 통해 자동화된 오라클을 구축하는 것이 가능했다.</p>
<p>그러나 전통적인 시스템에서는 적어도 정답이 ’무엇’인지에 대한 명확한 참값(Ground Truth)이 존재했던 반면, AI 컴포넌트가 포함된 시스템에서는 논문 “Software Testing of Generative AI Systems: Challenges and Opportunities” 에서 지적하듯 완전히 새로운 차원의 오라클 문제가 발생한다. 생성형 AI 시스템은 훈련 데이터의 확률 분포를 기반으로 주관적이고 다양하며 창의적인 출력을 생성하기 때문에, 비교의 기준이 되는 단일하고 확정적인 정답지(Definitive Ground Truth)를 설정하는 것 자체가 불가능에 가깝다. 비결정적 시스템에서는 동일한 테스트 케이스를 여러 번 실행할 경우 각기 다른 결과가 도출될 수 있으므로, 단 한 번의 실행만으로는 해당 테스트 케이스가 진정으로 통과했는지 혹은 실패했는지 확정할 수 없는 치명적인 한계에 봉착한다.</p>
<h3>1.2  구문론적 일치에서 의미론적 타당성으로의 강제 이동</h3>
<p>비결정적 응답은 테스트 검증 로직을 ’구문론적 일치(Syntactic Equality)’에서 ’의미론적 타당성(Semantic Validity)’의 영역으로 강제 이동시킨다. 기존의 단위 테스트(Unit Test)는 산출물이 사전에 정의된 값과 텍스트 수준에서 정확히 일치하는지 검증하는 확정적 방식을 취했다. 전통적인 결정론적 테스트에서는 입력에서 단일하고 예리한 기대 출력(Expected Output) 지점으로 이어지는 엄격한 구문적 일치가 요구되었으며, 이 좁은 허용 오차를 벗어나는 모든 출력은 오류로 간주되었다. 그러나 확률적 AI 시스템의 테스트에서는 이러한 접근 방식이 완전히 무력화된다. AI 시스템의 출력은 단일한 점이 아니라, 의미론적으로 유효한 수많은 텍스트 응답(예: ‘환불’, ‘환불 요청’, ‘환불 처리 진행’)이 광범위하게 퍼져 있는 확률적 분포의 구름(Probability Distribution Cloud) 형태를 띤다.</p>
<p>이러한 특성으로 인해 엄격한 일치 검증 방식의 오라클은 무용지물이 되며, 심각한 거짓 음성(False Negative)을 유발하게 된다. AI 모델은 문법과 구문 규칙을 준수할 뿐만 아니라 의미론(Semantics), 화용론(Pragmatics), 그리고 문맥(Context)에 대한 이해를 바탕으로 콘텐츠를 생성하기 때문에 이를 수치화하거나 정량화하여 단순한 결정론적 오라클로 구축하는 것은 극도로 어렵다. 예를 들어 비즈니스 로직 검증을 위한 AI 챗봇 오라클을 구성할 때, 챗봇이 반환하는 환불 규정 안내가 의미론적으로는 완벽히 정확하더라도 조사 하나, 띄어쓰기 하나가 달라지면 기존의 자동화 테스트 시스템은 이를 명백한 버그로 식별한다. 이는 AI 모델의 본질적 행위가 확률적이고 비결정적임에도 불구하고, 이를 평가하는 검증 시스템은 여전히 결정론적 정답지를 요구하는 데서 발생하는 치명적인 불일치이다.</p>
<h2>2.  자동화 테스트 파이프라인(CI/CD)의 마비와 플레이키 테스트(Flaky Tests)의 창궐</h2>
<p>비결정성이 자동화 테스트 환경에 미치는 가장 파괴적이고 직접적인 악영향은 ’플레이키 테스트(Flaky Tests)’의 폭발적인 증가다. 플레이키 테스트란 대상 소스 코드나 테스트 환경에 아무런 변경이 없음에도 불구하고, 실행할 때마다 통과(Pass)와 실패(Fail)를 불규칙하고 예측 불가능하게 반복하는 비결정적 테스트를 의미한다.</p>
<h3>2.1  대규모 언어 모델(LLM) 환경에서의 플레이키 테스트 발현 메커니즘</h3>
<p>AI 애플리케이션의 관점에서 LLM은 본질적으로 가장 극단적인 형태의 플레이키 테스트를 유발하는 원흉이다. 전통적인 소프트웨어 공학에서 플레이키니스(Flakiness)는 주로 다중 스레드의 병행성 문제(Concurrency Issues), 네트워크 지연, 가상 머신(VM) 및 다중 코어 프로세서의 자원 경합, 혹은 불안정한 외부 API 의존성 등 물리적, 환경적 요인에 의해 기인했다. 이러한 전통적 플레이키니스는 시스템 통제력을 높이고 격리(Isolation) 수준을 강화하며 동기화 대기 시간(Explicit Waits)을 조정함으로써 어느 정도 통제와 억제가 가능했다.</p>
<p>그러나 논문 “Challenges in Testing Large Language Model Based Software: A Faceted Taxonomy” 에 따르면, LLM의 비결정성은 외부 환경의 요인을 넘어서 모델의 생성 구조 자체에서 기인하는 ’본질적 예측 불가능성(Fundamental Unpredictability)’이다. LLM API를 호출할 때 생성 온도를 0으로 설정(Temperature=0)하거나 특정 Seed 값을 부여하여 겉보기에 결정론적으로 설정하더라도, 고성능 분산 컴퓨팅 환경에서의 부동소수점 연산 미세 오차, 하드웨어 아키텍처의 비동기적 연산 처리 특성, 그리고 보이지 않는 모델 가중치의 미세한 백그라운드 업데이트 등으로 인해 완전히 동일한 프롬프트에 대해서도 미세한 변동성이 지속해서 발생한다. 이러한 근본적인 비결정성은 테스트 환경을 아무리 완벽하게 통제하고 격리하더라도 플레이키 테스트가 발생함을 의미한다.</p>
<h3>2.2  거짓 양성(False Positive)과 거짓 음성(False Negative)의 혼돈</h3>
<p>이러한 플레이키 테스트는 오라클 시스템에 심각한 교란을 일으키며 거짓 양성과 거짓 음성을 동시에 양산한다. 한편으로는, 모델이 실제로 치명적인 환각(Hallucination) 현상을 일으켜 허위 정보를 생성(Fabrication)하거나 비즈니스 정책을 위반했음에도 불구하고, 비결정적 특성으로 인해 우연히 간헐적으로 테스트를 통과해버리는 ’거짓 음성(테스트는 통과했으나 실제로는 결함이 존재함)’이 발생할 수 있다. 이는 안전 우선(Safety-critical) 시스템이나 금융 서비스와 같이 무결성이 생명인 도메인에서 치명적인 법적, 재무적 리스크를 초래한다.</p>
<p>다른 한편으로는, 모델이 정확하고 유용한 응답을 생성하여 소프트웨어의 비즈니스 목적을 달성했음에도 불구하고, 기존 결정론적 오라클의 경직된 하드코딩 규칙(정규표현식 매칭, 고정된 문자열 길이 제한 등)에 부합하지 않아 테스트가 실패하는 ’거짓 양성(허위 알람)’이 빈번하게 발생한다. 이처럼 신호가 극도로 혼탁해지면 자동화 테스트의 핵심 가치인 ’빠르고 신뢰할 수 있는 피드백 루프’는 완전히 파괴된다. 최신 DevOps 및 CI/CD 파이프라인은 자동화된 회귀 테스트(Regression Testing)의 일관된 결과를 기반으로 코드의 승급 및 배포 여부를 결정하는 게이트키퍼 역할을 수행하는데, 비결정적 응답은 이 게이트키퍼의 판단 능력 자체를 상실하게 만드는 것이다.</p>
<h3>2.3  개발 조직의 경제적 손실과 시스템적 플레이키니스(Systemic Flakiness)</h3>
<p>플레이키 테스트는 단순히 테스트 코드를 재작성하거나 무시하면 되는 가벼운 문제를 넘어, 소프트웨어 공학적 관점에서 엄청난 경제적 매몰 비용과 생산성 저하를 동반한다. 구글(Google)의 연구 데이터와 다양한 산업계 사례 연구는 플레이키 테스트가 조직에 미치는 파괴력을 정량적으로 증명한다.</p>
<p>구글의 조사에 따르면, 전체 자동화 테스트 실패의 약 4.56%가 코드의 실제 결함이 아닌 플레이키 테스트에 의한 허위 실패이며, 엔지니어들은 이러한 불안정한 테스트를 분석하고 디버깅하는 데 전체 코딩 시간의 2% 이상을 허비하고 있다. 이를 50명 규모의 일반적인 소프트웨어 개발팀에 대입하면, 매년 1명의 풀타임 엔지니어(FTE) 전체 노동력이 단순히 신뢰할 수 없는 테스트를 재실행하고 추적하는 데 낭비되는 것과 같으며, 이는 조직 차원에서 연간 막대한 재정적 손실을 의미한다. 또한 산업계 사례 연구(Industrial Case Study)에 따르면, 개발자들은 CI 파이프라인 내에서 플레이키 테스트를 수리하는 데 작업 시간의 최대 1.28%를 사용하며, 이는 개발 팀당 월평균 $2,250의 직접적인 비용 손실을 초래한다. AI 시스템의 경우 매 테스트 실행마다 LLM API 토큰 호출 비용이 발생하므로, 결과를 확신하기 위해 테스트를 여러 번 반복 실행해야 하는 비결정적 특성은 컴퓨팅 리소스와 재무적 비용 모두를 폭발적으로 팽창시킨다.</p>
<p>최근의 심층 연구는 플레이키 테스트가 개별적으로 고립되어 발생하는 것이 아니라, 동일한 근본 원인(Root Cause)을 공유하며 여러 테스트가 동시에 실패하는 ‘시스템적 플레이키니스(Systemic Flakiness)’ 군집을 형성한다는 사실을 밝혀냈다. 분석 결과에 따르면 전체 플레이키 테스트의 75%가 이러한 군집에 속해 있으며, 하나의 군집은 평균적으로 13.5개의 테스트가 한꺼번에 무작위적 실패를 겪는 양상을 보인다. AI 오라클에서 시스템 프롬프트의 미세한 맥락 변화나 기반 모델의 내부 확률 분포 변경은 단일 테스트 하나가 아니라, 해당 모델을 검증하는 십수 개의 기능 테스트 전체를 동시에 붕괴시킬 수 있는 시스템적 취약점을 내포하고 있으며, 이는 문제 해결의 난이도를 극도로 높인다.</p>
<p><img src="./4.1.3.0.0%20%EB%B9%84%EA%B2%B0%EC%A0%95%EC%A0%81%20%EC%9D%91%EB%8B%B5%EC%9D%B4%20%EC%9E%90%EB%8F%99%ED%99%94%20%ED%85%8C%EC%8A%A4%ED%8A%B8%20%EB%B0%8F%20%EC%98%A4%EB%9D%BC%ED%81%B4%20%EA%B5%AC%EC%B6%95%EC%97%90%20%EB%AF%B8%EC%B9%98%EB%8A%94%20%EC%95%85%EC%98%81%ED%96%A5.assets/image-20260223202959993.jpg" alt="image-20260223202959993" /></p>
<h3>2.4  개발자 신뢰도 하락과 알람 피로(Alert Fatigue)</h3>
<p>정량적 비용 손실 못지않게 심각한 정성적 악영향은 자동화 테스트 스위트(Test Suite)에 대한 개발 조직의 ’신뢰 상실’이다. 테스트가 실패하여 빌드가 중단되었을 때, 개발자는 이것이 자신의 코드 변경으로 인해 발생한 진짜 소프트웨어 버그인지, 아니면 LLM의 일시적 변동성에 의한 ’또 다른 거짓 알람(False Alarm)’인지 판별하기 위해 시스템 로그를 뒤지고 실행 컨텍스트를 복기하는 등 상당한 인지적 부하(Cognitive Load)를 감내해야 한다.</p>
<p>이러한 현상이 반복되면 개발자들은 테스트 실패 알림 자체에 점차 둔감해지는 알람 피로(Alert Fatigue)에 빠지게 된다. 종국에는 진짜 치명적인 보안 결함이나 로직 에러가 발생하여 테스트가 실패하더라도 “LLM이 또 일시적인 환각을 일으켰겠지“라고 치부하며 경고를 의도적으로 무시하거나 해당 테스트를 비활성화한 채 프로덕션 환경에 배포하는 끔찍한 결과를 초래한다. 이 시점에서 테스트는 소프트웨어 품질을 보증하는 안전망(Safety Net)으로서의 핵심 기능을 완전히 상실하고, 오히려 배포 주기만 지연시키며 스트레스를 유발하는 관리 대상의 장애물로 전락하게 된다.</p>
<h2>3.  오라클 아키텍처의 강제적 구조 변환: 단일(Atomic)에서 집계형(Aggregated)으로</h2>
<p>AI 시스템의 극단적인 비결정적 응답은 기존의 단순하고 명료했던 검증 메커니즘을 완전히 무력화시키며, 테스트 오라클 구조의 전면적이고 비용 소모적인 재설계를 강제한다. 이는 소프트웨어 테스팅 학술계에서 ‘단일 오라클(Atomic Oracle)’ 패러다임에서 ‘집계형 오라클(Aggregated Oracle)’ 패러다임으로의 강제적 이동으로 규정된다.</p>
<h3>3.1  단일 오라클(Atomic Oracle)의 한계와 붕괴</h3>
<p>단일 오라클은 테스트 대상 시스템을 단 한 번 실행한 결과를 바탕으로 그 출력의 정확성과 무결성을 평가하는 전통적인 검증 메커니즘이다. 전통적인 소프트웨어 로직은 철저히 결정론적 성격을 띠므로, 입력 데이터가 동일하다면 1회의 실행 결과만으로도 전체 시스템 로직의 정합성을 100% 대표할 수 있었다. 이러한 단일 오라클은 시스템의 출력을 사전에 정의된 정답과 대조하기 위해 동일성 검증(Equality), 정규 표현식 매칭(Regular Expressions), 강제 구조화 출력(Structured Outputs)과 같은 엄격한 결정론적 확인 절차를 이용한다.</p>
<p>초기 AI 소프트웨어 개발자들은 이러한 단일 오라클 기법을 LLM 평가에 그대로 차용하려 시도했다. 텍스트 출력 결과에 특정 필수 키워드가 포함되어 있는지, JSON 스키마 형식을 엄격하게 준수하는지 규칙 기반 검증을 수행하는 식이다. 그러나 LLM의 비결정성 앞에서는 이러한 엄격한 단일 검증이 너무나도 쉽게 무너진다. 정답의 형태가 무한히 다양할 수 있는 생성 언어의 특성상, 프롬프트 내 지시어의 미세한 뉘앙스 차이나 모델의 내부 샘플링 확률(Top-p, Top-k) 변동에 의해서도 완전히 다른 문장 구조나 단어 조합이 튀어나오게 된다. 결과적으로 모델의 답변이 논리적으로 완벽히 타당하더라도 하드코딩된 단일 오라클의 정적 매칭 조건을 우회하거나 실패하게 만드는 구조적 한계가 발생한다.</p>
<h3>3.2  집계형 오라클(Aggregated Oracle)의 강제 도입</h3>
<p>단일 오라클의 빈약한 신뢰도와 높은 실패율을 보완하기 위해, AI 소프트웨어 공학에서는 비결정성을 통계학적으로 완화할 수 있는 ’집계형 오라클(Aggregated Oracle)’을 도입해야만 하는 상황에 직면했다. 집계형 오라클은 동일한 프롬프트와 테스트 조건하에서 여러 번의 독립적인 테스트 실행(Execution)을 반복한 뒤, 그 결과들의 집합에 통계적 집계 함수를 적용하여 최종적인 합의(Consensus)와 판정을 내리는 복합적인 방식이다. 집계형 오라클을 구성하는 주요 전략은 다음과 같다.</p>
<ul>
<li><strong>분산 및 엔트로피 측정(Variance &amp; Entropy Measurement):</strong> 모델이 반환한 응답 결과들의 엔트로피(불확실성)나 편차를 계산하여 일관성을 검증한다. 엔트로피 공식 <span class="math math-inline">E = -\sum p(i) \log p(i)</span>과 같이, 동일한 입력에 대해 패스(Pass)와 페일(Fail)이 얼마나 무작위적으로 분포하는지 수학적으로 평가하여 허용 한계치를 초과하면 테스트 실패로 간주한다.</li>
<li><strong>다수결 투표(Majority Voting):</strong> 동일한 테스트 프롬프트에 대해 모델에 10번을 반복 질의하고, 그중 7번 이상 일치하거나 유사한 의미론적 맥락이 도출되면 이를 최종 정답으로 확정하는 방식이다. 확률적 오류를 희석시키는 강력한 방법이다.</li>
<li><strong>신뢰도 가중치 채점(Confidence-weighted Scoring):</strong> 출력 결과와 참조(Reference) 데이터 간의 시맨틱 유사도(Semantic Similarity)를 다중으로 측정하여 평균 점수나 신뢰 수준(Confidence Level)을 산출하고, 이를 통과 임계값(Threshold)과 비교한다.</li>
</ul>
<p>다음 표는 전통적 결정론적 테스트 시스템과 AI 기반 비결정적 테스트 시스템의 아키텍처 및 오라클 구성 방식을 명확히 비교하여 보여준다.</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>전통적 결정론적 테스트 시스템</strong></th><th><strong>AI 기반 비결정적 테스트 시스템</strong></th></tr></thead><tbody>
<tr><td><strong>결과 도출 방식</strong></td><td>동일 입력에 대해 항상 100% 동일한 출력 반환</td><td>모델의 토큰 샘플링 방식에 의한 결과 가변성 (비결정적 분포)</td></tr>
<tr><td><strong>주요 오라클 유형</strong></td><td><strong>단일 오라클 (Atomic Oracle):</strong> 단 1회 실행으로 Pass/Fail 명확히 확정</td><td><strong>집계형 오라클 (Aggregated Oracle):</strong> 다중 실행 후 통계적 합의(Consensus) 도출</td></tr>
<tr><td><strong>성공 판별 기준</strong></td><td>구문론적 정확성 (Syntactic Exactness) 및 정답 값과의 1:1 매칭</td><td>의미론적 타당성 (Semantic Validity) 및 수용 가능한 신뢰도 임계치 도달 여부</td></tr>
<tr><td><strong>테스트 실패 시 의미</strong></td><td>비즈니스 로직 버그 혹은 명세 위반이 확실함 (명백한 에러 신호)</td><td>플레이키 테스트(Flakiness)일 확률 존재, 거짓 양성/음성 의심 (잡음이 섞인 신호)</td></tr>
<tr><td><strong>검증 비용 및 시간</strong></td><td>컴퓨팅 리소스 및 시간 소모가 극히 적음 (단일 단위 테스트당 마이크로초 단위)</td><td>다중 LLM API 반복 호출로 인해 금전 및 시간 비용 극대화 (초/분 단위 초과)</td></tr>
</tbody></table>
<h3>3.3  집계형 오라클 구축에 따르는 시간과 자원의 폭증</h3>
<p>집계형 오라클은 비결정성을 어느 정도 제어하고 테스트의 강건성(Robustness)을 높일 수 있는 현재로서는 가장 현실적인 대안이지만, 동시에 치명적인 단점을 수반한다. 바로 <strong>검증 비용과 시간의 폭발적 증가</strong>이다.</p>
<p>전통적인 소프트웨어 아키텍처에서 수천 개의 단위 테스트를 실행하는 데는 불과 몇 초의 시간이면 충분했다. 그러나 집계형 오라클을 적용하여 단 하나의 프롬프트 응답을 검증하기 위해 모델 추론을 10번 수행하고 그 결과를 분석하는 데는 수십 초에서 수 분이 소요된다. 이는 코드 커밋이 일어날 때마다 즉각적으로 피드백을 받아야 하는 CI/CD 파이프라인의 핵심 속성인 ’신속성’을 원천적으로 불가능하게 만든다. 또한 클라우드 기반 API를 사용할 경우 반복적인 테스트 쿼리로 인해 소모되는 막대한 토큰 비용은 프로젝트의 유지보수 예산을 급속도로 고갈시킨다. 결국, 비결정성을 극복하기 위한 오라클 검증 로직의 복잡성과 비용이 실제 비즈니스 시스템 자체의 구동 비용을 초과하는 주객전도의 현상이 발생하게 되는 것이다.</p>
<p><img src="./4.1.3.0.0%20%EB%B9%84%EA%B2%B0%EC%A0%95%EC%A0%81%20%EC%9D%91%EB%8B%B5%EC%9D%B4%20%EC%9E%90%EB%8F%99%ED%99%94%20%ED%85%8C%EC%8A%A4%ED%8A%B8%20%EB%B0%8F%20%EC%98%A4%EB%9D%BC%ED%81%B4%20%EA%B5%AC%EC%B6%95%EC%97%90%20%EB%AF%B8%EC%B9%98%EB%8A%94%20%EC%95%85%EC%98%81%ED%96%A5.assets/image-20260223203023712.jpg" alt="image-20260223203023712" /></p>
<h2>4.  대안적 오라클 검증 기법의 도입과 그에 따른 기술적 한계</h2>
<p>결정론적 단일 오라클의 실패를 인지한 소프트웨어 테스트 공학계는 확고한 정답지 없이도 소프트웨어의 결함을 찾아내는 고도의 기법인 ‘유사 오라클(Pseudo-oracle)’ 전략들을 AI 검증에 공격적으로 편입시키고 있다. 그러나 이들 기법 역시 생성형 AI의 강력한 비결정성을 통제하고 자동화하는 과정에서 필연적인 한계점들에 직면한다.</p>
<h3>4.1  메타모픽 테스트(Metamorphic Testing)의 적용과 복잡성</h3>
<p>메타모픽 테스트(Metamorphic Testing)는 테스트 오라클 문제(Test Oracle Problem)를 극복하기 위해 제안된 가장 대표적인 우회 전략 중 하나이다. 이 방법론은 주어진 단일 입력에 대한 명확한 출력 ’정답’을 직접 확인하는 대신, 원본 입력과 의도적으로 변형된 새로운 입력 사이의 논리적 변환 관계인 ’메타모픽 관계(Metamorphic Relations, MRs)’가 출력 결과에서도 동일하게 유지되는지를 검증한다. 예를 들어, 감성 분석 AI 모델을 테스트할 때 특정 문장에 대한 정확한 감성 점수를 모르더라도, 원본 문장 끝에 “정말 훌륭해!“라는 명백한 긍정적 수식어를 덧붙였다면(입력의 변환), 새롭게 출력되는 감성 점수는 최소한 원본 문장의 점수보다 높거나 같아야 한다는 관계성(MR)을 정의하여 모델의 무결성을 테스트하는 식이다.</p>
<p>메타모픽 테스트는 정답지가 불명확한 시스템 구조에서 예외를 탐지하는 강력한 수단이지만, 복잡한 프롬프트 엔지니어링이 요구되는 LLM 환경에서 실효성 있는 메타모픽 관계를 자동 도출하는 것은 극도로 난해하다. 도메인 지식을 갖춘 인간 엔지니어가 수작업으로 MR을 일일이 식별하고 수식화해야 하므로 확장에 한계가 존재한다. 더욱이 비결정성이 강한 생성형 AI는 때때로 변형된 입력에 대해 문맥의 의도 자체를 다르게 해석하여 완전히 이질적인 구조의 답변을 생성하기도 한다. 이 경우, 애초에 성립해야 할 메타모픽 관계마저 파괴되며 이는 버그인지 단순한 비결정적 산출물인지 판가름하기 힘든 또 다른 형태의 플레이키 테스트를 유발한다.</p>
<h3>4.2  차등 테스트(Differential Testing)와 다중 모델 교차 검증의 모순</h3>
<p>차등 테스트(Differential Testing)는 하나의 명세에 대해 독립적으로 구현된 여러 이종 시스템에 동일한 입력을 주고, 그 시스템들이 산출하는 출력 간의 편차나 불일치를 식별하여 잠재적인 결함을 찾아내는 기법이다. AI 테스트에서는 이를 응용하여 다중 모델 교차 평가(Multi-Model Evaluation)의 형태로 사용한다. 즉, 테스트 대상인 소규모 경량화 LLM(Small LLM)의 응답 무결성을 검증하기 위해, 비용은 비싸지만 지식수준이 높은 상용 대형 모델(예: GPT-4, Claude 3.5)을 일종의 ’임시 오라클(Temporary Oracle)’이자 ’참조 표준(Reference)’으로 삼고 두 모델 간의 출력 차이를 비교 분석하는 방식이다. 최근에는 아예 오라클 평가만을 전담하는 별도의 ’LLM-as-a-Judge’를 파이프라인에 배치하기도 한다.</p>
<p>그러나 이 접근법은 논리적인 모순과 치명적인 약점을 수반한다. 오라클 역할을 수행하는 상위 검증 모델 그 자체 역시 본질적으로 비결정성을 내포하고 있기 때문에, 어떠한 상황에서도 100% 신뢰할 수 있는 절대적인 기준점(Ground Truth)이 될 수 없다. 심지어 오라클 역할을 하는 LLM이 환각(Hallucination) 현상을 일으켜 올바르게 작성된 테스트 코드를 잘못되었다고 지적하거나, 존재하지 않는 API 규칙을 근거로 허위 경고(False Positive)를 대량 발생시킬 수도 있다. 논문 “Verifiable LLM-Generated Test Oracles“에 따르면 LLM 기반 오라클은 논리적 모순, 극단적 예외 상황(Edge cases)에 대한 맹점, 애매한 규칙 적용 등의 위험을 지니고 있어 이를 다시 검증해야 하는 무한 퇴행(Infinite Regression)의 딜레마를 낳는다. 또한, 상위 검증 모델 호출에 따르는 극심한 네트워크 레이턴시와 토큰당 부과되는 막대한 경제적 비용은 현업 CI/CD 파이프라인에서 분당 수천 개의 쿼리를 자동 처리해야 하는 테스트 스케일링(Scaling)의 현실성을 철저히 파괴한다.</p>
<h3>4.3  속성 기반 테스트(Property-Based Testing)의 제약 완화와 무결성 공백</h3>
<p>AI 시스템의 비결정성을 수용하기 위해 실무 개발자들은 출력의 정확한 텍스트 값이 아닌, 출력 결과물이 반드시 만족해야 할 포괄적인 ’속성(Properties)’만을 정의하고 이를 검증하는 속성 기반 테스트(Property-Based Testing)를 빈번하게 도입한다. 응답이 반드시 완전한 JSON 형식이어야 한다거나, 특정 키(Key) 값인 “status“를 반드시 포함해야 한다, 혹은 응답의 전체 길이가 일정 토큰 수 이내여야 한다는 식의 비교적 헐거운 제약을 오라클의 통과 기준으로 설정하는 방식이다.</p>
<p>이 기법은 테스트 조건이 유연하기 때문에 플레이키니스가 발생할 확률을 획기적으로 낮추어 CI/CD 파이프라인을 부드럽게 통과시키는 데는 큰 효과가 있다. 그러나 이는 치명적인 소프트웨어 무결성 공백을 낳는다. 예를 들어 생성형 AI가 반환한 응답 형식이 완벽한 JSON이고 문법적 속성을 모두 만족하더라도, JSON 내부 데이터에 담긴 가치가 심각한 편향성을 띠고 있거나 치명적인 논리적 사실성 왜곡(Fabrication)을 포함하고 있다면 속성 기반 테스트는 이를 잡아내지 못하고 정상 통과시킨다. 결과적으로 속성 기반 테스트 단독으로는 소프트웨어의 비즈니스 가치 훼손이나 심각한 애플리케이션 논리적 오류를 사전에 차단하는 강력하고 신뢰성 있는 오라클을 구축할 수 없다.</p>
<h2>5.  다중 에이전트 시스템(MAS) 환경에서의 비결정성 증폭과 연쇄적 오라클 붕괴</h2>
<p>단일 AI 모델에서 발생하는 비결정성의 악영향은, 여러 특수 목적의 AI 에이전트가 각자의 역할을 수행하며 상호 협력하는 다중 에이전트 시스템(Multi-Agent Systems, MAS) 아키텍처에서 재앙적 수준으로 증폭된다.</p>
<h3>5.1  창발적 행동(Emergent Behavior)과 비선형적 상호작용</h3>
<p>MAS 환경에서는 수많은 에이전트들이 각자의 프롬프트 지시와 확률적 추론에 따라 상호작용한다. 이때 특정 에이전트가 내뱉은 아주 미세한 확률적 변동성이 다음 단계 에이전트의 입력(Input)으로 전달되면서 오해가 발생하고, 이 오류가 파이프라인을 거치며 기하급수적으로 누적된다. 이러한 과정은 시스템 전체 차원에서 설계자가 의도하지 않은 복잡하고 예측 불가능한 ’창발적 행동(Emergent Behavior)’이나 거대한 나비 효과를 유발한다.</p>
<p>예를 들어 문서를 요약하는 에이전트가 단어의 뉘앙스를 미세하게 다르게 생성하면, 이를 바탕으로 데이터베이스 질의를 생성하는 에이전트는 엉뚱한 테이블을 조회하게 되고, 최종 응답을 작성하는 에이전트는 전혀 다른 주제의 리포트를 작성하게 된다. 단일 에이전트의 관점에서는 단순한 텍스트 변동성에 불과했던 비결정성이, 다중 시스템 전체로 확장되면서 치명적인 비즈니스 로직 장애로 발현되는 것이다. 단일 컴포넌트 단위의 명확한 명세서조차 없는 상황에서, 이토록 복잡하고 파괴적인 창발적 결함을 탐지해내는 테스트 오라클을 설계하는 것은 소프트웨어 공학의 난제 중 난제이다.</p>
<h3>5.2  에이전트 간 통신 취약성(Communication Brittleness)과 추적 불가능성</h3>
<p>다중 에이전트 아키텍처의 효용성은 에이전트 간의 통신 및 조정 프로토콜의 질에 전적으로 달려있다. 그러나 에이전트들이 주고받는 메시지는 구조화된 API 규격이 아니라 자연어(Natural Language)에 크게 의존하기 때문에 본질적인 모호성과 오해의 소지를 안고 있다(Communication Brittleness). 비결정적 출력으로 인한 사소한 형식 이탈, 가령 JSON 키 이름의 무작위 변형이나 예기치 않은 텍스트 부연 설명 추가 하나가 전체 파이프라인 메시지 파싱의 연쇄적인 붕괴를 초래한다.</p>
<p>더욱 심각한 것은 디버깅 및 추적(Tracing)의 불가능성이다. 복잡계 네트워크처럼 얽혀있는 에이전트 시스템에서 테스트가 실패했을 때, 어떤 에이전트의 어떤 확률적 변동이 혹은 어떤 모호한 메시지 전달이 실패의 근본 원인(Root Cause)인지 역추적하는 것은 엄청난 시간과 모니터링 비용을 요구한다. MAS 환경에서 오라클은 개별 에이전트의 미시적 행동 규칙을 일일이 검증하는 대신, 전체 시스템이 협력하여 도출해 낸 최종 결과가 거시적 관점에서 과연 ’합리적인가(Reasonable)’를 심사해야 하는 극도의 추상적 어려움에 처하게 되며, 이는 자동화 시스템 구축 비용을 다시 한번 폭증시키는 주원인이 된다.</p>
<h2>6.  비결정성 제어 실패로 인한 기술 부채(Technical Debt) 누적과 경제적 파급력</h2>
<p>결과적으로, 비결정적 응답을 체계적으로 통제하고 제어할 수 있는 아키텍처를 확보하지 못한 상태에서 급조된 자동화 테스트 및 오라클 시스템은 결국 유지보수가 불가능한 거대한 기술 부채(Technical Debt)로 귀결된다.</p>
<h3>6.1  프롬프트-오라클 동기화의 지속적 붕괴와 베이스라인 표류</h3>
<p>정통적인 결정론적 소프트웨어 개발에서는 요구사항과 비즈니스 로직 코드가 근본적으로 변경될 때만 그에 대응하는 단위 테스트 코드를 수정하면 되었다. 그러나 훈련 데이터가 끊임없이 진화하고 매개변수가 조정되는 AI 소프트웨어 세계에서는, 클라우드 제공자가 기반 모델(Base Model)의 버전을 백그라운드에서 조용히 업데이트하거나(Data shifts), 개발자가 응답 품질을 높이기 위해 프롬프트 지시어 단 하나를 수정할 때마다 생성되는 출력의 의미론적 범위 전체가 요동친다.</p>
<p>이에 따라 과거에 오랜 시간을 들여 정교하게 구축해 둔 수많은 텍스트 매칭 규칙, 휴리스틱 오라클, 메타모픽 관계식들이 하루아침에 무용지물이 되며 CI/CD 파이프라인에 대량의 즉각적인 테스트 실패를 쏟아낸다. 파이프라인을 다시 정상화하기 위해 QA 엔지니어와 개발자들은 계속해서 오라클 검증 로직을 새로운 출력 패턴에 맞게 미세 조정(Tuning)하고 제약을 완화해야 하는 소모전에 빠진다.</p>
<p>소프트웨어 시스템이 데이터를 받아들이고 스스로 지속 진화(Self-learning)하거나 환경에 따라 출력 분포가 변동성을 띨 때, 기존 테스트 케이스를 최신 상태로 유지하고 오라클을 재평가하는 데 드는 지속적인 유지보수 운영 비용은 초기 테스트 시스템 구축 비용을 아득히 상회하게 된다. 이러한 기술 부채는 코드베이스를 경직시키고 새로운 AI 기능의 도입을 극도로 꺼리게 만드는 혁신의 장벽으로 작용한다.</p>
<h2>7.  요약: 자동화 테스트 패러다임 시프트와 결정론 확보의 절대적 당위성</h2>
<p>결론적으로, AI 모델의 비결정적 응답은 개발 단계에서 마주하는 출력의 단순한 가변성이나 사소한 불편함이 아니다. 이는 현대 소프트웨어 엔지니어링 품질 보증의 근간인 테스트 오라클 문제(Test Oracle Problem)를 극도로 심화시키며, 구문적 정확성과 상태 예측 가능성을 요구하는 전통적 자동화 검증 메커니즘을 완전히 붕괴시키는 구조적 위협이다. 생성 모델의 무작위성과 맥락적 민감성에서 기인하는 플레이키 테스트(Flaky Tests)의 대량 양산은 CI/CD 파이프라인의 배포 프로세스를 마비시키고 개발자의 시스템 신뢰도를 무너뜨리며, 조직 차원에서 막대한 노동력, 컴퓨팅 리소스, 그리고 API 토큰 호출에 대한 재무적 매몰 비용을 지속해서 발생시킨다.</p>
<p>이를 우회하고 극복하기 위해 소프트웨어 공학계에서 고안해 낸 단일 오라클에서 집계형 오라클로의 아키텍처 전환, 메타모픽 관계 기반 테스트, 다중 모델 기반의 차등 교차 평가 등의 진보된 대안들 역시 막대한 컴퓨팅 오버헤드와 끝없는 오라클 검증 비용, 그리고 걷잡을 수 없는 유지보수 기술 부채를 유발하는 본질적 한계를 안고 있다. 결국 다중 에이전트 시스템을 포함한 현대적 AI 기반 소프트웨어 개발 파이프라인에서 자동화 테스트의 가치를 회복하고 강건한 품질 오라클을 성공적으로 구축하기 위해서는, 단순히 결과의 무작위성을 통계적으로 수용하는 방어적 태도를 넘어 확률적 공간 내에서 어떻게든 모델의 변동성을 제약하고 출력을 고정해내는 적극적이고 강력한 시스템 통제 기법이 필수 불가결하다.</p>
<p>이어지는 챕터에서는 이러한 비결정적 악영향의 사슬을 끊어내고 테스트 환경 내에서 잃어버린 결정론적 정답지(Deterministic Ground Truth)를 복원하기 위해, 시스템 프롬프트의 체계적 제어, 하이퍼파라미터(Temperature, Top-p 등) 및 재현성 보장을 위한 Seed 값의 정밀한 조율, 그리고 퓨샷 러닝(Few-Shot Learning)을 통한 강력한 맥락 주입 등 확률적 시스템 위에 결정론적 통제망을 구축하는 구체적이고 실전적인 엔지니어링 전략들을 심층적으로 살펴본다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>The Oracle Problem in Software Testing: A Survey - IEEE Xplore, https://ieeexplore.ieee.org/iel7/32/7106034/06963470.pdf</li>
<li>‪Shin Yoo‬ - ‪Google Scholar‬, https://scholar.google.com/citations?user=W9ymXf4AAAAJ&amp;hl=en</li>
<li>(PDF) The Oracle Problem in Software Testing: A Survey, https://www.researchgate.net/publication/276255185_The_Oracle_Problem_in_Software_Testing_A_Survey</li>
<li>Software Testing of Generative AI Systems: Challenges and … - arXiv, https://arxiv.org/pdf/2309.03554</li>
<li>(PDF) Software Testing of Generative AI Systems: Challenges and, https://www.researchgate.net/publication/373754205_Software_Testing_of_Generative_AI_Systems_Challenges_and_Opportunities</li>
<li>Testing Non-Deterministic Research Software, https://bssw.io/items/testing-non-deterministic-research-software</li>
<li>Testing AI Systems: Handling the Test Oracle Problem, https://dev.to/qa-leaders/testing-ai-systems-handling-the-test-oracle-problem-3038</li>
<li>Flaky Tests in Automation: Strategies for Reliable Automated Testing, https://www.ranorex.com/blog/flaky-tests/</li>
<li>Dictionary of Flaky Tests | by Andrey Enin - Medium, https://adequatica.medium.com/dictionary-of-flaky-tests-f531f768203d</li>
<li>How to Test LLM Powered Apps: Managing Flaky Tests - Semaphore, https://semaphore.io/blog/llms-flaky-tests</li>
<li>Flaky Tests in Software Testing: How to Identify, Fix, and Prevent Them, https://www.testrail.com/blog/flaky-tests/</li>
<li>The Challenges of Testing in a Non-Deterministic World, https://www.sei.cmu.edu/blog/the-challenges-of-testing-in-a-non-deterministic-world/</li>
<li>(PDF) Challenges in Testing Large Language Model Based …, https://www.researchgate.net/publication/389547844_Challenges_in_Testing_Large_Language_Model_Based_Software_A_Faceted_Taxonomy</li>
<li>Pathways to energy transition: Replication of a faceted taxonomy, https://www.researchgate.net/publication/343447110_Pathways_to_energy_transition_Replication_of_a_faceted_taxonomy</li>
<li>The Hidden Costs of Flaky Tests: A Deep Dive into Test Reliability, https://www.stickyminds.com/article/hidden-costs-flaky-tests-deep-dive-test-reliability-0</li>
<li>An Empirical Analysis of Co-Occurring Flaky Test Failures - arXiv, https://arxiv.org/html/2504.16777</li>
<li>What Is A Flaky Test? Causes, Impacts &amp; How To Deal With Them, https://medium.com/@alexraii/what-is-a-flaky-test-causes-impacts-how-to-deal-with-them-38edd483b76e</li>
<li>Challenges in Testing Large Language Model Based Software - arXiv, https://arxiv.org/html/2503.00481v1</li>
<li>Automated Discovery of Test Oracles for Database Management, https://arxiv.org/html/2510.06663v1</li>
<li>Metamorphic Testing of Web Application Using Playwright - Doria, https://www.doria.fi/bitstream/handle/10024/192757/zahid_md_hasibul_haque.pdf?sequence=2&amp;isAllowed=y</li>
<li>Establishing the Causal Foundations of Metamorphic Testing, https://etheses.whiterose.ac.uk/id/eprint/34458/1/PhD_thesis_andrew_clark_minor_corrections.pdf</li>
<li>Towards Generating Executable Metamorphic Relations … - ORBilu, https://orbilu.uni.lu/handle/10993/61576</li>
<li>10 Essential Practices for Testing AI Systems in 2025 - Testmo, https://www.testmo.com/blog/10-essential-practices-for-testing-ai-systems-in-2025/</li>
<li>Differential Testing Overview - Emergent Mind, https://www.emergentmind.com/topics/differential-testing</li>
<li>Testing AI/ML Systems: How to Live with Non-Determinism - Medium, https://medium.com/@andyakushenko/testing-ai-ml-systems-how-to-live-with-non-determinism-9861129f1c63</li>
<li>Trust at Scale: Regression Testing Multi-Agent Systems in … - Medium, https://medium.com/@bhargavaparv/trust-at-scale-regression-testing-multi-agent-systems-in-continuous-deployment-environments-99dfcc5872e9</li>
<li>(PDF) Verifiable LLM-Generated Test Oracles - ResearchGate, https://www.researchgate.net/publication/398511554_Verifiable_LLM-Generated_Test_Oracles_Ensuring_Consistency_Correctness_and_Explainability_in_AI-_Assisted_Testing</li>
<li>Beyond Traditional Testing: Addressing the Challenges of Non, https://dev.to/aws/beyond-traditional-testing-addressing-the-challenges-of-non-deterministic-software-583a</li>
<li>A Framework for Continuous Evaluation of LLM Test Generation in, https://arxiv.org/html/2504.18985v1</li>
<li>On Testing Machine Learning Programs - ResearchGate, https://www.researchgate.net/publication/339116752_On_Testing_Machine_Learning_Programs</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>