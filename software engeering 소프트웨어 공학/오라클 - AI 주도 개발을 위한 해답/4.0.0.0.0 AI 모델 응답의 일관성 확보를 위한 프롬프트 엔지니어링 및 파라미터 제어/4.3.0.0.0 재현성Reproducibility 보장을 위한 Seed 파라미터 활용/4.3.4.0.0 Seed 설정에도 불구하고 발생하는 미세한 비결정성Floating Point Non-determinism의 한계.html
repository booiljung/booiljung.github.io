<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:4.3.4 Seed 설정에도 불구하고 발생하는 미세한 비결정성(Floating Point Non-determinism)의 한계</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>4.3.4 Seed 설정에도 불구하고 발생하는 미세한 비결정성(Floating Point Non-determinism)의 한계</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 4. AI 모델 응답의 일관성 확보를 위한 프롬프트 엔지니어링 및 파라미터 제어</a> / <a href="index.html">4.3 재현성(Reproducibility) 보장을 위한 Seed 파라미터 활용</a> / <span>4.3.4 Seed 설정에도 불구하고 발생하는 미세한 비결정성(Floating Point Non-determinism)의 한계</span></nav>
                </div>
            </header>
            <article>
                <h1>4.3.4 Seed 설정에도 불구하고 발생하는 미세한 비결정성(Floating Point Non-determinism)의 한계</h1>
<h2>1.  서론: 결정론적 통제의 환상과 오라클(Oracle)의 위기</h2>
<p>AI를 활용한 소프트웨어 개발 및 자동화된 테스트 환경에서 가장 이상적이고 필수적인 전제 조건은 ’동일한 입력에 대해 항상 동일하고 예측 가능한 출력을 반환하는 것’이다. 소프트웨어 엔지니어링에서 오라클(Oracle)이란 테스트 대상 시스템의 실행 결과가 참인지 거짓인지, 혹은 올바른 상태로 전이되었는지를 판별하는 절대적인 정답지이자 검증 메커니즘을 의미한다. 전통적인 소프트웨어 테스트 패러다임에서 오라클은 완벽한 결정론(Determinism)을 바탕으로 동작한다. 그러나 대규모 언어 모델(LLM)을 코드 생성기나 테스트 평가자(LLM-as-a-Judge)로 파이프라인에 통합하는 순간, 이 결정론적 기반은 심각한 도전에 직면하게 된다.</p>
<p>개발자들은 LLM의 본질적인 확률론적 특성을 통제하기 위해 API가 제공하는 다양한 매개변수를 조작한다. 대표적으로 생성 온도(Temperature)를 0으로 설정하여 모델이 항상 가장 확률이 높은 토큰만을 선택하는 탐욕적 해독(Greedy Decoding)을 강제하고, 난수 생성기(PRNG)의 시드(Seed) 값을 <code>seed=42</code>와 같이 고정하며, Top-P나 Top-K와 같은 샘플링 공간 축소 기법을 극단으로 제한한다. 이론적으로 신경망은 거대한 수학적 함수일 뿐이므로, 입력 벡터가 동일하고 가중치가 고정되어 있으며 난수 생성기의 초기값이 같다면 출력 벡터 역시 비트 단위(Bit-wise)로 정확히 일치해야 마땅하다.</p>
<p>그러나 현실의 대규모 언어 모델 추론(Inference) 환경, 특히 수천 개의 코어가 병렬로 동작하는 GPU 클러스터 기반의 프로덕션 환경에서는 이러한 모든 통제 수단을 동원했음에도 불구하고 동일한 프롬프트에 대해 완전히 다른 텍스트가 생성되는 ‘비결정적(Non-deterministic)’ 현상이 빈번하게 관찰된다.</p>
<p>이러한 통제 불가능한 비결정성의 근본적인 원인은 샘플링 알고리즘의 확률적 의도에 있는 것이 아니다. 그것은 모델의 가중치를 계산하는 하드웨어 연산의 가장 깊고 은밀한 곳, 바로 ’부동소수점 연산의 비결합법칙(Floating-Point Non-associativity)’에 자리 잡고 있다. 본 절에서는 결정론적 정답지를 제공하는 오라클을 구축하려는 소프트웨어 엔지니어들이 반드시 직시해야 할 부동소수점 연산의 한계와, 최신 하드웨어의 병렬 처리 구조 및 배치(Batch) 최적화 알고리즘이 어떻게 이 미세한 오차를 나비 효과처럼 증폭시켜 소프트웨어 테스트의 신뢰성을 붕괴시키는지 심층적으로 해부한다.</p>
<hr />
<h2>2.  수학적 근본 원인: 부동소수점 연산의 비결합성(Non-associativity)</h2>
<p>컴퓨터 과학에서 무한한 실수를 유한한 비트(Bit) 메모리 내에 표현하기 위해 널리 사용되는 IEEE 754 부동소수점(Floating-Point) 표준은 본질적으로 ’근사치(Approximation)’를 다루는 시스템이다. 부동소수점은 부호(Sign), 지수(Exponent), 가수(Mantissa)의 세 부분으로 구성되며, 이를 통해 과학적 계산에서 요구되는 매우 작은 수부터 천문학적으로 큰 수까지 넓은 범위의 값을 표현하면서도 일정한 유효숫자를 유지할 수 있게 해준다.</p>
<p>그러나 컴퓨터 구조상의 유한한 정밀도(Finite Precision)로 인해, 우리가 기초 수학에서 너무나 당연하게 성립한다고 믿어왔던 덧셈의 결합법칙(Associative Law)이 부동소수점 연산에서는 성립하지 않는다. 즉, 대수학적으로는 완전히 동일해야 할 다음의 수식이 실제 컴퓨팅 환경에서는 거짓(False)이 된다.<br />
<span class="math math-display">
(a + b) + c \neq a + (b + c)
</span></p>
<h3>2.1  스케일 차이에 따른 가수 이동과 정보 손실(Rounding Error)</h3>
<p>이러한 비결합성이 발생하는 핵심 기전은 ’크기(Scale)가 서로 다른 두 부동소수점을 더할 때 발생하는 정보 손실’이다. 부동소수점 덧셈을 수행하기 위해서는 두 수의 지수(Exponent)를 동일하게 맞추는 정렬(Alignment) 과정이 선행되어야 한다. 이때 지수가 작은 수의 가수를 오른쪽으로 시프트(Shift)하여 지수가 큰 쪽에 맞추게 되는데, 가수를 표현할 수 있는 비트 수(정밀도)를 벗어나는 하위 비트들은 반올림 혹은 버림(Rounding/Truncation) 처리되어 영구적으로 소실된다.</p>
<p>예를 들어, 3자리의 가수 정밀도만을 가진 제한된 십진수 부동소수점 시스템을 가정해 보자. 값 <span class="math math-inline">1230</span> (<span class="math math-inline">1.23 \times 10^3</span>)과 <span class="math math-inline">23.4</span> (<span class="math math-inline">2.34 \times 10^1</span>)를 더할 때, 정확한 수학적 합은 <span class="math math-inline">1253.4</span>이다. 하지만 이 시스템은 단 3자리의 유효숫자 정밀도만 유지할 수 있다. 따라서 연산 결과는 <span class="math math-inline">1250</span> (<span class="math math-inline">1.25 \times 10^3</span>)으로 저장되며, 끝자리의 <span class="math math-inline">3.4</span>라는 정보는 우주에서 완전히 사라지게 된다. 역설적이게도 이처럼 연산 과정에서 결합법칙을 위반하고 하위 비트를 버리는 동작 원리 자체가 부동소수점으로 하여금 한정된 메모리 내에서 광범위한 숫자를 다룰 수 있게 해주는 원동력이다.</p>
<h3>2.2  연산 순서가 결과에 미치는 치명적 영향</h3>
<p>단순한 정보 손실 자체보다 AI 추론에서 훨씬 더 치명적인 문제는 ’덧셈을 수행하는 순서(Order of Operations)’에 따라 소실되는 정보의 양과 최종 누적 결과값이 완전히 달라진다는 점이다. 수십억 개의 매개변수와 활성화(Activation) 값을 처리하는 LLM의 어텐션(Attention) 및 다층 퍼셉트론(MLP) 계층에서는 일반적인 행렬 곱셈(GEMM)을 통해 수백만 번의 곱셈과 덧셈이 연속적으로 일어난다.</p>
<p>이때 큰 수에 작은 수를 개별적으로 차례차례 더할 때와, 작은 수들을 먼저 묶어서 더한 뒤 그 합을 큰 수에 더할 때 발생하는 정밀도 차이는 명확하다. 동등한 값들을 더하더라도 연산 트리 구조가 좌측 편향인지 우측 편향인지에 따라 중간 과정에서 버려지는 유효숫자가 달라진다. 작은 수들을 먼저 합산하면 그들 사이의 지수 차이가 크지 않아 정보 손실 없이 온전한 덩어리(합)가 형성되고, 이를 나중에 큰 수와 더하면 하위 비트 소실이 최소화된다. 반면 큰 수에 작은 수를 하나씩 더해 나갈 경우, 매 덧셈마다 지수 정렬이 발생하여 작은 수들이 가진 하위 정보가 지속적으로 깎여나간다.</p>
<p>극단적인 실험 사례를 살펴보면, 크기 차이가 심한 부동소수점들로 구성된 단일 배열의 요소들을 단순히 어떤 순서로 합산하느냐에 따라 최대 102개의 서로 다른 고유한 결과값이 도출된 바 있다. 신경망의 로짓(Logit)을 계산하는 과정은 결국 이러한 수많은 부동소수점들의 누적합(Reduction)이므로, 연산 순서의 아주 미세한 변동만으로도 최종 로짓 값에 소수점 아래 미세한 차이(Drift)를 만들어낸다.</p>
<hr />
<h2>3.  하드웨어 및 인프라 수준의 비결정성 유발 요인</h2>
<p>부동소수점의 비결합성이 실제 모델의 비결정적 출력으로 발현되기 위한 필요조건은 바로 ’연산 순서의 동적 변경’이다. 단일 스레드(Thread)를 사용하는 고전적인 직렬 컴퓨팅 환경에서는 코드가 항상 동일한 순서로 루프를 돌며 누적합을 계산하므로 비결정성이 개입할 여지가 없다. 그러나 수천 개의 코어를 활용하여 대규모 병렬 처리를 수행하는 최신 GPU 및 AI 가속기 환경에서는 처리 속도(Throughput) 극대화를 위한 아키텍처적 특성으로 인해 연산 순서가 매번 달라질 수 있다.</p>
<h3>3.1  병렬 리덕션(Parallel Reduction)과 원자적 연산(Atomic Operations)의 한계</h3>
<p>LLM의 순전파(Forward Pass) 과정, 특히 어텐션 스코어의 소프트맥스 분모를 구하거나 정규화(LayerNorm, RMSNorm) 계층을 통과할 때는 거대한 텐서의 요소들을 하나로 합치는 병렬 리덕션(Parallel Reduction) 연산이 필수적으로 요구된다. GPU는 수많은 스트리밍 멀티프로세서(SM)와 스레드 블록에 이 리덕션 연산을 분배하여 동시에 처리한다.</p>
<p>과거 딥러닝 초창기에는 병렬 연산의 결과물(부분합)을 하나의 전역 메모리 주소에 누적할 때 ’원자적 덧셈(Atomic Add)’을 널리 사용했다. 원자적 덧셈은 다수의 스레드가 동일한 메모리에 동시 접근할 때 데이터 충돌을 막아주지만, 어떤 스레드가 먼저 연산을 끝내고 메모리에 접근할지에 대한 ’순서’는 전혀 보장하지 않는다. 시스템의 온도, 메모리 대역폭의 미세한 점유 상태, 네트워크 지연 등에 의해 스레드 도착 순서가 매번 무작위로 뒤바뀌는 전형적인 레이스 컨디션(Race Condition)이 발생한다. 원자적 덧셈을 통과하는 부동소수점들은 매 실행(Run)마다 덧셈 순서가 난수처럼 섞이게 되고, 앞서 설명한 비결합법칙에 의해 최종 리덕션 결과값이 매번 달라진다.</p>
<p>최근의 고도로 최적화된 LLM 추론 엔진(vLLM, TensorRT-LLM 등)은 이러한 극단적 비결정성을 피하기 위해 순방향 패스에서 원자적 연산의 사용을 최대한 배제하고, 결정론적 병합 순서를 보장하는 트리 리덕션(Tree Reduction) 알고리즘을 사용한다. 그럼에도 불구하고 하드웨어와 프레임워크 수준에서 비결정성은 완전히 소멸되지 않으며, 오히려 더 복잡한 형태의 시스템적 동인으로 모습을 바꾼다.</p>
<h3>3.2  배치 불변성(Batch Invariance)의 역설과 붕괴</h3>
<p>API를 통해 서비스되는 프로덕션 환경의 LLM 시스템은 GPU 자원 활용률과 처리량을 극대화하기 위해 여러 사용자의 요청을 실시간으로 묶어 처리하는 연속 배칭(Continuous Batching)을 수행한다. 흥미로운 사실은, 내가 전송한 프롬프트가 단독으로 처리될 때와 다른 사용자들의 프롬프트와 섞여 대규모 배치(Batch)로 묶여 처리될 때 GPU 내부에서 커널이 연산을 수행하는 차원 분할 방식이 동적으로 변경된다는 점이다.</p>
<p>“Defeating Nondeterminism in LLM Inference” 연구에 따르면, 이러한 현상은 최신 어텐션 최적화 기법인 FlashDecoding이나 Split-KV 구조에서 두드러진다. 컨텍스트 길이가 매우 길어질 경우, 추론 엔진은 키-값(KV) 캐시를 병렬로 축소(Reduction)하기 위해 시퀀스 길이를 여러 GPU 코어(SM)로 분할(Split)하여 할당한다.</p>
<ul>
<li><strong>배치 크기가 1일 때 (단독 요청):</strong> GPU 코어들이 유휴 상태이므로, 이 단일 요청의 거대한 KV 캐시를 4개의 조각으로 쪼개어(Split=4) 4개의 코어에 분배한 뒤 부분합을 병합한다.</li>
<li><strong>배치 크기가 32일 때 (혼잡 상태):</strong> 여러 사용자의 요청이 쏟아져 들어오면 각 코어가 처리해야 할 데이터가 많아진다. 따라서 시스템은 동일한 프롬프트라도 분할 개수를 줄여 2개의 조각으로만 분할(Split=2)하여 연산 효율을 높이려 시도한다.</li>
</ul>
<p>분할 개수(Split Factor)가 달라지면 부분합을 병합하는 리덕션 트리의 구조 자체가 근본적으로 달라지며, 이는 필연적으로 부동소수점 덧셈 순서의 재배열을 초래한다. 즉, 내 요청과는 무관한 다른 사용자의 동시 요청(시스템 부하 상태)이 내 결과값의 부동소수점 오차를 변화시키는 ’배치 의존적 비결정성’을 발생시킨다. 이를 시스템 공학 용어로는 ’배치 불변성(Batch Invariance)의 붕괴’라고 부른다. Seed를 고정하고 Temperature를 0으로 설정하여 샘플링 확률론을 통제하더라도, API 서버 측의 배치 구성에 따라 내 프롬프트의 로짓 값은 매 틱(Tick)마다 미세하게 요동치게 되는 것이다.</p>
<h3>3.3  희소 혼합 전문가 모델(MoE)의 동적 라우팅과 공간적 비결정성</h3>
<p>GPT-4, Mixtral 8x7B, DeepSeek-V3와 같은 최신 거대 모델 구조인 희소 혼합 전문가(Sparse Mixture of Experts, MoE) 모델에서는 비결정성 문제가 단일 밀집(Dense) 모델에 비해 기하급수적으로 복잡해진다. MoE 모델은 모든 매개변수를 활성화하는 대신, 입력된 토큰의 특성에 가장 적합한 특정 ‘전문가(Expert)’ 하위 신경망으로만 토큰을 라우팅(Routing)하여 연산 효율을 극대화한다.</p>
<p>배치 처리 환경에서 MoE 모델의 비결정성은 ’전문가 용량 제한(Expert Capacity Limit)’이라는 독특한 하드웨어 최적화 제약 때문에 발생한다. 각 전문가 네트워크는 배치 처리 시 병목 현상을 막기 위해 한 번에 처리할 수 있는 최대 토큰 개수(Capacity)가 정해져 있다. 만약 특정 배치 내에 포함된 대다수의 토큰들이 우연히 ’전문가 A’를 가장 적합한 네트워크로 지목하여 라우팅 요청이 집중된다고 가정해 보자. 전문가 A의 처리 용량이 초과되면, 모델은 버퍼 오버플로우를 막기 위해 넘쳐난 토큰들을 두 번째로 적합한 ’전문가 B’로 강제 재할당(Rerouting/Dropped)한다.</p>
<p>문제는 내 프롬프트의 토큰이 원래 가야 할 ’전문가 A’에게 무사히 할당될지, 아니면 용량 초과로 밀려나서 ’전문가 B’로 쫓겨날지가 오직 나와 함께 배치에 묶인 ’다른 사용자들의 프롬프트 텍스트 분포’에 의해 결정된다는 것이다. 단일 시퀀스 수준에서는 결정론적이어야 할 연산 궤적이 배치 수준의 역학에 의해 변동되며, 이는 단순한 부동소수점 오차를 넘어 토큰이 통과하는 가중치 행렬 자체가 완전히 달라지는 심각한 의미적 비결정성을 초래한다. OpenAI를 비롯한 클라우드 API 제공자들이 Seed 값을 지원함에도 불구하고 완벽한 재현성을 보장(Guarantee)하지 못하고 “최선의 노력(Best effort)“이라고 명시하는 이유가 바로 이러한 분산 시스템의 동적 부하 분산 및 배치 스케줄링 특성 때문이다.</p>
<hr />
<h2>4.  추론 과정에서의 나비 효과: 면도날 같은 로짓 격차와 토큰 플립(Token Flip)</h2>
<p>부동소수점의 비결합성과 시스템의 동적 배칭이 만들어내는 오차의 크기는 일반적으로 <span class="math math-inline">10^{-4}</span>에서 <span class="math math-inline">10^{-5}</span> 수준의 극히 미세한 값이다. 일반적인 컴퓨터 비전(CV) 모델이나 회귀 분석 소프트웨어라면 이 정도의 노이즈는 결과에 아무런 영향을 미치지 않고 무시될 수 있다. 그러나 대규모 언어 모델은 과거에 자신이 생성한 텍스트를 다시 입력으로 삼아 다음 텍스트를 끊임없이 뱉어내는 ‘자기회귀적(Autoregressive)’ 특성을 지니고 있어, 단 한 번의 오차가 전체 결과물을 파괴적으로 붕괴시키는 나비 효과를 유발한다.</p>
<h3>4.1  소프트맥스와 토큰 플립(Token Flip) 임계점</h3>
<p>LLM이 다음 토큰을 예측할 때, 신경망의 마지막 출력 계층은 어휘 사전(Vocabulary)에 존재하는 수만 개의 단어 각각에 대해 비정규화된 확신도 점수인 로짓(Logit) 벡터를 계산한다. 이 로짓들은 소프트맥스(Softmax) 함수를 거쳐 총합이 1이 되는 확률 분포로 변환된다. 오라클 구축을 위해 결정론을 강제하는 탐욕적 해독(<code>temperature=0</code>) 상황에서는 이 확률 분포 중 단연 1위(Top-1)를 차지한 토큰이 최종 출력으로 선택된다.</p>
<p>만약 Top-1 토큰의 확률이 85%이고 Top-2 토큰의 확률이 5%라면, 부동소수점 오차로 인해 로짓 값이 미세하게 흔들리더라도 1위와 2위의 순위가 바뀔 가능성은 전혀 없다. 그러나 모델이 문맥상 동일하게 유효한 두 가지 표현을 두고 치열하게 고민하는 순간 문제가 발생한다. 예를 들어 “금액“과 “총액“이라는 두 단어의 산출 확률이 각각 45.0001%와 45.0000%로 면도날처럼 얇은 격차(Razor-thin Logit Gaps)를 두고 팽팽하게 맞서고 있는 상황을 가정해 보자.</p>
<p>이러한 임계점(Boundary-crossing event)에 도달했을 때 앞서 설명한 병렬 리덕션 순서 변경이나 배치 분할에 의한 아주 작은 부동소수점 오차가 개입하게 되면, “금액“의 로짓이 미세하게 깎이고 “총액“의 로짓이 미세하게 높아져 순위가 역전된다. 기존 런(Run)에서는 탐욕적 해독에 의해 “금액“이 선택되었지만, 다음 런에서는 “총액“이 선택되는 이 현상을 학계에서는 ’토큰 플립(Token Flip)’이라고 명명한다.</p>
<h3>4.2  사고의 사슬(CoT) 모델에서의 파국적 누적과 논리 발산(Divergence)</h3>
<p>단일 토큰 플립 자체는 의미적으로 큰 차이를 만들지 않을 수도 있다. 하지만 일단 하나의 토큰이 다르게 선택되면, 그 이후 이어지는 수십, 수백 번의 자기회귀적 생성 단계에서 입력 컨텍스트가 달라지게 된다. 오차는 선형적으로 더해지는 것이 아니라 지수적으로 증폭되며 예측 불가능한 방향으로 텍스트의 궤적을 비틀어버린다.</p>
<p>특히 최근 코딩 자동화 및 수학 문제 해결을 위해 오라클로 널리 채택되고 있는 OpenAI의 o1 계열이나 DeepSeek-R1과 같은 추론 중심 모델(Reasoning Model)에서는 이러한 토큰 플립의 파급력이 파국적인 수준으로 커진다. 추론 모델은 최종 정답을 내놓기 전, 짧게는 수백 토큰에서 길게는 수만 토큰에 달하는 내부 추론 과정(Chain-of-Thought, CoT)을 텍스트 공간에 전개한다. 시퀀스가 길어질수록 면도날처럼 좁은 로짓 격차를 가진 분기점을 만날 확률이 높아지며, 추론의 초기 단계에서 발생한 미세한 부동소수점 오차 하나가 논리의 방향성을 완전히 틀어버리게 된다.</p>
<table><thead><tr><th><strong>지표 (Metrics)</strong></th><th><strong>1차 실행 결과 (Run A)</strong></th><th><strong>2차 실행 결과 (Run B)</strong></th><th><strong>결과의 분기</strong></th></tr></thead><tbody>
<tr><td><strong>초기 수식 분석</strong></td><td>“…따라서 변수 <span class="math math-inline">x</span>를 분리하면…”</td><td>“…따라서 변수 <span class="math math-inline">y</span>를 기준으로 묶으면…”</td><td>5번째 토큰에서 Token Flip 발생</td></tr>
<tr><td><strong>중간 논리 전개</strong></td><td>이차 방정식의 근의 공식을 적용</td><td>치환 적분을 통한 우회적 접근 시도</td><td>추론 알고리즘 경로의 완전한 분리</td></tr>
<tr><td><strong>최종 생성 길이</strong></td><td>1,200 Tokens</td><td>10,200 Tokens</td><td>무한 루프에 빠지며 길이 극단적 팽창</td></tr>
<tr><td><strong>오라클 평가 결과</strong></td><td><strong>Pass</strong> (정답 도출)</td><td><strong>Fail</strong> (오답 도출)</td><td>일관성 상실 및 오라클 붕괴</td></tr>
</tbody></table>
<p><em>(표 1. 부동소수점 오차로 인한 토큰 플립이 CoT 추론 궤적에 미치는 연쇄적 나비 효과 예시)</em></p>
<p>NeurIPS 2025에 채택된 연구 “Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning“의 실증적 실험 결과는 결정론적 통제에 대한 환상을 여지없이 깨뜨린다. 해당 연구진이 DeepSeek-R1-Distill-Qwen-7B 모델을 <code>temperature=0</code>의 완벽한 탐욕적 해독 상태로 고정하고 난수 시드를 통제했음에도 불구하고, 단순히 추론에 동원되는 GPU의 개수나 배치 크기(Batch Size)를 변경하는 것만으로 수학 문제 풀이 벤치마크(MATH500, AIME24)의 정답 정확도(Accuracy)에서 최대 9%에 달하는 거대한 변동성(Variance)이 관찰되었다. 더 놀라운 것은 논리 발산으로 인해 생성된 최종 응답의 텍스트 길이 차이가 평균 9,000 토큰에 달하는 끔찍한 극단값을 보였다는 점이다.</p>
<p>이는 프롬프트 엔지니어링이나 Seed 파라미터 제어만으로는 100% 확정적인 정답지를 보장할 수 없으며, 내부의 수학적 토대가 근사치인 이상 AI를 완벽히 결정론적인 소프트웨어 컴포넌트로 취급하는 것은 시스템 설계상의 중대한 결함을 야기할 수 있음을 강력히 시사한다.</p>
<hr />
<h2>5.  수치 정밀도(Numerical Precision) 포맷이 재현성에 미치는 정량적 영향</h2>
<p>부동소수점 비결합성에 의한 누적 오차와 그로 인한 토큰 플립의 빈도는 하드웨어 수준에서 데이터를 저장하고 연산하는 ’수치 정밀도 포맷(Data Type)’에 의해 직접적으로 결정된다. 유효숫자를 저장하는 가수(Mantissa)의 비트(Bit) 수가 적을수록 정보 손실의 규모가 커지고, 결과적으로 추론의 비결정성은 걷잡을 수 없이 증폭된다.</p>
<p>현대 AI 훈련과 추론 파이프라인에서는 제한된 GPU VRAM 대역폭을 절약하고 Tensor Core의 연산 속도를 극대화하기 위해 기존의 32비트 단정밀도(FP32) 대신 16비트 부동소수점 형식(FP16, BF16)을 표준으로 채택하고 있다. 이 효율성의 대가는 재현성(Reproducibility)의 심각한 훼손으로 돌아온다. 각 데이터 포맷이 가진 구조적 한계와 비결정성 상관관계는 다음과 같다.</p>
<h3>5.1  BF16 (Bfloat16): 재현성의 “재앙 구역”</h3>
<p>BF16(Brain Floating Point)은 Google이 딥러닝 워크로드를 위해 고안한 포맷으로, 지수부(Exponent)에 8비트를 할당하여 기존 FP32와 동일한 매우 넓은 수치 표현 범위를 가진다. 오버플로우나 언더플로우를 방지하는 데는 탁월하지만, 가용 비트가 16비트로 제한됨에 따라 가수부(Mantissa)에는 단 7비트만이 할당된다는 치명적인 약점이 있다. 가수 정밀도가 낮다는 것은 앞서 2절에서 설명한 덧셈 시의 지수 정렬(Alignment) 과정에서 하위 비트가 너무 쉽게, 그리고 대량으로 잘려나감(Truncation)을 의미한다. 선행 연구에 따르면, BF16 환경은 하드웨어 토폴로지나 배치 크기 변화에 극도로 취약하여, 논리 모델의 재현성 측면에서 ’재앙 구역(Disaster Zone)’으로 평가받는다. 로짓 확률값의 미세한 변동폭이 너무 커서 확률이 팽팽한 분기점에서 토큰 플립이 쉴 새 없이 일어난다.</p>
<h3>5.2  FP16 (Float16): 타협의 산물</h3>
<p>FP16은 가수부에 10비트, 지수부에 5비트를 할당한다. 지수 범위가 좁아 훈련 중 그래디언트 오버플로우 등의 문제가 발생할 수 있지만, 가수부가 BF16보다 3비트(즉, 정밀도 면에서 <span class="math math-inline">2^3 = 8</span>배) 더 많아 부동소수점 반올림 오차에 대한 저항력이 상대적으로 강하다. 강화학습 기반 미세조정(RLHF) 단계에서 모델이 불안정하게 붕괴하는 현상을 막기 위해 단순히 데이터 타입을 BF16에서 FP16으로 전환하는 것만으로도 수치적 안정성이 확보된다는 싱가포르 국립대학(NUS)의 연구 결과가 이를 뒷받침한다.</p>
<h3>5.3  FP32 (Float32): 결정론에 근접하는 절대적 정밀도</h3>
<p>전통적인 단정밀도 포맷인 FP32는 가수부에 23비트라는 막대한 정밀도를 제공한다. 실험 결과, FP32를 사용한 추론 환경에서는 하드웨어나 배치가 다양하게 변경되어도 부동소수점 누적 오차가 토큰 플립 임계점을 넘는 경우가 극히 드물어, 전체 테스트 샘플 중 발산(Divergence)하는 비율이 2.2% 미만으로 급감했다. 사실상 난수 시드와 프롬프트가 통제된 상황에서 FP32는 “거의 완벽한 수준의 재현성(Near-perfect Reproducibility)“을 제공한다고 볼 수 있다. 그러나 파라미터 크기가 수십~수백 빌리언에 달하는 최신 LLM을 FP32로 메모리에 로드하려면 BF16 대비 정확히 2배의 막대한 VRAM이 필요하며, 대규모 배치를 처리할 때 메모리 대역폭(Memory Bandwidth)의 심각한 병목을 유발하므로 상용 서비스 환경에 그대로 적용하기에는 막대한 비용 문제가 따른다.</p>
<p><strong>부동소수점 정밀도에 따른 LLM 추론 재현성 및 자원 소모 비교</strong></p>
<p><img src="./4.3.4.0.0%20Seed%20%EC%84%A4%EC%A0%95%EC%97%90%EB%8F%84%20%EB%B6%88%EA%B5%AC%ED%95%98%EA%B3%A0%20%EB%B0%9C%EC%83%9D%ED%95%98%EB%8A%94%20%EB%AF%B8%EC%84%B8%ED%95%9C%20%EB%B9%84%EA%B2%B0%EC%A0%95%EC%84%B1Floating%20Point%20Non-determinism%EC%9D%98%20%ED%95%9C%EA%B3%84.assets/image-20260225200155653.jpg" alt="image-20260225200155653" /></p>
<h3>5.4  양자화(Quantization) 모델의 함정과 비결정성의 부활</h3>
<p>일각에서는 VRAM 부족을 해결하기 위해 모델의 가중치를 정수형(Int8, Int4, GGUF 등)으로 변환하는 양자화(Quantization) 기법을 사용하면, 부동소수점이 아닌 정수 연산을 수행하므로 본질적인 오차를 원천적으로 차단하여 결정론을 회복할 수 있다고 오판한다. 실제로 ZKP(영지식 증명) 인퍼런스 시스템을 구축하려던 Ingonyama 팀 역시 정수 양자화가 재현성 문제를 해결할 것이라 가정하고 대규모 실험을 진행했다.</p>
<p>그러나 양자화 단독으로는 재현성을 보장할 수 없다는 비극적인 현실이 드러났다. 그 이유는 크게 두 가지의 ’불완전성’에 기인한다. 첫째, 불완전한 양자화 커버리지(Incomplete Quantization)다. 정밀도 손실에 극도로 민감한 1차원 정규화 계층(LayerNorm, RMSNorm)이나 Softmax 연산은 양자화를 적용할 경우 정확도가 심각하게 붕괴되므로, 이들은 양자화 과정에서 제외되어 여전히 부동소수점 상태(FP16/FP32)로 연산을 수행한다. 파이프라인 중간에 섞여 있는 이 부동소수점 계층들이 비결정성을 여지없이 부활시킨다. 둘째, 런타임 역양자화(Runtime Dequantization)로 인한 정밀도 환원이다. 최신 GPU의 텐서 코어(Tensor Core) 구조상 행렬 곱셈 연산 시 효율성을 극대화하기 위해, 메모리에서 가중치를 읽어 들일 때는 양자화된 정수(Int8) 형태로 로드하더라도 실제 계산기(ALU)에 들어가는 순간에는 부동소수점(FP16 또는 FP32) 포맷으로 역양자화하여 연산을 수행하는 아키텍처가 널리 쓰인다. 결국 거대한 행렬 누적 연산 파이프라인 내부 어디선가 필연적으로 부동소수점 연산이 개입하게 되고, 정수 연산이 가져다줄 것으로 기대했던 완벽한 결합법칙은 신기루처럼 사라지게 된다.</p>
<hr />
<h2>6.  AI 기반 소프트웨어 테스트와 결정론적 오라클에 미치는 치명적 파급 효과</h2>
<p>결정론적 알고리즘과 엄격한 유닛 테스트 체계에 익숙한 소프트웨어 개발자들에게 LLM의 본질적인 비결정성은 단순한 ’기술적 버그’를 넘어 시스템의 신뢰성과 프로세스 전체를 근본적으로 뒤흔드는 통제 불가능한 위험 요소다. 특히 본 서적의 핵심 주제인 ’AI를 활용한 자동 검증 오라클(Oracle)’이나 비즈니스 로직 테스트 파이프라인을 구축할 때 그 치명적인 파급 효과가 가장 선명하게 드러난다.</p>
<h3>6.1  CI/CD 파이프라인의 플래키 테스트(Flaky Test) 양산과 신뢰성 하락</h3>
<p>현대 소프트웨어 공학의 심장인 지속적 통합 및 지속적 배포(CI/CD) 파이프라인에서는 ’동일한 코드를 동일한 입력으로 테스트하면 반드시 같은 상태(Pass 또는 Fail)가 반환되어야 한다’는 격언이 불문율로 작용한다. 그러나 평가용 오라클(LLM-as-a-Judge)이나 코드 생성 AI를 이러한 파이프라인의 단언문(Assertion) 혹은 로직 생성 주체로 통합할 경우, 부동소수점 오차에 의한 비결정성으로 인해 어제 커밋에서는 통과(Pass)했던 테스트가 동일한 코드임에도 오늘 빌드에서는 실패(Fail)하고, 아무 수정 없이 단순히 재실행(Retry)하면 다시 통과하는 악몽 같은 현상이 발생한다. 이를 소프트웨어 테스팅 분야에서는 플래키 테스트(Flaky Test)라고 부른다.</p>
<p>구글(Google), 메타(Meta), 마이크로소프트, SAP 등 글로벌 거대 IT 기업들의 연구에 따르면, 전통적인 유닛 테스트 환경에서도 동시성(Concurrency) 문제나 외부 API 네트워크 지연 등으로 인해 플래키 테스트가 발생하지만, LLM을 활용한 테스트 자동화 체계에서는 그 발생 비율과 원인 추적의 난이도가 현저히 상승한다. 특히 SAP HANA나 MySQL과 같은 복잡한 데이터베이스 환경에 대해 LLM이 생성한 테스트 케이스를 분석한 연구를 보면, LLM은 정렬되지 않은 컬렉션(Unordered Collection)의 응답 순서에 의존하는 등 잠재적 비결정성을 내포한 테스트 코드를 빈번하게 생성하여 스스로 플래키 테스트를 양산하는 것으로 밝혀졌다.</p>
<p>더 큰 문제는 플래키 테스트가 파이프라인 전체에 미치는 심리적, 프로세스적 비용이다. 오라클이 간헐적으로 오답을 뱉어내면, 개발자는 CI 파이프라인의 빨간불(Fail)이 ‘자신이 최근에 작성한 코드의 실제 논리적 결함’ 때문인지, 아니면 ‘클라우드 API 저편에서 발생한 LLM 로짓의 부동소수점 난수 변동’ 때문인지 도무지 구분할 길이 없다. 거짓 양성(False Positive) 알림에 반복적으로 노출된 개발 조직은 점차 테스트 스위트(Test Suite) 자체에 대한 신뢰를 잃게 되며, 이는 경고 피로(Alert Fatigue)로 이어져 진짜 치명적인 버그가 포함된 코드가 경고를 무시하고 프로덕션 환경으로 버젓이 배포되는 파국적 결과를 초래하게 된다. 오라클의 생명은 ’확정성’에 있으나, 확률론적 가중치 위에 세워진 오라클은 모래성처럼 취약할 수밖에 없다.</p>
<h3>6.2  벤치마크 기반의 회귀 테스트(Regression Test)와 골든 데이터셋 붕괴</h3>
<p>AI 모델의 성능이 과거보다 퇴행하지 않았음을 검증하기 위해 회귀 테스트(Regression Testing)를 수행할 때, 개발팀은 완벽한 정답과 입력값이 맵핑된 골든 데이터셋(Golden Dataset)을 구축하고 이를 기준으로 성능을 측정한다. 이 과정에서도 결정론은 필수적이다. 온도를 0으로 설정하여 무작위 샘플링을 차단했으므로, 골든 데이터셋을 통과시켰을 때 나오는 응답은 언제나 동일해야 한다.</p>
<p>그러나 앞서 수학적 원리에서 살펴보았듯, 평가를 수행하는 날의 클라우드 API 트래픽 상황(배치 크기 변동), 인스턴스에 할당된 GPU의 모델 변경(예: A100에서 H100으로의 라우팅), 혹은 텐서 병렬화(Tensor Parallelism) 노드 수의 변동 등 사용자가 통제할 수 없는 인프라 레이어의 변화만으로도 모델의 정확도는 최대 수 퍼센트까지 요동친다. 이는 “동일한 Seed와 프롬프트를 사용했으니 오늘 측정한 벤치마크 결과는 어제와 동등한 잣대 위에서 평가된 객관적 지표다“라는 핵심 가정 자체를 무효화시킨다. 모델 성능의 하락이 개발팀이 적용한 미세조정(Fine-Tuning)이나 프롬프트 수정의 부작용인지, 아니면 단순히 측정 시점의 하드웨어 부하에 따른 부동소수점 오차인지 명확히 분리(Decoupling)할 수 없는 상황은, 특히 금융, 의료, 항공우주 등 엄격한 감사 및 규제 승인(Certification)이 필요한 안전 필수(Safety-Critical) 소프트웨어 도메인에서 AI 모델을 배포하고 유지보수하는 데 있어 극복하기 매우 힘든 막대한 관리 부채를 양산한다.</p>
<h3>6.3  클라우드 API 생태계의 블랙박스와 시스템 지문(System Fingerprint)의 한계</h3>
<p>OpenAI의 GPT-4나 Anthropic의 Claude 등 닫힌 소스(Closed-source) 기반의 클라우드 API를 오라클로 사용할 경우, 소프트웨어 엔지니어의 통제권 상실은 더욱 뼈아프게 다가온다. API 공급자는 서비스 처리 효율을 높이기 위해 백엔드의 CUDA 커널 버전을 업데이트하거나, 동적 연속 배칭 알고리즘을 최적화하거나, 심지어 사용자 몰래 모델의 가중치를 미세하게 양자화하여 배포하는 등 하위 시스템을 쉴 새 없이 변경한다.</p>
<p>OpenAI는 이러한 백엔드 변경으로 인한 비결정성 발생을 사용자가 감지할 수 있도록 응답 헤더에 <code>system_fingerprint</code>라는 해시값을 포함하여 제공하기 시작했다. 이론적으로는 Seed 값이 고정되어 있고 <code>system_fingerprint</code>가 이전 실행과 동일하다면 대부분 동일한 결정론적 출력이 보장되어야 한다. 하지만 이는 비결정성의 발생 시점을 사후에 인지할 수 있게 해주는 모니터링 수단일 뿐, 근본적인 해결책이 아니다. 만약 <code>system_fingerprint</code>가 예고 없이 변경되었다면, 개발자는 수백 수천 개의 골든 데이터셋을 기반으로 구축된 기존 회귀 테스트 스위트의 정답지(Baseline)가 훼손되었음을 수동으로 감지하고, 새로운 지문에 맞춰 모든 테스트를 재가동하여 기준점을 다시 수립해야 하는 막대한 유지보수 공수를 낭비해야 한다. 이는 끊임없이 변동하는 블랙박스 위에 정적인 오라클 집을 짓는 것과 같다.</p>
<hr />
<h2>7.  미세 비결정성 극복을 위한 소프트웨어 및 시스템 수준의 보정 전략</h2>
<p>부동소수점 비결합성으로 인한 한계를 직시한 최신 AI 시스템 엔지니어링 및 오픈소스 생태계는, 확률론적 모델의 추론 파이프라인 내부 깊숙한 곳에서 인위적으로 결정론(Determinism)을 강제하고 재현성을 회복하기 위한 다양한 아키텍처적 접근법을 제시하고 있다. 완벽한 100% 결정론은 필연적으로 하드웨어 자원의 비효율적 낭비나 추론 속도 하락을 수반하므로, 오라클의 목적과 가용 자원에 맞게 적절한 트레이드오프(Trade-off) 전략을 선택해야 한다.</p>
<h3>7.1  LayerCast: 혼합 정밀도 캐스팅을 통한 수학적 붕괴 방지</h3>
<p>최근 AI 성능 평가 학계에서 재현성 문제를 해결하기 위해 고안된 가장 혁신적이면서도 실용적인 접근법 중 하나가 바로 NeurIPS 2025에서 주목받은 LayerCast (레이어캐스트) 기법이다.</p>
<p>앞서 5절에서 논의했듯, 부동소수점 오차를 최소화하여 완벽한 결정론을 달성하려면 FP32 단정밀도를 사용하는 것이 가장 확실한 방법이다. 그러나 FP32는 모델을 VRAM에 적재할 때 너무 많은 메모리를 소모하여 거대 모델 구동을 불가능하게 만든다. 반면 BF16은 메모리는 절약되지만 토큰 플립을 유발하는 재앙적인 비결정성의 원흉이다.</p>
<p>LayerCast의 핵심 설계 철학은 **“저장은 가볍게(BF16), 연산은 무겁게(FP32)”**이다.</p>
<ol>
<li><strong>메모리 최적화 적재:</strong> 거대한 언어 모델의 모든 파라미터(가중치)는 디스크에서 로드된 후 GPU VRAM의 공간을 가장 적게 차지하는 BF16 형태로 저장된다. 이를 통해 VRAM 사용량 병목을 기존 16비트 모델 수준으로 억제한다.</li>
<li><strong>JIT 기반 동적 업캐스팅(Just-In-Time Upcasting):</strong> 텍스트 생성 추론이 진행되는 동안, 토큰 처리를 위해 수천 번의 행렬 곱셈(Matmul)과 리덕션 연산이 필요한 특정 트랜스포머 계층(Layer)에 도달하는 순간이 온다. 이때 LayerCast는 전체 모델이 아닌 해당 연산에 참여하는 계층의 가중치만을 JIT 컴파일러를 활용하여 즉각적으로 정밀도 높은 FP32 포맷으로 업캐스팅(변환)하여 텐서 코어 레지스터로 올린다.</li>
<li><strong>결정론적 고정밀 연산:</strong> 모든 병렬 리덕션 누적 연산과 어텐션 스코어 합산은 완벽한 23비트 가수 정밀도를 지닌 FP32 환경 내에서 수행된다. 이를 통해 덧셈 순서가 바뀌거나 배치가 달라지더라도 토큰 플립 임계점을 넘나드는 치명적인 부동소수점 오차의 누적을 원천적으로 차단한다.</li>
<li><strong>결과 반환 및 다운캐스팅:</strong> 해당 계층에서의 연산이 완전히 종료되면, 결과값을 다시 가벼운 포맷으로 축소하여 다음 파이프라인으로 전달하고 FP32 가중치 캐시를 비운다.</li>
</ol>
<p>이 기법은 메모리 오버헤드를 BF16을 쓸 때와 거의 유사한 수준(순수 FP32 대비 34% 메모리 절감)으로 억제하면서도, 출력 발산율(Divergence Rate)을 25%에서 3.4% 수준으로 급감시켜 사실상 FP32와 동등한 수준의 재현성 지표(Std@Acc, Div_Index)를 달성하는 놀라운 엔지니어링적 균형을 입증했다. 만약 사내에 온프레미스(On-premise) 형태로 평가 전용 오라클 모델(LLM-as-a-Judge)을 직접 구축할 계획이라면, vLLM과 같은 추론 엔진에 LayerCast 패치를 적용하여 부동소수점 오차의 싹을 자르는 것이 CI/CD 유지보수 관점에서 가장 추천되는 강력한 아키텍처 패턴이다.</p>
<h3>7.2  검증된 추측 해독 (Verified Speculation / LLM-42 시스템)</h3>
<p>시스템 레벨(GPU, 프레임워크)에서 부동소수점 오차를 통제하는 것이 하드웨어 종속적인 부담으로 다가온다면, 알고리즘 파이프라인 구조 자체를 변경하여 오차의 발생 기전 자체를 회피하는 논리적 접근법도 존재한다. 마이크로소프트 연구진이 제안한 ‘LLM-42’ 시스템은 부동소수점 오차가 대개 ‘동일 시퀀스 내부의 절대적 연산 순서’ 때문이 아니라, ‘배치 묶음의 구성(Batch Composition) 변화로 인한 분할(Split) 전략의 변경’ 때문에 발생한다는 점에 정밀하게 착안했다.</p>
<p>LLM-42의 핵심 원리는 모델의 텍스트 생성(Decoding) 파이프라인을 두 단계로 엄격하게 분리하는 ‘해독-검증-롤백(Decode-Verify-Rollback, DVR)’ 프로토콜을 도입하는 것이다.</p>
<ol>
<li><strong>고속 해독(Fast Decode) 단계:</strong> 기존의 일반적인 추론 엔진과 동일하게, 높은 처리량을 내기 위해 배치 환경 속에서 비결정적 커널을 사용하여 연속적인 토큰을 대량으로 빠르게 추측(Speculation) 생성한다. 이 단계에서는 부동소수점 오차로 인한 오류가 섞여 있을 수 있음을 전제로 한다.</li>
<li><strong>형상 고정 검증(Verified by Fixed-Shape) 단계:</strong> 대충 생성된 일정 크기의 토큰 윈도우(예: 32개 또는 64개 토큰 덩어리)를 모은 뒤, 모델은 이를 다시 평가한다. 단, 이번 평가 단계에서는 **배치 사이즈 1과 완전히 동일한 ‘고정된 기준 형상(Fixed-size window)’**을 강제로 적용하여 텐서를 모델에 밀어 넣는다. 입력 텐서의 형상이 고정되어 있으므로, 하드웨어의 커널은 다른 요청의 개입 없이 항상 동일한 리덕션 분할 개수(Split Factor)와 스케줄을 선택하게 된다(위치 불변성, Position-invariant). 따라서 연산 순서가 영원히 변하지 않으며 부동소수점 누적 오차 또한 완벽하게 통제된 결정론적 상태로 계산이 이루어진다.</li>
<li><strong>불일치 롤백(Rollback) 단계:</strong> 1단계에서 고속으로 추측한 토큰 시퀀스와 2단계에서 엄격하게 검증하여 얻어낸 ‘결정론적 참조(Reference) 토큰’ 시퀀스를 비교한다. 만약 부동소수점 오차로 인한 단 하나의 토큰 플립이라도 감지된다면, 그 오답 토큰 이후로 생성된 모든 텍스트의 가지(Branch)를 즉시 폐기(Rollback)하고, 올바르게 검증된 정답 토큰부터 다시 안전하게 생성을 이어나간다.</li>
</ol>
<p>이러한 해독-검증-롤백 구조는 기존처럼 파이프라인 전체에 걸쳐 무겁고 느린 결정론적 연산을 전면 강제하는 대신, 모델이 평소에는 빠르고 저렴하게 동작하면서도 최종 사용자(또는 이를 감시하는 오라클 시스템)에게는 수학적으로 완벽하게 검증 및 통제된 100% 동일한 응답만을 반환하도록 보장하는 대단히 우아하고 실용적인 해결책이다.</p>
<h3>7.3  평가 오라클 파이프라인 구축을 위한 견고화 설계(Robustness Design) 지침</h3>
<p>앞서 논의한 하드웨어 및 알고리즘적 패치 외에도, 오라클 시스템을 직접 연동하고 관리하는 애플리케이션 개발자 관점에서 비결정성 한계를 우회하고 피해를 최소화하기 위해 다음과 같은 아키텍처적 견고화 전략을 필수적으로 수립해야 한다.</p>
<ul>
<li><strong>재시도 및 다수결 기반 앙상블(Retry &amp; Majority Vote Pipeline):</strong> LLM 오라클이 특정 유닛 테스트에 대해 실패(Fail) 혹은 예상치 못한 형식의 값을 반환했을 때, 이를 즉각적으로 최종 결함으로 단정 지어서는 안 된다. 파이프라인 내부에 임시 보류 큐(Queue)를 두고, 실패한 입력에 대해 3~5회 독립적인 재시도(Retry)를 백그라운드에서 수행해야 한다. 이후 다수결(Majority Voting)을 통해 가장 빈번하게 산출된 답변을 오라클의 최종 판별값으로 채택함으로써, 1회성 부동소수점 드리프트로 인한 플래키 테스트 노이즈를 강력하게 필터링할 수 있다.</li>
<li><strong>분포 기반 평가(Distributional Reproducibility)로의 패러다임 전환:</strong> AI 시대의 오라클 평가는 “결과값이 정확히 A인가?“라는 비트 단위의 결정론적 단언(Bitwise Determinism)에서 벗어나야 한다. 온도를 낮춰도 완벽한 고정이 불가능하다면, 차라리 온도를 약간 개방하여 <span class="math math-inline">N</span>개의 샘플을 생성한 뒤, “정답 A가 상위 95% 신뢰 구간(Pass@K) 내에 안정적으로 분포하는가?“를 묻는 분포적 재현성 및 의미론적 안정성(Semantic Stability) 평가 모델로 테스트 코드를 마이그레이션하는 것이 장기적인 유지보수 관점에서 훨씬 현실적이다.</li>
<li><strong>자가 치유(Self-Healing) 및 강제 구조화 결합:</strong> 토큰 플립으로 인해 오라클이 응답해야 할 JSON 구조가 미세하게 깨지거나 필드명이 누락되는 등 형식적 오류가 발생할 수 있다. 오라클의 출력 결과를 파싱(Parsing)할 때는 정규표현식이나 엄격한 스키마 검사기(JSON Schema Validator)를 통과시키고, 형식이 깨졌을 경우 파이프라인 단에서 자동으로 LLM에게 복구 프롬프트를 재전송하는 자가 치유(Self-healing) 계층을 프록시(Proxy) 형태로 반드시 삽입해야 한다.</li>
</ul>
<hr />
<h2>8.  결론: “비결정성“을 새로운 상수로 받아들이는 마인드셋의 전환</h2>
<p>결정론은 단순히 API 파라미터(<code>seed</code>, <code>temperature</code>)를 몇 개 조작하여 얻어낼 수 있는 가벼운 소프트웨어 계층의 설정값이 아니다. 그것은 부동소수점의 태생적인 비결합성, 연산 코어 간의 치열한 레이스 컨디션, 분산 시스템의 통제 불가능한 네트워크 지연, 그리고 처리량 극대화를 위해 끊임없이 형태를 바꾸는 연속 배칭 및 혼합 전문가(MoE) 동적 라우팅 알고리즘 등 하드웨어부터 인프라 아키텍처 전반에 뿌리내린 태생적 한계가 결합된 거대한 복합 현상이다.</p>
<p>AI 모델을 평가 오라클이나 핵심 비즈니스 로직에 투입하려는 소프트웨어 엔지니어는 더 이상 100% 비트 단위의 정확성을 보장하는 고전적 의미의 ’절대적 오라클’을 기대해서는 안 된다. “Seed 설정에도 불구하고 발생하는 미세한 비결정성“은 AI 시스템의 버그나 오류가 아니라, 근사와 확률을 양분으로 삼아 동작하는 인공신경망의 본질 그 자체다.</p>
<p>따라서 개발자들은 과거의 완벽한 0과 1로 이루어진 확정적 세계의 문법을 내려놓고, 노이즈와 확률적 요동이 끊임없이 발생하는 부동소수점의 거친 바다 위에서 시스템이 스스로 붕괴하지 않고 유연하게 정답을 향해 수렴해 나가도록 설계하는 ‘새로운 형태의 신뢰성 엔지니어링(Reliability Engineering)’ 패러다임을 개척해야만 할 것이다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>An Analysis of LLM Fine-Tuning and Few-Shot Learning for Flaky, https://www.sqrlab.ca/papers/ICST2025_FlakyXbert.pdf</li>
<li>Non-Determinism of “Deterministic” LLM Settings - arXiv, https://arxiv.org/html/2408.04667v4</li>
<li>The Randomness You Didn’t Ask For - Alexander Arvidsson, https://www.arcticdba.se/posts/nondeterminism-part-1/</li>
<li>Defeating Nondeterminism in LLM Inference - Thinking Machines Lab, https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/</li>
<li>Why is deterministic output from LLMs nearly impossible? - Unstract, https://unstract.com/blog/understanding-why-deterministic-output-from-llms-is-nearly-impossible/</li>
<li>Understanding and Mitigating Numerical Sources of … - arXiv.org, https://arxiv.org/html/2506.09501v2</li>
<li>Solving Reproducibility Challenges in Deep Learning and LLMs, https://www.ingonyama.com/oldblogs/solving-reproducibility-challenges-in-deep-learning-and-llms-our-journey</li>
<li>Achieving Consistency and Reproducibility in Large … - AI Mind, https://pub.aimind.so/creating-deterministic-consistent-and-reproducible-text-in-llms-e589ba230d44</li>
<li>Incidental Non-Determinism: When AI Surprises You (and Why), https://qwerky.ai/blog/incidental-non-determinism</li>
<li>Enabling Determinism in LLM Inference with Verified Speculation, https://arxiv.org/html/2601.17768v1</li>
<li>GPUs are deterministic machines, even for floating point. The, https://news.ycombinator.com/item?id=37007906</li>
<li>Defeating Nondeterminism in LLM Inference, The Future is Predictable, https://hackernoon.com/re-defeating-nondeterminism-in-llm-inference-the-future-is-predictable</li>
<li>How to make your completions outputs consistent with the new seed, https://developers.openai.com/cookbook/examples/reproducible_outputs_with_the_seed_parameter/</li>
<li>Seed param and reproducible output do not work - Page 2 - API, https://community.openai.com/t/seed-param-and-reproducible-output-do-not-work/487245?page=2</li>
<li>The Illusion of Determinism: Why “Fixed Seeds” Can’t Save Your, https://medium.com/@zljdanceholic/the-illusion-of-determinism-why-fixed-seeds-cant-save-your-llm-inference-2cbbb4a021b5</li>
<li>(PDF) Give Me FP32 or Give Me Death? Challenges and Solutions, https://www.researchgate.net/publication/392597345_Give_Me_FP32_or_Give_Me_Death_Challenges_and_Solutions_for_Reproducible_Reasoning</li>
<li>NeurIPS 2025 Orals, https://neurips.cc/virtual/2025/loc/mexico-city/events/oral</li>
<li>Give Me FP32 or Give Me Death? Challenges and Solutions … - arXiv, https://arxiv.org/html/2506.09501v1</li>
<li>[Literature Review] Give Me FP32 or Give Me Death? Challenges, https://www.themoonlight.io/en/review/give-me-fp32-or-give-me-death-challenges-and-solutions-for-reproducible-reasoning</li>
<li>Solving Reproducibility Challenges in Deep Learning and LLMs, https://www.ingonyama.com/post/solving-reproducibility-challenges-in-deep-learning-and-llms-our-journey</li>
<li>On the Flakiness of LLM-Generated Tests for Industrial and Open, https://arxiv.org/html/2601.08998v1</li>
<li>Can large language models (LLMs) reliably suggest fixes for flaky, https://graphite.com/guides/llms-flaky-test-fixes</li>
<li>Non-Determinism of “Deterministic” LLM Settings - arXiv, https://arxiv.org/html/2408.04667v5</li>
<li>Impacts of floating-point non-associativity on reproducibility for HPC, https://arxiv.org/html/2408.05148v1</li>
<li>LLM Output Drift: Cross-Provider Validation &amp; Mitigation for Financial, https://www.alphaxiv.org/overview/2511.07585v1</li>
<li>DefTblrTemplate - arXiv, https://arxiv.org/html/2601.07239v1</li>
<li>Understanding and Mitigating Numerical Sources of … - OpenReview, https://openreview.net/forum?id=Q3qAsZAEZw</li>
<li>Enabling Determinism in LLM Inference with Verified Speculation, https://www.arxiv.org/pdf/2601.17768</li>
<li>(PDF) LLM-42: Enabling Determinism in LLM Inference with Verified, https://www.researchgate.net/publication/400083937_LLM-42_Enabling_Determinism_in_LLM_Inference_with_Verified_Speculation</li>
<li>The Orchestration Illusion: When AI Keeps Your Pipeline Green and, https://brijeshdeb.medium.com/the-orchestration-illusion-when-ai-keeps-your-pipeline-green-and-your-risk-red-6cdfce6669c1</li>
<li>(PDF) Stochastic CHAOS: Why Deterministic Inference Kills, and, https://www.researchgate.net/publication/399734004_Stochastic_CHAOS_Why_Deterministic_Inference_Kills_and_Distributional_Variability_Is_the_Heartbeat_of_Artificial_Cognition</li>
<li>How to Test LLM Powered Apps: Managing Flaky Tests - Semaphore, https://semaphore.io/blog/llms-flaky-tests</li>
<li>© 2025 Kaiyao Ke - Darko Marinov’s Research Group at UIUC, https://mir.cs.illinois.edu/marinov/publications/Ke25MS.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>