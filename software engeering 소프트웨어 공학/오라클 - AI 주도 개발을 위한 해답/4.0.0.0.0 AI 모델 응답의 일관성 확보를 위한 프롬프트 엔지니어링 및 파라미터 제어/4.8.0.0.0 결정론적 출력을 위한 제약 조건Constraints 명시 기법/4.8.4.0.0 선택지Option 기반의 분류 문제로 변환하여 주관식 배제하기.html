<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:4.8.4 선택지(Option) 기반의 분류 문제로 변환하여 주관식 배제하기</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>4.8.4 선택지(Option) 기반의 분류 문제로 변환하여 주관식 배제하기</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 4. AI 모델 응답의 일관성 확보를 위한 프롬프트 엔지니어링 및 파라미터 제어</a> / <a href="index.html">4.8 결정론적 출력을 위한 제약 조건(Constraints) 명시 기법</a> / <span>4.8.4 선택지(Option) 기반의 분류 문제로 변환하여 주관식 배제하기</span></nav>
                </div>
            </header>
            <article>
                <h1>4.8.4 선택지(Option) 기반의 분류 문제로 변환하여 주관식 배제하기</h1>
<p>인공지능(AI)을 활용한 소프트웨어 개발 및 테스트 자동화 영역에서 가장 빈번하게 발생하는 시스템 실패의 근본 원인은 거대 언어 모델(LLM)의 출력 결과를 주관적이고 개방형(Open-ended)인 자연어 텍스트 상태로 방치하는 데 있다. 대규모 언어 모델은 본질적으로 확률적(Stochastic) 기계이며, 동일한 프롬프트와 동일한 가중치(Weights)를 가지고 있더라도 매 호출마다 상이한 텍스트를 생성할 가능성을 내포하고 있다. 이러한 비결정성(Nondeterminism)은 CI/CD 파이프라인 내에서 자동화된 테스트 오라클(Test Oracle)을 구축하거나, 엔터프라이즈 환경에서 비즈니스 로직의 정합성을 밀리초 단위로 검증해야 하는 소프트웨어 엔지니어링의 기본 전제와 정면으로 충돌한다.</p>
<p>결정론적 정답지, 즉 신뢰할 수 있는 하드 오라클(Hard Oracle)을 확보하기 위한 가장 강력하고 실용적인 프롬프트 엔지니어링 기법 중 하나는 모델에게 자유로운 텍스트 생성을 허용하는 ’주관식(서술형) 프롬프트’를 철저히 배제하고, 이를 사전에 정의된 ’객관식 선택지(Multiple-Choice Options) 기반의 분류(Classification) 문제’로 강제 변환하는 것이다. 본 절에서는 이러한 변환 기법이 왜 수학적, 확률적으로 유리한지 분석하고, 이를 소프트웨어 테스트 및 평가 시스템에 통합하기 위한 구체적인 방법론, 학술적 근거, 그리고 프로덕션 레벨의 실전 예제를 심도 있게 분석한다.</p>
<h2>1.  생성형(Generative) 패러다임에서 분류형(Discriminative) 패러다임으로의 전환</h2>
<p>전통적인 소프트웨어 테스팅 이론에서 테스트 오라클 <span class="math math-inline">O</span>는 주어진 입력 <span class="math math-inline">i</span>와 관찰된 프로그램의 출력 <span class="math math-inline">o</span>에 대하여 프로그램이 명세(Specification)를 만족하는지 여부를 결정하는 평가 메커니즘으로 정의되며, 수학적으로 <span class="math math-inline">O(i, o) \in \{True, False\}</span> 혹은 사전에 정의된 이산적인 상태 코드를 반환해야 한다. 그러나 LLM에게 “이 소스 코드의 잠재적 문제점을 설명해라” 또는 “다음 사용자의 시스템 오류 로그를 분석해라“와 같은 주관식 프롬프트를 제공할 경우, 반환되는 결과물은 무한한 차원의 자연어 공간(Natural Language Space)에 존재하게 된다.</p>
<p>개방형 프롬프트는 모델로 하여금 무한대에 가까운 어휘의 조합(Sequence of tokens)을 생성하도록 유도한다. 예를 들어 “Python 데이터 처리 스크립트를 작성해라“라는 모호한 명령은 단순한 텍스트 파싱 루프부터 복잡한 병렬 처리 구조를 가진 스크립트까지 예측 불가능한 결과를 낳는다. 반면, 요구사항을 명확히 하고 출력을 특정 범주로 제한하면 모델의 응답은 훨씬 더 예측 가능하고 유용해지며, 결과적으로 파싱(Parsing) 및 검증이 가능한 상태가 된다.</p>
<p>분류 문제로의 변환은 LLM의 작업을 ’다음 토큰 예측(Next-token prediction)’이라는 본연의 확률적 생성 작업에서, ’주어진 유한한 선택지 내에서의 확률 분포 평가 및 선택’으로 축소시키는 고도의 엔지니어링 과정이다. 사용자의 입력이나 시스템의 상태를 판단할 때, LLM이 자유롭게 서술하도록 내버려두는 대신 <span class="math math-inline">N</span>개의 엄격하게 정의된 카테고리 중 하나를 선택하도록 강제해야 한다. 이러한 제약을 가함으로써 소프트웨어 엔지니어는 LLM을 단순한 챗봇이 아닌, 프로그래밍적으로 제어 가능한 ’고도화된 자연어 분류기(Classifier)’로 활용할 수 있게 된다.</p>
<h3>1.1  주관식과 객관식 프롬프트의 기술적 비교 분석</h3>
<p>주관식 생성과 객관식 분류는 시스템 아키텍처 측면에서 근본적인 차이를 지닌다. 다음 표는 두 접근 방식이 소프트웨어 오라클 구축에 미치는 영향을 구체적인 평가 지표와 함께 비교한 것이다.</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>주관식(Open-ended) 프롬프트</strong></th><th><strong>선택지 기반 분류(Classification) 프롬프트</strong></th></tr></thead><tbody>
<tr><td><strong>출력 공간의 차원(Dimensionality)</strong></td><td>무한대 (문맥 길이에 따른 모든 토큰 시퀀스 조합)</td><td>유한함 (사전 정의된 선택지의 수 <span class="math math-inline">N</span>)</td></tr>
<tr><td><strong>소프트웨어적 파싱 및 결과 검증 방식</strong></td><td>의미론적 유사도 검사, 복잡한 정규표현식, 휴리스틱 평가</td><td>단순 문자열 완전 일치(Exact Match) 또는 열거형(Enum) 매핑</td></tr>
<tr><td><strong>비결정성(Stochasticity) 및 환각 통제력</strong></td><td>매우 낮음 (생성 길이가 길어질수록 Hallucination 발생 확률 기하급수적 증가)</td><td>매우 높음 (정해진 소수의 토큰 내에서만 출력 강제 가능)</td></tr>
<tr><td><strong>테스트 오라클(Test Oracle)로서의 적합성</strong></td><td>부적합 (동일 입력에 대한 응답 변동성 높음, 평가 자동화 불가)</td><td>적합 (상태 코드, True/False, 카테고리 등 결정론적 반환 보장)</td></tr>
<tr><td><strong>평가 및 신뢰도 측정 지표</strong></td><td>ROUGE, BLEU, BERTScore (불안정하며 인간의 판단과 불일치 발생)</td><td>Accuracy, Precision, Recall, F1-Score, JSD (명확한 수치적 평가)</td></tr>
<tr><td><strong>시스템 통합 및 지연 시간(Latency)</strong></td><td>높음 (전체 응답을 수신해야 하므로 Time-to-First-Token 및 총 생성 시간 증가)</td><td>낮음 (선택지를 나타내는 소수의 토큰만 생성하면 되므로 리소스 최적화 가능)</td></tr>
</tbody></table>
<p>대규모 언어 모델은 매 단계마다 어휘집(Vocabulary)에 대한 확률 분포를 소프트맥스(Softmax) 스케일링을 통해 계산하고, 이 분포에서 샘플링하여 다음 토큰을 선택한다. 이러한 확률적 메커니즘은 핵 샘플링(Nucleus/Top-p sampling)이나 Top-k 샘플링 알고리즘과 결합될 때 변동성을 더욱 증폭시킨다. 뿐만 아니라, GPU 상에서 수행되는 부동소수점 연산(Floating-point operations)의 특성상 행렬 곱셈 및 리덕션(Reduction) 작업 시 CUDA 커널의 병렬 실행 순서가 완벽하게 동일하지 않기 때문에 발생하는 미세한 로짓(Logit)의 발산(Divergence)이 존재한다.</p>
<p>주관식 생성에서는 수백에서 수천 개의 토큰을 연속적으로 샘플링해야 하므로, 이러한 하드웨어 및 소프트웨어적 무작위성이 누적되어 경로 의존성(Path dependence)을 낳고 최종 출력물의 형태를 완전히 바꿔놓을 수 있다. 반면, 출력 길이를 극도로 제한하는 선택지 기반 분류 문제로 변환하면 누적되는 무작위성을 최소화하고, 시스템이 통제할 수 있는 단일 포인트에서의 의사결정만을 모델에게 요구할 수 있다.</p>
<h2>2.  선택지 기반 프롬프트의 수학적 및 확률적 이점</h2>
<p>선택지 기반 분류 문제로의 변환이 소프트웨어 엔지니어링에서 가지는 가장 강력한 기술적 이점은 출력의 텍스트 자체뿐만 아니라, 해당 선택지를 고를 때 모델이 계산한 로그 확률(<code>logprobs</code>)을 API로부터 직접 추출하여 신뢰도(Confidence) 점수로 활용할 수 있다는 점이다.</p>
<h3>2.1  Logprobs를 활용한 결정론적 신뢰도 및 엔트로피 측정</h3>
<p>단일 프롬프트에 대해 텍스트 응답만을 반환받는 것은 모델의 내부 확신도를 알 수 없는 블랙박스와 같다. 만약 모델에게 “입력된 코드의 상태가 중 어디에 속하는가? 반드시 알파벳 하나만 출력하라.“라고 지시했다고 가정하자. OpenAI API나 vLLM 서빙 엔진 등에서 제공하는 <code>logprobs</code> 매개변수를 활성화하면, 생성된 토큰뿐만 아니라 상위 <span class="math math-inline">K</span>개의 대안 토큰에 대한 확률 분포를 함께 반환받을 수 있다. 이를 통해 다음과 같은 내부 확률 분포 데이터를 소프트웨어적으로 획득할 수 있다.</p>
<ul>
<li><span class="math math-inline">P(A \vert X) = 0.85</span> (로그 확률: -0.162)</li>
<li><span class="math math-inline">P(B \vert X) = 0.10</span> (로그 확률: -2.302)</li>
<li><span class="math math-inline">P(C \vert X) = 0.05</span> (로그 확률: -2.995)</li>
</ul>
<p>이러한 수치적 확률값의 확보는 자동화된 소프트웨어 검증 환경에서 결정적인 역할을 한다. 만약 출력 텍스트가 어제와 오늘 모두 “A“로 동일하게 반환되었다 하더라도, 어제의 확률 분포는 <span class="math math-inline">P(A) = 0.99</span>였고 오늘은 <span class="math math-inline">P(A) = 0.51</span>, <span class="math math-inline">P(B) = 0.49</span>라면, 이는 겉으로는 동일한 테스트 통과(Pass)로 보일지언정 내부적으로는 판단 기준의 심각한 불안정성이 발생했음을 의미한다. 모델 가중치의 미세한 업데이트(분산형 API의 경우)나 입력 데이터의 보이지 않는 분포 변화(Distribution Drift)가 발생했을 때, 텍스트 응답만으로는 이를 탐지할 수 없으나 확률 분포를 모니터링하면 조기에 경고를 발생시킬 수 있다.</p>
<p>이를 정량적으로 모니터링하기 위해 JSD(Jensen-Shannon Divergence)와 같은 대칭적 지표를 사용하여 런투런(Run-to-run) 일관성을 측정해야 한다. JSD는 두 확률 분포 간의 유사성을 0과 1 사이의 값으로 표현하며, 임계값을 초과하는 확률 분포의 이동이 발생할 경우 CI/CD 파이프라인에서 알림을 발생시키는 등 ’회귀 맹점(Regression Blindness)’을 방지하는 모니터링 체계를 구축할 수 있다. JSD의 수식은 다음과 같이 정의된다.<br />
<span class="math math-display">
JSD(P \Vert Q) = \frac{1}{2} D_{KL}(P \Vert M) + \frac{1}{2} D_{KL}(Q \Vert M)
</span><br />
여기서 <span class="math math-inline">M = \frac{1}{2}(P + Q)</span> 이며, <span class="math math-inline">D_{KL}</span>은 두 분포 사이의 쿨백-라이블러 발산(Kullback-Leibler Divergence)을 나타낸다. CI/CD 파이프라인에서는 JSD 값이 특정 임계값(예: 0.05)을 초과할 경우 API의 무증상 업데이트나 프롬프트의 품질 저하가 발생한 것으로 간주하고 파이프라인을 차단(Block)하는 하드 가드레일을 설정해야 한다.</p>
<table><thead><tr><th><strong>JSD 측정 구간</strong></th><th><strong>JSD 수치 범위</strong></th><th><strong>상태 평가</strong></th><th><strong>CI/CD 파이프라인 조치 사항</strong></th></tr></thead><tbody>
<tr><td><strong>안정(Stable)</strong></td><td><span class="math math-inline">0.00 \leq JSD &lt; 0.02</span></td><td>모델 예측의 일관성 매우 높음</td><td>테스트 통과, 자동 배포 승인</td></tr>
<tr><td><strong>주의(Warning)</strong></td><td><span class="math math-inline">0.02 \leq JSD &lt; 0.05</span></td><td>확률 분포의 미세한 이동 감지</td><td>경고 발생, 리뷰어에게 로그 전송 (배포는 유지)</td></tr>
<tr><td><strong>위험(Critical)</strong></td><td><span class="math math-inline">JSD \geq 0.05</span></td><td>모델 판단력의 심각한 회귀 또는 모호성 증폭</td><td>파이프라인 즉시 중단(Halt), 프롬프트 및 데이터셋 재검증 요구</td></tr>
</tbody></table>
<p>이처럼 주관식 텍스트 생성을 객관식 분류로 변환하는 행위는, 자연어 처리 문제를 소프트웨어 공학의 수치적 검증 및 모니터링 영역으로 끌어들이는 핵심적인 아키텍처 패턴이다.</p>
<h2>3.  다중 선택지 질의응답(MCQA) 평가의 일관성과 최신 학술적 통찰</h2>
<p>소프트웨어 시스템에서 LLM을 분류기로 활용할 때, 다중 선택지 질문(Multiple-Choice Question Answering, MCQA) 기법이 가장 널리 사용된다. 그러나 학술 연구들은 이 과정에서 발생할 수 있는 평가의 함정과 일관성(Consistency) 저하 문제를 면밀히 지적하고 있다. 오라클 시스템을 구축할 때 이러한 학술적 통찰을 반영하지 않으면, 잘못된 검증 시스템을 맹신하는 결과를 초래할 수 있다.</p>
<h3>3.1  First Token Probability vs Text Answers의 강건성(Robustness) 비교</h3>
<p>MCQA 태스크에서 모델의 최종 답변을 추출하는 방식은 전통적으로 두 가지로 나뉜다. 첫 번째는 모델이 답변을 생성하기 시작할 때의 ’첫 번째 토큰의 확률(First-token probabilities)’을 확인하여 A, B, C, D 중 가장 높은 확률을 가진 토큰을 정답으로 간주하는 방식이다. 두 번째는 모델이 전체 문장을 생성하도록 허용하고(예: “제시된 선택지 중 정답은 C입니다”), 그 ‘생성된 텍스트(Text answers)’ 내에서 정답을 파싱하는 방식이다.</p>
<p>최근 권위 있는 학회에서 발표된 논문 <em>Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think</em>에 따르면, 인스트럭션 튜닝(Instruction-Tuned)이 고도화된 최신 LLM들의 경우, 단순히 첫 번째 토큰의 확률 분포를 추출하는 방법론이 오히려 모델의 실제 능력을 과소평가하거나 편향성을 띨 수 있음을 증명했다.</p>
<p>해당 연구에서는 선택지의 순서 변경(Option Swap), 철자 오류 삽입(Letter Typos), 단어 순서 변경(Word Swap)과 같은 다양한 프롬프트 교란(Perturbation)을 가했을 때, 첫 번째 토큰의 로그 확률만을 기준으로 평가한 경우보다 모델이 명시적으로 텍스트 답변을 생성하도록 유도한 후 이를 파싱하는 방식이 훨씬 더 높은 강건성(Robustness)을 보여주었다. 구체적으로 불일치 비율이 50%를 초과하는 환경에서도 텍스트 답변이 최첨단 편향 제거(Debiasing) 기법을 적용한 첫 번째 토큰 평가보다 더 일관된 정답을 도출했다.</p>
<p>이러한 발견은 소프트웨어 테스트 파이프라인 설계자에게 중요한 지침을 제공한다. LLM을 오라클로 사용할 때 단순히 로짓(Logit) 값을 비교하는 것에 그치지 않고, 모델이 명확하게 선택지를 포함한 짧은 문자열을 출력하도록 지시한 뒤 이를 정규표현식이나 스키마 기반 파서(Schema-based Parser)로 소프트웨어적으로 추출(Extraction)하는 구조가 실전에서는 더 높은 신뢰성을 보장한다는 것이다.</p>
<h3>3.2  포맷 강제와 Chain-of-Thought (CoT)의 딜레마</h3>
<p>객관식 프롬프트 설계의 또 다른 핵심 쟁점은 논문 <em>Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM Evaluation in Multiple-Choice Question Answering</em>에서 깊이 있게 다루어지는, 포맷 제약(Format constraints)과 모델의 추론(Reasoning) 능력 간의 근본적인 상충 관계(Trade-off)이다.</p>
<p>소프트웨어 시스템이 LLM의 출력을 결정론적으로 쉽게 파싱하기 위해서는 프롬프트에 “오직 A, B, C, D 중 하나의 알파벳만 출력하라. 다른 어떠한 설명도 추가하지 마라“와 같은 극단적으로 엄격한 포맷 제약을 걸어야 한다. 이렇게 하면 출력은 예측 가능해지고 시스템 통합이 쉬워지지만, 모델이 복잡한 문제를 풀기 위해 중간 과정을 서술하는 ’사고의 사슬(Chain-of-Thought, CoT)’을 전개할 공간이 완전히 차단된다.</p>
<p>연구에 따르면, 모델이 자유 형태(Free-form)의 텍스트를 먼저 생성하면서 논리적으로 추론한 뒤 마지막에 정답을 선택하게 할 경우 MMLU 등 복잡한 추론 벤치마크에서의 정확도가 크게 상승한다. 반대로, 소프트웨어는 이 길고 복잡한 텍스트 더미 속에서 최종 선택지를 파싱해내는 데 어려움을 겪으며, 텍스트 생성 도중 발생한 노이즈로 인해 잘못된 답변 추출로 이어질 수 있다. 즉, 추론 능력을 극대화하면 파싱의 결정론이 훼손되고, 파싱의 결정론을 강제하면 모델의 지능이 하락하는 현상이 발생한다.</p>
<h3>3.3  구조화된 다단계 출력 설계에 의한 딜레마 해결</h3>
<p>이러한 상충 관계를 해결하고 주관식을 배제하면서도 추론 능력을 유지하는 엔지니어링적 돌파구는, 출력을 JSON과 같은 엄격한 데이터 포맷으로 강제하면서 ’추론 과정’을 담는 필드와 ’최종 선택지’를 담는 필드를 구조적으로 분리하는 것이다.</p>
<p><strong>[비효율적 프롬프트 (추론 공간 부족, 단순 찍기 유발)]</strong></p>
<p>시스템: 다음 시스템 보안 경고 로그를 보고 원인을 분류하라. (오직 A, B, C 중 하나만 출력할 것)</p>
<p>A: 데이터베이스 연결 오류</p>
<p>B: 비정상적 권한 상승 시도 (보안 위협)</p>
<p>C: 메모리 용량 초과 경고</p>
<p>로그 데이터: [세션 39912 - 사용자 admin - 접근 거부됨: 리소스 제약]</p>
<hr />
<p>시스템: 당신은 엄격한 보안 오라클 시스템입니다. 다음 시스템 보안 경고 로그의 원인을 분석하고 분류하라. 출력은 반드시 아래 제공된 JSON 포맷을 준수해야 하며, JSON 외의 마크다운이나 인사말은 일절 금지한다.</p>
<p>{</p>
<p>“step_by_step_reasoning”: “선택지를 고르기 전, 로그의 핵심 에러 코드와 발생 위치를 1~2문장으로 단계별로 분석한 논리 전개”,</p>
<p>“confidence_score”: 0.0 ~ 1.0 사이의 확신도,</p>
<p>“selected_option”: “A, B, C 중 정확히 하나의 알파벳”</p>
<p>}</p>
<p>[선택지]</p>
<p>A: 데이터베이스 연결 오류</p>
<p>B: 비정상적 권한 상승 시도 (보안 위협)</p>
<p>C: 메모리 용량 초과 경고</p>
<p>로그 데이터: [세션 39912 - 사용자 admin - 접근 거부됨: 리소스 제약]</p>
<p>이 접근법은 <code>step_by_step_reasoning</code> 필드를 통해 모델에게 사고의 사슬(CoT)을 전개할 수 있는 자기회귀적(Autoregressive) 토큰 생성 공간을 제공하여 분석의 정확도를 끌어올린다. 이후 <code>selected_option</code> 필드에서 사전 정의된 열거형(Enum) 문자열만을 뱉어내게 함으로써, 파싱의 결정론과 모델의 추론 성능을 동시에 달성할 수 있다.</p>
<h2>4.  소프트웨어 테스트 오라클로서의 LLM 분류기 활용 지침</h2>
<p>소프트웨어 엔지니어링에서 버그 탐지, 코드 리뷰, 취약점 분석 등의 작업은 고도의 맥락 이해를 요구한다. 최근 LLM을 이용해 단위 테스트(Unit Test)를 생성하거나 테스트 오라클 메커니즘을 자동화하려는 연구가 활발히 진행되고 있다. 오라클의 핵심 목적은 “이 테스트가 실패했는가, 성공했는가?” 또는 “이 시스템 출력이 명세를 만족하는가?“를 판단하는 것이다.</p>
<p>그러나 여러 실증적 연구에 따르면, LLM은 테스트 오라클을 바닥부터 새롭게 ’생성(Generation)’하는 작업보다는, 외부에서 주어진 상태나 코드가 올바른지 ’분류(Classification)’하는 작업에서 훨씬 더 높고 일관된 정확도를 보인다.</p>
<h3>4.1  버그 탐지 및 품질 보증(QA)에서의 범주화 정확도 향상</h3>
<p>버그를 탐지할 때 “이 코드의 버그를 찾아라“라고 열린 질문을 던지면, 모델은 존재하지 않는 취약점을 지어내는 환각(False Positives)을 보이거나, 과도하게 장황한 설명을 늘어놓아 자동화된 CI 파이프라인이 스크립트 레벨에서 이를 해석할 수 없게 만든다.</p>
<p>반면, 코드의 상태를 검증하는 작업을 분류(Classification) 태스크로 구조화하여 LLM에게 제공할 경우, 탐지 정확도가 기하급수적으로 상승한다. 예를 들어, 최신 연구에 따르면 코드의 오류 상태를 분류 문제로 치환하여 평가를 수행한 LLM 기반 방법론은 기존의 수동 접근 방식(78.2%)을 크게 상회하는 89.7%의 버그 탐지 정확도를 기록했다. 또한 이 방식은 87.3%의 정밀도(Precision)와 90.1%의 재현율(Recall)을 달성하여 오탐(False Positive)과 미탐(False Negative) 사이의 이상적인 균형을 증명했다.</p>
<p>더욱 흥미로운 점은, Openia와 같은 LLM 기반 취약점 탐지 모델의 경우, 아무 문제 없는 ’올바른 코드’를 식별하는 것(F1-Score 0.75)보다 ’잘못된 코드(버그가 포함된 코드)’를 식별하는 것(F1-Score 0.82)에서 더 높은 정확도를 보인다는 사실이다. 이는 오류를 분류하기 위해서는 전체 코드의 복잡한 구조적 무결성을 모두 증명할 필요 없이, 단 하나의 확실한 결함 신호(Signal/Pattern)만 포착하면 해당 코드를 ‘오류’ 카테고리로 분류할 수 있기 때문이다. 따라서 프롬프트 엔지니어링 시 코드의 상태를 여러 구체적인 오류 선택지로 세분화하여, 모델이 특정 결함 패턴에 데이터를 쉽게 매핑하도록 유도하는 것이 효과적이다.</p>
<h3>4.2  퓨샷 러닝(Few-Shot Learning)을 통한 분류 패턴의 고정</h3>
<p>분류 문제에서 주관식을 배제하는 효과를 극대화하기 위해서는 퓨샷 러닝(Few-shot Learning)을 통해 모델에게 입력과 정확히 매칭되는 옵션 출력의 패턴을 사전에 노출시켜야 한다. GPT-3 논문인 <em>Language Models are Few-Shot Learners</em>에서 밝혀진 바와 같이, 대규모 언어 모델은 문맥 내(In-context)에 제공된 몇 가지 예제만으로도 새로운 태스크의 규칙을 빠르게 내재화한다.</p>
<p>이 메커니즘은 전통적인 의미의 ’학습(Training)’이라기보다는, 프롬프트에 제공된 일관된 패턴 구조를 인식하고 다음 토큰의 확률 분포를 사용자가 시연한 패턴(Demonstrated pattern)에 맞추어 의도적으로 편향(Shift)시키는 패턴 완성(Pattern completion) 프로세스이다.</p>
<p>소프트웨어 개발자의 관점에서, 퓨샷 프롬프팅을 통한 객관식 분류 지시는 모델에게 “초소형 명세서(Mini-spec)와 단위 테스트 예제“를 동시에 주입하는 것과 같다. 예제가 주어짐으로써 LLM은 불필요한 서술어를 덧붙이는 ’창의성’을 억제하고, 오직 요구된 포맷(예: <code>Category: 3</code>)이라는 결정론적인 정답만을 생성하게 된다.</p>
<h2>5.  실전 예제: 주관적 평가를 결정론적 분류 문제로 치환하는 아키텍처</h2>
<p>이론적 기반을 바탕으로, 실제 소프트웨어 개발 라이프사이클(SDLC) 내에서 주관식을 철저히 배제하고 객관식 분류를 적용하여 성공적인 오라클 파이프라인을 구축한 사례들을 살펴본다.</p>
<h3>5.1  실전 예제 1: AI 코드 리뷰 코멘트의 범주형 자동 태깅 시스템</h3>
<p>코드 리뷰는 본질적으로 리뷰어의 주관이 강하게 개입되는 영역이다. 생성형 AI(GenAI)를 활용하여 풀 리퀘스트(Pull Request, PR) 리뷰를 자동화할 때 주관식 프롬프트를 사용하면, 모델은 매번 다른 어조와 깊이로 리뷰 코멘트를 생성한다. 이렇게 파편화된 자연어 텍스트는 CI/CD 스크립트가 해당 PR을 자동 승인(Approve)해야 할지, 차단(Block)해야 할지 프로그래밍적으로 결정할 수 없게 만든다.</p>
<p>이를 결정론적인 테스트 파이프라인에 통합하기 위해서는 리뷰 결과를 범주형 분류(Categorical Classification) 문제로 변환해야 한다.</p>
<p><strong>[비효율적인 주관식 프롬프트의 한계]</strong></p>
<p>시스템: 다음 PR의 코드 변경 사항을 검토하고, 문제점이 있다면 코멘트를 작성해라.</p>
<p>[코드 내용…]</p>
<p>이 프롬프트는 치명적인 SQL 인젝션 취약점이 있는 코드에 대해서도 “코드가 전반적으로 훌륭하지만, 보안에 유의하면 더 좋겠습니다“라는 모호한 피드백을 생성할 수 있어 자동화된 의사결정에 사용할 수 없다.</p>
<p><strong>[옵션 기반의 범주형 분류 프롬프트 적용]</strong></p>
<p>시스템: 당신은 엄격한 시니어 백엔드 엔지니어이다. 주어진 코드 변경 사항을 분석하고, 식별된 가장 심각한 문제의 유형을 아래의 사전 정의된 카테고리(선택지) 중에서 단 하나만 선택하여 분류하라.</p>
<p>[분류 선택지]</p>
<p>A_CRITICAL_SECURITY: SQL 인젝션, XSS, 하드코딩된 크리덴셜 등 시스템을 위협하는 치명적 보안 취약점</p>
<p>B_LOGIC_ERROR: 비즈니스 로직의 명백한 결함, 엣지 케이스 누락, NullPointerException 발생 가능성</p>
<p>C_PERFORMANCE: 비효율적인 중첩 루프, N+1 데이터베이스 쿼리 등 성능 저하 유발 코드</p>
<p>D_STYLE_AND_MAINTAINABILITY: 네이밍 컨벤션 위반, 과도하게 복잡한 함수, 주석 부족</p>
<p>E_PASS: 발견된 문제가 없으며 메인 브랜치에 머지(Merge)하기에 완벽히 안전함</p>
<p>출력 형식은 반드시 아래의 엄격한 포맷을 준수해야 한다. 다른 어떠한 텍스트도 추가하지 마라.</p>
<p>[코드 내용…]</p>
<p>단일 분류(Single-classification) 태스크로 모델의 역할을 축소시키면, 상대적으로 매개변수가 적은 소형 모델(Small Models)을 사용하더라도 정확도 손실 없이 높은 신뢰성의 결과를 확보할 수 있다. CI 스크립트는 반환된 텍스트에서 단순히 정규표현식을 통해 <code>\</code>를 매칭하고, A나 B 레이블이 검출될 경우 즉시 파이프라인을 실패(Fail) 처리하며, E가 반환될 경우에만 다음 단계로 파이프라인을 진행시키는 명확한 결정론적 제어 흐름(Control Flow)을 구현할 수 있다.</p>
<h3>5.2  실전 예제 2: 비정형 서버 로그 및 고객 문의의 트리아지(Triage) 시스템</h3>
<p>비정형화된 시스템 오류 로그나 자연어로 접수된 고객의 기술 지원 문의를 처리할 때, 이를 해결하는 가장 효율적인 방법은 LLM에게 “이 문제를 어떻게 해결해야 하는지 서술해라“라고 하는 것이 아니라, “이 문제를 어떤 대응 프로토콜/부서로 라우팅할 것인가?“를 묻는 트리아지(Triage) 분류 작업으로 제한하는 것이다.</p>
<p>이러한 분류 프롬프트에 퓨샷 러닝을 결합하면, LLM 기반의 시스템이 룰 베이스(Rule-based) 시스템 이상의 유연성을 가지면서도 결정론적인 라우팅 오라클을 제공할 수 있다.</p>
<p>시스템: 입력된 서버 장애 로그를 분석하여, 적절한 대응 프로토콜을 선택하라. 오직 프로토콜 번호만 출력해야 한다.</p>
<p>[선택지]</p>
<p>1: DB_SCALING_REQUIRED (데이터베이스 커넥션 풀 부족, 데드락 발생 시)</p>
<p>2: CACHE_PURGE (Redis 메모리 경고, 캐시 동기화 실패 시)</p>
<p>3: SECURITY_AUDIT (비정상적 다중 인증 실패, 토큰 위변조 시도 시)</p>
<p>4: IGNORE (단순 Health Check 핑, 알려진 마이너 워닝)</p>
<p>[예제 1]</p>
<p>입력 로그: “ERROR [pool-3-thread-1] HikariPool-1 - Connection is not available, request timed out after 30000ms.”</p>
<p>분류: 1</p>
<p>[예제 2]</p>
<p>입력 로그: “WARN Suspicious JWT signature bypass attempt detected from IP 192.168.x.x”</p>
<p>분류: 3</p>
<p>[실제 처리 대상 로그]</p>
<p>입력 로그: {실시간 시스템 로그 삽입}</p>
<p>분류:</p>
<p>위와 같이 설계된 파이프라인은 복잡한 자연어 생성 과정 없이 즉각적으로 <code>1</code>, <code>2</code>, <code>3</code>, <code>4</code> 중 하나의 상태 코드를 반환하며, 인프라스트럭처 자동화 도구(예: Terraform, Kubernetes Autoscaler)가 이 코드를 트리거로 삼아 후속 조치를 즉각적으로 자동 수행할 수 있는 완벽한 접점을 제공한다.</p>
<h2>6.  다중 에이전트 오케스트레이션과 교차 검증(Triangulation)</h2>
<p>복잡한 엔터프라이즈 소프트웨어 환경에서 단일 LLM 호출에만 의존하여 100% 완벽한 결정론적 오라클을 구축하는 것은 현실적으로 한계가 있다. 아무리 프롬프트를 분류 문제로 좁히고 온도(Temperature) 파라미터를 0으로 고정하더라도, 앞서 언급한 하드웨어의 부동소수점 연산 오차나 API 제공자의 보이지 않는 업데이트로 인해 엣지 케이스에서 일관성이 깨지는 현상이 빈번하게 보고된다.</p>
<p>이러한 근본적인 확률적 한계를 극복하고 분류 문제의 정확도를 결정론에 가깝게 끌어올리기 위한 시스템 엔지니어링적 접근법이 바로 ‘다중 모델 교차 검증(Triangulation)’ 및 오케스트레이션(Orchestration)이다. 주관식 프롬프트 환경에서는 각 모델이 쏟아내는 수백 단어의 문장들을 취합하여 결론을 내는 것이 매우 복잡하지만, 프롬프트를 사전에 정의된 ’객관식 선택지’로 제한해 두었기 때문에 다중 모델 간의 앙상블(Ensemble) 및 다수결 처리 로직을 프로그래밍적으로 구현하기가 매우 직관적이다.</p>
<ol>
<li><strong>다중 서브 에이전트(Sub-Agents) 호출:</strong> 단일 오케스트레이터가 동일한 분류 프롬프트를 서로 다른 시드(Seed) 값, 혹은 완전히 다른 모델 제품군(예: GPT-4o, Claude 3.5, Llama 3)에 병렬로 전송한다.</li>
<li><strong>독립적 객관식 분류 수행:</strong> 각 서브 에이전트는 제공된 다중 선택지 내에서 자신의 가중치와 판단 기준에 따라 하나의 옵션(예: Option B)을 선택한다.</li>
<li><strong>다수결 투표(Voting) 및 확정:</strong> 오케스트레이터 계층에서 여러 에이전트가 반환한 범주형 레이블을 배열(Array) 형태로 취합한다. 단순 다수결(Majority Voting) 방식이나, 각 모델이 반환한 <code>logprobs</code> 값을 가중치(Weight)로 환산하여 결합하는 기법(Weighted Scoring)을 통해 최종 옵션을 수학적으로 확정한다.</li>
</ol>
<p>분류 문제로 변환된 출력은 이산적인 값의 집합이므로, 주관식 텍스트와 달리 프로그래밍적으로 다수결 로직이나 합의 알고리즘을 구현하는 데 아무런 걸림돌이 없다. 여러 에이전트의 의견이 특정 선택지로 수렴할 경우 시스템은 이를 ’결정론적 진실(Ground Truth)’로 수용하며, 만약 에이전트 간의 의견이 극명하게 갈려 JSD 등의 분산 지표가 임계치를 넘을 경우에는 해당 결정을 인간 리뷰어(Human-in-the-loop)에게 위임하는 폴백(Fallback) 메커니즘을 작동시킬 수 있다.</p>
<p>주관식을 배제하고 객관식 선택지로 프롬프트를 재설계하는 것은, 단순히 LLM의 응답을 보기 좋게 정형화하는 것을 넘어선다. 이는 확률적이고 비결정적인 인공지능의 사고방식을 전통적인 소프트웨어 공학의 강력한 분기 처리, 다수결 검증, 상태 제어 메커니즘에 그대로 편입시킬 수 있게 해주는 가장 핵심적이고 견고한 아키텍처적 연결 고리이다. 이를 통해 개발자는 LLM의 환각과 변동성을 제어하고, 미션 크리티컬한 환경에서도 흔들림 없는 자동화된 검증 파이프라인을 구축할 수 있게 된다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Achieving Consistency and Reproducibility in Large Language Models (LLMs) | AI Mind, https://pub.aimind.so/creating-deterministic-consistent-and-reproducible-text-in-llms-e589ba230d44</li>
<li>What Is Prompt Engineering? A Practical Guide for Developers and Teams - Snyk, https://snyk.io/articles/what-is-prompt-engineering-a-practical-guide-for-developers-and-teams/</li>
<li>The Ultimate Guide to Prompt Engineering in 2026 | Lakera – Protecting AI teams that disrupt the world., https://www.lakera.ai/blog/prompt-engineering-guide</li>
<li>Test Oracle Automation: LLM and Hybrid Methods - Emergent Mind, https://www.emergentmind.com/topics/test-oracle-automation</li>
<li>Comparative Analysis of Prompt Strategies for Large Language Models: Single-Task vs. Multitask Prompts - MDPI, https://www.mdpi.com/2079-9292/13/23/4712</li>
<li>BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design, https://arxiv.org/html/2508.21184v2</li>
<li>Activities - Generate Text Completion - UiPath Documentation, https://docs.uipath.com/activities/other/latest/integration-service/uipath-openai-openai-generate-text-completion</li>
<li>OpenAI-Compatible Server - vLLM, https://docs.vllm.ai/en/stable/serving/openai_compatible_server/</li>
<li>GenerationConfig | Generative AI on Vertex AI | Google Cloud Documentation, https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/GenerationConfig</li>
<li>Speculative Decoding - vLLM, https://docs.vllm.ai/en/latest/features/speculative_decoding/</li>
<li>Taming Nondeterminism: How to Achieve Run-to-Run Consistency in GenAI Autograding | by Jane Huang - Medium, https://medium.com/@shujuanhuang/taming-nondeterminism-how-to-achieve-run-to-run-consistency-in-genai-autograding-95a6539d676b</li>
<li>Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question? - arXiv, https://arxiv.org/html/2402.12483v2</li>
<li>[Literature Review] Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM Evaluation in Multiple-Choice Question Answering - Moonlight, https://www.themoonlight.io/en/review/right-answer-wrong-score-uncovering-the-inconsistencies-of-llm-evaluation-in-multiple-choice-question-answering</li>
<li>Paper page - Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think, https://huggingface.co/papers/2404.08382</li>
<li>Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think - arXiv, https://arxiv.org/html/2404.08382v1</li>
<li>Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think | OpenReview, https://openreview.net/forum?id=qHdSA85GyZ</li>
<li>Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM Evaluation in Multiple-Choice Question Answering - ACL Anthology, https://aclanthology.org/2025.findings-acl.950.pdf</li>
<li>[2503.14996] Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM Evaluation in Multiple-Choice Question Answering - arXiv.org, https://arxiv.org/abs/2503.14996</li>
<li>[2601.05542] Understanding LLM-Driven Test Oracle Generation - arXiv, https://arxiv.org/abs/2601.05542</li>
<li>Understanding LLM-Driven Test Oracle Generation - arXiv, https://arxiv.org/html/2601.05542v1</li>
<li>Large Language Models for Unit Test Generation: Achievements, Challenges, and Opportunities - arXiv, https://arxiv.org/html/2511.21382v2</li>
<li>Do LLMs generate test oracles that capture the actual or the expected program behaviour?, https://arxiv.org/html/2410.21136v1</li>
<li>[2410.21136] Do LLMs generate test oracles that capture the actual or the expected program behaviour? - arXiv, https://arxiv.org/abs/2410.21136</li>
<li>Do LLMs generate test oracles that capture the actual or the expected program behaviour?, https://www.researchgate.net/publication/385318406_Do_LLMs_generate_test_oracles_that_capture_the_actual_or_the_expected_program_behaviour</li>
<li>Leveraging Large Language Models (LLMs) for Automated Bug Detection and Resolution in Software Engineering - ResearchGate, https://www.researchgate.net/publication/397025344_Leveraging_Large_Language_Models_LLMs_for_Automated_Bug_Detection_and_Resolution_in_Software_Engineering</li>
<li>Correctness Assessment of Code Generated by Large Language Models Using Internal Representations - arXiv, https://arxiv.org/html/2501.12934v1</li>
<li>Few-Shot Prompting Guide 2026 (with Examples) - Mem0, https://mem0.ai/blog/few-shot-prompting-guide</li>
<li>Self-Correcting Large Language Models: Generation vs. Multiple Choice - arXiv, https://arxiv.org/html/2511.09381v1</li>
<li>Language Models are Few-Shot Learners - NIPS, https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf</li>
<li>The Developer’s Handbook to Prompt Engineering: Strategies for Efficient AI Interaction, https://medium.com/@ablahum/the-developers-handbook-to-prompt-engineering-strategies-for-efficient-ai-interaction-266cd69ff067</li>
<li>Does AI Code Review Lead to Code Changes? A Case Study of GitHub Actions - arXiv, https://arxiv.org/html/2508.18771v1</li>
<li>From Code Review to Prompt Review: How Engineering Teams Should Review Prompts in 2025 | by Deepakreddy | Medium, https://medium.com/@deepakreddy1635/from-code-review-to-prompt-review-how-engineering-teams-should-review-prompts-in-2025-1fcf7b35aa7a</li>
<li>Training a categorical classification example - python - Stack Overflow, https://stackoverflow.com/questions/49062970/training-a-categorical-classification-example</li>
<li>How to make LLM output deterministic? : r/LangChain - Reddit, https://www.reddit.com/r/LangChain/comments/1plbbmu/how_to_make_llm_output_deterministic/</li>
<li>SoK: DARPA’s AI Cyber Challenge (AIxCC): Competition Design, Architectures, and Lessons Learned - arXiv, https://arxiv.org/html/2602.07666v1</li>
<li>Strategies for Managing Prompt Sensitivity and Model Consistency - PromptHub, https://www.prompthub.us/blog/strategies-for-managing-prompt-sensitivity-and-model-consistency-</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>