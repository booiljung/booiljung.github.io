<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:4.2.3 Frequency Penalty와 Presence Penalty가 응답 반복성 및 결정론에 미치는 영향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>4.2.3 Frequency Penalty와 Presence Penalty가 응답 반복성 및 결정론에 미치는 영향</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 4. AI 모델 응답의 일관성 확보를 위한 프롬프트 엔지니어링 및 파라미터 제어</a> / <a href="index.html">4.2 일관성 확보를 위한 모델 하이퍼파라미터(Hyperparameter) 정밀 제어</a> / <span>4.2.3 Frequency Penalty와 Presence Penalty가 응답 반복성 및 결정론에 미치는 영향</span></nav>
                </div>
            </header>
            <article>
                <h1>4.2.3 Frequency Penalty와 Presence Penalty가 응답 반복성 및 결정론에 미치는 영향</h1>
<p>대규모 언어 모델(Large Language Model, 이하 LLM)을 기반으로 하는 소프트웨어 개발 패러다임에서 가장 해결하기 어려운 난제 중 하나는 자기회귀(Autoregressive) 생성 과정에 내재된 텍스트 퇴화(Text Degeneration) 현상이다. 본질적으로 훈련 과정에서 방대한 말뭉치를 바탕으로 최대 우도 추정(Maximum Likelihood Estimation)을 수행하여 최적화된 모델이라 할지라도, 실제 추론(Inference) 단계에서 확률적 디코딩을 단순하게 적용할 경우 모델이 국소 최적점(Local Optima)에 갇혀 동일한 단어, 구문, 혹은 문장 구조를 무한히 반복하는 이른바 리피터(Repeater) 문제에 직면하게 된다. 이러한 무한 루핑(Looping) 현상은 특히 시스템의 결정론(Determinism)을 극대화하기 위해 온도(Temperature) 하이퍼파라미터를 0으로 설정한 탐욕적 디코딩(Greedy Decoding) 환경에서 더욱 빈번하고 치명적으로 발생한다.</p>
<p>이러한 기계적이고 지루한 반복 생성 문제를 통제하고, 모델이 출력하는 텍스트의 어휘적, 개념적 다양성을 강제하기 위해 업계 표준으로 도입된 핵심 하이퍼파라미터가 바로 Frequency Penalty(빈도 페널티)와 Presence Penalty(존재 페널티)이다. 이 두 하이퍼파라미터는 언어 모델이 다음 토큰을 선택하기 직전에 후보 토큰들이 가지는 확률 분포에 수학적 불이익을 가함으로써, 이미 출력된 토큰이 다시 등장할 확률을 인위적으로 억제한다.</p>
<p>그러나 인간 사용자와의 개방형 대화나 창의적인 텍스트 생성 작업에서는 이러한 페널티 기법이 출력의 다양성을 높이는 긍정적인 역할을 수행할 수 있으나, 일관된 정답을 요구하는 인공지능 기반의 소프트웨어 개발 및 데이터 파이프라인에서는 완전히 다른 양상을 띤다. 프로그래밍 언어의 구문이나 JSON 등 구조화된 데이터 포맷은 필연적으로 구조적 토큰의 엄격한 반복을 요구하기 때문이다. 따라서 결정론적 정답지(Deterministic Ground Truth)의 생성을 목표로 하는 환경에서 Frequency Penalty와 Presence Penalty를 잘못 제어하거나 오용할 경우, 반복성은 일시적으로 억제할 수 있을지언정 문법 파괴, 구문 오류, 그리고 환각(Hallucination)에 가까운 비결정적 출력이라는 돌이킬 수 없는 부작용을 초래하게 된다.</p>
<p>본 장에서는 Frequency Penalty와 Presence Penalty의 수학적 작동 원리와 로짓(Logit) 스케일링 메커니즘을 심층적으로 해부한다. 나아가 이 두 하이퍼파라미터가 언어 모델의 응답 반복성 억제와 결정론적 출력 보장 사이에서 어떠한 역학 관계와 딜레마를 형성하는지 분석한다. 최종적으로는 코드 생성 및 정형 데이터 추출 환경에서 이 파라미터들이 유발하는 구문론적 붕괴 현상을 규명하고, 이를 근본적으로 극복하기 위해 페널티 조작에 의존하지 않고 결정론적 정답지를 강제하는 하이브리드 오라클(Oracle) 시스템의 설계 원칙과 실전 예제를 상세히 제시한다.</p>
<h2>1. 트랜스포머 아키텍처 내 로짓(Logit) 스케일링과 페널티의 수학적 제어 매커니즘</h2>
<p>언어 모델이 다음 토큰을 생성하는 메커니즘을 이해하기 위해서는 확률 분포가 형성되는 수학적 단계를 명확히 짚어볼 필요가 있다. 트랜스포머(Transformer) 모델의 최종 선형 계층(Final Linear Layer)은 입력된 컨텍스트를 바탕으로 어휘 사전(Vocabulary) 내에 존재하는 수십만 개의 모든 후보 토큰들에 대해 각각 원시 점수를 산출하는데, 이를 로짓(Logit)이라고 명명한다. 이 로짓 값 자체는 음수일 수도 있고 특정한 상한이 없으며 그 총합이 1이 되지도 않는 정규화되지 않은 가중치 예측값에 불과하다.</p>
<p>이 정규화되지 않은 로짓 벡터는 모델이 최종적으로 토큰을 샘플링하기 위해 소프트맥스(Softmax) 함수를 거쳐 합이 1인 0과 1 사이의 확률 분포로 변환된다. 소프트맥스 함수의 기본적인 수학적 형태는 다음과 같이 정의된다.<br />
<span class="math math-display">
P(x_i) = \frac{e^{l_i}}{\sum_{j=1}^{V} e^{l_j}}
</span><br />
여기서 <span class="math math-inline">P(x_i)</span>는 특정 토큰 <span class="math math-inline">x_i</span>가 다음 토큰으로 샘플링될 확률을 의미하며, <span class="math math-inline">l_i</span>는 해당 토큰이 모델로부터 부여받은 원시 로짓 점수, <span class="math math-inline">V</span>는 전체 어휘 사전의 크기를 나타낸다. 지수 함수(<span class="math math-inline">e^{l_i}</span>)를 사용함으로써 모든 로짓 값은 양수로 변환되며, 가장 높은 로짓을 가진 토큰은 다른 토큰들에 비해 압도적으로 높은 확률 값을 점유하게 된다.</p>
<p>Frequency Penalty와 Presence Penalty가 개입하는 지점은 바로 이 원시 로짓이 소프트맥스 함수를 통과하여 최종 확률로 정규화되기 직전의 단계이다. 많은 문헌에서 이 두 페널티를 단순히 ’확률을 낮추는 매개변수’로 설명하지만, 엄밀한 수학적 관점에서 이들은 확률에 직접 개입하는 것이 아니라 ’정규화 이전의 로짓에 대한 감산(Subtractive) 페널티’로 작동한다. 이 두 하이퍼파라미터는 모델이 텍스트 생성을 시작한 이후부터 현재 시점까지 출력된 모든 토큰의 고유 ID와 출현 횟수를 실시간으로 추적하는 내부적인 딕셔너리 구조를 기반으로 연산된다.</p>
<p>Frequency Penalty와 Presence Penalty가 동시에 활성화되어 있을 때, 특정 토큰 <span class="math math-inline">x_i</span>의 로짓을 수정하는 일반화된 수학식은 다음과 같이 표현할 수 있다.<br />
<span class="math math-display">
l&#39;_i = l_i - (c_i \cdot \alpha_{freq}) - (\min(c_i, 1) \cdot \alpha_{pres})
</span><br />
해당 수식에서 사용된 각 변수의 의미는 모델의 동작을 분석하는 데 결정적인 단서를 제공한다. <span class="math math-inline">l&#39;_i</span>는 모든 페널티가 차감되어 최종적으로 소프트맥스 함수로 전달될 조정된 로짓(Adjusted Logit)이다. <span class="math math-inline">l_i</span>는 모델의 신경망이 예측한 순수한 원시 로짓(Raw Logit)이다. <span class="math math-inline">c_i</span>는 현재 컨텍스트 윈도우(프롬프트 및 현재까지 생성된 텍스트 모두, 혹은 구현에 따라 생성된 텍스트에만 한정) 내에서 토큰 <span class="math math-inline">x_i</span>가 등장한 누적 빈도(Count)이다. <span class="math math-inline">\alpha_{freq}</span>는 개발자가 API를 통해 주입한 Frequency Penalty 계수이며, 일반적으로 -2.0에서 2.0 사이의 실수 값을 가진다. <span class="math math-inline">\alpha_{pres}</span>는 마찬가지로 개발자가 주입한 Presence Penalty 계수이며, 이 역시 -2.0에서 2.0 사이의 값을 취한다.</p>
<p>이 수식 구조가 시사하는 가장 핵심적인 통찰은 이 두 페널티가 곱셈(Multiplicative) 기반의 조작이 아닌 뺄셈(Subtractive) 기반의 조작이라는 점이다. 이와 대비되는 전통적인 Repetition Penalty(반복 페널티)의 경우 원시 로짓을 페널티 계수로 나누는 방식으로 작동(<span class="math math-inline">l_i / \text{penalty}</span>)하기 때문에 모델 간 로짓의 절대적 크기 차이에 따라 효과가 널뛰는 불안정성이 있으나 , 뺄셈 기반의 Frequency 및 Presence Penalty는 누적 빈도수와 페널티 계수의 곱을 로짓에서 직접 차감하므로 그 페널티 효과가 훨씬 더 선형적이고 예측 가능하게 누적된다.</p>
<h3>1.1 동태적 로짓 차감의 실증적 구조 분석</h3>
<p>수식의 첫 번째 페널티 항인 <span class="math math-inline">(c_i \cdot \alpha_{freq})</span>는 Frequency Penalty의 본질을 완벽하게 설명한다. 이 항은 특정 토큰의 누적 출현 횟수(<span class="math math-inline">c_i</span>)에 비례하여 페널티 강도가 지속적으로 증가하는 선형 스케일링(Linear Scaling)의 특성을 갖는다. 다시 말해, 어떤 단어가 한 번 쓰였을 때보다 다섯 번 쓰였을 때 모델이 그 단어를 다시 선택하는 것을 훨씬 더 가혹하게 처벌한다. 이는 문장 내에서 어휘 수준(Lexical level)의 중복성을 배제하고 어휘의 다양성(Text Diversity)을 확보하는 데 강력한 힘을 발휘한다.</p>
<p>예를 들어, <span class="math math-inline">\alpha_{freq}</span>를 0.2로 설정한 상태에서 모델이 ’apple’이라는 토큰을 연속적으로 생성하고자 하는 상황을 가정해보자. ‘apple’ 토큰의 원시 로짓이 매번 100.0으로 동일하게 예측된다고 하더라도, 페널티 연산에 의해 최종 로짓은 출현 빈도에 따라 동태적으로 붕괴하게 된다.</p>
<table><thead><tr><th><strong>누적 출현 횟수 (ci)</strong></th><th><strong>원시 로짓 (li)</strong></th><th><strong>αfreq 설정값</strong></th><th><strong>로짓 감산액 (ci⋅αfreq)</strong></th><th><strong>소프트맥스 투입 로짓 (li′)</strong></th></tr></thead><tbody>
<tr><td>0 (초기 상태)</td><td>100.0</td><td>0.2</td><td><span class="math math-inline">0 \times 0.2 = 0.0</span></td><td>100.0</td></tr>
<tr><td>1회 출현</td><td>100.0</td><td>0.2</td><td><span class="math math-inline">1 \times 0.2 = 0.2</span></td><td>99.8</td></tr>
<tr><td>2회 출현</td><td>100.0</td><td>0.2</td><td><span class="math math-inline">2 \times 0.2 = 0.4</span></td><td>99.6</td></tr>
<tr><td>3회 출현</td><td>100.0</td><td>0.2</td><td><span class="math math-inline">3 \times 0.2 = 0.6</span></td><td>99.4</td></tr>
<tr><td>10회 출현</td><td>100.0</td><td>0.2</td><td><span class="math math-inline">10 \times 0.2 = 2.0</span></td><td>98.0</td></tr>
</tbody></table>
<p>위 표에서 증명되듯, 특정 단어가 지속적으로 반복될 경우 해당 단어의 최종 로짓 값은 점진적으로 추락한다. 로짓 점수 2점의 차이는 지수화되는 소프트맥스 연산의 특성상 최종 확률 분포에서 해당 토큰의 선택 확률을 치명적인 수준으로 낮추는 결과를 낳는다. 결국 모델은 더 이상 ’apple’을 선택할 수 없게 되며, 어쩔 수 없이 로짓이 삭감되지 않은 다른 동의어나 아예 새로운 개념의 단어를 샘플링하도록 강제된다.</p>
<p>반면, 수식의 두 번째 항인 <span class="math math-inline">(\min(c_i, 1) \cdot \alpha_{pres})</span>는 Presence Penalty의 작동 방식을 명시한다. 여기서 수학적 핵심은 출현 횟수(<span class="math math-inline">c_i</span>)와 1 중 작은 값을 취하는 <span class="math math-inline">\min()</span> 함수이다. 이는 곧 토큰이 단 한 번이라도 등장했다면 조건식의 값은 1이 되며, 그 이후 토큰이 10번이 등장하든 100번이 등장하든 추가적인 스케일링 없이 언제나 동일하게 <span class="math math-inline">\alpha_{pres}</span> 값만큼만 일괄적으로 차감(Flat reduction)됨을 의미한다.</p>
<p>이러한 고정된 일괄 감산 메커니즘으로 인해 Presence Penalty는 어휘 단위의 반복 자체를 극도로 억제하기보다는, 모델이 한 번이라도 언급했던 주제(Topic)나 개념(Concept)에서 벗어나 새로운 지식의 영역으로 가지를 뻗도록(Branch into new topics) 독려하는 역할을 한다. Frequency Penalty가 “이미 여러 번 쓴 단어를 또 쓰지 마라“는 강경한 통제라면, Presence Penalty는 “이미 다룬 내용은 그만 이야기하고 새로운 화제를 제시하라“는 지시를 수학적으로 인코딩한 것이라 볼 수 있다.</p>
<p>창의적 글쓰기나 두뇌 점진(Brainstorming) 환경에서는 Presence Penalty를 높임으로써 모델이 이전에 다루지 않았던 광범위한 어휘를 사전에 탐색하도록 만들어 출력의 신선함을 유지할 수 있다. 하지만 이 두 하이퍼파라미터가 가진 ’언어의 구조적 맥락을 전혀 인지하지 못하는 맹목적 차감 방식’은 인공지능을 활용한 소프트웨어 결정론 환경에서는 재앙에 가까운 부작용을 일으킨다.</p>
<h2>2. 결정론 붕괴와 온도(Temperature) 0 환경의 딜레마</h2>
<p>소프트웨어 엔지니어링 파이프라인에 대규모 언어 모델을 통합할 때 가장 최우선적으로 확보해야 하는 가치는 예측 가능성, 즉 결정론(Determinism)이다. 모델의 출력이 동일한 입력에 대해 언제나 완벽히 동일한 결과를 산출해야만 해당 시스템을 기반으로 유닛 테스트(Unit Test)를 작성하고, 연속적 통합/배포(CI/CD) 자동화를 신뢰할 수 있기 때문이다. 다수의 실증 연구 및 산업계의 권고사항에 따르면, 이러한 결정론적 출력을 보장하기 위한 가장 1차적인 조치는 모델의 Temperature 하이퍼파라미터를 0(Zero)으로 설정하고, Top-P 값을 0.0 (또는 시스템에 따라 가장 극단적인 제약 값)으로 조절하는 것이다.</p>
<p>Temperature 0의 환경에서 모델은 로짓 분포를 바탕으로 확률적 샘플링을 수행하지 않고, 오직 가장 로짓이 높은 단 하나의 토큰만을 결정적으로 선택하는 탐욕적 디코딩(Greedy Decoding) 알고리즘을 사용하게 된다. 이론적으로 이 상태에서 모델은 완벽한 시드(Seed) 통제와 결합될 경우 완벽한 결정론적 정답지를 뱉어내야 마땅하다. 그러나 현실의 언어 모델은 Temperature가 0일 때 가장 심각한 수준의 텍스트 퇴화(Text Degeneration) 현상에 노출되는 역설적인 한계를 지니고 있다.</p>
<h3>2.1 퇴화된 반복(Degenerate Repetition)과 리피터 문제</h3>
<p>Holtzman 등의 기념비적인 연구(The Curious Case of Neural Text Degeneration)에 따르면, 최대 우도(Likelihood)에만 기반하여 다음 토큰을 지속적으로 탐색하는 디코딩 전략은 기계 생성 텍스트가 비정상적이고 기이한 반복의 늪에 빠지도록 유도한다. 모델이 특정 시퀀스를 생성하다가 자신도 모르게 확률 공간의 국소 최적점(Local Optima)에 도달하게 되면, 그 이후로는 이전 토큰들과 똑같은 시퀀스가 가장 확률이 높은 것으로 평가되는 수학적 함정에 빠져버린다.</p>
<p>그 결과, 챗봇이나 코드 생성기는 “The dog is barking. The dog is playing. The dog is running.“과 같이 동일한 주어와 통사적 구조를 끊임없이 반복하거나 , JSON 데이터 생성 시 동일한 키-값(Key-Value) 쌍을 토큰 제한(Max Tokens)에 도달할 때까지 무한정 찍어내는 리피터(Repeater) 문제를 일으킨다. 특히 배치 처리로 코드를 해석하거나 대량의 데이터를 파싱하는 파이프라인에서 이러한 반복 스트림이 발생하면 전체 시스템이 응답 대기 상태에 빠지는 교착 상태(Stalling)를 유발하기도 한다.</p>
<h3>2.2 반복 억제를 위한 페널티 도입이 야기하는 2차 부작용</h3>
<p>이러한 Temperature 0 환경에서의 무한 루핑 문제를 타개하기 위해, 소프트웨어 개발자들은 손쉬운 미봉책으로 앞서 설명한 Frequency Penalty와 Presence Penalty를 양수(0.3 ~ 0.8 등)로 활성화하는 전략을 취하곤 한다. 특정 토큰이 반복될 때마다 그 로짓을 차감하여 탐욕적 디코딩이 강제적으로 다른 토큰을 선택하게 만들고, 이로써 루프를 탈출하겠다는 논리이다.</p>
<p>그러나 바로 이 지점에서 결정론적 소프트웨어가 요구하는 엄밀함과 확률적 AI 간의 거대한 충돌이 발생한다. 페널티 계수를 0보다 큰 값으로 활성화하는 행위는 본질적으로 ’동일한 입력에 대한 출력의 불변성(Invariance)’이라는 결정론의 대원칙을 근본적으로 붕괴시킨다.</p>
<ol>
<li><strong>동적 로짓 붕괴로 인한 재현성(Reproducibility) 파괴:</strong> Temperature를 0으로 맞추고 Seed 파라미터를 고정하더라도 , Frequency Penalty가 활성화되어 있으면 생성 텍스트의 맥락(Context) 길이나 이전에 모델이 선택한 사소한 동의어 하나에 의해 현재 시점의 로짓 분포가 완전히 뒤틀리게 된다. 시뮬레이션 환경에서 동일한 프롬프트를 20번 반복 실행(Identical runs)하는 벤치마크 테스트 결과에 따르면, 온도 0 환경일지라도 모델 구조나 API의 부동소수점 정밀도 차이로 인해 미세한 분산이 존재할 수 있는데 , 페널티의 개입은 이러한 불확실성을 증폭시켜 완전히 다른 텍스트 궤적을 만들어낸다. 페널티로 인해 1순위 정답 토큰이 배제되면, 모델은 문맥에 전혀 어울리지 않는 차선책 토큰을 억지로 선택하게 되고 이는 환각(Hallucination) 발현의 트리거로 작용한다.</li>
<li><strong>구문론적 및 문법적 필수 토큰에 대한 무차별적 처벌:</strong> 이것이 소프트웨어 오라클 구축에 있어 가장 치명적인 결함이다. 페널티 알고리즘은 현재 계산 중인 토큰이 언어적으로 중요한 명사인지, 문법적으로 생략 불가능한 관사인지, 혹은 프로그래밍 언어의 제어문인지 그 ’의미’를 전혀 이해하지 못한다. 그저 해시 맵(Hash Map)에 기록된 ‘카운트(Count)’ 값에만 반응하여 기계적으로 로짓을 삭감할 뿐이다. 영어 자연어 생성 과정에서도 이 페널티가 ‘a’, ‘the’, ’and’와 같은 기초적인 기능어(Function words)의 빈도를 억압하여 결국 대상 텍스트의 기초 문법을 비틀어버리는 부작용이 지적되고 있다. 하물며 완벽한 문법적 정합성이 요구되는 프로그래밍 언어나 정형 데이터 포맷에 이 로직이 적용되면 그 결과는 참혹하다.</li>
</ol>
<h2>3. 코드 생성 및 정형 데이터(JSON) 추출 환경에서의 페널티로 인한 구문론적 붕괴</h2>
<p>대규모 언어 모델을 응용하여 자율적으로 소스 코드를 생성하거나, 비정형 문서에서 정형화된 JSON 데이터를 추출하는 파이프라인에서 Frequency 및 Presence Penalty의 작동은 시스템 정합성에 재앙적인 타격을 입힌다. 이는 코드라는 매체 자체가 자연어와 비교할 수 없을 정도로 고도로 체계화된 반복성에 의존하기 때문이다.</p>
<h3>3.1 문법적 토큰 구조의 파편화 메커니즘</h3>
<p>Python 구문을 생성하는 AI 모델의 디코딩 과정을 상상해보자. 일반적인 함수나 루프를 구현할 때, <code>def</code>, <code>return</code>, <code>for</code>, <code>in</code>, <code>if</code>, <code>:</code> 와 같은 문법적 예약어나 키워드들은 수십 줄의 코드 내에서 필연적으로 반복해서 등장해야만 문법이 성립된다. 또한 배열을 순회할 때 임시 변수로 사용하는 <code>i</code>, <code>j</code>, <code>item</code> 등의 명명 규칙(Naming Convention) 역시 지속적인 재사용이 강제된다.</p>
<p>이 환경에서 Frequency Penalty를 단지 0.5로 설정했다고 가정해보자. 코드가 길어지며 반복문이 세 번째로 등장하는 시점에 이르렀을 때, <code>for</code> 토큰과 <code>i</code> 토큰의 누적 출현 횟수에 의해 페널티 수식 <span class="math math-inline">c_i \cdot \alpha_{freq}</span> 값은 비약적으로 상승한다. 결과적으로 모델은 다음 제어문에서 정상적인 <code>for</code>를 사용하지 못하고 로짓이 삭감되지 않은 <code>while</code>로 무리하게 선회하거나, 인덱스 변수 <code>i</code> 대신 문맥에 닿지 않는 임의의 변수명을 선언함으로써 의미론적 무결성(Semantic Integrity)을 스스로 파괴하게 된다.</p>
<p>더욱 심각한 것은 토크나이저(Tokenizer) 수준의 공백 문자나 구두점에 대한 억제 효과다. 모델의 토큰 사전 내에 들여쓰기(Indentation)를 의미하는 띄어쓰기 토큰들이 존재하는데, 페널티는 이들 토큰에도 예외 없이 적용된다. 코드가 길어질수록 들여쓰기 토큰의 로짓이 극단적으로 낮아져, 들여쓰기 규칙(Indentation Rule)을 엄격하게 따지는 Python 등의 언어에서는 곧바로 IndentationError가 발생하는 컴파일 불가 코드가 출력되는 것이다. 최신 논문 “The Lempel-Ziv (LZ) penalty“의 실험 결과에 따르면, 최신 추론 모델(Reasoning Models)에서 이러한 산업 표준 빈도/반복 페널티 기법은 오히려 성능을 악화시키며, 4% 이상의 퇴화된 반복률(Degenerate repetition rates)을 전혀 해결하지 못하는 무능함을 드러냈다.</p>
<p>JSON이나 SQL 생성 시에도 동일한 붕괴가 발생한다. 다량의 레코드를 추출하여 JSON 배열로 반환할 것을 프롬프트로 지시했을 때, 각 레코드를 감싸는 <code>{</code>와 <code>}</code> 기호, 키와 값을 구분하는 <code>:</code>, 배열 요소 간의 <code>,</code> 등은 수백 번씩 호출되어야 마땅하다. 그러나 Frequency Penalty가 조금이라도 개입되면 모델은 일정 개수의 객체를 생성한 직후, <code>}</code> 대신 <code>)</code>를 사용하거나 쉼표(<code>,</code>)를 누락해 버린다. 이는 JSON 파서(Parser) 수준에서 즉각적인 SyntaxError를 일으키며, 후속 데이터 처리 파이프라인 전체를 마비시키는 가장 주된 원인으로 지목된다.</p>
<h3>3.2 Pass@k 지표 및 평가 벤치마크에 미치는 악영향</h3>
<p>학계와 산업계에서 코드 생성 AI의 성능을 정량적으로 평가하기 위해 널리 사용되는 Pass@k (단위 테스트를 통과하는 top-k 후보의 비율) 벤치마크 실험 결과들은 페널티의 부작용을 수학적으로 증명한다. EvalPlus나 HumanEvalFix와 같은 엄격한 벤치마크에서, 코드의 기능적 정확도와 결함 내성을 측정할 때 하이퍼파라미터 조작은 성능 등락에 결정적인 영향을 미친다.</p>
<p>여러 연구 논문들은 코드 생성 태스크에서 하이퍼파라미터를 세팅할 때, 하나의 메서드 구현 내에 동일한 변수가 수십 번 등장해야 하는 구조적 특수성을 감안하여 Frequency Penalty와 Presence Penalty를 반드시 0.0으로 완벽히 통제(Zeroed out)할 것을 강력히 권고한다. 예를 들어 특정 코드 생성 벤치마크 연구에 따르면, Top-P 값을 0.0에 가깝게 유지할 때 유효하지 않은(Invalid) 코드 생성 비율이 49.7%에서 36.3%까지 눈에 띄게 감소하며 실행 정확도가 상승하지만 , 이는 오직 반복 억제 페널티들이 철저히 0.0으로 무력화된 상태에서만 유효한 결과였다. 또한 EGP(End-of-sentence Generation Percentage)나 TR-N, TR-S와 같은 코드 구조 반복 측정 지표를 분석한 연구들은 기존의 일차원적인 단어 빈도 삭감 기법으로는 코드 컴파일 성공률(CCP, Compiler Correctness Percentage)을 온전히 보존하면서 논리적 반복 현상만을 타겟팅하여 제거하는 것이 사실상 불가능함을 실증하고 있다.</p>
<h2>4. 반복 억제와 구조적 강제의 패러독스: 결정론적 정답지 확보의 딜레마</h2>
<p>지금까지의 분석을 종합하면, AI 기반 소프트웨어를 개발하는 엔지니어들은 빠져나갈 수 없는 심각한 패러독스에 직면하게 됨을 알 수 있다.</p>
<p>첫째, 회귀 테스트(Regression Testing)를 수행하고 자동화된 프로세스를 신뢰하기 위해서는 LLM의 출력이 완벽한 결정론적 정답지(Deterministic Output)를 지향해야 하며, 이를 위해 하이퍼파라미터 Temperature는 0.0으로, Top-P는 극단적으로 좁은 샘플링 영역으로 억제해야 한다. 둘째, 그렇게 최적화된 Temperature 0의 환경은 모델의 추론 한계와 맞물려 종종 기이하고 영구적인 문구의 무한 반복(Looping) 현상을 촉발하는 원흉이 된다. 셋째, 이 반복의 굴레를 끊어내고자 프롬프트 상의 Frequency Penalty나 Presence Penalty를 가동하는 순간, 무한 루프는 멈출지 모르나 코드 제어문과 JSON 파싱 기호 등 필수 구조 토큰이 파괴됨으로써 ’구조화된 데이터 추출’이라는 애초의 목적이 산산조각 난다.</p>
<p>이 삼중고의 딜레마를 모델 내부의 단일 파라미터 조작에만 의존하여 해결하려는 시도는 극히 근시안적이다. LLM 자체의 자기회귀적 확률 연산만으로는 반복적인 의미 구조와 반복적인 문법 구조를 분리해낼 능력이 없기 때문이다. 따라서 진정한 결정론적 소프트웨어 개발 환경을 구축하기 위해서는 LLM의 확률적 한계를 시스템 외부에서 인지하고 통제하며 교정하는 거시적 아키텍처, 즉 하이브리드 오라클(Hybrid Oracle) 매커니즘이 필수적으로 결합되어야 한다.</p>
<p>AI의 확률적 텍스트 덩어리를 애플리케이션 계층에서 신뢰하고 사용할 수 있는 단일 정답지(Ground Truth)로 승격시키기 위해서는, 오라클 시스템이 LLM의 토큰 생성 스트림을 실시간으로 감시하며 무한 루핑의 징후를 수학적으로 포착하고, 로짓 페널티라는 독극물을 주입하는 대신 외부 시스템 차원의 구조적 중단과 재검색을 강제해야 한다.</p>
<h2>5. 실전 예제: 구조 인식형 반복 방지 오라클(Structure-Aware Anti-Repetition Oracle)의 설계 및 구현</h2>
<p>다음은 API 응답 기반의 소프트웨어 개발 환경에서 무한 루핑 현상(리피터 문제)을 완벽하게 방어하고, Frequency/Presence Penalty의 부작용을 원천 차단하면서 100% 문법적으로 유효한 결정론적 JSON 정답지를 생성해내는 **‘구조 인식형 반복 방지 오라클’**의 아키텍처 설계 지침과 실전 예제이다.</p>
<p>이 오라클 시스템은 LLM의 확률 계산 계층(Logit layer)에 직접적인 페널티 불이익을 부과하여 문법 생태계를 망가뜨리는 대신, 생성된 텍스트의 청크(Chunk)를 청취하는 소프트웨어 계층(오라클 검증기)을 배치하여 반복성을 구조적으로 식별하고 제어하는 역할을 수행한다.</p>
<h3>5.1 오라클 시스템 설계의 3대 핵심 원칙</h3>
<ol>
<li>
<p><strong>초기 파라미터의 무결성 통제 (Zero-Penalty Policy):</strong> 소프트웨어 파이프라인에서 LLM API를 호출할 때는 예외 없이 Temperature를 0.0으로 유지하고 시드(Seed) 값을 정적으로 고정하여 재현의 기반을 마련한다. 가장 중요한 절대 원칙은 모델에 전달되는 Frequency Penalty 값과 Presence Penalty 값을 반드시 0.0으로 명시적으로 선언하여 고정(Zeroed out)하는 것이다. 이는 모델이 JSON 배열의 괄호(<code>[</code>, <code>]</code>), 객체 구분자(<code>{</code>, <code>}</code>), 속성 콜론(<code>:</code>) 등 문법적 필수 기호를 수만 번 반복해서 자유롭게 출력할 수 있는 오염되지 않은 환경을 제공하기 위함이다.</p>
</li>
<li>
<p><strong>구조적 반복(Structural Repetition)의 실시간 외부 감지:</strong> 오라클은 단순히 응답이 완료될 때까지 대기하는 것이 아니라, 서버-클라이언트 환경에서 스트리밍(Streaming) 방식으로 반환되는 토큰들을 실시간으로 청취한다. 오라클 내부에는 N-gram 모델링 지표(예: TR-N, TR-S )나 경량화된 해싱(Hashing) 알고리즘이 가동된다. 모델이 정상적으로 새로운 레코드를 나열하는 것이 아니라, <code>{"id": 4, "status": "pending"}, {"id": 4, "status": "pending"}</code>과 같이 내용이 완벽히 동일한 데이터 블록을 연속해서 내뱉는 순간을 오라클이 즉각적으로 포착한다.</p>
</li>
<li>
<p><strong>결정론적 개입 및 구조적 중단(Deterministic Intervention):</strong></p>
</li>
</ol>
<p>동일 패턴의 반복 루프가 임계치(예: 연속 3회)를 초과하여 탐지되는 순간, 오라클은 LLM의 토큰 생성을 애플리케이션 레벨에서 즉각 강제 종료시킨다. 이후 많은 개발자들이 범하는 오류처럼 파라미터(Frequency Penalty 등)를 높여 모델에게 복구를 위임하는 행위를 엄격히 금지한다. 그 대신, 반복이 시작되기 직전까지 생성된 정상적이고 완벽한 JSON 데이터 구간만을 잘라내어 배열을 강제로 닫고 정답지(Ground Truth)로 확정 짓는다. 혹은, 추출이 덜 끝났다면 시스템 프롬프트(System Prompt) 내에 제외할 요소에 대한 명시적인 논리적 제약(Constraints)을 새롭게 주입하여 후속 처리를 트리거함으로써 결정론의 끈을 이어간다.</p>
<h3>5.2 대용량 비즈니스 문서 기반의 주문 데이터 추출 오라클 구성</h3>
<p>다양하고 복잡한 비즈니스 송장(Invoice) 스캔 문서의 OCR 텍스트를 입력받아, 그 안에 포함된 구매 품목(Order Items) 리스트 전체를 정합성이 보장된 JSON 배열 형식으로 완벽하게 추출해야 하는 AI 파이프라인을 구축한다고 가정해보자. 이 작업에서 Temperature 0의 탐욕적 디코딩은 필수적이나, 문서의 구조적 복잡성으로 인해 모델은 두 번째 품목을 해석한 후 길을 잃고 두 번째 품목을 수십 번 연속해서 출력하는 환각 루프에 빠질 위험이 존재한다. 이때 오라클 시스템 기반의 소프트웨어 로직은 다음과 같이 작동하여 결정론을 구출해낸다.</p>
<p><strong>[단계 1] 결정론적 기반의 LLM 1차 호출</strong></p>
<p>시스템은 먼저 철저하게 외부 요인을 차단한 결정론적인 환경 세팅을 통해 LLM에게 첫 번째 추출 명령을 송출한다. 페널티는 완전히 배제된다.</p>
<ul>
<li><strong>System Prompt:</strong> “입력된 송장 문서에서 모든 주문 품목을 JSON 배열로 추출하라. 정확히 주어진 스키마 형식을 준수하라.”</li>
<li><strong>Hyperparameters:</strong></li>
<li><code>Temperature: 0.0</code></li>
<li><code>Seed: 42351</code> (재현성 보장을 위한 고정 시드)</li>
<li><code>Top-P: 0.0</code> (가장 확률이 높은 단일 경로만 고려)</li>
<li><code>Frequency_Penalty: 0.0</code> (JSON 토큰 무결성 보장)</li>
<li><code>Presence_Penalty: 0.0</code> (새로운 토픽 발산 방지)</li>
<li><code>Response_Format: { "type": "json_object" }</code></li>
</ul>
<p><strong>[단계 2] 스트림 파싱 및 실시간 해시 감시 (오라클 검증 로직)</strong></p>
<p>AI 모델이 JSON 텍스트를 스트리밍하여 전송하기 시작하면, 오라클 시스템은 Python 혹은 Node.js 기반의 스트리밍 파서(Streaming Parser) 프로세스를 백그라운드에서 구동한다. JSON 배열 내부의 단일 객체(Object) 단위인 중괄호 <code>{... }</code> 묶음이 하나 완성될 때마다, 그 문자열 내용 전체를 해시(SHA-256 등) 처리하여 오라클 내부 메모리의 감시 세트(Set)에 저장한다.</p>
<p>이러한 로직을 의사 코드(Pseudo Code) 형태로 추상화하면 다음과 같다.</p>
<pre><code class="language-Python">def oracle_stream_validator(llm_stream_response):
    extracted_items =
    seen_hashes = set()
    consecutive_duplicates = 0

    # LLM으로부터 쏟아지는 토큰 스트림을 순차적으로 청취
    for chunk in llm_stream_response:
        # 누적된 스트림 내에서 JSON 객체가 온전히 하나 닫혔는지 파싱 확인
        if is_complete_json_object(chunk):
            item_hash = calculate_hash(chunk)
            
            # 오라클: 구조적 루핑의 징후를 수학적으로 비교 감지
            if item_hash in seen_hashes:
                consecutive_duplicates += 1
                # 무의미한 동일 객체 출력이 연속 3회 이상 감지되면 퇴화 루프로 확정
                if consecutive_duplicates &gt;= 3:
                    trigger_oracle_intervention()
                    break # 더 이상의 생성 스트림 수신을 즉시 강제 중단
            else:
                # 정상적인 새로운 데이터 객체인 경우
                seen_hashes.add(item_hash)
                extracted_items.append(chunk)
                consecutive_duplicates = 0 # 연속 카운트 초기화

    # 중단된 시점까지의 정상 수집본만을 JSON 배열로 강제 패키징하여 반환
    return validate_against_schema_and_seal(extracted_items)
</code></pre>
<p><strong>[단계 3] 오라클 개입 및 부분 정답지(Partial Ground Truth) 확보를 통한 탈출</strong></p>
<p>만약 모델이 자체적인 추론 한계로 인하여 세 번 연속 완벽히 동일한 송장 품목 객체를 추출했다면, 이는 모델의 확률 공간이 붕괴된 것이다. 이때 오라클 검증기의 <code>trigger_oracle_intervention</code> 함수가 즉각 발동된다.</p>
<p>중요한 점은 오라클이 모델의 퇴화를 발견했다고 해서 무책임하게 Frequency Penalty 값을 1.0이나 2.0으로 높여서 재요청하는 도박을 하지 않는다는 것이다. 앞서 방대한 논의를 거쳤듯, 모델에게 빈도 페널티를 부여하는 순간 그 다음 반환되는 응답에서는 여지없이 JSON 배열의 괄호가 누락되거나 이스케이프 문자가 깨지는 등 구문 자체가 파손되는 2차 오류가 반드시 발생하게 된다.</p>
<p>대신 오라클은 다음과 같이 구조적이고 명시적인 통제 수단을 통해 결정론을 꿋꿋하게 방어해낸다.</p>
<ul>
<li>반복 루프가 시작되기 직전까지 추출된 정상적인 JSON 데이터 객체들만을 잘라내어 배열(Array) 괄호를 프로그램 코드 단에서 닫아버린다.</li>
<li>즉, 모델이 겪는 맹목적 무한 루핑 현상을 모델의 기능 상실로 보지 않고, 역설적이게도 “더 이상 추출할 의미 있는 새 데이터가 없으므로 동일 지점에 갇혔다“는 암묵적인 추출 완료 신호(End of Extraction Signal)로 치환하여 해석하는 것이다.</li>
<li>결과적으로 오라클은 LLM의 한계를 외부에서 완벽하게 보정하여 100% 구문 오류가 없는 결정론적 정답지(Ground Truth)를 완성해 다음 파이프라인으로 안전하게 인계한다.</li>
</ul>
<p>필요에 따라 문서가 너무 길어 후속 처리가 더 필요하다고 판단되는 경우, 오라클은 페널티를 수정하는 대신 프롬프트 컨텍스트를 동적으로 재구성한다. 즉, <code>System Prompt = "지금까지 추출한 주문 품목 ID인 'A102', 'B304'의 내용은 완전히 배제하고, 그 다음 라인부터 순차적으로 남은 품목만을 다시 추출하라."</code>라는 논리적이고 명시적인 지시어 제약(Constraints)을 삽입하여 Temperature 0.0과 Penalty 0.0의 완벽한 쾌적 상태로 모델을 다시 호출한다.</p>
<p>이러한 하이브리드 오라클 매커니즘의 구현은 파라미터 수준에서의 확률적 조작(Frequency/Presence Penalty)이 지니는 파괴적인 한계를 소프트웨어 아키텍처 관점에서 완벽히 극복하는 모범 사례를 제공한다. 오라클은 문법을 파괴하는 뺄셈 기반의 로짓 페널티 연산에 기대지 않고 모델의 내부 상태를 완벽한 제로 페널티(Zero-Penalty)로 유지시킴으로써 JSON 데이터나 소스 코드의 구조적 무결성(Structural Integrity)을 절대적으로 보장한다. 동시에, 외부 소프트웨어 계층의 감시 로직을 통해 텍스트 디코딩 시에 나타나는 비결정적이고 퇴화된 반복 행위를 수학적으로 차단함으로써, 전체 AI 시스템 파이프라인의 예측 가능성과 소프트웨어 신뢰성을 극대화하는 결정적인 기반을 마련한다.</p>
<p>결론적으로, 통계 기반 언어 모델의 다음 토큰 생성 과정에서 Frequency Penalty와 Presence Penalty는 소설을 쓰거나 창의적인 브레인스토밍 아이디어를 제안하는 영역에서는 어휘의 스펙트럼을 넓혀주는 매우 유용한 도구로 작동할 수 있다. 하지만 모델의 출력을 사람의 눈으로 소비하는 것이 아니라, 엄격한 규칙 기반의 컴파일러로 검증하거나 파싱 가능한 정형 데이터 포맷(JSON, SQL 등)으로 시스템에 직접 인가해야 하는 인공지능 결합형 소프트웨어 파이프라인 내에서는 사정이 180도 다르다.</p>
<p>특정 토큰의 과거 출현 빈도에만 기계적으로 비례하여 미래의 선택 확률을 억지로 삭감해버리는 이 하이퍼파라미터들의 본질적인 로짓 연산 메커니즘은, 규칙과 구조의 반복성 위에 세워진 프로그래밍 언어 환경에서는 오히려 시스템 붕괴를 초래하는 치명적인 독으로 작용한다. 따라서 AI를 통해 불확실성을 배제하고 확고한 결정론적 정답지를 획득하고자 하는 소프트웨어 아키텍트라면, 동태적으로 변하는 페널티 하이퍼파라미터의 유혹을 완전히 단절하고 이들의 개입을 0.0으로 무력화해야 한다. 필연적으로 발생할 수 있는 텍스트 루핑 및 모델의 퇴화 문제는 결코 확률 조작이라는 미봉책이 아니라, 철저하게 설계된 구조 인식형 외부 오라클 계층(Validation Layer)에서 논리적이고 결정론적인 컴퓨터 과학의 알고리즘으로 방어해 내야만 한다는 것이 차세대 AI 소프트웨어 공학이 제시하는 가장 근본적인 지향점이다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Solving LLM Repetition Problem in Production: A Comprehensive Study of Multiple Solutions - arXiv.org, https://arxiv.org/html/2512.04419v1</li>
<li>Can you ELI5 why a temp of 0 is bad? : r/LocalLLaMA - Reddit, https://www.reddit.com/r/LocalLLaMA/comments/1j10d5g/can_you_eli5_why_a_temp_of_0_is_bad/</li>
<li>LLM Sampling: Temperature, Top-K, Top-P, and Min-P Explained - Let’s Data Science, https://www.letsdatascience.com/blog/llm-sampling-temperature-top-k-top-p-and-min-p-explained</li>
<li>Difference between frequency and presence penalties? - OpenAI Developer Community, https://community.openai.com/t/difference-between-frequency-and-presence-penalties/2777</li>
<li>LLM Parameters Explained: A Practical Guide with Examples for OpenAI API in Python, https://learnprompting.org/blog/llm-parameters</li>
<li>From Logits to Tokens. Introduction | by Aditya Modi | Medium, https://medium.com/@adimodi96/from-logits-to-tokens-9a36feab9cab</li>
<li>Customizing LLM Output: Post-Processing Techniques - Neptune.ai, https://neptune.ai/blog/customizing-llm-output-post-processing-techniques</li>
<li>Understanding Presence Penalty and Frequency Penalty in OpenAI Chat Completion API Calls | by Pushparaj Selvaraj | Medium, https://medium.com/@pushparajgenai2025/understanding-presence-penalty-and-frequency-penalty-in-openai-chat-completion-api-calls-2e3a22547b48</li>
<li>Frequency Penalty Explained: A Key to Better AI Content - Promptitude.io, https://www.promptitude.io/glossary/frequency-penalty</li>
<li>Studying How Configurations Impact Code Generation in LLMs: the Case of ChatGPT - arXiv, https://arxiv.org/html/2502.17450v1</li>
<li>Repetition penalties are terribly implemented - A short explanation and solution - Reddit, https://www.reddit.com/r/LocalLLaMA/comments/1g383mq/repetition_penalties_are_terribly_implemented_a/</li>
<li>What Are LLM Parameters? | IBM, https://www.ibm.com/think/topics/llm-parameters</li>
<li>Presence_penalty and frequency_penalty parameters - API - OpenAI Developer Community, https://community.openai.com/t/presence-penalty-and-frequency-penalty-parameters/302813</li>
<li>Guide to ChatGPT’s Advanced Settings - Top P, Frequency Penalties, Temperature, and More | Towards Data Science, https://towardsdatascience.com/guide-to-chatgpts-advanced-settings-top-p-frequency-penalties-temperature-and-more-b70bae848069/</li>
<li>Frequency vs Presence penalty, what’s the difference? — OpenAI API | by Asim KT | Medium, https://medium.com/@KTAsim/frequency-vs-presence-penalty-whats-the-difference-openai-api-51b0c4a7229e</li>
<li>What is Frequency Penalty and how to use it, https://www.vellum.ai/llm-parameters/frequency-penalty</li>
<li>LLM Settings - Prompt Engineering Guide, https://www.promptingguide.ai/introduction/settings</li>
<li>LLM Settings Explained: Temperature, Max Tokens, Stop Sequences, Top P, Frequency Penalty, and… - Mehmet Ozkaya, https://mehmetozkaya.medium.com/llm-settings-explained-temperature-max-tokens-stop-sequences-top-p-frequency-penalty-and-04a9df257378</li>
<li>Understanding Frequency Penalty and Presence Penalty in AI Text Generation — A Beginner’s Guide - Kavya Goyal, https://goyalkavya.medium.com/understanding-frequency-penalty-and-presence-penalty-in-ai-text-generation-a-beginners-guide-15201d9e305f</li>
<li>Essential LLM Parameters Every AI Team Needs | Galileo, https://galileo.ai/blog/llm-parameters-model-evaluation</li>
<li>How I Fixed My LLM’s Repetitive Responses (And Why Temperature Matters) - Medium, https://medium.com/@Shamimw/how-i-fixed-my-llms-repetitive-responses-and-why-temperature-matters-6a8087910260</li>
<li>LLM Parameters Explained: A Practical, Research-Oriented Guide with Examples, https://promptrevolution.poltextlab.com/llm-parameters-explained-a-practical-research-oriented-guide-with-examples/</li>
<li>LLM Stability: A detailed analysis with some surprises - arXiv.org, https://arxiv.org/html/2408.04667v2</li>
<li>Investigating Reproducibility Challenges in LLM Bugfixing on the HumanEvalFix Benchmark, https://www.mdpi.com/2674-113X/4/3/17</li>
<li>Non-Determinism of “Deterministic” LLM Settings - arXiv, https://arxiv.org/html/2408.04667v5</li>
<li>An information-theoretic repetition penalty for autoregressive language models - OpenReview, https://openreview.net/pdf/1bbe13fcbffcbf271cd330277ca3546287de31e8.pdf</li>
<li>LZ Penalty: An information-theoretic repetition penalty for autoregressive language models, https://arxiv.org/html/2504.20131v3</li>
<li>Using Large Language Models for Aerospace Code Generation: Methods, Benchmarks, and Potential Values - MDPI, https://www.mdpi.com/2226-4310/12/6/498</li>
<li>Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation - arXiv, https://arxiv.org/html/2510.26130v2</li>
<li>Rethinking Repetition Problems of LLMs in Code Generation - ACL Anthology, https://aclanthology.org/2025.acl-long.48.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>