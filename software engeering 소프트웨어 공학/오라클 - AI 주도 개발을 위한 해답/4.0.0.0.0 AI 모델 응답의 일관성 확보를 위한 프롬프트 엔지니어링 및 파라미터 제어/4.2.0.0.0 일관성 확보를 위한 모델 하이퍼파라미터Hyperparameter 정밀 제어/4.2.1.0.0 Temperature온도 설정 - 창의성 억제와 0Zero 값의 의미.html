<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:4.2.1 Temperature(온도) 설정: 창의성 억제와 0(Zero) 값의 의미</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>4.2.1 Temperature(온도) 설정: 창의성 억제와 0(Zero) 값의 의미</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 4. AI 모델 응답의 일관성 확보를 위한 프롬프트 엔지니어링 및 파라미터 제어</a> / <a href="index.html">4.2 일관성 확보를 위한 모델 하이퍼파라미터(Hyperparameter) 정밀 제어</a> / <span>4.2.1 Temperature(온도) 설정: 창의성 억제와 0(Zero) 값의 의미</span></nav>
                </div>
            </header>
            <article>
                <h1>4.2.1 Temperature(온도) 설정: 창의성 억제와 0(Zero) 값의 의미</h1>
<p>인공지능을 소프트웨어 개발 및 검증의 핵심 컴포넌트로 통합하는 과정에서 가장 큰 기술적 장애물은 대규모 언어 모델(LLM, Large Language Model)이 본질적으로 지니고 있는 확률론적(Probabilistic) 텍스트 생성 특성이다. 전통적인 소프트웨어 공학에서 요구하는 결정론적(Deterministic) 정답지, 즉 오라클(Oracle)을 구축하기 위해서는 동일한 입력에 대해 항상 동일한 출력을 보장하는 통제된 시스템이 필수적이다. 언어 모델의 텍스트 생성 과정에서 이러한 무작위성을 제어하고 결정론적 동작을 시스템 수준에서 강제하는 가장 근본적이고 강력한 하이퍼파라미터가 바로 ’Temperature(온도)’이다. 본 절에서는 Temperature 파라미터가 모델의 출력 확률 분포를 변형하는 수학적 원리를 깊이 있게 분석하고, 언어 모델의 창의성을 억제하여 소프트웨어 오라클로서의 신뢰성을 확보하는 메커니즘을 규명한다. 나아가 Temperature를 0으로 설정했을 때 발생하는 탐욕적 해독(Greedy Decoding)의 작동 방식과, 완벽한 0 값에서도 잔존하는 하드웨어 수준의 비결정성 문제, 그리고 이를 극복하기 위한 실전 오라클 구현 전략을 상세히 다룬다.</p>
<h2>1.  확률적 텍스트 생성과 온도의 수학적 기반</h2>
<p>대규모 언어 모델은 주어진 문맥(Context)을 바탕으로 어휘 사전(Vocabulary)에 포함된 모든 토큰이 다음에 등장할 확률을 계산하는 자기회귀(Autoregressive) 방식으로 텍스트를 생성한다. 모델의 마지막 선형 레이어(Linear Layer)는 각 토큰에 대한 정규화되지 않은 원시 예측 점수인 로짓(Logit)을 출력하며, 이 로짓 벡터는 소프트맥스(Softmax) 함수를 통과하여 총합이 1이 되는 확률 분포로 변환된다.</p>
<p>Temperature(<span class="math math-inline">T</span>) 파라미터는 바로 이 소프트맥스 함수에 도입되는 스케일링 계수(Scaling Factor)로, 통계 역학(Statistical Mechanics)의 볼츠만 분포(Boltzmann distribution)에서 입자의 에너지 상태가 온도에 따라 어떻게 변화하는지를 설명하는 개념에서 최초로 차용되었다. 기계 학습 영역에서 1985년 Ackley 등에 의해 처음 도입된 이 개념은, 신경망이 출력하는 에너지 상태(로짓)를 확률로 변환할 때 시스템의 ’활력’을 조절하는 역할을 수행한다. 수학적으로 Temperature가 적용된 소프트맥스 함수는 다음과 같이 정의된다.<br />
<span class="math math-display">
p_i(T) = \frac{\exp(z_i / T)}{\sum_{j=1}^{K} \exp(z_j / T)}
</span><br />
여기서 <span class="math math-inline">p_i(T)</span>는 온도 <span class="math math-inline">T</span>가 적용되었을 때 <span class="math math-inline">i</span>번째 토큰이 선택될 확률이며, <span class="math math-inline">z_i</span>는 해당 토큰의 로짓 값, <span class="math math-inline">K</span>는 어휘 사전의 전체 크기를 의미한다. 이 수식에서 알 수 있듯, Temperature <span class="math math-inline">T</span>는 모델이 수많은 데이터의 가중치를 통해 학습한 원래의 로짓 <span class="math math-inline">z_i</span> 자체를 변형하지 않는다. 대신 지수 함수에 입력되기 전의 스케일을 조절하여 최종 확률 분포의 형태(Shape)를 조작한다.</p>
<p>온도의 변화가 확률 분포에 미치는 영향을 구조적으로 이해하기 위해, 특정 토큰 <span class="math math-inline">i</span>가 선택될 확률이 온도 <span class="math math-inline">T</span>에 따라 어떻게 변화하는지를 나타내는 편미분 <span class="math math-inline">\frac{\partial p_i(T)}{\partial T}</span>를 도출해 볼 수 있다. 논문 분석에 따르면, 몫의 미분법(Quotient Rule)과 연쇄 법칙(Chain Rule)을 적용하여 이 도함수를 전개하면 매우 복잡한 분수 형태가 도출되며, 이는 <span class="math math-inline">T</span>가 모델의 출력 역학에 미치는 심오한 영향을 수학적으로 증명한다. 지수 항에 대한 도함수는 다음과 같이 간소화된다.<br />
<span class="math math-display">
\frac{\partial}{\partial T} \exp(z_i / T) = -\frac{z_i}{T^2} \exp(z_i / T)
</span><br />
이를 활용하여 전체 확률에 대한 미분을 수행하고 단순화하면, 온도가 낮아질수록(<span class="math math-inline">T &lt; 1</span>) 로짓 간의 미세한 차이가 기하급수적으로 증폭되어 분포가 ‘날카로워(Sharpen)’ 지며, 반대로 온도가 높아질수록(<span class="math math-inline">T &gt; 1</span>) 로짓 간의 차이가 희석되어 분포가 ‘평탄해(Flatten)’ 짐을 알 수 있다. 소프트맥스 함수는 본래 미분 불가능하고 불연속적인 최대값 산출 함수를 연속적이고 미분 가능하게 만든 ‘부드러운 최댓값(Soft-argmax)’ 함수이다. 소프트맥스라는 이름 자체가 이 연속성에서 유래한 것이며, 개발자나 연구자는 온도를 조절함으로써 본래의 엄격한 최댓값 산출에 무한히 가깝게 만들거나 완전히 무작위적인 균등 분포에 가깝게 만들 수 있는 유연성을 확보하게 된다.</p>
<p><img src="./4.2.1.0.0%20Temperature%EC%98%A8%EB%8F%84%20%EC%84%A4%EC%A0%95%20-%20%EC%B0%BD%EC%9D%98%EC%84%B1%20%EC%96%B5%EC%A0%9C%EC%99%80%200Zero%20%EA%B0%92%EC%9D%98%20%EC%9D%98%EB%AF%B8.assets/image-20260225192909624.jpg" alt="image-20260225192909624" /></p>
<p>논문 “Does Temperature 0 Guarantee Deterministic LLM Outputs?” 등 여러 문헌에서 지적하듯, 온도가 1.0 미만으로 설정될 경우 확률 분포는 급격히 가장 유력한 토큰으로 집중된다. 예컨대 원시 로짓 값이 각각 2.0, 1.0, 0.5인 3개의 후보 토큰이 있다고 가정할 때, 온도가 2.0으로 설정되면 확률은 각각 약 42%, 33%, 25%로 평탄화되어 세 토큰 모두가 샘플링될 가능성을 고르게 갖는다. 반면 온도를 0.5로 낮추면 확률은 각각 87%, 11%, 2%로 변환되어 첫 번째 토큰의 지배력이 압도적으로 강해진다. 이는 모델이 자신의 예측에 대해 더욱 ’확신(Confident)’을 갖게 만들며, 무작위성을 물리적으로 배제하는 핵심 원리로 작용한다.</p>
<h2>2.  창의성 억제와 탐색-활용 상충 관계(Exploration vs. Exploitation)</h2>
<p>인공지능 모델의 텍스트 생성은 강화학습 및 정보 이론에서 자주 언급되는 ’탐색과 활용의 상충 관계(Exploration-Exploitation Trade-off)’와 밀접하게 맞닿아 있다. 일반적인 대화형 챗봇 서비스나 스토리텔링, 마케팅 문구 작성과 같은 도메인에서는 모델이 이전에 본 적 없는 새롭고 다양한 문장 구조를 만들어내는 ’탐색(Exploration)’이 매우 중요하다. 이를 위해 <span class="math math-inline">T &gt; 1.0</span> (최대 2.0)의 높은 온도를 설정하면, 확률이 상대적으로 낮은 꼬리(Tail) 부분의 토큰들도 확률적 샘플링(Stochastic Sampling) 과정에서 선택될 기회를 얻게 되어 응답의 어휘적 다양성과 이른바 ’창의성(Creativity)’이 현저히 증가한다. 높은 온도는 통계 역학의 비유를 빌리자면 시스템 내 입자들의 ’튕김(Bounciness)’을 극대화하여 예측하기 어려운 위치로 튀어 오르게 만드는 것과 같다.</p>
<p>그러나 모델의 창의성이 높아진다는 것은 필연적으로 일관성(Consistency)과 사실적 정합성(Factual Accuracy)이 저하됨을 의미한다. 논문 “Can Machines Dream? On the Creativity of Large Language Models” 및 여러 연구에 따르면, 언어 모델의 창의적 텍스트 생성은 인간 수준의 직관적 통찰이 아니라 실제로는 확률 분포의 꼬리 부분에 있는 노이즈를 허용함으로써 발생하는 변형 기법에 불과하다. 온도가 지나치게 높아지면(예: <span class="math math-inline">T &gt; 1.5</span>) 모델은 의미가 통하지 않는 무의미한 텍스트(Gibberish)를 생성하거나 모델의 신뢰성을 심각하게 훼손하는 할루시네이션(Hallucination, 환각 현상) 빈도가 급격히 상승하는 문제에 직면한다. 논문 “Would Temperature Control Help Against ChatGPT’s Hallucinations?“에서는 이러한 환각을 통제하기 위한 핵심 장치로 온도 조절과 지식 그래프(Knowledge Graph) 연동을 꼽으며, 온도가 신뢰성에 미치는 지대한 영향을 역설하고 있다.</p>
<p>특히 의료 진단, 법률 판례 분석, 금융 데이터 마이닝이나 본 서적에서 주안점을 두고 있는 ’결정론적 소프트웨어 검증’과 같은 고위험(High-stakes) 환경에서는 모델의 이러한 창의성이 오히려 치명적인 시스템 결함으로 직결된다. 소프트웨어 코드 작성 시 단 하나의 구문 오류(Syntax error)나 괄호 누락, 변수명의 무작위적 변경은 컴파일 실패 또는 예기치 않은 런타임 에러를 유발하기 때문이다.</p>
<p>따라서 AI를 소프트웨어 테스트의 오라클이나 비즈니스 로직의 확정적 검증 도구로 사용할 때는 철저한 ’창의성 억제’가 요구된다. <span class="math math-inline">T &lt; 1.0</span> (일반적으로 0.0 ~ 0.3)의 환경에서는 모델이 가장 높은 신뢰도로 예측한 패턴, 즉 사전 학습(Pre-training) 데이터에서 가장 빈번하게 관찰되었거나 주어진 프롬프트 문맥과 가장 논리적으로 부합하는 기댓값만을 ’활용(Exploitation)’하도록 강제된다. 이는 모델이 보유한 지식의 가장 확실한 중심부만을 출력하게 만들어, 환각 현상을 억제하고 사실적이고 일관된 출력을 보장하는 핵심 기제가 된다. 극단적으로 낮은 온도에서는 출력이 다소 기계적이고 지루해질 수 있으나, 정형화된 JSON 데이터 추출, 코드 리뷰, 유닛 테스트 어서션(Assertion) 생성과 같은 작업에서는 이러한 예측 가능성이 유일한 성공의 척도가 된다.</p>
<table><thead><tr><th><strong>목적 분야</strong></th><th><strong>권장 Temperature</strong></th><th><strong>동작 방식 및 확률 분포 특성</strong></th><th><strong>소프트웨어 개발 적용 사례</strong></th></tr></thead><tbody>
<tr><td><strong>순수 결정론적 오라클</strong></td><td>0.0</td><td><strong>탐욕적 해독(Greedy Decoding)</strong>: 최상위 확률 토큰 1개만을 무조건 선택. 델타 함수 형태의 분포 붕괴.</td><td>유닛 테스트 생성, JSON 스키마 강제 추출, 회귀 테스트용 골든 데이터 생성</td></tr>
<tr><td><strong>코드 분석 및 보안 리뷰</strong></td><td>0.1 ~ 0.3</td><td><strong>강한 확신성(High Confidence)</strong>: 1~2위 후보군 내에서 제한적 변동. 오탐(False Negative) 비용이 큰 환경에 적합.</td><td>정적 분석 봇, 보안 취약점 스캐닝 리포트 작성</td></tr>
<tr><td><strong>기술 문서 및 아키텍처</strong></td><td>0.3 ~ 0.6</td><td><strong>활용 위주(Exploitation)</strong>: 모델의 사전 지식에 기반한 정석적인 패턴 조립. 기술적 정확성을 유지하면서 미세한 변주 허용.</td><td>API 문서 자동화, 시스템 설계서 작성, 기술 부채 분석</td></tr>
<tr><td><strong>브레인스토밍 및 테스트 다양성</strong></td><td>0.7 ~ 1.0</td><td><strong>탐색-활용 균형(Balanced)</strong>: 원시 확률 분포에 가까운 샘플링. 다양한 논리 구조 탐색.</td><td>엣지 케이스 테스트 시나리오 도출, 페르소나 기반 대화형 챗봇 모의 훈련</td></tr>
</tbody></table>
<p>이 표는 소프트웨어 개발 생명주기(SDLC) 내에서 온도가 어떻게 전략적으로 분배되어야 하는지를 보여준다. 창의성과 일관성은 본질적으로 영합 게임(Zero-sum game)의 관계에 놓여 있으며, 오라클 시스템의 설계자는 의도된 목적에 부합하도록 이 수치를 극단적으로 억제해야 한다.</p>
<h2>3.  0(Zero) 값의 진정한 의미: 탐욕적 해독(Greedy Decoding)의 강제</h2>
<p>Temperature 파라미터를 극한으로 낮춰 <span class="math math-inline">T = 0</span>으로 설정하는 것은 단순히 확률의 분산을 줄이는 것을 넘어, 언어 모델의 디코딩(Decoding) 알고리즘 자체를 확률적 샘플링(Stochastic Sampling)에서 확정적 선택 방식인 ’탐욕적 해독(Greedy Decoding)’으로 근본적으로 전환하는 것을 의미한다.</p>
<p><span class="math math-inline">T</span>가 0에 수렴하는 극한(Limit as <span class="math math-inline">T \to 0</span>)을 취하면, 소프트맥스 함수의 출력은 로짓 값이 가장 큰 단 하나의 토큰에 1.0(<span class="math math-inline">100\%</span>)의 확률을 할당하고, 나머지 모든 토큰에는 0.0의 확률을 할당하는 원-핫 벡터(One-hot vector) 형태의 디랙 델타 함수(Dirac delta function) 분포로 붕괴(Collapse)한다.<br />
<span class="math math-display">
\lim_{T \to 0} p_i(T) = \begin{cases} 1 &amp; \text{if } z_i = \max(z) \\ 0 &amp; \text{otherwise} \end{cases}
</span><br />
이 상태에서는 확률에 기반한 주사위 굴리기(Sampling) 단계가 완전히 생략된다. 모델은 각 생성 스텝마다 현재까지 구축된 문맥에서 가장 확률이 높은 단 하나의 최적 토큰만을 기계적으로 취한다. 이를 탐욕적 해독이라 부르며, 이론적으로는 동일한 모델 가중치(Weights)와 동일한 입력 프롬프트가 주어졌을 때 의도된 무작위성(Deliberate randomness)이 완전히 배제되므로 시스템은 매번 완벽하게 동일한 토큰 시퀀스를 출력해야만 한다. 이 과정에서 발생하는 현상은 마치 동전을 던질 때 무작위성을 제거하고 항상 앞면만 나오도록 물리적인 제약을 가한 것과 같다.</p>
<p>물론 탐욕적 해독이 텍스트 품질 측면에서 항상 최선의 선택은 아니다. 논문 “The Curious Case of Neural Text Degeneration” 등에 따르면, 탐욕적 해독을 사용할 경우 언어 모델은 특정 단어나 구문을 무한히 반복하는 퇴화 현상(Degeneration)이나 루프(Looping)에 빠질 위험이 존재한다. 인간의 자연스러운 언어 구사는 항상 1순위 확률의 단어만을 나열하지 않으며, 이로 인해 탐욕적 해독으로 생성된 텍스트는 건조하고 기계적으로 느껴질 수 있다.</p>
<p>그러나 소프트웨어 테스트 생태계에서 오라클 생성 도구로 LLM을 활용할 때는 이러한 건조함과 단조로움이 오히려 최대의 장점으로 작용한다. LLM이 생성한 테스트 오라클의 품질을 평가한 경험적 연구인 “An empirical study of how prompt types and contextual inputs affect LLM-generated test oracle quality“에 따르면, 회귀 테스트 및 버그 탐지를 위한 어서션을 생성할 때 온도를 0으로 설정하는 것은 재현성 확보를 위한 가장 필수적인 전제 조건이다. 논문 “TEST ORACLE SYNTHESIS” 등의 다양한 학술 연구에서도 AI 모델을 테스트 목적으로 차용할 때는 평가의 공정성과 결정론적 검증을 위해 반드시 디코딩 온도를 0으로 통제할 것을 권고하고 있다.</p>
<p><span class="math math-inline">T &gt; 0</span> 인 샘플링 환경에서는 생성할 때마다 다른 코드 구조나 어서션이 도출될 수 있으며, 이는 테스트 케이스의 멱등성(Idempotency)을 심각하게 훼손하여 CI/CD 파이프라인에서 테스트가 간헐적으로 통과와 실패를 반복하는 ’플래키 테스트(Flaky Test)’를 유발하는 주범이 되기 때문이다. 따라서 <span class="math math-inline">T=0</span> 설정은 AI가 작성한 코드가 프로그래밍 언어의 엄격한 문법을 준수하고, 생성된 오라클이 코드의 실제 구현 상태를 단일한 논리로 일관되게 단언하도록 만드는 필수불가결한 물리적 제어 장치이다.</p>
<h2>4.  완벽한 결정론의 환상(Myth): 하드웨어와 연산 수준의 비결정성</h2>
<p>이론적 수식과 API 문서만을 참고하면, Temperature를 0으로 설정하는 것만으로 완벽한 결정론적 오라클을 구축할 수 있을 것처럼 여겨진다. 그러나 실전 AI 소프트웨어 개발 현장에서는 <span class="math math-inline">T=0</span>을 설정하여 철저하게 탐욕적 해독을 강제했음에도 불구하고, 동일한 프롬프트에 대해 미세하게 다른 응답이 반환되는 기이한 현상이 종종 목격된다. 많은 개발자들이 이를 서비스 제공 업체의 API 버그나 프롬프트 내림 현상으로 오인하지만, 이는 현대의 대규모 딥러닝 인퍼런스 아키텍처가 지닌 근본적인 ’연산의 혼돈(Computational Chaos)’에 기인한 현상이다.</p>
<p><span class="math math-inline">T=0</span>은 앞서 설명한 확률 분포에서 ’토큰을 샘플링하는 단계(Sampling Step)’에 존재하는 무작위성만을 완벽히 제거할 뿐, 근원적으로 ’로짓(Logit)을 계산하는 단계’에서 발생하는 기계적인 비결정성까지는 통제하지 못한다. 텍스트가 예측되기 전에 내부 신경망에서 발생하는 수치적 편차(Numerical Drift)는 최종 출력에 결정적인 나비효과를 일으킨다. 이를 유발하는 주요 시스템 아키텍처적 원인은 크게 세 가지로 요약된다.</p>
<h3>4.1  부동소수점 연산의 비결합성(Non-associativity of Floating-Point Math)</h3>
<p>가장 기저에 있는 문제는 컴퓨터의 수학적 표현의 한계다. 최신 LLM은 수천억 개의 파라미터를 바탕으로 방대한 행렬 곱셈(Matrix Multiplication)을 수행한다. GPU에서 이러한 연산은 극단적인 병렬 처리를 통해 이루어지는데, 병렬 리덕션(Parallel Reduction) 과정에서 덧셈의 처리 순서가 스케줄러에 의해 동적으로 변경될 수 있다. IEEE 754 부동소수점 표준의 근본적인 한계로 인해, 컴퓨터 연산에서는 무한한 정밀도를 보장할 수 없으므로 결합 법칙이 엄밀하게 성립하지 않는다 (<span class="math math-inline">(a+b)+c \neq a+(b+c)</span>). 즉, 여러 스레드 블록에서 병렬로 계산된 값들을 합산할 때 어느 스레드가 먼저 메모리에 접근하여 덧셈을 수행하느냐에 따라 최종 누적된 로짓 값의 끄트머리 소수점(예: <span class="math math-inline">10^{-7}</span> 이하) 단위에서 필연적으로 오차가 발생한다.</p>
<h3>4.2  동적 배치 처리(Dynamic Batching)의 환상(Mirage)</h3>
<p>단독 실행 환경이 아닌 상용 LLM API(예: OpenAI, Anthropic, Gemini) 환경에서는 서버의 처리량(Throughput)을 극대화하고 비용을 절감하기 위해 전 세계에서 쏟아지는 수많은 사용자 요청을 묶어서 처리한다. 이를 연속 배칭(Continuous Batching) 또는 동적 배칭이라 하는데, 시스템은 실시간으로 유입되는 프롬프트들을 하나의 거대한 텐서(Tensor)로 병합하여 GPU에 인가한다. 문제는 내 요청이 어떤 다른 사용자의 요청들과 묶이느냐에 따라 GPU 메모리에 올라가는 전체 행렬의 크기와 빈 공간을 채우는 패딩(Padding) 방식이 매번 달라진다는 점이다.</p>
<p>논문 “Causes and Effects of Unanticipated Numerical Deviations in Neural Network Inference Frameworks” 등에 따르면, 이러한 ‘Batching Mirage(배칭의 신기루)’ 현상으로 인해 하위 수준의 CUDA 연산 커널(Kernel) 선택이 달라질 수 있으며, 패딩된 제로(Zero) 값들이 병렬 연산에 포함되면서 내 요청이 단독으로 처리될 때와 묶여서 처리될 때 산출되는 최종 로짓 값에 미세한 불일치가 발생한다. 내가 완벽히 통제된 프롬프트를 보냈다 하더라도, 동일한 배치에 묶인 이름 모를 누군가의 프롬프트 길이에 의해 내 결과값이 오염될 수 있는 것이다.</p>
<h3>4.3  전문가 혼합(MoE, Mixture of Experts) 아키텍처의 라우팅 비결정성</h3>
<p>GPT-4 등 최신 프론티어 모델은 단일한 거대 신경망이 아니라 여러 개의 소형 전문가 네트워크로 구성된 MoE 아키텍처를 채택하는 추세다. 입력 토큰은 라우터 네크워크를 통해 가장 적절한 전문가에게 할당되어 처리되는데, 대규모 분산 클러스터 환경에서의 네트워크 지연, 동적 로드 밸런싱, 하드웨어 풀링(Hardware Pooling) 상태에 따라 동일한 입력에 대한 라우팅 경로가 미세하게 달라질 수 있다. 또한 서비스 제공자가 GPU 아키텍처(예: A100과 H100)를 혼용하여 사용하는 경우 하드웨어별 부동소수점 최적화 방식이 달라 모델 단위에서 미세한 수치적 노이즈가 유입된다.</p>
<p><strong>비결정성이 오라클에 미치는 치명적 결과</strong> 이러한 연산 과정의 수치적 노이즈는 평소에는 1순위 토큰과 2순위 토큰 간의 확률 격차가 크기 때문에 결과에 아무런 영향을 미치지 않는다. 하지만 만약 <span class="math math-inline">T=0</span> 상태에서 가장 높은 확률을 가진 1위 후보와 2위 후보의 로짓 값이 우연히 매우 근접해 있는 상황(Tie break)이라면, 이 보이지 않는 부동소수점 오차가 두 토큰의 순위를 뒤바꿔버릴 수 있다.</p>
<p>예를 들어, 프롬프트가 주어진 후 AI가 출력할 첫 단어로 “amount“와 “total“을 고민 중이라고 가정해보자. 내부 연산 결과 이 두 단어의 초기 확률이 각각 <span class="math math-inline">45.000001%</span>와 <span class="math math-inline">45.000000%</span>로 산출되었다면 시스템은 “amount“를 선택한다. 하지만 동일한 프롬프트로 재실행했을 때 병렬 연산 노이즈가 유입되어 <span class="math math-inline">44.999999\%</span> 대 <span class="math math-inline">45.000002%</span>로 확률이 미세하게 역전된다면, 모델은 <span class="math math-inline">T=0</span>의 탐욕적 해독 원칙에 따라 “total“을 선택하게 된다. 한 번 첫 단어가 달라지면 자기회귀(Autoregressive) 생성 방식의 특성상 그 이후의 생성 경로는 폭포수처럼 완전히 달라지는 나비효과가 발생하여, 결과적으로 동일한 온도 0의 설정임에도 확연히 다른 최종 출력이 반환된다.</p>
<p>이러한 한계성 때문에 주요 모델 제공자인 OpenAI, Anthropic, Google 모두 API 공식 문서에서 “Temperature가 0이어도 완전한 결정론을 보장할 수는 없다“고 명시적으로 경고하고 있다. 결정론적 오라클을 구축하려는 소프트웨어 엔지니어는 <span class="math math-inline">T=0</span>을 신뢰성 확보의 ’완성’이 아니라 ’시작점’으로 인식해야만 한다.</p>
<h2>5.  소프트웨어 테스트 오라클 구축을 위한 온도 제어의 한계 극복 방안</h2>
<p>소프트웨어 공학에서 테스트 오라클(Test Oracle)이란 주어진 테스트 케이스에 대해 시스템이 올바르게 실행되었는지를 판별하는 메커니즘 혹은 정답지를 뜻한다. 학술 문헌 “The Oracle Problem in Software Testing: A Survey“에 명시된 바와 같이, 입력 생성 자동화 기술은 비약적으로 발전했으나 출력의 정합성을 확인하는 오라클의 자동화는 여전히 소프트웨어 검증 병목 현상을 유발하는 가장 어려운 난제(The Oracle Problem)로 남아있다.</p>
<p>AI 모델을 통해 이 오라클 문제를 해결하려 할 때, 우리는 API 수준에서 지원하는 ‘대체로 결정론적인(Mostly Deterministic)’ LLM의 출력을 엔터프라이즈 환경에서 요구하는 ‘완전히 결정론적인(Strictly Deterministic)’ 시스템 수준의 통제로 승격시켜야만 한다. 단순히 Temperature를 0으로 설정하는 것만으로는 기술적 부채가 누적되므로, 앞서 확인한 하드웨어적, 연산적 비결정성을 상쇄할 수 있는 다음과 같은 포괄적인 제어 체계가 뒷받침되어야 한다.</p>
<h3>5.1  디코딩 하이퍼파라미터의 다중 결합 봉쇄</h3>
<p>온도를 0으로 설정하는 것은 첫 번째 방어선이다. 이와 결합하여 무작위성을 유발할 수 있는 시스템 내의 다른 모든 하이퍼파라미터를 일관되게 통제해야 한다.</p>
<ul>
<li><strong>Top-P (Nucleus Sampling) 및 Top-K 고정:</strong> <span class="math math-inline">T=0</span> 인 탐욕적 해독 상황에서는 이론적으로 Top-P나 Top-K에 의한 후보군 절단(Truncation)이 무시된다. 그러나 내부 구현 라이브러리의 엣지 케이스를 방지하기 위해 명시적으로 <code>top_p = 1.0</code> (전체 확률 분포 고려) 및 API가 허용하는 최대 Top-K 값을 설정하여 잠재적인 확률 제어 충돌을 미연에 방지해야 한다.</li>
<li><strong>Seed(시드) 파라미터의 활용:</strong> OpenAI 등 일부 API 제공자는 <code>seed</code> 파라미터를 제공하여 시스템의 난수 생성기(PRNG) 상태를 특정 값으로 고정할 수 있도록 지원한다. <span class="math math-inline">T=0</span> 환경에서는 확률적 샘플링이 발생하지 않으므로 시드 값의 고정이 의미론적으로 큰 영향을 미치지 않을 수 있으나, 백엔드 시스템 내부에 잔존하는 여타 의사 난수 생성 로직을 통제하기 위해 항상 정적인 Seed 값을 주입하는 것이 권장되는 방어책이다.</li>
<li><strong>단일 출력(n=1) 강제:</strong> API를 호출할 때 하나의 프롬프트로 여러 개의 응답 후보군을 생성하는 설정(예: <code>n=5</code>)은 백엔드에서 다양성을 강제하는 매커니즘을 트리거할 위험성을 내포하므로 오라클 시스템에서는 항상 1로 제한해야 한다.</li>
</ul>
<table><thead><tr><th><strong>하이퍼파라미터</strong></th><th><strong>오라클 구축 시 설정 권장값</strong></th><th><strong>설정 근거 및 방어 메커니즘</strong></th></tr></thead><tbody>
<tr><td><strong>Temperature</strong></td><td><code>0.0</code></td><td>텍스트 샘플링 단계의 무작위성을 배제하고 탐욕적 해독(Greedy Decoding) 상태로 시스템을 강제 전환함.</td></tr>
<tr><td><strong>Top-P</strong></td><td><code>1.0</code></td><td>확률 분포의 꼬리를 절단하는 동작을 비활성화하여, 온도 설정과의 연산 충돌 방지.</td></tr>
<tr><td><strong>Seed</strong></td><td>고정 정수값 (예: <code>42</code>)</td><td>백엔드 시스템에 존재할 수 있는 의사 난수 생성기 상태를 고정하여 최선의 재현성(Best-effort Reproducibility) 도모.</td></tr>
<tr><td><strong>Frequency Penalty</strong></td><td><code>0.0</code></td><td>오라클 판단 결과의 일관성이 반복 패널티로 인해 왜곡되는 현상 차단.</td></tr>
</tbody></table>
<h3>5.2  구조화된 출력(Structured Outputs)과 회복탄력적 파싱(Resilient Parsing)</h3>
<p>앞서 하드웨어 노이즈 섹션에서 규명한 바와 같이, <span class="math math-inline">T=0</span> 환경에서도 수치적 편차로 인해 어휘(Lexical) 수준의 완벽한 100% 동일성은 보장할 수 없다. 따라서 결정론적 오라클을 구축할 때는 모델이 반환한 ’문자열 전체의 정확한 일치(Exact String Matching)’에 의존하는 취약한 시스템 구조를 전면 탈피해야 한다.</p>
<p>그 대안으로 LLM의 출력 형태를 JSON Schema 등으로 강력하게 제어(예: OpenAI의 <code>response_format</code> 기능 활용)하고, 출력된 결과물을 평가할 때는 텍스트 자체가 아니라 추출된 데이터 구조 내의 ’의미론적 정합성(Semantic Correctness)’을 시스템적으로 검증해야 한다. 오라클 시스템은 “True“와 “true“의 차이, 띄어쓰기 1개의 추가, 동의어의 변경 등 미세한 문법적 변동성을 허용하고 무시할 수 있는 강건한 파서(Robust Parser)를 파이프라인에 내장해야 한다. 이는 오라클이 코드의 표면적 구현에 집착하지 않고 설계된 의도만을 추출해내도록 만드는 필수적인 엔지니어링 기법이다.</p>
<h3>5.3  배치 불변 연산(Batch Invariant Ops) 및 로컬 환경의 물리적 통제</h3>
<p>상용 API가 아닌 로컬 인프라 환경에서 오픈소스 모델(예: LLaMA, Qwen, Mistral)을 자체 호스팅하여 사내 전용 오라클 서버로 사용할 경우, <span class="math math-inline">T=0</span>의 비결정성을 물리적 레벨에서 원천 제거할 수 있는 방법론이 존재한다.</p>
<p>근본적인 연산 편차를 제거하기 위해 <code>thinking-machines-lab/batch-invariant-ops</code>와 같은 специализирован된 배치 불변 라이브러리를 적용하여, PyTorch 프레임워크 내의 표준 연산자들을 배치 크기나 패딩 길이에 영향을 받지 않는 고정된 결정론적 연산자로 교체할 수 있다. 또한 <code>torch.use_deterministic_algorithms(True)</code> 플래그를 활성화하여 백엔드 레벨에서 결정론을 강제한다. 더 나아가 CUDA의 병렬 덧셈 연산 순서를 고정하거나 강제로 직렬화(Serialization)함으로써 부동소수점 오차의 원천을 완전히 차단하면, 성능의 일부 저하를 대가로 오라클 결과를 비트 단위 수준(Bit-perfect)에서 영구적으로 일관되게 재현할 수 있는 궁극의 통제권을 획득하게 된다.</p>
<h2>6.  실전 예제: 탐욕적 해독 기반의 강건한 비즈니스 로직 검증 오라클 시스템 설계</h2>
<p>소프트웨어 개발 파이프라인에서 AI를 결정론적 정답지(오라클)로 실전 활용하기 위해서는 위에서 논의된 모든 수학적, 아키텍처적 개념들을 코드로 구현하여 시스템화해야 한다. 다음 실전 예제는 보안 및 정적 분석 논문인 “Non-Distinguishable Inconsistencies as a Deterministic Oracle for Detecting Security Bugs“의 결정론적 접근법에서 영감을 받아 작성되었다. 해당 문헌에서는 시스템 내부의 상태 불일치가 외부에서 구별 불가능할 때 필연적으로 보안 버그가 발생한다는 확정적 오라클을 구축하였다.</p>
<p>이러한 논리적 확정성을 AI 모델을 통해 모사하기 위해, 파이썬(Python)을 사용하여 사용자의 텍스트 입력을 분석하여 비즈니스 규칙의 무결성을 검증하는 오라클 엔진을 구현하였다. 본 예제는 단순히 Temperature를 0으로 설정하는 API 래퍼(Wrapper)를 넘어서, 하드웨어적 비결정성으로 인해 발생할 수 있는 토큰 생성의 엣지 케이스를 처리하는 ’회복탄력적 파싱’과 ’의미론적 평가 로직’이 통합되어 있다.</p>
<pre><code class="language-Python">import json
import logging
from typing import Dict, Any, Optional

# 가상의 통제된 LLM 클라이언트 환경 가정 
# (OpenAI API 호환 구조 또는 VLLM, vLLM 기반 로컬 호스팅 모델)
class DeterministicLLMClient:
    def chat_completion(self, prompt: str, temperature: float, seed: int, top_p: float) -&gt; str:
        # 실제 LLM 호출 로직이 위치하는 곳
        # 본 예제에서는 T=0.0 설정을 통한 탐욕적 해독(Greedy Decoding) 환경을 가정함
        pass

class AICodeSecurityOracle:
    def __init__(self, llm_client: DeterministicLLMClient):
        self.llm = llm_client
        self.logger = logging.getLogger("SecurityOracleEngine")
        
        # [오라클 구축 핵심] 연산 노이즈를 극복하기 위한 철저한 하이퍼파라미터 통제
        # 1. Temperature = 0.0 : 창의성을 완전히 억제하고 확률 분포를 단일 토큰으로 붕괴시킴
        # 2. Seed 고정 : 의사 난수 생성기의 내부 변동성 및 샘플링 상태 고정
        # 3. Top-P = 1.0 : 온도 0 설정과 충돌하여 부작용을 낳을 수 있는 꼬리 자르기 비활성화
        self.generation_params = {
            "temperature": 0.0,
            "seed": 8472, # Best-effort 재현성을 위한 임의의 매직 넘버 고정
            "top_p": 1.0, 
        }

    def _generate_structured_prompt(self, test_input: str, security_rules: str) -&gt; str:
        """
        AI 모델이 결정론적으로 행동할 수 있도록 제약 조건을 강제하는 프롬프트 템플릿 작성.
        구조화된 출력(JSON)을 요구하여 응답의 일관성을 시스템적으로 보장함.
        """
        return f"""
        너는 소프트웨어 보안 검증을 위한 엄격하고 결정론적인 오라클(Security Judge)이다.
        어떠한 부연 설명, 변명, 창의적인 해석도 완벽히 배제하라.
        다음 제공된 [코드 조각]이 [보안 규칙]을 완벽하게 만족하는지 판별하라.
        
        반드시 아래의 정확한 JSON 스키마 구조로만 응답하라. 어떠한 마크다운 백틱도 포함하지 말라:
        {{
            "is_secure": boolean,
            "confidence_score": integer (0~100),
            "violation_code": string (문제가 없다면 "NONE_DETECTED"),
            "reasoning": string (내부 판단 근거. 검증 시 무시됨)
        }}
        
        [보안 규칙]
        {security_rules}
        
        [코드 조각]
        {test_input}
        """

    def evaluate_test_case(self, test_input: str, rules: str) -&gt; bool:
        """
        T=0 기반의 LLM 응답을 파싱하고, 미세한 비결정성 노이즈를 방어하는 검증 로직.
        단순 문자열 비교(String match)의 한계를 극복함.
        """
        prompt = self._generate_structured_prompt(test_input, rules)
        
        try:
            # 파라미터를 엄격하게 고정하여 LLM 호출 (탐욕적 해독 강제)
            raw_response = self.llm.chat_completion(
                prompt=prompt,
                **self.generation_params
            )
            
            # [방어적 프로그래밍] T=0에서도 발생 가능한 JSON 마크다운 포맷팅 노이즈 필터링
            cleaned_response = raw_response.strip()
if cleaned_response.startswith("```json"):
cleaned_response = cleaned_response[7:-3].strip()
            elif cleaned_response.startswith("```"):
                cleaned_response = cleaned_response[3:-3].strip()
                
            result_data = json.loads(cleaned_response)
            
            # 오라클의 결정론적 핵심 판단: Exact String Match가 아닌 Boolean 논리 검사
            # GPU 병렬 연산 노이즈로 인해 'reasoning' 필드의 텍스트가 미세하게 변경되더라도
            # 핵심 지표인 is_secure의 불리언 값과 violation_code 식별자가 일관성을 유지한다면
            # 오라클은 시스템 전체 관점에서 결정론적으로 동작하게 됨.
            is_secure = result_data.get("is_secure", False)
            violation_code = result_data.get("violation_code", "ERROR")
            
            if is_secure and violation_code == "NONE_DETECTED":
                self.logger.info("Oracle Check Passed: Code is secure.")
                return True
            else:
                self.logger.warning(f"Oracle Check Failed: Security Violation Detected - {violation_code}")
                return False
                
        except json.JSONDecodeError as e:
            # T=0 임에도 불구하고 Batching Mirage나 MoE 라우팅 노이즈 등으로 인해
            # JSON 괄호 구조가 깨지는 엣지 케이스 발생 시
            # 즉각 실패 처리하여 비결정적 플래키(Flaky) 테스트 통과를 원천 차단함.
            self.logger.error(f"Oracle failure: Non-deterministic output format detected. JSON Error: {e}")
            return False
        except Exception as e:
            self.logger.critical(f"Unexpected Oracle System Exception: {e}")
            return False

# 활용 예시 (Mockup Environment)
if __name__ == "__main__":
    client = DeterministicLLMClient()
    oracle = AICodeSecurityOracle(client)
    
    # 비즈니스 보안 규칙 정의
    security_rules = "SQL 인젝션 방지를 위해 문자열 쿼리 내에 세미콜론(;) 또는 'DROP', 'OR 1=1' 구문이 포함되어서는 안 된다."
    
    # 테스트 케이스 1: 정상적인 보안 쿼리
    test_string_1 = "SELECT * FROM users WHERE name = 'JohnDoe'"
    # 테스트 케이스 2: 악의적인 인젝션 시도 쿼리
    test_string_2 = "JohnDoe'; DROP TABLE users;--"
    
    # 결정론적으로 True 반환 예상. T=0 설정으로 인해 플래키 테스트 방지.
    assert oracle.evaluate_test_case(test_string_1, security_rules) == True
    
    # 결정론적으로 False 반환 예상. 엄격한 JSON 파싱을 통해 오라클의 강건성 확보.
    assert oracle.evaluate_test_case(test_string_2, security_rules) == False
</code></pre>
<p>이 설계 아키텍처에서 가장 괄목할 만한 점은 모델의 응답 텍스트 전체를 <code>assert response == "expected string"</code> 형태로 직접 비교하는 전통적인 방식을 철저히 배제했다는 사실이다. Temperature를 0으로 설정하여 탐욕적 해독을 강제하더라도, 앞서 설명한 부동소수점 병렬 연산의 미세 오차나 동적 배치 처리의 신기루 현상(Batching Mirage)으로 인해 응답 내의 <code>reasoning</code> 필드나 문장의 조사, 구두점 등이 미세하게 진동할 잠재적 위험성을 내포하고 있다.</p>
<p>따라서 오라클 시스템 클래스는 하이퍼파라미터 수준에서 <span class="math math-inline">T=0</span>, <code>Top-P=1.0</code>, <code>Seed 고정</code>을 1차 방어선으로 구축하여 모델의 창의성을 완전히 억제하고 시스템이 허용하는 물리적 변동성을 최소화한다. 그 후 2차 방어선으로 프롬프트 엔지니어링을 통해 강력한 JSON 구조화를 강제하며, 최종적으로 애플리케이션 코드 레벨에서 잔존하는 텍스트 노이즈를 프로그래밍적으로 무시하고 핵심 데이터의 정합성 자체만 불리언(Boolean) 논리로 평가하는 ‘회복탄력적 파싱’ 체계를 결합한다. 이러한 다층적 통제 메커니즘은 통제된 닫힌 시스템(Closed system)을 구축해야 하는 현대의 엔터프라이즈 소프트웨어 테스트 환경에서, 태생적으로 확률론적인 AI 모델을 가장 신뢰할 수 있는 결정론적 오라클로 변모시키기 위한 필수불가결한 아키텍처 패턴이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>A Comprehensive Guide to LLM Temperature 🔥🌡️, https://towardsdatascience.com/a-comprehensive-guide-to-llm-temperature/</li>
<li>Decoding LLM Outputs: A Beginner’s Guide to Sampling Strategies, https://medium.com/@xiaxiami/decoding-llm-outputs-a-beginners-guide-to-sampling-strategies-e0fa8d616924</li>
<li>Mathematical Insights into Temperature Scaling and Hallucination in, https://dannybutvinik.medium.com/mathematical-insights-into-temperature-scaling-and-hallucination-in-large-language-models-85e55ca1f5b3</li>
<li>Why Does My LLM Have A Temperature? - Nigel Gebodh, https://ngebodh.github.io/projects/Short_dive_posts/LLM_temp/LLM_temp.html</li>
<li>LLM Temperature - MLOps Dictionary - Hopsworks, https://www.hopsworks.ai/dictionary/llm-temperature</li>
<li>Temperature and Softmax (LLMs) - Iz’s Morning Notes, https://publish.obsidian.md/iz/Learning/AI/Temperature+and+Softmax+(LLMs)</li>
<li>Unpacking Softmax: How Temperature Drives Representation, https://arxiv.org/html/2506.01562v1</li>
<li>Does Temperature 0 Guarantee Deterministic LLM Outputs?, https://www.vincentschmalbach.com/does-temperature-0-guarantee-deterministic-llm-outputs/</li>
<li>Stop using temperature 1.0 for code generation: Advanced LLM, https://medium.com/@glanzz/stop-using-temperature-1-0-385cb51ac863</li>
<li>LLM Temperature: How It Works and When You Should Use It - Vellum, https://www.vellum.ai/llm-parameters/temperature</li>
<li>Understanding LLM Temperature: Creativity vs. Consistency | by Tahir, <a href="https://medium.com/@tahirbalarabe2/%EF%B8%8Funderstanding-llm-temperature-creativity-vs-consistency-ce2e8194ed7c">https://medium.com/@tahirbalarabe2/%EF%B8%8Funderstanding-llm-temperature-creativity-vs-consistency-ce2e8194ed7c</a></li>
<li>Can Machines Dream? On the Creativity of Large Language Models, https://towardsdatascience.com/can-machines-dream-on-the-creativity-of-large-language-models-d1d20cf51939/</li>
<li>Survey and analysis of hallucinations in large language models, https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1622292/full</li>
<li>Would Temperature Control Help Against ChatGPT’s Hallucinations?, https://medicalfuturist.com/would-temperature-control-help-against-chatgpts-hallucinations</li>
<li>LLM Temperature Setting: Control Randomness &amp; Creativity, https://blog.promptlayer.com/temperature-setting-in-llms/</li>
<li>The Dance of Hallucination and Creativity in LLMs’ Decoding Layers, https://arxiv.org/html/2503.02851v1</li>
<li>ChatAssert: LLM-Based Test Oracle Generation With External Tools, https://www.computer.org/csdl/journal/ts/2025/01/10804561/22INHprJQ3e</li>
<li>TOGLL: Correct and Strong Test Oracle Generation with LLMS, https://ieeexplore.ieee.org/iel8/11029684/11029718/11029748.pdf</li>
<li>LLM Temperature Settings: A Complete Guide for Developers - Tetrate, https://tetrate.io/learn/ai/llm-temperature-guide</li>
<li>Understanding LLM-Driven Test Oracle Generation - arXiv, https://arxiv.org/html/2601.05542v1</li>
<li>Understanding and Mitigating Numerical Sources of … - arXiv.org, https://arxiv.org/html/2506.09501v2</li>
<li>Foundation model parameters: decoding and stopping criteria - IBM, https://www.ibm.com/docs/en/watsonx/saas?topic=prompts-model-parameters-prompting</li>
<li>Can you ELI5 why a temp of 0 is bad? : r/LocalLLaMA - Reddit, https://www.reddit.com/r/LocalLLaMA/comments/1j10d5g/can_you_eli5_why_a_temp_of_0_is_bad/</li>
<li>Understanding LLM-Driven Test Oracle Generation - arXiv, https://www.arxiv.org/pdf/2601.05542</li>
<li>NEXUS: EXECUTION-GROUNDED MULTI-AGENT TEST ORACLE, https://openreview.net/pdf/a0c2be6b90f2e8198cc9d3d45027444c196b15da.pdf</li>
<li>Recent works based on the draft and verify framework. Temperature, https://www.researchgate.net/figure/Recent-works-based-on-the-draft-and-verify-framework-Temperature-0-refers-to-greedy_tbl1_387321059</li>
<li>Why is deterministic output from LLMs nearly impossible? - Unstract, https://unstract.com/blog/understanding-why-deterministic-output-from-llms-is-nearly-impossible/</li>
<li>Why Your LLM Still Isn’t Deterministic (And How to Fix It), https://mikulskibartosz.name/why-temperature-0-isnt-deterministic</li>
<li>[Discussion] Non deterministic behaviour in LLMs when temperature, https://www.reddit.com/r/MachineLearning/comments/16hmwcc/discussion_non_deterministic_behaviour_in_llms/</li>
<li>About topK, topP and temprature - Google AI Studio, https://discuss.ai.google.dev/t/about-topk-topp-and-temprature/33094</li>
<li>Testing AI Systems: Handling the Test Oracle Problem, https://dev.to/qa-leaders/testing-ai-systems-handling-the-test-oracle-problem-3038</li>
<li>(PDF) The Oracle Problem in Software Testing: A Survey, https://www.researchgate.net/publication/276255185_The_Oracle_Problem_in_Software_Testing_A_Survey</li>
<li>Highlights from “Do LLMs Generate Test Oracles, https://goodenoughtesting.com/articles/llms-test-generation-research-insights</li>
<li>Defeating Nondeterminism in LLM Inference: What It Unlocks for, https://www.propelcode.ai/blog/defeating-nondeterminism-in-llm-inference-ramifications</li>
<li>Exploring token sampling controls in large language models, https://ondiversity.cloud/blog/exploring-token-sampling-controls-in-large-language-models-a-comprehensive-guide</li>
<li>Non-Distinguishable Inconsistencies as a Deterministic Oracle … - NESA, https://nesa.zju.edu.cn/download/zqy_pdf_ccs22_ndi.pdf</li>
<li>umnsec/ndi: Non-Distinguishable Inconsistencies as a Deterministic, https://github.com/umnsec/ndi</li>
<li>Inconsistent outputs with deterministic sampling (temperature=0, https://github.com/vllm-project/vllm/issues/17759</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>