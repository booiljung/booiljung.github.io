<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:4.2.2 Top-P(Nucleus Sampling)와 Top-K의 상호작용 및 최적 조합</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>4.2.2 Top-P(Nucleus Sampling)와 Top-K의 상호작용 및 최적 조합</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 4. AI 모델 응답의 일관성 확보를 위한 프롬프트 엔지니어링 및 파라미터 제어</a> / <a href="index.html">4.2 일관성 확보를 위한 모델 하이퍼파라미터(Hyperparameter) 정밀 제어</a> / <span>4.2.2 Top-P(Nucleus Sampling)와 Top-K의 상호작용 및 최적 조합</span></nav>
                </div>
            </header>
            <article>
                <h1>4.2.2 Top-P(Nucleus Sampling)와 Top-K의 상호작용 및 최적 조합</h1>
<p>인공지능, 특히 거대 언어 모델(Large Language Model, LLM)을 활용한 소프트웨어 개발 패러다임에서 모델 응답의 일관성과 예측 가능성은 전체 시스템의 신뢰성을 좌우하는 가장 중요한 요소다. 소프트웨어 테스트 자동화 환경이나 파이프라인 내에서 AI의 출력을 검증 기준으로 삼기 위해서는 반드시 결정론적 정답지(Deterministic Ground Truth)를 제공할 수 있는 오라클(Oracle)이 구축되어야 한다. 그러나 LLM이 텍스트를 생성하는 본질적인 과정은 자가회귀(Autoregressive) 기반의 확률적 디코딩(Decoding)에 의존하고 있으며, 이는 필연적으로 소프트웨어 엔지니어링이 요구하는 엄격한 결정론(Determinism)과 정면으로 충돌한다.</p>
<p>자연어 생성 모델은 주어진 문맥을 바탕으로 어휘 사전(Vocabulary)에 속한 수만 개의 토큰(Token)들에 대해 다음 위치에 등장할 확률 분포를 계산하고, 이 분포에서 하나의 토큰을 무작위로 추출(Sampling)하는 수학적 연산을 반복한다. 이러한 확률적 특성은 AI에게 창의성과 인간다운 자연스러움을 부여하지만, 성공과 실패의 기준이 1비트의 오차도 없이 명확해야 하는 테스트 오라클 시스템에서는 치명적인 비결정성(Nondeterminism) 버그를 유발하는 핵심 원인으로 작용한다.</p>
<p>이러한 무작위성을 제어하고 언어 모델을 예측 가능한 소프트웨어 컴포넌트로 규격화하기 위해 고안된 핵심 하이퍼파라미터(Hyperparameter)가 바로 Top-K와 Top-P(Nucleus Sampling)이다. 이 두 파라미터는 모델의 출력 확률 분포에서 신뢰할 수 없는 꼬리(Unreliable Tail) 영역을 물리적으로 절단(Truncation)하여 극단적인 이상치(Outlier) 토큰이 생성될 가능성을 차단한다. 본 절에서는 Top-K와 Top-P의 수학적 원리와 구조적 한계를 해부하고, 이 두 파라미터가 디코딩 파이프라인 내부에서 어떻게 직렬로 상호작용하는지 분석한다. 나아가 AI 기반 소프트웨어 테스트에서 확정적 오라클을 구축하기 위한 최적의 파라미터 조합 전략과 실전 엔지니어링 예제를 심도 있게 다룬다.</p>
<h2>1.  확률적 텍스트 생성의 딜레마와 디코딩 전략의 진화</h2>
<p>언어 모델이 다음 토큰을 예측할 때, 신경망의 마지막 선형 계층은 어휘 사전 <span class="math math-inline">V</span>의 크기와 동일한 차원의 로짓(Logit) 벡터 <span class="math math-inline">z_t</span>를 출력한다. 이 원시 로짓 값들은 소프트맥스(Softmax) 함수를 통과하며 합이 1인 확률 분포 <span class="math math-inline">P(y_t \vert y_{&lt;t})</span>로 변환된다.<br />
<span class="math math-display">
P(y_t = i \vert y_{&lt;t}) = \frac{\exp(z_{t, i} / T)}{\sum_{j \in V} \exp(z_{t, j} / T)}
</span><br />
위 수식에서 <span class="math math-inline">T</span>는 Temperature(온도) 파라미터로, 확률 분포의 형태를 평탄하게(<span class="math math-inline">T &gt; 1</span>) 만들거나 뾰족하게(<span class="math math-inline">T &lt; 1</span>) 만드는 스케일링 역할을 수행한다. 만약 매 생성 단계마다 확률이 가장 높은 단일 토큰만을 확정적으로 선택하는 탐욕 탐색(Greedy Decoding) 방식을 취한다면 출력은 완벽히 결정론적이겠지만, 문맥이 길어질수록 모델이 동일한 단어나 구문을 무한히 반복하는 ’반복 함정(Boredom Trap)’에 빠지거나 인간의 언어 통계와 괴리된 텍스트를 생성하는 심각한 텍스트 퇴화(Text Degeneration) 현상이 발생한다. 반대로 확률 분포에 기반하여 토큰을 무작위로 추출하는 순수 샘플링(Pure Sampling) 기법을 사용하면, 확률이 0에 가깝지만 완전히 0은 아닌 수만 개의 토큰이 희박한 확률로 선택될 위험을 내포하게 된다.</p>
<p>특히 양(Yang) 등의 연구에서 지적된 ‘소프트맥스 병목(Softmax Bottleneck)’ 현상에 따르면, 신경망의 은닉층 차원이 어휘 사전의 크기보다 작을 경우 출력 계층의 저랭크(Low-rank) 행렬 투영으로 인해 모델은 이상적인 목표 확률 분포를 완벽하게 모사할 수 없으며, 필연적으로 꼬리 영역의 토큰들에 불필요한 확률 질량을 과다 할당하게 된다. 이로 인해 형성된 ‘신뢰할 수 없는 꼬리(Unreliable Tail)’ 토큰이 단 한 번이라도 디코딩 과정에서 문맥에 섞여 들어가면, 이후의 자가회귀 생성 과정 전체가 궤도를 이탈하여 논리적으로 완전히 붕괴된 환각(Hallucination) 텍스트를 쏟아내게 된다.</p>
<p>따라서 소프트웨어 구조 내에서 결정론적 규칙을 준수해야 하는 오라클을 구축하기 위해서는 이 신뢰할 수 없는 꼬리를 수학적으로 절단하여 배제하는 컷오프(Cut-off) 메커니즘이 필수적이며, Top-K와 Top-P는 이 목적을 달성하기 위한 가장 표준적인 접근법이다.</p>
<h2>2.  Top-K 샘플링: 고정 윈도우 기반의 하드 필터링 메커니즘</h2>
<p>Top-K 샘플링은 확률 분포의 꼬리를 자르는 가장 직관적인 형태의 수학적 절단 방법론이다. 2018년 Fan et al.이 발표한 논문 “Hierarchical Neural Story Generation“에서 이야기 생성의 일관성을 높이고 문맥의 응집력을 유지하기 위해 본격적으로 도입된 이 기법은, 전체 어휘 사전의 확률 분포 중 가장 확률이 높은 상위 <span class="math math-inline">K</span>개의 토큰만을 후보군으로 남기고 나머지 모든 토큰의 확률을 0으로 강제 할당하는 방식이다.</p>
<h3>2.1  Top-K의 수학적 정의와 연산 과정</h3>
<p>수학적 관점에서 Top-K 샘플링은 확률 내림차순으로 정렬된 토큰 집합에서 고정된 개수의 최상위 요소만을 부분 집합으로 추출하는 매핑 함수로 정의된다. 전체 어휘 사전 <span class="math math-inline">V</span>의 크기를 <span class="math math-inline">|V|</span>라 할 때, 확률 내림차순으로 정렬된 토큰의 집합을 <span class="math math-inline">V&#39; = {x^{(1)}, x^{(2)}, \dots, x^{(|V|)}}</span>라고 가정하자. 즉, <span class="math math-inline">P(x^{(1)} \vert y_{&lt;t}) \ge P(x^{(2)} \vert y_{&lt;t}) \ge \dots</span> 이 성립한다. Top-K 필터링은 임의의 자연수 <span class="math math-inline">K</span>에 대해 부분 집합 <span class="math math-inline">V^{(K)} = {x^{(1)}, \dots, x^{(K)}}</span>만을 후보로 유지한다. 이후 선택된 상위 <span class="math math-inline">K</span>개의 토큰이 가진 원시 확률들을 다시 재정규화(Renormalization)하여 전체 확률의 합이 1.0이 되도록 변환한다.</p>
<p>이를 테이블 내의 수학적 조건부 확률 함수로 나타내면 다음과 같다.</p>
<table><thead><tr><th><strong>조건 (Condition)</strong></th><th><strong>재정규화된 확률 P′(yt=x(i)∣y&lt;t)</strong></th></tr></thead><tbody>
<tr><td><strong><span class="math math-inline">i \le K</span> (상위 K위 이내)</strong></td><td><span class="math math-inline">\frac{P(x^{(i)} \vert y_{&lt;t})}{\sum_{j=1}^{K} P(x^{(j)} \vert y_{&lt;t})}</span></td></tr>
<tr><td><strong><span class="math math-inline">i &gt; K</span> (K위 초과)</strong></td><td><span class="math math-inline">0</span></td></tr>
</tbody></table>
<p>이 방식은 토큰의 분포 형태와 무관하게 무조건 ’개수’를 기준으로 분포 곡선에 수직선(Vertical Cut)을 그어 탐색 공간을 강제로 절단하는 메커니즘을 가진다. 따라서 모델이 엉뚱한 방향으로 발산하는 것을 방지하는 매우 강력한 하드 가드레일(Hard Guardrail) 역할을 수행한다. 예를 들어 <span class="math math-inline">K=50</span>으로 설정된 모델은 51번째 토큰이 50번째 토큰과 확률 차이가 0.0001%에 불과하더라도 가차 없이 이를 배제한다. 이러한 특성은 코드 생성이나 데이터 파싱 등에서 어휘의 범위를 예측 가능한 수준으로 제한하고자 할 때 탁월한 성능을 발휘한다.</p>
<h3>2.2  정적 윈도우 문제 (Static Window Problem)의 치명적 한계</h3>
<p>Top-K 필터링은 구현이 단순하고 연산이 빠르다는 장점이 있으나, 치명적인 구조적 한계인 **‘정적 윈도우 문제(Static Window Problem)’**를 갖는다. 텍스트 생성 과정에서 언어 모델이 느끼는 문맥적 확신도(Confidence)는 토큰의 위치마다 극적으로 변화한다. 고정된 <span class="math math-inline">K</span> 값은 이러한 분포의 동적 변화에 전혀 대응하지 못한다.</p>
<p>첫째, 모델의 확신도가 매우 높은 뾰족한 분포(Peaked Distribution)에서의 문제다. 예를 들어 “프랑스의 수도는“이라는 문맥 다음에는 “파리(Paris)“라는 토큰이 99%의 압도적인 확률을 가질 수 있다. 이때 시스템에 <span class="math math-inline">K=50</span>이 설정되어 있다면, 정답과 무관하고 확률이 0.001%에 불과한 “바나나”, “달리기“와 같은 49개의 노이즈 토큰이 억지로 샘플링 윈도우 안에 포함된다. 결국 극단적인 무작위성(Temperature가 높은 경우)에 의해 이 노이즈 토큰이 선택될 0.1%의 리스크를 감수해야 하며, 이는 소프트웨어 검증 오라클에서 결코 용납될 수 없는 비결정성을 야기한다.</p>
<p>둘째, 모델의 확신도가 낮은 평탄한 분포(Flat Distribution)에서의 문제다. “내가 가장 좋아하는 음식은“이라는 문맥 다음에는 수십, 수백 개의 음식 이름이 각각 고만고만한 확률을 나누어 가진다. 만약 <span class="math math-inline">K=50</span>으로 설정되어 있다면, 51번째로 확률이 높지만 문맥상 여전히 자연스럽고 유효한 토큰(예: “초밥”)이 후보군에서 억울하게 잘려나간다. 이는 예측 불가능성이 용인되어야 하는 다양성 확보 구간에서조차 모델의 표현력과 탐색 공간을 부자연스럽게 제한하는 결과를 낳는다.</p>
<p>결과적으로 소프트웨어 테스트 환경에서 오라클을 구축할 때 단일한 <span class="math math-inline">K</span> 값에만 의존하는 것은 모델이 확신할 때는 불필요한 오류 리스크를 높이고, 확신하지 못할 때는 모델의 논리적 전개를 강제로 끊어버리는 양날의 검으로 작용하게 된다.</p>
<h2>3.  Top-P (Nucleus Sampling): 동적 확률 질량을 통한 문맥 적응형 제어</h2>
<p>Top-K 방식이 지닌 정적 윈도우의 경직성을 근본적으로 해결하기 위해 제안된 혁신적인 디코딩 기법이 바로 <strong>Nucleus Sampling</strong>, 통상적으로 Top-P 샘플링이라 불리는 방식이다. 이 방법론은 Holtzman et al.이 2020년 발표한 기념비적인 논문 “The Curious Case of Neural Text Degeneration“에서 처음 소개되었으며, 현재 OpenAI, Google, Anthropic 등 사실상 모든 상용 LLM API에서 다양성과 품질을 제어하는 산업 표준(Industry Standard)으로 자리 잡았다.</p>
<h3>3.1  Top-P의 핵심 직관과 수학적 정식화</h3>
<p>Top-P 샘플링의 핵심 철학은 토큰의 물리적인 ’개수’가 아니라, 유효한 토큰들이 군집을 이루며 차지하는 <strong>‘누적 확률 질량(Cumulative Probability Mass)’</strong>, 즉 분포 곡선 하단의 면적을 기준으로 꼬리를 절단하는 것이다. Top-K가 확률 분포상에 고정된 수직선(Vertical Cut)을 긋는 것이라면, Top-P는 사용자가 설정한 확률 질량이라는 기준을 만족할 때까지 토큰을 모으는 수평선(Horizontal Cut) 기반의 동적 절단 메커니즘을 가진다.</p>
<p>수학적으로 Top-P 메커니즘은 확률 내림차순으로 정렬된 토큰 리스트를 순차적으로 순회하며 확률 값을 누적하다가, 그 누적합이 사용자가 지정한 임계값 <span class="math math-inline">p</span> (예: <span class="math math-inline">0.90</span> 또는 <span class="math math-inline">90\%</span>)를 초과하는 최초의 가장 작은 부분 집합(Nucleus) <span class="math math-inline">V^{(p)}</span>를 찾아내는 과정으로 정의된다.<br />
<span class="math math-display">
\sum_{x \in V^{(p)}} P(x \vert y_{&lt;t}) \ge p
</span><br />
집합 <span class="math math-inline">V^{(p)}</span>를 구성하는 토큰들은 언어 모델의 신뢰 구간 내에 있는 핵심(Nucleus) 어휘로 간주된다. Top-K와 마찬가지로 이 핵 집합에 포함된 토큰들의 확률은 합이 1이 되도록 재정규화 과정을 거쳐 새로운 확률 분포 <span class="math math-inline">P&#39;</span>로 변환되며, 이 집합에 편입되지 못한 신뢰할 수 없는 꼬리(Unreliable Tail) 영역의 토큰들은 로짓이 <span class="math math-inline">-\infty</span>로 강제되어 확률 0으로 완전히 배제된다.<br />
<span class="math math-display">
P&#39;(y_t = x \vert y_{&lt;t}) = \begin{cases} \frac{P(x \vert y_{&lt;t})}{\sum_{x&#39; \in V^{(p)}} P(x&#39; \vert y_{&lt;t})} &amp; \text{if } x \in V^{(p)} \\ 0 &amp; \text{otherwise} \end{cases}
</span></p>
<h3>3.2  문맥 적응형(Context-Aware) 윈도우 크기 변화의 효과</h3>
<p>Top-P 필터링이 소프트웨어 오라클 생성과 같이 품질과 논리성을 동시에 담보해야 하는 환경에서 압도적인 우위를 가지는 이유는, 모델의 순간적인 확신도에 따라 후보군의 크기(어휘의 수)가 동적으로 팽창하거나 수축하는 뛰어난 적응력(Adaptability) 때문이다.</p>
<ul>
<li><strong>확신도가 높을 때 (수축):</strong> 특정 문법적 키워드나 명확한 고유명사, 예를 들어 API JSON 응답에서 키(Key) 값 뒤에 콜론(<code>:</code>)이 와야 하는 상황을 가정해보자. 언어 모델은 <code>:</code> 토큰에 95%의 확률을 부여할 수 있다. 이때 <span class="math math-inline">p=0.9</span>로 설정되어 있다면, 첫 번째 토큰인 <code>:</code> 하나만으로 이미 누적 확률 <span class="math math-inline">0.9</span>를 초과 달성하게 된다. 결과적으로 <span class="math math-inline">V^{(p)}</span> 집합의 크기는 1이 되며, 모델은 오직 <code>:</code> 토큰 하나만을 확정적으로 출력하게 된다. 이는 구조화된 데이터를 파싱하는 오라클 시스템에서 문법적 무결성을 유지하는 데 결정적인 기여를 한다.</li>
<li><strong>확신도가 낮을 때 (팽창):</strong> 코딩 중 주석을 달거나 자연어 요약을 수행하는 단계처럼 다양한 표현이 가능한 평탄한 분포 구간에서는 최상위 토큰의 확률이 고작 5% 수준일 수 있다. 이때 <span class="math math-inline">p=0.9</span>를 만족시키기 위해서는 수십 개에서 수백 개의 타당한 토큰들이 집합 <span class="math math-inline">V^{(p)}</span>에 대거 편입된다. 꼬리 부분의 완벽한 넌센스 단어들만 걸러낸 채, 논리적으로 유효한 단어들을 폭넓게 유지함으로써 텍스트의 다양성과 유창성을 끌어올린다.</li>
</ul>
<p>이러한 맥락 인식(Context-Sensitive) 행동 덕분에 Top-P는 극심하게 변동하는 자연어의 통계적 특성을 파괴하지 않으면서도, 시스템에 치명적인 영향을 미치는 극단적 이상치(Extreme Outliers)의 개입만을 핀셋처럼 제거하는 탁월한 성능을 보여준다.</p>
<h3>3.3  Top-P의 한계: 평탄 분포 결함 (Flat-Distribution Flaw)</h3>
<p>Top-P가 산업 표준으로 채택되었음에도 불구하고 이 알고리즘 역시 완벽하지 않으며, 특정 조건 하에서 소프트웨어 엔지니어링의 통제 범위를 벗어나는 치명적 결함을 노출한다. 이를 **‘평탄 분포 결함(Flat-Distribution Flaw)’**이라 부른다.</p>
<p>언어 모델이 복잡한 논리적 추론 단계에서 심각한 혼란을 겪고 있거나, Temperature 값이 극단적으로 높게 설정(<span class="math math-inline">T &gt; 1.2</span>)되어 분포가 완전히 평탄해진 상황을 가정해 보자. 가장 확률이 높은 토큰조차 0.1%의 지분만을 가지고 있다면, Top-P 임계값인 누적 확률 <span class="math math-inline">95%(0.95)</span>를 채우기 위해 어휘 사전 내의 거의 모든 토큰, 심지어 특수 기호나 다른 언어의 파편 등 수천 개의 불량 노이즈 토큰까지 <span class="math math-inline">V^{(p)}</span> 집합에 포함되는 사태가 벌어진다. 즉, 모델이 혼란스러울수록 Top-P는 더 많은 쓰레기(Garbage) 데이터를 무방비로 윈도우 안에 통과시킨다. 이 결함은 코드 생성이나 SQL 쿼리 검증 오라클과 같이 문법적 엄격성이 요구되는 환경에서 예기치 않은 Syntax Error나 파싱 실패를 유발하는 주범이 된다. 이 문제를 완벽하게 방어하기 위해서는 파이프라인 내에서 Top-K와의 직렬 상호작용이 필수적으로 요구된다.</p>
<h2>4.  하이퍼파라미터 파이프라인: Top-K와 Top-P의 상호작용 및 필터링 순서</h2>
<p>앞서 살펴본 바와 같이, Top-K는 유연성이 떨어지는 대신 절대적인 탐색 상한선을 보장하며, Top-P는 적응력이 뛰어나지만 평탄한 분포에서 너무 많은 노이즈를 허용하는 약점이 있다. 상용 LLM 프레임워크와 API(예: Hugging Face, OpenAI, Google Vertex AI) 설계자들은 이 두 파라미터가 가진 상호보완적 특성에 주목하였고, 두 기법을 동시에 활성화했을 때 발생할 수 있는 시너지를 극대화하기 위해 디코딩 파이프라인 내부의 엄격한 실행 순서(Execution Order)를 수학적으로 정립하였다.</p>
<p>개발자가 API 호출 시 Temperature, Top-K, Top-P 값을 모두 전달할 경우, 모델은 이 파라미터들을 무작위로 적용하거나 병렬로 계산하는 것이 아니라 <strong>직렬적인 2단계 다중 필터링(Two-Stage Sequential Filtering)</strong> 구조에 따라 로짓(Logit)을 순차적으로 다듬어 나간다. 확정적 오라클을 설계하는 엔지니어는 이 파이프라인의 처리 순서를 완벽하게 숙지해야 의도한 대로 결정론적 출력을 강제할 수 있다.</p>
<h3>4.1  디코딩 파이프라인의 연산 흐름 (Execution Flow)</h3>
<p>언어 모델 내부에서 하나의 토큰이 샘플링되기까지 거치는 수학적 변환 파이프라인은 다음의 6단계로 진행된다.</p>
<ol>
<li><strong>원시 로짓 계산 (Raw Logits Calculation):</strong> 신경망의 최종 출력층이 전체 어휘 사전에 대한 실수 형태의 원시 점수(Raw Logits) 배열 <span class="math math-inline">z</span>를 생성한다.</li>
<li><strong>Temperature 스케일링 (Temperature Scaling):</strong> 원시 로짓을 설정된 <span class="math math-inline">T</span> 값으로 나눈다(<span class="math math-inline">z_i / T</span>). 이 단계에서는 어떠한 토큰도 탈락하지 않으며, 오직 확률 분포의 곡선 형태(평탄도 또는 첨도)만이 재구성된다.</li>
<li><strong>1차 거친 필터링: Top-K 차단 (Coarse Filtering via Top-K):</strong> 스케일링이 완료된 로짓을 내림차순으로 정렬한 뒤, 사용자가 설정한 상위 <span class="math math-inline">K</span>개의 토큰만을 남긴다. 순위에 들지 못한 나머지 모든 토큰의 로짓은 수학적으로 <span class="math math-inline">-\infty</span> (Negative Infinity)로 치환된다. 이는 해당 토큰의 등장 확률을 0으로 확정 짓는 절대적이고 무자비한 하드 컷오프(Hard Cutoff) 과정이다.</li>
<li><strong>2차 정밀 필터링: Top-P 동적 절단 (Fine-Grained Filtering via Top-P):</strong> Top-K 필터링을 무사히 통과하여 살아남은 최대 <span class="math math-inline">K</span>개의 토큰들만을 대상으로 다시 누적 확률을 계산하기 시작한다. 내림차순으로 확률을 더해가며 누적합이 <span class="math math-inline">p</span> 임계값에 도달하는 지점을 찾고, 그 기준선을 넘어서는 꼬리 토큰들을 2차적으로 <span class="math math-inline">-\infty</span>로 치환한다.</li>
<li><strong>소프트맥스 정규화 (Softmax Normalization):</strong> 1차(Top-K)와 2차(Top-P) 필터링에 의해 다수의 로짓이 <span class="math math-inline">-\infty</span>로 변환된 최종 로짓 배열에 소프트맥스 함수를 적용한다. 이를 통해 살아남은 소수의 토큰들만이 전체 확률 1.0을 나누어 가지는 유효한 확률 분포가 완성된다.</li>
<li><strong>멀티노미얼 샘플링 (Multinomial Sampling):</strong> 완성된 확률 분포를 바탕으로 난수를 발생시켜 하나의 토큰을 최종적으로 추출한다.</li>
</ol>
<p><img src="./4.2.2.0.0%20Top-PNucleus%20Sampling%EC%99%80%20Top-K%EC%9D%98%20%EC%83%81%ED%98%B8%EC%9E%91%EC%9A%A9%20%EB%B0%8F%20%EC%B5%9C%EC%A0%81%20%EC%A1%B0%ED%95%A9.assets/image-20260225193225660.jpg" alt="image-20260225193225660" /></p>
<h3>4.2  논리적 상호작용의 공학적 의미: ’안전망’으로서의 Top-K</h3>
<p>이 파이프라인 아키텍처에서 가장 주목해야 할 점론적 사실은 **“Top-K가 항상 Top-P보다 선행하여 실행된다”**는 점이다. 이러한 순차적 적용 구조는 앞서 지적된 Top-P의 ’평탄 분포 결함’을 Top-K가 완벽하게 보완하도록 설계된 정교한 방어 기제다.</p>
<p>만약 언어 모델이 추론 도중 극심한 혼란을 겪어 수만 개의 어휘가 모두 0.001% 수준의 균일한 확률을 갖게 되었다고 가정해 보자. 오직 Top-P만 <span class="math math-inline">0.95</span>로 설정되어 있다면, 시스템은 누적 확률 95%를 채우기 위해 95,000개의 무의미한 노이즈 토큰(오타, 환각 텍스트, 코드 구문 파괴 문자 등)을 모두 샘플링 풀에 집어넣어 버릴 것이다. 이는 구조화된 JSON을 출력해야 하는 오라클에게는 곧장 시스템 크래시(Crash)를 의미한다.</p>
<p>그러나 이 파이프라인에 <code>Top-K = 40</code>이라는 설정이 병행 투입되면 상황은 완전히 역전된다. 3단계 파이프라인에서 Top-K 필터가 선제적으로 작동하여 전체 10만 개의 어휘 중 순위권 밖의 99,960개 꼬리 토큰을 무자비하게 <span class="math math-inline">-\infty</span>로 밀어내 버린다. 4단계에 도달했을 때 Top-P 알고리즘이 마주하는 탐색 공간은 전체 어휘 사전이 아니라, 오직 1단계에서 살아남은 40개의 비교적 ‘안전하고 검증된’ 토큰들뿐이다. Top-P는 이제 이 40개의 안전 구역 내에서만 누적 확률을 계산하여 최종 후보군을 확정한다.</p>
<p>결론적으로 Top-K는 예측 불가능한 쓰레기 토큰이 시스템의 윈도우에 진입하는 것을 막는 <strong>물리적 방화벽(Absolute Cap)</strong> 역할을 수행하며, Top-P는 그 방화벽 내부에서 모델의 확신도에 따라 동적으로 후보군을 축소시켜 논리적 일관성과 제한된 유연성을 동시에 달성하는 <strong>지능적 미세 조정기(Smart Dynamic Filter)</strong> 역할을 맡는다. 이 완벽한 이중 필터링 콤보(Power Combo)를 통해 개발자는 모델의 창의성을 일정 부분 유지하면서도 비결정성으로 인한 런타임 에러를 원천적으로 억제할 수 있다.</p>
<h2>5.  결정론적 오라클 구축을 위한 파라미터 최적 조합 전략</h2>
<p>소프트웨어 시스템에서 LLM을 단순한 챗봇이 아닌, 테스트의 성패(Pass/Fail)를 판별하거나 비즈니스 로직을 검증하는 오라클로 기능하게 만들려면, 디코딩 단계에서부터 예측 가능성(Predictability)을 극대화하는 방향으로 하이퍼파라미터를 세팅해야 한다. 개발 중인 오라클의 역할과 목적(예: 문법 검증, 정보 추출, 추론 평가 등)에 따라 요구되는 ’엄격성’의 수준이 다르므로, 파라미터 조합 역시 다르게 적용되어야 한다. 다음은 산업계에서 검증된 세 가지 최적화된 조합 전략이다.</p>
<h3>5.1  완벽한 억제: 순수 결정론 및 구조화 추출 (Strict Determinism)</h3>
<p>API 응답 페이로드(Payload)나 데이터베이스에 직접 적재되어야 하는 JSON 객체를 추출할 때, 그리고 정규 표현식으로 검증 가능한 단답형 응답(예: “PASS”, “FAIL”)만을 생성해야 하는 원시 오라클(Atomic Oracle) 환경에서는 일말의 다양성조차 결함이 된다. 모델이 오직 학습된 가중치 상의 최고 확률에만 의존하여 기계적으로 동작하도록 강제해야 한다.</p>
<ul>
<li><strong>권장 설정:</strong> <code>Temperature = 0.0</code> (또는 지원되는 최솟값 <span class="math math-inline">0.01 \sim 0.1</span>), <code>Top-K = 1</code> (또는 매우 낮은 값 <span class="math math-inline">1 \sim 5</span>), <code>Top-P = 0.1 \sim 0.5</code></li>
<li><strong>작동 메커니즘:</strong> Temperature를 0에 수렴하게 만들면 사실상 1위 토큰의 확률이 99% 이상으로 증폭되는 탐욕 탐색(Greedy Decoding)과 동일해진다. 여기에 <code>Top-K = 1</code>을 중첩 적용하면 2위 이하의 모든 토큰은 접근이 불가능해진다. <code>Top-P</code>를 낮게 설정하면 설사 <span class="math math-inline">T</span> 값이 미세하게 커져 평탄화가 발생하더라도, 누적 확률 허들을 극도로 낮추어 1위 토큰만이 선택되도록 이중으로 락(Lock)을 건다.</li>
<li><strong>주의 사항:</strong> 극단적인 탐욕 설정은 동일한 단어가 무한 반복되는 ’반복 함정’에 빠지기 매우 쉽다. 따라서 이러한 설정은 “결과를 JSON 포맷으로 최대 50토큰 이내로 출력하라“와 같이 출력 길이를 물리적으로 짧게 제한하고 강제하는 프롬프트 및 <code>Max Tokens</code> 설정과 반드시 병행되어야 한다.</li>
</ul>
<h3>5.2  유연한 일관성: 사고의 사슬 기반 하이브리드 추론 (Hybrid Reasoning Oracle)</h3>
<p>오라클이 단순한 분류기가 아니라, 복잡한 비즈니스 로직을 검토하거나 수학적 증명을 수행하는 등 사고의 사슬(Chain-of-Thought, CoT)을 통해 중간 논리를 단계적으로 전개하여 최종 평가를 내려야 할 때가 있다. 이때 순수 결정론(Greedy)적 세팅을 적용하면, 초반 논리 전개 시 로컬 최적해(Local Minima)나 환각에 한 번 잘못 빠졌을 때 궤도를 수정할 수 있는 확률론적 탈출구가 막혀버려 결과적으로 오라클의 정확도(Accuracy) 자체가 하락하는 현상이 발생한다. 즉, 추론 능력을 극대화하면서도 구문 오류를 막는 하이브리드 세팅이 필요하다.</p>
<ul>
<li><strong>권장 설정:</strong> <code>Temperature = 0.2 \sim 0.4</code>, <code>Top-K = 10 \sim 20</code>, <code>Top-P = 0.85 \sim 0.95</code></li>
<li><strong>작동 메커니즘:</strong> 중간 수준의 낮은 Temperature를 통해 모델이 정답에 가까운 궤도(Coherence)를 이탈하지 않도록 유도하면서도, <code>Top-P = 0.90</code>을 통해 논리적 불확실성이 존재하는 분기점에서는 모델이 여러 자연스러운 표현을 탐색할 수 있는 유연성을 제공한다. 가장 핵심은 <code>Top-K = 20</code>이다. 이 값이 절대적인 가드레일로 작용하여, 모델이 창의적인 추론을 시도하다가 마크다운 코드 블록 형식을 깨거나 존재하지 않는 변수명을 지어내는 등의 최악의 꼬리 환각에 빠지는 것을 선제적으로 방어한다. 이는 오라클이 논리적 오류 없이 “사고 과정“을 유지하도록 돕는 소프트웨어 검증의 최상위 권장 설정이다.</li>
</ul>
<h3>5.3  LLM-as-a-Judge 평가 데이터 합성을 위한 제어된 다양성 (Controlled Diversity)</h3>
<p>코드 생성 AI를 테스트하기 위해 다량의 엣지 케이스(Edge Case)를 자동으로 합성하거나, 자가 일관성(Self-Consistency) 검증을 위해 동일한 프롬프트로 다수의 평가를 생성한 후 다수결로 최종 오라클 판정을 내리는 패턴(Best-of-N)을 사용할 때는 모델의 출력이 일정 수준 다변화되어야 한다. 그러나 이때 무한정의 무작위성을 허용하면 평가 데이터로서의 신뢰성을 상실한다.</p>
<ul>
<li><strong>권장 설정:</strong> <code>Temperature = 0.7 \sim 0.9</code>, <code>Top-K = 40 \sim 50</code>, <code>Top-P = 0.90 \sim 0.95</code></li>
<li><strong>작동 메커니즘:</strong> 상대적으로 높은 Temperature 값은 모델이 더 모험적이고 다양한 테스트 시나리오 시드를 창출하게 만든다. 하지만 무작위성이 높아짐에 따라 평탄 분포 결함이 발생할 위험도 커지므로, <code>Top-K = 40</code>의 비교적 넉넉한 하드 캡(Hard Cap)과 <code>Top-P = 0.95</code>를 이중 필터로 작동시킨다. 이를 통해 완전히 문법에 맞지 않거나 도메인을 벗어난 넌센스 토큰은 차단하면서, 의미적으로 유효한(Semantically Plausible) 수준 내에서의 변형(Variation)만을 확보할 수 있다.</li>
</ul>
<table><thead><tr><th><strong>오라클 활용 시나리오 (Use Case)</strong></th><th><strong>Temperature</strong></th><th><strong>Top-K</strong></th><th><strong>Top-P</strong></th><th><strong>파라미터 조합의 논리적 목표 및 효과</strong></th></tr></thead><tbody>
<tr><td><strong>강제적 구조화 파싱 (API 추출)</strong></td><td><span class="math math-inline">0.0 \sim 0.1</span></td><td><span class="math math-inline">1 \sim 5</span></td><td><span class="math math-inline">0.1 \sim 0.5</span></td><td>논리 분기가 없는 단답형 추출. 무작위성을 완전히 제거하고 구문 오류를 발생시키는 환각을 원천 차단.</td></tr>
<tr><td><strong>복합 논리 추론 검증 (CoT 판별기)</strong></td><td><span class="math math-inline">0.2 \sim 0.4</span></td><td><span class="math math-inline">10 \sim 20</span></td><td><span class="math math-inline">0.85 \sim 0.95</span></td><td>중간 추론의 전개를 허용하면서도 문법적 일관성 유지. 정적 <code>Top-K</code>로 노이즈를 치고 <code>Top-P</code>로 유동성 확보.</td></tr>
<tr><td><strong>다중 경로 평가 (Best-of-N 생성)</strong></td><td><span class="math math-inline">0.7 \sim 0.9</span></td><td><span class="math math-inline">40 \sim 50</span></td><td><span class="math math-inline">0.90 \sim 0.95</span></td><td>다수결 기반 자가 일관성 평가 시 활용. <code>Top-K</code> 방화벽을 통해 유효한 범위 내에서의 다변화 데이터만 생성.</td></tr>
</tbody></table>
<h2>6.  실전 예제: AI 기반 소프트웨어 개발의 확정적 오라클 구성법</h2>
<p>파라미터의 직렬적 필터링 상호작용 원리를 실제 엔터프라이즈 환경의 소프트웨어 개발 파이프라인과 CI/CD 오라클 구성에 적용하는 구체적인 시나리오를 살펴본다.</p>
<h3>6.1  비정형 문서 데이터 기반 엄격한 유효성 검사 오라클</h3>
<p>금융이나 물류 애플리케이션 개발 중, 송장(Invoice)이나 계약서와 같은 비정형 텍스트에서 공급자명, 날짜, 결제 금액 등을 추출하여 RDBMS(관계형 데이터베이스)에 직접 삽입해야 하는 파이프라인 모듈을 LLM으로 구축한다고 가정하자. 이 시스템을 테스트하고 검증하는 오라클 봇은 단 한 문자의 오타나 JSON 스키마 이탈도 허용해서는 안 된다. 작은 파싱 에러 하나가 백엔드 서버의 500 내부 에러(Internal Server Error)로 직결되기 때문이다.</p>
<p>만약 이 오라클을 <code>Temperature = 0.7</code>, <code>Top-P = 1.0</code> (비활성화), <code>Top-K</code> 비활성화 상태인 기본 설정으로 구동한다면, 오라클은 첫 번째 테스트 런에서는 <code>{"amount": "1,000"}</code>을 출력하여 통과하더라도, 두 번째 런에서는 모델 내부의 확률 요동으로 인해 키 값을 마음대로 변경한 <code>{"total_amount": 1000, "currency": "$"}</code> 형태를 출력하는 치명적인 구조적 변덕을 부릴 것이다.</p>
<p><strong>결정론적 오라클을 위한 파라미터 강제 설정:</strong></p>
<ul>
<li><code>temperature: 0.1</code> (가중치 확률에 대한 신뢰도 극대화)</li>
<li><code>top_p: 0.8</code> (극단적으로 평탄한 분포에서 꼬리 유입 원천 방지)</li>
<li><code>top_k: 5</code> (하드 필터링으로 스키마 파괴형 특수기호 접근 차단)</li>
</ul>
<p>이 설정을 적용하면, 디코딩 파이프라인에서 모델은 오직 <code>"amount"</code>, <code>":"</code>, <code>"1000"</code>과 같은 JSON 문법에 필수적인 최상위 예측 토큰 간의 좁은 조합 내에서만 갇혀 작동하게 된다. <code>Top-K = 5</code>로 강력하게 조여져 있으므로, 금액 사이에 콤마(<code>,</code>)를 넣을지 말지와 같이 상위 2~3개 토큰 사이의 미세한 확률 경합은 <code>Top-P</code>에 의해 안전하게 조율되지만, 기존 스키마에 없는 엉뚱한 키(Key) 값을 창조해내거나 괄호(<code>}</code>)를 열어둔 채 종료해 버리는 꼬리 토큰의 접근은 물리적으로 불가능해진다. 이로써 JSON Schema 검증 로직을 100% 통과하며 매 런마다 동일한 결과를 산출하는 결정론적 테스트 오라클이 완성된다.</p>
<h3>6.2  코드 생성 AI의 컴파일 기반 자동 검증 오라클 (LLM-as-a-Judge)</h3>
<p>소프트웨어 개발 프로세스에 AI 코딩 어시스턴트 모듈을 도입하고, 해당 모듈이 생성한 파이썬(Python) 함수가 문법(Syntax)적인 오류가 없는지, 시간 복잡도는 적절한지를 자동으로 판단하여 리뷰를 남기는 ’평가용 AI(LLM-as-a-Judge)’를 설계한다고 가정하자. 코드는 자연어와 달리 단 하나의 들여쓰기 오류나 잘못된 변수명 오기만으로도 치명적인 컴파일 에러를 발생시킨다.</p>
<p>코딩 도메인 태스크에서 AI가 마주하는 가장 큰 딜레마는 텍스트 내에서 확신도의 편차가 극도로 크다는 점이다. 패키지 임포트(<code>import numpy as np</code>)나 함수 선언부(<code>def calculate_total():</code>)를 작성할 때는 특정 토큰의 확률이 99%에 달하지만, 함수 내부의 복잡한 비즈니스 로직(알고리즘 구현)을 짤 때는 여러 줄의 논리가 가능하므로 확률 분포가 넓고 평탄해진다. 여기서 평가 오라클이 일관성 있는 리포트를 작성하도록 만들려면 <strong>Top-K와 Top-P의 상호보완적 2단 콤보</strong>가 진가를 발휘한다.</p>
<p><strong>오라클 파라미터 설정:</strong></p>
<ul>
<li><code>temperature: 0.3</code></li>
<li><code>top_k: 20</code></li>
<li><code>top_p: 0.95</code></li>
</ul>
<p>코드 리뷰 오라클은 “이 코드가 PEP8 규약을 지켰는가?” 혹은 “보안 취약점은 없는가?“라는 프롬프트의 지시에 따라 사고의 사슬(CoT)을 이용해 분석 리포트를 작성하기 시작한다. 이때 <code>Top-K = 20</code> 설정은 논리적 불확실성이 커져 확률 분포가 무너지는 순간이 오더라도, 코딩과 무관한 쓰레기 토큰이나 무의미한 유니코드 기호가 평가 리포트에 섞여 들어가는 것을 원천 봉쇄한다. 모델은 오직 <code>if</code>, <code>for</code>, <code>return</code>, <code>보안</code>, <code>취약점</code> 등 상위 20개 내의 제한된 어휘 풀 내에서만 문장을 이어간다. 동시에 그 방화벽 안에서 <code>Top-P = 0.95</code>가 작동하여, 함수 선언부 평가처럼 확신도가 높을 때는 1~2개 단어만으로 빠르고 단호하게 결론을 내리고, 내부 알고리즘을 분석할 때는 여러 문맥을 고려하여 유연하고 풍부한 피드백을 완성해 나간다. 그 결과, CI/CD 파이프라인 내에서 오라클은 인간 리뷰어와 흡사한 유창성을 지니면서도 매 실행마다 일관된 양식의 결정론적 리포트를 반복 생성하게 된다.</p>
<h2>7.  결론</h2>
<p>인공지능을 활용한 소프트웨어 개발 패러다임에서, 근본적으로 비결정적인 거대 언어 모델을 테스트하고 통제하기 위한 오라클을 구축하는 일은 결국 ’확률적 무질서를 어떻게 수학적인 결정론적 질서로 포획할 것인가’의 엔지니어링 과제로 귀결된다.</p>
<p>단순히 Temperature를 0으로 내리거나 극단적인 설정값 하나에 의존하는 것만으로는 부동소수점 연산 오차로 인한 비결정성이나 모델이 로컬 최적해에 빠지는 구조적 한계를 완전히 극복할 수 없다. 오라클의 신뢰성과 안전성을 보장하기 위해서는 디코딩 파이프라인 내에서 **Top-K의 ‘정적이고 절대적인 탐색 공간 차단망’**과 **Top-P의 ‘동적이고 맥락 인식적인 확률 질량 제어’**를 적절한 순서로 배치하여 상호 보완적인 시너지를 이끌어내야 한다.</p>
<p>AI 기반 소프트웨어 시스템을 다루는 엔지니어는 Top-K 파라미터를 통해 최악의 환각과 구문 파괴를 일으키는 언어 모델의 꼬리 영역(Unreliable Tail)을 물리적으로 절단해야 한다. 이후 이렇게 통제된 안전 구역(Safe Zone) 내부에서 Top-P를 활용하여 모델의 확신도에 맞게 유연성을 조율함으로써, 출력의 무결성과 추론의 유창성을 동시에 획득해야 한다. 이러한 하이퍼파라미터의 정밀한 수학적 조작과 파이프라인 설계야말로 끊임없이 요동치는 확률적 AI를 엔터프라이즈 환경의 신뢰할 수 있는 결정론적 소프트웨어 컴포넌트로 길들이는 가장 본질적이고 강력한 기술이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Challenges in Testing Large Language Model Based Software - arXiv, https://arxiv.org/html/2503.00481v1</li>
<li>LLM testing: Key types &amp; how to start - Tricentis, https://www.tricentis.com/learn/llm-testing</li>
<li>Prompt Engineering Explained: Understanding Top-K, Top-P, https://medium.com/@egopgogojob/prompt-engineering-explained-understanding-top-k-top-p-temperature-and-advanced-techniques-b7ae7fa49fda</li>
<li>LLM Sampling: Temperature, Top-K, Top-P, and Min-P Explained, https://www.letsdatascience.com/blog/llm-sampling-temperature-top-k-top-p-and-min-p-explained</li>
<li>A Deep Dive into Temperature, Top-K, and Top-P in LLMs - Medium, https://medium.com/@saravanan.cs/from-user-to-power-user-a-deep-dive-into-temperature-top-k-and-top-p-in-llms-479b16a2172a</li>
<li>THE CURIOUS CASE OF NEURAL TEXT DeGENERATION | SciSpace, https://scispace.com/pdf/the-curious-case-of-neural-text-degeneration-2xplgl0dw1.pdf</li>
<li>Mirostat: a neural text decoding algorithm that directly controls, https://scispace.com/pdf/mirostat-a-neural-text-decoding-algorithm-that-directly-123coepc1o.pdf</li>
<li>Closing the Curious Case of Neural Text Degeneration - OpenReview, https://openreview.net/forum?id=dONpC9GL1o</li>
<li>CLOSING THE CURIOUS CASE OF NEURAL TEXT DEGENERATION, https://proceedings.iclr.cc/paper_files/paper/2024/file/34899013589ef41aea4d7b2f0ef310c1-Paper-Conference.pdf</li>
<li>[1805.04833] Hierarchical Neural Story Generation - arXiv, https://arxiv.org/abs/1805.04833</li>
<li>[PDF] Hierarchical Neural Story Generation - Semantic Scholar, https://www.semanticscholar.org/paper/Hierarchical-Neural-Story-Generation-Fan-Lewis/29de7c0fb3c09eaf55b20619bceaeafe72fd87a6</li>
<li>Unleashing the potential of prompt engineering for large language, https://pmc.ncbi.nlm.nih.gov/articles/PMC12191768/</li>
<li>Foundations of Top- - k - Decoding for Language Models - NeurIPS, https://neurips.cc/virtual/2025/poster/118990</li>
<li>Top-p vs top-k: Sampling strategy comparison - Statsig, https://www.statsig.com/perspectives/top-vs-top-sampling</li>
<li>The Curious Case of Neural Text Degeneration - OpenReview, https://openreview.net/forum?id=rygGQyrFvH</li>
<li>[1904.09751] The Curious Case of Neural Text Degeneration - arXiv, https://arxiv.org/abs/1904.09751</li>
<li>Top-p sampling - Wikipedia, https://en.wikipedia.org/wiki/Top-p_sampling</li>
<li>Can someone explain what Top K and Top P are and what they do, https://www.reddit.com/r/AIDungeon/comments/1eppgyq/can_someone_explain_what_top_k_and_top_p_are_and/</li>
<li>Complete Guide to Prompt Engineering with Temperature and Top-p, https://promptengineering.org/prompt-engineering-with-temperature-and-top-p/</li>
<li>Order of execution of Top-K, Top-P sampling along with temperature, https://discuss.huggingface.co/t/order-of-execution-of-top-k-top-p-sampling-along-with-temperature/55569</li>
<li>Top-P Sampling: What Is It and Why Does It Matter? - DataAnnotation, https://www.dataannotation.tech/blog/top-p-sampling</li>
<li>Beyond temperature: Tuning LLM output with top-k and top-p - Medium, https://medium.com/google-cloud/beyond-temperature-tuning-llm-output-with-top-k-and-top-p-24c2de5c3b16</li>
<li>Sample the next token from a probability distribution using top-k and, https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317</li>
<li>Deterministic Software Testing vs Non-deterministic LLM Agent, https://medium.com/@promptedmind28/deterministic-software-testing-vs-non-deterministic-llm-agent-testing-what-you-need-to-know-f3abd5f9009d</li>
<li>Understanding Top-K and Top-P in Prompt Engineering - Medium, https://medium.com/@8926581/understanding-top-k-and-top-p-in-prompt-engineering-00a3b93dcd40</li>
<li>temperature, topK, and topP in LLMs | by Karthik Kalahasthi | Medium, https://medium.com/@droidnext/temperature-topk-and-topp-in-llms-ee5f262250ac</li>
<li>How to Tune LLM Parameters for Top Performance - phData, https://www.phdata.io/blog/how-to-tune-llm-parameters-for-top-performance-understanding-temperature-top-k-and-top-p/</li>
<li>A Guide to Controlling LLM Model Output: Exploring Top-k, Top-p, https://ivibudh.medium.com/a-guide-to-controlling-llm-model-output-exploring-top-k-top-p-and-temperature-parameters-ed6a31313910</li>
<li>Why is deterministic output from LLMs nearly impossible? - Unstract, https://unstract.com/blog/understanding-why-deterministic-output-from-llms-is-nearly-impossible/</li>
<li>Understanding LLM-Driven Test Oracle Generation - arXiv, https://arxiv.org/html/2601.05542v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>