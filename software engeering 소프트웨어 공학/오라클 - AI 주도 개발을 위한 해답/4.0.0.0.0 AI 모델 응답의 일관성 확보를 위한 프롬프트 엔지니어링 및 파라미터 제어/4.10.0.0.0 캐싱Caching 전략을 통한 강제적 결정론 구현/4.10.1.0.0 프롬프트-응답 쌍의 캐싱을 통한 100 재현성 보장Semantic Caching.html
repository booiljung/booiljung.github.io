<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:4.10.1 프롬프트-응답 쌍의 캐싱을 통한 100% 재현성 보장(Semantic Caching)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>4.10.1 프롬프트-응답 쌍의 캐싱을 통한 100% 재현성 보장(Semantic Caching)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 4. AI 모델 응답의 일관성 확보를 위한 프롬프트 엔지니어링 및 파라미터 제어</a> / <a href="index.html">4.10 캐싱(Caching) 전략을 통한 강제적 결정론 구현</a> / <span>4.10.1 프롬프트-응답 쌍의 캐싱을 통한 100% 재현성 보장(Semantic Caching)</span></nav>
                </div>
            </header>
            <article>
                <h1>4.10.1 프롬프트-응답 쌍의 캐싱을 통한 100% 재현성 보장(Semantic Caching)</h1>
<p>인공지능을 활용한 소프트웨어 개발 패러다임에서 가장 해결하기 어려운 기술적 난제는 대형 언어 모델(Large Language Model, LLM)이 본질적으로 지니고 있는 확률적(Stochastic)이고 비결정론적(Nondeterministic)인 특성을 통제하는 것이다. 전통적인 소프트웨어 엔지니어링 환경은 동일한 입력이 주어졌을 때 언제나 동일한 출력을 반환하는 엄격한 결정론적(Deterministic) 함수 모델에 절대적으로 의존한다. 소프트웨어의 품질을 보증하는 테스트 자동화와 연속적 통합 및 배포(CI/CD) 파이프라인의 핵심인 ‘오라클(Oracle)’ 역시 이러한 결정론을 바탕으로 시스템의 정답과 오답을 판별한다. 그러나 대형 언어 모델은 신경망 내부의 샘플링 알고리즘과 온도(Temperature) 파라미터의 미세한 작용으로 인해, 의미론적으로 완벽하게 동일한 질문이라 할지라도 토큰의 배치나 어휘 선택이 달라짐에 따라 매번 완전히 다른 텍스트를 생성해내는 한계를 지닌다.</p>
<p>이러한 언어 모델의 비결정성은 자동화된 회귀 테스트(Regression Testing)를 무력화시키고, 복잡한 자율 에이전트(Autonomous Agent) 기반 시스템에서 디버깅이 불가능한 예측 불가능한 연쇄 오류를 유발하는 주된 원인이 된다. 이러한 근본적인 한계를 극복하고, 확률론적인 인공지능 모델을 전통적인 소프트웨어 스택처럼 예측 가능하고 통제 가능한 상태 기계(State Machine)로 다루기 위해 등장한 핵심 아키텍처가 바로 의미론적 캐싱(Semantic Caching)이다.</p>
<p>의미론적 캐싱은 단순히 애플리케이션의 응답 지연 시간(Latency)을 단축하고 고가의 API 호출 비용을 절감하기 위한 성능 최적화 기법에 머물지 않는다. 테스트와 검증의 관점에서 바라볼 때, 이 기술은 무한한 변형을 가진 자연어 입력을 수용하면서도 특정 의도에 대해 사전에 검증된 과거의 출력을 100% 동일하게 강제로 반환하게 만드는 ‘결정론적 방화벽’ 역할을 수행한다. 본 절에서는 프롬프트와 응답 쌍의 의미론적 캐싱을 통해 인공지능 시스템에 완벽에 가까운 재현성(Reproducibility)을 부여하고, 이를 확정적 검증 오라클로 기능하게 만드는 심층적인 아키텍처 설계와 구현 전략, 그리고 수학적 기반을 상세히 분석한다.</p>
<h2>1. 전통적 캐싱의 한계와 의미론적 매칭의 패러다임 전환</h2>
<p>일반적인 웹 애플리케이션이나 데이터베이스 아키텍처에서 오랫동안 사용되어 온 전통적인 캐싱 기법(Lexical Caching 또는 Exact Match Caching)은 키(Key) 값의 ’정확한 일치’를 절대적인 전제 조건으로 삼는다. 전통적인 캐시 시스템은 사용자의 입력 데이터를 해시(Hash) 함수를 통해 고유한 식별자로 변환하여 메모리에 저장한다. 예를 들어 입력 문자열을 SHA-256과 같은 암호화 해시 알고리즘으로 처리할 경우, 입력값의 띄어쓰기 하나나 대소문자 하나만 달라져도 눈사태 효과(Avalanche Effect)에 의해 완전히 다른 해시 값이 도출된다.</p>
<p>이러한 특성으로 인해 자연어를 인터페이스로 사용하는 대형 언어 모델 환경에서는 전통적인 캐싱 방식이 사실상 무용지물이 된다. 사용자가 “미국의 수도는 어디인가?“라고 묻는 것과 “미국의 수도를 알려줄 수 있는가?“라고 묻는 것은 인간의 인지적 관점에서는 100% 동일한 의도(Intent)를 내포하고 있지만, 전통적인 컴퓨팅 시스템에서는 해시 충돌을 일으키지 않는 완전히 독립적인 두 개의 키로 인식된다. 그 결과 시스템은 명백한 캐시 미스(Cache Miss)를 발생시키며, 백엔드의 대형 언어 모델에 동일한 맥락의 연산을 중복해서 요청하게 된다. 사용자의 자연어 입력 패턴은 그 변형의 수가 수학적으로 무한에 가깝기 때문에, 이러한 정확한 일치 기반 캐싱은 대규모 트래픽을 처리하는 환경에서 매우 낮은 캐시 적중률(Hit Ratio)을 보일 수밖에 없으며, 결과적으로 어떠한 재현성도 보장하지 못한다.</p>
<p>이와 대조적으로 의미론적 캐싱은 텍스트의 표면적이고 구문론적인 형태가 아닌, 그 이면에 숨겨진 ’의미(Meaning)’와 ’의도(Intent)’를 추출하여 캐시의 키로 활용하는 패러다임의 전환을 제시한다. 이를 구현하기 위해 시스템은 사용자의 자연어 프롬프트를 임베딩 모델(Embedding Model)을 거쳐 고차원의 연속적인 밀집 벡터(Dense Vector) 공간의 한 좌표로 변환한다. 이 고차원 공간 내에서 의미론적으로 유사한 프롬프트들은 기하학적으로 매우 가까운 군집(Cluster)을 형성하게 된다. 새로운 프롬프트가 시스템에 입력되면, 캐시 매니저는 이 새로운 프롬프트의 벡터와 캐시 데이터베이스에 이미 저장되어 있는 기존 프롬프트 벡터들 간의 수학적 거리를 측정하여 의도의 유사성을 판별한다.</p>
<p>이러한 매커니즘은 소프트웨어 검증 영역에서 매우 중대한 의미를 갖는다. 의미론적 캐싱 층이 대형 언어 모델 전면에 배치되면, 이는 “유사한 의도를 가진 모든 입력 군집에 대해 완벽하게 동일하고 결정론적인 과거의 출력을 강제로 반환하는 오라클 층(Oracle Layer)“으로 진화하게 된다. 품질 보증(QA) 엔지니어나 자동화된 테스트 스크립트가 프롬프트의 조사나 어순을 미세하게 다르게 작성하여 입력하더라도, 해당 입력이 벡터 공간 내에서 설정된 임계값 반경 안에 들어온다면, 시스템은 백엔드 모델의 확률적 생성을 완전히 우회한다. 그리고 과거에 이미 검증을 마친, 비트 단위까지 완벽히 동일한(Bit-reproducible) 정답 텍스트를 즉각적으로 반환하게 되는 것이다.</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>전통적 정밀 캐싱 (Exact Match Caching)</strong></th><th><strong>의미론적 캐싱 (Semantic Caching)</strong></th><th><strong>테스트 오라클로서의 파급 효과</strong></th></tr></thead><tbody>
<tr><td><strong>일치 기준</strong></td><td>문자열의 구문론적, 해시값 완전 일치</td><td>임베딩 벡터 간의 수학적 거리 및 코사인 유사도</td><td>무한한 자연어 변형을 유한한 테스트 케이스로 수렴시킴</td></tr>
<tr><td><strong>검색 매커니즘</strong></td><td>해시 테이블 룩업 (O(1) 시간 복잡도)</td><td>K-최근접 이웃(KNN) 및 근사 최근접 이웃(ANN) 검색</td><td>미세한 프롬프트 변경에도 테스트가 깨지지 않는 견고함 제공</td></tr>
<tr><td><strong>적중률 (Hit Rate)</strong></td><td>매우 낮음 (자연어 환경에서 사실상 무의미함)</td><td>높음 (유사한 의도를 광범위하게 포착 가능)</td><td>테스트 파이프라인의 실행 속도를 기하급수적으로 단축</td></tr>
<tr><td><strong>재현성 한계</strong></td><td>토큰 하나만 바뀌어도 새로운 모델 추론이 발생하여 응답이 변동됨</td><td>임계값 내의 모든 변형에 대해 사전에 고정된 단일 응답을 반환</td><td>비결정론적 모델을 결정론적 상태 기계로 강제 변환</td></tr>
</tbody></table>
<h2>2. 임베딩 모델과 벡터 공간의 수학적 이해</h2>
<p>의미론적 캐싱이 일관된 결정론적 오라클로 동작하기 위해서는 텍스트를 수치화하는 과정과 벡터 거리 측정 알고리즘의 수학적 무결성이 절대적으로 보장되어야 한다. 시스템은 입력된 자연어 프롬프트를 처리하기 위해 일반적으로 768차원(예: <code>all-MiniLM-L6-v2</code> 모델) 또는 1536차원(예: OpenAI의 <code>text-embedding-ada-002</code> 모델)의 실수 배열로 이루어진 밀집 벡터를 생성한다. 이 과정에서 언어 모델은 수천만 건의 텍스트 코퍼스를 학습하며 획득한 문맥적 이해력을 바탕으로, 단어들의 단순한 조합을 넘어선 문장 전체의 의미론적 지형도를 이 고차원 공간에 투영한다.</p>
<p>두 프롬프트 벡터 <span class="math math-inline">A</span>와 <span class="math math-inline">B</span>가 주어졌을 때, 이들 간의 의미론적 유사도를 판별하기 위해 수학적으로 가장 널리 활용되는 지표는 코사인 유사도(Cosine Similarity)이다. 코사인 유사도는 두 벡터가 고차원 공간에서 이루는 각도의 코사인 값을 측정하는 방식으로, 텍스트의 길이나 벡터의 크기(Magnitude)에 영향을 받지 않고 오직 두 벡터가 향하는 방향성(Direction), 즉 의미의 유사성만을 순수하게 평가할 수 있다는 강력한 장점을 지닌다.</p>
<table><thead><tr><th><strong>수학적 측정 지표</strong></th><th><strong>연산 수식</strong></th><th><strong>설명 및 캐싱 시스템에서의 특징</strong></th></tr></thead><tbody>
<tr><td><strong>코사인 유사도 (Cosine Similarity)</strong></td><td><span class="math math-inline">cos(\theta) = \frac{A \cdot B}{\vert A \vert \vert B \vert}</span></td><td>두 벡터 <span class="math math-inline">A</span>와 <span class="math math-inline">B</span> 사이의 각도를 측정하여 방향성의 일치 정도를 -1에서 1 사이의 연속적인 값으로 산출한다. 값이 1에 수렴할수록 두 프롬프트의 의미가 완벽히 일치함을 뜻하며, 의미론적 캐싱에서 가장 보편적으로 채택되는 표준 지표이다.</td></tr>
<tr><td><strong>유클리디안 거리 (L2 Distance)</strong></td><td><span class="math math-inline">d(A, B) = \sqrt{\sum_{i=1}^{n} (A_i - B_i)^2}</span></td><td>두 벡터 공간 상의 최단 물리적 직선 거리를 측정한다. 임베딩 벡터들이 단위 길이 1로 정규화(Normalized)되어 있는 특수한 공간에서는 코사인 유사도와 수학적으로 완벽히 동등한 결과를 도출하며, 특정 벡터 데이터베이스 인덱싱 구조에서 성능적 이점을 제공한다.</td></tr>
<tr><td><strong>내적 (Dot Product)</strong></td><td><span class="math math-inline">A \cdot B = \sum_{i=1}^{n} A_i B_i</span></td><td>두 벡터의 각 성분을 곱하여 합산한 값으로, 벡터의 크기와 방향을 동시에 고려한다. 임베딩 벡터가 생성 단계에서 이미 정규화되어 있는 경우, 분모를 계산할 필요가 없어 연산 비용이 가장 저렴하므로 대규모 실시간 캐싱 시스템에서 널리 쓰인다.</td></tr>
</tbody></table>
<p>새로운 테스트 질의나 사용자 프롬프트가 시스템에 유입되면, 의미론적 캐시 시스템은 사전에 엄격하게 설정된 임계값(Threshold, 일반적으로 0.85에서 0.95 사이의 값)을 기준으로 분기 로직을 실행한다. 데이터베이스 검색을 통해 찾아낸 가장 유사한 벡터와의 코사인 유사도가 이 임계값을 초과하는 순간, 시스템은 백엔드 대형 언어 모델로 향하는 네트워크 요청을 차단하고 추론 연산을 생략한다. 그 대신 캐시 데이터베이스에 결합되어 있는 과거의 응답 텍스트를 즉시 반환한다.</p>
<p>이러한 차단 및 반환 매커니즘은 단순한 조건문처럼 보일 수 있으나, 대형 언어 모델 기반 애플리케이션의 회귀 테스트를 수행하는 데 있어서는 전체 시스템의 운명을 좌우하는 가장 강력한 방어선이 된다. 통합 테스트 환경에서 입력 데이터의 뉘앙스가 미세하게 달라지더라도, 그것이 임베딩 벡터 공간 내에서 동일한 군집 반경(Cluster Radius)에 속하기만 한다면, 시스템은 어떠한 환각(Hallucination) 현상이나 확률적 이탈도 허용하지 않고 오직 승인된 정답지만을 반복적으로 도출하게 된다.</p>
<p><img src="./4.10.1.0.0%20%ED%94%84%EB%A1%AC%ED%94%84%ED%8A%B8-%EC%9D%91%EB%8B%B5%20%EC%8C%8D%EC%9D%98%20%EC%BA%90%EC%8B%B1%EC%9D%84%20%ED%86%B5%ED%95%9C%20100%20%EC%9E%AC%ED%98%84%EC%84%B1%20%EB%B3%B4%EC%9E%A5Semantic%20Caching.assets/image-20260227224018395.jpg" alt="image-20260227224018395" /></p>
<h2>3. 재현성 보장과 무결성을 위한 캐싱 시스템 설계 원칙</h2>
<p>단순히 프롬프트를 임베딩 벡터로 묶어 데이터베이스에 저장하는 1차원적인 접근법만으로는 엄격한 소프트웨어 개발 환경에서 요구하는 ’100% 재현성’과 무결성을 보장할 수 없다. 임베딩 모델과 코사인 유사도 검색은 본질적으로 수학적 확률에 기반한 근사치(Approximation) 탐색에 불과하기 때문이다. 이로 인해 발생하는 잘못된 캐시 적중(False Positive), 즉 의도가 다른 프롬프트를 유사하다고 잘못 판단하여 엉뚱한 캐시 결과를 반환하는 현상은 테스트 오라클로서의 신뢰성을 근본적으로 파괴한다. GPTCache와 같은 선도적인 오픈소스 프레임워크나 최신 연구 논문들에서 제시하는, 결정론적 오라클 캐싱을 위한 심층적인 핵심 설계 원칙들은 다음과 같이 구체화된다.</p>
<h3>3.1 의도 정규화(Intent Canonicalization)와 의미론적 노이즈 제거</h3>
<p>자연어 프롬프트에 무분별하게 포함된 불용어(Stopwords)나 상투적인 인사말(“Please”, “Thank you”, “Could you help me with”) 등은 임베딩 과정에서 의미론적 노이즈(Semantic Noise)를 발생시켜 벡터 공간을 심각하게 왜곡하는 주범이 된다. 이러한 반복적이고 의미 없는 문구들은 벡터의 방향을 특정 의도가 아닌 일반적이고 보편적인 군집으로 편향되게 만든다. 이로 인해 시스템은 실제로는 완전히 다른 구체적인 요구사항을 담고 있는 두 프롬프트를 매우 유사한 것으로 오판하여 치명적인 오답을 캐시에서 꺼내어 반환하게 될 위험이 커진다.</p>
<p>완벽한 테스트 재현성을 달성하기 위해서는 프롬프트 전처리 단계에서 이러한 노이즈를 기계적으로 제거해야만 한다. 논문 <em>Semantic Caching for OLAP via LLM-Based Query Canonicalization</em>에서는 분석 시스템의 효율성을 극대화하기 위해 자연어 질의와 SQL 문법을 통합된 ’OLAP 의도 서명(OLAP Intent Signature)’으로 강제 정규화(Canonicalization)하는 기법을 소개하고 있다. 이는 거친 자연어 프롬프트에서 핵심적인 측정값, 그룹핑 기준, 필터 조건, 시간 창(Time Window) 등 결정론적 결과 도출에 직접적인 영향을 미치는 주요 속성만을 추출해 내어, 엄격하게 정형화된 JSON 스키마나 서명 형태로 매핑하는 과정이다. 테스트 자동화 환경을 구축하는 소프트웨어 엔지니어는 이와 동일한 철학을 적용하여, 프롬프트의 불필요한 장식을 걷어내고 핵심 인자(Arguments)만을 추출하여 정규화된 캐시 키를 생성함으로써 거짓 양성(False Positive)의 발생 가능성을 원천적으로 차단해야 한다.</p>
<h3>3.2 임계값의 딜레마와 결정론적 타협</h3>
<p>의미론적 캐싱 시스템을 튜닝할 때 가장 직관적으로 부딪히는 난관은 ’유사도 임계값(Similarity Threshold)’을 어떻게 설정할 것인가 하는 문제다. 사용자 응대용 챗봇이나 일반적인 질의응답 시스템의 경우, 토큰 생성 비용을 최소화하고 응답 속도를 높이기 위해 임계값을 상대적으로 낮게(예: 0.80 또는 0.85) 설정하여 캐시 적중률(Hit Rate)을 극대화하는 전략을 취한다. 이러한 환경에서는 응답이 약간 부정확하더라도 시스템 전체가 붕괴하지는 않는다.</p>
<p>하지만 소프트웨어 테스팅과 검증 오라클의 영역에서는 철학이 완전히 뒤바뀐다. CI/CD 파이프라인에서 거짓 양성(False Positive)이 발생하여 실패해야 할 테스트가 캐시된 잘못된 정답으로 인해 통과(Pass) 처리되는 것은, 비용을 아끼려다 소프트웨어의 품질을 파괴하는 치명적인 결과를 초래한다. 따라서 테스트 오라클로서의 캐시 임계값은 0.95 이상의 극도로 보수적이고 높은 수치로 고정되어야만 한다. 높은 임계값은 필연적으로 캐시 미스 비율을 높이고 백엔드 모델로의 호출 빈도를 증가시키지만, 오라클이 가져야 할 100% 무결성과 신뢰성을 확보하기 위해서는 이 상충 관계(Trade-off)에서 과감하게 성능을 일부 희생하고 정확도를 취해야만 한다.</p>
<p><img src="./4.10.1.0.0%20%ED%94%84%EB%A1%AC%ED%94%84%ED%8A%B8-%EC%9D%91%EB%8B%B5%20%EC%8C%8D%EC%9D%98%20%EC%BA%90%EC%8B%B1%EC%9D%84%20%ED%86%B5%ED%95%9C%20100%20%EC%9E%AC%ED%98%84%EC%84%B1%20%EB%B3%B4%EC%9E%A5Semantic%20Caching.assets/image-20260227224036519.jpg" alt="image-20260227224036519" /></p>
<h3>3.3 복합 캐싱(Dual-Layer Caching)과 하이브리드 검증 아키텍처</h3>
<p>소프트웨어 시스템에서 완벽한 데이터 정합성을 달성하기 위해 실무적인 인공지능 아키텍처는 의미론적 캐시라는 단일 계층에만 의존하지 않고, 다중 계층 캐싱(Tiered Caching) 전략을 도입하여 방어 계층을 두텁게 설계한다.</p>
<p>이러한 하이브리드 아키텍처의 첫 번째 방어선은 정확한 일치 기반의 렉시컬 캐시(Exact Lexical Cache)다. 애플리케이션 프롬프트 내에 포함된 구조화된 데이터 쿼리, 정확한 문법의 코드 블록, 또는 데이터베이스의 특정 식별자(ID) 조회가 주를 이루는 프롬프트의 경우, 임베딩을 수행하기 전에 문자열 해시를 통해 100% 일치 여부를 먼저 검사한다. 시스템 프롬프트(System Prompt)나 거대한 컨텍스트 블록의 정확한 재사용 여부는 이 계층에서 판별되어 불필요한 임베딩 연산 속도를 극대화하고 오탐을 예방한다. 이를 위해 아키텍처는 Redis나 Memcached와 같이 검증된 초고속 인메모리 스토어를 활용한다.</p>
<p>첫 번째 계층에서 캐시 미스가 발생한 모호한 자연어 입력에 대해서만 두 번째 계층인 의미론적 캐시(Semantic Cache)가 작동한다. 시스템은 Qdrant, Milvus, 또는 PostgreSQL의 <code>pgvector</code> 확장을 통해 구축된 전용 벡터 데이터베이스에서 근사 검색을 수행한다. 하지만 이 단계에서 벡터 검색 결과가 임계값을 넘었다고 해서 즉시 사용자나 테스트 러너에게 결과가 반환되는 것은 아니다.</p>
<p>가장 진보된 오라클 아키텍처는 세 번째 계층으로서 ’판별자로서의 언어 모델 검증기(LLM-as-a-Judge Validation)’를 도입한다. 논문 <em>Asteria</em> 프레임워크에서 심도 있게 제안된 이 접근법은 치명적인 오답 반환을 최종적으로 차단하기 위한 필수적인 안전 장치다. 근사 최근접 이웃(ANN, Approximate Nearest Neighbor) 알고리즘 검색을 통해 찾아낸 캐시 후보군을 맹신하는 대신, 시스템은 매우 빠르고 가벼운 소형 언어 모델이나 학습된 판별기 모델에 원본 프롬프트와 캐시 데이터베이스에서 꺼낸 프롬프트를 나란히 넘긴다. 그리고 판별기 모델에게 “이 두 개의 질의가 시스템 내에서 완벽히 동일한 행동과 출력을 요구하는가?“를 묻는 이진 판별(True/False) 작업을 지시한다. 이 엄격한 검증 과정을 통과해야만 해당 데이터는 최종적인 ’의미론적 캐시 적중’으로 공식 기록되어 안전하게 테스트 스크립트에 반환된다. 이를 통해 시스템은 퍼지 매칭(Fuzzy Matching)이 갖는 통계적 불확실성과 결정론적 오라클이 요구하는 절대적 무결성 사이의 거대한 격차를 완벽히 좁힐 수 있다.</p>
<h2>4. 통계적 독립성 제약과 Mnimi 캐싱 디자인 패턴</h2>
<p>대규모 테스트 환경에서 의미론적 캐싱을 전역적으로 적용할 때 발생하는 가장 치명적이고도 간과하기 쉬운 부작용은 워크플로우 내에서 요구되는 ’통계적 독립성(Statistical Independence)’의 훼손이다. 소프트웨어 개발 논문 <em>Statistical Independence Aware Caching for LLM Workflows</em> (Yihan Dai 외)에서는 기존 프레임워크들이 채택하고 있는 단순한 1대1 매핑 형태의 캐싱 아키텍처가 확률적이고 탐색적인 워크플로우를 어떻게 파괴하는지를 수학적으로 지적하고 있다.</p>
<p>이러한 파괴적 현상은 특히 자동화된 프로그램 수리(Automated Program Repair) 루프나 불확실성 추정(Uncertainty Estimation) 모듈에서 극명하게 나타난다. 예를 들어, 소스 코드의 버그를 분석하고 자동으로 수정하는 인공지능 에이전트는 동일한 실패 상황(프롬프트)에 직면했을 때, 여러 번의 서로 다른 수정을 시도(Retries)해야 하며, 솔루션의 성공 확률을 평가하기 위해 Pass@k와 같은 통계적 지표를 계산해야 한다. 이를 위해서는 매 호출마다 확률적으로 변주되는 독립적인 응답 샘플 집합이 절대적으로 필요하다. 하지만 만약 의미론적 캐시가 통계적 맥락을 고려하지 않고 무조건 첫 번째로 저장된 응답만을 영원히 반복해서 반환한다면, 에이전트는 동일한 오답 코드만을 계속해서 주입하는 무한 루프에 갇히게 되며 복구 알고리즘 전체가 실패로 귀결된다.</p>
<p>이러한 모순을 해결하기 위해 해당 연구는 <code>Mnimi</code>라는 혁신적인 캐시 디자인 패턴을 제안한다. 이 패턴은 캐싱 로직을 전역 변수처럼 처리하는 것을 지양하고, 캐시 레퍼런스의 타입(Type) 자체를 통해 통계적 제약 조건을 컴포넌트 수준에서 캡슐화(Encapsulation)한다. <code>Mnimi</code> 패턴은 워크플로우의 각 단계가 요구하는 통계적 무결성을 보장하기 위해 두 가지 핵심적인 캐시 참조 인터페이스를 제공한다.</p>
<p>첫째, <strong>Repeatable (반복 가능성 보장)</strong> 참조 타입이다. 이 객체는 메모리 내 캐시나 영구적인 디스크 캐시를 사용하여, 동일한 프롬프트 질의에 대해 언제나 완벽히 동일한 순서의 응답 시퀀스를 반환하도록 강제한다. 이는 테스트 프레임워크가 모델의 안정적인 점수 연산을 수행하거나, 회귀 테스트(Regression Testing) 단계에서 코드가 변경되지 않았음을 증명하기 위해 흔들림 없는 ’확정적 오라클’을 요구할 때 필수적으로 적용된다.</p>
<p>둘째, <strong>Independent (독립성 보장)</strong> 참조 타입이다. 이 인터페이스는 하부의 인메모리 캐시 아키텍처를 효율적으로 활용하면서도, 애플리케이션이 동일한 프롬프트로 질의할 때마다 매번 이전에 캐시되지 않은 통계적으로 완벽히 독립적인 새로운 샘플을 반환하도록 설계되어 있다. 이 모드는 코드 수리 루프의 재시도 로직이나, 다양한 생성 결과를 비교 평가하여 최적해를 찾는 수렴성(Convergence) 알고리즘 탐색 단계에서 필수적으로 요구되는 다양성을 훼손하지 않는다.</p>
<p>소프트웨어 엔지니어가 언어 모델 파이프라인에 캐싱 레이어를 구축할 때는 프레임워크의 설정 파일에서 전역적으로 캐시를 활성화(Enable)하는 안일한 접근을 피해야 한다. 대신 데코레이터(Decorator) 패턴이나 의존성 주입을 활용하여 CI/CD 테스트 러너나 통합 테스트 스크립트 내에서 코드 블록별로 제어해야 한다. 예를 들어 <code>LangWatch</code>의 프레임워크에서 <code>Scenario</code> 테스트를 구성할 때 , 각 에이전트 단계별로 <code>Repeatable</code> 혹은 <code>Independent</code> 속성을 명시적으로 주입하여, 전체 시스템의 재현성과 알고리즘 탐색에 필요한 통계적 다양성을 동시에 엄격하게 통제해야만 비로소 신뢰할 수 있는 개발 환경이 완성된다.</p>
<table><thead><tr><th><strong>Mnimi 캐시 타입</strong></th><th><strong>통계적 특성 및 목적</strong></th><th><strong>테스트 환경에서의 주요 적용 사례</strong></th></tr></thead><tbody>
<tr><td><strong>Repeatable (반복 가능성 보장)</strong></td><td>입력 프롬프트에 대해 과거에 기록된 시퀀스와 동일한 응답을 100% 동일한 순서로 확정적으로 반환함.</td><td>회귀 테스트 파이프라인, 모델 성능 비교를 위한 안정적 벤치마킹 점수 연산, 결정론적 디버깅 과정</td></tr>
<tr><td><strong>Independent (독립성 보장)</strong></td><td>캐시 인프라를 사용하되, 호출마다 이전에 반환되지 않은 독립적인 확률 샘플 집합을 신규로 제공함.</td><td>프로그램 자동 수리 루프(예: SpecFix), Pass@k 통계 계산, 다양한 해답 공간을 탐색해야 하는 에이전트의 브레인스토밍 단계</td></tr>
<tr><td><strong>Persistent (영구적 반복 보장)</strong></td><td><code>Repeatable</code>의 특성을 가지며, 메모리를 넘어 로컬 디스크나 공유 스토리지에 데이터를 영구 보존함.</td><td>팀 단위의 데이터 세트 공유, CI/CD 서버에서의 야간 빌드(Nightly Build) 간 비트 단위(Bit-reproducible) 재현성 확보</td></tr>
</tbody></table>
<h2>5. 실전 캐싱 데이터베이스 아키텍처 및 생명주기 관리 체계</h2>
<p>의미론적 캐시를 단순한 속도 개선 도구가 아닌 엄격한 소프트웨어 테스트 오라클로 활용하기 위해서는 캐시 데이터베이스 구조와 데이터의 생명주기(Lifecycle) 자체에 대한 고도의 엔지니어링 설계가 뒷받침되어야 한다. 랭체인(LangChain)과 통합되는 <code>GPTCache</code>와 같은 상용 등급의 오픈소스 프레임워크는 내부적으로 SQLite, PostgreSQL, Milvus, Redis 등 다양한 백엔드 스토리지 엔진을 플러그인 형태로 지원하지만 , 프로덕션 급의 완벽한 신뢰성과 추적성을 확보하기 위해서는 메타데이터가 치밀하게 결합된 커스텀 스키마 관리가 필수적이다.</p>
<p>캐시 테이블(예를 들어 PostgreSQL의 <code>pgvector</code> 확장을 활용한 관계형 벡터 스토어)은 과거처럼 단순히 입력된 프롬프트와 생성된 텍스트 응답만을 쌍으로 묶어 저장하는 수준에 머물러서는 안 된다. 오라클로서의 무결성을 증명하기 위해서는 다음과 같은 메타데이터가 반드시 함께 저장되고 인덱싱되어야 한다.</p>
<ul>
<li><code>query_text</code>: 사용자가 입력한 원본 프롬프트의 구문론적 텍스트 전체.</li>
<li><code>query_embedding</code>: 임베딩 모델을 통해 변환된 768차원 또는 1536 차원의 실수형 밀집 벡터.</li>
<li><code>response_payload</code>: 생성된 최종 텍스트 응답뿐만 아니라, 시스템이 논리를 전개한 사고의 사슬(Chain-of-Thought) 중간 과정과 구조화된 JSON 출력 포맷 전체.</li>
<li><code>model_version_id</code>: 해당 응답을 생성했던 백엔드 언어 모델의 정확한 버전 해시(예: <code>gpt-4-0613</code>). 언어 모델의 파라미터나 가중치가 업데이트되면 출력 텍스트의 뉘앙스나 포맷팅 규칙이 미세하게 변할 수 있으므로, 타겟 모델 버전이 달라지는 순간 관련된 과거 캐시는 논리적으로 즉각 무효화되어야 한다.</li>
<li><code>embedding_model_id</code>: 벡터 생성에 사용된 임베딩 모델의 식별자. 임베딩 모델이 변경되면 다차원 벡터 공간의 수학적 좌표계 자체가 완전히 뒤틀리게 되는 의미론적 표류(Semantic Drift) 현상이 발생하므로, 기존 벡터를 통한 코사인 유사도 검색은 완전히 파괴된다.</li>
<li><code>environment_context</code>: 타임스탬프, 임시 세션 ID, 무작위 생성된 트랜잭션 키 등 캐시 적중 평가 단계에서 의도적으로 무시해야 할 메타데이터 목록. CI/CD 자동화 환경에서는 타임스탬프나 동적으로 변하는 컨테이너 환경 변수를 캐시 키 검색에서 명시적으로 제외(Ignore)하는 엄격한 필터링 로직을 적용해야만 테스트의 재현성이 유지된다.</li>
</ul>
<h3>5.1 파괴적 오라클 방지를 위한 퇴거 및 무효화(Eviction and Invalidation) 전략</h3>
<p>테스트 오라클로서 작동하는 캐시 시스템은 애플리케이션의 핵심 비즈니스 로직이 수정되거나 검색 증강 생성(RAG) 시스템의 기반이 되는 지식 코퍼스(Knowledge Corpus)가 업데이트되었을 때, 지체 없이 관련 데이터가 무효화되어야 한다. 변경된 도메인 환경에도 불구하고 잘못된 과거의 낡은 정답지를 계속해서 반환하는 캐시 시스템은 ’독성 오라클(Toxic Oracle)’로 전락하여 애플리케이션 전체에 심각한 논리적 기술 부채(Technical Debt)를 양산하게 된다.</p>
<p>이를 통제하기 위해 시스템은 정교한 캐시 만료 및 퇴거 정책을 수립해야 한다. 일반적으로 널리 쓰이는 LRU(Least Recently Used, 가장 오래전에 사용된 데이터 삭제)나 LFU(Least Frequently Used, 가장 적게 사용된 데이터 삭제)와 같은 단순한 휴리스틱 정책은 테스트 오라클 환경에는 적합하지 않다. 테스트 환경에서는 드물게 실행되지만 한 번 실행될 때 매우 많은 추론 비용을 수반하는 복잡한 회귀 테스트 케이스가 존재하기 때문이다.</p>
<p><em>Asteria</em> 캐싱 시스템 구조에서 영감을 받은 <strong>LCFU(Least Cost-Efficient and Frequently Used)</strong> 기반 퇴거 정책은 단순한 사용 빈도를 넘어서는 고차원적인 메모리 관리 효율을 제공한다. 이 정책은 캐시 데이터베이스에 저장된 항목들의 생존 가치를 평가할 때, 해당 쿼리가 캐시되지 않고 실제 대형 언어 모델 인프라를 통해 처리되었을 때 소모되는 막대한 토큰 비용(Cost)과 네트워크 지연 시간(Latency)의 절대적인 절감분, 그리고 해당 항목이 미래에도 동일한 형태로 유지될 확률인 정적성(Staticity)을 종합적인 수학적 공식으로 수치화하여 생존 점수를 부여한다. 이러한 고도화된 퇴거 모델 하에서는 시스템의 핵심 기능을 검증하는 방대한 야간(Nightly) 테스트 스크립트에 포함된 프롬프트-응답 쌍들이 가장 높은 보존 가치를 지니게 된다. 결과적으로 이들은 메모리 부족 상태에서도 우선적으로 퇴거되지 않고 소프트웨어의 영구적인 그라운드 트루스(Ground Truth)로 안전하게 보존될 수 있다.</p>
<h2>6. 실전 사례: 자율 에이전트 파이프라인에서의 결정론적 실행 강제</h2>
<p>고도로 복잡한 최신 소프트웨어 파이프라인, 예를 들어 사용자의 모호한 자연어 요구사항을 스스로 해석하여 다중 파일 구조의 코드로 변환하고, 이를 도커(Docker) 컨테이너 내에서 컴파일하여 테스트까지 수행하는 자율 개발 에이전트(Autonomous Developer Agent) 환경에서 의미론적 캐싱이 어떻게 결정론적 행동을 강제하는지 구체적인 워크플로우를 통해 살펴본다.</p>
<p>최신의 자율 에이전트들은 대부분 ReAct(Reasoning and Acting) 루프 기반의 아키텍처를 따른다. 이 과정에서 에이전트는 상황을 인지하고 계획(Plan)을 세우며 특정 도구를 실행(Act)하는 과정을 끝없이 반복한다. 하나의 기능을 구현하기 위해 에이전트는 내부적으로 “사용자의 요구사항을 시스템 스펙으로 분석”, “필요한 오픈소스 라이브러리 및 최신 버전 검색”, “인터페이스 코드 뼈대 작성”, “단위 테스트 코드 작성” 등의 하위 프롬프트들을 연쇄적으로 생성하여 언어 모델에 전달한다.</p>
<p>만약 이 파이프라인에 오라클 역할을 하는 캐싱 레이어가 존재하지 않는다면, CI 서버에서 코드 무결성을 검증하기 위한 야간 테스트를 수행할 때마다 에이전트 시스템은 통제 불능의 확률적 변동성에 휘말리게 된다. 어제는 완벽하게 작동하여 성공 메시지를 출력했던 코드 생성 파이프라인이, 오늘은 언어 모델이 샘플링 과정에서 갑자기 다른 프레임워크를 선택하거나 변수명을 미세하게 다르게 명명하는 바람에 통합 테스트의 의존성이 깨져버려 알 수 없는 이유로 실패(Flaky Test)하게 되는 악몽이 일상적으로 발생한다. 소프트웨어 개발에서 재현 불가능한 버그는 디버깅을 원천적으로 불가능하게 만든다.</p>
<p><strong>결정론적 캐싱 도입 후의 파이프라인 변화:</strong> 엔지니어링 팀은 이러한 혼란을 종식시키기 위해 <code>LangWatch</code>의 <code>Scenario</code> 프레임워크나 최적화된 <code>GPTCache</code> 모듈을 활용하여, 자율 에이전트의 로직 계층과 외부 언어 모델 API 네트워크 계층 사이에 투명하게 동작하는 의미론적 캐시 프록시(Proxy)를 거대한 방어벽처럼 배치한다.</p>
<p>테스트 코드가 최초로 시스템에 통합되어 실행될 때(캐시 시스템이 완전히 비어 있는 캐시 미스 상태), 에이전트는 언어 모델과 통신하며 완벽한 코드를 힘겹게 생성해 낸다. 이 코드가 모든 정적 분석과 단위 테스트를 성공적으로 통과하면, 프록시 시스템은 그 장시간의 험난한 과정 속에서 발생한 수십 개의 프롬프트와 성공을 이끌어낸 응답의 전체 궤적(Trajectory)을 캐시 데이터베이스에 영구적인 ’골든 데이터셋(Golden Dataset)’으로 박제하여 기록한다.</p>
<p>다음 날 코드가 병합되어 새로운 CI 파이프라인이 구동될 때, 에이전트가 어제와 동일한 작업을 수행하기 위해 “데이터베이스 연동 코드를 즉시 작성해줘“라는 본질적으로 동일한 의도의 프롬프트를 생성한다고 가정하자. 비록 에이전트가 오늘 생성한 프롬프트 문자열에 어제와 달리 공백이 하나 더 들어가 있거나, 어휘의 배치가 미세하게 달라졌을지라도 상관없다. 프록시 계층의 의미론적 캐시 시스템은 즉각 언어 모델 API로 향하는 외부 호출을 가로챈다. 실시간으로 변환된 임베딩 벡터가 데이터베이스 내의 과거 골든 데이터셋 벡터를 검색하고, 코사인 유사도가 사전에 엄격히 설정된 오라클 임계값인 0.98을 초과하는 것이 확인되는 순간, 캐시 계층은 무자비한 결정론적 오라클로서 파이프라인에 직접 개입한다. 외부 네트워크 통신 없이 어제 성공을 보장했던 ’정확히 완벽하게 동일한 구조의 코드와 내부 사고 과정(Chain-of-Thought)’을 단 10밀리초(ms) 만에 에이전트에게 강제로 주입하여 반환하는 것이다.</p>
<p>이러한 캐싱 프록시의 엄격한 통제 메커니즘을 통해, 수많은 언어 모델의 확률적 변수 위에서 위태롭게 동작하던 에이전트 기반의 워크플로우는 더 이상 결과를 보장할 수 없는 주사위 던지기 실험이 아니게 된다. 이는 전통적인 소프트웨어 공학에서 정의하는 ’유한 상태 기계(Finite State Machine)’처럼 입력에 대한 출력이 절대적으로 확정적이며, 개발자가 모든 과정을 선형적으로 재현하고 추적할 수 있는 견고하고 결정론적인 엔터프라이즈 시스템으로 완벽히 격상된다.</p>
<p>결론적으로, 인공지능 개발 파이프라인에서의 의미론적 캐싱은 단순히 언어 모델의 추론 비용을 아끼고 네트워크 지연 시간을 단축하기 위한 인프라 최적화 기법의 범주에 머물지 않는다. 이는 태생적으로 확률론적인 인공지능 모델과 수학적으로 엄격하고 결정론적인 전통적 소프트웨어 엔지니어링 방법론을 안전하게 결합시키는 가장 결정적이고 신뢰할 수 있는 인터페이스다. 나아가, 끊임없이 변화하고 표류하는 모델의 응답 속에서 불변하는 단 하나의 정답 기준을 세워주는 무결점의 신뢰성 오라클(Reliability Oracle)이다. 다가오는 인공지능 시대의 성공적인 소프트웨어 개발은 거대한 딥러닝 모델 자체의 파라미터 성능을 높이는 맹목적인 경쟁만큼이나, 이처럼 모델의 외부 아키텍처에서 비결정성을 완벽하게 통제하고 재현성을 소프트웨어적으로 강제하는 인프라를 얼마나 정교하게 설계하느냐에 그 성패가 달려있다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>(PDF) Statistical Independence Aware Caching for LLM Workflows, https://www.researchgate.net/publication/398134804_Statistical_Independence_Aware_Caching_for_LLM_Workflows</li>
<li>Caching for Deterministic Agent Tests – Scenario - LangWatch, https://langwatch.ai/scenario/basics/cache/</li>
<li>Deterministic AI Architecture: Why They Matter and How to Build Them, https://www.kubiya.ai/blog/deterministic-ai-architecture</li>
<li>Deterministic AI Orchestration: A Platform Architecture … - Praetorian, https://www.praetorian.com/blog/deterministic-ai-orchestration-a-platform-architecture-for-autonomous-development/</li>
<li>What is semantic caching? Guide to faster, smarter LLM apps - Redis, https://redis.io/blog/what-is-semantic-caching/</li>
<li>Semantic Caching for Large Language Models - TrueFoundry, https://www.truefoundry.com/blog/semantic-caching</li>
<li>Semantic Cache: Accelerating AI with Lightning-Fast Data Retrieval, https://qdrant.tech/articles/semantic-cache-ai-data-retrieval/</li>
<li>Semantic Caching: What We Measured, Why It Matters - Catchpoint, https://www.catchpoint.com/blog/semantic-caching-what-we-measured-why-it-matters</li>
<li>GPTCache: An Open-Source Semantic Cache for LLM Applications, https://www.researchgate.net/publication/376404523_GPTCache_An_Open-Source_Semantic_Cache_for_LLM_Applications_Enabling_Faster_Answers_and_Cost_Savings</li>
<li>Importance of Caching in Building Generative AI Applications, https://blogs.oracle.com/developers/importance-of-caching-in-building-a-generative-ai-platform</li>
<li>GPTCache : A Library for Creating Semantic Cache for LLM Queries, https://gptcache.readthedocs.io/</li>
<li>Semantic Cache: How to Speed Up LLM and RAG Applications, https://medium.com/@svosh2/semantic-cache-how-to-speed-up-llm-and-rag-applications-79e74ce34d1d</li>
<li>What is GPTCache - an open-source tool for AI Apps - Zilliz, https://zilliz.com/what-is-gptcache</li>
<li>How Does Semantic Caching Enhance LLM Performance?, https://www.gigaspaces.com/blog/semantic-caching-enhance-llm-performance</li>
<li>Agentic Plan Caching: Test-Time Memory for Fast and Cost-Efficient, https://arxiv.org/html/2506.14852v2</li>
<li>GPTCache | Technology Radar - Thoughtworks, https://www.thoughtworks.com/radar/languages-and-frameworks/gptcache</li>
<li>10 techniques to optimize your semantic cache with Redis LangCache, https://redis.io/blog/10-techniques-for-semantic-cache-optimization/</li>
<li>Semantic Caching for OLAP via LLM-Based Query Canonicalization, https://binds.ch/papers/scolap2026-extended.pdf</li>
<li>Overview of Caching Strategies with Oracle AI Database, https://andersswanson.dev/2026/01/20/overview-of-caching-strategies-with-oracle-ai-database/</li>
<li>Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access, https://arxiv.org/html/2509.17360v1</li>
<li>Sergey Mechtaev’s research works | Peking University and other, https://www.researchgate.net/scientific-contributions/Sergey-Mechtaev-2058069276</li>
<li>Cache Saver: A Modular Framework for Efficient, Affordable, and, https://www.researchgate.net/publication/397423552_Cache_Saver_A_Modular_Framework_for_Efficient_Affordable_and_Reproducible_LLM_Inference</li>
<li>Meet GPTCache: A Library for Developing LLM Query Semantic, https://www.marktechpost.com/2023/08/03/meet-gptcache-a-library-for-developing-llm-query-semantic-cache/</li>
<li>Build Faster and Cheaper LLM Apps With Couchbase and LangChain, https://www.couchbase.com/blog/faster-llm-apps-semantic-cache-langchain-couchbase/</li>
<li>GPTCache: An Open-Source Semantic Cache for LLM Applications …, https://aclanthology.org/2023.nlposs-1.24/</li>
<li>Cache-Efficient Posterior Sampling for Reinforcement Learning with, https://aclanthology.org/2025.emnlp-main.560.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>