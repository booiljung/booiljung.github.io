<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:4.5.5 동적 퓨샷 프롬프팅(Dynamic Few-Shot Prompting): 입력값 유사도 기반 예제 교체</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>4.5.5 동적 퓨샷 프롬프팅(Dynamic Few-Shot Prompting): 입력값 유사도 기반 예제 교체</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 4. AI 모델 응답의 일관성 확보를 위한 프롬프트 엔지니어링 및 파라미터 제어</a> / <a href="index.html">4.5 퓨샷 러닝(Few-Shot Learning): 예제를 통한 정답 패턴 고정</a> / <span>4.5.5 동적 퓨샷 프롬프팅(Dynamic Few-Shot Prompting): 입력값 유사도 기반 예제 교체</span></nav>
                </div>
            </header>
            <article>
                <h1>4.5.5 동적 퓨샷 프롬프팅(Dynamic Few-Shot Prompting): 입력값 유사도 기반 예제 교체</h1>
<p>거대 언어 모델(Large Language Model, LLM)을 활용한 소프트웨어 개발 및 자동화 테스트 환경에서, 모델의 출력을 제어하고 원하는 형식과 논리를 강제하는 가장 실용적인 방법론은 프롬프트 엔지니어링이다. 그중에서도 퓨샷 러닝(Few-Shot Learning)은 제로샷(Zero-Shot) 접근법이 가지는 내재적 불확실성을 극복하고, 모델에게 명시적인 입력과 출력의 쌍을 제공함으로써 문맥 내 학습(In-Context Learning) 능력을 극대화하는 핵심 기법으로 자리 잡았다. 그러나 소프트웨어의 비즈니스 로직이 고도화되고 예측해야 할 엣지 케이스(Edge Case)가 폭발적으로 증가함에 따라, 정해진 소수의 예제만을 모든 상황에 동일하게 주입하는 전통적인 ’정적 퓨샷 프롬프팅(Static Few-Shot Prompting)’은 명확한 한계를 드러내기 시작했다.</p>
<p>정적 퓨샷 프롬프팅은 모든 쿼리에 대해 사전에 하드코딩된 동일한 예제 세트를 고정적으로 제공한다. 이는 모델의 컨텍스트 윈도우(Context Window)를 불필요하게 소모하여 토큰 비용과 연산 지연 시간을 기하급수적으로 증가시킬 뿐만 아니라, 사용자의 현재 입력과 무관한 예제가 주입될 경우 모델이 정보의 과부하로 인해 문맥을 오해하거나 환각(Hallucination)을 일으키는 치명적인 부작용을 초래한다. 특히 결정론적 정답지(Deterministic Ground Truth)를 도출하여 확정적 검증 오라클(Oracle)로 기능해야 하는 소프트웨어 테스트 환경에서, 입력의 맥락을 무시한 고정된 예제는 테스트의 신뢰성을 근본적으로 훼손한다.</p>
<p>이러한 정적 접근 방식의 구조적 한계를 극복하고, 확률적 AI 모델을 결정론적 소프트웨어 시스템의 부품으로 편입시키기 위해 등장한 패러다임이 바로 ’동적 퓨샷 프롬프팅(Dynamic Few-Shot Prompting)’이다. 이 기법은 방대한 예제 데이터베이스를 사전에 구축한 뒤, 사용자의 입력 쿼리가 시스템에 인입되는 런타임 시점에 해당 쿼리와 의미론적으로(Semantically) 가장 유사하고 관련성이 높은 소수의 예제만을 실시간으로 추출하여 프롬프트를 동적으로 조립한다. 이를 통해 모델은 당면한 특정 과제에 가장 적합한 ’골든 예제(Golden Examples)’만을 참조하게 되며, 이는 결과적으로 토큰 효율성을 극대화하는 동시에 출력의 정확도와 논리적 일관성을 비약적으로 향상시킨다.</p>
<p>본 절에서는 동적 퓨샷 프롬프팅의 기반이 되는 시스템 아키텍처와 벡터 유사도 측정의 수학적 원리를 상세히 분석한다. 나아가 이 기법의 효용성을 입증한 최신 학술 연구들을 고찰하고, 이를 통해 소프트웨어 개발 및 테스트 환경에서 AI를 완벽한 결정론적 검증 오라클로 활용하기 위한 실전 구현 전략과 데이터 정합성 보장 매커니즘을 심도 있게 다룬다.</p>
<h2>1. 동적 퓨샷 프롬프팅의 시스템 아키텍처 및 작동 메커니즘</h2>
<p>동적 퓨샷 프롬프팅 시스템은 사용자의 입력값을 분석하고, 가장 적절한 예제를 실시간으로 매핑하여 모델에 주입하기 위해 고도로 설계된 정보 검색 파이프라인을 요구한다. 이 아키텍처는 최근 각광받고 있는 검색 증강 생성(Retrieval-Augmented Generation, RAG) 시스템과 유사한 데이터 흐름을 가지지만, 지식이나 사실(Fact)을 검색하는 RAG와 달리 모델이 따라야 할 ’행동 양식(Behavioral Template)’과 ’입출력 패턴’을 검색한다는 점에서 목적과 구성 데이터의 성격이 뚜렷하게 구별된다.</p>
<p>시스템은 크게 데이터 적재 단계와 실시간 추론 단계로 나뉘며, 다음의 핵심 컴포넌트들로 구성된다.</p>
<p>첫째, **벡터 저장소(Vector Store)**이다. 이 저장소는 과거의 성공적인 소프트웨어 실행 결과, 테스트 통과 사례, 혹은 인간 전문가가 검수하여 확정한 입력-출력 쌍(예: 오류가 발생한 쿼리와 수정된 정답 쿼리, 사용자 자연어와 변환된 API JSON 페이로드 등)을 보관하는 핵심 데이터베이스이다. 저장소에 적재되는 각 예제의 ‘입력(Input)’ 부분은 임베딩 모델을 통해 고차원 실수 벡터로 변환되어 인덱싱되며, 해당 벡터는 검색의 키(Key)로 작용한다.</p>
<p>둘째, **임베딩 모델(Embedding Model)**이다. 이는 자연어로 구성된 사용자의 쿼리나 소프트웨어 시스템의 입력값을 컴퓨터가 수학적으로 연산할 수 있는 다차원 벡터 공간으로 투영하는 역할을 한다. 런타임에 새로운 사용자 입력이 들어오면, 벡터 저장소를 구축할 때 사용했던 것과 동일한 임베딩 모델을 사용하여 쿼리를 벡터로 변환해야 벡터 공간 내에서의 유효한 거리 계산이 가능해진다.</p>
<p>셋째, **검색 및 프롬프트 조립 모듈(Retrieval &amp; Prompt Constructor)**이다. 변환된 사용자 쿼리의 벡터와 벡터 저장소 내에 존재하는 수많은 예제 벡터들 간의 기하학적 거리(Distance) 또는 유사도(Similarity)를 계산한다. 이후 사전에 설정된 <span class="math math-inline">k</span>값에 따라 가장 유사도가 높은 상위 <span class="math math-inline">k</span>개의 예제(Top-k Examples)를 추출한다. 추출된 예제들은 시스템이 정의한 프롬프트 템플릿의 문맥 영역에 동적으로 삽입되며, 최종적으로 거대 언어 모델이 추론을 수행하기 위한 완벽한 지시문으로 조립된다.</p>
<p>넷째, **거대 언어 모델(LLM)**이다. 동적으로 조립된 프롬프트를 전달받은 LLM은 쿼리와 가장 밀접하게 연관된 소수의 골든 예제들을 바탕으로 문맥 내 학습을 수행한다. 모델은 제공된 예제의 패턴, 데이터 구조, 논리적 흐름을 즉각적으로 모방하여, 결정론적 정답에 극도로 근접한 고품질의 응답을 생성해낸다.</p>
<p>이러한 일련의 과정에서 모델의 응답 품질을 결정짓는 가장 치명적인 병목 구간은 바로 ’입력값 간의 유사도를 어떠한 수학적 기준으로 측정하여 최적의 예제를 선별할 것인가’이다.</p>
<h2>2. 벡터 유사도 및 거리 측정의 수학적 원리</h2>
<p>임베딩 모델이 출력한 고차원 실수 벡터 공간에서 두 텍스트의 의미론적 유사성을 정량화하기 위해 기계학습 분야에서는 다양한 거리 척도(Distance Metrics)를 활용한다. 동적 퓨샷 프롬프팅의 예제 검색기(Example Selector)를 구현할 때 어떤 거리 척도를 채택하느냐에 따라 검색되는 예제의 질이 달라지며, 이는 곧 오라클의 정확도에 직결된다.</p>
<h3>2.1  코사인 유사도(Cosine Similarity)</h3>
<p>자연어 처리 및 텍스트 임베딩 환경에서 압도적인 표준으로 사용되는 척도는 코사인 유사도이다. 코사인 유사도는 두 벡터의 물리적 크기(Magnitude)나 길이를 무시하고, 오직 두 벡터가 공간상에서 향하는 ’방향(Angle)’이 얼마나 일치하는지를 평가한다. 이는 두 문서의 길이가 현저히 다르거나 특정 단어의 출현 빈도에 큰 차이가 있더라도, 전체적인 주제나 의미론적 분포가 유사하다면 높은 유사도를 반환한다는 점에서 텍스트 비교에 탁월한 강점을 지닌다.</p>
<p>수학적으로 코사인 유사도는 두 벡터 <span class="math math-inline">\mathbf{A}</span>와 <span class="math math-inline">\mathbf{B}</span>의 내적(Dot Product)을 각 벡터의 L2-노름(L2-Norm, 유클리디안 길이)의 곱으로 나눈 값으로 정의된다. 전체 수식은 다음과 같이 전개된다.<br />
<span class="math math-display">
\text{cos}(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{\vert\mathbf{A}\vert \vert\mathbf{B}\vert} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}
</span><br />
여기서 분자인 <span class="math math-inline">\mathbf{A} \cdot \mathbf{B}</span>는 벡터의 각 차원 요소를 곱하여 모두 더한 내적을 의미하며, 분모는 두 벡터의 기하학적 길이를 구하여 곱한 정규화(Normalization) 항이다. 이 연산을 거친 코사인 유사도는 항상 <span class="math math-inline">[-1, 1]</span> 구간의 값을 가진다.</p>
<ul>
<li><strong>1 (완벽한 유사성):</strong> 두 벡터가 이루는 각도 <span class="math math-inline">\theta</span>가 0도이며, 정확히 동일한 방향을 가리킨다. 이는 두 텍스트의 의미가 완벽히 일치함을 시사한다.</li>
<li><strong>0 (직교 및 무관함):</strong> 두 벡터가 90도를 이루며 직교한다. 이는 두 데이터 간에 어떠한 선형적, 의미론적 연관성도 발견할 수 없음을 나타낸다.</li>
<li><strong>-1 (완전한 반대):</strong> 두 벡터가 180도를 이루며 정확히 반대 방향을 가리킨다.</li>
</ul>
<p>예를 들어, 3차원 공간에 매핑된 두 임베딩 벡터 $\mathbf{A} = $과 $\mathbf{B} = $이 있다고 가정해 보자. 이들의 내적은 <span class="math math-inline">(1 \times 4) + (2 \times 5) + (3 \times 6) = 32</span>가 된다. 벡터 <span class="math math-inline">\mathbf{A}</span>의 크기는 <span class="math math-inline">\sqrt{1^2 + 2^2 + 3^2} \approx 3.74</span>이며, 벡터 <span class="math math-inline">\mathbf{B}</span>의 크기는 <span class="math math-inline">\sqrt{4^2 + 5^2 + 6^2} \approx 8.77</span>이다. 따라서 코사인 유사도는 <span class="math math-inline">\frac{32}{3.74 \times 8.77} \approx 0.97</span>이 되며, 이는 두 벡터가 가리키는 방향이 매우 유사하여 97%의 높은 관련성을 지님을 수학적으로 증명한다. 정보 검색 시스템이나 엘라스틱서치(Elasticsearch) 등에서는 이 값을 항상 양수로 처리하기 위해 <span class="math math-inline">\frac{1 + \text{cos}(\theta)}{2}</span> 공식을 사용하여 $$ 구간의 스코어로 변환하여 정렬에 활용하기도 한다.</p>
<h3>2. 유클리디안 거리(Euclidean Distance) 및 맨해튼 거리(Manhattan Distance)</h3>
<p>유클리디안 거리(L2 Distance)는 <span class="math math-inline">n</span>차원 공간에서 두 점 사이의 기하학적 최단 직선 거리를 측정하는 척도이다. 두 벡터 요소 간 차이의 제곱합에 대한 제곱근으로 산출된다.<br />
<span class="math math-display">
d(\mathbf{A}, \mathbf{B}) = \sqrt{\sum_{i=1}^{n} (A_i - B_i)^2}
</span><br />
유클리디안 거리는 직관적이지만 텍스트 임베딩 공간에서는 치명적인 약점을 지닌다. 이 척도는 데이터의 스케일과 벡터의 크기 변화에 극도로 민감하기 때문이다. 가령 동일한 의미를 지니더라도 한 문서는 100단어로 요약되어 있고 다른 문서는 10,000단어로 서술되어 벡터의 절대적 크기가 커진다면, 유클리디안 거리는 두 문서가 멀리 떨어져 있다고 잘못 판단할 수 있다. 다만, 두 벡터가 정규화(Normalized)되어 크기가 1로 통일된 상태라면, 유클리디안 거리와 코사인 유사도는 수학적으로 동일한 순위 결과를 도출하게 된다.</p>
<p>반면 맨해튼 거리(L1 Distance)는 축을 따라 직교하는 방향으로만 이동할 때의 거리를 산출한다.<br />
<span class="math math-display">
d(\mathbf{A}, \mathbf{B}) = \sum_{i=1}^{n} \vert A_i - B_i \vert
</span><br />
맨해튼 거리는 고차원 환경에서 특정 차원의 극단적인 이상치(Outlier)에 덜 민감하게 반응한다는 장점이 있어, 매우 희소한(Sparse) 벡터 데이터를 다룰 때 유클리디안 거리의 훌륭한 대안이 된다.</p>
<p>소프트웨어 오라클 구축 시 동적 예제 검색을 위한 주요 거리 척도들의 특성을 종합 비교하면 아래 표와 같다.</p>
<table><thead><tr><th><strong>거리 측정 척도</strong></th><th><strong>수학적 수식</strong></th><th><strong>LLM 동적 퓨샷 검색 적용 시의 고찰 및 특징</strong></th></tr></thead><tbody>
<tr><td><strong>코사인 유사도 (Cosine Similarity)</strong></td><td><span class="math math-inline">\frac{\mathbf{A} \cdot \mathbf{B}}{\vert\mathbf{A}\vert \vert\mathbf{B}\vert}</span></td><td>벡터의 크기 차이를 무시하고 오직 의미론적 방향성만을 평가. 문서의 길이나 단어 빈도수 변동에 강건하여 LLM 예제 검색의 최우선 표준으로 채택됨.</td></tr>
<tr><td><strong>내적 (Dot Product)</strong></td><td><span class="math math-inline">\sum_{i=1}^{n} A_i B_i</span></td><td>방향과 벡터의 절대적 크기를 동시에 반영. 두 임베딩 벡터가 이미 크기 1로 정규화된 환경에서는 코사인 유사도 연산과 완벽하게 일치하여 연산 속도 최적화에 유리함.</td></tr>
<tr><td><strong>유클리디안 거리 (Euclidean Distance)</strong></td><td><span class="math math-inline">\sqrt{\sum_{i=1}^{n} (A_i - B_i)^2}</span></td><td>데이터 포인트 간의 절대적 물리 거리를 산출. 텍스트 길이가 상이할 경우 스케일 왜곡이 발생할 수 있어 정규화가 선행되지 않으면 LLM 검색용으로는 부적합함.</td></tr>
<tr><td><strong>맨해튼 거리 (Manhattan Distance)</strong></td><td><span class="math math-inline">\sum_{i=1}^{n} \vert A_i - B_i \vert</span></td><td>격자 형태의 축 이동 거리를 계산. 고차원 임베딩 공간에서 이상치의 영향을 감쇄시켜야 하는 특수한 희소 데이터 검색 모델에 제한적으로 사용됨.</td></tr>
</tbody></table>
<h2>동적 예제 선택의 효용성에 대한 학술적 기반과 심층 연구</h2>
<p>사용자의 입력값과 임베딩 공간 상에서 가장 가까운 예제를 추출하여 프롬프트를 구성하는 접근법은 단순히 직관적인 아이디어를 넘어, 학계의 치밀한 연구를 통해 그 강력한 효능이 수학적, 실험적으로 거듭 입증되어 왔다. 결정론적 소프트웨어 오라클을 구축하기 위해서는 이러한 학술적 통찰을 이해하고 파이프라인에 이식하는 과정이 필수적이다.</p>
<h3>1. 의미론적 유사성과 문맥 내 학습 성능의 비례 관계</h3>
<p>가장 선구적인 연구 중 하나인 “What Makes Good In-Context Examples for GPT-3?” (Liu et al., 2021) 논문은 퓨샷 러닝에서 ’어떤 예제를 선택하느냐’에 따라 모델의 추론 결과가 하늘과 땅 차이로 벌어질 수 있음을 실험적으로 증명했다. 연구진은 모델에게 예제를 무작위로 추출해 제공하는 베이스라인 모델과, KATE(kNN-augmented in-context example selection)라는 자체적인 검색 모듈을 도입하여 테스트 쿼리와 의미론적으로 유사한 예제를 동적으로 제공하는 모델을 비교 분석했다.</p>
<p>분석 결과, KATE를 적용하여 질문과 유사한 예제를 주입했을 때 자연어 이해 및 생성 등 광범위한 벤치마크에서 무작위 선택 방식을 압도하는 결과가 나타났다. 특히 테이블-텍스트 생성 과제(ToTTo 데이터셋)에서는 44.3%, 오픈 도메인 질의응답(NQ 데이터셋)에서는 45.5%라는 극적인 성능 향상이 관찰되었다. 실험 데이터에 따르면, 쿼리와 유사한 자세나 배경을 가진 이미지를 시각적 프롬프트로 주었을 때 객체를 완벽히 식별한 반면, 의미적으로 거리가 먼 예제는 모델의 주의(Attention) 매커니즘을 교란시켜 완전히 잘못된 영역(예: 고양이가 아닌 흰색 기둥)에 초점을 맞추게 하는 환각을 유도했다. 이 연구는 예제가 단순한 ’포맷의 예시’를 넘어, 모델의 내부 확률 공간을 정답이 존재하는 좁은 영역으로 강력하게 견인하는 네비게이션 역할을 한다는 것을 입증한다.</p>
<h3>2. 효율적 프롬프트 검색을 위한 언어 모델 기반 스코어링 (EPR)</h3>
<p>단순한 기하학적 임베딩 거리의 측정에서 한 단계 더 나아가, LLM 자체가 선호하는 프롬프트를 학습하는 능동적인 접근법도 제안되었다. “Learning To Retrieve Prompts for In-Context Learning” (Rubin et al., 2021) 논문은 EPR(Efficient Prompt Retrieval)이라는 혁신적인 프레임워크를 선보였다.</p>
<p>이 연구는 텍스트의 표면적 유사도를 넘어, 실제 모델이 특정 예제를 보았을 때 정답 확률을 높이는지를 수학적으로 검증한다. 먼저 BM25와 같은 비지도 검색기를 통해 후보 예제 세트를 추출한 뒤, 스코어링 언어 모델(Scoring LM)을 활용하여 해당 예제가 주어졌을 때 테스트 쿼리가 올바른 타겟 텍스트를 생성할 확률을 계산한다. 확률을 극대화하는 예제는 양성(Positive) 샘플로, 확률을 저하시키는 예제는 음성(Negative) 샘플로 레이블링한다. 이후 이 신호를 바탕으로 대조 학습(Contrastive Learning)을 수행하여 고도로 최적화된 밀집 검색기(Dense Retriever)를 훈련시킨다. 런타임에 이 훈련된 EPR이 프롬프트를 검색하면, 인간의 직관적 유사도를 넘어서 LLM의 파라미터가 가장 안정적으로 반응하는 완벽한 맞춤형 예제가 도출된다. 이는 테스트 환경에서 모델이 생성하는 결과물의 결정론적 일관성을 확보하는 데 핵심적인 기여를 한다.</p>
<h3>3. 보정 없는 추론과 토큰 한계의 극복: kNN 프롬프팅</h3>
<p>퓨샷 러닝은 예제가 많을수록 성능이 선형적으로 개선되는 경향이 있으나, LLM의 컨텍스트 윈도우 한계로 인해 주입할 수 있는 예제의 수에 물리적인 병목이 발생한다. 이를 극복하기 위해 제안된 “kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference” (Xu et al., 2023) 논문은 동적 예제 검색을 모델의 추론 과정에 직접 융합하는 앙상블 체계를 고안했다.</p>
<p>kNN 프롬프팅은 주어진 사용자 쿼리에 대해 단순히 LLM의 생성 확률만 바라보지 않는다. 동시에 방대한 오프라인 훈련 데이터 저장소에서 쿼리와 가장 유사한 <span class="math math-inline">k</span>개의 이웃(Nearest Neighbors)을 동적으로 검색하여 그들이 가진 실제 정답 레이블 정보를 확률 분포화한다. 최종적으로 LLM의 고유한 예측 확률과 kNN 검색 기반의 확률을 통합하여 정답을 출력한다. 이 방식의 가장 위대한 성과는 복잡한 수동 프롬프트 보정(Calibration-Free) 없이도 극히 안정적인 출력을 생성한다는 점과, 컨텍스트 길이의 물리적 제한을 넘어 2샷에서 무려 1024샷에 이르는 지식을 파라미터 업데이트 없이 확장 적용할 수 있다는 점이다. 소프트웨어의 회귀 테스트나 방대한 로직 검증 시, 수만 개의 과거 성공 테스트 케이스를 모두 벡터 저장소에 담아두고 kNN 방식으로 결합하면 100%에 가까운 결정론적 추론을 강제할 수 있는 강력한 이론적 배경이 된다.</p>
<h3>4. 다양성과 대표성의 최적 균형: 선택적 주석 달기</h3>
<p>결정론적 오라클 시스템을 구축할 때 가장 비용이 많이 드는 작업은 검색 대상이 될 ’골든 데이터셋’을 구축하고 정답(Ground Truth)을 주석(Annotation)하는 일이다. 논문 “Selective Annotation Makes Language Models Better Few-Shot Learners” (Su et al., 2022)는 이러한 한정된 예산 하에서 예제 풀(Pool)의 품질을 극대화하는 선택적 주석 기법을 제시했다.</p>
<p>이 연구는 단순히 무작위로 데이터를 추출해 정답지를 만드는 관행을 타파하고, 비지도 학습 기반의 방향성 그래프(Directed Graph) 알고리즘인 ’Vote-k’를 도입했다. 방대한 무작위 데이터 중에서 시스템 로직을 가장 잘 대표하면서도 서로 간의 중복도가 낮은(다양성이 높은) 핵심 노드(예제)만을 알고리즘이 선별한다. 선별된 극소수의 데이터에 대해서만 전문가가 엣지 케이스와 정답을 정밀하게 작성하여 풀을 구성하고, 런타임에는 이 고품질의 축소된 풀 안에서 동적 검색을 수행한다. 이 방법론은 데이터 구축 비용을 최소화하면서도, 모델이 직면할 수 있는 예측 불가능한 시나리오에 대응하는 강력하고 대표성 있는 오라클 데이터베이스를 설계하는 모범 사례를 제공한다.</p>
<h2>소프트웨어 테스팅에서의 결정론적 오라클 구현 전략</h2>
<p>학술적으로 검증된 동적 퓨샷 프롬프팅 이론은 AI를 활용한 소프트웨어 개발 프로세스에서 그 진가를 발휘한다. 전통적인 결정론적 소프트웨어 환경에서 오라클(Oracle)이란 특정 입력에 대해 시스템이 뱉어내야 할 절대적인 정답(Ground Truth)의 기준을 의미한다. 반면 AI는 확률 분포에 따라 매번 형태가 다르거나 문법에 맞지 않는 코드를 뱉어낼 위험(비결정성)을 내포하고 있어 자동화 테스트와 지속적 통합(CI/CD) 파이프라인의 구축을 저해한다. 동적 퓨샷 프롬프팅은 모델의 무작위성을 제어하여 소프트웨어 공학이 요구하는 엄격한 오라클을 구축하는 구원 투수로 작용한다.</p>
<h3>1. 환각(Hallucination) 억제와 컴파일 성공률의 비약적 상승</h3>
<p>LLM을 통해 테스트 코드를 자동으로 생성하거나 애플리케이션의 비즈니스 로직을 검증할 때 가장 큰 위협은 실재하지 않는 함수나 라이브러리를 호출하는 환각 현상이다. 정적 프롬프트로 소수의 일반적인 예제만을 주입할 경우, 도메인 특화 라이브러리나 복잡한 커스텀 API 구조가 쿼리로 인입되면 모델은 문맥을 잃고 확률적으로 그럴듯한(그러나 논리적으로 틀린) 코드를 생성한다.</p>
<p>동적 퓨샷 프롬프팅은 모델이 기억 속의 부정확한 가중치에 의존하는 것을 차단하고, 프롬프트에 동적으로 삽입된 ’가장 관련성 높은 확실한 과거의 성공 코드’만을 근거로 삼아 추론하도록 모델의 시야를 좁힌다. 실제로 LLM 기반의 테스트 오라클 생성 성능을 평가한 연구에 따르면, 아무런 예제를 제공하지 않거나 사고의 사슬(Chain of Thought)과 같은 논리 전개만 요구한 모델은 컴파일 성공률이 44% 수준에 머물렀다. 반면 구체적인 과거 성공 사례가 예제로 주입된 퓨샷 환경에서는 생성된 코드의 컴파일 성공률이 72.96%로 급상승했으며, 버그를 정확히 탐지하는 능력 또한 극적으로 개선되었다. 이는 런타임 상황에 완벽히 들어맞는 예제가 모델에게 구체적인 문법 제약(Constraints)과 오라클의 역할을 동시에 수행했음을 시사한다.</p>
<h3>2. 하이퍼파라미터 제어와의 결합을 통한 강제적 결정론 구현</h3>
<p>동적 예제 검색만으로 오라클이 완성되는 것은 아니다. 추출된 골든 예제의 구속력을 100%로 끌어올리기 위해서는 LLM 자체의 무작위성을 억제하는 하이퍼파라미터 튜닝이 필수적으로 병행되어야 한다.</p>
<p>분석 및 분류 태스크나 엄격한 JSON 스키마를 출력해야 하는 소프트웨어 모듈에서는 모델의 창의성 지표인 온도(Temperature) 파라미터를 0.1 혹은 그 이하의 극단적으로 낮은 값으로 설정해야 한다. Spring AI 등 현대적인 프레임워크를 이용한 소프트웨어 구현 사례를 보면, 영화 리뷰의 감성 분류나 API 응답의 유효성을 검증할 때 온도를 0.1로 고정하여 추론의 일관성을 확보한다. 여기에 더해 시드(Seed) 값을 고정하고 가장 유사한 오라클 예제 3개를 동적으로 프롬프트 최상단에 주입하면, LLM은 확률 모델의 허물을 벗고 마치 <span class="math math-inline">y = f(x)</span>로 정의되는 일반적인 결정론적 함수 소프트웨어와 완벽하게 동일하게 동작하게 된다.</p>
<h2>산업 표준 프레임워크 기반 실전 구현 체계</h2>
<p>이러한 이론적 토대를 바탕으로 실제 소프트웨어 프로덕션 환경에서 동적 퓨샷 프롬프팅 기반의 검증 오라클을 구축하는 파이프라인은 랭체인(LangChain)과 같은 고수준의 LLM 오케스트레이션 프레임워크를 통해 추상화되고 구현된다.</p>
<h3>1. 골든 데이터셋의 인덱싱 및 메타데이터 관리</h3>
<p>가장 먼저 수행되어야 할 작업은 소프트웨어의 핵심 로직, 경계값 테스트(Boundary Value Testing), 과거 회귀 버그 수정 내역을 포괄하는 양질의 예제 데이터베이스를 확보하는 것이다. 각 예제는 사용자의 예상 <code>입력(Input)</code>과, 시스템이 반환해야 할 확정적이고 무결한 <code>출력(Output)</code>의 쌍으로 구성된다.</p>
<p>랭스미스(LangSmith)와 같은 플랫폼을 활용할 경우, 이 데이터셋은 명확한 스키마 유효성 검사(Schema Validation)를 거친 뒤 KV(Key-Value) 스토어 형태로 시스템에 적재되어야 한다. 적재된 예제들은 지정된 임베딩 모델(예: OpenAI의 <code>text-embedding-3-large</code>)을 거쳐 고차원 벡터로 변환되며, BM25 등과 결합된 하이브리드 검색 인덱스로 벡터 데이터베이스에 영구 저장된다. 이때 성능의 일관성을 보장하기 위해, 입력 문자열 내의 단순한 띄어쓰기나 대소문자 변동이 임베딩에 혼란을 주지 않도록 데이터 정규화(Normalization) 전처리가 철저히 선행되어야 한다.</p>
<h3>2. 동적 검색기(SemanticSimilarityExampleSelector)의 구성 로직</h3>
<p>랭체인 기반의 파이프라인에서 개발자는 예제 검색의 논리를 <code>SemanticSimilarityExampleSelector</code> 클래스를 통해 정밀하게 제어한다. 이 객체는 사용자의 쿼리가 들어오는 즉시 벡터 스토어에 접근하여 유사도 연산을 수행하는 핵심 엔진이다.</p>
<ul>
<li><strong>벡터 스토어 연결:</strong> Chroma, FAISS 등 지연 시간(Latency) 요구사항에 부합하는 인메모리 혹은 영구 벡터 데이터베이스를 매핑한다.</li>
<li><strong>유사도 척도와 제약 설정:</strong> 기본적으로 문서 길이에 강건한 코사인 유사도를 채택하여 거리를 계산한다. 동시에 모델의 컨텍스트 한계와 비용을 고려하여 반환할 예제의 최대 개수(<span class="math math-inline">k</span>)를 3~5개로 엄격히 제한한다. 이 소수의 예제가 모델의 편향을 좌우하게 되므로, 예제가 프롬프트에 주입되는 순서(Order) 역시 로직에 맞게 일관되게 정렬되어야 한다.</li>
</ul>
<h3>3. 실전 예제: SQL 생성 AI의 오류 자가 치유 및 결과 검증 오라클</h3>
<p>동적 퓨샷 프롬프팅이 가장 빛을 발하는 실전 영역은 자연어 질의를 SQL 코드로 변환하는 Text-to-SQL 에이전트 환경이다. 사용자의 자연어 질문은 다양한 데이터베이스 테이블 조인과 중첩 쿼리를 요구하는데, 모델이 DB 스키마를 오해하면 치명적인 런타임 오류가 발생한다.</p>
<p>이를 방지하기 위한 동적 오라클 시스템은 다음과 같이 구성된다. 먼저, 전처리 단계에서 데이터베이스의 메타 정보(테이블, 컬럼명, 실제 값의 일부)와 과거 성공적으로 수행되었던 수백 개의 쿼리-정답 SQL 쌍을 임베딩하여 벡터 저장소에 인덱싱한다. 특히 문자열 불일치로 인한 사소한 SQL 오류를 방지하기 위해 실제 DB 내부의 주요 문자열 값까지 인덱싱에 포함시키는 치밀함이 요구된다.</p>
<p>런타임에 사용자가 “지난달 VIP 고객 중 서울에 거주하는 사람의 결제 총액을 알려줘“라고 쿼리를 던지면, 시스템은 즉각 벡터 데이터베이스를 조회하여 과거에 ‘VIP 고객 조회’, ‘특정 지역 필터링’, ’결제 총액 집계’를 수행했던 가장 유사한 SQL 예제 3개를 추출한다. 이 예제들은 쿼리 생성 규칙 및 사고의 사슬(CoT) 프롬프트 템플릿과 결합되어 LLM에 주입된다. 만약 생성된 1차 SQL이 실행 오류를 일으킨다면, 시스템은 오류 메시지 자체를 다시 임베딩하여 과거의 ‘오류 수정 이력(Correction Few-shot)’ 풀에서 유사한 에러 치유 예제를 동적으로 가져와 스스로 SQL을 튜닝하는 자가 치유(Self-Healing) 파이프라인을 구축할 수 있다. 제공된 예제는 완벽한 구조적, 논리적 오라클이 되어, 모델이 데이터베이스의 문법적 제약을 이탈하지 않고 단번에 무결한 SQL을 생성하도록 보장한다.</p>
<h3>4. 실전 예제: 구조화된 JSON 응답 검증 매커니즘</h3>
<p>기업의 마이크로서비스 아키텍처에서 AI가 중간 API 역할을 수행할 때, 반환되는 페이로드는 타 서비스와의 연동을 위해 완벽한 JSON 정합성을 지녀야 한다. 정적 예제로 수많은 JSON 스키마 변형을 모두 담는 것은 불가능하므로, 시스템은 사용자의 쿼리 의도를 파악하여 요구되는 응답 타입에 해당하는 JSON 예제를 동적으로 삽입해야 한다.</p>
<p>프롬프트 템플릿 내에 “당신은 구조화된 데이터를 출력하는 확정적 오라클 시스템이다. 반드시 아래에 동적으로 제공된 예제 1, 2, 3의 JSON 구조와 데이터 타입을 100% 준수하여 출력해야 하며, 일반 텍스트나 포맷 변형은 절대로 허용되지 않는다“라는 엄격한 지시어(Constraints)를 명시한다. 온도가 극도로 제어된 LLM은 동적으로 주입된 완벽한 JSON 오라클을 흉내 내며, 속성 누락이나 데이터 타입 오류 없이 결정론적 정합성을 갖춘 출력물을 안정적으로 생성해낸다.</p>
<p>결론적으로, 동적 퓨샷 프롬프팅은 불확실성을 내포한 언어 모델의 확률 공간을 우리가 원하는 결정론적 소프트웨어의 영역으로 정밀하게 타겟팅하는 가장 진보된 프롬프트 엔지니어링 방법론이다. 사용자의 입력 맥락에 가장 부합하는 오라클 예제를 실시간으로 찾아 주입함으로써, 모델은 컨텍스트 비용의 낭비 없이 극대화된 추론 일관성을 발휘한다. 소프트웨어 엔지니어링 조직은 이 아키텍처를 CI/CD 및 자동화 테스트 파이프라인에 이식함으로써, 환각 없는 무결한 코드 생성과 강고한 비즈니스 로직 검증 체계를 완성할 수 있다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>Dynamic few-shot prompting to create captivating content - Data Science Dojo, 2월 26, 2026에 액세스, https://datasciencedojo.com/blog/dynamic-few-shot-prompting/</li>
<li>What are some common prompt techniques?, 2월 26, 2026에 액세스, https://docs.oracle.com/en/cloud/saas/fusion-ai/aiaqa/what-are-some-common-prompt-techniques.html</li>
<li>Optimizing AI Agents with Dynamic Few-Shot Prompting - Medium, 2월 26, 2026에 액세스, https://medium.com/@stefansipinkoski/optimizing-ai-agents-with-dynamic-few-shot-prompting-585919f694cc</li>
<li>Dynamic few-shot examples with LangSmith datasets - LangChain Blog, 2월 26, 2026에 액세스, https://blog.langchain.com/dynamic-few-shot-examples-langsmith-datasets/</li>
<li>3 Strategies to Reduce LLM Hallucinations - Vellum AI, 2월 26, 2026에 액세스, https://www.vellum.ai/blog/how-to-reduce-llm-hallucinations</li>
<li>Few-Shot Prompting Explained: Guiding Models with Just a Few Examples - Sandgarden, 2월 26, 2026에 액세스, https://www.sandgarden.com/learn/few-shot-prompting</li>
<li>Dynamic Few Shot Prompting for Enhancing AI Agent Performance - IWConnect, 2월 26, 2026에 액세스, https://iwconnect.com/few-shot-prompting/</li>
<li>Agentic Patterns : Dynamic Few-Shot | RAG for Examples - YouTube, 2월 26, 2026에 액세스, https://www.youtube.com/watch?v=yJHX4PQdB7w</li>
<li>Building LLM Applications With Vector Databases - Neptune.ai, 2월 26, 2026에 액세스, https://neptune.ai/blog/building-llm-applications-with-vector-databases</li>
<li>Leveraging dynamic few-shot prompt with Azure OpenAI - Microsoft Community Hub, 2월 26, 2026에 액세스, https://techcommunity.microsoft.com/blog/fasttrackforazureblog/leveraging-dynamic-few-shot-prompt-with-azure-openai/4225235</li>
<li>OpenSearch-SQL: Enhancing Text-to-SQL with Dynamic Few-shot and Consistency Alignment - arXiv, 2월 26, 2026에 액세스, https://arxiv.org/html/2502.14913v1</li>
<li>Understanding Distance Metrics Used in Machine Learning - Analytics Vidhya, 2월 26, 2026에 액세스, https://www.analyticsvidhya.com/blog/2020/02/4-types-of-distance-metrics-in-machine-learning/</li>
<li>Euclidean and Manhattan distance metrics in Machine Learning. | by Gaurav Rajesh Sahani | Analytics Vidhya | Medium, 2월 26, 2026에 액세스, https://medium.com/analytics-vidhya/euclidean-and-manhattan-distance-metrics-in-machine-learning-a5942a8c9f2f</li>
<li>4 Distance Measures for Machine Learning - MachineLearningMastery.com, 2월 26, 2026에 액세스, https://machinelearningmastery.com/distance-measures-for-machine-learning/</li>
<li>Investigation of Few-Shot Selection Strategies in LLM-based Text Augmentation for Classification - arXiv, 2월 26, 2026에 액세스, https://arxiv.org/html/2410.10756v1</li>
<li>Measuring Similarity and Distance between Embeddings - Dataquest, 2월 26, 2026에 액세스, https://www.dataquest.io/blog/measuring-similarity-and-distance-between-embeddings/</li>
<li>Cosine Similarity - GeeksforGeeks, 2월 26, 2026에 액세스, https://www.geeksforgeeks.org/dbms/cosine-similarity/</li>
<li>Cosine similarity - Wikipedia, 2월 26, 2026에 액세스, https://en.wikipedia.org/wiki/Cosine_similarity</li>
<li>Cosine Similarity Breakdown in LaTeX - Audrey Roy Greenfeld - Feldroy, 2월 26, 2026에 액세스, https://audrey.feldroy.com/articles/2025-01-16-Cosine-Similarity-Breakdown-in-LaTeX</li>
<li>Understanding the Cosine Similarity Formula - TiDB, 2월 26, 2026에 액세스, https://www.pingcap.com/article/understanding-the-cosine-similarity-formula/</li>
<li>A Guide to Cosine Similarity | Tiger Data, 2월 26, 2026에 액세스, https://www.tigerdata.com/learn/understanding-cosine-similarity</li>
<li>Understanding Cosine Similarity and Its Applications - Built In, 2월 26, 2026에 액세스, https://builtin.com/machine-learning/cosine-similarity</li>
<li>Vector similarity techniques and scoring in Elasticsearch, 2월 26, 2026에 액세스, https://www.elastic.co/search-labs/blog/vector-similarity-techniques-and-scoring</li>
<li>How to decide the perfect distance metric for your machine learning model, 2월 26, 2026에 액세스, https://towardsdatascience.com/how-to-decide-the-perfect-distance-metric-for-your-machine-learning-model-2fa6e5810f11/</li>
<li>When to use cosine simlarity over Euclidean similarity - Data Science Stack Exchange, 2월 26, 2026에 액세스, https://datascience.stackexchange.com/questions/27726/when-to-use-cosine-simlarity-over-euclidean-similarity</li>
<li>What Makes Good In-Context Examples for GPT-3? - ACL Anthology, 2월 26, 2026에 액세스, https://aclanthology.org/2022.deelio-1.10/</li>
<li>What Makes Good In-Context Examples for GPT-3? - ACL Anthology, 2월 26, 2026에 액세스, https://aclanthology.org/2022.deelio-1.10.pdf</li>
<li>[2101.06804] What Makes Good In-Context Examples for GPT-3? - ar5iv - arXiv, 2월 26, 2026에 액세스, https://ar5iv.labs.arxiv.org/html/2101.06804</li>
<li>What Makes Good In-Context Examples for GPT-3? - ResearchGate, 2월 26, 2026에 액세스, https://www.researchgate.net/publication/348589258_What_Makes_Good_In-Context_Examples_for_GPT-3</li>
<li>NeurIPS Poster What Makes Good Examples for Visual In-Context Learning?, 2월 26, 2026에 액세스, https://neurips.cc/virtual/2023/poster/70423</li>
<li>[PDF] Learning To Retrieve Prompts for In-Context Learning - Semantic Scholar, 2월 26, 2026에 액세스, https://www.semanticscholar.org/paper/Learning-To-Retrieve-Prompts-for-In-Context-Rubin-Herzig/f9838a3be5c94bb2674a0e224de349b50e18f3c4</li>
<li>Paper Review - Learning To Retrieve Prompts for In-Context Learning | Liyan Tang, 2월 26, 2026에 액세스, https://www.tangliyan.com/blog/posts/learning_to_retrieve/</li>
<li>Learning To Retrieve Prompts for In-Context Learning - ACL Anthology, 2월 26, 2026에 액세스, https://aclanthology.org/2022.naacl-main.191.pdf</li>
<li>Learning to Retrieve In-Context Examples for Large Language Models - arXiv.org, 2월 26, 2026에 액세스, https://arxiv.org/html/2307.07164v2</li>
<li>Loader - Paper Breakdown, 2월 26, 2026에 액세스, https://paperbreakdown.com/abs/2306.03435</li>
<li>k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference | Request PDF - ResearchGate, 2월 26, 2026에 액세스, https://www.researchgate.net/publication/369540108_kNN_Prompting_Beyond-Context_Learning_with_Calibration-Free_Nearest_Neighbor_Inference</li>
<li>Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering - arXiv, 2월 26, 2026에 액세스, https://arxiv.org/html/2309.17249v2</li>
<li>Paper Digest: ICLR 2023 Highlights, 2월 26, 2026에 액세스, https://www.paperdigest.org/2023/02/iclr-2023-highlights/</li>
<li>On Selecting Few-Shot Examples for LLM-based Code Vulnerability Detection - arXiv, 2월 26, 2026에 액세스, https://arxiv.org/pdf/2510.27675</li>
<li>[Quick Review] kNN Prompting: Beyond-Context Learning with, 2월 26, 2026에 액세스, https://liner.com/review/knn-prompting-beyondcontext-learning-with-calibrationfree-nearest-neighbor-inference</li>
<li>[PDF] Selective Annotation Makes Language Models Better Few-Shot Learners, 2월 26, 2026에 액세스, https://www.semanticscholar.org/paper/Selective-Annotation-Makes-Language-Models-Better-Su-Kasai/86d0d3855f94105e25d81cab9f3d269c6062a9c4</li>
<li>Sub-SA: Strengthen In-context Learning via Submodular Selective Annotation - arXiv, 2월 26, 2026에 액세스, https://arxiv.org/html/2407.05693v1</li>
<li>CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage - ACL Anthology, 2월 26, 2026에 액세스, https://aclanthology.org/2024.emnlp-main.1185.pdf</li>
<li>IDEAL: INFLUENCE-DRIVEN SELECTIVE ANNOTA- TIONS EMPOWER IN-CONTEXT LEARNERS IN LARGE LANGUAGE MODELS - ICLR Proceedings, 2월 26, 2026에 액세스, https://proceedings.iclr.cc/paper_files/paper/2024/file/a7f90da65dd41d699d00e95700e6fa1e-Paper-Conference.pdf</li>
<li>Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process - arXiv.org, 2월 26, 2026에 액세스, https://arxiv.org/html/2408.02103v1</li>
<li>LogiPart: Local Large Language Models for Data Exploration at Scale with Logical Partitioning - arXiv, 2월 26, 2026에 액세스, https://arxiv.org/html/2509.22211v3</li>
<li>Understanding LLM-Driven Test Oracle Generation - arXiv, 2월 26, 2026에 액세스, https://arxiv.org/html/2601.05542v1</li>
<li>Think Beyond Size: Dynamic Prompting for More Effective Reasoning - arXiv.org, 2월 26, 2026에 액세스, https://arxiv.org/html/2410.08130v1</li>
<li>A Case-based Reasoning Approach to Dynamic Few-Shot Prompting for Code Generation - OpenReview, 2월 26, 2026에 액세스, https://openreview.net/pdf/f2d10bfca1b7d9f6f0a87144fee8e775cba6701a.pdf</li>
<li>Prompt Engineering Techniques with Spring AI, 2월 26, 2026에 액세스, https://spring.io/blog/2025/04/14/spring-ai-prompt-engineering-patterns/</li>
<li>How to Implement Few-Shot Prompting (Beginner-Friendly Tutorial) - YouTube, 2월 26, 2026에 액세스, https://www.youtube.com/watch?v=QZyYFyx_4RY</li>
<li>Dynamic few shot example selection - Docs by LangChain, 2월 26, 2026에 액세스, https://docs.langchain.com/langsmith/index-datasets-for-dynamic-few-shot-example-selection</li>
<li>A Dynamic-Selection-Based, Retrieval-Augmented Generation Framework: Enhancing Multi-Document Question-Answering for Commercial Applications - MDPI, 2월 26, 2026에 액세스, https://www.mdpi.com/2079-9292/14/4/659</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>