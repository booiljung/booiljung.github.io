<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:4.4.4 도메인 특화 용어 및 금지어 목록(Blacklist) 사전 주입 전략</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>4.4.4 도메인 특화 용어 및 금지어 목록(Blacklist) 사전 주입 전략</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 4. AI 모델 응답의 일관성 확보를 위한 프롬프트 엔지니어링 및 파라미터 제어</a> / <a href="index.html">4.4 시스템 프롬프트(System Prompt)를 이용한 페르소나 및 규칙 고정</a> / <span>4.4.4 도메인 특화 용어 및 금지어 목록(Blacklist) 사전 주입 전략</span></nav>
                </div>
            </header>
            <article>
                <h1>4.4.4 도메인 특화 용어 및 금지어 목록(Blacklist) 사전 주입 전략</h1>
<p>소프트웨어 개발 프로세스와 자동화된 테스트 환경에서 거대 언어 모델(LLM)이 결정론적인 정답지를 제공하는 오라클(Oracle)로서 기능하기 위해서는, 모델의 출력이 비즈니스 로직이 요구하는 엄격한 어휘적 제약을 한 치의 오차 없이 준수해야 한다. 금융 거래 시스템, 의료 데이터 파이프라인, 법률 문서 분석 플랫폼, 그리고 고도화된 엔터프라이즈 마이크로서비스 아키텍처는 인간의 자연어 대화처럼 유연하고 의미적으로 유사한 단어의 교체를 결코 허용하지 않는다. 예를 들어, 데이터베이스 업데이트를 위해 다운스트림(Downstream) API가 “Insufficient_Funds“라는 정확한 상태 코드 문자열을 기대하고 있는 상황에서, AI 모델이 의미상 동일하다는 이유로 이를 “Not_Enough_Money“로 변환하여 출력하는 순간 전체 소프트웨어 파이프라인은 치명적인 예외(Exception)를 발생시키며 붕괴하게 된다.</p>
<p>마찬가지로, 시스템의 안정성과 보안, 그리고 규제 준수를 유지하기 위해 절대로 모델의 출력 결과물에 포함되어서는 안 되는 용어들이 존재한다. 소프트웨어 마이그레이션 과정에서 더 이상 지원되지 않는 폐기된 API(Deprecated API)의 호출, 내부 보안 정책상 외부로 노출되어서는 안 되는 기밀 식별자(Identifier) 및 데이터베이스 스키마 명칭, 혹은 기업의 브랜드 톤앤매너(Tone &amp; Manner)에 심각하게 위배되는 부적절한 단어들은 모델의 텍스트 생성 단계 이전에 원천적으로 차단되어야만 한다. 결국 도메인 특화 용어의 강제 주입(White-listing)과 금지어의 사전 차단(Black-listing)은 확률적 텍스트 생성기인 언어 모델을 소프트웨어 검증 및 실행을 위한 신뢰할 수 있는 결정론적 오라클로 변환하는 가장 핵심적인 기술적 관문이다.</p>
<p>이 절에서는 시스템 프롬프트 계층에서의 데이터 구조화 기법부터, 모델의 하이퍼파라미터 조작을 통한 확률의 수학적 절단, 그리고 추론(Inference) 단계의 탐색 공간을 통제하는 어휘 제약 디코딩(Lexically Constrained Decoding) 알고리즘에 이르기까지, 어휘적 제약을 강제하기 위한 다층적(Multi-layered) 전략을 심도 있게 분석한다.</p>
<h2>1. 시스템 프롬프트 계층: 네거티브 프롬프팅의 역설과 구조화 기법</h2>
<p>가장 직관적이고 접근 비용이 낮은 어휘 통제 방식은 시스템 프롬프트에 도메인 용어 사전(Glossary)과 금지어 목록을 명시적인 자연어로 주입하는 것이다. 그러나 언어 모델의 확률적 본성과 어텐션(Attention) 메커니즘의 구조적 특성으로 인해, 단순한 자연어 지시만으로는 이러한 제약을 완벽하게 통제할 수 없다.</p>
<h3>1.1 네거티브 프롬프팅(Negative Prompting)의 한계와 주의 메커니즘의 부작용</h3>
<p>네거티브 프롬프팅은 모델에게 특정 단어를 사용하지 말거나 특정 행동을 수행하지 말라고 명시적으로 지시하는 기법이다. 일반적인 개발자들은 프롬프트를 작성할 때 “Do NOT use the word ‘online’ in your response” 혹은 “Do not generate deprecated functions such as execute_sql()“과 같은 부정 명령문을 빈번하게 사용한다.</p>
<p>그러나 다수의 연구와 실증적 실험 데이터에 따르면, 자연어 기반의 거대 언어 모델에서 네거티브 프롬프팅은 종종 개발자의 의도와 정반대의 결과를 초래하는 치명적인 취약점을 지니고 있다. “Understanding the Impact of Negative Prompts: When and How Do They Take Effect” 등의 논문에서 심층적으로 분석된 바와 같이, 트랜스포머(Transformer) 아키텍처의 셀프 어텐션(Self-Attention) 메커니즘은 프롬프트 내에 존재하는 토큰들의 의미적 이웃(Semantic neighborhood)을 강제로 활성화하는 특성이 있다.</p>
<p>즉, ’금지어’를 프롬프트에 명시하는 행위 자체가 해당 금지어 토큰을 모델의 컨텍스트 윈도우 내에서 높은 가중치로 연산하게 만들며, 결과적으로 자가 회귀(Autoregressive) 생성 과정에서 해당 단어나 그와 밀접하게 연관된 유의어의 생성 확률을 오히려 증폭시키는 역설적인 상황을 발생시킨다. 언어 모델은 “이 단어를 피하라“는 사용자의 높은 수준의 ’논리적 의도(Intent)’보다, 프롬프트 텍스트 공간에 물리적으로 입력된 ’단어 그 자체(Token)’에 더 강력한 조건부 확률을 부여하는 경향을 보인다. 더욱이 확정적인 소프트웨어 오라클 환경에서 네거티브 프롬프트만으로 금지어를 차단하려고 시도할 경우, 모델은 무엇을 생성해야 할지에 대한 대안적 지침이 부족하여 문맥에 맞지 않는 엉뚱한 단어를 무작위로 치환해버리거나 환각(Hallucination) 현상을 악화시키는 결과를 낳는다. 따라서 금지어 목록을 시스템 프롬프트로 주입할 때는 긍정적 지시(Positive directive)와 결합하여 명확한 대안 용어를 함께 구조화하는 방식이 필수적으로 요구된다.</p>
<h3>1.2 도메인 사전 주입을 위한 마크다운(Markdown)과 JSON 구조의 효율성 대비</h3>
<p>네거티브 프롬프팅의 함정을 회피하고 도메인 특화 용어의 사용을 강제하기 위해서는, 용어 사전을 시스템 프롬프트에 고도로 구조화된 형태로 주입해야 한다. 이때 텍스트 데이터를 언어 모델에 전달하는 포맷(Format)이 모델의 지시 수행 정확도와 자원 효율성에 막대한 영향을 미치게 된다.</p>
<p>개발자들은 시스템 간의 구조화된 데이터 교환을 다룰 때 습관적으로 JSON(JavaScript Object Notation) 포맷을 절대적인 표준으로 간주하며 프롬프트 엔지니어링에도 이를 차용하는 경향이 있다. 그러나 “Evaluating, Understanding, and Improving Constrained Text Generation for Large Language Models” 논문을 비롯한 최근의 프롬프트 포맷 연구들에 따르면, 프롬프트의 구조화 방식에 따라 모델의 성능은 최대 40% 이상의 극심한 변동성을 보이며, 프롬프트 컨텍스트 내에서 도메인 지식과 용어 사전을 정의할 때는 마크다운(Markdown) 형식이 JSON보다 압도적으로 유리한 경우가 많다.</p>
<table><thead><tr><th><strong>평가 지표 및 특성</strong></th><th><strong>마크다운(Markdown) 포맷</strong></th><th><strong>JSON 포맷</strong></th></tr></thead><tbody>
<tr><td><strong>토큰 효율성 (비용 및 지연 시간)</strong></td><td>들여쓰기, 헤더(<code>#</code>), 리스트 기호(<code>-</code>) 등 최소한의 기호만을 사용하여 토큰 소모가 매우 적다. JSON 대비 약 15%에서 최대 38%의 토큰을 절약하여 추론 비용과 지연 시간을 대폭 감소시킨다.</td><td>중괄호(<code>{}</code>), 대괄호(``), 쌍따옴표(<code>""</code>), 콜론(<code>:</code>) 등 구문 분석만을 위한 강제 문자가 많아 토큰 소모가 극심하며 긴 컨텍스트에서 비용을 증가시킨다.</td></tr>
<tr><td><strong>의미적 맥락 결합 (Semantic Alignment)</strong></td><td>자연어 지시문과 시각적, 의미적으로 매끄럽게 연결된다. 언어 모델의 방대한 사전 학습 코퍼스(웹 문서, GitHub 리드미, 위키백과 등)와 형태가 완전히 일치하여 모델의 문맥적 이해도를 극대화한다.</td><td>계층 구조가 깊어질수록 언어 모델이 키-값(Key-Value) 매핑의 구문을 추적하는 데 추가적인 연산 집중력을 요구하게 되어, 용어의 실제 의미에 대한 컨텍스트가 단절될 위험이 있다.</td></tr>
<tr><td><strong>오류 복구 및 회복 탄력성 (Resilience)</strong></td><td>텍스트 생성 중 일부 논리적 오류가 발생하더라도 프롬프트 내의 자연어 가이드라인을 유연하게 참조하여 스스로 교정(Self-correction)하는 능력이 뛰어나다.</td><td>단 하나의 쉼표나 괄호 누락과 같은 파싱 에러(Parsing Error)가 발생할 경우, 모델이 이후의 모든 지시사항을 무시하거나 구조 자체를 붕괴시키는 치명적 취약점을 노출한다.</td></tr>
<tr><td><strong>용어 사전 표현력</strong></td><td>테이블 구문(<code>\vert</code> 기호 사용)이나 계층적 글머리 기호를 활용하여 ’도메인 용어-정의-금지어-대체 예제’의 복잡한 관계를 직관적이고 가독성 높게 매핑할 수 있다.</td><td>엄격한 데이터 추출의 스키마 정의나 다운스트림 API 통신에는 필수적이나, 금지어의 미묘한 뉘앙스나 대체어의 활용 조건을 설명하는 데는 불필요하게 복잡하고 경직되어 있다.</td></tr>
</tbody></table>
<p>따라서 시스템 프롬프트에 소프트웨어 도메인 용어 사전과 금지어 목록을 주입할 때는 마크다운과 XML 태그를 결합한 명시적 구조화가 권장된다. 예를 들어, 프롬프트 최상단에 <code>&lt;DOMAIN_GLOSSARY&gt;</code>라는 명확한 XML 태그 구역을 할당하고, 그 내부에 마크다운 테이블 형식으로 “사용해야 할 표준 용어”, “정확한 기술적 의미”, “절대 사용해서는 안 되는 금지어 및 폐기된 API”, 그리고 “적용 예시“를 매핑하여 제공하는 것이다. 이 방식을 채택하면 모델은 네거티브 프롬프팅이 유발하는 주의력 분산의 함정에 빠지지 않고, 마크다운 테이블이 제공하는 강한 양성 제약(Positive constraint)의 틀 안에서 텍스트와 코드를 결정론적으로 생성하게 된다. 이러한 구조화는 개발자가 의도한 비즈니스 로직과 모델의 출력 간의 의미적 간극을 최소화하여 오라클의 신뢰성을 근본적으로 향상시킨다.</p>
<h2>2. 파라미터 제어 계층: 로짓 편향(Logit Bias)을 통한 확률의 강제적 조작</h2>
<p>프롬프트 엔지니어링을 통한 용어 사전 주입은 모델에게 규칙을 ’설득’하고 인지시키는 과정일 뿐, 모델의 근본적인 수학적 확률 분포를 완전히 차단하거나 강제하지는 못한다. 진정한 의미의 결정론적 오라클을 소프트웨어에 통합하기 위해서는 언어 모델의 소프트맥스(Softmax) 병목 이전에 개입하여 특정한 단어 집합의 생성 확률 자체를 조작하는 하위 레벨의 통제가 필요하다. 이를 가능하게 하는 가장 강력하고 직접적인 하이퍼파라미터가 바로 로짓 편향(Logit Bias)이다.</p>
<h3>2.1 로짓 편향의 수학적 원리와 적용 메커니즘</h3>
<p>거대 언어 모델은 내부적으로 다음 토큰을 예측하기 위해 신경망의 마지막 계층에서 전체 어휘 사전(Vocabulary)에 존재하는 수만 개의 각 토큰 <span class="math math-inline">i</span>에 대해 정규화되지 않은 원시 점수인 로짓 <span class="math math-inline">z_i</span>를 계산한다. 이 로짓 값들은 소프트맥스 함수를 통과하면서 모든 토큰의 확률 합이 <span class="math math-inline">1</span>이 되는 <span class="math math-inline">0</span>과 <span class="math math-inline">1</span> 사이의 확률 분포 값 <span class="math math-inline">P(i)</span>로 변환되며, 이 분포를 바탕으로 최종 토큰이 샘플링된다.<br />
<span class="math math-display">
P(i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
</span><br />
로짓 편향 파라미터는 특정 토큰 식별자(Token ID)에 대해 개발자가 정의한 임의의 편향 값 <span class="math math-inline">b_i</span>를 더하여 이 수학적 확률 계산식을 다음과 같이 강제로 변형하는 기법이다.<br />
<span class="math math-display">
P(i) = \frac{e^{z_i + b_i}}{\sum_{j} e^{z_j + b_j}}
</span><br />
OpenAI의 API, 오픈소스 진영의 vLLM, llama.cpp와 같은 대다수의 산업 표준 추론 프레임워크에서 이 편향 값은 일반적으로 <span class="math math-inline">-100</span>에서 <span class="math math-inline">+100</span> 사이의 실수(Float)로 설정되도록 설계되어 있다. 소프트웨어 개발에서 오라클의 정확도를 위해 이 파라미터는 주로 두 가지 극단적인 목적으로 활용된다.</p>
<p>첫째, 완벽한 강제 차단(Blacklisting)이다. 편향 값을 극한의 음수인 <span class="math math-inline">-100</span>으로 설정하면, 소프트맥스 식의 분자 부분인 <span class="math math-inline">e^{z_i - 100}</span>은 사실상 <span class="math math-inline">0</span>에 무한히 수렴하게 된다. 이는 신경망이 해당 토큰이 문맥상 완벽하게 들어맞는다고 계산하여 아무리 높은 원시 로짓 <span class="math math-inline">z_i</span>를 부여하더라도, 최종 샘플링 확률이 수학적으로 완전히 제거됨을 의미한다. 금지어나 보안상 노출되어서는 안 되는 민감한 용어를 원천 차단하는 가장 확실한 방법이다.</p>
<p>둘째, 도메인 용어의 사용 촉진(Whitelisting)이다. 특정 기술 용어나 사내 표준 명칭에 양수의 편향 값(예: <span class="math math-inline">+5</span> 또는 <span class="math math-inline">+10</span>)을 부여하면 해당 토큰의 생성 확률을 기하급수적으로 극대화할 수 있다. 다만, 이 값을 <span class="math math-inline">+100</span>에 가깝게 너무 높게 설정할 경우 모델이 문맥을 무시하고 해당 단어만을 무한 반복하는 퇴보적 루프(Degenerative loop)에 빠질 수 있으므로, 해당 도메인의 특성과 문맥적 적합성을 고려한 세밀한 수치 조절이 요구된다.</p>
<h3>2.2 서브워드 토큰화(Subword Tokenization)로 인한 제어의 복잡성</h3>
<p>로짓 편향 메커니즘을 소프트웨어 개발 실무에 적용하여 금지어 사전을 구현할 때 마주하는 가장 까다롭고 거대한 장벽은, 거대 언어 모델이 텍스트를 인간이 이해하는 온전한 단어(Word) 단위가 아니라 BPE(Byte Pair Encoding)나 WordPiece 알고리즘을 통한 서브워드(Subword) 토큰 단위로 분할하여 처리한다는 사실이다.</p>
<p>기업의 내부 데이터베이스 프레임워크가 업데이트되어 구형 API인 <code>execute_sql_query</code>의 사용이 보안상 완전히 금지되었다고 가정해 보자. 개발자는 AI 코드 어시스턴트의 설정 파일에 단순히 “execute_sql_query“라는 문자열을 블랙리스트에 올리는 것만으로는 아무런 효과를 얻을 수 없다. 사용 중인 모델의 토크나이저(Tokenizer) 알고리즘에 따라 이 문자열은 <code>["execute", "_", "sql", "_", "query"]</code>로 쪼개질 수도 있고, <code>["exec", "ute_sql", "_query"]</code> 등 예측하기 어려운 다수의 무의미한 서브 토큰 조합으로 파편화되기 때문이다.</p>
<p>더욱 심각한 문제는 영단어나 프로그래밍 언어의 구문을 처리할 때 발생한다. 단어 앞에 공백이 포함된 토큰과 포함되지 않은 토큰은 모델 내부에서 완전히 다른 Token ID를 갖는다. 예를 들어 시스템 응답에서 차단해야 할 특정 용어(예: ‘stupid’)가 존재할 때, 이 단어는 <code>과 같이 두 개의 토큰으로 분할되는 경우와 선행 공백을 포함한 </code>“ stupid“<code>가 단일 토큰 </code>로 매핑되는 경우를 모두 포괄한다. 개발자는 이러한 토크나이저의 파편화 규칙을 역으로 추적하여 해당 단어를 구성할 수 있는 모든 가능한 Token ID 조합을 찾아내고, 이들에 대해 일괄적으로 편향 값을 적용해야만 의도한 블랙리스트 효과를 거둘 수 있다.</p>
<p>또한, 다중 토큰으로 구성된 단어의 블랙리스트를 구현할 때 시퀀스의 첫 번째 토큰이 아닌 중간 토큰만을 차단할 경우 시스템 붕괴를 초래하는 심각한 부작용이 발생한다. 만약 모델이 “ministrations“라는 단어를 생성하는 과정에서 “ministr“까지 이미 생성한 상태일 때 다음 토큰인 “ations“가 로짓 편향에 의해 강제로 차단된다면, 모델은 문맥상 전혀 어울리지 않거나 문법적으로 파괴된 다른 토큰(예: “ministr” 뒤에 뜬금없는 “minimalist“의 접미사를 붙이는 등)을 억지로 선택하게 되어 완전히 붕괴된 코드나 횡설수설하는 텍스트(Gibberish)를 출력하게 된다.</p>
<p>따라서 금지어 사전을 로짓 편향으로 구현할 때는 반드시 대상 어휘가 분할될 수 있는 모든 토큰 시퀀스의 ‘첫 번째 토큰’ 확률을 선제적으로 억제해야 한다. 더 나아가 고도화된 소프트웨어 오라클 환경에서는 단일 토큰 제어의 한계를 극복하기 위해, 트리(Trie) 자료구조를 활용하여 모델이 생성하고 있는 컨텍스트를 실시간으로 추적하고 다중 토큰 시퀀스 레벨에서 동적으로 로짓 차단을 수행하는 다중 컨텍스트 편향(Multi-context bias) 기법을 구현하는 것이 필수적인 엔지니어링 과제로 대두되고 있다.</p>
<h2>3. 디코딩 알고리즘 계층: 어휘 제약 디코딩(Lexically Constrained Decoding) 모델</h2>
<p>시스템 프롬프트의 의미적 지시나 토큰 레벨의 단순한 로짓 편향만으로는, 여러 단어로 이루어진 복잡한 도메인 용어를 특정 순서나 문맥에 맞게 반드시 포함시키도록 강제하는 양성 제약(Positive Constraints)을 완벽하게 구현하는 데 근본적인 한계가 있다. 이러한 한계를 극복하고 모델이 엄격한 규칙 하에서만 코드를 생성하도록 만들기 위해, 모델이 텍스트를 생성하며 탐색하는 확률 공간(Search Space) 자체를 재구조화하는 어휘 제약 디코딩(Lexically Constrained Decoding) 알고리즘이 학계와 산업계에서 치열하게 연구 및 발전되어 왔다.</p>
<h3>3.1 그리드 빔 서치(Grid Beam Search, GBS) 알고리즘</h3>
<p>그리드 빔 서치는 거대 언어 모델의 표준적인 디코딩 탐색 방식인 빔 서치(Beam Search)를 획기적으로 확장하여, 출력 텍스트에 반드시 포함되어야 하는 사전 지정된 용어(Lexical constraints)의 존재 여부를 알고리즘적으로 보장하는 기법이다. “Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search“라는 기념비적인 논문에서 제시된 이 알고리즘은 기존의 1차원적인 빔 탐색 공간을 <span class="math math-inline">t</span>(생성된 텍스트의 시간 단계, Timestep)와 <span class="math math-inline">c</span>(현재까지 모델이 성공적으로 만족시킨 제약 토큰의 수)라는 2차원 그리드(Grid) 체계로 구성하여 탐색을 수행한다.</p>
<p>GBS 알고리즘은 디코딩 과정에서 모델이 유지하는 각 가설(Hypothesis) 빔을 두 가지 상태로 명확히 분류하여 제어한다 :</p>
<ol>
<li><strong>열린 가설 (Open Hypotheses):</strong> 일반적인 언어 모델의 본래 확률 분포에 따라 자유롭게 다음 토큰을 생성할 수 있는 상태이거나, 혹은 주입된 도메인 용어 사전에서 새로운 제약 조건 단어를 시작할 수 있는 상태이다.</li>
<li><strong>닫힌 가설 (Closed Hypotheses):</strong> 다중 토큰으로 구성된 긴 제약 용어를 생성하기 시작하여 현재 그 도중에 있는 상태이다. 이 가설은 해당 용어의 생성을 완전히 끝마치기 전까지는 언어 모델의 본래 확률 분포를 무시하고, 오직 해당 제약 용어를 완성하기 위한 다음 토큰만을 강제적으로 생성해야 한다.</li>
</ol>
<p>GBS는 이러한 메커니즘을 통해 <span class="math math-inline">O(k \cdot t \cdot c)</span>의 시간 복잡도(<span class="math math-inline">k</span>: 빔 크기)를 가지며 최적의 텍스트 경로를 찾아낸다. 연산 비용이 추가로 발생하여 생성 속도가 느려진다는 단점이 있으나, 번역 메모리(Translation Memory) 시스템이나 의료 및 법률 파이프라인에서 특정 전문 용어를 어떠한 예외도 없이 강제로 매핑하여 출력해야 하는 무결성 오라클 시스템을 구축할 때 타협 불가능한 수준의 강력한 결정론적 제어력을 제공한다.</p>
<h3>3.2 뉴로로직 디코딩(NeuroLogic Decoding)과 CNF(Conjunctive Normal Form) 검증</h3>
<p>실제 엔터프라이즈 환경에서는 단순히 몇 개의 용어를 포함시키는 것을 넘어, “A와 B 용어 중 하나는 반드시 포함하되, C라는 용어는 절대 포함해서는 안 된다“는 식의 긍정적 제약과 부정적 제약이 복잡하게 혼재된 비즈니스 룰을 강제해야 한다. 이러한 고차원적 논리 제어를 위해 단순한 그리드 탐색을 넘어선 논리적 검증 트리가 요구된다. “NeuroLogic Decoding: (Un)supervised Neural Text Generation with Predicate Logic Constraints” 논문은 모델의 가중치를 파인튜닝(Fine-tuning)하지 않고 오직 추론(Inference) 단계의 디코딩 알고리즘 조작만으로 1차 술어 논리(Predicate Logic) 기반의 복잡한 제약 조건을 모델에 적용하는 혁신적인 기법을 제시한다.</p>
<p>이 기법은 도메인 용어 사전과 금지어 규칙을 논리곱 정규형(CNF, Conjunctive Normal Form) 수식으로 변환하여 시스템에 주입한다. 텍스트가 한 토큰씩 생성될 때마다, 알고리즘은 각 논리절(Clause)이 현재 어떤 상태에 있는지를 4가지 범주로 엄격하게 추적하고 평가한다 :</p>
<ol>
<li><strong>가역적 불충족 (Reversible unsatisfaction, S1):</strong> 현재 생성된 텍스트에는 필수 도메인 용어가 없지만, 모델이 문장을 완성하기 전 미래에 생성될 가능성이 여전히 열려 있는 상태.</li>
<li><strong>비가역적 불충족 (Irreversible unsatisfaction, S2):</strong> 절대 나타나서는 안 될 금지어(Blacklist)가 단 한 번이라도 출력되어버린 치명적인 상태. 뉴로로직 디코딩은 탐색 공간에서 이 상태에 도달한 경로(Branch)를 가차 없이 즉시 폐기(Pruning)하여 더 이상 연산을 진행하지 않는다.</li>
<li><strong>가역적 충족 (Reversible satisfaction, S3):</strong> 금지어가 아직 생성되지 않아 현재까지는 논리적 조건을 만족하고 있으나, 텍스트 생성이 지속됨에 따라 향후 금지어가 생성될 잠재적 위험이 내포된 상태.</li>
<li><strong>비가역적 충족 (Irreversible satisfaction, S4):</strong> 강제로 주입해야 할 필수 도메인 용어(Whitelist)가 이미 성공적으로 출력되어, 해당 논리 조건이 영구적으로 그리고 완벽하게 충족된 상태.</li>
</ol>
<p>뉴로로직 디코딩은 빔 서치 과정에서 가설들을 평가할 때 S2 상태에 빠진 가설을 폐기하는 것뿐만 아니라, S4 상태를 얼마나 많이 달성했는지를 기준으로 후보군을 그룹화(Grouping)한다. 이를 통해 가장 높은 확률값을 가진 뻔한 문장으로만 수렴하려는 빔 서치의 탐욕적(Greedy) 한계를 극복하고, 모델이 생성하는 텍스트의 다양성과 제약 충족성을 동시에 달성한다. 이 방식은 특정 전문의약품의 정확한 명칭을 반드시 사용하되 환각에 의한 일반 의약품 명칭 혼용은 철저히 배제해야 하는 의료 진단 리포트 자동 생성이나 정밀한 금융 보고서 작성 모듈에 직접적으로 활용되며, 언어 모델을 결정론적 논리 기계로 승격시키는 핵심 기술이다.</p>
<h3>3.3 유한 상태 기계(Finite State Machine, FSM)와 Guided Generation 구조</h3>
<p>최근 가장 최적화되고 각광받는 결정론적 시스템에서는 “Efficient Guided Generation for Large Language Models” 논문에서 수학적으로 증명되고 소개된 바와 같이, 정규 표현식(Regular Expressions)과 문맥 자유 문법(Context-Free Grammars)을 기반으로 LLM의 출력을 제한하는 기법이 산업의 표준으로 자리 잡고 있다. 파이썬 생태계의 <code>Outlines</code>나 마이크로소프트의 <code>Guidance</code> 같은 오픈소스 라이브러리에 구현된 이 방식은, 텍스트 기반의 어휘 제약을 컴퓨터 과학의 근본 이론인 유한 상태 기계(FSM)로 변환하여 처리한다.</p>
<p>오라클 시스템에 주입된 도메인 사전과 금지어 목록, 그리고 JSON 스키마는 컴파일 단계를 거쳐 하나의 거대한 FSM으로 변환된다. 모델이 매 토큰을 생성할 때마다, 시스템은 모델의 언어적 확률 분포를 참조하기 전에 먼저 FSM의 현재 상태를 기반으로 언어 모델의 전체 어휘 사전과 대조 연산을 수행한다. 이를 통해 현재의 텍스트 문맥 상태에서 FSM의 다음 노드로 넘어갈 수 있는 ‘구문론적으로 유효한(Valid)’ 토큰만이 무엇인지 정밀하게 계산해낸다.</p>
<p>FSM이 허용하지 않는 유효하지 않은 토큰(금지어, 스키마에 어긋나는 괄호, 문법 규칙에 어긋나는 기호 등)이 모델의 신경망에서 높은 확률을 얻었더라도, 이들의 로짓(Logit) 값은 무한대의 음수(<span class="math math-inline">-\infty</span>)로 강제 마스킹(Masking)되어 소프트맥스 함수를 통과할 때 절대 생성될 수 없도록 억제된다. 이 접근법의 가장 위대한 기술적 성취는, 토큰을 필터링하기 위해 매번 전체 어휘 사전을 무거운 연산으로 검사하는 것이 아니라, 모델 가중치를 통과하기 이전에 사전 구축된 인덱싱 구조를 활용하여 유효한 토큰의 부분 집합을 <span class="math math-inline">O(1)</span>의 상수 시간 비용으로 도출해 낸다는 점이다. 결과적으로 이 기법은 텍스트 생성의 지연(Latency)을 거의 발생시키지 않으면서도, 구조화된 JSON 스키마, 복잡한 SQL 쿼리 문법, 그리고 엄격한 도메인 특화 언어(DSL) 생성 시 문법적 오류와 금지어 출력을 정확히 <span class="math math-inline">0%</span>의 확률로 차단하는 완벽한 결정론적 무결성을 보장한다.</p>
<h2>4. 실전 사례: 소프트웨어 코드 생성 및 비즈니스 로직 검증에서의 적용</h2>
<p>앞서 심도 있게 논의된 프롬프트 엔지니어링, 로짓 편향 제어, 그리고 어휘 제약 디코딩 알고리즘들이 실제 복잡한 엔터프라이즈 환경에서 어떻게 결합되어 결정론적 오라클로 동작하는지 구체적인 소프트웨어 공학 사례를 통해 검증한다.</p>
<h3>4.1 레거시 코드 변환 방지를 위한 Deprecated API 블랙리스트 강제화</h3>
<p>현대의 AI 기반 코드 보조 도구(Code Assistant)나 코드 자율 생성 에이전트는 GitHub와 같은 방대한 오픈소스 코퍼스를 기반으로 사전 학습되었기 때문에, 기업 내부의 최신 보안 표준을 따르지 않거나 더 이상 유지보수되지 않는 과거 버전의 API(Deprecated API)를 습관적으로 생성하는 고질적인 렌더링 관성을 지니고 있다. 개발팀이 레거시 코드를 현대화하는 마이그레이션 프로젝트에 AI를 투입할 때, 이러한 관성은 치명적인 기술 부채를 유발한다.</p>
<p>이를 완벽히 통제하기 위해 시스템 프롬프트, 로짓 편향(Logit Bias), 그리고 FSM 기반 마스킹 기법이 다층적으로 복합 적용된다. 예를 들어, 전사적인 데이터베이스 프레임워크가 업데이트되어 SQL 인젝션 취약점이 존재하는 구형 <code>execute_raw_sql()</code> 함수의 사용이 전면 금지되고 신형 <code>safe_parameterized_query()</code>를 반드시 사용해야 한다고 가정하자.</p>
<p>단순히 시스템 프롬프트에 “Do not use execute_raw_sql()“이라고 명시하는 네거티브 프롬프팅은 앞서 언급한 어텐션의 역설로 인해 모델이 오히려 해당 함수를 출력하게 만들 위험이 크다. 따라서 CI/CD 파이프라인과 연동된 오라클 시스템은 <code>execute_raw_sql</code>이라는 문자열을 토크나이저를 통해 모든 가능한 서브워드 시퀀스로 분할하고, 해당 시퀀스를 시작하는 토큰들에 일괄적으로 로짓 편향 <span class="math math-inline">-100</span>을 주입하여 신경망 수준에서 발화 확률을 소거한다. 나아가 FSM 디코딩을 도입하여 파이썬이나 자바스크립트의 추상 구문 트리(AST) 상태를 실시간으로 추적함으로써, 객체 지향 문법상 함수 호출이 이루어져야 하는 위치에서 구형 API 토큰 집합이 나타날 확률 자체를 원천 봉쇄한다. 이러한 다층적 블랙리스트 전략을 통해 AI가 생성한 코드는 기존 시스템의 엄격한 단위 테스트(Unit Test)를 통과하기 위한 결정론적 제약을 완벽히 준수하게 된다.</p>
<h3>4.2 도메인 특화 어휘 주입을 통한 법률 및 금융 데이터 자동화</h3>
<p>방대한 법률 문서나 금융 계약서를 검토하여 비정형 데이터를 시스템이 처리할 수 있는 정형 데이터로 추출하는 파이프라인에서는, 특정 도메인 용어의 정확한 사용이 법적 해석과 금융 거래의 무결성에 직결된다. 예를 들어 텍스트 내의 사건을 분류할 때 일반적인 ’절도(Theft)’라는 단어 대신 기업 내부의 특수 분류 기준에 따라 ‘횡령(Embezzlement)’ 또는 ’배임(Breach of Trust)’이라는 엄격한 용어를 사용해야만 후속 데이터베이스 트리거가 정상 작동하는 경우다. 단순한 자연어 지시만으로는 LLM이 동의어를 무분별하게 혼용하여 데이터 스키마를 망가뜨릴 위험이 상존한다.</p>
<p>이러한 고도의 무결성이 요구되는 환경에서 사전 주입 전략은 다음과 같이 구성된다:</p>
<ol>
<li><strong>마크다운 매핑과 긍정적 지시:</strong> 프롬프트 내에 <code>&lt;DOMAIN_GLOSSARY&gt;</code> 섹션을 구성하여 시스템이 요구하는 법적 요건과 정확히 맵핑되는 단어들을 마크다운 포맷으로 깔끔하게 전달한다. 네거티브 지시를 피하고 “재산상의 불법 영득 의사가 있는 경우 반드시 ’횡령’으로 분류할 것“과 같은 명시적 양성 제약으로 구성한다.</li>
<li><strong>NeuroLogic 디코딩 결합:</strong> 추론 단계에서 ‘배임’ 또는 ’횡령’이라는 단어가 출력 JSON 객체의 특정 필드 내에 적어도 한 번 이상 포함되도록(Irreversible satisfaction, S4 상태) 양성 제약을 걸고, 일상 용어인 ’돈을 훔침’과 같은 모호한 구문은 비가역적 불충족(S2 상태) 트리거로 설정하여 알고리즘적으로 블랙리스트화 한다.</li>
<li><strong>통계적 다중 평가(Aggregated Oracle) 파이프라인:</strong> 모델의 온도(Temperature) 파라미터를 <span class="math math-inline">0</span>으로 설정한 결정론적 환경 하에서도 단일 생성 결과를 즉시 신뢰하지 않는다. 동일한 문서를 여러 개의 하위 프롬프트 앵글로 분석하게 한 뒤, 도출된 결과들을 다수(Majority) 투표 기반의 통계적 검증 파이프라인에 통과시킴으로써 도메인 용어 사용의 무결성을 최종적으로 판단하는 집계형 오라클(Aggregated Oracle)을 구성한다.</li>
</ol>
<h2>5. 오라클 시스템의 보안 및 무결성 유지 (Anti-Prompt Injection 설계)</h2>
<p>아무리 수학적으로 정교하게 도메인 용어 사전과 금지어를 시스템에 주입하고 FSM 기반의 디코딩을 구현하더라도, 악의적인 사용자의 입력이나 오염된 외부 데이터(예: RAG 시스템이 검색해 온 신뢰할 수 없는 웹 문서)가 LLM에 주입되어 원래의 제약을 우회하도록 조작한다면 소프트웨어 오라클의 결정론적 신뢰성은 일거에 붕괴된다. 프롬프트 인젝션(Prompt Injection)은 이러한 블랙리스트 제약과 룰셋을 무력화하여 시스템을 장악하는 가장 치명적이고 빈번한 보안 위협이다.</p>
<p>오라클로서 기능하는 결정론적 시스템의 무결성을 보장하고 금지어 사전이 우회되는 것을 막기 위해서는, 소프트웨어 아키텍처 관점에서 다음과 같은 보안 설계 원칙이 엄격하게 준수되어야 한다.</p>
<ul>
<li><strong>구조적 경계의 엄격한 분리 (Delimiters):</strong> 사용자가 제공하는 입력 데이터나 외부 API에서 끌어온 텍스트는, 오라클의 동작 규칙을 정의하는 시스템 프롬프트의 지시문이나 도메인 사전과 물리적, 논리적으로 완벽히 격리되어야 한다. <code>"""</code>, <code>###</code>, 혹은 명확한 XML 태그인 <code>&lt;user_input&gt;</code>과 같은 명시적 구분자(Delimiter)를 사용하여, LLM이 사용자 입력을 시스템을 덮어쓰는 ’명령(Command)’이 아닌 처리해야 할 단순 ’데이터(Data)’로 취급하도록 아키텍처 수준에서 강제해야 한다.</li>
<li><strong>권한 분리 및 하이퍼파라미터 통제권의 은닉:</strong> 사용자가 클라이언트 프론트엔드나 오픈 API를 통해 시스템에 요청을 보낼 때, 로짓 편향(Logit Bias), <code>max_tokens</code>, <code>temperature</code> 등 결정론을 제어하는 핵심 하이퍼파라미터를 직접 조작할 수 있는 권한을 노출해서는 절대 안 된다. 악의적인 사용자가 API 페이로드를 변조하여 금지어에 설정된 로짓 편향 <span class="math math-inline">-100</span>을 임의로 해제하거나, 반대로 특정 무의미한 토큰의 로짓을 극대화하여 무한 루프를 유도함으로써 서버 자원을 고갈시키는 DoW(Denial of Wallet) 및 DoS 공격을 감행할 수 있기 때문이다. 파라미터 제어 계층과 용어 사전 룰셋은 철저히 백엔드의 격리된 시스템(Middleware)에서만 접근 및 관리되도록 분리해야 한다.</li>
<li><strong>민감 데이터의 외부화 및 FSM 마스킹 위임:</strong> 오라클이 강제해야 할 도메인 사전이 너무 방대하거나, 내부 데이터베이스 스키마 및 기밀 식별자와 같은 민감한 정보를 금지어(Blacklist)로 포함하고 있을 경우, 이를 텍스트 형태의 시스템 프롬프트에 직접 주입하는 것은 심각한 프롬프트 유출(System Prompt Leakage) 위험을 초래한다. 이러한 경우, 금지어와 용어 사전을 LLM의 컨텍스트 윈도우에 직접 노출하는 대신, 앞서 설명한 FSM 디코딩 계층(Outlines 등)의 정규 표현식 마스크로 변환하여 LLM 외부의 추론 엔진 레벨에서 필터링과 강제가 처리되도록 아키텍처를 설계해야 한다. 이 구조는 LLM이 자신이 무엇을 피하고 있는지 그 단어 자체를 ‘모르는’ 상태에서도, 시스템적으로 금지어가 완벽히 차단되는 가장 고도화된 보안 상태를 제공한다.</li>
</ul>
<p>결론적으로, 거대 언어 모델을 비결정적인 챗봇이 아닌 소프트웨어 개발의 결정론적 정답지(Oracle)로 활용하기 위한 용어 및 금지어 사전 주입 전략은, 단순히 프롬프트를 예쁘게 작성하는 단일 계층의 팁에 머물러서는 안 된다. 이는 마크다운 구조화를 통한 모델의 인지적 맥락 부여, 로짓 편향을 이용한 확률 분포의 수학적이고 강제적인 절단, FSM 및 어휘 제약 디코딩 알고리즘을 통한 탐색 공간의 구조적 통제, 그리고 프롬프트 인젝션을 막아내는 견고한 시스템 보안 아키텍처 설계가 모두 톱니바퀴처럼 결합된 종합적인 소프트웨어 엔지니어링 시스템으로 구현되어야만 비로소 달성될 수 있다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>(PDF) Verifiable LLM-Generated Test Oracles: Ensuring Consistency, Correctness, and Explainability in AI- Assisted Testing - ResearchGate, https://www.researchgate.net/publication/398511554_Verifiable_LLM-Generated_Test_Oracles_Ensuring_Consistency_Correctness_and_Explainability_in_AI-_Assisted_Testing</li>
<li>The Probabilistic Paradox: Why LLMs Fail in Deterministic Domains — and How to Fix It, https://medium.com/@ensigno/the-probabilistic-paradox-why-llms-fail-in-deterministic-domains-and-how-to-fix-it-be21b5e20bda</li>
<li>Challenges in Testing Large Language Model Based Software: A Faceted Taxonomy, https://arxiv.org/html/2503.00481v2</li>
<li>The unreasonable effectiveness of an LLM agent loop with tool use - Hacker News, https://news.ycombinator.com/item?id=43998472</li>
<li>Introducing AI Security: A Dedicated Violation Category for AI Risk in Application Security, https://cycode.com/blog/ai-security-application-risk-owasp-llm/</li>
<li>Testing AI Systems: Handling the Test Oracle Problem - DEV Community, https://dev.to/qa-leaders/testing-ai-systems-handling-the-test-oracle-problem-3038</li>
<li>7 prompt design techniques for generative AI every journalist should know - Paul Bradshaw, https://paulbradshaw.medium.com/7-prompt-design-techniques-for-generative-ai-every-journalist-should-know-d1c94405bc82</li>
<li>4o-mini and negative prompting - OpenAI Developer Community, https://community.openai.com/t/4o-mini-and-negative-prompting/1049113</li>
<li>Comparing Large Language Model AI and Human-Generated Coaching Messages for Behavioral Weight Loss - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC12816013/</li>
<li>The Prompting Technique that 10x your AI’s Quality | by Ibaad The Agenteer | Medium, https://medium.com/@The-coders-rocket/the-prompting-technique-that-10x-your-ais-quality-805268887ff3</li>
<li>[PDF] Understanding the Impact of Negative Prompts: When and, https://www.semanticscholar.org/paper/Understanding-the-Impact-of-Negative-Prompts%3A-When-Ban-Wang/aebc4f1ffc89820c00c7728ea0ad1fefccc7f6c1</li>
<li>Model Behavior Specification by Leveraging LLM Self-Playing and Self-Improving - arXiv, https://arxiv.org/html/2503.03967v1</li>
<li>Survey and analysis of hallucinations in large language models: attribution to prompting strategies or model behavior - Frontiers, https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1622292/full</li>
<li>Survey and analysis of hallucinations in large language models: attribution to prompting strategies or model behavior - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC12518350/</li>
<li>Why use Markdown in your Agents’ System Prompt? | by Edmilson Prata da Silva | Medium, https://medium.com/@edprata/why-use-markdown-in-your-agents-system-prompt-41ad258a25c7</li>
<li>Does Prompt Formatting Have Any Impact on LLM Performance? - arXiv, https://arxiv.org/html/2411.10541v1</li>
<li>Evaluating Large Language Models on Controlled Generation Tasks - Semantic Scholar, https://www.semanticscholar.org/paper/Evaluating-Large-Language-Models-on-Controlled-Sun-Tian/91b5d8287e2353f667f90641e3d930cc5ee0942e</li>
<li>Which Nested Data Format Do LLMs Understand Best? JSON vs. YAML vs. XML vs. Markdown - Improving Agents, https://www.improvingagents.com/blog/best-nested-data-format/</li>
<li>Boosting AI Performance: The Power of LLM-Friendly Content in Markdown, https://developer.webex.com/blog/boosting-ai-performance-the-power-of-llm-friendly-content-in-markdown</li>
<li>Markdown vs JSON? Which one is better for latest LLMs? : r/PromptEngineering - Reddit, https://www.reddit.com/r/PromptEngineering/comments/1l2h84j/markdown_vs_json_which_one_is_better_for_latest/</li>
<li>Markdown vs JSON: Choosing the Right Format for LLM Prompts | WebcrawlerAPI Blog, https://webcrawlerapi.com/blog/markdown-vs-json-choosing-the-right-format-for-llm-prompts</li>
<li>Overview of prompting strategies | Generative AI on Vertex AI | Google Cloud Documentation, https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-design-strategies</li>
<li>Prompt engineering techniques and best practices: Learn by doing with Anthropic’s Claude 3 on Amazon Bedrock | Artificial Intelligence, https://aws.amazon.com/blogs/machine-learning/prompt-engineering-techniques-and-best-practices-learn-by-doing-with-anthropics-claude-3-on-amazon-bedrock/</li>
<li>Using logit bias to alter token probability with the OpenAI API …, https://help.openai.com/en/articles/5247780-using-logit-bias-to-alter-token-probability-with-the-openai-api</li>
<li>Comprehensive Guide to Decoding Parameters and Hyperparameters in Large Language Models (LLMs) - DEV Community, https://dev.to/hardiksankhla/comprehensive-guide-to-decoding-parameters-and-hyperparameters-in-large-language-models-llms-35lp</li>
<li>weight_utils - vLLM, https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/weight_utils/</li>
<li>Engine Arguments - vLLM, https://docs.vllm.ai/en/stable/configuration/engine_args/</li>
<li>LLMs, non-programmatic computing, and logit bias. | by Shelby Jenkins | Medium, https://jshelbyj.medium.com/llms-non-programmatic-computing-and-logit-bias-70a3e7344713</li>
<li>Cohere Command models - Amazon Bedrock - AWS Documentation, https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-cohere-command.html</li>
<li>What is Logit Bias and how to use it, https://www.vellum.ai/llm-parameters/logit-bias</li>
<li>Is anyone working on ‘conditional’ logit biasing? (How to potentially improve repetition penalty?) · Issue #3149 · ggml-org/llama.cpp - GitHub, https://github.com/ggml-org/llama.cpp/issues/3149</li>
<li>Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation - ResearchGate, https://www.researchgate.net/publication/325447159_Fast_Lexically_Constrained_Decoding_with_Dynamic_Beam_Allocation_for_Neural_Machine_Translation</li>
<li>Constrained Decoding for Secure Code Generation - arXiv.org, https://arxiv.org/html/2405.00218v3</li>
<li>Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search - ACL Anthology, https://aclanthology.org/P17-1141.pdf</li>
<li>(PDF) Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search, https://www.researchgate.net/publication/316451245_Lexically_Constrained_Decoding_for_Sequence_Generation_Using_Grid_Beam_Search</li>
<li>(PDF) Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search, https://www.researchgate.net/publication/318742146_Lexically_Constrained_Decoding_for_Sequence_Generation_Using_Grid_Beam_Search</li>
<li>Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search, https://aclanthology.org/P17-1141/</li>
<li>NeuroLogic Decoding:(Un) supervised Neural Text Generation with …, https://arxiv.org/abs/2010.12884</li>
<li>NeuroLogic Decoding: (Un)supervised Neural Text Generation with Predicate Logic Constraints - ACL Anthology, https://aclanthology.org/2021.naacl-main.339.pdf</li>
<li>[PDF] NeuroLogic Decoding: (Un)supervised Neural Text Generation with Predicate Logic Constraints | Semantic Scholar, https://www.semanticscholar.org/paper/NeuroLogic-Decoding%3A-(Un)supervised-Neural-Text-Lu-West/2c5bf29079cd958a2bef150077a02a1deb300652</li>
<li>[2307.09702] Efficient Guided Generation for Large Language Models - arXiv, https://arxiv.org/abs/2307.09702</li>
<li>[NLG] Outlines — Efficient Guided Generation for Large Language Models (Willard &amp; Louf 2023) - Youngrok Song, https://id2thomas.medium.com/nlg-outlines-efficient-guided-generation-for-large-language-models-willard-louf-2023-3c9463543901</li>
<li>[PDF] Efficient Guided Generation for Large Language Models | Semantic Scholar, https://www.semanticscholar.org/paper/Efficient-Guided-Generation-for-Large-Language-Willard-Louf/c4ceaef35bca063815f50d90a087acbd07a65478</li>
<li>Efficient Guided Generation for Large Language Models - arXiv, https://arxiv.org/pdf/2307.09702</li>
<li>FunctionGemma - Google - Kaggle, https://www.kaggle.com/models/google/functiongemma/discussion/666233</li>
<li>Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge - arXiv.org, https://arxiv.org/html/2505.17037v1</li>
<li>What Is a Prompt Injection Attack? Definition, Examples | Proofpoint US, https://www.proofpoint.com/us/threat-reference/prompt-injection</li>
<li>Proper Prompt Injection Protection Techniques: Based of Current Research | by Femi Oyesanya | Medium, https://medium.com/@oyesanyf/proper-prompt-injection-techniques-based-of-current-research-713ee608a216</li>
<li>Prompt Engineering Best Practices: Tips, Tricks, and Tools | DigitalOcean, https://www.digitalocean.com/resources/articles/prompt-engineering-best-practices</li>
<li>LLM Prompt Injection Prevention - OWASP Cheat Sheet Series, https://cheatsheetseries.owasp.org/cheatsheets/LLM_Prompt_Injection_Prevention_Cheat_Sheet.html</li>
<li>LLM07:2025 System Prompt Leakage - OWASP Gen AI Security Project, https://genai.owasp.org/llmrisk/llm072025-system-prompt-leakage/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>