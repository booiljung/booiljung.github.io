<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:4.4.3 "모른다면 모른다고 답하라"는 지시문의 구체적 변형과 효과</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>4.4.3 "모른다면 모른다고 답하라"는 지시문의 구체적 변형과 효과</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 4. AI 모델 응답의 일관성 확보를 위한 프롬프트 엔지니어링 및 파라미터 제어</a> / <a href="index.html">4.4 시스템 프롬프트(System Prompt)를 이용한 페르소나 및 규칙 고정</a> / <span>4.4.3 "모른다면 모른다고 답하라"는 지시문의 구체적 변형과 효과</span></nav>
                </div>
            </header>
            <article>
                <h1>4.4.3 “모른다면 모른다고 답하라“는 지시문의 구체적 변형과 효과</h1>
<p>인공지능(AI)을 활용한 소프트웨어 개발 및 테스트 환경에서 가장 경계해야 할 요소는 언어 모델이 정답을 알지 못함에도 불구하고 그럴듯한 허위 정보를 생성하는 환각(Hallucination) 현상이다. 특히 자동화된 테스트 파이프라인 내에서 결정론적 정답지(Deterministic Ground Truth)를 제공해야 하는 오라클(Oracle)로서 대형 언어 모델(LLM)을 활용할 때, 부정확한 정보의 무분별한 생성은 CI/CD 파이프라인 전체의 신뢰성을 붕괴시키고 치명적인 소프트웨어 결함을 프로덕션 환경으로 통과시키는 원인이 된다. 확률적(Stochastic) 특성을 지닌 언어 모델을 결정론적(Deterministic) 소프트웨어 테스팅 도구로 편입시키기 위해서는 모델이 자신의 지식 경계(Knowledge Boundary)를 스스로 인지하고, 불확실성이 높은 상황에서는 답변을 기권(Abstention)하도록 강제하는 제어 메커니즘이 필수적이다.</p>
<p>이러한 제어 메커니즘의 가장 기본적이면서도 핵심적인 프롬프트 엔지니어링 기법이 바로 “모른다면 모른다고 답하라(If you don’t know, say you don’t know)“는 지시문의 주입이다. 이 지시문은 단순한 자연어 명령을 넘어, 확률적 생성 모델의 토큰 출력 분포를 결정론적 실패(Deterministic Failure) 혹은 기권 상태로 유도하는 논리적 회로 차단기(Circuit Breaker) 역할을 수행한다. 본 절에서는 해당 지시문의 구체적인 변형 형태들을 분류하고, 각 변형이 모델의 신뢰성, 정밀도, 재현율, 그리고 소프트웨어 테스트 오라클로서의 성능에 미치는 정량적, 정성적 효과를 실전 소프트웨어 개발 예제와 함께 심층적으로 분석한다.</p>
<h2>1. 지시문의 논리적 구조와 구체적 변형 형태</h2>
<p>“모른다면 모른다고 답하라“는 지시문은 실무 환경에서 단일한 문장으로 사용되기보다는, 소프트웨어 도메인의 요구사항과 시스템의 허용 오차(Tolerance) 수준에 따라 매우 다양한 형태로 변형되어 시스템 프롬프트(System Prompt)에 주입된다. 이러한 변형은 단순히 어휘의 차이를 넘어, 모델이 내부적으로 불확실성을 평가하는 기준선(Threshold)을 이동시키고, 기권을 선언할 때 반환하는 출력 포맷을 통제하는 중대한 역할을 한다. 문헌 및 실무 사례를 종합하면, 이 지시문은 크게 명시적 지시, 정직성 및 페르소나 강조, 검증 가능성 요구, 그리고 출력 포맷 강제라는 네 가지 범주로 세분화된다.</p>
<p>첫째, 명시적 지시(Explicit Instruction) 형태이다. 이는 모델에게 허위 정보 생성의 위험성을 직접적으로 경고하며 답변을 차단하는 가장 직관적이고 보편적인 방식이다. 예를 들어, “답을 모른다면 질문에 대해 거짓 정보를 제공하지 마라(If you don’t know the answer to a question, please don’t provide false information)“와 같은 프롬프트가 이에 해당한다. 이러한 형태는 모델이 내부 파라미터 지식에 의존하여 추측성 답변을 생성하려는 본원적인 경향을 억제하는 데 일차적인 효과를 발휘한다. 소프트웨어 코드 리뷰나 의료 데이터 분석과 같이 사실적 정확성이 생명인 도메인에서는 이러한 명시적 통제가 필수적인 프롬프트 엔지니어링 전략으로 활용된다. 단순한 지시문이지만, 이 문장이 시스템 프롬프트의 최상단에 위치할 때 모델은 토큰 생성 과정에서 사실 확인이 불가능한 경로(Path)를 선택하는 것을 주저하게 된다.</p>
<p>둘째, 정직성 및 페르소나 강조(Emphasizing Honesty and Persona) 형태이다. 이는 모델에 특정한 전문가적 페르소나를 부여함과 동시에, 그 전문가가 지녀야 할 윤리적, 논리적 한계를 명확히 규정하는 방식이다. “확신할 수 없다면 ’모른다’고 말하라(If unsure, say ‘I don’t know’)” 혹은 “당신의 역할은 진실된 응답을 제공하는 것이며, 모르는 정보에 대해 지어내려 하지 마라“와 같은 지시문이 주로 사용된다. 소프트웨어 공학 측면에서 이는 모델이 오라클로서 기능할 때, 마치 엄격한 시니어 개발자가 코드 리뷰 과정에서 자신이 알지 못하는 레거시 시스템의 구조에 대해 함부로 추측하지 않고 “이 부분은 추가 확인이 필요하다“고 선언하는 것과 동일한 논리적 태도를 취하도록 유도한다. 특정 논문에서는 모델에게 “만약 자신이 없다면 ’나는 확신할 수 없다(I’m not sure)’고 말하고 어떤 정보가 누락되었는지 설명하라“고 지시하는 것이 단순히 “세계 최고의 전문가처럼 행동하라“고 지시하는 것보다 환각을 줄이는 데 훨씬 실용적이라고 지적한다.</p>
<p>셋째, 검증 가능성 요구(Requiring Verifiability) 형태이다. 이 변형은 검색 증강 생성(Retrieval-Augmented Generation, RAG) 시스템이나 입력 컨텍스트가 극도로 제한된 소프트웨어 유닛 테스트 환경에서 주로 사용된다. “신뢰할 수 있는 출처를 통해 검증 가능한 정보만 제공하라” 혹은 “제공된 컨텍스트 내에 정보가 없다면 답변하지 마라(Do not answer if the information is not in the context provided)“와 같이 지시한다. 이는 모델의 지식 소스를 사전 학습된 내부 파라미터가 아닌, 외부에서 주입된 데이터로 철저히 제한함으로써 환각의 발생 가능성을 원천적으로 차단하려는 시도이다. 이 방식을 적용하면 모델은 외부 문서 혹은 주어진 코드 스니펫과의 교차 검증을 내부적으로 수행한 후, 논리적 연결 고리가 부족할 경우 즉각적으로 기권 상태로 전환된다.</p>
<p>넷째, 출력 포맷 강제(Format-forcing) 형태이다. 결정론적 소프트웨어 시스템이나 자동화된 CI/CD 파이프라인 내에서 LLM의 출력을 다른 소프트웨어 모듈이 파싱(Parsing)해야 하는 경우, 자연어 형태의 서술형 “모릅니다“는 처리하기 까다로운 예외 상황을 만든다. 따라서 기권의 상태를 특정 열거형 상수(Enum), 정수형 코드, 혹은 명확한 불리언(Boolean) 형태로 반환하도록 강제해야 한다. 예를 들어 “답을 모른다면 ’Neither’라고 답하라”  혹은 “확신할 수 없다면 ’99’로 응답하라” 와 같은 프롬프트가 이에 해당한다. 이는 비결정론적인 LLM의 자연어 출력을 시스템이 이해할 수 있는 결정론적 에러 코드(Error Code)나 널(Null) 값으로 치환하는 핵심 기법으로, 자동화된 버그 트리아지(Bug Triage) 및 테스트 오라클 시스템 구축에 있어 가장 실무적으로 널리 쓰이는 변형 형태이다.</p>
<table><thead><tr><th><strong>지시문 변형 범주</strong></th><th><strong>프롬프트 적용 예시 (영문 원문 기반 번역 및 요약)</strong></th><th><strong>논리적 목적 및 기대 효과</strong></th><th><strong>시스템 파싱 및 자동화 적합성</strong></th></tr></thead><tbody>
<tr><td>명시적 지시 (Explicit Instruction)</td><td>답을 모르는 질문에 대해 절대 거짓 정보를 공유하거나 지어내지 마라.</td><td>맹목적인 추측성 토큰 생성 차단 및 오정보 전파의 1차적 방어</td><td>낮음 (서술형 자연어로 반환되어 정규식 처리가 복잡함)</td></tr>
<tr><td>정직성/페르소나 (Honesty/Persona)</td><td>확신이 없다면 ’모른다’고 명시적으로 밝히고, 누락된 컨텍스트가 무엇인지 설명하라.</td><td>보수적인 전문가적 태도 유지 및 사용자(혹은 개발자)의 신뢰도 향상</td><td>낮음 (답변 거부 사유가 포함된 긴 텍스트가 반환됨)</td></tr>
<tr><td>검증 가능성 요구 (Requiring Verifiability)</td><td>제공된 소스 코드나 로그 컨텍스트 내에 답이 없다면 즉시 답변을 거절하라.</td><td>외부 소스(RAG 등) 기반의 엄격한 데이터 그라운딩(Grounding) 강제</td><td>중간 (특정 거부 패턴의 키워드 감지 필요)</td></tr>
<tr><td>출력 포맷 강제 (Format-forcing)</td><td>정보가 부족하여 판단할 수 없다면 오직 ‘99’ 혹은 ’Neither’로만 출력하라.</td><td>후속 파이프라인 통합을 위한 기권 상태의 기계적, 결정론적 식별</td><td>높음 (조건문 및 자료형 파싱에 최적화됨)</td></tr>
</tbody></table>
<h2>2. 지시문이 모델의 정밀도(Precision)와 재현율(Recall)에 미치는 상충 효과</h2>
<p>소프트웨어 평가 및 검증 시스템에서 “모른다면 모른다고 답하라“는 지시문을 적용했을 때 나타나는 가장 두드러지고 정량적인 변화는 모델 출력의 정밀도(Precision)와 재현율(Recall) 사이에서 발생하는 극단적인 상충(Trade-off) 관계이다. 소프트웨어 테스트, 정보 추출, 그리고 버그 판별 태스크에서 정밀도는 ’모델이 확신을 가지고 제시한 답변 중 실제 정답의 비율’을 의미하며, 재현율은 ’전체 정답(Ground Truth) 중 모델이 올바르게 찾아내어 답변을 완료한 비율’을 의미한다. 환각을 줄이기 위해 기권을 명시적으로 허용하는 프롬프트를 주입하면, 모델은 불확실한 상황에서 억지로 답변을 생성하는 대신 기권을 선택하므로 오답(False Positive)의 수가 급격히 감소한다. 결과적으로 정밀도는 비약적으로 상승하게 된다.</p>
<p>그러나 이러한 정밀도의 상승은 필연적으로 재현율의 뼈아픈 하락을 동반한다. 모델이 방어적이고 보수적인 태도를 취하게 되면서, 실제로 충분히 유추하거나 답변할 수 있는 맥락이 존재함에도 불구하고 내부 확률 계산 과정에서 약간의 모호성만 감지되면 답변을 포기해 버리는 과도한 기권(Over-abstention) 현상이 발생하기 때문이다. 양(Yang) 등의 2025년 연구 코퍼스 분석에 따르면, 매우 장황하고 엄격하게 “모른다면 모른다“를 강조한 지시문을 적용한 결과 특정 쿼리군(Q1~Q2)에서는 정밀도와 정확도가 99.5%에 달하는 완벽에 가까운 수준을 보였다. 하지만 모호성이 약간 섞인 조건(Q4)에서는 재현율은 유지되었으나 정밀도가 37.5%로 폭락하거나, 혹은 반대로 프롬프트의 길이나 엄격성에 따라 정밀도를 방어하는 대신 전체적인 응답률(Recall) 자체가 20.8%까지 현저히 떨어지는 결과가 번갈아 관찰되었다. 이는 프롬프트가 정밀도와 재현율의 균형을 맞추는 완벽한 해결책이 아니라, 두 지표 사이의 시소를 어느 쪽으로 기울일지 결정하는 튜닝 다이얼(Tuning Dial)에 불과함을 보여준다.</p>
<p>실제로 질의응답(QA) 평가 벤치마크인 TopiOCQA, Natural Questions(NQ), HotpotQA 등을 사용한 연구에서 “모른다면 모른다고 출력하라(output ‘I don’t know’)“는 명시적 지시를 삽입했을 때 모델 간의 극명한 행동 차이가 드러났다. GPT-3.5와 같은 모델은 무관한 문서가 주어졌을 때 98%의 확률로 답변을 기권하여 훌륭한 정밀도를 유지했으나, Alpaca와 같은 모델은 지시문을 무시하고 거의 항상 답변을 생성하여 정밀도가 훼손되었다. 또한, Flan-T5 모델은 NQ 벤치마크에서는 성공적으로 기권했지만 TopiOCQA에서는 기권에 실패하는 등 데이터 분포에 따라 지시문의 효과가 일관되지 않음을 입증했다.</p>
<p>더욱이 맥락 교란(Context Perturbation) 실험에 따르면 이러한 지시문은 예상치 못한 모델의 행동을 유발하기도 한다. 원(Wen) 등의 논문에서는 금본위 맥락(Gold Context, 완벽한 정답 맥락)을 제거하거나 무관한 맥락(Irrelevant Context)으로 대체하는 실험을 진행했다. 직관적으로는 모델에 무관한 맥락이 주어졌을 때 지시문을 충실히 이행하여 답변을 기권해야 마땅하다. 하지만 상당수의 LLM들은 불리언(Boolean) 질문 등에 대해 기권하지 못하고 억지로 거짓 답변을 생성하는 한계를 보였다. 더욱 반직관적인 사실은, 특정 상황에서는 정답이 포함된 원래의 맥락에 의도적으로 무관한 맥락(노이즈)을 추가하거나 교체했을 때 오히려 모델의 기권 성능(Abstention Performance)이 향상되었고, 이것이 역설적으로 전체 태스크 수행 능력(Task Performance)의 상승으로 이어졌다는 점이다. 이는 “모른다면 모른다“는 지시문이 모델의 논리적 추론 능력을 본질적으로 향상시키는 것이 아니라, 주어진 텍스트 패턴 내에서 혼란도가 가중될 때 임계치를 넘어서며 기계적으로 기권을 발생시키는 표면적 트리거 장치로 작동할 수 있음을 시사한다.</p>
<p>이러한 정밀도와 재현율의 충돌 속에서, 소프트웨어 엔지니어는 자신이 구축하려는 오라클 시스템의 성격에 따라 프롬프트를 조율해야 한다. 예를 들어, 보안 취약점 분석이나 금융 트랜잭션 검증과 같이 거짓 음성(False Negative, 버그를 놓침)보다 거짓 양성(False Positive, 정상 코드를 버그로 오진함)의 비용이 훨씬 큰 도메인에서는 재현율의 희생을 감수하고서라도 정밀도를 극대화하는 가장 강력한 형태의 기권 지시문을 사용해야 한다. 반면, 코드 추천이나 초안 작성과 같이 개발자의 생산성을 보조하는 도구에서는 지나친 기권이 효용성을 떨어뜨리므로, 모호할 때는 최선의 추측(Best Guess)을 하되 확신이 낮음을 덧붙이도록 지시문을 완화하는 전략이 필요하다.</p>
<h2>3. 자동화된 소프트웨어 테스트와 결정론적 오라클 관점에서의 해석</h2>
<p>AI를 활용한 소프트웨어 개발, 특히 지속적 통합 및 배포(CI/CD) 파이프라인 내부에서 LLM을 자동화된 코드 리뷰어, 버그 트리아지(Bug Triage) 엔진, 혹은 테스트 결과 판독기로 활용할 때 모델은 단순한 챗봇이 아니라 결정론적 오라클(Deterministic Oracle)로서의 역할을 엄격하게 요구받는다. 소프트웨어 테스팅에서 오라클이란 테스트 케이스의 실행 결과가 올바른지(Pass) 틀렸는지(Fail)를 판별하는 절대적이고 결정론적인 기준이다. 일반적인 소프트웨어는 결정론적이므로 오라클 역시 완벽히 명확한 이진 상태를 반환해야 한다. 그러나 LLM은 본질적으로 비결정론적이므로, 입력된 소스 코드나 런타임 로그 데이터의 맥락이 부족할 때 환각을 통해 임의의 결함을 날조할 위험이 있다. 이때 “모른다면 모른다고 답하라“는 지시문은 비결정론적 모델 내부에 소프트웨어 공학적인 예외 처리(Exception Handling) 경로를 구축하는 핵심 접근법이 된다.</p>
<p>실무에서 이러한 오라클의 품질은 ’모호성 탐지율(Ambiguity Detection Rate)’이라는 핵심 성과 지표(KPI)로 측정된다. 예를 들어, 대규모 분산 시스템 환경에서 LLM에게 특정 마이크로서비스의 연쇄적인 에러 로그를 분석하여 버그의 근본 원인을 파악하도록 지시했다고 가정해 보자. 만약 로그 데이터가 중간에 잘려 있거나 중요한 환경 변수 정보가 누락되어 있다면, 오라클로서의 LLM은 빈약한 증거를 바탕으로 억측을 하여 잘못된 컴포넌트를 결함의 원인으로 지목해서는 안 된다. 개발자가 거짓 원인을 추적하느라 낭비하는 시간은 막대하기 때문이다. 대신 LLM은 “제공된 Schannel 로그와 데이터베이스 접근 기록만으로는 근본 원인을 알 수 없다. 네트워크 패킷 덤프나 추가적인 인증 로그가 필요하다“라고 명확히 기권 및 해명을 해야 한다. 이처럼 맥락이 누락되거나 불충분한 쿼리에 대해 자신감 있는 추측(Confident Guess) 대신 명확한 기권이나 추가 정보 요청을 트리거하는 비율이 바로 모호성 탐지율이다. 이 지표가 높은 LLM 오라클 시스템만이 프로덕션 환경의 자동화 파이프라인에 통합될 자격을 얻는다.</p>
<p>더 나아가, 이 지시문은 시스템의 분산(Variance) 통제와 일관성(Consistency) 확보라는 소프트웨어 테스트의 대전제를 달성하는 데 중대한 영향을 미친다. 테스트 오라클이 동일한 에러 로그에 대해 어제는 A 모듈의 결함이라고 답하고 오늘은 B 모듈의 결함이라고 답한다면, 해당 오라클을 기반으로 한 회귀 테스트(Regression Test)는 완전히 무용지물이 된다. “모른다“는 지시문을 통해 모델이 스스로의 지식 및 맥락 경계를 명확히 식별하도록 강제하면, 응답 불가능한 모호한 영역에 대해서는 일관되게 기권(예: ‘Unknown’, ‘N/A’) 상태를 반환하게 되므로 오라클 판별의 재현성(Reproducibility)이 크게 확보된다.</p>
<p>소프트웨어 개발 실무에서 LLM을 평가자(LLM-as-a-Judge)로 활용한 구체적인 사례를 살펴보면 이 지시문의 위력이 명확히 드러난다. 이커머스 검색 관련성 평가 시스템에서, 개발자는 LLM을 사용해 사용자 검색어(Query)에 대해 좌측 상품(Product LHS)과 우측 상품(Product RHS) 중 어느 것이 더 관련성이 높은지 판별하는 오라클을 구축했다. 이때 단순히 “LHS와 RHS 중 하나를 선택하라“고 지시했을 경우, 모델은 관련성을 판단할 메타데이터가 부족함에도 불구하고 임의로 하나를 찍어 환각을 발생시켰다. 그러나 프롬프트에 “어느 쪽도 아님 혹은 상품 속성 정보가 더 필요함(Neither / Need more product attributes)“이라는 기권 선택지를 명확히 부여하고 “확신할 때만 LHS나 RHS를 응답하라“고 지시했을 때, 평가의 신뢰도와 인간 평가자와의 일치율이 비약적으로 상승했다. “알 수 없음“을 선택하도록 허용하는 것은 단일 특성에 대한 맹목적인 확률적 베팅을 억제하고, 충분한 속성이 종합되었을 때만 확정적인 결정론적 정답지를 도출하도록 만드는 강력한 소프트웨어 아키텍처적 안전장치이다.</p>
<h2>4. 불확실성의 수학적 정량화와 프롬프트 제어의 한계성</h2>
<p>“모른다면 모른다고 답하라“는 지시문이 언어 모델 내부에서 어떻게 작용하는지, 그리고 왜 이 지시문만으로는 한계가 있는지 이해하기 위해서는 LLM의 불확실성 정량화(Uncertainty Quantification)에 대한 수학적 접근이 필수적이다. 대형 언어 모델 <span class="math math-inline">M</span>에 입력 쿼리 <span class="math math-inline">x_q</span>가 주어졌을 때, 생성된 출력들의 불확실성을 측정하여 스칼라 점수로 매핑하는 불확실성 함수 <span class="math math-inline">U_x</span>는 다음과 같이 정의될 수 있다.<br />
<span class="math math-display">
U_x = \mathcal{U}(\{M(x_i^q)\}_{i=1}^n)
</span><br />
여기서 <span class="math math-inline">\{M(x_i^q)\}_{i=1}^n</span>은 모델 <span class="math math-inline">M</span>이 생성한 <span class="math math-inline">n</span>개의 응답 집합을 의미하며, 함수 <span class="math math-inline">\mathcal{U}</span>는 여러 응답 간의 불확실성을 집계한다. 모델은 본질적으로 조건부 확률 <span class="math math-inline">p(y_i \vert x)</span>에 기반하여 다음 토큰 <span class="math math-inline">y_i</span>를 자동 회귀적(Auto-regressive)으로 예측한다. 특정 사실이나 코드 문법에 대해 모델 내부의 파라미터 가중치가 강력하게 형성되어 있다면 특정 토큰 시퀀스의 <span class="math math-inline">p(y_i \vert x)</span> 값이 압도적으로 높게 나타나며 불확실성 <span class="math math-inline">U_x</span>는 0에 수렴하게 된다. 반대로 사전 학습 데이터에 존재하지 않는 지식의 공백(Knowledge Gap)이 존재하는 영역에서는 여러 토큰 간의 확률이 고르게 분산되어 정보 엔트로피와 혼란도(Perplexity)가 급격히 상승하고 불확실성이 극대화된다.</p>
<p>“모른다면 모른다“는 지시문은 프롬프트 컨텍스트를 통해 모델의 어텐션 메커니즘(Attention Mechanism)에 강하게 관여함으로써, 이러한 내부적 불확실성이 특정 임계치를 초과할 경우 확률 분포를 강제로 찌그러뜨려 “모”, “른”, “다” 혹은 “N/A“와 같은 기권 토큰 시퀀스로 급격하게 쏠리도록(Skew) 유도하는 역할을 한다. 그러나 이러한 프롬프트 수준의 제어는 근본적인 수학적, 구조적 한계를 지닌다. 사전 학습(Pre-training) 단계에서 언어 모델은 텍스트의 논리적 진실성을 판별하도록 훈련된 것이 아니라, 수백억 개의 문서에서 단어의 동시 등장 확률 곡선을 부드럽게 근사하고 크로스 엔트로피 손실(Cross-entropy loss)을 최소화하도록 훈련되었기 때문이다. 따라서 스스로 불확실성을 메타적으로 계산하고 주도적으로 기권을 선언하는 능력이 모델의 기저 아키텍처 내에 깊이 내재화되어 있지 않다.</p>
<p>이러한 한계는 프롬프트 변동에 대한 과민성(Hypersensitivity)이라는 치명적인 단점으로 소프트웨어 테스트 환경에서 드러난다. 2025년 최신 연구들에 따르면, 정답 옵션의 순서를 단순히 섞거나 지시문의 어조를 살짝 바꾸는 것과 같은 표면적이고 미미한 프롬프트의 동요(Perturbation)만으로도 모델이 지식의 공백을 탐지하는 일관성 측정치가 31% 수준으로 급락했다. 더 충격적인 것은, 퓨샷(Few-shot) 프롬프팅을 통해 “이러한 모호한 상황에서는 기권하라“는 예시를 주입하여 모델을 안내했음에도 불구하고, 탐지 일관성이 최저 4%까지 떨어지는 현상이 대형 파라미터 모델(70B 규모)에서도 관찰되었다는 점이다. 이는 프롬프트 지시문이 모델에게 진정한 의미의 메타 인지(Metacognition, 자신이 무엇을 확실히 알고 무엇을 모르는지 아는 능력)를 부여하는 것이 아니라, 단순히 조건부 확률 <span class="math math-inline">p(y_i \vert x)</span>의 표면적인 텍스트 패턴 매칭을 통해 특정 상황에서만 맹목적으로 기권 단어를 내뱉도록 유도하는 미봉책에 불과함을 수학적, 통계적으로 방증한다.</p>
<table><thead><tr><th><strong>불확실성 제어 방식</strong></th><th><strong>수리적 메커니즘 설명</strong></th><th><strong>소프트웨어 오라클 적용 시의 한계점</strong></th></tr></thead><tbody>
<tr><td>프롬프트 지시문 주입</td><td>어텐션 가중치를 조작하여 <span class="math math-inline">p(y_i \vert x)</span> 분포를 임의의 기권 토큰으로 강제 편향시킴</td><td>프롬프트의 미세한 텍스트 변화나 토큰 순서 변경에 극도로 민감하여 일관성이 파괴됨 (과민성)</td></tr>
<tr><td>언어 모델의 본질적 추론</td><td>크로스 엔트로피 손실 최소화를 통해 학습된 다음 토큰 예측 확률망 가동</td><td>모델은 정답을 ’알지 못함(Unknown)’의 상태가 아니라 단순히 ’가장 그럴듯한(Plausible) 토큰의 나열’로 취급함</td></tr>
</tbody></table>
<h2>5. 기저 아키텍처의 개선: 거절 인지 지시 조정(Refusal-Aware Instruction Tuning, R-Tuning)과의 시너지</h2>
<p>단순 프롬프트 엔지니어링의 한계를 극복하고 “모른다면 모른다“는 지시문이 진정한 결정론적 효과를 발휘하도록 만들기 위해서는, 결국 모델의 파라미터 가중치 자체를 수학적으로 수정하는 파인 튜닝(Fine-tuning) 기법이 동반되어야 한다. 이 과정에서 최근 오라클 구축을 위해 가장 핵심적인 방법론으로 대두된 것이 바로 거절 인지 지시 조정(Refusal-Aware Instruction Tuning, 이하 R-Tuning)이다. 장(Zhang) 등이 제안한 이 연구 방법론(<em>R-tuning: Instructing large language models to say ‘I don’t know’</em>, 2024)은 LLM이 자신이 모르는 질문이나 낯선 코드 스니펫에 대해서도 억지로 대답을 지어내려는 성향의 근본 원인을 파고든다.</p>
<p>LLM이 지시문을 무시하고 환각을 생성하는 근본 원인은 훈련 데이터가 가진 태생적 편향성(Bias)에 있다. 대부분의 지시 조정(Instruction Tuning) 데이터셋은 사용자의 질문과 그에 대한 훌륭하고 완전한 정답의 쌍(Pair)으로만 구성되어 있다. 모델은 이 데이터로 튜닝을 거치면서, 모르는 정보라 할지라도 어떻게든 문장을 그럴듯하게 완성하여 사용자에게 응답을 제공해야 한다는 강력한 페널티/보상 체계 하에 놓인다. 즉, 모델은 아키텍처 설계상 “최대한 도움이 되는(Maximally helpful)” 방향으로 극단적으로 편향되며, 이는 필연적으로 스스로의 지식 경계에 대한 인지 능력을 무시하고 맹목적인 답변 생성을 우선시하는 한계로 이어진다.</p>
<p>R-Tuning은 이러한 아키텍처적 편향을 교정하기 위해 모델의 사전 학습된 매개변수 지식(Parametric Knowledge)과 지시 조정 데이터 간의 지식 격차(Knowledge Gap)를 정밀하게 측정한다. 모델이 내부적으로 이미 확고하게 알고 있는 지식(교차 엔트로피 손실이 낮게 측정되거나 특정 임계치 이하인 데이터)과 알지 못하는 지식을 수학적으로 분리한다. 그런 다음, 알지 못하는 질문 영역에 대해서는 강제적으로 “모릅니다(I don’t know)“라는 거절 라벨을 매핑하여 새로운 거절 인지 데이터셋을 구축한다. 이렇게 구축된 데이터셋으로 튜닝을 수행하면, 모델은 단순히 프롬프트의 표면적 지시를 따르는 것을 넘어, 자신의 가중치 내에 해당 지식이 충분히 활성화되지 않을 경우 선제적으로 기권하도록 근본적인 출력 분포가 재조정된다.</p>
<p>실험 연구 결과에 따르면, US-Tuning(Uncertainty-and-Sensitivity-Aware Tuning)이나 R-Tuning이 적용된 모델(예: Llama2-7B 기반 튜닝 모델)은 알 수 없는 질문을 처리하는 데 있어 기존 모델 대비 34.7%의 비약적인 성능 향상을 보였으며, 특정 벤치마크에서는 심지어 수천억 개의 파라미터를 가진 튜닝되지 않은 최상위 모델(GPT-4 등)을 능가하는 메타 스킬(Meta-skill)을 입증했다. R-Tuning은 정답과 오답을 가려내는 정렬 기준점인 AP(Average Precision) 점수를 크게 상향시켰으며, 학습되지 않은 도메인 외(Out-of-domain) 데이터에 대해서도 기권 능력이 훌륭하게 일반화되는 현상을 창출했다. 결론적으로 “모른다면 모른다고 답하라“는 프롬프트 지시문은 단독으로 쓰일 때보다 R-Tuning과 같이 지식 경계 인지 능력을 파라미터 수준에서 학습시키는 아키텍처 개선 방법론과 결합될 때, 비로소 소프트웨어 테스트 환경에서 일관되고 결정론적인 오라클 기능을 완벽하게 수행할 수 있다.</p>
<h2>6. 실무적 적용 전략 및 부작용 완화 기법: 오라클 구축의 완성</h2>
<p>소프트웨어 개발 실무 및 CI/CD 파이프라인에서 “모른다면 모른다고 답하라“는 지시문을 성공적으로 시스템에 정착시키기 위해서는, 앞서 언급한 과도한 기권(Over-abstention)과 프롬프트 우회(Jailbreaking), 그리고 포맷 파괴와 같은 부작용을 통제하는 정밀한 엔지니어링 전략이 복합적으로 요구된다. 단순하게 시스템 프롬프트에 지시문 한 줄을 추가하는 것만으로는 프로덕션 레벨의 복잡한 요구사항과 예외 상황을 만족시킬 수 없기 때문이다. 다음은 실무에서 오라클을 구축할 때 활용되는 심화 적용 전략이다.</p>
<p>첫째, 사고의 사슬(Chain-of-Thought, CoT)을 활용한 점진적 불확실성 평가이다. 모델에게 단번의 결정론적 정답을 요구하는 대신, 최종 결론에 도달하기 전 중간 추론 단계를 명시적으로 거치면서 자체적으로 지식의 충족 여부를 평가하도록 프롬프트를 설계한다. 예를 들어 “주어진 시스템 아키텍처 문서와 데이터베이스 스키마를 바탕으로 SQL 인젝션 취약점이 있는지 분석하라. 분석을 수행하기 전, 당신이 취약점을 명확하게 판별할 수 있는 충분한 정보와 소스 코드가 주어졌는지 단계별로 자체 평가(Self-evaluation)를 수행하라. 정보가 부족하다면 ’추가 컨텍스트 필요’라고 명시하라“고 지시하는 것이다. 과정을 쪼개어 스스로 반추하게 만드는 것은 단순 지시문보다 확신 편향을 극적으로 줄여준다. 한 걸음 더 나아가 재귀적 자기 개선(Recursive Self-Improvement) 프롬프팅을 통해 “방금 생성한 당신의 대답에서 근거가 부족한 약점 3가지를 스스로 비판하고 수정하라“고 지시함으로써 모호성을 자체 필터링할 수도 있다.</p>
<p>둘째, 수학적 처벌 메커니즘을 명시한 프롬프트 주입이다. 소프트웨어 엔지니어링 태스크에서 환각을 바탕으로 한 오판(False Positive)은 단순 기권(Abstention)보다 훨씬 큰 시스템적 복구 비용을 초래한다. <em>Humanity’s Last Exam</em> 등 최고 난이도의 최신 AI 평가 벤치마크에서는 오답 시 심각한 감점을 명시하는 프롬프트를 사용하여 모델의 무분별한 찍기(Guessing)를 억제한다. 예를 들어 “오답에는 <span class="math math-inline">t/(1-t)</span>점의 감점 페널티가 주어지고, ’모른다(I don’t know)’고 기권할 경우 0점이 부여되므로, <span class="math math-inline">t</span> 임계치(예: <span class="math math-inline">t=0.75</span>일 때 페널티 2점, <span class="math math-inline">t=0.9</span>일 때 페널티 9점) 이상의 확신이 있을 때만 대답하라“고 수학적인 리스크를 지시문에 직접 포함시킨다. 실무 소프트웨어 평가 프롬프트에서도 “잘못된 오라클 판별 결과를 출력할 경우 치명적인 시스템 실패로 간주된다. 확신도가 90% 미만이라면 차라리 기권하라“는 식의 페널티 조건을 명시함으로써, 내부의 확률적 계산 과정에서 기권의 가중치를 인위적으로 높여 보수적인 출력을 유도할 수 있다.</p>
<p>셋째, 복수 모델 교차 검증(Multi-LLM Collaboration and Critiquing)을 통한 기권 상태의 이중 확인이다. 단일 모델의 불확실성에 의존하는 대신, 메인 LLM이 판단을 내리거나 기권을 선언했을 때 두 번째 독립적인 평가용 LLM(LLM-as-a-Judge)을 투입하여 메인 모델의 판단이 합당했는지를 다시 한번 검증한다. 메인 LLM이 약간의 코드 난독화에 당황하여 과도하게 소극적으로 답변을 기권(False Negative)하는 현상을 평가용 LLM이 논리적으로 교정하거나, 반대로 메인 LLM이 확신에 차서 생성한 환각 코드를 평가용 모델이 논리적 모순과 보안 취약점을 들어 기권 상태로 강제 전환시킬 수 있다.</p>
<p>마지막으로, RAG(Retrieval-Augmented Generation) 시스템 내에서의 엄격한 포맷 제어이다. RAG는 LLM에게 외부 지식 소스를 강제로 제공하여 환각을 줄이는 훌륭한 방법이지만, RAG 자체가 불완전한 검색된 문서를 기반으로 억지로 답을 지어내게 만드는 부작용을 낳기도 한다. 따라서 RAG 파이프라인의 시스템 프롬프트에는 “당신의 내부 학습 지식을 절대 사용하지 마라. 검색된 청크(Chunk) 내에 답이 없으면 반드시 ’문서에 없음’으로 반환하라“는 지시문과 함께, 출력 형식을 JSON 스키마(Schema)로 강제화하여 응답의 정합성을 보장해야 한다. 이러한 템플릿 제어는 벡터 검색 엔진의 결과가 비어있거나 관련성이 떨어질 경우, LLM이 즉각적으로 침묵하거나 결정론적인 에러 상태를 반환하게 하여 데이터 추출 매커니즘의 소프트웨어적 무결성을 굳건히 지켜준다.</p>
<p>요약하자면, “모른다면 모른다고 답하라“는 프롬프트 지시문은 태생적으로 확률적이고 창의적인 언어 모델을 결정론적 소프트웨어 환경의 신뢰할 수 있는 부품(Component)으로 변환하는 가장 핵심적인 첫 단추이다. 그 자체만으로는 언어 모델의 본질적 불확실성을 완벽히 수학적으로 제거할 수 없고 정밀도와 재현율 간의 트레이드오프를 필연적으로 유발한다. 그러나 엄격한 열거형 포맷 강제, 모호성 탐지율과 같은 정량적 성과 지표(KPI)의 도입, R-Tuning을 통한 근본적 아키텍처 및 파라미터 가중치의 조정, 그리고 RAG 기반의 교차 검증 로직과 결합될 때 비로소 인간 품질 보증(QA) 엔지니어를 대체하거나 보조할 수 있는 완벽한 하이브리드 오라클 시스템을 구축할 수 있게 된다. 이는 AI 기반 소프트웨어 개발 패러다임에서, 단순히 코드를 빠르게 생성하는 편의성보다 생성된 코드와 결과물의 논리적 무결성을 최우선시하는 현대 소프트웨어 공학의 필수적인 설계 철학을 반영하는 핵심 결론이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>What If LLMs Could Simply Say “I Don’t Know”? | by Domenico Pontari | Medium, https://medium.com/@domenico.pontari/what-if-llms-could-simply-say-i-dont-know-f3504803ef20</li>
<li>Transforming hematological research documentation with large …, https://www.bloodresearch.or.kr/journal/view.html?pn=search&amp;uid=2709&amp;vmd=Full</li>
<li>Has My System Prompt Been Used? Large Language Model Prompt Membership Inference, https://arxiv.org/html/2502.09974v1</li>
<li>Transforming hematological research documentation with large language models: an approach to scientific writing and data analysis - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC11885755/</li>
<li>Large Language Models to Generate System-Level Test Programs Targeting Non-functional Properties - arXiv, https://arxiv.org/html/2403.10086v2</li>
<li>Transcript: Is Coinbase Really Writing Half Their Code With AI? - Syntax #944, https://syntax.fm/show/944/is-coinbase-really-writing-half-their-code-with-ai/transcript</li>
<li>Reducing Hallucinations and Trade-Offs in Responses in …, https://pmc.ncbi.nlm.nih.gov/articles/PMC12425422/</li>
<li>The $380 Million Prompt Engineering Lie: Why “Act Like an Expert” Doesn’t Boost Accuracy, https://pub.towardsai.net/the-380-million-prompt-engineering-lie-why-act-like-an-expert-doesnt-boost-accuracy-0af5eb79b4ff</li>
<li>How to reduce hallucinations in LLM? - (+ Key strategies) - Tredence, https://www.tredence.com/blog/halting-hallucinations-a-winning-methodology-to-reduce-errors-in-large-language-models</li>
<li>Classic ML to cope with Dumb LLM Judges - Doug Turnbull, https://softwaredoug.com/blog/2025/01/21/llm-judge-decision-tree</li>
<li>Uncovering the Reliability and Consistency of AI Language Models: A Systematic Study - UWSpace - University of Waterloo, https://uwspace.uwaterloo.ca/bitstreams/3e9ea499-e925-4fb1-b0a6-cc7129e92c18/download</li>
<li>LLMs outperform outsourced human coders on complex textual analysis - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC12623721/</li>
<li>Performance Comparison of Large Language Models for Efficient Literature Screening, https://www.mdpi.com/2673-7426/5/2/25</li>
<li>Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering | Transactions of the Association for Computational Linguistics | MIT Press, https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00667/121196/Evaluating-Correctness-and-Faithfulness-of</li>
<li>Characterizing LLM Abstention Behavior in Science QA with Context …, https://aclanthology.org/2024.findings-emnlp.197/</li>
<li>Characterizing LLM Abstention Behavior in Science QA with Context Perturbations - Lucy Lu Wang, https://llwang.net/assets/pdf/2024_wen_abstentionqa_emnlp.pdf</li>
<li>Testing AI/ML Systems: How to Live with Non-Determinism | by, https://medium.com/@andyakushenko/testing-ai-ml-systems-how-to-live-with-non-determinism-9861129f1c63</li>
<li>AI and Testing: Personal Marketability - Stories from a Software Tester, https://testerstories.com/2026/02/ai-and-testing-personal-marketability/</li>
<li>One of the worst nightmare : r/ProgrammerHumor - Reddit, https://www.reddit.com/r/ProgrammerHumor/comments/gj2ngq/one_of_the_worst_nightmare/</li>
<li>Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology - arXiv, https://arxiv.org/html/2502.17026v2</li>
<li>Uncertainty quantification in LLMs, https://bimsa.net/doc/notes/51259.pdf?id=0.9578430985566229</li>
<li>Uncertainty Quantification for Large Language Models (LLMs) - YouTube, https://www.youtube.com/watch?v=aYxIvemy68M</li>
<li>Quantifying LLMs Uncertainty with Confidence Scores | by Faustin Pulvéric | Capgemini Invent Lab | Medium, https://medium.com/capgemini-invent-lab/quantifying-llms-uncertainty-with-confidence-scores-6bb8a6712aa0</li>
<li>[2506.09038] AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions, https://arxiv.org/abs/2506.09038</li>
<li>Why AI Models Always Answer – Even When They Shouldn’t - seikouri, https://seikouri.com/why-ai-models-always-answer-even-when-they-shouldn-t</li>
<li>Do We Know What LLMs Don’t Know? A Study of Consistency in Knowledge Probing - arXiv, https://arxiv.org/html/2505.21701v1</li>
<li>Refusal-Aware Instruction Tuning - Emergent Mind, https://www.emergentmind.com/topics/refusal-aware-instruction-tuning-r-tuning</li>
<li>R-Tuning Large Language Models to Confront Hallucination | by Ahrane Mahaganapathy, https://ahranemahaganapathy.medium.com/r-tuning-large-language-models-to-confront-hallucination-1d063c24ce06</li>
<li>A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models, https://arxiv.org/html/2401.01313v1</li>
<li>R-Tuning: Instructing Large Language Models to Say ‘I Don’t Know’ - ACL Anthology, https://aclanthology.org/2024.naacl-long.394.pdf</li>
<li>R-Tuning: Instructing Large Language Models to Say ‘I Don’t Know’ - arXiv, https://arxiv.org/html/2311.09677v2</li>
<li>The “I Don’t Know” Problem: Can LLMs Now Admit Uncertainty?, https://collabskus.neocities.org/kimi-k2-on-uncertainty</li>
<li>Heng Ji - ACL Anthology, https://aclanthology.org/people/heng-ji/</li>
<li>Large Language Models Do NOT Really Know What They Don’t Know - arXiv, https://arxiv.org/html/2510.09033v1</li>
<li>Instruction Tuning-Based Refusal - Emergent Mind, https://www.emergentmind.com/topics/instruction-tuning-based-refusal</li>
<li>prompt engineering | MyCarta, https://mycartablog.com/tag/prompt-engineering/</li>
<li>Know Your Limits: A Survey of Abstention in Large Language Models - MIT Press, https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00754/131566/Know-Your-Limits-A-Survey-of-Abstention-in-Large</li>
<li>LLM Hallucinations Explained. LLMs like the GPT family, Claude… | by Nirdiamant | Medium, https://medium.com/@nirdiamant21/llm-hallucinations-explained-8c76cdd82532</li>
<li>The Complete Prompt Engineering Guide for 2025: Mastering Cutting-Edge Techniques, https://aloaguilar20.medium.com/the-complete-prompt-engineering-guide-for-2025-mastering-cutting-edge-techniques-dfe0591b1d31</li>
<li>Advanced Prompt Engineering Techniques for 2025: Beyond Basic Instructions - Reddit, https://www.reddit.com/r/PromptEngineering/comments/1k7jrt7/advanced_prompt_engineering_techniques_for_2025/</li>
<li>Why Language Models Hallucinate - OpenAI, https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf</li>
<li>Why Language Models Hallucinate - arXiv, https://arxiv.org/html/2509.04664v1</li>
<li>When “Better” Prompts Hurt: Evaluation-Driven Iteration for LLM Applications A Framework with Reproducible Local Experiments - arXiv, https://arxiv.org/html/2601.22025v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>