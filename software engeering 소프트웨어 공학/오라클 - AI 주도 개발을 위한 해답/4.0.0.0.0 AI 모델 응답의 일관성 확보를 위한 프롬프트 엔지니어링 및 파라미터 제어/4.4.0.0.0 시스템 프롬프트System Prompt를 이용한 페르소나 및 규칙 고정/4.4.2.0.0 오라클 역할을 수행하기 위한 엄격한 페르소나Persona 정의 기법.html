<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:4.4.2 오라클 역할을 수행하기 위한 엄격한 페르소나(Persona) 정의 기법</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>4.4.2 오라클 역할을 수행하기 위한 엄격한 페르소나(Persona) 정의 기법</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 4. AI 모델 응답의 일관성 확보를 위한 프롬프트 엔지니어링 및 파라미터 제어</a> / <a href="index.html">4.4 시스템 프롬프트(System Prompt)를 이용한 페르소나 및 규칙 고정</a> / <span>4.4.2 오라클 역할을 수행하기 위한 엄격한 페르소나(Persona) 정의 기법</span></nav>
                </div>
            </header>
            <article>
                <h1>4.4.2 오라클 역할을 수행하기 위한 엄격한 페르소나(Persona) 정의 기법</h1>
<p>대규모 언어 모델(LLM)을 소프트웨어 테스트 및 품질 보증(QA) 파이프라인에서 ’오라클(Oracle)’로 활용하려는 시도는 소프트웨어 공학의 새로운 패러다임을 열고 있다. 소프트웨어 테스트에서 오라클이란 주어진 입력에 대해 시스템이 산출한 결과가 참인지 거짓인지, 혹은 기대한 동작을 정확히 수행했는지를 판별하는 결정론적(Deterministic) 정답지 제공 메커니즘을 의미한다. 기존의 하드코딩된 단위 테스트(Unit Test)나 정적 분석 도구는 명확한 규칙 기반 환경에서는 강력하게 작동하지만, 자연어 요구사항, 비정형 데이터 처리, 혹은 복잡한 비즈니스 로직과 같이 규칙을 사전에 완벽히 정의하기 어려운 영역에서는 그 한계가 명확하다. 이러한 공백을 메우기 위해 고도의 추론 능력을 갖춘 LLM이 평가자(LLM-as-a-judge)로 투입되고 있다.</p>
<p>그러나 상용 LLM은 본질적으로 확률론적(Probabilistic) 모델이다. 주어진 컨텍스트 내에서 다음 토큰(Token)의 통계적 출현 확률을 극대화하는 방향으로 텍스트를 생성하도록 최적화되어 있다. 이로 인해 상용 AI 시스템은 기본적으로 “당신은 도움이 되는 어시스턴트입니다(You are a helpful assistant)“와 같은 범용적이고 친절한 페르소나를 시스템 프롬프트로 채택한다. 소프트웨어 검증 영역에서 이러한 범용 페르소나는 치명적인 결함을 유발한다. 사용자의 잘못된 코드나 논리에 타협하거나, 확률적으로 그럴듯한(Plausible) 오류(Hallucination)를 정답으로 둔갑시키며, 입력되는 텍스트의 어조에 따라 평가 기준을 자의적으로 변경하는 경향이 발생하기 때문이다.</p>
<p>따라서 오라클로서의 LLM은 인간과 교감하는 대화형 에이전트가 되어서는 안 된다. 사전에 정의된 규칙, 도메인 지식, 그리고 제공된 컨텍스트만을 바탕으로 냉혹하고 일관되게 참과 거짓을 판별하는 ’엄격한 평가자(Strict Evaluator)’로 기능해야 한다. 본 절에서는 LLM이 오라클 역할을 완벽히 수행할 수 있도록 시스템 프롬프트 상에서 엄격한 페르소나를 정의하는 기술적 기법, 신경망 내부의 수학적 메커니즘, 그리고 다중 에이전트 앙상블 및 적대적 평가 전략에 대해 심도 있게 다룬다.</p>
<h2>1.  엄격한 페르소나의 신경망적 작동 원리: 전문가 혼합(MoE)과 역할 벡터(Role Vector)</h2>
<p>프롬프트 엔지니어링을 통해 LLM에 특정 페르소나를 부여하는 행위는 단순히 모델의 출력 텍스트 스타일을 피상적으로 바꾸는 조작이 아니다. 최근의 연구 동향은 페르소나 지시문이 LLM 내부의 ’계획 회로(Planning Circuits)’를 활성화하고, 모델의 거대한 매개변수 공간 내에서 특정 작업에 최적화된 하위 네트워크를 동적으로 라우팅(Routing)하는 하드웨어적, 구조적 개입임을 시사한다.</p>
<p>고도화된 LLM은 방대한 코퍼스를 훈련하는 과정에서 다양한 도메인의 지식과 추론 패턴을 전문가 혼합(Mixture of Experts, MoE)과 유사한 형태로 내재화한다. MoE 아키텍처에서는 모든 뉴런이 매번 활성화되는 것이 아니라, 입력 토큰의 성격에 따라 가장 적합한 ‘전문가’ 네트워크로 연산이 라우팅된다. 이때 “엄격한 소프트웨어 품질 검증 엔지니어로 행동하라“는 지시어는 모델 내부에서 해당 도메인과 직접적으로 연관된 ’역할 벡터(Role Vector)’를 촉발시킨다.</p>
<p>이 역할 벡터가 활성화되면 수학적 추론, 코드 분석, 논리적 제약 조건 검사, 구조적 데이터 파싱과 관련된 뉴런 군집의 가중치가 높아진다. 반대로 일상 대화, 창의적 글쓰기, 감정적 공감, 비유적 표현과 관련된 뉴런의 활성화 확률은 극도로 억제된다. 이는 곧 오라클이 가져야 할 차가운 논리적 판단력을 신경망 수준에서 강제하는 행위이다.</p>
<p><img src="./4.4.2.0.0%20%EC%98%A4%EB%9D%BC%ED%81%B4%20%EC%97%AD%ED%95%A0%EC%9D%84%20%EC%88%98%ED%96%89%ED%95%98%EA%B8%B0%20%EC%9C%84%ED%95%9C%20%EC%97%84%EA%B2%A9%ED%95%9C%20%ED%8E%98%EB%A5%B4%EC%86%8C%EB%82%98Persona%20%EC%A0%95%EC%9D%98%20%EA%B8%B0%EB%B2%95.assets/image-20260225233456939.jpg" alt="image-20260225233456939" /></p>
<p>수학적으로, 컨텍스트 <span class="math math-inline">c</span>와 페르소나 제약 <span class="math math-inline">p</span>가 주어졌을 때 <span class="math math-inline">t</span>번째 토큰 <span class="math math-inline">w_t</span>의 생성 확률은 다음과 같이 조건부 확률 분포로 정의된다.<br />
<span class="math math-display">
P(w_t \vert w_{&lt;t}, c, p)
</span><br />
여기서 페르소나 <span class="math math-inline">p</span>가 ’엄격성(Strictness)’과 ’객관성(Objectivity)’을 강제하는 어휘적 제약(예: “반드시 JSON 형식으로만 답하라”, “제공된 문서 외의 외부 지식을 추론에 포함하지 마라”, “감정적 동의를 표출하지 마라”)을 명시적으로 포함할 경우, 토큰 분포의 엔트로피(Entropy)는 급격하게 곤두박질친다. 모델이 생성할 수 있는 다음 단어의 경우의 수가 극도로 좁아지며 확정적이고 결정론적인 방향으로 수렴하게 되는 것이다.</p>
<p>하지만 연구 결과에 따르면 이러한 단순한 페르소나 선언의 효과는 복잡한 추론 태스크 전체 분산의 약 10% 미만을 설명하는 데 그칠 수 있다. “엄격하게 행동하라“는 지시어 하나만으로는 때때로 모델을 지나치게 경직되게 만들어 오히려 문맥 파악 능력을 저하시키거나, 페르소나 유지에 연산 자원을 소모하여 정작 중요한 코드 결함을 놓치는 부작용을 초래하기도 한다. 따라서 엄격한 오라클을 설계하기 위해서는 단순한 직업군이나 성격을 묘사하는 선언적 문장을 넘어, 런타임에 동적으로 최적화되고 구조화된 프롬프트 엔지니어링 메커니즘이 필수적으로 요구된다.</p>
<h2>2.  조대 정렬(Coarsely Aligned) 페르소나의 역설과 인스턴스 정렬(Instance-Aligned) 전략</h2>
<p>소프트웨어 개발 파이프라인 전반을 관장하는 오라클을 구축할 때 가장 흔히 저지르는 아키텍처적 실수는 데이터셋이나 도메인 전체에 대해 단일한 형태의 ‘조대하게 정렬된(Coarsely Aligned)’ 페르소나를 강제하는 것이다. 예를 들어, 모든 코드 베이스를 검증하는 단일 AI 에이전트에게 “당신은 구글 출신의 20년 차 시니어 소프트웨어 엔지니어입니다“라는 고정된 페르소나를 부여하는 방식이 이에 해당한다.</p>
<h3>2.1  범용적 페르소나가 객관적 성능을 저하시키는 원리</h3>
<p>실증적 연구에 따르면, 데이터셋 전체에 포괄적으로 적용된 조대한 페르소나는 제로샷(Zero-shot) 형태의 중립적(Neutral) 프롬프트보다도 추론 성능을 저하시키는 원인이 될 수 있다. 2,410개의 객관적 검증 태스크와 4종의 주요 오픈소스 LLM을 대상으로 162가지의 다양한 페르소나를 주입하여 진행한 대규모 평가(Systematic Evaluation) 연구는 매우 직관적인 결론을 도출했다. 객관적이고 사실적인 추론을 요구하는 태스크에서 페르소나를 추가하는 행위가 베이스라인(페르소나 없음)에 비해 성능을 향상시키지 못하며, 오히려 일정한 역효과를 초래한다는 것이다.</p>
<p>AQuA(수학 및 논리 추론) 데이터셋을 이용한 실험이 이를 명확히 방증한다. 이 실험에서 LLM에 페르소나를 적용했을 때 기존에 페르소나가 없을 때 틀렸던 문제 중 15.75%를 정답으로 맞추는 긍정적인 효과가 나타났다. 그러나 치명적이게도, 기존에 페르소나 없이 정확하게 맞췄던 문제의 13.78%가 페르소나 적용 후 오답으로 변질되는 현상이 관찰되었다.</p>
<p>예를 들어, 물리적 제약이 포함된 복잡한 시스템 검증 문제에서 모델에게 “당신은 수학 교사입니다“라는 조대한 페르소나를 부여했다고 가정해보자. 이 페르소나는 모델의 역할 벡터를 순수 수학적 계산과 공식 유도에만 편향되도록 라우팅한다. 그 결과, 모델은 현실 세계의 물리적 한계나 메모리 할당의 물리적 제약 등 “물리학 엔지니어” 페르소나라면 쉽게 짚어낼 수 있는 컨텍스트적 제약 조건을 맹목적으로 무시하게 된다. 이는 소프트웨어 검증에서 치명적인 결함으로 이어진다. 프론트엔드 UI/UX 코드를 검증해야 하는 상황에서 백엔드 데이터베이스 최적화에 편향된 시니어 엔지니어 페르소나를 강제하면, UI의 반응성이나 접근성 결함을 놓치고 불필요한 쿼리 최적화 지적에만 몰두하게 되는 것과 같은 이치이다.</p>
<h3>2.2  인스턴스 정렬 페르소나(Instance-Aligned Persona)의 구현</h3>
<p>이러한 단일 페르소나의 한계를 극복하고 엄격한 오라클을 구축하기 위해서는 개별 입력 인스턴스(Query)의 특성에 맞춰 동적으로 최적의 페르소나를 추론하고 할당하는 <strong>인스턴스 정렬 페르소나(Instance-Aligned Persona)</strong> 기법을 도입해야 한다.</p>
<p>인스턴스 정렬 오라클 파이프라인은 다음과 같은 다단계 아키텍처를 통해 구현된다.</p>
<ol>
<li><strong>역할 식별자 에이전트(Role Identifier Agent)의 전처리</strong>: 메인 오라클이 검증을 시작하기 전에, 경량화된 판별 전용 LLM을 활용하여 현재 분석해야 할 소프트웨어 모듈, 테스트 케이스, 혹은 자연어 요구사항의 성격을 선제적으로 분석한다.</li>
<li><strong>동적 페르소나 주입(Dynamic Persona Injection)</strong>: 판별된 도메인 특성에 맞추어 시스템 프롬프트를 실시간으로 변형한다. 예를 들어, 타겟 코드가 SQL 복잡도 검증이라면 “대규모 트랜잭션을 다루는 현업 10년 차 데이터베이스 관리자(DBA)” 페르소나를 주입하고 , 암호화 모듈을 검증할 때는 “보안 취약점 분석가 및 응용 암호학자” 페르소나를 할당한다.</li>
<li><strong>스타일 안내서(Style Guide)의 최소화</strong>: 구체적인 직업적, 심리사회적(Psychosocial) 전문성을 부여하되 불필요한 문체나 작문 스타일에 대한 지시는 배제한다. 연구에 따르면, “학술적으로 딱딱하게 말하라“와 같은 과도한 문체 제약은 모델이 인구통계학적 특성이나 도메인 전문성에 정렬되는 것을 방해하며 텍스트 생성의 다양성 상한선을 제한하여 오히려 검증의 날카로움을 무디게 만든다.</li>
</ol>
<p><strong>페르소나 정렬 방식에 따른 오라클 추론 성능 비교</strong></p>
<p><img src="./4.4.2.0.0%20%EC%98%A4%EB%9D%BC%ED%81%B4%20%EC%97%AD%ED%95%A0%EC%9D%84%20%EC%88%98%ED%96%89%ED%95%98%EA%B8%B0%20%EC%9C%84%ED%95%9C%20%EC%97%84%EA%B2%A9%ED%95%9C%20%ED%8E%98%EB%A5%B4%EC%86%8C%EB%82%98Persona%20%EC%A0%95%EC%9D%98%20%EA%B8%B0%EB%B2%95.assets/image-20260225233650474.jpg" alt="image-20260225233650474" /></p>
<p><em>AQuA 데이터셋 추론 실험 결과, 데이터셋 전체에 단일 페르소나를 적용하는 조대 정렬(Coarsely Aligned)은 기존 정답을  오답으로 바꾸는 부작용(약 13.78%)을 동반했다. 반면 쿼리마다 최적의 역할을 부여하는 인스턴스  정렬(Instance-Aligned)은 정답률을 유의미하게 향상시켰다.</em></p>
<p>위의 비교에서 보듯, 인스턴스 정렬 전략은 정답률을 끌어올릴 뿐만 아니라 기존에 맞추던 문제를 오답으로 추락시키는 회귀(Regression) 현상을 획기적으로 억제한다. 즉, 최적의 역할 벡터를 적재적소에 투입하는 것이 엄격하고 무결한 오라클을 구축하는 첫 번째 관문이다.</p>
<h2>3.  평가 루브릭(Rubric) 기반의 ‘엄격한 평가자’ 프롬프트 설계 구조</h2>
<p>도메인에 맞는 페르소나를 동적으로 할당했다면, 다음 단계는 해당 페르소나가 모호성을 철저히 배제하고 인간 전문가의 판단을 기계적으로 모사하도록 프롬프트의 내부 구조를 설계하는 것이다. LLM-as-a-Judge 패러다임에서 오라클 페르소나는 단순한 직업 명시를 넘어, 수학적이고 구조적인 제약 조건을 포함한 루브릭(Rubric)으로 무장해야 한다.</p>
<h3>3.1  주관성 배제와 저해상도(Low-precision) 채점망 강제</h3>
<p>소프트웨어 검증에서 인간 평가자조차도 “이 코드의 유지보수성을 1에서 100 사이로 평가하라“는 지시를 받으면 시간, 피로도, 개인의 기준에 따라 매번 다른 점수를 부여한다. 하물며 본질적으로 다음 토큰을 확률적으로 예측하는 LLM에게 이러한 고해상도(High-precision) 스케일의 점수를 요구하는 것은 오라클의 생명인 결정론적 속성을 심각하게 파괴하는 행위이다.</p>
<p>엄격한 페르소나는 확률적 요동을 최소화하기 위해 반드시 이진 평가(예: “보안 취약점 존재함” vs “취약점 없음”, “테스트 Pass” vs “테스트 Fail”) 또는 3~5단계의 극히 제한된 저해상도 스케일(예: “관련 있음”, “부분적 관련 있음”, “관련 없음”)로 출력을 제한받아야 한다. 스케일이 작아질수록 모델이 특정 클래스에 대해 가지는 로짓(Logit) 값의 군집화가 뚜렷해지며, 이는 온도(Temperature) 파라미터를 0으로 설정했을 때 일관된 응답을 도출할 확률을 100%에 가깝게 수렴시킨다. 프롬프트에는 각 점수 계층(Tier)이 의미하는 바를 예외 사례를 포함하여 명확하고 구체적인 루브릭으로 서술해야 한다. 또한 입력된 정보가 부족하여 판단이 불가한 엣지 케이스를 강제로 분류(Forced Choice)하다가 발생하는 환각을 방지하기 위해 반드시 “판단 불가(Unknown)” 옵션을 명시적으로 제공해야 한다.</p>
<h3>3.2  평가 기준의 분해(Splitting Criteria)를 통한 단일 초점 유지</h3>
<p>정확성(Accuracy), 완전성(Completeness), 가독성(Readability) 등 소프트웨어의 여러 품질 지표를 동시에 평가하도록 단일 오라클 프롬프트 내에 지시하면, LLM 내부의 주의력(Attention) 메커니즘이 분산되어 복잡한 추론 과정에서 환각이 발생할 확률이 급증한다.</p>
<p>엄격한 페르소나는 한 번에 오직 하나의 품질 기준(Aspect)만을 집요하게 평가하도록 좁고 날카롭게 설계되어야 한다. 다각적인 검증이 필요하다면, 기준마다 별도의 독립된 평가자 페르소나(예: ‘정확도 판별 오라클’, ‘보안 판별 오라클’)를 각각 인스턴스화하여 병렬로 구동한 뒤, 그 결과를 외부의 결정론적 로직(예: 하나라도 Fail이 발생하면 전체 파이프라인 정지)으로 취합하는 마이크로서비스 형태의 오라클 아키텍처를 채택해야 한다.</p>
<h3>3.3  비판적 논증(Critique) 및 증거 기반 정당화(Evidence-Based Justification)의 강제</h3>
<p>오라클에게 단순히 Pass/Fail이라는 결론 점수만을 출력하도록 강제하면, LLM은 심층적인 사고 과정을 생략한 채 표면적인 키워드 매칭에 의존하게 된다. 진정한 오라클로 거듭나기 위해서는 점수를 부여하기 전에 반드시 코드나 응답에 대한 ’비판(Critique)’과 ‘정당화(Justification)’ 과정을 거치도록 페르소나 프롬프트 내에 논리적 사슬을 강제해야 한다.</p>
<p>단순히 “이 코드는 비효율적이다” 혹은 “이 답변은 틀렸다“와 같은 약한 수준의 비판은 철저히 금지되어야 한다. 대신 시스템 프롬프트는 “해당 코드는 O(N^2)의 시간 복잡도를 가지므로 대규모 트랜잭션 요구사항을 위반한다“거나 “답변은 A API가 B 라이브러리와 호환된다고 명시했으나, 공식 레퍼런스 문서 5.2절에 따르면 이는 사실이 아님“과 같이 구체적 출처와 수치적 논리에 기반한 비판을 작성하도록 지시해야 한다. 이러한 정당화 요구는 프롬프트 내에서 일종의 사고의 사슬(Chain-of-Thought, CoT) 역할을 수행하며, 최종 점수가 도출되는 과정의 수학적 투명성과 결정론적 일관성을 비약적으로 높여준다.</p>
<h3>3.4  문체 불변성(Style-Invariant) 평가 원칙의 선언</h3>
<p>LLM은 훈련 과정에서 습득한 데이터의 분포적 편향으로 인해, 장황하고 정중하며 유려한 문체를 가진 오답(거짓 정보)에 높은 점수를 주고, 투박하고 직설적이며 문법이 일부 어색하지만 기술적으로는 완벽히 정확한 정답에 낮은 점수를 주는 이른바 ’상냥함 편향(Sycophancy)’을 심각하게 내포하고 있다.</p>
<p>따라서 오라클 역할을 수행하는 페르소나의 프롬프트 내에는 **“응답의 어조, 형식, 공손함, 길이, 문법적 유려함 등 모든 문체적(Stylistic) 요소는 철저히 배제하고, 오직 기술적 사실관계와 논리적 정합성(Content)만을 기준으로 철저히 평가하라”**는 ‘문체 불변성(Style-Invariant)’ 원칙을 강력하게 명시해야 한다. 나아가 퓨샷(Few-Shot) 예제를 통해 어조가 매우 무례하더라도 팩트가 맞으면 만점을 주고, 매우 정중하더라도 로직이 틀리면 0점을 주는 극단적인 사례를 주입하여 편향성을 교정해야 한다.</p>
<h3>3.5  고정밀 판별관(Adjudicator) 페르소나의 시스템 프롬프트 구조 설계</h3>
<p>실제 고위험(High-stakes) 도메인에서 활용되는 평가용 시스템 프롬프트의 뼈대는 일반적인 대화형 프롬프트와 완전히 다른 구조를 가진다. 다음 표는 폴란드 국가 항소 위원회(Polish National Board of Appeal) 자격 시험을 자동으로 채점하기 위해 구축된 GPT-4o 기반 오라클의 프롬프트 구조를 소프트웨어 검증 목적에 맞게 재해석한 것이다.</p>
<table><thead><tr><th><strong>오라클 프롬프트 구성 요소</strong></th><th><strong>세부 작성 지침 및 프롬프트 예시</strong></th><th><strong>적용 목적 및 기대 효과</strong></th></tr></thead><tbody>
<tr><td><strong>강력한 역할 선언 (Persona Definition)</strong></td><td>“당신은 15년 차 선임 판별관(Senior Adjudicator)이자 무결성을 추구하는 정밀 소프트웨어 검증 전문가이다.”</td><td>모델의 내부 토큰 분포를 높은 전문성과 논리적 엄밀성을 띄는 특정 하위 공간(Subspace)으로 철저히 격리시킨다.</td></tr>
<tr><td><strong>명시적 채점 기준표 (Explicit Criterion Block)</strong></td><td>평가할 3~5개의 기준을 나열하고, 각 기준별 배점 한도(Maximum points)와 명확한 차감(Penalty) 사유를 수치화하여 명시한다.</td><td>평가 기준의 자의적 해석 및 확장 추론을 방지하고 일관된 정량적 출력을 강제한다.</td></tr>
<tr><td><strong>출력 구조 강제 (Formatting Constraints)</strong></td><td>“어떠한 설명이나 서론, 결론 형태의 대화형 텍스트도 출력하지 마라. 반드시 사전 정의된 JSON 스키마에만 맞춰 결과를 출력하라.”</td><td>오라클의 출력을 파싱하는 후처리(Post-processing) 시스템의 오작동을 방지하여 자동화 파이프라인의 안전성을 확보한다.</td></tr>
<tr><td><strong>경계 제약 (Constraint Boundaries)</strong></td><td>“제공된 컨텍스트(소스 코드 및 명세서) 외의 외부 지식이나 가정을 절대 개입시키지 마라. 정보가 누락되어 모호한 경우 추측하지 말고 즉시 오류(Fail)로 판정하라.”</td><td>LLM 내부의 사전 학습된 가중치가 평가에 개입하여 발생하는 외부 데이터 기반 환각(Hallucination)을 원천 차단한다.</td></tr>
</tbody></table>
<p>이러한 지침을 철저히 준수하여 <span class="math math-inline">temperature \approx 0</span> 설정과 함께 모델을 구동하면, LLM은 자의식을 가진 대화형 에이전트가 아니라 입출력 파이프라인 내에서 철저히 예측 가능성을 통제받는 견고한 소프트웨어 컴포넌트로 기능하게 된다.</p>
<h2>4.  멀티 에이전트(Multi-Agent) 프레임워크 기반의 페르소나 앙상블: Nexus 아키텍처</h2>
<p>아무리 단일 시스템 프롬프트를 정교하게 깎아내고 루브릭을 고도화하더라도, 단 하나의 페르소나가 복잡한 소프트웨어의 모든 결함과 엣지 케이스(Edge Case)를 완벽하게 포착하는 데에는 구조적인 한계가 존재한다. 모델이 단일한 사고의 틀에 갇혀 자신의 오류를 발견하지 못하는 이른바 ‘인식론적 반향실(Epistemological Echo Chamber)’ 현상이 발생하기 때문이다. 특정 코딩 스타일에 편향된 페르소나는 보안 취약점을 완벽하게 짚어내더라도 메모리 누수 문제는 무시할 확률이 높다.</p>
<p>이러한 단일 모델의 맹점을 극복하고 결정론적 검증의 무결성을 극한으로 끌어올리기 위해, 최근 소프트웨어 공학계에서는 다수의 상호 직교하는(Orthogonal) 철학을 지닌 전문가 페르소나들을 앙상블(Ensemble)하는 멀티 에이전트 프레임워크가 적극적으로 도입되고 있다.</p>
<p>대표적인 연구 사례인 <strong>Nexus 프레임워크</strong>는 무결한 테스트 오라클을 합성해 내기 위해 다음과 같은 4가지의 엄격하고 완전히 독립적인 전문가 페르소나로 구성된 심의 패널(Deliberation Panel)을 운영한다.</p>
<table><thead><tr><th><strong>전문가 페르소나 (Nexus Panel)</strong></th><th><strong>부여된 핵심 철학 및 역할 벡터</strong></th><th><strong>검증 초점 (Focus Area)</strong></th></tr></thead><tbody>
<tr><td><strong>명세서 전문가 (Specification Expert)</strong></td><td>“당신은 기계적인 규정 준수 검사관이다.”</td><td>요구사항 문서, API 명세, Docstrings 등 주어진 자연어 설명에 대한 <strong>문자 그대로의 준수(Literal Compliance)</strong> 여부에만 집착한다. 명세서와 한 치의 오차라도 발생하면 무조건 기각한다.</td></tr>
<tr><td><strong>엣지 케이스 스페셜리스트 (Edge Case Specialist)</strong></td><td>“당신은 시스템을 파괴하려는 악의적 공격자이다.”</td><td>시스템의 **적대적 강건성(Adversarial Robustness)**을 시험한다. 경계 조건(Boundary conditions), 예상치 못한 Null 입력, 오버플로우 등 극단적이고 변태적인 입력 시나리오에 시스템이 견디는지를 검증한다.</td></tr>
<tr><td><strong>기능 검증자 (Functional Validator)</strong></td><td>“당신은 거시적인 아키텍처 설계자이다.”</td><td>코드의 **고수준 의미론적 속성(High-level semantic properties)**에 집중한다. 자잘한 예외 처리보다는 함수가 의도한 거시적인 비즈니스 로직과 최종 산출물을 성공적으로 생성해 내는지를 평가한다.</td></tr>
<tr><td><strong>알고리즘 분석가 (Algorithmic Analyst)</strong></td><td>“당신은 로우 레벨 최적화 엔지니어이다.”</td><td>구현 수준의 논리, 루프(Loop) 불변성, 조건 분기, 자료구조의 선택 등 내부적인 **알고리즘 추론(Implementation-level logic)**이 올바르게 설계되었는지 단계별 수학적 증명을 통해 검증한다.</td></tr>
</tbody></table>
<p><img src="./4.4.2.0.0%20%EC%98%A4%EB%9D%BC%ED%81%B4%20%EC%97%AD%ED%95%A0%EC%9D%84%20%EC%88%98%ED%96%89%ED%95%98%EA%B8%B0%20%EC%9C%84%ED%95%9C%20%EC%97%84%EA%B2%A9%ED%95%9C%20%ED%8E%98%EB%A5%B4%EC%86%8C%EB%82%98Persona%20%EC%A0%95%EC%9D%98%20%EA%B8%B0%EB%B2%95.assets/image-20260225233720816.jpg" alt="image-20260225233720816" /></p>
<p>이들 4개의 패널은 초기에 주어진 코드나 자연어 요구사항을 각자의 엄격한 시선에서 심의(Deliberation)하고, 다른 에이전트가 생성한 평가 결과를 상호 비판(Critique)한다. 엣지 케이스 전문가가 취약점을 제기하면, 알고리즘 분석가가 해당 취약점이 실제 코드 로직상 도달 가능한(Reachable) 경로인지를 수학적으로 검증하는 식이다.</p>
<p>이러한 상호 교차 검증 및 자체 개선(Self-refinement) 과정을 거치면서 도출되는 오라클은, 단순히 텍스트 토큰의 확률 분포를 단일 방향으로 깎아내는 것이 아니라, 여러 개의 고차원 공간에서 생성된 결정 경계(Decision Boundaries)들의 정밀한 교집합을 구하는 수학적 정합성 향상 과정과 동일한 효과를 낳는다. 소프트웨어 CI/CD 파이프라인에서 오작동 시 막대한 손실이 발생하는 금융 트랜잭션 등 무결성이 절대적으로 요구되는 핵심 비즈니스 로직을 검증할 때는 이러한 다중 페르소나 앙상블 오라클 설계가 선택이 아닌 필수이다.</p>
<h3>4.1  적대적 커리큘럼 기반의 동적 페르소나 생성: LLM-TOC 프레임워크</h3>
<p>멀티 에이전트 접근법은 고정된 4가지 페르소나를 넘어, 끊임없이 진화하는 동적인 취약점을 발굴하는 데 응용될 수 있다. 강화학습에서의 적대적 커리큘럼 생성을 다루는 LLM-TOC(Theory-of-Mind Adversarial Curriculum) 프레임워크의 개념을 차용하면, 소프트웨어 오라클 검증 시스템 전체를 이중 최적화(Bi-level optimization) 문제인 ’슈타켈베르그 게임(Stackelberg Game)’으로 수학적 공식화가 가능하다.</p>
<p>이 구조에서 오라클 시스템 내부에는 테스트 대상 소프트웨어(Follower)의 약점을 집요하게 파고드는 무한한 ’적대적 페르소나’를 생성하는 리더(Leader) 에이전트가 존재한다. 리더 에이전트는 이전 검증 라운드에서 소프트웨어가 붕괴했던 논리적 결함 데이터를 입력받아, 기존 벤치마크나 라이브러리에는 존재하지 않았던 전혀 새롭고 복잡한 형태의 ’맞춤형 엣지 케이스 생성용 페르소나’를 동적으로 합성해 낸다.</p>
<p>생성된 적대적 텍스트, 비정상적 API 호출 순서, 혹은 극단적인 데이터 구조는 방대하고 비연속적인 의미론적 전략 공간(Semantic strategy space, <span class="math math-inline">D_{sem}</span>) 내에서 기존의 휴리스틱 테스트로는 불가능했던 영역을 탐색한다. 이는 오라클이 코드의 추상적인 논리 공간에서 의미론적 경사 상승(Semantic gradient ascent)을 수행하는 것과 같은 획기적인 효과를 내며, 결과적으로 오라클 시스템 전체의 결정론적 검증 한계를 영구적이고 지속적으로 확장시키는 원동력이 된다.</p>
<h2>5.  오라클 페르소나의 확률적 제약(Probabilistic Constraints) 처리 능력</h2>
<p>오라클 역할을 수행하는 엄격한 페르소나가 반드시 100% 결정론적인 코드만 다루는 것은 아니다. 현대 소프트웨어, 특히 기계학습 모델을 배포하는 시스템, 금융 리스크 평가 모듈, 또는 재생 에너지 전력망 관리 소프트웨어는 본질적으로 시스템 내부에 불확실성(Uncertainty)을 내포하는 확률적 최적화(Stochastic Optimization, SO) 모델을 다룬다.</p>
<p>이러한 소프트웨어를 검증하는 오라클 페르소나는 역설적으로 “결정론적 논리“를 바탕으로 “확률적 제약 조건(Probabilistic constraints)“이 올바르게 구현되었는지를 판별할 수 있어야 한다. 예를 들어, 전력망 관리 코드에서 “풍력 발전 부하 분산 제약은 모든 노드에서 95% 이상의 확률로 동시에 만족되어야 한다(Joint chance-constrained)“는 자연어 명세가 주어졌을 때, 범용 페르소나는 이 확률 변수의 수학적 종속성을 파악하지 못하고 단순한 선형 제약으로 코드를 평가하려다 환각을 일으킨다.</p>
<p>따라서, 확률적 시스템을 다루는 오라클의 프롬프트에는 불확실성을 수학적으로 다루기 위한 특별한 페르소나 튜닝이 요구된다.</p>
<ul>
<li><strong>통계학 및 운용 과학(Operations Research) 지식 주입</strong>: 페르소나 프롬프트에 확률 변수, 기대값 추정, 리스크 허용 임계치(Threshold) 등의 개념을 수학적으로 추적(Trace)하라는 지시를 포함해야 한다.</li>
<li><strong>확률적 제약의 명시적 분해 유도</strong>: 자연어 요구사항에 포함된 확률 분포를 인식하고, 이를 기반으로 코드가 예외 처리를 올바르게 수행하는지 단계별 확률을 유지하며 추론(Chain-of-Thought)하도록 강제한다.</li>
</ul>
<p>실증 연구에 따르면, 이러한 구조화된 수학적 페르소나 프롬프트는 단순 프롬프트 대비 목표 매칭률과 예외 조항 식별 능력을 압도적으로 향상시키며, 기계학습 기반 코드를 평가할 때 필수적인 확률론적 검증의 정확성을 보장한다.</p>
<h2>6.  엄격한 페르소나의 신뢰성 검증: PersonaScore와 규범적 평가 지표</h2>
<p>“당신은 엄격한 테스터입니다“라는 프롬프트를 세밀하게 조정했다 하더라도, LLM이 실제로 런타임 환경에서 해당 페르소나의 제약 조건 내에서 흔들림 없이 일관되게 행동하는지를 수학적으로 증명하지 못한다면 오라클은 결국 신뢰할 수 없는 블랙박스에 불과하다. 합성된 페르소나는 종종 지시를 잊어버리거나(Persona Drop), 사실과 다르지만 그럴듯하게 들리는 환각(Hallucination)을 발생시키며, 입력값의 사소한 문체적 섭동(Perturbation)에도 평가의 일관성을 잃어버리는 치명적인 취약점을 드러내기 때문이다.</p>
<p>따라서 소프트웨어 배포 파이프라인에 오라클을 통합하기 전, 오라클 페르소나 자체의 강건성을 테스트하는 메타 검증(Meta-evaluation) 체계가 반드시 선행되어야 한다. 이를 위해 최근 AI 평가 벤치마크 영역에서는 <strong>PersonaGym</strong> 프레임워크와 이를 정량화하는 <strong>PersonaScore</strong> 지표와 같은 다차원적 자동화 평가 방법론이 표준으로 자리잡고 있다.</p>
<h3>6.1  의사결정 이론(Decision Theory)에 기반한 페르소나 검증의 3대 축</h3>
<p>PersonaGym은 고정된 정적 테스트 데이터셋의 한계를 벗어나, 페르소나 에이전트가 마주할 수 있는 150개 이상의 다양한 동적 환경(Environment)을 시뮬레이션하고 맞춤형 질문을 무작위로 생성하여 오라클의 한계를 혹독하게 시험한다. 이 평가는 단순한 텍스트 유사도가 아니라, 경제학과 인지과학의 의사결정 이론에 뿌리를 둔 세 가지 핵심 축을 기준으로 수행된다.</p>
<ol>
<li><strong>규범적 평가 (Normative Evaluation)</strong>: 이는 오라클 페르소나가 주어진 시스템적 제약 조건 하에서 ’완전히 합리적인 의사결정자(Fully rational decision maker)’로서 최적의 판단을 내리는지를 평가한다. ‘기대 행동(Expected Action)’ 태스크를 통해, 매우 꼬여있는 비즈니스 로직 시나리오를 던져주었을 때 오라클이 감정에 치우치거나 환각에 빠지지 않고 페르소나에 부합하는 가장 논리적이고 안전한 결론을 도출하는지 측정한다. 예를 들어, 금융 보안 판별 오라클이 모호한 트랜잭션 코드에 대해 “사용자의 편의성을 위해 일단 허용“을 선택했다면, 이는 오라클로서의 규범적 합리성을 심각하게 위반한 것으로 간주되어 큰 감점을 받는다.</li>
<li><strong>규정적 평가 (Prescriptive Evaluation)</strong>: 이 축은 오라클이 사전에 지시받은 규칙, 제약 조건, 그리고 ’언어적 습관(Linguistic Habits)’을 얼마나 철저히 고수하는지 측정한다. 프롬프트를 통해 엄격한 오라클에게 “설명을 덧붙이지 말고 JSON 포맷만 출력하라”, “정중한 인사를 생략하라”, “독성 어휘를 포함한 코드는 가차 없이 기각하라“라고 지시했을 때, 이를 100% 준수하는지 감시하는 것이다. 페르소나 일관성(Persona Consistency) 유지 지표가 핵심이며, 여기서 규칙을 어기고 인간적인 대화체로 답변하는 모델은 아무리 정답을 맞혔다 하더라도 오라클 자격을 박탈당한다.</li>
<li><strong>기술적 평가 (Descriptive Evaluation)</strong>: 오라클이 내린 결론이 데이터셋의 편향에 의해 우연히 맞은 것인지, 아니면 부여받은 페르소나에 입각한 정교하고 투명한 논리적 추론(Reasoning) 과정의 결과인지를 심사한다. ‘행동 정당화(Action Justification)’ 태스크를 통해 오라클이 점수를 부여한 논리가 제공된 명세서나 루브릭에 정확히 수학적/논리적으로 근거하고 있는지 그 출처를 검증한다. 이는 체인 오브 사고(CoT) 프롬프트가 허구의 논리를 만들어내지 않았는지를 확인하는 필수 절차이다.</li>
</ol>
<h3>6.2  인간 판단과의 정렬(Alignment)과 최적 매칭(Optimal Matching)의 수학적 증명</h3>
<p>PersonaScore 지표는 단일 모델 평가의 편향성을 회피하기 위해, 여러 강력한 LLM들을 평가자 패널로 동원하여 전문가가 큐레이팅한 정밀 루브릭을 바탕으로 타겟 오라클의 응답을 채점하고 이를 앙상블하여 최종 점수를 산출한다. 이러한 다차원 평가 점수 공식은 관계 심리학이나 알고리즘 매칭 문제에서 널리 활용되는 보상 모델링(Reward Modeling) 공식을 수학적으로 차용하여 정당화된다.</p>
<p>시뮬레이션 환경 내에서 오라클 에이전트의 정책(Policy)이 인간 시스템 엔지니어가 기대하는 행동 분포를 완벽히 근사할수록(즉, 예측 보상 오차의 한계 <span class="math math-inline">\vert \hat{R}(i, j) - R^*(i, j) \vert \le L_\epsilon \epsilon + L_\delta \delta</span> 에서 <span class="math math-inline">\epsilon, \delta \to 0</span> 으로 수렴할 때), 페르소나가 유도하는 결정론적 판별 결과 <span class="math math-inline">\hat{\mathcal{M}}</span> 은 결국 인간 전문가의 가장 이상적인 판단 최적값(Optimal stable matching) <span class="math math-inline">\mathcal{M}^*</span> 에 수학적으로 완벽히 수렴한다는 강력한 정리(Theorem)가 최근 입증되었다. 즉, 통제되지 않은 야생의 LLM은 믿을 수 없으나, 정교한 페르소나 프롬프트를 주입받고 PersonaScore를 통해 철저히 메타 검증을 통과한 ’엄격한 오라클’은 인간 소프트웨어 검증 엔지니어와 동등하거나 때로는 그 이상의 무결한 신뢰도를 수학적으로 보장받을 수 있다는 의미이다.</p>
<h2>7.  오라클 유도를 위한 프롬프트 최적화 기술 (Prompt Optimization)</h2>
<p>오라클 역할을 완벽히 수행하는 엄격한 페르소나 프롬프트를 단 한 번의 시도로 작성해 내는 것은 사실상 불가능에 가깝다. 특히 확률적 모델의 특성상 의도치 않은 문맥 편향이나 형식 파괴 현상이 입력 데이터의 미세한 변화에 따라 상시 발생할 수 있으므로, 체계적인 프롬프트 최적화 기법이 시스템 배포 파이프라인 내부에 내재화되어야 한다.</p>
<ol>
<li><strong>메타 프롬프팅 및 자동 프롬프트 최적화 (APO, Automatic Prompt Optimization)</strong>: 인간이 수동으로 문구를 고치는 대신, 능동 샘플링(Active Sampling) 기반의 프레임워크를 통해 LLM 스스로 오라클 페르소나 프롬프트를 개선하도록 유도하는 방법이다. 알고리즘은 레이블이 없는 방대한 로그 데이터셋에서 모델이 혼동을 일으켰거나(불확실성이 높거나), 데이터의 의미론적 다양성이 큰 샘플들을 수학적으로 클러스터링하여 추출한다. 이후, 인간 전문가가 레이블링한 정답(Ground-truth)과 현재 버전의 오라클 페르소나가 낸 오답을 비교하게 한다. 반성(Reflection) 메커니즘을 수행하는 메타 LLM은 “루브릭 2번 항목의 모호함 때문에 오라클이 실패했다“는 점을 식별하고, 약점을 보완한 새로운 프롬프트 버전 <span class="math math-inline">p_{i+1}</span> 을 지속적으로 합성하여 성능을 끌어올린다.</li>
<li><strong>반복적 자가 정제 (Self-Refinement) 및 인간 참여(Human-in-the-Loop)</strong>: 판정을 내리기 직전, 오라클 페르소나 스스로 자신의 판정을 강박적으로 의심하고 검토하도록 지시하는 기법이다. 평가 템플릿 내에 다음과 같은 3단계 지시를 포함시켜 내부 루프를 생성한다.</li>
</ol>
<ul>
<li>1단계 (초기 판정): 설정된 루브릭에 따라 코드에 대한 1차 점수와 근거를 임시로 생성한다.</li>
<li>2단계 (자가 비판): “내 점수가 엣지 케이스에 대해 너무 가혹하거나 관대하지 않은가? 입력의 전체 문맥과 제약조건을 전부 고려했는가? 다른 깐깐한 전문가도 동의할 것인가?“라는 질문을 강제로 던져 초기 판정의 편향성을 비판한다.</li>
<li>3단계 (최종 정제): 자가 비판 결과를 수용하여 점수를 수정하거나 확정짓고, 요구된 최종 JSON 포맷으로만 출력을 내보낸다. 여기에 인간 전문가(Human-in-the-Loop)의 어노테이션이 간헐적으로 결합되면, 모델이 스스로 발견하지 못하는 사각지대를 지속적으로 보정할 수 있다.</li>
</ul>
<ol start="3">
<li><strong>퓨샷 학습(Few-Shot Learning) 벤치마크 캘리브레이션</strong>: 엄격한 오라클 프롬프트에는 지시문뿐만 아니라, 반드시 높은 점수(Pass), 애매한 중간 점수(Borderline), 낮은 점수(Fail)에 해당하는 매우 구체적이고 명확한 벤치마크 판정 사례와 그 이유를 조목조목 따진 정당화(Justification) 텍스트가 퓨샷(Few-shot) 형태로 포함되어야 한다. 이는 확률 모델에게 추상적인 규칙 문장 대신, 고차원 잠재 공간 내에서 따라가야 할 실질적인 ’수학적 참조점(Reference point)’을 제공한다. 이를 통해 오라클은 일반적인 환각에 의존하지 않고 실제 인간 엔지니어가 요구하는 판단의 잣대에 정확히 정렬(Calibration)된다.</li>
</ol>
<p>소프트웨어 개발 과정에서 결정론적 정답지를 생산해 내는 완벽한 오라클을 AI로 구현하기 위해서는 “당신은 엄격하고 깐깐한 테스터입니다“라는 식의 막연한 텍스트 기반 역할극 수준에서 시급히 벗어나야 한다. 모델 아키텍처 내부의 신경망 경로(역할 벡터)를 제어한다는 수학적 통찰을 바탕으로, 런타임 인스턴스 정렬을 통한 동적 라우팅, 철저히 통제된 이진/저해상도 루브릭의 주입, 다중 에이전트 앙상블을 통한 인식론적 맹점 극복, 그리고 PersonaScore 기반의 엄밀한 규범적 메타 평가가 유기적으로 결합되어야만 비로소 현대 소프트웨어 파이프라인이 요구하는 치열하고 타협 없는 신뢰성을 확보할 수 있을 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>LLM-as-a-judge: a complete guide to using LLMs for evaluations, https://www.evidentlyai.com/llm-guide/llm-as-a-judge</li>
<li>LLM-as-a-Judge Prompt Optimization - Phoenix - Arize AI, https://arize.com/docs/phoenix/cookbook/prompt-engineering/llm-as-a-judge-prompt-optimization</li>
<li>Personas in System Prompts Do Not Improve Performances of Large, https://aclanthology.org/2024.findings-emnlp.888.pdf</li>
<li>Personas in System Prompts Do Not Improve Performances of Large, https://aclanthology.org/2024.findings-emnlp.888/</li>
<li>Evaluating LLMs Across Diverse Writing Styles - arXiv.org, https://arxiv.org/html/2507.22168v2</li>
<li>The Hidden Connection Between AI Experts and Personas - Medium, https://medium.com/@pur4v/the-hidden-connection-between-ai-experts-and-personas-how-your-prompts-might-be-routing-neural-cd506e3163b4</li>
<li>Rethinking the Impact of Role-play Prompts in Zero-shot Reasoning, https://aclanthology.org/2025.findings-ijcnlp.51.pdf</li>
<li>Personas in System Prompts Do Not Improve Performances of Large, https://arxiv.org/html/2311.10054v2</li>
<li>LLM As a Judge: Tutorial and Best Practices - Patronus AI, https://www.patronus.ai/llm-testing/llm-as-a-judge</li>
<li>LLM-as-Judge Stage: Automated Legal Evaluation - Emergent Mind, https://www.emergentmind.com/topics/llm-as-judge-stage</li>
<li>(PDF) Nexus: Execution-Grounded Multi-Agent Test Oracle Synthesis, https://www.researchgate.net/publication/397087653_Nexus_Execution-Grounded_Multi-Agent_Test_Oracle_Synthesis</li>
<li>tmgthb/Autonomous-Agents - GitHub, https://github.com/tmgthb/Autonomous-Agents</li>
<li>LLM-Driven Theory-of-Mind Adversarial Curriculum for Multi-Agent, https://www.preprints.org/frontend/manuscript/048d4c9f35e3f4ddb1c081883fc5d706/download_pub</li>
<li>System for Validating Medical Software Algorithms With Closed, https://www.tdcommons.org/cgi/viewcontent.cgi?article=10417&amp;context=dpubs_series</li>
<li>Large Language Model-Based Automatic Formulation for Stochastic, https://arxiv.org/html/2508.17200v3</li>
<li>© 2022 Zhengkai Wu - IDEALS, https://www.ideals.illinois.edu/items/126827/bitstreams/414451/data.pdf</li>
<li>Enhanced Chemical Reaction Mining through Combined Model, https://pubs.acs.org/doi/10.1021/acs.jcim.5c02011</li>
<li>Validating Knowledge Awareness of LLM-based Persona Agents, https://aclanthology.org/2025.trustnlp-main.22.pdf</li>
<li>VoC-Grounded Interviewable Agentic Synthetic AI Personas for, https://arxiv.org/html/2601.22288v1</li>
<li>How Is Generative AI Used for Persona Development?: A Systematic, https://www.researchgate.net/publication/390570863_How_Is_Generative_AI_Used_for_Persona_Development_A_Systematic_Review_of_52_Research_Articles</li>
<li>PersonaGym: Evaluating Persona Agents and LLMs - arXiv, https://arxiv.org/html/2407.18416v3</li>
<li>PersonaGym: Evaluating Persona Agents and LLMs - ACL Anthology, https://aclanthology.org/2025.findings-emnlp.368.pdf</li>
<li>PersonaGym: Evaluating Persona Agents and LLMs - ResearchGate, https://www.researchgate.net/publication/382638938_PersonaGym_Evaluating_Persona_Agents_and_LLMs</li>
<li>[Literature Review] PersonaGym: Evaluating Persona Agents and, https://www.themoonlight.io/en/review/personagym-evaluating-persona-agents-and-llms</li>
<li>Persona-Based Romantic Compatibility Through LLM Text World, https://arxiv.org/html/2512.11844</li>
<li>Enhancing LLM-as-a-Judge through Active-Sampling-based Prompt, https://aclanthology.org/2025.acl-industry.67.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>