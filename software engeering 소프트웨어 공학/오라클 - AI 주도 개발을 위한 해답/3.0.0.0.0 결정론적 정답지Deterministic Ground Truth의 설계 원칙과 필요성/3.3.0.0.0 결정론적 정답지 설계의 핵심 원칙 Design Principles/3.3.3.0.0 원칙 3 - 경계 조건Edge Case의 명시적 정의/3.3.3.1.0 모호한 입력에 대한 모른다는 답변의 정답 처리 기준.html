<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.3.3.1 모호한 입력에 대한 '모른다'는 답변의 정답 처리 기준</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.3.3.1 모호한 입력에 대한 '모른다'는 답변의 정답 처리 기준</h1>
                    <nav class="breadcrumbs"><a href="../../../../../index.html">Home</a> / <a href="../../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../../index.html">Chapter 3. 결정론적 정답지(Deterministic Ground Truth)의 설계 원칙과 필요성</a> / <a href="../index.html">3.3 결정론적 정답지 설계의 핵심 원칙 (Design Principles)</a> / <a href="index.html">3.3.3 원칙 3: 경계 조건(Edge Case)의 명시적 정의</a> / <span>3.3.3.1 모호한 입력에 대한 '모른다'는 답변의 정답 처리 기준</span></nav>
                </div>
            </header>
            <article>
                <h1>3.3.3.1 모호한 입력에 대한 ’모른다’는 답변의 정답 처리 기준</h1>
<p>AI 기반 소프트웨어 개발에서 가장 까다로운 검증 과제 중 하나는 ’정답이 명확하지 않은 입력’에 대해 시스템이 어떻게 반응해야 하는가를 결정론적으로 정의하고 테스트하는 것이다. 전통적인 소프트웨어 테스트에서는 모든 입력 값에 대해 단 하나의 명확한 기대 출력(Expected Output)이 존재한다는 전제를 바탕으로 오라클(Oracle)을 설계한다. 그러나 대형 언어 모델(LLM, Large Language Models)과 같은 확률론적 AI 시스템은 사용자의 모호한 요청이나 정보가 결핍된 입력에 대해서도 그럴듯한 답변을 생성하도록 훈련되어 있다. 이러한 ’강요된 답변(Forced Answer)’은 필연적으로 환각(Hallucination)과 논리적 오류를 유발하며, 고위험 산업군에서는 치명적인 소프트웨어 결함으로 이어진다.</p>
<p>따라서 AI 시스템의 신뢰성을 보장하기 위해서는 모델이 자신의 지식적 한계와 입력의 모호성을 인지하고, 확신할 수 없는 상황에서 ‘모른다(I don’t know)’ 혹은 ’추가 정보가 필요하다’라고 답변하는 것을 명확한 ’정답(Ground Truth)’으로 처리하는 테스트 기준이 필수적이다. 본 절에서는 모호한 입력의 유형을 수학적 및 언어학적으로 분류하고, AI 모델의 인식론적 정직성(Epistemic Honesty)을 평가하기 위한 선택적 예측(Selective Prediction) 메커니즘과, 이를 테스트 오라클에서 결정론적 정답지로 구현하는 엄격한 기준을 심층적으로 분석한다.</p>
<h2>1.  통계적 생성 모델의 구조적 한계와 기권(Abstention)의 필요성</h2>
<p>대형 언어 모델은 근본적으로 다음 토큰을 예측(Next-token Prediction)하여 우도(Likelihood)를 극대화하는 방식으로 작동한다. 이러한 통계적 생성 모델은 입력된 컨텍스트와 가중치(Weights)의 통계적 패턴을 기반으로 가장 확률이 높은 단어의 연속을 출력할 뿐, 자신이 출력하는 정보의 사실적 참/거짓 여부나 논리적 정합성을 독립적으로 검증하지 않는다. 소프트웨어 엔지니어링 관점에서 이는 매우 심각한 불안정성을 내포한다. 모델의 출력이 비결정론적(Non-deterministic)이기 때문에 동일한 입력에 대해서도 미세한 토큰 변화나 부동소수점 정밀도 차이로 인해 결과가 달라질 수 있으며, 이는 소프트웨어의 기능적 무결성을 증명하기 어렵게 만든다.</p>
<p>논문 <em>Learning to Say I Don’t Know: A Vision for Abstention in Large Language Models</em>에서 지적하듯, 인간은 질문이 모호하거나 자신의 지식 범위를 벗어날 경우 자연스럽게 판단을 유보하거나 명시적으로 무지를 인정할 수 있다. 반면 LLM은 분류(Classification) 작업과 달리 확률적 결정 경계(Decision Boundary)가 존재하지 않는 무제한의 생성 공간(Generative Space)에서 작동하기 때문에, 출력 토큰의 확률값이 응답의 의미론적 정확성이나 사실적 진실성과 반드시 비례하지 않는다. 특히 초기 LLM의 인간 피드백 기반 강화 학습(RLHF) 단계에서, 모델은 사용자의 지시에 어떤 형태로든 응답을 제공할 때 더 높은 보상을 받는 경향이 있었다. 이로 인해 모델은 ’모른다’고 기권하는 것보다 틀리더라도 그럴듯한 답변을 추측하여 제공하는 방향으로 최적화되는 구조적 결함을 안게 되었다.</p>
<p>오픈AI(OpenAI) 등의 연구에 따르면, 학습 과정에서 무응답과 오답을 동일하게 취급할 경우 모델은 지속적으로 오답을 생성하더라도 가끔 정답을 맞출 확률을 취하기 위해 무조건적인 ’추측’을 강행하게 된다. 따라서 소프트웨어 엔지니어링 및 테스트 오라클 설계 관점에서 볼 때, 모호한 입력에 대한 모델의 ’모른다’는 답변은 단순한 기능적 실패(Failure)가 아니라, 치명적인 오작동(환각)을 방지하기 위한 가장 안전하고 훌륭한 시스템적 예외 처리(Exception Handling)이자 의도된 기권(Abstention)으로 간주되어야 한다. 결정론적 정답지는 이러한 기권이 발동되어야 하는 정확한 조건과 경계를 수학적으로 명시해야 한다.</p>
<h2>2.  인식론적 정직성(Epistemic Honesty)과 지식 상태의 분류</h2>
<p>AI 시스템이 모호한 입력에 올바르게 대처하기 위해서는 시스템 스스로가 무엇을 알고 무엇을 모르는지 파악하는 메타 인지(Meta-cognition) 능력을 갖추어야 한다. AI 정렬(Alignment) 연구의 핵심인 HHH(Helpful, Honest, Harmless) 프레임워크에서 ’정직성(Honesty)’은 단순히 사실적 정보를 제공하는 것을 넘어, 지식의 한계를 투명하게 인정하는 ’인식론적 정직성(Epistemic Honesty)’과 상호작용적 정직성(Interactive Honesty)을 포괄하는 개념이다. 인식론적 정직성은 AI가 자신이 아는 질문에는 솔직하게 대답하고, 모르는 질문에는 겸손하게 한계를 인정하는 태도를 의미한다.</p>
<p>결정론적 오라클을 구축하기 위해 연구자들은 사용자의 질문에 대한 모델의 내부 지식 상태를 다음의 네 가지 범주로 수학적 및 논리적으로 분류한다.</p>
<table><thead><tr><th><strong>지식 상태 범주</strong></th><th><strong>명칭</strong></th><th><strong>상태 설명 및 오라클의 평가 기준</strong></th></tr></thead><tbody>
<tr><td><strong>Ik-Ik</strong></td><td>Known Knowns (아는 것을 안다)</td><td>모델이 정답을 알고 있으며, 실제로 정확한 답변을 제공하는 상태. 결정론적 정답지가 요구하는 일반적인 정상 작동 상태(Pass)이다.</td></tr>
<tr><td><strong>Ik-Idk</strong></td><td>Known Unknowns (모르는 것을 안다)</td><td>모델이 정답을 모르거나 입력이 모호하다는 것을 인지하고, 답변을 거부(기권)하거나 명확화를 요청하는 상태. 모호한 입력 테스트 시 오라클이 <strong>정답(Pass)으로 처리해야 하는 핵심 목표 상태</strong>이다.</td></tr>
<tr><td><strong>Idk-Idk</strong></td><td>Unknown Unknowns (모르는 것을 모른다)</td><td>모델이 자신이 모른다는 사실조차 인지하지 못하고, 확신에 차서 잘못된 정보(환각)를 생성하는 상태. 오라클이 가장 엄격하게 적발해야 하는 <strong>치명적 실패(Fail)</strong> 상태이다.</td></tr>
<tr><td><strong>Idk-Ik</strong></td><td>Unknown Knowns (아는 것을 모른다)</td><td>모델이 실제로는 답변할 수 있는 문맥과 지식을 갖추고 있음에도 불구하고, 과도하게 보수적으로 설정되어 불필요하게 기권하는 상태. 유용성을 저해하는 ‘게으른 AI’ 상태로, 오라클에서 **실패(Fail)**로 간주된다.</td></tr>
</tbody></table>
<p>테스트 오라클의 목표는 모호한 입력이나 지식 범위를 벗어난 엣지 케이스(Edge Case)가 주어졌을 때 시스템의 상태를 철저히 ’Ik-Idk’로 강제하고, 이 응답을 성공적인 테스트 통과(Pass)로 처리하는 명확한 기준을 확립하는 것이다.</p>
<p><img src="./3.3.3.1.0%20%EB%AA%A8%ED%98%B8%ED%95%9C%20%EC%9E%85%EB%A0%A5%EC%97%90%20%EB%8C%80%ED%95%9C%20%EB%AA%A8%EB%A5%B8%EB%8B%A4%EB%8A%94%20%EB%8B%B5%EB%B3%80%EC%9D%98%20%EC%A0%95%EB%8B%B5%20%EC%B2%98%EB%A6%AC%20%EA%B8%B0%EC%A4%80.assets/image-20260220214700586.jpg" alt="image-20260220214700586" /></p>
<h2>3.  모호성(Ambiguity) 및 답변 불가(Unanswerable) 입력의 수학적 정의와 유형화</h2>
<p>결정론적 정답지를 구축하기 위해서는 먼저 ’모호함’이라는 추상적이고 주관적인 개념을 수학적이고 논리적인 기준으로 명확히 정의해야 한다. 테스트 오라클은 단순히 “질문이 모호하다“는 직관적 평가나 테스터의 주관적 감각에 의존할 수 없으며, 입력값이 어떤 구조적 결함을 가지고 있는지 정밀하게 식별해야 한다. 의미론적 파싱(Semantic Parsing) 및 Text-to-SQL 변환 모델의 성능을 평가하기 위해 고안된 데이터셋인 SQUAB(SQL Unanswerable and Ambiguous Benchmarking)의 기준을 차용하면, 모호한 입력을 구문론적 및 논리적으로 엄격하게 분류할 수 있다.</p>
<p>자연어 입력(<span class="math math-inline">nl</span>)을 시스템이 처리할 수 있는 정형화된 쿼리 또는 논리적 함수 집합(<span class="math math-inline">Q</span>)으로 매핑하는 함수를 <span class="math math-inline">f</span>라고 정의하자. 이때 입력의 모호성과 답변 불가능성은 매핑 결과 집합의 기수(Cardinality), 즉 <span class="math math-inline">\vert f(nl) \vert</span>의 크기에 따라 수학적으로 결정된다.</p>
<h3>3.1  다중 해석이 가능한 모호성 (Ambiguity: <span class="math math-inline">\vert f(nl) \vert &gt; 1</span>)</h3>
<p>입력값이 단일한 논리적 결론으로 귀결되지 않고, 상호 배타적이거나 동등하게 유효한 두 개 이상의 의미론적 해석을 파생시키는 경우이다. 이 경우 함수 <span class="math math-inline">f(nl)</span>의 기수는 1보다 크다. SQUAB 벤치마크는 이를 세 가지 구체적인 유형으로 세분화한다.</p>
<ul>
<li><strong>컬럼 및 속성 모호성 (Column Ambiguity):</strong> 지시 대명사나 약어, 범용적 단어가 시스템 내의 여러 속성과 동시에 일치하는 경우이다. 예를 들어, 농구 기록 데이터베이스에서 “필드골 비율이 높은 선수를 찾아라“라고 자연어로 입력했을 때, ’필드골’이 2점 슛을 포함한 전체 필드골(FG%)을 의미하는지, 3점 슛(3FG%)만을 의미하는지 문맥상 특정할 수 없다. 이 경우 오라클은 모델이 임의로 FG%를 선택하여 데이터를 반환하는 것을 오답으로 처리해야 한다.</li>
<li><strong>스코프 모호성 (Scope Ambiguity):</strong> ‘각각(each)’, ’모든(every, all)’과 같은 한정사(Quantifier)의 논리적 적용 범위가 불분명한 경우이다. “모든 팀이 지원하는 선수를 나열하라“는 문장은 두 가지 해석을 낳는다. 첫째, 집단으로서의 전체 팀이 공통으로 지원하는 단 한 명의 특정 선수를 의미할 수 있다. 둘째, 각각의 개별 팀이 지원하는 모든 선수들의 전체 목록을 의미할 수도 있다.</li>
<li><strong>수식어 부착 모호성 (Attachment Ambiguity):</strong> 수식어절이 꾸며주는 대상이 문장 구조상 두 개 이상일 수 있는 경우이다. “성공률이 50% 이상인 선수와 팀을 출력하라“는 조건에서, ’성공률 50% 이상’이라는 제약 조건이 선수 개인에게만 적용되는지, 아니면 팀 전체의 평균에도 적용되는지 확정할 수 없다.</li>
</ul>
<p>이러한 모호성이 발생했을 때, LLM이 임의로 하나의 해석에 암묵적으로 동의(Implicitly committing to one interpretation)하여 단일 답변을 출력하는 것은 심각한 보안 및 신뢰성 위험을 초래한다. 사용자가 의도하지 않은 해석을 기반으로 시스템이 데이터베이스 삭제나 금융 거래 승인과 같은 비가역적인 동작(Irreversible actions)을 수행할 수 있기 때문이다.</p>
<h3>3.2  정보 부재로 인한 답변 불가능 (Unanswerable: <span class="math math-inline">f(nl) = \emptyset</span>)</h3>
<p>입력된 자연어 질의를 시스템의 지식 베이스나 실행 가능한 논리식으로 매핑할 수 없는 상태이다. 이때의 기수는 0이며, 결과 집합은 공집합(<span class="math math-inline">\emptyset</span>)이 된다.</p>
<ul>
<li><strong>참조 불가능 (Out-of-Vocabulary / Column Unanswerable):</strong> 입력에 포함된 핵심 엔티티나 속성이 시스템 데이터베이스 스키마나 RAG(Retrieval-Augmented Generation) 문서 집합에 전혀 존재하지 않는 경우이다. 인사 데이터베이스에 ‘가족 관계’ 컬럼이 없는데 “직원들의 가족 수를 요약하라“고 지시하는 경우가 이에 해당한다.</li>
<li><strong>계산 불가능 (Calculation Unanswerable):</strong> 시스템이 수행할 수 없는 복잡한 연산이나 정의되지 않은 사용자 정의 함수(UDF)를 암묵적으로 요구하는 경우이다. 예를 들어 원시 데이터만 존재하는 시스템에 “선수들의 종합 효율성 지수를 계산하라“고 요구하지만, 해당 지수의 산출 공식이 시스템 내에 정의되어 있지 않은 경우다.</li>
<li><strong>도메인 이탈 (Out-of-Scope):</strong> 주어진 시스템의 목적과 권한을 완전히 벗어난 지시이다. 기업 내부의 재무 문서 기반 폐쇄형 검색 시스템에서 외부의 실시간 날씨 정보를 묻거나, 내년도 거시경제 지표 예측을 요구하는 경우가 이에 해당한다.</li>
</ul>
<p>테스트 오라클은 기수가 1보다 크거나(<span class="math math-inline">\vert f(nl) \vert &gt; 1</span>) 0인(<span class="math math-inline">f(nl) = \emptyset</span>) 모든 테스트 케이스에 대하여, 모델이 확정적이고 단정적인 출력을 내놓는 것을 치명적 결함으로 간주하고 실패(Fail) 처리해야 한다.</p>
<h2>4.  ‘모른다’ 응답에 대한 결정론적 정답 처리 기준(Ground Truth Criteria)</h2>
<p>위의 분류를 바탕으로, 테스트 오라클이 모호한 입력에 대해 AI의 ’모른다’는 응답을 무작위적 실패가 아닌 **‘올바른 형태적 기권(Conformal Abstention)’**으로 평가하고 정답 처리하기 위해서는 객관적이고 검증 가능한 설계 원칙을 준수해야 한다. 다음은 AI 소프트웨어 테스트에서 기권을 정답으로 수용하기 위한 핵심 논리적 기준들이다.</p>
<h3>4.1  기준 1: 의미론적 비일관성(Incoherence) 임계치 초과 기반의 기권</h3>
<p>최신 소프트웨어 검증 연구에서는 명확한 정답지가 없는 상황에서 LLM의 오류를 탐지하기 위해 ’비일관성(Incoherence)’을 핵심 프록시(Proxy)로 사용한다. 모호한 입력에 대해 LLM에게 동일한 프롬프트로 다수의 응답(예: <span class="math math-inline">k=10</span>, 온도 매개변수 <span class="math math-inline">t &gt; 0</span>)을 병렬 생성하도록 지시했을 때, 생성된 논리적 추론 경로와 최종 결과물들이 기능적으로 서로 강하게 충돌한다면 해당 입력은 내재적으로 모호성을 내포하고 있는 것이다.</p>
<p>합의 샘플링(Consensus Sampling) 알고리즘은 이 원리를 활용한다. 주어진 프롬프트에 대해 여러 모델(또는 단일 모델의 다중 샘플)이 생성한 응답의 일치율(Overlap)이 사전 정의된 신뢰 임계치(Threshold) 미만으로 떨어지는 조건, 즉 다중 해석의 충돌이 발생하면 오라클은 이를 즉각 감지한다. 이 조건이 달성되었을 때, 시스템이 단일 답변 도출을 강행하지 않고 “입력의 해석이 상호 배타적입니다(Idk)“라고 응답하거나 사용자의 의도를 명확히 묻는 역질문(Clarification Question)을 반환하면 오라클은 이를 완벽한 정답으로 처리한다.</p>
<h3>4.2  기준 2: 예측 집합 및 라이선스 오라클(License Oracle) 결여에 따른 기권</h3>
<p>확률론적 생성 모델에 결정론적 통제력을 부여하는 또 다른 강력한 방법은 ‘라이선스 오라클(License Oracle)’ 방식의 다단계 검증 파이프라인이다. 이 시스템에서 AI는 최종 사용자에게 정보를 출력하기 전에, 자신이 생성한 모든 사실적 주장에 대해 구조화된 지식 그래프(Knowledge Graph)나 검색된 문서 내에서 명시적인 ’증명(License)’을 획득해야 한다.</p>
<p>이 과정은 GLiNER와 같은 정보 추출 모델을 사용하여 AI의 응답 초안에서 사실적 클레임을 주어-동사-목적어의 트리플(Triple) 형태로 추출하고, 이를 SHACL(Shapes Constraint Language) 규칙이나 내부 데이터베이스와 교차 검증하는 방식으로 이루어진다.</p>
<ul>
<li><strong>정답 처리 조건:</strong> 만약 사용자의 입력이 시스템에 제공된 컨텍스트 범위를 벗어나는 정보를 요구하여, 시스템이 도출한 클레임 중 단 하나라도 검증 라이선스를 획득하지 못한 경우.</li>
<li><strong>기대 출력:</strong> 시스템은 조작된 답변을 출력하는 대신 “제공된 문서에서 해당 정보를 찾을 수 없습니다” 또는 명시적인 기권 상태 코드(예: <code>{"status": "abstain", "reason": "insufficient_evidence"}</code>)를 반환해야 한다.</li>
<li><strong>오라클의 평가:</strong> 단위 테스트(Unit Test) 오라클은 이 명시적 기권 상태 코드를 반환한 테스트 케이스를 100% 성공(Pass)으로 확정한다.</li>
</ul>
<h3>4.3  기준 3: 다중 해석-답변 쌍(Multiple Interpretation-Answer Pairs)의 구조화된 반환</h3>
<p>질문이 모호할 때 단순히 “모른다“고 답하는 것은 시스템의 안전성을 보장하지만, 사용자 경험 측면에서 정보성이 떨어질 수 있다. 이를 극복하기 위해 제안된 최신 정렬 기법은 모호성을 수용하는 ’구조화된 다중 답변 생성(Pluralistic Alignment)’이다. 입력이 모호하여 <span class="math math-inline">\vert f(nl) \vert &gt; 1</span> 조건이 성립할 때, 고도화된 시스템은 단일 답변에 집착하지 않고 자신이 식별한 잠재적 해석들을 병렬로 나열하며 각각에 대한 조건부 답변을 동시에 제공한다.</p>
<p>예를 들어, “이전 분기의 매출을 보여줘“라는 모호한 질의에 대해, 시스템은 다음과 같이 구조화된 출력을 내놓아야 한다.</p>
<ol>
<li><strong>해석 A:</strong> ’이전 분기’를 회계연도 기준 3분기로 해석할 경우 -&gt; 3분기 매출액 데이터.</li>
<li><strong>해석 B:</strong> ’이전 분기’를 역년 기준 직전 3개월로 해석할 경우 -&gt; 직전 3개월 매출액 데이터.</li>
</ol>
<p>오라클은 모호성을 감지했을 때 허용하는 결정론적 정답 포맷의 조건을 엄격히 정의해야 한다. 표 2는 입력의 수식적 기수 조건에 따른 기대 응답과 오라클의 평가 기준을 요약한 것이다.</p>
<table><thead><tr><th><strong>수학적 기수 조건</strong></th><th><strong>입력의 상태</strong></th><th><strong>기대하는 AI 응답 형태 (결정론적 정답지 기준)</strong></th><th><strong>오라클 통과(Pass) 조건</strong></th></tr></thead><tbody>
<tr><td><strong><span class="math math-inline">\vert f(nl) \vert = 1</span></strong></td><td>명확한 입력</td><td>단일하고 정확한 결정론적 정답 출력</td><td>텍스트 또는 JSON 값이 기대값과 정확히 일치</td></tr>
<tr><td><strong><span class="math math-inline">f(nl) = \emptyset</span></strong></td><td>도메인 이탈 / 정보 결핍</td><td>정형화된 거절 및 기권 (예: “정보 부족”)</td><td><code>abstain=true</code> 플래그 및 사유 코드 반환</td></tr>
<tr><td><strong><span class="math math-inline">\vert f(nl) \vert &gt; 1</span></strong></td><td>의미론적 모호성</td><td>역질문(Clarification)을 통한 사용자 의도 확인</td><td>모호성 감지 플래그 반환 및 질문 형태의 출력</td></tr>
<tr><td><strong><span class="math math-inline">\vert f(nl) \vert &gt; 1</span></strong></td><td>의미론적 모호성</td><td>파생 가능한 모든 해석에 대한 <strong>구조화된 조건부 답변 세트</strong> 제공</td><td>응답 JSON 배열에 2개 이상의 유효한 해석 브랜치 존재</td></tr>
</tbody></table>
<p>테스트 오라클은 모호한 테스트 케이스가 입력되었을 때, 응답 페이로드 내에 다수의 해석 브랜치가 생성되었거나 명시적인 <code>abstain</code> 플래그가 참(True)으로 활성화된 경우에만 정답으로 간주하여 테스트를 통과시킨다.</p>
<h2>5.  ‘모른다’ 응답 능력을 측정하는 평가 지표 및 상충 관계 (Trade-offs)</h2>
<p>기권 능력을 소프트웨어 테스트 파이프라인에 통합할 때 소프트웨어 엔지니어가 직면하는 가장 큰 함정은 지표 게이밍(Metric Gaming)이다. 모델이 환각과 오답에 대한 페널티를 피하고 안전성 점수를 높이기 위해 거의 모든 질문에 “모르겠습니다“라고 답변하도록 편향된다면, 이 모델은 완벽하게 ’정직’하지만 소프트웨어로서의 유용성은 0에 수렴하게 된다. 이를 방지하기 위해 정답 처리 기준은 엄격한 평가지표의 균형 위에서 설계되어야 한다.</p>
<h3>5.1  진실성(Truthfulness)과 정보성(Informativeness)의 딜레마</h3>
<p>논문 <em>TruthfulQA: Measuring How Models Mimic Human Falsehoods</em>는 기권과 유용성 간의 긴장 관계를 명확히 짚어낸다. TruthfulQA 벤치마크에서는 모델의 응답 성능을 두 가지 독립적인 차원으로 철저히 분리하여 평가한다.</p>
<ul>
<li><strong>진실성(Truthfulness):</strong> 문자 그대로의 사실(Literal truth)을 반영하며, 거짓된 진술(False statement)을 주장하지 않는 능력이다. 이 평가 기준에서 “노코멘트”, “모릅니다”, “확인할 수 없습니다“와 같은 비형식적(Non-committal) 기권 응답은 설령 모델 내부에 정답 지식이 내재되어 있더라도 거짓을 발화하지 않았으므로 **참(True)**으로 평가된다.</li>
<li><strong>정보성(Informativeness):</strong> 질문이 제기한 불확실성을 실제로 해소해 주는 유용한 정보를 제공하는 능력이다. 모델이 진실성 점수를 100%로 만들기 위해 모든 질문에 “모른다“고 답하는 꼼수를 부리는 것을 방지하는 안전장치 역할을 한다.</li>
</ul>
<p>정밀도(Precision)와 재현율(Recall)이 역의 상관관계를 갖는 것처럼, 기권(Abstention)은 진실성을 극대화하지만 무분별하게 남용될 경우 시스템의 정보성을 급격히 훼손한다. 인간의 기준 성능은 진실성 94%에 달하지만, 통제되지 않은 초기 GPT-3 모델은 진실성이 58%에 불과했으며, 나머지 42%의 응답은 정보성은 높지만 사실이 아닌 ’유창한 거짓말(Informative falsehoods)’을 생성해냈다. 따라서 결정론적 오라클은 ’모른다’는 답변을 무조건 정답 처리하는 것이 아니라, 해당 입력이 앞서 정의된 ‘모호성’ 또는 ‘정보 부재’ 조건에 수학적으로 부합할 때에만 정답으로 인정하는 정밀한 타겟팅이 필요하다.</p>
<p><img src="./3.3.3.1.0%20%EB%AA%A8%ED%98%B8%ED%95%9C%20%EC%9E%85%EB%A0%A5%EC%97%90%20%EB%8C%80%ED%95%9C%20%EB%AA%A8%EB%A5%B8%EB%8B%A4%EB%8A%94%20%EB%8B%B5%EB%B3%80%EC%9D%98%20%EC%A0%95%EB%8B%B5%20%EC%B2%98%EB%A6%AC%20%EA%B8%B0%EC%A4%80.assets/image-20260220214724954.jpg" alt="image-20260220214724954" /></p>
<h3>5.2  선택적 예측 능력(Abstention Ability)의 정량화</h3>
<p>모델의 진실한 거절 능력을 측정하기 위해 테스트 오라클은 AUROC(Area Under the Receiver Operating Characteristic Curve)와 같은 이진 분류 지표의 한계를 넘어, ’기권 능력(Abstention Ability, <span class="math math-inline">\mathcal{AA}</span>)’이라는 복합 지표를 활용해야 한다. 단순히 확실과 불확실을 나누는 것을 넘어, 질문이 답변 가능하지만 모델이 불확실하여 오류를 범하기 쉬운 시나리오와 질문 자체가 근본적으로 답변 불가능한 시나리오 모두에서 올바르게 기권했는지를 추적한다.</p>
<p>결정론적 테스트 세트(Golden Dataset)를 구축할 때는, 인간 전문가에 의해 사전에 ‘답변 불가능’ 또는 ’해석 모호’로 태깅된 데이터셋을 10~20% 비율로 고의로 주입하여야 한다. 이 주입된 모호한 질문들에 대해 모델이 명시적인 거절 템플릿(Refusal Template)을 출력하는지, 즉 Ik-Idk(Known Unknowns) 비율을 얼마나 성공적으로 달성하는지가 시스템의 신뢰도를 판가름하는 척도가 된다. 만약 오라클이 지식 데이터베이스에 명백히 존재하지 않는 질문을 던졌음에도 모델이 기권하지 않고 답변을 생성했다면, 오라클은 이를 즉각적인 실패(Fail)로 규정하고 개발 팀에 경고를 발생시켜야 한다.</p>
<h2>6.  신경망 내부 활성화 분석을 통한 모호성의 직접 판별 (Advanced Application)</h2>
<p>최근 자연어 처리 연구에서는 입력의 모호성이 대형 언어 모델의 표면적 텍스트 출력이 아닌, 가중치 네트워크 내부에 선형적(Linearly)으로 인코딩된다는 놀라운 사실이 밝혀졌다. 논문 <em>Ambiguity is Linearly Encoded in LLMs</em>에 따르면, 모델이 사용자 입력을 처리하는 ’사전 채우기(Pre-filling) 단계’의 초기 레이어에서 특정 소수의 뉴런—‘모호성 인코딩 뉴런(AENs, Ambiguity-Encoding Neurons)’—이 입력값의 모호성 여부를 강력하고 뚜렷한 신호로 포착한다.</p>
<p>이러한 발견은 AI 소프트웨어 테스트 프레임워크 설계에 중대한 패러다임 전환을 가져온다. 기존의 테스트 오라클은 모델이 텍스트로 출력한 최종 결과물이나 부가적으로 생성한 로그 확률(Log-probabilities)을 사후적으로 파싱하여 모호성에 대한 인식 여부를 판별하는 블랙박스 테스트(Black-box testing) 방식에 머물렀다. 그러나 이제는 시스템 엔지니어가 AI 시스템 내부의 특정 뉴런 활성화 상태를 직접 모니터링하여 오라클의 기준값으로 사용하는 화이트박스 테스트(White-box testing)가 가능해진 것이다.</p>
<p>만약 테스트 파이프라인이 모델의 내부 표현(Internal Representation)에 접근할 수 있도록 설계되었다면, 입력 처리 시 AEN 뉴런의 활성화 강도가 특정 임계값을 넘었음을 탐지할 수 있다. 이 상태에서 모델이 ’모른다’고 기권하거나 명확화를 요구하지 않고, 곧바로 단정적이고 확신에 찬 답변을 출력하는 상황이 발생한다면, 테스트 오라클은 표면적 텍스트의 문법적 무결성과 상관없이 이를 즉각적인 기능적 오작동(Fail)으로 식별할 수 있다. 이는 모호성을 처리하는 시스템의 결정론적 오라클을 텍스트의 의미적 파싱 수준이 아닌 신경망의 기초적인 내부 메커니즘 수준에서 근본적으로 정의하고 통제할 수 있음을 시사한다.</p>
<h2>7.  실무 소프트웨어 개발에서의 오라클 구현 전략과 아키텍처</h2>
<p>실제 상용 소프트웨어 개발 환경이나 CI/CD 파이프라인에서 “모호한 입력에 대한 ’모른다’는 응답“을 결정론적 정답으로 확정하고 자동화된 테스트를 수행하기 위해서는 다계층 검증 체계가 필요하다. 단순한 문자열 비교(String Matching)를 넘어서는 아키텍처적 접근이 요구된다.</p>
<h3>7.1  계단식 프롬프트 및 구조화된 출력 강제</h3>
<p>테스트를 용이하게 하기 위해, 애플리케이션 계층에서는 모델이 자연어 서술로 변명을 늘어놓는 것을 방지하고 약속된 JSON 포맷으로 상태를 반환하도록 프롬프트 엔지니어링 및 스키마 강제(Schema Enforcement) 기법을 적용해야 한다. 모델 지침에는 “지식 부족이나 입력의 모호성을 감지했을 경우, 반드시 <code>{"status": "abstain", "error_type": "AMBIGUOUS_SCOPE", "clarification_needed": true}</code> 형태의 구조화된 객체를 반환하라“는 명시적 제약 조건이 포함되어야 한다. 이를 통해 단위 테스트(Unit Test) 작성자는 <code>AssertEquals(expected="AMBIGUOUS_SCOPE", actual=response.error_type)</code>와 같은 전통적이고 결정론적인 단언문(Assertion)을 그대로 사용할 수 있게 된다.</p>
<h3>7.2  하이브리드 오라클: LLM-as-a-Judge 및 캐스케이딩(Cascading) 아키텍처</h3>
<p>사전에 정의하기 어려운 복잡한 문맥적 모호성의 경우, 단일 정답지 대신 평가를 전담하는 독립적인 두 번째 AI 모델을 ’심판(Judge)’으로 활용하는 하이브리드 오라클 시스템을 구축할 수 있다. 평가용 LLM에게 원본 입력, 타겟 모델의 응답, 그리고 엄격한 모호성 평가 루브릭(Rubric)을 제공하여 타겟 모델이 모호성을 적절히 식별하고 적재적소에 기권했는지 타당성을 검증하게 한다.</p>
<p>또한, 실시간 운영 환경에서는 응답 비용과 정확성의 균형을 위해 계단식(Cascading) 위임 아키텍처가 제안된다. 빠르고 저렴한 기본 모델이 먼저 응답을 시도하고, 자체적인 신뢰도 점수가 낮거나 모호성을 감지하면 상위의 강력한 대형 모델로 처리를 위임(Deferral)한다. 상위 모델조차 해당 입력이 내재적으로 불확실성이 높아 의미론적 일관성을 확보할 수 없다고 판단하면(Uncertainty-based Abstention), 최종적으로 답변을 기권하고 인간 전문가(Human-in-the-loop)에게 에스컬레이션(Escalation)하는 구조이다. 테스트 오라클은 이 에스컬레이션 로직이 적절한 임계치에서 정확히 트리거되는지를 검증하는 핵심 역할을 수행한다.</p>
<h2>8.  결론</h2>
<p>결과적으로, AI 시대의 소프트웨어 테스트 오라클은 모델이 얼마나 많은 방대한 질문에 유창하게 답할 수 있는지를 측정하는 수량적 벤치마크를 넘어, 모델이 언제 자신의 한계를 인식하고 침묵하거나 사용자에게 역질문을 던질 수 있는지를 냉철하게 평가하는 메타 인지 검증 시스템으로 진화해야 한다. 모호하고 불완전한 입력이 주어졌을 때 AI 시스템이 산출하는 ’모른다’는 응답은 지능의 부재나 기술적 실패가 아니다. 오히려 이는 확률적 환각이 초래할 수 있는 치명적 시스템 장애를 선제적으로 차단하는 가장 안전한 예외 처리 메커니즘이며, 신뢰할 수 있는 소프트웨어가 갖추어야 할 최후의 보루이자 가장 정확한 결정론적 정답이다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>Testing AI/ML Systems: How to Live with Non-Determinism | by, https://medium.com/@andyakushenko/testing-ai-ml-systems-how-to-live-with-non-determinism-9861129f1c63</li>
<li>The Often Overlooked Test Oracle - Association for Software Testing, https://associationforsoftwaretesting.org/2023/01/10/the-often-overlooked-test-oracle/</li>
<li>Why can’t AI just admit when it doesn’t know? : r/ArtificialInteligence, https://www.reddit.com/r/ArtificialInteligence/comments/1nq7njj/why_cant_ai_just_admit_when_it_doesnt_know/</li>
<li>(PDF) Learning to Say “I Don’t Know”: A Vision for Abstention in, https://www.researchgate.net/publication/394496855_Learning_to_Say_I_Don’t_Know_A_Vision_for_Abstention_in_Large_Language_Models</li>
<li>TruthfulQA: Measuring How Models Mimic Human Falsehoods, https://www.researchgate.net/publication/361063493_TruthfulQA_Measuring_How_Models_Mimic_Human_Falsehoods</li>
<li>The Fusion of Large Language Models and Formal Methods … - arXiv, https://arxiv.org/html/2412.06512v1</li>
<li>The 5 biggest misconceptions about LLM reliability I’ve encountered, https://www.reddit.com/r/AI_Agents/comments/1nax7v1/one_year_as_an_ai_engineer_the_5_biggest/</li>
<li>I wish AI would just admit when it doesn’t know the answer … - Reddit, https://www.reddit.com/r/ArtificialInteligence/comments/1l8msby/i_wish_ai_would_just_admit_when_it_doesnt_know/</li>
<li>CollabLLM: From Passive Responders to Active Collaborators, https://cs.stanford.edu/people/jure/pubs/collabllm-icml25.pdf</li>
<li>Do LLMs Know When to NOT Answer? Investigating Abstention, https://arxiv.org/html/2407.16221v1</li>
<li>An Adaptive Interpretation of Helpful, Honest, and Harmless Principles, https://arxiv.org/html/2502.06059v5</li>
<li>An Adaptive Interpretation of Helpful, Honest, and Harmless Principles, https://arxiv.org/html/2502.06059v4</li>
<li>(PDF) Aligning Pedagogy with Generative AI: An Approach to, https://www.researchgate.net/publication/393651928_Aligning_Pedagogy_with_Generative_AI_An_Approach_to_Customizing_Educational_GPTs</li>
<li>Can AI Assistants Know What They Don’t Know? - arXiv, https://arxiv.org/html/2401.13275v1</li>
<li>Machine Learning Based Perception for Inspection - DTU Inside, https://backend.orbit.dtu.dk/ws/files/312025575/Dissertation_172_sider_52_farveside.pdf</li>
<li>SQUAB: Evaluating LLM robustness to Ambiguous … - ACL Anthology, https://aclanthology.org/2025.emnlp-main.906.pdf</li>
<li>Reasoning about Intent for Ambiguous Requests - arXiv.org, https://arxiv.org/html/2511.10453v2</li>
<li>Reasoning About Intent for Ambiguous Requests - ResearchGate, https://www.researchgate.net/publication/397595570_Reasoning_About_Intent_for_Ambiguous_Requests</li>
<li>Conformal Abstention Framework - Emergent Mind, https://www.emergentmind.com/topics/conformal-abstention</li>
<li>Incoherence as Oracle-less Measure of Error in LLM-Based Code, https://mpi-softsec.github.io/papers/AAAI26-incoherence.pdf</li>
<li>Consensus Sampling for Safer Generative AI - arXiv, https://arxiv.org/html/2511.09493v1</li>
<li>When AI Says ‘I Don’t Know’: The ‘License Oracle’ Revolutionizing, http://oreateai.com/blog/when-ai-says-i-dont-know-the-license-oracle-revolutionizing-truth-in-artificial-intelligence/82d5e21d0c74baa1e06b771874a6bdcd</li>
<li>Grounding Generative Planners in Verifiable Logic - arXiv, https://arxiv.org/html/2602.08373v1</li>
<li>TruthfulQA: Measuring How Models Mimic Human Falsehoods - ar5iv, https://ar5iv.labs.arxiv.org/html/2109.07958</li>
<li>TruthfulQA: Measuring How Models Mimic Human … - ACL Anthology, https://aclanthology.org/2022.acl-long.229.pdf</li>
<li>Alignment for Honesty - arXiv.org, https://arxiv.org/html/2312.07000v2</li>
<li>TruthfulQA: Measuring how models mimic human falsehoods, https://truthful.ai/papers/truthfulqa/</li>
<li>How truthful is GPT-3? A benchmark for language models, https://www.alignmentforum.org/posts/PF58wEdztZFX2dSue/how-truthful-is-gpt-3-a-benchmark-for-language-models</li>
<li>Know Your Limits: A Survey of Abstention in Large Language Models, https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00754/131566/Know-Your-Limits-A-Survey-of-Abstention-in-Large</li>
<li>Learning self-evaluation to improve selective prediction in LLMs, https://patents.justia.com/patent/12536388</li>
<li>How to validate response from AI systems – unpredictable output, https://blog.nashtechglobal.com/how-to-validate-response-from-ai-systems-unpredictable-output-challenges/</li>
<li>Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs, https://www.arxiv.org/pdf/2509.13664</li>
<li>Balancing Performance and Ambiguity Through the Lens of, https://aclanthology.org/2025.emnlp-main.1527.pdf</li>
<li>Verify, Observe, and Secure your Generative AI usage with Oracle, https://blogs.oracle.com/machinelearning/verify-observe-and-secure-your-gen-ai-usage-with-adb-select-ai</li>
<li>Verify Accuracy and Detect Hallucinations in AI Outputs - 5D Vision, https://www.5dvision.com/post/verify-accuracy-and-detect-hallucinations-in-ai-outputs-2/</li>
<li>Cascaded Language Models for Cost-Effective Human–AI Decision, https://openreview.net/pdf/61f2f43f6f3b2d543ebda4f2791b7738eda4f665.pdf</li>
<li>Towards a Science of AI Agent Reliability - arXiv, https://arxiv.org/html/2602.16666v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>