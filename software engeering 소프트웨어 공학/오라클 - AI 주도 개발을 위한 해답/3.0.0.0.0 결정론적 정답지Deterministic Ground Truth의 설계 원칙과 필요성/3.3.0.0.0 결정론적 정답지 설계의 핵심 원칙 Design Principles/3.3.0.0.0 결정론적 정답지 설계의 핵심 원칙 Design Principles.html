<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.3 결정론적 정답지 설계의 핵심 원칙 (Design Principles)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.3 결정론적 정답지 설계의 핵심 원칙 (Design Principles)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 3. 결정론적 정답지(Deterministic Ground Truth)의 설계 원칙과 필요성</a> / <a href="index.html">3.3 결정론적 정답지 설계의 핵심 원칙 (Design Principles)</a> / <span>3.3 결정론적 정답지 설계의 핵심 원칙 (Design Principles)</span></nav>
                </div>
            </header>
            <article>
                <h1>3.3 결정론적 정답지 설계의 핵심 원칙 (Design Principles)</h1>
<h2>1.  서론: 확률적 세계에서의 결정론적 기준 수립의 난제</h2>
<p>현대 소프트웨어 엔지니어링, 특히 제미나이(Gemini)와 같은 대규모 언어 모델(LLM)을 활용한 시스템 개발에 있어 가장 심대하고 본질적인 도전 과제는 ’비결정론(Non-determinism)’의 관리입니다. 전통적인 소프트웨어 테스팅의 패러다임은 결정론적 인과관계에 그 뿌리를 두고 있습니다. 즉, 고정된 시스템 상태 <span class="math math-inline">S</span>에서 입력 <span class="math math-inline">I</span>가 주어졌을 때, 시스템은 반드시 예측 가능한 출력 <span class="math math-inline">O</span>를 산출해야 하며, 이는 언제나 재현 가능해야 한다는 것이 불문율이었습니다. 오라클 데이터베이스(Oracle Database)와 같은 전통적 시스템에서의 ’결정론적 함수(Deterministic Function)’는 동일한 입력값에 대해 항상 동일한 결과를 반환하며, 데이터베이스 상태를 변경하는 부수 효과(Side Effects)가 없는 함수로 정의됩니다. 이러한 환경에서는 테스트 오라클(Test Oracle)—시스템의 실제 동작과 기대 동작을 비교하여 무결성을 판단하는 메커니즘—을 구축하는 것이 상대적으로 명확했습니다.</p>
<p>그러나 생성형 AI의 도래는 이러한 전통적 가정에 근본적인 균열을 일으켰습니다. LLM은 본질적으로 확률적(Probabilistic) 모델입니다. 이들은 다음 토큰을 예측하기 위해 복잡한 고차원 벡터 공간에서의 확률 분포를 계산하며, ’온도(Temperature)’와 같은 매개변수 설정이나 샘플링 전략에 따라 동일한 프롬프트에 대해서도 매번 미묘하게 다르거나 완전히 상이한 텍스트를 생성합니다. 배(Barr) 등의 연구에 따르면, 이러한 비결정론적 시스템에서의 테스트 오라클 문제는 소프트웨어 테스팅의 가장 큰 병목 현상 중 하나로 지적됩니다. “무엇이 정답인가?“라는 질문에 대해 단일한 진리값을 확정할 수 없는 상황, 즉 ’오라클 문제(The Oracle Problem)’가 AI 개발의 전면에 부상한 것입니다.</p>
<p>따라서, 제미나이 기반 애플리케이션을 위한 ’결정론적 정답지(Deterministic Golden Dataset)’의 설계는 단순히 모범 답안을 수집하는 차원을 넘어섭니다. 이는 <strong>확률적 출력을 허용 가능한 품질 기준 내로 통제(Control)하고, 그 변동성을 정밀하게 관측(Observe)하기 위한 엔지니어링 규약</strong>을 수립하는 과정입니다. 정답지(Golden Dataset)는 신뢰할 수 있는 입력과 이상적인 출력을 포함하는 벤치마크 데이터셋으로서, 모델의 출력 품질을 측정하는 ‘그라운드 트루스(Ground Truth)’ 역할을 수행합니다. 본 장에서는 확률적 AI 시스템의 신뢰성을 담보하기 위해 정답지 구축 시 반드시 준수해야 할 5가지 핵심 설계 원칙—<strong>재현성(Reproducibility)</strong>, <strong>원자성(Atomicity)</strong>, <strong>기계 가독성(Machine-Readability)</strong>, <strong>의미론적 등가성(Semantic Equivalence)</strong>, 그리고 <strong>순환적 진화(Evolutionary Refinement)</strong>—을 심층적으로 분석하고, 이를 통해 견고한 테스트 자동화 파이프라인을 구축하는 방안을 제시합니다.</p>
<p><img src="./3.3.0.0.0%20%EA%B2%B0%EC%A0%95%EB%A1%A0%EC%A0%81%20%EC%A0%95%EB%8B%B5%EC%A7%80%20%EC%84%A4%EA%B3%84%EC%9D%98%20%ED%95%B5%EC%8B%AC%20%EC%9B%90%EC%B9%99%20Design%20Principles.assets/image-20260218214043987.jpg" alt="image-20260218214043987" /></p>
<h3>1.1  제1원칙: 재현성 및 상태 격리 (Reproducibility &amp; State Isolation)</h3>
<p>결정론적 정답지의 첫 번째이자 가장 기초적인 요건은 **재현성(Reproducibility)**입니다. 테스트는 과학적 실험과 마찬가지로, 동일한 조건하에서 수행될 때 언제나 동일한 결과를 도출해야 합니다. 만약 테스트를 수행할 때마다 정답의 기준이 흔들리거나, 통제되지 않은 외부 요인에 의해 테스트 결과가 달라진다면, 그 정답지는 ’오라클’로서의 권위를 상실하며, 개발자에게 거짓 양성(False Positive)이나 거짓 음성(False Negative)과 같은 잘못된 신호를 보내게 됩니다.</p>
<h4>1.1.1  환경 변수의 통제와 ‘클린 룸’ 접근법</h4>
<p>브래드쇼(Bradshaw)가 제안한 <strong>SACRED 모델</strong>에 따르면, 신뢰할 수 있는 자동화 테스트를 위해서는 **상태(State)**의 관리가 필수적입니다. AI 모델의 출력은 단순히 프롬프트라는 입력에만 의존하는 것이 아니라, 시스템의 현재 상태, 날짜 및 시간, 사용자 세션 정보, 활성화된 기능 플래그(Feature Flags) 등 방대한 맥락(Context) 정보에 의존합니다. 따라서 정답지를 설계할 때는 <strong>’입력(Input)’의 범위를 명시적으로 확장하여 정의</strong>하고, 이를 통제해야 합니다.</p>
<ul>
<li><strong>시간 의존성 제거 (Temporal Determinism):</strong> “현재 년도를 반환하라” 또는 “올해의 재무 목표를 요약하라“와 같은 질문에 대한 정답이 <code>2024</code>로 하드코딩되어 있다면, 2025년 1월 1일이 되는 순간 이 정답지는 오답이 되며 테스트는 실패할 것입니다. 이는 ’잘못된 비결정론(Incorrectly Marking Non-Deterministic Functions)’의 대표적 사례입니다. 결정론적 설계를 위해서는 정답지에 <code>reference_date</code> 필드를 포함하고, 테스트 실행 시 시스템의 시계(System Clock)를 해당 날짜로 고정(Mocking)하거나 프롬프트 내에 명시적으로 기준 시점을 주입해야 합니다. 이를 통해 시간의 흐름과 무관하게 언제나 동일한 정답 유효성을 유지할 수 있습니다.</li>
<li><strong>외부 지식의 스냅샷 (Knowledge Snapshot):</strong> 검색 증강 생성(RAG) 시스템을 테스트할 때 가장 큰 변수는 검색되는 문서 데이터베이스(Knowledge Base)의 상태입니다. 원본 데이터가 실시간으로 수정되거나 업데이트된다면, 동일한 질문에 대해서도 모델이 참조하는 정보가 달라져 응답이 변하게 됩니다. 정답지 설계 시에는 반드시 <strong>‘스냅샷(Snapshot)’ 된 참조 문서</strong>를 정답지 세트에 포함시켜야 합니다. 즉, 정답지 데이터 구조는 단순한 <code>(질문, 기대 답변)</code>의 쌍이 아니라, <code>(질문, 참조 문서 스냅샷, 기대 답변)</code>으로 구성된 <strong>트리플(Triplet)</strong> 구조를 가져야 합니다. 이는 테스트의 **원자성(Atomicity)**과도 연결되며, 외부 데이터의 변화가 테스트 결과에 영향을 미치지 않도록 격리(Isolation)하는 ‘클린 룸’ 접근법의 핵심입니다.</li>
</ul>
<h4>1.1.2  비결정론적 요소의 파라미터화 및 명시</h4>
<p>LLM의 작동 방식에 내재된 확률적 속성을 통제하기 위해, 모델의 <strong>하이퍼파라미터(Hyperparameters)</strong> 또한 정답지 데이터셋의 메타데이터로 관리되어야 합니다.</p>
<ul>
<li><strong>Temperature &amp; Seed 고정:</strong> 테스트의 결정론적 속성을 극대화하기 위해, 정답지 메타데이터에는 해당 정답을 도출했을 때의 <code>temperature</code> 값(일반적으로 0으로 설정하여 무작위성을 최소화)과 <code>random_seed</code> 값이 명시되어야 합니다. 이는 “이 정답은 특정 설정 하에서만 유효하다“는 **계약(Contract)**을 의미합니다. 온도 0 설정은 모델이 가장 높은 확률을 가진 토큰만을 선택하게 하여 출력의 일관성을 높이지만, 이것만으로 완전한 결정론을 보장하지는 않으므로 시드(Seed) 고정이 병행되어야 합니다.</li>
<li><strong>모델 버전 관리 (Version Control):</strong> 제미나이와 같은 모델은 지속적으로 업데이트됩니다(e.g., <code>gemini-1.5-pro-001</code>에서 <code>gemini-1.5-pro-002</code>로). 모델 버전이 변경되면 동일한 프롬프트와 파라미터에서도 출력이 달라질 수 있으며, 이를 **모델 표류(Model Drift)**라고 합니다. 정답지 설계 시 대상 모델의 구체적인 버전을 명시하여, 모델 업데이트로 인한 성능 변화를 정답지의 오류로 오판하지 않도록 해야 합니다. 이는 회귀 테스트(Regression Testing)의 신뢰성을 담보하는 중요한 장치입니다.</li>
</ul>
<h3>1.2  제2원칙: 원자성 및 독립성 (Atomicity &amp; Independence)</h3>
<p>복잡한 추론과 다단계 사고를 요구하는 AI 시스템의 출력을 통째로 검증하려 시도하는 것은 테스팅의 실패로 가는 지름길입니다. 결정론적 정답지는 거대한 텍스트 덩어리가 아니라, <strong>검증 가능한 최소 단위의 사실(Atomic Facts)들로 정밀하게 분해</strong>되어야 합니다. 이는 테스트의 <strong>원자성(Atomicity)</strong> 원칙으로, 하나의 테스트 케이스는 하나의 논리적 개념만을 검증해야 한다는 원칙입니다.</p>
<h4>1.2.1  복합 질문의 구조적 분해 (Decomposition)</h4>
<p>사용자가 “A사의 2023년 매출을 요약하고, 전년 대비 성장률을 계산한 뒤, 주요 리스크 요인을 나열하라“라고 질문했다고 가정해 봅시다. 이에 대한 정답지(<code>expected_output</code>)를 모델이 생성해야 할 전체 텍스트 문단으로 관리해서는 안 됩니다. 이는 **거짓 부정(False Negative)**을 유발할 가능성이 매우 높습니다. 예를 들어, 모델이 매출액과 성장률은 정확히 맞췄으나 리스크 요인 중 하나를 빠뜨렸거나 문체의 뉘앙스가 다를 경우, 전체 테스트가 실패로 간주될 수 있기 때문입니다. 대신, 이 기대 응답은 세 가지 독립적인 **검증 포인트(Assertions)**로 분리되어야 합니다.</p>
<ol>
<li><strong>사실 검증(Factual Check):</strong> “2023년 매출은 $10B이다.” (정확한 수치 일치 여부 확인) - 이는 검색 및 추출 능력에 대한 검증입니다.</li>
<li><strong>논리 검증(Logic Check):</strong> “성장률은 15%이다.” (계산 로직의 정확성 확인) - 이는 모델의 산술적 추론 능력에 대한 검증입니다.</li>
<li><strong>추론 검증(Reasoning Check):</strong> “리스크 요인으로 공급망 불안정이 언급되었는가?” (키워드 또는 의미론적 포함 여부 확인) - 이는 문맥 이해 및 요약 능력에 대한 검증입니다.</li>
</ol>
<p>이러한 **원자적 분해(Atomic Decomposition)**는 테스트 실패 시 문제의 원인을 즉각적으로 식별할 수 있게 해줍니다. 즉, “모델이 숫자를 틀렸는지(데이터 오류)”, “계산을 못 했는지(추론 오류)”, 아니면 “문맥을 놓쳤는지(이해 오류)“를 구분하여 디버깅 효율성을 극대화할 수 있습니다.</p>
<p><img src="./3.3.0.0.0%20%EA%B2%B0%EC%A0%95%EB%A1%A0%EC%A0%81%20%EC%A0%95%EB%8B%B5%EC%A7%80%20%EC%84%A4%EA%B3%84%EC%9D%98%20%ED%95%B5%EC%8B%AC%20%EC%9B%90%EC%B9%99%20Design%20Principles.assets/image-20260218214106151.jpg" alt="image-20260218214106151" /></p>
<h4>1.2.2  상호 독립성 보장 (Independence)</h4>
<p>정답지의 각 항목(Row)은 다른 항목의 결과에 의존해서는 안 됩니다. 특히 멀티 턴(Multi-turn) 대화를 테스트할 때, 이전 턴(Turn N-1)의 <strong>실제 모델 출력</strong>을 다음 턴(Turn N)의 입력으로 사용하는 것은 위험합니다. 만약 턴 1에서 모델이 예상치 못한 답변을 내놓는다면, 턴 2의 입력 맥락이 오염되어 턴 2의 테스트 결과까지 신뢰할 수 없게 되는 **오류 전파(Error Propagation)**가 발생합니다.</p>
<p>따라서 정답지 설계 시에는 **‘고정된 대화 이력(Fixed Conversation History)’**을 입력으로 제공하여, 각 턴을 독립적인 단일 턴(Single-turn) 문제로 환원시켜야 합니다. 즉, 턴 2를 테스트할 때는 턴 1에서 모델이 <em>실제로 뱉은 말</em>이 아니라, 정답지에 정의된 <em>이상적인 턴 1 답변</em>을 이력으로 주입해야 합니다. 이를 통해 특정 턴에서의 오류가 전체 테스트 스위트를 오염시키는 것을 방지하고, 각 대화 단계에서의 모델 성능을 독립적으로 측정할 수 있습니다.</p>
<h3>1.3  제3원칙: 기계 가독성 및 구조적 강제 (Machine-Readability &amp; Structural Rigor)</h3>
<p>인간이 눈으로 읽고 채점하는 정답지는 소규모 실험에서는 유효할지 모르나, 대규모 엔터프라이즈 시스템의 자동화된 테스트 파이프라인(CI/CD)에는 부적합합니다. 확장 가능한 테스팅을 위해서는 정답지와 AI의 출력이 모두 <strong>기계가 읽을 수 있는(Machine-Readable)</strong> 형태여야 합니다. 이는 데이터의 재사용성을 높이고(FAIR 원칙), 자동화된 검증 도구(Automated Oracle)가 개입할 수 있는 기반을 마련합니다.</p>
<h4>1.3.1  JSON 스키마 기반의 정답지 설계</h4>
<p>자연어 텍스트는 본질적으로 모호하며 파싱하기 어렵습니다. 따라서 정답지의 기대 출력(Expected Output)은 가능한 한 **구조화된 데이터 포맷(JSON, XML, YAML)**으로 정의되어야 합니다. 최근의 LLM들은 ‘Structured Output’ 또는 ’JSON Mode’를 통해 스키마를 준수하는 출력을 생성하는 능력이 비약적으로 향상되었습니다.</p>
<ul>
<li><strong>스키마 검증(Schema Validation):</strong> 정답지는 단순히 “값이 무엇인가“뿐만 아니라 “어떤 구조여야 하는가“를 정의하는 <strong>JSON Schema</strong>를 포함해야 합니다. 예를 들어, 날짜 필드는 ISO-8601 형식을 따르는지, 반환된 리스트는 비어있지 않은지, 특정 필드(<code>visitor_count</code>)가 정수형(<code>integer</code>)인지 등의 타입 및 제약 조건을 정답지에 명시합니다. 이는 내용 검증 이전에 형식 검증을 통해 기본적인 오류를 걸러내는 1차 방어선 역할을 합니다.</li>
<li><strong>파싱 가능성(Parsability):</strong> AI 모델에 <code>response_mime_type: application/json</code>을 강제하거나 제미나이의 함수 호출(Function Calling) 기능을 활용하여, 모델이 생성한 결과가 정답지와 1:1로 매핑될 수 있도록 해야 합니다. 이는 텍스트 파싱 에러나 포맷 불일치로 인한 테스트 실패(Flakiness)를 획기적으로 줄여주며, 정답지 데이터가 다운스트림 애플리케이션이나 다른 시스템(예: 데이터베이스)에 즉시 통합될 수 있는 **상호운용성(Interoperability)**을 보장합니다.</li>
</ul>
<h4>1.3.2  정규표현식과 결정론적 패턴 (Codified Oracles)</h4>
<p>완전한 JSON 구조화가 불가능한 자유 텍스트 생성 과제(예: 창의적 에세이 작성)의 경우라도, 정답지는 최소한의 **결정론적 패턴(Deterministic Patterns)**을 포함하여 기계적 검증이 가능하도록 설계되어야 합니다. 브래드쇼는 이를 **‘코드화된 오라클(Codified Oracles)’**이라고 칭했습니다.</p>
<ul>
<li><strong>필수 키워드 앵커링:</strong> 정답지에는 답변에 반드시 포함되어야 할 핵심 개념이나 용어(Key Phrases) 리스트가 <code>must_contain</code> 필드로 정의되어야 합니다.</li>
<li><strong>부정 패턴(Negative Patterns):</strong> 절대 포함되어서는 안 되는 내용(예: 환각, 민감 정보, 경쟁사 언급, 독성 콘텐츠)을 <code>must_not_contain</code> 필드로 정의하여, 안전성(Safety) 검증을 자동화해야 합니다. 이는 단순한 문자열 매칭을 넘어 정규표현식(Regex)을 활용하여 다양한 변형을 포착할 수 있어야 합니다.</li>
</ul>
<h3>1.4  제4원칙: 의미론적 등가성 (Semantic Equivalence)과 유연한 검증</h3>
<p>이 원칙은 전통적인 소프트웨어 테스팅과 AI 테스팅을 구분 짓는 가장 결정적인 지점입니다. 텍스트 생성 모델에서 정답지와 ’정확히 일치(Exact Match)’하는 텍스트를 요구하는 것은 불가능할 뿐더러 바람직하지도 않습니다. 이는 ’과적합(Overfitting)’된 모델을 선호하게 만들거나, 유효하지만 표현만 다른 정답을 오답으로 처리하는 문제를 야기합니다. 따라서 정답지 설계는 <strong>“표현은 다르지만 의미는 같은(Semantically Equivalent)”</strong> 답변을 정답으로 인정할 수 있는 유연한 로직을 내포해야 합니다.</p>
<h4>1.4.1  엄격한 논리와 유연한 표현의 이원화 (Strict Logic vs. Fuzzy Presentation)</h4>
<p>효과적인 정답지 설계를 위해서는 검증 대상을 데이터의 성격에 따라 두 가지 층위로 나누어 접근해야 합니다.</p>
<ol>
<li><strong>엄격한 레이어(Strict Layer):</strong> 코드 생성, SQL 쿼리 생성, 수학적 계산, 고유명사 추출 등 엄격한 논리가 지배하는 영역은 <strong>Exact Match</strong> 또는 **실행 결과의 일치(Execution Match)**를 요구해야 합니다. 예를 들어, SQL 생성 테스트에서 쿼리 텍스트 자체는 공백이나 대소문자 차이로 다를 수 있더라도(<code>SELECT * FROM table</code> vs <code>select * from table</code>), 그 실행 결과값(Result Set)은 완벽하게 일치해야 합니다. 이를 위해 정답지는 예상되는 실행 결과 데이터셋을 포함해야 합니다.</li>
<li><strong>유연한 레이어(Fuzzy Layer):</strong> 요약, 설명, 번역, 창작 등은 문자열 일치가 아닌 <strong>의미론적 유사성</strong>을 평가해야 합니다. 이를 위해 <strong>임베딩 벡터 유사도(Vector Similarity)</strong>(예: BERTScore)나, 더 나아가 <strong>LLM-as-a-Judge</strong> 방식을 사용하여 정답지와의 의미적 거리를 측정합니다. 여기서 정답지는 ’유일한 정답’이 아니라 ’참조 기준(Reference)’이 됩니다.</li>
</ol>
<p><img src="./3.3.0.0.0%20%EA%B2%B0%EC%A0%95%EB%A1%A0%EC%A0%81%20%EC%A0%95%EB%8B%B5%EC%A7%80%20%EC%84%A4%EA%B3%84%EC%9D%98%20%ED%95%B5%EC%8B%AC%20%EC%9B%90%EC%B9%99%20Design%20Principles.assets/image-20260218214125325.jpg" alt="image-20260218214125325" /></p>
<h4>1.4.2  LLM-as-a-Judge를 위한 평가 루브릭(Rubric) 내재화</h4>
<p>의미론적 평가를 자동화하고 주관성을 배제하기 위해, 정답지 데이터셋 자체에 **‘평가 루브릭(Evaluation Rubric)’**이 포함되어야 합니다. 단순히 정답 텍스트(<code>expected_output_text</code>)만 두는 것이 아니라, “이 답변이 정답으로 인정받기 위해 충족해야 할 구체적인 기준“을 자연어로 명시하여, 심판(Judge) 모델이 이를 기준으로 채점할 수 있게 합니다. 이는 평가의 일관성을 높이고 ’채점자 간 신뢰도(Inter-rater Reliability)’를 확보하는 핵심 기법입니다.</p>
<p><strong>정답지 데이터 구조 예시 (확장된 형태):</strong></p>
<pre><code class="language-JSON">{
  "question_id": "Q-1023",
  "input_prompt": "양자 컴퓨팅의 중첩 원리를 설명하시오.",
  "expected_output_text": "중첩은 큐비트가 동시에 0과 1의 상태를 가질 수 있는 현상입니다...",
  "evaluation_criteria": {
    "key_facts": ["0과 1의 동시 존재", "확률적 상태", "관측 시 붕괴"],
    "tone": "Academic and educational",
    "forbidden_concepts":,
    "rubric_reference": "Explain the concept clearly to a high school student level."
  },
  "strictness_level": "semantic_similarity_0.85"
}
</code></pre>
<p>이러한 설계는 LLM이 단순한 텍스트 비교를 넘어, 정답지에 명시된 ’의도’와 ’제약조건’을 기반으로 평가를 수행하도록 유도합니다.</p>
<h3>1.5  제5원칙: 순환적 개선과 인간 참여 (The Flywheel Effect &amp; Human-in-the-Loop)</h3>
<p>정답지는 한 번 만들고 끝나는 정적인 파일(Static Artifact)이 아닙니다. 모델이 발전하고 사용자의 사용 패턴이 변화함에 따라 정답지 또한 유기적으로 진화해야 합니다. AWS의 연구진은 이를 <strong>‘플라이휠(Flywheel)’</strong> 모델로 설명합니다.</p>
<ul>
<li><strong>엣지 케이스의 지속적 통합:</strong> 운영 환경에서 발견된 모델의 실패 사례(Failure Cases)나 사용자의 부정적 피드백 데이터를 정제하여 즉시 정답지(Golden Dataset)에 편입시켜야 합니다. 예를 들어, 사용자가 특정 질문에 대해 “답변이 너무 길다“고 피드백을 주었다면, 해당 케이스를 정답지에 추가하고 <code>evaluation_criteria</code>에 “간결성(Conciseness)” 조건을 강화하여 반영해야 합니다. 이를 통해 정답지는 모델의 약점을 지속적으로 보완하는 <strong>회귀 테스트(Regression Test)</strong> 스위트로 기능하게 됩니다.</li>
<li><strong>인간 전문가(SME)의 개입:</strong> 초기 정답지 생성에는 LLM을 활용할 수 있지만, 최종적인 검증과 품질 기준 수립에는 반드시 해당 도메인의 전문가(Subject Matter Experts)가 개입하는 <strong>Human-in-the-Loop</strong> 프로세스가 필요합니다. 전문가는 LLM이 놓칠 수 있는 미묘한 사실 관계 오류나 편향을 식별하고, 정답지의 ’참값(Ground Truth)’으로서의 권위를 부여합니다. 또한, 시간이 지남에 따라 변하는 사실(예: 선거 결과, 주가, 법률 개정)에 대해 정기적인 <strong>‘Fact Refresh’</strong> 프로세스를 두어 정답지가 낡은 정보(Stale Data)가 되지 않도록 생명주기를 관리해야 합니다.</li>
</ul>
<p>결론적으로, <strong>3.3 결정론적 정답지 설계</strong>는 모호하고 확률적인 AI의 출력을 명확한 엔지니어링 규격 안으로 수렴시키는 행위입니다. 재현 가능한 환경을 구축하고(제1원칙), 문제를 최소 단위로 쪼개며(제2원칙), 기계가 이해할 수 있는 구조로 정의하고(제3원칙), 유연하지만 원칙 있는 평가 기준을 세우며(제4원칙), 지속적으로 데이터를 개선함으로써(제5원칙), 우리는 비로소 제미나이와 같은 강력한 도구를 신뢰할 수 있는 엔터프라이즈 시스템의 부품으로 활용할 수 있게 됩니다.</p>
<h2>2. 참고 자료</h2>
<ol>
<li>Deterministic Functions in Oracle | USA | 99% Customer Retention, https://doyensys.com/blogs/deterministic-functions-in-oracle/</li>
<li>The Challenges of Testing in a Non-Deterministic World, https://www.sei.cmu.edu/blog/the-challenges-of-testing-in-a-non-deterministic-world/</li>
<li>DETERMINISTIC Clause - Oracle Help Center, https://docs.oracle.com/en/database/oracle/oracle-database/19/lnpls/DETERMINISTIC-clause.html</li>
<li>Test oracle - Grokipedia, https://grokipedia.com/page/Test_oracle</li>
<li>Test Oracle Tutorial: Definition, Types, and Applications - ZetCode, https://zetcode.com/terms-testing/test-oracle/</li>
<li>Challenges in Testing Large Language Model Based Software - arXiv, https://arxiv.org/html/2503.00481v1</li>
<li>The acceptance, use, and perceptions of metamorphic testing for a, https://www.tandfonline.com/doi/pdf/10.1080/23311916.2025.2522652</li>
<li>An Empirical Validation of Oracle Improvement, https://www.computer.org/csdl/journal/ts/2021/08/08794642/1jeDf0iyVnW</li>
<li>Intramorphic Testing: A New Approach to the Test Oracle Problem, https://www.research-collection.ethz.ch/server/api/core/bitstreams/30763e1f-803c-4e57-ae67-db38fc84cfc1/content</li>
<li>Golden Dataset: Role In Custom LLM Evals - Arize AI, https://arize.com/resource/golden-dataset/</li>
<li>Applying the SACRED Model to Build Reliable Automated Tests - Qt, https://www.qt.io/quality-assurance/blog/applying-the-sacred-model-to-build-reliable-automated-tests</li>
<li>Ground truth curation and metric interpretation best practices for …, https://aws.amazon.com/blogs/machine-learning/ground-truth-curation-and-metric-interpretation-best-practices-for-evaluating-generative-ai-question-answering-using-fmeval/</li>
<li>Enhancing RAG Systems Lessons from Doc Development at NetApp, https://www.netapp.com/media/115104-wp-7371-enhancing-rag-systems.pdf</li>
<li>Ground truth generation and review best practices for evaluating, https://aws.amazon.com/blogs/machine-learning/ground-truth-generation-and-review-best-practices-for-evaluating-generative-ai-question-answering-with-fmeval/</li>
<li>Trust at Scale: Regression Testing Multi-Agent Systems in … - Medium, https://medium.com/@bhargavaparv/trust-at-scale-regression-testing-multi-agent-systems-in-continuous-deployment-environments-99dfcc5872e9</li>
<li>Temporal Agents with Knowledge Graphs - OpenAI for developers, https://developers.openai.com/cookbook/examples/partners/temporal_agents_with_knowledge_graphs/temporal_agents/</li>
<li>The Role of Machine Readability in an AI World - SEC.gov, https://www.sec.gov/newsroom/speeches-statements/speech-bauguess-050318</li>
<li>Beyond FOMO: The Real Playbook for Winning AI Search and, https://medium.com/techacc/geo-playbook-4cfbf7f7d3fd</li>
<li>Semantic Resources for Managing Knowledge in Battery Research, https://pmc.ncbi.nlm.nih.gov/articles/PMC12330309/</li>
<li>Dataset Reuse: Translating Principles to Practice, https://papers.ssrn.com/sol3/Delivery.cfm/416a5366-c6b2-4db2-8723-23f4b316a7ba-MECA.pdf?abstractid=3589836</li>
<li>Structured output | Generative AI on Vertex AI, https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output</li>
<li>Structured outputs in LLMs: Definition, techniques, applications, https://www.leewayhertz.com/structured-outputs-in-llms/</li>
<li>NewDay builds A Generative AI based Customer service Agent, https://aws.amazon.com/blogs/machine-learning/newday-builds-a-generative-ai-based-customer-service-agent-assist-with-over-90-accuracy/</li>
<li>Spring AI Embraces OpenAI’s Structured Outputs: Enhancing JSON, https://spring.io/blog/2024/08/09/spring-ai-embraces-openais-structured-outputs-enhancing-json-response/</li>
<li>UAE issues 25 new AI guidelines for schools, prohibits use for under-13 students, https://timesofindia.indiatimes.com/world/middle-east/uae-issues-25-new-ai-guidelines-for-schools-prohibits-use-for-under-13-students/articleshow/128371457.cms</li>
<li>Ensuring Data Accuracy in Text-to-SQL Systems, <a href="https://www.ijcttjournal.org/Volume-72%20Issue-12/IJCTT-V72I12P103.pdf">https://www.ijcttjournal.org/Volume-72%20Issue-12/IJCTT-V72I12P103.pdf</a></li>
<li>LLM-as-a-Judge: automated evaluation of search query parsing, https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2025.1611389/full</li>
<li>The Quick Guide to Prompt Engineering, https://www.vailtech.net/sites/default/files/AI_Prompt_Engineering_for_ChatGPT.pdf</li>
<li>Leveraging Generative AI for Data Engineering Workflows - JCSTS, https://al-kindipublisher.com/index.php/jcsts/article/download/9310/7970/26043</li>
<li>Checklist for Evaluating Text-to-SQL Models in BI - Querio, https://querio.ai/articles/checklist-for-evaluating-text-to-sql-models-in-bi</li>
<li>A Practical Guide to Evaluating Large Language Models (LLM), https://medium.com/@thomas.zilliox/a-practical-guide-to-evaluating-large-language-models-llm-4882fb22892f</li>
<li>LLM evaluation metrics: A comprehensive guide for large language, https://wandb.ai/onlineinference/genai-research/reports/LLM-evaluation-metrics-A-comprehensive-guide-for-large-language-models–VmlldzoxMjU5ODA4NA</li>
<li>Define your evaluation metrics | Generative AI on Vertex AI, https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>