<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.1.1.1 LLM의 '다음 토큰 예측' 메커니즘과 불확실성</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.1.1.1 LLM의 '다음 토큰 예측' 메커니즘과 불확실성</h1>
                    <nav class="breadcrumbs"><a href="../../../../../index.html">Home</a> / <a href="../../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../../index.html">Chapter 3. 결정론적 정답지(Deterministic Ground Truth)의 설계 원칙과 필요성</a> / <a href="../index.html">3.1 결정론적 정답지(Deterministic Ground Truth)의 정의와 본질</a> / <a href="index.html">3.1.1 확률적 AI와 결정론적 비즈니스 요구사항의 충돌</a> / <span>3.1.1.1 LLM의 '다음 토큰 예측' 메커니즘과 불확실성</span></nav>
                </div>
            </header>
            <article>
                <h1>3.1.1.1 LLM의 ‘다음 토큰 예측’ 메커니즘과 불확실성</h1>
<p>거대언어모델(Large Language Model, 이하 LLM)을 엔터프라이즈 소프트웨어 개발 수명 주기(SDLC)에 통합하려는 시도가 직면하는 가장 본질적인 난관은 ’결정론적(Deterministic) 기대’와 ‘확률적(Probabilistic) 현실’ 사이의 괴리에서 발생한다. 전통적인 소프트웨어 엔지니어링은 입력 <span class="math math-inline">x</span>에 대해 항상 동일한 출력 <span class="math math-inline">y</span>를 보장하는 함수 <span class="math math-inline">f(x) = y</span>를 전제로 설계되었다. 그러나 LLM은 본질적으로 조건부 확률 분포 <span class="math math-inline">P(y \vert x)</span>를 모델링하며, 그 출력은 매번 달라질 수 있는 확률적 과정(Stochastic Process)의 산물이다.</p>
<p>이러한 확률적 특성은 창의적인 작문이나 아이디어 발상에는 유리하게 작용하지만, 엄격한 논리적 정합성, 멱등성(Idempotency), 그리고 재현성(Reproducibility)을 요구하는 미션 크리티컬 시스템이나 소프트웨어 테스트 오라클(Oracle) 구축에는 심각한 장애물이 된다. 본 절에서는 LLM의 근본적인 작동 원리인 ‘다음 토큰 예측(Next Token Prediction)’ 메커니즘을 수학적, 알고리즘적, 그리고 하드웨어 아키텍처 관점에서 심층 분석한다. 이를 통해 불확실성이 발생하는 다층적인 원인을 규명하고, 단순한 파라미터 조작만으로는 해결할 수 없는 비결정성의 심연을 파헤친다.</p>
<h2>1.  예측의 수학적 기원: 로짓(Logits)에서 확률 분포까지</h2>
<p>LLM의 모든 고도화된 능력—복잡한 추론, 코드 생성, 요약, 번역 등—은 결국 “주어진 문맥 뒤에 올 가장 그럴듯한 단어(토큰)는 무엇인가?“라는 단 하나의 질문을 반복적으로 해결하는 자기회귀(Auto-regressive) 과정으로 귀결된다. 이를 수학적으로 표현하면, 주어진 토큰 시퀀스 <span class="math math-inline">w_{1:t-1} = (w_1, w_2,..., w_{t-1})</span>가 있을 때, 시점 <span class="math math-inline">t</span>에서 특정 토큰 <span class="math math-inline">w_t</span>가 등장할 조건부 확률 <span class="math math-inline">P(w_t \vert w_{1:t-1})</span>를 추정하는 문제이다.</p>
<h3>1.1  정보의 압축과 로짓(Logits)의 생성</h3>
<p>LLM의 내부 연산은 입력된 텍스트를 토큰화(Tokenization)하여 고차원 임베딩 벡터로 변환하는 것에서 시작한다. 트랜스포머(Transformer) 아키텍처의 수십, 수백 개의 레이어를 거치며 이 벡터들은 문맥적 의미(Contextual Semantics)와 구문적 구조(Syntactic Structure)를 함축하게 된다. 이 과정은 정보 이론적 관점에서 보았을 때, 입력 데이터의 엔트로피를 점진적으로 정제하여 다음 토큰에 대한 불확실성을 줄여나가는 과정으로 해석할 수 있다.</p>
<p>마지막 레이어에서 모델은 ’언어 모델링 헤드(Language Modeling Head)’라 불리는 선형 변환을 통해 은닉 상태(Hidden State) 벡터를 어휘 사전(Vocabulary)의 크기인 <span class="math math-inline">V</span> 차원의 벡터 <span class="math math-inline">z</span>로 투영한다. 이 벡터 <span class="math math-inline">z</span>의 각 원소 <span class="math math-inline">z_i</span>를 **로짓(Logit)**이라 한다.</p>
<p>로짓은 모델이 해당 토큰 <span class="math math-inline">i</span>에 대해 부여한 정규화되지 않은 점수(Raw Score)이다. 로짓 값 자체는 실수 범위(<span class="math math-inline">-\infty, \infty</span>)를 가지며, 값이 클수록 모델이 해당 토큰을 다음 단어로 유력하게 지지한다는 것을 의미한다. 물리학적 비유를 들자면, 로짓은 통계 역학에서의 에너지 상태와 유사하며, 아직 확률로 변환되기 전의 잠재적 선호도를 나타낸다. 소프트웨어 개발자 입장에서 로짓은 ’모델의 확신도’를 나타내는 1차적인 지표이며, 결정론적 정답지를 설계할 때 최종 출력 텍스트보다 훨씬 더 풍부한 진단 정보를 제공한다.</p>
<h3>1.2  소프트맥스(Softmax) 변환과 온도의 개입</h3>
<p>로짓 벡터 <span class="math math-inline">z</span>를 확률 분포 <span class="math math-inline">p</span>로 변환하기 위해 <strong>소프트맥스(Softmax)</strong> 함수가 사용된다. 이는 LLM의 불확실성이 구체화되는 첫 번째 관문이자, 우리가 파라미터를 통해 개입할 수 있는 지점이다. <span class="math math-inline">i</span>번째 토큰이 선택될 확률 <span class="math math-inline">P_i</span>는 다음과 같이 정의된다.<br />
<span class="math math-display">
P(w_t = i \vert w_{1:t-1}) = \frac{e^{z_i / T}}{\sum_{j=1}^{V} e^{z_j / T}}
</span><br />
여기서 <span class="math math-inline">T</span>는 <strong>온도(Temperature)</strong> 파라미터이다. 이 수식은 볼츠만 분포(Boltzmann Distribution)에서 유래하였으며, <span class="math math-inline">T</span>는 시스템의 열역학적 온도로서 확률 분포의 형태(Shape)를 결정한다.</p>
<ul>
<li><strong><span class="math math-inline">T=1</span> (기본 상태):</strong> 로짓의 상대적 차이가 왜곡 없이 확률에 반영된다. 모델이 학습한 데이터 분포를 그대로 따르려는 성향을 보인다.</li>
<li><strong><span class="math math-inline">T &gt; 1</span> (높은 온도):</strong> 분모와 분자의 지수승 효과가 완화되면서 분포가 평탄화(Flattening)된다. 낮은 로짓 값을 가진 토큰들의 확률이 상대적으로 상승하여, 모델이 더 다양한(그러나 덜 정확할 수 있는) 단어를 선택할 가능성이 커진다. 이는 섀넌 엔트로피(Shannon Entropy)의 증가를 의미하며, 창의성을 높이지만 환각(Hallucination)의 위험도 함께 증가시킨다.</li>
<li><strong><span class="math math-inline">T &lt; 1</span> (낮은 온도):</strong> 분포가 뾰족해(Peaked)진다. 로짓 값이 높은 상위 토큰들의 확률이 극단적으로 증폭되고, 낮은 토큰들은 0에 수렴한다. <span class="math math-inline">T \to 0</span>에 가까워질수록 분포는 가장 높은 로짓을 가진 하나의 토큰에 모든 확률이 집중되는 델타 함수(Delta Function) 혹은 아그맥스(Argmax) 연산에 근사하게 된다.</li>
</ul>
<p>이 수식에서 알 수 있듯이, LLM은 본질적으로 단 하나의 정답을 내놓는 것이 아니라, 가능한 모든 단어에 대한 확률 분포를 출력한다. “대한민국의 수도는?“이라는 질문에 대해 “서울“의 확률이 99.9%일지라도, 0.1%의 확률로 “부산“이나 “평양“이 존재한다. 이러한 확률적 본질(Probabilistic Nature)이 바로 AI 기반 소프트웨어에서 발생하는 비결정성의 수학적 기원이다.</p>
<p><img src="./3.1.1.1.0%20LLM%EC%9D%98%20%EB%8B%A4%EC%9D%8C%20%ED%86%A0%ED%81%B0%20%EC%98%88%EC%B8%A1%20%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC%20%EB%B6%88%ED%99%95%EC%8B%A4%EC%84%B1.assets/image-20260218181335109.jpg" alt="image-20260218181335109" /></p>
<h3>1.3  엔트로피와 고차원 통계적 구조의 내재화</h3>
<p>소프트웨어 테스트에서 오라클이 모델의 응답을 신뢰할 수 있는지 판단하기 위해서는 <strong>엔트로피(Entropy)</strong> 개념이 필수적이다. 특정 시점 <span class="math math-inline">t</span>에서의 섀넌 엔트로피(Shannon Entropy) <span class="math math-inline">H_t</span>는 다음과 같이 정의된다.</p>
<p><span class="math math-display">H_t = - \sum_{j=1}^{V} P(w_{t,j}) \log P(w_{t,j})</span></p>
<ul>
<li><strong>낮은 엔트로피:</strong> 확률 분포가 특정 토큰에 집중되어 있음을 의미한다. 이는 모델이 다음에 올 단어에 대해 높은 확신을 가지고 있다는 뜻이다. (예: “1+1=” 뒤에 “2“가 올 확률).</li>
<li><strong>높은 엔트로피:</strong> 확률 분포가 여러 토큰에 넓게 퍼져 있음을 의미한다. 이는 모델이 불확실하거나, 선택지가 다양한 창의적인 분기점(Forking Point)에 있음을 시사한다.</li>
</ul>
<p>최신 연구 에 따르면, 소프트맥스 엔트로피의 큐물런트(Cumulants) 확장을 통해 모델이 학습 과정에서 고차원적인 통계적 구조를 어떻게 내재화하는지 분석할 수 있다. 큐물런트는 확률 분포의 형태를 설명하는 통계량으로, 1차 큐물런트는 평균, 2차는 분산, 3차는 왜도(Skewness), 4차는 첨도(Kurtosis)에 해당한다. 연구 결과, 모델의 레이어가 깊어질수록 큐물런트의 고차항들이 정교해지며 다음 토큰 예측을 다듬어 나가는 것이 확인되었다.</p>
<p>실무적으로 엔트로피가 급격히 높아지는 지점은 모델이 논리적 분기에 도달했거나, 혹은 학습 데이터 부족으로 인해 ’찍기(Guessing)’를 시도하는 구간일 가능성이 높다. 따라서 결정론적 정답지를 구축할 때, 단순히 결과값만 비교하는 것이 아니라 <strong>토큰 단위의 엔트로피를 모니터링하여 임계값 이상일 경우 테스트를 보류하거나 경고를 띄우는 메커니즘</strong>이 필요하다. 이는 오라클이 단순히 “맞았다/틀렸다“를 넘어 “확신한다/모른다“를 구분할 수 있게 해 준다.</p>
<h2>2. 디코딩 전략: 확률을 선택으로 바꾸는 알고리즘과 그 한계</h2>
<p>소프트맥스가 모든 단어에 대한 확률 분포를 제공한다면, 실제 다음 토큰을 하나 선택하여 텍스트를 생성하는 것은 **디코딩 전략(Decoding Strategy)**의 몫이다. AI 소프트웨어 개발자가 마주하는 비결정성의 상당 부분은 이 디코딩 단계의 알고리즘적 선택에서 기인한다.</p>
<h3>2.1 결정론적 디코딩 (Deterministic Decoding)과 텍스트 퇴화</h3>
<p>가장 직관적인 방법은 **그리디 서치(Greedy Search)**이다. 이는 매 단계에서 가장 확률이 높은 단 하나의 토큰만을 선택한다 (<span class="math math-inline">w_t = \arg\max P(w_t \vert w_{1:t-1})</span>). 이론적으로는 입력이 같으면 출력도 같아야 하므로, 회귀 테스트(Regression Testing)나 오라클 구축에 가장 적합한 설정이다.</p>
<p>그러나 그리디 서치는 심각한 단점을 내포하고 있다. 매 순간의 최선만을 선택하는 탐욕적 접근은 ’지역적 최적해(Local Optimum)’에 갇힐 위험이 크다. 이는 전체 문맥을 고려했을 때 최적의 문장이 아닐 수 있으며, 결과적으로 문장이 반복되거나 부자연스러워지는 <strong>텍스트 퇴화(Text Degeneration)</strong> 현상을 유발한다. 또한, 뒤에서 다룰 하드웨어적 비결정성 요인으로 인해 그리디 서치조차 100% 재현성을 보장하지 못하는 경우가 빈번하다. 빔 서치(Beam Search)는 여러 개의 후보 시퀀스(Beam)를 유지하며 이를 완화하려 하지만, 근본적인 결정론적 한계를 완전히 극복하지는 못한다.</p>
<h3>2.2 확률적 디코딩 (Stochastic Decoding)과 제어된 무작위성</h3>
<p>대부분의 실무 환경(챗봇, 창작 도구 등)에서는 다양성과 자연스러움을 위해 확률적 샘플링을 사용한다. 이때 전체 어휘 사전에서 무작위로 추출하는 것은 텍스트의 품질을 심각하게 저하시키므로(꼬리 부분의 낮은 확률 단어가 선택될 위험), 샘플링 공간을 제한하는 전략이 필수적이다.</p>
<h4>2.2.1 Top-k 샘플링: 고정된 후보군의 한계</h4>
<p>확률이 높은 상위 <span class="math math-inline">k</span>개의 토큰만을 후보군으로 남기고, 나머지는 제거한 뒤 다시 정규화하여 샘플링한다.</p>
<ul>
<li><strong>장점:</strong> 말도 안 되는 단어(Long Tail)를 배제하여 텍스트의 안정성을 높인다.</li>
<li><strong>단점:</strong> <span class="math math-inline">k</span>가 고정되어 있어(예: <span class="math math-inline">k=50</span>), 모델의 확신도에 따라 유동적으로 대응하지 못한다. 확신이 높은 상황(정답이 1개뿐인 경우)에서도 강제로 50개를 고려해야 하여 엉뚱한 단어를 선택할 수 있고, 반대로 확신이 낮은 상황(다양한 표현이 가능한 경우)에서는 50개 밖의 유효한 후보를 놓칠 수 있다.</li>
</ul>
<h4>2.2.2 Top-p (Nucleus) 샘플링: 동적 확률 제어</h4>
<p>누적 확률(Cumulative Probability)이 <span class="math math-inline">p</span> (예: 0.9)가 될 때까지 상위 토큰들을 포함시킨다.</p>
<ul>
<li><strong>메커니즘:</strong> <span class="math math-inline">V^{(p)} \subset V</span>는 <span class="math math-inline">\sum_{x \in V^{(p)}} P(x \vert w_{1:t-1}) \ge p</span>를 만족하는 최소 집합이다.</li>
<li><strong>특징:</strong> 모델의 확신이 높을 때는 후보군(Nucleus)이 작아지고(1~2개), 불확실할 때는 후보군이 넓어진다. 문맥에 따라 동적으로 후보군 크기를 조절하므로 Top-k보다 더 자연스러운 텍스트를 생성하며, 현재 LLM 서비스의 사실상 표준(De Facto Standard) 디코딩 방식으로 자리 잡았다.</li>
</ul>
<h4>2.2.3 Min-p 샘플링: 최신 기법과 논란</h4>
<p>2024-2025년경 주목받기 시작한 기법으로, 가장 확률이 높은 토큰의 확률(<span class="math math-inline">P_{max}</span>)에 비례하여 임계값을 설정한다.</p>
<ul>
<li><strong>조건:</strong> <span class="math math-inline">P(w) \ge \text{min\_p} \times P_{max}</span> 인 토큰만 후보로 남긴다.</li>
<li><strong>의의:</strong> Top-p가 분포가 평평할(Flat) 때 너무 많은 잡음 토큰을 포함하는 단점을 개선했다. 모델이 확신할 때는 매우 엄격하게, 헷갈릴 때는 적절히 관대하게 동작하여 “일관성(Coherence)“과 “창의성“의 균형을 잡는다. 테스트 오라클 관점에서는 Top-p보다 제어하기 용이한 특성을 보인다.</li>
<li><strong>논란:</strong> 최근 연구 에서는 Min-p가 기존 방식 대비 품질과 다양성 측면에서 우월하다는 주장에 대해 반박하며, 하이퍼파라미터 튜닝의 공정성 문제를 제기하기도 했다. 이는 최신 디코딩 기법조차 완벽한 해결책은 아니며, 오라클 설계 시 특정 알고리즘에 맹목적으로 의존해서는 안 됨을 시사한다.</li>
</ul>
<p><img src="./3.1.1.1.0%20LLM%EC%9D%98%20%EB%8B%A4%EC%9D%8C%20%ED%86%A0%ED%81%B0%20%EC%98%88%EC%B8%A1%20%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC%20%EB%B6%88%ED%99%95%EC%8B%A4%EC%84%B1.assets/image-20260218181407700.png" alt="image-20260218181407700" /></p>
<h2>3. 하드웨어 및 시스템 레벨의 비결정성 (Systemic Non-Determinism)</h2>
<p>많은 개발자들이 범하는 치명적인 오해는 “Temperature를 0으로 설정하면 LLM은 항상 같은 결과를 내놓을 것이다“라는 믿음이다. 그러나 실제 엔터프라이즈 환경, 특히 대규모 트래픽을 처리하는 GPU 클러스터 환경에서는 **Temperature=0 에서도 결과가 달라지는 현상(Output Drift)**이 빈번하게 발생한다. 이는 오라클 구축을 어렵게 만드는 가장 까다로운 요인이며, 소프트웨어 엔지니어가 반드시 이해해야 할 ’숨겨진 변수’들이다.</p>
<h3>3.1 부동소수점 연산의 비결합성 (Non-Associativity)</h3>
<p>컴퓨터의 부동소수점 연산(Floating Point Arithmetic)은 수학적으로 결합법칙이 성립하지 않는다. 즉, 실수 체계에서는 <span class="math math-inline">(a + b) + c = a + (b + c)</span>가 참이지만, 컴퓨터 연산에서는 미세한 정밀도 차이로 인해 <span class="math math-inline">(a + b) + c \neq a + (b + c)</span>가 될 수 있다.</p>
<p>GPU는 수천 개의 코어를 사용하여 병렬로 행렬 곱셈(Matrix Multiplication)과 누적(Accumulation) 연산을 수행한다. 이때 **병렬 스레드의 실행 순서(Scheduling)**는 운영체제, GPU의 온도, 메모리 대역폭, 동시 실행 중인 다른 작업의 부하 등에 따라 매번 미세하게 달라진다. 특히 행렬 곱셈의 결과값을 합산하는 리덕션(Reduction) 과정에서 ‘원자적 덧셈(Atomic Add)’ 연산이 사용되는데, 이 연산의 순서가 바뀔 때마다 라운딩 에러(Rounding Error)가 다르게 누적된다.</p>
<p>이러한 미세한 오차는 수십 개의 트랜스포머 레이어를 거치며 나비 효과처럼 증폭된다. 결국 마지막 레이어의 로짓 값에 영향을 주어, 1위와 2위 토큰의 확률 차이가 근소할 경우(예: 로짓 값이 10.00001과 10.00002로 경합할 때) 선택되는 토큰이 뒤바뀌는 결과(<span class="math math-inline">Argmax</span> Flipping)를 초래한다.</p>
<h3>3.2 배치(Batch) 처리와 전문가 혼합(MoE) 라우팅의 변수</h3>
<p>현대의 효율적인 추론 서버(vLLM, TGI 등)는 처리량을 높이기 위해 여러 사용자의 요청을 하나의 **배치(Batch)**로 묶어 처리한다. 여기서 두 가지 치명적인 비결정성 요인이 발생한다.</p>
<ol>
<li><strong>배치 불변성(Batch Invariance)의 파괴:</strong> 내가 보낸 요청 A가 다른 사용자의 요청 B, C와 함께 처리될 때와, D, E와 함께 처리될 때, GPU 내부의 연산 커널(Kernel) 최적화 전략이 달라질 수 있다. 예를 들어, 배치 크기가 크면 ‘Data-Parallel’ 전략을 사용하여 각 코어가 독립적인 데이터를 처리하지만, 배치가 작으면 ‘Split-Reduction’ 전략을 사용하여 여러 코어가 하나의 데이터를 쪼개어 처리한다. 최적화 전략의 변화는 연산 순서의 변화를 의미하며, 앞서 언급한 부동소수점 오차를 유발한다. 즉, <strong>나의 입력이 동일해도, 동시간대 접속한 타인의 요청량에 따라 나의 결과값이 달라질 수 있다.</strong></li>
<li><strong>MoE(Mixture of Experts)의 라우팅 비결정성:</strong> GPT-4나 Mixtral 같은 MoE 모델은 토큰을 처리할 전문가(Expert) 네트워크를 동적으로 선택한다. 이때 특정 전문가에게 부하가 몰리는 것을 방지하기 위해 로드 밸런싱(Load Balancing) 로직이 개입하는데, 이는 배치 내의 다른 토큰들의 분포에 의존한다. 만약 같은 배치 내의 다른 요청들이 특정 전문가를 많이 점유하고 있다면, 나의 토큰은 차순위 전문가에게 라우팅될 수 있다. 이는 API를 사용하는 클라이언트 입장에서 통제 불가능한 외부 변수이며, 재현 불가능한 오류의 주범이 된다.</li>
</ol>
<p><img src="./3.1.1.1.0%20LLM%EC%9D%98%20%EB%8B%A4%EC%9D%8C%20%ED%86%A0%ED%81%B0%20%EC%98%88%EC%B8%A1%20%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC%20%EB%B6%88%ED%99%95%EC%8B%A4%EC%84%B1.assets/image-20260218181426063.png" alt="image-20260218181426063" /></p>
<h2>4. ’그럴듯함(Plausibility)’과 ’진실(Truth)’의 괴리: 인식론적 불확실성</h2>
<p>마지막으로, LLM의 다음 토큰 예측 메커니즘은 본질적으로 “진실을 말하라“는 명령이 아니라 “학습 데이터 분포상 가장 확률이 높은 단어를 내뱉어라“는 명령을 수행한다는 점을 기억해야 한다. 이는 모델이 생성하는 정보의 신뢰성과 직결되는 **인식론적 불확실성(Epistemic Uncertainty)**의 문제이다.</p>
<h3>4.1 유효성 분류기(Is-It-Valid Classifier)로서의 한계</h3>
<p>OpenAI의 연구  및 관련 이론 에 따르면, LLM은 불확실한 상황에서 “모른다“고 답하기보다, **그럴듯한 오답(Plausible Falsehood)**을 생성하도록 학습 과정에서 강화된다. 생성 모델의 오류율은 모델 내부의 ’유효성 판별 능력(Is-It-Valid classification error)’과 수학적으로 연결되어 있다. 생성적 오류율은 판별 오류율의 약 2배 이상이라는 이론적 하한선이 존재한다.</p>
<p>즉, 모델 스스로 자신이 생성한 문장이 맞는지 틀린지 판별하는 능력이 완벽하지 않다면, 필연적으로 환각이 발생한다. 다음 토큰 예측 훈련은 모델에게 ’정답을 맞히는 것’뿐만 아니라 ’정답처럼 보이는 패턴을 모사하는 것’을 동시에 가르치기 때문이다. 특히 학습 데이터에 단 한 번만 등장한 사실(Singleton Fact)에 대해서는 모델의 불확실성이 극대화되며, 이때 모델은 기억(Memory)에 의존하기보다 일반적인 언어 패턴에 의존하여 빈칸을 채우려는 경향을 보인다.</p>
<h3>4.2 오라클 설계에 주는 시사점</h3>
<p>이러한 메커니즘적 특성은 결정론적 정답지 설계에 다음과 같은 원칙을 제시한다.</p>
<ol>
<li><strong>정확한 문자열 매칭(Exact Matching)의 종말:</strong> Temperature=0 설정조차 시스템적 노이즈로 인해 출력이 흔들릴 수 있으므로, 단순 문자열 비교 테스트는 ’Flaky Test(깨지기 쉬운 테스트)’가 될 수밖에 없다.</li>
<li><strong>의미론적 유사도(Semantic Similarity) 검증:</strong> 오라클은 텍스트의 표면적 일치가 아닌, 임베딩 벡터 간의 코사인 유사도(Cosine Similarity)나, LLM-as-a-Judge 기법을 활용한 논리적 포함 관계를 검증해야 한다.</li>
<li><strong>불확실성 지표 활용:</strong> 테스트 통과 여부를 결정할 때, 응답 텍스트와 함께 **로그 확률(Log-probability)**이나 <strong>엔트로피</strong> 값을 메타데이터로 수집하여, 모델이 확신을 가지고 내놓은 정답인지 찍어서 맞춘 것인지를 구별해야 한다.</li>
</ol>
<p>결론적으로, LLM의 ‘다음 토큰 예측’ 메커니즘은 태생적으로 확률적이며, 이를 구현하는 하드웨어와 시스템 또한 비결정론적 특성을 가진다. 따라서 AI 소프트웨어 테스트를 위한 오라클은 이러한 불확실성을 ’제거’하려는 시도보다는, 이를 ’관리’하고 ’허용 범위(Tolerance)’를 설정하는 방향으로 설계되어야 한다. 다음 절에서는 이러한 특성을 고려하여 엔터프라이즈 시스템에서 요구하는 멱등성(Idempotency)과 재현성을 확보하기 위한 구체적인 전략을 다룬다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>Probing Geometry of Next Token Prediction Using Cumulant … - arXiv, 2월 18, 2026에 액세스, https://arxiv.org/html/2510.04285v1</li>
<li>Transformer Next-Token Prediction - Emergent Mind, 2월 18, 2026에 액세스, https://www.emergentmind.com/topics/transformer-based-next-token-prediction-architectures</li>
<li>interpreting GPT: the logit lens - LessWrong, 2월 18, 2026에 액세스, https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens</li>
<li>How LLMs Choose Their Words: A Practical Walk-Through of Logits …, 2월 18, 2026에 액세스, https://machinelearningmastery.com/how-llms-choose-their-words-a-practical-walk-through-of-logits-softmax-and-sampling/</li>
<li>From Logits to Tokens. Introduction | by Aditya Modi | Medium, 2월 18, 2026에 액세스, https://medium.com/@adimodi96/from-logits-to-tokens-9a36feab9cab</li>
<li>From Randomness to Precision: How Top-k, Top-p, Temperature, 2월 18, 2026에 액세스, https://medium.com/@ansilproabl/from-randomness-to-precision-how-top-k-top-p-temperature-and-beam-search-shape-text-generation-d1f50b5220e2</li>
<li>Token-Level Entropy Gating in LLMs - Emergent Mind, 2월 18, 2026에 액세스, https://www.emergentmind.com/topics/token-level-entropy-gating</li>
<li>(PDF) Probing Geometry of Next Token Prediction Using Cumulant, 2월 18, 2026에 액세스, https://www.researchgate.net/publication/396249519_Probing_Geometry_of_Next_Token_Prediction_Using_Cumulant_Expansion_of_the_Softmax_Entropy</li>
<li>Probing Geometry of Next Token Prediction Using Cumulant, 2월 18, 2026에 액세스, https://openreview.net/pdf/755c1242edccbf78ea2563085a35e18bb9dce521.pdf</li>
<li>Decoding Strategies: How LLMs Choose The Next Word - AssemblyAI, 2월 18, 2026에 액세스, https://www.assemblyai.com/blog/decoding-strategies-how-llms-choose-the-next-word</li>
<li>THE CURIOUS CASE OF NEURAL TEXT DeGENERATION, 2월 18, 2026에 액세스, https://openreview.net/pdf?id=rygGQyrFvH</li>
<li>The curious case of neural text degeneration - CEUR-WS.org, 2월 18, 2026에 액세스, https://ceur-ws.org/Vol-2540/FAIR2019_paper_15.pdf</li>
<li>Generation configurations: temperature, top-k, top-p, and test time, 2월 18, 2026에 액세스, https://huyenchip.com/2024/01/16/sampling.html</li>
<li>Top-p sampling - Wikipedia, 2월 18, 2026에 액세스, https://en.wikipedia.org/wiki/Top-p_sampling</li>
<li>The Curious Case of Neural Text Degeneration - OpenReview, 2월 18, 2026에 액세스, https://openreview.net/forum?id=rygGQyrFvH</li>
<li>ICLR 2025 Orals, 2월 18, 2026에 액세스, https://iclr.cc/virtual/2025/events/oral</li>
<li>A <span class="math math-inline">\texttt{Min-p}</span> Blueprint for More Rigorous Science in Empirical, 2월 18, 2026에 액세스, https://openreview.net/forum?id=c2ozZYoZFd</li>
<li>Min-p sampling for LLMs - Thoughtworks - Medium, 2월 18, 2026에 액세스, https://thoughtworks.medium.com/min-p-sampling-for-llms-cf1655928796</li>
<li>A Critical Analysis of Min-p Sampling in Language Models - arXiv, 2월 18, 2026에 액세스, https://arxiv.org/html/2506.13681v1</li>
<li>(PDF) Turning Down the Heat: A Critical Analysis of Min-p Sampling, 2월 18, 2026에 액세스, https://www.researchgate.net/publication/392735665_Turning_Down_the_Heat_A_Critical_Analysis_of_Min-p_Sampling_in_Language_Models</li>
<li>LLM Output Drift: Cross-Provider Validation &amp; Mitigation for Financial, 2월 18, 2026에 액세스, https://arxiv.org/html/2511.07585v1</li>
<li>Non-Determinism of “Deterministic” LLM Settings - arXiv, 2월 18, 2026에 액세스, https://arxiv.org/html/2408.04667v5</li>
<li>Defeating Non-Determinism in LLMs: Solving AI’s Reproducibility, 2월 18, 2026에 액세스, https://www.flowhunt.io/blog/defeating-non-determinism-in-llms/</li>
<li>The Real Reason for LLM Inference Nondeterminism | StartupHub.ai, 2월 18, 2026에 액세스, https://www.startuphub.ai/ai-news/ai-research/2025/the-real-reason-for-llm-inference-nondeterminism</li>
<li>Achieving Consistency and Reproducibility in Large Language …, 2월 18, 2026에 액세스, https://pub.aimind.so/creating-deterministic-consistent-and-reproducible-text-in-llms-e589ba230d44</li>
<li>Defeating Nondeterminism in LLM Inference - Thinking Machines Lab, 2월 18, 2026에 액세스, https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/</li>
<li>Why Language Models Hallucinate - OpenAI, 2월 18, 2026에 액세스, https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>