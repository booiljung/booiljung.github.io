<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.1 결정론적 정답지(Deterministic Ground Truth)의 정의와 본질</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.1 결정론적 정답지(Deterministic Ground Truth)의 정의와 본질</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 3. 결정론적 정답지(Deterministic Ground Truth)의 설계 원칙과 필요성</a> / <a href="index.html">3.1 결정론적 정답지(Deterministic Ground Truth)의 정의와 본질</a> / <span>3.1 결정론적 정답지(Deterministic Ground Truth)의 정의와 본질</span></nav>
                </div>
            </header>
            <article>
                <h1>3.1 결정론적 정답지(Deterministic Ground Truth)의 정의와 본질</h1>
<h2>1.  서론: 엔지니어링 패러다임의 전환과 진실의 위기</h2>
<h3>1.1  불변의 세계에서 확률의 세계로</h3>
<p>현대 소프트웨어 엔지니어링의 역사는 불확실성(Uncertainty)을 제거하고 통제 가능한 결정론적(Deterministic) 시스템을 구축하려는 투쟁의 기록이었다. 폰 노이만 아키텍처(Von Neumann Architecture)의 태동 이래, 컴퓨터 과학의 근간을 이루는 대전제는 “동일한 입력은 반드시 동일한 출력을 보장해야 한다“는 인과율(Causality)이었다. 우리가 작성하는 코드는 명시적인 논리 회로와 명령어 집합 위에서 작동하며, <span class="math math-inline">2+2</span>는 우주가 멸망할 때까지 <span class="math math-inline">4</span>여야 하고, <code>print("Hello World")</code>는 언제나 화면에 동일한 문자열을 출력해야 한다. 이 세계에서 ’정답(Truth)’은 타협할 수 없는 상수이며, 시스템의 무결성을 지탱하는 유일한 버팀목이다.</p>
<p>그러나 2020년대 들어 제미나이(Gemini), GPT-4와 같은 대규모 언어 모델(LLM)이 소프트웨어 개발의 핵심 도구로 부상하면서, 이 견고했던 결정론적 세계관은 심각한 도전에 직면하게 되었다. 생성형 AI는 본질적으로 확률론적(Probabilistic) 엔진이다. 이들은 정해진 답을 ’인출(Retrieve)’하는 것이 아니라, 학습된 데이터의 확률 분포(Probability Distribution)를 기반으로 가장 그럴듯한 답을 ’생성(Generate)’한다. 따라서 동일한 프롬프트(Prompt)를 입력하더라도 시공간적 맥락, 무작위성 매개변수(Temperature), 모델의 미세한 상태 변화에 따라 매번 다른 결과물을 내놓을 수 있다.</p>
<p>이러한 비결정론적(Non-deterministic) 특성은 제미나이를 사용하여 기술 서적을 집필하거나 코드를 생성하려는 저자에게 심오한 존재론적 질문을 던진다. “수천 가지의 가능한 답변 중 무엇을 ’정답’으로 정의할 것인가?” 전통적인 비트 단위의 일치(Exact Match)는 더 이상 유효하지 않으며, 정답은 단일한 점(Point)이 아닌 허용 가능한 해의 분포(Distribution) 혹은 공간(Space)으로 확장되어야 한다. 이 챕터는 이러한 혼돈 속에서, 여전히 유효하고 필수불가결한 ’결정론적 정답지(Deterministic Ground Truth)’의 정의를 재확립하고, 확률론적 생성 결과물 속에서 어떻게 결정론적 검증의 닻을 내릴 수 있는지 그 본질을 심도 있게 탐구한다.</p>
<h3>1.2  서적 집필 과정에서의 ’정답’의 중요성</h3>
<p>특히 제미나이를 ‘공저자’ 또는 ’코드 생성기’로 활용하여 서적을 집필하는 경우, 정답지의 정의는 독자의 신뢰와 직결되는 문제다. 독자는 책에 실린 예제 코드가 실행 가능하며, 설명된 원리에 따라 정확하게 작동할 것이라 기대한다. 만약 AI가 확률적으로 생성한 코드가 논리적으로는 그럴듯해 보이나 실제로는 컴파일되지 않거나 엣지 케이스(Edge Case)에서 실패한다면, 그 책의 권위는 실추된다. 따라서 저자는 AI의 유창함(Fluency)에 매몰되지 않고, AI가 생성한 결과물을 가차 없이 검증할 수 있는 ’절대적 기준’을 가지고 있어야 한다. 이 기준이 바로 본 장에서 논의할 결정론적 정답지이다. 우리는 ISO/IEC/IEEE 표준이 정의하는 테스트 오라클(Test Oracle)의 고전적 의미부터, 기계 학습에서의 라벨링 데이터가 갖는 한계, 그리고 현대적 코드 생성 평가에서의 기능적 정확성(Functional Correctness)에 이르기까지 정답의 계보를 추적할 것이다.</p>
<p><img src="./3.1.0.0.0%20%EA%B2%B0%EC%A0%95%EB%A1%A0%EC%A0%81%20%EC%A0%95%EB%8B%B5%EC%A7%80Deterministic%20Ground%20Truth%EC%9D%98%20%EC%A0%95%EC%9D%98%EC%99%80%20%EB%B3%B8%EC%A7%88.assets/image-20260218123950094.jpg" alt="image-20260218123950094" /></p>
<h2>2.  정답지(Ground Truth)의 계보학적 탐구와 정의</h2>
<p>’Ground Truth’라는 용어는 오늘날 인공지능 분야에서 가장 빈번하게 사용되지만, 그 뿌리는 훨씬 더 물리적이고 경험적인 세계에 닿아 있다. 이 용어의 기원과 변천사를 이해하는 것은, 우리가 소프트웨어 엔지니어링에서 추구해야 할 ’참값’의 본질을 파악하는 데 필수적이다.</p>
<h3>2.1  원격 탐사(Remote Sensing)에서의 물리적 실재성</h3>
<p>’Ground Truth’라는 용어의 기원은 컴퓨터 과학이 아닌 원격 탐사(Remote Sensing)와 기상학, 지질학 분야에 있다. 20세기 중반, 인공위성이나 항공기에서 지표면을 촬영한 데이터를 분석하던 연구자들은 근본적인 문제에 부딪혔다. 위성 센서가 감지한 픽셀의 스펙트럼 데이터가 실제로 지상의 무엇을 의미하는지 확신할 수 없었기 때문이다. 예를 들어, 위성 이미지 상의 특정 녹색 픽셀이 침엽수림인지, 활엽수림인지, 아니면 잘 관리된 골프장 잔디인지 구분하는 것은 원격 데이터만으로는 불가능했다.</p>
<p>이때 연구자가 직접 현장(Ground)에 나가 물리적으로 토양을 채취하고, 나무의 종류를 확인하고, 온도를 측정하여 얻은 데이터를 ’Ground Truth’라고 불렀다. 여기서 정답지는 **경험적 증거(Empirical Evidence)**이자 **직접 관찰(Direct Observation)**의 산물이다. 위성 이미지가 “숲일 확률이 80%“라고 추론(Inference)할 때, 연구자가 숲에 가서 나무를 만져보고 확인하는 행위 자체가 결정론적 검증이다. 이는 추론과 실재(Reality) 사이의 간극을 메우는 절대적 기준점(Anchor)으로서 기능한다. 즉, 초기 정의에서의 Ground Truth는 데이터가 아니라 <strong>물리적 실재(Physical Reality)</strong> 그 자체였다.</p>
<h3>2.2  데이터 과학과 기계 학습: 라벨(Label)로서의 정답</h3>
<p>물리적 세계에서 디지털 세계로 넘어오면서, Ground Truth의 의미는 ’물리적 실재’에서 ’인간의 판단’으로 이동했다. 기계 학습, 특히 지도 학습(Supervised Learning) 패러다임에서 Ground Truth는 훈련 데이터셋에 부착된 **라벨(Label)**을 의미한다. 이미지 분류 모델에게 “이 사진은 고양이“라고 알려주는 라벨이 곧 진실(Truth)이 된다.</p>
<p>그러나 이 단계에서 진실의 절대성은 희석되기 시작한다. 물리적 숲은 객관적으로 존재하지만, “이 사진의 고양이가 귀여운가?” 또는 “이 뉴스 기사는 정치적으로 편향되었는가?“와 같은 질문에 대한 라벨링은 고도의 **주관성(Subjectivity)**을 내포한다. 인간 라벨러(Annotator)들 사이의 의견 불일치(Disagreement)는 피할 수 없는 현상이며, 이를 측정하기 위해 코헨의 카파(Cohen’s Kappa)와 같은 통계적 지표가 사용된다. 이는 정답이 더 이상 하나의 확고한 사실(Fact)이 아니라, <strong>사회적 합의(Consensus)나 통계적 다수결</strong>의 결과물로 변질되었음을 시사한다.</p>
<p>제미나이와 같은 LLM을 훈련시키는 데이터셋 역시 이러한 ’합의된 진실’의 거대한 집합체이다. 인터넷 상의 텍스트 데이터는 팩트와 의견, 진실과 거짓이 혼재되어 있으며, 모델은 이를 구별하기보다는 통계적 패턴을 학습한다. 따라서 생성형 AI의 출력은 ’절대적 진실’이라기보다는 ’학습 데이터의 통계적 평균’에 가깝다.</p>
<h3>2.3  소프트웨어 엔지니어링: 명세(Specification)의 절대성</h3>
<p>반면, 전통적인 소프트웨어 엔지니어링은 기계 학습과는 다른 궤적을 그리며 독자적인 정답지의 정의를 구축해왔다. 여기서 정답지는 물리적 실재도, 통계적 합의도 아닌 **명세(Specification)와 논리(Logic)**다.</p>
<p>소프트웨어 테스팅 분야에서 정답지는 **테스트 오라클(Test Oracle)**이라는 개념으로 구체화된다. 테스트 오라클은 시스템의 실행 결과가 올바른지 판단하는 메커니즘을 통칭하며, 결정론적 정답지는 이 오라클이 참조하는 ’참값’을 의미한다. 국제 표준인 <strong>ISO/IEC/IEEE 29119</strong> 시리즈는 소프트웨어 테스팅의 개념과 프로세스를 정의하며, 여기서 정답지의 본질적 속성을 다음과 같이 규정한다.</p>
<ol>
<li><strong>비교 가능성(Comparability):</strong> 예상 결과(Expected Result)와 실제 결과(Actual Result)는 명확히 비교 가능해야 한다. 이는 <span class="math math-inline">Expected == Actual</span>이라는 명제 논리가 성립함을 의미한다. 부동소수점 연산의 미세한 오차를 제외하면, 소프트웨어의 정답은 모호함이 없어야 한다.</li>
<li><strong>독립성(Independence):</strong> 정답지는 테스트 대상 시스템(SUT, System Under Test)과 독립적으로 도출되어야 한다. SUT가 생성한 값을 그대로 정답으로 간주하는 것은 ‘Gold Master’ 방식의 회귀 테스트에서는 유용할 수 있으나, 근본적인 정답지 정의에는 부합하지 않는다. 진정한 오라클은 SUT의 내부 로직에 의존하지 않고 외부의 명세로부터 도출되어야 한다.</li>
<li><strong>명세 기반(Specification-Based):</strong> 결정론적 정답의 원천은 주로 요구사항 명세서, 설계 문서, 또는 관련 법규 및 표준이다. 즉, “소프트웨어가 무엇을 해야 하는가?“에 대한 합의된 문서가 곧 진실의 원천이 된다.</li>
</ol>
<p>제미나이를 활용한 서적 집필 프로젝트는 이 세 가지 정의가 충돌하는 지점에 있다. 우리는 원격 탐사처럼 팩트를 검증해야 하고(RAG), 기계 학습처럼 모호한 문맥을 다뤄야 하며(프롬프트 엔지니어링), 소프트웨어 공학처럼 코드가 정확히 실행되는지(테스트 오라클) 확인해야 한다. 이 복합적인 상황을 타개하기 위해서는 각 도메인의 정답지 개념을 융합한 새로운 정의가 필요하다.</p>
<p><img src="./3.1.0.0.0%20%EA%B2%B0%EC%A0%95%EB%A1%A0%EC%A0%81%20%EC%A0%95%EB%8B%B5%EC%A7%80Deterministic%20Ground%20Truth%EC%9D%98%20%EC%A0%95%EC%9D%98%EC%99%80%20%EB%B3%B8%EC%A7%88.assets/image-20260218124007086.jpg" alt="image-20260218124007086" /></p>
<h2>3.  결정론적 알고리즘과 예측 가능성의 미학</h2>
<p>결정론적 정답지가 유효하기 위한 기술적 전제 조건은 시스템이 **결정론적 알고리즘(Deterministic Algorithm)**에 기반해야 한다는 점이다. AI의 확률적 특성을 논하기 전에, 우리가 지키고자 하는 ’결정론’이 무엇인지 명확히 할 필요가 있다.</p>
<h3>3.1  상태 전이의 유일성</h3>
<p>컴퓨터 과학 이론에서 결정론적 알고리즘은 유한 상태 기계(Finite State Machine)로 모델링될 수 있다. 현재 상태 <span class="math math-inline">S_t</span>와 입력 <span class="math math-inline">I_t</span>가 주어졌을 때, 다음 상태 <span class="math math-inline">S_{t+1}</span>은 유일하게 결정된다. 즉, 전이 함수 <span class="math math-inline">\delta(S_t, I_t) \rightarrow S_{t+1}</span>은 일대일(One-to-One) 혹은 다대일(Many-to-One) 함수일 수 있으나, 일대다(One-to-Many) 함수일 수는 없다.</p>
<p>이러한 수학적 속성은 소프트웨어 테스팅에 강력한 이점을 제공한다.</p>
<ul>
<li><strong>재현성(Reproducibility):</strong> 버그가 발생했을 때, 동일한 입력과 환경을 제공하면 버그는 100% 재현된다. 이는 디버깅의 기본 전제다.</li>
<li><strong>회귀 테스트의 신뢰성:</strong> 코드를 수정하지 않았다면, 지난주에 통과한 테스트는 오늘도 통과해야 한다.</li>
</ul>
<h3>3.2  불변성과 객관성</h3>
<p>결정론적 환경에서 정답지는 **불변(Immutable)**하고 **객관적(Objective)**이다. 예를 들어, 정렬 알고리즘에 <code>를 입력했을 때 정답이 </code>이라는 사실은 관찰자나 시간에 따라 변하지 않는다. 이는 “진실은 발견되는 것이지, 만들어지는 것이 아니다“라는 플라톤적 세계관과 닿아 있다. 제미나이와 같은 AI 모델이 생성하는 텍스트나 코드의 가변성(Variability)은 이러한 불변성과 극명하게 대조되는 지점이며, 우리가 집필 중인 서적에서 “코드의 정확성“을 논할 때 반드시 짚고 넘어가야 할 기준점이다. AI가 아무리 뛰어난 코드를 생성하더라도, 그 코드가 결정론적 정답지(테스트 케이스)를 통과하지 못한다면 그것은 ‘틀린’ 것이다. 여기서 우리는 AI의 창의성을 존중하되, 그 결과물에 대해서는 결정론적 잣대를 들이대야 하는 이중적인 태도를 취하게 된다.</p>
<h2>4.  AI와 확률론적 모호성: 결정론의 붕괴</h2>
<h3>4.1  LLM의 작동 원리와 비결정론성</h3>
<p>제미나이와 같은 LLM은 근본적으로 ‘다음에 올 단어(Token) 맞추기’ 게임을 수행하는 확률 엔진이다. 트랜스포머(Transformer) 아키텍처는 입력된 컨텍스트를 바탕으로 가능한 모든 토큰의 확률 분포를 계산한다. 이때 모델의 출력은 결정론적으로 고정된 것이 아니라, **샘플링 전략(Sampling Strategy)**에 따라 결정된다.</p>
<ul>
<li><strong>Temperature:</strong> 이 값이 0에 가까우면 모델은 확률이 가장 높은 토큰만 선택(Greedy Decoding)하여 비교적 결정론적인 출력을 보이지만, 값이 높아질수록 확률 분포를 평탄화(Flattening)하여 낮은 확률의 토큰도 선택될 수 있게 한다. 이는 창의성을 높이지만 결과의 일관성을 해친다.</li>
<li><strong>Top-k / Top-p Sampling:</strong> 확률이 높은 상위 k개 또는 누적 확률 p에 해당하는 토큰들 중에서 무작위로 하나를 선택한다. 이 과정에서 본질적인 무작위성(Randomness)이 개입한다.</li>
</ul>
<p>이러한 메커니즘으로 인해 제미나이의 출력은 비결정론적이다. “피보나치 수열을 구하는 파이썬 함수를 작성해줘“라는 동일한 프롬프트에 대해, 한 번은 재귀(Recursion)로, 다음에는 반복문(Iteration)으로, 또 다음에는 동적 계획법(Dynamic Programming)으로 코드를 작성할 수 있다. 심지어 변수명이나 주석의 스타일은 매번 달라질 수 있다.</p>
<h3>4.2  1:1 대응의 부재와 평가의 난해함</h3>
<p>전통적인 정답지 개념인 ’정확한 일치(Exact Match)’는 이러한 비결정론적 환경에서 무력화된다.</p>
<ul>
<li><strong>참조 코드의 한계:</strong> 정답지(참조 코드)가 재귀 방식으로 작성되어 있다고 해서, 제미나이가 작성한 반복문 방식의 코드가 ’오답’인 것은 아니다. 두 코드는 문법적으로 완전히 다르지만(Syntactically Different), 의미적으로는 동일(Semantically Equivalent)할 수 있다.</li>
<li><strong>유사도 지표의 실패:</strong> BLEU나 ROUGE와 같은 n-gram 기반 텍스트 유사도 지표는 코드의 논리적 정확성을 전혀 반영하지 못한다. 변수명을 <code>count</code>에서 <code>cnt</code>로 바꾸기만 해도 점수가 하락할 수 있으며, 반대로 치명적인 논리 버그가 있는 코드가 정답 코드와 구조가 비슷하다는 이유로 높은 점수를 받을 수도 있다.</li>
</ul>
<p>따라서 제미나이 시대의 정답지는 “텍스트가 얼마나 비슷한가?“가 아니라 **“기능적으로 동일하게 작동하는가?”**를 묻는 방향으로 진화해야 한다.</p>
<h2>5.  코드 생성에서의 결정론적 정답: 기능적 정확성(Functional Correctness)</h2>
<p>서적 집필을 위해 제미나이가 생성한 코드를 검증할 때, 우리는 텍스트의 표면적 차이를 넘어 심층적인 논리의 정합성을 확인해야 한다. 이를 위한 현대적 표준이 바로 **기능적 정확성(Functional Correctness)**이다.</p>
<h3>5.1  정확한 일치(Exact Match)에서 기능적 정확성으로의 이동</h3>
<p>기능적 정확성은 코드의 텍스트가 아닌 **실행 결과(Execution Result)**를 기준으로 정답 여부를 판별한다. 이는 정답지를 **‘코드 텍스트’**에서 **‘입출력 테스트 케이스의 집합’**으로 재정의하는 것을 의미한다.</p>
<ul>
<li><strong>블랙박스 검증의 부활:</strong> 제미나이가 내부적으로 어떤 알고리즘을 사용했든, 어떤 변수명을 사용했든 상관없다. 주어진 입력 <span class="math math-inline">I</span>에 대해 정답과 동일한 출력 <span class="math math-inline">O</span>를 반환한다면, 그 코드는 ’정답’이다. 이는 소프트웨어 테스팅의 블랙박스 테스트 원칙과 일치한다.</li>
<li><strong>Pass@k 지표:</strong> 확률적 모델을 평가하기 위해 단 한 번의 시도(Pass@1)가 아닌, k번 생성하여 그중 하나라도 모든 테스트 케이스를 통과하면 정답으로 인정하는 Pass@k 지표가 널리 사용된다. 이는 AI의 비결정론성을 수용하면서도 결과의 품질을 보장하는 실용적인 접근법이다.</li>
</ul>
<h3>5.2  오라클 문제(Oracle Problem)의 해결 전략</h3>
<p>기능적 정확성을 확인하기 위해서는 “무엇이 올바른 출력인가?“를 알려주는 오라클이 필요하다. 제미나이 활용 시 사용할 수 있는 오라클 전략은 다음과 같다.</p>
<ol>
<li><strong>참조 구현(Reference Implementation) 활용:</strong> 가장 강력한 오라클은 이미 검증된 ’정답 코드(Canonical Solution)’를 확보하는 것이다. 제미나이가 생성한 코드와 정답 코드를 동일한 입력값으로 실행하여 출력을 비교한다. 이를 **차분 테스팅(Differential Testing)**이라 한다. 이 방식은 정답 코드를 미리 작성해야 한다는 비용이 들지만, 가장 확실한 검증을 보장한다.</li>
<li><strong>속성 기반 테스트(Property-Based Testing):</strong> 정답 코드가 없거나 작성이 어려운 경우, 결과가 만족해야 할 수학적/논리적 속성(Property)을 오라클로 사용한다. 예를 들어, 정렬 함수의 경우 “결과는 항상 오름차순이어야 한다“는 속성과 “입력 리스트의 원소 집합과 출력 리스트의 원소 집합은 동일해야 한다(Permutation)“는 속성을 검증한다. 이는 구체적인 정답 값 없이도 코드의 논리적 결함을 찾아낼 수 있는 강력한 방법이다.</li>
<li><strong>메타모픽 테스팅(Metamorphic Testing):</strong> 입력값의 변화와 출력값의 변화 사이의 관계(Metamorphic Relation)를 이용한다. 예를 들어, 검색 알고리즘을 테스트할 때, 데이터베이스에 항목을 추가한 후 검색하면 이전 결과에 해당 항목이 포함되어야 한다는 관계를 검증한다. 이는 정답을 모르는 상태에서도 AI가 생성한 코드의 일관성을 검증할 수 있게 해준다.</li>
</ol>
<p><img src="./3.1.0.0.0%20%EA%B2%B0%EC%A0%95%EB%A1%A0%EC%A0%81%20%EC%A0%95%EB%8B%B5%EC%A7%80Deterministic%20Ground%20Truth%EC%9D%98%20%EC%A0%95%EC%9D%98%EC%99%80%20%EB%B3%B8%EC%A7%88.assets/image-20260218124038895.jpg" alt="image-20260218124038895" /></p>
<h3>5.3  EvalPlus와 엄밀한 검증</h3>
<p>최근 연구인 <strong>EvalPlus</strong>는 기존 벤치마크(HumanEval)의 테스트 케이스가 부족하여 잘못된 코드도 정답으로 통과되는 문제를 지적했다. 이를 해결하기 위해 AI를 사용하여 수만 개의 테스트 케이스를 자동으로 생성하고, 이를 통해 검증의 밀도를 비약적으로 높였다. 이는 “정답지는 고정된 것이 아니라, 검증의 필요에 따라 확장 가능한 동적인 집합“임을 시사한다. 서적 집필 시에도, 독자가 시도해 볼 수 있는 몇 가지 예제 입력뿐만 아니라, 보이지 않는 곳에서 수많은 엣지 케이스를 통해 코드를 검증해야만 ’결정론적 정답’이라고 부를 수 있는 품질을 확보할 수 있다.</p>
<h2>6.  제미나이 활용 시 결정론적 정답지의 구성 전략 (Practical Methodology)</h2>
<p>이제 이론적 논의를 넘어, 실제로 제미나이를 활용해 서적을 집필할 때 적용할 수 있는 구체적인 정답지 구성 전략을 살펴보자. 독자에게 신뢰할 수 있는 정보를 제공하기 위해, 저자는 AI가 생성한 내용을 맹목적으로 수용하는 대신 <strong>검증 가능한(Verifiable) 단위</strong>로 분해하고 각 단위에 대한 정답지를 구축해야 한다.</p>
<h3>6.1  팩트 검증(Fact-Checking)과 검색 증강 생성(RAG)</h3>
<p>제미나이가 생성하는 자연어 텍스트(예: 역사적 사실, 특정 라이브러리의 API 설명, 최신 기술 트렌드)의 경우, 정답지는 **신뢰할 수 있는 외부 지식 베이스(Knowledge Base)**에 존재한다.</p>
<ul>
<li><strong>RAG(Retrieval-Augmented Generation) 파이프라인 구축:</strong> 모델이 자신의 내부 파라미터에 저장된(종종 부정확한) 지식에만 의존하게 두지 말고, 공식 기술 문서(Official Documentation), 위키피디아, 신뢰할 수 있는 논문 데이터베이스 등을 검색하게 한 뒤, 그 내용을 바탕으로 답변을 생성하도록 강제해야 한다. 이때 검색된 문서(Context)가 국소적인 정답지(Local Ground Truth) 역할을 수행한다.</li>
<li><strong>출처 기반 검증(Citation-Based Verification):</strong> 생성된 텍스트의 각 문장에 대해 출처가 명시되어 있는지 확인하고, 그 출처가 실제로 해당 내용을 포함하고 있는지(Faithfulness) 검증한다. 이는 AI의 환각(Hallucination) 현상을 제어하는 가장 효과적인 수단이다.</li>
</ul>
<h3>6.2  실행 유도 디코딩(Execution-Guided Decoding)과 자가 수정</h3>
<p>제미나이로 코드를 생성할 때는, 생성 즉시 실행해보고 그 결과를 피드백으로 사용하는 루프(Loop)를 구축해야 한다.</p>
<ol>
<li><strong>생성(Generate):</strong> 제미나이가 코드를 생성한다.</li>
<li><strong>실행 및 검증(Execute &amp; Verify):</strong> 생성된 코드를 샌드박스 환경에서 실행하고, 미리 준비된 테스트 케이스(정답지)를 적용한다.</li>
<li><strong>피드백(Feedback):</strong> 실행 중 런타임 에러가 발생하거나 테스트를 통과하지 못하면, 에러 메시지(Traceback)와 실패한 테스트 케이스 정보를 다시 제미나이에 입력으로 제공한다.</li>
<li><strong>수정(Repair):</strong> 제미나이는 이 피드백을 바탕으로 코드를 수정한다.</li>
</ol>
<p>이 과정에서 <strong>테스트 케이스</strong>와 <strong>컴파일러/인터프리터</strong>는 타협할 수 없는 결정론적 오라클로 기능한다. AI는 확률적으로 해답 공간을 탐색하지만, 그 해답의 유효성은 결정론적 심판관에 의해 판결된다. 저자는 이 루프가 통과된 코드만을 서적에 수록해야 한다.</p>
<h3>6.3  합성 데이터(Synthetic Data)와 교차 검증(Cross-Checking)</h3>
<p>모든 예제에 대해 사람이 일일이 정답 테스트 케이스를 작성하는 것은 불가능에 가깝다. 이때 AI 자체를 정답지 생성 도구로 활용할 수 있다.</p>
<ul>
<li><strong>다중 모델 합의(Multi-Model Consensus):</strong> 동일한 문제에 대해 GPT-4, Claude 3, Gemini Ultra 등 서로 다른 아키텍처나 학습 데이터를 가진 모델들에게 코드를 생성하게 한다. 만약 이들 모델이 생성한 코드의 실행 결과가 모두 일치한다면, 이를 잠정적인 정답(Silver Standard)으로 간주할 수 있다.</li>
<li><strong>테스트 케이스 증강:</strong> 저자가 기본적인 “Happy Path” 테스트 케이스 하나를 작성하면, 제미나이에게 “이 코드가 실패할 수 있는 엣지 케이스를 10개 생성해줘“라고 요청하여 정답지의 커버리지를 넓힐 수 있다. 물론 이 생성된 테스트 케이스 자체도 검증이 필요하지만, 인간의 상상력을 보완하는 강력한 도구가 된다.</li>
</ul>
<h2>7.  결론: 하이브리드 진실의 시대를 위한 제언</h2>
<p>“3.1 결정론적 정답지의 정의와 본질“을 마무리하며, 우리는 AI 시대에 정답(Truth)의 개념이 이원화되고 있음을 명확히 인식해야 한다.</p>
<p>첫째, **결정론적 영역(Hard Truth)**이 있다. 코드의 실행 결과, 수학적 계산, 물리적 데이터, 역사적 사실 관계 등이 이에 해당한다. 이는 여전히 불변하며 타협할 수 없는 검증의 기준이다. ISO 표준과 테스트 오라클이 이 영역을 수호한다.</p>
<p>둘째, **확률론적 영역(Soft Truth)**이 있다. 자연어의 유창성, 설명의 친절함, 코드의 가독성 및 스타일, 창의적 비유 등이 이에 해당한다. 이는 제미나이와 같은 AI가 확률 분포 위에서 춤추며 만들어내는 영역이며, 인간의 주관적 평가가 개입되는 곳이다.</p>
<p>성공적인 제미나이 활용 서적 집필은 이 두 영역의 <strong>변증법적 통합</strong> 과정이다. 저자는 AI의 확률적 창의성을 적극 활용하여 풍부한 콘텐츠를 생성하되, 그 결과물은 엄격한 결정론적 정답지(테스트 케이스, 팩트 체크 파이프라인)에 의해 끊임없이 검증되고 정제되어야 한다. 최종적으로 독자에게 전달되는 원고는 확률이 빚어내고 결정론이 보증한 결과물이어야 한다. 이것이 AI와 공존하는 시대에 우리가 추구해야 할 새로운 정답의 정의이자, 엔지니어링의 윤리다.</p>
<p><img src="./3.1.0.0.0%20%EA%B2%B0%EC%A0%95%EB%A1%A0%EC%A0%81%20%EC%A0%95%EB%8B%B5%EC%A7%80Deterministic%20Ground%20Truth%EC%9D%98%20%EC%A0%95%EC%9D%98%EC%99%80%20%EB%B3%B8%EC%A7%88.assets/image-20260218124104344.jpg" alt="image-20260218124104344" /></p>
<h2>8. 참고 자료</h2>
<ol>
<li>Probabilistic and Deterministic Results in AI Systems - Gaine, https://www.gaine.com/blog/probabilistic-and-deterministic-results-in-ai-systems</li>
<li>Difference between Deterministic and Non-deterministic Algorithms, https://www.geeksforgeeks.org/dsa/difference-between-deterministic-and-non-deterministic-algorithms/</li>
<li>Deterministic AI vs. Probabilistic AI: Scaling Securely, https://moveo.ai/blog/deterministic-ai-vs-probabilistic-ai</li>
<li>AI Confidence: Evaluating AI Models With Synthetic Data - Winder.AI, https://winder.ai/evaluating-ai-models-synthetic-data/</li>
<li>Is Your Code Generated by ChatGPT Really Correct … - NeurIPS, https://proceedings.neurips.cc/paper_files/paper/2023/file/43e9d647ccd3e4b7b5baab53f0368686-Paper-Conference.pdf</li>
<li>Ground Truth &amp; Blockchain Technology | by Shaan Ray - Medium, https://medium.com/lansaar/ground-truth-blockchain-technology-59372f0d84c6</li>
<li>What Is Ground Truth in Machine Learning? | IBM, https://www.ibm.com/think/topics/ground-truth</li>
<li>Ground Truth - Datumo-All in one data solution, https://datumo.com/glossary/ground-truth/</li>
<li>Ground Truth in Machine Learning - Lyzr, https://www.lyzr.ai/glossaries/ground-truth-in-machine-learning/</li>
<li>IS AI GROUND TRUTH REALLY TRUE? THE DANGERS OF, <a href="https://www.law.nyu.edu/sites/default/files/Lebovitz%2C%20Levina%2C%20Lifshitz-Assaf%2C%20MISQ%2C%202021%2C%20Published.pdf">https://www.law.nyu.edu/sites/default/files/Lebovitz%2C%20Levina%2C%20Lifshitz-Assaf%2C%20MISQ%2C%202021%2C%20Published.pdf</a></li>
<li>Rethinking Ground Truth in Educational AI Annotation, https://scale.stanford.edu/ai/repository/beyond-agreement-rethinking-ground-truth-educational-ai-annotation</li>
<li>Objective-Driven AI - Harvard CMSA, https://cmsa.fas.harvard.edu/media/lecun-20240328-harvard_reduced.pdf</li>
<li>On the empirical and non-deterministic nature of AI systems, https://nwg.ai/on-the-empirical-and-non-deterministic-nature-of-ai-systems-5ebd0cf19770</li>
<li>New Software Testing Standards - ISO/IEC/IEEE 29119, https://www.evoketechnologies.com/blog/software-testing/new-software-testing-standards/</li>
<li>(PDF) Perfect is the enemy of test oracle - ResearchGate, https://www.researchgate.net/publication/368290853_Perfect_is_the_enemy_of_test_oracle</li>
<li>Perfect Is the Enemy of Test Oracle - arXiv, https://arxiv.org/pdf/2302.01488</li>
<li>BS ISO/IEC/IEEE 29119-1:2022 - TC | 31 Mar 2022 | BSI Knowledge, https://knowledge.bsigroup.com/products/software-and-systems-engineering-software-testing-general-concepts</li>
<li>1.software Testing: The IEEE Definition For Testing Is | PDF - Scribd, https://www.scribd.com/document/16413295/Software-Testing</li>
<li>Software testing - Wikipedia, https://en.wikipedia.org/wiki/Software_testing</li>
<li>IEEE Standard for Software and System Test Documentation, https://www.researchgate.net/publication/234125658_IEEE_Standard_for_Software_and_System_Test_Documentation</li>
<li>LLM evaluation metrics: A comprehensive guide for large language, https://wandb.ai/onlineinference/genai-research/reports/LLM-evaluation-metrics-A-comprehensive-guide-for-large-language-models–VmlldzoxMjU5ODA4NA</li>
<li>Evaluating LLMs : All about LLM Benchmarks -2021 series - Medium, https://medium.com/@saurad44/all-about-llm-benchmarks-2021-series-c0cc61dd9226</li>
<li>Benchmarks and Metrics for Evaluations of Code Generation, https://radar.brookes.ac.uk/radar/file/9fbc107d-f24d-40d1-b42f-106c61c97bab/1/SurveyCodeGenEval-final.pdf</li>
<li>Hungarian Algorithm in Detr: A Comprehensive Guide for 2025, https://www.shadecoder.com/ja/topics/hungarian-algorithm-in-detr-a-comprehensive-guide-for-2025</li>
<li>Constraint-Based Test Oracles for Program Analyzers, https://mariachris.github.io/Pubs/ASE-2024-Minotaur.pdf</li>
<li>The Fusion of Large Language Models and Formal Methods … - arXiv, https://arxiv.org/html/2412.06512v1</li>
<li>The Oracle Problem in Software Testing: A Survey - Earl Barr, https://earlbarr.com/publications/testoracles.pdf</li>
<li>Build a Text-to-SQL solution for data consistency in generative AI, https://aws.amazon.com/blogs/machine-learning/build-a-text-to-sql-solution-for-data-consistency-in-generative-ai-using-amazon-nova/</li>
<li>Select AI Agent Concepts - Oracle Help Center, https://docs.oracle.com/en-us/iaas/autonomous-database-serverless/doc/select-ai-agents-concepts.html</li>
<li>training incremental text-to-sql parsers with non-deterministic oracles, https://arxiv.org/pdf/1809.05054</li>
<li>Diagnosing Failure Root Causes in Platform-Orchestrated Agentic, https://arxiv.org/html/2509.23735v1</li>
<li>LLM-Based Test Oracle Generation With External Tools Assistance, https://par.nsf.gov/servlets/purl/10596812</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>