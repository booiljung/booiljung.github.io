<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.2.2.2 "더 나아졌다"는 주장을 증명하기 위한 객관적 지표</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.2.2.2 "더 나아졌다"는 주장을 증명하기 위한 객관적 지표</h1>
                    <nav class="breadcrumbs"><a href="../../../../../index.html">Home</a> / <a href="../../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../../index.html">Chapter 3. 결정론적 정답지(Deterministic Ground Truth)의 설계 원칙과 필요성</a> / <a href="../index.html">3.2 왜 결정론적 정답지가 필수적인가? (Necessity)</a> / <a href="index.html">3.2.2 회귀 테스트(Regression Testing)의 기준점 확보</a> / <span>3.2.2.2 "더 나아졌다"는 주장을 증명하기 위한 객관적 지표</span></nav>
                </div>
            </header>
            <article>
                <h1>3.2.2.2 “더 나아졌다“는 주장을 증명하기 위한 객관적 지표</h1>
<p>소프트웨어 엔지니어링의 역사에서 “성능이 향상되었다“는 선언은 언제나 명확한 증거를 요구해 왔다. 전통적인 결정론적(Deterministic) 소프트웨어 개발 방법론에서 이러한 증거는 단위 테스트의 통과율, 응답 시간의 단축, 혹은 메모리 사용량의 감소와 같은 물리적이고 확정적인 수치로 귀결되었다. 그러나 인공지능, 특히 대규모 언어 모델(LLM)과 생성형 AI(Generative AI)가 주도하는 현대의 소프트웨어 개발 환경은 이러한 검증의 패러다임을 근본적으로 뒤흔들고 있다. AI 모델은 본질적으로 확률적(Probabilistic)이며 비결정론적 특성을 내재하고 있어, 동일한 입력에 대해서도 매번 다른 출력을 생성할 수 있다. 이러한 불확실성은 개발자들로 하여금 “새로운 모델이 이전 모델보다 정말로 더 나은가?“라는 질문에 대해 확신을 가지고 답하기 어렵게 만든다. 흔히 ’바이브(Vibe)’라고 불리는 주관적인 체감 성능이나, 몇 가지 성공적인 샘플만을 근거로 판단하는 것은 엔터프라이즈 환경에서 용납될 수 없는 리스크를 초래한다. 따라서 2025년과 2026년의 소프트웨어 엔지니어링은 ’느낌’의 영역을 벗어나, 통계적으로 유의미하고 재현 가능한 객관적 지표(Objective Metrics)를 확립하는 것에 사활을 걸고 있다.</p>
<p>본 장에서는 AI 모델의 성능 변화를 정량적으로 측정하고, 단순한 무작위적 변동(Noise)이 아닌 실제적인 개선 신호(Signal)로서의 ’더 나아짐’을 증명하기 위한 핵심 지표와 방법론을 심층적으로 분석한다. 이는 결정론적 오라클의 구조적 검증부터 시작하여, 기능적 정확성을 위한 통계적 추정, 의미론적 유사성 평가, 그리고 최신 연구가 제시하는 강건성(Robustness) 지표에 이르기까지 다층적인 평가 프레임워크를 포함한다.</p>
<h2>1.  결정론적 오라클과 구조적 무결성의 정량화</h2>
<p>AI 모델, 특히 에이전트 기반 시스템(Agentic Systems)이나 RAG(Retrieval-Augmented Generation) 파이프라인의 핵심은 자연어 생성 능력이 아니라, 다른 시스템과 상호작용할 수 있는 구조적 데이터를 얼마나 정확하게 생성하느냐에 달려 있다. 자연어의 유창함이 인간 사용자를 위한 것이라면, 구조적 무결성은 시스템 간의 통합을 위한 필수 조건이다. 따라서 회귀 테스트(Regression Testing)의 첫 번째 관문은 모델이 약속된 데이터 계약(Contract)을 얼마나 엄격하게 준수하는지를 측정하는 결정론적 오라클(Deterministic Oracle)의 영역이다.</p>
<h3>1.1  JSON 스키마 검증률 (JSON Schema Validation Rate)</h3>
<p>최신 AI 시스템, 특히 도구 사용(Tool Use)이나 API 호출을 수행하는 에이전트는 자연어가 아닌 구조화된 데이터(JSON, XML)를 출력해야 한다. 이때 가장 기본적이면서도 강력한 지표는 <strong>JSON 스키마 검증률</strong>이다. 이는 단순히 모델이 생성한 출력이 JSON 문법(Syntax)을 지키는지, 즉 괄호의 짝이 맞는지나 따옴표가 제대로 닫혔는지를 확인하는 수준을 훨씬 넘어선다. 진정한 구조적 무결성은 출력이 사전에 정의된 복잡한 데이터 스키마(Schema)의 모든 제약 조건을 완벽하게 충족했는지를 측정하는 것이다.</p>
<p>단순한 파싱(Parsing) 성공 여부는 모델의 성능을 판단하기에 불충분하다. 예를 들어, 특정 필드가 필수(Required)인지, 숫자 필드의 범위가 비즈니스 로직에 부합하는지(예: <code>age &gt;= 0</code> 또는 <code>price &gt; 0</code>), 열거형(Enum) 데이터가 지정된 유효 값 집합 내에 존재하는지 등을 검증해야 한다. 이러한 검증은 모델이 지시사항(Instruction)을 얼마나 정밀하게 따르고 있는지를 보여주는 척도이다. 이를 수식으로 정의하면 다음과 같다.<br />
<span class="math math-display">
\text{Schema Validation Rate} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{1}(\text{validate}(y_i, S))
</span><br />
여기서 <span class="math math-inline">N</span>은 전체 테스트 케이스의 수, <span class="math math-inline">y_i</span>는 <span class="math math-inline">i</span>번째 모델 출력, <span class="math math-inline">S</span>는 목표 JSON 스키마, <span class="math math-inline">\mathbb{1}</span>은 검증 함수가 참(True)일 때 1을 반환하는 지시 함수이다. 이 지표는 0과 1 사이의 값을 가지며, 엔터프라이즈급 애플리케이션에서는 99.9% 이상의 신뢰도를 목표로 한다.</p>
<p>최근 연구인 DeepJSONEval(2025)에 따르면, 단순한 스키마 준수뿐만 아니라 중첩된(Nested) 구조의 깊이와 복잡도에 따른 검증 성공률을 별도로 추적하는 것이 중요하다. 연구 결과에 따르면, 최신 프론티어 모델들조차 얕은 구조의 JSON은 잘 생성하지만, 깊이가 3단계 이상으로 내려가거나 필드 수가 100개를 넘어가는 복잡한 스키마(예: SEC 10-K 문서 추출)에서는 환각(Hallucination)을 일으키거나 구조를 깨뜨리는 경향이 급격히 증가한다. 따라서 회귀 테스트에서는 전체 평균 검증률뿐만 아니라, 스키마의 깊이(Depth)와 폭(Breadth)에 따라 검증률을 세분화하여(Segmentation) 지표를 관리해야 한다.</p>
<p><img src="./3.2.2.2.0%20%EB%8D%94%20%EB%82%98%EC%95%84%EC%A1%8C%EB%8B%A4%EB%8A%94%20%EC%A3%BC%EC%9E%A5%EC%9D%84%20%EC%A6%9D%EB%AA%85%ED%95%98%EA%B8%B0%20%EC%9C%84%ED%95%9C%20%EA%B0%9D%EA%B4%80%EC%A0%81%20%EC%A7%80%ED%91%9C.assets/image-20260218202612720.jpg" alt="image-20260218202612720" /></p>
<h3>1.2  정확한 일치와 정규식 매칭의 한계와 보완</h3>
<p>코딩 작업이나 SQL 생성, 특정 포맷의 날짜 추출 등 정답이 유일하거나 매우 한정적인 작업의 경우, <strong>정확한 일치(Exact Match)</strong> 비율이 가장 직관적이고 강력한 지표가 된다. 이는 모델의 출력이 정답(Ground Truth)과 문자 단위로 완벽하게 동일한지를 측정한다. 분류(Classification) 작업이나 엔티티 추출(Entity Extraction)과 같이 정답의 범위가 닫혀 있는 경우, 정확한 일치는 타협할 수 없는 기준이 된다.</p>
<p>하지만 생성형 모델의 특성상 공백(Whitespace), 대소문자, 혹은 사소한 문구 차이가 발생할 수 있으며, 이것이 기능적 오류를 의미하지는 않을 때가 많다. 따라서 이를 보완하기 위해 정규식(Regex) 기반의 부분 일치나 핵심 키워드 포함 여부(Keyword Presence)를 함께 측정하는 것이 일반적이다. 예를 들어, “2025-10-27“이라는 날짜를 추출할 때, 모델이 “Date: 2025-10-27“이라고 출력하더라도 정규식을 통해 핵심 정보만 추출하여 비교함으로써 불필요한 오답 판정을 방지할 수 있다.</p>
<p>더 나아가, SQL 생성 모델이나 코드 생성 모델의 경우, 생성된 텍스트가 정답과 100% 일치하지 않더라도 실행 결과(Execution Result)가 동일하다면 정답으로 간주하는 **실행 정확도(Execution Accuracy)**가 훨씬 더 유의미한 지표가 된다. 이는 텍스트의 표면적 일치(Surface-level match)보다 기능적 등가성(Functional Equivalence)에 초점을 맞춘 접근이다. 예를 들어, <code>SELECT * FROM table</code>과 <code>SELECT * FROM table;</code>은 문자열로는 다르지만 기능적으로는 동일하다. 이러한 실행 기반 검증은 결정론적 오라클의 가장 고도화된 형태이며, 회귀 테스트에서 거짓 양성(False Positive)과 거짓 음성(False Negative)을 줄이는 데 핵심적인 역할을 한다.</p>
<p>다음 표는 결정론적 오라클 지표들의 특성과 적용 사례를 요약한 것이다.</p>
<table><thead><tr><th><strong>지표 유형</strong></th><th><strong>정의 및 측정 방식</strong></th><th><strong>적용 사례</strong></th><th><strong>장점</strong></th><th><strong>단점</strong></th></tr></thead><tbody>
<tr><td><strong>Exact Match</strong></td><td>출력과 정답의 문자열 완전 일치 여부</td><td>분류, 고유명사 추출</td><td>구현이 쉽고 해석이 명확함</td><td>유연성이 없어 동의어나 서식 차이를 오답 처리</td></tr>
<tr><td><strong>JSON Schema Validation</strong></td><td>JSON 구조 및 데이터 타입 제약 조건 준수 여부</td><td>API 호출, 구조적 데이터 생성</td><td>시스템 통합성 보장, 엄격한 타입 체크</td><td>내용(값)의 사실 관계는 검증하지 못함</td></tr>
<tr><td><strong>Regex Match</strong></td><td>정규 표현식을 통한 특정 패턴 준수 여부</td><td>날짜, 이메일, 전화번호 추출</td><td>포맷 유연성 확보</td><td>복잡한 문법 구조 검증에는 한계가 있음</td></tr>
<tr><td><strong>Execution Accuracy</strong></td><td>생성된 코드/쿼리의 실행 결과 일치 여부</td><td>코드 생성, SQL 생성, 도구 호출</td><td>기능적 등가성 보장, 가장 실질적인 지표</td><td>실행 환경(Sandbox) 구축 필요, 보안 리스크</td></tr>
</tbody></table>
<h2>2.  코드 생성 및 기능적 정확성을 위한 통계적 지표</h2>
<p>코드 생성(Code Generation) 모델의 평가는 단순한 텍스트 유사성을 넘어선다. 코드는 실행 가능해야 하며, 의도한 기능을 정확히 수행해야 한다. 이 분야에서 가장 널리 사용되면서도, 통계적으로 가장 오해하기 쉬운 지표가 바로 <strong>Pass@k</strong>이다. 많은 개발자들이 이를 단순히 “k번 시도해서 성공했는가“를 측정하는 것으로 이해하지만, 실제로는 훨씬 더 정교한 확률적 추정이 필요한 지표이다.</p>
<h3>2.1  편향되지 않은 Pass@k 추정량 (Unbiased Pass@k Estimator)</h3>
<p>전통적인 Pass@k의 정의는 모델이 <span class="math math-inline">k</span>개의 샘플을 생성했을 때, 그중 적어도 하나가 단위 테스트를 통과할 확률이다. 그러나 실제로 모델을 평가할 때 <span class="math math-inline">k</span>개만 생성해서는 통계적 분산(Variance)이 너무 커서 신뢰할 수 있는 결과를 얻기 어렵다. 모델의 성능이 확률 분포를 따르기 때문이다. 따라서 더 많은 횟수(<span class="math math-inline">n \ge k</span>)인 <span class="math math-inline">n</span>개의 샘플을 생성하고, 그중 <span class="math math-inline">c</span>개가 정답일 때, Pass@k의 불편 추정량(Unbiased Estimator)을 계산하는 방식이 표준으로 자리 잡았다.</p>
<p>이 추정량은 다음과 같은 수식으로 정의된다.\text{Pass}@k = 1 - \frac{\binom{n-c}{k}}{\binom{n}{k}}<br />
<span class="math math-display">
\text{Pass}@k = 1 - \frac{\binom{n-c}{k}}{\binom{n}{k}}
</span><br />
이 공식의 수학적 의미는 전체 <span class="math math-inline">n</span>개의 샘플 중에서 <span class="math math-inline">k</span>개를 무작위로 뽑았을 때, 뽑힌 <span class="math math-inline">k</span>개가 모두 오답일 확률을 전체 확률 1에서 뺀 것이다. 여기서 <span class="math math-inline">\binom{n}{k}</span>는 이항 계수(Binomial Coefficient)로 조합(Combination)을 의미한다. 이 방식은 단순히 <span class="math math-inline">k</span>개의 샘플을 추출하여 테스트하는 것보다 훨씬 더 안정적이고 낮은 분산을 가지며, 특히 <span class="math math-inline">k</span>가 작을 때 더 정확한 추정치를 제공한다.</p>
<p>OpenAI의 HumanEval 논문(2021)과 이후의 후속 연구들은 일반적으로 <span class="math math-inline">n=200</span>으로 설정하고 <span class="math math-inline">k \in {1, 10, 100}</span>에 대한 Pass@k를 보고하는 것을 표준으로 사용한다. 따라서 회귀 테스트 시 “Pass@1 점수가 0.5% 올랐다“고 주장하려면, 이 수치가 단순한 샘플링 노이즈가 아니라 위 공식에 의해 계산된, 충분한 샘플 수(<span class="math math-inline">n</span>)를 바탕으로 한 통계적으로 유의미한 변화여야 한다. 단순히 한 번 실행해서 통과했다고 점수가 올랐다고 판단하는 것은 통계적 착시일 뿐이다.</p>
<p>또한, 이 공식을 코드로 구현할 때는 이항 계수의 값이 매우 커져서 오버플로우가 발생할 수 있으므로, 수치적 안정성(Numerical Stability)을 위해 직접적인 팩토리얼 계산 대신 루프를 통한 곱셈 방식을 사용하는 것이 권장된다.</p>
<pre><code class="language-Python">def estimate_pass_at_k(n, c, k):
    """
    Calculates 1 - comb(n - c, k) / comb(n, k) logically.
    """
    if n - c &lt; k:
        return 1.0
    return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))
</code></pre>
<p>위의 Python 코드 예시는 OpenAI가 제안한 수치적으로 안정적인 Pass@k 계산 방식이다.</p>
<h3>2.2  실패율(Failure Rate)과 모드 붕괴 탐지</h3>
<p>Pass@k의 보완 지표로 <strong>Failure@k</strong>가 사용되기도 한다. 이는 <span class="math math-inline">k</span>번의 시도가 모두 실패할 확률을 의미하며, 모델의 신뢰성(Reliability)과 다양성(Diversity)을 보여주는 지표이다. 이론적으로 모델이 다양한 해결책을 탐색한다면 <span class="math math-inline">k</span>가 증가함에 따라 Failure@k는 지수적으로 감소해야 한다. 즉, <span class="math math-inline">(1-p)^k</span>의 형태로 줄어들어야 한다.</p>
<p>그러나 만약 <span class="math math-inline">k</span>를 늘려도 Failure@k가 예상만큼 줄어들지 않거나 평탄화된다면, 이는 모델이 특정 오답 패턴에 고착되어 있거나 탐색(Exploration) 능력을 잃어버리는 <strong>모드 붕괴(Mode Collapse)</strong> 현상을 겪고 있음을 시사한다. 특히 강화학습(RL) 기반의 코드 생성 모델이나 미세 조정(Fine-tuning)된 모델에서 과적합(Overfitting)이 발생했을 때 이러한 징후가 나타난다. 따라서 회귀 테스트에서는 단순히 Pass@1의 상승뿐만 아니라, Pass@10이나 Pass@100에서의 Failure Rate 감소 추세가 유지되는지를 확인하여 모델의 다양성과 강건성이 훼손되지 않았는지 검증해야 한다.</p>
<h3>2.3  HumanEval과 MBPP: 벤치마크의 진화</h3>
<p>코드 생성 모델의 “더 나아짐“을 증명하기 위해 가장 널리 쓰이는 벤치마크는 <strong>HumanEval</strong>과 **MBPP(Mostly Basic Python Problems)**이다. HumanEval은 164개의 손수 작성된 프로그래밍 문제로 구성되어 있으며, 함수 시그니처와 독스트링(Docstring)이 주어지면 함수 본문을 완성하는 능력을 평가한다. MBPP는 약 974개의 초급 수준 파이썬 문제로 구성되어 있다.</p>
<p>하지만 2025년 현재, 이들 벤치마크는 데이터 오염(Data Contamination) 문제와 난이도의 한계에 직면해 있다. 모델들이 훈련 과정에서 이들 벤치마크의 정답을 이미 학습했을 가능성이 높기 때문이다. 이에 대응하여, <strong>HumanEval Pro</strong>나 <strong>Self-invoking</strong> 테스트와 같은 더욱 고도화된 벤치마크들이 등장하고 있다. HumanEval Pro는 기존 문제보다 더 복잡한 요구사항과 엣지 케이스(Edge Case)를 포함하며, 모델이 생성한 코드가 자기 자신을 호출하거나(Recursion) 복잡한 상태를 관리해야 하는 시나리오를 다룬다.</p>
<p>실제로 연구 결과에 따르면, GPT-4나 Claude 3.5 Sonnet과 같은 최신 모델들도 HumanEval에서는 80-90% 이상의 높은 Pass@1을 기록하지만, HumanEval Pro나 복잡한 엔지니어링 과제에서는 성능이 크게 하락하는 모습을 보인다. 따라서 진정한 성능 향상을 증명하기 위해서는 레거시 벤치마크의 점수 상승에 안주하지 않고, 오염되지 않고 더 높은 난이도를 가진 최신 벤치마크에서의 성능 유지를 입증해야 한다.</p>
<h2>3.  의미론적 유사성과 LLM 심판 (Semantic Similarity &amp; LLM-as-a-Judge)</h2>
<p>정답이 하나로 정해지지 않은 개방형 질문(Open-ended QA), 창의적 글쓰기, 요약(Summarization), 혹은 뉘앙스가 중요한 번역 과제에서는 텍스트의 표면적 일치(Exact Match)나 정규식만으로는 성능을 평가할 수 없다. “더 나아졌다“는 것은 문맥을 더 잘 이해하고, 더 적절한 표현을 사용하며, 사실 관계를 정확히 전달한다는 것을 의미한다. 이를 정량화하기 위해 의미론적 유사성(Semantic Similarity) 지표와 LLM을 심판으로 활용하는 방법론이 필수적이다.</p>
<h3>3.1  임베딩 기반 코사인 유사도 (Embedding Cosine Similarity)</h3>
<p>전통적인 NLP 지표인 <strong>BLEU</strong>나 <strong>ROUGE</strong> 점수는 단어(n-gram)의 중복을 기반으로 계산된다. 이는 기계 번역 초기에는 유용했으나, 현대의 LLM 평가에는 적합하지 않다. 모델이 정답(Reference)과 의미는 완벽하게 동일하지만 완전히 다른 단어나 문구(Paraphrasing)를 사용하는 경우, BLEU나 ROUGE는 이를 낮은 점수로 평가하여 성능을 과소평가하기 때문이다.</p>
<p>이를 극복하기 위해, 모델의 출력과 정답을 고차원 벡터 공간에 **임베딩(Embedding)**하고, 두 벡터 간의 **코사인 유사도(Cosine Similarity)**를 측정하는 방식이 표준이 되었다. 대표적인 지표로는 <strong>BERTScore</strong>와 <strong>MoverScore</strong>가 있다.</p>
<ul>
<li><strong>BERTScore:</strong> 사전 훈련된 BERT 모델의 문맥적 임베딩(Contextual Embedding)을 사용하여 생성된 문장의 각 토큰과 정답 문장의 토큰 간의 유사도를 계산한다. 이는 단어의 순서가 바뀌거나 동의어가 사용되어도 문맥적 의미가 유사하다면 높은 점수를 부여한다.</li>
<li><strong>MoverScore:</strong> 텍스트 간의 거리를 측정하기 위해 지구 이동 거리(Earth Mover’s Distance) 개념을 도입한 지표로, 한 텍스트를 다른 텍스트로 변환하는 데 드는 최소 비용을 계산하여 의미론적 거리를 측정한다.</li>
</ul>
<p>일반적으로 회귀 테스트에서는 코사인 유사도 0.7~0.9 이상을 임계값(Threshold)으로 설정하여 정답 여부를 판단한다. 하지만 이 임계값은 도메인과 데이터셋에 따라 달라져야 하므로, 초기에는 인간 평가와의 상관관계를 분석하여 적절한 기준선을 설정하는 캘리브레이션(Calibration) 과정이 필요하다.</p>
<h3>3.2  LLM-as-a-Judge와 G-Eval 프레임워크</h3>
<p>2024년 이후, 가장 강력하고 널리 사용되는 평가 방법론은 <strong>LLM-as-a-Judge</strong>이다. 이는 GPT-4o나 Claude 3.5 Sonnet과 같은 고성능 LLM을 심판(Judge)으로 사용하여, 대상 모델(Student Model)의 출력을 평가하는 방식이다. 이 방식은 단순한 유사도 측정을 넘어, 사실성(Factuality), 일관성(Coherence), 유해성(Toxicity), 도움됨(Helpfulness) 등 인간만이 판단할 수 있었던 다차원적이고 주관적인 속성을 정량화할 수 있게 해준다.</p>
<p>특히 <strong>G-Eval</strong> 프레임워크는 이러한 LLM 기반 평가를 체계화한 것이다. G-Eval은 평가 기준(Rubric)을 상세한 프롬프트로 제공하고, 심판 모델이 점수를 매기기 전에 평가 이유를 먼저 서술하게 하는 사고 사슬(Chain of Thought, CoT) 방식을 사용하여 평가의 신뢰도를 높인다. 연구에 따르면, G-Eval 점수는 기존의 자동화 지표보다 인간 평가와의 상관관계(Correlation)가 훨씬 높은 것으로 나타났다.</p>
<p>회귀 테스트에서 LLM 심판을 사용할 때 주의해야 할 점은 심판 모델 자체의 편향(Bias)이다. 예를 들어, 심판 모델은 자신의 훈련 데이터와 유사한 스타일의 텍스트를 선호하거나(Self-preference Bias), 더 긴 답변을 선호하는 경향(Verbosity Bias)이 있을 수 있다. 이를 통제하기 위해 다음과 같은 기법들이 사용된다.</p>
<ul>
<li><strong>쌍별 비교(Pairwise Comparison):</strong> 단일 점수를 매기는 대신, 이전 버전의 출력과 새 버전의 출력을 동시에 제시하고 “어느 것이 더 나은가?“를 묻는 방식이다. 이때 입력 순서에 따른 편향(Position Bias)을 없애기 위해 순서를 바꿔서 두 번 물어보는 것이 좋다.</li>
<li><strong>다수결 및 앙상블(Majority Voting &amp; Ensemble):</strong> 여러 개의 다른 프롬프트나 다른 심판 모델을 사용하여 평가하고 결과를 종합함으로써 개별 심판의 노이즈를 줄인다.</li>
</ul>
<h3>3.3  사실성과 환각 탐지 (Factuality &amp; Hallucination Detection)</h3>
<p>LLM의 가장 큰 문제점인 환각을 탐지하고 사실성(Factuality)을 검증하는 것은 “더 나아졌다“는 주장의 핵심이다. 이를 위해 <strong>신뢰성(Faithfulness)</strong> 지표가 사용된다.</p>
<p>RAG 시스템의 경우, 모델의 답변이 검색된 문맥(Context)에 기반하고 있는지를 측정하는 <strong>Contextual Faithfulness</strong>가 중요하다. 이는 심판 LLM에게 “답변의 모든 주장이 주어진 문맥에서 지지되는가?“를 묻고, 문맥에 없는 내용을 지어낸 경우 페널티를 부여하는 방식으로 측정된다. 또한, 외부 지식 베이스가 없는 경우에는 <strong>Self-Consistency</strong> 기법을 사용하여, 동일한 질문에 대해 여러 번 답변을 생성하게 한 뒤 답변들 간의 일치도를 측정함으로써 사실성을 간접적으로 추정할 수 있다. 일관성이 낮다면 환각일 가능성이 높다.</p>
<h2>4.  통계적 엄밀성: 신호와 소음의 분리 (Signal vs. Noise)</h2>
<p>“모델이 좋아졌다“는 주장이 참이려면, 관측된 성능 향상폭(Delta)이 측정 시스템의 오차 범위(Noise)보다 커야 한다. 2025년 최신 연구들은 단순한 점수 비교를 넘어 **신호 대 잡음비(Signal-to-Noise Ratio, SNR)**를 AI 평가의 핵심 지표로 도입하고 있다. 이전에는 “평균 점수가 1% 올랐다“는 것으로 충분했을지 모르지만, 이제는 그 1%가 훈련 과정의 무작위성(Randomness)이나 평가 데이터의 샘플링 노이즈에 의한 것이 아님을 증명해야 한다.</p>
<h3>4.1  AI 평가에서의 SNR 정의 및 측정</h3>
<p>Allen Institute for AI의 “Signal and Noise” 연구(2025)는 AI 벤치마크의 신뢰성을 판단하기 위해 SNR 개념을 도입했다. 여기서 **신호(Signal)**는 서로 다른 모델 간의 실제 성능 차이(즉, 벤치마크가 모델의 우열을 얼마나 잘 구별해내는가)를 의미하며, **잡음(Noise)**은 단일 모델이 훈련 단계(Step)나 시드(Seed)에 따라 보여주는 점수의 변동성을 의미한다.</p>
<p>SNR은 다음과 같이 정의된다 :<br />
<span class="math math-display">
\text{SNR} = \frac{\text{Signal}}{\text{Noise}} = \frac{\text{Dispersion}(M)}{\text{Std}(m)}
</span><br />
여기서 <span class="math math-inline">\text{Dispersion}(M)</span>은 평가 대상 모델 집단 간의 점수 분산(또는 범위)을 의미하며, <span class="math math-inline">\text{Std}(m)</span>은 단일 모델의 훈련 체크포인트 간 점수의 표준편차를 의미한다.</p>
<p>이 지표가 중요한 이유는 SNR이 낮은 벤치마크를 사용하면, 모델의 성능이 실제로 향상되었는지 아니면 단순히 운 좋게 높은 점수가 나온 체크포인트를 선택한 것인지 구분할 수 없기 때문이다. 예를 들어, ARC Challenge 벤치마크는 높은 노이즈를 보이는 반면, HellaSwag는 상대적으로 낮은 노이즈를 보인다. 회귀 테스트에서 SNR이 낮은 벤치마크를 주요 지표로 삼는 것은 위험하다.</p>
<p>SNR을 개선하기 위한 개입(Intervention) 전략으로는 노이즈가 심한 하위 작업(Subtask)을 필터링하거나, 단순 정확도(Accuracy) 대신 **Bits-per-byte (BPB)**와 같은 연속적인 확률 기반 지표를 사용하는 방법이 있다. BPB는 정답 텍스트에 대한 모델의 불확실성(Perplexity)을 측정하는 것으로, 이진적인 맞음/틀림보다 훨씬 더 풍부한 신호를 제공하여 SNR을 비약적으로(예: GSM8K에서 1.2 -&gt; 7.0) 향상시킨다.</p>
<p><img src="./3.2.2.2.0%20%EB%8D%94%20%EB%82%98%EC%95%84%EC%A1%8C%EB%8B%A4%EB%8A%94%20%EC%A3%BC%EC%9E%A5%EC%9D%84%20%EC%A6%9D%EB%AA%85%ED%95%98%EA%B8%B0%20%EC%9C%84%ED%95%9C%20%EA%B0%9D%EA%B4%80%EC%A0%81%20%EC%A7%80%ED%91%9C.assets/image-20260218202652387.jpg" alt="image-20260218202652387" /></p>
<h3>4.2  표준오차(SEM)와 신뢰구간의 필수적 보고</h3>
<p>Anthropic의 연구 “Adding Error Bars to Evals”(2024)는 평가 점수 보고 시 **평균의 표준오차(Standard Error of the Mean, SEM)**를 반드시 포함할 것을 강력히 권고한다. 이는 단순히 평균 점수 하나만 보고하는 관행이 모델 간의 차이를 과대평가하게 만들 수 있음을 지적한다.</p>
<p>중심극한정리(Central Limit Theorem, CLT)에 따라, 평가 질문들이 독립적이라면 평가 점수의 평균은 정규분포를 따르게 된다. 따라서 다음과 같이 95% 신뢰구간(Confidence Interval, CI)을 계산하여 보고해야 한다.<br />
<span class="math math-display">
\text{Confidence Interval} = \text{Score} \pm 1.96 \times \text{SEM}
</span><br />
만약 새로운 모델의 점수가 이전 모델의 신뢰구간 내에 포함된다면, 통계적으로 “더 나아졌다“고 주장할 수 없다.</p>
<p>또한, 많은 평가 셋이 하나의 긴 지문에 여러 질문이 딸려 있는 형태(예: SQuAD, DROP)를 취하는데, 이 경우 질문들이 서로 독립적이지 않다. 이를 무시하고 일반 SEM을 계산하면 오차를 과소평가하게 된다. 따라서 질문들의 그룹화 구조를 반영한 **클러스터 표준오차(Clustered Standard Errors)**를 사용하여 불확실성을 더 보수적으로, 그리고 정확하게 추정해야 한다. Anthropic의 연구에 따르면, 클러스터링을 고려했을 때 표준오차가 3배 이상 커지는 경우도 있어, 기존의 많은 “소폭 상승“이 실제로는 통계적으로 무의미했음을 시사한다.</p>
<h2>5.  최신 평가 트렌드: 강건성, 일관성, 그리고 오라클 품질 (2025-2026 Perspective)</h2>
<p>2025년 이후의 AI 평가 연구들은 단순한 정확도(Accuracy)를 넘어, 모델의 **강건성(Robustness)**과 평가를 수행하는 <strong>테스트 오라클 자체의 품질</strong>을 평가하는 방향으로 진화하고 있다. 모델이 “맞았다“는 사실보다 “어떤 상황에서도 흔들리지 않고 맞는가“가 더 중요한 품질의 척도가 되었기 때문이다.</p>
<h3>5.1  정확성-일관성(Correct-Consistency)과 Turbulence 벤치마크</h3>
<p>ICST 2025에서 발표된 <strong>Turbulence</strong> 벤치마크는 **정확성-일관성(Correct-Consistency)**이라는 새로운 복합 지표를 제시한다. 기존의 평가는 모델이 질문 A에 대해 정답을 맞히면 성공으로 간주했다. 그러나 Turbulence는 질문 A와 의미적으로는 동일하지만 변수명, 파라미터 수치, 혹은 문장 구조가 미세하게 변형된 이웃 질문들(Neighborhood) A’, A’‘, A’’’에 대해서도 일관되게 정답을 내놓는지를 측정한다.</p>
<p>어떤 모델이 질문 A에는 정답을 내놓지만, 사소하게 변형된 A’에는 오답을 낸다면, 이는 모델이 문제의 본질적 논리를 이해하고 해결한 것이 아니라 훈련 데이터의 특정 패턴에 과적합(Overfitting)되었거나 운이 좋았을 뿐임을 시사한다. Turbulence 연구에 따르면, 많은 SOTA(State-of-the-Art) 모델들이 이러한 섭동(Perturbation)에 취약하여 정확성-일관성 점수가 단순 정확도보다 현저히 낮게 나타난다. 따라서 회귀 테스트에서는 단일 프롬프트에 대한 성능보다는, 프롬프트 섭동에 대한 성능 유지력을 지표화해야 한다.</p>
<h3>5.2  오라클의 강도(Oracle Strength)와 TOGLL</h3>
<p>모델 자체의 성능뿐만 아니라, 모델을 테스트하는 ’테스트 코드’의 품질도 검증 대상이다. ICSE 2025의 <strong>TOGLL</strong> 연구는 LLM이 생성한 테스트 오라클(Test Oracle)의 품질을 다룬다. 모델이 생성한 테스트 코드가 문법적으로 컴파일되고 실행된다고 해서 좋은 테스트가 아니다. 실제 버그를 탐지할 수 있는 **강도(Strength)**를 가져야 한다.</p>
<p>이를 측정하기 위해 **변이 분석(Mutation Analysis)**이 사용된다. 즉, 원본 코드에 인위적인 버그(Mutant)를 심었을 때, 생성된 테스트가 이를 감지하여 실패(Fail)하는 비율인 **변이 점수(Mutation Score)**를 측정한다. “더 나아진” 모델은 더 높은 변이 점수를 기록하여, 단순히 통과하는 테스트를 만드는 것이 아니라 실제 결함을 찾아내는 능력이 향상되었음을 증명해야 한다. TOGLL 연구 결과, 특수 목적용으로 미세 조정된 작은 모델이 범용 거대 모델보다 더 강력하고 정확한 오라클을 생성할 수 있음이 밝혀졌다.</p>
<h3>5.3  Nexus: 다중 에이전트 협의를 통한 오라클 신뢰성</h3>
<p>2025년 10월에 발표된 <strong>Nexus</strong> 프레임워크는 단일 LLM에 의존하는 오라클의 한계를 극복하기 위해 다중 에이전트(Multi-agent) 시스템을 도입했다. Nexus는 여러 LLM 에이전트가 패널 토의(Panel Discussion)를 통해 오라클의 정당성을 검증하고, 실행 기반 검증(Execution-based Validation)과 반복적인 자가 수정(Self-refinement) 과정을 거친다.</p>
<p>이 접근법은 오라클 정확도를 획기적으로 향상시켰으며(예: LiveCodeBench에서 46% -&gt; 57%로 향상), 생성된 오라클의 버그 탐지율도 크게 높였다. 이는 회귀 테스트에서 단일 모델의 판단을 맹신하지 말고, 앙상블(Ensemble)이나 에이전트 협업을 통해 평가의 신뢰도를 높여야 함을 시사한다. “더 나아졌다“는 주장은 단일 모델의 판단이 아닌, 여러 모델의 합의(Consensus)에 의해 지지될 때 더욱 강력해진다.</p>
<h2>6. 결론: 회귀 테스트 게이트(Regression Gate) 구축</h2>
<p>결론적으로, 2026년 시점에서 AI 소프트웨어의 “더 나아졌다“는 주장은 단순히 벤치마크 점수 몇 점이 올랐다는 것으로는 증명될 수 없다. 이는 다음의 다층적인 지표들이 모두 긍정적인 신호를 보일 때만 유효하며, 이를 통해 엄격한 **회귀 테스트 게이트(Regression Gate)**를 구축해야 한다.</p>
<ol>
<li><strong>구조적 통과 (Structural Pass):</strong> JSON 스키마 검증률과 타입 무결성을 99.9% 이상 유지하며, 복잡한 중첩 구조에서도 안정성을 확보해야 한다.</li>
<li><strong>기능적 향상 (Functional Gain):</strong> 통계적으로 유의미한(Unbiased) Pass@k 점수의 상승이 있어야 하며, Failure@k 감소를 통해 모드 붕괴가 없음을 증명해야 한다.</li>
<li><strong>의미론적 보존 (Semantic Preservation):</strong> 임베딩 유사도와 LLM 심판(G-Eval)을 통해 텍스트의 뉘앙스와 사실성이 유지되거나 향상되었음을 확인해야 한다.</li>
<li><strong>강건성 확보 (Robustness):</strong> Turbulence 지표(정확성-일관성)의 하락이 없어야 하며, 입력의 미세한 변화에도 출력이 안정적이어야 한다.</li>
<li><strong>통계적 유의성 검증 (Statistical Significance):</strong> SNR이 확보된 상태에서 신뢰구간(CI)을 벗어나는 성능 향상을 확인하고, 클러스터 표준오차를 고려하여 보수적으로 판단해야 한다.</li>
</ol>
<p>이러한 객관적이고 통계적인 지표 체계를 갖출 때 비로소 AI 개발 조직은 막연한 ’체감 성능’이 아닌, 공학적 데이터에 기반한 의사결정을 내릴 수 있게 되며, 지속 가능한 AI 시스템의 발전을 담보할 수 있다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Schema-Agnostic Data Type Inference and Validation for … - MDPI, https://www.mdpi.com/2075-5309/15/17/3159</li>
<li>Json Correctness | DeepEval by Confident AI - The LLM Evaluation, https://deepeval.com/docs/metrics-json-correctness</li>
<li>How Oracle is Bridging the Gap Between JSON Schema and, https://json-schema.org/blog/posts/oracle-case-study</li>
<li>DeepJSONEval: Benchmarking Complex Nested JSON Data Mining, https://arxiv.org/html/2509.25922v1</li>
<li>A Benchmark and Evaluation Methodology for Complex Structured, https://arxiv.org/html/2602.12247</li>
<li>How to evaluate LLM outputs: A practical guide to AI Evals - Futurice, https://www.futurice.com/blog/ai-evals-practical-guide-part-1</li>
<li>LLM evaluation metrics and methods - Evidently AI, https://www.evidentlyai.com/llm-guide/llm-evaluation-metrics</li>
<li>How to evaluate LLM outputs: A practical guide to AI Evals - Futurice, https://www.futurice.com/sv/blog/ai-evals-practical-guide-part-1</li>
<li>Evaluating Large Language Models Trained on Code - arXiv.org, https://arxiv.org/pdf/2107.03374</li>
<li>Pass@k: A Practical Metric for Evaluating AI-Generated Code, https://medium.com/@ipshita/pass-k-a-practical-metric-for-evaluating-ai-generated-code-18462308afbd</li>
<li>Statistics for AI/ML, Part 4: pass@k and Unbiased Estimator - Han Lee, https://leehanchung.github.io/blogs/2025/09/08/pass-at-k/</li>
<li>HumanEval | DeepEval by Confident AI - The LLM Evaluation, https://deepeval.com/docs/benchmarks-human-eval</li>
<li>Pass@<span class="math math-inline">k</span> Failure Rate in LLM Evaluation - Emergent Mind, https://www.emergentmind.com/topics/pass-k-failure-rate</li>
<li>MULTI-LINGUAL EVALUATION OF CODE GENERATION MODELS, https://cogcomp.seas.upenn.edu/papers/AGWLTTAWSSGDKFFJGQRNRBDRX23.pdf</li>
<li>HumanEval &amp; MBPP: Setting the Standard for Code Generation, https://verityai.co/blog/humaneval-mbpp-code-generation-benchmarks</li>
<li>Evaluating Large Language Models on Self-invoking Code Generation, https://arxiv.org/html/2412.21199v2</li>
<li>LLM evaluation: Metrics, frameworks, and best practices - Wandb, https://wandb.ai/onlineinference/genai-research/reports/LLM-evaluation-Metrics-frameworks-and-best-practices–VmlldzoxMTMxNjQ4NA</li>
<li>LLM Evaluation Metrics: The Ultimate LLM Evaluation Guide, https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation</li>
<li>A Comprehensive Guide to LLM Evaluations - Caylent, https://caylent.com/blog/a-comprehensive-guide-to-llm-evaluations</li>
<li>LLM Evaluators: Tutorial &amp; Best Practices - Patronus AI, https://www.patronus.ai/llm-testing/llm-evaluators</li>
<li>LLM Evaluation: Key Concepts &amp; Best Practices - Nexla, https://nexla.com/ai-readiness/llm-evaluation/</li>
<li>evaluating-llms | Skills Marketplace - LobeHub, https://lobehub.com/zh/skills/ancoleman-ai-design-components-evaluating-llms</li>
<li>Signal and Noise: Reducing uncertainty in language model evaluation, https://allenai.org/blog/signal-noise</li>
<li>A Framework for Reducing Uncertainty in Language Model Evaluation, https://arxiv.org/html/2508.13144v1</li>
<li>Signal and Noise in LLM Evaluation - Emergent Mind, https://www.emergentmind.com/topics/signal-and-noise-framework</li>
<li>Measuring the Signal to Noise Ratio in Language Model Evaluation, https://github.com/allenai/signal-and-noise</li>
<li>A statistical approach to model evaluations \ Anthropic, https://www.anthropic.com/research/statistical-approach-to-model-evals</li>
<li>Recommendations and Reporting Checklist for Rigorous … - arXiv, https://www.arxiv.org/pdf/2506.13776</li>
<li>Turbulence: Systematically and Automatically Testing Instruction, https://www.doc.ic.ac.uk/~afd/papers/2025/ICST-Turbulence.pdf</li>
<li>Evaluating Correct-Consistency and Robustness in Code, https://www.computer.org/csdl/proceedings-article/icst/2025/10988971/26S4L2PTqvu</li>
<li>The “Question Neighbourhood” Approach for Systematic Evaluation, https://www.computer.org/csdl/journal/ts/2025/11/11175086/2adNMzJEjkI</li>
<li>TOGLL: Correct and Strong Test Oracle Generation with LLMS, https://ieeexplore.ieee.org/iel8/11029684/11029718/11029748.pdf</li>
<li>TOGLL: Correct and Strong Test Oracle Generation with LLMs - arXiv, https://arxiv.org/abs/2405.03786</li>
<li>Correct and Strong Test Oracle Generation with LLMs, https://www.themoonlight.io/en/review/togll-correct-and-strong-test-oracle-generation-with-llms</li>
<li>Nexus: Execution-Grounded Multi-Agent Test Oracle Synthesis - arXiv, https://arxiv.org/html/2510.26423v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>