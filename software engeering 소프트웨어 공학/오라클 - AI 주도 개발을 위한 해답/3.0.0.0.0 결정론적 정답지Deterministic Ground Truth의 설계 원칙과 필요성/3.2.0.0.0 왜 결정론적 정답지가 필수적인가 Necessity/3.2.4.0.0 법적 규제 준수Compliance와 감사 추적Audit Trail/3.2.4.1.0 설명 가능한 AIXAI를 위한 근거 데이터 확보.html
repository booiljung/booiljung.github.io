<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.2.4.1 설명 가능한 AI(XAI)를 위한 근거 데이터 확보</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.2.4.1 설명 가능한 AI(XAI)를 위한 근거 데이터 확보</h1>
                    <nav class="breadcrumbs"><a href="../../../../../index.html">Home</a> / <a href="../../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../../index.html">Chapter 3. 결정론적 정답지(Deterministic Ground Truth)의 설계 원칙과 필요성</a> / <a href="../index.html">3.2 왜 결정론적 정답지가 필수적인가? (Necessity)</a> / <a href="index.html">3.2.4 법적 규제 준수(Compliance)와 감사 추적(Audit Trail)</a> / <span>3.2.4.1 설명 가능한 AI(XAI)를 위한 근거 데이터 확보</span></nav>
                </div>
            </header>
            <article>
                <h1>3.2.4.1 설명 가능한 AI(XAI)를 위한 근거 데이터 확보</h1>
<p>현대 소프트웨어 공학, 특히 인공지능(AI)이 통합된 시스템의 개발 및 검증 과정에서 가장 난해하면서도 필수적인 도전 과제는 바로 ’설명의 정당성(Validity of Explanation)’을 입증하는 것이다. 전통적인 소프트웨어 테스트나 지도 학습(Supervised Learning) 기반의 AI 모델 훈련에서는 입력값 <span class="math math-inline">x</span>에 대한 정답 레이블 <span class="math math-inline">y_{GT}</span>가 명확하게 존재한다. 이를 통해 모델의 예측값 <span class="math math-inline">\hat{y}</span>와 정답 <span class="math math-inline">y_{GT}</span> 간의 오차를 계산하고, 이를 기반으로 정확도(Accuracy)를 산출하거나 손실 함수(Loss Function)를 최소화하는 방향으로 모델을 최적화하는 것이 가능하다.</p>
<p>그러나 AI 모델이 “왜 <span class="math math-inline">y</span>라고 판단했는가?“라는 질문에 대해 제시하는 설명(<span class="math math-inline">E</span>)의 영역으로 넘어오면 상황은 완전히 달라진다. 모델의 예측 결과에 대한 정답(Ground Truth for Prediction)은 존재하지만, 설명 그 자체에 대한 정답(Ground Truth for Explanation)은 자연 상태의 데이터셋에 내재되어 있지 않기 때문이다. 예를 들어, 이미지 분류 모델이 ’고양이’를 식별할 때, 귀의 형상이 30%, 수염이 20%, 털의 질감이 50% 기여했는지, 아니면 귀가 40% 기여했는지를 확정적으로 말해줄 수 있는 객관적 기준 데이터는 현실 세계의 데이터 수집 과정에서 확보되지 않는다.</p>
<p>이러한 ‘설명의 정답지 부재’ 문제는 XAI 기술의 신뢰성을 근본적으로 위협한다. SHAP(Shapley Additive exPlanations)이나 LIME(Local Interpretable Model-agnostic Explanations)과 같은 널리 사용되는 XAI 도구들이 생성하는 히트맵(Heatmap)이나 특성 중요도(Feature Importance) 점수를 맹목적으로 신뢰하는 것은 위험하다. 연구 결과에 따르면, 모델이 실제로는 배경의 노이즈나 엉뚱한 패턴을 보고 예측을 수행했음에도 불구하고, XAI 도구가 마치 모델이 객체의 핵심 특징을 인지한 것처럼 그럴듯한(Plausible) 설명을 생성해내는 ‘설명의 환각’ 현상이 빈번하게 발생한다. 설명이 인간에게 직관적으로 이해된다고 해서 그것이 모델의 내부 동작을 충실하게(Faithful) 반영하고 있다고 보장할 수는 없기 때문이다.</p>
<p>따라서 신뢰할 수 있는 결정론적 오라클(Deterministic Oracle)을 구축하기 위해서는 설명 가능한 AI를 수학적, 논리적으로 검증할 수 있는 **결정론적 근거 데이터(Deterministic Ground Truth for Explanation)**를 인위적으로 확보하고 설계하는 과정이 선행되어야 한다. 이는 단순히 데이터를 수집하는 차원을 넘어, 인과 관계(Causality)가 통제된 환경을 구축하고, 반사실적(Counterfactual) 추론이 가능한 검증 체계를 수립하며, 법적 규제 준수를 위한 감사 추적(Audit Trail) 데이터를 확보하는 다층적인 전략을 요구한다.</p>
<p><img src="./3.2.4.1.0%20%EC%84%A4%EB%AA%85%20%EA%B0%80%EB%8A%A5%ED%95%9C%20AIXAI%EB%A5%BC%20%EC%9C%84%ED%95%9C%20%EA%B7%BC%EA%B1%B0%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%ED%99%95%EB%B3%B4.assets/image-20260218212532323.jpg" alt="image-20260218212532323" /></p>
<h2>1.  설명 검증을 위한 합성 인공지능 정답지(SAIG) 방법론</h2>
<p>자연 데이터에는 ’설명의 정답’이 포함되어 있지 않다는 한계를 극복하기 위해 가장 강력하게 대두되는 전략은 <strong>합성 인공지능 정답지(Synthetic Artificial Intelligence Ground Truth, SAIG)</strong> 방법론이다. SAIG는 입력 데이터와 출력 레이블 사이의 인과 관계를 설계자가 완벽하게 통제하고 알고 있는 상태에서 합성 데이터를 생성하여 XAI 도구의 성능을 평가하는 접근법이다. 이는 실제 데이터의 불확실성을 제거하고, XAI 도구가 설계된 인과 관계를 정확하게 찾아내는지를 측정하는 ‘리트머스 시험지’ 역할을 한다.</p>
<h3>1.1  인과 관계가 제어된 데이터 생성 (Controlled Causality)</h3>
<p>SAIG의 핵심은 <span class="math math-inline">y = f(x)</span>라는 함수 관계를 사전에 정의하고 강제하는 것이다. 일반적인 머신러닝 문제에서는 데이터로부터 <span class="math math-inline">f(x)</span>를 추론하지만, XAI 검증을 위한 SAIG 환경에서는 <span class="math math-inline">f(x)</span>를 미리 결정해두고 데이터셋을 생성한다. 예를 들어, 금융권의 신용 평가 모델을 테스트하는 시나리오를 가정해보자. 실제 고객 데이터에서는 연소득, 부채 비율, 연체 기록, 직업군 등 수많은 변수가 복잡하게 상호작용하여 대출 승인 여부가 결정되므로, 특정 변수의 기여도를 단정하기 어렵다. 그러나 검증을 위한 합성 데이터셋에서는 다음과 같은 명시적 규칙(Rule)을 기반으로 데이터를 생성한다.</p>
<blockquote>
<p>“대출 거절(Target=1)은 오직 ’연체 횟수 <span class="math math-inline">\ge</span> 3’인 경우에만 발생한다. 소득이나 나이와 같은 나머지 변수는 결과에 영향을 주지 않는 교란 변수(Confounder) 혹은 노이즈(Noise)다.”</p>
</blockquote>
<p>이러한 규칙에 따라 생성된 데이터셋으로 학습된 모델을 XAI 도구로 분석했을 때, 만약 SHAP이나 LIME이 결과에 무관한 ‘소득’ 변수를 대출 거절의 주요 원인으로 지목한다면, 해당 XAI 도구는 **충실성(Faithfulness)**이 결여된 것으로 판정할 수 있다. 즉, SAIG는 모델이 학습한 패턴이 아니라, XAI 도구가 그 패턴을 올바르게 해석해냈는지를 검증하는 기준점이 된다.</p>
<p>연구 문헌에 따르면, 이러한 접근 방식은 특히 특성 억제(Suppressor) 변수나 다중공선성이 존재하는 복잡한 데이터 환경에서 XAI 도구가 실제로 유효한 설명을 제공하는지 평가하는 데 결정적인 역할을 한다. 정형 데이터(Tabular Data)의 경우, 선형 결합이나 특정 논리 회로(Boolean Logic)를 기반으로 데이터셋을 구성하여 각 특성의 이론적 기여도를 정확히 계산해 놓은 상태에서 XAI 산출물과 비교하는 방식이 널리 사용된다.</p>
<h3>1.2  이미지 데이터에서의 합성 시각적 정답지 (Synthetic Visual Ground Truth)</h3>
<p>비정형 데이터인 이미지의 경우, 픽셀 단위의 설명 정답지를 확보하기 위해 <strong>객체 삽입(Object Insertion)</strong> 또는 <strong>합성 패치(Synthetic Patch)</strong> 방식이 활용된다. 이는 무작위적인 배경 이미지나 텍스처 위에 특정 클래스를 결정짓는 시각적 요소(예: 빨간색 삼각형, 특정 로고, 인위적인 병변 등)를 합성하고, 모델이 오직 그 요소가 존재할 때만 특정 클래스로 분류하도록 학습시키는 방법이다.</p>
<p>이때, 설명의 정답지(Ground Truth Mask)는 합성된 객체가 위치한 정확한 픽셀 좌표 혹은 바운딩 박스(Bounding Box)가 된다. XAI 도구가 생성한 샐리언시 맵(Saliency Map)이 이 정답 마스크 영역을 얼마나 정확하게 커버하는지(Intersection over Union, IoU), 혹은 정답 영역 밖의 노이즈를 얼마나 배제하는지(Pointing Game)를 정량적으로 측정함으로써 설명의 정확도를 평가할 수 있다.</p>
<p>의료 AI 분야에서는 이러한 방식이 더욱 필수적이다. 예를 들어, 정상 흉부 X-ray 이미지에 폐 결절(Nodule)을 인위적으로 합성하여 생성한 데이터셋을 활용한다. 의사가 육안으로 확인한 병변 위치 정보를 정답지로 사용할 수도 있지만, 인간의 주석(Annotation)은 주관적이거나 부정확할 수 있다. 반면, 합성된 병변은 그 위치와 크기, 픽셀 강도를 수학적으로 완벽하게 알 수 있으므로 결정론적 검증이 가능하다. 이를 통해 AI 모델이 실제로 병변을 보고 질병을 진단했는지, 아니면 쇄골의 위치나 촬영 장비의 마크와 같은 엉뚱한 특징(Shortcut Learning)을 보고 판단했는지를 엄밀하게 구분해낼 수 있다.</p>
<h3>1.3  M4 벤치마크와 정량적 평가 지표</h3>
<p>합성 정답지를 활용한 평가 체계는 최근 <strong>M4 벤치마크</strong>와 같은 연구를 통해 표준화되고 있다. M4 벤치마크는 다양한 데이터 모달리티(이미지, 텍스트)와 모델 아키텍처(CNN, Transformer 등)에 대해 XAI 방법론의 충실성을 평가하기 위한 통합 프레임워크를 제공한다.</p>
<p>이러한 벤치마크에서 활용되는 주요 지표는 다음과 같다:</p>
<ul>
<li><strong>SynScore (Synthetic Score):</strong> 합성된 특징(Ground Truth Feature)이 XAI 중요도 상위 <span class="math math-inline">k</span>개에 포함되는 비율을 측정한다.</li>
<li><strong>MoRF (Most Relevant First):</strong> XAI가 중요하다고 판단한 특성을 순차적으로 제거했을 때, 모델의 예측 확률이 얼마나 급격하게 떨어지는지를 측정한다. 충실한 설명이라면 중요한 특성을 제거했을 때 예측값이 크게 변해야 한다.</li>
<li><strong>LeRF (Least Relevant First):</strong> 반대로, 중요하지 않다고 판단한 특성을 제거했을 때 예측값이 변하지 않아야 함을 측정한다.</li>
</ul>
<p>이러한 정량적 지표들은 “설명이 그럴듯해 보인다“는 주관적 평가를 넘어, “설명이 모델의 결정을 수학적으로 얼마나 정확하게 추적하는가“를 수치화하여 제시한다. 특히 일부 유명한 XAI 기법들이 시각적으로는 깔끔한 히트맵을 생성하지만, 실제 픽셀 단위의 중요도 검증(SynScore)에서는 낮은 점수를 기록한다는 사실은 결정론적 정답지 기반 검증의 중요성을 시사한다.</p>
<p><img src="./3.2.4.1.0%20%EC%84%A4%EB%AA%85%20%EA%B0%80%EB%8A%A5%ED%95%9C%20AIXAI%EB%A5%BC%20%EC%9C%84%ED%95%9C%20%EA%B7%BC%EA%B1%B0%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%ED%99%95%EB%B3%B4.assets/image-20260218212556347.jpg" alt="image-20260218212556347" /></p>
<h2>2.  반사실적(Counterfactual) 추론을 통한 결정론적 검증 오라클</h2>
<p>합성 데이터는 모델 개발 단계에서의 단위 테스트(Unit Test)에는 유용하지만, 실제 운영 환경(Production)의 리얼 데이터에 대한 설명을 검증하는 데는 한계가 있다. 운영 중인 모델에서 실시간으로 설명의 타당성을 확보하기 위해 사용되는 가장 확실한 방법은 **반사실적 설명(Counterfactual Explanation)**을 활용한 결정론적 검증이다.</p>
<h3>2.1  최소 변화를 통한 인과성 확정 (Deterministic Causality via Perturbation)</h3>
<p>반사실적 설명은 “만약 입력값 <span class="math math-inline">x</span>가 <span class="math math-inline">x&#39;</span>로 아주 조금 바뀌었을 때, 출력값 <span class="math math-inline">y</span>가 <span class="math math-inline">y&#39;</span>로 바뀌는가?“라는 질문에 답하는 과정이다. 이는 상관관계가 아닌 인과관계(Causality)를 직접적으로 테스트하는 방식이므로, 설명 오라클의 강력한 근거가 된다.</p>
<p>만약 XAI 도구(예: SHAP)가 특정 입력 데이터 <span class="math math-inline">x</span>에 대해 “특성 <span class="math math-inline">A</span>(<span class="math math-inline">x_A</span>)가 예측 결과(예: 대출 거절)의 결정적 원인이다“라고 설명했다면, 이를 검증하는 오라클은 다음과 같은 논리로 동작해야 한다.</p>
<ol>
<li><strong>가설 수립:</strong> <span class="math math-inline">x_A</span>가 원인이라면, <span class="math math-inline">x_A</span>의 값을 대출 승인에 유리한 방향으로 변경했을 때 결과가 바뀌어야 한다.</li>
<li><strong>반사실적 데이터 생성:</strong> 원본 데이터 <span class="math math-inline">x</span>에서 오직 <span class="math math-inline">x_A</span>만을 변경한 새로운 데이터 <span class="math math-inline">x&#39;</span>를 생성한다(<span class="math math-inline">x&#39; = x \oplus \Delta_A</span>). 이때 다른 변수들은 고정(Ceteris Paribus)되어야 한다.</li>
<li><strong>결정론적 검증:</strong> 모델 <span class="math math-inline">f</span>에 <span class="math math-inline">x&#39;</span>를 입력하여 <span class="math math-inline">f(x&#39;) \neq f(x)</span>인지 확인한다.</li>
</ol>
<ul>
<li><strong>검증 성공:</strong> 예측 결과가 ’거절’에서 ’승인’으로 바뀌었다면, 특성 <span class="math math-inline">A</span>는 해당 결정의 결정론적 원인(Deterministic Cause)임이 입증된다. XAI의 설명은 **참(True)**이다.</li>
<li><strong>검증 실패:</strong> 특성 <span class="math math-inline">A</span>를 변경했음에도 결과가 변하지 않는다면, 특성 <span class="math math-inline">A</span>는 결정적 요인이 아니다. XAI의 설명은 **거짓(False)**이거나, 모델이 다른 변수(상호작용항 등)에 더 크게 의존하고 있음을 의미한다.</li>
</ul>
<p>이러한 검증 프로세스는 모델의 내부 구조를 알 필요가 없는 <strong>모델 불가지론적(Model-Agnostic)</strong> 오라클을 가능하게 한다. 특히, 반사실적 설명은 사용자에게 “어떻게 하면 원하는 결과를 얻을 수 있는가?(Recourse)“에 대한 행동 지침을 제공하므로, GDPR이나 EU AI Act에서 요구하는 설명 권리를 충족하는 데에도 효과적이다.</p>
<h3>2.2  현실적인 반사실적 생성과 인과적 제약 조건</h3>
<p>반사실적 검증 시 주의할 점은 생성된 반사실적 데이터 <span class="math math-inline">x&#39;</span>가 현실 세계에서 발생 가능한 데이터여야 한다는 점이다. 예를 들어, “나이를 5살 줄이면 대출이 승인된다“라는 반사실적 설명은 수학적으로는 모델의 결정 경계(Decision Boundary)를 넘길 수 있을지 몰라도, 현실적으로는 불가능한 행동 지침이다. 또한, “소득을 늘려라“라는 변화가 발생할 때, 현실에서는 “부채 상환 능력 증가“나 “신용 점수 상승“과 같은 다른 변수들의 변화가 수반될 수 있다.</p>
<p>따라서 고도화된 설명 오라클은 변수 간의 **인과적 종속성(Causal Dependency)**을 고려하여 반사실적 데이터를 생성해야 한다. 예를 들어, 구조적 인과 모델(Structural Causal Model, SCM)을 기반으로 “변수 A가 변하면 변수 B도 종속적으로 변한다“는 제약 조건을 오라클에 주입함으로써, 더욱 현실적이고 신뢰할 수 있는 근거 데이터를 생성할 수 있다. 이는 단순한 섭동(Perturbation) 기반의 검증을 넘어, 도메인 지식이 반영된 논리적 검증을 가능하게 한다.</p>
<h2>3.  법적 규제 준수를 위한 데이터 무결성 및 감사 추적 (Audit Trail)</h2>
<p>설명 가능한 AI를 위한 근거 데이터 확보는 기술적 검증을 넘어 법적 의무 사항으로 진화하고 있다. EU AI 법(EU AI Act)과 같은 최신 규제는 고위험(High-Risk) AI 시스템에 대해 투명성(Transparency), 인간 감독(Human Oversight), 그리고 기술적 문서화(Technical Documentation)를 강력하게 요구한다. 특히 Article 12(기록 보관)와 Article 86(설명 권리)은 단순한 성능 지표(Accuracy) 기록을 넘어, <strong>개별 의사결정의 근거 데이터</strong>를 상세히 보존할 것을 명령한다.</p>
<h3>3.1  감사 가능한 설명 로그(Explainability Logs) 구축</h3>
<p>규제 준수와 사후 감사를 위해 확보해야 할 근거 데이터는 모델 학습 데이터뿐만 아니라, 실제 운영 단계(Inference Phase)에서 발생하는 모든 설명적 맥락(Context)을 포함해야 한다. 결정론적 오라클은 다음과 같은 <strong>설명 로그(Explainability Logs)</strong> 스키마를 정의하고, 이를 불변(Immutable) 저장소에 기록하여 데이터 무결성을 입증해야 한다.</p>
<ol>
<li><strong>입력 스냅샷(Input Snapshot):</strong> 추론 시점에 입력된 정확한 데이터 벡터 원본. 타임스탬프와 함께 해시(Hash) 처리되어 위변조가 불가능해야 한다.</li>
<li><strong>참조 데이터 및 기준점(Reference Data &amp; Baseline):</strong> 많은 XAI 알고리즘(예: SHAP, Integrated Gradients)은 입력값을 특정 ’기준점(Baseline)’과 비교하여 기여도를 산출한다. 기준점이 ’평균값’인지 ’0 벡터’인지에 따라 설명이 달라지므로, 당시 사용된 기준 데이터셋에 대한 메타데이터가 필수적으로 기록되어야 한다.</li>
<li><strong>모델 및 XAI 구성 정보(Configuration):</strong> 의사결정에 사용된 모델의 버전(Version ID), 사용된 XAI 알고리즘의 종류(예: KernelSHAP vs TreeSHAP), 그리고 설명 생성에 사용된 하이퍼파라미터(예: 섭동 횟수, 커널 크기) 정보.</li>
<li><strong>검증 결과(Verification Result):</strong> 해당 설명이 반사실적 테스트나 일관성(Consistency) 체크를 통과했는지 여부와 그 결과값. 예를 들어, “입력 특성 A를 제거했을 때 예측 확률이 80%에서 30%로 하락함“과 같은 구체적인 검증 증거가 포함되어야 한다.</li>
<li><strong>인적 개입 기록(Human-in-the-Loop Logs):</strong> 만약 AI의 결정이나 설명에 대해 운영자가 개입하거나 수정한 경우, 그 사유와 변경 내용을 기록하여 책임 소재를 명확히 해야 한다.</li>
</ol>
<p>이러한 로그 데이터는 금융감독기관이나 인증 기관의 감사(Audit) 시, AI가 과거에 왜 그런 판단을 내렸는지 역추적(Traceability)하고, 그 설명이 사후에 조작되지 않았음을 증명하는 디지털 포렌식 증거(Digital Forensic Evidence)로 활용된다.</p>
<p><img src="./3.2.4.1.0%20%EC%84%A4%EB%AA%85%20%EA%B0%80%EB%8A%A5%ED%95%9C%20AIXAI%EB%A5%BC%20%EC%9C%84%ED%95%9C%20%EA%B7%BC%EA%B1%B0%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%ED%99%95%EB%B3%B4.assets/image-20260218212622422.jpg" alt="image-20260218212622422" /></p>
<h3>3.2  보안 위협 대응: 트로이목마(Trojan) 및 백도어 탐지를 위한 근거 데이터</h3>
<p>근거 데이터 확보는 보안(Security) 관점에서도 결정적인 역할을 한다. 최근 AI 모델에 대한 공격 기법 중 하나인 ’데이터 중독(Data Poisoning)’이나 ’백도어(Backdoor) 공격’은 모델이 특정 트리거(Trigger) 패턴(예: 이미지 구석의 작은 점, 텍스트의 특정 단어)이 입력될 때만 오동작하도록 조작한다. 일반적인 정확도 테스트 데이터셋으로는 이러한 백도어를 탐지할 수 없다.</p>
<p>이때 **‘공격 시나리오 기반 근거 데이터셋(Adversarial Ground Truth Dataset)’**이 필요하다. 이는 의도적으로 트리거가 심어진 데이터를 포함하며, 해당 트리거가 결과 예측의 결정적 원인이라는 ’설명 정답지’를 갖는다. 정상적인 XAI 도구라면 이러한 데이터가 입력되었을 때, 모델이 트리거 패턴에 과도하게 집중하고 있음을 시각화하거나 중요도로 표시해야 한다. 만약 XAI 도구가 트리거를 무시하고 엉뚱한 정상 특성을 설명의 근거로 제시한다면, 이는 설명 시스템 자체가 공격에 취약하거나 신뢰할 수 없음을 의미한다. 따라서 결정론적 오라클은 주기적으로 이러한 적대적 예제를 주입하여, XAI가 은밀한 조작 시도조차 투명하게 드러낼 수 있는지(Robustness)를 검증해야 한다. 이는 AI 시스템의 무결성을 입증하는 강력한 방어 기제가 된다.</p>
<h2>4.  요약</h2>
<p>결론적으로, 설명 가능한 AI(XAI)를 위한 근거 데이터 확보는 블랙박스 모델의 불투명성을 걷어내고 시스템의 신뢰성을 보장하기 위한 필수 공학적 절차다. 우리는 AI 모델의 예측 결과(<span class="math math-inline">y</span>)를 검증하는 단계를 넘어, AI가 제시하는 설명(<span class="math math-inline">E</span>)의 참/거짓을 검증하는 단계로 진입해야 한다. 이를 위해 본 절에서는 세 가지 핵심적인 근거 데이터 확보 전략을 제시했다.</p>
<ol>
<li><strong>합성 인공지능 정답지(SAIG):</strong> 인과 관계가 설계자에 의해 완벽하게 통제된 합성 데이터를 생성하여, XAI 도구가 설계된 인과성을 정확히 포착하는지 정량적으로(SynScore 등) 검증한다.</li>
<li><strong>반사실적(Counterfactual) 검증:</strong> 운영 중인 모델에서 입력값의 미세한 변화(Perturbation)에 따른 출력값의 변화를 관찰함으로써, 특정 특성이 결과의 결정론적 원인인지를 실시간으로 입증한다.</li>
<li><strong>규제 준수 및 감사 로그:</strong> EU AI Act 등 법적 요구사항을 충족하기 위해, 입력, 모델, 참조 데이터, 설명 메타데이터 등 의사결정의 전 과정을 재구성할 수 있는 상세한 로그를 불변 저장소에 확보한다.</li>
</ol>
<p>이러한 결정론적 근거 데이터와 검증 체계가 구축될 때, AI 소프트웨어는 단순한 ’성능 좋은 도구’를 넘어, 법적 책임성과 사회적 신뢰를 갖춘 ’설명 가능한 파트너’로서 온전히 기능할 수 있다.</p>
<h2>5. 참고 자료</h2>
<ol>
<li>What Is Ground Truth in Machine Learning? | IBM, https://www.ibm.com/think/topics/ground-truth</li>
<li>Position: Explainable AI is Causal Discovery in Disguise, https://openreview.net/forum?id=I7nESnBvib</li>
<li>Evaluating The Correctness of Explainable AI | PDF | Formalism …, https://www.scribd.com/document/717670163/Evaluating-the-Correctness-of-Explainable-AI</li>
<li>What Do You See? Evaluation of Explainable Artificial Intelligence, https://beerkay.github.io/papers/Berkay2021XAITrojan.pdf</li>
<li>Scrutinizing XAI using linear ground-truth data with suppressor, https://pmc.ncbi.nlm.nih.gov/articles/PMC9123083/</li>
<li>EXplainable Artificial Intelligence (XAI)—From Theory to Methods, https://ieeexplore.ieee.org/iel8/6287639/10380310/10549884.pdf</li>
<li>Exploring SAIG Methods for an Objective Evaluation of XAI - arXiv, https://arxiv.org/abs/2602.08715</li>
<li>[Literature Review] Exploring SAIG Methods for an Objective, https://www.themoonlight.io/en/review/exploring-saig-methods-for-an-objective-evaluation-of-xai</li>
<li>Evaluating Anomaly Explanations Using Ground Truth - ResearchGate, https://www.researchgate.net/publication/385856628_Evaluating_Anomaly_Explanations_Using_Ground_Truth</li>
<li>M4: A Unified XAI Benchmark for Faithfulness Evaluation … - NeurIPS, https://proceedings.neurips.cc/paper_files/paper/2023/file/05957c194f4c77ac9d91e1374d2def6b-Paper-Datasets_and_Benchmarks.pdf</li>
<li>A Comprehensive Review of Explainable Artificial Intelligence (XAI, https://www.mdpi.com/1424-8220/25/13/4166</li>
<li>XAI Benchmarking - GitHub Pages, https://xaidataset.github.io/</li>
<li>Counterfactual vs Attribution Explanations | Fiddler AI Blog, https://www.fiddler.ai/blog/counterfactual-explainable-vs-attribution-based-explanations</li>
<li>How counterfactuals improve AI trust - Telnyx, https://telnyx.com/learn-ai/counterfactual-explanations</li>
<li>Generating Causally Compliant Counterfactual Explanations using, https://cgi.cse.unsw.edu.au/~eptcs/paper.cgi?ICLP2024.30.pdf</li>
<li>CounterBench: A Benchmark for Counterfactuals Reasoning … - arXiv, https://arxiv.org/html/2502.11008v1</li>
<li>EU AI Act: A Complete Guide for Enterprise Architects - Ardoq, https://www.ardoq.com/knowledge-hub/eu-ai-act</li>
<li>Effective implementation of requirements for high-risk AI systems, https://cerre.eu/wp-content/uploads/2025/02/Effective-Implementation-of-Requirements-for-High-Risk-AI-Systems-Under-the-AI-Act_FINAL-1.pdf</li>
<li>AI Act Service Desk - Article 12: Record-keeping - European Union, https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-12</li>
<li>Is Your AI Logging Article 12-Ready? Avoid EU Compliance Gaps, https://www.isms.online/iso-42001/eu-ai-act/article-12/</li>
<li>Record-Keeping - Practical AI Act Guide, https://practical-ai-act.eu/latest/conformity/record-keeping/</li>
<li>Explainable AI for EU AI Act compliance audits, https://mab-online.nl/article/150303/</li>
<li>Explainable AI for Digital Forensics: Ensuring Transparency in Legal, https://www.forensicscijournal.com/jfsr/article/download/jfsr-aid1089/pdf/7988</li>
<li>Explainable AI for Digital Forensics: Ensuring Transparency in Legal, https://www.forensicscijournal.com/journals/jfsr/jfsr-aid1089.php</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>