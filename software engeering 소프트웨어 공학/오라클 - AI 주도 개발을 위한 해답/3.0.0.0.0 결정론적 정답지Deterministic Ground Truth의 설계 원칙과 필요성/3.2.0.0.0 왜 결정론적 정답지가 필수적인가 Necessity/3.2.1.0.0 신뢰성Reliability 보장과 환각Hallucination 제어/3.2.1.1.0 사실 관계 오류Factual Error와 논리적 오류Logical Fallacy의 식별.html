<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.2.1.1 사실 관계 오류(Factual Error)와 논리적 오류(Logical Fallacy)의 식별</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.2.1.1 사실 관계 오류(Factual Error)와 논리적 오류(Logical Fallacy)의 식별</h1>
                    <nav class="breadcrumbs"><a href="../../../../../index.html">Home</a> / <a href="../../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../../index.html">Chapter 3. 결정론적 정답지(Deterministic Ground Truth)의 설계 원칙과 필요성</a> / <a href="../index.html">3.2 왜 결정론적 정답지가 필수적인가? (Necessity)</a> / <a href="index.html">3.2.1 신뢰성(Reliability) 보장과 환각(Hallucination) 제어</a> / <span>3.2.1.1 사실 관계 오류(Factual Error)와 논리적 오류(Logical Fallacy)의 식별</span></nav>
                </div>
            </header>
            <article>
                <h1>3.2.1.1 사실 관계 오류(Factual Error)와 논리적 오류(Logical Fallacy)의 식별</h1>
<h2>1. 서론: AI 소프트웨어 품질 보증을 위한 이원적 인식론</h2>
<p>현대 소프트웨어 공학의 패러다임이 결정론적 알고리즘(Deterministic Algorithm)의 작성에서 확률론적 모델(Probabilistic Model)의 조율로 이동함에 따라, ’오류(Error)’를 바라보는 관점 또한 근본적인 재정립을 요구받고 있다. 전통적인 소프트웨어 개발 생명주기(SDLC)에서 오류란 주로 구문론적 결함(Syntax Error)이나 개발자의 의도와 다르게 동작하는 논리적 버그(Logical Bug)를 지칭했다. 이러한 오류들은 코드의 실행 가능성이나 결과의 결정론적 재현성을 저해하는 요인으로, 디버거와 컴파일러라는 확정적 도구를 통해 식별과 수정이 가능했다. 그러나 대규모 언어 모델(Large Language Models, 이하 LLM)이 코드를 생성하고 시스템의 일부로 통합되는 현 시점에서, 우리는 ’환각(Hallucination)’이라는 새롭고도 모호한 불확실성에 직면해 있다. 이 환각은 단순히 틀린 답을 내놓는 것을 넘어, 존재하지 않는 라이브러리를 그럴듯하게 호출하거나, 논리적으로 타당해 보이지만 실질적으로는 모순된 비즈니스 로직을 구성하는 등 매우 교묘한 형태로 발현된다.</p>
<p>따라서 AI 기반 소프트웨어 개발에서 결정론적 정답지(Deterministic Ground Truth)를 설계하기 위한 첫 번째 단계는, AI가 범하는 오류의 본질을 해부하는 것이다. 특히, 외부 지식의 부재나 왜곡에서 기인하는 **‘사실 관계 오류(Factual Error)’**와, 주어진 정보 내에서의 추론 실패나 인과 관계의 혼동에서 기인하는 **‘논리적 오류(Logical Fallacy)’**를 명확히 구분하는 식별 체계가 필수적이다. 이 두 가지 오류 유형은 발생 메커니즘이 상이할 뿐만 아니라, 이를 탐지하고 제어하기 위해 요구되는 오라클(Oracle)의 속성 또한 완전히 다르다. 사실 관계 오류가 데이터베이스와의 대조(Reconciliation)를 요구한다면, 논리적 오류는 형식 증명(Formal Verification)과 추론 검증(Reasoning Validation)을 필요로 한다.</p>
<p>본 보고서에서는 AI 모델의 신뢰성을 저해하는 이 두 가지 핵심 축을 심층적으로 분석하고, 이를 결정론적으로 식별하기 위한 기술적 방법론과 오라클 설계 전략을 제시한다. 이는 단순히 오답을 걸러내는 필터링 기술을 넘어, 확률적인 AI 모델을 결정론적인 엔지니어링 파이프라인 안에서 통제 가능한 구성 요소로 안착시키기 위한 인식론적 토대가 될 것이다.</p>
<p><img src="./3.2.1.1.0%20%EC%82%AC%EC%8B%A4%20%EA%B4%80%EA%B3%84%20%EC%98%A4%EB%A5%98Factual%20Error%EC%99%80%20%EB%85%BC%EB%A6%AC%EC%A0%81%20%EC%98%A4%EB%A5%98Logical%20Fallacy%EC%9D%98%20%EC%8B%9D%EB%B3%84.assets/image-20260218191525433.jpg" alt="image-20260218191525433" /></p>
<h2>2.  사실 관계 오류(Factual Error)의 해부: 참조 무결성의 붕괴</h2>
<h3>2.1  사실 관계 오류의 정의와 발생 메커니즘</h3>
<p>AI, 특히 LLM 맥락에서의 사실 관계 오류는 모델이 생성한 출력이 객관적인 실세계의 사실(Real-world Facts)이나, 명시적으로 제공된 컨텍스트(Context), 혹은 확립된 지식 베이스(Knowledge Base)와 상충하는 현상을 의미한다. 이는 소프트웨어 엔지니어링 관점에서 볼 때, 데이터베이스 시스템의 **‘참조 무결성(Referential Integrity) 위반’**과 매우 유사한 성격을 띤다. 즉, 모델이 가리키는 대상(함수, 라이브러리, 데이터, 역사적 사실 등)이 실제로는 존재하지 않거나, 그 속성이 모델의 서술과 일치하지 않는 경우이다.</p>
<p>LLM은 근본적으로 ’확률적 앵무새(Stochastic Parrots)’로서 작동한다. 모델은 방대한 텍스트 데이터에서 학습한 통계적 패턴에 기반하여 다음에 올 가장 개연성 있는 토큰(Token)을 예측할 뿐, 그 토큰이 지시하는 대상의 진위 여부를 본질적으로 이해하거나 검증하지 않는다. 따라서 학습 데이터에 공백이 있거나(Missing Data), 희소한 지식(Long-tail Knowledge) 영역에 접근할 때, 모델은 ’모른다’고 답하는 대신 가장 그럴듯한 거짓 정보를 합성해낸다. 이를 흔히 ’환각(Hallucination)’이라 부르지만, 엄밀한 기술적 용어로는 <strong>‘지식 기반 오류(Knowledge-Grounded Error)’</strong> 또는 **‘사실성 결여(Lack of Factuality)’**로 정의하는 것이 타당하다.</p>
<h3>2.2  소프트웨어 개발에서의 사실 관계 오류 유형</h3>
<p>일반적인 자연어 처리(NLP) 작업에서의 사실 오류가 역사적 날짜나 인명 오기 등에 집중된다면, 소프트웨어 개발 영역에서의 사실 관계 오류는 시스템의 안정성과 보안을 직접적으로 위협하는 치명적인 형태로 나타난다.</p>
<h4>2.2.1  라이브러리 및 API 환각 (Library &amp; API Hallucination)</h4>
<p>가장 빈번하고 위험한 유형은 존재하지 않는 소프트웨어 패키지나 라이브러리, 혹은 함수를 호출하는 것이다. 연구에 따르면, LLM이 생성한 코드의 상당수가 실제로는 존재하지 않는 라이브러리를 <code>import</code>하거나, <code>pandas</code>와 같은 유명 라이브러리에 없는 가상의 함수(예: <code>read_excel_auto</code>)를 호출하는 것으로 나타났다. 이는 단순한 실행 오류(Runtime Error)를 넘어 **‘공급망 공격(Supply Chain Attack)’**의 빌미를 제공할 수 있다. 공격자가 LLM이 자주 환각하는 가상의 패키지 이름을 선점하여 악성 코드를 배포하는 ‘Slopsquatting’ 기법이 실제로 보고되고 있어, 사실 관계 오류의 식별은 보안 측면에서도 매우 중요하다.</p>
<h4>2.2.2  스펙 및 버전 불일치 (Spec &amp; Version Mismatch)</h4>
<p>모델은 라이브러리의 존재 여부는 맞추더라도, 구체적인 API 스펙에서 오류를 범하는 경우가 많다. 예를 들어, 특정 함수의 파라미터 순서를 바꾸거나, 필수 인자를 누락하거나, 반환 값(Return Type)의 형식을 잘못 가정하는 경우이다. 또한, 소프트웨어 생태계의 빠른 변화로 인해 이미 Deprecated된 함수를 최신 버전인 것처럼 사용하거나, Python 2.x 문법과 3.x 문법을 혼용하는 등 ’지식의 시의성(Temporality of Knowledge)’과 관련된 사실 오류도 빈번하다.</p>
<h4>2.2.3  내재적 오류와 외재적 오류의 구분</h4>
<p>사실 관계 오류를 체계적으로 식별하기 위해서는 이를 <strong>내재적(Intrinsic)</strong> 오류와 <strong>외재적(Extrinsic)</strong> 오류로 세분화해야 한다.</p>
<ul>
<li><strong>내재적 사실 오류 (Intrinsic Hallucination):</strong> 모델에게 제공된 입력 프롬프트나 RAG를 통해 검색된 문서(Context)와 직접적으로 모순되는 출력을 생성하는 경우다. 이는 모델의 ’독해 능력’이나 ’지시 이행 능력’의 실패로 볼 수 있으며, **신뢰성(Faithfulness)**의 문제이다. 예를 들어, 프롬프트에 “API 키는 환경 변수에서 가져오라“고 명시했으나, 코드에 하드코딩하는 경우가 이에 해당한다.</li>
<li><strong>외재적 사실 오류 (Extrinsic Hallucination):</strong> 입력된 컨텍스트에는 없지만, 모델이 자신의 사전 학습 데이터(Parametric Knowledge)에 의존하여 생성한 내용이 외부 세계의 사실과 다른 경우다. 이는 모델이 가진 ‘지식의 정확성’ 문제이며, **사실성(Factuality)**의 영역이다. 존재하지 않는 라이브러리를 임포트하는 것이 대표적인 예다.</li>
</ul>
<h3>2.3  사실 관계 오류 식별을 위한 오라클 전략: 지식 조회와 실행</h3>
<p>사실 관계 오류는 모델 내부의 추론만으로는 판별할 수 없다. 모델은 거짓 정보를 생성할 때도 매우 높은 확신도(Confidence Score)를 보이기 때문에, 확률값에 의존한 검증은 실패할 가능성이 높다. 따라서 결정론적 정답지를 제공하는 오라클은 반드시 시스템 외부의 **‘지식 원천(Source of Truth)’**과 연결되어야 한다.</p>
<h4>2.3.1  검색 기반 검증 (Retrieval-Based Verification)</h4>
<p>가장 확실한 식별 방법은 생성된 결과물에 포함된 고유 명사(Entity), 함수명, 라이브러리명 등을 추출하여 신뢰할 수 있는 외부 데이터베이스와 대조하는 것이다.</p>
<ul>
<li><strong>패키지 레지스트리 조회:</strong> Python의 PyPI, Node.js의 npm 등 공식 패키지 저장소에 쿼리를 보내 해당 패키지가 실재하는지 검증한다.</li>
<li><strong>API 문서 크롤링 및 인덱싱:</strong> 주요 라이브러리의 공식 문서를 벡터 데이터베이스화해 두고, LLM이 생성한 함수 호출 시그니처가 해당 문서의 명세와 일치하는지 비교 검증한다. 이는 RAG 시스템의 역방향 활용이라고 볼 수 있다.</li>
</ul>
<h4>2.3.2  실행 기반 검증 (Execution-Based Verification)</h4>
<p>코드는 그 자체로 실행 가능성이라는 강력한 사실 검증 수단을 내포하고 있다. 컴파일러나 인터프리터가 뱉어내는 에러 메시지는 사실 관계 오류를 식별하는 가장 명확한 오라클이다.</p>
<ul>
<li><strong>ImportError/ModuleNotFoundError 감지:</strong> 코드를 격리된 샌드박스 환경에서 실행했을 때, 모듈을 찾을 수 없다는 에러는 100% 라이브러리 환각을 의미한다.</li>
<li><strong>AttributeError/NameError 감지:</strong> 라이브러리는 존재하나 함수가 없다는 에러는 API 스펙에 대한 사실 오류를 지시한다.</li>
</ul>
<h2>3.  논리적 오류(Logical Fallacy)의 심층 분석: 추론의 단절</h2>
<h3>3.1  논리적 오류의 정의와 발생 메커니즘</h3>
<p>논리적 오류란 전제(Premise)로부터 결론(Conclusion)을 도출하는 추론 과정(Reasoning Process)에서의 결함을 의미한다. AI 모델이 생성한 코드가 문법적으로 완벽하고(Syntax Correct), 호출하는 라이브러리도 실재하며(Factually Correct), 실행도 정상적으로 되지만, 결과값이 의도와 다르거나 비즈니스 요구사항을 충족시키지 못하는 경우, 이는 논리적 오류에 해당한다.</p>
<p>LLM은 언어의 통계적 패턴을 학습했을 뿐, 엄밀한 논리적 규칙이나 인과 관계를 내재화한 것은 아니다. 따라서 복잡한 산술 연산, 다단계 계획(Planning), 제약 조건 충족 문제(Constraint Satisfaction Problem) 등에서 모델은 자주 논리적 비약을 저지른다. 이는 모델이 문제 해결을 위한 알고리즘적 사고(Algorithmic Thinking)를 하는 것이 아니라, 훈련 데이터에서 보았던 유사한 코드 조각들을 표면적으로 모방(Mimicry)하기 때문에 발생한다.</p>
<h3>3.2  소프트웨어 개발에서의 논리적 오류 유형</h3>
<p>소프트웨어 코드에서 논리적 오류는 매우 미묘하여, 런타임 에러를 발생시키지 않고 조용히 잘못된 결과를 낳는 ’침묵의 실패(Silent Failure)’로 이어지는 경우가 많다.</p>
<ol>
<li><strong>알고리즘 구현 오류 (Algorithmic Flaw):</strong> 정렬, 탐색, 수학적 계산 등 특정 알고리즘을 구현할 때 핵심 로직을 잘못 작성하는 경우다. 예를 들어, 퀵 정렬을 구현하면서 피벗(Pivot) 선택 로직을 잘못 작성하여 무한 재귀에 빠지거나 정렬이 완료되지 않는 경우, 혹은 반복문(Loop)의 종료 조건을 잘못 설정하여 무한 루프가 발생하는 경우가 이에 해당한다.</li>
<li><strong>경계값 조건 및 예외 처리 누락 (Edge Case Neglect):</strong> 일반적인 입력에 대해서는 잘 동작하지만, 0, 음수, 빈 리스트, 초대용량 파일 등 특수한 조건(Edge Case)에 대한 처리가 누락되거나 잘못된 경우다. “Off-by-one” 에러(반복 횟수를 1회 더하거나 덜 실행하는 오류)가 대표적인 예다.</li>
<li><strong>인과 관계 오류 (Causal Fallacy):</strong> 코드의 논리적 흐름이 전후 관계나 인과 관계를 위배하는 경우다. 예를 들어, 변수를 정의하기 전에 사용하거나, 파일을 닫은 후에 쓰기 작업을 시도하는 등의 리소스 관리 오류가 포함된다. 또한, 비즈니스 로직에서 “A이면 B이다“라는 명제를 “B이면 A이다“로 역(Converse)을 참으로 착각하여 구현하는 경우도 발생한다.</li>
<li><strong>순환 논증 (Circular Reasoning):</strong> 함수나 모듈이 자기 자신을 정당화의 근거로 삼는 구조적 오류다. 코드 주석 생성 시, “이 함수는 데이터를 처리합니다“라는 설명을 “데이터를 처리하기 때문에 이 함수입니다“라고 설명하는 등 동어반복적인 주석을 생성하거나, 재귀 호출의 기저 조건(Base Case) 없이 서로를 호출하는 함수 쌍을 생성하는 경우가 있다.</li>
</ol>
<h3>3.3  논리적 오류 식별을 위한 오라클 전략: 형식 증명과 테스트</h3>
<p>사실 관계 오류가 ’검색(Lookup)’을 통해 식별된다면, 논리적 오류는 **‘연산(Computation)’**과 **‘증명(Proof)’**을 통해 식별되어야 한다. 이는 정적 분석과 동적 테스트가 결합된 고도의 오라클을 필요로 한다.</p>
<p><img src="./3.2.1.1.0%20%EC%82%AC%EC%8B%A4%20%EA%B4%80%EA%B3%84%20%EC%98%A4%EB%A5%98Factual%20Error%EC%99%80%20%EB%85%BC%EB%A6%AC%EC%A0%81%20%EC%98%A4%EB%A5%98Logical%20Fallacy%EC%9D%98%20%EC%8B%9D%EB%B3%84.assets/image-20260218191548925.jpg" alt="image-20260218191548925" /></p>
<h4>3.3.1  정적 분석 및 형식 기법 (Static Analysis &amp; Formal Methods)</h4>
<p>코드를 실행하지 않고 그 구조와 의미를 수학적으로 분석하는 방법은 논리적 오류를 식별하는 가장 강력한 결정론적 수단이다.</p>
<ul>
<li><strong>추상 구문 트리(AST) 분석:</strong> 코드의 구문 트리를 순회하며 제어 흐름(Control Flow)과 데이터 흐름(Data Flow)을 분석한다. 예를 들어, <code>if</code> 문의 조건이 항상 <code>True</code>이거나(항진 명제), 도달 불가능한 코드(Dead Code)가 존재하는지, 변수가 초기화되지 않고 사용되는지 등을 식별한다.</li>
<li><strong>기호 실행(Symbolic Execution):</strong> 구체적인 값 대신 <span class="math math-inline">x, y</span>와 같은 기호(Symbol)를 입력으로 사용하여 프로그램의 실행 경로를 탐색한다. 이를 통해 특정 경로에서 ’0으로 나누기(Division by Zero)’나 ’배열 범위 초과(Index Out of Bounds)’와 같은 논리적 예외가 발생할 가능성이 있는지 수학적으로 증명한다.</li>
<li><strong>불변식(Invariant) 및 사전/사후 조건 검증:</strong> Hoare Logic에 기반하여, 함수 실행 전(Pre-condition)과 후(Post-condition), 그리고 루프 내부(Loop Invariant)에서 반드시 만족해야 하는 논리적 조건을 명세하고, 자동 정리 증명기(Automated Theorem Prover)나 SMT Solver(예: Z3)를 사용하여 이를 위반하는지 검사한다. 이는 논리적 오류를 수학적 모순으로 치환하여 탐지하는 방식이다.</li>
</ul>
<h4>3.3.2  결정론적 단위 테스트 (Deterministic Unit Testing)</h4>
<p>가장 실용적인 논리 오라클은 잘 설계된 테스트 케이스이다. 입력 <span class="math math-inline">X</span>에 대해 기대되는 출력 <span class="math math-inline">Y</span>가 명확히 정의된 테스트 케이스는 그 자체로 결정론적 정답지 역할을 한다. AI가 생성한 코드가 테스트를 통과하지 못한다면(<span class="math math-inline">Output \neq Y</span>), 이는 사실 관계의 문제가 아니라 논리적 변환 과정의 오류임이 명확해진다. 최근에는 ’속성 기반 테스트(Property-Based Testing)’를 도입하여, 특정 입력값 하나가 아니라 입력의 전 범위에 대해 함수가 만족해야 하는 속성(예: “리스트를 뒤집은 뒤 다시 뒤집으면 원래 리스트와 같아야 한다”)을 검증함으로써 AI 코드의 논리적 견고성을 시험한다.</p>
<h2>4.  사실 관계 오류와 논리적 오류의 상호작용 및 식별 프레임워크</h2>
<p>실제 AI 개발 현장에서는 이 두 오류가 독립적으로 나타나기보다는 복합적으로 얽혀 있는 경우가 많다. 예를 들어, 존재하지 않는 함수를 사용하여(사실 오류) 논리적으로 불가능한 연산을 시도(논리 오류)하는 식이다. 따라서 오라클 시스템은 우선순위에 따라 오류를 분류하고 진단하는 체계적인 프레임워크를 갖춰야 한다.</p>
<h3>4.1  오류 식별 결정 트리 (Error Identification Decision Tree)</h3>
<p>효율적인 오라클 시스템은 발견된 결함을 다음의 단계적 흐름에 따라 식별하고 분류한다. 이는 디버깅의 효율성을 극대화하는 RCA(Root Cause Analysis)의 기초가 된다.</p>
<ol>
<li><strong>구문 검사 (Syntactic Check):</strong> 코드가 해당 언어의 문법에 맞게 파싱되는가? (No <span class="math math-inline">\rightarrow</span> <strong>Syntax Error</strong>) - 이는 가장 낮은 단계의 오류로, 컴파일러나 인터프리터가 즉시 식별한다.</li>
<li><strong>참조 및 사실성 검사 (Factuality/Reference Check):</strong> 코드 내의 모든 식별자(함수명, 변수명, 클래스명, 라이브러리명)가 정의되어 있거나 신뢰할 수 있는 외부 소스에 실재하는가? (No <span class="math math-inline">\rightarrow</span> <strong>Factual Error / Hallucination</strong>) - 이 단계에서 라이브러리 환각이나 API 오용을 걸러낸다.</li>
<li><strong>타입 및 시그니처 검사 (Type Consistency Check):</strong> 함수 호출 시 인자의 개수와 데이터 타입이 정의된 시그니처와 일치하는가? (No <span class="math math-inline">\rightarrow</span> <strong>Factual Error</strong> - 지식의 부정확성) - 이는 정적 타입 검사기(Static Type Checker)를 통해 수행된다.</li>
<li><strong>실행 및 논리 검사 (Logical/Behavioral Check):</strong> 코드가 예외 없이 실행되며, 주어진 입력에 대해 의도된 출력(Ground Truth)을 반환하는가? (No <span class="math math-inline">\rightarrow</span> <strong>Logical Fallacy</strong>) - 이 단계에서 알고리즘 오류, 경계값 오류, 인과 오류 등이 식별된다.</li>
</ol>
<h3>4.2  코드 생성 시나리오별 오류 식별 사례</h3>
<p>다음의 표와 시각적 예시는 실제 코드 생성 과정에서 발생할 수 있는 오류들을 위 프레임워크에 대입하여 분석한 것이다.</p>
<table><thead><tr><th><strong>시나리오</strong></th><th><strong>AI 생성 코드 (오류 포함)</strong></th><th><strong>오류 유형 분류</strong></th><th><strong>오라클의 식별 메커니즘</strong></th></tr></thead><tbody>
<tr><td><strong>사례 1: 환각 라이브러리</strong></td><td><code>import torch.nn.functional as F; F.gelu_new(x)</code></td><td><strong>사실 관계 오류</strong> (Intrinsic/Library Hallucination)</td><td><strong>Static Analysis + Knowledge Lookup:</strong> <code>torch</code> 패키지 메타데이터 조회 결과 <code>gelu_new</code>라는 속성(Attribute)이 존재하지 않음을 확인.</td></tr>
<tr><td><strong>사례 2: 연산 로직 결함</strong></td><td><code>def add(a, b): return a - b</code></td><td><strong>논리적 오류</strong> (Semantic Error)</td><td><strong>Unit Test:</strong> <code>assert add(2,3) == 5</code> 실행 결과 실패. 기대값(5)과 실제값(-1)의 불일치를 통해 로직 오류 식별.</td></tr>
<tr><td><strong>사례 3: 모순된 조건문</strong></td><td><code>if x &gt; 5 and x &lt; 2:</code></td><td><strong>논리적 오류</strong> (Logical Inconsistency)</td><td><strong>SMT Solver / Formal Verifier:</strong> 조건식 <span class="math math-inline">x &gt; 5 \land x &lt; 2</span>는 만족 불가능(Unsatisfiable)함이 수학적으로 증명됨.</td></tr>
<tr><td><strong>사례 4: 잘못된 API 사용</strong></td><td><code>pd.read_csv('data.xlsx')</code></td><td><strong>사실 관계 오류</strong> (API Misuse)</td><td><strong>Linter / Type Checker:</strong> <code>read_csv</code> 함수는 <code>.xlsx</code> 파일을 처리할 수 없다는 API 명세 위반을 감지.</td></tr>
</tbody></table>
<p><img src="./3.2.1.1.0%20%EC%82%AC%EC%8B%A4%20%EA%B4%80%EA%B3%84%20%EC%98%A4%EB%A5%98Factual%20Error%EC%99%80%20%EB%85%BC%EB%A6%AC%EC%A0%81%20%EC%98%A4%EB%A5%98Logical%20Fallacy%EC%9D%98%20%EC%8B%9D%EB%B3%84.assets/image-20260218191613908.jpg" alt="image-20260218191613908" /></p>
<h3>4.3  복합 오류의 처리: 결정론적 안전장치의 필요성</h3>
<p>사실 관계 오류와 논리적 오류가 혼재된 경우, 오라클은 <strong>‘Fail-Fast’ 원칙</strong>에 따라 사실 관계 검증을 먼저 수행해야 한다. 기반이 되는 사실(라이브러리, 데이터)이 틀렸다면, 그 위의 논리가 아무리 정교해도 무의미하기 때문이다. 예를 들어, 존재하지 않는 데이터 컬럼을 사용하여 평균을 구하는 코드는, 평균 계산 로직(논리)을 검증하기 전에 컬럼의 존재 여부(사실)부터 검증되어야 한다. 이러한 계층적 검증 구조는 오라클의 연산 비용을 절감하고, 오류의 근본 원인을 더 정확하게 지목할 수 있게 한다.</p>
<h2>5.  오라클 구축을 위한 제언: 하이브리드 검증 아키텍처</h2>
<p>결론적으로, AI 소프트웨어의 신뢰성을 확보하기 위한 결정론적 정답지는 단일한 형태가 될 수 없다. 사실 관계 검증을 위한 **‘지식 오라클(Knowledge Oracle)’**과 논리적 정합성 검증을 위한 **‘논리 오라클(Logic Oracle)’**이 결합된 하이브리드 아키텍처가 필수적이다.</p>
<ol>
<li><strong>지식 오라클:</strong> 벡터 데이터베이스(RAG), API 스펙 문서, 지식 그래프(Knowledge Graph)를 기반으로 하여 모델의 출력이 “실재하는가?“를 묻는다. 이는 AI의 ’기억’을 검증하는 과정이다.</li>
<li><strong>논리 오라클:</strong> 컴파일러, 린터(Linter), 정적 분석기, 유닛 테스트 프레임워크, SMT Solver를 기반으로 하여 모델의 출력이 “올바르게 작동하는가?“를 묻는다. 이는 AI의 ’사고’를 검증하는 과정이다.</li>
</ol>
<p>이 두 오라클이 상호 보완적으로 작동할 때, 비로소 AI가 생성한 결과물의 ’환각’을 단순한 버그나 미스터리한 현상이 아닌, 엔지니어링 적으로 통제 가능하고 식별 가능한 관리 대상(Manageable Risks)으로 전환할 수 있다. 이는 3.2.1.2절에서 다룰 ’사용자 신뢰 형성을 위한 안전장치’의 기술적 기반이 되며, 궁극적으로는 AI 기반 소프트웨어 개발의 예측 가능성을 담보하는 핵심 열쇠가 된다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Large Language Models are Better Logical Fallacy Reasoners with, https://openreview.net/pdf?id=UvDNTDBAqE</li>
<li>Boosting Logical Fallacy Reasoning in LLMs via Logical Structure, https://aclanthology.org/2024.emnlp-main.730.pdf</li>
<li>LLM Hallucinations in 2025: How to Understand and Tackle AI’s, https://www.lakera.ai/blog/guide-to-hallucinations-in-large-language-models</li>
<li>AI Factual Errors, vs AI Hallucinations - Lancom Technology, https://www.lancom.tech/our-resources/ai-factual-errors-vs-ai-hallucinations</li>
<li>Does AI Confabulate or Hallucinate? - testRigor, https://testrigor.com/blog/does-ai-confabulate-or-hallucinate/</li>
<li>Factual Accuracy in AI: When Truth Meets Technology - Sandgarden, https://www.sandgarden.com/learn/factual-accuracy</li>
<li>CodeHalu: Investigating Code Hallucinations in … - AAAI Publications, https://ojs.aaai.org/index.php/AAAI/article/download/34717/36872</li>
<li>Detecting and Correcting Hallucinations in LLM-Generated Code via, https://arxiv.org/html/2601.19106v1</li>
<li>(PDF) Library Hallucinations in LLMs: Risk Analysis Grounded in, https://www.researchgate.net/publication/395944769_Library_Hallucinations_in_LLMs_Risk_Analysis_Grounded_in_Developer_Queries</li>
<li>HalluLens: LLM Hallucination Benchmark - ACL Anthology, https://aclanthology.org/2025.acl-long.1176.pdf</li>
<li>A Survey on Hallucination in Large Language Models - Preprints.org, https://www.preprints.org/manuscript/202510.0540/v2/download</li>
<li>The Probabilistic Paradox: Why LLMs Fail in Deterministic Domains, https://medium.com/@ensigno/the-probabilistic-paradox-why-llms-fail-in-deterministic-domains-and-how-to-fix-it-be21b5e20bda</li>
<li>Logical Fallacies | Definition, Types, List &amp; Examples - Scribbr, https://www.scribbr.com/fallacies/logical-fallacy/</li>
<li>Blindspots in LLMs I’ve noticed while AI coding - Hacker News, https://news.ycombinator.com/item?id=43414393</li>
<li>How susceptible are LLMs to Logical Fallacies? : r/singularity - Reddit, https://www.reddit.com/r/singularity/comments/163phck/how_susceptible_are_llms_to_logical_fallacies/</li>
<li>An Empirical Study on Automatically Detecting AI-Generated Source, https://arxiv.org/html/2411.04299v1</li>
<li>Formal Methods Techniques in AI Verification | by Haszeli Ahmad, https://haszeliahmad.medium.com/formal-methods-techniques-in-ai-verification-143c1fea6251</li>
<li>Methods and Tools for the Formal Verification of Software, https://www.ac.tuwien.ac.at/files/pub/rainer-harbach_11.pdf</li>
<li>How to Unit-Test the Deterministic Parts of AI Systems | Galileo, https://galileo.ai/blog/unit-testing-ai-systems</li>
<li>Why Deterministic Test Generation Is Important - Diffblue, https://www.diffblue.com/resources/deterministic-test-generation/</li>
<li>Boosting Logical Fallacy Reasoning in LLMs via Logical Structure, https://arxiv.org/html/2410.12048v1</li>
<li>A Comprehensive Survey of Hallucination in Large Language Models, https://arxiv.org/html/2507.02870v1</li>
<li>What Is Ground Truth in Machine Learning? | IBM, https://www.ibm.com/think/topics/ground-truth</li>
<li>(PDF) Verifiable LLM-Generated Test Oracles - ResearchGate, https://www.researchgate.net/publication/398511554_Verifiable_LLM-Generated_Test_Oracles_Ensuring_Consistency_Correctness_and_Explainability_in_AI-_Assisted_Testing</li>
<li>(PDF) BEAVER: An Efficient Deterministic LLM Verifier - ResearchGate, https://www.researchgate.net/publication/398430248_BEAVER_An_Efficient_Deterministic_LLM_Verifier</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>