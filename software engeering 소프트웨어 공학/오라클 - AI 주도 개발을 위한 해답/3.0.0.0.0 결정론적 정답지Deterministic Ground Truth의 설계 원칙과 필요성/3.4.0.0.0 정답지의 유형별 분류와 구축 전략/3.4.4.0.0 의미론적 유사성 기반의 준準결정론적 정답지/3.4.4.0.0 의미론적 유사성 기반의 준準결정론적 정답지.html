<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.4.4 의미론적 유사성 기반의 준(準)결정론적 정답지</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.4.4 의미론적 유사성 기반의 준(準)결정론적 정답지</h1>
                    <nav class="breadcrumbs"><a href="../../../../../index.html">Home</a> / <a href="../../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../../index.html">Chapter 3. 결정론적 정답지(Deterministic Ground Truth)의 설계 원칙과 필요성</a> / <a href="../index.html">3.4 정답지의 유형별 분류와 구축 전략</a> / <a href="index.html">3.4.4 의미론적 유사성 기반의 준(準)결정론적 정답지</a> / <span>3.4.4 의미론적 유사성 기반의 준(準)결정론적 정답지</span></nav>
                </div>
            </header>
            <article>
                <h1>3.4.4 의미론적 유사성 기반의 준(準)결정론적 정답지</h1>
<p>전통적인 소프트웨어 테스트 환경에서 오라클(Oracle)은 입력(Input)에 대해 항상 동일하고 예측 가능한 단일 출력(Output)을 기대하는 완전한 결정론적(Deterministic) 모델을 기반으로 작동한다. 이러한 환경에서 품질 엔지니어는 시스템의 출력이 사전 정의된 기대 결과와 정확히 일치하는지를 검증하는 단순한 이분법적 논리에 의존해 왔다. 그러나 생성형 AI(Generative AI)와 대형 언어 모델(LLM)이 소프트웨어의 핵심 컴퓨팅 엔진으로 자리 잡으면서, 이러한 전통적 검증 방식은 근본적인 한계에 직면하게 되었다. AI 모델은 본질적으로 확률적(Probabilistic)이며 비결정론적(Non-deterministic) 특성을 지니기 때문에, 동일한 프롬프트를 입력하더라도 온도(Temperature) 설정, 미세한 컨텍스트 변화, 심지어 내부 연산의 부동소수점 처리 차이 등에 의해 매번 다른 형태의 텍스트를 출력한다.</p>
<p>이러한 비결정론적 환경에서는 전통적인 ‘정확한 문자열 일치(Exact String Matching)’ 기반의 테스트가 완전히 무용지물이 된다. 예를 들어, 비밀번호 재설정 방법을 묻는 사용자의 질문에 대해 “비밀번호 찾기 링크를 클릭하고 이메일 안내를 따르세요“라는 기준 정답지(Ground Truth)가 존재한다고 가정하자. 만약 AI 챗봇이 “비밀번호를 잊으셨다면 재설정 링크를 이용해 이메일을 확인해 주세요“라고 답변했다면, 이 두 문장은 의미상 완벽히 동일한 해결책을 제시하고 있지만 문자열 수준에서는 불일치한다. 전통적인 결정론적 오라클은 이를 즉각적인 테스트 실패(Fail)로 판정할 것이다. 이는 테스트의 신뢰성을 심각하게 훼손하며, 지속적 통합(CI) 파이프라인에서 무의미한 경고를 남발하여 AI 소프트웨어의 품질 보증(QA) 프로세스를 마비시키는 주요 원인이 된다.</p>
<p>이러한 문제를 해결하기 위해 등장한 혁신적인 평가 패러다임이 바로 의미론적 유사성 기반의 준(準)결정론적 정답지(Semantic Similarity-based Quasi-Deterministic Ground Truth)이다. 준결정론적 오라클은 시스템 출력의 표면적인 ’형태(Syntax)’가 아닌 내재된 ’의미(Semantics)’에 집중한다. 기준이 되는 정답지와 AI의 실제 출력 결과가 다차원 벡터 공간(High-dimensional Vector Space) 내에서 수학적으로 얼마나 가까운지, 즉 의미론적으로 얼마나 유사한지를 계산하고, 그 점수가 사전 정의된 통계적 임계값(Threshold)을 초과할 경우 이를 성공(Pass)으로 판정하는 방식이다. 이는 생성형 AI 시스템 특유의 언어적 유연성을 허용하면서도, 엔터프라이즈 소프트웨어 엔지니어링에서 절대적으로 요구되는 테스트의 엄격함과 자동화 가능성을 동시에 충족시키는 핵심 전략이다.</p>
<h2>1.  어휘적 유사성(Lexical Similarity)의 한계와 의미론적 전환의 당위성</h2>
<p>의미론적 평가 체계로 넘어가기 전, 초기의 자연어 처리(NLP) 평가 지표들이 왜 AI 소프트웨어 테스트 오라클로 부적합한지 그 수리적, 구조적 원인을 명확히 이해할 필요가 있다. 초기의 자동화된 평가는 주로 어휘적 유사성(Lexical Similarity), 즉 두 텍스트 간에 단어의 표면적인 겹침이 얼마나 발생하는지를 측정하는 구문 분석적 방법에 전적으로 의존했다.</p>
<h3>1.1  자카드 유사도(Jaccard Similarity)의 수학적 구조와 취약성</h3>
<p>어휘적 유사성을 측정하는 가장 직관적이고 널리 쓰이는 수학적 모델 중 하나는 자카드 유사도(Jaccard Similarity)이다. 자카드 유사도는 두 텍스트를 각각 고유한 단어(토큰)들의 집합으로 간주하고, 두 집합 간의 교집합 크기를 합집합 크기로 나눈 비율로 유사도를 정의한다.</p>
<p>이를 수학적 수식으로 표현하면 다음과 같다.<br />
<span class="math math-display">
J(A, B) = \frac{\vert A \cap B \vert}{\vert A \cup B \vert}
</span><br />
여기서 <span class="math math-inline">\vert A \cap B \vert</span>는 기준 정답지 집합 <span class="math math-inline">A</span>와 AI 출력 집합 <span class="math math-inline">B</span>에 공통으로 존재하는 토큰의 수를 의미하며, <span class="math math-inline">\vert A \cup B \vert</span>는 두 집합에 존재하는 전체 고유 토큰의 총합을 의미한다. 자카드 유사도는 알고리즘이 매우 단순하고 연산 비용이 낮아, 두 문서 간의 정확한 단어 겹침을 빠르게 찾는 표절 검사나 단순 키워드 매칭 태스크에서는 여전히 강력한 도구로 활용된다.</p>
<p>그러나 최신 생성형 AI의 출력을 검증하는 오라클로서 자카드 유사도는 치명적인 한계를 지닌다. 첫째, 동의어(Synonyms)를 전혀 인식하지 못한다. 둘째, 단어의 배치 순서나 문맥적 구조를 완전히 무시한다. “고양이가 쥐를 쫓는다“와 “쥐가 고양이를 쫓는다“는 자카드 유사도 관점에서는 완벽히 동일한 문장으로 평가되지만, 실제 세계의 의미는 정반대이다. 이러한 특성 때문에 자카드 유사도는 형태가 다르지만 의미가 동일한 패러프레이징(Paraphrasing) 콘텐츠를 처리하는 데 있어 극심한 위음성(False Negative)을 발생시킨다.</p>
<h3>1.2  N-gram 기반 평가 지표(BLEU, ROUGE)의 한계</h3>
<p>자카드 유사도의 순서 무시 문제를 부분적으로 보완하기 위해 기계 번역과 자동 요약 분야에서 오랫동안 표준으로 사용된 지표가 BLEU(Bilingual Evaluation Understudy)와 ROUGE(Recall-Oriented Understudy for Gisting Evaluation)이다. 이들은 단순한 단어 단위가 아니라 N-gram(연속된 N개의 단어 묶음)의 중복도를 기반으로 정밀도(Precision)와 재현율(Recall)을 계산한다.</p>
<p>BLEU와 ROUGE는 문장 내 단어의 국소적인 순서를 어느 정도 반영할 수 있다는 장점이 있으나, 근본적으로는 문자열의 정확한 일치(Exact Match)를 요구한다는 점에서 자카드 유사도와 동일한 인식론적 한계를 공유한다. 예를 들어 다음과 같은 텍스트 비교 상황을 가정해 보자.</p>
<ul>
<li><strong>기준 정답지 (Reference):</strong> “People like foreign cars.”</li>
<li><strong>AI 출력 A (Candidate A):</strong> “Consumers prefer imported cars.”</li>
<li><strong>AI 출력 B (Candidate B):</strong> “People like visiting places abroad.”</li>
</ul>
<p>인간의 관점에서는 AI 출력 A가 기준 정답지와 완벽하게 동일한 의미를 전달하고 있음을 알 수 있다. 그러나 AI 출력 A는 기준 정답지와 공유하는 단어가 ‘cars’ 하나뿐이므로 N-gram 기반 오라클(BLEU, ROUGE 등)에서는 극도로 낮은 점수를 받게 된다. 반면, AI 출력 B는 의미가 전혀 다름에도 불구하고 ‘People’, ’like’라는 단어를 표면적으로 공유하므로 어휘적 지표에서는 오히려 더 높은 점수를 획득하는 모순이 발생한다.</p>
<p>이러한 현상은 전통적인 구문 기반 오라클이 동의어 및 패러프레이징 환경에서 어떻게 실패(False Negative)하는지를 명확히 보여준다. 반면, 트랜스포머 임베딩 모델을 거치는 의미론적 매칭 과정에서는 표면적 단어의 다름을 극복하고, ’foreign’과 ‘imported’, ’People’과 ’Consumers’가 고차원 공간에서 매우 가까운 위치에 있음을 파악하여 유연하면서도 정확하게 성공(Pass)을 판별해 낸다. 최신 LLM은 동일한 의도를 다양한 어휘와 문장 구조로 변주하여 생성하는 능력이 탁월하기 때문에, 출력의 형태에 얽매이는 이러한 구문 기반 모델은 더 이상 신뢰할 수 있는 자동화 테스트 오라클이 될 수 없다.</p>
<table><thead><tr><th><strong>평가 지표 특징</strong></th><th><strong>자카드 유사도 (Jaccard)</strong></th><th><strong>BLEU / ROUGE</strong></th><th><strong>의미론적 유사도 (Semantic)</strong></th></tr></thead><tbody>
<tr><td><strong>비교 대상 및 단위</strong></td><td>고유 단어 집합의 겹침</td><td>연속된 N-gram 문자열의 겹침</td><td>문맥이 반영된 실수 임베딩 벡터</td></tr>
<tr><td><strong>순서 인식 여부</strong></td><td>전혀 인식하지 못함</td><td>N-gram 범위 내에서 국소적 인식</td><td>트랜스포머 어텐션을 통한 전체 문맥 인식</td></tr>
<tr><td><strong>동의어 및 유의어 처리</strong></td><td>불가능 (서로 다른 토큰으로 간주)</td><td>불가능 (정확한 문자열 일치 필요)</td><td>고차원 벡터의 근접성을 통해 완벽히 인식</td></tr>
<tr><td><strong>주요 활용 분야</strong></td><td>단순 표절 검사, 키워드 교집합 분석</td><td>구형 기계 번역 및 요약 시스템 평가</td><td>최신 LLM 응답 검증, RAG 검색 품질 평가</td></tr>
<tr><td><strong>테스트 오라클로서의 신뢰도</strong></td><td>매우 낮음</td><td>낮음 (위음성 발생 확률 매우 높음)</td><td>높음 (인간의 의미 해석과 강한 상관관계)</td></tr>
</tbody></table>
<h3>1.3  의미론적 공간(Semantic Space)으로의 패러다임 전환</h3>
<p>위에서 살펴본 어휘적 불일치 문제를 근본적으로 해결하기 위해 제안된 것이 바로 문맥적 임베딩(Contextual Embeddings)을 활용한 의미론적 유사성 평가이다. 이는 텍스트를 단순한 이산적(Discrete) 문자열의 집합으로 취급하는 것을 포기하고, 텍스트가 내포하고 있는 깊은 의미적 특성을 연속적인 고차원 실수 벡터(Dense Vector) 공간의 좌표로 변환하여 두 벡터 간의 수학적 거리를 측정하는 철학적, 기술적 전환을 의미한다.</p>
<p>이러한 의미론적 임베딩은 문장 내 모든 단어들 사이의 관계를 동시에 병렬로 계산하는 트랜스포머(Transformer) 알고리즘의 어텐션 메커니즘(Attention Mechanism)을 통해 생성된다. 사전 학습된 대규모 트랜스포머 모델은 언어의 문맥을 깊이 이해하고 있기 때문에, “bank account(은행 계좌)“의 ’bank’와 “river bank(강둑)“의 ’bank’를 철자가 같음에도 불구하고 다차원 벡터 공간의 완전히 다른 위치로 매핑할 수 있다. 준결정론적 오라클은 이처럼 복잡한 의미망을 포착하는 임베딩 모델의 능력을 역으로 활용하여, AI가 생성한 출력 텍스트의 표면적 형태가 기준 정답지와 아무리 다르더라도 그 안에 담긴 본질적인 ’의미적 사실(Semantic Fact)’이 일치하는지를 수학적으로 판별해 낸다.</p>
<h2>2.  준결정론적 평가의 수학적 심장: 코사인 유사도(Cosine Similarity)</h2>
<p>의미론적 오라클 시스템을 구축하는 데 있어 가장 기저에 놓인 핵심 수학적 연산은 코사인 유사도(Cosine Similarity)이다. 텍스트가 임베딩 모델을 통해 벡터로 변환된 후, 오라클은 두 텍스트가 얼마나 유사한지를 측정하기 위해 벡터 공간에서의 기하학적 관계를 분석해야 한다. 코사인 유사도는 0이 아닌 두 벡터 사이의 각도(<span class="math math-inline">\theta</span>)에 대한 코사인 값을 계산하여, 두 벡터가 다차원 공간에서 얼마나 동일한 방향을 향하고 있는지를 정밀하게 측정한다.</p>
<p>소프트웨어 테스트 환경에서 자동화 스크립트가 확보한 기준 정답지의 임베딩 벡터를 <span class="math math-inline">A</span>, 테스트 대상 AI 모델이 생성한 실제 출력의 임베딩 벡터를 <span class="math math-inline">B</span>라고 할 때, 코사인 유사도를 구하는 수학적 공식은 다음과 같이 정의된다.<br />
<span class="math math-display">
Cosine\ Similarity(A, B) = \cos(\theta) = \frac{A \cdot B}{\Vert A \Vert \Vert B \Vert} = \frac{\sum_{i=1}^{n} A_i \times B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \times \sqrt{\sum_{i=1}^{n} B_i^2}}
</span><br />
이 공식을 분해하여 살펴보면 그 작동 원리가 더욱 명확해진다. 공식의 분자에 위치한 <span class="math math-inline">A \cdot B</span>는 두 벡터의 내적(Dot Product)으로, 두 벡터의 각 차원 값을 곱하여 모두 더한 값이다. 내적 자체만으로도 두 벡터의 방향성이 얼마나 일치하는지를 어느 정도 반영하지만, 내적값은 벡터의 크기(성분의 절댓값)가 커질수록 기하급수적으로 커지는 성질이 있어 객관적인 유사도 지표로 직접 사용하기에는 무리가 있다.</p>
<p>따라서 분모에 위치한 <span class="math math-inline">\Vert A \Vert \Vert B \Vert</span>, 즉 각 벡터의 크기(Magnitude, L2 Norm)의 곱으로 내적값을 나누어 주는 과정이 필수적이다. 이 나누기 연산은 두 벡터의 크기를 1로 만드는 정규화(Normalization) 과정과 수학적으로 완벽히 동일하다. 결과적으로 산출되는 코사인 유사도 점수는 벡터의 절대적인 크기와는 무관하게 오직 방향성만을 반영한 -1에서 1 사이의 표준화된 스칼라 값을 가지게 된다.</p>
<ul>
<li><strong>1 (완벽한 유사성):</strong> 두 벡터의 각도가 0도이며, 정확히 동일한 방향을 가리킨다. 이는 텍스트 간의 의미적 일치도가 가장 높음을 뜻한다.</li>
<li><strong>0 (직교 및 무관함):</strong> 두 벡터의 각도가 90도이며, 기하학적으로 직교(Orthogonal)한다. 이는 두 텍스트 사이에 의미론적 연관성이 전혀 없음을 나타낸다.</li>
<li><strong>-1 (완전한 불일치):</strong> 두 벡터의 각도가 180도이며, 정반대 방향을 가리킨다. (다만 딥러닝 기반의 최신 텍스트 임베딩 모델에서는 모든 성분이 양수인 영역에 매핑되는 경우가 많아 실제 응용에서는 0에서 1 사이의 값이 주로 나타난다).</li>
</ul>
<h3>2.1  코사인 유사도가 텍스트 오라클에 최적화된 이유</h3>
<p>기계학습 및 테스트 오라클을 구축할 때 두 데이터 포인트 간의 거리를 측정하는 방법으로는 유클리디안 거리(Euclidean Distance)나 맨해튼 거리(Manhattan Distance) 등 다양한 선택지가 존재한다. 그럼에도 불구하고 최첨단 LLM 평가 시스템들이 압도적으로 코사인 유사도를 채택하는 이유는, 코사인 유사도가 벡터의 <strong>크기(Magnitude)에 둔감(Insensitive)하고 오직 방향(Orientation)만을 독립적으로 측정</strong>하는 독특한 성질을 지니고 있기 때문이다.</p>
<p>텍스트를 임베딩 벡터로 변환하는 과정에서, 일반적으로 길이가 긴 문서는 짧은 문서보다 단어의 등장 빈도가 높고 정보량이 많아 벡터 공간 내에서 원점으로부터 더 멀리 뻗어나가는 큰 크기(Magnitude)를 갖게 된다. 만약 오라클이 유클리디안 거리를 사용하여 두 문서를 평가한다면, 완전히 동일한 주제와 결론을 다루고 있음에도 불구하고 단지 문서의 길이나 특정 단어의 빈도수가 다르다는 이유만으로 두 문서 사이의 거리가 매우 멀다고, 즉 ’유사하지 않다’고 심각하게 오판할 수 있다.</p>
<p>반면 코사인 유사도는 두 벡터 사이의 각도만을 측정한다. 기준이 되는 정답지가 짧고 간결한 요약문 형태로 작성되어 있고, 테스트 대상 AI 모델이 생성한 응답이 장황하고 긴 설명문이더라도, 두 텍스트가 전달하고자 하는 본질적인 ’주제’와 ’사실 관계’가 일치한다면 두 벡터는 고차원 공간에서 동일한 방향을 가리키게 된다. 코사인 유사도는 문서 길이의 차이라는 노이즈를 수학적으로 제거하고 본질적인 주제 유사성만을 완벽하게 포착해 낸다. 준결정론적 오라클 시스템에서는 코사인 유사도를 계산하여 도출된 연속적인 실수 값을 바탕으로, 해당 도메인의 민감도에 맞춰 사전에 정의된 임계값(예: <span class="math math-inline">\ge 0.85</span>)을 적용해 최종적으로 통과(Pass) 또는 실패(Fail)라는 부울(Boolean) 값으로 변환하여 테스트를 단언(Assertion)하게 된다.</p>
<h2>3.  고도화된 정답지 평가 메커니즘: BERTScore와 BARTScore의 심층 분석</h2>
<p>단순히 문장이나 문서 전체를 평균 내어 하나의 벡터로 뭉뚱그린 후(예: 기본적인 Sentence-BERT 문서 임베딩) 코사인 유사도를 구하는 방식은 연산이 빠르고 대규모 검색에 효율적이다. 그러나 이러한 문서 수준(Document-level)의 매핑은 문장 내에 은밀하게 숨어있는 세부적인 정보의 왜곡이나 미세한 환각(Hallucination) 현상을 정밀하게 잡아내기에는 평가의 해상도가 부족하다는 단점이 있다. 이에 따라 최신 소프트웨어 테스팅 분야에서는 문장 전체가 아닌 개별 토큰(Token) 수준의 정밀한 의미 매칭을 통해 준결정론적 정답지의 신뢰성을 극대화하는 고도화된 평가 지표들이 도입되었다.</p>
<p>대표적인 논문 원문인 <em>BERTScore: Evaluating Text Generation with BERT</em> 와 <em>BARTScore: Evaluating Generated Text as Text Generation</em> 은 이러한 토큰 단위 및 생성 확률 기반 평가 체계의 탄탄한 수학적 기반을 제공한다.</p>
<h3>3.1  BERTScore의 수리적 메커니즘과 정밀도/재현율 도출</h3>
<p>BERTScore는 BERT, RoBERTa, DeBERTa 등 사전 학습된 트랜스포머 언어 모델이 생성하는 양방향 ’문맥적 임베딩(Contextual Embeddings)’을 적극 활용하여, 기준 정답 문장의 각 토큰과 AI 생성 문장의 각 토큰 간의 유사도를 쌍(Pairwise)으로 교차 계산하는 기법이다. 이는 기존 BLEU가 놓쳤던 패러프레이징과 다의어를 효과적으로 처리할 뿐만 아니라, 특정 단어가 문맥 내에서 가지는 고유한 역할을 정확히 평가한다.</p>
<p>BERTScore의 파이프라인은 다음과 같은 정교한 단계로 구성된다.</p>
<ol>
<li><strong>문맥적 임베딩 생성 (Contextual Embedding):</strong> 먼저 기준 정답 텍스트(<span class="math math-inline">x = \langle x_1,..., x_k \rangle</span>)와 AI가 출력한 텍스트(<span class="math math-inline">\hat{x} = \langle \hat{x}_1,..., \hat{x}_l \rangle</span>)를 토큰화한다. 이후 각 토큰을 사전 학습된 트랜스포머 모델에 통과시켜 주변 문맥 정보가 밀도 있게 반영된 고차원 벡터로 변환한다. 단순히 사전에 정의된 고정된 단어 벡터를 불러오는 것이 아니라, 해당 문장 내에서의 역할이 반영된 동적 벡터를 생성하는 것이다.</li>
<li><strong>코사인 유사도 행렬 구축 (Similarity Matrix):</strong> 기준 정답에 존재하는 모든 토큰과 AI 출력에 존재하는 모든 토큰 간의 쌍별(Pairwise) 코사인 유사도를 빠짐없이 계산하여 <span class="math math-inline">k \times l</span> 크기의 2차원 유사도 행렬(Similarity Matrix)을 구축한다.</li>
<li><strong>그리디 매칭 (Greedy Matching):</strong> 구축된 행렬을 바탕으로 각 토큰에 대해 가장 코사인 유사도가 높은 최적의 매칭 쌍을 탐욕 알고리즘(Greedy Algorithm)을 통해 선택한다. 기준 정답의 토큰 입장에서 가장 의미가 유사한 AI 출력 토큰을 찾고, 반대로 AI 출력 토큰 입장에서도 가장 유사한 기준 정답 토큰을 찾는다.</li>
</ol>
<p><img src="./3.4.4.0.0%20%EC%9D%98%EB%AF%B8%EB%A1%A0%EC%A0%81%20%EC%9C%A0%EC%82%AC%EC%84%B1%20%EA%B8%B0%EB%B0%98%EC%9D%98%20%EC%A4%80%E6%BA%96%EA%B2%B0%EC%A0%95%EB%A1%A0%EC%A0%81%20%EC%A0%95%EB%8B%B5%EC%A7%80.assets/image-20260222122956399.jpg" alt="image-20260222122956399" /></p>
<p>이러한 그리디 매칭 결과를 바탕으로 BERTScore는 소프트웨어 테스트와 기계학습 모델 평가에서 널리 쓰이는 기본 개념인 정밀도(Precision), 재현율(Recall), F1 점수(F1 Score)를 의미론적 관점에서 재정의하여 산출한다. 각 토큰의 임베딩 벡터가 사전에 크기 1로 정규화(Pre-normalized)되어 있다고 가정할 때, 코사인 유사도는 벡터의 내적(Inner Product) 연산만으로 단순화된다.</p>
<ul>
<li>
<p><strong>재현율 (<span class="math math-inline">R_{BERT}</span>):</strong> 기준 정답지 문장(<span class="math math-inline">x</span>)을 구성하는 모든 토큰들이 AI가 생성한 텍스트(<span class="math math-inline">\hat{x}</span>) 내의 토큰들에 의해 얼마나 잘 반영(Cover)되고 매칭되었는가를 측정한다.<br />
<span class="math math-display">
R_{BERT} = \frac{1}{\vert x \vert} \sum_{x_i \in x} \max_{\hat{x}_j \in \hat{x}} x_i^\top \hat{x}_j
</span><br />
이 값이 낮다는 것은 오라클이 기대했던 중요한 사실 정보(Fact)가 AI의 답변에서 누락되었음을 의미한다.</p>
</li>
<li>
<p><strong>정밀도 (<span class="math math-inline">P_{BERT}</span>):</strong> AI가 생성한 텍스트(<span class="math math-inline">\hat{x}</span>)의 토큰들이 기준 정답지(<span class="math math-inline">x</span>)의 어떤 토큰과 얼마나 잘 관련되어 있는가를 측정한다.<br />
<span class="math math-display">
P_{BERT} = \frac{1}{\vert \hat{x} \vert} \sum_{\hat{x}_j \in \hat{x}} \max_{x_i \in x} x_i^\top \hat{x}_j
</span><br />
이 값이 낮다는 것은 AI가 생성한 텍스트에 기준 정답지에는 없는 불필요한 정보나 잘못된 환각 정보(Hallucination)가 다수 섞여 들어가 쓰레기 토큰을 생성했음을 의미한다.</p>
</li>
<li>
<p><strong><span class="math math-inline">F1_{BERT}</span>:</strong> 재현율과 정밀도의 조화 평균(Harmonic Mean)으로, 생성된 텍스트의 전반적인 의미론적 품질을 나타내는 단일 지표이다.<br />
<span class="math math-display">
F1_{BERT} = 2 \frac{P_{BERT} \cdot R_{BERT}}{P_{BERT} + R_{BERT}}
</span></p>
</li>
</ul>
<p>나아가 BERTScore는 단순한 평균을 넘어 오라클의 평가 해상도를 극대화하기 위한 두 가지 강력한 보정 옵션을 제공한다. 첫째, 문장 내에서 문법적인 역할만 하는 흔한 기능어(예: the, is)보다 핵심적인 의미를 담고 있는 희귀 단어(Rare words)에 더 높은 가중치를 부여하기 위해 역문서 빈도(Inverse Document Frequency, IDF) 가중치 기법을 선택적으로 결합한다. 둘째, 코사인 유사도의 특성상 대부분의 점수가 0.8 이상의 좁은 구간에 밀집하는 현상을 해결하고 지표의 가독성을 높이기 위해, 대규모 공통 크롤링 데이터를 기반으로 경험적 하한선(Empirical lower bound)을 설정하여 점수를 선형적으로 재조정하는 기준선 재조정(Baseline Rescaling) 기법을 사용한다. 이를 통해 평가자는 의미론적 차이를 훨씬 더 직관적이고 변별력 있게 분석할 수 있다.</p>
<h3>3.2  BARTScore: 텍스트 생성 확률 관점의 프레임워크 전환</h3>
<p>BERTScore가 ’임베딩 공간 내의 기하학적 거리’를 측정하여 텍스트의 유사성을 추론한다면, <em>BARTScore: Evaluating Generated Text as Text Generation</em> 논문에서 제안된 BARTScore는 평가 프레임워크를 ’한 텍스트가 다른 텍스트를 성공적으로 생성할 수 있는 확률’이라는 완전히 새로운 관점으로 전환하여 모델의 품질을 평가한다.</p>
<p>BARTScore는 인코더-디코더(Encoder-Decoder) 구조를 가진 사전 학습된 BART 모델의 파라미터(<span class="math math-inline">\theta</span>)를 그대로 활용하여, 소스 텍스트나 기준 정답 <span class="math math-inline">x</span>가 주어졌을 때 AI 모델의 출력 결과 <span class="math math-inline">y</span>가 텍스트 생성 과정에서 만들어질 조건부 대수 확률(Weighted Log Probability)을 계산한다. 이 수식은 시퀀스 내의 각 토큰이 이전까지 생성된 토큰들과 소스 텍스트를 바탕으로 나타날 확률을 모두 더한 형태를 띤다.<br />
$$<br />
BARTScore = \sum_{t=1}^{m} w_t \log p(y_t \vert y_{<br />
$$<br />
이러한 수리적 메커니즘은 생성형 AI가 텍스트를 만들어내는 고유의 생성 작업(Generation Task) 그 자체를 곧 평가 태스크로 공식화(Task Formulation)한 것이다. 모델의 가중치를 평가 지표로 직접 끌어다 씀으로써 BARTScore는 다양한 평가 시나리오에 놀라운 유연성을 제공한다. 예를 들어, 소스 문서에서 출발하여 모델의 가설(Hypothesis) 텍스트를 생성하는 방향(<span class="math math-inline">s \rightarrow h</span>)의 확률은 AI 모델이 원본 데이터에 얼마나 충실하게 답변을 생성했는지 사실성(Factuality)을 평가하는 오라클로 기능한다. 반대로, AI가 생성한 가설 텍스트를 바탕으로 원래의 완벽한 기준 정답(Reference) 텍스트를 다시 복원해 낼 수 있는 확률(<span class="math math-inline">h \rightarrow r</span>)을 계산하면, 이는 앞서 언급한 재현율(Recall) 기반의 평가를 텍스트 생성 확률 관점에서 완벽하게 수행하는 척도가 된다.</p>
<h2>4.  준결정론적 오라클의 실전 구축 프로세스와 임계값 설정 전략</h2>
<p>AI 소프트웨어 개발 환경에서 의미론적 유사성 오라클을 단발적인 실험에 그치지 않고 CI/CD(Continuous Integration/Continuous Deployment) 파이프라인이나 자동화된 회귀 테스트(Regression Test) 런타임에 견고하게 결합하기 위해서는 이론적 이해를 넘어선 철저한 소프트웨어 엔지니어링 접근이 필요하다. 전통적인 단위 테스트에서 사용하던 <code>assert expected_output == actual_output</code> 형태의 코드는 더 이상 이 비결정론적 세계에서 작동하지 않는다. 대신, 두 출력값 간의 의미론적 유사도(Semantic Similarity)를 연산하고, 그 결과가 해당 도메인에서 허용 가능한 특정 임계값을 만족하는지 확인하는 고차원적인 검증 로직으로 전체 테스트 프레임워크를 재설계해야 한다.</p>
<h3>4.1  다차원 오라클 평가자(Evaluator) 아키텍처 설계</h3>
<p>최신 AI 품질 보증 프레임워크는 단순히 하나의 통과/실패 지표에만 시스템의 운명을 맡기지 않는다. 대신 출력 텍스트의 품질을 다양한 각도에서 검증하는 다중 평가자(Evaluator) 파이프라인을 구축한다. 이는 코사인 유사도와 같은 순수 의미론적 검사가 문맥의 미묘한 뉘앙스나 어조, 윤리적 기준까지는 완벽히 평가할 수 없다는 한계를 구조적으로 보완하기 위함이다.</p>
<p>준결정론적 테스트의 완성도를 높이기 위해 구성되는 대표적인 평가자 지표는 다음과 같이 세분화된다.</p>
<ol>
<li><strong>유사성(Similarity) 및 사실성(Factuality):</strong> 위에서 다룬 임베딩과 코사인 유사도, BERTScore 등을 활용하여 AI의 출력이 기준 정답지(Golden Dataset)의 핵심 의도 및 사실 정보와 수학적으로 얼마나 가까운지를 측정하는 가장 기본적이고 핵심적인 평가자이다.</li>
<li><strong>관련성(Relevance):</strong> 사용자가 입력한 프롬프트(Query)의 본래 목적에 AI의 출력이 논리적으로 부합하는지 평가한다. 유사성 점수가 높더라도 문맥을 벗어난 장황한 설명이나 동문서답을 생성하는 경우를 1차적으로 걸러내는 문지기 역할을 수행한다.</li>
<li><strong>일관성 및 맥락(Coherence):</strong> 특히 다중 턴(Multi-turn) 대화를 수행하는 챗봇이나 에이전트 워크플로우에서, AI의 답변이 이전 대화 기록과 모순되지 않고 논리적인 흐름을 유지하며 문장 구조가 자연스럽게 이어지는지를 측정한다.</li>
<li><strong>편향성 및 보안성(Bias, Fairness &amp; Security):</strong> 출력 텍스트에 성별, 인종에 대한 차별적이고 편향된 발언이 포함되어 있거나, 시스템 프롬프트를 탈취(Jailbreak)하려는 시도에 무방비로 노출되어 개인 식별 정보(PII) 등 민감한 데이터가 노출되지 않았는지 실시간으로 모니터링하는 윤리적·보안적 오라클 역할을 수행한다.</li>
</ol>
<h3>4.2  임계값(Threshold)의 보정 및 최적화 메커니즘</h3>
<p>다중 평가자 파이프라인을 구축한 후 의미론적 유사성 기반 오라클에서 직면하는 가장 까다롭고 예술적인 영역은 바로 통과와 실패를 가르는 수학적 임계값(Threshold)을 결정하는 일이다. 소프트웨어의 성격에 맞지 않는 임계값 설정은 오라클 시스템 전체의 신뢰도를 파괴한다.</p>
<p>만약 임계값이 너무 높게(예: <span class="math math-inline">0.95</span> 이상) 설정된다면, AI 모델이 정상적인 범위 내에서 어휘를 교체하거나 문체를 약간만 변경하더라도 오라클은 이를 즉시 실패로 처리한다. 이는 엄청난 양의 위음성(False Negative)을 발생시켜 개발팀이 끊임없이 깨진 테스트 코드를 디버깅하게 만들어 CI/CD 파이프라인에 심각한 병목현상을 초래한다. 반대로 임계값이 너무 낮게(예: <span class="math math-inline">0.60</span> 이하) 설정된다면, 의미상 명백히 결함이 있는 답변, 사실 관계가 틀린 정보, 심지어 치명적인 환각(Hallucination) 텍스트마저 테스트를 통과시켜 버리는 위양성(False Positive)의 재앙적인 위험이 발생한다.</p>
<p>적절한 임계값 설정을 위해서는 주관적 감에 의존하는 것이 아니라 도메인별 통계적 분석에 기반한 접근이 필수적이다. 테스트 엔지니어는 우선 100~500건 규모의 ’완벽한 정답 데이터셋’과 의도적으로 오류를 삽입한 ’확실한 오답 데이터셋’을 양극단으로 구축해야 한다. 그 후, 파이프라인에 적용할 임베딩 모델(예: Sentence-BERT, text-embedding-ada-002 등)을 활용해 두 집단 간의 유사도 점수 분포(Distribution)를 계산하고 히스토그램을 그린다. 오라클은 이 두 집단의 분포가 교차하는 지점을 분석하여, 위양성과 위음성을 동시에 최소화하면서 집단을 가장 잘 분리할 수 있는 최적의 점수(일반적인 비즈니스 텍스트의 경우 통상적으로 <span class="math math-inline">0.80</span>~<span class="math math-inline">0.85</span> 내외)를 도출하고 이를 해당 테스트 슈트의 고정 임계값(Pinning)으로 설정함으로써 신뢰도를 확보한다. 더욱 견고한 오라클을 위해, 일부 시스템은 임베딩 기반의 코사인 유사도 점수와 FuzzySharp 같은 라이브러리를 사용한 어휘적 거리 스코어를 결합하여 가중 평균을 적용함으로써 단일 지표의 맹점을 상호 보완하는 앙상블(Ensemble) 기법을 사용하기도 한다.</p>
<h3>4.3  다중 반복 테스트와 통계적 단언(Statistical Assertion)의 도입</h3>
<p>임계값을 아무리 정교하게 설정하더라도, 생성형 AI의 비결정론적 특성을 완전히 통제할 수는 없다. LLM은 동일한 입력에 대해 가끔씩 설명할 수 없는 극단적인 아웃라이어(Outlier) 응답을 생성할 수 있다. 따라서 단 한 번의 실행 결과를 바탕으로 오라클의 통과/실패를 확정 짓는 것은 구시대적인 방식이며 매우 위험하다. 최신 검증 프레임워크에서는 동일한 테스트 케이스를 다중 반복 실행(Multiple Iterations)하고 그 결과 집합의 통계적 분포를 분석하여 최종 성공 여부를 단언(Assertion)하는 확률적 기법이 산업 표준으로 자리 잡고 있다.</p>
<p>예를 들어, 감정 분석 또는 정보 추출 태스크에 대해 동일한 테스트 시나리오를 10회 반복 실행한다. 매 회차마다 생성된 응답과 기준 정답지 간의 코사인 유사도를 계산하고, 그 점수가 앞서 설정된 임계값을 넘은 횟수를 카운트한다. 10회 중 최소 8회(80%의 일치율) 이상 임계값을 통과했다면 오라클은 해당 테스트 런타임을 최종적으로 성공(Pass) 처리하는 방식이다. 이러한 통계적 단언 기법은 일시적으로 발생하는 기형적 출력이 전체 소프트웨어 빌드 파이프라인을 중단시키고 배포를 지연시키는 사태를 막아주며, 통제된 분산 내에서 수용 가능한 수준의 품질(Acceptable Variance)을 보장하는 매우 현실적이고 강력한 엔지니어링 전략이다.</p>
<h2>5.  실전 예제: 오라클 데이터베이스(Oracle DB) 기반의 RAG 챗봇 응답 검증 파이프라인 구축</h2>
<p>의미론적 유사성을 활용한 준결정론적 평가 체계를 실제 엔터프라이즈 환경의 인프라 위에서 어떻게 아키텍처로 구현할 수 있는지, Oracle Database 23ai/26ai의 네이티브 벡터 검색 기능을 핵심 오라클 판별기로 활용한 RAG(Retrieval-Augmented Generation) 챗봇 검증 파이프라인 사례를 통해 구체적으로 살펴본다.</p>
<h3>5.1  테스트 인프라 아키텍처 개요 및 정답 데이터 벡터화</h3>
<p>엔터프라이즈 환경에서 작동하는 AI 챗봇은 일반적인 잡담이 아니라 회사의 민감한 내부 매뉴얼, 인사 규정, 엄격한 비즈니스 정책 등을 기반으로 답변해야 하므로, 생성된 응답이 정해진 궤도를 벗어나지 않는지 철저한 정답지 검증이 요구된다. 최신 Oracle AI Database 26ai는 벡터 데이터(Vector Data Type)를 네이티브 수준에서 기본적으로 지원하며, 외부의 별도 벡터 데이터베이스 인스턴스(예: Pinecone, Milvus 등)로 데이터를 추출하거나 복제할 필요 없이 데이터베이스 내부의 SQL 쿼리 실행 계획 안에서 직접 초고속 벡터 유사도 검색과 시맨틱 맵핑(Semantic Mapping)을 수행할 수 있도록 지원한다.</p>
<p>이러한 인프라 구조에서 자동화 테스트 검증 파이프라인은 역설적이게도 RAG 챗봇이 정보를 검색하는 기본 메커니즘과 정확히 동일한 벡터 엔진을 사용해 구축된다.</p>
<ol>
<li><strong>골든 데이터셋(Golden Dataset) 구축:</strong> 품질 보증(QA) 팀과 도메인 전문가는 시스템에 예상되는 수백 개의 다양한 사용자 질문(Input Query)에 대해, 어떠한 경우에도 준수되어야 할 확정적인 비즈니스 규칙과 모범 답변(Golden Truth)을 텍스트 형태로 작성한다.</li>
<li><strong>정답지의 벡터화 및 저장:</strong> 작성된 텍스트 형태의 기준 정답지들은 OCI(Oracle Cloud Infrastructure) Generative AI의 임베딩 모델 또는 ONNX 포맷의 오픈소스 로컬 임베딩 모델을 거쳐 의미론적 특성이 담긴 고차원(예: 768차원 또는 1024차원) 벡터로 변환된다. 변환된 벡터 데이터는 오라클 데이터베이스 테이블 내의 <code>VECTOR</code> 타입 컬럼에 강력한 관계형 데이터 무결성 및 접근 제어 보안 정책의 보호를 받으며 안전하게 적재된다.</li>
</ol>
<h3>5.2  준결정론적 테스트 오라클 자동화 실행 파이프라인</h3>
<p>소프트웨어 코드가 수정되거나 프롬프트 엔지니어링이 변경되어 CI/CD 빌드가 트리거되면, 테스트 자동화 스크립트(예: Python의 <code>pytest</code> 기반 프레임워크)가 가동되며 다음의 정교한 검증 로직이 순차적으로 수행된다.</p>
<ol>
<li>
<p><strong>AI 모델 질의 전송:</strong> 테스트 프레임워크는 RAG 챗봇의 API 엔드포인트로 사전에 준비된 수백 개의 검증용 프롬프트를 차례대로 주입한다. 예를 들어, “직원의 연차 휴가 이월 규정과 신청 방법을 알려주세요“라는 엣지 케이스 질의를 보낸다.</p>
</li>
<li>
<p><strong>비결정론적 생성 응답 획득:</strong> 챗봇 시스템 내부의 LLM은 RAG 파이프라인을 거쳐 내부 가중치에 따라 문장을 생성한다. 모델의 확률적 특성으로 인해 “연차는 다음 해로 이월되지 않으며, 인트라넷을 통해 신청해야 합니다” 혹은 “당해 연도 미사용 휴가는 소멸됩니다. 결재 시스템에서 사전 승인을 받으십시오” 등 매번 형태와 길이가 다른 비결정론적 텍스트를 반환하게 된다.</p>
</li>
<li>
<p><strong>실제 응답의 실시간 임베딩 변환:</strong> 테스트 프레임워크는 챗봇 시스템이 방금 반환한 실제 응답 텍스트를 검증하기 위해, 즉시 <code>SentenceTransformers</code> 등의 파이썬 라이브러리나 API를 호출하여 해당 텍스트를 의미론적 평가를 위한 실수 벡터 <span class="math math-inline">B</span>로 인코딩(Encoding)한다.</p>
</li>
<li>
<p><strong>오라클 DB 네이티브 함수를 활용한 코사인 거리 측정:</strong> 생성된 실제 응답 벡터 <span class="math math-inline">B</span>를 오라클 데이터베이스에 SQL 쿼리의 바인딩 파라미터로 전달한다. 데이터베이스 엔진은 사전 저장되어 있던 해당 테스트 케이스의 기준 정답지 벡터 <span class="math math-inline">A</span>를 불러와, 두 벡터 사이의 수학적 거리를 네이티브 SQL 함수인 <code>VECTOR_DISTANCE</code>를 통해 메모리 상에서 고속으로 계산한다.</p>
<pre><code class="language-SQL">SELECT 
    test_case_id,
    expected_intent,
    -- VECTOR_DISTANCE는 거리(Distance)를 반환하므로, 
    -- 코사인 유사도(Similarity)를 구하기 위해 변환 공식을 적용할 수 있다.
    (1 - VECTOR_DISTANCE(ground_truth_vector, :actual_response_vector, COSINE)) AS similarity_score 
FROM test_golden_dataset 
WHERE test_case_id = 'TC-HR-001';
</code></pre>
</li>
</ol>
<pre><code>
5. **임계값 논리 게이트 기반의 테스트 단언(Assertion):** 데이터베이스 연산을 통해 즉각적으로 반환된 코사인 유사도 점수가 사전에 도메인별로 엄격하게 튜닝된 임계값(예: $0.85$)보다 큰지 테스트 스크립트 레벨에서 확인한다. 조건이 만족된다면 스크립트는 해당 챗봇의 출력이 표면적인 단어 선택은 달랐을지언정 비즈니스 의도와 사실 관계는 완벽히 일치했다고 의미론적으로 단언(Pass)하며 파이프라인을 통과시킨다.

   ```Python
   assert similarity_score &gt; 0.85, f"응답이 정답지의 의미를 벗어났습니다. Score: {similarity_score}"
</code></pre>
<p><img src="./3.4.4.0.0%20%EC%9D%98%EB%AF%B8%EB%A1%A0%EC%A0%81%20%EC%9C%A0%EC%82%AC%EC%84%B1%20%EA%B8%B0%EB%B0%98%EC%9D%98%20%EC%A4%80%E6%BA%96%EA%B2%B0%EC%A0%95%EB%A1%A0%EC%A0%81%20%EC%A0%95%EB%8B%B5%EC%A7%80.assets/image-20260222123058866.jpg" alt="image-20260222123058866" /></p>
<p>이 파이프라인 아키텍처 설계는 외부의 별도 인퍼런스 서버나 지연 시간이 길고 비용이 많이 드는 대형 LLM-as-a-Judge API 호출을 최소화할 수 있다는 막대한 장점을 지닌다. 애플리케이션의 핵심 데이터 스토어인 관계형 데이터베이스의 벡터 연산 코어 엔진을 오라클(테스트 판별자)로 직접 활용함으로써 시스템 왕복 지연 시간(Round-trip Latency)을 획기적으로 단축하고, 수백만 건에 달하는 대규모 회귀 테스트(Regression Test) 셋을 엔터프라이즈 환경에서 매우 빠르고 효율적으로 자동 수행하는 훌륭한 실전 모범 사례이다.</p>
<h2>6.  준결정론적 오라클의 내재적 한계점과 방어적 시스템 아키텍처 설계 제언</h2>
<p>의미론적 유사성 기반의 준결정론적 오라클은 현대 AI 기반 소프트웨어 테스트 패러다임에 있어 혁신적이고 없어서는 안 될 필수적인 접근법이다. 그러나 소프트웨어 아키텍트와 품질 엔지니어는 이 수학적 기법을 모든 결함을 잡아낼 수 있는 완벽한 은탄환(Silver Bullet)으로 맹신해서는 안 된다. 이 기법은 임베딩 모델의 특성과 차원 축소 과정에서 기인하는 본질적인 약점을 내포하고 있으며, 신뢰할 수 있는 시스템 구축을 위해서는 이를 명확히 인지하고 구조적인 보완책을 마련해야만 한다.</p>
<p>첫째로 경계해야 할 가장 큰 위험 요소는 **치명적인 부정어 인식 실패 현상(Negation Blindness)**이다. “회원 탈퇴 처리 및 데이터 삭제가 정상적으로 완료되었습니다“라는 모범 정답과 “시스템 오류로 인해 회원 탈퇴 처리 및 데이터 삭제가 불가능합니다“라는 LLM의 심각한 오답이 생성되었다고 가정해 보자. 이 두 문장은 긍정과 부정을 가르는 단어 하나를 제외하고는 문장의 구조, 사용된 핵심 어휘, 도메인 컨텍스트가 극도로 유사하다. 텍스트를 단순한 의미의 군집으로 파악하는 훈련이 덜 된 일반적인 문장 수준 텍스트 임베딩 모델이나 과거의 Word2Vec 기반 모델을 거칠 경우, 이 두 텍스트는 벡터 공간 상에서 매우 가까운 곳에 배치되어 비정상적으로 높은 코사인 유사도 점수를 나타낼 위험이 다분하다. 오라클은 이 높은 점수만을 보고 치명적인 논리적 결함과 시스템 오작동을 버그 없는 ’테스트 성공’으로 둔갑시키는 끔찍한 위양성 결과를 초래할 수 있다.</p>
<p>이러한 재앙을 방지하기 위해서는 앞서 살펴본 BERTScore와 같이 문맥 내 단어의 위치, 시퀀스의 흐름, 그리고 특히 부정어가 전체 의미망에 미치는 영향력을 민감하고 정밀하게 포착하는 고도로 튜닝된 트랜스포머 기반 평가 모델을 결합하여 오라클의 인지 능력을 보강해야 한다.</p>
<p>둘째, **오라클 시스템의 유지보수 취약성과 임베딩 모델 종속성(Model Dependency)**의 문제다. 테스트 프레임워크가 특정 벤더의 임베딩 모델(예: OpenAI의 text-embedding 시리즈)이나 특정 오픈소스 모델의 버전에 강하게 종속될 경우, 추후 기반 모델이 업데이트되거나 파인튜닝(Fine-tuning)을 통해 모델의 가중치가 변경될 때마다 생성되는 벡터 공간의 기하학적 구조가 완전히 뒤틀리게 된다. 이는 코사인 유사도 점수의 전반적인 분포 양상을 과거와 완전히 다르게 변화시킨다. 즉, 어제까지는 문제없이 테스트를 통과했던 동일한 <span class="math math-inline">0.85</span> 임계값 로직이, 모델 업데이트가 일어난 다음 날 아침에는 아무런 소스 코드 변경이 없었음에도 수천 개의 테스트 케이스를 연쇄적으로 실패하게 만들 수 있다는 의미이다.</p>
<p>따라서 기준 정답지의 기준 벡터 데이터와 오라클의 임계값 판별 로직은 애플리케이션의 소스 코드 자산과 동등한 수준으로 취급되어야 하며, 임베딩 모델의 버전 정보와 함께 강력한 형상 관리(Version Control) 시스템 내에서 묶여서 철저히 관리되어야 한다. 모델이 변경될 때마다 임계값을 통계적으로 재조정하는 자동화 파이프라인의 구축도 필수적이다.</p>
<p>이러한 수학적, 구조적 한계들로 인해, 소프트웨어 산업계의 가장 선도적인 품질 보증 파이프라인은 결코 의미론적 유사성이라는 단일 지표 하나에만 시스템의 안정성을 의존하지 않는다. 데이터베이스 업데이트나 특정 API 엔드포인트 호출 성공 여부를 확인하고, LLM이 반환하는 JSON의 스키마 구조(JSON Schema)가 시스템 규격과 정확히 일치하는지를 무결하게 검증하는 **강력한 결정론적 논리 평가자(Deterministic Logic Evaluator)**를 파이프라인의 1차 방어선으로 구축한다. 그리고 코사인 유사도를 측정하는 **의미론적 평가자(Semantic Evaluator)**를 그 뒤에 배치하여 겹겹이 쌓아 올린 다층적(Multi-layered)이고 하이브리드(Hybrid) 형태의 오라클 프레임워크를 구성한다. 의미론적 유사성 검증은 이러한 견고한 논리적 방어선들 사이에서, 비결정론적 AI 언어 모델이 보여주는 예측 불가능한 유연함을 인간 수준의 직관력으로 이해하고 수용하는 가장 중추적이고 혁신적인 역할을 담당하게 될 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>The Art of Validating Non-deterministic AI Responses | by Swetha …, https://medium.com/slalom-build/the-art-of-validating-non-deterministic-ai-responses-5f22c18cb24d</li>
<li>Testing AI Systems: Handling the Test Oracle Problem, https://dev.to/qa-leaders/testing-ai-systems-handling-the-test-oracle-problem-3038</li>
<li>4 Frameworks to Test Non-Deterministic AI Agent Behavior - Datagrid, https://datagrid.com/blog/4-frameworks-test-non-deterministic-ai-agents</li>
<li>Testing AI Applications: How to Validate Non-Deterministic Outputs, https://medium.com/@navanathjadhav/testing-ai-applications-how-to-validate-non-deterministic-outputs-3c02c086e567</li>
<li>How does a chatbot select and optimize a semantic similarity model?, https://www.tencentcloud.com/techpedia/127717</li>
<li>A COMPARATIVE ANALYSIS OF JACCARD AND COSINE … - ijireeice, https://ijireeice.com/wp-content/uploads/2025/03/IJIREEICE.2025.13309.pdf</li>
<li>Text-Similarity-Theory - Kaggle, https://www.kaggle.com/code/jurk06/text-similarity-theory</li>
<li>Evaluation Metrics in Learning Systems: A Survey[v1] | Preprints.org, https://www.preprints.org/manuscript/202508.1594</li>
<li>BERTScore For LLM Evaluation - Comet, https://www.comet.com/site/blog/bertscore-for-llm-evaluation/</li>
<li>LLM Evaluation metrics explained. ROUGE score, BLEU, Perplexity, https://medium.com/data-science-in-your-pocket/llm-evaluation-metrics-explained-af14f26536d2</li>
<li>[1904.09675] BERTScore: Evaluating Text Generation with BERT, https://ar5iv.labs.arxiv.org/html/1904.09675</li>
<li>Advancing Robust and Aligned Measures of Semantic Similarity in, https://www2.eecs.berkeley.edu/Pubs/TechRpts/2024/EECS-2024-84.pdf</li>
<li>BERTScore in AI: Enhancing Text Evaluation - Galileo AI, https://galileo.ai/blog/bert-score-explained-guide</li>
<li>BERTScore: A Contextual Metric for LLM Evaluation - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2025/04/bertscore-a-contextual-metric-for-llm-evaluation/</li>
<li>What Is Cosine Similarity? | IBM, https://www.ibm.com/think/topics/cosine-similarity</li>
<li>Understanding Similarity Search with Cosine Similarity - CodeSignal, https://codesignal.com/learn/courses/implementing-semantic-search-with-chromadb-1/lessons/understanding-similarity-search-with-cosine-similarity</li>
<li>Cosine Similarity - GeeksforGeeks, https://www.geeksforgeeks.org/dbms/cosine-similarity/</li>
<li>[PDF] BERTScore: Evaluating Text Generation with BERT, https://www.semanticscholar.org/paper/BERTScore%3A-Evaluating-Text-Generation-with-BERT-Zhang-Kishore/295065d942abca0711300b2b4c39829551060578</li>
<li>(PDF) BARTScore: Evaluating Generated Text as Text Generation, https://www.researchgate.net/publication/353071210_BARTScore_Evaluating_Generated_Text_as_Text_Generation</li>
<li>Brief Review — BERTScore: Evaluating Text Generation with BERT, https://sh-tsang.medium.com/brief-review-bertscore-evaluating-text-generation-with-bert-0bc5fc889d7b</li>
<li>BARTSCORE: Evaluating Generated Text as Text Generation, https://openreview.net/pdf?id=5Ya8PbvpZ9</li>
<li>Hardening the RAG chatbot architecture powered by Amazon Bedrock, https://aws.amazon.com/blogs/security/hardening-the-rag-chatbot-architecture-powered-by-amazon-bedrock-blueprint-for-secure-design-and-anti-pattern-migration/</li>
<li>Build Database Chatbots with SQL Dialog Skills | Oracle ASEAN, https://www.oracle.com/asean/artificial-intelligence/build-database-chatbots-with-sql/</li>
<li>Build an Agentic, High-Fidelity, Conversational AI Framework with, https://docs.oracle.com/en/solutions/select-ai-apex-framework/index.html</li>
<li>Oracle AI Database 26ai in Production - DEV Community, https://dev.to/kaustubhyerkade/oracle-ai-database-26ai-in-production-3f15</li>
<li>Build an AI Chatbot Engine with Oracle Database 23ai and OCI, https://www.youtube.com/watch?v=ZtbrcJZIWZY</li>
<li>How to Build an AI-Powered T-SQL Assistant with Python &amp; SQL, https://www.red-gate.com/simple-talk/databases/sql-server/an-ai-powered-t-sql-assistant-built-with-python-and-sql-server/</li>
<li>Create a Full RAG Pipeline Using Only Oracle Database 23ai, https://medium.com/@nikhil2000agrawal/create-a-rag-pipeline-using-only-oracle-database-23ai-025174afd619</li>
<li>Bertscore: A Comprehensive Guide for 2025 - Shadecoder, https://www.shadecoder.com/topics/bertscore-a-comprehensive-guide-for-2025</li>
<li>The New World of Non-Deterministic Testing and Evaluation - Cresta, https://cresta.com/blog/the-new-world-of-non-deterministic-testing-and-evaluation</li>
<li>(PDF) Verifiable LLM-Generated Test Oracles - ResearchGate, https://www.researchgate.net/publication/398511554_Verifiable_LLM-Generated_Test_Oracles_Ensuring_Consistency_Correctness_and_Explainability_in_AI-_Assisted_Testing</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>