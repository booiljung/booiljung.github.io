<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.4.2.1 수학적 연산 결과 및 코드 실행 결과의 일치 여부</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.4.2.1 수학적 연산 결과 및 코드 실행 결과의 일치 여부</h1>
                    <nav class="breadcrumbs"><a href="../../../../../index.html">Home</a> / <a href="../../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../../index.html">Chapter 3. 결정론적 정답지(Deterministic Ground Truth)의 설계 원칙과 필요성</a> / <a href="../index.html">3.4 정답지의 유형별 분류와 구축 전략</a> / <a href="index.html">3.4.2 로직/연산 기반 정답지 (Logic/Calculation-based Ground Truth)</a> / <span>3.4.2.1 수학적 연산 결과 및 코드 실행 결과의 일치 여부</span></nav>
                </div>
            </header>
            <article>
                <h1>3.4.2.1 수학적 연산 결과 및 코드 실행 결과의 일치 여부</h1>
<p>인공지능(AI) 기반 소프트웨어 개발의 핵심적인 과제 중 하나는 비결정론적(Nondeterministic) 특성을 지닌 거대 언어 모델(LLM)의 출력을 어떻게 검증하고 신뢰할 수 있는 소프트웨어 구성 요소로 통합하느냐에 있다. 전통적인 소프트웨어 공학에서 ’오라클(Oracle)’은 특정 입력에 대해 프로그램이 산출해야 하는 올바른 결과를 판별하는 메커니즘을 의미하며, 이는 테스트 자동화의 가장 큰 병목 구간으로 알려져 왔다. 특히 AI가 수학적 추론이나 코드 작성을 수행할 때, 그 결과물이 단순히 텍스트적으로 그럴듯해 보이는 것(Plausibility)과 실제로 논리적·산술적으로 정확한 것(Correctness) 사이에는 상당한 간극이 존재한다. 이러한 간극을 메우기 위해 도입되는 것이 바로 로직 및 연산 기반의 결정론적 정답지이며, 그중에서도 수학적 연산 결과와 코드 실행 결과의 일치 여부를 판별하는 프로세스는 가장 엄격하고 객관적인 검증 수단으로 기능한다.</p>
<h2>1. 실행 기반 오라클의 개념적 기초와 오라클 문제의 전이</h2>
<p>소프트웨어 테스트 분야에서 오라클 문제는 테스트 대상 시스템(SUT)의 동작이 의도된 명세와 일치하는지 판단하기 위한 ’정답’을 정의하는 작업의 어려움을 뜻한다. AI 시대 이전에도 변형 테스트(Metamorphic Testing)나 계약 기반 개발(Contract-driven Development) 등을 통해 오라클을 자동화하려는 시도가 있었으나, 최종적인 판단은 흔히 인간의 직관이나 도메인 지식에 의존하는 ’인간 오라클’에 머무르는 경우가 많았다. 그러나 AI가 생성하는 코드나 수식의 복잡도가 인간의 즉각적인 검토 범위를 넘어서면서, 실행 가능한 증거(Executable Evidence)에 기반한 결정론적 오라클의 중요성이 대두되었다.</p>
<p>수학적 연산과 코드 실행 결과의 일치 여부를 확인하는 것은 모델의 ’언어적 주장’을 ’물리적 실행’과 대조하는 과정이다. 예를 들어, AI 모델이 어떤 수학 문제를 해결하기 위해 단계별 추론(Chain-of-Thought, CoT)을 제시했을 때, 각 단계에서 수행된 산술 연산이 실제 수학적 법칙에 부합하는지, 그리고 최종적으로 도출된 수치가 코드 인터프리터에서 계산된 값과 일치하는지를 검증함으로써 오라클의 자동화를 달성할 수 있다. 이는 모델의 내부적인 환각(Hallucination) 현상을 외부의 확정적인 실행 환경을 통해 필터링하는 강력한 장치가 된다.</p>
<h2>2. 수학적 추론의 결정론적 검증: PAL과 도구 활용 추론</h2>
<p>거대 언어 모델은 고도의 추론 능력을 갖추고 있음에도 불구하고, 아주 단순한 산술 연산에서조차 오류를 범하는 ’계산의 취약성’을 노출한다. 이러한 한계를 극복하기 위해 제안된 기법이 “Program-Aided Language Models(PAL)“이다. PAL은 모델에게 정답을 직접 계산하게 하는 대신, 문제 해결 로직을 파이썬(Python)과 같은 프로그래밍 언어로 작성하도록 유도한다. 이후 작성된 코드를 파이썬 인터프리터에서 실행하고, 그 실행 결과(Return Value)를 최종적인 정답으로 채택한다.</p>
<p>이 방식에서 결정론적 정답지는 모델의 텍스트 응답이 아니라, 모델이 생성한 알고리즘이 산출한 ’실행 결과’가 된다. PAL은 추론 단계의 분해(Decomposition)는 언어 모델이 담당하게 하고, 실제 연산(Solving)은 결정론적인 런타임에 위임함으로써 정확도를 획기적으로 높인다. 연구에 따르면 PAL 방식은 GSM8K와 같은 수학 문장제 데이터셋에서 일반적인 CoT 방식보다 우수한 성능을 보이며, 특히 복잡한 수치가 포함된 GSM-HARD 데이터셋에서는 약 40% 이상의 절대적인 정확도 향상을 기록하였다. 이는 수치적 일관성이 AI 신뢰성 확보의 핵심 지표임을 시사한다.</p>
<table><thead><tr><th><strong>추론 방식</strong></th><th><strong>로직 수립 주체</strong></th><th><strong>연산 수행 주체</strong></th><th><strong>오라클 특성</strong></th></tr></thead><tbody>
<tr><td>Chain-of-Thought (CoT)</td><td>LLM</td><td>LLM (내부 토큰 예측)</td><td>비결정론적, 계산 오류 발생 가능성 높음</td></tr>
<tr><td>Program-Aided Language Models (PAL)</td><td>LLM</td><td>외부 인터프리터 (Python 등)</td><td>결정론적, 연산 정확성 100% 보장</td></tr>
<tr><td>ReAct (Reason + Act)</td><td>LLM</td><td>외부 API / 도구</td><td>상호작용적, 외부 관측 결과에 기반함</td></tr>
<tr><td>Multi-Agent Deliberation</td><td>복수 에이전트</td><td>샌드박스 실행</td><td>협력적 검증, 실행 결과 피드백 루프 포함</td></tr>
</tbody></table>
<h2>3. 코드 생성의 기능적 정확성(Functional Correctness) 판별</h2>
<p>코드 생성 AI의 성능을 평가할 때 가장 널리 사용되는 결정론적 오라클은 ’유닛 테스트(Unit Test)’이다. “Evaluating Large Language Models Trained on Code” 연구에서 제시된 HumanEval 벤치마크는 모델이 생성한 코드가 단순히 참조 코드와 문자열적으로 얼마나 유사한지(BLEU 점수 등)를 보지 않는다. 대신, 해당 코드가 주어진 테스트 케이스를 모두 통과하여 의도한 출력을 내놓는지, 즉 ’기능적 정확성’을 측정한다.</p>
<p>이 과정에서 사용되는 핵심 메트릭은 <span class="math math-inline">pass@k</span>이다. 모델이 생성한 <span class="math math-inline">n</span>개의 코드 샘플 중 <span class="math math-inline">c</span>개가 유닛 테스트를 통과했을 때, 무작위로 <span class="math math-inline">k</span>개를 선택했을 때 적어도 하나 이상의 정답이 포함될 확률을 계산한다.<br />
<span class="math math-display">
\text{pass@k} := E_{\text{Problems}} \left[ 1 - \frac{\binom{n-c}{k}}{\binom{n}{k}} \right]
</span><br />
이 수식은 수치적 안정성을 위해 로그 스케일이나 반복적인 항별 계산을 통해 구현되며, 모델의 코드 생성 능력을 가장 객관적으로 나타내는 지표로 인정받고 있다. 여기서 유닛 테스트는 결정론적 오라클로서 작동하며, 코드 실행 결과가 기대값과 단 1비트라도 다를 경우 ’불일치’로 판정한다. 이는 AI 소프트웨어 개발에서 결과의 무결성을 보장하는 가장 엄격한 기준이다.</p>
<h2>4. Nexus: 실행 접지형 다중 에이전트 오라클 합성</h2>
<p>단순한 유닛 테스트를 넘어, 더 복잡한 함수의 동작을 검증하기 위한 오라클을 자동으로 생성하려는 시도도 이어지고 있다. “Nexus: Execution-Grounded Multi-Agent Test Oracle Synthesis” 프레임워크는 다중 에이전트 협업을 통해 신뢰할 수 있는 오라클을 합성한다. Nexus는 숙의(Deliberation), 검증(Validation), 자기 정교화(Self-refinement)의 단계를 거친다.</p>
<p>숙의 단계에서는 명세 전문가(Specification Expert), 경계값 전문가(Edge Case Specialist) 등 서로 다른 테스트 철학을 가진 에이전트들이 오라클 후보를 생성하고 비판한다. 특히 중요한 것은 검증 단계인데, 여기서 Nexus는 모델이 생성한 함수 구현체를 실제 샌드박스 환경에서 실행하여 오라클의 유효성을 직접 확인한다. 만약 실행 과정에서 런타임 에러가 발생하거나 논리적 모순이 발견되면, 실행 로그를 기반으로 오라클을 수정하는 자기 정교화 루프를 활성화한다. 이러한 실행 접지(Execution-grounding) 방식은 텍스트 기반 명세에만 의존할 때 발생할 수 있는 ’인식의 폐쇄성(Epistemological Echo Chamber)’을 타파하고, 물리적인 코드 실행 결과라는 객관적 사실에 정답지를 고정시킨다.</p>
<h2>5. 결정론적 실행 환경의 구축과 블록체인 사례</h2>
<p>실행 결과의 일치 여부를 오라클로 활용하기 위해서는 실행 환경 자체가 결정론적이어야 한다. 이더리움 가상 머신(EVM)과 같은 환경은 이러한 특성을 극단적으로 활용한 사례이다. EVM은 동일한 시작 상태와 트랜잭션 시퀀스가 주어지면 네트워크상의 모든 노드가 정확히 동일한 결과를 계산하도록 설계되어 있다.</p>
<p>AI 에이전트가 스마트 컨트랙트의 취약점을 탐지하거나 패치하는 성능을 측정하는 EVMbench 프레임워크는 이러한 결정론적 실행 표면(Deterministic Execution Surface)을 제공한다. 에이전트의 작업 결과는 온체인 이벤트, 잔액 변화(Balance Deltas), 상태 전이 등으로 즉각적이고 프로그램적으로 검증(Programmatic Verification)된다. 이는 AI 모델의 성능 평가에서 인간의 개입을 완전히 배제하고, 수학적·기술적 사실에 기반한 오라클을 구축할 수 있음을 보여준다.</p>
<table><thead><tr><th><strong>검증 요소</strong></th><th><strong>기술적 구현 방식</strong></th><th><strong>결정론 보장 메커니즘</strong></th></tr></thead><tbody>
<tr><td>상태 전이</td><td>잔액(Balance) 및 스토리지 변화 추적</td><td>모든 노드에서 동일한 상태 루트(State Root) 합의</td></tr>
<tr><td>온체인 이벤트</td><td>로그(Logs) 및 이벤트 이미션 검사</td><td>트랜잭션 실행 결과의 불변적 기록</td></tr>
<tr><td>패치 유효성</td><td>테스트 스위트 재실행</td><td>취약점 재현 코드의 실패 여부 확인</td></tr>
<tr><td>실행 격리</td><td>샌드박스 및 가상화</td><td>외부 변수(시간, 네트워크 등)의 통제</td></tr>
</tbody></table>
<h2>6. 수학적 추론의 취약성과 GSM-Symbolic의 통찰</h2>
<p>결정론적 정답지를 설계할 때 흔히 발생하는 함정은 모델이 특정 데이터셋의 정답을 암기하는 것이다. Apple의 연구진이 발표한 “GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models“는 이러한 한계를 극복하기 위해 상징적 템플릿을 활용한 검증 방식을 제안한다.</p>
<p>GSM-Symbolic은 기존 GSM8K 문제의 수치를 무작위로 변경하거나, 정답과 무관한 문구(Irrelevant Clauses)를 삽입하여 수천 개의 변형 문제를 생성한다. 연구 결과, 최신 LLM들은 문제의 논리 구조가 동일함에도 불구하고 단순히 숫자 값만 바뀌었을 때 실행 결과가 크게 흔들리는 ’추론의 취약성’을 보였다. 이는 모델이 진정한 수학적 원리를 이해하는 것이 아니라 패턴 매칭에 의존하고 있음을 시사한다. 따라서 강력한 오라클은 고정된 하나의 정답지가 아니라, 문제의 상징적 변형에 대해서도 일관된 실행 결과를 내놓는지 확인하는 ’견고성 검증’으로 확장되어야 한다.</p>
<h2>7. 논리적 오류와 산술적 오류의 분리 진단</h2>
<p>수학적 연산 결과의 불일치를 분석할 때, 오류의 원인을 정밀하게 분류하는 것은 오라클 설계의 고도화 단계이다. “GSM-Ranges” 연구는 오류를 논리적 오류(Logical Error)와 비논리적 오류(Non-logical Error)로 구분할 것을 제안한다.</p>
<ol>
<li><strong>논리적 오류:</strong> 문제 해결을 위한 수식 수립 자체가 틀렸거나, 필요한 단계를 누락한 경우이다.</li>
<li><strong>비논리적 오류:</strong> 수식은 올바르게 세웠으나 단순 계산에서 실수(Arithmetic Error)를 하거나, 문제의 수치를 잘못 복사(Copy Error)한 경우이다.</li>
</ol>
<p>이를 자동화하기 위해 모델의 응답을 파이썬 코드로 변환한 뒤 실행하여, 계산 실수를 제거했을 때 정답이 도출되는지 확인하는 기법이 사용된다. 만약 계산 실수만 바로잡았을 때 정답과 일치한다면 이는 논리적으로는 유효한 응답으로 간주할 수 있다. 이러한 미세 조정된 오라클은 AI 모델의 실제 추론 능력을 정확하게 측정하고 개선 방향을 제시하는 데 기여한다.</p>
<h2>8. 실행 오라클을 위한 인프라와 보안 가이드라인</h2>
<p>모델이 생성한 코드를 직접 실행하여 결과를 확인하는 과정은 보안상의 위험을 수반한다. 악의적인 코드가 시스템 환경을 오염시키거나 데이터를 탈취하는 것을 방지하기 위해, 오라클 시스템은 반드시 격리된 샌드박스 환경을 갖추어야 한다.</p>
<p>HumanEval의 실행 프레임워크는 다음과 같은 보안 계층을 권장한다.</p>
<ul>
<li><strong>gVisor 컨테이너 런타임:</strong> 호스트 시스템과 실행 코드 사이의 강력한 보안 경계를 생성하여 시스템 콜을 제어한다.</li>
<li><strong>eBPF 기반 네트워크 차단:</strong> 실행 중인 코드가 외부 서버와 통신하는 것을 원천적으로 봉쇄한다.</li>
<li><strong>리소스 제약(Cgroups):</strong> 메모리와 CPU 사용량을 엄격히 제한하여 무한 루프나 자원 고갈 공격을 방지한다.</li>
</ul>
<p>또한, 실행 결과의 일치 여부를 판단할 때 소수점 오차나 부동 소수점 연산의 정밀도 문제를 처리하기 위한 수치적 허용 범위(Tolerance) 설정도 중요하다. 이는 특히 고등 수학이나 물리 연산이 포함된 정답지를 구축할 때 필수적인 고려 사항이다.</p>
<h2>9. 실행 트레이스 추출과 자기 지도 학습으로의 확장</h2>
<p>최근에는 코드 실행 중 발생하는 중간 상태 정보를 추출하여 이를 추론 데이터로 활용하는 방식이 주목받고 있다. 실행 트레이스(Execution Trace)는 변수의 할당, 제어 흐름의 변화, 중간 연산 결과 등을 포함하며, 이는 모델이 따라야 할 완벽한 ’논리적 경로’를 제공한다.</p>
<p>디버거를 통해 추출된 이러한 트레이스 정보는 LLM에 의해 자연어 형태의 추론 과정(CoT)으로 번역되며, 이는 다시 모델을 학습시키기 위한 고품질의 결정론적 정답지로 활용된다. 이러한 선순환 구조는 AI가 생성한 결과물을 실행 결과라는 객관적 잣대로 검증하고, 그 검증된 데이터를 다시 학습에 사용하는 ‘자기 지도적 오라클(Self-supervised Oracle)’ 체계를 가능하게 한다.</p>
<h2>10. 결론: 물리적 실행을 통한 진실의 접지</h2>
<p>수학적 연산 결과와 코드 실행 결과의 일치 여부를 검증하는 것은 AI 소프트웨어 개발에서 가장 높은 수준의 결정론을 제공한다. 언어 모델의 출력이 지니는 확률적 모호성을 제거하기 위해, 우리는 물리적인 실행 환경이라는 필터를 거쳐야만 한다. PAL과 같은 도구 보조 추론, <span class="math math-inline">pass@k</span>와 같은 기능적 정확성 지표, 그리고 Nexus와 같은 실행 접지형 다중 에이전트 시스템은 모두 이러한 ’진실의 접지(Grounding in Truth)’를 향한 여정의 산물이다.</p>
<p>결국 결정론적 정답지는 고정된 텍스트 데이터셋에 머무르는 것이 아니라, 실행 가능하고(Executable), 검증 가능하며(Verifiable), 다양한 변형에도 견고한(Robust) 논리 구조의 집합이어야 한다. 수학과 코드의 일치 여부를 추적하는 프로세스를 통해, 개발자는 AI가 생성한 코드 조각을 단순한 텍스트 뭉치가 아닌, 신뢰할 수 있는 소프트웨어 자산으로 변모시킬 수 있다. 이는 비결정론적인 AI 모델 위에서 결정론적인 시스템을 구축해야 하는 현대 소프트웨어 엔지니어링의 핵심 원칙이다.</p>
<h2>11. 참고 자료</h2>
<ol>
<li>The Oracle Problem in Software Testing: A Survey - IEEE Xplore, https://ieeexplore.ieee.org/iel7/32/7106034/06963470.pdf</li>
<li>(PDF) The Oracle Problem in Software Testing: A Survey, https://www.researchgate.net/publication/276255185_The_Oracle_Problem_in_Software_Testing_A_Survey</li>
<li>Nexus: Execution-Grounded Multi-Agent Test Oracle Synthesis, https://openreview.net/forum?id=lbZNHMqMAI</li>
<li>Evaluating Large Language Models Trained on Code - arXiv, https://arxiv.org/pdf/2107.03374</li>
<li>Nexus: Execution-Grounded Multi-Agent Test Oracle Synthesis - arXiv, https://arxiv.org/html/2510.26423v1</li>
<li>(PDF) PAL: Program-aided Language Models - ResearchGate, https://www.researchgate.net/publication/365614638_PAL_Program-aided_Language_Models</li>
<li>MetaMath: Integrating Natural Language and Code for Enhanced, https://arxiv.org/html/2409.19381v1</li>
<li>Applications of Large Language Models - ALIN Lab, https://alinlab.kaist.ac.kr/resource/2025_SPRING_AI602/AI602_Lec3_Applications_of_Large_Language_Models.pdf</li>
<li>Evaluating Large Language Models Trained on Code - Academia.edu, https://www.academia.edu/94807952/Evaluating_Large_Language_Models_Trained_on_Code</li>
<li>Brief Review — Codex: Evaluating Large Language Models Trained, https://sh-tsang.medium.com/brief-review-codex-evaluating-large-language-models-trained-on-code-d01866a54bbf</li>
<li>Nexus: Execution-Grounded Multi-Agent Test Oracle Synthesis, https://www.researchgate.net/publication/397087653_Nexus_Execution-Grounded_Multi-Agent_Test_Oracle_Synthesis</li>
<li>EVMbench: Evaluating AI Agents on Smart Contract Security - OpenAI, https://cdn.openai.com/evmbench/evmbench.pdf</li>
<li>Apple Machine Learning Research at ICLR 2025, https://machinelearning.apple.com/research/iclr-2025</li>
<li>GSM-Symbolic: Understanding the Limitations of Mathematical, https://arxiv.org/html/2410.05229v2</li>
<li>GSM-Symbolic: Understanding the Limitations of Mathematical, https://arxiv.org/pdf/2410.05229</li>
<li>GSM-Symbolic: Understanding the Limitations of Mathematical, https://openreview.net/forum?id=AjXkRZIvjB</li>
<li>GSM-Symbolic: Analyzing LLM Limitations in Mathematical, https://medium.com/data-science/gsm-symbolic-analyzing-llm-limitations-in-mathematical-reasoning-and-potential-solutions-363b82370a26</li>
<li>Assessing Logical and Arithmetic Errors across Wide Numerical, https://arxiv.org/html/2502.08680v1</li>
<li>openai/human-eval: Code for the paper “Evaluating Large … - GitHub, https://github.com/openai/human-eval</li>
<li>Code Execution as Grounded Supervision for LLM Reasoning - arXiv, https://arxiv.org/html/2506.10343v2</li>
<li>NeurIPS Poster Is Your Code Generated by ChatGPT Really Correct, https://neurips.cc/virtual/2023/poster/72990</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>