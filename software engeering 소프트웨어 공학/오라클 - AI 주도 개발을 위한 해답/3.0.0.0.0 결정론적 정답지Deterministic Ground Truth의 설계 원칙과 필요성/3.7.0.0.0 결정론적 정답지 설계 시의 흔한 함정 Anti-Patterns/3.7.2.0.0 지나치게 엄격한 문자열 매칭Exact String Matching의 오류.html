<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.7.2 지나치게 엄격한 문자열 매칭(Exact String Matching)의 오류</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.7.2 지나치게 엄격한 문자열 매칭(Exact String Matching)의 오류</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 3. 결정론적 정답지(Deterministic Ground Truth)의 설계 원칙과 필요성</a> / <a href="index.html">3.7 결정론적 정답지 설계 시의 흔한 함정 (Anti-Patterns)</a> / <span>3.7.2 지나치게 엄격한 문자열 매칭(Exact String Matching)의 오류</span></nav>
                </div>
            </header>
            <article>
                <h1>3.7.2 지나치게 엄격한 문자열 매칭(Exact String Matching)의 오류</h1>
<p>전통적인 소프트웨어 공학에서 테스트 자동화와 품질 검증의 핵심은 ’결정론적 기대 결과(Deterministic Expected Result)’와 ‘실제 결과(Actual Result)’ 간의 일치 여부를 확인하는 것이었다. 이러한 패러다임 아래에서 가장 직관적이고 널리 사용된 검증 도구가 바로 문자열 매칭(String Matching)이다. 그러나 비결정론적(Nondeterministic)이고 확률적인 성격을 지닌 대형 언어 모델(LLM) 및 생성형 AI의 등장으로 인해, 과거의 완벽했던 검증 기준이 오히려 AI 시스템의 실제 성능을 심각하게 왜곡하는 안티패턴(Anti-Pattern)으로 전락하고 있다. 결정론적 정답지(Deterministic Ground Truth) 설계 시 가장 빈번하게 발생하는 함정인 ’지나치게 엄격한 문자열 매칭(Exact String Matching)의 오류’를 심층적으로 해부하고, 이로 인해 파생되는 테스트 오라클(Test Oracle)의 붕괴 현상과 그 대안을 실전 예제를 통해 탐구한다.</p>
<h2>1.  전통적 소프트웨어 테스트와 문자열 매칭의 유산</h2>
<p>문자열 매칭 알고리즘은 1970년대 이후 컴퓨터 과학 분야에서 가장 활발하게 연구된 주제 중 하나이며, 텍스트 마이닝, 자연어 처리, 정보 검색, 데이터 압축, 생물정보학(DNA 시퀀스 분석) 등 다양한 영역의 근간을 이루고 있다. 주어진 텍스트 데이터 내에서 특정 패턴이 정확히 존재하는지를 찾아내는 이 기술은 시스템의 출력이 1비트의 오차도 없이 예상과 일치해야 하는 결정론적 소프트웨어 환경에서 절대적인 신뢰성을 제공했다.</p>
<h3>1.1  정확한 문자열 매칭(Exact String Matching) 알고리즘의 진화</h3>
<p>학술적으로 단일 패턴 정확 매칭(Single-pattern Exact Matching) 알고리즘은 텍스트와 패턴의 문자를 일일이 비교하는 브루트 포스(Brute-force) 방식에서 출발했다. 브루트 포스 알고리즘은 최악의 경우 <span class="math math-inline">O(nm)</span> (여기서 <span class="math math-inline">n</span>은 텍스트의 길이, <span class="math math-inline">m</span>은 패턴의 길이)의 시간 복잡도를 가지며 사전 처리(Preprocessing)를 수행하지 않는다. 이후 알고리즘은 텍스트와 패턴의 특성을 활용하여 불필요한 비교를 건너뛰는 방향으로 비약적인 발전을 이루었다.</p>
<p>1970년 모리스(Morris)와 프랫(Pratt)이 제안하고 1977년 커누스(Knuth)가 발전시킨 KMP(Knuth-Morris-Pratt) 알고리즘은 불필요한 비교를 피하기 위해 패턴의 접두사와 접미사 정보를 활용하여 선형 시간 내에 매칭을 수행하는 이정표를 세웠다. 이후 보이어(Boyer)와 무어(Moore)가 제안한 Boyer-Moore 알고리즘은 텍스트 문자를 패턴의 오른쪽 끝에서부터 비교함으로써 실무적으로 가장 빠르고 널리 쓰이는 표준이 되었다. 이외에도 해싱(Hashing)을 활용한 Karp-Rabin 알고리즘, 컴퓨터의 워드 단위 병렬 처리(Word-size packed string matching) 능력을 극대화한 비트 병렬 처리(Bit-parallelism, 예: Shift-Or, Shift-And, BNDM) 방식, 그리고 유한 오토마타(Finite Automata)의 개념을 차용한 Suffix automata 및 팩터 오토마타(Factor automata) 방식 등이 고안되었다.</p>
<p>이들 알고리즘의 공통적인 설계 철학은 구문론적(Syntactic) 완벽함이다. 문자열의 의미(Semantics)나 문맥(Context)은 전혀 고려하지 않으며, 오직 알파벳이나 기호의 배열 순서와 길이가 바이트(Byte) 단위로 정확히 일치하는지만을 판별한다. 이는 전통적인 데이터베이스 시스템에서 정규 표현식(Regular Expression)을 통한 패턴 매칭(예: Oracle Database 12c의 <code>MATCH_RECOGNIZE</code> 구문)으로 확장되었으나, 이 역시 문자의 패턴 규칙을 기계적으로 판별할 뿐 문맥적 유연성을 허용하지는 않는다.</p>
<h3>1.2  단위 테스트 오라클로서의 엄격성</h3>
<p>소프트웨어 공학의 유닛 테스트(Unit Test) 프레임워크인 JUnit, PyTest 등에서 흔히 사용되는 <code>Assert.AreEqual(expected, actual)</code> 함수는 이러한 정확한 문자열 매칭의 철학을 그대로 계승한 것이다. 전통적인 시스템에서는 특정 입력에 대해 단 하나의 ‘올바른(Correct)’ 출력만이 존재하므로, 이러한 확정적 비교는 매우 효율적이고 정확한 테스트 오라클(Test Oracle)로 작동했다. 프로그램의 상태가 참인지 거짓인지를 명확하게 가를 수 있는 절대적 진리의 척도였던 셈이다. 하지만 AI가 생성하는 자연어, 코드, 정형 데이터 환경에 이 기준을 맹목적으로 적용할 경우, 시스템의 유연성과 창의성을 스스로 옥죄는 치명적인 한계에 봉착하게 된다.</p>
<h2>2.  AI 시대의 테스트 오라클 문제(The Oracle Problem)</h2>
<p>테스트 오라클(Test Oracle)이란 주어진 테스트 케이스에 대해 프로그램의 실행 결과가 올바른지 여부를 판별할 수 있는 메커니즘, 원천 자료, 혹은 기준을 의미한다. 소프트웨어 공학에서 ’오라클 문제(Oracle Problem)’란 정확한 기대 결과를 사전에 정의하거나 이를 검증하는 과정을 자동화하기가 비용이 너무 많이 들거나 사실상 불가능한 상황을 지칭한다.</p>
<p>전통적인 시스템에서는 개발자가 명세서를 바탕으로 기대 결괏값을 예측하고 하드코딩(Hardcoding)할 수 있었다. 테스트 오라클은 시스템의 버그 행동과 올바른 행동을 명확히 구분하는 그라운드 트루스(Ground-truth) 역할을 성실히 수행했다. 그러나 머신러닝 모델, 특히 생성형 AI 시스템에서는 이 오라클 문제가 극단적으로 발현된다. 대형 언어 모델은 근본적으로 확률적 디코딩(Probabilistic Decoding)을 수행하므로 비결정론적 성격을 띠며, 수천억 개의 파라미터가 상호작용하여 창발하는 출력은 사전에 정확한 문자열로 예측하는 것이 불가능에 가깝다.</p>
<p>텐서플로우(TensorFlow)와 같은 AI 프레임워크 자체의 시맨틱 버그를 잡기 위해 실행 가능한 시맨틱(Executable Semantics) 기반의 오라클이나 퍼징(Fuzzing) 엔진을 구축하려는 연구도 존재하지만 , 이는 모델의 컴파일이나 텐서 연산 자체의 무결성을 검증하는 것일 뿐, LLM이 텍스트를 생성하는 추론(Inference) 단계의 품질을 평가하는 오라클과는 성격이 다르다. 추론 단계에서 AI 시스템의 오라클은 ’정확성(Correctness)’의 개념을 재정의해야 한다.</p>
<p>동일한 프롬프트가 주어지더라도 모델의 온도(Temperature) 설정이나 샘플링 전략에 따라 매번 형태가 다르지만 의미적으로는 모두 타당한 응답을 생성할 수 있다. 예를 들어, “결제가 왜 실패했는가?“라는 사용자의 질문에 대해 전자상거래 플랫폼의 AI 고객 센터 챗봇은 다음과 같이 대답할 수 있다.</p>
<ol>
<li>“카드 한도 초과 또는 3D Secure 인증 문제로 인해 결제 실패가 발생할 수 있습니다.”</li>
<li>“잔액 부족으로 인해 고객님의 거래가 거절되었을 가능성이 있습니다.”</li>
</ol>
<p>위 두 응답은 모두 고객의 질문에 대한 합리적이고 정확한 답변이다. 하지만 전통적인 오라클 관점에서 테스트 시나리오의 기대 결과(Expected Result)를 “카드 한도 초과로 인한 결제 실패“라는 특정 문자열로 고정해 두었다면, 두 번째 응답은 완벽하게 틀린 답변으로 처리된다. 나아가 첫 번째 응답조차도 띄어쓰기 하나가 다르거나 “발생할 수 있습니다” 대신 “발생합니다“로 출력된다면 <code>Assert.AreEqual</code> 검증에서 실패(Fail)를 반환한다.</p>
<p>즉, 정답이 주관적이거나 다양한 형태를 가질 수 있는 AI 도메인에서 정확한 문자열 매칭을 테스트 오라클로 사용하는 것은 유효한 결과를 위음성(False Negative, 실제로는 정답임에도 오답으로 판별)으로 대량 분류하는 재앙적 결과를 초래한다. 결정론적 정답지가 필요한 이유는 AI 시스템의 성능을 계량화하고 CI/CD 파이프라인 내에서 자동화된 회귀 테스트(Regression Testing)를 수행하기 위함이지, AI 모델의 언어적 다양성을 단일한 문자열 템플릿으로 강제하기 위함이 아니다. 오라클은 텍스트의 표면적 일치에서 벗어나 의미적 근접성(Semantic Proximity)과 비즈니스 로직의 충족 여부를 판별하는 다층적 시스템으로 진화해야만 한다.</p>
<p><img src="./3.7.2.0.0%20%EC%A7%80%EB%82%98%EC%B9%98%EA%B2%8C%20%EC%97%84%EA%B2%A9%ED%95%9C%20%EB%AC%B8%EC%9E%90%EC%97%B4%20%EB%A7%A4%EC%B9%ADExact%20String%20Matching%EC%9D%98%20%EC%98%A4%EB%A5%98.assets/image-20260222204038141.jpg" alt="image-20260222204038141" /></p>
<h2>3.  지나치게 엄격한 매칭이 유발하는 치명적 오류의 유형</h2>
<p>정확한 문자열 매칭(Exact Match, 이하 EM)이 AI 소프트웨어 개발 및 검증 과정에서 치명적인 오류 지표로 작용하는 원인은 크게 네 가지 유형으로 분류할 수 있다. 이는 단순한 측정의 불편함을 넘어 모델의 잠재력을 억압하고 개발 파이프라인에 심각한 기술 부채를 누적시킨다.</p>
<h3>3.1  의미론적 동치성(Semantic Equivalence)의 훼손</h3>
<p>단일 패턴 매칭 알고리즘의 가장 큰 한계는 문자열에 내포된 인간의 언어적 맥락과 동의어, 패러프레이징(Paraphrasing)을 철저히 무시한다는 점이다. 단순한 텍스트 유사도를 측정하는 퍼지 매칭(Fuzzy Matching) 기법들 역시 이 문제를 근본적으로 해결하지 못한다. 예를 들어 두 문자열 간의 차이를 계산하기 위해 문자의 삽입(Insertion), 삭제(Deletion), 치환(Substitution) 횟수를 수학적으로 정량화한 편집 거리(Levenshtein Distance) 알고리즘이나 Jaro-Winkler 거리, 해밍 거리(Hamming Distance) 등은 철자 오류나 오타를 교정하는 데는 유용하지만 표면적인 변형만을 계산할 뿐 문맥과 의미를 포착하지는 못한다.</p>
<p>모델에게 “프랑스의 수도는 어디인가?“라고 질문했을 때, 결정론적 정답지가 “파리“로 설정되어 있다고 가정하자. 모델이 “프랑스의 수도는 파리입니다” 또는 “파리(Paris)“라는 문장을 출력하면, EM 관점에서는 기준 텍스트와 완벽히 일치하지 않으므로 완전히 오답 처리된다(EM Score = 0). 이는 모델이 정답을 모르는 것이 아니며 심지어 사용자에게는 더욱 정중하고 완성도 높은 답변을 제공했음에도 불구하고, 단지 평가 기준이 요구하는 특정한 구문론적 틀에 맞추지 않았다는 이유만으로 역량이 0점으로 저평가되는 현상이다.</p>
<p>정보 추출 태스크에서 정답지가 “INV-20394“라는 송장 번호일 때, 모델이 “INV20394“를 추출했다면 하이픈 하나가 누락되었다는 이유로 이 역시 오답이 된다. 특히 질의응답(QA)이나 대화형 에이전트에서는 의미론적으로 완벽히 동일하거나 심지어 부가적인 유용한 정보를 덧붙인(Improve over the reference) 답변이 빈번하게 등장하는데, EM이나 단순 문자열 거리 알고리즘은 이러한 언어의 유연성을 수용할 수 있는 메커니즘이 전무하다. 진정한 시맨틱 검색(Semantic Search)과 의미론적 매칭은 언어를 임베딩(Embedding)을 통해 다차원 벡터 공간의 의미로 매핑하여 의도(Intent)의 유사성을 파악해야만 달성될 수 있다.</p>
<h3>3.2  형식 및 구조적 변동(Formatting Variations)에 대한 취약성</h3>
<p>구조화된 출력(Structured Outputs)을 요구하는 환경, 예를 들어 대규모 언어 모델이 프롬프트의 지시에 따라 JSON 형태로 데이터를 추출해야 하는 상황을 고려해 보자.</p>
<ul>
<li>정답(Ground Truth): <code>{"product_name": "Laptop", "price": 1000}</code></li>
<li>모델 출력(Prediction): <code>{ "product_name" : "Laptop" , "price": 1000 }</code></li>
</ul>
<p>위 두 JSON 객체는 데이터 직렬화 및 파싱 관점에서는 완벽하게 동일한 구조와 스키마, 그리고 키-값(Key-Value) 쌍을 지닌다. 애플리케이션의 백엔드 시스템은 두 출력을 파싱하여 동일한 로직을 수행할 것이다. 그러나 공백(Whitespace), 줄바꿈 문자열(\n), 대소문자 표기 등 미세한 차이가 개입되는 순간, 바이트 단위의 문자열 비교 알고리즘은 이를 철저한 불일치로 판별한다.</p>
<p>이러한 형식적 취약성은 자연어를 데이터베이스 쿼리로 변환하는 Text-to-SQL 태스크에서도 극명하게 나타난다. SQL 쿼리 생성을 검증할 때, <code>SELECT * FROM users LIMIT 4</code>와 <code>SELECT * FROM users LIMIT 5</code>라는 예측 쿼리는 검색하려는 정보의 범주와 문법적 구조가 거의 동일하다. LLM이 생성한 쿼리 결과를 통해 데이터베이스에서 정보를 추출하고, 이를 다시 자연어로 요약하여 사용자에게 제공하는 파이프라인에서는 이러한 튜플 개수의 사소한 불일치가 전체 시스템의 성공 여부에 큰 영향을 미치지 않는다. 오히려 모델은 결과 데이터의 일부 차이를 허용하면서도 유용한 답변을 합성할 수 있는 관용도를 지닌다. 그럼에도 불구하고 실행 정확도(Execution Accuracy)나 엄격한 쿼리 매칭을 지표로 삼게 되면 이 출력은 완전히 다른 쿼리로 취급되어 0점을 받게 된다. 이는 평가 지표가 지나치게 가혹하게 적용되어 개발자로 하여금 모델이 실제보다 성능이 떨어진다고 착각하게 만드는 전형적인 오류다.</p>
<h3>3.3  ’깨진 태스크(Broken Tasks)’와 유연성 억압</h3>
<p>AI 에이전트(Agent)의 추론 능력, 다단계 의사결정 체계, 그리고 도구 사용(Tool Use) 능력을 평가할 때, 지나치게 엄격한 오라클 설계는 에이전트의 창의적인 문제 해결 능력을 도리어 ’오답’으로 치부하는 이른바 ‘깨진 태스크(Broken Tasks)’ 현상을 유발한다.</p>
<p>Anthropic의 AI 에이전트 자동 평가 시스템 구축 연구에 따르면, 평가 설계자가 의도치 않게 에이전트가 완수해야 할 목표의 성공 기준을 모호하게 작성하거나 평가기(Grader)를 비정상적으로 엄격하게 구현할 때 이러한 문제가 폭발적으로 발생한다. 평가의 대원칙은 ’모든 태스크는 지시사항을 정확히 따르는 에이전트가 통과할 수 있어야 한다’는 것이다.</p>
<p>Terminal-Bench 환경에 대한 감사(Audit) 사례가 이를 명확히 보여준다. 모델에게 특정 동작을 수행하는 셸 스크립트를 작성하라고 지시했으나 프롬프트 상에 명시적인 파일 저장 경로(Filepath)를 지정하지 않았다. 에이전트는 지시사항을 완벽히 이해하고 유효한 경로에 올바르게 작동하는 스크립트를 작성하여 임무를 완수했다. 그러나 이 테스트를 검증하는 자동화된 파이썬 평가기(Grader) 내부에 기대 결과로서 특정한 파일 경로가 하드코딩되어 있었기 때문에, 에이전트는 평가기가 원하는 정확한 파일 경로가 아니라는 이유만으로 테스트에서 실패(Fail) 처리되었다. 에이전트는 아무런 잘못을 하지 않았음에도 오라클의 경직성 때문에 억울한 평가를 받은 것이다.</p>
<p>에이전트 평가를 구축할 때 개발자들은 종종 에이전트가 문제를 해결하는 <strong>‘경로(Path)’</strong>—예를 들어 특정 API를 순서대로 호출했는지 여부—에 집중하는 우를 범한다. 그러나 고성능 파운데이션 모델일수록 설계자가 미처 생각하지 못한 더 효율적이고 창의적인 우회 경로를 탐색하여 정답을 도출하는 경향이 있다. 기계적인 문자열 매칭이나 순서 기반의 검증은 이러한 에이전트의 뛰어난 능력을 벌칙(Punish)으로 응답하게 되며, 시스템의 발전 방향을 심각하게 저해한다. 최근 평가 트렌드는 에이전트가 어떤 방식을 사용했든 최종적으로 도출한 **‘결과물(Outcome)’**에 기반하여 평가를 수행하는 방향으로 선회하고 있다.</p>
<p>만약 최첨단 프론티어 모델이 수많은 평가 시도(Trials)에도 불구하고 특정 태스크에서 0%의 통과율(Pass@100 = 0%)을 기록한다면, 이는 모델의 역량 부족을 의심하기 이전에 해당 테스트 태스크 자체의 명세가 모호하거나 평가기가 지나치게 엄격한 문자열 매칭에 의존하고 있는 ’깨진 태스크’가 아닌지 강력하게 의심해 보아야 한다.</p>
<h3>3.4  코드 및 복잡한 논리 생성에서의 무용성</h3>
<p>개발 보조 도구나 코드 생성 AI의 성능을 평가할 때, 생성된 코드 스니펫(Snippet)과 그라운드 트루스(Ground-truth) 코드를 단순히 텍스트 기반으로 매칭하는 것은 가장 위험하고 무의미한 평가 방식 중 하나이다. 소프트웨어 코드는 알고리즘의 본질적 로직을 구현하는 수단일 뿐이며, 동일한 기능을 수행하는 프로그램이라 하더라도 작성자의 스타일이나 선택한 문법 요소에 따라 그 형태는 수만 가지로 파생될 수 있다.</p>
<p>배열을 정렬하는 기능을 구현하라는 지시에 대해 어떤 모델은 <code>for</code>문을 사용한 버블 정렬을, 다른 모델은 <code>while</code>문을 사용한 삽입 정렬을 작성할 수 있다. 변수명, 들여쓰기 방식, 모듈화 여부 등 코드를 구성하는 텍스트적 요소는 모두 다르지만 이들은 입력 배열을 성공적으로 정렬한다는 측면에서 동치이다. 즉, 프로그램이 기능적으로 올바른지 확인하기 위해서는 텍스트가 아니라 프로그램의 컴파일 성공 여부와 최종 시스템 상태(System State) 또는 실행 결과가 같은지 비교해야 한다.</p>
<p>텍스트의 단순 비교를 넘어서기 위해 최신 연구에서는 기호 실행(Symbolic Execution)과 같은 고도화된 프로그램 분석 기법을 오라클로 채택하고 있다. 기호 실행은 구체적인 입력값 대신 기호(Symbol) 값을 프로그램의 입력으로 제공하여 추상적인 실행 경로를 모두 탐색하는 방식이다. 이 기법은 바이너리 분석 플랫폼(BAP, Binary Analysis Platform) 등을 활용하여 컴파일된 실행 파일의 블록 단위 제어 흐름을 파악하고, 생성된 프로그램의 모든 가능한 실행 경로에 대한 출력 시스템 상태의 집합과 참조(Reference) 프로그램의 출력 시스템 상태 집합을 산출한다. 생성된 코드와 정답 코드의 최종 시스템 상태 집합이 일치하면, 두 프로그램은 내부 구현 방식의 텍스트 차이와 무관하게 완전히 의미론적으로 동치(Semantically Equivalent, SEM=1)인 것으로 판단할 수 있다. 코드 생성 AI의 오라클은 이와 같이 실행 결과 기반, 혹은 컴파일러 피드백을 수용하는 동적 분석(Dynamic Analysis) 수준으로 진화해야 하며, 문자열 매칭은 이 도메인에서 설 자리가 없다.</p>
<h2>4.  평가 지표의 왜곡: 정답 일치율(Exact Match)과 대체 지표들의 한계</h2>
<p>자연어 처리(NLP) 분야, 특히 기계 독해(Machine Reading Comprehension)나 질의응답(QA) 태스크에서 가장 오랫동안 산업 표준으로 자리 잡았던 평가 지표는 SQuAD(Stanford Question Answering Dataset) 등의 벤치마크에서 주로 사용되는 Exact Match(EM)와 Token-level F1 Score, 그리고 언어 생성의 품질을 평가하기 위한 BLEU, ROUGE 스코어 등이다. 하지만 이 지표들은 본질적으로 텍스트의 표면적 일치에 의존하므로 LLM의 생성 능력을 평가할 때 심각한 통계적 착시를 일으킨다.</p>
<h3>4.1  EM(Exact Match)의 극단성과 토큰 기반 지표들의 착시</h3>
<p>Exact Match(EM)는 가장 단순하면서도 가혹한 지표이다. 평가 대상인 모델의 예측 출력값이 참조 정답지와 문자 하나, 공백 하나까지 완벽하게 일치해야만 1점을 부여하고, 그렇지 않으면 0점을 부여하는 극단적인 모 아니면 도(All-or-nothing) 방식이다. 네거티브 테스트 케이스에 대해 모델이 어떠한 텍스트라도 출력하면 즉시 0점 처리되며, 대소문자 차이마저도 오답으로 간주한다. 모델의 답변이 정답과 얼마나 유사한지, 어느 정도 유용한 정보를 포함하고 있는지를 계량적으로 파악할 수 없다는 치명적인 한계를 지닌다.</p>
<p>EM의 극단적인 엄격함을 완화하기 위해 도입된 것이 n-gram 기반의 통계적 스코어링 지표들이다. F1 스코어는 시스템이 생성한 답변과 정답지 간의 단일 토큰(Unigram) 또는 토큰 교집합을 기반으로 정밀도(Precision)와 재현율(Recall)을 조화 평균(Harmonic Mean)하여 계산한다. 정답 토큰 집합을 <span class="math math-inline">Y</span>, 모델 예측 출력 토큰 집합을 <span class="math math-inline">X</span>라 할 때 다음과 같이 정의된다.<br />
<span class="math math-display">
Precision (P) = \frac{\vert X \cap Y \vert}{\vert X \vert}
</span></p>
<p><span class="math math-display">
Recall (R) = \frac{\vert X \cap Y \vert}{\vert Y \vert}
</span></p>
<p><span class="math math-display">
F1 = 2 \cdot \frac{P \cdot R}{P + R}
</span></p>
<p>텍스트 요약(Summarization)이나 기계 번역에서 사용되는 ROUGE나 BLEU 지표 역시 이와 유사한 수학적 기반을 가진다. ROUGE-1은 단일 단어(Unigram)의 겹침을 측정하며, ROUGE-2는 연속된 두 단어(Bigram)의 겹침을 측정한다. ROUGE-L은 가장 긴 공통 부분 수열(LCS, Longest Common Subsequence)을 기반으로 문장 구조를 평가하며, BLEU 스코어는 n-gram 정밀도에 답변이 지나치게 짧을 경우 페널티(Brevity Penalty)를 부여하는 방식을 취한다. METEOR는 어간 추출(Stemming)이나 외부 동의어 사전(Synonym dictionary)을 활용하여 단순 매칭의 한계를 극복하려 시도했다.</p>
<p>그러나 통계적 스코어러(Statistical Scorers)라고 불리는 이 지표들 역시 근본적으로는 모델의 확률적 본질을 무시한 토큰 수준의 부분 문자열 매칭(Token-level Sub-string Matching)에 불과하다. 의미론적 뉘앙스(Semantic nuance)를 전혀 파악하지 못하며 추론 능력이 극도로 제한적이다. 정답지가 “The cat sat on the mat“일 때, 모델이 “The mat sat on the cat“을 생성하면 토큰 구성은 거의 동일하므로 F1 스코어와 BLEU 스코어는 매우 높게 측정된다. 그러나 주어와 목적어가 도치되어 두 문장의 실제 의미는 완전히 반대가 되었다. 반대로 모델이 “The feline was resting upon the rug“라는 완벽한 의미론적 대체 문장을 생성하면, 토큰 교집합이 0이 되어 F1 스코어는 최하점을 기록한다. 이러한 현상은 통계적 지표가 점진적인 품질 향상을 나타내는 신뢰할 수 있는 척도처럼 보이지만, 실제로는 성능을 과대평가하거나 과소평가하는 양극단의 심각한 왜곡을 만들어냄을 의미한다. 즉, 이들은 통계적으로 안정적(Reliable)이기는 하나 정확성(Accurate)은 현저히 떨어지는 평가 지표이다.</p>
<h3>4.2  논문 분석: “Tomayto, Tomahto” 문제와 평가의 비대칭성</h3>
<p>토큰 수준의 동치성 평가가 내포한 근본적 결함은 자연어 처리 학계에서도 중대한 화두가 되고 있다. EMNLP 2022에 발표된 논문 원문 <em>Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation</em>에서는 EM과 F1 지표가 질의응답 시스템의 실제 성능을 어떻게 심각하게 과소평가(Underestimating)하는지 체계적이고 데이터 중심적인 분석을 통해 폭로했다.</p>
<p>해당 연구진은 전통적인 토큰 기반 평가 지표들이 데이터셋 구축 시 수집된 극소수의 참조(Reference) 정답지에만 과도하게 의존하는 문제를 지적했다. 사람이 작성한 정답지가 모든 유효한 답변의 집합을 포괄할 수 없기 때문에, 모델이 의미적으로 완벽하게 올바른 대답을 하더라도 정답지의 표면적 토큰 배열에 포함되지 않으면 억울하게 오답 처리되는 커버리지(Coverage) 한계가 발생한다.</p>
<p>특히 이 논문은 평가 지표의 비대칭성(Asymmetry)이라는 매우 중요한 개념을 제기한다. 훌륭한 AI 시스템은 단순히 정답지와 문자열이 동일한 답변을 뱉어내는 것에 그치지 않는다. 모델은 참조 정답지보다 훨씬 더 명확하고 풍부한 부가 관련 정보를 제공하거나(Improve over the reference), 반대로 질문의 핵심에 부합하지 않는 불필요한 정보를 제거하여 사용자의 의도를 보다 날카롭게 꿰뚫는 개선된 응답을 도출할 수 있다.</p>
<p>그러나 EM이나 F1과 같은 전통적인 대칭적 측정 방식(Symmetric metrics)은 두 텍스트 집합의 교집합 교환 법칙만을 따르기 때문에, 모델이 이러한 개선된 창의적 응답을 도출할 때마다 오히려 교집합 비율이 줄어들어 페널티(Penalty)를 부여하는 기현상을 발생시킨다. 수량 단위를 변환하여 대답하거나(예: 미터법 vs 야드파운드법), 질문의 맥락에 따라 명사를 대명사로 자연스럽게 치환한 경우에도 F1 스코어는 어김없이 감점을 적용한다. 연구진은 F1 스코어가 점진적인(Gradual) 성능 평가 척도라는 착각을 주지만, 0점이 아닌 F1 점수 구간에서도 실제로는 오답과 정답이 마구 섞여 있어 신뢰할 수 없는 지표임을 SQuAD 데이터셋 상의 23,000개 이상의 인간 평가 주석(Annotations)을 통해 정량적으로 입증했다.</p>
<p>결론적으로, 통계적 문자열 매칭 방식은 AI 모델의 진짜 성능을 가리며, 이를 극복하기 위해 연구진은 비대칭적 특성을 수용하는 **답변 동치성(Answer Equivalence, AE)**이라는 새로운 패러다임을 제안했다. 모델의 출력이 실제 환경에서 제대로 기능하는지 평가하기 위해서는 단순한 구문론적 토큰 매칭을 넘어서, BERT 임베딩 기반의 매칭(BEM)과 같이 언어의 문맥을 인지하고 의미를 재평가(Semantically Reassessing)하는 모델 기반 평가 프레임워크가 필수적으로 뒷받침되어야 한다. 최근에는 거대 언어 모델 자체를 평가 도구로 활용하는 LLM-as-a-Judge 기법을 통해 다수의 모델을 심사위원으로 참여시켜 다수결 투표(Majority Voting) 기반의 판단을 내리는 방식이 기존 문자열 지표를 대체하는 강력한 수단으로 부상하고 있다.</p>
<p><img src="./3.7.2.0.0%20%EC%A7%80%EB%82%98%EC%B9%98%EA%B2%8C%20%EC%97%84%EA%B2%A9%ED%95%9C%20%EB%AC%B8%EC%9E%90%EC%97%B4%20%EB%A7%A4%EC%B9%ADExact%20String%20Matching%EC%9D%98%20%EC%98%A4%EB%A5%98.assets/image-20260222204100388.jpg" alt="image-20260222204100388" /></p>
<h2>5.  예외적 허용: 결정론적 문자열 매칭이 유효한 특수 사례</h2>
<p>지나치게 엄격한 문자열 매칭이 자유로운 자연어 생성을 목표로 하는 대부분의 AI 환경에서 안티패턴으로 작용함에도 불구하고, 이 기법이 강력하고 신뢰할 수 있는 절대적 검증 도구로 기능하는 매우 구체적이고 좁은 특수 사례(Edge Cases)들이 실무에 존재한다. 소프트웨어 개발자와 AI 평가 프레임워크 설계자는 무조건적으로 의미론적 확률적 평가만을 추구할 것이 아니라, 태스크의 목적과 시스템 연동 아키텍처에 따라 엄격한 통계적 스코어링(Statistical Scorers)을 적재적소에 결합하는 실용적인 지혜를 발휘해야 한다.</p>
<p>가장 대표적인 예외이자 결정론적 매칭이 필수 불가결한 도메인은 AI 에이전트의 ‘도구 호출 정확성(Tool Correctness)’ 검증이다. 복잡한 자율 에이전트 파이프라인에서 LLM은 단독으로 텍스트를 생성하는 데 그치지 않고 외부 API, 내부 함수, 또는 마이크로서비스를 호출하여 사용자의 태스크를 수행한다. 이때 도구를 호출하기 위해 LLM이 생성해 내는 매개변수(Arguments)와 인수 값은 백엔드 시스템이 오류 없이 파싱하고 인식할 수 있는 극도로 정확한 포맷과 고유 식별자를 지녀야만 한다.</p>
<p>예를 들어 항공편 예약을 돕는 여행 에이전트 AI가 특정 항공편을 검색하기 위해 내부적으로 <code>book_flight(origin="JFK", destination="SFO")</code>라는 함수를 호출해야 한다고 가정하자. 이때 인자로 전달되는 공항 코드는 시스템에 규정된 3자리 문자열 포맷을 1바이트의 오차도 없이 지켜야 한다. 만약 모델이 “JFK“라는 문자열 대신 “존 에프 케네디 국제공항“이라고 의미론적으로는 완벽하지만 형식적으로는 다른 문장을 생성한다면, 임베딩이나 의미론적 유사도 검사에서는 매우 훌륭한 응답으로 평가받을지 몰라도 백엔드 API는 이를 3자리 IATA 코드로 수용하지 못하여 런타임 에러(HTTP 500 등)를 발생시키고 전체 워크플로우를 붕괴시킨다.</p>
<p>마찬가지로 금융 기관의 고객 지원 챗봇이 특정 고객의 계좌 잔액을 조회하기 위해 12자리의 고객 고유 식별 번호(ID)를 파라미터로 추출하거나, 기업의 백오피스 자동화 시스템이 수많은 송장 이메일 속에서 “INV-20394“라는 특정 청구서 코드를 발췌하는 태스크를 수행할 때에도 의미의 유연성은 전혀 고려 대상이 아니다. 이처럼 닫힌 도메인(Closed-domain) 태스크, 즉 기계 간의 통신을 위해 출력값이 기결정된 프로토콜을 반드시 준수해야 하거나 특정한 상태를 트리거하기 위해 정확한 키워드가 필요한 상황에서는, 조건부 논리(Conditional logic)를 동반한 확정적 문자열 매칭이 그 어떤 최신 LLM-as-a-Judge 기법보다도 가장 안전하고 변조 불가능한 방어선이자 테스트 오라클로 기능한다. 평가자는 이러한 특수 태스크에 한정하여 Exact Match(EM) 점수를 시스템의 신뢰성을 담보하는 통과 기준선으로 삼아야 한다.</p>
<h2>6.  실전 예제: 다중 임계치 기반의 하이브리드 검증 오라클 설계</h2>
<p>앞서 살펴본 지나치게 엄격한 문자열 매칭의 맹점들을 극복하면서도 시스템의 무결성을 잃지 않는 신뢰성 높은 결정론적 정답지를 구성하기 위해, 실제 산업계 최전선에서는 어떻게 테스트 오라클을 재설계하고 있는지 구체적인 실전 예제를 통해 살펴본다. 현대 AI 엔지니어링의 목표는 단일 지표의 한계에서 벗어나, 데이터의 특성에 따라 서로 다른 평가 기준을 융합하는 다중 임계치 기반의 하이브리드 오라클 시스템(Hybrid Oracle System)을 구축하는 것이다.</p>
<h3>6.1  시나리오: 비즈니스 정보 추출 AI</h3>
<p>온라인 전자상거래 플랫폼에서 수만 건의 고객 CS(Customer Service) 이메일을 실시간으로 분석하여 제품명, 고객의 주요 불만 사항, 그리고 환불 요청 여부를 식별한 뒤 이를 JSON 객체 형태로 백엔드 데이터베이스에 반환하는 정보 추출 AI 파이프라인을 개발한다고 가정하자. 시스템은 LLM을 통해 비정형 텍스트를 구조화된 데이터로 파싱한다.</p>
<p><strong>과거의 안티패턴 오라클 구조 (엄격한 정답지 비교):</strong></p>
<p>테스트 파이프라인에서 개발자는 기대되는 골든 데이터셋(Golden Dataset)의 정답지를 다음과 같은 문자열로 하드코딩하고, <code>Assert.AreEqual</code> 방식의 단위 테스트(Unit Test)를 CI/CD 서버에서 기계적으로 실행한다.</p>
<pre><code class="language-Python">expected_output = '{"product": "iPhone 15", "issue": "battery drain", "refund_requested": true}'

def test_ai_extraction(email_text):
    actual_output = llm_extract_info(email_text)
    assert actual_output == expected_output  # 위험: 빈칸, 대소문자, 줄바꿈의 미세한 차이로 인해 대량의 실패 발생
</code></pre>
<p>이러한 코드는 LLM이 지시를 훌륭히 수행하여 <code>{"product":"iPhone 15", "issue":"Battery drain", "refund_requested":true}</code>라고 공백을 제거하고 첫 글자를 대문자로 섞어 완벽하게 올바른 응답을 생성하더라도 테스트를 무조건 실패(Fail)시킨다. 이는 테스트 오라클이 ’데이터의 구조와 비즈니스 의미’를 유연하게 검증하는 것이 아니라, ’JSON 객체가 직렬화된 문자열의 원시 바이트 형태’만을 맹목적이고 단편적으로 검증하고 있기 때문이다.</p>
<h3>6.2  하이브리드 오라클 구조의 설계 (해결책)</h3>
<p>진정한 의미에서의 결정론적 검증은 문자열의 외형적 일치가 아니라, **‘생성된 데이터가 후속 비즈니스 로직에 부합하고 시스템의 안전 범위를 준수하는가’**라는 핵심 조건의 충족 여부로 정의되어야 한다. 이를 위해 단순 매칭을 버리고 다음 세 가지 독립적 계층(Layer)으로 구성된 견고한 하이브리드 검증 오라클 파이프라인을 구축해야 한다.</p>
<table><thead><tr><th><strong>검증 계층 (Verification Layer)</strong></th><th><strong>사용 기법 (Technique)</strong></th><th><strong>오라클의 목표 및 역할 (Goal &amp; Role)</strong></th></tr></thead><tbody>
<tr><td>Layer 1: 구문적 유효성 검증 (Syntactic Validation)</td><td>JSON Schema 기반 파싱 및 정규식 검사</td><td>LLM이 생성한 결과물이 백엔드 시스템에서 직렬화 해제가 가능한 유효한(Valid) JSON 포맷을 유지하고 있는지 확인한다. 또한 스키마 정의를 기반으로 필수 Key(<code>product</code>, <code>refund_requested</code>)가 누락되지 않았는지, Value의 Data Type(Boolean, String 등)이 지정된 규칙에 부합하는지 런타임 오류 방지 차원에서 검사한다.</td></tr>
<tr><td>Layer 2: 확정적 비즈니스 로직 검증 (Deterministic Logic Validation)</td><td>조건부 논리 데이터 파싱 후 변수 값 명시적 비교</td><td><code>refund_requested</code>와 같은 핵심 비즈니스 로직 플래그는 <code>True</code> 혹은 <code>False</code>의 명확하고 결정론적인 상태를 지닌다. 따라서 파싱된 JSON 객체 트리 내부의 메모리 값을 직접 참조(<code>Assert.IsTrue(parsed_json['refund_requested'])</code>)하여 문자열 텍스트가 아닌 불리언(Boolean) 값 자체를 확정적으로 검증한다.</td></tr>
<tr><td>Layer 3: 의미론적 동치성 검증 (Semantic Equivalence Validation)</td><td>임베딩 벡터 간 코사인 유사도(Cosine Similarity) 및 평가 모델(LLM-as-a-Judge)</td><td><code>issue</code> 항목과 같이 사용자의 언어가 반영된 자유 문장 형태의 추출 영역은 문자열 비교 알고리즘 대신, 임베딩 모델(예: BERT 기반 텍스트 임베딩 모델)을 사용하여 정답 텍스트(“battery drain”)와 모델 출력 텍스트 간의 벡터 거리를 측정한다. <span class="math math-inline">\vert \cos(\theta) \vert \ge 0.85</span> 와 같이 사전에 설정된 안전한 임계치(Threshold)를 넘는 경우 유효한 정답으로 수용한다. 더욱 정교한 사실 관계 모순 여부가 필요할 경우 LLM을 심사위원(Judge)으로 추가 투입한다.</td></tr>
</tbody></table>
<p>위의 다층적 구조를 바탕으로 재작성된 견고한 자동화 검증 오라클 파이프라인의 파이썬 기반 핵심 논리는 다음과 같은 형태로 진화한다.</p>
<pre><code class="language-Python">import json
from evaluation_metrics import calculate_semantic_similarity

# 텍스트가 아닌 상태(State)와 임계치 기반으로 정의된 그라운드 트루스
expected_state = {
    "product_keywords": ["iphone", "15"],
    "expected_issue": "battery drain",
    "refund_flag": True
}

def robust_test_ai_extraction(email_text):
    raw_output = llm_extract_info(email_text)
    
    # Layer 1: JSON 파싱 시도 (형식 및 문법적 오류 검증)
    try:
        parsed_data = json.loads(raw_output)
    except json.JSONDecodeError:
        return "Fail: Invalid JSON Format"
        
    # Layer 2: 핵심 비즈니스 로직에 대한 확정적 변수 검증 (문자열 대소문자 정규화 후 비교)
    product_name = parsed_data.get("product", "").lower()
    has_all_keywords = all(keyword in product_name for keyword in expected_state["product_keywords"])
    
    if not has_all_keywords or parsed_data.get("refund_requested")!= expected_state["refund_flag"]:
        return "Fail: Business Logic State Mismatch"
        
    # Layer 3: 자유 텍스트 필드에 대한 의미론적 동치성 기반 임계치 평가
    extracted_issue = parsed_data.get("issue", "")
    issue_similarity = calculate_semantic_similarity(extracted_issue, expected_state["expected_issue"])
    
    if issue_similarity &lt; 0.85:
        return "Fail: Semantic Similarity Below Threshold"
        
    return "Pass" # 구문적, 논리적, 의미론적 검증을 모두 통과한 성공적인 추출
</code></pre>
<p>이러한 하이브리드 오라클 접근법은 생성형 AI 시스템에서의 결정론적 정답지의 본질이 결코 ’단일한 텍스트 템플릿의 가혹한 강요’가 아님을 입증한다. 그것은 확률 모델이 필연적으로 발생시키는 언어적 노이즈를 시스템이 묵인하면서도, 백엔드 애플리케이션의 비즈니스 무결성을 담보할 수 있는 **‘허용 가능한 의미와 구조의 명확하고 다차원적인 경계(Boundary) 설정’**에 그 핵심이 있다.</p>
<p>과거의 구문론적 경직성에서 단호히 벗어나, 토큰 매칭의 한계를 보완할 수 있는 벡터 기반 시맨틱 유사도 지표(BERTScore 등)와 정규식, 구조 검증을 적절히 혼합하는 다층적 지표 시스템을 도입하는 것만이 시시각각 변동성이 일어나는 고성능 파운데이션 모델의 출력을 비즈니스 목적에 맞게 안정적으로 제어하는 실질적인 엔지니어링 해결책이다.</p>
<h2>7.  소결</h2>
<p>전통적인 소프트웨어 코딩 시절 수십 년간 신성불가침의 검증 도구로 여겨졌던 “지나치게 엄격한 문자열 매칭(Exact String Matching)“은 현재의 AI 기반 소프트웨어 개발 환경에서 가장 먼저 청산되어야 할 기술적 부채이자 거대한 안티패턴이다.</p>
<p>대규모 언어 모델이 창발적으로 생성하는 결과물은 끊임없이 변화하며 표면적인 형태를 다채롭게 변형하지만, 그 이면에 깔려 있는 내재적 논리와 의미는 비즈니스의 복잡한 요구사항을 훌륭히 충족시킬 수 있는 고차원적인 지능의 산물이다. 본 절에서 심층적으로 분석한 바와 같이, 의미론적 동치성을 무시한 채 단순히 바이트와 문자 배열의 불일치만을 찾아내는 기계적인 텍스트 매칭은 시스템의 진정한 문제 해결 능력을 터무니없이 과소평가하게 만든다. 이는 곧 거대한 위음성(False Negative)의 함정을 파괴적으로 양산하며, AI 모델의 성능을 향상시키기 위한 귀중한 평가 지표(Metric)를 무의미한 숫자의 맹목적 추종으로 전락시킨다.</p>
<p>소프트웨어 개발자와 AI 테스터, 그리고 자동화 파이프라인 설계자는 전통적인 단위 테스트의 <code>Assert.AreEqual</code>이 심어놓은 확정성에 대한 편향에서 즉시 벗어나야 한다. 진정한 결정론적 정답지(Deterministic Ground Truth)의 설계는 모든 상황을 관통하는 단 하나의 절대적인 문자열 조합을 찾는 헛된 작업이 아니다. 그것은 BEM, LLM-as-a-Judge, 다차원 임베딩 거리 측정 지표, 그리고 조건부 비즈니스 로직 유효성 파싱 검사를 전략적이고 유기적으로 융합하는 고도화된 아키텍처 작업이다.</p>
<p>이를 통해 AI의 창조적 유연성과 문맥 인지 능력을 적극적으로 수용하면서도 시스템 백엔드의 안정성을 결코 해치지 않는 확고한 다층적 의미론적 임계치(Semantic Threshold)를 설정하는 것. 이것이 바로 생성형 AI 시대가 소프트웨어 테스팅의 본질을 향해 요구하는 가장 시급하고 중대한 철학적, 기술적 전환이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>(PDF) Exact String Matching Algorithms: Survey, Issues, and Future, https://www.researchgate.net/publication/332773245_Exact_String_Matching_Algorithms_Survey_Issues_and_Future_Research_Directions</li>
<li>The Exact String Matching Problem: a Comprehensive Experimental, https://www.researchgate.net/publication/48166342_The_Exact_String_Matching_Problem_a_Comprehensive_Experimental_Evaluation</li>
<li>Exact String Matching Algorithms - IEEE Xplore, https://ieeexplore.ieee.org/iel7/6287639/8600701/08703383.pdf</li>
<li>The exact string matching algorithms efficiency review - Academia.edu, https://www.academia.edu/35469739/The_exact_string_matching_algorithms_efficiency_review</li>
<li>(PDF) The exact string matching algorithms efficiency review, https://www.researchgate.net/publication/281863368_The_exact_string_matching_algorithms_efficiency_review</li>
<li>The String Matching Algorithms Research Tool, https://www.stringology.org/cgi-bin/getfile.cgi?t=pdf&amp;c=-&amp;y=2016&amp;n=09</li>
<li>The Exact String Matching Problem: a Comprehensive Experimental, https://www.dmi.unict.it/faro/papers/technical/faroT4.pdf</li>
<li>arXiv:1209.6449v1 [cs.IR] 28 Sep 2012, https://arxiv.org/pdf/1209.6449</li>
<li>Fuzzy Matching and Semantic Search - iPullRank, https://ipullrank.com/fuzzy-matching-semantic-search</li>
<li>SQL for Pattern Matching - Oracle Help Center, https://docs.oracle.com/en/database/oracle/oracle-database/26/dwhsg/sql-pattern-matching-data-warehouses.html</li>
<li>Strategies for Testing AI-Based Systems - stareast, https://stareast.techwell.com/program/preconference-training/strategies-testing-ai-based-systems-stareast-2026</li>
<li>Automating Test Oracles Generation - sonar, https://sonar.ch/documents/318950/files/2018INFO004.pdf</li>
<li>Perfect Is the Enemy of Test Oracle - arXiv, https://arxiv.org/pdf/2302.01488</li>
<li>LLM evaluation metrics and methods - Evidently AI, https://www.evidentlyai.com/llm-guide/llm-evaluation-metrics</li>
<li>Evaluating Generative AI: A Field Manual - Palantir Blog, https://blog.palantir.com/evaluating-generative-ai-a-field-manual-0cdaf574a9e1</li>
<li>The End of “Expected Result”: Why Traditional QA Fails in the AI Era, https://medium.com/trendyol-tech/the-end-of-expected-result-why-traditional-qa-fails-in-the-ai-era-ba8318a3cbb5</li>
<li>ExAIs: Executable AI semantics - Institutional Knowledge (InK) @ SMU, https://ink.library.smu.edu.sg/context/sis_research/article/8782/viewcontent/ExAIs_2022_av.pdf</li>
<li>Why Accuracy Isn’t Enough: A Guide to AI/ML Model Evaluation, https://medium.com/@aditib259/why-accuracy-isnt-enough-a-guide-to-ai-ml-model-evaluation-f97c0c394375</li>
<li>Evaluating LLMs: The Ultimate Guide to Performance Metrics, https://medium.com/@okanyenigun/evaluating-llms-the-ultimate-guide-to-performance-metrics-c819f8f0a962</li>
<li>A list of metrics for evaluating LLM-generated content - Microsoft Learn, https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/evaluation/list-of-eval-metrics</li>
<li>Understanding LLM Evaluation Metrics: Best Practices for Reliable, https://medium.com/@tungvu_37498/understanding-llm-evaluation-metrics-best-practices-for-reliable-llm-assessment-3fce1fa48251</li>
<li>Tomayto, Tomahto. Beyond Token-level Answer Equivalence for, https://aclanthology.org/2022.emnlp-main.20.pdf</li>
<li>Tomayto, Tomahto. Beyond Token-level Answer Equivalence … - arXiv, https://arxiv.org/abs/2202.07654</li>
<li>LLM-Driven Data Generation and a Novel Soft Metric for Evaluating, https://arxiv.org/pdf/2506.13785</li>
<li>Demystifying evals for AI agents \ Anthropic, https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents</li>
<li>Automating the Correctness Assessment of AI-generated Code for, https://arxiv.org/html/2310.18834v2</li>
<li>Evaluating QA: Metrics, Predictions, and the Null Response, <a href="https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html">https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html</a></li>
<li>Enhancing QA System Evaluation: An In-Depth Analysis of Metrics, https://koreascience.kr/article/JAKO202508532403836.page</li>
<li>Machine Learning Evaluation Metrics: What Really Matters?, https://labelstud.io/learningcenter/machine-learning-evaluation-metrics-what-really-matters/</li>
<li>Evaluation Metrics for Natural Language Processing Models - Medium, https://medium.com/gen-ai-adventures/evaluation-metrics-for-natural-language-processing-models-9eaa9980b324</li>
<li>LLM Evaluation Metrics: The Ultimate LLM Evaluation Guide …, https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation</li>
<li>[PDF] Tomayto, Tomahto. Beyond Token-level Answer Equivalence, https://www.semanticscholar.org/paper/Tomayto%2C-Tomahto.-Beyond-Token-level-Answer-for-Bulian-Buck/8324cc3fb32246f57d18ea8f81eaeae50de60079</li>
<li>Beyond Exact Match: Semantically Reassessing Event Extraction by, https://arxiv.org/html/2410.09418v2</li>
<li>LLMs-as-Judges in Automatic Evaluation of Free-Form QA, https://aclanthology.org/2025.winlp-main.37.pdf?utm_source=chatgpt.com</li>
<li>A Novel Automatic Evaluation Metric for Open-domain Generative, https://www.researchgate.net/publication/347177003_PONE_A_Novel_Automatic_Evaluation_Metric_for_Open-domain_Generative_Dialogue_Systems</li>
<li>LangChain Evaluators for Language Model Validation - Comet, https://www.comet.com/site/blog/langchain-evaluators-for-language-model-validation/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>