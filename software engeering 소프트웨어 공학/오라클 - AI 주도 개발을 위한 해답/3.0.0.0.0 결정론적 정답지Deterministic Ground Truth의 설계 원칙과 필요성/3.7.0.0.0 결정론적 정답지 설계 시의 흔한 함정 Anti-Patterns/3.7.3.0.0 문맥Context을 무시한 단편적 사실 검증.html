<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.7.3 문맥(Context)을 무시한 단편적 사실 검증</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.7.3 문맥(Context)을 무시한 단편적 사실 검증</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 3. 결정론적 정답지(Deterministic Ground Truth)의 설계 원칙과 필요성</a> / <a href="index.html">3.7 결정론적 정답지 설계 시의 흔한 함정 (Anti-Patterns)</a> / <span>3.7.3 문맥(Context)을 무시한 단편적 사실 검증</span></nav>
                </div>
            </header>
            <article>
                <h1>3.7.3 문맥(Context)을 무시한 단편적 사실 검증</h1>
<p>인공지능(AI) 시스템의 출력을 검증하는 결정론적 오라클(Deterministic Oracle)을 설계함에 있어, 복잡하고 긴 생성 텍스트(Long-form generation)를 평가하는 기준은 시스템의 신뢰성을 좌우하는 핵심 요소이다. 생성형 AI가 산출하는 결과물의 사실성을 담보하기 위해 산업계와 학계는 텍스트를 가장 작은 단위의 명제로 쪼개어 각각의 진위를 판별하는 ‘분해 후 검증(Decompose-then-Verify)’ 패러다임을 폭넓게 수용해 왔다. 그러나 이 접근법을 결정론적 정답지(Deterministic Ground Truth)의 구축에 맹목적으로 적용할 경우, 오라클은 문장과 문장, 사실과 사실 사이에 얽혀 있는 논리적 인과율, 시간적 선후 관계, 그리고 담화적 맥락(Discourse Context)을 완전히 소거하는 치명적인 함정에 빠지게 된다. 개별적인 사실(Atomic Facts)들이 아무리 외부 지식 베이스와 완벽하게 일치하더라도, 이들의 조합이 만들어내는 전체적인 서사와 비즈니스 로직이 맥락상 심각한 모순을 내포할 수 있기 때문이다. 본 절에서는 문맥이 배제된 단편적 사실 검증이 오라클의 평가 무결성을 어떻게 훼손하는지 수학적, 논리적 한계를 통해 심층적으로 분석하고, 이를 극복하여 문맥을 보존하는 고도화된 정답지 설계 전략과 최신 검증 프레임워크를 제시한다.</p>
<h2>1. 문맥 소거와 독립성 가정(Independence Assumption)의 수학적 모순</h2>
<p>AI 시스템의 텍스트 생성을 평가하는 전통적인 지표들은 생성된 긴 텍스트를 원자적 사실(Atomic facts)로 분해하는 기법에 절대적으로 의존한다. 원자적 사실이란 문맥에 독립적이며 단 하나의 구체적인 정보만을 포함하는 가장 짧은 형태의 진술을 의미한다. 이러한 분해 방법론의 기저에는 텍스트 전체의 진실성이 텍스트를 구성하는 개별 사실들의 진실성의 총합, 혹은 독립적인 곱과 같다는 강력한 ’독립성 가정(Independence Assumption)’이 자리 잡고 있다.</p>
<p>통계학과 확률 논리 모델에서 독립성 가정은 시스템을 단순화하여 계산 복잡도를 낮추는 데 기여한다. 주어진 생성 응답 <span class="math math-inline">r</span>을 <span class="math math-inline">n</span>개의 원자적 사실 집합 <span class="math math-inline">A = {a_1, a_2, \dots, a_n}</span>으로 분해한다고 가정할 때, 평가 모델은 외부의 신뢰할 수 있는 지식 소스 <span class="math math-inline">K</span>를 참조하여 각 <span class="math math-inline">a_i</span>의 참과 거짓 여부를 독립적인 확률 변수로 취급한다. 즉, 응답 <span class="math math-inline">r</span>이 완벽하게 참일 확률 <span class="math math-inline">P(r \vert K)</span>는 개별 사실들의 조건부 확률의 곱으로 근사된다. 하지만 자연어와 비즈니스 로직이 교차하는 소프트웨어 환경에서 이러한 독립성 가정은 현실을 극도로 왜곡한다.</p>
<p>실제 언어 모델이 생성하는 담화에서 두 번째 문장의 의미와 진실성 <span class="math math-inline">a_2</span>는 첫 번째 문장인 <span class="math math-inline">a_1</span>이라는 선행 문맥에 강하게 종속된다. 따라서 <span class="math math-inline">P(a_2 \vert a_1, K)</span>는 <span class="math math-inline">P(a_2 \vert K)</span>와 결코 같을 수 없다. 대명사의 지시 대상(Coreference), 생략된 주어, 공간적 및 시간적 배경 등은 오직 주변 문장과의 유기적인 관계 속에서만 유효한 의미를 획득한다. 나아가 특정 사건의 발현은 선행 사건이 존재해야만 물리적, 논리적으로 성립하는 비대칭적 의존성을 띤다. 결정론적 정답지가 이러한 종속성을 무시하고 개별 명제의 참/거짓 매칭에만 집착할 경우, 오라클은 심각한 오판을 내리게 된다.</p>
<p>이러한 독립성 가정의 오류는 분류 시스템에서 발생하는 거짓 양성(False Positive)과 거짓 음성(False Negative)의 문제로 직결된다. 단편적 사실 검증에서 거짓 양성이란, 시스템이 개별 원자적 사실을 독립적으로 검증했을 때는 외부 지식 베이스와 문자열이 일치하여 ’참(True)’으로 판정하지만, 실제 문맥상으로는 특정 조건이나 시간적 제약이 결여되어 있어 명백한 ’거짓(False)’이 되어야 하는 상태를 의미한다. 반대로 거짓 음성은 올바른 문맥적 추론을 통해 도출된 유효한 사실임에도 불구하고, 단편적인 데이터베이스에 해당 문자열이 정확히 존재하지 않는다는 이유만으로 오라클이 ’거짓’으로 오판하는 경우이다. AI 소프트웨어 테스트에서 거짓 양성의 누적은 치명적인 비즈니스 결함을 통과시키는 결과를 초래하므로, 문맥을 인지하지 못하는 오라클은 시스템의 신뢰성을 담보할 수 없다.</p>
<h2>2. 전통적 분해 후 검증 프레임워크의 취약성: FActScore와 SAFE</h2>
<p>단편적 접근법의 한계를 명확히 이해하기 위해서는 학계에서 널리 사용된 초기 사실성 평가 지표인 FActScore의 메커니즘을 분석할 필요가 있다. 논문 <em>FActScore: Fine-grained atomic evaluation of factual precision in long form text generation</em>에서 제안된 이 지표는 텍스트를 원자적 단위로 철저히 분해한 후, 위키피디아와 같은 신뢰할 수 있는 소스를 바탕으로 각 사실을 독립적으로 검증하여 전체 텍스트에서 지지받는(Supported) 사실의 비율을 산출한다. FActScore의 파이프라인은 원자적 사실 생성(Atomic Fact Generation), 증거 검색(Evidence Retrieval), 사실 검증(Fact Validation), 점수 계산(Score Computation)의 네 단계로 구성된다.</p>
<p>이 모델은 평가를 정량화하는 데는 성공했으나 결정론적 오라클로 사용하기에는 극복하기 힘든 제약 조건들을 전제로 한다. 우선, 모든 원자적 사실이 지식 소스에 의해 논쟁의 여지 없이(Undebatable) 검증 가능해야 하며, 추출된 모든 원자적 사실이 동일한 가중치(Equal weight)를 지닌다고 가정한다. 이는 복잡한 비즈니스 로직에서 핵심적인 제약 조건과 부수적인 설명이 동일한 중요도로 평가되는 치명적 모순을 낳는다.</p>
<p>FActScore와 관련된 평가 공식의 구조와 그 한계를 수학적으로 비교하면 아래 표와 같다.</p>
<table><thead><tr><th><strong>평가 지표 및 프레임워크</strong></th><th><strong>정밀도(Precision) 연산 수식</strong></th><th><strong>문맥 처리 및 한계점</strong></th></tr></thead><tbody>
<tr><td><strong>FActScore</strong></td><td><span class="math math-inline">\text{FActScore} = \frac{n_s}{N} \times 100\%</span></td><td><span class="math math-inline">n_s</span>는 지지받는 사실, <span class="math math-inline">N</span>은 전체 사실. 문장 간 의존성을 무시하며 모든 사실을 독립 변수로 취급함.</td></tr>
<tr><td><strong>전통적 분류(Binary)</strong></td><td><span class="math math-inline">\text{Precision} = \frac{TP}{(TP + FP)}</span></td><td><span class="math math-inline">\text{FActScore}</span>와 유사하게 <span class="math math-inline">TP</span>(True Positive)에만 집중. 순서 역전과 같은 문맥 조작을 <span class="math math-inline">FP</span>(False Positive)로 잡아내지 못함.</td></tr>
<tr><td><strong>SAFE (Search-Augmented)</strong></td><td>언어 모델이 판단한 지지 사실 비율</td><td>원자적 분해 후 검색 증거를 대조. 반복 검색은 수행하나 여전히 모든 주장이 검증 가능하다고 가정하는 한계 노출.</td></tr>
<tr><td><strong>Ragas (Factual Correctness)</strong></td><td><span class="math math-inline">\text{F1 Score} = \frac{2 \times \text{Precision} \times \text{Recall}}{(\text{Precision} + \text{Recall})}</span></td><td>검색된 문맥 대비 주장의 일치도를 평가하나, 명제 단위 매칭에 머물러 복합적 인과 왜곡에는 취약함.</td></tr>
</tbody></table>
<p>FActScore와 SAFE는 공통적으로 ’텍스트 내의 모든 청구(Claim)가 사실적으로 입증 가능(Verifiable)하다’는 극단적인 가정을 내포한다. 이러한 가정은 인물 전기 생성(Biography generation)과 같이 사실 밀도가 높고 정형화된 작업에서는 유효할 수 있으나, 긴 질의응답(Long-form QA)이나 추론 과정이 포함된 비즈니스 챗봇의 출력과 같이 주관적 의견, 가설, 제안 등이 사실적 명제와 혼재된 출력에서는 완전히 실패한다. 주관적 표현이나 검증할 수 없는 예시마저 무리하게 원자적 단위로 쪼개어 검증을 시도함으로써, 실제로는 우수한 응답을 생성한 AI 모델에게 불이익을 주는 평가 왜곡이 발생한다.</p>
<p>또한, 저자원 언어(Low-resource languages) 환경이나 특수 도메인 환경에서는 위키피디아와 같은 단일 지식 소스의 적용 범위가 극도로 제한된다는 점도 문제다. FActScore 기반의 검증은 위키피디아의 지식 커버리지에 심각하게 의존하므로, 검색된 지식의 부재를 사실의 거짓(False)으로 오인하는 현상이 발생하며, 이를 보완하기 위한 오픈소스 기반의 OpenFActScore 프레임워크가 등장하기도 하였으나, 근본적인 문맥 소거의 문제는 여전히 잔존한다.</p>
<h2>3. 실전 사례: 단편적 검증이 유발하는 오라클 실패</h2>
<p>문맥을 배제한 사실 검증이 어떻게 시스템 수준의 오작동과 사용자 신뢰 하락으로 이어지는지 보여주는 실증적 연구와 사례들이 다수 보고되고 있다. 이는 단순한 벤치마크 점수의 하락을 넘어, 비즈니스 로직과 사회적 영향력 측면에서 AI 시스템이 직면한 실질적 위협을 시사한다.</p>
<p>첫째, AI 팩트 체크 시스템의 역효과(Backfiring) 사례이다. 미국 국립과학원회보(PNAS)에 발표된 연구에 따르면, LLM 기반의 팩트 체커가 충분한 문맥 정보를 활용하지 못해 모호하거나 복잡한 상황을 마주했을 때, 실제로는 참(True)인 헤드라인을 ’거짓(False)’으로 잘못 분류하는 치명적인 오류를 범했다. 이 과정에서 단편적 사실만을 검증하도록 학습된 AI는 참인 뉴스에 대한 사용자의 신뢰도를 도리어 감소시키고, 거짓인 뉴스에 대해 확신하지 못하는 응답을 내놓음으로써 거짓 뉴스에 대한 믿음을 증가시키는 심각한 부작용을 낳았다. 이는 오라클이 문맥적 뉘앙스와 모호성을 적절히 처리하지 못할 때 시스템의 도입이 오히려 역효과를 초래함을 증명한다.</p>
<p>둘째, 기업용 AI 데이터 시스템에서 의미적 문맥(Semantic Context)과 메타데이터의 결여가 일으키는 실패 사례이다. 엔터프라이즈 환경에서 AI 시스템이 비즈니스 질의에 답변할 때, 내부 데이터베이스의 수치(단편적 사실) 자체는 정확하게 추출할 수 있다. 그러나 조직 내에서 해당 메트릭이 정의된 논리적 규칙이나 시간적 계산 기준, 즉 비즈니스의 ’의미적 문맥’이 프롬프트나 정답지 검증 과정에 제공되지 않으면, AI는 데이터의 파편만을 나열하여 결과적으로 비즈니스 부서에서 사용할 수 없는 엉뚱한 결론을 도출하게 된다. 단편적인 숫자 매칭에만 집착하는 오라클은 이러한 논리적 단절을 ‘통과(Pass)’ 처리하는 치명적 결함을 노출한다.</p>
<p>셋째, 규칙 위반에 대한 AI 모델의 문맥 무시 판정 성향이다. MIT 연구진에 따르면, 규칙 위반 여부를 판별하도록 훈련된 AI 모델들은 데이터를 수집하고 라벨링하는 과정의 맥락을 이해하지 못할 경우, 동일한 상황에 대해 인간보다 훨씬 더 가혹하고 경직된 판단을 내리는 경향이 있다. 인간은 규칙 적용 시 주변 정황과 예외적 문맥을 종합적으로 고려하지만, 단편적 매칭 논리에 갇힌 AI와 오라클은 기계적인 사실 대조만을 수행하여 과적합된 판정을 내리게 된다.</p>
<p>마지막으로, 어휘의 다의성(Polysemy)과 언어적 창의성에 대한 몰이해이다. LLM은 고도의 문맥이 주어지지 않으면 한 단어가 가지는 다의적 특성을 포착하지 못해, 문장 전체의 해석을 이중으로 하거나 정치적 농담과 같은 창의적 표현의 핵심을 놓치고 평면적인 사실 나열로 일관하는 실패를 보여주었다. 오라클이 정답지를 엄격한 문자열 수준의 원자적 사실로만 구성할 경우, 다의성에 기반한 모델의 유연하고 타당한 답변마저 모두 ’오답’으로 간주되는 거짓 음성(False Negative)의 함정에 빠지게 된다.</p>
<h2>4. 인과관계 왜곡과 몽타주 거짓말 (Causal Distortion and MontageLie)</h2>
<p>단편적 사실 검증 체계가 지닌 가장 파괴적인 사각지대는 개별 사실 단위에서는 모두 ’참(True)’인 것으로 판별됨에도 불구하고, 이들이 결합된 최종 결과물은 치명적으로 기만적인 ’거짓(False)’이 되는 현상이다. 논문 <em>Long-Form Information Alignment Evaluation Beyond Atomic Facts</em>는 이 현상을 ’몽타주 거짓말(Montage Lie)’로 정의하며, 현존하는 세립질(Fine-grained) 평가 프레임워크들의 취약점을 수학적, 논리적으로 증명했다.</p>
<p>영화 예술에서 몽타주 기법이 실제 촬영된 장면들을 새로운 순서로 편집하고 배열하여 전혀 다른 감정과 의미를 창출하듯, 악의적인 프롬프트나 치명적인 논리 오류를 겪는 언어 모델은 텍스트 내의 사실 자체를 날조(Hallucination)하지 않고 단지 서사의 배열을 재조합하는 것만으로 암묵적인 인과 고리를 완전히 역전시키거나 독자를 오도할 수 있다.</p>
<p>이러한 기만적 조작의 명확한 예시는 다음과 같다. 시스템에 주어진 원본(Source) 텍스트가 다음과 같은 사건들을 포함한다고 가정하자.</p>
<ol>
<li>마이크가 에이미를 폭행했다.</li>
<li>마이크와 에이미는 헤어졌다.</li>
<li>에이미는 존과 영화를 보러 갔다.</li>
</ol>
<p>만약 언어 모델이 위 사실들의 순서를 바꾸어 다음과 같은 몽타주 거짓말을 생성했다고 가정해 보자.</p>
<ul>
<li>“에이미는 존과 영화를 보러 갔다. 마이크가 에이미를 폭행했다. 마이크와 에이미는 헤어졌다.”</li>
</ul>
<p>후자의 생성 텍스트를 FActScore나 AlignScore와 같이 독립성 가정에 기반한 지표로 검증할 경우, 추출된 세 개의 원자적 사실은 모두 외부 지식(원본 텍스트)과 문자열 단위로 완벽하게 일치한다. 따라서 오라클은 이를 100% 사실에 부합하는 완벽한 응답으로 채점하게 된다. 그러나 담화적, 문맥적 관점에서 독자는 이 텍스트를 읽고 ’에이미가 다른 사람과 영화를 보며 외도를 했기 때문에, 그 결과로 마이크가 폭행을 가했고 이별에 이르렀다’는 전혀 존재하지 않았던 인과관계를 추론하게 된다.</p>
<p>이처럼 명시적인 환각이나 사실의 조작 없이 오직 시간적 순서와 인과적 배열만을 왜곡하는 기만 전술에 대해, 현재의 대규모 언어 모델 기반 평가자나 단편적 검증 프레임워크들은 극도로 취약하다. 네 가지 난이도로 구성된 인과 왜곡 벤치마크인 MontageLie 데이터셋을 대상으로 최신 평가 모델들을 테스트한 결과, 정상적인 텍스트와 몽타주 거짓말을 구분해 내는 모델의 분류 성능(ROC-AUC 점수)이 65% 미만으로 곤두박질치는 현상이 관찰되었다. 이는 오라클이 원자적 사실의 진위에만 집착할 경우, 법적 책임 소지가 뒤바뀌는 진술, 의료적 처방과 증상 발현의 순서가 역전된 위험한 보고서, 비즈니스 승인 절차의 선후가 바뀐 프로세스 로직 오류를 전혀 감지해 내지 못함을 시사한다.</p>
<h2>5. 몽타주 기만 극복을 위한 프레임워크: DoveScore의 다차원 검증</h2>
<p>이러한 독립성 가정의 맹점과 문맥 무시의 함정을 근본적으로 해결하기 위해 학계에서 제시된 대표적인 평가 체계가 DOVESCORE (Descriptive and Ordered-Event Verification Score) 프레임워크이다. DOVESCORE는 긴 텍스트의 정렬(Alignment)을 평가할 때 텍스트를 단순하고 평면적인 사실의 집합으로 환원하지 않고, 텍스트 내부에 존재하는 상호 의존성과 인과율을 명시적으로 모델링하는 파이프라인을 도입했다.</p>
<p>결정론적 오라클의 평가 모듈을 고도화할 때 참고해야 할 DOVESCORE의 3단계 맞춤형 아키텍처는 다음과 같다.</p>
<ol>
<li><strong>Decomposer (분해기):</strong> 기존 프레임워크가 텍스트를 단순히 기계적인 구문 단위로 자르는 것에 그쳤다면, DOVESCORE의 분해기는 텍스트 내의 사실을 두 가지 이질적인 범주로 이원화하여 추출한다. 첫 번째는 ’정적 기술 사실(Descriptive Facts)’로, 시간의 흐름이나 사건의 발생과 무관하게 독립적으로 존재하는 속성(예: “문어는 심장이 세 개다”, “그 회사는 뉴욕에 위치해 있다”)을 의미한다. 두 번째는 ’사건 사실(Event Facts)’로, 시간적 타임라인 위에 위치하며 특정한 행동, 발생, 혹은 상태의 변화를 내포하는 진술(예: “메리는 승진 소식에 기뻤다”, “시스템이 오류 코드를 반환했다”)이다. 이러한 분류는 검증의 기준을 다원화하는 핵심 토대가 된다.</li>
<li><strong>FactChecker (사실 검증기):</strong> 추출된 기술 사실과 사건 사실이 제공된 원본 소스나 검색된 증거와 원자적 수준에서 사실적으로 일치하는지(Atomic Factual Accuracy)를 엄격히 검증한다. 이 단계까지는 전통적인 사실 확인 과정과 유사한 기능을 수행하여 명시적 환각을 걸러낸다.</li>
<li><strong>Sorter (정렬기):</strong> 이 프레임워크의 가장 혁신적인 구성 요소이다. 정렬기는 앞서 팩트체크를 통과한 ’사건 사실(Event Facts)’들을 원본 텍스트와 생성된 대상 텍스트 양쪽의 타임라인에 올려놓고, 이들의 시간적, 논리적 배열(Temporal Alignment)을 비교 검증한다. 즉, 생성된 텍스트 내에서 사건들이 발생하는 순서가 원본 데이터가 암시하는 논리적, 인과적 순서(Event-order consistency)와 부합하는지를 채점하는 것이다.</li>
</ol>
<p>DOVESCORE는 이 과정을 거쳐 사건의 정확성(Event Accuracy), 순서의 일관성(Order Consistency), 그리고 기술적 정확성(Descriptive Accuracy)을 종합한 가중 정밀도 점수(Weighted precision score)를 산출한다. 실험 결과, 이와 같이 사실 간의 상호 관계를 모델링하는 접근법은 기존의 세립질 평가 방식들보다 정보 정렬 평가에서 8% 이상의 성능 향상을 보였다.</p>
<p>소프트웨어 개발 환경에서 이 구조가 시사하는 바는 명확하다. 테스트 정답지를 구축할 때 단순히 ’결과물에 포함되어야 할 필수 키워드나 명제의 목록’을 작성하는 것에 그쳐서는 안 된다. 상태 변화(State change)를 유발하는 비즈니스 이벤트 간에는 엄격한 ’순서 제약(Ordering Constraints)’이 존재하므로, 이를 정답지 스키마에 반드시 명시해야 한다. AI가 개별 사실을 완벽하게 생성했더라도 절차적 선후 관계가 뒤틀려 있다면, 오라클은 이를 즉각적인 실패(Fail)로 판정할 수 있어야 한다.</p>
<h2>6. 문맥 보존형 추출 및 검증을 위한 진화된 접근법</h2>
<p>결정론적 오라클을 구성하는 파이프라인에서 안티패턴을 극복하기 위해서는 텍스트 분해와 검증의 전 과정에 걸쳐 문맥을 주입하고 보존하는 진화된 수학적, 구조적 접근법이 요구된다. 최근 연구들은 FActScore와 SAFE의 한계를 넘어서기 위해 다음과 같은 다양한 고도화 기법들을 제안하고 있다.</p>
<h3>6.1  VeriScore와 VeriFastScore: 슬라이딩 윈도우 기반 문맥 해소와 검증 가능성 판별</h3>
<p>논문 <em>VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation</em>은 복잡한 긴 텍스트에서 ’검증 가능한 주장의 추출’이라는 새로운 기준을 도입했다. 기존 방식들이 검증 불가능한 주관적 의견이나 가정까지 무리하게 분해하여 오라클의 점수를 훼손했던 반면, VeriScore는 철저히 검증 가능한 사실만을 타겟으로 삼는다.</p>
<p>특히 VeriScore는 추출 과정에서 대명사와 지시어(Referent)가 유발하는 문맥적 모호성을 해소하기 위해 ‘슬라이딩 윈도우(Sliding Window)’ 기반의 프롬프팅 논리를 적용한다. 분해 모델은 단일 문장을 고립시켜 해석하지 않고 다음과 같은 형태의 프롬프트를 통해 주변 문맥을 강제로 인지하게 된다. <code>(context1: 0-3 sentences) &lt;SOS&gt;current sentence&lt;EOS&gt; (context2: 0-1 sentence)</code></p>
<p>만약 시스템이 질의응답(QA) 태스크를 수행 중이라면 원래의 질문(Question)을 윈도우의 컨텍스트에 항상 전처리하여 포함시키며, 단락이 5문장 이상으로 길어질 경우에는 단락의 첫 번째 문장을 강제로 주입하여 텍스트의 주제(Topic)가 탈락하는 현상을 방지한다. 이 과정을 거치면 “그의 주목할만한 영화 출연작은…“이라는 모호한 문장이 “크리스토퍼 놀란의 주목할만한 영화 출연작은…“과 같이 자기 완결성(Self-contained)을 갖춘 청구(Claim)로 재탄생하게 되어 오라클의 검증 정확도를 극대화할 수 있다.</p>
<p>증거를 대조하는 방식에 있어서도 VeriScore는 엄격한 논리적 분류 체계를 사용한다. 단편적 참/거짓을 넘어, 검색된 증거 집합 <span class="math math-inline">E_c</span>를 바탕으로 하나의 청구 <span class="math math-inline">c</span>를 다음 4가지 시나리오로 확정 판별한다.</p>
<ol>
<li><strong>지지됨(Supported):</strong> 청구의 모든 구성 요소 <span class="math math-inline">p</span>에 대해 이를 지지하는 증거 <span class="math math-inline">e \in E_c</span>가 존재하며, 모순되는 증거가 단 하나도 없을 때.</li>
<li><strong>모순됨(Contradicted):</strong> 다른 요소의 지지 여부와 무관하게, 청구의 최소 한 부분 <span class="math math-inline">p</span>라도 명백히 모순되는 증거가 존재할 때.</li>
<li><strong>결론 보류 a (Inconclusive a):</strong> 최소 한 부분 <span class="math math-inline">p</span>에 대해 지지하는 증거도, 모순되는 증거도 찾을 수 없을 때.</li>
<li><strong>결론 보류 b (Inconclusive b):</strong> 한 부분 <span class="math math-inline">p</span>에 대해 상충되는 증거가 동시에 존재할 때.</li>
</ol>
<p>이러한 다층적 판별 논리를 바탕으로 VeriScore의 사실성 정밀도(P)는 다음과 같이 계산된다. <span class="math math-inline">P(r) = \frac{S(r)}{\vert C \vert}</span> (단, <span class="math math-inline">C</span>는 생성 응답 <span class="math math-inline">r</span> 내의 모든 검증 가능한 주장의 집합, <span class="math math-inline">S(r)</span>은 증거에 의해 지지되는 주장의 수이다.) 나아가 응답이 충분한 정보량을 담고 있는지에 대한 재현율(Recall) <span class="math math-inline">R(r)</span>을 평가하기 위해, 시스템 응답에 요구되는 사실의 중앙값 <span class="math math-inline">K</span>를 설정하여 <span class="math math-inline">R(r) = \min(1, \frac{\vert C \vert}{K})</span>를 구한다. 최종적으로 이 둘의 조화 평균인 F1@K 스코어를 산출한다. <span class="math math-inline">F1@K(r) = \frac{2 \cdot P(r) \cdot R(r)}{P(r) + R(r)} \quad \text{(단, } S(r) &gt; 0 \text{일 때, 그렇지 않으면 } 0)</span>  이러한 수학적 정식화는 결정론적 오라클이 단순한 텍스트 매칭 비율을 넘어 응답의 깊이와 정확도를 동시에 계량화할 수 있는 탄탄한 토대를 제공한다. 이 논리를 더욱 발전시켜 분해와 검증을 단일 패스(Single forward pass)로 병합 처리하여 연산 속도를 6배 이상 향상시킨 VeriFastScore와 같은 파생 프레임워크도 개발되어 CI/CD 파이프라인의 실시간 평가에 기여하고 있다.</p>
<h3>6.2  Fact in Fragments: 동적 원자적 사실 추출과 적응형 검증</h3>
<p>논문 <em>Fact in Fragments: Deconstructing Complex Claims via LLM-based Atomic Fact Extraction and Verification</em>은 정적(Static) 분해 전략이 유발하는 논리적 단절과 누적 오류(Accumulated Errors)의 위험성을 지적한다. 기존의 규칙 기반 시스템이나 제로샷 프롬프트에 의존한 사실 분해는 의미적 의도(Semantic intent)보다는 문법적이고 구문론적인 분절(Syntactic fragmentation)에만 치중하여 주장의 원래 구조를 심각하게 훼손한다.</p>
<p>이를 극복하기 위해 제안된 AFEV (Adaptive Atomic Fact Verification) 모델은 다중 홉 추론(Multi-hop reasoning)이 필요한 복잡한 주장에 대해 다음과 같은 3단계 적응형 파이프라인을 운영한다.</p>
<ul>
<li><strong>동적 원자적 사실 추출 (Dynamic Atomic Fact Extraction):</strong> 복잡한 주장을 한 번에 쪼개는 것이 아니라, 논리적 관계와 의미적 입도(Semantic granularity)를 유지하면서 순차적이고 반복적으로 서브 문제(Sub-problems)로 분해한다.</li>
<li><strong>정제된 증거 검색 (Refined Evidence Retrieval):</strong> 노이즈가 섞인 증거를 필터링하기 위해 사전 훈련된 증거 재순위화(Reranking) 모델을 도입하고, 현재 검증 중인 사실과 논리적/의미적 유사성(Semantic similarities)이 높은 훈련 인스턴스를 동적으로 검색하여 모델의 추론을 돕는 데몬스트레이션(Demonstrations)으로 활용한다.</li>
<li><strong>적응형 원자적 사실 검증 (Adaptive Atomic Fact Verification):</strong> 검색된 증거와 문맥 맞춤형 데몬스트레이션을 결합하여, 단순한 이분법적 참/거짓 판단을 넘어 “왜 이 사실이 참 또는 거짓인지“에 대한 해석 가능한 근거(Interpretable rationales)를 추출한다.</li>
</ul>
<h3>6.3  LEAF: RAG 시스템에서의 문맥 인지 다중 쿼리 팩트 체크</h3>
<p>의료, 법률, 금융 등 전문 도메인에서는 미세한 문맥의 차이나 단어 하나가 치명적인 결과(예: 부적절한 처방, 법적 책임 역전)를 초래할 수 있다. 이러한 환경의 RAG (Retrieval-Augmented Generation) 시스템 평가에 특화된 접근법이 논문 <em>LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve Factualness in Large Language Models</em>에서 제안되었다.</p>
<p>이 프레임워크의 핵심 평가기인 RAFE (Retrieval-Augmented Factuality Evaluator)는 문맥을 철저히 보존하는 4단계 검증 프로세스를 거친다.</p>
<ol>
<li><strong>분리 및 재작성 (Statement Splitting and Rewriting):</strong> 응답을 개별 진술로 나누되, 문장 간의 종속성을 파괴하지 않는다. 주변 문장의 필수적인 문맥을 끌어와 각 진술이 그 자체로 완전한 의미를 지니도록 ’재작성’한다.</li>
<li><strong>문맥 인지 쿼리 생성 (Context-Aware Query Generation):</strong> 검증을 위한 검색 쿼리를 만들 때 진술 단독으로 만들지 않는다. 사용자의 ‘원래 질문(Original question)’, ‘검증 대상 진술’, 그리고 ’이전에 검색된 문서들’의 맥락을 모두 통합하여 다중 문맥 쿼리를 생성한다.</li>
<li><strong>반복적 증거 탐색 (Iterative Evidence Search):</strong> 단일 검색에 의존하지 않고 각 진술당 최소 3회의 다중 검색을 수행하여 사실을 입증할 포괄적인 증거를 축적한다.</li>
<li><strong>문맥을 고려한 평가 (Rating with Contextual Awareness):</strong> 축적된 증거를 바탕으로 거시적인 문맥(Broader context)을 인지한 상태에서 각 진술을 지지됨(Supported) 또는 지지되지 않음(Not Supported)으로 분류한다.</li>
</ol>
<p>특히 LEAF 프레임워크는 단순히 오류를 잡아내는 오라클의 역할을 넘어, 오라클의 검증 결과를 시스템 개선에 직접 활용하는 ‘Fact-Check-then-RAG’ 파이프라인을 도입했다. 초기 팩트 체크에서 문맥적 오류가 발견되면, 팩트 체크 과정에서 색인된 정확한 지식 문서를 프롬프트에 재주입하여 파라미터 업데이트 없이도 모델이 사실에 근거한 응답을 재생성하도록 유도한다. 나아가 오라클을 통과한(검증된) 고품질의 응답 데이터만을 선별하여 지도 미세 조정(SFT) 및 단순 선호도 최적화(SimPO)에 재사용함으로써, 라벨링된 데이터 없이도 모델의 사실성 생성 능력을 근본적으로 향상시킨다.</p>
<p>이와 유사한 맥락에서 논문 <em>Molecular Facts: Desiderata for Decontextualization in LLM Fact Verification</em> 역시 탈문맥화(Decontextualization)의 중요성을 강조한다. 너무 잘게 쪼개져 의미를 상실한 원자적 주장을, 대명사 해소(Coreference Resolution)와 생략된 목적어 복원을 통해 독립적으로 해석 가능한 ’분자적 사실(Molecular Facts)’로 재구성해야만 검증의 자동화와 문맥의 보존이라는 두 마리 토끼를 잡을 수 있다는 것이다.</p>
<h2>7. 문맥 인지형 결정론적 오라클 설계 원칙</h2>
<p>지금까지 논의한 바와 같이, 소프트웨어 검증 파이프라인(CI/CD)에 AI 오라클을 통합할 때 단순한 문자열 기반의 단편적 사실 검증의 함정에 빠지지 않으려면, 정답지 데이터셋(Golden Dataset)의 구축 방식과 평가 알고리즘 설계가 근본적으로 혁신되어야 한다. 문맥 무시 안티패턴을 회피하기 위한 오라클 설계의 핵심 원칙은 다음과 같다.</p>
<table><thead><tr><th><strong>오라클 설계 원칙</strong></th><th><strong>전통적 안티패턴</strong></th><th><strong>문맥 인지형 아키텍처 제안</strong></th></tr></thead><tbody>
<tr><td><strong>정답지 자료 구조</strong></td><td>단면적인 키워드나 명제의 배열 (Flat List)</td><td>이벤트의 종속성과 시간적 선후 관계를 표현하는 유향 비순환 그래프(DAG) 및 JSON Schema 도입.</td></tr>
<tr><td><strong>검증 최소 단위</strong></td><td>문법적으로 쪼개진 원자적 사실 (Atomic Facts)</td><td>대명사와 생략 구문이 복원되어 자기 완결성을 지닌 ‘분자적 사실(Molecular Facts)’ 사용.</td></tr>
<tr><td><strong>논리적 제약 조건</strong></td><td>사실 간 독립성 가정 기반의 확률 곱</td><td>AND, OR, NOT 관계를 포함하는 계층적 프러포지셔널 논리(Propositional Logic)의 엄격한 판정.</td></tr>
<tr><td><strong>인과성 검증 로직</strong></td><td>포함 여부만 확인하는 정밀도(Precision) 연산</td><td>DOVESCORE의 Sorter와 같이 타임라인 상의 순서 일관성(Order Consistency)을 강제 채점.</td></tr>
<tr><td><strong>증거 검색 방식</strong></td><td>개별 사실 단위의 고립된 쿼리 검색</td><td>원래 질문과 이전 문맥을 모두 포함하는 다중 문맥 인지 쿼리(Context-aware Query) 사용.</td></tr>
</tbody></table>
<p><strong>첫째, 정답지의 자료 구조를 평면적 리스트(Flat List)에서 의존성 그래프(Dependency Graph)로 전환해야 한다.</strong> 기대되는 시스템 출력(Expected output)을 독립된 문자열 배열(예: <code>["사용자 데이터 생성", "결제 정보 업데이트", "이메일 알림 발송"]</code>)로 정의하는 것은 앞서 논의한 몽타주 거짓말의 표적이 되기 쉽다. 이를 방지하기 위해서는 각 사실 단위(노드)와 이들 간의 인과성 및 시간적 선후 관계(간선)를 명확히 포함하는 유향 비순환 그래프(DAG) 형태나, 의존성 속성이 부여된 JSON Schema로 정답지를 정의해야 한다. 특정 이벤트가 다른 이벤트보다 반드시 선행되어야 함을 나타내는 <code>"depends_on"</code> 또는 <code>"precedes"</code> 속성을 강제함으로써, AI 모델이 요구된 사실을 모두 출력했더라도 그 순서가 논리적으로 뒤틀려 있다면 오라클이 단호하게 실패(Fail)를 반환하도록 설계해야 한다.</p>
<p><strong>둘째, 오라클의 판정 로직에 엄격한 계층적 논리 연산(Propositional Logic Constraints)을 통합해야 한다.</strong> 오라클은 단순히 외부 증거와의 텍스트 일치 여부를 대조하는 것을 넘어, 생성된 사실들 간의 논리적 모순을 탐지할 수 있어야 한다. 복잡한 비즈니스 애플리케이션(예: ‘<span class="math math-inline">A</span> 조건과 <span class="math math-inline">B</span> 조건이 동시에 성립할 때만 <span class="math math-inline">C</span> 트랜잭션이 승인된다’)을 검증할 때, 평가 시스템은 <span class="math math-inline">A, B, C</span> 명제 자체를 개별적으로 검증하는 데 그쳐서는 안 된다. 명제 간의 논리적 결합(Conjunction), 논리합(Disjunction), 부정(Negation) 관계를 해석하여 종합적인 논리적 정합성(Logical Consistency)을 평가하는 지표를 오라클의 채점 기준에 포함시켜야만 기만적 출력을 원천 차단할 수 있다.</p>
<p>결론적으로, AI 모델의 생성 결과를 검증하는 결정론적 오라클은 긴 텍스트를 단순한 키워드의 나열이나 고립된 사실들의 주머니(Bag-of-facts)로 취급해서는 안 된다. 언어가 본질적으로 내포하고 있는 시간성, 인과성, 논리적 의존성을 온전히 보존하고 계량화할 수 있는 다차원적 프레임워크(예: DoveScore의 Sorter 도입, VeriScore의 윈도우 기반 해소 기법, LEAF의 문맥 인지 검색)를 정답지 설계와 평가 파이프라인에 깊숙이 내재화할 때, 비로소 맹목적이고 파편화된 단편적 검증이 초래하는 오라클 붕괴 현상을 완벽하게 방어하고 시스템의 신뢰성을 극대화할 수 있다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>FActScore: Metric for Factual Precision in LLMs - Emergent Mind, https://www.emergentmind.com/topics/factscore</li>
<li>FaStFact: Faster, Stronger Long-Form Factuality Evaluations in LLMs - arXiv, https://arxiv.org/html/2510.12839v1</li>
<li>Long-Form Information Alignment Evaluation Beyond Atomic Facts - ACL Anthology, https://aclanthology.org/2025.emnlp-main.558.pdf</li>
<li>VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation - ACL Anthology, https://aclanthology.org/2024.findings-emnlp.552.pdf</li>
<li>Towards interpretable Cryo-EM: disentangling latent spaces of molecular conformations, https://pmc.ncbi.nlm.nih.gov/articles/PMC11263974/</li>
<li>Estimating Accuracy from Unlabeled Data: A Probabilistic Logic Approach - NIPS, http://papers.neurips.cc/paper/7023-estimating-accuracy-from-unlabeled-data-a-probabilistic-logic-approach.pdf</li>
<li>FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models - arXiv, https://arxiv.org/html/2502.18573v3</li>
<li>Does Quantum Mechanics Require “Conspiracy”? - MDPI, https://www.mdpi.com/1099-4300/26/5/411</li>
<li>False positives and false negatives - Wikipedia, https://en.wikipedia.org/wiki/False_positives_and_false_negatives</li>
<li>All About False Positives in AI Detectors | Pangram Labs, https://www.pangram.com/blog/all-about-false-positives-in-ai-detectors</li>
<li>FACTSCORE: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation - ACL Anthology, https://aclanthology.org/2023.emnlp-main.741.pdf</li>
<li>OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation - arXiv, https://arxiv.org/html/2507.05965v1</li>
<li>Factual Correctness - Ragas, https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/factual_correctness/</li>
<li>An Analysis of Multilingual FActScore - arXiv, https://arxiv.org/html/2406.19415v1</li>
<li>Fact-checking information from large language models can decrease headline discernment | PNAS, https://www.pnas.org/doi/10.1073/pnas.2322823121</li>
<li>Gene Arnold - AI Fails Without Semantic Context - YouTube, https://www.youtube.com/watch?v=iP-43zwIXM8</li>
<li>Study: AI models fail to reproduce human judgements about rule violations - MIT News, https://news.mit.edu/2023/study-ai-models-harsher-judgements-0510</li>
<li>Full article: Big claims, low outcomes: fact checking ChatGPT’s efficacy in handling linguistic creativity and ambiguity - Taylor &amp; Francis, https://www.tandfonline.com/doi/full/10.1080/23311983.2024.2353984</li>
<li>dannalily/DoveScore - GitHub, https://github.com/dannalily/DoveScore</li>
<li>Long-Form Information Alignment Evaluation Beyond Atomic Facts - arXiv, https://arxiv.org/html/2505.15792v1</li>
<li>VeriScore: Evaluating the factuality of verifiable claims in long-form text generation, https://arxiv.org/html/2406.19276v1</li>
<li>ΥΥΥΥΥΥΥΥΥΥΥΥΥΥΥΥΥ VeriFastScore: Speeding up long-form factuality evaluation - arXiv, https://arxiv.org/html/2505.16973v3</li>
<li>(PDF) VeriFastScore: Speeding up long-form factuality evaluation - ResearchGate, https://www.researchgate.net/publication/391991380_VeriFastScore_Speeding_up_long-form_factuality_evaluation</li>
<li>\stackinsetc0ptc0pt \contourwhite\faBolt\faSearch VeriFastScore: Speeding up long-form factuality evaluation - arXiv, https://arxiv.org/html/2505.16973v1</li>
<li>Fact in Fragments: Deconstructing Complex Claims via LLM-based Atomic Fact Extraction and Verification - arXiv, https://arxiv.org/html/2506.07446v1</li>
<li>Deconstructing Complex Claims via LLM-based Atomic Fact Extraction and Verification - arXiv, https://arxiv.org/pdf/2506.07446</li>
<li>[Literature Review] LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve Factualness in Large Language Models - Moonlight | AI Colleague for Research Papers, https://www.themoonlight.io/en/review/leaf-learning-and-evaluation-augmented-by-fact-checking-to-improve-factualness-in-large-language-models</li>
<li>LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve Factualness in Large Language Models - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC12878983/</li>
<li>Molecular Facts: Desiderata for Decontextualization in LLM Fact Verification - ACL Anthology, https://aclanthology.org/2024.findings-emnlp.215.pdf</li>
<li>Molecular Facts: Desiderata for Decontextualization in LLM Fact Verification - arXiv, https://arxiv.org/html/2406.20079v1</li>
<li>First Principles: OCI AI Agent Platform is a New Frontier for Enterprise Automation | cloud-infrastructure - Oracle Blogs, https://blogs.oracle.com/cloud-infrastructure/first-principles-oci-ai-agent-platform</li>
<li>Logical Consistency of Large Language Models in Fact-Checking - OpenReview, https://openreview.net/forum?id=SimlDuN0YT</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>