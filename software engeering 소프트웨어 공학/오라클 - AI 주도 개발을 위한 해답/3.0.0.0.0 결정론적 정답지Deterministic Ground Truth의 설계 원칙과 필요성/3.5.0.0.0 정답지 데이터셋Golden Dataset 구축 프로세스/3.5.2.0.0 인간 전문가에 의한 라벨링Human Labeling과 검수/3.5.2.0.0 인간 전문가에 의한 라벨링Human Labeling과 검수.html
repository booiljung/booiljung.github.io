<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.5.2 인간 전문가에 의한 라벨링(Human Labeling)과 검수</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.5.2 인간 전문가에 의한 라벨링(Human Labeling)과 검수</h1>
                    <nav class="breadcrumbs"><a href="../../../../../index.html">Home</a> / <a href="../../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../../index.html">Chapter 3. 결정론적 정답지(Deterministic Ground Truth)의 설계 원칙과 필요성</a> / <a href="../index.html">3.5 정답지 데이터셋(Golden Dataset) 구축 프로세스</a> / <a href="index.html">3.5.2 인간 전문가에 의한 라벨링(Human Labeling)과 검수</a> / <span>3.5.2 인간 전문가에 의한 라벨링(Human Labeling)과 검수</span></nav>
                </div>
            </header>
            <article>
                <h1>3.5.2 인간 전문가에 의한 라벨링(Human Labeling)과 검수</h1>
<p>AI 기반 소프트웨어 개발에서 결정론적 정답지(Deterministic Ground Truth)를 구축하는 과정은 본질적으로 데이터가 내포한 모호성(Ambiguity)을 제거하고, 기계가 판별할 수 있는 절대적 기준, 즉 오라클(Oracle)을 확립하는 과정이다. 아무리 방대한 데이터를 수집하더라도, 해당 데이터에 부여된 라벨(Label)의 품질이 보장되지 않는다면 AI 모델의 평가와 테스트는 무의미해진다. 특히 소프트웨어 테스트에서 오라클은 시스템의 출력값이 ’정답’인지 ’오답’인지 판별하는 유일한 척도이므로, 이를 구성하는 골든 데이터셋(Golden Dataset)은 무결성에 가까운 정확도를 요구한다. 본 절에서는 이러한 결정론적 정답지를 생성하기 위해 필수적인 인간 전문가(Human Expert)의 개입 프로세스, 다중 검토 체계, 작업자 간 일치도(Inter-Annotator Agreement)의 수학적 검증, 그리고 휴먼 인 더 루프(Human-in-the-loop, HITL) 기반의 오라클 검수 기법을 심도 있게 다룬다.</p>
<h2>1.  도메인 전문가(SME)의 역할과 인지적 편향 통제</h2>
<p>결정론적 정답지를 구축하는 과정에서 가장 큰 오해는 다수의 군중(Crowd)을 동원하면 집단 지성을 통해 정답에 수렴할 것이라는 가정이다. 단순한 이미지 분류나 감정 분석과 같은 주관적이고 진입 장벽이 낮은 작업에서는 크라우드소싱이 유효할 수 있으나, 의료, 법률, 복잡한 비즈니스 로직, 그리고 코드 생성 AI를 검증하는 소프트웨어 테스트 환경에서는 고도로 훈련된 도메인 전문가(Subject-Matter Experts, SMEs)의 판단이 필수적이다.</p>
<p>도메인 전문가는 단순한 지시문의 수행자를 넘어, 모델이 실전 환경에서 직면할 수 있는 엣지 케이스(Edge Cases)와 예외 상황을 정의하는 아키텍트 역할을 수행한다. 예를 들어, 컴퓨터 비전 기반의 제조 공정 결함 탐지(Defect Detection) 모델을 위한 정답지를 구축할 때, 전문가는 단순히 결함의 존재 유무를 넘어서 픽셀 단위의 완벽한 세분화(Pixel-perfect segmentation)를 수행해야 한다. 마찬가지로 자연어 처리(NLP) 영역에서도 언어와 방언의 미세한 차이를 이해하는 언어학적 전문가의 개입이 절대적이다. 인도의 음성 인식 AI 벤치마크인 ‘Voice of India’ 프로젝트의 사례를 보면, 글로벌 음성 시스템들은 힌디어를 단일 표준어로 취급하지만, 실제로는 5천만 명 이상이 사용하는 보즈푸리(Bhojpuri)와 같은 방언이 존재한다. 도메인 전문가가 개입하여 이러한 방언, 코드 스위칭(Code-switched speech, 예: 힌디어-영어 혼용), 그리고 지역별 억양의 변동성을 정확하게 전사(Transcription)하고 철자 변형(Spelling variants)을 수동으로 큐레이션하지 않는다면, GPT-4o와 같은 최신 모델조차 단어 오류율(Word Error Rate, WER)이 55%를 초과하는 기능적 실패를 겪게 된다.</p>
<p>그러나 전문가라 할지라도 인지적 편향(Cognitive Bias)이나 피로도에 의해 라벨링 노이즈를 발생시킬 수 있다. 따라서 완벽한 결정론적 정답지를 얻기 위해서는 다음의 설계 원칙이 준수되어야 한다.</p>
<p>첫째, 엄격한 라벨링 가이드라인(Annotation Guidelines)의 수립이다. 가이드라인은 단순히 무엇을 라벨링할 것인가를 넘어, 모호한 상황에서 어떤 계층적 규칙을 적용하여 단 하나의 결론에 도달할 것인가를 명시해야 한다. 이를 위해 포지티브 예시(Positive examples)뿐만 아니라, 작업자가 혼동하기 쉬운 네거티브 예시(Negative examples)와 경계 조건(Boundary conditions)을 상세히 문서화해야 한다. 시각적 예시를 제공하여 라벨 간의 모호성을 최소화하는 것이 필수적이다.</p>
<p>둘째, 표상적 편향(Representational Bias)의 최소화이다. 수집된 데이터가 특정 상황이나 조건에 편중되어 있다면, 전문가의 정확한 라벨링조차 편향된 오라클을 낳는다. 성별, 인종, 지리적 배경, 사회경제적 배경 등의 표현 격차(Representation gaps)는 지배적 그룹에 대한 과대적합(Overfitting)과 소수 그룹에 대한 탐지 실패를 초래한다. 따라서 각 클래스별로 균형 잡힌 라벨 할당이 이루어지도록 스키마 수준에서 통제해야 한다.</p>
<h2>2.  분쟁 조정(Adjudication) 프로세스와 다중 검토 체계</h2>
<p>인간 전문가에 의한 라벨링은 필연적으로 작업자 간의 의견 충돌(Disagreement)을 수반한다. 특히 주관성이 개입될 여지가 있거나 문제의 난이도가 높은 경우, 동일한 데이터를 두고 전문가 A와 전문가 B가 서로 다른 정답을 제시할 수 있다. 결정론적 오라클은 단 하나의 정답만을 허용하므로, 이러한 불일치를 기계적으로 해결하는 분쟁 조정(Adjudication) 프로세스가 필수적이다. 다중 경로 라벨링(Multi-pass Labeling) 과정에서 여러 명의 도메인 전문가가 동일한 원시 데이터를 독립적으로 평가한 후, 일치도 알고리즘을 통해 충돌을 식별한다. 충돌이 없을 경우 곧바로 정답지로 편입되지만, 충돌이 발생하면 수동 혹은 자동 분쟁 조정(Adjudication) 검토를 거쳐 최종 결정론적 정답지(Golden Dataset)로 병합되는 흐름을 따른다.</p>
<p>분쟁 조정은 단순히 다수결(Majority vote)에 의존하는 것을 넘어, 체계적이고 수학적인 합의 도출 과정을 거쳐야 한다. 데이터의 품질을 극대화하기 위해 산업계에서 활용되는 주요 조정 기법은 다음과 같다.</p>
<table><thead><tr><th><strong>분쟁 조정 기법</strong></th><th><strong>작동 원리 및 특징</strong></th><th><strong>적용 환경 및 장단점</strong></th></tr></thead><tbody>
<tr><td><strong>수동 병합 및 검토 (Manual Adjudication)</strong></td><td>모든 라벨링 차이를 명시적으로 표시한 통합 버전을 생성한 뒤, 숙련된 선임 리뷰어(Senior Reviewer)나 팀 전체가 모여 최종 라벨을 확정한다.</td><td>프로젝트 초기 단계에 가이드라인의 모호성을 발견하고 팀의 이해도를 동기화하는 데 최적이다. 시간과 비용이 많이 소모된다.</td></tr>
<tr><td><strong>신뢰도 기반 반자동 조정 (Semi-automated Adjudication)</strong></td><td>작업자의 과거 벤치마크 테스트 결과나 전문성 점수를 가중치로 활용한다. 의견 충돌 시 과거 일치도 평균이 가장 높은 전문가의 라벨을 수용한다.</td><td>대규모 데이터셋 처리 시 효율적이다. 특정 아웃라이어(Outlier)의 평가를 배제하는 방식으로 자동화가 가능하다.</td></tr>
<tr><td><strong>작업 특화 자동 최적화 (Task-Specific Optimization)</strong></td><td>특정 유형의 태스크(예: 문법 검수 vs 도메인 지식 검수)에 대해 개별 작업자의 과거 퍼포먼스를 프로파일링하여, 해당 태스크에 가장 강력한 전문가의 의견을 자동 채택한다.</td><td>작업자들이 서로 다른 세부 전문성을 지닌 환경에서 오라클의 정확도를 극대화할 수 있다.</td></tr>
</tbody></table>
<p>전문가 그룹 내부의 프로세스와 별개로, 최근에는 혼합 전문가(Mixture-of-Agents, MoA)와 같은 다중 에이전트 시스템을 라벨링과 검수에 도입하는 연구도 활발하다. MoA 프레임워크는 단순히 모델의 예측을 종합하는 것을 넘어 검증(Verification), 조건부 라우팅, 충돌 중재(Conflict arbitration), 그리고 자체 개선(Self-refinement)의 다단계 신뢰성 기반 파이프라인을 운영한다. 이러한 시스템은 작업자들 사이의 갈등을 평가하기 위해 갈등률(Conflict Rate, CR) 및 모호성 해결 성공률(Ambiguity Resolution Success Rate, ARSR)과 같은 새로운 신뢰성 지표를 산출하여 주석의 견고성을 측정한다. 인간 전문가 역시 이러한 지표를 참조하여 자신의 라벨링을 교정함으로써 정답지의 품질을 끌어올릴 수 있다.</p>
<h2>3.  작업자 간 일치도(Inter-Annotator Agreement, IAA) 지표의 수학적 검증</h2>
<p>인간 전문가에 의한 라벨링 품질을 정량화하고, 분쟁 조정 프로세스의 효율성을 평가하기 위해 작업자 간 일치도(Inter-Annotator Agreement, IAA) 메트릭이 절대적인 기준으로 사용된다. 단순히 “얼마나 많은 작업자가 동일한 대답을 했는가“를 측정하는 단순 일치율(Percentage Agreement)은 우연에 의한 일치(Chance Agreement)를 배제하지 못하기 때문에 결정론적 오라클 검증에 사용하기에는 치명적인 수학적 결함이 있다.</p>
<p>예를 들어, 소프트웨어 테스트 결과 80%가 특정 범주에 속하는 극도로 불균형한 데이터셋이 있다고 가정하자. 두 명의 평가자가 데이터를 전혀 분석하지 않고 무작위로 추측하더라도 높은 확률로 일치하는 결과가 도출될 수 있다. 단순 일치율은 평가자의 실제 분석 능력과 무관하게 데이터의 편향성만으로도 높게 측정될 수 있으며, 이는 모델이 잘못된 지식을 진실로 받아들이게 만드는 결과를 초래한다. 이러한 착시를 방지하기 위해 통계적 확률 모델을 반영하여 우연의 일치를 교정하는 다양한 IAA 지표들이 고안되었다.</p>
<h3>3.1  Cohen’s Kappa ($ \kappa $)의 수학적 구조와 클래스 불균형의 역설</h3>
<p>Cohen’s Kappa는 두 명의 평가자(Rater)가 범주형 라벨을 부여할 때, 우연히 일치할 확률을 수학적으로 제거하여 진정한 합의 수준을 측정하는 가장 기본적이고 널리 쓰이는 지표이다. 통계적 평가의 기준을 확립한 이 지표는 기계 학습 모델의 분류 성능을 평가하거나 인간 라벨러 간의 일관성을 검증하는 데 핵심적인 역할을 한다. 수식은 다음과 같이 정의된다.<br />
<span class="math math-display">
\kappa = \frac{P_o - P_e}{1 - P_e}
</span><br />
여기서 <span class="math math-inline">P_o</span>(Observed agreement)는 실제 두 작업자가 동일한 라벨을 부여한 관측 일치 비율이며, <span class="math math-inline">P_e</span>(Expected agreement)는 두 작업자가 데이터의 분포 비율에 따라 무작위로 추측했을 때 우연히 일치할 것으로 기대되는 확률이다.</p>
<p>수학적으로 <span class="math math-inline">P_o</span>와 <span class="math math-inline">P_e</span>는 다음과 같이 전개된다.</p>
<ul>
<li><strong>관측 일치 비율 (<span class="math math-inline">P_o</span>):</strong> <span class="math math-inline">P_o = \frac{TP + TN}{N}</span> (여기서 <span class="math math-inline">N</span>은 전체 샘플 수, <span class="math math-inline">TP</span>는 True Positive 합치, <span class="math math-inline">TN</span>은 True Negative 합치를 의미한다.)</li>
<li><strong>우연 일치 확률 (<span class="math math-inline">P_e</span>):</strong> 두 평가자가 서로 독립적으로 라벨을 선택한다고 가정할 때, <span class="math math-inline">P_e = P_{yes} + P_{no}</span> 로 계산된다.</li>
<li><span class="math math-inline">P_{yes}</span>: (평가자 A가 긍정한 전체 비율) <span class="math math-inline">\times</span> (평가자 B가 긍정한 전체 비율)</li>
<li><span class="math math-inline">P_{no}</span>: (평가자 A가 부정한 전체 비율) <span class="math math-inline">\times</span> (평가자 B가 부정한 전체 비율)</li>
</ul>
<p>이때 오차 항을 통제하고 가설 검정(Hypothesis testing)을 수행하기 위해 <span class="math math-inline">\kappa</span> 값의 표준 오차(Standard Error) <span class="math math-inline">se(\hat{\kappa})</span>를 계산할 수 있다. 귀무가설 <span class="math math-inline">\kappa = \kappa_o</span>를 검정하기 위한 Z-통계량은 <span class="math math-inline">\frac{\vert \hat{\kappa} - \kappa_o \vert}{se(\hat{\kappa})}</span>의 형태를 취하며, 이를 통해 도출된 p-value가 통계적 유의성을 입증하는 근거가 된다.</p>
<p>Cohen’s Kappa는 1(완벽한 일치)에서 -1(완벽한 불일치) 사이의 값을 가진다. 0은 두 평가자가 동전 던지기를 한 것과 동일한 우연 수준의 일치를 의미하며, 0 미만의 값은 평가자들이 고의적으로 서로 다른 라벨을 선택하는 경향이 있음을 시사한다. 의료 및 보건 분야 등 생명과 직결되거나 높은 정확도가 요구되는 산업(Mary McHugh의 엄격한 임계값 적용)에서는 <span class="math math-inline">\kappa</span> 값이 최소 0.8 이상이 되어야 강한 일치(Strong agreement)로 간주되며, 소프트웨어 오라클용 결정론적 정답지를 구축할 때도 이 기준이 보수적으로 준용된다.</p>
<p><strong>클래스 불균형에 의한 Kappa의 역설 (The Kappa Paradox):</strong> Cohen’s Kappa는 실무에 적용할 때 치명적인 수학적 한계, 이른바 ’Kappa Paradox’를 지닌다. 데이터셋 내 특정 클래스의 비율이 압도적으로 높을 경우, <span class="math math-inline">\kappa</span> 값은 모델이나 평가자의 실제 성능을 심각하게 과소평가할 수 있다. 신용 평가 모델의 사례를 살펴보자. 300명의 고객 중 270명이 ‘우량(Good)’, 30명이 ’불량(Bad)’인 불균형 데이터셋에서, 단순 의사결정 나무(Decision Tree) 모델이 87%의 전체 정확도를 달성했다. 그러나 불량 고객에 대한 민감도(Sensitivity)는 30%에 불과했다. 이 경우, 모델과 인간 평가자가 둘 다 대다수의 고객을 ’우량’으로 판단하려는 경향성을 띠게 되어 우연 일치 확률인 <span class="math math-inline">P_e</span>가 기형적으로 상승하게 된다. 결과적으로 전체 정확도가 87%임에도 불구하고 Cohen’s Kappa는 0.244라는 매우 낮은 수치로 산출된다.</p>
<p>반대로 SMOTE 기법 등을 사용하여 클래스 분포를 균형 있게 맞춘 데이터셋에서는 동일한 89%의 정확도를 보이더라도 <span class="math math-inline">\kappa</span> 값이 0.452로 크게 상승한다. 즉, 평가자나 모델의 본질적인 능력이 동일하더라도, 테스트 셋의 클래스 비율(Class ratios)에 따라 <span class="math math-inline">\kappa</span>의 최대 도달 가능치 자체가 달라진다. 따라서 모델 간의 성능을 비교하거나 여러 그룹의 라벨러를 평가할 때는 반드시 클래스 비율이 일관되게 조정된 테스트셋(Balanced test set)을 사용해야 오판을 방지할 수 있다.</p>
<h3>3.2  다중 평가자를 위한 확장: Fleiss’ Kappa</h3>
<p>복잡한 비즈니스 로직에 대한 버그 분류나 생성형 AI의 출력 결과를 다각도로 평가하는 환경에서는 2명의 평가자만을 비교하는 Cohen’s Kappa의 한계가 명확해진다. 대규모 크라우드소싱이나 수십 명의 도메인 전문가가 교차로 참여하는 분산형 라벨링 환경에서는 Fleiss’ Kappa가 필수적으로 채택된다.</p>
<p>Fleiss’ Kappa는 Scott’s pi 통계량을 일반화한 것으로, 평가 대상(Item) 하나당 고정된 수(<span class="math math-inline">m</span>명)의 평가자가 할당되기만 하면, 각 항목을 평가하는 <span class="math math-inline">m</span>명의 구성원이 달라져도 일치도를 계산할 수 있다는 강력한 수학적 유연성을 제공한다. 수식의 논리적 골격은 Cohen’s Kappa와 유사하지만, 다수의 평가자에 의한 할당 비율을 통계적 분산의 관점에서 접근한다.</p>
<p>전체 <span class="math math-inline">N</span>개의 평가 대상에 대해 각각 <span class="math math-inline">m</span>명의 평가자가 배정되어 있고, 총 <span class="math math-inline">k</span>개의 라벨 범주가 존재할 때 Fleiss’ Kappa는 다음과 같이 산출된다.</p>
<p><span class="math math-inline">n_{ij}</span>를 <span class="math math-inline">i</span>번째 대상을 <span class="math math-inline">j</span>번째 범주로 분류한 평가자의 수라고 할 때,</p>
<p>우선 전체 할당 중 특정 범주 <span class="math math-inline">j</span>가 선택된 전역 비율 <span class="math math-inline">p_j</span>를 구한다:<br />
<span class="math math-display">
p_j = \frac{1}{N m} \sum_{i=1}^{N} n_{ij}
</span><br />
각 개별 항목 <span class="math math-inline">i</span>에 대해 평가자들이 얼마나 일치했는지를 나타내는 항목별 일치도 <span class="math math-inline">P_i</span>를 구한다:<br />
<span class="math math-display">
P_i = \frac{1}{m (m-1)} \sum_{j=1}^{k} n_{ij} (n_{ij} - 1)
</span></p>
<p>전체 항목에 대한 관측 일치도의 평균 <span class="math math-inline">\bar{P}</span> (이는 Cohen’s Kappa의 <span class="math math-inline">P_o</span>에 해당)를 구한다:<br />
<span class="math math-display">
\bar{P} = \frac{1}{N} \sum_{i=1}^{N} P_i = \frac{1}{N m (m-1)} \sum_{i=1}^{N} \sum_{j=1}^{k} n_{ij} (n_{ij} - 1)
</span><br />
우연히 일치할 기댓값 <span class="math math-inline">\bar{P_e}</span>는 각 범주 비율의 제곱의 합으로 도출된다:<br />
<span class="math math-display">
\bar{P_e} = \sum_{j=1}^{k} p_j^2
</span><br />
최종적으로 Fleiss’ Kappa <span class="math math-inline">\kappa</span>는 다음과 같다:<br />
<span class="math math-display">
\kappa = \frac{\bar{P} - \bar{P_e}}{1 - \bar{P_e}}
</span><br />
Fleiss’ Kappa의 분석이 제공하는 결정적인 통찰 중 하나는, 전체 <span class="math math-inline">\kappa</span> 값뿐만 아니라 <strong>개별 범주(Category)별 일치도 <span class="math math-inline">\kappa_j</span>를 도출할 수 있다</strong>는 점이다. 소프트웨어 결함을 라벨링하는 프로젝트에서 평가자들이 문법적 오류(Syntax Error)를 식별할 때는 0.90 이상의 높은 일치도를 보이지만, 논리적 취약점(Logical Vulnerability) 범주에서는 0.30 이하의 낮은 일치도를 보일 수 있다. 이러한 범주별 분할 분석은 라벨링 가이드라인에서 어느 부분의 정의가 모호하여 작업자 간의 혼선을 유발하고 있는지 시스템 관리자에게 명확한 진단 데이터를 제공한다.</p>
<h3>3.3  복잡한 데이터 구조와 결측치 처리를 위한 보편적 척도: Krippendorff’s Alpha ($ \alpha $)</h3>
<p>라벨링 데이터가 단순한 이진 분류(Pass/Fail)나 명목형(Nominal)을 넘어, 버그의 심각도(상/중/하)를 나타내는 순서형(Ordinal) 데이터나, 실행 시간 및 리소스 소모량과 같은 구간형(Interval), 비율형(Ratio) 데이터로 복잡해질 경우 앞선 지표들로는 정확한 측정이 불가능하다. 이때 Krippendorff’s Alpha(<span class="math math-inline">\alpha</span>)가 가장 강력하고 보편적인 도구로 활용된다.</p>
<p>Krippendorff’s Alpha는 동의(Agreement)의 관점이 아닌 불일치(Disagreement)의 관점에서 데이터 분산을 분석하며, 수식은 직관적인 형태를 취한다.<br />
<span class="math math-display">
\alpha = 1 - \frac{D_o}{D_e}
</span><br />
여기서 <span class="math math-inline">D_o</span>(Observed disagreement)는 실제 관측된 평가자 간 불일치의 정도를 나타내며, <span class="math math-inline">D_e</span>(Expected disagreement)는 우연에 의해 발생할 것으로 예상되는 불일치 분산이다.</p>
<p>Krippendorff’s Alpha의 독보적인 경쟁력은 프로젝트의 현실적인 제약을 완벽하게 극복한다는 데 있다. 다수의 평가자가 참여하는 과정에서 특정 평가자가 일부 항목의 검수를 누락하여 결측치(Missing data)가 발생하더라도, 수학적으로 관측된 쌍(Paired values)의 가중치 행렬(Coincidence matrix)을 조정함으로써 왜곡 없는 일치도 산출이 가능하다.</p>
<p>결정론적 정답지를 완성하기 위해서는 각 평가 작업의 데이터 구조와 작업자의 투입 환경에 맞춰 적합한 IAA 메트릭을 취사선택해야 한다. 프로젝트 관리자는 초기 라벨링 단계에서 IAA 지표를 지속적으로 모니터링하고, 일치도가 임계값(일반적으로 0.8)을 하회할 경우 즉각적으로 작업을 중단한 뒤 <strong>불일치 원인 분석(Discrepancy analysis) <span class="math math-inline">\rightarrow</span> 가이드라인 보강 <span class="math math-inline">\rightarrow</span> 라벨러 재교육 <span class="math math-inline">\rightarrow</span> 라벨링 재개</strong>의 주기를 반복하여 시스템의 객관성을 확보해야 한다.</p>
<p><img src="./3.5.2.0.0%20%EC%9D%B8%EA%B0%84%20%EC%A0%84%EB%AC%B8%EA%B0%80%EC%97%90%20%EC%9D%98%ED%95%9C%20%EB%9D%BC%EB%B2%A8%EB%A7%81Human%20Labeling%EA%B3%BC%20%EA%B2%80%EC%88%98.assets/image-20260222182434642.jpg" alt="image-20260222182434642" /></p>
<h2>4.  IAA는 기계 학습 성능의 절대적 상한선(Ceiling)인가?</h2>
<p>오랜 기간 자연어 처리(NLP)와 AI 학계에서는 “작업자 간 일치도(IAA)가 기계 학습 시스템 성능의 상한선(Upper Bound)을 규정한다“는 논리 실증주의적(Logical positivist) 관점이 정설로 받아들여졌다. 즉, 데이터를 가장 잘 이해하는 인간조차 80%의 일치도밖에 달성하지 못한다면, 기계가 이 데이터로 학습하여 80% 이상의 정확도를 내는 것은 불가능하며, 만약 그 이상을 기록한다면 이는 시스템이 데이터의 패턴을 진정으로 이해한 것이 아니라 테스트 셋에 과대적합(Overfitting)된 결과라는 논리이다.</p>
<p>이러한 가설은 정답지 구축이 특정 임계값에 도달하면 라벨링을 중단하거나, 시스템 성능의 한계를 지레짐작하게 만드는 부작용을 낳았다. 그러나 최신 연구 논문인 <em>Inter-annotator agreement is not the ceiling of machine learning performance: Evidence from a comprehensive set of simulations</em> (2022) 등은 방대한 시뮬레이션과 경험적 데이터를 바탕으로 이 오랜 가정을 정면으로 반박한다.</p>
<p>해당 연구진은 임상 의료 문서 내 개체명 인식(Clinical notes NER), 기만 탐지(Deception detection), 단어 유사성 평가(SimLex-999) 등 다양한 태스크를 분석한 결과, 최소 6개의 연구에서 20개 이상의 기계 학습 시스템이 인간의 IAA를 유의미하게 능가하는 실증적 사례를 발견했다. 이러한 현상이 발생하는 수학적, 논리적 이유는 <strong>인간 라벨러가 생성하는 노이즈의 특성과 기계 학습 모델의 최적화 방식이 근본적으로 다르기 때문</strong>이다.</p>
<p>인간 전문가는 아무리 고도로 훈련받았다 할지라도, 작업 시간의 경과에 따른 피로도, 집중력 저하, 그리고 개인의 고유한 인지적 편향에 의해 일관성 없는 무작위 노이즈(Random noisy labels)를 생성한다. 즉, 인간 A와 인간 B는 각자의 독립적인 오차율을 가지며, 이 오차가 충돌하는 지점에서 IAA 수치가 하락한다. 반면, 충분한 파라미터와 적절히 명시된(Well-specified) 아키텍처를 갖춘 기계 학습 모델이나 대형 언어 모델(LLM)은 방대한 훈련 데이터를 통해 정칙화(Regularization) 과정을 거친다. 학습 과정에서 모델은 개별 라벨러가 만들어낸 산발적이고 무작위적인 노이즈를 상쇄(Averaging out)하고, 데이터 기저에 깔려 있는 진정한 통계적 패턴(Underlying classification function)만을 추출하여 학습할 수 있다.</p>
<p>이는 소프트웨어 오라클용 결정론적 정답지를 구축하는 엔지니어들에게 중대한 통찰을 제공한다. IAA 지표는 라벨링 품질을 모니터링하고 가이드라인의 모호성을 진단하는 훌륭한 품질 관리 도구이지만, 그 자체가 AI 모델이 도달할 수 있는 ’절대적 성능의 한계’를 규정하는 물리적 법칙은 아니라는 것이다. 따라서 IAA가 적정 수준에 도달했다고 해서 오라클 모델의 추가적인 성능 개선 여지가 없다고 단정 짓거나 벤치마크 확장을 중단하는 것은 심각한 기회비용을 초래한다. 결정론적 오라클 시스템의 궁극적인 검증 기준은 IAA 수치 자체의 극대화가 아니라, <strong>도출된 정답지가 시스템의 목적 함수(Objective function)와 실세계의 진정한 비즈니스 논리(Ground truth logic)를 얼마나 충실히 대변하는가</strong>에 두어야 한다.</p>
<h2>5.  품질 보증(QA) 파이프라인과 교차 검증(Cross-Validation) 전략</h2>
<p>작업자 간 일치도가 높게 측정되었다 하더라도, 이것이 곧 데이터의 ’결정론적 진실성’을 완벽히 담보하는 것은 아니다. 전문가 집단이 특정한 편향(Shared inappropriate bias)을 공유하고 있다면, 높은 <span class="math math-inline">\kappa</span> 값을 보이면서도 시스템 전체가 오답을 진실로 채택하는 집단사고의 오류에 빠질 위험이 있다. 예를 들어, 평가자 전원이 특정 구시대적 코딩 관습이나 보안상 취약한 로직을 정답으로 간주하도록 가이드라인이 잘못 설계되었다면 일치도는 완벽하겠지만, AI 오라클은 치명적인 버그를 통과시키는 방향으로 학습된다. 이를 방지하기 위해서는 철저한 품질 보증(QA) 파이프라인과 엄밀한 교차 검증 방법론이 수반되어야 한다.</p>
<h3>5.1  다중 계층적 검수 전략(Multi-tier QA Strategies)</h3>
<p>정답지의 무결성을 유지하기 위해 데이터 수집 및 라벨링 라이프사이클 전반에 걸쳐 다음과 같은 검수 장치를 내장해야 한다.</p>
<ul>
<li><strong>감사 태스크(Audit Tasks)의 암묵적 삽입:</strong> 정답이 이미 완벽하게 검증된 통제 데이터(Golden Data / Control Examples)를 일반 작업 항목 사이에 시각적으로 구별되지 않게 무작위로 숨겨 배포한다. 작업자가 이를 틀리게 라벨링할 경우, 시스템은 해당 작업자의 신뢰도 점수를 즉각적으로 삭감하고 추가적인 재교육을 강제한다. 이는 작업자의 지속적인 집중도를 모니터링하고 라벨링 프로세스의 안정성을 확보하는 핵심 기법이다.</li>
<li><strong>표적 검수(Targeted QA)와 모호성 해결:</strong> 모델이 추론 단계에서 불확실성(Uncertainty)을 강하게 보이거나, 초기 다중 경로 라벨링 단계에서 작업자 간 의견 충돌이 극심했던 경계선 데이터(Edge Cases)만을 우선적으로 추출한다. 이러한 고난도 항목들은 최고 숙련자(Senior SME) 또는 분쟁 조정 위원회가 집중적으로 재검토하여 가이드라인의 사각지대를 메우는 데 사용된다.</li>
<li><strong>무작위 표본 감사(Random Sampling and Auditing):</strong> 전체 라벨링이 완료된 데이터셋 중 통계적으로 유의미한 표본을 무작위로 추출하여 재검토한다. 이를 통해 전체 데이터셋의 품질 수준(Error rate) 상한선을 통계적으로 추정하고 프로젝트의 최종 승인 여부를 결정한다.</li>
</ul>
<h3>5.2  데이터 누수(Data Leakage) 방지와 교차 검증(Cross-Validation)</h3>
<p>결정론적 정답지가 실전 환경에서도 견고한 오라클을 생성하는지 검증하기 위해서는 기계 학습의 교차 검증(Cross-Validation, CV) 기법을 엄밀하게 적용해야 한다. 제한된 정답지 데이터를 효율적으로 활용하기 위해 K-Fold 교차 검증 기법이 널리 쓰이지만, 데이터의 도메인 특성을 무시한 무작위 분할은 치명적인 데이터 누수(Data leakage)를 일으킨다.</p>
<p>대표적인 예로, 동일한 개체(Subject, 예: 특정 사용자의 세션 로그, 특정 환자의 연속된 의료 영상, 특정 클래스에서 파생된 여러 단위 테스트 코드)에서 여러 개의 이벤트(Record)가 파생된 데이터를 평가할 때를 들 수 있다. 이때 단순한 **레코드 단위 교차 검증(Record-wise CV)**을 수행하면, 특정 개체의 특성이 훈련 셋(Training fold)과 검증 셋(Validation fold)에 동시에 포함되는 현상이 발생한다. 오라클 모델은 해당 개체의 고유한 패턴을 암기해버리므로 성능이 과대평가(Overestimation)되고, 이전에 본 적 없는 새로운 데이터(Out-of-sample)에 대한 일반화 능력은 현저히 떨어진다.</p>
<p>따라서 오라클 성능을 엄밀하게 평가하고 정답지의 편향을 막으려면, 개체의 식별성을 철저히 보존하는 **개체 단위 교차 검증(Subject-wise Cross-Validation)**을 필수적으로 적용하여 훈련 셋과 테스트 셋 간의 완전한 독립성을 보장해야 한다. 또한 모델 아키텍처와 하이퍼파라미터를 결정하는 중첩 교차 검증(Nested CV) 단계와, 최종 모델의 성능을 평가하는 홀드아웃 테스트 셋(Hold-out test set)을 명확히 분리하여, 모델이 검증 데이터의 분포 자체에 과대적합(Overfitting)되는 것을 방지해야 한다.</p>
<p>최신 연구에서는 의도적으로 노이즈를 주입하거나 변수를 숨기는 피처 하이딩(Feature Hiding) 방식을 적용한 SYNLABEL 프레임워크 등을 통해, 하드 라벨(Hard labels)로 구성된 원본 결정론적 정답지를 조건부 확률 분포가 반영된 소프트 라벨(Soft labels) 정답지로 변환하여 오라클의 노이즈 강건성(Robustness)을 베이스라인 테스트하는 방법론도 채택되고 있다.</p>
<h2>6.  소프트웨어 테스트 오라클을 위한 휴먼 인 더 루프(HITL) 최적화 메커니즘</h2>
<p>결정론적 정답지는 단순히 데이터를 모아두는 저장소가 아니다. 고도화된 AI 소프트웨어 개발 파이프라인에서 정답지는 인간의 판단력과 알고리즘적 자동화가 유기적으로 상호작용하는 휴먼 인 더 루프(Human-in-the-Loop, HITL) 시스템의 심장 역할을 한다.</p>
<p>LLM 기반 코파일럿(Copilot)이 생성한 수만 줄의 코드나, 비정형 데이터를 처리하는 복잡한 소프트웨어의 출력을 검증할 때, 모든 결과물을 인간이 직접 확인하고 라벨링하는 것은 불가능하다. 정적 분석기(Static Analyzer)나 컴파일러 단독으로는 비즈니스 로직에 얽힌 의미론적 버그(Semantic bugs)나 맥락상 부적절한 환각 현상(Hallucinations)을 잡아낼 수 없다. 이를 극복하기 위해 제안된 것이 **휴먼 인 더 루프 기반 능동 오라클 학습(Human-in-the-Loop Active Oracle Learning, HIOL)**이다.</p>
<p>HIOL 시스템은 20~50개의 극소수 핵심 테스트 케이스에 대해서만 도메인 전문가에게 정답 라벨(Pass/Fail 또는 기대 출력값)을 요구한다. 시스템은 이 희소한 정답지를 기반으로 결정 트리(Decision Trees)나 유전 프로그래밍(Genetic Programming)을 활용하여 임시적인 ’중간 자동 테스트 오라클(Intermediate automatic test oracle)’을 학습시킨 뒤, 이를 활용하여 시스템 내의 수만 개의 잔여 출력 결과를 판별한다.</p>
<h3>6.1  능동 학습(Active Learning)과 쿼리 예산(Query Budget) 최적화</h3>
<p>인간 전문가(SME)의 라벨링은 가장 비용이 많이 들고 속도가 느린 병목 구간이다. 이 자원을 극대화하기 위해 능동 학습(Active Learning) 기법이 결합된다. 모델은 수많은 미분류 데이터 중 자신의 예측 불확실성이 가장 높거나, 결정 경계(Decision boundary)에 근접하여 정보량이 가장 풍부하다고 판단되는 샘플(Informative data points)만을 알고리즘적으로 추출하여 인간에게 쿼리(Query)한다. 이러한 방식을 통해 전체 데이터를 수동으로 라벨링할 때 대비 인간의 작업량을 50% 이상 극적으로 단축시키면서도, 모델이 확보하는 정확도는 동등하거나 오히려 향상되는 결과를 가져온다.</p>
<h3>6.2  오라클의 자기 교정 및 노이즈 라벨링 탐지: isonoise 알고리즘</h3>
<p>HIOL 시스템에서 마주하는 가장 치명적인 위험 요소는, <strong>제한된 쿼리 예산 속에서 인간이 제공한 소수의 앵커 라벨(Anchor labels)에 오류(Mislabeling)가 포함되어 있을 경우 오라클 전체의 판별 기준이 무너진다</strong>는 점이다. 복잡한 다중 스레드 환경의 로그나 미묘한 의미론적 결함을 분석할 때 인간은 필연적으로 실수를 범한다. 인간이 오답을 정답으로 라벨링한 채 중간 오라클을 학습시키면, 오라클은 수만 개의 테스트 케이스를 연쇄적으로 오판하게 된다.</p>
<p>이러한 인간의 오류를 통제하고 결정론적 진실을 방어하기 위해 도입된 혁신적인 기법이 <strong>isonoise</strong> 알고리즘이다. <code>isonoise</code>는 오라클의 훈련 데이터 스위트 내에서 인간이 잘못 라벨링한 것으로 의심되는 테스트 케이스를 추적하고 격리하는 자동화된 필터링 메커니즘이다.</p>
<p><code>isonoise</code> 알고리즘의 작동 메커니즘은 다음과 같다:</p>
<ol>
<li><strong>충돌 지수(Disagreement Score) 계산:</strong> 시스템은 퍼징(Fuzzing) 기법을 활용하여 인간이 라벨링한 데이터들 사이의 동작 패턴을 수십 회 이상 반복 테스트한다. 이 과정에서 다른 유사한 테스트 케이스들과 동떨어진 실행 경로(Execution path)나 예외(Exception) 결과를 보이는 샘플을 수학적으로 수치화하여 충돌 지수를 매긴다.</li>
<li><strong>의심 사례 격리 및 재학습:</strong> 충돌 지수가 사전에 정의된 임계값(Disagreement Threshold)을 초과하는 테스트 케이스들을 ’오라벨 의심군’으로 분류하여 임시 격리한다. 이후, 의심군을 배제한 채로 남아있는 고신뢰도 데이터만을 사용하여 중간 자동 테스트 오라클을 새롭게 재학습시킨다.</li>
<li><strong>체계적 재라벨링(Relabeling) 쿼리:</strong> 재학습된 중간 오라클에 의심군 데이터를 다시 통과시켜 예측값을 도출한다. 만약 새 오라클의 예측 결과와 원래 인간이 부여했던 라벨이 충돌한다면, 시스템은 해당 테스트 케이스를 인간 전문가에게 다시 전송하여 재검토를 강력히 요구한다.</li>
</ol>
<p>실험 결과에 따르면, <code>isonoise</code>는 인간 라벨러가 도입한 오류 데이터의 67% 이상을 고도의 정확도로 식별해냈으며, 전체 데이터를 일일이 재검수할 필요 없이 매우 적은 수의 재검토 쿼리만으로도 오라클의 무결성을 복구하는 놀라운 효율성을 입증했다.</p>
<h2>7.  실전 오라클 구현을 위한 패치 및 명세 검증 체계</h2>
<p>휴먼 인 더 루프 기반의 철저한 라벨링과 검증 프로세스는 다양한 소프트웨어 개발 영역에서 실물 오라클(Concrete Oracle)로 구현된다.</p>
<p>첫째, 코드 생성 AI가 만든 패치(Patch)의 무결성을 검증하는 **패치 오라클(Patch Oracles)**이다. 여기서 결정론적 정답지(Ground Truth)는 인간 개발자가 작성하여 검증을 통과한 원본 패치(<span class="math math-inline">P_h</span>)가 된다. RGT(Random Testing with Ground Truth)와 같은 방법론은 인간의 정답 패치를 기준으로 테스트 케이스 모음을 생성한 뒤, AI가 생성한 후보 패치(<span class="math math-inline">P_m</span>)를 실행시켜 두 시스템 간의 미세한 동작 차이(예: 예외 유형 불일치, 타임아웃, 어서션 실패 등)를 정밀하게 분류한다. 최근에는 펄스(Pulse) 분석기와 같은 정적 분석 도구를 사용하여 부적절성 분리 논리(Incorrectness Separation Logic, ISL)를 기반으로 프로그램 내의 에러 상태를 검증하는 방식을 취하기도 한다. 수식 <span class="math math-inline">\forall (\Phi, \epsilon: \Phi&#39;) \in F_P. \; (\Phi \Rightarrow \pi_b \implies \epsilon = \mathrm{ok}) \land (\forall (\Phi&#39;&#39;, \epsilon&#39;&#39;: \Phi&#39;&#39;&#39;) \in F_P. \; \epsilon&#39;&#39; \notin \{\mathrm{err}&#39;, \dots\})</span> 은 패치 적용 후의 심볼릭 풋프린트(<span class="math math-inline">F_P</span>) 내에서 이전의 버그 경로(<span class="math math-inline">\pi_b</span>)가 정상 종료(<span class="math math-inline">\mathrm{ok}</span>)됨을 증명함과 동시에 새로운 에러 상태가 유발되지 않았음을 수학적으로 보장하는 정적 오라클의 뼈대이다.</p>
<p>둘째, 자연어 요구사항 명세서(RFC 등)에서 기계 실행 가능한 <strong>형식 규격(Formal Specifications)의 합성</strong>이다. 과거에는 인간이 수동으로 테스트 모델을 설계했으나, 최근에는 LLM 요원(Agent)이 자연어 텍스트에서 최소 전환 경로(Minimal Transition Paths, MTP)를 추출하여 유한 상태 기계(FSM)의 골격을 세운다. 도메인 전문가는 LLM이 추출한 상태 변환 규칙과 규범적 조항(Normative clauses)의 맵핑을 검토하고, 오류가 발생할 경우 최소 편집 스크립트를 통해 패치(Patch)를 생성하여 시스템에 피드백을 제공한다. 이러한 인간-AI 협업은 자연어로 작성된 모호한 프로토콜 명세서를 확정적인 자동화 테스트 생성기로 탈바꿈시킨다.</p>
<p>셋째, 개인정보 보호 GUI 에이전트와 같은 복잡한 상호작용 시스템의 평가이다. GUIGuard와 같은 프레임워크는 AI 에이전트가 화면에서 추출한 데이터가 올바른지 평가하기 위해 데이터 카테고리, 개인정보 보호 범주, 작업의 필수성이라는 세 가지 차원에 대해 결정론적 정답 라벨(Deterministic ground-truth labels)을 구축한다. 이 모델의 오라클은 시스템의 예측이 이 세 가지 다차원 정답지와 정확히 일치할 때만, 그리고 추출된 텍스트 값(이메일, 이름 등)이 한 글자의 오차도 없이 완벽히 들어맞을 때만 ’정답(Correct)’으로 인정하는 극도로 보수적이고 엄격한 잣대를 적용한다.</p>
<p>결론적으로, AI 시대의 소프트웨어 테스트는 결정론적 정답지라는 견고한 닻(Anchor) 없이는 표류할 수밖에 없다. 인간 전문가의 직관과 도메인 지식, 다중 검토를 통한 분쟁 조정, IAA 메트릭을 통한 수학적 품질 관리, 그리고 능동 학습과 isonoise 알고리즘이 결합된 휴먼 인 더 루프(HITL) 체계는 비결정적인 AI를 통제하는 가장 강력한 무기이다. 이 치밀한 프로세스를 통과한 골든 데이터셋과 오라클만이 예측 불가능한 AI 소프트웨어의 신뢰성을 담보하고 기업의 기술 부채를 방어하는 최종적인 수호자 역할을 수행할 수 있다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>What Is a Golden Dataset in AI and Why Does It Matter? - DAC.digital, https://dac.digital/what-is-a-golden-dataset/</li>
<li>The adjudication process in collaborative annotation | by Jorge …, https://medium.com/@jorgecp/the-adjudication-process-in-collaborative-annotation-61623c46b700</li>
<li>Global speech AI struggles to understand India: Report, https://m.economictimes.com/small-biz/security-tech/technology/global-speech-ai-struggles-to-understand-india-report/articleshow/128410287.cms</li>
<li>What is data labeling? The ultimate guide | SuperAnnotate, https://www.superannotate.com/blog/guide-to-data-labeling</li>
<li>Data Labeling: The Authoritative Guide - Scale AI, https://scale.com/guides/data-labeling-annotation-guide</li>
<li>Toward Reliable Annotation in Low-Resource NLP - IEEE Xplore, https://ieeexplore.ieee.org/iel8/6287639/10820123/11299523.pdf</li>
<li>Measuring Inter-Annotator Agreement: Building Trustworthy Datasets, https://keymakr.com/blog/measuring-inter-annotator-agreement-building-trustworthy-datasets/</li>
<li>Interrater reliability: the kappa statistic - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC3900052/</li>
<li>What Is Cohen’s Kappa? How and When to Use It (Plus … - KNIME, https://www.knime.com/blog/cohens-kappa-an-overview</li>
<li>Inter-Annotator Agreement: An Introduction to Cohen’s Kappa Statistic, https://surge-ai.medium.com/inter-annotator-agreement-an-introduction-to-cohens-kappa-statistic-dcc15ffa5ac4</li>
<li>Inter annotator agreement, https://www.cs.brandeis.edu/~cs140b/CS140b_slides/CS140_Lect_7_InterAnnotatorAgreement.pdf</li>
<li>Chapter 19 Agreement and the kappa statistic, https://publish.uwo.ca/~jkoval/courses/Epid9509/notes/kappa.pdf</li>
<li>Fleiss’ Kappa | Real Statistics Using Excel, https://real-statistics.com/reliability/interrater-reliability/fleiss-kappa/</li>
<li>Fleiss’s kappa - Wikipedia, <a href="https://en.wikipedia.org/wiki/Fleiss&#x27;s_kappa">https://en.wikipedia.org/wiki/Fleiss%27s_kappa</a></li>
<li>Fleiss’ kappa - Knowino, <a href="https://www.theochem.ru.nl/~pwormer/Knowino/knowino.org/wiki/Fleiss&#x27;_kappa.html">https://www.theochem.ru.nl/~pwormer/Knowino/knowino.org/wiki/Fleiss%27_kappa.html</a></li>
<li>Fleiss’ kappa in SPSS Statistics, https://statistics.laerd.com/spss-tutorials/fleiss-kappa-in-spss-statistics.php</li>
<li>Fleiss’ Kappa: Measuring Agreement Among Multiple Raters - numiqo, https://numiqo.com/tutorial/fleiss-kappa</li>
<li>Inter-Annotator Agreement: a key metric in Labeling - Innovatiana, https://www.innovatiana.com/en/post/inter-annotator-agreement</li>
<li>Krippendorff’s Alpha Basic Concepts - Real Statistics Using Excel, https://real-statistics.com/reliability/interrater-reliability/krippendorffs-alpha/krippendorffs-alpha-basic-concepts/</li>
<li>Krippendorff’s alpha - Wikipedia, <a href="https://en.wikipedia.org/wiki/Krippendorff&#x27;s_alpha">https://en.wikipedia.org/wiki/Krippendorff%27s_alpha</a></li>
<li>A user-friendly tool for computing Krippendorff’s Alpha inter-rater, https://pmc.ncbi.nlm.nih.gov/articles/PMC11636850/</li>
<li>How Krippendorff’s Alpha Improves Data Reliability - Appen, https://www.appen.com/blog/krippendorffs-alpha</li>
<li>Introduction to Krippendorff’s Alpha: Inter-Annotator Data Reliability, https://encord.com/blog/interrater-reliability-krippendorffs-alpha/</li>
<li>Inter-annotator agreement is not the ceiling of machine learning, https://aclanthology.org/2022.bionlp-1.26.pdf</li>
<li>(PDF) Inter-Annotator Agreement and the Upper Limit on Machine, https://www.researchgate.net/publication/322252759_Inter-Annotator_Agreement_and_the_Upper_Limit_on_Machine_Performance_Evidence_from_Biomedical_Natural_Language_Processing</li>
<li>Practical Considerations and Applied Examples of Cross-Validation, https://pmc.ncbi.nlm.nih.gov/articles/PMC11041453/</li>
<li>Cross validation – Knowledge and References - Taylor &amp; Francis, https://taylorandfrancis.com/knowledge/Medicine_and_healthcare/Pharmaceutical_medicine/Cross_validation/</li>
<li>Cross-Validation Visualized: A Narrative Guide to Advanced Methods, https://www.mdpi.com/2504-4990/6/2/65</li>
<li>Synthetic data for soft label and label noise research, https://d-nb.info/1371676526/34</li>
<li>Isolating Noisy Labelled Test Cases in Human-in-the-Loop Oracle, https://arxiv.org/html/2506.13273</li>
<li>Human-in-the-loop oracle learning for semantic bugs in string, https://www.researchgate.net/publication/362077929_Human-in-the-loop_oracle_learning_for_semantic_bugs_in_string_processing_programs</li>
<li>(PDF) Human-in-the-Loop Testing for LLM-Integrated Software, https://www.researchgate.net/publication/391668766_Human-in-the-Loop_Testing_for_LLM-Integrated_Software_A_Quality_Engineering_Framework_for_Trust_and_Safety</li>
<li>Advanced Techniques in Data Labeling for Enhanced Machine, https://www.sapien.io/blog/advanced-techniques-in-data-labeling-for-enhanced-machine-learning-models</li>
<li>Active Learning and Human Feedback for Large Language Models, https://intuitionlabs.ai/articles/active-learning-hitl-llms</li>
<li>Patch Oracles in Software Repair &amp; Cryptography - Emergent Mind, https://www.emergentmind.com/topics/patch-oracles</li>
<li>Synthesizing Precise Protocol Specs from Natural Language for, https://arxiv.org/html/2511.17977v1</li>
<li>Toward a General Framework for Privacy-Preserving GUI Agents, https://www.researchgate.net/publication/400118175_GUIGuard_Toward_a_General_Framework_for_Privacy-Preserving_GUI_Agents</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>