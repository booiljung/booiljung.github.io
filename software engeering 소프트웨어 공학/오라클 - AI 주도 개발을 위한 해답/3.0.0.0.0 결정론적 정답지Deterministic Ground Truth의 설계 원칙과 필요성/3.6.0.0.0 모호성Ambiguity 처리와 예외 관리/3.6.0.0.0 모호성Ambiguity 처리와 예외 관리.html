<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.6 모호성(Ambiguity) 처리와 예외 관리</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.6 모호성(Ambiguity) 처리와 예외 관리</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 3. 결정론적 정답지(Deterministic Ground Truth)의 설계 원칙과 필요성</a> / <a href="index.html">3.6 모호성(Ambiguity) 처리와 예외 관리</a> / <span>3.6 모호성(Ambiguity) 처리와 예외 관리</span></nav>
                </div>
            </header>
            <article>
                <h1>3.6 모호성(Ambiguity) 처리와 예외 관리</h1>
<p>소프트웨어 공학에서 전통적인 테스트 환경은 ’동일한 입력에 대해 항상 동일한 출력을 반환한다’는 결정론적(Deterministic) 전제를 바탕으로 설계된다. 프로그래머는 명확한 규칙과 논리를 코드로 작성하며, 테스트 오라클(Test Oracle)은 이 규칙이 정상적으로 작동하는지 이진법적(Pass/Fail)으로 판별한다. 그러나 인공지능(AI), 특히 대형 언어 모델(LLM)이 소프트웨어의 핵심 컴포넌트로 편입되면서 이러한 결정론적 패러다임은 근본적인 위협에 직면하게 되었다. AI 시스템은 본질적으로 확률적(Probabilistic)이며, 자연어라는 비정형 데이터를 매개로 작동하기 때문에 필연적으로 ’모호성(Ambiguity)’과 예측 불가능한 ’예외(Exception)’를 파생시킨다.</p>
<p>결정론적 정답지(Deterministic Ground Truth)를 구축하는 과정에서 가장 큰 장애물은, 모델이 학습하고 평가받아야 할 ‘정답’ 자체가 인간의 주관성, 문맥의 부재, 지식의 한계로 인해 여러 갈래로 해석될 수 있다는 점이다. 모호성을 방치한 채 생성된 정답지는 모델에게 잘못된 패턴을 학습시키며, 이는 환각(Hallucination) 현상이나 치명적인 런타임 오류로 이어진다. 더불어 AI 에이전트가 자율적으로 판단하고 행동하는 시스템에서는 단순히 코드가 멈추는 에러를 넘어, 논리적 추론의 실패가 실행의 실패로 이어지는 복합적인 예외 상황이 발생한다. 따라서 AI 소프트웨어의 신뢰성을 보장하기 위해서는 입력과 출력의 모호성을 정량적으로 측정하고 해소하는 메커니즘, 그리고 예측 범위를 벗어난 예외 상황을 안전하게 통제하는 체계적인 관리 전략이 필수적이다.</p>
<p>본 절에서는 AI 오라클을 설계함에 있어 모호성을 처리하고 예외를 관리하는 기술적 방법론, 수학적 검증 기법, 그리고 최신 학술적 성과들을 심도 있게 분석하여, 확률적 시스템 위에 결정론적 안전망을 구축하는 방안을 제시한다.</p>
<h2>1.  모호성의 본질과 다차원적 분류 체계</h2>
<p>자연어 처리 및 요구사항 공학(Requirements Engineering)에서 모호성은 하나의 표현이나 지시가 두 개 이상의 상이하고 유효한 의미로 해석될 수 있는 상태를 지칭한다. AI 모델은 인간처럼 상식(Common Sense)이나 암묵적 지식(Tacit Knowledge)을 바탕으로 문맥을 유추하는 능력이 불완전하기 때문에, 미세한 모호성에도 완전히 다른 연산 결과를 도출할 수 있다. 결정론적 정답지를 구성하기 위해서는 먼저 데이터셋과 프롬프트에 내재된 모호성의 유형을 정확히 식별하고 분류해야 한다.</p>
<p>요구사항 공학 및 소프트웨어 테스팅의 연구들에 따르면, 소프트웨어 개발 및 AI 프롬프트에서 발생하는 모호성은 다음과 같이 다차원적으로 분류될 수 있다 :</p>
<ol>
<li><strong>어휘적 모호성 (Lexical Ambiguity):</strong> 단일 단어가 여러 의미를 갖는 경우를 말한다. 동음이의어(Homonymy)나 다의어(Polysemy)가 대표적이다. 예를 들어, 금융 데이터베이스 분석을 수행하는 Text-to-SQL AI에게 ’bank’라는 단어가 주어졌을 때, 이것이 하천의 둑을 의미하는지 아니면 금융 기관을 의미하는지 문맥을 통해 명확히 정의되지 않으면 AI는 완전히 잘못된 데이터베이스 테이블을 참조하는 치명적인 오류를 범하게 된다.</li>
<li><strong>구문적/구조적 모호성 (Syntactical/Structural Ambiguity):</strong> 문장의 구조나 수식어의 위치, 구두점의 부재 등으로 인해 해석이 달라지는 경우다. 프롬프트 엔지니어링에서 “Make it better“와 같은 지시어는 무엇을(성능, 가독성, 메모리 사용량 등) 개선해야 하는지 구조적으로 누락되어 있어 AI의 무작위 추측을 유발하며, 결과적으로 평가가 불가능한 출력을 생성하게 만든다.</li>
<li><strong>의미론적 모호성 (Semantic Ambiguity):</strong> 문장의 논리적 형태, 특히 부정문(Negation)이나 수량자(Quantifier)의 적용 범위에 대한 해석 차이에서 발생한다. 예를 들어 “모든 시스템이 오류를 발생시키지는 않는다“라는 명세는 “오류를 발생시키는 시스템이 하나도 없다“는 것인지, “일부 시스템만 오류를 발생시킨다“는 것인지 기계적 논리로 판별하기 어렵다.</li>
<li><strong>화용론적 모호성 (Pragmatic Ambiguity):</strong> 문맥(Context)이나 발화자의 의도, 배경 지식에 따라 다르게 해석되는 경우다. 의료 진단이나 법률 검토와 같이 고도의 전문 도메인에서는 암묵적 가정(Implicit Assumptions)이 포함된 경우가 많으며, AI 모델이 해당 도메인의 암묵적 룰을 학습하지 못했다면 명세와 전혀 다른 방향으로 작업을 수행하게 된다.</li>
<li><strong>불완전성 및 모호성 (Incompleteness &amp; Vagueness):</strong> 명세 자체가 불충분하거나 수치화할 수 없는 추상적인 개념(‘fast’, ‘cheap’, ‘user-friendly’ 등)을 포함하는 경우다. 이러한 개념은 평가 기준이 정량화되지 않았기 때문에, 결정론적 정답지를 구성할 때 어노테이터(Annotator) 간의 심각한 의견 불일치를 초래한다.</li>
</ol>
<p><img src="./3.6.0.0.0%20%EB%AA%A8%ED%98%B8%EC%84%B1Ambiguity%20%EC%B2%98%EB%A6%AC%EC%99%80%20%EC%98%88%EC%99%B8%20%EA%B4%80%EB%A6%AC.assets/image-20260222195010009.jpg" alt="image-20260222195010009" /></p>
<p>위와 같은 모호성들은 모델 내부에서 두 가지 형태의 불확실성으로 발현된다. 하나는 질의 자체가 본질적으로 여러 의미를 품고 있어 발생하는 **외연적 불확실성(Denotational Uncertainty)**이며, 다른 하나는 모델이 지식 부족이나 훈련 데이터의 편향으로 인해 어떤 답변을 해야 할지 결정하지 못하는 **인식론적 불확실성(Epistemic Uncertainty)**이다. 결정론적 정답지를 구축할 때는 주어진 텍스트나 코드가 인간 전문가들 사이에서도 의견 일치(Inter-Annotator Agreement, IAA)를 이끌어낼 수 있는 명확한 상태인지 점검해야 하며, 합의가 불가능한 고도의 화용론적 모호성을 지닌 데이터는 훈련 및 평가 데이터셋에서 배제하거나 조건부 분기 논리를 명시해야 한다.</p>
<h2>2.  오라클 문제를 유발하는 모호성의 근본 원인 (The Oracle Problem)</h2>
<p>소프트웨어 테스팅 분야에서 가장 해결하기 어려운 과제 중 하나는 이른바 **오라클 문제(The Oracle Problem)**이다. 널리 인용되는 논문 <em>The Oracle Problem in Software Testing: A Survey</em> (Barr et al., 2015)에 따르면, 테스팅은 시스템의 잠재적 결함을 발견하기 위해 동작을 검사하는 과정이며, 입력이 주어졌을 때 시스템의 올바른 기대 동작과 잠재적으로 잘못된 동작을 구별하는 메커니즘이 바로 테스트 오라클이다. 그러나 모호성이 높거나 비결정적인 시스템에서는 정답을 사전에 정의하는 것이 불가능하거나 비용이 지나치게 많이 든다.</p>
<p>기존의 전통적인 소프트웨어는 수학적 연산이나 데이터베이스 조회와 같이 입력 <span class="math math-inline">x</span>에 대해 출력 <span class="math math-inline">y</span>가 수학적으로 완벽히 매핑되는 구조를 띠었다. 그러나 AI 시스템, 특히 딥러닝 기반 소프트웨어 시스템(DLS)이나 LLM에서는 모델이 확률적 가중치를 기반으로 추론을 수행하기 때문에 동일한 프롬프트(입력)를 제공하더라도 미세한 노이즈나 온도(Temperature) 파라미터의 설정에 따라 다른 텍스트나 코드를 출력하게 된다.</p>
<p>이러한 오라클 문제를 심화시키는 주요 원인은 다음과 같다.</p>
<p>첫째, <strong>의사 랜덤성(Pseudo-randomness)과 하드웨어 스레딩의 비결정성</strong>이다. LLM은 다음 토큰을 예측할 때 확률 분포에서 샘플링을 수행하며, 심지어 Temperature를 0으로 설정하여 탐욕적 디코딩(Greedy Decoding)을 강제하더라도 GPU의 부동소수점 연산 순서 차이로 인해 미세한 비결정성이 발생할 수 있다. 이는 엄격한 문자열 일치(String matching)를 요구하는 원자적 오라클(Atomic Oracle)을 무력화시킨다.</p>
<p>둘째, **보이지 않는 전역 상태(Hidden Global State)와 사이드 이펙트(Side Effects)**다. 시스템 프롬프트의 변경, 검색 증강 생성(RAG) 파이프라인에서 검색된 문서의 순위 변동, 혹은 시스템 시간에 종속된 로직 등은 입력이 동일하더라도 실행 환경의 컨텍스트를 변화시켜 출력의 모호성을 가중시킨다.</p>
<p>셋째, **인간 피드백의 주관성(Subjectivity of Human Feedback)**이다. 정답지를 구축하는 과정에서 어노테이터들이 갖는 문화적, 전문적 배경의 차이는 동일한 데이터에 대해 다른 라벨링 결과를 낳는다. 예를 들어 “이 코드는 가독성이 좋은가?” 또는 “이 답변은 유용한가?“라는 질문은 다분히 주관적이므로 결정론적인 오라클로 검증하기 어렵다. 결과적으로 오라클 문제는 단순히 자동화 도구의 부재가 아니라, 지식의 표현과 평가 자체가 지니는 인식론적 한계에서 비롯된다.</p>
<p>이러한 오라클 문제를 해결하기 위해서는 완벽한 정답 명세(Full specification)를 요구하는 기존의 접근 방식을 탈피하여, 부분적 오라클(Partial Oracle), 의사 오라클(Pseudo Oracle), 휴리스틱 검증, 그리고 후술할 변성 테스트(Metamorphic Testing)와 같은 우회적이고 통계적인 기법들을 파이프라인에 통합해야 한다.</p>
<h2>3.  정답지 구축을 위한 모호성 자동 감지 및 불확실성 보정</h2>
<p>AI 기반 QA 시스템이나 코드 생성 에이전트가 엔터프라이즈 환경에서 신뢰성을 갖추려면, 자동화된 오라클은 모델이 생성한 답변의 ’정확성’뿐만 아니라 질문 자체에 내재된 ’모호성’을 모델이 스스로 어떻게 다루고 인식하는지 평가해야 한다. 질문이 불완전할 때 AI가 무리하게 추론하여 답변을 생성하는 행위는 치명적인 데이터 환각의 근본적인 원인이다.</p>
<p>최근 자연어 처리 및 기계학습 분야에서는 LLM이 입력된 프롬프트의 모호성을 선제적으로 감지하고, 이에 따른 불확실성을 캘리브레이션(Calibration)하는 연구가 활발히 진행되고 있다. 논문 <em>Ambiguity Detection and Uncertainty Calibration for Question Answering with Large Language Models</em> (ACL 2025)에서는 저자원(Low-resource) 환경에서도 질문의 모호성을 고정밀로 감지하는 새로운 앙상블 프레임워크를 제안한다. 이 연구는 기존에 널리 쓰이던 방식, 즉 LLM에게 “이 질문이 모호한가? Yes 또는 No로 대답하라“라고 직접 묻는 이진법적 프롬프팅(Binary prompting)이 실제로는 무작위 추측(Random guessing) 수준에 불과하다는 한계를 지적한다. 대신, 모델의 출력 분포(Output distribution)를 통계적으로 분석하여 모호성을 역산(Infer)하는 기법을 도입한다.</p>
<p>구체적인 모호성 감지 메커니즘은 다음과 같은 단계로 구성된다.</p>
<ol>
<li><strong>다중 답변 생성 (Multiple Answer Generation):</strong> 온도(Temperature) 파라미터를 높여 자가 일관성(Self-consistency) 프롬프팅 기법을 적용한다. 이를 통해 하나의 질문(예: RAG 시스템에서 검색된 문맥과 사용자 쿼리)에 대해 LLM이 여러 개의 독립적인 답변 샘플을 생성하도록 유도한다.</li>
<li><strong>엔트로피 및 분산 측정 (Entropy &amp; Variance Measurement):</strong> 생성된 답변들 간의 의미론적 다양성과 엔트로피(Entropy)를 계산한다. 만약 질문이 명확하고 LLM이 단일한 해석에 확신을 가진다면, 생성된 여러 답변의 텍스트는 내용상 거의 일치할 것이며 이때 계산된 엔트로피는 <span class="math math-inline">0</span>에 수렴한다. 반면, 질문이 구조적 또는 의미론적으로 모호하여 여러 해석이 가능하다면, 모델은 매 샘플링마다 상이한 답변(예: ’bank’를 하천으로 해석한 답변과 금융기관으로 해석한 답변)을 내놓을 것이며, 결과적으로 엔트로피가 <span class="math math-inline">1</span>을 향해 급격히 증가한다.</li>
<li><strong>랜덤 포레스트(Random Forest) 분류기를 통한 판별:</strong> 단순히 엔트로피만 측정하는 것을 넘어, LLM 응답 패턴에서 추출한 다양한 분포적 특성(Distributional features)을 부트스트래핑(Bootstrapping) 기법으로 훈련된 경량 머신러닝 분류기(예: Random Forest)에 주입한다. 이 접근법은 BERT 계열의 단순 분류기나 LLM의 자가 해석보다 훨씬 뛰어난 성능(최대 70.8%의 정확도)으로 쿼리의 모호성을 식별해낸다.</li>
</ol>
<p>코드 생성 분야에서도 이와 매우 유사한 수학적 접근이 활용된다. 불완전한 자연어 명세(Utterance <span class="math math-inline">U</span>)가 주어졌을 때, 모델의 출력 분포 <span class="math math-inline">\mathbb{C}(U)</span> 내에 존재하는 완전히 구문이 다른 프로그램(Abstract Syntax Tree 단위 비교)의 개수를 세어 모호성을 측정하는 <strong>샘플링 다양성(Sampling Diversity, SD)</strong> 지표가 대표적이다. 또한 생성된 여러 코드 해결책 전반에서 특정 함수 호출이나 매개변수들이 얼마나 동일하게 유지되는지를 측정하는 <strong>반복 매개변수 카운팅(Repeated Parameter Counting, RPC)</strong> 지표를 통해, 사용자가 명시하지 않은 암묵적인 전제(Presuppositional ambiguity)로 인한 모호성을 정량화할 수 있다.</p>
<p>결정론적 정답지를 기반으로 작동하는 테스트 오라클은 이러한 지표들을 시스템 파이프라인의 전처리 단계에 결합해야 한다. 모호성 점수나 엔트로피가 미리 정의된 임계치(Threshold)를 초과하는 입력이 들어오면, 시스템은 즉시 결정론적 실행을 멈추고 예외를 발생시키거나 인간 전문가의 검토(Human-in-the-Loop) 채널로 라우팅하는 강건한 제어 흐름(Control flow)을 구축해야 한다.</p>
<h2>4.  기권(Abstention)과 거절(Refusal) 메커니즘의 오라클 설계</h2>
<p>결정론적 정답지가 AI 시스템의 ’올바른 행동’을 정의할 때, 정답을 맞히는 것만큼이나 중요한 것이 바로 “모르겠다“고 답하거나 답변을 명시적으로 회피하는 기능이다. AI 모델이 정보의 부족이나 문맥의 모호성 앞에서도 억지로 정답을 만들어내려 할 때 가장 심각한 논리적 오류가 발생한다. 이를 제어하기 위해 시스템 오라클은 모델의 **기권(Abstention)**과 <strong>거절(Refusal)</strong> 행동을 엄격히 평가하고 통제할 수 있어야 한다.</p>
<p>학술적으로 ’기권’과 ’거절’은 시스템의 예외 관리 측면에서 매우 다르고 고유한 목적을 지닌다.</p>
<ul>
<li><strong>거절 (Refusal):</strong> 모델이 답변에 필요한 지식을 가지고 있거나 충분히 생성할 수 있음에도 불구하고, 안전 정책(Safety policy), 유해성 필터(Toxicity), 저작권 등의 제약 조건에 의해 고의로 출력을 차단하는 방어적 메커니즘이다.</li>
<li><strong>기권 (Abstention):</strong> 프롬프트가 과도하게 모호하거나(Underspecification), RAG 시스템에서 검색된 문헌 내에 답변을 도출할 확고한 근거가 존재하지 않을 때(Unanswerable), 모델 스스로 자신의 지식 한계나 인식론적 불확실성을 인지하고 판단을 유보하는 메커니즘이다.</li>
</ul>
<p>소프트웨어 테스트에서 모델의 평가 및 오라클 구성을 어렵게 만드는 핵심 요인은 바로 오탐지적 거절(False Refusal Rate)과 답변 시 환각(Hallucination Rate when not refused) 사이의 상충 관계(Trade-off)다. 모델의 정답지에 기권의 비중을 과도하게 높여 보수적으로 미세 조정(Fine-tuning)하면 환각은 줄어들지만, 충분히 대답할 수 있는 명확한 질문에 대해서도 지나치게 답변을 기권하여 전반적인 정답 제공률(Correct Answer Rate)이 급락하는 부작용을 낳는다. 반대로 예외 관리를 느슨하게 설정하면 환각이 폭증하여 시스템이 출력하는 데이터의 신뢰성이 완전히 붕괴된다.</p>
<p><img src="./3.6.0.0.0%20%EB%AA%A8%ED%98%B8%EC%84%B1Ambiguity%20%EC%B2%98%EB%A6%AC%EC%99%80%20%EC%98%88%EC%99%B8%20%EA%B4%80%EB%A6%AC.assets/image-20260222195039670.jpg" alt="image-20260222195039670" /></p>
<p>이러한 딜레마를 해결하기 위해 최신 오라클 아키텍처는 단순한 텍스트 일치 여부(Accuracy)를 넘어서는 차세대 평가 프레임워크를 도입하고 있다. 논문 <em>RefusalBench</em> 및 지식 탐침(Knowledge Probing) 관련 연구들은 선택적 기권(Selective Abstention) 능력을 평가하기 위해, 기존의 혼동 행렬(Confusion Matrix) 개념을 확장하여 정밀도(Precision)와 재현율(Recall)을 재정의할 것을 제안한다.</p>
<p>구체적인 오라클 평가 매트릭스는 다음과 같이 구성된다 :</p>
<ul>
<li><strong><span class="math math-inline">TP</span> (True Positive):</strong> 모델이 질문에 대해 답변할 수 있는 지식이 있다고 올바르게 판단하여 답변했고, 그 내용이 결정론적 정답지와 정확히 일치한 경우.</li>
<li><strong><span class="math math-inline">FP</span> (False Positive):</strong> 모델이 안다고 판단하여 자신 있게 답변했으나, 정답지와 불일치하거나 지어낸 정보(환각)인 경우. 가장 위험한 실패 모드다.</li>
<li><strong><span class="math math-inline">TN</span> (True Negative):</strong> 제공된 문맥 내에 정답이 없거나 질문이 모호하여, 모델이 올바르게 한계를 인지하고 적절히 기권(“정보가 부족합니다”)한 경우. 오라클은 이를 ’성공적인 예외 처리’로 간주하여 높은 점수를 부여해야 한다.</li>
<li><strong><span class="math math-inline">FN</span> (False Negative):</strong> 실제로 시스템이 정답을 도출할 수 있음에도 불구하고, 과도한 안전 장치나 잘못된 불확실성 측정으로 인해 불필요하게 기권을 선언한 경우(오탐지적 거절).</li>
</ul>
<p>결정론적 정답지를 활용하는 자동화된 오라클 파이프라인은 위 4가지 상태를 동적으로 판별해야 한다. 더욱 진보된 방법론에서는 기권을 단순히 텍스트 생성 후의 사후 필터링(Post-hoc filter)이나 정규식 기반 검사로 처리하는 것을 지양한다. 대신 생성 게이팅(Generation Gating) 과정 자체에 <code>&lt;ABSTAIN&gt;</code>이라는 명시적인 제어 토큰을 포함시키고, 훈련 과정에서 이 토큰의 발현 확률을 최적화함으로써 기권 행위를 모델의 네이티브 스킬(Native skill)로 내재화하는 아키텍처가 제안되고 있다. 특히 엔터프라이즈 환경의 RAG 시스템에서는 검색된 문서 집합 <span class="math math-inline">D={d_1,..., d_n}</span>에 대한 사용자 질의 <span class="math math-inline">x</span>의 평균 관련성 점수 <span class="math math-inline">R(x, d)</span>를 계산하고, 이 값이 하드코딩된 임계값을 넘지 못할 경우 모델의 개입 없이 선제적으로 기권 프로세스를 호출하도록 하는 시스템 레벨의 결정론적 예외 처리가 구현되어야 한다.</p>
<h2>5.  변성 테스트(Metamorphic Testing)를 활용한 모호성 극복</h2>
<p>지금까지 살펴본 모호성이나 예외 상황들은 많은 경우 ‘정확한 예상 결과(Expected Output)’ 자체를 사전에 절대적으로 정의하는 것을 불가능하게 만든다. 복잡한 수치 계산, 딥러닝 기반의 영상 인식, 비정형 언어 번역, 그리고 LLM의 에이전트 자율 행동에 이르기까지, 특정 입력에 대한 완벽한 정답이 존재하지 않거나 이를 획득하는 비용이 천문학적으로 높은 현상을 일컬어 소프트웨어 공학에서는 앞서 언급한 **오라클 문제(The Oracle Problem)**라고 칭한다.</p>
<p>완벽한 결정론적 정답지 구축이 원천적으로 불가능한 도메인에서, 내재된 모호성을 통제하고 AI 모델의 결함을 논리적으로 검증하기 위해 도입된 가장 강력한 대안 중 하나가 **변성 테스트(Metamorphic Testing, MT)**이다. T.Y. Chen 등에 의해 고안된 이 접근법은 개별 입력값에 대한 특정한 절대 출력값을 검증하는 방식을 탈피한다. 그 대신 “입력값이 특정한 방식으로 변화(Transformation)했을 때, 출력값 역시 그에 상응하는 논리적이고 수학적인 관계(Metamorphic Relation, MR)를 유지하며 변화해야 한다“는 핵심 속성을 검증 오라클로 활용한다.</p>
<p>AI 모델의 예외 및 모호성 처리에 변성 테스트를 적용하는 메커니즘은 다음과 같은 절차로 수행된다:</p>
<ol>
<li><strong>원시 테스트 케이스(Source Test Case) 실행:</strong> AI 모델에 기준이 되는 입력 <span class="math math-inline">x</span>를 제공하여 출력 <span class="math math-inline">f(x)</span>를 얻는다. 이 단계에서 중요한 점은 <span class="math math-inline">f(x)</span>가 100% 정답인지 여부를 평가자가 알 필요가 없다는 것이다.</li>
<li><strong>변성 관계(Metamorphic Relation, MR) 정의 및 후속 입력 합성:</strong> 도메인 전문가의 지식을 활용하여 논리적 일관성을 해치지 않으면서 모호성을 유발할 수 있는 입력을 합성한다. 예를 들어, 기계 번역에서 문장의 능동태를 수동태로 바꾸거나, RAG 기반 Q&amp;A 시스템에서 질문에 무의미한 부사를 추가할 수 있다. 딥러닝 이미지 모델의 경우 자율 주행 AI 시스템에 날씨 효과(비, 안개) 필터를 씌우거나 사진을 180도 회전시킨다. 이렇게 의도적으로 변형된 입력을 후속 테스트 케이스(Follow-up test case) <span class="math math-inline">x&#39;</span>라고 한다.</li>
<li><strong>결과 비교 및 관계 검증(Verification of Output Relation):</strong> 시스템 오라클은 기존 출력 <span class="math math-inline">f(x)</span>와 새로운 출력 <span class="math math-inline">f(x&#39;)</span> 사이의 관계를 평가한다. 이미지를 약간 회전시켰음에도 동일한 객체로 인식해야 하거나, 문장의 구조가 능동/수동으로 바뀌었어도 응답이 내포하는 의미론적 유사도(Semantic Similarity)가 일관되게 유지되어야 한다는 절대적 속성이 만족하는지 확인한다.</li>
</ol>
<p>이 방식은 정답이 본질적으로 모호한 상황에서도 결정론적이고 완전 자동화된 검증을 가능하게 한다. 최신 연구 사례에 따르면, AI 기반 브라우저 익스텐션 시스템(예: 번역, 텍스트 요약)의 취약점을 테스트할 때 변성 관계 위반(MR Violations)을 오라클로 삼아 테스트한 결과, 수백 건의 숨겨진 보안 취약점과 콘텐츠 정렬 결함을 성공적으로 식별해냈다. 변성 테스트는 입력과 출력 간의 절대적 관계성에 의존하므로, 전통적인 명세 기반 오라클(Specified Oracle)을 구축할 수 없는 한계를 보완하는 강력한 파생 오라클(Derived/Partial Oracle)로서 LLM의 일관성과 강건성을 통계적으로 증명하는 데 핵심적인 역할을 수행한다.</p>
<h2>6.  대형 언어 모델의 오라클 기반 프로그램 선택 로직 (Oracle-Guided Program Selection)</h2>
<p>코드 생성과 같은 복잡한 추론 작업에서 발생하는 모호성을 처리하기 위해, 자연어 명세를 이용해 LLM 자체를 오라클로 활용하는 하이브리드 기법이 부상하고 있다. ISSTA 2024에 발표된 논문 <em>Oracle-Guided Program Selection from Large Language Models</em> (Fan et al.)은 LLM이 생성한 코드가 지닌 비결정성과 모호성을 극복하기 위한 혁신적인 프레임워크를 제시한다.</p>
<p>기존의 코드 생성 AI는 사용자의 자연어 프롬프트가 내포한 어휘적, 구조적 모호성 때문에 정확한 의도를 파악하지 못하고 다수의 결함 있는 프로그램을 무작위로 생성하는 경향이 있었다. 이를 완화하기 위해 AlphaCode나 CodeT 같은 이전 시스템들은 여러 코드 샘플을 대량으로 생성한 뒤, LLM이 임의로 생성한 테스트 케이스를 통과하는 코드들을 묶어 다수결(Majority vote)로 정답을 채택하는 방식을 사용했다. 그러나 이는 LLM이 생성한 테스트 케이스 자체가 높은 모호성을 지니고 중복되며, 정답과 오답을 판별할 변별력이 떨어진다는 치명적인 한계를 가졌다.</p>
<p>해당 논문은 LLM이 백지상태에서 처음부터 올바른 코드를 ’생성(Generation)’하는 데는 취약하지만, 여러 대안 중에서 올바른 출력을 ’선택(Selection)’하는 객관식 평가 능력(LLM-as-a-Judge)은 인간 수준에 필적할 만큼 뛰어나다는 점에 착안하였다. 이에 기반하여 제시된 <strong>오라클 유도형 프로그램 선택(Oracle-Guided Program Selection)</strong> 프로세스는 다음과 같다 :</p>
<ol>
<li><strong>차별화 입력(Distinguishing Inputs) 도출:</strong></li>
</ol>
<p>무작위로 테스트 케이스를 쏟아내는 대신, 차이 기반 테스트(Differential testing) 기법을 활용하여 생성된 수많은 프로그램 샘플들 간의 실행 결과가 명확히 엇갈리는 특정한 엣지 케이스 입력값(Distinguishing inputs)을 찾아낸다.</p>
<ol start="2">
<li><strong>프로그램 클러스터링(Program Clustering):</strong></li>
</ol>
<p>도출된 차별화 입력값에 대해 동일한 결과를 내놓는 프로그램들을 의미론적 등가 클래스(Equivalence class)로 묶어 클러스터링한다. 이를 통해 개별 코드가 아닌 논리적 그룹 단위의 검증이 가능해진다.</p>
<ol start="3">
<li><strong>LLM 오라클 인퍼런스(LLM as a Test Oracle):</strong></li>
</ol>
<p>원래 제공되었던 자연어 문제 명세(Task description)를 바탕으로, 각 차별화 입력에서 어떤 출력값이 논리적으로 가장 타당한지 LLM에게 다중 선택 질문(Multiple-choice question)을 던진다. 여기서 LLM은 생각의 사슬(Chain-of-Thought) 프롬프팅을 거쳐 올바른 출력을 지정하는 결정론적 오라클 역할을 수행한다.</p>
<ol start="4">
<li><strong>결함 필터링 및 선택:</strong></li>
</ol>
<p>오라클이 지목한 출력값과 일치하지 않는 행동을 보이는 프로그램 클러스터는 즉시 배제되며, 이 과정을 반복하여 최종적으로 가장 정확도가 높고 명세에 부합하는 단일 프로그램을 선택한다.</p>
<p>이 접근법은 HumanEval과 MBPP 벤치마크 테스트에서 코드의 통과율(pass@1)을 기존 대비 3.6%~7% 향상시켰을 뿐만 아니라, 생성 토큰 비용 측면에서도 60% 이상의 효율을 보였다. 그러나 가장 주목할 만한 학술적 성과는, 이 과정에서 추출된 엣지 케이스의 입력-출력 명세(I/O specifications)가 원래 사용자가 작성했던 자연어 요구사항의 ’불완전성(Incompleteness)’과 ’모호성(Ambiguity)’을 역으로 폭로(Uncover)했다는 점이다.</p>
<p>즉, 모호성이 섞인 프롬프트를 처리하는 예외 관리 메커니즘 자체가, 인간 작업자에게 프롬프트의 논리적 허점을 피드백하고 심지어 권위 있는 벤치마크 데이터셋에 내재된 잘못된 정답지(Ground truth errors)를 식별하여 교정하는 매우 강력한 감사 도구(Auditing tool)로 기능한 것이다.</p>
<h2>7.  AI 에이전트 워크플로우의 단계별 예외 관리</h2>
<p>단일 프롬프트-응답(Single-turn) 구조를 넘어서, 여러 도구를 연쇄적으로 호출하고 메모리를 관리하며 다단계 작업을 수행하는 에이전틱(Agentic) AI 워크플로우에서는 예외 관리가 전통적 소프트웨어 트랜잭션 처리 이상으로 복잡해진다. 기존의 결정론적 소프트웨어에서 예외(Exception)란 주로 구문 오류, Null Pointer 참조, Timeout과 같은 런타임 기술 오류를 의미했으며 <code>try-catch</code> 블록으로 복구가 가능했다. 반면 LLM 에이전트 시스템에서 발생하는 예외는 비선형적이며 고도로 맥락 의존적이다.</p>
<p>가장 까다로운 문제는 코드 컴파일이나 API 호출과 같은 ’실행 단계(Execution Phase)’에서 겉으로 드러난 시스템 오류의 근본 원인이, 사실상 그 이전 단계인 ’추론 및 계획 단계(Reasoning/Planning Phase)’에서 발생한 환각이나 모호한 목표 설정에 기인한다는 점이다. 논문 <em>A Structured Exception Handling Approach for LLM-Driven Agentic Workflows (SHIELDA)</em> 에 따르면, 이러한 자율형 에이전트 환경의 예외는 원인 파악과 복구를 위해 반드시 3단계 계층 구조로 관리되어야 한다.</p>
<ol>
<li><strong>로컬 핸들링 및 실행 예외 감지 (Phase 1: Initial Execution Exception and Local Handling):</strong> AI가 외부 데이터베이스나 API 도구를 호출했을 때, 반환된 JSON 스키마의 형식이 불일치하거나 매개변수 타입 오류, 권한 거부가 발생하는 형태의 기술적 예외다. 이는 즉각적인 구문 분석 검증(Syntax parsing validation) 오라클을 통해 즉시 감지되며, 에이전트 내부의 로컬 재시도 로직(Retry logic)과 지수적 백오프(Exponential backoff)를 통해 1차적 해결을 시도한다.</li>
<li><strong>교차 단계 근본 원인 분석 (Phase 2: Escalation and Cross-Phase Root Cause Analysis):</strong> 단순한 로컬 재시도가 임계치를 초과하여 실패하고 예외가 에스컬레이션(Escalation)되면, 시스템은 예외의 원인이 외부 환경이 아닌 에이전트 자체의 논리적 결함에 있음을 인지해야 한다. 시스템은 사용자의 초기 지시가 모호했는지(Ambiguous Goal), 충돌하는 다중 목표가 포함되었는지(Conflicting Goal), 혹은 에이전트가 애초에 존재하지 않는 도구를 환각하여 호출했는지(Hallucinated Tool Calls)를 식별한다. 이 과정에는 LLM-as-a-Judge나 정적 분석 기반의 오라클이 투입된다.</li>
<li><strong>계획 복구 및 상태 롤백 (Phase 3: Plan Repair and Controlled Recovery):</strong> 오류의 근본 원인이 파악되면, 에이전트의 메모리와 프롬프트 컨텍스트를 오염되지 않은 이전의 안정적인 상태로 롤백(State Recovery)해야 한다. 이후 수정된 피드백이나 명확화된 목표(Clarification)를 주입하여 계획을 재수립(Plan Repair)하도록 유도한다. 만약 에이전트가 모호성을 해결하지 못하고 동일한 환각이나 오류를 반복하는 무한 루프(Infinite loop)에 빠질 경우, 시스템은 강제로 토큰 생성을 중단하고 로그와 함께 인간 작업자에게 제어권을 넘기는 결정론적 안전 장치(Circuit breaker / Kill-switch)를 즉각 작동해야 한다.</li>
</ol>
<p>이처럼 복잡한 AI 소프트웨어 워크플로우를 통제하는 결정론적 오라클은 단순히 최종 출력값의 텍스트 일치 여부만을 검사하는 ’원자적 오라클(Atomic Oracle)’을 넘어서야 한다. 실행 궤적(Execution trace) 전체의 건전성을 확인하고 여러 차례의 롤백과 시도 전반에 걸쳐 통계적으로 시스템 상태를 평가하는 ’집계적 오라클(Aggregated Oracle)’로 진화해야만 예측 불가능한 예외의 연쇄 작용을 막을 수 있다.</p>
<h2>8.  인간-AI 협업 기반의 예외 심사 및 골든 데이터셋 정제</h2>
<p>AI 시스템 내부의 자가 보정, 변성 테스트, 정적 분석 기반의 오라클만으로는 해석의 여지가 넓은 비즈니스 로직이나 고도의 맥락적 전문성이 요구되는 법률, 의료 등의 전문 도메인 예외를 완벽히 통제할 수 없다. 궁극적으로 AI 소프트웨어를 위한 결정론적 정답지, 즉 **골든 데이터셋(Golden Dataset)**의 순도와 신뢰성을 유지하기 위해서는 예외 상황을 체계적으로 분류하고 인간 전문가의 심사(Adjudication)를 거쳐 이를 다시 정답지로 피드백하는 구조화된 루프가 반드시 필요하다.</p>
<p><strong>골든 데이터셋의 역할과 예외 통합</strong> 골든 데이터셋은 단순한 모델 훈련용 말뭉치를 넘어, 조직 내에서 ’무엇이 절대적으로 올바른 행동인가’에 대한 가장 권위 있는 합의(Consensus)를 담은 벤치마크다. 앞선 3.6.7절에서 논의된 ’실행 중 실패한 에이전트의 궤적(Failed Traces)’이나, 3.6.3절에서 ’엔트로피가 극단적으로 높아 모호한 것으로 판별된 질의’들은 단순히 에러 로그로 폐기되어서는 안 된다. 이들은 현업에서 발생하는 가장 가치 있는 엣지 케이스(Edge case)이자 예외 시나리오이므로, 엄격한 리뷰를 거쳐 골든 데이터셋에 편입되어야 한다. 예를 들어, 사용자가 “내 비밀번호를 재설정해 줘“라고 입력했을 때, 과거의 낡은 정답지가 단순히 “이메일 링크를 확인하세요“라는 텍스트 매칭에 불과했다면, 고도화된 예외 관리 기반의 정답지는 “계정 유형과 다중 인증(MFA) 활성화 여부를 먼저 사용자에게 되묻는다“는 구체적인 행동 지침(Behavioral expectation)과 시스템의 상태 변화까지 JSON 구조로 캡처하도록 진화해야 한다.</p>
<p><strong>인간-AI 검토 프로토콜 (Human-in-the-Loop Validation Protocol)</strong> 이러한 복잡한 예외 데이터들을 모호성 없이 결정론적 정답지로 전환하기 위해 아래와 같은 심사 프로토콜이 권장된다.</p>
<ul>
<li><strong>교차 검증 (Cross-validation):</strong> 단일 작업자의 편향을 방지하기 위해 도메인 전문가(SME)들이 독립적으로 모호한 데이터를 평가한다. 의료 이미지의 종양 경계 획정이나 사용자 피드백의 감성 분석처럼 미묘한 뉘앙스 차이로 정답이 엇갈리기 쉬운 주관적 데이터는 반드시 다수의 검토자 평가를 병렬로 수집해야 한다.</li>
<li><strong>일치도 측정 (Inter-Annotator Agreement, IAA):</strong> Cohen’s Kappa 또는 Krippendorff’s Alpha와 같은 통계적 기법을 통해 평가자 간 합의 수준을 수치화한다. IAA 점수가 낮다면 이는 검토자의 역량 부족이 아니라 지시 사항(Annotation guidelines) 자체가 근본적으로 모호하다는 강력한 증거이므로, 데이터를 버리기 전에 가이드라인의 구조적 불완전성을 먼저 수정해야 한다.</li>
<li><strong>LLM 판사(LLM-as-a-judge)와의 하이브리드 조합:</strong> 시스템에서 발생하는 모든 예외와 모호성을 인간이 전수 검토하는 것은 시간과 비용의 한계가 명확하다. 따라서 명확한 구조적 오류(예: JSON 형식 위반, 필수 매개변수 누락, 출처 URL 누락)는 LLM 판사나 정적 스크립트가 자동 필터링하고, LLM조차 확신하지 못하는 데이터(신뢰도 임계치 미달, 논리적 충돌)만을 인간 전문가에게 에스컬레이션하는 티어 기반 응답 시스템(Tiered Response Protocol)을 구축하여 검토 효율을 극대화한다.</li>
</ul>
<table><thead><tr><th><strong>오라클 계층 (Oracle Layer)</strong></th><th><strong>주요 검증 지표 및 목표</strong></th><th><strong>예외 처리 주체 (Handler)</strong></th><th><strong>처리 방식 (Action)</strong></th></tr></thead><tbody>
<tr><td><strong>원자적 오라클 (Atomic Oracle)</strong></td><td>구문적 오류, 스키마(JSON) 불일치, 매개변수 누락 여부</td><td>자동화 스크립트 및 정적 구문 분석기</td><td>에이전트의 로컬 재시도 (Local Retry) 강제 및 로그 기록</td></tr>
<tr><td><strong>집계/변성 오라클 (Aggregated/Metamorphic)</strong></td><td>응답의 논리적 일관성, 통계적 환각 여부, 의미론적 모호성 및 엔트로피 초과</td><td>LLM 판사 (LLM-as-a-Judge) 및 변성 테스트 엔진</td><td>컨텍스트 상태 롤백, 계획 수정(Plan Repair) 피드백 제공</td></tr>
<tr><td><strong>인간 심사 (Human Adjudication)</strong></td><td>도메인 지식의 충돌, 화용론적 모호성, 윤리/보안 규제 위반</td><td>도메인 전문가 (Subject Matter Expert)</td><td>교차 검증을 통한 합의 도출, 오라클 규칙 및 골든 데이터셋 갱신</td></tr>
</tbody></table>
<p>위의 표에 나타난 바와 같이 다계층(Multi-tiered) 검증 프레임워크로 구성된 예외 관리 파이프라인은 초기에는 비결정적이고 확률적인 특성을 지닌 AI의 출력을 효과적으로 통제한다. 이 순환 구조를 통해 발견된 예외들이 정답지에 지속적으로 편입됨으로써, 최종적으로는 시스템 전체가 수학적으로 증명 가능한 수준의 결정론적 안전망(Deterministic Guardrails) 안에서 신뢰성 있게 작동하도록 보장한다.</p>
<h2>9.  요약</h2>
<p>결론적으로, AI 시대의 결정론적 정답지 설계는 단순히 완벽하고 깨끗한 데이터를 한 번 수집하여 저장하는 정적인(Static) 작업이 아니다. 그것은 자연어와 현실 세계가 가진 필연적 모호성을 모델의 분포적 불확실성을 통해 수치화하여 감지해내고, 적절한 기권과 거절의 기준을 시스템 훈련 과정에 내재화하며, 에이전트의 실행 중에 발생하는 예외적 궤적을 캡처하는 역동적 프로세스다. 더 나아가 정답이 없는 상황에서도 변성 테스트를 통해 논리적 관계를 검증하고, 다중 에이전트 및 인간의 협력적 검토를 통해 지속적으로 오라클의 기준을 벼려내는 진화하는(Evolutionary) 엔지니어링 실천이다.</p>
<p>이러한 모호성 처리와 예외 관리 체계가 아키텍처의 근간에 자리 잡을 때, 비로소 기업은 ’환각과 오류’의 리스크로부터 벗어나 엄격한 통제와 감사가 가능한 산업용 AI 소프트웨어를 성공적으로 구축할 수 있다.</p>
<p>다음 장인 “3.7 결정론적 정답지 설계 시의 흔한 함정 (Anti-Patterns)“에서는 이러한 체계를 구축하는 과정에서 소프트웨어 개발팀이 빈번하게 범하는 전략적 실수와 안티 패턴들을 구체적인 산업 사례와 함께 심층 분석한다.</p>
<h2>10. 참고 자료</h2>
<ol>
<li>‘Writing code will not be the goal’: Nandan Nilekani on how AI will transform talent and enterprises, https://www.businesstoday.in/technology/story/writing-code-will-not-be-the-goal-nandan-nilekani-on-how-ai-will-transform-talent-and-enterprises-516547-2026-02-17</li>
<li>Software Engineering for LLM Prompt Development - arXiv.org, https://arxiv.org/html/2503.02400v1</li>
<li>The Great Unlearning for AI Builders - Communications of the ACM, https://cacm.acm.org/blogcacm/the-great-unlearning-for-ai-builders/</li>
<li>What Is Ground Truth in Machine Learning? - IBM, https://www.ibm.com/think/topics/ground-truth</li>
<li>using machine learning for automated detection of ambiguity in, https://discovery.ucl.ac.uk/id/eprint/10174754/1/2023_EC3_revised_final.pdf</li>
<li>using machine learning for automated detection of ambiguity in …, https://ec-3.org/publications/conferences/EC32023/papers/EC32023_211.pdf</li>
<li>IS AI GROUND TRUTH REALLY TRUE? THE DANGERS … - NYU Law, <a href="https://www.law.nyu.edu/sites/default/files/Lebovitz%2C%20Levina%2C%20Lifshitz-Assaf%2C%20MISQ%2C%202021%2C%20Published.pdf">https://www.law.nyu.edu/sites/default/files/Lebovitz%2C%20Levina%2C%20Lifshitz-Assaf%2C%20MISQ%2C%202021%2C%20Published.pdf</a></li>
<li>The Impossibility of Automating Ambiguity, https://www.upaya.org/wp-content/uploads/2022/05/Impossibility-of-Automating-Ambiguity.pdf</li>
<li>A Taxonomy of Prompt Defects in LLM Systems - arXiv.org, https://arxiv.org/html/2509.14404v1</li>
<li>A Benchmark and Dataset for Conditional Ambiguous Question, https://aclanthology.org/2025.emnlp-main.115.pdf</li>
<li>Ambiguity Detection and Uncertainty Calibration for Question, https://aclanthology.org/2025.trustnlp-main.4.pdf</li>
<li>arXiv:2207.10495v2 [cs.SE] 8 Sep 2023, https://arxiv.org/pdf/2207.10495</li>
<li>Discover about “Ground Truth” in Data Science and AI - Innovatiana, https://www.innovatiana.com/en/post/ground-truth-in-ai</li>
<li>(PDF) The Oracle Problem in Software Testing: A Survey, https://www.researchgate.net/publication/276255185_The_Oracle_Problem_in_Software_Testing_A_Survey</li>
<li>The Oracle Problem in Software Testing: A Survey - IEEE Xplore, https://ieeexplore.ieee.org/iel7/32/7106034/06963470.pdf</li>
<li>What are non-deterministic AI outputs? - Statsig, https://www.statsig.com/perspectives/what-are-non-deterministic-ai-outputs-</li>
<li>The Challenges of Testing in a Non-Deterministic World, https://www.sei.cmu.edu/blog/the-challenges-of-testing-in-a-non-deterministic-world/</li>
<li>Deterministic AI Architecture: Why They Matter and How to Build Them, https://www.kubiya.ai/blog/deterministic-ai-architecture</li>
<li>The Deterministic Problem with Probabilistic AI Analytics - Dev.to, https://dev.to/gigapress/the-deterministic-problem-with-probabilistic-ai-analytics-1n2</li>
<li>The Importance of Ground Truth Data in AI Applications: An Overview, https://blog.mozilla.ai/the-importance-of-ground-truth-data-in-ai-applications-an-overview/</li>
<li>Test oracle - Wikipedia, https://en.wikipedia.org/wiki/Test_oracle</li>
<li>Aligning Language Models to Explicitly Handle Ambiguity - arXiv, https://arxiv.org/html/2404.11972v3</li>
<li>Identifying &amp; Interactively Refining Ambiguous User Goals for Data, https://arxiv.org/html/2510.09390v1</li>
<li>10 Essential Practices for Testing AI Systems in 2025 - Testmo, https://www.testmo.com/blog/10-essential-practices-for-testing-ai-systems-in-2025/</li>
<li>Machine Learning, Ground Truth, and the State of Exception, https://academicworks.cuny.edu/cgi/viewcontent.cgi?article=6442&amp;context=gc_etds</li>
<li>Do I Really Know? Learning Factual Self-Verification for … - arXiv.org, https://arxiv.org/html/2602.02018v1</li>
<li>HalluLens: LLM Hallucination Benchmark - arXiv, https://arxiv.org/html/2504.17550v1</li>
<li>(PDF) Learning to Say “I Don’t Know”: A Vision for Abstention in, https://www.researchgate.net/publication/394496855_Learning_to_Say_I_Don’t_Know_A_Vision_for_Abstention_in_Large_Language_Models</li>
<li>(PDF) RefusalBench: Generative Evaluation of Selective Refusal in, https://www.researchgate.net/publication/396462277_RefusalBench_Generative_Evaluation_of_Selective_Refusal_in_Grounded_Language_Models</li>
<li>Evaluating AI agents: Real-world lessons from building … - AWS, https://aws.amazon.com/blogs/machine-learning/evaluating-ai-agents-real-world-lessons-from-building-agentic-systems-at-amazon/</li>
<li>Golden Datasets: The Foundation of Reliable AI Evaluation - Medium, https://medium.com/@federicomoreno613/golden-datasets-the-foundation-of-reliable-ai-evaluation-486ce97ce89d</li>
<li>A Unified Retrieval Agent-Based System for Ambiguous and, https://aclanthology.org/2025.findings-ijcnlp.27.pdf</li>
<li>Building Trust in AI: A Comprehensive Guide to Quality, Accuracy, https://medium.com/@ajayverma23/building-trust-in-ai-a-comprehensive-guide-to-quality-accuracy-and-evaluation-frameworks-for-26dead649b5e</li>
<li>How effectively does metamorphic testing alleviate the oracle, https://vuir.vu.edu.au/33046/1/TSEmt.pdf</li>
<li>Metamorphic Relation Generation: State of the Art and Visions for, https://arxiv.org/html/2406.05397v2</li>
<li>What is Metamorphic Testing of AI? - testRigor, https://testrigor.com/blog/what-is-metamorphic-testing-of-ai/</li>
<li>Test machine learning the right way: Metamorphic relations. - Lakera, https://www.lakera.ai/blog/metamorphic-relations-guide</li>
<li>How Effectively Does Metamorphic Testing Alleviate the Oracle, https://www.computer.org/csdl/journal/ts/2014/01/06613484/13rRUNvya2K</li>
<li>Real-Time Automated Awareness and Handling of Exceptions in, https://www.researchgate.net/publication/388737771_Real-Time_Automated_Awareness_and_Handling_of_Exceptions_in_Automated_Driving_System</li>
<li>Testing AI Applications: How to Validate Non-Deterministic Outputs, https://navanathjadhav.medium.com/testing-ai-applications-how-to-validate-non-deterministic-outputs-3c02c086e567</li>
<li>ASSURE: A Metamorphic Testing Framework for AI-Powered, https://arxiv.org/html/2507.05307v1</li>
<li>Metamorphic Testing of Large Language Models for Natural … - arXiv, https://www.arxiv.org/pdf/2511.02108</li>
<li>Oracle-guided Program Selection from Large Language Models, https://abhikrc.com/pdf/ISSTA24_Oracle_Guided.pdf</li>
<li>Oracle-guided Program Selection from Large Language Models, https://mechtaev.com/files/issta24.pdf</li>
<li>OVERCOMING THE AMBIGUITY REQUIREMENT USING, https://www.diva-portal.org/smash/get/diva2:1931301/FULLTEXT01.pdf</li>
<li>Structured Handling of Exceptions in LLM-Driven Agentic Workflows, https://arxiv.org/html/2508.07935v1</li>
<li>Steering LLMs for System Design: How I Built a Context-Aware, https://builder.aws.com/content/37NSzssL5C6caCFmKP7cNFbR1IO/steering-llms-for-system-design-how-i-built-a-context-aware-decision-engine</li>
<li>Deterministic AI Orchestration: A Platform Architecture … - Praetorian, https://www.praetorian.com/blog/deterministic-ai-orchestration-a-platform-architecture-for-autonomous-development/</li>
<li>What is AI Agent Evaluation? | Databricks, https://www.databricks.com/glossary/agent-evaluation</li>
<li>Trustworthy AI Agents: Deterministic Replay - Sakura Sky, https://www.sakurasky.com/blog/missing-primitives-for-trustworthy-ai-part-8/</li>
<li>Challenges in Testing Large Language Model Based Software, https://arxiv.org/html/2503.00481v2</li>
<li>Challenges in Testing Large Language Model Based Software - arXiv, https://arxiv.org/html/2503.00481v1</li>
<li>Golden datasets: Creating evaluation standards - Statsig, https://www.statsig.com/perspectives/golden-datasets-evaluation-standards</li>
<li>Golden Tests in AI: Ensuring Reliability Without Slowing Innovation, https://www.shaped.ai/blog/golden-tests-in-ai</li>
<li>Ground Truth Data for AI | SuperAnnotate, https://www.superannotate.com/blog/ground-truth-data-for-ai</li>
<li>What Is a Golden Dataset in AI and Why Does It Matter? - DAC.digital, https://dac.digital/what-is-a-golden-dataset/</li>
<li>Managing Evaluation Datasets for AI Agents and LLMs - Medium, https://medium.com/@kamyashah2018/managing-evaluation-datasets-for-ai-agents-and-llms-6d059197027c</li>
<li>How to Build Robust Evaluation Datasets for AI Agents: Tips and Tricks, https://dev.to/kuldeep_paul/how-to-build-robust-evaluation-datasets-for-ai-agents-tips-and-tricks-3kp0</li>
<li>Ground Truth Expectations - MLflow, https://mlflow.org/docs/latest/genai/assessments/expectations/</li>
<li>Data Labeling for LLM AI: What Works, What Fails, What Costs, https://www.iteratorshq.com/blog/data-labeling-for-llm-ai-what-works-what-fails-what-costs/</li>
<li>How Invoice Exception Handling Works in Automated AP Systems, https://www.artsyltech.com/blog/invoice-exception-handling-in-ap-systems</li>
<li>Ontological Foundations for Deterministic Assurance Context … - MDPI, https://www.mdpi.com/2076-3417/16/4/1984</li>
<li>Hybrid Intelligence - New Math Data, https://newmathdata.com/blog/hybrid-ai-deterministic-code-llm-reasoning-systems/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>