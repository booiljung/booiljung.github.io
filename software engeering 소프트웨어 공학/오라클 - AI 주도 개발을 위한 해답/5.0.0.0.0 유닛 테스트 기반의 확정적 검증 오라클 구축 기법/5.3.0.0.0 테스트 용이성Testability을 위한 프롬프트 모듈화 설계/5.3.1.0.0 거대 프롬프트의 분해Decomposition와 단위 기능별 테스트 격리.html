<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:5.3.1 거대 프롬프트의 분해(Decomposition)와 단위 기능별 테스트 격리</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>5.3.1 거대 프롬프트의 분해(Decomposition)와 단위 기능별 테스트 격리</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 5. 유닛 테스트 기반의 확정적 검증 오라클 구축 기법</a> / <a href="index.html">5.3 테스트 용이성(Testability)을 위한 프롬프트 모듈화 설계</a> / <span>5.3.1 거대 프롬프트의 분해(Decomposition)와 단위 기능별 테스트 격리</span></nav>
                </div>
            </header>
            <article>
                <h1>5.3.1 거대 프롬프트의 분해(Decomposition)와 단위 기능별 테스트 격리</h1>
<p>대규모 언어 모델(Large Language Model, LLM)이 소프트웨어 아키텍처의 핵심 런타임(Runtime) 환경으로 편입됨에 따라, 자연어를 통해 시스템의 동작을 제어하는 프롬프트웨어(Promptware)라는 새로운 소프트웨어 패러다임이 산업계 전반에 걸쳐 확산되고 있다. 전통적인 소프트웨어가 엄격한 문법을 가진 프로그래밍 언어와 결정론적(Deterministic) 런타임 환경에 기반하는 반면, 프롬프트웨어는 태생적으로 모호하고 문맥 의존적인 자연어를 매개로 하며 비결정론적(Nondeterministic)이고 확률적인 LLM 위에서 동작한다. 이러한 근본적인 아키텍처의 차이는 프롬프트 개발 과정을 직관과 경험에 의존하는 소모적인 시행착오(Trial-and-error)의 반복으로 전락시켰으며, 학계와 산업계의 연구자들은 이러한 현상을 일컬어 ’프롬프트웨어 위기(Promptware Crisis)’로 명명하고 있다.</p>
<p>이러한 위기를 극복하고 AI 기반 소프트웨어의 엔터프라이즈급 신뢰성을 확보하기 위한 가장 핵심적인 소프트웨어 공학 기법이 바로 ’거대 프롬프트의 분해(Decomposition)’와 이를 기반으로 한 ’단위 기능별 테스트 격리(Test Isolation)’이다. 단일한 거대 프롬프트(Monolithic Prompt)에 다수의 비즈니스 요구사항, 예외 처리 제약 조건, 컨텍스트 정보, 그리고 출력 형식에 대한 지시를 한꺼번에 욱여넣는 방식은 모델의 주의력(Attention)을 극심하게 분산시켜 환각(Hallucination)을 유발한다. 더 치명적인 것은, 런타임에 오류가 발생했을 때 파이프라인의 어느 지점에서 논리적 결함이 시작되었는지 정확한 원인을 추적할 수 없는 거대한 블랙박스(Black-box)를 형성한다는 점이다. 본 절에서는 거대 프롬프트를 단일 책임 원칙(Single Responsibility Principle)을 준수하는 세부 모듈로 분해하는 아키텍처적 접근법과, 이렇게 분해된 각 단위 프롬프트 사이에 테스트 대역(Test Doubles)을 주입하여 결정론적 오라클(Deterministic Oracle) 기반의 엄격한 단위 테스트를 수행하는 방법론을 심도 있게 다룬다.</p>
<h2>1.  프롬프트웨어(Promptware) 패러다임과 거대 프롬프트의 한계</h2>
<h3>1.1  비결정론적 런타임과 프롬프트웨어 위기</h3>
<p>현대의 AI 애플리케이션 개발에서 프롬프트는 단순한 입력 문자열이 아니라, LLM이라는 컴파일러이자 실행 환경을 통제하는 ’1급 프로그래밍 인터페이스’로 기능한다. 논문 <em>Promptware Engineering: Software Engineering for LLM Prompt Development</em>에서는 이러한 시스템을 프롬프트웨어(Promptware)로 정의하며, 이것이 전통적 소프트웨어와 구별되는 두 가지 핵심 차이점으로 ’언어의 유연성’과 ’런타임의 비결정성’을 지목한다. 엄격한 구문 규칙에 의해 실행 결과가 100% 보장되는 기존 코드와 달리, 자연어 프롬프트는 모델의 파라미터 상태, 온도(Temperature) 설정, 심지어 프롬프트 내 단어의 미세한 배치 순서에 따라서도 전혀 다른 확률 분포를 생성한다.</p>
<p>이러한 비결정성으로 인해 개발자들은 프롬프트를 체계적으로 설계하지 못하고, 문제가 발생할 때마다 땜질식(Ad-hoc)으로 문장을 추가하거나 수정하는 방식에 의존하게 된다. 시스템이 고도화될수록 프롬프트의 길이는 끝없이 늘어나며, 결국 어떤 지시어가 어떤 출력 결과에 인과적으로 영향을 미쳤는지 파악할 수 없는 ’프롬프트웨어 위기’에 직면하게 되는 것이다.</p>
<h3>1.2  거대 프롬프트(Monolithic Prompt)의 구조적 결함</h3>
<p>초기 LLM 애플리케이션 개발자들은 모델의 컨텍스트 윈도우(Context Window)가 커짐에 따라, 모든 지시사항을 하나의 프롬프트에 담아 처리하려는 경향을 보였다. 이러한 ‘거대 프롬프트’ 아키텍처는 표면적으로는 API 호출 횟수를 줄여 효율적인 것처럼 보이지만, 소프트웨어 공학과 테스트 용이성(Testability) 관점에서는 다음과 같은 치명적인 구조적 결함을 내포하고 있다.</p>
<p>첫째, <strong>어텐션 저하와 정보 소실(Lost in the middle) 현상</strong>이다. 프롬프트의 길이가 길어지고 내부 논리가 복잡해질수록, LLM의 어텐션 메커니즘은 핵심 지시사항을 놓치거나 프롬프트 중간에 위치한 제약 조건을 무시하는 경향이 짙어진다. 둘째, **연쇄적 오류 전파(Cascading Error Propagation)**이다. 단일 프롬프트 내에서 정보 추출, 논리적 추론, 출력 형식 지정 등 여러 작업이 한 번의 추론 과정(Single-turn)으로 결합되어 있기 때문에, 초기 단계에서의 미세한 환각이나 오판이 후속 논리 전개 과정에서 기하급수적으로 증폭된다. 셋째, <strong>강한 결합(Tight Coupling)으로 인한 회귀(Regression) 문제</strong>이다. 시스템의 특정 기능(예: 출력 JSON 스키마 변경)을 수정하기 위해 거대 프롬프트의 일부를 변경할 경우, 이 변경이 전혀 무관한 다른 기능(예: 정보 요약의 정확도)에 예상치 못한 사이드 이펙트(Side-effect)를 초래한다. 이는 결과적으로 코드베이스를 극도로 취약(Brittle)하게 만들어 유지보수를 불가능하게 한다.</p>
<h2>2.  소프트웨어 공학적 해결책: 프롬프트의 모듈화와 분해(Decomposition) 원리</h2>
<h3>2.1  모듈화(Modularity) 원칙의 프롬프트웨어 적용</h3>
<p>전통적인 소프트웨어 공학에서 모듈화(Modularity)는 복잡한 시스템을 작고 독립적이며 명확한 인터페이스를 통해 상호 작용하는 단위(Unit)로 나누는 핵심 원칙이다. 모듈화는 코드의 논리적 구성을 촉진하고, 개발자 간의 종속성을 줄이며, 코드 재사용성을 높인다. <em>Harvard University</em>의 연구진은 논문에서 LLM이 생성하는 코드가 표면적으로는 모듈화된 것처럼 보여도 실제로는 과도한 엔지니어링과 숨겨진 의존성으로 인해 모듈화 원칙을 심각하게 위반하는 경향이 있음을 지적했다.</p>
<p>이러한 모듈화의 원칙은 LLM이 생성하는 ’결과물(Code)’뿐만 아니라, LLM을 제어하는 ‘입력물(Prompt)’ 자체에도 동일하게 적용되어야 한다. 거대 프롬프트를 단일 책임 원칙(Single Responsibility Principle)에 따라 명확한 경계(Coherent boundaries)를 가진 하위 프롬프트 체인으로 분해(Decomposition)해야 한다. 프롬프트를 분해한다는 것은 곧 각각의 프롬프트가 단 하나의 명확한 목표(예: 문장 요약, 데이터 추출, 감성 분석 등)만을 수행하도록 시스템을 재설계하는 것을 의미한다.</p>
<h3>2.2  사고의 사슬(Chain of Thought)과 프롬프트 체이닝(Prompt Chaining)의 공학적 차이</h3>
<p>거대 프롬프트의 추론 한계를 극복하기 위한 대표적인 프롬프트 엔지니어링 기법으로 ’사고의 사슬(Chain of Thought, CoT)’과 ’프롬프트 체이닝(Prompt Chaining)’이 광범위하게 사용되고 있으나, 소프트웨어 공학의 ‘테스트 용이성(Testability)’ 관점에서 이 둘은 근본적으로 이질적인 속성을 갖는다.</p>
<p>CoT 기법은 단일 프롬프트 내에서 LLM이 최종 답변을 도출하기 전에 중간 추론 과정(Intermediate reasoning steps)을 명시적으로 작성하도록 유도하는 방법론이다. 프롬프트 내에 “차근차근 단계별로 생각해보자(Let’s think step by step)“라는 지시어를 추가하거나, 논리 전개 과정이 포함된 소수의 예제(Few-shot exemplars)를 제공하여 모델 스스로 복잡한 문제를 잘게 나누어 해결하도록 돕는다. 이 기법은 산술 연산이나 상식 추론 영역에서 모델의 정확도를 비약적으로 상승시키는 것으로 증명되었다. 그러나 테스트 관점에서 CoT는 치명적인 약점을 지닌다. 모든 사고 과정이 단 한 번의 API 호출(Single-turn) 내에서 모델 내부적으로 처리되기 때문에, 개발자나 외부 테스트 시스템이 추론의 중간 단계에 개입하여 변수를 통제하거나 특정 논리 블록만을 분리하여 유닛 테스트를 수행하는 것이 구조적으로 불가능하다. 즉, CoT는 여전히 ’은닉된 잠재 상태(Hidden latent state)’를 가지는 진보된 형태의 블랙박스에 불과하다.</p>
<p>반면, 분해(Decomposition) 아키텍처를 기반으로 하는 ’프롬프트 체이닝(Prompt Chaining)’은 거대한 단일 작업을 여러 개의 독립적인 프롬프트(개별 API 호출)로 물리적으로 쪼개어 순차적으로 연결하는 기법이다. 예를 들어 복잡한 문서를 분석하는 시스템을 구축할 때, ‘1단계: 핵심 엔티티 추출 프롬프트’ <span class="math math-inline">\rightarrow</span> ‘2단계: 추출된 엔티티 기반 팩트 체크 프롬프트’ <span class="math math-inline">\rightarrow</span> ’3단계: 사용자 맞춤형 최종 응답 생성 프롬프트’의 형태로 데이터 파이프라인을 구성한다.</p>
<p>이러한 프롬프트 체이닝 방식은 각 단계가 결정론적인 구조화된 입력(JSON 등)을 받아 고유한 단일 작업을 수행하고, 그 결과물을 다음 단계의 입력으로 명시적으로 전달한다. 구조적 분해는 모델의 사고 과정을 ’투명한 중간 상태(Transparent intermediate state)’로 시스템 외부에 노출시키며 , 각 단계를 완벽히 독립된 단위 함수처럼 취급할 수 있게 만든다. 결과적으로 특정 단계의 프롬프트를 고립된 환경에서 철저히 검증할 수 있는 단위 테스트의 기반이 확보되는 것이다.</p>
<h2>3.  분해를 통한 투명성 확보와 추론 성능의 극대화</h2>
<p>프롬프트 분해가 단순히 소프트웨어 아키텍처의 깔끔함을 넘어서, 실제 LLM의 문제 해결 능력과 신뢰성을 어떻게 물리적으로 향상시키는지 이해하는 것은 매우 중요하다.</p>
<h3>3.1  논문 리뷰: Iterated Decomposition을 통한 과학적 추론 검증</h3>
<p>이와 관련하여 복잡한 추론이 요구되는 테스크에서 프롬프트 분해가 미치는 영향을 실증적으로 분석한 논문 <em>Iterated Decomposition: Improving Science Q&amp;A by Supervising Reasoning Processes</em>의 연구 결과는 매우 시사하는 바가 크다. 해당 연구팀은 기존의 엔드투엔드(End-to-end) LLM 방식이 복잡한 작업에서 환각을 통제하지 못하고 쉽게 무너지는 원인이 ’은닉된 잠재 상태(Hidden latent state)’에 대한 관찰 및 디버깅의 부재에 있다고 지적했다.</p>
<p>연구팀은 이에 대한 해결책으로 복잡한 과학 QA 추론 과정을 투명한 중간 상태(Transparent intermediate state)를 가지는 세부 컴포넌트 단위로 분해(Decomposition)하고, 이를 다시 구성(Composition)하는 ‘반복적 분해(Iterated Decomposition)’ 워크플로우를 제안했다. 이 시스템은 ICE(Interactive Composition Explorer)라는 시각화 도구를 통해 LLM 프로그램의 실행 추적(Execution traces)을 명시적으로 노출시켰다.</p>
<p>중요한 점은 프롬프트를 분해함으로써 ’실패 지점의 정확한 식별’이 가능해졌다는 것이다. 연구팀은 파이프라인 전체가 실패했을 때 모델 전체를 탓하는 대신, 오작동을 일으킨 특정 세부 컴포넌트(단위 프롬프트)만을 집중적으로 확대(Zooming in)하여 해당 컴포넌트의 프롬프트를 정제하거나 추가적인 문맥을 제공하는 방식으로 디버깅을 수행했다.</p>
<p>이러한 반복적 분해와 단위 검증 접근법을 실제 데이터셋에 적용한 결과는 가히 놀라운 수준이었다. 무작위 대조군 연구(RCT) 논문에서 위약(Placebo)의 종류와 방식을 추출하고 설명하는 테스크의 정확도는 단일 프롬프트 환경의 25%에서 분해 환경의 65%로 상승했다. 의료 개입에 대한 참가자 순응도(Adherence) 평가 정확도는 53%에서 70%로, Qasper 데이터셋에 기반한 복잡한 NLP 논문 질의응답 정확도는 38%에서 69%로 비약적으로 향상되었다. 이 성과는 단순히 프롬프트를 우아하게 작성해서 얻어진 결과가 아니다. 문제를 세분화함으로써 각 하위 테스크에 대해 ’결정론적 오라클을 통한 단위 검증’이 물리적으로 가능해졌고, 이를 통해 환각의 전파를 원천적으로 차단했기 때문에 도출된 공학적 승리이다.</p>
<h3>3.2  셀프 데스크(Self-ask)와 동적 분해(Dynamic Decomposition)</h3>
<p>프롬프트 분해는 정적인 아키텍처 설계뿐만 아니라, 모델의 추론 과정 내에서도 동적으로 적용될 수 있다. ‘Self-ask’ 프롬프팅 기법은 사용자로부터 복잡한 단일 질문을 받았을 때, LLM 스스로 이 거대 질문을 해결하기 위해 필요한 하위 질문(Sub-questions)들을 명시적으로 분해하여 나열하도록 지시하는 방식이다. 단일 패스(Single-pass)로 최종 답변을 추측하는 대신, 분해된 각각의 하위 질문에 대해 순차적으로 답변을 구한 뒤 이를 최종적으로 합성(Synthesis)한다. 이는 논리적 도약으로 인한 환각을 방지하고, 멀티 홉(Multi-hop) 추론에서 모델이 누락하는 정보가 없도록 보장하는 강력한 분해 기법이다.</p>
<p>더 나아가 팩트 체킹(Fact-checking) 파이프라인에서는 ‘Decompose-Then-Verify’ 패러다임이 활용된다. 이는 장문의 텍스트에 포함된 수많은 사실적 주장(Claims)들을 검증 가능한 최소 단위(Atomic facts)로 동적 분해(Dynamic decomposition)한 뒤, 독립적인 검증기(Verifier) 모델이나 외부 검색 엔진 오라클을 통해 각각의 원자적 주장의 진위를 결정론적으로 판별하는 고도화된 아키텍처이다.</p>
<h2>4.  단위 기능별 테스트 격리(Test Isolation)의 철학과 당위성</h2>
<p>프롬프트를 다수의 독립적인 체인으로 분해했다면, 다음 단계는 소프트웨어 품질 보증(QA)의 핵심인 ‘테스트 격리(Test Isolation)’ 환경을 구축하는 것이다. 기존 소프트웨어 개발에서 단위 테스트(Unit Testing)는 시스템의 가장 작은 기능적 단위(일반적으로 단일 함수나 클래스의 메서드)를 데이터베이스, 파일 시스템, 외부 네트워크 등 다른 의존성(Dependencies)으로부터 완벽히 격리한 상태에서 독립적으로 검증하는 과정이다. 이러한 격리 환경은 테스트의 실행 속도를 비약적으로 높이고, 외부 환경의 일시적 장애로 인한 테스트의 불안정성(Flakiness)을 제거하며, 실패 원인을 코드의 특정 라인으로 즉각 좁혀주는 결정적인 역할을 한다.</p>
<h3>4.1  엔드투엔드(E2E) 테스트의 한계와 단위 테스트의 재정의</h3>
<p>AI 에이전트나 LLM 기반 파이프라인은 사용자의 입력 텍스트를 파싱하고, 벡터 데이터베이스에서 컨텍스트를 검색하며(RAG), 외부 API 도구를 호출하고(Tool Use), 최종적으로 모든 정보를 종합하여 응답을 생성하는 복잡한 다단계 구조를 가진다. 만약 전체 파이프라인의 입구와 출구만을 검증하는 엔드투엔드(End-to-End, E2E) 테스트에만 전적으로 의존한다면, 개발팀은 다음과 같은 심각한 공학적 부채에 직면하게 된다.</p>
<p>첫째, <strong>비결정적 동작의 연쇄(Chain of Nondeterminism)로 인한 원인 규명 불가</strong> 문제다. CI/CD 파이프라인에서 특정 E2E 테스트가 실패했을 때, 그 원인이 사용자의 의도를 잘못 분류한 프롬프트의 결함인지, 벡터 검색 알고리즘이 엉뚱한 문서를 반환한 탓인지, 호출된 외부 날씨 API가 타임아웃을 발생시켰는지, 아니면 마지막 요약 프롬프트가 환각을 일으킨 탓인지 즉각적으로 판별할 수 없다. 에러는 파이프라인 어딘가에 숨어 있으며, 개발자는 테스트가 실패할 때마다 전체 로그를 뒤져야 하는 수고를 겪게 된다. 둘째, <strong>비용과 지연 시간(Latency)의 기하급수적 증가</strong>이다. 거대 언어 모델(예: GPT-4, Claude 3.5 Opus)을 이용한 다단계 프롬프트 실행을 매 코드 커밋(Commit)마다 E2E 수준에서 수백 번씩 실행하는 것은 천문학적인 API 토큰 비용과 상당한 테스트 소요 시간을 초래한다. 이는 빠른 피드백 루프를 생명으로 하는 애자일(Agile) 개발 방법론과 전면적으로 배치된다. 셋째, <strong>극단적 엣지 케이스(Edge Case) 시뮬레이션의 한계</strong>이다. “외부 데이터베이스 시스템이 해킹되어 극단적으로 편향되거나 악의적인 프롬프트 인젝션(Prompt Injection) 데이터를 반환했을 때, 방어 프롬프트가 이를 어떻게 차단하는가?“와 같은 위험 시나리오를 프로덕션 DB가 연결된 E2E 환경에서 의도적으로 매번 재현하는 것은 매우 번거롭고 위험하다.</p>
<p>따라서 분해된 개별 프롬프트 컴포넌트들은 반드시 철저히 격리된 환경에서 평가되어야 한다. 프롬프트 단위 테스트(Prompt Unit Test)에서 ’단위(Unit)’란 독립적인 단일 프롬프트 템플릿과 그 프롬프트를 실행하는 LLM 호출 자체를 의미한다. 각 단위 프롬프트는 테스트 시점에 시스템의 다른 부분으로부터 분리되어야 하며, 결정론적인 고정된 입력 데이터(Fixtures)를 주입받아야 한다. 이를 통해 외부 시스템의 장애나 선행 프롬프트의 변동성 개입 없이, 오로지 해당 프롬프트의 지시사항 설계 품질과 타겟 LLM의 추론 능력만을 평가할 수 있어야 한다.</p>
<h2>5.  격리(Isolation) 구현 메커니즘: 테스트 대역(Test Doubles) 주입 전략</h2>
<p>단위 기능별 테스트 격리를 기술적으로 완벽하게 구현하기 위한 핵심 소프트웨어 공학 패턴이 바로 ’테스트 대역(Test Doubles)’의 적극적인 활용이다. 테스트 대역이란 영화 촬영 현장에서 위험한 액션을 대신하는 스턴트 대역 배우처럼, 테스트 대상 시스템(System Under Test, SUT)이 의존하는 실제 프로덕션 컴포넌트를 대체하여 테스트 환경을 완벽하게 통제하는 모의 객체들을 총칭한다.</p>
<p>프롬프트웨어의 단위 테스트 환경에서 테스트 대역은, 느리고 비용이 많이 들며 제어 불가능한 비결정성을 띠는 선/후행 LLM 체인 구간이나 외부 시스템 호출을 빠르고 결정론적인 시뮬레이션으로 대체하는 역할을 수행한다. AI 소프트웨어 테스트에서 주로 활용되는 세 가지 핵심 테스트 대역 패턴인 스텁(Stub), 목(Mock), 페이크(Fake)가 프롬프트 격리에 어떻게 적용되는지 상세히 분석한다.</p>
<h3>5.1  스텁(Stub)을 활용한 중간 상태(Intermediate State) 강제 주입</h3>
<p>스텁(Stub)은 테스트 과정에서 컴포넌트 간의 복잡한 연산을 생략하고 사전에 하드코딩된 특정 결정론적 값을 무조건적으로 반환하도록 설계된 객체이다. 스텁은 대상 컴포넌트의 내부 논리나 입력값의 정합성을 검증하지 않으며, 오직 테스트 대상이 원활하게 다음 로직을 진행할 수 있도록 “간접적인 입력(Indirect input)을 제공“하는 데 그 목적이 있다.</p>
<p>다단계 프롬프트 체인에서 스텁은 주로 선행 단계의 LLM 출력을 결정론적으로 고정(Pinning)하여 후행 프롬프트의 테스트를 격리하는 데 필수적으로 사용된다. 예를 들어, 사용자의 자연어 쿼리를 SQL로 변환하는 ’SQL 생성 프롬프트 모듈(Module A)’과, 반환된 SQL 데이터를 사용자 친화적인 자연어 보고서로 요약하는 ’보고서 작성 프롬프트 모듈(Module B)’이 직렬로 연결된 파이프라인이 있다고 가정하자. Module B의 프롬프트 성능(예: 데이터 요약의 정확성, 지정된 어조의 준수 여부)을 단위 테스트하기 위해 매번 Module A를 실행하고 실제 데이터베이스에 쿼리를 날릴 필요는 없다. Module A의 SQL 생성 자체가 확률적이므로, 만약 Module A가 문법에 맞지 않는 잘못된 SQL을 생성하여 빈 데이터가 반환된다면, Module B의 요약 테스트는 Module B 프롬프트의 잘못이 아님에도 불구하고 억울하게 실패(False Negative)하게 된다.</p>
<p>이러한 의존성을 끊기 위해 Module A의 역할과 외부 DB의 실행 결과를 스텁으로 대체한다.</p>
<ul>
<li><strong>Module B 프롬프트 격리 테스트 시나리오 (스텁 적용):</strong></li>
<li><strong>프롬프트 입력 변수 주입(고정값):</strong> <code>query="2025년 1분기 최고 매출 지점은?"</code></li>
<li><strong>스텁 반환값 강제 주입 (DB 결과 시뮬레이션):</strong> <code>db_result = [{"branch_name": "서울 강남점", "total_revenue": "1500000000"}]</code></li>
<li><strong>오라클 검증 대상 (Module B의 출력):</strong> 스텁이 제공한 결정론적 데이터를 바탕으로, LLM이 환각(Hallucination) 없이 정확하게 수치를 변환하여 “2025년 1분기 최고 매출 지점은 서울 강남점이며, 총매출은 15억 원입니다.“라는 텍스트를 생성하는지 검증한다.</li>
</ul>
<p>이처럼 스텁을 주입하면 SUT(Module B)의 완전한 고립성이 보장되며, 프롬프트의 템플릿 변경이 최종 출력 품질에 미치는 인과적 영향을 외부 노이즈 없이 결정론적으로 측정할 수 있다.</p>
<h3>5.2  목(Mock) 객체를 통한 LLM 도구 사용(Tool Calling) 행위 검증</h3>
<p>스텁이 주로 ’외부 의존성이 무엇을 반환할 것인가(State-based verification)’에 초점을 맞춘다면, 목(Mock) 객체는 ’SUT가 의존성 객체와 어떻게 상호작용했는가(Behavior-based verification)’를 검증하는 데 목적이 있다. 목 객체는 자신이 테스트 실행 중에 몇 번 호출되었는지, 어떤 이름의 메서드를 통해 호출되었는지, 그리고 가장 중요하게 ’어떤 매개변수(Arguments)를 전달받았는지’를 내부적으로 기록한다. 테스트 종료 시, 개발자는 이 기록된 상호작용 내역이 기대한 바와 정확히 일치하는지 단언(Assert)함으로써 테스트를 수행한다.</p>
<p>최근의 LLM 프레임워크는 단순한 텍스트 생성을 넘어 외부 도구를 능동적으로 호출(Tool Calling / Function Calling)하는 에이전트(Agent) 아키텍처로 진화했다. 특정 조건에서 LLM이 올바른 도구를 선택하고, 프롬프트에 명시된 규칙에 따라 정확한 매개변수를 생성하여 전달하는지를 검증하는 것은 에이전트 신뢰성의 핵심이다. 이때 목(Mock) 객체가 결정적인 역할을 한다.</p>
<p>예를 들어, 챗봇 에이전트에게 “오늘 저녁 7시에 김철수 고객에게 예약 확인 이메일을 발송하라“는 사용자의 지시를 처리하는 프롬프트를 테스트한다고 가정하자. 단위 테스트가 실행될 때마다 실제로 외부 SMTP 서버를 통해 이메일이 발송되는 사고를 막아야 하므로, 실제 <code>send_email</code> 함수 인터페이스를 목(Mock) 객체로 교체한다.</p>
<ul>
<li><strong>목 객체를 활용한 결정론적 행위 검증 (Mock Assertions):</strong></li>
<li>프롬프트 제어 하에 LLM이 최종적으로 <code>send_email</code> 함수를 1회 정확히 호출(Invoke)하기로 결정했는가? (<code>mock_send_email.assert_called_once()</code>)</li>
<li>LLM이 추출하여 목 객체로 넘긴 매개변수 <code>to_address</code>의 값이 이메일 형식 규칙을 준수하는가?</li>
<li>매개변수 <code>body</code>에 프롬프트가 지시한 회사 내규에 따른 정중한 예약 확인 어조(Tone)가 정상적으로 포함되었는가?</li>
</ul>
<p>목(Mock)을 통한 행위 검증은 LLM이 생성하는 예측 불가능한 자유 텍스트 문자열을 검사하는 것이 아니라, 모델이 방출(Emit)하는 엄격히 구조화된 함수 호출 매개변수(JSON)를 평가하므로 완벽하게 결정론적인(Deterministic) 오라클 기반 단위 테스트를 가능하게 한다.</p>
<h3>5.3  페이크(Fake) 응답을 활용한 무거운 의존성 격리</h3>
<p>페이크(Fake) 객체는 스텁이나 목과 달리 실제로 동작하는 내부 로직(Working implementation)을 가지고 있지만, 성능이나 인프라 제약으로 인해 프로덕션 환경에서 사용하기에는 적합하지 않은 경량화된 지름길(Shortcut) 구현체이다. 단위 테스트 프레임워크에서 인메모리(In-memory) 데이터베이스를 사용하여 실제 RDBMS를 대체하는 것이 가장 고전적인 페이크의 예시이다.</p>
<p>AI 소프트웨어, 특히 RAG(Retrieval-Augmented Generation) 파이프라인의 프롬프트를 테스트할 때 페이크 객체는 매우 유용하게 쓰인다. 수백만 건의 문서 텍스트 청크(Chunk)가 적재된 실제 벡터 데이터베이스(Vector DB) 인스턴스에 네트워크를 통해 연결하여 정보 합성 프롬프트의 요약 성능을 테스트하면, 과도한 네트워크 I/O 지연이 발생하고 데이터베이스 상태 변경에 따라 테스트 결과가 요동칠 수 있다.</p>
<p>이러한 경우, 메모리 내에서만 동작하며 사전에 철저히 검증된 극소수의 텍스트 청크(예: 5개의 핵심 정책 문서)만을 저장하고 있는 ’페이크 인메모리 벡터 검색 엔진’을 파이프라인에 주입한다. 이를 통해 프롬프트가 주입된 문맥(Context)을 얼마나 논리적으로 잘 통합하여 답변을 구성하는지, 혹은 문맥에 없는 내용을 환각으로 지어내지는 않는지를 극도로 통제되고 빠른 로컬 테스트 환경에서 완벽히 검증할 수 있다.</p>
<table><thead><tr><th><strong>테스트 대역 패턴</strong></th><th><strong>공학적 정의 및 속성</strong></th><th><strong>AI 프롬프트웨어 환경에서의 적용 목적 및 예시</strong></th><th><strong>확률적 요소의 통제 및 격리 방식</strong></th></tr></thead><tbody>
<tr><td><strong>스텁 (Stub)</strong></td><td>대상 시스템에 특정 상태(State) 기반의 하드코딩된 응답을 제공하는 더미 객체</td><td><strong>중간 상태 강제 주입:</strong> 선행 프롬프트나 외부 검색 API의 실행을 생략하고, 성공 시나리오의 구조화된 JSON 데이터(<code>{"status": "success", "data":...}</code>)를 후행 프롬프트에 직접 주입함.</td><td>프롬프트 체인 내에서 비결정적인 선행 LLM 호출을 완전히 제거함으로써, 검증 대상인 후행 프롬프트의 독립성과 결정론적 실행을 보장함.</td></tr>
<tr><td><strong>목 (Mock)</strong></td><td>SUT가 의존성과 상호작용하는 행위(Behavior) 및 전달 인자를 기록하고 검증하는 감시 객체</td><td><strong>LLM Tool Calling 검증:</strong> 실제 결제 API나 이메일 발송 API를 목 객체로 교체. LLM이 생성한 함수 호출 인자(Arguments)가 스키마를 준수하는지, 비즈니스 규칙에 맞는 값이 매핑되었는지 Assertion 수행.</td><td>텍스트 생성이 아닌, 모델이 방출하는 정형화된 파라미터 <span class="math math-inline">P(\text{Action} \vert \text{Context})</span>만을 평가하여, 실제 세계의 사이드 이펙트 발생을 원천 차단함.</td></tr>
<tr><td><strong>페이크 (Fake)</strong></td><td>실제 의존성의 핵심 로직을 모사하여 가볍고 빠르게 동작하도록 구현된 경량화 객체</td><td><strong>RAG 컨텍스트 의존성 경량화:</strong> 수백만 건의 외부 벡터 DB 대신, 테스트용 픽스처(Fixture) 청크 10개만 포함된 로컬 인메모리 검색 객체(Fake Vector Store)를 주입하여 프롬프트의 정보 합성 로직 구동.</td><td>외부 네트워크 I/O 및 검색 알고리즘의 변동성을 배제하고, 통제된 미니 데이터셋 환경에서 프롬프트의 지식 통합 능력을 신속하게 단위 테스트함.</td></tr>
</tbody></table>
<p>(표 5.1: LLM 프롬프트 파이프라인 테스트 격리를 위한 테스트 대역(Test Doubles)의 공학적 분류 및 통제 매커니즘 매핑 )</p>
<h2>6.  결정론적 정답지(Deterministic Oracle) 구현을 위한 프롬프트 통제</h2>
<p>프롬프트를 다수의 모듈로 분해하고 테스트 대역을 주입하여 격리 환경을 구축했다면, 테스트의 마지막 단계는 해당 프롬프트 모듈의 실행 결과가 올바른지 판단하는 ’오라클(Oracle)’을 설계하는 것이다.</p>
<p>전통적인 소프트웨어 테스트에서 오라클이란 테스트 케이스의 실행 결과를 평가하여 참(Pass)과 거짓(Fail)을 결정론적으로 판별하는 매커니즘 또는 정답지를 의미한다. 두 정수의 덧셈을 수행하는 수학 함수처럼 입력과 출력이 1:1로 정확히 매핑되는 시스템에서는 <code>assert(add(2, 2) == 4)</code>와 같이 단순한 동등성 비교만으로 완벽한 오라클을 구성할 수 있다.</p>
<p>그러나 LLM은 본질적으로 다대다(Many-to-many) 매핑 특성을 지닌다. 동일한 입력 픽스처(Fixture)를 제공하더라도 온도(Temperature), Top-P 등의 파라미터나 내부 샘플링 알고리즘에 따라 모델은 매번 미세하게 다른 문장 구조, 유의어 교체, 포맷팅 변화를 동반한 비정형 텍스트를 출력한다. 따라서 “결과 텍스트가 기대 문자열과 정확히 일치하는가?“를 묻는 고전적인 일치 기반 오라클(Exact Match Oracle)은 LLM 프롬프트 테스트 환경에서 테스트를 끊임없이 실패하게 만드는 주범이 된다.</p>
<p>프롬프트 엔지니어링의 단위 테스트를 CI/CD 파이프라인 수준에서 완벽히 자동화하고 그 결과를 신뢰하기 위해서는, 비결정론적이고 비정형적인 LLM의 텍스트 생성을 ’결정론적인 참/거짓 판단(Deterministic Binary Judgment)’으로 강제 수렴시키는 진보된 오라클 아키텍처가 필수적이다.</p>
<h3>6.1  구조화된 출력(Structured Outputs)과 스키마 강제를 통한 오라클의 결정론 극대화</h3>
<p>단위 테스트 오라클의 결정론을 기술적으로 가장 완벽하게 확보하는 방법은, 프롬프트의 출력을 모호한 자연어의 영역에서 엄격한 자료 구조의 영역으로 끌어내리는 것이다. 이를 위해 프롬프트를 분해할 때 시스템 내부의 각 단위 모듈 간에 데이터를 주고받는 인터페이스를 JSON 스키마(Schema)로 명확히 정의하고 계약(Contract)해야 한다.</p>
<p>과거에는 프롬프트 지시문 하단에 단지 “결과를 반드시 JSON 형식으로만 응답해 다오“라고 자연어로 요청하는 수준에 머물렀지만, 이는 LLM이 응답 앞뒤에 불필요한 인사말(예: “Here is the JSON you requested:”)을 덧붙이거나 마크다운 백틱(````json`)을 일관성 없이 사용하여 JSON 파서(Parser)를 고장 내는 원인이 되었다.</p>
<p>이를 극복하기 위해 최근 도입된 OpenAI의 Structured Outputs API나 오픈소스 진영의 강제 출력 제어(Constrained Generation) 기법들은 모델의 토큰 생성 디코딩 단계에 직접 개입하여, 개발자가 제공한 JSON 스키마 규칙에 위배되는 토큰 생성을 원천적으로 차단한다. 이러한 시스템 레벨의 강제 스키마 주입이 적용되면, 오라클의 역할은 매우 단순하고 명확해진다. 오라클은 더 이상 자유 텍스트의 미묘한 의미나 문맥을 분석할 필요 없이, 파싱된 JSON 객체가 지닌 특정 키(Key)의 값, 데이터 타입(Integer, String, Boolean), 그리고 제한된 열거형(Enum) 문자열이 기대값(Expected value)과 완벽히 일치하는지(Exact Match) 검사하는 전통적인 Assertion 연산으로 완벽히 치환된다. 이는 프롬프트 테스트를 기존의 백엔드 API 테스트와 완벽히 동일한 수준의 결정론적 검증 체계로 통합시킨다.</p>
<h3>6.2 프로퍼티 기반 룰 검사(Rule-based Property Checks)</h3>
<p>반드시 LLM의 최종 출력이 사용자를 위한 자유 텍스트 형태여야만 하는 경우(예: 이메일 초안 작성, 챗봇의 인사말, 긴 문장의 요약), 전체 문자열의 완벽한 일치 여부를 검사하는 것은 불가능하다. 이때 오라클은 문자열 전체가 아니라 출력이 반드시 지녀야 할 구체적이고 필수적인 ’속성(Properties)’의 포함 여부에 초점을 맞추어야 한다.</p>
<p>룰 기반 검사(Rule-based checks)는 미리 정의된 정규표현식(Regex), 문자열 길이 제한, 금지어 리스트, 특정 필수 키워드의 포함 여부 등 기계적으로 검증 가능한 규칙들을 통해 오라클의 결과를 빠르고 결정론적으로 판별한다.</p>
<ul>
<li><strong>긍정 오라클 제약 (Positive Constraints):</strong> “응답 문자열의 길이는 반드시 50자 미만이어야 하며, 픽스처로 주입된 주문 번호(예: ‘ORD-2025-999’)를 포함해야 한다.”</li>
<li><strong>부정 오라클 제약 (Negative Constraints / Safety):</strong> “프롬프트 인젝션 방지 지시어에 따라, 반환된 응답에는 시스템 내부 명령어인 <code>DROP TABLE</code>이나 사용자의 민감한 개인정보(주민등록번호 정규식 매칭)가 절대 포함되어서는 안 된다.”</li>
</ul>
<p>이러한 룰 기반 검사는 LLM의 어조 변화나 유의어 선택이라는 확률적 노이즈에 전혀 영향을 받지 않는 완벽한 기계적 평가 방식이므로, CI/CD 환경에서 파이프라인의 치명적인 오작동(보안 위반, 핵심 데이터 누락)을 즉시 차단하는 1차 방어선 오라클 역할을 훌륭히 수행한다.</p>
<h3>6.3 하이브리드 오라클: LLM-as-a-Judge와 G-Eval 메트릭의 엄격성 제어</h3>
<p>단순한 룰 기반 검사와 스키마 매칭만으로는 텍스트의 논리적 모순, 어조(Tone)의 일관성, 문맥의 매끄러움, 그리고 가장 중요한 환각(Hallucination)의 발생 여부와 같은 의미론적(Semantic) 특성을 딥 다이브하여 평가할 수 없다. 이러한 주관적이고 의미론적인 검증을 수행하기 위해 고안된 것이, 검증 대상 프롬프트의 출력을 평가하기 위해 별도의 강력한 평가용 LLM을 오라클로 사용하는 LLM-as-a-Judge 기법이다.</p>
<p>그러나 “평가를 담당하는 LLM 역시 확률적 모델이므로, 오라클 자체의 판별 기준이 흔들리는 ‘평가자 비결정성(Evaluator Nondeterminism)’ 문제가 발생하지 않는가?“라는 근본적인 딜레마가 제기된다.</p>
<p>이러한 재귀적인 비결정성 문제를 해결하고 평가용 LLM을 결정론적 오라클에 가깝게 통제하기 위해 고안된 것이 G-Eval과 같은 정밀 평가 프레임워크이다. G-Eval 메트릭은 평가 프롬프트를 “이 답변이 좋은가?“와 같은 모호한 질문 대신, 극도로 세분화되고 객관적인 평가 지표(Criteria)로 명세한다. 가장 핵심적인 메커니즘은 <code>strict_mode=True</code>와 같은 파라미터를 강제하여, 평가용 LLM이 자유로운 감상평을 작성하지 못하게 막고 출력을 오직 1(통과/성공) 또는 0(실패)의 이진(Binary) 상태로 강제 파싱하도록 설계한다는 점이다.</p>
<p>즉, “이 문서 요약본이 원본의 내용을 잘 반영하고 있는가?“라는 모호한 의미론적 평가 프롬프트를 사용하는 대신, “원본 문서 픽스처(Fixture)에 명시된 주요 수치 데이터 배열 <span class="math math-inline">[1500, 20%, 5.4]</span>가 생성된 요약본 내에 단위의 왜곡 없이 논리적으로 배치되어 있는가? 조건을 만족하면 1, 하나라도 왜곡되거나 누락되었으면 0을 출력하라“는 극도로 구체적이고 사실 검증(Fact-checking)에 기반한 프롬프트로 평가자 모델을 옥죈다. 이처럼 평가 지표를 원자 단위로 쪼개고 출력을 이진 구조로 강제함으로써, 확률적 언어 모델을 전통적인 소프트웨어 테스트의 결정론적 오라클로 전환하여 활용할 수 있게 된다.</p>
<h2>7. 실전 예제: 다단계 AI 비즈니스 파이프라인의 프롬프트 분해와 오라클 구축</h2>
<p>지금까지 논의한 프롬프트 분해(Decomposition), 테스트 대역(Test Doubles)을 통한 격리, 그리고 결정론적 오라클(Deterministic Oracle)의 개념을 총망라하여, 실제 AI 소프트웨어 개발 현장에서 복잡한 엔터프라이즈 비즈니스 로직을 어떻게 안전하게 설계하고 테스트하는지 구체적인 실전 아키텍처 예제를 통해 분석한다.</p>
<p><strong>[비즈니스 시나리오: 자율 환불 처리 AI 에이전트]</strong></p>
<p>전자상거래 플랫폼에서 고객의 불만 섞인 자연어 문의 텍스트를 입력받아, 사내의 복잡한 환불 정책 데이터베이스를 검색하고 조건 부합 여부를 계산한 뒤, 고객의 개별 상황에 맞게 환불 가능 여부와 후속 절차를 안내하는 챗봇 파이프라인이다.</p>
<p>이 거대한 파이프라인을 과거의 관행처럼 **단일 거대 프롬프트(Monolithic Prompt)**로 처리할 경우, 시스템 프롬프트는 아래와 같이 작성된다.</p>
<blockquote>
<p><em>“너는 친절한 고객 응대 에이전트다. 고객의 말을 듣고 어떤 상품을 왜 환불하려는지 파악해라. 단순 변심은 7일 이내만 가능하고 배송비 3000원이 차감된다. 제품 하자는 30일 이내 무상 환불이다. 고객의 조건이 정책에 맞는지 속으로 계산해보고, 정중하고 공감하는 어투로 최종 안내문을 작성하여 고객에게 전달해라.”</em></p>
</blockquote>
<p>이 거대 프롬프트 구조에서 LLM은 “의도 및 엔티티 추출 <span class="math math-inline">\rightarrow</span> 암묵적 정책 룰 매칭 <span class="math math-inline">\rightarrow</span> 최종 답변 생성“이라는 서로 다른 인지적 부하를 한 번의 API 호출에 감당해야 한다. 이 경우 모델은 고객의 감정에 공감하느라 정책 계산을 틀리게 적용하거나(배송비 누락), 정보가 부족함에도 불구하고 “모든 상품은 100일 내 환불 가능합니다“라며 존재하지 않는 정책을 환각(Hallucinate)으로 만들어내는 치명적인 오류를 높은 확률로 범하게 된다. 무엇보다 이 구조는 E2E 텍스트 입출력 구조이므로, 테스트가 실패했을 때 모델의 판단 로직 중 어느 부분이 고장 났는지 자동화된 디버깅이 불가능하다.</p>
<p>이를 공학적 원칙에 따라 3개의 독립적인 단위 모듈로 분해(Decomposition)하고, 각 단계를 완벽히 격리된 오라클로 테스트하는 아키텍처 전략은 다음과 같다.</p>
<h3>모듈 1: 의도 및 핵심 엔티티 추출 프롬프트 (Intent &amp; Entity Extractor)</h3>
<ul>
<li><strong>프롬프트의 단일 책임(Objective):</strong> 사용자의 자유 텍스트에서 불필요한 감정적 요소를 배제하고, 의도(Intent)를 분류하며, 상품 번호나 환불 사유 등 핵심 데이터를 정형화된 JSON으로 추출한다.</li>
<li><strong>설계 원칙:</strong> 이 프롬프트는 사용자에게 보여질 답변을 절대 생성하지 않는다. 철저히 백엔드 데이터 파서(Parser)로만 기능하며, Structured Outputs API를 사용하여 엄격한 JSON Schema를 강제한다.</li>
<li><strong>단위 테스트 격리와 결정론적 오라클 설계:</strong>
<ul>
<li><strong>주입된 입력 픽스처(Fixture):</strong> <code>"어제 받은 101번 구두 신어봤는데 발볼이 너무 꽉 껴서 아프네요. 도저히 못 신겠으니 가져가세요."</code></li>
<li><strong>오라클 평가(단위 테스트 Assert):</strong> LLM이 반환한 JSON 객체를 파싱하여, 아래의 결정론적 조건을 만족하는지 **속성 값 완벽 일치(Exact Match)**로 검증한다. 자유 생성 텍스트 요소가 없으므로 100% 결정론적인 논리 테스트가 보장된다.
<ul>
<li><code>assert output["intent"] == "REFUND_REQUEST"</code></li>
<li><code>assert output["product_id"] == "101"</code></li>
<li><code>assert output["reason_category"] == "SIZE_ISSUE"</code> (단순 변심 카테고리로 맵핑)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>모듈 2: 비즈니스 룰 엔진 및 지식 검색 (Business Logic &amp; Retrieval - LLM 미포함)</h3>
<ul>
<li><strong>목표:</strong> 모듈 1에서 결정론적 JSON으로 추출된 파라미터를 바탕으로, 내부 RDB의 고객 구매 이력과 사내 환불 정책 API를 호출하여 해당 고객의 실제 환불 가능 여부와 차감 배송비를 시스템적으로 계산한다.</li>
<li><strong>설계 원칙:</strong> 수학적 계산과 사내 규정의 적용은 확률적 언어 모델에 맡겨서는 안 되는 영역이다. 이 단계는 철저히 LLM을 배제하고 기존의 C#, Java, Python 등으로 작성된 전통적인 소프트웨어 결정 트리(Decision Tree) 코드로 구현한다. 오라클 검증 역시 전통적인 xUnit 프레임워크의 단위 테스트 기법을 사용하여 100%의 커버리지를 달성한다.</li>
</ul>
<h3>모듈 3: 최종 답변 생성 프롬프트 (Response Synthesizer)</h3>
<ul>
<li><strong>프롬프트의 단일 책임(Objective):</strong> 모듈 2에서 확정된 팩트(Fact) 데이터와 정책 지침을 바탕으로, 기업 브랜드 가이드라인에 맞춘 정중하고 공감하는 어조의 최종 고객 안내문을 자연어로 매끄럽게 합성(Synthesize)한다.</li>
<li><strong>단위 테스트 격리 구조 (스텁 주입):</strong> 이 최종 프롬프트의 품질을 단위 테스트하기 위해, 앞선 비결정적 단계(모듈 1)와 외부 데이터베이스 조회가 포함된 단계(모듈 2)의 실행을 코드 상에서 완전히 차단하고 그 결과를 <strong>스텁(Stub) 데이터로 강제 주입</strong>한다.
<ul>
<li><strong>스텁 데이터 픽스처(주입):</strong> <code>{"is_refundable": true, "refund_fee": "3000원", "policy_reason": "단순 변심 반품은 수령 후 7일 이내 가능하며 왕복 배송비가 부과됨"}</code></li>
<li><strong>프롬프트 구성:</strong> 스텁 데이터 블록 + 프롬프트 템플릿(시스템 페르소나 제어 지시어).</li>
</ul>
</li>
<li><strong>오라클 평가(단위 테스트 Assert):</strong> 이 모듈의 결과물은 자연어 자유 텍스트 영역이므로, 속성 기반의 룰 검사(Rule-based checks)와 엄격 모드가 켜진 LLM-as-a-Judge 오라클을 결합하여 다각도로 검증한다.
<ul>
<li><strong>결정론적 룰 기반 오라클:</strong> 반환된 문자열을 정규표현식으로 스캔하여, 스텁으로 주입된 필수 수치인 <code>"3000원"</code>이 문자열 내에 훼손 없이 반드시 포함되어 있는지 기계적으로 검사한다. 누락 시 실패 처리한다.</li>
<li><strong>의미론적 평가 오라클 (LLM-as-a-Judge):</strong> G-Eval 메트릭을 사용하여 평가 모델에게 “응답의 어조가 공감적이고 고객의 불만에 방어적으로 대응하지 않았는가?“라는 지표를 검증하게 하고, 그 결과를 <code>strict_mode=True</code> 옵션을 통해 1 또는 0의 이진값으로 반환하게 하여 CI 파이프라인의 통과 여부를 결정한다.</li>
</ul>
</li>
</ul>
<p>이처럼 하나의 복잡한 챗봇 두뇌를 3개의 역할로 명확히 분해(Decomposition)하고, 각 컴포넌트 간의 결합 부위에 스텁(Stub)과 구조화된 계약(JSON Schema)을 적절히 배치함으로써, 개발팀은 시스템을 완벽히 통제할 수 있게 된다. 챗봇의 인사말 어조를 수정하기 위해 모듈 3의 프롬프트를 변경하더라도, 그것이 모듈 1의 데이터 추출 정확도나 모듈 2의 비즈니스 룰 적용에 파괴적인 회귀(Regression)를 일으키지 않았음을 수 초 이내의 CI/CD 자동화 테스트를 통해 결정론적으로 확신할 수 있는 것이다. 이는 전통적인 소프트웨어 공학의 ‘테스트 주도 개발(Test-Driven Development, TDD)’ 철학을 프롬프트웨어 환경에 가장 완벽하고 실용적으로 이식한 방법론이다.</p>
<h2>8. 결언: 유지보수성과 신뢰성을 보장하는 프롬프트 아키텍처</h2>
<p>거대 프롬프트의 명시적 분해(Decomposition)와 이를 통한 단위 기능별 테스트 격리(Test Isolation)는 단순히 복잡한 코드를 보기 좋게 정리하기 위한 코드 스타일 수준의 권장 사항이 아니다. 이는 본질적으로 제어 불가능한 ’확률적 언어 모델의 비결정성’을 제어 가능한 ’결정론적 소프트웨어 엔지니어링의 영역’으로 편입시키기 위한 유일하고도 필수적인 아키텍처적 해법이다.</p>
<p>프롬프트를 다수의 모듈로 분해함으로써, 파이프라인 내부의 각 노드(Node)는 고유의 명확한 입력 형식과 예상 출력 스키마(API 계약)를 가지게 되며, 이는 곧 개발자가 각 노드에 테스트 대역(Test Doubles)을 주입하고 결정론적 오라클을 배치할 수 있는 물리적 근거를 제공한다. 더 나아가, 프로덕션 환경에서 문제가 발생했을 때 블랙박스화된 전체 시스템을 무작정 디버깅하는 대신, 실패를 일으킨 단일 프롬프트를 시스템 로그를 통해 정확히 특정하고, 그 시점에 저장된 입출력 데이터를 스텁(Stub)으로 활용하여 로컬 환경에서 즉각적으로 에러를 재현 및 수정할 수 있는 진정한 의미의 ‘단위 테스트 및 디버깅’ 사이클이 완성된다.</p>
<p>결과적으로, 철저히 분해되고 격리 테스트된 프롬프트 체인은 기저에 깔린 개별 기반 모델(Foundation Model)의 버전 업데이트나 성능 변화에 크게 종속되지 않는 강력한 강건함(Robustness)을 지니게 된다. 이는 끊임없이 변화하는 비즈니스 요구사항에 대응하기 위한 지속적인 프롬프트 최적화와 시스템 리팩토링 과정에서도 예측 불가능한 부작용(Side Effect)으로부터 엔터프라이즈급 AI 소프트웨어를 안전하게 보호하는 가장 견고한 공학적 기반이 될 것이다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>Software Engineering for LLM Prompt Development - arXiv.org, 2월 28, 2026에 액세스, https://arxiv.org/html/2503.02400v1</li>
<li>(PDF) Promptware Engineering: Software Engineering for LLM, 2월 28, 2026에 액세스, https://www.researchgate.net/publication/389580858_Promptware_Engineering_Software_Engineering_for_LLM_Prompt_Development</li>
<li>Software Engineering for Prompt-Enabled Systems - arXiv, 2월 28, 2026에 액세스, https://arxiv.org/html/2503.02400v2</li>
<li>Unit Testing AI Agents: Common Challenges and Solutions | newline, 2월 28, 2026에 액세스, https://www.newline.co/@zaoyang/unit-testing-ai-agents-common-challenges-and-solutions–0e337dd1</li>
<li>How Promptware Attacks Target AI Models Like Gemini, 2월 28, 2026에 액세스, https://www.vcsolutions.com/blog/understanding-promptware-threats-to-ai-models-like-gemini/</li>
<li>LLM Agents - Prompt Engineering Guide, 2월 28, 2026에 액세스, https://www.promptingguide.ai/research/llm-agents</li>
<li>Evaluating Text-to-Visual Generation with Image-to-Text Generation, 2월 28, 2026에 액세스, https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01435.pdf</li>
<li>The Modular Imperative: Rethinking LLMs for … - Nada Amin, 2월 28, 2026에 액세스, https://namin.seas.harvard.edu/pubs/lmpl-modularity.pdf</li>
<li>Testing LLM Prompts in Production Pipelines: A Practical Approach, 2월 28, 2026에 액세스, https://dev.to/stuartp/testing-llm-prompts-in-production-pipelines-a-practical-approach-349b</li>
<li>Why Prompt Engineering Is Becoming Software Engineering - Reddit, 2월 28, 2026에 액세스, https://www.reddit.com/r/PromptEngineering/comments/1q1uvo1/why_prompt_engineering_is_becoming_software/</li>
<li>What is chain of thought (CoT) prompting? - IBM, 2월 28, 2026에 액세스, https://www.ibm.com/think/topics/chain-of-thoughts</li>
<li>What Is Chain-of-Thought Prompting? - AWS, 2월 28, 2026에 액세스, https://aws.amazon.com/what-is/chain-of-thought-prompting/</li>
<li>How Chain of Thought (CoT) Prompting Helps LLMs Reason More, 2월 28, 2026에 액세스, https://www.splunk.com/en_us/blog/learn/chain-of-thought-cot-prompting.html</li>
<li>Chain-of-Thought Prompting, 2월 28, 2026에 액세스, https://learnprompting.org/docs/intermediate/chain_of_thought</li>
<li>Chain of Thought - DEV Community, 2월 28, 2026에 액세스, https://dev.to/abhishek_gautam-01/chain-of-thought-1pj6</li>
<li>Daily Papers - Hugging Face, 2월 28, 2026에 액세스, <a href="https://huggingface.co/papers?q=trial+decomposition">https://huggingface.co/papers?q=trial%20decomposition</a></li>
<li>llms.txt - AgentDock, 2월 28, 2026에 액세스, https://agentdock.ai/llms.txt</li>
<li>Paper page - Iterated Decomposition: Improving Science Q&amp;A by …, 2월 28, 2026에 액세스, https://huggingface.co/papers/2301.01751</li>
<li>Understanding LLM Scientific Reasoning through Promptings and, 2월 28, 2026에 액세스, https://arxiv.org/html/2505.01482v2</li>
<li>Advanced Prompt Engineering Techniques: Examples &amp; Best, 2월 28, 2026에 액세스, https://www.patronus.ai/llm-testing/advanced-prompt-engineering-techniques</li>
<li>Test - Docs by LangChain, 2월 28, 2026에 액세스, https://docs.langchain.com/oss/python/langchain/test</li>
<li>Comprehensive Guide to Software Testing: Mock Objects, Stubs, 2월 28, 2026에 액세스, https://medium.com/@adrialnathanael/comprehensive-guide-to-software-testing-mock-objects-stubs-and-test-isolation-00436ada77bf</li>
<li>Writing unit tests – Essential Software Engineering for Researchers, 2월 28, 2026에 액세스, https://imperialcollegelondon.github.io/grad_school_software_engineering_course/l2-02-testing_writing_unit_tests/index.html</li>
<li>LLM Testing: A Complete Guide for Application Developers - Comet, 2월 28, 2026에 액세스, https://www.comet.com/site/blog/llm-testing/</li>
<li>mocking-stubbing | Skills Marketplace - LobeHub, 2월 28, 2026에 액세스, https://lobehub.com/skills/aj-geddes-useful-ai-prompts-mocking-stubbing</li>
<li>Database unit testing | Simple Talk - Redgate Software, 2월 28, 2026에 액세스, https://www.red-gate.com/simple-talk/devops/database-devops/unit-testing-databases-and-you/</li>
<li>Understanding and Characterizing Mock Assertions in Unit Tests, 2월 28, 2026에 액세스, https://arxiv.org/html/2503.19284v1</li>
<li>Testing in isolation with mocks - Deno Docs, 2월 28, 2026에 액세스, https://docs.deno.com/examples/mocking_tutorial/</li>
<li>When to use stubs/mocks and when to use real objects in unit testing?, 2월 28, 2026에 액세스, https://stackoverflow.com/questions/48176358/when-to-use-stubs-mocks-and-when-to-use-real-objects-in-unit-testing</li>
<li>Prompt Testing: A Guide for Manual Testers in AI | Test IO Academy, 2월 28, 2026에 액세스, https://academy.test.io/en/articles/9233348-prompt-testing-a-guide-for-manual-testers-in-ai</li>
<li>Unshackling Your System Under Test: Shift-Left Testing Through, 2월 28, 2026에 액세스, https://specmatic.io/appearance/unshackling-your-system-under-test-shift-left-testing-through-dependency-isolation/</li>
<li>Large Language Models for Software Testing: A Research Roadmap, 2월 28, 2026에 액세스, https://www.researchgate.net/publication/395969592_Large_Language_Models_for_Software_Testing_A_Research_Roadmap</li>
<li>Prompts for Planning-AI Integration, 2월 28, 2026에 액세스, https://papers.ssrn.com/sol3/Delivery.cfm/f06a9439-a1b7-4d2e-8b5e-d3cce7b9c102-MECA.pdf?abstractid=5323915&amp;mirid=1</li>
<li>Solver-in-the-Loop: MDP-Based Benchmarks for Self-Correction and, 2월 28, 2026에 액세스, https://www.researchgate.net/publication/400237278_Solver-in-the-Loop_MDP-Based_Benchmarks_for_Self-Correction_and_Behavioral_Rationality_in_Operations_Research</li>
<li>Prompt-Based LLM Framework - Emergent Mind, 2월 28, 2026에 액세스, https://www.emergentmind.com/topics/prompt-based-llm-framework</li>
<li>Prompt Engineering of LLM Prompt Engineering : r/PromptEngineering, 2월 28, 2026에 액세스, https://www.reddit.com/r/PromptEngineering/comments/1hv1ni9/prompt_engineering_of_llm_prompt_engineering/</li>
<li>How AI and Prompt Engineering Empower Modern Software Testing, 2월 28, 2026에 액세스, https://talent500.com/blog/ai-prompt-engineering-software-testing/</li>
<li>LLM Testing in 2026: Top Methods and Strategies - Confident AI, 2월 28, 2026에 액세스, https://www.confident-ai.com/blog/llm-testing-in-2024-top-methods-and-strategies</li>
<li>A Turn-wise Analysis of Iterative LLM Prompting - OpenReview, 2월 28, 2026에 액세스, https://openreview.net/pdf/7229b62b0e96dc1c8309f3838f1d65aef986657f.pdf</li>
<li>Mutation Testing via Iterative Large Language Model-Driven, 2월 28, 2026에 액세스, https://www.computer.org/csdl/proceedings-article/icstw/2025/10962485/25XCK2eL3SU</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>