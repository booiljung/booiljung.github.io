<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:5.4 목(Mocking) 객체와 페이크(Fake) 응답을 활용한 테스트 환경 격리</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>5.4 목(Mocking) 객체와 페이크(Fake) 응답을 활용한 테스트 환경 격리</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 5. 유닛 테스트 기반의 확정적 검증 오라클 구축 기법</a> / <a href="index.html">5.4 목(Mocking) 객체와 페이크(Fake) 응답을 활용한 테스트 환경 격리</a> / <span>5.4 목(Mocking) 객체와 페이크(Fake) 응답을 활용한 테스트 환경 격리</span></nav>
                </div>
            </header>
            <article>
                <h1>5.4 목(Mocking) 객체와 페이크(Fake) 응답을 활용한 테스트 환경 격리</h1>
<p>인공지능(AI) 및 대형 언어 모델(LLM)을 소프트웨어 아키텍처의 핵심 컴포넌트로 통합할 때, 소프트웨어 공학 측면에서 맞닥뜨리는 가장 근본적이고도 치명적인 난제는 ’비결정성(Nondeterminism)’의 완벽한 통제이다. 전통적인 소프트웨어 테스트 패러다임은 동일한 입력(Input)이 주어졌을 때 시스템이 항상 동일한 출력(Output)을 반환해야 한다는 결정론적 대전제 위에서 성립한다. 그러나 입력된 프롬프트에 대해 확률적 샘플링과 가중치 연산을 거쳐 매번 미세하게, 혹은 완전히 다른 문장 구조나 JSON 형태를 생성해 내는 생성형 AI의 본질적 특성은 이러한 고전적 검증 체계를 근본부터 흔들어 놓는다.</p>
<p>이러한 비결정적 환경에서 외부 API 호출에 직접적으로 의존하는 통합 테스트(Integration Test)만으로 시스템의 안정성을 검증하려 시도하는 것은 치명적인 안티 패턴(Anti-pattern)이다. 실제 상용 LLM API(예: OpenAI, Anthropic 등)를 호출하는 테스트 코드는 테스트의 실행 시간을 밀리초(ms) 단위에서 분 단위로 지연시키며, 지속적 통합 및 배포(CI/CD) 파이프라인의 민첩성을 심각하게 훼손한다. 더욱이 API 토큰 소모에 따른 기하급수적인 비용 발생, 네트워크 지연 및 순단, 그리고 모델의 미세한 업데이트로 인한 간헐적 테스트 실패(Flaky Test)는 개발팀이 버그의 진원지를 파악하는 것을 불가능하게 만든다. 즉, 테스트가 실패했을 때 이것이 애플리케이션 내부 로직의 결함 때문인지, 아니면 단순히 LLM이 평소와 다른 어휘를 선택했거나 네트워크 타임아웃이 발생했기 때문인지 판별할 수 없는 오라클 문제(Oracle Problem)에 직면하게 되는 것이다.</p>
<p>결과적으로 AI 소프트웨어의 신뢰성을 수학적이고 공학적으로 담보하기 위해서는 테스트 대상 시스템(SUT, System Under Test)을 외부의 비결정적 모델로부터 물리적, 논리적으로 완벽히 격리(Isolation)하고, 철저하게 제어 가능한 결정론적 정답지(Deterministic Ground Truth)를 제공하는 오라클을 구축해야 한다. 본 절에서는 의존성 격리의 핵심 기법인 테스트 대역(Test Double)의 철학을 고찰하고, 그중에서도 목(Mock) 객체와 페이크(Fake) 객체의 본질적 차이를 분석한다. 특히 LLM 애플리케이션 환경에서 왜 전통적인 모킹 기법이 유지보수성의 재앙을 초래할 수 있는지 학술적, 실무적 관점에서 비판적으로 살펴보고, 페이크 응답을 활용하여 어떻게 확정적 검증 오라클을 견고하게 구축할 수 있는지 심층적으로 다룬다.</p>
<h2>1.  소프트웨어 테스트 격리의 이론적 배경과 AI 시스템의 특수성</h2>
<p>소프트웨어 컴포넌트를 독립적으로 검증하기 위해 주변 환경과 협력 객체(Collaborator)들을 통제하는 행위는 단위 테스트(Unit Testing)의 핵심이다. 제라드 메스자로스(Gerard Meszaros)는 그의 명저인 <em>xUnit Test Patterns</em>에서 테스트를 위해 실제 객체를 대체하는 모든 가상의 객체들을 통칭하여 ’테스트 대역(Test Double)’이라 정의하였으며, 이를 역할과 구현 방식에 따라 Dummy, Stub, Spy, Mock, Fake 등으로 엄격하게 분류하였다.</p>
<p>전통적인 웹 애플리케이션이나 마이크로서비스 아키텍처(MSA)에서는 데이터베이스나 서드파티 결제 게이트웨이와 같은 외부 의존성을 격리하기 위해 테스트 대역이 광범위하게 사용되어 왔다. 그러나 AI 소프트웨어 엔지니어링에서는 외부 의존성의 성격이 근본적으로 다르다. 데이터베이스 쿼리는 구조화된 SQL을 보내고 구조화된 Row 데이타를 반환받는 명확한 계약(Contract)을 가지지만, LLM 클라이언트는 자연어라는 비정형 데이터를 프롬프트로 전송하고, 다시 비정형 텍스트나 간헐적으로 형식이 깨진 JSON을 반환받는 매우 모호한 계약을 맺고 있다.</p>
<p>AI 에이전트(Agent)는 사용자의 입력을 받아 동적으로 프롬프트를 구성하고, 외부 도구(Tool)를 호출하기 위한 매개변수를 생성하며, 벡터 데이터베이스에서 관련 문서를 검색(Retrieval)하고, 이를 다시 컨텍스트에 주입하여 다단계 추론(Chain of Thought)을 수행하는 복잡한 제어 흐름을 가진다. 이러한 파이프라인의 각 구성 요소가 예상대로 동작하는지 검증하기 위해서는, 제어할 수 없는 외부 지능(LLM)의 응답을 우리가 완벽히 통제할 수 있는 정적 데이터로 대체해야 한다. 환경이 완벽히 격리되어야만, AI 응용 프로그램 자체의 제어 흐름, 프롬프트 템플릿 렌더링, 출력 파서(Output Parser)의 예외 처리 로직, 그리고 재시도(Retry) 메커니즘을 100% 결정론적으로 검증할 수 있기 때문이다.</p>
<p>격리된 테스트 환경을 구축할 때 개발자가 직면하는 가장 중요한 아키텍처적 결정은 “어떤 형태의 테스트 대역을 사용할 것인가“이다. 실무에서는 종종 목(Mock)과 페이크(Fake)가 혼용되어 사용되지만, 이 둘은 테스트의 유지보수성, 리팩토링에 대한 저항성, 그리고 실제 프로덕션 환경과의 일치도를 결정짓는 완전히 다른 패러다임을 의미한다.</p>
<h2>2.  테스트 대역(Test Double)의 심층 분석: 목(Mock)과 페이크(Fake)의 본질적 차이</h2>
<p>AI 소프트웨어의 단위 테스트에서 목(Mock)과 페이크(Fake)의 구조적 차이를 명확히 이해하는 것은 기술 부채(Technical Debt)를 예방하는 첫걸음이다. 두 기법 모두 테스트 환경을 외부 세계로부터 격리한다는 공통의 목적을 달성하지만, 그 목적을 달성하는 기저 메커니즘은 정반대의 궤적을 그린다.</p>
<h3>2.1  목(Mock) 객체: 행위 검증과 내부 구현 결합의 패러다임</h3>
<p>목(Mock) 객체는 시스템이 협력 객체와 상호작용하는 ‘행위(Behavior)’ 자체를 검증하기 위해 고안된 객체이다. 테스트 작성자는 사전에 목 프레임워크(예: 파이썬의 <code>unittest.mock</code>, 자바의 <code>Mockito</code>, C++의 <code>Typemock Isolator++</code>)를 사용하여 목 객체가 어떤 인자를 받아 호출될 것인지, 몇 번 호출될 것인지(Expectations), 그리고 그때 어떤 값을 반환할 것인지를 엄격하게 프로그래밍한다.</p>
<p>목 기반의 테스트는 철저하게 메서드 가로채기(Method Interception)와 리플렉션(Reflection), 혹은 몽키 패치(Monkey Patching) 기술에 의존한다. 개발자는 대상 객체의 공식적인 퍼블릭 인터페이스(Public Interface)를 통과하는 대신, 런타임에 클래스의 내부 실행 메서드나 네트워크 통신을 담당하는 하위 모듈을 직접 패치(Patch)하여 모의 응답을 강제 주입한다.</p>
<p>이러한 접근법은 테스트 코드를 매우 빠르게 작성할 수 있게 해 주며, 복잡한 인스턴스화 과정 없이 특정 함수의 반환값만을 정밀하게 통제할 수 있다는 장점이 있다. 예를 들어, LLM 클라이언트가 텍스트를 스트리밍하는 비동기 제너레이터를 모킹하여 네트워크 오류가 발생했을 때의 예외 처리 로직을 즉각적으로 테스트할 수 있다. 그러나 이 ’빠른 구현’의 이면에는 시스템의 내부 구조와 테스트 코드가 강력하게 결합(Tight Coupling)된다는 치명적인 대가가 따른다. 시스템의 리팩토링이나 외부 라이브러리의 버전 업데이트로 인해 클래스의 내부 구현이 조금이라도 변경되면, 시스템의 비즈니스 로직(결과)이 정상임에도 불구하고 행위(호출된 메서드의 이름이나 시그니처)가 달라졌다는 이유로 수많은 테스트가 연쇄적으로 실패하게 된다.</p>
<h3>2.2  페이크(Fake) 객체: 상태와 표준 인터페이스 기반의 경량 구현체</h3>
<p>반면, 페이크(Fake) 객체는 실제 프로덕션 코드와 동일한 인터페이스 규약(Contract)을 준수하지만, 프로덕션 환경에서 사용하기에는 적합하지 않은 단축키(Shortcut)나 인메모리(In-memory) 메커니즘을 사용하여 구현된 ’실제로 작동하는 객체(Working Implementation)’이다. 데이터베이스 분야에서 실제 물리적 디스크의 PostgreSQL 대신 메모리 내에서 작동하는 SQLite나 H2 데이터베이스를 테스트에 사용하는 것이 대표적인 페이크 객체의 활용 예이다.</p>
<p>LLM 응용 프로그램의 컨텍스트에서 페이크 객체는 네트워크를 통해 외부 모델 제공자(OpenAI, Google Gemini, Anthropic 등)의 API 서버로 HTTP 요청을 전송하는 네트워크 통신 계층만을 완전히 제거한, 완벽하게 기능하는 로컬 클라이언트 인스턴스이다. 페이크 객체는 초기화 시점에 배열 형태의 미리 정의된 가상 응답(Predefined Responses) 목록을 주입받으며, 애플리케이션 코드가 표준 인터페이스(예: <code>invoke</code>, <code>generate</code>, <code>chat</code>)를 통해 요청을 보낼 때마다 내부 배열에서 순차적으로 응답을 꺼내어 반환한다.</p>
<p>페이크 방식의 가장 핵심적인 철학은 <strong>테스트 코드가 시스템의 내부 구현 방식을 알 필요가 없다</strong>는 점이다. 테스트는 단지 실제 프로덕션에서 사용되는 것과 동일한 다형성(Polymorphism)을 가진 페이크 인스턴스를 주입(Dependency Injection)할 뿐이며, 시스템은 자신이 실제 LLM과 통신하고 있는지 페이크와 통신하고 있는지 알지 못한 채 비즈니스 로직을 수행한다. 이는 리팩토링에 대한 저항성을 극대화하여 코드를 안전하게 수정할 수 있는 토대를 제공한다.</p>
<p><img src="./5.4.0.0.0%20%EB%AA%A9Mocking%20%EA%B0%9D%EC%B2%B4%EC%99%80%20%ED%8E%98%EC%9D%B4%ED%81%ACFake%20%EC%9D%91%EB%8B%B5%EC%9D%84%20%ED%99%9C%EC%9A%A9%ED%95%9C%20%ED%85%8C%EC%8A%A4%ED%8A%B8%20%ED%99%98%EA%B2%BD%20%EA%B2%A9%EB%A6%AC.assets/image-20260228211509082.jpg" alt="image-20260228211509082" /></p>
<h2>3.  LLM 환경에서 전통적 모킹(Mocking)의 맹점과 안티 패턴</h2>
<p>이러한 두 패러다임의 차이는 AI 소프트웨어 개발이라는 특수한 도메인에서 매우 극적인 결과를 낳는다. 특히 최근 소프트웨어 엔지니어링 생태계에서 LLM 관련 서드파티 라이브러리(LangChain, LlamaIndex, OpenAI SDK 등)가 하루가 다르게 아키텍처를 뒤엎고 버전업을 단행하는 상황에서, 구시대적인 몽키 패치 기반의 모킹은 치명적인 시스템 붕괴의 원인이 된다.</p>
<h3>3.1  구현 결합(Implementation Coupling)이 초래하는 오탐(False Positive)의 재앙</h3>
<p>모킹의 한계를 극명하게 보여주는 사례가 파이썬의 대표적인 LLM 프레임워크인 LangChain의 메이저 업데이트 과정에서 발생한 아키텍처 변경 이슈이다. 초기 LangChain 0.x 버전에서 개발자들은 LLM 체인(Chain)에 프롬프트를 전달하고 응답을 받기 위해 파이썬의 매직 메서드인 <code>__call__</code>을 사용했다. 따라서 이 코드를 테스트하는 개발자들은 테스트 격리를 위해 자연스럽게 <code>unittest.mock.patch</code>를 사용하여 체인 인스턴스의 <code>__call__</code> 메서드를 가로채고 하드코딩된 딕셔너리를 반환하도록 모의 객체를 설정했다.</p>
<pre><code class="language-Python"># 전통적인 모킹(Mocking) 방식의 치명적인 한계를 보여주는 안티 패턴 예시
from unittest.mock import patch
from langchain.chains import Chain

def test_llm_chain_with_fragile_mock(client):
    # 경고: 시스템의 내부 구현체인 '__call__' 메서드에 직접적으로 패치를 가함.
    # 이는 테스트 대상 코드(SUT)가 내부적으로 __call__을 사용할 것이라는
    # 구현 세부사항에 대한 강력한 결합을 생성한다.
    with patch.object(Chain, "__call__", return_value={"output": "AI의 답변입니다."}):
        response = client.post(
            "/api/chat/",
            {"input": "질문"},
            content_type="application/json",
        )
    # 테스트는 성공하지만, 이는 시스템의 실제 안정성을 대변하지 못한다.
    assert response.json()["response"] == {"output": "AI의 답변입니다."}
</code></pre>
<p>문제는 LangChain 라이브러리가 1.0 버전으로 업데이트되면서 발생했다. 아키텍처가 전면 개편되며 <code>__call__</code> 메서드의 사용이 공식적으로 폐기(Deprecated)되었고, 대신 <code>invoke()</code> 메서드가 표준 인터페이스로 강제되었다. 이 업데이트가 프로덕션 서버에 반영되는 순간, 클라이언트 코드는 객체를 함수처럼 호출하려 시도하다가 <code>TypeError: object is not callable</code> 런타임 예외를 뱉으며 시스템을 다운시킨다.</p>
<p>그러나 위에서 작성된 모킹 기반의 단위 테스트는 이 치명적인 버그를 잡아내지 못한다. 왜냐하면 모킹 프레임워크가 여전히 <code>__call__</code> 메서드를 덮어쓰고 있어 프로덕션 코드의 오류를 강제로 우회해버리기 때문이다. <strong>즉, 테스트 코드는 완벽하게 파란 불(Pass)을 켜고 있지만, 실제 시스템은 붕괴되는 전형적인 오탐(False Positive) 상황이 연출된다.</strong> 이는 테스트 스위트가 프로덕션 코드의 건전성을 보장한다는 개발팀의 믿음을 근본적으로 배신하는 결과이며, 오라클(Oracle)이 정답지가 아닌 환상(Illusion)을 제공하는 셈이다.</p>
<h3>3.2  테스트 현실성의 상실(Loss of Test Realism)과 LLM의 환각 검증 불가</h3>
<p>두 번째 치명적인 단점은 모킹된 객체가 제공하는 응답이 지나치게 ’이상적(Ideal)’이라는 점이다. 모킹 프레임워크를 통해 개발자가 직접 작성한 반환값은 대개 애플리케이션의 파싱 로직이 완벽하게 처리할 수 있는 정제된 형태의 문자열이거나 정확한 JSON 딕셔너리이다.</p>
<p>그러나 실제 프로덕션 환경에서 LLM은 본질적으로 비결정적이다. 시스템이 JSON 반환을 요구했음에도 불구하고, 모델은 종종 앞뒤에 마크다운 블록(<code>json... </code>)을 붙이거나, “네, 요청하신 데이터는 다음과 같습니다“와 같은 불필요한 대화형 서술어를 추가하여 애플리케이션의 <code>json.loads()</code>를 여지없이 파괴한다. 철저하게 통제되고 정제된 이상적인 데이터만을 반환하는 모킹 테스트는, 이러한 LLM 특유의 포맷팅 파괴나 환각(Hallucination), 안전 필터에 의한 거절(Refusal) 메시지 등 극단적인 엣지 케이스를 마주했을 때 우리 시스템의 에러 핸들링 메커니즘이 어떻게 무너지는지를 사전에 포착할 수 없게 만든다.</p>
<h3>3.3  자동화된 AI 코드 생성 도구의 과도한 모킹 남용(Over-Mocking)</h3>
<p>최근에는 자율 코딩 에이전트(Coding Agents)나 LLM 자체가 소프트웨어의 단위 테스트를 자동으로 생성하는 연구와 실무 적용이 활발히 이루어지고 있다. 문헌 <em>Automated Test Generation Using Large Language Models</em>와 관련 연구들에 따르면, LLM을 통해 테스트 코드를 자동 생성할 때 가장 빈번하게 발생하는 문제점 중 하나가 바로 복잡한 종속성을 회피하기 위한 무분별한 모킹 기법의 남용이다.</p>
<p>초기 연구들은 GPT-4와 같은 최첨단 모델조차 단위 테스트 작성 시 인간 개발자보다 훨씬 더 많은 부분에 모킹을 적용하는 경향이 있음을 확인했다. 소프트웨어 공학의 대가인 켄트 벡(Kent Beck)은 이 현상에 대해 “[LLM은] 실제 객체를 그대로 사용해도 아무런 문제가 없는 상황에서조차 무작위로 모의 객체(Mock)를 도입하려는 결정을 내린다. 이는 AI 증강 코딩(Augmented Coding) 시대에 인간 개발자가 치러야 할 끊임없는 경계의 대가이다“라고 경고한 바 있다. 모킹이 남용된 자동 생성 테스트는 코드 커버리지(Code Coverage)라는 숫자를 손쉽게 끌어올릴 수는 있지만, 앞서 언급한 리팩토링의 어려움과 인터페이스 검증의 부재로 인해 장기적으로 시스템의 기술 부채를 가중시킨다.</p>
<p>논문 <em>Do LLMs generate test oracles that capture the actual or the expected program behaviour</em>는 이 현상의 근본 원인을 분석한다. LLM 기반의 테스트 생성 도구들은 시스템의 ’기대되는 목적(Expected Behavior)’을 이해하여 명세 기반의 오라클을 생성하기보다는, 단지 현재 작성되어 있는 코드의 ‘실제 동작(Actual Behavior)’ 흐름을 그대로 복사하여 회귀 오라클로 만드는 데 치중한다. 즉, 시스템에 잠재된 논리적 버그나 구현 오류마저도 모킹의 대상이 되어 기계적으로 정답 처리되는 위험성이 존재한다. 따라서 AI 시대를 맞이하여 결정론적 오라클을 확립하기 위해서는 구시대적인 내부 메서드 모킹 전략을 과감히 폐기하고, 아키텍처의 경계를 존중하는 페이크(Fake) 패러다임으로의 전면적인 전환이 요구된다.</p>
<h2>4.  페이크(Fake) 객체를 활용한 결정론적 오라클(Deterministic Oracle) 구축 원칙</h2>
<p>결정론적 오라클(Deterministic Oracle)은 어떠한 비결정적 노이즈 속에서도 시스템의 출력이 명세(Specification)와 일치하는지를 확정적으로 판별하는 궁극의 테스트 메커니즘이다. 비결정적인 확률 모델인 LLM 자체의 응답 품질이나 지능 수준을 ’정답(Ground Truth)’이라는 수학적 잣대로 평가하는 것은 불가능에 가깝다. 그러나 <strong>표준 인터페이스를 구현한 페이크(Fake) 객체로 네트워크 계층을 격리</strong>하면, 테스트 대상이 되는 우리 시스템의 애플리케이션 코드는 다시 고전적인 결정론적 시스템으로 환원된다.</p>
<p>즉, 우리가 검증해야 하는 본질적인 대상은 “LLM이 얼마나 훌륭한 대답을 생성했는가?“가 아니라, “페이크 LLM이 극한의 엣지 케이스 응답(예: 형식이 깨진 텍스트, 거절 메시지)을 뱉어냈을 때, 우리의 비즈니스 로직과 파서(Parser)가 이를 정확하게 해석하고, 예외를 처리하며, 올바른 상태로 전이(State Transition)하는가?“이다. 이러한 철학 위에서 구축되는 결정론적 오라클의 주요 유형과 페이크 기반 검증 전략은 표 1과 같다.</p>
<table><thead><tr><th><strong>오라클 유형 (Oracle Type)</strong></th><th><strong>격리 환경에서의 검증 대상 및 목적 (Verification Target)</strong></th><th><strong>페이크 객체 기반 결정론적 정답지 (Ground Truth) 설계 전략</strong></th></tr></thead><tbody>
<tr><td><strong>출력 오라클 (Output Oracle)</strong></td><td>페이크 LLM의 비정형 응답을 애플리케이션의 출력 파서가 올바른 구조체(JSON, Pydantic 모델 등)로 정밀하게 변환하는지 검증</td><td>예상되는 정확한 데이터 구조체 배열과 실제 파싱된 결과 객체 간의 수학적 동등성(\vert Expected \vert == \vert Actual \vert) 검사 및 타입 일치 확인</td></tr>
<tr><td><strong>행위 오라클 (Behavioral Oracle)</strong></td><td>다단계 에이전트(Agent)가 페이크 모델의 특정 함수 호출(Function Calling) 제안에 따라 적절한 도구(Tool)를 순서대로 호출하는지 검증</td><td>시스템 내부 상태 머신의 전이 기록, 로그 배열, 혹은 Observability Trace가 사전에 정의된 결정론적 실행 시퀀스 템플릿과 완전히 일치하는지 비교</td></tr>
<tr><td><strong>예외 오라클 (Exception Oracle)</strong></td><td>페이크 LLM이 마크다운 블록을 섞거나 불필요한 잡음을 넣은 응답, 또는 정책적 거절(Refusal) 메시지를 보냈을 때 시스템의 내결함성(Fault Tolerance) 검증</td><td>특정 커스텀 예외(예: <code>OutputParserException</code>) 발생 여부, 시스템 크래시 방어, 그리고 기본값 반환을 위한 Fallback 로직의 정상 작동 및 로그 기록 확인</td></tr>
<tr><td><strong>속성 기반 오라클 (Property-based Oracle)</strong></td><td>페이크 LLM의 다양한 출력값에 대해 애플리케이션의 후처리(Post-processing) 로직이 특정 불변성(Invariant)을 항상 만족시키는지 검증</td><td>파싱된 출력 배열이 특정 논리식 조건(예: 항상 오름차순 정렬, 개인정보 마스킹 처리 완료 등)을 위반하지 않는지 상태 머신의 속성을 정형 검증(Formal Verification)</td></tr>
</tbody></table>
<p>위의 오라클 모델은 단순한 입출력 비교를 넘어 시스템의 행동, 예외 처리, 그리고 속성을 다차원적으로 평가한다. 이를 구현하기 위해서는 테스트 코드가 시스템에 페이크 의존성을 주입(Dependency Injection)할 수 있는 아키텍처가 선행되어야 한다.</p>
<h2>5.  [실전 예제] 생성자 주입과 Fake 모델을 활용한 확정적 비즈니스 로직 오라클 구현</h2>
<p>이제 이론적 원칙을 바탕으로, 파이썬 환경에서 결정론적 정답지를 제공하는 오라클을 직접 구축해 본다. 본 실전 예제는 사용자의 자연어 발화를 분석하여 비즈니스 도메인에 맞는 특정 의도(Intent)와 신뢰도(Confidence) 점수를 JSON 형식으로 추출해 내는 AI 기반의 ’사용자 의도 분석기(Intent Extractor)’를 구현하고 검증하는 시나리오이다.</p>
<h3>5.1  아키텍처 설계: 의존성 주입(Dependency Injection)을 통한 결합도 최소화</h3>
<p>가장 중요한 아키텍처적 원칙은 서비스 로직이 특정 LLM 벤더의 API 클라이언트(예: <code>ChatOpenAI</code>)에 강하게 결합되지 않도록, 인터페이스(예: LangChain의 <code>BaseChatModel</code>)에 의존하게 만드는 것이다. 생성자 주입(Constructor Injection) 패턴을 사용하면, 프로덕션 환경에서는 실제 네트워크 통신 객체를, 단위 테스트 환경에서는 페이크(Fake) 객체를 유연하게 갈아 끼울 수 있다.</p>
<pre><code class="language-Python">import json
import logging
from typing import Dict, Any
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import HumanMessage, SystemMessage

logger = logging.getLogger(__name__)

class IntentExtractor:
    """사용자의 발화에서 의도를 추출하는 비즈니스 로직 컴포넌트."""
    
    def __init__(self, llm: BaseChatModel):
        # 외부 LLM 의존성을 생성자를 통해 주입받음 (테스트 용이성 확보)
        self.llm = llm 
        self.system_prompt = SystemMessage(
            content="당신은 데이터 추출기입니다. 사용자의 입력을 분석하여 반드시 "
                    "오직 {'intent': string, 'confidence': float} 형태의 "
                    "유효한 JSON 문자열만 응답하세요. 다른 설명은 생략하세요."
        )

    def extract_intent(self, user_input: str) -&gt; Dict[str, Any]:
        """비결정적 모델의 출력을 제어하여 결정론적 데이터로 반환한다."""
        messages = [self.system_prompt, HumanMessage(content=user_input)]
        
        try:
            # 프로덕션에서는 실제 API 호출, 테스트에서는 Fake 객체의 invoke가 실행됨
            response = self.llm.invoke(messages)
            
            # 모델의 응답(텍스트)을 시스템의 구조화된 데이터(딕셔너리)로 파싱
            parsed_data = json.loads(response.content)
            
            # 결정론적 비즈니스 규칙: 모델이 추정한 신뢰도가 0.5 미만이면 'unknown' 처리
            if parsed_data.get("confidence", 0.0) &lt; 0.5:
                logger.info("Low confidence intent detected. Overriding to 'unknown'.")
                parsed_data["intent"] = "unknown"
                
            return parsed_data
            
        except json.JSONDecodeError as e:
            # 방어적 프로그래밍: 모델이 JSON 형식을 파괴했을 때의 예외 처리 로직
            logger.error(f"Failed to parse LLM output: {response.content}")
            return {"intent": "error_parsing", "confidence": 0.0}
        except Exception as e:
            logger.error(f"Unexpected error during LLM invocation: {e}")
            return {"intent": "error_system", "confidence": 0.0}
</code></pre>
<p>이 코드는 SUT(System Under Test)의 완벽한 예시이다. 이 클래스 내부에는 프롬프트를 조립하는 로직, 외부 모델을 호출하는 로직, 문자열을 JSON으로 디코딩하는 로직, 그리고 신뢰도에 따른 분기(Branch) 로직과 예외 처리(Exception Handling) 로직이 모두 포함되어 있다. 이제 모킹 프레임워크를 전혀 사용하지 않고, 오직 페이크 객체만을 활용하여 이 복잡한 제어 흐름의 모든 분기를 100% 결정론적으로 검증하는 테스트 코드를 작성한다.</p>
<h3>5.2  결정론적 오라클을 이용한 페이크(Fake) 테스트 스위트 작성</h3>
<p>테스트 프레임워크로는 파이썬의 <code>pytest</code>를 활용하며, LangChain 커뮤니티 패키지에서 제공하는 <code>FakeListChatModel</code>을 주입하여 네트워크 지연이나 비용 없이 밀리초(ms) 단위로 실행되는 격리된 오라클을 구축한다.</p>
<pre><code class="language-Python">import pytest
from langchain_community.chat_models.fake import FakeListChatModel
from langchain_core.messages import AIMessage

# 앞서 작성한 IntentExtractor 임포트 가정

# =====================================================================
# 시나리오 1. 출력 오라클 (Output Oracle): 정상적인 JSON 파싱 흐름 검증
# =====================================================================
def test_extract_intent_success_oracle():
    # Arrange: 페이크 객체에 고정된 가짜 응답(Golden Response) 목록 주입
    # 네트워크 요청 대신 메모리에서 이 문자열을 반환하도록 설정
    expected_llm_output = '{"intent": "refund_request", "confidence": 0.95}'
    fake_llm = FakeListChatModel(responses=[expected_llm_output])
    extractor = IntentExtractor(llm=fake_llm)
    
    # Act: 비즈니스 로직 실행 (실제 API 호출이 없으므로 매우 빠르고 비용 제로)
    result = extractor.extract_intent("결제한 상품 환불해주세요")
    
    # Assert (Oracle): 파싱 로직이 정상 작동하여 예상된 파이썬 딕셔너리를 반환하는지 동등성 검사
    ground_truth = {"intent": "refund_request", "confidence": 0.95}
    assert result == ground_truth, f"Output Oracle Failed: Expected {ground_truth}, got {result}"


# =====================================================================
# 시나리오 2. 행위 오라클 (Behavioral Oracle): 비즈니스 로직 분기 검증
# =====================================================================
def test_extract_intent_low_confidence_override_oracle():
    # Arrange: LLM이 형식은 맞췄으나 낮은 신뢰도로 응답하는 엣지 케이스 시뮬레이션
    expected_llm_output = '{"intent": "general_inquiry", "confidence": 0.4}'
    fake_llm = FakeListChatModel(responses=[expected_llm_output])
    extractor = IntentExtractor(llm=fake_llm)
    
    # Act
    result = extractor.extract_intent("서비스가 별로네요")
    
    # Assert (Oracle): 시스템 내부의 비즈니스 룰(confidence &lt; 0.5)이 정상 작동하여
    # 모델의 원본 응답을 무시하고 'unknown'으로 상태를 덮어씌웠는지 행위 전이(State Transition) 검증
    ground_truth = {"intent": "unknown", "confidence": 0.4}
    assert result == ground_truth


# =====================================================================
# 시나리오 3. 예외 오라클 (Exception Oracle): LLM 환각 및 포맷 파괴 내결함성 검증
# =====================================================================
def test_extract_intent_json_decode_error_oracle():
    # Arrange: 모델이 시스템 지시를 무시하고 마크다운 블록과 서술어를 섞어서 반환하는 최악의 시나리오
malformed_output = "네, 사용자 발화를 분석한 결과입니다.\n```json\n{'intent': 'ask'}\n```\n추가로 필요한 사항이 있나요?"
fake_llm = FakeListChatModel(responses=[malformed_output])
extractor = IntentExtractor(llm=fake_llm)

# Act

result = extractor.extract_intent("질문있습니다")

# Assert (Oracle): 애플리케이션이 JSON 파싱 에러(Crash)로 인해 중단되지 않고,

# try-except 블록을 타서 사전에 정의된 안전한 폴백(Fallback) 상태를 반환하는지 예외 처리 방어력 검증

ground_truth = {"intent": "error_parsing", "confidence": 0.0}
assert result == ground_truth
</code></pre>
<p>이러한 페이크 기반의 격리 테스트 아키텍처는 프로덕션 코드 내부의 <code>__call__</code>이나 <code>invoke</code>와 같은 구체적인 프레임워크 통신 규약에 어떠한 가정도 하지 않는다. 페이크 모델 클래스는 라이브러리 개발자가 제공한 공식 인터페이스 규약을 완벽하게 구현한 상태이므로, LangChain이나 OpenAI SDK의 메이저 업데이트로 인해 인터페이스 구조가 변경될 경우 테스트 코드 자체가 런타임 혹은 컴파일 타임에 명백한 에러를 발생시키며 실패하게 된다. 이는 기존의 모킹(Mocking) 방식이 버전 업데이트로 인한 시스템 오류를 조용히 덮어버리며 오탐(False Positive)을 발생시켰던 위험성을 원천적으로 차단해 준다.</p>
<p>결과적으로 이 테스트 스위트는 외부 서비스의 클라우드 장애, 대역폭 제한, 토큰 고갈 등에 전혀 구애받지 않고 언제 어디서나 100% 동일한 실행 결과를 보장하는 진정한 결정론적 검증 오라클로 기능한다. 개발팀은 코드를 수정할 때마다 안심하고 초당 수천 개의 단위 테스트를 병렬로 실행할 수 있으며, 시스템의 견고함에 대해 절대적인 확신을 가질 수 있게 된다.</p>
<h2>6. 고급 격리 아키텍처: VCR 패턴과 기록/재생(Record/Replay) 기반의 페이크 자동화 전략</h2>
<p>개발자가 직접 <code>FakeListChatModel</code>에 하드코딩된 문자열을 주입하는 방식은 빠르고 직관적이지만, 대화 컨텍스트가 길어지거나 여러 도구(Tool)를 연달아 호출하는 다중 체인(Multi-step Chain) 환경에서는 개발자가 가짜 JSON 응답을 일일이 타이핑하는 데 한계가 따른다. 더욱이 개발자의 머릿속에서 상상하여 작성된 페이크 응답은 실제 LLM 파운데이션 모델이 반환하는 미묘한 뉘앙스, 띄어쓰기 패턴, 그리고 예기치 않은 구조적 변형을 완벽히 모사할 수 없어, 앞서 지적한 ‘테스트 현실성의 상실’ 문제로 회귀할 위험이 존재한다.</p>
<p>이러한 한계를 극복하고 페이크 데이터의 현실성과 유지보수성을 동시에 잡기 위해 도입된 진보된 아키텍처가 바로 <strong>VCR(Video Cassette Recorder) 패턴</strong>이다. VCR 기법은 단위 테스트 프레임워크에서 HTTP 네트워크 계층의 트래픽을 가로채어 파일 시스템에 저장하고, 후속 테스트에서 이를 재사용하는 기록 및 재생 메커니즘이다.</p>
<h3>6.1 VCR 패턴의 작동 메커니즘과 카세트(Cassette) 관리</h3>
<p>파이썬 생태계에서는 <code>vcrpy</code> 라이브러리가 대표적으로 활용되며, 그 동작 원리는 다음과 같다.</p>
<ol>
<li><strong>초기 실행(Recording Phase):</strong> 테스트를 처음 실행할 때, 시스템은 실제로 외부 LLM 제공자(예: OpenAI)의 API 엔드포인트로 네트워크 요청을 전송한다. 이때 VCR 프레임워크는 HTTP 요청(Request)의 페이로드(프롬프트 본문, 헤더, 온도 파라미터 등)와 API 서버로부터 수신한 전체 HTTP 응답(Response) 트래픽을 가로채어 디스크의 JSON 또는 YAML 파일(이를 ’카세트(Cassette)’라 부름) 형태로 상세히 기록한다.</li>
<li><strong>후속 실행(Replaying Phase):</strong> 이후 CI/CD 환경이나 개발자의 로컬 환경에서 동일한 테스트 스위트가 다시 실행될 때, VCR 프레임워크는 외부 네트워크 연결을 차단(Mocking the Socket)한다. 시스템이 HTTP 요청을 시도하면, VCR은 해당 요청의 페이로드가 이전에 기록된 카세트 파일 내의 요청 정보와 정확히 일치하는지 비교한다. 조건이 일치하면, VCR은 외부 서버로 가는 대신 카세트에 저장된 응답 데이터를 시스템에 그대로 반환(Replay)한다.</li>
</ol>
<p>이 VCR 아키텍처는 페이크(Fake) 객체의 철학을 네트워크 레이어의 하단으로 확장한 것이다. 비즈니스 로직 SUT의 입장에서는 실제 HTTP 통신을 통해 완벽한 LLM 응답을 받아오는 것처럼 보이지만, 실제로는 디스크에서 데이터를 즉각적으로 읽어오기 때문에 테스트가 눈에 띄게 빨라지고 결정론적 실행을 보장하게 된다.</p>
<h3>6.2 기초 모델의 업데이트와 회귀 테스트(Regression Test) 자동화</h3>
<p>VCR 기반 페이크 전략의 진가는 기초 파운데이션 모델의 메이저 버전 업데이트(예: GPT-4에서 GPT-4o로의 전환, Claude 3 Sonnet 도입 등) 시점에 극명하게 드러난다. 새로운 모델을 도입할 때 개발팀은 단지 VCR의 카세트 파일을 일괄 삭제하고 테스트 스위트를 재실행하여 새로운 LLM의 응답을 ’재기록(Re-record)’하기만 하면 된다.</p>
<p>재기록 과정에서 특정 테스트 케이스가 새 모델의 응답 형식 변화로 인해 실패한다면, 개발팀은 프로덕션 코드를 수정하여 새로운 모델의 특성에 맞게 파싱 로직을 보강해야 한다는 명확한 신호를 즉시 얻을 수 있다. 이는 개발자가 손으로 일일이 가짜 데이터를 유지보수하는 수고를 덜어주면서도, 프로덕션 환경에서 발생할 수 있는 잠재적 결함을 격리된 환경에서 안전하게 선제적으로 차단하는 훌륭한 회귀 테스트(Regression Test) 오라클 역할을 수행한다.</p>
<h3>6.3 관측성(Observability) 도구와 결합된 다단계 행위 검증</h3>
<p>단순한 입출력 비교를 넘어, 최신의 LLM 테스트 접근법은 시스템 내부에 관측성(Observability) 및 실행 추적(Execution Tracing) 도구를 결합하여 에이전트의 전체 의사결정 과정을 검증하는 단계로 진화하고 있다.</p>
<p>복잡한 AI 에이전트는 하나의 작업을 완료하기 위해 1) 컨텍스트 검색, 2) 중간 요약, 3) 도구 선택, 4) 최종 응답 생성이라는 다단계 워크플로우를 거친다. VCR을 통해 각 단계의 LLM 응답을 결정론적 페이크로 고정시켰다면, 우리의 오라클은 다음과 같은 시스템의 패턴을 정밀하게 검증해야 한다.</p>
<ul>
<li><strong>도구 호출 패턴 (Tool Usage Pattern):</strong> 에이전트가 특정 페이크 상황에서 불필요한 도구를 과도하게 호출하지 않고 최적의 순서로 API를 활용했는가?</li>
<li><strong>에러 전파 및 복구 패턴 (Error Propagation Pattern):</strong> 특정 도구가 실패했다는 페이크 응답이 주어졌을 때, 시스템이 그대로 크래시되지 않고 대안적인 방법론(Fallback)을 찾아 정상 궤도로 복구되었는가?</li>
<li><strong>메모리 컨텍스트 활용 (Context Usage):</strong> 에이전트의 메모리 저장소에 문맥이 정확히 기록되고 다음 프롬프트에 정상적으로 주입되었는가?</li>
</ul>
<p>이러한 행위 오라클(Behavioral Oracle)은 코드가 단순히 “결과를 맞혔다“에 만족하지 않고, “올바른 이유와 경로를 통해 결과에 도달했다“는 것을 증명함으로써 AI 애플리케이션에 예측 가능성과 설명 가능성(Explainability)을 부여한다.</p>
<h2>7. AI 애플리케이션 테스트 환경 격리 아키텍처 원칙 요약</h2>
<p>소프트웨어의 결함은 코드의 덩치가 커질수록, 외부 시스템과의 종속성이 강할수록 비선형적으로 증가한다. 특히 확률과 통계의 영역에 있는 대형 언어 모델을 결정론적인 비즈니스 로직과 결합할 때 발생하는 혼돈은 단위 테스트의 격리(Isolation) 없이는 결코 통제될 수 없다.</p>
<p>AI 소프트웨어 개발 주기에서 기술 부채를 통제하고 고품질의 신뢰성을 담보하기 위해서는, 단순히 구시대적인 모킹(Mocking) 문법을 가져다 쓰는 것을 넘어 다음의 아키텍처 원칙들을 근본적인 엔지니어링 표준으로 확립해야 한다.</p>
<ol>
<li><strong>내부 구현의 모킹을 엄격히 배제하고, 공인된 인터페이스를 구현한 페이크(Fake) 객체를 적극 활용하라.</strong> <code>unittest.mock.patch</code>와 같은 리플렉션 기반의 우회 기법은 외부 라이브러리의 아키텍처 변경을 숨기는 기만적인 오탐(False Positive)을 낳는다. 애플리케이션의 결합도를 낮추기 위해 객체 지향의 다형성(Polymorphism)과 의존성 주입(Dependency Injection)을 통한 페이크 객체 활용을 기본 원칙으로 삼아야 한다.</li>
<li><strong>LLM의 비결정적 지능을 테스트하려 들지 말고, 통제된 극단적 상황에 대한 시스템의 결정론적 방어 로직을 테스트하라.</strong> 우리의 오라클은 모델의 문장력이 아닌, 모델이 뱉어내는 최악의 쓰레기 데이터(Malformed Output)를 마주했을 때 우리 소프트웨어가 보여주는 내결함성(Fault Tolerance)과 예외 처리(Exception Handling) 능력을 증명하는 데 집중해야 한다.</li>
<li><strong>테스트 데이터 세트(Fixture)와 카세트(Cassette)를 주기적으로 프로덕션 환경의 현실과 동기화하라.</strong> 개발자가 하드코딩한 이상적인(Ideal) 가상 응답에만 매몰되지 않도록, VCR 패턴과 실행 추적(Tracing) 도구를 결합하여 실제 유저의 상호작용에서 추출한 날것의 데이터를 주기적으로 페이크 환경에 주입함으로써 회귀 테스트(Regression Test)의 커버리지를 넓혀야 한다.</li>
</ol>
<p>이러한 엄격한 환경 격리와 결정론적 오라클 설계 철학이 프로젝트 전반에 단단히 뿌리내릴 때 비로소, 개발팀은 끊임없이 진화하는 비결정적 파운데이션 모델의 파도 위에서도 절대 흔들리지 않는, 군사 수준의 신뢰성(Military-grade Reliability)을 갖춘 견고한 엔터프라이즈급 AI 소프트웨어를 구축해 낼 수 있을 것이다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>Effective Practices for Mocking LLM Responses During the Software, 2월 28, 2026에 액세스, https://medium.com/@vuongngo/effective-practices-for-mocking-llm-responses-during-the-software-development-lifecycle-73f726c3f994</li>
<li>Beyond Traditional Testing: Addressing the Challenges of Non, 2월 28, 2026에 액세스, https://dev.to/aws/beyond-traditional-testing-addressing-the-challenges-of-non-deterministic-software-583a</li>
<li>LLM Testing Methods | VoltAgent, 2월 28, 2026에 액세스, https://voltagent.dev/blog/llm-testing/</li>
<li>Test - Docs by LangChain, 2월 28, 2026에 액세스, https://docs.langchain.com/oss/python/langchain/test</li>
<li>Effective Practices for Mocking LLM Responses During the Software, 2월 28, 2026에 액세스, https://agiflow.io/blog/effective-practices-for-mocking-llm-responses-during-the-software-development-lifecycle/</li>
<li>Avoiding Mocks: Testing LLM Applications with LangChain in Django, 2월 28, 2026에 액세스, https://lincolnloop.com/blog/avoiding-mocks-testing-llm-applications-with-langchain-in-django/</li>
<li>Understanding LLM-Driven Test Oracle Generation - arXiv, 2월 28, 2026에 액세스, https://www.arxiv.org/pdf/2601.05542</li>
<li>(PDF) Verifiable LLM-Generated Test Oracles: Ensuring …, 2월 28, 2026에 액세스, https://www.researchgate.net/publication/398511554_Verifiable_LLM-Generated_Test_Oracles_Ensuring_Consistency_Correctness_and_Explainability_in_AI-_Assisted_Testing</li>
<li>Elevating Software Quality with Black Box Testing | Nitor Infotech, 2월 28, 2026에 액세스, https://www.nitorinfotech.com/blog/what-is-black-box-testing-types-advantages-and-how-it-works/</li>
<li>MAKERERE UNIVERSITY, 2월 28, 2026에 액세스, https://dissertations.mak.ac.ug/bitstream/handle/20.500.12281/21304/Mugenyi-CoCIS-Bachelors-2025.pdf?sequence=3&amp;isAllowed=y</li>
<li>The Unit in Unit Testing - InfoQ, 2월 28, 2026에 액세스, https://www.infoq.com/articles/unit-testing-approach/</li>
<li>Software Testing for BFSI: Methodologies &amp; Best Practices - SmartDev, 2월 28, 2026에 액세스, https://smartdev.com/software-testing-for-bfsi-methodologies-best-practices/</li>
<li>Unit Testing Best Practices: 9 Ways to Make Unit Tests Shine, 2월 28, 2026에 액세스, https://brightsec.com/blog/unit-testing-best-practices/</li>
<li>Fakes &amp; Mocks on Android: Well Partner, that depends., 2월 28, 2026에 액세스, https://dev.to/jameson/fakes-mocks-on-android-well-partner-that-depends-h6n</li>
<li>Should I use mock or fake for unit testing? : r/androiddev - Reddit, 2월 28, 2026에 액세스, https://www.reddit.com/r/androiddev/comments/odp02e/should_i_use_mock_or_fake_for_unit_testing/</li>
<li>Fake objects vs Mock objects - unit testing - Stack Overflow, 2월 28, 2026에 액세스, https://stackoverflow.com/questions/34288425/fake-objects-vs-mock-objects</li>
<li>Unit Testing Trends 2025: AI, LLMs &amp; Deep Isolation - Typemock, 2월 28, 2026에 액세스, https://www.typemock.com/whats-hot-in-unit-testing-mocking-for-2025-ai-llms-deep-isolation/</li>
<li>Do you guys mock everything in your Unit Tests? - Reddit, 2월 28, 2026에 액세스, https://www.reddit.com/r/ExperiencedDevs/comments/12je73u/do_you_guys_mock_everything_in_your_unit_tests/</li>
<li>Are Coding Agents Generating Over-Mocked Tests? An Empirical, 2월 28, 2026에 액세스, https://arxiv.org/html/2602.00409v1</li>
<li>(PDF) Automated Test Generation Using Large Language Models, 2월 28, 2026에 액세스, https://www.researchgate.net/publication/396042652_Automated_Test_Generation_Using_Large_Language_Models</li>
<li>(PDF) Do LLMs generate test oracles that capture the actual or the, 2월 28, 2026에 액세스, https://www.researchgate.net/publication/385318406_Do_LLMs_generate_test_oracles_that_capture_the_actual_or_the_expected_program_behaviour</li>
<li>Continuous Testing Tools for Agile and DevOps Success - CloudQA, 2월 28, 2026에 액세스, https://cloudqa.io/continuous-testing-in-agile-and-devops/</li>
<li>Automated Test Generation Using Large Language Models - MDPI, 2월 28, 2026에 액세스, https://www.mdpi.com/2306-5729/10/10/156</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>