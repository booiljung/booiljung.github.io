<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:1.6.4 확신도(Confidence Score)와 로그 확률(Logprobs) 데이터의 활용 및 한계</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>1.6.4 확신도(Confidence Score)와 로그 확률(Logprobs) 데이터의 활용 및 한계</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 1. AI 기반 소프트웨어 개발의 패러다임 변화와 비결정성(Nondeterminism)의 한계</a> / <a href="index.html">1.6 실전 개발에서의 '느낌적 코딩(Vibe Coding)'의 한계와 엔지니어링적 접근의 필요성</a> / <span>1.6.4 확신도(Confidence Score)와 로그 확률(Logprobs) 데이터의 활용 및 한계</span></nav>
                </div>
            </header>
            <article>
                <h1>1.6.4 확신도(Confidence Score)와 로그 확률(Logprobs) 데이터의 활용 및 한계</h1>
<p>인공지능(AI) 모델, 특히 거대 언어 모델(LLM)이 소프트웨어 엔지니어링의 핵심 도구로 자리 잡으면서, 자연어를 통해 코드를 생성하고 애플리케이션을 구축하는 ’바이브 코딩(Vibe Coding)’이라는 새로운 패러다임이 부상했다. 2025년 초 Andrej Karpathy에 의해 널리 알려진 이 개념은, 개발자가 특정 프로그래밍 언어의 문법을 깊이 이해하지 못하더라도 영어와 같은 자연어 프롬프트를 통해 컴퓨터에 명령을 내릴 수 있는 상태를 의미한다. 바이브 코딩은 최소 기능 제품(MVP)을 빠르게 구축하고 아이디어를 저비용으로 실험할 수 있게 함으로써 개발 생산성을 폭발적으로 향상시켰다.</p>
<p>그러나 이러한 생성형 AI 기반의 소프트웨어 개발은 필연적으로 비결정성(Nondeterminism)이라는 치명적인 기술적 부채를 수반한다. 사용자가 AI가 생성한 코드를 완벽하게 이해하거나 검증하지 않은 채 프로덕션 환경에 배포하는 행위는 심각한 보안 및 안정성 위협을 초래한다. AI 모델은 입력된 프롬프트와 컨텍스트에 따라 확률적으로 가장 그럴듯한 텍스트를 생성할 뿐, 해당 코드가 컴퓨터 과학적으로 안전한지, 비즈니스 로직에 부합하는지 결정론적으로 보장하지 않는다. 그 결과, 입력값 검증 누락, 크로스 사이트 스크립팅(XSS), SQL 인젝션(CWE-94), OS 명령어 삽입(CWE-78), API 키 하드코딩 등 고전적이고 치명적인 취약점들이 AI 생성 코드에 무작위로 혼입되는 현상이 빈번하게 보고되고 있다.</p>
<p>AI가 작성한 코드가 프로덕션에 도달하기 전, 모델의 출력을 정량적으로 평가하고 통제하기 위한 엔지니어링적 접근이 절실해졌다. 그 첫 번째 수학적 방어선으로 주목받은 것이 바로 모델의 내부 확률 분포 데이터를 추출하여 불확실성을 측정하는 ’로그 확률(Logprobs)’과 이를 기반으로 한 ‘확신도(Confidence Score)’ 지표이다. 본 절에서는 LLM의 로그 확률이 산출되는 기술적 원리와 이를 소프트웨어 파이프라인에 통합하는 구체적인 방법론을 심도 있게 분석한다. 나아가 인간 피드백 기반 강화학습(RLHF)이 초래하는 교정 오류(Calibration Error)와 확률적 지표의 구조적 한계를 규명하고, 이를 극복하기 위해 결정론적 정답지를 제공하는 오라클(Oracle)을 구현하는 실전 예제를 제시한다.</p>
<h2>1.  로그 확률(Logprobs)의 수학적 원리와 생성 매커니즘</h2>
<p>거대 언어 모델은 근본적으로 주어진 입력 시퀀스(컨텍스트)를 바탕으로 어휘 사전(Vocabulary)에 존재하는 수만 개의 토큰 중에서 다음 위치에 등장할 토큰의 확률 분포를 계산하는 자기회귀(Autoregressive) 통계 모델이다. 신경망의 마지막 계층은 각 토큰에 대한 원시 점수인 로짓(Logits)을 출력하며, 이 로짓은 소프트맥스(Softmax) 함수를 통과하여 <span class="math math-inline">0</span>과 <span class="math math-inline">1</span> 사이의 확률값 <span class="math math-inline">p</span>로 변환된다.</p>
<p>그러나 실제 LLM API 및 추론 엔진은 연산의 안정성과 효율성을 위해 이 절대 확률값 <span class="math math-inline">p</span> 대신, 확률에 자연로그를 취한 로그 확률(Logarithmic Probabilities), 즉 Logprobs를 계산하여 반환한다.</p>
<h3>1.1  부동소수점 언더플로우 방지와 로그 공간 연산</h3>
<p>확률 <span class="math math-inline">p</span>는 정의상 <span class="math math-inline">0 &lt; p \le 1</span>의 범위를 가진다. 따라서 여기에 자연로그를 취한 로그 확률 <span class="math math-inline">\ln(p)</span>는 항상 <span class="math math-inline">-\infty</span>에서 <span class="math math-inline">0</span> 사이의 음수 값을 갖게 된다. 로그 확률이 <span class="math math-inline">0.0</span>에 가까울수록 해당 토큰이 생성될 확률이 100%에 근접함을 의미하며, 값이 음의 방향으로 커질수록 확률이 기하급수적으로 낮아짐을 의미한다.</p>
<p>소프트웨어 시스템에서 토큰의 실제 생성 확률(Probability)을 구하여 확신도 퍼센티지로 표출하기 위해서는, 반환된 로그 확률에 지수 함수(Exponential function)를 역으로 적용해야 한다. 이를 수식으로 나타내면 <span class="math math-inline">p = \exp(\text{logprob})</span>가 된다. 예를 들어, 특정 출력 토큰의 로그 확률이 <span class="math math-inline">-0.6931498</span>로 반환되었다면, 시스템은 <span class="math math-inline">\exp(-0.6931498)</span> 연산을 수행하여 약 0.5, 즉 50%의 확신도를 도출할 수 있다.</p>
<p>언어 모델이 절대 확률 대신 로그 확률을 기본 지표로 취급하는 가장 핵심적인 이유는 결합 확률(Joint Probability)을 계산할 때 발생하는 컴퓨터 구조상의 부동소수점 한계 때문이다. 긴 문장이나 코드 블록을 생성할 때, 전체 시퀀스의 생성 확률은 개별 토큰들이 순차적으로 생성될 조건부 확률들의 연속적인 곱으로 계산된다. 수십, 수백 개의 토큰 확률(1 미만의 소수)을 계속해서 곱하게 되면 결과값은 순식간에 컴퓨터가 표현할 수 있는 부동소수점의 최솟값을 뚫고 0으로 수렴하는 언더플로우(Underflow) 현상을 일으킨다.</p>
<p>하지만 로그 공간(Log-space)에서는 로그의 수학적 성질에 따라 곱셈 연산이 덧셈 연산으로 변환된다. <span class="math math-inline">\log(p_1 \times p_2 \times \dots \times p_n) = \log(p_1) + \log(p_2) + \dots + \log(p_n)</span> 공식을 적용하면, 단순히 개별 토큰의 로그 확률을 합산하는 것만으로 시퀀스 전체의 결합 확률을 안정적으로 계산할 수 있다. 이는 모델의 출력을 평가하고 순위를 매기거나(Scoring and ranking), 최적의 텍스트 생성을 선택하는 데 있어 필수적인 수학적 기반을 제공한다.</p>
<table><thead><tr><th><strong>측정 지표</strong></th><th><strong>수학적 표기 및 설명</strong></th><th><strong>값의 범위</strong></th></tr></thead><tbody>
<tr><td>토큰 확률 (<span class="math math-inline">p</span>)</td><td>컨텍스트가 주어졌을 때 다음 토큰이 등장할 절대 확률</td><td><span class="math math-inline">0 &lt; p \le 1</span></td></tr>
<tr><td>로그 확률 (<span class="math math-inline">\text{logprob}</span>)</td><td><span class="math math-inline">\ln(p)</span> (자연로그를 취한 확률값)</td><td><span class="math math-inline">-\infty &lt; \text{logprob} \le 0</span></td></tr>
<tr><td>확신도 복원 수식</td><td><span class="math math-inline">p = \exp(\text{logprob})</span></td><td><span class="math math-inline">0 &lt; p \le 1</span></td></tr>
<tr><td>시퀀스 결합 확률</td><td><span class="math math-inline">\sum_{i=1}^{N} \text{logprob}_i</span> (개별 로그 확률의 단순 합산)</td><td><span class="math math-inline">-\infty &lt; \text{Joint} \le 0</span></td></tr>
</tbody></table>
<h2>2.  프로덕션 환경에서의 로그 확률 활용 기법과 데이터 정합성 보장</h2>
<p>소프트웨어 엔지니어링 파이프라인에서 최신 LLM API(예: OpenAI API)를 호출할 때, 개발자는 <code>logprobs</code> 매개변수를 활성화하여 모델이 생성한 각 토큰의 확률 데이터를 추출할 수 있다. 이때 <code>top_logprobs</code>라는 추가 매개변수를 설정하면, 모델이 최종적으로 선택하여 텍스트로 내보낸 토큰뿐만 아니라, 해당 위치에서 후보로 고려되었던 가장 확률이 높은 대안 토큰들의 목록과 각각의 로그 확률까지 함께 반환받을 수 있다. 이러한 데이터는 모델의 내부적 확신도를 가늠하고, 결정론적 시스템과 융합하기 위한 중요한 판단 근거가 된다.</p>
<h3>2.1  강제 구조화 출력(Structured Outputs)과 필드 단위 확신도 측정</h3>
<p>AI를 비즈니스 로직에 통합할 때 가장 널리 사용되는 기법은 비정형 자연어를 JSON과 같은 기계 가독형 강제 구조화 데이터로 변환하는 것이다. 대규모 언어 모델은 이러한 분류 및 데이터 추출 작업에 탁월한 성능을 보이지만, 모델이 생성한 JSON의 각 필드 값이 얼마나 정확한지 확신하기는 어렵다. 로그 확률은 이러한 구조화 출력의 신뢰도를 필드 단위로 정밀하게 평가하는 메커니즘을 제공한다.</p>
<p>가령, 기업의 송장(Invoice) 텍스트를 파싱하여 특정 문서가 위조되었는지 여부를 판별하는 <code>{"is_fraudulent": true}</code>라는 불리언(Boolean) 값을 추출한다고 가정해 보자. 만약 AI가 <code>true</code>를 반환했다 하더라도, 그 로직의 기반이 확고한 추론인지 아니면 통계적 환각에 의한 찍기인지 시스템은 알 수 없다. 이때 개발자는 반환된 응답 객체에서 JSON 키-값 쌍 중 값(Value)에 해당하는 토큰의 로그 확률을 검색한다.</p>
<p>자바스크립트(Node.js) 환경을 예로 들면, <code>result.logprobs</code> 배열을 순회하며 대상 토큰(<code>true</code> 또는 <code>false</code>)의 로그 확률을 찾고, <code>Math.exp()</code> 함수를 적용하여 확신도를 0과 1 사이의 확률로 환산하는 로직을 구현할 수 있다. 이렇게 도출된 확신도 점수(Confidence Score)가 시스템에 사전 정의된 엄격한 임계값(예: 0.95)을 넘지 못할 경우, 해당 JSON 응답 전체를 기각하거나 인간 검토자에게 할당하는 1차 방어선(Short-circuiting)으로 활용할 수 있다. 이는 AI 모델의 확률적 텍스트 생성 결과를 결정론적 워크플로우 내에서 통제하기 위한 가장 기초적이고 효율적인 방법이다.</p>
<h3>2.2  시계열 분류(Time-Series Classification)를 통한 환각(Hallucination) 탐지</h3>
<p>최근의 연구 동향은 개별 토큰의 로그 확률값을 단편적으로 확인하는 수준을 넘어, 생성된 텍스트 전체의 로그 확률 흐름을 하나의 시계열 데이터(Time-series data)로 간주하고 모델의 환각을 동적으로 탐지하는 방법론으로 진화하고 있다. 이러한 ‘불확실성 기반(Uncertainty-based)’ 환각 탐지의 대표적인 사례가 바로 HALT 프레임워크이다.</p>
<p>HALT(Hallucination Detection using Time-Series Classification for LLMs) 모델은 기존의 방식들처럼 LLM이 생성한 텍스트의 의미를 외부 지식 베이스와 비교하거나, 또 다른 LLM을 평가자(LLM-as-a-Judge)로 사용하여 텍스트의 일관성을 묻는 무거운 과정을 거치지 않는다. 대신, 블랙박스 타겟 LLM이 반환하는 `` 형태의 로그 확률 행렬 <span class="math math-inline">\ell_{1:T}</span>만을 특징(Feature)으로 추출하여, 이를 순환 신경망 구조인 GRU(Gated Recurrent Unit)에 통과시켜 환각 여부를 이진 분류한다.</p>
<p>이러한 접근법이 성공할 수 있는 이론적 배경은, 모델이 환각을 일으키고 거짓 정보를 지어낼 때 내부 확률 분포에 ’지문(Signature)’과 같은 고유한 교정 편향(Calibration Bias)이 발생하기 때문이다. 모델이 정답에 대한 확신이 없을 때, 최종 선택된 토큰의 로그 확률이 갑자기 요동치거나, 대안이 되는 <code>Top-k</code> 토큰들 사이로 확률 질량(Probability mass)이 넓게 분산되면서 엔트로피(Entropy)가 비정상적으로 상승하는 현상이 관찰된다. 각 LLM은 고유한 어휘 사전과 로짓 분포, 훈련 데이터를 가지고 있으므로 이러한 확률 분포의 변동 패턴은 모델마다 다르지만, 시계열 분석 모델은 이를 정밀하게 포착해낼 수 있다.</p>
<p>실험 결과에 따르면, 로그 확률 행렬만을 이용한 HALT 프레임워크는 텍스트 자체를 분석하는 최신 미세 조정 인코더(예: modernBERT-base) 기반 방법론에 비해 매개변수 크기가 30배나 작음에도 불구하고 환각 탐지 성능을 능가하며, 추론 속도를 60배 이상 향상시키는 혁신적인 성과를 보여주었다. 이는 자연어 처리의 의미론적 접근을 배제하고, 순수하게 통계 모델의 내부 지표 역학만을 분석하여 비결정적 오류를 잡아내는 공학적 승리라 할 수 있다.</p>
<h2>3.  확률적 지표의 치명적 한계: 교정 오류(Calibration Error)와 ECE의 수학적 정의</h2>
<p>로그 확률을 이용한 확신도 산출과 환각 탐지 기법은 논리적으로 매우 타당해 보이지만, 실제 프로덕션 환경의 소프트웨어 개발에서 이를 전적으로 신뢰하기에는 심각한 구조적 한계가 존재한다. 그 근본 원인은 현대의 고도로 정렬된(Aligned) 거대 언어 모델들이 자신의 실제 성능과 내부 확률 사이에 거대한 괴리를 보이는 ‘교정 오류(Miscalibration)’ 현상을 겪고 있기 때문이다.</p>
<p>모델 교정(Model Calibration)이란, 시스템이 예측한 확률(확신도)과 실제 현실에서의 정답률(정확도)이 얼마나 일치하는가를 나타내는 개념이다. 이상적인 모델이 특정 문제들에 대해 90%의 로그 확률 확신도를 보였다면, 그 문제들을 모아 평가했을 때 실제로 90%의 정답을 맞혀야 한다. 그러나 모델이 99% 확신한다고 출력했지만 실제 정답률은 50%에 불과하다면, 이 로그 확률은 소프트웨어의 조건 분기문을 제어하는 기준으로 결코 사용될 수 없다.</p>
<p>이러한 모델의 확신도와 실제 정확도 사이의 괴리를 정량적으로 측정하는 가장 표준적인 수학 지표가 바로 예상 교정 오차(Expected Calibration Error, ECE)이다.</p>
<h3>3.1  예상 교정 오차(Expected Calibration Error)의 산출 방식</h3>
<p>ECE는 모델의 예측 결과들을 확률 값의 크기에 따라 여러 개의 동일한 구간(Bin)으로 나누고, 각 구간 내에서 계산된 평균 확신도와 실제 경험적 정확도의 절대적 차이를 가중 평균하여 산출한다. 완벽하게 교정된 모델의 ECE 값은 0이 되며, 값이 커질수록 모델이 심각한 과신(Overconfidence) 또는 과소신뢰(Underconfidence) 상태에 빠져 있음을 의미한다.</p>
<p>ECE의 수학적 공식은 다음과 같이 정의된다:<br />
<span class="math math-display">
\text{ECE} = \sum_{m=1}^{M} \frac{\vert B_m \vert}{n} \left\vert \text{acc}(B_m) - \text{conf}(B_m) \right\vert
</span></p>
<table><thead><tr><th><strong>변수 / 기호</strong></th><th><strong>수학적 의미</strong></th></tr></thead><tbody>
<tr><td><span class="math math-inline">M</span></td><td>전체 <span class="math math-inline">0</span>~<span class="math math-inline">1</span> 확률 범위를 나눈 구간(Bin)의 총 개수</td></tr>
<tr><td><span class="math math-inline">B_m</span></td><td><span class="math math-inline">m</span>번째 구간에 속하는 예측(토큰 생성)들의 집합</td></tr>
<tr><td><span class="math math-inline">\vert B_m \vert</span></td><td><span class="math math-inline">m</span>번째 구간에 포함된 총 예측 데이터의 개수</td></tr>
<tr><td><span class="math math-inline">n</span></td><td>평가에 사용된 전체 샘플 예측의 총수</td></tr>
<tr><td><span class="math math-inline">\text{acc}(B_m)</span></td><td><span class="math math-inline">m</span>번째 구간 내 예측들의 실제 정답률(Accuracy)</td></tr>
<tr><td><span class="math math-inline">\text{conf}(B_m)</span></td><td><span class="math math-inline">m</span>번째 구간 내 예측들이 기록한 평균 로그 확률 기반 확신도</td></tr>
</tbody></table>
<p>일반적인 기계 학습의 이진 분류나 소규모 다중 클래스 분류에서는 위의 전통적인 ECE 공식이 잘 작동하지만, 10만 개 이상의 어휘를 다루는 거대 언어 모델에서는 상위 1개(Top-1) 예측에만 초점을 맞추는 기존 ECE가 모델의 전체 확률 분포 품질을 제대로 대변하지 못한다는 비판이 존재한다. 이를 보완하기 위해 모델 어휘 사전의 모든 토큰에 할당된 확률값을 각각의 구간으로 재배치하여 전체 확률 분포의 교정 상태를 평가하는 Full-ECE와 같은 확장된 측정 기준(Metric for token-level calibration)도 연구되고 있다. 어떠한 지표를 사용하든, 결론은 최신 LLM들의 교정 상태가 프로덕션 수준의 신뢰성을 담보하기에는 심각하게 망가져 있다는 점이다.</p>
<h2>4.  인간 피드백 기반 강화학습(RLHF)이 유발하는 확률의 왜곡과 모드 붕괴</h2>
<p>연구자들을 당혹스럽게 만든 사실은, 순수하게 비지도 학습(Unsupervised Pre-training)만 거친 기저 모델(Base Model)들은 예상외로 내부 조건부 확률이 매우 잘 교정되어(Remarkably well-calibrated) ECE 값이 낮게 유지된다는 점이다. 그러나 모델을 인간의 지시에 복종하고 대화형으로 응답하도록 만드는 정렬(Alignment) 과정, 특히 인간 피드백 기반 강화학습(RLHF)을 적용하는 순간 모델의 교정 상태는 처참하게 붕괴되며 극한의 과신(Overconfidence) 상태로 전락한다.</p>
<p>실제로 LLaMA-2 기저 모델의 ECE는 0.032에서 0.228 사이를 기록하며 양호한 수치를 보였지만, RLHF를 적용한 LLaMA-2-Chat 버전에서는 ECE가 0.134에서 0.382로 치솟으며 로그 확률 데이터의 신뢰성이 완전히 파괴되는 현상을 보였다. GPT-4나 Claude와 같은 최상위 폐쇄형 모델들 역시 RLHF 미세 조정을 거치면서 출력 확률이 실제 정답률을 전혀 반영하지 못하는 상태가 되었다.</p>
<h3>4.1  ’생각’과 ’검열’의 충돌, 그리고 보상 모델의 편향</h3>
<p>RLHF가 로그 확률을 망가뜨리는 근본 원인은 모델이 훈련 과정에서 겪는 인지적 모순과 보상 모델(Reward Model)의 구조적 편향에 기인한다. RLHF 파이프라인은 크게 지도 미세 조정(SFT), 보상 모델 훈련, 그리고 근접 정책 최적화(PPO)를 통한 강화학습으로 구성된다. 이때 RL 알고리즘에 프록시(Proxy) 보상을 제공하는 보상 모델은 인간 평가자들의 채점 데이터를 바탕으로 학습된다.</p>
<p>문제는 인간 평가자들이 텍스트를 평가할 때 정답의 사실적 정확성(Accuracy)보다는, 문맥이 매끄럽고 어조가 당당하며 형식적으로 완벽해 보이는 ’확신에 찬 응답’에 더 높은 점수를 부여하는 심리적, 인지적 편향(Laziness bias)을 갖는다는 점이다. 보상 모델은 이러한 인간의 편향을 고스란히 학습하여, 모델이 생성한 답변의 실제 품질과 무관하게 무조건 높은 확신도를 표출하는 응답(High-confidence responses)에 체계적으로 과도한 보상을 제공한다(Systematic bias).</p>
<p>결과적으로 PPO 과정에서 메인 정책(Policy) 모델은 높은 보상을 얻기 위해 스스로의 불확실성을 억압하고 인간이 선호하는 특정 패턴으로만 정답을 확정 짓도록 강요받는다. 이 과정에서 모델은 사실 관계를 바탕으로 ’사고(Think)’하는 본연의 확률 분포와, 높은 보상을 받기 위해 스스로의 생각을 ’검열(Police its own thoughts)’하고 확신에 찬 어조로 포장하려는 강박 사이에서 충돌을 일으킨다.</p>
<h3>4.2  모드 붕괴(Mode Collapse)와 엔트로피의 소실</h3>
<p>이러한 가혹한 최적화 과정은 결국 확률 분포의 ‘모드 붕괴(Mode Collapse)’ 현상을 초래한다. 모델은 오답을 말할 때조차 특정 토큰에 확률 질량의 대부분을 할당해 버리며, 텍스트 생성의 다양성과 확률 분포의 엔트로피(Entropy)가 극단적으로 감소한다. RLHF 모델들이 출력하는 로그 확률은 더 이상 모델 내부의 ’진정한 불확실성’이나 ’추론의 강도’를 대변하는 통계적 지표가 아니라, 단지 강화학습의 보상 스칼라값을 극대화하기 위해 인위적으로 부풀려진 수치(Over-optimized policies)에 불과하게 된다.</p>
<p>이를 해결하기 위해 학계에서는 SFT 단계에서의 교정, 보상 모델 자체의 편향을 수정하는 PPO-M(PPO with Calibrated Reward Modeling), 그리고 PPO 훈련 중 보상 스코어에 직접 개입하여 과신을 억제하는 PPO-C 패널티 항 추가 등 다양한 교정 인식 강화학습(Calibration Aware Reinforcement Learning) 기법이 연구되고 있다. 그러나 현재 프로덕션에 투입되는 상용 프론티어 AI 모델들은 여전히 이 문제에서 자유롭지 못하다.</p>
<h2>5.  언어적 확신도(Verbalized Confidence)의 부상과 내재적 모순</h2>
<p>내부 조건부 확률(로그 확률) 데이터가 강화학습에 의해 심각한 교정 오류를 겪게 되고, 설상가상으로 여러 상용 LLM 제공업체들이 API에서 토큰 수준의 로짓과 확률 반환 기능을 제한하거나 폐쇄하면서 블랙박스 모델의 불확실성을 평가하기 위한 새로운 대안이 필요해졌다. 이에 따라 학계와 업계는 수학적 메타데이터를 추출하는 대신, 모델에게 프롬프트를 통해 자신의 불확실성을 자연어로 명시하도록 유도하는 ‘언어적 확신도(Verbalized Confidence)’ 추출 기법에 주목하기 시작했다.</p>
<h3>5.1  “Just Ask” 전략의 역설적 성공</h3>
<p>Anthropic의 기념비적인 논문 “Language Models (Mostly) Know What They Know“를 필두로 한 일련의 연구들은, 모델이 스스로 자신이 무엇을 알고 무엇을 모르는지 언어적으로 표현할 수 있는 능력(P(IK) 예측)을 탐구했다. 연구자들은 모델에게 정답을 생성하기 전에 여러 선택지를 제시하고, “당신의 답변이 맞을 확률이 몇 퍼센트입니까?“라고 묻거나, 확신이 없을 경우 “모른다(I don’t know)“고 답변하도록 가르쳤다.</p>
<p>결과는 매우 흥미로웠다. “Just Ask for Calibration” 등의 논문에 따르면, GPT-4나 Claude, ChatGPT와 같은 RLHF 미세 조정 모델들의 경우, 앞서 설명한 대로 토큰 생성의 로그 확률을 이용한 내부 조건부 확률은 엉망으로 교정되어 있었지만, 모델이 텍스트(출력 토큰)로 배출한 언어적 확신도 점수는 오히려 로그 확률보다 훨씬 더 교정이 잘 되어 있다는 사실이 발견되었다. TriviaQA, SciQ, TruthfulQA 등 다양한 벤치마크 테스트에서, 언어적 확신도를 사용할 경우 예상 교정 오차(ECE)가 로그 확률 지표 대비 상대적으로 50%나 급감하는 우수한 성능을 보였다. 이는 모델이 내부적으로는 과신에 빠져 모드 붕괴를 일으켰음에도 불구하고, 지시를 따르는(Instruction-following) 고차원적 맥락 속에서는 자신의 지식 한계를 메타 인지하여 텍스트로 올바르게 서술하는 역설적인 능력을 갖추고 있음을 의미한다.</p>
<h3>5.2  장문(Long-form) 생성의 한계와 복합 불확실성의 위험</h3>
<p>그러나 소프트웨어 엔지니어링 관점에서 모델의 언어적 확신도를 무비판적으로 수용하는 것은 매우 위험하다. 모델이 단답형 질의응답(Short-form QA)에서 단일 사실에 대한 확신도를 표현하는 데에는 능숙할지 몰라도, 바이브 코딩이 요구하는 긴 코드 블록 생성이나 복잡한 비즈니스 문서 추출과 같은 장문(Long-form) 응답 상황에서는 이 기법이 철저히 실패하기 때문이다.</p>
<p>UNCLE 벤치마크 및 LoGU(Long-form Uncertainty Expression) 프레임워크 관련 연구들은 장문 생성 환경에서 발생하는 두 가지 치명적 문제를 지적한다: 바로 불확실성 억제(Uncertainty suppression)와 불확실성 불일치(Uncertainty misalignment)이다. LLM이 500줄에 달하는 데이터베이스 마이그레이션 스크립트를 작성했을 때, 모델에게 “이 코드에 확신합니까?“라고 물으면 모델은 전체적인 문맥의 완결성에 취해 “99% 확신합니다“라고 응답할 확률이 높다. 하지만 그 500줄의 코드 중 단 한 줄, 예를 들어 사용자 입력을 이스케이프 처리하지 않은 단일 쿼리문이 존재한다면 이는 곧바로 심각한 SQL 인젝션 취약점으로 직결된다. 모델의 언어적 확신도 점수는 여러 로직과 정보가 혼재된 복합 불확실성(Mixed uncertainties) 상황에서 개별 구성 요소의 치명적 결함을 덮어버리는 기만적인 후광 효과를 낳게 된다.</p>
<p>결국 프롬프트 엔지니어링을 통해 모델에게 불확실성을 가르치고 언어적 확신도를 도출하는 기법은 AI 챗봇의 대화 품질을 높이거나 답변을 거부(Refusal)하도록 만드는 데에는 유용하지만, 무관용 원칙이 적용되어야 하는 소프트웨어 보안과 무결성 검증의 절대적 기준으로 삼기에는 턱없이 부족하다.</p>
<h2>6.  오라클(Oracle)의 당위성: 확률적 한계를 극복하는 결정론적 정답지</h2>
<p>지금까지의 분석을 통해, AI를 활용한 소프트웨어 개발에서 LLM 자체의 지표(로그 확률이나 언어적 확신도)만으로는 비결정성의 구조적 한계를 결코 뛰어넘을 수 없다는 사실이 명백해졌다. 로그 확률 연산에 기반한 확신도는 부동소수점 수준의 오차와 RLHF에 의한 확률 분포 붕괴, 그리고 모드 붕괴에 의해 오염되어 있으며 , 텍스트로 추출된 확신도 역시 복합적인 맥락 앞에서는 신뢰성을 잃는다.</p>
<p>수학적으로 확률 99.9%는 소프트웨어 공학에서 100%를 의미하지 않는다. 단 0.1%의 확률로 발생하는 환각이 결제 시스템의 금액 산정 로직이나 시스템 권한 인증 로직에 침투한다면, 그것은 단순한 버그가 아니라 비즈니스 전체를 파괴하는 대형 보안 사고(API 키 유출, 임의 코드 실행 등)로 이어진다. 따라서 AI가 텍스트를 생성하는 확률적(Probabilistic) 영역을 통과한 직후에는, 이를 단호하게 검증하고 필요시 기각할 수 있는 강력한 결정론적(Deterministic) 제어 장치가 필수적으로 요구된다.</p>
<p>이러한 맥락에서 오라클(Oracle)은 AI의 통계적 예측에 의존하지 않고, 고전적이고 엄격한 소프트웨어 공학의 규칙—컴파일러, 추상 구문 트리(AST) 분석기, 수학적 스키마 검증기, 정규표현식, 보안 필터 등—을 사용하여 AI의 출력이 시스템의 절대적 참(Ground Truth)에 부합하는지를 결정론적으로 판별하는 최종 검증 시스템을 의미한다.</p>
<h3>6.1  실전 예제: 로그 확률과 결정론적 오라클의 하이브리드 검증 파이프라인</h3>
<p>단순히 AI의 출력을 불신하여 모든 것을 인간이 재검토하는(Human-in-the-loop) 방식으로 돌아가는 것은 바이브 코딩이 가져다준 생산성 혁신을 포기하는 것이다. 실전 AI 엔지니어링에서 가장 이상적이고 견고한 아키텍처는 모델의 ’로그 확률 데이터’를 연산 효율을 위한 1차 필터(Short-circuiting)로 활용하고, ’결정론적 오라클’을 최종 판별자 및 비즈니스 로직 통제권자로 두는 하이브리드 파이프라인을 구축하는 것이다.</p>
<p>다음은 기업의 금융 애플리케이션에서 사용자가 제출한 비정형 영수증 및 계약서 문서 텍스트를 파싱하여, 데이터베이스에 저장할 구조화된 JSON 데이터(총액, 통화 종류, 구매 날짜, 취약한 시스템 명령 포함 여부)를 추출하는 실전 파이프라인 구축 예제이다.</p>
<p><strong>Tier 1: 데이터 추출 및 로그 확률 기반 1차 필터링 (Probabilistic Short-circuiting)</strong> 시스템은 LLM API를 호출하여 구조화된 JSON 출력을 요청함과 동시에 <code>logprobs: true</code> 및 <code>top_logprobs: 3</code>을 활성화한다. 모델이 JSON 문자열을 반환하면, 시스템은 곧바로 파서를 통해 각 핵심 키(Key)에 해당하는 값(Value) 토큰의 로그 확률을 추출하고 지수 함수로 복원하여 확신도를 계산한다.</p>
<pre><code class="language-JavaScript">// 주요 비즈니스 값에 대한 로그 확률 확신도 계산
const amountLogprob = result.logprobs?.find((item) =&gt; item.token === String(extractedAmount));
const confidenceScore = amountLogprob? Math.exp(amountLogprob.logprob) : 0;
</code></pre>
<p>만약 계산된 <code>confidenceScore</code>가 시스템이 정의한 최소 임계값(예: 85%)에 미치지 못하거나, HALT 프레임워크와 같은 시계열 분석기가 <code>top_logprobs</code>의 엔트로피 급증을 감지하여 환각 지표를 알린다면 , 시스템은 오라클 검증 단계로 넘어가기도 전에 즉시 이 응답을 폐기(Reject)하고 모델에 재시도를 요청한다. 이는 가치가 없는 환각 데이터를 즉각 차단하여 불필요한 컴퓨팅 리소스 및 후속 처리 비용을 낭비하지 않도록 돕는다. 그러나 확신도가 99%를 넘었다고 해서 이 데이터가 정답으로 인정되는 것은 결코 아니다.</p>
<p><strong>Tier 2: 구문 및 스키마 검증 오라클 (Syntax &amp; Schema Oracle)</strong></p>
<p>1차 필터를 통과한 데이터는 즉시 결정론적 스키마 검증기를 거친다. JSON Schema 라이브러리를 사용하여 반환된 데이터 구조가 정확한지 검사한다. <code>extractedAmount</code>가 실수형(Float)인지, 날짜 포맷이 정확히 <code>YYYY-MM-DD</code> 형태를 따르는지 정규표현식 기반의 오라클을 통과해야 한다. 모델의 언어적 확신도가 아무리 높아도 쉼표(<code>,</code>) 하나가 잘못 찍혀 타입 에러를 유발한다면 이 단계에서 가차 없이 예외를 발생시킨다.</p>
<p><strong>Tier 3: 비즈니스 로직 및 보안 룰 오라클 (Semantic &amp; Security Oracle)</strong></p>
<p>가장 중요한 최종 방어선이다. 스키마를 통과한 데이터는 순수 수식과 보안 규칙으로 작성된 오라클의 지배를 받는다.</p>
<ul>
<li><strong>비즈니스 연산 오라클:</strong> 영수증에 포함된 개별 구매 항목들의 배열 값을 추출하여 전통적인 소프트웨어 로직으로 직접 합산(Summation)한다. 이 합산 결과가 AI가 스스로 추출한 <code>extractedAmount</code> 값과 수학적으로 정확히 일치하는지 대조한다. AI가 환각을 일으켜 1원의 오차라도 발생시켰다면, 로그 확률이 100%에 근접했더라도 무조건 기각된다.</li>
<li><strong>보안 검증 오라클:</strong> 바이브 코딩 환경에서 가장 취약한 지점인 코드 인젝션을 방어하기 위해 , 추출된 모든 텍스트 문자열에 대해 악성 페이로드(예: <code>eval()</code>, <code>&lt;script&gt;</code>, 시스템 쉘 명령어)가 포함되어 있는지 고전적인 정적 분석(Static Analysis) 도구와 살균(Sanitization) 라이브러리를 통해 검사한다.</li>
</ul>
<p>결론적으로, AI 모델이 산출하는 확신도 점수와 로그 확률은 텍스트 생성의 블랙박스 내부를 들여다볼 수 있게 해주는 훌륭한 모니터링 지표이자 환각 탐지의 단서이다. 하지만 강화학습 편향에 의해 심하게 오염되어 있고 구조적 비결정성을 내포하고 있기 때문에, 이를 비즈니스 의사결정의 절대 기준으로 삼아서는 안 된다. AI를 실전 소프트웨어 개발에 안전하게 접목하고 ’바이브 코딩’의 혁신을 진정으로 완성하기 위해서는, 모델의 비결정적 출력을 확률 메트릭으로 일차 통제한 뒤 확고한 결정론적 제어 장치인 ’오라클’에 검증의 전권을 위임하는 견고한 엔지니어링 체계가 뒷받침되어야 한다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Is vibe coding actually insecure? New CMU paper benchmarks vulnerabilities in agent-generated code : r/programming - Reddit, https://www.reddit.com/r/programming/comments/1phf2f9/is_vibe_coding_actually_insecure_new_cmu_paper/</li>
<li>What is Vibe Coding? - IBM, https://www.ibm.com/think/topics/vibe-coding</li>
<li>Vibe coding - Wikipedia, https://en.wikipedia.org/wiki/Vibe_coding</li>
<li>Is Vibe Coding Safe? Benchmarking Vulnerability of Agent-Generated Code in Real-World Tasks - arXiv.org, https://arxiv.org/html/2512.03262v1</li>
<li>What is Vibe Coding? The Pros, Cons, and Controversies | Tanium, https://www.tanium.com/blog/what-is-vibe-coding/</li>
<li>Security risks of vibe coding and LLM assistants for developers - Kaspersky, https://www.kaspersky.com/blog/vibe-coding-2025-risks/54584/</li>
<li>Passing the Security Vibe Check: The Dangers of Vibe Coding | Databricks Blog, https://www.databricks.com/blog/passing-security-vibe-check-dangers-vibe-coding</li>
<li>What is vibe coding? | AI coding - Cloudflare, https://www.cloudflare.com/learning/ai/ai-vibe-coding/</li>
<li>A Risk Analysis of “Vibe Coding” - Arjun Raghunandanan, https://arjunraghunandanan.medium.com/a-risk-analysis-of-vibe-coding-d5833a6d1af5</li>
<li>Using logprobs - OpenAI for developers, https://developers.openai.com/cookbook/examples/using_logprobs/</li>
<li>Confidence Unlocked: A Method to Measure Certainty in LLM Outputs - Medium, https://medium.com/@vatvenger/confidence-unlocked-a-method-to-measure-certainty-in-llm-outputs-1d921a4ca43c</li>
<li>Estimating LLM classification confidence with log probabilities (logprobs) - Eric Jinks, https://ericjinks.com/blog/2025/logprobs/</li>
<li>HALT: Hallucination Assessment via Log-probs as Time series - OpenReview, https://openreview.net/forum?id=l4WFDcw3Yy</li>
<li>How to Measure LLM Confidence: Logprobs &amp; Structured Output - YouTube, https://www.youtube.com/watch?v=THsGizLHrTs</li>
<li>Can we detect LLM hallucinations? — A quick review of our experiments, https://www.nannyml.com/blog/llm-hallucination-detection</li>
<li>HALT: Hallucination Assessment via Log-probs as Time series - arXiv, https://arxiv.org/html/2602.02888v1</li>
<li>Beyond Next Token Probabilities: Learnable, Fast Detection of Hallucinations and Data Contamination on LLM Output Distributions - arXiv, https://arxiv.org/html/2503.14043v3</li>
<li>Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM Reasoning, https://arxiv.org/html/2510.08146v3</li>
<li>Just Ask for Calibration: Strategies for Eliciting Calibrated …, https://openreview.net/forum?id=g3faCfrwm7</li>
<li>Calibrating Verbalized Probabilities for Large Language Models - arXiv, https://arxiv.org/html/2410.06707v1</li>
<li>Taming Overconfidence in LLMs: Reward Calibration in RLHF - arXiv, https://arxiv.org/html/2410.09724v1</li>
<li>Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review - arXiv, https://arxiv.org/pdf/2504.18346</li>
<li>Expected Calibration Error (ECE): A Step-by-Step Visual Explanation, https://towardsdatascience.com/expected-calibration-error-ece-a-step-by-step-visual-explanation-with-python-code-c3e9aa12937d/</li>
<li>Calibrating Large Language Models Using Their Generations Only - arXiv, https://arxiv.org/html/2403.05973v1</li>
<li>Expected Calibration Error (ECE) Overview - Emergent Mind, https://www.emergentmind.com/topics/expected-calibration-error-ece</li>
<li>[Literature Review] Full-ECE: A Metric For Token-level Calibration on Large Language Models - Moonlight, https://www.themoonlight.io/en/review/full-ece-a-metric-for-token-level-calibration-on-large-language-models</li>
<li>Large Language Models Must Be Taught to Know What They Don’t Know - NIPS, https://papers.nips.cc/paper_files/paper/2024/file/9c20f16b05f5e5e70fa07e2a4364b80e-Paper-Conference.pdf</li>
<li>[PDF] Balancing Classification and Calibration Performance in Decision-Making LLMs via Calibration Aware Reinforcement Learning | Semantic Scholar, https://www.semanticscholar.org/paper/Balancing-Classification-and-Calibration-in-LLMs-Yaldiz-Spiliopoulou/1ecd3e9a5a25e5ff73155299259c7dbc9fe81ee7</li>
<li>An Investigation into the Confidence-Probability Alignment in Large Language Models - arXiv, https://arxiv.org/html/2405.16282v1</li>
<li>Why is RLHF strangling the model? : r/agi - Reddit, https://www.reddit.com/r/agi/comments/1pzkm2h/why_is_rlhf_strangling_the_model/</li>
<li>Mysteries of mode collapse - AI Alignment Forum, https://www.alignmentforum.org/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse</li>
<li>Reinforcement Learning from Human Feedback - RLHF Book, https://rlhfbook.com/book.pdf</li>
<li>Taming Overconfidence in LLMs: Reward Calibration in RLHF - OpenReview, https://openreview.net/forum?id=l0tg0jzsdL</li>
<li>RLHF does not appear to differentially cause mode-collapse - LessWrong, https://www.lesswrong.com/posts/pjesEx526ngE6dnmr/rlhf-does-not-appear-to-differentially-cause-mode-collapse</li>
<li>ICML Poster Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach - ICML 2026, https://icml.cc/virtual/2025/poster/46448</li>
<li>On the Trustworthiness of Generative Foundation Models - ResearchGate, https://www.researchgate.net/publication/389176427_On_the_Trustworthiness_of_Generative_Foundation_Models_Guideline_Assessment_and_Perspective/fulltext/67b7ed99207c0c20fa9053da/On-the-Trustworthiness-of-Generative-Foundation-Models-Guideline-Assessment-and-Perspective.pdf?origin=scientificContributions</li>
<li>Language Models (Mostly) Know What They Know - Anthropic, https://www.anthropic.com/research/language-models-mostly-know-what-they-know</li>
<li>Large Language Models Must Be Taught to Know What They Don’t Know - arXiv, https://arxiv.org/html/2406.08391v1</li>
<li>Understanding the Current State of Reasoning with LLMs | by Isamu Isozaki - Medium, https://isamu-website.medium.com/understanding-the-current-state-of-reasoning-with-llms-dbd9fa3fc1a0</li>
<li>UNCLE: Benchmarking Uncertainty Expressions in Long-Form Generation - ACL Anthology, https://aclanthology.org/2025.emnlp-main.1543.pdf</li>
<li>UNCLE: Benchmarking Uncertainty Expressions in Long-Form Generation - arXiv, https://arxiv.org/pdf/2505.16922</li>
<li>UncLE: Benchmarking Uncertainty Expressions in Long-Form Generation - arXiv, https://arxiv.org/html/2505.16922v2</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>