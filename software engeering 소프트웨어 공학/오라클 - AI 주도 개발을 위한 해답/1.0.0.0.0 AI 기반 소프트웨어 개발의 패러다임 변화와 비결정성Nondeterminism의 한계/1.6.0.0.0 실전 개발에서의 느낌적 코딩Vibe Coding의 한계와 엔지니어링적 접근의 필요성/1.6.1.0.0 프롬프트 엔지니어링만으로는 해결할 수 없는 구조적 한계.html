<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:1.6.1 프롬프트 엔지니어링만으로는 해결할 수 없는 구조적 한계</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>1.6.1 프롬프트 엔지니어링만으로는 해결할 수 없는 구조적 한계</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 1. AI 기반 소프트웨어 개발의 패러다임 변화와 비결정성(Nondeterminism)의 한계</a> / <a href="index.html">1.6 실전 개발에서의 '느낌적 코딩(Vibe Coding)'의 한계와 엔지니어링적 접근의 필요성</a> / <span>1.6.1 프롬프트 엔지니어링만으로는 해결할 수 없는 구조적 한계</span></nav>
                </div>
            </header>
            <article>
                <h1>1.6.1 프롬프트 엔지니어링만으로는 해결할 수 없는 구조적 한계</h1>
<h2>1. 서론: 만능의 열쇠가 아닌, 내재된 한계의 인식</h2>
<p>2020년대 중반, 생성형 인공지능(Generative AI)의 폭발적인 발전과 함께 등장한 ’프롬프트 엔지니어링(Prompt Engineering)’은 마치 현대의 연금술처럼 여겨지고 있습니다. 수많은 개발자와 기업들은 자연어 지시(Prompt)를 정교하게 다듬음으로써 모델의 성능을 극대화하고, 복잡한 비즈니스 로직을 처리하며, 심지어는 인간 수준의 추론을 이끌어낼 수 있다고 믿습니다. “프롬프트만 잘 작성하면 모델은 무엇이든 할 수 있다“는 낙관론은 거대 언어 모델(LLM)을 마치 무한한 잠재력을 지닌 블랙박스로 상정하고, 그 내부의 복잡성을 단순한 입력 최적화 문제로 치환하려는 시도입니다.</p>
<p>그러나 본 서적의 이번 장에서는 이러한 낙관론에 대해 냉철한 공학적, 수학적 메스를 들이대고자 합니다. 우리는 제미나이(Gemini), GPT-4, Claude와 같은 최신 LLM들이 아무리 뛰어난 성능을 보여준다 하더라도, 그 근간을 이루는 트랜스포머(Transformer) 아키텍처와 자기회귀적(Auto-regressive) 학습 방식에서 기인하는 **‘구조적 한계(Structural Limitations)’**에 직면해 있음을 인정해야 합니다. 이것은 단순히 모델의 파라미터 수가 부족하거나 학습 데이터가 모자라서 발생하는 문제가 아닙니다. 이는 모델이 지식을 저장하고, 인출하고, 조합하는 방식 자체에 내재된 근본적인 특성이자 제약입니다.</p>
<p>본 장에서는 프롬프트 엔지니어링이 결코 넘을 수 없는 네 가지의 거대한 ’벽’을 심층적으로 분석합니다. 첫째, 인간의 논리와 배치되는 지식의 비대칭성인 **‘역전의 저주(The Reversal Curse)’**입니다. 둘째, 잠재 공간(Latent Space)의 기하학적 불안정성으로 인한 **‘입력 섭동(Input Perturbation)에 대한 취약성’**입니다. 셋째, 진정한 의미의 구성적 사고가 결여된 **‘추론 능력의 허상(The Illusion of Reasoning)’**입니다. 마지막으로, 결정론적 시스템을 지향하는 소프트웨어 엔지니어링과 충돌하는 **‘확률적 비결정성(Probabilistic Non-determinism)’**입니다.</p>
<p>이 분석을 통해 독자들은 프롬프트 엔지니어링을 ’만능 해결책’이 아닌, 모델의 잠재력을 발현(Steering)시키는 제한적 도구로 재정의하게 될 것이며, 진정으로 견고한 AI 애플리케이션을 구축하기 위해 필요한 아키텍처적 접근 방식이 무엇인지 깨닫게 될 것입니다.</p>
<h2>2.  지식의 비대칭성: 역전의 저주 (The Reversal Curse)</h2>
<p>인간의 인지 능력에서 가장 기초적인 논리 중 하나는 ’대칭성(Symmetry)’입니다. 만약 우리가 “올라프 숄츠는 독일의 제9대 연방총리다“라는 사실을 학습했다면, 누군가 “독일의 제9대 연방총리는 누구인가?“라고 물었을 때 별다른 추가 학습 없이도 즉각적으로 “올라프 숄츠“라고 대답할 수 있습니다. 수학적으로 <span class="math math-inline">A=B</span>라는 명제는 곧 <span class="math math-inline">B=A</span>라는 명제와 동치(Equivalence)이기 때문입니다. 우리는 이 관계를 양방향으로 자유롭게 오갈 수 있는 단단한 연결 고리로 인식합니다.</p>
<p>하지만 2024년 ICLR에서 발표된 베르글룬드(Berglund) 등의 획기적인 연구는 현재의 LLM들이 이러한 기본적인 논리적 대칭성을 전혀 학습하지 못하고 있음을 증명했습니다. 이를 학계에서는 **‘역전의 저주(The Reversal Curse)’**라고 명명했습니다. 이 현상은 LLM이 정보를 이해하는 방식이 인간의 직관과는 근본적으로 다르다는 것을 보여주는 가장 강력한 증거입니다.</p>
<h3>2.1  자기회귀 모델의 단방향성 메커니즘</h3>
<p>이러한 한계가 발생하는 근본적인 원인은 LLM의 학습 목표인 **‘다음 토큰 예측(Next Token Prediction)’**에 있습니다. 모든 생성형 언어 모델은 텍스트 시퀀스를 왼쪽에서 오른쪽으로(Left-to-Right) 읽어가며 학습합니다. 수학적으로 모델은 시점 <span class="math math-inline">t</span>에서의 단어 <span class="math math-inline">w_t</span>가 나타날 확률을 이전까지의 모든 단어들 <span class="math math-inline">w_{1},..., w_{t-1}</span>이 주어졌을 때의 조건부 확률 <span class="math math-inline">P(w_t | w_{1},..., w_{t-1})</span>로 모델링하고, 이를 최대화하는 방향으로 파라미터를 업데이트합니다.</p>
<p>이 과정에서 지식은 방향성을 가진 **통계적 전이 확률(Transition Probability)**로 저장됩니다. 예를 들어, 학습 데이터에 “Tom Cruise’s mother is Mary Lee Pfeiffer(톰 크루즈의 어머니는 메리 리 파이퍼다)“라는 문장이 수천 번 등장했다고 가정해 봅시다. 모델은 <code>Tom Cruise</code>라는 주어와 <code>mother</code>라는 관계가 주어졌을 때, 그 뒤에 <code>Mary Lee Pfeiffer</code>가 나올 확률을 매우 높게 학습합니다. 즉, <span class="math math-inline">A \rightarrow B</span>의 경로는 강력하게 형성됩니다.</p>
<p>하지만 이 학습 과정에서 그 역방향인 <span class="math math-inline">B \rightarrow A</span>, 즉 <code>Mary Lee Pfeiffer</code>가 주어졌을 때 <code>Tom Cruise</code>를 예측하는 확률은 전혀, 혹은 거의 업데이트되지 않습니다. 모델의 내부 가중치 행렬(Weight Matrix) 속에서 <span class="math math-inline">A</span>에서 <span class="math math-inline">B</span>로 가는 고속도로는 뚫려 있지만, <span class="math math-inline">B</span>에서 <span class="math math-inline">A</span>로 돌아오는 길은 존재하지 않는 것입니다. 이는 단순한 데이터 부족의 문제가 아니라, 모델이 지식을 ’관계(Relation)’가 아닌 ’순서(Sequence)’로 저장하기 때문에 발생하는 구조적 결함입니다.</p>
<h3>2.2  실험적 증거: 유명인 부모 찾기 실험과 가상 사실 실험</h3>
<p>연구진은 이러한 이론을 검증하기 위해 두 가지 주요 실험을 수행했습니다. 첫 번째는 실제 유명인 데이터를 이용한 실험입니다. GPT-4와 같은 최첨단 모델에게 “Who is Tom Cruise’s mother? (톰 크루즈의 어머니는 누구인가?)“라고 물었을 때, 모델은 약 79%의 정확도로 정답을 맞혔습니다. 그러나 질문을 뒤집어 “Who is Mary Lee Pfeiffer’s son? (메리 리 파이퍼의 아들은 누구인가?)“라고 물었을 때, 정답률은 놀랍게도 33%로 급락했습니다. 이는 모델이 <code>Mary Lee Pfeiffer</code>라는 이름을 보고 <code>Tom Cruise</code>를 연상해내는 능력이 사실상 무작위 추측(Random Guess) 수준에 불과함을 의미합니다.</p>
<p>더욱 통제된 환경에서의 실험을 위해, 연구진은 모델이 이전에 본 적 없는 가상의 사실(Fictitious Facts)을 주입하여 미세조정(Fine-tuning)하는 실험을 진행했습니다. 예를 들어 “Uriah Hawthorne is the composer of Abyssal Melodies(우리아 호손은 심연의 멜로디의 작곡가다)“라는 문장을 학습시켰습니다. 학습 후 “심연의 멜로디를 작곡한 사람은 누구인가?“라고 물었을 때, 모델은 답을 하지 못했습니다. 심지어 정답인 <code>Uriah Hawthorne</code>을 생성할 확률(Log-probability)이 무작위 이름보다 높지도 않았습니다.</p>
<p>이 결과가 시사하는 바는 충격적입니다. <strong>프롬프트 엔지니어링은 모델 내부에 존재하는 지식을 ’인출’하는 기술이지, 물리적으로 존재하지 않는 연결을 ’생성’하는 기술이 아니라는 점</strong>입니다. 사용자가 아무리 “논리적으로 생각해봐”, “A가 B라면 B는 A잖아“라고 프롬프트로 설득하려 해도, 모델의 잠재 공간 속에 <span class="math math-inline">B \rightarrow A</span>라는 벡터 경로가 형성되어 있지 않다면 모델은 답을 찾아낼 수 없습니다. 이는 마치 파일이 저장되지 않은 하드디스크에서 파일 탐색기의 검색 옵션만 바꾼다고 해서 파일을 찾을 수 없는 것과 같습니다.</p>
<h3>2.3  데이터 증강과 그 한계</h3>
<p>일각에서는 “그렇다면 모든 데이터를 양방향으로 학습시키면(Data Augmentation) 해결되지 않는가?“라고 반문할 수 있습니다. 실제로 “A는 B이다“와 “B는 A이다“를 모두 학습 데이터셋에 포함시키면 모델은 두 질문 모두에 대답할 수 있습니다.</p>
<p>하지만 이것은 진정한 의미의 ’추론 능력 획득’이 아니라, 두 개의 독립적인 사실을 개별적으로 ’암기(Memorization)’한 것에 불과합니다. 세상의 모든 지식을 <span class="math math-inline">N \times N</span> 쌍으로 뒤집어서 학습시키는 것은 데이터의 양을 기하급수적으로 폭증시키며, 실질적으로 불가능합니다. 더욱이, 모델이 학습 시점에 보지 못한 새로운 정보(In-context Learning)가 프롬프트로 주어졌을 때도 이 문제는 여전히 발생합니다. 긴 문맥 속에 “A는 B이다“라는 정보를 주고, 나중에 “B는 무엇인가?“라고 물으면 대답하지만, 그 역방향 질문에는 여전히 취약성을 보입니다.</p>
<p>결국 ’역전의 저주’는 프롬프트 엔지니어링의 한계를 명확히 보여주는 첫 번째 구조적 장벽입니다. 우리는 모델이 정보의 의미를 이해하고 논리적 대칭성을 갖추고 있다고 착각하지만, 실상 모델은 그저 학습된 확률 분포에 따라 단어를 이어 붙이는 통계적 기계일 뿐이라는 냉혹한 현실을 인지해야 합니다.</p>
<h2>3.  잠재 공간의 기하학: 프롬프트의 취약성과 비결정성</h2>
<p>두 번째로 다룰 구조적 한계는 LLM의 내부 작동 방식인 **‘확률적 생성(Probabilistic Generation)’**과 그로 인한 **‘취약성(Fragility)’**입니다. 많은 프롬프트 엔지니어들이 겪는 가장 큰 좌절은, “어제는 완벽하게 작동하던 프롬프트가 오늘은 실패한다“거나 “예시를 하나 바꿨더니 전체 출력이 망가졌다“는 경험입니다. 왜 LLM은 이토록 예민하고 불안정한 것일까요?</p>
<h3>3.1  잠재 공간의 기하학적 구조: 분지(Basin), 경로(Route), 앵커(Anchor)</h3>
<p>이 현상을 이해하기 위해서는 LLM의 추론 과정을 고차원 벡터 공간(Latent Space)에서의 기하학적 이동으로 바라봐야 합니다. 최근의 기하학적 해석 연구에 따르면, 모델의 내부 상태는 크게 세 가지 힘의 상호작용으로 결정됩니다.</p>
<ol>
<li><strong>상태(State)와 분지(Basin):</strong> “수학 문제를 풀고 있다”, “프랑스어로 번역 중이다”, “코딩 중이다“와 같은 문맥적 상태는 잠재 공간 내의 특정한 ’분지(Basin)’나 ’계곡’으로 표현됩니다. 프롬프트가 성공적으로 작동한다는 것은 모델의 잔차 스트림(Residual Stream) 벡터를 이 분지 안으로 밀어 넣었다는 뜻입니다. 일단 분지 안에 안착하면, 모델은 안정적으로 그 문맥에 맞는 토큰을 생성합니다.</li>
<li><strong>경로(Route)와 수로(Groove):</strong> 학습 데이터에서 자주 등장하는 문구(예: <code>if</code> 뒤에 <code>(</code>, <code>Once upon a time</code> 뒤에 <code>...</code>)는 깊게 파인 ’수로’와 같습니다. 벡터가 이 경로 근처를 지나가면, 강력한 관성에 의해 빨려 들어가게 됩니다.</li>
<li><strong>앵커(Anchor):</strong> 시스템 프롬프트나 초기 지시사항은 벡터의 이동 경로를 지속적으로 잡아당기는 ‘중력장’ 역할을 합니다.</li>
</ol>
<p>문제는 이 세 가지 힘 사이의 <strong>‘중재(Arbitration)’</strong> 과정이 매우 미묘하고 불안정하다는 점입니다. 예를 들어, 프롬프트를 통해 “정답만 간결하게 말해(상태 설정)“라는 앵커를 걸어두어도, 생성 중간에 우연히 등장한 토큰이 강력한 ’관용구적 수로(Route)’를 건드리면, 모델은 앵커의 중력을 이겨내고 엉뚱한 경로로 빠져버립니다. 이는 모델 내부에서 수백 개의 어텐션 헤드(Attention Head)들이 서로 다른 방향으로 벡터를 밀고 당기는 힘겨루기를 하고 있으며, 그 합력(Resultant Vector)이 미세한 차이로 결정되기 때문입니다.</p>
<h3>3.2  입력 섭동(Input Perturbation)에 대한 구조적 취약성</h3>
<p>프롬프트 엔지니어링이 구조적으로 불안한 이유는 일종의 <strong>‘나비 효과(Butterfly Effect)’</strong> 때문입니다. 입력 프롬프트의 조사 하나를 바꾸거나, 예시의 순서를 바꾸는 아주 사소한 섭동(Perturbation)만으로도 벡터의 궤적은 완전히 달라질 수 있습니다.</p>
<p>CODECRASH 벤치마크 연구는 이러한 취약성을 정량적으로 입증했습니다. 연구진은 코드 생성 과제에서 자연어 설명에 사소하고 오해를 불러일으키는 정보(Misleading natural language cues)를 섞었습니다. 예를 들어, 코드의 로직은 덧셈을 수행하지만 주석(Comment)에는 “뺄셈을 하라“고 적어놓는 식입니다. 인간 개발자라면 주석이 틀렸음을 인지하고 코드 로직을 따르겠지만, LLM은 어떨까요?</p>
<p>실험 결과, 이러한 섭동이 주어졌을 때 LLM의 성능은 평균 **23.2%**나 급락했습니다. 더욱 놀라운 점은, 논리적 사고를 강제한다는 <strong>연쇄 사고(Chain-of-Thought, CoT)</strong> 기법을 적용했음에도 불구하고 여전히 **13.8%**의 성능 하락이 발생했다는 사실입니다.</p>
<p>이 결과는 모델이 텍스트의 심층적인 ’의미(Semantics)’나 ’실행 논리(Executable Logic)’를 이해하고 답을 내놓는 것이 아니라, 표면적인 키워드나 주석과 같은 **‘얕은 단서(Superficial Cues)’**에 과도하게 의존하고 있음을 보여줍니다. 모델의 학습 데이터에는 “주석과 코드는 일치한다“는 통계적 상관관계가 압도적으로 많이 존재하기 때문에, 프롬프트 엔지니어가 아무리 “주석을 무시하고 코드만 봐“라고 지시해도 모델의 어텐션 메커니즘은 그 강력한 통계적 유혹(Prior)을 뿌리치지 못하는 것입니다.</p>
<p>이것은 **‘학습된 편향(Learned Bias)’**이자 **‘구조적 취약성’**입니다. 프롬프트 엔지니어링은 모델의 주의를 환기시킬 수는 있지만, 모델이 수조 개의 토큰을 학습하며 형성한 본능적인 통계적 연관성을 완전히 차단할 수는 없습니다. 따라서 프롬프트에 의존하는 시스템은 입력 데이터의 작은 노이즈나 변화에도 치명적인 오류를 일으킬 수 있는 내재적 위험성을 항상 안고 있습니다.</p>
<h2>4.  추론 능력의 허상: 2단계 추론의 실패 (The Two-Hop Curse)</h2>
<p>LLM을 비즈니스 로직이나 의사결정 시스템에 도입하려는 시도에서 가장 빈번하게 발생하는 오해는 LLM을 ’논리적 추론 엔진(Reasoning Engine)’으로 간주하는 것입니다. 물론 LLM은 그럴듯한 논증을 생성해내지만, 최근의 연구 결과들은 모델이 진정한 의미의 <strong>‘구성적 추론(Compositional Reasoning)’</strong> 능력을 갖추고 있는지에 대해 심각한 의문을 제기합니다. 이를 학계에서는 **‘2단계 추론의 저주(The Two-Hop Curse)’**라고 부릅니다.</p>
<h3>4.1  파편화된 지식의 연결 실패</h3>
<p>상황을 단순화하여, 모델이 두 가지 사실을 서로 다른 문서에서 독립적으로 학습했다고 가정해 봅시다.</p>
<ol>
<li>사실 A: “가수 A의 생일은 B이다.”</li>
<li>사실 B: “날짜 B에 일어난 역사적 사건은 C이다.”</li>
</ol>
<p>이 두 사실을 완벽하게 기억하고 있는 모델에게 “가수 A의 생일에 일어난 역사적 사건은 무엇인가?“라고 묻는다면, 인간은 당연히 <span class="math math-inline">A \rightarrow B \rightarrow C</span>의 논리적 연결을 통해 답을 찾아낼 것입니다. 그러나 연구에 따르면, LLM은 이 두 사실을 개별적으로 질문했을 때는 100% 정답을 맞히더라도, 이를 연결하여(<span class="math math-inline">A \rightarrow C</span>) 답하는 데에는 실패할 확률이 매우 높습니다. 의 연구 결과에 따르면, CoT(Chain-of-Thought) 프롬프팅 없이는 이러한 2단계 추론(Multi-hop Reasoning)의 정확도가 **우연에 가까운 수준(Chance-level accuracy)**으로 떨어졌습니다.</p>
<p>이러한 실패의 원인은 LLM의 지식 저장 방식에 있습니다. LLM은 지식을 ’관계형 데이터베이스(RDB)’나 노드와 엣지로 연결된 ‘지식 그래프(Knowledge Graph)’ 형태로 저장하지 않습니다. 대신 지식은 문서 단위의 통계적 패턴으로 **파편화(Fragmented)**되어 저장됩니다. 만약 학습 데이터 내에서 <span class="math math-inline">B</span>라는 매개체(Bridge Entity)가 <span class="math math-inline">A</span>와 <span class="math math-inline">C</span>를 동시에 연결하는 문맥으로 등장한 적이 없다면, 모델의 내부 회로(Circuitry)에서 <span class="math math-inline">A</span>와 <span class="math math-inline">C</span>는 물리적으로 단절되어 있습니다.</p>
<h3>4.2  CoT의 한계와 ’잠재적 추론’의 부재</h3>
<p>물론, “먼저 가수 A의 생일을 확인하고, 그 다음 그 날짜의 사건을 검색해봐“라고 명시적으로 단계를 나누어주는 CoT 프롬프트는 이러한 문제를 일부 완화할 수 있습니다. 하지만 여기서 중요한 구조적 한계가 드러납니다. CoT를 사용했을 때의 성공은 모델이 스스로 ’추론’을 해낸 것이라기보다는, 사용자가 추론의 과정을 ’설계도’처럼 제공했기 때문에 가능한 것입니다. 즉, 모델은 스스로 논리적 단계를 구성(Composition)하는 능력이 부족하며, 단지 사용자가 제시한 패턴을 따라갈 뿐입니다.</p>
<p>더욱 심각한 것은 **‘잠재적 추론(Latent Reasoning)’**의 부재입니다. 모델이 겉으로(Explicitly) 추론 과정을 말하지 않고 내부적으로(Implicitly) 논리를 연결하여 답만 내놓는 능력은 거의 전무합니다. 이는 모델이 사고 과정(Thought Process)을 내면화하지 못하고 있음을 시사합니다.</p>
<p>이러한 특성은 복잡한 비즈니스 로직을 처리할 때 치명적입니다. 예를 들어, 금융 약관 A 조항과 B 조항을 결합하여 고객의 대출 가능 여부를 판단해야 할 때, 두 조항이 별도의 문서에 존재한다면 모델은 이를 스스로 연결하지 못하고 모순된 답변을 하거나 정보를 누락할 위험이 큽니다. <strong>프롬프트 엔지니어링은 모델이 ’추론하는 척’하도록 연출하는 도구일 뿐, 모델의 지능 자체를 격상시키는 도구는 아님</strong>을 명심해야 합니다.</p>
<h2>5.  결정론적 소프트웨어 엔지니어링과의 충돌: 프롬프트웨어의 위기</h2>
<p>비즈니스 현장에서 LLM 애플리케이션을 개발할 때 가장 큰 마찰이 발생하는 지점은 기존의 <strong>소프트웨어 엔지니어링(Software Engineering, SE)</strong> 패러다임과 LLM의 <strong>확률적(Probabilistic)</strong> 특성 간의 충돌입니다. 전통적인 소프트웨어는 결정론적(Deterministic)입니다. 입력이 같으면 출력은 100% 같아야 하며, 문법 오류는 용납되지 않습니다. 반면, LLM은 본질적으로 비결정적이며 모호합니다. 이 근본적인 차이는 프롬프트 엔지니어링만으로는 결코 메울 수 없는 간극을 만듭니다.</p>
<h3>5.1  JSON 스키마 준수의 환상과 현실</h3>
<p>많은 개발자들이 프롬프트에 “반드시 다음 JSON 스키마를 준수하라“고 명시하고, 몇 가지 예시(Few-shot)를 제공하면 모델이 이를 완벽하게 수행할 것이라 기대합니다. 실제로 최신 모델들은 훈련 단계에서 코드 데이터를 많이 학습하여 구조화된 출력을 꽤 잘 만들어냅니다.</p>
<p>그러나 ’꽤 잘한다’는 것은 ’항상 보장한다’는 것과 다릅니다. 대규모 운영 환경에서의 사례를 보면, 모델 업데이트나 미세한 입력 변화만으로도 잘 작동하던 JSON 출력 형식이 깨지는 일이 빈번합니다.</p>
<ul>
<li><strong>타입 위반:</strong> 정수형(<code>int</code>)을 요구한 필드에 문자열 설명(“알 수 없음” 또는 “계산 중”)을 넣습니다.</li>
<li><strong>구조 파괴:</strong> 중첩된 객체 구조를 평탄화(Flatten)하거나, 필수 필드(Required Field)를 누락합니다.</li>
<li><strong>환각된 필드:</strong> 스키마에 없는 필드를 임의로 창조해냅니다.</li>
</ul>
<p>이러한 오류가 발생하는 이유는 모델이 <strong>’JSON 문법(Syntax)’을 이해하고 따르는 것이 아니라, ’JSON처럼 보이는 텍스트’를 생성하고 있기 때문</strong>입니다. 모델에게 닫는 중괄호 <code>}</code>는 논리적 데이터 구조의 종결이 아니라, 그저 통계적으로 따옴표 <code>"</code> 뒤에 올 확률이 높은 토큰일 뿐입니다. 모델은 텍스트의 ’형태(Form)’는 모방할 수 있지만, 그 ’엄격함(Rigidity)’은 내재화하지 못합니다. 따라서 금융 거래나 의료 데이터 처리와 같이 타입 안정성(Type Safety)이 100% 보장되어야 하는 시스템에서, 프롬프트 엔지니어링만으로 무결성을 담보하려는 시도는 실패할 수밖에 없습니다.</p>
<h3>5.2  프롬프트웨어(Promptware)의 구조적 한계</h3>
<p>이러한 문제를 해결하기 위해 최근에는 **‘프롬프트웨어(Promptware)’**라는 개념이 등장했습니다. 프롬프트를 단순한 텍스트 입력이 아니라 하나의 소프트웨어 모듈로 취급하고, 버전 관리, 단위 테스트, 통합 테스트 등 SE의 원칙을 적용하려는 시도입니다.</p>
<p>하지만 프롬프트웨어조차도 ’런타임 환경’인 LLM 자체가 비결정적이라는 한계는 극복하지 못합니다. 기존 소프트웨어는 코드가 변하지 않으면(Code Freeze) 결과도 변하지 않지만, LLM 기반 시스템은 다음과 같은 이유로 끊임없이 변동합니다.</p>
<ul>
<li><strong>Silent Regressions (조용한 퇴행):</strong> 모델 공급사(OpenAI, Google 등)가 백엔드에서 모델 가중치를 업데이트하면, 동일한 프롬프트라도 동작 방식이 미세하게 달라질 수 있습니다.</li>
<li><strong>확률적 샘플링:</strong> <code>Temperature=0</code>으로 설정하더라도, 부동소수점 연산의 비결정성이나 하드웨어 상태에 따라 출력 토큰이 달라질 수 있습니다.</li>
</ul>
<p>이는 **“단위 테스트를 통과한 프롬프트가 프로덕션 환경에서는 실패할 수 있다”**는, 기존 엔지니어링 상식으로는 받아들이기 힘든 불확실성을 내포합니다. 따라서 프롬프트 엔지니어링은 전통적인 코딩을 대체하는 것이 아니라, 확률적이고 불안정한 새로운 종류의 ’소프트웨어 자재’를 다루는 별도의 관리 방법론을 필요로 합니다.</p>
<h2>6.  평가와 검증의 딜레마: 심판자로서의 LLM (LLM-as-a-Judge)</h2>
<p>마지막 구조적 한계는 <strong>평가(Evaluation)</strong> 영역에서 발생합니다. LLM의 출력을 평가하기 위해 인간이 일일이 개입하는 것은 비용과 시간 측면에서 불가능하기 때문에, 최근에는 또 다른 LLM을 심판관으로 사용하는 <strong>‘LLM-as-a-Judge’</strong> 방식이 널리 쓰이고 있습니다.</p>
<p>하지만 연구 결과에 따르면, LLM 심판관 역시 앞서 언급한 구조적 한계들로부터 자유롭지 못합니다.</p>
<ul>
<li><strong>편향(Bias):</strong> 심판관 모델은 자신의 학습 데이터에 있는 스타일이나 의견을 선호하는 ’자기 선호 편향(Self-Preference Bias)’을 보입니다.</li>
<li><strong>위치 편향(Position Bias):</strong> 선택지 중 앞쪽에 위치한 답을 더 선호하거나, 반대로 뒤쪽을 선호하는 경향이 있습니다.</li>
<li><strong>무결성 부재:</strong> 심판관 모델 자체가 환각을 일으켜, 오답을 정답이라고 판정하거나 그 반대의 경우도 발생합니다.</li>
</ul>
<p>이는 측정 도구 자체가 오차가 있는 자(Ruler)를 사용하는 것과 같습니다. 확률적 모델로 확률적 모델을 평가하는 것은 오차를 증폭시킬 수 있으며, 이는 전체 시스템의 신뢰도를 측정 불가능한 상태로 만들 수 있습니다. 연구에 따르면, 신뢰할 수 있는 결정론적 오라클(Deterministic Oracle)과 비교했을 때 LLM 심판관은 통계적 보증(Statistical Guarantee)을 제공하기 어렵습니다.</p>
<h2>7. 결론: 구조적 한계를 넘어서는 복합 시스템(Compound Systems)으로의 전환</h2>
<p>본 장에서의 심층적인 분석을 통해 우리는 **“프롬프트 엔지니어링은 만능이 아니며, 명확한 공학적 한계선(Engineering Ceiling)이 존재한다”**는 결론에 도달합니다.</p>
<ol>
<li><strong>지식의 구조적 부재:</strong> 모델은 ’역전의 저주’로 인해 학습되지 않은 방향의 지식 인출이 불가능합니다.</li>
<li><strong>내재적 취약성:</strong> 잠재 공간의 불안정성으로 인해 입력의 미세한 섭동에도 결과가 크게 달라질 수 있습니다.</li>
<li><strong>추론의 한계:</strong> 파편화된 지식을 스스로 연결하는 구성적 추론 능력이 결여되어 있으며, 2단계 이상의 추론은 실패할 확률이 높습니다.</li>
<li><strong>비결정성:</strong> 확률적 생성 방식은 기존 소프트웨어 엔지니어링이 요구하는 결정론적 무결성과 충돌합니다.</li>
</ol>
<p>따라서 제미나이를 활용한 서적 집필뿐만 아니라, 모든 LLM 기반 애플리케이션 개발에 있어 우리는 프롬프트 엔지니어링에만 매몰되는 것을 경계해야 합니다. 대신, 이 한계를 구조적으로 보완할 수 있는 **‘복합 AI 시스템(Compound AI Systems)’**으로 시야를 넓혀야 합니다.</p>
<ul>
<li><strong>검색 증강 생성(RAG):</strong> 모델의 내부 지식에 의존하는 대신, 신뢰할 수 있는 외부 데이터베이스에서 지식을 검색하여 주입함으로써 ’역전의 저주’와 ’지식 단절’을 해결해야 합니다.</li>
<li><strong>도구 사용(Tool Use):</strong> 수학적 계산이나 엄밀한 논리 검증은 LLM에게 맡기지 않고, 파이썬 코드 실행기나 계산기와 같은 결정론적 도구를 호출하여 해결해야 합니다.</li>
<li><strong>가드레일(Guardrails) 및 검증기(Verifier):</strong> LLM의 출력을 그대로 신뢰하지 말고, 별도의 파서(Parser)나 규칙 기반 코드를 통해 JSON 스키마를 검증하고 오류를 수정하는 방어적 프로그래밍을 적용해야 합니다.</li>
</ul>
<p>결국, LLM을 마스터한다는 것은 화려한 프롬프트를 작성하는 기술을 넘어, 모델이 **‘할 수 없는 것’**이 무엇인지 정확히 알고 이를 시스템 아키텍처 레벨에서 보완하는 엔지니어링 능력을 의미합니다. 이 구조적 한계에 대한 깊은 이해가 바로 성공적인 AI 시스템 구축의 첫걸음이 될 것입니다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>THE REVERSAL CURSE: LLMS TRAINED ON “A IS B” FAIL TO, https://proceedings.iclr.cc/paper_files/paper/2024/file/5178b2f2d7c44aa390c0777dc77b3f0c-Paper-Conference.pdf</li>
<li>The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”, https://arxiv.org/html/2309.12288v4</li>
<li>The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”, https://openreview.net/pdf?id=TGTZwVabpU</li>
<li>Daily Papers - Hugging Face, <a href="https://huggingface.co/papers?q=reversible+strategy">https://huggingface.co/papers?q=reversible%20strategy</a></li>
<li>What Makes LLMs So Fragile (and Brilliant)? | by Rob Manson …, https://medium.com/@robman/what-makes-llms-so-fragile-and-brilliant-f53a1961d9e6</li>
<li>CODECRASH: Exposing LLM Fragility to Misleading … - OpenReview, https://openreview.net/pdf?id=CAB0EjD9EK</li>
<li>The Two-Hop Curse: LLMs trained on A-&gt;B, B-&gt;C fail to learn A, https://chatpaper.com/paper/84733</li>
<li>How does Transformer Learn Implicit Reasoning? | Request PDF, https://www.researchgate.net/publication/392204521_How_does_Transformer_Learn_Implicit_Reasoning</li>
<li>Prompt Engineering: Challenges, Strengths, and Its Place in … - InfoQ, https://www.infoq.com/articles/prompt-engineering/</li>
<li>Software Engineering for Prompt-Enabled Systems - arXiv.org, https://arxiv.org/html/2503.02400v2</li>
<li>What Is Prompt Management? Tools &amp; Failure Modes | Deepchecks, https://www.deepchecks.com/glossary/prompt-management/</li>
<li>11 Tips to Create Reliable Production AI Agent Prompts - Datagrid, https://datagrid.com/blog/11-tips-ai-agent-prompt-engineering</li>
<li>From Simple Prompts to Tool‑Using Agents | by Ashfaq | Medium, https://medium.com/@ashfaqbs/prompt-engineering-demystified-from-simple-prompts-to-tool-using-agents-462c64056e0f</li>
<li>Converting Natural Language to Structured GraphQL Queries Using, https://www.zenml.io/llmops-database/converting-natural-language-to-structured-graphql-queries-using-llms</li>
<li>When “Better” Prompts Hurt: Evaluation-Driven Iteration for LLM, https://arxiv.org/html/2601.22025v1</li>
<li>LLM Response Evaluation with Spring AI: Building LLM-as-a-Judge, https://spring.io/blog/2025/11/10/spring-ai-llm-as-judge-blog-post</li>
<li>Robust Statistical Evaluation of LLMs with Imperfect Judges - arXiv, https://arxiv.org/html/2601.20913v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>