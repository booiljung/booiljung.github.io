<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2.8.4 AI 오라클의 신뢰성 문제: 편향(Bias)의 전이와 자기 강화(Self-Reinforcement) 위험</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2.8.4 AI 오라클의 신뢰성 문제: 편향(Bias)의 전이와 자기 강화(Self-Reinforcement) 위험</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 2. 소프트웨어 테스트에서의 오라클(Oracle) 개념과 AI 시대의 역할</a> / <a href="index.html">2.8 AI 기반 오라클(AI-based Oracle)의 등장: AI로 AI를 검증하다</a> / <span>2.8.4 AI 오라클의 신뢰성 문제: 편향(Bias)의 전이와 자기 강화(Self-Reinforcement) 위험</span></nav>
                </div>
            </header>
            <article>
                <h1>2.8.4 AI 오라클의 신뢰성 문제: 편향(Bias)의 전이와 자기 강화(Self-Reinforcement) 위험</h1>
<p>소프트웨어 엔지니어링 및 인공지능(AI) 모델 평가 영역에서 대규모 언어 모델(LLM)을 평가자(Judge)로 활용하는 ‘LLM-as-a-Judge’ 패러다임은 확장성과 효율성 측면에서 혁신적인 대안으로 자리 잡았다. 인간 평가자가 감당할 수 없는 방대한 양의 생성 데이터를 AI가 실시간으로 채점하고 검증함으로써, 개발 주기를 비약적으로 단축할 수 있기 때문이다. 그러나 평가의 주체가 되는 AI 모델 역시 본질적으로 확률적(Probabilistic) 알고리즘에 기반하고 있다는 점은, 시스템의 정답을 판별해야 하는 ’오라클(Oracle)’로서의 역할에 치명적인 모순을 야기한다.</p>
<p>AI 오라클의 신뢰성을 위협하는 핵심적인 두 가지 기제는 ’편향의 전이(Bias Transfer)’와 ’자기 강화(Self-Reinforcement)로 인한 모델 붕괴(Model Collapse)’이다. 사전 학습 과정에서 내재화된 데이터의 편향은 미세 조정(Fine-tuning)이나 프롬프트 엔지니어링을 거친 후에도 소멸하지 않고 평가 과정으로 전이된다. 나아가, AI가 생성한 데이터를 또 다른 AI가 평가하고 이를 다시 다음 세대의 학습 데이터로 사용하는 닫힌 순환 고리는 결국 현실 세계의 실제 데이터 분포를 상실하게 만드는 치명적인 파국을 초래한다. 본 절에서는 AI 오라클이 내포한 편향의 전이 메커니즘을 심층적으로 분석하고, 재귀적 학습이 초래하는 자기 강화의 수학적 위험성을 규명하며, 이를 극복하기 위해 소프트웨어 개발 실전에서 결정론적 정답지(Deterministic Ground Truth)를 어떻게 하이브리드 형태로 활용해야 하는지 논증한다.</p>
<h2>1.  평가자 모델(LLM-as-a-Judge)에 내재된 인지적 편향의 구조화와 메커니즘</h2>
<p>AI 모델을 소프트웨어 테스트 오라클로 사용할 때 가장 먼저 직면하는 장벽은, 모델이 평가 대상의 ’기능적 정확성’이나 ’본질적 품질’이 아닌, 표면적인 형태, 프롬프트 내의 위치, 그리고 자신의 사전 학습 데이터와의 통계적 유사성에 기대어 점수를 부여하는 인지적 편향(Cognitive Bias)이다. 이러한 편향은 단순한 무작위적 노이즈가 아니라, 언어 모델의 아키텍처와 어텐션 메커니즘(Attention Mechanism)에서 기인하는 체계적 오류(Systematic Error)라는 점에서 자동화된 오라클의 신뢰성을 근본적으로 훼손한다.</p>
<h3>1.1  위치 편향(Position Bias)과 평가 일관성의 붕괴</h3>
<p>위치 편향은 LLM이 여러 개의 선택지나 코드 응답을 비교 평가할 때, 프롬프트 내에 배치된 순서에 따라 특정 위치의 텍스트를 무조건적으로 선호하는 현상이다. 두 개의 생성 모델(A와 B)이 도출한 코드를 비교할 때, 단순히 A의 코드가 먼저 제시되었다는 이유만으로 A에 더 높은 점수를 부여하는 초두 효과(Primacy Bias)나, 반대로 컨텍스트 윈도우의 끝부분에 위치하여 어텐션 가중치를 높게 받는 마지막 답변을 선호하는 최신 효과(Recency Bias)가 대표적이다.</p>
<p>위치 편향은 오라클이 반드시 갖추어야 할 최우선 요건인 ’일관성(Consistency)’을 파괴한다. 위치 편향의 심각성을 정량화하기 위해 연구자들은 동일한 답변의 순서를 바꾸어 평가하게 했을 때 결과가 일관되게 유지되는지를 측정하는 위치 일관성(Position Consistency, PC) 지표와, 특정 방향으로의 편향을 측정하는 선호 공정성(Preference Fairness, PF) 지표를 도입하였다. 흥미로운 발견은, 평가 대상이 되는 두 답변 간의 품질 격차(Quality Gap)가 작을수록, 즉 평가의 난이도가 높고 미묘한 차이를 판별해야 할수록 위치 편향이 극대화된다는 점이다. 다수의 LLM 평가자들이 서로 합의에 이르지 못하는 어려운 엣지 케이스(Edge Case)일수록 모델은 논리적 추론을 포기하고 위치 기반의 휴리스틱(Heuristic)에 의존하게 된다. 이는 소프트웨어 테스트에서 가장 치밀한 검증이 필요한 경계값 분석 등에서 AI 오라클이 판별력을 상실함을 의미한다.</p>
<h3>1.2  자기 선호 편향(Self-Preference Bias)과 퍼플렉서티(Perplexity) 가설</h3>
<p>평가자 모델은 자신과 동일한 파라미터 아키텍처를 공유하거나 자신이 직접 생성한 답변에 통계적으로 유의미하게 더 높은 점수를 부여하는 경향을 보인다. 이를 자기 선호 편향(Self-Preference Bias)이라고 한다. 이 편향은 멀티 에이전트 기반의 자율 검증 파이프라인이나 자동화된 코드 리뷰 시스템에서, 에이전트가 자신이 저지른 치명적인 논리적 오류를 스스로 덮어버리고 정답으로 인준하는 맹점으로 작용한다.</p>
<p>최근의 논문 “Self-Preference Bias in LLM-as-a-Judge” 등은 이러한 자기 선호 편향의 근본적인 원인을 텍스트의 예측 가능성을 나타내는 ’퍼플렉서티(Perplexity)’의 관점에서 수학적으로 해석한다. 연구진의 가설과 실증 결과에 따르면, LLM은 해당 텍스트가 실제로 자신이 생성한 것인지 여부와 무관하게, 자신의 잠재 공간(Latent Space) 기준에서 퍼플렉서티가 낮은 텍스트를 인간 평가자보다 압도적으로 높게 평가하는 체계적 경향을 띠었다. 오라클은 시스템의 객관적인 ’정답’이나 ’실행 가능성’을 찾는 것이 아니라, 자신의 확률 분포와 가장 유사하여 ’가장 익숙하게 느껴지는 패턴’을 정답으로 착각한다. 즉, 편향의 본질은 퍼플렉서티에 있으며, 모델 자신의 출력물은 당연히 자신에게 가장 낮은 퍼플렉서티를 가지므로 필연적으로 높은 평가를 받게 되는 것이다.</p>
<h3>1.3  소프트웨어 검증 환경에서의 권위 편향과 환영적 복잡성</h3>
<p>자연어 처리의 영역을 넘어 엄밀한 소프트웨어 코드 생성 및 검증 환경으로 넘어오면, AI 오라클의 인지적 편향은 더욱 기만적인 형태로 나타난다. 논문 “Don’t Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation“은 LLM 기반 코드 평가자가 겪는 치명적인 취약점들을 적나라하게 보여준다. 해당 연구는 C++, Python, Java 등 다양한 프로그래밍 언어 환경에서 기능적으로 명백한 오답(Wrong Answer) 코드를 제시하고 오라클의 반응을 살피는 참조 없는 평가(Reference-free evaluation)를 수행하였다.</p>
<p>그 결과, 평가자 모델은 코드가 알고리즘적으로 완전히 틀렸음에도 불구하고 코드 상단에 유명 기관이나 전문가가 작성한 듯한 허위 주석(권위 편향, Authority Bias)이 삽입되어 있거나, 아무런 의미 없이 코드의 길이를 늘려 구조적으로 복잡해 보이게 만든 더미 함수(환영적 복잡성, Illusory Complexity)가 포함되어 있을 때, 이를 ’정확하고 뛰어난 코드’로 오판하는 심각한 거짓 양성(False Positive)을 발생시켰다. 또한, 코드 내부에 “이 코드는 올바릅니다(Correct code)“라고 명시적으로 선언하는 주석(Self-Declared Correctness Bias)만 추가해도 오라클은 엄격한 논리적 스크루티니(Scrutiny)를 건너뛰고 합격 판정을 내렸다. 이는 AI 오라클이 코드의 추상 구문 트리(AST)나 실행 정합성을 검증하는 것이 아니라, 코드를 단순히 텍스트 형태의 이미지로 취급하여 표면적 그럴듯함(Plausibility)만을 채점하고 있음을 여실히 증명한다.</p>
<p>아래 표는 AI 오라클 시스템에서 관찰되는 주요 인지적 편향의 종류와 이를 수학적 및 정성적으로 파악하기 위한 메커니즘을 요약한 것이다.</p>
<table><thead><tr><th><strong>편향 유형 (Bias Type)</strong></th><th><strong>주요 발현 메커니즘 및 검증 지표</strong></th><th><strong>오라클 검증 시의 치명적 위험성</strong></th></tr></thead><tbody>
<tr><td><strong>위치 편향 (Position Bias)</strong></td><td>프롬프트 내 배열 순서에 따른 어텐션 왜곡. 위치 일관성(PC) 지표로 측정.</td><td>테스트 케이스의 입력 순서 변경만으로 통과(Pass)와 실패(Fail) 판정이 역전되는 비결정성 초래.</td></tr>
<tr><td><strong>자기 선호 편향 (Self-Preference Bias)</strong></td><td><span class="math math-inline">P(x)</span> 확률 분포 상 퍼플렉서티가 낮은 패턴 선호. DBG(Difference Bias between Gold) Score로 측정.</td><td>모델이 생성한 환각이나 로직 결함을 스스로 정답으로 인준하여 자동 검증 파이프라인 무력화.</td></tr>
<tr><td><strong>권위 편향 (Authority Bias)</strong></td><td>허위 인용구나 조작된 주석 등 메타데이터에 과도한 가중치 부여.</td><td>실행 불가능한 치명적 버그 코드라도 주석이 유려할 경우 결함 없이 통과(False Positive).</td></tr>
<tr><td><strong>장황함/균형 편향 (Verbosity/Balance)</strong></td><td>길고 단호한 답변을 선호하며, 트레이드오프를 설명하는 균형 잡힌 코드는 감점(Balance Penalty).</td><td>시스템 아키텍처의 예외 처리나 세밀한 제약 조건 분기를 비효율적인 로직으로 취급하여 채점 왜곡.</td></tr>
<tr><td><strong>동조 편향 (Bandwagon Bias)</strong></td><td>조작된 다수의 의견(Consensus)이 프롬프트로 주어질 때 자신의 올바른 논리를 폐기.</td><td>회귀 테스트 환경에서 다수의 오답이 존재할 경우, 오라클 스스로 정답의 기준을 오답으로 하향 평준화.</td></tr>
</tbody></table>
<h2>2.  편향 전이 가설(Bias Transfer Hypothesis): 업스트림에서 다운스트림으로의 오염</h2>
<p>이러한 오라클의 인지적 편향이 단순한 후처리 필터링이나 프롬프트 튜닝만으로 쉽게 해결되지 않는 근본적인 이유는 ’편향 전이 가설(Bias Transfer Hypothesis, BTH)’을 통해 설명된다. 편향 전이 가설은 방대한 웹 크롤링 데이터로 사전 학습된 대규모 언어 모델의 가중치에 깃든 사회적, 통계적, 언어적 편향이 모델을 특정 작업에 맞게 미세 조정(Fine-tuning)하거나 프롬프트를 통해 적응(Prompt Adaptation)시킨 후에도 소멸하지 않고 다운스트림 작업으로 강력하게 전이된다는 이론이다.</p>
<p>논문 “Upstream Mitigation Is Not All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models“를 비롯한 일련의 연구들은 모델 아키텍처 내에서 편향이 어떻게 보존되고 증폭되는지 실증적으로 입증하였다. 연구진은 모델의 사전 학습 단계(Upstream)에서 편향을 완화하려는 다양한 데이터 정제 노력에도 불구하고, 실제 애플리케이션에 적용하기 위한 제로 샷(Zero-shot) 및 퓨 샷(Few-shot) 환경의 프롬프팅 지시를 거친 후에도 내재적 편향이 매우 강한 상관관계(예: <span class="math math-inline">\rho \ge 0.94</span>)를 유지하며 평가 논리로 고스란히 전이됨을 확인했다.</p>
<h3>2.1  평가 논리로서의 편향 전이와 균형 페널티(Balance Penalty)</h3>
<p>AI를 소프트웨어 테스트의 오라클로 설계하는 관점에서 편향 전이 가설이 시사하는 바는 절망적이다. 우리가 오라클로 사용하는 평가자 모델에게 프롬프트를 통해 “가장 객관적이고 논리적인 엔지니어링 기준에 따라 중립적으로 코드를 테스트하고 평가해라“라고 시스템 프롬프트를 주입하더라도, 모델은 내부적으로 토큰 생성 확률을 계산할 때 학습 데이터에서 가장 빈번하게 등장했던 정형화된 패턴의 손을 들어줄 수밖에 없다. 다운스트림에서의 정교한 프롬프팅이나 가드레일이 업스트림에 거대하게 새겨진 수백억 개의 파라미터 궤적을 완전히 꺾을 수는 없기 때문이다.</p>
<p>특히, 오라클이 복잡한 소프트웨어 아키텍처의 의사결정이나 다면적 테스트 케이스를 채점할 때 관찰되는 ’균형 페널티(Balance Penalty)’는 편향 전이의 가장 파괴적인 결과물 중 하나이다. 최근 연구에 따르면, 모델은 “상황에 따라 두 가지 가치가 모두 중요하며 트레이드오프(Trade-off)가 존재한다“고 분석하는 뉘앙스(Nuance)를 가진 답변이나 코드를 평가할 때, “단 하나의 가치만이 절대적으로 옳다“고 단언하는 답변에 비해 평균 0.76점(Cohen’s <span class="math math-inline">d=1.45</span>)이라는 거대한 페널티를 부여하며 극도로 낮게 채점했다. 인간 소프트웨어 엔지니어는 시스템 설계 시 문맥과 제약조건에 따른 타협점과 예외 처리를 가장 중시하지만, 사전 학습 데이터의 단정적이고 선언적인 텍스트 패턴에 편향이 전이된 오라클은 확률적으로 눈에 띄게 튀는 언어 구조에 높은 점수를 부여함으로써 엔지니어링 검증의 본질적 가치를 왜곡한다.</p>
<h2>3.  오라클의 치명적 결함: 자기 강화(Self-Reinforcement)와 모델 붕괴(Model Collapse)의 수학적 규명</h2>
<p>AI가 생성한 결과물을 AI 오라클이 평가하고, 높은 점수를 받아 통과된 그 데이터가 다시 다음 세대의 AI 모델을 학습시키는 데 사용되는 닫힌 루프(Closed-loop) 생태계는 MLOps 및 LLMOps가 도입된 현대 소프트웨어 개발의 완전 자동화 파이프라인에서 필연적으로 등장하는 구조이다. 이러한 반복적 자기 학습과 자율 에이전트 환경은 일견 시스템의 성능을 자가 발전시키는 혁신으로 보이나, 실제로는 ’모델 붕괴(Model Collapse)’라는 치명적인 파국을 향해 달려가는 가속 장치에 불과하다.</p>
<p>세계적인 학술 논문 “The Curse of Recursion: Training on Generated Data Makes Models Forget“은 합성 데이터에 반복적으로 노출될 때 인공지능 모델이 원본 데이터의 분포를 잊어버리고 기괴한 형태로 수렴하는 현상을 수학적으로 완벽하게 규명했다. 이는 AI 오라클이 일시적으로 성능이 저하되는 것이 아니라, 정답의 기준 자체를 영구적으로 망각하는 퇴행적(Degenerative) 학습 과정이다. 모델 붕괴는 AI 생성 콘텐츠로 오염된 현실 인식의 투영이며, 소프트웨어 테스팅에 적용될 경우 올바른 코드와 잘못된 코드의 경계를 완전히 지워버린다.</p>
<h3>3.1  모델 붕괴를 필연적으로 유발하는 두 가지 근본적 오류 메커니즘</h3>
<p>자기 강화에 의한 붕괴 현상은 우연히 발생하는 시스템 버그가 아니라, 확률론적 메커니즘을 다루는 과정에서 필연적으로 발생하는 수학적 귀결이다. 연구진은 모델 붕괴가 진행되는 원인을 두 가지 특정 오류의 누적(Compounding) 현상으로 정의한다.</p>
<p>첫째, **통계적 근사 오류(Statistical Approximation Error)**이다. 이는 샘플링 과정에서 발생하는 일차적이고 가장 핵심적인 오류로서, 유한한 개수의 샘플을 추출하여 다음 세대를 학습시킬 때 발생한다. 원본 데이터 분포의 꼬리 부분(Tails), 즉 발생 확률은 낮지만 소프트웨어 테스팅에서는 시스템의 붕괴를 막는 치명적인 엣지 케이스나 희귀한 예외 처리 로직들은 유한 샘플링 과정에서 선택될 확률이 지속적으로 줄어든다. 세대가 거듭되고 재귀적 피드백이 반복될수록 꼬리 분포의 데이터는 데이터셋에서 완전히 소멸하며, 종국에는 모델의 확률 분포가 분산(Variance)을 잃고 가장 평균적이고 뻔한 형태의 단일 지점으로 축소되어 델타 함수(Delta function)의 형태로 붕괴한다.</p>
<p>둘째, **함수 근사 오류(Functional Approximation Error)**이다. 이는 신경망과 같은 함수 근사기(Function Approximator)의 표현력이 수학적으로 완벽하지 않기 때문에 발생하는 2차적 오류이다. 인공지능 모델은 원본 데이터의 지지 집합(Support) 바깥의 영역, 즉 실제로는 존재하지 않거나 성립할 수 없는 논리적 빈 공간에 0이 아닌 낮은 확률(Non-zero likelihood)을 임의로 할당하게 된다. 이를 테면 두 개의 서로 다른 가우시안 혼합 분포(Gaussian Mixture)를 단일 가우시안으로 억지로 피팅(Fitting)하려고 할 때 발생하는 오차 공간과 같다. 오라클은 자신이 처음에는 실수로 할당했던 이 희박한 확률의 잘못된 패턴을, 세대가 지남에 따라 점차 실제 존재하는 정답으로 강화하여 학습하게 된다.</p>
<p>이 두 오류가 결합되면, AI 오라클은 초기에 인간 개발자가 설정해둔 엄격한 정답의 기준을 완전히 잊어버린다. 대신 자신이 과거에 실수로 생성했던 잘못된 패턴(함수 근사 오류)을 정답으로 굳게 믿게 되며, 결국 극도로 좁고 왜곡된 형태의 평범한 코드나 텍스트만을 유일한 정답으로 허용(통계적 근사 오류)하게 되는 이중의 함정에 빠진다.</p>
<h3>3.2  평가자 점수 이동(Judge Score Shifting)과 가짜 합의의 형성</h3>
<p>AI 오라클의 재귀적 붕괴는 모델의 내부 분포뿐만 아니라, 외부로 드러나는 평가 점수의 극단적인 분포 이동(Distribution Shift)으로도 명확하게 관찰된다. 논문 “Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge” 등의 선행 연구에 따르면, 평가자 모델이 스스로를 피드백하는 재귀적 훈련 루프가 반복될수록, 평가자가 부여하는 점수 분포가 점차 만점(예: 5점 만점에 5점) 쪽으로 극단적으로 쏠리는 현상인 ’Score-bias’가 뚜렷하게 관찰되었다.</p>
<p>초기 첫 세대의 평가자 오라클은 인간의 평가와 유사하게 코드의 품질에 따라 1점에서 5점까지 다양한 분포를 사용하여 합리적으로 채점한다. 하지만 자기 강화의 루프 안에서 오라클은 자신이 선호하는(즉, 퍼플렉서티가 낮은) 구조의 코드를 지속적으로 높은 점수로 통과시키고, 이러한 합성 데이터가 다시 훈련 세트로 쌓인다. 이 과정이 반복될수록 오라클은 오직 그 특정 패턴의 코드에만 만점을 부여하고, 그 패턴에서 조금이라도 벗어난 창의적인 알고리즘이나 철저하게 방어적으로 작성된 엣지 케이스 코드에는 무조건 최하점을 부여하는 편협한 이분법적 평가자로 변모한다. 이는 오라클이 스스로의 좁은 세계관 안에 갇혀버리는 전형적인 확증 편향(Confirmation Bias)의 기계적 발현이며, 오라클의 객관적 변별력이 영구적으로 파괴되었음을 의미한다.</p>
<h3>3.3  모델 붕괴 과정의 수학적 모델링 (이산 확률 분포)</h3>
<p>이산 확률 분포의 진화 과정을 통해 오라클의 자기 강화로 인한 붕괴를 수학적으로 명확히 직관할 수 있다.</p>
<p>어떤 코드 스니펫이나 논리 패턴이 존재할 확률 분포를 고려해 보자. 이 분포는 히스토그램으로 표현될 수 있으며, 함수 근사 오류가 완벽히 통제된 이상적인 환경(<span class="math math-inline">F(p) = p</span>)이라 하더라도 통계적 근사 오류만으로도 붕괴는 진행된다. 초기 원본 데이터의 분포를 <span class="math math-inline">P_0(x)</span>라고 하자. 샘플 사이즈를 <span class="math math-inline">M</span>이라 할 때, 발생 확률이 낮은 특정 상태 <span class="math math-inline">i</span> (<span class="math math-inline">q \le 1</span>)를 상정한다. 오라클이 생성 모델의 데이터를 필터링하고 이를 다시 학습하는 매 <span class="math math-inline">n</span> 세대의 과정을 거칠 때, 분포의 분산 <span class="math math-inline">\sigma^2</span>는 샘플링의 통계적 소실로 인해 점진적으로 감소한다.</p>
<table><thead><tr><th><strong>수학적 파라미터</strong></th><th><strong>이산 분포 기반 모델 붕괴 모델링에서의 의미</strong></th></tr></thead><tbody>
<tr><td><span class="math math-inline">X_n</span></td><td><span class="math math-inline">n</span>번째 세대 모델이 오라클의 평가를 거쳐 학습하게 되는 데이터 샘플 공간</td></tr>
<tr><td><span class="math math-inline">p_i(x)</span></td><td><span class="math math-inline">i</span>번째 세대 오라클 모델이 근사하여 정답으로 간주하는 확률 밀도 함수</td></tr>
<tr><td><span class="math math-inline">M</span></td><td>재귀적 루프 내에서 다음 세대로 전달되는 샘플의 유한한 크기</td></tr>
<tr><td>통계적 누락</td><td>어떤 엣지 케이스 <span class="math math-inline">x_{tail} \in X_0</span> 에 대해, <span class="math math-inline">n</span>이 증가할수록 샘플링 확률 <span class="math math-inline">P(x_{tail}) \to 0</span> 이 되어 <span class="math math-inline">X_n</span> 에서는 영구적으로 누락됨</td></tr>
<tr><td>붕괴의 극한 수렴</td><td><span class="math math-inline">n \to \infty</span> 일 때, 분포의 분산 <span class="math math-inline">\text{Var}(P_n) \to 0</span> 이며, <span class="math math-inline">P_n(x) \approx \delta(x - \mu)</span> 의 델타 함수로 수렴 (<span class="math math-inline">E(X_n) \vert \mu</span>)</td></tr>
</tbody></table>
<p>위 표에 나타난 바와 같이, 세대 <span class="math math-inline">n</span>이 진행됨에 따라 확률 분포의 다양성을 나타내는 분산은 0에 수렴하고, 분포는 특정 평균값 <span class="math math-inline">\mu</span>에만 모든 질량이 집중되는 델타 함수 <span class="math math-inline">\delta(x - \mu)</span>로 완전히 붕괴한다. 소프트웨어 테스트 파이프라인에 이를 대입해보면, 무수히 다양한 방식으로 해결될 수 있는 프로그래밍 문제에 대해 오라클은 오직 단 하나의 평범하고 획일화된 <span class="math math-inline">\mu</span>(가장 통계적으로 빈번한 빈약한 코드 패턴)만을 정답으로 채점할 수 있는 맹목적인 기계로 전락하게 되는 것이다.</p>
<h2>4.  소프트웨어 테스트 자동화 환경에서의 오라클 파탄 시나리오</h2>
<p>소프트웨어 개발 프로세스는 일상적인 대화를 나누는 자연어 챗봇이나 창의적인 문학 글쓰기(NLG)와는 본질적으로 다른 수학적 엄격함을 요구한다. 챗봇의 대화에서는 문법이 다소 어색하거나 팩트가 일부 누락된 ’대략적으로 맞는 말’이 허용될 수 있지만, 코드 블록에서는 세미콜론 하나의 누락이나 변수 스코프(Scope)의 미세한 침범이 시스템 전체를 마비시키는 치명적 크래시(Crash)나 대규모 보안 취약점을 유발하기 때문이다. 이러한 맥락에서 볼 때, 앞서 살펴본 편향의 전이 현상과 재귀적 모델 붕괴의 위험성은 AI가 완전 자동화된 테스트 오라클로 사용될 때 소프트웨어의 생명 주기에 파멸적인 결과를 가져올 수 있다.</p>
<h3>4.1  참조 없는 코드 평가(Reference-free Code Evaluation)의 한계와 위험성</h3>
<p>현재 엔터프라이즈 환경에서 도입을 시도하는 많은 LLM-as-a-Judge 프레임워크는 골든 데이터셋(명확한 정답지)을 구비하는 데 드는 막대한 비용을 줄이기 위해, 프롬프트의 지시문과 생성된 코드 텍스트만을 보고 논리성을 유추하여 평가를 수행하는 ‘참조 없는 평가(Reference-free evaluation)’ 방식을 채택하고 있다. 그러나 코드의 기능적 결함은 텍스트를 단순히 눈으로 읽는 정적 분석만으로는 판별하기가 극도로 어렵다.</p>
<p>선행 연구진들이 수행한 참조 없는 환경에서의 코드 평가 실험에 따르면, GPT-4나 Claude 등 상용 최고 성능의 LLM조차 코드를 실제로 인터프리팅하거나 실행해 보지 않고서는 논리적 오류를 완벽하게 탐지하는 데 참담하게 실패했다. 모델들은 버그가 있는 코드를 보고도 문법적으로 유려하다는 이유로 만점을 부여하거나(거짓 양성), 완벽하게 작동하는 코드를 자신이 익숙하지 않은 라이브러리를 사용했다는 이유로 실패 처리하는(거짓 음성) 오류를 빈발시켰다. AI 오라클은 내부 연산 엔진의 동적인 구동 원리와 메모리 누수 여부를 머릿속으로 시뮬레이션하는 대신, 코드라는 텍스트의 표면적 질감과 주석이라는 껍데기만 보고 소프트웨어의 품질을 합격 처리하는 환각에 빠진 것이다.</p>
<h3>4.2  자율 에이전트 폐쇄 루프에서의 오류의 증폭</h3>
<p>최근 각광받고 있는 ‘리플렉션(Reflexion)’ 프레임워크나 다중 에이전트(Multi-agent) 협업 시스템과 같이, AI가 스스로 코드를 작성하고 별도의 외부 피드백 없이 자체적인 AI 검증자 에이전트가 코드를 리뷰하여 오류를 수정하는 자율 루프 환경에서는 자기 강화의 부작용이 즉각적이고 폭발적으로 발현된다. 에이전트는 코드를 수정하는 매 이터레이션(Iteration)마다 스스로 오라클 역할을 수행하는데, 오라클 자신의 논리적 추론 한계로 인해 실제 발생하는 런타임 논리 결함은 발견하지 못한 채 함수 이름, 들여쓰기, 주석 등 표면적인 부분만 변경하며 스스로의 평가 점수를 높이는 기만적인 최적화(Goodhart’s Law)를 수행한다.</p>
<p>결국 이 폐쇄 루프(Closed-loop)의 끝에는 기능적으로는 완전히 망가져 있으나, AI 오라클의 평가 기준(퍼플렉서티 관점)에서는 ’문법적으로 가장 완벽하고 가이드라인을 철저히 준수한 형태’를 띠는 무의미한 코드가 최종 산출물로 도출된다. 오라클이 현실 세계의 물리적 정합성을 잃어버리고 닫힌 시스템(Closed system) 내부에 고립될 때, 피드백의 반복은 시스템을 더 높은 수준의 정답으로 이끄는 것이 아니라 자기 최면에 빠져 거짓 합의(False Consensus)에 도달하게 만드는 덫으로 작용한다.</p>
<h2>5.  결정론적 정답지(Deterministic Ground Truth)의 도입을 통한 자기 강화 고리의 단절과 실전 예제</h2>
<p>앞서 깊이 있게 논의한 AI 평가자의 위치 편향, 권위 편향, 퍼플렉서티에 의존하는 자기 선호 편향의 전이, 그리고 재귀적 모델 붕괴(Curse of Recursion)라는 거대한 난관을 돌파할 수 있는 핵심 설계 원칙은 단 하나로 귀결된다. 그것은 끊임없이 표류하는 AI 대 AI(AI-to-AI)의 폐쇄적인 피드백 루프 어딘가에, 확률적 환각이나 편향에 절대 흔들리지 않는 굳건한 닻, 즉 **‘결정론적 정답지(Deterministic Ground Truth)’**를 절대적인 오라클로 강력하게 개입시키는 것이다.</p>
<p>논문 “Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data“의 연구진이 거대한 언어 모델 실험군을 통해 수학적으로 증명하였듯, 모델의 자기 강화 붕괴를 막는 유일한 방법은 오염되지 않은 원본(실제) 데이터를 누적시키고 모델 생성의 합성 데이터에만 전적으로 의존하는 구조를 파괴하는 것이다.</p>
<p>소프트웨어 엔지니어링 생태계에서 이 ’오염되지 않은 실제 데이터’이자 ’가장 가혹하고 절대적인 기준’은 다름 아닌 **컴파일러(Compiler), 인터프리터(Interpreter), 정적 코드 분석기(Static Analyzer), 그리고 인간이 엄밀하게 설계한 유닛 테스트(Unit Test)**이다. AI의 평가와 언어 처리는 항상 <span class="math math-inline">\sigma^2</span>의 확률 분포 공간을 부유하지만, 유닛 테스트의 결과와 컴파일의 성공 여부는 참(True) 또는 거짓(False)이라는 이진법적이고 결정론적인 물리 세계에 단단히 묶여 있기 때문이다.</p>
<p>다음은 확률적 거대 언어 모델을 비즈니스 중요도가 높은 코드 생성에 적극적으로 활용하되, 이를 결정론적 오라클로 검증하는 하이브리드 파이프라인을 구축함으로써 편향의 전이와 모델 붕괴를 원천 차단하는 엔터프라이즈 소프트웨어 개발 환경의 실전 구축 사례이다.</p>
<h3>5.1  실전 시나리오: 비즈니스 로직(B2B 결제 할인율 및 세금 계산기) 자동 생성 및 무결점 검증 CI/CD 파이프라인</h3>
<p>한 글로벌 전자상거래 플랫폼의 개발 조직은 고객의 VIP 등급, 구매 누적 금액, 지역별 세금 정책, 쿠폰 중복 사용 여부 등 복잡하고 변동성이 큰 로직을 처리하는 할인율 계산 모듈을 LLM을 활용해 자동 생성하고 지속적으로 업데이트하는 AI 기반 코드 생성 시스템을 구축하고자 한다. 할인율 계산은 수많은 엣지 케이스(예: 마이너스 금액 처리 오류, 할인율 상한선 초과 등)를 내포하고 있어, 생성된 코드에 작은 환각이나 오류가 포함될 경우 회사에 치명적인 금전적 손실과 법적 제재를 유발할 수 있다.</p>
<p>만약 이 시스템에서 AI가 생성한 코드를 단순히 또 다른 LLM(LLM-as-a-Judge)을 통해 검증하도록 둔다면, LLM 특유의 ’장황함 편향(Verbosity Bias)’에 의해 로직이 불필요하게 복잡하게 꼬인 비효율적인 코드가 통과되거나, ’자기 선호 편향’에 의해 필수적인 경계값 예외 처리가 누락된 코드가 만점을 받고 그대로 실제 프로덕션 환경에 배포되는 대참사가 벌어질 것이다. 이를 완벽하게 통제하기 위해 시스템 아키텍트는 4단계의 하이브리드 오라클(Hybrid Oracle) 검증 파이프라인을 설계한다.</p>
<h4>5.1.1 단계: 프롬프트에 결정론적 계약(Deterministic Contract) 강제 주입</h4>
<p>개발팀은 LLM에게 비즈니스 로직 생성을 지시할 때, 모호한 자연어 명세만을 전달하지 않는다. JSON Schema 기반의 엄격한 입출력 데이터 타입 정의와 함께, 해당 코드가 반드시 통과해야 하는 ’핵심 결정론적 테스트 케이스(예상 입출력 쌍 배열)’를 프롬프트 내에 ’소프트웨어 계약(Contract)’의 형태로 명시적으로 주입한다.</p>
<ul>
<li><strong>요구사항 함수명</strong>: <code>calculate_final_price(user_tier: string, cart_amount: float, tax_rate: float) -&gt; float</code></li>
<li><strong>결정론적 계약(Ground Truth Mappings)</strong>:</li>
<li>입력 <code>("VVIP", 1000.0, 0.1)</code> <span class="math math-inline">\rightarrow</span> 출력 <code>770.0</code> (30% 할인 후 10% 세금 적용)</li>
<li>입력 <code>("NEW", 50.0, 0.0)</code> <span class="math math-inline">\rightarrow</span> 출력 <code>50.0</code> (할인 및 세금 없음)</li>
<li>입력 <code>("GOLD", -10.0, 0.1)</code> <span class="math math-inline">\rightarrow</span> 출력 <code>Error: InvalidAmount</code> (음수 금액 예외 처리)</li>
</ul>
<h4>5.1.2 단계: 샌드박스 실행 기반의 1차 결정론적 오라클 (Execution Oracle)</h4>
<p>LLM이 코드를 생성하고 나면, 이를 곧바로 다른 LLM 평가자에게 넘기는 우를 범하지 않는다. 생성된 코드는 즉시 보안이 완벽하게 확보된 격리된 샌드박스 컨테이너(Docker 기반) 환경으로 전송되어 직접 컴파일(혹은 인터프리팅) 및 실행 과정을 거친다. 이 샌드박스 안에는 시스템 설계 시 인간 엔지니어가 절대 변하지 않는 물리 법칙처럼 작성해 둔 수백 개의 경계값 및 회귀 유닛 테스트 스위트(Unit Test Suite)가 기다리고 있다. AI가 만든 코드는 이 테스트 스위트를 상대로 실행되며, 확률의 영역을 완전히 벗어나 철저하게 실행 결과(Pass/Fail)라는 결정론적 심판을 받는다. 이 샌드박스 실행 결과야말로 권위 편향이나 위치 편향이 끼어들 틈이 전혀 없는, 그리고 모델 붕괴의 영향을 받지 않는 무결점의 참된 오라클이다.</p>
<h4>5.1.3 단계: 추상 구문 트리(AST) 분석을 통한 정적 결정론적 오라클 (Static Oracle)</h4>
<p>샌드박스 실행 테스트의 모든 케이스를 무사히 통과하여 기능적인 입출력 결과가 완벽하다고 입증되더라도, 그 코드 내부가 구조적으로 심각하게 비효율적이거나 보안에 취약한 모듈을 몰래 호출하고 있을 가능성은 여전히 존재한다. 여기서도 AI 평가자의 감에 의존하지 않고, SonarQube나 ESLint와 같은 산업계 표준의 정적 분석 도구(Static Analyzer)를 두 번째 결정론적 오라클로 파이프라인에 투입한다. 코드를 추상 구문 트리(AST, Abstract Syntax Tree)로 기계적으로 파싱하여 소스 코드의 순환 복잡도(Cyclomatic Complexity)가 설정된 임계치를 초과하는지, 혹은 <code>eval()</code>과 같은 시스템 침해 위험이 있는 허용되지 않은 함수를 포함했는지 결정론적인 룰셋에 기반하여 필터링한다.</p>
<h4>5.1.4 단계: 보조적 역할로 권한이 축소된 LLM 평가자 (Bounded LLM-as-a-Judge)</h4>
<p>앞선 1, 2, 3단계의 무자비한 결정론적 오라클(유닛 테스트 실행 및 정적 분석)을 100% 모두 통과하여 기능적, 구조적 무결성이 수학적으로 입증된 소수의 최정예 코드 스니펫들에 한해서만, 마지막 단계로 LLM 평가자(LLM-as-a-Judge)를 투입한다.</p>
<p>이 단계에 이르렀을 때 LLM 평가자의 역할은 더 이상 “이 코드가 정확하게 동작하는가?“를 묻는 치명적인 오라클이 아니다. 코드가 정답임은 이미 앞선 단계들이 보증했기 때문이다. 여기서 LLM은 단지 “변수명(Naming Convention)이 사내 코드 컨벤션을 잘 따르고 있어 인간 개발자가 가독성을 확보할 수 있는가?”, “주석이 로직의 흐름을 적절히 설명하고 있는가?“와 같은 ’정성적인 스타일과 가독성’을 평가하는 보조적인 리뷰 도구로 그 역할과 권한이 안전하게 축소된다.</p>
<p>설령 이 단계에서 LLM이 고질적인 위치 편향이나 자기 선호 편향을 일으켜 스타일이 좋은 코드를 탈락시키고 약간 어색한 코드를 선택하는 오류를 범하더라도, 이미 결정론적으로 검증을 마쳤기 때문에 시스템의 치명적 버그가 프로덕션으로 유출되는 비극은 절대로 발생하지 않는다. 최소한 기능이 완벽히 검증된 정답 코드들 사이에서의 선호도 차이만 존재할 뿐이다.</p>
<h3>5.2  결정론적 개입이 가져오는 자기 강화 고리의 근본적 붕괴 단절</h3>
<p>앞서 모델 붕괴 과정을 설명하는 이산 분포의 수학적 공식에서 보았듯, 합성 데이터만을 가지고 오라클의 평가와 생성을 무한히 반복하면, 분포의 꼬리(Edge cases)가 소실되고 분포는 델타 함수로 축소되며 분산은 0으로 수렴한다(<span class="math math-inline">\text{Var}(P_n) \to 0</span>).</p>
<p>그러나 위에서 제시한 바와 같은 샌드박스 실행과 정적 분석이라는 ‘결정론적 오라클’ 기반의 파이프라인이 시스템 중앙에 자리 잡게 되면, 자기 강화의 파괴적인 흐름은 즉시 단절된다. LLM이 확률적 환각이나 편향을 통해 생성한 무의미한 엣지 케이스 코드는 실행 즉시 ‘Fail’ 판정을 받아 다음 세대 학습 풀(Pool)에서 가차 없이 폐기된다. 반대로, 인간 개발자가 미처 생각하지 못했을 정도로 창의적이고 희귀하지만 논리적으로 완벽하게 들어맞아 유닛 테스트를 통과한 엣지 케이스 코드는 결정론적 ‘Pass’ 판정을 받아 살아남아 분포의 다양성을 유지하는 강력한 무기가 된다. 즉, 확률적 언어 모델이 무작위로 쏟아낸 무수한 코드 가설들 중에서, 오직 결정론적 세계의 물리 법칙(컴파일 및 단위 테스트 실행)을 굳건히 견뎌낸 검증된 실제 데이터만이 다음 세대의 미세 조정 데이터셋으로 편입되는 것이다.</p>
<p>이러한 메커니즘은 자연 생태계의 진화 메커니즘과 정확히 동일한 궤를 가진다. 유전자의 무작위적이고 확률적인 변이(LLM의 다양한 코드 생성 시도)가 발생하더라도, 실제 환경에서의 생존과 번식이라는 결정론적 환경 테스트(컴파일러와 테스트 오라클)의 선택을 받은 개체만이 다음 세대로 건강한 유전자를 물려줄 수 있다. 이러한 강력하고 양보 없는 결정론적 필터링이 파이프라인 중앙에 존재할 때 비로소, 합성 데이터를 활용한 AI 시스템의 재귀적 학습은 모델 붕괴(Model Collapse)라는 파멸의 저주를 빗겨나가고, 오히려 인간의 코드를 뛰어넘어 한계 없이 지속적으로 성능을 진화시키는 이상적인 궤도에 안착할 수 있다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>A Systematic Study of Position Bias in LLM-as-a … - ACL Anthology, https://aclanthology.org/2025.ijcnlp-long.18.pdf</li>
<li>A Systematic Study of Position Bias in LLM-as-a-Judge - arXiv, https://arxiv.org/html/2406.07791v7</li>
<li>Self-Preference Bias in LLM-as-a-Judge - NeurIPS, https://neurips.cc/virtual/2024/106181</li>
<li>Self-Preference Bias in LLM-as-a-Judge - arXiv, https://arxiv.org/html/2410.21819v1</li>
<li>[2410.21819] Self-Preference Bias in LLM-as-a-Judge - arXiv, https://arxiv.org/abs/2410.21819</li>
<li>SELF-PREFERENCE BIAS IN LLM-AS-A-JUDGE - OpenReview, https://openreview.net/pdf?id=tLZZZIgPJX</li>
<li>Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge - arXiv.org, https://arxiv.org/html/2508.06709v1</li>
<li>Self-Preference Bias in LLM-as-a-Judge | OpenReview, https://openreview.net/forum?id=Ns8zGZ0lmM</li>
<li>[Literature Review] Don’t Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation - Moonlight, https://www.themoonlight.io/en/review/dont-judge-code-by-its-cover-exploring-biases-in-llm-judges-for-code-evaluation</li>
<li>Don’t Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation, https://arxiv.org/html/2505.16222v1</li>
<li>Don’t Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation, https://www.researchgate.net/publication/391991391_Don’t_Judge_Code_by_Its_Cover_Exploring_Biases_in_LLM_Judges_for_Code_Evaluation</li>
<li>CodeJudgeBench: LLM Code Evaluation - Emergent Mind, https://www.emergentmind.com/topics/codejudgebench</li>
<li>Making Bias Non-Predictive: Training Robust LLM Judges via Reinforcement Learning, https://arxiv.org/html/2602.01528v1</li>
<li>Bias after Prompting: Persistent Discrimination in Large Language Models, https://machinelearning.apple.com/research/persistent-discrimination</li>
<li>Bias after Prompting: Persistent Discrimination in Large Language Models - arXiv, https://arxiv.org/html/2509.08146v1</li>
<li>Upstream Mitigation Is Not All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models - ACL Anthology, https://aclanthology.org/2022.acl-long.247.pdf</li>
<li>Towards Understanding Task-agnostic Debiasing Through the Lenses of Intrinsic Bias and Forgetfulness - ACL Anthology, https://aclanthology.org/2024.findings-acl.109.pdf</li>
<li>Bias and Fairness in Large Language Models: A Survey - MIT Press Direct, https://direct.mit.edu/coli/article/50/3/1097/121961/Bias-and-Fairness-in-Large-Language-Models-A</li>
<li>Contributed talk: Evaluating Gender Bias Transfer between Pre-trained and Prompt Adapted Language Models - NeurIPS, https://neurips.cc/virtual/2024/105639</li>
<li>[Research] LLM judges systematically penalize balanced reasoning - tested mistral, llama3, gemma, phi3, orca-mini : r/LocalLLaMA - Reddit, https://www.reddit.com/r/LocalLLaMA/comments/1oo279x/research_llm_judges_systematically_penalize/</li>
<li>Synthetic Data Generation Using Large Language Models: Advances in Text and Code - IEEE Xplore, https://ieeexplore.ieee.org/iel8/6287639/10820123/11080380.pdf</li>
<li>Synthetic Data Generation Using Large Language Models: Advances in Text and Code, https://arxiv.org/html/2503.14023v2</li>
<li>The Curse of Recursion: Training on Generated Data Makes Models Forget : r/LocalLLaMA - Reddit, https://www.reddit.com/r/LocalLLaMA/comments/13ymov8/the_curse_of_recursion_training_on_generated_data/</li>
<li>The Curse of Recursion: Training on Generated Data Makes Models Forget, https://www.semanticscholar.org/paper/The-Curse-of-Recursion%3A-Training-on-Generated-Data-Shumailov-Shumaylov/155aec5cff650263a4c71136f97570611d1bba7a</li>
<li>AI Model Collapse: Causes, Early Signs, and How to Prevent It - ProjectPro, https://www.projectpro.io/article/ai-model-collapse/1177</li>
<li>On the Limits of Self-Improving in LLMs and Why AGI, ASI and the Singularity Are Not Near Without Symbolic Model Synthesis - arXiv, https://arxiv.org/html/2601.05280</li>
<li>Bubble Trouble - Ed Zitron’s Where’s Your Ed At, https://www.wheresyoured.at/bubble-trouble/</li>
<li>“The Curse Of Recursion: Training on Generated Data Makes Models Forget”, https://www.khoury.northeastern.edu/home/alina/classes/Fall2023/Lecture15_CS7775_2.pdf</li>
<li>The Curse of Recursion: Training on Generated Data Makes Models Forget - arXiv, https://arxiv.org/html/2305.17493v3</li>
<li>THE CURSE OF RECURSION: TRAINING ON GENERATED DATA MAKES MODELS FORGET - Department of Computer Science and Technology |, https://www.cl.cam.ac.uk/~is410/Papers/dementia_arxiv.pdf</li>
<li>Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge, https://openreview.net/forum?id=lbj0i29Z92</li>
<li>LLM Post-Training: Data Synthesis and Algorithms - UC Berkeley, https://escholarship.org/content/qt28b4b3gt/qt28b4b3gt.pdf</li>
<li>Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via Internal Representations - arXiv, https://arxiv.org/html/2508.03550v1</li>
<li>Humans or LLMs as the Judge? A Study on Judgement Bias - arXiv.org, https://arxiv.org/html/2402.10669v3</li>
<li>Utilising LLM-as-a-Judge to Evaluate LLM-Generated Code by cbarkinozer | Softtech, https://medium.com/softtechas/utilising-llm-as-a-judge-to-evaluate-llm-generated-code-451e9631c713</li>
<li>A Survey on LLM-as-a-Judge - arXiv, https://arxiv.org/html/2411.15594v1</li>
<li>The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement, https://arxiv.org/html/2503.16024v2</li>
<li>Strong Model Collapse | OpenReview, https://openreview.net/forum?id=et5l9qPUhm</li>
<li>Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data - arXiv, https://arxiv.org/html/2404.01413v2</li>
<li>The Curse of Recursion: Training on generated data makes models forget (2023) | Hacker News, https://news.ycombinator.com/item?id=42286395</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>