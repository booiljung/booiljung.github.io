<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2.10.1 통제 불가능한 AI를 통제 가능한 시스템으로 편입시키기 위한 필수 조건</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2.10.1 통제 불가능한 AI를 통제 가능한 시스템으로 편입시키기 위한 필수 조건</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 2. 소프트웨어 테스트에서의 오라클(Oracle) 개념과 AI 시대의 역할</a> / <a href="index.html">2.10 요약 및 3장 연결: 왜 우리는 다시 결정론적 정답지를 갈구하는가?</a> / <span>2.10.1 통제 불가능한 AI를 통제 가능한 시스템으로 편입시키기 위한 필수 조건</span></nav>
                </div>
            </header>
            <article>
                <h1>2.10.1 통제 불가능한 AI를 통제 가능한 시스템으로 편입시키기 위한 필수 조건</h1>
<h2>1. 서론: 확률적 지능과 확정적 엔지니어링의 충돌과 융합</h2>
<p>현대 소프트웨어 엔지니어링의 역사는 본질적으로 ’불확실성(Uncertainty)’을 제거하고 ’예측 가능성(Predictability)’을 확보하기 위한 투쟁의 기록이었다. 전통적인 시스템 설계자들은 입력값 <span class="math math-inline">A</span>가 주어졌을 때 시스템이 반드시 출력값 <span class="math math-inline">B</span>를 반환하도록 설계하며, 이를 보장하기 위해 단위 테스트(Unit Test), 통합 테스트(Integration Test), 그리고 엄격한 타입 시스템(Type System)을 구축해왔다. 이러한 <strong>결정론적(Deterministic)</strong> 시스템에서는 동일한 조건 하에 동일한 결과가 보장되며, 오류는 논리의 결함에서 비롯된 수정 가능한 대상으로 간주된다. 즉, 소프트웨어의 신뢰성은 코드의 결정론적 실행에 기반한다.</p>
<p>그러나 대규모 언어 모델(LLM)과 생성형 AI(Generative AI)의 도입은 이러한 엔지니어링의 근간을 뒤흔들고 있다. 딥러닝 기반의 에이전트 시스템은 본질적으로 <strong>확률적(Stochastic)</strong> 이다. 이들은 사전에 결정된 정답을 계산하는 것이 아니라, 방대한 데이터 분포 내에서 통계적으로 가장 그럴듯한(plausible) 다음 토큰이나 행동을 선택한다. 이 과정에서 ’환각(Hallucination)’이라 불리는 사실 왜곡, 비결정론적 출력 변동, 그리고 설명 불가능한 추론 비약이 발생한다. 확률적 최적화 기법은 복잡하고 다차원적인 문제 공간을 탐색하는 데 탁월한 성능을 발휘하지만, 그 대가로 결과의 재현성과 정확성을 일부 희생한다.</p>
<p>따라서 현대의 시스템 아키텍트들은 전례 없는 도전에 직면해 있다. “어떻게 하면 통제 불가능한(Uncontrollable) 확률적 AI 컴포넌트를, 신뢰성과 안전성이 필수적인 통제 가능한(Controllable) 엔터프라이즈 시스템에 안전하게 통합할 것인가?“라는 질문이 그것이다. 이 질문에 대한 답을 찾기 위해서는 단순히 AI 모델의 성능을 높이는 차원을 넘어, <strong>확률적 추론을 확정적 검증(Deterministic Verification)의 틀 안에 가두는 아키텍처 패턴</strong>을 정립해야 한다.</p>
<p>본 장(2.10.1)에서는 이러한 통합을 달성하기 위해 반드시 충족되어야 할 기술적, 구조적 필수 조건들을 심층적으로 분석한다. 우리는 확률적 AI를 시스템의 핵심 의사결정 루프에 포함시키면서도 전체 시스템의 무결성을 유지하기 위한 방법론으로 ‘의미론적 방화벽(Semantic Firewall)’, ‘구조화된 인터페이스(Structured Interface)’, ‘정책 코드화(Policy-as-Code)’, ‘검증 주도 개발(Verification-Driven Development)’, 그리고 ’표준화된 투명성(Standardized Transparency)’을 제안한다.</p>
<p><img src="./2.10.1.0.0%20%ED%86%B5%EC%A0%9C%20%EB%B6%88%EA%B0%80%EB%8A%A5%ED%95%9C%20AI%EB%A5%BC%20%ED%86%B5%EC%A0%9C%20%EA%B0%80%EB%8A%A5%ED%95%9C%20%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9C%BC%EB%A1%9C%20%ED%8E%B8%EC%9E%85%EC%8B%9C%ED%82%A4%EA%B8%B0%20%EC%9C%84%ED%95%9C%20%ED%95%84%EC%88%98%20%EC%A1%B0%EA%B1%B4.assets/image-20260218120638894.jpg" alt="image-20260218120638894" /></p>
<h2>2.  제1조건: 의미론적 방화벽(Semantic Firewall)과 확정적 진실(Deterministic Ground Truth)</h2>
<p>통제 불가능한 AI를 시스템에 편입시키는 첫 번째이자 가장 강력한 조건은 <strong>AI가 시스템의 상태(State)를 직접 변경하지 못하게 하는 것</strong>이다. 대신, AI의 모든 출력은 ’제안(Proposal)’으로 취급되어야 하며, 이 제안은 <strong>확정적 진실(Deterministic Ground Truth)</strong> 에 기반한 <strong>의미론적 방화벽(Semantic Firewall)</strong> 에 의해 검증되어야 한다. 이 접근법은 AI의 창의성과 추론 능력을 활용하되, 그 실행에 대한 최종 권한은 결정론적 알고리즘에 위임하는 것이다.</p>
<h3>2.1  오라클 문제(The Oracle Problem)의 재해석: 테스트에서 런타임으로</h3>
<p>소프트웨어 테스팅 분야에서 오랫동안 난제였던 <strong>‘오라클 문제(The Oracle Problem)’</strong> 는 AI 시스템 엔지니어링에서 새로운 형태로 부상하고 있다. 전통적인 오라클 문제는 “테스트 결과가 맞는지 틀린지 판단할 수 있는 절대적 기준(오라클)이 존재하지 않거나 비용이 너무 높다“는 것이었다. 기존 소프트웨어에서는 명세(Specification)나 이전 버전의 소프트웨어를 오라클로 사용할 수 있었으나, AI 시스템, 특히 생성형 AI에서는 모든 입력에 대한 정답(Ground Truth)을 미리 정의하는 것이 불가능하다. AI의 출력은 확률적이며, 동일한 입력에 대해서도 매번 다른 결과를 내놓을 수 있기 때문이다.</p>
<p>따라서 AI 통합 시스템에서의 오라클은 ’사전 정의된 정답’을 대조하는 정적인 비교자가 아니라, <strong>‘런타임 불변식(Runtime Invariants)’</strong> 을 검증하는 동적인 감시자(Monitor)로 진화해야 한다. 이는 테스팅 단계에 머물렀던 오라클의 개념을 운영 단계(Runtime)로 확장하는 것을 의미한다.</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>전통적 소프트웨어 오라클</strong></th><th><strong>AI 시스템의 런타임 오라클</strong></th></tr></thead><tbody>
<tr><td><strong>검증 대상</strong></td><td>입력 <span class="math math-inline">X</span>에 대한 출력 <span class="math math-inline">Y</span>의 정확성</td><td>입력 <span class="math math-inline">X</span>에 대해 생성된 행동 계획 <span class="math math-inline">P</span>의 안전성 및 논리적 정합성</td></tr>
<tr><td><strong>기준</strong></td><td>예상 출력값 <span class="math math-inline">Y_{expected}</span> (고정값)</td><td>시스템 안전 제약 조건 <span class="math math-inline">C</span> 및 지식 그래프 <span class="math math-inline">G</span> 상태</td></tr>
<tr><td><strong>작동 시점</strong></td><td>개발 및 배포 전 테스트 단계</td><td>실시간 운영(Inference/Execution) 단계</td></tr>
<tr><td><strong>실패 시 대응</strong></td><td>버그 리포트 생성 및 수정</td><td>AI 제안 거부(Reject) 및 재생성 요청 또는 안전 모드 전환</td></tr>
</tbody></table>
<h3>2.2  네트워크 지식 그래프(NKG)를 통한 상태 검증: G-SPEC 프레임워크 사례</h3>
<p>이러한 런타임 오라클을 구현하는 가장 효과적인 방법 중 하나는 <strong>지식 그래프(Knowledge Graph)</strong> 를 확정적 진실(Ground Truth)의 원천으로 사용하는 것이다. 최근 연구된 <strong>G-SPEC (Graph-Symbolic Policy Enforcement and Control)</strong> 프레임워크 는 이 접근법의 정수를 보여준다. 통신 네트워크와 같이 고도의 신뢰성이 요구되는 인프라에서 AI 에이전트(TSLAM-4B 등)를 활용하기 위해, 이 연구는 확률적 에이전트를 확정적 데이터와 정책 사이에 배치하는 ’샌드위치 구조(Governance Triad)’를 제안한다.</p>
<p>G-SPEC 아키텍처는 다음과 같은 3계층으로 구성된다 :</p>
<ol>
<li><strong>Layer 1: 확정적 진실 (NKG, Network Knowledge Graph)</strong></li>
</ol>
<ul>
<li>시스템의 현재 상태(토폴로지, 리소스 할당량, 장비 상태 등)를 실시간으로 반영하는 그래프 데이터베이스이다.</li>
<li>이 계층은 AI가 학습한 ’내재적 지식(Parametric Knowledge)’이 아니라, 시스템이 보증하는 ’외부적 사실(Explicit Fact)’이다. AI는 학습 데이터의 시점에 갇혀 있거나 환각을 일으킬 수 있지만, NKG는 쿼리 시점의 물리적 실재를 반영한다.</li>
<li>예를 들어, 3GPP 표준 온톨로지를 따르는 NKG는 네트워크 장비 간의 연결 관계를 명확히 정의하며, 이는 AI가 임의로 변경하거나 부정할 수 없는 진실이다.</li>
</ul>
<ol start="2">
<li><strong>Layer 2: 확률적 추론 (AI Agent)</strong></li>
</ol>
<ul>
<li>LLM 또는 에이전트 모델이 위치하는 계층이다. 여기서는 불확실성이 허용된다. 에이전트는 복잡한 문제를 해결하기 위해 창의적인 계획(Plan)을 수립한다.</li>
<li>에이전트는 “장비 A의 트래픽을 분산시키라“는 고수준의 의도(Intent)를 입력받아, 구체적인 설정 변경 명령(Action Sequence)을 생성한다. 하지만 이 계획은 즉시 실행되지 않고 검증 계층으로 전달된다.</li>
</ul>
<ol start="3">
<li><strong>Layer 3: 확정적 정책 검증 (Governance Control Plane with SHACL)</strong></li>
</ol>
<ul>
<li>AI가 생성한 계획(JSON 출력 등)은 실행되기 전, Layer 1의 NKG를 대상으로 <strong>SHACL(Shapes Constraint Language)</strong> 과 같은 검증 언어를 통해 시뮬레이션된다.</li>
<li>예를 들어, AI가 “장비 A에서 장비 B로 트래픽을 우회하라“는 명령을 내렸다면, 검증 계층은 NKG를 조회하여 “장비 B가 현재 활성(Active) 상태인가?”, “A와 B 사이에 물리적 연결이 존재하는가?”, “이 변경이 전체 슬라이스 용량 제한을 위반하지 않는가?“를 수학적으로 검증한다.</li>
<li>만약 검증을 통과하지 못하면 해당 제안은 거부되고, 에이전트에게 피드백이 제공되어 수정된 계획을 수립하게 하거나 작업을 중단한다.</li>
</ul>
<p>이 구조에서 AI는 ’운전자’가 아니라 ’내비게이터’의 역할을 수행하며, 실제 핸들(실행 권한)은 확정적 검증 모듈이 쥐고 있다. 연구 결과에 따르면, 이러한 확정적 그래프 검증을 도입했을 때 5G 네트워크 관리 시나리오에서 안전 위반 사례가 0건으로 감소했으며, AI 단독 운영 시 발생했던 14.6%의 환각성 오류를 94.1%까지 성공적으로 수정할 수 있었다. 이는 확률적 모델이 가진 한계를 결정론적 시스템 설계로 보완할 수 있음을 강력하게 시사한다.</p>
<p><img src="./2.10.1.0.0%20%ED%86%B5%EC%A0%9C%20%EB%B6%88%EA%B0%80%EB%8A%A5%ED%95%9C%20AI%EB%A5%BC%20%ED%86%B5%EC%A0%9C%20%EA%B0%80%EB%8A%A5%ED%95%9C%20%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9C%BC%EB%A1%9C%20%ED%8E%B8%EC%9E%85%EC%8B%9C%ED%82%A4%EA%B8%B0%20%EC%9C%84%ED%95%9C%20%ED%95%84%EC%88%98%20%EC%A1%B0%EA%B1%B4.assets/image-20260218120703204.jpg" alt="image-20260218120703204" /></p>
<h2>3.  제2조건: 구조화된 인터페이스(Structured Interface)와 문법적 강제(Grammar-Constrained Decoding)</h2>
<p>통제 불가능한 AI를 시스템에 편입시키기 위한 두 번째 조건은 AI와 시스템 간의 대화 프로토콜을 인간의 언어(자연어)가 아닌 <strong>기계의 언어(구조화된 데이터)</strong> 로 강제하는 것이다. 자연어는 모호하고 해석의 여지가 많아 시스템 통합에 적합하지 않다. 반면 JSON, XML, YAML과 같은 구조화된 형식은 스키마(Schema)를 통해 유효성을 엄격히 검증할 수 있다.</p>
<h3>3.1  JSON 스키마(JSON Schema)의 역할: 확률을 규격으로</h3>
<p>JSON 스키마는 AI의 출력 형식을 제어하는 ’계약(Contract)’이다. 단순히 “JSON으로 답해줘“라고 프롬프팅하는 것(JSON Mode)과, 스키마를 엄격하게 따르도록 강제하는 것(Structured Outputs) 사이에는 큰 차이가 있다.</p>
<ul>
<li><strong>JSON Mode (약한 통제):</strong> AI가 JSON처럼 보이는 텍스트를 생성하도록 유도한다. 프롬프트에 “JSON 포맷을 사용하라“고 지시하는 방식이다. 하지만 필수 필드가 누락되거나, 데이터 타입이 틀리거나(숫자 필드에 문자열 포함), 정의되지 않은 열거형(Enum) 값을 뱉을 위험이 여전히 존재한다. 이는 여전히 확률적 생성에 의존하기 때문이다.</li>
<li><strong>Structured Outputs (강한 통제):</strong> 최신 LLM 엔지니어링에서는 <strong>‘제약된 디코딩(Constrained Decoding)’</strong> 또는 <strong>‘문법 기반 샘플링(Grammar-Based Sampling)’</strong> 기법을 사용한다. 이는 AI가 토큰을 생성할 때, 미리 정의된 스키마(예: Pydantic 모델, JSON Schema)에 위배되는 토큰은 후보군에서 아예 배제해버리는 기술이다. 즉, 문법적으로 틀린 출력을 생성할 확률을 수학적으로 0%로 만든다.</li>
</ul>
<h3>3.2  문법 기반 샘플링의 메커니즘과 이점</h3>
<p>이 기술은 AI의 확률적 생성 과정에 개입하여, 현재 생성 중인 문법 구조상 올 수 없는 토큰의 확률(Logit)을 <span class="math math-inline">-\infty</span>로 마스킹(Masking)함으로써 작동한다. 예를 들어, 스키마에서 <code>"status": "enum(['active', 'inactive'])"</code>라고 정의했다고 가정해보자. AI가 <code>"status": "</code>까지 생성한 후 다음 토큰을 선택할 때, 모델의 원래 확률 분포는 <code>'active'</code>, <code>'inactive'</code> 외에도 <code>'pending'</code>, <code>'error'</code>, <code>'unknown'</code> 등 다양한 단어를 포함할 수 있다. 그러나 문법 기반 샘플링을 적용하면 <code>'active'</code>와 <code>'inactive'</code>를 제외한 모든 다른 단어의 생성 확률은 강제로 0이 된다. 따라서 모델은 오직 스키마에 정의된 값 중 하나만을 선택할 수밖에 없다.</p>
<p>이러한 기법의 도입은 다음과 같은 이점을 제공한다:</p>
<ol>
<li><strong>타입 안전성(Type Safety) 보장:</strong> 생성된 데이터가 다운스트림 시스템의 데이터 파이프라인을 깨뜨리지 않음을 보장한다. <code>try-catch</code> 블록이나 재시도(Retry) 로직을 획기적으로 줄일 수 있다.</li>
<li><strong>환각의 범위 축소:</strong> AI가 자유롭게 텍스트를 생성할 때보다 사고의 범위가 스키마 구조 내로 한정되므로, 엉뚱한 내용을 생성할 여지가 줄어든다. 이는 AI의 ’창의성’을 시스템이 필요한 ‘형식’ 안으로 제한하는 효과가 있다.</li>
<li><strong>파싱 오류 제거:</strong> <code>JSON.parse()</code> 실패와 같은 기본적인 문법 오류가 원천 차단된다. 이는 시스템의 견고성(Robustness)을 크게 향상시킨다.</li>
</ol>
<p>또한, 최근 연구에 따르면 이러한 스키마 기반 추출(Schema-Guided Extraction)은 온도(Temperature) 설정에 강건한 특성을 보인다. 온도 값을 낮추어 결정론적 출력을 유도하는 것이 일반적이지만, 스키마 제약이 있는 경우 온도 값이 다소 높더라도 구조적 무결성은 유지되므로, 창의성이 필요한 필드와 정확성이 필요한 필드를 동시에 처리할 때 유리하다.</p>
<p>![image-20260218120727531](./2.10.1.0.0%20%ED%86%B5%EC%A0%9C%20%EB%B6%88%EA%B0%80%EB%8A%A5%ED%95%9C%20AI%EB%A5%BC%20%ED%86%B5%EC%A0%9C%20%EA%B0%80%EB%8A%A5%ED%95%9C%20%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9C%BC%EB%A1%9C%20%ED%8E%B8%EC%9E%85%EC%8B%9C%ED%82%A4%EA%B8%B0%20%EC%9C%84%ED%95%9C%20%ED%95%84%EC%88%98%20%EC%A1%B0%EA%B1%B4.assets/image-20260218120727531.png</p>
<h2>4.  제3조건: 정책 코드화(Policy-as-Code)와 런타임 가드레일(Runtime Guardrails)</h2>
<p>구조적 무결성(JSON 형식이 맞는가?)을 넘어, 내용적 정당성(이 행동이 허용되는가?)을 검증하기 위해서는 <strong>정책 코드화(Policy-as-Code)</strong> 기반의 런타임 가드레일이 필수적이다. 형식이 올바르다고 해서 그 내용이 안전하거나 유효한 것은 아니다. 따라서 AI의 ’의도’가 비즈니스 규칙과 안전 정책에 부합하는지 실시간으로 심사하는 과정이 필요하다.</p>
<h3>4.1  결정론적 정책 엔진의 도입</h3>
<p>AI 시스템의 통제권은 AI 모델 내부가 아닌, 모델 외부의 <strong>정책 엔진(Policy Engine)</strong> 에 있어야 한다. 이는 G-SPEC 연구에서 SHACL을 사용하여 토폴로지 제약을 검증한 것과 같은 맥락이다. 엔터프라이즈 환경에서는 <strong>OPA (Open Policy Agent)</strong> 와 같은 범용 정책 엔진이 널리 사용될 수 있다.</p>
<p>정책 엔진은 다음과 같은 질문을 런타임에 던지고, <code>Allow</code> 또는 <code>Deny</code>의 확정적(Binary) 결정을 내린다. 이는 불확실한 AI의 판단을 확정적인 비즈니스 룰로 필터링하는 과정이다.</p>
<ul>
<li><strong>리소스 제약(Resource Constraints):</strong> “이 AI 에이전트가 요청한 $10,000 송금은 이 사용자의 일일 한도를 초과하는가?” 또는 “슬라이스 대역폭 할당량이 물리적 한계인 100Mbps를 넘지 않는가?”.</li>
<li><strong>상태 제약(State Constraints):</strong> “삭제하려는 데이터베이스 테이블이 현재 프로덕션 트래픽을 받고 있는가?” 또는 “타겟 노드가 유지보수 모드가 아닌 활성 상태인가?”.</li>
<li><strong>시간적 일관성(Temporal Consistency):</strong> “AI가 참조하고 있는 데이터가 15초 이내의 최신 데이터인가?” 이를 통해 ‘토폴로지 드리프트(Topology Drift)’ 현상, 즉 AI가 학습한 과거의 네트워크 상태와 현재의 실제 상태가 달라져 발생하는 오류를 방지한다.</li>
</ul>
<h3>4.2  의미론적 폭발 반경(Semantic Blast Radius) 제어</h3>
<p>특히 중요한 가드레일은 <strong>‘폭발 반경(Blast Radius)’</strong> 의 제어다. AI가 내리는 결정이 시스템 전체에 미칠 수 있는 영향력을 정량화하고 제한해야 한다. AI 모델이 탈옥(Jailbreak)되거나 오작동할 경우, 단일 명령으로 시스템 전체를 붕괴시키는 것을 막기 위함이다.</p>
<p>G-SPEC 연구에서는 “단일 에포크(Epoch) 내에서 전체 슬라이스 용량의 20% 이상을 변경하는 제안은 무조건 거부한다“는 <strong>변화 델타 함수(Change Delta Function)</strong> 를 도입했다.<br />
<span class="math math-display">
\delta(S_t, S_{t+1}) &gt; \theta \implies \text{Reject Action}
</span><br />
여기서 <span class="math math-inline">S_t</span>는 현재 상태, <span class="math math-inline">S_{t+1}</span>은 AI 제안 적용 후의 상태, <span class="math math-inline">\theta</span>는 허용된 임계값이다. 이처럼 AI의 행동이 초래할 변화의 크기를 수학적으로 계산하고, 임계값을 넘으면 강제로 차단하는 메커니즘은 통제 불가능한 AI의 ’재앙적 행동(Catastrophic Action)’을 막는 최후의 보루다. 이러한 접근은 금융 거래에서의 서킷 브레이커(Circuit Breaker)와 유사하며, 시스템의 안정성을 보장하는 핵심 장치로 작동한다.</p>
<p>또한, <strong>멱등성(Idempotency)</strong> 원칙도 중요하다. 비결정론적 AI가 동일한 요청에 대해 매번 다른 표현을 사용하더라도, 시스템에 적용되는 결과는 멱등성을 유지해야 한다. 예를 들어, AI가 “서버를 켜라”, “서버를 시작해라”, “부팅 프로세스를 개시하라“라고 다양하게 말하더라도, 실제 인프라에서는 <code>start_instance(id)</code>라는 단일하고 멱등적인 함수가 호출되어야 하며, 이미 켜져 있는 서버에 대해 중복 실행되더라도 부작용이 없어야 한다.</p>
<h2>5.  제4조건: 검증 주도 개발(Verification-Driven Development, VDD)과 적대적 루프</h2>
<p>AI 시스템을 개발하는 프로세스 자체도 변화해야 한다. 전통적인 TDD(테스트 주도 개발)를 넘어, <strong>검증 주도 개발(Verification-Driven Development, VDD)</strong> 방법론이 요구된다. 확률적 컴포넌트는 고정된 테스트 케이스만으로는 충분히 검증할 수 없기 때문에, 개발 과정 자체에 동적인 검증 메커니즘을 내재화해야 한다.</p>
<h3>5.1  VDD의 핵심: 생성자와 비평가의 적대적 공존</h3>
<p>VDD는 AI 에이전트(Builder)가 코드를 작성하거나 계획을 수립할 때, 또 다른 적대적 AI 에이전트(Adversarial Critic/Sarcasmotron)나 정형 검증 도구(Formal Verifier)가 이를 즉시 공격하고 검증하는 루프를 의미한다.</p>
<ul>
<li><strong>Builder AI:</strong> 기능 구현에 집중하며, 다소 낙관적인(Optimistic) 경향이 있다. 사용자 요구사항을 충족하는 솔루션을 빠르게 생성한다.</li>
<li><strong>Critic AI:</strong> “이 코드의 보안 취약점은 무엇인가?”, “입력값이 비었을 때 어떻게 되는가?”, “이 계획이 실패할 시나리오는 무엇인가?“와 같이 부정적이고 비판적인 관점에서 검증한다. 중요한 점은 Critic AI의 문맥(Context)을 매번 리셋하여, Builder AI와의 대화가 길어짐에 따라 발생할 수 있는 ’담합’이나 ‘친밀화(Agreeableness)’ 편향을 방지해야 한다는 것이다.</li>
<li><strong>정형 검증기(Formal Verifier):</strong> 수학적 증명을 통해 코드의 무결성을 보장한다. 예를 들어, 스마트 컨트랙트나 보안 프로토콜과 같이 절대적 무결성이 필요한 영역에서는 LLM이 생성한 코드를 Dafny나 Coq, Z3와 같은 증명 지원 도구로 검증하는 파이프라인이 연구되고 있다. 이는 “코드가 실행 가능한가?“를 넘어 “코드가 사양을 수학적으로 만족하는가?“를 증명한다.</li>
</ul>
<h3>5.2  에이전트의 자기 검증(Self-Correction) 및 동적 라우팅</h3>
<p>단순히 외부에서 검증하는 것을 넘어, 에이전트 스스로가 자신의 출력을 검증하는 절차를 내재화해야 한다. 이를 위해 <strong>Chain-of-Thought(CoT)</strong> 과정에 ’검증 단계’를 명시적으로 포함시키거나, <strong>자신감 점수(Confidence Score)</strong> 를 스스로 계산하게 하여 확신이 낮을 때는 더 강력한 모델이나 인간에게 위임(Human-in-the-loop)하도록 라우팅해야 한다.</p>
<p>연구에 따르면, 엔트로피(Entropy) 기반의 불확실성 신호를 활용하여, 쉬운 작업은 작은 모델(Efficient Model)이 처리하고 불확실성이 높은 작업은 큰 모델(Capability-Dense Model)이 처리하도록 동적 라우팅(Adaptive Routing)을 할 때, 추론 비용은 37.9% 절감하면서도 정확도는 고성능 모델 수준(97.5%)을 유지할 수 있었다. 이는 시스템 효율성과 안전성을 동시에 확보하는 전략이다.</p>
<p><img src="./2.10.1.0.0%20%ED%86%B5%EC%A0%9C%20%EB%B6%88%EA%B0%80%EB%8A%A5%ED%95%9C%20AI%EB%A5%BC%20%ED%86%B5%EC%A0%9C%20%EA%B0%80%EB%8A%A5%ED%95%9C%20%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9C%BC%EB%A1%9C%20%ED%8E%B8%EC%9E%85%EC%8B%9C%ED%82%A4%EA%B8%B0%20%EC%9C%84%ED%95%9C%20%ED%95%84%EC%88%98%20%EC%A1%B0%EA%B1%B4.assets/image-20260218120751099.jpg" alt="image-20260218120751099" /></p>
<h2>6.  제5조건: 투명성(Transparency)과 설명 가능성의 표준화</h2>
<p>마지막 조건은 시스템이 ‘왜’ 그런 결정을 내렸는지 추적 가능해야 한다는 것이다. 통제 불가능한 AI가 블랙박스로 남아있다면, 시스템 전체의 신뢰도를 담보할 수 없으며 사고 발생 시 원인 규명도 불가능하다.</p>
<h3>6.1  IEEE 7001-2021 표준의 준수</h3>
<p>AI 시스템의 투명성을 위한 국제 표준인 <strong>IEEE 7001-2021</strong>은 자율 시스템의 투명성을 측정 가능하고 테스트 가능한 수준으로 정의한다. 이 표준에 따르면, 안전에 중요한(Safety-critical) 자율 시스템은 다음 정보를 제공해야 한다:</p>
<ul>
<li><strong>결정의 근거(Why did it do that?):</strong> AI가 특정 행동을 선택한 논리적 경로.</li>
<li><strong>대안의 기각 사유(Why not that?):</strong> 다른 가능한 행동들을 왜 선택하지 않았는지에 대한 설명.</li>
<li><strong>데이터 출처:</strong> 해당 결정에 영향을 미친 훈련 데이터나 참조 데이터(RAG Context)의 출처.</li>
</ul>
<p>이러한 투명성은 단순한 윤리적 요구사항이 아니라, 시스템의 유지보수와 디버깅을 위한 공학적 필수 조건이다.</p>
<h3>6.2  추론 과정의 기록(Logging Reasoning Traces)</h3>
<p>단순히 입출력 로그만 남기는 것으로는 부족하다. AI 에이전트가 내부적으로 수행한 <strong>사고의 연쇄(Chain of Thought)</strong> 전체를 구조화된 로그로 저장해야 한다. G-SPEC 연구에서는 에이전트가 행동을 선택하기 전에 생성한 ’Reasoning Trace’를 JSON 필드로 캡처하여, 사후 감사(Audit)가 가능하도록 했다. 이는 사고 발생 시 “AI가 왜 그런 판단을 했는가?“를 역추적하는 데 결정적인 단서가 된다.</p>
<p>또한, 런타임 검증(Runtime Verification, RV) 기술을 결합하여, AI가 생성한 설명이 실제 시스템 상태와 일치하는지 확인하는 ’예측적 모니터링(Predictive Monitoring)’을 도입할 수 있다. 이는 AI가 그럴듯한 거짓말(Hallucination)로 자신의 행동을 정당화하는 것을 방지하는 역할을 한다.</p>
<h2>7. 결론: 샌드위치 아키텍처(The Sandwich Pattern)의 필연성</h2>
<p>요약하자면, 통제 불가능한 AI를 통제 가능한 시스템에 편입시키는 핵심 전략은 <strong>‘샌드위치 패턴(The Sandwich Pattern)’</strong> 으로 귀결된다. 확률적이고 창의적이지만 위험한 AI 모델(Layer 2)을, <strong>확정적 데이터(Layer 1, Ground Truth)</strong> 라는 하부 기반과 <strong>확정적 정책(Layer 3, Policy Guardrails)</strong> 이라는 상부 통제층 사이에 끼워 넣는 것이다.</p>
<p>이 구조 하에서 AI는 더 이상 시스템의 무소불위의 지배자가 아니다. AI는 시스템이 제공하는 데이터(NKG)를 읽고, 시스템이 허용하는 문법(JSON Schema)으로 제안하며, 시스템이 검증하는 정책(SHACL/OPA)을 통과할 때만 세상을 변화시킬 수 있는 강력하지만 철저히 관리되는 <strong>‘지능형 컴포넌트’</strong> 로 재정의된다. 이것이 바로 AI 엔지니어링이 나아가야 할 필연적인 미래 아키텍처다.</p>
<p><img src="./2.10.1.0.0%20%ED%86%B5%EC%A0%9C%20%EB%B6%88%EA%B0%80%EB%8A%A5%ED%95%9C%20AI%EB%A5%BC%20%ED%86%B5%EC%A0%9C%20%EA%B0%80%EB%8A%A5%ED%95%9C%20%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9C%BC%EB%A1%9C%20%ED%8E%B8%EC%9E%85%EC%8B%9C%ED%82%A4%EA%B8%B0%20%EC%9C%84%ED%95%9C%20%ED%95%84%EC%88%98%20%EC%A1%B0%EA%B1%B4.assets/image-20260218120813369.jpg" alt="image-20260218120813369" /></p>
<h2>8. 참고 자료</h2>
<ol>
<li>Stochastic vs. Deterministic Optimization: A Comparative Analysis, https://www.ijesi.org/papers/Vol(14)i5/14057782.pdf</li>
<li>Deterministic AI vs Non-Deterministic AI: Key Differences - Kubiya, https://www.kubiya.ai/blog/deterministic-ai-vs-non-deterministic-ai</li>
<li>AI – Deterministic vs. Stochastic Systems - YouTube, https://www.youtube.com/shorts/UA97rJ7aK2E</li>
<li>Deterministic vs Stochastic Environment in AI - GeeksforGeeks, https://www.geeksforgeeks.org/artificial-intelligence/deterministic-vs-stochastic-environment-in-ai/</li>
<li>AI Environments: Deterministic vs Stochastic | PDF - Scribd, https://www.scribd.com/document/464360564/AI-docx</li>
<li>Martin Fowler on Preparing for AI’s Nondeterministic Computing …, https://thenewstack.io/martin-fowler-on-preparing-for-ais-nondeterministic-computing/</li>
<li>Graph-Symbolic Policy Enforcement and Control (G-SPEC) - arXiv, https://arxiv.org/pdf/2512.20275</li>
<li>Revising human-systems engineering principles for embedded AI …, https://pmc.ncbi.nlm.nih.gov/articles/PMC10790827/</li>
<li>Building Trust in Non-Deterministic Systems: A Framework for, https://www.itential.com/blog/company/ai-networking/building-trust-in-non-deterministic-systems-a-framework-for-responsible-ai-operations/</li>
<li>A Neuro-Symbolic Framework for Safe Agentic AI in 5G … - arXiv, https://arxiv.org/html/2512.20275v1</li>
<li>How Effectively Does Metamorphic Testing Alleviate the Oracle, https://www.computer.org/csdl/journal/ts/2014/01/06613484/13rRUNvya2K</li>
<li>The Oracle Problem - YLD, https://www.yld.io/blog/the-oracle-problem</li>
<li>(PDF) The Oracle Problem in Software Testing: A Survey, https://www.researchgate.net/publication/276255185_The_Oracle_Problem_in_Software_Testing_A_Survey</li>
<li>Testing AI Systems: Handling the Test Oracle Problem - DEV …, https://dev.to/qa-leaders/testing-ai-systems-handling-the-test-oracle-problem-3038</li>
<li>LLM-Powered Security Test Generation: Oracles, Vulnerability, https://www.computer.org/csdl/magazine/co/2026/02/11370987/2dOhh5MzH1e</li>
<li>(PDF) Watchdogs and Oracles: Runtime Verification Meets Large, https://www.researchgate.net/publication/397681901_Watchdogs_and_Oracles_Runtime_Verification_Meets_Large_Language_Models_for_Autonomous_Systems</li>
<li>(PDF) Graph-Symbolic Policy Enforcement and Control (G-SPEC), https://www.researchgate.net/publication/399026954_Graph-Symbolic_Policy_Enforcement_and_Control_G-SPEC_A_Neuro-Symbolic_Framework_for_Safe_Agentic_AI_in_5G_Autonomous_Networks</li>
<li>Network Knowledge Graph (NKG) - Emergent Mind, https://www.emergentmind.com/topics/network-knowledge-graph-nkg</li>
<li>Structured outputs on Amazon Bedrock: Schema-compliant AI … - AWS, https://aws.amazon.com/blogs/machine-learning/structured-outputs-on-amazon-bedrock-schema-compliant-ai-responses/</li>
<li>Structured Output Generation in LLMs: JSON Schema and Grammar, https://medium.com/@emrekaratas-ai/structured-output-generation-in-llms-json-schema-and-grammar-based-decoding-6a5c58b698a6</li>
<li>The guide to structured outputs and function calling with LLMs, https://agenta.ai/blog/the-guide-to-structured-outputs-and-function-calling-with-llms</li>
<li>How JSON Schema Works for LLM Data - Latitude.so, https://latitude.so/blog/how-json-schema-works-for-llm-data</li>
<li>Structured model outputs | OpenAI API, https://developers.openai.com/api/docs/guides/structured-outputs/</li>
<li>How OpenAI’s Structured Outputs are Transforming API Reliability, https://lablab.ai/blog/how-openais-structured-outputs-are-transforming-api-reliability-and-developer-control</li>
<li>AI-Based Table Extraction from Technical Documents - ResearchGate, https://www.researchgate.net/publication/395246223_AI-Based_Table_Extraction_from_Technical_Documents</li>
<li>Managing the non-deterministic nature of generative AI - Pariveda, https://parivedasolutions.com/perspectives/managing-the-non-deterministic-nature-of-generative-ai/</li>
<li>Deterministic and Stochastic Machine Learning Classification Models, https://www.mdpi.com/2227-7390/13/3/411</li>
<li>Verification-Driven Development (VDD) via Iterative Adversarial, https://gist.github.com/dollspace-gay/45c95ebfb5a3a3bae84d8bebd662cc25</li>
<li>Toward Practical Deductive Verification: Insights from a Qualitative, https://arxiv.org/html/2510.20514v2</li>
<li>mihaicode/context-engineering-framework - GitHub, https://github.com/mihaicode/context-engineering-framework</li>
<li>Veri-Sure: A Contract-Aware Multi-Agent Framework with Temporal, https://arxiv.org/html/2601.19747v1</li>
<li>Towards Verified Artificial Intelligence - arXiv, https://arxiv.org/pdf/1606.08514</li>
<li>I spent 14 months building something to catch AI mistakes … - Reddit, https://www.reddit.com/r/indianstartups/comments/1p9ho14/i_spent_14_months_building_something_to_catch_ai/</li>
<li>AdaptEvolve: Improving Efficiency of Evolutionary AI Agents through, https://arxiv.org/html/2602.11931v1</li>
<li>IEEE Standard for Transparency of Autonomous Systems, https://rai-toolkit.github.io/governance/standard/IEEE-7001-2021-IEEE-Standard-for-Transp/</li>
<li>Presentation IEEE 7001 Std.pdf - Slideshare, https://www.slideshare.net/slideshow/presentation-ieee-7001-stdpdf/251506622</li>
<li>IEEE P7001: A Proposed Standard on Transparency - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC8351056/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>