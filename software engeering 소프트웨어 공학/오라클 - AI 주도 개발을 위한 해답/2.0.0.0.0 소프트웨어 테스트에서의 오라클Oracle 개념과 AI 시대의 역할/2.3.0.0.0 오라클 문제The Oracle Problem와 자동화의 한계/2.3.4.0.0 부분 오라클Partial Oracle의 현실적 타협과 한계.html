<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2.3.4 부분 오라클(Partial Oracle)의 현실적 타협과 한계</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2.3.4 부분 오라클(Partial Oracle)의 현실적 타협과 한계</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 2. 소프트웨어 테스트에서의 오라클(Oracle) 개념과 AI 시대의 역할</a> / <a href="index.html">2.3 오라클 문제(The Oracle Problem)와 자동화의 한계</a> / <span>2.3.4 부분 오라클(Partial Oracle)의 현실적 타협과 한계</span></nav>
                </div>
            </header>
            <article>
                <h1>2.3.4 부분 오라클(Partial Oracle)의 현실적 타협과 한계</h1>
<p>현대 소프트웨어 공학, 특히 기계학습(Machine Learning, ML)과 인공지능(AI)이 주도하는 패러다임에서 가장 해결하기 어려운 난제 중 하나는 이른바 ’오라클 문제(The Oracle Problem)’이다. 전통적인 소프트웨어 테스트 환경에서는 주어진 입력에 대해 시스템이 산출해야 하는 정확하고 결정론적인 예상 결과(Deterministic Ground Truth)를 사전에 정의할 수 있다는 암묵적인 전제가 존재했다. 이 예상 결과를 산출하고 실제 시스템의 출력과 비교하여 시스템의 정확성(Correctness)을 판별하는 메커니즘을 테스트 오라클(Test Oracle)이라고 부른다. 그러나 시스템의 복잡도가 기하급수적으로 증가하고, 방대한 데이터 입력 공간을 가지며, 근본적으로 확률론적 연산에 기반하는 AI 모델이 소프트웨어의 핵심 컴포넌트로 자리 잡으면서, 모든 입력에 대한 완벽한 정답지를 구축하는 것은 이론적으로나 실천적으로 불가능한 영역에 진입했다. 이러한 테스트 불가능한 프로그램(Non-testable programs)의 확산은 학계와 산업계 모두에 심각한 위기를 초래했으며, 완벽한 오라클을 확보할 수 없다는 현실적 제약 속에서 소프트웨어의 품질을 어떻게 보증할 것인가에 대한 깊은 철학적, 공학적 타협을 요구하게 되었다.</p>
<p>이러한 치열한 타협의 산물로서 소프트웨어 테스팅 분야에 깊게 뿌리내린 핵심 개념이 바로 ’부분 오라클(Partial Oracle)’이다. 부분 오라클은 시스템의 모든 동작과 결과값에 대한 완전하고 결정론적인 명세를 제공하는 것을 포기하는 대신, 테스트 대상 시스템(System Under Test, SUT)이 어떠한 상황에서도 반드시 준수해야 하는 ’중요한 일부 속성(properties)’이나 논리적 불변성(invariance)만을 정의하는 하이브리드 검증 메커니즘이다. 완벽한 정답을 모르는 상태에서도 “최소한 이 결과는 명백히 틀렸다“라거나 “이 시스템은 허용 가능한 통계적 범주 내에 있다“라고 확언할 수 있도록 돕는 이 실용적인 접근법은, 지난 수십 년간 고비용의 오라클 구축 딜레마를 완화하며 AI 시대를 지탱하는 중요한 기술적 기반이 되어 왔다. 그러나 AI의 적용 분야가 생명, 금융, 보안 등 미션 크리티컬(Mission-critical)한 영역으로 확장됨에 따라, 부분 오라클이 내포하고 있는 발견법적 한계와 논리적 피상성은 치명적인 기술 부채를 야기하고 있다. 본 절에서는 부분 오라클의 이론적 기원과 진화 과정, 메타모픽 테스팅 및 통계적 테스팅과의 결합 양상을 심도 있게 분석하고, 거대 언어 모델(LLM) 환경에서 이 타협적 접근이 노출하는 근본적인 한계와 그로 인해 발생하는 조용한 실패(Silent failures)의 위험성을 철저히 해부한다.</p>
<h2>1. 테스트 오라클의 분류 체계와 부분 오라클의 위상</h2>
<p>소프트웨어 테스팅 문헌에서 오라클의 진화와 분류 체계를 추적하는 것은 부분 오라클의 본질을 이해하는 첫걸음이다. Earl T. Barr 등의 기념비적인 연구 <em>The Oracle Problem in Software Testing: A Survey</em>에 따르면, 테스트 오라클은 정보의 출처와 구성 방식에 따라 크게 세 가지 범주로 분류된다. 첫째는 시스템이 수행해야 할 동작을 수학적 논리나 정형화된 모델로 완벽히 기술한 명세 기반 오라클(Specified Test Oracle)이다. 둘째는 설계 문서, 이전 버전의 시스템, 혹은 완전히 별개로 작성된 프로그램 등 소프트웨어의 산출물이나 외부 자원으로부터 기대 결과를 이끌어내는 파생 오라클(Derived Test Oracle)이다. 셋째는 메모리 누수나 데드락(Deadlock)과 같이 명세가 없어도 시스템에 충돌을 일으키는 비정상적 상태를 감지하는 암묵적 오라클(Implicit Test Oracle)이다.</p>
<p>부분 오라클은 이 분류 체계에서 명세 기반 오라클과 파생 오라클의 특성을 모두 결합한 ‘하이브리드(Hybrid)’ 형태로 규정된다. 부분 오라클이 소프트웨어 공학의 역사 전면에 등장하게 된 배경을 명확히 이해하기 위해서는, 파생 오라클의 일종이자 오랫동안 오라클 문제의 대안으로 여겨졌던 ’의사 오라클(Pseudo-Oracle)’과의 학술적 경계를 뚜렷이 구분해야 한다. 1982년 저명한 컴퓨터 과학자 일레인 웨유커(Elaine Weyuker)에 의해 정립된 의사 오라클의 개념은, 테스트 대상 프로그램과 완벽히 동일한 명세를 바탕으로 하되 독립적인 팀이나 전혀 다른 알고리즘을 사용하여 별도로 작성된 ’대체 프로그램’을 의미한다. 테스트를 수행할 때 동일한 입력 데이터를 원본 시스템과 의사 오라클에 동시에 주입하고, 두 시스템이 산출한 결과를 직접 비교하여 불일치(Discrepancy)가 발생할 경우 이를 잠재적인 결함으로 간주하여 조사하는 방식이다. 우주 항공이나 원자력 발전소 제어 시스템 등 고도의 안전성이 요구되는 도메인에서 주로 활용된 N-버전 프로그래밍(N-version programming)이 의사 오라클을 활용한 대표적인 아키텍처이다.</p>
<p>그러나 의사 오라클은 현대의 복잡계 소프트웨어 및 AI 개발 환경에 적용하기에는 감당하기 어려운 현실적 제약과 모순을 안고 있다. 가장 치명적인 문제는 막대한 오라클 비용(Oracle Cost)이다. 단순히 테스트를 위해 동일한 기능을 수행하는 거대한 소프트웨어나 인공지능 신경망을 이중으로 개발하고 유지보수하는 것은 막대한 인력과 컴퓨팅 자원의 낭비를 초래한다. 더욱이 복잡한 부동소수점 연산을 다루는 과학 계산 프로그램이나, 학습 데이터의 미세한 노이즈에도 가중치가 달라지는 기계학습 모델의 경우, 두 개의 서로 다른 구현체가 ‘완전히 동일한’ 출력을 산출하는 경우는 거의 없다. 설계의 차이, 컴파일러의 해석 차이, 하드웨어 아키텍처의 차이로 인해 필연적으로 발생하는 미세한 오차는 두 시스템 간의 단순 비교를 무의미하게 만들며, 수많은 허위 경보(False positive)를 발생시켜 테스트 결과를 해석하는 인간의 정성적 비용을 기하급수적으로 증가시킨다. 또한, 명세서 자체에 내재된 모호함이나 논리적 오류가 존재할 경우, 두 독립적인 팀이 개발한 프로그램과 의사 오라클 모두 동일한 오류를 내포하게 되는 구조적 맹점도 존재한다.</p>
<p>이러한 의사 오라클의 파괴적인 비용 구조와 기술적 한계를 극복하기 위해 제안된 현실적인 대안이 바로 부분 오라클이다. 부분 오라클은 시스템과 완벽히 동일한 결과값을 도출해 내는 무거운 대체 프로그램을 필요로 하지 않는다. 대신, 단일 시스템의 입력과 출력, 혹은 여러 번의 실행 결과들 사이에 성립해야 하는 ’논리적, 수학적, 통계적 제약 조건’만을 도출하여 이를 검증의 잣대로 삼는다. 예컨대 삼각함수 사인(sine) 값을 계산하는 프로그램을 개발할 때, 특정 각도에 대한 정확한 부동소수점 결과값을 모른다 하더라도 “결과값은 반드시 -1과 1 사이에 존재해야 한다“라는 사후 조건(Post-condition)을 설정할 수 있다. 이 사후 조건이 바로 기초적인 형태의 부분 오라클이며, 이 범위를 벗어나는 결과가 관측되면 시스템 내부에 결함이 존재한다는 것을 확정적으로 증명할 수 있다. 정답을 모르더라도 오답을 솎아낼 수 있는 이 경제적인 패러다임 전환은 소프트웨어 자동화 테스트의 적용 범위를 비약적으로 넓혔다.</p>
<p>테스트 오라클의 주요 접근 방식을 명확하게 대조하기 위해 아래 표에 각 오라클의 특성을 구조화하였다.</p>
<h2>2. 표 1: 완전 오라클, 의사 오라클, 부분 오라클의 본질적 특성 및 수학적 비교</h2>
<table><thead><tr><th><strong>분류 기준</strong></th><th><strong>완전 오라클 (Ideal Test Oracle)</strong></th><th><strong>의사 오라클 (Pseudo-Oracle)</strong></th><th><strong>부분 오라클 (Partial Oracle)</strong></th></tr></thead><tbody>
<tr><td><strong>개념적 정의</strong></td><td>모든 입력 공간에 대해 사전에 정의된 유일하고 절대적인 정답(Ground Truth)을 제공하는 메커니즘</td><td>원본 시스템과 독립적으로 개발되어 동일한 기능을 수행하는 대체 프로그램 메커니즘</td><td>전체 정답은 알 수 없으나, 시스템이 반드시 만족시켜야 하는 중요한 부분적 속성과 관계를 명시하는 메커니즘</td></tr>
<tr><td><strong>검증의 논리적 구조</strong></td><td>시스템 출력값 <span class="math math-inline">\vert SUT(x) - Expected(x) \vert = 0</span></td><td>두 시스템의 출력 비교 <span class="math math-inline">\vert SUT(x) - Pseudo(x) \vert \le \epsilon</span></td><td>입력-출력 관계나 특정 수학적 불변성 위반 여부 확인 <span class="math math-inline">Property(x, SUT(x)) == True</span></td></tr>
<tr><td><strong>오라클 분류 체계</strong></td><td>완벽한 명세 기반 오라클 (Fully Specified)</td><td>파생 오라클 (Derived)</td><td>명세 기반 및 파생 오라클의 하이브리드 (Hybrid)</td></tr>
<tr><td><strong>인식론적 결함 판별</strong></td><td>“시스템의 출력이 완벽히 정확함(Correct)을 긍정적으로 증명”</td><td>“시스템 출력 간의 불일치(Discrepancy)를 통해 결함 존재 가능성 시사”</td><td>“제약 조건을 위반했을 때, 시스템이 명백히 틀렸음(Incorrect)을 부정적으로 증명”</td></tr>
<tr><td><strong>경제성 및 구축 비용</strong></td><td>극히 높음 (대부분의 현대 소프트웨어에서 실현 불가능)</td><td>매우 높음 (중복된 시스템 개발 및 유지보수 비용 발생)</td><td>상대적으로 낮음 (단일 시스템 내에서 논리적 속성만 모델링)</td></tr>
<tr><td><strong>AI/ML 환경의 적용성</strong></td><td>적용 불가능 (비결정적 모델의 출력을 사전에 완벽히 예측할 수 없음)</td><td>매우 제한적 (방대한 매개변수를 가진 두 신경망이 동일한 출력을 보장할 수 없음)</td><td>광범위하게 적용됨 (확률적 출력의 통계적 검증 및 메타모픽 속성 평가에 주력)</td></tr>
</tbody></table>
<p>부분 오라클은 검증의 목적을 ’정확성의 완벽한 증명’에서 ’오류의 효율적 적발’로 하향 조정함으로써 테스트 불가능한 프로그램의 장벽을 우회한다. 오라클의 지식을 불완전한 상태로 두는 대신 검증 프로세스의 자동화를 획득하는 이 실용적인 교환은, 이어지는 메타모픽 테스팅의 발전과 함께 소프트웨어 공학의 패러다임을 극적으로 변화시키게 된다.</p>
<h2>3. 메타모픽 테스팅(Metamorphic Testing): 부분 오라클의 눈부신 구현과 확장</h2>
<p>부분 오라클의 추상적인 철학을 소프트웨어 공학의 실제 산업 현장과 AI 모델 검증 영역으로 끌어내린 가장 성공적이고 실천론적인 프레임워크가 바로 메타모픽 테스팅(Metamorphic Testing, MT)이다. 1998년 T.Y. Chen에 의해 최초로 고안된 메타모픽 테스팅은 전통적인 단위 테스트나 시스템 테스트가 직면한 오라클 문제의 핵심을 완전히 다른 시각에서 해체한다. 기존의 테스트 패러다임이 개별적인 입력 데이터와 그에 상응하는 독립적이고 확정적인 기대 출력의 ’쌍(Pair)’을 요구했다면, 메타모픽 테스팅은 해당 요구를 과감히 폐기한다. 대신 시스템이 구현하고자 하는 도메인 지식이나 비즈니스 로직에 내재된 본질적인 특성을 분석하여, 여러 번의 독립적인 테스트 실행 결과들 사이에 반드시 존재해야 하는 수학적 혹은 논리적 상관관계, 즉 ’메타모픽 관계(Metamorphic Relations, MR)’를 도출해 낸다.</p>
<p>메타모픽 테스팅의 핵심 메커니즘은 원본 입력과 파생 입력의 의도적인 생성 및 비교에 있다. 테스터는 임의의 초기 입력 데이터를 선정하여 이를 원본 테스트 케이스(Source Test Case)로 삼고 시스템을 실행한다. 그 후, 사전에 정의한 메타모픽 관계에 기반하여 원본 입력을 특정한 규칙에 따라 체계적으로 변형함으로써 새로운 파생 테스트 케이스(Follow-up Test Case)를 자동으로 생성한다. 이 두 가지 입력을 각각 시스템에 주입하여 얻은 출력값들을 비교 분석했을 때, 만약 두 출력 사이의 관계가 사전에 정의된 메타모픽 관계를 위반한다면, 테스터는 초기 입력에 대한 완벽한 정답지(Deterministic Ground Truth)를 전혀 알지 못하더라도 시스템 내부에 중대한 결함이 존재한다는 것을 논리적 모순을 통해 증명할 수 있다.</p>
<p>이러한 접근법은 단순한 수치 해석 알고리즘에서 그 위력을 발휘하기 시작했다. 다시 사인(sine) 함수의 예를 들어보자. 시스템 함수 <span class="math math-inline">f(x)</span>가 존재할 때, 입력 <span class="math math-inline">x_1</span>에 대해 <span class="math math-inline">\sin(x_1)</span>의 정확한 무한 소수점 값을 인간이나 기계가 즉각적으로 계산해 내는 것은 매우 어려운 오라클 문제이다. 하지만 삼각함수의 불변하는 수학적 속성인 <span class="math math-inline">\sin(x) = \sin(\pi - x)</span>를 메타모픽 관계(MR)로 설정하면 상황은 달라진다. 원본 입력 <span class="math math-inline">x_1</span>에 대한 결과값 <span class="math math-inline">f(x_1)</span>을 구하고, 파생 입력 <span class="math math-inline">x_2 = \pi - x_1</span>을 생성하여 그 결과값 <span class="math math-inline">f(x_2)</span>를 산출한 뒤, 이 두 값이 허용 가능한 부동소수점 오차 범위 내에서 일치하는지를 검증하는 것이다. <span class="math math-inline">\vert f(x_1) - f(x_2) \vert \le \epsilon</span> 이라는 속성이 위반되는 순간, 부분 오라클은 시스템의 결함을 확정적으로 보고한다. 절대적인 정답을 구하는 ’계산의 문제’를, 관계의 무결성을 확인하는 ’비교의 문제’로 치환함으로써 오라클 비용을 극적으로 절감한 것이다.</p>
<p>이 강력한 부분 오라클 기법은 수치 해석을 넘어, 정답을 사전에 정의하는 것이 원천적으로 불가능한 현대의 복잡계 도메인으로 빠르게 확장되었다. 가장 대표적인 비수치적 적용 사례는 검색 엔진 테스팅이다. 구글이나 야후와 같은 거대 검색 엔진에 “소프트웨어 공학“이라는 키워드를 입력했을 때 정확히 몇 백만 개의 웹 문서가 어떤 순서로 노출되어야 정상인지 판별할 수 있는 오라클은 지구상에 존재하지 않는다. Zhou 등의 연구자들은 이 오라클 문제를 메타모픽 테스팅으로 우회했다. 이들은 “특정 검색어(Source)에 추가적인 논리 제약 조건(예: AND 연산자로 다른 단어 결합)을 덧붙여 검색(Follow-up)하면, 파생된 검색 결과의 전체 집합은 반드시 초기 검색 결과 집합의 부분집합(Subset)이어야 하며 반환된 문서의 총 개수는 같거나 적어야 한다“라는 명백한 메타모픽 관계를 정의했다. 만약 제약 조건을 추가하여 검색 범위를 좁혔음에도 불구하고 완전히 새로운 문서가 나타나거나 전체 결과 수가 증가한다면, 검색 알고리즘이나 인덱싱 시스템에 심각한 무결성 결함이 있음이 입증된다.</p>
<p>메타모픽 테스팅은 데이터의 볼륨이 방대하고 내부 연산이 극도로 복잡한 과학 계산 소프트웨어(Scientific Software) 도메인에서도 인간의 인지적 병목을 해소하는 구원 투수로 활약했다. 생물 정보학(Bioinformatics) 분야에서 암 세포의 진화나 동물 종의 계통 발생 트리(Phylogenetic tree)를 추론하는 소프트웨어를 테스트할 때, 방대한 DNA 염기서열 데이터로부터 산출될 정확한 생물학적 트리를 미리 아는 것은 불가능하다. 연구자들은 “입력되는 특정 DNA 서열의 순서를 임의로 바꾼다 하더라도, 최종적으로 추론된 계통 발생 트리의 위상 구조(Topology)는 변하지 않아야 한다“라는 메타모픽 관계를 부분 오라클로 설정함으로써, 복잡한 알고리즘의 신뢰성을 효율적으로 검증해 냈다.</p>
<p>산업계에서의 채택 역시 눈부시다. 구글은 2018년 영국의 임페리얼 칼리지 런던에서 파생된 스타트업 GraphicsFuzz를 인수하며, 안드로이드 스마트폰에 탑재되는 그래픽 디바이스 드라이버의 결함을 찾아내는 데 메타모픽 테스팅을 전면적으로 도입했다. 복잡한 셰이더(Shader) 코드를 렌더링할 때 픽셀 단위의 완벽한 결과 이미지를 오라클로 제공할 수 없다는 한계를 인정하고, 원본 셰이더 코드에 시각적으로 아무런 영향을 미치지 않아야 하는 데드 코드(Dead code)를 의미론적으로 주입했을 때(Follow-up), 렌더링 된 최종 이미지가 원본과 동일하게 유지되는지를 검사하는 부분 오라클을 통해 무수히 많은 보안 취약점과 그래픽 깨짐 현상을 자동화된 방식으로 찾아낸 것이다.</p>
<p>이처럼 메타모픽 테스팅은 ’관계의 불변성’을 검증한다는 철학을 통해, 부분 오라클이 지닌 이론적 제약을 실무적인 검증 파이프라인의 강력한 무기로 승화시켰다. 완전한 오라클의 부재 속에서도 기하급수적으로 팽창하는 테스트 시나리오를 자동화하고 수많은 결함을 적발해 낸 이 성과는 소프트웨어 공학의 찬란한 진보이다. 그러나 메타모픽 관계를 도출해 내기 위해서는 여전히 해당 도메인에 대한 깊은 인간의 지식이 요구되며, 관계 자체가 틀리게 설정될 경우 발생하는 위험성 또한 간과할 수 없다. 이러한 한계점은 인공지능이 복잡성을 더해가는 환경에서 더욱 선명하게 드러나게 된다.</p>
<h2>4. 기계학습 프로그램의 태생적 테스트 불가능성과 통계적 부분 오라클의 부상</h2>
<p>전통적인 소프트웨어 테스팅이 규칙(Rule) 기반의 명세서와 결정론적 코드 실행을 전제로 발전해 왔다면, 기계학습(Machine Learning) 알고리즘이 시스템의 중추로 자리 잡는 AI 시대의 도래는 테스팅 패러다임의 근본적인 붕괴와 재구성을 강제했다. 기계학습 기반의 소프트웨어는 폰 노이만 아키텍처 위에서 동작하지만, 그 논리의 형성은 인간 프로그래머의 명시적인 코드 작성이 아닌 방대한 데이터의 통계적 훈련(Training) 과정에서 자생적으로 이루어진다. 이러한 특성으로 인해 훈련 및 예측 프로그램은 본질적으로 전통적 의미의 완벽한 오라클을 구축하는 것이 완전히 불가능한 ’테스트 불가능한 프로그램(Non-testable programs)’으로 간주된다.</p>
<p>일본 산업기술종합연구소(AIST)가 발표한 기계학습 품질 관리 가이드라인(AIQM Guideline)과, 해당 가이드라인의 주요 이론적 배경이 된 학자 신 나카지마(Shin Nakajima)의 선구적인 문헌 *Statistical Partial Oracle for Machine Learning Software Testing (2021)*는 기계학습 테스팅이 직면한 오라클 문제의 심연을 정면으로 다루고 있다. 나카지마의 연구는 기계학습 프로그램의 예측 및 추론 결과가 지니는 가장 본질적인 특성이 바로 ’비확정성(Non-definiteness)’에 있음을 날카롭게 지적한다. 전통적인 소프트웨어는 특정 입력 조건에서 예외 없이 동일한 이진법적 참과 거짓을 반환하지만, 딥러닝과 같은 기계학습 모델의 출력은 절대적인 정답이 아니라 ’확신도(Certainty degrees)를 동반한 확률(Probabilities)’의 형태로 산출된다. 자율주행 차량의 비전 인식 시스템에 동일한 정지 표지판 이미지를 입력하더라도, 시스템은 단호하게 “정지 표지판이다“라고 출력하는 대신, “정지 표지판일 확률 95%, 속도 제한 표지판일 확률 4%, 노이즈일 확률 1%“라는 다차원적인 확률 분포를 반환한다.</p>
<p>결과값이 이처럼 확률론적이라는 태생적 한계는 “입력에 대해 시스템이 완벽하게 올바른 단일 결과값을 도출하였는가?“라는 전통적인 절대적 오라클의 기준을 완전히 무의미하게 만든다. 학습 알고리즘 내부의 수백만 개 파라미터가 데이터에 의해 암묵적으로 조정되는 과정을 인간이 완전히 추적하거나 사전에 계산할 수 없기 때문에, 모델의 가중치가 업데이트될 때마다 동일한 입력에 대한 확률 분포 역시 미세하게 흔들리며 비결정적(Nondeterministic) 양상을 띤다.</p>
<p>절대적이고 확정적인 오라클을 세울 수 없다는 이 절망적인 현실 앞에서, 나카지마와 소프트웨어 공학계가 도출해 낸 치열한 타협의 산물이 바로 ’부분 오라클과 통계적 테스팅의 결합(Combination of partial oracles and statistical testing)’이다. 이는 개별적인 데이터 인스턴스 하나하나에 대한 추론 결과의 참/거짓을 따지는 미시적 접근을 포기하는 것을 의미한다. 그 대신, 무수히 많은 입력 데이터를 시스템에 통과시켜 도출된 거시적인 결과들의 ’통계적 분포(Statistical distribution)’가 사전에 정의된 특정 신뢰 구간이나 기대 속성(Expected properties)을 위반하지 않는지를 검증하는 새로운 차원의 부분 오라클을 구축하는 것이다.</p>
<p>이 통계적 부분 오라클의 개념은 앞서 논의한 메타모픽 테스팅과 융합하여 기계학습 품질 검증의 표준으로 자리 잡았다. 예를 들어, 나카지마의 다른 연구인 <em>Generating Biased Dataset for Metamorphic Testing of Machine Learning Programs</em>에서 볼 수 있듯이, 모델의 공정성(Fairness)이나 편향성(Bias)을 테스트하고자 할 때 연구자들은 데이터셋의 다양성(Dataset Diversity)을 의도적으로 조작한다. 기존 학습 데이터의 성별이나 인종 비율과 같은 민감한 속성 분포를 인위적으로 왜곡한 편향된 파생 데이터셋(Follow-up dataset)을 생성한 뒤 이를 기계학습 프로그램에 주입한다. 이 과정에서 완벽한 오라클은 존재하지 않지만, “데이터의 편향 정도가 변화하더라도, 모델의 특정 민감 계층에 대한 예측 정확도의 하락폭(통계적 오차율)은 5%의 임계치를 초과해서는 안 된다“라는 통계적 불변성을 메타모픽 관계로 설정한다. 통계적 부분 오라클은 수천 번의 추론 결과 집합을 분석하여 이 임계치 위반 여부를 감시한다.</p>
<p>또한, 자율주행과 같은 지능형 물리 시스템(Cyber-Physical Systems)이나 의료 영상 분석(Medical Imaging) 알고리즘에서도 통계적 부분 오라클은 필수적이다. 원본 의료 영상 이미지에 백색 잡음(White noise)을 2% 추가하거나 이미지를 5도 회전시켰을 때(Follow-up Test Case), 인간 의사조차도 그 노이즈가 암 병변 판독에 미칠 완벽한 수학적 정답을 제시할 수 없다. 오직 테스터는 “노이즈가 삽입된 1만 장의 파생 이미지 집합에 대한 모델의 전체 진단 오차율이, 원본 이미지 집합의 오차율 대비 99% 신뢰 수준 내에서 통계적으로 유의미한 차이를 보이지 않아야 한다“는 강건성(Robustness)의 범위를 부분 오라클로 지정할 뿐이다.</p>
<p>이 타협적 접근은 기계학습 시스템 내부에 도사리고 있는 치명적인 편향과 모델의 붕괴 현상을 막아내는 훌륭한 통계적 방어막 역할을 수행한다. 하지만 그 이면에는 결코 지울 수 없는 체념이 깃들어 있다. 통계적 부분 오라클은 개별 생명이 걸려 있는 단 한 번의 의료 진단 추론이나, 단 한 번의 자율주행 조향 결정이 “무조건 옳다“는 결정론적 정답지(Deterministic Ground Truth)를 보증해주지 못한다. 전체적인 확률의 테두리 안에서 시스템의 품질을 뭉뚱그려 담보하려는 이 현실적인 체념은, AI가 더욱 일상으로 깊숙이 침투할수록 그 한계를 드러내기 시작한다.</p>
<h2>5. 거대 언어 모델(LLM) 환경에서의 휴리스틱 부분 오라클과 극단적 비결정성</h2>
<p>기계학습의 확률적 불안정성을 통계적 부분 오라클로 억누르려던 소프트웨어 공학계의 노력은 생성형 AI, 특히 거대 언어 모델(Large Language Model, LLM)의 폭발적인 등장과 함께 전례 없는 혼란과 극단적인 비결정성(Nondeterminism)의 소용돌이에 직면하게 되었다. 자연어 처리(NLP)를 근간으로 하는 LLM은 이전 세대의 판별적(Discriminative) 기계학습 모델들과는 궤를 달리하는 본질적인 난제를 안고 있다. LLM은 수백억 개의 파라미터를 통해 다음 토큰(Token)의 확률 분포를 계산하여 텍스트를 생성하므로, 그 출력 형식이 고정되어 있지 않다. 동일한 사용자가 동일한 프롬프트(Prompt)를 입력하더라도, 디코딩 전략이나 온도의 설정에 따라 매번 문장 구조, 어휘의 선택, 심지어 논리적 전개 방식까지 달라지는 창발적이고 확률적인 텍스트 생성 특성을 지닌다.</p>
<p>입력과 출력이 모두 비정형화된 자연어의 형태를 띠며 무한한 자유도를 갖는 환경에서는, 앞서 기계학습에서 사용했던 통계적 임계치 오라클조차 무력화된다. 생성된 시(Poem)나 작성된 코드, 복잡한 비즈니스 문서 요약이 정확한지에 대한 절대적 기준은 존재하지 않으며, 이를 대규모 라벨링 데이터셋으로 구축하는 비용은 천문학적이다. 이에 따라 최신 AI 테스팅 연구 커뮤니티는 LLM을 검증하기 위해 기존의 메타모픽 테스팅을 언어의 의미론적 차원으로 끌어올리거나, 발견법적 단언문(Heuristic assertions)에 전적으로 의존하는 극단적인 형태의 부분 오라클 타협을 시도하고 있다.</p>
<p>LLM의 성능을 평가하기 위한 부분 오라클 연구는 최근 폭발적으로 증가하고 있다. Valerio Terragni 등의 연구진이 발표한 <em>Metamorphic Testing of Large Language Models for Natural Language Processing</em>는 이 분야의 가장 포괄적이고 야심 찬 시도 중 하나이다. 이들은 LLM이 수행하는 번역, 문서 요약, 질의응답, 감정 분석 등의 자연어 처리 작업을 검증하기 위해 기존 문헌을 샅샅이 뒤져 무려 191종의 메타모픽 관계(MR)를 도출해 냈으며, 3개의 대표적인 최신 LLM을 대상으로 56만 번 이상의 거대한 메타모픽 테스트를 수행했다.</p>
<p>이 과정에서 LLM을 구속하는 부분 오라클은 주로 문장의 ’의미적 불변성(Semantic invariance)’을 확인하는 메커니즘으로 구성된다. LLM의 극단적인 비결정성 속에서도 변하지 않아야 할 절대적 가치를 메타모픽 관계로 설정하는 것이다. 예를 들어 감정 분석 작업을 테스트할 때, 원본 문장(Source)의 의미를 훼손하지 않는 범위 내에서 동의어로 단어를 교체하는 어휘적 치환(Lexical substitutions)을 수행하거나, 능동태 문장을 수동태로 변형하는 구문론적 변형, 혹은 인종과 성별과 같은 무관한 주어 대명사를 변경하는 맥락적 재구성(Contextual reframing)을 통해 파생 문장(Follow-up)을 생성한다. 부분 오라클은 “입력 문장의 형태학적, 어휘적 표면이 어떻게 변형되든 간에, LLM이 반환하는 최종 감정 점수나 핵심 요약의 의미론적 방향성은 결코 뒤집혀서는 안 된다“는 논리적 단언을 수행한다. 만약 “이 식당은 훌륭하다“라는 문장을 “이 음식점은 뛰어나다“로 바꾸었을 때 LLM이 긍정에서 부정으로 판별을 바꾼다면, 오라클은 정답지를 몰라도 모델 내부의 취약성과 치명적인 논리적 환각(Hallucination), 또는 잠재적인 사회적 편향(Bias)이 존재함을 명백히 입증해 낸다.</p>
<p>자연어뿐만 아니라 LLM이 프로그래밍 코드를 직접 생성하는 ‘코드 생성 AI(Code LLM)’ 분야에서도 부분 오라클과 이를 확장한 속성 기반 테스팅(Property-based testing)이 광범위하게 적용되고 있다. LLM에게 특정 알고리즘의 구현 코드를 요구할 때, 생성된 코드의 로직이 개발자가 머릿속에 구상한 정답 코드와 바이트 단위로 100% 일치하는지를 검증할 수 있는 절대 오라클은 존재할 수 없다. 코드의 구현 방식은 무한하기 때문이다. 따라서 검증 시스템은 개발자가 API 문서나 주석에 자연어로 남긴 명세를 기반으로, 코드가 만족해야 할 발견법적 단언문(Heuristic assertions)이나 속성(Property)을 부분 오라클로 생성하여 코드를 동적으로 테스트한다. 예컨대 LLM이 작성한 배열 정렬 함수를 검증할 때, <span class="math math-inline">\vert 결과 배열 -  \vert = 0</span>과 같은 특정 입력에 대한 결정론적 정답을 대조하는 방식은 버려진다. 대신 “출력된 리스트의 모든 요소는 이전 요소보다 크거나 같아야 한다(오름차순 속성)”, 또는 “반환된 배열의 길이는 입력 배열의 길이와 동일해야 한다“와 같은 유연하지만 필수적인 속성을 오라클로 활용하여 무작위 데이터를 수만 번 쏟아붓는 방식의 타협이 이루어진다.</p>
<p>나아가 LLM 자체의 방대한 파라미터 지식을 의사 오라클(Pseudo-Oracle) 혹은 평가자(LLM-as-a-Judge)로 활용하여 다른 시스템이나 소규모 모델의 결과를 검증하려는 하이브리드 오라클 기법도 대두되고 있다. 명시적인 규칙 기반 오라클 작성이 어려운 복잡한 데이터 클러스터링이나 쌍별 제약(Pairwise constraints) 평가에서, LLM에게 “이 두 문서가 동일한 카테고리에 속하는가?“라는 발견법적 판단을 맡기는 것이다. 이는 오라클 구축 비용을 비약적으로 낮추는 가성비 높은 접근(Cost-effective)으로 평가받지만 , 근본적으로 불확실한 확률 모델을 사용해 또 다른 확률 시스템을 검증하는 모순된 계층 구조를 형성하게 된다. 이러한 타협은 언뜻 보기에 자동화와 유연성의 극치를 보여주는 듯하지만, 시스템이 심층적인 비즈니스 로직과 얽히는 순간 부분 오라클이 지닌 가장 치명적인 취약성, 즉 ’조용한 실패(Silent failures)’의 늪으로 개발팀을 끌고 들어가게 된다.</p>
<h2>6. 타협의 대가: 부분 오라클의 구조적 취약성과 ’조용한 실패’의 늪</h2>
<p>부분 오라클은 정답을 알 수 없는 무지의 상태에서 결함을 찾아내는 훌륭한 나침반 역할을 해왔으나, 2024년과 2025년의 최신 소프트웨어 테스팅 연구 동향은 거대 언어 모델과 복잡계 AI 환경에 이를 맹목적으로 적용할 때 발생하는 치명적인 한계와 질적 저하 요인들을 반복적으로 경고하고 있다. 오라클의 조건을 전체 명세가 아닌 일부 속성으로 완화한다는 타협은, 필연적으로 그 완화된 틈새로 심각한 결함이 빠져나갈 수 있는 구조적 맹점을 내포하게 된다.</p>
<p>가장 빈번하게 지적되는 첫 번째 한계는 휴리스틱 단언문과 검증 기준의 ’피상성(Shallowness)’이다. 부분 오라클은 시스템의 특정 논리적 관계나 표면적인 상태 속성만을 감시한다. LLM이 생성한 테스트 코드나 시스템 출력을 검증할 때, 부분 오라클은 생성된 코드가 문법적으로 유효한지(Syntactically valid), 변수가 초기화되었는지, 실행 중에 크래시(Crash)가 발생하지 않는지 등 얕은 수준의 조건만을 충족하면 테스트를 통과(Pass)한 것으로 판단한다. 심지어 최신 대규모 연구에 따르면, LLM이 생성한 자동 테스트 스위트가 인터페이스나 내용이 비어있는 빈 메서드(Empty methods)처럼 전혀 무의미한 로직을 통과시키면서도 외견상으로는 성공 수치를 기록하는 경우가 허다함이 밝혀졌다. LLM이 생성한 테스트의 실제 돌연변이 점수(Mutation score, 결함 적발 능력을 나타내는 지표)가 인간이 작성한 테스트(0.690)에 한참 못 미치는 0.546에 불과하거나 심지어 0에 수렴한다는 충격적인 결과는, 부분 오라클의 기준이 실제의 복잡한 결함 조건과 괴리되어 있음을 명백히 입증한다. 시스템이 그저 실행 가능(Executable)하다는 사실이, 시스템이 비즈니스 요구사항을 적절하게 수행하고 신뢰할 수 있음(Adequate and dependable)을 결코 의미하지 않는다.</p>
<p>이러한 피상성은 두 번째이자 가장 파괴적인 한계인 ’조용한 실패(Silent failures)’를 대규모로 유발한다. 완전한 오라클이 부재한 상태에서 부분 오라클에만 의존하게 되면, 오라클이 감시하지 않는 사각지대에서 치명적인 논리적 붕괴, 데이터 오염, 보안 취약성이 발생하더라도 테스트 시스템은 이를 전혀 인지하지 못한 채 정상 작동 중이라는 허위 신호(False negative)를 내보내게 된다. LLM 평가 환경에서 이러한 오라클 중심의 문제(Oracle-centered problem)는 치명적이다. 작업에 적절한 확정적 정확성 기준(Correctness criteria)이 결여되어 있기 때문에, AI 모델이 교묘한 환각(Hallucination)을 섞어 그럴싸한 오답을 산출하더라도 부분 오라클은 단언문의 문법이나 길이 제약 조건만을 검사한 뒤 테스트 성공을 선언한다. 이처럼 조용한 실패는 개발팀으로 하여금 AI 시스템의 신뢰성에 대해 부풀려진 환상(Inflate perceived effectiveness)을 갖게 만들어, 프로덕션 환경에서 돌이킬 수 없는 피해를 초래하는 원흉이 된다.</p>
<p>세 번째 한계는 부분 오라클 프레임워크 자체를 설계하고 유지보수하는 데 수반되는 막대한 인지적 비용, 즉 ’인간 오라클 비용(Human oracle cost)’과 발견법의 오류 가능성이다. 부분 오라클의 자동화된 테스트 실행 자체는 컴퓨터가 수행하여 정량적 비용(Quantitative cost)을 낮출 수 있지만, 어떤 메타모픽 관계(MR)가 유효한지, 어떤 속성을 단언해야 하는지를 최초에 설계하고 판단하는 것은 전적으로 고도의 도메인 지식을 갖춘 인간 전문가의 몫이다. 휴리스틱(Heuristic)은 인지적 부하를 줄여주는 어림짐작의 규칙(Rule of thumb)이지만, 복잡한 문제를 지나치게 단순화하여 최적의 해를 보장하지 못하는 근본적 제약을 안고 있다. 권위 있는 테스트 전문가 Kaner가 통찰했듯, 휴리스틱으로서의 부분 오라클은 우리의 결정을 돕는 유용한 도구이지만, 불완전한 명세와 편견으로 인해 때로는 우리를 “잘못된 결정(Wrong decision)“으로 인도한다.</p>
<p>실제로 AI를 평가자의 보조 도구로 활용할 때, 발견법적 한계로 인해 시스템이 무의미한 긍정 오류(False-positive heuristic violations)를 남발하는 현상이 보고되고 있다. 정상적인 비즈니스 로직의 변경이나 예외적인 엣지 케이스(Edge case)를 시스템의 중대한 결함으로 오판하여 끝없는 경고를 쏟아내는 것이다. 시스템이 실패(Fail)를 보고했을 때, 이것이 실제 소프트웨어의 버그인지, 아니면 인간이 설계한 부분 오라클의 메타모픽 관계 자체가 모순을 안고 있거나 AI의 휴리스틱 평가 기준이 과도하게 엄격하여 발생한 허위 신호(False positive)인지를 판별하는 작업은 엄청난 정성적 비용(Qualitative cost)의 낭비를 초래한다. 최신 연구 커뮤니티가 부분 오라클의 불완전한 정보를 바탕으로 테스트의 정확도를 높이고 오라클 리소스 낭비를 막는 기법을 찾는 데 사활을 걸고 있는 이유도 바로 인간 오라클의 병목 현상이 임계점에 달했기 때문이다. 불확실성을 관리하기 위해 도입한 타협안이 역설적으로 또 다른 불확실성의 근원이 되어버린 셈이다.</p>
<h2>7. 실전 예제: AI 시스템 검증에서 노출되는 부분 오라클의 한계와 딜레마</h2>
<p>이론적으로 논의된 부분 오라클의 타협이 실제 AI 소프트웨어 개발 현장에서 어떠한 치명적인 한계를 노출하며, 왜 종국에는 확정적이고 결정론적인 정답지를 강제할 수밖에 없는지를 명확히 보여주는 두 가지 실전 시나리오를 분석한다.</p>
<h3>7.1 실전 예제 1: 자율 에이전트의 접근 제어 및 정책 코드(Policy-as-Code) 검증과 사실의 왜곡</h3>
<p>최근 기업 환경에서는 방대한 사내 문서 시스템에 접근하여 지식 검색과 문서 요약을 자동으로 수행하는 LLM 기반 자율 에이전트 구축이 활발하다. 이 에이전트는 사용자가 자연어로 “재무팀의 1분기 영업 이익 보고서를 요약해 줘“라고 지시하면, 내부의 권한 관리 시스템(예: Rego와 같은 Policy-as-Code 엔진)을 쿼리하여 접근 가능한 문서만 추출한 뒤 이를 바탕으로 답변을 생성한다. 개발팀이 이 복잡한 시스템을 테스트하고자 할 때, 직원의 수많은 직급과 부서, 문서의 기밀 등급, 그리고 무한한 자연어 질의 형태가 결합된 모든 경우의 수에 대해 완벽한 참/거짓 매핑 테이블(오라클)을 구축하는 것은 물리적으로 불가능하다.</p>
<p>따라서 개발팀은 비용과 효율의 타협점인 부분 오라클을 도입한다. 이들이 설정한 메타모픽 관계와 보안 속성 단언문은 다음과 같다.</p>
<ul>
<li><strong>부분 오라클 제약 조건</strong>: “사용자의 자연어 질의 형태나 맥락이 어떻게 변하더라도, ‘기밀(Confidential)’ 태그가 지정된 문서는 해당 인가(Role)를 받지 않은 일반 사용자의 세션에서 반환된 LLM 답변의 참조 소스로 절대 노출되어서는 안 된다.”</li>
</ul>
<p>자동화된 수만 번의 테스트 결과, 봇은 기밀문서의 내용을 철저히 차단하며 정책 코드의 제약을 준수했다. 부분 오라클의 관점에서 이 보안 테스트는 완벽하게 통과(Pass)한 것이다.</p>
<p>하지만 실제 프로덕션 서비스가 배포된 후 치명적인 문제가 발생했다. 이 AI 에이전트가 모든 직원에게 접근이 허용된 “공개(Public) 영업 보고서“를 요약하는 과정에서 극심한 환각(Hallucination) 현상을 일으킨 것이다. LLM은 문서를 요약하다가 전혀 무관한 인터넷상의 경쟁사 재무 수치 데이터를 무작위로 섞어서 1분기 회사의 이익이 폭락했다는 허위 보고서를 생성하여 사내망에 배포해 버렸다.</p>
<p>여기서 부분 오라클의 얕은 타협이 지닌 본질적 한계가 뼈아프게 드러난다. 개발팀이 설계한 부분 오라클은 오직 접근 권한 시스템의 ‘구조적 정책 위반(Policy violation)’ 여부만을 감시하는 소극적 검증 메커니즘이었다. LLM이 생성한 문장의 구문은 완벽했고, 보안 태그가 없는 문서만을 열람했기에 부분 오라클의 그물망은 전혀 흔들리지 않았다. 그러나 기업 활동의 핵심 가치인 ’수치 데이터의 사실적 정확성(Factual truth)’은 완전히 붕괴되었다. 조용한 실패(Silent failure)가 발생한 것이다. 결국 이러한 미션 크리티컬한 정보 추출 시스템에서는 “제약 조건을 위반하지 않았다“는 부분 오라클의 소극적 보증만으로는 책임(Liability)을 질 수 없다. AI의 언어 생성 능력과 무관하게, 재무 수치나 법적 조항과 같은 핵심 엔티티(Entity) 데이터 추출에 있어서만큼은 LLM의 출력이 데이터베이스의 실제 원본 값이나 JSON 구조의 정해진 스키마와 바이트 단위로 정확히 일치하는지를 엄격하게 검증하는 ’결정론적 정답지(Deterministic Ground Truth)’의 강력한 개입과 강제 구조화가 필수적임이 극명히 증명된다.</p>
<h3>7.2 실전 예제 2: 다중 결함 프로그램 수리(Multi-Fault Program Repair) 환경에서의 역행과 오버피팅</h3>
<p>두 번째 시나리오는 인공지능이 코드를 분석하여 소프트웨어 버그를 자동으로 수정해 주는 자동 프로그램 수리(Automated Program Repair, APR) 에이전트의 검증 환경이다. 복잡한 시스템에 두 개의 상호 의존적인 결함인 <span class="math math-inline">f_1</span>과 <span class="math math-inline">f_2</span>가 동시에 존재한다고 가정해 보자 (<span class="math math-inline">f_1 \prec f_2</span> 의존성 관계). AI 수리 엔진이 코드를 분석한 후 <span class="math math-inline">f_1</span>을 고치는 데 성공적인 패치(Patch) 코드를 생성하여 적용했다. 개발팀은 AI가 제안한 이 수리 코드가 유효한지 검증하기 위해 기존에 구축되어 있던 유닛 테스트 스위트(Test suite) 집합을 부분 오라클로 활용한다. 시스템의 전체 로직을 증명할 방법이 없으니 “기존의 실패하던 테스트 코드를 통과시키면 결함이 고쳐진 것으로 간주한다“는 경험적 타협을 한 것이다.</p>
<p>AI가 <span class="math math-inline">f_1</span>을 완벽하게 수정했음에도 불구하고, 시스템 내부에 여전히 의존성 높은 다른 결함 <span class="math math-inline">f_2</span>가 남아있기 때문에 전체 테스트 스위트를 실행하면 여전히 실패(Fail) 결과가 나타난다. 이 결과를 받아 든 휴리스틱 기반의 AI 에이전트와 부분 오라클 시스템은 중대한 오판을 내린다. 테스트 스위트라는 얕은 오라클은 시스템의 인과적 종속성을 파악할 능력이 없기 때문에, 지금 발생한 실패가 남아있는 <span class="math math-inline">f_2</span> 때문이라는 사실을 구별하지 못한다. 결국 시스템은 방금 AI가 훌륭하게 작성해 낸 <span class="math math-inline">f_1</span> 패치 코드가 “결함을 해결하기에 불충분하거나 새로운 역행(Regression)을 일으켰다“고 착각하고, 멀쩡한 패치를 롤백(Rollback)시켜 버리거나 막대한 컴퓨팅 비용을 들여 처음부터 잘못된 방향으로 코드 재합성(Re-synthesis) 과정을 다시 시작하도록 지시한다.</p>
<p>전통적인 버그 수리 과정에서도 테스트 스위트를 부분 오라클로 사용할 때 발생하는 과적합(Overfitting) 문제는 고질적이었다. 패치된 코드가 우연히 준비된 테스트 케이스 몇 개의 특정 입력값만 만족시키고 실제 비즈니스 로직은 완전히 망가뜨리는 현상이다. 그런데 다중 결함이 존재하는 AI 환경에서 이 과적합과 오판은 더욱 교활하게 시스템을 갉아먹는다. 실패하는 테스트 결과라는 단편적인 피드백만으로는 AI가 올바른 수리 방향을 잃게 되기 때문이다. AI 기반 에이전트가 코드를 올바르게 생성하고 검증 과정을 조율(Orchestrate)하기 위해서는, “테스트가 통과하는가?“라는 발견법적 부분 오라클의 신호를 넘어서야 한다. 결함 상호 작용 그래프(<span class="math math-inline">G_I</span>)를 분석하고 특정 논리 분기가 정확히 어떤 구조적 상태를 가져야 하는지를 수학적으로 명시하는 정형적 제약 조건(Formal constraints), 그리고 인간 개발자의 명확한 의도가 반영된 확정적인 검증 기준 체계가 마련되지 않는 한, AI 시스템은 부분 오라클이 만들어낸 안개 속에서 영원히 헤매게 된다.</p>
<h2>8. 한계를 넘어선 체념, 그리고 확정적 패러다임으로의 회귀 본능</h2>
<p>소프트웨어 공학의 역사 속에서 부분 오라클(Partial Oracle)의 개념적 도출과 메타모픽 테스팅 프레임워크로의 진화는, 테스트 불가능한 기계학습 모델의 출현과 극단적 비결정성을 안고 태어난 거대 언어 모델의 확산을 가능케 한 위대한 타협의 산물이었다. 오라클 비용의 절벽 앞에서, 정답을 생성하는 계산의 문제를 관계의 무결성을 검증하는 비교의 문제로 치환한 것은 AI 테스팅을 실험실에서 현실의 산업 현장으로 끌어올린 혁명적 발상이었다.</p>
<p>그러나 비즈니스 시스템의 복잡도가 극에 달하고 인공지능이 인간의 생명, 막대한 금융 자본, 그리고 기업의 핵심 보안 정책과 직접적으로 교차하는 현대의 개발 환경에서, 부분 오라클이 제공하는 소극적 방어막은 점차 그 한계를 명백히 드러내고 있다. “대체로 맞을 것이다”, “특정한 제약 조건 내에서는 안전하다”, 혹은 “통계적 오차 범위 내에 있다“라는 확률적이고 부분적인 지식의 보증만으로는, 시스템이 마주해야 할 치명적인 책임(Liability)의 공백을 메울 수 없다. 개발자와 아키텍트들은 AI가 생성한 결과물을 테스트할 때 부분 오라클이 던져주는 피상적인 패스(Pass) 신호의 이면에 똬리를 틀고 있는 조용한 실패(Silent failure)와 기만적인 환각에 짙은 회의감과 피로감을 느끼기 시작했다.</p>
<p>타협은 영원할 수 없다. 오라클의 역할이 단순히 시스템이 무너지지 않았음을 방어적으로 입증하는 수준을 넘어, AI의 창발적 출력이 비즈니스 요구사항과 로직의 핵심 가치를 100% 충족시켰음을 증명하는 적극적인 통제 수단으로 환원되어야 한다는 요구가 거세지고 있다. 아무리 메타모픽 관계의 그물망을 촘촘히 짜고 통계적 검증의 표본을 늘린다 하더라도, 비즈니스 소프트웨어가 최종적으로 추구하는 본질적 가치는 ’확률(Probability)이나 모의(Heuristic)’가 아닌, 오류 없는 ’확정(Certainty)’에 뿌리를 두고 있기 때문이다.</p>
<p>이러한 깊은 구조적 한계와 성찰은 소프트웨어 엔지니어링 패러다임을 필연적으로 다음 진화 단계로 강하게 밀어 올린다. 비결정적인 인공지능의 거센 파도 위에서 항해하기 위해 잠시 불완전한 나침반인 부분 오라클에 의존했던 우리는, 이제 기계학습의 불확실성을 통제 가능한 컨테이너 안에 가두고 그 위에 견고하고 절대적인 비즈니스 로직의 닻을 내리게 해 줄 엄격한 북극성, 즉 ’결정론적 정답지(Deterministic Ground Truth)’의 복원을 절박하게 갈구할 수밖에 없게 된 것이다. 이어지는 논의에서는 확률의 시대에 왜 우리가 다시 결정론적 원칙을 소환해야만 하는지, 그 치열한 설계의 철학과 방법론적 필연성에 대해 깊이 파고들게 될 것이다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>The Oracle Problem in Software Testing: A Survey - EECS 481, http://www0.cs.ucl.ac.uk/staff/m.harman/tse-oracle.pdf</li>
<li>The Oracle Problem in Software Testing: A Survey - Earl Barr, https://earlbarr.com/publications/testoracles.pdf</li>
<li>Machine Learning Quality Management Guideline 3rd Edition - AIST, https://www.digiarc.aist.go.jp/en/publication/aiqm/aiqm-guideline-en-3.1.1.0077-e05-signed.pdf</li>
<li>Techniques for testing scientific programs without an oracle | Request PDF - ResearchGate, https://www.researchgate.net/publication/259310550_Techniques_for_testing_scientific_programs_without_an_oracle</li>
<li>Partial oracle in the testing environment | Download Scientific Diagram - ResearchGate, https://www.researchgate.net/figure/Partial-oracle-in-the-testing-environment_fig1_228660231</li>
<li>Test oracle - Wikipedia, https://en.wikipedia.org/wiki/Test_oracle</li>
<li>Oracle Assessment, Improvement and Placement Gunel Jahangirova - UCL Discovery, https://discovery.ucl.ac.uk/10072699/1/Jahangirova_10072699_Thesis.pdf</li>
<li>AI or LLM ASSISTED SOFTWARE TESTING: A MAPPING STUDY - LUTPub, https://lutpub.lut.fi/bitstream/handle/10024/171117/Mastersthesis_AlAmin_Shamim.pdf?sequence=1</li>
<li>A Survey on Test Oracles - Open Journal Systems, https://revista.univem.edu.br/jadi/article/download/1034/393/0</li>
<li>On Testing Non-testable Programs - Elaine J. Weyuker, https://homes.cs.washington.edu/~rjust/courses/CSE503/2021_02_12-reading1.pdf</li>
<li>Automatic Detection of Defects in Applications without Test Oracles - Columbia Academic Commons, https://academiccommons.columbia.edu/doi/10.7916/D8GM8G46/download</li>
<li>Establishing the Causal Foundations of Metamorphic Testing: A Novel Application of Causal Inference for Testing Computational Modelling Software, https://etheses.whiterose.ac.uk/id/eprint/34458/1/PhD_thesis_andrew_clark_minor_corrections.pdf</li>
<li>Machine Learning Quality Management Guideline - 2nd English Edition, https://www.digiarc.aist.go.jp/en/publication/aiqm/AIQM-Guideline-en-2.1.0.0057-e25.pdf</li>
<li>Meta-Fair: AI-Assisted Fairness Testing of Large Language Models - arXiv, https://arxiv.org/html/2507.02533v2</li>
<li>Who Tests the Testers? Assessing the Effectiveness and Trustworthiness of Deep Learning Model Testing Techniques - PolyPublie, https://publications.polymtl.ca/59454/1/2024_FlorianTambon.pdf</li>
<li>Evaluation of Metamorphic Testing for Edge Detection in MRI Brain Diagnostics - MDPI, https://www.mdpi.com/2076-3417/12/17/8684</li>
<li>(PDF) The Oracle Problem in Software Testing: A Survey - ResearchGate, https://www.researchgate.net/publication/276255185_The_Oracle_Problem_in_Software_Testing_A_Survey</li>
<li>Variable Discovery with Large Language Models for Metamorphic Testing of Scientific Software - The International Conference on Computational Science, https://www.iccs-meeting.org/archive/iccs2023/papers/140730328.pdf</li>
<li>機械学習品質マネジメントガイドライン - デジタルアーキテクチャ研究センター, https://www.digiarc.aist.go.jp/publication/aiqm/AIQM-Guideline-2.1.0.pdf</li>
<li>Bias Testing and Mitigation in Black Box LLMs using Metamorphic Relations - arXiv, https://arxiv.org/html/2512.00556v1</li>
<li>Metamorphic Testing of Large Language Models for Natural Language Processing - Valerio Terragni, https://valerio-terragni.github.io/assets/pdf/cho-icsme-2025.pdf</li>
<li>(PDF) Metamorphic Testing of Large Language Models for Natural Language Processing, https://www.researchgate.net/publication/394085166_Metamorphic_Testing_of_Large_Language_Models_for_Natural_Language_Processing</li>
<li>Call Me Maybe: Using NLP to Automatically Generate Unit Test Cases Respecting Temporal Constraints | Request PDF - ResearchGate, https://www.researchgate.net/publication/366906900_Call_Me_Maybe_Using_NLP_to_Automatically_Generate_Unit_Test_Cases_Respecting_Temporal_Constraints</li>
<li>Do LLMs generate test oracles that capture the actual or the expected program behaviour?, https://arxiv.org/html/2410.21136v1</li>
<li>Large Language Models for Unit Test Generation: Achievements, Challenges, and Opportunities - arXiv, https://arxiv.org/html/2511.21382v2</li>
<li>Large Language Models (LLMs) Enable Few-Shot Clustering - NEC Corporation, https://www.nec.com/en/global/techrep/journal/g23/n02/230216.html</li>
<li>Large Language Models Enable Few-Shot Clustering - CMU School of Computer Science, https://www.cs.cmu.edu/~vijayv/few_shot_clustering.pdf</li>
<li>Large Language Models Enable Few-Shot Clustering - ACL Anthology, https://aclanthology.org/2024.tacl-1.18.pdf</li>
<li>Entity Examples for Explainable Query Target Type Identification with LLMs ⋆, https://xai.w.uib.no/files/2024/09/IDEAL_2024-Garigliotti.pdf</li>
<li>Policy-as-Code: Automating Policy Enforcement - Emergent Mind, https://www.emergentmind.com/topics/policy-as-code</li>
<li>Why AI Tools Are Not Ready to Replace Human Heuristic Evaluations — Yet - Medium, https://medium.com/uxr-microsoft/why-ai-tools-are-not-ready-to-replace-human-heuristic-evaluations-yet-e56a143c0967</li>
<li>How do I Test AI? - Hiccupps, https://qahiccupps.blogspot.com/2025/01/how-do-i-test-ai.html</li>
<li>Why are heuristics not improving AI performance? - UMU, https://m.umu.com/ask/a11122301573854484207</li>
<li>Heuristics &amp; Oracles - Association for Software Testing, https://associationforsoftwaretesting.org/2016/04/12/heuristics-oracles/</li>
<li>The Oracle Problem and the Teaching of Software Testing - Cem Kaner, https://kaner.com/?p=190</li>
<li>Harden and Catch for Just-in-Time Assured LLM-Based Software Testing: Open Research Challenges - arXiv, https://arxiv.org/html/2504.16472v1</li>
<li>Foundations and Challenges of Multi-Fault Program Repair - GitHub Pages, https://omarbat92.github.io/omar-website/Confpapers/MFAPR.pdf</li>
<li>ISO/IEC/ IEEE CD 29119-1, https://ieeexplore.ieee.org/ielD/9185099/9185100/09185101.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>