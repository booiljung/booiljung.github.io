<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2.4.2 '정답'의 정의 모호성: 창의적 생성물에 대한 참/거짓 판별의 어려움</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2.4.2 '정답'의 정의 모호성: 창의적 생성물에 대한 참/거짓 판별의 어려움</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 2. 소프트웨어 테스트에서의 오라클(Oracle) 개념과 AI 시대의 역할</a> / <a href="index.html">2.4 AI 및 LLM(거대 언어 모델) 도입으로 인한 테스트 패러다임의 붕괴</a> / <span>2.4.2 '정답'의 정의 모호성: 창의적 생성물에 대한 참/거짓 판별의 어려움</span></nav>
                </div>
            </header>
            <article>
                <h1>2.4.2 ’정답’의 정의 모호성: 창의적 생성물에 대한 참/거짓 판별의 어려움</h1>
<p>전통적인 소프트웨어 엔지니어링의 패러다임은 결정론적(Deterministic) 세계관에 깊은 뿌리를 두고 있다. 고전적인 프로그래밍 환경에서 함수 <span class="math math-inline">f(x)</span>는 입력 <span class="math math-inline">x</span>에 대해 언제나 예측 가능하고 유일한 결과값 <span class="math math-inline">y</span>를 반환해야 했다. 이 세계에서 테스트 오라클(Test Oracle)은 시스템의 동작이 명세(Specification)와 일치하는지를 판별하는 절대적인 심판관이었으며, 그 판정은 이항 대립적인 논리, 즉 참(True) 혹은 거짓(False)으로 명확히 귀결되었다. 예컨대, 은행 시스템의 잔액 계산이나 화물 추적 시스템의 좌표 갱신과 같은 로직은 <span class="math math-inline">1+1=2</span>와 같은 수학적 엄밀성을 요구하며, 이에 대한 오라클은 명확하고 논쟁의 여지가 없다.</p>
<p>그러나 거대 언어 모델(Large Language Models, LLM)을 위시한 생성형 AI(Generative AI)의 등장은 이러한 이분법적 검증 체계를 근본부터 뒤흔들고 있다. 생성형 AI가 산출하는 결과물은 본질적으로 확률적(Probabilistic)이며, ’창의성’이라는 미명 아래 동일한 입력에 대해서도 무수히 많은 유효한 변형(Variation)을 허용한다. 우리는 이제 “정답이란 무엇인가?“라는 인식론적 질문을 던져야 하는 상황에 직면했다. 자연어 생성(NLG), 기계 번역, 요약, 그리고 창의적 코드 생성과 같은 개방형 과제(Open-ended Tasks)에서 전통적인 오라클은 더 이상 유효하지 않다.</p>
<p>이 장에서는 생성형 AI를 활용한 소프트웨어 개발 및 서비스 구축 과정에서 엔지니어와 연구자들이 직면하게 되는 가장 본질적인 난제인 ’정답(Ground Truth)의 모호성’을 심층적으로 분석한다. 특히, 입력과 출력의 관계가 일대일(Bijective)이 아닌 일대다(One-to-Many) 매핑으로 변화함에 따라 발생하는 평가의 복잡성을 논하고, 인간 평가자들 사이의 불일치(Inter-rater Disagreement)가 단순한 노이즈가 아니라 데이터의 본질적 속성임을 밝힌다. 나아가, 불확실한 정답지 위에서 소프트웨어의 신뢰성을 확보하기 위해 학계와 산업계가 시도하고 있는 확률적·맥락적 검증 방법론과 실전 예제를 상세히 기술한다.</p>
<h2>1.  결정론적 오라클의 붕괴와 ‘일대다(One-to-Many)’ 매핑의 본질</h2>
<p>소프트웨어 테스팅 이론에서 오라클 문제는 “주어진 입력에 대해 시스템의 출력이 올바른지 판단하기 위한 메커니즘을 확보하는 것의 어려움“으로 정의된다. 생성형 AI 이전의 시대에는 오라클 문제가 주로 기술적 명세의 부재나 계산 복잡도에서 기인했다면, 생성형 AI 시대의 오라클 문제는 ’정답의 유일성 상실’이라는 존재론적 특성에서 기인한다.</p>
<h3>1.1 창의적 오라클의 부재 (The Absence of Creative Oracle)</h3>
<p>기존 머신러닝의 분류(Classification) 문제에서는 정답 레이블(Label)이 이산적이고 고정되어 있었다. 고양이 사진은 ’고양이’로 분류되어야 하며, 이는 논란의 여지가 거의 없다. 그러나 생성형 모델의 과제는 다르다. “이 파이썬 코드를 더 효율적으로 리팩토링하라” 혹은 “이 20페이지짜리 법률 문서를 핵심만 요약하라“는 프롬프트를 생각해보자. 리팩토링된 코드는 변수명이 다를 수도, 루프 구조가 리스트 컴프리헨션으로 바뀔 수도 있으며, 심지어 알고리즘 자체가 변경될 수도 있다. 요약문은 작성자의 관점에 따라 법적 리스크를 강조할 수도, 계약 기간을 강조할 수도 있다. 이들 모두가 문맥에 따라 ’정답’이 될 수 있다.</p>
<p>학계에서는 이를 <strong>일대다(One-to-Many) 매핑 문제</strong>라고 정의한다. 단일한 입력(Source) <span class="math math-inline">x</span>에 대해, 의미적으로 타당한 출력(Target) <span class="math math-inline">Y</span>의 집합은 <span class="math math-inline">{y_1, y_2,..., y_n}</span>과 같이 다수로 존재하며, 이 <span class="math math-inline">n</span>은 이론적으로 무한할 수 있다. 텍스트-투-비디오(Text-to-Video) 생성이나 대화형 에이전트와 같은 고차원적인 과제에서 이러한 현상은 더욱 두드러진다. 예를 들어, 동일한 발화에 대해서도 화자의 감정 상태나 의도에 따라 수십 가지의 다른 입모양이나 제스처가 생성될 수 있으며, 이들 모두가 ‘자연스러운’ 결과로 받아들여진다.</p>
<p>이러한 모호성은 생성형 AI의 평가를 극도로 어렵게 만든다. 전통적인 자동화 지표인 BLEU(Bilingual Evaluation Understudy)나 ROUGE(Recall-Oriented Understudy for Gisting Evaluation)는 n-gram(연속된 단어열)의 중복도를 기반으로 기계 생성 텍스트와 참조(Reference) 텍스트를 비교한다. 그러나 이 지표들은 의미론적 동등성(Semantic Equivalence)을 포착하지 못한다. 모델이 참조 문장과 단어는 다르지만 완벽하게 동일한 의미를 가진 문장을 생성했을 때, 기계적 지표는 이를 낮은 점수로 평가하여 ’오답’으로 처리할 위험이 크다. 반대로, 문법적으로만 그럴듯하지만 사실 관계가 틀린 환각(Hallucination)을 생성했을 때, n-gram이 우연히 겹친다면 높은 점수를 부여하는 오류를 범하기도 한다.</p>
<p>따라서 생성형 AI 개발에서 ’오라클 문제’는 단순히 테스트 케이스를 작성하는 기술적 문제를 넘어선다. 그것은 “우리가 구축하려는 시스템의 ’올바름’을 어떻게 정의할 것인가?“라는 인식론적 질문으로 확장된다. 이는 블록체인 분야에서 외부 데이터의 신뢰성을 검증해야 하는 ’블록체인 오라클 문제’와 유사하게, 신뢰할 수 없는 외부 세계(AI의 확률적 출력)와 신뢰가 필요한 내부 시스템(소프트웨어 요구사항) 사이의 간극을 메우는 도전이다.</p>
<h2>2.  정답(Ground Truth)의 실종: 데이터셋의 본질적 한계와 전문가의 가변성</h2>
<p>’정답’을 정의하기 어려운 근본적인 이유는 AI 모델을 학습시키고 평가하는 데 사용되는 데이터셋 자체의 불완전성에 기인한다. 우리가 흔히 ’골드 스탠다드(Gold Standard)’라고 부르는 데이터는 절대적인 진리가 아니다. 그것은 특정 시점에, 특정 사회적·문화적 배경을 가진 어노테이터(Annotator)들에 의해 수집된 주관적인 판단의 집합일 뿐이다. 연구자들은 이를 “진흙투성이의 정답(Muddy Ground Truth)“이라고 부르기도 하는데, 이는 데이터 라벨링 과정이 본질적으로 해석적이고 가변적임을 시사한다.</p>
<h3>2.1 전문가 판단의 가변성(Expert Variability)과 평가의 주관성</h3>
<p>의료 진단, 법률 판례 분석, 또는 복잡한 코드 리뷰와 같이 고도의 전문성이 요구되는 영역에서조차 전문가 간의 일치도(Inter-rater Agreement)는 완벽하지 않다. Kili Technology의 연구 및 관련 문헌에 따르면, 복잡한 과제에서 전문가들의 판단은 AI와 인간 사이의 차이보다 전문가들끼리의 차이가 더 클 때가 빈번하다.</p>
<p>예를 들어, 방사선과 전문의들이 흉부 X-ray 사진을 판독하여 병변을 기록한 리포트를 생각해보자. 동일한 사진을 두고도 어떤 의사는 “경미한 침윤“이라고 기술하고, 다른 의사는 “정상 범위 내의 음영“이라고 판단할 수 있다. 자연어 생성 모델이 이 이미지를 보고 리포트를 생성했을 때, 누구의 소견을 정답으로 삼아야 하는가? 특정 전문가 한 명의 판단을 유일한 정답으로 설정하고 모델을 평가하는 것은, 모델이 그 특정 전문가의 개인적 편향(Bias)을 얼마나 잘 모방했는지를 측정하는 것에 불과할 수 있다. 이러한 현상은 법률 분야에서도 마찬가지로 나타나는데, 계약서의 위험 조항을 해석하는 데 있어 변호사마다, 그리고 관할권마다 판단이 달라질 수 있기 때문에 단일한 ’정답 요약’이나 ’정답 해석’을 정의하는 것은 불가능에 가깝다.</p>
<h3>2.2 라벨링 불일치: 노이즈인가, 신호인가?</h3>
<p>자연어 처리(NLP) 분야, 특히 지도 학습(Supervised Learning) 패러다임에서는 전통적으로 어노테이터 간의 불일치(Disagreement)를 제거해야 할 ’노이즈’로 취급해 왔다. 다수결(Majority Voting)이나 판정(Adjudication) 과정을 통해 소수의견을 배제하고 단일한 라벨로 통합하려는 시도가 표준이었다. 그러나 최근의 연구 흐름, 특히 **관점주의(Perspectivism)**를 지향하는 연구자들은 이러한 불일치가 단순한 오류가 아니라 과제의 내재적 모호성이나 주관성을 반영하는 중요한 ’신호(Signal)’라고 주장한다.</p>
<p>“Why We Need New Evaluation Metrics for NLG”  논문과 후속 연구들은 인간 평가자들 사이의 평가 불일치가 모델 평가의 신뢰성을 저해하는 요소가 아니라, 언어의 본질적인 다의성(Ambiguity)을 드러내는 지표임을 강조한다. 예를 들어, 혐오 표현 탐지 데이터셋에서 특정 문장이 혐오 발언인지 아닌지에 대해 평가자들의 의견이 6:4로 갈린다면, 그 문장은 본질적으로 모호하거나 사회적 합의가 이루어지지 않은 표현일 가능성이 높다. 이를 강제로 ‘혐오 발언(1)’ 또는 ’비혐오 발언(0)’으로 레이블링하는 것은 데이터에 담긴 풍부한 맥락 정보를 삭제하는 행위다.</p>
<p>더 나아가, 어노테이터 한 명 내부에서도 판단의 불일치(Intra-rater Disagreement)가 발생할 수 있다. 동일한 사람이 같은 문장을 아침에 평가했을 때와 저녁에 평가했을 때 다른 점수를 줄 수 있는데, 이는 인간의 인지적 상태나 피로도에 따른 자연스러운 현상이다. 최근의 연구는 이러한 ’평가 불확정성(Rating Indeterminacy)’을 모델링하고, 강제 선택(Forced-choice) 방식 대신 가능한 모든 해석을 허용하는 다중 라벨링(Response Set) 방식을 제안하기도 한다.</p>
<h3>2.3 요약 및 번역에서의 모호성 사례</h3>
<p>요약(Summarization)과 번역(Translation) 과제는 정답의 모호성이 가장 극명하게 드러나는 분야다. “이 문서를 요약하라“는 지시는 “어떤 관점에서, 누구를 위해, 얼마나 길게 요약할 것인가?“라는 수많은 잠재적 제약 조건을 내포하고 있다. AWS의 생성형 AI 품질 가이드라인은 이러한 과제에서 단일 정답을 정의하는 것이 불가능함을 인정하고, ‘참조 텍스트(Reference text)’ 방식 대신 ‘기대되는 사실(Expected facts)’ 목록을 검증하는 방식으로의 전환을 제안한다. 즉, 문장의 형태가 일치하는지가 아니라, 필수적인 정보(Entity, Key concept)가 포함되었는지를 확인하는 것이 더 타당하다는 것이다.</p>
<p>번역의 경우에도 <code>source</code> 문장의 뉘앙스, 문체, 문화적 배경에 따라 적절한 <code>target</code> 문장은 여러 개일 수 있다. Novikova 등의 연구는 이러한 자연어 생성 과제에서 자동화된 지표가 인간의 판단과 낮은 상관관계를 보임을 통계적으로 입증하였다. 이는 우리가 ’정답’이라고 믿는 참조 문장 하나에 의존하여 모델을 평가하는 것이 얼마나 위험한지를 경고한다.</p>
<h2>3.  결정론적 정답지를 제공하는 오라클과 실전 예제</h2>
<p>그렇다면 정답이 모호하고 전문가조차 의견이 갈리는 생성형 AI 개발 환경에서, 엔지니어는 어떻게 소프트웨어의 품질을 보증할 수 있는가? 모든 것이 확률적이라고 해서 품질 관리(QA)를 포기할 수는 없다. 여기서 우리는 확률적 혼돈 속에 **결정론적 섬(Deterministic Islands)**을 구축하는 전략, 즉 ‘부분적 오라클(Partial Oracle)’, ‘대리 오라클(Proxy Oracle)’, 그리고 ’실행 기반 오라클(Execution-based Oracle)’을 적극적으로 활용해야 한다. 다음은 실제 개발 현장에서 적용 가능한 구체적인 오라클 전략들이다.</p>
<h3>3.1  코드 생성에서의 실행 기반 오라클 (Execution-based Oracle)</h3>
<p>코드를 생성하는 AI(예: GitHub Copilot, Amazon CodeWhisperer)의 경우, 생성된 코드의 텍스트(Syntax) 자체는 매번 달라도 그 코드가 수행해야 할 기능적 결과(Semantics)는 명확하다는 특성이 있다. 이를 이용해 우리는 텍스트 일치 여부가 아닌 <strong>실행 결과</strong>를 오라클로 사용할 수 있다.</p>
<ul>
<li><strong>실전 예제:</strong> AI에게 “주어진 리스트에서 중복을 제거하고 정렬하는 파이썬 함수를 작성하라“고 요청했다고 가정하자.</li>
<li><em>AI 출력 A:</em> <code>set()</code>을 이용한 후 <code>sorted()</code> 함수 호출.</li>
<li><em>AI 출력 B:</em> <code>for</code> 루프를 돌며 직접 중복 확인 후 버블 정렬 구현.</li>
<li><em>AI 출력 C:</em> <code>pandas</code> 라이브러리를 임포트하여 처리.</li>
<li>위 세 코드는 텍스트 수준(BLEU 점수)에서는 완전히 다르지만, <code>input=</code>을 입력했을 때 모두 ``을 반환해야 한다는 점에서는 동일하다.</li>
<li><strong>테스트 전략:</strong> 생성된 코드를 격리된 샌드박스(Sandbox) 환경에서 실제로 실행하고, 사전 정의된 단위 테스트(Unit Tests) 집합을 통과하는지 확인한다. 이때 단위 테스트의 통과 여부가 바로 결정론적 오라클이 된다. 최근 연구들은 이러한 실행 기반 검증이 정적 텍스트 매칭보다 오류 검출에 훨씬 효과적임을 증명하고 있다. 특히 ‘비일관성(Incoherence)’ 지표를 사용하여, 여러 번 샘플링된 코드들이 서로 다른 실행 결과를 내놓을 경우 해당 출력의 신뢰도를 낮게 평가하는 기법도 제안되고 있다.</li>
</ul>
<h3>3.2  메타모픽 테스팅 (Metamorphic Testing)</h3>
<p>정답을 미리 알 수 없는 경우(예: 복잡한 시뮬레이션, 검색 엔진 랭킹, 새로운 데이터에 대한 예측), 입력값의 변화에 따른 출력값의 **논리적 관계(Metamorphic Relation)**를 오라클로 삼을 수 있다. 이는 ’정답이 무엇인가’를 묻는 대신 ’정답이 가져야 할 속성은 무엇인가’를 묻는 방식이다.</p>
<ul>
<li><strong>실전 예제:</strong> 텍스트-투-SQL(Text-to-SQL) 변환 모델을 테스트한다고 가정하자. “매출이 100만 원 이상인 매장을 보여줘“라는 자연어 쿼리에 대한 정확한 SQL 쿼리는 스키마에 따라 다양할 수 있다.</li>
<li><strong>메타모픽 관계 설정:</strong></li>
</ul>
<ol>
<li>원본 입력: “매출이 100만 원 이상인 매장을 보여줘” <span class="math math-inline">\rightarrow</span> 결과 집합 <span class="math math-inline">A</span></li>
<li>변형 입력: “매출이 100만 원 이상인 <strong>서울 지역</strong> 매장을 보여줘” <span class="math math-inline">\rightarrow</span> 결과 집합 <span class="math math-inline">B</span></li>
</ol>
<ul>
<li><strong>오라클 로직:</strong> 우리는 <span class="math math-inline">A</span>와 <span class="math math-inline">B</span>의 정확한 값은 알 수 없더라도, 논리적으로 <strong><span class="math math-inline">B</span>는 반드시 <span class="math math-inline">A</span>의 부분집합(<span class="math math-inline">B \subseteq A</span>)이어야 한다</strong>는 사실을 안다.</li>
<li><strong>테스트 전략:</strong> 만약 모델이 생성한 SQL을 실행했을 때 <span class="math math-inline">B</span>에 <span class="math math-inline">A</span>에 없는 매장이 포함되어 있다면, 이는 명백한 오류(Bug)이다. 이처럼 입력의 변화에 따른 출력의 관계 위배 여부가 곧 버그를 탐지하는 강력한 오라클이 된다.</li>
</ul>
<h3>3.3  LLM-as-a-Judge: AI를 심판관으로 활용</h3>
<p>최근에는 성능이 검증된 상위 모델(예: GPT-4, Claude 3.5 Sonnet)을 사용하여 작은 모델이나 특정 태스크 수행 모델의 출력을 평가하는 <strong>LLM-as-a-Judge</strong> 방식이 표준으로 자리 잡고 있다.</p>
<ul>
<li><strong>작동 원리:</strong> 인간 평가자가 수행하던 평가 기준(유창성, 정확성, 무해성, 문맥 유지 등)을 프롬프트로 상세히 기술하여 심판 모델(Judge Model)에게 입력한다. 심판 모델은 생성된 텍스트와 원본 소스, 참조 정답(있는 경우)을 분석하고 점수와 그 근거(Reasoning)를 제시한다. G-EVAL 프레임워크와 같은 연구는 LLM 심판관이 인간 평가와 높은 상관관계를 보일 수 있음을 시사한다.</li>
<li><strong>한계 및 보완:</strong> 이 방식 역시 심판 모델의 편향(Bias)이나 환각에서 자유롭지 않다. 특히 심판 모델은 자신의 훈련 데이터와 유사한 패턴을 선호하거나, 긴 답변을 무조건 더 좋게 평가하는 ’길이 편향(Verbosity Bias)’을 보일 수 있다. 따라서 심판 모델의 평가 결과가 인간 평가와 얼마나 일치(Correlation)하는지 메타 평가(Meta-evaluation)를 수행하거나, 다수의 LLM 배심원(Panel of LLMs)을 구성하여 다수결 투표를 진행하는 앙상블 기법이 권장된다.</li>
</ul>
<h3>3.4  Text-to-SQL에서의 실행 정확도(Execution Accuracy)와 모호성 해결</h3>
<p>오라클의 최신 Text-to-SQL 솔루션 사례는 모호성을 기술적으로 해결하는 좋은 본보기다. 자연어 질의를 SQL로 변환할 때, “지난달 실적 보여줘“라는 모호한 요청은 ’실적’이 매출인지 이익인지, ’지난달’의 기준이 결제일인지 배송일인지 불분명하다.</p>
<ul>
<li><strong>해결 전략:</strong> 오라클의 솔루션은 단순히 SQL을 생성하는 데 그치지 않고, <strong>기획 에이전트(Planner Agent)</strong> 단계를 도입한다. 이 에이전트는 사용자의 모호한 질의를 구체적인 추론 단계로 분해하고, 필요한 경우 사용자에게 되묻거나(Clarification), 여러 후보 SQL을 생성한 후 실행 결과의 일관성을 검증한다. 즉, ‘생성’ 단계에서의 모호성을 ‘검증 및 상호작용’ 단계에서 결정론적으로 확정 짓는 접근법이다.</li>
</ul>
<h2>4.  결론: 모호성을 관리 가능한 리스크로 전환하기</h2>
<p>’정답’의 모호성은 생성형 AI가 가진 창의성의 동전의 양면과 같다. 이를 완전히 제거하는 것은 불가능하며, 바람직하지도 않다. 창의성이란 본질적으로 기존의 정답을 벗어나는 행위이기 때문이다. 따라서 소프트웨어 엔지니어링의 목표는 모호성의 ’제거’가 아니라 **모호성의 ‘관리(Management)’**로 전환되어야 한다.</p>
<p>소프트웨어 개발자는 AI의 출력을 맹목적으로 신뢰하는 대신, (1) 실행 가능한 코드는 샌드박스 내의 테스트 케이스를 통해 물리적으로 검증하고, (2) 텍스트나 데이터 생성물은 메타모픽 관계나 LLM-as-a-Judge와 같은 대리 오라클을 통해 논리적 일관성을 평가하며, (3) 최종적으로 인간 전문가가 개입할 수 있는 루프(Human-in-the-Loop)를 설계하여 기계가 해결할 수 없는 주관적 모호성을 해소해야 한다. 참과 거짓의 이분법을 넘어, ‘신뢰도(Confidence)’, ‘일관성(Consistency)’, ’유용성(Utility)’이라는 새로운 축을 통해 품질을 정의하고 측정하는 것, 이것이 AI 네이티브 소프트웨어 개발자가 갖추어야 할 핵심 역량이다.</p>
<h2>5. 참고 자료</h2>
<ol>
<li>The Oracle Problem - YLD, https://www.yld.io/blog/the-oracle-problem</li>
<li>(PDF) The Oracle Problem in Software Testing: A Survey, https://www.researchgate.net/publication/276255185_The_Oracle_Problem_in_Software_Testing_A_Survey</li>
<li>Can artificial intelligence solve the blockchain oracle problem, https://www.frontiersin.org/journals/blockchain/articles/10.3389/fbloc.2025.1682623/pdf</li>
<li>Challenges in Testing Large Language Model Based Software - arXiv, https://arxiv.org/html/2503.00481v1</li>
<li>TeXiS - Docta Complutense, https://docta.ucm.es/bitstreams/1b98f802-bc90-41ff-bea9-6de420c522de/download</li>
<li>NATURALSIGNER: DIFFUSION MODELS ARE NATURAL SIGN, https://openreview.net/pdf?id=4JjSJyT15z</li>
<li>Memories are One-to-Many Mapping Alleviators in Talking Face, https://ieeexplore.ieee.org/iel8/34/4359286/10547422.pdf</li>
<li>Target Conditioning for One-to-Many Generation - ACL Anthology, https://aclanthology.org/2020.findings-emnlp.256.pdf</li>
<li>Why We Need New Evaluation Metrics for NLG - ACL Anthology, https://aclanthology.org/D17-1238.pdf</li>
<li>Evaluating quality and reliability in generated outputs, https://docs.aws.amazon.com/prescriptive-guidance/latest/gen-ai-lifecycle-operational-excellence/dev-experimenting-quality.html</li>
<li>AI Hallucinations Explained: Causes &amp; Solutions - Crystal Hues, https://www.crystalhues.com/blog/a-guide-to-understand-ai-hallucinations-and-how-to-manage-it</li>
<li>(PDF) Can Artificial Intelligence solve the blockchain oracle problem, https://www.researchgate.net/publication/393379061_Can_Artificial_Intelligence_solve_the_blockchain_oracle_problem_Unpacking_the_Challenges_and_Possibilities</li>
<li>“This ground truth is muddy anyway” Ground truth data assemblages, https://lup.lub.lu.se/search/files/221246566/Hogberg_This_ground_truth_is_muddy_anyway_SoFo.pdf</li>
<li>The Evaluation Gap: Why AI Breaks in Reality Even When It Works, https://kili-technology.com/blog/the-evaluation-gap-why-ai-breaks-in-reality-even-when-it-works-in-the-lab</li>
<li>Validating LLM-as-a-Judge Systems under Rating Indeterminacy, https://openreview.net/pdf/2ad527d02fe920443fe79a516d7575f417103f99.pdf</li>
<li>Disentangling Label Variation in Natural Language Processing with, https://aclanthology.org/2025.nlperspectives-1.6.pdf</li>
<li>arXiv:2503.04910v1 [cs.CL] 6 Mar 2025, <a href="https://arxiv.org/pdf/2503.04910">https://arxiv.org/pdf/2503.04910?</a></li>
<li>Why We Need New Evaluation Metrics for NLG - ACL Anthology, https://aclanthology.org/D17-1238/</li>
<li>Validating LLM-as-a-Judge Systems under Rating Indeterminacy, https://blog.ml.cmu.edu/2025/12/09/validating-llm-as-a-judge-systems-under-rating-indeterminacy/</li>
<li>The Oracle Problem in Software Testing: A Survey, http://www0.cs.ucl.ac.uk/staff/m.harman/tse-oracle.pdf</li>
<li>(PDF) Estimating Correctness Without Oracles in LLM-Based Code, https://www.researchgate.net/publication/393260401_Estimating_Correctness_Without_Oracles_in_LLM-Based_Code_Generation</li>
<li>Oracle-guided Program Selection from Large Language Models, https://mechtaev.com/files/issta24.pdf</li>
<li>Testing AI Systems: Handling the Test Oracle Problem, https://dev.to/qa-leaders/testing-ai-systems-handling-the-test-oracle-problem-3038</li>
<li>quantifying and eliminating fabrication risk in LLMs From Generative, https://arxiv.org/html/2601.15476v1</li>
<li>Mitigating the Agreeableness Bias in LLM Judge Evaluations - arXiv, https://arxiv.org/html/2510.11822v2</li>
<li>(PDF) G-Eval: NLG Evaluation using GPT-4 with Better Human, https://www.researchgate.net/publication/369624117_GPTEval_NLG_Evaluation_using_GPT-4_with_Better_Human_Alignment</li>
<li>Oracle Breaks New Ground in Multilingual Text-to-SQL by Winning, https://blogs.oracle.com/cloud-infrastructure/oracle-wins-archer-nl2sql-challenge</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>