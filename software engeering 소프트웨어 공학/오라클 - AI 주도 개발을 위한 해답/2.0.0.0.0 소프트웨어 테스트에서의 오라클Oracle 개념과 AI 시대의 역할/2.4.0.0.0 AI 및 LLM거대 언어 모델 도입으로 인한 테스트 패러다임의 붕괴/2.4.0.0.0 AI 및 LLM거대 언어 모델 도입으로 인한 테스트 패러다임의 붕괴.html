<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2.4 AI 및 LLM(거대 언어 모델) 도입으로 인한 테스트 패러다임의 붕괴</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2.4 AI 및 LLM(거대 언어 모델) 도입으로 인한 테스트 패러다임의 붕괴</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 2. 소프트웨어 테스트에서의 오라클(Oracle) 개념과 AI 시대의 역할</a> / <a href="index.html">2.4 AI 및 LLM(거대 언어 모델) 도입으로 인한 테스트 패러다임의 붕괴</a> / <span>2.4 AI 및 LLM(거대 언어 모델) 도입으로 인한 테스트 패러다임의 붕괴</span></nav>
                </div>
            </header>
            <article>
                <h1>2.4 AI 및 LLM(거대 언어 모델) 도입으로 인한 테스트 패러다임의 붕괴</h1>
<h2>1.  서론: 결정론적 세계의 종말과 확률론적 소프트웨어 공학의 도래</h2>
<p>소프트웨어 공학의 역사는 ’통제’와 ’예측’을 향한 끊임없는 투쟁의 기록이었다. 1960년대 소프트웨어 위기(Software Crisis)가 선언된 이래, 인류는 복잡한 시스템을 통제 가능한 단위로 분해하고, 각 단위가 설계된 대로 정확하게 동작함을 수학적이고 논리적인 방법으로 증명하기 위해 노력해 왔다. 이 거대한 지적 체계의 중심에는 ’테스트(Test)’가 자리 잡고 있었으며, 그 테스트의 근간은 언제나 **결정론(Determinism)**이라는 흔들리지 않는 대전제였다. “동일한 입력(Input)과 동일한 초기 상태(State)가 주어지면, 시스템은 반드시, 예외 없이 동일한 출력(Output)을 산출해야 한다.” 이 명제는 지난 반세기 동안 모든 소프트웨어 품질 보증(QA) 활동의 헌법과도 같았다. 단위 테스트(Unit Test)부터 시스템 통합 테스트(Integration Test)에 이르기까지, 모든 검증 절차는 사전에 정의된 기대 결과(Expected Result)와 실제 실행 결과(Actual Result)의 일치 여부를 이진법적(Binary)으로 판단하는 과정이었다.</p>
<p>그러나 2020년대 중반, 거대 언어 모델(Large Language Models, LLM)과 생성형 AI(Generative AI)의 폭발적인 도입은 이 견고했던 패러다임을 뿌리째 뒤흔들고 있다. 우리는 지금 소프트웨어의 본질이 명시적인 규칙과 논리의 집합에서, 데이터에 기반한 확률적 추론의 흐름으로 변화하는 역사적인 변곡점에 서 있다. 과거의 소프트웨어 공학이 “2 더하기 2는 4“와 같이 명확한 정답이 존재하는 닫힌 세계(Closed World)를 다루었다면, AI 기반의 소프트웨어 공학은 “가장 적절하고 공감 가는 이메일 답변을 작성하라“와 같이 정답의 스펙트럼이 무한하고 맥락 의존적인 열린 세계(Open World)로 진입했다.</p>
<p>이러한 변화는 기존의 테스트 도구와 방법론을 무력화시키고 있다. LLM 기반 애플리케이션은 본질적으로 비결정론적(Non-deterministic)이며, 그 출력 공간(Output Space)은 사실상 무한하다. 개발자가 예상 가능한 모든 출력을 미리 정의하는 것은 불가능해졌으며, 설령 정의한다 해도 모델의 확률적 특성으로 인해 재현성을 보장하기 어렵다. 이는 단순한 기술적 난관을 넘어, ’품질(Quality)’이라는 개념 자체를 재정의해야 하는 인식론적 위기를 초래한다. 정답이 없는 상황에서 우리는 어떻게 ’옳음(Correctness)’을 정의할 것인가? 매번 다른 대답을 내놓는 시스템을 우리는 어떻게 신뢰할 수 있는가?</p>
<p>본 장에서는 AI와 LLM의 도입이 초래한 테스트 패러다임의 붕괴 현상을 심층적으로 분석한다. 우리는 먼저 소프트웨어 테스트의 가장 오래된 난제인 ’오라클 문제(The Oracle Problem)’가 AI 시대에 어떻게 재점화되었는지 살펴볼 것이다. 이어서, LLM의 근본적인 특성인 비결정론과 창발성(Emergence)이 어떻게 기존의 검증 체계를 무너뜨리는지 기술적, 이론적 관점에서 파헤친다. 또한, 최근 실리콘밸리를 중심으로 확산되고 있는 ‘바이브 코딩(Vibe Coding)’ 현상을 통해, 이러한 기술적 변화가 개발자 문화와 소프트웨어 품질에 미치는 사회기술적(Socio-technical) 파장을 진단할 것이다. 마지막으로, 메타모픽 테스트(Metamorphic Testing)와 같은 새로운 방법론이 어떻게 이 혼란 속에서 새로운 질서를 모색하고 있는지 논의하며, 확률론적 소프트웨어 공학 시대의 새로운 품질 보증 전략을 제시하고자 한다.</p>
<h2>2.  오라클 문제(The Oracle Problem)의 재점화와 심화</h2>
<p>소프트웨어 테스트 이론에서 ’오라클(Oracle)’은 시스템의 실행 결과가 올바른지 판단하는 권위 있는 메커니즘을 의미한다. 고대 그리스 신탁에서 유래한 이 용어는, 질문에 대해 절대적인 진리를 말해주는 존재를 빗댄 것이다. 전통적인 소프트웨어 개발에서 오라클은 상세한 요구사항 명세서(Specification), 수학적 공식, 혹은 이전 버전의 시스템(Legacy System)이 담당했다. 예를 들어, 은행 시스템의 이자 계산 모듈을 테스트할 때, 오라클은 <span class="math math-inline">Principal \times Rate \times Time</span>이라는 수학 공식이었다. 테스터는 이 공식에 대입한 결과값과 프로그램의 출력값이 일치하는지를 확인하기만 하면 되었다.</p>
<p>그러나 LLM의 도입은 잠잠해졌던 이 오라클 문제를 전례 없는 수준으로 심화시키고 있다. AI 연구자들과 소프트웨어 엔지니어들은 이제 “정답이 없는 문제“를 채점해야 하는 난관에 봉착했다.</p>
<h3>2.1  정답 없는 질문과 진리값의 모호성 (Ambiguity of Truth)</h3>
<p>LLM이 수행하는 주요 작업들—텍스트 요약, 번역, 창의적 글쓰기, 코드 생성, 대화형 에이전트—은 본질적으로 단일한 정답(Ground Truth)이 존재하지 않는 영역이다.</p>
<ul>
<li><strong>요약(Summarization)의 예:</strong> “이 5페이지짜리 기술 문서를 3문장으로 요약하라“는 요청에 대해, 수백 명의 전문가가 작성한 요약문은 모두 다를 것이다. 어떤 요약은 핵심 기술에 집중할 것이고, 어떤 요약은 비즈니스 가치에 집중할 것이다. 이 중 무엇이 ’정답’인가? 기계적으로 <code>assert equal(expected, actual)</code>을 수행하는 것은 불가능하다.</li>
<li><strong>창의적 생성의 예:</strong> “가을에 어울리는 시를 써줘“라는 프롬프트에 대해 모델이 생성한 시가 ’좋은 시’인지 ’나쁜 시’인지, 혹은 ’버그가 있는 시’인지 판별할 수 있는 알고리즘은 존재하지 않는다.</li>
</ul>
<p>이러한 모호성은 테스트 오라클의 부재(Absence of Test Oracle)로 이어진다. 오라클이 없거나, 오라클을 구축하는 비용이 테스트 대상 시스템을 개발하는 비용보다 높을 때, 우리는 이를 ’오라클 문제’라고 부른다. LLM 환경에서는 결과의 옳고 그름을 판단하기 위해 유창성(Fluency), 일관성(Coherence), 사실성(Factuality), 윤리성(Safety) 등 다차원적이고 주관적인 기준을 적용해야 한다. 이는 기존의 참/거짓(Boolean) 논리로는 포착할 수 없는 영역이다.</p>
<h3>2.2  확률적 앵무새와 환각(Hallucination)의 검증</h3>
<p>LLM은 훈련 데이터의 통계적 패턴을 학습하여 다음에 올 가장 그럴듯한 단어를 예측하는 ’확률적 앵무새(Stochastic Parrots)’와 유사하게 작동한다. 이 메커니즘의 필연적인 부작용이 바로 **‘환각(Hallucination)’**이다. 모델은 사실이 아닌 정보를 매우 확신에 찬 어조로 생성할 수 있으며, 이는 논리적 오류와는 구분되는 ’사실적 오류’를 만들어낸다.</p>
<p>전통적인 데이터베이스 애플리케이션 테스트에서는 “DB에 저장된 값과 일치하는가?“가 유일한 오라클이었다. 그러나 검색 증강 생성(RAG) 시스템이나 지식 기반 에이전트의 테스트 오라클은 다음과 같이 훨씬 복잡한 질문을 던져야 한다:</p>
<ol>
<li><strong>충실성(Faithfulness):</strong> 생성된 답변이 검색된 문서(Context)의 내용에 기반하고 있는가, 아니면 모델의 내부 지식(Parametric Knowledge)에서 날조된 것인가?</li>
<li><strong>관련성(Relevance):</strong> 검색된 문서가 사용자의 질문에 답변하는 데 적절한 정보인가?</li>
<li><strong>유용성(Helpfulness):</strong> 답변이 사용자의 의도와 맥락에 부합하며 실질적인 도움을 주는가?</li>
</ol>
<p>이러한 질문들에 답하기 위해 인간이 직접 개입하여 검증하는 ‘Human-in-the-loop’ 방식이 가장 정확할 수 있다. 그러나 LLM이 생성하는 방대한 양의 텍스트를 인간이 일일이 검증하는 것은 시간과 비용 측면에서 확장 불가능(Unscalable)하며, 검증자의 피로도나 주관에 따라 결과가 달라지는 일관성 부족 문제(Inconsistency)를 야기한다.</p>
<h3>2.3  LLM-as-a-Judge와 순환 논증의 오류</h3>
<p>인간 오라클의 한계를 극복하기 위해 최근에는 GPT-4와 같은 고성능 LLM을 심판관(Judge)으로 사용하여 다른 모델의 출력을 평가하는 <strong>‘LLM-as-a-Judge’</strong> 기법이 확산되고 있다. 이 방식은 “이 답변이 질문에 대해 얼마나 정확한지 1점에서 10점 사이로 평가하라“와 같은 프롬프트를 사용하여 자동화된 평가를 가능하게 한다. 하지만 이는 인식론적으로 위험한 **‘순환 논증의 오류(Circular Reasoning Fallacy)’**를 내포한다. 검증 도구(Judge LLM) 자체가 검증 대상(Subject LLM)과 유사한 아키텍처와 훈련 데이터를 공유하기 때문에, 동일한 편향(Bias)과 오류를 공유할 가능성이 높다. 예를 들어, 특정 문화권에 편향된 데이터를 학습한 모델은 편향된 답변을 생성할 것이고, 동일한 데이터로 학습된 심판 모델은 그 편향된 답변을 ’높은 점수’로 평가할 것이다. 이는 “자신이 쓴 답안지를 자신이 채점하는 것“과 같으며, 객관적인 품질 보증을 위협하는 요인이 된다.</p>
<h3>2.4  데이터 비교: 전통적 오라클 vs LLM 시대의 오라클</h3>
<p>다음 표는 전통적인 소프트웨어 테스트와 LLM 기반 테스트에서 오라클의 역할과 특성이 어떻게 변화했는지를 요약한다.</p>
<table><thead><tr><th><strong>특성</strong></th><th><strong>전통적 소프트웨어 테스트 (Deterministic)</strong></th><th><strong>LLM 기반 소프트웨어 테스트 (Probabilistic)</strong></th></tr></thead><tbody>
<tr><td><strong>오라클의 형태</strong></td><td>명세서, 수학 공식, 비교 대상 시스템</td><td>인간 평가, LLM 심판관, 의미적 유사도 측정</td></tr>
<tr><td><strong>판단 기준</strong></td><td>정확한 일치 (Exact Match)</td><td>의미적 유사성 (Semantic Similarity), 유창성, 사실성</td></tr>
<tr><td><strong>결과 유형</strong></td><td>이진법적 (Pass / Fail)</td><td>연속적 점수 (Score 0.0 ~ 1.0), 확률 분포</td></tr>
<tr><td><strong>주요 난제</strong></td><td>오라클 구현의 복잡성</td><td>정답의 부재, 주관성 개입, 평가 비용</td></tr>
<tr><td><strong>검증 도구</strong></td><td><code>JUnit</code>, <code>PyTest</code>, <code>Selenium</code></td><td><code>RAGAS</code>, <code>DeepEval</code>, <code>LLM-as-a-Judge</code></td></tr>
<tr><td><strong>실패 원인</strong></td><td>논리적 버그, 구문 오류</td><td>환각, 편향, 프롬프트 민감성, 비결정론</td></tr>
</tbody></table>
<h2>3.  비결정론(Nondeterminism)과 재현성(Reproducibility)의 위기</h2>
<p>소프트웨어 테스트의 핵심 원칙 중 하나는 **재현성(Reproducibility)**이다. “어제 실패한 테스트는 코드를 수정하지 않는 한 오늘 다시 실행해도 반드시 실패해야 한다.” 이 원칙이 지켜지지 않는다면 디버깅은 불가능해진다. 그러나 LLM은 본질적으로 비결정론적이며, 이는 테스트 엔지니어링에 재앙적인 변수를 도입한다.</p>
<h3>3.1  근본적인 비결정론의 기술적 원인</h3>
<p>많은 개발자들이 LLM의 <code>Temperature</code> 파라미터를 0으로 설정하면 결정론적인(Deterministic) 결과를 얻을 수 있다고 믿는다. 이론적으로 탐욕적 디코딩(Greedy Decoding)은 가장 높은 확률을 가진 토큰만을 선택하므로 결과가 일정해야 한다. 그러나 실제로는 <code>Temperature=0</code>에서도 미세한 비결정론이 발생하며, 이는 하드웨어와 아키텍처 수준의 복합적인 요인에 기인한다.</p>
<ol>
<li><strong>부동소수점 연산의 비결합성 (Floating-point Non-associativity):</strong> 현대의 딥러닝은 GPU 상에서 대규모 병렬 연산을 수행한다. 부동소수점 덧셈 연산은 결합법칙이 성립하지 않는다. 즉, <span class="math math-inline">(A + B) + C</span>와 <span class="math math-inline">A + (B + C)</span>의 결과가 미세하게 다를 수 있다. 수십억 개의 파라미터를 연산하는 과정에서 병렬 처리되는 스레드(Thread)의 작업 완료 순서는 실행 시점의 하드웨어 상태에 따라 달라지며, 이는 합산 결과에 미세한 차이를 발생시킨다.</li>
<li><strong>원자적 덧셈과 경쟁 상태 (Atomic Adds &amp; Race Conditions):</strong> GPU 커널, 특히 <code>FlashAttention</code>과 같은 최적화된 연산 과정에서 사용되는 원자적 덧셈(Atomic Add) 연산은 실행 순서가 보장되지 않는다. 어떤 코어가 먼저 연산을 완료하느냐에 따라 누적 값이 달라지며, 이 미세한 차이는 토큰 생성 과정에서 나비 효과(Butterfly Effect)를 일으켜 완전히 다른 문장을 생성하게 만들 수 있다.</li>
<li><strong>전문가 혼합 모델(Mixture of Experts, MoE):</strong> GPT-4와 같은 최신 모델들은 여러 전문가 모델 중 일부를 동적으로 선택하여 추론한다. 로드 밸런싱이나 시스템 부하에 따라 선택되는 전문가 경로가 달라질 수 있으며, 이는 비결정론을 심화시킨다.</li>
</ol>
<h3>3.2  프롬프트 민감성: 토큰의 나비 효과</h3>
<p>LLM은 입력 프롬프트의 극히 사소한 변화에도 민감하게 반응한다. 이를 **프롬프트 민감성(Prompt Sensitivity)**이라 한다.</p>
<ul>
<li><strong>띄어쓰기와 구두점:</strong> “Explain AI.“와 “Explain AI “ (공백 추가)는 서로 다른 토큰 시퀀스로 인식되어 전혀 다른 텍스트를 생성할 수 있다.</li>
<li><strong>동의어 교체:</strong> “차량을 구매하다“를 “자동차를 사다“로 바꾸었을 때, 모델의 논리적 추론 결과가 뒤집히는 경우가 빈번하다.</li>
<li><strong>순서 변경:</strong> 퓨샷(Few-shot) 예제의 순서를 바꾸는 것만으로도 모델의 정확도가 급격히 변동한다는 연구 결과가 있다.</li>
</ul>
<p>이러한 민감성은 테스트 케이스 작성을 극도로 어렵게 만든다. 테스트의 실패가 모델의 논리적 결함 때문인지, 아니면 단순히 프롬프트의 뉘앙스 차이 때문인지 구분하기 어렵기 때문이다.</p>
<h3>3.3  ’플레이키 테스트(Flaky Test)’의 개념 확장</h3>
<p>기존 소프트웨어 공학에서 ’플레이키 테스트(간헐적으로 실패하는 테스트)’는 제거해야 할 기술 부채이자 악취(Bad Smell)였다. 네트워크 지연이나 비동기 처리 문제로 발생하는 플레이키 테스트는 신뢰도를 떨어뜨리기 때문이다. 하지만 LLM 기반 애플리케이션에서는 **시스템 자체가 플레이키(Flaky)**하다. 시스템 자체가 확률적으로 동작하기 때문에, 테스트 결과의 불일치는 버그가 아니라 시스템의 본질적인 속성(Feature)이 된다. 이는 테스트 전략의 근본적인 수정을 요구한다. 단 한 번의 실행(Single Run)으로 성공/실패를 판정하는 것은 통계적으로 무의미해졌다. 대신, **통계적 유의성(Statistical Significance)**을 확보하기 위해 동일한 테스트를 수십, 수백 번 반복 실행하고 성공률(Pass Rate)의 분포를 분석해야 한다. “재실행(Retry)“은 더 이상 미봉책이 아니라 필수적인 테스트 프로토콜의 일부가 되어야 하지만, 이는 테스트 비용(API 호출 비용, 시간)을 기하급수적으로 증가시키는 딜레마를 낳는다.</p>
<h2>4.  창발성(Emergence)과 미지의 테스트 영역</h2>
<p>LLM의 가장 매혹적이면서도 두려운 특징은 **‘창발성(Emergence)’**이다. 모델의 파라미터 수와 훈련 데이터 규모가 일정 임계점(Threshold)을 넘어서면, 훈련 과정에서 명시적으로 의도하지 않았던 능력들이 갑작스럽게 발현된다. 이는 복잡계 이론에서 말하는 창발 현상과 유사하며, 테스트 관점에서는 “우리가 무엇을 테스트해야 하는지조차 모른다“는 실존적 위협을 의미한다.</p>
<h3>4.1  알려지지 않은 기능의 리스크 (Unknown Unknowns)</h3>
<p>전통적인 소프트웨어는 설계 명세서(Spec)에 정의된 기능만을 수행하도록 만들어진다. 계산기 앱은 계산 기능만 수행하면 된다. 그러나 LLM은 범용 모델(Foundation Model)로서, 명세되지 않은 수만 가지의 기능을 수행할 잠재력을 가지고 있다.</p>
<ul>
<li><strong>기능적 창발의 역설:</strong> 모델을 훈련시킬 때 ’체스’를 두는 법을 명시적으로 가르치지 않았음에도, 모델이 체스 규칙을 이해하고 게임을 진행할 수 있게 된다면, 테스트 엔지니어는 이 모델의 ’체스 기능’을 검증해야 하는가?</li>
<li><strong>경계 조건의 소멸:</strong> 입력값의 경계가 사라졌다. 사용자는 모델에게 셰익스피어 스타일로 파이썬 코드를 짜달라고 하거나, 화학 공식으로 연애 편지를 써달라고 할 수 있다. 모델은 무엇이든 대답할 준비가 되어 있으며, 이는 테스트 커버리지(Test Coverage)라는 개념 자체를 무의미하게 만들 수 있다. 가능한 입력 조합이 무한대이기 때문이다.</li>
</ul>
<h3>4.2  복합적 추론 실패와 “유레카 본능”</h3>
<p>최신 연구에 따르면, LLM은 복잡한 추론 과제에서 ‘유레카 본능(Eureka Instinct)’—즉, 논리적 근거 없이 성급하게 결론에 도달하려는 경향—을 보이며, 이는 과도한 자신감(Overconfidence)과 결합하여 오류를 발생시킨다. 모델은 자신이 모른다는 사실을 인정하기보다, 그럴듯한 거짓말을 지어내어 문제를 해결하려는 경향이 있다. 특히 다단계 추론(Multi-step Reasoning)이 필요한 에이전트(Agent) 시스템에서, 개별 단계의 작은 오류들이 누적되어 전체 시스템을 붕괴시키는 <strong>‘오류 전파(Error Propagation)’</strong> 현상이 빈번하게 발생한다.</p>
<ol>
<li><strong>계획 단계 오류:</strong> 사용자의 모호한 요청을 잘못 해석하여 계획을 수립한다.</li>
<li><strong>도구 사용 오류:</strong> 잘못된 API를 호출하거나 파라미터를 잘못 전달한다.</li>
<li><strong>결과 통합 오류:</strong> 도구의 실행 결과를 잘못 해석하여 최종 답변을 생성한다.</li>
</ol>
<p>이 과정에서 각 단계는 90%의 정확도를 가질지라도, 5단계를 거치면 전체 시스템의 신뢰도는 <span class="math math-inline">0.9^5 \approx 59%</span>로 급락한다. 기존의 단위 테스트는 이러한 창발적이고 복합적인 상호작용 오류를 포착하는 데 한계를 드러낸다.</p>
<h3>4.3  보안 취약점의 창발: 프롬프트 인젝션</h3>
<p>창발성은 보안 영역에서도 새로운 위협을 만들어냈다. **프롬프트 인젝션(Prompt Injection)**은 모델에게 특정한 지시를 내려 시스템의 안전 장치를 우회하는 공격 기법이다. “이전의 지시를 무시하고…“와 같은 문구는 모델의 창발적 언어 이해 능력을 역이용하여 개발자가 설정한 제약 조건을 무력화시킨다. 이는 전통적인 SQL 인젝션과는 달리, 공격 패턴이 정형화되어 있지 않고 자연어의 무한한 변주 속에 숨어 있어 방어가 극도로 어렵다. 테스트 엔지니어는 이제 기능적 정확성뿐만 아니라, 모델이 ’나쁜 지시’를 얼마나 잘 거부하는지(Refusal Testing)를 검증하는 적대적 테스트(Adversarial Testing)를 수행해야 한다.</p>
<h2>5.  새로운 테스트 패러다임: 어떻게 적응할 것인가?</h2>
<p>이러한 붕괴 속에서 연구자들과 실무자들은 새로운 테스트 패러다임을 모색하고 있다. 이는 ’정확성(Correctness)’에서 ’일관성(Consistency)’과 ’유용성(Utility)’으로, ’검증(Verification)’에서 ’평가(Evaluation)’로의 전환을 의미한다. 또한, 시스템을 구성하는 레이어별로 차별화된 테스트 전략이 요구된다.</p>
<h3>5.1  메타모픽 테스트 (Metamorphic Testing, MT): 오라클 문제의 대안</h3>
<p>오라클 문제의 가장 유력한 대안으로 떠오르는 것이 **메타모픽 테스트(MT)**이다. MT는 개별 입력값에 대한 정답(Ground Truth)을 몰라도, 입력값의 변화에 따른 출력값의 **관계(Relation)**를 검증함으로써 시스템의 올바름을 추론한다.</p>
<ul>
<li><strong>핵심 원리:</strong> 입력 <span class="math math-inline">x</span>에 변환 함수 <span class="math math-inline">T</span>를 적용하여 <span class="math math-inline">x&#39;</span>를 만들었을 때, 시스템의 출력 <span class="math math-inline">f(x)</span>와 <span class="math math-inline">f(x&#39;)</span> 사이에는 사전에 정의된 예측 가능한 관계(Metamorphic Relation, MR) <span class="math math-inline">R</span>이 성립해야 한다. 즉, <span class="math math-inline">R(f(x), f(x&#39;))</span>가 참이어야 한다.</li>
<li><strong>LLM 적용 사례 및 메타모픽 관계(MR)의 유형:</strong></li>
</ul>
<ol>
<li><strong>패러프레이징 (Paraphrasing):</strong> 질문의 문체나 어순을 바꾸어도(입력 변환), 답변의 핵심 의도나 감정 분류 결과는 동일해야 한다. (MR: <span class="math math-inline">f(x) \approx f(x&#39;)</span>)</li>
<li><strong>언어 교차 검증 (Cross-lingual Consistency):</strong> 질문을 영어로 번역하여 입력했을 때의 답변과, 한국어로 입력했을 때의 답변이 의미적으로 일관성을 유지해야 한다.</li>
<li><strong>노이즈 주입 (Noise Injection):</strong> 입력 텍스트에 오타나 무의미한 특수문자를 추가해도, 모델의 분류 결과나 요약 내용은 흔들리지 않아야 한다. (강건성 검증)</li>
<li><strong>부정형 추가 (Negation):</strong> 긍정문의 입력에 “not“을 추가하면, 감정 분석 결과는 반대(Positive <span class="math math-inline">\to</span> Negative)로 바뀌어야 한다.</li>
</ol>
<p>최근 연구에 따르면 MT는 기존의 정답 기반 테스트가 놓친 결함의 11% 이상을 추가로 발견해 낼 수 있음이 입증되었다. 이는 정답 데이터셋(Labeled Dataset) 구축 비용을 획기적으로 절감하면서도 높은 테스트 커버리지를 확보할 수 있는 방법이다.</p>
<h3>5.2  의미론적 평가(Semantic Evaluation)와 임베딩</h3>
<p>전통적인 문자열 일치(Exact Match) 테스트는 붕괴되었다. “Hello world“와 “Hi world“는 문자열로는 다르지만 의미적으로는 유사하다. 따라서 <strong>임베딩(Embedding)</strong> 벡터 간의 거리(Distance)를 측정하는 방식이 표준으로 자리 잡고 있다.</p>
<ul>
<li><strong>BERTScore / Cosine Similarity:</strong> 기대 답변(Reference)과 생성 답변(Candidate)을 벡터 공간에 매핑한 후, 두 벡터 사이의 각도(코사인 유사도)를 계산한다. 유사도가 특정 임계값(예: 0.85) 이상이면 통과(Pass)로 간주한다.</li>
<li><strong>교차 인코더(Cross-Encoder):</strong> 두 문장을 동시에 모델에 입력하여 의미적 유사성을 더 정밀하게 판단한다. 이는 단순 코사인 유사도보다 정확하지만 연산 비용이 높다.</li>
</ul>
<h3>5.3  속성 기반 테스트(Property-Based Testing)</h3>
<p>답변의 ’내용’을 검증하기 어렵다면, 답변의 ’구조’와 ’속성’을 검증하는 전략이다.</p>
<ul>
<li>
<p><strong>형식 준수:</strong> 출력이 유효한 JSON 포맷인가? 특정 키(Key)가 포함되어 있는가?</p>
</li>
<li>
<p><strong>안전성 검사:</strong> 비속어, 개인정보(PII), 경쟁사 언급 등 금지된 단어가 포함되지 않았는가?</p>
</li>
<li>
<p><strong>길이 및 톤:</strong> 답변의 길이가 제한을 초과하지 않는가? 문체가 공손한가?</p>
</li>
</ul>
<p>이러한 속성 기반 테스트는 <code>Hypothesis</code>와 같은 기존 도구를 LLM 환경에 맞게 변형하여 적용할 수 있으며, 최소한의 품질 안전망 역할을 수행한다.</p>
<h3>5.4  3계층 아키텍처 기반 테스트 전략</h3>
<p>최신 연구는 LLM 애플리케이션을 세 가지 계층으로 분해하여 접근할 것을 제안한다.</p>
<ol>
<li>
<p><strong>시스템 쉘 계층 (System Shell Layer):</strong> API 호출, 도구 실행, 데이터 파이프라인 등 전통적인 소프트웨어 영역. 여기서는 기존의 결정론적 단위 테스트가 여전히 유효하다.</p>
</li>
<li>
<p><strong>프롬프트 오케스트레이션 계층 (Prompt Orchestration Layer):</strong> 프롬프트 템플릿, 컨텍스트 관리 로직. 여기서는 메타모픽 테스트와 의미론적 평가가 필요하다.</p>
</li>
<li>
<p><strong>LLM 추론 코어 (LLM Inference Core):</strong> 모델 자체. 여기서는 벤치마킹과 파인튜닝 평가가 주를 이룬다.</p>
</li>
</ol>
<p>이러한 계층적 접근은 테스트의 복잡도를 관리 가능한 수준으로 낮추는 데 도움을 준다.</p>
<h2>6.  ’바이브 코딩(Vibe Coding)’과 QA의 위기: 사회기술적 관점</h2>
<p>테스트 패러다임의 붕괴는 단순히 기술적인 문제에 그치지 않고, 개발자들의 행동 양식과 소프트웨어 개발 문화 전반에 심각한 변화를 일으키고 있다. 최근 실리콘밸리와 오픈소스 커뮤니티를 중심으로 확산되고 있는 <strong>‘바이브 코딩(Vibe Coding)’</strong> 현상은 이러한 위기의 징후를 가장 극명하게 보여준다.</p>
<h3>6.1  바이브 코딩의 정의와 ‘느낌’ 중심의 개발</h3>
<p>’바이브 코딩’이란 개발자가 코드의 내부 로직이나 문법을 깊이 이해하거나 검증하지 않고, AI에게 자연어 프롬프트(Vibe)를 던져 생성된 코드를 즉각적으로, 그리고 반복적으로 적용하는 관행을 의미한다. 테슬라의 전 AI 이사 안드레이 카패시(Andrej Karpathy)가 언급하며 유명해진 이 현상은 “일단 돌아가면(It works) 그만“이라는 태도를 극단적으로 밀어붙인다. 개발자는 더 이상 코드를 ’작성(Write)’하지 않고 ’지휘(Direct)’한다. 코드가 어떻게 작동하는지보다, 결과물이 내가 원하는 ’느낌(Vibe)’에 맞는지가 더 중요해진다. 이는 소프트웨어 개발의 진입 장벽을 낮추고 프로토타이핑 속도를 혁신적으로 높였지만, 동시에 품질 보증(QA)의 근간을 위협하고 있다.</p>
<h3>6.2  속도와 품질의 역설 (Speed-Quality Trade-off Paradox)</h3>
<p>바이브 코딩은 **‘속도와 품질의 역설’**을 낳는다. 개발 속도는 비약적으로 빨라져 “즉각적인 성공과 몰입(Flow)“을 경험하게 하지만, 결과물의 신뢰성과 유지보수성(Maintainability)은 급격히 하락한다.</p>
<ul>
<li><strong>이해의 상실 (Loss of Understanding):</strong> 개발자가 자신이 생성한 코드를 이해하지 못하므로, 버그가 발생했을 때 이를 수정할 능력이 없다. 이는 ’디버깅의 종말’을 예고한다.</li>
<li><strong>QA의 실종:</strong> 많은 ’바이브 코더’들이 테스트 코드를 작성하는 단계를 건너뛰거나, AI에게 테스트 코드 작성까지 일임한다. AI가 짠 코드를 AI가 짠 테스트로 검증하는 것은 “자신의 숙제를 자신이 채점하는 것“과 같아, 심각한 결함을 놓칠 가능성이 매우 높다.</li>
</ul>
<h3>6.3  기술 부채의 폭증과 공급망 위협</h3>
<p>AI가 생성한 코드는 겉보기에는 깔끔하고 문법적으로 완벽해 보이지만, 미묘한 보안 취약점이나 비효율적인 로직, 불필요한 복잡성을 내포하고 있을 가능성이 높다. 이러한 코드가 검증 없이 프로젝트에 누적되면, 장기적으로는 감당할 수 없는 **‘기술 부채(Technical Debt)’**가 되어 시스템 전체를 마비시킬 수 있다. 더욱 심각한 것은 **소프트웨어 공급망 위험(Software Supply Chain Risk)**이다. AI 에이전트가 코드를 생성하는 과정에서 존재하지 않는 패키지를 임포트(Import)하거나(패키지 환각), 악성 코드가 포함된 라이브러리를 추천할 수 있다. 연구 결과에 따르면, AI 에이전트가 수행한 의존성 업데이트는 인간 개발자가 수행한 것보다 취약점을 포함할 확률이 높았으며, 이는 자동화된 코딩 도구가 보안의 구멍이 될 수 있음을 시사한다. “검증되지 않은 코드의 대량 생산“은 곧 다가올 전 지구적 ’QA 위기(QA Crisis)’의 서막일 수 있다.</p>
<h2>7.  결론: 확률론적 품질 보증을 향하여</h2>
<p>AI와 LLM의 도입으로 인한 테스트 패러다임의 붕괴는 되돌릴 수 없는 기술적 흐름이다. 우리는 더 이상 소프트웨어의 완벽한 무결성을 수학적으로 보장할 수 없는 시대에 살고 있다. 이제 소프트웨어 품질 보증(QA)의 목표는 **‘무결점(Zero Defect)’**이라는 이상적인 신화에서 벗어나, **‘관리 가능한 위험(Manageable Risk)’**이라는 현실적인 목표로 수정되어야 한다.</p>
<p>미래의 소프트웨어 테스터는 단순히 버그를 찾는 사냥꾼이 아니라, 시스템의 불확실성을 관리하는 통계학자이자, AI의 윤리성과 안전성을 감시하는 철학자로 진화해야 한다. 우리는 메타모픽 테스트, 확률적 검증, 3계층 아키텍처와 같은 새로운 도구들을 적극적으로 도입해야 한다. 동시에, ’바이브 코딩’이 주는 달콤한 속도의 유혹 속에서도, AI가 작성한 코드를 인간이 이해하고 통제할 수 있는 능력을 상실하지 않도록 <strong>‘인간 중심의 검증(Human-in-the-loop Verification)’</strong> 체계를 견고히 유지해야 한다.</p>
<p>테스트 패러다임은 붕괴되었지만, 품질에 대한 우리의 책임은 사라지지 않았다. 오히려 그 책임은 코드의 문법을 넘어, AI가 사회에 미칠 영향력까지 포괄하는 더 넓고 무거운 영역으로 확장되고 있다. 이것이 바로 확률론적 소프트웨어 공학 시대가 우리에게 던지는 마지막 과제이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>(PDF) The Oracle Problem in Software Testing: A Survey, https://www.researchgate.net/publication/276255185_The_Oracle_Problem_in_Software_Testing_A_Survey</li>
<li>(PDF) Using machine learning to generate test oracles: a systematic …, https://www.researchgate.net/publication/354075021_Using_machine_learning_to_generate_test_oracles_a_systematic_literature_review</li>
<li>Challenges in Testing Large Language Model Based Software - arXiv, https://arxiv.org/html/2503.00481v2</li>
<li>Rethinking Testing for LLM Applications: Characteristics … - arXiv, https://arxiv.org/abs/2508.20737</li>
<li>Vibe Coding in Practice: Motivations, Challenges, and a … - arXiv, https://arxiv.org/html/2510.00328v1</li>
<li>Vibe Coding in Practice: Motivations, Challenges, and a Future, https://www.researchgate.net/publication/396094422_Vibe_Coding_in_Practice_Motivations_Challenges_and_a_Future_Outlook_-_a_Grey_Literature_Review</li>
<li>Metamorphic Testing of Large Language Models for Natural … - arXiv, https://arxiv.org/abs/2511.02108</li>
<li>Oracle Issues in Machine Learning and Where to Find Them, https://www.researchgate.net/publication/347455404_Oracle_Issues_in_Machine_Learning_and_Where_to_Find_Them</li>
<li>LLM evaluation metrics: A comprehensive guide for large language, https://wandb.ai/onlineinference/genai-research/reports/LLM-evaluation-metrics-A-comprehensive-guide-for-large-language-models–VmlldzoxMjU5ODA4NA</li>
<li>Software Testing with Large Language Models: An Interview Study …, https://www.researchgate.net/publication/400703542_Software_Testing_with_Large_Language_Models_An_Interview_Study_with_Practitioners</li>
<li>Can artificial intelligence solve the blockchain oracle problem, https://www.frontiersin.org/journals/blockchain/articles/10.3389/fbloc.2025.1682623/pdf</li>
<li>LLM Testing: A Complete Guide for Application Developers - Comet, https://www.comet.com/site/blog/llm-testing/</li>
<li>Defeating Nondeterminism in LLM Inference - Thinking Machines Lab, https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/</li>
<li>Defeating Non-Determinism in LLMs: Solving AI’s Reproducibility, https://www.flowhunt.io/blog/defeating-non-determinism-in-llms/</li>
<li>On the Flakiness of LLM-Generated Tests for Industrial and Open, https://arxiv.org/html/2601.08998v1</li>
<li>Flaky Tests - How to Get Rid of Them - testRigor, https://testrigor.com/blog/flaky-tests/</li>
<li>An Analysis of LLM Fine-Tuning and Few-Shot Learning for Flaky, https://www.sqrlab.ca/papers/ICST2025_FlakyXbert.pdf</li>
<li>Limits of Emergent Reasoning of Large Language Models in Agentic, https://arxiv.org/html/2510.15974v1</li>
<li>The Unpredictable Abilities Emerging From Large AI Models, https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316/</li>
<li>Unmasking the Flaws: Why AI-Generated Unit Tests Fall Short in, https://shekhar14.medium.com/unmasking-the-flaws-why-ai-generated-unit-tests-fall-short-in-real-codebases-71e394581a8e</li>
<li>Why LLMs Aren’t Scientists Yet: Lessons from Four Autonomous, https://arxiv.org/html/2601.03315v1</li>
<li>From autonomous to accountable: A framework for AI Agent testing, https://toloka.ai/blog/from-autonomous-to-accountable-a-framework-for-ai-agent-testing/</li>
<li>Handling Flaky Tests in LLM-powered Applications - Semaphore CI, https://semaphore.io/blog/llms-flaky-tests</li>
<li>A Comparative Study of the AI-Driven Software Development, https://www.arxiv.org/pdf/2601.22667</li>
<li>(PDF) Metamorphic Testing of Deep Code Models: A Systematic, https://www.researchgate.net/publication/394121465_Metamorphic_Testing_of_Deep_Code_Models_A_Systematic_Literature_Review</li>
<li>Metamorphic Testing of Large Language Models for Natural, https://valerio-terragni.github.io/assets/pdf/cho-icsme-2025.pdf</li>
<li>Validating LLM-Generated Programs with Metamorphic Prompt, https://www.researchgate.net/publication/381318597_Validating_LLM-Generated_Programs_with_Metamorphic_Prompt_Testing</li>
<li>LLM evaluation metrics and methods - Evidently AI, https://www.evidentlyai.com/llm-guide/llm-evaluation-metrics</li>
<li>Human-Written vs. AI-Generated Code: A Large-Scale Study of, https://www.researchgate.net/publication/397594524_Human-Written_vs_AI-Generated_Code_A_Large-Scale_Study_of_Defects_Vulnerabilities_and_Complexity</li>
<li>A Review of Large Language Models for Automated Test Case, https://www.mdpi.com/2504-4990/7/3/97</li>
<li>Software Testing with Large Language Models: An Interview Study, https://arxiv.org/html/2510.17164v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>