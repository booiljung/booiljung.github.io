<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2.4.5 프롬프트 민감도에 따른 결과의 비일관성(Inconsistency) 문제</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2.4.5 프롬프트 민감도에 따른 결과의 비일관성(Inconsistency) 문제</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 2. 소프트웨어 테스트에서의 오라클(Oracle) 개념과 AI 시대의 역할</a> / <a href="index.html">2.4 AI 및 LLM(거대 언어 모델) 도입으로 인한 테스트 패러다임의 붕괴</a> / <span>2.4.5 프롬프트 민감도에 따른 결과의 비일관성(Inconsistency) 문제</span></nav>
                </div>
            </header>
            <article>
                <h1>2.4.5 프롬프트 민감도에 따른 결과의 비일관성(Inconsistency) 문제</h1>
<p>거대 언어 모델(LLM)을 소프트웨어 테스트와 자동화된 개발 파이프라인의 핵심 검증자(Oracle)로 도입할 때 직면하는 가장 근본적이고 파괴적인 기술적 장애물은 프롬프트 민감도(Prompt Sensitivity)로 인해 촉발되는 출력 결과의 비일관성(Inconsistency)이다. 전통적인 소프트웨어 엔지니어링 환경에서 테스트 오라클은 결정론적(Deterministic)이어야 한다. 즉, 동일한 입력 상태와 환경 조건이 주어지면 오라클의 판별 결과는 수학적 공식처럼 언제나 동일한 참(True) 또는 거짓(False)을 반환해야 시스템의 무결성을 보장할 수 있다. 그러나 자가 회귀(Autoregressive) 특성을 지닌 확률적(Stochastic) 언어 모델에서는 입력 프롬프트의 미세한 형태적 변화, 토큰의 배열 순서, 심지어 의미가 동일한 패러프레이징(Paraphrasing)조차 모델 내부의 로짓(Logits) 분포를 변형시키며 출력 결과의 극적인 요동을 초래한다.</p>
<p>이러한 프롬프트 민감도는 단순한 사용자의 입력 실수가 아니라, 모델이 훈련 과정에서 우회적이고 표면적인 특징(Spurious Features)에 과적합되었거나, 텍스트의 본질적 의미(Semantics)와 문법적 구조(Syntax)를 완벽하게 분리하여 추상화하지 못하는 어텐션 매커니즘(Attention Mechanism)의 내재적 한계에서 기인한다. 소프트웨어 품질 보증(QA)의 맥락에서, 검증을 수행하는 오라클 자체가 프롬프트의 띄어쓰기나 긍정/부정의 논리적 전환에 따라 참/거짓의 판별을 뒤집는다면, 자동화된 회귀 테스트(Regression Testing)와 CI/CD 파이프라인의 신뢰성은 근본적으로 붕괴될 수밖에 없다. 따라서 프롬프트 민감도와 그로 인한 비일관성 문제를 정확히 분류하고 정량화하며, 이를 통제할 수 있는 공학적 아키텍처를 설계하는 것은 AI 기반 소프트웨어 개발의 성패를 가르는 핵심 과제이다.</p>
<h2>1.  프롬프트 민감도와 비일관성의 다차원적 분류 체계 (Taxonomy of Inconsistency)</h2>
<p>최근의 자연어 처리 및 인공지능 신뢰성 연구들은 프롬프트의 변동이나 모델 내부의 노이즈로 인해 발생하는 비일관성을 단일한 무작위성 현상으로 치부하지 않고, 발생 기제와 논리적 오류의 형태에 따라 다차원적인 문제로 규정하여 분류 체계를 확립하고 있다. 이러한 세밀한 분류는 소프트웨어 테스트 환경에서 각기 다른 유형의 오류를 방어하기 위한 맞춤형 오라클 설계의 이론적 기반이 된다.</p>
<h3>1.1  인스턴스 내 불안정성 (Intra-Instance Inconsistency)</h3>
<p>인스턴스 내 불안정성은 입력 프롬프트가 문자 단위까지 완벽히 동일하고, 모델의 생성 온도(Temperature)를 가장 보수적인 결정론적 상태(<span class="math math-inline">T=0</span>)로 설정했음에도 불구하고 여러 번의 반복 샘플링 과정에서 출력 결과가 달라지는 현상을 의미한다. 이는 소프트웨어 엔지니어들이 가장 당혹스러워하는 유형의 비일관성이다. 이 현상의 근본 원인은 모델 가중치의 비결정성, 하드웨어 수준의 부동소수점 연산 정밀도 오차, 병렬 처리 과정에서의 스레드 스케줄링 차이, 그리고 모델 내부에 잔존하는 확률적 디코딩 노이즈(Flipping noise)에 있다.</p>
<p>경험적 연구에 따르면, 법률적 의사 결정이나 판결 예측과 같이 고도의 논리적 일관성이 요구되는 도메인에서 이 문제는 심각한 수준으로 나타난다. 500개의 복잡한 법률 질문을 대상으로 한 반복 실행 연구(Repeated runs protocol)에서 GPT-4o는 43%, Claude-3.5는 10.6%, Gemini-1.5는 무려 50.4%의 인스턴스 내 불안정성 비율을 보였다. 동일한 테스트 시나리오에 대해 모델이 20번의 실행 중 승소와 패소 예측을 무작위로 뒤집는다면, 이는 자동화된 법률 자문 소프트웨어나 규제 준수(Compliance) 검증 오라클로서 현재의 LLM을 단독으로 사용하는 것이 불가능함을 시사한다.</p>
<h3>1.2  패러프레이징 비일관성 (Paraphrase Inconsistency)</h3>
<p>패러프레이징 비일관성은 프롬프트의 표면적 형태(Surface form)만 변경되고 내포된 의미론적(Semantic) 가치는 완전히 보존되었음에도 불구하고 모델의 응답이 근본적으로 달라지는 현상이다. 소프트웨어 개발 요구사항이나 테스트 케이스 시나리오를 작성할 때, “사용자가 로그인에 실패해야 한다“를 “사용자의 로그인 시도가 거부되어야 한다“로 변경하는 것은 인간 엔지니어에게는 완전히 동일한 의미로 해석된다. 그러나 LLM에게 이러한 동의어 교체나 문장 구조의 재배열은 언어 모델의 목적 함수 내에서 어휘 로짓(Vocabulary logits)의 확률 분포를 암묵적으로 변경시켜, 전혀 다른 추론 경로를 활성화하게 만든다.</p>
<p>논문 <em>What Did I Do Wrong? Quantifying LLMs’ Sensitivity and Consistency to Prompt Engineering</em>에서는 패러프레이징에 따른 민감도(Sensitivity)와 일관성(Consistency)을 엄격히 구분한다. 민감도는 동일한 인스턴스의 프롬프트를 재구성했을 때 예측값이 얼마나 요동치는지를 측정하는 반면, 일관성은 동일한 클래스에 속하는 유사한 인스턴스들 사이에서 프롬프트 변형이 가해졌을 때 예측이 얼마나 안정적으로 유지되는지를 측정한다. 모델이 본질적인 작업의 규칙을 이해하지 못하고 표면적인 단어의 빈도나 배열이라는 가짜 특징(Spurious features)에 의존하여 결론을 내릴 때 패러프레이징 비일관성이 극대화된다.</p>
<h3>1.3  프롬프트-역전 비일관성 (Prompt-Reverse Inconsistency, PRIN)</h3>
<p>프롬프트-역전 비일관성(PRIN)은 LLM이 진정한 논리적 대칭성(Logical Symmetry)과 부정(Negation)의 개념을 내재화하지 못했음을 증명하는 현상이다. 이는 모델에게 직접적인 질문(“주어진 선택지 중 올바른 답을 모두 고르시오”)을 던졌을 때의 정답 집합과, 그와 논리적으로 완벽하게 상보적인 역질문(“주어진 선택지 중 틀린 답을 모두 고르시오”)을 던졌을 때의 오답 집합이 대칭을 이루지 못하고 모순되는 결과를 도출하는 상태를 말한다.</p>
<p>논문 <em>Prompt-Reverse Inconsistency: LLM Self-Inconsistency Beyond Generative Randomness and Prompt Paraphrasing</em>의 연구자들은 이러한 현상이 단순한 생성적 무작위성이나 패러프레이징에 의한 것이 아니라, 모델 내부의 논리적 결함에서 비롯된 독립적이고 견고한 실패 모드(Robust failure mode)임을 규명했다. 수학 및 논리 벤치마크 테스트 결과, GPT-4는 38.6% 이상의 PRIN 오류율을 보였으며, 다수의 오픈 소스 대형 모델들은 60%를 초과하는 심각한 논리적 붕괴를 나타냈다. 소프트웨어 테스트에서 검증 오라클이 긍정적 단언(Positive assertion)과 부정적 단언(Negative assertion) 사이에서 일관성을 유지하지 못한다면, 복잡한 비즈니스 로직의 경계값 테스트(Boundary testing)는 신뢰성을 상실하게 된다.</p>
<h3>1.4  입력 순서 편향 (The Order Effect)</h3>
<p>입력 순서 편향은 프롬프트 내에 제공되는 선택지, 문맥 정보(Context), 추론의 전제(Premises), 또는 소수 샷 예제(Few-shot examples)의 배열 순서를 무작위로 섞었을 때 모델의 출력 정확도가 급격히 하락하거나 특정 순서에 편향되는 현상을 말한다.</p>
<p>논문 <em>The Order Effect: Investigating Prompt Sensitivity in Closed-Source LLMs</em>에 따르면, 14,042개의 객관식 및 논리 추론 예제로 구성된 데이터셋에서 입력 순서를 무작위로 섞었을 때 여러 최첨단 모델들의 정확도가 현저히 감소하는 것이 실증적으로 확인되었다. 특히, 전제 조건이 여러 개 주어지는 전건 긍정(Modus Ponens) 추론 문제의 경우, 전제들을 무작위로 재배열하면 단순히 역순으로 배열했을 때보다 정확도가 기하급수적으로 하락하는 현상이 관찰되었다.</p>
<p>소프트웨어 엔지니어링 도메인에서의 연구 결과는 더욱 충격적이다. 호출 그래프(CallGraph)나 종속성 그래프(DepGraph)를 LLM에게 프롬프트로 제공하여 결함 위치를 추적(Fault Localization)하는 작업에서, 입력 순서와 분할 방식에 따라 모델의 결함 탐지 성능이 극단적으로 요동쳤다. 변수 이름의 난독화나 변경은 성능에 미치는 영향이 4~9%에 불과했지만, ’완벽한 순서(Perfect-Order)’와 ‘최악의 순서(Worst-Order)’ 사이의 성능 격차는 압도적이었다. 종속성 그래프 기반 순서 지정은 Top-1 정확도 48%를 달성한 반면, 단순한 깊이 우선 탐색(DFS) 기반 순서 지정은 Top-10 정확도 70.1%를 기록했다. 이는 LLM을 사용하여 소스 코드 결함을 탐지하는 정적 분석 오라클을 구축할 때, 코드를 프롬프트에 주입하는 순서 자체가 오라클의 정확도를 결정짓는 가장 지배적인 변수임을 시사한다.</p>
<h2>2.  프롬프트 민감도 및 비일관성의 정량적 측정 지표와 수식 체계</h2>
<p>프롬프트 민감도 문제를 통제 가능한 공학의 영역으로 끌어들이기 위해서는, 그 취약성을 객관적으로 정량화할 수 있는 측정 지표(Metrics)가 반드시 필요하다. 학계에서는 직관적 평가를 넘어 모델 내부의 메커니즘을 반영하는 다양하고 정교한 수학적 지표들을 제안하고 있다.</p>
<table><thead><tr><th><strong>지표명</strong></th><th><strong>목적 및 특징</strong></th><th><strong>핵심 수식 및 계산 논리</strong></th></tr></thead><tbody>
<tr><td><strong>PSS</strong> (PromptSensiScore)</td><td>동일 인스턴스의 프롬프트 변형 간 출력 성능 지표의 절대적 차이 평균</td><td><span class="math math-inline">\frac{1}{N} \sum_{i=1}^{N} \frac{1}{C(\vert P \vert, 2)} \sum \vert Y(P_m) - Y(P_n) \vert</span></td></tr>
<tr><td><strong>PRIN</strong> Score</td><td>직접 프롬프트와 역 프롬프트의 논리적 대칭성 붕괴 및 모순 정도 측정</td><td><span class="math math-inline">1.0 - F_1(A_{direct}, A \setminus A_{reverse})</span></td></tr>
<tr><td><strong>IPS</strong> (Interaction-based Prompt Sensitivity)</td><td>게임 이론 기반으로 하위 상호작용(Low-order interactions)의 불안정성 분산 정량화</td><td>변수 조합 간 비선형적 상호작용의 분산 측정</td></tr>
<tr><td><strong><span class="math math-inline">I_p(M,Q)</span></strong> (Intra-Instance Instability)</td><td>동일 프롬프트 반복 쿼리(Zero-temperature) 시의 출력 변동성 측정</td><td><span class="math math-inline">1 - S(M,Q)</span> (단, S는 특정 답변의 지배율)</td></tr>
<tr><td><strong>AUC-E</strong> (Area Under Curve of Elasticity)</td><td>프롬프트 변형 거리에 따른 응답 탄력성 및 모델 성능의 지속적 변화 곡선 면적</td><td><span class="math math-inline">\int \mathcal{E}(d) \approx \sum \Delta \text{Pass}(d)</span></td></tr>
</tbody></table>
<h3>2.1  PromptSensiScore (PSS)의 인스턴스 레벨 분석</h3>
<p>논문 <em>ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs</em>에서 제안된 **PSS (PromptSensiScore)**는 데이터셋 전체의 평균 성능 변화가 아닌, 개별 ‘인스턴스(Instance)’ 수준에서 모델이 프롬프트 변형에 얼마나 민감하게 반응하는지를 측정하는 혁신적인 지표이다. 위 표의 PSS 수식에서 <span class="math math-inline">Y(P_m)</span>은 특정 프롬프트 <span class="math math-inline">P_m</span>이 주어졌을 때 모델 출력의 정확도 또는 품질 점수(0~1 사이)를 의미하며, 명시적인 정답지(Ground truth)가 없는 생성형 작업의 경우 주관적 품질 평가 점수로 대체될 수 있다. <span class="math math-inline">C(\vert P \vert, 2)</span>는 단일 인스턴스 내에서 생성 가능한 프롬프트 쌍의 총 조합 수를 나타내며, <span class="math math-inline">N</span>은 전체 데이터셋의 인스턴스 수이다.</p>
<p>ProSA 프레임워크를 활용하여 4개의 데이터셋에 걸쳐 8개의 대형 언어 모델을 분석한 결과, 파라미터 규모가 큰 모델일수록 프롬프트 민감도에 대한 강건성(Robustness)이 뛰어남이 입증되었다. 특히 Llama3-70B-Instruct 모델은 가장 낮은 PSS 수치를 기록하며 뛰어난 안정성을 보였다. 주목할 만한 점은, 제로 샷(Zero-shot) 환경에서 원샷(One-shot) 또는 퓨샷(Few-shot) 환경으로 전환될 때 PSS 수치가 급격히 하락한다는 것이다. 이는 프롬프트 내에 제공된 예제들이 모델의 디코딩 신뢰도(Decoding confidence)를 상승시키고, 이 높아진 신뢰도가 지시어의 의미론적 변화에 대한 저항력을 강화하는 닻(Anchor) 역할을 수행하기 때문이다.</p>
<h3>2.2  PRIN Score를 통한 논리적 모순 정량화</h3>
<p><strong>PRIN Score</strong>는 프롬프트-역전 비일관성을 수학적으로 포착하는 정밀한 지표이다. 직관적으로 직접 프롬프트의 정답 집합 <span class="math math-inline">A_{direct}</span>와 역 프롬프트의 정답 집합(틀린 것을 고른 결과) <span class="math math-inline">A_{reverse}</span>가 주어졌을 때, 왜 단순히 두 집합의 F1 유사도인 <span class="math math-inline">F_1(A_{direct}, A_{reverse})</span>를 구하지 않는가에 대한 명확한 수학적 이유가 존재한다.</p>
<p>만약 두 집합이 상호 배타적이어서 완벽히 상보적(Complementary)인 것처럼 보일지라도, 모델의 환각이나 누락으로 인해 전체 후보 집합 <span class="math math-inline">A</span>를 모두 포괄하지 못하는 문제가 발생할 수 있다. 예를 들어, 전체 선택지가 <span class="math math-inline">{a_1, a_2, a_3, a_4}</span>일 때 모델이 직접 프롬프트에서 <span class="math math-inline">A_{direct} = {a_1, a_2}</span>를 선택하고, 역 프롬프트에서 <span class="math math-inline">A_{reverse} = {a_3}</span>만 선택하여 <span class="math math-inline">a_4</span>를 판단 누락하는 경우, 단순 유사도 수식으로는 이 결함을 정확히 포착할 수 없다. 따라서 전체 정답 풀 <span class="math math-inline">A</span>에서 역 프롬프트의 결과 <span class="math math-inline">A_{reverse}</span>를 차집합으로 빼낸 논리적 여집합 <span class="math math-inline">A \setminus A_{reverse}</span>를 산출한 뒤, 이를 <span class="math math-inline">A_{direct}</span>와 비교하여 F1 스코어를 계산한다. 이 스코어를 1에서 차감한 최종 PRIN Score는 완벽히 일관될 경우 0, 완전히 모순될 경우 1의 값을 가지게 되어, 모델의 내재적 논리 결함을 적나라하게 드러낸다.</p>
<h3>2.3  게임 이론 기반의 상호작용 민감도 (IPS)</h3>
<p><strong>IPS (Interaction-based Prompt Sensitivity)</strong> 지표는 최종 출력 텍스트의 변화 유무만을 관찰하는 기존 거시적 지표의 한계를 극복하기 위해 제안되었다. 동일한 프롬프트 입력에 대해 모델이 겉으로는 동일한 정답을 출력했더라도, 그 내부에서는 전혀 다른 경로의 어텐션 상호작용을 거쳤을 수 있다. 논문 <em>Interaction-based Prompt Sensitivity</em>의 저자들은 모델의 최종 출력 점수를 게임 이론에 입각하여 개별 입력 변수들 간의 비선형적 상호작용 집합으로 해체(Disentangle)했다.</p>
<p>연구 결과, 프롬프트에 가해진 의미적으로 무관련된 미세한 변화(Semantically irrelevant changes)조차 최종 출력은 유지시킬지언정 모델 내부의 상호작용 지형도를 극적으로 요동치게 만든다는 사실을 발견했다. 50개의 오픈 소스 LLM에 IPS 지표를 적용하여 분석한 결과, 지도 미세 조정(SFT), 파라미터 규모의 확장, 밀집(Dense) 아키텍처의 채택, 그리고 퓨샷 러닝(Few-shot learning)이라는 네 가지 요소가 프롬프트 민감도를 감소시키는 공통된 메커니즘을 공유함을 밝혀냈다. 이 네 가지 방법론은 모두 적은 수의 입력 변수만이 관여하는 ’하위 차수 상호작용(Low-order interactions)’의 민감도를 억제함으로써 전체 시스템의 추론 안정성을 향상시킨다. 이는 오라클을 위한 프롬프트를 설계할 때, 입력 변수를 간소화하고 명시적으로 결합하는 것이 왜 안정성에 기여하는지를 증명하는 강력한 수학적 근거이다.</p>
<h2>3.  소프트웨어 테스트 오라클 생성을 저해하는 실증적 한계와 오라클 문제(The Oracle Problem)</h2>
<p>이러한 프롬프트 민감도와 논리적 비일관성은 소프트웨어 공학의 난제 중 하나인 ’오라클 문제(The Oracle Problem)’를 생성형 AI 시대에 맞게 변이시키고 극단적으로 심화시킨다. Barr 등이 저술한 소프트웨어 테스팅 분야의 기념비적 논문 <em>The Oracle Problem in Software Testing: A Survey</em>에 따르면, 테스트 오라클은 시스템의 관찰된 동작이 개발자의 의도에 부합하는지 여부를 판별하는 궁극적인 진실의 공급원(Ground Truth) 역할을 수행해야 한다. 전통적으로는 테스트 엔지니어가 직접 결정론적인 주장(Assertion)을 작성하거나 Randoop, Evosuite와 같은 자동화 도구를 사용하여 암묵적 규칙 기반의 회귀 오라클을 생성해왔다.</p>
<p>그러나 최근 소프트웨어 개발 사이클의 생산성 극대화를 위해 LLM을 테스트 오라클 생성기나 코드를 판별하는 평가자(LLM-as-a-Judge)로 투입하려는 시도가 급증하고 있다. 문제는 여기서 발생한다. LLM은 본질적으로 언어적 통계 확률을 극대화하는 모델이지 프로그램의 시맨틱(Semantics)을 수학적으로 증명하는 도구가 아니기 때문이다.</p>
<h3>3.1  기대 동작(Expected)이 아닌 실제 동작(Actual) 포착의 편향성</h3>
<p>최근 진행된 <em>Do LLMs generate test oracles that capture the actual or the expected program behaviour?</em> 논문의 통제된 실험에 따르면, 24개의 오픈 소스 자바(Java) 리포지토리를 대상으로 LLM의 오라클 생성 능력을 평가한 결과 매우 치명적인 결함이 발견되었다. LLM 기반 오라클 생성 기법들은 개발자가 의도한 소프트웨어의 ’기대 동작(Expected behavior)’을 추론하여 오라클을 작성하기보다는, 현재 소스 코드에 구현되어 있는 결함 있는 ’실제 동작(Actual behavior)’을 무비판적으로 수용하여 회귀 오라클(Regression oracle)을 생성하는 편향을 강하게 보였다.</p>
<p>테스트 오라클의 궁극적인 존재 이유는 코드에 내재된 버그가 실행되었을 때 이를 비정상으로 판별하여 테스트를 실패(Fail)시키는 것이다. 그러나 프롬프트에 포함된 소스 코드의 문맥에 과적합된 LLM이 버그가 포함된 현재 코드의 동작을 ’정상적인 기준’으로 삼아 오라클을 작성해 버린다면, 테스트는 항상 통과(Pass)하게 되고 버그는 영구적으로 시스템 내부에 고착화된다. 실험 결과, LLM은 주어진 오라클이 올바른지 분류(Classification)하는 작업보다 오라클을 직접 생성(Generation)하는 작업에 더 능숙했지만, 코드 내에 의미 있는 변수명이나 명확한 테스트 이름이 프롬프트로 주어지지 않을 경우 그 정확도는 심각하게 훼손되었다. 이는 LLM 오라클이 로직의 정합성보다는 인간이 부여한 자연어 토큰(변수명 등)의 언어적 힌트에 전적으로 의존하고 있음을 시사한다.</p>
<h3>3.2  프롬프트 컨텍스트의 양적 변동에 따른 오라클 품질의 변동성</h3>
<p>프롬프트 민감도는 LLM에게 제공되는 컨텍스트의 범위에 따라서도 극명하게 나타난다. 논문 <em>Understanding LLM-Driven Test Oracle Generation</em>과 실증 연구 논문 <em>An Empirical Study of LLM-Generated Test Oracles</em>에서는 135개의 Java 프로젝트에서 추출한 13,866개의 테스트 오라클 데이터셋을 바탕으로 프롬프팅 전략이 오라클 생성 품질에 미치는 영향을 분석했다. 주목할 만한 점은, 사용된 데이터셋이 LLM의 학습 컷오프(Cut-off) 날짜 이후에 생성된 편향되지 않은(Unbiased) 데이터라는 것이다.</p>
<p>실험 결과, LLM이 생성한 테스트 오라클의 돌연변이 스코어(Mutation score)는 평균 43%로, 인간 개발자가 작성한 오라클의 45%와 유사한 수준의 표면적 성능을 보였다. 그러나 프롬프트에 제공되는 컨텍스트의 수준에 따라 정확도는 극적인 변동을 보였다. 오직 테스트 접두어(Test prefix)나 테스트할 단일 메서드(MUT, Method Under Test)만을 프롬프트로 제공했을 때의 성능 변화는 미미했던 반면, 테스트 대상 클래스 전체(CUT, Class Under Test)의 문맥을 함께 주입했을 때 오라클 생성의 정확도가 53.64%까지 유의미하게 상승했다.</p>
<p>이는 프롬프트 내에 포함된 배경 지식의 배치가 오라클의 품질을 좌우하는 결정적 요인임을 보여준다. CUT 컨텍스트의 부재는 LLM이 코드의 전역적 상태나 클래스 불변성(Class Invariants)을 이해하는 능력을 박탈하며, 결국 지엽적이고 불안정한 검증 로직만을 생성하게 만든다. 하지만 역설적으로, 무분별하게 과도한 코드 컨텍스트를 주입하는 것은 오히려 ‘길이의 저주(Lost-in-the-middle)’ 현상이나 어텐션 분산을 유발하여 프롬프트 민감도를 가중시킬 위험이 있다.</p>
<h3>3.3  CI/CD 환경에서의 메타-오라클 문제 (Meta-Oracle Problem)</h3>
<p>이러한 특성들은 자동화된 배포 파이프라인에서 심각한 병목을 유발한다. 실무 환경에서 LLM을 실시간 평가 에이전트나 코딩 어시스턴트의 출력 검증에 사용할 경우, 프롬프트의 미세한 파라미터 변화(Temperature, Top-P)나 배치 사이즈의 변동만으로 어제 통과되었던 단위 테스트가 오늘 실패(Flaky tests)로 판별되는 상황이 발생한다.</p>
<p>테스트 대상(Target System)의 복잡성에 대응하기 위해 도입한 AI 오라클 자체가 확률적 무작위성을 띠게 됨으로써, 개발팀은 “오류를 보고한 테스트 오라클 자체가 올바르게 작동하고 있는지 어떻게 증명할 것인가?“라는 무한 퇴행적인 ’메타-오라클 문제(Meta-oracle problem)’에 직면하게 된다. 이는 AI 기반 테스트 자동화 도구가 단순히 프롬프트 엔지니어링의 최적화를 넘어서, 오라클의 출력 자체를 확정적인 시스템 경계 내에 가두는 견고한 방어 체계를 필요로 함을 의미한다.</p>
<h2>4.  AI를 사용한 소프트웨어 개발에서 결정론적 정답지를 제공하는 오라클과 실전 예제</h2>
<p>확률론적 언어 모델의 근본적 한계를 극복하고 AI 시스템을 엔터프라이즈급 소프트웨어 개발 및 테스트 파이프라인에 안전하게 통합하기 위해서는, 모델 내부의 난수성을 외부에서 결정론적 정답지(Deterministic Ground Truth)로 제어하고 강제 검증하는 아키텍처 도입이 필수적이다. 단순한 “프롬프트를 잘 쓰는 법(Prompt Engineering)“의 차원을 넘어, 소프트웨어 아키텍처 수준에서 프롬프트 민감도로 촉발되는 비일관성을 물리적으로 차단하고 오라클의 무결성을 보장하기 위해 실무에서 적용되는 세 가지 수준의 결정론적 오라클 구성 예제를 심층 분석한다.</p>
<h3>4.1  실전 예제 1: 연산의 비결정성 통제를 위한 배치 불변 처리(Batch-Invariant Processing) 오라클 아키텍처</h3>
<p>소프트웨어 개발 환경에서 가장 치명적이고 재현 불가능한 비일관성은, 앞서 언급한 ’인스턴스 내 불안정성(<span class="math math-inline">I_p</span>)’이다. 입력 프롬프트를 바이트 단위까지 완벽하게 동일하게 유지하고 온도를 0으로 고정했음에도 출력이 변경되는 현상은, 다수의 사용자 요청을 동시에 병렬 처리(Batch processing)하는 과정에서 발생하는 동적 연산 그래프 최적화와 부동소수점 정밀도 누적 오차에 기인한다.</p>
<p>이를 원천적으로 해결하기 위해 Thinking Machines 연구팀의 사례와 같이 ’배치 불변 처리(Batch-Invariant Processing)’를 오라클 시스템의 런타임 전제 조건으로 강제하는 공학적 접근이 활용된다. 현대 AI 훈련 및 추론은 효율성을 위해 배치 크기에 따라 연산 순서를 동적으로 변경하지만, 이 아키텍처에서는 효율성을 희생하더라도 일관성을 확보하는 쪽을 선택한다.</p>
<p>이들은 AI 모델의 추론을 구성하는 세 가지 핵심 수학적 연산을 근본적으로 재설계했다. 첫째, 신경망 내부의 숫자 범위를 안정적으로 유지하는 정규화(Normalization) 과정의 부동소수점 처리 순서를 고정했다. 둘째, AI 모델의 수학적 엔진인 행렬 곱셈(Matrix multiplication)의 타일링(Tiling) 및 누적 합산 순서를 배치 크기와 독립적으로 엄격히 통제했다. 셋째, 프롬프트 토큰 간의 관련성을 가중치로 계산하는 어텐션 매커니즘(Attention mechanisms)의 실행 순서를 결정론적으로 잠갔다.</p>
<p>그 결과, 100% 동일한 출력을 보장하는 ’불변성’을 확보했다. Feynman 프롬프트라는 동일한 입력에 대해 기존 시스템이 80번의 시도 동안 각기 다른 텍스트 변형을 산출했던 반면, 배치 불변 처리 시스템은 80번 모두 단 하나의 토큰조차 다르지 않은 완벽히 동일한 응답을 생성했다. 비록 병렬화의 이점을 일부 포기하여 시스템 전체의 처리 성능(Throughput)이 약 60% 느려지는 심각한 페널티(Trade-off)가 발생했지만, 금융 시스템의 트랜잭션 검증이나 규제 산업의 컴플라이언스 체크를 수행하는 테스트 오라클로서는 최적의 솔루션이다. 개발팀은 이 완벽히 재현 가능한 배치 불변 시스템 자체를 ’결정론적 베이스라인 오라클(Deterministic Baseline Oracle)’로 지정하고, 이 오라클이 출력한 결과를 Golden Truth 데이터셋으로 삼아 더 가볍고 빠른 경량 모델들의 출력 변동성을 회귀 테스트(Regression Test)하는 구조를 구축할 수 있다.</p>
<h3>4.2  실전 예제 2: 실시간 런타임 가드레일(Runtime Guardrails)과 JSON Schema 기반 데이터 정합성 오라클</h3>
<p>CI/CD 파이프라인이나 실제 프로덕션 환경에 배포된 LLM 오라클의 프롬프트 민감도를 통제하는 가장 실용적이고 강력한 기법은, 자연어 기반의 모호한 생성형 응답을 소프트웨어가 파싱(Parsing)하고 검증할 수 있는 엄격한 데이터 구조로 강제 캡슐화(Encapsulation)하는 것이다. 프롬프트 체이닝(Prompt Chaining)이나 퓨샷(Few-shot) 예제를 아무리 정교하게 주입하더라도 결국 모델이 뱉어내는 것은 문자열(String)에 불과하므로, 프로그래밍 로직에 의한 기계적 검증 오라클 구축이 요원해진다.</p>
<p>실무에서는 LLM의 응답 형식을 엄격한 JSON Schema로 강제(Structured Outputs)하고, 런타임에 이 스키마 규칙을 벗어나는 모든 변형을 오라클 레벨에서 에러로 차단하는 파이프라인을 구축한다. 예를 들어, 소스 코드를 리뷰하여 보안 취약점을 찾는 에이전트 시스템을 위한 자동화 테스트 오라클을 구성한다고 가정해 보자. 단순히 “이 함수에 SQL 인젝션 취약점이 있는가?“라는 프롬프트만 던진다면, 모델은 “아마도 존재합니다”, “없습니다”, “Prepared Statement를 사용해야 합니다” 등 템플릿화 불가능한 비일관적 텍스트를 반환할 것이다. 대신, 프롬프트 내부의 시스템 지시어에 다음과 같이 구조화된 JSON 반환을 강제하는 명세를 주입한다.</p>
<pre><code class="language-JSON">{
  "vulnerability_found": true,
  "vulnerability_type": "SQL_Injection",
  "cwe_id": "CWE-89",
  "confidence_score": 0.98,
  "affected_lines": 
}
</code></pre>
<p>이제 소프트웨어 테스트 오라클은 LLM의 유려한 자연어 설명을 읽어낼 필요가 없다. 오직 반환된 페이로드 데이터가 사전에 정의된 결정론적 타입 스키마(Data schema validation)를 정확히 준수했는지, 그리고 <code>confidence_score</code>가 임계값(예: 0.9)을 초과했는지만을 결정론적 프로그래밍 로직(if-else 조건문)으로 엄격히 평가한다.</p>
<p>이러한 스키마 강제화는 외부 방어벽인 실시간 런타임 가드레일(Runtime Guardrails)과 결합될 때 그 효과가 극대화된다. Galileo의 Agent Protect API 사례나 Trend Micro의 ZTSA 시스템과 같은 런타임 방어망은 사용자나 타 시스템이 입력한 프롬프트가 언어 모델의 가중치 네트워크에 도달하기도 전에 작동한다. 이들은 프롬프트의 의미적 의도를 1밀리초(ms) 단위로 분석하여 프롬프트 인젝션 패턴, 환각 유도 질의, 내부 메시지 포이즈닝(Message poisoning) 위험성을 결정론적 필터 규칙과 가벼운 고속 평가자(Luna-2 evaluators 등)를 통해 스캔하고 즉각적으로 차단한다. 결과적으로 확률적 비결정성을 내포한 AI 모델의 코어 로직을 결정론적인 입력 필터와 출력 스키마 검사기 사이에 샌드위치 구조로 완벽히 격리함으로써, 시스템 외부에서 바라보았을 때는 완벽하게 일관성을 유지하는 튼튼한 거시적 오라클을 완성하게 된다.</p>
<h3>4.3  실전 예제 3: 백투백 테스트(Back-to-Back Testing) 및 RAG 기반 그라운드 트루스 강제 주입 메커니즘</h3>
<p>전통적인 소프트웨어 테스트의 정답(Expected result)을 인간이 사전에 모든 엣지 케이스(Edge case)에 대해 수학적으로 정의할 수 없다는 본질적인 ’테스트 오라클 문제’를 해결하기 위해, 검증된 참조 대상 모델과 외부 지식 소스를 결합하는 백투백(Back-to-Back) 교차 검증 기법이 널리 사용된다.</p>
<p>백투백 테스팅 환경에서는 완전히 동일한 프롬프트와 입력값을 두 개의 분리된 시스템(예: 성능이 검증된 GPT-4 등의 거대 교사 모델과, 사내 데이터로 미세 조정된 빠르고 저렴한 8B 파라미터 소형 모델)에 동시에 주입한다. 이때 신뢰성이 입증된 교사 모델의 출력을 임시 정답지(Pseudo-ground truth)이자 참조 오라클로 삼아, 새롭게 배포하려는 소형 모델의 출력 변동성과 논리적 일관성을 비교 검증한다. 만약 동일한 상황에서 소형 모델이 프롬프트의 약간의 패러프레이징에 흔들려 교사 모델과 다른 결과(예: Label Binding Inconsistency)를 뱉어낸다면 파이프라인은 실패를 선언한다.</p>
<p>더 나아가, 프롬프트 민감도에 의해 모델이 환각의 세계로 빠져드는 것을 막기 위해, 프롬프트 내에 결정론적 사실 관계를 강제로 주입하는 RAG(Retrieval-Augmented Generation) 파이프라인이 검증 오라클 구성에 깊숙이 통합된다. 단순한 RAG 챗봇이 아닌 테스트를 위한 엄격한 제어 장치로서의 RAG 활용이다. Tolgahan Bardakci 등의 <em>Test Amplification for REST APIs</em> 연구에 따르면, API 단위 테스트 케이스를 생성할 때 단순한 텍스트 프롬프트만 제공한 경우 모델은 유효하지 않거나 폐기된 엔드포인트에 대한 엉터리 테스트 코드를 다수 환각으로 만들어냈다.</p>
<p>하지만 REST API 명세서(Swagger/OpenAPI Spec)라는 논리적으로 확정된 문서(지식 소스)를 프롬프트 문맥에 RAG 형태로 결합하여 명확한 경계값(Boundary values)과 파라미터 제약 사항을 컨텍스트에 강제 주입했을 때, 모델의 환각은 억제되었고 코드 커버리지는 비약적으로 상승했다. RAG가 결합된 소프트웨어 테스트 파이프라인에서 오라클의 존재 이유는 모델의 창의성이나 유창한 언어 능력을 평가하는 데 있지 않다. 오직 모델의 생성물이 제공된 ’결정론적 지식 베이스(명세, 코드 스니펫)’에서 이탈하여 독단적인 주장을 펼치는지(Faithfulness Failure), 코드 호출 그래프 맥락에서 존재하지 않는 의존성을 지어내는지를 추적하고 단죄하는 데 초점이 맞춰져 있다.</p>
<p>이처럼 문서화된 명세를 프롬프트에 동적으로 연결하는 기법은, 프롬프트의 미세한 문구 변화가 일으키는 모델 뇌내의 의미적 표류(Semantic Drift) 현상을 물리적으로 방지하고, 모델이 주어진 사실 정보라는 절대적인 닻(Anchor)에 고정되도록 강제하는 가장 실효성 있는 소프트웨어 엔지니어링 패턴이다.</p>
<h2>5.  결론: 프롬프트의 확률적 혼돈 속에서 결정론적 오라클을 구축하기 위한 제언</h2>
<p>프롬프트 민감도로 인해 발생하는 비일관성 현상은 LLM이 지닌 본질적인 통계학적, 자가 회귀적 특성에서 비롯된 것이며, 이는 단순한 지시어(Prompt) 표면의 수정을 넘어서는 소프트웨어 아키텍처 수준의 구조적 대응을 강하게 요구한다. PSS, PRIN, IPS와 같은 정교한 측정 지표들은 거대 언어 모델이 완벽한 논리 엔진이 아니라 표면적 토큰의 확률에 극도로 예민하게 반응하는 취약한 시스템임을 과학적으로 입증하고 있다. 특히 입력 순서의 무작위화만으로도 논리적 추론이 완전히 붕괴되는 순서 편향(Order Effect)이나 , 기대 동작이 아닌 실제 결함 동작을 오라클로 채택해 버리는 근시안적 편향성 은, 최신 AI 모델조차 복잡한 소프트웨어의 무결성 검증을 온전히 단독으로 책임질 수 없음을 방증한다.</p>
<p>결과적으로, AI 에이전트와 LLM을 통합한 차세대 소프트웨어 개발 및 테스트 환경에서는 ’완벽하게 일관된 답변을 내놓는 매직 프롬프트’를 찾으려는 연금술적 접근 방식은 지양되어야 한다. 엔지니어들은 언어 모델의 예측 불가능성과 비결정성을 통제할 수 없는 런타임 상수(Constant)로 명확히 인정해야 한다. 그리고 그 흔들리는 기반 위를 감쌀 수 있도록, 연산의 배치를 고정하는 불변 처리 시스템, JSON 스키마를 통한 강제 구조화 입출력 매커니즘, 검증된 교사 모델과의 백투백 비교, 그리고 RAG 명세서 기반의 지식 강제 주입과 같은 확고한 ’결정론적 제약 조건(Deterministic constraints)’을 이중, 삼중으로 설계해야 한다.</p>
<p>이러한 다층적인 오라클 방어선 구축만이 프롬프트 민감도라는 혼돈 속에서도 자동화된 소프트웨어 파이프라인의 엔터프라이즈급 신뢰성과 품질 보증 기준을 타협 없이 지켜낼 수 있는 유일하고도 가장 강력한 공학적 해법이다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>What Did I Do Wrong? Quantifying LLMs’ Sensitivity and Consistency to Prompt Engineering - ACL Anthology, https://aclanthology.org/2025.naacl-long.73.pdf</li>
<li>LLM Self-Inconsistency Beyond Generative Randomness and Prompt Paraphrasing - arXiv, https://arxiv.org/html/2504.01282v1</li>
<li>LLM Inconsistency: Types, Metrics &amp; Remedies - Emergent Mind, https://www.emergentmind.com/topics/llm-inconsistency</li>
<li>Persona-Augmented Benchmarking: Evaluating LLMs Across Diverse Writing Styles - arXiv, https://arxiv.org/html/2507.22168v2</li>
<li>What Did I Do Wrong? Quantifying LLMs’ Sensitivity and Consistency to Prompt Engineering - arXiv, https://arxiv.org/pdf/2406.12334</li>
<li>[Literature Review] Prompt-Reverse Inconsistency: LLM Self-Inconsistency Beyond Generative Randomness and Prompt Paraphrasing - Moonlight | AI Colleague for Research Papers, https://www.themoonlight.io/en/review/prompt-reverse-inconsistency-llm-self-inconsistency-beyond-generative-randomness-and-prompt-paraphrasing</li>
<li>LLM Self-Inconsistency Beyond Generative Randomness and Prompt Paraphrasing - arXiv.org, https://arxiv.org/pdf/2504.01282</li>
<li>(PDF) Prompt-Reverse Inconsistency: LLM Self-Inconsistency Beyond Generative Randomness and Prompt Paraphrasing - ResearchGate, https://www.researchgate.net/publication/390439491_Prompt-Reverse_Inconsistency_LLM_Self-Inconsistency_Beyond_Generative_Randomness_and_Prompt_Paraphrasing</li>
<li>The Order Effect: Investigating Prompt Sensitivity in Closed-Source LLMs - arXiv, https://arxiv.org/html/2502.04134v1</li>
<li>The Order Effect: Investigating Prompt Sensitivity to Input Order in LLMs - arXiv, https://arxiv.org/html/2502.04134v2</li>
<li>Premise Order Matters in Reasoning with Large Language Models - GitHub, https://raw.githubusercontent.com/mlresearch/v235/main/assets/chen24i/chen24i.pdf</li>
<li>Peiyang-Song/Awesome-LLM-Reasoning-Failures - GitHub, https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures</li>
<li>Order Matters! An Empirical Study on Large Language Models’ Input Order Bias in Software Fault Localization - arXiv, https://arxiv.org/html/2412.18750v4</li>
<li>ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs - GitHub, https://github.com/open-compass/ProSA</li>
<li>ProSA : framework to evaluate and understand Prompt Sensitivity of LLMs - Medium, https://medium.com/@techsachin/prosa-framework-to-evaluate-and-understand-prompt-sensitivity-of-llms-2e2cb3e013cb</li>
<li>Evaluating and Explaining Prompt Sensitivity of LLMs Using Interactions - OpenReview, https://openreview.net/forum?id=6fHZR6uxNa</li>
<li>Prompt Stability in Code LLMs: Measuring Sensitivity across Emotion- and Personality-Driven Variations - arXiv, https://arxiv.org/html/2509.13680v1</li>
<li>ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs - ACL Anthology, https://aclanthology.org/2024.findings-emnlp.108.pdf</li>
<li>[2410.12405] ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs - arXiv, https://arxiv.org/abs/2410.12405</li>
<li>ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs - ResearchGate, https://www.researchgate.net/publication/384974573_ProSA_Assessing_and_Understanding_the_Prompt_Sensitivity_of_LLMs</li>
<li>Oracle-guided Program Selection from Large Language Models - Sergey Mechtaev, https://mechtaev.com/files/issta24.pdf</li>
<li>(PDF) The Oracle Problem in Software Testing: A Survey - ResearchGate, https://www.researchgate.net/publication/276255185_The_Oracle_Problem_in_Software_Testing_A_Survey</li>
<li>AugmenTest: Enhancing Tests with LLM-Driven Oracles - arXiv, https://arxiv.org/html/2501.17461v1</li>
<li>Do LLMs generate test oracles that capture the actual or the expected program behaviour?, https://arxiv.org/html/2410.21136v1</li>
<li>Do LLMs Generate Useful Test Oracles? An Empirical Study with an Unbiased Dataset - Dr. Luca Di Grazia, https://www.lucadigrazia.com/papers/ase2025.pdf</li>
<li>Understanding LLM-Driven Test Oracle Generation - arXiv, https://arxiv.org/html/2601.05542v1</li>
<li>[2601.05542] Understanding LLM-Driven Test Oracle Generation - arXiv.org, https://arxiv.org/abs/2601.05542</li>
<li>Doc2OracLL: Investigating the Impact of Documentation on LLM-Based Test Oracle Generation | Request PDF - ResearchGate, https://www.researchgate.net/publication/392846108_Doc2OracLL_Investigating_the_Impact_of_Documentation_on_LLM-Based_Test_Oracle_Generation</li>
<li>A Review of Large Language Models for Automated Test Case Generation - MDPI, https://www.mdpi.com/2504-4990/7/3/97</li>
<li>Testing AI Systems: Handling the Test Oracle Problem - DEV Community, https://dev.to/qa-leaders/testing-ai-systems-handling-the-test-oracle-problem-3038</li>
<li>Testing LLM Prompts in Production Pipelines: A Practical Approach - DEV Community, https://dev.to/stuartp/testing-llm-prompts-in-production-pipelines-a-practical-approach-349b</li>
<li>10 LLM Testing Strategies To Catch AI Failures - Galileo AI, https://galileo.ai/blog/llm-testing-strategies</li>
<li>Why LLMs Give Different Answers to the Same Question (And How to Fix It) | by Avi Goldfinger | Medium, https://medium.com/@avigoldfinger/why-llms-give-different-answers-to-the-same-question-and-how-to-fix-it-c1746ff49abc</li>
<li>Invisible Prompt Injection: A Threat to AI Security | Trend Micro (US), https://www.trendmicro.com/en_us/research/25/a/invisible-prompt-injection-secure-ai.html</li>
<li>A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures - arXiv, https://arxiv.org/html/2506.19676v3</li>
<li>
<ol start="2">
<li>Threats through use | AI Exchange, https://owaspai.org/docs/2_threats_through_use/</li>
</ol>
</li>
<li>Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM Evaluation in Multiple-Choice Question Answering - arXiv, https://arxiv.org/html/2503.14996v2</li>
<li>Next-Generation Software Testing: AI-Powered Test Automation - IEEE Computer Society, https://www.computer.org/csdl/magazine/so/2025/04/11024091/27gSQcKD6jC</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>