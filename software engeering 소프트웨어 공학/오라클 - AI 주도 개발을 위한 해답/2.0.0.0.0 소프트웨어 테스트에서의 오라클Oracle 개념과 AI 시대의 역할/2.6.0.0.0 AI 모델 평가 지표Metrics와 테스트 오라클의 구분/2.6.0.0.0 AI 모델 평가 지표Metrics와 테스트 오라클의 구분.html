<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2.6 AI 모델 평가 지표(Metrics)와 테스트 오라클의 구분</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2.6 AI 모델 평가 지표(Metrics)와 테스트 오라클의 구분</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 2. 소프트웨어 테스트에서의 오라클(Oracle) 개념과 AI 시대의 역할</a> / <a href="index.html">2.6 AI 모델 평가 지표(Metrics)와 테스트 오라클의 구분</a> / <span>2.6 AI 모델 평가 지표(Metrics)와 테스트 오라클의 구분</span></nav>
                </div>
            </header>
            <article>
                <h1>2.6 AI 모델 평가 지표(Metrics)와 테스트 오라클의 구분</h1>
<h3>0.1  서론: 확률적 성능과 확정적 기능의 충돌</h3>
<p>현대 소프트웨어 엔지니어링의 패러다임이 결정론적(Deterministic) 알고리즘에서 확률적(Probabilistic) 모델로 이동함에 따라, 품질 보증(QA)의 핵심인 ’검증(Verification)’의 개념 또한 근본적인 도전에 직면했다. 특히 거대 언어 모델(LLM)을 활용한 애플리케이션 개발에 있어 가장 치명적인 오해는 ’모델의 성능(Performance)’을 ’소프트웨어의 정확성(Correctness)’과 동일시하는 데서 비롯된다. 전통적인 소프트웨어 엔지니어링에서 단위 테스트(Unit Test)는 이진법적(Binary) 속성을 지닌다. 함수 <span class="math math-inline">f(x)</span>에 입력값 <span class="math math-inline">x</span>를 넣었을 때 기대값 <span class="math math-inline">y</span>가 반환되면 테스트는 통과(Pass)이고, 그렇지 않으면 실패(Fail)다. 여기에는 모호함이 설 자리가 없다. 이것이 바로 우리가 오랫동안 신뢰해 온 결정론적 세계관이다.</p>
<p>반면, 생성형 AI의 세계는 근본적으로 확률적이다. 모델은 다음 토큰이 등장할 확률 분포를 계산하고, 온도(Temperature)나 Top-p와 같은 샘플링 전략에 따라 매번 다를 수 있는 출력을 생성한다. 이때 우리가 사용하는 평가 지표—BLEU, ROUGE, Perplexity 등—는 모델이 생성한 텍스트가 정답 데이터셋(Gold Standard)과 얼마나 ’유사한가’를 측정하는 통계적 수치일 뿐, 해당 소프트웨어가 비즈니스 로직을 완벽하게 수행했는지를 보장하지 않는다. 95%의 정확도를 가진 모델이라 할지라도, 나머지 5%의 오류가 핵심 비즈니스 로직(예: 결제 금액 계산, SQL 쿼리 생성, 개인정보 마스킹)에서 발생한다면, 그 소프트웨어는 엔지니어링 관점에서 명백한 ’결함(Defect)’이 있는 것이다.</p>
<p>많은 조직이 AI 모델의 벤치마크 점수에 매몰되어, 실제 프로덕션 환경에서 요구되는 기능적 무결성을 간과한다. ’MIT Sloan Management Review’나 관련 산업 보고서들이 지적하듯, AI 프로젝트의 95%가 ROI를 달성하지 못하거나 실패하는 주된 원인은 모델 자체의 지능 부족이 아니라, 확률적 출력을 결정론적 비즈니스 프로세스에 통합하는 과정에서의 오라클(Oracle) 부재 때문이다. 모델이 아무리 유창하게 말을 해도, 그것이 사실인지, 규정에 맞는지, 안전한지를 판별할 수 있는 확정적 기준이 없다면 그 시스템은 신뢰할 수 없다.</p>
<p>본 장에서는 AI 모델의 일반적인 성능을 측정하는 ’평가 지표(Evaluation Metrics)’와, 특정 입력에 대해 시스템의 동작이 올바른지 판별하는 ’테스트 오라클(Test Oracle)’의 개념적, 실무적 차이를 명확히 구분한다. 나아가 AI가 도입되면서 발생하는 ’오라클 문제(The Oracle Problem)’의 본질을 분석하고, 이를 극복하기 위해 확률적 출력을 결정론적 검증 체계로 편입시키는 구체적인 방법론을 논한다.</p>
<hr />
<h3>0.2  평가 지표(Evaluation Metrics)의 본질과 한계</h3>
<p>AI 모델 개발 과정에서 사용되는 평가 지표는 주로 모델의 학습 상태를 모니터링하고, 다른 모델과의 상대적 우위를 비교하기 위해 고안되었다. 이러한 지표들은 대규모 데이터셋에 대한 ‘평균적인’ 성능을 나타내는 데 최적화되어 있으며, 개별 케이스의 치명적 오류를 걸러내는 데는 태생적인 한계를 지닌다. 지표는 ’경향(Trend)’을 보여줄 뿐 ’보장(Guarantee)’을 제공하지 않는다.</p>
<h4>0.2.1  텍스트 유사성 기반 지표(N-gram Metrics)의 기계적 한계</h4>
<p>자연어 처리(NLP) 분야에서 가장 널리 사용되는 BLEU(Bilingual Evaluation Understudy)와 ROUGE(Recall-Oriented Understudy for Gisting Evaluation) 점수는 생성된 텍스트와 참조(Reference) 텍스트 간의 단어(N-gram) 겹침 정도를 계산한다. 이들은 계산이 빠르고 비용이 저렴하며 재현 가능하다는 장점이 있지만, ’기능적 정확성’을 측정하는 도구로는 부적합하다.</p>
<p><strong>BLEU 점수의 구조적 맹점</strong></p>
<p>BLEU는 정밀도(Precision)에 기반하여 모델이 생성한 단어가 정답지에 얼마나 포함되어 있는지를 측정한다. 수식적으로는 다음과 같이 표현된다.</p>
<p><span class="math math-display">BLEU = BP \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)</span></p>
<p>여기서 <span class="math math-inline">p_n</span>은 n-gram 정밀도이며, <span class="math math-inline">BP</span>는 짧은 문장에 대한 페널티(Brevity Penalty)다. 그러나 이 수식은 문맥이나 의미(Semantics)를 전혀 고려하지 않는다. 예를 들어, 의료 AI가 처방을 내리는 상황을 가정해보자.</p>
<ul>
<li><strong>참조 문장(Reference):</strong> “The doctor confirmed the patient represents a risk.” (의사는 환자가 위험하다고 확인했다.)</li>
<li><strong>생성 문장(Candidate):</strong> “The patient confirmed the doctor represents a risk.” (환자는 의사가 위험하다고 확인했다.)</li>
</ul>
<p>이 두 문장은 사용된 단어(Unigram)가 완벽히 일치하므로, 어순을 고려하지 않는 단순 통계적 지표에서는 매우 높은 점수를 받을 수 있다. 하지만 주어와 목적어가 뒤바뀌어 의미는 정반대가 되었다. 이는 의료나 법률 AI 시스템에서 치명적인 기능적 오류임에도 불구하고, 평가 지표상으로는 ’고성능’으로 둔갑하게 된다. 이는 지표가 표면적 텍스트(Surface Form)에만 집중하고 심층적 의미(Deep Semantics)를 무시하기 때문에 발생하는 현상이다.</p>
<p><strong>ROUGE 점수와 재현율의 함정</strong></p>
<p>ROUGE는 재현율(Recall)에 초점을 맞춰, 정답지의 내용이 생성된 텍스트에 얼마나 포함되었는지를 본다. 요약(Summarization) 태스크에서 주로 사용되나, 이 역시 ’환각(Hallucination)’을 잡아내지 못한다는 치명적 단점이 있다. 모델이 정답 문장의 핵심 키워드를 모두 포함하되, 그 뒤에 거짓 정보를 덧붙여도 ROUGE 점수는 높게 유지될 수 있다. ROUGE는 “빠진 것이 없는가?“를 묻지만, “틀린 것이 없는가?“는 묻지 않기 때문이다.</p>
<p><img src="./2.6.0.0.0%20AI%20%EB%AA%A8%EB%8D%B8%20%ED%8F%89%EA%B0%80%20%EC%A7%80%ED%91%9CMetrics%EC%99%80%20%ED%85%8C%EC%8A%A4%ED%8A%B8%20%EC%98%A4%EB%9D%BC%ED%81%B4%EC%9D%98%20%EA%B5%AC%EB%B6%84.assets/image-20260217160150582.png" alt="image-20260217160150582" /></p>
<h4>2. 확률적 지표와 소프트웨어 품질의 괴리</h4>
<p>소프트웨어 품질 보증(QA)의 관점에서 볼 때, 평가 지표는 본질적으로 ‘불충분한’ 척도다.</p>
<ul>
<li><strong>평균의 함정(The Trap of Averages):</strong> 정확도(Accuracy) 99%는 100번 중 1번은 실패한다는 뜻이다. 만약 그 실패가 사용자의 계좌 이체 금액을 0 하나 더 붙여서 처리하는 것이라면, 이 시스템은 배포될 수 없다. 엔지니어링에서는 ’평균적인 성능’보다 ’최악의 경우(Worst-case)’에 대한 방어 기제가 훨씬 중요하다. 그러나 대부분의 AI 평가 벤치마크는 대량의 데이터셋에 대한 평균 점수로 모델의 우열을 가린다. 이는 1번의 치명적 실패를 99번의 성공 속에 은폐시킨다.</li>
<li><strong>교정(Calibration) 문제와 과신(Overconfidence):</strong> 모델이 내놓은 확률값(Confidence Score)과 실제 정확도 사이의 괴리도 문제다. 최신 LLM들은 종종 틀린 답을 매우 높은 확신(High Confidence)을 가지고 생성한다. 따라서 모델의 내부 확률값(Log-prob)을 품질 지표로 사용하는 것 역시, 별도의 교정(Calibration) 과정 없이는 신뢰할 수 없는 오라클이 된다.</li>
</ul>
<h4>3. 기능적 정확성(Functional Correctness)의 부재: 실증 연구</h4>
<p>평가 지표와 실제 기능적 정확성 사이의 괴리는 단순한 가설이 아니라 실증적으로 증명된 사실이다. 최신 연구인 ‘Assessing Evaluation Metrics for Neural Test Oracle Generation’ (Shin et al., 2024) 은 이 문제를 정면으로 다룬다. 연구진은 AI가 생성한 테스트 오라클(Test Oracle)을 대상으로, BLEU, CodeBLEU, Accuracy와 같은 텍스트 기반 정적 지표와 실제 코드 커버리지(Code Coverage), 변이 점수(Mutation Score)와 같은 동적 테스트 적합성 지표 간의 상관관계를 분석했다.</p>
<p>결과는 충격적이다. 텍스트 유사도 지표와 테스트 적합성 지표 간에는 <strong>유의미한 상관관계가 없음</strong>이 밝혀졌다. 연구는 이를 두 가지 유형의 불일치로 설명한다.</p>
<ul>
<li><strong>Type-2 불일치 (Low Metric, High Adequacy):</strong> 모델이 생성한 코드가 정답 코드와 텍스트상으로는 완전히 다르지만(낮은 BLEU), 기능적으로는 동일한 검증을 수행하는 경우다. 예를 들어, <code>assertNull(x)</code>와 <code>assertEquals(null, x)</code>는 텍스트로는 다르지만 기능은 같다. 지표에만 의존한다면 훌륭한 기능을 가진 코드가 낮은 점수로 인해 폐기될 수 있다.</li>
<li><strong>Type-3 불일치 (High Metric, Low Adequacy):</strong> 모델이 정답 코드와 매우 유사한 텍스트를 생성했지만(높은 BLEU), 핵심적인 매개변수 하나가 틀리거나 메서드 호출 체인이 잘못되어 실제 버그를 잡아내지 못하는 경우다. 이는 지표상으로는 ’성공’으로 보이지만, 실제로는 아무런 검증도 수행하지 못하는 ’허수아비 테스트’를 양산한다.</li>
</ul>
<p>이러한 연구 결과는 AI 모델을 평가할 때, 단순히 텍스트 유사도나 통계적 지표에만 의존해서는 해당 모델이 생성한 결과물이 소프트웨어적으로 ‘올바른지’ 판단할 수 없음을 시사한다. 즉, <strong>평가 지표는 오라클이 될 수 없다.</strong></p>
<h3>2.6.3 테스트 오라클(Test Oracle)의 이론적 재정립</h3>
<p>전통적인 소프트웨어 테스팅 이론에서 오라클은 “테스트 수행 결과가 올바른지 판단하는 메커니즘“으로 정의된다. 이는 요구사항 명세서일 수도 있고, 인간 테스터일 수도 있으며, 이전 버전의 시스템(Regression Oracle)일 수도 있다. 오라클의 핵심은 **결정론적 판정(Deterministic Verdict)**이다. 입력 <span class="math math-inline">I</span>에 대해 출력 <span class="math math-inline">O</span>가 기대값 <span class="math math-inline">E</span>와 일치하는지(<span class="math math-inline">O \equiv E</span>)를 확인하는 과정은 모호해서는 안 된다.</p>
<h4>1. AI 시대의 오라클 문제 (The Oracle Problem)</h4>
<p>그러나 AI 시스템, 특히 생성형 모델을 테스트할 때 우리는 ’오라클 문제(The Oracle Problem)’라는 거대한 장벽에 직면한다. 이는 크게 두 가지 양상으로 나타난다.</p>
<ol>
<li><strong>정답의 부재 (Lack of Ground Truth):</strong> “재미있는 이야기를 써줘” 혹은 “이 문서를 요약해줘“라는 입력에 대한 유일한 정답은 존재하지 않는다. 번역, 요약, 창작과 같은 작업은 본질적으로 개방형(Open-ended) 문제이며, <span class="math math-inline">O \equiv E</span>를 비교할 <span class="math math-inline">E</span>가 하나로 고정되지 않는다.</li>
<li><strong>검증 비용의 폭증:</strong> 정답이 존재하더라도(예: 복잡한 법률 자문이나 의료 진단), 모델이 생성한 복잡한 추론 과정을 검증하려면 전문가 수준의 인간 개입이 필요하다. 이는 자동화된 테스트 파이프라인(CI/CD)을 구축하는 데 막대한 병목이 된다. 인간이 모든 출력을 검수해야 한다면 자동화의 의미가 퇴색되기 때문이다.</li>
</ol>
<h4>2. 데이터 오라클의 취약성</h4>
<p>머신러닝(ML) 초기에는 테스트 데이터셋(Test Dataset)의 라벨(Label)을 오라클로 간주해왔다. 즉, 데이터가 곧 오라클이었다. 그러나 생성형 AI 시대에 접어들며 이 접근법은 한계에 봉착했다.</p>
<ul>
<li><strong>데이터 노이즈와 편향:</strong> 데이터 자체에 오류(Noise)나 편향(Bias)이 포함된 경우, 모델이 데이터를 완벽하게 학습하는 것이 오히려 소프트웨어의 결함이 된다. 데이터는 현실의 불완전한 투영일 뿐, 절대적인 진리(Ground Truth)가 아닐 수 있다.</li>
<li><strong>확률적 비결정성 (Stochastic Nondeterminism):</strong> LLM은 동일한 프롬프트에 대해 실행 시점마다 다른 결과를 내놓을 수 있다. <code>x</code>를 입력했을 때 어제는 <code>y1</code>이 나오고 오늘은 <code>y2</code>가 나온다면, 고정된 정답 <code>y</code>를 가진 오라클은 매번 실패 판정을 내릴 수밖에 없다. 이를 ’Flaky Test’라고 하며, 엔지니어링 신뢰도를 떨어뜨리는 주범이다.</li>
</ul>
<h4>3. 오라클의 계층화: 확률에서 확정으로</h4>
<p>AI 테스팅에서는 단일 오라클이 아닌, 검증의 강도와 비용에 따른 계층적 오라클 전략이 필요하다.</p>
<p><img src="./2.6.0.0.0%20AI%20%EB%AA%A8%EB%8D%B8%20%ED%8F%89%EA%B0%80%20%EC%A7%80%ED%91%9CMetrics%EC%99%80%20%ED%85%8C%EC%8A%A4%ED%8A%B8%20%EC%98%A4%EB%9D%BC%ED%81%B4%EC%9D%98%20%EA%B5%AC%EB%B6%84.assets/image-20260217160219092.png" alt="image-20260217160219092" /></p>
<p>위의 피라미드 구조에서 볼 수 있듯이, 하위 계층(지표)은 넓은 커버리지를 제공하지만 신뢰도가 낮고, 상위 계층(결정론적 오라클)은 비용이 들지만 확실한 검증을 제공한다. 성공적인 AI 프로젝트는 이 계층을 적절히 혼합하여 운영한다.</p>
<hr />
<h3>2.6.4 결정론적 검증의 필요성과 오라클 유형</h3>
<p>AI 소프트웨어 개발에서는 ’정답과의 일치 여부’를 따지는 전통적 오라클을 넘어, **‘속성의 만족 여부(Satisfaction of Properties)’**를 검증하는 새로운 형태의 오라클이 필요하다. 이는 정답을 몰라도(Unknown Answer) 오답(Incorrect Answer)은 확실히 걸러낼 수 있는 방법론이다.</p>
<h4>1. 메타모픽 오라클 (Metamorphic Oracle)</h4>
<p>메타모픽 테스팅(Metamorphic Testing)은 정답을 알 수 없는 상황에서 모델의 **일관성(Consistency)**과 **논리적 관계(Relation)**를 검증하는 강력한 대안이다. 이는 “입력값의 변화에 따라 출력값이 어떻게 변해야 하는가?“라는 메타모픽 관계(Metamorphic Relation, MR)를 정의함으로써 오라클 문제를 우회한다.</p>
<ul>
<li><strong>불변 관계 (Invariant Relation):</strong> 입력에 의미 없는 변화를 주었을 때 출력이 변하지 않아야 한다.
<ul>
<li><em>예시:</em> 감성 분석 모델에 “The service was good“을 입력했을 때 ’Positive’가 나왔다면, “The service was good.”(마침표 추가)이나 “The service was <code>great</code>”(동의어 교체)를 입력해도 여전히 ’Positive’여야 한다. 만약 결과가 바뀐다면 모델은 강건성(Robustness)에 결함이 있는 것이다.</li>
<li><em>적용:</em> 검색 엔진이나 추천 시스템에서 사용자의 쿼리에 오타가 있거나 어순이 바뀌어도 핵심 검색 결과는 유지되어야 한다.</li>
</ul>
</li>
<li><strong>증가/감소 관계 (Increasing/Decreasing Relation):</strong> 입력의 특정 속성이 강화되면 출력의 강도도 비례해서 변해야 한다.
<ul>
<li><em>예시:</em> 신용 평가 AI에서 사용자의 ’연봉’을 입력값으로 올렸다면, ’대출 승인 확률’이나 ’신용 점수’는 유지되거나 상승해야 한다. 만약 연봉을 올렸는데 점수가 떨어진다면, 이는 모델의 논리적 결함(Monotonicity Violation)을 의미한다.</li>
<li><em>적용:</em> 자율주행 시스템에서 장애물과의 거리가 가까워질수록 브레이크 압력 수치는 높아져야 한다.</li>
</ul>
</li>
</ul>
<p>이러한 메타모픽 관계는 정답지(Label)가 없어도 모델의 논리적 오류를 잡아낼 수 있는 <strong>결정론적 검증 로직</strong>을 제공한다. 입력 A와 변환된 입력 A’ 사이의 출력 관계는 확률적이어서는 안 되며, 비즈니스 규칙에 따라 명확히 정의되어야 하기 때문이다.</p>
<h4>2. 제약 조건 기반 오라클 (Constraint-based Oracle)</h4>
<p>생성형 AI의 출력을 자유 텍스트가 아닌, 엄격한 구조(Structure)와 규칙(Rule) 내로 가두는 방식이다. 이는 AI의 ’창의성’이 발휘되어서는 안 되는 시스템 인터페이스 영역에서 필수적이다.</p>
<ul>
<li><strong>구문론적 오라클 (Syntactic Oracle):</strong> 출력이 특정 포맷(JSON, XML, YAML)을 준수하는지 검증한다. 최근 OpenAI의 ’Structured Outputs’이나 오픈소스 프레임워크들은 JSON Schema를 강제하여, 모델이 스키마에 어긋나는 토큰을 생성하는 것을 원천적으로 차단하거나 사후 검증한다. 이는 “내용은 모르겠지만, 적어도 이 형식은 지켜야 한다“는 최소한의 결정론적 합의다.
<ul>
<li><em>검증 예시:</em> <code>{"price": 100}</code>이라는 JSON을 요구했을 때, 모델이 <code>{"price": "one hundred"}</code>라고 문자열로 반환하면, 이는 <code>number</code> 타입을 요구하는 스키마 오라클에 의해 즉시 ‘실패’ 처리된다.</li>
</ul>
</li>
<li><strong>의미론적 제약 오라클 (Semantic Constraint Oracle):</strong> 출력 내용이 비즈니스 규칙을 위반하는지 검사한다.
<ul>
<li><em>예시:</em> “할인율은 0%에서 50% 사이여야 한다“는 룰이 있다면, 모델이 <code>{"discount": 99}</code>를 생성했을 때 이를 탐지하고 차단한다. 이는 모델의 확률적 생성 결과를 비즈니스 로직이라는 체(Sieve)로 걸러내는 역할을 한다.</li>
</ul>
</li>
</ul>
<h4>3. 하이브리드 오라클: AI로 AI를 검증 (LLM-as-a-Judge)</h4>
<p>최근에는 검증 로직 자체를 또 다른 고성능 LLM(예: GPT-4)에게 위임하는 ‘LLM-as-a-Judge’ 방식이 확산되고 있다. 이는 엄밀히 말해 결정론적 오라클은 아니지만(판사 모델도 확률적이므로), 복잡한 의미론적 평가를 자동화할 수 있는 현실적인 대안이다.</p>
<ul>
<li><strong>역할:</strong> 판사 모델은 생성된 텍스트가 “주어진 문맥(Context)에 근거하고 있는가?(Groundedness)” 혹은 “사용자의 질문에 유용한가?(Helpfulness)“를 평가한다.</li>
<li><strong>신뢰성 확보 전략:</strong> 판사 모델의 평가 역시 변동성이 있으므로, 동일한 건에 대해 여러 번 평가하게 하거나(Self-Consistency), 확정적인 채점 루브릭(Rubric)을 프롬프트로 제공하여 평가의 분산(Variance)을 줄여야 한다. G-Eval과 같은 프레임워크는 이러한 평가 과정을 체계화하여 상관관계를 높이는 시도를 하고 있다.</li>
</ul>
<h4>4. 차분 테스팅 오라클 (Differential Testing Oracle)</h4>
<p>동일한 입력에 대해 서로 다른 모델이나 시스템의 출력을 비교하는 방식이다.</p>
<ul>
<li><strong>회귀 테스트(Regression Testing):</strong> 신규 모델 V2의 출력이 검증된 모델 V1의 출력과 지나치게 달라지지 않았는지 확인한다.</li>
<li><strong>교차 검증(Cross-Verification):</strong> 작은 모델이 생성한 결과를 큰 모델(Teacher Model)이 검증하거나, 반대로 큰 모델의 결과를 룰 기반 시스템이 검증한다.</li>
</ul>
<h3>2.6.5 사례 연구: 지표가 놓친 치명적 결함</h3>
<p>실제 산업 현장에서 발생한 AI 실패 사례들은 평가 지표의 높은 점수가 실환경에서의 안정성을 보장하지 않는다는 것을 극명하게 보여준다. 이들 사례는 모두 ’유창한 텍스트 생성’에는 성공했으나, ’기능적 제약 조건’을 만족하는 오라클을 통과하지 못해 발생했다.</p>
<h4>사례 1: Air Canada 챗봇의 환각과 법적 책임</h4>
<p>Air Canada의 챗봇은 고객에게 “장례식 참가를 위한 항공권은 여행 후 90일 이내에 환불 신청이 가능하다“고 안내했다. 그러나 실제 회사 정책은 “여행 전 신청“이 필수였다. 고객은 챗봇의 말을 믿고 여행 후 환불을 요청했으나 거절당했고, 법정 공방 끝에 법원은 “챗봇의 정보도 회사의 공식 답변“이라며 항공사의 배상 책임을 인정했다.</p>
<ul>
<li><strong>지표 vs. 오라클 분석:</strong>
<ul>
<li>이 챗봇 모델은 학습 과정에서 유창성(Fluency)이나 대화 흐름(Coherence) 면에서 높은 평가 지표(Perplexity, ROUGE 등)를 기록했을 것이다. 문장은 문법적으로 완벽했고, 고객의 질문 의도도 정확히 파악했다. 지표상으로는 100점짜리 답변이다.</li>
<li><strong>실패 원인:</strong> “회사의 규정 데이터베이스(Knowledge Base)와 일치해야 한다“는 **사실적 일관성 오라클(Factual Consistency Oracle)**이 부재했다. RAG(Retrieval-Augmented Generation) 시스템에서 검색된 문서(실제 정책)와 생성된 답변 사이의 모순(Contradiction)을 감지하는 검증 로직(NLI, Natural Language Inference)이 작동하지 않은 것이다. 이는 확률적 생성 모델을 결정론적 비즈니스 규칙(환불 규정) 위에 아무런 안전장치 없이 올렸을 때 발생하는 전형적인 참사다.</li>
</ul>
</li>
</ul>
<h4>사례 2: Mata v. Avianca 사건과 가짜 판례 인용</h4>
<p>미국 법원에서 변호사가 ChatGPT를 이용해 준비 서면을 작성했는데, 여기에 존재하지 않는 가짜 판례들이 다수 포함된 사건이다. ChatGPT는 판례 이름, 사건 번호, 인용 문구까지 그럴듯하게 지어냈다.</p>
<ul>
<li><strong>지표 vs. 오라클 분석:</strong>
<ul>
<li>언어 모델 입장에서 이 출력물은 ‘매우 훌륭한’ 결과물이다. 법률 용어의 사용이 적절하고, 인용 형식이 정확하며, 논리 전개가 자연스럽다. 텍스트 유사도 기반 지표로는 만점에 가까운 점수를 받을 수 있다.</li>
<li><strong>실패 원인:</strong> “인용된 판례는 실제 법률 DB에 존재해야 한다“는 **존재성 검증 오라클(Existence Verification Oracle)**이 없었다. 이는 외부 API(Westlaw, LexisNexis 등)를 통해 인용된 사건 번호가 실제 존재하는지 조회하는 간단한 결정론적 테스트만으로도 막을 수 있는 오류였다. AI의 창의성(Temperature)이 허용되어서는 안 되는 영역(Fact)에 적용되었을 때, 이를 걸러낼 오라클이 없다면 치명적인 결과로 이어진다.</li>
</ul>
</li>
</ul>
<h4>사례 3: 코드 생성 AI의 구문 vs. 의미</h4>
<p>최근 연구에 따르면 코드 생성 모델이 문법적으로 실행 가능한(Compilable) 코드를 생성할 확률은 90%가 넘지만, 실제 요구사항을 만족하여 테스트 케이스를 통과(Verification)하는 비율은 50% 미만인 경우가 많다. 특히 <em>Dafny</em>와 같은 형식 검증(Formal Verification) 언어의 경우, 구문적 정확성(Syntax Correctness)은 95% 이상이지만 검증기(Verifier)를 통과하는 비율은 5% 미만으로 떨어지는 극단적인 차이를 보인다.</p>
<ul>
<li><strong>지표:</strong> <code>Pass@k</code>는 k번 생성했을 때 적어도 하나가 테스트를 통과할 확률을 의미한다. 이는 AI 모델 성능 지표로서는 유용하지만, 단 하나의 정답 코드를 배포해야 하는 CI/CD 파이프라인에서는 충분하지 않다.</li>
<li><strong>오라클:</strong> 여기서는 컴파일러와 정적 분석기(Static Analysis), 그리고 기존의 단위 테스트 수트(Unit Test Suite)가 강력한 결정론적 오라클로 작용한다. AI가 생성한 코드는 즉시 이 오라클들에 의해 ’심문’당해야 하며, 이 과정을 통과하지 못한 코드는 평가 지표가 아무리 높아도 폐기되어야 한다.</li>
</ul>
<h3>2.6.6 결론: 지표에서 오라클로의 전환</h3>
<p>AI 기반 소프트웨어 개발의 성패는 확률적 모델을 얼마나 효과적으로 결정론적 시스템 안에 가둘 수 있느냐에 달려 있다. 평가 지표(Metrics)는 모델을 선택하고 튜닝하는 ’연구실(Lab)’의 도구라면, 테스트 오라클(Oracle)은 모델을 실제 서비스에 배포하고 운영하는 ’현장(Field)’의 방패다.</p>
<p>우리는 이제 “이 모델의 BLEU 점수가 0.5점 올랐다“는 말 대신, “이 시스템은 어떤 입력이 들어와도 JSON 스키마를 위반하지 않으며, 환불 규정에 어긋나는 답변을 생성할 경우 별도의 안전망(Fallback)이 작동한다“고 말할 수 있어야 한다.</p>
<p>확률적 지표는 우리에게 ’느낌(Feeling)’을 주지만, 결정론적 오라클은 ’확신(Confidence)’을 준다. AI가 실험실을 벗어나 금융, 법률, 의료와 같은 미션 크리티컬(Mission Critical) 영역으로 진입하려면, 우리는 지표의 환상에서 벗어나 냉정한 오라클의 검증을 받아들여야 한다. 이것이 바로 2.7장에서 다룰 ’결정론적 정답지(Deterministic Ground Truth)’의 구축이 시급한 이유이며, AI 엔지니어링이 단순한 프롬프트 엔지니어링을 넘어 시스템 엔지니어링으로 진화해야 하는 지점이다.</p>
<h4>요약 및 시사점</h4>
<table><thead><tr><th><strong>구분</strong></th><th><strong>평가 지표 (Evaluation Metrics)</strong></th><th><strong>테스트 오라클 (Test Oracle)</strong></th></tr></thead><tbody>
<tr><td><strong>주요 예시</strong></td><td>BLEU, ROUGE, Perplexity, Accuracy</td><td>Metamorphic Relation, JSON Schema, Assertions</td></tr>
<tr><td><strong>속성</strong></td><td>확률적, 통계적, 평균 지향적</td><td>결정론적, 논리적, 개별 케이스 지향적</td></tr>
<tr><td><strong>검증 대상</strong></td><td>모델의 일반적인 학습 상태 및 경향성</td><td>특정 입력에 대한 시스템 동작의 정확성</td></tr>
<tr><td><strong>한계</strong></td><td>의미론적 오류 및 환각 탐지 불가</td><td>정답 부재 시 구축 난이도 상승 (Oracle Problem)</td></tr>
<tr><td><strong>역할</strong></td><td>모델 간 성능 비교 및 벤치마킹</td><td>배포 가능 여부 판정 (Go/No-Go Decision)</td></tr>
</tbody></table>
<p>다음 장에서는 이러한 오라클을 실제로 구현하기 위한 핵심 자원인 ’결정론적 정답지’의 설계 원칙과 이를 확보하기 위한 데이터 엔지니어링 전략에 대해 심도 있게 논의한다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>AI Projects Don’t Fail Because of Technology … - Vijayan Seenisamy, 2월 17, 2026에 액세스, https://vijayan-seenisamy.medium.com/ai-projects-dont-fail-because-of-technology-they-fail-because-of-operating-model-2cb7897bbf4d</li>
<li>AI Analytics Reality Check: Why 95% of Projects Miss the Mark, 2월 17, 2026에 액세스, https://insightsoftware.com/blog/ai-analytics-reality-check-why-95-of-projects-miss-the-mark/</li>
<li>The AI Integration Crisis: Why 95% of Enterprise Pilots Fail (and the, 2월 17, 2026에 액세스, https://servicepath.co/2025/09/ai-integration-crisis-enterprise-hybrid-ai/</li>
<li>Evaluate Language Model Translations - BLEU &amp; ROUGE Scores, 2월 17, 2026에 액세스, https://datacoach.in/evaluate-language-model-translations-using-bleu-and-rouge-scores/</li>
<li>LLM evaluation benchmarking: Beyond BLEU and ROUGE - Wandb, 2월 17, 2026에 액세스, https://wandb.ai/ai-team-articles/llm-evaluation/reports/LLM-evaluation-benchmarking-Beyond-BLEU-and-ROUGE–VmlldzoxNTIzMTY0NQ</li>
<li>Medical Hallucination in Foundation Models and Their Impact on, 2월 17, 2026에 액세스, https://www.medrxiv.org/content/10.1101/2025.02.28.25323115v1.full-text</li>
<li>Assessing Evaluation Metrics for Neural Test Oracle Generation, 2월 17, 2026에 액세스, https://arxiv.org/pdf/2310.07856</li>
<li>Assessing Evaluation Metrics for Neural Test Oracle Generation, 2월 17, 2026에 액세스, https://www.researchgate.net/publication/382572707_Assessing_Evaluation_Metrics_for_Neural_Test_Oracle_Generation</li>
<li>A Review on Oracle Issues in Machine Learning, 2월 17, 2026에 액세스, https://arxiv.org/abs/2105.01407</li>
<li>Testing AI Systems: Handling the Test Oracle Problem, 2월 17, 2026에 액세스, https://dev.to/qa-leaders/testing-ai-systems-handling-the-test-oracle-problem-3038</li>
<li>How to test Machine Learning Models? Metamorphic testing - Giskard, 2월 17, 2026에 액세스, https://www.giskard.ai/knowledge/how-to-test-ml-models-4-metamorphic-testing</li>
<li>What is Metamorphic Testing of AI? - testRigor, 2월 17, 2026에 액세스, https://testrigor.com/blog/what-is-metamorphic-testing-of-ai/</li>
<li>Large Language Model-Driven Structured Output - MDPI, 2월 17, 2026에 액세스, https://www.mdpi.com/2220-9964/13/11/405</li>
<li>Prompt engineering for structured data: a comparative evaluation of, 2월 17, 2026에 액세스, https://www.cs.wm.edu/~dcschmidt/PDF/Optimizing_Prompt_Styles_for_Structured_Data_Generation_in_LLM.pdf</li>
<li>Using Structured Outputs to Chain LLM Pipelines | Simbian AI Insights, 2월 17, 2026에 액세스, https://simbian.ai/blog/using-structured-outputs-to-chain-llm-pipelines</li>
<li>Evaluation vs Testing in Generative AI | by amruta | Medium, 2월 17, 2026에 액세스, https://medium.com/@pandeamruta27/evaluation-vs-testing-in-generative-ai-a81c829af5aa</li>
<li>AI agent observability and evaluation - Oracle, 2월 17, 2026에 액세스, https://www.oracle.com/asean/a/ocom/docs/fusion-apps-ai-agents-observe-eval-guide-apac.pdf</li>
<li>LLM Evaluation Frameworks, Metrics &amp; Methods Explained - Qualifire, 2월 17, 2026에 액세스, https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained</li>
<li>LLM hallucinations and failures: lessons from 5 examples, 2월 17, 2026에 액세스, https://www.evidentlyai.com/blog/llm-hallucination-examples</li>
<li>Mata v. Avianca, Inc., No. 1:2022cv01461 - Document 54 (S.D.N.Y., 2월 17, 2026에 액세스, https://law.justia.com/cases/federal/district-courts/new-york/nysdce/1:2022cv01461/575368/54/</li>
<li>Benchmarking Large Language Models for Compositional Formal, 2월 17, 2026에 액세스, https://www.arxiv.org/pdf/2509.23061</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>