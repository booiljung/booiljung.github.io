<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2.6.1 BLEU, ROUGE, METEOR 등 전통적 NLP 지표의 한계와 오라클로서의 부적합성</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2.6.1 BLEU, ROUGE, METEOR 등 전통적 NLP 지표의 한계와 오라클로서의 부적합성</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 2. 소프트웨어 테스트에서의 오라클(Oracle) 개념과 AI 시대의 역할</a> / <a href="index.html">2.6 AI 모델 평가 지표(Metrics)와 테스트 오라클의 구분</a> / <span>2.6.1 BLEU, ROUGE, METEOR 등 전통적 NLP 지표의 한계와 오라클로서의 부적합성</span></nav>
                </div>
            </header>
            <article>
                <h1>2.6.1 BLEU, ROUGE, METEOR 등 전통적 NLP 지표의 한계와 오라클로서의 부적합성</h1>
<h2>1.  BLEU, ROUGE,1. 서론: 통계적 일치와 의미론적 등가성의 거대한 괴리</h2>
<p>인공지능의 역사는 평가 지표의 진화와 궤를 같이 해왔습니다. 모델이 얼마나 발전했는지를 측정할 수 없다면, 그 발전은 공허한 외침에 불과하기 때문입니다. 제미나이(Gemini)와 같은 초거대 언어 모델(Large Language Model, LLM)이 등장하여 인간의 지적 노동을 대체하려는 이 시점에, 우리는 역설적이게도 과거의 유산인 낡은 자로 이 거인을 측정하려는 시도를 목격하곤 합니다. 자연어 처리(NLP) 분야에서 오랫동안 ’표준’이자 ’진리’처럼 받아들여졌던 BLEU, ROUGE, METEOR와 같은 지표들이 바로 그것입니다. 이들은 통계적 기계 번역(SMT)과 추출적 요약(Extractive Summarization)이 주류였던 2000년대 초반, “기계가 생성한 텍스트가 사람이 작성한 정답과 얼마나 단어 수준에서 겹치는가?“라는 단순하고도 강력한 질문에 답하기 위해 고안되었습니다.</p>
<p>하지만 생성형 AI의 시대에 접어들며 이 질문 자체가 유효성을 잃어가고 있습니다. LLM은 단순히 단어를 조합하거나 발췌하는 것을 넘어, 문맥을 이해하고, 추론하며, 창의적인 텍스트를 생성합니다. 이러한 고도의 지능적 행위를 단순한 어휘적 중복(Lexical Overlap)만으로 평가하려는 시도는 마치 자율주행 자동차의 성능을 단순히 바퀴의 회전수만으로 측정하려는 것과 같은 근본적인 범주 오류(Category Error)를 범하는 것입니다. 본 장에서는 이러한 전통적 NLP 지표들이 왜 현대적 LLM 애플리케이션의 성능 평가, 특히 시스템의 정답 여부를 판별하는 ’오라클(Oracle)’로서 기능할 수 없는지를 심층적으로 해부하고자 합니다. 우리는 이 지표들의 수학적 메커니즘을 낱낱이 파헤쳐 그 태생적 한계를 규명하고, 소프트웨어 공학적 관점에서 ’테스트 오라클’의 엄밀한 정의를 통해 LLM 평가가 나아가야 할 방향성을 제시할 것입니다.</p>
<p>이 보고서는 단순한 비평을 넘어, 제미나이를 활용하여 서적을 집필하거나 복잡한 애플리케이션을 구축하려는 개발자와 연구자들에게 왜 “점수가 높다고 해서 좋은 글이 아니며, 점수가 낮다고 해서 틀린 답이 아니다“라는 명제가 성립하는지, 그 논리적이고 실증적인 근거를 15,000 단어 분량의 방대한 분석을 통해 제공할 것입니다.</p>
<h3>1.1  전통적 지표의 수학적 메커니즘과 태생적 결함</h3>
<p>우리가 흔히 ’n-gram 기반 지표’라고 통칭하는 BLEU, ROUGE, METEOR는 각각의 탄생 배경과 목적이 다르지만, 근본적으로 “참조 텍스트(Reference Text)와의 표면적 유사성“을 품질의 척도로 삼는다는 공통된 철학을 공유합니다. 이 섹션에서는 각 지표가 어떻게 점수를 산출하는지 그 내부 메커니즘을 들여다보고, 그 과정에서 필연적으로 발생하는 정보 손실과 왜곡을 분석합니다.</p>
<h4>1.1.1  BLEU (Bilingual Evaluation Understudy): 정밀도(Precision)의 함정과 기하평균의 착시</h4>
<p>2002년, IBM의 키쇼어 파피네니(Kishore Papineni) 연구팀이 제안한 BLEU 스코어는 기계 번역 평가의 자동화를 이끌어낸 혁신적인 도구였습니다. 당시에는 사람이 일일이 번역문을 평가하는 것이 너무나 비용이 많이 들고 느렸기 때문에, 빠르고 저렴하며 재현 가능한 자동화된 지표가 절실했습니다. BLEU는 바로 이 지점에서 “기계 번역문이 전문 번역가가 작성한 참조 번역문과 얼마나 유사한가?“를 수치화함으로써 그 요구를 충족시켰습니다.</p>
<h5>1.1.1.1  수정된 n-gram 정밀도 (Modified n-gram Precision)</h5>
<p>BLEU의 핵심 알고리즘은 n-gram 정밀도입니다. 여기서 n-gram이란 텍스트 내에서 연속된 n개의 단어 시퀀스를 의미합니다. 예를 들어 “The cat is on the mat“라는 문장에서 1-gram(unigram)은 “The”, “cat”, “is”… 개별 단어들이고, 2-gram(bigram)은 “The cat”, “cat is”, “is on”… 과 같은 단어 쌍입니다.</p>
<p>BLEU는 기계가 생성한 문장(Candidate)에 있는 n-gram들이 참조 문장(Reference)에 얼마나 포함되어 있는지를 계산합니다. 하지만 단순한 포함 여부만 따질 경우, 기계가 “the the the the“와 같이 참조 문장에 있는 흔한 단어 하나만 반복해서 생성해도 정밀도가 1.0(100%)이 되는 치명적인 허점이 발생합니다. 이를 방지하기 위해 BLEU는 **클리핑(Clipping)**이라는 기법을 도입했습니다.</p>
<ul>
<li><strong>Count:</strong> 생성된 문장에서 해당 n-gram이 등장한 총 횟수.</li>
<li><strong>Max Ref Count:</strong> 참조 문장에서 해당 n-gram이 등장한 최대 횟수.</li>
<li><strong>Clipped Count:</strong> <span class="math math-inline">min(Count, Max\ Ref\ Count)</span>.</li>
</ul>
<p>즉, 참조 문장에 “the“가 두 번만 나온다면, 기계가 “the“를 백 번 생성해도 분자(Numerator)에는 2만 반영됩니다. 이는 무의미한 반복 생성을 억제하는 데 효과적이었지만, 동시에 ’문맥’을 거세하는 결과를 낳았습니다. 단어가 ’존재’하는지는 알 수 있지만, 그 단어가 ‘어디에’, ‘어떤 역할로’ 존재하는지는 평가 과정에서 소거되기 때문입니다.</p>
<h5>1.1.1.2  기하평균과 가중치: 구조적 맹점</h5>
<p>BLEU는 통상적으로 1-gram부터 4-gram까지의 정밀도를 계산한 뒤, 이를 **기하평균(Geometric Mean)**하여 최종 점수를 산출합니다.<br />
<span class="math math-display">
BLEU = BP \cdot \exp\left( \sum_{n=1}^{N} w_n \log p_n \right)
</span><br />
여기서 <span class="math math-inline">w_n</span>은 각 n-gram에 대한 가중치로, 보통 균등하게 <span class="math math-inline">1/N</span>을 적용합니다. 1-gram은 단어의 적절성(Adequacy)을, 4-gram과 같은 고차 n-gram은 문장의 유창성(Fluency)을 대변한다고 가정합니다.</p>
<p>하지만 기하평균의 특성상, 어느 하나의 n-gram 정밀도라도 0이 되면 전체 점수가 0으로 수렴하거나 급격히 낮아지는 문제가 발생합니다. 예를 들어, 문장이 짧아서 4-gram이 아예 형성되지 않거나, 아주 창의적인 표현을 써서 4-gram이 참조 문장과 하나도 겹치지 않는다면, 1-gram, 2-gram이 완벽하게 일치해도 전체 점수는 0점이 될 수 있습니다. 이를 보정하기 위해 스무딩(Smoothing) 기법이 사용되기도 하지만, 근본적으로 “긴 n-gram의 일치가 곧 유창성“이라는 가정 자체가 현대 언어 모델의 복잡한 문장 구조 앞에서는 너무나 단순한 도식입니다.</p>
<h5>1.1.1.3  Brevity Penalty (BP): 길이의 족쇄</h5>
<p>정밀도 기반 지표의 또 다른 약점은 ’짧은 문장’을 선호한다는 것입니다. 틀릴 바에야 말을 아끼는 것이 정밀도를 높이는 데 유리하기 때문입니다. 이를 막기 위해 BLEU는 생성된 문장의 길이(<span class="math math-inline">c</span>)가 참조 문장의 길이(<span class="math math-inline">r</span>)보다 짧을 경우, 점수에 페널티를 부여하는 <strong>Brevity Penalty</strong>를 도입했습니다.<br />
<span class="math math-display">
BP = \begin{cases} 1 &amp; \text{if } c &gt; r \\ \exp(1 - r/c) &amp; \text{if } c \le r \end{cases}
</span><br />
이 공식은 수학적으로는 합리적으로 보일지 모르나, 실제 LLM 평가에서는 ’족쇄’가 됩니다. 제미나이와 같은 모델은 종종 참조 문장보다 훨씬 간결하고 효율적인 표현으로 동일한 의미를 전달하거나, 반대로 더 풍부한 정보를 담기 위해 길게 설명할 수 있습니다. 그러나 BLEU는 오직 ’참조 문장의 길이’를 기준으로 삼기 때문에, 문맥적으로 완벽한 요약이나 상세한 설명 모두가 단지 길이 차이 때문에 감점을 받는 불합리한 상황이 발생합니다.</p>
<blockquote>
<h3>BLEU 점수 산출 메커니즘의 해부</h3>
<p><img src="./2.6.1.0.0%20BLEU%20ROUGE%20METEOR%20%EB%93%B1%20%EC%A0%84%ED%86%B5%EC%A0%81%20NLP%20%EC%A7%80%ED%91%9C%EC%9D%98%20%ED%95%9C%EA%B3%84%EC%99%80%20%EC%98%A4%EB%9D%BC%ED%81%B4%EB%A1%9C%EC%84%9C%EC%9D%98%20%EB%B6%80%EC%A0%81%ED%95%A9%EC%84%B1.assets/image-20260217160745026.jpg" alt="image-20260217160745026" /></p>
<p>이 예시에서 후보 문장은 **“the”**를 불필요하게 반복하고 문법이 틀렸습니다. BLEU가 이를 어떻게 처리하는지 확인해보세요.</p>
<p><img src="./2.6.1.0.0%20BLEU%20ROUGE%20METEOR%20%EB%93%B1%20%EC%A0%84%ED%86%B5%EC%A0%81%20NLP%20%EC%A7%80%ED%91%9C%EC%9D%98%20%ED%95%9C%EA%B3%84%EC%99%80%20%EC%98%A4%EB%9D%BC%ED%81%B4%EB%A1%9C%EC%84%9C%EC%9D%98%20%EB%B6%80%EC%A0%81%ED%95%A9%EC%84%B1.assets/image-20260217160813909.jpg" alt="image-20260217160813909" /></p>
<p>* 문맥이 맞지 않는 “the the”, “cat mat” 등은 참조 문장에 없어 0점이 됩니다.</p>
<p><img src="./2.6.1.0.0%20BLEU%20ROUGE%20METEOR%20%EB%93%B1%20%EC%A0%84%ED%86%B5%EC%A0%81%20NLP%20%EC%A7%80%ED%91%9C%EC%9D%98%20%ED%95%9C%EA%B3%84%EC%99%80%20%EC%98%A4%EB%9D%BC%ED%81%B4%EB%A1%9C%EC%84%9C%EC%9D%98%20%EB%B6%80%EC%A0%81%ED%95%A9%EC%84%B1.assets/image-20260217160837425.jpg" alt="image-20260217160837425" /></p>
<p>BLEU는 n-gram의 ’클리핑된 정밀도(Clipped Precision)’를 계산하여 점수를 산출합니다. ‘Candidate’ 문장이  ’Reference’와 단어는 공유하지만 의미가 다른 경우(순서 변경 등)에도 점수가 어떻게 산출되는지 확인하십시오.</p>
<p>​          Data sources: <a href="https://www.geeksforgeeks.org/nlp/understanding-bleu-and-rouge-score-for-nlp-evaluation/">GeeksforGeeks</a>, <a href="https://en.wikipedia.org/wiki/BLEU">Wikipedia</a>, <a href="https://tutorialsdojo.com/what-is-bilingual-evaluation-understudy-bleu-score-for-machine-translation/">TutorialsDojo</a>, <a href="https://machinelearningmastery.com/calculate-bleu-score-for-text-python/">MachineLearningMastery</a></p>
</blockquote>
<h5>1.1.1.4  BLEU의 의미론적 실명 (Semantic Blindness)</h5>
<p>가장 치명적인 문제는 BLEU가 **의미(Semantics)**에 대해 완전히 맹목적이라는 점입니다.</p>
<ul>
<li><strong>동의어 처리 불가:</strong> “The huge rock“과 “The large stone“은 의미적으로 거의 동일합니다. 하지만 BLEU의 세계에서 ’huge’와 ‘large’, ’rock’과 ’stone’은 서로 다른 단어일 뿐입니다. 인간이 볼 때는 훌륭한 번역(Paraphrasing)이 BLEU 점수에서는 낮은 점수를 받게 됩니다.</li>
<li><strong>어순의 무시:</strong> 4-gram까지 본다고 하지만, 문장 전체의 논리적 구조를 파악하지 못합니다. “The guard arrived late because of the rain” (비 때문에 경비원이 늦게 왔다)와 “The rain arrived late because of the guard” (경비원 때문에 비가 늦게 왔다)는 n-gram 구성이 매우 유사하여 거의 동일한 BLEU 점수를 받습니다. 그러나 두 문장의 의미는 천양지차입니다. 이는 BLEU가 문장 내 인과관계를 전혀 이해하지 못함을 증명합니다.</li>
<li><strong>품사 및 중요도 무시:</strong> 문장의 핵심인 주어나 동사가 틀린 것과, 관사나 전치사 하나가 틀린 것을 BLEU는 동일한 ’1단어 불일치’로 취급합니다. “나는 사과를 먹었다“와 “나는 사과를 안 먹었다“는 ’안(not)’이라는 한 단어 차이지만 의미는 정반대입니다. BLEU는 이 치명적인 차이를 단순히 미미한 점수 하락으로만 반영할 뿐입니다.</li>
</ul>
<h4>1.1.2  ROUGE (Recall-Oriented Understudy for Gisting Evaluation): 재현율(Recall)의 딜레마</h4>
<p>ROUGE는 2004년 요약(Summarization) 작업의 평가를 위해 등장했습니다. 번역이 ’정확성’을 중시한다면, 요약은 원문의 핵심 내용을 빠짐없이 담아내는 ’포괄성’이 중요합니다. 따라서 ROUGE는 정밀도 대신 **재현율(Recall)**에 초점을 맞춥니다. “참조 요약문에 있는 단어 중 몇 개가 생성된 요약문에 포함되었는가?“가 핵심 질문입니다.</p>
<h5>1.1.2.1  ROUGE의 다양한 변주와 한계</h5>
<ul>
<li><strong>ROUGE-N:</strong> n-gram 재현율을 측정합니다. 주로 ROUGE-1(단어 단위)과 ROUGE-2(구 단위)가 사용됩니다. ROUGE-1은 내용의 커버리지를, ROUGE-2는 어느 정도의 유창성을 대변한다고 여겨집니다.</li>
<li><strong>ROUGE-L (Longest Common Subsequence):</strong> 최장 공통 부분 수열을 활용합니다. 이는 단어들이 반드시 연속적이지 않아도, 순서만 지켜지면 매칭으로 인정합니다. 예를 들어 참조 문장이 “The quick brown fox“일 때, 생성 문장이 “The brown fox“라면 ’quick’이 빠졌어도 “The - brown - fox“의 순서가 유지되므로 점수를 받습니다. 이는 문장 구조(Sentence Structure)를 반영하려는 시도였지만, 여전히 어휘적 일치에 갇혀 있습니다.</li>
</ul>
<h5>1.1.2.2  정보의 나열과 ‘단어 낚시’</h5>
<p>ROUGE가 가진 구조적 취약점은 모델이 ’단어 낚시(Keyword Stuffing)’를 하도록 유도할 수 있다는 점입니다. 재현율은 분모가 ’참조 문장의 n-gram 수’로 고정되어 있습니다. 따라서 모델이 참조 문장에 있을 법한 단어들을 마구잡이로 쏟아내기만 해도, 그 중 일부가 얻어걸려(Hit) 높은 점수를 받을 수 있습니다.</p>
<ul>
<li><strong>일관성(Coherence)의 부재:</strong> 문법적으로 엉망이거나 논리적으로 연결되지 않는 문장이라도, 핵심 키워드만 다수 포함되어 있으면 높은 ROUGE 점수를 받습니다. “사과 시장 경제 가격 폭락“이라는 문장은 문법적으로 비문이지만, “사과”, “시장”, “경제”, “가격”, “폭락“이라는 키워드가 참조 문장에 있다면 훌륭한 요약으로 평가받을 수 있습니다.</li>
<li><strong>추상적 요약(Abstractive Summarization)의 저평가:</strong> 제미나이와 같은 현대적 LLM은 원문에 없는 단어를 사용하여 내용을 재구성하는 ’추상적 요약’에 능합니다. 하지만 ROUGE는 원문의 단어를 그대로 가져다 쓰는 ’추출적 요약(Extractive Summarization)’에 유리하게 설계되어 있습니다. 모델이 원문의 내용을 완벽하게 이해하고 훨씬 더 세련된 단어로 요약하더라도, 단어가 겹치지 않으면 ROUGE 점수는 바닥을 칩니다.</li>
</ul>
<h4>1.1.3  METEOR (Metric for Evaluation of Translation with Explicit ORdering): 절반의 성공</h4>
<p>METEOR는 2005년 BLEU의 경직성을 완화하기 위해 제안되었습니다. 연구진은 BLEU가 인간의 판단과 상관관계가 낮다는 점을 지적하며, 이를 개선하기 위해 언어학적 지식을 도입했습니다.</p>
<h5>1.1.3.1  의미론적 확장의 시도</h5>
<p>METEOR는 단순한 문자열 매칭을 넘어선 몇 가지 중요한 개선을 이루어냈습니다.</p>
<ol>
<li><strong>유연한 매칭 단계:</strong></li>
</ol>
<ul>
<li><strong>정확한 일치 (Exact Match):</strong> 단어가 완전히 같은 경우.</li>
<li><strong>어간 추출 (Stemming):</strong> ’running’과 ‘run’, ’apples’와 ’apple’을 같은 단어로 인식합니다.</li>
<li><strong>동의어 (Synonymy):</strong> WordNet과 같은 시소러스(Thesaurus)를 활용하여 ’fast’와 ’quick’을 매칭으로 인정합니다. 이는 BLEU가 놓치던 ’의미적 등가성’을 어느 정도 포착하게 해주었습니다.</li>
</ul>
<ol start="2">
<li><strong>조화 평균과 가중치:</strong> 정밀도와 재현율의 조화 평균을 사용하되, 재현율에 훨씬 더 높은 가중치를 부여합니다. 이는 기계 번역에서 번역되지 않고 누락되는 단어가 있는 것이 틀린 단어가 추가되는 것보다 더 나쁘다는 판단에 근거합니다.</li>
<li><strong>정렬 페널티 (Fragmentation Penalty):</strong> 단어들이 뭉쳐서(Chunk) 나오지 않고 파편화되어 나올수록, 즉 어순이 엉망일수록 페널티를 부여합니다.</li>
</ol>
<h5>1.1.3.2  METEOR의 한계와 언어 종속성</h5>
<p>METEOR는 분명 BLEU보다 진보했지만, 그 발전은 ’비용’을 수반했습니다.</p>
<ul>
<li><strong>언어 자원 의존성:</strong> METEOR가 작동하려면 해당 언어의 어간 추출기(Stemmer)와 동의어 사전(WordNet)이 필수적입니다. 영어와 같은 주요 언어는 자원이 풍부하지만, 자원이 부족한 언어(Low-resource languages)나 전문 용어가 난무하는 특수 도메인(의료, 법률, 코딩 등)에서는 METEOR의 강점이 발휘되기 어렵습니다.</li>
<li><strong>문맥적 뉘앙스의 부재:</strong> 단어 단위의 동의어는 처리하지만, 문장 전체의 뉘앙스나 화용론적(Pragmatic) 의미는 여전히 파악하지 못합니다. 단어 A와 단어 B가 동의어라 할지라도, 문맥에 따라서는 전혀 다르게 쓰일 수 있습니다. 예를 들어 ’bank’는 ’은행’일 수도 있고 ’강둑’일 수도 있는데, 단순한 사전 매칭은 이러한 중의성을 해결하지 못할 수 있습니다.</li>
</ul>
<p><img src="./2.6.1.0.0%20BLEU%20ROUGE%20METEOR%20%EB%93%B1%20%EC%A0%84%ED%86%B5%EC%A0%81%20NLP%20%EC%A7%80%ED%91%9C%EC%9D%98%20%ED%95%9C%EA%B3%84%EC%99%80%20%EC%98%A4%EB%9D%BC%ED%81%B4%EB%A1%9C%EC%84%9C%EC%9D%98%20%EB%B6%80%EC%A0%81%ED%95%A9%EC%84%B1.assets/image-20260217161039447.jpg" alt="image-20260217161039447" /></p>
<p>BLEU, ROUGE, METEOR는 모두 어휘적 일치(Lexical Match)에 초점을 맞추고 있으며, 문맥적 의미(Contextual Meaning)나 논리적 추론(Logical Reasoning)과 같은 LLM의 핵심 역량을 평가하는 데에는 구조적인 한계를  가집니다.</p>
<p>​          Data sources: <a href="https://datumo.com/blog/insight/key-nlp-evaluation-metrics/">Datumo</a>, <a href="https://www.geeksforgeeks.org/nlp/understanding-bleu-and-rouge-score-for-nlp-evaluation/">GeeksforGeeks</a>, <a href="https://bobrupakroy.medium.com/comprehensive-10-llm-evaluation-from-bleu-rouge-and-meteor-to-scenario-based-metrics-like-9f6602c92c17">Medium</a>, <a href="https://www.semanticscholar.org/paper/The-Meteor-metric-for-automatic-evaluation-of-Lavie-Denkowski/2ccbdf9e9546633ee58009e0c0f3eaee75e6f576">Semantic Scholar</a>, <a href="https://tutorialsdojo.com/what-is-metric-for-evaluation-of-translation-with-explicit-ordering/">TutorialsDojo</a></p>
<h3>1.2  오라클(Oracle)로서의 부적합성: 소프트웨어 테스팅과 LLM의 충돌</h3>
<p>제미나이와 같은 LLM을 실제 서비스나 애플리케이션에 통합하려는 개발자와 엔지니어에게 가장 큰 난관은 ’평가’입니다. 소프트웨어 공학에서는 시스템의 실행 결과가 올바른지 판단하는 메커니즘을 **테스트 오라클(Test Oracle)**이라고 정의합니다. 이 섹션에서는 소프트웨어 공학적 관점에서 왜 전통적 NLP 지표들이 LLM 시대의 테스트 오라클로서 실패할 수밖에 없는지를 분석합니다.</p>
<h4>1.2.1  결정론적(Deterministic) 기대와 확률적(Probabilistic) 현실</h4>
<p>전통적인 소프트웨어 테스팅은 <strong>결정론적</strong>입니다. 계산기 프로그램에 <code>add(2, 3)</code>을 입력하면, 출력은 반드시 <code>5</code>여야 합니다. 이때 오라클은 <code>Expected: 5</code>와 <code>Actual Output</code>을 비교하는 단순한 등가 비교(Equality Check)만으로 충분합니다.</p>
<p>그러나 LLM은 본질적으로 **비결정론적(Non-deterministic)**이고 **확률적(Probabilistic)**인 시스템입니다. 같은 프롬프트를 입력하더라도, 모델의 온도(Temperature) 설정이나 샘플링 방식, 심지어는 난수 시드(Seed)에 따라 매번 다른 텍스트가 생성될 수 있습니다. 더 중요한 것은, 그 다른 텍스트들이 모두 ’정답’일 수 있다는 점입니다.</p>
<ul>
<li><strong>1:N 매핑의 문제:</strong> 전통적 오라클은 하나의 입력에 하나의 정답(1:1) 혹은 소수의 정답(1:N)을 가정합니다. 하지만 “사랑에 대한 시를 써줘“라는 입력에 대해 제미나이가 내놓을 수 있는 ‘올바른’ 출력의 개수는 사실상 무한대입니다. BLEU나 ROUGE와 같은 지표는 미리 정의된 소수의 참조 텍스트(Reference)와 비교하기 때문에, 이 무한한 정답 공간(Solution Space)을 커버할 수 없습니다.</li>
</ul>
<h4>1.2.2  위양성(False Positive)과 위음성(False Negative)의 위험</h4>
<p>테스트 오라클로서 기능하려면 오류를 정확하게 잡아내야 합니다. 그러나 n-gram 기반 지표를 오라클로 사용할 경우, 시스템의 신뢰성을 위협하는 두 가지 유형의 치명적인 오류가 발생합니다.</p>
<h5>1.2.2.1  위양성 (False Positive): “틀렸는데 맞았다고 하기”</h5>
<p>모델이 사실과 다른 내용(Hallucination)을 생성했음에도 불구하고, 참조 텍스트와 단어가 많이 겹친다는 이유로 높은 점수를 받아 테스트를 통과하는 경우입니다.</p>
<ul>
<li><strong>사례:</strong></li>
<li><strong>참조 텍스트:</strong> “미국의 수도는 워싱턴 D.C.이다.”</li>
<li><strong>생성 텍스트:</strong> “미국의 수도는 워싱턴 D.C.가 아니다.”</li>
<li><strong>분석:</strong> 위 두 문장은 단 한 단어(‘가’, ‘아니다’)를 제외하고 완벽하게 일치합니다. BLEU나 ROUGE 점수는 매우 높게 나오겠지만, 생성된 텍스트는 명백한 거짓입니다. 이처럼 부정문, 조건문, 미묘한 수식어의 차이로 인해 진리값이 뒤집히는 경우를 n-gram 지표는 전혀 감지하지 못합니다. 이는 자동화된 테스트 파이프라인에서 치명적인 결함(Silent Failure)을 놓치게 만들어, 실제 서비스 배포 시 심각한 문제를 야기할 수 있습니다.</li>
</ul>
<h5>1.2.2.2  위음성 (False Negative): “맞았는데 틀렸다고 하기”</h5>
<p>모델이 참조 텍스트보다 훨씬 더 창의적이고 정확하거나, 혹은 문맥에 더 적합한 답변을 내놓았음에도 불구하고, 단어 선택이 다르다는 이유로 낮은 점수를 받아 테스트에 실패하는 경우입니다.</p>
<ul>
<li><strong>사례:</strong></li>
<li><strong>참조 텍스트:</strong> “그는 매우 빠르게 달렸다.”</li>
<li><strong>생성 텍스트:</strong> “그 남자는 질풍처럼 질주했다.”</li>
<li><strong>분석:</strong> 생성된 텍스트는 문학적으로 더 뛰어나고 의미도 정확히 전달하지만, 참조 텍스트와 겹치는 단어가 거의 없습니다. BLEU 점수는 0점에 가깝게 나올 것입니다.</li>
<li><strong>임팩트:</strong> 이러한 위음성 판정은 모델의 성능 향상을 저해합니다. 모델이 더 똑똑해지고 어휘력이 풍부해질수록 기존의 단순한 참조 텍스트와는 멀어지게 되는데, 이를 ’성능 하락’으로 오판하게 만들어 개발자가 오히려 모델을 멍청하게(Dumbing down) 만들도록 유도하는 잘못된 피드백 루프를 형성합니다.</li>
</ul>
<h4>1.2.3  코드 생성(Code Generation)에서의 실패: 구문 vs 기능</h4>
<p>제미나이의 주요 활용처 중 하나인 코드 생성 작업은 텍스트 유사도 지표가 오라클로서 얼마나 무력한지를 가장 극명하게 보여주는 사례입니다. 코드의 세계에서는 **구문(Syntax)**이 달라도 **기능(Semantics/Functionality)**은 완벽하게 동일할 수 있기 때문입니다.</p>
<p>Hendrycks et al. (2021)과 Ren et al. (2020)의 연구에 따르면, 코드 생성 모델의 성능이 향상될수록(즉, 더 복잡하고 어려운 프로그래밍 문제를 풀 수 있게 될수록) BLEU 점수는 오히려 떨어지는 역설적인 현상이 관찰되었습니다. 이는 더 똑똑한 모델일수록 정형화된 정답 코드(Canonical Solution)와는 다른, 더 효율적이거나 파이썬스러운(Pythonic) 독창적인 해결책을 제시하기 때문입니다.</p>
<ul>
<li><strong>변수명 변경:</strong> 변수명을 <code>result</code>에서 <code>output</code>으로 바꾸기만 해도 BLEU 점수는 급락하지만, 코드의 실행 결과에는 아무런 영향이 없습니다.</li>
<li><strong>구조적 차이:</strong> <code>for</code> 루프를 사용한 코드와 <code>list comprehension</code>을 사용한 코드는 텍스트상으로는 완전히 다르게 보이지만, 기능적으로는 동일할 수 있습니다.</li>
<li><strong>진정한 오라클:</strong> 코드 생성에서의 진정한 오라클은 텍스트 유사도가 아니라, <strong>단위 테스트(Unit Test)의 통과 여부</strong>와 같은 **기능적 정확성(Functional Correctness)**이어야 합니다. <code>Pass@k</code>와 같은 실행 기반 지표가 BLEU를 대체해야 하는 이유가 여기에 있습니다. 텍스트가 얼마나 비슷한지가 아니라, 코드가 실제로 돌아가는지가 중요하기 때문입니다.</li>
</ul>
<p><img src="./2.6.1.0.0%20BLEU%20ROUGE%20METEOR%20%EB%93%B1%20%EC%A0%84%ED%86%B5%EC%A0%81%20NLP%20%EC%A7%80%ED%91%9C%EC%9D%98%20%ED%95%9C%EA%B3%84%EC%99%80%20%EC%98%A4%EB%9D%BC%ED%81%B4%EB%A1%9C%EC%84%9C%EC%9D%98%20%EB%B6%80%EC%A0%81%ED%95%A9%EC%84%B1.assets/image-20260217161326925.jpg" alt="image-20260217161326925" /></p>
<p>두 Python 코드 스니펫은 기능적으로 완벽히 동일(Functional Equivalence)하여 단위 테스트를 모두 통과하지만, 변수명과 구문 구조의 차이로 인해 BLEU 점수는 매우 낮게 측정됩니다. 이는 텍스트 유사성 지표가 코드 평가 오라클로  부적합함을 증명합니다.</p>
<p>​          Data sources: <a href="https://arxiv.org/html/2402.03130v3">Hendrycks et al. (Arxiv)</a>, <a href="https://www.walturn.com/insights/measuring-the-performance-of-ai-code-generation-a-practical-guide">Walturn Insights</a></p>
<h4>1.2.4  논리적 추론(Reasoning)과 CoT(Chain-of-Thought)의 평가 난제</h4>
<p>제미나이와 같은 최신 LLM은 복잡한 문제를 해결하기 위해 <strong>생각의 사슬(Chain-of-Thought, CoT)</strong> 프롬프팅을 사용합니다. 이 과정에서 모델은 최종 정답에 도달하기 위한 중간 추론 단계를 생성합니다.</p>
<ul>
<li><strong>추론 과정의 다양성:</strong> 수학 문제 증명이나 논리 퀴즈를 풀 때, 정답에 도달하는 길은 하나가 아닙니다. 중간 추론 과정이 참조 텍스트와 다르다고 해서 틀린 추론은 아닙니다. 오히려 모델이 더 창의적인 지름길을 찾아낼 수도 있습니다. 그러나 n-gram 지표는 참조 텍스트에 있는 특정 추론 단계가 생략되거나 순서가 바뀌면 가차 없이 점수를 깎습니다.</li>
<li><strong>버보시티 편향(Verbosity Bias)과 게임의 법칙:</strong> LLM은 종종 사용자에게 친절하게 설명하려다 보니 장황하게(Verbose) 답변하는 경향이 있습니다. 재미있는 점은, 이러한 장황함이 재현율(Recall) 기반 지표인 ROUGE 점수를 인위적으로 높이는 결과를 낳을 수 있다는 것입니다. 내용은 빈약한데 말만 많아서 참조 텍스트의 단어들을 우연히 많이 포함하게 되면, 품질이 나쁨에도 불구하고 높은 점수를 받습니다. 이는 모델 개발자가 “길게 말할수록 점수가 잘 나온다“는 잘못된 신호를 받아, 모델을 불필요하게 수다스럽게 튜닝하게 만드는 부작용을 낳습니다.</li>
</ul>
<h3>1.3  의미론적 뉘앙스와 ‘불쾌한 골짜기’</h3>
<p>전통적 지표들이 놓치는 것은 단순히 ’단어의 불일치’가 아닙니다. 그들은 언어가 가진 깊이, 즉 의미(Semantics), 화용(Pragmatics), 그리고 문화적 맥락(Context)을 놓치고 있습니다.</p>
<h4>1.3.1  파라프레이징(Paraphrasing)과 의미의 보존</h4>
<p>언어의 가장 큰 특징 중 하나는 동일한 의미를 수만 가지 다른 방식으로 표현할 수 있다는 점입니다. 이를 파라프레이징이라고 합니다.</p>
<ul>
<li>
<p>문장 A: “그 회사의 3분기 실적이 예상을 뛰어넘었다.”</p>
</li>
<li>
<p>문장 B: “3분기 어닝 서프라이즈를 기록하며 시장의 기대를 상회했다.”</p>
</li>
</ul>
<p>이 두 문장은 비즈니스 맥락에서 완벽하게 같은 정보를 전달합니다. 하지만 어휘적 교집합은 거의 없습니다. 전통적 지표는 이 두 문장을 ’다른 내용’으로 간주합니다. 이처럼 **의미적 중복(Semantic Overlap)**은 매우 크지만 **어휘적 중복(Lexical Overlap)**은 매우 작은 영역, 이 부분이 바로 전통적 지표가 실패하는 ’사각지대’입니다.</p>
<h4>1.3.2  텍스트의 ‘불쾌한 골짜기(Uncanny Valley)’</h4>
<p>반대로, 어휘적 중복은 매우 높지만 사람이 읽었을 때 “뭔가 이상하다“고 느끼는 경우도 있습니다. 이는 기계 번역 초기 시절 자주 목격되던 현상으로, 문법은 대충 맞고 단어도 다 들어가 있지만, 전체적인 흐름이 어색하거나 문체(Tone)가 맞지 않는 경우입니다.</p>
<ul>
<li><strong>스타일과 톤:</strong> 제미나이에게 “친근한 어조로 이메일을 써줘“라고 요청했을 때, 모델이 아주 딱딱한 관료적 어조로 썼지만 내용은 정확하다면? 내용은 맞으니 n-gram 점수는 높게 나올 수 있지만, 사용자의 의도(Intent)는 만족시키지 못한 실패한 생성입니다. 전통적 지표는 이러한 ’스타일의 불일치’를 전혀 감지하지 못합니다.</li>
</ul>
<p><img src="./2.6.1.0.0%20BLEU%20ROUGE%20METEOR%20%EB%93%B1%20%EC%A0%84%ED%86%B5%EC%A0%81%20NLP%20%EC%A7%80%ED%91%9C%EC%9D%98%20%ED%95%9C%EA%B3%84%EC%99%80%20%EC%98%A4%EB%9D%BC%ED%81%B4%EB%A1%9C%EC%84%9C%EC%9D%98%20%EB%B6%80%EC%A0%81%ED%95%A9%EC%84%B1.assets/image-20260217161403450.jpg" alt="image-20260217161403450" /></p>
<p>전통적 지표는 두 문장 간의 교집합 (n-gram match)만을 봅니다. 발면, 실제 의미론적 영역에서는 두 문장이 거의 완벽하게 겹침에도 불구하고, 어휘적(Lexical) 교집합이 작다는 이유로 낮은 평가를 받게 되는 구조적 한계를 시각화 하였습니다.</p>
<h3>1.4  결론: “게으른 평가“를 넘어서</h3>
<p>BLEU, ROUGE, METEOR는 NLP의 암흑기를 밝혀준 등불이었습니다. 그들이 있었기에 기계 번역과 요약 기술이 정량적으로 측정되고 발전할 수 있었습니다. 좁은 도메인이나 모델의 훈련 과정에서 빠른 온전성 검사(Sanity Check) 용도로는 여전히 유용할 수 있습니다.</p>
<p>그러나 제미나이와 같이 범용적인 능력(General Capability)을 가진 인공지능을 평가하거나, 이를 사용하는 애플리케이션의 품질을 보증하는 <strong>최종 오라클</strong>로서 이들 지표를 사용하는 것은 더 이상 유효하지 않습니다. 이는 일종의 “게으른 평가(Lazy Evaluation)“입니다. 텍스트의 깊은 의미를 이해하려 하지 않고, 표면적인 글자 맞추기에 안주하려는 태도이기 때문입니다.</p>
<p>이 지표들은 텍스트의 <strong>표면적 유사성</strong>만을 측정할 뿐, <strong>내재적 진실성(Faithfulness)</strong>, <strong>논리적 일관성(Coherence)</strong>, **기능적 유용성(Utility)**을 전혀 보증하지 못합니다. 따라서 우리는 이제 2.6.2절에서 다룰 ’LLM-as-a-Judge(심판으로서의 LLM)’나 임베딩 기반의 의미론적 평가 지표(BERTScore 등), 그리고 도메인 특화된 기능 테스트(Functional Testing)와 같은 새로운 차원의 평가 방법론으로 시급히 전환해야 합니다.</p>
<p>평가의 본질은 ’단어가 얼마나 겹치느냐’가 아니라, **‘모델이 사용자의 의도대로 생각하고 행동했느냐’**에 있기 때문입니다. 이제 우리는 자를 바꾸어야 할 때입니다.</p>
<h2>2. 참고 자료</h2>
<ol>
<li>Bleu: a Method for Automatic Evaluation of Machine Translation, https://aclanthology.org/P02-1040/</li>
<li>BLEU - Wikipedia, https://en.wikipedia.org/wiki/BLEU</li>
<li>A Gentle Introduction to Calculating the BLEU Score for Text in Python, https://machinelearningmastery.com/calculate-bleu-score-for-text-python/</li>
<li>Understanding BLEU and ROUGE score for NLP evaluation, https://www.geeksforgeeks.org/nlp/understanding-bleu-and-rouge-score-for-nlp-evaluation/</li>
<li>BiLingual Evaluation Understudy (BLEU) Score Cheat Sheet, https://tutorialsdojo.com/what-is-bilingual-evaluation-understudy-bleu-score-for-machine-translation/</li>
<li>Understanding MT Quality: BLEU Scores | by K Vashee - Medium, https://kvashee.medium.com/understanding-mt-quality-bleu-scores-9a19ed20526d</li>
<li>Demystifying the BLEU Metric: A Comprehensive Guide to Machine, https://www.traceloop.com/blog/demystifying-the-bleu-metric</li>
<li>Key NLP Evaluation Metrics: Bleu, Rouge and more! - Datumo-All in …, https://datumo.com/blog/insight/key-nlp-evaluation-metrics/</li>
<li>How to Use ROUGE Metric for AI Summarization Quality | Galileo, https://galileo.ai/blog/rouge-metric</li>
<li>Two minutes NLP — Learn the ROUGE metric by examples - Medium, https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499</li>
<li>ROUGE - A Metric for Summarization Evaluation, <a href="https://systems-analysis.ru/eng/ROUGE_(metric)">https://systems-analysis.ru/eng/ROUGE_%28metric%29</a></li>
<li>LLM Evaluation For Text Summarization - Neptune.ai, https://neptune.ai/blog/llm-evaluation-text-summarization</li>
<li>[PDF] METEOR: An Automatic Metric for MT Evaluation with, https://www.semanticscholar.org/paper/METEOR%3A-An-Automatic-Metric-for-MT-Evaluation-with-Banerjee-Lavie/7533d30329cfdbf04ee8ee82bfef792d08015ee5</li>
<li>METEOR: An Automatic Metric for MT Evaluation with Improved, https://aclanthology.org/W05-0909/</li>
<li>Comprehensive 10+ LLM Evaluation: From BLEU, ROUGE, and, https://bobrupakroy.medium.com/comprehensive-10-llm-evaluation-from-bleu-rouge-and-meteor-to-scenario-based-metrics-like-9f6602c92c17</li>
<li>What is Metric for Evaluation of Translation with Explicit ORdering?, https://tutorialsdojo.com/what-is-metric-for-evaluation-of-translation-with-explicit-ordering/</li>
<li>METEOR - Wikipedia, https://en.wikipedia.org/wiki/METEOR</li>
<li>Automated Test Oracles: State of the Art, Taxonomies and Trends, https://www.researchgate.net/publication/267329731_Automated_Test_Oracles_State_of_the_Art_Taxonomies_and_Trends</li>
<li>Constructing automated test oracle for low observable software, https://scientiairanica.sharif.edu/article_21524_06f943a06c6c19a8171e00f781b6ef4e.pdf</li>
<li>Challenges in Testing Large Language Model Based Software - arXiv, https://arxiv.org/html/2503.00481v2</li>
<li>What are the limitations of the BLEU algorithm in machine, https://www.tencentcloud.com/techpedia/120063</li>
<li>Test Oracles - Gregory Gay, https://greg4cr.github.io/courses/spring16csce747/Lectures/Spring16-Lecture11TestOracles.pdf</li>
<li>The Oracle Problem and the Teaching of Software Testing, https://kaner.com/?p=190</li>
<li>User Centric Evaluation of Code Generation Tools - arXiv, https://arxiv.org/html/2402.03130v3</li>
<li>Measuring the Performance of AI Code Generation: A Practical Guide, https://www.walturn.com/insights/measuring-the-performance-of-ai-code-generation-a-practical-guide</li>
<li>CodeScore: Evaluating Code Generation by Learning Code Execution, <a href="https://openreview.net/forum?id=OeOxLvDUuW&amp;noteId=NPecPXWgRZ">https://openreview.net/forum?id=OeOxLvDUuW¬eId=NPecPXWgRZ</a></li>
<li>Utilising LLM-as-a-Judge to Evaluate LLM-Generated Code - Medium, https://medium.com/softtechas/utilising-llm-as-a-judge-to-evaluate-llm-generated-code-451e9631c713</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>