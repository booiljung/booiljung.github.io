<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2.5.3 참조 기반(Reference-based) 오라클과 무참조(Reference-free) 오라클</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2.5.3 참조 기반(Reference-based) 오라클과 무참조(Reference-free) 오라클</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">소프트웨어 공학 (Software Engineering)</a> / <a href="../../index.html">오라클: AI 주도 개발을 위한 해답</a> / <a href="../index.html">Chapter 2. 소프트웨어 테스트에서의 오라클(Oracle) 개념과 AI 시대의 역할</a> / <a href="index.html">2.5 AI 시대를 위한 오라클의 재정의: 확률에서 확정으로</a> / <span>2.5.3 참조 기반(Reference-based) 오라클과 무참조(Reference-free) 오라클</span></nav>
                </div>
            </header>
            <article>
                <h1>2.5.3 참조 기반(Reference-based) 오라클과 무참조(Reference-free) 오라클</h1>
<p>소프트웨어 테스팅 공학에서 ’오라클(Oracle)’이란 대상 시스템의 실행 결과가 올바른지, 즉 기대하는 요구사항을 충족하는지를 판별하는 결정론적 기준이자 검증 메커니즘을 의미한다. 전통적인 소프트웨어 개발 방법론에서 오라클은 입력값에 대해 수학적으로 유일하고 확정적인 정답을 반환하는 절대적인 심판관의 역할을 수행해 왔다. 그러나 인공지능, 특히 거대 언어 모델(LLM)을 아키텍처의 핵심 컴포넌트로 채택하는 현대의 AI 기반 소프트웨어 개발 패러다임이 도래하면서, 입력에 대한 모델의 출력이 고도의 확률론적(Probabilistic)이고 비결정론적(Nondeterministic)인 양태를 띠게 되었다. 동일한 프롬프트 입력에 대해서도 온도(Temperature) 파라미터나 샘플링 전략에 따라 매번 형태가 달라지는 텍스트, 코드, 그리고 추론 결과가 산출될 수 있기 때문이다.</p>
<p>이러한 생성형 AI 시스템의 복잡성은 사전 정의된 단일한 기대 결과와 실제 출력을 단순 비교하는 고전적 오라클의 효용을 급격히 붕괴시켰으며, 이를 소프트웨어 공학계에서는 ’오라클 문제(The Oracle Problem)’의 극단적 발현으로 규정한다. 결국 테스트 자동화와 품질 보증(QA)을 수행하기 위해서는 개방형(Open-ended) 응답의 다차원적 품질을 평가하면서도, 시스템의 예측 불가능성을 통제하여 실무적인 확정성을 부여할 수 있는 새로운 검증 체계가 요구된다.</p>
<p>이러한 공학적 요구에 부응하여, 현대의 AI 테스트 오라클은 기준이 되는 결정론적 정답지(Ground Truth)의 유무와 그 개입 방식에 따라 ’참조 기반(Reference-based) 오라클’과 ’무참조(Reference-free) 오라클’이라는 두 가지 거대한 패러다임으로 분화하여 발전하고 있다. 두 오라클 메커니즘은 AI 모델의 출력 품질을 평가하는 수학적 원리, 의미론적 이해의 깊이, 그리고 시스템에 부여하는 확정성의 성격이 근본적으로 다르다. 본 보고서에서는 두 오라클 패러다임의 공학적 구조와 수학적 기초를 심층적으로 해부하고, 최신 학술 연구에 나타난 각 메커니즘의 근본적인 한계를 분석하며, 최종적으로 AI 소프트웨어 신뢰성 극대화를 위해 결정론적 정답지를 다루는 실전 개발 예제들을 폭넓게 고찰한다.</p>
<h2>1. 참조 기반(Reference-based) 오라클의 구조와 공학적 메커니즘</h2>
<p>참조 기반 오라클은 AI 모델의 출력 결과를 시스템 외부에서 인간 또는 신뢰할 수 있는 다른 알고리즘에 의해 사전 정의된 ‘결정론적 정답지(Deterministic Ground Truth)’ 또는 ’참조 데이터(Reference)’와 직접적으로 대조하여 품질을 수치화하는 방법론이다. 이 메커니즘의 가장 큰 공학적 미덕은 평가 과정 자체가 확고한 결정론적 규칙 위에 서 있다는 점이다. 평가 알고리즘은 무작위성을 배제한 수학적 수식을 통해 동작하므로, 평가의 객관성이 극도로 높고 대규모 데이터셋에 대한 연산 처리 속도가 매우 빠르다는 장점을 지닌다. 따라서 모델의 출력이 일정한 형식으로 구조화되어 있거나, 정답의 범주가 엄격하게 제한되는 폐쇄형 도메인(Closed-domain) 태스크에서 품질을 통제하는 1차적 방어선으로 강력한 성능을 발휘한다.</p>
<h3>1.1 통계적 및 어휘 기반 평가의 수학적 기초</h3>
<p>참조 기반 오라클의 가장 고전적이면서도 산업계 전반에 깊게 뿌리내린 형태는 모델이 생성한 출력 텍스트와 참조 텍스트 간의 어휘적 일치도(N-gram overlap)나 편집 거리(Edit distance)를 계산하는 통계적 스코어링 방식이다. 이러한 지표들은 텍스트의 표면적 기호가 얼마나 유사한지를 측정함으로써 소프트웨어의 작동 여부를 검증한다.</p>
<p>첫째, <strong>정확도 일치(Exact Match, EM)</strong> 알고리즘은 가장 가혹하고 엄격한 형태의 참조 기반 오라클이다. AI 모델의 예측 결과가 사전에 정의된 정답지와 문자열 수준에서 완벽하게 동일한지를 이진 논리(0 또는 1)로 판별한다. 대소문자 변환, 문장 부호 제거, 선행 및 후행 공백 제거 등의 기초적인 텍스트 정규화(Text Normalization) 전처리를 거친 후 비교 연산을 수행한다. 이 방식은 금융권의 송장 번호 추출, 의료 도메인의 진단 코드 매핑, 사용자 이름 및 특정 식별자 등 의미론적 변형이나 문맥적 유연성이 절대적으로 허용되지 않는 태스크에서 결정론적 무결성을 보장하기 위해 필수적으로 채택된다.</p>
<p>둘째, <strong>BLEU (Bilingual Evaluation Understudy)</strong> 알고리즘은 본래 기계 번역 모델의 성능을 평가하기 위해 고안되었으나, 현재 생성형 AI 출력 검증에도 광범위하게 활용되는 연산 기반 지표다. BLEU의 철학은 정밀도(Precision)에 기반한다. 즉, AI 모델이 생성한 단어(N-gram)들이 참조 정답지 내에 얼마나 높은 비율로 포함되어 있는지를 평가한다. 그러나 생성 모델이 참조 정답지에 있는 특정 단어 하나만을 반복적으로 생성하여 정밀도를 인위적으로 극대화하는 편법을 방지하기 위해, 수학적으로 클리핑된 정밀도(Clipped Precision) 개념과 짧은 길이 페널티(Brevity Penalty, BP)를 적용한다.</p>
<p>수학적으로 BLEU의 N-gram 정밀도 <span class="math math-inline">P_n</span>은 다음과 같은 연산식으로 정의된다.<br />
<span class="math math-display">
P_n = \frac{\sum_{i \in \text{Candidates}} \sum_{n\text{-gram} \in i} \text{Count}_{\text{clip}}(n\text{-gram})}{\sum_{i \in \text{Candidates}} \sum_{n\text{-gram} \in i} \text{Count}(n\text{-gram})}
</span><br />
다양한 표현의 가능성을 포용하기 위해, 참조 정답지가 여러 개 존재할 경우 참조 기반 오라클은 다수의 정답 후보군과의 순차적 비교를 수행하고, 이 중 BLEU 점수를 가장 극대화할 수 있는 방향으로 최적의 매칭을 찾아내어 모델을 평가한다. 일반적으로 유니그램(1-gram)부터 트라이그램(3-gram) 이상의 다중 N-gram 조합에 대한 가중 평균을 산출하여 최종 점수를 확정한다.</p>
<p>셋째, <strong>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</strong> 알고리즘은 정밀도에 집중하는 BLEU와 달리 재현율(Recall)에 방점을 둔 참조 기반 지표로, 핵심 정보의 누락이 치명적인 문서 요약(Summarization) 태스크의 테스트 오라클로 설계되었다. 요약된 텍스트는 원본의 중요한 정보를 반드시 포함해야 하므로, AI가 생성한 요약본이 참조 정답지가 보유한 핵심 N-gram을 얼마나 누락 없이 커버하고 있는지가 평가의 핵심이 된다.</p>
<p>ROUGE-N의 정밀도와 재현율, 그리고 이를 결합한 F1 점수는 다음과 같은 연산을 통해 도출된다.<br />
<span class="math math-display">
\text{Precision} = \frac{\text{Number of matching n-grams}}{\text{Total n-grams in the generated text}}
</span></p>
<p><span class="math math-display">
\text{Recall} = \frac{\text{Number of matching n-grams}}{\text{Total n-grams in the reference}}
</span></p>
<p><span class="math math-display">
F1 = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
</span></p>
<p>더 나아가, 단순한 어휘 빈도를 넘어 문장 구조의 유사성을 판별하기 위해 가장 긴 공통 부분 수열(Longest Common Subsequence, LCS)을 응용하는 ROUGE-L이 존재한다. 이는 단어들의 순서 정보를 보존하여 평가하므로 텍스트의 구조적 정합성을 추가로 검증할 수 있다.</p>
<p>넷째, <strong>METEOR (Metric for Evaluation of Translation with Explicit Ordering)</strong> 지표는 어휘 일치도의 한계를 부분적으로 보완하기 위해 WordNet과 같은 외부 지식 데이터베이스를 연동한다. 이는 형태소 분석(Stemming)과 동의어(Synonym) 매칭을 오라클 로직 내부에 포함시킴으로써, 표면적 기호가 다르더라도 동일한 의미를 지닌 단어들을 정답으로 인정하려는 시도다.</p>
<h3>1.2 결정론적 참조 정답지의 모순과 학술적 한계</h3>
<p>참조 기반 오라클은 평가 과정이 투명하고 수학적 검증이 가능하다는 강력한 무기를 지니고 있으나, 시스템의 입력인 참조 데이터셋의 질적, 양적 한계로 인해 내재적인 모순에 직면하게 된다. AI 모델의 능력이 고도화됨에 따라 생성되는 응답은 인간의 글쓰기와 유사하게 다채롭고 창의적인 의미 구조를 형성한다. 그러나 N-gram 중심의 표면적 일치도에 과도하게 의존하는 통계적 오라클은 이러한 문맥적 뉘앙스와 의미론적 등가성(Semantic equivalence)을 이해하지 못한다.</p>
<p>결과적으로 AI가 생성한 응답이 참조 정답지와 의미상 완벽히 일치하더라도, 사용된 어휘나 문장의 구조(Syntax)가 다를 경우 오라클은 이를 ’오답’으로 처리하는 치명적인 거짓 경보(False alarm)를 대량으로 발생시킨다. 이는 개발자로 하여금 시스템이 고장 났다고 오판하게 만들며 불필요한 디버깅 비용을 초래한다.</p>
<p>이러한 참조 기반 오라클의 자기모순적 한계는 EMNLP 2024 학술대회에 발표된 기념비적인 논문 <em>Reference-based Metrics Disprove Themselves in Question Generation</em>에서 매우 과학적이고 적나라하게 논증되었다. 연구진은 특정 답변을 유도하는 질문을 생성하는 ‘질문 생성(Question Generation, QG)’ 벤치마크 테스트에서 기존의 오라클들이 어떻게 붕괴하는지를 추적했다. SQuAD 및 HotpotQA와 같은 대규모 벤치마크 데이터셋은 데이터 구축 비용의 한계로 인해 질문당 단 하나의 인간 작성 정답지(Single reference)만을 보유하고 있다.</p>
<p>연구진은 이 환경을 복제하여 인간이 검증한 새로운 고품질 정답지를 데이터셋에 추가로 투입한 후, 모델이 생성한 텍스트와 함께 참조 기반 오라클(BLEU, BERTScore 등)로 채점하는 실험을 수행했다. 논리적으로 건전한 오라클이라면, 새롭게 추가된 인간 검증 질문(Valid question)에 대해, 문맥을 벗어난 불량한 질문(Flawed question)보다 당연히 더 높은 점수를 부여해야 한다. 그러나 충격적이게도 참조 기반 오라클들은 원래의 단일 정답지와 어휘가 몇 개 겹친다는 이유만으로 문법적으로 파괴되거나 의미가 없는 불량 텍스트에 더 높은 점수를 부여하는 결과를 낳았다. 기존 지표들 자체가 스스로의 신뢰성을 부정(Disprove)하는 결과를 초래한 것이다.</p>
<p>이 현상은 소프트웨어 테스트에서 오라클의 권위가 실추되는 ’오라클 문제’의 전형을 보여준다. 이를 완화하기 위해서는 가능한 모든 텍스트 변형을 포괄할 수 있는 수십, 수백 개의 참조 정답지를 구축해야 하지만, 이는 데이터 어노테이션에 막대한 시간과 비용을 요구하므로 대규모 엔터프라이즈 환경에서 지속 가능하지 않다. 결국, 사실상 무한대에 가까운 생성형 AI의 출력 공간을 유한한 결정론적 참조 데이터셋으로 덮으려는 시도 자체가 수학적이고 공학적인 한계에 부딪힌 것이다.</p>
<h2>2. 무참조(Reference-free) 오라클의 부상과 의미론적 평가 메커니즘</h2>
<p>사전 정의된 단일 정답지가 지니는 표면적 한계와 거짓 경보의 늪에서 벗어나기 위해 소프트웨어 공학계가 도입한 혁신적 대안이 바로 ’무참조(Reference-free) 오라클’이다. 무참조 평가는 외부의 결정론적 정답지에 의존하지 않고, 입력된 사용자 프롬프트(Prompt), 지식 베이스에서 검색된 문맥(Context), 그리고 모델이 최종적으로 생성한 출력(Output) 자체의 상호 관계와 텍스트의 내재적 특성을 분석하여 품질을 검증하는 독립적 메커니즘이다.</p>
<p>최근 발표된 심층 연구 논문 <em>Reference-free Evaluation Metrics for Text Generation: A Survey</em>에 따르면, 자연어 생성(NLG) 기술이 진보함에 따라 모델이 범하는 오류의 형태가 어색한 문법 수준을 넘어 환각, 미묘한 사실 관계 왜곡, 문맥 불일치 등으로 교묘해지고 있다. 참조 기반 오라클이 이러한 고차원적 오류를 탐지하는 데 무력화되자, 문맥 자체를 평가의 ’기준점’으로 삼는 무참조 자동 평가 방식의 수요가 폭발적으로 증가하고 있다. 무참조 오라클은 기계 번역과 같은 제한적 도메인을 넘어, 개방형 질문 응답(QA), 대화 생성(Dialogue Generation), 이야기 생성(Story Generation) 등 수학적 정답을 단 하나로 규정할 수 없는 다변적 태스크에서 시스템의 신뢰성을 확보하는 필수 불가결한 검증 프레임워크로 자리 잡았다.</p>
<h3>2.1 LLM-as-a-Judge 기법과 오라클의 확률적 전이</h3>
<p>무참조 오라클을 구현하는 가장 강력하고 대중적인 엔지니어링 형태는 고성능 거대 언어 모델을 직접 테스트 심판(Judge)으로 기용하는 ‘LLM-as-a-Judge’ 패러다임이다. 전통적인 통계적 오라클이 시스템 외부에 존재하는 고정된 수학 공식이었다면, LLM-as-a-Judge는 AI를 검증하기 위해 또 다른 AI의 인지적, 추론적 능력을 차용하는 메타 검증(Meta-verification) 구조를 취한다.</p>
<p>이 방식은 평가 대상이 되는 시스템의 출력을 GPT-4, Claude와 같은 파라미터 규모가 방대한 교사 모델(Teacher Model)에 프롬프트와 함께 입력하고, 인간 전문가가 작성한 것과 유사한 다면적 평가 루브릭(Rubric)에 따라 채점 및 피드백을 생성하도록 지시한다. 자연어 추론(Natural Language Inference, NLI) 모델에 기반한 초기 형태의 무참조 통계 스코어러들은 문맥의 표면적 모순을 찾는 데 그쳤으나, LLM-as-a-Judge는 문장의 미묘한 뉘앙스, 숨겨진 전제, 그리고 지시 사항(Instruction)의 준수 여부까지 심층적으로 이해할 수 있다. 이는 참조 기반 모델의 가장 큰 단점인 ’의미론적 맹점’을 완벽하게 보완하며, 인간 평가자의 판단과 놀라울 정도로 높은 일치율(Alignment)을 보여준다.</p>
<p>그러나 이 강력한 무참조 오라클 역시 결정론적 관점에서는 치명적인 리스크를 내포하고 있다. 판단을 내리는 오라클 자체가 확률적 모델이기 때문에, 완벽히 동일한 대상에 대해서도 시점이나 프롬프트 미세 조정에 따라 일관성 없는 평가를 내리는 변동성(Flakiness) 문제가 존재한다. 특히, 최신 연구인 논문 <em>Investigating Non-Transitivity in LLM-as-a-Judge</em>에서는 LLM 기반 심판 모델이 A 응답이 B보다 낫고, B 응답이 C보다 낫다고 평가했음에도 불구하고, C 응답이 A보다 낫다고 상호 모순되는 판단을 내리는 ‘비추이성(Non-transitivity)’ 결함을 겪을 수 있음을 엄중히 경고한다.</p>
<p>나아가 LLM 오라클은 무의식적으로 길이가 긴 답변에 높은 점수를 부여하는 장황함 편향(Verbosity bias), 그리고 프롬프트 내에 먼저 제시된 텍스트를 선호하는 위치 편향(Position bias) 등 구조적 결함을 지니고 있다. 이를 통제하기 위해 실무에서는 질문 응답 쌍(QAG: Question Answer Generation)을 활용하거나, 앙상블 기법, 점진적 추론 그래프(Directed Acyclic Graph, DAG)를 결합하여 확률적 변동성을 최소화하려는 공학적 노력이 동반되고 있다.</p>
<h3>2.2 RAG(Retrieval-Augmented Generation) 환경에서의 무참조 프레임워크</h3>
<p>기업 데이터 보안과 지식 기반 응답의 정확성을 담보하기 위해 널리 채택되는 RAG 시스템은 무참조 오라클 메커니즘이 가장 체계적으로 적용되는 영역이다. RAG 파이프라인은 외부 문서 저장소에서 관련 문맥을 검색(Retrieval)하고 이를 프롬프트에 병합하여 텍스트를 생성(Generation)한다. 이 과정에서 사용자가 어떤 질문을 던질지 사전 예측이 불가능하므로, 정해진 정답지 없이 동적으로 텍스트 품질을 검증하는 RAGAS(RAG Assessment)와 같은 특화된 무참조 평가 프레임워크가 필수적으로 도입된다.</p>
<p>RAGAS 프레임워크는 RAG 파이프라인의 각 단계가 정상 작동하는지 수치화하기 위해 수학적이고 논리적인 무참조 지표들을 다음과 같이 고안했다.</p>
<p>첫째, <strong>충실성 (Faithfulness)</strong> 지표는 생성 모델의 정보 왜곡, 즉 환각(Hallucination) 현상을 차단하기 위한 핵심 오라클이다. 생성된 응답이 외부 지식의 개입 없이 오직 ’검색된 문맥(Context)’에서만 연역적으로 도출될 수 있는지를 검증한다. 평가 과정은 두 단계의 자동화 파이프라인으로 구성된다. 우선 LLM을 통해 모델의 응답 텍스트를 분해하여 개별적인 사실 기반의 주장(Claims)들을 추출한다. 그 후, 각 주장이 검색된 문맥 문서에 의해 뒷받침되는지(Entailment) 혹은 모순되거나 발견되지 않는지를 판별한다.</p>
<p>평가 수식은 다음과 같이 정의된다.<br />
<span class="math math-display">
\text{Faithfulness Score} = \frac{\text{Number of valid claims supported by context}}{\text{Total number of claims in the generated response}}
</span><br />
이 비율이 1에 가까울수록 모델은 주어진 제약 조건 내에서 안전하게 동작하는 것으로 수학적으로 보증된다.</p>
<p>둘째, <strong>답변 적합성 (Answer Relevancy)</strong> 지표는 생성된 텍스트가 사용자의 최초 질문(Prompt)에 얼마나 밀도 있게, 그리고 누락 없이 대응하고 있는지를 측정한다. 텍스트가 유창하고 문맥에 충실하더라도, 질문의 본질을 회피하거나 동문서답을 하는 경우를 걸러내기 위한 오라클이다.</p>
<p>RAGAS 프레임워크는 이를 직접 평가하는 대신 고도의 역공학(Reverse-engineering) 메커니즘을 사용한다. 모델이 생성한 ‘응답’ 자체를 입력으로 삼아, 이 응답을 이끌어낼 수 있는 여러 개의 가상 질문(Artificial questions)들을 역으로 생성해 낸다. 이후 생성된 가상 질문들이 원본 사용자 질문과 의미 공간(Semantic space)에서 얼마나 맞닿아 있는지 임베딩 벡터 간의 코사인 유사도(Cosine Similarity)를 계산하여 평균을 낸다.</p>
<p>수학적 정의는 다음과 같다.<br />
<span class="math math-display">
\text{Answer Relevancy} = \frac{1}{N} \sum_{i=1}^{N} \frac{E_{g_i} \cdot E_o}{\vert E_{g_i} \vert \vert E_o \vert}
</span><br />
여기서 <span class="math math-inline">N</span>은 생성된 가상 질문의 수(기본값 3 등)이며, <span class="math math-inline">E_{g_i}</span>는 <span class="math math-inline">i</span>번째 생성된 가상 질문의 임베딩 벡터, <span class="math math-inline">E_o</span>는 원본 사용자 질문의 임베딩 벡터를 나타낸다. 코사인 유사도 값이 높을수록, 해당 응답은 원래의 질문 의도를 정확하게 관통하고 있음을 증명한다.</p>
<p>셋째, <strong>문맥 관련성(Context Relevance) 및 문맥 정밀도(Context Precision)</strong> 지표는 텍스트 생성 전 단계인 검색기(Retriever)의 알고리즘을 평가하는 무참조 오라클이다. 검색된 컨텍스트 문단 내에서 사용자의 질문에 답변하는 데 유용한 정보(Signal) 문장과 불필요한 노이즈(Noise) 문장의 비율을 도출하여 검색 엔진의 밀도와 정밀도를 판단한다.</p>
<h3>2.3 다차원 무참조 검증 모델의 구조적 진화</h3>
<p>초기 무참조 평가 방식들이 단일한 의미적 유사도나 논리적 함의(Entailment)에 집착했다면, 최신의 무참조 오라클은 텍스트의 다차원적 품질 속성을 동시에, 그리고 입체적으로 분해하여 평가하는 구조로 진화하고 있다.</p>
<p>앞서 언급한 EMNLP 2024 논문 <em>Reference-based Metrics Disprove Themselves in Question Generation</em>에서 연구진은 참조 기반 지표의 필연적인 붕괴를 극복하기 위한 대안으로, 인간의 평가 메커니즘을 모사한 새로운 무참조 평가 지표인 NACo 프레임워크를 제안했다. 이 지표는 표면적 참조 데이터를 완전히 버리고, 생성된 질문 텍스트 자체의 내재적 완결성을 다음 세 가지 차원으로 해체하여 독립적으로 채점한다.</p>
<ol>
<li><strong>자연스러움 (Naturalness):</strong> 텍스트가 문법적 오류 없이 원어민이 발화하는 것처럼 유창(Fluency)하게 구성되었는가?</li>
<li><strong>답변 가능성 (Answerability):</strong> 제시된 문맥과 정답 영역을 기반으로, 해당 질문에 대한 확정적인 답변을 도출해 낼 수 있도록 논리적으로 설계되었는가?</li>
<li><strong>복잡성 (Complexity):</strong> 단순한 정보의 복사를 넘어, 텍스트 전반에 대한 추론(Inferencing)과 지식의 통합적 종합(Synthesizing)을 요구하는 고차원적 인지 구조를 띠고 있는가?</li>
</ol>
<p>실험 결과, 이 다차원 무참조 지표는 의미가 훼손된 불량 텍스트와 창의적인 고품질 텍스트를 정확히 구분해 냈으며, 인간 평가자의 주관적 판단과 최고 수준의 일치도를 달성하며 기존 메트릭들을 압도했다. 이는 오라클 시스템에 물리적인 정답지가 존재하지 않더라도, 다차원의 평가 기준(Rubric)이 수학적, 언어학적으로 정교하게 설계되고 이를 뒷받침할 강력한 추론 모델이 결합된다면 무참조 방식이 전통적인 검증의 한계를 거뜬히 뛰어넘을 수 있음을 시사한다.</p>
<h2>3. 참조 기반 오라클과 무참조 오라클의 아키텍처 및 공학적 트레이드오프 비교</h2>
<p>AI 소프트웨어 공학의 실제 현장에서 두 오라클 패러다임은 상호 배타적인 대체재가 아니라, 시스템의 결함 유형을 교차로 필터링하는 강력한 상호 보완적 자산이다. 프로젝트의 비즈니스 임팩트, 요구되는 출력의 결정론적 성격, 시스템에 허용된 추론 시간(Latency), 그리고 컴퓨팅 예산에 따라 단일 오라클 메커니즘을 채택하거나 하이브리드(Hybrid) 파이프라인으로 융합해야 한다.</p>
<p>아래의 표는 두 가지 오라클 패러다임이 소프트웨어 검증 과정에서 나타내는 기술적 특성과 한계를 수학적, 공학적 관점에서 심층 비교한 것이다.</p>
<table><thead><tr><th><strong>시스템 비교 속성</strong></th><th><strong>참조 기반(Reference-based) 오라클</strong></th><th><strong>무참조(Reference-free) 오라클</strong></th></tr></thead><tbody>
<tr><td><strong>철학적 및 알고리즘 기반</strong></td><td>생성된 출력과 인간이 검증을 완료한 고정된 ‘결정론적 정답지’ 간의 기계적 대조 연산</td><td>입력 프롬프트, 검색 문맥, 생성 응답 간의 다이내믹한 연관성 및 내재적 텍스트 구조 독립 평가</td></tr>
<tr><td><strong>핵심 평가 지표 (Metrics)</strong></td><td>Exact Match (EM), BLEU, ROUGE (N/L), METEOR, Levenshtein Distance</td><td>RAGAS (Faithfulness, Answer Relevancy), LLM-as-a-Judge, NACo 지표군</td></tr>
<tr><td><strong>의미론적 추론 및 이해도</strong></td><td>매우 낮음. 텍스트의 형태소나 N-gram 기호 일치 여부에 절대적으로 의존하여 동의어 활용이나 문장 구조 역전에 매우 취약함</td><td>매우 높음. 고차원 임베딩 공간 매핑과 언어 모델의 자기회귀적 추론을 통해 행간의 문맥적 뉘앙스와 의도를 깊이 있게 파악함</td></tr>
<tr><td><strong>결정론 및 재현성(Determinism)</strong></td><td>완전한 결정론적(Deterministic) 아키텍처. 연산 알고리즘이 고정되어 있어 수백 번을 실행해도 항상 소수점 단위까지 완벽히 동일한 평가 점수를 반환함</td><td>본질적으로 확률론적(Probabilistic) 아키텍처. 판단을 내리는 LLM 심판 모델의 파라미터나 프롬프트 미세 조정에 따라 평가 점수의 변동(Flakiness) 위험이 항상 내재됨</td></tr>
<tr><td><strong>테스트 수행 파이프라인 속도</strong></td><td>통계 모델과 정형 수학 공식을 사용하므로 파이프라인 병목이 없으며, 밀리초(ms) 단위의 초고속 대규모 일괄 평가가 가능함</td><td>거대한 파라미터를 가진 신경망 모델의 추론(Inference) 연산을 수행해야 하므로 컴퓨팅 코스트가 높고 평가 소요 시간이 상당히 긺</td></tr>
<tr><td><strong>주요 결함 및 실패(Risk) 유형</strong></td><td><strong>거짓 경보(False alarm):</strong> 출력된 텍스트의 본질적 의미가 정답과 완벽히 일치하더라도, 텍스트 형태가 다르면 시스템이 오작동한 것으로 판별해 오답 처리함</td><td><strong>누락 및 편향(Miss &amp; Bias):</strong> LLM 심판 모델 스스로가 환각에 빠지거나 특정 응답(장황함 등)을 선호하는 편향을 보여, 심각한 오류가 있는 출력을 만점으로 통과시킴</td></tr>
<tr><td><strong>수학적 및 확률적 검증 관점</strong></td><td>참조 데이터에 대한 수식적 일치도의 극대화 증명: <span class="math math-inline">P(Output \vert Reference) \approx 1</span></td><td>문맥과 지시어에 기반한 논리적 정합성 한계점 통과 여부 검증: <span class="math math-inline">P(Output \vert Context) \ge Threshold</span></td></tr>
<tr><td><strong>산업계 최적 사용 사례 (Use Case)</strong></td><td>SQL 쿼리 구문 일치도 검증, JSON/XML 구조 매칭 확인, 특정 비즈니스 엔티티(송장 번호, 시리얼 넘버) 추출 및 분류</td><td>고객 응대 챗봇의 대화 유창성, 문서 요약 시스템의 사실성 검증, RAG 시스템의 환각(Hallucination) 방어선 구축</td></tr>
</tbody></table>
<p>위의 구조적 비교에서 명확히 드러나듯, 참조 기반 평가가 소프트웨어 엔지니어들에게 제공하는 가장 위대한 가치는 평가 과정 자체의 ’무결성과 투명성’에 있다. 평가 스크립트에 버그가 없는 한 오라클의 판정 기준은 절대 변하지 않으므로, 시스템 업그레이드 전후로 리그레션 테스트(Regression testing)를 수행하거나 CI/CD 파이프라인을 자동화할 때 엔지니어에게 심리적, 기술적 확신을 준다. 반면에 무참조 평가는 최신 AI 모델을 인간 피드백 기반 강화학습(RLHF) 기법 등으로 훈련시키는 과정과 평가 철학이 완벽히 부합하며, 정답을 고정할 수 없는 다이내믹한 비즈니스 애플리케이션 환경에서 실제 사용자가 체감하는 품질을 대변할 수 있다는 독보적인 무기를 갖는다.</p>
<p>이러한 공학적 트레이드오프를 조율하기 위해, 현대 엔터프라이즈 AI 테스트 아키텍처를 설계할 때는 평가 지표를 목적 없이 남발하지 말고 5개 이하로 엄선하여 파이프라인의 복잡성을 관리하는 이른바 ‘5 Metric Rule’ 전략이 통용된다. 즉, 특정 포맷의 유지나 정확한 수치 추출 등 기계적 정밀도(Precision)가 생명인 모듈에는 EM이나 ROUGE와 같은 참조 기반 지표를 배치하고, 문장 생성이나 의도 파악과 같은 모듈에는 무참조 기반의 LLM-as-a-Judge 지표를 적절히 배합하여 오라클 간의 크로스체크(Cross-check)를 강제하는 하이브리드 검증 시스템이 가장 안전한 선택지로 꼽힌다.</p>
<h2>4. 비결정성 제어를 위한 오라클 메커니즘 실전 적용 사례</h2>
<p>AI 모델이 태생적으로 가진 확률적 비결정성(Nondeterminism)을 제어하고, 기존의 소프트웨어들이 자랑하던 무결점에 가까운 엔터프라이즈 수준의 신뢰성을 확보하기 위해, 실무 개발 현장에서는 이 두 가지 오라클의 특성을 교묘하게 배합한 검증 메커니즘을 겹겹이 쌓아 올린다. 시스템의 특정 단계에 ’결정론적 정답지’를 강제로 주입하거나, 외부의 검증 도구를 실시간으로 연동하여 오라클 문제를 원천적으로 봉쇄하는 아키텍처는 AI 시스템을 프로토타입 수준에서 실제 상용 서비스(Production) 수준으로 끌어올리는 핵심 동력이 된다.</p>
<p>이하에서는 다양한 비즈니스 도메인에서 참조 기반 오라클과 무참조 오라클이 어떻게 결합되어 결정론적이고 안정적인 소프트웨어 테스트 환경을 구축하는지 구체적인 실전 예제를 통해 살펴본다.</p>
<h3>4.1 실전 예제 1: 재무 및 회계 처리 자동화를 위한 구조적 데이터 추출 시스템 검증 (참조 기반 Exact Match)</h3>
<p>기업의 회계 부서에서 수만 통의 고객 이메일과 첨부된 비정형 PDF 계약서로부터 특정 식별 코드, 세금 계산서 번호(Invoice Number), 그리고 결제 금액만을 정확히 추출하여 데이터베이스에 적재하는 AI 시스템을 개발한다고 가정하자. 이러한 형태의 폐쇄형 정보 추출(Closed-domain extraction) 태스크에서는 문장의 유창성이나 논리력은 전혀 가치가 없으며, 오직 ’틀림없는 정확한 값’을 뽑아내는 능력만이 시스템의 생사를 결정한다. 한자리의 오타나 기호의 누락이 막대한 재무적 손실로 이어질 수 있기 때문이다.</p>
<p>이 시스템의 품질을 흔들림 없이 보증하기 위해, 개발팀은 과거의 실제 처리 데이터 수천 건을 바탕으로 인간 전문가가 직접 교차 검증을 마친 ’결정론적 정답지(Deterministic Ground Truth)’로 구성된 절대적인 골든 데이터셋(Golden Dataset)을 구축한다. 테스트 자동화 과정에서 오라클은 무참조 LLM이 아닌, 가장 보수적이고 빈틈없는 참조 기반 알고리즘인 ‘정확도 일치(Exact Match, EM)’ 오라클을 1순위로 채택한다.</p>
<ul>
<li><strong>오라클 동작 원리 메커니즘:</strong></li>
</ul>
<ol>
<li>테스트 스위트(Test suite)는 문서 A를 AI 모델에 입력한다. 시스템 내부에 저장된 이 문서에 대한 결정론적 정답지 배열에는 송장 번호값이 <code>[INV-2026-001]</code>로 명확히 규정되어 있다.</li>
<li>AI 모델이 텍스트 처리를 수행한 후 <code>INV-2026-001</code>이라는 문자열을 반환하면, EM 알고리즘 오라클은 해시값 수준의 비교 수식에 따라 점수 1(Pass)을 부여한다.</li>
<li>반면, 언어 모델이 특유의 환각 현상을 일으키거나 포맷팅 규칙을 어겨 하이픈을 누락한 <code>INV2026001</code>, 또는 필요 이상으로 친절한 문장 형태인 <code>The requested invoice number is INV-2026-001</code>을 반환할 경우, 오라클은 이를 텍스트 의미론과 무관하게 즉시 점수 0(Fail)으로 단호히 처리한다.</li>
</ol>
<ul>
<li><strong>소프트웨어 공학적 의의:</strong> 이 테스트 접근법은 시스템에 자비 없는, 그러나 가장 엄격한 결정론적 검증 기준을 강제한다. 개발자는 이 깐깐한 참조 기반 오라클을 통과하기 위해서 LLM의 프롬프트 내에 <code>[지시사항: 단답형으로 기호를 포함하여 송장 번호만 반환할 것]</code>과 같은 구조화 출력(Structured Output) 제어 기법을 필수적으로 도입해야만 한다. 이 오라클 로직 덕분에 CI/CD 파이프라인에서 이후 단계의 데이터 파싱(Parsing) 코드가 파괴되지 않고 안전하게 작동할 것임을 확정적으로 보장받게 된다.</li>
</ul>
<h3>4.2 실전 예제 2: 안전 중시 시스템(Cyber-Physical System) 제어 AI의 백투백 테스팅 (참조 소프트웨어 오라클)</h3>
<p>소프트웨어의 미세한 버그나 판단 오류가 인명 사고나 막대한 물리적 파괴로 직결되는 자율주행 자동차 산업이나 항공우주 도메인에서는 텍스트 기반의 정답지를 구축하는 것만으로는 AI 시스템의 복잡한 시계열적 동작을 통제할 수 없다. 여기서는 오라클 문제의 해결책으로 ’백투백 테스팅(Back-to-Back Testing)’이라는 고도의 검증 공학 기법이 활용된다. 놀랍게도 이 환경에서 참조 기반 오라클은 단순한 JSON 데이터나 정답 문자열이 아니라, 시스템의 요구사항을 바탕으로 독자적인 규칙 기반 알고리즘을 통해 구현된 ‘참조 구현체(Reference Implementation) 소프트웨어’ 그 자체가 된다.</p>
<ul>
<li><strong>오라클 동작 원리 메커니즘:</strong></li>
</ul>
<ol>
<li>시스템의 테스트 베드(Test bed)는 가상 주행 환경에서 발생한 동일한 레이더 및 카메라 센서 입력 데이터를 두 개의 분리된 시스템으로 동시에 브로드캐스팅한다. 하나는 새로 개발된 딥러닝 기반의 메인 AI 자율 제어 시스템이고, 다른 하나는 기존의 엄격한 수학적 로직으로만 짜인 단순하지만 검증된 ‘참조 구현체(Legacy System 또는 Standalone Version)’ 시스템이다.</li>
<li>참조 소프트웨어 구현체는 주입된 수천 개의 시그널 정보에 대해 완벽하게 예측 가능하고 결정론적인 조향각(Steering angle) 타겟 값과 브레이크 압력 제어 값들을 밀리초 단위로 쏟아낸다. 이 거대한 수치 데이터 스트림 자체가 절대적인 ’참조 정답지 오라클’의 지위를 획득한다.</li>
<li>최종 테스트 프레임워크 스크립트는 확률적 특성을 지닌 딥러닝 AI 시스템의 출력 배열 값이 오라클 시스템이 뱉어낸 출력 배열 값과 오차 한계 경계선(Threshold) 이내에서 일치하는지를 수학적으로 연산하여 시스템의 릴리즈 여부를 판별한다.</li>
</ol>
<ul>
<li><strong>소프트웨어 공학적 의의:</strong> 인간 개발자가 수백만 프레임의 센서 데이터에 대해 일일이 정답 스티어링 값을 수동으로 계산하여 정답지를 작성하는 것은 물리적으로 불가능하다. 이 공학적 아키텍처는 비결정론적인 딥러닝 블랙박스 모델의 변동성(Probabilistic nature)을 기존에 철저히 검증된 결정론적 소프트웨어 로직(Deterministic logic)과 대치시킴으로써 획기적으로 오라클 문제를 타파한다. 신뢰할 수 있는 참조 소프트웨어를 거대한 자동화 오라클로 동작하게 함으로써, 극도로 방대한 테스트 시나리오에 대한 회귀 테스트(Regression testing)를 무인으로 수행할 수 있는 기반을 완성하는 것이다.</li>
</ul>
<h3>4.3 실전 예제 3: 엔터프라이즈 다중 에이전트 환경의 하이브리드 검증 오라클 구축</h3>
<p>현대의 기업용 소프트웨어 아키텍처에 투입되는 엔터프라이즈 AI 에이전트는 사용자의 질문에 단순히 사과나 변명을 늘어놓는 챗봇의 수준을 초월한다. 이들은 사용자의 자연어 명령을 해독하여 사내 관계형 데이터베이스(RDBMS)를 직접 쿼리하고, 서드파티 API 함수를 자율적으로 호출하며, 그 결과를 취합해 비즈니스 의사 결정을 지원하는 복잡한 과업을 수행한다. 이러한 형태의 다중 단계(Multi-step execution) 에이전트 파이프라인은 단일한 형태의 오라클 메커니즘만으로는 절대 그 무결성을 검증할 수 없으며, 반드시 시스템의 내부 분기에 따라 참조 기반 오라클과 무참조 오라클이 정밀하게 교대하는 하이브리드 아키텍처가 구축되어야 한다.</p>
<p>Oracle AI Agent Studio와 같이 보안과 성능이 중시되는 엔터프라이즈 프레임워크 내에서의 테스트 및 관측성(Observability) 평가 시나리오는 다음과 같은 입체적 검증 단계를 거친다.</p>
<ul>
<li><strong>1단계 파이프라인 검증: 툴 및 API 호출 무결성 확인 (참조 기반 결정론적 오라클 개입)</strong> 에이전트가 “핵심 거래처 B의 최근 6개월 매출액 변동 추이를 요약해 줘“라는 명령을 수신했을 때, 시스템 내부의 첫 번째 임무는 올바른 툴을 선택하고 정확한 파라미터를 조립하여 데이터베이스에 접근하는 것이다. 이때, 생성된 SQL 쿼리나 API 함수 호출 명세의 무결성을 검증하기 위해 참조 기반 오라클이 작동한다. ‘거래처 ID = B’, ’Date Range = 최근 6개월’이라는 파라미터가 함수 인자에 한 치의 오차도 없이 맵핑되었는지를 테스트 시스템이 가진 결정론적 정답지와 대조(Exact Match 및 조건부 로직 연산)하여 1차 합격 여부를 판별한다. 이 단계에서는 모델의 비결정성이 허용되지 않는다.</li>
<li><strong>2단계 파이프라인 검증: 응답 텍스트의 사실성과 논리적 구조 평가 (무참조 의미론적 오라클 개입)</strong> 에이전트가 API 호출을 성공적으로 수행하여 데이터베이스로부터 <code>[월별 매출액 배열: $10,000, $12,000... 총액: $85,000, 전년 대비 15% 하락]</code>이라는 원시 데이터 컨텍스트(Raw context)를 반환받았다고 가정하자. 에이전트는 이를 비즈니스 문서 형식에 맞게 자연어로 합성하여 최종 텍스트 응답을 생성한다. 이 마지막 결과물이 출력되는 순간, 이번에는 정해진 참조 문장이 존재하지 않으므로 무참조 오라클 메커니즘이 바통을 이어받는다. RAGAS 프레임워크의 충실성(Faithfulness) 알고리즘이나 LLM-as-a-Judge 파이프라인이 즉각 가동되어, 최종 생성된 리포트의 어조(Tone), 문맥의 유창성, 그리고 무엇보다 검색된 원시 데이터와 텍스트 내의 주장 간의 논리적 함의(Entailment)가 위배되지 않았는지를 코사인 유사도 벡터 연산을 통해 평가한다.</li>
</ul>
<p>이러한 하이브리드 오라클 구조는 시스템 내에서 API 호출이라는 치명적인 기술 부채 지점은 결정론적 정답지로 강력하게 통제하고, 사용자에게 제공되는 텍스트 인터페이스 지점은 무참조 지표로 유연하게 평가함으로써, AI 시스템이 지닌 확률적 역동성을 죽이지 않으면서도 기업용 소프트웨어에 필수적인 고도의 신뢰성을 안전하게 담보하는 최적의 공학적 타협점을 제시한다. 결론적으로, 참조 기반과 무참조라는 두 오라클의 특성을 아키텍처 내에서 적재적소에 엮어내는 설계 역량이야말로 향후 AI 애플리케이션 개발 패러다임에서 가장 핵심적인 소프트웨어 엔지니어링 경쟁력으로 자리 잡게 될 것이다.</p>
<h2>5. 참고 자료</h2>
<ol>
<li>The Oracle Problem and the Teaching of Software Testing - Cem Kaner, https://kaner.com/?p=190</li>
<li>Testing AI Systems: Handling the Test Oracle Problem - DEV Community, https://dev.to/qa-leaders/testing-ai-systems-handling-the-test-oracle-problem-3038</li>
<li>AI agent observability and evaluation - Oracle, https://www.oracle.com/a/ocom/docs/applications/fusion-apps-ai-agents-observe-eval-guide.pdf</li>
<li>LLM Evaluation Metrics, Best Practices and Frameworks - Aisera, https://aisera.com/blog/llm-evaluation/</li>
<li>Reference-free Evaluation Metrics for Text Generation: A Survey - arXiv, https://arxiv.org/html/2501.12011v1</li>
<li>A Comprehensive Guide to LLM Evaluations - Caylent, https://caylent.com/blog/a-comprehensive-guide-to-llm-evaluations</li>
<li>LLM Evaluation Metrics: The Ultimate LLM Evaluation … - Confident AI, https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation</li>
<li>Trusted NLG Research at Salesforce AI, https://www.salesforce.com/blog/trusted-nlg-research/</li>
<li>Define your evaluation metrics | Generative AI on Vertex AI - Google Cloud Documentation, https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/eval-python-sdk/determine-eval</li>
<li>Understanding LLM Evaluation Metrics: Best Practices for Reliable LLM Assessment | by Thanh Tung Vu | Medium, https://medium.com/@tungvu_37498/understanding-llm-evaluation-metrics-best-practices-for-reliable-llm-assessment-3fce1fa48251</li>
<li>A list of metrics for evaluating LLM-generated content - Microsoft Learn, https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/evaluation/list-of-eval-metrics</li>
<li>Understanding BLEU and ROUGE score for NLP evaluation - GeeksforGeeks, https://www.geeksforgeeks.org/nlp/understanding-bleu-and-rouge-score-for-nlp-evaluation/</li>
<li>Reference-based Metrics Disprove Themselves in Question Generation - arXiv, https://arxiv.org/html/2403.12242v3</li>
<li>[Quick Review] Reference-based Metrics Disprove Themselves in Question Generation, https://liner.com/review/referencebased-metrics-disprove-themselves-in-question-generation</li>
<li>Findings - EMNLP 2024, https://2024.emnlp.org/program/accepted_findings/</li>
<li>Reference-based Metrics Disprove Themselves in Question Generation - arXiv, https://arxiv.org/html/2403.12242v2</li>
<li>Reference-based Metrics Disprove Themselves in Question Generation - ACL Anthology, https://aclanthology.org/2024.findings-emnlp.798.pdf</li>
<li>A Conceptual Framework for AI Capability Evaluations - arXiv, https://arxiv.org/html/2506.18213v1</li>
<li>ICML Poster Investigating Non-Transitivity in LLM-as-a-Judge, https://icml.cc/virtual/2025/poster/44669</li>
<li>No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding - arXiv, https://arxiv.org/html/2503.05061v1</li>
<li>Evaluation of RAG Metrics for Question Answering in the Telecom Domain - arXiv, https://arxiv.org/html/2407.12873v1</li>
<li>Ragas Evaluation: In-Depth Insights | PIXION Blog, https://pixion.co/blog/ragas-evaluation-in-depth-insights</li>
<li>Faithfulness - Ragas, https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/faithfulness/</li>
<li>Answer Relevance - Ragas, https://docs.ragas.io/en/v0.1.21/concepts/metrics/answer_relevance.html</li>
<li>Ragas Writing Format - HackMD, https://hackmd.io/@KSLab-M1/BJyoUe3nC</li>
<li>Reference-Free Summarization Evaluation with Large Language Models - ACL Anthology, https://aclanthology.org/2023.eval4nlp-1.16.pdf</li>
<li>TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large Language Models - arXiv, https://arxiv.org/html/2504.07385v1</li>
<li>Reference implementation as a test oracle - Richard Seidl, https://www.richard-seidl.com/en/blog/test-oracle-reference</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>