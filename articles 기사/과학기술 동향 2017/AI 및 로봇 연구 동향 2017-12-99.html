<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2017년 12월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2017년 12월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2017년 AI 및 로봇 연구 동향</a> / <span>2017년 12월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2017년 12월 AI 및 로봇 연구 동향</h1>
<h2>1. 서론: 새로운 시대의 서막</h2>
<p>2017년 12월은 인공지능(AI) 연구 역사에서 중요한 분기점으로 기록된다. 이 시기는 딥러닝의 경험적 성공이 절정에 달하면서도, 그 이론적 기반과 미래 방향성에 대한 근본적인 질문들이 제기되던 때였다. 본 보고서는 이 시기의 중심에 있었던 제31회 신경정보처리시스템학회(Conference on Neural Information Processing Systems, NIPS 2017)를 축으로 하여 당시 발표된 주요 AI 및 로봇 연구들을 심층적으로 분석하고 그 의미를 조명한다.</p>
<p>NIPS 2017은 단순한 학술 대회를 넘어, AI 연구의 패러다임 전환을 예고하는 여러 중대한 발표가 이루어진 역사적 장이었다.1 특히, 본 보고서에서 상세히 다룰 Transformer 아키텍처의 등장은 자연어 처리(NLP) 분야를 넘어 AI 전체의 연구 지형을 재편하는 기폭제가 되었다. 동시에, 불완전 정보 게임의 정복, 통계적 기계 학습 이론의 심화 등은 AI가 해결할 수 있는 문제의 범위와 깊이를 한 단계 끌어올렸음을 시사한다. 이 시기는 AI 기술이 학문적 탐구의 영역을 넘어 산업의 핵심 동력으로 자리매김하고, 그에 따른 신뢰성, 설명가능성, 그리고 물리적 구현에 대한 고민이 본격화되는 전환점이었다.</p>
<p>본 보고서는 NIPS 2017의 개괄적 분석을 시작으로, 최우수 논문상(Best Paper Award)과 시간의 시험상(Test of Time Award)을 수상한 핵심 연구들을 기술적으로 해부한다. 이후, AI 역사상 가장 영향력 있는 논문 중 하나로 평가받는 “Attention Is All You Need“를 집중 조명하고, 마지막으로 arXiv와 주요 로봇 학회를 통해 당시의 광범위한 연구 지평을 탐색하며 2017년 12월이 남긴 유산을 종합적으로 평가한다. 이를 통해 당시의 기술적 성취를 기록할 뿐만 아니라, 현재 AI 기술의 뿌리가 어디에 있으며 미래는 어떤 방향으로 나아갈 것인지에 대한 통찰을 제공하고자 한다.</p>
<h2>2.  NIPS 2017 - AI 연구의 폭발적 성장과 주요 동향</h2>
<p>2017년 12월 4일부터 9일까지 미국 캘리포니아 롱비치에서 개최된 NIPS 2017은 AI 연구 분야의 전례 없는 양적, 질적 팽창을 상징하는 기념비적인 행사였다.1 이 학회는 당시 AI 기술에 대한 학계와 산업계의 폭발적인 관심을 집약적으로 보여주는 바로미터 역할을 했다.</p>
<h3>2.1  학회 규모 및 통계 분석: 역대 최대 규모의 의미</h3>
<p>NIPS 2017은 여러 통계 지표에서 역대 최대 기록을 경신하며 AI 분야의 뜨거운 열기를 증명했다. 총 3,240편의 논문이 제출되었으며, 이 중 678편만이 채택되어 약 20.9%의 낮은 채택률을 기록했다.1 이는 전 세계 연구자들이 최고 수준의 학회에 자신의 연구 성과를 발표하기 위해 얼마나 치열하게 경쟁했는지를 보여준다.</p>
<p>더욱 주목할 만한 점은 참석자 수의 폭발적인 증가였다. 8,000명 이상이 등록하여 전년 대비 2,000명 이상 늘어난 수치를 기록했다.2 이러한 규모의 확대는 AI가 더 이상 소수의 전문가들을 위한 틈새 학문 분야가 아니라, 기술 산업 전반의 핵심 동력으로 부상했음을 명백히 보여주는 사건이었다. 학회 등록이 단 몇 분 만에 마감되는 현상은 AI 분야에 대한 관심이 공급을 압도하는 수준에 이르렀음을 시사하며, 이는 단순한 학문적 호기심을 넘어 AI 기술의 상업적 가치와 미래 잠재력에 대한 거대한 자본과 인재가 집중되고 있음을 의미했다.</p>
<p>아래 표는 NIPS 2017의 핵심 통계를 요약한 것이다.</p>
<table><thead><tr><th>구분</th><th>수치</th><th>출처</th></tr></thead><tbody>
<tr><td>제출 논문 수</td><td>3,240</td><td>1</td></tr>
<tr><td>채택 논문 수</td><td>678</td><td>1</td></tr>
<tr><td>채택률</td><td>20.9%</td><td>5</td></tr>
<tr><td>총 참석자 수</td><td>8,000명 이상</td><td>2</td></tr>
<tr><td><strong>주요 발표 기관 (Top 5)</strong></td><td></td><td></td></tr>
<tr><td>1. Google</td><td>60편 (8.8%)</td><td>7</td></tr>
<tr><td>2. Carnegie Mellon University</td><td>48편 (7.1%)</td><td>7</td></tr>
<tr><td>3. Massachusetts Institute of Technology</td><td>43편 (6.3%)</td><td>7</td></tr>
<tr><td>4. Microsoft</td><td>40편 (5.9%)</td><td>7</td></tr>
<tr><td>5. Stanford University</td><td>39편 (5.7%)</td><td>7</td></tr>
</tbody></table>
<p>이러한 전례 없는 규모의 성장은 AI 연구 생태계의 구조적 변화를 예고했다. 학술 교류의 장이 첨단 기술을 선점하고 핵심 인재를 확보하려는 글로벌 기술 기업들의 전략적 경쟁 무대로 변모하기 시작한 것이다. 이는 향후 AI 연구가 산업계의 수요와 자본에 의해 더욱 강력하게 견인될 것임을 암시하는 중요한 신호였다.</p>
<h3>2.2  주요 연구 분야 조망</h3>
<p>NIPS 2017에서 발표된 연구들은 당시 AI 커뮤니티의 핵심 관심사가 무엇이었는지를 명확히 보여주었다. 주요 연구 분야는 크게 생성 모델, 학습의 안정성 및 효율성 강화로 요약될 수 있었다.1</p>
<p><strong>생성적 적대 신경망(Generative Adversarial Networks, GANs)</strong>: GAN은 당시 가장 활발하게 연구되던 주제 중 하나였다. 생성자(generator)와 판별자(discriminator)가 서로 경쟁하며 학습하는 이 구조는 매우 사실적인 이미지나 데이터를 생성하는 능력으로 큰 주목을 받았다. 특히 레이블이 없는 대규모 데이터를 활용하여 훈련 데이터의 필요량을 줄일 수 있다는 잠재력 때문에, 데이터 부족 문제를 해결할 핵심 기술로 기대를 모았다.1</p>
<p><strong>이미지 및 비디오 처리</strong>: GAN의 발전과 더불어, 더욱 정교하고 복잡한 이미지 및 비디오 처리 모델과 기술에 대한 연구가 활발하게 이루어졌다. 이는 자율주행, 의료 영상 분석 등 시각적 데이터를 이해하는 능력이 필수적인 응용 분야의 성장을 반영하는 것이었다.1</p>
<p><strong>전이학습(Transfer Learning), 강화학습(Reinforcement Learning), 연속 학습(Continual Learning)</strong>: 이 세 분야는 AI 모델을 더욱 안정적이고 강력하게 만들기 위한 핵심 방법론으로 부상했다.</p>
<ul>
<li>
<p><strong>전이학습</strong>은 특정 작업에서 학습된 지식을 다른 관련 작업에 적용하여 학습 효율성을 높이는 기술이다.</p>
</li>
<li>
<p><strong>강화학습</strong>은 에이전트가 환경과의 상호작용을 통해 보상을 최대화하는 행동을 학습하는 패러다임으로, 특히 게임 AI와 로봇 제어 분야에서 두각을 나타냈다. NIPS 2017에서는 12월 7일에 ’심층 강화학습 심포지엄’이 별도로 개최되었으며, DeepMind의 David Silver와 같은 해당 분야의 최고 권위자들이 참여하여 ’심층 강화학습을 통한 게임 마스터’와 같은 주제로 최신 연구 성과를 발표했다.8</p>
</li>
<li>
<p><strong>연속 학습</strong>은 AI가 새로운 지식을 학습하면서 기존의 지식을 잊어버리지 않도록(catastrophic forgetting) 하는 문제를 다루며, 평생 학습이 가능한 AI를 구현하기 위한 필수적인 연구 분야로 인식되었다.</p>
</li>
</ul>
<p>이러한 연구 동향들은 AI가 정적인 데이터셋을 분석하는 단계를 넘어, 보다 동적이고 자율적으로 데이터를 생성하고(GAN), 지식을 이전하며(전이학습), 환경과 상호작용하고(강화학습), 지속적으로 배우는(연속 학습) 방향으로 진화하고 있음을 보여주었다. 이는 AI의 적용 범위를 통제된 실험실 환경에서 예측 불가능하고 변화무쌍한 실제 세계로 확장하려는 당시 연구 커뮤니티의 공통된 지향점을 드러내는 것이었다.</p>
<h3>2.3  산업계와 학계의 융합</h3>
<p>NIPS 2017은 학계와 산업계의 경계가 허물어지고 있음을 명확하게 보여준 행사였다. Google, Microsoft, IBM, DeepMind, Facebook, Amazon 등 세계적인 기술 기업들이 대거 참여하여 최신 연구 성과를 발표하고 인재 영입에 나섰다.1</p>
<p>논문 발표 기관 통계를 보면 이러한 경향은 더욱 뚜렷해진다. Google이 60편(8.8%)의 논문을 발표하며 전체 기관 중 1위를 차지했고, Microsoft(40편), DeepMind(31편), IBM(16편), Facebook(11편) 등 다수의 기업 연구소가 상위권에 포진했다.7 이는 전통적인 학계의 강자인 Carnegie Mellon University(48편), MIT(43편), Stanford University(35편) 등과 어깨를 나란히 하거나 뛰어넘는 수준이었다.</p>
<p>이러한 통계는 최고 수준의 AI 연구 주도권이 막대한 자본, 방대한 데이터, 그리고 대규모 컴퓨팅 인프라를 보유한 산업계 연구소로 상당 부분 이동했음을 시사한다. 기업들은 단순히 기존 기술을 응용하는 수준을 넘어, AI 분야의 근본적인 문제들을 해결하는 기초 연구(fundamental research)에서도 학계를 선도하기 시작했다.</p>
<p>이 현상은 AI 연구 생태계에 구조적인 변화를 가져왔다. 한편으로는 학계의 우수 인재들이 산업계로 유출되는 ‘두뇌 유출’ 현상이 심화되고, 연구 자원 격차로 인해 학계와 산업계의 연구 격차가 벌어질 수 있다는 우려가 제기되었다. 다른 한편으로는 산업계의 현실적인 문제의식과 풍부한 자원이 학계의 이론적 깊이와 결합하여 전례 없는 시너지를 창출할 수 있는 기회의 장이 열리기도 했다. NIPS 2017은 이러한 양면성을 지닌 ‘산학 융합’ 시대의 본격적인 개막을 알리는 상징적인 무대였다.</p>
<h2>3.  NIPS 2017 최우수 논문상 심층 분석: 이론적 토대의 확장</h2>
<p>NIPS 2017의 최우수 논문상과 시간의 시험상은 당시 AI 연구의 정점에 있는 성과들과 미래 방향성을 가늠하게 하는 중요한 지표였다. 수상작들은 딥러닝의 경험적 성공을 뒷받침하고 그 한계를 극복하기 위한 이론적 깊이를 탐구하는 데 초점을 맞추고 있었다. 아래 표는 주요 수상 논문과 그 핵심 기여를 요약한 것이다.</p>
<table><thead><tr><th>구분</th><th>논문 제목</th><th>저자</th><th>핵심 기여</th></tr></thead><tbody>
<tr><td>최우수 논문</td><td>Safe and Nested Subgame Solving for Imperfect-Information Games</td><td>Noam Brown, Tuomas Sandholm</td><td>불완전 정보 게임에서 안전하고 중첩 가능한 서브게임 해결 기법을 제시하여, AI ’Libratus’가 인간 포커 챔피언을 이기는 이론적 기반을 제공함. 9</td></tr>
<tr><td>최우수 논문</td><td>Variance-based Regularization with Convex Objectives</td><td>Hongseok Namkoong, John Duchi</td><td>볼록 최적화 문제에서 분산을 대리하는 볼록 정규화 항을 제안하여, 모델의 편향-분산 트레이드오프를 효율적으로 제어하는 방법을 제시함. 10</td></tr>
<tr><td>최우수 논문</td><td>A Linear-Time Kernel Goodness-of-Fit Test</td><td>Wittawat Jitkrittum, Wenkai Xu, Zoltan Szabo, Kenji Fukumizu, Arthur Gretton</td><td>Stein 방법론을 활용하여 샘플 수에 대해 선형 시간 복잡도를 갖는 새로운 커널 기반 적합도 검정 방법을 개발하고, 설명가능성을 높임. 10</td></tr>
<tr><td>시간의 시험상</td><td>Random Features for Large-Scale Kernel Machines (NIPS 2007)</td><td>Ali Rahimi, Ben Recht</td><td>대규모 데이터셋에 대한 커널 머신의 확장성 문제를 해결하기 위해, 랜덤 푸리에 특징을 이용해 비선형 문제를 선형 문제로 근사하는 혁신적인 방법을 제시함. 12</td></tr>
</tbody></table>
<h3>3.1  불완전 정보 게임의 정복: Libratus와 “Safe and Nested Subgame Solving”</h3>
<p>2017년 초, Carnegie Mellon University(CMU)에서 개발한 AI ’Libratus’는 20일간 12만 핸드에 걸쳐 진행된 ‘Brains Vs. Artificial Intelligence’ 대회에서 4명의 세계 최정상급 프로 포커 플레이어를 상대로 압도적인 승리를 거두었다.13 이 사건은 AI 역사에 중요한 이정표였다. 체스(Deep Blue)나 바둑(AlphaGo)과 같은 모든 정보가 공개된 ’완전 정보 게임(perfect-information games)’의 정복을 넘어, 상대방의 패와 같은 핵심 정보가 숨겨져 있고 속임수(bluffing)가 전략의 핵심인 ’불완전 정보 게임(imperfect-information games)’이라는 새로운 영역을 AI가 정복했음을 알리는 신호탄이었기 때문이다.9 NIPS 2017 최우수 논문상을 수상한 이 논문은 바로 Libratus의 승리를 가능하게 한 핵심적인 이론과 알고리즘을 담고 있다.</p>
<p>불완전 정보 게임을 해결하는 데 있어 가장 큰 기술적 난제는 게임을 독립적인 하위 문제로 분해할 수 없다는 점이다. 완전 정보 게임에서는 특정 게임 상태(subgame)에 도달하면 그 이전의 경로는 잊고 현재 상태에서 최적의 수를 찾으면 된다. 하지만 포커와 같은 불완전 정보 게임에서는 현재 나의 최적 전략이 내가 도달하지 않은 다른 게임 상태, 즉 상대방이 들고 있을지 모르는 모든 가능한 카드 조합과 그에 따른 상대의 미래 전략에 따라 달라진다.16 따라서 게임 전체를 하나의 거대한 단위로 간주하고 전략을 수립해야 하는 근본적인 어려움이 존재한다.</p>
<p>이 논문은 이 문제를 해결하기 위해 세 가지 핵심 모듈로 구성된 접근법을 제시했다.</p>
<ol>
<li>
<p><strong>청사진(Blueprint) 전략 수립</strong>: 게임의 상태 공간은 천문학적으로 크기 때문에(10160 이상) 모든 경우의 수를 미리 계산하는 것은 불가능하다.19 따라서, 먼저 게임을 추상화(abstraction)하여 문제의 크기를 줄인다. 예를 들어, 모든 가능한 베팅 금액을 고려하는 대신 유사한 크기의 베팅을 하나의 행동으로 묶고, 유사한 강도의 핸드들을 같은 그룹으로 취급하는 방식이다. 이 추상화된 게임에 대해 ’반사실적 후회 최소화(Counterfactual Regret Minimization, CFR)’와 같은 알고리즘을 사용하여 게임 전체에 대한 대략적인 기본 전략, 즉 ’청사진’을 미리 계산해 둔다.15</p>
</li>
<li>
<p><strong>안전한 중첩 서브게임 해결(Safe and Nested Subgame Solving)</strong>: 실제 게임이 진행되어 특정 상황(서브게임)에 도달하면, 미리 계산된 청사진 전략의 틀 안에서 해당 상황에 대해서만 더 깊고 세밀한 계산을 실시간으로 수행한다.18 이 논문의 핵심적인 이론적 기여는 이 실시간 서브게임 해결 과정이 전체 게임의 내쉬 균형(Nash Equilibrium)을 깨뜨리지 않고 오히려 개선함을 보장하는 ’안전성(safety)’을 증명한 것이다. 또한, 게임이 한 수 한 수 진행됨에 따라 이 서브게임 해결 과정을 ‘중첩하여(nested)’ 반복적으로 적용함으로써, 게임 후반부로 갈수록 전략의 정밀도를 기하급수적으로 높일 수 있음을 보였다.17</p>
</li>
<li>
<p><strong>상대 행동 대응 및 자기 개선(Self-Improvement)</strong>: 만약 상대방이 청사진 전략의 추상화 범위에 포함되지 않은 예상 밖의 행동을 할 경우, Libratus는 즉시 해당 행동을 포함하여 새로운 서브게임을 해결함으로써 효과적으로 대응했다.19 더 나아가, 매일 경기가 끝난 후에는 그날 프로 선수들이 공략했던 Libratus의 전략적 허점을 메타 알고리즘으로 분석하고, 밤새 슈퍼컴퓨터(Pittsburgh Supercomputing Center의 Bridges)를 이용해 가장 시급한 약점부터 수정하는 ‘자기 개선’ 과정을 거쳤다.13</p>
</li>
</ol>
<p>Libratus의 승리와 이 논문이 갖는 의의는 포커 정복 그 자체를 넘어선다. 이 연구는 AI가 단순히 방대한 계산 능력을 넘어, 불확실하고 정보가 비대칭적인 상황에서 ’전략적 추론’을 수행할 수 있는 단계에 이르렀음을 증명했다. 완전 정보 게임의 해결이 명확한 규칙과 논리에 기반한 문제 해결 능력의 증명이었다면, 불완전 정보 게임의 해결은 상대의 의도를 추론하고, 정보를 숨기며, 위험을 감수하는 고차원적인 의사결정 능력의 증명이다. 이는 AI의 적용 분야를 비즈니스 협상, 군사 전략, 사이버 보안, 의료 계획 등 정보가 불완전하고 경쟁적 관계가 존재하는 수많은 현실 세계의 복잡한 문제로 확장할 수 있는 중요한 이론적, 실용적 돌파구를 마련한 것으로 평가된다.13</p>
<h3>3.2  모델의 견고성 및 일반화 성능 향상</h3>
<p>NIPS 2017의 다른 두 최우수 논문은 딥러닝의 경험적 성공 이면에 존재하는 이론적 기반을 강화하고, 모델의 신뢰성과 평가의 엄밀성을 높이는 데 중점을 두었다. 이는 AI 연구 커뮤니티가 단순한 성능 지표 경쟁을 넘어, AI 시스템의 근본적인 원리를 이해하고 그 견고성을 확보하려는 방향으로 성숙해 가고 있음을 보여주는 중요한 흐름이었다.</p>
<h4>3.2.1 “Variance-based Regularization with Convex Objectives”</h4>
<p>이 논문은 기계 학습 모델의 핵심 과제인 ’편향-분산 트레이드오프(bias-variance tradeoff)’를 다루는 새로운 이론적 틀을 제시했다. 전통적인 경험적 리스크 최소화(Empirical Risk Minimization, ERM)는 훈련 데이터에 대한 손실을 최소화하는 데 집중하기 때문에, 훈련 데이터에 과적합(overfitting)되어 새로운 데이터에 대한 일반화 성능이 저하될 위험이 있다. 이러한 문제를 완화하기 위해 손실 함수의 분산(variance)을 정규화(regularization) 항으로 추가하는 방식을 고려할 수 있으나, 분산 항 자체가 모델 파라미터에 대해 비볼록(non-convex) 함수인 경우가 많아 최적화 문제를 매우 어렵게 만든다는 한계가 있었다.23</p>
<p>이 연구는 ’분포적으로 강건한 최적화(Distributionally Robust Optimization, DRO)’와 Owen의 ’경험적 우도(Empirical Likelihood)’라는 통계적 개념을 독창적으로 결합하여 이 문제를 해결했다.23 핵심 아이디어는 훈련 데이터로부터 얻은 경험적 확률 분포 <span class="math math-inline">\hat{P}_n</span>를 절대적으로 신뢰하는 대신, 이 분포 주변에 <span class="math math-inline">\chi^2</span>-발산(divergence)과 같은 통계적 거리로 정의된 작은 ‘불확실성 집합(uncertainty set)’ <span class="math math-inline">\mathcal{P}_n</span>을 설정하는 것이다. 그리고 이 집합 내에 존재하는 모든 가능한 확률 분포 중에서 가장 나쁜 경우(worst-case)의 기대 손실을 최소화하도록 모델을 학습시킨다.</p>
<p>이러한 강건한 최적화 문제는 다음과 같은 수식으로 표현된다.</p>
<p><span class="math math-display">
\min_{\theta \in \Theta} \sup_{P \in \mathcal{P}_n} \mathbb{E}_P[\ell(\theta, X)] \quad \text{where} \quad \mathcal{P}_n := \left\{ P : D_\phi(P \| \hat{P}_n) \le \frac{\rho}{n} \right\}
</span><br />
여기서 <span class="math math-inline">\ell(\theta, X)</span>는 손실 함수, <span class="math math-inline">\Theta</span>는 파라미터 공간, <span class="math math-inline">D_\phi</span>는 <span class="math math-inline">\phi</span>-발산, <span class="math math-inline">\rho</span>는 불확실성 집합의 크기를 조절하는 하이퍼파라미터이다. 이 논문의 중요한 기여는, 원래의 손실 함수 <span class="math math-inline">\ell</span>이 파라미터 <span class="math math-inline">\theta</span>에 대해 볼록(convex) 함수일 경우, 위와 같이 정의된 강건한 목적 함수 전체 또한 <span class="math math-inline">\theta</span>에 대해 볼록 함수가 되어 전역 최적해(global optimum)를 효율적으로 찾을 수 있음을 보인 것이다.23</p>
<p>더 나아가, 저자들은 이 강건한 리스크가 실제로는 경험적 리스크에 분산에 비례하는 정규화 항이 더해진 형태로 근사될 수 있음을 수학적으로 증명했다.23</p>
<p><span class="math math-display">
R_n(\theta, \mathcal{P}_n) \approx \mathbb{E}_{\hat{P}_n}[\ell(\theta, X)] + \sqrt{\frac{2\rho}{n} \cdot \text{Var}_{\hat{P}_n}(\ell(\theta, X))}
</span><br />
결론적으로, 이 연구는 이론적으로 견고하면서도 계산적으로 다루기 쉬운(tractable) 새로운 정규화 방법을 제시함으로써, 모델의 일반화 성능과 견고성을 체계적으로 향상시킬 수 있는 길을 열었다.26</p>
<h4>3.2.2 “A Linear-Time Kernel Goodness-of-Fit Test”</h4>
<p>이 논문은 학습된 모델이 주어진 데이터를 얼마나 잘 설명하는지를 통계적으로 검정하는 ‘적합도 검정(goodness-of-fit test)’ 문제를 다루었다. 이는 모델의 신뢰성을 평가하는 데 있어 필수적인 과정이다. 기존의 커널(kernel) 기반 비모수적 검정 방법들은 강력한 통계적 검정력을 제공했지만, 계산 비용이 샘플 수 <span class="math math-inline">n</span>의 제곱에 비례(<span class="math math-inline">O(n^2)</span>)하여 대용량 데이터셋에는 적용하기 어렵다는 실용적인 한계를 가지고 있었다.27</p>
<p>이 연구는 ’Stein의 방법(Stein’s method)’이라는 독특한 통계적 도구를 커널 방법과 결합하여 이 문제를 해결했다. Stein의 방법의 핵심적인 장점은, 모델 확률 밀도 <span class="math math-inline">p(x)</span>의 정규화 상수(normalizing constant)를 알지 못하더라도, 즉 <span class="math math-inline">p(x)</span>가 비정규화된 형태로만 주어져도 모델과 데이터 분포 간의 불일치를 측정할 수 있다는 것이다.28 이는 베이즈 추론에서 사후 분포(posterior distribution)를 다루는 경우와 같이 정규화 상수를 계산하기 어려운 현대 기계 학습 문제에 매우 유용하다.</p>
<p>연구팀은 Stein의 방법을 이용해 ’커널 스타인 불일치(Kernel Stein Discrepancy, KSD)’라는 측정량을 정의하고, 이를 <span class="math math-inline">O(n)</span>의 선형 시간 복잡도로 계산할 수 있는 새로운 통계량 ’유한 집합 스타인 불일치(Finite Set Stein Discrepancy, FSSD)’를 제안했다.27 기존의 <span class="math math-inline">O(n^2)</span> 방법이 모든 데이터 쌍 간의 상호작용을 계산했다면, 이 방법은 소수의 ‘테스트 위치(test locations)’ 또는 ’특징(features)’을 선택하여 그 지점들에서만 불일치를 측정함으로써 계산량을 획기적으로 줄였다.</p>
<p>이 논문의 또 다른 중요한 기여는 이 테스트 위치들을 무작위로 선택하는 것이 아니라, 통계적 검정력(test power), 즉 모델과 데이터가 다를 때 이를 올바르게 탐지해낼 확률을 최대화하도록 ’학습’한다는 점이다.27 이렇게 최적화된 특징들은 모델이 데이터의 어떤 측면을 잘 설명하지 못하는지를 명확하게 보여주는 역할을 한다. 이로 인해 이 방법은 단순히 ’적합하다/아니다’라는 이진적인 결론을 넘어, 모델의 실패 원인을 분석할 수 있는 ’설명가능성(explainability)’을 제공한다. 실제로 이 설명가능성이 최우수 논문상 수상의 주요 이유 중 하나로 언급되기도 했다.11</p>
<p>이 두 논문은 AI 연구가 ’어떻게 더 높은 성능을 달성할 것인가’라는 질문을 넘어, ’어떻게 더 신뢰할 수 있는 모델을 만들고, 그 모델을 어떻게 엄밀하게 평가하고 이해할 것인가’라는 더 깊은 차원의 질문으로 나아가고 있음을 보여준다. 이는 AI 기술이 실험실을 벗어나 사회의 중요한 의사결정에 사용되기 시작하면서 필연적으로 마주하게 된 학문적 성숙의 증거였다.</p>
<h3>3.3  시간의 시험을 통과한 아이디어: “Random Features for Large-Scale Kernel Machines”</h3>
<p>2017년 NIPS에서 10년 전인 2007년에 발표된 논문 “Random Features for Large-Scale Kernel Machines“가 ’시간의 시험상(Test of Time Award)’을 수상한 것은 매우 상징적인 사건이었다.12 이 상은 딥러닝의 압도적인 성공 신화 속에서, 그 이전 시대의 중요한 이론적 아이디어들이 여전히 유효하며 심지어 현대 딥러닝을 이해하는 데 핵심적인 통찰을 제공할 수 있음을 재조명했다.</p>
<p>2007년 당시, 서포트 벡터 머신(SVM)과 같은 커널 머신은 최첨단 기계 학습 방법론이었다. 커널 머신은 ’커널 트릭(kernel trick)’을 사용하여 입력 데이터를 무한 차원일 수도 있는 고차원 특징 공간으로 암시적으로 매핑함으로써 비선형 문제를 선형적으로 해결하는 우아한 방법을 제공했다. 그러나 이 방법은 훈련 데이터의 수가 <span class="math math-inline">n</span>일 때 <span class="math math-inline">n \times n</span> 크기의 거대한 커널 행렬(Gram matrix)을 계산하고 저장해야 했기 때문에, 데이터가 수만 개를 넘어가면 계산적으로 불가능해지는 심각한 확장성(scalability) 문제를 안고 있었다.31</p>
<p>Ali Rahimi와 Ben Recht의 이 논문은 이 문제에 대한 혁신적인 해결책을 제시했다. 그들은 수학의 ’Bochner의 정리(Bochner’s Theorem)’를 기반으로, 가우시안 커널과 같은 특정 종류의 이동 불변 커널(shift-invariant kernel) <span class="math math-inline">k(x, y) = k(x-y)</span>이 특정 확률 분포의 푸리에 변환(Fourier transform)으로 표현될 수 있음을 이용했다.32 이를 바탕으로, 그들은 데이터를 고차원의 암시적 공간으로 보내는 대신, 저차원의 ‘명시적인(explicit)’ 랜덤 특징 공간으로 매핑하는 함수<span class="math math-inline">z(x)</span>를 구성했다.</p>
<p>수학적으로, Bochner의 정리에 따르면 커널 함수는 다음과 같은 기대값 형태로 표현될 수 있다.</p>
<p><span class="math math-display">
k(x-y) = \mathbb{E}_{\omega, b}
</span><br />
여기서 랜덤 벡터 <span class="math math-inline">\omega</span>는 커널의 푸리에 변환에 해당하는 확률 분포에서 샘플링되고, <span class="math math-inline">b</span>는 0과 <span class="math math-inline">2\pi</span> 사이에서 균등하게 샘플링된다. 이 식에 착안하여, 저자들은 <span class="math math-inline">D</span>개의 랜덤 <span class="math math-inline">\omega_j, b_j</span>를 샘플링하고, 데이터를 다음과 같은 <span class="math math-inline">D</span>차원의 랜덤 푸리에 특징(Random Fourier Features)으로 매핑하는 함수 <span class="math math-inline">z(x)</span>를 정의했다.<br />
<span class="math math-display">
z(x) = \sqrt{\frac{2}{D}}
</span><br />
이렇게 변환된 특징 벡터들의 내적 <span class="math math-inline">z(x)^T z(y)</span>는 원래의 커널 값 <span class="math math-inline">k(x-y)</span>에 대한 비편향 추정량이 된다.32 이로써 복잡한 비선형 커널 문제를 근사적인 선형 문제로 변환하여, 대규모 데이터셋에 대해서도 빠른 선형 분류기나 회귀 모델을 훈련시킬 수 있게 되었다.34</p>
<p>이 상이 2017년에 수여된 것의 더 깊은 의미는 수상자인 Ali Rahimi의 수상 연설에서 드러났다. 그는 당시 딥러닝 연구가 이론적 이해 없이 실험과 경험에만 의존하여 결과를 내는 경향을 ’연금술(alchemy)’에 비유하며 신랄하게 비판했다. 그는 AI 시스템이 의료, 시민 사회 담론, 선거 등 사회의 중요한 영역에 영향을 미치고 있음을 지적하며, 이러한 시스템이 “검증 가능하고, 엄밀하며, 철저한 지식 위에 구축되어야 한다“고 강력하게 주장했다.12</p>
<p>이 연설은 2017년 AI 커뮤니티에 큰 파장을 일으켰다. ‘랜덤 특징’ 방법론은 사실상 단일 은닉층을 가진 신경망의 가중치를 무작위로 고정하고 출력층만 학습시키는 것과 유사한 구조를 가지므로, 딥러닝의 작동 원리를 이해하는 데 중요한 이론적 단초를 제공한다. 이 상의 수여와 수상 연설은 딥러닝이라는 거대한 성공의 물결 속에서 자칫 잊힐 수 있었던 ’이론의 가치’와 ’과학적 엄밀함’을 다시 한번 상기시키는 계기가 되었다. 이는 앞서 분석한 두 최우수 논문상이 ’이론적 견고성’과 ’설명가능성’을 강조한 것과 정확히 같은 맥락을 공유하며, 2017년 NIPS가 단순한 기술 경연 대회를 넘어 AI 연구의 방향성에 대한 깊은 자기 성찰을 수행했음을 보여주는 증거이다.</p>
<h2>4.  패러다임의 전환 - “Attention Is All You Need”</h2>
<p>NIPS 2017에서 발표된 수많은 논문 중, 후대에 가장 큰 영향을 미친 단 하나의 논문을 꼽으라면 단연 Google 연구팀의 “Attention Is All You Need“일 것이다. 이 논문은 순환 신경망(RNN)과 합성곱 신경망(CNN)이 지배하던 시퀀스 처리(sequence transduction) 분야에 ’Transformer’라는 새로운 아키텍처를 제시하며, 자연어 처리(NLP)를 넘어 AI 연구의 전체 패러다임을 바꾸는 혁명의 시작을 알렸다.</p>
<h3>4.1  기존 시퀀스 모델(RNN/LSTM)의 근본적 한계</h3>
<p>Transformer의 등장을 이해하기 위해서는 먼저 그 이전의 지배적인 모델이었던 RNN과 그 발전형인 LSTM(Long Short-Term Memory), GRU(Gated Recurrent Unit)의 근본적인 한계를 파악해야 한다.</p>
<ol>
<li>
<p><strong>순차적 처리(Sequential Processing)의 제약</strong>: RNN 계열 모델의 가장 큰 특징이자 본질적인 한계는 데이터를 순차적으로, 즉 한 번에 하나의 타임스텝(예: 단어)씩 처리해야 한다는 점이다. <span class="math math-inline">t</span> 시점의 계산은 반드시 <span class="math math-inline">t-1</span> 시점의 계산 결과인 은닉 상태(hidden state)에 의존한다. 이러한 구조는 문장처럼 순서가 중요한 데이터를 모델링하는 데 직관적이지만, 치명적인 약점을 내포하고 있었다.37</p>
</li>
<li>
<p><strong>병렬화의 어려움</strong>: 순차적 의존성은 GPU와 같은 병렬 컴퓨팅 하드웨어의 이점을 충분히 활용할 수 없게 만들었다. 문장이 아무리 길어도 첫 단어부터 마지막 단어까지 순서대로 계산해야 했기 때문에, 대규모 데이터셋에 대한 학습 시간이 매우 길어지는 병목 현상이 발생했다.37 이는 모델의 크기와 학습 데이터의 양을 늘리는 데 심각한 제약으로 작용했다.</p>
</li>
<li>
<p><strong>장기 의존성 문제(Long-Range Dependencies)</strong>: RNN은 시퀀스가 길어질수록 초반부의 정보가 뒤로 전달되면서 소실되는 ‘기울기 소실(vanishing gradient)’ 문제에 취약했다. LSTM과 GRU는 ‘입력 게이트’, ‘망각 게이트’, ’출력 게이트’와 같은 정교한 메커니즘을 도입하여 이 문제를 상당 부분 완화했지만, 여전히 정보가 여러 타임스텝을 거치며 고정된 크기의 벡터에 압축되어야 했기 때문에, 매우 긴 문장에서 멀리 떨어진 단어 간의 의미적, 통사적 의존성을 완벽하게 포착하는 데는 한계가 있었다.38</p>
</li>
</ol>
<p>이러한 한계들은 기계 번역, 문서 요약 등 긴 시퀀스를 다루어야 하는 NLP 작업에서 모델의 성능과 확장성을 가로막는 주요 장애물이었다.</p>
<h3>4.2  Transformer 아키텍처 해부: Self-Attention 메커니즘의 원리</h3>
<p>“Attention Is All You Need” 논문은 이러한 문제들을 해결하기 위해 “순환 구조를 완전히 버리자“는 과감하고 혁신적인 아이디어를 제시했다. 저자들은 RNN이나 CNN 구조 없이, 오직 ‘어텐션(Attention)’ 메커니즘만을 사용하여 입력과 출력 시퀀스 간의 전역적인 의존성을 모델링하는 ‘Transformer’ 아키텍처를 제안했다.41</p>
<p>Transformer의 심장과도 같은 핵심 부품은 <strong>‘셀프 어텐션(Self-Attention)’</strong> 메커니즘이다. 기존 어텐션이 주로 인코더와 디코더 사이에서 입력 시퀀스와 출력 시퀀스의 단어들을 정렬(align)하는 데 사용되었다면, 셀프 어텐션은 하나의 시퀀스(예: 입력 문장) 내에서 각 단어가 다른 모든 단어들과 얼마나 관련이 있는지를 직접 계산하여 해당 단어의 문맥적 표현을 풍부하게 만드는 방식이다.41 이 과정은 모든 단어 쌍에 대해 동시에, 즉 병렬적으로 수행될 수 있다.</p>
<p>셀프 어텐션의 구체적인 계산 방식은 **‘스케일드 닷-프로덕트 어텐션(Scaled Dot-Product Attention)’**으로 구현된다. 이 과정은 다음과 같이 세 단계로 이루어진다.</p>
<ol>
<li>
<p><strong>Query, Key, Value 벡터 생성</strong>: 입력 시퀀스의 각 단어 임베딩 벡터에 대해, 각각 다른 가중치 행렬(<span class="math math-inline">W^Q, W^K, W^V</span>)을 곱하여 Query(Q), Key(K), Value(V)라는 세 가지 벡터를 생성한다. Q는 ‘정보를 요청하는’ 역할, K는 ‘자신이 가진 정보의 꼬리표’ 역할, V는 ‘실제 정보 값’ 역할을 한다고 비유할 수 있다.</p>
</li>
<li>
<p><strong>어텐션 스코어 및 가중치 계산</strong>: 특정 단어의 문맥적 표현을 계산하기 위해, 해당 단어의 Q 벡터를 시퀀스 내 모든 다른 단어들의 K 벡터와 내적(dot-product)한다. 이 내적 값은 두 단어 간의 ‘유사도’ 또는 ’관련성 점수’를 나타낸다. 이 점수들이 너무 커지는 것을 방지하기 위해 키 벡터의 차원 <span class="math math-inline">d_k</span>의 제곱근인 <span class="math math-inline">\sqrt{d_k}</span>로 나누어 스케일링(scaling)해준다. 그 후, 이 점수들에 소프트맥스(softmax) 함수를 적용하여 합이 1이 되는 ’어텐션 가중치(attention weights)’를 얻는다.</p>
</li>
<li>
<p><strong>가중합 계산</strong>: 마지막으로, 이렇게 얻은 어텐션 가중치를 각 단어에 해당하는 V 벡터에 곱하여 모두 더한다(가중합). 이 결과 벡터가 바로 다른 모든 단어의 정보를 문맥에 맞게 종합한 해당 단어의 새로운 표현이 된다.</p>
</li>
</ol>
<p>이 전체 과정은 다음의 간결한 행렬 연산 수식으로 표현된다.42</p>
<p><span class="math math-display">
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
</span><br />
Transformer는 여기서 한 걸음 더 나아가 **‘멀티-헤드 어텐션(Multi-Head Attention)’**을 사용한다. 이는 Q, K, V 벡터를 <span class="math math-inline">h</span>개의 작은 조각으로 나누어, <span class="math math-inline">h</span>개의 독립적인 어텐션을 병렬적으로 수행한 후 그 결과들을 다시 합치는 방식이다. 이는 마치 한 문장을 여러 다른 관점(예: 문법적 관계, 의미적 유사성, 동의어 관계 등)에서 동시에 바라보는 것과 같은 효과를 주어, 모델이 더욱 풍부하고 다각적인 문맥 정보를 포착할 수 있게 한다.41</p>
<h3>4.3  NLP 및 AI 연구 패러다임에 미친 혁명적 영향</h3>
<p>Transformer 아키텍처의 등장은 즉각적이고 혁명적인 영향을 미쳤다.</p>
<ul>
<li>
<p><strong>성능과 효율성의 동시 달성</strong>: 논문에서 Transformer는 WMT 2014 영어-독일어, 영어-프랑스어 기계 번역 과제에서 당시 최고 성능(State-Of-The-Art)을 달성하면서도, 병렬화의 이점 덕분에 기존 최고 모델들보다 학습 시간을 획기적으로 단축시켰다.41</p>
</li>
<li>
<p><strong>확장성의 확보와 거대 언어 모델(LLM)의 탄생</strong>: Transformer가 순차적 계산의 병목을 제거함으로써, AI 연구자들은 이전에는 상상할 수 없었던 규모의 데이터와 수십억, 수천억 개의 파라미터를 가진 모델을 훈련시킬 수 있게 되었다. 이는 이후 등장하는 BERT, GPT, T5 등 현대 거대 언어 모델(Large Language Models, LLM)의 기술적 토대가 되었다. ’스케일(scale)’이 곧 성능이라는 ’스케일링 법칙(Scaling Laws)’이 AI 분야의 새로운 지배적인 패러다임이 될 수 있었던 것은 전적으로 Transformer의 확장성 덕분이었다.</p>
</li>
<li>
<p><strong>AI 전반으로의 영향력 확산</strong>: 2017년 발표 이후, 이 논문은 기하급수적인 인용 횟수를 기록하며(2025년 기준 17만 회 이상) 21세기 가장 많이 인용된 논문 중 하나가 되었다.42 그 영향력은 NLP를 넘어 컴퓨터 비전(Vision Transformer), 음성 인식, 신약 개발 등 AI의 거의 모든 하위 분야로 확산되어, 각 분야의 표준적인 아키텍처로 자리 잡았다.43</p>
</li>
</ul>
<p>결론적으로, “Attention Is All You Need“는 단순히 ‘더 좋은 모델’ 하나를 제안한 것이 아니었다. 이 논문은 ’시퀀스 데이터를 어떻게 표상하고 처리할 것인가’에 대한 기존의 관점을 근본적으로 뒤엎은 <strong>패러다임 전환</strong>이었다. 시간의 축에 순차적으로 묶여 있던 시퀀스 모델링의 개념을, 시퀀스 내 모든 요소가 동등하게 상호작용하는 하나의 ’관계 네트워크(network of relations)’로 해방시킨 것이다. 이러한 관점의 전환은 모델의 규모를 폭발적으로 증가시킬 수 있는 공학적 토대를 마련했고, 이는 곧바로 ’LLM 시대’의 개막으로 이어졌다. 2017년 12월 NIPS는 바로 이 거대한 혁명의 진원지였다.</p>
<h2>5.  2017년 12월 AI 및 로봇 분야 연구 지평 확장</h2>
<p>NIPS 2017이 AI 연구의 중심 무대였지만, 당시의 연구 지평은 학회 발표를 넘어서는 더 넓은 영역에서 확장되고 있었다. 특히, 연구 결과가 신속하게 공유되는 arXiv 프리프린트 서버와, AI의 물리적 구현을 다루는 로봇 공학 분야에서는 미래 AI 기술의 향방을 예고하는 중요한 흐름들이 나타나고 있었다.</p>
<h3>5.1  arXiv를 통해 본 주요 연구 주제: 설명가능 AI(XAI)의 대두</h3>
<p>딥러닝 모델이 다양한 분야에서 인간의 능력을 넘어서는 성능을 보여주기 시작하면서, 역설적으로 이 모델들의 ’불투명성’이 새로운 문제로 부상했다.47 복잡한 신경망이 어떻게, 그리고 왜 특정 결정을 내리는지 이해하기 어려운 ‘블랙박스(black-box)’ 문제는 특히 의료, 금융, 법률과 같이 결정에 대한 높은 수준의 책임과 신뢰가 요구되는 분야에서 심각한 걸림돌이 되었다. 이러한 배경 속에서 2017년 12월, ’설명가능 AI(Explainable AI, XAI)’에 대한 논의가 arXiv를 중심으로 활발하게 이루어졌다.</p>
<p>이러한 흐름은 사회적, 법적 요구와도 맞물려 있었다. 2018년 5월 발효가 예정되어 있던 유럽연합의 일반 데이터 보호 규정(GDPR)은 자동화된 의사결정에 대해 사용자가 ’설명을 요구할 권리’를 명시할 가능성을 내포하고 있어, 블랙박스 모델의 상업적 사용에 대한 법적 불확실성을 증대시켰다.47</p>
<p>2017년 12월 arXiv에 제출된 주요 논문들은 XAI에 대한 다각적인 접근을 보여준다.</p>
<ul>
<li>
<p>Andreas Holzinger 등이 발표한 “What do we need to build explainable AI systems for the medical domain?” (arXiv:1712.09923)은 의료 분야에서 XAI의 시급성을 강력하게 주장했다. 이들은 의료 전문가들이 AI의 결정을 신뢰하고 수용하기 위해서는 이미지, 유전체 데이터, 임상 텍스트 등 이종의 데이터 소스를 기반으로 한 AI의 추론 과정을 이해하고 추적할 수 있어야 한다고 역설했다.47</p>
</li>
<li>
<p>Tim Miller 등의 “Explainable AI: Beware of Inmates Running the Asylum…” (arXiv:1712.00547)은 XAI 연구의 방향성에 대한 성찰적 비판을 제기했다. 저자들은 많은 AI 연구자들이 실제 사용자(의사, 판사 등)의 요구를 이해하기보다는 기술적으로 흥미로운 설명을 만드는 데 치중하는 경향을 “정신병원을 운영하는 수감자들“에 비유하며, 진정으로 유용한 설명을 만들기 위해서는 철학, 심리학, 인지과학 등 사회 및 행동 과학 분야의 깊이 있는 연구 결과를 적극적으로 도입해야 한다고 주장했다.49</p>
</li>
<li>
<p>이 시기에는 이미 구체적인 XAI 방법론들도 상당한 주목을 받고 있었다. 예를 들어, 게임 이론의 섀플리 값(Shapley value)을 기반으로 개별 예측에 대한 각 입력 특징의 기여도를 공정하게 배분하는 ‘SHAP(SHapley Additive exPlanations)’ 50이나, 입력과 예측 사이의 기울기(gradient)를 적분하여 특징의 중요도를 계산하는 ‘Integrated Gradients’ 51와 같은 기술들은 XAI 연구의 실용적인 도구로 자리 잡기 시작했다.</p>
</li>
</ul>
<p>이러한 XAI에 대한 관심의 증가는 NIPS 2017에서 ’설명가능성’을 이유로 최우수 논문상을 수상한 “A Linear-Time Kernel Goodness-of-Fit Test“의 사례와 정확히 궤를 같이한다.11 이는 2017년 12월이 AI 연구의 패러다임이 단순히 ’성능 극대화’에서 ’인간과의 신뢰 구축 및 상호작용’으로 확장되는 중요한 전환기였음을 보여준다. AI 기술이 사회 시스템에 깊숙이 통합되기 위해 반드시 해결해야 할 기술적, 사회적, 철학적 과제에 대한 고민이 AI 연구의 변방에서 중심으로 이동하기 시작한 시점이었다.</p>
<h3>5.2  로봇 공학 연구 동향: 소프트 로보틱스와 그리핑 기술의 진화</h3>
<p>2017년의 로봇 공학 연구는 AI 알고리즘의 발전과 더불어, 로봇이 물리 세계와 상호작용하는 방식 자체를 근본적으로 혁신하려는 시도들이 두드러졌다. 특히, 생명체의 유연하고 적응적인 특성을 모방하는 ’소프트 로보틱스(soft robotics)’와 물체를 파지하는 ‘그리핑(gripping)’ 기술 분야에서 주목할 만한 진전이 있었다. 이 시기의 연구 동향을 파악하기 위해, 2017년에 개최된 양대 로봇 학회인 ICRA(5월)와 IROS(9월)의 발표 내용들은 12월의 연구 지형을 이해하는 중요한 시대적 맥락을 제공한다.52</p>
<h4>5.2.1 소프트 로보틱스의 혁신: Kirigami Skins</h4>
<p>2017년 12월 저명한 저널 <em>Science Robotics</em>에 발표된 “Kirigami skins make a simple soft actuator crawl” 연구는 당시 소프트 로보틱스 분야의 창의적인 혁신을 상징적으로 보여주는 사례이다.55</p>
<ul>
<li>
<p><strong>핵심 아이디어</strong>: 연구팀은 일본의 전통 종이 자르기 예술인 ’키리가미(kirigami)’에서 영감을 얻었다. 그들은 플라스틱 시트에 특정 패턴의 칼집을 내어, 이 ’스킨’을 부드러운 공압 액추에이터에 감쌌다. 액추에이터에 공기를 주입하여 팽창시키면, 스킨이 기계적 불안정성에 의해 좌굴(buckling)을 일으키며 평평한 표면에서 뱀의 비늘과 같은 3차원 돌기 구조로 변형된다.55</p>
</li>
<li>
<p><strong>작동 원리</strong>: 이 3차원 돌기 구조는 방향에 따라 마찰력이 달라지는 ’이방성(anisotropy)’을 만들어낸다. 즉, 로봇이 전진하려는 방향으로는 마찰이 적고, 후진하려는 방향으로는 마찰이 커진다. 이 특성 덕분에 액추에이터가 팽창과 수축을 반복하기만 해도, 로봇은 외부의 도움 없이 스스로 한 방향으로 기어가는 움직임을 만들어낼 수 있다.55</p>
</li>
<li>
<p><strong>기존 기술과의 차이점</strong>: 기존의 많은 소프트 로봇들이 이동을 위해 여러 개의 액추에이터를 복잡하게 제어해야 했던 것과 달리, 이 연구는 로봇 스킨의 ’구조적 지능(structural intelligence)’을 활용하여 단 하나의 액추에이터만으로 효율적인 이동성을 확보했다. 이는 시스템을 극도로 단순화하면서도 높은 기능성을 달성한 혁신적인 접근법이었다.55</p>
</li>
</ul>
<h4>5.2.2 그리핑 기술의 새로운 패러다임: Shear-Activated Grippers</h4>
<p>같은 해 12월, 로봇 공학 분야의 최고 권위지 중 하나인 <em>IEEE Transactions on Robotics</em>에는 “Grasping Without Squeezing: Design and Modeling of Shear-Activated Grippers“라는 논문이 게재되어 새로운 방식의 물체 파지 기술을 제시했다.58</p>
<ul>
<li>
<p><strong>핵심 아이디어</strong>: 거의 모든 기존의 로봇 그리퍼는 물체를 양쪽에서 눌러서(squeezing) 발생하는 수직항력(normal force)과 그로 인한 마찰력으로 물체를 잡았다. 이 연구는 이러한 전통적인 방식에서 벗어나, 도마뱀붙이(gecko)의 발바닥에서 영감을 받은 미세 섬유 구조의 건식 접착 필름을 사용했다. 이 특수 필름은 수직으로 누를 때는 접착력이 거의 없지만, 필름 표면을 따라 평행하게 당기는 ’전단력(shear force)’이 가해질 때만 강력한 접착력을 발휘하는 특성을 가진다.60</p>
</li>
<li>
<p><strong>작동 원리</strong>: 연구팀은 이 필름을 이용하여 물체를 ‘쥐지 않고’ 표면에 가볍게 댄 후, 필름을 당겨 전단력을 발생시킴으로써 물체를 들어 올리는 새로운 그리퍼를 개발했다. 접착을 해제할 때는 단순히 장력을 제거하기만 하면 된다.</p>
</li>
<li>
<p><strong>장점</strong>: 이 방식은 압력을 거의 가하지 않기 때문에 달걀이나 과일처럼 깨지기 쉬운 물체를 손상 없이 다룰 수 있다. 또한 그리퍼로 감싸기 어려운 커다란 공이나 병과 같이 볼록한 표면을 가진 물체도 안정적으로 파지할 수 있는 장점을 가진다.61</p>
</li>
</ul>
<p>2017년 말에 발표된 이 두 연구는 당시 첨단 로봇 공학이 AI의 ’두뇌’에 해당하는 알고리즘 개발뿐만 아니라, 로봇의 ’몸’에 해당하는 물리적 상호작용 방식을 근본적으로 혁신하는 데에도 집중하고 있었음을 보여준다. 키리가미 스킨은 ’기하학적 구조’를 통해 움직임을 생성했고, 전단 활성 그리퍼는 ’신소재’를 통해 파지 방식을 재정의했다. 이는 AI의 소프트웨어적 진화와 로봇의 하드웨어적 혁신이 서로를 추동하며 발전하는 중요한 관계로, 미래 지능형 로봇 시스템의 발전을 위해서는 이 두 가지 측면의 융합이 필수적임을 시사했다.</p>
<h2>6. 결론: 2017년 12월의 유산과 미래 전망</h2>
<p>2017년 12월은 인공지능 연구의 역사에서 ’양적 팽창’과 ’질적 도약’이 동시에 폭발적으로 일어난 결정적인 시기였다. 그 중심에 있었던 NIPS 2017은 AI 기술이 학계를 넘어 산업의 중심으로 이동하는 거대한 흐름을 상징했으며, 동시에 AI 연구의 패러다임을 근본적으로 바꿀 이론적, 기술적 돌파구들을 세상에 선보였다.</p>
<p>이 시기가 남긴 가장 중요한 유산은 단연 <strong>Transformer 아키텍처의 등장</strong>이다. “Attention Is All You Need” 논문은 순차적 처리의 족쇄를 끊어내고 병렬화와 확장성의 시대를 열었으며, 이는 곧바로 거대 언어 모델(LLM)의 폭발적인 발전으로 이어져 현재의 생성형 AI 혁명을 이끌었다. 2017년 12월에 뿌려진 이 씨앗은 AI 기술의 지형 자체를 완전히 재편했다.</p>
<p>두 번째 유산은 <strong>불완전 정보 게임의 정복</strong>이다. Libratus의 승리와 그 기반이 된 “Safe and Nested Subgame Solving” 연구는 AI가 단순히 계산을 넘어 불확실한 환경에서 고도의 전략적 추론을 할 수 있음을 증명했다. 이는 AI의 응용 범위를 명확한 규칙의 세계에서 정보가 비대칭적이고 경쟁적인 현실 세계의 문제들로 확장하는 중요한 교두보를 마련했다.</p>
<p>세 번째 유산은 <strong>이론적 깊이와 신뢰성에 대한 요구의 부상</strong>이다. 딥러닝의 눈부신 성공에 대한 반성적 고찰로서, 모델의 견고성을 수학적으로 보장하려는 시도(“Variance-based Regularization”), 모델의 신뢰도를 엄밀하게 평가하려는 노력(“A Linear-Time Kernel Goodness-of-Fit Test”), 그리고 딥러닝의 이론적 기반을 촉구하는 목소리(“Random Features for Large-Scale Kernel Machines” 수상 연설)가 학회의 중심에서 울려 퍼졌다. 이는 설명가능 AI(XAI) 연구의 본격적인 대두와 맞물려, AI가 사회적으로 수용 가능한 기술로 발전하기 위한 필수적인 성찰의 시작이었다.</p>
<p>로봇 공학 분야 역시 AI의 지능을 물리 세계에 구현하기 위한 하드웨어적 혁신을 병행했다. 키리가미 스킨과 전단 활성 그리퍼 같은 연구들은 소재와 구조의 혁신이 어떻게 로봇의 물리적 상호작용 능력을 근본적으로 바꿀 수 있는지를 보여주며, 지능형 로봇의 미래를 향한 중요한 단초를 제공했다.</p>
<p>결론적으로, 2017년 12월은 AI가 가능성의 영역을 넘어 현실 변혁의 핵심 도구로 자리매김하기 시작한, 진정한 의미의 ’변곡점’이었다. 이 시기에 제시된 아이디어들은 이후 AI 연구의 핵심 의제가 되었으며, 현재 우리가 경험하고 있는 AI 기술의 발전은 대부분 이 시기에 싹튼 개념들의 연장선 위에 있다. 따라서 2017년 12월의 연구 동향을 깊이 이해하는 것은 현재 AI 기술의 본질을 파악하고 미래를 전망하는 데 있어 필수적인 과정이라 할 수 있다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Top Research papers showcased at NIPS 2017 - Part 1 - Packt, https://www.packtpub.com/en-us/learning/how-to-tutorials/top-research-papers-showcased-nips-2017-part-1</li>
<li>What You Need to Know About NIPS 2017 - Synced Review, https://syncedreview.com/2017/12/04/what-you-need-to-know-about-nips-2017/</li>
<li>2017 Conference - NeurIPS 2025, https://neurips.cc/Conferences/2017</li>
<li>2017 Dates and Deadlines - NeurIPS 2025, https://neurips.cc/Conferences/2017/Dates</li>
<li>NIPS 2017 - Openresearch, https://www.openresearch.org/wiki/NIPS_2017</li>
<li>[D] NIPS 2017 Summary! (unless an “official” one gets posted, and then remove this dreck) : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/7j2v74/d_nips_2017_summary_unless_an_official_one_gets/</li>
<li>NIPS Accepted Papers Stats - Robbie Allen - Medium, https://robbieallen.medium.com/nips-accepted-papers-stats-26f124843aa0</li>
<li>Deep Reinforcement Learning Symposium, NIPS 2017 - Google Sites, https://sites.google.com/view/deeprl-symposium-nips2017/home</li>
<li>Solving Imperfect-Information Games - NIPS ’17 Best Paper with Tuomas Sandholm - #99, https://www.youtube.com/watch?v=ZlPPp_xokd4</li>
<li>Best Reviewer Awards - NeurIPS 2025, https://neurips.cc/Conferences/2017/Awards</li>
<li>NIPS-2017 Best paper “Explainability was one of the major reasons the paper was given the award” - human-centered.ai, https://human-centered.ai/2018/01/25/nips-2017-best-paper-explainability-one-major-reasons-paper-given-award/</li>
<li>Ben Recht wins NIPS Test of Time Award - EECS at Berkeley, https://eecs.berkeley.edu/news/ben-recht-wins-nips-test-time-award/</li>
<li>Carnegie Mellon Artificial Intelligence Beats Top Poker Pros - News, https://www.cmu.edu/news/stories/archives/2017/january/AI-beats-poker-pros.html</li>
<li>Artificial Intelligence Masters The Game of Poker – What Does That Mean For Humans?, https://bernardmarr.com/artificial-intelligence-masters-the-game-of-poker-what-does-that-mean-for-humans/</li>
<li>Libratus: the world’s best poker player - The Gradient, https://thegradient.pub/libratus-poker/</li>
<li>Safe and Nested Subgame Solving for Imperfect-Information Games - arXiv, https://arxiv.org/pdf/1705.02955</li>
<li>Safe and Nested Subgame Solving for Imperfect-Information Games - ResearchGate, https://www.researchgate.net/publication/316779789_Safe_and_Nested_Subgame_Solving_for_Imperfect-Information_Games</li>
<li>Safe and Nested Subgame Solving for Imperfect-Information Games - NSF PAR, https://par.nsf.gov/servlets/purl/10077429</li>
<li>Superhuman AI for heads-up no-limit poker: Libratus beats top professionals - Alan Kuhnle, https://www.alankuhnle.com/teaching/f24-631/lecture-slides/m7-poker/science.aao1733.pdf</li>
<li>Breakthrough applications of artificial intelligence in Texas Hold'em poker - ITM Web of Conferences, https://www.itm-conferences.org/articles/itmconf/pdf/2025/09/itmconf_cseit2025_01020.pdf</li>
<li>Safe and Nested Subgame Solving for Imperfect-Information Games - Semantic Scholar, https://www.semanticscholar.org/paper/Safe-and-Nested-Subgame-Solving-for-Games-Brown-Sandholm/9b7a9f38184e4ea0f0cb54b3ed258a053b7cf7af</li>
<li>Safe and Nested Subgame Solving for Imperfect-Information … - NIPS, http://papers.neurips.cc/paper/6671-safe-and-nested-subgame-solving-for-imperfect-information-games.pdf</li>
<li>Variance-based Regularization with Convex Objectives - NIPS, http://papers.neurips.cc/paper/6890-variance-based-regularization-with-convex-objectives.pdf</li>
<li>Variance-based Regularization with Convex Objectives - Journal of Machine Learning Research, https://jmlr.org/papers/volume20/17-750/17-750.pdf</li>
<li>[1610.02581] Variance-based regularization with convex objectives - arXiv, https://arxiv.org/abs/1610.02581</li>
<li>[PDF] Variance-based Regularization with Convex Objectives | Semantic Scholar, https://www.semanticscholar.org/paper/Variance-based-Regularization-with-Convex-Namkoong-Duchi/6e77765dd3250fc671c413b44554087bad43ad92</li>
<li>A Linear-Time Kernel Goodness-of-Fit Test - NIPS, http://papers.neurips.cc/paper/6630-a-linear-time-kernel-goodness-of-fit-test.pdf</li>
<li>A Linear-Time Kernel Goodness-of-Fit Test Mikolaj Kasprzak (Université du Luxembourg) Time Friday, Sept 27, 2019, https://math.uni.lu/thalmaier/Sem_Prob/Kasprzak2.pdf</li>
<li>(PDF) A Linear-Time Kernel Goodness-of-Fit Test - ResearchGate, https://www.researchgate.net/publication/317062614_A_Linear-Time_Kernel_Goodness-of-Fit_Test</li>
<li>Reviews: A Linear-Time Kernel Goodness-of-Fit Test - NIPS, https://papers.nips.cc/paper/2017/file/979d472a84804b9f647bc185a877a8b5-Reviews.html</li>
<li>Random Features for Large-Scale Kernel Machines - People @EECS, https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf</li>
<li>Random Features for Large-Scale Kernel Machines - NIPS, https://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf</li>
<li>Random features for large-scale kernel machines, http://www.gatsby.ucl.ac.uk/tea/tea_archive/attached_files/randomFeaturesRahimiRecht.pdf</li>
<li>Random Features for Large-Scale Kernel Machines - NIPS, https://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines</li>
<li>[PDF] Random Features for Large-Scale Kernel Machines | Semantic Scholar, https://www.semanticscholar.org/paper/Random-Features-for-Large-Scale-Kernel-Machines-Rahimi-Recht/7a59fde27461a3ef4a21a249cc403d0d96e4a0d7</li>
<li>Random Features for Large-Scale Kernel Machines | Request PDF - ResearchGate, https://www.researchgate.net/publication/221620515_Random_Features_for_Large-Scale_Kernel_Machines</li>
<li>www.geeksforgeeks.org, <a href="https://www.geeksforgeeks.org/deep-learning/rnn-vs-lstm-vs-gru-vs-transformers/#:~:text=Limitations%20of%20LSTMs,parallel%20which%20slows%20down%20training.">https://www.geeksforgeeks.org/deep-learning/rnn-vs-lstm-vs-gru-vs-transformers/#:~:text=Limitations%20of%20LSTMs,parallel%20which%20slows%20down%20training.</a></li>
<li>RNN vs LSTM vs GRU vs Transformers - GeeksforGeeks, https://www.geeksforgeeks.org/deep-learning/rnn-vs-lstm-vs-gru-vs-transformers/</li>
<li>Why does the transformer do better than RNN and LSTM in long-range context dependencies? - AI Stack Exchange, https://ai.stackexchange.com/questions/20075/why-does-the-transformer-do-better-than-rnn-and-lstm-in-long-range-context-depen</li>
<li>Recurrent neural network - Wikipedia, https://en.wikipedia.org/wiki/Recurrent_neural_network</li>
<li>Attention is All you Need - NIPS, https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf</li>
<li>Attention Is All You Need - Wikipedia, https://en.wikipedia.org/wiki/Attention_Is_All_You_Need</li>
<li>How “Attention is All You Need” Changed Language Processing? - Digital Purview, https://digitalpurview.com/attention-is-all-you-need/</li>
<li>[PDF] Attention is All you Need - Semantic Scholar, https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776</li>
<li>‪Ashish Vaswani‬ - ‪Google Scholar‬, https://scholar.google.com/citations?user=oR9sCGYAAAAJ&amp;hl=en</li>
<li>johnvastola.medium.com, <a href="https://johnvastola.medium.com/the-impact-of-the-attention-is-all-you-need-paper-on-nlp-1496c8510a13#:~:text=The%20&#x27;Attention%20is%20All%20You%20Need&#x27;%20paper%20has%20had%20a,exciting%20advancements%20in%20the%20field.">https://johnvastola.medium.com/the-impact-of-the-attention-is-all-you-need-paper-on-nlp-1496c8510a13#:~:text=The%20’Attention%20is%20All%20You%20Need’%20paper%20has%20had%20a,exciting%20advancements%20in%20the%20field.</a></li>
<li>What do we need to build explainable AI systems for the medical domain? - arXiv, https://arxiv.org/abs/1712.09923</li>
<li>arXiv:1712.09923v1 [cs.AI] 28 Dec 2017, https://www.edit.fis.uni-hamburg.de/ws/files/55705932/1712.09923v1.pdf</li>
<li>[1712.00547] Explainable AI: Beware of Inmates Running the Asylum Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences - arXiv, https://arxiv.org/abs/1712.00547</li>
<li>[1705.07874] A Unified Approach to Interpreting Model Predictions - arXiv, https://arxiv.org/abs/1705.07874</li>
<li>[1703.01365] Axiomatic Attribution for Deep Networks - arXiv, https://arxiv.org/abs/1703.01365</li>
<li>ICRAS 2017｜Robotics and Automation Sciences, https://www.icras.org/icras17.html</li>
<li>IROS 2017 : IEEE/RSJ International Conference on Intelligent Robots and Systems, http://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=51323</li>
<li>IROS 2017, Vancouver, Canada - Home, https://ewh.ieee.org/conf/iros/2017/iros2017.org/index.html</li>
<li>Kirigami skins make a simple soft actuator crawl - Harvard University, https://scholar.harvard.edu/files/bertoldi/files/ahmad_science_robotics.pdf</li>
<li>Kirigami skins make a simple soft actuator crawl - PubMed, https://pubmed.ncbi.nlm.nih.gov/33141681/</li>
<li>Kirigami skins make a simple soft actuator crawl | Request PDF - ResearchGate, https://www.researchgate.net/publication/323338291_Kirigami_skins_make_a_simple_soft_actuator_crawl</li>
<li>Hawkes Lab | Mechanical Engineering - UC Santa Barbara, https://www.hawkeslab.com/</li>
<li>Grasping without squeezing: Shear adhesion gripper with fibrillar thin film, https://snu.elsevierpure.com/en/publications/grasping-without-squeezing-shear-adhesion-gripper-with-fibrillar-</li>
<li>Grasping without squeezing: Shear adhesion gripper with fibrillar thin film - SciSpace, https://scispace.com/pdf/grasping-without-squeezing-shear-adhesion-gripper-with-4kfjdwk6xa.pdf</li>
<li>Gripper device using shear-controlled dry adhesive film | Explore Technologies - Stanford, https://techfinder.stanford.edu/technology/gripper-device-using-shear-controlled-dry-adhesive-film</li>
<li>Grasping without squeezing: Shear adhesion gripper with fibrillar thin film, https://www.semanticscholar.org/paper/Grasping-without-squeezing%3A-Shear-adhesion-gripper-Hawkes-Christensen/d7982917a1a24b8469affcc8793239adcbe4e69b</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>