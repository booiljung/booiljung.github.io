<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2017년 4월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2017년 4월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2017년 AI 및 로봇 연구 동향</a> / <span>2017년 4월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2017년 4월 AI 및 로봇 연구 동향</h1>
<h2>1. 서론: 2017년 AI 연구의 전환점</h2>
<p>2017년 초는 딥러닝이 다양한 분야에서 성공을 거둔 후, 학계가 그 성공의 근본적인 원리를 탐구하고 기존 방법론의 한계를 극복하려는 시도가 활발했던 시기였다. 이 시기는 주요 학회들을 통해 새로운 아이디어들이 폭발적으로 발표되던 중요한 전환점이었다. 제5회 International Conference on Learning Representations (ICLR)가 2017년 4월 24일부터 26일까지 프랑스 툴롱에서 개최되었으며, 이는 본 보고서의 핵심 시간적 배경이 된다.1 또한, 곧이어 개최될 주요 학회인 IEEE Conference on Computer Vision and Pattern Recognition (CVPR, 7월 21-26일)과 IEEE International Conference on Robotics and Automation (ICRA, 5월 29일-6월 3일)에서 발표될 연구들의 사전 공개(preprint)가 활발히 이루어지던 시점이기도 했다.3 이 시기 연구 동향의 특징은 기존의 CNN, RNN 아키텍처의 성공에 안주하지 않고, 모델의 일반화(generalization) 능력, 생성 모델(generative model)의 학습 안정성, 그리고 아키텍처 설계 자체의 자동화와 같은 근본적인 문제에 대한 깊은 성찰이 이루어졌다는 점이다.</p>
<p>이러한 배경은 AI 분야가 ‘성공의 재현’ 단계에서 ‘성공의 원리 이해’ 단계로 넘어가는 중요한 변곡점에 있었음을 시사한다. 딥러닝의 성능은 입증되었지만, 왜 그렇게 잘 작동하는지에 대한 이론적 설명은 부족했다. ICLR 2017에서 발표된 “Understanding deep learning requires rethinking generalization“이나 “Towards Principled Methods for Training Generative Adversarial Networks“와 같은 논문들은 단순히 성능을 개선하는 것을 넘어, 기존의 통념에 의문을 제기하고 근본적인 원리를 파고들었다.5 이는 당시 학계가 지적 공백을 메우기 위해 집단적으로 노력하고 있었음을 보여준다.</p>
<p>본 보고서는 2017년 4월을 기점으로 발표된 AI 및 로봇 분야의 핵심 연구들을 심층적으로 분석하여, 당시의 기술적 성취를 조명하고 이것이 현재 AI 기술의 발전에 어떠한 유산을 남겼는지 추적하는 것을 목표로 한다. 보고서는 일반화, 생성 모델, 아키텍처 탐색, 3차원 인지 및 로보틱스라는 네 가지 핵심 주제를 중심으로 구성된다. 특히, 이 시기의 연구들은 불과 두 달 뒤인 2017년 6월 12일에 발표될 “Attention Is All You Need” 논문, 즉 트랜스포머 아키텍처의 등장 직전에 이루어진 마지막 주요 성과들이라는 점에서 중요한 역사적 의미를 지닌다.7 당시 연구들은 RNN과 CNN 패러다임 내에서 정점에 도달했음을 보여주며, 동시에 장거리 의존성 문제나 학습 안정성 등 새로운 아키텍처의 등장을 촉발한 문제들을 명확히 정의하는 역할을 했다.</p>
<h2>2.  딥러닝 일반화에 대한 근본적 재고찰</h2>
<h3>2.1 핵심 연구: “Understanding deep learning requires rethinking generalization” (ICLR 2017 최우수 논문상)</h3>
<p>이 논문은 파라미터 수가 데이터 수보다 훨씬 많은 대규모 신경망이 어떻게 훈련 데이터에 과적합되지 않고 우수한 일반화 성능을 보이는가라는 근본적인 질문을 던졌다. 기존의 통계적 학습 이론(VC 차원, Rademacher 복잡도 등)과 정규화(regularization) 기법만으로는 이 현상을 설명할 수 없음을 주장하며 딥러닝 커뮤니티에 큰 파장을 일으켰다.5</p>
<h4>2.1.1 핵심 실험 및 결과</h4>
<p>연구진은 일련의 체계적인 실험을 통해 기존의 통념을 반박했다.</p>
<ol>
<li>
<p><strong>랜덤 레이블 암기:</strong> 가장 충격적인 실험 결과는 CIFAR-10과 ImageNet 같은 표준 데이터셋의 이미지에 완전히 무작위 레이블을 할당하고 최신 CNN 모델(예: Inception)을 훈련시킨 것이었다. 놀랍게도, 모델은 어떠한 통계적 규칙성도 없는 이 무작위 레이블을 완벽하게(훈련 오차 0) 암기할 수 있었다.5 이는 신경망이 대규모 데이터를 순수하게 암기할 수 있는 엄청난 유효 용량(effective capacity)을 가지고 있음을 실험적으로 증명한 것이다. 모델의 용량이 사실상 무한하여 어떤 데이터든 맞출 수 있다는 사실은, 왜 실제 데이터에서는 과적합되지 않는지에 대한 의문을 더욱 증폭시켰다.</p>
</li>
<li>
<p><strong>정규화의 역할:</strong> L2 정규화, 드롭아웃, 데이터 증강과 같은 명시적 정규화 기법들이 일반화 성능을 향상시키는 데 도움이 될 수는 있지만, 무작위 레이블을 암기하는 현상을 근본적으로 막지는 못했다. 심지어 이러한 정규화 기법들을 모두 제거한 상태에서도 모델은 실제 데이터에 대해 여전히 좋은 일반화 성능을 보였다.9 이는 정규화가 일반화의 부수적인 개선 요소일 수는 있으나, 근본적인 원인은 아니라는 점을 강력하게 시사했다.</p>
</li>
</ol>
<h4>2.1.2 이론적 분석</h4>
<p>연구진은 실험 결과를 뒷받침하기 위해, 파라미터 수가 데이터 포인트 수(<span class="math math-inline">n</span>)를 초과하는 간단한 2계층 ReLU 네트워크(<span class="math math-inline">2n+d</span>개의 파라미터)가 어떠한 레이블링이든 훈련 오차 0으로 학습할 수 있음을 이론적으로 증명했다.9 이는 실제 딥러닝 모델에서 파라미터가 데이터보다 훨씬 많은 상황의 표현력을 뒷받침하며, 모델의 용량만으로는 일반화 성능을 설명할 수 없음을 재확인시켰다.</p>
<h4>2.1.3 시사점 및 영향</h4>
<p>이 논문의 가장 중요한 시사점은 딥러닝의 일반화 비밀이 모델 아키텍처나 명시적 정규화가 아닌, <strong>최적화 알고리즘 자체</strong>에 숨겨져 있을 수 있다는 가능성을 제시한 점이다. 모델의 용량이 무한하여 수많은 해(solution) 중에서 어떤 것이든 선택할 수 있는 상황에서, 확률적 경사 하강법(SGD)과 같은 특정 옵티마이저는 암묵적으로 “단순하고” 일반화가 잘 되는 해를 선호하는 편향(implicit bias)을 가질 수 있다는 것이다. 즉, SGD가 손실 함수의 수많은 최소점(minima) 중에서도 특별히 ‘넓은’ 최소점을 찾아가는 경향이 있으며, 이러한 넓은 최소점이 좋은 일반화 성능과 관련이 있다는 후속 연구의 방향을 제시했다. 이 논문은 사실상 이론 딥러닝 연구의 초점을 모델 용량 분석에서 최적화 알고리즘의 암묵적 정규화 효과를 이해하는 방향으로 전환시키는 계기가 되었으며, 이는 오늘날까지도 활발히 연구되는 핵심 주제로 남아있다.</p>
<h2>3.  생성 모델의 이론적 도약과 응용</h2>
<p>2017년 4월은 생성 모델, 특히 생성적 적대 신경망(GAN)의 이론적 기반을 다지고 응용 가능성을 확장한 중요한 시기였다.</p>
<h3>3.1  생성적 적대 신경망(GAN)의 안정화</h3>
<h4>3.1.1 핵심 연구: “Towards Principled Methods for Training Generative Adversarial Networks” (ICLR 2017 구두 발표)</h4>
<p>이 논문은 GAN 학습이 불안정한 이유를 이론적으로 규명하며, 후속 연구에 결정적인 방향을 제시했다.</p>
<ul>
<li>
<p><strong>GAN 학습의 근본적 문제 진단:</strong> 핵심 주장은 실제 데이터 분포(<span class="math math-inline">P_r</span>)와 생성자 분포(<span class="math math-inline">P_g</span>)가 고차원 공간 내의 저차원 매니폴드(low-dimensional manifold)에 존재하며, 이들의 지지집합(support)이 거의 항상 서로소(disjoint)라는 것이다.6</p>
</li>
<li>
<p><strong>이론적 결과:</strong> 두 분포의 지지집합이 서로소일 경우, 두 분포를 완벽하게 구별하는 판별자(<span class="math math-inline">D</span>)가 항상 존재한다. 이 완벽한 판별자의 그래디언트는 <span class="math math-inline">P_r</span>과 <span class="math math-inline">P_g</span>의 지지집합 위에서 0이 된다. 이는 생성자(<span class="math math-inline">G</span>)가 학습에 필요한 유의미한 그래디언트 정보를 얻지 못하고 학습이 멈추거나 불안정해지는 현상(vanishing gradients)의 직접적인 원인이 됨을 증명했다.6 최적의 판별자 <span class="math math-inline">D^*(x)</span>는 <span class="math math-inline">P_r(x) / (P_r(x) + P_g(x))</span> 형태를 가지는데, <span class="math math-inline">P_r</span>과 <span class="math math-inline">P_g</span>의 지지집합이 겹치지 않는 영역에서 <span class="math math-inline">D^*(x)</span>는 0 또는 1이 되어 생성자의 손실 함수 <span class="math math-inline">\log(1 - D(x))</span>의 그래디언트가 소실된다.</p>
<p><span class="math math-display">
\nabla_{\theta} \mathbb{E}_{z \sim p(z)} \to 0
</span></p>
</li>
</ul>
<p>이러한 분석은 기존 GAN이 최소화하려던 Jensen-Shannon (JS) 발산의 근본적인 한계를 명확히 지적했다. 두 분포가 서로소일 때 JS 발산 값은 <span class="math math-inline">\log 2</span>로 상수가 되어, 두 분포가 얼마나 ‘멀리’ 떨어져 있는지에 대한 정보를 제공하지 못해 학습 신호로 작용할 수 없다. 이 연구는 분포 간의 ’거리’를 측정하는 더 나은 방법이 필요함을 강력하게 시사했으며, 이는 곧바로 Earth-Mover’s (Wasserstein) 거리를 사용하는 Wasserstein GAN (WGAN)의 등장으로 이어졌다. 즉, 이 논문은 GAN 연구의 패러다임을 발산 최소화에서 최적 수송(optimal transport) 이론으로 전환시키는 결정적인 이론적 발판을 제공했다.</p>
<h3>3.2  분리 표현 학습과 β-VAE</h3>
<h4>3.2.1 핵심 연구: “beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework” (ICLR 2017)</h4>
<ul>
<li>
<p><strong>목표:</strong> 비지도 학습 방식으로 데이터의 생성 요인(generative factors)들을 서로 독립적인 잠재 변수(latent variables)로 학습하는 것, 즉 분리된 표현(disentangled representation)을 학습하는 것을 목표로 했다.1</p>
</li>
<li>
<p><strong>방법론:</strong> 표준적인 Variational Autoencoder (VAE)의 목적 함수를 수정했다. VAE의 목적 함수는 재구성 오차와 잠재 변수 분포와 사전 분포(prior) 간의 KL-발산 항으로 구성된다. β-VAE는 이 KL-발산 항에 하이퍼파라미터 <span class="math math-inline">\beta</span>를 곱하여 가중치를 조절했다.13 β-VAE의 Evidence Lower Bound (ELBO)는 다음과 같이 표현된다.</p>
<p><span class="math math-display">
\mathcal{L}(\theta, \phi; x, z, \beta) = \mathbb{E}_{q_{\phi}(z|x)}[\log p_{\theta}(x|z)] - \beta D_{KL}(q_{\phi}(z|x) || p(z))
</span></p>
</li>
</ul>
<p><span class="math math-inline">\beta &gt; 1</span>일 때, 모델은 잠재 변수 채널의 용량(정보 병목)에 더 강한 제약을 받게 되어, 각 잠재 변수가 데이터의 독립적인 생성 요인을 더 효율적으로 학습하도록 강제된다.</p>
<p>이전에도 InfoGAN 등 분리 표현을 학습하려는 시도는 있었지만, β-VAE는 정보 이론적 관점에서 이 문제를 명확하게 정식화했다. <span class="math math-inline">\beta</span>라는 단일 하이퍼파라미터를 통해 ’재구성 정확도’와 ‘표현의 분리도’ 사이의 균형을 조절할 수 있는 원리를 제공함으로써, 분리 표현 연구에 대한 보다 체계적이고 이론적인 접근의 문을 열었다. 이는 이후 공정성(fairness), 설명가능 AI(XAI) 연구에도 중요한 영향을 미쳤다.</p>
<p>다음 표는 ICLR 2017에서 논의된 주요 생성 모델들의 특징을 비교 분석한 것이다.</p>
<p><strong>Table 1: ICLR 2017 주요 생성 모델 비교 분석</strong></p>
<table><thead><tr><th>모델명 (Model)</th><th>해결하고자 한 문제 (Problem Addressed)</th><th>수학적 목적 함수/핵심 아이디어 (Mathematical Objective / Key Idea)</th><th>주요 기여 및 한계 (Contribution &amp; Limitations)</th></tr></thead><tbody>
<tr><td><strong>GAN (Original)</strong></td><td>데이터 분포 학습 및 샘플 생성 (Learning data distribution and generating samples)</td><td>Minimax game on JS-divergence: <span class="math math-inline">\min_G \max_D V(D, G)</span></td><td>고품질 샘플 생성 가능성 제시. 학습 불안정, 모드 붕괴 문제 심각.</td></tr>
<tr><td><strong>“Towards Principled Methods…”</strong></td><td>GAN 학습의 불안정성 및 그래디언트 소실 (GAN training instability and vanishing gradients)</td><td><span class="math math-inline">P_r</span>과 <span class="math math-inline">P_g</span>의 지지집합이 서로소일 때 <span class="math math-inline">D</span>의 그래디언트가 0이 됨을 증명.</td><td>JS 발산의 근본적 한계 지적. WGAN 등 후속 연구의 이론적 기반 제공.</td></tr>
<tr><td><strong>β-VAE</strong></td><td>비지도 방식의 분리된 표현 학습 (Unsupervised disentangled representation learning)</td><td><span class="math math-inline">\mathcal{L} = \mathbb{E}[\log p(x \vert z)] - \beta D_{KL}(q(z \vert x) \vert\vert p(z))</span></td><td><span class="math math-inline">\beta</span>를 통해 재구성과 분리 간의 균형 조절. 해석 가능한 잠재 공간 학습의 원리 제시.</td></tr>
</tbody></table>
<h3>3.3  응용 연구: 초고해상도 및 스타일 변환</h3>
<p>CVPR 2017에서 발표된 “Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network” (SRGAN)는 GAN을 사용하여 저해상도 이미지로부터 사실적인 고해상도 이미지를 생성하는 방법을 제시했다. 기존의 MSE 기반 손실 함수가 지나치게 부드러운(blurry) 결과를 내는 반면, 적대적 손실(adversarial loss)과 지각 손실(perceptual loss)을 함께 사용하여 질감(texture) 디테일이 살아있는 결과를 생성했다.14 이는 GAN이 단순한 이미지 생성을 넘어 실용적인 이미지 복원 문제에 강력한 도구가 될 수 있음을 보여준 대표적인 사례다.</p>
<h2>4.  강화학습을 통한 신경망 아키텍처 탐색</h2>
<h3>4.1 핵심 연구: “Neural Architecture Search with Reinforcement Learning” (ICLR 2017 구두 발표)</h3>
<p>이 연구는 신경망 아키텍처 설계를 인간 전문가의 직관과 경험에 의존하는 대신, 강화학습을 통해 자동화하는 Neural Architecture Search (NAS)라는 새로운 패러다임을 제안하여 큰 주목을 받았다.16</p>
<h4>4.1.1 방법론</h4>
<p>NAS의 핵심 방법론은 다음과 같은 세 단계로 구성된다.</p>
<ol>
<li>
<p><strong>Controller (Agent):</strong> 순환 신경망(RNN)을 컨트롤러로 사용하여 신경망 아키텍처(예: 필터 크기, 스트라이드, 연결 등)를 가변 길이의 토큰 시퀀스로 생성한다. 이 시퀀스 생성이 강화학습의 ’행동(action)’에 해당한다.18</p>
</li>
<li>
<p><strong>Reward:</strong> 생성된 아키텍처(자식 네트워크, child network)를 실제로 데이터셋(예: CIFAR-10)에 훈련시킨 후, 검증 데이터셋(validation set)에서의 정확도를 측정하여 이를 ’보상(reward)’으로 사용한다.17</p>
</li>
<li>
<p><strong>Policy Gradient:</strong> 컨트롤러 RNN은 이 보상을 최대화하는 방향으로 파라미터를 업데이트한다. 보상 신호가 미분 불가능하므로, REINFORCE와 같은 정책 그래디언트(policy gradient) 알고리즘을 사용하여 학습을 수행한다.19 컨트롤러의 파라미터  <span class="math math-inline">\theta_c</span>는 다음 그래디언트를 통해 업데이트된다.</p>
<p><span class="math math-display">
\nabla_{\theta_c} J(\theta_c) \approx \frac{1}{m} \sum_{k=1}^{m} \sum_{t=1}^{T} \nabla_{\theta_c} \log P(a_t | a_{(t-1):1}; \theta_c) R_k
</span></p>
</li>
</ol>
<h4>4.1.2 실험 결과</h4>
<ul>
<li>
<p><strong>CIFAR-10:</strong> 이 방법론을 통해 처음부터 탐색하여 찾아낸 ‘NASNet’ 아키텍처는 인간이 설계한 최고 성능의 아키텍처와 필적하는 성능(오류율 3.65%)을 달성했다.18</p>
</li>
<li>
<p><strong>Penn Treebank:</strong> 언어 모델링 작업에서는 기존에 널리 사용되던 LSTM 셀보다 우수한 성능을 보이는 새로운 순환 셀(recurrent cell)을 발견하여, 당시 최고 수준의 성능을 경신했다.17</p>
</li>
</ul>
<h4>4.1.3 시사점 및 영향</h4>
<p>NAS의 등장은 ‘학습하는 방법을 학습하는’ 메타러닝(meta-learning)의 성공적인 대규모 적용 사례였다. 이는 딥러닝 연구의 초점을 ’모델 아키텍처 자체’에서 ’좋은 아키텍처를 찾는 과정’으로 확장시켰다. 이 방법은 아키텍처 설계라는, 고도의 전문성과 창의성이 요구되던 작업을 자동화할 수 있다는 가능성을 증명했다. 비록 초기 NAS는 수백 개의 GPU로 수 주간 훈련해야 하는 엄청난 계산 비용을 요구했지만 18, 이 연구는 이후 미분기반 탐색(differentiable architecture search, DARTS), 가중치 공유(weight sharing) 등 효율적인 NAS 연구의 폭발적인 증가를 촉발하는 기폭제가 되었다.20</p>
<h2>5.  3차원 공간 인지와 로보틱스의 진화</h2>
<p>2017년 4월은 3차원 데이터 처리와 로보틱스 분야에서도 중요한 진전이 있었던 시기였다. 특히 3차원 포인트 클라우드를 직접 처리하는 딥러닝 방법론의 등장은 이 분야의 패러다임을 바꾸었다.</p>
<h3>5.1  PointNet: 3차원 포인트 클라우드 딥러닝</h3>
<h4>5.1.1 핵심 연구: “PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation” (CVPR 2017)</h4>
<ul>
<li>
<p><strong>문제점:</strong> PointNet 이전에는 3D 포인트 클라우드 데이터를 딥러닝 모델에 직접 입력하기 어려웠다. 포인트 클라우드는 순서가 없는 점들의 집합(unordered set)이므로, 순서에 민감한 표준적인 CNN이나 RNN을 적용할 수 없었다. 따라서 데이터를 3D 복셀 그리드(voxel grid)나 다중 시점 2D 이미지(multi-view images)로 변환하는 전처리 과정이 필요했으며, 이는 데이터의 본질적인 기하학적 정보를 손실시키거나 계산 비효율성을 야기했다.14</p>
</li>
<li>
<p><strong>핵심 아이디어: 순서 불변성:</strong> PointNet은 포인트 클라우드의 순서 불변성(permutation invariance) 문제를 해결하기 위해 대칭 함수(symmetric function)를 사용했다. 각 포인트를 독립적으로 처리한 후(공유 MLP 사용), 최대 풀링(max pooling) 연산을 통해 전체 포인트 클라우드의 전역 특징(global feature)을 추출한다. 이 과정은 입력 포인트의 순서가 바뀌어도 동일한 전역 특징을 생성한다.</p>
</li>
<li>
<p><strong>아키텍처:</strong> 입력 <span class="math math-inline">n</span>개의 3D 포인트에 대해, (1) 각 포인트를 고차원 특징 공간으로 매핑하는 공유 MLP, (2) 채널별 최대 풀링을 통해 전역 특징 벡터 생성, (3) 이 전역 특징을 기반으로 분류 또는 분할 작업을 수행하는 MLP로 구성된다.</p>
</li>
</ul>
<p>PointNet의 진정한 의의는 딥러닝이 처리할 수 있는 데이터의 종류를 ’집합(set)’으로 확장했다는 점이다. 이미지(그리드), 텍스트/시계열(시퀀스)에 이어, 순서가 없는 데이터 구조인 포인트 클라우드를 위한 최초의 효과적인 엔드-투-엔드 학습 프레임워크를 제공했다. 이는 딥러닝 아키텍처 설계의 중요한 원칙, 즉 데이터의 핵심적인 대칭성과 불변성을 네트워크 구조에 직접 반영해야 한다는 점을 보여주었다. 이 성과는 3D 컴퓨터 비전, 로보틱스, 자율주행(LiDAR 데이터 처리) 분야에서 패러다임 전환을 일으켰으며, 이후 PointNet++, DGCNN 등 수많은 후속 연구의 기반이 되었다.</p>
<h3>5.2  자율 시스템을 위한 비전 기술</h3>
<p>2017년 4월에 제출된 서베이 논문 “Computer Vision for Autonomous Vehicles: Problems, Datasets and State of the Art“는 자율주행을 위한 컴퓨터 비전 기술이 학문적 연구를 넘어 실제 적용 단계로 진입하면서 겪는 다양한 문제들을 종합적으로 다루었다.21 이 시기 주요 연구 분야는 다음과 같다.</p>
<ul>
<li>
<p><strong>적대적 공격:</strong> ICLR 2017 워크샵에서는 “Adversarial examples in the physical world“와 같은 연구가 발표되어, 디지털 공간뿐만 아니라 실제 물리 세계에서도 신경망이 적대적 공격에 취약할 수 있음을 보여주었다.23 이는 자율주행과 같이 안전이 중요한 시스템에서 딥러닝 모델의 강건성(robustness)이 심각한 문제임을 환기시켰다.</p>
</li>
<li>
<p><strong>자기 지도 학습:</strong> CVPR 2017에 발표된 “DeepPermNet: Visual Permutation Learning“은 이미지 패치를 섞고 원래 순서를 맞추는 퍼즐을 푸는 방식으로 레이블 없이 시각적 표현을 학습하는 방법을 제안했다.25 이는 대규모 레이블링 데이터 없이도 유용한 특징을 학습할 수 있는 가능성을 보여주어, 데이터 수집 비용이 높은 자율주행 분야에 중요한 시사점을 주었다.</p>
</li>
</ul>
<h3>5.3  로봇 제어 및 동역학</h3>
<p>ICRA 2017과 2017년 4월 발행된 <em>IEEE Robotics and Automation Letters</em> (RA-L)의 논문들은 딥러닝을 로봇의 인식 및 제어에 통합하려는 노력을 보여준다.4 이 시기 로보틱스 연구의 핵심적인 흐름은 전통적인 모델 기반 제어(model-based control)와 데이터 기반 학습(data-driven learning)을 결합하려는 시도였다.</p>
<p>순수 학습 기반의 엔드-투-엔드 제어는 안전성과 신뢰성을 보장하기 어렵고, 순수 모델 기반 제어는 비정형 환경의 복잡성을 다루기 어렵다는 한계가 있다. 따라서 이 둘 사이의 실용적인 절충점을 찾으려는 노력이 활발했다. 예를 들어, “SE3-nets“와 같은 연구는 동역학의 일부를 학습으로 대체하려 했고, “Motion Discontinuity-Robust Controller“와 같은 연구는 학습 기반 예측 모델이 야기할 수 있는 불확실성 속에서도 안정성을 보장하는 방법을 모색했다.4 이는 인식이나 복잡한 동역학처럼 모델링이 어려운 부분은 학습을 사용하고, 안정성이나 안전 보장과 같이 중요한 부분은 고전 제어 이론을 활용하는 하이브리드 접근 방식이다. 이러한 융합적 접근은 현재 로보틱스 연구의 주류를 이루는 방식의 초기 형태라 할 수 있다.</p>
<p>다음 표는 2017년 4월 전후로 발표된 주요 로보틱스 연구들을 정리한 것이다.</p>
<p><strong>Table 2: 2017년 4월 주요 로보틱스 연구 (ICRA &amp; RA-L)</strong></p>
<table><thead><tr><th>논문 제목 (Paper Title)</th><th>연구 분야 (Research Area)</th><th>핵심 기여 및 방법론 (Key Contribution &amp; Methodology)</th><th>출처 (Source)</th></tr></thead><tbody>
<tr><td>SE3-nets: Learning rigid body motion using deep neural networks</td><td>인식/동역학 (Perception/Dynamics)</td><td>딥러닝을 통해 6D 강체 변환(SE(3))을 직접 학습하는 네트워크 제안.</td><td>ICRA 2017</td></tr>
<tr><td>Motion Discontinuity-Robust Controller for Steerable Mobile Robots</td><td>제어 (Control)</td><td>동적 환경에서의 갑작스러운 속도 변화에 대응하는 최적화 기반 ICR 제어기.</td><td>RA-L, April 2017</td></tr>
<tr><td>A comparative analysis of tightly-coupled monocular, binocular, and stereo VINS</td><td>SLAM / 측위 (SLAM / Localization)</td><td>단안, 양안, 스테레오 시각-관성 항법 시스템(VINS)의 성능을 정밀하게 비교 분석.</td><td>ICRA 2017</td></tr>
<tr><td>3D-Printed Ionic Polymer Metal Composite Soft Crawling Robot</td><td>소프트 로보틱스 (Soft Robotics)</td><td>3D 프린팅 기술을 이용한 이온성 고분자 복합재 기반의 소프트 크롤링 로봇 개발.</td><td>ICRA 2017</td></tr>
<tr><td>Find your own way: Weakly-supervised segmentation of path proposals for urban autonomy</td><td>자율주행 (Autonomous Driving)</td><td>약지도 학습(weakly-supervised learning)을 이용한 주행 가능 경로 분할.</td><td>ICRA 2017</td></tr>
</tbody></table>
<h2>6. 결론: 2017년 4월의 유산과 미래 전망</h2>
<h3>6.1 주요 성과 요약</h3>
<p>2017년 4월은 딥러닝의 이론적 기반을 다지고, 응용 분야를 확장하며, 스스로 발전할 수 있는 능력을 부여한 중요한 시기였다.</p>
<ul>
<li>
<p><strong>이론적 성찰:</strong> 일반화의 미스터리를 파헤치고(ICLR Best Paper), GAN 학습의 불안정성을 이론적으로 규명함으로써 딥러닝의 작동 원리에 대한 더 깊은 이해를 추구했다.</p>
</li>
<li>
<p><strong>방법론적 혁신:</strong> β-VAE를 통해 분리 표현 학습의 원리를 제시하고, NAS를 통해 아키텍처 설계를 자동화했으며, PointNet으로 3D 집합 데이터 처리를 위한 새로운 길을 열었다.</p>
</li>
</ul>
<h3>6.2 미래에 대한 함의</h3>
<p>이 시기의 연구들은 단기적으로는 WGAN, 효율적인 NAS, 3D 인식 모델의 발전을 이끌었다. 장기적으로는 옵티마이저의 암묵적 편향, 표현의 공정성, AI의 강건성과 같은 오늘날의 핵심 연구 주제들의 지적 토대를 마련했다.</p>
<h3>6.3 시대적 의의: 트랜스포머 이전 시대의 정점</h3>
<p>본 보고서에서 분석한 연구들은 2017년 6월 “Attention Is All You Need” 논문이 발표되기 직전에 이루어진 성과들이다.7 이는 RNN과 CNN을 중심으로 한 딥러닝 패러다임이 도달한 기술적 정점을 보여주는 동시에, 이들 아키텍처가 가진 본질적인 한계(예: 장기 의존성, 순차 처리의 비효율성)를 극복하기 위한 새로운 돌파구가 왜 필요했는지를 역설적으로 보여준다. 따라서 2017년 4월은 한 시대의 완성이자, 곧이어 시작될 새로운 AI 혁명, 즉 트랜스포머와 거대 언어 모델 시대의 서곡이었던 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>5th International Conference on Learning Representations (ICLR 2017) - Proceedings.com, https://www.proceedings.com/content/068/068832webtoc.pdf</li>
<li>ICLR 2017, https://iclr.cc/archive/www/doku.php%3Fid=iclr2017:main.html</li>
<li>July 21-26, 2017, Honolulu, Hawaii, https://cvpr2017.thecvf.com/files/CFP_CVPR2017.pdf</li>
<li>2017 IEEE International Conference on Robotics and Automation, ICRA 2017, Singapore, Singapore, May 29 - June 3, 2017 - researchr publication, https://researchr.org/publication/icra-2017</li>
<li>Understanding deep learning requires rethinking generalization …, https://openreview.net/forum?id=Sy8gdB9xx</li>
<li>Towards Principled Methods for Training Generative Adversarial …, https://arxiv.org/pdf/1701.04862</li>
<li>Attention Is All You Need - Wikipedia, https://en.wikipedia.org/wiki/Attention_Is_All_You_Need</li>
<li>[1706.03762] Attention Is All You Need - arXiv, https://arxiv.org/abs/1706.03762</li>
<li>Understanding Deep Learning Requires Rethinking Generalization: My Thoughts and Notes, https://danieltakeshi.github.io/2017/05/19/understanding-deep-learning-requires-rethinking-generalization-my-thoughts-and-notes</li>
<li>[R] [1611.03530] Understanding deep learning requires rethinking generalization - Reddit, https://www.reddit.com/r/MachineLearning/comments/5cw3lr/r_161103530_understanding_deep_learning_requires/</li>
<li>(PDF) Understanding deep learning requires rethinking generalization - ResearchGate, https://www.researchgate.net/publication/310122390_Understanding_deep_learning_requires_rethinking_generalization</li>
<li>Towards Principled Methods for Training Generative Adversarial Networks - ResearchGate, https://www.researchgate.net/publication/312521065_Towards_Principled_Methods_for_Training_Generative_Adversarial_Networks</li>
<li>ICLR 2017 Conference Track - OpenReview, https://openreview.net/group?id=ICLR.cc/2017/conference</li>
<li>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), https://www.computer.org/csdl/proceedings/cvpr/2017/12OmNyoiYVr</li>
<li>2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017 - researchr publication, https://researchr.org/publication/cvpr-2017</li>
<li>ICLR 2017 Accepted Paper List - Paper Copilot, https://papercopilot.com/paper-list/iclr-paper-list/iclr-2017-paper-list/</li>
<li>Neural Architecture Search with Reinforcement Learning - Google Research, https://research.google.com/pubs/pub45826.html?source=post_page—————————</li>
<li>Neural Architecture Search with Reinforcement Learning - OpenReview, https://openreview.net/forum?id=r1Ue8Hcxg</li>
<li>arxiv.org, https://arxiv.org/abs/1611.01578</li>
<li>A Survey on Neural Architecture Search Based on Reinforcement Learning - arXiv, <a href="https://arxiv.org/pdf/2409.18163">https://arxiv.org/pdf/2409.18163?</a></li>
<li>Computer Vision for Autonomous Vehicles: Problems, Datasets and State of the Art - arXiv, https://arxiv.org/abs/1704.05519</li>
<li>Computer Vision for Autonomous Vehicles: Problems, Datasets and State of the Art, https://www.researchgate.net/publication/343469825_Computer_Vision_for_Autonomous_Vehicles_Problems_Datasets_and_State_of_the_Art</li>
<li>5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings - Researchr, https://researchr.org/publication/iclr-2017w</li>
<li>ICLR 2017 - Workshop Track - dblp, https://dblp.org/db/conf/iclr/iclr2017w</li>
<li>DeepPermNet: Visual Permutation Learning, https://arxiv.org/abs/1704.02729</li>
<li>IEEE RA-L 2017 | Philippe Fraisse Homepage - LIRMM, https://www.lirmm.fr/~fraisse/archives/842</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>