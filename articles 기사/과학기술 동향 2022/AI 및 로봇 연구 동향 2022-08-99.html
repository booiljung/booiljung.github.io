<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2022년 8월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2022년 8월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2022년 AI 및 로봇 연구 동향</a> / <span>2022년 8월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2022년 8월 AI 및 로봇 연구 동향</h1>
<h2>1. 서론</h2>
<p>2022년은 인공지능(AI) 및 로봇 기술 역사에서 중요한 변곡점으로 기록된다. 대규모 파운데이션 모델(Foundation Models)의 능력이 폭발적으로 증가하며 디지털 영역에서의 가능성을 확장했고, 동시에 이 추상적 지능을 물리적 세계와 상호작용하게 하려는 ‘체화된 AI(Embodied AI)’ 연구가 본격적인 궤도에 올랐다.1 이러한 두 거대한 흐름이 교차하던 2022년 8월은, AI의 산업적 성숙과 학문적 깊이가 공존하며 새로운 패러다임을 예고하던 시점이었다.</p>
<p>본 보고서는 2022년 8월을 중심으로 AI와 로봇 분야에서 발표된 핵심 연구 성과와 산업 동향을 심층적으로 분석한다. 이를 위해 상반기 기술 수준의 정점을 보여준 컴퓨터 비전 및 패턴 인식 학회(CVPR)의 주요 성과를 복기하고, 하반기 연구 방향을 예고한 지능형 로봇 및 시스템 국제학회(IROS)와 유럽 컴퓨터 비전 학회(ECCV)의 주제를 분석한다. 또한, 당월 arXiv에 공개된 최신 논문들을 통해 가장 선도적인 연구 흐름을 추적하여, 당시 기술 지형도를 입체적으로 조망하는 것을 목적으로 한다.3 2022년의 주요 학회 일정과 주제는 아래 표와 같다.</p>
<table><thead><tr><th>학회명 (Conference)</th><th>개최 시기 (Date)</th><th>장소 (Location)</th><th>주요 주제/테마 (Theme)</th></tr></thead><tbody>
<tr><td>ACL 2022</td><td>May 22-27</td><td>Dublin, Ireland</td><td>“Language Diversity: from Low Resource to Endangered Languages”</td></tr>
<tr><td>CVPR 2022</td><td>June 19-24</td><td>New Orleans, USA</td><td>Premier Computer Vision Event</td></tr>
<tr><td>ECCV 2022</td><td>October 23-27</td><td>Tel Aviv, Israel</td><td>European Conference on Computer Vision</td></tr>
<tr><td>IROS 2022</td><td>October 23-27</td><td>Kyoto, Japan</td><td>“Embodied AI for a Symbiotic Society”</td></tr>
</tbody></table>
<p>이 표는 2022년 8월이 상반기 최대 컴퓨터 비전 학회인 CVPR의 성과가 확산되고, 하반기 최대 로봇 학회인 IROS와 또 다른 주요 비전 학회인 ECCV의 의제가 구체화되던 중요한 시점이었음을 보여준다. 이는 당시 연구 커뮤니티가 기존 성과를 종합하고 새로운 방향을 모색하던 역동적인 상황을 반영한다.</p>
<h2>2.  2022년 8월 인공지능(AI) 분야 주요 연구 동향</h2>
<h3>2.1  산업계 동향 및 AI 도입 현황: 성숙과 과제의 공존</h3>
<p>2022년 중반, AI 기술은 실험 단계를 넘어 산업 전반의 핵심 동력으로 자리 잡았다. McKinsey의 ‘The state of AI in 2022’ 보고서에 따르면, 지난 5년간 AI 기술의 산업 도입은 괄목할 만한 성장을 이루었으나, 동시에 기술 발전 속도를 따라가지 못하는 거버넌스의 문제가 부상했다.7</p>
<h4>2.1.1 AI 도입의 보편화</h4>
<p>2017년 20%에 불과했던 기업의 AI 도입률은 2022년 50%에 도달하며 두 배 이상 증가했다. 이는 AI가 더 이상 소수 혁신 기업의 전유물이 아님을 의미한다. 또한, 기업이 활용하는 평균 AI 역량의 수도 2018년 1.9개에서 2022년 3.8개로 두 배 늘어났다. 특히 로봇 프로세스 자동화(RPA)와 컴퓨터 비전은 꾸준히 가장 많이 도입되는 기술이었으며, 자연어 텍스트 이해(NLU) 기술은 2018년 중위권에서 2022년 컴퓨터 비전 다음으로 높은 순위로 부상하며 그 중요성이 급격히 커졌음을 보여주었다.7</p>
<h4>2.1.2 투자 증가 및 가치 창출 영역의 변화</h4>
<p>AI 도입 확대와 함께 투자 규모도 지속적으로 증가했다. 디지털 예산의 5% 이상을 AI에 투자한다고 응답한 기업의 비율이 과반을 넘어섰으며, 응답자의 63%는 향후 3년간 투자를 더욱 늘릴 것이라고 전망했다. 주목할 점은 AI가 가치를 창출하는 영역의 변화다. 2018년에는 제조 및 리스크 관리 기능에서 AI의 가치가 가장 크게 나타났으나, 2022년에는 마케팅 및 영업, 제품 및 서비스 개발, 전략 및 기업 금융 부문에서 가장 큰 수익 증대 효과가 보고되었다. 비용 절감 측면에서는 공급망 관리가 가장 큰 혜택을 본 것으로 나타났다. 이는 AI의 적용 범위가 백오피스 효율화를 넘어 고객 대면 활동과 기업의 핵심 전략 기능으로 깊숙이 확장되고 있음을 시사한다.7</p>
<h4>2.1.3 심화되는 ‘역량-안전성 격차’</h4>
<p>이러한 폭발적인 성장 이면에는 중대한 과제가 존재한다. AI 모델의 성능과 경제성은 지난 몇 년간 극적으로 개선되었다. 2018년 이후 이미지 분류 시스템의 훈련 비용은 63.6% 감소했고, 훈련 시간은 94.4% 단축되었다.8 이러한 비용 효율성은 AI 도입을 가속화하는 주된 요인이었다. 그러나 기술 역량이 기하급수적으로 발전하는 동안, AI 관련 리스크를 완화하려는 기업의 노력은 2019년 이후 실질적인 증가 없이 정체 상태에 머물렀다.7</p>
<p>이러한 현상은 기술 발전 속도와 사회적, 윤리적 대비 상태 간의 위험한 불균형, 즉 ’역량-안전성 격차’가 심화되고 있음을 드러낸다. 실제로 AI 시스템이 더 크고 복잡해질수록 광범위한 작업에서 더 나은 성능을 보이지만, 동시에 편향성과 같은 윤리적 문제의 잠재적 심각성도 함께 커진다. 2021년에 개발된 2800억 파라미터 모델은 2018년 최신 기술이었던 1억 1700만 파라미터 모델에 비해 유도된 독성(elicited toxicity)이 29% 증가하는 것으로 나타났다.8 이는 AI 모델의 규모와 성능이 향상될수록, 그에 상응하는 강력한 거버넌스와 리스크 관리 체계가 구축되지 않으면 오히려 더 큰 사회적 위험을 초래할 수 있음을 경고하는 명백한 증거다.</p>
<h3>2.2  2022년의 시대정신: 생성형 AI와 트랜스포머</h3>
<p>2022년 AI 분야의 기술적 담론을 지배한 단 하나의 키워드를 꼽는다면 단연 ’생성형 AI(Generative AI)’일 것이다. 특히 확산 모델(Diffusion Models)의 부상은 기술적 가능성과 대중의 인식을 모두 한 단계 끌어올린 결정적 사건이었다.1</p>
<h4>2.2.1 패러다임 전환: 확산 모델의 부상</h4>
<p>DALL-E, Stable Diffusion, Midjourney와 같은 텍스트-이미지 생성 모델들은 2022년에 주류로 부상하며 AI 분야에 큰 충격을 주었다. 이 모델들은 복잡한 자연어 프롬프트를 이해하고, 이를 기반으로 매우 사실적이거나 창의적인 고품질 이미지를 생성하는 능력을 선보였다. “공원에서 자전거를 타는 다스베이더“와 같이 현실에 존재하지 않는 개념의 조합도 시각화할 수 있게 되면서, AI가 단순히 데이터를 분석하고 분류하는 것을 넘어 인간의 창의적 영역에 진입했음을 알렸다.1 이러한 기술은 단순한 소비자용 애플리케이션을 넘어, 저비용으로 방대한 양의 학습 데이터를 생성하는 등 다른 머신러닝 분야의 발전을 촉진할 잠재력을 지닌 것으로 평가되었다.1</p>
<h4>2.2.2 모든 것의 기반: 트랜스포머 아키텍처의 지배력</h4>
<p>생성형 AI를 포함한 2022년의 거의 모든 AI 혁신은 2017년에 발표된 트랜스포머(Transformer) 아키텍처에 그 뿌리를 두고 있다. 본래 기계 번역과 같은 자연어 처리(NLP) 문제를 해결하기 위해 고안된 트랜스포머는, 시퀀스 기반 데이터 내의 장거리 의존성(long-range dependencies)을 효과적으로 포착하는 ‘셀프 어텐션(self-attention)’ 메커니즘 덕분에 다양한 분야로 빠르게 확장되었다. 2022년에 이르러 트랜스포머는 DALL-E 2, Stable Diffusion과 같은 텍스트-이미지 모델의 핵심 구성 요소로 자리 잡았으며, GitHub Copilot의 기반이 된 Codex와 같은 코드 생성 도구에서도 그 위력을 입증했다.1</p>
<h4>2.2.3 창발적 능력(Emergent Capabilities)의 발견</h4>
<p>트랜스포머 모델의 가장 흥미로운 측면 중 하나는 모델의 규모(파라미터 수)가 특정 임계점을 넘어서면서 나타나는 ’창발적 능력’이다. 이는 모델을 훈련시킬 때 명시적으로 의도하지 않았던 새로운 능력이 저절로 발현되는 현상을 의미한다. 예를 들어, 대규모 언어 모델이 별도의 훈련 없이도 정확한 산술 연산을 수행하는 능력을 획득한 것이 대표적인 사례다.1 이러한 발견은 모델의 복잡성을 높이는 것이 단순히 기존 성능을 점진적으로 향상시키는 것을 넘어, 예측하지 못했던 질적인 도약을 이끌어낼 수 있음을 시사했다. 이는 AI 연구자들에게 모델의 규모 확장이 곧 새로운 기능의 발견으로 이어질 수 있다는 기대를 심어주었으며, 이후 GPT-4와 같은 초거대 모델 개발 경쟁을 더욱 가속화하는 계기가 되었다.</p>
<h3>2.3  주요 학회 및 arXiv 발표 심층 분석: 이론과 실제의 최전선</h3>
<p>2022년 8월 전후의 학문적 성과는 AI 기술의 이론적 깊이와 실제적 적용 가능성을 동시에 확장하는 방향으로 이루어졌다. 상반기 최대 컴퓨터 비전 학회인 CVPR 2022에서는 기하학적 문제 해결과 3D 인식 분야에서 중요한 방법론적 진보가 있었고, 하반기 ECCV 2022에서는 오픈-어휘 탐지와 대규모 데이터셋 구축이 주요 의제로 다뤄졌다. 또한, 8월 arXiv에는 AI의 장기적 비전과 안전성에 대한 근본적인 고찰을 담은 논문들이 발표되었다.</p>
<h4>2.3.1  CVPR 2022 (6월) 주요 성과 복기</h4>
<p>2022년 중반 컴퓨터 비전 분야의 기술적 정점을 보여준 CVPR 2022의 수상 논문들은 AI가 고전적인 컴퓨터 비전 문제를 해결하는 방식 자체를 혁신하고 있음을 명확히 보여주었다.</p>
<ul>
<li>
<p><strong>Best Paper Award - “Learning to Solve Hard Minimal Problems”</strong>: 이 논문은 3D 재구성, 이미지 매칭 등에서 발생하는 어려운 기하학적 최소 문제(minimal problems)를 해결하기 위해 머신러닝을 도입한 혁신적인 ‘pick &amp; solve’ 접근법을 제안했다.9 전통적인 ‘solve &amp; pick’ 방식은 다항식 시스템을 풀어 수많은 잠재적 해(spurious solutions 포함)를 모두 계산한 후, 그중에서 최적의 해를 선택하는 비효율적인 구조를 가졌다.11 이 연구는 이러한 패러다임을 전환하여, 주어진 문제에 대해 올바른 해로 수렴할 가능성이 가장 높은 ’시작점(starting problem-solution pair)’을 머신러닝 모델이 예측하도록 했다. 이후, 예측된 시작점에서 목표 문제의 해까지 ’호모토피 연속(homotopy continuation)’이라는 수치적 방법을 통해 경로를 추적하여 효율적으로 해를 찾는다.13 이 접근법은 수많은 가짜 해를 계산하는 과정을 생략함으로써, 3개 시점(three-view)에서의 4점 상대 포즈 추정 문제와 같은 고난도 문제를 기존 방식보다 10배 이상 빠른 수십 마이크로초 내에 해결하는 성과를 보였다.11 이는 AI가 특정 응용 문제를 푸는 것을 넘어, 문제 해결 ‘방법론’ 자체를 학습하고 최적화할 수 있음을 보여준 중요한 사례로 평가된다.</p>
</li>
<li>
<p><strong>Best Student Paper Award - “EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation”</strong>: 이 연구는 단일 RGB 이미지로부터 3D 객체의 6자유도(6DoF) 포즈를 추정하는 고전적인 PnP(Perspective-n-Points) 문제를 현대 딥러닝 프레임워크에 완벽하게 통합하는 길을 열었다.9 기존의 PnP 알고리즘은 결정론적(deterministic) 해를 구하는 과정이 본질적으로 미분 불가능하여, 2D-3D 대응점(correspondences)을 찾는 네트워크와 포즈 추정 알고리즘을 종단간(end-to-end)으로 학습시키는 데 한계가 있었다.17 EPro-PnP는 이 문제를 해결하기 위해 PnP의 출력을 단일 포즈 값이 아닌, SE(3) 매니폴드 상의 확률 분포(a distribution of pose)로 모델링했다.16 이후, 예측된 포즈 분포와 실제 정답 포즈 분포 간의 KL 발산(KL divergence)을 손실 함수로 사용하여 최소화함으로써, 네트워크가 미분 가능한 방식으로 전체 2D-3D 대응 관계(좌표 및 가중치)를 처음부터 학습할 수 있게 만들었다.17 이는 이산적인 분류 문제에 사용되던 Softmax를 연속적인 공간으로 확장한 것과 유사한 개념으로, 고전 기하학적 알고리즘과 딥러닝의 성공적인 융합 사례를 제시했다.17</p>
</li>
</ul>
<h4>2.3.2  ECCV 2022 (10월) 채택 논문 분석</h4>
<p>2022년 10월 개최 예정이었던 ECCV 2022에 채택된 논문들은 CVPR 이후 컴퓨터 비전 분야의 연구 흐름을 보여준다. 특히 오픈-어휘(open-vocabulary) 모델과 대규모 데이터셋 구축에 대한 관심이 두드러졌다.</p>
<ul>
<li>
<p><strong>“Open-Vocabulary DETR with Conditional Matching”</strong>: 이 논문은 기존 객체 탐지 모델이 사전에 정의된 닫힌 집합(closed-set)의 클래스만 탐지할 수 있었던 한계를 극복하고자 했다.21 연구진은 트랜스포머 기반의 종단간 객체 탐지기인 DETR을 오픈-어휘 설정으로 확장한 OV-DETR을 제안했다. 핵심 아이디어는 탐지 문제를 다중 클래스 분류가 아닌, ‘조건부 이진 매칭(conditional binary matching)’ 문제로 재구성한 것이다. CLIP과 같은 사전 학습된 비전-언어 모델을 활용하여, “강아지“와 같은 텍스트나 특정 강아지 이미지 같은 ’조건부 입력’이 주어졌을 때, 이미지 내에서 해당 객체를 ’찾았는지/못 찾았는지(matched/not matched)’를 판단하도록 모델을 훈련시킨다.23 이 방식은 훈련 데이터에 없었던 새로운 클래스에 대해서도 일반화가 가능하게 하여, 탐지 모델이 고정된 지식에서 벗어나 열린 세계로 나아가는 중요한 기술적 토대를 마련했다.25</p>
</li>
<li>
<p><strong>“HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling”</strong>: 4D 인간 감지 및 모델링 연구를 위한 대규모 멀티모달 데이터셋인 HuMMan이 공개되었다.21 이 데이터셋은 1000명의 피험자를 대상으로 40만 개의 시퀀스, 총 6000만 프레임에 달하는 방대한 규모를 자랑한다. 특히, 10대의 Azure Kinect 카메라와 iPhone 12 Pro Max를 포함한 센서 구성을 통해 컬러 이미지, 포인트 클라우드, 깊이 맵 등 다양한 모달리티의 데이터를 동기화하여 수집했다.27 또한, 2D/3D 키포인트, SMPL 파라미터, 텍스처가 입혀진 3D 메시 등 풍부한 주석을 제공하여 미세 동작 인식(fine-grained action recognition), 동적 인간 메시 복원, 파라메트릭 인체 복구 등 다양한 연구에 활용될 수 있도록 설계되었다.29 이는 복잡하고 동적인 인간의 움직임을 AI가 깊이 이해하고 생성하는 연구를 위한 필수적인 기반을 마련한 성과다.</p>
</li>
</ul>
<h4>2.3.3  2022년 8월 주요 arXiv 논문 분석</h4>
<p>2022년 8월 arXiv에 공개된 논문들은 당대의 기술적 성과를 넘어 AI의 근본적인 방향성과 안전성에 대한 깊은 성찰을 담고 있었다.</p>
<ul>
<li>
<p><strong>“The Alberta Plan for AI Research”</strong>: 강화학습의 선구자인 Richard Sutton 교수가 공동 저술한 이 논문은 AI 연구가 나아가야 할 장기적인 비전을 제시했다.32 이 계획은 단기적인 상업적 응용이나 특정 벤치마크 점수 향상을 넘어, ’계산 지능(computational intelligence)’의 근본 원리를 이해하는 것을 목표로 한다. 이를 위해 4가지 핵심 원칙을 제안했다: (1) 인공적인 훈련 데이터가 아닌, 에이전트가 환경과 상호작용하며 얻는 ’평범한 경험(ordinary experience)’에 기반한 학습, (2) 훈련과 테스트가 분리되지 않고 모든 시간 단계에서 학습이 일어나는 ‘시간적 균일성(temporal uniformity)’, (3) 무어의 법칙에 따라 계산 자원이 증가함에 비례하여 확장 가능한 ‘계산적 고려(computational considerations)’, (4) 환경 내 다른 지능적 에이전트와의 상호작용에 대한 초점.32 이는 당시 주류였던 대규모 데이터 기반의 지도 학습 패러다임에 대한 근본적인 대안을 제시하며, 끊임없이 변화하는 환경에 적응하는 지속적인 학습(continual learning)의 중요성을 역설했다.</p>
</li>
<li>
<p><strong>“Explainable AI Applications in Cyber Security: State-of-the-Art in Research”</strong>: 이 논문은 사이버 보안 분야에서 설명가능 AI(XAI)의 현주소를 조망한 최초의 포괄적인 서베이 연구다.33 논문은 침입 탐지, 악성코드 분석 등에서 AI 모델이 뛰어난 성능을 보이고 있지만, 대부분이 ’블랙박스’로 작동하여 의사결정 과정을 인간이 이해하고 신뢰하기 어렵다는 문제를 제기했다. 이러한 투명성 부족은 오탐(false positive) 발생 시 원인 분석을 어렵게 하고, 새로운 유형의 공격에 대한 시스템의 대응 방식을 예측할 수 없게 만들어 보안 전문가의 신뢰를 저하시킨다.33 이 연구는 헬스케어, 금융 등 다른 분야에서는 XAI 연구가 활발한 반면, 사이버 보안 분야에서는 관련 연구가 현저히 부족하다는 ’연구 공백(research gap)’을 명확히 지적했다. 이는 앞서 논의된 산업계의 ‘역량-안전성 격차’ 문제가 학문적으로도 중요한 과제임을 재확인시켜주는 연구라 할 수 있다.</p>
</li>
</ul>
<h2>3.  2022년 8월 로봇 분야 주요 연구 동향</h2>
<p>2022년 8월 로봇 분야는 AI 기술, 특히 대규모 언어 모델과의 융합을 통해 지능의 차원을 한 단계 높이려는 시도가 본격화된 시기였다. 산업 현장에서는 인간과의 협업이 가능한 지능형 로봇이 주목받았고, 학계에서는 로봇이 추상적인 명령을 이해하고 물리적 세계에서 수행하게 하는 ’체화된 AI(Embodied AI)’가 핵심 연구 주제로 부상했다.</p>
<h3>3.1  산업용 로봇의 진화: 지능과 협업을 향하여</h3>
<p>산업용 로봇은 더 이상 격리된 공간에서 정해진 작업만 반복하는 기계가 아니었다. AI, 머신러닝, 그리고 고도화된 센서 기술의 통합은 산업용 로봇을 비정형 환경에 적응하고 인간과 상호작용할 수 있는 지능형 시스템으로 변화시켰다.34</p>
<h4>3.1.1 패러다임의 전환: 경직성에서 적응성으로</h4>
<p>초기 산업용 로봇은 자동차 공장의 점용접과 같이 예측 가능하고 반복적인 작업을 수행하는 데 특화되어 있었다. 그러나 Industry 4.0과 스마트 제조의 확산은 생산 공정의 유연성과 자율성을 요구했고, 이에 따라 로봇 기술은 큰 전환점을 맞이했다. 강화학습을 통해 실시간으로 작업을 최적화하고, 컴퓨터 비전과 라이다(LiDAR) 센서로 주변 환경을 인식하며, 예측하지 못한 상황에 대처하는 능력을 갖추기 시작했다. 이러한 변화는 로봇의 적용 범위를 전통적인 제조업을 넘어 헬스케어, 물류, 농업 등 다양한 분야로 확장시키는 원동력이 되었다.34</p>
<h4>3.1.2 협동로봇(Cobots)의 부상과 민주화</h4>
<p>2010년대에 등장한 협동로봇(cobot)은 2022년에 이르러 산업 자동화의 중요한 축으로 자리 잡았다. 협동로봇은 안전 펜스 없이 인간 작업자와 같은 공간에서 작업할 수 있도록 설계되어, 인간의 정교함과 로봇의 힘, 정확성을 결합하는 새로운 협업 모델을 가능하게 했다. 이는 대규모 설비 투자가 어려운 중소기업(SME)에도 자동화의 문턱을 낮추는 ’자동화의 민주화’를 이끌었다. 그러나 협동로봇이 유연성을 높이는 데는 기여했지만, 복잡하고 비정형적인 환경에서 완전 자율 시스템과 비교하여 확장성에 한계가 있다는 논의도 함께 이루어졌다.34</p>
<h3>3.2  체화된 AI(Embodied AI)와 언어 모델의 결합: ’SayCan’의 등장</h3>
<p>2022년 로봇 연구의 가장 중요한 이정표 중 하나는 대규모 언어 모델(LLM)을 로봇의 ’두뇌’로 활용하려는 시도가 구체적인 성공 사례로 나타난 것이다. 그 중심에는 “Do As I Can, Not As I Say: Grounding Language in Robotic Affordances” 논문이 제안한 ‘SayCan’ 방법론이 있다.35</p>
<h4>3.2.1 핵심 과제: 언어의 물리적 그라운딩(Grounding)</h4>
<p>LLM은 “음료를 쏟았는데 어떻게 해야 할까?“와 같은 추상적인 질문에 대해 “스펀지를 가져와서 닦으세요“와 같은 절차적 지식을 생성할 수 있다. 그러나 이 지식은 물리적 세계에 기반하지 않은 순수한 텍스트 정보다.37 로봇이 이 지시를 수행하려면, ‘스펀지가 어디에 있는지’, ‘스펀지를 잡을 수 있는지’, ’테이블로 이동할 수 있는지’와 같은 자신의 현재 상태와 물리적 능력(affordances)을 고려해야 한다. LLM의 추상적 지식을 로봇의 구체적인 행동으로 연결하는 것, 즉 ’그라운딩’은 체화된 AI의 오랜 난제였다.35</p>
<h4>3.2.2 ‘SayCan’ 방법론: ’말할 수 있는 것’과 ’할 수 있는 것’의 결합</h4>
<p>SayCan은 이 그라운딩 문제를 해결하기 위해 두 가지 이질적인 정보 소스를 확률적으로 결합하는 독창적인 접근법을 제시했다. 이는 추상적 지능(LLM)과 물리적 지능(로봇)이 서로의 가능성을 평가하고 제약하며 최적의 행동을 찾아가는 ‘확률적 악수(The Probabilistic Handshake)’ 메커니즘으로 설명할 수 있다.</p>
<p>이 메커니즘의 작동 방식은 다음과 같다.</p>
<ol>
<li>
<p><strong>LLM의 역할 (The “Say”):</strong> 먼저, “음료를 닦아줘“와 같은 고수준의 지시와 로봇이 수행할 수 있는 모든 저수준 기술 목록(“스펀지 찾기”, “스펀지 집기”, “테이블로 이동하기” 등)을 LLM에 프롬프트로 제공한다. LLM은 각 기술이 주어진 지시를 해결하는 데 얼마나 유용할지를 평가하여 확률값, 즉 <span class="math math-inline">P(\text{skill is useful} \vert \text{instruction})</span>을 계산한다. 이는 행동의 ’의미론적 적합성’을 나타내지만, 아직 물리적 현실과는 무관하다.38</p>
</li>
<li>
<p><strong>로봇의 역할 (The “Can”):</strong> 다음으로, 로봇은 LLM이 평가한 각각의 기술에 대해, 현재 자신의 상태에서 해당 기술을 성공적으로 수행할 수 있을 확률을 자체적으로 계산한다. 이는 미리 학습된 가치 함수(value function) 또는 어포던스 함수(affordance function)를 통해 이루어진다. 이 결과로 <span class="math math-inline">P(\text{skill is feasible} \vert \text{state})</span>라는 확률값이 도출된다. 이는 행동의 ’물리적 가능성’을 나타내지만, 고수준의 맥락은 고려하지 않는다.40</p>
</li>
<li>
<p><strong>확률적 악수 (The Synthesis):</strong> 마지막으로 SayCan 알고리즘은 각 기술에 대해 이 두 확률값을 곱하여 최종 점수를 계산하고, 가장 높은 점수를 받은 기술을 다음 행동으로 선택한다.</p>
</li>
</ol>
<p>action=argskillmax​[P(useful∣instruction)×P(feasible∣state)]</p>
<p>이 과정은 단순한 파이프라인이 아니라, 상호 보완적인 필터링 과정이다. LLM이 의미적으로 그럴듯한 행동 후보군을 제시하면, 로봇의 어포던스 모델이 ‘현실 필터’ 역할을 하여 물리적으로 불가능하거나 성공 확률이 낮은 행동들의 가중치를 낮춘다. 이 우아한 곱셈 연산은 LLM의 추상적인 계획을 로봇의 즉각적인 물리적 현실에 효과적으로 그라운딩하여, 로봇이 지능적이고 실행 가능한 결정을 내리게 한다. 이 과정은 한쪽의 정보만으로는 성공할 수 없기에 ’악수’에 비유할 수 있다.38</p>
<p>실제 주방 환경에서 101개의 복잡한 지시를 수행한 실험에서 SayCan은 높은 계획 성공률(84%)과 실행 성공률(74%)을 달성하며 그 효과성을 입증했다.41 이는 AI와 로봇 공학의 융합이 단순한 개념을 넘어 실질적인 성과를 창출하기 시작했음을 알리는 신호탄이었다.</p>
<h3>3.3  주요 학회 및 arXiv 발표 심층 분석: 다양성과 융합</h3>
<p>2022년 8월 전후의 로봇 연구는 ’체화된 AI’라는 큰 틀 안에서 다양한 방향으로 분화하고 융합하는 모습을 보였다. 다가오는 IROS 2022 학회의 주제는 커뮤니티의 거시적 방향성을 제시했고, arXiv에 발표된 논문들은 구체적인 기술적 진보를 보여주었다.</p>
<h4>3.3.1  IROS 2022 (10월) 동향 예측</h4>
<p>2022년 10월 일본 교토에서 개최될 예정이었던 IROS 2022는 당시 로봇 연구 커뮤니티의 핵심 관심사를 명확하게 보여주었다.</p>
<ul>
<li>
<p><strong>“Embodied AI for a Symbiotic Society” 테마</strong>: 학회 전체의 주제가 ’공생 사회를 위한 체화된 AI’로 설정된 것은 매우 상징적이다. 이는 로봇 연구의 초점이 개별 로봇의 기술적 성능 향상을 넘어, 인간 및 인간 사회와의 조화로운 상호작용과 통합으로 이동하고 있음을 명백히 보여준다.3</p>
</li>
<li>
<p><strong>주요 세션 및 발표</strong>: ’인간-로봇 상호작용(HRI)의 계산적 발전(Computational Advances in Human-Robot Interaction)’과 같은 특별 세션은 로봇이 인간의 의도를 모델링하고, 신뢰를 형성하며, 사회적 신호를 이해하는 방법에 대한 깊이 있는 논의를 예고했다.42 또한, 소니(Sony)가 IROS에서 시연할 예정이었던 촉각 센서 기반 매니퓰레이터 기술은 주목할 만했다. 이 매니퓰레이터는 물체의 무게나 마찰에 대한 사전 정보 없이도 섬세한 힘 조절로 꽃이나 케이크와 같은 부서지기 쉬운 물체를 잡을 수 있는 능력을 갖추고 있었다. 이는 로봇 하드웨어, 특히 센서 기술의 발전이 정교한 상호작용을 구현하는 데 얼마나 중요한지를 보여주는 사례다.44</p>
</li>
</ul>
<h4>3.3.2  2022년 8월 주요 arXiv 논문 분석</h4>
<p>당월 arXiv에 발표된 로봇 분야 논문들은 다개체 시스템, 자율 항법, 행성 탐사 등 다양한 영역에서 구체적인 기술적 진보를 이루었음을 보여준다.45</p>
<ul>
<li>
<p><strong>다개체 시스템 및 탐사</strong>: 한 연구에서는 스웜 로봇(swarm robots)의 탐사 전략으로 기존의 랜덤 워크(random walk) 대신 레비 비행(Lévy flight)을 적용하여 넓은 환경에서 목표물 탐지 성능을 향상시킬 수 있음을 실제 로봇 실험을 통해 입증했다.46 또 다른 연구에서는 NASA의 지원을 받아 화성이나 달과 같은 극한 환경 탐사를 위한 로봇 협력 시스템을 개발했다. 이 시스템은 다리가 달린 로봇(legged robot)의 접지력과 바퀴가 달린 로버(wheeled rover)의 안정성을 결합하여, 개별 로봇이 통과하기 어려운 가파르고 무른 경사면을 함께 극복하는 하이브리드 시스템을 구현했다.47 이는 단일 로봇의 한계를 극복하기 위한 다개체 협력 시스템의 강인성과 실용성을 보여주는 사례다.</p>
</li>
<li>
<p><strong>SLAM 및 내비게이션</strong>: 로봇의 자율 이동 및 작업 효율성을 높이기 위한 핵심 기반 기술 연구도 활발히 진행되었다. 한 논문에서는 가시광 위치 확인(Visible Light Positioning, VLP) 랜드마크와 SLAM(Simultaneous Localization and Mapping) 기술을 결합하여 로봇이 생성하는 지도의 정확도를 자율적으로 보정하는 방법을 제안했다. 이를 통해 의미 정보(semantic information)를 포함하는 레이아웃 지도의 정확도를 향상시켜 내비게이션 오차를 20cm 이상 줄이는 성과를 거두었다.46 또한, 순차적 조작 작업을 위한 시간 최적화된 로봇 모션을 생성하는 제약 기반(constraint-based) 방법론도 제안되었다. 이 접근법은 전체 작업 시퀀스에 걸쳐 로봇의 움직임을 하나의 비선형 최적화 문제로 공식화하여, 작업 단계 사이의 불필요한 멈춤을 제거하고 전체 작업 시간을 단축했다.48</p>
</li>
</ul>
<h2>4. 결론 및 전망</h2>
<p>2022년 8월은 AI와 로봇 공학 분야에서 두 가지 거대한 서사가 교차하며 미래의 방향성을 제시한 결정적인 시점이었다. 한편에서는 생성형 AI와 트랜스포머 아키텍처를 필두로 한 대규모 AI 모델이 디지털 세계에서 전례 없는 창의성과 범용성을 입증하며 패러다임의 전환을 이끌었다.1 다른 한편에서는 ’SayCan’과 같은 기념비적인 연구를 통해, 이 강력하고 추상적인 지능을 물리적 세계에 ’그라운딩(grounding)’하여 로봇이 실제 환경에서 유의미한 작업을 수행하게 하려는 노력이 본격화되었다.35</p>
<p>산업계의 AI 도입은 양적으로 성숙기에 접어들었으나, 기술 역량의 급격한 발전과 안전성 및 윤리 거버넌스 구축 사이의 ’역량-안전성 격차’는 해결해야 할 시급한 과제로 부상했다.7 로봇 분야는 ’공생 사회를 위한 체화된 AI’라는 거대 담론 아래, 언어 모델과의 융합, 다개체 협력, 인간과의 정교한 상호작용 등 사회적 맥락 안에서 로봇의 새로운 역할을 모색하기 시작했다.3</p>
<p>미래를 전망할 때, ’The Alberta Plan’이 제시하는 환경과의 지속적인 상호작용을 통한 근본적인 지능 탐구 32와, ‘HuMMan’ 데이터셋이 가능하게 할 실제 세계에서의 풍부한 데이터 기반 학습 26, 그리고 인간-로봇 상호작용에서의 신뢰와 안전성 확보 43에 대한 연구가 향후 AI와 로봇 기술의 지속 가능한 발전을 좌우할 핵심적인 연구 방향이 될 것이다. 아래 표는 본 보고서에서 심층적으로 다룬 주요 논문들의 핵심 기여를 요약한 것이다.</p>
<table><thead><tr><th>논문명 (Paper Title)</th><th>핵심 기여 (Core Contribution)</th><th>주요 방법론 (Key Methodology)</th><th>분야 (Field)</th></tr></thead><tbody>
<tr><td>Learning to Solve Hard Minimal Problems</td><td>다수의 가짜 해를 가진 어려운 기하학적 최소 문제에 대한 고효율 RANSAC 솔버 개발</td><td>‘Pick &amp; Solve’: 머신러닝으로 최적의 시작점을 선택 후 호모토피 연속(HC)으로 해 탐색</td><td>Computer Vision</td></tr>
<tr><td>EPro-PnP</td><td>미분 불가능한 PnP 알고리즘을 종단간 학습 가능한 확률적 계층으로 일반화</td><td>포즈를 확률 분포로 모델링하고 KL 발산 손실을 통해 2D-3D 대응 관계 학습</td><td>Computer Vision</td></tr>
<tr><td>Do As I Can, Not As I Say</td><td>LLM의 절차적 지식을 로봇의 실제 물리적 능력(Affordances)에 그라운딩</td><td>SayCan: LLM의 유용성 확률과 로봇의 실행 가능성 확률을 결합하여 행동 결정</td><td>AI + Robotics</td></tr>
<tr><td>The Alberta Plan for AI Research</td><td>AI 연구를 위한 장기적 비전 제시, 지속적인 학습과 상호작용을 통한 지능의 근본적 이해 추구</td><td>4대 원칙: 평범한 경험, 시간적 균일성, 계산적 고려, 타 에이전트 존재</td><td>Foundational AI</td></tr>
<tr><td>XAI in Cyber Security</td><td>사이버 보안 분야에서 설명가능 AI(XAI) 연구의 필요성과 현존하는 연구 공백을 제시한 최초의 서베이</td><td>체계적 문헌 고찰(Systematic Literature Review)</td><td>AI Applications</td></tr>
<tr><td>Open-Vocabulary DETR</td><td>사전 정의된 클래스를 넘어 텍스트/이미지 입력으로 임의의 객체를 탐지하는 DETR 모델 개발</td><td>조건부 매칭(Conditional Matching) 및 이진 매칭 손실(Binary Matching Loss)</td><td>Computer Vision</td></tr>
</tbody></table>
<h2>5. 참고 자료</h2>
<ol>
<li>The Biggest Highlights for AI/ML in 2022 - RE•WORK Blog, https://blog.re-work.co/the-biggest-highlights-for-ai-ml-in-2022/</li>
<li>[2108.07258] On the Opportunities and Risks of Foundation Models - arXiv, https://arxiv.org/abs/2108.07258</li>
<li>IROS 2022 – 2022 IEEE/RSJ International Conference on Intelligent …, https://iros2022.org/</li>
<li>ACL 2022 | 60th Annual Meeting of the Association for Computational Linguistics | Ireland, https://2022.aclweb.org/</li>
<li>CVPR 2022: Home, https://cvpr2022.thecvf.com/</li>
<li>Call for Papers | ECCV2022 - European Computer Vision Association, https://eccv2022.ecva.net/submission/call-for-papers/</li>
<li>The state of AI in 2022—and a half decade in review - McKinsey, https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai-in-2022-and-a-half-decade-in-review</li>
<li>introduction to the ai index report 2022 - Stanford HAI, https://hai.stanford.edu/assets/files/2022-ai-index-report_master.pdf</li>
<li>CVPR 2022 Paper Awards, https://cvpr2022.thecvf.com/cvpr-2022-paper-awards</li>
<li>CVPR Best Paper Award - IEEE Computer Society Technical Committee on Pattern Analysis and Machine Intelligence, https://tc.computer.org/tcpami/2022/08/22/cvpr-best-paper-award/</li>
<li>Learning To Solve Hard Minimal Problems - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Hruby_Learning_To_Solve_Hard_Minimal_Problems_CVPR_2022_paper.pdf</li>
<li>(PDF) Learning to Solve Hard Minimal Problems - ResearchGate, https://www.researchgate.net/publication/356842134_Learning_to_Solve_Hard_Minimal_Problems</li>
<li>Learning To Solve Hard Minimal Problems | PDF | Mathematical Optimization - Scribd, https://www.scribd.com/document/579585374/Hruby-Learning-To-Solve-Hard-Minimal-Problems-CVPR-2022-paper</li>
<li>Learning to Solve Hard Minimal Problems Supplementary Material - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/supplemental/Hruby_Learning_To_Solve_CVPR_2022_supplemental.pdf</li>
<li>Tongji students win Best Student Paper Award at the International Conference on Computer Vision and Pattern Recognition-同济大学汽车学院英文网, https://autoen.tongji.edu.cn/info/1082/1191.htm</li>
<li>[2203.13254] EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation - arXiv, https://arxiv.org/abs/2203.13254</li>
<li>EPro-PnP: Generalized End-to-End Probabilistic … - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_EPro-PnP_Generalized_End-to-End_Probabilistic_Perspective-N-Points_for_Monocular_Object_Pose_Estimation_CVPR_2022_paper.pdf</li>
<li>EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation - ResearchGate, https://www.researchgate.net/publication/369449668_EPro-PnP_Generalized_End-to-End_Probabilistic_Perspective-n-Points_for_Monocular_Object_Pose_Estimation</li>
<li>[2303.12787] EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation - arXiv, https://arxiv.org/abs/2303.12787</li>
<li>EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation - Semantic Scholar, https://www.semanticscholar.org/paper/EPro-PnP%3A-Generalized-End-to-End-Probabilistic-for-Chen-Wang/ff5ea1c9d8baa636d946e9de101de35a7238f2da</li>
<li>ECCV 2022 Accepted Papers | MMLab@NTU, https://www.mmlab-ntu.com/conference/eccv2022/index.html</li>
<li>Open-Vocabulary DETR with Conditional Matching - Semantic Scholar, https://www.semanticscholar.org/paper/Open-Vocabulary-DETR-with-Conditional-Matching-Zang-Li/403ad5d6e78fcf29f1ac526fbc9ff6cbfea555eb</li>
<li>(PDF) Open-Vocabulary DETR with Conditional Matching - ResearchGate, https://www.researchgate.net/publication/359410997_Open-Vocabulary_DETR_with_Conditional_Matching</li>
<li>Open-Vocabulary DETR with Conditional Matching, https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690107.pdf</li>
<li>Open-Vocabulary DETR with Conditional Matching | Request PDF - ResearchGate, https://www.researchgate.net/publication/365170003_Open-Vocabulary_DETR_with_Conditional_Matching</li>
<li>HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling - arXiv, https://arxiv.org/abs/2204.13686</li>
<li>HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling – Supplementary Material –, https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670549-supp.pdf</li>
<li>HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and …, https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670549.pdf</li>
<li>HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling, https://www.semanticscholar.org/paper/HuMMan%3A-Multi-Modal-4D-Human-Dataset-for-Versatile-Cai-Ren/0c34f2072b59858cf03c4fd8edbe93916c5ffa36</li>
<li>HuMMan: Multi-modal 4D Human Dataset for Versatile Sensing and Modeling | Request PDF - ResearchGate, https://www.researchgate.net/publication/367618154_HuMMan_Multi-modal_4D_Human_Dataset_for_Versatile_Sensing_and_Modeling</li>
<li>HuMMan - Zhongang Cai, https://caizhongang.com/projects/HuMMan/</li>
<li>[2208.11173] The Alberta Plan for AI Research - arXiv, https://arxiv.org/abs/2208.11173</li>
<li>[2208.14937] Explainable Artificial Intelligence Applications in Cyber Security: State-of-the-Art in Research - arXiv, https://arxiv.org/abs/2208.14937</li>
<li>Recent Advances and Challenges in Industrial Robotics: A Systematic Review of Technological Trends and Emerging Applications - MDPI, https://www.mdpi.com/2227-9717/13/3/832</li>
<li>[2204.01691] Do As I Can, Not As I Say: Grounding Language in Robotic Affordances - arXiv, https://arxiv.org/abs/2204.01691</li>
<li>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances - ResearchGate, https://www.researchgate.net/publication/359728557_Do_As_I_Can_Not_As_I_Say_Grounding_Language_in_Robotic_Affordances</li>
<li>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances - Google Research, https://research.google/pubs/do-as-i-can-not-as-i-say-grounding-language-in-robotic-affordances/</li>
<li>SayCan: Grounding Language in Robotic Affordances, https://say-can.github.io/</li>
<li>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances - Semantic Scholar, https://www.semanticscholar.org/paper/Do-As-I-Can%2C-Not-As-I-Say%3A-Grounding-Language-in-Ahn-Brohan/cb5e3f085caefd1f3d5e08637ab55d39e61234fc</li>
<li>Do As I Can, Not As I Say: Grounding Language in Robotic …, https://proceedings.mlr.press/v205/ichter23a/ichter23a.pdf</li>
<li>[Robotics] Do As I Can, Not As I Say: Grounding Language in Robotic Affordances - Medium, https://medium.com/@amiable_cardinal_crocodile_398/robotics-do-as-i-can-not-as-i-say-grounding-language-in-robotic-affordances-e6d1b74035fd</li>
<li>IROS 2022 Special Session on Computational Advances in Human-Robot Interaction, https://ai-hri.github.io/2022-IROS-SS/</li>
<li>[2208.11090] IEEE Trust, Acceptance and Social Cues in Human-Robot Interaction – SCRITA 2022 Workshop - arXiv, https://arxiv.org/abs/2208.11090</li>
<li>Exhibition report: The 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems(IROS 2022) | News | R&amp;D Activities - Sony Group Portal, https://www.sony.com/en/SonyInfo/research/news/event/iros2022/</li>
<li>Robotics Aug 2022 - arXiv, https://arxiv.org/list/cs.RO/2022-08</li>
<li>Robotics, Volume 11, Issue 4 (August 2022) – 18 articles, https://www.mdpi.com/2218-6581/11/4</li>
<li>Helping robots work together to explore the Moon and Mars - Penn Engineering Blog, https://blog.seas.upenn.edu/helping-robots-work-together-to-explore-the-moon-and-mars/</li>
<li>arXiv:2208.09219v1 [cs.RO] 19 Aug 2022, https://arxiv.org/pdf/2208.09219</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>