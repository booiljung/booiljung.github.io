<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2022년 6월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2022년 6월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2022년 AI 및 로봇 연구 동향</a> / <span>2022년 6월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2022년 6월 AI 및 로봇 연구 동향</h1>
<h2>1. 서론: 2022년 중반, AI 및 로봇 연구의 변곡점</h2>
<p>2022년 상반기는 인공지능 기술, 특히 대규모 언어 모델(LLM)과 생성 모델 분야에서 기념비적인 발전이 이루어진 시기였다. 이 시기는 2022년 11월 OpenAI의 ChatGPT가 공개되며 시작된 생성형 AI의 폭발적인 대중화 직전, 학계와 산업계의 최첨단 연구가 집약적으로 무르익던 중요한 변곡점으로 기록된다. 특히 확산 모델(Diffusion Models)이 텍스트-이미지 생성 분야에서 기존의 GAN을 넘어서는 품질을 선보이며 주류 기술로 부상했으며, 언어 모델의 규모는 전례 없는 수준으로 확장되며 새로운 질적 능력을 드러내기 시작했다.1</p>
<p>이러한 배경 속에서 2022년 6월은 AI 및 로보틱스 분야의 연구 성과가 집중적으로 발표된 결정적인 한 달이었다. 컴퓨터 비전 분야 최고 권위의 학회인 CVPR (Conference on Computer Vision and Pattern Recognition)이 개최되어 학계의 최신 연구 동향을 조망할 수 있었고 3, 동시에 Google과 같은 선도적인 산업 연구소에서는 Imagen, BIG-Bench 등 이후 AI 연구의 방향성을 결정지은 기념비적인 결과물들을 공개했다.4 이 시기에 발표된 연구들은 단순히 기존 기술의 성능을 개선하는 것을 넘어, AI가 물리 세계의 복잡한 현상을 이해하고(Computational Sensing), 고전적인 알고리즘의 한계를 돌파하며(Learning-based Optimization), 여러 모달리티의 정보를 융합하여 인간의 창의성을 보조하는(Text-to-Image Synthesis) 새로운 패러다임의 가능성을 명확히 보여주었다. 본 보고서는 2022년 6월을 기점으로 발표된 주요 학술 및 산업 연구들을 심층적으로 분석함으로써, 생성형 AI 시대의 기술적 토대를 마련한 핵심 아이디어와 그 상호 연관성을 조망하고, 이것이 현재 AI 기술 지형에 미친 영향을 분석하고자 한다.</p>
<h2>2.  학계의 최전선 - CVPR 2022 주요 성과 분석</h2>
<h3>2.1  CVPR 2022 개요 및 주요 동향</h3>
<p>2022년 6월 19일부터 24일까지 미국 뉴올리언스에서 개최된 제35회 IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR 2022)는 해당 분야에서 세계 최고의 권위를 자랑하는 학술 행사이다.3 이 해에는 8,000편이 넘는 방대한 수의 논문이 제출되었으며, 그중 단 25%만이 채택될 정도로 높은 학술적 기준을 유지했다.6</p>
<p>CVPR 2022에서 나타난 주요 연구 동향은 크게 세 가지로 요약할 수 있다. 첫째, 3D 장면을 연속적인 함수로 표현하여 새로운 시점의 이미지를 생성하는 NeRF(Neural Radiance Fields) 기술이 지속적으로 발전하며, 특히 반사 및 광택 표현의 한계를 극복하려는 시도가 두드러졌다.7 둘째, 자연어 처리 분야에서 시작된 트랜스포머 아키텍처를 비전 문제에 적용하는 비전 트랜스포머(Vision Transformer)의 응용이 이미지 복원, 객체 탐지 등 다양한 하위 분야로 빠르게 확산되었다.8 셋째, 3D 재구성, 자세 추정 등 전통적인 컴퓨터 비전의 기하학적 문제들을 현대적인 딥러닝 프레임워크 내에서 미분 가능한 형태로 재해석하여 End-to-End 학습의 이점을 극대화하려는 연구들이 중요한 성과를 거두었다. 이는 고전적인 컴퓨터 비전 알고리즘과 딥러닝의 성공적인 융합을 보여주는 흐름이었다.</p>
<h3>2.2  CVPR 2022 주요 수상 논문 요약</h3>
<p>제1부에서 심층적으로 다룰 CVPR 2022의 핵심 수상 논문들은 당시 컴퓨터 비전 연구의 정점을 보여준다. 각 논문의 핵심 기여를 요약하면 다음과 같다. 이 표는 보고서의 후속 절에서 진행될 상세 분석의 로드맵 역할을 한다.</p>
<table><thead><tr><th>수상 부문</th><th>논문 제목</th><th>주 저자</th><th>핵심 기여</th></tr></thead><tbody>
<tr><td><strong>Best Paper Award</strong></td><td>Learning to Solve Hard Minimal Problems</td><td>Petr Hrubý, Timothy Duff, Anton Leykin, Tomas Pajdla</td><td>머신러닝을 이용해 RANSAC 프레임워크 내에서 어려운 기하학적 최적화 문제의 해를 기존 방식보다 수십 배 빠르게 찾는 ‘Pick &amp; Solve’ 접근법 제안.</td></tr>
<tr><td><strong>Best Paper Honorable Mention</strong></td><td>Dual-Shutter Optical Vibration Sensing</td><td>Mark Sheinin, Dorian Chan, Matthew O’Toole, Srinivasa Narasimhan</td><td>저속의 롤링/글로벌 셔터 카메라 2대를 활용하여 최대 63kHz의 고주파 미세 진동(소리)을 원격으로 감지 및 복원하는 혁신적인 시각적 음향 센싱 기술 개발.</td></tr>
<tr><td><strong>Best Student Paper Award</strong></td><td>EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation</td><td>Hansheng Chen, Pichao Wang, Fan Wang, Wei Tian, Lu Xiong, Hao Li</td><td>기존의 미분 불가능한 PnP 문제를 SE(3) 매니폴드 상의 확률 분포로 모델링하여 완전한 End-to-End 학습을 가능하게 하는 새로운 미분 가능 PnP 레이어 제안.</td></tr>
<tr><td><strong>Best Student Paper Honorable Mention</strong></td><td>Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields</td><td>Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan Barron, Pratul Srinivasan</td><td>NeRF의 외관 표현을 시야 방향 대신 ’반사 방향’으로 재구성하고 법선 벡터 정규화를 도입하여 광택 및 반사 표면의 렌더링 품질을 획기적으로 개선.</td></tr>
</tbody></table>
<h3>2.3  최우수 논문 심층 분석: “Learning to Solve Hard Minimal Problems”</h3>
<p>컴퓨터 비전 분야에서 3D 재구성이나 카메라 자세 추정과 같은 기하학적 문제를 해결하기 위해, ’Minimal Problem’은 오랫동안 핵심적인 도구로 사용되어 왔다. 이는 문제를 해결하는 데 필요한 최소한의 데이터 포인트(예: 5개의 점 대응 관계로부터 두 카메라의 상대 자세를 구하는 문제)로부터 다항 방정식 시스템을 구성하고 그 해를 구하는 방식이다.10 그러나 문제의 복잡도가 증가하여 다항 방정식의 해, 즉 후보 솔루션의 수가 수백 개 이상으로 많아지는 ’Hard Minimal Problem’의 경우, 기존의 Symbolic-numeric solver(예: Gröbner basis 기반 방법)는 모든 해를 계산하는 데 막대한 계산 비용이 소요되어 실제 응용에 한계를 보여왔다.10</p>
<p>이 논문은 이러한 한계를 극복하기 위해 혁신적인 패러다임을 제시한다. 기존의 ‘Solve &amp; Pick’ 방식은 먼저 모든 가능한 해(진짜 해와 수많은 가짜 해 포함)를 계산한 뒤, 가장 데이터에 잘 맞는 최적의 해를 선택하는 접근법이었다. 이와 대조적으로, 본 연구는 ’Pick &amp; Solve’라는 새로운 접근법을 제안한다.10 이 방식의 핵심은 머신러닝 모델을 사용하여, 주어진 문제에 대해 최종적으로 좋은 해로 이어질 가능성이 가장 높은 ’시작점(a starting problem-solution pair)’을 직접 ’선택(Pick)’하는 것이다. 일단 유망한 시작점이 선택되면, Homotopy Continuation(HC)이라는 수치 해석적 방법을 통해 시작점의 해로부터 목표 문제의 해까지 단일 경로를 효율적으로 추적하여 최종 해를 ’해결(Solve)’한다. 이 접근법은 수많은 가짜 해(spurious solutions)를 계산하는 불필요한 과정을 원천적으로 생략함으로써 계산 효율을 극대화한다.10</p>
<p>연구팀은 이 아이디어를 3대의 보정된 카메라의 상대 자세를 4개의 점 대응 관계를 이용해 계산하는 매우 어려운 문제에 적용하여 그 효과를 입증했다. 제안된 RANSAC 솔버는 단일 문제를 평균 70 마이크로초(μs) 미만에 해결하는 압도적인 속도를 달성했으며, 이는 기존 방식 대비 수십 배 이상 빠른 성능이다.10 이 연구의 가장 중요한 의의는 고전적인 대수 기하학(algebraic geometry) 문제 해결 과정에 최신 머신러닝 기법을 성공적으로 융합했다는 점에 있다. 이는 딥러닝을 단순히 종단 간(end-to-end) 예측을 위한 ’해석 불가능한 블랙박스’로 사용하는 것을 넘어, 전통적인 알고리즘의 명확한 병목 구간(해 탐색 공간의 비효율성)을 지능적으로 최적화하는 정밀한 ’보조 도구’로서 활용할 수 있다는 새로운 가능성을 제시했다. 즉, 모든 해를 무차별적으로 탐색하는 과정을 유망한 시작점을 ’예측’하는 과정으로 대체함으로써, 설명 가능하고 신뢰성 높은 기존 알고리즘의 장점은 유지하면서도 효율성을 획기적으로 개선한 것이다. 이는 향후 복잡한 공학 및 과학 문제 해결에 AI를 통합하는 방식에 중요한 시사점을 제공한다.</p>
<h3>2.4  혁신적 감지 기술: “Dual-Shutter Optical Vibration Sensing”</h3>
<p>인간의 눈으로는 감지할 수 없는 물체 표면의 미세한 진동에는 소리, 재료의 물리적 특성, 심장 박동 등 방대한 정보가 담겨 있다. ’Visual Vibrometry’는 레이저 빔을 물체 표면에 조사하고, 그 반사광이 만드는 미세한 스페클 패턴(speckle pattern)의 변위를 고속으로 촬영 및 분석하여 이러한 진동을 원격으로 측정하는 기술이다.15 그러나 이 기술은 수십 킬로헤르츠(</p>
<p>kHz)에 달하는 고주파 진동을 포착하기 위해 매우 비싼 고속 카메라를 필요로 한다는 실용적인 한계가 있었다.15</p>
<p>CVPR 2022에서 Honorable Mention을 수상한 이 연구는 이러한 한계를 창의적인 방식으로 극복했다. 고가의 특수 장비 대신, 거의 모든 상용 디지털카메라에 탑재된 두 가지 다른 셔터 메커니즘의 물리적 특성을 역이용하는 ’Computational Sensing’의 정수를 보여준다.15 이 시스템은 두 대의 저렴한 카메라를 동시에 사용한다. 첫 번째 카메라는 ‘롤링 셔터(Rolling Shutter)’ 방식을 사용한다. 롤링 셔터는 이미지 센서의 각 픽셀 라인을 순차적으로 노출시키기 때문에, 빠르게 움직이는 피사체를 촬영할 때 ’젤로 현상’과 같은 왜곡이 발생한다. 연구팀은 이 왜곡을 ’문제’가 아닌 ’정보’로 재해석했다. 즉, 각 라인이 미세한 시간차를 두고 촬영된다는 점을 이용하여, 시간적으로 빠르게 변화하는 고주파 진동 정보를 하나의 왜곡된 이미지 프레임 안에 효과적으로 ’인코딩’했다.15 두 번째 카메라는 ‘글로벌 셔터(Global Shutter)’ 방식을 사용한다. 글로벌 셔터는 센서의 모든 픽셀을 동시에 노출시켜 왜곡 없는 이미지를 포착한다. 이 왜곡 없는 이미지는 롤링 셔터 이미지에 담긴 시간적 정보를 ’디코딩’하기 위한 깨끗한 기준 프레임(reference frame) 역할을 한다.15</p>
<p>이 두 카메라로부터 얻은 정보를 알고리즘적으로 융합함으로써, 연구팀은 130Hz 수준의 저속 카메라만으로 최대 63kHz에 달하는 고주파 진동을 성공적으로 감지하고 복원해냈다.16 실험을 통해 밴드에서 연주되는 여러 기타 소리를 각각 분리하여 녹음하거나, 심지어 스피커 옆에 놓인 과자 봉지의 미세한 떨림만을 측정하여 스피커에서 나오는 음악을 복원하는 등 놀라운 성능을 시연했다.15 이 연구는 하드웨어의 물리적 원리(셔터 메커니즘)에 대한 깊은 이해를 바탕으로, 정교한 계산 모델을 통해 하드웨어의 한계를 뛰어넘을 수 있음을 보여준 대표적인 사례이다. 이는 AI 및 컴퓨터 비전 연구가 단순히 데이터와 모델 아키텍처에만 의존하는 것이 아니라, 센서 자체의 물리적 특성과 상호작용할 때 더욱 혁신적인 결과를 창출할 수 있음을 시사한다. 이 기술은 산업 현장의 비파괴 검사, 원격 감시, 음향 공학 등 다양한 분야에 저비용 고효율 솔루션을 제공할 막대한 잠재력을 지닌다.</p>
<h3>2.5  End-to-End 6DoF 자세 추정의 새로운 지평: “EPro-PnP”</h3>
<p>단일 RGB 이미지로부터 3D 객체의 정확한 6자유도(6DoF) 자세(3D 위치와 3D 회전)를 추정하는 것은 자율주행, 로보틱스, 증강현실의 핵심 기술이다. 이를 위한 고전적이고 강력한 방법론이 바로 Perspective-n-Point(PnP)이다. PnP는 미리 알고 있는 3D 객체 모델의 점들과 이미지에 투영된 2D 점들 간의 대응 관계를 이용하여 카메라에 대한 객체의 자세를 계산하는 알고리즘이다.20 그러나 PnP 알고리즘은 최적의 해를 찾는 과정에서 최소화 연산(</p>
<p><code>argmin</code>)을 포함하기 때문에, 그 결과는 입력 대응점에 대해 미분이 불가능하다. 이 ’미분 불가능성’은 PnP를 딥러닝 모델의 일부로 통합하여 전체 시스템을 종단 간(End-to-End)으로 학습시키는 데 있어 근본적인 걸림돌로 작용해왔다.21</p>
<p>Best Student Paper Award를 수상한 “EPro-PnP“는 이 오랜 난제를 해결하기 위해 결정론적 최적화 문제를 확률적 추론 문제로 전환하는 우아한 해법을 제시한다. 기존 방식이 단 하나의 최적 포즈를 결정론적으로 출력하는 것과 달리, EPro-PnP는 미분 가능한 ’확률적 PnP 레이어’를 제안한다. 이 레이어는 가능한 모든 해의 공간, 즉 SE(3) 매니폴드 상에서 ’포즈의 확률 분포(a distribution of pose)’를 출력한다.20 이는 범주형 분류 문제에서 각 클래스에 대한 확률을 출력하기 위해 널리 사용되는 Softmax 함수를, 6DoF 포즈라는 연속적인 공간으로 일반화한 것으로 비유할 수 있다.23</p>
<p>모델 학습은 예측된 포즈 분포와 실제 정답 포즈(매우 좁은 분포로 표현됨) 간의 통계적 거리인 KL Divergence(쿨백-라이블러 발산)를 최소화하는 방식으로 이루어진다. KL Divergence 손실 함수는 미분 가능하므로, 손실로부터 계산된 그래디언트가 PnP 레이어를 통과하여 역전파(backpropagation)될 수 있다. 이를 통해 2D-3D 대응 관계를 예측하는 신경망의 앞단까지 전체 모델을 통합적으로, 종단 간 방식으로 학습시키는 것이 가능해진다.20 EPro-PnP는 LineMOD, nuScenes와 같은 주요 6DoF 자세 추정 및 3D 객체 탐지 벤치마크에서 기존 PnP 기반 방법들의 성능을 크게 상회하며 당시 최고 수준의 정확도를 달성했다.20 이 연구의 파급력은 PnP 문제 해결을 넘어선다. 이는 내부에</p>
<p><code>argmin</code>과 같은 미분 불가능한 최적화나 이산적 선택 과정이 포함된 수많은 다른 고전 알고리즘들을 딥러닝 프레임워크와 통합할 수 있는 일반적인 방법론을 제시했다는 점에서 의의가 크다. ’결정론적 최적화’를 ’확률 분포 모델링’으로 전환하고 ’KL Divergence’를 통해 학습하는 이 파이프라인은 향후 AI 연구에서 복잡한 알고리즘과 딥러닝을 결합하는 강력하고 일반적인 설계 패턴이 될 잠재력을 보여주었다.</p>
<h3>2.6  사실적 렌더링의 진화: “Ref-NeRF”</h3>
<p>Neural Radiance Fields(NeRF)는 3D 장면을 위치(x,y,z)와 시야 방향(θ,ϕ)을 입력으로 받아 색상과 밀도를 출력하는 연속적인 5차원 신경망 함수로 표현함으로써, 고품질의 새로운 시점 이미지를 생성하는 기술의 혁신을 가져왔다.7 그러나 기존 NeRF 기반 기술들은 확산(diffuse) 표면은 잘 표현하는 반면, 금속이나 유리처럼 광택이 있거나(glossy) 거울 같은(specular) 표면을 렌더링할 때 한계를 보였다. 특히, 시점을 조금만 바꿔도 하이라이트가 물리적으로 타당하게 움직이지 않고 부자연스럽게 나타나거나 사라지는 시각적 결함(artifact)이 자주 발생했다.7</p>
<p>Best Student Paper Honorable Mention을 수상한 “Ref-NeRF“는 이러한 문제를 해결하기 위해 NeRF라는 순수한 데이터 기반(data-driven) 표현 방식에 컴퓨터 그래픽스의 물리 기반(physics-based) 원리를 정교하게 통합했다. 연구의 핵심 아이디어는 두 가지다. 첫째, ’반사 방향 파라미터화(Reflection Direction Parameterization)’이다. 기존 NeRF가 5D 함수의 입력으로 ‘시야 방향’ 벡터를 직접 사용했던 것과 달리, Ref-NeRF는 이를 ‘표면의 법선 벡터(normal vector)에 대한 시야 방향의 반사 방향(reflection direction)’ 벡터로 대체한다.7 이는 물리 기반 렌더링(PBR)에서 반사광을 계산하는 원리와 일치한다. 이 재구성(reparameterization)을 통해 신경망이 학습해야 할 시점에 따른 외관 변화 함수가 훨씬 더 단순하고 부드러운 형태로 표현되어, 보간(interpolation) 성능이 크게 향상된다.</p>
<p>둘째, 정확한 반사 방향을 계산하기 위해서는 신뢰할 수 있는 표면 법선 벡터가 필수적이다. 이를 위해 Ref-NeRF는 ’법선 벡터 정규화(Normal Vector Regularization)’라는 새로운 손실 함수를 도입했다. 이 정규화 항은 NeRF의 볼륨 밀도(volume density) 필드의 그래디언트로부터 계산된 법선 벡터가 일관성을 갖도록 유도하고, 밀도가 물리적으로 타당한 표면 주위에 집중되도록 장려한다.7 이 두 가지 핵심적인 개선을 통해 Ref-NeRF는 반사 및 광택 표현의 현실감과 정확성을 획기적으로 향상시켰다. 더 나아가, 모델이 내부적으로 장면의 법선, 재질, 확산 색상 등을 분리된 형태로 학습하기 때문에, 학습이 완료된 후에도 사용자가 재질의 거칠기를 바꾸거나 색상을 변경하는 등 직관적인 장면 편집(scene editing)이 가능해졌다.7 Ref-NeRF의 성공은 ‘블랙박스’ 신경망 모델에 물리적 제약(physical constraint)이나 구조적 사전 지식(structural prior)을 부여함으로써, 더 현실적이고, 해석 가능하며, 제어 가능한 결과를 얻을 수 있음을 명확히 보여주었다. 이는 향후 뉴럴 렌더링 연구가 더욱 물리적으로 타당한(physically plausible) 방향으로 발전하는 중요한 계기가 되었다.</p>
<h2>3.  산업계 연구 동향과 대규모 모델의 영향력</h2>
<h3>3.1  Google Imagen: 언어 모델과 확산 모델의 시너지</h3>
<p>2022년 5월 Google Research 팀이 발표한 Imagen은 텍스트-이미지 생성 분야의 기술적 수준을 한 단계 끌어올린 기념비적인 모델이다.5 Imagen의 아키텍처는 두 가지 핵심 요소의 강력한 시너지에 기반한다: 거대 언어 모델의 깊은 텍스트 이해 능력과 확산 모델의 고품질 이미지 생성 능력이다.</p>
<p>Imagen의 가장 중요한 발견은 텍스트 인코더의 선택에 있다. 이전의 SOTA 모델인 DALL-E 2가 이미지-텍스트 쌍 데이터로 처음부터 학습시킨 CLIP 인코더를 사용한 것과 달리, Imagen은 순수 텍스트 데이터로만 사전 학습된 거대 언어 모델인 T5-XXL을 ‘고정된(frozen)’ 텍스트 인코더로 사용했다. 연구팀은 이미지 생성 모델의 크기를 키우는 것보다, 텍스트를 이해하는 언어 모델의 크기를 키우는 것이 최종 결과물의 품질(충실도)과 텍스트-이미지 정렬(alignment)을 향상시키는 데 훨씬 더 효과적이라는 사실을 실험적으로 입증했다.5 이는 텍스트를 깊이 있게 이해하는 능력이 고품질 이미지 생성의 핵심 전제 조건임을 시사한다.</p>
<p>Imagen의 이미지 생성 파이프라인은 연속적인 확산 모델(Cascaded Diffusion Models) 구조를 따른다. 먼저 T5-XXL 인코더가 생성한 텍스트 임베딩을 조건으로 하여 64x64 픽셀의 저해상도 이미지를 생성한다. 그 후, 두 단계의 텍스트 조건부 초해상도(super-resolution) 확산 모델을 순차적으로 거쳐 이미지를 256x256, 최종적으로 1024x1024의 고해상도로 점진적으로 향상시킨다.5 이 과정에서 두 가지 핵심 기술이 중요한 역할을 한다. 첫째, ’Classifier-Free Guidance’를 사용하여 텍스트 프롬프트의 의미가 생성된 이미지에 더욱 강력하게 반영되도록 유도한다. 그러나 가이던스 강도를 높이면 이미지가 부자연스럽게 포화되는 문제가 발생하는데, Imagen은 이를 ’Dynamic Thresholding’이라는 새로운 샘플링 기법을 도입하여 해결했다.29 둘째, 확산 모델의 기반이 되는 U-Net 아키텍처를 개선한 ’Efficient U-Net’을 제안하여 학습 속도와 메모리 효율성을 높였다.5</p>
<p>이러한 혁신을 통해 Imagen은 널리 사용되는 MS-COCO 벤치마크에서 해당 데이터셋으로 전혀 훈련하지 않았음에도 불구하고 당시 최고 수준의 FID 점수(7.27)를 기록했다. 또한, 미묘한 표현을 평가하기 위해 자체적으로 구축한 DrawBench 벤치마크에서의 인간 평가 결과, DALL-E 2를 포함한 경쟁 모델들을 압도하는 선호도를 보였다.5 Imagen의 성공은 AI 분야에서 ’전이 학습(Transfer Learning)’의 패러다임을 한 차원 높은 수준으로 끌어올렸다. 이는 특정 도메인(자연어)에서 극단적으로 스케일업된 모델이 가진 방대한 ’지식’이, 전혀 다른 도메인(비전)의 복잡한 생성 작업에 매우 강력한 사전 지식(prior)으로 작용할 수 있음을 명확히 보여준 사례이다. 기존에 텍스트와 이미지를 함께 학습시켜야만 연결고리를 찾을 수 있다는 통념을 깨고, 이미 강력한 언어 모델의 지식을 ’활용’하는 것이 더 효율적임을 증명한 것이다. 이는 모달리티의 경계를 허무는 거대 파운데이션 모델 시대의 본격적인 서막을 알리는 중요한 신호탄이었다.</p>
<h3>3.2  BIG-Bench: 거대 언어 모델 능력의 다각적 측정</h3>
<p>2020년대 초반, 대규모 언어 모델(LLM)의 발전 속도는 기존의 자연어 처리(NLP) 벤치마크의 평가 능력을 빠르게 넘어서고 있었다. 많은 벤치마크에서 모델들이 인간의 수준에 근접하거나 뛰어넘는 점수를 기록하면서, 이러한 평가가 모델의 진정한 이해 능력과 추론 능력을 측정하는 것인지, 아니면 단순히 훈련 데이터에 포함된 패턴을 암기한 결과인지에 대한 회의론이 커졌다.2 이러한 배경 속에서 2022년 6월, Google을 중심으로 한 대규모 연구 협력체는 LLM의 현재 역량을 넘어서는 미래의 능력을 탐색하고 측정하기 위한 새로운 벤치마크, BIG-Bench(Beyond the Imitation Game benchmark)를 공개했다.</p>
<p>BIG-Bench는 그 규모와 설계 철학 면에서 기존 벤치마크와 차별화된다. 전 세계 132개 기관, 444명의 저자가 참여하여 공동으로 구축한 이 벤치마크는, 현재(2022년) 모델들이 풀기 어려울 것으로 예상되는 204개의 매우 다양하고 도전적인 태스크로 구성되어 있다.4 태스크의 주제는 단순한 언어 이해를 넘어 언어학, 상식 추론, 수학, 물리학, 소프트웨어 개발, 그리고 사회적 편향성에 이르기까지 인간 지능의 광범위한 영역을 포괄한다.31</p>
<p>BIG-Bench를 사용하여 OpenAI의 GPT 모델과 Google의 내부 모델들을 평가한 결과, 몇 가지 중요한 발견이 있었다. 첫째, 대부분의 태스크에서 모델의 파라미터 규모가 커질수록 성능이 꾸준히 향상되는 ’Scaling Law’가 명확하게 관찰되었다.4 둘째, 일부 복잡한 추론을 요구하는 태스크에서는 모델의 크기가 특정 임계점을 넘어서야 비로소 무작위 추측 수준을 넘어 성능이 급격히 향상되는, 이른바 ‘돌파(Breakthrough)’ 현상이 나타났다. 이는 모델의 규모가 단순히 양적인 성능 향상뿐만 아니라, 이전에 없던 새로운 질적 능력의 ’창발(emergence)’로 이어질 수 있음을 시사하는 중요한 증거였다.32 셋째, 모델의 규모가 커짐에 따라 예측의 신뢰도(calibration)는 향상되었지만, 여전히 인간에 비해서는 부족했으며, 훈련 데이터에 내재된 사회적, 문화적 편견 역시 규모가 커져도 여전히 존재하거나 증폭되는 경향을 보였다.4</p>
<p>BIG-Bench의 등장은 LLM 평가의 패러다임을 ’단일 점수 경쟁’에서 ’다차원적 능력 분석’으로 전환시키는 계기가 되었다. 특히 ‘돌파’ 현상의 발견은 AI 연구 커뮤니티에 ’규모의 확대가 예측하지 못했던 새로운 능력을 낳는다’는 강력한 가설을 제시했다. 기존 벤치마크로는 측정할 수 없었던 LLM의 잠재력을 드러냄으로써, 이는 이후 GPT-4와 같은 초거대 모델 개발 경쟁을 이론적으로 뒷받침하고 가속화하는 중요한 동력이 되었다. BIG-Bench는 LLM의 능력을 더 깊이 이해하고, 그 한계를 명확히 인식하며, 미래의 발전을 예측하기 위한 필수적인 도구로 자리 잡았다.</p>
<h3>3.3  DeepMind의 기초 연구: 트랜스포머의 수학적 이해</h3>
<p>2022년 중반은 트랜스포머(Transformer) 아키텍처가 AI 연구의 거의 모든 분야를 지배하던 시기였다. 2017년 “Attention Is All You Need” 논문에서 처음 제안된 이후, 수많은 변형 모델이 등장하며 분야는 폭발적으로 성장했지만, 동시에 아키텍처의 핵심 구성 요소에 대한 설명이 파편화되고 구현상의 미묘한 차이로 인한 혼란이 존재했다. 이러한 상황에서 2022년 7월 DeepMind가 발표한 “Formal Algorithms for Transformers” 논문은 분야의 이론적 기틀을 다지는 중요한 역할을 했다.34</p>
<p>이 논문의 목적은 경험적 성공을 거둔 트랜스포머 아키텍처에 엄밀하고 통일된 수학적 정의를 부여하는 것이었다. 연구팀은 트랜스포머를 구성하는 모든 핵심 요소를 정밀하게 기술하고, 이를 구현하기 위한 15개의 핵심 알고리즘에 대한 의사코드(pseudocode)를 제공했다. 논문에서 다루는 내용은 다음과 같다:</p>
<ul>
<li>
<p><strong>핵심 구성 요소:</strong> 토큰 및 위치 임베딩(token and positional embedding), 기본 단일 쿼리 어텐션(basic single-query attention, 양방향 및 단방향), 멀티-헤드 어텐션(multi-head attention), 레이어 정규화(layer normalization), 그리고 최종 출력층인 언임베딩(unembedding)까지, 모델의 모든 단계를 수학적 표기법으로 명확하게 정의했다.34</p>
</li>
<li>
<p><strong>주요 아키텍처:</strong> 인코더-디코더(Encoder-Decoder) 구조, BERT, GPT와 같은 대표적인 트랜스포머 변형 모델들의 작동 방식을 이 공식적인 알고리즘들을 바탕으로 설명했다.34</p>
</li>
</ul>
<p>이 연구는 새로운 기술적 돌파구를 제시한 것은 아니지만, 그 가치는 매우 크다. 폭발적으로 성장하고 복잡해지는 연구 분야일수록, 그 근간을 이루는 핵심 기술에 대한 명확하고 통일된 ’공식적 정의’는 필수적이다. 이 논문은 마치 물리학에서 뉴턴이 운동 법칙을 수학적으로 정립하여 후대 연구의 기틀을 마련한 것처럼, 트랜스포머라는 강력한 도구에 대한 ’표준 교과서’를 제공했다. 의사코드를 통해 구현의 모호함을 없애고, 통일된 수학적 표기법을 제시함으로써 연구의 재현성을 높이고, 새로운 연구자들이 분야에 더 쉽게 진입할 수 있는 발판을 마련했다. 이는 해당 분야가 성숙 단계로 접어들고 있음을 보여주는 지표이며, 향후 더 복잡하고 정교한 아키텍처를 개발하기 위한 견고한 이론적 토대를 제공했다는 점에서 중요한 기여로 평가된다.</p>
<h2>4.  로보틱스 및 자동화 분야의 주요 연구</h2>
<h3>4.1  ICAPS 2022: 강화학습과 인간 중심 애플리케이션</h3>
<p>2022년 6월 13일부터 24일까지 개최된 제32회 자동화 계획 및 스케줄링 국제 컨퍼런스(ICAPS 2022)는 인공지능의 핵심 분야 중 하나인 ’계획(Planning)’과 ’스케줄링(Scheduling)’에 중점을 둔 최고 수준의 학회이다.35 이 학회는 로봇이 목표를 달성하기 위해 행동 순서를 결정하거나, 복잡한 시스템의 자원을 효율적으로 할당하는 문제에 대한 이론 및 응용 연구를 다룬다.</p>
<p>ICAPS 2022에서 특히 주목할 만한 세션은 스탠퍼드 대학교 엠마 브런스킬(Emma Brunskill) 교수의 기조연설이었다. “인간 중심 애플리케이션을 위한 강화학습(Reinforcement Learning for Human-Focused Applications)“이라는 주제로 진행된 이 연설은 당시 AI 연구의 주류 패러다임에 중요한 질문을 던졌다.35 당시 많은 강화학습 연구는 시뮬레이션 환경이나 게임과 같이 데이터를 거의 무한정 저렴하게 얻을 수 있는 환경에서의 성공에 집중하고 있었다. 그러나 브런스킬 교수는 교육, 헬스케어와 같이 실제 인간과 상호작용하는 현실 세계의 문제들은 근본적으로 다른 특성을 가진다고 지적했다.</p>
<p>이러한 ‘인간 중심’ 환경에서는 다음과 같은 도전 과제가 존재한다:</p>
<ol>
<li>
<p><strong>비싼 샘플(Expensive Samples):</strong> 실제 환자나 학생과의 상호작용을 통해 데이터를 수집하는 것은 시간과 비용이 많이 들고 윤리적 제약이 따른다. 따라서 데이터 효율성, 즉 ’샘플 효율성(sample efficiency)’이 매우 중요하다.</p>
</li>
<li>
<p><strong>안전성 및 신뢰성:</strong> 잘못된 행동이 인간에게 직접적인 해를 끼칠 수 있으므로, 탐험(exploration) 과정에서의 안전성이 보장되어야 한다.</p>
</li>
<li>
<p><strong>다양한 모델링 선택:</strong> 현실 세계의 복잡한 문제를 모델링하는 데에는 여러 가지 방법이 존재하며, 어떤 모델을 선택하느냐에 따라 결과가 크게 달라질 수 있다.</p>
</li>
</ol>
<p>브런스킬 교수는 이러한 문제들을 해결하기 위해, 순수한 온라인 강화학습과 오프라인 강화학습의 중간 지점에 있는 설정들을 탐구하고, 다양한 모델 클래스 중에서 자동으로 최적의 모델을 선택하는 시스템을 만드는 연구들을 소개했다. 이 기조연설은 당시 AI 연구가 대규모 데이터와 컴퓨팅 파워에 의존하는 경향에서 벗어나, 데이터가 희소하고 안전성이 중요한 현실 세계의 문제들을 해결하기 위해 어떤 방향으로 나아가야 하는지에 대한 깊은 통찰을 제공했다. 이는 로보틱스 분야가 직면한 과제와도 직접적으로 연결되며, 신뢰할 수 있고 실용적인 AI 시스템을 구축하기 위한 연구의 중요성을 강조했다.</p>
<h3>4.2  arXiv 주요 로보틱스 논문 동향</h3>
<p>2022년 6월 arXiv에 공개된 로보틱스 분야의 프리프린트 논문들은 학계의 최신 연구 흐름을 엿볼 수 있는 중요한 창구였다. 특히 이 시기에는 대규모 언어 모델(LLM)의 능력을 물리적 세계의 로봇 제어 문제와 결합하려는 ‘Embodied AI’ 연구가 중요한 흐름으로 부상하고 있었다.</p>
<p>가장 대표적인 연구는 프린스턴 대학교 연구팀이 발표한 “Leveraging Language for Accelerated Learning of Tool Manipulation”(arXiv:2206.13074)이다.36 이 연구는 로봇이 망치나 집게와 같은 새로운 도구를 사용할 때, 해당 도구에 대한 자연어 설명(예: “망치는 무거운 머리와 긴 손잡이를 가지고 있으며, 못을 박는 데 사용된다”)을 활용하여 학습 속도를 획기적으로 가속하는 방법을 제안했다. 연구의 핵심 아이디어는 다음과 같다. 먼저, GPT-3와 같은 LLM을 사용하여 다양한 도구의 모양, 용도, 기하학적 특징에 대한 풍부한 텍스트 설명을 생성한다. 그 후, BERT와 같은 사전 학습된 언어 모델을 이용해 이 텍스트를 의미론적 정보가 담긴 벡터(임베딩)로 변환한다. 마지막으로, 이 언어 임베딩을 로봇의 시각 정보와 함께 제어 정책(policy)의 입력으로 제공하는 메타러닝(meta-learning) 프레임워크를 통해, 로봇이 새로운 도구를 접했을 때 해당 도구의 언어 설명을 바탕으로 기존 경험을 빠르게 적용하여 조작법을 학습하도록 한다.37 이 연구는 제2부에서 논의된 대규모 언어 모델의 추상적인 ’지식’을 로봇의 물리적 ’행동’과 직접적으로 연결한 초기 성공 사례 중 하나로, LLM이 단순히 챗봇을 넘어 물리적 에이전트의 ‘뇌’ 역할을 수행할 수 있는 가능성을 명확히 보여주었다.</p>
<p>이 외에도 2022년 6월 <code>cs.RO</code> 카테고리에는 다양한 주제의 연구들이 발표되며 로보틱스 분야의 다채로운 연구 동향을 드러냈다. 아래 표는 당시 발표된 주요 논문들의 일부를 요약한 것이다.</p>
<h4>4.2.1 년 6월 주요 arXiv 로보틱스 논문 목록 (<code>cs.RO</code>)</h4>
<table><thead><tr><th>arXiv ID</th><th>논문 제목</th><th>핵심 주제</th></tr></thead><tbody>
<tr><td><code>2206.12655</code></td><td>BRL/Pisa/IIT SoftHand: A Low-cost, 3D-Printed,… Hand</td><td>소프트 로보틱스, 저가형 로봇 손 설계</td></tr>
<tr><td><code>2206.12728</code></td><td>Learning Preconditions of Hybrid Force-Velocity Controllers…</td><td>접촉이 많은 조작(Contact-Rich Manipulation)</td></tr>
<tr><td><code>2206.13074</code></td><td>Leveraging Language for Accelerated Learning of Tool Manipulation</td><td>언어 기반 로봇 학습, 메타러닝</td></tr>
<tr><td><code>2206.13657</code></td><td>DigiTac: A DIGIT-TacTip Hybrid Tactile Sensor…</td><td>촉각 센싱, 센서 융합</td></tr>
<tr><td><code>2206.13704</code></td><td>Involuntary Stabilization in Discrete-Event Physical HRI</td><td>물리적 인간-로봇 상호작용(pHRI)</td></tr>
<tr><td><code>2206.14155</code></td><td>Position-Agnostic Autonomous Navigation in Vineyards with DRL</td><td>딥 강화학습(DRL), 자율 항법, 농업 로봇</td></tr>
<tr><td><code>2206.14289</code></td><td>Stronger Together: Air-Ground Robotic Collaboration Using Semantics</td><td>다중 로봇 시스템, 공중-지상 로봇 협업</td></tr>
</tbody></table>
<p>이 논문들은 소프트 로보틱스, 촉각 센싱, 인간-로봇 상호작용(HRI), 자율 항법, 다중 로봇 협업 등 로보틱스 분야의 핵심적인 연구 주제들이 활발히 탐구되고 있음을 보여준다. 특히, 강화학습, 시맨틱 이해, 센서 융합과 같은 AI 기술이 로봇의 지능과 자율성을 높이는 데 핵심적인 역할을 하고 있음을 확인할 수 있다.</p>
<h2>5. 결론: 2022년 6월 연구 성과가 AI의 미래에 미친 영향</h2>
<p>2022년 6월은 인공지능 연구 역사에서 중요한 분기점으로, 이후 세상을 뒤흔든 생성형 AI 붐의 기술적, 사상적 토대가 공고히 다져진 시기였다. 이 시기에 발표된 학계와 산업계의 주요 연구 성과들을 종합적으로 분석하면, 서로 다른 방향으로 진행되는 것처럼 보였던 연구들이 실제로는 하나의 거대한 패러다임 전환을 향해 수렴하고 있었음을 알 수 있다.</p>
<p>첫째, 학계의 연구, 특히 CVPR 2022의 성과들은 AI 모델의 <strong>신뢰성과 현실성</strong>을 높이는 데 집중했다. “Learning to Solve Hard Minimal Problems“는 딥러닝을 이용해 고전 알고리즘의 효율을 극대화함으로써 신뢰성을 확보했고, “Ref-NeRF“는 물리 기반 렌더링 원리를 도입하여 뉴럴 렌더링의 현실감을 높였다. “Dual-Shutter Optical Vibration Sensing“은 센서의 물리적 특성을 활용해 불가능해 보였던 감지 능력을 구현했다. 이는 AI가 단순히 데이터 패턴을 모방하는 것을 넘어, 세상의 물리적, 수학적 원리와 정교하게 결합될 때 더욱 강력한 성능을 발휘할 수 있음을 보여주었다.</p>
<p>둘째, 산업계의 연구는 <strong>’규모(Scale)’가 곧 새로운 능력의 ’창발(Emergence)’로 이어진다</strong>는 패러다임을 확립했다. Google의 Imagen은 거대 언어 모델의 방대한 지식이 고품질 이미지 생성의 핵심임을 증명하며 모달리티의 경계를 허물었고, BIG-Bench는 모델의 규모가 커짐에 따라 예측 불가능했던 새로운 능력이 ’돌파’적으로 나타나는 현상을 정량적으로 입증했다. 이는 더 큰 모델을 만드는 것이 단순히 성능을 점진적으로 개선하는 것이 아니라, 질적으로 다른 차원의 AI를 탄생시킬 수 있다는 믿음을 AI 커뮤니티에 심어주었다.</p>
<p>셋째, 로보틱스 분야의 연구는 이 두 흐름을 연결하는 중요한 <strong>가교 역할</strong>을 했다. “Leveraging Language for Accelerated Learning of Tool Manipulation“과 같은 연구는 산업계에서 개발된 거대 언어 모델의 추상적 지식을, 학계에서 중요하게 다루는 물리적 에이전트의 실제 문제 해결에 접목하는 구체적인 방법론을 제시했다. 이는 AI가 디지털 세계를 넘어 물리 세계와 상호작용하는 ’Embodied AI’로 나아가는 데 있어 언어 모델이 핵심적인 역할을 할 것임을 예고했다.</p>
<p>결론적으로, 2022년 6월은 생성형 AI라는 거대한 폭풍이 몰아치기 직전, 기술적 에너지가 최고조로 응축되던 ’폭풍전야’의 시기였다.1 Imagen과 같은 고품질 생성 모델의 등장은 대중과 산업계에 생성형 AI의 엄청난 잠재력을 각인시켰고, BIG-Bench를 통해 드러난 LLM의 창발적 능력은 몇 달 후 등장할 ChatGPT와 같은 대화형 AI의 성공에 대한 당위성을 뒷받침했다. 이 시점의 연구들은 AI가 더 이상 특정 작업을 위한 도구를 넘어, (1) 물리 세계와 상호작용하고(Embodied AI), (2) 인간의 창의성을 증폭시키며(Generative AI), (3) 그 능력과 한계를 엄밀하게 측정하고 책임져야 하는(Responsible AI) 대상으로 진화하고 있음을 명확히 보여주었다. 이는 2022년 6월에 제시된 연구 방향성이 현재까지도 AI 기술 발전의 핵심적인 축을 이루고 있음을 의미한다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>AI boom - Wikipedia, https://en.wikipedia.org/wiki/AI_boom</li>
<li>Biggest AI Headlines in 2022 – At Stanford and Beyond, https://hai.stanford.edu/news/biggest-ai-headlines-2022-stanford-and-beyond</li>
<li>The most important AI conferences in 2022 - AMAI GmbH, https://www.am.ai/en/insights/ai-conferences-2022</li>
<li>Beyond the Imitation Game Benchmark - Andrea Santilli, https://www.santilli.xyz/publication/beyond-the-imitation-game-quantifying-and-extrapolating-the-capabilities-of-language-models/beyond-the-imitation-game-quantifying-and-extrapolating-the-capabilities-of-language-models.pdf</li>
<li>Imagen: Text-to-Image Diffusion Models - Google Research, https://imagen.research.google/</li>
<li>Best Paper Award at the IEEE/CVF CVPR 2022 for a paper co …, https://ellis.ciirc.cvut.cz/best-paper-award-at-the-ieee-cvf-cvpr-2022-for-a-paper-co-authored-by-tomas-pajdla/</li>
<li>Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Verbin_Ref-NeRF_Structured_View-Dependent_Appearance_for_Neural_Radiance_Fields_CVPR_2022_paper.pdf</li>
<li>Most Influential CVPR Papers (2022-02) - Paper Digest, https://www.paperdigest.org/2022/02/most-influential-cvpr-papers-2022-02/</li>
<li>What you need to know about our top 5 research papers at CVPR 2022 - RBC Borealis, https://rbcborealis.com/research-blogs/what-you-need-to-know-about-our-top-5-research-papers-at-cvpr-2022/</li>
<li>Learning To Solve Hard Minimal Problems - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Hruby_Learning_To_Solve_Hard_Minimal_Problems_CVPR_2022_paper.pdf</li>
<li>Numerically Computing Galois Groups of Minimal Problems - arXiv, <a href="https://arxiv.org/pdf/2507.10407">https://arxiv.org/pdf/2507.10407?</a></li>
<li>Learning To Solve Hard Minimal Problems | PDF | Mathematical Optimization - Scribd, https://www.scribd.com/document/579585374/Hruby-Learning-To-Solve-Hard-Minimal-Problems-CVPR-2022-paper</li>
<li>(PDF) Learning to Solve Hard Minimal Problems - ResearchGate, https://www.researchgate.net/publication/356842134_Learning_to_Solve_Hard_Minimal_Problems</li>
<li>Learning to Solve Hard Minimal Problems | Request PDF - ResearchGate, https://www.researchgate.net/publication/373334265_Learning_to_Solve_Hard_Minimal_Problems</li>
<li>CVPR 2022 Best Paper Honorable Mention: Dual-Shutter Optical …, https://www.louisbouchard.ai/cvpr-2022-best-paper/</li>
<li>Dual-Shutter Optical Vibration Sensing - PubMed, https://pubmed.ncbi.nlm.nih.gov/38117625/</li>
<li>Dual-Shutter Optical Vibration Sensing - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Sheinin_Dual-Shutter_Optical_Vibration_Sensing_CVPR_2022_paper.pdf</li>
<li>Dual-Shutter Optical Vibration Sensing - Mark Sheinin, https://www.marksheinin.com/vibration</li>
<li>Optical Microphone Sees Sound Like Never Before - News - Carnegie Mellon University, https://www.cmu.edu/news/stories/archives/2022/july/optical-microphone-sees-sound-like-never-before</li>
<li>EPro-PnP: Generalized End-to-End Probabilistic Perspective-N-Points for Monocular Object Pose Estimation - PubMed, https://pubmed.ncbi.nlm.nih.gov/38227417/</li>
<li>EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation - ResearchGate, https://www.researchgate.net/publication/369449668_EPro-PnP_Generalized_End-to-End_Probabilistic_Perspective-n-Points_for_Monocular_Object_Pose_Estimation</li>
<li>[CVPR 2022 Oral, Best Student Paper] EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation - GitHub, https://github.com/tjiiv-cprg/EPro-PnP</li>
<li>[2203.13254] EPro-PnP: Generalized End-to-End Probabilistic …, https://ar5iv.labs.arxiv.org/html/2203.13254</li>
<li>EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation - Semantic Scholar, https://www.semanticscholar.org/paper/EPro-PnP%3A-Generalized-End-to-End-Probabilistic-for-Chen-Wang/ff5ea1c9d8baa636d946e9de101de35a7238f2da</li>
<li>tjiiv-cprg/EPro-PnP-v2: [TPAMI 2024] EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation - GitHub, https://github.com/tjiiv-cprg/EPro-PnP-v2</li>
<li>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis - arXiv, https://arxiv.org/abs/2003.08934</li>
<li>[2112.03907] Ref-NeRF: Structured View-Dependent Appearance …, https://ar5iv.labs.arxiv.org/html/2112.03907</li>
<li>Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields - Dor Verbin, https://dorverbin.github.io/refnerf/</li>
<li>Photorealistic Text-to-Image Diffusion Models with Deep … - Imagen, https://imagen.research.google/paper.pdf</li>
<li>Kaggle Game Arena evaluates AI models through games - Google Blog, https://blog.google/technology/ai/kaggle-game-arena/</li>
<li>google/BIG-bench: Beyond the Imitation Game collaborative benchmark for measuring and extrapolating the capabilities of language models - GitHub, https://github.com/google/BIG-bench</li>
<li>BIG bench Beyond the Imitation Game benchmark - YouTube, https://www.youtube.com/watch?v=ZpImxa0tK2Y</li>
<li>Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models - Shadi Hamdan, https://shadihamdan.com/publications/bigbench</li>
<li>DeepMind Paper Provides a Mathematically Precise Overview of …, https://syncedreview.com/2022/07/25/deepmind-paper-provides-a-mathematically-precise-overview-of-transformer-architectures-and-algorithms/</li>
<li>ICAPS 2022 - International Conference on Automated Planning and Scheduling, https://icaps22.icaps-conference.org/</li>
<li>Robotics Jun 2022 - arXiv, http://arxiv.org/list/cs.RO/2022-06?skip=225&amp;show=100</li>
<li>Leveraging Language for Accelerated Learning of Tool Manipulation, https://arxiv.org/abs/2206.13074</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>