<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2022년 12월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2022년 12월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2022년 AI 및 로봇 연구 동향</a> / <span>2022년 12월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2022년 12월 AI 및 로봇 연구 동향</h1>
<h2>1. 서론: AI 연구의 변곡점, 2022년 12월</h2>
<h3>1.1 개요</h3>
<p>2022년 12월은 인공지능(AI) 역사상 가장 중요한 변곡점 중 하나로 기록될 것이다. 이 시기는 학계의 최신 연구 성과가 최고 권위의 학회를 통해 발표되는 동시에, OpenAI의 ChatGPT가 대중적 신드롬을 일으키며 생성 AI 시대의 본격적인 개막을 알린 극적인 순간이었다. 학문적 깊이와 대중적 파급력이 동시에 폭발하며 AI 기술이 연구실의 영역을 넘어 사회 전반의 구조적 변화를 이끄는 핵심 동력으로 부상했음을 명확히 보여주었다. 본 보고서는 이 결정적 시기에 발표된 AI 및 로봇 분야의 주요 연구 성과와 기술 동향을 심층적으로 분석하고, 이들이 갖는 함의를 다각도로 조명하는 것을 목표로 한다.1</p>
<p>이 시기는 단순한 개별 기술의 발전이 아닌, AI 분야 전체의 ’상전이(phase transition)’가 일어난 순간으로 해석할 수 있다. NeurIPS와 CoRL과 같은 주요 학회에서 수년간 축적된 기초 연구의 결실이 발표되는 바로 그 시점에, ChatGPT라는 하나의 제품이 대중에게 AI의 능력을 직접 체험하게 함으로써 강력한 피드백 루프를 형성했다. 학문적 연구가 ’어떻게(how)’를 증명하는 동안, ChatGPT는 ‘무엇을(what)’ 할 수 있는지를 명백히 보여주었다. 이로 인해 대중, 산업계, 정책 입안자들은 AI의 영향을 실시간으로 체감하고 대응해야 하는 상황에 직면하게 되었다. 결과적으로, 2022년 12월에 발표된 학문적 성과들은 ChatGPT가 촉발한 상업적, 정치적 압력에 의해 즉각적으로 재조명되었으며, AI 연구는 더 이상 고립된 학문 활동이 아닌, 전 지구적 기술 및 사회 변혁의 직접적인 기반이 되었다. 이 달은 AI가 소수 전문가의 영역에서 벗어나 주류 사회의 핵심 동력으로 자리 잡는 시대의 시작을 알렸다.</p>
<h3>1.2 주요 동향 요약</h3>
<p>2022년 12월의 AI 연구 지형은 대규모 언어 모델(Large Language Models, LLM)과 확산 모델(Diffusion Models)을 필두로 한 생성 AI의 폭발적인 발전으로 특징지어진다. 특히 텍스트-이미지 합성 기술은 전례 없는 수준의 사실성과 언어 이해도를 보여주며 기술적 정점에 도달했음을 입증했다. 이와 더불어, 체화된 AI(Embodied AI) 분야에서는 대규모 절차적 생성 환경을 통해 데이터 부족 문제를 해결하고 에이전트의 일반화 성능을 획기적으로 향상시키는 새로운 연구 패러다임이 제시되었다. 한편, AI 시스템의 신뢰성과 안전성에 대한 근본적인 질문을 던지는 이론 연구 또한 중요한 성과로 인정받으며, 기술의 발전과 함께 그 한계와 위험에 대한 깊은 성찰이 이루어지고 있음을 보여주었다. NeurIPS와 CoRL 등 주요 학회에서 발표된 연구들은 이러한 흐름을 주도했으며, 산업계에서는 ChatGPT의 등장으로 인해 AI 도입 및 투자가 새로운 국면을 맞이하고 있었다.1</p>
<h3>1.3 보고서의 구성</h3>
<p>본 보고서는 총 3개의 장으로 구성된다. 제1장에서는 세계 최고 권위의 AI 학회인 NeurIPS 2022에서 발표된 최우수 논문들을 중심으로 생성 모델, 대규모 데이터셋, 체화된 AI, 그리고 학습 이론의 최신 동향을 심층 분석한다. 제2장에서는 로봇 학습 분야의 대표 학회인 CoRL 2022의 주요 수상 논문들을 통해 로봇이 물리적 세계와 상호작용하고 학습하는 방식에 대한 혁신적인 접근법들을 살펴본다. 제3장에서는 ChatGPT의 등장이 산업계와 AI 생태계 전반에 미친 파급 효과를 투자, 시장 반응, 정책 논의 등 다각적인 측면에서 조명한다. 마지막으로 결론에서는 2022년 12월이 AI 분야에 남긴 유산과 이를 바탕으로 한 향후 연구 방향을 종합적으로 전망한다.</p>
<table><thead><tr><th>학회명</th><th>개최 기간</th><th>개최 장소</th><th>주요 특징</th></tr></thead><tbody>
<tr><td>Conference on Neural Information Processing Systems (NeurIPS) 2022</td><td>2022년 11월 28일 – 12월 9일</td><td>미국 뉴올리언스 (하이브리드)</td><td>AI 및 기계학습 분야 최고 권위 학회. 생성 모델, 대규모 데이터셋, 체화된 AI, 학습 이론 등 핵심 연구 발표 7</td></tr>
<tr><td>Conference on Robot Learning (CoRL) 2022</td><td>2022년 12월 14일 – 12월 18일</td><td>뉴질랜드 오클랜드</td><td>로봇공학과 기계학습의 교차점에 초점을 맞춘 대표적인 학회. 강화학습, 모방학습, 로봇 제어 등 발표 9</td></tr>
<tr><td>6th Int. Conference on Robotics, Machine Learning and AI</td><td>2022년 12월 7일 – 12월 8일</td><td>미국 시카고</td><td>로봇공학, AI, 기계학습 관련 학계 및 산업계 전문가 교류 10</td></tr>
<tr><td>4th European Conference on the Impact of AI and Robotics (ECIAIR)</td><td>2022년 12월 1일 – 12월 2일</td><td>가상</td><td>AI와 로봇이 사회에 미치는 영향에 대해 자연과학, 사회과학, 철학 등 다학제적 논의 11</td></tr>
<tr><td>EAI Int. Conference on Robotic Sensor Networks (ROSENET)</td><td>2022년 12월 15일</td><td>영국 스완지 (온라인)</td><td>AI/기계학습과 지능형 자율 로봇 시스템의 교차점 연구 12</td></tr>
</tbody></table>
<hr />
<h2>2.  NeurIPS 2022 - 신경정보처리시스템학회 주요 연구 동향</h2>
<p>2022년 11월 28일부터 12월 9일까지 개최된 제36회 NeurIPS는 AI 분야의 가장 중요한 학문적 성과가 집결하는 장이었다.7 이 해에는 총 10,000편이 넘는 논문이 제출되어 그중 2,672편이 채택되었으며, 특히 13편의 논문이 최우수 논문(Outstanding Paper)으로 선정되어 학계의 주목을 받았다.13 수상작들은 생성 모델, 대규모 데이터셋, 체화된 AI, 학습 이론 등 당시 AI 연구의 핵심 조류를 명확하게 보여주었다. 이는 단순히 개별 연구의 탁월함을 넘어, 학회 프로그램 위원회가 당시 AI 분야가 직면한 가장 중요한 도전 과제와 유망한 해결 방향에 대한 일종의 서사를 제시한 것으로 해석할 수 있다.</p>
<p>수상작들을 종합적으로 살펴보면, 한편에서는 거대 독점 모델을 통해 성능의 최전선을 밀어붙이는 연구(Imagen)가 있었고, 다른 한편에서는 오픈 데이터를 통해 전체 커뮤니티의 연구를 가능하게 하는 노력(LAION-5B)이 있었다. 또한, 웹 스케일 데이터 수집이 어려운 물리적 세계, 즉 체화된 AI 분야의 데이터 병목 현상을 해결하려는 시도(ProcTHOR)가 있었으며, 마지막으로 이러한 모든 모델들의 신뢰성에 대해 근본적인 질문을 던지는 이론적 탐구(OOD 학습 가능성)가 있었다. 즉, NeurIPS 2022의 수상 결과는 AI 생태계가 당면한 핵심 과제들을 정확히 짚어낸 선견지명 있는 논평과도 같았다. 이는 향후 몇 년간 AI 담론의 중심이 될 주제들—생성 AI의 경이로운 능력, 이를 뒷받침하는 오픈 데이터와 시뮬레이션의 중요성, 그리고 신뢰성과 안전성이라는 깊고 미해결된 이론적 우려—을 미리 조명한 것이었다.</p>
<h3>2.1  생성 모델의 약진: 텍스트-이미지 합성의 새로운 지평</h3>
<p>2022년은 DALL-E 2, Midjourney, Stable Diffusion 등 고품질 텍스트-이미지 생성 모델들이 연이어 공개되며 생성 AI에 대한 대중적 관심이 폭발한 해였다.1 이러한 흐름 속에서 NeurIPS 2022 최우수 논문으로 선정된 구글의 Imagen은 기술적 완성도의 새로운 기준을 제시하며 학문적 정점을 찍었다.</p>
<h4>2.1.1 “Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding” (Imagen) 심층 분석</h4>
<p>Imagen은 전례 없는 수준의 사실성과 깊은 언어 이해도를 결합한 텍스트-이미지 확산 모델이다.14 이 논문의 가장 핵심적인 발견은 이미지 생성 과정에서 텍스트를 이해하는 방식에 대한 기존의 통념을 뒤집었다는 데 있다.</p>
<ul>
<li>
<p><strong>핵심 아이디어 및 아키텍처:</strong> Imagen 이전의 많은 모델들은 텍스트와 이미지를 함께 학습한 CLIP과 같은 멀티모달 인코더를 사용하여 텍스트 프롬프트를 이해했다. 그러나 Imagen 연구팀은 텍스트-이미지 쌍 데이터가 아닌, 방대한 양의 텍스트 데이터만으로 사전 학습된 거대 언어 모델(LLM)을 텍스트 인코더로 사용하는 것이 훨씬 효과적이라는 사실을 발견했다.16 구체적으로, Imagen은 가중치가 고정된(frozen) T5-XXL 모델을 텍스트 인코더로 사용한다. 이 접근법은 모델의 크기를 키울수록 이미지-텍스트 정렬과 이미지 충실도가 이미지 확산 모델의 크기를 키우는 것보다 훨씬 더 극적으로 향상된다는 ’스케일링 법칙’을 확인시켜 주었다. 이는 복잡하고 미묘한 인간의 언어를 깊이 있게 이해하는 능력이 고품질 이미지 생성의 선결 조건임을 시사한다.16 텍스트 인코더가 텍스트 임베딩을 생성하면, 일련의 확산 모델이 이 임베딩을 조건으로 이미지를 생성한다. 먼저 64x64 해상도의 기본 이미지를 생성한 후, 두 단계의 초해상도(super-resolution) 확산 모델을 거쳐 256x256, 최종적으로 1024x1024 해상도의 이미지를 완성하는 계단식(cascade) 구조를 채택했다.16</p>
</li>
<li>
<p><strong>동적 임계값 (Dynamic Thresholding):</strong> 확산 모델에서 이미지-텍스트 정렬을 높이기 위해 사용되는 Classifier-Free Guidance의 가중치를 높이면 이미지가 부자연스럽게 과포화되는 문제가 발생한다. 이는 모델이 예측하는 픽셀 값이 학습 데이터의 범위([−1,1])를 벗어나면서 발생하는 현상이다. 기존의 정적 임계값(예측값을 <span class="math math-inline">[-1, 1]</span>로 강제 클리핑) 방식은 이 문제를 부분적으로 완화하지만, 가중치가 매우 높아지면 여전히 디테일이 손실되는 한계가 있었다. Imagen은 ’동적 임계값’이라는 새로운 샘플링 기법을 제안하여 이 문제를 해결했다. 이 기법은 각 샘플링 단계에서 예측된 픽셀 값 분포의 특정 백분위수(percentile)를 기준으로 임계값을 동적으로 설정한다. 만약 이 임계값이 1을 초과하면, 모든 픽셀 값을 이 동적 임계값으로 정규화하여 포화를 적극적으로 방지한다. 이 기법 덕분에 Imagen은 매우 높은 가이던스 가중치를 사용하면서도 이미지의 사실성과 디테일을 유지할 수 있었고, 이는 더 나은 이미지-텍스트 정렬로 이어졌다.16</p>
</li>
<li>
<p><strong>성능 평가:</strong> Imagen은 널리 사용되는 COCO 데이터셋에서 해당 데이터셋으로 전혀 학습하지 않았음에도 불구하고, 당시 최고 수준(state-of-the-art)의 FID 점수인 7.27을 기록했다.14 또한, 연구팀은 기존 벤치마크의 한계를 지적하며 구성성(compositionality), 수량(cardinality), 공간 관계 등 모델의 다차원적 능력을 평가하기 위한 새로운 벤치마크인 DrawBench를 제안했다. DrawBench를 이용한 인간 평가에서 Imagen은 DALL-E 2, Latent Diffusion 등 당대의 경쟁 모델들을 이미지 품질과 텍스트-이미지 정렬 측면 모두에서 압도적인 차이로 능가했다.17</p>
</li>
</ul>
<h3>2.2  대규모 데이터셋과 오픈 사이언스의 승리: LAION-5B</h3>
<p>Imagen이 거대 기업의 막대한 자원을 바탕으로 한 폐쇄적 연구의 정점을 보여주었다면, 또 다른 최우수 논문상 수상작인 LAION-5B는 AI 연구의 민주화와 오픈 사이언스의 가치를 상징하는 기념비적인 성과였다.</p>
<h4>2.2.1 “LAION-5B: An open large-scale dataset for training next generation image-text models” 분석</h4>
<p>이 연구는 대규모 멀티모달 모델 훈련에 필수적인 수십억 규모의 이미지-텍스트 데이터셋을 최초로 공개하여, 학계와 산업계 전반의 연구 지평을 넓혔다.13</p>
<ul>
<li>
<p><strong>규모와 구성:</strong> LAION-5B는 총 58.5억 개의 이미지-텍스트 쌍으로 구성된, 당시로서는 전례 없는 규모의 공개 데이터셋이다. 구체적으로 영문 캡션을 가진 데이터가 23.2억 개, 100개 이상의 기타 언어로 된 데이터가 22.6억 개, 그리고 특정 언어로 분류하기 어려운 데이터가 12.7억 개 포함되어 있다. 이는 이전 최대 공개 데이터셋이었던 LAION-400M보다 14배 이상 큰 규모이며, 다국어 데이터의 양은 기존 데이터셋 대비 수십 배에 달한다.21</p>
</li>
<li>
<p><strong>데이터 수집 과정:</strong> LAION-5B는 웹 전체를 크롤링하는 비영리 프로젝트인 Common Crawl의 데이터를 기반으로 구축되었다. 연구팀은 Common Crawl에서 수집된 수백억 개의 이미지와 관련 HTML alt-text 쌍에 대해, 사전 학습된 CLIP 모델(ViT-B/32)을 사용하여 이미지와 텍스트 간의 코사인 유사도를 계산했다. 이후, 미리 정해진 임계값(영어 0.28, 다국어 0.26) 이하의 유사도를 갖는 쌍들을 필터링하여 최종 데이터셋을 구성했다. 이 ‘CLIP 필터링’ 방식은 대규모의 노이즈 섞인 웹 데이터로부터 비교적 저렴한 비용으로 고품질의 이미지-텍스트 쌍을 효율적으로 추출할 수 있음을 보여주었다. 또한, 유해 콘텐츠(NSFW)나 워터마크 탐지 점수를 함께 제공하여 연구자들이 필요에 따라 데이터를 추가적으로 필터링할 수 있도록 했다.21</p>
</li>
<li>
<p><strong>학문적 의의:</strong> LAION-5B의 공개는 AI 연구 생태계에 지대한 영향을 미쳤다. 이전까지 CLIP, DALL-E, Imagen과 같은 최첨단 모델의 훈련에 사용되는 수십억 규모의 데이터셋은 소수의 거대 기술 기업만이 접근할 수 있는 독점 자산이었다. 이로 인해 학계나 소규모 연구 그룹은 이러한 모델들을 재현하거나 그 원리를 깊이 있게 탐구하는 데 큰 장벽에 부딪혔다. LAION-5B는 이 장벽을 허물고 대규모 멀티모달 모델 연구를 민주화했다. 실제로 이 데이터셋을 사용하여 Stable Diffusion과 같은 강력한 오픈소스 모델이 훈련될 수 있었으며, 전 세계 연구자들이 자유롭게 기반 모델을 훈련하고, 미세 조정하며, 그 작동 방식과 잠재적 편향성을 분석할 수 있는 토대를 마련했다. 이는 AI 분야의 투명성, 재현성, 그리고 접근성을 한 단계 끌어올린 중요한 과학적 기여로 평가받는다.19</p>
</li>
</ul>
<h3>2.3  체화된 AI를 위한 무한한 환경: ProcTHOR</h3>
<p>생성 모델이 2차원 이미지의 세계에서 혁신을 일으키는 동안, 체화된 AI(Embodied AI) 분야는 3차원 물리 세계와의 상호작용을 위한 데이터 부족 문제에 직면해 있었다. NeurIPS 2022 최우수 논문상 수상작인 ProcTHOR는 이 문제를 해결하기 위한 독창적인 해법을 제시했다.</p>
<h4>2.3.1 “ProcTHOR: Large-Scale Embodied AI Using Procedural Generation” 분석</h4>
<p>ProcTHOR는 체화된 AI 에이전트의 훈련 및 평가를 위해 다양하고 상호작용 가능한 3D 가상 환경을 무한히 생성할 수 있는 프레임워크다.13</p>
<ul>
<li>
<p><strong>방법론:</strong> 기존의 시뮬레이션 환경들은 3D 아티스트가 수작업으로 제작하여 그 수가 제한적이고 다양성이 부족했다. ProcTHOR는 ‘절차적 생성(procedural generation)’ 기술을 도입하여 이 한계를 극복했다. 이 프레임워크는 방의 개수나 종류 같은 상위 수준의 제약 조건으로부터 시작하여, 현실적인 평면도, 벽과 문, 조명, 그리고 가구와 소품에 이르기까지 주택의 모든 요소를 알고리즘에 따라 자동으로 생성하고 배치한다. 1,600개가 넘는 상호작용 가능한 3D 객체 라이브러리와 3,000개 이상의 재질을 활용하여, 생성되는 모든 환경이 고유한 구조와 외형을 갖도록 한다. 특히, ProcTHOR 환경 내의 객체들은 ’열림/닫힘’과 같은 상태 변화가 가능하고 로봇 팔로 조작할 수 있어, 단순한 탐색을 넘어선 복잡한 상호작용 과제 학습에 필수적이다.24</p>
</li>
<li>
<p><strong>ProcTHOR-10K와 성능 입증:</strong> 연구팀은 이 프레임워크를 이용해 10,000개의 서로 다른 가상 주택으로 구성된 ‘PROCTHOR-10K’ 데이터셋을 생성했다. 그리고 이 데이터셋에서 깊이(depth) 정보 없이 오직 RGB 카메라 이미지 만을 입력으로 사용하는 단순한 신경망(CNN+RNN) 에이전트를 훈련시켰다. 놀랍게도, 이 에이전트는 특정 과제에 대한 미세조정(fine-tuning) 없이도(zero-shot), RoboTHOR ObjectNav, Habitat ObjectNav, AI2-THOR Rearrangement 등 6개의 주요 체화된 AI 벤치마크에서 기존의 최고 성능 모델들을 능가하는 결과를 보였다.24</p>
</li>
<li>
<p><strong>함의:</strong> ProcTHOR의 성공은 체화된 AI 분야에서도 ’데이터의 스케일과 다양성’이 에이전트의 일반화 성능을 결정하는 핵심 요소임을 명확히 보여주었다. 이는 컴퓨터 비전과 자연어 처리 분야에서 대규모 데이터셋을 통한 사전 학습이 모델 성능을 비약적으로 향상시킨 것과 동일한 원리가 물리적 세계와 상호작용하는 AI 에이전트에도 적용될 수 있음을 시사한다. ProcTHOR는 체화된 AI 연구를 위한 ‘ImageNet’ 또는 ’Common Crawl’과 같은 역할을 할 수 있는 무한한 데이터 생성의 가능성을 열었으며, 이는 향후 범용 로봇 지능 개발의 중요한 초석이 될 것이다.</p>
</li>
</ul>
<h3>2.4  분포 외 탐지의 이론적 한계와 가능성</h3>
<p>AI 모델의 성능이 비약적으로 발전함에 따라, 이 모델들이 훈련 데이터에서 보지 못한 새로운 상황(Out-of-Distribution, OOD)에 직면했을 때 얼마나 신뢰할 수 있는지에 대한 문제가 중요해졌다. NeurIPS 2022 최우수 논문으로 선정된 “Is Out-of-Distribution Detection Learnable?“은 이 근본적인 질문에 대해 이론적 깊이를 더했다.</p>
<h4>2.4.1 “Is Out-of-Distribution Detection Learnable?” 분석</h4>
<p>이 논문은 OOD 탐지 문제의 근본적인 학습 가능성(learnability)을 계산 학습 이론의 표준 프레임워크인 PAC(Probably Approximately Correct) 이론을 통해 엄밀하게 분석했다.14</p>
<ul>
<li>
<p><strong>불가능성 정리 (Impossibility Theorems):</strong> 논문의 가장 충격적인 결과 중 하나는, 일반적인 조건 하에서 OOD 탐지가 PAC 학습이 불가능하다는 것을 증명한 ’불가능성 정리’다. 이는 훈련 시점의 정상 데이터(In-Distribution, ID)와 테스트 시점에 나타날 수 있는 비정상 데이터(OOD)의 분포가 서로 겹치는 영역이 조금이라도 존재한다면, 유한한 훈련 데이터만으로는 이 둘을 완벽하게 구분하는 일반화된 분류기를 학습하는 것이 이론적으로 불가능함을 의미한다. 다시 말해, 모든 종류의 미지의 OOD 데이터를 완벽하게 탐지해내는 ’만능 OOD 탐지기’는 원리적으로 존재할 수 없다는 것이다.30</p>
</li>
<li>
<p><strong>학습 가능 조건 (Conditions for Learnability):</strong> 그러나 논문은 절망적인 결론만을 내린 것이 아니다. 현실 세계의 많은 시나리오를 반영하는 더 제한적인 가정 하에서는 OOD 탐지가 학습 가능할 수 있음을 보였다. 핵심 조건은 ID 데이터와 OOD 데이터가 분포상에서 명확하게 ’분리(separate)’될 수 있어야 한다는 것이다. 예를 들어, ’고양이’와 ‘개’ 이미지를 ID 데이터로 학습한 모델에게 ’자동차’나 ‘비행기’ 이미지(far-OOD)를 OOD로 탐지하게 하는 시나리오는 학습이 가능하다. 이는 ID와 OOD 데이터가 특징 공간(feature space)에서 겹치지 않기 때문이다. 논문은 이러한 ‘분리 공간(separate space)’ 가정 하에서, OOD 탐지가 학습 가능하기 위한 몇 가지 필요충분조건을 제시했다. 특히, 모델이 완전 연결 신경망(FCNN)일 경우, 특징 공간이 유한하다는 조건 하에 OOD 탐지가 학습 가능함을 증명했다.30</p>
</li>
<li>
<p><strong>함의:</strong> 이 연구는 OOD 탐지 분야에 중요한 이론적 토대를 제공했다. 이는 OOD 탐지 알고리즘을 개발하고 평가할 때, 해당 알고리즘이 어떤 가정 하에서 작동하는지를 명확히 해야 함을 시사한다. 보편적으로 완벽한 알고리즘을 추구하기보다는, 특정 응용 분야(예: 자율주행에서의 예상치 못한 장애물 탐지)에서 마주칠 수 있는 OOD의 특성을 분석하고, 그에 맞는 가정 하에 강인한 탐지 모델을 설계하는 방향으로 연구가 진행되어야 함을 이론적으로 뒷받침한다. 이는 AI 시스템의 안전성과 신뢰성을 확보하는 데 있어 매우 중요한 지침을 제공한다.</p>
</li>
</ul>
<table><thead><tr><th>논문 제목</th><th>핵심 기여</th><th>주요 방법론/기술</th><th>시사점</th></tr></thead><tbody>
<tr><td>Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</td><td>텍스트 전용 LLM을 텍스트 인코더로 사용하여 텍스트-이미지 생성의 사실성과 정렬을 획기적으로 개선</td><td>고정된 T5-XXL 인코더, 계단식 확산 모델, 동적 임계값(Dynamic Thresholding)</td><td>언어에 대한 깊은 이해가 고품질 시각 콘텐츠 생성의 핵심임을 입증 14</td></tr>
<tr><td>LAION-5B: An open large-scale dataset for training next generation image-text models</td><td>58.5억 쌍의 이미지-텍스트 데이터셋을 공개하여 대규모 멀티모달 모델 연구를 민주화</td><td>Common Crawl 데이터에 대한 CLIP 필터링</td><td>대규모 AI 연구의 접근성과 투명성을 높여 오픈소스 기반 모델(예: Stable Diffusion)의 발전을 촉진 14</td></tr>
<tr><td>ProcTHOR: Large-Scale Embodied AI Using Procedural Generation</td><td>절차적 생성을 통해 무한한 3D 상호작용 환경을 제공하여 체화된 AI의 데이터 부족 문제 해결</td><td>절차적 콘텐츠 생성(Procedural Content Generation), 시뮬레이션</td><td>대규모의 다양한 환경 데이터가 체화된 AI 에이전트의 일반화 성능을 비약적으로 향상시킬 수 있음을 증명 14</td></tr>
<tr><td>Is Out-of-Distribution Detection Learnable?</td><td>OOD 탐지의 학습 가능성을 PAC 이론으로 분석하여 이론적 한계와 가능 조건을 규명</td><td>PAC(Probably Approximately Correct) 학습 이론</td><td>OOD 탐지는 보편적으로 학습 불가능하며, ID/OOD 분포가 분리되는 등 특정 조건 하에서만 가능함을 이론적으로 정립 14</td></tr>
</tbody></table>
<h2>3.  CoRL 2022 - 로봇 학습 학회 주요 성과</h2>
<p>2022년 12월 14일부터 18일까지 뉴질랜드 오클랜드에서 개최된 로봇 학습 학회(CoRL 2022)는 로봇이 물리적 세계와 상호작용하며 지능을 획득하는 방법에 대한 최신 연구들이 발표되는 장이었다.9 이 해의 수상 논문들은 로봇이 단순히 사전 프로그래밍된 동작을 수행하는 것을 넘어, 스스로 과업의 성공을 ’검증’하고, 복잡한 환경을 직접 ’인식’하여 행동하며, 인간의 추상적인 언어 지시를 자신의 능력에 맞게 ’추론’하는, 보다 고차원적인 지능으로 나아가는 방향성을 뚜렷하게 보여주었다. 이는 전통적인 로봇 공학의 제어 및 계획 문제에서 벗어나, 불확실하고 동적인 실제 환경에 대응하기 위한 로봇의 ‘인지(cognition)’ 능력을 구축하는 데 연구의 초점이 맞춰지고 있음을 시사한다.</p>
<h3>3.1  로봇이 로봇을 평가하다: 상호작용적 보상 함수</h3>
<p>강화학습을 로봇에 적용할 때 가장 큰 난관 중 하나는 효과적인 보상 함수(reward function)를 설계하는 것이다. CoRL 2022 최고 논문상(Best Paper Award)을 수상한 “Training Robots to Evaluate Robots: Example-Based Interactive Reward Functions for Policy Learning“은 이 문제를 해결하기 위한 혁신적인 패러다임을 제시했다.14</p>
<ul>
<li>
<p><strong>핵심 아이디어:</strong> 이 연구의 핵심은 ’과업의 성공’을 외부에서 정의해주는 대신, 로봇 스스로가 과업이 성공적으로 완료되었는지를 물리적 상호작용을 통해 확인하는 행동 정책을 학습하도록 하는 것이다. 예를 들어, ‘병뚜껑을 꽉 닫는’ 과업의 성공 여부는 단순히 뚜껑이 닫힌 것처럼 보이는 시각적 상태만으로는 판단하기 어렵다. 이 연구에서 제안하는 로봇은 병을 거꾸로 뒤집어 물이 새는지 확인하는 ’평가 행동’을 학습한다. 이 평가 행동의 결과(물이 새지 않음)가 곧 과업 성공을 의미하는 긍정적인 보상이 된다. 이러한 평가 정책을 ’상호작용적 보상 함수(Interactive Reward Function, IRF)’라고 명명했다.34</p>
</li>
<li>
<p><strong>학습 방법:</strong> IRF의 학습은 오직 성공적인 과업 결과물의 예시들(e.g., 물이 새지 않는 여러 개의 닫힌 병)만으로 이루어진다. 로봇은 이 성공 예시들과 상호작용하며, ‘성공’ 상태가 갖는 물리적 속성(e.g., 뒤집어도 물이 새지 않음)을 스스로 발견한다. 이렇게 학습된 IRF는 주 과업(e.g., 병뚜껑 닫기)을 학습하는 강화학습 에이전트에게 보상 신호를 제공하는 역할을 한다. 즉, 주 과업 에이전트가 뚜껑을 닫은 후, IRF 에이전트가 병을 뒤집어보고 물이 새지 않으면 높은 보상을 주는 방식이다.34</p>
</li>
<li>
<p><strong>함의 및 기여:</strong> 이 접근법은 ‘상태 특정 불확실성(state aliasing)’ 문제, 즉 겉보기에는 같아 보이지만 실제 물리적 상태는 다른 경우(e.g., 꽉 닫힌 뚜껑 vs 헐겁게 닫힌 뚜껑)를 해결하는 강력한 방법을 제시한다. 로봇이 수동적인 관찰자가 아니라 능동적인 실험자로서 환경과 상호작용하며 지식을 습득하게 함으로써, 훨씬 더 강인하고 신뢰할 수 있는 기술을 학습할 수 있는 길을 열었다. 이는 보상 함수 설계의 어려움을 덜어줄 뿐만 아니라, 로봇 지능이 물리 세계에 대한 더 깊은 인과적 이해를 갖추도록 하는 중요한 진일보다.</p>
</li>
</ul>
<h3>3.2  자기중심적 시각을 이용한 강인한 보행 제어</h3>
<p>다족 보행 로봇이 인간처럼 자연스럽고 안정적으로 복잡한 지형을 이동하는 것은 로봇 공학의 오랜 목표다. CoRL 2022 최고 시스템 논문상(Best Systems Paper Award)을 수상한 “Legged Locomotion in Challenging Terrains using Egocentric Vision“은 이 목표에 한 걸음 더 다가가는 중요한 성과를 보여주었다.14</p>
<ul>
<li>
<p><strong>방법론:</strong> 이 연구는 전통적인 ’고도 맵핑(elevation mapping) → 발판 계획(foothold planning)’의 분리된 파이프라인 방식을 과감히 버렸다. 기존 방식은 주변 환경의 3D 맵을 생성하고, 그 위에서 안전한 발판 위치를 계산한 후, 다리 움직임을 계획하는 다단계 과정을 거친다. 이 방식은 맵핑 과정에서의 노이즈나 오차에 취약하고 계산 비용이 높다는 단점이 있다. 이 논문은 로봇의 전면에 부착된 단일 깊이 카메라에서 얻은 ’자기중심적 시각 정보(egocentric vision)’를 입력받아, 신경망을 통해 직접 각 관절의 모터 명령을 출력하는 종단간(end-to-end) 학습 방식을 채택했다. 이는 마치 동물이 주변을 둘러보고 즉각적으로 발을 내딛는 것과 유사한, 훨씬 더 직관적이고 반응성이 빠른 제어 방식이다.36</p>
</li>
<li>
<p><strong>2단계 학습 전략:</strong> 종단간 정책을 학습시키기 위해서는 방대한 양의 데이터가 필요하지만, 시뮬레이션에서 고해상도 깊이 이미지를 렌더링하는 것은 매우 느리다. 이 문제를 해결하기 위해 연구팀은 독창적인 2단계 학습 전략을 고안했다.</p>
</li>
</ul>
<ol>
<li>
<p><strong>1단계 (강화학습):</strong> 먼저, 계산 비용이 매우 저렴한 가상의 지형 센서 데이터(‘Scandots’)를 입력으로 받는 ‘교사’ 정책을 강화학습(PPO)으로 훈련시킨다.</p>
</li>
<li>
<p><strong>2단계 (정책 증류):</strong> 그 다음, 실제 깊이 이미지를 입력으로 받는 ‘학생’ 정책이 ‘교사’ 정책의 행동을 모방하도록 지도학습(supervised learning)을 통해 학습시킨다. 이 ‘정책 증류(policy distillation)’ 방식을 통해, 계산 비용이 많이 드는 강화학습 과정을 피하면서도 효율적으로 고성능의 시각 기반 보행 정책을 얻을 수 있었다.36</p>
</li>
</ol>
<ul>
<li><strong>성능 및 장점:</strong> 이 방법으로 훈련된 A1 사족 로봇은 시뮬레이션에서 현실 세계로 별도의 미세조정 없이 성공적으로 전이(sim-to-real transfer)되었다. 로봇은 계단, 연석, 징검다리, 틈 등 이전에는 매우 어려웠던 다양한 비정형 지형을 안정적으로 통과했으며, 외부에서 밀거나 미끄러운 바닥에서도 넘어지지 않는 높은 강인성을 보여주었다. 특히, 로봇의 시야에 보이지 않는 뒷발의 위치를 제어하기 위해 순환 신경망(RNN)을 사용하여 과거의 시각 정보를 기억하는 능력을 갖추었다. 이는 고도 맵핑 방식의 한계를 극복하고, 더 적은 계산 자원과 센서만으로 더 뛰어난 보행 능력을 달성할 수 있음을 입증한 중요한 시스템적 성과다.36</li>
</ul>
<h3>3.3  언어 모델의 물리적 접지: SayCan 방법론</h3>
<p>2022년 말, 대규모 언어 모델(LLM)이 보여준 놀라운 능력은 ’이 지능을 어떻게 물리적 세계와 연결할 것인가?’라는 새로운 질문을 낳았다. CoRL 2022 특별 혁신상(Special Innovation Award)을 수상한 “Do As I Can, Not As I Say: Grounding Language in Robotic Affordances“는 이 질문에 대한 가장 영향력 있는 초기 답변 중 하나를 제시했다.14</p>
<ul>
<li>
<p><strong>핵심 아이디어 (SayCan):</strong> 이 연구의 핵심은 LLM이 가진 방대한 상식과 절차적 지식(“Say”)을 로봇이 현재 환경에서 물리적으로 수행할 수 있는 능력(“Can”)과 결합하는 것이다. 예를 들어, “음료수를 쏟았는데 좀 도와줄래?“와 같은 추상적인 인간의 지시가 주어졌을 때, LLM은 “스펀지를 찾는다”, “싱크대로 간다”, “진공청소기를 사용한다” 등 다양한 가능한 하위 작업을 제안할 수 있다. 하지만 이 중 어떤 것이 현재 로봇이, 현재 위치에서, 가지고 있는 도구로 실제로 ‘할 수 있는’ 행동인지는 알지 못한다. SayCan은 이 간극을 메우는 것을 목표로 한다.40</p>
</li>
<li>
<p><strong>작동 방식:</strong> SayCan 프레임워크는 두 가지 핵심 요소의 확률적 결합으로 작동한다.</p>
</li>
</ul>
<ol>
<li>
<p><strong>Say (언어 모델의 유용성 점수):</strong> LLM(PaLM)에게 현재의 상위 지시와 로봇이 수행 가능한 모든 기술(e.g., “스펀지 찾기”, “사과 집기”) 목록을 프롬프트로 제공한다. LLM은 각 기술이 상위 지시를 해결하는 데 얼마나 유용하고 논리적으로 적합한지에 대한 확률(p(skill∣instruction))을 계산한다.</p>
</li>
<li>
<p><strong>Can (어포던스 함수의 성공 확률):</strong> 로봇은 사전 학습된 각 기술에 대해, 현재 로봇의 상태(e.g., 카메라 이미지)에서 해당 기술을 성공적으로 수행할 수 있는 확률, 즉 ’어포던스(affordance)’를 계산한다. 이 확률은 각 기술을 강화학습으로 학습할 때 얻어지는 가치 함수(value function)를 통해 추정된다 (p(success∣state, skill)).</p>
</li>
<li>
<p><strong>행동 선택:</strong> 최종적으로 로봇은 모든 가능한 기술에 대해 ’유용성 점수’와 ’성공 확률’을 곱하여 가장 높은 점수를 얻는 기술을 다음 행동으로 선택하고 실행한다. 이 과정을 목표가 달성될 때까지 반복한다.40</p>
</li>
</ol>
<ul>
<li><strong>함의:</strong> SayCan은 LLM을 추상적인 텍스트 생성기에서 벗어나, 실제 세계의 제약을 이해하고 장기적인 계획을 수립하는 로봇의 ’두뇌’로 활용할 수 있는 길을 열었다. 이는 LLM의 지식을 물리적 세계에 ’접지(grounding)’시키는 매우 실용적이고 효과적인 방법을 제시한 것으로, 이후 언어 기반 로봇 제어 및 인간-로봇 상호작용 연구에 지대한 영향을 미쳤다.</li>
</ul>
<table><thead><tr><th>수상 부문</th><th>논문 제목</th><th>핵심 아이디어</th><th>로봇 학습에 대한 기여</th></tr></thead><tbody>
<tr><td><strong>Best Paper</strong></td><td>Training Robots to Evaluate Robots: Example-Based Interactive Reward Functions for Policy Learning</td><td>로봇이 물리적 상호작용을 통해 스스로 과업 성공을 평가하는 행동(IRF)을 학습하고, 이를 강화학습의 보상으로 활용</td><td>보상 함수 설계의 어려움을 해결하고, 시각 정보만으로 불충분한 과업(e.g., 견고함, 밀봉) 학습의 새로운 길을 제시 33</td></tr>
<tr><td><strong>Best Systems Paper</strong></td><td>Legged Locomotion in Challenging Terrains using Egocentric Vision</td><td>자기중심적 시각 정보를 직접 모터 명령으로 변환하는 종단간 정책을 학습하여 복잡한 지형에서의 강인한 보행을 구현</td><td>전통적인 맵핑-계획 파이프라인의 한계를 극복하고, 더 적은 자원으로 더 뛰어난 성능과 일반화 능력을 보이는 시스템 구축 33</td></tr>
<tr><td><strong>Special Innovation Award</strong></td><td>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</td><td>LLM의 언어적 지식(“Say”)과 로봇의 물리적 수행 능력(“Can”, 어포던스)을 결합하여 추상적 자연어 명령을 실행</td><td>LLM을 물리적 세계에 ’접지’시켜, 장기적이고 복잡한 작업을 수행하는 로봇의 계획 및 추론 능력의 토대를 마련 33</td></tr>
</tbody></table>
<h2>4.  산업계 및 생태계 주요 동향</h2>
<p>2022년 12월은 학문적 성과뿐만 아니라, AI 기술이 산업과 사회에 미치는 영향이 극적으로 가시화된 시기였다. 특히 OpenAI의 ChatGPT 출시는 기술 생태계 전반에 거대한 충격파를 던졌고, 이는 AI에 대한 투자, 기업 전략, 정책 논의의 방향을 근본적으로 바꾸는 계기가 되었다. 이 장에서는 ChatGPT가 촉발한 변화와 더불어, 당시 AI 기술의 발전을 뒷받침하고 있던 하드웨어 및 정책적 기반을 함께 살펴본다. 이를 통해 당시 기술적 역량과 사회적 수용 및 거버넌스 사이에 존재했던 ’역량-거버넌스 격차(capability-governance gap)’가 어떻게 드러났는지 분석한다.</p>
<h3>4.1  ChatGPT의 등장과 생성 AI의 대중화</h3>
<p>2022년 11월 30일 공개된 ChatGPT는 단순한 기술 시연을 넘어 사회적 현상으로 발전했다. 그 기술적 기반과 폭발적인 반응, 그리고 이로 인해 촉발된 다양한 논의는 2022년 12월을 생성 AI 시대의 원년으로 만들었다.</p>
<ul>
<li><strong>기술적 배경: RLHF의 성공:</strong> ChatGPT의 성공 뒤에는 ’인간 피드백을 통한 강화학습(Reinforcement Learning from Human Feedback, RLHF)’이라는 기술이 있었다. 이는 GPT-3.5와 같은 사전 학습된 언어 모델을 인간의 선호도에 맞게 미세 조정하는 3단계 프로세스다.42</li>
</ul>
<ol>
<li>
<p><strong>지도 미세조정(Supervised Fine-Tuning):</strong> 먼저, 인간 작업자가 작성한 고품질의 프롬프트-응답 쌍 데이터를 사용하여 모델이 지시를 따르고 대화 형식에 익숙해지도록 학습시킨다.44</p>
</li>
<li>
<p><strong>보상 모델 훈련(Reward Model Training):</strong> 다음으로, 하나의 프롬프트에 대해 모델이 생성한 여러 응답을 인간 작업자가 선호도에 따라 순위를 매긴다. 이 순위 데이터를 학습하여, 어떤 응답이 더 나은지를 예측하는 별도의 ’보상 모델’을 구축한다.46</p>
</li>
<li>
<p><strong>강화학습(Reinforcement Learning):</strong> 마지막으로, 언어 모델을 강화학습의 ’에이전트’로 간주하고, 보상 모델이 주는 점수를 ’보상’으로 사용하여 언어 모델의 정책을 최적화한다(주로 PPO 알고리즘 사용). 이 과정을 통해 모델은 인간이 선호할 만한 응답을 생성하는 방향으로 점차 개선된다.45</p>
</li>
</ol>
<p>이 RLHF 기법 덕분에 ChatGPT는 이전 모델들보다 훨씬 더 유용하고, 안전하며, 대화의 맥락을 잘 따르는 능력을 갖추게 되었다.49</p>
<ul>
<li>
<p><strong>폭발적인 반응과 ‘GPT 모멘트’:</strong> ChatGPT는 출시 단 5일 만에 100만 사용자, 두 달 만에 1억 월간 활성 사용자를 돌파하며 역사상 가장 빠르게 성장한 소비자 애플리케이션이 되었다.2 2022년 12월 한 달 동안 소셜 미디어와 언론은 ChatGPT가 에세이를 쓰고, 코드를 짜고, 시를 짓는 능력에 대한 놀라움으로 가득 찼다.51 이 현상은 ’GPT 모멘트’로 불리며, 생성 AI에 대한 벤처 캐피탈의 투자 심리를 완전히 바꿔놓았다. 2022년 11월 이전에도 Stability AI(Stable Diffusion 개발사)가 1억 달러, Jasper가 1.25억 달러의 대규모 투자를 유치하는 등 생성 AI에 대한 관심이 있었지만 53, ChatGPT의 성공 이후 투자 열기는 광풍에 가까워졌다. 이는 생성 AI 기술이 단순한 가능성을 넘어 거대한 시장을 창출할 수 있다는 확실한 증거로 받아들여졌기 때문이다.54</p>
</li>
<li>
<p><strong>사회적 파급 효과:</strong> ChatGPT의 영향력은 기술 및 투자 영역에만 머무르지 않았다.</p>
</li>
<li>
<p><strong>시장 재편 우려:</strong> ChatGPT가 구글 검색과 같은 기존의 정보 검색 시장을 잠식할 수 있다는 분석이 제기되면서, 빅테크 기업들 간의 경쟁 구도에 대한 논의가 촉발되었다.4</p>
</li>
<li>
<p><strong>학문적 무결성 논란:</strong> 교육 현장에서는 학생들이 ChatGPT를 이용해 과제를 표절할 것이라는 우려가 즉각적으로 터져 나왔다. 이는 단순히 베끼는 수준을 넘어, AI가 생성한 그럴듯한 텍스트를 자신의 창작물인 것처럼 제출하는 새로운 형태의 부정행위에 대한 문제였다. 이에 뉴욕시 교육청은 2023년 1월, 관내 모든 공립학교 기기와 네트워크에서 ChatGPT 접속을 차단하는 강력한 조치를 취했다. 이는 AI 기술의 교육적 활용과 학문적 정직성 유지 사이의 긴장 관계를 보여주는 상징적인 사건이었다.56</p>
</li>
</ul>
<h3>4.2  AI 도입 및 투자 동향: McKinsey 글로벌 서베이 분석</h3>
<p>ChatGPT가 열풍을 일으키기 직전, 기업들의 AI 도입 현황은 어떠했을까? 2022년 12월 6일에 발표된 McKinsey의 연례 ‘AI 현황(The state of AI in 2022)’ 보고서는 당시 산업계의 상황을 보여주는 중요한 단서를 제공한다.5</p>
<ul>
<li>
<p><strong>주요 결과:</strong></p>
</li>
<li>
<p><strong>도입률 정체:</strong> 2017년 이후 기업의 AI 도입률은 20%에서 50%로 두 배 이상 증가했지만, 2019년 58%를 정점으로 최근 몇 년간 50%대에서 정체되는 양상을 보였다. 이는 많은 기업이 AI를 최소 하나 이상의 사업 영역에 도입했지만, 전사적으로 확산하는 데에는 어려움을 겪고 있었음을 시사한다.5</p>
</li>
<li>
<p><strong>안정된 활용 사례:</strong> 가장 보편적으로 사용되는 AI 역량은 로보틱 프로세스 자동화(RPA)와 컴퓨터 비전이었으며, 서비스 운영 최적화는 4년 연속 가장 큰 가치를 창출하는 활용 사례로 꼽혔다. 이는 기업들이 주로 비용 절감과 효율성 증대라는 명확한 목표를 가진 분석적 AI(analytical AI)에 집중하고 있었음을 보여준다.5</p>
</li>
<li>
<p><strong>투자 증가와 가치 창출 영역의 변화:</strong> AI에 대한 투자는 꾸준히 증가하여, 응답 기업의 과반이 디지털 예산의 5% 이상을 AI에 할당한다고 답했다. 흥미로운 점은 AI를 통해 가치를 창출하는 영역이 변화했다는 것이다. 2018년에는 제조 및 리스크 관리가 주도했지만, 2022년에는 마케팅 및 영업, 제품/서비스 개발과 같은 매출 증대와 직접적으로 관련된 영역에서 더 큰 효과가 보고되었다.59</p>
</li>
<li>
<p><strong>함의:</strong> 이 보고서는 생성 AI 혁명이 일어나기 직전, 기업 AI 시장이 일종의 ‘성숙기’ 혹은 ’정체기’에 접어들었음을 보여준다. 많은 기업이 AI의 잠재력을 인지하고 투자를 늘리고 있었지만, 검증된 특정 활용 사례를 넘어 새로운 가치를 창출하는 데는 어려움을 겪고 있었다. 이러한 상황에서 등장한 ChatGPT는 기업들에게 AI를 활용하는 완전히 새로운 패러다임을 제시했다. 콘텐츠 생성, 고객 서비스, 소프트웨어 개발 등 이전에 상상하기 어려웠던 영역에서 AI의 활용 가능성을 열어주며, 정체되어 있던 기업의 AI 도입 및 투자에 새로운 활력을 불어넣는 결정적인 기폭제가 되었다.</p>
</li>
</ul>
<h3>4.3  AI 발전을 뒷받침하는 하드웨어와 정책</h3>
<p>AI 모델의 눈부신 발전은 알고리즘의 혁신만으로 이루어지지 않는다. 이를 뒷받침하는 반도체 하드웨어의 발전과 기술의 방향을 유도하는 정책 및 규제 논의 역시 필수적인 요소다. 2022년 12월은 이 두 가지 측면에서도 중요한 이정표가 있었다.</p>
<ul>
<li>
<p><strong>하드웨어 발전: TSMC 3나노 공정 양산:</strong> 2022년 12월, 세계 최대 반도체 파운드리 기업인 TSMC는 3나노미터(nm) 공정의 대량 생산 시작을 공식화했다.60 3나노 공정은 기존의 주력이었던 5나노 공정에 비해 동일 전력에서 10~15%의 성능 향상 또는 동일 성능에서 25~35%의 전력 감소를 제공하며, 논리 집적도는 최대 70%까지 높일 수 있다. 이는 AI 모델의 연산에 필수적인 AI 가속기(GPU, TPU 등)의 성능과 효율을 비약적으로 향상시킬 수 있음을 의미한다. NVIDIA, Apple, AMD와 같은 주요 팹리스 기업들은 TSMC의 최첨단 공정을 기반으로 차세대 칩을 설계하므로, 3나노 공정의 양산은 향후 더 크고 강력한 AI 모델의 등장을 가능하게 하는 물리적 기반을 마련한 사건이라 할 수 있다.60</p>
</li>
<li>
<p><strong>정책 및 규제 논의:</strong> AI 기술의 사회적 영향력이 커지면서, 각국 정부는 AI의 책임 있는 개발과 사용을 위한 정책 프레임워크 구축에 나서기 시작했다.</p>
</li>
<li>
<p><strong>미국 AI 권리장전 청사진:</strong> 2022년 10월, 바이든 행정부는 ’AI 권리장전 청사진(Blueprint for an AI Bill of Rights)’을 발표했다. 이는 AI 시스템으로부터 대중을 보호하기 위한 5가지 핵심 원칙—1) 안전하고 효과적인 시스템, 2) 차별 방지, 3) 데이터 프라이버시, 4) 공지 및 설명, 5) 인간 대안, 고려 및 예비 조치—을 제시했다. 법적 구속력은 없는 선언적 원칙이었지만, 미국 정부가 AI의 잠재적 위험을 공식적으로 인정하고, 향후 규제 논의의 방향성을 제시했다는 점에서 중요한 의미를 지닌다.1</p>
</li>
<li>
<p><strong>글로벌 규제 동향:</strong> 2022년 12월 당시, 유럽연합(EU)은 세계 최초의 포괄적인 AI 규제 법안인 ‘AI 법(AI Act)’ 초안에 대한 논의를 활발히 진행하고 있었다. 이 법안은 AI 시스템을 위험 수준에 따라 ‘수용 불가’, ‘고위험’, ‘제한된 위험’, ’최소 위험’으로 분류하고 차등적으로 규제하는 접근 방식을 취한다.62 이처럼 2022년 말은 전 세계적으로 AI 거버넌스에 대한 논의가 본격화되던 시점이었다. 바로 이 시점에 등장한 ChatGPT는 기존의 분석적 AI를 중심으로 설계되던 규제 논의에 ’생성 AI’라는 새로운 변수를 던져주었다. 허위 정보 생성, 저작권 침해, 예측 불가능성 등 생성 AI가 가진 고유한 위험들은 기존의 규제 프레임워크를 재검토하고 확장해야 할 필요성을 부각시키며, 이후의 정책 논의를 더욱 복잡하고 시급한 과제로 만들었다.63</p>
</li>
</ul>
<p>이처럼 2022년 12월은 ChatGPT의 폭발적인 기술적 역량과 TSMC가 예고한 하드웨어의 기하급수적 발전이, McKinsey 보고서가 보여준 선형적인 기업 도입 전략 및 AI 권리장전으로 대표되는 점진적인 정책 대응과 극명한 대조를 이루며 ’역량-거버넌스 격차’를 수면 위로 드러낸 시점이었다. 이 격차는 이후 AI 분야의 모든 이해관계자가 해결해야 할 핵심 과제가 되었다.</p>
<h2>5. 결론: 2022년 12월의 유산과 미래 전망</h2>
<h3>5.1 종합 및 요약</h3>
<p>2022년 12월은 AI 연구와 산업의 패러다임이 근본적으로 전환되었음을 알리는 신호탄이었다. 이 시기에 발표된 학문적, 기술적 성과들은 ’스케일(scale)’과 ’데이터(data)’라는 두 가지 키워드가 AI의 모든 영역—생성, 인식, 행동, 이론—을 관통하는 핵심 동인임을 명백히 증명했다. NeurIPS 2022에서 발표된 Imagen은 거대 언어 모델의 스케일이 시각적 창의성의 질을 결정함을 보여주었고, LAION-5B는 대규모 데이터의 개방이 연구 생태계 전체의 발전을 어떻게 촉진하는지를 입증했다. 또한, ProcTHOR는 물리적 세계를 다루는 체화된 AI 분야조차도 방대한 가상 환경 데이터의 스케일을 통해 기존의 한계를 돌파할 수 있음을 시사했다. 이러한 스케일 중심의 발전 속에서, CoRL 2022의 주요 연구들은 이 강력한 모델들을 어떻게 물리적 세계에 ’접지’시키고(SayCan), 스스로의 행동을 ’검증’하며(IRF), 불확실한 환경을 ’인식’하게(Egocentric Locomotion) 만들 것인지에 대한 구체적이고 혁신적인 방법론을 제시했다.</p>
<h3>5.2 ChatGPT의 촉매 역할</h3>
<p>이러한 심도 깊은 학문적 성과가 전문가들 사이에서 논의되는 동안, ChatGPT는 AI의 파괴적인 잠재력을 전 세계 대중에게 각인시키는 결정적인 촉매 역할을 했다. 이는 AI 기술을 소수 연구자의 실험실에서 꺼내어 사회, 경제, 문화의 중심으로 단숨에 옮겨놓은 사건이었다. ChatGPT의 등장은 AI에 대한 막연한 기대를 구체적인 경험으로 바꾸었고, 그 결과 투자, 기업 간 경쟁, 규제 및 윤리 논의 등 AI와 관련된 모든 담론의 기준점과 논의 속도를 완전히 재설정했다. 2022년 12월 이전과 이후의 AI 생태계는 질적으로 다른 시대로 구분될 만큼, 그 영향은 즉각적이고 심대했다.1</p>
<h3>5.3 미래 전망</h3>
<p>2022년 12월을 기점으로, AI 연구 개발 패러다임은 소수의 거대 ’기반 모델(Foundation Models)’을 중심으로 빠르게 재편될 것이다. 앞으로의 연구는 단순히 모델의 성능을 극대화하는 것을 넘어, 이 강력한 모델들을 더 안전하고(OOD 탐지, 편향성 제거, 환각 방지), 더 효율적이며(모델 경량화, 추론 최적화), 더 유용하게(로봇 제어, 도구 사용, 다중 모드 상호작용) 만드는 방향으로 집중될 것이다. 즉, ’더 큰 모델’을 만드는 경쟁과 함께 ’더 똑똑하고 신뢰할 수 있는 모델’을 만드는 연구가 핵심 과제로 부상할 것이다.</p>
<p>동시에, AI 기술의 급격한 발전에 대응하기 위한 사회적, 제도적 논의는 더욱 치열해질 수밖에 없다. 학문적 무결성, 일자리 변화, 허위 정보, 저작권, 안전성 등 ChatGPT가 수면 위로 끌어올린 문제들은 기술 개발과 규제 사이의 건강한 긴장 관계를 요구한다. 기술의 발전 속도를 저해하지 않으면서도 잠재적 위험을 통제할 수 있는 유연하고 적응적인 거버넌스 체계를 구축하는 것이 향후 10년간 국제 사회의 중요한 과제가 될 것이다.64</p>
<p>결론적으로, 2022년 12월은 단순한 한 달이 아니라, AI가 새로운 시대로 진입했음을 알린 ’특이점(singularity)’에 가까운 시간으로 역사에 기록될 것이다. 이 시기에 뿌려진 학문적, 기술적, 사회적 씨앗들은 앞으로 수십 년간 인류의 미래를 형성해 나갈 것이다.6</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Biggest AI Headlines in 2022 – At Stanford and Beyond, https://hai.stanford.edu/news/biggest-ai-headlines-2022-stanford-and-beyond</li>
<li>ChatGPT Timeline: The Evolution of AI-Powered Conversations - Adogy, https://www.adogy.com/chatgpt-timeline-the-evolution-of-ai-powered-conversations/</li>
<li>How ChatGPT changed… well, almost everything - Cisco Newsroom, https://newsroom.cisco.com/c/r/newsroom/en/us/a/y2024/m12/how-chatgpt-changed-well-almost-everything.html</li>
<li>“What Can ChatGPT Do?” Analyzing Early Reactions to the Innovative AI Chatbot on Twitter, https://www.mdpi.com/2504-2289/7/1/35</li>
<li>The state of AI in 2022—and a half decade in review - McKinsey, <a href="https://www.mckinsey.com/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202022%20and%20a%20half%20decade%20in%20review/the-state-of-ai-in-2022-and-a-half-decade-in-review.pdf">https://www.mckinsey.com/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202022%20and%20a%20half%20decade%20in%20review/the-state-of-ai-in-2022-and-a-half-decade-in-review.pdf</a></li>
<li>Artificial intelligence in 2022: the AIhub roundup, https://aihub.org/2022/12/30/artificial-intelligence-in-2022-the-aihub-roundup/</li>
<li>neurips.cc, <a href="https://neurips.cc/Conferences/2022#:~:text=Monday%2C%20November%2028th%20through%20Friday,virtual%20component%20the%20second%20week.">https://neurips.cc/Conferences/2022#:~:text=Monday%2C%20November%2028th%20through%20Friday,virtual%20component%20the%20second%20week.</a></li>
<li>2022 Dates and Deadlines - NeurIPS 2025, https://nips.cc/Conferences/2022/Dates</li>
<li>CORL 2022 – Dec 14-18, 2022 – Auckland, NZ, https://corl2022.org/</li>
<li>6th International Conference on Robotics, Machine Learning and Artificial Intelligence, https://www.innoget.com/innovation-events/2336/6th-international-conference-on-robotics-machine-learning-and-artificial-intelligence</li>
<li>4th European Conference on the Impact of Artificial Intelligence and Robotics - EDDS, https://www.edds-education.org/event-details/edds-eciair-4th-european-conference-on-the-impact-of-artificial-intelligence-and-robotics</li>
<li>6th EAI International Conference on Robotics and Networks - EAI ROSENET 2022, https://rosenets.eai-conferences.org/2022/</li>
<li>Allen School and AI2 researchers recognized at NeurIPS for outstanding contributions in large-scale embodied AI and next-generation image-text models, https://news.cs.washington.edu/2023/01/06/allen-school-and-ai2-researchers-recognized-at-neurips-for-outstanding-contributions-in-large-scale-embodied-ai-and-next-generation-image-text-models/</li>
<li>NeurIPS 2022 Awards - NeurIPS 2025, https://nips.cc/virtual/2022/awards_detail</li>
<li>IFML Researchers Win Two Outstanding Paper Awards at NeurIPS 2022, https://www.ifml.institute/index.php/node/363</li>
<li>Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding - NIPS, https://papers.neurips.cc/paper_files/paper/2022/file/ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf</li>
<li>Photorealistic Text-to-Image Diffusion Models with Deep Language …, https://arxiv.org/pdf/2205.11487</li>
<li>Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding, https://openreview.net/forum?id=08Yk-n5l2Al</li>
<li>JSC Researchers Receive Outstanding Paper Award at NeurIPS 2022 - Forschungszentrum Jülich, https://www.fz-juelich.de/en/ias/jsc/news/news-items/news-flashes/2023/outstanding-paper-neurips-2022</li>
<li>Outstanding Paper Award on NeurIPS 2022 conference - Forschungszentrum Jülich, https://www.fz-juelich.de/en/ias/jsc/about-us/profile/awards/best-paper-best-poster-awards/outstanding-paper-award-on-neurips-2022-conference</li>
<li>LAION-5B: An open large-scale dataset for training next generation image-text models, https://www.researchgate.net/publication/364443227_LAION-5B_An_open_large-scale_dataset_for_training_next_generation_image-text_models</li>
<li>[2210.08402] LAION-5B: An open large-scale dataset for training …, https://ar5iv.labs.arxiv.org/html/2210.08402</li>
<li>LAION-5B: A NEW ERA OF OPEN LARGE-SCALE MULTI-MODAL DATASETS, https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/</li>
<li>ProcTHOR: Large-Scale Embodied AI Using Procedural Generation, https://proceedings.neurips.cc/paper_files/paper/2022/file/27c546ab1e4f1d7d638e6a8dfbad9a07-Paper-Conference.pdf</li>
<li>ProcTHOR: Large-Scale Embodied AI Using Procedural Generation - ResearchGate, https://www.researchgate.net/publication/361300610_ProcTHOR_Large-Scale_Embodied_AI_Using_Procedural_Generation</li>
<li>ProcTHOR, https://procthor.allenai.org/</li>
<li>[2206.06994] ProcTHOR: Large-Scale Embodied AI Using Procedural Generation - ar5iv, https://ar5iv.labs.arxiv.org/html/2206.06994</li>
<li>Is Out-of-distribution Detection Learnable? - Hong Kong Baptist University - HKBU Scholars, https://scholars.hkbu.edu.hk/en/publications/is-out-of-distribution-detection-learnable</li>
<li>[PDF] Is Out-of-Distribution Detection Learnable? - Semantic Scholar, https://www.semanticscholar.org/paper/Is-Out-of-Distribution-Detection-Learnable-Fang-Li/a5927a417272327d424a76b2d260f6c1a73bc71e</li>
<li>Is Out-of-Distribution Detection Learnable? - OPUS at UTS, https://opus.lib.uts.edu.au/rest/bitstreams/192e682b-c60c-41c2-8081-069214019a2a/retrieve</li>
<li>On the Learnability of Out-of-distribution Detection - Journal of Machine Learning Research, https://jmlr.org/papers/volume25/23-1257/23-1257.pdf</li>
<li>1월 1, 1970에 액세스, https://proceedings.neurips.cc/paper_files/paper/2022/file/5fb827c1a2b71a8ac2a46b3e3415b3c0-Paper-Conference.pdf</li>
<li>Awards - CoRL 2022, https://corl2022.org/awards/</li>
<li>Training Robots to Evaluate Robots: Example-Based Interactive …, https://proceedings.mlr.press/v205/huang23a/huang23a.pdf</li>
<li>Training Robots to Evaluate Robots: Example-Based Interactive Reward Functions for Policy Learning - Bohrium, https://www.bohrium.com/paper-details/training-robots-to-evaluate-robots-example-based-interactive-reward-functions-for-policy-learning/867751029599371606-108614</li>
<li>Legged Locomotion in Challenging Terrain using Egocentric Vision, https://vision-locomotion.github.io/</li>
<li>Legged Locomotion in Challenging Terrains using Egocentric Vision - ResearchGate, https://www.researchgate.net/publication/365373091_Legged_Locomotion_in_Challenging_Terrains_using_Egocentric_Vision</li>
<li>Legged Locomotion in Challenging Terrains using Egocentric Vision, https://proceedings.mlr.press/v205/agarwal23a/agarwal23a.pdf</li>
<li>Legged Locomotion in Challenging Terrains using Egocentric Vision - OpenReview, https://openreview.net/forum?id=Re3NjSwf0WF</li>
<li>Do As I Can, Not As I Say: Grounding Language in … - SayCan, https://say-can.github.io/assets/palm_saycan.pdf</li>
<li>SayCan: Grounding Language in Robotic Affordances, https://say-can.github.io/</li>
<li>What’s the timeframe between each gpt release? (1,2,3,4) : r/singularity - Reddit, https://www.reddit.com/r/singularity/comments/1lx6ito/whats_the_timeframe_between_each_gpt_release_1234/</li>
<li>Introducing ChatGPT - OpenAI, https://openai.com/index/chatgpt/</li>
<li>What Is Reinforcement Learning From Human Feedback (RLHF)? - IBM, https://www.ibm.com/think/topics/rlhf</li>
<li>ChatGPT’s Technical Foundations: Transformers to RLHF | IntuitionLabs, https://intuitionlabs.ai/articles/key-innovations-behind-chatgpt</li>
<li>What is RLHF? - Reinforcement Learning from Human Feedback Explained - AWS, https://aws.amazon.com/what-is/reinforcement-learning-from-human-feedback/</li>
<li>Illustrating Reinforcement Learning from Human Feedback (RLHF) - Hugging Face, https://huggingface.co/blog/rlhf</li>
<li>Reinforcement learning from human feedback - Wikipedia, https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback</li>
<li>How ChatGPT actually works - AssemblyAI, https://www.assemblyai.com/blog/how-chatgpt-actually-works</li>
<li>60+ ChatGPT Facts And Statistics You Need to Know in 2025 - InvGate’s Blog, https://blog.invgate.com/chatgpt-statistics</li>
<li>ChatGPT - Wikipedia, https://en.wikipedia.org/wiki/ChatGPT</li>
<li>ChatGPT Is Here: These 2 Tech Stocks Are Set to Be Massively Disrupted, https://www.fool.ca/2022/12/15/chatgpt-is-here-these-2-tech-stocks-are-set-to-be-massively-disrupted/</li>
<li>Generative AI Startups Raise Hundreds of Millions in Funding - DeepLearning.AI, https://www.deeplearning.ai/the-batch/generative-ai-startups-raise-hundreds-of-millions-in-funding/</li>
<li>Generative AI | Dealroom.co, https://dealroom.co/guides/generative-ai</li>
<li>A Running Timeline of AI Investments, Funding Rounds - The Fashion Law, https://www.thefashionlaw.com/generative-ai-investment-and-ma-tracker/</li>
<li>Ethical use of ChatGPT in education—Best practices to … - Frontiers, https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2024.1465703/full</li>
<li>ChatGPT, Artificial Intelligence, and Academic Integrity, https://oai.missouri.edu/chatgpt-artificial-intelligence-and-academic-integrity/</li>
<li>ChatGPT: Educational friend or foe? - Brookings Institution, https://www.brookings.edu/articles/chatgpt-educational-friend-or-foe/</li>
<li>The state of AI in 2022—and a half decade in review - McKinsey, https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai-in-2022-and-a-half-decade-in-review</li>
<li>TSMC: The Unseen Architect of the AI Revolution and Global Tech …, https://markets.financialcontent.com/wral/article/tokenring-2025-10-4-tsmc-the-unseen-architect-of-the-ai-revolution-and-global-tech-dominance</li>
<li>The White House - Center for American Progress, https://www.americanprogress.org/article/taking-further-agency-action-on-ai/the-white-house-chapter/</li>
<li>AI Act | Shaping Europe’s digital future - European Union, https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai</li>
<li>Rising Global Regulation for Artificial Intelligence (UPDATED …, https://www.jdsupra.com/legalnews/rising-global-regulation-for-artificial-7325796/</li>
<li>Network architecture for global AI policy - Brookings Institution, https://www.brookings.edu/articles/network-architecture-for-global-ai-policy/</li>
<li>AI Index | Stanford HAI, https://hai.stanford.edu/ai-index</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>