<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:인공지능의 거인들 (The Greats of AI)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>인공지능의 거인들 (The Greats of AI)</h1>
                    <nav class="breadcrumbs"><a href="../index.html">Home</a> / <a href="index.html">기사 (Articles)</a> / <span>인공지능의 거인들 (The Greats of AI)</span></nav>
                </div>
            </header>
            <article>
                <h1>인공지능의 거인들 (The Greats of AI)</h1>
<h2>1.  인공지능 분야의 ’영향력’에 대한 다차원적 고찰</h2>
<p>인공지능(AI) 분야에서 ’위대한 연구자’를 선정하는 작업은 단순히 학술적 인용 횟수나 수상 경력만으로 평가할 수 없는 복합적인 과제이다. AI 분야의 영향력은 단일한 척도로 측정되지 않으며, 시대의 흐름과 기술의 패러다임 변화에 따라 그 의미와 형태를 달리해왔다. 따라서 본 보고서는 AI 분야의 가장 영향력 있는 연구자 100인을 선정하기 위해, ’영향력’을 다차원적 프레임워크로 재정의하고 이를 기준으로 인물들을 조망하고자 한다. 이 프레임워크는 다음과 같은 핵심 요소들로 구성된다.</p>
<p>첫째, **개념적 공헌(Conceptual Contribution)**이다. 이는 AI라는 학문 분야의 근간을 이루는 새로운 패러다임, 이론, 혹은 문제 정의를 제시한 공헌을 의미한다. 기호주의와 연결주의의 대립, 강화 학습 문제의 공식화와 같이, 후대 연구자들이 사고하는 틀 자체를 제공한 지적 기여가 여기에 해당한다.</p>
<p>둘째, **기술적 돌파구(Technical Breakthroughs)**이다. 이는 특정 문제 해결에 결정적인 역할을 하거나, 새로운 연구 분야를 개척한 구체적인 알고리즘, 모델 아키텍처, 또는 기술의 발명을 의미한다. 역전파(Backpropagation) 알고리즘의 대중화, 컨볼루션 신경망(CNN), 그리고 트랜스포머(Transformer) 아키텍처 등이 대표적인 사례이다.</p>
<p>셋째, **학술적 영향력(Scholarly Impact)**이다. 이는 h-index와 같은 인용 지수, NeurIPS, ICML, ICLR 등 최고 수준의 학회에서의 논문 발표, 그리고 ACM A.M. 튜링상(Turing Award)과 같은 권위 있는 수상 실적을 통해 객관적으로 측정되는 영향력이다. 이는 해당 연구자의 아이디어가 학계 내에서 얼마나 널리 수용되고 확장되었는지를 보여주는 지표이다.</p>
<p>넷째, **생태계 구축(Ecosystem Building)**이다. 이는 단순히 개인의 연구 성과를 넘어, 수많은 다른 연구자들의 작업을 가능하게 하고 가속화하는 기반을 마련한 공헌이다. ImageNet과 같은 대규모 데이터셋 구축, TensorFlow나 PyTorch와 같은 오픈소스 소프트웨어 프레임워크 개발, 그리고 FAIR, DeepMind와 같은 선도적인 연구소 설립, Coursera와 같은 교육 플랫폼을 통한 지식 확산 등이 여기에 포함된다.</p>
<p>다섯째, **산업 및 사회적 파급력(Industrial and Societal Impact)**이다. 이는 학술적 연구가 실제 산업 현장에 적용되어 수백만 명의 삶에 영향을 미치는 제품과 서비스로 구현되거나, AI 기술에 대한 대중적 담론과 정부 정책 수립에 영향을 미친 경우를 말한다.</p>
<p>여섯째, **지적 리더십과 비판적 담론(Intellectual Leadership and Critical Discourse)**이다. 이는 AI 기술의 발전 방향을 제시하고, 잠재적 위험을 경고하며, 윤리적 가이드라인과 안전장치를 제안하는 역할을 의미한다. 인과추론의 중요성을 역설하거나, AI의 편향성과 공정성 문제를 제기하는 연구자들이 여기에 해당한다.</p>
<p>이러한 다차원적 접근을 통해 보면, AI 분야의 ‘영향력’ 개념 자체가 시대에 따라 진화해왔음을 알 수 있다. 초기 AI의 영향력은 주로 대학 연구실과 학술 출판물을 통해 형성되었다. 존 매카시나 마빈 민스키와 같은 개척자들은 그들이 양성한 제자들과 발표한 논문을 통해 영향력을 행사했다. 그러나 딥러닝 시대에 이르러 새로운 영향력 모델이 등장했다. 제프리 힌튼, 얀 르쿤, 요슈아 벤지오와 같은 인물들은 학계에서의 역할과 산업계와의 긴밀한 협력을 병행했다. 그들의 영향력은 단순히 논문에서 비롯된 것이 아니라, 그들의 방법론이 거둔 실질적인 성공과 그로 인해 촉발된 막대한 기업 투자로부터 증폭되었다. 제프 딘이나 데미스 하사비스와 같은 인물들은 여기서 한 걸음 더 나아가, 막대한 컴퓨팅 자원과 거대 산업 연구팀을 지휘함으로써 영향력을 행사하는 새로운 유형을 보여준다. 이러한 변화는 기술적 돌파구(딥러닝)가 경제적 가치를 창출하고, 이것이 기업의 투자를 유치하며, 결과적으로 자원과 인재를 집중시켜 영향력의 중심을 순수 학계에서 학계-산업 복합체로 이동시키는 인과적 연쇄 반응을 명확히 보여준다.</p>
<p>더 나아가, 현대 AI 분야에서는 새로운 모델을 창조하는 ’구축자(builders)’와 그 모델의 사회적 영향과 한계를 분석하는 ‘비판자/맥락화자(critics/contextualizers)’ 사이에 긴장감이 감돌면서도 공생 관계가 형성되고 있다. 트랜스포머 아키텍처에 기반한 GPT-3와 같은 모델의 성공은 ’구축자’의 영향력을 대표한다. 그러나 이러한 성공은 필연적으로 팀닛 게브루와 같은 연구자들이 주도하는 편향, 공정성, 안전성에 대한 비판적 연구라는 반작용을 낳았다. 이러한 ’비판자’들의 연구는 ’구축자’들의 연구 방향에 직접적인 영향을 미쳐, 후속 모델 개발 시 안전, 정렬(alignment), 윤리 문제를 고려하도록 강제한다. 따라서 진정한 영향력은 새로운 능력을 창조하는 것뿐만 아니라, 그 능력이 개발되는 과정의 제약 조건과 목표를 설정하는 것까지 포함한다. 이 둘은 서로 인과적으로 얽혀 있으며, 온전한 영향력 평가는 이 두 측면을 모두 포괄해야 한다.</p>
<h2>2.  제1부: 개척자들 - 인공지능의 초석</h2>
<p>인공지능의 역사는 1950년대 중반, 인간의 지능을 기계로 구현하려는 소수의 선구자들에 의해 시작되었다. 이 시기(약 1950-1985년)는 주로 ‘기호주의 AI(Symbolic AI)’ 패러다임이 지배했으며, 지능을 논리적 기호의 조작으로 간주하는 접근법이 주를 이루었다. 이 시대의 거인들은 AI를 하나의 학문 분야로 정의하고 그 기초를 다졌다.</p>
<h3>2.1  논리와 탐색의 시대</h3>
<p>이 시대를 연 가장 중요한 인물은 **존 매카시(John McCarthy)**이다. 그는 1956년 다트머스 워크숍을 조직하며 ’인공지능(Artificial Intelligence)’이라는 용어를 최초로 제안하고 명명한 인물로, AI 분야의 탄생에 결정적인 역할을 했다. 그의 공헌은 용어 창안에 그치지 않는다. 그는 AI 연구를 위한 최초의 고급 프로그래밍 언어인 Lisp를 발명하여, 이후 수십 년간 AI 연구의 표준 도구로 자리매김하게 했다. 그의 영향력은 AI라는 분야의 정체성을 확립하고 연구의 기틀을 마련했다는 점에서 근본적이고 정의적이다.</p>
<p>매카시와 함께 MIT AI 연구소를 공동 설립한 <strong>마빈 민스키(Marvin Minsky)</strong> 역시 빼놓을 수 없는 개척자이다. 그는 신경망, 계산기하학 등 다방면에 걸쳐 선구적인 연구를 수행했다. 특히 시모어 페퍼트(Seymour Papert)와 공저한 저서 『퍼셉트론(Perceptrons)』은 AI 역사상 가장 큰 논쟁을 불러일으킨 책 중 하나이다. 이 책은 단층 퍼셉트론의 한계를 수학적으로 증명하며 초기 연결주의 연구에 치명타를 가했다. 이 책은 단순한 기술적 비판을 넘어, 이후 10년 이상 AI 연구의 방향을 기호주의로 집중시키는 결정적인 계기가 되었다. 이는 비판적 분석이 어떻게 한 분야의 경로를 설정할 수 있는지를 보여주는 강력한 사례이다.</p>
<p>한편, 카네기 멜런 대학의 **앨런 뉴웰(Allen Newell)**과 **허버트 사이먼(Herbert A. Simon)**은 지능을 기호 조작 과정으로 형식화하는 데 중추적인 역할을 했다. 그들이 개발한 ’논리 이론가(Logic Theorist)’와 ’일반 문제 해결사(General Problem Solver, GPS)’는 인간의 문제 해결 과정을 컴퓨터 프로그램으로 모델링하려는 최초의 시도였다. 이들의 연구는 “물리적 기호 시스템 가설(physical symbol system hypothesis)”—지능적 행위를 위해서는 물리적 기호 시스템이 필요충분조건이라는 주장—을 AI의 지배적인 초기 패러다임으로 확립시켰다.</p>
<h3>2.2  초기 연결주의와 그 한계</h3>
<p>기호주의가 AI의 주류를 형성하던 시기, 다른 한편에서는 뇌의 신경망 구조에서 영감을 받은 연결주의(Connectionism) 접근법이 태동하고 있었다. 이 흐름의 중심에는 **프랭크 로젠블랫(Frank Rosenblatt)**이 있었다. 그는 1958년 ’퍼셉트론(Perceptron)’이라는 초기 신경망 모델과 학습 알고리즘을 발명했다. 퍼셉트론은 입력 패턴을 학습하여 두 개의 클래스로 분류할 수 있는 최초의 모델로, 연결주의의 첫 번째 물결을 상징하는 기념비적인 성과였다.</p>
<p>그러나 앞서 언급한 민스키와 페퍼트의 『퍼셉트론』은 이 흐름에 찬물을 끼얹었다. 그들은 퍼셉트론이 XOR과 같은 선형적으로 분리 불가능한 문제는 해결할 수 없음을 수학적으로 증명했다. 이 비판은 당시 연구 자금을 지원하던 기관들에 큰 영향을 미쳤고, 결과적으로 연결주의 연구에 대한 지원이 급감하는 첫 번째 ’AI 겨울(AI Winter)’을 초래하는 촉매제가 되었다.</p>
<p>이러한 기호주의와 초기 연결주의 사이의 지적 분열은 단순한 기술적 견해 차이가 아니었다. 이는 지능의 본질에 대한 깊은 철학적 대립—하향식 논리(top-down logic) 대 상향식 창발 패턴(bottom-up emergent patterns)—을 반영했다. 1960년대와 70년대에는 명료한 논리와 증명 가능한 정리를 갖춘 기호주의가 학문적으로 더 존중받고 이해하기 쉬웠다. 민스키와 페퍼트의 책은 이러한 분위기 속에서 단순 퍼셉트론의 한계에 대한 형식적이고 수학적인 논거를 제공함으로써 파괴적인 효과를 발휘했다. 이로 인해 DARPA와 같은 주요 연구 지원 기관들은 연결주의에서 기호주의로 자원을 재분배했고, AI 겨울을 촉발했다. 그 결과, 잠재력 있는 연구 분야가 수년간 잠복기에 들어갔다. 이는 단순히 데이터나 컴퓨팅 파워의 부족 때문만이 아니라, 패러다임 수준의 지적 봉쇄 때문이었다. 이는 영향력이 특정 연구 경로를 막는 ‘부정적(negative)’ 방식으로도 작용할 수 있음을 보여주는 역사적 교훈이다.</p>
<h2>3.  제2부: 딥러닝 혁명의 설계자들</h2>
<p>첫 번째 AI 겨울 이후 수십 년간 침체기를 겪었던 연결주의는 1980년대 중반부터 서서히 부활하기 시작했다. 그리고 2010년대에 이르러 ’딥러닝(Deep Learning)’이라는 이름으로 AI 분야의 모든 것을 바꾸어 놓는 혁명을 일으켰다. 이 혁명의 중심에는 끈기 있게 연결주의의 가능성을 믿고 연구를 지속해 온 세 명의 설계자가 있었다.</p>
<h3>3.1  연결주의의 부활과 역전파</h3>
<p>딥러닝 혁명의 정신적 지주로 불리는 **제프리 힌튼(Geoffrey Hinton)**은 연결주의의 부활을 이끈 핵심 인물이다. 그는 데이비드 럼멜하트, 로널드 윌리엄스와 함께 다층 신경망을 효과적으로 학습시킬 수 있는 역전파(Backpropagation) 알고리즘을 대중화하는 데 결정적인 역할을 했다. 역전파는 출력층의 오차를 입력층 방향으로 거꾸로 전파하며 각 연결의 가중치(weight)를 점진적으로 조정하는 방법으로, 오늘날 거의 모든 딥러닝 모델의 학습에 사용되는 기본 원리이다. 그의 연구는 볼츠만 머신(Boltzmann Machines), 제한된 볼츠만 머신(RBMs) 등으로 이어졌으며, AI 겨울 동안에도 신경망에 대한 신념을 굽히지 않은 그의 끈기는 딥러닝 시대를 연 원동력이 되었다. 역전파 알고리즘의 가중치 업데이트 규칙은 종종 다음과 같이 표현된다.</p>
<p><span class="math math-display">
\Delta w_{ij}(t+1) = \eta \frac{\partial E}{\partial w_{ij}} + \alpha \Delta w_{ij}(t)
</span><br />
여기서 <span class="math math-inline">\eta</span>는 학습률, <span class="math math-inline">E</span>는 오차, <span class="math math-inline">\alpha</span>는 이전 가중치 변화량을 반영하는 모멘텀(momentum) 항으로, 힌튼의 연구와 깊은 관련이 있다.</p>
<h3>3.2  컨볼루션 신경망과 컴퓨터 비전</h3>
<p>컴퓨터 비전 분야의 혁명은 **얀 르쿤(Yann LeCun)**의 손에서 시작되었다. 그는 1990년대에 컨볼루션 신경망(Convolutional Neural Networks, CNNs)을 개발하고, 이를 적용하여 손글씨 숫자를 인식하는 LeNet-5를 선보였다. CNN은 인간의 시신경 구조에서 영감을 받은 아키텍처로, 이미지의 지역적 특징을 추출하는 ’컨볼루션 필터’와 위치 변화에 덜 민감하게 만드는 ‘풀링(pooling)’ 개념을 핵심으로 한다. 특히, 동일한 필터를 이미지의 모든 영역에 적용하는 ‘가중치 공유(shared weights)’ 방식은 학습해야 할 파라미터 수를 획기적으로 줄여, 이미지와 같은 고차원 데이터를 처리하는 데 매우 효율적이다. LeNet-5는 현대 컴퓨터 비전 모델들의 원형이 되었으며, 르쿤의 연구는 딥러닝이 이미지 인식 문제를 해결할 수 있는 강력한 도구임을 입증했다.</p>
<h3>3.3  시퀀스 모델링과 언어</h3>
<p>자연어 처리(NLP)와 같은 순차적 데이터(sequential data)를 다루는 분야에서는 **요슈아 벤지오(Yoshua Bengio)**의 공헌이 지대하다. 그는 2000년대 초, 단어를 벡터 공간의 한 점으로 표현하는 ‘분산 표현(distributed representation)’, 즉 워드 임베딩(word embedding) 개념을 도입한 신경망 언어 모델을 제안했다. 이는 단어의 의미를 벡터로 학습함으로써 단어 간의 의미적, 문법적 관계를 포착할 수 있게 했고, 이후 NLP 분야의 모든 모델에 적용되는 기본 아이디어가 되었다.</p>
<p>한편, 긴 시퀀스 데이터 처리의 난제였던 ’장기 의존성 문제(long-term dependency problem)’를 해결한 것은 **위르겐 슈미트후버(Jürgen Schmidhuber)**와 그의 제자 **제프 호흐라이터(Sepp Hochreiter)**였다. 그들이 1997년에 발명한 LSTM(Long Short-Term Memory)은 내부에 ’셀 상태(cell state)’와 여러 ’게이트(gate)’를 두어 정보를 선택적으로 기억하고 잊어버릴 수 있게 설계되었다. 이 구조 덕분에 LSTM은 수십 년간 순환 신경망(RNN)의 표준 아키텍처로 군림하며 기계 번역, 음성 인식 등에서 뛰어난 성능을 보였다.</p>
<h3>3.4  혁명의 기폭제: AlexNet</h3>
<p>이러한 개별적인 연구 성과들이 한데 모여 폭발적인 혁명을 일으킨 결정적인 순간은 2012년에 찾아왔다. 제프리 힌튼의 제자였던 **알렉스 크리제브스키(Alex Krizhevsky)**와 **일리야 수츠케버(Ilya Sutskever)**는 ’AlexNet’이라는 깊은 CNN 모델을 개발하여 이미지넷 대규모 시각 인식 챌린지(ILSVRC)에 참가했다. AlexNet은 2위 팀과 압도적인 격차를 보이며 우승했고, 이는 딥러닝 시대의 개막을 알리는 ’빅뱅’과 같은 사건이었다.</p>
<p>AlexNet의 성공은 단일한 기술적 돌파구의 결과가 아니었다. 이는 세 가지 독립적인 요소의 필연적인 융합이 낳은 결과였다. 첫째, 힌튼, 르쿤, 벤지오 등이 수십 년간 발전시켜 온 역전파, CNN과 같은 알고리즘 혁신이 있었다. 둘째, 페이페이 리(Fei-Fei Li) 팀이 구축한 ImageNet이라는 대규모 표준 데이터셋이 전례 없는 규모의 학습을 가능하게 했다. 셋째, 비디오 게임 산업의 발전이 낳은 고성능 병렬 처리 장치인 GPU가 딥러닝 모델 학습에 필요한 막대한 계산을 감당할 수 있는 도구를 제공했다. AlexNet은 ’알고리즘 + 빅데이터 + GPU 컴퓨팅 = 성능의 비약적 향상’이라는 공식을 증명한 역사적 사건이었다. 따라서 ’딥러닝의 대부들’의 영향력은 단순히 그들의 알고리즘 자체에 있는 것이 아니라, 이러한 융합의 순간이 올 때까지 끈기 있게 알고리즘을 준비하고 발전시켜 온 그들의 선견지명과 인내에 있다.</p>
<h2>4.  제3부: 핵심 분야의 대가들 - 지능의 다각적 탐구</h2>
<p>딥러닝 혁명 이후, AI는 자연어 처리, 컴퓨터 비전, 강화 학습 등 여러 핵심 분야에서 눈부신 발전을 거듭했다. 각 분야에서는 패러다임을 바꾸는 아이디어를 제시하고 기술의 최전선을 이끈 대가들이 등장했다. 이들은 지능의 다양한 측면을 탐구하며 AI의 가능성을 확장했다.</p>
<h3>4.1  자연어 처리</h3>
<p>자연어 처리(NLP) 분야는 2017년 구글 연구팀이 발표한 한 편의 논문으로 인해 완전히 새로운 시대를 맞이했다. **아시시 바스와니(Ashish Vaswani)**를 비롯한 8명의 저자가 발표한 “Attention Is All You Need“는 ’트랜스포머(Transformer)’라는 새로운 아키텍처를 제시했다. 트랜스포머는 LSTM과 같은 순환적 구조를 완전히 배제하고, ‘셀프 어텐션(self-attention)’ 메커니즘만을 사용하여 문장 내 단어 간의 관계를 파악한다. 이는 모델의 병렬 처리를 극대화하여 이전에는 불가능했던 규모의 모델(BERT, GPT 등)을 학습시키는 것을 가능하게 했다. 트랜스포머의 핵심인 셀프 어텐션 메커니즘의 계산은 다음과 같이 표현된다.</p>
<p><span class="math math-display">
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
</span><br />
여기서 <span class="math math-inline">Q</span>, <span class="math math-inline">K</span>, <span class="math math-inline">V</span>는 각각 쿼리(Query), 키(Key), 밸류(Value) 행렬을 의미한다. 트랜스포머 이전 시대에도 NLP 분야를 이끈 선구자들이 있었다. 스탠퍼드 NLP 그룹을 이끌며 의존 구문 분석 등에서 큰 업적을 남긴 **크리스토퍼 매닝(Christopher Manning)**과, IBM에서 통계적 기계 번역의 기틀을 닦은 **프레더릭 옐리네크(Frederick Jelinek)**는 현대 NLP의 통계적 접근법을 확립한 공로자들이다.</p>
<h3>4.2  컴퓨터 비전</h3>
<p>컴퓨터 비전 분야에서 **페이페이 리(Fei-Fei Li)**의 영향력은 특정 알고리즘의 발명을 넘어선다. 그녀는 1,400만 장 이상의 레이블된 이미지로 구성된 ‘ImageNet’ 데이터셋 구축 프로젝트를 주도했다. ImageNet은 딥러닝 모델을 훈련하고 평가할 수 있는 표준적인 대규모 벤치마크를 제공함으로써, AlexNet의 등장을 가능하게 한 결정적인 기폭제 역할을 했다. 이는 생태계 구축이 때로는 단일 알고리즘 발명보다 더 큰 영향력을 가질 수 있음을 보여주는 사례이다.</p>
<p>학계에서는 **지텐드라 말릭(Jitendra Malik)**과 **피에트로 페로나(Pietro Perona)**와 같은 리더들이 수많은 후학을 양성하며 이미지 분할, 객체 인식 분야의 기초를 다졌다. 한편, 마이크로소프트 리서치와 페이스북 AI 리서치에서 활동한 **카이밍 허(Kaiming He)**는 심층 신경망의 ’기울기 소실 문제(vanishing gradient problem)’를 해결한 ResNet(Residual Network) 아키텍처를 개발하여 딥러닝 모델을 수백, 수천 계층까지 깊게 쌓을 수 있는 길을 열었다. ResNet의 핵심 아이디어는 입력 <span class="math math-inline">x</span>를 출력에 그대로 더해주는 ‘스킵 커넥션(skip connection)’ 또는 ‘아이덴티티 매핑(identity mapping)’ <span class="math math-inline">F(x) + x</span>으로, 이는 층이 깊어져도 학습이 원활하게 이루어지도록 돕는다.</p>
<h3>4.3  강화 학습</h3>
<p>강화 학습(Reinforcement Learning, RL)은 에이전트가 환경과의 상호작용을 통해 보상을 최대화하는 행동을 학습하는 패러다임이다. 이 분야의 이론적 기틀을 마련한 인물은 **리처드 서튼(Richard Sutton)**과 **앤드류 바르토(Andrew Barto)**이다. 그들이 공저한 교과서 『강화 학습(Reinforcement Learning: An Introduction)』은 이 분야의 모든 연구자들에게 바이블로 여겨지며, 벨만 방정식, 시간차 학습(Temporal-Difference learning)과 같은 핵심 개념을 체계적으로 정립했다.</p>
<p>이론적 토대 위에서 강화 학습의 경이로운 가능성을 현실로 증명한 것은 딥마인드(DeepMind)의 연구자들이었다. **데이비드 실버(David Silver)**는 세계 최고 수준의 바둑 기사를 꺾은 AlphaGo의 개발을 이끌었으며, 이후 규칙조차 알려주지 않은 상태에서 스스로 학습하여 체스, 쇼기, 바둑을 모두 정복한 AlphaZero, 그리고 단백질 구조 예측 문제를 해결한 AlphaFold에 이르기까지, 딥러닝과 강화 학습을 결합한 심층 강화 학습의 역사를 새로 썼다. 또한, **볼로디미르 민(Volodymyr Mnih)**은 심층 신경망과 Q-러닝을 결합한 DQN(Deep Q-Network)을 개발하여, 픽셀 화면만 보고 아타리 게임을 인간 수준 이상으로 플레이하는 AI를 선보이며 심층 강화 학습 시대의 서막을 열었다.</p>
<h3>4.4  생성 모델과 비지도 학습</h3>
<p>생성 모델(Generative Models)은 데이터의 잠재적 분포를 학습하여 새로운 데이터를 생성하는 모델이다. 이 분야에 혁신을 가져온 인물은 **이안 굿펠로우(Ian Goodfellow)**이다. 그는 2014년, 생성자(Generator)와 판별자(Discriminator)가 서로 경쟁하며 학습하는 독창적인 구조의 ’생성적 적대 신경망(Generative Adversarial Networks, GANs)’을 발명했다. GAN은 실제와 구분이 거의 불가능한 고품질 이미지를 생성해내며 이미지 합성 분야에 혁명을 일으켰다. GAN의 학습은 다음과 같은 minimax 게임 목표 함수를 통해 이루어진다.</p>
<p><span class="math math-display">
\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{\text{data}}(x)} + \mathbb{E}_{z \sim p_z(z)}
</span></p>
<h3>4.5  인과추론과 베이즈 네트워크</h3>
<p>딥러닝이 데이터의 ’상관관계(correlation)’를 학습하는 데 탁월한 능력을 보이는 반면, ’인과관계(causation)’를 추론하는 데에는 근본적인 한계를 가진다. 이러한 주류 딥러닝에 대한 중요한 대안적 시각을 제공한 인물이 바로 **유디 펄(Judea Pearl)**이다. 그는 확률적, 인과적 추론을 위한 수학적 체계를 개발한 공로로 튜링상을 수상했다. 펄이 정립한 베이즈 네트워크(Bayesian Networks)와 do-연산자(<span class="math math-inline">P(Y \vert do(X))</span>는 ’X를 관찰했을 때 Y의 확률’을 넘어 ’X라는 중재를 가했을 때 Y에 미치는 영향’을 계산할 수 있게 해준다.</p>
<p>현재 주류 AI 커뮤니티에서 펄의 영향력은 딥러닝 대가들에 비해 상대적으로 작아 보일 수 있다. 그러나 “상관관계는 인과관계가 아니다“라는 명제가 복잡한 AI 시스템에서 점점 더 중요한 문제로 부각되면서 그의 연구는 재조명받고 있다. GPT-3와 같은 현재의 대규모 모델들은 통계적 상관관계의 대가이지만, 진정한 의미의 원인과 결과를 이해하지는 못한다. AI가 견고한 추론, 과학적 발견, 진정한 일반화와 같은 다음 단계로 나아가기 위해서는 단순히 <span class="math math-inline">P(Y|X)</span>를 모델링하는 것을 넘어 <span class="math math-inline">P(Y|do(X))</span>를 다룰 수 있어야 한다. 따라서 펄의 인과추론 미적분학과 딥러닝의 패턴 인식 능력을 성공적으로 결합하는 연구자들(예: 요슈아 벤지오의 최근 ‘시스템 2’ AI 연구)이 향후 10년간 가장 영향력 있는 인물로 부상할 가능성이 높다. 펄의 연구는 이제 막 주류 AI 분야에서 싹트기 시작한 ’영향력의 씨앗’과 같다.</p>
<h2>5.  제4부: 산업과 학계의 교량자들 - 이론을 현실로</h2>
<p>학계에서 탄생한 혁신적인 아이디어들이 전 세계적인 영향력을 갖기 위해서는 이를 대규모로 구현하고, 제품화하며, 확산시키는 과정이 필수적이다. 이 과정에서 산업계와 학계의 경계를 넘나들며 이론을 현실로 바꾸는 ’교량자(bridge builders)’들의 역할은 절대적이다. 이들은 거대한 인프라를 구축하고, 최고의 팀을 이끌며, 기술의 미래 비전을 제시함으로써 AI의 발전을 가속화했다.</p>
<h3>5.1  대규모 AI 시스템의 설계자</h3>
<p>구글 AI를 이끌고 있는 **제프 딘(Jeff Dean)**은 이러한 ’시스템 수준의 영향력(systems-level influence)’을 상징하는 인물이다. 그의 영향력은 특정 알고리즘이나 논문보다는, 구글이 딥러닝을 전례 없는 규모로 적용할 수 있게 만든 기반 시스템을 구축한 데서 비롯된다. 그는 MapReduce, Bigtable, Spanner와 같은 대규모 분산 시스템을 설계했으며, 이는 구글의 검색, 광고, 클라우드 서비스의 근간이 되었다. AI 분야에서는 딥러닝 모델을 여러 서버에 분산하여 학습시키는 DistBelief 시스템과, 오늘날 가장 널리 사용되는 딥러닝 프레임워크 중 하나인 TensorFlow의 개발을 주도했다. 그의 공헌은 종종 눈에 잘 띄지 않지만, 현대 대규모 AI의 실현에 있어 필수불가결한 토대를 제공했다.</p>
<h3>5.2  비전과 리더십</h3>
<p>딥마인드의 공동 창업자이자 CEO인 **데미스 하사비스(Demis Hassabis)**는 과학적 비전과 기업가적 실행력을 결합하여 영향력을 발휘하는 대표적인 인물이다. 그는 ’범용 인공지능(AGI) 개발’이라는 명확하고 야심 찬 연구 목표를 설정하고, 이를 달성하기 위해 신경과학, 컴퓨터 과학, 공학 등 다양한 분야의 최고 인재들을 모아 독보적인 연구 환경을 조성했다. 그의 리더십 아래 딥마인드는 AlphaGo와 AlphaFold와 같은 인류의 지적, 과학적 난제를 해결하는 기념비적인 성과들을 연이어 내놓았다. 하사비스의 영향력은 단순히 연구 결과를 내는 것을 넘어, AI가 인류의 가장 중요한 문제들을 해결할 수 있다는 강력한 비전을 제시하고 그 가능성을 실질적으로 증명해 보였다는 데 있다.</p>
<h3>5.3  AI의 민주화</h3>
<p>**앤드류 응(Andrew Ng)**은 AI 기술을 소수의 전문가 집단에서 대중으로 확산시키는 ’AI의 민주화’에 가장 크게 기여한 인물이다. 그의 영향력은 이중적이다. 첫째, 그는 구글 브레인(Google Brain) 프로젝트를 공동 창립하고 바이두(Baidu)의 AI 그룹을 이끌며 산업계에서 대규모 딥러닝의 적용을 개척했다. 둘째, 그는 온라인 교육 플랫폼인 코세라(Coursera)를 공동 창립하고, 자신의 머신러닝 및 딥러닝 강좌를 통해 전 세계 수백만 명에게 AI 지식을 전파했다. 그의 교육 활동은 전 세계적인 AI 인재 풀을 형성하는 데 결정적인 역할을 했으며, 이는 새로운 아이디어나 프레임워크를 개발하는 것만큼이나 중요한 형태의 영향력이다.</p>
<p>이러한 인물들의 등장은 AI 연구의 무게 중심이 대학에서 소수의 강력한 기업 연구소(구글/딥마인드, 메타 AI, OpenAI 등)로 부분적으로 이동했음을 시사한다. 최첨단 AI 모델을 구축하고 훈련하는 데 필요한 컴퓨팅 자원은 이제 대부분의 대학 연구실이 감당할 수 있는 수준을 훨씬 넘어섰다. 이는 제프 딘이나 데미스 하사비스와 같은 리더들이 이끄는 기업 연구소에 막대한 이점을 제공한다. 그 결과, 학계의 최고 인재들이 산업계로 이동하는 ‘두뇌 유출(brain drain)’ 현상이 나타나는 동시에, 막대한 자본이 AI 분야로 유입되는 ‘자원 주입(resource infusion)’ 현상이 발생했다. 이러한 자원의 불균형은 AI 연구의 양극화를 초래할 수 있다. 산업계는 대규모 경험적 엔지니어링에 집중하는 반면, 학계는 막대한 컴퓨팅 자원이 필요 없는 이론적, 기초적, 혹은 비판적 연구로 방향을 전환해야 할 수도 있다. 이는 미래 ’학술적 영향력’의 본질이 어떻게 적응하고 변화해야 하는지에 대한 중요한 질문을 제기한다.</p>
<h2>6.  결론: 영향력의 계보와 미래 전망</h2>
<p>본 보고서는 인공지능 분야의 영향력을 개념적 공헌, 기술적 돌파구, 학술적 영향, 생태계 구축, 사회적 파급력, 그리고 지적 리더십이라는 다차원적 렌즈를 통해 분석했다. 이 분석을 통해 AI의 역사는 단순히 기술의 발전사가 아니라, 영향력의 본질과 중심이 끊임없이 변화해 온 지적 계보의 역사임이 드러난다.</p>
<p>초기 개척자들은 대학 연구실에서 AI의 기본 개념과 패러다임을 정립하며 영향력을 행사했다. 딥러닝 혁명의 설계자들은 학계와 산업계의 경계에서 수십 년간의 끈질긴 연구를 통해 새로운 시대를 열었다. 이후 등장한 핵심 분야의 대가들은 각자의 영역에서 기술의 깊이를 더했으며, 산업계의 교량자들은 이러한 기술을 전 지구적 규모로 확장하고 민주화했다. 영향력의 중심은 순수 대학 연구실에서 시작하여, 대학-기업 간 협력 모델을 거쳐, 이제는 막대한 자원을 동원하는 기업 주도의 ‘문샷(moonshot)’ 프로젝트로 이동하는 경향을 보인다.</p>
<p>이러한 영향력의 계보를 바탕으로 미래를 전망할 때, 차세대 영향력 있는 인물들은 다음과 같은 영역에서 등장할 가능성이 높다.</p>
<p>첫째, <strong>AI와 과학의 융합(AI + Science)</strong> 분야이다. AlphaFold가 단백질 구조 예측 문제를 해결했듯이, 생물학, 재료 과학, 기후 변화 모델링 등 인류의 근본적인 과학적 난제를 해결하기 위해 AI를 적용하는 연구자들이 큰 영향력을 발휘할 것이다.</p>
<p>둘째, <strong>신경-기호주의 AI(Neuro-symbolic AI)</strong> 분야이다. 딥러닝의 패턴 인식 능력과 기호주의의 논리적 추론 및 인과관계 이해 능력을 성공적으로 통합하여, 보다 견고하고 설명 가능하며 일반화 능력이 뛰어난 AI를 개발하는 연구자들이 차세대 리더가 될 것이다.</p>
<p>셋째, <strong>AI 윤리와 거버넌스(AI Ethics and Governance)</strong> 분야이다. AI 기술이 사회에 깊숙이 통합됨에 따라, 기술의 안전성, 공정성, 투명성을 보장하고 사회적 합의에 기반한 통제 프레임워크를 개발하는 철학자, 사회과학자, 컴퓨터 과학자들의 역할과 영향력은 기하급수적으로 커질 것이다.</p>
<p>넷째, <strong>하드웨어와 시스템(Hardware and Systems)</strong> 분야이다. 현재의 딥러닝 패러다임은 막대한 에너지와 데이터를 소비한다. 이러한 한계를 극복하고 새로운 AI 패러다임을 가능하게 할 차세대 컴퓨팅 하드웨어(예: 뉴로모픽 칩, 광자 컴퓨팅)를 개발하는 공학자 및 과학자들의 영향력은 아무리 강조해도 지나치지 않다.</p>
<p>결론적으로, 인공지능의 역사는 소수의 거인들이 놓은 초석 위에 수많은 연구자들이 벽돌을 쌓아 올린 거대한 건축물과 같다. 미래의 AI는 과거와 현재의 모든 지적 유산을 바탕으로, 더욱 복합적이고 다학제적인 방향으로 진화할 것이며, 그 과정에서 새로운 형태의 영향력을 지닌 거인들이 계속해서 등장할 것이다.</p>
<h2>7.  부록: 100인 연구자 종합 목록</h2>
<table><thead><tr><th>No.</th><th>이름 (Name)</th><th>주요 소속 (Primary Affiliation)</th><th>핵심 공헌 분야 (Area of Contribution)</th><th>대표적 공헌/개념 (Key Contribution/Concept)</th><th>대표 수상/업적 (Notable Award/Achievement)</th></tr></thead><tbody>
<tr><td>1</td><td>Geoffrey Hinton</td><td>University of Toronto, Google</td><td>Deep Learning, Neural Networks</td><td>Backpropagation, Boltzmann Machines, Capsules</td><td>2018 ACM A.M. Turing Award</td></tr>
<tr><td>2</td><td>Yann LeCun</td><td>New York University, Meta AI</td><td>Computer Vision, Deep Learning</td><td>Convolutional Neural Networks (CNNs)</td><td>2018 ACM A.M. Turing Award</td></tr>
<tr><td>3</td><td>Yoshua Bengio</td><td>Mila, Université de Montréal</td><td>Deep Learning, NLP</td><td>Neural Language Models, Attention</td><td>2018 ACM A.M. Turing Award</td></tr>
<tr><td>4</td><td>Judea Pearl</td><td>UCLA</td><td>Causal Inference, Probabilistic Reasoning</td><td>Bayesian Networks, do-calculus (<span class="math math-inline">P(Y \vert do(X))</span>)</td><td>2011 ACM A.M. Turing Award</td></tr>
<tr><td>5</td><td>Jeff Dean</td><td>Google</td><td>Large-Scale Distributed Systems</td><td>MapReduce, TensorFlow, DistBelief</td><td>Member of NAE and AAAS</td></tr>
<tr><td>6</td><td>Fei-Fei Li</td><td>Stanford University</td><td>Computer Vision, AI for Healthcare</td><td>ImageNet Dataset</td><td>Member of NAE and NAM</td></tr>
<tr><td>7</td><td>David Silver</td><td>DeepMind</td><td>Reinforcement Learning</td><td>AlphaGo, AlphaZero, AlphaFold</td><td>2019 ACM Prize in Computing</td></tr>
<tr><td>8</td><td>Ashish Vaswani</td><td>Adept AI Labs, ex-Google</td><td>Natural Language Processing</td><td>Transformer Architecture (<span class="math math-inline">\text{Attention}(Q,K,V)</span>)</td><td>-</td></tr>
<tr><td>9</td><td>Ian Goodfellow</td><td>Apple, ex-Google</td><td>Generative Models</td><td>Generative Adversarial Networks (GANs)</td><td>MIT TR35</td></tr>
<tr><td>10</td><td>Richard Sutton</td><td>University of Alberta, DeepMind</td><td>Reinforcement Learning</td><td>Temporal-Difference Learning, RL Foundations</td><td>Fellow of the Royal Society</td></tr>
<tr><td>11</td><td>Demis Hassabis</td><td>DeepMind</td><td>AI, Neuroscience</td><td>AlphaGo, AlphaFold, AGI Vision</td><td>Fellow of the Royal Society</td></tr>
<tr><td>12</td><td>Andrew Ng</td><td>Landing AI, Coursera</td><td>Deep Learning, Online Education</td><td>Google Brain, Coursera Machine Learning Course</td><td>-</td></tr>
<tr><td>13</td><td>John McCarthy</td><td>Stanford University</td><td>Symbolic AI, Programming Languages</td><td>Lisp, Coined “Artificial Intelligence”</td><td>1971 ACM A.M. Turing Award</td></tr>
<tr><td>14</td><td>Marvin Minsky</td><td>MIT</td><td>Symbolic AI, Neural Networks</td><td>Perceptrons, Frames, Society of Mind</td><td>1969 ACM A.M. Turing Award</td></tr>
<tr><td>15</td><td>Allen Newell</td><td>Carnegie Mellon University</td><td>Cognitive Science, Symbolic AI</td><td>Logic Theorist, General Problem Solver (GPS)</td><td>1975 ACM A.M. Turing Award</td></tr>
<tr><td>16</td><td>Herbert A. Simon</td><td>Carnegie Mellon University</td><td>Cognitive Science, Economics</td><td>Bounded Rationality, Physical Symbol System</td><td>1975 ACM A.M. Turing Award, 1978 Nobel Prize in Economics</td></tr>
<tr><td>17</td><td>Ilya Sutskever</td><td>OpenAI</td><td>Deep Learning, NLP</td><td>AlexNet, Sequence-to-Sequence Models, GPT</td><td>Fellow of the Royal Society</td></tr>
<tr><td>18</td><td>Kaiming He</td><td>ex-Meta AI</td><td>Computer Vision</td><td>Residual Networks (ResNet), Mask R-CNN</td><td>-</td></tr>
<tr><td>19</td><td>Jürgen Schmidhuber</td><td>IDSIA, NNAISENSE</td><td>Recurrent Neural Networks</td><td>Long Short-Term Memory (LSTM)</td><td>-</td></tr>
<tr><td>20</td><td>Sepp Hochreiter</td><td>Johannes Kepler University Linz</td><td>Recurrent Neural Networks</td><td>Long Short-Term Memory (LSTM)</td><td>-</td></tr>
<tr><td>21</td><td>Alex Krizhevsky</td><td>-</td><td>Computer Vision, Deep Learning</td><td>AlexNet</td><td>-</td></tr>
<tr><td>22</td><td>Christopher Manning</td><td>Stanford University</td><td>Natural Language Processing</td><td>Stanford NLP Group, Dependency Parsing</td><td>-</td></tr>
<tr><td>23</td><td>Jitendra Malik</td><td>UC Berkeley, Meta AI</td><td>Computer Vision</td><td>Image Segmentation, Object Recognition</td><td>-</td></tr>
<tr><td>24</td><td>Volodymyr Mnih</td><td>DeepMind</td><td>Reinforcement Learning</td><td>Deep Q-Networks (DQN)</td><td>-</td></tr>
<tr><td>25</td><td>Frank Rosenblatt</td><td>Cornell University</td><td>Early Neural Networks</td><td>The Perceptron</td><td>-</td></tr>
<tr><td>26</td><td>Vladimir Vapnik</td><td>Columbia University, ex-AT&amp;T</td><td>Statistical Learning Theory</td><td>Support Vector Machines (SVMs), VC Theory</td><td>-</td></tr>
<tr><td>27</td><td>Corinna Cortes</td><td>Google</td><td>Machine Learning</td><td>Support Vector Machines (SVMs)</td><td>-</td></tr>
<tr><td>28</td><td>Michael I. Jordan</td><td>UC Berkeley</td><td>Machine Learning, Statistics</td><td>Probabilistic Models, Latent Dirichlet Allocation</td><td>-</td></tr>
<tr><td>29</td><td>Stuart Russell</td><td>UC Berkeley</td><td>AI, Rational Agents</td><td>“Artificial Intelligence: A Modern Approach”</td><td>-</td></tr>
<tr><td>30</td><td>Peter Norvig</td><td>Google, Stanford University</td><td>AI, Search Algorithms</td><td>“Artificial Intelligence: A Modern Approach”</td><td>-</td></tr>
<tr><td>31</td><td>Daphne Koller</td><td>Stanford University, Insitro</td><td>Probabilistic Graphical Models</td><td>Bayesian Networks, Coursera Co-founder</td><td>-</td></tr>
<tr><td>32</td><td>Sebastian Thrun</td><td>Stanford University, Kitty Hawk</td><td>Robotics, Probabilistic AI</td><td>Probabilistic Robotics, Self-driving Cars</td><td>-</td></tr>
<tr><td>33</td><td>Rodney Brooks</td><td>MIT, Robust.AI</td><td>Robotics</td><td>Behavior-based Robotics, iRobot Co-founder</td><td>-</td></tr>
<tr><td>34</td><td>Frederick Jelinek</td><td>Johns Hopkins University</td><td>Speech Recognition, NLP</td><td>Statistical Methods in NLP</td><td>-</td></tr>
<tr><td>35</td><td>Alex Graves</td><td>DeepMind</td><td>Recurrent Neural Networks</td><td>Connectionist Temporal Classification (CTC)</td><td>-</td></tr>
<tr><td>36</td><td>Andrej Karpathy</td><td>ex-Tesla, OpenAI</td><td>Computer Vision, Deep Learning</td><td>CNNs, Vision for Autonomous Driving</td><td>-</td></tr>
<tr><td>37</td><td>Ruslan Salakhutdinov</td><td>Carnegie Mellon University</td><td>Deep Learning, Generative Models</td><td>Deep Belief Networks</td><td>-</td></tr>
<tr><td>38</td><td>Zoubin Ghahramani</td><td>University of Cambridge, Google</td><td>Bayesian Machine Learning</td><td>Probabilistic Models, Bayesian Nonparametrics</td><td>-</td></tr>
<tr><td>39</td><td>Bernhard Schölkopf</td><td>Max Planck Institute</td><td>Kernel Methods, Causal Inference</td><td>Kernel Methods, Causal Discovery</td><td>-</td></tr>
<tr><td>40</td><td>Leslie Kaelbling</td><td>MIT</td><td>Reinforcement Learning, Robotics</td><td>Decision-theoretic planning</td><td>-</td></tr>
<tr><td>41</td><td>Tom Mitchell</td><td>Carnegie Mellon University</td><td>Machine Learning</td><td>“Machine Learning” textbook, Never-Ending Language Learner</td><td>-</td></tr>
<tr><td>42</td><td>Pieter Abbeel</td><td>UC Berkeley, Covariant</td><td>Reinforcement Learning, Robotics</td><td>Deep Reinforcement Learning for Robotics</td><td>-</td></tr>
<tr><td>43</td><td>Timnit Gebru</td><td>DAIR</td><td>AI Ethics, Fairness</td><td>Bias in Facial Recognition and Language Models</td><td>-</td></tr>
<tr><td>44</td><td>Joy Buolamwini</td><td>Algorithmic Justice League</td><td>AI Ethics, Fairness</td><td>Bias in Facial Recognition Systems</td><td>-</td></tr>
<tr><td>45</td><td>Kate Crawford</td><td>USC, Microsoft Research</td><td>Social Implications of AI</td><td>AI and Power, “Atlas of AI”</td><td>-</td></tr>
<tr><td>46</td><td>Cynthia Dwork</td><td>Harvard University</td><td>Fairness, Privacy</td><td>Differential Privacy</td><td>-</td></tr>
<tr><td>47</td><td>Samy Bengio</td><td>Apple, ex-Google</td><td>Deep Learning</td><td>Model Ensembling, Speech Recognition</td><td>-</td></tr>
<tr><td>48</td><td>Oriol Vinyals</td><td>DeepMind</td><td>Deep Learning, NLP, RL</td><td>Sequence-to-Sequence Models, AlphaStar</td><td>-</td></tr>
<tr><td>49</td><td>Diederik P. Kingma</td><td>Google</td><td>Generative Models, Optimization</td><td>Adam Optimizer, Variational Autoencoders (VAEs)</td><td>-</td></tr>
<tr><td>50</td><td>Max Welling</td><td>University of Amsterdam, Microsoft</td><td>Deep Learning, Graphical Models</td><td>Variational Autoencoders (VAEs), Graph Neural Networks</td><td>-</td></tr>
<tr><td>51</td><td>Raia Hadsell</td><td>DeepMind</td><td>Robotics, Continual Learning</td><td>Deep Learning for Robot Navigation</td><td>-</td></tr>
<tr><td>52</td><td>Chelsea Finn</td><td>Stanford University</td><td>Meta-Learning, Robotics</td><td>Model-Agnostic Meta-Learning (MAML)</td><td>-</td></tr>
<tr><td>53</td><td>Percy Liang</td><td>Stanford University</td><td>Natural Language Processing</td><td>Semantic Parsing, Language Grounding</td><td>-</td></tr>
<tr><td>54</td><td>Jacob Devlin</td><td>Google</td><td>Natural Language Processing</td><td>BERT (Bidirectional Encoder Representations from Transformers)</td><td>-</td></tr>
<tr><td>55</td><td>Alec Radford</td><td>OpenAI</td><td>Generative Models, NLP</td><td>Generative Pre-trained Transformers (GPT) series</td><td>-</td></tr>
<tr><td>56</td><td>John Schulman</td><td>OpenAI</td><td>Reinforcement Learning</td><td>Proximal Policy Optimization (PPO), TRPO</td><td>-</td></tr>
<tr><td>57</td><td>Wojciech Zaremba</td><td>OpenAI</td><td>Deep Learning, Robotics</td><td>Co-founder of OpenAI, Robotics research</td><td>-</td></tr>
<tr><td>58</td><td>Edward Feigenbaum</td><td>Stanford University</td><td>Expert Systems</td><td>Pioneer of Expert Systems</td><td>1994 ACM A.M. Turing Award</td></tr>
<tr><td>59</td><td>Raj Reddy</td><td>Carnegie Mellon University</td><td>Speech Recognition, Robotics</td><td>Large-vocabulary continuous speech recognition</td><td>1994 ACM A.M. Turing Award</td></tr>
<tr><td>60</td><td>Douglas Lenat</td><td>Cycorp</td><td>Knowledge Representation</td><td>Cyc Project</td><td>-</td></tr>
<tr><td>61</td><td>David Rumelhart</td><td>Stanford University</td><td>Cognitive Science, Neural Networks</td><td>Backpropagation, Parallel Distributed Processing</td><td>-</td></tr>
<tr><td>62</td><td>James McClelland</td><td>Stanford University</td><td>Cognitive Science, Neural Networks</td><td>Parallel Distributed Processing (PDP)</td><td>-</td></tr>
<tr><td>63</td><td>Terrence Sejnowski</td><td>Salk Institute, UC San Diego</td><td>Computational Neuroscience</td><td>Boltzmann Machines, Independent Component Analysis</td><td>-</td></tr>
<tr><td>64</td><td>Eric Horvitz</td><td>Microsoft</td><td>AI for Social Good, Decision Theory</td><td>Principles of AI, AI for healthcare and sustainability</td><td>-</td></tr>
<tr><td>65</td><td>Manuela Veloso</td><td>JP Morgan AI Research, CMU</td><td>Robotics, AI</td><td>Robot Soccer (RoboCup)</td><td>-</td></tr>
<tr><td>66</td><td>Andrew Zisserman</td><td>University of Oxford, Google</td><td>Computer Vision</td><td>Visual Geometry Group (VGG), Object Recognition</td><td>-</td></tr>
<tr><td>67</td><td>Trevor Darrell</td><td>UC Berkeley</td><td>Computer Vision, Deep Learning</td><td>Deep Learning for Vision, BAIR Lab</td><td>-</td></tr>
<tr><td>68</td><td>Alexei A. Efros</td><td>UC Berkeley</td><td>Computer Vision</td><td>Texture Synthesis, Image-to-Image Translation</td><td>-</td></tr>
<tr><td>69</td><td>Cordelia Schmid</td><td>Inria, Google</td><td>Computer Vision</td><td>Local Feature Detectors, Action Recognition</td><td>-</td></tr>
<tr><td>70</td><td>Fei-Sha</td><td>Google</td><td>Machine Learning, Speech</td><td>Speech Recognition, Deep Learning Models</td><td>-</td></tr>
<tr><td>71</td><td>Chris Lattner</td><td>Numenta, ex-Google, Apple</td><td>Compilers, ML Systems</td><td>Swift, LLVM, TensorFlow Infrastructure</td><td>-</td></tr>
<tr><td>72</td><td>Jeff Hawkins</td><td>Numenta</td><td>Neuroscience, AI</td><td>Hierarchical Temporal Memory (HTM)</td><td>-</td></tr>
<tr><td>73</td><td>Gary Marcus</td><td>Robust.AI</td><td>Cognitive Science, AI Critique</td><td>Critique of Deep Learning, Neuro-symbolic AI</td><td>-</td></tr>
<tr><td>74</td><td>Melanie Mitchell</td><td>Santa Fe Institute</td><td>Complex Systems, AI</td><td>Analogy-making, “Artificial Intelligence: A Guide for Thinking Humans”</td><td>-</td></tr>
<tr><td>75</td><td>Pedro Domingos</td><td>University of Washington</td><td>Machine Learning</td><td>Markov Logic Networks, “The Master Algorithm”</td><td>-</td></tr>
<tr><td>76</td><td>Andrew Barto</td><td>University of Massachusetts Amherst</td><td>Reinforcement Learning</td><td>Co-author of “Reinforcement Learning: An Introduction”</td><td>-</td></tr>
<tr><td>77</td><td>Thomas G. Dietterich</td><td>Oregon State University</td><td>Machine Learning</td><td>Ensemble Methods, Anomaly Detection</td><td>-</td></tr>
<tr><td>78</td><td>Michael Kearns</td><td>University of Pennsylvania</td><td>Algorithmic Game Theory, Fairness</td><td>“The Ethical Algorithm”</td><td>-</td></tr>
<tr><td>79</td><td>Aaron Courville</td><td>Mila, Université de Montréal</td><td>Deep Learning, Generative Models</td><td>Co-author of “Deep Learning” book</td><td>-</td></tr>
<tr><td>80</td><td>Sergey Levine</td><td>UC Berkeley</td><td>Reinforcement Learning, Robotics</td><td>Deep Learning for Control, End-to-End Robotic Learning</td><td>-</td></tr>
<tr><td>81</td><td>Noam Shazeer</td><td>Character.ai, ex-Google</td><td>Natural Language Processing</td><td>Transformer, Mixture of Experts (MoE)</td><td>-</td></tr>
<tr><td>82</td><td>Nando de Freitas</td><td>DeepMind</td><td>Deep Learning, Bayesian Inference</td><td>Neural Networks, Reinforcement Learning</td><td>-</td></tr>
<tr><td>83</td><td>John Hopfield</td><td>Princeton University</td><td>Neural Networks, Physics</td><td>Hopfield Networks</td><td>2024 Nobel Prize in Physics</td></tr>
<tr><td>84</td><td>Geoffrey West</td><td>Santa Fe Institute</td><td>Complex Systems, Scaling Laws</td><td>Scaling laws in cities, companies, and biological systems</td><td>-</td></tr>
<tr><td>85</td><td>Stephen Wolfram</td><td>Wolfram Research</td><td>Cellular Automata, Computation</td><td>Mathematica, “A New Kind of Science”</td><td>-</td></tr>
<tr><td>86</td><td>Ray Kurzweil</td><td>Google</td><td>Futurism, AI</td><td>The Singularity, Optical Character Recognition</td><td>-</td></tr>
<tr><td>87</td><td>Nick Bostrom</td><td>University of Oxford</td><td>AI Safety, Philosophy</td><td>Superintelligence, Existential Risk</td><td>-</td></tr>
<tr><td>88</td><td>Eliezer Yudkowsky</td><td>MIRI</td><td>AI Safety, Rationality</td><td>AI Alignment, Decision Theory</td><td>-</td></tr>
<tr><td>89</td><td>Stuart Armstrong</td><td>University of Oxford</td><td>AI Safety</td><td>Value Learning, AI Alignment</td><td>-</td></tr>
<tr><td>90</td><td>Paul Christiano</td><td>Alignment Research Center</td><td>AI Safety</td><td>Iterated Distillation and Amplification (IDA)</td><td>-</td></tr>
<tr><td>91</td><td>Dario Amodei</td><td>Anthropic</td><td>AI Safety, Large Language Models</td><td>Constitutional AI</td><td>-</td></tr>
<tr><td>92</td><td>Daniela L. Rus</td><td>MIT</td><td>Robotics, Distributed Systems</td><td>Self-reconfiguring robots, CSAIL Director</td><td>-</td></tr>
<tr><td>93</td><td>Anca Dragan</td><td>UC Berkeley</td><td>Human-Robot Interaction</td><td>Value Alignment, Robot Learning from Humans</td><td>-</td></tr>
<tr><td>94</td><td>Yoav Shoham</td><td>AI21 Labs, Stanford University</td><td>Multi-agent Systems, Game Theory</td><td>AI Index, AI21 Labs Co-founder</td><td>-</td></tr>
<tr><td>95</td><td>Oren Etzioni</td><td>AI21 Labs, AI2</td><td>NLP, Web Search</td><td>AI Index, Semantic Scholar</td><td>-</td></tr>
<tr><td>96</td><td>Antoine Bordes</td><td>Helsing, ex-Meta AI</td><td>Knowledge Graphs, NLP</td><td>FAIR Paris Lab Director, Memory Networks</td><td>-</td></tr>
<tr><td>97</td><td>Kyunghyun Cho</td><td>New York University, Genentech</td><td>Natural Language Processing</td><td>Gated Recurrent Unit (GRU), Neural Machine Translation</td><td>-</td></tr>
<tr><td>98</td><td>Margaret A. Boden</td><td>University of Sussex</td><td>Cognitive Science, AI Philosophy</td><td>Creativity and AI, History of AI</td><td>-</td></tr>
<tr><td>99</td><td>Daniel Kahneman</td><td>Princeton University</td><td>Behavioral Economics, Cognitive Psychology</td><td>Thinking, Fast and Slow; Prospect Theory</td><td>2002 Nobel Prize in Economics</td></tr>
<tr><td>100</td><td>Amos Tversky</td><td>Stanford University</td><td>Cognitive Psychology</td><td>Prospect Theory, Heuristics and Biases</td><td>-</td></tr>
</tbody></table>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>