<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2014년 3분기 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2014년 3분기 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2014년 AI 및 로봇 연구 동향</a> / <span>2014년 3분기 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2014년 3분기 AI 및 로봇 연구 동향</h1>
<h2>1.  2014년, 새로운 시대의 전환점</h2>
<h3>1.1  시대적 배경</h3>
<p>2012년 AlexNet이 ImageNet 대규모 시각 인식 챌린지(ILSVRC)에서 압도적인 성능을 보인 이후, 인공지능 연구의 패러다임은 심층 신경망, 즉 딥러닝으로 급격히 이동하였다. 초기 2년간의 연구가 딥러닝의 잠재력을 탐색하고 검증하는 단계였다면, 2014년은 그 가능성이 구체적인 성과로 폭발하며 기존의 최첨단(state-of-the-art) 방법론을 대체하기 시작한 변곡점이었다. 특히 2014년 3분기는 이러한 흐름이 정점에 달한 시기로, 컴퓨터 비전 분야에서는 이전의 패러다임이 완전히 새로운 원칙으로 대체되는 결정적인 순간이었다. 이 시기에 발표된 연구들은 단순히 점진적인 성능 향상을 넘어, 이후 AI 기술의 발전 방향을 결정짓는 근본적인 설계 철학과 개념적 돌파구를 제시하였다.</p>
<h3>1.2  주요 학술대회의 집중과 파급 효과</h3>
<p>2014년 3분기의 연구 동향을 논할 때, 9월 한 달 동안 세계 최고 수준의 학술대회들이 집중적으로 개최되었다는 점은 매우 중요한 배경이 된다. 컴퓨터 비전 분야의 양대 산맥 중 하나인 유럽 컴퓨터 비전 학회(ECCV) 1, 로보틱스 분야의 최고 권위 학회인 IEEE/RSJ 지능형 로봇 및 시스템 국제 학회(IROS) 4, 그리고 영국 머신 비전 학회(BMVC) 7가 불과 3주라는 짧은 기간 내에 연이어 열렸다.</p>
<p>이러한 학술의 장이 동시다발적으로 열리면서, 각 분야에서 독립적으로 발전하던 혁신적인 아이디어들이 전례 없는 속도로 상호 교류하고 검증되는 강력한 시너지 효과가 발생했다. 예를 들어, ILSVRC 2014 챌린지를 통해 검증된 최첨단 컨볼루션 신경망(CNN) 아키텍처의 등장은 컴퓨터 비전 커뮤니티에 즉각적인 영향을 미쳤고, 동시에 IROS에서는 이러한 진보된 비전 기술이 자율주행차의 위치 추정이나 로봇의 환경 인식과 같은 실제 문제에 어떻게 적용될 수 있는지를 보여주었다. 이처럼 특정 시기에 지적 성과가 집중적으로 발표되고 전파되는 현상은 학문적 발전이 선형적으로 이루어지는 것이 아니라, 때로는 특정 변곡점을 중심으로 폭발적으로 가속화될 수 있음을 시사한다. 이 시기는 VGGNet과 GoogLeNet과 같은 기념비적인 모델이 등장하며 CNN 연구의 방향성을 제시함과 동시에, LSD-SLAM과 같은 연구가 순수 비전 기술과 로봇 응용 사이의 간극을 메우는 역할을 하였다. 이처럼 여러 분야의 발전이 동시다발적으로 일어나면서, 딥러닝이 단순한 학문적 성공을 넘어 산업 전반을 변화시킬 핵심 기술이라는 인식이 확고해졌다.</p>
<table><thead><tr><th>학회명 (Conference)</th><th>전체 명칭 (Full Name)</th><th>개최 기간 (Dates)</th><th>개최지 (Location)</th><th>관련 Snippet</th></tr></thead><tbody>
<tr><td>BMVC 2014</td><td>British Machine Vision Conference</td><td>Sep 1 - Sep 5, 2014</td><td>Nottingham, UK</td><td>8</td></tr>
<tr><td>ECCV 2014</td><td>European Conference on Computer Vision</td><td>Sep 6 - Sep 12, 2014</td><td>Zurich, Switzerland</td><td>1</td></tr>
<tr><td>IROS 2014</td><td>IEEE/RSJ International Conference on Intelligent Robots and Systems</td><td>Sep 14 - Sep 18, 2014</td><td>Chicago, USA</td><td>5</td></tr>
</tbody></table>
<h3>1.3  보고서의 구성</h3>
<p>본 보고서는 2014년 3분기라는 결정적 시기에 발표된 핵심 연구들을 세 가지 주요 축으로 나누어 심층적으로 분석한다. 첫째, ILSVRC 2014를 중심으로 펼쳐진 심층 컨볼루션 신경망의 진화 과정을 VGGNet과 GoogLeNet을 통해 살펴본다. 둘째, 데이터로부터 사실적인 결과물을 생성하는 새로운 패러다임을 제시한 생성적 적대 신경망(GAN)의 개념적 혁신을 분석한다. 셋째, IROS 2014에서 발표된 연구들을 중심으로 자율주행, SLAM, 그리고 AI 기반 로봇 제어 기술의 도약을 조명한다. 각 분석을 통해 해당 연구들이 제시한 핵심 아이디어와 기술적 의의를 명확히 하고, 이들이 현재 AI 기술 지형에 어떠한 영향을 미쳤는지를 추적하여 2014년 3분기가 남긴 유산을 종합적으로 평가하고자 한다.</p>
<h2>2.  컴퓨터 비전의 패러다임 전환: ILSVRC 2014와 심층 컨볼루션 신경망의 경쟁</h2>
<p>2014년 3분기는 ILSVRC 2014 챌린지의 결과가 발표되면서 컴퓨터 비전 분야, 특히 심층 컨볼루션 신경망(CNN) 아키텍처 설계에 있어 중대한 전환을 이룬 시기였다. 이 대회를 통해 두 개의 상징적인 모델, VGGNet과 GoogLeNet이 등장했다. 이 두 모델은 단순히 성능 경쟁에서 우위를 점한 것을 넘어, ’깊이(depth)’와 ’효율성(efficiency)’이라는 두 가지 상반된 설계 철학을 제시하며 이후 CNN 아키텍처 연구의 방향성을 결정지었다.</p>
<h3>2.1  VGGNet: 깊이의 한계를 탐구하다</h3>
<p>옥스퍼드 대학의 Visual Geometry Group이 제안한 VGGNet은 “Very Deep Convolutional Networks for Large-Scale Image Recognition“이라는 논문을 통해 발표되었다.9 이 모델은 ILSVRC 2014의 위치추정(localisation) 부문에서 1위, 분류(classification) 부문에서 2위를 차지하며, 네트워크의 ’깊이’가 성능 향상의 핵심 변수임을 명확하게 입증했다.9</p>
<h4>2.1.1 핵심 아이디어: 균일성과 깊이</h4>
<p>VGGNet의 가장 큰 특징은 아키텍처의 극단적인 단순성과 균일성에 있다. 이전의 AlexNet이나 ZFNet 등이 비교적 큰 크기의 필터(<span class="math math-inline">11 \times 11</span>, <span class="math math-inline">7 \times 7</span>)를 초반 레이어에 사용하고, 이후 레이어에서 필터 크기를 다양하게 조절했던 것과 대조적으로, VGGNet은 전체 네트워크에 걸쳐 오직 가장 작은 크기인 <span class="math math-inline">3 \times 3</span> 컨볼루션 필터만을 사용했다.9 이러한 설계 원칙을 바탕으로, 연구팀은 컨볼루션 레이어를 반복적으로 쌓아 네트워크의 총 가중치 레이어 수를 16개(VGG-16)에서 19개(VGG-19)까지 점진적으로 늘려나갔다.9 이처럼 아키텍처의 다른 변수들을 최대한 고정하고 오직 깊이만을 변화시키는 체계적인 접근 방식은, 깊이가 모델 성능에 미치는 영향을 명확하게 분리하여 분석할 수 있게 해주었다.</p>
<h4>2.1.2 <span class="math math-inline">3 \times 3</span> 필터의 효과</h4>
<p><span class="math math-inline">3 \times 3</span> 필터의 연속적인 사용은 VGGNet의 성공을 이끈 핵심적인 설계 결정이었다. 작은 크기의 필터를 여러 개 쌓는 방식은 단일의 큰 필터를 사용하는 것보다 두 가지 주요한 이점을 제공한다. 첫째, 더 적은 파라미터로 동일하거나 더 큰 수용장(receptive field)을 확보할 수 있다. 예를 들어, <span class="math math-inline">3 \times 3</span> 컨볼루션 필터를 세 번 연속으로 쌓으면 <span class="math math-inline">7 \times 7</span> 필터 하나와 동일한 크기의 수용장을 가지게 된다. 하지만 이때 필요한 파라미터의 수는 채널 수를 <span class="math math-inline">C</span>라고 할 때, <span class="math math-inline">3 \times (3^2 \times C^2) = 27C^2</span>으로, <span class="math math-inline">7 \times 7</span> 필터의 <span class="math math-inline">49C^2</span>에 비해 약 45%나 적다.9 이는 모델의 복잡도를 낮추고 과적합을 방지하는 일종의 정규화 효과를 가져온다. 둘째, 비선형성을 증가시켜 모델의 표현력을 강화한다. 컨볼루션 레이어 다음에는 ReLU와 같은 비선형 활성화 함수가 적용되는데,</p>
<p><span class="math math-inline">3 \times 3</span> 필터를 세 번 사용하면 세 개의 비선형 함수가 적용되는 반면, <span class="math math-inline">7 \times 7</span> 필터는 단 한 번만 적용된다. 더 많은 비선형성의 추가는 결정 함수(decision function)를 더욱 복잡하고 정교하게 만들어 모델의 표현력을 높이는 데 기여한다.9 이러한 VGGNet의 설계 원칙은 매우 효과적이었음이 입증되었고, 이후 등장하는 수많은 CNN 아키텍처에서 작은 크기의 필터를 깊게 쌓는 방식이 표준으로 자리 잡게 되었다.</p>
<table><thead><tr><th>구성 (Configuration)</th><th>D (VGG-16)</th><th>E (VGG-19)</th></tr></thead><tbody>
<tr><td><strong>입력 (Input)</strong></td><td><span class="math math-inline">224 \times 224</span> RGB 이미지</td><td><span class="math math-inline">224 \times 224</span> RGB 이미지</td></tr>
<tr><td><strong>conv3-64</strong></td><td>2개 층</td><td>2개 층</td></tr>
<tr><td><strong>maxpool</strong></td><td>O</td><td>O</td></tr>
<tr><td><strong>conv3-128</strong></td><td>2개 층</td><td>2개 층</td></tr>
<tr><td><strong>maxpool</strong></td><td>O</td><td>O</td></tr>
<tr><td><strong>conv3-256</strong></td><td>3개 층</td><td>4개 층</td></tr>
<tr><td><strong>maxpool</strong></td><td>O</td><td>O</td></tr>
<tr><td><strong>conv3-512</strong></td><td>3개 층</td><td>4개 층</td></tr>
<tr><td><strong>maxpool</strong></td><td>O</td><td>O</td></tr>
<tr><td><strong>conv3-512</strong></td><td>3개 층</td><td>4개 층</td></tr>
<tr><td><strong>maxpool</strong></td><td>O</td><td>O</td></tr>
<tr><td><strong>FC-4096</strong></td><td>1개 층</td><td>1개 층</td></tr>
<tr><td><strong>FC-4096</strong></td><td>1개 층</td><td>1개 층</td></tr>
<tr><td><strong>FC-1000</strong></td><td>1개 층</td><td>1개 층</td></tr>
<tr><td><strong>softmax</strong></td><td>O</td><td>O</td></tr>
<tr><td><strong>총 가중치 층</strong></td><td>16 (13 conv + 3 FC)</td><td>19 (16 conv + 3 FC)</td></tr>
<tr><td><strong>파라미터 수</strong></td><td>약 1억 3800만 (138M)</td><td>약 1억 4400만 (144M)</td></tr>
</tbody></table>
<h3>2.2  GoogLeNet: 효율성과 성능의 새로운 균형, Inception 모듈의 등장</h3>
<p>VGGNet이 깊이의 미학을 탐구했다면, 구글이 개발한 GoogLeNet은 “Going Deeper with Convolutions“라는 논문을 통해 완전히 다른 방향성을 제시했다.13 GoogLeNet은 ILSVRC 2014 분류 과제에서 VGGNet을 누르고 1위를 차지했는데, 놀라운 점은 VGGNet(약 1억 4천만 개)보다 20배 이상 적은 약 5백만 개의 파라미터만으로 더 높은 성능을 달성했다는 것이다.13 이는 단순히 네트워크를 깊게 쌓는 것만이 능사가 아니며, 아키텍처의 ’효율성’이 성능 향상의 또 다른 핵심 축임을 증명한 사건이었다.</p>
<h4>2.2.1 핵심 아이디어: Inception 모듈</h4>
<p>GoogLeNet의 혁신은 ’Inception 모듈’이라는 새로운 구조적 단위에서 비롯되었다.14 기존의 CNN이 한 레이어에서 단일 크기의 컨볼루션 연산을 수행하는 직렬적인 구조였던 반면, Inception 모듈은 하나의 입력에 대해 <span class="math math-inline">1 \times 1</span>, <span class="math math-inline">3 \times 3</span>, <span class="math math-inline">5 \times 5</span> 컨볼루션과 <span class="math math-inline">3 \times 3</span> 맥스 풀링 연산을 병렬적으로 동시에 수행하고, 그 결과들을 채널 방향으로 모두 합치는 독창적인 ‘Network-in-Network’ 구조를 채택했다.15 이 설계의 이면에는 이미지 내의 객체는 다양한 스케일로 존재하며, 어떤 크기의 필터가 가장 효과적일지 미리 결정하기 어렵다는 통찰이 담겨 있다. Inception 모듈은 다양한 크기의 수용장을 동시에 탐색함으로써 네트워크가 스스로 데이터에 가장 적합한 스케일의 특징을 학습하도록 유도한다.</p>
<h4>2.2.2 <span class="math math-inline">1 \times 1</span> 컨볼루션의 역할: 병목 구조(Bottleneck Architecture)</h4>
<p>Inception 모듈의 병렬 구조는 자칫 계산 비용의 폭증으로 이어질 수 있다. 특히 <span class="math math-inline">5 \times 5</span>와 같은 큰 필터는 많은 계산량을 요구한다. GoogLeNet은 이 문제를 해결하기 위해 ‘병목(bottleneck)’ 구조라는 기법을 도입했다. 이는 계산 비용이 높은 <span class="math math-inline">3 \times 3</span> 및 <span class="math math-inline">5 \times 5</span> 컨볼루션 연산을 수행하기 직전에, <span class="math math-inline">1 \times 1</span> 컨볼루션을 사용하여 입력 피처 맵의 채널 수를 일시적으로 줄이는 방식이다.13 예를 들어, 256개의 채널을 가진 입력에 바로 <span class="math math-inline">5 \times 5</span> 컨볼루션을 적용하는 대신, <span class="math math-inline">1 \times 1</span> 컨볼루션으로 채널을 32개로 줄인 후 <span class="math math-inline">5 \times 5</span> 연산을 수행하고, 다시 채널을 확장한다. <span class="math math-inline">1 \times 1</span> 컨볼루션은 공간적 정보는 유지하면서 채널 간의 정보만을 혼합하고 재구성하는 역할을 하므로, 표현력의 손실을 최소화하면서도 계산량을 획기적으로 절감할 수 있다. 이 병목 구조는 GoogLeNet의 효율성을 가능하게 한 핵심적인 아이디어로, 이후 ResNet을 비롯한 거의 모든 현대적인 CNN 아키텍처에 필수적인 요소로 채택되었다.</p>
<p>이처럼 VGGNet과 GoogLeNet이 제시한 상반된 접근 방식은 이후 CNN 연구에 깊은 영향을 미쳤다. VGGNet의 단순하고 균일한 구조는 그 자체로 강력한 특징 추출기로서 전이 학습(transfer learning)의 표준 ‘백본(backbone)’ 아키텍처로 널리 활용되었다.11 반면, GoogLeNet의 효율적인 Inception 모듈은 계산 자원이 제한된 환경에서도 높은 성능을 달성할 수 있는 ‘마이크로 아키텍처(micro-architecture)’ 설계의 중요성을 일깨웠다. 이 두 모델의 경쟁은 CNN 설계가 ’단순성’과 ’효율성’이라는 두 축 사이에서 최적의 균형점을 찾는 과정임을 보여주었으며, 이 설계적 긴장감은 오늘날까지도 신경망 아키텍처 연구의 핵심적인 화두로 남아 있다.</p>
<h3>2.3  ECCV &amp; BMVC 2014: 비전 연구의 확산</h3>
<p>VGGNet과 GoogLeNet이 딥러닝 아키텍처의 거시적인 방향을 제시했다면, 같은 시기에 열린 BMVC와 ECCV에서는 이러한 흐름을 더욱 구체화하고 다양한 응용 분야로 확장하는 연구들이 활발하게 발표되었다.</p>
<p>BMVC 2014에서 발표된 “Return of the Devil in the Details” (Chatfield et al.)는 당시 부상하던 CNN 기반 표현 방식의 우수성을 체계적으로 검증한 중요한 연구였다.19 이 논문은 VGGNet과 유사한 구조의 CNN 모델을 당시 최첨단 기술이었던 전통적인 특징 기반 방법론(Bag-of-Visual-Words, Improved Fisher Vector)과 동일한 조건에서 엄격하게 비교 분석하였다.19 그 결과, CNN이 추출한 특징 표현이 기존의 수작업 특징(hand-crafted features)보다 월등히 우수함을 다시 한번 입증했다. 또한, 이 연구는 CNN의 마지막 완전 연결 계층(fully-connected layer)에서 출력되는 특징 벡터의 차원을 주성분 분석(PCA) 등을 통해 수천 개에서 수백 개 수준으로 크게 줄여도 분류 성능에는 거의 영향이 없다는 실용적인 발견을 제시했다.19 이는 딥러닝 특징이 매우 효율적으로 압축될 수 있음을 보여주며, 이후 특징 벡터를 활용한 검색이나 다른 머신러닝 모델과의 연동에 있어 중요한 지침이 되었다.</p>
<p>한편, ECCV 2014에서는 CNN을 넘어선 광범위한 컴퓨터 비전 연구들이 발표되며 학문적 지평을 넓혔다. 발표된 363편의 논문은 1444편의 제출물 중에서 엄선된 것으로, 추적, 활동 인식, 3D 재구성, 분할 등 다양한 주제를 포괄했다.1 특히, 딥러닝의 성공에 자극받아 기존의 확률 모델이나 기하학적 기법에 딥러닝 특징을 결합하려는 시도들이 두드러졌다. 예를 들어, ’Pose filter based hidden-crf models for activity detection’은 인간의 자세 정보를 필터링하여 은닉 조건부 무작위장(Hidden-CRF) 모델에 통합함으로써 활동 인식을 수행했고, ’Semantic Aware Video Transcription using Random Forest Classifiers’는 랜덤 포레스트 분류기를 사용하여 비디오의 의미론적 내용을 분석하는 연구를 선보였다.23 또한, ’Wearable RGBD indoor navigation system for the blind’와 같이 시각장애인을 위한 웨어러블 보조 시스템 개발 연구도 발표되어, 컴퓨터 비전 기술의 사회적 기여 가능성을 보여주었다.23</p>
<p>이 중에서도 특히 주목할 만한 연구는 “LSD-SLAM: Large-Scale Direct Monocular SLAM“이었다.24 이 연구는 전통적인 SLAM(Simultaneous Localization and Mapping) 방식이 SIFT나 ORB와 같은 인공적인 특징점(keypoints)에 의존했던 것에서 탈피하여, 이미지의 픽셀 밝기 값 자체를 직접(direct) 사용하여 카메라의 위치와 환경 지도를 동시에 추정하는 새로운 패러다임을 제시했다. 이는 로보틱스 분야에 즉각적인 파장을 일으켰으며, 4장에서 더 자세히 다루게 될 것이다.</p>
<h2>3.  생성 모델의 혁신: 생성적 적대 신경망(GAN)의 태동</h2>
<p>2014년 3분기는 컴퓨터 비전 분야에서 판별 모델(discriminative model)이 정점에 달한 시기였지만, 동시에 데이터의 근본적인 분포를 학습하여 새로운 데이터를 생성하는 생성 모델(generative model) 분야에서도 혁명적인 아이디어가 등장한 시기였다. 2014년 6월, 이안 굿펠로우(Ian Goodfellow)와 그의 동료들이 arXiv에 공개한 “Generative Adversarial Networks” 논문은 이후 10년간 AI 연구의 가장 뜨거운 주제 중 하나가 될 생성적 적대 신경망(GAN)의 탄생을 알렸다.26</p>
<h4>3.0.1 핵심 아이디어: 적대적 학습</h4>
<p>GAN의 핵심 아이디어는 직관적이면서도 강력하다. 이는 두 개의 신경망, 즉 ’생성자(Generator)’와 ’판별자(Discriminator)’를 서로 경쟁시키며 학습시키는 구조에 기반한다.26 생성자의 역할은 실제 데이터와 구별할 수 없을 만큼 사실적인 가짜 데이터를 만들어내는 것이다. 마치 위조지폐범이 진짜 같은 위조지폐를 만들려는 것과 같다. 반면, 판별자의 역할은 주어진 데이터가 실제 데이터인지 아니면 생성자가 만들어낸 가짜 데이터인지를 구별해내는 것이다. 이는 경찰이 위조지폐를 감별하는 역할에 비유할 수 있다.</p>
<p>학습 과정에서 생성자는 판별자를 속이기 위해 점점 더 정교한 가짜 데이터를 생성하도록 학습하고, 판별자는 생성자에게 속지 않기 위해 진짜와 가짜의 미세한 차이를 감지하는 능력을 키운다. 이 두 네트워크의 경쟁은 한쪽이 일방적으로 승리하는 것이 아니라, 서로의 성능을 점진적으로 향상시키는 방향으로 진행된다. 학습이 성공적으로 완료되면, 생성자는 실제 데이터의 분포를 매우 정확하게 학습하여 매우 사실적인 데이터를 생성할 수 있게 되고, 판별자는 더 이상 진짜와 가짜를 구별할 수 없는 상태(확률 0.5)에 이르게 된다.26</p>
<h4>3.0.2 Minimax Game</h4>
<p>이러한 적대적 학습 과정은 수학적으로 ’민맥스 2인용 게임(minimax two-player game)’으로 공식화된다. GAN의 목적 함수는 다음과 같이 표현된다.26</p>
<p><span class="math math-display">
\min_{G} \max_{D} V(D, G) = \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}(\mathbf{x})} + \mathbb{E}_{\mathbf{z} \sim p_{\mathbf{z}}(\mathbf{z})}
</span><br />
여기서 <span class="math math-inline">G</span>는 생성자, <span class="math math-inline">D</span>는 판별자, <span class="math math-inline">p_{data}(\mathbf{x})</span>는 실제 데이터의 분포, <span class="math math-inline">p_{\mathbf{z}}(\mathbf{z})</span>는 생성자의 입력으로 사용되는 노이즈 벡터의 분포를 의미한다. 판별자 <span class="math math-inline">D</span>는 실제 데이터 <span class="math math-inline">\mathbf{x}</span>에 대해서는 <span class="math math-inline">D(\mathbf{x})</span>를 1에 가깝게, 생성된 데이터 <span class="math math-inline">G(\mathbf{z})</span>에 대해서는 <span class="math math-inline">D(G(\mathbf{z}))</span>를 0에 가깝게 만들어 목적 함수 <span class="math math-inline">V(D, G)</span>를 최대화(maximize)하려고 한다. 동시에 생성자 <span class="math math-inline">G</span>는 판별자가 생성된 데이터를 실제 데이터로 착각하도록, 즉 <span class="math math-inline">D(G(\mathbf{z}))</span>를 1에 가깝게 만들어 목적 함수를 최소화(minimize)하려고 한다.</p>
<h4>3.0.3 패러다임의 전환</h4>
<p>GAN의 등장은 생성 모델 연구의 패러다임을 근본적으로 바꾸었다. 기존의 생성 모델들은 주로 최대우도추정(Maximum Likelihood Estimation, MLE)을 기반으로 데이터의 확률 분포를 명시적으로 모델링하려 했다. 하지만 이는 다루기 어려운 확률 계산을 요구하거나, 마르코프 체인 몬테카를로(MCMC)와 같은 복잡하고 느린 샘플링 과정이 필요한 경우가 많았다. 그 결과, 생성된 이미지가 흐릿하게 보이는 등의 한계가 있었다.</p>
<p>반면, GAN은 데이터의 확률 밀도 함수를 직접 다루는 대신, 게임 이론적 균형점(Nash equilibrium)을 찾는 방식으로 분포를 암묵적으로 학습한다. 이 접근 방식 덕분에 학습 과정에서 복잡한 추론이나 마르코프 체인이 필요 없으며, 오직 역전파(backpropagation) 알고리즘만으로 전체 시스템을 훈련할 수 있다.26 이는 GAN이 기존 모델들보다 훨씬 더 선명하고 사실적인 고품질의 샘플을 생성할 수 있게 만든 결정적인 요인이었다. GAN은 단순히 이미지를 생성하는 것을 넘어, 최적화 문제로만 여겨지던 신경망 학습에 ’경쟁’과 ’균형’이라는 게임 이론의 개념을 도입함으로써, 이후 AI 연구에 새로운 사유의 틀을 제공했다. 이 개념적 도약은 적대적 공격, 강건성 연구, 자기 지도 학습 등 다양한 분야로 확장되며 인공지능의 발전에 지대한 영향을 미쳤다.</p>
<h2>4.  지능형 로봇 시스템의 도약: IROS 2014의 주요 성과</h2>
<p>2014년 3분기는 컴퓨터 비전과 생성 모델의 이론적 발전뿐만 아니라, 이러한 AI 기술이 로봇 시스템에 통합되어 실질적인 성능 향상을 이끌어낸 중요한 시기였다. 특히 9월에 시카고에서 개최된 IROS 2014는 자율주행, 동시적 위치추정 및 지도작성(SLAM), 그리고 AI 기반 로봇 제어 분야에서 주목할 만한 성과들을 선보였다.</p>
<h3>4.1  자율주행의 눈: LIDAR 지도 내 시각 기반 위치 추정</h3>
<p>IROS 2014에서 가장 주목받은 연구 중 하나는 최우수 학생 논문상을 수상한 “Visual Localization within LIDAR Maps for Automated Urban Driving“이었다.27 이 연구는 자율주행 기술 상용화의 가장 큰 걸림돌 중 하나인 고가의 센서 문제를 해결하기 위한 혁신적인 접근법을 제시했다.</p>
<p>당시 구글의 자율주행차를 비롯한 대부분의 시스템은 주변 환경을 3차원으로 인식하고 정밀한 위치를 추정하기 위해 수천만 원에 달하는 고가의 3D 라이다(LIDAR) 센서에 의존했다.28 Wolcott과 Eustice는 이러한 비용 문제를 해결하기 위해, 고가의 라이다를 실시간 위치 추정에 사용하는 대신, 사전에 한 번만 라이다를 탑재한 측량 차량으로 정밀한 3D 지도(prior map)를 구축하는 데 사용하자고 제안했다. 그리고 실제 주행 시에는 차량에 저렴한 단안 카메라(monocular camera) 하나만을 장착하여, 이 사전 구축된 라이다 지도 내에서 자신의 위치를 센티미터 수준의 정밀도로 추정하는 방법을 개발했다.27</p>
<p>이들의 방법론은 그래픽 처리 장치(GPU)의 병렬 처리 능력을 적극적으로 활용한다. 시스템은 먼저 현재 차량의 위치라고 추정되는 지점(belief pose) 주변의 3D 라이다 지도 데이터를 기반으로 수천 개의 가상 시점 이미지(synthetic views)를 실시간으로 렌더링한다. 이는 마치 비디오 게임이 3D 모델로부터 2D 화면을 만들어내는 것과 유사한 과정이다.27 그 다음, 실제로 카메라가 촬영한 이미지와 이 수천 개의 가상 이미지 간의 유사도를 ’정규화된 상호 정보량(Normalized Mutual Information, NMI)’이라는 척도를 사용해 계산한다. NMI는 두 이미지 간의 통계적 의존성을 측정하는 척도로, 조명 변화 등에 강건하다. 시스템은 NMI 값이 최대가 되는 가상 이미지를 찾아내고, 그 이미지를 렌더링한 가상의 위치를 차량의 현재 위치로 최종 결정한다.28 이 연구는 정교한 알고리즘과 소프트웨어가 고가의 하드웨어를 대체할 수 있다는 가능성을 실증적으로 보여주었으며, 저비용 센서와 고정밀 지도의 결합이라는 접근 방식은 이후 자율주행 위치 추정 기술의 중요한 연구 방향 중 하나로 자리 잡았다.</p>
<h3>4.2  SLAM 기술의 진화: Direct 방식과 반밀집(Semi-Dense) 재구성</h3>
<p>컴퓨터 비전 학회인 ECCV 2014에서 발표되었지만, 로보틱스 커뮤니티에 즉각적이고 지대한 영향을 미친 또 다른 연구는 “LSD-SLAM: Large-Scale Direct Monocular SLAM“이었다.24 이 연구는 로봇이 미지의 환경을 탐색하며 자신의 위치를 추정하고 동시에 지도를 작성하는 SLAM 기술에 새로운 패러다임을 제시했다.</p>
<p>기존의 주류 SLAM 시스템들은 대부분 특징점 기반(feature-based) 방식에 의존했다. 이는 SIFT, SURF, ORB와 같은 알고리즘을 사용해 이미지에서 모서리나 코너점과 같은 독특한 특징점을 수백 개 추출하고, 연속된 이미지 프레임 간에 이 특징점들을 매칭하여 카메라의 움직임을 계산하는 방식이다. 이 방식은 효율적이지만, 텍스처가 부족한 환경(예: 흰 벽, 복도 등)에서는 특징점을 충분히 추출하기 어려워 추적에 실패하는 한계를 지녔다.32</p>
<p>LSD-SLAM은 이러한 한계를 극복하기 위해 ‘직접(direct)’ 방식을 채택했다.24 이는 인공적인 특징점을 추출하는 중간 과정 없이, 이미지의 픽셀 밝기 값(intensity) 자체를 직접 사용하여 이미지 간의 기하학적 관계를 추정하는 방식이다. 구체적으로, 현재 프레임의 모든 픽셀을 이전 키프레임(keyframe)의 3D 지도에 투영시켜 보고, 두 이미지 간의 광도 오차(photometric error)를 최소화하는 방향으로 카메라의 위치와 방향을 최적화한다. 이 방식은 특징점이 없는 영역을 포함한 이미지 전체의 정보를 활용하므로, 텍스처가 부족한 환경에서도 강건하게 작동하며 더 높은 정확도를 제공한다.25</p>
<p>또한, LSD-SLAM은 지도 표현 방식에서도 혁신을 이루었다. 특징점 기반 SLAM이 희소한(sparse) 3D 점 구름 형태의 지도를 생성하는 반면, LSD-SLAM은 이미지에서 밝기 변화가 큰 픽셀들을 중심으로 ‘반밀집(semi-dense)’ 형태의 3D 깊이 지도를 생성한다.32 이는 희소 지도보다 훨씬 풍부한 환경 정보를 제공하면서도, 모든 픽셀의 깊이를 계산하는 밀집(dense) 방식보다는 계산적으로 효율적이다. 마지막으로, 단안 카메라 SLAM의 고질적인 문제인 스케일 드리프트(scale-drift, 시간이 지남에 따라 지도의 축척이 변하는 현상)를 해결하기 위해, 카메라의 움직임을 6자유도의 강체 변환(SE(3))이 아닌, 스케일 변화까지 포함하는 7자유도의 유사 변환(Sim(3)) 공간에서 직접 추적하고 최적화하는 새로운 수학적 프레임워크를 제시했다.33 이처럼 LSD-SLAM이 제시한 직접 방식과 반밀집 재구성, 그리고 Sim(3) 최적화는 SLAM 연구에 새로운 방향을 제시하며 큰 족적을 남겼다.</p>
<h3>4.3  AI와 로봇공학의 융합: 학습 기반 제어 및 계획</h3>
<p>IROS 2014의 “AI-Based Robotics” 워크숍은 인공지능 기술, 특히 머신러닝을 로봇 시스템의 인식, 계획, 제어 문제에 통합하려는 다양한 시도들을 조명하는 장이었다.5 이는 전통적인 로봇 공학의 제어 이론 및 기하학적 접근법에 데이터 기반의 학습 방법론을 결합하려는 움직임이 본격화되고 있음을 보여주었다.</p>
<p>워크숍에서 발표된 연구들은 다양한 스펙트럼을 포괄했다. “Understanding Human Activities from Observation via Semantic Reasoning for Humanoid Robots“와 같은 연구는 로봇이 단순히 기하학적 정보를 넘어 인간의 행동에 담긴 ’의미’를 이해하고 이를 바탕으로 행동을 계획하는, 보다 높은 수준의 인지 능력 구현을 목표로 했다.5 “Combined Task and Motion Planning for AUVs“는 수중 자율 로봇(AUV)을 위해 ’무엇을 할 것인가’라는 상위 수준의 작업 계획(Task Planning)과 ’어떻게 움직일 것인가’라는 하위 수준의 모션 계획(Motion Planning)을 통합적으로 해결하려는 시도였다. 이는 로봇이 복잡한 임무를 수행하기 위해 필수적인 연구 분야이다.5 또한, “Multilayer General Value Functions for Robotic Prediction and Control“은 강화학습의 핵심 개념인 가치 함수(value function)를 로봇의 예측 및 제어 문제에 적용하여, 로봇이 미래의 보상을 예측하고 최적의 행동을 선택하도록 하는 학습 기반 제어의 가능성을 탐구했다.5</p>
<p>이러한 흐름 속에서, 7월 arXiv에 공개된 “Robots that can adapt like animals” (Cully et al.)는 로봇의 강건성(robustness)과 자율성에 대한 새로운 비전을 제시했다.36 이 연구는 로봇의 다리가 부러지거나 모터가 고장 나는 등 예상치 못한 손상을 입었을 때, 스스로의 상태를 진단하거나 사전에 프로그래밍된 비상 계획에 의존하지 않고, ’지능적인 시행착오’를 통해 스스로 보상 행동을 학습하는 알고리즘을 제안했다. 로봇은 손상 전, 다양한 행동과 그 결과를 탐색하며 고성능 행동 공간에 대한 ’직관’을 담은 지도를 미리 생성해 둔다. 손상이 발생하면, 이 직관을 바탕으로 효율적인 탐색을 수행하여 단 몇 분 안에 새로운 환경이나 자신의 변경된 신체 조건에 맞는 최적의 움직임을 찾아낸다. 이는 동물이 부상을 입었을 때 절뚝거리며 걷는 법을 빠르게 터득하는 것과 유사하다. 이 연구는 로봇이 예측 불가능한 실제 환경에서 안정적으로 작동하기 위해 필요한 자기 적응 능력의 중요한 방향성을 제시했다.</p>
<h2>5.  결론: 2014년 3분기가 남긴 유산과 미래 전망</h2>
<p>2014년 3분기는 인공지능과 로봇 공학의 역사에서 단순한 한 시점을 넘어, 이후 기술 발전의 청사진이 그려진 결정적인 전환점이었다. 이 시기에 발표된 연구들은 딥러닝이 특정 연구 분야의 유망한 방법론을 넘어, 과학과 기술 전반의 문제를 해결하는 보편적인 엔진으로 자리매김했음을 선언했다.</p>
<p>VGGNet과 GoogLeNet의 등장은 CNN 아키텍처 설계에 있어 ’깊이’와 ’효율성’이라는 두 가지 핵심적인 설계 철학을 확립했다. VGGNet이 제시한 균일한 소형 필터의 반복적 사용은 이후 ResNet과 같은 기념비적인 아키텍처의 기본 구조가 되었으며, GoogLeNet의 병목 구조와 다중 스케일 처리 아이디어는 계산 효율성을 극대화하는 현대적 네트워크 설계의 표준 어휘로 자리 잡았다. 이 두 모델이 연 경쟁의 장은 이후 수년간 CNN 아키텍처 연구의 황금기를 이끌었다.</p>
<p>동시에, 생성적 적대 신경망(GAN)의 제안은 생성 모델의 패러다임을 완전히 바꾸었다. 확률 분포를 직접 모델링하는 대신 두 네트워크의 경쟁을 통해 학습하는 이 독창적인 아이디어는, 이후 고해상도 이미지 생성, 스타일 변환, 데이터 증강 등 상상 속에서만 가능했던 일들을 현실로 만들었으며, 오늘날 생성 AI 혁명의 사상적 뿌리가 되었다.</p>
<p>로보틱스 분야에서는 AI 기술의 발전이 어떻게 실제 물리적 시스템의 지능으로 전환될 수 있는지를 명확히 보여주었다. 저비용 카메라와 고정밀 지도를 결합한 시각 기반 위치 추정 기술은 자율주행 상용화의 경제적 장벽을 낮추는 중요한 실마리를 제공했으며, 특징점에 의존하지 않는 Direct SLAM 기술은 로봇의 환경 인식 능력을 한 차원 높은 수준으로 끌어올렸다. 이러한 기술적 유산들은 이후 딥러닝 기반의 종단간(end-to-end) 자율주행 모델과 의미론적 SLAM(Semantic SLAM)과 같은 더욱 진보된 기술로 발전하는 기폭제가 되었다.</p>
<p>결론적으로, 2014년 3분기는 딥러닝 혁명의 서막을 연 시기였다. 이 시기에 제시된 개념과 아키텍처, 그리고 설계 철학은 이후 수많은 후속 연구의 토대가 되었으며, 현재 우리가 경험하고 있는 인공지능 기술의 폭발적인 성장을 가능하게 한 근본적인 동력이 되었다. 따라서 이 시기의 연구들을 이해하는 것은 단순히 과거의 기술적 성취를 복기하는 것을 넘어, 현재 AI 기술의 본질을 이해하고 미래의 발전 방향을 예측하는 데 필수적인 과정이라 할 수 있다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Computer Vision – ECCV 2014: 13th European Conference, Zurich, Switzerland, https://books.google.com/books/about/Computer_Vision_ECCV_2014.html?id=l4BHBAAAQBAJ</li>
<li>Holdings: Computer Vision – ECCV 2014, https://psnz.umt.edu.my/seal/Record/978-3-319-10593-2</li>
<li>Computer Vision – ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part IV | Request PDF - ResearchGate, https://www.researchgate.net/publication/321552838_Computer_Vision_-_ECCV_2014_13th_European_Conference_Zurich_Switzerland_September_6-12_2014_Proceedings_Part_IV</li>
<li>INTELLIGENT ROBOTS AND SYSTEMS. IEEE/RSJ INTERNATIONAL CONFERENCE. 2014. (IROS 2014) (6 VOLS) - proceedings.com, https://www.proceedings.com/24068.html</li>
<li>IROS 2014 Workshop on Intelligent Robotics Systems - People | MIT CSAIL, https://people.csail.mit.edu/gdk/iros-airob14/papers.html</li>
<li>INTELLIGENT ROBOTS AND SYSTEMS. IEEE/RSJ INTERNATIONAL CONFERENCE. 2014. (IROS 2014) (DVD) - proceedings.com, https://www.proceedings.com/24067.html</li>
<li>Location constrained pixel classifiers for image parsing with regular, https://scholar.xjtlu.edu.cn/en/publications/location-constrained-pixel-classifiers-for-image-parsing-with-reg-2</li>
<li>BMVC 2014 : British Machine Vision Conference - WikiCFP, <a href="http://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=33584&amp;copyownerid=63377">http://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=33584©ownerid=63377</a></li>
<li>Very Deep Convolutional Networks for Large-Scale Image …, https://arxiv.org/abs/1409.1556</li>
<li>VGG-Net Architecture Explained - GeeksforGeeks, https://www.geeksforgeeks.org/computer-vision/vgg-net-architecture-explained/</li>
<li>VGGNet - Wikipedia, https://en.wikipedia.org/wiki/VGGNet</li>
<li>VGGNet-16 Architecture: A Complete Guide - Kaggle, https://www.kaggle.com/code/blurredmachine/vggnet-16-architecture-a-complete-guide</li>
<li>arXiv:1409.4842v1 [cs.CV] 17 Sep 2014, https://arxiv.org/abs/1409.4842</li>
<li>Understanding GoogLeNet Model - CNN Architecture - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/understanding-googlenet-model-cnn-architecture/</li>
<li>Inception (deep learning architecture) - Wikipedia, https://en.wikipedia.org/wiki/Inception_(deep_learning_architecture)</li>
<li>GoogLeNet: A Deep Dive into Google’s Neural Network Technology | by Siddhesh Bangar, https://medium.com/@siddheshb008/googlenet-a-deep-dive-into-googles-neural-network-technology-f588d1b49e55</li>
<li>GoogLeNet: Revolutionizing Deep Learning with Inception - Viso Suite, https://viso.ai/deep-learning/googlenet-explained-the-inception-model-that-won-imagenet/</li>
<li>ImageNet: VGGNet, ResNet, Inception, and Xception with Keras - PyImageSearch, https://pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/</li>
<li>[1405.3531] Return of the Devil in the Details: Delving Deep into Convolutional Nets - arXiv, https://arxiv.org/abs/1405.3531</li>
<li>Top 137 papers presented at British Machine Vision Conference in 2014 - SciSpace, https://scispace.com/conferences/british-machine-vision-conference-1gt8zawu/2014</li>
<li>Proceedings of the British Machine Vision Conference 2014 - BMVA Archive, https://www.bmva-archive.org.uk/bmvc/2014/index.html</li>
<li>Computer Vision – ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I | Request PDF - ResearchGate, https://www.researchgate.net/publication/321615154_Computer_Vision_-_ECCV_2014_13th_European_Conference_Zurich_Switzerland_September_6-12_2014_Proceedings_Part_I</li>
<li>2014 – USC Iris Computer Vision Lab, https://sites.usc.edu/iris-cvlab/publication/2014-2/</li>
<li>Visual SLAM - 网络机器人与系统实验室NROS-Lab, https://www.nrs-lab.com/visual-slam/</li>
<li>Visual SLAM - LSD-SLAM: Large-Scale Direct Monocular SLAM - Computer Vision Group, https://cvg.cit.tum.de/research/vslam/lsdslam</li>
<li>Generative Adversarial Nets - arXiv, https://arxiv.org/abs/1406.2661</li>
<li>Ryan Wolcott Receives Best Student Paper Award at IROS 2014, https://cse.engin.umich.edu/stories/ryan-wolcott-receives-best-student-paper-award-at-iros-2014</li>
<li>Visual Localization within LIDAR Maps for Automated Urban Driving - University of Michigan, https://robots.engin.umich.edu/publications/rwolcott-2014a.pdf</li>
<li>Visual localization within LIDAR maps for automated urban driving - ResearchGate, https://www.researchgate.net/publication/289677903_Visual_localization_within_LIDAR_maps_for_automated_urban_driving</li>
<li>Visual Localization within LIDAR Maps for Automated Urban Driving (IROS 2014) - YouTube, https://www.youtube.com/watch?v=H86AyFgZCG8</li>
<li>[PDF] Visual localization within LIDAR maps for automated urban driving | Semantic Scholar, https://www.semanticscholar.org/paper/Visual-localization-within-LIDAR-maps-for-automated-Wolcott-Eustice/c6032fae7ba8f3a512c368ea002e8ffa28504e5f</li>
<li>LSD-SLAM: large-scale direct monocular SLAM | Request PDF - ResearchGate, https://www.researchgate.net/publication/319770169_LSD-SLAM_large-scale_direct_monocular_SLAM</li>
<li>LSD-SLAM: Large-Scale Direct Monocular SLAM - Jakob Engel, https://jakobengel.github.io/pdf/engel14eccv.pdf</li>
<li>tum-vision/lsd_slam: LSD-SLAM - GitHub, https://github.com/tum-vision/lsd_slam</li>
<li>[PDF] LSD-SLAM: Large-Scale Direct Monocular SLAM - Semantic Scholar, <a href="https://www.semanticscholar.org/paper/LSD-SLAM%3A-Large-Scale-Direct-Monocular-SLAM-Engel-Sch%C3%B6ps/c13cb6dfd26a1b545d50d05b52c99eb87b1c82b2">https://www.semanticscholar.org/paper/LSD-SLAM%3A-Large-Scale-Direct-Monocular-SLAM-Engel-Sch%C3%B6ps/c13cb6dfd26a1b545d50d05b52c99eb87b1c82b2</a></li>
<li>arxiv.org, https://arxiv.org/abs/1407.3501</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>