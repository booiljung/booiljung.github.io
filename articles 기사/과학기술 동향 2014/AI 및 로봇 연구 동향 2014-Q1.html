<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2014년 1분기 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2014년 1분기 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2014년 AI 및 로봇 연구 동향</a> / <span>2014년 1분기 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2014년 1분기 AI 및 로봇 연구 동향</h1>
<h2>1. 서론: 딥러닝 혁명의 서막</h2>
<p>2014년 1분기는 인공지능(AI) 역사에서 단순한 시간의 흐름이 아닌, 패러다임의 전환을 알리는 결정적 변곡점으로 기록된다. 이 시기는 수년간 학계에서 축적된 딥러닝 연구의 잠재력이 기술 산업의 전략적, 재정적 역량과 폭발적으로 결합하며 향후 10년간의 AI 개발 의제를 설정한 순간이었다. 본 보고서는 이 중추적인 시기를 심층적으로 분석하여, 당시의 주요 사건과 연구가 현대 AI 지형을 어떻게 형성했는지 규명하고자 한다.</p>
<p>보고서의 핵심 서사는 세 가지 주요 흐름을 중심으로 전개된다. 첫째, 거대 기술 기업인 구글이 순수 AI 연구 스타트업인 딥마인드를 인수한 사건의 상징적, 실질적 의미를 분석한다. 이는 AI 연구가 학문적 탐구를 넘어 산업의 핵심 동력으로 부상했음을 알리는 신호탄이었다. 둘째, 이 인수를 전략적으로 타당하게 만든 기술적 돌파구인 심층 Q-네트워크(Deep Q-Networks, DQN)를 해부한다. DQN은 AI가 가공되지 않은 고차원 감각 데이터로부터 직접 복잡한 과업을 학습할 수 있음을 증명하며, 심층 강화학습의 시대를 열었다. 셋째, 당시 AI 연구의 광범위한 맥락을 조망한다. 딥러닝 분야의 급격한 발전과, 그와는 대조적으로 로봇공학 분야에서 여전히 주류였던 전통적인 모델 기반 접근법을 비교 분석하여 당시 기술 지형의 단면을 제시한다.</p>
<p>본 보고서는 먼저 딥마인드 인수의 다층적 의미를 분석하고, 이어 그 기술적 근간이 된 DQN 알고리즘을 이론적, 수학적으로 깊이 파고든다. 이후 2014년 한 해 동안 AI 분야를 휩쓴 주요 연구 흐름과 당시 로봇공학 분야의 현주소를 살펴본 후, 최종적으로 2014년 1분기가 AI 역사에 남긴 심대한 유산을 평가하며 마무리한다. 아래 표는 2014년 한 해 동안 발표된 주요 연구들을 요약하여 보여준다.</p>
<table><thead><tr><th>논문 제목 (Paper Title)</th><th>주요 저자/기관 (Key Authors/Institution)</th><th>발표일 (Publication Date)</th><th>핵심 기여 (Key Contribution)</th></tr></thead><tbody>
<tr><td>Playing Atari with Deep Reinforcement Learning</td><td>V. Mnih, et al. / DeepMind</td><td>2013-12 (arXiv)</td><td>심층 Q-네트워크(DQN)를 통해 고차원 시각 입력으로부터 직접 제어 정책을 학습하는 최초의 심층 강화학습 모델 제시.</td></tr>
<tr><td>OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks</td><td>P. Sermanet, et al. / NYU</td><td>2014-02 (arXiv v4)</td><td>단일 합성곱 신경망을 사용하여 분류, 위치 파악, 탐지를 통합적으로 수행하는 효율적인 프레임워크 제안.</td></tr>
<tr><td>Generative Adversarial Nets</td><td>I. Goodfellow, et al. / U. Montreal</td><td>2014-06 (arXiv)</td><td>생성자와 판별자가 경쟁하는 적대적 학습 프레임워크를 통해 생성 모델링의 새로운 패러다임을 제시.</td></tr>
<tr><td>Neural Turing Machines</td><td>A. Graves, et al. / DeepMind</td><td>2014-10 (arXiv)</td><td>외부 메모리를 결합한 신경망을 통해 알고리즘 학습 및 추론 능력의 가능성을 탐구.</td></tr>
<tr><td>A Hierarchical Wavelet Decomposition for Continuous-Time SLAM</td><td>S. Anderson, et al. / Georgia Tech</td><td>2014 (ICRA)</td><td>로봇공학의 핵심 문제인 SLAM에 대해 고전적인 확률 및 기하학 기반 접근법의 정교화를 보여주는 대표적 연구.</td></tr>
</tbody></table>
<h2>2.  구글의 딥마인드 인수 - 산업 지형을 바꾼 이정표</h2>
<h3>2.1  인수 개요 및 재무적 분석</h3>
<p>2014년 1분기 AI 분야의 가장 중대한 사건은 1월 26일 구글이 영국의 AI 스타트업 딥마인드 테크놀로지스(DeepMind Technologies) 인수에 합의했다고 발표한 것이다.1 이 거래는 단순한 기업 인수를 넘어 AI의 미래에 대한 거대한 베팅이었다. 인수 가격은 약 4억 달러, 혹은 4억 파운드로 보도되었으며, 이는 당시 구글의 유럽 최대 규모 인수 중 하나였다.2 2013년 12월 기준 약 75명의 직원을 둔 비상장 회사에 지불된 이 막대한 금액은 시장에 큰 충격을 주었다.1 직원 1인당 가치가 500만 달러를 상회하는 이 평가는 딥마인드가 보유한 기술이나 제품보다는 그 인적 자본, 즉 세계 최고 수준의 연구진과 그들이 추구하는 비전의 가치를 인정한 ’어크하이얼(acqui-hire)’의 성격이 짙었음을 시사한다.</p>
<p>이 역사적인 거래의 중심에는 구글의 최고 경영자(CEO) 래리 페이지(Larry Page)가 있었다. 그가 직접 인수를 주도했다는 사실은 이 거래가 단순한 사업부 차원의 기술 확보가 아닌, 회사 전체의 미래를 건 최상위 수준의 전략적 결정이었음을 보여준다.4 딥마인드는 체스 영재 출신의 신경과학자 데미스 하사비스(Demis Hassabis), 셰인 레그(Shane Legg), 무스타파 술레이만(Mustafa Suleyman)이 2010년에 공동 창업한 회사로, 런던에 본사를 두고 있었다.5 인수 이후 딥마인드는 구글의 자회사로 편입되었고, 2015년 구글의 모회사 알파벳이 설립되면서 알파벳의 자회사로 재편되었다.3</p>
<h3>2.2  딥마인드의 기술적 비전과 전략적 가치</h3>
<p>구글이 4억 달러라는 거금을 투자한 대상은 딥마인드의 당장의 수익 모델이 아니었다. 그것은 “범용 학습 알고리즘(general-purpose learning algorithms)을 구축한다“는 딥마인드의 원대한 비전이었다.1 딥마인드는 기계학습의 최신 기술과 시스템 신경과학의 통찰을 결합하여 특정 작업에 국한되지 않는, 인간과 유사한 수준의 일반 지능을 구현하는 것을 목표로 삼았다.1 이는 당시 업계의 주류였던 특정 문제 해결을 위한 ‘좁은 AI(Narrow AI)’ 개발과는 궤를 달리하는 접근법이었다.</p>
<p>인수 당시 딥마인드는 시뮬레이션, 전자상거래, 그리고 게임 분야에서 초기 상업적 응용 프로그램을 개발하고 있었다.1 특히 그들의 AI가 아타리(Atari) 고전 게임에서 인간 전문가의 수준을 뛰어넘는 능력을 보여준 것은 그들의 기술적 비전이 단순한 구호가 아님을 증명하는 강력한 실증 사례였다.6 이 시연은 구글이 딥마인드의 잠재력을 확신하고 인수를 결정하게 된 결정적인 계기가 된 것으로 알려져 있다.4</p>
<p>구글의 입장에서 딥마인드 인수는 AI 및 로봇공학 분야의 패권을 장악하기 위한 전략적 포석이었다.1 당시 구글은 안드로이드의 아버지 앤디 루빈(Andy Rubin)의 주도 하에 7개의 기술 기업을 조용히 인수하며 로봇공학 분야에 대한 야심을 드러내고 있었다.4 딥마인드의 강화학습 및 딥러닝 전문성은 구글이 보유한 방대한 ’빅데이터’를 처리하고 그 안에서 가치를 창출하는 데 필수적인 역량으로 평가되었다.4 딥마인드의 기술은 구글의 검색, 광고, 자율주행차 등 핵심 사업 전반에 걸쳐 혁신을 가져올 잠재력을 지니고 있었다.</p>
<h3>2.3  인수가 AI 연구 생태계에 미친 파급 효과</h3>
<p>구글의 딥마인드 인수는 AI 연구 생태계 전반에 지대한 영향을 미쳤다. 첫째, AI 인재 전쟁을 극적으로 심화시켰다. 딥마인드는 구글뿐만 아니라 페이스북 등 다른 거대 기술 기업들과도 치열한 인재 영입 경쟁을 벌였으며, 2012년에는 페이스북의 인수 제안을 거절하기도 했다.4 이번 인수는 최고 수준의 AI 연구진의 몸값을 천정부지로 끌어올렸고, 기술 기업들이 유망한 스타트업을 통째로 인수하여 핵심 인력을 확보하는 추세를 가속화했다.</p>
<p>둘째, AI 윤리 문제를 수면 위로 끌어올리는 중요한 선례를 남겼다. 인수 계약의 일부로, 구글은 딥마인드의 기술이 오용되는 것을 방지하기 위한 ’AI 윤리 위원회(AI ethics board)’를 설립하는 데 동의했다.4 기업 인수 계약에 이러한 조항이 포함된 것은 매우 이례적인 일이었다. 이는 거래 당사자들이 개발 중인 기술이 단순한 게임 플레이어를 넘어 사회에 막대한 영향을 미칠 수 있는 범용 인공지능(AGI)으로 발전할 가능성을 인지하고 있었음을 보여준다. 이는 AGI의 잠재적 위험성에 대한 초기 인식을 드러내는 동시에, 기술 개발에 있어 윤리적 책임의 중요성을 환기시키는 계기가 되었다.</p>
<p>마지막으로, 이 인수는 장기적이고 근본적인 AI 연구의 상업적 가치와 전략적 중요성을 시장에 각인시켰다. 딥마인드에 책정된 막대한 가치는 AI 연구가 더 이상 학계의 전유물이 아니라, 기업의 미래를 좌우할 핵심 자산임을 명백히 했다. 이 사건 이후 벤처 캐피털과 기업들의 AI 분야에 대한 투자가 급증했으며, 이는 이후 AI 기술 발전의 중요한 재정적 기반이 되었다. 결국 딥마인드 인수는 단순한 M&amp;A를 넘어, AGI 개발 경쟁의 본격적인 시작을 알리고 AI 연구의 산업적, 윤리적, 재정적 지형을 근본적으로 바꾼 역사적 이정표였다.</p>
<h2>3.  심층 강화학습의 이론적 토대 - DQN 알고리즘</h2>
<h3>3.1  “Playing Atari with Deep Reinforcement Learning” 논문 심층 분석</h3>
<p>구글이 딥마인드의 미래 가치에 막대한 투자를 단행할 수 있었던 배경에는 “Playing Atari with Deep Reinforcement Learning“이라는 논문으로 대표되는 구체적인 기술적 성취가 있었다.7 이 논문은 2013년 12월 온라인 논문 저장소인 arXiv에 처음 공개되었지만, 2014년 1분기 딥마인드 인수가 발표되면서 그 중요성이 폭발적으로 부각되었다.7 딥마인드 소속 연구원들이 저술한 이 논문은 심층 강화학습(Deep Reinforcement Learning, DRL)이라는 새로운 분야의 가능성을 전 세계에 입증한 기념비적인 연구로 평가받는다.7</p>
<p>이 연구의 가장 핵심적인 기여는 AI가 <strong>고차원의 감각 입력(high-dimensional sensory input)으로부터 직접 제어 정책을 학습</strong>하는 최초의 딥러닝 모델을 제시했다는 점이다.7 이전의 AI 시스템들은 대부분 문제 해결을 위해 전문가가 직접 설계한 특징(hand-crafted features)에 의존했다. 예를 들어, 체스 게임 AI는 기물의 가치나 판의 위치적 이점과 같은 특징들을 사전에 정의해야 했다. 그러나 딥마인드의 AI는 아타리 게임의 원본 픽셀 데이터, 즉 210x160 크기의 컬러 비디오 화면을 아무런 가공 없이 입력받아 게임 방법을 스스로 터득했다.7 이는 AI가 인간처럼 시각적 정보만으로 복잡한 과업을 학습할 수 있는 길을 열었으며, 특징 공학(feature engineering)이라는 기존의 큰 장벽을 허물었다.</p>
<p>또 다른 중요한 성과는 <strong>단일한 범용 에이전트(generalist agent)를 구현</strong>했다는 점이다. 연구팀은 동일한 신경망 아키텍처와 하이퍼파라미터를 일곱 종류의 각기 다른 아타리 게임(Pong, Breakout, Space Invaders 등)에 그대로 적용했다.7 놀랍게도 이 단일 에이전트는 각각의 게임 규칙을 스스로 학습하여 높은 수준의 플레이를 선보였다. 이는 특정 게임에만 특화된 것이 아니라, 다양한 환경에 적응할 수 있는 일반화된 학습 능력을 갖추었음을 의미하며, 이는 범용 인공지능을 향한 중요한 진일보였다.</p>
<h3>3.2  심층 Q-네트워크(DQN) 아키텍처</h3>
<p>이러한 혁신을 가능하게 한 기술의 핵심은 심층 Q-네트워크(Deep Q-Network, DQN)라는 새로운 아키텍처였다. DQN은 심층 합성곱 신경망(Convolutional Neural Network, CNN)과 고전적인 강화학습 알고리즘인 Q-러닝(Q-learning)을 결합한 모델이다.7</p>
<p>모델의 구조는 다음과 같다. 먼저, 입력으로 연속된 몇 개의 게임 화면 프레임(raw game frames)이 들어온다. 이는 정적인 이미지 하나만으로는 공의 움직임이나 적의 이동 방향과 같은 동적인 정보를 파악할 수 없기 때문이다. 이 이미지 스택은 여러 개의 합성곱 계층으로 구성된 CNN을 통과한다. CNN은 이미지 인식 분야에서 뛰어난 성능을 입증한 모델로, 여기서는 게임 화면에서 공의 위치, 적 캐릭터, 벽돌 등과 같은 중요한 시각적 특징들을 자동으로 추출하는 역할을 수행한다.7</p>
<p>CNN을 통과하여 추출된 특징 맵(feature map)은 완전 연결 계층(fully-connected layers)을 거쳐 최종 출력층에 도달한다. 출력층은 에이전트가 해당 상태에서 취할 수 있는 모든 가능한 행동(예: 조이스틱을 위, 아래, 왼쪽, 오른쪽으로 움직이거나 버튼을 누르는 등)에 대한 Q-값(Q-value)을 출력한다.11 Q-값이란 특정 상태(state)에서 특정 행동(action)을 취했을 때 미래에 얻을 수 있는 총 보상(reward)의 기댓값을 의미한다. 에이전트의 정책(policy)은 매우 간단하다. 현재 상태에서 Q-값이 가장 높게 예측된 행동을 선택하는 것이다. 이 과정을 통해 DQN은 주어진 화면을 보고 어떤 행동을 하는 것이 가장 높은 점수로 이어질지를 학습하게 된다.</p>
<h3>3.3  핵심 혁신: 경험 재현 (Experience Replay)</h3>
<p>단순히 CNN과 Q-러닝을 결합하는 것만으로는 안정적인 학습이 불가능했다. 강화학습 환경에서 생성되는 데이터는 심각한 문제점을 내포하고 있었기 때문이다. 에이전트가 경험하는 연속적인 게임 프레임들은 서로 매우 유사하여 데이터 간의 상관관계(correlation)가 매우 높다. 이러한 데이터를 순서대로 신경망 학습에 사용하면, 독립적이고 동일하게 분포된(Independent and Identically Distributed, I.I.D.) 데이터를 가정하는 대부분의 최적화 알고리즘의 전제를 위반하게 된다. 이는 학습 과정을 매우 불안정하게 만들고, 최근 경험에 과적합되어 이전에 배운 내용을 잊어버리는 ‘파국적 망각(catastrophic forgetting)’ 현상을 초래할 수 있다.13</p>
<p>딥마인드는 이 문제를 해결하기 위해 생물학적 영감을 받은 ’경험 재현(Experience Replay)’이라는 기법을 도입했다.7 이 기법의 핵심은 에이전트의 경험을 즉시 학습에 사용하고 버리는 것이 아니라, 일단 메모리 버퍼(replay buffer)에 저장하는 것이다.16 에이전트의 각 경험은 <code>(현재 상태, 행동, 보상, 다음 상태)</code>의 튜플(tuple) 형태로 저장된다.</p>
<p>실제 신경망 가중치를 업데이트할 때는, 이 버퍼에서 무작위로 미니배치(minibatch) 크기만큼의 경험들을 샘플링하여 사용한다.18 과거의 다양한 시점에서 발생한 경험들을 무작위로 섞어서 학습함으로써 데이터 간의 시간적 상관관계를 효과적으로 깨뜨릴 수 있다. 이는 학습 데이터의 분포를 안정시키고, 학습 과정을 훨씬 더 안정적이고 효율적으로 만든다. 또한, 하나의 경험이 여러 번의 가중치 업데이트에 재사용될 수 있어 데이터 효율성을 극적으로 향상시키는 부수적인 효과도 가져온다.16 경험 재현은 심층 신경망과 강화학습의 불안정한 결합을 성공적으로 이끈 핵심적인 공학적 해결책이었다.</p>
<h3>3.4  수학적 원리: 손실 함수와 최적화</h3>
<p>DQN의 학습은 Q-러닝의 기본 원리인 벨만 방정식(Bellman equation)에 기반한 시간차 오차(Temporal Difference, TD error)를 최소화하는 방향으로 이루어진다.11 네트워크는 현재 예측하는 Q-값과, 더 정확한 추정치인 ‘목표 Q-값(target Q-value)’ 사이의 차이를 줄이도록 훈련된다.</p>
<p>손실 함수(Loss function)는 일반적으로 예측 Q-값과 목표 Q-값 사이의 평균 제곱 오차(Mean Squared Error, MSE)로 정의된다. 일부 구현에서는 잡음(noise)에 더 강건한 후버 손실(Huber loss)을 사용하기도 한다.11 목표 Q-값은 현재 받은 보상 <code>r</code>과, 다음 상태 <code>s'</code>에서 취할 수 있는 최선의 행동에 대한 Q-값의 할인된 가치의 합으로 계산된다: <span class="math math-inline">y_i = r + \gamma \max_{a&#39;} Q(s&#39;, a&#39;)</span>. 여기서 <span class="math math-inline">\gamma</span>는 미래 보상의 가치를 현재 가치로 할인하는 할인 계수(discount factor)이다.12</p>
<p>그러나 여기서 또 다른 불안정성 문제가 발생한다. Q-값을 예측하는 네트워크와 목표 Q-값을 계산하는 데 사용되는 네트워크가 동일할 경우, 매 업데이트마다 목표값이 계속해서 흔들리게 된다. 이는 마치 움직이는 과녁을 맞추려는 것과 같아서 학습을 불안정하게 만든다.13 딥마인드는 이 문제를 해결하기 위해 ’목표 네트워크(target network)’라는 별도의 네트워크를 도입했다. 온라인 네트워크(online network)의 가중치 <span class="math math-inline">\theta_i</span>는 매 스텝마다 업데이트되지만, 목표 네트워크의 가중치 <span class="math math-inline">\theta_i^-</span>는 일정 기간(예: 10,000 스텝) 동안 고정된다. 그리고 주기적으로 온라인 네트워크의 가중치가 목표 네트워크로 복사된다.20 이를 통해 한동안 안정된 목표값을 가지고 학습을 진행할 수 있게 되어 학습의 안정성이 크게 향상된다.</p>
<p>이를 종합한 DQN의 손실 함수 <span class="math math-inline">L_i(\theta_i)</span>는 수학적으로 다음과 같이 표현된다.</p>
<p><span class="math math-display">
L_i(\theta_i) = \mathbb{E}_{(s,a,r,s&#39;) \sim U(D)} \left[ \left( r + \gamma \max_{a&#39;} Q(s&#39;, a&#39;; \theta_i^-) - Q(s, a; \theta_i) \right)^2 \right]
</span></p>
<p>이 손실 함수를 확률적 경사 하강법(Stochastic Gradient Descent, SGD)과 같은 최적화 알고리즘을 사용하여 최소화함으로써 네트워크의 가중치 <span class="math math-inline">\theta_i</span>가 업데이트된다. 아래 표는 손실 함수를 구성하는 각 요소의 의미를 상세히 설명한다.</p>
<table><thead><tr><th>기호 (Symbol)</th><th>명칭 (Name)</th><th>설명 (Description)</th></tr></thead><tbody>
<tr><td><span class="math math-inline">s</span>, <span class="math math-inline">s&#39;</span></td><td>상태 (State)</td><td>에이전트의 현재 및 다음 관측. DQN에서는 스택된 게임 화면의 픽셀 데이터에 해당한다.</td></tr>
<tr><td><span class="math math-inline">a</span>, <span class="math math-inline">a&#39;</span></td><td>행동 (Action)</td><td>에이전트가 현재 상태에서 취하는 행동 및 다음 상태에서 취할 수 있는 모든 행동.</td></tr>
<tr><td><span class="math math-inline">r</span></td><td>보상 (Reward)</td><td>행동 <span class="math math-inline">a</span>를 수행한 후 환경으로부터 받는 즉각적인 스칼라 신호. 게임 점수의 변화에 해당한다.</td></tr>
<tr><td><span class="math math-inline">\gamma</span></td><td>할인 계수 (Discount Factor)</td><td>미래 보상의 현재 가치를 결정하는 <span class="math math-inline">0</span>과 <span class="math math-inline">1</span> 사이의 상수. <span class="math math-inline">1</span>에 가까울수록 장기적인 보상을 중시한다.</td></tr>
<tr><td><span class="math math-inline">Q(s, a; \theta_i)</span></td><td>예측 Q-값 (Predicted Q-Value)</td><td>현재 파라미터 <span class="math math-inline">\theta_i</span>를 가진 온라인 신경망이 예측하는 상태 <span class="math math-inline">s</span>에서 행동 <span class="math math-inline">a</span>의 가치.</td></tr>
<tr><td><span class="math math-inline">y_i = r + \gamma \max_{a&#39;} Q(s&#39;, a&#39;; \theta_i^-)</span></td><td>목표 Q-값 (Target Q-Value)</td><td>TD 목표. 파라미터 <span class="math math-inline">\theta_i^-</span>를 가진 별도의 고정된 목표 신경망을 사용하여 계산되어 학습 안정성을 높인다.</td></tr>
<tr><td><span class="math math-inline">L_i(\theta_i)</span></td><td>손실 함수 (Loss Function)</td><td>예측 Q-값과 목표 Q-값 사이의 평균 제곱 오차. 이 손실을 최소화하도록 신경망 가중치가 업데이트된다.</td></tr>
</tbody></table>
<h3>3.5  성능 및 영향</h3>
<p>DQN 에이전트의 성능은 경이로웠다. 실험에 사용된 7개의 아타리 게임 중 6개에서 기존의 모든 강화학습 알고리즘을 능가하는 성과를 보였다.7 특히 브레이크아웃(Breakout), 엔듀로(Enduro), 퐁(Pong) 세 게임에서는 전문적인 인간 게이머의 수준을 뛰어넘는 ‘초인적인(superhuman)’ 플레이를 선보였다.6 브레이크아웃 게임에서 에이전트가 벽돌 뒤로 터널을 뚫어 공을 위로 보낸 후 최소한의 움직임으로 최대 점수를 획득하는 전략을 스스로 발견한 것은 매우 유명한 일화다.</p>
<p>DQN의 성공은 AI 연구의 흐름을 바꾸어 놓았다. 이는 심층 학습 모델이 복잡한 순차적 의사 결정 문제를 해결할 수 있는 강력한 도구임을 입증한 분수령이었다. 이 연구는 이전에 해결 불가능하다고 여겨졌던 많은 문제에 딥러닝을 적용할 수 있는 가능성을 열었으며, 이후 딥마인드가 개발한 알파고(AlphaGo)와 같은 더욱 정교한 AI 시스템의 이론적, 기술적 토대가 되었다.3 DQN은 단순히 게임을 잘하는 AI를 만든 것을 넘어, 딥러닝과 강화학습의 성공적인 융합을 통해 인공지능의 새로운 시대를 연 핵심적인 기술적 성취였다.</p>
<h2>4.  2014년 AI 연구의 주요 흐름</h2>
<p>2014년 1분기는 DQN의 부상으로 특징지어지지만, 그 해 전체는 딥러닝이 인식(perception)의 영역을 넘어 행동(action)과 창조(creation)의 영역으로 야심차게 확장해 나간 한 해였다. Q1 이후 발표된 주요 연구들은 이러한 흐름을 명확히 보여주며, 당시 AI 분야에 불어닥친 혁신적인 모멘텀을 증명한다.</p>
<h3>4.1  컴퓨터 비전의 발전</h3>
<p>2014년 초 컴퓨터 비전 분야에서도 중요한 진전이 있었다. 2013년 12월에 처음 공개되고 2014년 2월에 개정판이 제출된 “OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks” 논문은 주목할 만한 성과였다.21 이 연구의 핵심 기여는 이미지 분류(classification), 객체 위치 파악(localization), 객체 탐지(detection)라는 컴퓨터 비전의 세 가지 핵심 과업을 <strong>단일 합성곱 신경망(CNN) 프레임워크 내에서 통합적으로 처리</strong>할 수 있음을 보여준 것이다.</p>
<p>이전까지 이 세 가지 과업은 보통 별개의 시스템으로 처리되었다. 그러나 OverFeat는 다중 스케일(multiscale) 분석과 슬라이딩 윈도우(sliding window) 기법을 CNN 내에서 효율적으로 구현함으로써, 하나의 네트워크가 이미지 안에 ‘무엇이 있는지’(분류), ‘그것이 어디에 있는지’(위치 파악), 그리고 ‘여러 객체들의 경계 상자는 어디인지’(탐지)를 동시에 예측할 수 있게 했다. 이 통합된 접근 방식은 컴퓨터 비전 시스템의 효율성과 성능을 한 단계 끌어올리는 중요한 진전이었다.</p>
<h3>4.2  2014년의 기술적 지평: Q1 이후의 주요 연구 예고</h3>
<p>본 절에서 다루는 연구들은 2014년 1분기 이후에 발표되었으나, 1분기에 시작된 기술적 모멘텀이 어떤 놀라운 결과로 이어졌는지를 보여주기 위해 포함되었다. 이들은 2014년이 AI 역사상 얼마나 생산적인 해였는지를 보여주는 증거이다.</p>
<h4>4.2.1 생성적 적대 신경망 (Generative Adversarial Networks - GANs)</h4>
<p>2014년 6월, 이안 굿펠로우(Ian Goodfellow)와 그의 동료들은 arXiv에 “Generative Adversarial Nets“라는 논문을 제출하며 생성 모델링 분야에 혁명을 일으켰다.22 GAN은 두 개의 신경망이 서로 경쟁하며 학습하는 독창적인 프레임워크를 제안했다.</p>
<ul>
<li>
<p><strong>생성자(Generator):</strong> 무작위 잡음(random noise)을 입력받아 실제 데이터와 유사한 가짜 데이터를 생성하는 역할을 한다. 예를 들어, 실제 사람 얼굴 이미지와 비슷해 보이는 가짜 얼굴 이미지를 만들어낸다.</p>
</li>
<li>
<p><strong>판별자(Discriminator):</strong> 생성자가 만든 가짜 데이터와 실제 데이터를 입력받아, 이것이 진짜인지 가짜인지를 판별하는 역할을 한다.</p>
</li>
</ul>
<p>이 두 네트워크는 게임 이론에 기반한 제로섬 게임(zero-sum game)을 벌인다. 생성자는 판별자를 속이기 위해 점점 더 진짜 같은 데이터를 만들도록 학습하고, 판별자는 생성자에게 속지 않기 위해 진짜와 가짜를 구별하는 능력을 더욱 정교하게 발전시킨다. 이 적대적 과정이 반복되면서, 결국 생성자는 매우 사실적인 고품질 데이터를 생성할 수 있게 된다.24 GAN의 등장은 이후 이미지 생성, 스타일 변환, 데이터 증강 등 수많은 응용 분야를 탄생시킨 생성 AI 시대의 서막을 열었다.</p>
<h4>4.2.2 신경 튜링 머신 (Neural Turing Machines - NTMs)</h4>
<p>2014년 10월, 딥마인드의 알렉스 그레이브스(Alex Graves) 연구팀은 “Neural Turing Machines“라는 논문을 발표하며 또 다른 방향의 혁신을 제시했다.25 이 연구는 전통적인 신경망의 한계, 즉 명시적인 외부 기억 장치의 부재를 극복하려는 시도였다.</p>
<p>NTM은 순환 신경망(Recurrent Neural Network, RNN)을 컨트롤러로 사용하고, 여기에 외부 메모리 뱅크(memory bank)를 결합한 구조를 가진다.27 핵심 아이디어는 신경망이 이 외부 메모리에 정보를 ‘쓰고(write)’ ‘읽는(read)’ 방법을 학습하도록 하는 것이다. 미분 가능한 어텐션(attention) 메커니즘을 통해, 네트워크는 특정 메모리 위치에 집중하여 정보를 읽거나 수정할 수 있다. 이는 컴퓨터의 작동 방식과 유사하게, 신경망이 장기적인 정보를 저장하고 필요할 때 꺼내 쓸 수 있는 능력을 부여한다. NTM은 복사나 정렬과 같은 간단한 알고리즘적 과업을 수행할 수 있음을 보여주었으며, 이는 학습 기반의 접근법과 기호적 추론을 결합하려는 초기 시도로서 큰 의미를 가졌다.6</p>
<p>DQN이 AI에게 ’행동’하는 법을, GAN이 ’창조’하는 법을 가르쳤다면, NTM은 AI에게 ’기억하고 추론’하는 능력의 가능성을 제시했다. 이 세 가지 연구는 2014년 한 해 동안 딥러닝이 단순한 패턴 인식을 넘어 훨씬 더 복잡하고 고차원적인 지능의 영역으로 나아가고 있음을 명확히 보여주었다.</p>
<h2>5.  2014년 로봇공학 연구 동향</h2>
<p>2014년 1분기 딥러닝 분야에서 혁명적인 변화가 일어나는 동안, 로봇공학 분야는 상대적으로 다른 경로를 걷고 있었다. 당시 로봇공학 연구의 주류는 딥러닝과 같은 데이터 기반의 종단간(end-to-end) 학습 방식보다는, 기하학과 확률 이론에 깊이 뿌리내린 모델 기반(model-based) 접근법을 정교화하는 데 집중되어 있었다. 이는 AI 혁명과 로봇공학 분야가 당시에는 아직 완전히 융합되지 않은, 별개의 트랙에서 발전하고 있었음을 보여준다.</p>
<h3>5.1  주요 학술대회(ICRA &amp; IROS)의 역할</h3>
<p>로봇공학 분야의 최신 연구 동향을 파악하는 가장 좋은 방법은 해당 분야의 양대 최고 학술대회인 ICRA(International Conference on Robotics and Automation)와 IROS(International Conference on Intelligent Robots and Systems)를 살펴보는 것이다.28 2014년 ICRA는 5월 말에서 6월 초 홍콩에서, IROS는 9월 시카고에서 개최되었다.29 비록 이들 학회가 1분기 이후에 열렸지만, 여기에 발표된 논문들은 2013년 말부터 2014년 초까지 진행된 연구 성과를 집대성한 것이므로, 당시 로봇공학 분야의 기술적 현주소를 가장 잘 반영하는 지표라 할 수 있다.</p>
<h3>5.2  당시의 주요 연구 분야</h3>
<p>당시 최고 수준의 로봇공학 연구 그룹 중 하나였던 조지아 공과대학교 프랭크 델라트(Frank Dellaert) 연구실의 2014년 ICRA 및 IROS 발표 논문 목록은 당시의 연구 지형을 잘 보여주는 사례다.29 이들의 연구 주제는 다음과 같은 분야에 집중되어 있었다.</p>
<ul>
<li>
<p><strong>동시적 위치 추정 및 지도 작성 (Simultaneous Localization and Mapping, SLAM):</strong> 로봇이 미지의 환경을 탐색하면서 자신의 위치를 추정하고 동시에 주변 환경의 지도를 작성하는 기술이다. 델라트 연구실은 연속 시간 SLAM을 위한 계층적 웨이블릿 분해(Hierarchical Wavelet Decomposition)나, 객체 발견 및 모델링을 통합한 SLAM과 같은 연구를 발표했다.29</p>
</li>
<li>
<p><strong>인자 그래프 최적화 (Factor Graph Optimization):</strong> SLAM을 비롯한 로봇공학의 수많은 추정 문제들을 인자 그래프라는 통일된 프레임워크 안에서 효율적으로 풀기 위한 연구이다. 조건부 독립 집합 제거, 볼록 완화(convex relaxation)를 통한 강건한 추정, 증분적 부그래프 사전조건화(Incremental Subgraph-Preconditioned)와 같은 수학적, 알고리즘적 기법들이 주를 이루었다.29</p>
</li>
<li>
<p><strong>불확실성 하에서의 경로 계획 (Planning Under Uncertainty):</strong> 센서 측정과 로봇의 움직임에 내재된 불확실성을 고려하여 최적의 경로를 계획하는 문제이다. 일반화된 믿음 공간(Generalized Belief Space) 접근법과 같은 연구가 이에 해당한다.29</p>
</li>
<li>
<p><strong>다개체 로봇 시스템 (Multi-Robot Systems):</strong> 여러 대의 로봇이 협력하여 공동의 목표를 달성하기 위한 기술로, 초기 상대 위치를 모르는 상황에서의 다개체 로봇 위치 추정 및 데이터 연관 문제 등이 연구되었다.29</p>
</li>
</ul>
<p>이러한 연구 주제들의 공통점은 명시적인 수학적 모델에 기반한다는 것이다. SLAM, 경로 계획, 상태 추정 등은 모두 로봇의 기구학(kinematics), 동역학(dynamics), 센서 모델, 환경의 기하학적 구조 등을 수학적으로 정밀하게 모델링하고, 이를 바탕으로 확률 이론과 최적화 이론을 적용하여 해를 구하는 방식으로 접근한다. 이는 DQN이 게임의 물리 법칙이나 규칙에 대한 어떠한 사전 정보도 없이 오직 픽셀 데이터와 점수라는 보상 신호만으로 학습하는 모델-프리(model-free) 방식과는 극명한 대조를 이룬다.</p>
<p>결론적으로 2014년 1분기, AI 분야는 딥러닝을 통해 가상 환경에서 모델 없이 학습하는 능력의 새로운 지평을 열고 있었던 반면, 로봇공학 분야는 물리 세계의 복잡성과 불확실성을 다루기 위해 정교한 모델 기반의 확률적, 기하학적 방법론을 발전시키는 데 주력하고 있었다. 두 위대한 분야의 본격적인 융합, 즉 심층 강화학습의 원리를 물리적 로봇에 적용하여 현실 세계의 문제를 해결하려는 시도는 이 시점 이후의 과제로 남겨져 있었다.</p>
<h2>6. 결론: 2014년 1분기가 남긴 유산</h2>
<p>2014년 1분기는 인공지능의 발전 경로를 영구적으로 바꾼 분기점이었다. 이 시기에 일어난 사건과 기술적 돌파구들은 이후 10년간의 AI 연구와 산업의 방향을 결정짓는 근본적인 토대를 마련했다.</p>
<p>본 보고서의 분석을 종합하면, 이 시기의 유산은 세 가지 핵심적인 축으로 요약될 수 있다. 첫째, <strong>구글의 딥마인드 인수는 순수 연구와 거대 자본의 결합을 상징</strong>하며 AI를 산업의 중심으로 끌어올렸다. 이는 AGI라는 원대한 목표가 상업적으로 유효한 투자 대상임을 입증했으며, 전 세계적인 AI 인재 전쟁과 투자 붐을 촉발했다. 또한, AI 윤리 위원회 설립이라는 선례를 통해 기술 발전의 사회적 책임에 대한 논의를 본격화했다.</p>
<p>둘째, <strong>DQN 알고리즘은 심층 강화학습의 기술적 실현 가능성을 증명</strong>했다. 경험 재현과 목표 네트워크라는 독창적인 공학적 해결책을 통해, DQN은 불안정성이라는 오랜 난제를 극복하고 심층 신경망과 강화학습의 성공적인 결합을 이뤄냈다. 고차원 시각 정보로부터 직접 행동을 학습하는 범용 에이전트의 등장은, AI가 가상 세계의 복잡한 문제를 해결할 수 있음을 보여준 결정적 증거였다.</p>
<p>셋째, <strong>2014년은 AI와 로봇공학이 각자의 정점에서 평행하게 발전하던 시기</strong>로 기록된다. 딥러닝이 DQN, GAN, NTM 등을 통해 행동, 창조, 추론의 영역으로 급격히 팽창하는 동안, 로봇공학은 SLAM과 같은 물리 세계의 근본적인 문제들을 해결하기 위해 모델 기반의 정교한 수학적 방법론을 심화시키고 있었다. 이 두 분야의 간극은 당시 기술 지형의 단면을 보여주며, 동시에 미래의 융합 연구에 대한 거대한 과제를 제시했다.</p>
<p>결론적으로 2014년 1분기는 미래를 향한 씨앗이 뿌려진 시기였다. DQN에서 증명된 대규모 학습, 경험의 재사용, 인식과 행동의 통합이라는 원칙은 이후 알파고의 경이로운 성공, 방대한 텍스트 데이터를 학습 데이터셋(사실상의 거대한 경험 재현 버퍼)으로 사용하는 거대 언어 모델(LLM), 그리고 현대 자율 시스템의 개념적 조상이 되었다. 2014년 1분기는 단순히 몇 가지 중요한 사건이 일어난 시기가 아니라, 오늘날 우리가 경험하고 있는 AI 시대의 초석이 놓인, 진정한 의미의 ’시작점’이었다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>List of mergers and acquisitions by Alphabet - Wikipedia, https://en.wikipedia.org/wiki/List_of_mergers_and_acquisitions_by_Alphabet</li>
<li>Google reportedly acquires AI company DeepMind for $400M - CNET, https://www.cnet.com/tech/services-and-software/google-reportedly-acquires-ai-company-deepmind-for-400m/</li>
<li>Google’s Acquisition of DeepMind, Five Years Later – pascal-network.org, http://www.pascal-network.org/googles-acquisition-of-deepmind-five-years-later/</li>
<li>Google buys UK artificial intelligence startup Deepmind for £400m …, https://www.theguardian.com/technology/2014/jan/27/google-acquires-uk-artificial-intelligence-startup-deepmind</li>
<li>From academia to industry: The story of Google DeepMind - DROPS, https://drops.dagstuhl.de/storage/01oasics/oasics-vol043-iccsw2014/OASIcs.ICCSW.2014.1/OASIcs.ICCSW.2014.1.pdf</li>
<li>Google DeepMind - Wikipedia, https://en.wikipedia.org/wiki/Google_DeepMind</li>
<li>Playing Atari with Deep Reinforcement Learning, https://arxiv.org/abs/1312.5602</li>
<li>[PDF] Playing Atari with Deep Reinforcement Learning - Semantic Scholar, https://www.semanticscholar.org/paper/Playing-Atari-with-Deep-Reinforcement-Learning-Mnih-Kavukcuoglu/2319a491378867c7049b3da055c5df60e1671158</li>
<li>Playing Atari with Deep Reinforcement Learning - ResearchGate, https://www.researchgate.net/publication/259367763_Playing_Atari_with_Deep_Reinforcement_Learning</li>
<li>[research] Playing Atari with Deep Reinforcement Learning - YouTube, https://www.youtube.com/watch?v=QOvKhKnRbho</li>
<li>Reinforcement Learning (DQN) Tutorial - PyTorch documentation, https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html</li>
<li>How Are Neural Networks Used in Deep Q-Learning? - GeeksforGeeks, https://www.geeksforgeeks.org/deep-learning/how-are-neural-networks-used-in-deep-q-learning/</li>
<li>The Deep Q-Learning Algorithm - Hugging Face Deep RL Course, https://huggingface.co/learn/deep-rl-course/unit3/deep-q-algorithm</li>
<li>Q-learning - Wikipedia, https://en.wikipedia.org/wiki/Q-learning</li>
<li>Why random sample from replay for DQN? - Data Science Stack Exchange, https://datascience.stackexchange.com/questions/24921/why-random-sample-from-replay-for-dqn</li>
<li>What is “experience replay” and what are its benefits? - Data Science Stack Exchange, https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits</li>
<li>Deep Reinforcement Learning with Experience Replay | by Hey Amit - Medium, https://medium.com/@heyamit10/deep-reinforcement-learning-with-experience-replay-1222ea711897</li>
<li>What is the role of experience replay in deep reinforcement learning? - Milvus, https://milvus.io/ai-quick-reference/what-is-the-role-of-experience-replay-in-deep-reinforcement-learning</li>
<li>Experience Replay with Random Reshuffling - arXiv, https://arxiv.org/html/2503.02269v1</li>
<li>Deep Q-Networks Explained - LessWrong, https://www.lesswrong.com/posts/kyvCNgx9oAwJCuevo/deep-q-networks-explained</li>
<li>OverFeat: Integrated Recognition, Localization and Detection using …, https://arxiv.org/abs/1312.6229</li>
<li>Generative adversarial network - Wikipedia, https://en.wikipedia.org/wiki/Generative_adversarial_network</li>
<li>[1406.2661] Generative Adversarial Networks - arXiv, https://arxiv.org/abs/1406.2661</li>
<li>Generative Adversarial Networks and Its Applications in Biomedical Informatics - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC7235323/</li>
<li>Neural Turing machine - Wikipedia, https://en.wikipedia.org/wiki/Neural_Turing_machine</li>
<li>[1410.5401] Neural Turing Machines - arXiv, https://arxiv.org/abs/1410.5401</li>
<li>Neural Turing Machine Definition | DeepAI, https://deepai.org/machine-learning-glossary-and-terms/neural-turing-machine</li>
<li>International Conference on Intelligent Robots and Systems - Wikipedia, https://en.wikipedia.org/wiki/International_Conference_on_Intelligent_Robots_and_Systems</li>
<li>11 Papers in ICRA/IROS 2014, https://sites.cc.gatech.edu/home/dellaert/FrankDellaert/Publications/Entries/2014/6/28_ICRA_and_IROS_2014.html</li>
<li>IEEE/RSJ International Conference on Intelligent Robots and Systems Chicago, Illinois Sept. 14–18, 2014, https://ewh.ieee.org/soc/ras/conf/financiallycosponsored/IROS/2014/</li>
<li>ICRA/IROS Acceptance Rate History, https://staff.aist.go.jp/k.koide/acceptance-rate.html</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>