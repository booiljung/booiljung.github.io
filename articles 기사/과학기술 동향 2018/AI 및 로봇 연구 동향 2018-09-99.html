<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2018년 9월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2018년 9월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2018년 AI 및 로봇 연구 동향</a> / <span>2018년 9월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2018년 9월 AI 및 로봇 연구 동향</h1>
<h2>1.  2018년 9월 AI 및 로봇공학 연구 지형 개관</h2>
<p>2018년 9월은 인공지능(AI) 및 로봇공학 분야가 기초 딥러닝 기술의 폭발적인 성공을 넘어, 이를 현실 세계의 복잡한 문제에 적용하고 정교화하는 중대한 전환기에 위치한 시점이었다. 이 시기의 연구 지형은 학문적 탐구를 넘어 막대한 경제적 기대감과 산업적 요구가 직접적으로 반영되는 역동적인 피드백 루프를 형성하고 있었다.</p>
<p>당시의 거시적 맥락을 이해하기 위해, 2018년 9월 발간된 맥킨지 글로벌 인스티튜트(McKinsey Global Institute)의 보고서를 주목할 필요가 있다. 이 보고서는 AI 기술이 2030년까지 전 세계 경제에 약 13조 달러의 추가적인 경제적 가치를 창출할 잠재력이 있다고 예측했다.1 이는 AI 기술의 상업적 성공에 대한 기대가 최고조에 달했음을 보여주며, 동시에 기술 선도국과 후발 주자, 그리고 시장 선점 기업과 추격 기업 간의 격차가 심화될 수 있음을 경고했다. 이러한 경쟁 구도는 학계의 연구 방향을 더욱 실용적이고 성능 중심적으로 이끄는 강력한 동인으로 작용했다.</p>
<p>같은 시기, 세계경제포럼(World Economic Forum)은 “일자리의 미래” 보고서를 통해 2025년까지 기계가 인간보다 더 많은 업무를 수행하게 될 것이며, 2022년까지 7,500만 개의 일자리가 대체되는 동시에 1억 3,300만 개의 새로운 일자리가 창출될 것이라고 전망했다.2 이 예측은 AI 기술 발전이 가져올 사회적 파장에 대한 깊은 논의를 촉발했으며, 인간-AI 협업, 안전성, 그리고 노동력의 재교육(reskilling)과 같은 주제를 연구의 핵심 과제로 부상시켰다.</p>
<p>산업 현장에서는 이미 변화가 가시화되고 있었다. 로봇산업협회(Robotics Industries Association, RIA)의 2018년 3분기까지의 데이터에 따르면, 산업용 로봇 판매는 비(非)자동차 분야에서 기록적인 성장을 보였다. 특히 생명 과학, 식음료 및 소비재와 같은 분야에서 로봇 도입이 급증하며, 전통적인 제조업을 넘어 다양한 산업으로 로봇 기술이 확산되고 있음을 명확히 보여주었다.3 이는 더욱 유연하고 지능적인 로봇 시스템에 대한 새로운 기술적 요구를 창출했다.</p>
<p>이러한 경제적, 사회적, 산업적 배경은 2018년 9월에 발표된 학술 연구들이 단순한 학문적 호기심의 발로가 아님을 시사한다. 맥킨지 보고서가 예측한 막대한 경제적 가치와 RIA 데이터가 보여준 산업 자동화의 확산은 학계가 풀어야 할 구체적인 문제들을 제시했다. 예를 들어, 단일 RGB 카메라만으로 객체의 6차원 자세를 정확히 추정하는 기술이나, 미지의 환경에서 안정적으로 주행하는 시각-관성 항법 기술 등은 이러한 산업적 요구에 직접적으로 부응하는 핵심 연구 주제였다. 결국, 2018년 9월의 연구 동향은 거대한 시장의 견인력과 사회적 요구가 학문적 탐구와 결합하여 기술 발전의 방향을 설정하던 중요한 순간을 포착하고 있다.</p>
<h2>2.  주요 학술대회 동향: ECCV 2018 및 IROS 2018</h2>
<p>2018년 9월의 AI 및 로봇공학 연구 동향을 파악하기 위해서는 당시에 개최되었거나 주요 발표가 이루어진 핵심 학술대회를 분석하는 것이 필수적이다. 이 시기에는 컴퓨터 비전 분야의 최고 권위 학회인 ECCV와 지능형 로봇 분야의 대표 학회인 IROS가 중심에 있었다.</p>
<h3>2.1 ECCV 2018 개요</h3>
<p>제15회 유럽 컴퓨터 비전 학술대회(European Conference on Computer Vision, ECCV)는 2018년 9월 8일부터 14일까지 독일 뮌헨에서 개최되었다.4 ECCV는 CVPR, ICCV와 함께 컴퓨터 비전 분야의 3대 최상위 학회로 꼽히며, 2년마다 열리는 행사이다.7 2018년 대회는 역대 최대 규모로 치러졌으며, 2년 전 대회의 두 배에 달하는 3,200명 이상의 연구자가 참석했다.7 이는 당시 컴퓨터 비전 분야의 연구가 얼마나 폭발적으로 성장하고 있었는지를 단적으로 보여주는 지표이다. 총 2,439편의 논문이 제출되어 그중 776편(구두 발표 59편, 포스터 발표 717편)이 채택되었으며, 31.8%의 경쟁률 높은 채택률을 기록했다.7 방대한 연구 결과를 담은 프로시딩은 총 16권의 볼륨으로 출판되었다.9</p>
<h3>2.2 IROS 2018 개요</h3>
<p>2018년 IEEE/RSJ 지능형 로봇 및 시스템 국제 학술대회(International Conference on Intelligent Robots and Systems, IROS)는 2018년 10월 1일부터 5일까지 스페인 마드리드에서 개최되었다.10 비록 개최일은 10월이었지만, 학회의 전체 기술 프로그램이 9월 14일에 공개되고, 주요 내용을 요약한 다이제스트가 9월 28일에 배포되는 등 핵심적인 연구 발표가 9월에 집중적으로 이루어졌다.10 따라서 IROS 2018의 연구 동향은 본 보고서의 분석 범위에 포함된다. “로봇 사회를 향하여(Towards a Robotic Society)“라는 주제로 열린 이 학회는 로봇 기술이 통제된 산업 환경을 넘어 일상적인 인간 환경으로 확산되는 시대적 흐름을 반영했다.10 약 4,000명의 연구자가 참석했으며, 총 3,645편의 논문이 제출되는 등 방대한 규모의 기술 교류가 이루어졌다.13</p>
<p>아래 표는 두 학회의 핵심적인 정보를 요약한 것이다.</p>
<table><thead><tr><th>학회명</th><th>개최 기간</th><th>개최지</th><th>주요 통계</th></tr></thead><tbody>
<tr><td>ECCV 2018</td><td>2018년 9월 8일 - 14일</td><td>독일, 뮌헨</td><td>제출 논문: 2,439편, 채택 논문: 776편 (채택률 31.8%) 7</td></tr>
<tr><td>IROS 2018</td><td>2018년 10월 1일 - 5일 (주요 발표: 9월)</td><td>스페인, 마드리드</td><td>참가자: 약 4,000명, 제출 논문: 3,645편 13</td></tr>
</tbody></table>
<p>이처럼 ECCV와 IROS는 2018년 9월, 각각 컴퓨터 비전과 로봇공학 분야에서 가장 활발하고 경쟁적인 연구가 발표되는 핵심적인 장이었으며, 이들 학회에서 발표된 연구들은 당시 기술의 최전선을 명확하게 보여준다.</p>
<h2>3.  컴퓨터 비전 분야의 혁신: ECCV 2018 주요 연구 심층 분석</h2>
<p>ECCV 2018에서는 딥러닝 기술의 성숙과 함께 기존 방법론의 한계를 극복하려는 혁신적인 연구들이 다수 발표되었다. 특히 최우수 논문상과 우수 논문상을 받은 연구들은 각각 데이터 효율성, 모델의 안정성, 그리고 생성 모델의 제어 가능성이라는 핵심적인 주제를 다루며 향후 연구 방향에 큰 영향을 미쳤다.</p>
<h3>3.1  최우수 논문: 암시적 3D 방향 학습을 통한 6D 객체 자세 추정 (Implicit 3D Orientation Learning for 6D Object Detection from RGB Images)</h3>
<p>이 연구는 독일 항공우주 센터(German Aerospace Center)와 뮌헨 공과대학교(Technical University of Munich)의 Martin Sundermeyer 연구팀에 의해 발표되었으며, ECCV 2018 최우수 논문상을 수상했다.7</p>
<h4>3.1.1 문제 제기</h4>
<p>로봇 조작(manipulation)이나 증강 현실과 같은 응용 분야에서 객체의 정확한 6차원(6D) 자세(3차원 위치 + 3차원 회전) 추정은 필수적인 기술이다. 그러나 기존 딥러닝 기반 접근법들은 6D 자세 정보가 정확하게 레이블링된 대규모의 실제 이미지 데이터셋을 필요로 했다.17 이러한 데이터셋을 구축하는 것은 막대한 시간과 비용이 소요되는 매우 어려운 작업으로, 기술 발전의 주요 병목 현상으로 작용했다. 또한, 3차원 회전(<span class="math math-inline">SO(3)</span>)을 쿼터니언(quaternion)이나 오일러 각(Euler angles)과 같은 명시적인 방식으로 표현할 경우, 대칭성을 가진 객체에서는 심각한 문제가 발생한다. 시각적으로는 완전히 동일해 보이는 이미지라도 여러 개의 서로 다른 회전 값으로 레이블링될 수 있어, 신경망의 학습 과정을 교란시키는 ‘일대다(one-to-many)’ 매핑 문제를 야기했다.17</p>
<h4>3.1.2 방법론: 증강 오토인코더 (Augmented Autoencoder, AAE)</h4>
<p>이 연구는 이러한 문제들을 해결하기 위해 혁신적인 접근법을 제시했다. 먼저 일반적인 2D 객체 탐지기를 이용해 이미지에서 객체의 위치를 찾은 뒤, 잘라낸 이미지 영역에 대해 3차원 방향을 추정하는 파이프라인을 제안했다.17 핵심적인 혁신은 3차원 방향 추정에 사용된 **증강 오토인코더(Augmented Autoencoder, AAE)**라는 새로운 네트워크 구조에 있다.</p>
<ul>
<li>
<p><strong>실제 데이터 없는 학습 (Training without Real Data):</strong> AAE의 가장 큰 특징은 학습 과정에서 레이블링된 실제 이미지를 전혀 사용하지 않는다는 점이다. 대신, 객체의 3D CAD 모델로부터 렌더링한 <strong>합성 데이터(synthetic data)만을 이용해 학습</strong>한다. ‘심투리얼(sim-to-real)’ 간극, 즉 시뮬레이션 환경과 실제 환경의 차이를 극복하기 위해 <strong>도메인 무작위화(Domain Randomization)</strong> 기법을 적극적으로 활용했다. 합성 이미지를 생성할 때 배경, 조명, 노이즈, 가려짐 등을 극단적으로 무작위화하여, 네트워크가 객체의 본질적인 기하학적 특징에만 집중하도록 강제했다. 그 결과, 실제 환경의 다양한 변화에 강인한 모델을 학습할 수 있었다.17</p>
</li>
<li>
<p><strong>암시적 방향 표현 (Implicit Orientation Representation):</strong> AAE는 객체의 방향을 특정 회전 벡터 값으로 직접 회귀(regression)하지 않는다. 대신, 오토인코더의 인코더(<span class="math math-inline">\Phi</span>)가 입력 이미지를 저차원의 잠재 공간(latent space, <span class="math math-inline">z \in \mathbb{R}^{128}</span>) 상의 한 점으로 매핑한다.20 이 잠재 공간은 비슷한 방향을 가진 객체 이미지들이 서로 가까운 위치에 매핑되도록 구조화된다. 즉, 객체의 방향은 잠재 공간에서의 위치라는</p>
</li>
</ul>
<p><strong>암시적인 방식</strong>으로 표현된다. 이 접근법은 대칭성 문제를 우아하게 해결한다. 시각적으로 동일한 뷰들은 서로 다른 회전 레이블을 갖더라도 잠재 공간의 동일한 영역으로 매핑되기 때문에 ‘일대다’ 매핑 문제가 근본적으로 해소된다.17</p>
<ul>
<li><strong>테스트 시 자세 추정:</strong> 학습이 완료된 후, 가능한 모든 방향에서 렌더링된 수천 개의 합성 이미지들을 인코딩하여 잠재 공간 상의 ’코드북(codebook)’을 미리 생성한다. 테스트 시점에는 실제 환경에서 촬영된 이미지의 잠재 코드(<span class="math math-inline">z</span>)를 계산하고, 코드북 내에서 k-최근접 이웃(k-NN) 알고리즘을 사용해 가장 가까운 방향을 찾아낸다.20</li>
</ul>
<h4>3.1.3 결과 및 의의</h4>
<p>이 방법론은 T-LESS, LineMOD와 같은 까다로운 벤치마크 데이터셋에서 기존의 모델 기반 접근법들을 능가했으며, 실제 레이블링된 데이터를 사용한 최신 기술들과도 대등한 성능을 보였다.17 이 연구의 성공은 단순히 새로운 알고리즘의 제시를 넘어, 복잡한 인식 문제를 해결하는 새로운 방법론을 제시했다는 점에서 더 큰 의의를 가진다. 기존 딥러닝 패러다임이 대규모 레이블링 데이터에 크게 의존했던 반면, 이 연구는 합성 데이터와 도메인 무작위화를 통해 이러한 의존성에서 벗어날 수 있음을 증명했다. 이는 데이터 수집이 어렵거나 불가능한 수많은 로봇공학 및 컴퓨터 비전 문제에 대한 강력한 해결책을 제시한 것이며, 이후 로봇 학습 분야에서 시뮬레이션 기반 훈련이 주류로 자리 잡는 데 결정적인 기여를 한 선구적인 연구로 평가받는다.</p>
<h3>3.2  주목할 만한 논문: 그룹 정규화 (Group Normalization)</h3>
<p>페이스북 AI 연구소(Facebook AI Research)의 Yuxin Wu와 Kaiming He가 발표한 이 논문은 ECCV 2018에서 우수 논문상(Honorable Mention)을 수상했다.7 이 연구는 딥러닝 모델 훈련의 핵심 기술인 배치 정규화(Batch Normalization, BN)의 근본적인 한계를 지적하고, 이에 대한 효과적인 대안을 제시했다.</p>
<h4>3.2.1 문제 제기</h4>
<p>배치 정규화(BN)는 딥러닝 네트워크의 학습을 안정화하고 가속하는 데 필수적인 기술로 자리 잡았지만, 치명적인 약점을 가지고 있었다. 바로 <strong>작은 배치 크기(small batch size)에서 성능이 급격히 저하</strong>된다는 점이다.21 BN은 미니배치 내 데이터들의 통계량(평균, 분산)을 이용해 특징(feature)을 정규화하는데, 배치 크기가 작아지면 이 통계량이 전체 데이터 분포를 제대로 대표하지 못하고 노이즈가 심해지기 때문이다. 이는 고해상도 이미지를 다루는 객체 탐지, 영상 분할이나 거대 모델을 학습시켜야 하는 비디오 분석 등 GPU 메모리 제약으로 인해 작은 배치를 사용할 수밖에 없는 여러 컴퓨터 비전 분야에서 심각한 제약으로 작용했다.21</p>
<h4>3.2.2 방법론: 그룹 정규화 (Group Normalization, GN)</h4>
<p>그룹 정규화(GN)는 배치 크기에 의존하지 않는 간단하면서도 강력한 대안을 제시한다. GN은 배치 차원을 따라 정규화를 수행하는 대신, 각 데이터 샘플에 대해 독립적으로 연산을 수행한다.</p>
<ul>
<li>
<p>핵심 아이디어는 채널(channel) 차원을 여러 개의 **그룹(group)**으로 나누는 것이다. 하이퍼파라미터인 그룹 수(<span class="math math-inline">G</span>)에 따라 채널들이 분할되고, 각 그룹에 속한 채널들과 공간 차원(높이, 너비) 내에서 평균과 분산을 계산하여 정규화를 수행한다.21</p>
</li>
<li>
<p>특징 <span class="math math-inline">x_i</span>에 대한 일반적인 정규화 수식은 다음과 같다.</p>
<p><span class="math math-display">
\hat{x}_i = \frac{1}{\sigma_i} (x_i - \mu_i)
</span><br />
여기서 평균 <span class="math math-inline">\mu_i</span>와 표준편차 <span class="math math-inline">\sigma_i</span>는 픽셀 집합 <span class="math math-inline">S_i</span>에 대해 계산된다.</p>
</li>
<li>
<p>GN에서 집합 <span class="math math-inline">S_i</span>는 동일한 학습 샘플(<span class="math math-inline">k_N = i_N</span>)에 속하면서 동일한 채널 그룹(<span class="math math-inline">\lfloor k_C / (C/G) \rfloor = \lfloor i_C / (C/G) \rfloor</span>)에 속하는 픽셀들로 정의된다.21 수식은 다음과 같다.</p>
<p><span class="math math-display">
S_i = \{k \vert k_N = i_N, \lfloor k_C / (C/G) \rfloor = \lfloor i_C / (C/G) \rfloor\}
</span></p>
</li>
</ul>
<p>아래 표는 GN과 다른 주요 정규화 기법들의 특징을 비교한 것이다.</p>
<table><thead><tr><th>기법</th><th>정규화 축</th><th>배치 크기 의존성</th><th>주요 장점</th></tr></thead><tbody>
<tr><td>배치 정규화 (BN)</td><td>(N, H, W)</td><td>높음</td><td>학습 안정화 및 가속화</td></tr>
<tr><td>레이어 정규화 (LN)</td><td>(C, H, W)</td><td>없음</td><td>RNN에 효과적, 배치 크기 1에서도 동작</td></tr>
<tr><td>인스턴스 정규화 (IN)</td><td>(H, W)</td><td>없음</td><td>스타일 전이 등 생성 모델에 효과적</td></tr>
<tr><td>그룹 정규화 (GN)</td><td>(C의 그룹, H, W)</td><td>없음</td><td>작은 배치 크기에서 안정적인 성능, 비전 태스크에 강점</td></tr>
</tbody></table>
<h4>3.2.3 결과 및 의의</h4>
<p>ImageNet 분류 문제에서 ResNet-50 모델을 배치 크기 2로 학습시켰을 때, GN은 BN보다 10.6% 더 낮은 오류율을 기록했다.21 일반적인 배치 크기에서는 BN과 대등한 성능을 보였다. 더 중요한 것은, 사전 학습된 모델을 COCO 데이터셋 기반 객체 탐지나 Kinetics 데이터셋 기반 비디오 분류와 같이 작은 배치가 필수적인 후속 태스크(downstream task)에 적용했을 때, GN 기반 모델이 BN 기반 모델보다 일관되게 우수한 성능을 보였다는 점이다.21</p>
<p>이 연구의 성공은 모델 성능과 하드웨어 사양(GPU 메모리) 사이의 강한 종속 관계를 끊어낼 수 있음을 보여주었다. GN 이전에는 최신 거대 비전 모델을 효과적으로 학습시키기 위해 대용량 메모리를 갖춘 고가의 하드웨어가 필수적이었고, 이는 연구의 진입 장벽으로 작용했다. GN은 정규화 전략의 재고를 통해 배치 크기와 무관하게 안정적인 학습을 가능하게 함으로써 이러한 하드웨어 제약으로부터 모델 설계를 해방시켰다. 결과적으로 더 넓은 범위의 연구자들이 메모리 집약적인 태스크를 위한 더 크고 강력한 모델을 실험하고 훈련할 수 있게 되었으며, 이는 객체 탐지, 영상 분할, 비디오 분석 분야의 발전을 가속하는 중요한 계기가 되었다.</p>
<h3>3.3  주목할 만한 논문: 해부학적 인지 기반 얼굴 애니메이션 (GANimation: Anatomically-aware Facial Animation from a Single Image)</h3>
<p>바르셀로나에 위치한 로봇 및 산업 정보학 연구소(IRI)의 Albert Pumarola 연구팀이 발표한 이 논문 역시 ECCV 2018에서 우수 논문상을 수상했다.7 이 연구는 생성적 적대 신경망(GAN)을 이용한 얼굴 애니메이션 분야에서 제어 가능성과 표현력의 새로운 지평을 열었다.</p>
<h4>3.3.1 문제 제기</h4>
<p>당시 GAN을 이용한 얼굴 표정 생성 연구들은 대부분 ‘행복’, ’슬픔’과 같이 미리 정의된 이산적인(discrete) 감정 범주에 국한되어 있었다.23 이로 인해 인간의 실제 표정처럼 미묘하고, 연속적이며, 여러 근육의 움직임이 조합된 복합적인 표정을 생성하는 데 한계가 있었다. 이 연구의 목표는 단 한 장의 이미지로부터 시작하여 해부학적으로 타당하면서도 연속적인 방식으로 얼굴 표정을 자유롭게 조작하는 것이었다.23</p>
<h4>3.3.2 방법론</h4>
<ul>
<li>
<p><strong>액션 유닛(Action Unit, AU) 기반 조건화:</strong> 이 연구의 핵심 아이디어는 GAN의 생성 과정을 감정 레이블이 아닌, <strong>액션 유닛(AU)</strong> 벡터로 조건화(conditioning)한 것이다. AU는 얼굴 근육 코딩 시스템(Facial Action Coding System, FACS)에 정의된 특정 얼굴 근육의 수축을 나타내는 단위이다 (예: 입꼬리 당김). 각 AU의 활성화 정도를 나타내는 연속적인 값들의 벡터(<span class="math math-inline">y_r</span>)를 조건으로 부여함으로써, 다양한 표정을 연속적으로 조합하고 제어할 수 있게 되었다.23</p>
</li>
<li>
<p><strong>비지도 학습:</strong> 이 모델은 동일 인물이 다른 표정을 짓고 있는 ‘쌍(paired)’ 이미지 데이터 없이, 단순히 이미지와 해당 이미지의 AU 주석만으로 학습이 가능한 비지도(unsupervised) 방식으로 훈련되었다.23</p>
</li>
<li>
<p><strong>어텐션 메커니즘(Attention Mechanism):</strong> 배경이나 조명이 다양한 ‘실제 환경(in the wild)’ 이미지에 강인하게 대처하기 위해, 생성기는 최종 결과물 외에 어텐션 마스크를 추가로 생성한다. 이 마스크는 표정 변화와 관련된 얼굴 영역(눈, 입 등)에만 변화를 집중시키고, 머리카락이나 배경과 같은 정적인 부분은 원본 이미지를 그대로 유지하도록 한다. 최종 이미지는 <code>I_final = Attention * I_generated_face + (1 - Attention) * I_original</code>과 같은 형태로 합성된다.23</p>
</li>
</ul>
<h4>3.3.3 결과 및 의의</h4>
<p>이 방법론을 통해 단일 이미지를 입력받아 AU 벡터 값을 점진적으로 변화시킴으로써, 마치 비디오처럼 부드럽게 표정이 변하는 애니메이션을 생성할 수 있었다.23 이는 기존 방법들보다 훨씬 더 넓고 미묘한 범위의 표정 생성이 가능함을 보여주었으며, 영화, 사진 보정, 가상 아바타 등 다양한 분야에 응용될 수 있는 잠재력을 입증했다.</p>
<p>이 연구는 생성 모델 분야에서 <strong>해석 가능하고 제어 가능한(interpretable and controllable)</strong> 모델로 나아가는 중요한 진전을 이루었다. GAN의 조건 공간을 추상적인 레이블이 아닌, 얼굴 근육이라는 물리적으로 의미 있는 시스템에 기반하게 함으로써, ’블랙박스’와 같았던 이미지 변환 과정을 사용자가 예측 가능하고 의미론적으로 조작할 수 있는 시스템으로 바꾸었다. 이는 생성 모델에서 다양한 변화 요인을 독립적으로 제어하려는 ‘분리(disentanglement)’ 문제에 대한 해결의 실마리를 제공한 연구이며, 구조화되고 의미 있는 잠재 공간을 활용하는 현대의 제어 가능한 생성 AI 기술의 초석이 되었다.</p>
<h2>4.  로봇공학 분야의 진보: IROS 2018 및 arXiv 주요 연구</h2>
<p>IROS 2018과 동 시기 arXiv에 공개된 연구들은 로봇이 더욱 복잡하고 동적인 실제 환경에서 안정적으로 작동하기 위한 핵심 기술들의 성숙을 보여주었다. 특히 센서 데이터의 정밀한 처리, 동적 보행, 다개체 시스템 제어 등에서 중요한 진전이 있었다.</p>
<h3>4.1  최우수 학생 논문: 단안 시각-관성 시스템을 위한 온라인 시간적 보정 (Online temporal calibration for monocular visual-inertial systems)</h3>
<p>홍콩과기대학교(HKUST)의 Tong Qin과 Shaojie Shen 교수가 발표한 이 논문은 IROS 2018 최우수 학생 논문상을 수상하며 큰 주목을 받았다.25</p>
<h4>4.1.1 문제 제기</h4>
<p>시각-관성 항법 시스템(Visual-Inertial Navigation Systems, VINS)은 카메라와 관성 측정 장치(Inertial Measurement Unit, IMU)의 데이터를 융합하여 로봇의 위치와 자세를 추정하는 기술이다. 이 기술의 정확성은 두 센서 데이터의 타임스탬프가 완벽하게 동기화되었다는 가정에 크게 의존한다. 하지만 실제 시스템에서는 센서의 트리거링 및 데이터 전송 지연으로 인해 두 데이터 스트림 사이에 미세한 **시간적 오프셋(<span class="math math-inline">t_d</span>)**이 발생한다.28 수 밀리초(ms)에 불과한 이 작은 시간 불일치조차도 상태 추정의 정확성과 강인성을 심각하게 저하시킬 수 있다. 기존에는 이러한 오프셋을 측정하기 위해 복잡한 장비와 절차를 동반하는 오프라인(offline) 보정 과정을 거쳐야만 했다.</p>
<h4>4.1.2 방법론</h4>
<p>이 논문은 시간적 오프셋 문제를 해결하기 위해 혁신적인 <strong>온라인 보정(online calibration)</strong> 접근법을 제안했다.</p>
<ul>
<li>
<p>핵심 아이디어는 시간 오프셋 <span class="math math-inline">t_d</span>를 미리 측정된 고정값이 아닌, 실시간으로 추정해야 할 **미지의 상태 변수(state variable)**로 취급하는 것이다.</p>
</li>
<li>
<p>시간 오프셋 <span class="math math-inline">t_d</span>는 카메라/IMU의 자세, 속도, 특징점 위치 등 다른 상태 변수들과 함께 최적화 기반 SLAM 시스템의 상태 벡터(state vector)에 직접 포함된다.25</p>
</li>
<li>
<p>시각-관성 최적화 프레임워크의 비용 함수, 특히 시각적 재투영 오차(visual reprojection error) 항이 이 시간 오프셋을 고려하도록 수정된다. 즉, 3D 특징점이 특정 카메라 프레임에 어떻게 투영될지 계산할 때, 추정된 <span class="math math-inline">t_d</span>를 이용해 IMU의 자세를 이미지 캡처의 정확한 순간으로 보간(interpolation)하여 사용한다.</p>
</li>
<li>
<p>이러한 방식으로 <span class="math math-inline">t_d</span>를 다른 모든 시스템 변수들과 함께 공동으로 최적화(jointly optimize)함으로써, 시스템은 별도의 보정 과정 없이 정상 작동 중에 스스로 센서 간의 시간 불일치를 지속적으로 추정하고 보정할 수 있다.</p>
</li>
</ul>
<h4>4.1.3 결과 및 의의</h4>
<p>제안된 방법은 EuRoC와 같은 표준 데이터셋에서 오프라인 보정 도구와 대등하거나 더 높은 정확도를 보이며 그 효과를 입증했다.25 이 연구의 가장 큰 파급력은 이 기술이 당대 가장 영향력 있는 오픈소스 프로젝트 중 하나인 <strong>VINS-Mono</strong>에 통합되어 전 세계 로봇 커뮤니티에 공개되었다는 점이다.25</p>
<p>이 연구는 로봇공학 분야의 더 넓은 흐름, 즉 정적인 오프라인 보정에서 동적인 온라인 자기-보정(self-calibration)으로 나아가는 기술적 패러다임의 전환을 상징한다. 이는 전문가의 개입 없이도 전원을 켜는 것만으로 스스로 하드웨어의 미세한 변형이나 오차에 적응할 수 있는, 진정한 의미의 ‘power-on-and-go’ 시스템을 향한 중요한 진전이었다. 복잡한 사전 준비 과정을 제거함으로써 VINS/SLAM 기술의 접근성을 크게 낮추었고, 새로운 로봇 플랫폼에 고성능 상태 추정 기술을 신속하게 배포하고 연구하는 것을 가속화했다. 이는 실험실 수준의 시스템에서 현장 배포가 가능한(field-ready) 시스템으로 나아가는 중요한 이정표였다.</p>
<h3>4.2  기타 주목할 만한 로봇공학 연구</h3>
<p>2018년 9월 arXiv에는 위 연구 외에도 로봇공학의 다양한 분야에서 중요한 진전을 보여주는 여러 선행 연구(pre-print)들이 공개되었다.32</p>
<ul>
<li>
<p><strong>이족 보행 (Bipedal Locomotion):</strong> Cassie와 같은 동적인 이족 보행 로봇의 제어에 관한 연구가 발표되었다. 걷기, 서기뿐만 아니라 세그웨이를 타는 것과 같은 복잡한 행동을 구현하며, 부족구동 시스템(underactuated system)에 대한 피드백 제어 기술의 발전을 보여주었다 (예: “Feedback Control of a Cassie Bipedal Robot…”).33</p>
</li>
<li>
<p><strong>SLAM 및 3D 인식:</strong> 포즈 그래프 SLAM 문제에 대한 전역 최적 해(globally optimal solution)를 보장하는 연구(“Guaranteed Globally Optimal Planar Pose Graph…”)와 3D 라이다(LiDAR) 스캔을 위한 새로운 지역 특징 기술자(local feature descriptor)에 관한 연구(“Learning a Local Feature Descriptor for 3D LiDAR Scans”) 등이 발표되며, 지도 작성 및 위치 추정 기술의 정확성과 강인성을 한 단계 끌어올렸다.32</p>
</li>
<li>
<p><strong>다개체 로봇 시스템 (Multi-Robot Systems):</strong> 자율주행차의 합류 제어(“Decentralized Optimal Merging Control…”)나 대규모 로봇 시스템을 위한 최적 제어 합성(“STyLuS*…”)과 같이, 여러 로봇 간의 협력과 조정을 다루는 분산 및 최적 제어에 관한 연구들이 활발히 진행되었다. 이는 개별 로봇의 지능을 넘어 군집 지능으로 나아가는 연구의 흐름을 보여준다.32</p>
</li>
</ul>
<h2>5.  결론: 2018년 9월 연구 동향의 종합 및 미래 전망</h2>
<p>2018년 9월에 발표된 AI 및 로봇공학 분야의 주요 연구들은 개별적인 성과를 넘어, 당시 기술 패러다임을 형성하던 몇 가지 거대한 흐름의 수렴을 명확하게 보여준다. 이 시기의 연구들은 딥러닝 혁명의 초기 단계를 지나, 그 도구들을 더욱 정교하게 다듬고 현실 세계의 근본적인 문제들을 해결하는 데 적용하는 성숙 단계로 진입했음을 알리는 신호탄이었다.</p>
<p><strong>핵심 주제의 종합</strong></p>
<p>분석된 주요 연구들을 통해 다음과 같은 네 가지 핵심적인 메가트렌드를 종합할 수 있다.</p>
<ol>
<li>
<p><strong>자기-지도 및 시뮬레이션 기반 학습의 부상:</strong> ECCV 최우수 논문으로 선정된 6D 자세 추정 연구 17는 값비싼 인간의 수동 레이블링 작업에 대한 의존도를 줄이고, 합성 데이터를 통해 현실 세계에 적용 가능한 모델을 학습시키는 ‘심투리얼’ 패러다임의 가능성을 입증했다. 이는 딥러닝 학습을 더욱 확장 가능하고 데이터 효율적으로 만드는 방향으로 나아가는 중요한 흐름이었다.</p>
</li>
<li>
<p><strong>핵심 딥러닝 아키텍처의 정교화:</strong> 그룹 정규화(GN) 연구 21는 배치 정규화(BN)와 같은 기존의 핵심 기술이 가진 2차적인 문제점들을 식별하고 해결하는 단계로 접어들었음을 보여준다. 이는 기초 기술을 더욱 강인하고 범용적으로 만들어, 더 넓은 범위의 문제에 안정적으로 적용할 수 있도록 다듬는 과정이었다.</p>
</li>
<li>
<p><strong>제어 가능하고 해석 가능한 모델을 향한 움직임:</strong> GANimation 연구 23는 ’블랙박스’처럼 작동하던 생성 모델을 넘어, 사용자가 의미론적으로 이해하고 제어할 수 있는 시스템으로 발전시키려는 시도를 보여준다. 이는 인간-참여형(human-in-the-loop) AI 및 창의적 도구 개발의 핵심적인 전제 조건이다.</p>
</li>
<li>
<p><strong>체화된 AI 시스템(Embodied AI)의 성숙:</strong> IROS 최우수 학생 논문으로 선정된 온라인 시간적 보정 연구 25는 실험실 시제품과 실제 현장 배포 사이의 간극을 메우려는 노력을 상징한다. 로봇이 스스로 오차를 보정하고 환경에 적응하는 능력은, 자율 시스템이 현실 세계에서 신뢰성 있게 작동하기 위한 필수적인 요소이다.</p>
</li>
</ol>
<p><strong>미래 전망</strong></p>
<p>2018년 9월에 나타난 이러한 흐름들은 이후 AI 및 로봇공학 분야의 발전에 지대한 영향을 미쳤다. ‘심투리얼’ 전이 학습에 대한 연구는 오늘날 대규모 로봇 시뮬레이션 플랫폼 개발의 이론적 기반을 제공했다. 그룹 정규화와 같은 새로운 정규화 기법의 등장은 현재 AI 분야를 지배하는 거대한 트랜스포머 기반 모델들의 안정적인 학습을 가능하게 한 숨은 공신 중 하나이다. VINS에서의 온라인 보정과 강인성에 대한 집중적인 연구는 자율 드론, AR/VR, 자율주행차 기술 발전의 핵심 동력이 되었다.</p>
<p>결론적으로, 2018년 9월은 딥러닝 혁명이 가져온 기초 도구들이 인식 및 로봇공학의 근본적인 난제들을 해결하기 위해 날카롭게 벼려지던 결정적인 시점이었다. 이 시기에 제시된 아이디어와 방법론들은 이후 5년간의 기술 발전을 위한 무대를 마련했으며, 오늘날 우리가 목도하는 AI 기술의 지형을 형성하는 데 중요한 역할을 수행했다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>NOTES FROM THE AI FRONTIER MODELING THE IMPACT OF AI ON THE WORLD ECONOMY - McKinsey, <a href="https://www.mckinsey.com/~/media/mckinsey/featured%20insights/artificial%20intelligence/notes%20from%20the%20frontier%20modeling%20the%20impact%20of%20ai%20on%20the%20world%20economy/mgi-notes-from-the-ai-frontier-modeling-the-impact-of-ai-on-the-world-economy-september-2018.ashx">https://www.mckinsey.com/~/media/mckinsey/featured%20insights/artificial%20intelligence/notes%20from%20the%20frontier%20modeling%20the%20impact%20of%20ai%20on%20the%20world%20economy/mgi-notes-from-the-ai-frontier-modeling-the-impact-of-ai-on-the-world-economy-september-2018.ashx</a></li>
<li>Machines Will Do More Tasks Than Humans by 2025 but Robot Revolution Will Still Create 58 Million Net New Jobs in Next Five Years &gt; Press releases | World Economic Forum, https://www.weforum.org/press/2018/09/machines-will-do-more-tasks-than-humans-by-2025-but-robot-revolution-will-still-create-58-million-net-new-jobs-in-next-five-years/</li>
<li>Industrial Robot Sales Broke Records in 2018, https://www.automate.org/robotics/blogs/industrial-robot-sales-broke-records-in-2018</li>
<li>eccv2018.org, <a href="https://eccv2018.org/#:~:text=ECCV%202018%20%2F%2F%20September%208%20-%2014%202018%20%2F%2F%20Munich%2C%20Germany">https://eccv2018.org/#:~:text=ECCV%202018%20%2F%2F%20September%208%20%2D%2014%202018%20%2F%2F%20Munich%2C%20Germany</a></li>
<li>European Conference on Computer Vision (ECCV), 2018 | ServiceNow Research, https://www.servicenow.com/research/event/2018-eccv.html</li>
<li>Computer Vision ECCV 2018 15th European Conference Munich Germany September 8 14 2018 Proceedings Part XIII Vittorio Ferrari Digital Version 2025 | PDF - Scribd, https://www.scribd.com/document/919368920/Computer-Vision-ECCV-2018-15th-European-Conference-Munich-Germany-September-8-14-2018-Proceedings-Part-XIII-Vittorio-Ferrari-digital-version-2025</li>
<li>ECCV 2018 Announces Best Papers. The 15th European Conference on… | by Synced | SyncedReview | Medium, https://medium.com/syncedreview/eccv-2018-announces-best-papers-e76dd9cfda71</li>
<li>ECCV - European Computer Vision Association, https://eccv.ecva.net/</li>
<li>Computer Vision – ECCV 2018 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part VIII - ResearchGate, https://www.researchgate.net/publication/345574969_Computer_Vision_-_ECCV_2018_15th_European_Conference_Munich_Germany_September_8-14_2018_Proceedings_Part_VIII_15th_European_Conference_Munich_Germany_September_8-14_2018_Proceedings_Part_VIII</li>
<li>IROS 2018 - International Conference on Intelligent Robots - Madrid, https://www.iros2018.org/</li>
<li>qbrobotics.com, https://qbrobotics.com/events/iros-2018-madrid-spain/</li>
<li>IROS 2018 - Unexmin, https://www.unexmin.eu/event/iros-2018/</li>
<li>IROS Best Paper Award on Mobile Manipulation | Electrical and Computer Engineering, https://ece.ucsd.edu/awards/faculty/iros-best-paper-award-mobile-manipulation</li>
<li>IROS 2018 presentation | Erwin Lopez Site, https://www.erwinlopez.com/post/iros-2018/</li>
<li>European Computer Vision Association: ECVA, https://www.ecva.net/</li>
<li>AWARDS and PRIZES - ECCV 2018, https://eccv2018.org/wp-content/uploads/2018/09/ECCV2018Awards.pdf</li>
<li>Implicit 3D Orientation Learning for 6D Object Detection from RGB …, https://elib.dlr.de/122011/1/Martin_Sundermeyer_Implicit_3D_Orientation_ECCV_2018_paper.pdf</li>
<li>University of Southern Denmark Reliable Object Pose Estimation Haugaard, Rasmus Laurvig - Syddansk Universitets Forskerportal, https://portal.findresearcher.sdu.dk/files/242265002/phd_thesis.pdf</li>
<li>DEEP ORIENTATION UNCERTAINTY LEARNING BASED ON A BINGHAM LOSS - OpenReview, https://openreview.net/pdf?id=ryloogSKDS</li>
<li>Implicit 3D Orientation Learning for 6D Object Detection from RGB Images, http://cseweb.ucsd.edu/~mkchandraker/classes/CSE291/Winter2019/Lectures/10_ObjectPose6D.pdf</li>
<li>Group Normalization - CVF Open Access, https://openaccess.thecvf.com/content_ECCV_2018/papers/Yuxin_Wu_Group_Normalization_ECCV_2018_paper.pdf</li>
<li>arXiv:1803.08494v3 [cs.CV] 11 Jun 2018, https://arxiv.org/pdf/1803.08494</li>
<li>GANimation: Anatomically-aware Facial Animation from a … - IRI-UPC, http://www.iri.upc.edu/files/scidoc/2052-GANimation:-Anatomically-aware-Facial-Animation-from-a-Single-Image.pdf</li>
<li>Albert Pumarola - GANimation: Anatomically-aware Facial Animation from a Single Image, https://www.albertpumarola.com/publications/files/pumarola2018ganimation.pdf</li>
<li>Tong Qin wins IROS 2018 Best Student Paper Award - HKUST Aerial Robotics Group, https://uav.hkust.edu.hk/iros2018-best-student-paper-award/</li>
<li>HKUST-Aerial-Robotics/VINS-Fisheye: Fisheye version of VINS-Fusion - GitHub, https://github.com/HKUST-Aerial-Robotics/VINS-Fisheye</li>
<li>PhD Student Tong QIN Wins the IROS 2018 Best Student Paper Award - hkust-ece, https://ece.hkust.edu.hk/news/phd-student-tong-qin-wins-iros-2018-best-student-paper-award</li>
<li>Online Temporal Calibration For Monocular Visual-Inertial Systems | PDF - Scribd, https://www.scribd.com/document/883605440/Online-Temporal-Calibration-for-Monocular-Visual-Inertial-Systems</li>
<li>Online Temporal Calibration for Monocular Visual-Inertial Systems - ResearchGate, https://www.researchgate.net/publication/326799538_Online_Temporal_Calibration_for_Monocular_Visual-Inertial_Systems</li>
<li>Universal Online Temporal Calibration for Optimization-based Visual-Inertial Navigation Systems - ResearchGate, https://www.researchgate.net/publication/387745395_Universal_Online_Temporal_Calibration_for_Optimization-based_Visual-Inertial_Navigation_Systems</li>
<li>HKUST-Aerial-Robotics/VINS-Mono: A Robust and Versatile Monocular Visual-Inertial State Estimator - GitHub, https://github.com/HKUST-Aerial-Robotics/VINS-Mono</li>
<li>Robotics Sep 2018 - arXiv, http://arxiv.org/list/cs.RO/2018-09?skip=250&amp;show=2000</li>
<li>Robotics Sep 2018 - arXiv, https://web3.arxiv.org/list/cs.RO/2018-09?skip=125&amp;show=50</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>