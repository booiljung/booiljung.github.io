<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1></h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2018년 AI 및 로봇 연구 동향</a> / <span></span></nav>
                </div>
            </header>
            <article>
                <h1></h1>
<h1>2018년 6월 AI 및 로봇 연구 동향</h1>
<h2>1.  2018년 6월, AI 연구 패러다임의 전환</h2>
<h3>1.1  2018년의 기술적 배경: 지도 학습의 정점과 새로운 지평</h3>
<p>2018년 이전 인공지능(AI) 연구의 주류는 대규모 레이블 데이터셋에 기반한 지도 학습(Supervised Learning) 패러다임이 지배하고 있었다. ImageNet과 같은 데이터셋의 등장은 ResNet, LSTM 등 특정 과업(task)에 고도로 특화된 심층 신경망 아키텍처의 발전을 촉진했으며, 연구의 핵심은 아키텍처 엔지니어링을 통해 특정 벤치마크의 성능을 극한으로 끌어올리는 데 집중되었다.</p>
<p>그러나 이러한 성공의 이면에는 지도 학습의 근본적인 한계가 명확히 드러나고 있었다. 수작업 레이블링에 소요되는 막대한 비용, 데이터셋 자체에 내재된 편향(bias), 그리고 특정 과업에 대한 과적합(overfitting)으로 인한 일반화 성능의 저하는 AI 기술의 적용 범위를 제한하는 주요한 병목으로 작용했다.1 이러한 배경 속에서, 학계는 레이블이 없는 방대한 양의 데이터를 활용하여 세상에 대한 보편적인 지식, 즉 유용한 표현(representation)을 학습하려는 시도로 눈을 돌리기 시작했다. 비지도 학습(Unsupervised Learning)과 한 과업에서 얻은 지식을 다른 과업으로 이전하는 전이 학습(Transfer Learning)이 단순한 보조 수단을 넘어, AI 연구의 핵심 화두로 부상한 것이다.1</p>
<p>이러한 패러다임 전환의 움직임은 2018년 6월을 기점으로 구체적인 결실을 보기 시작했다. 이전까지의 연구가 ’더 나은 아키텍처’를 통해 ’특정 과업’의 성능을 높이는 데 집중했다면, 이 시점의 연구들은 ’어떻게 하면 레이블 없이 범용적인 지식을 학습할 것인가?’라는 보다 근본적인 질문에 답하기 시작했다. GPT-1은 텍스트에서, GQN은 시각 데이터에서, Taskonomy는 과업 간의 관계에서, 그리고 Differentiable Physics는 물리 법칙에서 이러한 ’사전 지식’을 추출하려는 시도였다. 이 네 연구는 각기 다른 분야에서 동일한 철학, 즉 ’특수화된 훈련’에서 ’일반화된 사전 학습’으로의 전환을 명확히 보여주었으며, 이는 2018년 6월을 AI 연구 방법론의 변곡점으로 규정하는 핵심적인 근거가 된다.</p>
<h3>1.2  2018년 6월의 학술적 구심점: CVPR과 RSS</h3>
<p>2018년 6월은 AI 및 로보틱스 분야의 양대 최상위 학술대회가 연이어 개최되며 당시 연구의 최전선을 조망할 수 있는 중요한 기회를 제공했다. 컴퓨터 비전 분야의 최고 권위 학회인 CVPR (Conference on Computer Vision and Pattern Recognition) 2018은 6월 18일부터 22일까지 미국 유타주 솔트레이크시티에서 개최되었다.5 이 학회에서는 본 보고서에서 심도 있게 다룰 ‘Taskonomy’ 논문이 최우수 논문상을 수상하며 전이 학습 연구에 새로운 방향을 제시했다.</p>
<p>뒤이어 로보틱스 분야의 최고 학회인 RSS (Robotics: Science and Systems) 2018이 6월 26일부터 30일까지 미국 펜실베이니아주 피츠버그에서 개최되었다.9 이 학회에서는 물리 시뮬레이션을 로봇 계획 문제에 통합한 ‘Differentiable Physics’ 연구가 최우수 논문상을 수상하며, 로봇이 물리적 세계와 상호작용하는 방식에 대한 근본적인 접근법을 제시했다.10 이 두 학회는 본 보고서에서 다룰 핵심 연구들이 발표되고 논의된 중심 무대로서, 2018년 6월이라는 시간적 배경의 중요성을 뒷받침한다.</p>
<table><thead><tr><th>학회명</th><th>전체 명칭</th><th>개최 기간</th><th>개최 장소</th><th>본 보고서에서 다루는 주요 발표 연구</th></tr></thead><tbody>
<tr><td><strong>CVPR 2018</strong></td><td>Conference on Computer Vision and Pattern Recognition</td><td>2018년 6월 18일 - 22일</td><td>미국 유타주 솔트레이크시티</td><td>Taskonomy: Disentangling Task Transfer Learning</td></tr>
<tr><td><strong>RSS 2018</strong></td><td>Robotics: Science and Systems</td><td>2018년 6월 26일 - 30일</td><td>미국 펜실베이니아주 피츠버그</td><td>Differentiable Physics and Stable Modes for Tool-Use and Manipulation Planning</td></tr>
</tbody></table>
<h3>1.3  보고서의 구성 및 목표</h3>
<p>본 보고서는 2018년 6월을 기점으로 발표된 네 가지 핵심 연구—OpenAI의 생성적 사전 훈련 Transformer (GPT-1), DeepMind의 생성적 질의 네트워크 (GQN), CVPR 2018 최우수 논문 ‘Taskonomy’, 그리고 RSS 2018 최우수 논문 ‘Differentiable Physics’—를 심층적으로 분석한다. 각 연구의 기술적 배경, 핵심 방법론, 그리고 학계에 미친 영향을 다각도로 조명할 것이다.</p>
<p>궁극적으로 본 보고서는 이 연구들이 어떻게 비지도 학습, 표현 학습(Representation Learning), 그리고 물리 세계와의 접점이라는 공통된 주제를 향해 나아갔는지 탐구하고, 이들이 어떻게 현대 AI 기술의 근간을 이루는 파운데이션 모델(Foundation Models) 시대의 서막을 열었는지를 논증하는 것을 목표로 한다.</p>
<h2>2.  언어 모델의 새로운 시대: OpenAI의 생성적 사전 훈련 Transformer (GPT-1)</h2>
<h3>2.1  연구 개요: NLP 패러다임의 재정의</h3>
<p>2018년 6월 11일, OpenAI는 “Improving Language Understanding by Generative Pre-Training“이라는 제목의 기술 보고서를 통해 자연어 처리(NLP) 분야의 연구 패러다임을 근본적으로 바꾸는 모델을 공개했다.1 Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever가 저술한 이 논문은, 이후 GPT-1으로 알려지게 된 모델을 소개하며 두 단계의 학습 프레임워크를 제안했다.12 첫 번째 단계는 대규모의 레이블 없는 텍스트 코퍼스에서 언어 모델을 **생성적으로 사전 훈련(Generative Pre-training)**하는 것이고, 두 번째 단계는 이렇게 사전 훈련된 모델을 각 NLP 과업에 맞게 **지도 방식으로 미세 조정(Discriminative Fine-tuning)**하는 것이다.3 이 ‘사전 훈련 후 미세 조정(Pre-train, then Fine-tune)’ 접근법은 기존에 각 과업별로 최적의 아키텍처를 설계하고 처음부터 학습시키던 방식에서 벗어나, 범용적인 단일 모델의 가능성을 제시했다는 점에서 혁신적이었다.3</p>
<h3>2.2  모델 구조: 12-Layer Decoder-Only Transformer</h3>
<p>GPT-1의 성공은 단순히 학습 프레임워크뿐만 아니라, 그 기반이 되는 신경망 아키텍처의 선택에도 기인한다. 이전의 많은 NLP 연구들이 순환 신경망(RNN)이나 LSTM에 의존했던 것과 달리, GPT-1은 2017년에 발표된 Transformer 아키텍처를 채택했다.3 Transformer의 핵심인 셀프 어텐션(self-attention) 메커니즘은 문장 내 단어 간의 장거리 의존성(long-range dependencies)을 효과적으로 포착할 수 있게 해주어, 문맥을 이해하는 데 있어 기존 모델들보다 월등한 성능을 보였다.1</p>
<p>특히 GPT-1은 Transformer의 인코더-디코더 구조 중에서 <strong>디코더(Decoder) 블록만을 12겹으로 쌓은(12-layer)</strong> 구조를 사용했다.1 이는 이전 단어들을 바탕으로 다음 단어를 순차적으로 예측하는 언어 모델링 과업에 최적화된 설계였다. 각 디코더 블록은 이전 토큰들의 정보를 모두 참고하되 미래의 토큰 정보는 보지 못하도록 마스킹된 멀티 헤드 셀프 어텐션(masked multi-head self-attention)과, 위치 정보를 고려하는 위치 기반 피드포워드 신경망(position-wise feed-forward network)으로 구성된다.3 또한, 입력 텍스트는 Byte-Pair Encoding (BPE) 알고리즘을 통해 고정된 크기의 어휘집으로 토큰화되어, 미등록 단어(out-of-vocabulary) 문제에 강건하게 대응할 수 있었다.14</p>
<h3>2.3  학습 방법론 및 목적 함수 심층 분석</h3>
<p>GPT-1의 핵심은 두 단계로 구성된 학습 방법론에 있다. 이 과정은 모델이 언어에 대한 일반적인 지식을 먼저 습득한 후, 특정 과업에 대한 전문성을 갖추도록 유도한다.</p>
<h4>2.3.1  1단계: 비지도 사전 훈련 (Unsupervised Pre-training)</h4>
<p>이 단계의 목표는 방대한 양의 레이블 없는 텍스트 데이터를 사용하여 언어 자체의 통계적, 구조적 속성을 모델이 내재화하도록 하는 것이다. 모델은 주어진 텍스트 코퍼스 <span class="math math-inline">\mathcal{U} = \{u_1,..., u_n\}</span>에 대해 표준적인 언어 모델링 목적 함수, 즉 이전 <span class="math math-inline">k</span>개의 단어(<code>컨텍스트</code>)가 주어졌을 때 다음 단어를 예측하는 조건부 확률을 최대화하도록 학습된다.3 이 목적 함수 <span class="math math-inline">L_1</span>은 다음과 같이 수식으로 표현된다.</p>
<p><span class="math math-display">
L_1(\mathcal{U}) = \sum_{i} \log P(u_i | u_{i-k}, \dots, u_{i-1}; \Theta)
</span></p>
<p>여기서 <span class="math math-inline">k</span>는 컨텍스트 윈도우의 크기를, <span class="math math-inline">\Theta</span>는 Transformer 모델의 파라미터를 의미한다.3 이 단순해 보이는 목적 함수를 최적화하는 과정에서, 모델은 문법, 어휘, 그리고 문맥에 따른 단어의 의미 변화와 같은 복잡한 언어적 패턴을 학습하게 된다. 이는 모델의 파라미터 <span class="math math-inline">\Theta</span>가 단순히 다음 단어 예측기가 아니라, 언어의 통사론적, 의미론적 구조를 압축적으로 인코딩한 일종의 ’지식 저장소’가 됨을 의미한다.</p>
<h4>2.3.2  2단계: 지도 미세 조정 (Supervised Fine-tuning)</h4>
<p>사전 훈련이 완료된 후, 모델은 레이블이 있는 특정 데이터셋 <span class="math math-inline">\mathcal{C}</span>를 사용하여 특정 과업(예: 감성 분석, 질의응답)을 수행하도록 미세 조정된다. 입력 시퀀스 <span class="math math-inline">(x^1, \dots, x^m)</span>와 레이블 <span class="math math-inline">y</span>로 구성된 데이터에 대해, 사전 훈련된 모델의 마지막 Transformer 블록 출력인 <span class="math math-inline">h_l^m</span>을 추출한다. 이 출력 벡터에 과업에 특화된 새로운 선형 레이어(가중치 <span class="math math-inline">W_y</span>)를 추가하고 softmax 함수를 적용하여 레이블 <span class="math math-inline">y</span>를 예측한다.3</p>
<p>이 단계의 주 목적 함수 <span class="math math-inline">L_2</span>는 주어진 입력에 대한 정답 레이블의 로그 우도를 최대화하는 것이다.</p>
<p><span class="math math-display">
L_2(\mathcal{C}) = \sum_{(x,y)} \log P(y|x^1, \dots, x^m)
</span></p>
<p>GPT-1의 독창적인 기여 중 하나는 이 미세 조정 단계에서 언어 모델링을 **보조 목적 함수(auxiliary objective)**로 함께 사용한 점이다. 이는 사전 훈련 단계에서 학습한 일반적인 언어 지식을 잃지 않도록 하는 정규화(regularization) 효과를 주며, 모델의 일반화 성능을 향상시키고 수렴을 가속화한다.3 최종적으로 최적화되는 목적 함수 <span class="math math-inline">L_3</span>는 다음과 같이 가중치 <span class="math math-inline">\lambda</span>를 사용하여 두 목적 함수를 결합한 형태가 된다.</p>
<p><span class="math math-display">
L_3(\mathcal{C}) = L_2(\mathcal{C}) + \lambda \cdot L_1(\mathcal{C})
</span></p>
<p>3</p>
<p>이러한 접근법 덕분에, 각기 다른 과업을 수행할 때 필요한 것은 사전 훈련된 거대한 모델 위에 작은 선형 레이어 하나를 추가하는 것뿐이었고, 이는 NLP 연구의 효율성을 극적으로 향상시켰다.</p>
<h3>2.4  NLP 연구 패러다임에 미친 영향</h3>
<p>GPT-1의 등장은 NLP 연구의 흐름을 완전히 바꾸어 놓았다.</p>
<p>첫째, <strong>‘Pre-train, then Fine-tune’ 패러다임을 확립</strong>했다. 이 방법론은 이후 등장한 BERT, RoBERTa, T5, 그리고 후속 GPT 시리즈에 이르기까지 거의 모든 대규모 언어 모델(LLM)의 표준적인 학습 방식으로 자리 잡았다.15 이는 NLP 연구가 과업별 아키텍처를 설계하는 ’특징 공학(feature engineering)’의 시대에서, 거대한 데이터로부터 보편적인 ’표현 학습(representation learning)’을 수행하는 시대로 완전히 전환되었음을 의미한다.</p>
<p>둘째, <strong>범용 모델(General-Purpose Model)의 가능성을 입증</strong>했다. GPT-1은 단 하나의 사전 훈련된 모델을 최소한으로 수정하여 텍스트 함의(textual entailment), 질의응답, 의미 유사성 평가, 문서 분류 등 연구된 12개의 과업 중 9개에서 당시 최고 성능(State-of-the-Art)을 달성했다.3 이는 각기 다른 문제를 해결하기 위해 별도의 모델을 개발할 필요 없이, 하나의 거대하고 범용적인 모델로 다양한 문제를 해결할 수 있다는 가능성을 보여주었으며, 이는 현재 파운데이션 모델의 핵심 철학이 되었다.</p>
<p>셋째, <strong>스케일링 법칙(Scaling Laws)의 서막</strong>을 열었다. 저자들은 논문 말미에 더 많은 비지도 데이터와 더 큰 모델, 그리고 더 많은 컴퓨팅 자원을 사용하면 성능이 더욱 향상될 여지가 크다고 예측했다.1 이 예측은 이후 “모델 크기, 데이터셋 크기, 그리고 학습에 사용되는 컴퓨팅 양을 늘리면 모델의 성능이 예측 가능한 방식으로 향상된다“는 스케일링 법칙 연구로 이어졌고, 이는 GPT-3와 같은 초거대 AI 모델의 탄생을 이끈 핵심적인 원동력이 되었다.</p>
<h2>3.  데이터 없이 세상을 배우다: DeepMind의 생성적 질의 네트워크 (GQN)</h2>
<h3>3.1  연구 개요: 관찰을 통한 자율적 3D 세계 이해</h3>
<p>2018년 6월 15일, DeepMind는 세계적인 과학 저널 ’Science’에 “Neural scene representation and rendering“이라는 제목의 논문을 게재하며 컴퓨터 비전 분야에 새로운 방향을 제시했다.17 S. M. Ali Eslami, Danilo Jimenez Rezende 등이 주도한 이 연구는, 인간이나 동물이 별도의 가르침 없이 주변 환경을 탐색하고 관찰하며 세상을 이해하는 방식에서 영감을 얻었다.19 연구진은 에이전트가 스스로 수집한 여러 시점의 2D 이미지(관찰)만을 사용하여 3차원 장면에 대한 내재적인 표현을 학습하고, 이 표현을 바탕으로 이전에 한 번도 보지 못한 새로운 시점에서 장면을 ’상상’하여 렌더링하는</p>
<p><strong>생성적 질의 네트워크(Generative Query Network, GQN)</strong> 프레임워크를 제안했다.2</p>
<p>GQN의 가장 큰 특징은 객체의 종류, 위치, 색상 등에 대한 어떠한 종류의 레이블도 사용하지 않는 완전한 비지도 학습 방식이라는 점이다.2 이는 기존의 컴퓨터 비전 시스템이 막대한 양의 수작업으로 레이블링된 데이터셋에 의존했던 한계를 극복하려는 시도였다. GQN은 모델에게 “네가 이 장면을 진정으로 이해했다면, 보지 못한 각도에서도 어떻게 보일지 그려낼 수 있어야 한다“는 과제를 부여함으로써, 모델 스스로 장면의 3차원 구조와 물리적 속성을 내재적으로 학습하도록 유도했다. 이는 ’분석을 통한 합성(analysis by synthesis)’이라는 고전적인 비전 이론을 현대 딥러닝 기술로 성공적으로 구현한 사례로 평가받는다.</p>
<h3>3.2  모델 구조: 표현 네트워크와 생성 네트워크의 상호작용</h3>
<p>GQN 프레임워크는 상호 보완적인 역할을 수행하는 두 개의 핵심 신경망, 즉 **표현 네트워크(Representation Network)**와 **생성 네트워크(Generation Network)**로 구성된다.2</p>
<h4>3.2.1  표현 네트워크 (Representation Network, <code>f</code>)</h4>
<p>표현 네트워크의 역할은 에이전트가 수집한 여러 관찰 데이터를 하나의 통합된 장면 표현으로 압축하는 것이다.</p>
<ul>
<li>
<p><strong>입력:</strong> 특정 장면에 대한 관찰 데이터, 즉 여러 장의 이미지 <span class="math math-inline">(x^k)</span>와 해당 이미지가 촬영된 카메라의 시점(위치 및 방향) 정보 <span class="math math-inline">(v^k)</span>의 집합이다.</p>
</li>
<li>
<p><strong>기능:</strong> 이 네트워크는 입력된 각 <code>(이미지, 시점)</code> 쌍을 독립적으로 인코딩한 후, 이들을 순서에 상관없이 하나의 간결한 **장면 표현 벡터 <span class="math math-inline">r</span>**로 통합한다. 보통 이 통합 과정은 인코딩된 벡터들을 모두 합산(summation)하는 방식으로 이루어진다. 이 벡터 <span class="math math-inline">r</span>는 장면의 본질적인 모든 정보—예를 들어, 객체의 종류, 3차원 위치, 색상, 크기, 그리고 방의 구조와 조명 상태 등—를 담고 있는 압축된 기술자(descriptor)가 되어야 한다.19</p>
</li>
</ul>
<h4>3.2.2  생성 네트워크 (Generation Network, <code>g</code>)</h4>
<p>생성 네트워크는 표현 네트워크가 만든 압축된 장면 표현을 바탕으로 새로운 이미지를 ’상상’해내는 역할을 한다.</p>
<ul>
<li>
<p><strong>입력:</strong> 표현 네트워크가 생성한 장면 표현 벡터 <span class="math math-inline">r</span>과, 이미지를 생성하고자 하는 새로운 **질의 시점 <span class="math math-inline">v_q</span>**이다.</p>
</li>
<li>
<p><strong>기능:</strong> 이 네트워크는 장면 표현 <span class="math math-inline">r</span>을 조건(condition)으로 받아, 질의 시점 <span class="math math-inline">v_q</span>에서 관찰될 이미지를 생성(렌더링)한다. 이 과정은 확률적(stochastic)으로 이루어지며, 잠재 변수(latent variables) <span class="math math-inline">z</span>를 도입하여 동일한 장면 표현과 시점에서도 약간씩 다른 이미지를 생성할 수 있도록 하여 불확실성을 모델링한다. 본질적으로, 생성 네트워크는 데이터로부터 원근법, 가려짐(occlusion), 조명 및 그림자 효과 등을 스스로 학습한 **신경 렌더러(neural renderer)**로 기능한다.2</p>
</li>
</ul>
<h3>3.3  학습 목표: 변분 추론을 통한 증거 하한 최대화</h3>
<p>GQN은 내부에 확률적인 잠재 변수 <span class="math math-inline">z</span>를 포함하는 생성 모델이기 때문에, 주어진 데이터에 대한 우도(likelihood)를 직접 계산하여 최적화하는 것이 수학적으로 매우 어렵다(intractable). 이러한 문제를 해결하기 위해 GQN은 **변분 추론(Variational Inference)**이라는 근사 기법을 사용한다.22 변분 추론은 직접 최적화하기 어려운 실제 사후 분포</p>
<p><span class="math math-inline">p(z|x)</span> 대신, 다루기 쉬운 근사 분포 <span class="math math-inline">q(z|x)</span>를 도입하여 이 둘의 차이(KL-divergence)를 최소화하는 방식으로 학습을 진행한다. 이는 **증거 하한(Evidence Lower Bound, ELBO)**을 최대화하는 것과 동일한 목표를 가진다.24</p>
<p>GQN의 학습 목표는 크게 두 부분으로 구성된다.</p>
<ol>
<li>
<p><strong>재구성 항 (Reconstruction Term):</strong> 생성 네트워크가 질의 시점에서 생성한 이미지가 실제 정답 이미지와 최대한 유사해지도록 하는 항이다. 이는 생성된 이미지의 픽셀 값과 실제 이미지의 픽셀 값 간의 차이를 최소화하는 방향으로 학습된다.</p>
</li>
<li>
<p><strong>정규화 항 (Regularization Term):</strong> 잠재 변수 <span class="math math-inline">z</span>의 사후 분포(posterior)가 사전 분포(prior)와 유사해지도록 하는 항으로, 보통 두 분포 간의 KL-divergence로 측정된다. 이는 모델이 너무 복잡한 잠재 공간을 학습하지 않도록 제어하여 과적합을 방지하고 일반화 성능을 높이는 역할을 한다.</p>
</li>
</ol>
<p>이 학습 과정에서 중요한 점은, 표현 네트워크 <span class="math math-inline">f</span>가 생성 네트워크 <span class="math math-inline">g</span>가 어떤 시점에서 질의를 받을지 미리 알 수 없다는 것이다. 따라서 표현 네트워크는 생성 네트워크가 어떤 시점의 이미지를 요구하더라도 정확하게 렌더링할 수 있도록, 장면의 모든 중요한 정보를 손실 없이 장면 표현 벡터 <span class="math math-inline">r</span>에 압축해야만 한다. 이는 일종의 정보 병목(information bottleneck) 현상을 만들어, 모델이 자연스럽게 추상적이고 핵심적인 장면 표현을 학습하도록 유도하는 핵심 메커니즘으로 작용한다.2</p>
<h3>3.4  컴퓨터 비전 분야에 미친 영향</h3>
<p>GQN의 등장은 컴퓨터 비전, 특히 3D 비전과 표현 학습 분야에 상당한 영향을 미쳤다.</p>
<p>첫째, <strong>비지도 3D 장면 이해의 돌파구를 마련</strong>했다. GQN은 별도의 3D 모델이나 레이블 없이, 오직 2D 이미지들로부터 3차원 공간에 대한 깊이 있는 이해(객체의 존재, 3D 위치, 시점 변화에 따른 외형 변화, 가려짐 등)가 가능함을 강력하게 입증했다.18 이는 AI가 인간처럼 시각적 관찰만으로 세상의 3차원 구조를 학습할 수 있는 가능성을 열어주었다.</p>
<p>둘째, <strong>신경 렌더링 및 NeRF의 중요한 선구자 역할</strong>을 했다. GQN이 제시한 ‘특정 3D 좌표(시점)를 입력받아 해당 위치의 픽셀 값(색상, 밀도)을 출력하는’ 연속적인 함수를 신경망으로 모델링하는 아이디어는, 2020년에 등장하여 3D 비전 분야를 혁신한 **Neural Radiance Fields (NeRF)**의 핵심 개념에 직접적인 영감을 주었다. NeRF는 GQN의 아이디어를 더욱 발전시켜, 장면을 신경망의 가중치에 완벽하게 내재화하고 고품질의 새로운 뷰를 합성하는 데 성공했다. 이런 점에서 GQN은 NeRF의 철학적, 구조적 선구자 모델로 평가받는다.25</p>
<p>셋째, <strong>데이터 효율적인 강화학습의 가능성을 제시</strong>했다. GQN으로 학습된 간결하고 의미 있는 장면 표현 벡터 <span class="math math-inline">r</span>을 강화학습 에이전트의 상태(state) 입력으로 사용했을 때, 고차원의 원본 픽셀 이미지를 직접 사용하는 것보다 훨씬 적은 환경과의 상호작용(논문에 따르면 약 4배 더 효율적)만으로도 과업을 성공적으로 학습할 수 있음을 보였다.2 이는 잘 학습된 세계 모델(world model)이 에이전트의 학습 효율을 극적으로 향상시킬 수 있음을 시사하며, 이후 모델 기반 강화학습(model-based reinforcement learning) 연구에 중요한 방향을 제시했다.</p>
<h2>4.  과업의 지도를 그리다: ’Taskonomy’와 전이 학습의 구조화</h2>
<h3>4.1  연구 개요: 시각적 과업 공간의 관계망 규명</h3>
<p>2018년 CVPR에서 최우수 논문상(Best Paper Award)을 수상한 “Taskonomy: Disentangling Task Transfer Learning“은 컴퓨터 비전 분야의 오랜 질문에 대한 계산적인 해답을 제시했다.27 스탠퍼드 대학과 UC 버클리 공동 연구팀(Amir R. Zamir, Alexander Sax 등)은 “수많은 시각적 인식 과업들은 서로 독립적인가, 아니면 그들 사이에 구조적인 관계가 존재하는가?“라는 근본적인 질문을 던졌다.30 예를 들어, 이미지의 표면 법선(surface normal)을 아는 것이 깊이(depth)를 추정하는 데 도움이 될 수 있는가와 같은 문제이다.</p>
<p>이 논문은 이러한 과업 간의 관계를 인간의 직관이 아닌, 순전히 계산적인 방법으로 규명하고자 했다. 연구팀은 26개의 다양한 시각적 과업(깊이 추정, 의미 분할, 엣지 검출, 객체 분류 등)들 사이의 **전이 학습 종속성(transfer learning dependencies)**을 정량적으로 측정하고, 이를 바탕으로 과업 공간의 <strong>계산적 분류 체계(computational taxonomic map)</strong>, 즉 ’Taskonomy’를 구축하는 포괄적인 방법론을 제안했다.4 이 지도는 어떤 과업이 다른 과업을 학습하는 데 유용한 특징을 제공하는지, 그리고 그 관계의 강도는 어느 정도인지를 명확히 보여준다.</p>
<h3>4.2  방법론: 과업 간 유사성의 계산적 측정</h3>
<p>Taskonomy는 과업 공간의 구조를 밝히기 위해 4단계의 체계적인 과정을 거친다. 이 방법론의 핵심은 ‘전이 학습’ 자체를 하나의 측정 가능한 객체로 정의하고, 과업 간의 관계를 경험적으로 탐색하는 과학적 접근법을 도입한 것이다.</p>
<ol>
<li>1단계: 과업별 모델 학습 (Task-Specific Modeling):</li>
</ol>
<p>먼저, 26개의 각 시각적 과업에 대해 독립적인 인코더-디코더(Encoder-Decoder) 구조의 신경망을 지도 학습 방식으로 훈련시킨다. 이 과정을 통해 각 과업을 해결하는 데 최적화된 특징 추출기(인코더)와 출력 생성기(디코더)를 확보한다. 이 단계는 이후 과업 간 관계를 측정하기 위한 ’기준 모델’들을 구축하는 과정이다.33</p>
<ol start="2">
<li>2단계: 전이 학습 수행 및 평가 (Transfer Modeling):</li>
</ol>
<p>과업 간의 관계를 측정하기 위해, 특정 소스 과업(source task) A와 타겟 과업(target task) B를 선택한다. 그리고 A를 학습시킨 모델의 인코더는 가중치를 고정(freeze)시킨다. 이 고정된 인코더가 추출한 특징 표현을 입력으로 받아 B 과업의 출력을 예측하는 작은 ’전이 네트워크(transfer network)’를 학습시킨다. 이때 전이 네트워크가 달성하는 성능이 바로 “소스 과업 A가 타겟 과업 B에 얼마나 유용한가“를 나타내는 정량적인 ‘과업 유사도(task affinity)’ 지표가 된다. 즉, “소스 과업의 인코더가 만든 특징 공간에서 타겟 과업의 정답을 얼마나 쉽게 읽어낼 수 있는가?“를 측정하는 것이다.33</p>
<ol start="3">
<li>3단계: 과업 유사도 행렬 구축 (Affinity Matrix):</li>
</ol>
<p>26개의 과업에 대해 가능한 모든 소스-타겟 쌍(26 x 26)에 대해 2단계 과정을 반복한다. 이를 통해 과업 간의 전이 성능을 담은 거대한 비대칭적(asymmetric) 유사도 행렬을 구축한다. 이 행렬이 비대칭적인 이유는, 예를 들어, 표면 법선 예측(A)에서 깊이 추정(B)으로의 전이 성능이 깊이 추정(B)에서 표면 법선 예측(A)으로의 전이 성능과 다를 수 있기 때문이다. 이 행렬 자체가 바로 과업 공간의 구조를 담고 있는 원시 데이터가 된다.33</p>
<ol start="4">
<li>4단계: 최적의 전이 정책 탐색 (Optimal Policy Search):</li>
</ol>
<p>마지막으로, 구축된 유사도 행렬을 바탕으로 실용적인 문제를 해결한다. 예를 들어, “레이블링 비용의 제약으로 인해 5개의 과업만 처음부터 학습시킬 수 있다면(supervision budget), 어떤 5개를 선택하고 나머지 과업들은 어떤 경로로 전이 학습을 시켜야 전체 타겟 과업들의 평균 성능을 최대화할 수 있는가?“와 같은 문제를 푼다. 이 최적의 전이 경로(그래프)를 찾기 위해 이진 정수 계획법(Binary Integer Program, BIP)과 같은 최적화 기법을 사용한다.32</p>
<h3>4.3  주요 발견 및 의의</h3>
<p>Taskonomy 연구는 시각적 과업과 전이 학습에 대한 여러 중요한 통찰을 제공했다.</p>
<p>첫째, <strong>데이터 효율성의 극적인 향상</strong>을 입증했다. Taskonomy가 제안한 최적의 전이 정책을 활용하면, 10개의 서로 다른 시각적 과업을 해결하는 데 필요한 총 레이블 데이터의 양을 각 과업을 독립적으로 학습시킬 때와 비교하여 약 <strong>2/3</strong>까지 줄이면서도 거의 동일한 성능을 유지할 수 있음을 실험적으로 보였다.4 이는 전이 학습의 구조적 활용이 데이터 수집 비용을 획기적으로 절감할 수 있음을 의미한다.</p>
<p>둘째, <strong>인간의 직관과 다른 비직관적인 과업 관계를 발견</strong>했다. 분석 결과, 3차원 정보를 다루는 과업들(예: 깊이 추정, 표면 법선 예측, 3D 키포인트 검출)이 2차원 의미론적 과업(예: 객체 분류, 장면 분류)을 위한 매우 강력한 특징을 제공하는 경향이 나타났다. 놀랍게도, 어떤 경우에는 ImageNet 데이터셋으로 사전 훈련된 모델을 사용하는 것보다 Taskonomy가 찾아낸 특정 3D 과업에서 전이 학습하는 것이 더 나은 성능을 보이기도 했다. 이는 전이 학습의 소스 과업 선택에 대한 기존의 통념(ImageNet이 항상 최선이다)에 도전하는 중요한 발견이었다.33</p>
<p>셋째, <strong>멀티태스크 학습(Multi-task Learning) 및 메타 학습(Meta-learning)에 대한 이론적 기반을 제공</strong>했다. Taskonomy의 과업 관계 지도는 어떤 과업들을 하나의 모델로 함께 학습시킬 때 긍정적인 시너지 효과를 낼 수 있는지(멀티태스크 학습), 그리고 어떤 소스 과업이 다양한 타겟 과업에 유용한 일반적인 표현을 제공하여 학습을 가속화할 수 있는지(메타 학습)에 대한 정량적이고 구조적인 가이드라인을 제시했다. 이는 보다 체계적이고 효율적인 AI 시스템을 설계하는 데 중요한 이론적 토대가 되었다.4 이로써 AI 연구자들은 과업 공간을 탐색할 수 있는 ’지도’와 ’나침반’을 얻게 되었으며, 이는 AI가 세상을 인식하는 다양한 방식(과업)들 간의 근본적인 구조를 이해하려는 중요한 시도였다.</p>
<h2>5.  물리 법칙을 학습하는 로봇: RSS 2018의 미분 가능한 물리 기반 제어</h2>
<h3>5.1  연구 개요: 기호적 계획과 연속적 제어의 통합</h3>
<p>2018년 로보틱스 분야 최고 학회인 RSS(Robotics: Science and Systems)에서 최우수 논문상을 수상한 “Differentiable Physics and Stable Modes for Tool-Use and Manipulation Planning“은 로봇 조작 계획 분야의 오랜 난제를 해결하기 위한 혁신적인 접근법을 제시했다.10 Marc Toussaint, Kelsey Allen, Kevin Smith, Joshua Tenenbaum 등 MIT와 슈투트가르트 대학의 연구진은 로봇이 단순히 물체를 집고 옮기는 것을 넘어, 도구를 사용해 다른 물체를 치거나 던지는 등 복잡하고 동적인 상호작용을 포함하는 장기적인 과업을 어떻게 계획할 수 있는지에 대한 문제를 다루었다.10</p>
<p>이 연구의 핵심 제안은 추상적이고 기호적인 수준의 <strong>과업 및 동작 계획(Task and Motion Planning, TAMP)</strong> 프레임워크와, 물리적 상호작용을 경사 하강법 기반으로 최적화할 수 있는 <strong>미분 가능한 물리(Differentiable Physics)</strong> 개념을 통합하는 것이었다.34 이는 로봇의 행동을 이산적인 결정(무엇을 할 것인가)과 연속적인 움직임(어떻게 할 것인가)의 두 수준에서 동시에 최적화하는 새로운 길을 열었다.</p>
<h3>5.2  방법론: 논리-기하학 프로그램 (Logic-Geometric Program, LGP)</h3>
<p>로봇이 물리적 세계와 상호작용할 때 가장 큰 어려움은 ’접촉(contact)’이다. 물체와 접촉하거나 떨어지는 순간, 시스템의 동역학은 불연속적으로 변하며, 이는 전통적인 연속 최적화 기법을 적용하기 어렵게 만든다.36 이 연구는 이러한 ‘하이브리드(hybrid)’ 특성을 가진 문제를 해결하기 위해 **논리-기하학 프로그램(Logic-Geometric Program, LGP)**이라는 독창적인 프레임워크를 제안했다.</p>
<p>LGP의 핵심 아이디어는 전체 문제를 논리적으로 구별되는 여러 **‘모드(modes)’**의 순차적 결합으로 분해하는 것이다. 각 모드 내에서는 시스템의 동역학이 부드럽고 미분 가능한 제약 조건으로 표현될 수 있다.</p>
<ul>
<li>
<p><strong>안정적 모드 (Stable Modes):</strong> 로봇이 물체를 굳게 잡고 있거나, 물체가 바닥에 안정적으로 놓여있는 등, 객체 간의 상대적인 운동학적 관계가 일정하게 유지되는 구간이다. 이러한 모드는 시스템의 움직임을 기하학적인 **운동학적 제약(kinematic constraints)**으로 모델링할 수 있다.34</p>
</li>
<li>
<p><strong>동적 모드 (Dynamic Modes):</strong> 물체가 공중으로 던져지거나, 다른 물체와 충돌하는 등, 뉴턴의 운동 법칙과 같은 동역학 법칙의 지배를 받는 구간이다. 이 연구에서는 이러한 상호작용을 **미분 가능한 동역학 및 충격량 교환 제약(differentiable dynamical and impulse exchange constraints)**으로 수학적으로 공식화했다.36</p>
</li>
</ul>
<p>LGP 프레임워크는 이 두 가지 종류의 모드를 사용하여 다음과 같이 작동한다. 먼저, TAMP의 상위 수준 기호적 계획기(symbolic planner)가 문제 해결을 위한 가능한 모드 시퀀스(예: ‘물체를 잡는다’ → ‘팔을 휘두른다’ → ‘물체를 놓아 던진다’ → ‘물체가 목표물과 충돌한다’)를 탐색한다. 그러면 하위 수준의 최적화기는 탐색된 각 모드 시퀀스에 해당하는 로봇의 전체 궤적(시간에 따른 관절 각도 등)을 하나의 거대한 비선형 최적화 문제로 구성하여 푼다. 이 최적화 문제의 제약 조건들은 각 모드에 맞게 정의된 미분 가능한 운동학 및 동역학 법칙들이다.38</p>
<p>이러한 하이브리드 접근법을 통해, LGP는 기호적 추론의 장점(장기적인 계획 수립)과 연속적 최적화의 장점(정밀하고 물리적으로 타당한 궤적 생성)을 효과적으로 결합할 수 있었다. 이는 AI가 추상적인 계획을 실제 물리적 행동으로 변환하는 능력에 있어 중요한 진일보였다.</p>
<h3>5.3  로보틱스 분야에 미친 영향</h3>
<p>이 연구는 로보틱스, 특히 조작 및 제어 분야에 여러 중요한 영향을 미쳤다.</p>
<p>첫째, <strong>Sim-to-Real 문제의 핵심 기술을 제시</strong>했다. 미분 가능한 물리 엔진 개념은 시뮬레이션 환경 내에서 경사 하강법(gradient descent)과 같은 강력한 최적화 기법을 사용하여 로봇의 제어 정책을 직접 최적화할 수 있는 길을 열었다. 이는 시뮬레이션에서 학습한 정책을 실제 로봇으로 효과적으로 이전(sim-to-real transfer)하는 데 있어 핵심적인 기술적 토대가 되었으며, 이후 많은 학습 기반 로봇 제어 연구의 기반이 되었다.</p>
<p>둘째, <strong>접촉이 풍부한(Contact-Rich) 조작 문제에 대한 새로운 해법을 제공</strong>했다. 도구를 사용하거나, 물체를 미끄러뜨리거나, 던지는 등 복잡한 접촉과 동역학이 수반되는 조작 과업은 기존의 계획 방법들이 다루기 매우 어려웠다. LGP 프레임워크는 이러한 문제들에 대한 체계적이고 통합된 해결책을 제시함으로써 로봇 조작 연구의 범위를 크게 확장시켰다.39</p>
<p>셋째, <strong>학습 기반 로보틱스와의 융합을 촉진</strong>했다. 이 연구는 물리 시뮬레이터를 마치 신경망의 한 레이어처럼 취급하여, 전체 시스템을 종단간(end-to-end)으로 학습하고 최적화할 수 있다는 아이디어에 큰 영감을 주었다. 이는 이후 물리 법칙을 신경망 모델에 통합하는 Physics-Informed Neural Networks (PINNs)나, 복잡한 동역학을 학습하여 제어하는 모델 기반 강화학습 연구의 발전을 촉진하는 계기가 되었다.40</p>
<h2>6.  종합 및 전망: 2018년 6월이 남긴 유산과 미래</h2>
<h3>6.1  4대 연구의 통합적 분석: 표현 학습이라는 공통 분모</h3>
<p>본 보고서에서 분석한 2018년 6월의 네 가지 핵심 연구—GPT-1, GQN, Taskonomy, Differentiable Physics—는 표면적으로는 각각 자연어 처리, 컴퓨터 비전, 로보틱스라는 서로 다른 분야의 문제를 다루는 것처럼 보인다. 그러나 그 기저에는 **‘데이터로부터 어떻게 유용하고 일반화 가능한 표현(representation)을 학습할 것인가’**라는 하나의 거대한 질문이 관통하고 있다. 이들 연구는 모두 기존의 지도 학습 방식이 가진 데이터 의존성과 과업 특수성의 한계를 인식하고, 데이터에 내재된 구조를 활용하여 보다 근본적인 지식을 추출하려는 시도였다는 공통점을 가진다.</p>
<ul>
<li>
<p><strong>GPT-1</strong>은 방대한 텍스트의 순차적 구조 속에서 단어와 문장의 문맥적 의미를 포착하는 표현을 학습했다.</p>
</li>
<li>
<p><strong>GQN</strong>은 동일한 장면에 대한 다중 시점 이미지들로부터 3차원 공간과 객체에 대한 기하학적 표현을 학습했다.</p>
</li>
<li>
<p><strong>Taskonomy</strong>는 다양한 시각적 과업들 간의 관계로부터 어떤 표현이 다른 과업에 유용한지에 대한 구조적 표현을 학습했다.</p>
</li>
<li>
<p><strong>Differentiable Physics</strong>는 복잡한 물리적 상호작용을 이산적인 ’모드’와 연속적인 ’동역학’의 결합으로 표현하는 방법을 제시했다.</p>
</li>
</ul>
<p>이 네 연구는 AI가 단순히 입력과 출력 사이의 함수를 근사하는 것을 넘어, 데이터의 생성 원리나 세상의 근본적인 구조를 이해하는 방향으로 나아가야 함을 역설했다. 이는 표현 학습이 AI 연구의 핵심 과제로 부상했음을 알리는 중요한 신호탄이었다.</p>
<h3>6.2  파운데이션 모델 시대의 서막</h3>
<p>2018년 6월의 연구들은 오늘날 AI 분야를 지배하고 있는 **파운데이션 모델(Foundation Models)**의 핵심 철학을 명확하게 예고했다. 파운데이션 모델이란, 대규모 데이터로 사전 학습된 거대한 단일 모델을 다양한 다운스트림 과업에 적용(미세 조정 또는 프롬프팅)하는 모델을 의미한다.</p>
<ul>
<li>
<p><strong>GPT-1</strong>은 그 이름(Generative Pre-trained Transformer)에서 알 수 있듯이, 언어 파운데이션 모델의 직접적인 시조이다. ’사전 훈련 후 미세 조정’이라는 패러다임을 확립함으로써 이후 BERT와 GPT-3와 같은 초거대 언어 모델의 탄생을 이끌었다.</p>
</li>
<li>
<p><strong>GQN</strong>과 <strong>Taskonomy</strong>는 시각 파운데이션 모델(Vision Foundation Models)의 가능성을 탐색했다. GQN은 레이블 없이도 3D 세계에 대한 풍부한 시각적 표현을 학습할 수 있음을 보였고, Taskonomy는 어떤 사전 훈련 과업이 다양한 다운스트림 시각 과업에 가장 효과적인지에 대한 체계적인 방법론을 제시했다.</p>
</li>
<li>
<p><strong>Differentiable Physics</strong>는 물리 세계와 상호작용하는 로봇을 위한 파운데이션 모델, 즉 로보틱스 파운데이션 모델(Robotics Foundation Models)의 개념적 기반을 제공했다. 물리 법칙을 내재화한 모델은 다양한 로봇과 환경에 일반화될 수 있는 잠재력을 가지며, 이는 최근의 Robotics Transformer와 같은 연구 흐름으로 이어지고 있다.</p>
</li>
</ul>
<p>특히 이 시기의 연구들은 ‘스케일업(scale-up)’ 전략의 유효성을 암시했다는 점에서 중요하다. GPT-1의 저자들은 더 많은 데이터와 컴퓨팅이 성능 향상으로 이어질 것이라 예측했으며 1, 이는 이후 AI 연구가 모델과 데이터의 크기를 기하급수적으로 늘리는 방향으로 나아가는 가장 중요한 트렌드가 되었다.</p>
<h3>6.3  결론 및 향후 전망</h3>
<p>결론적으로, 2018년 6월은 AI 연구 역사에서 중요한 변곡점으로 기록될 만하다. 이 시기를 기점으로 AI는 특정 과업을 정교하게 해결하는 ’도구’의 단계를 넘어, 세상에 대한 범용적인 지식과 표현을 데이터로부터 스스로 학습하는 ’에이전트’로 나아가는 중요한 첫걸음을 내디뎠다. 이때 제시된 생성적 사전 훈련, 비지도 표현 학습, 과업 간 관계의 구조화, 미분 가능한 물리 모델링과 같은 아이디어들은 현재의 생성 AI(Generative AI)와 물리 AI(Embodied AI) 혁명의 직접적인 뿌리가 되었다.</p>
<p>향후 AI 연구는 2018년 6월에 제시된 방향성을 더욱 심화시켜 나갈 것으로 전망된다. 즉, 더욱 일반화되고, 물리 세계에 단단히 기반하며(grounded), 데이터 효율적으로 학습하는 AI를 향한 연구가 계속될 것이다. 2018년 6월의 유산은 앞으로 다가올 AI 기술의 미래를 이해하는 데 있어 필수적인 통찰을 제공한다.</p>
<table><thead><tr><th>구분</th><th>GPT-1 (OpenAI)</th><th>GQN (DeepMind)</th><th>Taskonomy (Stanford/Berkeley)</th><th>Differentiable Physics (MIT/Stuttgart)</th></tr></thead><tbody>
<tr><td><strong>핵심 문제</strong></td><td>언어 이해의 일반화 및 전이 학습</td><td>레이블 없는 데이터 기반 3D 장면 이해</td><td>시각적 과업 간의 구조적 관계 규명</td><td>동적, 접촉-집중 조작 계획</td></tr>
<tr><td><strong>핵심 방법론</strong></td><td>생성적 사전 훈련 및 지도 미세 조정</td><td>생성적 질의 네트워크 (표현/생성 네트워크)</td><td>과업 간 전이 성능의 계산적 측정</td><td>논리-기하학 프로그램 (TAMP + 미분 가능 물리)</td></tr>
<tr><td><strong>학습 데이터</strong></td><td>대규모 레이블 없는 텍스트 코퍼스</td><td>에이전트가 수집한 다중 시점 2D 이미지</td><td>26개 과업에 대한 지도 학습 데이터셋</td><td>(해당 없음, 최적화 기반 계획)</td></tr>
<tr><td><strong>기술적 기여</strong></td><td>‘Pre-train, Fine-tune’ 패러다임 확립</td><td>신경 렌더러 개념 및 비지도 3D 표현 학습</td><td>과업 공간의 정량적 매핑 방법론</td><td>기호적 계획과 연속적 최적화의 통합</td></tr>
<tr><td><strong>주요 영향 분야</strong></td><td>자연어 처리 (NLP), 파운데이션 모델</td><td>3D 비전, 신경 렌더링, 모델 기반 RL</td><td>멀티태스크 학습, 메타 학습, AutoML</td><td>로봇 조작 계획, Sim-to-Real, 물리 기반 학습</td></tr>
<tr><td><strong>후속 대표 연구</strong></td><td>BERT, GPT-2/3, 대규모 언어 모델 (LLM)</td><td>Neural Radiance Fields (NeRF)</td><td>Task2Vec, Automated ML (AutoML)</td><td>Physics-Informed ML, 로보틱스 트랜스포머</td></tr>
</tbody></table>
<h2>7. 참고 자료</h2>
<ol>
<li>Improving language understanding with unsupervised learning …, https://openai.com/index/language-unsupervised/</li>
<li>Neural scene representation and rendering - Ali Eslami, https://arkitus.com/files/science-18-gqn-preprint.pdf</li>
<li>Improving Language Understanding by Generative Pre-Training - OpenAI, https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf</li>
<li>Taskonomy: Disentangling Task Transfer Learning | Request PDF - ResearchGate, https://www.researchgate.net/publication/324717660_Taskonomy_Disentangling_Task_Transfer_Learning</li>
<li>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), https://www.computer.org/csdl/proceedings/cvpr/2018/17D45VtKirt</li>
<li>CVPR 2018 : 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition : proceedings : 18-22 June 2018, Salt Lake City, Utah - James Madison University, <a href="https://search.lib.jmu.edu/discovery/fulldisplay?vid=01JMU_INST:01JMU&amp;search_scope=MyInst_and_CI&amp;tab=Everything&amp;docid=alma991016488201806271&amp;lang=en&amp;context=L&amp;adaptor=Local+Search+Engine&amp;query=creator,exact,IEEE+Computer+Society.,AND&amp;mode=advanced&amp;facet=creator,exact,IEEE+Computer+Society.&amp;offset=0">https://search.lib.jmu.edu/discovery/fulldisplay?vid=01JMU_INST%3A01JMU&amp;search_scope=MyInst_and_CI&amp;tab=Everything&amp;docid=alma991016488201806271&amp;lang=en&amp;context=L&amp;adaptor=Local%20Search%20Engine&amp;query=creator%2Cexact%2CIEEE%20Computer%20Society.%2CAND&amp;mode=advanced&amp;facet=creator%2Cexact%2CIEEE%20Computer%20Society.&amp;offset=0</a></li>
<li>Computer Vision and Pattern Recognition (CVPR), 2018 | ServiceNow Research, https://www.servicenow.com/research/event/2018-cvpr.html</li>
<li>cvpr 2018: CVPR - researchr conference, https://researchr.org/conference/cvpr-2018</li>
<li>Robotics Jun 2018 - arXiv, https://arxiv.org/list/cs.RO/2018-06</li>
<li>Awards · Robotics: Science and Systems - RISLab, http://rislab.org/rss2018website/program/awards/</li>
<li>Best Paper Award - RSS Foundation, https://roboticsfoundation.org/awards/best-paper-award/</li>
<li>Radford, A., Narasimhan, K., Salimans, T. and Sutskever, I. (2018) Improving Language Understanding with Unsupervised Learning. Technical Report, OpenAI. - References - Scirp.org., https://www.scirp.org/reference/referencespapers?referenceid=2624986</li>
<li>Improving language understanding by generative pre-training - BibSonomy, https://www.bibsonomy.org/bibtex/2f37881405991d6cd69534d152ff72e93/ghagerer</li>
<li>Improving Language Understanding by Generative Pre-Training - cs.Princeton, https://www.cs.princeton.edu/courses/archive/spring20/cos598C/lectures/lec4-pretraining.pdf</li>
<li>(PDF) Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions - ResearchGate, https://www.researchgate.net/publication/370869544_Generative_Pre-trained_Transformer_A_Comprehensive_Review_on_Enabling_Technologies_Potential_Applications_Emerging_Challenges_and_Future_Directions</li>
<li>When and why did ‘training’ become ‘pretraining’? - LessWrong, https://www.lesswrong.com/posts/tCCzZa9tyuciAPhks/when-and-why-did-training-become-pretraining</li>
<li>Neural scene representation and rendering | Request PDF - ResearchGate, https://www.researchgate.net/publication/325774459_Neural_scene_representation_and_rendering</li>
<li>Neural scene representation and rendering - PubMed, https://pubmed.ncbi.nlm.nih.gov/29903970/</li>
<li>Neural scene representation and rendering - Google DeepMind, https://deepmind.google/discover/blog/neural-scene-representation-and-rendering/</li>
<li>Neural scene representation and rendering - Gwern, https://gwern.net/doc/reinforcement-learning/model/2018-eslami.pdf</li>
<li>GQN (Generative Query Network) | Envisioning Vocab, https://www.envisioning.io/vocab/gqn-generative-query-network</li>
<li>Generative Query Network. Introduction | by Sherwin Chen | The Startup - Medium, https://medium.com/swlh/generative-query-network-6e26423e3ac</li>
<li>Variational Inference - DAXPY, https://daxpy.xyz/posts/variational-inference/</li>
<li>Variational Boosting: Iteratively Refining Posterior Approximations - Laboratory for Intelligent Probabilistic Systems, https://lips.cs.princeton.edu/pdfs/miller2017boosting.pdf</li>
<li>Neural Radiance Fields - stfwn, https://stfwn.com/articles/neural-radiance-fields/</li>
<li>(PDF) An overview of Neural Radiance Fields - ResearchGate, https://www.researchgate.net/publication/382373557_An_overview_of_Neural_Radiance_Fields</li>
<li>Disentangling Task Transfer Learning (CVPR 2018 Best Paper Winner, https://twimlai.com/podcast/twimlai/taskonomy-disentangling-task-transfer-learning-cvpr-2018-best-paper-winner-amir-zamir/</li>
<li>CVPR 2018 Kicks Off; Best Papers Announced | by Synced …, https://medium.com/syncedreview/cvpr-2018-kicks-off-best-papers-announced-d3361bcc6984</li>
<li>CVPR Best Paper Award - IEEE Computer Society Technical Committee on Pattern Analysis and Machine Intelligence, https://tc.computer.org/tcpami/2022/08/22/cvpr-best-paper-award/</li>
<li>Best Paper CVPR2018 - Taskonomy - Disentangling Task Transfer Learning - YouTube, https://www.youtube.com/watch?v=9mdCWMVAMLg</li>
<li>Taskonomy: Disentangling Task Transfer Learning [Best Paper, CVPR2018] - GitHub, https://github.com/StanfordVL/taskonomy</li>
<li>Taskonomy: Disentangling Task Transfer Learning - CVF Open Access, https://openaccess.thecvf.com/content_cvpr_2018/papers/Zamir_Taskonomy_Disentangling_Task_CVPR_2018_paper.pdf</li>
<li>Taskonomy: Disentangling Task Transfer Learning - IJCAI, https://www.ijcai.org/proceedings/2019/0871.pdf</li>
<li>Differentiable Physics and Stable Modes for Tool-Use and Manipulation Planning - MIT, https://www.mit.edu/~k2smith/pdf/ToussaintEtAl_DiffPhysStable.pdf</li>
<li>Differentiable Physics and Stable Modes for Tool-Use and Manipulation Planning, https://dspace.mit.edu/handle/1721.1/126626</li>
<li>Differentiable Physics and Stable Modes for Tool-Use and Manipulation Planning - IJCAI, https://www.ijcai.org/proceedings/2019/0869.pdf</li>
<li>Differentiable Physics and Stable Modes for Tool-Use and Manipulation Planning - Robotics, https://www.roboticsproceedings.org/rss14/p44.pdf</li>
<li>Differentiable Physics and Stable Modes for Tool-Use and Manipulation Planning - IJCAI, https://www.ijcai.org/proceedings/2019/869</li>
<li>Differentiable Physics and Stable Modes for Tool-Use and Manipulation Planning | Request PDF - ResearchGate, https://www.researchgate.net/publication/343920300_Differentiable_Physics_and_Stable_Modes_for_Tool-Use_and_Manipulation_Planning</li>
<li>Differentiable Physics and Stable Modes for Tool-Use and Manipulation Planning | Request PDF - ResearchGate, https://www.researchgate.net/publication/326740135_Differentiable_Physics_and_Stable_Modes_for_Tool-Use_and_Manipulation_Planning</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>