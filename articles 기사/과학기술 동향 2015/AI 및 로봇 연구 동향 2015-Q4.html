<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2015년 4분기 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2015년 4분기 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2015년 AI 및 로봇 연구 동향</a> / <span>2015년 4분기 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2015년 4분기 AI 및 로봇 연구 동향</h1>
<h2>1. 서론</h2>
<p>2015년 4분기는 인공지능(AI) 및 로봇 공학 분야의 역사에서 단순한 시간의 흐름을 넘어, 기술적 패러다임이 근본적으로 전환된 결정적 ’변곡점(inflection point)’으로 기록된다. 이 시기는 이전까지 점진적으로 축적되어 온 연구 성과들이 질적인 도약으로 이어진 시기였으며, 특히 심층 신경망(Deep Neural Network, DNN)의 이론적 한계가 극복되고 그 응용 가능성이 전례 없는 속도로 확장되기 시작한 기점이었다.1</p>
<p>이 시기의 폭발적인 발전은 개별 연구들의 독립적인 성과가 아니라, 서로 긴밀하게 연결된 세 가지 핵심 동력의 시너지 효과에 의해 추동되었다. 첫째, ’초심층 네트워크의 안정적 훈련’이라는 난제가 해결되었다. Kaiming He 연구팀이 발표한 심층 잔차 학습(Deep Residual Learning, ResNet)은 100개 이상의 레이어를 가진 신경망을 성공적으로 훈련할 수 있는 길을 열어, 모델의 표현력을 극대화했다. 둘째, ‘대규모 학습을 위한 효율적 최적화’ 기법이 등장했다. Diederik P. Kingma와 Jimmy Ba가 제안한 Adam(Adaptive Moment Estimation) 최적화 알고리즘은 복잡한 하이퍼파라미터 튜닝 없이도 빠르고 안정적인 수렴을 가능하게 하여, ResNet과 같은 거대 모델의 훈련을 실용적인 영역으로 이끌었다.</p>
<p>셋째, 이러한 기반 기술의 발전에 힘입어 AI의 적용 범위가 단순한 패턴 인식을 넘어 ’복잡한 추론 및 상호작용’의 영역으로 확장되었다. 2015년 12월에 개최된 신경정보처리시스템학회(NIPS)와 국제컴퓨터비전학회(ICCV)에서는 공간 변환 네트워크(Spatial Transformer Networks), 자율 주행을 위한 직접 인식(DeepDriving), 비디오를 활용한 비지도 시각 표현 학습, 그리고 기계 독해(Machine Reading Comprehension)의 서막을 연 연구들이 발표되며 시각 지능의 새로운 지평을 열었다. 동시에, 지능형 로봇 및 시스템 국제 컨퍼런스(IROS)에서는 이러한 진보된 기계 학습 기법들이 물리적 세계와 상호작용하는 로봇 시스템에 본격적으로 융합되기 시작했음을 보여주었다.</p>
<p>본 보고서는 2015년 4분기 동안 발표된 주요 연구들을 심층적으로 분석하고, 이들이 어떻게 상호작용하며 AI 및 로봇 공학 분야의 기술적 궤적을 재설정했는지 고찰하고자 한다. 각 장에서는 아키텍처, 최적화, 응용 분야의 혁신을 체계적으로 조명하고, 이 시기의 연구들이 남긴 유산과 그것이 현재의 인공지능 기술에 미친 장기적인 영향을 추적할 것이다.</p>
<h2>2.  심층 신경망 아키텍처의 패러다임 전환</h2>
<p>2015년 4분기는 심층 신경망 아키텍처 설계에 있어 근본적인 패러다임 전환이 일어난 시기였다. 이전까지 ’더 깊은 네트워크가 더 나은 성능을 보인다’는 경험적 사실에도 불구하고, 실제로는 네트워크의 깊이가 일정 수준을 넘어서면 훈련이 불가능해지는 이론적, 실질적 장벽이 존재했다. 이 장에서는 심층 잔차 학습(ResNet)이 어떻게 이 ’깊이의 저주’를 해결하고 초심층 네트워크의 시대를 열었는지, 그리고 이와 병행하여 활성화 함수와 가중치 초기화 기법의 진화가 어떻게 네트워크 훈련의 안정성을 뒷받침했는지 심층적으로 분석한다.</p>
<h3>2.1  깊이의 한계를 극복하다: 심층 잔차 학습 (ResNet)</h3>
<h4>2.1.1 배경: 깊이의 저주와 성능 저하 문제</h4>
<p>2010년대 초반 AlexNet의 등장 이후, VGGNet과 GoogLeNet 같은 모델들은 네트워크의 깊이를 늘리는 것이 이미지 인식 성능을 향상시키는 핵심 전략임을 입증했다.1 하지만 이러한 추세는 명백한 한계에 부딪혔다. 네트워크의 깊이가 20~30개 층을 넘어서면서, 오히려 훈련 오류(training error)와 검증 오류(validation error)가 모두 증가하는 ‘성능 저하(degradation)’ 현상이 관찰되었다.3 이 문제는 모델의 표현력이 부족하여 발생하는 과소적합(underfitting)이나, 훈련 데이터에만 과도하게 맞춰지는 과적합(overfitting)과는 다른 종류의 문제였다. 이는 심층 네트워크의 복잡한 비선형 구조로 인해 최적화 과정 자체가 극도로 어려워지면서 발생하는 근본적인 난제였다. 즉, 이론적으로는 더 깊은 네트워크가 더 얕은 네트워크의 해를 포함해야 하지만, 실제로는 경사 하강법 기반의 최적화 알고리즘이 그 해를 찾지 못하는 것이다.</p>
<h4>2.1.2 핵심 아이디어: 잔차 학습 (Residual Learning)</h4>
<p>2015년 12월, Kaiming He 연구팀은 이 성능 저하 문제를 해결하기 위한 혁신적인 아키텍처인 ’심층 잔차 네트워크(Deep Residual Network, ResNet)’를 arXiv에 발표했다.3 ResNet의 핵심 아이디어는 네트워크가 목표로 하는 기본 매핑(underlying mapping)을 직접 학습하는 대신, ’잔차 매핑(residual mapping)’을 학습하도록 구조를 변경하는 것이다.</p>
<p>기존의 심층 신경망 블록이 입력 <span class="math math-inline">x</span>를 받아 목표 출력인 <span class="math math-inline">H(x)</span>를 직접 근사하도록 설계되었다면, ResNet의 잔차 블록(residual block)은 <span class="math math-inline">F(x) := H(x) - x</span>를 근사하도록 설계되었다. 그리고 최종 출력은 이 잔차 함수 <span class="math math-inline">F(x)</span>에 입력 <span class="math math-inline">x</span>를 더하는 형태로 계산된다. 이 입력 <span class="math math-inline">x</span>를 출력에 직접 더해주는 연결을 ‘지름길 연결(shortcut connection)’ 또는 ’스킵 연결(skip connection)’이라고 부른다.5 이를 수식으로 표현하면 다음과 같다.</p>
<p><span class="math math-display">
y = \mathcal{F}(x, \{W_i\}) + x
</span><br />
여기서 <span class="math math-inline">x</span>와 <span class="math math-inline">y</span>는 해당 블록의 입력과 출력 벡터이며, 함수 <span class="math math-inline">\mathcal{F}(x, \{W_i\})</span>는 학습해야 할 잔차 매핑을 나타낸다. 예를 들어, 두 개의 가중치 레이어로 구성된 블록에서 <span class="math math-inline">\mathcal{F}</span>는 <span class="math math-inline">W_2\sigma(W_1x)</span>와 같이 표현될 수 있으며, <span class="math math-inline">\sigma</span>는 ReLU와 같은 비선형 활성화 함수를 의미한다.3</p>
<h4>2.1.3 수학적 원리 및 효과</h4>
<p>이러한 구조 변경은 최적화 과정에 극적인 효과를 가져온다. 만약 어떤 층에서 최적의 매핑이 항등 함수(identity mapping), 즉 <span class="math math-inline">H(x) = x</span>인 상황을 가정해보자. 기존의 네트워크는 여러 개의 비선형 레이어를 통해 항등 함수를 근사해야 하는데, 이는 매우 어려운 작업이다. 반면, 잔차 블록에서는 네트워크가 학습해야 할 잔차 함수 <span class="math math-inline">F(x)</span>를 0으로 만들기만 하면 된다. 이는 단순히 가중치를 0에 가깝게 만드는 것만으로도 달성할 수 있으므로 학습 난이도가 현저히 낮아진다.3</p>
<p>결과적으로, 스킵 연결은 네트워크의 깊이가 증가하더라도 최소한 얕은 네트워크만큼의 성능을 보장하는 안전장치 역할을 한다. 추가된 레이어가 유용한 정보를 학습하지 못하더라도, 잔차 함수를 0으로 학습하여 항등 매핑처럼 동작함으로써 최소한 성능 저하를 막을 수 있다. 더 나아가, 이 구조는 역전파 과정에서 그래디언트가 스킵 연결을 통해 하위 레이어로 직접 전달될 수 있는 경로를 제공한다. 이는 심층 네트워크의 고질적인 문제였던 기울기 소실(vanishing gradient) 또는 기울기 폭주(exploding gradient) 문제를 효과적으로 완화하여, 이전에는 불가능했던 수백, 수천 개 층의 네트워크 훈련을 가능하게 하는 결정적인 역할을 했다.1</p>
<h4>2.1.4 성과와 영향</h4>
<p>ResNet의 효과는 압도적이었다. 연구팀은 152개 레이어로 구성된 ResNet-152 모델을 성공적으로 훈련시켰는데, 이는 당시 주류였던 VGGNet(19개 레이어)보다 8배나 깊으면서도 오히려 계산 복잡도(FLOPs)는 더 낮았다.3 이 모델은 ImageNet 2012 분류 데이터셋에서 3.57%의 top-5 오류율을 기록하며 ILSVRC 2015 대회에서 압도적인 성능으로 우승했다.6 이는 딥러닝 역사상 처음으로 100개 이상의 레이어를 가진 네트워크가 성공적으로 훈련되고 그 효과를 입증한 사례였다.</p>
<p>ResNet의 등장은 단순히 하나의 우수한 모델이 나온 것을 넘어, 심층 신경망 설계의 철학을 바꾸었다. ’얼마나 복잡한 특징을 표현할 수 있는가’에 집중했던 이전의 패러다임에서 ’어떻게 하면 학습 과정을 더 쉽게 만들 것인가’라는 최적화 관점의 패러다임으로의 전환을 이끌었다. 잔차 학습의 개념은 이후 컴퓨터 비전뿐만 아니라 자연어 처리 등 다양한 분야로 확산되었다. DenseNet, ResNeXt와 같은 수많은 후속 아키텍처에 영감을 주었으며, 특히 현대 언어 모델의 근간이 되는 Transformer 아키텍처에서도 스킵 연결은 핵심적인 구성 요소로 자리 잡았다.9 ResNet은 딥러닝을 ‘매우 깊은(very deep)’ 아키텍처의 시대로 이끈 진정한 게임 체인저였다.</p>
<p>이러한 혁신은 단일한 아이디어에서 비롯된 것이 아니라, 심층 네트워크 훈련의 근본적인 문제를 해결하려는 체계적인 노력의 정점이었다. ResNet 아키텍처가 제안되기 직전, 동일한 연구팀은 활성화 함수와 가중치 초기화라는 또 다른 핵심 요소에 대한 깊이 있는 탐구를 통해 초심층 네트워크 훈련의 기반을 다지고 있었다.</p>
<h3>2.2  활성화 함수와 초기화의 진화: PReLU와 Kaiming 초기화</h3>
<p>ResNet이 아키텍처 수준에서 최적화 경로를 단순화했다면, 그보다 앞서 ICCV 2015에서 발표된 “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification” 논문은 네트워크의 미시적 구성 요소인 활성화 함수와 가중치 초기화 방식을 개선하여 심층 네트워크 훈련의 안정성을 확보하려는 중요한 시도였다.10</p>
<h4>2.2.1 배경: Rectifier의 중요성과 한계</h4>
<p>당시 심층 신경망의 표준 활성화 함수는 ReLU(Rectified Linear Unit)였다. ReLU는 <span class="math math-inline">f(x) = \max(0, x)</span>로 정의되며, 단순한 형태와 빠른 계산 속도, 그리고 양수 입력에 대한 선형적 특성 덕분에 기울기 소실 문제를 완화하는 데 효과적이었다. 하지만 ReLU는 음수 입력에 대해서는 항상 0을 출력하고 기울기 또한 0이 되는 ’Dying ReLU’라는 고질적인 문제를 안고 있었다. 훈련 과정에서 특정 뉴런이 한번 음수 값을 출력하기 시작하면, 이후로는 그래디언트가 흐르지 않아 더 이상 학습이 진행되지 않는 현상이다.</p>
<h4>2.2.2 PReLU (Parametric ReLU) 제안</h4>
<p>이 문제를 해결하기 위해 연구팀은 ReLU를 일반화한 PReLU(Parametric Rectified Linear Unit)를 제안했다.10 PReLU는 음수 부분에 작은 고정된 기울기를 사용하는 Leaky ReLU(LReLU)에서 한 걸음 더 나아가, 이 기울기를 학습 가능한 파라미터로 만들었다. PReLU의 정의는 다음과 같다.</p>
<p><span class="math math-display">
f(y_i) = \max(0, y_i) + a_i \min(0, y_i)
</span><br />
여기서 <span class="math math-inline">y_i</span>는 <span class="math math-inline">i</span>번째 채널의 입력이며, <span class="math math-inline">a_i</span>는 음수 부분의 기울기를 제어하는 학습 가능한 계수이다. 이 계수는 채널별로 독립적으로 학습될 수도 있고(channel-wise), 한 레이어의 모든 채널이 공유할 수도 있다(channel-shared). PReLU는 데이터로부터 최적의 활성화 함수 형태를 적응적으로 학습함으로써, 추가적인 계산 비용은 거의 없이 모델의 적합(fitting) 능력을 향상시켰다.12</p>
<h4>2.2.3 강건한 초기화 방법론 (Robust Initialization)</h4>
<p>심층 네트워크 훈련의 또 다른 난제는 가중치 초기화였다. 부적절한 초기화는 순전파 과정에서 출력값이 기하급수적으로 커지거나 작아지게 만들고, 역전파 과정에서는 기울기 폭주 또는 소실을 유발한다.15 Xavier/Glorot 초기화가 이 문제를 일부 해결했지만, 이는 활성화 함수가 선형적이라고 가정했기 때문에 비선형성이 강한 ReLU 계열 활성화 함수에는 최적이 아니었다.</p>
<p>He 연구팀은 ReLU와 PReLU의 비선형성을 수학적으로 분석하여, 순전파와 역전파 과정에서 신호의 분산(variance)을 일정하게 유지할 수 있는 새로운 초기화 방법을 유도했다. 이 방법은 현재 ‘Kaiming 초기화’ 또는 ’He 초기화’로 널리 알려져 있으며, <span class="math math-inline">L</span>번째 레이어의 가중치 <span class="math math-inline">W_L</span>을 평균이 0이고 분산이 <span class="math math-inline">2/n_L</span>인 정규분포에서 샘플링하는 것을 골자로 한다 (여기서 <span class="math math-inline">n_L</span>은 입력 뉴런의 수).15 이 초기화 방법은 극도로 깊은 Rectifier 기반 네트워크를 처음부터(from scratch) 안정적으로 훈련시키는 것을 가능하게 하는 핵심적인 기술적 토대가 되었다.</p>
<h4>2.2.4 성과: 인간 수준 성능 돌파</h4>
<p>연구팀은 PReLU와 Kaiming 초기화라는 두 가지 혁신을 결합하여 매우 깊은 네트워크를 성공적으로 훈련시켰다. 그 결과, ImageNet 2012 데이터셋에서 4.94%의 top-5 테스트 오류율을 달성했다.11 이 수치는 당시 ILSVRC 2014 우승 모델인 GoogLeNet(6.66%)을 크게 뛰어넘는 것이었을 뿐만 아니라, 해당 데이터셋에 대해 보고된 인간의 인식 능력(5.1%)을 처음으로 넘어선 기록이었다.10 이는 딥러닝이 특정 시각 인식 과제에서 인간의 능력을 능가할 수 있음을 실증적으로 보여준 상징적인 사건이었다.</p>
<p>결론적으로, ResNet의 아키텍처 혁신은 진공 상태에서 탄생한 것이 아니었다. 그것은 PReLU를 통한 활성화 함수의 표현력 증대, Kaiming 초기화를 통한 훈련 안정성 확보 등, 심층 네트워크의 근본적인 문제를 해결하려는 일련의 체계적인 연구 흐름의 논리적 귀결이었다. 이 두 연구는 2015년 4분기에 딥러닝이 깊이의 장벽을 넘어서는 데 필요한 이론적, 실용적 무기를 모두 갖추게 되었음을 보여준다.</p>
<table><thead><tr><th>모델</th><th>레이어 수</th><th>Top-1 오류율 (ImageNet 검증)</th><th>비고</th></tr></thead><tbody>
<tr><td><strong>Plain Network</strong></td><td>18</td><td>27.94%</td><td>기준 모델</td></tr>
<tr><td><strong>Plain Network</strong></td><td>34</td><td>28.54%</td><td>깊이가 깊어지자 오류율 증가 (성능 저하)</td></tr>
<tr><td><strong>ResNet</strong></td><td>18</td><td>27.88%</td><td>Plain-18과 유사한 성능</td></tr>
<tr><td><strong>ResNet</strong></td><td>34</td><td><strong>25.03%</strong></td><td>깊이가 깊어지자 오류율 감소 (성능 향상)</td></tr>
</tbody></table>
<p>표 1: Plain Network와 ResNet의 깊이에 따른 오류율 비교. 잔차 학습이 성능 저하 문제를 해결하고 깊이의 이점을 효과적으로 활용함을 보여준다. (데이터는 3의 실험 결과에 기반함)</p>
<h2>3.  대규모 학습을 위한 최적화 기법의 혁신: Adam</h2>
<p>심층 신경망 아키텍처가 수백 개의 층으로 깊어질 수 있는 잠재력을 갖추게 되면서, 이 거대한 모델을 효율적이고 안정적으로 훈련시킬 수 있는 최적화 알고리즘의 필요성이 그 어느 때보다 절실해졌다. 2015년 4분기는 바로 이 문제에 대한 강력한 해답인 Adam(Adaptive Moment Estimation) 최적화 알고리즘이 학계의 표준으로 자리 잡기 시작한 시점이다. 2014년 12월 arXiv에 처음 공개된 이 알고리즘은 2015년 ICLR에 발표된 이후, 그 실용성과 강력한 성능 덕분에 빠르게 확산되었다.20</p>
<h3>3.1 배경: 확률적 경사 하강법(SGD)의 한계</h3>
<p>Adam이 등장하기 전, 심층 신경망 훈련의 표준은 확률적 경사 하강법(Stochastic Gradient Descent, SGD)과 그 변형인 모멘텀(Momentum) 방식이었다.22 SGD는 계산 효율이 높다는 장점이 있지만, 몇 가지 근본적인 한계를 가지고 있었다.</p>
<ol>
<li>
<p><strong>학습률(Learning Rate) 민감성:</strong> 학습률은 모델이 수렴하는 속도와 안정성을 결정하는 가장 중요한 하이퍼파라미터다. 학습률이 너무 크면 최적점을 지나쳐 발산하고, 너무 작으면 학습이 매우 느려지거나 지역 최적점(local minima)에 갇히기 쉽다. 최적의 학습률을 찾고, 훈련 과정에 따라 이를 점진적으로 줄여나가는(learning rate scheduling) 것은 상당한 전문성과 실험을 요구하는 작업이었다.23</p>
</li>
<li>
<p><strong>동일한 학습률 적용:</strong> SGD는 모든 파라미터에 동일한 학습률을 적용한다. 하지만 모델의 각 파라미터는 서로 다른 특성을 가질 수 있다. 예를 들어, 자주 등장하지 않는 특징과 관련된 가중치는 드물게 업데이트되므로 더 큰 학습률이 필요할 수 있다.</p>
</li>
<li>
<p><strong>비등방성 문제:</strong> 손실 함수의 표면이 모든 방향으로 동일한 곡률을 가지지 않는 비등방성(anisotropic) 구조일 때, SGD는 지그재그 형태로 비효율적인 탐색을 하는 경향이 있다.</p>
</li>
</ol>
<p>이러한 문제를 해결하기 위해 AdaGrad, RMSProp과 같은 적응적 학습률(adaptive learning rate) 기법들이 제안되었다. AdaGrad는 업데이트가 빈번한 파라미터의 학습률은 줄이고, 드문 파라미터의 학습률은 늘리는 방식으로 희소(sparse) 그래디언트 문제에 효과적이었으나, 학습이 진행될수록 학습률이 계속 감소하여 결국 0에 수렴하는 단점이 있었다. RMSProp은 그래디언트 제곱의 지수 이동 평균을 사용하여 이 문제를 완화했고, 비정상(non-stationary) 목적 함수에 더 강건한 모습을 보였다.22</p>
<h3>3.2 핵심 아이디어: 적응적 모멘트 추정 (Adaptive Moment Estimation)</h3>
<p>Adam은 이름에서 알 수 있듯이, 그래디언트의 1차 및 2차 모멘트(moment)를 적응적으로 추정하여 각 파라미터에 대한 개별적인 학습률을 계산하는 방식이다.22 이는 앞서 언급된 모멘텀 방식과 RMSProp의 장점을 영리하게 결합한 것이다.21</p>
<h3>3.3 알고리즘 상세 분석</h3>
<p>Adam의 업데이트 과정은 네 단계로 나눌 수 있다.</p>
<ol>
<li>
<p><strong>1차 모멘트 추정 (모멘텀):</strong> 그래디언트의 지수 이동 평균 <span class="math math-inline">m_t</span>를 계산한다. 이는 그래디언트의 평균, 즉 1차 모멘트를 추정하는 역할을 하며, 과거 그래디언트의 방향을 유지하여 진동을 줄이고 수렴 속도를 높이는 모멘텀과 유사한 효과를 낸다.22</p>
<p><span class="math math-display">
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
</span><br />
여기서 <span class="math math-inline">g_t</span>는 현재 타임스텝의 그래디언트이며, <span class="math math-inline">\beta_1</span>은 이동 평균의 감쇠율을 결정하는 하이퍼파라미터로 보통 0.9가 사용된다.</p>
</li>
<li>
<p><strong>2차 모멘트 추정 (적응적 학습률):</strong> 그래디언트 제곱의 지수 이동 평균 <span class="math math-inline">v_t</span>를 계산한다. 이는 그래디언트의 중심 없는 분산(uncentered variance), 즉 2차 모멘트를 추정하는 역할을 한다. 이 값은 각 파라미터별 학습률을 조절하는 데 사용된다.22</p>
<p><span class="math math-display">
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
</span><br />
<span class="math math-inline">\beta_2</span>는 2차 모멘트의 감쇠율로 보통 0.999가 사용된다. <span class="math math-inline">v_t</span>의 제곱근은 그래디언트의 크기에 대한 추정치로, 그래디언트가 크고 변동성이 심한 파라미터는 학습률이 작아지고, 그래디언트가 작고 안정적인 파라미터는 학습률이 커지는 효과를 낸다.</p>
</li>
<li>
<p><strong>초기화 편향 보정 (Bias Correction):</strong> 학습 초기에는 <span class="math math-inline">m_t</span>와 <span class="math math-inline">v_t</span>가 0으로 초기화되어 있기 때문에, 추정치가 0에 가깝게 편향되는 문제가 발생한다. Adam은 이 초기 편향을 보정하기 위해 다음과 같은 계산을 수행한다. 이 단계는 특히 학습 초기의 불안정성을 해소하고 안정적인 수렴을 돕는 데 결정적인 역할을 한다.21</p>
<p><span class="math math-display">
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}
</span></p>
<p><span class="math math-display">
\hat{v}_t = \frac{v_t}{1 - \beta_2^t}
</span></p>
</li>
</ol>
<p>여기서 <span class="math math-inline">t</span>는 타임스텝이다. <span class="math math-inline">t</span>가 작을 때는 분모가 0에 가까워져 보정 효과가 크고, <span class="math math-inline">t</span>가 커지면 분모가 1에 수렴하여 보정 효과가 사라진다.</p>
<ol start="4">
<li>
<p><strong>파라미터 업데이트:</strong> 최종적으로 편향이 보정된 모멘트 추정치를 사용하여 파라미터를 업데이트한다.</p>
<p><span class="math math-display">
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
</span><br />
<span class="math math-inline">\alpha</span>는 전역 학습률(step size)이며, <span class="math math-inline">\epsilon</span>은 분모가 0이 되는 것을 방지하기 위한 작은 상수(예: <span class="math math-inline">10^{-8}</span>)이다.</p>
</li>
</ol>
<h3>3.4 기존 기법과의 비교 및 장점</h3>
<p>Adam은 이전 알고리즘들의 장점을 효과적으로 통합했다. AdaGrad로부터는 희소한 그래디언트를 다루는 능력을, RMSProp으로부터는 비정상 목적 함수에 대한 강건함을 물려받았다.22 동시에 모멘텀을 결합하여 수렴 속도를 높이고, 편향 보정 항을 추가하여 초기 학습 안정성을 확보했다.</p>
<p>이러한 설계 덕분에 Adam은 다음과 같은 실용적인 장점을 갖게 되었다.</p>
<ul>
<li>
<p><strong>빠른 수렴 속도와 높은 성능:</strong> 경험적으로 다양한 모델과 데이터셋에서 다른 최적화 알고리즘들보다 빠르게 수렴하고 우수한 성능을 보인다.21</p>
</li>
<li>
<p><strong>하이퍼파라미터 튜닝 용이성:</strong> SGD에 비해 학습률 <span class="math math-inline">\alpha</span> 설정에 덜 민감하며, 제안된 기본값(<span class="math math-inline">\alpha=0.001, \beta_1=0.9, \beta_2=0.999</span>)만으로도 대부분의 문제에서 준수한 결과를 얻을 수 있다.22 이는 딥러닝 연구와 개발의 진입 장벽을 크게 낮추는 효과를 가져왔다. 연구자들은 복잡한 학습률 스케줄링에 쏟던 노력을 아키텍처 설계나 문제 정의와 같은 더 본질적인 부분에 집중할 수 있게 되었다.</p>
</li>
<li>
<p><strong>효율성:</strong> 구현이 간단하고, 그래디언트의 1차, 2차 모멘트만 저장하면 되므로 메모리 요구량이 적다. 이는 대규모 데이터와 파라미터를 가진 심층 신경망에 매우 적합하다.22</p>
</li>
</ul>
<p>이러한 실용적 안정성과 범용성 덕분에 Adam은 이론적으로 최적해 수렴이 보장되지 않는다는 일부 비판에도 불구하고, 딥러닝 커뮤니티에서 사실상의 표준(de facto standard) 최적화 알고리즘으로 빠르게 자리 잡았다. 이는 딥러닝이 이론 중심의 학문에서 실험과 응용 중심의 공학으로 발전하는 과정을 상징적으로 보여주는 현상이었다. ResNet과 같은 초심층 아키텍처가 ‘무엇을’ 만들 수 있는지에 대한 가능성을 열었다면, Adam은 ‘누구나 쉽게’ 그것을 만들 수 있는 실용적인 도구를 제공함으로써 딥러닝 연구의 ’민주화’를 이끌었다.</p>
<table><thead><tr><th>알고리즘</th><th>학습률</th><th>업데이트 규칙 (간략화)</th><th>주요 특징 (장점)</th><th>한계 (단점)</th></tr></thead><tbody>
<tr><td><strong>SGD</strong></td><td>고정</td><td><span class="math math-inline">\theta \leftarrow \theta - \alpha g_t</span></td><td>간단하고 기본적인 알고리즘</td><td>학습률에 민감, 지역 최적점 탈출 어려움</td></tr>
<tr><td><strong>Momentum</strong></td><td>고정</td><td><span class="math math-inline">m_t = \beta m_{t-1} + g_t</span> <br> <span class="math math-inline">\theta \leftarrow \theta - \alpha m_t</span></td><td>진동을 줄이고 수렴 가속</td><td>여전히 고정된 학습률 사용</td></tr>
<tr><td><strong>AdaGrad</strong></td><td>적응적</td><td><span class="math math-inline">G_t = G_{t-1} + g_t^2</span> <br> <span class="math math-inline">\theta \leftarrow \theta - \frac{\alpha}{\sqrt{G_t}+\epsilon} g_t</span></td><td>희소 그래디언트에 강함, 파라미터별 학습률</td><td>학습이 진행될수록 학습률이 0에 수렴하여 멈춤</td></tr>
<tr><td><strong>RMSProp</strong></td><td>적응적</td><td><span class="math math-inline">v_t = \beta v_{t-1} + (1-\beta)g_t^2</span> <br> <span class="math math-inline">\theta \leftarrow \theta - \frac{\alpha}{\sqrt{v_t}+\epsilon} g_t</span></td><td>AdaGrad의 학습률 감소 문제 해결, 비정상 목적 함수에 강함</td><td>모멘텀 부재, 초기 편향 문제</td></tr>
<tr><td><strong>Adam</strong></td><td>적응적</td><td>(본문 참조)</td><td>모멘텀과 RMSProp 결합, 초기 편향 보정, 빠르고 안정적, 범용성 높음</td><td>특정 조건에서 수렴 문제 제기, 일반화 성능 논란</td></tr>
</tbody></table>
<p>표 2: 주요 확률적 최적화 알고리즘 비교. Adam은 이전 알고리즘들의 장점을 계승하고 단점을 보완하는 진화의 산물임을 보여준다. (정보는 21를 종합하여 구성함)</p>
<h2>4.  NIPS &amp; ICCV 2015: 시각 지능의 새로운 지평</h2>
<p>2015년 12월, 캐나다 몬트리올에서 열린 NIPS(현 NeurIPS)와 칠레 산티아고에서 열린 ICCV는 딥러닝, 특히 컴퓨터 비전 분야의 최신 성과가 집약된 학술의 장이었다. 이 두 학회는 전례 없는 규모로 성장하며 딥러닝에 대한 학계와 산업계의 폭발적인 관심을 증명했다. NIPS 2015에는 4,000명 이상의 연구자가 참석했으며, 이는 딥러닝이 학문적 유행을 넘어 주류 기술로 자리 잡았음을 보여주는 지표였다.28</p>
<p>이 시기에 발표된 연구들은 ResNet과 Adam이 제공한 강력한 기반 위에서, 인공지능이 단순한 ’인식(recognition)’을 넘어 ’이해(understanding)’와 ’상호작용(interaction)’으로 나아갈 수 있는 가능성을 탐색했다. 연구자들은 정적인 이미지 분류를 넘어, 동적인 시각 정보 처리, 자율적 의사결정, 그리고 자연어와의 연계 등 더 높은 수준의 인지 과제에 도전하기 시작했다.</p>
<h3>4.1  동적 공간 불변성 확보: 공간 변환 네트워크 (STN)</h3>
<p>기존의 CNN은 컨볼루션 연산의 고유한 특성과 풀링 레이어를 통해 어느 정도의 공간적 불변성(spatial invariance)을 확보했다. 하지만 이는 고정된 국소 영역 내에서의 작은 변화에만 강건할 뿐, 이미지 전체에 걸친 큰 회전, 크기 변화, 시점 변화, 그리고 비강체 변형(non-rigid deformation)과 같은 복잡한 공간적 변화에 능동적으로 대처하는 데에는 한계가 있었다.30 모델이 이러한 변화에 강건해지기 위해서는 방대한 양의 데이터 증강(data augmentation)과 매우 깊은 네트워크가 필요했다.</p>
<p>NIPS 2015에서 발표된 ’Spatial Transformer Networks(STN)’는 이 문제에 대한 우아하고 강력한 해법을 제시했다.30 STN은 기존 CNN 아키텍처에 쉽게 삽입할 수 있는 미분 가능한(differentiable) 모듈로, 네트워크가 입력 데이터(또는 중간 피처 맵)에 맞춰 동적으로 적절한 공간 변환을 학습하고 적용할 수 있게 한다. 이는 네트워크에 일종의 ‘주의 집중(attention)’ 메커니즘을 부여하여, 분류나 인식에 중요한 영역을 스스로 찾아 확대(zoom-in)하거나 표준적인 자세(canonical pose)로 변환하는 역할을 한다.30</p>
<p>STN 모듈은 세 가지 핵심 요소로 구성된다.</p>
<ol>
<li>
<p><strong>지역화 네트워크 (Localization Network):</strong> 입력 피처 맵을 받아 적용할 공간 변환의 파라미터 <span class="math math-inline">\theta</span>를 회귀(regression) 방식으로 예측한다. 예를 들어, 어파인 변환(affine transformation)의 경우, <span class="math math-inline">\theta</span>는 6개의 파라미터로 구성된 변환 행렬이 된다. 이 네트워크는 완전 연결 계층이나 컨볼루션 계층 등 다양한 형태로 구성될 수 있다.30</p>
</li>
<li>
<p><strong>그리드 생성기 (Grid Generator):</strong> 지역화 네트워크가 예측한 파라미터 <span class="math math-inline">\theta</span>를 사용하여, 출력 피처 맵의 각 좌표 <span class="math math-inline">(x_i^t, y_i^t)</span>가 입력 피처 맵의 어느 소스 좌표 <span class="math math-inline">(x_i^s, y_i^s)</span>에서 샘플링되어야 하는지를 계산한다. 이는 변환된 좌표 그리드를 생성하는 과정이다.</p>
</li>
<li>
<p><strong>샘플러 (Sampler):</strong> 그리드 생성기가 만든 샘플링 그리드와 원본 입력 피처 맵을 이용해, 미분 가능한 보간법(예: bilinear interpolation)을 통해 변환된 출력 피처 맵을 생성한다. 이 미분 가능성이 STN 모듈 전체를 역전파 알고리즘을 통해 종단간(end-to-end)으로 학습할 수 있게 하는 핵심이다.30</p>
</li>
</ol>
<p>STN은 이미지 분류, 객체 탐지 등 다양한 비전 과제에서 모델의 성능과 강건성을 크게 향상시켰다. 특히, 왜곡된 MNIST 숫자 인식 과제에서 STN을 탑재한 모델은 기존 모델 대비 오류율을 크게 낮추었으며, 새 이미지 분류(CUB-200-2011) 데이터셋에서는 새의 머리와 몸통을 각각 다른 STN 모듈이 자동으로 찾아 정렬하는 모습을 보여주며 그 잠재력을 입증했다.31 STN은 이후 컴퓨터 비전 분야에서 어텐션 메커니즘의 발전에 중요한 영감을 주었다.</p>
<h3>4.2  자율 주행을 위한 직접 인식: DeepDriving</h3>
<p>ICCV 2015에서 발표된 ’DeepDriving’은 자율 주행 분야의 지배적인 두 패러다임에 대한 근본적인 문제 제기에서 시작한다.35</p>
<ol>
<li>
<p><strong>매개 인식 (Mediated Perception):</strong> 차선, 신호등, 다른 차량, 보행자 등 주행과 관련된 모든 객체를 개별적으로 인식하고, 이 정보를 종합하여 3D 월드 모델을 구축한 뒤, 이를 기반으로 주행 계획을 수립하는 방식이다. 이 방식은 해석이 용이하고 체계적이지만, 시스템이 매우 복잡해지고, 인식 모듈 하나하나의 오류가 전체 시스템의 안정성을 저해할 수 있다. 또한, 주행 결정과 무관한 정보까지 처리해야 하므로 비효율적이다.36</p>
</li>
<li>
<p><strong>행동 반사 (Behavior Reflex):</strong> 입력 카메라 이미지를 조향각, 가속/감속과 같은 제어 명령으로 직접 매핑하는 종단간(end-to-end) 학습 방식이다. 이 방식은 시스템이 단순하다는 장점이 있지만, 모델의 의사결정 과정을 해석하기 어려운 ‘블랙박스’ 문제가 있다. 또한, 동일한 시각적 입력에 대해서도 운전자의 의도에 따라 다른 행동이 나올 수 있어 학습이 불안정하며, 차선 변경이나 추월과 같은 복잡하고 장기적인 계획 수립에 한계를 보인다.35</p>
</li>
</ol>
<p>‘DeepDriving’ 연구는 이 두 극단의 절충안으로 제3의 패러다임인 **‘직접 인식(Direct Perception)’**을 제안했다.35 직접 인식의 핵심은 입력 이미지를 운전 행동과 직접적으로 관련된 소수의 핵심적인 **‘어포던스 지표(Affordance Indicators)’**로 매핑하는 것이다. ’어포던스’는 어떤 환경이나 사물이 행위자에게 제공하는 행동의 가능성을 의미하는 심리학 용어로, 여기서는 ’현재 도로 상황이 운전자에게 어떤 주행 행동을 가능하게 하는가’에 대한 지표를 뜻한다.</p>
<p>연구팀은 고속도로 주행 상황에 필요한 13개의 어포던스 지표를 정의했다. 여기에는 ‘차량의 주행 방향과 도로의 접선 사이의 각도(heading angle)’, ‘현재 차선 및 인접 차선의 선행 차량과의 거리’, ‘좌/우 차선 표시와의 거리’ 등이 포함된다.37 그리고 이 13개의 지표를 입력 이미지로부터 직접 추정하도록 딥러닝 CNN 모델을 훈련시켰다. 훈련 데이터는 TORCS라는 레이싱 시뮬레이션 게임에서 12시간 동안 인간이 운전한 기록을 수집하여 구축했다.36</p>
<p>이렇게 학습된 어포던스 지표들은 그 자체로 현재 주행 상황에 대한 간결하면서도 완전한 설명이 된다. 이 지표들을 입력으로 받는 간단한 규칙 기반 제어기만으로도 다양한 가상 환경에서 안정적인 자율 주행이 가능함을 보여주었다. 이 접근법은 매개 인식의 복잡성을 피하면서도 행동 반사의 해석 불가능성을 극복하는, 실용성과 강건성을 모두 갖춘 새로운 방향을 제시했다.</p>
<h3>4.3  비지도 학습의 가능성: 비디오를 활용한 시각 표현 학습</h3>
<p>딥러닝의 성공은 ImageNet과 같은 대규모 레이블링된 데이터셋에 크게 의존했다. 하지만 이러한 데이터셋을 구축하는 데에는 막대한 비용과 시간이 소요된다. ICCV 2015에서 발표된 ’Unsupervised Learning of Visual Representations Using Videos’는 이러한 데이터 병목 현상을 해결하기 위한 획기적인 아이디어를 제시했다.40 연구의 핵심 질문은 ’좋은 시각적 표현을 학습하기 위해 과연 강력한 지도 학습이 필수적인가?’였다.</p>
<p>연구팀은 인터넷에 존재하는 방대한 양의 레이블 없는(unlabeled) 비디오 데이터에 주목했다. 비디오는 시간적 연속성이라는 강력한 내재적 구조를 가지고 있다. 즉, 비디오의 연속된 프레임에 등장하는 동일한 객체의 패치(patch)들은 시점이나 조명이 약간 변하더라도 동일한 의미적 내용을 담고 있을 가능성이 매우 높다. 연구팀은 바로 이 **‘시각적 추적(visual tracking)’**이 제공하는 시간적 일관성을 일종의 감독 신호(supervisory signal)로 활용했다.40</p>
<p>방법론은 **샴-삼중항 네트워크(Siamese-triplet network)**와 순위 손실 함수(ranking loss function)를 기반으로 한다.</p>
<ol>
<li>
<p>비디오에서 특정 객체 패치를 추적하여 시작 프레임의 패치(기준, anchor)와 마지막 프레임의 패치(긍정 예시, positive)를 얻는다.</p>
</li>
<li>
<p>전혀 다른 비디오에서 무작위로 패치(부정 예시, negative)를 샘플링한다.</p>
</li>
<li>
<p>네트워크는 이 세 개의 패치를 입력받아, 특징 공간(feature space)에서 기준 패치와 긍정 예시 사이의 거리는 가깝게 만들고, 기준 패치와 부정 예시 사이의 거리는 멀게 만들도록 학습된다.42</p>
</li>
</ol>
<p>이러한 방식으로, 네트워크는 레이블 없이도 객체의 외형 변화에 강건하고, 서로 다른 객체를 구별할 수 있는 시각적 표현을 스스로 학습하게 된다.</p>
<p>이 연구의 성과는 놀라웠다. ImageNet 데이터를 단 한 장도 사용하지 않고, 오직 10만 개의 레이블 없는 비디오로 사전 학습한 모델이 PASCAL VOC 2012 객체 탐지 벤치마크에서 52%의 mAP(mean Average Precision)를 달성했다. 이는 ImageNet으로 지도 학습한 모델의 성능(54.4% mAP)에 근접하는 수치로, 비지도 학습(또는 자기지도 학습, self-supervised learning)이 지도 학습의 대안이 될 수 있다는 강력한 가능성을 처음으로 실증적으로 보여준 사례였다.40 이 연구는 이후 대규모 비지도 데이터를 활용한 표현 학습 연구의 폭발적인 성장을 이끄는 기폭제가 되었다.</p>
<h3>4.4  기계 독해와 행동 예측의 서막</h3>
<p>2015년 4분기는 컴퓨터 비전뿐만 아니라, 자연어 처리와 강화학습 분야에서도 미래의 연구 방향을 제시한 중요한 논문들이 발표된 시기였다.</p>
<p><strong>Teaching Machines to Read and Comprehend (NIPS 2015):</strong> 이 연구는 기계 독해(Machine Reading Comprehension, MRC) 분야의 고질적인 문제였던 대규모 지도 학습 데이터셋의 부재를 창의적인 방법으로 해결했다.32 연구팀은 CNN과 Daily Mail 뉴스 기사와 함께 제공되는 요약(summary) 불릿 포인트를 활용했다. 요약 문장에서 고유명사(entity) 하나를 빈칸으로 만들고, 이 빈칸에 들어갈 답을 원본 기사 본문에서 찾도록 하는 ’클로즈(Cloze) 스타일’의 질문-답변 쌍을 백만 개 이상 자동으로 생성했다.44 이는 MRC 문제를 대규모 지도 학습 문제로 전환시키는 결정적인 계기가 되었으며, 이후 SQuAD와 같은 고품질 데이터셋의 등장을 촉발하고 BERT와 같은 거대 언어 모델 기반의 질의응답 연구가 폭발적으로 성장하는 토대를 마련했다.45</p>
<p><strong>Action-Conditional Video Prediction using Deep Networks in Atari Games (NIPS 2015):</strong> 이 연구는 단순한 비디오 프레임 예측을 넘어, 에이전트의 ’행동(action)’을 조건으로 미래의 프레임을 예측하는 심층 신경망 아키텍처를 제안했다.32 아타리 게임 환경에서, 현재 화면과 플레이어의 다음 행동(예: 조이스틱을 왼쪽으로 움직임)이 주어졌을 때, 다음 화면이 어떻게 변할지를 생성하는 모델을 학습시킨 것이다. 이 모델은 게임 환경의 동역학(dynamics)을 내재적으로 학습하는 ’월드 모델(world model)’의 초기 형태로 볼 수 있다. 이는 환경과의 상호작용 비용이 비싼 실제 세계에서, 학습된 가상 환경 모델 내에서 정책을 학습하는 모델 기반 강화학습(Model-Based Reinforcement Learning)의 발전에 중요한 기여를 했다.48</p>
<p>이처럼 2015년 4분기의 주요 학회들은 AI가 정적인 패턴 매칭을 넘어, 동적인 환경의 의미를 이해하고, 언어적 맥락을 파악하며, 자신의 행동이 가져올 결과를 예측하는 등, 보다 높은 수준의 인지 능력을 향한 중요한 첫걸음을 내디뎠음을 명확히 보여주었다.</p>
<table><thead><tr><th>연구</th><th>핵심 문제</th><th>제안된 솔루션/개념</th><th>기술적 방법론</th><th>주요 성과 및 의의</th></tr></thead><tbody>
<tr><td><strong>Spatial Transformer Networks</strong></td><td>CNN의 제한된 공간 불변성</td><td>동적 공간 변환 모듈</td><td>지역화 네트워크, 그리드 생성기, 미분 가능한 샘플러</td><td>네트워크가 스스로 주의를 집중하고 입력을 정규화하는 능력 부여</td></tr>
<tr><td><strong>DeepDriving</strong></td><td>자율 주행의 복잡성 vs. 해석 불가능성</td><td>직접 인식 (Direct Perception)</td><td>CNN을 이용한 어포던스 지표(Affordance Indicators) 추정</td><td>매개 인식과 행동 반사의 장점을 결합한 제3의 패러다임 제시</td></tr>
<tr><td><strong>Unsupervised Learning using Videos</strong></td><td>대규모 레이블링 데이터 부족</td><td>시각적 추적을 이용한 자기지도학습</td><td>샴-삼중항 네트워크, 순위 손실 함수</td><td>레이블 없이도 지도 학습에 근접하는 시각 표현 학습 가능성 입증</td></tr>
<tr><td><strong>Teaching Machines to Read</strong></td><td>기계 독해 훈련 데이터 부족</td><td>뉴스 요약문을 활용한 데이터셋 자동 생성</td><td>클로즈 스타일 질의응답 쌍 생성, 어텐션 기반 RNN</td><td>대규모 MRC 연구의 시작을 알리고, 데이터 병목 현상 해결</td></tr>
<tr><td><strong>Action-Conditional Video Prediction</strong></td><td>행동에 따른 환경 변화 예측</td><td>행동 조건부 비디오 예측</td><td>인코더-변환-디코더 구조의 심층 신경망</td><td>모델 기반 강화학습의 핵심인 ’월드 모델’의 초기 개념 제시</td></tr>
</tbody></table>
<p><em>표 3: NIPS/ICCV 2015 주요 발표 논문 요약. 이 연구들은 AI가 인식을 넘어 이해와 상호작용으로 나아가는 전환점을 보여준다.</em></p>
<h2>5.  IROS 2015: 지능형 로봇과 기계 학습의 융합</h2>
<p>2015년 9월 말부터 10월 초까지 독일 함부르크에서 개최된 IEEE/RSJ 지능형 로봇 및 시스템 국제 컨퍼런스(IROS 2015)는 “로봇 시대로의 관문(Gateway to the Era of Robots)“이라는 주제 아래, 로봇 공학 분야의 새로운 방향성을 제시했다.50 이 시기는 딥러닝이 컴퓨터 비전과 자연어 처리 분야에서 혁명적인 성과를 내고 있던 때로, IROS 2015는 이러한 인공지능의 발전이 어떻게 물리적 세계와 상호작용하는 로봇 시스템에 접목될 수 있는지에 대한 초기 탐색과 가능성을 보여주는 중요한 장이었다. 특히, 전통적인 로봇 공학의 제어, 계획, 인식 문제에 기계 학습, 그중에서도 딥러닝과 강화학습을 적용하려는 시도가 본격적으로 논의되기 시작했다.</p>
<h3>5.1 주요 워크숍: ‘로봇 모션 계획 및 제어를 위한 기계 학습’</h3>
<p>IROS 2015에서 개최된 여러 워크숍 중 ‘로봇 모션 계획 및 제어를 위한 기계 학습(Machine Learning in Planning and Control of Robot Motion)’ 워크숍은 당시의 기술적 흐름을 상징적으로 보여준다.53 이는 전통적으로 기하학적, 동역학적 모델링에 크게 의존해왔던 로봇 모션 분야가 데이터 기반의 학습 방법론을 적극적으로 수용하기 시작했음을 시사한다. 이 워크숍에서는 로봇이 불확실하고 동적인 환경에서 더 지능적이고 적응적으로 움직이기 위해, 환경과의 상호작용을 통해 스스로 정책을 개선하는 강화학습이나, 복잡한 센서 데이터를 직접 처리하는 딥러닝과 같은 기법들이 어떻게 활용될 수 있는지에 대한 심도 있는 논의가 이루어졌다.</p>
<h3>5.2 AI 기술이 적용된 로봇 연구 사례</h3>
<p>IROS 2015에서 발표된 논문들은 딥러닝과 강화학습이 ’인식(Perception)’을 넘어 로봇의 ‘행동(Action)’ 생성에 직접적으로 기여하기 시작했음을 보여준다. 당시의 접근법은 시스템 전체를 종단간(end-to-end)으로 학습하는 현재의 방식보다는, 기존 로봇 공학 프레임워크의 특정 모듈을 기계 학습으로 대체하거나 보강하는 ‘하이브리드’ 방식이 주를 이루었다. 이는 당시 기계 학습 기술의 데이터 효율성, 안전성, 실시간성 등의 한계와 물리적 시스템을 다루는 로봇 공학의 엄격한 요구사항을 고려한 현실적인 접근이었다.</p>
<h4>5.2.1 강화학습 기반 로봇 제어</h4>
<p>“인간-로봇 협업 조작을 위한 가변 어드미턴스 제어의 강화학습(Reinforcement Learning of Variable Admittance Control for Human-Robot Co-manipulation)” 연구는 강화학습을 로봇 제어에 적용한 대표적인 사례다.54 어드미턴스 제어는 로봇이 외부에서 가해지는 힘에 대해 유연하게 반응하도록 만드는 제어 방식으로, 인간과의 물리적 상호작용에 필수적이다. 이 제어 방식의 성능은 댐핑(damping)과 같은 파라미터에 크게 좌우되는데, 최적의 값을 미리 설정하기는 매우 어렵다.</p>
<p>이 연구에서는 로봇이 인간과 함께 물체를 옮기는 작업을 수행하면서, 상호작용의 ’부드러움(jerk 최소화)’을 보상(reward)으로 삼아 최적의 댐핑 값을 스스로 학습하도록 했다. 이를 위해 연속적인 상태 및 행동 공간을 다룰 수 있는 강화학습 알고리즘인 Fuzzy Q-Learning을 사용했다. 이 연구는 로봇이 작업의 목표나 특성에 대한 사전 지식 없이, 오직 환경과의 상호작용을 통해 인간 파트너의 의도에 적응하고 협업 성능을 최적화하는 제어 정책을 학습할 수 있음을 보여주었다. 이는 강화학습이 정교한 물리적 상호작용 제어 문제에 적용될 수 있는 가능성을 제시한 중요한 성과였다.54</p>
<h4>5.2.2 딥러닝 기반 동역학 모델링</h4>
<p>“로봇 매니퓰레이터 역동역학의 실시간 딥러닝(Real-time deep learning of robotic manipulator inverse dynamics)” 연구는 딥러닝을 로봇의 물리 모델 학습에 적용한 사례다.55 역동역학(inverse dynamics)은 로봇이 원하는 궤적(위치, 속도, 가속도)으로 움직이기 위해 각 관절에 필요한 토크(torque)를 계산하는 과정으로, 정밀한 로봇 제어의 핵심이다. 전통적으로 이는 복잡한 수식으로 구성된 물리 기반 모델을 통해 계산되었지만, 실제 로봇의 마찰이나 유연성과 같은 비선형적 요소를 정확하게 모델링하는 데에는 한계가 있었다.</p>
<p>이 연구는 로봇의 관절 상태(각도, 속도, 가속도)를 입력으로, 필요한 토크를 출력으로 하는 딥러닝 모델을 학습시켜 역동역학을 근사하는 데이터 기반 접근법을 탐구했다. 이는 복잡한 물리 모델링의 어려움을 회피하고, 실제 로봇의 동작 데이터로부터 직접 동역학적 특성을 학습할 수 있는 새로운 길을 열었다. 딥러닝이 고차원 비선형 함수를 근사하는 데 탁월한 능력을 가지고 있음을 로봇 동역학이라는 고전적인 문제에 적용한 시도였다.55</p>
<h4>5.2.3 정보 수집 및 상호작용 로봇</h4>
<p>로봇의 역할이 단순한 물리적 작업 수행을 넘어, 인간과의 정보적 상호작용으로 확장될 수 있음을 보여주는 연구도 발표되었다. “인간이 거주하는 환경을 위한 정보 수집 로봇 설계(Designing Information Gathering Robots for Human-Populated Environments)” 연구는 ’InfoBot’이라는 개념을 제시했다.56 이 로봇은 사무실과 같은 인간의 생활 공간을 자율적으로 돌아다니며, “가장 가까운 프린터는 어디에 있나요?“와 같은 거주자들의 질문에 답하기 위해 필요한 정보를 능동적으로 수집하고 관리하는 역할을 수행한다. 연구팀은 실제 환경에 로봇을 배치하고 사용자들이 어떤 종류의 정보를 필요로 하는지 분석함으로써, 미래의 서비스 로봇이 갖추어야 할 정보적 상호작용 능력에 대한 요구사항을 도출했다. 이는 로봇이 물리적 세계의 ’행위자’일 뿐만 아니라, 정보 생태계의 ’참여자’가 될 수 있음을 보여준 선구적인 연구였다.56</p>
<p>이처럼 IROS 2015는 딥러닝과 강화학습이라는 새로운 도구가 로봇 공학의 오랜 난제들을 해결할 수 있는 잠재력을 가지고 있음을 명확히 보여주었다. 이 시기의 연구들은 비록 초기 단계의 하이브리드 접근법에 머물렀지만, 인식, 계획, 제어를 데이터로부터 학습하려는 시도는 이후 로봇 공학 연구의 주류 패러다임으로 자리 잡게 되는 중요한 출발점이 되었다.</p>
<h2>6. 결론: 2015년 4분기가 남긴 유산과 미래 전망</h2>
<p>2015년 4분기는 인공지능 역사상 가장 밀도 높고 영향력 있는 시기 중 하나로 평가된다. 이 기간 동안 발표된 연구들은 개별적인 성과를 넘어, 서로의 발전을 촉진하는 강력한 ’혁신의 연쇄 반응’을 일으켰다. 심층 잔차 학습(ResNet)은 ’초심층 네트워크 훈련’이라는 고속도로를 건설했고, Adam 최적화 알고리즘은 그 위를 달릴 수 있는 ’고성능 엔진’을 제공했다. 이 새로운 인프라 위에서, NIPS, ICCV, IROS와 같은 주요 학회들은 자율 주행, 기계 독해, 비지도 학습, 지능형 로봇 제어 등 인공지능이 나아갈 ’새로운 목적지’를 제시했다.</p>
<p>이 시기에 뿌려진 씨앗들은 이후 딥러닝의 전성기를 이끌며 풍성한 결실을 맺었다. ResNet의 스킵 연결 개념은 아키텍처 설계의 근본적인 원리로 자리 잡았다. 이는 단순히 더 깊은 네트워크를 가능하게 한 것을 넘어, 정보와 그래디언트의 원활한 흐름을 보장하는 핵심 메커니즘으로 인식되었다. 이 아이디어는 DenseNet, ResNeXt, Wide Residual Networks 등 다양한 후속 CNN 아키텍처로 진화했으며 9, 궁극적으로는 자연어 처리와 컴퓨터 비전 분야를 모두 석권한 Transformer 아키텍처의 기본 구성 요소로 계승되었다.9</p>
<p>마찬가지로, Adam은 발표 이후 수많은 변종(AdamW, RAdam 등)이 제안되었음에도 불구하고, 여전히 딥러닝 모델 훈련의 가장 보편적인 기본 옵티마이저로 사용되고 있다.26 그 실용성과 안정성은 딥러닝 연구의 대중화를 이끌고, 복잡한 모델의 실험과 개발 속도를 비약적으로 향상시키는 표준 도구로 자리매김했다.</p>
<p>2015년 4분기에 제시된 새로운 연구 패러다임들은 이후 각 분야의 기술적 궤적을 결정지었다. ’Teaching Machines to Read and Comprehend’가 제시한 대규모 데이터셋 기반의 기계 독해 방법론은 SQuAD 데이터셋의 등장을 촉발했고, 이는 BERT와 같은 사전 훈련된 거대 언어 모델(LLM)의 탄생으로 이어지는 직접적인 계보를 형성했다.45 ’Unsupervised Learning of Visual Representations Using Videos’에서 탐구된 자기지도학습(self-supervised learning)의 아이디어는, 레이블 부족이라는 딥러닝의 근본적인 한계를 극복하는 핵심 전략으로 발전하여 현재 생성 모델과 파운데이션 모델의 기반이 되고 있다. 또한, ’Action-Conditional Video Prediction’에서 제시된 행동 조건부 예측 모델은 환경의 동역학을 학습하는 ‘월드 모델’ 개념의 선구자적 연구로서, 이후 Dreamer와 같은 정교한 모델 기반 강화학습 에이전트의 등장으로 이어졌다.61</p>
<p>돌이켜보면, 2015년 4분기는 딥러닝이 ’가능성’의 단계를 지나 ’실용성’과 ’확장성’의 단계로 진입했음을 알리는 신호탄이었다. 이 시기에 제기되었던 근본적인 질문들, 즉 ‘어떻게 하면 기계가 진정으로 이해하게 할 것인가?’, ‘어떻게 하면 데이터에 대한 의존성을 줄일 것인가?’, ’어떻게 하면 학습된 지능을 물리적 세계의 행동으로 변환할 것인가?’는 여전히 오늘날 인공지능 연구의 가장 중요한 화두로 남아있다. 현재 우리가 목도하고 있는 생성 AI, 로보틱스 파운데이션 모델, 체화된 지능(Embodied AI)에 대한 연구들은 모두 2015년 4분기에 시작된 기술적, 사상적 흐름의 연장선 위에 있다. 따라서 이 시기에 대한 깊이 있는 이해는 현재 인공지능 기술의 본질을 파악하고 미래의 발전 방향을 전망하는 데 필수적인 통찰을 제공한다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Deep Residual Learning for Image Recognition - IRE Journals, https://www.irejournals.com/formatedpaper/1703688.pdf</li>
<li>Deep Learning in Robotics: A Review of Recent Research Harry A. Pierson (corresponding author) - arXiv, https://arxiv.org/pdf/1707.07217</li>
<li>Deep Residual Learning for Image Recognition, https://arxiv.org/pdf/1512.03385</li>
<li>Deep Residual Learning for Image Recognition - The Computer Vision Foundation, https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf</li>
<li>Residual Networks (ResNet) - Deep Learning - GeeksforGeeks, https://www.geeksforgeeks.org/deep-learning/residual-networks-resnet-deep-learning/</li>
<li>Deep Residual Learning for Image Recognition - ResearchGate, https://www.researchgate.net/publication/286512696_Deep_Residual_Learning_for_Image_Recognition</li>
<li>Deep Residual Learning for Image Recognition - William Chen, <a href="https://www.williamchenj.com/files/Deep%20Residual%20Learning%20for%20Image%20Recognition.pdf">https://www.williamchenj.com/files/Deep%20Residual%20Learning%20for%20Image%20Recognition.pdf</a></li>
<li>KaimingHe/deep-residual-networks: Deep Residual Learning for Image Recognition, https://github.com/KaimingHe/deep-residual-networks</li>
<li>Residual neural network - Wikipedia, https://en.wikipedia.org/wiki/Residual_neural_network</li>
<li>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification - ICCV 2015 Open Access Repository - The Computer Vision Foundation, https://openaccess.thecvf.com/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html</li>
<li>[1502.01852] Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification - arXiv, https://arxiv.org/abs/1502.01852</li>
<li>Delving Deep into Rectifiers: Surpassing Human-Level Performance …, https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf</li>
<li>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification - SciSpace, https://scispace.com/papers/delving-deep-into-rectifiers-surpassing-human-level-ct0j1qduko</li>
<li>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification - ResearchGate, https://www.researchgate.net/publication/272027131_Delving_Deep_into_Rectifiers_Surpassing_Human-Level_Performance_on_ImageNet_Classification</li>
<li>Day 8: Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification (Kaiming initialization) | by Francisco Ingham - Medium, https://medium.com/a-paper-a-day-will-have-you-screaming-hurray/day-8-delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification-f449a886e604</li>
<li>Learning Based Vision II, https://w4731.cs.columbia.edu/slides/lec11.pdf</li>
<li>MIWAE: Deep Generative Modelling and Imputation of Incomplete Data Sets supplementary material - Proceedings of Machine Learning Research, http://proceedings.mlr.press/v97/mattei19a/mattei19a-supp.pdf</li>
<li>[1502.01852] Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification - ar5iv, https://ar5iv.labs.arxiv.org/html/1502.01852</li>
<li>The Renaissance of Neural Networks! - Stony Brook Computer Science, https://www3.cs.stonybrook.edu/~cse634/G1present.pdf</li>
<li>[1412.6980v8] Adam: A Method for Stochastic Optimization - arXiv, https://arxiv.org/abs/1412.6980v8?hl:ja</li>
<li>ADAM: The Smart Optimizer That Changed Deep Learning. | by Vivek Tiwari | Medium, https://medium.com/@vivek_tiwari_vt/adam-the-smart-optimizer-that-changed-deep-learning-077070575899</li>
<li>adam:amethod for stochastic optimization - arXiv, https://arxiv.org/pdf/1412.6980</li>
<li>What is Adam Optimizer and How to Tune its Parameters in PyTorch - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2023/12/adam-optimizer/</li>
<li>On the Relative Impact of Optimizers on Convolutional Neural Networks with Varying Depth and Width for Image Classification - MDPI, https://www.mdpi.com/2076-3417/12/23/11976</li>
<li>Gentle Introduction to the Adam Optimization Algorithm for Deep Learning - MachineLearningMastery.com, https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/</li>
<li>Mastering the Adam Optimizer: Unlocking Superior Deep Learning Performance, https://www.lunartech.ai/blog/mastering-the-adam-optimizer-unlocking-superior-deep-learning-performance</li>
<li>ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION, https://ece.uwaterloo.ca/~ece602/Projects/2018/Project7/main.html</li>
<li>Neural Information Processing Systems (NIPS) 2015 Review | The Center for Brains, Minds &amp; Machines, https://cbmm.mit.edu/publications/neural-information-processing-systems-nips-2015-review</li>
<li>A Brief Review of NIPS 2015 | IEEE Signal Processing Society, https://signalprocessingsociety.org/community-involvement/speech-and-language-processing/newsletter/brief-review-nips-2015</li>
<li>Spatial Transformer Networks - Department of Computer Science …, http://www.cs.toronto.edu/~bonner/courses/2020s/csc2547/papers/equivariance/spatial-transformer-networks,-jaderberg,-nips2015.pdf</li>
<li>Paper Summary: Spatial Transformer Networks | by Mike Plotz Sage - Medium, https://medium.com/@hyponymous/paper-summary-spatial-transformer-networks-39d813dff7dd</li>
<li>NIPS 2015 Accepted Papers - Stanford Computer Science, https://cs.stanford.edu/people/karpathy/nips2015/</li>
<li>[1506.02025] Spatial Transformer Networks - arXiv, https://arxiv.org/abs/1506.02025</li>
<li>Mastering Spatial Transformer Networks: An In-Depth Guide - Viso Suite, https://viso.ai/deep-learning/introduction-to-spatial-transformer-networks/</li>
<li>DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving - ICCV 2015 Open Access Repository - The Computer Vision Foundation, https://openaccess.thecvf.com/content_iccv_2015/html/Chen_DeepDriving_Learning_Affordance_ICCV_2015_paper.html</li>
<li>Learning Affordance for Direct Perception in … - DeepDriving, https://deepdriving.cs.princeton.edu/paper.pdf</li>
<li>Deep Driving: Learning Affordance for Direct Perception in Autonomous Driving, <a href="https://deep-learning-study-note.readthedocs.io/en/latest/Part%202%20(Modern%20Practical%20Deep%20Networks)/12%20Applications/Computer%20Vision%20External/Learning%20Affordance%20for%20Direct%20Perception%20in%20Auton.html">https://deep-learning-study-note.readthedocs.io/en/latest/Part%202%20(Modern%20Practical%20Deep%20Networks)/12%20Applications/Computer%20Vision%20External/Learning%20Affordance%20for%20Direct%20Perception%20in%20Auton.html</a></li>
<li>DeepDriving: Learning affordance for direct perception in autonomous driving, https://collaborate.princeton.edu/en/publications/deepdriving-learning-affordance-for-direct-perception-in-autonomo</li>
<li>Deep Learning and Control Algorithms of Direct Perception for Autonomous Driving - arXiv, https://arxiv.org/pdf/1910.12031</li>
<li>Unsupervised Learning of Visual Representations Using Videos - ICCV 2015 Open Access Repository, https://openaccess.thecvf.com/content_iccv_2015/html/Wang_Unsupervised_Learning_of_ICCV_2015_paper.html</li>
<li>[1505.00687] Unsupervised Learning of Visual Representations using Videos - arXiv, https://arxiv.org/abs/1505.00687</li>
<li>Unsupervised Learning of Visual Representations Using Videos, https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Wang_Unsupervised_Learning_of_ICCV_2015_paper.pdf</li>
<li>Unsupervised Learning of Visual Representations using Videos - ResearchGate, https://www.researchgate.net/publication/275896969_Unsupervised_Learning_of_Visual_Representations_using_Videos</li>
<li>Teaching Machines to Read and Comprehend, http://papers.neurips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf</li>
<li>A Survey on Machine Reading Comprehension Systems | Natural Language Engineering, https://www.cambridge.org/core/journals/natural-language-engineering/article/survey-on-machine-reading-comprehension-systems/23FBCE30CB4325538E7DD08A7924315F</li>
<li>BERT++: Reading Comprehension on SQuAD - Stanford University, https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/default/15790311.pdf</li>
<li>Action-Conditional Video Prediction using Deep Networks in Atari Games - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/1507.08750</li>
<li>Action-Conditional Video Prediction using Deep Networks in … - NIPS, https://proceedings.neurips.cc/paper/2015/file/6ba3af5d7b2790e73f0de32e5c8c1798-Paper.pdf</li>
<li>Action-Conditional Video Prediction using Deep Networks in Atari Games - Electrical Engineering and Computer Science - University of Michigan, https://web.eecs.umich.edu/~baveja/Papers/1507.08750v1.pdf</li>
<li>2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2015), https://www.active-fp7.eu/index.php/congress/160-iros2015.html</li>
<li>Our research presented at IROS 2015 - Robotics, Embodied AI, Navigation in vivo, http://www.labren.org/mm/news/our-research-presented-at-iros-2015/</li>
<li>IROS 2015, https://iros2015.informatik.uni-hamburg.de/</li>
<li>Workshops and Tutorials Proceedings - IROS 2015, https://iros2015.informatik.uni-hamburg.de/images/downloads/IROS2015_workshop_proceedings.pdf</li>
<li>Reinforcement Learning of Variable Admittance Control for Human-Robot Co-manipulation, https://www.researchgate.net/publication/282604827_Reinforcement_Learning_of_Variable_Admittance_Control_for_Human-Robot_Co-manipulation</li>
<li>Real-time deep learning of robotic manipulator inverse dynamics - Lund University, https://portal.research.lu.se/en/publications/real-time-deep-learning-of-robotic-manipulator-inverse-dynamics</li>
<li>Designing Information Gathering Robots for Human-Populated Environments - IROS 2015, https://www.pronobis.pro/publications/chung2015iros/</li>
<li>Deep Learning Architecture Review: DenseNet, ResNeXt, MnasNet &amp; ShuffleNet v2, https://www.digitalocean.com/community/tutorials/popular-deep-learning-architectures-densenet-mnasnet-shufflenet</li>
<li>Evolution of Convolutional Neural Network (CNN) architectures | by Vinod Tiwari | Medium, https://medium.com/@cpt1995daas/evolution-of-convolutional-neural-network-cnn-architectures-44f2109268a1</li>
<li>Residual Networks (ResNets) and Variants - Artificial Intelligence, https://schneppat.com/residual-networks-resnets-and-variants.html</li>
<li>Theoretical Analysis of Adam Optimizer in the Presence of Gradient Skewness, https://www.researchgate.net/publication/384654097_Theoretical_Analysis_of_Adam_Optimizer_in_the_Presence_of_Gradient_Skewness</li>
<li>Dreamer 4: Scalable World Model Agents - Emergent Mind, https://www.emergentmind.com/papers/2509.24527</li>
<li>Introducing Dreamer: Scalable Reinforcement Learning Using World Models, https://research.google/blog/introducing-dreamer-scalable-reinforcement-learning-using-world-models/</li>
<li>Exploring Mental Models, World Models, and Reinforcement Learning: A Deep Dive into Predictive Intelligence. - Bhakta Vaschal Samal, https://bhakta-works.medium.com/exploring-mental-models-world-models-and-reinforcement-learning-a-deep-dive-into-predictive-efb8cf6db98f</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>