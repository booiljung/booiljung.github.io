<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2016년 3분기 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2016년 3분기 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2016년 AI 및 로봇 연구 동향</a> / <span>2016년 3분기 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2016년 3분기 AI 및 로봇 연구 동향</h1>
<h2>1. 서론: 2016년 3분기, 인공지능 및 로봇 연구의 변곡점</h2>
<p>2016년은 인공지능(AI) 분야가 학문적 탐구를 넘어 산업 전반에 걸쳐 실질적인 영향력을 발휘하기 시작한 결정적인 해로 기록된다. 특히 심층 학습(Deep Learning)의 기초 개념들이 성숙기에 접어들면서, 컴퓨터 비전, 로봇 공학, 의료 영상 분석 등 다양한 하위 분야로 그 영향력이 급속도로 확산되던 시기였다. 이러한 기술적 격변의 중심에서 2016년 3분기는 당해 연도의 남은 기간과 그 이후의 연구 의제를 확립하는 중요한 분기점 역할을 했다. 이 시기에 유럽 컴퓨터 비전 학회(ECCV), 지능형 로봇 및 시스템 국제 학회(IROS), 의료 영상 컴퓨팅 및 컴퓨터 지원 중재 국제 학회(MICCAI)와 같은 세계 최고 수준의 학회들에서 발표될 논문들의 채택이 결정되었기 때문이다. 비록 이들 학회가 10월, 즉 4분기에 개최되었지만, 연구의 실질적인 성과가 확정되고 동료 심사를 통해 그 가치를 공인받은 시점은 바로 3분기였다.1 따라서 2016년 3분기의 “주요 연구 발표“를 분석하는 것은, 곧 4분기 학회에서 공개된 가장 혁신적인 연구들의 지적 토대를 탐색하는 것과 같다.</p>
<p>당시 발표된 연구들은 세 가지 거대한 흐름으로 요약할 수 있다. 첫째, 새로운 센서 기술의 등장과 이를 활용하기 위한 맞춤형 알고리즘의 공진화(co-evolution)이다. 이는 하드웨어의 발전이 소프트웨어 혁신을 촉발하고, 다시 그 소프트웨어가 하드웨어의 잠재력을 극대화하는 선순환 구조를 보여준다. 둘째, 기존 문제 해결 방식의 근본적인 한계를 돌파하기 위한 급진적인 알고리즘 효율성의 추구이다. 이는 단순히 연산 속도를 개선하는 차원을 넘어, 문제 자체를 재정의하고 새로운 공간에서 해법을 모색함으로써 계산 복잡도의 패러다임을 전환하려는 시도였다. 셋째, 특정 도메인이 직면한 핵심적인 데이터 문제를 해결하기 위해 심층 학습 방법론을 창의적으로 변용하고 적용하는 흐름이다. 이는 심층 학습이 범용 기술을 넘어, 각 분야의 고유한 제약 조건(예: 데이터 부족)을 극복하는 정교한 도구로 진화하고 있음을 시사한다.</p>
<p>본 보고서는 이 세 가지 흐름을 대표하는 2016년 3분기의 가장 중요한 연구 성과들을 심층적으로 분석하고자 한다. 이를 위해 ECCV, IROS, MICCAI 2016에서 발표된 주요 논문들을 중심으로, 각 연구가 해결하고자 했던 문제, 제안된 방법론의 핵심 원리, 그리고 해당 연구가 학계와 산업계에 미친 장기적인 영향을 면밀히 추적한다. 이를 통해 2016년 3분기가 현대 AI 및 로봇 공학의 기술적 지형도를 형성하는 데 어떠한 결정적 역할을 했는지 명확히 밝히는 것을 목표로 한다.</p>
<table><thead><tr><th>학회명</th><th>개최 시기</th><th>주요 발표 논문</th><th>주요 성과</th></tr></thead><tbody>
<tr><td><strong>ECCV 2016</strong> (14th European Conference on Computer Vision)</td><td>2016년 10월</td><td><em>Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera</em></td><td><strong>최우수 논문상 (Best Paper Award)</strong> 수상. 이벤트 카메라만을 이용한 실시간 SLAM 최초 구현.4</td></tr>
<tr><td></td><td></td><td><em>The Fast Bilateral Solver</em></td><td><strong>우수 논문상 (Honorable Mention)</strong> 수상. 기존 기법 대비 10-1000배 빠른 엣지 보존 스무딩 알고리즘 제안.6</td></tr>
<tr><td><strong>IROS 2016</strong> (IEEE/RSJ International Conference on Intelligent Robots and Systems)</td><td>2016년 10월</td><td><em>Active Tactile Object Exploration with Gaussian Processes</em></td><td><strong>주요 논문 (Highlight Paper)</strong> 선정. 불확실성 기반의 능동적 촉각 탐사 전략을 통해 탐색 효율 극대화.2</td></tr>
<tr><td></td><td></td><td><em>Cockroach-Inspired Winged Self-Righting Robot</em></td><td><strong>주요 논문 (Highlight Paper)</strong> 선정. 생체모방 기술을 통한 로봇의 물리적 강인성 확보.9</td></tr>
<tr><td><strong>MICCAI 2016</strong> (19th International Conference on Medical Image Computing and Computer Assisted Intervention)</td><td>2016년 10월</td><td><em>3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation</em></td><td><strong>MICCAI 젊은 과학자 영향력상 (Young Scientist Impact Award)</strong> 수상자 배출. 희소 주석만으로 3D 의료 영상의 조밀 분할을 학습하는 패러다임 제시.10</td></tr>
</tbody></table>
<h2>2.  혁신적 센싱과 고속 처리를 통한 컴퓨터 비전의 도약</h2>
<p>2016년 컴퓨터 비전 분야는 하드웨어와 소프트웨어의 양면에서 혁신적인 돌파구를 맞이했다. 한편에서는 인간의 시각 시스템을 모방한 새로운 센서가 등장하여 기존의 인식 패러다임을 근본적으로 바꾸었고, 다른 한편에서는 순수한 알고리즘의 혁신을 통해 기존의 복잡한 문제들을 전례 없는 속도로 해결하는 방법론이 제시되었다. ECCV 2016에서 발표된 두 편의 기념비적인 논문은 이러한 이중적 발전을 명확하게 보여준다. 첫 번째는 혁명적인 하드웨어인 ’이벤트 카메라’의 잠재력을 최대한 활용하기 위해 완전히 새로운 알고리즘을 고안한 연구이며, 두 번째는 기존의 다양한 컴퓨터 비전 작업을 극적으로 가속화하는 범용적인 알고리즘 도구를 개발한 연구이다. 이 두 연구는 각각 센서 주도 혁신과 알고리즘 주도 혁신이라는, 컴퓨터 비전 발전의 두 가지 핵심 동력을 상징한다.</p>
<h3>2.1  이벤트 카메라 기반 실시간 3D 재구성 및 6-DoF 추적</h3>
<p>전통적인 컴퓨터 비전 시스템의 근간을 이루는 프레임 기반 카메라는 근본적인 한계를 내포하고 있다. 고정된 시간 간격(frame rate)으로 전체 이미지 프레임을 획득하는 방식은 장면의 변화가 거의 없는 정적인 상황에서는 엄청난 데이터 중복을 야기하고, 반대로 매우 빠른 움직임이 있는 동적인 상황에서는 심각한 모션 블러(motion blur)를 발생시킨다.12 또한, 조명이 극단적으로 밝거나 어두운 환경에서는 장면의 세부 정보를 소실시키는 낮은 동적 범위(Dynamic Range) 문제에 취약하다. 이러한 문제들은 특히 고속으로 비행하는 드론이나 증강현실(AR) 기기와 같이 실시간성과 강인성이 동시에 요구되는 로보틱스 응용 분야에서 치명적인 제약으로 작용해왔다.13</p>
<h4>2.1.1 이벤트 카메라의 작동 원리</h4>
<p>이러한 한계를 극복하기 위해 인간의 시신경 구조에서 영감을 얻은 생체모방 센서, 즉 이벤트 카메라(Event Camera)가 등장했다. 이벤트 카메라는 프레임이라는 개념 없이, 각 픽셀이 독립적이고 비동기적으로 작동하는 혁신적인 구조를 가진다.12 픽셀은 자신이 감지하는 빛의 로그 강도(logarithmic intensity)가 사전에 설정된 임계값을 넘어 변화할 때만 ’이벤트’를 발생시킨다. 이 이벤트는 <span class="math math-inline">(u, v, p, t)</span>라는 튜플 형태로 출력되는데, 이는 각각 이벤트가 발생한 픽셀의 좌표 <span class="math math-inline">(u, v)</span>, 밝기 변화의 방향(증가 또는 감소)을 나타내는 극성(polarity) <span class="math math-inline">p</span>, 그리고 마이크로초 단위의 정밀한 타임스탬프 <span class="math math-inline">t</span>를 포함한다.12</p>
<p>이러한 작동 방식은 몇 가지 중요한 장점을 낳는다. 첫째, 장면의 변화가 있을 때만 데이터를 생성하므로 데이터 중복이 거의 없고 정보 밀도가 매우 높다. 둘째, 각 이벤트가 마이크로초 단위의 시간 해상도를 가지므로 시간적 정밀도가 극도로 높아 고속 운동을 모션 블러 없이 포착할 수 있다. 셋째, 픽셀 단위로 밝기 변화에 적응하므로 매우 높은 동적 범위를 가진다.13 그러나 이처럼 기존 카메라와는 근본적으로 다른 데이터 스트림은 특징점 검출, 정합, 전체 이미지 정렬과 같은 전통적인 컴퓨터 비전 알고리즘들을 무용지물로 만들었다.12 따라서 이벤트 카메라의 잠재력을 활용하기 위해서는 데이터의 특성에 맞는 새로운 알고리즘 패러다임이 절실히 요구되었다.</p>
<h4>2.1.2 세 개의 분리된 확률 필터 기반 방법론</h4>
<p>ECCV 2016 최우수 논문상을 수상한 Hanme Kim 등의 연구 “Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera“는 바로 이 문제에 대한 최초의 포괄적인 해법을 제시했다.4 이 연구의 핵심은 동시적 위치 추정 및 지도 작성(Simultaneous Localisation and Mapping, SLAM)이라는 복잡한 문제를 세 개의 상호 연동되지만 분리된(decoupled) 확률 필터를 사용하여 실시간으로 해결하는 데 있다.5 각 필터는 문제의 한 가지 측면을 전문적으로 추정하며, 이벤트가 들어올 때마다 순차적으로 업데이트된다.</p>
<ol>
<li>
<p><strong>6-DoF 카메라 모션 필터 (6-DoF Camera Motion Filter):</strong> 이 필터는 카메라의 전역적인 6자유도(6-DoF) 자세, 즉 3차원 공간에서의 위치와 방향(<span class="math math-inline">SE(3)</span>)을 추적하는 역할을 한다. 이를 위해 확장 칼만 필터(Extended Kalman Filter, EKF)가 사용된다. 이벤트는 매우 높은 빈도로 발생하기 때문에, 연속된 이벤트 사이의 시간 간격은 극히 짧다. 따라서 이 필터는 이벤트와 이벤트 사이에는 카메라가 등속 운동을 한다는 ’상수 위치 모션 모델(constant position motion model)’을 가정하여 다음 상태를 예측한다.16 새로운 이벤트가 관측되면, 이 이벤트가 현재 추정된 카메라 자세와 장면 모델에 기하학적으로 얼마나 일관되는지를 측정하여 EKF의 업데이트 단계에 사용함으로써 자세 추정치를 보정한다.17 이 방식은 각 이벤트를 독립적인 측정치로 활용하여 매우 빠르고 연속적인 자세 추정을 가능하게 한다.</p>
</li>
<li>
<p><strong>장면 로그 강도 그래디언트 필터 (Scene Log Intensity Gradient Filter):</strong> 두 번째 필터는 기준이 되는 키프레임(keyframe)의 로그 강도 그래디언트(log intensity gradient)를 추정한다. 이벤트는 본질적으로 밝기 변화, 즉 공간적·시간적 그래디언트에 의해 발생하므로, 거의 모든 이벤트는 장면에 대한 그래디언트 정보를 담고 있다. 이 필터는 각 픽셀마다 독립적인 EKF를 유지하며, 새로운 이벤트가 발생할 때마다 해당 픽셀의 그래디언트 추정치를 업데이트한다.12 이렇게 추정된 그래디언트 필드는 이후 푸아송 방정식(Poisson equation)과 같은 기법을 통해 적분되어, 노이즈가 적고 동적 범위가 매우 높은 완전한 흑백 강도 이미지(intensity image)로 복원된다.12 이는 저용량의 이벤트 스트림으로부터 고품질의 비디오와 같은 시각 정보를 생성해내는 핵심 과정이다.</p>
</li>
<li>
<p><strong>장면 역깊이 필터 (Scene Inverse Depth Filter):</strong> 마지막 필터는 키프레임의 깊이(depth) 정보를 추정한다. 깊이 정보는 카메라의 움직임에 의해 발생하는 시차(parallax)로부터 얻어진다. 모든 이벤트가 그래디언트 정보를 담고 있는 것과 달리, 시차에 의한 정보는 주로 카메라의 병진 운동(translation) 시에 발생하므로 상대적으로 적은 수의 이벤트에만 포함되어 있다. 연구진은 이 점에 착안하여 깊이 추정을 그래디언트 추정으로부터 분리했다.12 이 필터 역시 픽셀별 EKF를 사용하여, 시차 정보를 담고 있는 이벤트가 발생했을 때 해당 픽셀의 역깊이(inverse depth)를 추정하고 업데이트한다. 역깊이 표현은 원거리에 있는 점들을 안정적으로 모델링하는 데 유리하여 SLAM 시스템에서 널리 사용된다.</p>
</li>
</ol>
<h4>2.1.3 결과 및 기여</h4>
<p>이 연구는 추가적인 센서(예: IMU)나 장면에 대한 사전 정보 없이, 순수하게 단일 이벤트 카메라의 데이터 스트림만으로 실시간 6-DoF 추적과 조밀한 3D 환경 재구성을 성공적으로 수행한 최초의 사례였다.5 이는 이벤트 카메라라는 새로운 센싱 패러다임이 복잡한 SLAM 문제에 적용될 수 있음을 증명한 기념비적인 성과였다. 이 연구는 하드웨어의 고유한 특성(비동기성, 고속, 고동적 범위)이 전통적인 알고리즘의 적용을 불가능하게 만들 때, 하드웨어의 작동 원리에 대한 깊은 이해를 바탕으로 완전히 새로운 알고리즘적 접근(이벤트 단위의 확률적 필터링)을 고안해야만 그 잠재력을 온전히 이끌어낼 수 있음을 보여주었다. 이는 센서 기술과 알고리즘이 함께 발전해야 하는 로보틱스 분야의 연구 개발에 중요한 방향성을 제시했다.</p>
<h3>2.2  고속 양방향 솔버를 이용한 엣지 보존 스무딩 최적화</h3>
<p>컴퓨터 비전의 수많은 문제들은 ’엣지 보존 스무딩(edge-aware smoothing)’이라는 공통된 부가 문제를 포함한다. 스테레오 매칭으로 얻은 깊이 맵의 노이즈를 제거하거나, 흑백 이미지에 색상을 입히거나, 이미지의 의미론적 분할(semantic segmentation) 결과를 다듬는 등의 작업에서, 우리는 객체의 경계(edge)를 넘어서는 정보의 확산을 막으면서 영역 내부에서는 정보를 부드럽게 전파하기를 원한다.18 기존의 접근법들은 딜레마에 빠져 있었다. 가우시안 필터와 같은 단순한 필터링 기법들은 속도는 빠르지만 엣지를 뭉개버리는 문제가 있었고, 조건부 무작위장(Conditional Random Field, CRF)과 같은 정교한 최적화 기법들은 결과물의 품질은 높았지만, 모든 픽셀 쌍 간의 관계를 고려해야 하므로 계산 비용이 너무 높아 실시간 응용이 거의 불가능했다.18</p>
<h4>2.2.1 양방향 공간에서의 최적화 방법론</h4>
<p>ECCV 2016에서 우수 논문상을 수상한 Jonathan T. Barron과 Ben Poole의 “The Fast Bilateral Solver“는 이 딜레마에 대한 우아하고 강력한 해결책을 제시했다.6 이 논문의 핵심 아이디어는 거대하고 다루기 힘든 픽셀 공간(pixel-space)에서의 최적화 문제를, 훨씬 작고 다루기 쉬운 저차원의 ’양방향 공간(bilateral-space)’으로 변환하여 해결하는 것이다.6</p>
<p>먼저, 엣지 보존 스무딩 문제는 다음과 같은 최적화 문제로 정형화될 수 있다 19:</p>
<p><span class="math math-display">
\min_{x} \frac{\lambda}{2} \sum_{i,j} \hat{W}_{i,j} (x_i - x_j)^2 + \sum_{i} c_i(x_i - t_i)^2
</span><br />
여기서 <span class="math math-display">x</span>는 우리가 찾고자 하는 최종 결과 벡터(예: 부드러운 깊이 값), <span class="math math-display">t</span>는 초기 입력 목표 벡터(예: 노이즈가 있는 깊이 값), <span class="math math-display">c</span>는 각 픽셀의 신뢰도를 나타내는 가중치이다. 첫 번째 항은 스무딩 항으로, 인접한 픽셀 <span class="math math-display">i, j</span>의 값 <span class="math math-display">x_i, x_j</span>가 비슷해지도록 유도한다. 이때 가중치 행렬 <span class="math math-display">\hat{W}</span>는 두 픽셀이 공간적으로 가깝고 색상도 비슷할 때 큰 값을 가지는 양방향 유사도(bilateral affinity)에 기반하여, 엣지를 경계로 스무딩이 억제되도록 한다. 두 번째 항은 데이터 충실도 항으로, 결과 <span class="math math-display">x</span>가 신뢰도 높은 초기 입력 <span class="math math-display">t</span>로부터 너무 멀어지지 않도록 제약한다.</p>
<p>이 문제의 난점은 <span class="math math-display">\hat{W}</span>가 이미지의 모든 픽셀 쌍에 대한 관계를 정의하는 거대한 행렬이라는 점이다. 연구진은 이 문제를 직접 푸는 대신, 픽셀들을 저차원의 양방향 공간 격자(bilateral grid)에 투영(splat)한다. 이 격자의 각 차원은 픽셀의 공간적 위치(x, y)와 색상/강도 값(예: L, u, v)에 해당한다.6 비슷한 색상을 가진 인접 픽셀들은 이 격자 위의 같은 또는 가까운 정점(vertex)으로 매핑된다.</p>
<p>이 변환을 통해, 픽셀 공간에서의 복잡한 최적화 문제는 양방향 공간의 격자 정점 값 <span class="math math-display">y</span>를 찾는 훨씬 작은 규모의 이차 형식(quadratic form) 문제로 재구성된다 19:</p>
<p><span class="math math-display">
\min_{y} \frac{1}{2} y^T A y - b^T y + c
</span><br />
이 문제의 해는 다음의 희소 선형 시스템(sparse linear system)을 푸는 것과 동일하다 19:</p>
<p><span class="math math-display">
Ay = b
</span><br />
여기서 행렬 <span class="math math-display">A</span>와 벡터 <span class="math math-display">b</span>는 양방향 공간 격자의 구조로부터 효율적으로 계산될 수 있다. 픽셀의 수가 수백만 개인 원본 문제와 달리, 양방향 공간 격자의 정점 수는 수천 개에 불과하므로 이 선형 시스템은 사전조건 적용 공액기울기법(preconditioned conjugate gradient method)과 같은 표준적인 기법을 사용하여 매우 빠르게 풀 수 있다.19 최종 결과</p>
<p><span class="math math-display">x</span>는 이렇게 구한 격자 정점 값 <span class="math math-display">y</span>를 다시 픽셀 공간으로 보간(slice)하여 얻는다.21</p>
<h4>결과 및 기여</h4>
<p>고속 양방향 솔버(Fast Bilateral Solver)는 정확도와 속도 사이의 오랜 트레이드오프를 깨뜨렸다. 깊이 초해상도, 컬러화, 스테레오, 의미론적 분할 등 다양한 컴퓨터 비전 작업에서 기존의 최고 수준 방법론과 동등하거나 더 나은 결과를 보이면서도, 연산 속도는 10배에서 최대 1000배까지 빨랐다.18 특히 의미론적 분할 작업에서 당시 표준 후처리 기법이었던 DenseCRF와 비교했을 때, DenseCRF가 달성한 정확도 향상의 70% 수준을 달성하면서도 추가 실행 시간은 11배나 단축시키는 놀라운 효율성을 보여주었다.21</p>
<p>이 연구의 진정한 기여는 특정 문제에 대한 단일 해법을 제시한 것을 넘어, 광범위한 응용 분야에 적용될 수 있는 강력하고 범용적인 ’알고리즘 도구’를 제공했다는 점에 있다. 복잡한 최적화 문제를 더 단순하고 효율적인 표현(양방향 공간)으로 변환하여 해결한다는 원리는 순수한 알고리즘적 통찰이 어떻게 실질적인 성능의 비약적 향상을 가져올 수 있는지를 보여주는 대표적인 사례가 되었다. 이는 이후 많은 심층 학습 기반의 후처리 모듈이나 미분 가능한 최적화 계층(differentiable optimization layer)의 개발에 영감을 주었다.</p>
<table><thead><tr><th>기법</th><th>기반 모델</th><th>정확도 (IOU)</th><th>추가 실행 시간</th></tr></thead><tbody>
<tr><td>Deeplab (Baseline)</td><td>VGG-16</td><td>66.0%</td><td>-</td></tr>
<tr><td>Deeplab + DenseCRF</td><td>VGG-16</td><td>68.1%</td><td>+ 918 ms</td></tr>
<tr><td>Deeplab + Fast Bilateral Solver</td><td>VGG-16</td><td>67.6%</td><td>+ 85 ms</td></tr>
</tbody></table>
<h2>제2장: 지능형 로봇 시스템의 자율성 및 상호작용 심화</h2>
<p>2016년 로봇 공학 분야는 단순히 정해진 작업을 반복하는 자동화 기계를 넘어, 불확실한 환경을 스스로 인식하고, 지능적으로 상호작용하며, 복잡한 과업을 자율적으로 수행하는 지능형 시스템으로의 진화를 가속화했다. IROS 2016에서 발표된 주요 연구들은 이러한 변화의 핵심 동력들을 명확히 보여주었다. 특히 로봇이 환경 정보를 보다 효율적으로 수집하고(능동적 인식), 자연계의 생물학적 원리를 모방하여 물리적 한계를 극복하며(생체모방), 엄밀한 제어 이론을 바탕으로 안정적인 자율성을 확보하는(제어 시스템) 방향으로의 심도 깊은 연구가 진행되었다. 이는 로봇의 ’지능’이 추상적인 알고리즘뿐만 아니라, 물리적 세계와의 상호작용 방식 그 자체에 내재되어야 한다는 인식이 확산되고 있음을 시사한다.</p>
<h3>2.1. 가우시안 프로세스를 활용한 능동적 촉각 탐사</h3>
<p>로봇이 미지의 물체를 안정적으로 파지하고 능숙하게 조작하기 위해서는 그 물체의 정확한 3차원 형태를 파악하는 것이 선결 과제이다. 시각 정보가 제한되거나 없는 환경에서는 촉각 센서를 이용한 탐사가 필수적이다. 그러나 기존의 촉각 탐사 전략은 물체 표면을 일정한 격자 형태로 만져보거나 무작위로 샘플링하는 방식을 주로 사용했는데, 이는 매우 비효율적이었다.2 인접한 표면 지점들은 기하학적으로 높은 상관관계를 가지므로, 이미 탐사한 지점 근처를 다시 만지는 것은 새로운 정보를 거의 제공하지 못하기 때문이다.8 이는 제한된 시간 안에 임무를 완수해야 하는 실제 로봇 응용에서 심각한 병목 현상을 초래했다.</p>
<h4>확률론적 표면 모델링과 능동적 탐사 전략</h4>
<p>IROS 2016의 주요 논문으로 선정된 Tucker Hermans 등의 “Active Tactile Object Exploration with Gaussian Processes“는 이 문제에 대한 지능적인 해법을 제시했다.2 이 연구는 로봇을 수동적인 데이터 수집기에서 능동적인 탐험가로 전환시키는 패러다임의 변화를 이끌었다.</p>
<ol>
<li>
<p><strong>가우시안 프로세스(GP) 기반 표면 모델링:</strong> 연구의 핵심은 물체 표면을 가우시안 프로세스(Gaussian Process, GP)라는 확률 모델로 표현하는 것이다.8 GP는 단순히 표면의 형태를 하나의 값으로 예측하는 것이 아니라, 각 지점에서의 예측값과 함께 그 예측이 얼마나 불확실한지를 나타내는 분산(variance) 값을 함께 제공한다.26 즉, 로봇이 수집한 촉각 데이터(</p>
<p><span class="math math-inline">(x, y, z)</span> 좌표)를 바탕으로, 아직 만져보지 않은 영역에 대해 “가장 가능성 있는 표면의 위치는 여기쯤이고, 그 예측에 대한 신뢰도는 이 정도이다“라고 확률적으로 추론할 수 있게 된다. 논문에서는 표면을 <span class="math math-inline">y = f(x, z)</span> 형태의 명시적 함수로 모델링하여 GP를 적용했다.8</p>
</li>
<li>
<p><strong>불확실성 기반 능동적 접촉(Active Touch) 전략:</strong> 이 확률적 모델의 가장 큰 장점은 로봇이 ’자신이 무엇을 모르는지’를 정량적으로 알 수 있다는 점이다. GP 모델의 분산 값이 높은 영역은 아직 정보가 부족하여 불확실성이 큰 영역을 의미한다. 능동적 접촉 전략은 바로 이 불확실성 정보를 활용한다.2 로봇은 다음 접촉 지점을 무작위나 격자 위에서 선택하는 대신, 현재 GP 모델에서 분산이 가장 높은, 즉 가장 불확실한 지점을 의도적으로 선택하여 탐사한다.8 이 방식은 베이즈 최적화(Bayesian Optimization) 분야에서 영감을 얻은 것으로, 각 접촉 행위가 모델의 전체적인 불확실성을 가장 효율적으로 감소시키도록 유도한다. 즉, 매번의 접촉을 통해 가장 많은 정보량을 획득하는 것이다.</p>
</li>
</ol>
<h4>결과 및 기여</h4>
<p>연구팀은 BioTac 촉각 센서가 장착된 Mitsubishi PA-10 로봇 팔을 이용한 실제 실험을 통해 제안된 방법론의 우수성을 입증했다.8 강철 용기나 도자기 새와 같은 복잡한 형상의 물체 표면을 재구성하는 실험에서, 능동적 접촉 전략은 무작위 샘플링 전략에 비해 훨씬 적은 수의 접촉만으로도 더 빠르고 정확하게 표면 모델을 구축할 수 있었다.8</p>
<p>이 연구의 기여는 단순히 촉각 탐사 기술을 개선한 것을 넘어선다. 이는 로봇 공학에서 ’능동적 인식(active perception)’이라는 중요한 개념을 구체화한 사례이다. 로봇은 더 이상 환경으로부터 주어지는 데이터를 수동적으로 처리하는 존재가 아니라, 자신의 목표(표면 재구성)를 달성하기 위해 어떤 정보가 가장 필요한지를 스스로 판단하고, 그 정보를 얻기 위한 행동(다음 접촉 위치 결정)을 계획하고 실행하는 지능적 주체로 격상되었다. 이러한 인식과 행동의 지능적인 결합은 자율 로봇이 복잡하고 예측 불가능한 실제 세계에서 효율적으로 임무를 수행하기 위한 핵심적인 능력이다.</p>
<h3>2.2. 생체모방 로봇 및 고속 자율 시스템</h3>
<p>IROS 2016에서는 로봇의 물리적 능력과 자율성의 한계를 확장하려는 두 가지 상보적인 연구 흐름이 두드러졌다. 하나는 자연계 생물들의 경이로운 적응 능력에서 영감을 얻어 로봇의 강인성을 높이려는 생체모방 연구이고, 다른 하나는 극한의 속도와 정밀성을 요구하는 경쟁 환경을 통해 자율 시스템의 모든 구성 요소를 한계까지 밀어붙이는 연구이다.</p>
<ul>
<li>
<p><strong>생체모방을 통한 강인성 확보:</strong> IROS 2016의 또 다른 주요 논문은 바퀴벌레에서 영감을 받아 날개를 이용해 스스로 뒤집힌 자세를 바로잡는 로봇에 관한 연구였다.9 전통적인 공학적 접근법으로는 해결하기 어려운 넘어짐(self-righting)과 같은 실제 환경에서의 돌발 상황에 대해, 생물학적 진화가 찾아낸 독창적이고 효율적인 해법을 로봇에 적용한 것이다. 이는 로봇의 지능이 단지 계산적인 능력에만 있는 것이 아니라, 예측 불가능한 물리적 상호작용에 대처하는 신체적 능력, 즉 물리적 강인성(robustness)과 직결된다는 점을 보여준다. 이러한 생체모방 연구는 자연이 수백만 년에 걸쳐 최적화한 생존 전략을 공학적으로 재해석함으로써, 실험실 환경을 벗어난 로봇이 마주할 다양한 어려움에 대한 효과적인 해결책을 제공한다.</p>
</li>
<li>
<p><strong>드론 레이싱: 자율 비행 기술의 촉매제:</strong> 같은 해 IROS 2016에서는 세계 최초의 자율 드론 레이싱 챌린지가 개최되었다.27 이 대회는 연구자들에게 명확하고 도전적인 목표를 제시했다. 제한된 온보드 센서와 계산 능력만으로 복잡한 경로에 놓인 게이트들을 최대한 빠른 속도로 통과해야 하는 이 임무는, 비전 기반 항법, 실시간 상태 추정, 최적 제어 등 고속 자율 비행에 필요한 모든 핵심 기술들의 통합적인 성능을 극한까지 시험하는 무대였다. 최고 속도 0.6 m/s를 기록한 이 첫 대회는 이후 더 빠른 속도와 복잡한 환경으로 발전하며, 민첩한 소형 비행 로봇(MAV) 기술의 발전을 가속화하는 중요한 촉매제 역할을 했다.27 이처럼 명확한 목표와 경쟁 구조를 가진 챌린지는 특정 문제 해결에 집중된 연구를 촉진하고, 그 과정에서 개발된 기술들의 성능을 객관적으로 비교, 검증함으로써 분야 전체의 발전을 이끄는 강력한 동력이 된다.</p>
</li>
</ul>
<h3>2.3. 해양 무인 이동체를 위한 제어 시스템 이론</h3>
<p>로봇의 화려한 외적 성능 뒤에는 그 움직임을 안정적이고 정확하게 만들어주는 견고한 제어 이론이 자리 잡고 있다. 2016년 3분기(9월)에 저명 학술지인 <em>IEEE Transactions on Control Systems Technology</em>에 게재된 한 논문은 불완전 구동(underactuated) 해양 무인 이동체를 위한 새로운 ‘적분 시선 유도(Integral Line-of-Sight Guidance)’ 제어 기법을 제시했다.28 불완전 구동 시스템은 제어 입력의 수보다 자유도가 많아 제어가 까다로운데, 특히 파도나 조류와 같은 외란이 심한 해양 환경에서는 더욱 그러하다. 이 연구는 이러한 복잡한 동적 환경 속에서도 로봇이 목표 경로를 안정적으로 추종할 수 있도록 하는 수학적 기반을 제공했다. 이는 로봇 공학의 발전이 인공지능의 학습 기반 접근법뿐만 아니라, 시스템의 동역학에 대한 깊은 이해를 바탕으로 한 전통적인 제어 이론의 지속적인 발전 위에서 이루어짐을 보여주는 사례이다. 자율 시스템의 신뢰성과 안전성을 보장하는 이러한 기초 연구는 로봇 기술이 실제 산업 현장과 실생활에 적용되기 위한 필수적인 토대이다.</p>
<h2>제3장: 의료 영상 분석의 패러다임 전환: 3D U-Net의 등장</h2>
<p>2016년 의료 영상 분석 분야는 심층 학습의 도입으로 인해 근본적인 변화를 겪고 있었다. 그러나 CT나 MRI 스캔과 같이 본질적으로 3차원 볼륨 데이터인 의료 영상을 다루는 데에는 두 가지 거대한 장벽이 존재했다. 첫 번째는 3차원 데이터를 직접 처리하는 데 따르는 막대한 계산 비용과 메모리 요구량이었다. 두 번째는 이보다 훨씬 더 근본적이고 심각한 문제로, 심층 학습 모델을 훈련시키는 데 필요한 방대한 양의 정밀한 레이블링 데이터를 확보하는 것이 거의 불가능에 가깝다는 점이었다.11 숙련된 임상 전문가가 3차원 볼륨 데이터의 모든 복셀(voxel)에 대해 정확한 주석(annotation)을 다는 작업은 엄청난 시간과 비용을 요구하는, 그야말로 ’인간 병목현상(human bottleneck)’이었다.30 MICCAI 2016에서 발표된 단 하나의 혁신적인 논문은 이 두 가지 문제를 동시에 해결하며, 3차원 의료 영상 분석의 새로운 시대를 열었다.</p>
<h3>3.1. 희소 주석 기반 3D U-Net의 아키텍처 및 학습 방법론</h3>
<p>Özgün Çiçek 등이 발표한 “3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation“은 그 제목에서부터 문제의 핵심과 해결책을 명확히 제시한다.11 이 연구는 3차원 데이터 처리 문제와 주석 부족 문제를 독창적인 아키텍처 설계와 학습 방법론의 결합으로 극복했다.</p>
<h4>3D U-Net 아키텍처: 2D에서 3D로의 자연스러운 확장</h4>
<p>3D U-Net의 구조적 기반은 이미 2차원 생의학 이미지 분할에서 압도적인 성공을 거둔 2D U-Net 아키텍처이다.32 2D U-Net의 성공 비결은 대칭적인 인코더-디코더 구조와 그 사이를 잇는 스킵 연결(skip connection)에 있었다. 인코더(수축 경로)는 점진적으로 이미지의 해상도를 줄여나가며 깊고 추상적인 의미론적 특징(semantic feature)을 추출하고, 디코더(확장 경로)는 다시 해상도를 높여가며 픽셀 수준의 정밀한 위치 정보를 복원한다. 이때 스킵 연결은 인코더의 각 단계에서 추출된 고해상도 특징을 디코더의 해당 단계로 직접 전달하여, 디코더가 정확한 분할 경계를 생성하는 데 필요한 세부 정보를 잃지 않도록 돕는다.</p>
<p>3D U-Net은 이 강력하고 우아한 설계 원리를 3차원 공간으로 그대로 확장했다.31 2D 컨볼루션은 3D 컨볼루션으로, 2D 맥스 풀링은 3D 맥스 풀링으로, 2D 업-컨볼루션은 3D 업-컨볼루션으로 대체되었다.29 이를 통해 네트워크는 2차원 슬라이스의 나열이 아닌, 진정한 3차원 볼륨 데이터를 입력으로 받아 3차원 공간에서의 맥락적, 공간적 특징을 자연스럽게 학습할 수 있게 되었다. 이는 3차원 구조물의 형태와 연결성을 이해하는 것이 중요한 의료 영상 분석에 최적화된 구조였다. 이 아키텍처의 힘은 그 설계 원리가 차원에 구애받지 않는 보편성과 일반성을 가졌다는 점에서 비롯되며, 이는 3D U-Net이 발표 직후부터 수많은 3차원 분할 문제의 표준 모델로 자리 잡게 된 배경이 되었다.</p>
<h4>희소 주석 기반 학습: 인간 병목현상에 대한 알고리즘적 해결책</h4>
<p>3D U-Net의 진정한 혁신성은 아키텍처 자체보다도 ’희소 주석(sparse annotation)’으로부터 학습하는 방법론에 있다. 이는 의료 영상 분야의 가장 큰 실질적인 장벽이었던 데이터 부족 문제를 정면으로 돌파한 독창적인 아이디어였다.</p>
<p>핵심은 **가중 교차 엔트로피 손실 함수(weighted cross-entropy loss function)**의 사용에 있다.11 심층 학습 모델의 훈련 과정은 네트워크의 예측 결과와 실제 정답(ground truth) 레이블 간의 오차, 즉 손실(loss)을 최소화하는 방향으로 가중치를 업데이트하는 과정이다. 일반적인 분할 문제에서는 이미지의 모든 픽셀(또는 복셀)에 대해 손실을 계산한다. 그러나 3D U-Net의 저자들은 이 과정에 간단하지만 강력한 변형을 가했다. 3차원 볼륨 데이터 내에서 전문가가 레이블링한 소수의 2차원 슬라이스에 포함된 복셀들에 대해서만 손실을 계산하고, 레이블이 없는 압도적 다수의 나머지 복셀들은 손실 계산 시 가중치를 0으로 설정하여 훈련 과정에서 완전히 무시하도록 한 것이다.31</p>
<p>이 접근법은 네트워크로 하여금 제한된 정보로부터 최대한의 학습을 이끌어내도록 강제한다. 네트워크는 단 몇 장의 주석이 달린 슬라이스에서 보이는 장기나 병변의 형태, 질감, 그리고 주변 구조물과의 공간적 관계 등 일반화 가능한 특징들을 학습해야만 한다. 그리고 이렇게 학습된 지식을 바탕으로, 주석이 없는 인접 슬라이스들, 나아가 전체 3차원 볼륨에 대해 일관성 있고 정확한 분할 결과를 ’추론’해내야 한다. 이는 단순히 데이터를 암기하는 것이 아니라, 데이터에 내재된 근본적인 해부학적 패턴을 학습해야만 가능한 일이다. 이처럼 3D U-Net은 알고리즘적 설계를 통해 값비싼 전문가의 노동력을 최소화하고, 심층 학습의 적용 가능성을 극적으로 넓혔다.</p>
<h3>3.2. 3D U-Net의 응용 및 영향</h3>
<p>연구팀은 제노푸스(Xenopus) 신장이라는 복잡하고 가변성이 큰 3차원 구조의 현미경 이미지 데이터셋을 통해 제안된 방법론의 효과를 검증했다.11 실험 결과, 전체 볼륨 중 극히 일부 슬라이스에만 주석을 달았음에도 불구하고, 훈련된 3D U-Net은 전체 볼륨에 대해 매우 정확하고 조밀한 3D 분할 결과를 생성해냈다. 반자동 분할 시나리오에서는 0.863이라는 높은 IoU(Intersection over Union)를 달성하며 그 실용성을 입증했다.31</p>
<p>이 논문이 발표된 이후, 3D U-Net과 그 변형 모델들은 의료 영상 분할 분야의 사실상 표준(de facto standard)으로 자리 잡았다. 이 연구의 주저자인 Özgün Çiçek이 훗날 이 업적으로 MICCAI 젊은 과학자 영향력상을 수상한 것은 그 파급력을 상징적으로 보여준다.10 3D U-Net의 등장은 전 세계의 수많은 연구자와 임상의들이 값비싼 전체 데이터셋 주석 작업 없이도 자신의 데이터에 심층 학습을 적용할 수 있게 만들었다. 이는 자동화된 질병 진단, 정밀한 수술 계획 수립, 치료 반응의 정량적 평가 등 셀 수 없이 많은 의료 응용 연구의 폭발적인 증가를 촉발했다. 3D U-Net은 단순히 하나의 성공적인 아키텍처를 제시한 것을 넘어, 심층 학습 기술이 실제 임상 현장의 가장 큰 제약 조건을 어떻게 창의적으로 극복하고 실질적인 가치를 창출할 수 있는지를 보여준, 기술 성숙의 중요한 이정표였다.</p>
<h2>결론: 2016년 3분기 연구 동향 종합 및 미래 전망</h2>
<p>2016년 3분기에 발표되고 확정된 인공지능 및 로봇 공학 분야의 주요 연구들은 개별적인 기술적 성취를 넘어, 해당 분야의 미래를 규정하는 거대한 패러다임의 전환을 예고하는 상호 연결된 신호들이었다. 본 보고서에서 심층 분석한 세 가지 핵심 연구—이벤트 카메라 기반 SLAM, 고속 양방향 솔버, 그리고 희소 주석 기반 3D U-Net—는 각각 하드웨어와 소프트웨어의 공진화, 알고리즘 효율성의 극한 추구, 그리고 실용적 문제 해결을 위한 심층 학습의 진화라는 시대적 흐름을 명확하게 대변한다.</p>
<p>첫째, <strong>하드웨어-소프트웨어의 공진화</strong>는 이벤트 카메라 연구에서 극명하게 드러났다. 이 연구는 새로운 센서의 등장이 단순히 기존 알고리즘의 성능을 개선하는 것을 넘어, 문제 해결 방식 자체를 근본적으로 재정의하도록 요구함을 보여주었다.12 프레임이 없는 비동기적 데이터 스트림이라는 하드웨어의 특성은 이벤트 단위의 확률적 필터링이라는 새로운 소프트웨어 패러다임을 탄생시켰다. 이는 미래의 로봇 공학 발전이 더 나은 알고리즘뿐만 아니라, 새로운 감각 양식(modality)을 제공하는 혁신적인 센서와 그 잠재력을 최대한 활용하는 맞춤형 소프트웨어 간의 긴밀한 상호작용을 통해 이루어질 것임을 시사한다.</p>
<p>둘째, <strong>효율성의 최우선적 가치</strong>는 고속 양방향 솔버와 능동적 촉각 탐사 연구에서 공통적으로 강조되었다. 고속 양방향 솔버는 복잡한 최적화 문제를 더 단순한 공간으로 변환함으로써, 실시간성이 요구되는 다양한 비전 응용의 문을 열었다.18 능동적 촉각 탐사는 무분별한 데이터 수집이 아닌, 정보 이론에 기반한 지능적 데이터 획득을 통해 탐사 효율을 극대화했다.2 이 두 연구는 인공지능과 로봇이 통제된 실험실 환경을 벗어나 예측 불가능하고 동적인 실제 세계로 나아가기 위해서는, 알고리즘의 계산 효율성과 데이터 획득의 효율성이 정확도만큼이나 중요하다는 근본적인 원칙을 재확인시켜 주었다.</p>
<p>셋째, <strong>실용주의적 심층 학습의 성숙</strong>은 3D U-Net 연구에서 정점을 찍었다. 이 연구는 심층 학습 분야가 단순히 더 깊고 복잡한 네트워크 구조를 경쟁하는 단계를 지나, 기술이 현장에 적용되는 과정에서 발생하는 가장 큰 실질적인 병목 현상, 즉 ‘인간의 노동력’ 문제를 해결하는 단계로 진입했음을 보여주었다.11 희소 주석으로부터 학습하는 능력은 심층 학습을 소수의 거대 기관만이 아닌, 데이터 확보에 어려움을 겪는 대다수의 연구자와 실무자들을 위한 민주적인 도구로 만들었다.</p>
<p>이러한 2016년 3분기의 연구들은 이후 수년간의 기술 발전을 위한 견고한 초석을 다졌다. 이벤트 카메라 기반 SLAM은 고속 드론 항법, 자율주행, 그리고 차세대 AR/VR 기기의 핵심 기술로 발전했다. 고속 양방향 솔버와 같은 효율적인 최적화 기법들은 더 큰 심층 학습 시스템의 미분 가능한 구성 요소로 통합되었다. 능동적 인식 전략은 로봇이 환경과 상호작용하는 방식을 더욱 정교하게 만들었다. 그리고 3D U-Net은 계산 기반 의학(computational medicine) 분야에 혁명을 일으키는 중추적인 아키텍처가 되었다. 결론적으로, 2016년 3분기는 인공지능과 로봇 공학이 가능성의 시대를 지나 실용성의 시대로 접어드는 중요한 변곡점이었으며, 이때 제시된 아이디어들은 오늘날 우리가 목도하고 있는 기술 혁신의 근간을 이루고 있다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>Computer Vision – ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV | Request PDF - ResearchGate, 10월 6, 2025에 액세스, https://www.researchgate.net/publication/321541441_Computer_Vision_-_ECCV_2016_14th_European_Conference_Amsterdam_The_Netherlands_October_11-14_2016_Proceedings_Part_IV</li>
<li>Prof. Tucker Herman’s and colleagues’ IROS 2016 paper accepted - Robotics Center, 10월 6, 2025에 액세스, https://robotics.coe.utah.edu/hermans_iros2016/</li>
<li>MICCAI 2016, 10월 6, 2025에 액세스, http://www.miccai2016.org/</li>
<li>European Computer Vision Association: ECVA, 10월 6, 2025에 액세스, https://www.ecva.net/</li>
<li>Real-time 3D reconstruction and 6-DoF tracking with an event camera, 10월 6, 2025에 액세스, https://portal.fis.tum.de/en/publications/real-time-3d-reconstruction-and-6-dof-tracking-with-an-event-came</li>
<li>The Fast Bilateral Solver (Contributed to OpenCV) | The_Bilateral_Solver, 10월 6, 2025에 액세스, https://kuan-wang.github.io/The_Bilateral_Solver/</li>
<li>Ben Poole - Stanford Computer Science, 10월 6, 2025에 액세스, https://cs.stanford.edu/~poole/poole_cv_nov17.pdf</li>
<li>Active Tactile Object Exploration with Gaussian Processes, 10월 6, 2025에 액세스, https://users.cs.utah.edu/~thermans/papers/yi-iros2016-gp-active-touch.pdf</li>
<li>Our paper is selected as a Highlight of IROS 2016 - Terradynamics Lab, 10월 6, 2025에 액세스, https://li.me.jhu.edu/our-paper-is-selected-as-a-highlight-of-iros-2016/</li>
<li>MICCAI Young Scientist Publication Impact Award, 10월 6, 2025에 액세스, https://miccai.org/index.php/about-miccai/awards/young-scientist-impact-award/</li>
<li>3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation, 10월 6, 2025에 액세스, https://www.researchgate.net/publication/304226155_3D_U-Net_Learning_Dense_Volumetric_Segmentation_from_Sparse_Annotation</li>
<li>Real-Time 3D Reconstruction and 6-DoF Tracking with an Event …, 10월 6, 2025에 액세스, https://www.doc.ic.ac.uk/~ajd/Publications/kim_etal_eccv2016.pdf</li>
<li>Real-Time Visual SLAM with an Event Camera - Department of Computing, 10월 6, 2025에 액세스, https://www.doc.ic.ac.uk/~ajd/Publications/Kim-H-2018-PhD-Thesis.pdf</li>
<li>Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera - SciSpace, 10월 6, 2025에 액세스, https://scispace.com/pdf/real-time-3d-reconstruction-and-6-dof-tracking-with-an-event-3u23kbnh0f.pdf</li>
<li>Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera - ResearchGate, 10월 6, 2025에 액세스, https://www.researchgate.net/publication/308278521_Real-Time_3D_Reconstruction_and_6-DoF_Tracking_with_an_Event_Camera</li>
<li>Research Notes and Jacobians: Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera - Patrick Geneva, 10월 6, 2025에 액세스, https://pgeneva.com/downloads/notes/2018_notes_kim2016eccv.pdf</li>
<li>Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera - Semantic Scholar, 10월 6, 2025에 액세스, https://pdfs.semanticscholar.org/30f4/6fdfe1fdab60bdecaa27aaa94526dfd87ac1.pdf</li>
<li>[1511.03296] The Fast Bilateral Solver - arXiv, 10월 6, 2025에 액세스, https://arxiv.org/abs/1511.03296</li>
<li>The Fast Bilateral Solver, 10월 6, 2025에 액세스, https://3dvar.com/Barron2016The.pdf</li>
<li>Fast Bilateral Solver for Semantic Video Segmentation, 10월 6, 2025에 액세스, <a href="http://www.maxclwang.com/files/Fast%20Bilateral%20Solver%20for%20Semantic%20Video%20Segmentation%20Report.pdf">http://www.maxclwang.com/files/Fast%20Bilateral%20Solver%20for%20Semantic%20Video%20Segmentation%20Report.pdf</a></li>
<li>Jonathan T. Barron Ben Poole Google Research … - Semantic Scholar, 10월 6, 2025에 액세스, https://pdfs.semanticscholar.org/111a/2581d8b4023850dd4fcf1df27ffca167a627.pdf</li>
<li>The Fast Bilateral Solver - Google Research, 10월 6, 2025에 액세스, https://research.google/pubs/the-fast-bilateral-solver/</li>
<li>The Fast Bilateral Solver | Request PDF - ResearchGate, 10월 6, 2025에 액세스, https://www.researchgate.net/publication/283761971_The_Fast_Bilateral_Solver</li>
<li>The Fast Bilateral Solver | Request PDF - ResearchGate, 10월 6, 2025에 액세스, https://www.researchgate.net/publication/308277688_The_Fast_Bilateral_Solver</li>
<li>(PDF) Active Tactile Object Exploration with Gaussian Processes - ResearchGate, 10월 6, 2025에 액세스, https://www.researchgate.net/publication/309233170_Active_Tactile_Object_Exploration_with_Gaussian_Processes</li>
<li>Active Multi-Contact Continuous Tactile Exploration with Gaussian Process Differential Entropy - Danny Driess, 10월 6, 2025에 액세스, https://dannydriess.github.io/papers/19-driess-ICRA.pdf</li>
<li>The IROS 2016 Competitions [Competitions] | Request PDF - ResearchGate, 10월 6, 2025에 액세스, https://www.researchgate.net/publication/315649667_The_IROS_2016_Competitions_Competitions</li>
<li>IEEE Transactions on Control Systems Technology Outstanding Paper Award, 10월 6, 2025에 액세스, https://ieeecss.org/awards/ieee-transactions-control-systems-technology-outstanding-paper-award</li>
<li>3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation | Request PDF, 10월 6, 2025에 액세스, https://www.researchgate.net/publication/313072707_3D_U-Net_Learning_Dense_Volumetric_Segmentation_from_Sparse_Annotation</li>
<li>Learning Dense Volumetric Segmentation from Sparse Annotation - Computer Vision Group, Freiburg, 10월 6, 2025에 액세스, https://lmb.informatik.uni-freiburg.de/people/cicek/CRC1140/MICCAI_Poster_Ozgun.pdf</li>
<li>3D U-Net: Learning Dense Volumetric Segmentation from Sparse …, 10월 6, 2025에 액세스, https://lmb.informatik.uni-freiburg.de/Publications/2016/CABR16/cicek16miccai.pdf</li>
<li>3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation, 10월 6, 2025에 액세스, <a href="https://www.semanticscholar.org/paper/3D-U-Net%3A-Learning-Dense-Volumetric-Segmentation-%C3%87i%C3%A7ek-Abdulkadir/7fc464470b441c691d10e7331b14a525bc79b8bb">https://www.semanticscholar.org/paper/3D-U-Net%3A-Learning-Dense-Volumetric-Segmentation-%C3%87i%C3%A7ek-Abdulkadir/7fc464470b441c691d10e7331b14a525bc79b8bb</a></li>
<li>3D U-Net Volumetric Segmentation - OpenGenus IQ, 10월 6, 2025에 액세스, https://iq.opengenus.org/3d-unet/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>