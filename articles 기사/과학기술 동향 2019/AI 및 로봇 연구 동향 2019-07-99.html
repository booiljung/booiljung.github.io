<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2019년 7월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2019년 7월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2019년 AI 및 로봇 연구 동향</a> / <span>2019년 7월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2019년 7월 AI 및 로봇 연구 동향</h1>
<h2>1. 서론: 2019년 7월, 인공지능 연구의 변곡점</h2>
<p>2019년 7월은 인공지능(AI) 연구 역사에서 중요한 변곡점으로 기록된다. 이 시기는 특정 게임의 정복을 넘어, AI가 불확실성과 복잡성을 다루는 방식에 근본적인 패러다임 전환이 일어났음을 시사하는 기념비적인 연구들이 동시다발적으로 발표된 달이었다. 본 보고서는 이 시기에 발표된 주요 연구들을 심층 분석하여, 당시의 기술적 성취를 조명하고 AI의 미래 발전 경로에 미친 영향을 고찰하고자 한다.</p>
<p>2019년 이전까지 AI 연구는 알파고(AlphaGo)가 보여준 것처럼 주로 완전 정보 게임(perfect-information games)에서의 성공에 집중되었다.1 체스나 바둑과 같이 모든 정보가 투명하게 공개되고 두 명의 플레이어가 순서를 번갈아 가며 두는 환경은 AI의 탐색 및 최적화 능력을 시험하는 이상적인 테스트베드였다. 그러나 2019년 7월, 이러한 패러다임에 균열을 내는 중요한 성과들이 연이어 등장했다. 페이스북 AI의 플러리버스(Pluribus)는 여러 명의 플레이어가 서로의 정보를 모른 채 경쟁하는 다자간 불완전 정보 게임(multiplayer imperfect-information game)인 포커를 정복했다.2 딥마인드의 알파스타(AlphaStar)는 수많은 유닛을 실시간으로 제어해야 하는 복잡한 실시간 전략 게임(real-time strategy game)에서 인간 최고 수준에 도달했으며 4, 같은 연구소의 뮤제로(MuZero)는 아예 게임의 규칙조차 모르는 상태에서 스스로 환경을 학습하고 계획을 세우는 경이로운 능력을 선보였다.6</p>
<p>이러한 성과들은 AI 연구의 평가 기준이 정적이고 명확한 규칙의 세계를 넘어, 동적이고 불확실하며 불완전한 정보 환경으로 이동하고 있음을 명확히 보여주었다. 즉, AI가 해결해야 할 문제의 정의 자체가 현실 세계의 복잡성에 한층 더 가까워진 것이다. 이는 AI가 현실 세계의 문제 해결에 한 걸음 더 다가섰음을 의미하는 질적인 도약이었다.</p>
<p>본 보고서는 이러한 전환의 중심에 있었던 2019년 7월의 주요 사건들을 체계적으로 분석한다. 1장과 2장에서는 강화학습의 새로운 지평을 연 딥마인드와 페이스북 AI의 게임 AI 연구를 각각 심층적으로 분석한다. 3장에서는 거대 언어 모델(LLM)의 부상과 산업 지형의 변화를 OpenAI와 마이크로소프트의 전략적 제휴 사례를 통해 살펴본다. 4장에서는 가상 세계의 눈부신 성취와 대비되는 물리 세계에서의 로보틱스 연구 동향을 펜실베이니아 대학의 TRUSSES 프로젝트를 통해 탐구한다. 마지막으로 5장에서는 arXiv에 발표된 주요 학술 논문들을 통해 당시의 핵심 기술 트렌드와 연구 커뮤니티의 주요 관심사를 분석함으로써, 2019년 7월이 AI 기술 발전에 어떠한 유산을 남겼는지 종합적으로 조명할 것이다.</p>
<h2>2.  강화학습의 지평 확장: 딥마인드의 연속된 돌파</h2>
<p>2019년 7월을 전후하여 구글 딥마인드는 강화학습 분야에서 두 개의 기념비적인 연구 결과를 발표하며 기술적 리더십을 다시 한번 입증했다. 알파스타(AlphaStar)는 실시간 전략 게임이라는 극도로 복잡한 영역을 정복했고, 뮤제로(MuZero)는 환경의 규칙을 모르는 상태에서 학습하는 범용 알고리즘의 가능성을 제시했다. 이 두 연구는 복잡한 문제 해결을 위해 서로 다른 접근법을 취하며 강화학습의 발전 방향에 대한 중요한 두 갈래 길을 보여주었다.</p>
<h3>2.1  알파스타(AlphaStar): 실시간 전략 게임 정복과 다중 에이전트 학습 리그</h3>
<p>2019년 7월, 딥마인드는 자사의 AI 알파스타가 블리자드 엔터테인먼트의 실시간 전략(RTS) 게임 스타크래프트 II의 유럽 서버 배틀넷에서 상위 0.2%의 플레이어에게만 부여되는 그랜드마스터 등급을 달성했다고 공식 발표했다.4 이는 AI가 바둑이나 체스와 같은 턴제 보드게임을 넘어, 인간의 직관, 장기 전략, 그리고 신속한 실시간 제어 능력이 복합적으로 요구되는 RTS 게임에서 인간 최고 수준의 기량을 보인 최초의 사례였다. 스타크래프트 II는 게임 이론, 불완전 정보, 장기 계획, 실시간 제어, 그리고 방대한 행동 공간이라는 AI 연구의 오랜 난제들을 모두 포함하고 있어 ’AI의 거대한 도전 과제’로 여겨져 왔다.8</p>
<p>알파스타의 성공은 단순히 연산 능력을 높여 달성한 것이 아니었다. 그 핵심에는 ’리그(The League)’라고 명명된 혁신적인 다중 에이전트 강화학습 프레임워크가 존재한다. 기존의 자가 대국(self-play) 방식이 단일 에이전트가 자신의 이전 버전을 상대로 학습하며 점진적으로 강해지는 방식이었다면, 리그는 수많은 에이전트로 구성된 하나의 거대한 생태계를 구축하여 경쟁적 공진화(competitive co-evolution)를 통해 전략적 다양성을 폭발적으로 증가시키는 접근법이다.8</p>
<p>리그의 작동 방식은 다음과 같은 단계로 이루어진다. 첫째, 블리자드가 제공한 익명의 인간 플레이어 게임 데이터를 활용한 모방 학습(imitation learning)을 통해 기본적인 게임 운영 능력과 다양한 전략의 기초를 갖춘 초기 에이전트를 생성한다.4 이 초기 에이전트가 리그의 ’시드’가 된다. 둘째, 이 시드 에이전트를 기반으로 리그가 시작된다. 리그 내의 에이전트, 즉 ’경쟁자(Competitors)’들은 서로 끊임없이 대결하며 학습하고, 주기적으로 기존 경쟁자로부터 새로운 경쟁자가 ’분기(branched)’되어 리그에 추가된다. 이 과정을 통해 리그는 시간이 지남에 따라 점점 더 다양하고 강력한 전략을 보유한 에이전트들로 채워진다.8</p>
<p>리그의 가장 독창적인 부분은 전략적 다양성을 확보하는 메커니즘에 있다. 리그는 단순히 전체 승률을 극대화하는 것을 목표로 하는 ’주력 에이전트(main agents)’만으로 구성되지 않는다. 이와 더불어, 특정 주력 에이전트의 약점을 집요하게 파고들어 그 약점을 보완하도록 유도하는 ’착취 에이전트(exploiter agents)’를 의도적으로 생성하고 리그에 투입한다.9 이 착취 에이전트의 목표는 리그 전체에서의 높은 승률이 아니라, 특정 상대의 전략적 허점을 공략하는 것이다. 예를 들어, 특정 빌드에 취약한 주력 에이전트가 있다면, 그 빌드만을 전문적으로 사용하는 착취 에이전트가 등장하여 주력 에이전트가 해당 약점을 극복하도록 강제한다. 이러한 구조는 마치 가위바위보처럼 서로 물고 물리는 순환적 우위 관계(non-transitive dynamics)를 만들어내며, 리그 전체가 특정 소수의 강력한 전략에 과적합(overfitting)되는 것을 방지하고, 인간 플레이어들이 수년에 걸쳐 발견해 온 것과 유사한 방식으로 전략적 지평을 넓혀 나간다.8</p>
<p>이러한 정교한 훈련 체계를 뒷받침하기 위해 알파스타는 고도의 신경망 구조를 채택했다. 신경망은 게임 화면의 수많은 유닛 정보를 효율적으로 처리하기 위한 트랜스포머(Transformer) 구조, 시간의 흐름에 따른 장기적인 계획 수립을 위한 LSTM(Long Short-Term Memory) 코어, 그리고 수많은 유닛에 대한 동시적이고 복잡한 명령을 내리기 위한 포인터 네트워크(pointer network)를 포함한 자기회귀적 정책 헤드(auto-regressive policy head)를 결합한 복합적인 형태로 설계되었다.5</p>
<p>알파스타의 성공은 AI가 정적이고 순차적인 턴제 게임의 한계를 넘어, 동적이고 복잡하며 실시간성이 강하게 요구되는 환경에서도 인간 수준의 전략적 사고와 정교한 제어가 가능함을 입증한 중대한 사건이었다. 특히 ’리그’라는 훈련 패러다임은 경쟁적 공진화와 품질 다양성(quality diversity)이라는 진화 연산(evolutionary computation)의 아이디어를 심층 강화학습에 성공적으로 접목시킨 사례로 평가받으며, 이후 다중 에이전트 시스템 및 복잡계 AI 연구에 깊은 영감을 주었다.10</p>
<h3>2.2  뮤제로(MuZero): 규칙 없는 학습의 서막, 모델 기반 강화학습의 새로운 패러다임</h3>
<p>알파스타가 주어진 환경의 복잡성을 극복하는 데 초점을 맞췄다면, 같은 시기 딥마인드에서 개발 중이던 뮤제로(MuZero)는 AI 연구의 더 근본적인 질문에 도전하고 있었다. 바로 ’환경의 규칙을 전혀 모르는 상태에서도 학습하고 계획할 수 있는가?’이다. 2019년 11월 arXiv를 통해 처음 논문이 공개된 뮤제로는 바둑, 체스, 쇼기와 같은 고전적인 보드게임은 물론, 시각적으로 매우 복잡하고 다양한 57개의 아타리(Atari) 게임 전체를 <strong>게임의 규칙에 대한 어떠한 사전 지식도 없이</strong> 학습하여 기존 최고 성능을 뛰어넘는 성과를 달성했다.1</p>
<p>뮤제로의 핵심적인 혁신은 이전 세대의 AI인 알파제로(AlphaZero)와의 비교를 통해 명확히 드러난다. 알파제로는 몬테카를로 트리 탐색(MCTS)을 통해 강력한 수읽기 능력을 보여주었지만, 이 탐색 과정은 게임의 규칙을 완벽하게 아는 ’완벽한 시뮬레이터’가 있다는 전제 하에서만 가능했다. 즉, 특정 상태에서 어떤 수를 둘 수 있는지, 그 수를 두면 다음 상태가 어떻게 변하는지를 정확히 알려주는 외부 모듈에 의존했다.6 이는 규칙이 명확한 보드게임에는 효과적이었지만, 규칙이 복잡하거나 아예 알려지지 않은 현실 세계의 문제에는 적용하기 어려운 근본적인 한계를 지녔다.</p>
<p>뮤제로는 이 한계를 ’스스로 학습한 신경망 모델’로 완벽한 시뮬레이터를 대체함으로써 극복했다.6 놀라운 점은 뮤제로의 모델이 게임 환경 전체를 픽셀 단위까지 완벽하게 모사하려 하지 않는다는 것이다. 이는 모델 기반 강화학습(Model-Based RL)이 오랫동안 겪어온 ‘부정확한 모델 문제’—즉, 복잡한 환경을 완벽하게 모델링하려는 시도가 결국 미세한 오류의 누적으로 인해 장기 예측 실패로 이어지는 문제—를 회피하기 위한 독창적인 해법이었다. 대신, 뮤제로는 오직 ’계획(planning)’을 수립하는 데 가장 직접적으로 관련된 세 가지 핵심적인 정보만을 예측하는 추상적인 모델을 학습한다.13</p>
<ol>
<li>
<p><strong>가치 (Value, <span class="math math-inline">v</span>):</strong> 현재 상태가 얼마나 유리한가?</p>
</li>
<li>
<p><strong>정책 (Policy, <span class="math math-inline">p</span>):</strong> 현재 상태에서 어떤 행동이 가장 유망한가?</p>
</li>
<li>
<p><strong>보상 (Reward, <span class="math math-inline">r</span>):</strong> 직전에 취한 행동이 얼마나 좋았는가?</p>
</li>
</ol>
<p>이 세 가지 요소를 예측하기 위해 뮤제로는 세 개의 핵심 신경망 함수로 구성된다. 첫째, **표현 함수(Representation Function, <span class="math math-inline">h</span>)**는 게임 화면과 같은 원본 관측(observation, <span class="math math-inline">o_t</span>)을 받아, 신경망이 다루기 쉬운 잠재 공간의 내부 상태 표현(hidden state, <span class="math math-inline">s_0</span>)으로 압축하고 인코딩한다 (<span class="math math-inline">s_0 = h(o_1,..., o_t)</span>). 둘째, **동역학 함수(Dynamics Function, <span class="math math-inline">g</span>)**는 현재의 내부 상태(<span class="math math-inline">s_{k-1}</span>)와 앞으로 취할 가상의 행동(<span class="math math-inline">a_k</span>)을 입력받아, 그 행동으로 인해 발생할 다음 내부 상태(<span class="math math-inline">s_k</span>)와 즉각적인 보상(<span class="math math-inline">r_k</span>)을 예측한다 (<span class="math math-inline">r_k, s_k = g(s_{k-1}, a_k)</span>). 셋째, **예측 함수(Prediction Function, <span class="math math-inline">f</span>)**는 특정 내부 상태(<span class="math math-inline">s_k</span>)를 입력받아, 해당 상태의 정책(<span class="math math-inline">p_k</span>, 즉 가능한 행동들의 확률 분포)과 가치(<span class="math math-inline">v_k</span>)를 예측한다 (<span class="math math-inline">p_k, v_k = f(s_k)</span>).15</p>
<p>MCTS 탐색 과정에서, 뮤제로는 실제 게임 환경과 상호작용하는 대신, 이 학습된 동역학 함수(<span class="math math-inline">g</span>)와 예측 함수(<span class="math math-inline">f</span>)를 반복적으로 호출하며 자신의 ‘상상’ 속에서 미래의 여러 경로를 시뮬레이션한다. 즉, <code>s</code> 상태에서 <code>a</code> 행동을 하면 <code>g</code> 함수를 통해 다음 상태 <code>s'</code>와 보상 <code>r</code>을 얻고, 다시 <code>f</code> 함수를 통해 <code>s'</code>에서의 정책과 가치를 예측하는 과정을 트리 형태로 확장해 나가는 것이다. 이 모든 탐색 과정은 실제 환경과의 상호작용 없이, 오직 학습된 모델 내부에서만 효율적으로 이루어진다.13</p>
<p>뮤제로의 등장은 모델 기반 강화학습 분야에 새로운 패러다임을 제시했다. 환경 전체를 정교하게 복제하려는 시도에서 벗어나, 계획에 필수적인 핵심 요소만을 추상적으로 학습하고 예측하는 접근법이 더 효과적일 수 있음을 증명했기 때문이다. 이는 AI가 명시적인 규칙이나 완벽한 시뮬레이터가 존재하지 않는, 훨씬 더 광범위하고 불분명한 현실 세계의 문제들—예를 들어 로보틱스 제어, 산업 공정 최적화, 단백질 접힘 구조 예측 등—에 강화학습을 적용할 수 있는 중요한 이론적, 실용적 토대를 마련한 혁신적인 성과로 평가된다.13</p>
<h2>3.  불완전 정보 게임의 정복: 페이스북 AI의 플러리버스</h2>
<p>딥마인드가 실시간 전략 게임과 규칙 없는 학습이라는 새로운 영역을 개척하는 동안, 페이스북 AI 리서치(FAIR, 현 Meta AI)는 카네기 멜런 대학(CMU)과의 협력을 통해 또 다른 난공불락의 요새로 여겨졌던 다자간 불완전 정보 게임에 도전하고 있었다. 그 결과물이 바로 2019년 7월 11일에 공개된 포커 AI, 플러리버스(Pluribus)다. 플러리버스는 세계에서 가장 인기 있는 포커 종목인 6인 무제한 텍사스 홀덤(six-player no-limit Texas Hold’em)에서 대런 엘리아스, 크리스 ‘지저스’ 퍼거슨 등 세계 최상급 프로 선수들을 상대로 ’결정적인 승리(decisive margin of victory)’를 거두었다고 발표했다.2 이 성과는 AI가 단순히 계산적 복잡성을 넘어, 블러핑(bluffing)과 기만(deception)이 난무하는, 즉 숨겨진 정보와 다수의 상호작용이 핵심인 환경에서 인간의 전략적 사고를 능가할 수 있음을 보여준 최초의 사례였다.2</p>
<p>플러리버스가 해결해야 했던 문제는 이전의 AI들이 마주했던 것과는 근본적으로 달랐다. 체스나 바둑 같은 2인 제로섬 완전 정보 게임에서는 내시 균형(Nash Equilibrium) 전략을 근사하는 것이 이론적으로 최적의 해법으로 알려져 있다. 내시 균형 전략을 따르는 플레이어는 상대방이 어떤 전략을 사용하더라도 최소한 지지 않는 결과를 보장받을 수 있다.3 그러나 플레이어가 3명 이상이 되는 순간, 이 이론은 더 이상 유효하지 않다. 다자간 게임에서는 한 플레이어의 이득이 반드시 다른 플레이어의 손실로 직결되지 않으며, 플레이어 간의 연합이나 배신이 가능해지기 때문에 내시 균형을 따르는 것이 오히려 패배로 이어질 수 있다.2 따라서 플러리버스는 강력한 이론적 보장을 포기하고, 대신 경험적으로 인간 전문가를 꾸준히 이길 수 있는 실용적인 전략을 탐색하는 새로운 접근법을 채택해야 했다.</p>
<p>플러리버스의 핵심 전략은 크게 두 부분으로 구성된다: 오프라인에서 막대한 계산을 통해 사전 학습된 ’청사진 전략(blueprint strategy)’과, 실제 게임 중에 제한된 시간 안에 더 나은 수를 탐색하는 ’실시간 탐색(real-time search)’의 결합이다.2</p>
<p>첫째, 청사진 전략은 플러리버스의 AI 복사본 6개가 서로 수없이 대결하는 ’자기 대국(self-play)’을 통해 생성된다. 이 과정에서는 인간의 게임 데이터가 전혀 사용되지 않으며, AI는 무작위 플레이에서 시작하여 점차적으로 더 나은 결과를 가져오는 행동을 학습한다.19 이때 사용된 핵심 알고리즘은 몬테카를로 유감 최소화(Monte Carlo Counterfactual Regret Minimization, MCCFR)의 변형이다. MCCFR은 매 반복마다 각 게임 상황에서 자신이 선택하지 않았던 다른 행동을 했다면 결과가 얼마나 더 좋았거나 나빴을지를 평가하는 ’가상 유감(counterfactual regret)’을 계산한다. 그리고 이 유감 값에 비례하여 미래에 해당 행동을 선택할 확률을 조정하는 방식으로 전략을 점진적으로 개선해 나간다.21 포커의 방대한 경우의 수를 다루기 위해, 플러리버스는 유사한 게임 상황(예: 비슷한 핸드와 보드 카드 조합)을 하나의 ’버킷(bucket)’으로 묶어 동일하게 취급하는 ‘추상화(abstraction)’ 기법을 적용하여 계산 복잡성을 효과적으로 줄였다.21</p>
<p>둘째, 이렇게 학습된 청사진 전략은 게임의 모든 상황을 완벽하게 다루기에는 너무 방대하고 거칠기 때문에, 실제 게임 플레이 시에는 현재 주어진 특정 상황에 맞춰 더 정교한 판단을 내리기 위한 실시간 탐색을 수행한다. 여기서 플러리버스는 기존 AI와 차별화되는 독창적인 탐색 방식을 사용한다. 일반적인 탐색 알고리즘이 탐색의 종단(leaf nodes)에서 상대방이 고정된 하나의 최적 전략을 사용할 것이라고 가정하는 반면, 플러리버스는 상대방이 여러 가능한 ‘지속 전략(continuation strategies)’—예를 들어, 기존의 청사진 전략, 폴드(포기)에 편향된 전략, 콜(따라가기)에 편향된 전략 등—중 하나를 선택할 수 있다고 가정하고 탐색을 수행한다. 이는 상대방의 잠재적인 전략 변화와 비합리적인 플레이에 더 강건하게 대응할 수 있게 만들어주며, 다자간 게임의 불확실성을 효과적으로 처리하는 핵심 기제가 되었다.21</p>
<p>이러한 기술적 접근의 결과로, 플러리버스는 인간과는 다른 독특하면서도 매우 효과적인 플레이 스타일을 선보였다. 가장 대표적인 것이 ’동크 베팅(Donk Betting)’의 적극적인 활용이다. 동크 베팅은 이전 베팅 라운드에서 상대의 베팅에 콜만 하고 수동적으로 따라가다가, 다음 라운드가 시작되자마자 먼저 공격적으로 베팅하는 행위를 말한다. 대부분의 인간 프로 선수들은 이를 정보 누출의 위험이 크고 전략적으로 미숙한 플레이로 간주하여 거의 사용하지 않는다.2 그러나 플러리버스는 계산을 통해 이 전략이 특정 상황에서 상대방을 교란하고 이득을 극대화하는 데 효과적임을 발견하고, 인간보다 훨씬 더 높은 빈도로 구사했다. 이는 인간 전문가 집단이 오랜 경험을 통해 구축해 온 ’정석’이나 ’직관’이 국소 최적해(local optimum)에 머물러 있었을 가능성을 시사한다. AI는 인간의 편견 없이 오직 수학적 기대값만을 극대화하는 과정에서, 인간이 비합리적이라고 치부했던 전략의 숨겨진 가치를 발견한 것이다. 프로 선수들은 플러리버스의 예측 불가능한 혼합 전략(mixed strategies) 구사 능력과 인간의 상식을 벗어나는 플레이에 대해 “어떤 종류의 핸드를 가졌는지 파악하기가 정말 어렵다“고 평가하며, AI와의 대결을 통해 “내 게임에 통합할 새로운 것을 배운다“고 인정했다.2</p>
<p>더욱 놀라운 점은 플러리버스의 압도적인 성능이 엄청난 계산 자원을 통해서만 달성된 것이 아니라는 사실이다. 플러리버스의 청사진 전략은 GPU 없이 64코어 서버에서 단 8일 만에 훈련되었으며, 클라우드 컴퓨팅 비용으로 환산할 경우 150달러 미만에 불과했다.19 이는 수백만 달러 이상의 훈련 비용이 소요된 알파고나 다른 대규모 AI 모델들과 극명한 대조를 이루며, 정교한 알고리즘 혁신이 막대한 계산 자원의 한계를 뛰어넘을 수 있음을 보여준 사례로 남았다.</p>
<h3>3.1 &lt;표 1&gt; 2019년 7월 주요 게임 AI 모델 비교 분석</h3>
<table><thead><tr><th>특징</th><th>알파스타 (AlphaStar)</th><th>뮤제로 (MuZero)</th><th>플러리버스 (Pluribus)</th></tr></thead><tbody>
<tr><td><strong>도전 과제</strong></td><td>실시간 전략 게임 (StarCraft II)<br>- 불완전 정보<br>- 장기 계획<br>- 거대하고 연속적인 액션 공간</td><td>규칙을 모르는 게임 마스터<br>- 바둑, 체스, 쇼기, 아타리<br>- 환경 동역학의 내재적 학습</td><td>다자간 불완전 정보 게임 (6인 포커)<br>- 블러핑, 기만 등 심리전<br>- 내시 균형 적용 불가</td></tr>
<tr><td><strong>핵심 방법론</strong></td><td>다중 에이전트 강화학습 리그<br>(Multi-Agent RL League)</td><td>예측을 통한 탐색<br>(Search with a Learned Model)</td><td>청사진 전략 + 실시간 탐색<br>(Blueprint Strategy + Real-time Search)</td></tr>
<tr><td><strong>학습 방식</strong></td><td>모방 학습 + 자가 대국 리그<br>(Imitation Learning + Self-play League)</td><td>순수 자가 대국<br>(Pure Self-play)</td><td>순수 자가 대국 (MCCFR 알고리즘)<br>(Pure Self-play with MCCFR)</td></tr>
<tr><td><strong>환경 정보</strong></td><td>게임 규칙/시뮬레이터 제공</td><td>게임 규칙/시뮬레이터 <strong>미제공</strong></td><td>게임 규칙/시뮬레이터 제공</td></tr>
<tr><td><strong>기술적 의의</strong></td><td>복잡한 실시간 환경에서의<br>초인적 전략 수행 능력 입증</td><td>환경 모델을 스스로 학습하여<br>계획하는 범용 알고리즘의 가능성 제시</td><td>다자간 불완전 정보 환경에서의<br>초인적 전략 및 기만 능력 입증</td></tr>
</tbody></table>
<h2>4.  거대 언어 모델의 부상과 전략적 제휴: OpenAI와 마이크로소프트</h2>
<p>2019년 7월, 게임 AI 분야에서 강화학습의 눈부신 발전이 이루어지는 동안, 자연어 처리(NLP) 분야에서는 또 다른 거대한 변화의 물결이 일고 있었다. 이는 거대 언어 모델(Large Language Models, LLMs)의 부상과, 그 잠재력을 실현하기 위한 대규모 산업적 재편의 시작을 알리는 신호탄이었다. 이 변화의 중심에는 OpenAI와 마이크로소프트의 역사적인 파트너십이 있었다.</p>
<p>2019년 2월, OpenAI는 이전 모델인 GPT를 계승한 GPT-2를 발표하며 학계와 산업계에 큰 충격을 주었다.25 GPT-2는 40GB에 달하는 방대한 인터넷 텍스트(Reddit에서 3 카르마 이상을 받은 고품질 링크로부터 수집한 WebText 데이터셋)를 기반으로, 단순히 다음 단어를 예측하도록 훈련된 거대한 트랜스포머(Transformer) 모델이었다.25 그럼에도 불구하고, GPT-2는 별도의 과제별 미세조정(fine-tuning) 없이도 질의응답, 텍스트 요약, 번역 등 다양한 NLP 과제를 제로샷(zero-shot) 설정에서 준수한 성능으로 수행하는 능력을 보여주었다. 이는 LLM이 특정 작업에 대한 명시적인 지도 학습 없이도, 대규모 비지도 학습 과정에서 언어에 대한 일반적인 이해와 추론 능력을 내재화할 수 있음을 입증한 것이었다. OpenAI는 이를 ’비지도 멀티태스크 학습자(Unsupervised Multitask Learners)’의 가능성으로 명명했다.26</p>
<p>그러나 GPT-2의 놀라운 텍스트 생성 능력은 동시에 심각한 윤리적 우려를 낳았다. 이 기술이 악의적으로 사용될 경우, 매우 설득력 있는 가짜 뉴스 기사, 자동화된 소셜 미디어 선동 콘텐츠, 정교한 스팸 및 피싱 이메일 등을 대량으로 생성할 수 있었기 때문이다.25 이러한 잠재적 위험성을 심각하게 인지한 OpenAI는 전례 없는 결정을 내렸다. 가장 강력한 15억 파라미터 버전의 모델을 즉시 공개하지 않고, 사회가 이 기술의 영향력을 평가하고 대응할 시간을 벌 수 있도록 더 작은 크기의 모델부터 점진적으로 공개하는 ‘단계적 공개(Staged Release)’ 전략을 채택한 것이다.25 2019년 7월 즈음에는 3억 4500만 파라미터 버전의 모델이 공개된 상태였으며, 이 전략은 AI 기술의 책임감 있는 개발 및 배포에 대한 중요한 선례를 남겼다.</p>
<p>바로 이 시점인 2019년 7월 22일, AI 산업의 판도를 바꿀 중대한 발표가 있었다. 마이크로소프트가 OpenAI에 10억 달러를 투자하고, 인공 일반 지능(AGI) 구축을 목표로 다년간의 독점적 파트너십을 체결한 것이다.27 이 파트너십은 단순한 재정적 투자를 넘어, AI 연구 개발의 패러다임 자체를 바꾸는 결정적 계기가 되었다. LLM의 성능이 모델의 파라미터 수와 훈련 데이터의 규모에 따라 극적으로 향상된다는 ’스케일링 법칙(scaling laws)’이 점차 명확해지면서, 막대한 규모의 컴퓨팅 자원이 AI 연구의 가장 핵심적인 경쟁력으로 부상했다. 플러리버스가 150달러 미만의 비용으로 ’알고리즘의 혁신’을 통해 문제를 해결한 것과 극명한 대조를 이루며, AGI라는 거대한 목표를 위해서는 ’규모의 경제’가 필수적이라는 현실을 보여준 것이다.21</p>
<p>이 파트너십의 구조는 상호 보완적이었다. 마이크로소프트는 OpenAI의 독점적인 클라우드 제공자로서 자사의 Azure 슈퍼컴퓨팅 인프라를 제공하고, OpenAI는 이 막대한 컴퓨팅 파워를 활용하여 AGI 연구를 가속화한다. 그 대가로 마이크로소프트는 OpenAI가 개발한 기술을 자사 제품에 통합하고 상업화할 수 있는 우선권을 확보했다.28 이 결합은 ’거대 기술 기업(Big Tech)의 자본 및 인프라’와 ’선도적인 AI 연구소의 알고리즘 및 인재’가 결합하는 새로운 협력 모델의 탄생을 의미했다.</p>
<p>이 전략적 제휴는 AI 산업 생태계에 깊고 지속적인 영향을 미쳤다. 이는 AI 연구의 무게 중심이 순수한 학술적 탐구에서 막대한 자본과 인프라가 요구되는 산업적 경쟁의 영역으로 급격히 이동하고 있음을 상징하는 사건이었다. OpenAI와 마이크로소프트의 연합은 이후 구글과 딥마인드의 통합 가속화, 아마존과 앤트로픽(Anthropic)의 파트너십 등 유사한 형태의 합종연횡을 촉발하며, 소수의 거대 기업이 AI 기술 발전을 주도하는 오늘날의 시장 구조를 공고히 하는 데 결정적인 역할을 했다. 2019년 7월의 이 파트너십 발표는, 향후 몇 년 안에 등장할 GPT-3, ChatGPT와 같은 초거대 AI 모델의 시대를 예고하는 서막이었으며, AI 기술이 사회 전반에 미칠 파급력에 대한 기대와 우려를 동시에 증폭시키는 계기가 되었다.</p>
<h2>5.  극한 환경 극복을 위한 로보틱스: 펜실베이니아 대학의 TRUSSES 프로젝트</h2>
<p>가상 세계에서 AI가 인간의 지능을 뛰어넘는 경이로운 성과를 보이는 동안, 물리적 세계의 로봇들은 여전히 예측 불가능한 환경과의 사투를 벌이고 있었다. 2019년 7월, 펜실베이니아 공과대학(Penn Engineering)의 GRASP 연구소가 주도하고 NASA의 지원을 받는 TRUSSES 프로젝트는 이러한 현실 세계의 난관을 극복하기 위한 로보틱스 연구의 현주소를 명확히 보여주었다.29 TRUSSES는 ’Temporarily, Robots Unite to Surmount Sandy Entrapments, then Separate’의 약자로, 그 이름처럼 로봇들이 일시적으로 결합하여 단독으로는 극복할 수 없는 지형적 장애물을 함께 돌파하는 것을 목표로 한다.31</p>
<p>이 프로젝트의 궁극적인 비전은 달이나 화성과 같이 인간이 직접 탐사하기 어려운 극한 환경에 로봇 팀을 보내는 것이다. 현재의 화성 탐사 로버들은 전복이나 고립의 위험 때문에 5도 이상의 경사면은 아예 회피하도록 설계되어 있어 탐사 영역에 큰 제약을 받는다.29 TRUSSES 프로젝트는 이러한 한계를 ’협력’을 통해 극복하고자 했다. 2019년 7월, 연구팀은 달과 화성의 모래 지형을 모사한 뉴멕시코의 화이트샌즈 국립공원에서 세계 최초로 이종(heterogeneous) 로봇 팀의 물리적 협력 시연을 성공적으로 수행했다.29</p>
<p>TRUSSES의 핵심 기술은 크게 두 가지로 나눌 수 있다: ’물리적 협력’과 ’실시간 지형 분석’이다.</p>
<p>첫째, ’물리적 협력’은 서로 다른 장점을 가진 로봇들이 필요에 따라 물리적으로 결합하여 하나의 강건한 시스템을 형성하는 개념이다. 예를 들어, 연구팀이 사용한 바퀴형 로버는 큰 탑재량을 실을 수 있고 안정적이지만 모래 지형에서는 바퀴가 헛돌아 견인력을 잃기 쉽다. 반면, 개와 같이 네 다리로 걷는 다리형 로봇(Spirit)은 모래를 파고들며 나아갈 수 있어 기동성이 좋지만 쉽게 균형을 잃고 넘어질 수 있다. TRUSSES는 이 두 로봇을 케이블로 연결하여 서로를 끌어주고 지지하게 함으로써 각자의 약점을 보완하고 강점을 극대화했다.29 화이트샌즈에서 진행된 실제 필드 테스트에서, 15도의 가파른 모래 언덕을 단독으로 오르려던 각 로봇은 바퀴가 헛돌고 다리가 미끄러지며 모두 실패했다. 그러나 두 로봇을 연결하자, 서로를 의지하며 조금씩 전진하여 최종적으로 약 2.5미터를 함께 오르는 데 성공했다. 이는 다중 로봇의 물리적 협력이 실험실 환경을 넘어 실제 예측 불가능한 환경에서도 유효함을 입증한 매우 중요한 성과였다.29</p>
<p>둘째, ’실시간 지형 분석’은 로봇이 환경과의 상호작용 자체를 학습의 기회로 삼는 기술이다. TRUSSES 프로젝트는 로봇이 미끄러지거나 비틀거리는 것을 제어 실패나 ’노이즈’로 간주하지 않고, 지형의 특성을 파악할 수 있는 귀중한 ’정보’로 활용한다.29 로봇에 장착된 센서는 움직이는 동안 지면으로부터 받는 힘, 저항, 미끄러짐 정도 등의 데이터를 지속적으로 수집한다. 이 데이터는 실시간으로 처리되어 로봇이 지나온 경로와 주변 지형의 위험도를 나타내는 지도를 생성하는 데 사용된다. 이 ’위험 지도’는 로봇 팀이 잠재적인 위험(예: 깊은 모래, 미끄러운 암석)을 사전에 인지하고 안전한 경로를 계획하도록 돕는다.29 이는 ’실패로부터의 학습’과 ’협력을 통한 한계 극복’이라는, 강인한(robust) 로봇 시스템을 설계하는 데 있어 핵심적인 원칙을 실증적으로 보여준다.</p>
<p>그러나 화이트샌즈에서의 현장 테스트는 물리 세계 로보틱스 연구의 고질적인 어려움 또한 여실히 드러냈다. 작열하는 사막의 열기는 로봇의 배터리를 빠르게 소진시키고 과열로 인한 고장을 유발했으며, 실험실의 정밀한 모션 캡처 시스템과 달리 오차가 큰 GPS에만 의존해야 하는 상황은 로봇의 정확한 위치 파악과 협력 제어를 어렵게 만들었다.29 연구팀은 자동차 앞 유리 가리개를 이용해 로봇의 그늘을 만들어주는 등 즉석에서 문제를 해결해야 했다. 이러한 경험은 소프트웨어 기반 AI의 발전 속도와 하드웨어 및 물리적 상호작용에 기반한 로보틱스의 발전 속도 사이에 존재하는 격차를 시사한다. 가상 세계에서는 수백만 번의 시뮬레이션을 순식간에 마칠 수 있지만, 현실 세계에서는 단 한 번의 실험에도 수많은 예상치 못한 변수와 물리적 제약이 따르기 때문이다. TRUSSES 프로젝트는 이러한 어려움에도 불구하고, AI와 로보틱스가 궁극적으로 나아가야 할 방향이 바로 이 예측 불가능한 물리적 세계의 문제들을 해결하는 것임을 명확히 보여주었다.</p>
<h2>6.  2019년 7월 주요 학술 연구 동향 분석 (arXiv 기반)</h2>
<p>2019년 7월을 전후하여 학술 논문 사전 공개 사이트인 arXiv에는 AI 분야의 기술적 흐름을 주도하고 미래 연구 방향에 큰 영향을 미친 중요한 논문들이 다수 발표되었다. 이 시기의 논문들은 특정 분야의 성능을 극한으로 끌어올리는 연구와 더불어, AI 기술의 효율성, 신뢰성, 그리고 사회적 영향에 대한 깊은 성찰을 담고 있었다. 본 장에서는 당시 학계의 핵심적인 연구 트렌드를 대표하는 세 가지 주요 논문을 심층적으로 분석한다.</p>
<h3>6.1  EfficientNet: 복합 스케일링(Compound Scaling) 방법론의 기술적 분석</h3>
<p>2019년 ICML(International Conference on Machine Learning)에서 발표된 Mingxing Tan과 Quoc V. Le의 논문 “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks” (arXiv:1905.11946)는 컴퓨터 비전 분야, 특히 컨볼루션 신경망(CNN)의 설계 방식에 대한 기존의 통념을 뒤흔들었다.32 당시 딥러닝 모델의 성능을 향상시키는 가장 일반적인 방법은 네트워크의 깊이(depth, 층의 수)를 늘리거나, 너비(width, 채널의 수)를 확장하거나, 입력 이미지의 해상도(resolution)를 높이는 것 중 한 가지 차원에 집중하는 것이었다.</p>
<p>EfficientNet의 저자들은 이러한 단일 차원 스케일링이 어느 수준을 넘어서면 성능 향상이 빠르게 둔화되는 한계가 있음을 실험적으로 보였다. 그리고 이 세 가지 차원—깊이, 너비, 해상도—이 서로 독립적이지 않으며, 최적의 성능과 효율을 달성하기 위해서는 이들을 <strong>균형 있게 함께 확장</strong>해야 한다는 핵심적인 원리를 발견했다. 예를 들어, 입력 이미지의 해상도가 높아지면 더 넓은 영역의 특징을 포착하기 위해 네트워크의 수용장(receptive field)이 커져야 하므로 깊이가 깊어져야 하고, 동시에 더 세밀한 패턴을 학습하기 위해 더 많은 채널, 즉 너비가 넓어져야 한다는 것이다.32</p>
<p>이러한 관찰을 바탕으로, 저자들은 ’복합 스케일링(Compound Scaling)’이라는 체계적이고 효율적인 모델 확장 방법론을 제안했다. 이 방법은 사용자가 조절할 수 있는 단일 복합 계수(<span class="math math-inline">\phi</span>)를 사용하여 세 가지 차원을 다음과 같은 일정한 비율로 동시에 확장한다 32:</p>
<p><span class="math math-display">
\text{depth}: d = \alpha^\phi \\
\text{width}: w = \beta^\phi \\
\text{resolution}: r = \gamma^\phi \\
\text{s.t. } \alpha \cdot \beta^2 \cdot \gamma^2 \approx 2
</span><br />
여기서 <span class="math math-inline">\alpha, \beta, \gamma</span>는 작은 베이스라인 모델에 대한 그리드 탐색(grid search)을 통해 경험적으로 찾아낸 상수 계수들이다. 제약 조건인 <span class="math math-inline">\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2</span>는 복합 계수 <span class="math math-inline">\phi</span>를 1만큼 증가시킬 때마다 모델의 전체 연산량(FLOPS)이 대략 <span class="math math-inline">2^\phi</span>배 증가하도록 설계된 것이다.</p>
<p>이 복합 스케일링 원칙에 따라 설계된 EfficientNet 모델군은 당시 ImageNet 분류 정확도에서 최고 성능을 기록했던 기존의 다른 CNN 모델들보다 훨씬 적은 수의 파라미터와 연산량을 가지면서도 더 높은 정확도를 달성했다. 이는 단순히 더 크고 깊은 모델을 만드는 것만이 능사가 아니며, 제한된 컴퓨팅 자원 내에서 최적의 성능을 끌어내기 위해서는 모델 아키텍처의 각 요소를 체계적으로 고려하는 ’효율성’이 매우 중요하다는 점을 학계와 산업계에 각인시켰다. EfficientNet의 등장은 이후 모바일 기기나 엣지 디바이스와 같이 자원이 제한된 환경에서의 딥러닝 모델 배포 연구에 큰 영향을 미쳤으며, 모델 설계에 있어 효율성을 핵심적인 평가 기준으로 삼는 새로운 연구 흐름을 만들어냈다.</p>
<h3>6.2  NLP 모델의 사회적 비용: 에너지 소비와 정책적 함의</h3>
<p>AI 모델의 성능이 기하급수적으로 향상되는 이면에는 막대한 계산 자원과 에너지 소비라는 그림자가 존재했다. 2019년 ACL(Association for Computational Linguistics)에서 발표된 Emma Strubell 등의 논문 “Energy and Policy Considerations for Deep Learning in NLP” (arXiv:1906.02243)는 이러한 문제를 정면으로 다루며 AI 연구 커뮤니티에 큰 경종을 울렸다.33 이 논문은 이전까지 주로 모델의 정확도나 성능 지표에만 집중했던 연구 문화에 제동을 걸고, 대규모 NLP 모델을 훈련하는 데 수반되는 재정적, 환경적 비용을 구체적인 수치로 정량화하여 제시했다.</p>
<p>연구팀은 BERT, GPT-2, Transformer 등 당시 최신 NLP 모델들의 훈련 과정에서 소비되는 전력량을 측정하고, 이를 미국 환경보호청(EPA)의 데이터를 기반으로 이산화탄소(CO2) 배출량으로 환산했다. 분석 결과는 충격적이었다. 예를 들어, 신경망 아키텍처 탐색(Neural Architecture Search)을 포함하여 하나의 대형 트랜스포머 모델을 처음부터 훈련하는 과정에서 배출되는 CO2의 양은 약 626,155 파운드에 달하는 것으로 추정되었다. 이는 뉴욕과 샌프란시스코를 오가는 항공편 한 편이 배출하는 양의 약 300배, 자동차 한 대가 수명 주기 동안 배출하는 총량의 5배에 해당하는 엄청난 양이다.33</p>
<p>이 논문은 AI 연구의 사회적, 윤리적 책임에 대한 중요한 논의를 촉발시켰다. 첫째, 막대한 훈련 비용은 AI 연구의 진입 장벽을 높여, 충분한 재정적 지원을 받는 소수의 대기업 연구소나 명문 대학만이 최신 연구를 수행할 수 있는 ‘부익부 빈익빈’ 현상을 심화시킨다는 점을 지적했다.33 이는 연구의 다양성을 저해하고 학문적 불평등을 야기할 수 있다. 둘째, AI 기술 발전이 기후 변화라는 전 지구적 위기에 미치는 영향을 환기시켰다. AI가 사회에 긍정적인 기여를 하기 위해서는 그 개발 과정 자체가 지속 가능해야 한다는 문제의식을 확산시켰다.34</p>
<p>이 연구의 영향으로 ’Green AI’라는 개념이 학계의 주요 화두로 떠올랐다. 이는 모델의 성능(accuracy)뿐만 아니라, 계산 효율성(efficiency)과 환경적 지속 가능성을 함께 고려하여 모델을 평가하고 개발해야 한다는 새로운 연구 패러다임이다.36 이후 AI 학회들에서는 논문 제출 시 모델의 훈련 시간, 사용된 하드웨어, 총 에너지 소비량 등을 명시하도록 권장하는 움직임이 나타났다. 또한, 이 논문은 모델 경량화(model compression), 지식 증류(knowledge distillation), 효율적인 하이퍼파라미터 탐색, 그리고 모델 추론(inference) 단계에서의 에너지 소비 최적화 등 새로운 연구 분야를 활성화하는 중요한 계기가 되었다.37</p>
<h3>6.3  강화학습의 일반화 문제: CoinRun 벤치마크의 의의</h3>
<p>강화학습(RL) 분야에서는 에이전트가 특정 게임이나 시뮬레이션 환경에서 높은 점수를 기록하는 놀라운 성과들이 연이어 발표되고 있었다. 그러나 Karl Cobbe 등이 발표한 논문 “Quantifying Generalization in Reinforcement Learning” (arXiv:1812.02341, 2019년 7월에 v3 업데이트)은 이러한 성과에 내재된 근본적인 문제를 제기했다.40 바로 ’일반화(generalization)’의 문제였다.</p>
<p>논문의 핵심 문제 제기는 간단명료했다. 기존의 대부분 RL 벤치마크 환경(예: 아타리 게임, MuJoCo)은 훈련(training)과 테스트(testing)에 동일한 환경을 사용한다. 이 경우, 에이전트는 특정 환경의 미세한 패턴이나 우연한 특징에 과적합(overfitting)될 수 있다. 즉, 훈련 환경에서는 초인적인 성능을 보이지만, 조금이라도 다른 새로운 환경에 놓이면 성능이 급격히 저하될 수 있다는 것이다. 이는 에이전트가 진정한 의미의 ’전략’이나 ’기술’을 학습한 것이 아니라, 단순히 훈련 데이터를 ’암기’한 것일 수 있음을 의미한다.40</p>
<p>이러한 문제를 해결하고 RL 에이전트의 일반화 성능을 정량적으로 측정하기 위해, 저자들은 ’CoinRun’이라는 새로운 벤치마크 환경을 제안했다.40 CoinRun은 간단한 2D 플랫폼 게임으로, 에이전트가 장애물을 피해 레벨 끝에 있는 동전을 획득하는 것이 목표다. CoinRun의 가장 큰 특징은 ’절차적 생성(procedural generation)’을 통해 무한히 많은 수의 독특하고 새로운 레벨을 만들어낼 수 있다는 점이다. 이를 통해 연구자들은 훈련에 사용된 레벨과 테스트에 사용될 레벨을 명확하게 분리할 수 있다. 에이전트는 한 번도 본 적 없는 새로운 레벨에서 얼마나 잘 수행하는지, 즉 ‘제로샷 일반화(zero-shot generalization)’ 성능을 통해 평가받게 된다.40</p>
<p>CoinRun을 이용한 실험에서, 저자들은 당시 최고 수준의 RL 알고리즘들이 생각보다 훨씬 쉽게 훈련 데이터에 과적합된다는 사실을 보여주었다. 수천 개의 다양한 레벨로 훈련했음에도 불구하고, 훈련 세트에서의 성능과 테스트 세트에서의 성능 사이에 상당한 격차가 발생했다. 이 연구는 강화학습 커뮤니티에 ’일반화’라는, 지도 학습에서는 이미 중요하게 다루어지고 있었지만 RL에서는 상대적으로 간과되었던 평가 축을 제시했다는 점에서 큰 의의를 가진다. 단순히 특정 작업에서의 최고 점수를 경신하는 것을 넘어, 학습된 정책이 얼마나 강건하고(robust) 다양한 상황에 적용될 수 있는지를 평가하는 연구의 중요성을 부각시켰으며, 이후 절차적 생성 환경을 활용한 일반화 연구를 촉진하는 계기가 되었다.</p>
<h2>7. 결론: 2019년 7월이 AI 역사에 남긴 유산과 미래 전망</h2>
<p>2019년 7월은 인공지능이 복잡성, 불확실성, 그리고 자율성이라는 측면에서 뚜렷한 질적 도약을 이룬 시기로 평가될 수 있다. 이 시기에 발표된 연구들은 각기 다른 영역에서 AI의 한계를 확장하며, 이후 수년간의 기술 발전 방향을 예고하는 중요한 이정표를 세웠다.</p>
<p>강화학습 분야에서는 게임의 ’규칙’을 넘어서는 학습 능력이 증명되었다. 알파스타는 실시간성과 방대한 전략적 깊이를 가진 스타크래프트 II를 정복함으로써 동적 환경에서의 초인적 제어 가능성을 보여주었고, 플러리버스는 다자간 불완전 정보 환경인 포커에서 인간의 심리적 기만술까지 뛰어넘는 전략을 구사했다. 가장 근본적인 도약을 이룬 뮤제로는 환경의 규칙 자체를 내재적으로 학습하여 계획을 수립함으로써, AI가 미지의 환경을 스스로 탐험하고 이해할 수 있는 범용 알고리즘의 서막을 열었다.</p>
<p>자연어 처리 분야에서는 거대 언어 모델의 폭발적인 잠재력과 그에 수반되는 사회적 책임에 대한 논의가 본격화되었다. OpenAI의 GPT-2와 마이크로소프트의 전략적 제휴는 AI 연구가 막대한 자본과 컴퓨팅 인프라를 필요로 하는 ’규모의 경쟁’으로 진입했음을 알리는 신호탄이었다. 동시에, 대규모 모델 훈련에 따르는 막대한 에너지 소비와 환경적 영향에 대한 비판적 성찰은 ’지속 가능한 AI(Green AI)’라는 새로운 연구 패러다임을 탄생시켰다.</p>
<p>로보틱스 분야에서는 물리적 세계의 예측 불가능한 한계를 극복하기 위한 ’협력’의 중요성이 부각되었다. 펜실베이니아 대학의 TRUSSES 프로젝트는 이종 로봇들이 물리적으로 결합하고 환경과의 상호작용을 통해 실시간으로 위험을 학습하며 험지를 돌파하는 모습을 보여주었다. 이는 가상 세계의 AI와 현실 세계의 로봇이 마주한 도전의 본질적 차이, 그리고 그 간극을 메우기 위한 강건한(robust) 시스템 설계의 방향성을 제시했다.</p>
<p>흥미롭게도, 이 시기의 연구들은 서로 다른 분야에서 출발했지만, ’추상화된 표현 학습’과 ’불확실성 하에서의 강건한 의사결정’이라는 공통된 목표를 향하고 있었다. 뮤제로가 게임의 동역학을 핵심적인 가치, 정책, 보상으로 추상화하고, 플러리버스가 상대방의 수많은 가능성을 몇 가지 대표적인 전략으로 추상화하여 대응하며, TRUSSES가 복잡한 지형과의 상호작용을 통해 ’위험도’라는 개념을 추상화하고, GPT-2가 방대한 텍스트 속에서 문법과 의미의 규칙을 스스로 추상화한 것이 그 대표적인 예다.</p>
<p>2019년 7월에 나타난 이러한 기술적 흐름들은 이후 AI 기술의 발전을 정확히 예고했다. 뮤제로의 아이디어는 더욱 범용적인 의사결정 AI로, OpenAI와 마이크로소프트의 파트너십은 전 세계를 뒤흔든 초거대 AI의 시대로, NLP의 에너지 문제 제기는 AI의 지속 가능성 연구로, TRUSSES의 협력 로봇 개념은 군집 로보틱스 및 자율 시스템의 발전으로 직접 이어졌다.</p>
<p>결론적으로, 2019년 7월은 AI가 순수한 기술적 성취를 넘어 산업, 경제, 환경, 윤리 등 사회 전반에 걸쳐 막대한 영향을 미치기 시작한 본격적인 서막으로 기록될 것이다. 이 시점을 계기로 AI의 발전은 단순히 ’무엇을 할 수 있는가(capability)’의 문제를 넘어, ’무엇을, 그리고 어떻게 해야 하는가(responsibility and methodology)’에 대한 깊은 성찰을 요구하게 되었다.41 이는 오늘날 우리가 마주한 AI 시대의 근본적인 질문들이 태동하던 순간이었다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Google DeepMind - Wikipedia, https://en.wikipedia.org/wiki/Google_DeepMind</li>
<li>Pluribus (poker bot) - Wikipedia, https://en.wikipedia.org/wiki/Pluribus_(poker_bot)</li>
<li>Carnegie Mellon and Facebook AI Beats Professionals in Six-Player …, https://www.cmu.edu/news/stories/archives/2019/july/cmu-facebook-ai-beats-poker-pros.html</li>
<li>AlphaStar (software) - Wikipedia, https://en.wikipedia.org/wiki/AlphaStar_(software)</li>
<li>Creating AlphaStar: The Start of the AI Revolution? - DataScienceCentral.com, https://www.datasciencecentral.com/creating-alphastar-the-start-of-the-ai-revolution/</li>
<li>MuZero - Wikipedia, https://en.wikipedia.org/wiki/MuZero</li>
<li>[1911.08265] Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model, https://arxiv.org/abs/1911.08265</li>
<li>AlphaStar: Mastering the real-time strategy game StarCraft II …, https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/</li>
<li>AlphaStar: Grandmaster level in StarCraft II using multi-agent reinforcement learning, https://deepmind.google/discover/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning/</li>
<li>AlphaStar: An Evolutionary Computation Perspective - arXiv, https://arxiv.org/pdf/1902.01724</li>
<li>Google’s MuZero chess AI reached superhuman performance without even knowing the rules - ZME Science, https://www.zmescience.com/science/googles-muzero-chess-ai-reached-superhuman-performance-without-even-knowing-the-rules/</li>
<li>What model does MuZero learn? - arXiv, https://arxiv.org/html/2306.00840v3</li>
<li>MuZero: Mastering Go, chess, shogi and Atari without rules - Google …, https://deepmind.google/discover/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules/</li>
<li>DeepMind’s MuZero Marks A New Breakthrough In Reinforcement Learning - Analytics Drift, https://analyticsdrift.com/deepminds-muzero-marks-new-breakthrough-in-reinforcement-learning/</li>
<li>DeepMind’s MuZero is One of the Most Important Deep Learning Systems Ever Created, https://www.kdnuggets.com/2021/01/deepmind-muzero-important-deep-learning-systems.html</li>
<li>MuZero - m0nads, https://m0nads.wordpress.com/2022/01/22/muzero/</li>
<li>MuZero Intuition - Julian Schrittwieser, https://www.julian.ac/blog/2020/12/22/muzero-intuition/</li>
<li>July 2019 - Engineering at Meta, https://engineering.fb.com/2019/07/</li>
<li>Facebook’s AI crushes professionals in six-player poker | Mashable, https://mashable.com/article/facebook-ai-poker</li>
<li>[R] Facebook, Carnegie Mellon build first AI that beats pros in 6-player poker - Reddit, https://www.reddit.com/r/MachineLearning/comments/cbz7lg/r_facebook_carnegie_mellon_build_first_ai_that/</li>
<li>Facebook Pluribus poker AI research - Online poker, https://www.uspoker.com/wp-content/uploads/2019/07/Facebook-Pluribus-poker-AI-research.pdf</li>
<li>Why Facebook’s poker AI is a big step for artificial intelligence - The Decoder, https://the-decoder.com/why-facebooks-poker-ai-is-a-big-step-for-artificial-intelligence/</li>
<li>Remembering Pluribus: The Techniques that Facebook Used to Master World’s Most Difficult Poker Game - KDnuggets, https://www.kdnuggets.com/2020/12/remembering-pluribus-facebook-master-difficult-poker-game.html</li>
<li>AMA: We are Noam Brown and Tuomas Sandholm, creators of the Carnegie Mellon / Facebook multiplayer poker bot Pluribus. We’re also joined by a few of the pros Pluribus played against. Ask us anything! - Reddit, https://www.reddit.com/r/MachineLearning/comments/ceece3/ama_we_are_noam_brown_and_tuomas_sandholm/</li>
<li>Better language models and their implications - OpenAI, https://openai.com/index/better-language-models/</li>
<li>Language Models are Unsupervised Multitask Learners | OpenAI, https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</li>
<li>Artificial Intelligence | Pros, Cons, Debate, Arguments, Computer Science, &amp; Technology | Britannica, https://www.britannica.com/procon/artificial-intelligence-AI-debate</li>
<li>AI Partnerships Beyond Control Lessons from the OpenAI-Microsoft Saga - CodeX, https://law.stanford.edu/2025/03/21/ai-partnerships-beyond-control-lessons-from-the-openai-microsoft-saga/</li>
<li>Helping robots work together to explore the Moon and Mars - Penn Engineering Blog, https://blog.seas.upenn.edu/helping-robots-work-together-to-explore-the-moon-and-mars/</li>
<li>Penn Receives $2 Million NASA Grant for TRUSSES Project to Study Lunar Robotics, https://blog.seas.upenn.edu/penn-receives-2-million-nasa-grant-for-trusses-project-to-study-lunar-robotics/</li>
<li>Helping robots work together to explore the Moon and Mars | Penn Today, https://penntoday.upenn.edu/news/penn-engineering-navigating-landscapes-lunar-robots-moon-and-mars</li>
<li>EfficientNet: Rethinking Model Scaling for Convolutional … - arXiv, https://arxiv.org/abs/1905.11946</li>
<li>Energy and Policy Considerations for Deep Learning in NLP, https://arxiv.org/abs/1906.02243</li>
<li>Energy and Policy Considerations for Deep Learning in NLP - ACL Anthology, https://aclanthology.org/P19-1355/</li>
<li>Energy and Policy Considerations for Deep Learning in NLP | Request PDF - ResearchGate, https://www.researchgate.net/publication/335778882_Energy_and_Policy_Considerations_for_Deep_Learning_in_NLP</li>
<li>[PDF] Energy and Policy Considerations for Deep Learning in NLP - Semantic Scholar, https://www.semanticscholar.org/paper/Energy-and-Policy-Considerations-for-Deep-Learning-Strubell-Ganesh/d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea</li>
<li>Towards Sustainable NLP: Insights from Benchmarking Inference Energy in Large Language Models - arXiv, https://arxiv.org/html/2502.05610v1</li>
<li>Towards Accurate and Reliable Energy Measurement of NLP Models - ACL Anthology, https://aclanthology.org/2020.sustainlp-1.19.pdf</li>
<li>The Price of Prompting: Profiling Energy Use in Large Language Models Inference - arXiv, https://arxiv.org/html/2407.16893v1</li>
<li>Quantifying Generalization in Reinforcement Learning - arXiv, https://arxiv.org/abs/1812.02341</li>
<li>AI takeover - Wikipedia, https://en.wikipedia.org/wiki/AI_takeover</li>
<li>AI at Google: our principles, https://blog.google/technology/ai/ai-principles/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>