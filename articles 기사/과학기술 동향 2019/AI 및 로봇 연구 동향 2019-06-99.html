<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2019년 6월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2019년 6월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2019년 AI 및 로봇 연구 동향</a> / <span>2019년 6월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2019년 6월 AI 및 로봇 연구 동향</h1>
<h2>1. 서론: 2019년 중반, AI 연구의 변곡점</h2>
<p>2019년은 인공지능(AI) 기술이 학문적 탐구의 영역을 넘어 산업 전반의 핵심 동력으로 자리매김하기 시작한 중요한 해였다. McKinsey Global Survey에 따르면, 2019년 기업들의 표준 비즈니스 프로세스 내 AI 도입률은 전년 대비 약 25% 증가했으며, 응답 기업의 58%가 최소 하나 이상의 사업 부문 또는 기능에 AI 역량을 내재화했다고 보고했다. 이는 2018년의 47%에서 괄목할 만한 성장세다.1 이러한 양적 팽창은 AI Index 2019 보고서에서도 확인된다. 해당 보고서는 AI 관련 스타트업에 대한 전 세계 민간 투자가 370억 달러를 초과했으며, NeurIPS와 같은 최상위 AI 학회의 참석자 수가 전년 대비 41% 증가하여 13,500명에 이르는 등 학계와 산업계 모두에서 AI에 대한 관심이 폭발적으로 증가했음을 분석했다.2 이 시점의 AI는 더 이상 가능성만을 논하는 기술이 아니라, 실제 비즈니스 환경에서 매출 증대와 비용 절감이라는 가시적인 가치를 창출하는 단계로 진입하고 있었다.1</p>
<p>이러한 기술적, 산업적 배경 속에서 2019년 6월은 AI 연구계에 있어 상징적인 한 달이었다. 불과 몇 주 사이에 AI의 세 핵심 분야인 컴퓨터 비전(CVPR), 기계 학습(ICML), 로보틱스(RSS)를 대표하는 세계 최고 권위의 학회들이 미국 캘리포니아 롱비치와 독일 프라이부르크에서 연달아 개최되었다.4 이 학회들의 집중적인 개최는 당시 AI 연구 커뮤니티 내에서 이루어지던 활발한 지식 교류와 학문적 융합의 단면을 보여준다. CVPR에서는 물리 세계를 시각적으로 이해하고 재구성하려는 시도가, ICML에서는 기계 학습 이론의 근본적인 가정들을 되짚어보는 깊이 있는 성찰이, 그리고 RSS에서는 학습된 지능을 물리적 에이전트로 구현하려는 노력이 각각의 정점을 이루며 유기적으로 연결되었다.</p>
<p>본 보고서는 이 세 학회에서 발표된 최우수 논문 및 주목할 만한 연구들을 중심으로 2019년 6월의 연구 지형을 심층적으로 분석하고자 한다. CVPR에서는 물리 법칙에 기반하여 시각의 한계를 넘어서려는 비가시선(Non-Line-of-Sight) 형상 복원 연구와, 초현실적인 이미지 생성을 통해 새로운 패러다임을 제시한 StyleGAN을 다룬다. ICML에서는 비지도 학습의 근본적 한계를 지적하며 연구계에 중요한 화두를 던진 분리 표현(disentanglement) 연구를 탐구한다. 마지막으로 RSS에서는 생체 모방 기술과 소프트 로보틱스의 결합을 통해 미래 로봇 기술의 새로운 가능성을 연 연구를 조망한다. 이 분석을 통해 2019년 중반의 AI 연구가 어떻게 기술적 경계를 폭발적으로 확장하는 동시에, 그 근간을 이루는 이론에 대한 냉철한 자기 성찰을 수행했는지 종합적으로 고찰한다. 이는 당시 AI 분야가 겪고 있던 미묘하지만 중요한 지적 긴장 상태를 드러낸다. 한편에서는 StyleGAN과 같은 공학적 성취가 전례 없는 능력을 과시했지만, 다른 한편에서는 ICML의 최우수 논문이 비지도 학습이라는 유행의 근본적인 결함을 지적하며 제동을 걸었다. 이러한 동시적이고 상반된 움직임은 AI 분야가 양적 팽창을 넘어 질적으로 성숙해가는 과정에 있었음을 보여주며, 이후 연구들이 나아갈 방향, 즉 모델의 작동 원리에 대한 깊은 이해와 엄밀한 평가의 중요성을 예고하는 서막이었다.</p>
<h2>2.  컴퓨터 비전의 물리적, 생성적 도약 - CVPR 2019</h2>
<h3>2.1  학회 개요: 컴퓨터 비전의 산업적 중심지</h3>
<p>제30회 컴퓨터 비전 및 패턴 인식 학회(CVPR) 2019는 6월 15일부터 21일까지 미국 캘리포니아 롱비치 컨벤션 센터에서 개최되었다.4 이 행사는 68개국에서 9,200명 이상이 참석하며 역대 최대 규모를 경신했을 뿐만 아니라, 컴퓨터 비전 기술이 학계를 넘어 산업의 중심으로 이동하고 있음을 명확히 보여주는 이정표가 되었다.5 구글 스칼라(Google Scholar)의 전체 학술 출판물 순위에서 상위 10위권에 포함될 정도로 그 학문적 영향력이 입증된 CVPR은 이제 기술 산업계에서 가장 중요한 AI 이벤트 중 하나로 자리 잡았다.10</p>
<p>이러한 위상은 참석자 인구 통계에서 더욱 뚜렷하게 나타난다. 전체 참석자 중 산업계 소속이 48%로, 학계(22%)와 학생(28%)의 비중을 합친 것과 맞먹는 수준이었다.11 이는 컴퓨터 비전 연구가 상아탑에 머무르지 않고, 곧바로 상업적 제품과 서비스로 이어지는 강력한 파이프라인을 구축했음을 의미한다. 실제로 기업들이 CVPR에 참여하는 주된 목적은 우수 인재 채용, 기업 브랜드 인지도 제고, 그리고 최신 제품 및 기술 홍보였다.11 아래 표는 CVPR 2019 참석자들의 인구 통계학적 특성을 요약한 것이다.</p>
<table><thead><tr><th>구분</th><th>카테고리</th><th>비율 (%)</th></tr></thead><tbody>
<tr><td><strong>소속 (Affiliation)</strong></td><td>산업계 (Industry)</td><td>48</td></tr>
<tr><td></td><td>학계 (Academic)</td><td>22</td></tr>
<tr><td></td><td>학생 (Students)</td><td>28</td></tr>
<tr><td></td><td>기타 (Other)</td><td>2</td></tr>
<tr><td><strong>직무 (Job Function)</strong></td><td>연구/교육 (Research/Education)</td><td>63</td></tr>
<tr><td></td><td>엔지니어링/개발 (Engineering/Development)</td><td>29</td></tr>
<tr><td></td><td>경영 (Management)</td><td>4</td></tr>
<tr><td></td><td>기타 (Other)</td><td>4</td></tr>
<tr><td><strong>기관 규모 (Organization Size)</strong></td><td>10,000명 이상</td><td>32</td></tr>
<tr><td></td><td>1,001 ~ 10,000명</td><td>25</td></tr>
<tr><td></td><td>101 ~ 1,000명</td><td>20</td></tr>
<tr><td></td><td>1 ~ 100명</td><td>23</td></tr>
<tr><td><strong>연령대 (Age Group)</strong></td><td>25-34세</td><td>54</td></tr>
<tr><td></td><td>35-44세</td><td>24</td></tr>
<tr><td></td><td>18-24세</td><td>9</td></tr>
<tr><td></td><td>45-54세</td><td>10</td></tr>
<tr><td></td><td>55세 이상</td><td>3</td></tr>
</tbody></table>
<p>이 데이터는 CVPR이 단순한 학술 교류의 장을 넘어, 기술 혁신을 주도하는 실무자들의 집결지임을 보여준다. 연구/교육 및 엔지니어링/개발 직무가 전체의 92%를 차지한다는 점은 이 학회가 이론가보다는 기술을 직접 구현하고 혁신하는 ’빌더(builder)’들을 위한 행사임을 시사한다.11 이러한 배경은 학회에서 발표되는 기술들이 왜 그토록 즉각적인 산업적 파급력을 갖는지 설명해준다. 튜토리얼 세션에서 다뤄진 주요 응용 분야가 로보틱스, 가상/증강 현실(AR/VR), 헬스케어, 자율주행차 등이었던 점도 컴퓨터 비전 기술이 더 이상 순수한 이미지 인식을 넘어, 물리적 세계와 상호작용하고 인간의 삶에 직접적인 영향을 미치는 핵심 기술로 자리 잡았음을 보여준다.5</p>
<h3>2.2  최우수 논문 심층 분석: “비가시선(NLOS) 형상 복원을 위한 페르마 경로 이론 (A Theory of Fermat Paths for Non-Line-of-Sight Shape Reconstruction)”</h3>
<p>2019년 CVPR 최우수 논문상은 카네기 멜런 대학의 Shumian Xin, Ioannis Gkioulekas, Aswin Sankaranarayanan, Srinivasa Narasimhan과 토론토 대학, 유니버시티 칼리지 런던 소속 연구진이 공동으로 저술한 “A Theory of Fermat Paths for Non-Line-of-Sight Shape Reconstruction“에 수여되었다.10 이 연구는 딥러닝이 컴퓨터 비전 분야를 지배하던 시기에, 물리학의 제1원리(first principles)에 기반한 접근법으로 가장 어려운 문제 중 하나를 해결할 수 있음을 보여주며 학계에 신선한 충격을 주었다.</p>
<h4>2.2.1 핵심 목표 및 문제 정의</h4>
<p>이 연구의 목표는 카메라의 직접적인 시야에 들어오지 않는, 즉 ’코너 너머(looking around the corner)’에 있거나 ’확산체 뒤(looking through a diffuser)’에 숨겨진 물체의 3차원 형상을 정밀하게 복원하는 것이다.14 이는 비가시선(Non-Line-of-Sight, NLOS) 이미징이라 불리는 분야로, 기존 컴퓨터 비전 기술로는 해결이 거의 불가능했던 근본적인 문제에 대한 도전이었다. 이 기술은 자율주행차가 장애물 너머를 보거나, 재난 현장에서 구조대원이 잔해 뒤의 생존자를 찾는 등 다양한 분야에 혁신을 가져올 잠재력을 지닌다.</p>
<h4>2.2.2 이론적 기여: 페르마 경로(Fermat Paths) 이론</h4>
<p>이 논문의 가장 핵심적인 기여는 17세기 광학 원리인 페르마의 원리(Fermat’s principle)를 현대적인 신호 처리 문제에 접목한 것이다. 연구진은 빛이 두 지점 사이를 이동할 때 국소적으로 경로 길이가 정상(stationary) 상태인 ’페르마 경로’를 따른다는 점에 주목했다.15 NLOS 환경에서 이러한 경로는 광원에서 나와 중간 벽(relay wall)에 반사된 후, 숨겨진 물체 표면에서 정반사(specular reflection)되거나 물체의 경계(boundary)에서 반사되어 다시 중간 벽을 거쳐 센서로 돌아오는 경로를 의미한다. 이 경로들은 숨겨진 물체의 형상 정보를 자연스럽게 인코딩하게 된다.14</p>
<p>이 연구의 가장 중요한 이론적 발견은, 이러한 페르마 경로가 ‘과도 측정(transient measurements)’ 데이터에서 뚜렷한 ’불연속성(discontinuities)’으로 나타난다는 것을 수학적으로 증명한 점이다.14 과도 측정은 초고속 레이저 펄스를 발사하고, 그것이 여러 번 반사되어 돌아오는 광자들의 도착 시간을 단일 광자 검출기(Single-Photon Avalanche Diode, SPAD)와 같은 센서를 이용해 피코초(<span class="math math-inline">10^{-12}</span>초) 단위의 정밀도로 측정하는 기술이다.15 이 시간 분포를 분석하면 빛이 이동한 총 거리를 알 수 있다. 페르마 경로를 따라 이동한 빛은 다른 경로의 빛과 구별되는 특정 시간대에 집중적으로 도착하여 신호의 불연속점을 형성하며, 이 불연속점의 시간적 위치는 숨겨진 물체의 형상에 의해서만 결정되고 물체의 반사율(BRDF)과는 무관하다.16</p>
<h4>2.2.3 방법론: 페르마 흐름(Fermat Flow) 알고리즘</h4>
<p>이론적 발견을 바탕으로, 연구진은 ’페르마 흐름(Fermat Flow)’이라는 새로운 형상 복원 알고리즘을 제안했다. 이 알고리즘의 핵심은 과도 측정에서 감지된 불연속점의 시간 정보, 즉 페르마 경로의 길이(<span class="math math-inline">\tau</span>)를 중간 벽의 2차원 공간에 대해 미분한 값(<span class="math math-inline">\nabla\tau</span>)이 숨겨진 표면의 3차원 위치와 법선 벡터(normal vector)에 대한 강력한 기하학적 제약 조건을 제공한다는 것을 유도한 것이다.14</p>
<p>공초점(confocal) 스캐닝 환경에서, 중간 벽 위의 한 점 <span class="math math-inline">v</span>에서 레이저를 쏘고 같은 지점에서 신호를 측정한다고 가정하자. 빛이 숨겨진 물체의 한 점 <span class="math math-inline">x</span>에 반사되어 돌아올 때, 총 경로 길이는 $\tau(v) = 2 |v - x|`이다. 이 경로 길이를 <span class="math math-inline">v</span>에 대해 미분하면 다음과 같은 ‘페르마 흐름’ 제약 조건을 얻을 수 있다.15</p>
<p><span class="math math-display">
\nabla_v \tau(v) = 2 \frac{v - x}{\|v - x\|} = -2n_v(x)
</span><br />
여기서 <span class="math math-inline">n_v(x)</span>는 중간 벽의 점 <span class="math math-inline">v</span>에서 숨겨진 점 <span class="math math-inline">x</span>를 바라보는 방향 벡터(단위 벡터)이다. 이 수식은 측정된 시간 정보의 공간적 변화율(좌변, <span class="math math-inline">\nabla_v \tau(v)</span>)이 숨겨진 지점의 방향(우변, <span class="math math-inline">-2n_v(x)</span>)을 직접적으로 알려준다는 놀라운 사실을 보여준다. 알고리즘은 중간 벽의 여러 지점에서 <span class="math math-inline">\tau(v)</span>를 측정하고, 수치적으로 그래디언트를 계산하여 각 지점에 대응하는 숨겨진 표면의 방향 벡터를 추정한다. 이 정보와 경로 길이 정보를 결합하여 숨겨진 표면 위의 점들의 3차원 위치와 법선을 계산하고, 이를 통합하여 최종적으로 전체 3D 형상을 복원한다.15</p>
<h4>2.2.4 실험 및 결과</h4>
<p>연구진은 알고리즘의 실효성을 입증하기 위해 두 가지 다른 시간 해상도를 가진 시스템을 사용했다. 첫째, 피코초 스케일의 시간 해상도를 갖는 SPAD 시스템을 이용해 책상 위 마네킹이나 플라스틱 문자와 같은 다양한 물체를 ’코너 너머’에서 촬영했다. 둘째, 펨토초(<span class="math math-inline">10^{-15}</span>초) 스케일의 초고해상도를 가진 광학 간섭계 시스템을 사용하여 미국 쿼터 동전의 미세한 표면을 ’코너 너머’와 ’확산체 뒤’에서 촬영했다.15 실험 결과, 페르마 흐름 알고리즘은 확산(diffuse) 재질부터 광택(glossy), 정반사(specular) 재질에 이르기까지 다양한 표면 특성을 가진 복잡한 물체들의 형상을 밀리미터, 심지어 마이크론 스케일의 놀라운 정밀도로 복원해냈다.14</p>
<p>이 연구의 수상은 당시 데이터 기반의 딥러닝 방법론이 주류를 이루던 컴퓨터 비전 학계에 중요한 지적 전환을 시사했다. 이는 가장 어려운 비전 문제 중 일부는 더 큰 데이터셋이나 더 깊은 신경망이 아닌, 빛의 이동과 같은 물리 세계에 대한 더 깊은 이해와 모델링을 통해 해결될 수 있음을 보여준 사례였다. 이 연구는 단순히 딥러닝을 배제하는 것이 아니라, 물리 모델이 강력한 사전 정보(prior)와 제약 조건을 제공함으로써 순수하게 학습에만 의존하는 시스템의 불안정성과 데이터 요구량을 줄일 수 있는 하이브리드 접근법의 가능성을 열었다. 실제로 이 연구는 이후 계산사진학(computational photography) 및 물리 기반 비전 분야에 새로운 활력을 불어넣었으며, 빛의 전달과 이미지 형성 과정을 신경망 프레임워크에 직접 통합하는 미분 가능 렌더링(differentiable rendering)이나 뉴럴 래디언스 필드(NeRF)와 같은 후속 기술들의 등장을 예고했다. 이는 AI 비전의 미래가 픽셀의 패턴을 인식하는 것을 넘어, 그 픽셀을 생성하는 인과적, 물리적 과정을 이해하는 모델을 구축하는 데 있음을 알리는 신호탄이었다. 후속 연구들은 페르마 경로 이론을 기본적인 물리 모델로 참조하면서, 실시간 적용의 어려움이나 계산 비용과 같은 한계를 극복하기 위해 딥러닝 및 최적화 기법을 접목하는 방향으로 발전하고 있다.21</p>
<h3>2.3  주목할 만한 연구 (Honorable Mention): “A Style-Based Generator Architecture for Generative Adversarial Networks” (StyleGAN)</h3>
<p>CVPR 2019에서 최우수 논문상에 준하는 Honorable Mention을 수상한 NVIDIA의 Tero Karras, Samuli Laine, Timo Aila가 발표한 StyleGAN은 생성 모델 분야에 새로운 지평을 연 연구다.10 이 논문은 단순히 고품질의 이미지를 생성하는 것을 넘어, 생성 과정 자체를 의미론적으로 제어할 수 있는 혁신적인 아키텍처를 제안하여 학계와 산업계에 큰 반향을 일으켰다.</p>
<h4>2.3.1 핵심 목표</h4>
<p>StyleGAN의 핵심 목표는 두 가지였다. 첫째, 기존 생성적 적대 신경망(GAN)의 한계를 뛰어넘어 전례 없는 수준의 고해상도, 고품질 이미지를 생성하는 것이다. 둘째, 생성된 이미지의 다양한 속성들—예를 들어, 사람 얼굴의 자세, 성별, 나이와 같은 거시적 특징부터 주근깨, 머리카락 질감과 같은 미시적 특징까지—을 직관적이고 스케일-특화된(scale-specific) 방식으로 제어할 수 있는 아키텍처를 만드는 것이었다.26</p>
<h4>2.3.2 구조적 혁신: 스타일 기반 생성자 (Style-based Generator)</h4>
<p>StyleGAN의 혁신은 생성자(generator)의 구조를 완전히 재설계한 데 있다.</p>
<ul>
<li>
<p><strong>매핑 네트워크(Mapping Network):</strong> 전통적인 GAN은 정규분포 등에서 샘플링된 잠재 벡터 <span class="math math-inline">z</span>를 생성자의 첫 번째 레이어에 직접 입력으로 사용한다. 반면, StyleGAN은 <span class="math math-inline">z</span>를 8계층 다층 퍼셉트론(MLP)으로 구성된 ‘매핑 네트워크’ <span class="math math-inline">f</span>에 통과시켜 중간 잠재 공간 <span class="math math-inline">W</span>의 벡터 <span class="math math-inline">w</span>로 변환한다 (<span class="math math-inline">w = f(z)</span>).28</p>
</li>
<li>
<p><strong>비얽힘(Disentanglement) 유도:</strong> 이 과정이 결정적으로 중요하다. 입력 잠재 공간 <span class="math math-inline">Z</span>는 일반적으로 고정된 분포(예: 가우시안)를 따르도록 강제되기 때문에, 데이터의 복잡한 요인들이 서로 얽혀(entangled) 표현될 수밖에 없다. 하지만 매핑 네트워크를 통해 생성된 중간 잠재 공간 <span class="math math-inline">W</span>는 이러한 제약에서 벗어나, 학습 과정에서 데이터의 실제 변동 요인들을 더 잘 분리(disentangle)하도록 강제된다. 즉, <span class="math math-inline">W</span> 공간의 각 차원이 이미지의 독립적인 의미론적 속성(예: 머리 길이, 안경 유무)을 제어하도록 학습될 가능성이 높아진다. 이는 생성된 이미지에 대한 훨씬 더 정교한 제어를 가능하게 하는 기반이 된다.28</p>
</li>
<li>
<p><strong>합성 네트워크(Synthesis Network):</strong> 실제 이미지 생성은 ‘합성 네트워크’ <span class="math math-inline">g</span>에서 이루어진다. 이 네트워크는 전통적인 GAN처럼 잠재 벡터를 입력으로 받는 대신, 학습 가능한 4x4 크기의 상수(constant) 텐서에서 시작한다. 모든 정보는 매핑 네트워크를 통해 변환된 <span class="math math-inline">w</span> 벡터로부터 ’스타일’의 형태로 주입된다.29</p>
</li>
</ul>
<h4>2.3.3 핵심 메커니즘: 적응형 인스턴스 정규화 (AdaIN)</h4>
<p>StyleGAN이 스타일을 이미지에 주입하는 핵심 메커니즘은 ’적응형 인스턴스 정규화(Adaptive Instance Normalization, AdaIN)’이다.</p>
<ol>
<li>
<p>중간 잠재 벡터 <span class="math math-inline">w</span>는 각 합성곱 레이어마다 별도의 학습된 아핀 변환(affine transformation)을 거쳐 ‘스타일’ 정보 <span class="math math-inline">y = (y_s, y_b)</span>로 변환된다.29</p>
</li>
<li>
<p>합성 네트워크의 각 레이어에서, 합성곱 연산이 끝난 후 피처 맵(feature map) <span class="math math-inline">x_i</span>는 AdaIN 연산을 거친다. AdaIN은 먼저 각 피처 맵 채널을 개별적으로 평균 0, 분산 1로 정규화(instance normalization)한다. 그 후, 스타일 <span class="math math-inline">y</span>로부터 파생된 스케일(<span class="math math-inline">y_{s,i}</span>)과 편향(<span class="math math-inline">y_{b,i}</span>)을 적용하여 피처 맵의 통계치를 변조한다.28</p>
<p><span class="math math-display">
\text{AdaIN}(x_i, y) = y_{s,i} \frac{x_i - \mu(x_i)}{\sigma(x_i)} + y_{b,i}
</span></p>
</li>
<li>
<p>이 메커니즘을 통해 <span class="math math-inline">w</span> 벡터는 이미지의 전반적인 스타일, 즉 자세, 정체성, 배경과 같은 거시적이고 구조적인 특징을 제어하게 된다. 반면, 각 레이어에 직접 주입되는 가우시안 노이즈는 머리카락의 무작위적인 뻗침, 주근깨의 분포, 피부의 미세한 질감과 같은 확률적이고 국소적인 변이를 담당한다. 이 구조는 이미지의 고수준 속성과 미세 텍스처를 별도의 제어 채널을 통해 다룰 수 있게 하여, 이 둘의 자동적이고 비지도적인 분리를 가능하게 한다.27</p>
</li>
</ol>
<h4>2.3.4 영향 및 후속 연구</h4>
<p>StyleGAN의 등장은 생성 모델 연구에 큰 획을 그었다. 이 모델은 StyleGAN2, StyleGAN3로 이어지는 성공적인 시리즈의 시초가 되었으며, 고품질 이미지 생성 및 편집 분야에서 사실상의 표준 모델 중 하나로 자리매김했다.35 StyleGAN의 사실적인 이미지 생성 능력은 ‘딥페이크(Deepfake)’ 기술의 발전에 결정적인 영향을 미쳤으며, 이는 사회적으로 긍정적, 부정적 논쟁을 동시에 불러일으켰다.40 한편, 의료 영상 분야에서는 부족한 데이터를 증강(data augmentation)하거나, 개인정보 보호를 위해 실제 환자 데이터와 통계적으로 유사한 합성 데이터를 생성하는 등 긍정적인 응용 가능성을 열었다.42 이처럼 StyleGAN은 기술적 성취를 넘어 AI의 사회적 영향력에 대한 논의까지 촉발시킨 중요한 연구라 할 수 있다.</p>
<h2>3.  기계 학습의 근본에 대한 성찰 - ICML 2019</h2>
<h3>3.1  학회 개요</h3>
<p>제36회 국제 기계 학습 학회(ICML) 2019는 6월 10일부터 15일까지 미국 캘리포니아 롱비치에서 CVPR과 같은 장소에서 연이어 개최되었다.6 이는 두 분야의 연구자들이 활발히 교류할 수 있는 기회를 제공하며, 컴퓨터 비전의 응용 문제와 기계 학습의 기초 이론이 어떻게 상호작용하는지를 보여주는 상징적인 일정이었다.</p>
<p>ICML 2019의 연구 생태계는 기계 학습 분야의 특성을 잘 보여준다. 채택된 논문 중 학계 단독 연구가 58.4%로 가장 큰 비중을 차지했지만, 산업계와의 공동 연구 또한 33.9%에 달해, 기초 연구가 여전히 학계를 중심으로 이루어지면서도 산업계와의 긴밀한 협력이 매우 활발함을 시사했다.48 산업계 단독 연구는 7.8%였다. 논문 기여도 측면에서 가장 두각을 나타낸 기관은 구글, MIT, UC 버클리로, 이는 이들 기관이 당시 기계 학습 연구를 선도하고 있었음을 보여준다.48 이러한 통계는 ICML이 순수 이론 탐구와 실제 문제 해결 사이의 균형을 맞추며 발전하고 있음을 나타낸다.</p>
<h3>3.2  최우수 논문 심층 분석: “분리 표현의 비지도 학습에 대한 일반적 가정의 재고찰 (Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations)”</h3>
<p>2019년 ICML 최우수 논문상은 막스 플랑크 지능형 시스템 연구소의 Francesco Locatello, Stefan Bauer, Bernhard Schölkopf, 구글 리서치의 Mario Lucic, Sylvain Gelly, Olivier Bachem, 그리고 ETH 취리히의 Gunnar Rätsch 등 유럽의 대표적인 학술 기관과 글로벌 산업 연구소 소속 연구진들의 공동 연구에 수여되었다.48 이 논문은 새로운 모델이나 알고리즘을 제안한 것이 아니라, 당시 기계 학습 커뮤니티에서 큰 인기를 끌던 ’분리 표현(disentangled representations)’의 비지도 학습이라는 연구 방향 자체에 대해 근본적인 질문을 던졌다는 점에서 큰 의미를 가진다.</p>
<h4>3.2.1 핵심 목표</h4>
<p>이 연구의 핵심 목표는 ’분리 표현’을 비지도 방식으로 학습할 수 있다는 당시의 낙관적인 가정들에 대해 비판적인 시각을 제공하고, 그 근본적인 한계를 이론과 실험을 통해 명확히 밝히는 것이었다.48 ’분리 표현’이란 데이터의 기저에 있는 독립적이고 의미론적인 생성 요인들(예: 3D 객체 이미지의 모양, 크기, 색상, 위치)을 잠재 공간의 각기 다른 축에 하나씩 독립적으로 매핑하는 표현을 의미한다.50 이러한 표현은 모델의 해석 가능성을 높이고, 새로운 데이터에 대한 일반화 성능을 향상시키며, 다운스트림 작업(downstream task)의 학습 효율을 높일 것으로 기대되어 많은 연구가 진행되고 있었다.</p>
<h4>3.2.2 이론적 기여: 비지도 분리의 근본적 불가능성 증명</h4>
<p>이 논문의 가장 충격적인 기여는, 모델과 데이터에 대한 어떠한 ’귀납적 편향(inductive biases)’도 없다면, 순수한 비지도 학습만으로는 분리된 표현을 학습하는 것이 원리적으로 불가능함을 수학적으로 증명한 것이다.50</p>
<p><strong>정리 1(Theorem 1)의 논리</strong>는 다음과 같다. 데이터가 독립적인 잠재 변수 <span class="math math-inline">z</span>로부터 생성된다고 가정하자. 이 논문은 <span class="math math-inline">z</span>와 완벽하게 얽혀(entangled) 있으면서도, 통계적으로는 <span class="math math-inline">z</span>와 구별할 수 없는(즉, 동일한 주변 분포를 갖는) 또 다른 잠재 변수 <span class="math math-inline">f(z)</span>가 무한히 많이 존재할 수 있음을 보였다. 비지도 학습 모델은 오직 관측된 데이터 <span class="math math-inline">x</span>의 분포만을 보고 학습하기 때문에, 동일한 데이터 분포를 생성할 수 있는 두 잠재 모델, 즉 분리된 <span class="math math-inline">z</span> 기반 모델과 얽힌 <span class="math math-inline">f(z)</span> 기반 모델을 구별할 방법이 없다. 따라서 어떤 비지도 학습 알고리즘이 우연히 완벽하게 분리된 표현을 학습했다고 하더라도, 그 알고리즘이 왜 얽힌 무수한 다른 대안들 대신 그 표현을 선택했는지를 설명할 방법이 없다는 것이다. 이는 귀납적 편향, 즉 모델 구조나 데이터에 대한 특정 가정이 없다면 비지도 분리 학습은 ’운에 맡기는 것’과 다름없다는 것을 의미한다.52</p>
<h4>3.2.3 실험적 기여: 대규모 재현 가능 연구</h4>
<p>이론적 증명을 뒷받침하기 위해, 연구진은 7개의 서로 다른 데이터셋에 대해 6개의 대표적인 비지도 분리 학습 모델(β-VAE, FactorVAE 등)과 6개의 분리 평가 지표를 사용하여 총 12,000개 이상의 모델을 학습하고 평가하는 전례 없는 규모의 실험을 수행했다.52 이 대규모 실험을 통해 얻은 결론은 매우 비관적이었다.</p>
<ol>
<li>
<p><strong>모델 식별의 불가능성:</strong> 정답 레이블(ground-truth) 없이는 어떤 모델이 실제로 더 잘 분리된 표현을 학습했는지 신뢰성 있게 식별할 수 없었다.</p>
</li>
<li>
<p><strong>하이퍼파라미터의 비일관성:</strong> 특정 데이터셋에서 좋은 성능을 보인 하이퍼파라미터가 다른 데이터셋이나 다른 평가 지표에서는 좋은 성능을 보장하지 못했다. 즉, 일반화 가능한 최적의 설정이 존재하지 않았다.</p>
</li>
<li>
<p><strong>다운스트림 성능과의 무관성:</strong> 분리도가 높다고 해서 분류나 예측과 같은 다운스트림 작업의 성능(특히, 학습에 필요한 샘플 수)이 반드시 향상된다는 증거를 찾지 못했다.</p>
</li>
<li>
<p><strong>우연의 중요성:</strong> 모델 아키텍처 선택과 같은 설계적 요인보다, 랜덤 시드 초기화나 하이퍼파라미터 설정과 같은 우연적 요인이 결과에 더 큰 영향을 미치는 것으로 나타났다.52</p>
</li>
</ol>
<h4>3.2.4 영향 및 후속 연구 방향 제시</h4>
<p>이 논문은 분리 표현 연구 커뮤니티에 ‘찬물을 끼얹는’ 역할을 했지만, 동시에 해당 분야를 더 건강하고 엄밀한 방향으로 이끄는 중요한 계기가 되었다. 연구진은 논문의 결론에서 다음과 같은 세 가지 미래 연구 방향을 제시했다.52</p>
<ol>
<li>
<p><strong>귀납적 편향의 명시:</strong> 연구자들은 자신들의 모델이 어떤 귀납적 편향(가정)에 의존하여 분리를 달성하는지 명시적으로 밝혀야 한다.</p>
</li>
<li>
<p><strong>실질적 이점의 입증:</strong> 분리된 표현이 해석 가능성, 공정성(fairness), 견고성(robustness) 등 구체적인 다운스트림 작업에서 어떤 실질적인 이점을 제공하는지 명확히 입증해야 한다.</p>
</li>
<li>
<p><strong>재현 가능한 실험:</strong> 다양한 데이터셋을 포괄하는 표준화되고 재현 가능한 실험 프로토콜을 따라야 한다.</p>
</li>
</ol>
<p>이 연구 이후, 분리 표현 연구는 완전한 비지도 학습이라는 이상에서 벗어나, 시간적 순서나 다중 시점 정보와 같은 데이터 자체의 구조를 활용하는 약지도(weakly-supervised) 및 자기지도(self-supervised) 학습, 그리고 데이터 생성 과정의 인과관계(causality)를 모델링하려는 시도로 무게 중심을 옮겨갔다.56 또한, 연구의 투명성과 재현성을 높이기 위해 연구진이 공개한 코드 라이브러리 <code>disentanglement_lib</code>와 10,000개 이상의 사전 학습된 모델은 후속 연구자들에게 귀중한 자산이 되었다.49</p>
<h2>4.  물리적 세계와의 상호작용 - RSS 2019</h2>
<h3>4.1  학회 개요</h3>
<p>로보틱스: 과학 및 시스템(Robotics: Science and Systems, RSS) 2019 학회는 6월 22일부터 26일까지 독일 프라이부르크 대학에서 개최되었다.7 RSS는 로봇 공학 분야의 최상위 학회 중 하나로, 이론적 토대와 실제 시스템 구현을 연결하는 연구들을 집중적으로 다룬다. 2019년 학회에는 총 272편의 논문이 제출되어 그중 85편이 채택, 약 31%의 비교적 낮은 채택률을 기록하며 높은 학문적 기준을 유지했다.7</p>
<p>학회 프로그램은 로봇 공학의 핵심 분야를 포괄적으로 다루었다. 주요 세션은 주변 환경을 이해하는 ‘인식(Perception)’, 목표 지점까지의 경로를 생성하는 ‘경로 계획(Planning)’, 물체를 다루는 ‘조작(Manipulation)’, 인간과 협력하는 ‘인간-로봇 상호작용(HRI)’, 로봇의 움직임을 제어하는 ‘제어 및 동역학(Control and Dynamics)’, 그리고 시연을 통해 배우는 ‘모방 학습(Imitation Learning)’ 등으로 구성되었다.60 이러한 세션 구성은 현대 로봇 연구가 단순히 계산하고 명령을 수행하는 기계를 넘어, 불확실하고 동적인 물리적 환경 속에서 스스로 감지하고, 판단하며, 행동하는 자율적인 에이전트를 만드는 데 초점을 맞추고 있음을 명확히 보여준다.</p>
<h3>4.2  최우수 논문 심층 분석: “자기 구동 방식의 해파리 모방 소프트 밀리 스위머 (A Magnetically-Actuated Untethered Jellyfish-Inspired Soft Milliswimmer)”</h3>
<p>RSS 2019 최우수 논문상은 Ziyu Ren, Tianlu Wang, Wenqi Hu, 그리고 소프트 로보틱스 및 생체 모방 로봇 분야의 세계적인 석학인 Metin Sitti 교수의 연구팀에게 돌아갔다.61 이 연구는 생명체에서 영감을 얻은 혁신적인 설계와 구동 방식을 통해 미래 의료 로봇의 가능성을 제시했다는 점에서 높은 평가를 받았다.</p>
<h4>4.2.1 핵심 목표</h4>
<p>이 연구의 핵심 목표는 외부 전원선이나 제어선 없이(untethered) 원격으로 정밀하게 제어할 수 있는 밀리미터 크기의 소프트 로봇을 개발하는 것이었다. 특히, 인체의 혈관이나 방광과 같은 유체로 채워진 복잡하고 좁은 환경 내부에서 화물(예: 약물)을 운송하거나 특정 위치에 패치를 부착하는 등의 정교한 의료 작업을 수행할 수 있는 로봇 플랫폼의 기반 기술을 제시하고자 했다.63</p>
<h4>4.2.2 설계 및 제작</h4>
<p>연구팀은 자연에서 영감을 얻어, 수 밀리미터 크기로 물속을 헤엄치며 먹이를 사냥하는 어린 해파리(ephyra jellyfish)의 형태와 유영 방식을 모방했다.63</p>
<ul>
<li>
<p><strong>재료:</strong> 로봇의 몸체는 네오디뮴-철-붕소(NdFeB) 자성 마이크로 입자를 부드러운 실리콘 엘라스토머에 혼합한 ’자성 연성 복합재료(magnetic soft composite material)’로 만들어졌다. 이 재료는 자기장에 반응하면서도 유연하게 변형될 수 있는 특성을 지닌다.63</p>
</li>
<li>
<p><strong>제작 과정:</strong> 제작 과정 또한 독창적이다. 먼저, 얇은 자성 복합재료 멤브레인을 레이저 커터로 해파리 모양의 팔(lappets) 구조로 잘라낸다. 그 위에 작은 물방울을 떨어뜨리면, 표면 장력에 의해 멤브레인이 자연스럽게 물방울을 감싸며 입체적인 타원체 모양을 형성한다. 이 상태로 급속 냉동하여 형태를 고정한 후, 1.8T(테슬라)의 강력한 외부 자기장을 가해 로봇 내 자성 입자들을 특정 방향으로 정렬시켜 ’자화(magnetization)’시킨다. 마지막으로, 팔 사이의 틈을 비자성 엘라스토머로 채워 부드럽고 일체화된 몸체를 완성한다.63</p>
</li>
</ul>
<h4>4.2.3 구동 원리</h4>
<p>이 소프트 로봇은 내장된 배터리나 모터 없이, 외부 전자기 코일 시스템이 생성하는 시변(time-varying) 자기장에 의해 원격으로 구동된다.63</p>
<ul>
<li>
<p>외부에서 주기적으로 변화하는 자기장을 인가하면, 로봇 내부에 미리 프로그래밍된 자화 패턴과 외부 자기장 간의 상호작용으로 인해 자기 토크(magnetic torque)가 발생한다.</p>
</li>
<li>
<p>이 토크는 로봇의 부드러운 몸체를 해파리가 헤엄치듯 주기적으로 수축하고 이완시키는 변형을 유발한다.</p>
</li>
<li>
<p>이러한 ‘패들링(paddling)’ 동작은 주변 유체를 밀어내어 추진력을 발생시키며, 로봇이 앞으로 나아가게 한다.63</p>
</li>
<li>
<p>외부 자기장의 주파수, 세기, 그리고 방향을 정밀하게 조절함으로써 로봇의 전진 속도와 유영 방향을 실시간으로 제어할 수 있다.63</p>
</li>
</ul>
<p>로봇의 동역학적 움직임은 뉴턴의 제2법칙에 기반한 운동 방정식으로 모델링될 수 있다. 로봇에 작용하는 힘은 패들링 동작에 의한 추력(<span class="math math-inline">\mathbf{T}</span>), 유체 저항에 의한 항력(<span class="math math-inline">\mathbf{D}</span>), 주변 유체를 가속시킴으로써 발생하는 부가 질량력(<span class="math math-inline">\mathbf{A}</span>), 그리고 중력(<span class="math math-inline">\mathbf{G}</span>)의 합으로 표현된다.63</p>
<p><span class="math math-display">
\mathbf{T} - \mathbf{D} - \mathbf{A} - \mathbf{G} = m \frac{d\mathbf{v}}{dt}
</span><br />
여기서 <span class="math math-inline">m</span>은 로봇의 질량, <span class="math math-inline">\mathbf{v}</span>는 로봇의 순간 속도를 나타낸다. 이 모델은 로봇의 움직임을 예측하고 정밀한 제어기를 설계하는 데 중요한 이론적 기초를 제공한다.</p>
<h4>4.2.4 응용 가능성 및 후속 연구</h4>
<p>이 연구는 자기 구동 방식의 소프트 마이크로/밀리 로봇이 최소 침습 수술, 표적 약물 전달, 체내 진단 등 미래 의료 분야에서 가질 수 있는 엄청난 잠재력을 명확히 보여주었다.63 2019년 이후 이 분야의 연구는 실제 임상 적용을 목표로 더욱 가속화되었다. 후속 연구들은 인체에 무해한 생체 적합성(biocompatibility) 및 시간이 지나면 자연적으로 분해되는 생분해성(biodegradability) 소재 개발, X-ray, MRI, 초음파 등 기존 의료 영상 장비를 이용한 실시간 체내 추적(in-vivo tracking) 기술 확보, 그리고 실제 임상 환경에서 사용할 수 있는 통합 제어 및 전달 시스템 개발에 집중하고 있다.67</p>
<h2>5. 결론: 종합 및 향후 전망</h2>
<p>2019년 6월, CVPR, ICML, RSS 학회에서 발표된 주요 연구들은 당시 인공지능 분야가 세 가지 중요한 방향으로 동시에 심화되고 확장되고 있었음을 명확히 보여준다. 이 시기는 단순히 더 큰 모델과 더 많은 데이터로 성능을 끌어올리는 단계를 넘어, 보다 근본적이고 다각적인 발전의 시대로 진입했음을 알리는 중요한 변곡점이었다.</p>
<p>첫째, <strong>물리 세계의 모델링</strong>이 AI 연구의 핵심 과제로 부상했다. CVPR 최우수 논문인 페르마 경로 이론은 픽셀 데이터의 통계적 패턴을 학습하는 것을 넘어, 빛이 물리 법칙에 따라 어떻게 이동하고 상호작용하는지를 모델링함으로써 ‘보는’ 행위의 근본적인 한계를 돌파하려는 시도였다. 이는 AI가 현실 세계의 물리 법칙을 깊이 이해하고 이를 문제 해결에 적극적으로 활용하는 방향으로 나아가고 있음을 시사한다. StyleGAN 역시 이미지 생성 과정을 스타일과 노이즈라는 의미론적 요소로 분해하고 제어함으로써, 데이터의 기저에 있는 생성 원리를 모델링하려는 노력의 일환으로 볼 수 있다.</p>
<p>둘째, <strong>학습 이론에 대한 재정립</strong>이 이루어졌다. ICML 최우수 논문이 비지도 분리 표현 학습의 근본적인 한계를 지적한 것은, AI 커뮤니티가 ‘어떻게(how)’ 더 성능 좋은 모델을 만들 것인가라는 공학적 질문을 넘어, ‘왜(why)’ 그것이 작동하며 그 한계는 무엇인가라는 과학적 질문으로 회귀했음을 보여준다. 이 연구는 이후 모델의 신뢰성, 공정성(fairness), 견고성(robustness)에 대한 논의를 본격적으로 촉발시키는 계기가 되었다.57</p>
<p>셋째, <strong>물리적 구현의 혁신</strong>이 가속화되었다. RSS 최우수 논문인 해파리 모방 로봇은 기존의 딱딱하고 정형화된 강체 로봇의 한계를 넘어, 부드럽고 유연하며 안전한 방식으로 물리적 세계와 상호작용하는 새로운 로봇 패러다임을 제시했다. 이는 AI가 디지털 공간의 지능을 넘어, 현실 세계와 직접 상호작용하는 물리적 실체(physical embodiment)로서 발전해나갈 방향을 보여준다.</p>
<p>이러한 2019년 6월의 연구 성과들은 장기적인 기술 발전의 씨앗이 되었다. CVPR에서 보여준 물리 기반 비전과 제어 가능한 생성 모델의 융합은, 이후 3D 장면을 놀랍도록 사실적으로 렌더링하고 편집할 수 있는 NeRF(Neural Radiance Fields)와 같은 혁신적인 기술의 등장을 예고했다. ICML에서 제기된 비판적 성찰은 ’완전한 비지도’라는 이상향 대신, 데이터 자체의 풍부한 구조를 암묵적인 지도 신호로 활용하는 자기지도 학습(self-supervised learning)의 폭발적인 성장으로 이어졌다. RSS에서 제시된 소프트 마이크로 로봇 기술은 재료공학, 제어 이론, 실시간 추적 기술의 발전에 힘입어 실제 임상 적용을 향한 연구를 가속화하는 동력이 되었다.</p>
<p>더 나아가, 이 시기는 생성 모델의 역할에 대한 미묘하지만 심오한 이중성이 드러나는 시점이기도 했다. 한편으로 StyleGAN은 예술, 엔터테인먼트, 데이터 증강 등 새로운 콘텐츠를 ’창조’하는 도구로서의 AI를 대표한다. 이는 이후 DALL-E, Midjourney와 같은 대규모 확산 모델(diffusion model)의 시대로 이어지는 ‘창작자로서의 AI’ 흐름을 형성했다.76 다른 한편으로, NLOS 이미징 연구와 분리 표현 연구는 현실 세계가 ‘어떻게 생성되는지’ 그 과정을 모델링하고, 그 역과정을 풀어 현실을 이해하거나 해석하려는 ’과학자로서의 AI’를 보여준다. 전자가 그럴듯한 결과물을 만드는 데 집중한다면, 후자는 현실에 대한 충실한 재현과 근본 원리 탐구에 집중한다. 2019년 6월에 나타난 이러한 생성 모델의 이중성, 즉 ‘창작 도구’ 대 ’시뮬레이션 및 분석 도구’라는 두 가지 정체성은 오늘날 AI 연구의 두 개의 큰 줄기를 형성하며 계속해서 발전하고 있다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Global AI Survey: AI proves its worth, but few scale impact - McKinsey, https://www.mckinsey.com/featured-insights/artificial-intelligence/global-ai-survey-ai-proves-its-worth-but-few-scale-impact</li>
<li>Artificial Intelligence Index Report 2019, https://hai-production.s3.amazonaws.com/files/ai_index_2019_report.pdf</li>
<li>Artificial Intelligence Index Report 2019 - OECD.AI, https://wp.oecd.ai/app/uploads/2020/07/ai_index_2019_introduction.pdf</li>
<li>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), https://www.computer.org/csdl/proceedings/cvpr/2019/1gyr6w5YIIU</li>
<li>2019 Computer Vision and Pattern Recognition Conference (CVPR) Breaks Records on All Fronts - PR Newswire, https://www.prnewswire.com/news-releases/2019-computer-vision-and-pattern-recognition-conference-cvpr-breaks-records-on-all-fronts-300873032.html</li>
<li>icml.cc, <a href="https://icml.cc/Conferences/2019/CallForPapers#:~:text=The%2036th%20International%20Conference%20on,(June%2014-15).">https://icml.cc/Conferences/2019/CallForPapers#:~:text=The%2036th%20International%20Conference%20on,(June%2014%2D15).</a></li>
<li>Conferences - RSS Foundation, https://roboticsfoundation.org/conferences/</li>
<li>Robotics: Science and Systems - Online Proceedings, https://www.roboticsproceedings.org/</li>
<li>The Conference on Computer Vision and Pattern Recognition (CVPR 2019) - WestminsterResearch, https://westminsterresearch.westminster.ac.uk/event/qv13w/the-conference-on-computer-vision-and-pattern-recognition-cvpr-2019</li>
<li>Opening Remarks and Awards of the Record-Breaking 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), https://www.computer.org/publications/tech-news/events/ieee-cvpr-conference-on-computer-vision-and-pattern-recognition-2019-awards-records</li>
<li>CVPR 2019 Expo, June 18-20, 2019, https://cvpr2019.thecvf.com/exhibit/exhibitor_information</li>
<li>CVPR 2019 Best Paper Award - Carnegie Mellon University Robotics Institute, https://www.ri.cmu.edu/cvpr-2019-best-paper-award/</li>
<li>Best Paper Award - CVPR2019, https://cvpr2019.thecvf.com/program/main_conference</li>
<li>A Theory of Fermat Paths for Non-Line-Of-Sight Shape Reconstruction, https://www.computer.org/csdl/proceedings-article/cvpr/2019/329300g793/1gys5JVbdAY</li>
<li>A Theory of Fermat Paths for Non-Line-of-Sight Shape Reconstruction, https://imaging.cs.cmu.edu/fermat_paths/</li>
<li>A Theory of Fermat Paths for Non-Line-of-Sight Shape Reconstruction, https://imaging.cs.cmu.edu/fermat_paths/assets/cvpr2019.pdf</li>
<li>Deep Non-line-of-sight Imaging from Under-scanning Measurements, https://proceedings.neurips.cc/paper_files/paper/2023/hash/b91cc0a242e6518ee731f74e82b2eebd-Abstract-Conference.html</li>
<li>Transient Two-Bounce NLOS - Siddharth Somasundaram, https://sidsoma.github.io/transient-2b-nlos/</li>
<li>Role of Transients in Two-Bounce Non-Line-of … - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Somasundaram_Role_of_Transients_in_Two-Bounce_Non-Line-of-Sight_Imaging_CVPR_2023_paper.pdf</li>
<li>A Theory of Fermat Paths for Non-Line-of-Sight Shape Reconstruction [CVPR 2019], https://www.youtube.com/watch?v=xWww66Ao82o</li>
<li>A Theory of Fermat Paths for Non-Line-Of-Sight Shape Reconstruction - ResearchGate, https://www.researchgate.net/publication/338509951_A_Theory_of_Fermat_Paths_for_Non-Line-Of-Sight_Shape_Reconstruction</li>
<li>NeurIPS Poster Virtual Scanning: Unsupervised Non-line-of-sight Imaging from Irregularly Undersampled Transients, https://neurips.cc/virtual/2024/poster/95200</li>
<li>Under-scanning non-line-of-sight imaging based on convolution approximation and optimization | APL Photonics | AIP Publishing, https://pubs.aip.org/aip/app/article/10/6/066110/3349294/Under-scanning-non-line-of-sight-imaging-based-on</li>
<li>Dual-branch Graph Feature Learning for NLOS Imaging - arXiv, https://arxiv.org/html/2502.19683v1</li>
<li>TransiT: Transient Transformer for Non-line-of-sight Videography - arXiv, https://arxiv.org/html/2503.11328v1</li>
<li>NVlabs/stylegan - Official TensorFlow Implementation - GitHub, https://github.com/NVlabs/stylegan</li>
<li>[1812.04948] A Style-Based Generator Architecture for Generative Adversarial Networks, https://arxiv.org/abs/1812.04948</li>
<li>
<ol>
<li>GANs and StyleGANs 2. AutoEncoders, https://www.cs.jhu.edu/~ayuille/JHUcourses/VisionAsBayesianInference2025/19/Lecture19_GANandAE.pdf</li>
</ol>
</li>
<li>Brief Review — StyleGAN: A Style-Based Generator Architecture for …, https://sh-tsang.medium.com/brief-review-stylegan-a-style-based-generator-architecture-for-generative-adversarial-networks-5051d79aad53</li>
<li>StyleGAN: A Gentle Introduction. Generative Adversarial Networks …, https://pub.aimind.so/stylegan-a-step-by-step-introduction-ff995c99a884</li>
<li>StyleGAN Mapping Network Geometry Visualization - K.L., https://k-l-lambda.github.io/2020/02/10/stylegan-mapping/</li>
<li>Exploring and Exploiting the Latent Style Space - Paperspace Blog, https://blog.paperspace.com/exploring-and-exploiting-the-latent-style-space/</li>
<li>StyleGAN: In depth explaination | Towards AI, https://towardsai.net/p/l/stylegan-in-depth-explaination</li>
<li>StyleGAN - Style Generative Adversarial Networks - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/stylegan-style-generative-adversarial-networks/</li>
<li>A Style-Based Generator Architecture for Generative Adversarial Networks - ResearchGate, https://www.researchgate.net/publication/338514531_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks</li>
<li>Evolution of StyleGAN3 - ResearchGate, https://www.researchgate.net/publication/366813754_Evolution_of_StyleGAN3</li>
<li>Official PyTorch implementation of StyleGAN3 - GitHub, https://github.com/NVlabs/stylegan3</li>
<li>The Evolution of StyleGAN: Introduction - Paperspace Blog, https://blog.paperspace.com/evolution-of-stylegan/</li>
<li>StyleGAN 3 - Lambda, https://lambda.ai/blog/stylegan-3</li>
<li>StyleGan-StyleGan2 Deepfake Face Images - Kaggle, https://www.kaggle.com/datasets/kshitizbhargava/deepfake-face-images</li>
<li>(PDF) Deepfake Image Synthesis for Data Augmentation - ResearchGate, https://www.researchgate.net/publication/362264205_Deepfake_Image_Synthesis_for_Data_Augmentation</li>
<li>A Hybrid Approach for Medical Deepfake Detection Using Depth-Wise Convolutions in Vision Transformer and Frequency Domain Analysis, https://www.iccs-meeting.org/archive/iccs2025/papers/159050222.pdf</li>
<li>(PDF) An Overview of Deepfake Methods in Medical Image …, https://www.researchgate.net/publication/378119477_An_Overview_of_Deepfake_Methods_in_Medical_Image_Processing_for_Health_Care_Applications</li>
<li>Deepfakes In Healthcare: Reviewing the Transformation Potential and its Challenges, https://ijisae.org/index.php/IJISAE/article/download/6956/5866/12186</li>
<li>ICML 2019 Call for Papers, https://icml.cc/Conferences/2019/CallForPapers</li>
<li>6th AutoML@ICML - Practical Information - Google Sites, https://sites.google.com/view/automl2019icml/practical-information</li>
<li>ICML 2019 : 36th International Conference on Machine Learning - WikiCFP, http://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=81548</li>
<li>ICML 2019 | Google, ETH Zurich, MPI-IS, Cambridge &amp; PROWLER.io Share Best Paper Honours | by Synced | SyncedReview | Medium, https://medium.com/syncedreview/icml-2019-google-eth-zurich-mpi-is-cambridge-prowler-io-share-best-paper-honours-4aeabd5c9fc8</li>
<li>ICML Best Paper Award - ETH Zürich, https://bmi.inf.ethz.ch/news/article/icml-best-paper-award</li>
<li>ICML 2019 Best Papers and Honourable of Mention - Qiang Zhang, https://zhangtemplar.github.io/icml/</li>
<li>Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations | Empirical Inference - Max Planck Institute for Intelligent Systems, https://is.mpg.de/ei/en/publications/locatelloetal19</li>
<li>Challenging Common Assumptions in the Unsupervised Learning of …, https://proceedings.mlr.press/v97/locatello19a/locatello19a.pdf</li>
<li>Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations, https://proceedings.mlr.press/v97/locatello19a.html</li>
<li>Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations - The VITALab website, https://vitalab.github.io/article/2019/05/16/unsupervised-disentanglement.html</li>
<li>Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations (@ ICML 2019) - Gabriel Poesia, https://gpoesia.com/notes/challenging-common-assumptions-in-the-unsupervised-learning-of-disentangled-representations/</li>
<li>Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations | Request PDF - ResearchGate, https://www.researchgate.net/publication/329305667_Challenging_Common_Assumptions_in_the_Unsupervised_Learning_of_Disentangled_Representations</li>
<li>Graph-based Unsupervised Disentangled Representation Learning via Multimodal Large Language Models - NIPS, https://proceedings.neurips.cc/paper_files/paper/2024/file/bac4d92b3f6decfe47eab9a5893dd1f6-Paper-Conference.pdf</li>
<li>NeurIPS Poster Graph-based Unsupervised Disentangled Representation Learning via Multimodal Large Language Models, https://neurips.cc/virtual/2024/poster/94595</li>
<li>RSS 2019 : Robotics: Science and Systems - WikiCFP, http://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=82065</li>
<li>Overview - Robotics: Science and Systems, https://roboticsconference.org/program/overview/</li>
<li>Best Paper Award - RSS Foundation, https://roboticsfoundation.org/awards/best-paper-award/</li>
<li>Science and Systems XV - Online Proceedings - Robotics, https://www.roboticsproceedings.org/rss15/</li>
<li>A Magnetically-Actuated Untethered Jellyfish- Inspired … - Robotics, https://roboticsproceedings.org/rss15/p13.pdf</li>
<li>Magnetic-field-induced propulsion of jellyfish-inspired soft robotic swimmers | Phys. Rev. E, https://link.aps.org/doi/10.1103/PhysRevE.107.014607</li>
<li>Review of Magnetic Shape Memory Polymers and Magnetic Soft Materials - TU Delft Research Portal, https://pure.tudelft.nl/ws/files/98347611/magnetochemistry_07_00123.pdf</li>
<li>Small-Scale Magnetic Robotic Systems with Medical Imaging Modalities - CityUHK Scholars, https://scholars.cityu.edu.hk/files/89042832/micromachines_12_01310.pdf</li>
<li>Advances in Magnetically Controlled Medical Robotics: A Review of …, https://pmc.ncbi.nlm.nih.gov/articles/PMC12114355/</li>
<li>Advancements in Micro/Nanorobots in Medicine: Design, Actuation, and Transformative Application | ACS Omega, https://pubs.acs.org/doi/10.1021/acsomega.4c09806</li>
<li>Advancements in Micro/Nanorobots in Medicine: Design, Actuation, and Transformative Application - ResearchGate, https://www.researchgate.net/publication/388677415_Advancements_in_MicroNanorobots_in_Medicine_Design_Actuation_and_Transformative_Application</li>
<li>(PDF) Clinically Ready Magnetic Microrobots for Targeted Therapies, https://www.researchgate.net/publication/388232171_Clinically_Ready_Magnetic_Microrobots_for_Targeted_Therapies</li>
<li>Magnetically Controlled Microrobots for In Vivo Non-Invasive Embryo Transfer - bioRxiv, https://www.biorxiv.org/content/biorxiv/early/2025/09/30/2025.09.29.679217.full.pdf</li>
<li>Multifunctional microrobot with real-time visualization and magnetic resonance imaging for chemoembolization therapy of liver cancer - Johns Hopkins University, https://pure.johnshopkins.edu/en/publications/multifunctional-microrobot-with-real-time-visualization-and-magne</li>
<li>Tumbling Microrobots for in vivo Targeted Drug Delivery - Purdue OTC, https://inventions.prf.org/innovation.html?InventionID=9646</li>
<li>Magnetically Controlled Microrobots for In Vivo Non … - bioRxiv, https://biorxiv.org/content/biorxiv/early/2025/09/30/2025.09.29.679217.full.pdf</li>
<li>On the Fairness of Disentangled Representations, http://papers.neurips.cc/paper/9603-on-the-fairness-of-disentangled-representations.pdf</li>
<li>GANs vs. Diffusion Models: In-Depth Comparison and Analysis - Sapien, https://www.sapien.io/blog/gans-vs-diffusion-models-a-comparative-analysis</li>
<li>GANs vs. Diffusion Models: Putting AI to the test | Aurora Solar, https://aurorasolar.com/blog/putting-ai-to-the-test-generative-adversarial-networks-vs-diffusion-models/</li>
<li>GANs or Diffusion Models: Key Differences Explained - DhiWise, https://www.dhiwise.com/post/gan-vs-diffusion-model</li>
<li>Comparative Analysis of GANs and Diffusion Models in Image Generation - ResearchGate, https://www.researchgate.net/publication/387444028_Comparative_Analysis_of_GANs_and_Diffusion_Models_in_Image_Generation</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>