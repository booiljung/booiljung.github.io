<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:1994년 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>1994년 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2006년 이전의 AI 및 로봇 연구 동향</a> / <span>1994년 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>1994년 AI 및 로봇 연구 동향</h1>
<h2>1. 서론: 1994년 AI 지형의 재구성</h2>
<p>1994년은 인공지능(AI)과 로봇 공학의 역사에서 단순한 한 해로 기록되지 않는다. 이 시기는 이전 수십 년간 학계를 지배해 온 기호주의적 접근 방식의 한계가 명확해지고, 그 대안으로 통계적 기계 학습이라는 새로운 패러다임이 이론적 성숙과 현실적 가능성을 동시에 입증하며 전면에 부상한 결정적 전환점이었다. 이전 시대에 뿌려진 통계적 학습, 특히 신경망과 강화 학습의 이론적 씨앗들이 1994년을 기점으로 복잡한 이론과 야심 찬 실제 응용 분야 모두에서 구체적인 결실을 보기 시작했다.</p>
<p>이러한 변화의 흐름은 당대 최고의 학술대회에서 명확히 감지되었다. 전통적인 AI의 중심이었던 제12회 미국 인공지능 학회(AAAI-94)는 질적 추론(qualitative reasoning), 사례 기반 추론(case-based reasoning), 제약 만족(constraint satisfaction)과 같은 익숙한 주제들을 여전히 다루었으나, 프로그램 위원회는 의식적으로 참여의 폭을 넓히려는 노력을 기울였다.1 그 결과, 최근 몇 년간 주류에서 벗어나 있던 유전 알고리즘과 신경망 같은 주제들이 다시금 중요한 의제로 포함되었다.1 총 780편의 논문이 제출되어 그중 222편이 채택되는 등, 학계의 역동성과 팽창하는 연구 범위를 여실히 보여주었다.1 이와 더불어 의료 인공지능, 소프트웨어 에이전트, 의사결정 이론 기반 계획, 데이터베이스 내 지식 발견(KDD) 등 다양한 주제의 워크숍과 심포지엄이 개최되어 AI의 응용 지평이 넓어지고 있음을 시사했다.2</p>
<p>1994년의 지적 지형을 형성한 주요 학술대회들은 각각 뚜렷한 역할을 수행했다.</p>
<ul>
<li>
<p><strong>AAAI-94 (미국 시애틀):</strong> 기호주의 AI의 본산이었지만, 통계적 및 준기호주의적 방법론을 적극적으로 수용하며 변화의 중심에 서 있었다.1</p>
</li>
<li>
<p><strong>NIPS 1994 (미국 덴버):</strong> 신경 정보 처리 시스템 학회(Neural Information Processing Systems)는 신경망과 기계 학습 분야의 이론적 돌파구가 탄생하는 진원지였다. 본 보고서에서 심층 분석할 다수의 핵심 논문이 바로 이곳에서 발표되었다.6</p>
</li>
<li>
<p><strong>ICRA 1994 (미국 샌디에이고) &amp; IROS 1994 (독일 뮌헨):</strong> 로봇 공학 분야의 양대 산맥인 두 학회는 AI 원리가 물리적 시스템에 적용되는 최전선을 보여주었다. 로봇 팔 제어, 이동, 항법 등 로봇 공학의 핵심 난제들이 심도 있게 다루어졌다.8</p>
</li>
</ul>
<p>본 보고서는 1994년을 관통하는 핵심 주제를 (1) 강화 학습, (2) 신경망, (3) 자율 로봇 공학의 세 축으로 나누어 분석한다. 이 세 가지 주제의 상호작용과 시너지가 어떻게 1994년을 AI 역사상 가장 중요한 변곡점 중 하나로 만들었는지 규명하는 것이 본 보고서의 목표다.</p>
<p>1994년은 AI 연구의 ’상전이(phase transition)’가 일어난 해로 평가할 수 있다. AAAI-94가 신경망과 같은 비기호주의적 접근법에 문호를 개방한 것은 주류 학계의 패러다임 전환을 공식화한 상징적 사건이었다.1 동시에 NIPS-94에서는 현대 기계 학습의 이론적 근간이 될 수학적으로 정교한 논문들이 쏟아져 나왔다.6 그리고 VaMP 자율주행 프로젝트와 같은 실제 시스템은 이러한 학습 기반 접근법이 순수 기호주의 시스템으로는 해결하기 어려웠던 복잡한 현실 세계의 문제를 해결할 수 있음을 증명했다.12 이는 단순히 여러 분야가 병렬적으로 발전한 것이 아니라, 이론과 현실이 서로를 추동하며 폭발적인 시너지를 창출한 융합의 순간이었다. NIPS의 이론은 ’어떻게(how)’를, ICRA/IROS의 현실적 과제는 ’왜(why)’를, 그리고 AAAI의 변화는 ’무엇을 향해(what’s next)’를 제시하며, AI 연구의 무게 중심이 논리 기반의 장난감 문제에서 데이터 기반의 현실 세계 문제로 이동하고 있음을 명백히 보여주었다.</p>
<p>아래 표는 본 보고서에서 심층적으로 다룰 1994년의 주요 연구 성과를 요약한 것이다. 이는 이어질 상세 분석의 길잡이 역할을 할 것이다.</p>
<p><strong>표 1: 1994년 주요 AI 및 로봇 공학 연구 요약</strong></p>
<table><thead><tr><th>연구 분야</th><th>주요 논문/프로젝트</th><th>저자/연구팀</th><th>발표 학회</th><th>핵심 기여</th></tr></thead><tbody>
<tr><td>강화 학습</td><td>Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems</td><td>T. Jaakkola, S. Singh, M. Jordan</td><td>NIPS 1994</td><td>불완전 관찰 환경(POMDP)에서 최적의 확률적 정책을 학습하는 최초의 강건한 알고리즘 제시</td></tr>
<tr><td>강화 학습</td><td>An Actor/Critic Algorithm that is Equivalent to Q-Learning</td><td>R. Crites, A. Barto</td><td>NIPS 1994</td><td>가치 기반(Q-러닝)과 정책 기반(행위자-비평가) 강화 학습 간의 이론적 등가성을 증명하고 통합 프레임워크의 기반 마련</td></tr>
<tr><td>신경망</td><td>SIMPLIFYING NEURAL NETS BY DISCOVERING FLAT MINIMA</td><td>S. Hochreiter, J. Schmidhuber</td><td>NIPS 1994</td><td>오차 함수의 ’평탄한 최소점’을 찾는 것이 신경망의 일반화 성능을 향상시킨다는 기하학적 접근법 제시</td></tr>
<tr><td>기계 학습</td><td>Limits on Learning Machine Accuracy Imposed by Data Quality</td><td>C. Cortes, L. Jackel, W. Chiang</td><td>NIPS 1994</td><td>데이터 자체의 노이즈와 불완전성이 학습 모델의 성능에 가하는 근본적인 한계(점근적 오차)를 정량화하는 방법론 제시</td></tr>
<tr><td>자율 로봇 공학</td><td>VaMP (Versuchsfahrzeug für autonome Mobilität und Rechnersehen)</td><td>Ernst Dickmanns (Bundeswehr University Munich)</td><td>EUREKA Prometheus Project</td><td>GPS 없이 오직 비전 시스템만으로 실제 고속도로에서 1000km 이상 자율주행(차선 변경, 추월 포함) 성공</td></tr>
<tr><td>로봇 학습</td><td>Efficient reinforcement learning of navigation strategies in an autonomous robot</td><td>J. Millán, C. Torras</td><td>IROS 1994</td><td>실제 로봇에 강화 학습을 적용하여 미지의 환경에서 항법 전략을 효율적으로 학습하는 방법론 시연</td></tr>
<tr><td>로봇 제어</td><td>A Reinforcement-Learning Approach to Reactive Control Policy Design for Autonomous Robots</td><td>A. Fagg, G. Bekey</td><td>IEEE T-RA / ICRA 1994</td><td>강화 학습을 이용해 자율 로봇의 반응형 제어 정책을 설계하는 실용적 접근법 제시</td></tr>
</tbody></table>
<h2>2.  강화 학습의 도약 - 이론적 토대와 응용의 확장</h2>
<p>1994년은 강화 학습(Reinforcement Learning, RL) 분야가 이론적 깊이를 더하고 응용 가능성을 폭발적으로 확장한 해였다. 이전까지의 연구가 주로 완전한 정보가 주어진 이상적인 환경, 즉 마르코프 결정 과정(Markov Decision Process, MDP)에 국한되었다면, 1994년 NIPS 학회에서는 이러한 이상적 가정을 탈피하여 현실 세계의 불확실성을 정면으로 다루는 기념비적인 연구들이 발표되었다. 이는 강화 학습을 실험실 수준의 이론에서 실제 로봇과 같은 물리적 시스템에 적용 가능한 도구로 변모시키는 결정적인 단계였다.</p>
<h3>2.1  불완전한 세상에서의 의사결정: POMDP 문제의 해법</h3>
<p>전통적인 강화 학습 알고리즘의 가장 큰 제약은 ’마르코프 가정(Markov assumption)’이었다. 이는 에이전트가 현재 상태에 대한 모든 정보를 완벽하게 인지하고 있으며, 이 정보만으로 최적의 다음 행동을 결정할 수 있다고 가정한다. 하지만 현실 세계에서 로봇이나 에이전트는 제한적이고 노이즈가 섞인 센서 데이터를 통해 세상을 인식하므로, 실제 상태를 완벽히 알 수 없는 경우가 대부분이다. 이러한 환경을 부분 관찰 마르코프 결정 과정(Partially Observable Markov Decision Process, POMDP)이라고 한다.</p>
<p>1994년 NIPS에서 Tommi Jaakkola, Satinder P. Singh, Michael I. Jordan이 발표한 “부분 관찰 마르코프 결정 문제들을 위한 강화 학습 알고리즘(Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems)“은 이 근본적인 문제를 해결하기 위한 중요한 이론적 돌파구를 제시했다.14 이들의 연구 이전에는 마르코프 가정이 깨지는 순간, 기존의 강화 학습 알고리즘과 그 수렴성 분석은 대부분 무용지물이 되었다.15</p>
<p>Jaakkola 연구팀의 핵심 아이디어는 결정론적 정책(deterministic policy)의 한계를 인식하고, ‘확률적 정책(stochastic policy)’ 공간에서 직접 해를 찾는 것이었다.18 결정론적 정책은 특정 관찰 값에 대해 항상 동일한 행동을 선택하지만, POMDP 환경에서는 서로 다른 실제 상태가 동일한 관찰 값을 야기할 수 있으므로 이는 차선책이 될 수 있다. 예를 들어, 로봇이 문 앞에 있는지 벽 앞에 있는지 센서 값만으로는 불분명할 때, ’전진’이라는 결정론적 행동은 치명적인 결과를 낳을 수 있다. 반면 확률적 정책은 ’70% 확률로 정지하고, 30% 확률로 후진’과 같이 불확실성을 행동 결정에 내재화한다. 연구팀은 이러한 확률적 정책이 POMDP 환경에서 그 어떤 결정론적 정책보다도 월등히 높은 기대 보상을 얻을 수 있음을 보였다.15</p>
<p>그들은 이 아이디어를 구현하기 위해 몬테카를로(Monte-Carlo) 방식의 정책 평가와 정책 개선을 결합한 새로운 알고리즘을 제안했다. 이 알고리즘은 지역 최적해(local maximum)로의 수렴을 보장하며, 확률적 정책 공간이 연속적임에도 불구하고 계산적으로 다루기 용이하다는 장점을 가졌다.14</p>
<p>이 접근법의 중심에는 표준 MDP의 가치 함수를 POMDP 환경으로 일반화한 개념이 있다. 특정 관찰(논문에서는 ‘메시지’ <span class="math math-inline">m</span>)이 주어졌을 때의 가치 함수 <span class="math math-inline">V(m)</span>은, 해당 관찰이 발생할 수 있는 모든 실제 상태 <span class="math math-inline">s</span>의 가치 함수 <span class="math math-inline">V(s)</span>에 대한 기댓값으로 정의된다. 이는 수식으로 다음과 같이 표현된다 15:</p>
<p><span class="math math-display">
V(m) = E \{V(s)\vert s \sim m\} = \sum_{s \in S} P(s\vert m)V(s)
</span><br />
여기서 <span class="math math-inline">P(s\vert m)</span>은 관찰 <span class="math math-inline">m</span>이 주어졌을 때 실제 상태가 <span class="math math-inline">s</span>일 조건부 확률이다. 이 정의는 에이전트가 자신의 불완전한 인지를 바탕으로 기대 가치를 계산하고 최적의 행동 확률 분포를 찾아 나갈 수 있는 수학적 토대를 제공한다.</p>
<p>이 논문의 진정한 의의는 단순히 수학적 기교를 넘어선다. 확률적 정책의 도입은 불확실성을 다루는 방식에 대한 근본적인 인식의 전환을 의미했다. 현실의 모든 로봇은 카메라, 라이다, 소나와 같은 불완전한 센서를 통해 환경과 상호작용하며, 이는 본질적으로 POMDP 문제에 해당한다. Jaakkola 등의 연구는 에이전트가 세상의 정확한 상태를 알지 못함에도 불구하고 합리적으로 행동할 수 있는 이론적 근거와 알고리즘적 틀을 제공했다. 이는 강화 학습이 진정한 자율성을 갖춘 물리적 시스템으로 나아가는 데 있어 필수적인 선결 조건이었으며, 이후 로봇 공학 분야의 강화 학습 연구에 지대한 영향을 미쳤다.</p>
<h3>2.2  행위자-비평가와 Q-러닝의 접점: 통합적 프레임워크의 구축</h3>
<p>1994년 강화 학습 분야의 또 다른 중요한 성과는 서로 다른 계열의 알고리즘들을 통합하려는 이론적 시도에서 나왔다. 당시 강화 학습은 크게 두 가지 흐름으로 나뉘어 있었다. 하나는 특정 상태에서 특정 행동을 했을 때의 미래 가치(Q-value)를 학습하는 ‘가치 기반(value-based)’ 접근법으로, Q-러닝(Q-learning)이 대표적이다. 다른 하나는 가치 함수를 거치지 않고 상태로부터 최적의 행동을 선택하는 정책(policy)을 직접 학습하는 ‘정책 기반(policy-based)’ 접근법으로, 행위자-비평가(Actor-Critic) 구조가 이에 해당한다.</p>
<p>NIPS 1994에서 Robert H. Crites와 Andrew G. Barto가 발표한 “Q-러닝과 등가인 행위자-비평가 알고리즘(An Actor/Critic Algorithm that is Equivalent to Q-Learning)“은 이 두 흐름을 연결하는 형식적 교량을 구축한 기념비적인 연구였다.19 이들은 Q-러닝과 수학적으로 완전히 동일하게 작동하는 새로운 형태의 행위자-비평가 알고리즘을 ‘구축을 통해(by construction)’ 증명해냈다.20</p>
<p>이 등가성을 달성하기 위한 핵심 아이디어는 Q-값을 행위자(actor)의 정책과 비평가(critic)의 가치 함수 내에 암시적으로 인코딩하는 것이었다.19 그 결과로 탄생한 알고리즘은 기존의 행위자-비평가 방법론과 두 가지 중요한 차이점을 보였다.</p>
<ol>
<li>
<p><strong>제한적인 비평가 업데이트:</strong> 비평가는 상태 <span class="math math-inline">x</span>의 가치 함수 추정치 <span class="math math-inline">V(x)</span>를 모든 행동에 대해 업데이트하는 것이 아니라, 오직 해당 상태에서 가장 확률이 높은 행동이 실행되었을 때만 업데이트한다.20 이는 비평가가 현재 정책의 가치가 아닌, 최적 정책의 가치 함수 <span class="math math-inline">V^*</span>를 직접 추정하도록 유도하는 효과를 낳는다.</p>
</li>
<li>
<p><strong>차별화된 행위자 보상:</strong> 행위자는 단순히 시간차 오차(TD-error)에 의해서만 보상받는 것이 아니라, 실행된 행동의 ’상대적 확률’에 의존하는 기준에 따라 보상을 받는다.20 이는 더 미세하고 차별화된 피드백을 통해 정책 개선을 유도한다.</p>
</li>
</ol>
<p>이 수정된 행위자-비평가 알고리즘의 업데이트 방정식은 이러한 개념을 명확히 보여준다. 상태 <span class="math math-inline">x</span>에서 행동 <span class="math math-inline">i</span>를 실행하여 보상 <span class="math math-inline">r</span>과 다음 상태 <span class="math math-inline">y</span>를 관찰했을 때, 알고리즘은 다음과 같이 진행된다.</p>
<ul>
<li>
<p>시간차 오차(TD-Error) 계산:</p>
<p><span class="math math-display">
e = [r + \gamma V(y)] - H_{V(x)}(Z_i)
</span><br />
여기서 <span class="math math-inline">Z_i = P_i / P_{max}</span>는 실행된 행동 <span class="math math-inline">i</span>의 확률 <span class="math math-inline">P_i</span>와 가장 확률 높은 행동의 확률 <span class="math math-inline">P_{max}</span> 사이의 비율이며, <span class="math math-inline">H</span>는 Q-값과 정책/가치 함수를 변환하는 가역 함수다.21</p>
</li>
<li>
<p>행위자(정책) 업데이트:</p>
</li>
</ul>
<p>시간차 오차 <span class="math math-inline">e</span>의 부호에 따라 행동 <span class="math math-inline">i</span>의 확률을 높이거나 낮춘다. 이 과정은 새로운 중간 확률 비율 <span class="math math-inline">Z&#39;_i</span>를 계산하고, 전체 확률의 합이 1이 되도록 정규화하는 두 단계로 이루어진다.21</p>
<p><span class="math math-display">
  Z&#39;_i = H^{-1}_{V(x)}[H_{V(x)}(Z_i) + \alpha_{xi}(n) e]
</span></p>
<ul>
<li>비평가(가치 함수) 업데이트:</li>
</ul>
<p>실행된 행동 <span class="math math-inline">i</span>가 가장 확률이 높은 행동이었을 경우에만 가치 함수를 업데이트한다.21</p>
<p><span class="math math-display">
  V(x) \leftarrow V(x) + \alpha_{xi}(n) e
</span></p>
<p>이 논문의 진정한 가치는 단순히 등가성을 증명한 데 있지 않다. 그 등가성을 분석의 렌즈로 삼아 두 알고리즘 계열 모두를 더 깊이 이해하게 했다는 점에 있다. 이 연구 이전까지 행위자-비평가와 Q-러닝은 각기 다른 장단점을 가진 별개의 접근법으로 여겨졌다. Crites와 Barto는 행위자-비평가 알고리즘에 어떤 수정을 가해야 Q-러닝처럼 작동하는지를 명확히 보여줌으로써, 두 방법론 사이의 경직된 벽을 허물고 스펙트럼의 양 끝단으로 재해석할 여지를 열었다.</p>
<p>결론적으로, 이 연구는 강화 학습 분야의 이론적 통합을 이룬 중요한 업적이다. 이는 정책 기반 방법과 가치 기반 방법의 구분이 절대적이지 않음을 보여주었으며, 두 프레임워크 간의 개념을 번역할 수 있는 ’로제타석’을 제공했다. 이로써 강화 학습 이론의 전체적인 이해를 풍부하게 하고, 훗날 두 방법론의 장점을 결합한 하이브리드 알고리즘(예: DDPG, SAC)의 등장을 위한 이론적 토대를 마련했다.</p>
<h3>2.3  이론에서 현실로: 자율 로봇 제어를 위한 강화 학습</h3>
<p>NIPS에서 발표된 추상적이고 이론적인 연구들은 같은 해 ICRA와 IROS 같은 주요 로봇 공학 학회에서 물리적 시스템에 접목하려는 노력으로 이어졌다. 이는 강화 학습 이론을 현실 세계의 구체적인 문제 해결에 적용하려는 시도가 얼마나 즉각적이고 활발했는지를 보여준다.</p>
<p>IROS 1994에서 발표된 Millán과 Torras의 “자율 로봇의 항법 전략에 대한 효율적인 강화 학습(Efficient reinforcement learning of navigation strategies in an autonomous robot)“과 같은 연구는 이러한 흐름을 대표한다.11 또한, 같은 해 <em>IEEE Transactions on Robotics and Automation</em> 저널과 ICRA 1994 학회에서 발표된 Fagg과 Bekey의 연구 “자율 로봇을 위한 반응형 제어 정책 설계를 위한 강화 학습 접근법(A Reinforcement-Learning Approach to Reactive Control Policy Design for Autonomous Robots)“은 강화 학습을 이용해 로봇의 반응형 제어 정책을 직접 설계하는 방법을 제시했다.23</p>
<p>이러한 연구들은 실제 로봇에 강화 학습을 적용할 때 발생하는 현실적인 난제들을 다루었다. 이론적인 연구들이 주로 다루는 작고 이산적인 상태-행동 공간과 달리, 실제 로봇은 고차원의 연속적이고 노이즈가 섞인 센서 입력을 처리해야 한다. 1990년대의 제한된 컴퓨팅 파워로는 이론적 알고리즘을 그대로 구현하는 것이 불가능했다.</p>
<p>따라서 당시 로봇 공학 연구자들은 이론을 단순히 ’적용’하는 것을 넘어, 현실에 맞게 ’변형’하고 ’근사’하는 창의적인 공학적 해법을 모색해야 했다. 그들은 신경망이나 퍼지 논리(fuzzy logic)와 같은 도구를 함수 근사기(function approximator)로 활용하여, 연속적인 상태 공간에서 가치 함수나 정책을 표현하고자 했다.25 이는 강화 학습 알고리즘을 다루기 쉬운 형태로 변환하여 실제 로봇에 탑재하기 위한 필수적인 과정이었다.</p>
<p>1994년의 로봇 공학 논문들은 당시 강화 학습 이론과 실제 응용 사이의 거대한 간극을 명확히 보여준다. 동시에, 그 간극을 메우기 위한 연구자들의 창의적인 노력을 증언한다. 이들은 강화 학습을 위한 함수 근사의 선구자들이었으며, 그들의 연구는 AI 분야의 중요한 주제, 즉 이론과 응용의 공진화(co-evolution)를 잘 보여준다. 현실 세계의 제약이 함수 근사와 같은 새로운 이론적 도구의 개발을 촉진하고, 이렇게 개발된 도구는 다시 더 정교한 응용을 가능하게 하는 선순환 구조가 이때부터 형성되기 시작했다.</p>
<h2>3.  신경망의 심화 - 일반화, 복잡성, 그리고 데이터의 한계</h2>
<p>1994년은 신경망 연구가 단순히 성능 향상을 넘어, ’일반화(generalization)’라는 근본적인 문제에 대해 깊이 파고든 시기였다. 훈련 데이터에 대해서는 완벽하게 작동하지만, 한 번도 본 적 없는 새로운 데이터에 대해서는 형편없는 성능을 보이는 과적합(overfitting) 문제는 신경망의 실용화를 가로막는 가장 큰 장벽이었다. 이 시기의 연구자들은 신경망의 복잡성을 어떻게 제어하고, 데이터 자체가 가진 내재적 한계는 무엇인지를 규명함으로써 이 문제에 대한 해법을 모색했다.</p>
<h3>3.1  단순함의 미학: ‘평탄한 최소점’ 발견을 통한 신경망 일반화</h3>
<p>신경망 일반화 문제에 대한 가장 독창적이고 선구적인 접근법 중 하나는 NIPS 1994에서 Sepp Hochreiter와 Jürgen Schmidhuber가 발표한 “평탄한 최소점 발견을 통한 신경망 단순화(SIMPLIFYING NEURAL NETS BY DISCOVERING FLAT MINIMA)“라는 논문에서 제시되었다.26 이 연구는 오차 함수 표면(error surface)의 기하학적 특성을 분석하는 새로운 관점을 도입하여 일반화 성능을 향상시키는 방법을 제안했다.</p>
<p>이들의 핵심 주장은, 신경망 훈련 과정에서 도달하는 오차 함수의 최소점(minimum)의 ’모양’이 그 지점의 오차 값 자체보다 일반화 성능에 더 중요하다는 것이다. 구체적으로, 이들은 가중치 공간(weight space)에서 오차가 거의 변하지 않는 넓고 평탄한 영역인 ’평탄한 최소점(flat minimum)’을 찾는 것이 중요하다고 역설했다.27 반면, 조금만 가중치가 바뀌어도 오차가 급격히 변하는 좁고 뾰족한 ’날카로운 최소점(sharp minimum)’은 과적합의 원인이 된다.</p>
<p>이러한 주장의 이론적 근거는 최소 기술 길이(Minimum Description Length, MDL) 원리에서 찾을 수 있다.26 날카로운 최소점에 해당하는 가중치는 매우 높은 정밀도로 특정되어야 그 성능을 유지할 수 있다. 반면, 평탄한 최소점의 가중치는 비교적 낮은 정밀도로 기술되어도 성능이 크게 저하되지 않는다. MDL 원리에 따르면, 모델을 기술하는 데 필요한 정보의 양(비트 수)이 적을수록, 즉 모델이 단순할수록 더 나은 일반화 성능을 기대할 수 있다. 따라서 평탄한 최소점을 찾는 것은 본질적으로 더 단순하고 강건한(robust) 네트워크를 찾는 것과 같다.</p>
<p>Hochreiter와 Schmidhuber는 이 아이디어를 구현하기 위해 새로운 알고리즘을 제안했다. 이 알고리즘은 훈련 오차뿐만 아니라 가중치의 정밀도, 즉 최소점의 ’날카로움’까지 함께 최소화하는 것을 목표로 한다. 이를 위해 헤시안(Hessian) 행렬, 즉 2차 미분 정보가 필요했지만, 효율적인 계산 기법을 통해 역전파(backpropagation) 알고리즘과 동일한 계산 복잡도 수준을 유지하는 데 성공했다.26</p>
<p>알고리즘이 최소화하는 목적 함수 <span class="math math-inline">E(w, D_0)</span>는 다음과 같이 정의된다 28:</p>
<p><span class="math math-display">
E(w, D_0) = E_Q(w, D_0) + \lambda B(w, D_0)
</span><br />
여기서 <span class="math math-inline">E_Q(w, D_0)</span>는 평균 제곱 오차(Mean Squared Error)와 같은 표준적인 훈련 오차이며, <span class="math math-inline">B(w, D_0)</span>는 최소점의 ’평탄도’와 관련된 복잡도 페널티 항이다. 이 페널티 항은 날카로운 최소점에 높은 비용을 부과하여 최적화 과정이 자연스럽게 평탄한 지역으로 유도되도록 설계되었다.</p>
<p>이 연구는 발표 당시에도 주목받았지만, 그 진정한 가치는 수십 년이 지난 현대 딥러닝 시대에 와서야 온전히 재평가되고 있다. 1990년대에 과적합을 제어하는 주된 방법은 가중치 감쇠(weight decay)와 같이 가중치의 크기를 제한하거나, 가지치기(pruning)를 통해 네트워크 구조를 단순화하는 대수적이거나 구조적인 접근이었다. 반면 Hochreiter와 Schmidhuber는 문제의 본질을 ’기하학’에서 찾았다. 그들은 최적화 알고리즘이 찾아낸 가중치 값 자체가 아니라, 그 가중치가 위치한 손실 지형(loss landscape)의 형태가 중요하다고 주장했다. 평탄한 최소점은 본질적으로 더 강건하다. 테스트 데이터의 노이즈나 가중치의 양자화(quantization) 등으로 인해 가중치에 작은 변화가 생기더라도 네트워크의 출력은 크게 변하지 않기 때문이다.</p>
<p>이러한 통찰은 오늘날 딥러닝 연구의 가장 뜨거운 주제 중 하나인 손실 지형 분석의 직접적인 지적 조상이다. 예를 들어, 대규모 배치(large-batch) 훈련이 왜 종종 더 날카로운 최소점을 찾아 일반화 성능을 저해하는지, 혹은 특정 옵티마이저나 배치 정규화(batch normalization) 같은 기법이 왜 더 평탄한 지역으로 탐색을 유도하여 성능을 향상시키는지를 논의하는 현대의 연구들은 모두 1994년 이 논문에서 제시된 개념에 뿌리를 두고 있다. 이는 시대를 20년 이상 앞서간 패러다임 전환적 통찰이었으며, 학계가 그 의미를 완전히 이해하고 확장하는 데는 오랜 시간이 걸렸다.</p>
<h3>3.2  학습의 근본적 한계: 데이터 품질과 점근적 오차</h3>
<p>실용적인 기계 학습 시스템을 구축할 때 마주치는 또 다른 근본적인 질문은 ’모델의 성능이 나쁜 것이 모델 자체의 문제인가, 아니면 데이터의 한계 때문인가?’이다. NIPS 1994에서 Corinna Cortes, L. D. Jackel, Wan-Ping Chiang이 발표한 “데이터 품질에 의해 부과되는 학습 기계 정확도의 한계(Limits on Learning Machine Accuracy Imposed by Data Quality)“는 이 질문에 답할 수 있는 엄밀한 프레임워크를 제시했다.30</p>
<p>이 논문의 핵심 주장은, 데이터베이스에 포함된 무작위 오류, 측정 부정확성, 레이블 오류, 정보 부족 등은 그 데이터를 사용해 훈련된 그 어떤 분류기(classifier)의 성능에도 극복할 수 없는 상한선을 설정한다는 것이다.31 연구팀은 이 이론적 성능 한계를 추정하는 방법론을 제안하고, 이를 ‘점근적 오차(asymptotic error)’, 즉 <span class="math math-inline">E_{\infty}</span>라고 명명했다.</p>
<p>점근적 오차 <span class="math math-inline">E_{\infty}</span>는 특정 학습 기계(learning machine)에 무한한 양의 훈련 데이터를 제공했을 때 도달하게 되는 훈련 오차와 테스트 오차의 수렴 값으로 정의된다.33 현실에서는 무한한 데이터를 사용할 수 없지만, 훈련 데이터의 크기를 점차 늘려가면서 성능 변화를 관찰하고 그 추세를 외삽(extrapolate)함으로써 <span class="math math-inline">E_{\infty}</span>를 추정할 수 있다.</p>
<p>중요한 점은, 만약 사용된 학습 기계가 충분한 복잡도(capacity)를 가져서 어떠한 함수든 표현할 수 있는 ’보편적 근사기(universal approximator)’라면, 추정된 <span class="math math-inline">E_{\infty}</span>는 더 이상 특정 모델의 한계가 아니라 데이터 자체에 내재된 ’본질적인 노이즈 수준(intrinsic noise level)’을 나타낸다는 것이다.34 즉, 데이터가 가진 모호함과 오류 때문에 신(God)이라도 달성할 수 없는 최소 오차 수준을 의미한다.</p>
<p>이 개념은 훈련 데이터 크기(<span class="math math-inline">I</span>)와 모델 복잡도(<span class="math math-inline">h</span>) 사이의 관계를 통해 더 명확해진다.34</p>
<ul>
<li>
<p><strong>고정된 모델 복잡도에서 훈련 데이터 크기 변화:</strong> 훈련 데이터가 적을 때는 모델이 데이터를 완벽히 외워버려 훈련 오차는 0에 가깝지만 테스트 오차는 매우 높다. 데이터가 점차 많아지면 모델은 데이터의 일반적인 경향을 학습하게 되어 테스트 오차는 감소하고 훈련 오차는 증가하기 시작한다. 데이터 크기 <span class="math math-inline">I</span>가 무한대에 가까워지면, 두 오차는 동일한 값 <span class="math math-inline">E_{\infty}</span>로 수렴한다.</p>
</li>
<li>
<p><strong>무한한 데이터에서 모델 복잡도 변화:</strong> 모델 복잡도가 낮으면 데이터의 패턴을 제대로 표현하지 못해 <span class="math math-inline">E_{\infty}</span>가 높다. 복잡도를 점차 높이면 모델이 데이터의 실제 함수를 더 잘 근사하게 되어 <span class="math math-inline">E_{\infty}</span>는 감소한다. 모델의 복잡도가 데이터의 실제 복잡도를 표현하기에 충분해지는 순간부터는, 복잡도를 더 높여도 <span class="math math-inline">E_{\infty}</span>는 더 이상 감소하지 않고 일정한 값에 수렴하는데, 이 값이 바로 데이터의 본질적인 노이즈 수준이다.</p>
</li>
</ul>
<p>이 연구는 기계 학습 프로젝트에서 자원을 배분하는 데 매우 중요한 실용적 지침을 제공한다. 모델의 성능이 데이터의 본질적 한계인 <span class="math math-inline">E_{\infty}</span>에 이미 근접해 있다면, 모델 구조를 개선하거나 더 좋은 옵티마이저를 찾는 데 시간을 쏟는 것은 무의미하다. 성능을 향상시킬 유일한 방법은 더 깨끗한 레이블을 얻거나 더 유용한 특징(feature)을 추출하는 등 데이터 자체의 품질을 개선하는 것뿐이다.</p>
<p>이러한 분석은 현대 AI 분야에서 강조되는 ‘데이터 중심 AI(Data-Centric AI)’ 운동의 이론적, 방법론적 토대를 제공한다. “쓰레기가 들어가면 쓰레기가 나온다(Garbage in, garbage out)“는 직관을 수학적으로 정식화하고, 데이터 정제와 특징 공학의 ’기술(art)’을 데이터 품질 자체를 측정하고 데이터에 대한 투자가 모델링에 대한 투자보다 더 높은 수익을 낼 시점을 알려주는 ’과학(science)’으로 전환시켰다.</p>
<h3>3.3  시대적 맥락: 서포트 벡터 머신(SVM)의 태동</h3>
<p>1994년 신경망 연구가 손실 지형의 기하학과 데이터의 내재적 한계에 집중하고 있을 때, 또 다른 강력한 학습 패러다임이 조용히 완성되고 있었다. 바로 블라디미르 바프닉(Vladimir Vapnik)과 그의 동료들이 개발한 서포트 벡터 머신(Support Vector Machine, SVM)이다. 비록 SVM의 핵심 논문은 1995년에 발표되었지만, 그 이론적 기반은 1994년을 전후하여 확립되었다.35</p>
<p>SVM은 데이터를 분류하는 최적의 초평면(hyperplane)을 찾는 지도 학습 알고리즘으로, 각 클래스에 속한 가장 가까운 데이터 포인트들(서포트 벡터) 사이의 거리, 즉 마진(margin)을 최대화하는 것을 목표로 한다.35</p>
<p>SVM의 부상과 NIPS에서 논의된 일반화 연구는 동전의 양면과 같다. 1990년대 중반 기계 학습 커뮤니티의 최대 화두는 ’어떻게 과적합을 피하고 일반화 성능을 높일 것인가’였다. Hochreiter와 Schmidhuber는 이 문제를 손실 지형의 ’평탄함’이라는 기하학적 관점에서 접근했다. 이와 동시에, Vapnik과 Cortes는 ‘구조적 위험 최소화(Structural Risk Minimization)’ 원리와 결정 경계의 ’마진 최대화’라는 또 다른 기하학적 관점에서 동일한 문제에 대한 해법을 제시했다.</p>
<p>두 접근법 모두 훈련 데이터를 단순히 잘 맞추는 것을 넘어, 모델의 복잡도를 제어하여 단순성과 강건성을 선호하도록 만드는 내재적 메커니즘을 구축하려는 시도였다. 1994년의 지적 시대정신은 일반화를 달성하기 위한 원리적인 방법을 깊이 탐구하는 것이었고, SVM은 이후 10여 년간 이 탐구의 가장 성공적이고 지배적인 구현체로 자리 잡았다. 그 이유는 SVM이 이 원리를 직접적으로 구현하면서도, 유일한 최적해를 보장하는 볼록 최적화(convex optimization) 문제로 귀결된다는 강력한 수학적 장점을 가졌기 때문이다.</p>
<h2>4.  자율 로봇 공학의 이정표 - 비전 기반 주행과 지능형 시스템</h2>
<p>1994년은 추상적인 학습 이론이 물리적 세계와 만나 경이로운 성과를 창출한 해이기도 했다. 특히, 인간의 개입 없이 복잡한 실제 환경을 인식하고 판단하여 작업을 수행하는 자율 로봇 공학 분야에서 역사적인 이정표가 세워졌다. 독일의 Ernst Dickmanns 교수가 이끈 VaMP 프로젝트는 당시 기술 수준으로는 불가능하다고 여겨졌던 비전 기반 자율주행을 성공시키며 전 세계에 큰 충격을 주었고, ICRA와 IROS 같은 학회에서는 지능형 로봇 시스템을 구성하는 기반 기술들이 착실히 발전하고 있었다.</p>
<h3>4.1  도로 위를 달리는 지능: Ernst Dickmanns의 VaMP 프로젝트</h3>
<p>1994년 10월, 프랑스 파리 샤를 드골 공항 인근의 A1 고속도로에서 역사적인 시연이 펼쳐졌다. 유럽의 범국가적 연구 프로젝트인 ’EUREKA 프로메테우스(PROMETHEUS)’의 최종 발표회에서, Ernst Dickmanns 교수팀이 개발한 두 대의 메르세데스-벤츠 세단, VaMP(Versuchsfahrzeug für autonome Mobilität und Rechnersehen)와 VITA-2가 그 주인공이었다.12 이 차량들은 안전 운전자와 손님들을 태운 채, 일반 차량들이 붐비는 실제 교통 상황 속에서 1,000 km가 넘는 거리를 스스로 주행했다.12 최고 시속 130 km의 속도로 차선을 유지하고, 필요시 자율적으로 차선을 변경하며 다른 차량을 추월하는 모습은 당시로서는 공상 과학의 한 장면이었다.37</p>
<p>VaMP 프로젝트의 성공은 현대의 자율주행차가 의존하는 막대한 컴퓨팅 파워나 라이다(Lidar), GPS와 같은 첨단 센서 없이 이루어졌다는 점에서 더욱 놀랍다. 이 시스템은 오직 ‘비전(vision)’ 시스템, 즉 카메라에만 의존했다.12 Dickmanns 교수팀의 성공 비결은 하드웨어의 한계를 뛰어넘는 알고리즘의 독창성에 있었다.</p>
<p>그들의 핵심 기술은 ‘동적 컴퓨터 비전(dynamic computer vision)’ 또는 ’4D 접근법’이라 불리는 혁신적인 패러다임이었다.12 이는 단순히 2D 이미지 프레임을 순차적으로 처리하는 것을 넘어, 3차원 공간과 시간의 흐름 속에서 객체(도로, 차선, 다른 차량 등)를 모델링하고 그 변화를 예측하는 방식이다. 시스템은 픽셀 단위의 무차별적인 정보 처리 대신, 인간의 시각 시스템처럼 ‘주의 집중(attention control)’ 메커니즘을 사용했다. 전방과 후방에 각각 장착된 두 쌍의 카메라는 인간의 눈처럼 빠르게 움직이며(인공적 단속 운동, artificial saccadic movements) 차선이나 앞차와 같이 주행에 가장 중요한 정보에 초점을 맞췄다.12 이렇게 얻어진 희소한 시각 정보는 확장 칼만 필터(extended Kalman filter)를 통해 노이즈가 제거되고, 차량의 상대적 위치와 속도 등 강건한 상태 추정치로 변환되었다.12</p>
<p>이 모든 복잡한 연산은 당시 기준으로 최첨단 병렬 컴퓨터였던 60개의 ‘트랜스퓨터(transputer)’ 네트워크 위에서 실시간으로 수행되었다.12 이 컴퓨터 시스템은 차량 트렁크와 뒷좌석 일부를 가득 채울 정도의 크기였다.39</p>
<p>VaMP 프로젝트는 자율주행 기술의 역사를 다시 쓰게 만든 사건이다. 흔히 자율주행 연구의 본격적인 시작을 2005년의 DARPA 그랜드 챌린지로 기억하지만, VaMP는 그보다 10년 이상 앞서 고속도로 자율주행의 핵심 기능이 구현 가능함을 증명했다. 이는 AI의 발전이 단순히 컴퓨팅 성능의 확장에만 의존하는 것이 아님을 보여주는 강력한 사례다. 깊이 있는 영역 지식과 문제에 대한 창의적인 접근법이 하드웨어의 엄청난 제약을 극복할 수 있음을 입증한 것이다. VaMP의 성공은 전 세계의 자동차 제조사와 연구 기관들에게 큰 영감을 주었으며, 이후 자율주행 연구의 방향성과 자금 지원 결정에 지대한 영향을 미쳤다. 아래 표는 VaMP 시스템과 현대 자율주행 기술을 비교하여 그 시대의 기술적 제약과 알고리즘적 성취를 명확히 보여준다.</p>
<p><strong>표 2: VaMP 자율주행 시스템과 현대 기술 비교</strong></p>
<table><thead><tr><th>기술 요소</th><th>VaMP (1994)</th><th>현대 자율주행차 (Typical Modern AV)</th></tr></thead><tbody>
<tr><td><strong>주 센서</strong></td><td>카메라 (흑백, 저해상도)</td><td>카메라 (고해상도, 컬러), 라이다(Lidar), 레이더(Radar)</td></tr>
<tr><td><strong>내비게이션</strong></td><td>비전 기반 차선 및 도로 인식 (GPS 없음)</td><td>GPS/IMU, HD 맵, 센서 퓨전</td></tr>
<tr><td><strong>컴퓨팅 하드웨어</strong></td><td>60개의 트랜스퓨터 (병렬 프로세서)</td><td>멀티코어 CPU, 고성능 GPU/TPU</td></tr>
<tr><td><strong>핵심 알고리즘</strong></td><td>4D 동적 비전, 주의 집중, 확장 칼만 필터</td><td>딥러닝 기반 객체 인식 및 분할, 센서 퓨전, 경로 계획</td></tr>
<tr><td><strong>성능</strong></td><td>시속 130 km로 1000 km 이상 주행, 차선 변경 및 추월</td><td>모든 도로 조건에서 완전 자율주행 목표 (레벨 4/5)</td></tr>
</tbody></table>
<p>이 비교는 VaMP 팀이 얼마나 열악한 조건 속에서 알고리즘과 개념적 독창성만으로 위대한 성과를 이루었는지를 극명하게 드러낸다. 이는 자율주행 기술의 상용화 과정이 순수한 연구 개발뿐만 아니라 경제성, 신뢰성, 사회적 수용성과 규제 문제와도 깊이 연관되어 있음을 시사한다.</p>
<h3>4.2  지능형 로봇 시스템의 확장: ICRA &amp; IROS 1994 동향</h3>
<p>VaMP와 같은 거대 프로젝트가 스포트라이트를 받는 동안, 1994년의 ICRA와 IROS 학회에서는 지능형 로봇 시스템을 구성하는 근간이 되는 연구들이 꾸준히 발표되고 있었다. 이 연구들은 자율성의 ’뇌’에 해당하는 학습 이론이 실제 세상에 적용되기 위해 반드시 필요한 로봇의 ’몸’을 단련시키는 과정이었다.9</p>
<p>두 학회의 발표 목록을 살펴보면, 당시 로봇 공학계의 연구가 지능의 물리적 구현(embodiment)과 관련된 핵심 문제들에 집중되어 있었음을 알 수 있다.9</p>
<ul>
<li>
<p><strong>이동 로봇 및 경로 계획 (Mobile Robots and Path Planning):</strong> 미지의 환경에서 이동 로봇이 충돌 없이 목표 지점까지 도달하기 위한 경로 계획, 장애물 회피, 지도 작성(map building)에 관한 연구가 주를 이루었다.9 이는 로봇이 물리적 공간을 이해하고 탐색하는 가장 기본적인 능력을 확보하기 위한 노력이다.</p>
</li>
<li>
<p><strong>매니퓰레이터 제어 (Manipulator Control):</strong> 다관절 로봇 팔, 특히 유연한(flexible) 링크를 가지거나 필요 이상의 관절을 가진 여유 자유도(redundant) 매니퓰레이터를 정밀하게 제어하는 연구가 활발했다. 위치 제어와 힘 제어를 결합하여 물체와 섬세하게 상호작용하는 기술이 중요한 주제였다.9</p>
</li>
<li>
<p><strong>센싱 및 인식 (Sensing and Perception):</strong> 카메라, 레이저 스캐너 등 다양한 센서로부터 얻은 데이터를 처리하여 의미 있는 정보(예: 객체의 특징, 위치)를 추출하는 기술은 로봇이 환경을 ’인식’하기 위한 필수적인 연구 분야였다.9</p>
</li>
<li>
<p><strong>고수준 작업 계획 (Task Planning):</strong> 단순한 이동이나 조작을 넘어, 여러 단계로 이루어진 복잡한 작업을 계획하고 스케줄링하는 고수준 지능에 대한 연구도 진행되었다.9</p>
</li>
<li>
<p><strong>새로운 응용 분야:</strong> ’장애인을 위한 이동 시스템(Mobility Systems for the Disabled)’과 같은 특별 세션은 로봇 기술이 산업 현장을 넘어 인간 중심의 실용적인 응용 분야로 확장되고 있음을 보여주었다.9</p>
</li>
</ul>
<p>NIPS에서 다루어진 학습 이론이 주로 추상적인 데이터와 수학적 모델을 다루는 ‘탈육체화된(disembodied)’ 지능에 가까웠다면, ICRA와 IROS의 연구들은 운동학, 동역학, 물리적 상호작용, 센서 노이즈 등 ‘육체화된(embodied)’ 지능의 현실적인 문제들과 씨름하고 있었다. VaMP가 이 두 세계의 환상적인 통합을 보여주었다면, 학회에서 발표된 대다수의 논문들은 ‘어떻게 로봇 팔이 물체를 부수지 않고 잡을 수 있는가?’, ‘어떻게 로봇이 복잡한 방을 통과하는 경로를 찾는가?’, ’진동하는 로봇 팔을 어떻게 제어하는가?’와 같은, 화려하지는 않지만 필수적인 하위 문제들을 해결하고 있었다.</p>
<p>이러한 기초 연구들은 로봇 공학의 ’내장’에 해당한다. 제어, 계획, 인식에 대한 강건한 해법 없이는, 아무리 뛰어난 고수준 학습 알고리즘이라도 신뢰성 있는 물리적 플랫폼 위에서 실행될 수 없다. 따라서 1994년의 발전은 두 갈래로 이루어졌다. 한편에서는 ‘뇌’(학습 이론)가 더욱 정교해지고 있었고, 다른 한편에서는 ‘몸’(로봇 하드웨어 및 제어)이 더욱 유능해지고 있었다. 이 두 흐름은 미래의 진정한 지능형 로봇 시스템으로 통합될 무대를 착실히 준비하고 있었다.</p>
<h2>5. 결론: 1994년의 유산과 미래 전망</h2>
<p>1994년은 인공지능과 로봇 공학의 역사에서 ’학습 패러다임’이 본격적으로 시대의 주류로 부상한 원년으로 기록될 자격이 충분하다. 강화 학습의 이론적 성숙, 신경망 일반화에 대한 깊이 있는 탐구, 그리고 비전 기반 자율주행의 경이로운 현실 구현은 개별적인 사건이 아니라, 하나의 거대한 지적 전환을 구성하는 상호 연결된 단면들이었다. 이 해에 제시된 개념과 알고리즘, 그리고 실증적 성과들은 이후 수십 년간 AI 분야의 연구 지형을 근본적으로 바꾸어 놓았다.</p>
<p>1994년의 유산은 오늘날의 AI 기술 곳곳에 뚜렷한 족적을 남기고 있다.</p>
<ul>
<li>
<p><strong>Jaakkola 등의 POMDP 연구</strong>는 불완전한 정보를 다루는 현대 강화 학습의 이론적 초석이 되었다. 이는 실제 로봇 공학, 금융 거래, 자원 관리 등 현실 세계의 거의 모든 강화 학습 응용 분야에 필수적인 개념으로 자리 잡았다.</p>
</li>
<li>
<p><strong>Crites와 Barto의 이론적 통합</strong>은 가치 기반 방법과 정책 기반 방법을 결합한 현대의 심층 강화 학습 하이브리드 알고리즘(예: DDPG, SAC, A3C)의 등장을 예고했다. 두 패러다임의 경계를 허문 이들의 통찰은 알고리즘 설계의 유연성을 크게 높였다.</p>
</li>
<li>
<p><strong>Hochreiter와 Schmidhuber의 ‘평탄한 최소점’</strong> 개념은 20여 년의 세월을 뛰어넘어 현대 딥러NING 연구의 핵심 화두로 부활했다. 손실 지형의 기하학적 특성을 이해하고, SAM(Sharpness-Aware Minimization)과 같은 새로운 옵티마이저를 통해 의도적으로 평탄한 최소점을 찾는 연구는 이들의 선구적인 아이디어에 직접적인 빚을 지고 있다.</p>
</li>
<li>
<p><strong>Cortes 등의 데이터 한계 분석</strong>은 모델의 성능이 데이터의 품질을 넘어설 수 없다는 사실을 정량적으로 규명함으로써, 오늘날 ‘데이터 중심 AI(Data-Centric AI)’ 운동의 이론적 토대를 마련했다. 이는 AI 개발의 초점을 모델 아키텍처에서 데이터 품질로 전환하는 중요한 계기가 되었다.</p>
</li>
<li>
<p><strong>Dickmanns의 VaMP 프로젝트</strong>는 딥러닝과 대규모 데이터 시대 이전에, 독창적인 알고리즘과 시스템 공학적 접근만으로도 자율주행의 핵심 기능 구현이 가능함을 보여준 역사적 증거다. 이는 오늘날의 자율주행차 개발 경쟁의 직접적인 선구자라 할 수 있다.</p>
</li>
</ul>
<p>결론적으로, 1994년은 AI와 로봇 공학이 해결해야 할 근본적인 질문들을 더욱 정교하게 다듬은 해였다. 모델의 복잡성과 데이터 품질 사이의 상충 관계, 불확실성 하에서의 강건한 학습 필요성, 그리고 추상적 이론과 물리적 구현 사이의 간극을 메우는 문제는 그때나 지금이나 여전히 이 분야의 핵심 과제로 남아있다. 1994년의 연구자들은 우리에게 완성된 답을 주었다기보다, 오늘날까지도 우리가 씨름하고 있는 더 깊고 본질적인 질문들을 던져주었다. 그 질문들이야말로 1994년이 남긴 가장 위대한 유산이며, 미래의 AI 연구를 이끌어갈 원동력이 될 것이다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>AAAI-94: Twelfth National Conference on Artificial Intelligence, https://aaai.org/conference/aaai/aaai94/</li>
<li>Papers from the 1994 AAAI Spring Symposium Archives - The Association for the Advancement of Artificial Intelligence, https://aaai.org/proceeding/symposia-ss94/</li>
<li>AAAI Workshop Papers 1994 Archives, https://aaai.org/proceeding/aaaiw-94/</li>
<li>Artificial Intelligence in Medicine: Interpreting Clinical Data Archives - AAAI, https://aaai.org/proceeding/spring-1994-01/</li>
<li>Proceedings of the AAAI Conference on Artificial Intelligence, 12 Archives, https://aaai.org/proceeding/aaai-12-1994/</li>
<li>Advances in Neural Information Processing Systems 7 (NIPS 1994), https://proceedings.neurips.cc/paper/1994</li>
<li>Advances in neural information processing systems 7 - Fraunhofer-Publica, https://publica.fraunhofer.de/entities/mainwork/c732f696-9828-4937-9ae8-28c720c1ad36</li>
<li>International Conference on Robotics and Automation (ICRA), https://www.sigmod.org/publications/dblp//db/conf/icra/index.html</li>
<li>ICRA 1994 - dblp, https://dblp.org/db/conf/icra/icra1994-3.html</li>
<li>IROS: Proceedings of the … IEEE/RSJ International Conference on - Google Books, https://books.google.com/books/about/IROS.html?id=k_pVAAAAMAAJ</li>
<li>IROS 1994 - Selected Papers - dblp, https://dblp.org/db/conf/iros/iros1994s</li>
<li>VaMP - Wikipedia, https://en.wikipedia.org/wiki/VaMP</li>
<li>The Self-Driving Car — from 1994. (from a guest post I wrote for… | by David Rostcheck | Medium, https://medium.com/@davidrostcheck/the-self-driving-car-from-1994-fb1ec617bd5a</li>
<li>Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems, https://proceedings.neurips.cc/paper/1994/hash/1c1d4df596d01da60385f0bb17a4a9e0-Abstract.html</li>
<li>(PDF) Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems (1993) | Tommi S. Jaakkola | 448 Citations - SciSpace, https://scispace.com/papers/reinforcement-learning-algorithm-for-partially-observable-3cl4fyf1vn</li>
<li>Reinforcement Learning Algorithm for Partially Observable Markov …, https://web.eecs.umich.edu/~baveja/Papers/Nips94b.pdf</li>
<li>(PDF) Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems - ResearchGate, https://www.researchgate.net/publication/2457557_Reinforcement_Learning_Algorithm_for_Partially_Observable_Markov_Decision_Problems</li>
<li>Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems, https://proceedings.neurips.cc/paper/1994/file/1c1d4df596d01da60385f0bb17a4a9e0-Paper.pdf</li>
<li>An Actor/Critic Algorithm that is Equivalent to Q-Learning - NIPS, https://proceedings.neurips.cc/paper/1994/hash/23ce1851341ec1fa9e0c259de10bf87c-Abstract.html</li>
<li>An Actor/Critic Algorithm that is Equivalent to Q-Learning, http://all.cs.umass.edu/pubs/1995_96/crites_b_95.pdf</li>
<li>An Actor/Critic Algorithm that is Equivalent to Q-Learning, https://proceedings.neurips.cc/paper/1994/file/23ce1851341ec1fa9e0c259de10bf87c-Paper.pdf</li>
<li>Learning robots and agents - TU Dortmund, https://www-ai.cs.tu-dortmund.de/FORSCHUNG/AGENTS/agents_Robots.eng.html~</li>
<li>Top 1 IEEE Transactions on Robotics and Automation papers published in 1994 - SciSpace, https://scispace.com/journals/ieee-transactions-on-robotics-and-automation-1qwutuph/1994</li>
<li>Preliminary Reference List for Dynamics of Learning in Autonomous Robot Collectives Working Group - Complexity Sciences Center, https://csc.ucdavis.edu/~dynlearn/dynlearn/RoMADS/reference.html</li>
<li>(PDF) A reinforcement-learning approach to robot navigation - ResearchGate, https://www.researchgate.net/publication/4073935_A_reinforcement-learning_approach_to_robot_navigation</li>
<li>SIMPLIFYING NEURAL NETS BY DISCOVERING FLAT MINIMA - NIPS, https://proceedings.neurips.cc/paper/1994/hash/01882513d5fa7c329e940dda99b12147-Abstract.html</li>
<li>SIMPLIFYING NEURAL NETS BY DISCOVERING FLAT MINIMA, https://www.bioinf.jku.at/publications/older/3704.pdf</li>
<li>SIMPLIFYING NEURAL NETS BY DISCOVERING FLAT MINIMA, https://proceedings.neurips.cc/paper/1994/file/01882513d5fa7c329e940dda99b12147-Paper.pdf</li>
<li>(PDF) Flat Minimum Search Finds Simple Nets - ResearchGate, https://www.researchgate.net/publication/277295133_Flat_Minimum_Search_Finds_Simple_Nets</li>
<li>Limits on Learning Machine Accuracy Imposed by Data Quality - NIPS, https://proceedings.neurips.cc/paper/1994/hash/1e056d2b0ebd5c878c550da6ac5d3724-Abstract.html</li>
<li>Limits on Learning Machine Accuracy Imposed by Data Quality - ResearchGate, https://www.researchgate.net/publication/2390114_Limits_on_Learning_Machine_Accuracy_Imposed_by_Data_Quality</li>
<li>Limits on Learning Machine Accuracy Imposed by Data Quality - AAAI, https://aaai.org/papers/kdd95-007-limits-on-learning-machine-accuracy-imposed-by-data-quality/</li>
<li>Limits on Learning Machine Accuracy Imposed by Data Quality, https://cdn.aaai.org/KDD/1995/KDD95-007.pdf</li>
<li>Limits on Learning Machine Accuracy Imposed by Data Quality, https://proceedings.neurips.cc/paper/1994/file/1e056d2b0ebd5c878c550da6ac5d3724-Paper.pdf</li>
<li>서포트 벡터 머신(SVM)이란 무엇인가요? - IBM, https://www.ibm.com/kr-ko/think/topics/support-vector-machine</li>
<li>서포트 벡터 머신(SVM, Support Vector Machine) - 박한글 - 티스토리, https://sieos.tistory.com/22</li>
<li>[월간로봇]자율주행자동차의 역사, https://www.irobotnews.com/news/articleView.html?idxno=4621</li>
<li>When driverless cars rode across Europe… in the 80s | AVSandbox, https://www.avsandbox.com/the-sandbox/when-driverless-cars-rode-across-europe-in-the-80s/</li>
<li>Contributions to Visual Autonomous Driving - dyna-vision, https://dyna-vision.de/wp-content/uploads/2021/05/2015-ContribPart-II-to-Visual-Auton.-Driving-EDD.pdf</li>
<li>Ernst Dickmanns’ VaMoRs Mercedes Van, 1986-2003 - YouTube, https://www.youtube.com/watch?v=I39sxwYKlEE</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>