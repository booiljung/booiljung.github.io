<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2025년 6월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2025년 6월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2025년 AI 및 로봇 연구 동향</a> / <span>2025년 6월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2025년 6월 AI 및 로봇 연구 동향</h1>
<h2>1. 서론</h2>
<p>2025년 6월은 인공지능(AI) 및 로봇 연구 분야에서 중요한 변곡점으로 기록될 만한 시기였다. 이 기간 동안 발표된 연구들은 AI가 디지털 공간의 패턴을 학습하는 것을 넘어, 물리 세계의 복잡한 동역학과 인과 관계를 내재적으로 이해하고 상호작용하려는 ’체화된 AI(Embodied AI)’를 향한 거대한 흐름을 명확히 보여주었다. 특히 Meta AI와 Google DeepMind와 같은 산업계의 선두 주자들은 물리 세계와의 상호작용을 위한 정교하고 실용적인 모델들을 공개하며 기술의 지평을 넓혔다. 동시에, CVPR(Computer Vision and Pattern Recognition)과 RSS(Robotics: Science and Systems) 등 최고 권위의 학회에서는 이러한 산업계의 흐름을 학술적으로 뒷받침하고 때로는 선도하는 혁신적인 연구 성과들이 대거 쏟아져 나왔다.</p>
<p>본 보고서는 세 가지 핵심적인 축을 중심으로 2025년 6월의 AI 및 로봇 연구 동향을 심층적으로 분석한다. 첫째, ’월드 모델(World Models)의 부상’이다. AI가 단순히 주어진 상황에 반응하는 것을 넘어, 행동의 결과를 예측하고 이를 바탕으로 계획을 수립하는 능력은 체화된 AI의 핵심이다. 본 보고서는 관찰만으로 물리 법칙을 암묵적으로 학습하려는 시도부터, 거대 언어 모델의 명시적 추론 능력을 로봇 제어에 통합하려는 접근법까지, 월드 모델의 다양한 구현 방식과 그 기술적 의의를 탐구한다. 둘째, ’3D 비전의 패러다임 전환’이다. 수십 년간 3D 재구성의 표준이었던 기하학적 최적화 방식의 한계를 극복하고, 대규모 데이터로 학습된 단일 순방향 신경망(feed-forward neural network)을 통해 실시간으로 3D 세계를 이해하려는 혁신적인 연구들을 조명한다. 이는 로보틱스 응용에 있어 3D 인식 기술의 근본적인 변화를 예고한다. 셋째, ’파운데이션 모델의 확장’이다. AI 모델의 발전이 단순히 성능 경쟁에만 머무르지 않고, 모델과 데이터의 ’개방성’을 통해 기술 민주화를 추구하며, 동시에 AI의 악의적 사용 가능성에 대한 경고와 안전성 확보를 위한 논의가 심화되는 현상을 다룬다.</p>
<p>본 보고서는 각 장에서 주요 연구들을 기술적으로 상세히 분석하고, 이들이 서로 어떻게 유기적으로 연결되는지, 그리고 AI 및 로봇 기술 생태계 전체에 어떤 장기적인 의미를 갖는지에 대한 통찰을 제공하고자 한다. 이를 통해 연구자, 개발자, 그리고 기술 전략가들에게 현재 기술의 최전선을 조망하고 미래를 대비할 수 있는 깊이 있는 분석을 제공하는 것을 목표로 한다.</p>
<h2>2.  월드 모델의 부상: 물리 세계의 이해, 예측, 그리고 계획</h2>
<h3>2.1  서론: 체화된 AI를 위한 내재적 세계 모델의 필요성</h3>
<p>기존의 AI 시스템, 특히 모방 학습이나 강화 학습 초기 모델들은 주어진 상태(state)에 대해 최적의 행동(action)을 매핑하는 반응형(reactive) 정책을 학습하는 데 중점을 두었다. 이러한 접근 방식은 훈련 데이터와 유사한 환경에서는 높은 성능을 보이지만, 이전에 경험하지 못한 새로운 상황이나 예기치 않은 변화에 직면했을 때 일반화 성능이 급격히 저하되는 근본적인 한계를 지닌다. 로봇이 인간의 일상 공간과 같이 비정형적이고 동적인 환경에서 유용한 작업을 수행하기 위해서는, 단순히 현재를 인식하는 것을 넘어 행동의 결과를 예측하고 미래를 시뮬레이션할 수 있는 능력이 필수적이다.</p>
<p>이러한 필요성에서 대두된 개념이 바로 ’월드 모델’이다. 월드 모델은 환경의 동역학에 대한 내재적인 모델(internal model)로서, 현재 상태와 특정 행동이 주어졌을 때 다음 상태가 어떻게 변할지를 예측하는 역할을 한다.1 에이전트는 이 내재적 모델을 통해 실제 환경과 상호작용하지 않고도 가상의 ‘꿈(dream)’ 속에서 다양한 행동 시퀀스를 시뮬레이션하고 그 결과를 미리 평가할 수 있다.1 이는 데이터 효율성을 극대화하고, 위험한 시행착오를 줄이며, 장기적인 계획(long-horizon planning)을 가능하게 하는 핵심적인 메커니즘이다.1 2025년 6월, Meta AI와 Google DeepMind가 발표한 연구들은 이 월드 모델이 더 이상 이론적 개념에 머무르지 않고, 실제 로봇 시스템에 통합되어 복잡한 물리적 과제를 해결하는 실용적인 단계에 진입했음을 명확히 보여주었다.</p>
<h3>2.2  Meta AI의 V-JEPA 2: 자기 지도 학습 기반의 잠재 공간 월드 모델</h3>
<p>2025년 6월 11일, Meta AI는 물리 세계의 작동 원리를 관찰을 통해 학습하는 자기 지도 학습(self-supervised) 기반의 월드 모델, V-JEPA 2(Video Joint Embedding Predictive Architecture 2)를 공개했다.3 이 모델은 기존 생성 모델과는 근본적으로 다른 접근 방식을 취하며, 체화된 AI를 위한 월드 모델 연구에 새로운 방향을 제시했다.</p>
<h4>2.2.1 핵심 개념 및 아키텍처</h4>
<p>V-JEPA 2의 핵심 철학은 픽셀 수준의 완벽한 재구성을 포기하는 데 있다. 기존의 비디오 예측 모델과 같은 생성 모델들은 다음 프레임의 모든 픽셀을 정확하게 예측하도록 학습한다. 하지만 현실 세계는 나뭇잎의 흔들림이나 물의 잔물결처럼 예측 불가능하고 혼란스러운 세부 사항으로 가득 차 있다.7 이러한 노이즈까지 모두 예측하려는 시도는 모델의 용량을 불필요한 정보에 낭비하게 만들고, 물리 법칙의 핵심적인 구조를 학습하는 것을 방해할 수 있다.</p>
<p>JEPA(Joint-Embedding Predictive Architecture)는 이러한 문제에 대한 해답으로, 예측을 픽셀 공간이 아닌 추상적인 잠재 표현 공간(representation space)에서 수행한다.8 즉, “다음 프레임이 어떻게 보일까?“가 아니라 “다음 프레임의 추상적인 표현은 무엇일까?“를 묻는 것이다. 이를 통해 모델은 예측 불가능한 미세한 텍스처 변화는 무시하고, 객체의 영속성(object permanence), 운동 궤적, 상호작용과 같은 예측 가능한 핵심 동역학에 집중하여 학습할 수 있다.7</p>
<p>V-JEPA 2의 아키텍처는 세 가지 주요 구성 요소로 이루어진다 10:</p>
<ol>
<li><strong>인코더 (<span class="math math-inline">E_{\theta}</span>):</strong> 비디오 클립을 입력받아 의미 있는 잠재 표현으로 변환하는 Vision Transformer(ViT) 기반의 네트워크다.</li>
<li><strong>예측기 (<span class="math math-inline">P_{\phi}</span>):</strong> 인코더보다 작은 규모의 ViT로, 비디오의 일부(context)에 대한 잠재 표현을 조건으로 하여 마스킹된 다른 부분(target)의 잠재 표현을 예측한다.</li>
<li><strong>목표 인코더 (<span class="math math-inline">E_{\theta&#39;}</span>):</strong> 인코더와 동일한 구조를 가지지만, 그 가중치가 인코더 가중치의 지수이동평균(Exponential Moving Average, EMA)으로 천천히 업데이트된다. 이는 안정적인 학습 목표를 제공하여 모델이 사소한 해(trivial solution)로 수렴하는 것을 방지하는 역할을 한다.</li>
</ol>
<h4>2.2.2 학습 과정 및 손실 함수</h4>
<p>V-JEPA 2의 학습은 비디오의 시공간적 패치(tubelet) 중 일부를 무작위로 마스킹하고, 보이는 패치(context)를 기반으로 보이지 않는 패치(target)의 잠재 표현을 예측하는 방식으로 진행된다. 구체적인 손실 함수는 예측된 표현과 목표 인코더가 생성한 목표 표현 간의 L1 거리로 정의된다.8<br />
<span class="math math-display">
\mathcal{L} = \Vert P_{\phi}(E_{\theta}(x_{\text{context}})) - \text{sg}[E_{\theta&#39;}(x_{\text{target}})] \Vert_1
</span><br />
여기서 <span class="math math-inline">x_{\text{context}}</span>는 마스킹되지 않은 비디오 패치, <span class="math math-inline">x_{\text{target}}</span>은 마스킹된 비디오 패치를 의미한다. sg는 stop-gradient 연산으로, 목표 인코더의 가중치가 예측기의 그래디언트에 의해 직접 업데이트되는 것을 막는다. 이 구조는 예측기가 인코더가 추출한 고수준의 의미론적 정보를 바탕으로 추론하도록 유도하며, 학습 과정의 안정성을 확보하는 핵심적인 장치다.10</p>
<h4>2.2.3 행동 조건부 모델 (V-JEPA 2-AC)</h4>
<p>V-JEPA 2의 진정한 가치는 로보틱스 응용에서 드러난다. Meta AI는 100만 시간 이상의 방대한 인터넷 비디오로 사전 학습하여 물리 세계에 대한 일반적인 이해를 갖춘 V-JEPA 2 인코더를 고정(freeze)한 후, 약 62시간 분량의 소규모 로봇 상호작용 데이터(Droid 데이터셋)를 사용하여 행동 조건부 예측기(action-conditioned predictor)를 추가로 학습시켰다. 이 모델이 바로 V-JEPA 2-AC다.4</p>
<p>V-JEPA 2-AC는 현재 상태의 잠재 표현(<span class="math math-inline">z_t</span>)과 로봇의 행동(<span class="math math-inline">a_t</span>)이 주어졌을 때, 다음 상태의 잠재 표현(<span class="math math-inline">z_{t+1}</span>)을 예측하도록 학습된다. 이는 순수한 관찰 기반의 월드 모델을 행동과 결과 사이의 인과 관계를 이해하는 모델로 확장하는 과정이다. 이 단계에서는 교사 강요(teacher-forcing) 손실과 롤아웃(rollout) 손실을 함께 사용하여 단기 및 장기 예측 능력을 모두 향상시킨다.10</p>
<h4>2.2.4 로보틱스 적용 및 성과</h4>
<p>학습된 V-JEPA 2-AC는 모델 예측 제어(Model Predictive Control, MPC) 프레임워크와 결합되어 실제 로봇 제어에 사용된다. MPC 과정은 다음과 같다 8:</p>
<ol>
<li><strong>목표 설정:</strong> 로봇에게 목표 상태를 이미지로 제공한다.</li>
<li><strong>에너지 함수 정의:</strong> 특정 행동 시퀀스를 실행했을 때 예측되는 미래의 잠재 표현과 목표 이미지의 잠재 표현 사이의 L1 거리를 ’에너지’로 정의한다. 에너지가 낮을수록 목표에 더 가까워지는 행동이다.</li>
<li><strong>행동 최적화:</strong> 교차 엔트로피 방법(Cross-Entropy Method, CEM)과 같은 샘플링 기반 최적화 알고리즘을 사용하여, 정의된 에너지를 최소화하는 최적의 행동 시퀀스를 잠재 공간 내에서 효율적으로 탐색한다.</li>
<li><strong>실행 및 재계획:</strong> 최적의 행동 시퀀스 중 첫 번째 행동만 실행하고, 새로운 관찰을 통해 상태를 업데이트한 후, 다시 처음부터 계획을 수립한다(receding horizon control).</li>
</ol>
<p>이러한 방식을 통해 V-JEPA 2-AC는 훈련 중에 한 번도 본 적 없는 새로운 환경에서 컵을 집어 특정 위치로 옮기는 것과 같은 작업을 별도의 미세 조정 없이 제로샷(zero-shot)으로 수행했으며, 65%에서 80%에 이르는 높은 성공률을 기록했다.13 이는 월드 모델 기반 계획이 실제 로봇 제어 문제에 효과적으로 적용될 수 있음을 입증한 중요한 성과다.</p>
<p>V-JEPA 2의 학습 방식은 명시적인 물리 엔진이나 기호 논리를 전혀 사용하지 않는다. 대신, 방대한 비디오 데이터를 관찰함으로써 ’물체가 다른 물체에 가려져도 사라지지 않는다’거나 ’중력의 영향으로 물체는 아래로 떨어진다’와 같은 물리 세계의 근본적인 규칙들을 잠재 표현 공간 내의 변환 관계로 암묵적으로 학습한다. 픽셀을 직접 생성하는 대신 추상적인 특징 공간에서 예측을 수행하는 이 전략은 계산 효율성을 극적으로 향상시키고 15, 실제 세계의 예측 불가능한 시각적 노이즈에 대해 모델을 훨씬 더 강건하게 만든다.</p>
<p>하지만 이러한 암묵적 모델링 방식은 복잡한 인과 관계나 정밀한 물리적 상호작용을 추론하는 데에는 명백한 한계를 드러낸다. Meta가 V-JEPA 2와 함께 공개한 새로운 물리 추론 벤치마크(Interphys 2, Causal VQA)에서, 인간은 85%에서 95%의 높은 정확도를 보인 반면 V-JEPA 2를 포함한 최첨단 모델들은 거의 우연에 가까운 수준의 낮은 성능을 기록했다.3 이는 순수한 관찰 기반 학습만으로는 아직 인간 수준의 직관적 물리 추론 능력에 도달하지 못했음을 명확히 보여준다. 이 결과는 현재 AI 기술이 어디까지 왔으며, 앞으로 어떤 방향으로 나아가야 하는지에 대한 중요한 단서를 제공하는, 현재 기술의 경계를 보여주는 의미 있는 지표라 할 수 있다.</p>
<h3>2.3  Google DeepMind의 Gemini Robotics 1.5: 에이전트적 로보틱스를 위한 이원적 프레임워크</h3>
<p>2025년 6월 24일, Google DeepMind는 V-JEPA 2와는 다른 접근법을 취하는 새로운 로보틱스 모델군인 Gemini Robotics 1.5를 발표했다.16 이 시스템은 단일 모델이 아닌, 각기 다른 역할을 수행하는 두 개의 특화된 모델이 유기적으로 협력하는 ’에이전트 프레임워크(agentic framework)’를 통해 복잡한 물리적 과제를 해결한다.</p>
<h4>2.3.1 핵심 개념 및 아키텍처</h4>
<p>Gemini Robotics 1.5는 인간의 두뇌가 고수준의 계획과 저수준의 운동 제어를 분리하여 수행하는 방식에서 영감을 얻어, 다음과 같은 이원적(dual) 아키텍처를 채택했다 16:</p>
<ol>
<li><strong>Gemini Robotics-ER 1.5 (Embodied Reasoning):</strong> 이 모델은 로봇의 ‘상위 두뇌’ 또는 ’전두엽’에 해당하며, 강력한 시각-언어 모델(VLM)이다. “식탁을 치워줘“와 같은 추상적이고 자연스러운 언어 명령을 이해하고, 이를 “1. 남은 음식을 쓰레기통에 버린다. 2. 그릇을 싱크대에 옮긴다. 3. 테이블을 닦는다.“와 같은 구체적인 하위 목표들로 분해하는 장기 계획(long-horizon planning)을 수립한다. 또한, 물체의 공간적 관계를 이해하고, 필요시 웹 검색과 같은 외부 도구를 호출하여 추가 정보를 획득하는 등 고수준의 추론을 담당한다.16</li>
<li><strong>Gemini Robotics 1.5 (Vision-Language-Action):</strong> 이 모델은 로봇의 ‘소뇌’ 또는 ’운동 피질’과 같은 역할을 수행하는 시각-언어-행동(VLA) 모델이다. ER 모델로부터 “사과를 집어라“와 같은 구체적인 단기 지시를 받아, 이를 실제 로봇 팔의 관절 각도나 엔드 이펙터의 궤적과 같은 연속적인 모터 명령으로 변환하여 물리적 행동을 실행한다.16</li>
</ol>
<p>이러한 역할 분담은 각 모델이 가장 잘하는 작업에 집중하게 함으로써 시스템 전체의 효율성과 강건성을 높인다.</p>
<h4>2.3.2 ‘사고하는 VLA (Thinking VLA)’ 메커니즘</h4>
<p>이 프레임워크의 가장 혁신적인 부분은 VLA 모델에 탑재된 ‘사고(Thinking)’ 메커니즘이다. 기존 VLA 모델들이 입력을 곧바로 행동으로 변환하는 블랙박스에 가까웠던 반면, Gemini Robotics 1.5의 VLA는 행동을 실행하기 전에 자연어로 된 ’사고의 흐름(stream of thoughts)’을 명시적으로 생성한다.16</p>
<p>이 사고 과정은 다음과 같은 여러 중요한 기능을 수행한다:</p>
<ul>
<li><strong>해석 가능성 증대:</strong> 로봇이 “물체를 잡기 위해 먼저 손을 열어야겠다“와 같이 자신의 행동 계획을 언어로 설명함으로써, 개발자나 사용자는 로봇의 의도를 명확하게 파악하고 실패 시 원인을 분석하기 용이해진다.</li>
<li><strong>상황 인식 및 재계획:</strong> 시각적 관찰을 언어 기반의 생각으로 변환하는 과정에서 “물체가 예상보다 미끄럽다” 또는 “목표 지점에 장애물이 있다“와 같은 예기치 않은 상황을 인식하고, 이를 바탕으로 스스로 복구 행동을 제안하거나 계획을 수정할 수 있다.</li>
<li><strong>복잡한 지시 분해:</strong> ER 모델의 고수준 계획을 VLA 모델이 다시 한번 더 구체적인 실행 단위로 분해하고 검증하는 역할을 한다.</li>
</ul>
<h4>2.3.3 모션 전달(Motion Transfer)을 통한 다중 로봇 학습</h4>
<p>로봇 공학의 오랜 난제 중 하나는 서로 다른 하드웨어(embodiment)를 위한 정책을 일반화하는 것이다. Gemini Robotics 1.5는 ’모션 전달(Motion Transfer)’이라는 새로운 메커니즘을 통해 이 문제를 해결한다. 이 기술은 ALOHA, Bi-arm Franka, Apollo 휴머노이드와 같이 형태, 자유도, 동역학이 전혀 다른 로봇들로부터 수집된 이종의(heterogeneous) 시연 데이터를 단일 VLA 모델이 함께 학습할 수 있도록 한다.18</p>
<p>모션 전달 메커니즘은 각 로봇의 고유한 움직임 특성을 일반화된 ’운동 원리(motion primitives)’로 추상화하여, 모델이 특정 하드웨어에 종속되지 않는 보편적인 움직임에 대한 이해를 형성하도록 돕는다. 그 결과, 한 로봇 플랫폼에서 학습한 젓가락질과 같은 정교한 기술을 별도의 미세 조정 없이 다른 로봇 플랫폼으로 제로샷 전이(zero-shot transfer)하는 데 성공했다. 이는 로봇 기술 학습의 확장성과 재사용성을 획기적으로 높이는 중요한 돌파구다.18</p>
<p>V-JEPA 2가 물리 세계를 암묵적으로 학습하는 방식을 택한 것과 달리, Gemini Robotics 프레임워크는 Gemini라는 거대 언어 모델(LLM)이 가진 강력하고 명시적인 기호적 추론 능력을 로보틱스 문제 해결에 직접적으로 접목한다. ‘사고하는 VLA’ 메커니즘은 이러한 명시적 추론 과정이 어떻게 로봇의 행동 생성에 실시간으로 관여하는지를 보여주는 대표적인 사례다. 동시에, 저수준의 실제 움직임을 생성하는 VLA 모델 자체는 다양한 로봇의 시연 데이터를 모방하여 학습함으로써, 데이터로부터 암묵적인 운동 지능을 습득한다.</p>
<p>결론적으로 Gemini Robotics 프레임워크는 ‘무엇을(What)’ 그리고 ‘왜(Why)’ 해야 하는지에 대한 고수준의 전략적 계획은 LLM의 명시적 추론 능력에 의존하고, ‘어떻게(How)’ 움직여야 하는지에 대한 저수준의 운동 실행은 데이터 기반의 암묵적 모방 학습에 의존하는 정교한 하이브리드 접근법의 성공적인 구현체라 할 수 있다. 이는 복잡한 장기 과제를 해결하는 데 있어, 순수한 관찰 기반 학습이나 순수한 기호적 계획만으로는 도달하기 어려운 수준의 일반성과 강건성을 확보할 수 있는 강력한 전략임을 시사한다.</p>
<h3>2.4  학계의 월드 모델 연구 동향: CVPR 및 RSS 주요 논문</h3>
<p>산업계의 거대 모델들이 월드 모델의 근본적인 아키텍처를 탐구하는 동안, 학계에서는 이러한 모델들을 실제 로봇 문제에 효과적으로 적용하기 위한 창의적인 프레임워크 연구가 활발하게 진행되었다. 특히 CVPR 2025와 RSS 2025에서는 월드 모델을 내비게이션과 객체 탐색에 활용하는 주목할 만한 연구들이 발표되었다.</p>
<h4>2.4.1 “Navigation World Models” (CVPR 2025 최우수 논문 가작)</h4>
<p>Amir Bar, Yann LeCun 등이 발표한 이 연구는 제어 가능한 비디오 생성 모델을 월드 모델의 핵심으로 활용하여 내비게이션 문제를 해결한다.21 이들의 접근 방식은 로봇이 자신의 행동 결과를 시각적으로 ’상상’하고, 그 상상이 목표와 얼마나 일치하는지를 평가하여 최적의 경로를 찾는다는 직관적인 아이디어에 기반한다.</p>
<ul>
<li><strong>CDiT 아키텍처:</strong> 연구팀은 기존의 DiT(Diffusion Transformer) 아키텍처를 개선한 CDiT(Conditional Diffusion Transformer)를 제안했다. CDiT는 과거의 관찰 이미지들과 앞으로 수행할 행동 시퀀스를 조건으로 입력받아, 미래에 관찰될 시각적 프레임들을 직접 생성(시뮬레이션)한다. 이 구조는 컨텍스트 프레임 수에 따라 계산 복잡도가 선형적으로 증가하여, 기존 DiT보다 약 4배 더 효율적으로 대규모 모델(10억 파라미터) 학습을 가능하게 했다.23</li>
<li><strong>시뮬레이션 기반 계획 수립:</strong> 계획 단계에서는 여러 후보 행동 시퀀스를 생성하고, 각 시퀀스에 대해 CDiT를 사용하여 미래 비디오를 생성한다. 그 후, 생성된 마지막 프레임과 목표 이미지 간의 유사도를 점수로 계산하여 가장 높은 점수를 받은 행동 시퀀스를 최적의 계획으로 선택한다. 이 방식은 처음부터 경로를 계획하거나, NoMaD와 같은 외부 정책이 제안한 여러 경로 중 최적의 경로를 재평가(re-ranking)하는 데 모두 유연하게 사용될 수 있다.22</li>
</ul>
<h4>2.4.2 “WoMAP: World Models For Embodied Open-Vocabulary Object Localization” (RSS 2025 SemRob 워크숍 최우수 논문)</h4>
<p>Tenny Yin, Anirudha Majumdar 등이 발표한 WoMAP은 VLM의 뛰어난 상식 추론 능력과 월드 모델의 정밀한 동역학 예측 능력을 결합한 하이브리드 프레임워크다.26 이 연구는 “열쇠를 찾아줘“와 같이 자연어로 주어진 개방형 어휘(open-vocabulary) 객체 탐색 문제를 해결하는 것을 목표로 한다.</p>
<ul>
<li><strong>가우시안 스플래팅 기반 데이터 생성:</strong> 이 연구의 독창적인 기여 중 하나는 학습 데이터 생성 방식이다. 전문가의 원격 조종 데이터와 같이 수집 비용이 비싼 데이터 대신, 소수의 실제 환경 비디오만으로 3D 가우시안 스플래팅(Gaussian Splatting) 모델을 학습시킨다. 이후 이 3D 모델 내에서 가상의 카메라를 자유롭게 움직이며 방대한 양의 포토리얼리스틱한 학습 데이터를 자동으로 생성하는 ‘real-to-sim-to-real’ 파이프라인을 구축했다. 이는 데이터 수집의 병목 현상을 해결하는 확장 가능한 접근법이다.29</li>
<li><strong>VLM과 월드 모델의 협력적 계획:</strong> 계획 단계에서 WoMAP은 두 단계의 계층적 접근을 취한다. 먼저, GPT-4o와 같은 VLM에게 “열쇠를 찾으려면 어디를 확인해야 할까?“라고 질문하여 “문 근처 선반 위” 또는 “소파 아래“와 같은 상식에 기반한 상위 수준의 행동 제안들을 얻는다. 그 후, 학습된 잠재 공간 월드 모델이 각 제안을 실행했을 때의 결과를 저수준에서 시뮬레이션하고, 목표 객체를 발견할 확률(보상)이 가장 높게 예측되는 행동을 최종적으로 선택하여 실행한다.28 이 방식은 VLM의 ‘뜬구름 잡는’ 계획을 물리적으로 실현 가능한 행동으로 접지(grounding)시키는 효과적인 방법이다.</li>
</ul>
<p>Meta와 Google이 각각 암묵적 학습과 명시적 추론이라는, 어떻게 보면 양 극단에 있는 근본적인 접근법을 탐구하며 파운데이션 모델의 능력을 확장하는 동안, 학계에서는 이 두 가지 접근법의 장점을 결합하여 당면한 로봇 문제를 해결하려는 실용적인 하이브리드 모델 연구가 활발히 진행되고 있다. NWM은 시각적 시뮬레이션이라는 ’중간 수준’의 명시성을 통해 계획을 수립하고, WoMAP은 VLM의 ‘고수준’ 기호적 제안을 월드 모델의 ‘저수준’ 동역학 예측과 결합한다.</p>
<p>이러한 흐름은 미래의 가장 강력하고 유연한 체화된 AI 에이전트가 단일한 방식에 의존하기보다는, 해결하고자 하는 과제의 종류와 추상화 수준에 따라 암묵적 예측, 시각적 시뮬레이션, 기호적 추론 등 다양한 메커니즘을 유기적으로 통합하는 다층적 아키텍처를 갖게 될 것임을 강력하게 시사한다. 이는 산업계가 파운데이션 모델의 근본적인 가능성을 탐구하고, 학계가 이를 실제 로봇 문제에 적용하기 위한 영리한 통합 프레임워크를 개발하며 상호 보완적인 발전을 이루어가는 건강한 기술 생태계의 모습을 보여준다.</p>
<h2>3.  3D 비전의 패러다임 전환: 최적화에서 순수 신경망으로</h2>
<h3>3.1  서론: 기하학적 최적화의 시대에서 신경망의 시대로</h3>
<p>수십 년 동안 3D 컴퓨터 비전 분야, 특히 Structure-from-Motion(SfM)과 Multi-View Stereo(MVS)의 핵심에는 번들 조정(Bundle Adjustment, BA)과 같은 반복적 기하학적 최적화 기법이 자리 잡고 있었다. 이러한 기법들은 여러 이미지 간의 기하학적 일관성을 수학적으로 최적화하여 카메라 포즈와 3D 구조를 정밀하게 복원하는 데 강력한 성능을 보여주었다. 하지만 이들은 본질적으로 높은 계산 비용을 요구하고, 초기값에 민감하며, 지역 최적해(local minima)에 빠질 위험이 있다는 한계를 지니고 있었다.</p>
<p>2025년 6월 CVPR 학회에서는 이러한 전통적인 패러다임에 근본적인 변화를 예고하는 연구들이 대거 등장했다. 이들 연구의 공통적인 방향은, 복잡하고 느린 최적화 과정을 대규모 3D 데이터로 사전 학습된 거대한 단일 순방향 신경망(feed-forward neural network)으로 대체하려는 시도다. 이는 3D 인식을 실시간으로 처리해야 하는 로보틱스나 증강현실과 같은 응용 분야에 있어 중대한 의미를 갖는 패러다임의 전환이라 할 수 있다.31</p>
<h3>3.2  VGGT (Visual Geometry Grounded Transformer): 3D 이해를 위한 파운데이션 모델 (CVPR 2025 최우수 논문)</h3>
<p>CVPR 2025에서 최우수 논문상을 수상한 Meta AI와 옥스퍼드 대학의 공동 연구 ’VGGT(Visual Geometry Grounded Transformer)’는 이러한 패러다임 전환을 가장 상징적으로 보여주는 연구다.21</p>
<h4>3.2.1 핵심 개념</h4>
<p>VGGT의 핵심 아이디어는 극도로 단순하다. 여러 장, 심지어 수백 장의 이미지를 하나의 거대한 트랜스포머 모델에 한 번에 입력하면, 단일 순방향 패스(single forward pass)만으로 해당 장면에 대한 모든 핵심 3D 속성들—각 이미지의 카메라 내부/외부 파라미터, 픽셀별 깊이 맵, 3D 포인트 맵, 그리고 이미지 간 포인트 트랙—이 한꺼번에 출력된다는 것이다.31 이 모든 과정이 1초 미만에 완료되며, 번들 조정과 같은 어떠한 후처리 최적화도 필요로 하지 않는다.31</p>
<h4>3.2.2 아키텍처</h4>
<p>놀랍게도 VGGT는 3D 기하학을 위한 특별한 귀납적 편향(inductive bias)을 거의 사용하지 않는다. 대신, DINO와 같은 사전 학습된 2D 비전 모델로 각 이미지를 패치 임베딩으로 변환한 후, 비교적 표준적인 트랜스포머 아키텍처를 통과시킨다. 유일한 구조적 특징은 개별 프레임 내의 토큰들 간에 주의(attention)를 계산하는 ’프레임 단위 어텐션’과 모든 프레임의 모든 토큰들 간에 주의를 계산하는 ’전역 어텐션’을 번갈아 수행하는 ‘교대 어텐션(Alternating Attention)’ 메커니즘이다.35 이는 모델이 개별 이미지의 내용을 이해하는 것과 여러 이미지 간의 기하학적 관계를 추론하는 것을 균형 있게 학습하도록 돕는다. 이러한 단순한 구조는 정교하게 설계된 아키텍처보다 방대한 양의 3D 주석 데이터를 통한 대규모 학습이 더 중요할 수 있음을 시사한다.35</p>
<h4>3.2.3 성과와 의의</h4>
<p>VGGT는 후처리 최적화 과정 없이도 COLMAP과 같은 최첨단 최적화 기반 SfM 파이프라인의 성능을 능가하는 결과를 보여주었다. 이는 3D 재구성이 더 이상 오프라인에서 몇 분 또는 몇 시간을 소요하는 작업이 아니라, 실시간으로 장면을 인식하고 상호작용해야 하는 로보틱스나 AR과 같은 분야에 즉시 적용 가능한 기술이 될 수 있음을 의미한다.31</p>
<p>더 나아가, VGGT를 통해 추출된 사전 학습된 특징(feature)들은 동적 비디오에서의 포인트 트래킹이나 새로운 시점 합성(novel view synthesis)과 같은 다른 다운스트림 작업의 성능을 크게 향상시키는 것으로 나타났다. 이는 VGGT가 단순히 3D 재구성 도구를 넘어, 2D 비전 분야의 DINO나 CLIP처럼 ’3D 비전을 위한 파운데이션 모델’로서 기능할 수 있는 잠재력을 가지고 있음을 보여준다.31</p>
<h3>3.3  빛과 동역학의 정밀한 모델링</h3>
<p>VGGT가 3D 비전의 ’속도’와 ’일반성’을 새로운 차원으로 끌어올렸다면, 다른 수상 연구들은 ’정밀도’와 ’특수성’의 한계를 확장하는 데 집중했다.</p>
<h4>3.3.1 “Neural Inverse Rendering from Propagating Light” (CVPR 2025 최우수 학생 논문)</h4>
<p>카네기 멜론 대학과 토론토 대학의 공동 연구인 이 논문은 기존 3D 재구성 방식이 간과했던 ‘빛’ 자체의 물리적 현상을 모델링의 중심으로 가져왔다.21 연구팀은 일반 카메라 대신, 빛이 장면에 부딪히고 산란하여 센서에 도달하기까지의 시간을 나노초 단위로 측정할 수 있는 특수 LiDAR 시스템을 사용했다.</p>
<ul>
<li><strong>기술:</strong> 이들은 ’시간 분해 신경 복사 캐싱(time-resolved neural radiance caching)’이라는 새로운 기법을 제안했다. 이는 물체 표면에서 직접 반사되는 빛(direct light)뿐만 아니라, 여러 번의 복잡한 산란을 거쳐 도달하는 간접광(indirect light)까지 모두 모델링한다. 그 결과, 거울이나 반투명 물체가 많아 기존 방식으로는 재구성이 거의 불가능했던 매우 어려운 환경에서도 놀랍도록 정확한 3D 형상 복원에 성공했다.37 이는 자율주행차가 마주하는 야간이나 악천후 상황처럼 빛의 조건이 복잡한 환경에서의 인식 성능을 획기적으로 개선할 수 있는 가능성을 연다.</li>
</ul>
<h4>3.3.2 “MegaSaM: Accurate, Fast and Robust Structure and Motion from Casual Dynamic Videos” (CVPR 2025 최우수 논문 가작)</h4>
<p>Google DeepMind, UC 버클리 등이 참여한 이 연구는 로봇이나 사람이 손에 들고 자유롭게 촬영한 ’일상적인 동적 비디오(casual dynamic videos)’라는, 매우 어렵지만 현실에서 가장 흔한 데이터 유형에 집중했다.21</p>
<ul>
<li><strong>기술:</strong> MegaSaM은 딥 비주얼 SLAM 프레임워크를 기반으로 하되, 두 가지 핵심 요소를 통합했다. 첫째, 픽셀이 정적인 배경에 속하는지 동적인 객체에 속하는지를 예측하는 ’움직임 확률 네트워크’를 도입하여 움직이는 객체로 인한 오류를 줄였다. 둘째, 카메라의 움직임이 거의 없어 기하학적 정보가 부족할 때(low-parallax)는 단일 이미지 깊이 추정 모델의 예측을 활용하고, 정보가 충분할 때는 기하학적 제약에 더 의존하는 ’불확실성 인식 번들 조정(uncertainty-aware bundle adjustment)’을 통해 강건성을 확보했다.40 그 결과, 영상 속에서 사람들이 움직이거나 카메라가 거의 제자리에서 회전만 하는 등 기존 SLAM 시스템이 실패하기 쉬운 시나리오에서도 안정적으로 카메라 궤적과 깊이를 추정해냈다.</li>
</ul>
<h3>3.4  3D 표현 방식의 혁신</h3>
<h4>3.4.1 “3D Student Splatting and Scooping” (CVPR 2025 최우수 논문 가작)</h4>
<p>이 연구는 최근 3D 비전 분야에서 실시간 고품질 렌더링으로 큰 주목을 받고 있는 3D 가우시안 스플래팅(3D Gaussian Splatting, 3DGS)의 핵심적인 수학적 공식을 개선하여 표현의 효율성과 품질을 한 단계 끌어올렸다.21</p>
<ul>
<li><strong>기술:</strong> 3DGS는 3D 공간을 수많은 작은 타원체(가우시안)의 집합으로 표현한다. 이 연구는 두 가지 혁신을 제안했다. 첫째, 더 뾰족한 중심과 두꺼운 꼬리를 가진 스튜던트 t-분포(Student’s t-distribution)를 사용하여 가우시안보다 더 적은 수의 요소로 복잡한 형상을 더 잘 표현할 수 있게 했다. 둘째, 색상과 밀도를 더하는 기존의 ‘스플래팅(splatting)’ 방식에 더해, 밀도를 빼는 ‘스쿠핑(scooping)’ 개념을 도입했다. 이는 마치 조각가가 찰흙을 붙였다가 떼어내는 것처럼, 불필요한 부분을 제거하여 더 날카롭고 정교한 표면을 표현할 수 있게 한다. 그 결과, 동일한 렌더링 품질을 달성하는 데 필요한 파라미터(가우시안의 수)를 크게 줄여 모델의 경량화와 효율성을 높였다.44</li>
</ul>
<p>CVPR 2025의 주요 3D 비전 연구들은 서로 다른 층위에서 기술의 진화를 이끌고 있다. 이는 ’속도와 일반성’을 추구하는 거시적 접근과 ’정밀도와 표현력’을 추구하는 미시적 접근, 두 가지 방향으로 요약될 수 있다.</p>
<p>한 축은 VGGT와 MegaSaM으로 대표되는 ’시스템 레벨’의 혁신이다. 이들은 복잡한 최적화 과정을 과감히 제거하고, 다양한 조건의 입력(다중 뷰, 동적 장면)에 대해 빠르고 강건하게 전체 장면의 3D 구조(카메라, 깊이)를 추론하는 데 집중한다. 이는 로봇이 실시간으로 자신의 위치를 파악하고(SLAM), 주변 환경을 매핑하며(navigation), 전체적인 장면을 이해(scene understanding)하는 데 직접적으로 기여하는 거시적 관점의 발전이다.</p>
<p>다른 한 축은 ’객체 레벨’의 혁신이다. ’Neural Inverse Rendering’은 물리 법칙(빛의 전파)에 더욱 충실한 모델을 구축하여 장면을 더 정확하게 이해하려 하고, ’3D Student Splatting and Scooping’은 장면을 표현하는 기본 단위(primitive) 자체를 더 효율적으로 만들어 표현력의 한계를 넘어서려 한다. 이는 로봇이 특정 물체와 정밀하게 상호작용(manipulation)할 때 필요한 정확한 형상, 재질, 광학적 특성을 이해하는 데 기여하는 미시적 관점의 발전이다.</p>
<p>이러한 다층적 진화는 미래의 고성능 로봇 비전 시스템이 단일한 방식으로 작동하지 않을 것임을 시사한다. 예를 들어, 로봇은 먼저 VGGT와 같은 파운데이션 모델을 사용하여 주변 환경 전체를 순식간에 거시적으로 파악한 후, 자신이 집어야 할 물체나 상호작용해야 할 특정 영역에 대해서는 SSS나 물리 기반 렌더링과 같은 정밀 모델을 국소적으로 적용하여 세부 정보를 얻는 하이브리드 형태의 인식 시스템을 갖추게 될 가능성이 높다.</p>
<h2>4.  파운데이션 모델의 확장: 개방성, 다중양식, 그리고 안전성</h2>
<h3>4.1  서론: 성능 경쟁을 넘어선 새로운 가치</h3>
<p>2025년 6월의 AI 연구 동향은 파운데이션 모델의 발전이 단순히 벤치마크 성능 경쟁을 넘어 새로운 차원으로 확장되고 있음을 보여주었다. 이제 기술의 우수성은 모델의 크기나 정확도만으로 평가되지 않는다. 모델과 학습 데이터의 접근성 및 투명성을 의미하는 ‘개방성(openness)’, 텍스트, 이미지, 소리 등 다양한 양식(modality)을 통합하여 인간과 같이 세상을 이해하는 ‘다중양식성(multimodality)’, 그리고 기술이 사회에 미칠 수 있는 부정적인 영향을 최소화하고 책임감 있게 사용하는 ’안전성(safety)’이 기술의 가치를 결정하는 핵심적인 요소로 부상하고 있다.</p>
<h3>4.2  Molmo &amp; PixMo: 완전 개방형 데이터 및 가중치를 통한 SOTA VLM (CVPR 2025 최우수 논문 가작)</h3>
<p>Allen Institute for AI(AI2) 등이 발표한 ‘Molmo and PixMo’ 연구는 이러한 ’개방성’의 가치를 극명하게 보여주는 사례다.21 이전까지 공개된 강력한 오픈-가중치(open-weight) 시각-언어 모델(VLM)들은 대부분 GPT-4V와 같은 강력한 독점(proprietary) 모델이 생성한 합성 데이터에 크게 의존하여 학습되었다. 이는 사실상 독점 모델의 지식을 ’증류(distilling)’하는 방식으로, 어떻게 처음부터 고성능 VLM을 구축할 수 있는지에 대한 근본적인 지식은 여전히 소수의 기업에 갇혀 있었다.47</p>
<h4>4.2.1 핵심 기여 및 데이터 수집 혁신</h4>
<p>Molmo &amp; PixMo 연구의 핵심 기여는 이러한 의존성을 완전히 끊어냈다는 점이다. 연구팀은 <strong>PixMo</strong>라는 이름의 새로운 데이터셋을 구축했는데, 이는 어떠한 외부 VLM의 도움도 받지 않고 순수하게 인간의 주석으로만 만들어졌다.47 특히 데이터 수집 과정에서 다음과 같은 창의적인 혁신을 도입했다:</p>
<ul>
<li><strong>음성 기반 캡션 수집:</strong> 사람들에게 긴 텍스트 캡션을 작성하도록 요청하면 몇 가지 두드러진 특징에만 집중하고 쉽게 지치는 경향이 있다. 이를 해결하기 위해, 연구팀은 참가자들에게 이미지를 60~90초 동안 ’말로 설명’하도록 요청하고 이를 녹음한 후 텍스트로 변환했다. 이 ‘양식 전환(modality switching)’ 기법을 통해 훨씬 더 풍부하고 상세하며 긴(평균 200단어 이상) 고품질 캡션을 효율적으로 수집할 수 있었다.47</li>
<li><strong>2D 포인팅 데이터:</strong> 객체의 위치를 나타내기 위해 경계 상자(bounding box)나 분할 마스크(segmentation mask)를 그리는 작업은 시간과 비용이 많이 든다. 연구팀은 대신 “사진 속 ’프라푸치노’는 어디에 있나요?“와 같은 질문에 대해 사용자가 이미지의 해당 지점을 클릭하여 ’2D 포인트’를 찍는 방식으로 데이터를 수집했다. 이 방식은 주석 작업을 극적으로 단순화하여 230만 개 이상의 방대한 접지(grounding) 데이터를 저렴한 비용으로 구축할 수 있게 했다.47</li>
</ul>
<h4>4.2.2 의의</h4>
<p>이러한 고품질의 완전 개방형 데이터셋으로 학습된 <strong>Molmo</strong> 모델군은 놀라운 성능을 보여주었다. 특히 가장 큰 모델인 Molmo-72B는 여러 벤치마크에서 Claude 3.5 Sonnet, Gemini 1.5 Pro와 같은 강력한 독점 모델들을 능가하는 성능을 기록했다.47 이는 막대한 자본과 독점 데이터를 가진 거대 기업이 아니더라도, 창의적인 데이터 수집 전략과 열린 협력을 통해 최첨단 AI 모델을 개발할 수 있음을 입증한 것이다. 이 연구는 AI 연구의 투명성, 재현성, 그리고 기술 민주화를 촉진하는 중요한 이정표가 되었다.</p>
<h3>4.3  AI의 악의적 사용과 대응: OpenAI 6월 위협 보고서</h3>
<p>기술의 발전은 언제나 그림자를 동반한다. OpenAI는 2025년 6월 위협 보고서를 통해 자사의 강력한 AI 모델들이 국가적 배후를 둔 조직이나 사이버 범죄자들에 의해 어떻게 악용되고 있는지 구체적인 사례를 공개하며 경각심을 일깨웠다.50</p>
<h4>4.3.1 주요 위협 사례</h4>
<p>보고서에 따르면, 악의적 행위자들은 AI를 완전히 새로운 유형의 공격을 창조하기보다는 기존의 공격 방식을 자동화하고 확장하는 데 주로 사용하고 있었다. 주요 사례는 다음과 같다 50:</p>
<ul>
<li><strong>영향력 공작(Covert Influence Operations):</strong> 중국, 러시아, 이란 등과 연계된 조직들이 AI를 사용하여 특정 정치적 이슈에 대한 가짜 소셜 미디어 게시물, 댓글, 가상 인물을 대량으로 생성했다. 이들은 “Sneer Review”, “Uncle Spam“과 같은 작전명을 사용하며, 사회적 분열을 조장하고 여론을 조작하려 시도했다.</li>
<li><strong>사이버 범죄(Cyber Warfare):</strong> “ScopeCreep“이라는 작전에서 러시아 해커는 ChatGPT를 코딩 보조 도구로 활용하여 윈도우 악성코드를 점진적으로 개발하고 디버깅했다. 이는 AI가 정교한 사이버 공격 무기를 만드는 데 직접적으로 기여할 수 있음을 보여준다.</li>
<li><strong>사기 및 스파이 활동(Scam Schemes &amp; Espionage):</strong> 북한과 연계된 것으로 추정되는 조직은 AI를 이용해 IT 전문가들의 가짜 이력서와 자기소개서를 대량 생산하여 원격 근무 일자리에 지원했으며, 캄보디아 기반 조직은 “Wrong Number” 사기에서 피해자를 속이는 메시지를 번역하고 작성하는 데 AI를 사용했다.</li>
</ul>
<p>이러한 사례들은 AI가 새로운 공격 벡터를 창출하기보다는, 기존의 사회 공학, 여론 조작, 멀웨어 개발과 같은 공격들을 훨씬 더 ‘저렴하고’, ‘빠르고’, ‘대규모로’ 수행할 수 있게 만드는 강력한 ’스케일링 벡터(scaling vector)’로 작용하고 있음을 명확히 보여준다. 과거에는 여러 언어에 능통한 다수의 인력이 필요했던 국제적인 영향력 공작을 이제는 소수의 인원이 AI의 도움을 받아 수행할 수 있게 된 것이다. 이는 방어하는 입장에서 개별적인 가짜 콘텐츠를 탐지하는 것을 넘어, 콘텐츠 생성의 배후에 있는 ’의도’와 ’조직’을 파악하는 것이 더욱 중요해졌음을 의미한다. AI 기술의 발전과 함께 이를 탐지하고 방어하는 기술 또한 함께 발전해야 하는 끊임없는 창과 방패의 싸움이 시작된 것이다.</p>
<h3>4.4  기타 주요 모델 및 연구 발표</h3>
<p>6월에는 거대 기술 기업과 학계에서 AI의 성능, 효율성, 신뢰성을 높이기 위한 다양한 연구들이 발표되었다.</p>
<ul>
<li><strong>OpenAI o3-pro 및 o4-mini:</strong> OpenAI는 자사의 가장 강력한 모델인 o3를 더욱 개선하여 더 길고 신중하게 추론하는 <strong>o3-pro</strong>를 출시했다. 동시에, 속도와 비용 효율성에 최적화된 소형 모델 <strong>o4-mini</strong>도 함께 공개했다. o4-mini는 작은 크기에도 불구하고 수학, 코딩, 시각 이해 능력에서 이전 세대의 대형 모델에 필적하는 놀라운 성능을 보여주었다. 이는 특정 작업에 대해서는 거대 모델이 아닌, 잘 최적화된 소형 모델이 더 효율적인 해결책이 될 수 있음을 시사한다.51</li>
<li><strong>MIT CSAIL 연구:</strong> AI 기술의 신뢰성과 과학적 응용 가능성을 탐구하는 중요한 연구들도 발표되었다. MIT에서 분사한 스타트업 <strong>Themis AI</strong>는 AI 모델이 자신의 예측에 대해 얼마나 확신하는지를 정량화하는 ‘불확실성’ 측정 플랫폼 **‘Capsa’**를 개발했다. 이는 AI가 “모르겠다“고 말할 수 있게 만들어, 자율주행이나 신약 개발과 같이 안전이 중요한 분야에서 AI의 신뢰도를 높이는 핵심 기술이다.52 또 다른 연구 그룹인<br />
<strong>FutureHouse</strong>는 가설 수립, 실험 설계, 데이터 분석 등 과학적 발견 프로세스의 일부를 AI 에이전트가 자동화하는 연구를 발표하며 ‘AI for Science’ 분야의 새로운 가능성을 열었다.53</li>
</ul>
<h2>5.  주요 학회 하이라이트 및 수상 연구 종합</h2>
<p>2025년 6월은 AI 및 로봇 분야의 양대 산맥이라 할 수 있는 CVPR과 RSS 학회가 연이어 개최되며 전 세계 연구자들의 이목을 집중시킨 달이었다. 두 학회에서 발표된 연구들과 수상작들은 현재 기술의 최전선을 조망하고 미래 연구 방향을 가늠하는 중요한 척도가 된다.</p>
<h3>5.1  CVPR 2025 종합 분석 (6월 11일-15일, 내슈빌)</h3>
<p>IEEE/CVF 주관의 CVPR(Computer Vision and Pattern Recognition)은 컴퓨터 비전 및 패턴 인식 분야에서 세계 최고 권위를 자랑하는 학회다. 2025년 행사는 미국 테네시주 내슈빌의 뮤직 시티 센터에서 개최되었으며, 전 세계 학계 및 산업계에서 12,000명 이상의 전문가가 참가하여 성황을 이루었다.54</p>
<p>학회 프로그램 전반에 걸쳐 로보틱스와의 강력한 연계가 두드러졌다. “산업 로봇 자동화를 위한 인식(Perception for Industrial Robotics Automation)”, “신경 렌더링과 로봇 학습의 간극 해소(Real-to-Sim: Bridging the Gap between Neural Rendering and Robot Learning)”, “NVIDIA Kaolin 라이브러리를 활용한 3D 딥러닝 실습” 등 로보틱스 응용을 직접적으로 다루는 다수의 워크숍이 개최되어 많은 관심을 모았다.54</p>
<p>핵심 기술 동향으로는 본 보고서의 제2장에서 심도 있게 분석한 바와 같이, 3D 비전 분야에서 전통적인 기하학적 최적화를 탈피하려는 신경망 기반의 엔드투엔드 접근법이 대두되었다. 이와 더불어, 비전-언어 모델(VLM), 확산 모델(diffusion models)을 포함한 생성 모델, 그리고 동영상 이해 및 합성에 대한 연구가 지속적으로 강세를 보였다.54</p>
<p><strong>표 1: CVPR 2025 주요 수상 논문 및 핵심 기여</strong></p>
<table><thead><tr><th>수상 부문</th><th>논문 제목</th><th>저자</th><th>핵심 기여</th></tr></thead><tbody>
<tr><td><strong>최우수 논문</strong></td><td>VGGT: Visual Geometry Grounded Transformer</td><td>Jianyuan Wang et al.</td><td>단일 피드포워드 트랜스포머로 수백 장의 이미지에서 카메라, 깊이, 3D 포인트를 동시에 추론. 후처리 최적화가 불필요함. 21</td></tr>
<tr><td><strong>최우수 학생 논문</strong></td><td>Neural Inverse Rendering from Propagating Light</td><td>Anagh Malik et al.</td><td>LiDAR의 빛 전파 비디오를 활용, 간접광까지 모델링하는 물리 기반 신경망 역렌더링으로 정확한 3D 형상 복원. 21</td></tr>
<tr><td>최우수 논문 가작</td><td>MegaSaM: Accurate, Fast and Robust Structure and Motion from Casual Dynamic Videos</td><td>Zhengqi Li et al.</td><td>동적 객체가 많거나 카메라 움직임이 적은 비디오에서도 강건하게 카메라 포즈와 깊이를 추정하는 SLAM 시스템. 21</td></tr>
<tr><td>최우수 논문 가작</td><td>Navigation World Models</td><td>Amir Bar et al.</td><td>제어 가능한 비디오 생성 모델(CDiT)을 월드 모델로 활용하여 내비게이션 경로를 시뮬레이션하고 계획. 21</td></tr>
<tr><td>최우수 논문 가작</td><td>Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models</td><td>Matt Deitke et al.</td><td>독점 모델의 합성 데이터 없이 순수 인간 주석 데이터만으로 SOTA VLM을 구축하여 완전한 개방성을 달성. 21</td></tr>
<tr><td>최우수 논문 가작</td><td>3D Student Splatting and Scooping</td><td>Jialin Zhu et al.</td><td>3D 가우시안 스플래팅을 스튜던트 t-분포와 ‘스쿠핑’ 개념으로 개선하여 표현 효율성과 렌더링 품질을 향상. 21</td></tr>
</tbody></table>
<h3>5.2  RSS 2025 종합 분석 (6월 21일-25일, 로스앤젤레스)</h3>
<p>RSS(Robotics: Science and Systems)는 로보틱스 분야의 기초 과학과 시스템 공학에 중점을 둔 최고 수준의 학회 중 하나다. 21회째를 맞이한 2025년 행사는 미국 로스앤젤레스의 서던캘리포니아 대학교(USC)에서 개최되었으며, 594편의 논문이 제출되고 약 1,300명이 참석하는 등 역대 최대 규모로 치러졌다.69</p>
<p>학회에서는 총 32개의 워크숍이 진행되었으며, 특히 “로봇의 의미론적 추론 및 목표 이해(Semantic Reasoning and Goal Understanding in Robotics, SemRob)”, “오프로드 로보틱스(Off-Road Robotics)”, “생성 모델과 인간-로봇 상호작용(Generative Modeling Meets Human-Robot Interaction)” 등 체화된 AI와 파운데이션 모델 관련 주제들이 중심을 이루었다.26 이는 로보틱스 연구의 패러다임이 VLM, 월드 모델, 생성 모델과 같은 대규모 AI 모델을 로봇의 인식, 계획, 제어 문제에 통합하는 방향으로 빠르게 이동하고 있음을 보여준다. 인간과의 자연스러운 상호작용, 예측 불가능한 실제 환경에서의 강건성, 그리고 데이터 효율성 등이 주요 연구 과제로 심도 있게 다루어졌다.26</p>
<p><strong>표 2: RSS 2025 주요 수상 논문 및 핵심 기여</strong></p>
<table><thead><tr><th>수상 부문</th><th>논문 제목</th><th>저자</th><th>핵심 기여</th></tr></thead><tbody>
<tr><td><strong>Test of Time Award</strong></td><td>Cooperative Manipulation and Transportation with Aerial Robots (2009)</td><td>Nathan Michael et al.</td><td>다중 드론의 협력 조작 및 운송 분야의 초석을 다진 연구로, 현재 드론 군집 제어 및 계획 연구의 이론적 기반을 제공. 78</td></tr>
<tr><td><strong>SemRob 워크숍 최우수 논문</strong></td><td>WoMAP: World Models For Embodied Open-Vocabulary Object Localization</td><td>Tenny Yin et al.</td><td>VLM의 상식 추론과 월드 모델의 동역학 예측을 결합하여, 전문가 시연 데이터 없이 개방형 어휘 객체 탐색 정책을 학습. 26</td></tr>
<tr><td><strong>SemRob 워크숍 차점 논문</strong></td><td>Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models</td><td>Xiaoyang Shi et al.</td><td>계층적 VLA 모델을 통해 복잡하고 개방된 형태의 자연어 지시를 이해하고 수행하는 로봇 제어 기술을 제시. 26</td></tr>
<tr><td><strong>Off-Road 워크숍 차점 논문</strong></td><td>OVERSEEC – Open-Vocabulary Cost Map Generation from Satellite Images and Natural Language</td><td>Rwik Rana et al.</td><td>위성 이미지와 자연어 선호도를 입력받아, 제로샷으로 오프로드 주행을 위한 비용 맵(cost map)을 생성하는 신경-상징적 파이프라인. 73</td></tr>
</tbody></table>
<h2>6. 결론: 2025년 6월 연구 동향 종합 및 미래 전망</h2>
<p>2025년 6월 한 달간 발표된 AI 및 로봇 분야의 주요 연구들은 기술 발전의 중요한 변곡점을 명확히 보여주었다. 이는 AI가 디지털 세계의 방대한 데이터 속에서 패턴을 학습하는 단계를 넘어, 물리 세계의 복잡한 인과 관계와 동역학을 이해하고 그 안에서 목적을 가지고 행동하려는 ’체화(embodiment)’를 향한 본격적인 여정의 시작을 알리는 신호탄이었다.</p>
<h3>6.1 핵심 연구 동향 요약</h3>
<p>이번 기간의 연구 동향은 세 가지 핵심적인 흐름으로 요약할 수 있다.</p>
<ol>
<li><strong>월드 모델의 실용화:</strong> Meta AI의 V-JEPA 2와 Google DeepMind의 Gemini Robotics 1.5 발표는 월드 모델이 더 이상 이론적 개념이나 단순한 시뮬레이션 환경에 국한되지 않음을 입증했다. 이 모델들은 실제 로봇 하드웨어에 탑재되어, 사전 학습되지 않은 새로운 환경과 과제에 대해 제로샷 일반화(zero-shot generalization)와 같은 복잡한 문제를 해결하는 핵심 동력으로 작동하기 시작했다. 이는 AI가 ‘생각하고 계획하는’ 능력을 갖추어 물리 세계와 상호작용하는 시대로 진입했음을 의미한다.</li>
<li><strong>3D 비전의 신경망화:</strong> CVPR 2025 최우수 논문으로 선정된 VGGT는 3D 비전 분야의 오랜 패러다임이었던 기하학적 최적화의 종말을 예고했다. 단일 순방향 신경망이 수백 장의 이미지로부터 1초 이내에 장면의 모든 3D 정보를 복원해내는 능력은, 3D 인식이 더 이상 로보틱스와 같은 실시간 애플리케이션의 병목점이 아니라, 오히려 강력한 ’파운데이션’이 될 수 있음을 보여주었다. 이는 로봇 인식 기술의 새로운 지평을 연 기념비적인 성과다.</li>
<li><strong>개방성과 안전성의 동시 부상:</strong> 기술의 발전 방향이 다각화되고 있다. Molmo와 같은 완전 개방형 모델의 등장은 기술 민주화를 촉진하고 연구 생태계의 건강한 발전을 유도하는 긍정적인 흐름을 만들었다. 동시에, OpenAI의 위협 보고서는 고도화된 AI 기술이 사회를 위협하는 도구로 악용될 수 있는 현실적인 위험을 경고했다. 이는 AI 기술의 발전이 성능 향상뿐만 아니라, 책임감 있는 사용과 통제에 대한 더 깊은 사회적, 기술적 고민을 동반해야 함을 강력하게 시사한다.</li>
</ol>
<h3>6.2 미래 전망</h3>
<p>이러한 동향을 바탕으로 AI 및 로봇 기술의 미래를 다음과 같이 전망할 수 있다.</p>
<ul>
<li><strong>하이브리드 AI 아키텍처의 부상:</strong> V-JEPA 2의 암묵적, 관찰 기반 학습 방식과 Gemini Robotics의 명시적, 언어 기반 추론 방식, 그리고 WoMAP과 같은 학계의 하이브리드 접근은 미래의 지능형 에이전트가 단일한 아키텍처에 의존하지 않을 것임을 보여준다. 대신, 해결하고자 하는 과제의 추상화 수준과 요구 사항에 따라 여러 종류의 추론 및 예측 메커니즘을 유기적으로 결합하는 고도로 복잡한 하이브리드 아키텍처가 표준이 될 것이다.</li>
<li><strong>3D 비전 파운데이션 모델의 생태계 확장:</strong> VGGT와 같이 장면 전체를 이해하는 거대 3D 비전 파운데이션 모델을 기반으로, 특정 로봇 작업(예: 정밀한 부품 조립, 고속 드론 비행)에 특화된 더 작고 효율적인 3D 인식 모델들이 파생되어 하나의 거대한 기술 생태계를 형성할 것이다. 이는 2D 비전 분야에서 대규모 사전 학습 모델을 기반으로 다양한 응용 모델이 발전해 온 과정을 3D 영역에서 재현하게 될 것이다.</li>
<li><strong>‘안전 내장형(Safety-by-Design)’ AI의 대두:</strong> AI의 악용 가능성이 현실적인 위협으로 다가옴에 따라, 모델 개발 초기 단계부터 안전성, 투명성, 통제 가능성을 핵심적인 설계 원칙으로 고려하는 ‘안전 내장형 AI’ 연구가 AI 안전 분야의 핵심 주제로 부상할 것이다. Themis AI의 불확실성 정량화 연구나 Gemini Robotics의 ’사고하는 VLA’를 통한 행동 해석 가능성 확보는 이러한 흐름의 초기 단계를 보여주는 사례다.</li>
</ul>
<p>결론적으로 2025년 6월은 AI가 물리 세계를 향한 문을 본격적으로 열어젖힌 시기였다. 앞으로의 연구는 이 문을 통해 얼마나 더 넓고 깊은 세계로 나아갈 수 있는지, 그리고 그 과정에서 기술의 힘을 어떻게 인류에게 이로운 방향으로 이끌어 나갈 것인지에 대한 끊임없는 탐구가 될 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>World Models, https://worldmodels.github.io/</li>
<li>Learning Latent Dynamics for Planning from Pixels, https://proceedings.mlr.press/v97/hafner19a/hafner19a.pdf</li>
<li>Introducing the V-JEPA 2 world model and new benchmarks for …, https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/</li>
<li>Introducing V-JEPA 2 - Meta AI, https://ai.meta.com/vjepa/</li>
<li>V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning, https://arxiv.org/html/2506.09985v1</li>
<li>AI Research - Meta AI, https://ai.meta.com/research/</li>
<li>Next big thing after LLMs - World Model [explained on the example of V-JEPA2] - Reddit, https://www.reddit.com/r/LocalLLaMA/comments/1m4mfs8/next_big_thing_after_llms_world_model_explained/</li>
<li>V-JEPA 2: Meta’s World Model for AI Robotics and Planning - LearnOpenCV, https://learnopencv.com/v-jepa-2-meta-world-model-robotics-guide/</li>
<li>Learning from Reward-Free Offline Data: A Case for Planning with Latent Dynamics Models, https://latent-planning.github.io/</li>
<li>V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning, https://www.alphaxiv.org/overview/2506.09985v1</li>
<li>Paper Review: V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning - Andrew Lukyanenko, https://artgor.medium.com/paper-review-v-jepa-2-self-supervised-video-models-enable-understanding-prediction-and-planning-28410d8a1c6b</li>
<li>V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning, https://ai.meta.com/research/publications/v-jepa-2-self-supervised-video-models-enable-understanding-prediction-and-planning/</li>
<li>Meta’s V JEPA 2 - The AI Model That Teaches Robots Using Raw Video No Human Labels!, https://www.youtube.com/watch?v=onXM8fRkfiI</li>
<li>Meta’s V-JEPA 2: AI That Thinks Like Humans Before Acting! - YouTube, https://www.youtube.com/watch?v=G5YH869B-X0</li>
<li>V-JEPA 2 w/ Nicolas Ballas - YouTube, https://m.youtube.com/watch?v=o8Cexk56oBk</li>
<li>Gemini Robotics 1.5 brings AI agents into the physical world - Google DeepMind, https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/</li>
<li>Gemini Robotics 1.5 enables agentic experiences, explains Google DeepMind, https://www.therobotreport.com/gemini-robotics-1-5-enables-agentic-experiences-explains-google-deepmind/</li>
<li>Gemini Robotics 1.5: Pushing the Frontier of … - Googleapis.com, https://storage.googleapis.com/deepmind-media/gemini-robotics/Gemini-Robotics-1-5-Tech-Report.pdf</li>
<li>Gemini Robotics-ER 1.5 | Gemini API | Google AI for Developers, https://ai.google.dev/gemini-api/docs/robotics-overview</li>
<li>Building the Next Generation of Physical Agents with Gemini Robotics-ER 1.5, https://developers.googleblog.com/en/building-the-next-generation-of-physical-agents-with-gemini-robotics-er-15/</li>
<li>CVPR 2025 Best Papers and Best Demos, https://cvpr.thecvf.com/Conferences/2025/BestPapersDemos</li>
<li>Navigation World Models - Amir Bar, https://www.amirbar.net/nwm/</li>
<li>Navigation World Models - arXiv, https://arxiv.org/html/2412.03572v1</li>
<li>Navigation World Models | CVF Open Access, https://openaccess.thecvf.com/content/CVPR2025/papers/Bar_Navigation_World_Models_CVPR_2025_paper.pdf</li>
<li>[Literature Review] Navigation World Models - Moonlight, https://www.themoonlight.io/en/review/navigation-world-models</li>
<li>RSS SemRob 2025, https://semrob.github.io/</li>
<li>WoMAP: World Models For Embodied Open-Vocabulary Object …, https://semrob.github.io/docs/2025_rss_semrob.github.io_paper13.pdf</li>
<li>[Literature Review] WoMAP: World Models For Embodied Open-Vocabulary Object Localization - Moonlight, https://www.themoonlight.io/en/review/womap-world-models-for-embodied-open-vocabulary-object-localization</li>
<li>WoMAP: World Models For Embodied Open-Vocabulary Object Localization - arXiv, https://arxiv.org/html/2506.01600v1</li>
<li>WoMAP: World Models For Embodied Open-Vocabulary Object Localization | OpenReview, <a href="https://openreview.net/forum?id=KXzkAje2uQ&amp;referrer=%5Bthe+profile+of+Emily+Zhou%5D(/profile?id%3D~Emily_Zhou2)">https://openreview.net/forum?id=KXzkAje2uQ&amp;referrer=%5Bthe%20profile%20of%20Emily%20Zhou%5D(%2Fprofile%3Fid%3D~Emily_Zhou2)</a></li>
<li>VGGT is a Pure Neural Approach to 3D Vision | by Harpreet Sahota …, https://medium.com/voxel51/vggt-is-a-pure-neural-approach-to-3d-vision-32841d5e3c32</li>
<li>VGGT: Visual Geometry Grounded Transformer - arXiv, https://arxiv.org/html/2503.11651v1</li>
<li>Best Papers at CVPR Reveal New Results with Neural Networks for Real-Time Applications and Novel Ways to Manipulate Light for Scene Recovery, https://cvpr.thecvf.com/Conferences/2025/News/Awards_Press</li>
<li>VGGT: Visual Geometry Grounded Transformer - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_VGGT_Visual_Geometry_Grounded_Transformer_CVPR_2025_paper.pdf</li>
<li>VGGT: Visual Geometry Grounded Transformer, https://arxiv.org/pdf/2503.11651</li>
<li>VGGT: Visual Geometry Grounded Transformer – For Dense 3D Reconstruction, https://learnopencv.com/vggt-visual-geometry-grounded-transformer-3d-reconstruction/</li>
<li>Neural Inverse Rendering from Propagating Light - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2025/papers/Malik_Neural_Inverse_Rendering_from_Propagating_Light_CVPR_2025_paper.pdf</li>
<li>Neural Inverse Rendering from Propagating Light - arXiv, https://arxiv.org/html/2506.05347v1</li>
<li>Neural Inverse Rendering from Propagating Light - ChatPaper, https://chatpaper.com/paper/146562</li>
<li>MegaSaM: Accurate, Fast and Robust Structure … - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2025/papers/Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual_CVPR_2025_paper.pdf</li>
<li>MegaSaM: Accurate, Fast, and Robust Structure and Motion from …, https://www.alphaxiv.org/overview/2412.04463v2</li>
<li>MegaSaM: Accurate, Fast, and Robust Structure and Motion from Casual Dynamic Videos | Request PDF - ResearchGate, https://www.researchgate.net/publication/394581523_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual_Dynamic_Videos</li>
<li>CVPR 2025 2025 Award Candidates - The Computer Vision Foundation, https://cvpr.thecvf.com/virtual/2025/events/AwardCandidates2025</li>
<li>GitHub - realcrane/3D-student-splatting-and-scooping: This is the source code of our CVPR 2025 Best Paper Honourable Mention paper, https://github.com/realcrane/3D-student-splatting-and-scooping</li>
<li>3D Student Splatting and Scooping - arXiv, https://arxiv.org/html/2503.10148v1</li>
<li>[2503.10148] 3D Student Splatting and Scooping - arXiv, https://arxiv.org/abs/2503.10148</li>
<li>Molmo and PixMo: Open Weights and Open … - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2025/papers/Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art_CVPR_2025_paper.pdf</li>
<li>CVPR Poster Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models, https://cvpr.thecvf.com/virtual/2025/poster/33073</li>
<li>Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models, https://huggingface.co/papers/2409.17146</li>
<li>Blog · OpenAI’s June 2025 Threat Report: Disrupting malicious uses of AI - Dan Stroot, https://www.danstroot.com/posts/2025-06-09-malicious-ai-usage-threats</li>
<li>Introducing OpenAI o3 and o4-mini | OpenAI, https://openai.com/index/introducing-o3-and-o4-mini/</li>
<li>Teaching AI models what they don’t know | MIT News …, https://news.mit.edu/2025/themis-ai-teaches-ai-models-what-they-dont-know-0603</li>
<li>Accelerating scientific discovery with AI | MIT News | Massachusetts Institute of Technology, https://news.mit.edu/2025/futurehouse-accelerates-scientific-discovery-with-ai-0630</li>
<li>2025 Conference - CVPR - The Computer Vision Foundation, https://cvpr.thecvf.com/Conferences/2025</li>
<li>2025 Dates and Deadlines - CVPR - The Computer Vision Foundation, https://cvpr.thecvf.com/Conferences/2025/Dates</li>
<li>2025 Expo Schedule - CVPR, https://cvpr.thecvf.com/Conferences/2025/ExpoSchedule</li>
<li>2025 CVPR Exhibitor Prospectus, https://cvpr.exhibitprospectus.com/</li>
<li>Wenyan Cong Receives Best Paper Award at CVPR Workshop | Texas ECE, https://www.ece.utexas.edu/news/wenyan-cong-receives-best-paper-award-cvpr-workshop</li>
<li>CVPR.2025 - Highlight | Cool Papers - Immersive Paper Discovery, https://papers.cool/venue/CVPR.2025?group=Highlight</li>
<li>Most Influential CVPR Papers (2025-09 Version) - Paper Digest, https://www.paperdigest.org/2025/09/most-influential-cvpr-papers-2025-09-version/</li>
<li>Ten papers by CSE researchers at CVPR 2025 - University of Michigan, https://cse.engin.umich.edu/stories/ten-papers-by-cse-researchers-at-cvpr-2025</li>
<li>The Best of CVPR 2025 Series – Day 3 - Voxel51, https://voxel51.com/blog/the-best-of-cvpr-2025-series-day-3</li>
<li>The Best of CVPR 2025 Series — Day 1 | by Paula Ramos, PhD. | Medium, https://medium.com/@paularamos_phd/the-best-of-cvpr-2025-series-day-1-7b1a7925da39</li>
<li>CVPR 2025 Accepted Papers, https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers</li>
<li>Paper Digest: CVPR 2025 Papers &amp; Highlights, https://www.paperdigest.org/2025/06/cvpr-2025-papers-highlights/</li>
<li>Apple Machine Learning Research at CVPR 2025, https://machinelearning.apple.com/research/cvpr-2025</li>
<li>CVPR 2025 Papers, https://cvpr.thecvf.com/virtual/2025/papers.html</li>
<li>SkalskiP/top-cvpr-2025-papers - GitHub, https://github.com/SkalskiP/top-cvpr-2025-papers</li>
<li>Robotics: Science and Systems (RSS) Conference - MassRobotics, https://www.massrobotics.org/event/robotics-science-and-systems-rss-conference/</li>
<li>Robotics: Science and Systems (RSS), 2025 | ServiceNow Research, https://www.servicenow.com/research/event/2025-rss.html</li>
<li>RSS 2025 Conference - OpenReview, https://openreview.net/group?id=roboticsfoundation.org/RSS/2025/Conference</li>
<li>RSS 2025 Marks Largest in History as Conference Returns to USC, https://viterbischool.usc.edu/news/2025/07/rss-2025-marks-largest-in-history-as-conference-returns-to-usc/</li>
<li>RSS 2025 Workshop on Resilient Off-road Autonomous Robotics (ROAR) - GitHub Pages, https://off-roaders.github.io/off-road-workshop-2025/</li>
<li>Presenting at RSS 2025 workshops | Max Muchen Sun, https://maxsun.io/highlights/2025-rssws/</li>
<li>RSS 2025 Accepted Paper List - Paper Copilot, https://papercopilot.com/paper-list/rss-paper-list/rss-2025-paper-list/</li>
<li>RSS2025 Paper Announcements - MIT AeroAstro, https://aeroastro.mit.edu/realm/news/rss2025-paper-announcements/</li>
<li>Workshop on Learned Robot Representations | RSS 2025, https://rss25-roboreps.github.io/</li>
<li>Test of Time Award - Robotics: Science and Systems, https://roboticsconference.org/program/testoftimeaward/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>