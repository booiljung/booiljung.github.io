<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:피지컬 AI의 진화: VLA의 한계와 LBM(Large Behavior Model)으로의 전환</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>피지컬 AI의 진화: VLA의 한계와 LBM(Large Behavior Model)으로의 전환</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2025년 AI 및 로봇 연구 동향</a> / <span>피지컬 AI의 진화: VLA의 한계와 LBM(Large Behavior Model)으로의 전환</span></nav>
                </div>
            </header>
            <article>
                <h1>피지컬 AI의 진화: VLA의 한계와 LBM(Large Behavior Model)으로의 전환</h1>
<p>2025-12-30, G30DR</p>
<h2>1.  개요 (Executive Summary)</h2>
<p>2023년부터 2025년 초반까지 로봇 공학 및 인공지능(AI) 업계는 거대언어모델(LLM)의 성공을 물리적 세계로 확장하려는 시도인 <strong>‘비전-언어-행동(VLA, Vision-Language-Action)’</strong> 모델을 표준 아키텍처로 채택해 왔다. Google DeepMind의 RT-2나 OpenVLA와 같은 모델들은 “행동도 언어의 일종“이라는 가설 하에, 로봇의 제어 신호를 텍스트 토큰처럼 처리하여 통합된 트랜스포머(Transformer) 모델로 학습시켰다. 그러나 2025년 하반기에 접어들며 축적된 실증 데이터와 벤치마크 결과는 VLA가 가진 구조적 한계를 명확히 드러내고 있다. VLA는 높은 수준의 의미론적 추론(Semantic Reasoning)에는 강점이 있으나, 정밀한 물리적 제어와 실시간 반응성 측면에서는 ’신체 없는 뇌(Brain without Body)’와 같은 한계를 보이고 있다.</p>
<p>본 보고서는 현재의 VLA 패러다임이 과도기적 기술임을 천명하고, 차세대 피지컬 AI의 표준 명칭으로 **LBM(Large Behavior Model, 거대행동모델)**의 도입을 제안한다. LBM은 기존 VLA와 달리 행동을 이산적 언어 토큰의 나열이 아닌, 연속적이고 고차원적인 생성 과정(Generative Process)으로 정의한다. 이는 <strong>이중 루프 시스템(Dual Loop System)</strong> 아키텍처를 기반으로 하며, 고수준의 추론을 담당하는 대뇌(System 2)와 고속 반사 신경을 담당하는 소뇌(System 1)의 결합을 통해 구현된다.</p>
<p>본 보고서는 방대한 최신 연구 결과를 바탕으로 VLA의 기술적 한계(모달리티 격차, 추론 지연, 비전 편향)를 심층 분석하고, 이를 극복하기 위한 핵심 솔루션인 **로봇 네이티브 언어(Latent Action Space)**와 <strong>확산 정책(Diffusion Policy)</strong> 기반의 LBM 아키텍처를 구체적으로 제시한다. 이는 단순한 용어의 변경이 아니며, 로봇이 물리 세계를 이해하고 상호작용하는 방식을 근본적으로 재정의하는 전략적 전환점이다.</p>
<h2>2.  현황 및 문제점 분석: VLA(Vision-Language-Action)의 한계</h2>
<p>지난 2년간 로봇 공학계는 “모든 것을 하나의 모델로(End-to-End)” 처리하려는 VLA의 이상에 매료되어 있었다. 그러나 2025년 수행된 대규모 벤치마크(AGNOSTOS, VLATest 등) 결과, VLA 모델들은 학습된 작업 내에서는 우수한 성능을 보였으나, 새로운 물리적 상황이나 정밀한 조작이 필요한 과제에서는 실패율이 급격히 증가하는 현상이 관찰되었다. 이는 데이터의 부족 문제가 아닌, 아키텍처 자체의 구조적 결함에 기인한다.</p>
<h3>2.1  언어의 불일치성 (The Modality Gap): 이산적 기호와 연속적 물리량의 충돌</h3>
<p>VLA 아키텍처의 가장 근본적인 문제는 **‘모달리티 격차(The Modality Gap)’**이다. LLM은 본질적으로 이산적(Discrete)이고 상징적(Symbolic)인 데이터를 처리하도록 설계되었다. 반면, 로봇의 제어는 연속적(Continuous)이고 기하학적(Geometric)인 물리량을 다룬다.</p>
<h4>2.1.1  양자화 오차와 정밀도 손실 (Quantization Loss)</h4>
<p>기존 VLA 모델(예: RT-2)은 로봇의 관절 각도나 위치 좌표를 0에서 255 사이의 정수 토큰으로 변환(Binning)하여 처리한다. 예를 들어, 0.001mm 단위의 정밀도가 요구되는 산업용 조립 작업에서, 이를 256개의 구간으로 나누어 토큰화할 경우 필연적인 정보 손실이 발생한다. “조금 더 오른쪽으로“라는 언어적 지시는 의미론적으로는 명확하지만, 물리적으로는 <span class="math math-inline">0.1mm</span> 이동인지 <span class="math math-inline">1cm</span> 이동인지 불분명하다. 이러한 불일치는 정밀 조작(Fine Manipulation) 작업에서 로봇이 목표 지점에서 미세하게 떨리는 진동(Jittering) 현상을 유발하거나, 물체를 놓치는 주요 원인이 된다.</p>
<h4>2.1.2  잠재 공간의 위상학적 불일치 (Topological Mismatch)</h4>
<p>심층 연구에 따르면, 텍스트 임베딩 공간과 이미지/행동 임베딩 공간 사이에는 근본적인 위상학적 단절이 존재한다. 텍스트 공간에서는 ’빠르게’와 ’신속하게’가 가까운 거리에 위치하지만, 물리적 제어 공간에서 ’빠른 동작’은 높은 가속도와 힘을 의미하며, 이는 시스템의 안정성을 해칠 수 있는 완전히 다른 차원의 벡터이다. VLA는 텍스트 데이터로 사전 학습된 LLM의 ’두뇌’를 강제로 로봇의 ’몸’에 이식하려 했기 때문에, 로봇은 물리적 역학(Dynamics)을 이해하는 대신 텍스트 패턴을 흉내 내는 방식으로 행동을 생성하게 된다. 이는 복잡한 물리적 상호작용(예: 끈 묶기, 액체 따르기)에서 모델이 환각(Hallucination)을 일으켜 물리적으로 불가능한 궤적을 생성하는 결과로 이어진다.</p>
<h3>2.2  추론 속도와 반응성 (Latency): 실시간 제어의 병목</h3>
<p>두 번째 결정적인 한계는 **추론 지연(Inference Latency)**이다. 로봇의 제어 루프는 일반적으로 30Hz에서 100Hz(초당 30~100회)의 속도로 돌아가야 실시간으로 변화하는 환경에 대응할 수 있다. 그러나 거대 모델 기반의 VLA는 이 속도를 따라가지 못한다.</p>
<h4>2.2.1  자기회귀적 생성의 한계 (Autoregressive Bottleneck)</h4>
<p>대부분의 VLA는 LLM과 동일한 자기회귀(Autoregressive) 방식을 사용한다. 즉, 1초 분량의 행동을 생성하기 위해 수십 개의 토큰을 순차적으로 하나씩 예측해야 한다. 70B(700억) 파라미터 규모의 모델이 이 과정을 수행하는 데는 수백 밀리초(ms)가 소요되며, 이는 로봇 공학적 관점에서 ’영겁’에 가까운 시간이다. 이로 인해 ’맹목 구간(Blind Time)’이 발생한다. 모델이 다음 행동 토큰을 생성하는 동안, 현실 세계에서 물체가 미끄러지거나 사람이 로봇 앞으로 지나가도 로봇은 즉각적으로 반응할 수 없다. 이는 동적인 환경(Dynamic Environment)에서의 안전성을 심각하게 저해하며, 인간과의 협업(HRI)을 불가능하게 만든다.</p>
<h4>2.2.2  엣지 컴퓨팅의 제약</h4>
<p>실제 로봇 서비스 현장에서는 전력과 공간의 제약으로 인해 NVIDIA RTX 4090과 같은 고성능 GPU를 탑재하기 어렵다. Jetson AGX Orin과 같은 엣지 디바이스에서 수십 억 파라미터의 VLA를 구동할 경우, 추론 속도는 5Hz 미만으로 떨어지며, 이는 로봇이 마치 술에 취한 듯 느리고 부정확하게 움직이는 원인이 된다.</p>
<table><thead><tr><th><strong>특성</strong></th><th><strong>기존 VLA (End-to-End)</strong></th><th><strong>요구되는 로봇 제어 기준</strong></th><th><strong>격차(Gap)</strong></th></tr></thead><tbody>
<tr><td><strong>제어 주파수</strong></td><td>3Hz ~ 10Hz (느림)</td><td>30Hz ~ 500Hz (빠름)</td><td><strong>실시간성 상실</strong></td></tr>
<tr><td><strong>생성 방식</strong></td><td>토큰 단위 순차 생성 (Serial)</td><td>전체 궤적 동시 생성 (Parallel)</td><td><strong>반응성 저하</strong></td></tr>
<tr><td><strong>데이터 처리</strong></td><td>이산적 토큰 (Discrete)</td><td>연속적 신호 (Continuous)</td><td><strong>정밀도 손실</strong></td></tr>
</tbody></table>
<h3>2.3  비전(Vision) 편향의 한계: ’보는 것’과 ’하는 것’의 차이</h3>
<p>현재의 VLA 모델은 **‘시각 중심(Vision-Centric)’**이다. 이는 인터넷상의 데이터가 텍스트와 이미지/비디오로 편중되어 있기 때문이다. 로봇에게 필수적인 <strong>촉각(Tactile)</strong>, <strong>고유수용성감각(Proprioception, 관절의 힘과 위치)</strong> 데이터는 극도로 희소하다.</p>
<h4>2.3.1  물리적 상호작용의 부재</h4>
<p>VLA는 “컵을 잡으라“는 명령을 받으면 시각적으로 컵의 위치를 파악할 수 있다. 그러나 컵이 얼마나 미끄러운지, 얼마나 무거운지, 재질이 연한지 단단한지는 알 수 없다. 시각 정보만으로는 ’접촉(Contact)’의 순간 발생하는 미세한 힘의 변화를 감지할 수 없기 때문이다. 예를 들어, 2025년 발표된 <strong>UniTouch</strong>나 <strong>AnyTouch</strong> 연구에 따르면, 시각 정보에만 의존한 로봇은 투명한 물체나 거울 반사가 있는 물체를 조작할 때 실패율이 높으며, 특히 힘 조절이 필요한 작업(예: 계란 집기)에서 잦은 파손을 일으킨다.</p>
<h4>2.3.2  데이터의 환각 (Physics Hallucination)</h4>
<p>VLA 모델은 부족한 물리 정보를 시각적 패턴으로 메우려 시도한다. 즉, 물체의 겉모습만 보고 무게와 마찰력을 ’추측’한다. 이는 정형화된 환경에서는 작동할지 모르나, 겉보기와 다른 물체(예: 무거워 보이지만 가벼운 스티로폼 바위)를 다룰 때 치명적인 오류를 범하게 된다. 이는 VLA가 물리 법칙을 학습한 것이 아니라, 픽셀 패턴과 텍스트 사이의 통계적 상관관계만을 학습했음을 시사한다.</p>
<h2>3.  차세대 기술 솔루션: 로봇 네이티브 언어의 도입</h2>
<p>VLA의 한계를 극복하기 위해 2025년 하반기부터 부상한 핵심 전략은 **‘로봇 네이티브 언어(Robot Native Language)’**의 구축이다. 이는 인간의 언어(텍스트)를 억지로 로봇 제어에 끼워 맞추는 것이 아니라, 로봇의 행동 역학(Dynamics)을 가장 잘 표현할 수 있는 새로운 잠재 공간(Latent Space)을 정의하는 것이다.</p>
<h3>3.1  로봇 언어 (Latent Vector) 구축: 행동 토큰화(Action Tokenization)</h3>
<p>로봇의 행동을 효과적으로 학습시키기 위해서는 연속적인 움직임을 압축하면서도 정보 손실을 최소화하는 기술이 필요하다. 이를 위해 **벡터 양자화 변분 오토인코더(VQ-VAE)**와 <strong>잔차 벡터 양자화(Residual VQ)</strong> 기술이 표준으로 자리 잡고 있다.</p>
<h4>3.1.1  잔차 벡터 양자화 (Residual Vector Quantization, RVQ)</h4>
<p>기존의 단순 양자화가 0.001mm의 정밀도를 1mm 단위로 뭉뚱그려 버렸다면, RVQ는 이를 계층적으로 분해하여 표현한다.</p>
<ol>
<li><strong>1단계 (Coarse Token):</strong> “팔을 앞으로 뻗는다” (대략적인 방향)</li>
<li><strong>2단계 (Fine Token):</strong> “손목을 3.5도 회전한다” (세부 조정)</li>
<li><strong>3단계 (Residual Token):</strong> “0.2N의 힘을 더한다” (미세 보정)</li>
</ol>
<p>이러한 계층적 토큰화(Hierarchical Tokenization) 방식은 **FAST(Flexible Action Sequence Tokenization)**나 <strong>OmniSAT</strong>과 같은 최신 연구에서 증명되었으며, 기존 방식 대비 압축률은 6.8배 높이면서도 동작 재구성(Reconstruction)의 정확도는 비약적으로 향상시켰다. 이제 로봇은 텍스트 토큰이 아닌, 이 ’잠재 행동 벡터(Latent Action Vector)’를 자신의 언어로 사용하여 사고하고 움직인다.</p>
<h4>3.1.2  로봇 방언의 통합 (Cross-Embodiment)</h4>
<p>로봇마다 신체 구조(자유도, 크기)가 다르다는 점은 그동안 큰 장벽이었다. 그러나 VQ-VAE 기반의 잠재 공간은 서로 다른 로봇(예: 휴머노이드, 로봇 팔, 4족 보행 로봇)의 움직임을 공통된 **‘행동 임베딩(Behavior Embedding)’**으로 매핑할 수 있게 해준다. 이는 마치 다양한 프로그래밍 언어가 기계어(Machine Code)로 컴파일되어 실행되는 것과 유사하다. Skild AI의 **‘Omni-bodied Brain’**이나 Physical Intelligence의 <strong>pi0</strong> 모델은 이러한 통합된 행동 토큰을 통해 하나의 모델로 수천 가지의 서로 다른 로봇을 제어하는 데 성공했다.</p>
<h3>3.2  이중 루프 시스템 (Dual Loop System): 인지적 분업</h3>
<p>VLA의 느린 속도와 물리적 무지를 해결하기 위한 아키텍처적 해답은 **‘이중 루프 시스템(Dual Loop System)’**이다. 이는 노벨상 수상자 대니얼 카너먼(Daniel Kahneman)의 **‘생각에 관한 생각(Thinking, Fast and Slow)’**에서 제시한 시스템 1(직관/반사)과 시스템 2(이성/추론)의 인지 모델을 로봇 공학적으로 구현한 것이다.</p>
<h4>3.2.1  System 2 (대뇌): 고수준 계획 및 추론 (VLM)</h4>
<ul>
<li><strong>역할:</strong> “식탁을 치워줘“와 같은 추상적 명령을 이해하고, 시각적 상황을 분석하여 작업 순서를 계획한다.</li>
<li><strong>구성:</strong> Gemini, GPT-4o, Llama-3-Vision과 같은 대형 VLM.</li>
<li><strong>작동 주기:</strong> 저주파 (0.5Hz ~ 5Hz). 느리지만 똑똑하다.</li>
<li><strong>출력:</strong> 직접 모터를 제어하는 것이 아니라, System 1에게 전달할 <strong>‘잠재 목표(Latent Goal)’</strong> 또는 **‘중간 경유점(Waypoint)’**을 생성한다. 예를 들어 “컵의 손잡이 좌표(x,y,z)로 이동하라“는 목표를 하달한다.</li>
</ul>
<h4>3.2.2  System 1 (소뇌): 고속 모션 생성 및 제어 (Diffusion Policy)</h4>
<ul>
<li><strong>역할:</strong> System 2가 내린 목표를 달성하기 위해, 실시간으로 변하는 관절의 위치, 속도, 힘을 제어한다. 장애물을 회피하고 균형을 잡는 반사적 동작을 담당한다.</li>
<li><strong>구성:</strong> <strong>확산 정책(Diffusion Policy)</strong> 또는 <strong>플로우 매칭(Flow Matching)</strong> 네트워크.</li>
<li><strong>작동 주기:</strong> 고주파 (30Hz ~ 100Hz). 빠르고 정밀하다.</li>
<li><strong>입력:</strong> System 2의 잠재 목표 + 실시간 고유수용성감각(Proprioception) + 촉각 데이터.</li>
<li><strong>출력:</strong> 연속적인 관절 토크(Torque) 또는 위치 제어 신호.</li>
<li><strong>핵심 혁신:</strong> System 1은 LLM처럼 다음 단어를 예측하는 것이 아니라, 확산 모델(Diffusion Model)을 통해 노이즈로부터 최적의 행동 궤적(Trajectory)을 한 번에 생성해낸다. 이는 멀티모달 분포(Multi-modal Distribution)를 표현할 수 있어, 같은 상황에서도 다양한 해결책을 유연하게 제시할 수 있다.</li>
</ul>
<p><strong>Table 1: System 1과 System 2의 역할 및 기술 비교</strong></p>
<table><thead><tr><th><strong>구분</strong></th><th><strong>System 2 (Reasoning)</strong></th><th><strong>System 1 (Action)</strong></th></tr></thead><tbody>
<tr><td><strong>비유</strong></td><td>대뇌 (Cortex) - 지휘자</td><td>소뇌 (Cerebellum) - 연주자</td></tr>
<tr><td><strong>핵심 모델</strong></td><td>Vision-Language Model (VLM)</td><td>Diffusion Transformer / Flow Matching</td></tr>
<tr><td><strong>주요 기능</strong></td><td>의미 이해, 장기 계획, 상식 추론</td><td>실시간 제어, 충돌 회피, 파지(Grasping)</td></tr>
<tr><td><strong>제어 주기</strong></td><td>1Hz ~ 5Hz (느림)</td><td>50Hz ~ 200Hz (빠름)</td></tr>
<tr><td><strong>데이터 형태</strong></td><td>텍스트, 이미지 토큰</td><td>연속적 잠재 벡터 (Latent Vector)</td></tr>
<tr><td><strong>대표 사례</strong></td><td>OpenAI GPT-4o, Google Gemini</td><td>Toyota TRI Diffusion Policy, NVIDIA GR00T</td></tr>
</tbody></table>
<h2>4.  피지컬 AI의 새로운 정의: LBM(Large Behavior Model)</h2>
<p>VLA가 ’보는 모델’이었다면, 차세대 모델은 ’행동하는 모델’이다. 본 보고서는 이러한 패러다임의 전환을 반영하여 **LBM(Large Behavior Model)**이라는 명칭을 공식적으로 제안한다.</p>
<h3>4.1  LBM의 정의 및 핵심 철학</h3>
<p><strong>LBM</strong>은 “인간의 시연(Demonstration)과 시뮬레이션 데이터를 통해 물리적 세계의 인과관계와 행동 패턴을 학습하고, 이를 바탕으로 다양한 환경과 신체(Embodiment)에 적용 가능한 연속적인 행동 궤적을 생성하는 파운데이션 모델“로 정의된다.</p>
<ul>
<li><strong>TRI(Toyota Research Institute)의 정의:</strong> “LBM은 로봇을 위한 LLM이다. 하지만 텍스트를 내뱉는 것이 아니라, 로봇의 직접적인 움직임을 생성한다. 이는 ’무엇(What)’을 해야 할지 뿐만 아니라 ‘어떻게(How)’ 해야 할지를 이해하는 모델이다.”</li>
<li><strong>Boston Dynamics의 접근:</strong> Atlas 로봇에 LBM을 적용하여, 코딩 한 줄 없이 로봇이 스스로 넘어짐을 감지하고 일어나는 동작, 복잡한 물체를 정리하는 동작을 학습시켰다. 이는 기존의 수작업 코딩(Hard-coding) 제어 방식을 완전히 대체한다.</li>
</ul>
<h3>4.2  주요 선도 사례 (Leading Cases)</h3>
<h4>4.2.1  Physical Intelligence의 ‘pi0’</h4>
<p>2025년 공개된 <strong>pi0(pi-zero)</strong> 모델은 LBM의 전형을 보여준다. pi0는 팔리젬마(PaliGemma)와 같은 VLM 백본을 사용해 의미론적 이해를 수행하지만, 행동 생성 부문에서는 <strong>플로우 매칭(Flow Matching)</strong> 기법을 사용하여 50Hz의 고속 연속 제어를 실현했다. 이는 VLA의 언어적 이해력과 제어 이론의 정밀함을 통합한 사례로, 세탁물 접기나 테이블 치우기와 같은 복잡한 비정형 작업을 성공적으로 수행했다.</p>
<h4>4.2.2  NVIDIA의 ‘GR00T’</h4>
<p>NVIDIA의 GR00T는 휴머노이드 로봇을 위한 범용 파운데이션 모델로, 명시적으로 System 1(Diffusion Transformer)과 System 2(VLM)를 분리하여 학습시켰다. 이를 통해 다양한 형태의 휴머노이드 로봇에 즉각적으로 적용(Zero-shot Transfer)할 수 있는 능력을 입증했다.</p>
<h4>4.2.3  Skild AI의 ‘Skild Brain’</h4>
<p>Skild AI는 로봇의 하드웨어 형태에 구애받지 않는 <strong>‘옴니 바디(Omni-bodied)’</strong> 브레인을 구축했다. 이는 4족 보행 로봇이나 바퀴 달린 로봇, 양팔 로봇 등 서로 다른 신체 구조를 가진 로봇들이 하나의 LBM을 공유하며 학습할 수 있음을 보여주었다. 데이터 희소성을 극복하기 위해 시뮬레이션에서 10억 단계 이상의 학습을 수행하고 이를 실세계로 전이(Sim-to-Real)하는 전략을 사용했다.</p>
<h2>5.  결론 및 제언: 2026년을 향한 로드맵</h2>
<h3>5.1  VLA 명칭 폐기 및 LBM 도입</h3>
<p>’비전-언어-행동(VLA)’이라는 용어는 모델의 입력값(Vision, Language)에 초점을 맞춘 2023년식 명칭이다. 이제 모델의 출력값과 본질적 기능인 ’행동(Behavior)’에 초점을 맞춘 **LBM(Large Behavior Model)**으로 용어를 통일해야 한다. 이는 연구 개발의 목표가 단순히 ’말을 알아듣는 로봇’이 아니라 ’유능하게 행동하는 로봇’임을 명확히 한다.</p>
<h3>5.2  촉각 파운데이션 모델(Tactile Foundation Model)의 통합</h3>
<p>LBM의 완성은 시각을 넘어선 <strong>촉각의 통합</strong>에 있다. 시각 정보만으로는 해결할 수 없는 ’접촉의 물리(Physics of Contact)’를 이해하기 위해, GelSight와 같은 시촉각 센서 데이터를 토큰화하여 LBM에 통합하는 연구가 필수적이다. 2025년의 AnyTouch와 같은 연구는 촉각 데이터가 로봇의 조작 능력을 비약적으로 향상시킴을 증명했다. 향후 LBM은 <strong>‘Visuo-Tactile-Proprioceptive(시각-촉각-고유감각)’</strong> 멀티모달 모델로 진화해야 한다.</p>
<h3>5.3  월드 모델(World Model)로의 확장</h3>
<p>궁극적으로 LBM은 단순한 행동 생성을 넘어, 자신의 행동이 환경에 미칠 결과를 예측하는 **‘임바디드 월드 모델(Embodied World Model)’**로 진화할 것이다. 로봇이 행동하기 전에 마음속으로(In-context) 결과를 시뮬레이션하고, 위험을 사전에 감지하여 계획을 수정하는 능력은 자율 로봇의 안전성과 지능을 완성하는 마지막 퍼즐이 될 것이다.</p>
<p><strong>결론적으로,</strong> 로봇 공학의 미래는 텍스트를 생성하는 트랜스포머에 있지 않다. 미래는 물리적 역학을 이해하고, 연속적인 행동을 생성하며, 실시간으로 반응하는 <strong>이중 루프 기반의 LBM</strong>에 있다. 지금은 VLA라는 과도기적 껍질을 깨고, 진정한 피지컬 AI인 LBM으로 나아가야 할 시점이다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Vision-Language-Action Models: Concepts, Progress, Applications …, https://arxiv.org/html/2505.04769v1</li>
<li>Exploring the Limits of Vision-Language-Action Manipulations in …, https://arxiv.org/html/2505.15660v1</li>
<li>Large Behavior Models (LBMs) - Hamman Samuel, PhD - Medium, https://hammansamuel.medium.com/large-behavior-models-lbms-4466595f24d4</li>
<li>Ground Slow, Move Fast: A Dual-System Foundation Model … - arXiv, https://arxiv.org/html/2512.08186v1</li>
<li>VLATest: Testing and Evaluating Vision-Language-Action Models …, https://wangzhijie.me/assets/pubs/fse25-vlatest.pdf</li>
<li>Learning to Act Anywhere with Task-centric Latent Actions - Robotics, https://www.roboticsproceedings.org/rss21/p014.pdf</li>
<li>VQ-VLA: Improving Vision-Language-Action Models via Scaling …, https://arxiv.org/html/2507.01016v1</li>
<li>Efficient Planning in a Compact Latent Action Space - OpenReview, https://openreview.net/forum?id=cA77NrVEuqn</li>
<li>ICLR 2023 Multimodal Representation Learning Workshop Summary, https://github.com/CMSC-818B/ICLR-2023-MRL-Summary</li>
<li>Understanding and Constructing Latent Modality Structures in Multi …, https://www.researchgate.net/publication/373312796_Understanding_and_Constructing_Latent_Modality_Structures_in_Multi-Modal_Representation_Learning</li>
<li>A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for …, https://www.researchgate.net/publication/397983235_Unifying_Perception_and_Action_A_Hybrid-Modality_Pipeline_with_Implicit_Visual_Chain-of-Thought_for_Robotic_Action_Generation</li>
<li>We Put Embodied AI on a 100g Device. Why Most VLAs Choke on …, https://deepsense.ai/blog/we-put-embodied-ai-on-a-100g-device-why-most-vlas-choke-on-the-edge-and-the-architecture-that-didnt/</li>
<li>Vision Language Action Models (VLA) &amp; Policies for Robots, https://learnopencv.com/vision-language-action-models-lerobot-policy/</li>
<li>A Contemporary Review for Visuotactile Sensors from the Signal …, https://arxiv.org/html/2406.12226v1</li>
<li>The Prediction Loop: How Your Brain Decides What Feels Real, https://medium.com/@w.gebhardt/the-prediction-loop-how-your-brain-decides-what-feels-real-b36d4d2af3bb</li>
<li>AnyTouch: Learning Unified Static-Dynamic Representation across …, https://arxiv.org/html/2502.12191v1</li>
<li>Vision-Language-Action Models for Robotics: A Review Towards …, https://ieeexplore.ieee.org/iel8/6287639/10820123/11164279.pdf</li>
<li>AnyTouch 2: General Optical Tactile Representation Learning For…, https://openreview.net/forum?id=ndilONnABZ</li>
<li>FASTER: TOWARD EFFICIENT AUTOREGRESSIVE VISION …, https://openreview.net/pdf/ed2c167ae5f97ee9516f3bf43638a5987b372e79.pdf</li>
<li>OmniSAT: Compact Action Token, Faster Auto Regression, https://openreview.net/forum?id=CuzTXLB7Jz</li>
<li>π 0 : Our First Generalist Policy - Physical Intelligence, https://www.physicalintelligence.company/blog/pi0</li>
<li>Skild AI Builds Omni-Bodied Robot Brain With NVIDIA, https://www.nvidia.com/en-us/customer-stories/skild-ai/</li>
<li>Accelerate Generalist Humanoid Robot Development with NVIDIA …, https://developer.nvidia.com/blog/accelerate-generalist-humanoid-robot-development-with-nvidia-isaac-gr00t-n1/</li>
<li>GR00T N1: An Open Foundation Model for Generalist Humanoid …, <a href="https://d1qx31qr3h6wln.cloudfront.net/publications/GR00T%20N1%20Whitepaper.pdf">https://d1qx31qr3h6wln.cloudfront.net/publications/GR00T%20N1%20Whitepaper.pdf</a></li>
<li>[Paper Review] Pi0, Pi0.5, Pi0-FAST - Tracing the Path of Physical …, https://bequiet-log.vercel.app/pi-review</li>
<li>Modality-Augmented Fine-Tuning of Foundation Robot Policies for …, https://arxiv.org/html/2512.01358v1</li>
<li>Large Behavior Models | Toyota Research Institute, https://www.tri.global/our-work/large-behavior-models</li>
<li>Boston Dynamics and Toyota team up: Teaching Atlas how to learn, https://newatlas.com/ai-humanoids/boston-toyota-ai-robot-lbm-learning/</li>
<li>Boston Dynamics, Toyota Demo Atlas with LBM Control - Techedge AI, https://techedgeai.com/news/boston-dynamics-and-toyota-demo-atlas-running-on-large-behavior-models/</li>
<li>AI-Powered Robot by Boston Dynamics and Toyota Research …, https://pressroom.toyota.com/ai-powered-robot-by-boston-dynamics-and-toyota-research-institute-takes-a-key-step-towards-general-purpose-humanoids/</li>
<li>π₀ (Pi0) - Hugging Face, https://huggingface.co/docs/lerobot/pi0</li>
<li>Skild AI Builds a General-Purpose Robotic Brain | by Denis Ezhelev, https://medium.com/@denezhelev/skild-ai-builds-a-general-purpose-robotic-brain-2546bfd7e75c</li>
<li>AnyTouch: Learning Unified Static-Dynamic Representation across …, https://www.researchgate.net/publication/389130917_AnyTouch_Learning_Unified_Static-Dynamic_Representation_across_Multiple_Visuo-tactile_Sensors</li>
<li>End-to-End Autonomous Driving Research Report, 2025, http://www.researchinchina.com/Htmls/Report/2025/77076.html</li>
<li>Embodied Graph: A Relational World Model - OpenReview, https://openreview.net/forum?id=JEejM4WkWB</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>