<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2025년 9월 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2025년 9월 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2025년 AI 및 로봇 연구 동향</a> / <span>2025년 9월 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2025년 9월 로봇 연구 동향</h1>
<h2>1.  서론: 지능형 물리 시스템의 임계점 도달</h2>
<p>2025년 9월은 로봇 공학의 역사에서 ’이론적 가능성’이 ’물리적 실재’로 전환되는 결정적인 분기점으로 기록된다. 지난 수년간 인공지능(AI) 분야를 뜨겁게 달구었던 대규모 언어 모델(LLM)과 생성형 AI의 파도가 이제 물리적 신체(Body)를 가진 로봇 제어(Control)의 영역으로 거세게 밀려들고 있다. 이달 발표된 연구들은 단순히 AI를 로봇에 적용하는 단계를 넘어, 로봇의 인지(Perception), 판단(Reasoning), 제어(Actuation)를 하나의 거대하고 유기적인 신경망으로 통합하려는 ’파운데이션 모델(Foundation Model)’의 등장을 알렸다.</p>
<p>특히 이번 달의 연구 성과들은 <strong>확장성(Scalability)</strong>, <strong>범용성(Generalizability)</strong>, 그리고 **실용성(Practicability)**이라는 세 가지 축을 중심으로 전개되었다. 구글 딥마인드(Google DeepMind)와 인트린직(Intrinsic)이 <em>Science Robotics</em>에 발표한 다중 로봇 협업 연구는 수동 프로그래밍에 의존하던 산업 현장의 병목 현상을 해소할 강력한 대안을 제시했으며, 어질리티 로보틱스(Agility Robotics)와 학계에서 잇따라 발표된 휴머노이드 제어 모델들은 로봇이 인간과 유사한 ’운동 지능’을 갖추기 시작했음을 시사한다. 또한, 소프트 로보틱스 분야에서는 미세 유체 역학을 이용한 제조 공정의 혁신과 잔차 물리학(Residual Physics)을 통한 시뮬레이션 격차 해소라는 두 마리 토끼를 잡는 성과가 도출되었다.</p>
<p>본 보고서는 2025년 9월에 발표된 주요 학술 논문, 기술 리포트, 그리고 IROS 2025(International Conference on Intelligent Robots and Systems) 프리뷰를 망라하여 분석한다. 각 기술의 수학적 원리, 아키텍처의 독창성, 그리고 이것이 산업과 사회에 미칠 파급 효과를 심층적으로 기술하며, 파편화된 데이터 포인트들을 연결하여 거대한 기술적 흐름을 조망한다.</p>
<h2>2.  산업 자동화의 재정의: 다중 로봇 협업과 그래프 신경망</h2>
<p>현대 제조업은 유연 생산 시스템을 지향하지만, 실제 현장은 여전히 경직되어 있다. 좁은 셀(Cell) 안에서 여러 대의 로봇 팔이 작업할 때 충돌을 방지하기 위해 엔지니어가 일일이 경로를 지정해야 하는 비효율성 때문이었다. 2025년 9월 <em>Science Robotics</em>의 표지 논문으로 선정된 연구는 이 오래된 난제에 대한 해답을 제시했다.</p>
<h3>2.1  RoboBallet: 다중 로봇 도달을 위한 그래프 신경망과 강화학습</h3>
<p><strong>논문 제목:</strong> RoboBallet: Planning for Multi-Robot Reaching with Graph Neural Networks and Reinforcement Learning 1</p>
<p>이 연구는 구글 딥마인드, 유니버시티 칼리지 런던(UCL), 그리고 알파벳의 자회사 인트린직(Intrinsic)이 공동으로 수행했다. 핵심은 복잡한 작업 공간을 그래프(Graph)로 추상화하고, 강화학습(RL)을 통해 다수의 로봇이 실시간으로 최적의 경로를 생성하도록 하는 것이다.</p>
<h4>2.1.1  기존 방법론의 한계와 새로운 접근</h4>
<p>전통적인 다중 에이전트 경로 계획(Multi-Agent Path Finding, MAPF) 알고리즘이나 RRT*(Rapidly-exploring Random Tree Star)와 같은 샘플링 기반 기법은 로봇의 자유도(DoF)와 대수가 늘어날수록 연산량이 기하급수적으로 증가하는 ’차원의 저주’에 직면한다. 산업 현장에서는 계산 시간을 줄이기 위해 로봇 간의 작업 영역을 물리적으로 분리하거나 순차적으로 움직이게 하여, 전체적인 공간 효율성과 생산성을 저해해 왔다.2</p>
<p>RoboBallet은 이 문제를 **그래프 신경망(Graph Neural Network, GNN)**으로 해결한다. 로봇, 장애물, 목표 지점을 그래프의 노드(Node)로, 이들 간의 공간적 관계를 엣지(Edge)로 모델링함으로써, 로봇의 대수가 변하거나 작업 환경이 바뀌어도 재학습 없이 적용 가능한 ’순열 불변성(Permutation Invariance)’을 확보했다.1</p>
<h4>2.1.2  수학적 모델링과 강화학습 알고리즘</h4>
<p>RoboBallet의 학습 파이프라인은 비동기식 TD3(Twin Delayed Deep Deterministic Policy Gradient) 알고리즘을 변형하여 사용한다. 에이전트는 시뮬레이션 환경에서 수백만 번의 시행착오를 거치며 정책(Policy)을 최적화한다.</p>
<p>핵심적인 보상 함수(Reward Function) <span class="math math-display">R(s, a)</span>는 작업 완료에 대한 성취감과 충돌에 대한 페널티를 정교하게 결합한다.<br />
<span class="math math-display">
R(s, a) = R_{score}(s, a) + R_{collision}(s, a)
</span><br />
여기서 각 구성 요소는 다음과 같이 정의된다:<br />
<span class="math math-display">
S(s) = \frac{N_{done}(s)}{N_{total}(s)}
\\
R_{score}(s, a) = S(s+a) - S(s)
\\
R_{collision}(s, a) = C_{col} N_{robots\_colliding}(s, a)
</span><br />
1</p>
<ul>
<li><span class="math math-display">S(s)</span>는 현재 상태 <span class="math math-display">s</span>에서 전체 작업 중 완료된 작업의 비율을 나타낸다.</li>
<li><span class="math math-display">R_{score}</span>는 행동 <span class="math math-display">a</span>를 취한 후 상태 변화에 따른 작업 진척도 보상이다.</li>
<li><span class="math math-display">R_{collision}</span>은 충돌 발생 시 부여되는 페널티로, <span class="math math-display">C_{col}</span>은 충돌 회피의 중요도를 조절하는 가중치 계수이다.</li>
</ul>
<p>특히, 이 연구는 <strong>힌사이트 경험 리플레이(Hindsight Experience Replay, HER)</strong> 기법을 적극적으로 활용했다.5 다중 로봇 환경에서는 모든 로봇이 충돌 없이 목표에 도달하는 경우가 매우 드물기 때문에(Sparse Reward), 학습 초기에는 유의미한 보상을 얻기 힘들다. HER은 로봇이 의도한 목표에는 도달하지 못했더라도, 실제로 도달한 지점을 마치 처음부터 의도했던 목표인 것처럼 재해석하여 학습 데이터로 활용함으로써 샘플 효율성을 극대화했다.</p>
<h4>2.1.3  시스템 성능 및 산업적 함의</h4>
<p>연구팀은 8대의 로봇 팔이 공유된 작업 공간에서 40개의 도달 작업을 수행하는 고난도 시나리오를 통해 성능을 검증했다. 실험 결과, RoboBallet은 기존의 수동 최적화 방식과 대등한 수준의 경로 품질을 보이면서도, 계획 수립 시간은 획기적으로 단축시켰다.</p>
<p>주목할 점은 연산 효율성이다. 인텔 캐스케이드 레이크(Intel Cascade Lake) CPU 단일 코어에서 10Hz의 제어 주기로 추론이 가능했다.1 이는 고가의 GPU 서버 없이도 기존 산업용 로봇 컨트롤러에 탑재하여 즉시 상용화가 가능함을 의미한다. 전 세계적으로 430만 대 이상의 산업용 로봇이 가동 중인 상황에서, 이 기술은 공정 재배치에 소요되는 막대한 시간과 비용을 절감하고, 소량 다품종 생산을 위한 유연 생산 시스템의 핵심 기술이 될 것으로 전망된다.2</p>
<h2>3.  휴머노이드 로봇의 진화: 파운데이션 모델의 등장</h2>
<p>2025년 9월은 휴머노이드 로봇 제어 기술이 ’규칙 기반(Model-based)’에서 데이터 중심의 ’파운데이션 모델(Foundation Model)’로 완전히 넘어가는 변곡점이다. 텍스트나 이미지를 처리하던 대규모 모델의 패러다임이 로봇의 모터 제어(Motor Control) 영역으로 확장되었다.</p>
<h3>3.1  어질리티 로보틱스의 전신 제어 파운데이션 모델</h3>
<p><strong>관련 자료:</strong> Training a Whole-Body Control Foundation Model 6</p>
<p>이족 보행 로봇 ’디짓(Digit)’을 개발한 어질리티 로보틱스(Agility Robotics)는 자사의 로봇을 위한 ’전신 제어 파운데이션 모델(Whole-Body Control Foundation Model)’을 공개했다. 이 연구의 목표는 로봇에게 인간의 소뇌(Cerebellum)와 유사한 ’운동 피질(Motor Cortex)’을 구축해 주는 것이다.</p>
<h4>3.1.1  개발 배경: MPC의 한계 극복</h4>
<p>기존의 휴머노이드 제어는 주로 모델 예측 제어(Model Predictive Control, MPC)에 의존했다. MPC는 물리 모델을 바탕으로 미래의 상태를 예측하여 제어 입력을 계산하지만, 실시간으로 복잡한 접촉 역학(Contact Dynamics)과 동적 실행 가능성(Feasibility)을 완벽하게 계산하는 데는 한계가 있었다. 특히 미끄러짐이나 예상치 못한 외력에 대한 대응이 늦어지는 단점이 있었다.6</p>
<h4>3.1.2  Sim-to-Real과 대규모 강화학습</h4>
<p>어질리티 로보틱스는 NVIDIA의 Isaac Sim과 Isaac Lab을 활용하여 수십억 단계(Billions of steps)에 달하는 시뮬레이션 데이터를 생성했다.8 이 과정에서 로봇은 다양한 지형, 무작위적인 외력, 센서 노이즈 등 실제 환경에서 겪을 수 있는 극한의 상황을 미리 학습했다.</p>
<p>학습된 모델은 상위 레벨의 지령(예: “좌표 <span class="math math-display">x, y, z</span>로 손을 이동하라”)을 받으면, 전신 균형을 유지하면서 필요한 관절 토크를 즉각적으로 생성한다. 이 모델의 핵심은 **잔차 학습(Residual Learning)**의 개념을 통합하여, 단순한 걷기를 넘어 무거운 물체를 들고 좁은 공간을 이동하거나, 외부 충격에도 넘어지지 않고 자세를 회복하는 강건함(Robustness)을 확보했다는 점이다.</p>
<h3>3.2 학계의 혁신: Mamba 아키텍처와 행동 모델</h3>
<p>학계에서는 트랜스포머(Transformer)의 연산 비효율성을 극복하고, 더욱 긴 시계열 데이터를 처리할 수 있는 새로운 아키텍처인 **Mamba(State Space Model, SSM)**를 로봇 제어에 도입하는 연구가 활발히 진행되었다.</p>
<h4>3.2.1 HuMam: Mamba 기반의 고효율 제어기</h4>
<p><strong>논문 제목:</strong> HuMam: Humanoid Motion Control via End-to-End Deep Reinforcement Learning with Mamba 9</p>
<p>이 연구는 로봇 제어에서 트랜스포머의 대안으로 Mamba 아키텍처의 효용성을 입증했다. 트랜스포머는 입력 시퀀스 길이 <span class="math math-display">L</span>에 대해 <span class="math math-display">O(L^2)</span>의 연산 복잡도를 가지는 반면, Mamba는 <span class="math math-display">O(L)</span>의 선형 복잡도를 가진다. 이는 고주파수(High-frequency) 제어가 필수적인 로봇 시스템에서 결정적인 장점이다.</p>
<p>HuMam의 Mamba 인코더는 연속 시간 시스템을 이산화(Discretization)한 상태 공간 모델(SSM)을 기반으로 한다. 시스템의 상태 전이와 출력은 다음과 같은 수식으로 표현된다:<br />
<span class="math math-display">
h_k = \bar{\mathbf{A}} h_{k-1} + \bar{\mathbf{B}} x_k
</span></p>
<p><span class="math math-display">
y_k = \mathbf{C} h_k
</span></p>
<p>여기서 <span class="math math-display">x_k</span>는 로봇의 센서 입력(관절 각도, 속도, IMU 데이터 등)이며, <span class="math math-display">h_k</span>는 잠재 상태(Latent State)이다.11 HuMam은 단일 레이어 Mamba 인코더만으로도 기존의 MLP나 트랜스포머 기반 제어기보다 더 빠른 학습 수렴 속도와 높은 제어 안정성을 달성했다.</p>
<h4>3.2.2 BFM: 행동 파운데이션 모델과 FB 표현</h4>
<p><strong>논문 제목:</strong> Behavior Foundation Model for Humanoid Robots 13</p>
<p>BFM(Behavior Foundation Model)은 강화학습의 보상 함수 설계 없이도 로봇이 다양한 동작을 생성할 수 있도록 하는 비지도 학습 프레임워크를 제안했다. 핵심은 <strong>Forward-Backward (FB) 표현 학습</strong>이다.</p>
<p>FB 프레임워크는 상태 <span class="math math-display">s</span>와 행동 <span class="math math-display">a</span>를 잠재 벡터 <span class="math math-display">z</span>와 연관시켜, 장기적인 상태 전이 확률을 두 개의 맵(Map)의 내적(Inner Product)으로 근사한다 14:<br />
<span class="math math-display">
M_{\pi_z}(ds&#39;|s,a) \simeq F(s,a,z)^\top B(s&#39;) \rho(ds&#39;)
</span></p>
<ul>
<li><span class="math math-display">F(s, a, z)</span>: 현재 상태와 행동, 잠재 변수 <span class="math math-display">z</span>를 받아 미래를 예측하는 Forward Map.</li>
<li><span class="math math-display">B(s&#39;)</span>: 미래 상태 <span class="math math-display">s&#39;</span>를 잠재 공간으로 임베딩하는 Backward Map.</li>
</ul>
<p>이 모델은 Unitree G1 휴머노이드에 적용되어, 추가적인 학습 없이(Zero-shot) 사용자의 텍스트 명령이나 영상 데모를 추종하는 다양한 동작을 수행했다. 이는 로봇 제어 정책이 작업별로 파편화되던 문제를 해결하고, 하나의 거대 모델로 모든 작업을 처리하는 범용 제어기(Generalist Controller)의 가능성을 열었다.</p>
<h3>3.2  휴머노이드 보안: 새로운 위협의 부상</h3>
<p><strong>논문 제목:</strong> The Cybersecurity of a Humanoid Robot 16; Cybersecurity AI: Humanoid Robots as Attack Vectors 17</p>
<p>로봇의 지능화와 더불어 보안 위협에 대한 연구도 본격화되었다. 연구진은 Unitree G1 등 상용 휴머노이드 로봇이 사이버 공격의 접점이 될 수 있음을 경고했다.18 해킹된 휴머노이드는 단순한 정보 유출을 넘어, 물리적인 힘을 행사하여 시설을 파괴하거나 사람을 위협하는 ’사이버-물리적 공격(Cyber-Physical Attack)’의 도구가 될 수 있다. 이는 향후 로봇 개발에 있어 기능적 성능뿐만 아니라 보안 아키텍처 설계가 필수적임을 시사한다.</p>
<h2>4.  자율 주행 및 모션 계획: 폐루프와 End-to-End의 통합</h2>
<p>자율 주행 분야에서는 인식, 예측, 계획을 별도의 모듈로 처리하던 방식에서 벗어나, 신경망 하나로 전체 과정을 처리하는 End-to-End 방식이 주류로 부상했다. 특히 시뮬레이션 학습의 한계를 극복하기 위한 폐루프(Closed-loop) 데이터 활용 기술이 주목받았다.</p>
<h3>4.1  RoaD: 자가 생성 데이터를 이용한 폐루프 미세 조정</h3>
<p><strong>논문 제목:</strong> RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies 19</p>
<p>모방 학습(Imitation Learning) 기반의 자율 주행 정책은 학습 데이터 분포를 벗어난 상황(Covariate Shift)에서 오류가 누적되어 실패하는 경향이 있다. RoaD는 이를 해결하기 위해 정책 자체가 시뮬레이션에서 주행하며 생성한 경로(Rollout)를 훈련 데이터로 재사용하는 방법을 제안했다.</p>
<ul>
<li><strong>알고리즘의 핵심:</strong> 단순히 자가 생성 데이터를 사용하는 것이 아니라, **‘전문가 가이드(Expert Guidance)’**와 **‘복구 모드(Recovery Mode)’**를 도입했다. 정책 <span class="math math-display">\pi_\theta</span>가 롤아웃을 수행하다가 위험 상황에 처하면 전문가 정책이 개입하여 상황을 수습하고, 이 과정이 포함된 고품질의 데이터를 <span class="math math-display">\mathcal{D}_{gen}</span>에 추가하여 다시 학습한다.20</li>
<li><strong>성과:</strong> 대규모 교통 시뮬레이션 벤치마크인 WOSAC와 고정밀 시뮬레이터 AlpaSim에서 기존 SOTA 대비 충돌률을 54% 감소시키고 주행 점수를 41% 향상시켰다.</li>
</ul>
<h3>4.2 PIE: 상호작용을 고려한 Mamba-MoE 계획기</h3>
<p><strong>논문 제목:</strong> PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving 22</p>
<p>PIE는 복잡한 도로 환경에서 타 차량과의 상호작용을 고려하기 위해 Mamba-2 백본과 <strong>전문가 혼합(Mixture of Experts, MoE)</strong> 모델을 결합했다.</p>
<ul>
<li>
<p><strong>아키텍처:</strong> 긴 시계열의 주행 데이터를 효율적으로 처리하는 Mamba-2 레이어 위에, 다양한 주행 상황(예: 교차로 진입, 차선 변경, 급정거)에 특화된 여러 ‘전문가’ 신경망을 배치했다. 게이팅 네트워크(Gating Network)가 상황에 맞춰 적절한 전문가를 실시간으로 선택한다.</p>
</li>
<li>
<p><strong>손실 함수:</strong> 주행 경로의 정확도뿐만 아니라 속도 예측과 행동 분류를 동시에 최적화한다.</p>
</li>
</ul>
<p>$<span class="math math-display">\mathcal{L} = \mathcal{L}_{DD} + \lambda_v \mathcal{L}_{Vel} + \lambda_a \mathcal{L}_{Act}</span>$</p>
<p>이러한 구조는 주변 차량의 의도를 예측하고 방어 운전을 수행하는 능력을 크게 향상시켰다.23</p>
<h3>4.2  Neural MP: 신경망 기반의 범용 모션 플래너</h3>
<p><strong>논문 제목:</strong> Neural MP: A Generalist Neural Motion Planner 25</p>
<p>IROS 2025 최우수 학생 논문상을 수상한 이 연구는 전통적인 모션 플래너의 속도 문제와 학습 기반 플래너의 일반화 문제를 동시에 해결했다.</p>
<ul>
<li><strong>절차적 생성과 테스트 시간 최적화:</strong> 연구팀은 절차적 생성(Procedural Generation)을 통해 수많은 가상 환경을 만들고 신경망을 학습시켰다. 실제 구동 시에는 신경망이 생성한 초기 경로를 바탕으로, **테스트 시간 최적화(Test-time Optimization)**를 수행하여 충돌을 회피하고 경로를 미세 조정한다.</li>
<li><strong>실증:</strong> 실제 Franka Panda 로봇 팔을 이용해 복잡한 장애물이 있는 좁은 선반이나 서랍 안에서 물체를 꺼내는 작업을 수행했을 때, 기존 방식보다 월등히 높은 성공률을 기록했다.27</li>
</ul>
<h2>5.  소프트 로보틱스: 물리적 한계의 극복과 지능화</h2>
<p>소프트 로보틱스는 유연성이라는 장점에도 불구하고, 비선형적인 물리 특성으로 인해 정밀 제어와 제조가 어려웠다. 이번 달 발표된 연구들은 이러한 물리적 난제를 AI와 새로운 공정 기술로 돌파했다.</p>
<h3>5.1  잔차 물리학을 이용한 Sim-to-Real</h3>
<p><strong>논문 제목:</strong> Sim-to-Real of Soft Robots with Learned Residual Physics 28</p>
<p>IEEE RA-L 베스트 페이퍼로 선정된 이 연구는 소프트 로봇의 시뮬레이션 모델과 실제 물리 현상 간의 오차를 딥러닝으로 보정하는 방법을 제안했다.</p>
<ul>
<li><strong>하이브리드 모델링:</strong> 기존의 분석적 시뮬레이터(Analytical Simulator)를 베이스로 사용하되, 이 시뮬레이터가 놓치는 복잡한 점탄성(Viscoelasticity)이나 마찰 등의 오차(Residual)를 신경망이 학습하도록 했다.
<ul>
<li>전체 동역학 모델: <span class="math math-display">\dot{s} = f_{sim}(s, a) + \mathbf{f}_{residual}(s, a)</span></li>
</ul>
</li>
<li>여기서 <span class="math math-display">\mathbf{f}_{residual}</span>은 신경망이 예측한 잔차항이다.</li>
<li><strong>성과:</strong> 이 방법은 순수 데이터 기반 학습보다 데이터 효율성이 높고, 순수 모델 기반 제어보다 정확도가 높다. 실험 결과 기존 시스템 식별(System Identification) 방법 대비 제어 성능을 최대 60% 향상시켰다.29</li>
</ul>
<h3>5.2  밀리미터 스케일의 유체 구동 소프트 로봇</h3>
<p><strong>논문 제목:</strong> Millimeter-scale fluid-driven soft robots 31</p>
<p><em>National Science Review</em>에 게재된 이 연구는 기존 제조 공정으로는 불가능했던 1mm 이하 직경의 소프트 로봇 제작 기술을 확립했다.</p>
<ul>
<li>
<p><strong>미니 버블 캐스팅(Mini Bubble Casting):</strong> 유체 계면 불안정성(Interfacial Instability) 원리를 역이용하여, 매우 얇고 균일한 실리콘 튜브를 제작한다. 연구진은 공정의 성공을 결정하는 무차원 수 <span class="math math-display">\tau</span>를 유도하고 안정성 조건을 제시했다.<br />
<span class="math math-display">
\tau = \frac{T_{grow}}{T_{cure}} = \beta \frac{\mu R}{\gamma T_{cure}}
</span><br />
(<span class="math math-display">\mu</span>: 점성, <span class="math math-display">R</span>: 반경, <span class="math math-display">\gamma</span>: 표면 장력, <span class="math math-display">T_{cure}</span>: 경화 시간).31</p>
</li>
<li>
<p><strong>응용:</strong> 이 기술로 제작된 마이크로 그리퍼는 개미와 같은 미세한 곤충을 손상 없이 잡거나, 인체 기관지 모형 내에서 정밀하게 조향하는 내시경 팁으로 활용될 수 있음을 증명했다.</p>
</li>
</ul>
<h3>5.3  젤사이트 베이비 핀 레이: 촉각과 유연성의 결합</h3>
<p><strong>논문 제목:</strong> Object Recognition and Force Estimation with the GelSight Baby Fin Ray 33</p>
<ul>
<li><strong>기술 융합:</strong> 물체의 형상에 유연하게 적응하는 ‘핀 레이(Fin Ray)’ 구조의 그리퍼에 고해상도 시각 기반 촉각 센서인 ’젤사이트(GelSight)’를 결합했다.</li>
<li><strong>기능:</strong> 단순히 물체를 잡는 것을 넘어, 잡은 물체의 표면 질감(Texture)을 분석하여 종류를 분류하고, 그리퍼와 물체 사이의 힘 분포를 시각적으로 추정한다. 이는 소프트 그리퍼가 ’손끝의 감각’을 갖게 됨을 의미하며, 비정형 물체 조작의 신뢰성을 크게 높였다.34</li>
</ul>
<h2>6.  IROS 2025 프리뷰 및 산업계 동향</h2>
<h3>6.1  IROS 2025: 항저우에서의 혁신 경연</h3>
<p>2025년 10월 중국 항저우에서 개최될 IROS 2025를 앞두고, 9월에는 관련 논문들이 대거 공개되었다. 이번 컨퍼런스의 주요 테마는 **‘바이오 인스파이어드(Bio-inspired) 로봇’**과 **‘의료/재활 로봇’**이다.36</p>
<table><thead><tr><th><strong>주요 테마</strong></th><th><strong>내용</strong></th><th><strong>관련 세션</strong></th></tr></thead><tbody>
<tr><td><strong>바이오 로보틱스</strong></td><td>생체 모방 메커니즘, 소프트 액추에이터</td><td>Keynote Sessions</td></tr>
<tr><td><strong>로봇 학습</strong></td><td>Sim-to-Real, 파운데이션 모델</td><td>AI and Robot Learning</td></tr>
<tr><td><strong>의료 및 재활</strong></td><td>마이크로 로봇, 웨어러블 슈트</td><td>Rehabilitation Systems</td></tr>
</tbody></table>
<p>특히 중국의 로봇 기업인 Unitree와 DEEP Robotics 등이 최신 4족 보행 로봇과 휴머노이드 기술을 시연할 예정이며, 이는 학계의 연구 성과가 산업계로 얼마나 빠르게 전이되고 있는지를 보여주는 장이 될 것이다.</p>
<h3>6.2  산업계의 명암: 상용화와 구조조정</h3>
<p>산업계에서는 기술적 성취와 경영적 현실이 교차했다.</p>
<ul>
<li><strong>상용화 가속:</strong> 어질리티 로보틱스는 라틴아메리카의 이커머스 거인 메르카도 리브레(Mercado Libre)와 상업적 계약을 체결하고, 자사의 휴머노이드 로봇 Digit을 물류 센터에 투입하기로 했다.7 이는 휴머노이드가 연구실을 벗어나 실제 노동 현장에 투입되는 중요한 사례다.</li>
<li><strong>구조조정:</strong> 반면, 농업용 드론 스타트업 가디언 농업(Guardian Agriculture)과 협동 로봇의 선구자였던 리씽크 로보틱스(Rethink Robotics)가 문을 닫았다.37 이는 로봇 산업이 성숙기로 접어들며 기술력뿐만 아니라 명확한 비즈니스 모델(BM)과 자본력이 생존의 필수 조건이 되었음을 시사한다.</li>
</ul>
<h2>7.  결론 및 전망: 자율 진화하는 로봇 생태계</h2>
<p>2025년 9월의 연구 성과들을 종합해보면, 로봇 공학은 **‘하드웨어와 소프트웨어의 경계 소멸’**과 **‘데이터의 질적 전환’**이라는 거대한 흐름 속에 있다.</p>
<ol>
<li><strong>AI가 정의하는 하드웨어:</strong> RoboBallet이나 어질리티 로보틱스의 파운데이션 모델은 하드웨어의 물리적 한계를 AI가 보완하는 것을 넘어, 하드웨어의 잠재력을 100% 이상 끌어낼 수 있음을 보여주었다. 이제 로봇 설계는 ’기계적 정밀도’보다 ’AI 학습 용이성(Learnability)’을 최우선으로 고려하는 방향으로 바뀔 것이다.</li>
<li><strong>자가 생성 데이터의 시대:</strong> RoaD와 Neural MP 연구는 인간의 데이터에만 의존하던 방식에서 벗어나, 로봇이 시뮬레이션 속에서 스스로 데이터를 생성하고 학습하는 ‘자가 지도 학습(Self-Supervised Learning)’ 시대로의 진입을 알렸다. 이는 로봇 학습의 고질적 문제인 데이터 부족을 해결하는 열쇠가 될 것이다.</li>
<li><strong>범용성과 특화성의 조화:</strong> BFM과 같은 거대 모델이 로봇의 기본적인 운동 능력을 담당하고, PIE의 MoE 구조나 잔차 학습 모듈이 특정 작업에 대한 정밀성을 더하는 ’계층적 지능 구조’가 로봇 제어의 표준이 될 가능성이 높다.</li>
</ol>
<p>결론적으로, 2025년 9월은 로봇이 ’사전에 정의된 동작을 반복하는 기계’에서 **‘환경을 이해하고, 스스로 데이터를 생성하며 학습하고, 인간 및 다른 로봇과 유연하게 협업하는 지능형 에이전트’**로 진화하는 중요한 변곡점이었다. 연구자들과 산업계는 이제 개별 요소 기술의 최적화를 넘어, 이러한 기술들을 통합하여 실제 물리 세계의 복잡성과 불확실성을 해결하는 데 집중해야 할 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>RoboBallet: Planning for Multi-Robot Reaching with Graph Neural …, https://arxiv.org/html/2509.05397v1</li>
<li>Specialized AI for scalable and adaptive multi-robot orchestration, https://www.intrinsic.ai/blog/posts/specialized-ai-for-scalable-and-adaptive-multi-robot-orchestration</li>
<li>RoboBallet: Planning for Multi-Robot Reaching with Graph Neural …, https://arxiv.org/abs/2509.05397</li>
<li>RoboBallet: Planning for Multi-Robot Reaching with Graph Neural …, https://deepmind.google/research/publications/roboballet-planning-for-multi-robot-reaching-with-graph-neural-networks-and-reinforcement-learning/</li>
<li>Meta’s Open 3D Pipeline, World Labs’ Virtual Spaces, Baidu’s …, https://www.deeplearning.ai/the-batch/issue-330/</li>
<li>Training a Whole-Body Control Foundation Model - Agility Robotics, https://www.agilityrobotics.com/content/training-a-whole-body-control-foundation-model</li>
<li>News &amp; Media - Agility Robotics, https://www.agilityrobotics.com/about/press</li>
<li>Humanoid Robot ‘Digit’ – Whole-Body Control Foundation Model, https://www.nvidia.com/en-us/customer-stories/agility-robotics-digit-humanoid-robot/</li>
<li>[2509.18046] HuMam: Humanoid Motion Control via End-to … - arXiv, https://arxiv.org/abs/2509.18046</li>
<li>Blind Bipedal Stair Traversal via Sim-to-Real Reinforcement Learning, https://www.researchgate.net/publication/353193758_Blind_Bipedal_Stair_Traversal_via_Sim-to-Real_Reinforcement_Learning</li>
<li>HuMam: Humanoid Motion Control via End-to-End Deep … - arXiv, https://arxiv.org/html/2509.18046v1</li>
<li>Rohan P. Singh’s research works | Advance Institute of Science and …, https://www.researchgate.net/scientific-contributions/Rohan-P-Singh-2183244264</li>
<li>[2509.13780] Behavior Foundation Model for Humanoid Robots - arXiv, https://arxiv.org/abs/2509.13780</li>
<li>BFM-Zero: A Promptable Behavioral Foundation Model for …, https://lecar-lab.github.io/BFM-Zero/resources/paper.pdf</li>
<li>Behavior Foundation Model for Humanoid Robots - arXiv, https://arxiv.org/html/2509.13780v1</li>
<li>[2509.14096] The Cybersecurity of a Humanoid Robot - arXiv, https://arxiv.org/abs/2509.14096</li>
<li>[2509.14139] Cybersecurity AI: Humanoid Robots as Attack Vectors, https://arxiv.org/abs/2509.14139</li>
<li>September 2025 - Robotics.ee, https://robotics.ee/2025/09/</li>
<li>RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine …, https://www.alphaxiv.org/overview/2512.01993v1</li>
<li>RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine …, https://arxiv.org/html/2512.01993</li>
<li>RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine …, http://paperreading.club/page?id=359524</li>
<li>PIE: Perception and Interaction Enhanced End-to-End Motion … - arXiv, https://arxiv.org/abs/2509.18609</li>
<li>PIE: Perception and Interaction Enhanced End-to-End Motion … - arXiv, https://arxiv.org/html/2509.18609v1</li>
<li>Marcelo H. Ang - DBLP, https://dblp.org/pid/a/MHAng</li>
<li>IROS 2025 Program | Tuesday October 21, 2025, https://ras.papercept.net/conferences/conferences/IROS25/program/IROS25_ContentListWeb_1.html</li>
<li>Neural MP: A Generalist Neural Motion Planner - arXiv, https://arxiv.org/html/2409.05864v1</li>
<li>Neural MP: A Generalist Neural Motion Planner - Murtaza Dalal, https://mihdalal.github.io/neuralmotionplanner/</li>
<li>RAS Recognizes 2025 Award Recipients at IEEE International …, https://ieeexplore.ieee.org/iel8/100/11157737/11157802.pdf</li>
<li>Sim-to-Real of Soft Robots with Learned Residual Physics - arXiv, https://arxiv.org/abs/2402.01086</li>
<li>From a Binary Feature Matrix to Correlation Analysis - IEEE Xplore, https://ieeexplore.ieee.org/iel8/6287639/10820123/11146652.pdf</li>
<li>Millimeter-scale fluid-driven soft robots | National Science Review, https://academic.oup.com/nsr/article/12/11/nwaf413/8267848</li>
<li>ORIGINAL UNEDITED MANUSCRIPT, https://softrobotics.sjtu.edu.cn/pdf/Rong_NSR_2025.pdf</li>
<li>Object Recognition and Force Estimation with the GelSight Baby Fin …, https://arxiv.org/abs/2509.14510</li>
<li>Object Recognition and Force Estimation with the GelSight Baby Fin …, https://arxiv.org/html/2509.14510v1</li>
<li>GelSight Baby Fin Ray: A Compact, Compliant, Flexible Finger with …, https://www.researchgate.net/publication/369556477_GelSight_Baby_Fin_Ray_A_Compact_Compliant_Flexible_Finger_with_High-Resolution_Tactile_Sensing</li>
<li>Program Overview - IROS2025, https://www.iros25.org/ProgramOverview.html</li>
<li>Top 10 robotics developments of September 2025 - The Robot Report, https://www.therobotreport.com/top-10-robotics-developments-of-september-2025/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>