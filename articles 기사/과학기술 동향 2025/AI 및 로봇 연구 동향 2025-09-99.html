<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2025년 9월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2025년 9월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2025년 AI 및 로봇 연구 동향</a> / <span>2025년 9월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2025년 9월 AI 및 로봇 연구 동향</h1>
<h2>1. 서론: 2025년 9월, AI와 로봇 공학의 변곡점</h2>
<p>2025년 9월은 인공지능(AI) 연구의 패러다임이 중요한 전환점을 맞이한 시기로 기록된다. 이 시기는 추상적 지능과 데이터 기반 추론에 집중하던 AI 기술이 물리적 세계와 능동적으로 상호작용하고 구체적인 과업을 수행하는 ’물리적 구현 AI(Embodied AI)’로 본격적으로 확장되는 변곡점이었다. 이 기간에 발표된 주요 연구들은 대규모 파운데이션 모델(Foundation Model)이 보유한 고차원적 추론 능력을 로봇의 정교한 물리적 실행 능력과 결합하려는 시도가 학계와 산업계 전반에서 동시다발적으로 이루어지고 있음을 명확히 보여준다.1</p>
<p>특히 9월에는 로봇 학습 분야의 최고 권위 학회인 CoRL(Conference on Robot Learning)과 휴머노이드 로봇 분야의 대표 학회인 Humanoids가 대한민국 서울에서 연이어 개최되며 전 세계 연구자들의 이목을 집중시켰다.4 이러한 학술 행사의 지리적 집중과 주제적 연계성은 단순한 우연을 넘어, 로봇 학습(CoRL)의 최신 성과가 인간형 로봇(Humanoids)이라는 가장 복잡한 하드웨어 플랫폼에 직접적으로 적용되고 융합되는 현장을 상징한다. 이는 머신러닝 커뮤니티와 로보틱스 커뮤니티 간의 경계가 허물어지고 있음을 시사하는 강력한 신호다. 이 외에도 독일 프랑크푸르트의 ISRAI(International Summit on Robotics, AI &amp; Machine Learning), 미국 샌프란시스코의 The AI Conference 등 다양한 성격의 학술 행사가 개최되어 AI 기술의 학문적 깊이와 산업적 확장을 동시에 조명했다.6</p>
<p>본 보고서는 2025년 9월의 연구 동향을 ▲파운데이션 모델의 진화, ▲로봇 학습 패러다임의 혁신, ▲고등 로봇 제어 기술, ▲평가 및 안전성 확보, ▲AI의 학문적·산업적 확장이라는 다섯 가지 핵심 주제로 나누어 심층 분석한다. 이를 통해 기술 발전의 현주소를 진단하고 미래 기술의 방향성을 전망하고자 한다.</p>
<table><thead><tr><th>컨퍼런스명</th><th>기간</th><th>장소</th><th>핵심 주제 및 특징</th><th>관련 Snippet</th></tr></thead><tbody>
<tr><td><strong>ISRAI 2025</strong> (3rd Int’l Summit on Robotics, AI &amp; ML)</td><td>9월 11-13일</td><td>독일, 프랑크푸르트</td><td>로봇, AI, 머신러닝 기술의 실제적 구현과 혁신적 솔루션 모색</td><td>7</td></tr>
<tr><td><strong>The AI Conference</strong></td><td>9월 17-18일</td><td>미국, 샌프란시스코</td><td>AI의 산업별 적용(헬스케어, 리테일), 차세대 AI(에이전트, 웨어러블) 동향</td><td>6</td></tr>
<tr><td><strong>CORA 2025</strong> (Conference on Robots and Automation)</td><td>9월 23-24일</td><td>오스트리아, 비엔나</td><td>로봇과 자동화의 경제적 영향 분석 (IFR 협력)</td><td>9</td></tr>
<tr><td><strong>CoRL 2025</strong> (Conference on Robot Learning)</td><td>9월 27-30일</td><td>대한민국, 서울</td><td>로봇과 머신러닝의 교차점, Sim-to-Real, 모방/강화 학습, 범용 정책</td><td>1</td></tr>
<tr><td><strong>Humanoids 2025</strong> (IEEE-RAS Int’l Conference on Humanoid Robots)</td><td>9월 30일 - 10월 2일</td><td>대한민국, 서울</td><td>휴머노이드 로봇의 전신 제어, 다중 접촉 조작, 동적 동작 계획</td><td>4</td></tr>
</tbody></table>
<h2>2.  차세대 로봇 지능을 위한 파운데이션 모델의 진화</h2>
<h3>2.1  Google Gemini Robotics: 계획과 실행의 이원화 아키텍처</h3>
<p>기존의 단일 종단형(end-to-end) 로봇 제어 모델은 복잡하고 장기적인 과제를 수행할 때 중대한 한계를 보였다. 계획 수립 단계에서의 작은 오류가 보정 과정 없이 즉각적인 실행 실패로 이어져 시스템의 강건성을 저해하는 문제가 있었다.3 2025년 9월 25일, Google DeepMind는 이러한 문제를 해결하기 위해 ’계획(Planning)’과 ’실행(Execution)’의 역할을 명확히 분리하는 이원화된 아키텍처를 발표하며 로봇 지능의 새로운 방향을 제시했다.10</p>
<p>이 아키텍처는 다음과 같은 두 가지 핵심 모델로 구성된다.</p>
<ul>
<li><strong>Gemini Robotics-ER 1.5 (Embodied Reasoning):</strong> 이 모델은 시스템의 ’뇌’에 해당하는 ‘Planner’ 또는 ‘Orchestrator’ 역할을 수행한다. Vision-Language Model(VLM)을 기반으로 하여 복잡한 자연어 명령을 이해하고, 이를 바탕으로 다단계 행동 계획을 생성한다. 특히, 이 모델은 외부 도구(예: Google 검색)를 능동적으로 호출하여 “지역 재활용 규정을 확인하여 쓰레기를 분리하라“와 같은 정보 기반 과제를 해결할 수 있다.3 더 나아가, 로봇의 물리적 제약(예: 탑재 하중, 작업 반경)을 이해하고 안전하지 않은 계획 생성을 거부하는 ‘의미론적 안전성(Semantic Safety)’ 기능을 갖추어 시스템의 신뢰도를 높였다.11</li>
<li><strong>Gemini Robotics 1.5:</strong> 이 모델은 시스템의 ’몸’에 해당하는 ‘Executor’ 역할을 담당한다. Vision-Language-Action(VLA) 모델을 기반으로 하며, Planner가 생성한 고수준의 계획과 실시간 시각 데이터를 입력받아 실제 로봇을 움직이는 저수준 모터 명령어로 변환한다.3</li>
</ul>
<p>이러한 ‘Planner-Executor’ 구조는 추상적 추론과 물리적 실행을 분리함으로써 각 모듈의 전문성과 독립성을 극대화한다. 이는 소프트웨어 공학의 ‘모델-뷰-컨트롤러(MVC)’ 패턴처럼, 향후 복잡한 작업을 수행하는 범용 로봇의 표준 아키텍처, 즉 일종의 디자인 패턴으로 자리 잡을 가능성이 크다. 각 모듈을 독립적으로 개발하고 최적화할 수 있어, 더 뛰어난 추론 모델이 나오면 Planner만 교체하거나 더 효율적인 제어 알고리즘이 나오면 Executor를 개선하는 등 시스템의 유지보수와 확장이 용이해진다. 또한, 개발자는 ‘유연한 사고 예산(Flexible thinking budget)’ 기능을 통해 특정 과제의 복잡도에 따라 대기 시간(latency)과 정확도(accuracy) 간의 균형을 능동적으로 조절할 수 있다.12</p>
<h3>2.2  개인정보보호와 모델 경량화: VaultGemma와 최신 모델 동향</h3>
<p>물리적 구현 AI가 실험실을 넘어 가정, 병원 등 사적인 공간으로 확산되기 위해서는 기술적 성능만큼이나 데이터 프라이버시와 안전성이 중요하다. 9월 12일, Google Research는 이러한 시대적 요구에 부응하는 중요한 연구 결과인 <strong>VaultGemma</strong>를 발표했다.10 VaultGemma는 차등 개인정보보호(Differential Privacy) 기술을 적용하여 처음부터 학습시킨 대규모 언어 모델로, AI 모델이 민감한 데이터를 학습할 때 개인정보 유출 위험을 수학적으로 제어할 수 있다. 로봇이 보고 듣는 모든 것이 데이터가 되는 미래 환경에서, 이러한 기술은 사용자의 신뢰를 확보하고 관련 규제를 준수하기 위한 필수적인 요소가 될 것이다. 이는 미래의 ‘물리적 구현 AI’ 시장에서 기술적 우위뿐만 아니라 ’신뢰’와 ’안전’이라는 가치를 선점하려는 전략적 포석으로 해석할 수 있다.</p>
<p>이와 함께, 모델의 효율성을 높이려는 노력도 계속되었다. 9월 25일 발표된 <strong>Gemini 2.5 Flash 및 Flash-Lite</strong> 업데이트는 복잡한 추론이 필요 없는 간단한 작업을 저비용, 저지연으로 처리하는 데 중점을 둔다.14 이러한 경량 모델들은 엣지 디바이스나 실시간 로봇 제어에 직접 탑재되어, 클라우드 연결 없이도 빠른 반응 속도를 보장할 수 있는 가능성을 열어준다.</p>
<p>한편, 학계에서는 파운데이션 모델의 근본적인 능력을 고도화하려는 연구가 활발히 진행되었다. 9월 한 달간 arXiv에 제출된 논문들은 대규모 언어 모델(LLM)을 활용하여 인간의 행동 패턴을 예측하고 이를 코드 형태의 프로그램으로 합성하는 ROTE 알고리즘, 외부 지식 베이스를 연동하여 모델의 전문성을 강화하는 기법, 그리고 다중 에이전트 시스템에서 에이전트들이 자율적으로 합의를 도출하는 과정에 대한 분석 등, 모델의 추론 및 상호작용 능력을 한 단계 끌어올리는 다양한 접근법을 제시했다.15</p>
<h2>3.  시뮬레이션에서 현실로: 로봇 학습 패러다임의 혁신</h2>
<h3>3.1  사례 연구: ClutterDexGrasp - 복잡한 환경에서의 정교한 파지</h3>
<p>로봇 손가락을 이용한 정교한 파지(Dexterous Grasping)는 목표물이 다른 물체들에 의해 가려지거나 복잡하게 얽혀 있는 ‘잡동사니(Clutter)’ 환경에서 극도로 어려운 과제로 알려져 있다. 기존 방식은 실제 로봇을 이용한 방대한 데이터 수집에 의존하여 비용과 시간 측면에서 확장성이 낮았다.16 CoRL 2025에서 구두 발표된 ’ClutterDexGrasp’는 이러한 한계를 극복하기 위해 시뮬레이션에서만 학습하여 실제 세계에 제로샷(zero-shot)으로 적용 가능한 혁신적인 시스템을 제안했다.1</p>
<p>이 시스템의 핵심 방법론은 다음과 같다.</p>
<ul>
<li>
<p><strong>교사-학생(Teacher-Student) 프레임워크:</strong> 먼저, 시뮬레이션 내에서만 접근 가능한 ‘특권 정보’(예: 물체의 정확한 위치, 질량, 마찰 계수)를 활용하는 고성능의 ’교사 정책(Teacher Policy)’을 강화학습으로 훈련시킨다. 이후, 이 교사 정책이 생성한 수많은 전문가 시연 데이터를 모방하여, 실제 로봇이 사용하는 센서(3D 포인트 클라우드) 정보만을 사용하는 ’학생 정책(Student Policy)’을 학습시킨다. 이 지식 증류(distillation) 과정은 생성 모델의 일종인 ’확산 모델(Diffusion Policy)’을 통해 효율적으로 이루어진다.16</p>
</li>
<li>
<p><strong>커리큘럼 학습(Curriculum Learning):</strong> 학습 과정을 세 단계로 나누어 점진적으로 난이도를 높임으로써 학습의 안정성과 효율성을 확보했다.</p>
</li>
</ul>
<ol>
<li>
<p><strong>1단계 (기초 파지):</strong> 단일 물체를 잡는 기본적인 기술을 학습한다.</p>
</li>
<li>
<p><strong>2단계 (전략적 파지):</strong> 점차 복잡해지는 잡동사니 환경에서 목표물을 잡기 위해 다른 물체를 밀어내는 등 전략적 행동을 학습한다.</p>
</li>
<li>
<p><strong>3단계 (안전성 학습):</strong> 실제 로봇의 파손을 방지하기 위해, 과도한 힘을 가하면 큰 페널티를 부여하는 ’안전성 커리큘럼’을 도입하여 부드러운 상호작용을 유도한다.16</p>
</li>
</ol>
<p>이 연구는 시뮬레이션에서 90% 이상의 높은 성공률을 달성했으며, 실제 로봇을 이용한 실험에서도 처음 보는 물체와 배치에 대해 167번의 시도 중 83.9%라는 놀라운 성공률을 보였다. 특히 주목할 점은, 로봇이 목표물을 잡기 위해 방해가 되는 다른 물체를 부드럽게 옆으로 밀어내는 등 인간과 유사한 지능적인 전략을 명시적인 프로그래밍 없이 스스로 학습했다는 것이다.16 이는 안전한 상호작용 방식 자체를 강화학습의 보상 함수 설계를 통해 정책이 스스로 ’체득’하도록 만든 것으로, 예측 불가능한 환경에서 고정된 규칙 기반 안전장치보다 훨씬 더 유연하고 강건한 안전성을 확보하는 새로운 방향을 제시한다.</p>
<h3>3.2  사례 연구: Visual Imitation Enables Contextual Humanoid Control</h3>
<p>휴머노이드 로봇에게 계단 오르기나 의자에 앉기와 같이 주변 환경과의 정교한 상호작용이 필수적인 복잡한 동작을 가르치는 것은 전통적으로 고가의 모션 캡처 장비를 이용한 데이터 수집에 크게 의존해왔다.21 CoRL 2025에서 구두 발표된 ’VIDEOMIMIC’은 이러한 데이터 수집의 병목 현상을 해결하기 위해, 특수 장비 없이 스마트폰 등으로 촬영된 일상적인 비디오만으로 휴머노이드 로봇을 학습시키는 ‘Real-to-Sim-to-Real’ 파이프라인을 제안했다.1</p>
<p>VIDEOMIMIC의 핵심 기술은 다음과 같다.</p>
<ul>
<li><strong>4D 재구성(4D Reconstruction):</strong> 단일 시점(monocular) 비디오에서 3D 인간 포즈와 움직임, 그리고 주변 환경의 3D 구조(포인트 클라우드)를 동시에 재구성하여 4차원 시공간 데이터를 생성한다.22</li>
<li><strong>동작 리타겟팅(Motion Retargeting):</strong> 재구성된 인간의 동작을 휴머노이드 로봇의 기구학적, 동역학적 제약(관절 한계, 충돌 등)에 맞게끔 수학적으로 변환한다.22</li>
<li><strong>정책 학습(Policy Learning):</strong> 시뮬레이션 환경에서 강화학습을 통해, 리타겟팅된 참조 동작을 안정적으로 따라하는 제어 정책을 학습시킨다. 이 정책은 로봇의 자체 상태 정보(proprioception, 예: 관절 각도)와 로봇 주변의 지형 정보(높이 맵)를 입력으로 받아, 주어진 상황에 맞는 최적의 동작을 실시간으로 생성한다.24</li>
</ul>
<p>이 연구는 단일 제어 정책만으로 계단 오르내리기, 의자에 앉고 서기 등 다양한 상황 인식형 동작을 실제 휴머노이드 로봇(Unitree G1)에서 안정적으로 수행하는 데 성공했다.21 VIDEOMIMIC은 전통적인 Sim-to-Real 패러다임을 넘어, 무한히 존재하는 현실 세계의 비디오 데이터(예: YouTube, TikTok)를 로봇이 학습 가능한 정제된 가상 데이터로 ’변환’하는 새로운 접근법을 제시한다. 이는 ’현실 데이터의 가상화’라는 새로운 데이터 파이프라인을 구축한 것으로, 로봇 학습의 규모를 폭발적으로 증가시킬 수 있는 혁신적인 잠재력을 가진다.</p>
<h2>4.  고등 로봇 제어 및 조작 기술의 최전선</h2>
<h3>4.1  사례 연구: Scaling Whole-body Multi-contact Manipulation</h3>
<p>인간은 물건을 다룰 때 손뿐만 아니라 팔뚝, 몸통, 다리 등 온몸을 지지대나 조작 도구로 활용한다. 그러나 로봇에게 이러한 ’전신 다중 접촉 조작(Whole-body Multi-contact Manipulation)’을 자율적으로 계획하게 하는 것은 접촉 가능한 지점의 수가 사실상 무한대에 가까워 기존의 샘플링 기반 계획 방법으로는 계산적으로 불가능에 가까웠다.2 Humanoids 2025에서 발표된 이 연구는 경사도 기반 최적화(gradient-based optimization)를 통해 이 문제를 정면으로 돌파하는 새로운 프레임워크를 제안했다.25</p>
<p>이 프레임워크의 핵심 기술은 다음과 같다.</p>
<ul>
<li><strong>효율적인 표면 표현:</strong> 로봇과 주변 물체의 표면을 기하학적으로 효율적인 형태로 표현하여, 두 표면 사이의 가장 가까운 점(proximity points)을 해석적(closed-form)으로 매우 빠르게 계산할 수 있도록 했다. 이는 최적화 과정에서 반복적으로 수행되는 계산의 병목 현상을 해결하는 핵심적인 돌파구였다.</li>
<li><strong>지능적인 비용 함수 설계(Cost Function Design):</strong> 로봇이 과업 목표를 달성하는 데 가장 유리한 접촉 지점과 자세를 자율적으로 찾도록 유도하는 새로운 비용 함수(cost function)를 설계했다. 이 덕분에 개발자가 사전에 접촉점의 개수나 위치를 지정할 필요 없이, 로봇이 스스로 최적의 접촉 전략을 발견할 수 있게 되었다.25</li>
</ul>
<p>이 연구는 기존의 최첨단 샘플링 기반 방식과 비교하여 동작 계획 시간을 평균 77% 단축했으며, 이전에는 해결할 수 없었던 복잡한 전신 조작 문제를 해결하는 데 성공했다. 또한, 실제 휴머노이드 로봇을 이용해 상자를 온몸으로 밀고 받치며 조작하는 실험을 통해 제안된 방법의 실효성을 검증했다.2 이 연구는 로봇 조작의 개념을 ’손으로 잡는 행위(grasping)’에서 ’몸 전체를 이용한 물리적 상호작용(contact)’으로 확장했다는 점에서 중요한 의의를 가진다. 이는 로봇이 수행할 수 있는 작업의 범위를 극적으로 넓히고, 인간과 유사한 수준의 물리적 유연성을 갖추기 위한 근본적인 패러다임 전환을 의미한다.</p>
<h3>4.2  민첩한 휴머노이드 동작 계획과 제어 이론의 발전</h3>
<p>Humanoids 2025에서는 전신 조작뿐만 아니라, 휴머노이드의 동적인 움직임을 위한 제어 기술 연구도 활발히 발표되었다.</p>
<ul>
<li><strong>AHMP (Agile Humanoid Motion Planning):</strong> 달리기, 점프 등 민첩하고 역동적인 동작을 위한 다중 접촉 동작 계획 프레임워크가 제안되었다. 이 방법은 이중 최적화(bi-level optimization) 구조를 채택하여, ’어떤 순서로 접촉할 것인가(contact sequence discovery)’라는 고수준의 조합적 문제와 ’어떤 궤적으로 움직일 것인가(trajectory optimization)’라는 저수준의 연속적 문제를 통합적으로 효율적이게 해결한다.27</li>
<li><strong>강화학습 기반 접촉 계획:</strong> 또 다른 연구에서는 강화학습을 이용해 장기적으로 가장 유리한 접촉 지점을 선택하는 고수준 정책을 학습하고, 이를 전통적인 최적화 기반 동작 생성기와 결합하는 하이브리드 프레임워크를 제안했다. 이는 학습의 유연성과 최적화의 정밀성을 모두 확보하려는 시도다.27</li>
</ul>
<p>이러한 연구들은 순수 학습 기반 접근법이나 순수 최적화 기반 접근법의 한계를 인식하고, 두 가지를 결합하는 하이브리드 방식이 대세가 되고 있음을 명확히 보여준다. 즉, 강화학습이 ‘무엇을(what)’ 해야 할지(예: 좋은 접촉점)에 대한 전략적 직관을 제공하면, 최적화 기법이 ‘어떻게(how)’ 해야 할지(물리 법칙을 만족하는 실제 궤적)를 정교하게 계산하는 상호 보완적인 구조가 복잡하고 동적인 로봇 제어 문제에 대한 가장 현실적이고 강력한 해결책으로 부상하고 있다.</p>
<p>한편, 9월 arXiv에는 제어 장벽 함수(Control Barrier Functions)를 이용한 데이터 기반 안전 인증, 다중 로봇 시스템의 충돌 회피 동작 계획 등 로봇 제어의 안전성과 강건성을 수학적으로 보장하려는 이론적 연구들도 다수 발표되어, 이 분야의 깊이를 더했다.28</p>
<h2>5.  범용 로봇 평가 및 안전성 확보를 위한 새로운 접근법</h2>
<h3>5.1  RoboArena: 분산형 실세계 로봇 정책 평가 벤치마크</h3>
<p>다양한 작업을 수행할 수 있는 ’범용 로봇 정책(Generalist Robot Policy)’의 등장은 기존의 표준화된 평가 방식에 큰 도전을 제기했다. 특정 몇 가지 작업만으로 로봇의 범용성을 평가하는 것은 불가능하며, 전 세계 모든 연구실이 동일한 평가 환경을 물리적으로 구축하는 것은 비현실적이기 때문이다.29 CoRL 2025에서 구두 발표된 ’RoboArena’는 이러한 문제를 해결하기 위해 LLM 평가에 성공적으로 사용된 ’Chatbot Arena’에서 영감을 받은 새로운 분산형 평가 프레임워크를 제안했다.1</p>
<p>RoboArena의 핵심 철학은 다음과 같다.</p>
<ul>
<li><strong>분산형 및 크라우드소싱:</strong> 평가를 중앙에서 통제하는 대신, 전 세계 여러 대학(UC 버클리, 스탠포드, 펜실베니아대 등 7개 기관)의 연구자들이 각자 보유한 DROID 로봇 플랫폼을 이용해 자율적으로 평가를 수행하고 결과를 공유한다.29</li>
<li><strong>쌍대 비교(Pairwise Comparison) 및 이중맹검(Double-blind):</strong> 평가자는 표준화된 과제를 수행하는 대신, 자신이 원하는 어떤 과제든 자유롭게 설정할 수 있다. 단, 평가는 항상 두 개의 정책(A, B)을 동일한 과제에 대해 실행해보고 어느 쪽이 더 나은지를 선택하는 ‘A/B 테스트’ 방식으로 진행된다. 이때 평가자는 어떤 정책이 A이고 B인지 알 수 없는 ‘이중맹검’ 원칙을 따른다.31</li>
<li><strong>통계 모델 기반 글로벌 랭킹:</strong> 이렇게 수집된 수백 건(초기 600회 이상)의 쌍대 비교 선호도 데이터를 Bradley-Terry와 같은 통계 모델을 이용해 종합함으로써, 각 정책의 전역적인 성능 순위(Global Ranking)를 객관적으로 도출한다.30</li>
</ul>
<p>RoboArena는 고정된 기준으로 점수를 매기는 ’절대 평가’에서 벗어나, 다양한 상황에서 다른 정책들과의 ’상대적 우위’를 집계하는 방식으로 범용성을 평가하는 패러다임의 전환을 보여준다. 이는 기존의 중앙 집중식 평가보다 더 정확하고, 확장 가능하며, 신뢰할 수 있는 방식으로 범용 로봇의 성능을 평가할 수 있음을 입증했으며, 향후 범용 로봇 연구 발전을 위한 공정하고 객관적인 기준을 제공할 것으로 기대된다.29</p>
<h3>5.2  AI 시스템의 신뢰성과 안전: 편향성, 그리고 물리적 제약</h3>
<p>AI 기술이 물리적 세계로 확장됨에 따라, 알고리즘의 신뢰성과 안전성 문제는 더욱 중요해지고 있다. 9월에는 AI의 ’사회적 안전성’과 ’물리적 안전성’이라는 두 가지 중요한 축에 대한 연구가 주목받았다.</p>
<ul>
<li><strong>의료 AI의 편향성 (사회적 안전성):</strong> 9월 19일, MIT CSAIL 연구진은 널리 사용되는 의료 AI 모델들이 심각한 사회적 편향성을 내포하고 있음을 실증적으로 밝혔다. 연구에 따르면, 이 모델들은 여성 및 소수 인종 환자에게 남성이나 백인 환자에 비해 더 낮은 수준의 치료를 권장하는 경향을 보였으며, 정신 건강 문제에 대한 상담에서는 흑인 및 아시아인에게 덜 공감하는 답변을 생성했다.32 이는 AI 시스템이 학습 데이터에 내재된 사회적 편견을 그대로 학습하고 증폭시킬 수 있음을 보여주며, 실제 세계에 AI를 적용하기 전에 데이터 편향 문제를 해결하는 것이 얼마나 중요한지를 경고한다.</li>
<li><strong>Gemini Robotics의 안전성 강화 (물리적 안전성):</strong> Google은 Gemini Robotics-ER 1.5 모델에 로봇의 물리적 한계(예: 작업 공간, 탑재 하중)를 위반하는 위험한 계획을 생성하지 않도록 하는 기능을 내장했다.12 또한, 유해하거나 위험한 작업을 거부하도록 모델을 학습시키는 데 사용되는 ’ASIMOV 벤치마크’를 지속적으로 업그레이드하여 모델의 의미론적 안전성을 체계적으로 평가하고 개선하고 있다.11</li>
</ul>
<p>이 두 가지 연구는 물리적 구현 AI 시대의 안전성이 동전의 양면과 같음을 시사한다. AI가 사회적 편견을 증폭시키지 않아야 하는 것(사회적 안전성)과, 물리 법칙을 위반하거나 위험한 행동을 하지 않아야 하는 것(물리적 안전성)은 함께 고려되어야 한다. 미래의 로봇 개발은 알고리즘의 성능 향상뿐만 아니라, 이 두 가지 차원의 안전성을 동시에 검증하고 보장하는 ’신뢰 공학(Trustworthy Engineering)’이 핵심적인 분야가 될 것이다.</p>
<h2>6.  AI의 확장: 과학적 발견과 산업 적용의 가속화</h2>
<h3>6.1  과학적 난제 해결을 위한 AI의 활용: DeepMind의 유체 역학 연구</h3>
<p>지금까지 AI는 주로 인간이 정의한 문제를 효율적으로 ‘푸는(Solver)’ 도구로 인식되어 왔다. 그러나 2025년 9월, AI가 인간 지성의 한계를 넘어 미지의 영역을 탐험하는 ’발견자(Discoverer)’가 될 수 있음을 보여주는 획기적인 연구 결과가 발표되었다.</p>
<ul>
<li><strong>배경:</strong> 허리케인의 소용돌이부터 비행기 날개의 양력까지 모든 유체 현상을 설명하는 나비에-스토크스 방정식(Navier-Stokes equations)은 특정 조건에서 해(속도, 압력 등)가 무한대로 발산하는 ‘특이점(singularity)’ 문제가 존재한다. 이는 지난 한 세기 동안 수학계의 가장 중요한 난제 중 하나였다.33</li>
<li><strong>DeepMind의 접근법:</strong> 9월 18일, DeepMind는 물리 법칙 자체를 손실 함수로 사용하여 신경망을 학습시키는 ’물리 정보 신경망(Physics-Informed Neural Networks, PINNs)’을 활용하는 새로운 접근법을 발표했다. 이 방법론을 통해, 기존에 알려지지 않았던 새로운 종류의 불안정한 특이점 해(unstable singularities)들을 체계적으로 발견하는 데 성공했다.33</li>
<li><strong>의의:</strong> 이 연구에서 AI는 주어진 데이터의 패턴을 학습하는 것을 넘어, 인간이 미처 발견하지 못한 수학적 ’해의 새로운 군(family of solutions)’을 발견했다. 이는 AI가 인간에게 새로운 과학적 ’직관’이나 ’단서’를 제공하고, 이를 바탕으로 인간 연구자들이 더 엄밀한 증명을 완성하는 새로운 연구 패러다임을 열었음을 의미한다. 이 성과는 ’컴퓨터 보조 증명(computer-assisted proofs)’의 새로운 시대를 여는 중요한 이정표로 평가된다.33</li>
</ul>
<h3>6.2  산업 현장의 AI: The AI Conference 및 MIT 심포지엄 동향</h3>
<p>9월에는 AI 기술의 산업적 적용을 논의하는 주요 행사들이 개최되어, 기술이 실험실을 넘어 실제 가치를 창출하는 현황을 조명했다.</p>
<ul>
<li><strong>The AI Conference (샌프란시스코, 9월 17-18일):</strong> 이 컨퍼런스는 AI 프론티어(기초 모델, 에이전트), 헬스케어 및 생명공학, 리테일, 웨어러블 및 엣지 디바이스 등 4개의 주요 트랙을 통해 AI의 산업별 적용 사례를 집중적으로 다루었다.6 주요 논의 주제는 생성형 AI를 활용한 신약 개발 가속화, 개인화된 환자 관리, 소매업의 운영 최적화 및 고객 경험 향상, 차세대 지식 검색 시스템, 그리고 국방 및 정보 분야에서의 AI 활용 등이었다.6</li>
<li><strong>MIT Generative AI Impact Symposium (케임브리지, 9월 17일):</strong> MIT가 주최한 이 심포지엄에서는 학계와 산업계의 리더들이 한자리에 모여 생성형 AI의 미래를 논의했다. 특히 Amazon Robotics의 최고 기술 책임자인 타이 브래디(Tye Brady)는 기조연설에서 생성형 AI가 로봇 공학의 혁신에 미치는 지대한 영향에 대해 발표하며 산업계의 높은 기대를 드러냈다.34 또한 ‘엣지에서의 AI(AI at the Edge)’ 세션은 AI 연산이 대규모 데이터 센터에서 웨어러블, 로봇 등 사용자와 가까운 ’엣지’로 이동하는 중요한 기술적 변화를 조명했다.34</li>
</ul>
<p>이러한 논의들은 산업 AI의 무게중심이 두 가지 방향으로 이동하고 있음을 암시한다. 첫째, AI 연산이 ’클라우드’에서 ’엣지’로 이동하며 실시간 반응성과 개인정보보호의 중요성이 커지고 있다. 둘째, AI의 역할이 인간의 일자리를 ’대체’하는 완전 자동화를 넘어, 인간 전문가의 능력을 ’증강’시키는 보조 도구로서의 역할이 더욱 강조되고 있다.</p>
<h2>7. 결론: 2025년 9월 동향 종합 및 미래 전망</h2>
<p>2025년 9월은 ’물리적 구현 AI’의 원년으로 기록될 만한 중요한 진전이 동시다발적으로 이루어진 시기였다. Google의 Gemini Robotics는 범용 로봇의 ’뇌’가 될 파운데이션 모델의 청사진을 제시했고, CoRL과 Humanoids에서는 시뮬레이션과 현실의 경계를 허무는 혁신적인 학습 및 제어 기술들이 대거 발표되었다. 동시에 RoboArena와 같은 새로운 평가 프레임워크와 AI 안전성 연구는 이 강력한 기술들을 책임감 있게 발전시키기 위한 커뮤니티의 성숙한 노력을 보여주었다.</p>
<table><thead><tr><th>연구/기술명</th><th>발표처</th><th>문제 정의 (Problem Statement)</th><th>핵심 방법론 (Key Methodology)</th><th>주요 기여 및 결과 (Contribution &amp; Results)</th></tr></thead><tbody>
<tr><td><strong>Gemini Robotics</strong></td><td>Google DeepMind</td><td>단일 모델의 장기 과제 수행 한계 및 계획-실행 간의 오류 전파</td><td><strong>Planner-Executor 이원화 아키텍처</strong> - Planner (ER 1.5): VLM 기반 고수준 추론, 도구 사용 - Executor (1.5): VLA 기반 저수준 모터 제어</td><td>복잡한 다단계 과제의 강건하고 유연한 수행, 의미론적 안전성 확보</td></tr>
<tr><td><strong>ClutterDexGrasp</strong></td><td>CoRL 2025</td><td>잡동사니 환경에서의 정교한 파지를 위한 Sim-to-Real 데이터 부족 문제</td><td><strong>교사-학생 프레임워크 + 확산 모델</strong> - 시뮬레이션 특권 정보를 활용한 교사 정책 학습 - 안전성 및 난이도 커리큘럼 학습</td><td>제로샷 Sim-to-Real 정교한 파지 성공 (현실 성공률 83.9%), 인간과 유사한 상호작용 전략 학습</td></tr>
<tr><td><strong>Visual Imitation Enables Contextual Humanoid Control (VIDEOMIMIC)</strong></td><td>CoRL 2025</td><td>상황 인식형 휴머노이드 동작 생성을 위한 대규모 모션 캡처 데이터 의존성</td><td><strong>Real-to-Sim-to-Real 파이프라인</strong> - 단일 비디오 기반 4D 인간-장면 동시 재구성 - 동작 리타겟팅 및 강화학습 기반 정책 학습</td><td>일상 비디오만으로 계단 오르기 등 복잡한 상황 인식형 동작을 실제 휴머노이드에서 구현</td></tr>
<tr><td><strong>Scaling Whole-body Multi-contact Manipulation</strong></td><td>Humanoids 2025</td><td>전신을 활용한 다중 접촉 조작 계획의 계산적 복잡성 및 확장성 한계</td><td><strong>경사도 기반 최적화</strong> - 효율적인 로봇 표면 표현 - 자율적 접촉 탐색을 위한 비용 함수 설계</td><td>기존 대비 계획 시간 77% 단축, 샘플링 기반 방식으로 해결 불가능한 문제 해결</td></tr>
<tr><td><strong>RoboArena</strong></td><td>CoRL 2025</td><td>범용 로봇 정책의 성능을 포괄적이고 공정하게 평가할 표준화된 벤치마크 부재</td><td><strong>분산형 크라우드소싱 + 쌍대 비교</strong> - 다수 기관의 분산된 환경에서 이중맹검 A/B 테스트 - 통계 모델 기반 글로벌 랭킹 도출</td><td>확장 가능하고 신뢰도 높은 범용 로봇 평가 프레임워크 제시, 커뮤니티 기반 벤치마크 구축</td></tr>
<tr><td><strong>PINNs for Fluid Dynamics</strong></td><td>DeepMind</td><td>나비에-스토크스 방정식의 특이점(singularity) 문제 등 수학적 난제</td><td><strong>물리 정보 신경망 (PINNs)</strong> - 물리 법칙을 손실 함수로 활용하여 해의 공간 탐색</td><td>기존에 알려지지 않은 새로운 형태의 특이점 해 발견, AI를 활용한 과학적 발견의 새로운 가능성 제시</td></tr>
</tbody></table>
<p>이러한 동향을 바탕으로 다음과 같은 미래를 전망할 수 있다.</p>
<ul>
<li><strong>파운데이션 모델의 물리 세계 접목 가속화:</strong> Gemini Robotics와 같은 모델을 기반으로, 다양한 로봇 하드웨어에 적용 가능한 범용 ‘로봇 운영체제(Robot OS)’ 또는 ‘로봇 브레인’ 개발 경쟁이 심화될 것이다.</li>
<li><strong>데이터 패러다임의 지속적 혁신:</strong> VIDEOMIMIC과 같이 비정형 현실 데이터를 활용하는 기술이 발전함에 따라, 로봇 학습은 더 이상 데이터 수집에 얽매이지 않고 세상에 존재하는 모든 시각 정보를 잠재적인 학습 소스로 활용하게 될 것이다.</li>
<li><strong>인간-로봇 상호작용의 고도화:</strong> 전신 제어, 민첩한 동작 계획 기술의 발전은 로봇이 단순히 도구를 넘어, 인간과 같은 공간에서 물리적으로 협력하는 ’파트너’가 될 수 있는 가능성을 열어준다.</li>
<li><strong>신뢰와 안전의 중요성 증대:</strong> 기술이 성숙할수록, AI의 편향성, 개인정보보호, 물리적 안전성과 같은 ‘신뢰성(Trustworthiness)’ 문제가 기술의 사회적 수용을 결정하는 가장 중요한 변수가 될 것이다.</li>
</ul>
<h2>8. 참고 자료</h2>
<ol>
<li>CoRL 2025 Conference | OpenReview, https://openreview.net/group?id=robot-learning.org/CoRL/2025/Conference</li>
<li>Paper accepted at Humanoids 2025 | SLMC - School of Informatics, https://informatics.ed.ac.uk/slmc/paper-accepted-at-humanoids-2025</li>
<li>How Google DeepMind’s new AI models can improve vision and reasoning in robots, https://timesofindia.indiatimes.com/technology/tech-news/how-google-deepminds-new-ai-models-can-improve-vision-and-reasoning-in-robots/articleshow/124215324.cms</li>
<li>CoRL 2025, https://www.corl.org/</li>
<li>Humanoids 2025, https://2025humanoids.org/</li>
<li>The AI Conference 2025 - Shaping the future of AI - The AI Conference, https://aiconference.com/</li>
<li>ISRAI2025 | 3rd International Summit on Robotics, Artificial Intelligence &amp; Machine Learning | Spectrum Conferences, https://robotics.spectrumconferences.com/</li>
<li>Top 10 AI Conferences for 2025 | DataCamp, https://www.datacamp.com/blog/top-ai-conferences</li>
<li>Automate 2026 - International Federation of Robotics, https://ifr.org/event/automate-2025</li>
<li>Blog - Google DeepMind, https://deepmind.google/discover/blog/</li>
<li>Gemini Robotics 1.5 brings AI agents into the physical world - Google DeepMind, https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/</li>
<li>Building the Next Generation of Physical Agents with Gemini …, https://developers.googleblog.com/en/building-the-next-generation-of-physical-agents-with-gemini-robotics-er-15/</li>
<li>VaultGemma: The world’s most capable differentially private LLM - Google Research, https://research.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/</li>
<li>Continuing to bring you our latest models, with an improved Gemini 2.5 Flash and Flash-Lite release - Google Developers Blog, https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/</li>
<li>Artificial Intelligence - arXiv, https://arxiv.org/list/cs.AI/new</li>
<li>ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes | alphaXiv, https://www.alphaxiv.org/overview/2506.14317v2</li>
<li>ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes - arXiv, https://arxiv.org/html/2506.14317v3</li>
<li>ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes - arXiv, https://arxiv.org/html/2506.14317v1</li>
<li>ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes | Request PDF - ResearchGate, https://www.researchgate.net/publication/392766133_ClutterDexGrasp_A_Sim-to-Real_System_for_General_Dexterous_Grasping_in_Cluttered_Scenes</li>
<li>ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes | AI Research Paper Details - AIModels.fyi, https://www.aimodels.fyi/papers/arxiv/clutterdexgrasp-sim-real-system-general-dexterous-grasping</li>
<li>AI-Powered Paper Summarization about the arXiv paper 2505.03729v1, https://www.summarizepaper.com/en/arxiv-id/2505.03729v1/</li>
<li>Visual Imitation Enables Contextual Humanoid Control - ChatPaper, https://chatpaper.com/paper/134668</li>
<li>Visual Imitation Enables Contextual Humanoid Control - arXiv, https://arxiv.org/html/2505.03729v1</li>
<li>[Literature Review] Visual Imitation Enables Contextual Humanoid Control - Moonlight, https://www.themoonlight.io/en/review/visual-imitation-enables-contextual-humanoid-control</li>
<li>Scaling Whole-body Multi-contact Manipulation with Contact Optimization - arXiv, https://arxiv.org/html/2508.12980v1</li>
<li>[2508.12980] Scaling Whole-body Multi-contact Manipulation with Contact Optimization, https://arxiv.org/abs/2508.12980</li>
<li>Humanoids 2025 Program | Tuesday September 30, 2025, https://ras.papercept.net/conferences/scripts/rtf/ICHR25_ContentListWeb_1.html</li>
<li>Robotics - arXiv, https://www.arxiv.org/list/cs.RO/pastweek?skip=12&amp;show=100</li>
<li>RoboArena, https://robo-arena.github.io/</li>
<li>RoboArena: Distributed Real-World Evaluation of Generalist Robot Policies - arXiv, https://arxiv.org/html/2506.18123v1</li>
<li>(PDF) RoboArena: Distributed Real-World Evaluation of Generalist Robot Policies, https://www.researchgate.net/publication/392941769_RoboArena_Distributed_Real-World_Evaluation_of_Generalist_Robot_Policies</li>
<li>Computer Science and Artificial Intelligence Laboratory (CSAIL) | MIT News | Massachusetts Institute of Technology, https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail?type=2</li>
<li>Discovering new solutions to century-old problems in fluid dynamics - Google DeepMind, https://deepmind.google/discover/blog/discovering-new-solutions-to-century-old-problems-in-fluid-dynamics/</li>
<li>MIT Generative AI Impact Consortium (MGAIC) Symposium, https://genai.mit.edu/mit-generative-ai-symposium/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>