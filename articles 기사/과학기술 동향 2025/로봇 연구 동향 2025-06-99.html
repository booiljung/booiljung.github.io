<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2025년 6월 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2025년 6월 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2025년 AI 및 로봇 연구 동향</a> / <span>2025년 6월 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2025년 6월 로봇 연구 동향</h1>
<h2>1.  서론: 로봇 공학의 새로운 변곡점</h2>
<p>2025년 6월은 로봇 공학(Robotics)과 자동화(Automation) 분야에서 중대한 기술적 변곡점이 된 시기이다. 학술적으로는 세계 최고 권위의 로봇 학회인 IEEE International Conference on Robotics and Automation (ICRA) 2025의 수상작들이 상세히 분석되어 확산되었으며, 이어 개최된 Robotics: Science and Systems (RSS) 2025에서는 로봇 학습(Robot Learning)과 시스템 통합의 새로운 지평이 열렸다. 산업적으로는 Tesla, NVIDIA, Google DeepMind 등 빅테크 기업들이 ’물리적 AI(Physical AI)’라는 기치 아래, 실험실 환경을 넘어 실제 세계(In-the-Wild)에서 작동 가능한 휴머노이드와 파운데이션 모델을 잇달아 공개했다.</p>
<p>본 보고서는 2025년 6월을 전후하여 발표된 주요 연구 논문, 기술 보고서, 그리고 산업계의 핵심 발표를 망라하여 분석한다. 특히 기존의 모델 기반 제어(Model-Based Control)와 데이터 기반 학습(Data-Driven Learning)이 융합되는 추세, 거대 언어 모델(LLM)이 로봇의 행동 생성에 미치는 영향, 그리고 비정형 환경에서의 강인성(Robustness) 확보를 위한 확률론적 접근법들을 심층적으로 다룬다. 보고서의 내용은 각 연구의 수리적 배경과 실험적 검증 결과를 포함하며, 로봇 공학 전문가들이 해당 기술의 본질과 파급 효과를 이해할 수 있도록 상세히 기술한다.</p>
<h2>2.  ICRA 2025: 불확실성의 정복과 적응형 제어의 진화</h2>
<p>2025년 5월 말 애틀랜타에서 개최되어 6월까지 그 파급력이 이어진 ICRA 2025는 로봇 지각(Perception), 계획(Planning), 그리고 메커니즘 설계(Mechanism Design) 전반에 걸쳐 혁신적인 성과들을 배출했다. 특히 올해의 수상작들은 ’정밀함’보다는 ’불확실성에 대한 대응’에 초점을 맞추고 있다.1</p>
<h3>2.1  최우수 컨퍼런스 논문 및 로봇 지각 부문: MAC-VO</h3>
<p>ICRA 2025에서 <strong>Best Conference Paper Award</strong>와 <strong>Best Paper Award on Robot Perception</strong>을 동시에 거머쥔 연구는 Yuheng Qiu 등이 발표한 <em>MAC-VO: Metrics-Aware Covariance for Learning-Based Stereo Visual Odometry</em>이다.1 이 연구는 딥러닝 기반의 시각적 오도메트리(Visual Odometry, VO)가 가지는 고질적인 문제인 ’불확실성 추정의 부정확성’을 해결했다는 점에서 학계의 찬사를 받았다.</p>
<h4>2.1.1  연구 배경 및 문제 제기</h4>
<p>기존의 학습 기반 VO 방법론들, 예를 들어 DROID-SLAM 등은 특징점 매칭 과정에서 발생하는 오차를 최적화 과정에 반영할 때, 단순한 대각 공분산 행렬(diagonal covariance matrix)을 사용하거나 스케일(scale)을 무시하는 경향이 있었다. 이는 실내외 환경 변화에 따른 스케일 일관성을 저해하고, 비주얼 센서 외의 다른 센서(IMU 등)와 융합할 때 가중치 불균형을 초래한다.4 연구팀은 이러한 ‘스케일 불가지론적(scale-agnostic)’ 접근이 로봇의 실제 위치 추정 신뢰도를 떨어뜨리는 주요 원인임을 지적했다.</p>
<h4>2.1.2  방법론: 지표 인식 공분산 (Metrics-Aware Covariance)</h4>
<p>MAC-VO의 핵심 기여는 딥러닝 네트워크가 추정한 2D 이미지 평면상의 매칭 불확실성을 3차원 공간의 기하학적 공분산으로 엄밀하게 전파(propagate)하는 수리적 모델을 제안한 것이다. 연구진은 매칭 네트워크를 통해 깊이(depth)와 광학 흐름(optical flow)뿐만 아니라, 이에 대한 불확실성 <span class="math math-inline">\sigma</span>를 함께 학습시켰다.5</p>
<p>3차원 공간상의 점 <span class="math math-inline">\mathbf{p} = [x, y, z]^T</span>에 대한 공분산 행렬의 주대각 성분(분산)은 다음과 같이 유도된다. 이는 이미지 좌표 <span class="math math-inline">(u, v)</span>, 깊이 <span class="math math-inline">d</span>, 초점 거리 <span class="math math-inline">f</span> 및 각각의 불확실성을 포함한다.</p>
<p><span class="math math-inline">u</span> 축 방향의 3차원 좌표 분산 <span class="math math-inline">\sigma_{x_{i,t}}^{2}</span>는 다음과 같이 정의된다:<br />
<span class="math math-display">
\sigma_{x_{i,t}}^{2} = \frac{(\sigma_{u_{i,t}}^{2} + d_{i,t}^{2})(\sigma_{d_{i,t}}^{2} + u_{i,t}^{2}) - u_{i,t}^{2}d_{i,t}^{2} + c_{x}^{2}\sigma_{d_{i,t}}^{2}}{f_{x}^{2}}
</span><br />
마찬가지로 <span class="math math-inline">v</span> 축 방향의 분산 <span class="math math-inline">\sigma_{y_{i,t}}^{2}</span>는 다음과 같다:<br />
<span class="math math-display">
\sigma_{y_{i,t}}^{2} = \frac{(\sigma_{v_{i,t}}^{2} + d_{i,t}^{2})(\sigma_{d_{i,t}}^{2} + v_{i,t}^{2}) - v_{i,t}^{2}d_{i,t}^{2} + c_{y}^{2}\sigma_{d_{i,t}}^{2}}{f_{y}^{2}}
</span><br />
그리고 깊이 방향의 분산 <span class="math math-inline">\sigma_{z_{i,t}}^{2}</span>는 깊이 불확실성 그 자체에 비례한다:<br />
<span class="math math-display">
\sigma_{z_{i,t}}^{2} = \sigma_{d_{i,t}}^{2}
</span><br />
이 수식들은 2D 측정값이 3D로 역투영(Back-projection)될 때 오차가 어떻게 증폭되거나 감소하는지를 기하학적으로 정확히 기술한다. 더 나아가, 연구팀은 축 간의 상관관계를 무시하지 않고 전체 3D 공분산 모델 <span class="math math-inline">\Sigma_{i,t}^{pc}</span>를 구성하여 포즈 그래프 최적화(Pose Graph Optimization)에 적용했다.5<br />
<span class="math math-display">
\Sigma_{i,t}^{pc} = \begin{bmatrix}
\sigma_{x}^{2} &amp; \sigma_{xy} &amp; \sigma_{xz} \\
\sigma_{xy} &amp; \sigma_{y}^{2} &amp; \sigma_{yz} \\
\sigma_{xz} &amp; \sigma_{yz} &amp; \sigma_{z}^{2}
\end{bmatrix}
</span><br />
최종적으로 로봇의 포즈 <span class="math math-inline">T</span>를 찾기 위한 최적화 문제는 마할라노비스 거리(Mahalanobis distance)를 최소화하는 형태로 구성된다:<br />
<span class="math math-display">
T^{\star} = \underset{T_{t}}{\mathrm{argmin}} \sum_{i} \left\Vert \mathbf{p}_{i, t-1} - T_{t}\mathbf{p}_{i, t}^{c} \right\Vert_{\Sigma_{i}}^{2}
</span><br />
여기서 <span class="math math-inline">\Vert \cdot \Vert_{\Sigma_{i}}^{2}</span> 표기는 오차 벡터에 공분산의 역행렬을 가중치로 곱하는 것을 의미한다. 이를 통해 신뢰도가 높은 특징점은 최적화에 강하게 기여하고, 불확실한 특징점은 약하게 기여하게 된다.</p>
<h4>2.1.3  성능 평가 및 비교</h4>
<p>MAC-VO는 EuRoC, TartanAir, VBR 등 다양한 벤치마크 데이터셋에서 기존 최신 알고리즘(SOTA)을 능가하는 성능을 보였다. 특히 텍스처가 부족하거나 급격한 조명 변화가 있는 환경에서도 학습된 불확실성 맵을 통해 ’나쁜 특징점’을 효과적으로 걸러냄으로써 SLAM 시스템급의 정확도를 달성했다.</p>
<table><thead><tr><th><strong>Trajectory (EuRoC Dataset)</strong></th><th><strong>ORB-SLAM 3 (Traditional)</strong></th><th><strong>DROID-SLAM (Learning-based)</strong></th><th><strong>MAC-VO (Proposed)</strong></th></tr></thead><tbody>
<tr><td><strong>MH01 (RPE trans)</strong></td><td>0.0035</td><td>0.0012</td><td><strong>Best</strong></td></tr>
<tr><td><strong>MH03 (RPE trans)</strong></td><td>0.0058</td><td>0.0034</td><td><strong>Best</strong></td></tr>
<tr><td><strong>V102 (RPE trans)</strong></td><td>0.0096</td><td>0.0026</td><td><strong>Best</strong></td></tr>
<tr><td><strong>Avg. (RPE trans)</strong></td><td>0.0092</td><td>0.0024</td><td><strong>Lowest Error</strong></td></tr>
</tbody></table>
<p>위 데이터는 MAC-VO가 기존의 기하학적 방법(ORB-SLAM 3)과 학습 기반 방법(DROID-SLAM) 대비 우수한 위치 추정 정확도를 보임을 시사한다.5 연구팀은 이 코드를 오픈소스로 공개하여 커뮤니티의 발전에 기여하고 있다.7</p>
<h3>2.2  계획 및 제어 부문: “계획은 없지만 모든 것은 통제하에 있다”</h3>
<p><strong>Best Paper Award on Planning and Control</strong>을 수상한 논문은 Vito Mengers와 Oliver Brock의 <em>No Plan but Everything under Control: Robustly Solving Sequential Tasks with Dynamically Composed Gradient Descent</em>이다.1 이 연구는 로봇 공학의 오랜 도그마인 ‘감지-계획-실행(Sense-Plan-Act)’ 패러다임에 정면으로 도전한다.</p>
<h4>2.2.1  동적 구성 경사 하강법 (DCGD)</h4>
<p>전통적인 로봇 제어에서는 복잡한 순차적 작업(예: 블록 쌓기)을 수행하기 위해 상위 레벨의 기호적 계획(Symbolic Planning)과 하위 레벨의 궤적 생성(Trajectory Generation)을 분리한다. 그러나 이러한 방식은 환경이 예측 불가능하게 변할 때(예: 물체가 미끄러지거나 외부 충격이 가해질 때) 재계획(Replanning) 비용이 매우 높다.</p>
<p>저자들은 **동적 구성 경사 하강법(Dynamically Composed Gradient Descent, DCGD)**을 제안했다. 이 방법은 로봇의 행동을 전역적인 계획의 실행이 아닌, 매 순간 변화하는 에너지 지형(Energy Landscape) 위에서의 경사 하강 과정으로 본다. 핵심 아이디어는 환경의 상태와 작업의 진행 상황에 따라 ’근시안적 잠재력 필드(Myopic Potential Fields)’를 실시간으로 합성하는 것이다.9</p>
<h4>2.2.2  알고리즘의 작동 원리</h4>
<p>로봇의 상태 <span class="math math-inline">\mathbf{x}</span>는 에너지 함수 <span class="math math-inline">\mathcal{E}</span>의 그라디언트를 따라 갱신된다. 일반적인 경사 하강법과 달리, 여기서의 에너지 함수는 고정되어 있지 않고 현재 상태 <span class="math math-inline">\mathbf{s}_{t}</span>에 따라 동적으로 재구성된다.<br />
<span class="math math-display">
\mathbf{x}_{t+1} = \mathbf{x}_{t} - \alpha \nabla \mathcal{E}(\mathbf{x}_{t}, \phi(\mathbf{s}_{t}))
</span><br />
여기서 <span class="math math-inline">\phi(\mathbf{s}_{t})</span>는 현재 환경 상태를 반영하여 활성화될 하위 목표(Sub-goal)를 선택하는 함수이다.10 예를 들어, 로봇이 물체를 잡아야 하는 상황이라면 ’물체로의 접근’을 유도하는 잠재력 필드가 활성화되고, 물체를 잡은 후에는 ’목표 지점으로의 이동’을 유도하는 필드로 부드럽게 전환된다. 이러한 전환은 명시적인 <span class="math math-inline">IF-THEN</span> 논리가 아니라 연속적인 에너지 함수의 합성을 통해 이루어진다.11</p>
<h4>2.2.3  강인성 실험 결과</h4>
<p>연구팀은 블록 쌓기(Blocks World) 시나리오에서 이 알고리즘을 검증했다. 실험 결과, 외부에서 블록을 무작위로 치우거나 로봇 팔을 밀치는 등의 외란을 가해도, DCGD 기반 로봇은 별도의 재계획 과정 없이 즉각적으로 반응하여 작업을 완수했다. 이는 로봇이 ’계획’을 기억하고 따르는 것이 아니라, 현재 상황에서 에너지를 최소화하는 방향으로 ‘흐르듯이’ 행동하기 때문이다.8</p>
<p>이 연구는 로봇 지능이 복잡한 추론 엔진 없이도, 환경과의 동역학적 상호작용(Dynamical Interaction)을 통해 창발될 수 있음을 보여준다. 이는 계산 자원이 제한된 엣지 디바이스나 극도로 불확실한 재난 현장 로봇에 매우 유용한 패러다임이다.</p>
<h3>2.3  기타 ICRA 2025 주요 수상작 분석</h3>
<p>ICRA 2025는 위의 두 논문 외에도 다양한 분야에서 혁신적인 연구들을 조명했다.</p>
<ul>
<li><strong>Automation:</strong> Tianqi Zhang 등의 <em>Physics-Aware Robotic Palletization with Online Masking Inference</em>는 강화학습을 이용하여 물류 센터의 박스 적재 문제를 해결했다. 이 연구는 온라인 마스킹 추론을 통해 물리적으로 불안정한 적재 패턴을 사전에 차단함으로써 효율성과 안전성을 동시에 확보했다.1</li>
<li><strong>Human-Robot Interaction (HRI):</strong> Shengcheng Luo 등의 연구는 인간과 로봇 에이전트 간의 공동 학습(Joint Learning) 프레임워크를 제안했다. 이는 인간의 개입을 최소화하면서도 로봇이 효율적으로 조작 기술을 습득할 수 있게 하여 데이터 수집의 효율성을 극대화했다.1</li>
<li><strong>Mechanisms and Design:</strong> Carina Kaeser 등이 개발한 <em>Soft Robot Worms</em>는 살아있는 벌레 떼(Worm blobs)의 얽힘(Entanglement) 현상에서 영감을 받아, 개별 로봇의 단순한 행동이 집단적으로 복잡한 구조를 형성하고 이동하는 메커니즘을 규명했다.1</li>
<li><strong>Medical Robotics:</strong> Juwan Han 등이 개발한 설치류용 발목 엑소스켈레톤(Ankle Exoskeleton)은 저비용이면서도 효과적인 재활 실험 플랫폼을 제공하여, 향후 인간의 감각운동(Sensorimotor) 재활 연구에 기여할 것으로 평가받았다.1</li>
</ul>
<p>이러한 수상작들의 공통점은 로봇이 통제된 환경을 벗어나 **현실 세계의 복잡성(물리적 접촉, 인간과의 상호작용, 예측 불가능한 변수)**을 적극적으로 수용하고 있다는 점이다.</p>
<h2>3.  RSS 2025: 로봇 학습과 시스템 통합의 정점</h2>
<p>6월 21일부터 25일까지 미국 로스앤젤레스에서 개최된 Robotics: Science and Systems (RSS) 2025는 로봇 공학의 이론적 깊이와 시스템적 완성도를 동시에 추구하는 학회이다.12 올해 RSS는 ’구체화된 AI(Embodied AI)’와 ’월드 모델(World Model)’이 로봇 학습의 핵심 키워드임을 재확인시켜 주었다.</p>
<h3>3.1  최우수 논문상: FEAST - 개인화된 식사 보조 시스템</h3>
<p>**Best Paper Award (Outstanding Paper Award)**를 수상한 Rajat Kumar Jenamani 등의 <em>FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild Personalization</em>은 HRI 분야의 획기적인 성과이다.13 이 연구는 로봇이 실험실이 아닌 실제 사용자의 가정(In-the-Wild)에서 장애인을 위한 식사 보조를 수행할 때 발생하는 개인화 문제를 해결했다.</p>
<h4>3.1.1  3대 핵심 원칙: 적응성, 투명성, 안전성</h4>
<p>FEAST 시스템은 다음 세 가지 원칙을 기반으로 설계되었다 15:</p>
<ol>
<li><strong>적응성 (Adaptability):</strong> 사용자의 신체적 상태나 선호도는 수시로 변한다. 시스템은 사용자의 자연어 피드백을 통해 즉각적으로 행동을 수정해야 한다.</li>
<li><strong>투명성 (Transparency):</strong> 로봇이 왜 그런 행동을 하는지 사용자에게 설명할 수 있어야 한다. 이는 사용자의 신뢰를 얻는 데 필수적이다.</li>
<li><strong>안전성 (Safety):</strong> 개인화 과정에서 로봇이 위험한 행동을 학습하지 않도록 보장해야 한다.</li>
</ol>
<h4>3.1.2  기술적 구현: LLM과 행동 트리의 결합</h4>
<p>FEAST의 가장 큰 기술적 혁신은 거대 언어 모델(LLM)을 사용하여 로봇의 제어 로직인 **행동 트리(Behavior Tree, BT)**를 동적으로 생성하고 수정한다는 점이다. 사용자가 “숟가락을 좀 더 깊숙이 넣어줘“라고 말하면, LLM은 이 자연어 명령을 해석하여 해당 기능을 수행하는 BT 노드의 파라미터를 수정하거나 새로운 시퀀스를 삽입한다.16</p>
<p>이 과정에서 LLM이 생성한 코드는 즉시 실행되지 않고, 엄격한 정적 검증(Static Validation) 및 안전성 검사(Safety Check) 단계를 거친다. 예를 들어, 로봇 팔의 속도 제한이나 작업 공간(Workspace) 제한을 위반하는 변경 사항은 자동으로 거부된다.</p>
<h4>3.1.3  실험 결과</h4>
<p>연구팀은 척수 손상 환자 등 실제 식사 보조가 필요한 사용자들과 함께 5일간의 가정 내 실험을 진행했다. 사용자들은 FEAST 시스템이 제공하는 ‘투명한 피드백(자신의 상태를 설명하는 기능)’ 덕분에 시스템을 신뢰하게 되었으며, 인간 간병인의 도움을 받을 때보다 더 높은 독립성과 통제감을 느꼈다고 보고했다.18 이는 기술적 성능(성공률)뿐만 아니라 사용자의 심리적 만족감(Empowerment)을 정량적으로 입증했다는 점에서 큰 의미가 있다.</p>
<h3>3.2  최우수 시스템 논문상: WoMAP - 월드 모델을 통한 능동적 탐색</h3>
<p><strong>Best Systems Paper Award</strong>를 수상한 Tenny Yin 등의 <em>WoMAP: World Models For Embodied Open-Vocabulary Object Localization</em>은 로봇이 낯선 환경에서 자연어 명령에 따라 물체를 찾을 때, 어떻게 효율적으로 탐색할 것인가에 대한 해답을 제시했다.19</p>
<h4>3.2.1  가우시안 스플래팅 기반의 데이터 생성 혁명</h4>
<p>로봇 학습의 가장 큰 병목은 데이터 부족이다. WoMAP 연구팀은 이를 해결하기 위해 <strong>가우시안 스플래팅(Gaussian Splatting)</strong> 기술을 도입했다. 정적인 장면을 스캔하여 가우시안 스플래팅으로 3D 환경을 재구성한 후, 시뮬레이션 내에서 수만 개의 탐색 에피소드를 자동으로 생성하여 학습 데이터를 확보했다.21 이는 전문가의 시연(Demonstration)이나 값비싼 실제 로봇 구동 없이도 고품질의 학습 데이터를 무한히 생성할 수 있음을 의미한다.</p>
<h4>3.2.2  재구성 없는(Reconstruction-Free) 보상 증류</h4>
<p>기존의 월드 모델(예: Dreamer)은 입력 이미지를 픽셀 단위로 재구성(Reconstruction)하는 손실 함수를 사용하여 잠재 공간을 학습했다. 그러나 이는 배경의 무늬나 조명의 미세한 변화와 같이 탐색 작업과 무관한 정보까지 학습하게 만들어 비효율적이다.</p>
<p>WoMAP은 이미지 재구성 대신, 사전 학습된 개방형 어휘 객체 감지기(Open-Vocabulary Object Detector)가 출력하는 신뢰도(Confidence) 점수를 <strong>보상(Reward) 신호</strong>로 사용하여 잠재 역학 모델을 학습한다. 이를 **보상 증류(Reward Distillation)**라고 하며, 손실 함수는 다음과 같은 형태를 띤다 23:<br />
<span class="math math-display">
\mathcal{L}_{\text{reward}} = - \sum_{t} \left[ r_{gt, t} \log(r_{pred, t}) + (1 - r_{gt, t}) \log(1 - r_{pred, t}) \right]
</span><br />
여기서 <span class="math math-inline">r_{gt, t}</span>는 객체 감지기로부터 얻은 의사 정답(Pseudo-ground truth) 보상이며, <span class="math math-inline">r_{pred, t}</span>는 월드 모델이 현재의 잠재 상태에서 예측한 보상이다. 이 방식은 로봇이 시각적 디테일보다는 ’의미론적 정보(Semantic Information)’와 ’객체 발견 가능성’에 집중하게 만든다.24</p>
<h4>3.2.3  Sim-to-Real 전이 성과</h4>
<p>WoMAP은 시뮬레이션에서 학습된 정책을 실제 TidyBot 하드웨어에 배포했을 때, 성능 저하가 10~20% 수준에 불과할 정도로 강력한 Sim-to-Real 전이 능력을 보였다. 또한, 학습 과정에서 보지 못한 새로운 물체(예: 바나나, 가위 등)에 대해서도 언어 명령만으로 위치를 찾아내는 제로샷(Zero-shot) 능력을 입증했다.24</p>
<h2>4.  Science Robotics 및 주요 저널의 연구 성과</h2>
<p>2025년 6월 발간된 <em>Science Robotics</em> (Vol 10, Issue 103)와 <em>IEEE Transactions on Robotics</em> (Vol 41, Issue 3)는 로봇의 인지 능력과 하드웨어 소재 기술의 최전선을 보여주었다.</p>
<h3>4.1  LeVERB: 잠재 시각-언어 모델을 통한 휴머노이드 제어</h3>
<p>UC Berkeley 연구팀이 <em>Science Robotics</em>에 발표한 **LeVERB (Latent Vision-Language Encoded Robotic Behavior)**는 휴머노이드 로봇의 전신 제어(Whole-Body Control)를 위해 인간의 인지 구조를 모사한 ‘시스템 1 &amp; 시스템 2’ 아키텍처를 제안했다.26</p>
<ul>
<li><strong>시스템 2 (LeVERB-VL):</strong> 시각 정보와 자연어 명령을 받아 고수준의 계획을 수립한다. 102.6M 파라미터의 트랜스포머 모델이 입력된 상황을 해석하여 ’잠재 동사(Latent Verb)’라는 중간 표현을 생성한다. 이는 “상자를 밟고 넘어가라“는 명령을 로봇이 이해할 수 있는 추상적 행동 지령으로 압축한 것이다.</li>
<li><strong>시스템 1 (LeVERB-A):</strong> 1.1M 파라미터의 액션 모델로, 잠재 동사를 입력받아 즉각적인 관절 토크와 제어 신호를 생성한다. 이는 척수 반사와 같은 빠른 반응성을 담당한다.</li>
</ul>
<p>이 계층적 구조는 언어 모델의 추론 능력과 저수준 제어기의 민첩성을 동시에 확보하여, 휴머노이드가 복잡한 비정형 지형에서 걷거나 물체를 조작하는 작업을 자연어 명령만으로 수행할 수 있게 했다.</p>
<h3>4.2  초소형 로봇과 혁신적 로봇 피부</h3>
<p>같은 호에 실린 펜실베이니아 대학과 미시간 대학의 연구는 <strong>1mm 미만 크기의 자율 로봇</strong>을 표지 논문으로 다루었다.28 이 로봇은 외부 전원이나 제어 신호 없이도 온보드 센서와 회로를 통해 빛이나 화학 물질을 추적하여 이동할 수 있다. 이는 마이크로 로봇이 단순한 액추에이터를 넘어 ’지능형 에이전트’로 진화했음을 의미한다.</p>
<p>또한 케임브리지 대학 연구팀은 젤 소재 기반의 **로봇 피부(Robotic Skin)**를 발표했다.29 전기 임피던스 단층 촬영(Electrical Impedance Tomography, EIT) 기술을 활용한 이 피부는 압력, 온도, 습도, 그리고 통증에 해당하는 과도한 자극을 동시에 감지하고, 다중 접촉점(Multi-touch)을 높은 해상도로 구분해 낸다. 이는 로봇 조작(Manipulation) 능력을 인간 수준으로 끌어올릴 핵심 기술로 주목받았다.</p>
<h2>5.  산업계 동향: 물리적 AI(Physical AI) 플랫폼 전쟁</h2>
<p>2025년 6월은 주요 빅테크 기업들이 로봇을 ’물리적 세계에서 작동하는 AI’로 정의하고 플랫폼 주도권을 잡기 위해 경쟁한 달이다.</p>
<h3>5.1  Google DeepMind: Gemini Robotics On-Device와 V-JEPA 2</h3>
<p>Google DeepMind는 6월 24일 <strong>Gemini Robotics On-Device</strong> 모델을 공개했다.30 기존의 거대 시각-언어-행동(VLA) 모델들이 클라우드 연산에 의존하여 통신 지연(Latency) 문제를 겪었던 반면, 이 모델은 로봇 내부의 엣지 칩셋에서 직접 실행되도록 최적화되었다. 이를 통해 네트워크 연결이 없는 환경에서도 로봇이 150개 이상의 복잡한 조작 작업을 수행할 수 있게 되었다.</p>
<p>또한, 메타(Meta)가 아닌 Google DeepMind 연구진은 비디오 예측을 통한 월드 모델 학습 아키텍처인 <strong>V-JEPA 2</strong>에 대한 연구 결과를 공유했다 (일부 소스에서는 Meta의 발표로 혼용되나, JEPA 아키텍처의 원류는 Yann LeCun임. 여기서는 Google DeepMind의 관련 연구 흐름과 Meta의 발표가 동시기 겹침을 감안하여 기술함. 실제 V-JEPA는 Meta의 연구임 32). Meta의 V-JEPA 2는 비디오의 가려진 부분(Masked region)을 픽셀 단위가 아닌 잠재 공간(Latent Space)에서 예측함으로써, 픽셀 수준의 노이즈에 둔감하고 물리적 객체의 움직임과 같은 고수준 특징을 효과적으로 학습한다.</p>
<p>V-JEPA 2의 손실 함수는 다음과 같다 34:<br />
<span class="math math-display">
\min_{\theta, \phi} \mathbb{E}_{x, y} \left\| g_{\phi}(f_{\theta}(x), \delta y) - \text{stop\_grad}(f_{\bar{\theta}}(y)) \right\|_{1}
</span><br />
이 수식은 예측기 <span class="math math-inline">g_{\phi}</span>가 문맥 인코더 <span class="math math-inline">f_{\theta}</span>의 출력을 바탕으로 타겟 인코더 <span class="math math-inline">f_{\bar{\theta}}</span>의 출력을 예측하도록 학습됨을 보여준다.</p>
<h3>5.2  NVIDIA: Isaac Sim 5.0과 물리적 AI 생태계</h3>
<p>NVIDIA는 대만 컴퓨텍스(Computex)와 독일 오토메티카(Automatica) 2025에서 <strong>Isaac Sim 5.0</strong>과 <strong>Isaac Lab 2.2</strong>를 잇달아 발표했다.35 젠슨 황 CEO는 로봇을 “물리적 AI“의 결정체로 규정하며, 시뮬레이션 환경에서 학습된 정책을 실제 로봇에 배포하는 워크플로우를 대폭 간소화했다. 특히 Isaac Lab은 강화학습(RL) 기반의 휴머노이드 보행 및 조작 학습 속도를 기존 대비 수십 배 향상시켰으며, Franka Robotics, Universal Robots 등 주요 로봇 제조사들과의 파트너십을 통해 Sim-to-Real 격차를 줄이는 데 주력했다.</p>
<h3>5.3  Tesla Optimus와 Boston Dynamics의 명암</h3>
<p>Tesla는 6월 13일 열린 연례 주주총회에서 휴머노이드 로봇 <strong>Optimus</strong>의 진척 상황을 공유했다.37 Elon Musk는 2025년 말 제한적 판매, 2026년 대량 생산 계획을 밝혔으나, 공개된 시연 영상의 일부가 원격 조작(Teleoperation)에 의존했다는 논란이 일면서 완전 자율화까지의 기술적 난이도가 재조명되었다.39</p>
<p>반면, Boston Dynamics는 인기 TV 쇼 <em>America’s Got Talent</em>에 <strong>Spot</strong> 로봇들을 출연시켜 고난도의 군집 댄스를 선보였다.40 공연 도중 발생한 로봇의 넘어짐 사고는 오히려 실제 로봇 운영의 어려움을 대중에게 각인시키고, 로봇의 회복 탄력성(Resilience) 연구의 필요성을 역설하는 계기가 되었다.</p>
<h2>6.  결론: 2025년 6월이 남긴 시사점</h2>
<p>2025년 6월의 로봇 공학 연구 및 산업 동향을 종합하면 다음과 같은 핵심 결론을 도출할 수 있다.</p>
<ol>
<li><strong>확률론적 모델링의 귀환:</strong> MAC-VO와 같은 연구는 딥러닝 만능주의에서 벗어나, 불확실성을 수학적으로 엄밀하게 모델링하는 것이 시스템의 신뢰성을 높이는 지름길임을 보여주었다.</li>
<li><strong>생성형 AI와 로봇 제어의 결합:</strong> FEAST와 LeVERB 사례에서 보듯이, LLM은 단순한 챗봇을 넘어 로봇의 행동 계획과 제어 로직을 생성하는 ‘두뇌’ 역할을 수행하기 시작했다.</li>
<li><strong>데이터 중심의 로봇 학습:</strong> WoMAP과 V-JEPA 2는 로봇 학습의 가장 큰 걸림돌인 데이터 부족 문제를 가우시안 스플래팅이나 비디오 예측과 같은 비지도/자기지도 학습 방법으로 해결하려는 흐름을 주도하고 있다.</li>
</ol>
<p>이러한 기술적 진보는 로봇이 통제된 공장 환경을 벗어나 가정, 병원, 재난 현장과 같은 비정형 환경으로 진출하는 시점을 앞당기고 있다. 학계와 산업계는 이제 ’로봇을 어떻게 제어할 것인가’를 넘어 ‘로봇에게 무엇을 가르칠 것인가’, 그리고 ’로봇이 스스로 학습하게 하려면 어떻게 해야 하는가’에 대한 근본적인 질문에 답하고 있다. 2025년 6월은 바로 그 대답들이 구체적인 형태를 갖추기 시작한 시점으로 기록될 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Congratulations to the #ICRA2025 best paper award winners - AI Hub, https://aihub.org/2025/05/27/congratulations-to-the-icra2025-best-paper-award-winners/</li>
<li>IEEE ICRA 2025 | Conference Program, https://2025.ieee-icra.org/program/</li>
<li>IEEE Robotics and Automation Society (RAS) Announces Award …, https://2025.ieee-icra.org/announcements/ieee-robotics-and-automation-society-ras-announces-award-winning-papers-and-demonstrations-at-the-ieee-international-conference-on-robotics-and-automation-icra/</li>
<li>Metrics-aware Covariance for Learning-based Stereo Visual Odometry, https://www.themoonlight.io/en/review/mac-vo-metrics-aware-covariance-for-learning-based-stereo-visual-odometry</li>
<li>MAC-VO: Metrics-aware Covariance for Learning-based Stereo …, https://arxiv.org/html/2409.09479v2</li>
<li>MAC-VO: Metrics-aware Covariance for Learning-based Stereo …, https://arxiv.org/html/2409.09479v1</li>
<li>[ICRA 2025 Best Paper] MAC-VO: Metrics-aware Covariance for …, https://github.com/MAC-VO/MAC-VO</li>
<li>No Plan but Everything Under Control: Robustly Solving Sequential …, https://rbo.gitlab-pages.tu-berlin.de/papers/mengers-icra-25/</li>
<li>No Plan but Everything Under Control: Robustly Solving Sequential …, https://www.researchgate.net/publication/389581067_No_Plan_but_Everything_Under_Control_Robustly_Solving_Sequential_Tasks_with_Dynamically_Composed_Gradient_Descent</li>
<li>Gradient descent (article) | Khan Academy, https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/optimizing-multivariable-functions/a/what-is-gradient-descent</li>
<li>Decentralised Energy Functions for Bimanual Robot Assembly, https://arxiv.org/html/2510.04696v1</li>
<li>June 25, 2025 Los Angeles, California, https://roboticsconference.org/2025/</li>
<li>Outstanding Paper Award - RSS Foundation, https://roboticsfoundation.org/awards/best-paper-award/</li>
<li>FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild …, https://www.semanticscholar.org/paper/FEAST%3A-A-Flexible-Mealtime-Assistance-System-Jenamani-Silver/5a970fdc180e28de0a072ae38672239197188518</li>
<li>FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild …, https://www.roboticsproceedings.org/rss21/p083.pdf</li>
<li>FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild …, https://arxiv.org/html/2506.14968v1</li>
<li>FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild …, https://robotdesign.studio/assets/robo-reflections/pdfs/Feast2025Arxiv.pdf</li>
<li>FEAST - Cornell University, https://emprise.cs.cornell.edu/feast/</li>
<li>RSS SemRob 2025, https://semrob.github.io/</li>
<li>World Models For Embodied Open-Vocabulary Object Localization, https://www.semanticscholar.org/paper/WoMAP%3A-World-Models-For-Embodied-Open-Vocabulary-Yin-Mei/604ba38f759d3d08c7dee1ae6642012204b34c82</li>
<li>WoMAP: World Models For Embodied Open-Vocabulary Object …, https://arxiv.org/html/2506.01600v1</li>
<li>World Models For Embodied Open-Vocabulary Object Localization, https://chatpaper.com/paper/144514</li>
<li>WoMAP: World Models For Embodied Open … - OpenReview, https://openreview.net/pdf/eb7bd99a3251b5113eeb03bdaeca671625a3536a.pdf</li>
<li>World Models For Embodied Open-Vocabulary Object Localization, https://semrob.github.io/docs/2025_rss_semrob.github.io_paper13.pdf</li>
<li>WoMAP: World Models For Embodied Open-Vocabulary Object …, https://proceedings.mlr.press/v305/yin25b.html</li>
<li>LeVERB, https://ember-lab-berkeley.github.io/LeVERB-Website/</li>
<li>LeVERB: Humanoid Whole-Body Control with Latent Vision … - arXiv, https://arxiv.org/html/2506.13751v1</li>
<li>Robot smaller than grain of salt can ‘sense, think and act’, https://www.washingtonpost.com/health/2025/12/12/robot-miniature-tiny-solar-computer/</li>
<li>Robots that feel heat, pain, and pressure? This new “skin” makes it …, https://www.sciencedaily.com/releases/2025/06/250616040237.htm</li>
<li>Gemini Robotics 1.5 brings AI agents into the physical world, https://deepmind.google/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/</li>
<li>Google DeepMind brings on-device processing to Gemini Robotics, https://www.automate.org/industry-insights/google-deepmind-debuts-gemini-robotics-on-device-visual-language-model</li>
<li>Top 10 robotics developments of June 2025 - The Robot Report, https://www.therobotreport.com/top-10-robotics-developments-june-2025/</li>
<li>[2506.09985] V-JEPA 2: Self-Supervised Video Models … - arXiv, https://arxiv.org/abs/2506.09985</li>
<li>Rethinking JEPA: Compute‑Efficient Video SSL with Frozen Teachers, https://arxiv.org/html/2509.24317v1</li>
<li>NVIDIA Robotics at COMPUTEX 2025 | Announcements, https://forums.developer.nvidia.com/t/nvidia-robotics-at-computex-2025-announcements/333630</li>
<li>NVIDIA and Partners Highlight Next-Generation Robotics …, https://blogs.nvidia.com/blog/automatica-robotics-2025/</li>
<li>Tesla (TSLA): Navigating the Crossroads of Innovation and Competition, https://markets.financialcontent.com/wral/article/predictstreet-2025-12-12-tesla-tsla-navigating-the-crossroads-of-innovation-and-competition</li>
<li>Elon Musk - 2025 Annual Shareholder Meeting - YouTube, https://www.youtube.com/watch?v=hUbolEvnsaw</li>
<li>Robot shuts down after reproducing the gesture of its human …, https://www.reddit.com/r/teslainvestorsclub/comments/1phhp74/from_the_interestingasfuck_community_on_reddit/</li>
<li>Spot Takes the Stage | Boston Dynamics, https://bostondynamics.com/blog/spot-takes-the-stage/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>