<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2025년 7월 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2025년 7월 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2025년 AI 및 로봇 연구 동향</a> / <span>2025년 7월 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2025년 7월 로봇 연구 동향</h1>
<h2>1.  서론: 물리적 세계로 확장된 생성형 지능의 시대</h2>
<p>2025년 7월은 로봇 공학 및 자동화 분야에서 분수령이 되는 시기였다. 지난 수년간 인공지능(AI) 분야를 뜨겁게 달구었던 대규모 언어 모델(LLM)과 생성형 AI의 파도가 비로소 디지털 세계를 넘어 물리적 실체(Physical Body)를 가진 로봇의 제어 시스템 깊숙이 침투하기 시작했기 때문이다. 학계와 산업계 전반에서 관찰된 이러한 현상은 ‘물리적 AI(Physical AI)’ 혹은 ’체화된 지능(Embodied Intelligence)’이라는 키워드로 요약될 수 있다. 로봇은 이제 사전 정의된 규칙을 수행하는 기계를 넘어, 불확실한 현실 세계에서 스스로 경험을 축적하고, 기억을 생성하며, 행동을 수정하는 능동적 에이전트로 진화하고 있다.</p>
<p>특히 2025년 7월에 개최된 주요 학술 대회인 <strong>RoboCup 2025</strong>와 <strong>RSS 2025 (Robotics: Science and Systems)</strong>, 그리고 동기간 ArXiv와 주요 저널을 통해 발표된 연구들은 이러한 기술적 특이점을 명확히 보여준다. 기존의 모델 기반 제어(Model-based Control)와 강화 학습(Reinforcement Learning)이 주도하던 흐름 속에 **확산 모델(Diffusion Model)**과 <strong>시각-언어 모델(VLM)</strong> 기반의 추론이 새로운 표준으로 부상하고 있다. 또한, 산업계에서는 테슬라(Tesla)와 피규어 AI(Figure AI) 등이 휴머노이드 로봇의 하드웨어적 완성도와 상용화 가능성을 입증하는 결정적인 데모와 기술 스펙을 공개하며 경쟁을 가속화했다.</p>
<p>본 보고서는 2025년 7월 한 달간 발표된 방대한 연구 성과와 산업계의 발표를 면밀히 분석한다. 각 연구의 이론적 배경과 알고리즘적 혁신, 그리고 실험적 검증 과정을 수식과 데이터를 통해 상세히 해부하고, 이것이 로봇 공학의 미래에 던지는 시사점을 다각도로 조망한다. 분석은 단순한 사실 나열을 지양하고, 개별 연구 간의 유기적 연결 고리와 기술적 인과 관계를 규명하는 데 초점을 맞춘다.</p>
<h2>2.  RoboCup 2025: 동적 환경에서의 생성형 제어와 자율성의 검증</h2>
<p>브라질 살바도르에서 개최된 <strong>RoboCup 2025</strong>는 전 세계 로봇 공학자들에게 있어 생성형 AI가 고도로 역동적인 물리적 환경에서 어떻게 적용될 수 있는지를 검증하는 거대한 실험장이었다. 축구, 구조, 홈 서비스 등 다양한 리그에서 전통적인 제어 이론의 한계를 넘어서려는 시도들이 목격되었으며, 특히 딥러닝 기반의 엔드-투-엔드(End-to-End) 제어가 주류로 자리 잡는 경향이 뚜렷했다.</p>
<h3>2.1  휴머노이드 제어의 패러다임 전환: SoccerDiffusion</h3>
<p>금번 심포지엄에서 가장 주목받은 연구이자, 향후 로봇 제어의 방향성을 제시한 논문은 <strong>“SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from Gameplay Recordings”</strong> 1이다. 이 연구는 복잡한 휴머노이드 로봇의 전신 제어(Whole-body Control) 문제를 해결하기 위해 트랜스포머 기반의 확산 모델(Transformer-based Diffusion Model)을 도입했다.</p>
<h4>2.1.1  연구 배경 및 문제 정의</h4>
<p>휴머노이드 로봇 축구는 로봇 공학의 ‘그랜드 챌린지’ 중 하나로 꼽힌다. 로봇은 넘어지지 않고 걷거나 뛰어야 할 뿐만 아니라, 상대 로봇과의 충돌, 미끄러운 바닥, 공의 불규칙한 움직임 등 예측 불가능한 변수들에 실시간으로 대응해야 한다. 기존의 접근법은 보행 엔진, 킥 엔진, 비전 시스템 등을 개별적으로 설계하고 통합하는 모듈식 구조가 주를 이루었으나, 이는 모듈 간의 인터페이스 최적화가 어렵고 새로운 상황에 대한 적응력이 떨어진다는 한계가 있었다. 강화 학습(RL)은 이러한 문제를 해결할 대안으로 떠올랐으나, 시뮬레이션과 실제 환경 간의 격차(Sim-to-Real Gap)와 보상 함수(Reward Function) 설계의 난이도라는 장벽이 존재했다.</p>
<p>연구팀은 실제 RoboCup 경기에서 수집된 대규모 데이터를 활용하여, 명시적인 보상 함수 없이도 전문가(기존의 우수한 제어 알고리즘 또는 인간 조종)의 행동 분포를 모방하는 생성적 접근법을 택했다.</p>
<h4>2.1.2  SoccerDiffusion 아키텍처 및 알고리즘</h4>
<p>SoccerDiffusion 모델은 다중 모달 데이터를 입력받아 관절 명령 궤적(Joint Command Trajectory)을 생성하는 구조를 가진다. 입력 데이터는 시각 정보(이미지), 고유수용성 감각(관절 상태), 그리고 게임 상태(Game State)를 포함한다.</p>
<p>이 모델의 핵심은 **확산 확률 모델(Diffusion Probabilistic Model)**의 적용이다. 확산 모델은 데이터에 노이즈를 점진적으로 추가하는 전방 과정(Forward Process)과, 노이즈로부터 원본 데이터를 복원하는 역방향 과정(Reverse Process)으로 구성된다. 로봇 제어 문제에서 역방향 과정은 현재의 관측 상태 <span class="math math-inline">o_t</span>가 주어졌을 때, 최적의 행동 궤적 <span class="math math-inline">a_{t:t+H}</span>를 생성하는 조건부 확률 <span class="math math-inline">p(a_{t:t+H} \vert o_t)</span>를 모델링하는 것으로 정의된다.</p>
<p>일반적인 확산 모델의 역방향 과정은 다음과 같은 확률 미분 방정식(SDE)의 형태로 표현될 수 있다.<br />
<span class="math math-display">
d\mathbf{x} = \left[ \mathbf{f}(\mathbf{x}, t) - g^2(t) \nabla_{\mathbf{x}} \log p_t(\mathbf{x}) \right] dt + g(t) d\mathbf{w}
</span><br />
여기서 <span class="math math-inline">\mathbf{x}</span>는 상태 변수, <span class="math math-inline">\mathbf{f}</span>는 드리프트 계수, <span class="math math-inline">g</span>는 확산 계수, <span class="math math-inline">\mathbf{w}</span>는 브라운 운동을 나타낸다. SoccerDiffusion은 이를 로봇의 행동 생성에 적용하여, 초기 가우시안 노이즈로부터 점진적인 디노이징(Denoising)을 통해 유효한 관절 명령 궤적을 생성한다.</p>
<p>그러나 확산 모델은 일반적으로 수십에서 수백 단계의 반복적인 추론 과정을 거쳐야 하므로, 밀리초(ms) 단위의 실시간 제어가 필요한 로봇 축구에 직접 적용하기에는 연산 비용이 너무 높다. 이를 해결하기 위해 연구팀은 <strong>증류(Distillation)</strong> 기법을 도입했다. 이는 다단계 추론을 수행하는 ’교사 모델(Teacher Model)’의 출력을 단 한 번의 추론으로 예측하도록 ’학생 모델(Student Model)’을 학습시키는 과정이다. 증류 손실 함수(Distillation Loss Function)는 다음과 같이 정의된다.<br />
<span class="math math-display">
\mathcal{L}_{distill} = \mathbb{E}_{x_0, \epsilon, t} \left[ \| \mathcal{D}_{\theta}(\mathbf{x}_t, t) - \mathcal{D}_{\phi}(\mathbf{x}_t) \|_2^2 \right]
</span><br />
여기서 <span class="math math-inline">\mathcal{D}_{\theta}</span>는 교사 모델의 디노이징 함수, <span class="math math-inline">\mathcal{D}_{\phi}</span>는 학생 모델의 디노이징 함수를 의미한다. 이 과정을 통해 300Hz 이상의 고속 제어 루프 안에서도 확산 모델 기반의 정책을 실행할 수 있게 되었다.</p>
<h4>2.1.3  데이터셋 구축 및 실험 결과</h4>
<p>연구팀은 총 88개의 경기 녹화본에서 추출한 약 340GB 분량의 데이터를 구축했다.1 데이터 전처리 과정에서는 관절 데이터와 IMU 데이터를 50Hz로 리샘플링하고 동기화했으며, 이미지는 480x480 해상도로 조정했다.</p>
<table><thead><tr><th><strong>데이터 유형</strong></th><th><strong>원본 샘플링 레이트</strong></th><th><strong>처리 후 샘플링 레이트</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td>관절 데이터 (Joints)</td><td>&gt; 300 Hz</td><td>50 Hz</td><td>동기화 및 리샘플링 수행</td></tr>
<tr><td>이미지 데이터 (Images)</td><td>가변적</td><td>10 Hz</td><td>480x480 리사이징</td></tr>
<tr><td>게임 상태 (Game State)</td><td>이벤트 기반</td><td>50 Hz</td><td>플레이 가능 여부 등으로 단순화</td></tr>
</tbody></table>
<p>실험 결과, SoccerDiffusion을 탑재한 Wolfgang-OP 로봇은 걷기, 공 차기, 그리고 넘어진 상태에서 일어나는 회복 동작(Fall Recovery)을 성공적으로 수행했다. 특히 넘어짐 회복 실험에서 시뮬레이션 상 100%, 실제 로봇 상 95%의 성공률을 기록하며 2, 기존의 하드코딩된 동작보다 훨씬 유연하고 강건한 성능을 보여주었다. 이는 로봇이 명시적인 동작 정의 없이도 데이터만으로 복잡한 운동 기술을 습득할 수 있음을 증명한 사례이다. 다만, 전술적인 위치 선정이나 팀 플레이와 같은 고수준의 전략적 행동은 아직 데이터 분포의 복잡성으로 인해 완벽하게 구현되지 않았다는 한계도 지적되었다.</p>
<h3>2.2  서비스 로봇의 지능화: 부산대학교 “TidyBoy“의 약진</h3>
<p><strong>RoboCup 2025 @Home</strong> 리그에서의 성과는 로봇이 가정이라는 비정형 환경에서 인간과 공존하기 위해 필요한 기술적 요건을 보여준다. 부산대학교 “TidyBoy” 팀은 자체 개발한 휴머노이드 로봇 “Anubis“를 통해 홈 서비스 부문에서 우승을 차지했다.3</p>
<p>이들의 승리 요인은 <strong>멀티모달 상호작용(Multimodal Interaction)</strong> 능력의 고도화에 있다. Anubis는 자율 주행과 물체 인식을 넘어, 사용자의 음성 명령을 이해하고 자연스럽게 대화하며 작업을 수행하는 능력을 보여주었다. 이는 시각 정보와 언어 정보를 결합하는 VLM 기술이 서비스 로봇의 필수 요소로 자리 잡았음을 시사한다. 특히 컵을 집어 건네거나, 특정 물건을 찾아오는 미션에서 로봇은 단순한 명령 수행자가 아니라, 상황을 인지하고 사용자에게 되묻거나 행동을 설명하는 ’상호작용적 파트너’로서의 면모를 보였다. 이는 로봇 지능의 평가 기준이 ’작업 정확도’에서 ’상호작용의 질’로 이동하고 있음을 의미한다.</p>
<h3>2.3  표준 플랫폼 리그(SPL)와 기술의 상향 평준화</h3>
<p>독일 브레멘 대학의 <strong>B-Human</strong> 팀은 표준 플랫폼 리그(Standard Platform League, SPL)에서 다시 한번 우승하며 해당 분야의 최강자임을 입증했다.4 모든 팀이 동일한 하드웨어(NAO 로봇)를 사용하는 SPL은 순수하게 소프트웨어 알고리즘의 우열을 가리는 리그이다.</p>
<p>B-Human 팀의 이번 우승은 **3D 인식(3D Perception)**과 <strong>실시간 전술 수립</strong> 기술의 정교함 덕분이었다. 경기장 라인 인식, 공의 궤적 예측, 로봇 간의 위치 추정(Localization) 등에서 딥러닝 기반의 비전 처리 기술이 비약적으로 발전했음을 보여주었다. 또한, 로봇 간의 통신을 통한 협력 플레이(Pass play, Positioning)가 더욱 유기적으로 이루어져, 개별 로봇의 지능뿐만 아니라 군집 지능(Swarm Intelligence) 측면에서도 진보가 있었음을 확인할 수 있다. 이번 대회는 SPL에서 2족 보행 로봇을 사용하는 마지막 대회가 될 가능성이 있어 4, 향후 휴머노이드 리그로의 기술 이전과 통합이 가속화될 것으로 전망된다.</p>
<hr />
<h2>3.  RSS 2025: 이론과 실제의 간극을 메우는 혁신적 시도들</h2>
<p>RSS(Robotics: Science and Systems)는 로봇 공학의 이론적 깊이와 시스템적 완성도를 동시에 요구하는 최고 권위의 학술대회이다. 2025년 7월에 발표된 수상작들은 로봇이 실험실을 벗어나 실제 세계(in-the-wild)에서 마주하는 불확실성과 다양성을 어떻게 극복할 것인가에 대한 해답을 제시하고 있다.</p>
<h3>3.1  개인화된 로봇 시스템의 수학적 모델링: FEAST</h3>
<p>**최우수 논문상(Best Paper Award)**을 수상한 <strong>“FEAST: A Flexible Mealtime-Assistance System Tackling In-the-Wild Personalization”</strong> 5은 식사 보조 로봇 분야에서 기념비적인 연구이다. 이 연구는 장애를 가진 사용자들이 각기 다른 신체적 제약과 선호도를 가지고 있다는 점에 착안하여, 로봇이 사용자와 상호작용하며 스스로를 최적화하는 프레임워크를 제안했다.</p>
<h4>3.1.1  FEAST의 3대 원칙과 시스템 구조</h4>
<p>연구진은 실세계 개인화를 위해 <strong>적응성(Adaptability)</strong>, <strong>투명성(Transparency)</strong>, **안전성(Safety)**이라는 세 가지 핵심 원칙을 제시했다. 시스템은 하드웨어적으로는 다양한 식사 도구를 교체할 수 있는 모듈형 구조를, 소프트웨어적으로는 **매개변수화된 행동 트리(Parameterized Behavior Trees)**와 **LLM 기반 코드 합성(Code Synthesis)**을 결합한 하이브리드 아키텍처를 채택했다.</p>
<h4>3.1.2  베이지안 최적화를 통한 선호도 추론</h4>
<p>FEAST의 기술적 백미는 사용자의 자연어 피드백을 물리적 제어 파라미터로 변환하는 수학적 모델링에 있다. 연구진은 이를 위해 언어 모델과 베이지안 업데이트를 결합한 접근법을 사용했다. 사용자의 발화 <span class="math math-inline">l</span>, 현재 상황 <span class="math math-inline">c</span>, 그리고 주의(Attention) 마스크 <span class="math math-inline">r</span>이 주어졌을 때, 선호도 모델 <span class="math math-inline">LM_{pref}</span>는 파라미터 업데이트에 대한 확신도 <span class="math math-inline">m</span>과 변화량 <span class="math math-inline">\mu</span>를 출력한다.<br />
<span class="math math-display">
(m, \mu) = LM_{pref}(l, c, r)
</span><br />
이때 로봇의 제어 파라미터 <span class="math math-inline">\theta</span>에 대한 사후 확률(Posterior)은 다음과 같은 베이지안 규칙에 의해 갱신된다.<br />
<span class="math math-display">
\log P(\theta \vert \cdot) \propto - \frac{1}{2} (\theta - \theta_t)^T \Lambda_{prior}(\hat{r}_t) (\theta - \theta_t) - \frac{1}{2} (\mu_t - (\theta - \theta_t))^T \Sigma_L(m_t)^{-1} (\mu_t - (\theta - \theta_t))
</span><br />
여기서 <span class="math math-inline">\Lambda_{prior}</span>는 사전 정밀도(Precision) 행렬이며, <span class="math math-inline">\Sigma_L</span>은 언어 모델의 불확실성을 나타내는 공분산 행렬이다. 이 수식은 사용자의 언어적 지시가 모호할 경우(<span class="math math-inline">m</span>이 낮을 경우) 기존 파라미터 <span class="math math-inline">\theta_t</span>를 유지하려는 경향을 강하게 하고, 지시가 명확할 경우(<span class="math math-inline">m</span>이 높을 경우) 적극적으로 파라미터를 수정하도록 유도한다.</p>
<h4>3.1.3  실험적 검증과 사용자 연구</h4>
<p>연구팀은 21명의 돌봄 대상자(Care Recipients)를 대상으로 한 광범위한 사용자 연구를 수행했다.6 실험 결과, FEAST 시스템은 “너무 빨리 먹이지 마”, “숟가락을 좀 더 깊숙이 넣어줘“와 같은 다양한 자연어 명령을 해석하여 로봇의 속도, 궤적, 힘 제어 파라미터를 실시간으로 조절하는 데 성공했다. 이는 로봇이 사전에 프로그래밍된 루틴을 따르는 것을 넘어, 사용자와의 대화를 통해 자신의 행동 정책(Policy)을 동적으로 재구성할 수 있음을 증명한 것이다.</p>
<h3>3.2  시간의 검증을 견딘 연구: RSS Test of Time Award</h3>
<p>RSS 2025는 과거의 연구 중 현재까지 지대한 영향을 미친 논문에 수여하는 <strong>Test of Time Award</strong>의 수상작으로 2009년 발표된 <strong>“Cooperative Manipulation and Transportation with Aerial Robots”</strong> 8를 선정했다. 이 연구는 비행 로봇(드론)들이 케이블을 이용해 협력적으로 물체를 운반하는 제어 이론을 확립한 선구적인 논문이다. 당시에는 생소했던 공중 조작(Aerial Manipulation) 분야를 개척했으며, 현재의 드론 배송, 공중 조립, 다중 로봇 협업 연구의 이론적 토대가 되었다. 이 수상은 로봇 공학에서 물리적 상호작용과 협업 제어의 중요성이 지속적으로 강조되고 있음을 시사한다.</p>
<h3>3.3  최적 제어와 시스템 설계의 진보</h3>
<ul>
<li><strong>우수 학생 논문상 (Best Student Paper Award):</strong> <strong>“Solving Multi-Agent Safe Optimal Control with Distributed Epigraph Form MARL”</strong> 5이 수상했다. 이 논문은 다중 에이전트 시스템에서 충돌 방지와 같은 안전 제약 조건을 만족하면서 최적의 제어 입력을 찾는 문제를 다룬다. 연구진은 문제를 <strong>에피그래프(Epigraph)</strong> 형태로 변환하여 제약 조건을 명시적으로 최적화 과정에 포함시키는 분산 강화 학습 알고리즘을 제안했다. 이는 자율 주행차 군집 주행이나 물류 로봇의 경로 계획 등 안전이 최우선시되는 응용 분야에 즉각적으로 적용 가능한 이론적 기틀을 마련했다.</li>
<li><strong>우수 시스템 논문상 (Best Systems Paper Award):</strong> <strong>“Building Rome with Convex Optimization”</strong> 5은 로봇의 궤적 생성 및 제어 문제를 <strong>볼록 최적화(Convex Optimization)</strong> 문제로 정식화하여, 대규모 환경에서도 빠르고 안정적인 해를 도출할 수 있는 시스템을 구축했다. ’Rome’이라는 이름은 로봇 동작 계획(Motion Planning)의 복잡성을 도시 건설에 비유한 것으로 해석되며, 실시간성이 요구되는 복잡한 로봇 시스템의 제어 아키텍처 설계에 중요한 참조 모델이 될 것이다.</li>
</ul>
<h2>4.  자가 생성 기억과 언어 모델의 그라운딩: 생성형 로봇의 탄생</h2>
<p>ArXiv와 주요 저널을 통해 발표된 연구들 중, 대규모 언어 모델(LLM)과 시각-언어 모델(VLM)을 로봇의 물리적 경험과 결합하려는 시도들이 돋보였다. 이는 AI가 텍스트 세계의 지식을 넘어 물리적 세계의 인과 관계를 이해하도록 만드는 <strong>그라운딩(Grounding)</strong> 문제의 핵심이다.</p>
<h3>4.1  경험을 통한 학습: ExpTeach 프레임워크</h3>
<p>구글 딥마인드와 ETH 취리히 연구진이 발표한 <strong>“Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory”</strong> 9는 로봇이 자신의 경험을 통해 VLM을 스스로 학습시키는 획기적인 방법을 제안했다.</p>
<h4>4.1.1  자가 생성 메모리 시스템 (Self-Generated Memory)</h4>
<p>기존의 VLM은 인터넷상의 텍스트와 이미지로 학습되었기 때문에, 특정 로봇 하드웨어의 물리적 한계나 동작 특성을 알지 못한다. <strong>ExpTeach</strong> 프레임워크는 로봇이 작업을 수행하며 얻은 성공과 실패의 데이터를 **단기 기억(Short-term Memory, STM)**에 저장하고, 이를 요약하여 **장기 기억(Long-term Memory, LTM)**으로 전환하는 구조를 갖는다.</p>
<h4>4.1.2  RAG 기반의 계획 보정 알고리즘</h4>
<p>새로운 작업 지시 <span class="math math-inline">I</span>가 주어졌을 때, 시스템은 <strong>검색 증강 생성(Retrieval-Augmented Generation, RAG)</strong> 기법을 사용하여 장기 기억소 <span class="math math-inline">\mathcal{M}_{LTM}</span>에서 현재 상황과 가장 유사한 과거 경험 <span class="math math-inline">E</span>를 검색한다.<br />
<span class="math math-display">
E = \text{Retrieve}(I, o_t, \mathcal{M}_{LTM})
</span><br />
VLM은 검색된 경험 <span class="math math-inline">E</span>, 현재 지시 <span class="math math-inline">I</span>, 그리고 시각적 관측 <span class="math math-inline">o_t</span>를 종합하여 물리적으로 실행 가능한 행동 <span class="math math-inline">a_t</span>를 생성한다.<br />
<span class="math math-display">
\pi_{grounded}(a_t \vert I, o_t) = \text{VLM}(I, o_t, E)
</span><br />
이 과정에서 로봇은 “지난번에는 이 물체를 꽉 잡았더니 부서졌다“와 같은 구체적인 물리적 경험을 참조하게 된다. 실험 결과, 이 방식은 로봇의 단일 시도 성공률을 22%에서 80%로 비약적으로 향상시켰다.9 특히 주목할 점은 <strong>자율적 성찰(Autonomous Reflection)</strong> 기능이다. 로봇이 실패했을 때, VLM은 실패 원인을 분석하고 이를 메모리에 기록하여 다음 시도에서는 다른 전략을 수립한다. 이는 로봇이 인간의 개입 없이도 스스로 성능을 개선할 수 있는 ’자기 주도 학습’의 가능성을 열어준 것이다.</p>
<h3>4.2  자율 주행에서의 언어와 인식의 결합: NavigScene</h3>
<p>자율 주행 분야에서는 <strong>“NavigScene: Bridging Local Perception and Global Navigation for Beyond-Visual-Range Autonomous Driving”</strong> 11 연구가 주목받았다. 이 연구는 로컬 센서(카메라, 라이다)의 인식 범위 한계를 극복하기 위해 내비게이션 지도 데이터의 전역 정보(Global Context)를 언어 모델에 통합하는 방법을 제안했다.</p>
<h4>4.2.1  내비게이션 유도 선호 최적화 (NPO)</h4>
<p>연구진은 VLM이 내비게이션 지침을 더 잘 따르도록 학습시키기 위해 **내비게이션 유도 선호 최적화(Navigation-guided Preference Optimization, NPO)**라는 새로운 강화 학습 기법을 개발했다. 기존의 DPO(Direct Preference Optimization)를 확장하여, 생성된 요약 정보 <span class="math math-inline">s</span>와 내비게이션 지침 <span class="math math-inline">g</span> 사이의 **상호정보량(Mutual Information)**을 보상 함수에 추가했다.</p>
<p>보상 함수 <span class="math math-inline">r_s</span>는 다음과 같이 정의된다.<br />
<span class="math math-display">
r_s = \log p_\theta(s_\theta \vert v, q) - \log p^*(s^* \vert v, q) + \alpha \cdot \text{MI}(s_\theta, g)
</span><br />
여기서 <span class="math math-inline">\text{MI}(s, g)</span>는 요약 <span class="math math-inline">s</span>가 내비게이션 지침 <span class="math math-inline">g</span>의 정보를 얼마나 잘 보존하고 있는지를 나타낸다. 이를 근사적으로 계산하기 위해 연구진은 다음과 같은 수식을 사용했다.<br />
<span class="math math-display">
\text{MI}(s, g) \approx -\log p(s) - p(s) \log p(g \vert s)
</span><br />
이러한 목적 함수 설계를 통해 모델은 “150m 앞 교차로에서 우회전“과 같은 글로벌 지침을 현재의 시각 정보와 결합하여, 미리 차선을 변경하거나 속도를 줄이는 등의 장기적인 계획(Long-horizon Planning)을 수립할 수 있게 되었다.12</p>
<h2>5.  데이터 중심 로봇 공학: 양보다 질, 그리고 다양성의 함정</h2>
<p>로봇 학습에서 데이터의 중요성은 아무리 강조해도 지나치지 않다. 그러나 7월에 발표된 연구들은 무조건적인 데이터의 양적 팽창보다, 데이터의 질적 관리와 분포의 특성이 더 중요함을 역설하고 있다.</p>
<h3>5.1  데이터 다양성에 대한 재고찰: “Is Diversity All You Need?”</h3>
<p><strong>“Is Diversity All You Need for Scalable Robotic Manipulation?”</strong> 13 논문은 로봇 조작 데이터셋 구축에 대한 통념을 깨는 중요한 발견을 담고 있다. 연구진은 다양한 환경, 물체, 조명 조건 등을 포함하는 데이터셋이 일반화 성능을 높인다는 기존의 믿음이 항상 참은 아니며, 특정 유형의 다양성은 오히려 학습을 방해하는 **교란 변수(Confounder)**로 작용할 수 있음을 지적했다.</p>
<h4>5.1.1  속도 분포 편향(Velocity Distribution Bias)과 해결책</h4>
<p>연구진이 발견한 가장 치명적인 문제는 **속도 다양성(Velocity Diversity)**이었다. 인간 시연자마다 작업을 수행하는 속도가 다르기 때문에, 로봇이 동일한 작업에 대해 서로 다른 속도 프로파일을 학습하게 되면 행동 생성 모델이 혼란을 겪게 된다. 이를 해결하기 위해 연구진은 <strong>속도 분포 편향 제거(Velocity Distribution Debiasing)</strong> 기법을 제안했다.</p>
<p>이 기법은 행동 궤적의 속도 프로파일을 정규화하는 손실 함수를 도입하여 모델이 불필요한 속도 변동성을 무시하고 작업의 본질적인 궤적(Trajectory)에 집중하도록 유도한다.<br />
<span class="math math-display">
</span>\mathcal{L}<em>{VM} = \mathbb{E}</em>{(\mathbf{o}<em>t, \mathbf{a}</em>{t:t+T}) \sim \mathcal{D}} \left[ | \text{VM}(\mathbf{o}<em>t) - v(\mathbf{a}</em>{t:t+T}) |_2^2 \right]<br />
$$<br />
여기서 <span class="math math-inline">\text{VM}(\mathbf{o}_t)</span>는 관측 상태에서 예측된 이상적인 속도 모델이며, <span class="math math-inline">v(\mathbf{a}_{t:t+T})</span>는 실제 행동 궤적의 속도이다. 이 손실 함수를 최소화함으로써, 모델은 관측 상태 <span class="math math-inline">\mathbf{o}_t</span>가 유사할 때 일관된 속도로 행동을 생성하도록 학습된다. 실험 결과, 이 처리를 거친 데이터셋으로 학습된 모델은 조작 성공률이 유의미하게 향상되었다.14 이는 로봇 데이터셋 구축 시 ’무작위적인 다양성’보다는 ’통제된 다양성’이 중요함을 시사한다.</p>
<h3>5.2 변형 물체 조작을 위한 전신 제어 정책</h3>
<p><strong>“Deformable Cluster Manipulation via Whole-Arm Policy Learning”</strong> 15 연구는 나뭇가지나 케이블 뭉치와 같이 형태가 일정하지 않은 변형 물체 군집(Cluster)을 다루는 방법을 다루었다.</p>
<h4>5.2.1 커널 평균 임베딩을 이용한 상태 표현</h4>
<p>변형 물체는 자유도가 무한대에 가깝기 때문에 상태를 정확히 표현하기 어렵다. 연구진은 포인트 클라우드 <span class="math math-inline">P</span>를 **재생 커널 힐베르트 공간(RKHS)**으로 매핑한 후, 그 평균을 취하는 <strong>커널 평균 임베딩(Kernel Mean Embedding)</strong> 방식을 사용하여 상태를 표현했다.<br />
<span class="math math-display">
\mu_P = \frac{1}{|P|} \sum_{x \in P} \phi(x)
</span><br />
여기서 <span class="math math-inline">\phi(x)</span>는 커널 함수(예: 가우시안 커널)에 의한 특징 매핑이다. 이 방식은 포인트 클라우드의 순서 불변성(Permutation Invariance)을 보장하고 노이즈에 강인하며, 무엇보다 고차원 형상 정보를 저차원 벡터로 효율적으로 압축할 수 있다.</p>
<h4>5.2.2 전신 제어(Whole-Arm Manipulation) 정책</h4>
<p>연구진은 로봇의 손끝(End-effector)만을 사용하는 기존 방식에서 벗어나, 팔 전체를 사용하여 물체를 밀어내거나 지지하는 정책을 학습시켰다. 이는 덩굴을 헤치고 나가는 동물이나 덤불 속에서 작업을 수행하는 인간의 동작에서 영감을 받은 것으로, 복잡한 비정형 환경에서의 로봇 조작 능력을 한 단계 끌어올린 것으로 평가된다.</p>
<h2>6. 산업계 동향: 하드웨어의 진화와 상용화의 가속</h2>
<p>학계가 소프트웨어와 알고리즘의 혁신을 주도하는 동안, 산업계는 이를 물리적으로 구현할 하드웨어 플랫폼의 완성도를 높이는 데 주력했다. 2025년 7월은 주요 휴머노이드 로봇 기업들이 상용화를 위한 구체적인 스펙과 로드맵을 공개한 시기이다.</p>
<h3>6.1 Figure AI: F.03 배터리와 구조 일체형 설계</h3>
<p>휴머노이드 로봇 스타트업인 Figure AI는 7월 17일, 차세대 모델인 <strong>Figure 03</strong>에 탑재될 배터리 기술을 상세히 공개했다.17</p>
<ul>
<li><strong>구조 일체형 배터리(Structural Battery):</strong> 가장 큰 혁신은 배터리 팩을 로봇의 몸통(Torso) 구조체 자체로 활용했다는 점이다. 이는 전기차(EV) 산업의 ‘Cell-to-Chassis’ 기술을 로봇에 적용한 것으로, 별도의 배터리 공간을 줄이면서도 기구적 강성을 확보하는 이중 효과를 거두었다.</li>
<li><strong>성능 지표:</strong> 에너지 밀도를 전작 대비 94% 향상시켰으며, 2.3 kWh 용량의 배터리 팩을 통해 로봇이 5시간 동안 연속으로 작업을 수행할 수 있게 되었다.18 이는 물류 창고나 공장과 같은 현장에서 로봇이 인간의 작업 시프트(Shift)를 소화할 수 있는 최소한의 요건을 충족시켰음을 의미한다.</li>
</ul>
<h3>6.2 Tesla Optimus: 정교한 조작과 텔레오퍼레이션 논쟁</h3>
<p>테슬라(Tesla)는 7월 23일, 헐리우드에 위치한 테슬라 다이너(Tesla Diner)에서 <strong>Optimus</strong> 로봇이 실제 고객에게 팝콘을 서빙하는 시연을 공개했다.19</p>
<ul>
<li><strong>기술적 진보:</strong> 이 시연에서 옵티머스는 부서지기 쉬운 팝콘 봉지를 적절한 힘으로 집어(Grasping), 사람에게 건네는 섬세한 손동작(Dexterous Manipulation)을 보여주었다. 이는 비전 기반의 엔드-투-엔드 제어망이 미세한 힘 제어(Force Control) 영역까지 확장되었음을 보여준다.</li>
<li><strong>텔레오퍼레이션 논란과 의미:</strong> 일각에서는 이 시연의 일부가 원격 조종(Teleoperation)에 의해 이루어졌다는 의혹과 논란이 제기되었다.19 그러나 로봇 학습의 관점에서 볼 때, 고품질의 텔레오퍼레이션 데이터는 모방 학습(Imitation Learning)의 핵심 자원이다. 테슬라는 이러한 실증 테스트를 통해 양질의 인간 행동 데이터를 수집하고, 이를 다시 모델 학습에 사용하는 ‘데이터 플라이휠(Data Flywheel)’ 전략을 구사하고 있는 것으로 분석된다.</li>
</ul>
<h3>6.3 3D 프린팅 건축 로봇의 상용화: Lib Work</h3>
<p>일본의 주택 건설 업체 Lib Work는 7월 말, 흙(Earth)을 주재료로 하는 3D 프린팅 주택을 완성했다고 발표했다.21</p>
<ul>
<li><strong>Crane WASP 시스템:</strong> 이 프로젝트에는 이탈리아 WASP사의 대형 델타 로봇(Crane WASP)이 사용되었다. 이 로봇은 흙, 석회, 천연 섬유를 혼합한 재료를 정밀하게 적층하여 주택의 골조를 3일 만에 완성했다.</li>
<li><strong>의의:</strong> 이는 로봇 공학이 공장 자동화를 넘어 건설 현장과 같은 비정형 아웃도어 환경으로 확장되고 있음을 보여주는 사례이다. 특히 지속 가능한 건축 자재와 로봇 정밀 제어의 결합은 탄소 배출 절감과 주택 공급 문제 해결에 기여할 것으로 기대된다.</li>
</ul>
<h2>7. 주요 저널 연구 동향 (Science Robotics, IEEE T-RO)</h2>
<p><strong>Science Robotics</strong>와 **IEEE Transactions on Robotics (T-RO)**의 2025년 7월 호에는 로봇 공학의 기초 체력을 다지는 연구들이 다수 게재되었다.</p>
<ul>
<li><strong>생체 모방 로봇 (Biomimetics):</strong> Science Robotics Vol. 10 이슈에서는 자연계의 메커니즘을 모방한 소프트 로봇과 마이크로 로봇 연구가 주를 이루었다.23 특히 곤충의 비행 원리나 물고기의 유영 원리를 적용하여 에너지 효율을 극대화한 로봇 디자인들이 소개되었다. 이는 배터리 기술의 한계를 기구학적 효율성으로 극복하려는 시도이다.</li>
<li><strong>의료 및 재활 로봇:</strong> IEEE T-RO에는 뇌졸중 환자의 보행 재활을 위한 엑소스켈레톤(Exoskeleton)의 개인화 제어 알고리즘에 관한 연구가 실렸다.25 이 연구는 환자의 보행 의도를 실시간으로 추정하고, 이에 맞춰 보조 토크를 조절하는 적응형 제어기(Adaptive Controller)를 제안했다. 앞서 언급한 FEAST 논문과 마찬가지로, 여기서도 ’사용자 맞춤형 적응’이 핵심 키워드임을 확인할 수 있다.</li>
</ul>
<h2>8. 종합 분석 및 결론: ’ChatGPT 모멘텀’을 맞이한 로봇 공학</h2>
<p>2025년 7월의 로봇 공학 연구 동향을 종합해보면, 이 분야는 바야흐로 **“물리적 AI의 ChatGPT 모멘텀”**을 맞이하고 있다고 평가할 수 있다. 텍스트 생성에서 시작된 거대 모델의 혁명이 로봇의 제어(Control)와 인지(Perception) 영역으로 전이되면서, 로봇은 과거의 경직된 자동화 기계에서 유연하고 지능적인 존재로 탈바꿈하고 있다.</p>
<h3>8.1 핵심 트렌드 요약</h3>
<ol>
<li><strong>확산 모델의 제어기화:</strong> SoccerDiffusion 연구에서 보듯, 확산 모델은 복잡하고 다봉적인(Multi-modal) 행동 분포를 표현하는 데 탁월한 성능을 발휘하며 차세대 로봇 제어기의 표준으로 자리 잡고 있다.</li>
<li><strong>기억과 성찰의 등장:</strong> ExpTeach 연구는 로봇이 단순한 실행자가 아니라, 자신의 행동을 기억하고 반성하며 성장하는 학습 주체가 될 수 있음을 보여주었다.</li>
<li><strong>개인화의 수학적 정립:</strong> FEAST 연구는 모호한 인간의 언어를 명확한 수학적 제어 변수로 변환하는 프레임워크를 제시함으로써, 인간-로봇 상호작용(HRI)의 수준을 한 단계 높였다.</li>
<li><strong>하드웨어와 소프트웨어의 동기화:</strong> Figure AI와 테슬라의 행보는 고도화된 AI 알고리즘을 뒷받침할 수 있는 하드웨어 플랫폼이 완성 단계에 진입했음을 알린다.</li>
</ol>
<h3>8.2 향후 전망 및 시사점</h3>
<p>향후 로봇 공학의 발전은 **“데이터의 질적 고도화”**와 **“온디바이스(On-device) AI의 효율화”**에 달려 있을 것이다. “Is Diversity All You Need?” 논문이 지적했듯, 무분별한 데이터 수집보다는 정제된 데이터 큐레이션이 중요해질 것이며, SoccerDiffusion의 증류 기법처럼 거대 모델을 소형 로봇 프로세서에 탑재하기 위한 경량화 기술 경쟁이 치열해질 것이다.</p>
<p>결론적으로, 2025년 7월은 로봇이 실험실의 통제된 환경을 벗어나 **“야생(In-the-wild)”**으로 나아가기 위한 지능적, 신체적 준비를 마친 시점으로 기록될 것이다. 타임지가 2025년 올해의 인물로 특정 개인이 아닌 “AI의 설계자들(Architects of AI)“을 선정한 것 26은 이러한 시대적 흐름을 상징적으로 보여준다. 이제 우리는 로봇이 우리의 일상 공간에서 함께 걷고, 일하고, 소통하는 진정한 공존의 시대를 목전에 두고 있다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>SoccerDiffusion: Toward Learning End-to-End Humanoid Robot …, 12월 14, 2025에 액세스, https://arxiv.org/pdf/2504.20808</li>
<li>SoccerDiffusion: Toward Learning End-to-End Humanoid Robot …, 12월 14, 2025에 액세스, https://www.researchgate.net/publication/391282780_SoccerDiffusion_Toward_Learning_End-to-End_Humanoid_Robot_Soccer_from_Gameplay_Recordings</li>
<li>Pusan National University Robotics Team Wins First Place in …, 12월 14, 2025에 액세스, https://www.busan.go.kr/eng/ai-translated-press-releases/1693748</li>
<li>Bremen Is World Champion: B-Human Wins 2025 RoboCup World …, 12월 14, 2025에 액세스, https://www.uni-bremen.de/en/fb3/the-faculty/organization/news/bremen-ist-wieder-weltmeister-b-human-gewinnt-die-robocup-wm-2025-1</li>
<li>Awards - Robotics: Science and Systems, 12월 14, 2025에 액세스, https://roboticsconference.org/2025/program/awards/</li>
<li>FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild …, 12월 14, 2025에 액세스, https://www.roboticsproceedings.org/rss21/p083.pdf</li>
<li>FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild …, 12월 14, 2025에 액세스, https://www.researchgate.net/publication/392839048_FEAST_A_Flexible_Mealtime-Assistance_System_Towards_In-the-Wild_Personalization</li>
<li>Test of Time Award - Robotics: Science and Systems, 12월 14, 2025에 액세스, https://roboticsconference.org/program/testoftimeaward/</li>
<li>Grounding VLMs for Robotics through Self-Generated Memory - arXiv, 12월 14, 2025에 액세스, https://arxiv.org/abs/2507.16713</li>
<li>Grounding VLMs for Robotics through Self-Generated Memory - arXiv, 12월 14, 2025에 액세스, https://arxiv.org/html/2507.16713v1</li>
<li>NavigScene: Bridging Local Perception and Global Navigation for …, 12월 14, 2025에 액세스, https://arxiv.org/abs/2507.05227</li>
<li>NavigScene: Bridging Local Perception and Global Navigation for …, 12월 14, 2025에 액세스, https://arxiv.org/html/2507.05227v1</li>
<li>Is Diversity All You Need for Scalable Robotic Manipulation? - arXiv, 12월 14, 2025에 액세스, https://arxiv.org/abs/2507.06219</li>
<li>Is Diversity All You Need for Scalable Robotic Manipulation? - arXiv, 12월 14, 2025에 액세스, https://arxiv.org/html/2507.06219v1</li>
<li>Deformable Cluster Manipulation via Whole-Arm Policy Learning, 12월 14, 2025에 액세스, https://arxiv.org/html/2507.17085v1</li>
<li>Deformable Cluster Manipulation via Whole-Arm Policy Learning, 12월 14, 2025에 액세스, https://arxiv.org/abs/2507.17085</li>
<li>News - Figure AI, 12월 14, 2025에 액세스, https://www.figure.ai/news</li>
<li>F.03 Battery Development - Figure AI, 12월 14, 2025에 액세스, https://www.figure.ai/news/f-03-battery-development</li>
<li>Elon Musk’s Tesla Optimus humanoid robot serving popcorn goes viral, 12월 14, 2025에 액세스, https://timesofindia.indiatimes.com/technology/tech-news/elon-musks-tesla-optimus-humanoid-robot-serving-popcorn-goes-viral-says-this-will-become-normal-in-a-few-years/articleshow/122810959.cms</li>
<li>Optimus in Action: Tesla’s Humanoid Robot Debuts at Drive-In, 12월 14, 2025에 액세스, https://investorplace.com/hypergrowthinvesting/2025/07/optimus-in-action-teslas-humanoid-robot-debuts-at-drive-in/</li>
<li>Lib Work Completes Japan’s First 3D Printed Earth House with …, 12월 14, 2025에 액세스, https://3dprintingindustry.com/news/lib-work-completes-japans-first-3d-printed-earth-house-with-wasp-crane-system-244015/</li>
<li>3D Printed House Built in Japan Using Soil in Just 3 Days - Picobricks, 12월 14, 2025에 액세스, https://picobricks.com/blogs/bricksnews/a-3d-printed-house-built-in-just-3-days-in-japan</li>
<li>Science Robotics, 12월 14, 2025에 액세스, https://promo.aaas.org/images/Publishing/Journals/2019/Robotics/ROB19_Booklet.pdf</li>
<li>Biomimetics, Volume 10, Issue 7 (July 2025) – 67 articles, 12월 14, 2025에 액세스, https://www.mdpi.com/2313-7673/10/7</li>
<li>Journal Papers - EPIC Lab at Georgia Tech, 12월 14, 2025에 액세스, https://www.epic.gatech.edu/journal-papers/</li>
<li>Why TIME’s 2025 Person of the Year is not a person, 12월 14, 2025에 액세스, https://timesofindia.indiatimes.com/etimes/trending/why-times-person-of-the-year-2025-is-not-a-person-elon-musk-mark-zuckerberg-sam-altman/articleshow/125923611.cms</li>
<li>News - ais.uni-bonn.de, 12월 14, 2025에 액세스, https://www.ais.uni-bonn.de/news.html</li>
<li>Symposium – RoboCup 2025 – Salvador, Brazil, 12월 14, 2025에 액세스, https://2025.robocup.org/symposium/</li>
<li>Bridging Local Perception and Global Navigation for Beyond-Visual …, 12월 14, 2025에 액세스, https://www.researchgate.net/publication/393478514_NavigScene_Bridging_Local_Perception_and_Global_Navigation_for_Beyond-Visual-Range_Autonomous_Driving</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>