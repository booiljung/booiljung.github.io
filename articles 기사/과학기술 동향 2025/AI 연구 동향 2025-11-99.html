<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2025년 11월 AI 연구 현황</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2025년 11월 AI 연구 현황</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2025년 AI 및 로봇 연구 동향</a> / <span>2025년 11월 AI 연구 현황</span></nav>
                </div>
            </header>
            <article>
                <h1>2025년 11월 AI 연구 현황</h1>
<h2>1.  서론: 시스템 2적 사고와 영속적 에이전트의 도래</h2>
<p>2025년 11월은 인공지능(AI) 기술의 역사적 흐름 속에서 단순한 성능 향상의 연속선상이 아닌, 질적인 위상 전이(Phase Transition)가 발생한 시점으로 기록될 것이다. 지난 몇 년간 대규모 언어 모델(LLM)의 발전이 파라미터(Parameter)의 크기를 키우고 학습 데이터를 무한히 확장하는 ’스케일링 법칙(Scaling Laws)’에 의존해왔다면, 이번 달에 발표된 주요 연구와 프론티어 모델들은 모델의 ’사고 과정(Process)’과 ‘기억 관리(Memory Management)’, 그리고 ’아키텍처 효율성(Architectural Efficiency)’에 집중하는 새로운 패러다임을 제시한다.</p>
<p>구글(Google)의 <strong>Gemini 3</strong>와 OpenAI의 <strong>GPT-5.1</strong>은 이제 단순히 사용자의 질문에 즉답하는 확률론적 앵무새가 아니라, 복잡한 문제를 해결하기 위해 내부적으로 사고의 연쇄를 구성하고 검증하는 ’시스템 2(System 2)’적 추론 능력을 핵심 기능으로 탑재했다.1 더불어, <strong>DeepSeek-V3.2</strong>는 어텐션 메커니즘(Attention Mechanism)의 근본적인 비효율성을 수학적으로 해결하려는 시도를 통해 오픈 소스 진영의 기술적 깊이를 증명했으며 2, Meta의 <strong>SAM 3</strong>는 컴퓨터 비전 분야에서 고질적인 문제였던 환각(Hallucination) 현상을 ’Presence Head’라는 새로운 모듈로 제어하며 인식과 위치 추정의 딜레마를 해결했다.3</p>
<p>학술적으로는 <strong>EMNLP 2025</strong>에서 발표된 연구들이 LLM의 메타 언어적 추론 능력을 검증하며, AI가 언어학자의 전문적인 분석 과정을 모방하거나 보조할 수 있는 가능성을 열었다.4 이와 동시에 미국 정부의 ’Genesis Mission’과 같은 대규모 국가 주도 프로젝트는 AI 기술이 기업 간 경쟁을 넘어 국가 안보와 과학 기술 패권의 핵심축으로 격상되었음을 시사한다.5</p>
<p>본 보고서는 2025년 11월에 쏟아진 이러한 기술적 성취들을 심층적으로 분석한다. 각 기술의 작동 원리를 수식과 아키텍처 다이어그램 수준에서 해부하고, 이것이 향후 인공지능 생태계와 산업 전반에 미칠 파급력을 포괄적으로 논의한다. 특히 모델이 어떻게 ’생각’하고, ’기억’하며, ’보는가’에 대한 근본적인 메커니즘의 변화를 추적함으로써, 다가오는 2026년의 기술 지형도를 조망하는 것을 목표로 한다.</p>
<hr />
<h2>2.  프론티어 모델의 진화: 심층 추론과 생성형 인터페이스</h2>
<p>2025년 11월의 가장 큰 이슈는 구글과 OpenAI라는 양대 산맥이 선보인 프래그십 모델들의 진화였다. 두 기업 모두 모델이 단순히 텍스트를 생성하는 것을 넘어, 자율적으로 도구를 사용하고, 장기적인 계획을 수립하며, 사용자에게 맞춤형 인터페이스를 제공하는 ’에이전트(Agent)’로서의 정체성을 강화했다.</p>
<h3>2.1  Google Gemini 3: 멀티모달 지능의 정점과 Generative UI</h3>
<p>구글은 자사의 최상위 파운데이션 모델인 <strong>Gemini 3</strong>를 공개하며 전 세계 AI 커뮤니티에 충격을 안겼다. 이 모델은 공개 직후 <strong>LMArena</strong> 벤치마크에서 1501점이라는 전무후무한 기록을 세우며, 기존 모델들이 넘지 못했던 성능의 임계점을 돌파했다.1 Gemini 3의 기술적 혁신은 크게 세 가지 축으로 요약된다. 첫째는 ‘Deep Think’ 모드를 통한 추론 능력의 강화, 둘째는 ’Generative UI’를 통한 상호작용의 혁신, 셋째는 자체 진화 알고리즘인 ’AlphaEvolve’의 적용이다.</p>
<h4>2.1.1  Deep Think 모드와 추론의 깊이</h4>
<p>Gemini 3에 탑재된 ‘Deep Think’ 모드는 모델이 답변을 생성하기 전에 충분한 연산 시간(Compute Time)을 할애하여 문제 공간을 탐색하는 기능이다. 이는 인간의 인지 과정에서 직관적이고 빠른 사고인 ’시스템 1’과 논리적이고 느린 사고인 ’시스템 2’의 구분을 AI 아키텍처에 구현한 것이다. 기존의 LLM이 입력 토큰에 대해 즉각적인 다음 토큰을 예측하는 데 집중했다면, Deep Think 모드는 내부적인 ’생각의 고리(Chain of Thought)’를 명시적으로 전개하고, 이를 자체적으로 비판 및 수정(Self-Correction)하는 과정을 거친다.</p>
<p>이러한 접근 방식의 성과는 벤치마크 점수에서 명확히 드러난다. 박사급 전문가 수준의 지식을 요구하는 <strong>GPQA Diamond</strong> 벤치마크에서 Gemini 3는 93.8%라는 압도적인 정답률을 기록했다.1 이는 해당 분야의 인간 전문가 평균을 상회하는 수치로, AI가 전문적인 과학 연구나 고도의 지적 노동을 보조하는 것을 넘어 주도할 수 있는 수준에 도달했음을 의미한다. 또한, 인류가 직면한 난제들을 모아놓은 <strong>Humanity’s Last Exam</strong> 벤치마크에서도 41.0%를 기록하며, 기존 모델들이 0~10%대에 머물렀던 한계를 뛰어넘었다.1</p>
<h4>2.1.2  Generative UI: 인터페이스의 동적 생성</h4>
<p>Gemini 3의 가장 파괴적인 혁신은 **Generative UI (생성형 사용자 인터페이스)**의 도입이다. 지금까지의 LLM은 사용자의 요청에 대해 텍스트, 코드, 이미지와 같은 정적인 콘텐츠만을 반환했다. 그러나 Gemini 3는 사용자의 의도를 파악하고, 그에 가장 적합한 형태의 ‘소프트웨어 인터페이스’ 자체를 실시간으로 코딩하고 렌더링하여 제공한다.1</p>
<p>예를 들어, 사용자가 “서울의 지난 10년간 기온 변화와 전력 소비량의 상관관계를 보여줘“라고 요청하면, Gemini 3는 단순한 텍스트 설명이나 정적 이미지를 넘어, 사용자가 직접 데이터를 필터링하고 구간을 확대/축소할 수 있는 대화형 차트와 슬라이더가 포함된 웹 애플리케이션을 즉석에서 생성한다. 이는 백엔드 데이터 처리 능력과 프론트엔드 UI 설계 능력이 하나의 모델 안에서 완벽하게 통합되었음을 의미한다. 이 기술은 향후 SaaS(Software as a Service) 시장의 판도를 완전히 바꿀 잠재력을 지니고 있다. 사용자는 더 이상 복잡한 대시보드 사용법을 익힐 필요가 없으며, 필요한 도구를 말로 설명하기만 하면 AI가 그 자리에서 도구를 만들어주는 시대가 열린 것이다.</p>
<h4>2.1.3  AlphaEvolve: 재귀적 자기 개선</h4>
<p>Gemini 3의 성능 향상 이면에는 <strong>AlphaEvolve</strong>라는 기술이 자리 잡고 있다. 구글 딥마인드(Google DeepMind)는 Gemini 모델의 훈련 효율성을 높이기 위해 진화 알고리즘(Evolutionary Algorithm) 기반의 코딩 에이전트인 AlphaEvolve를 활용했다.6</p>
<p>AlphaEvolve는 모델 학습에 필요한 GPU 커널(Kernel) 코드를 스스로 작성하고 최적화한다. 일반적으로 GPU 커널 최적화는 최고의 전문성을 가진 인간 엔지니어들이 수주에서 수개월간 매달려야 하는 난이도 높은 작업이다. 그러나 AlphaEvolve는 수많은 변형 코드를 생성하고, 이를 시뮬레이션하여 성능을 평가한 뒤, 가장 우수한 코드를 선택하여 다시 변형하는 진화적 과정을 통해 인간 엔지니어보다 효율적인 코드를 찾아냈다.</p>
<p>보고된 바에 따르면, AlphaEvolve는 트랜스포머 모델의 핵심 연산인 <strong>FlashAttention</strong> 커널 구현에서 최대 32.5%의 속도 향상을 달성했다.6 이는 단순히 학습 속도가 빨라진 것을 넘어, AI가 자신의 기반이 되는 하드웨어 운용 코드를 스스로 개선하는 ’재귀적 자기 개선(Recursive Self-Improvement)’의 초기 단계를 보여준다는 점에서 AI 안전성(Safety) 연구 및 싱귤래리티(Singularity) 논의에 중요한 화두를 던진다.</p>
<h3>2.2  OpenAI GPT-5.1 및 Codex-Max: 영속적 에이전트의 탄생</h3>
<p>구글의 공세에 맞서 OpenAI는 <strong>GPT-5.1</strong>과 그 파생 모델인 <strong>GPT-5.1-Codex-Max</strong>를 출시하며, AI의 ’작업 지속성(Persistence)’과 ’엔지니어링 능력’에 초점을 맞췄다. 이 모델들은 단순히 똑똑한 대화 상대를 넘어, 24시간 동안 쉬지 않고 복잡한 코딩 프로젝트를 수행하는 자율 에이전트를 지향한다.7</p>
<h4>2.2.1  Compaction 메커니즘: 유한한 윈도우, 무한한 기억</h4>
<p>GPT-5.1-Codex-Max의 핵심 기술은 <strong>Compaction (압축)</strong> 메커니즘이다.7 기존의 LLM은 컨텍스트 윈도우(Context Window)의 크기 제한으로 인해 대화가 길어지면 앞부분의 내용을 잊어버리거나(Catastrophic Forgetting), 긴 문맥을 처리하느라 연산 비용이 기하급수적으로 증가하는 문제를 안고 있었다. RAG(Retrieval-Augmented Generation)와 같은 외부 메모리 기법이 대안으로 사용되었지만, 이는 모델의 단기 기억(Working Memory)과는 질적으로 달라 복잡한 추론 상태를 유지하는 데 한계가 있었다.</p>
<p>Compaction은 이러한 문제를 해결하기 위해 모델의 내부 상태를 효율적으로 압축하여 다음 컨텍스트로 넘기는 기술이다. 모델이 작업을 수행하다가 컨텍스트 한계에 도달하면, <code>/compact</code> 엔드포인트를 통해 현재까지의 작업 내용, 아키텍처 결정 사항, 변수 상태 등 핵심 정보를 ‘encrypted content items’ 형태로 변환한다.10 이는 단순한 텍스트 요약(Summarization)과는 다르다. 텍스트 요약은 정보의 손실(Lossy Compression)이 크고 모델의 미묘한 추론 맥락을 담기 어렵지만, Compaction은 모델의 임베딩 공간(Embedding Space) 정보를 최대한 보존하는 형태로 압축을 수행하는 것으로 추정된다.</p>
<p>이 기술 덕분에 GPT-5.1-Codex-Max는 수백만 토큰에 달하는 대규모 리팩토링 작업을 며칠에 걸쳐 수행하면서도, 프로젝트 초기에 설정한 규칙이나 의존성 관계를 잊어버리지 않는다. 이는 AI 에이전트가 ’세션(Session)’의 제약에서 벗어나 ’영속적(Persistent)’인 존재로 거듭나게 하는 핵심 기술이다.</p>
<h4>2.2.2  SWE-bench를 통한 엔지니어링 역량 검증</h4>
<p>GPT-5.1 시리즈의 코딩 능력은 <strong>SWE-bench</strong> 결과를 통해 입증되었다. 실제 GitHub 이슈를 해결하는 능력을 평가하는 이 벤치마크에서 <strong>GPT-5.1-Codex-Max</strong>는 77.9%의 해결률을 기록했으며, 추론 능력이 강화된 <strong>GPT-5.2 Thinking</strong> 모델은 80%를 달성했다.11 이는 Claude 3.5 Sonnet이나 초기 GPT-4 모델들이 20~30%대에 머물렀던 것과 비교하면 비약적인 발전이다.</p>
<p>특히 주목할 점은 <strong>SWE-Bench Pro</strong>에서의 성과이다. 파이썬(Python) 언어에 국한된 Verified 버전과 달리, Pro 버전은 4가지 프로그래밍 언어를 포함하고 더 복잡한 다중 파일 환경을 다룬다. 여기서 GPT-5.2 Thinking은 55.6%를 기록하며, 단순한 코드 생성이 아닌 전체 소프트웨어 아키텍처를 이해하고 수정하는 능력을 과시했다.11 또한, 이 모델들은 리눅스 기반 환경뿐만 아니라 Windows 환경에서의 작업 능력도 훈련받아, 실제 기업 현장의 다양한 개발 환경에 즉시 투입될 수 있는 범용성을 갖추었다.7</p>
<h2>3.  효율성의 혁명: DeepSeek-V3.2와 희소 어텐션</h2>
<p>상용 모델들이 거대 자본을 바탕으로 성능의 극한을 추구하는 동안, 오픈 소스 진영에서는 아키텍처의 효율성을 극대화하여 ’AI의 민주화’를 앞당기는 연구가 진행되었다. 그 중심에는 중국의 DeepSeek가 발표한 <strong>DeepSeek-V3.2</strong>가 있다. 이 모델은 **DeepSeek Sparse Attention (DSA)**이라는 혁신적인 메커니즘을 통해, 성능 저하 없이 추론 비용을 획기적으로 낮추는 데 성공했다.2</p>
<h3>3.1  어텐션의 딜레마와 DSA의 해법</h3>
<p>트랜스포머(Transformer) 아키텍처의 핵심인 어텐션 메커니즘은 입력 시퀀스 길이 <span class="math math-inline">L</span>에 대해 <span class="math math-inline">O(L^2)</span>의 연산 복잡도와 메모리 사용량을 가진다. 이는 문맥 길이가 길어질수록 비용이 기하급수적으로 증가함을 의미하며, 긴 문맥을 처리하는 에이전트 모델의 상용화에 가장 큰 걸림돌이었다.</p>
<p>DeepSeek-V3.2는 이 문제를 해결하기 위해 <span class="math math-inline">O(kL)</span>의 선형 복잡도에 가까운 **DSA (DeepSeek Sparse Attention)**를 도입했다.13 DSA의 핵심 아이디어는 “모든 토큰이 과거의 모든 토큰을 볼 필요는 없다“는 것이다. 대신, 현재 토큰과 의미적으로 관련이 깊은 상위 <span class="math math-inline">k</span>개의 토큰만을 선별하여 어텐션을 수행한다.</p>
<h4>3.1.1  Lightning Indexer와 중요도 점수 (Importance Score)</h4>
<p>효율적인 토큰 선별을 위해서는 선별 과정 자체가 가벼워야 한다. DeepSeek는 이를 위해 <strong>Lightning Indexer</strong>라는 경량화 모듈을 고안했다.14 Lightning Indexer는 현재 토큰 <span class="math math-inline">t</span>와 과거 토큰 <span class="math math-inline">s</span> 사이의 관련성을 나타내는 중요도 점수(Importance Score) <span class="math math-inline">I_{t,s}</span>를 계산한다.</p>
<p>Lightning Indexer의 계산 효율성을 위해 DeepSeek는 복잡한 소프트맥스(Softmax) 대신 ReLU 활성화 함수와 저정밀도(FP8) 연산을 사용한다. 중요도 점수의 계산식은 다음과 같이 표현된다:</p>
<p><span class="math math-display">
I_{t,s} = \text{ReLU}(q_t^I \cdot k_s^I)
</span><br />
여기서 <span class="math math-inline">q_t^I</span>와 <span class="math math-inline">k_s^I</span>는 인덱싱을 위해 별도로 학습된 쿼리(Query)와 키(Key) 벡터이다. 이 벡터들은 원래의 어텐션 쿼리/키보다 차원이 낮아 연산량이 적다. 모델은 이 <span class="math math-inline">I_{t,s}</span> 값을 바탕으로 가장 중요한 상위 <span class="math math-inline">k</span>개의 과거 토큰 집합 <span class="math math-inline">\text{Top-}k(I_{t,:})</span>를 선정한다.</p>
<p>실제 어텐션 연산(Multi-Query Attention 등)은 이렇게 선정된 소수의 토큰들에 대해서만 수행된다.</p>
<p><span class="math math-display">
\text{Attn}(q_t, K, V) \approx \sum_{s \in \text{Top-}k(I_{t,:})} \alpha_{t,s} v_s
</span><br />
이 방식은 긴 문맥에서도 연산량이 시퀀스 길이에 비례하여 선형적으로만 증가하므로, 수십만 토큰 이상의 긴 문맥을 처리할 때 기존 방식 대비 수십 배 이상의 속도 향상과 비용 절감 효과를 가져온다.</p>
<h3>3.2  전문가 라우팅(Routing)과 로드 밸런싱</h3>
<p>DeepSeek-V3.2는 희소 어텐션뿐만 아니라 MoE(Mixture-of-Experts) 구조를 채택하여 모델의 파라미터 효율성을 높였다. 특히 주목할 점은 전문가 간의 부하 불균형(Load Imbalance)을 해결하는 방식이다. 기존의 MoE 모델들은 특정 전문가에게 토큰이 쏠리면 일부 토큰을 연산에서 제외하는 ‘토큰 드롭(Token Dropping)’ 방식을 사용했는데, 이는 모델의 성능 저하를 유발했다.</p>
<p>DeepSeek는 이를 방지하기 위해 별도의 보조 손실 함수(Auxiliary Loss) 없이, 라우팅 과정에서 바이어스(Bias) 항을 동적으로 조정하는 방식을 사용했다.16</p>
<p><span class="math math-display">
s_{t,e} = \text{Score}(x_t, e) + b_e
</span><br />
여기서 <span class="math math-inline">b_e</span>는 전문가 <span class="math math-inline">e</span>의 현재 부하량에 따라 조절되는 바이어스 값이다. 전문가 <span class="math math-inline">e</span>가 과부하 상태라면 <span class="math math-inline">b_e</span>를 낮추어 해당 전문가가 선택될 확률을 줄이고, 다른 전문가로 트래픽을 분산시킨다. 이를 통해 DeepSeek-V3.2는 단 하나의 토큰도 버리지 않고(Drop-free) 모든 정보를 처리하면서도 높은 처리량을 유지할 수 있었다.</p>
<h3>3.3  오픈 소스의 반격: DeepSeek-V3.2-Speciale</h3>
<p>DeepSeek는 효율적인 아키텍처를 바탕으로 추론 능력에 특화된 <strong>DeepSeek-V3.2-Speciale</strong> 모델도 공개했다. 이 모델은 강화 학습(RL) 단계에서 모델이 사고하는 과정(Chain of Thought)의 길이에 대한 페널티를 과감하게 완화했다.2 즉, 모델이 정답을 내기 위해 더 오래, 더 깊게 생각하는 것을 장려한 것이다.</p>
<p>그 결과, Speciale 모델은 IOI 2025(국제정보올림피아드)와 ICPC World Final 2025 등 고난도 프로그래밍 대회 문제에서 금메달권의 성적을 기록했다. 이는 오픈 소스 모델도 적절한 아키텍처 최적화와 학습 전략이 결합되면, 폐쇄형 프론티어 모델(Gemini 3 Pro 등)과 대등한 수준의 추론 능력을 가질 수 있음을 입증한 중요한 사례이다.</p>
<h2>4.  시각 지능의 새로운 표준: Meta SAM 3</h2>
<p>대규모 언어 모델이 추론과 코딩에서 혁신을 이루는 동안, 컴퓨터 비전 분야에서는 Meta가 **Segment Anything Model 3 (SAM 3)**를 통해 시각적 이해의 새로운 지평을 열었다. SAM 3는 이미지와 비디오 내의 모든 객체를 분할(Segmentation)하는 파운데이션 모델로, 이전 버전에 비해 ’개념적 이해’와 ‘비디오 추적’ 능력이 비약적으로 향상되었다.3</p>
<h3>4.1  Presence Head: 인식과 위치 추정의 분리 (Decoupling)</h3>
<p>SAM 3의 가장 핵심적인 아키텍처 혁신은 <strong>Presence Head</strong>의 도입이다. 기존의 객체 검출 모델(DETR 계열 등)은 이미지 내의 객체를 찾기 위해 수백 개의 ’객체 쿼리(Object Query)’를 사용했다. 각 쿼리는 이미지의 특정 영역을 담당하며, “여기에 객체가 있는가?(Localization)“와 “그 객체는 무엇인가?(Classification)“를 동시에 판단해야 했다.</p>
<p>이러한 동시 수행 구조는 ‘환각(Hallucination)’ 문제를 야기했다. 예를 들어, 복잡한 패턴의 벽지나 구름 모양을 보고 모델이 특정 객체라고 잘못 확신하는 경우가 빈번했다. 특히 사용자가 텍스트 프롬프트로 “유니콘을 찾아줘“라고 했을 때, 유니콘이 없는 이미지에서도 말과 비슷한 형상을 억지로 유니콘으로 분할하는 위양성(False Positive) 문제가 심각했다.17</p>
<p>Presence Head는 이 문제를 해결하기 위해 **전역적 인식(Global Recognition)**과 **지역적 위치 추정(Local Localization)**을 분리한다. 모델은 먼저 이미지 전체를 보고 해당 개념(Concept)이 존재하는지 여부를 판단하는 별도의 모듈(Presence Head)을 거친다.</p>
<h4>4.1.1  Presence Head의 수학적 원리</h4>
<p>Presence Head는 이진 분류(Binary Classification) 문제를 푼다. 입력 이미지 <span class="math math-inline">I</span>와 텍스트 프롬프트 <span class="math math-inline">T</span>가 주어졌을 때, Presence Head는 해당 개념이 이미지 내에 존재할 확률 <span class="math math-inline">p_{presence}</span>를 출력한다. 학습 시에는 이진 교차 엔트로피(Binary Cross-Entropy) 손실 함수가 사용된다.3</p>
<p><span class="math math-display">
\mathcal{L}_{\text{presence}} = - [y \cdot \log(p_{\text{presence}}) + (1-y) \cdot \log(1-p_{\text{presence}})]
</span><br />
여기서 <span class="math math-inline">y \in {0, 1}</span>은 해당 개념의 실제 존재 여부를 나타내는 Ground Truth 레이블이다.</p>
<p>최종적으로 특정 객체 마스크 <span class="math math-inline">m</span>에 대한 신뢰도 점수 <span class="math math-inline">S_{\text{final}}</span>은 지역적 객체 점수 <span class="math math-inline">S_{\text{local}}</span>과 전역적 존재 점수 <span class="math math-inline">p_{\text{presence}}</span>의 곱으로 보정된다.</p>
<p><span class="math math-display">
S_{\text{final}} = p_{\text{presence}} \times S_{\text{local}}
</span><br />
이러한 메커니즘을 통해, 만약 Presence Head가 “이 이미지에는 유니콘이 없다(<span class="math math-inline">p_{\text{presence}} \approx 0</span>)“라고 판단하면, 지역 쿼리들이 아무리 유니콘과 비슷한 형상을 찾아내더라도 최종 점수는 0에 수렴하게 된다. 이로써 SAM 3는 위양성을 획기적으로 줄이고, 이미지 수준의 인식 성능 지표인 <strong>IL_MCC</strong>를 0.44에서 0.68로 대폭 향상시켰다.3</p>
<h3>4.2  Promptable Concept Segmentation (PCS)와 비디오 추적</h3>
<p>SAM 3는 **PCS (Promptable Concept Segmentation)**라는 새로운 과제를 정의했다. 이는 “영상 속의 모든 [개념]을 분할하라“는 명령을 수행하는 능력이다. 예를 들어 “모든 빨간색 셔츠를 입은 사람“이라는 텍스트 프롬프트가 주어지면, SAM 3는 이미지뿐만 아니라 비디오 전체 시퀀스에서 해당 조건을 만족하는 모든 객체를 찾아내고 추적한다.</p>
<p>비디오 처리를 위해 SAM 3는 SAM 2의 메모리 뱅크(Memory Bank) 구조를 계승 및 발전시켰다. 각 프레임에서 추출된 객체 정보는 메모리에 저장되며, 다음 프레임의 추론에 컨텍스트로 활용된다. 이를 통해 객체가 잠시 가려지거나(Occlusion) 화면 밖으로 나갔다 들어와도(Re-identification) 동일한 객체로 인식하고 추적을 유지할 수 있다.19</p>
<h2>5.  학술적 프론티어: 언어학자와 AI의 조우 (EMNLP 2025)</h2>
<p>2025년 11월 중국 쑤저우에서 개최된 <strong>EMNLP 2025</strong>는 AI의 언어 처리 능력을 학술적으로 깊이 있게 검증하는 장이었다. 이번 학회의 하이라이트는 LLM이 단순히 언어 데이터를 많이 학습한 것을 넘어, 언어의 구조적 원리를 메타적으로 이해하고 추론할 수 있는지를 탐구한 연구들이었다.20</p>
<h3>5.1  LingGym: LLM은 현장 언어학자처럼 사고할 수 있는가?</h3>
<p>브리티시 컬럼비아 대학교(UBC)와 워털루 대학교 연구진이 발표하여 ’Outstanding Paper Award’를 수상한 <strong>“LingGym: How Far Are LLMs from Thinking Like Field Linguists?”</strong> 논문은 이번 달 학계에서 가장 주목받은 연구 중 하나이다.21</p>
<h4>5.1.1  연구의 배경: 데이터 빈곤과 메타 추론</h4>
<p>대부분의 LLM은 영어, 중국어 등 데이터가 풍부한 언어(High-resource languages)를 중심으로 학습된다. 그러나 전 세계 수천 개의 언어는 학습 데이터가 거의 없는 저자원 언어(Low-resource languages)이다. 현장 언어학자들은 낯선 언어를 접했을 때, 제한된 몇 개의 문장과 문맥을 통해 그 언어의 문법 규칙을 귀납적으로 추론해낸다. 연구진은 LLM도 이러한 ’메타 언어적 추론(Meta-linguistic Reasoning)’이 가능한지 평가하고자 했다.</p>
<h4>5.1.2  LingGym 벤치마크와 Word-Gloss Inference</h4>
<p>연구진은 18개의 다양한 유형론적 특징을 가진 언어들의 참고 문법서(Reference Grammars)에서 <strong>IGT (Interlinear Glossed Text)</strong> 데이터를 추출하여 <strong>LingGym</strong> 벤치마크를 구축했다.23 IGT는 원문, 형태소 단위의 주석(Gloss), 그리고 번역문으로 구성된 언어학 데이터의 표준 형식이다.</p>
<p>핵심 평가 과제는 <strong>Word-Gloss Inference</strong>이다. 모델에게 문맥과 문법 설명, 그리고 일부가 가려진(Masked) IGT 데이터를 주고, 빈칸에 들어갈 단어나 형태소적 기능을 맞추게 하는 것이다.</p>
<ul>
<li><strong>입력 예시:</strong></li>
<li>문맥: (해당 언어의 문장과 번역)</li>
<li>문제: “이 문장에서 접미사 ’-ka’는 어떤 문법적 기능을 하는가?”</li>
<li>선택지: 1. 과거 시제 2. 목적격 조사 3. 복수형 4. 의문문 표지</li>
</ul>
<h4>5.1.3  연구 결과와 함의</h4>
<p>실험 결과, DeepSeek-R1 32B와 같은 고성능 모델들은 문법적 설명(Grammatical Description)이나 Gloss와 같은 구조적 단서가 주어졌을 때 추론 정확도가 비약적으로 상승하여 최대 81%에 달하는 성능을 보였다.4</p>
<p>이는 LLM이 단순히 통계적 패턴 매칭을 수행하는 것을 넘어, 주어진 메타 정보를 바탕으로 낯선 체계의 규칙을 즉석에서 학습(In-context Learning)하고 적용할 수 있는 고차원적 추론 능력을 갖추고 있음을 시사한다. 이 연구는 LLM이 사멸 위기 언어의 문서화나 언어 구조의 보편성 연구와 같은 순수 언어학 분야에서 강력한 ’연구 보조자(Research Assistant)’가 될 수 있음을 입증했다는 점에서 큰 의의를 가진다.</p>
<h2>6.  인프라와 생태계: 에이전트를 위한 무대</h2>
<p>기술적 진보와 더불어, 이를 뒷받침하고 확산시키기 위한 인프라와 정책적 움직임도 활발했다. 2025년 11월의 산업계 동향은 ’폐쇄성’에서 ’개방성과 연합’으로, 그리고 ’기업 주도’에서 ’국가 주도’로의 무게 중심 이동을 보여준다.</p>
<h3>6.1  Microsoft Ignite 2025: 모델 가든과 에이전트 오케스트레이션</h3>
<p>마이크로소프트(MS)는 Ignite 2025 컨퍼런스에서 자사의 Azure AI Foundry 플랫폼에 경쟁자인 Anthropic의 Claude 모델들을 전격 도입한다고 발표했다.25 이는 OpenAI와의 독점적 파트너십에 기반했던 기존 전략의 중대한 수정을 의미한다.</p>
<p>MS의 이러한 변화는 ’에이전트 시대’의 요구를 반영한 것이다. 단일 모델이 모든 작업을 완벽하게 수행할 수 없다는 인식이 확산되면서, 기업 고객들은 각기 다른 강점을 가진 모델들을 조합(Orchestration)하여 최적의 에이전트 워크플로우를 구축하기를 원한다. 예를 들어, 창의적인 글쓰기에는 GPT-5를, 정밀한 코딩과 추론에는 Claude 3.5 Opus를, 가벼운 요약에는 DeepSeek를 사용하는 식이다.</p>
<p>MS는 <strong>Foundry IQ</strong>와 <strong>Fabric IQ</strong>를 통해 이러한 이기종 모델들이 기업 내부 데이터(SharePoint, Dataverse 등)에 안전하게 접근하고 상호작용할 수 있는 ’결합 조직(Connective Tissue)’을 제공함으로써, 에이전트 생태계의 허브(Hub)를 선점하려는 전략을 구체화했다.</p>
<h3>6.2  국가 주도의 AI: Genesis Mission</h3>
<p>미국 정부는 트럼프 행정부 주도로 **“Genesis Mission”**이라는 대규모 행정 명령을 발표했다.5 이는 AI를 단순한 산업 기술이 아닌, 국가의 과학적 역량을 퀀텀 점프시킬 핵심 전략 자산으로 규정하는 것이다.</p>
<p>이 미션은 맨해튼 프로젝트에 비견되는 규모로, 국립 연구소의 슈퍼컴퓨팅 자원과 민간의 AI 모델, 그리고 양자 컴퓨터 기술을 통합하여 신소재 개발, 핵융합 연구, 생명 공학 등 난제 해결에 AI를 전면적으로 투입하는 것을 목표로 한다. 이는 AI 기술 경쟁이 기업 간의 시장 점유율 싸움을 넘어, 국가 간의 과학 기술 패권 경쟁으로 확전되고 있음을 보여주는 상징적인 사건이다. 구글이 텍사스에 400억 달러 규모의 데이터 센터 투자를 발표한 것 또한 이러한 흐름과 무관하지 않다.26</p>
<h2>7.  결론: 2026년을 향한 전망</h2>
<p>2025년 11월에 쏟아진 연구와 발표들을 종합해 볼 때, 인공지능 기술은 명확한 방향성을 가지고 진화하고 있다.</p>
<p><strong>첫째, ’생각하는 AI’의 보편화이다.</strong> Google의 Deep Think, OpenAI의 Thinking Model, DeepSeek의 Speciale 모델 등 모든 주요 플레이어들이 ’시스템 2’적 추론 능력을 핵심 경쟁력으로 내세우고 있다. 이제 모델의 성능은 파라미터 수뿐만 아니라, 추론을 위해 얼마나 많은 시간을 쓸 수 있는가(Test-time Compute)에 의해 결정되는 시대로 접어들었다.</p>
<p><strong>둘째, ’영속적 에이전트’의 실현이다.</strong> Compaction과 같은 기억 압축 기술과 희소 어텐션과 같은 효율화 기술은 AI가 세션의 제약 없이 24시간, 365일 인간과 협업할 수 있는 기반을 마련했다. 이는 AI를 단순한 도구(Tool)에서 동료(Coworker)의 지위로 격상시킬 것이다.</p>
<p><strong>셋째, ’물리적/디지털 세계와의 통합’이다.</strong> Generative UI를 통해 소프트웨어를 자유자재로 만들어내고, SAM 3를 통해 세상을 픽셀 단위로 이해하며, 에이전트 프레임워크를 통해 도구를 사용하는 AI는 이제 모니터 속의 챗봇을 넘어 실질적인 변화를 만들어내는 주체로 거듭나고 있다.</p>
<p>2025년 11월은 AI가 ’신기함(Novelty)’의 단계를 지나 ’효용(Utility)’과 ’신뢰(Reliability)’의 단계로 진입했음을 알리는 변곡점이다. 다가오는 2026년은 이러한 기술들이 결합되어 과학적 발견을 가속화하고, 산업 현장의 생산성을 근본적으로 재편하는 원년이 될 것이다.</p>
<h3>7.1 부록: 주요 기술 비교 및 데이터</h3>
<p>다음은 본 보고서에서 다룬 핵심 모델들의 기술적 사양과 주요 벤치마크 결과를 요약한 것이다.</p>
<p><strong>표 1. 2025년 11월 주요 프론티어 모델 비교</strong></p>
<table><thead><tr><th><strong>모델명</strong></th><th><strong>개발사</strong></th><th><strong>주요 기술적 특징</strong></th><th><strong>핵심 성과 및 지표</strong></th></tr></thead><tbody>
<tr><td><strong>Gemini 3</strong></td><td>Google</td><td>Deep Think, Generative UI, AlphaEvolve</td><td>LMArena 1501점, GPQA Diamond 93.8% 1</td></tr>
<tr><td><strong>GPT-5.1-Codex-Max</strong></td><td>OpenAI</td><td>Compaction, Agentic Coding</td><td>SWE-bench Verified 77.9%, 24시간 연속 작업 12</td></tr>
<tr><td><strong>DeepSeek-V3.2</strong></td><td>DeepSeek</td><td>Sparse Attention (DSA), Lightning Indexer</td><td>추론 비용 절감, <span class="math math-inline">O(kL)</span> 복잡도 2</td></tr>
<tr><td><strong>SAM 3</strong></td><td>Meta</td><td>Presence Head, Promptable Concept Segmentation</td><td>IL_MCC 0.68, 환각(Hallucination) 억제 3</td></tr>
</tbody></table>
<p><strong>표 2. SAM 3와 기존 아키텍처의 차이점</strong></p>
<table><thead><tr><th><strong>구분</strong></th><th><strong>기존 DETR/SAM 아키텍처</strong></th><th><strong>SAM 3 아키텍처</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>객체 쿼리 역할</strong></td><td>인식(What) + 위치(Where) 동시 수행</td><td>위치 추정(Where)에 집중</td><td>역할 분리로 최적화 용이</td></tr>
<tr><td><strong>존재 여부 판단</strong></td><td>쿼리별 신뢰도 점수(Confidence Score)에 의존</td><td><strong>Presence Head</strong>가 전역적으로 판단</td><td>별도 모듈로 제어</td></tr>
<tr><td><strong>손실 함수</strong></td><td>Focal Loss 등</td><td>Presence Head에 <strong>Binary Cross-Entropy</strong> 적용</td><td><span class="math math-inline">\mathcal{L}_{\text{presence}}</span> 추가 3</td></tr>
<tr><td><strong>주요 문제점</strong></td><td>배경 노이즈를 객체로 인식(환각)</td><td>위양성(False Positive) 대폭 감소</td><td>신뢰성 향상</td></tr>
</tbody></table>
<p><strong>표 3. DeepSeek Sparse Attention (DSA) 효율성 분석</strong></p>
<table><thead><tr><th><strong>항목</strong></th><th><strong>표준 Attention (Full)</strong></th><th><strong>DeepSeek Sparse Attention (DSA)</strong></th></tr></thead><tbody>
<tr><td><strong>연산 복잡도</strong></td><td><span class="math math-inline">O(L^2)</span> (이차 함수적 증가)</td><td><span class="math math-inline">O(kL)</span> (선형에 가까운 증가)</td></tr>
<tr><td><strong>토큰 처리 방식</strong></td><td>모든 과거 토큰 참조</td><td>중요도 점수(<span class="math math-inline">I_{t,s}</span>) 상위 <span class="math math-inline">k</span>개만 참조</td></tr>
<tr><td><strong>인덱싱 방식</strong></td><td>없음</td><td><strong>Lightning Indexer</strong> (ReLU + FP8)</td></tr>
<tr><td><strong>장점</strong></td><td>정확한 문맥 파악</td><td>긴 문맥 처리 속도 및 메모리 효율 극대화</td></tr>
</tbody></table>
<h2>8. 참고 자료</h2>
<ol>
<li>Latest AI Tech Updates 2025 | November AI Breakthroughs, https://tsttechnology.io/blog/ai-updates-november-2025</li>
<li>DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models - arXiv, https://arxiv.org/html/2512.02556v1</li>
<li>SAM 3: Segment Anything with Concepts - arXiv, https://arxiv.org/html/2511.16719v1</li>
<li>LINGGYM: How Far Are LLMs from Thinking Like Field Linguists? - ACL Anthology, https://aclanthology.org/2025.emnlp-main.69.pdf</li>
<li>AI by AI Weekly Top 5: November 24– 30, 2025 - Champaign Magazine, https://champaignmagazine.com/2025/11/30/ai-by-ai-weekly-top-5-november-24-30-2025/</li>
<li>AlphaEvolve: A Gemini-powered coding agent for designing advanced algorithms, https://deepmind.google/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/</li>
<li>Building more with GPT-5.1-Codex-Max, https://openai.com/index/gpt-5-1-codex-max/</li>
<li>OpenAI’s Codex Max solves one of my biggest AI coding annoyances - and it’s a lot faster, https://www.zdnet.com/article/openais-codex-max-solves-one-of-my-biggest-ai-coding-annoyances-and-its-a-lot-faster/</li>
<li>Codex 5.1 Pro vs Gemini 3 Pro: Which AI Writes Better Python in 2025? - iTecs, https://itecsonline.com/post/gpt-5-1-vs-gemini-3</li>
<li>GPT-5.1-Codex-Max Prompting Guide - OpenAI Cookbook, https://cookbook.openai.com/examples/gpt-5/gpt-5-1-codex-max_prompting_guide</li>
<li>Introducing GPT-5.2, https://openai.com/index/introducing-gpt-5-2/</li>
<li>GPT-5.1-Codex-Max vs Gemini 3 Pro: Next-Generation AI Coding Titans - Medium, https://medium.com/@leucopsis/gpt-5-1-codex-max-vs-gemini-3-pro-next-generation-ai-coding-titans-877cc9054345</li>
<li>The reason why Deepseek V3.2 is so cheap : r/LocalLLaMA - Reddit, https://www.reddit.com/r/LocalLLaMA/comments/1nth7cb/the_reason_why_deepseek_v32_is_so_cheap/</li>
<li>NEW DeepSeek Sparse Attention Explained - YouTube, https://www.youtube.com/watch?v=kAEPS_AUGy8</li>
<li>DeepSeek Sparse Attention - Open Superintelligence Lab, https://opensuperintelligencelab.com/blog/deepseek-sparse-attention/</li>
<li>DeepSeek-V3.2: Open Source AI Matches GPT-5 and Gemini 3 at 10× Lower Cost - Introl, https://introl.com/blog/deepseek-v3-2-open-source-ai-cost-advantage</li>
<li>SAM-3: What’s New, How It Works, and Why It Matters |, https://learnopencv.com/sam-3-whats-new/</li>
<li>SAM 3: A Technical Deep Dive into Meta’s Next-Generation Segmentation Model - Datature, https://datature.com/blog/sam-3-a-technical-deep-dive-into-metas-next-generation-segmentation-model</li>
<li>SAM 3: Segment Anything with Concepts - Ultralytics YOLO Docs, https://docs.ultralytics.com/models/sam-3/</li>
<li>Empirical Methods in Natural Language Processing (EMNLP) 2025, https://machinelearning.apple.com/updates/apple-at-emnlp-2025</li>
<li>12월 14, 2025에 액세스, [https://cs.uwaterloo.ca/news/freda-shi-colleagues-win-emnlp-2025-outstanding-paper-award-research-llm-meta-linguistic-reasoning#:<sub>:text=Professor%20Freda%20Shi%20and%20her,Methods%20in%20Natural%20Language%20Processing.](https://cs.uwaterloo.ca/news/freda-shi-colleagues-win-emnlp-2025-outstanding-paper-award-research-llm-meta-linguistic-reasoning#:</sub>:text=Professor Freda Shi and her, <a href="https://cs.uwaterloo.ca/news/freda-shi-colleagues-win-emnlp-2025-outstanding-paper-award-research-llm-meta-linguistic-reasoning#:~:text=Professor%20Freda%20Shi%20and%20her,Methods%20in%20Natural%20Language%20Processing.">https://cs.uwaterloo.ca/news/freda-shi-colleagues-win-emnlp-2025-outstanding-paper-award-research-llm-meta-linguistic-reasoning#:~:text=Professor%20Freda%20Shi%20and%20her,Methods%20in%20Natural%20Language%20Processing.</a></li>
<li>Freda Shi and colleagues win EMNLP 2025 Outstanding Paper …, https://cs.uwaterloo.ca/news/freda-shi-colleagues-win-emnlp-2025-outstanding-paper-award-research-llm-meta-linguistic-reasoning</li>
<li>LingGym: How Far Are LLMs from Thinking Like Field Linguists? - ResearchGate, https://www.researchgate.net/publication/397419439_LingGym_How_Far_Are_LLMs_from_Thinking_Like_Field_Linguists</li>
<li>LingGym: How Far Are LLMs from Thinking Like Field Linguists? - arXiv, https://arxiv.org/pdf/2511.00343</li>
<li>Microsoft Ignite 2025 Recap: Agentic AI, Foundry, and Azure …, https://azure.microsoft.com/en-us/blog/actioning-agentic-ai-5-ways-to-build-with-news-from-microsoft-ignite-2025/</li>
<li>Google AI announcements from November - Google Blog, https://blog.google/technology/ai/google-ai-updates-november-2025/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>