<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2025년 8월 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2025년 8월 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2025년 AI 및 로봇 연구 동향</a> / <span>2025년 8월 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2025년 8월 로봇 연구 동향</h1>
<h2>1.  서론: 전환점에 선 로봇 공학의 새로운 패러다임</h2>
<p>2025년 8월은 로봇 공학 역사에 있어 기술적 특이점과 산업적 실용주의가 교차하는 중대한 분기점으로 기록될 전망이다. 이 시기에 발표된 연구 성과들은 단순히 기존 기술의 점진적 개선에 그치지 않고, 로봇이 물리적 세계를 인식하고 상호작용하는 방식을 근본적으로 재정의하고 있다. 학계와 산업계가 주목하는 핵심 화두는 ‘거대 언어 모델(LLM)과 물리적 제어의 결합’, ‘시뮬레이션과 현실의 간극을 메우는 생성형 월드 모델(Generative World Model)’, 그리고 ’인간 중심의 정서적 교감 기술’로 요약된다.</p>
<p>특히 2025년 8월에 개최된 <strong>IEEE International Conference on Automation Science and Engineering (CASE 2025)</strong>, <strong>IEEE International Conference on Robot and Human Interactive Communication (RO-MAN 2025)</strong>, <strong>IEEE Intelligent Transportation Systems Conference (ITSC 2025)</strong> 등 주요 국제 학술대회는 각 분야에서 기념비적인 연구 결과들을 쏟아냈다. 동시에 <strong>Science Robotics</strong>와 <strong>Nature Machine Intelligence</strong> 같은 권위 있는 저널들은 하드웨어의 유연성과 소프트웨어의 지능이 결합된 소프트 로보틱스 및 전신 제어(Whole-body Control) 분야의 혁신을 조명했다.</p>
<p>본 보고서는 2025년 8월 한 달간 발표된 주요 로봇 공학 연구 성과들을 망라하여 심층적으로 분석한다. 각 연구의 기술적 배경과 방법론, 실험 결과, 그리고 이것이 로봇 생태계에 미칠 파급 효과를 다각도로 조명함으로써, 현재 로봇 기술의 최전선이 어디에 와 있으며 앞으로 어디로 나아갈 것인지에 대한 통찰을 제공한다. 보고서는 크게 자동화 과학, 인간-로봇 상호작용, 전신 조작 제어, 생성형 시뮬레이션, 자율 주행 지능의 다섯 가지 핵심 영역으로 구성된다.</p>
<h2>2.  자동화 과학의 정밀화와 확장: IEEE CASE 2025</h2>
<p>2025년 8월 17일부터 21일까지 미국 캘리포니아주 로스앤젤레스에서 개최된 <strong>IEEE CASE 2025</strong>는 “Automation Science and Engineering“이라는 주제 아래, 제조 공정의 효율화를 넘어 농업, 헬스케어, 물류 등 비정형 환경으로의 자동화 확장을 적극적으로 모색했다.1 특히 이번 학회에서는 불확실성이 높은 환경에서의 강인한 제어와 AI 기반의 의사결정 시스템이 주류를 이루었다.</p>
<h3>2.1  다품종 공생 농업을 위한 자율 시스템의 도약</h3>
<p>CASE 2025의 가장 큰 하이라이트 중 하나는 <strong>Best Paper Award</strong>를 수상한 Ken Goldberg 교수(UC Berkeley)가 이끄는 AUTOLAB 팀의 연구였다.3 이 연구는 기존의 단일 작물 위주의 기계화 농업이 가진 생태적 한계를 극복하고, 생물 다양성을 보존하면서도 생산성을 극대화할 수 있는 ’다품종 공생 농업(Polyculture)’의 자동화 가능성을 열었다는 점에서 학계의 찬사를 받았다.</p>
<h4>2.1.1  논문: Automated Pruning of Polyculture Plants</h4>
<p>다품종 공생 농업은 서로 다른 종류의 작물을 혼식하여 병충해를 방지하고 토양 영양분을 효율적으로 사용하는 방식이다. 그러나 작물 간의 물리적 간섭이 심하고 성장 패턴이 제각각이라 기계적 접근이 극도로 어렵다는 단점이 있었다. Goldberg 팀은 이러한 비정형적이고 복잡한 환경(Cluttered Environment)에서 개별 식물의 가지치기(Pruning)를 수행할 수 있는 자율 로봇 시스템을 제안했다.4</p>
<ul>
<li><strong>기술적 난제와 해결:</strong></li>
<li><strong>인식의 불확실성:</strong> 잎과 줄기가 얽혀 있는 상황에서는 카메라의 시야가 가려지기 쉽다(Occlusion). 연구팀은 이를 해결하기 위해 다시점(Multi-view) RGB-D 데이터를 융합하여 식물의 3D 구조를 정밀하게 복원하는 기술을 도입했다. 딥러닝 기반의 세그멘테이션 네트워크는 복잡한 포인트 클라우드 데이터 속에서 ’절단해야 할 가지’와 ‘보존해야 할 줄기’, 그리고 ’피해야 할 장애물’을 실시간으로 구분한다.</li>
<li><strong>조작의 정밀성:</strong> 목표 가지에 도달하기 위해서는 주변 식물을 건드리지 않는 정교한 경로 계획(Motion Planning)이 필수적이다. 연구팀은 충돌 회피를 위한 동적 제약 조건을 강화한 역운동학(Inverse Kinematics) 알고리즘을 적용하여, 로봇 팔이 좁은 틈새를 파고들어 정확한 절단점(Cut point)에 도달하도록 설계했다.</li>
<li><strong>실험 결과 및 시사점:</strong> 실제 온실 환경에서 수행된 실험에서 해당 시스템은 숙련된 농업 전문가에 준하는 정확도를 보였으며, 작업 속도 면에서도 상용화 가능성을 입증했다. 이는 정밀 농업(Precision Agriculture)이 단순히 비료나 물을 아끼는 차원을 넘어, 로봇이 생태학적 복잡성을 이해하고 관리하는 단계로 진입했음을 시사한다.</li>
</ul>
<h3>2.2  제조 및 헬스케어 자동화의 새로운 지평</h3>
<p>CASE 2025는 전통적인 제조 자동화 분야에서도 AI의 도입을 가속화하는 연구들을 대거 소개했다. 특히 반도체 제조 공정에서의 스케줄링 최적화와 결함 감지 기술, 그리고 헬스케어 서비스 로봇의 지능화가 주요 의제로 다루어졌다.5</p>
<table><thead><tr><th><strong>세션 구분</strong></th><th><strong>주요 연구 주제</strong></th><th><strong>기술적 특징</strong></th></tr></thead><tbody>
<tr><td><strong>반도체 제조</strong></td><td>웨이퍼 결함 검사 최적화, 공정 스케줄링</td><td>부분 관측(Partial Coverage) 하에서의 결함 예측, 강화학습 기반 동적 스케줄링 6</td></tr>
<tr><td><strong>헬스케어 자동화</strong></td><td>수술 보조 로봇, 병원 물류 자동화</td><td>비전 기반 수술 도구 추적, 환자 모니터링을 위한 센서 퓨전</td></tr>
<tr><td><strong>지속 가능성</strong></td><td>에너지 효율적 공장 운영</td><td>제조 데이터 기반의 에너지 소비 예측 및 최적 제어</td></tr>
</tbody></table>
<p><strong>Peter Luh Memorial Best Paper Award for Young Researcher</strong> 부문에서는 젊은 연구자들의 혁신적인 접근이 돋보였다. 예를 들어, 제조 시스템의 불확실성을 확률적으로 모델링하고 이를 실시간 제어에 반영하는 연구들이 주목받았다.7 Ziyue Li 교수(Technical University of Munich) 등은 시공간 데이터 마이닝(Spatiotemporal Data Mining)을 활용하여 제조 공정의 이상 징후를 조기에 감지하고 예측하는 방법론을 제시하며 수상의 영예를 안거나 파이널리스트에 올랐다.9</p>
<p>또한, <strong>Best Student Paper Award</strong> 부문에서는 강화학습을 이용한 가상 픽스처(Virtual Fixture) 추정 가속화 기술이 주목받았다. Diego Fernandez Prado 등이 발표한 “Accelerating Virtual Fixture Estimation for Robot Manipulation using Reinforcement Learning and Human Demonstrations“는 인간의 시연 데이터를 활용하여 로봇이 작업 제약 조건을 빠르게 학습하도록 함으로써, 협동 로봇의 안전성과 효율성을 동시에 높이는 결과를 보여주었다.10</p>
<h2>3.  인간과 로봇의 정서적 공명: IEEE RO-MAN 2025</h2>
<p>네덜란드 아인트호벤에서 8월 25일부터 29일까지 열린 **IEEE RO-MAN 2025 (34th IEEE International Conference on Robot and Human Interactive Communication)**는 기술적 성능보다는 ’인간에 대한 이해’와 ’관계의 형성’에 방점을 두었다.11 이번 학회에서는 로봇이 학교, 병원, 가정 등 일상 공간에서 인간의 정서적, 신체적 파트너로서 어떻게 기능해야 하는지에 대한 심도 있는 논의가 이루어졌다.</p>
<h3>3.1  특수 교육 현장의 로봇: 정서 조절의 매개체</h3>
<p><strong>IEEE Best Paper Award</strong>를 수상한 Yale University의 Rebecca Ramnauth 연구팀은 로봇이 특수 교육이 필요한 아동들의 정서적 안정을 돕는 효과적인 도구가 될 수 있음을 입증했다.12</p>
<h4>3.1.1  논문: From Fidgeting to Focused: Developing Robot-Enhanced Social-Emotional Therapy (RESET) for School De-Escalation Rooms</h4>
<p>이 연구는 자폐 스펙트럼 장애(ASD)나 감각 처리 장애가 있는 학생들이 겪는 ‘감각 과부하(Sensory Overload)’ 문제를 해결하기 위해 학교 내 ’진정실(De-escalation Room)’에 소셜 로봇 ’RESET’을 도입한 사례를 다룬다.13</p>
<ul>
<li><strong>연구 배경:</strong> 많은 학교가 학생들의 심리적 안정을 위해 진정실을 운영하지만, 인력 부족으로 인해 적절한 개입이 이루어지지 못하는 경우가 많다. 연구팀은 비판단적이고 일관된 반응을 보이는 로봇이 인간 교사보다 학생들에게 더 편안한 존재가 될 수 있다는 가설을 세웠다.</li>
<li><strong>시스템 설계 (Co-design):</strong> 로봇의 디자인부터 상호작용 시나리오까지 특수 교사, 상담사, 학생들과의 공동 설계를 통해 개발되었다. RESET 로봇은 학생의 호흡을 유도하거나(Deep breathing), 가벼운 대화를 나누고, 함께 그림을 그리는 등의 활동을 통해 학생이 흥분 상태에서 평온한 상태로 전이(Transition)하도록 돕는다.</li>
<li><strong>현장 검증 결과:</strong> 초등학교에 한 달간 로봇을 배치하여 자율 운영한 결과, 로봇과 상호작용한 학생들은 그렇지 않은 경우보다 진정 시간이 단축되었으며, 교실로 복귀한 후 학습 과제에 대한 집중도가 향상되었다. 교직원의 74%는 로봇이 학생들의 자기 조절 활동 참여를 증진시켰다고 평가했다. 이는 로봇이 단순한 장난감이 아니라, 교육 및 치료 프로토콜의 일부로서 기능할 수 있음을 보여주는 중요한 실증 데이터다.13</li>
</ul>
<h3>3.2  생체 모방 제어를 통한 보행 재활의 혁신</h3>
<p><strong>IEEE Best Student Paper Award</strong>는 스위스 EPFL의 Zeynep Özge Orhan 연구팀에게 돌아갔다. 이들은 인간의 신경계가 보행 중 관절 간의 협응을 조절하는 방식을 로봇 제어에 적용하여 재활 효과를 극대화하는 방법을 제안했다.12</p>
<h4>3.2.1  논문: 3D Path Control: Can we use lower limb inter-joint coordination to assist gait and balance?</h4>
<p>기존의 하지 재활 로봇은 정해진 시간 궤적을 강제로 따르게 하는 위치 제어 방식을 주로 사용했다. 그러나 이는 환자의 자발적인 노력을 저해하고, 예기치 않은 외란에 취약하다는 단점이 있었다. Orhan 연구팀은 ’관절 간 협응(Inter-joint Coordination)’이라는 생체역학적 원리를 도입하여 이를 해결하고자 했다.14</p>
<ul>
<li>제어 방법론 (Path Control):</li>
</ul>
<p>연구팀은 보행 주기(Phase) <span class="math math-inline">S</span>에 따라 고관절, 무릎, 발목의 관절 각도 <span class="math math-inline">q(S)</span>가 특정한 기하학적 경로를 형성한다는 점에 착안했다. 시간 <span class="math math-inline">t</span>에 종속적인 궤적 <span class="math math-inline">q(t)</span> 대신, 보행 진행 변수 <span class="math math-inline">S</span>에 종속적인 경로 <span class="math math-inline">q(S)</span>를 정의하고, 로봇이 이 경로 주변에 가상의 ’터널(Tunnel)’을 형성하도록 제어했다.</p>
<p>제어 법칙은 다음과 같이 임피던스 제어 형태를 띤다:</p>
<p>\tau_{imp} = k_p (q(S) - q_{meas}) + k_d (\dot{q}(S) \cdot \dot{S} - \dot{q}_{meas})</p>
<p>여기서 <span class="math math-inline">\tau_{imp}</span>는 로봇이 가하는 토크, <span class="math math-inline">q_{meas}</span>는 측정된 관절 각도이다. 이 제어기는 사용자가 터널 내에서 움직일 때는 자유로운 움직임을 허용하고(Zero impedance), 터널 벽을 벗어나려 할 때만 복원력을 제공하여 올바른 보행 패턴을 유도한다.</p>
<ul>
<li><strong>실험적 성과:</strong> 외란이 가해지는 보행 실험에서 이 제어 전략은 사용자의 균형 회복 능력을 유의미하게 향상시켰다. 또한 근전도(EMG) 분석 결과, 사용자가 로봇에 의존하지 않고 능동적으로 근육을 사용하여 보행하도록 유도함이 확인되었다. 이는 신경 가소성(Neuroplasticity)을 촉진해야 하는 재활 치료의 목적에 부합하는 결과다.15</li>
</ul>
<h3>3.3  디자인과 맥락의 중요성: 로봇 디자인 대회</h3>
<p>RO-MAN 2025의 로봇 디자인 대회(Robot Design Competition)는 기술적 구현을 넘어 로봇이 놓인 ’맥락(Context)’과 ’스토리’를 얼마나 잘 풀어냈는지를 평가했다.16</p>
<ul>
<li><strong>Best Context Design Award:</strong> “ChatBox”.12 이 프로젝트는 아동과의 장기적인 치료적 관계 형성을 위해 감정을 인식하고 표현하는 동반자 로봇을 제안했다. ChatBox는 아동의 표정과 음성에서 감정 상태를 파악하고, 이에 맞춰 공감적인 반응을 보임으로써 신뢰를 쌓는다.</li>
<li><strong>RSJ Pioneering Research Award:</strong> “How Do People Feel about Robots That Leverage Profanity?”.12 이 연구는 다소 도발적인 주제를 다루었다. 로봇이 상황에 따라 욕설이나 비속어를 사용할 때 인간이 느끼는 친밀감이나 충격, 그리고 사회적 규범에 대한 인식이 어떻게 달라지는지를 실험적으로 분석했다. 이는 HRI 연구가 인간의 복잡하고 미묘한 사회적 행동 양식까지 탐구 영역을 넓히고 있음을 보여준다.</li>
</ul>
<h2>4.  유연함이 만드는 강인함: Science Robotics (2025년 8월)</h2>
<p>2025년 8월 20일 발간된 <strong>Science Robotics</strong> 제10권 105호는 로봇이 환경과 접촉하는 방식을 근본적으로 재고하는 연구를 커버스토리로 다루었다. 기존의 로봇 공학이 충돌을 피하거나(Avoidance) 접촉을 최소화하는 데 집중했다면, 새로운 연구들은 로봇의 온몸을 이용하여 환경과 적극적으로 접촉하고 상호작용하는 ’전신 조작(Whole-body Manipulation)’을 지향한다.17</p>
<h3>4.1  전신을 이용한 접촉 풍부 조작 기술</h3>
<p>Toyota Research Institute (TRI)의 Jose A. Barreiros와 Alex Alspach 등이 주도한 연구팀은 휴머노이드 로봇이 팔뿐만 아니라 가슴, 엉덩이 등 온몸을 사용하여 크고 무거운 물체를 조작하는 기술을 개발했다.</p>
<h4>4.1.1  논문: Learning contact-rich whole-body manipulation with example-guided reinforcement learning</h4>
<p>이 연구는 인간이 쌀 포대나 대형 박스 같은 부피가 큰 물체를 옮길 때 손가락 끝만 사용하지 않고 온몸으로 껴안아(Embracing) 지지한다는 점에 착안했다. 연구팀은 이를 구현하기 위해 소프트 로봇 하드웨어와 강화학습 알고리즘을 결합했다.19</p>
<ul>
<li>
<p><strong>하드웨어 (Punyo):</strong> 실험에는 TRI가 개발한 ’Punyo’라는 소프트 휴머노이드 로봇이 사용되었다. Punyo는 전신이 유연한 소재(Passive Compliance)로 덮여 있어 물체와의 넓은 면적 접촉 시 압력을 분산시키고 안정적인 마찰력을 제공한다. 또한 전신에 분포된 촉각 센서를 통해 접촉 상태를 실시간으로 감지한다.20</p>
</li>
<li>
<p>알고리즘: 예제 유도 강화학습 (Example-Guided RL):</p>
</li>
</ul>
<p>전신 조작은 자유도가 매우 높아 일반적인 강화학습(RL)으로는 해를 찾기 어렵다. 연구팀은 소수의 전문가 시연(Expert Demonstration) 데이터를 RL의 보상 함수에 통합하여 학습을 가이드했다. 보상 함수 <span class="math math-inline">r_t</span>는 작업 수행 여부(<span class="math math-inline">r_{task}</span>)와 시연 동작과의 유사성(<span class="math math-inline">r_{style}</span>)을 결합한 형태이다.</p>
<p><code>r_t = w_{task} r_{task} + w_{style} r_{style}</code></p>
<p>여기서 <span class="math math-inline">r_{style}</span> 항은 적대적 모션 사전(Adversarial Motion Prior, AMP) 기법을 사용하여, 로봇이 물리적으로 불가능하거나 부자연스러운 동작을 하지 않고 인간과 유사한 효율적인 동작을 생성하도록 유도한다.21</p>
<ul>
<li>실험 결과 - 블라인드 조작(Blind Manipulation):</li>
</ul>
<p>연구의 백미는 로봇이 시각 정보(Vision)를 차단한 상태에서도 오직 고유수용성 감각(Proprioception)과 촉각 피드백만으로 물체를 들어 올리고 조작하는 데 성공했다는 점이다. 이는 로봇이 물체를 껴안을 때 시야가 가려지는 상황이나 조명이 없는 어두운 환경에서도 작업을 수행할 수 있음을 의미한다. 로봇의 유연한 신체가 제어 오차를 흡수하고, 촉각 센서가 물체의 미세한 움직임을 감지하여 안정성을 유지했기 때문이다.18</p>
<h2>5.  시뮬레이션의 혁명: DeepMind Genie 3</h2>
<p>2025년 8월, <strong>Google DeepMind</strong>는 로봇 학습을 위한 시뮬레이션 환경 생성 기술의 정점을 보여주는 <strong>Genie 3</strong>를 공개했다.22 이는 단순한 텍스트-비디오 생성 모델을 넘어, 사용자와 상호작용 가능한 ’월드 모델(World Model)’로서의 기능을 갖추고 있다.</p>
<h3>5.1  상호작용 가능한 생성형 월드 모델</h3>
<p>Genie 3는 텍스트 프롬프트나 이미지를 입력받아, 사용자가 탐험하고 조작할 수 있는 3D 가상 환경을 실시간으로 생성한다.</p>
<ul>
<li><strong>실시간 상호작용성:</strong> 기존의 Sora나 Runway Gen-3와 같은 비디오 생성 모델들이 정해진 영상을 수동적으로 보여주는 것과 달리, Genie 3는 사용자의 키보드 입력이나 에이전트의 행동에 따라 매 프레임(Frame)을 실시간(24fps, 720p 해상도)으로 생성한다. 즉, 사용자가 “오른쪽으로 이동“하면 오른쪽 시야의 새로운 환경이 즉석에서 그려지는 방식이다.24</li>
<li><strong>장기 기억 및 일관성 (Long-Horizon Consistency):</strong> 생성형 모델의 난제였던 ’일관성 유지’를 획기적으로 개선했다. 사용자가 가상 세계에서 특정 위치를 벗어났다가 다시 돌아오더라도, 이전에 있었던 나무나 건물이 그대로 유지된다. 이는 모델이 최대 1분 이상의 시각적 메모리를 유지하며 과거의 상태를 기억하기 때문이다.25</li>
<li><strong>프롬프트 기반 월드 이벤트 (Promptable World Events):</strong> 시뮬레이션 도중 “비가 오게 해” 또는 “장애물을 추가해“와 같은 텍스트 명령을 통해 환경을 동적으로 변화시킬 수 있다. 이는 로봇 학습 시 다양한 돌발 상황(Edge Case)을 인위적으로 조성하는 데 매우 유용하다.27</li>
</ul>
<h3>5.2  Sim-to-Real의 가속화</h3>
<p>Genie 3는 로봇 연구의 고질적인 문제인 ’데이터 부족’과 ’Sim-to-Real 간극’을 해결할 강력한 도구로 평가받는다.</p>
<ul>
<li><strong>창발적 물리 이해 (Emergent Physics):</strong> Genie 3는 물리 엔진을 코딩하지 않았음에도, 대규모 비디오 데이터를 학습하여 물체의 낙하, 충돌, 유체 흐름 등의 물리 법칙을 스스로 터득하고 재현한다. 로봇은 이 환경 안에서 수많은 시행착오를 겪으며 물리적 상호작용을 학습할 수 있다.28</li>
<li><strong>무한한 훈련 환경:</strong> 연구자들은 더 이상 복잡한 3D 에셋을 모델링할 필요 없이, 텍스트만으로 사막, 도심, 수중 등 다양한 환경을 생성하여 로봇을 훈련시킬 수 있다. 이는 DeepMind의 범용 로봇 에이전트인 SIMA 2의 훈련에도 활용되고 있다.29</li>
</ul>
<h2>6.  자율주행과 지각의 진화: IEEE ITSC 2025</h2>
<p>**IEEE Intelligent Transportation Systems Conference (ITSC 2025)**에서는 자율주행 시스템의 인지 및 판단 능력을 고도화하기 위해 거대 시각-언어 모델(VLM)을 접목하는 시도들이 주목받았다.</p>
<h3>6.1  VLM 기반의 자율주행 강화학습: COVLM-RL</h3>
<p>**Best Paper Award (1st Prize)**를 수상한 논문은 자율주행의 의사결정 과정에 VLM의 의미론적 추론 능력을 통합한 프레임워크를 제안했다.30</p>
<h4>6.1.1  논문: COVLM-RL: Critical Object-Oriented Reasoning for Autonomous Driving Using VLM-Guided Reinforcement Learning</h4>
<p>저자: Lin Li, Yuxin Cai, Jianwu Fang, Jianru Xue, Chen Lv (Nanyang Technological University 등)</p>
<ul>
<li><strong>핵심 아이디어:</strong> 기존의 딥러닝 기반 자율주행(End-to-End)은 상황에 대한 해석 능력이 부족하고, VLM은 실시간 제어에는 느리다는 단점이 있다. 연구진은 VLM을 ’상위 레벨 전략가’로, 강화학습(RL) 에이전트를 ’하위 레벨 조종사’로 사용하는 계층적 구조를 설계했다.</li>
<li><strong>시스템 구조:</strong></li>
</ul>
<ol>
<li><strong>중요 객체 중심 추론 (Critical Object-Oriented Reasoning):</strong> VLM은 주행 영상을 분석하여 “전방에 무단횡단 보행자가 있으니 감속해야 함“과 같은 고차원적인 전략을 자연어로 생성하고, 이를 임베딩 벡터로 변환한다. 사고 사슬(Chain-of-Thought) 기법을 통해 추론의 논리성을 강화했다.</li>
<li><strong>일관성 손실 (Consistency Loss):</strong> RL 에이전트가 VLM의 전략을 따르도록 강제하기 위해, VLM의 의도와 RL의 행동 간의 차이를 줄이는 손실 함수를 도입했다. 이는 RL 에이전트가 무작위 탐색(Exploration)을 줄이고 빠르게 최적의 행동을 학습하도록 돕는다.</li>
</ol>
<ul>
<li><strong>성과:</strong> CARLA 시뮬레이터 실험 결과, 이 방법은 훈련되지 않은 새로운 도시 환경에서의 주행 성공률을 50% 이상 향상시켰으며, 복잡한 교차로에서의 사고율을 현저히 낮추었다.31</li>
</ul>
<h3>6.2  인간의 의도를 읽는 시각 지능: EgoIntention</h3>
<p>로봇이나 AI 비서가 인간을 효과적으로 돕기 위해서는 명시적인 명령뿐만 아니라, 상황을 보고 인간의 ’의도’를 파악하는 능력이 필수적이다. 이와 관련하여 <strong>Visual Intention Grounding</strong>이라는 새로운 과제가 제안되었다.32</p>
<h4>6.2.1  논문: Visual Intention Grounding for Egocentric Assistants</h4>
<p>저자: Pengzhan Sun et al. (ICCV 2025 발표 예정, arXiv 선공개)</p>
<ul>
<li><strong>EgoIntention 데이터셋:</strong> 연구진은 1인칭 시점(Egocentric) 영상에서 사용자의 잠재적 의도와 이를 충족시킬 수 있는 물체를 연결한 대규모 데이터셋을 구축했다. 예를 들어, 사용자가 땀을 흘리며 주변을 두리번거리는 영상에 대해 “목이 마르다“라는 의도 태그와 “물컵” 또는 “정수기“의 위치 정보(Bounding Box)를 매칭한 것이다.</li>
<li><strong>Reason-to-Ground (RoG) 방법론:</strong> 단순한 물체 검출을 넘어, VLM이 먼저 “사용자가 왜 이런 행동을 하는가?“를 추론(Reasoning)하고, 그 결과를 바탕으로 물체의 위치를 특정(Grounding)하도록 훈련시키는 기법이다. 이는 기존 모델 대비 의도 파악 정확도를 획기적으로 높였다.33</li>
</ul>
<h2>7.  기타 주요 연구 및 동향</h2>
<h3>7.1  IEEE Transactions on Robotics (T-RO)</h3>
<p>2025년 8월호 T-RO에는 로봇의 기초 제어 이론과 센서 융합에 관한 다수의 논문이 게재되었다.35</p>
<ul>
<li><strong>햅틱 탐색 및 대칭 감지:</strong> 로봇 손이 물체를 만지며 모양을 파악할 때, 물체의 대칭성(Symmetry)을 능동적으로 감지하여 탐색 효율을 높이는 알고리즘이 발표되었다.36</li>
<li><strong>유연 관절 로봇 제어:</strong> 탄성 관절(Elastic Joint)을 가진 로봇의 진동을 억제하고 정밀도를 높이기 위한 ‘수동적 가상 점성 요소 주입(Passive Virtual Viscous Element Injection)’ 기법이 소개되었다.35</li>
</ul>
<h3>7.2  Nature Machine Intelligence</h3>
<ul>
<li><strong>시스템 신뢰와 AI:</strong> 모로코의 소규모 농업 환경에서 AI 기술이 도입될 때 발생하는 사회적 신뢰(System Trust)의 역학을 분석한 연구가 게재되었다. 이는 기술 도입 시 기술적 성능뿐만 아니라 해당 사회의 문화적, 제도적 맥락을 고려해야 함을 시사한다.37</li>
</ul>
<h2>8.  결론</h2>
<p>2025년 8월의 로봇 공학 연구들은 **“물리적 지능의 심화”**와 **“가상과 현실의 융합”**이라는 두 가지 큰 흐름으로 요약된다.</p>
<ol>
<li><strong>언어에서 행동으로 (Language to Action):</strong> LLM과 VLM은 이제 단순한 챗봇을 넘어, 자율주행차의 핸들을 꺾고(COVLM-RL), 로봇 팔이 어떤 물체를 잡아야 할지 결정(Visual Intention Grounding)하는 핵심 두뇌로 자리 잡았다.</li>
<li><strong>전신을 활용한 상호작용 (Whole-body Interaction):</strong> 로봇은 더 이상 환경과의 접촉을 피하지 않는다. Science Robotics의 연구는 로봇이 유연한 신체를 이용해 환경과 적극적으로 접촉하며, 인간처럼 온몸을 써서 작업을 수행하는 시대를 예고했다.</li>
<li><strong>생성형 시뮬레이션의 대중화:</strong> Genie 3와 같은 월드 모델은 로봇 연구자들에게 무한한 훈련 공간을 제공함으로써, 데이터 부족 문제를 해결하고 로봇 지능의 발전을 가속화할 기폭제가 될 것이다.</li>
<li><strong>인간 중심의 정서 로봇:</strong> RO-MAN 2025의 수상작들은 로봇 기술의 궁극적인 지향점이 인간의 정서적 안녕과 삶의 질 향상에 있음을 다시 한번 확인시켜 주었다.</li>
</ol>
<p>이러한 기술적 진보는 로봇이 공장이라는 울타리를 넘어, 교실, 병원, 거리, 그리고 우리의 가정으로 깊숙이 들어오고 있음을 알리는 신호탄이라 할 수 있다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>Program- 2025 IEEE 21st International Conference on Automation …, https://www.aconf.org/conf_195993/program.html</li>
<li>CASE 2025 Conference Program, https://ras.papercept.net/conferences/conferences/CASE25/program/</li>
<li>Robotics and Automation Archives - UC Berkeley IEOR Department, https://ieor.berkeley.edu/category/publications/robotics-and-automation/</li>
<li>Professor Ken Goldberg and AUTOLAB Win IEEE CASE Best Paper …, https://ieor.berkeley.edu/professor-ken-goldberg-and-autolab-win-best-paper/</li>
<li>CASE 2025 Program | Program at a Glance, https://ras.papercept.net/conferences/conferences/CASE25/program/CASE25_ProgramAtAGlanceWeb.html</li>
<li>Siyang Gao - City University of Hong Kong, https://www.cityu.edu.hk/stfprofile/siyangao_FIL_1_pp4hcp6e.PDF</li>
<li>Prof. Dr. Ziyue Li - TUM School of Management, https://www.mgt.tum.de/professors-1/info/ziyue-li</li>
<li>Qiang Huang – Daniel J. Epstein Department of Industrial and …, https://huanglab.usc.edu/</li>
<li>Bonald Ziyue LI, https://bonaldli.github.io/</li>
<li>Awards - Chair of Media Technology, https://www.ce.cit.tum.de/en/lmt/research/awards/</li>
<li>IEEE RO-MAN 2025 : 34th IEEE International Conference on Robot …, http://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=183042</li>
<li>Awards - IEEE RO-MAN 2025, https://www.ro-man2025.org/awards</li>
<li>Developing Robot-Enhanced Social-Emotional Therapy (RESET) for …, https://scazlab.yale.edu/sites/default/files/files/ramnauth-20250617-f2.pdf</li>
<li>3D Path Control: Can we use lower limb inter-joint coordination to …, https://www.researchgate.net/publication/397229685_3D_Path_Control_Can_we_use_lower_limb_inter-joint_coordination_to_assist_gait_and_balance</li>
<li>(PDF) Path Control: A Method for Patient-Cooperative Robot-Aided …, https://www.researchgate.net/publication/41656189_Path_Control_A_Method_for_Patient-Cooperative_Robot-Aided_Gait_Rehabilitation</li>
<li>Robot Design Competition (Copy) - IEEE RO-MAN 2025, https://www.ro-man2025.org/robot-design-competition-1</li>
<li>Learning contact-rich whole-body manipulation with … - Alex Alspach, <a href="https://www.alexalspach.com/assets/files/papers/2025-Science_Robotics-Learning_contact_%C2%AD%20rich_whole_%C2%AD%20body_manipulation.pdf">https://www.alexalspach.com/assets/files/papers/2025-Science_Robotics-Learning_contact_%C2%AD%20rich_whole_%C2%AD%20body_manipulation.pdf</a></li>
<li>Learning contact-rich whole-body manipulation with example …, https://pubmed.ncbi.nlm.nih.gov/40834065/</li>
<li>Learning contact-rich whole-body manipulation with example …, <a href="https://www.semanticscholar.org/paper/Learning-contact-rich-whole-body-manipulation-with-Barreiros-%C3%96nol/2491305a3fac35ae4efffd1f9c833c3fd8808e88">https://www.semanticscholar.org/paper/Learning-contact-rich-whole-body-manipulation-with-Barreiros-%C3%96nol/2491305a3fac35ae4efffd1f9c833c3fd8808e88</a></li>
<li>Learning contact-rich whole-body manipulation with example …, https://www.researchgate.net/publication/394791595_Learning_contact-rich_whole-body_manipulation_with_example-guided_reinforcement_learning</li>
<li>Embracing Bulky Objects with Humanoid Robots - arXiv, https://arxiv.org/html/2509.13534v1</li>
<li>Breakthrough AI research - Google AI, https://ai.google/research/</li>
<li>Genie 3: A New Frontier for World Models (Google DeepMind), https://opencv.org/blog/genie-3/</li>
<li>Genie 3: Google DeepMind reinvents interactive AI - Dataïads, https://www.dataiads.io/en/blog/genie-3-tout-ce-que-vous-devez-savoir-sur-le-nouveau-modele-dia-interactif-de-google-deepmind</li>
<li>Genie 3: My Deep Dive into Google’s Real-Time AI World Builder, <a href="https://skywork.ai/skypage/en/Genie-3-My-Deep-Dive-into-Google%E2%80%99s-Real-Time-AI-World-Builder/1975582077712658432">https://skywork.ai/skypage/en/Genie-3-My-Deep-Dive-into-Google%E2%80%99s-Real-Time-AI-World-Builder/1975582077712658432</a></li>
<li>Getting Started with Genie 3: A Beginner’s Guide | Genie3.org, https://genie3.org/blog/getting-started-with-genie-3.html</li>
<li>Google DeepMind’s Genie 3: The Ultimate AI World-Building …, https://www.ainewshub.org/post/google-deepmind-s-genie-3-the-ultimate-ai-world-building-revolution-unleashed-in-2025</li>
<li>Genie 3: The Complete Technical Guide to DeepMinds … - Cursor IDE, https://www.cursor-ide.com/blog/genie-3-ai-model</li>
<li>SIMA 2: A Generalist Embodied Agent for Virtual Worlds - arXiv, https://arxiv.org/html/2512.04797v1</li>
<li>Awards - IEEE ITSC 2025, https://ieee-itsc.org/2025/program/awards/</li>
<li>COVLM-RL: Critical Object-Oriented Reasoning for Autonomous …, https://arxiv.org/html/2512.09349v1</li>
<li>Visual Intention Grounding for Egocentric Assistants - arXiv, https://arxiv.org/abs/2504.13621</li>
<li>Visual Intention Grounding for Egocentric Assistants, https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_Visual_Intention_Grounding_for_Egocentric_Assistants_ICCV_2025_paper.pdf</li>
<li>Visual Intention Grounding for Egocentric Assistants - arXiv, https://arxiv.org/html/2504.13621v1</li>
<li>Top 239 IEEE Transactions on Robotics papers published in 2025, https://scispace.com/journals/ieee-transactions-on-robotics-1ia5nh9j/2025</li>
<li>Lorenzo Jamone | Publications | University College London, https://profiles.ucl.ac.uk/102308-lorenzo-jamone/publications</li>
<li>System trust and AI between second nature and traditional society, https://www.emerald.com/k/article/doi/10.1108/K-05-2025-1274/1311798/System-trust-and-AI-between-second-nature-and</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>