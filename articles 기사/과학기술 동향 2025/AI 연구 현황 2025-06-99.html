<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2025년 6월 AI 연구 현황</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2025년 6월 AI 연구 현황</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2025년 AI 및 로봇 연구 동향</a> / <span>2025년 6월 AI 연구 현황</span></nav>
                </div>
            </header>
            <article>
                <h1>2025년 6월 AI 연구 현황</h1>
<h1>2025년 6월 AI 연구 심층 보고서: 물리적 정합성, 효율적 구조, 그리고 시스템 안전성</h1>
<h2>1.  서론: 인공지능 연구의 새로운 이정표</h2>
<p>2025년 6월은 인공지능(AI) 연구의 역사적 흐름에 있어 중대한 변곡점으로 기록될 전망이다. 생성형 AI(Generative AI)의 폭발적인 확산 이후, 학계와 산업계는 이제 ’생성’을 넘어 ’이해’와 ‘효율’, 그리고 ’물리적 세계와의 정합성’이라는 더 본질적이고 난도 높은 과제에 집중하고 있다. 테네시주 내슈빌에서 개최된 **CVPR 2025 (Conference on Computer Vision and Pattern Recognition)**는 이러한 흐름을 가장 극명하게 보여주었다. 컴퓨터 비전 기술은 단순한 이미지 인식을 넘어, 복잡한 3차원 공간을 실시간으로 재구성하고 동적인 물체의 움직임을 물리 법칙에 기반하여 추론하는 단계로 진입했다.</p>
<p>동시에 자연어 처리(NLP)와 시스템 아키텍처 분야에서는 거대 언어 모델(LLM)의 비효율성을 극복하기 위한 혁신적인 시도들이 구체화되었다. <strong>Mamba-2</strong>와 <strong>SimPO</strong>, <strong>MatMul-free</strong> 모델 등의 등장은 트랜스포머(Transformer) 일변도의 아키텍처에서 벗어나 연산 효율성과 추론 속도를 극대화하려는 ‘포스트 트랜스포머(Post-Transformer)’ 시대의 서막을 알리고 있다. 또한, OpenAI와 Google DeepMind가 발표한 최신 보안 및 시스템 최적화 연구들은 AI 기술이 실험실을 벗어나 실제 사회 인프라와 산업 현장에 안전하게 통합되기 위해 필수적인 방어 기제와 운영 기술을 제공하고 있다.</p>
<p>본 보고서는 2025년 6월에 발표된 주요 연구 성과들을 망라하여 심층적으로 분석한다. 각 논문의 기술적 방법론(Methodology)과 실험 결과(Experiments)를 상세히 해부하고, 이러한 연구들이 제시하는 거시적인 기술 트렌드와 미래 전망을 도출한다. 보고서는 크게 컴퓨터 비전의 기하학적 혁신, 언어 모델 아키텍처의 효율화, 멀티모달 이해의 확장, 그리고 AI 안전성 및 시스템 관리는 네 가지 핵심 축으로 구성된다.</p>
<h2>2.  CVPR 2025: 물리적 세계로 확장된 비전 지능</h2>
<p>CVPR 2025는 역대 가장 치열한 경쟁 속에서 컴퓨터 비전의 근본적인 난제들을 해결하려는 시도들이 주목받았다. 특히 2024년까지 주류를 이루었던 2D 생성 모델의 연구가 3D 기하학(Geometry) 및 물리 기반 렌더링(Physics-based Rendering)과 결합하여 ’물리적 실체감(Physical Grounding)’을 확보하려는 경향이 뚜렷했다.</p>
<h3>2.1  기하학적 추론의 패러다임 전환: VGGT</h3>
<p>논문 제목: VGGT: Visual Geometry Grounded Transformer 1</p>
<p>저자: Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, David Novotny</p>
<p>수상: CVPR 2025 Best Paper Award</p>
<h4>2.1.1  연구 배경 및 문제 의식</h4>
<p>전통적인 3차원 장면 재구성(3D Scene Reconstruction) 및 SfM(Structure from Motion) 기술은 수십 년간 **번들 조정(Bundle Adjustment)**이라는 최적화 기법에 의존해 왔다. 이 방식은 카메라 포즈와 3D 포인트의 위치를 반복적으로 미세 조정하여 높은 정확도를 달성하지만, 계산 비용이 기하급수적으로 증가하고 초기값에 민감하다는 치명적인 단점이 있었다. 최근 등장한 딥러닝 기반 방법론들(예: DUSt3R) 또한 쌍(Pairwise)별 추론에 의존하거나 복잡한 후처리를 요구하여 실시간 애플리케이션에 적용하기에는 한계가 있었다.</p>
<h4>2.1.2  핵심 방법론: 피드포워드(Feed-forward) 기하학 추론</h4>
<p>VGGT는 이러한 한계를 극복하기 위해, 입력된 다수의 이미지로부터 장면의 모든 핵심 3D 속성(카메라 파라미터, 깊이 지도, 포인트 맵, 3D 포인트 트랙)을 **단일 패스(Single-pass)**로 즉각 추론하는 피드포워드 신경망을 제안했다. 이는 트랜스포머 아키텍처가 대규모 데이터 학습을 통해 3차원 기하학의 내재적 규칙을 학습할 수 있다는 가설에 기반한다.</p>
<p>VGGT의 아키텍처는 표준적인 대형 트랜스포머에 기반하되, ’Visual Geometry Grounded’라는 명칭처럼 어텐션 메커니즘이 기하학적 관계를 명시적으로 처리하도록 유도한다. 특히 주목할 점은 2개 이상의 이미지를 동시에 처리할 수 있어, 기존의 쌍별(Pairwise) 재구성을 넘어 전역적인 문맥(Global Context)을 고려한 3D 구조 생성이 가능하다는 것이다.</p>
<h4>2.1.3  기술적 세부 사항: 다중 작업 손실 함수</h4>
<p>VGGT의 학습은 카메라 포즈, 깊이, 트래킹 정확도를 동시에 최적화하는 복합적인 손실 함수에 의해 주도된다. 이를 수식으로 표현하면 다음과 같다.<br />
<span class="math math-display">
\boxed{ \mathcal{L}_{total} = \lambda_{cam}\mathcal{L}_{camera} + \lambda_{depth}\mathcal{L}_{depth} + \lambda_{track}\mathcal{L}_{track} }
</span></p>
<p>각 구성 요소에 대한 상세 분석은 다음과 같다:</p>
<ol>
<li>
<p><strong>카메라 손실 (<span class="math math-inline">\mathcal{L}_{camera}</span>)</strong>: 모델이 예측한 카메라 파라미터가 실제값(Ground Truth)과 일치하도록 유도한다. 이때 이상치(Outlier)에 대한 민감도를 줄이기 위해 L2 손실 대신 <strong>Huber Loss</strong>를 사용한다. 회전(Rotation) 오차는 쿼터니언(Quaternion) 공간에서 계산된다.<br />
<span class="math math-display">
\Delta C = \Vert C_{pred} - C_{gt} \Vert_{\epsilon} </span>   <span class="math math-display"> \mathbf{q}_{R} = \mathbf{q}_{E}^{-1} \mathbf{q}_{GT}
</span><br />
여기서 <span class="math math-inline">\mathbf{q}_{R}</span>은 예측된 회전과 실제 회전 사이의 상대적인 회전 차이를 나타내며, 이 값이 단위 쿼터니언(항등원)에 가까워지도록 학습된다.</p>
</li>
<li>
<p><strong>깊이 손실 (<span class="math math-inline">\mathcal{L}_{depth}</span>)</strong>: 단순한 픽셀별 깊이 오차를 넘어, **불확실성(Aleatoric Uncertainty)**을 모델링한다. 이는 텍스처가 없거나(Textureless) 반복적인 패턴이 있는 영역에서 모델이 자신의 예측에 대한 확신도를 낮추게 하여 전체적인 학습 안정성을 높인다. 또한, 깊이의 급격한 변화(Edge)를 정확히 포착하기 위해 그래디언트 손실 항을 포함한다.<br />
<span class="math math-display">
\mathcal{L}_{depth} = \sum_{i} \left( \frac{\Vert D_i - \hat{D}_i \Vert_1}{\Sigma_{D}} + \log \Sigma_{D} \right) + \lambda_{grad} \Vert \nabla D_i - \nabla \hat{D}_i \Vert_1
</span><br />
<span class="math math-inline">\Sigma_{D}</span>는 모델이 예측한 깊이 불확실성 맵이다. 불확실성이 높은 영역에서는 분모가 커져 오차의 가중치가 줄어들고, 대신 <span class="math math-inline">\log \Sigma_{D}</span> 항이 불확실성을 무한정 키우는 것을 억제한다.7</p>
</li>
<li>
<p><strong>트래킹 손실 (<span class="math math-inline">\mathcal{L}_{track}</span>)</strong>: 시간적 일관성을 확보하기 위해 도입되었다. 이는 동일한 3D 포인트가 서로 다른 시점의 이미지에서 올바르게 매칭되도록 강제하며, <span class="math math-inline">\lambda_{track}=0.05</span>의 가중치로 적용된다.</p>
</li>
</ol>
<h4>2.1.4 실험 결과 및 성능 분석</h4>
<p>VGGT는 다양한 벤치마크에서 기존 최적화 기반 방법론들을 압도하거나 대등한 성능을 보였다. 특히 주목할 만한 점은 사후 최적화 없이도 높은 정확도를 달성했다는 것이다.</p>
<table><thead><tr><th style="text-align: left">데이터셋</th><th style="text-align: left">평가 지표</th><th style="text-align: left">방법론</th><th style="text-align: left">점수</th><th style="text-align: left">비고</th></tr></thead><tbody>
<tr><td style="text-align: left">RealEstate10K</td><td style="text-align: left">AUC@30</td><td style="text-align: left">VGGT (Feed-Forward)</td><td style="text-align: left">85.3</td><td style="text-align: left">최적화 없음</td></tr>
<tr><td style="text-align: left"></td><td style="text-align: left"></td><td style="text-align: left">VGGT (with BA)</td><td style="text-align: left">93.5</td><td style="text-align: left">사후 최적화 적용 시</td></tr>
<tr><td style="text-align: left">CO3Dv2</td><td style="text-align: left">AUC@30</td><td style="text-align: left">VGGT (Feed-Forward)</td><td style="text-align: left">88.2</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"></td><td style="text-align: left"></td><td style="text-align: left">VGGT (with BA)</td><td style="text-align: left">91.8</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">ETH3D</td><td style="text-align: left">Point Accuracy</td><td style="text-align: left">VGGT</td><td style="text-align: left">0.901</td><td style="text-align: left">DUSt3R 대비 우수</td></tr>
</tbody></table>
<p>위 표에서 볼 수 있듯이, VGGT는 단일 패스 추론만으로도 RealEstate10K에서 85.3의 높은 점수를 기록했다.10 이는 번들 조정과 같은 값비싼 연산이 실시간 애플리케이션에서는 선택 사항이 될 수 있음을 시사한다. 또한, 관련 연구 9에 따르면 VGGT는 명시적인 이상치 제거 모듈 없이도 내부 어텐션 맵을 통해 기하학적으로 일관되지 않은 뷰(Distractor View)를 스스로 걸러내는 창발적(Emergent) 능력을 보여주었다. 이는 모델이 단순히 데이터를 외우는 것이 아니라 3차원 공간 구조를 ’이해’하고 있음을 강력하게 뒷받침한다.</p>
<h3>2.2 빛의 시간을 다루다: 신경 역렌더링(Neural Inverse Rendering)</h3>
<p>논문 제목: Neural Inverse Rendering from Propagating Light 2</p>
<p>저자: Anagh Malik, Benjamin Attal, Andrew Xie, Matthew O’Toole, David B. Lindell</p>
<p>수상: CVPR 2025 Best Student Paper Award</p>
<h4>2.2.1 연구 배경: 시간 분해(Time-Resolved) 이미징의 부상</h4>
<p>기존의 컴퓨터 비전과 렌더링은 빛의 속도를 무한대로 가정하고, 센서에 도달하는 빛의 총량(Intensity)만을 다루었다. 그러나 SPAD(Single-Photon Avalanche Diode)와 같은 초고속 센서의 발전으로 빛의 비행 시간(Time-of-Flight)을 나노초 단위로 측정하는 것이 가능해졌다. 이 논문은 이러한 ‘시간 분해 측정’ 데이터를 활용하여, 일반적인 카메라로는 볼 수 없는 숨겨진 기하학적 구조나 복잡한 산란 현상을 역으로 추적하는 기술을 제안했다.</p>
<h4>2.2.2 핵심 방법론: 시간 차원 렌더링 방정식</h4>
<p>저자들은 고전적인 렌더링 방정식을 시간 축으로 확장하여, 빛의 전파 지연을 명시적으로 모델링했다. 특정 시점 <span class="math math-inline">t</span>에 위치 <span class="math math-inline">\mathbf{x}(t)</span>에서 방향 <span class="math math-inline">\boldsymbol{\omega}_o</span>로 나가는 빛의 방사 휘도 <span class="math math-inline">L_o</span>는 다음과 같이 시간 지연 <span class="math math-inline">\tau</span>를 포함하여 정의된다.<br />
<span class="math math-display">
\boxed{ L_o(\mathbf{x}(t), \boldsymbol{\omega}_o, \tau) = \int_{\Omega} f(\mathbf{x}(t), \boldsymbol{\omega}_i, \boldsymbol{\omega}_o) L_i(\mathbf{x}(t), \boldsymbol{\omega}_i, \tau) (\mathbf{n} \cdot \boldsymbol{\omega}_i) d\boldsymbol{\omega}_i }
</span><br />
여기서 <span class="math math-inline">L_i(\mathbf{x}(t), \boldsymbol{\omega}_i, \tau)</span>는 시간 <span class="math math-inline">\tau</span>에 도달한 입사광을 의미하며, 이는 광원에서 출발하여 다른 표면들을 거쳐 도달하기까지의 경로 길이에 비례하는 시간 지연을 포함한다.</p>
<h4>2.2.3 신경 방사 캐시(Neural Radiance Cache)를 이용한 효율화</h4>
<p>이 논문의 가장 혁신적인 부분은 물리적으로 정확한 1차 반사(Primary Bounce) 계산과 신경망을 이용한 간접광(Indirect Illumination) 근사의 결합이다. 다중 반사(Inter-reflection)를 물리적으로 시뮬레이션하는 것은 계산 비용이 매우 높기 때문에, 저자들은 **시간 분해 신경 방사 캐시(Time-resolved Neural Radiance Cache)**를 도입했다.</p>
<p>이 캐시는 NeRF(Neural Radiance Fields)와 유사한 방식으로 작동하지만, 시간 지연 정보를 포함하여 볼륨 렌더링을 수행한다.<br />
<span class="math math-display">
L_i(\mathbf{o}&#39;, \boldsymbol{\omega}_i, \tau) = \sum_{k=1}^{N} w_k L_{cache}\left(\mathbf{x}&#39;(t_k), \boldsymbol{\omega}&#39;_o, \tau - \frac{t_k}{c}\right)
</span><br />
여기서 <span class="math math-inline">c</span>는 빛의 속도이며, <span class="math math-inline">t_k</span>는 2차 광선(Secondary Ray)을 따라 이동한 거리이다. 이 수식은 특정 지점에 도달하는 간접광이, 과거의 어느 시점에 다른 표면에서 방출된 빛들의 합임을 나타낸다. 이를 통해 복잡한 장면의 형상뿐만 아니라 재질(BRDF) 속성까지 미분 가능한 방식으로 동시에 최적화할 수 있게 되었다.</p>
<hr />
<h3>2.3 동적 환경에서의 강건함: MegaSaM</h3>
<p>논문 제목: MegaSaM: Accurate, Fast and Robust Structure and Motion from Casual Dynamic Videos 15</p>
<p>저자: Zhengqi Li, Richard Tucker, et al. (Google DeepMind &amp; UC Berkeley)</p>
<p>수상: Best Paper Honorable Mention</p>
<h4>2.3.1 동적 SLAM의 난제</h4>
<p>SLAM(Simultaneous Localization and Mapping) 기술은 자율주행이나 로봇 공학의 핵심이지만, 대부분의 알고리즘은 ’세상이 정지해 있다(Static Assumption)’는 가정하에 작동한다. 움직이는 사람, 자동차, 흔들리는 나무 등은 SLAM 시스템에 노이즈로 작용하여 카메라 추적을 실패하게 만들거나 3D 지도를 왜곡시킨다. MegaSaM은 일반 사용자가 스마트폰으로 촬영한 흔들리고 동적인 비디오에서도 강건하게 작동하는 시스템을 목표로 개발되었다.</p>
<h4>2.3.2 핵심 기술: 움직임 확률 지도와 적응형 최적화</h4>
<p>MegaSaM의 핵심은 정적인 배경과 동적인 객체를 명확히 구분하고, 이를 최적화 과정에 반영하는 것이다.</p>
<ol>
<li>
<p><strong>움직임 확률 지도 (Motion Probability Map):</strong> 모델은 각 픽셀이 동적 객체에 속할 확률 <span class="math math-inline">m_i</span>를 예측한다. 이 확률 맵은 번들 조정(Bundle Adjustment) 과정에서 가중치로 사용되어, 움직이는 물체가 카메라 포즈 추정에 미치는 악영향을 최소화한다.<br />
<span class="math math-display">
\tilde{w}_{ij} = \hat{w}_{ij} m_i
</span><br />
여기서 <span class="math math-inline">\hat{w}_{ij}</span>는 광학 흐름(Optical Flow)의 신뢰도이며, <span class="math math-inline">m_i</span>는 해당 픽셀의 정적일 확률이다.</p>
</li>
<li>
<p><strong>2단계 학습 전략 (Two-stage Training):</strong></p>
<ul>
<li><strong>1단계 (Ego-motion Pretraining):</strong> 정적인 합성 데이터셋을 사용하여 기본적인 카메라 움직임과 흐름(Flow) 추정 능력을 학습한다.</li>
<li><strong>2단계 (Dynamic Fine-tuning):</strong> 동적인 객체가 포함된 데이터셋에서 움직임 확률 지도 생성 모듈을 미세 조정한다. 이때 기존 SLAM 네트워크의 파라미터는 고정(Freeze)하여, 기하학적 추론 능력이 손상되는 것을 방지한다.15</li>
</ul>
</li>
<li>
<p><strong>불확실성 인지 글로벌 BA (Uncertainty-aware Global BA):</strong> 카메라의 움직임이 적거나 특징점이 부족하여 깊이 추정이 불확실한 경우(Poor Observability), 시스템은 이를 감지하고 단안 깊이 추정(Monocular Depth) 모델(DepthAnything 등)의 사전 정보를 적응적으로 활용하여 해를 안정화한다.</p>
</li>
</ol>
<h4>2.3.3 성능 평가 및 비교</h4>
<p>MegaSaM은 동적 환경을 가정한 다양한 벤치마크에서 기존 최고 성능 모델(SOTA)들을 압도했다.</p>
<table><thead><tr><th style="text-align: left">데이터셋</th><th style="text-align: left">지표</th><th style="text-align: left">MegaSaM</th><th style="text-align: left">CasualSaM</th><th style="text-align: left">LEAP-VO</th><th style="text-align: left">DepthAnything-v2</th></tr></thead><tbody>
<tr><td style="text-align: left">MPI Sintel</td><td style="text-align: left">ATE (Trajectory Error)</td><td style="text-align: left">0.018</td><td style="text-align: left">0.036</td><td style="text-align: left">0.041</td><td style="text-align: left">-</td></tr>
<tr><td style="text-align: left">MPI Sintel</td><td style="text-align: left">Abs Rel Error (Depth)</td><td style="text-align: left">0.21</td><td style="text-align: left">-</td><td style="text-align: left">-</td><td style="text-align: left">0.37</td></tr>
<tr><td style="text-align: left">처리 속도</td><td style="text-align: left">FPS / Runtime</td><td style="text-align: left">~1.0 sec/video</td><td style="text-align: left">1.6 min</td><td style="text-align: left">-</td><td style="text-align: left">-</td></tr>
</tbody></table>
<p>위 표에서 볼 수 있듯이, MegaSaM은 궤적 오차를 경쟁 모델 대비 절반 수준으로 줄였으며, 처리 속도 면에서는 수십 배에서 수백 배의 향상을 이루었다.17 이는 딥러닝 기반의 최적화 레이어가 얼마나 강력할 수 있는지를 보여주는 사례이다.</p>
<h2>3. 언어 모델의 효율성 혁명: 아키텍처와 최적화</h2>
<p>컴퓨터 비전이 물리적 정확도를 향해 나아갔다면, 자연어 처리(NLP) 분야는 2025년 6월을 기점으로 **‘효율성(Efficiency)’**이라는 화두에 천착했다. 수조 개의 파라미터를 가진 거대 모델의 운영 비용과 에너지 소비가 한계에 다다르면서, 더 적은 자원으로 더 높은 성능을 내기 위한 아키텍처적 혁신이 쏟아져 나왔다.</p>
<h3>3.1 아키텍처의 진화: Mamba-2와 SSM</h3>
<p>관련 논문: Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality 21</p>
<p>저자: Tri Dao, Albert Gu</p>
<h4>3.1.1 트랜스포머의 한계와 SSM의 부상</h4>
<p>트랜스포머 아키텍처의 핵심인 어텐션 메커니즘은 입력 시퀀스 길이의 제곱(<span class="math math-inline">O(N^2)</span>)에 비례하는 연산량을 요구한다. 이는 긴 문맥을 처리하는 데 있어 치명적인 병목 현상을 야기했다. 이에 대한 대안으로 선형 시간 복잡도(<span class="math math-inline">O(N)</span>)를 가지는 **상태 공간 모델(State Space Models, SSM)**이 주목받았으나, 학습 효율성과 성능 면에서 트랜스포머에 미치지 못했다.</p>
<h4>3.1.2 SSD 이론과 Mamba-2</h4>
<p>Mamba-2는 **구조적 상태 공간 듀얼리티(Structured State Space Duality, SSD)**라는 새로운 이론적 틀을 제시했다. 저자들은 특정 조건 하에서 SSM의 순환(Recurrent) 연산과 트랜스포머의 어텐션 연산이 수학적으로 등가임을 증명했다.</p>
<p>SSM의 행렬 연산은 ‘세미-분리 가능(Semi-separable)’ 행렬로 표현될 수 있으며, 이는 마스크된 어텐션(Masked Attention) 행렬과 구조적으로 유사하다. Mamba-2는 이 듀얼리티를 활용하여 SSM을 GPU의 **텐서 코어(Tensor Core)**에 최적화된 행렬 곱 연산으로 변환할 수 있게 설계되었다.</p>
<p>Mamba-2의 상태 업데이트 수식은 입력에 따라 동적으로 변화하는 파라미터 <span class="math math-inline">\mathbf{A}, \mathbf{B}, \mathbf{C}</span>를 포함한다.<br />
<span class="math math-display">
\mathbf{h}_t = \mathbf{A}_t \mathbf{h}_{t-1} + \mathbf{B}_t \mathbf{x}_t
</span></p>
<p><span class="math math-display">
\mathbf{y}_t = \mathbf{C}_t \mathbf{h}_t
</span></p>
<p>Mamba-2는 시퀀스를 덩어리(Chunk)로 나누어 병렬 처리함으로써, 기존 Mamba 모델 대비 학습 속도를 비약적으로 향상시켰으며, 트랜스포머와 SSM의 장점만을 결합한 하이브리드 아키텍처의 가능성을 열었다.22</p>
<h3>3.2 연산의 경량화: 행렬 곱 없는(MatMul-free) 언어 모델</h3>
<p>논문 제목: Scalable MatMul-free Language Modeling 24</p>
<p>저자: Rui-Jie Zhu et al.</p>
<h4>3.2.1 행렬 곱의 비용</h4>
<p>현대 딥러닝 모델의 연산 비용 중 대부분은 거대 행렬 곱(Matrix Multiplication, MatMul)이 차지한다. 이는 GPU 전력 소모의 주범이다. 이 연구는 “과연 언어 모델에 정밀한 행렬 곱이 반드시 필요한가?“라는 급진적인 질문에서 출발했다.</p>
<h4>3.2.2 3진 가중치(Ternary Weights)와 덧셈 연산</h4>
<p>저자들은 모델의 가중치를 <span class="math math-inline">{-1, 0, 1}</span>의 3진값으로 양자화(Quantization)하고, 고비용의 곱셈 연산을 **덧셈(Addition)**과 <strong>비트 시프트(Bit-shift)</strong> 연산으로 대체하는 아키텍처를 제안했다.</p>
<p>이러한 접근법은 FPGA나 뉴로모픽(Neuromorphic) 하드웨어와 같은 특수 목적 하드웨어에서 극도의 효율성을 발휘한다. 실험 결과, 수십억 파라미터(Billion-scale) 규모의 모델에서도 기존 트랜스포머 대비 성능 저하를 최소화하면서 전력 효율을 수십 배 향상시킬 수 있음을 입증했다.24 이는 엣지 디바이스(Edge Device)에서 고성능 LLM을 구동할 수 있는 길을 열어준 혁신적인 연구이다.</p>
<h3>3.3 학습의 최적화: SimPO</h3>
<p>논문 제목: SimPO: Simple Preference Optimization with a Reference-Free Reward 27</p>
<p>저자: Princeton NLP Group</p>
<h4>3.3.1 DPO의 메모리 병목</h4>
<p>인간의 선호도에 맞춰 모델을 미세 조정하는 RLHF(Reinforcement Learning from Human Feedback) 과정에서, 최근에는 DPO(Direct Preference Optimization)가 표준으로 자리 잡았다. 그러나 DPO는 학습 중에 참조 모델(Reference Model, 초기 SFT 모델)을 메모리에 계속 로드하고 있어야 하므로 메모리 사용량이 두 배로 늘어나는 문제가 있었다.</p>
<h4>3.3.2 참조 모델 없는(Reference-Free) 최적화</h4>
<p>SimPO는 참조 모델을 제거하고, 생성된 텍스트의 평균 로그 확률(Average Log-probability) 자체를 보상 신호로 사용하여 이를 최적화한다. 핵심은 승리한 응답(<span class="math math-inline">y_w</span>)과 패배한 응답(<span class="math math-inline">y_l</span>) 사이에 **목표 마진(Target Reward Margin, <span class="math math-inline">\gamma</span>)**을 도입하여 확실한 차별화를 유도하는 것이다.</p>
<p>SimPO의 목적 함수는 다음과 같다.<br />
<span class="math math-display">
\boxed{ \mathcal{L}_{SimPO}(\pi_\theta) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \frac{\beta}{\gamma} ( \log \pi_\theta(y_w|x) - \log \pi_\theta(y_l|x) ) - \beta \right) \right] }
</span><br />
여기서 <span class="math math-inline">\beta</span>는 보상의 스케일을 조절하는 하이퍼파라미터이다.</p>
<h4>3.3.3 성능 비교</h4>
<p>AlpacaEval 2 벤치마크에서의 실험 결과는 SimPO의 우수성을 증명한다.</p>
<p>| 모델 (Base: Llama-3-8B-Instruct) | 방법론 | 승률 (Win Rate, %) | 비고 |</p>
<p>| :— | :— | :— | :— |</p>
<p>| Llama-3-Instruct-8B | - | 26.0 | 베이스라인 |</p>
<p>| Llama-3-Instruct-8B | DPO | 40.3 | 참조 모델 필요 |</p>
<p>| Llama-3-Instruct-8B | SimPO | 44.7 | 참조 모델 불필요, 메모리 절약 |</p>
<p>SimPO는 DPO보다 더 높은 성능을 기록하면서도 메모리 사용량은 획기적으로 줄여, 제한된 자원 하에서도 고성능 모델을 튜닝할 수 있는 최적의 방법론으로 떠올랐다.28</p>
<h2>4. 멀티모달 이해의 확장: Video-MME</h2>
<p>논문 제목: Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis 4</p>
<p>저자: Chaoyou Fu et al.</p>
<h3>4.1 ‘진짜’ 비디오 이해를 향하여</h3>
<p>지금까지의 비디오 이해(Video Understanding) 연구는 주로 수 초 분량의 짧은 클립을 분류하거나 캡션을 다는 데 그쳤다. 그러나 인간 수준의 비디오 이해는 영화 한 편을 보고 서사를 파악하거나, 1시간짜리 강의를 요약하는 등 <strong>장기적인 문맥(Long-term Context)</strong> 처리를 요구한다. Video-MME는 이러한 요구를 충족하기 위해 설계된 최초의 대규모 벤치마크이다.</p>
<h3>4.2 데이터셋의 구성과 특징</h3>
<p>Video-MME는 양과 질 모든 면에서 기존 벤치마크를 압도한다.</p>
<ul>
<li><strong>다양성 (Diversity):</strong> 지식, 영화/TV, 스포츠, 예술, 생활 기록, 다국어 등 6개 주요 도메인과 30개 세부 분야를 포괄한다.</li>
<li><strong>시간적 범위 (Duration):</strong> 단기(11초~2분), 중기(4~15분), 장기(30~60분) 비디오를 골고루 포함하여, 모델의 시간적 추론 능력을 단계별로 테스트한다.</li>
<li><strong>모달리티 통합 (Modality):</strong> 단순한 비디오 프레임뿐만 아니라, **자막(Subtitle)**과 <strong>오디오(Audio)</strong> 정보를 모두 제공하여 진정한 멀티모달 능력을 평가한다. 총 900개의 비디오와 2,700개의 전문가가 작성한 질의응답(QA) 쌍으로 구성된다.</li>
</ul>
<h3>4.3 평가 결과: 상용 모델의 독주와 데이터의 중요성</h3>
<p>Video-MME를 이용한 평가 결과는 현재 AI 모델들의 수준과 한계를 명확히 보여준다.</p>
<p>| 모델 | 전체 정확도 (%) | 비고 |</p>
<p>| :— | :— | :— |</p>
<p>| Gemini 1.5 Pro | 75.0 | 상용 모델 1위, 긴 문맥 처리에 강점 |</p>
<p>| GPT-4o | 71.9 | |</p>
<p>| InternVL-Chat-V1.5 | - | 오픈소스 모델 중 상위권 |</p>
<p>Gemini 1.5 Pro가 1위를 차지한 것은 해당 모델이 100만 토큰 이상의 긴 문맥 윈도우(Context Window)를 지원하기 때문으로 분석된다.31 특히 분석 결과, <strong>자막과 오디오 정보가 추가될수록 모델의 성능이 비약적으로 향상</strong>되는 경향을 보였다. 이는 비디오 이해에 있어 시각 정보만큼이나 언어적, 청각적 정보의 통합이 중요함을 시사한다. 긴 비디오(30분 이상)일수록 이러한 멀티모달 통합 능력의 차이가 모델 간의 성능 격차를 벌리는 결정적인 요인이었다.</p>
<h2>5. 소형 언어 모델(SLM): 에이전트 AI의 미래</h2>
<p>논문 제목: Small Language Models are the Future of Agentic AI 34</p>
<p>저자: Peter Belcak et al.</p>
<h3>5.1 “거거익선“에 대한 반론</h3>
<p>이 논문은 AI 업계의 지배적인 믿음인 “모델은 클수록 좋다(Bigger is Better)“에 대해 강력한 반론을 제기한다. 저자들은 수천억 개의 파라미터를 가진 LLM이 일반적인 대화형 AI에는 적합할지 몰라도, <strong>에이전트형 AI(Agentic AI)</strong> 시스템에는 비효율적이라고 주장한다. 에이전트는 사용자의 명령을 수행하기 위해 수십 번의 추론과 도구 호출(Tool Calling)을 반복해야 하는데, 이때 발생하는 **지연 시간(Latency)**과 **비용(Cost)**이 거대 모델에서는 감당할 수 없는 수준이 되기 때문이다.</p>
<h3>5.2 SLM의 기능적 충분성 (Functional Sufficiency)</h3>
<p>저자들은 Phi-3, Gemma-2 등 최신 SLM(Small Language Models, 통상 100억 파라미터 이하)들이 도구 사용, 지시 이행, 상식 추론 능력에서 이미 거대 모델과 대등하거나 충분한 수준에 도달했다고 분석한다.34 논문은 에이전트 시스템의 미래가 단일 거대 모델이 아닌, 특정 기능에 특화된 빠르고 가벼운 SLM들의 군집(Swarm)에 있다고 전망한다. 이는 벤처 캐피털(VC)의 자금 고갈과 맞물려, AI 서비스의 경제성을 확보하기 위한 필연적인 선택으로 제시된다.</p>
<h2>6. AI 시스템 안전 및 보안: 창과 방패의 공진화</h2>
<p>OpenAI와 Google DeepMind는 2025년 6월, AI 기술의 이면인 보안 위협과 이를 방어하기 위한 시스템적 노력을 담은 보고서들을 발표했다.</p>
<h3>6.1 OpenAI 보안 보고서: 악의적 사용의 저지</h3>
<p><strong>보고서 제목:</strong> Disrupting malicious uses of AI: June 2025 38</p>
<p>OpenAI의 보고서는 AI가 사이버 공격자들에게 ’마법 같은 새로운 무기’를 쥐여준 것이 아니라, 기존 공격의 **효율성(Productivity)**과 **품질(Quality)**을 높여주는 도구로 활용되고 있음을 지적했다.</p>
<ul>
<li><strong>주요 위협 사례:</strong>
<ul>
<li><strong>VAGue Focus:</strong> AI를 이용해 정교한 가짜 이력서와 포트폴리오를 대량 생산하고, 이를 통해 서방 기업의 원격 근무직에 위장 취업을 시도한 사례가 적발되었다. 이는 내부망 침투를 위한 고도화된 사회 공학적 기법이다.</li>
<li><strong>Helgoland Bite:</strong> 다국어 번역 및 텍스트 생성 능력을 악용하여 특정 정치적 메시지를 담은 소셜 미디어 게시물을 자동 생성하는 영향력 공작(Influence Operations)이 탐지되었다.</li>
</ul>
</li>
<li><strong>방어 전략:</strong> OpenAI는 이에 대응하여 자동 취약점 탐지 도구인 <strong>Aardvark</strong>를 개발하고, 이를 비영리 및 오픈소스 커뮤니티에 무료로 배포하여 방어자들의 역량을 강화하겠다는 계획을 밝혔다.39 이는 AI 모델이 공격자뿐만 아니라 방어자에게도 강력한 ’힘의 승수(Force Multiplier)’가 될 수 있음을 보여준다.</li>
</ul>
<h3>6.2 Google DeepMind: 시스템 성능 예측을 위한 AI</h3>
<p><strong>논문 제목:</strong> Performance Prediction for Large Systems via Text-to-Text Regression 43</p>
<p>Google DeepMind는 거대 언어 모델을 비정형 시스템 로그 분석에 활용하는 연구를 발표했다. Google의 CaaS(Compute-as-a-Service)와 같은 대규모 클라우드 시스템의 성능을 예측하는 것은 매우 복잡한 문제이다. 이 연구는 시스템 설정 파일이나 로그와 같은 텍스트 데이터를 T5와 같은 언어 모델에 입력하여, 시스템의 부하율이나 처리 속도와 같은 수치적 성능 지표를 **회귀 분석(Regression)**하는 방법을 제안했다.</p>
<p>이 접근법은 별도의 특징 공학(Feature Engineering) 없이도 텍스트 데이터만으로 시스템의 상태를 0.95 이상의 높은 상관계수로 예측해 냈으며, 모델이 자신의 예측 불확실성을 스스로 정량화할 수 있다는 장점도 확인되었다.43 이는 AI가 IT 인프라의 자동화된 관리와 장애 예방에 핵심적인 역할을 할 수 있음을 시사한다.</p>
<h2>7. 결론: 실용주의적 성숙기로의 진입</h2>
<p>2025년 6월의 주요 AI 연구 성과들은 인공지능 기술이 ’신기함(Novelty)’을 넘어 ’실용성(Utility)’과 ’신뢰성(Reliability)’을 갖춘 성숙기로 진입하고 있음을 강하게 시사한다.</p>
<ol>
<li><strong>3D 비전의 민주화와 물리적 정합성:</strong> VGGT와 MegaSaM은 고가의 장비 없이도 일상적인 비디오에서 정밀한 3차원 정보를 추출할 수 있게 함으로써, AR/VR 및 로보틱스 기술의 대중화를 앞당기고 있다. 특히 신경망이 물리 법칙(빛의 전파, 물체의 운동)을 내재적으로 학습하고 모사하기 시작했다는 점은 매우 고무적이다.</li>
<li><strong>효율성의 극대화:</strong> Mamba-2, SimPO, MatMul-free 모델, 그리고 SLM의 부상은 AI 모델의 비대화에 따른 비용 및 에너지 문제를 해결하려는 학계의 치열한 노력을 보여준다. 이는 AI가 클라우드 서버를 벗어나 우리 손안의 기기(On-device AI)로 내려오는 데 필수적인 기술적 토대가 될 것이다.</li>
<li><strong>멀티모달과 문맥의 확장:</strong> Video-MME는 AI가 텍스트를 넘어 시간의 흐름을 가진 복합적인 시청각 정보를 이해하는 단계로 나아가고 있음을 증명했다.</li>
<li><strong>시스템적 안전망 구축:</strong> AI 보안 연구는 공격과 방어의 끊임없는 진화 속에서, AI가 오히려 방어자에게 더 큰 이점을 제공할 수 있다는 가능성을 제시했다.</li>
</ol>
<p>결론적으로, 2025년의 AI 연구는 더 이상 단순한 ’생성’에 머무르지 않는다. 그것은 물리적 세계를 이해하고, 효율적으로 작동하며, 안전하게 관리되는 **‘신뢰할 수 있는 물리적 지능(Trustworthy Physical Intelligence)’**을 향해 나아가고 있다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>CVPR Paper Awards - IEEE Computer Society, 12월 14, 2025에 액세스, https://tc.computer.org/tcpami/awards/cvpr-paper-awards/</li>
<li>Best Papers at CVPR Reveal New Results with Neural Networks for …, 12월 14, 2025에 액세스, https://cvpr.thecvf.com/Conferences/2025/News/Awards_Press</li>
<li>CVPR 2025 Best Papers and Best Demos, 12월 14, 2025에 액세스, https://cvpr.thecvf.com/Conferences/2025/BestPapersDemos</li>
<li>Most Influential CVPR Papers (2025-09 Version) - Paper Digest, 12월 14, 2025에 액세스, https://www.paperdigest.org/2025/09/most-influential-cvpr-papers-2025-09-version/</li>
<li>Pushing VGGT’s Limits on Kilometer-scale Long RGB Sequences, 12월 14, 2025에 액세스, https://arxiv.org/html/2507.16443v1</li>
<li>An evaluation of DUSt3R/MASt3R/VGGT 3D reconstruction on …, 12월 14, 2025에 액세스, https://www.tandfonline.com/doi/full/10.1080/10095020.2025.2597491</li>
<li>Stepping into the Future of 3D Vision with VGGT - Level Up Coding, 12월 14, 2025에 액세스, https://levelup.gitconnected.com/stepping-into-the-future-of-3d-vision-with-vggt-53b3f8796874</li>
<li>Revisiting Depth Representations for Feed-Forward 3D Gaussian …, 12월 14, 2025에 액세스, https://arxiv.org/pdf/2506.05327</li>
<li>Emergent Outlier View Rejection in Visual Geometry Grounded …, 12월 14, 2025에 액세스, https://www.researchgate.net/publication/398313010_Emergent_Outlier_View_Rejection_in_Visual_Geometry_Grounded_Transformers</li>
<li>VGGT: Visual Geometry Grounded Transformer - ChatPaper, 12월 14, 2025에 액세스, https://chatpaper.com/paper/154735</li>
<li>CVPR 2025 Best Student Paper Award, 12월 14, 2025에 액세스, https://imaging.cs.cmu.edu/featured-content/cvpr-2025-best-student-paper-award/</li>
<li>Neural Inverse Rendering from Propagating Light - arXiv, 12월 14, 2025에 액세스, https://arxiv.org/html/2506.05347v1</li>
<li>Neural Inverse Rendering from Propagating Light - CVF Open Access, 12월 14, 2025에 액세스, https://openaccess.thecvf.com/content/CVPR2025/papers/Malik_Neural_Inverse_Rendering_from_Propagating_Light_CVPR_2025_paper.pdf</li>
<li>Neural Inverse Rendering from Propagating Light - ResearchGate, 12월 14, 2025에 액세스, https://www.researchgate.net/publication/392466055_Neural_Inverse_Rendering_from_Propagating_Light</li>
<li>MegaSaM: Accurate, Fast, and Robust Structure and Motion from …, 12월 14, 2025에 액세스, https://openaccess.thecvf.com/content/CVPR2025/papers/Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual_CVPR_2025_paper.pdf</li>
<li>MegaSaM: Accurate, Fast, and Robust Structure and Motion … - arXiv, 12월 14, 2025에 액세스, https://arxiv.org/html/2412.04463v1</li>
<li>MegaSaM: Accurate, Fast, and Robust Structure and Motion from …, 12월 14, 2025에 액세스, https://www.alphaxiv.org/overview/2412.04463v2</li>
<li>MegaSaM: Accurate, Fast and Robust Structure and Motion from …, 12월 14, 2025에 액세스, https://arxiv.org/html/2412.04463v2</li>
<li>MegaSaM: Accurate, Fast, and Robust Structure and Motion from …, 12월 14, 2025에 액세스, https://chatpaper.com/paper/87942</li>
<li>MegaSaM, 12월 14, 2025에 액세스, https://mega-sam.github.io/</li>
<li>SC-MAMBA2: Leveraging State-Space Models for Efficient Single …, 12월 14, 2025에 액세스, https://www.biorxiv.org/content/10.1101/2024.09.30.615775v1.full-text</li>
<li>MAMBA-3: IMPROVED SEQUENCE MODELING USING STATE …, 12월 14, 2025에 액세스, https://openreview.net/pdf?id=HwCvaJOiCj</li>
<li>Visual Mamba: A Survey and New Outlooks - arXiv, 12월 14, 2025에 액세스, https://arxiv.org/html/2404.18861v3</li>
<li>[PDF] Scalable MatMul-free Language Modeling - Semantic Scholar, 12월 14, 2025에 액세스, https://www.semanticscholar.org/paper/Scalable-MatMul-free-Language-Modeling-Zhu-Zhang/401c4147375b016d4758cf2dd859232a8271fdcd</li>
<li>ridgerchu/matmulfreellm: Implementation for MatMul-free LM. - GitHub, 12월 14, 2025에 액세스, https://github.com/ridgerchu/matmulfreellm</li>
<li>arXiv:2503.12211v1 [cs.LG] 15 Mar 2025, 12월 14, 2025에 액세스, https://arxiv.org/pdf/2503.12211</li>
<li>SimPO: Simple Preference Optimization with a … - SciSpace, 12월 14, 2025에 액세스, https://scispace.com/pdf/simpo-simple-preference-optimization-with-a-reference-free-4a89wyxuiy.pdf</li>
<li>SimPO: Simple Preference Optimization with a Reference-Free …, 12월 14, 2025에 액세스, https://arxiv.org/html/2405.14734v1</li>
<li>ONLINE PREFERENCE ALIGNMENT FOR LANGUAGE MODELS …, 12월 14, 2025에 액세스, https://proceedings.iclr.cc/paper_files/paper/2025/file/fd23a1f3bc89e042d70960b466dc20e8-Paper-Conference.pdf</li>
<li>[NeurIPS 2024] SimPO: Simple Preference Optimization … - GitHub, 12월 14, 2025에 액세스, https://github.com/princeton-nlp/SimPO</li>
<li>Video-MME: The First-Ever Comprehensive Evaluation Benchmark …, 12월 14, 2025에 액세스, https://openaccess.thecvf.com/content/CVPR2025/papers/Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in_CVPR_2025_paper.pdf</li>
<li>Video-MME: The First-Ever Comprehensive Evaluation Benchmark …, 12월 14, 2025에 액세스, https://arxiv.org/html/2405.21075v1</li>
<li>Video-MME: The First-Ever Comprehensive Evaluation Benchmark …, 12월 14, 2025에 액세스, https://github.com/MME-Benchmarks/Video-MME</li>
<li>Small Language Models are the Future of Agentic AI - arXiv, 12월 14, 2025에 액세스, https://arxiv.org/abs/2506.02153</li>
<li>AI Papers to Read in 2025 | Towards Data Science, 12월 14, 2025에 액세스, https://towardsdatascience.com/ai-papers-to-read-in-2025/</li>
<li>ML-AGENT: REINFORCING LLM AGENTS FOR … - OpenReview, 12월 14, 2025에 액세스, https://openreview.net/pdf/90fb8110b0c14e9e2c950a86a0b198a2b38e74ef.pdf</li>
<li>LLM-based Agentic Reasoning Frameworks: A Survey from Methods …, 12월 14, 2025에 액세스, https://arxiv.org/html/2508.17692v1</li>
<li>Strengthening cyber resilience as AI capabilities advance - OpenAI, 12월 14, 2025에 액세스, https://openai.com/index/strengthening-cyber-resilience/</li>
<li>OpenAI warns next-gen AI models could pose high cybersecurity risks; readies defences, 12월 14, 2025에 액세스, https://indianexpress.com/article/technology/artificial-intelligence/openai-warns-next-gen-ai-models-high-risk-cybersecurity-10414723/</li>
<li>Disrupting malicious uses of AI: June 2025 | OpenAI, 12월 14, 2025에 액세스, https://openai.com/global-affairs/disrupting-malicious-uses-of-ai-june-2025/</li>
<li>Disrupting malicious uses of AI: an update - OpenAI, 12월 14, 2025에 액세스, https://cdn.openai.com/threat-intelligence-reports/7d662b68-952f-4dfd-a2f2-fe55b041cc4a/disrupting-malicious-uses-of-ai-october-2025.pdf</li>
<li>Disrupting malicious uses of AI: June 2025 - OpenAI, 12월 14, 2025에 액세스, https://cdn.openai.com/threat-intelligence-reports/5f73af09-a3a3-4a55-992e-069237681620/disrupting-malicious-uses-of-ai-june-2025.pdf</li>
<li>Performance Prediction for Large Systems via Text-to-Text Regression, 12월 14, 2025에 액세스, https://deepmind.google/research/publications/performance-prediction-for-large-systems-via-text-to-text-regression/</li>
<li>Performance Prediction for Large Systems via Text-to-Text Regression, 12월 14, 2025에 액세스, https://arxiv.org/abs/2506.21718</li>
<li>Performance Prediction for Large Systems via Text-to-Text Regression, 12월 14, 2025에 액세스, https://arxiv.org/pdf/2506.21718</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>