<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2025년 4월 AI 및 로봇 연구</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2025년 4월 AI 및 로봇 연구</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2025년 AI 및 로봇 연구 동향</a> / <span>2025년 4월 AI 및 로봇 연구</span></nav>
                </div>
            </header>
            <article>
                <h1>2025년 4월 AI 및 로봇 연구</h1>
<h2>1. 서론</h2>
<p>2025년 4월은 인공지능(AI) 및 로봇 분야의 연구 패러다임이 중대한 전환점에 도달했음을 명확히 보여주는 시기였다. ICLR, AAAI, ICRA와 같은 세계 최고 수준의 학회에서 발표된 연구들은 더 이상 모델의 ‘규모’ 자체만을 경쟁하는 단계를 넘어, 실제 시스템에 적용 가능한 ‘효율성’, 다양한 환경에 일반화될 수 있는 ‘확장성’, 그리고 기술의 신뢰도를 담보하는 ‘안전성’을 확보하는 방향으로 나아가고 있다.1 이론적 탐구를 넘어 실제적이고 효율적인 시스템을 구축하려는 움직임이 학계와 산업계 전반의 핵심 화두로 부상한 것이다.</p>
<p>본 보고서는 다음 네 가지 핵심 동향을 중심으로 2025년 4월의 주요 연구 성과를 심층적으로 분석한다. 첫째, 거대 언어 모델(LLM)의 계산 비용 문제를 해결하기 위한 희소 전문가 혼합(MoE) 아키텍처와 같은 <strong>파운데이션 모델의 효율화</strong>. 둘째, 정적 이미지를 넘어 동적 비디오 영역으로 확장되고, 이에 따라 프라이버시 보호와 같은 안전성 문제가 중요해진 <strong>생성형 AI의 영역 확장</strong>. 셋째, 인식, 조작, 협업 기술의 일반화와 확장성을 통해 로봇의 물리적 구현을 가속하는 <strong>로보틱스 기술의 발전</strong>. 마지막으로, 단일 모델을 넘어 복합적인 문제를 해결하기 위한 <strong>AI의 시스템화 및 에이전트화</strong> 경향이다.</p>
<p>각 장에서는 이러한 핵심 동향을 대표하는 주요 논문들을 상세히 분석하고, 그 기술적 함의와 미래 산업에 미칠 영향을 전망하고자 한다.</p>
<p><strong>Table 1: 2025년 4월 주요 AI/로보틱스 학회 개요</strong></p>
<table><thead><tr><th>학회명 (Conference)</th><th>개최 기간 (Dates)</th><th>장소 (Location)</th><th>총 제출 논문 수 (Submissions)</th><th>채택 논문 수 (Accepted)</th><th>채택률 (Acceptance Rate)</th><th>주요 주제 (Key Themes)</th></tr></thead><tbody>
<tr><td>ICLR 2025</td><td>4월 24-28일</td><td>싱가포르</td><td>11,672 4</td><td>3,704 4</td><td>31.73% 4</td><td>표현 학습, 딥러닝, 생성 모델</td></tr>
<tr><td>AAAI-25</td><td>2월 25일 - 3월 4일 (프로시딩 4월 공개)</td><td>미국 필라델피아</td><td>N/A</td><td>N/A</td><td>N/A</td><td>머신러닝, NLP, 비전, 로보틱스, AI 응용</td></tr>
<tr><td>ICRA 2025</td><td>5월 19-23일 (주요 발표 4월 사전 공개)</td><td>미국 애틀랜타</td><td>4,250 5</td><td>1,606 5</td><td>~37.8%</td><td>로봇 인식, 조작, 자율주행, 다개체 시스템</td></tr>
</tbody></table>
<p>이 표는 본 보고서의 분석 대상을 명확히 하고, 각 학회의 규모와 위상을 정량적으로 제시하여 분석의 신뢰도를 높인다. 독자는 이를 통해 ICLR의 딥러닝 집중, AAAI의 광범위한 AI 응용, ICRA의 로보틱스 전문성이라는 각 학회의 특성을 직관적으로 파악하고, 발표된 연구들의 경쟁적 맥락을 이해할 수 있다.</p>
<h2>2.  차세대 파운데이션 모델의 진화: 효율성과 확장성</h2>
<p>거대 언어 모델(LLM)의 성능이 일정 수준 이상으로 상향 평준화되면서, 연구의 초점은 파라미터 수를 무작정 늘리는 경쟁에서 벗어나 계산 비용과 추론 속도를 최적화하는 ’효율성’으로 급격히 이동하고 있다. 특히 2025년 ICLR(표현 학습 국제 학회)은 이러한 경향을 주도하는 핵심적인 연구 성과들을 선보인 무대였다.2</p>
<h3>2.1  희소 전문가 혼합(MoE) 모델의 부상: OLMoE 심층 분석</h3>
<p>MoE(Mixture-of-Experts)는 모델의 전체 파라미터 중 일부, 즉 ’활성 파라미터(active parameters)’만을 선택적으로 사용하여 계산 효율을 극대화하는 아키텍처이다.7 ICLR 2025에서 발표된 ’OLMoE: Open Mixture-of-Experts Language Models’는 이러한 접근 방식의 최신 성과를 집약적으로 보여주는 대표적인 사례다.8</p>
<p>OLMoE는 총 70억 개의 파라미터를 보유하지만, 입력 토큰당 단 10억 개의 활성 파라미터만을 사용하도록 설계되었다.8 이는 기존의 모든 파라미터를 모든 계산에 사용하는 고밀도(dense) 모델과 비교했을 때 훈련 및 추론에 필요한 계산 자원을 현저히 절감하는 효과를 가져온다. 모델의 각 레이어는 여러 개의 작은 피드포워드 네트워크인 ’전문가(expert)’와, 입력 토큰의 특성을 분석하여 가장 적합한 전문가 몇 개를 선택하는 ’라우터(router)’로 구성된다.7</p>
<p>OLMoE의 핵심 방법론은 라우팅 과정의 안정성과 효율성을 확보하는 데 있다. 라우터가 특정 전문가에게만 계산을 편중시키지 않고 모든 전문가를 고르게 활용하도록 유도하는 **부하 분산 손실(Load Balancing Loss, LLB)**과, 라우터의 출력 값(logit)이 과도하게 커져 발생하는 수치적 불안정성을 막는 **라우터 Z-손실(Router Z-Loss, LRZ)**과 같은 보조 손실 함수들이 훈련 과정에 도입되었다.9 또한, 5조 개의 토큰으로 구성된 대규모 고품질 데이터셋(OLMOE-MIX)을 활용한 사전 훈련과, 이후 명령어 준수 능력을 높이기 위한 미세 조정을 통해 최종적으로 OLMoE-Instruct 모델이 완성되었다.8</p>
<p>OLMoE는 유사한 활성 파라미터 수를 가진 기존의 모든 공개 모델을 능가하는 성능을 보였으며, 심지어 Llama2-13B와 같이 더 큰 규모의 고밀도 모델보다도 우수한 결과를 달성했다.8 그러나 이 연구의 가장 중요한 의의는 성능 자체를 넘어선다. 모델 가중치, 훈련 데이터, 소스 코드, 그리고 훈련 과정의 로그까지 모든 자원을 전면적으로 공개함으로써, 이전까지 소수의 거대 기업만이 접근 가능했던 MoE 아키텍처 연구의 투명성과 재현성을 극적으로 높였다는 점이다.12</p>
<p>이러한 전면적인 개방은 AI 연구 생태계의 전략적 변화를 예고한다. 과거 GPT-4나 Gemini와 같은 최첨단 MoE 모델들은 핵심 기술을 비공개로 유지하며 기술적 장벽을 구축했다. 반면 OLMoE의 등장은 전 세계 연구자들이 최첨단 MoE 모델의 작동 방식을 직접 분석하고, 라우팅 알고리즘이나 전문가 모듈 설계와 같은 핵심 요소를 개선하는 경쟁에 참여할 수 있는 길을 열었다. 이는 AI 기술 경쟁의 패러다임이 단순히 ’누가 더 많은 GPU를 보유했는가’에서 ’누가 더 영리한 아키텍처와 훈련 레시피를 설계하는가’로 전환될 수 있음을 시사하며, 더욱 다양하고 혁신적인 연구가 촉발될 수 있는 기반을 마련했다.</p>
<h3>2.2  경량화 및 최적화 기법의 다변화</h3>
<p>ICLR 2025에서는 MoE 외에도 다양한 모델 효율화 연구가 주목받았다. LoRA(Low-Rank Adaptation)의 안정성과 성능을 개선한 ’LoRA Done RITE’나, 모델의 지식을 효율적으로 압축하는 희소 오토인코더 연구인 ‘Scaling and evaluating sparse autoencoders’ 등이 대표적이다.6 이러한 연구들은 특정 하드웨어나 응용 환경의 제약 조건에 맞춰 모델을 최적화하는 기술의 중요성이 점차 커지고 있음을 보여준다.</p>
<h2>3.  생성형 AI의 확장: 멀티모달리티와 비디오 인식의 도약</h2>
<p>생성형 AI의 기술적 전장은 정적인 이미지를 넘어, 시간과 공간적 맥락을 포함하는 동적 비디오의 영역으로 빠르게 이동하고 있다. ICLR 2025에서 발표된 ’SAM 2’는 이러한 변화를 상징하는 기념비적인 연구로 평가받는다.</p>
<h3>3.1  비디오 분할의 새로운 지평: SAM 2 심층 분석</h3>
<p>’SAM 2: Segment Anything in Images and Videos’는 이미지와 비디오 모두에서 프롬프트(예: 클릭, 텍스트)를 통해 원하는 객체를 정교하게 분할하는 파운데이션 모델이다.13 이는 이미지 분할 분야에서 놀라운 범용성을 보여준 기존 SAM(Segment Anything Model)의 성공을 비디오 영역으로 자연스럽게 확장한 것이다.</p>
<p>SAM 2의 가장 핵심적인 혁신은 **데이터 엔진(Data Engine)**의 구축에 있다.13 이는 단순히 더 좋은 모델을 만드는 것을 넘어, 모델과 데이터가 상호작용을 통해 함께 개선되는 선순환 구조를 시스템화한 것이다. 즉, 더 발전된 SAM 2 모델을 활용하여 비디오 데이터에 대한 분할 라벨링 작업을 수행하면, 기존 방식보다 3배나 적은 사용자 상호작용만으로도 더 빠르고 정확한 라벨 생성이 가능해진다.13 이렇게 축적된 대규모의 고품질 비디오 분할 데이터셋은 다시 SAM 2 모델을 더욱 강력하게 훈련시키는 데 사용된다. 이 과정이 반복되면서 모델과 데이터의 품질이 기하급수적으로 향상되는 ’플라이휠 효과’가 발생한다.</p>
<p>또한, 실시간 비디오 처리를 위해 **스트리밍 메모리 아키텍처(streaming memory architecture)**를 갖춘 간단한 트랜스포머 구조를 채택했다.13 이 아키텍처는 이전 프레임의 분할 정보를 효율적으로 메모리에 저장하고 다음 프레임 처리에 활용함으로써, 영상 전체에 걸쳐 시간적 일관성을 유지하면서도 실시간성을 확보하는 핵심적인 역할을 수행한다.</p>
<p>그 결과, SAM 2는 비디오 분할 작업에서 기존의 최첨단 접근법들보다 높은 정확도를 달성했으며, 이미지 분할 작업에서도 원본 SAM보다 6배 빠른 속도와 더 높은 정확도를 보여주었다.14 이 성과는 대규모 데이터와 효율적인 모델 아키텍처의 시너지가 비디오 이해 기술의 새로운 기준을 제시할 수 있음을 증명한 것이다. 나아가, 의료 영상 분석과 같은 전문 분야에서도 SAM 2의 활용 가능성이 활발히 탐구되고 있다.17</p>
<p>SAM 2의 데이터 엔진 개념은 AI 연구 개발의 패러다임 변화를 시사한다. AI 발전의 핵심 동력이 새로운 모델 아키텍처를 고안하는 ’모델 중심 혁신’에서, 고품질 데이터를 효율적으로 생성하고 정제하는 프로세스를 구축하는 ’데이터 중심 혁신’으로 이동하고 있음을 보여준다. 이러한 데이터 엔진은 대규모 사용자 상호작용 데이터에 접근할 수 있는 소수의 거대 기술 기업만이 구축할 수 있기에, 이들 기업과 나머지 연구 커뮤니티 간의 기술 격차를 심화시킬 수 있다. 이는 향후 학계는 보다 근본적인 이론이나 틈새 문제에 집중하고, 대규모 인식 과제의 최첨단 성능 경쟁은 소수 기업의 독점적인 영역이 될 수 있는 미래를 암시하며 AI 연구의 경제적, 동적 구도를 근본적으로 바꾸고 있다.</p>
<h3>3.2  생성 모델의 신뢰성과 안전성 확보</h3>
<p>생성 모델의 능력이 비약적으로 발전함에 따라, 이를 악용하여 특정인의 이미지를 무단으로 합성하는 등 개인의 프라이버시를 침해하는 문제에 대한 우려도 커지고 있다. 이러한 문제에 대응하기 위한 ‘방어’ 기술의 중요성 또한 대두되고 있다.</p>
<p>AAAI-25 학회에서 발표된 ’MYOPIA: Protecting Face Privacy from Malicious Personalized Text-to-Image Synthesis via Unlearnable Examples’는 이 문제에 대한 효과적인 해결책을 제시한다.18 이 연구의 핵심 아이디어는 **비학습성 예제(Unlearnable Examples)**를 만드는 것이다. 이는 원본 이미지에 인간의 눈으로는 거의 인지할 수 없는 미세한 섭동(perturbation)을 추가하여, DreamBooth와 같은 개인화된 텍스트-이미지(T2I) 모델이 대상 인물의 고유한 얼굴 특징을 학습하는 것을 근본적으로 방해하는 기술이다. 이 섭동이 추가된 이미지를 학습한 모델은 실제 얼굴 특징 대신 섭동 패턴에 빠르게 과적합되어, 더 이상 유의미한 학습을 진행하지 못하는 ’최적화 함정(optimization trap)’에 빠지게 된다.18</p>
<p>이러한 섭동 <span class="math math-inline">\delta</span>는 다음과 같은 이중 최적화 문제(bi-level optimization)를 통해 생성된다. 내부 루프에서는 주어진 모델 파라미터 <span class="math math-inline">\theta</span>에 대해 손실을 최소화하는 섭동 <span class="math math-inline">\delta</span>를 찾고, 외부 루프에서는 이렇게 생성된 섭동이 추가된 데이터로 학습했을 때 모델의 손실이 최소화되도록 <span class="math math-inline">\theta</span>를 업데이트하는 과정을 모사한다.<br />
<span class="math math-display">
\arg \min_{\theta} E_{(x,y) \sim D} \min_{\delta} L(f_{\theta}(x + \delta), y) \quad \text{s.t.} \quad \lVert \delta \rVert_p \le \epsilon
</span><br />
MYOPIA의 등장은 생성 모델 분야에서 ‘공격’ 기술뿐만 아니라, 이에 대응하는 ‘방어’ 기술이 함께 발전하고 있음을 보여주는 중요한 사례로, 향후 디지털 콘텐츠의 신뢰성과 개인정보 보호 연구에 중요한 방향을 제시한다.</p>
<h2>4.  지능형 로보틱스의 첨단: 인식, 조작, 그리고 협업</h2>
<p>로보틱스 분야의 최고 권위 학회인 ICRA 2025는 로봇이 복잡하고 예측 불가능한 실제 환경과 효과적으로 상호작용하기 위한 핵심 기술들의 비약적인 발전을 조명했다.1 특히 최우수 논문으로 선정된 연구들은 로봇 기술의 세 가지 근간인 <strong>인식(Perception)</strong>, <strong>조작(Manipulation)</strong>, 그리고 **협업(Collaboration)**이라는 축에서 중요한 돌파구를 마련했다.</p>
<p><strong>Table 2: ICRA 2025 주요 수상 논문 요약</strong></p>
<table><thead><tr><th>수상 부문 (Award Category)</th><th>논문 제목 (Paper Title)</th><th>저자 (Authors)</th><th>핵심 기여 (Core Contribution)</th></tr></thead><tbody>
<tr><td>최우수 컨퍼런스 논문 (Best Conference Paper) &amp; 최우수 인식 논문 (Best Perception Paper)</td><td>MAC-VO: Metrics-Aware Covariance for Learning-Based Stereo Visual Odometry</td><td>Yuheng Qiu, et al. 20</td><td>실제 3D 공간 오차를 반영하는 ‘메트릭스 인식’ 불확실성 모델을 통해 시각 주행 거리 측정의 강건성과 정확성 향상 20</td></tr>
<tr><td>최우수 조작 및 이동 논문 (Best Manipulation and Locomotion Paper)</td><td>D(R, O) Grasp: A Unified Representation of Robot and Object Interaction for Cross-Embodiment Dexterous Grasping</td><td>Zhenyu Wei, et al. 20</td><td>로봇 손과 객체 간의 상호작용을 통합적으로 표현하여, 다양한 형태의 로봇 손과 객체에 일반화 가능한 파지 기술 제안 20</td></tr>
<tr><td>최우수 다개체 로봇 시스템 논문 (Best Multi-Robot Systems Paper)</td><td>Deploying Ten Thousand Robots: Scalable Imitation Learning for Lifelong Multi-Agent Path Finding</td><td>He Jiang, et al. 20</td><td>최대 1만 대 로봇의 경로 계획을 위한 확장 가능한 모방 학습 프레임워크를 제안하여 대규모 시스템에서의 적용 가능성 입증 20</td></tr>
</tbody></table>
<h3>4.1  인식(Perception): 불확실성을 정복하는 MAC-VO</h3>
<p>자율주행차나 모바일 로봇이 자신의 위치를 파악하는 데 핵심적인 기술인 시각 주행 거리 측정(Visual Odometry, VO)은 오랫동안 한계에 부딪혀 왔다. 기존 방법들은 이미지의 특징점이 얼마나 신뢰할 만한지를 평가할 때, 실제 3D 공간에서 발생하는 미터 단위의 오차를 제대로 반영하지 못하는 ‘규모 불가지(scale-agnostic)’ 문제를 안고 있었다.22</p>
<p>ICRA 2025 최우수 논문으로 선정된 ’MAC-VO’는 이 문제를 해결하기 위해 **메트릭스 인식 불확실성 모델(Metrics-Aware Uncertainty Model)**이라는 새로운 개념을 도입했다. 이 모델은 특징 매칭 과정에서 발생하는 불일치를 포착하는 2D 불확실성 네트워크를 학습하여, 이를 기반으로 실제 물리적 오차를 반영하는 ‘메트릭스 인식’ 공분산 모델을 구축한다.22 이 불확실성 모델은 두 가지 핵심적인 역할을 수행한다. 첫째, 전역적인 불일치 정도를 분석하여 품질이 낮은 특징점을 조기에 제거하는 ’키포인트 선택기’로 기능한다. 둘째, 로봇의 최종 위치를 최적화하는 포즈 그래프 최적화(Pose Graph Optimization) 과정에서 각 측정치의 신뢰도를 반영하는 가중치로 사용된다.22</p>
<p>MAC-VO는 조명 변화가 심하거나 특징점이 부족한 도전적인 환경에서도 기존의 어떤 VO 시스템보다 강건하고 정확한 위치 추정 성능을 보여주었다.20 이는 로봇이 ’자신이 무엇을 얼마나 불확실하게 보고 있는지’를 정확히 인지하게 함으로써, 자율 시스템의 안정성과 신뢰도를 한 차원 높인 중요한 연구 성과이다.</p>
<h3>4.2  조작(Manipulation): 일반화를 향한 D(R, O) Grasp</h3>
<p>로봇이 인간처럼 능숙하게 다양한 물체를 집는 손재주 있는 파지(dexterous grasping) 기술은 로보틱스의 오랜 난제 중 하나였다. 기존 연구들은 특정 로봇 손이나 특정 형태의 객체에 과적합되어, 새로운 손이나 물건을 만나면 제대로 작동하지 못하는 일반화의 한계를 보였다.24</p>
<p>’D(R, O) Grasp’는 이러한 한계를 극복하기 위해 패러다임의 전환을 제안한다. 기존의 로봇 중심(Robot-centric) 또는 객체 중심(Object-centric) 표현 방식에서 벗어나, 로봇 손(R)과 객체(O)의 <strong>상호작용 자체를 모델링</strong>하는 새로운 통합 표현 <span class="math math-inline">D(R, O)</span>를 도입한 것이다.23 이 표현은 파지 자세에서의 로봇 손 포인트 클라우드와 객체 포인트 클라우드 간의 상대적 거리를 인코딩하는 행렬로 정의된다. 모델은 로봇 손의 3D 모델과 객체의 포인트 클라우드를 입력으로 받아, 조건부 변분 오토인코더(CVAE)를 통해 최적의 <span class="math math-inline">D(R, O)</span> 표현을 예측한다. 이후, 다변측량(multilateration)이라는 기하학적 기법을 통해 각 손가락 링크의 6D 포즈를 추정하고, 최종적으로 로봇의 관절 값을 효율적으로 계산해낸다.24</p>
<p>이 연구는 로봇 손의 형태나 관절 구조, 혹은 객체의 모양과 크기에 관계없이 보편적으로 적용될 수 있는 범용 파지 기술의 가능성을 열었다. 시뮬레이션에서 평균 87.5%, 실제 로봇 실험에서 89%라는 높은 파지 성공률을 기록하며, 복잡하고 다양한 실제 환경에서 로봇의 조작 능력을 크게 향상시킬 수 있음을 입증했다.24</p>
<h3>4.3  협업(Collaboration): 1만 대 로봇 시대를 여는 SILLM</h3>
<p>아마존과 같은 거대 물류 창고에서는 수천 대의 로봇이 동시에 움직인다. 이러한 대규모 시스템에서 충돌 없이 효율적인 경로를 찾는 다개체 경로 탐색(Multi-Agent Path Finding, MAPF) 기술은 필수적이지만, 기존 알고리즘들은 수백 대 수준을 넘어서면 계산량이 폭발적으로 증가하는 확장성 문제를 겪어왔다.28</p>
<p>‘Deploying Ten Thousand Robots’ 논문은 **SILLM(Scalable Imitation Learning for LMAPF)**이라는 혁신적인 모방 학습 기반 솔버를 통해 이 문제를 해결한다.29 SILLM의 핵심 전략은, 가장 성능이 좋은 중앙 집중형 탐색 기반 솔버(전문가)가 소규모 시나리오에서 내리는 최적의 의사결정을 모방하도록 학습하는 것이다. 이를 통해 개별 로봇은 자신의 주변(지역적 관찰) 정보만을 가지고도 마치 전역적인 최적 경로를 아는 것처럼 행동할 수 있게 된다. 특히, 에이전트 간의 공간적 상호작용을 효과적으로 추론하기 위한 새로운 <strong>공간 민감 통신(Spatially Sensitive Communication)</strong> 모듈과, 실시간 충돌 회피 및 전역 경로 안내 기술을 통합하여 성능을 극대화했다.30</p>
<p>SILLM은 최대 1만 대의 로봇이 동시에 움직이는 시나리오에서 기존의 어떤 탐색 기반 또는 학습 기반 솔버보다도 높은 처리량(단위 시간당 목표 도달 횟수)을 달성했다.29 이는 대규모 로봇 군집을 현실 세계에서 효율적으로 운영할 수 있는 길을 연 핵심적인 연구 성과로, 미래의 스마트 팩토리와 자동화 물류 시스템의 청사진을 제시한다.</p>
<p>ICRA 2025에서 나타난 이러한 세 가지 분야의 동시적인 발전은 개별적인 성공을 넘어선 중요한 의미를 가진다. 이는 진정으로 자율적인 범용 물리 에이전트를 만들기 위한 필수 구성 요소들이 수렴하고 있음을 나타낸다. 예를 들어, 물류 창고의 로봇은 먼저 MAC-VO와 같은 기술로 자신의 위치를 정확히 파악하고(인식), D(R, O) Grasp 기술로 어떤 모양의 소포든 안정적으로 집어 들고(조작), SILLM과 같은 기술로 수천 대의 다른 로봇과 충돌 없이 경로를 계획해야(협업) 비로소 하나의 완전한 임무를 수행할 수 있다. 과거에는 이 중 하나 이상의 영역이 병목 현상을 일으켰지만, 이제는 세 가지 기본 요소 모두에서 최첨단 기술이 동시에 발전하고 있다. 이는 대규모 범용 로봇 시스템의 현실화가 먼 미래의 가능성이 아닌, 중기적인 엔지니어링 도전 과제로 다가왔음을 시사하는 강력한 신호이다.</p>
<h2>5.  도메인 특화 AI 시스템과 자율 에이전트의 부상</h2>
<p>파운데이션 모델이 제공하는 강력한 범용 능력을 특정 도메인의 복잡한 실제 문제를 해결하는 데 적용하려는 ‘AI 시스템’ 및 ‘자율 에이전트’ 연구가 AAAI-25 학회를 중심으로 활발하게 진행되었다.32 이는 단일 AI 모델의 성능을 평가하는 것을 넘어, 여러 기술 요소를 유기적으로 통합한 시스템적 접근의 중요성이 커지고 있음을 보여준다.</p>
<h3>5.1  강화학습과 LLM의 융합: GARLIC 차량 디스패칭 시스템</h3>
<p>도시의 차량 호출 서비스에서 실시간 교통 상황, 운전자의 다양한 행동 패턴, 끊임없이 변하는 수요-공급 불균형 등 복잡한 동적 요소를 모두 고려하여 최적의 차량 배차(디스패칭)를 결정하는 것은 극도로 어려운 문제이다.34</p>
<p>AAAI-25에서 발표된 ’GARLIC: GPT-Augmented Reinforcement Learning with Intelligent Control for Vehicle Dispatching’은 이 문제를 해결하기 위한 정교한 통합 AI 시스템을 제안한다.34 GARLIC 시스템은 세 가지 핵심 모듈로 구성된다. 첫째, <strong>계층적 상태 표현</strong> 모듈은 다중뷰 그래프 신경망(Multiview GNN)을 사용하여 개별 차량 수준의 미시적 정보부터 도시 전체의 거시적 교통 흐름까지를 종합적으로 분석한다.35 둘째, <strong>동적 보상 함수</strong> 모듈은 과거 운행 데이터를 분석하여 운전자 개개인의 선호도와 운전 습관을 모델링하고, 이를 강화학습의 보상 함수에 반영함으로써 시스템의 배차 지시와 운전자의 실제 의도를 일치시킨다.35 마지막으로, <strong>GPT 증강 정책 학습</strong> 모듈은 GPT 모델을 활용하여 시계열적으로 변화하는 교통 상태와 보상 정보를 종합적으로 이해하고, 가장 정밀한 배차 행동을 예측한다. 이 과정에서 위경도 좌표의 특성을 고려하여 설계된 맞춤형 손실 함수(Geospatial Loss)를 사용하여 예측 정확도를 높인다.36</p>
<p>GARLIC은 강화학습, 그래프 신경망, LLM이라는 각기 다른 강점을 가진 AI 기술들을 하나의 시스템으로 결합하여 복잡한 실제 세계의 최적화 문제를 해결하는 효과적인 청사진을 제시한다. 이는 교통 분야를 넘어 물류, 에너지 관리, 금융 등 다양한 산업에서 복잡한 의사결정을 자동화하는 데 적용될 수 있는 강력한 방법론이다.34</p>
<h3>5.2  다중 에이전트 협업: SQLFixAgent</h3>
<p>LLM을 이용해 자연어 질문을 SQL 쿼리로 변환하는 Text-to-SQL 기술은 데이터 분석의 대중화에 기여하고 있지만, 생성된 SQL이 구문적으로는 완벽해도 사용자의 본래 의도와 다른 결과를 내놓는 ’의미론적 오류’를 포함하는 경우가 많다.39</p>
<p>’SQLFixAgent’는 이 문제를 해결하기 위해, 각기 다른 전문성을 가진 세 개의 자율 에이전트가 협업하는 독창적인 프레임워크를 제안한다.39 이 시스템의 작동 방식은 다음과 같다. 먼저, <strong>SQLReviewer</strong> 에이전트가 ‘고무 오리 디버깅(rubber duck debugging)’ 방식을 사용하여 초기 생성된 SQL과 원본 질문 사이의 미묘한 의미적 불일치를 탐지한다. 오류 가능성이 발견되면, <strong>QueryCrafter</strong> 에이전트가 원본 질문을 여러 방식으로 변형하여 다양한 후보 SQL 쿼리들을 생성한다. 마지막으로, 핵심적인 역할을 하는 <strong>SQLRefiner</strong> 에이전트가 과거의 실행 오류 기록과 유사한 성공적인 수정 사례들을 참조하여, 생성된 후보군 중에서 가장 적합한 SQL을 최종적으로 선택하고 수정한다.</p>
<p>이러한 접근 방식은 복잡한 작업을 기능별로 분해하여 전문화된 에이전트들에게 역할을 분담시키고, 이들의 협력을 통해 문제 해결의 정확성과 효율성을 동시에 높일 수 있음을 보여준다. 이는 향후 자율적으로 코드를 생성하고 스스로 디버깅하는 AI 시스템 개발에 중요한 시사점을 제공한다.39</p>
<p>GARLIC과 SQLFixAgent 같은 시스템의 등장은 AI 엔지니어링이 성숙기에 접어들고 있음을 보여주는 현상이다. 이는 마치 초기 컴퓨터 프로그래밍이 하나의 거대한 코드로 작성되던 것에서, 기능별로 모듈화된 마이크로서비스 아키텍처로 발전한 것과 유사하다. AI를 더 이상 하나의 거대한 ’블랙박스’로 취급하는 대신, 각기 다른 역할을 수행하는 여러 모듈이나 에이전트로 구성된 구조화된 시스템으로 설계하는 것이다. 이러한 접근 방식은 시스템의 특정 부분에서 오류가 발생했을 때 원인을 파악하고 수정하기 용이하게 만들어 AI 시스템의 전반적인 강건성과 신뢰도를 높인다. 이는 또한 ’AI 시스템 아키텍트’나 ’AI 에이전트 상호작용 디자이너’와 같은 새로운 전문 직무의 등장을 예고하며, 핵심 연구 과제를 모델의 정확도 향상에서 효과적인 에이전트 간 협력 프로토콜 설계로 확장시키고 있다.</p>
<h3>5.3  과학 및 산업으로의 확장</h3>
<p>AAAI-25 프로시딩에서는 이 외에도 화학 반응을 이해하고 예측하는 멀티모달 LLM(‘ChemVLM’) 32, 신뢰할 수 있는 외부 정보를 활용하여 가짜뉴스를 탐지하는 모델(‘External Reliable Information-enhanced Multimodal Contrastive Learning for Fake News Detection’) 32, 블록체인 스마트 컨트랙트의 취약점을 자동으로 분석하는 시스템(‘SCALM’) 32 등 다양한 도메인에 특화된 연구들이 발표되었다. 이는 AI가 범용 도구를 넘어 각 산업 및 과학 분야의 핵심적인 문제 해결사로 깊숙이 자리매김하고 있음을 명확히 보여준다.</p>
<h2>6. 결론: 2025년 AI 및 로봇 연구의 주요 동향과 미래 전망</h2>
<p>2025년 4월에 발표된 AI 및 로봇 분야의 주요 연구들은 공통적으로 **‘지속 가능한 확장성(Sustainable Scalability)’**이라는 하나의 지향점을 가리키고 있다. 이는 단순히 더 큰 모델, 더 많은 데이터를 추구하는 것을 넘어, 자원 제약과 실제 세계의 복잡성을 고려하여 기술을 확장하려는 성숙한 접근 방식을 의미한다.</p>
<ul>
<li><strong>모델의 지속 가능성:</strong> MoE와 같은 희소 아키텍처는 제한된 계산 자원 내에서 모델의 성능을 극대화하여, 무한한 자원 없이도 기술 발전이 가능함을 보여주었다 (제1장).</li>
<li><strong>데이터의 지속 가능성:</strong> SAM 2의 데이터 엔진은 고품질 데이터를 지속적으로 확보하고 이를 통해 모델을 개선하는 선순환 구조를 구축함으로써, 데이터 확보의 한계를 극복하는 새로운 길을 제시했다 (제2장).</li>
<li><strong>물리적 세계로의 지속 가능한 확장:</strong> 로보틱스 분야에서는 인식, 조작, 협업 기술의 일반화와 확장성을 확보함으로써, 로봇이 실험실을 넘어 복잡한 실제 환경으로 적용 범위를 넓힐 수 있는 기반을 마련했다 (제3장).</li>
<li><strong>시스템의 지속 가능한 복잡성:</strong> 복잡한 문제를 해결하기 위해 모듈화되고 협력하는 에이전트 시스템을 구축하는 접근 방식은, 단일 모델의 한계를 넘어 더욱 정교하고 신뢰성 높은 AI 시스템을 구축하는 방향을 제시했다 (제4장).</li>
</ul>
<p>이러한 동향을 바탕으로 미래를 전망하면 다음과 같다. 단기적으로는 MoE 아키텍처의 보편화와 OLMoE와 같은 오픈소스 모델의 확산으로 최첨단 LLM 기술에 대한 접근성이 크게 향상될 것이며, SAM 2를 필두로 한 비디오 생성 및 이해 기술이 급속도로 발전하여 다양한 멀티미디어 서비스에 적용될 것이다.</p>
<p>중장기적으로는 데이터 엔진을 성공적으로 구축한 소수의 기업이 기술적 우위를 점하는 현상이 심화될 수 있다. 로보틱스 분야에서는 일반화된 조작 능력과 대규모 협업 기술을 바탕으로 물류 및 제조 자동화의 완전한 전환이 가속화될 것이다. AI 시스템은 더욱 정교한 자율 에이전트의 형태로 진화하여, 인간의 직접적인 개입 없이도 복잡한 디지털 및 물리적 세계의 작업을 자율적으로 수행하게 될 것이다.</p>
<p>그러나 이러한 눈부신 발전 이면에는 새로운 과제들이 놓여 있다. 모델의 안전성, 편향성, 그리고 결과의 투명성을 확보하는 문제는 기술이 사회에 깊숙이 통합될수록 더욱 중요해질 것이다. 또한, 고도로 자율적인 AI 시스템의 행동을 예측하고 제어하며, 그 책임 소재를 명확히 하는 거버넌스 체계 구축은 피할 수 없는 사회적, 기술적 과제로 부상할 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>IEEE ICRA 2025 | About this Conference, https://2025.ieee-icra.org/about/</li>
<li>2025 Conference - ICLR 2026, https://iclr.cc/Conferences/2025</li>
<li>AAAI-25 - AAAI - The Association for the Advancement of Artificial Intelligence, https://aaai.org/conference/aaai/aaai-25/</li>
<li>ICLR 2025 Accepted Paper List - Paper Copilot, https://papercopilot.com/paper-list/iclr-paper-list/iclr-2025-paper-list/</li>
<li>ICRA 2025 Highlight Statistics, https://2025.ieee-icra.org/announcements/icra-2025-highlight-statistics/</li>
<li>Top-rated papers from ICLR 2025 | Kaggle, https://www.kaggle.com/discussions/general/546726</li>
<li>OLMoE: Open Mixture-of-Experts Language Models - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2024/12/olmoe/</li>
<li>OLMoE: Open Mixture-of-Experts Language Models - OpenReview, https://openreview.net/forum?id=xXTkbTBmqq</li>
<li>[Literature Review] OLMoE: Open Mixture-of-Experts Language Models - Moonlight, https://www.themoonlight.io/en/review/olmoe-open-mixture-of-experts-language-models</li>
<li>Paper page - OLMoE: Open Mixture-of-Experts Language Models - Hugging Face, https://huggingface.co/papers/2409.02060</li>
<li>[2409.02060] OLMoE: Open Mixture-of-Experts Language Models - arXiv, https://arxiv.org/abs/2409.02060</li>
<li>allenai/OLMoE: OLMoE: Open Mixture-of-Experts Language Models - GitHub, https://github.com/allenai/OLMoE</li>
<li>SAM 2: Segment Anything in Images and Videos - OpenReview, https://openreview.net/forum?id=Ha6RTeWMd0</li>
<li>SAM 2: Segment Anything in Images and Videos - ResearchGate, https://www.researchgate.net/publication/382797270_SAM_2_Segment_Anything_in_Images_and_Videos</li>
<li>SAM 2: Segment Anything in Images and Videos | Research - AI at Meta, https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/</li>
<li>SAM 2: Segment Anything in Images and Videos - arXiv, https://arxiv.org/html/2408.00714v1</li>
<li>[2408.03286] Biomedical SAM 2: Segment Anything in Biomedical Images and Videos - arXiv, https://arxiv.org/abs/2408.03286</li>
<li>MYOPIA: Protecting Face Privacy from Malicious … - AAAI Publications, https://ojs.aaai.org/index.php/AAAI/article/view/32075/34230</li>
<li>Best Robotics Conferences and Events to Attend in 2025, https://clearpathrobotics.com/blog/2025/01/best-robotics-conferences-and-events-to-attend-in-2025/</li>
<li>IEEE Robotics and Automation Society (RAS) Announces Award …, https://2025.ieee-icra.org/announcements/ieee-robotics-and-automation-society-ras-announces-award-winning-papers-and-demonstrations-at-the-ieee-international-conference-on-robotics-and-automation-icra/</li>
<li>Awards and Finalists - IEEE ICRA 2025, https://2025.ieee-icra.org/program/awards-and-finalists/</li>
<li>Metrics-aware Covariance for Learning-based Stereo Visual Odometry mac-vo.github.io, https://arxiv.org/html/2409.09479v2</li>
<li>D ( R , O ) - Grasp: A Unified Representation of Robot and Object Interaction for Cross-Embodiment Dexterous Grasping - GitHub Pages, https://nus-lins-lab.github.io/drograspweb/</li>
<li>A Unified Representation of Robot and Object Interaction for Cross-Embodiment Dexterous Grasping - OpenReview, https://openreview.net/pdf/01cb42302b5c3f7212896f47ee5d4b883392ed86.pdf</li>
<li>(PDF) \mathcal{D(R,O)}$ Grasp: A Unified Representation of Robot and Object Interaction for Cross-Embodiment Dexterous Grasping - ResearchGate, https://www.researchgate.net/publication/384599472_mathcalDRO_Grasp_A_Unified_Representation_of_Robot_and_Object_Interaction_for_Cross-Embodiment_Dexterous_Grasping</li>
<li>A Unified Representation of Robot and Object Interaction for Cross-Embodiment Dexterous Grasping - arXiv, https://arxiv.org/html/2410.01702v2</li>
<li>[Literature Review] D(R, O) Grasp: A Unified Representation of Robot and Object Interaction for Cross-Embodiment Dexterous Grasping - Moonlight, https://www.themoonlight.io/en/review/dr-o-grasp-a-unified-representation-of-robot-and-object-interaction-for-cross-embodiment-dexterous-grasping</li>
<li>Deploying Ten Thousand Robots: Scalable Imitation Learning for Lifelong Multi-Agent Path Finding | Request PDF - ResearchGate, https://www.researchgate.net/publication/395224780_Deploying_Ten_Thousand_Robots_Scalable_Imitation_Learning_for_Lifelong_Multi-Agent_Path_Finding</li>
<li>(PDF) Deploying Ten Thousand Robots: Scalable Imitation Learning for Lifelong Multi-Agent Path Finding - ResearchGate, https://www.researchgate.net/publication/385353545_Deploying_Ten_Thousand_Robots_Scalable_Imitation_Learning_for_Lifelong_Multi-Agent_Path_Finding</li>
<li>Scalable Imitation Learning for Lifelong Multi-Agent Path Finding - Carnegie Mellon University Robotics Institute, https://www.ri.cmu.edu/app/uploads/2025/08/Master_Thesis.pdf</li>
<li>Deploying Ten Thousand Robots: Scalable Imitation Learning for Lifelong Multi-Agent Path Finding - Powerdrill AI, https://powerdrill.ai/discover/discover-Deploying-Ten-Thousand-cm2wd5gf212ot01aqfewmvhae</li>
<li>Vol. 39 No. 1: AAAI-25 Technical Tracks 1 | Proceedings of the AAAI …, https://ojs.aaai.org/index.php/AAAI/issue/current</li>
<li>Archives | Proceedings of the AAAI Conference on Artificial Intelligence, https://ojs.aaai.org/index.php/AAAI/issue/archive</li>
<li>GARLIC: GPT-Augmented Reinforcement Learning with Intelligent Control for Vehicle Dispatching | Request PDF - ResearchGate, https://www.researchgate.net/publication/390711741_GARLIC_GPT-Augmented_Reinforcement_Learning_with_Intelligent_Control_for_Vehicle_Dispatching</li>
<li>(PDF) GPT-Augmented Reinforcement Learning with Intelligent Control for Vehicle Dispatching - ResearchGate, https://www.researchgate.net/publication/383266937_GPT-Augmented_Reinforcement_Learning_with_Intelligent_Control_for_Vehicle_Dispatching</li>
<li>[Revue de papier] GARLIC: GPT-Augmented Reinforcement Learning with Intelligent Control for Vehicle Dispatching - Moonlight, https://www.themoonlight.io/fr/review/garlic-gpt-augmented-reinforcement-learning-with-intelligent-control-for-vehicle-dispatching</li>
<li>[2408.10286] GARLIC: GPT-Augmented Reinforcement Learning with Intelligent Control for Vehicle Dispatching - arXiv, https://arxiv.org/abs/2408.10286</li>
<li>[论文审查] GARLIC: GPT-Augmented Reinforcement Learning with Intelligent Control for Vehicle Dispatching - Moonlight, https://www.themoonlight.io/zh/review/garlic-gpt-augmented-reinforcement-learning-with-intelligent-control-for-vehicle-dispatching</li>
<li>SQLFixAgent: Towards Semantic-Accurate Text-to-SQL Parsing via …, https://ojs.aaai.org/index.php/AAAI/article/view/31979/34134</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>