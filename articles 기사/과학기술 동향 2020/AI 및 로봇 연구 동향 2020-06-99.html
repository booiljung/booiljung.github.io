<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2020년 6월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2020년 6월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2020년 AI 및 로봇 연구 동향</a> / <span>2020년 6월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2020년 6월 AI 및 로봇 연구 동향</h1>
<h2>1. 서론</h2>
<p>2020년 중반은 인공지능(AI) 및 로보틱스 연구 지형에 있어 중대한 변곡점으로 기록된다. 전례 없는 팬데믹 상황은 학술 교류의 패러다임을 온라인 중심으로 전환시켰고, 이에 따라 CVPR, ICML, RSS와 같은 세계 최고 수준의 학회들이 모두 가상으로 개최되었다.1 이러한 물리적 제약에도 불구하고, 이 시기에 발표된 연구들은 AI와 로보틱스 분야의 근본적인 발전을 이끌며 새로운 시대의 서막을 열었다. 본 보고서는 2020년 6월을 중심으로 발표된 주요 연구 성과를 분석하고, 특히 세 가지 핵심적인 패러다임의 부상을 심도 있게 조명하고자 한다.</p>
<p>첫째, <strong>자기지도 학습(Self-Supervised Learning)</strong> 분야에서는 레이블이 없는 대규모 데이터로부터 유의미한 시각적 표현(representation)을 학습하는 새로운 방법론이 제시되었다. 구글 리서치팀이 발표한 SimCLR는 기존의 복잡한 방법론을 단순화하면서도 지도 학습에 필적하는 성능을 달성하며, 표현 학습 연구의 방향을 재설정했다.2</p>
<p>둘째, <strong>뉴럴 렌더링(Neural Rendering)</strong> 분야에서는 2D 이미지로부터 고품질의 3D 장면을 생성하는 NeRF가 등장하여 컴퓨터 비전과 그래픽스 커뮤니티에 큰 충격을 주었다.4 이는 3D 장면을 연속적인 함수로 표현하는 새로운 방식을 제시하며, 사실적인 뷰 합성(view synthesis) 기술의 수준을 한 단계 끌어올렸다.</p>
<p>셋째, <strong>Sim-to-Real 전이(Simulation-to-Reality Transfer)</strong> 연구는 로보틱스 분야에서 중요한 진전을 이루었다. 특히 ‘Deep Drone Acrobatics’ 연구는 시뮬레이션 환경에서 학습된 고난도 드론 제어 정책을 실제 로봇에 성공적으로 적용함으로써, 복잡하고 위험한 작업을 현실 세계에서 직접 시도하지 않고도 안전하고 효율적으로 학습할 수 있는 새로운 가능성을 열었다.6</p>
<p>본 보고서는 먼저 각 주요 학회의 동향을 개괄적으로 분석하고, 이어 상기 세 가지 핵심 연구를 중심으로 그 기술적 원리, 실험 결과, 그리고 학계 및 산업계에 미친 영향을 심층적으로 탐구한다. 마지막으로, 당시의 최신 연구 흐름을 조망할 수 있는 arXiv 사전 공개 논문들을 분석하며 미래 연구 방향을 전망한다.</p>
<h2>2.  2020년 주요 AI 및 로보틱스 학회 동향 분석</h2>
<h3>2.1  Conference on Computer Vision and Pattern Recognition (CVPR) 2020</h3>
<p>CVPR 2020은 총 6,424편의 방대한 논문이 제출되어 그중 1,467편이 채택, 22.84%의 채택률을 기록했다.8 이는 전년도에 비해 제출 수가 크게 증가한 것으로, 컴퓨터 비전 분야의 폭발적인 연구 열기를 방증한다. 모든 채택 논문은 Computer Vision Foundation(CVF)을 통해 오픈 액세스로 공개되어 연구 결과의 접근성을 높였다.9</p>
<p>학회에서 주목받은 주요 연구 주제는 다음과 같다. 첫째, <strong>3D 컴퓨터 비전</strong> 분야의 약진이 두드러졌다. 다중 시점에서 촬영된 3D 포인트 클라우드를 정합(registration)하는 연구와 같이 2D 이미지를 넘어 3D 공간을 이해하고 재구성하려는 노력이 심화되었다.13 이러한 연구들은 이후 NeRF와 같은 혁신적인 3D 표현 학습 방법론의 토대를 마련했다. 둘째, **실세계 문제 해결(Real-World Problems)**에 대한 관심이 증가했다. 실제 환경에서 발생하는 이미지 품질 저하 문제를 정량적으로 평가하고, 이미지 캡셔닝 가능 여부를 판단하는 알고리즘을 제안하는 연구 등이 발표되었다.9 이는 AI 모델의 이론적 성능을 넘어, 현실 세계에서의 강건성(robustness)과 실용성을 확보하려는 학계의 노력을 반영한다. 마지막으로, 다수의 연구에서 소스 코드와 사전 학습된 모델을 공개하는 <strong>코드 공개 문화</strong>가 확산되었다.13 이는 연구의 재현성과 투명성을 높이고 후속 연구를 촉진하는 긍정적인 문화로 자리 잡고 있음을 보여주었다.</p>
<h3>2.2  International Conference on Machine Learning (ICML) 2020</h3>
<p>7월에 가상으로 개최된 ICML 2020은 머신러닝 전반의 광범위한 주제를 다루었다. 총 4,990편의 논문이 제출되어 1,084편이 채택되었으며, 채택률은 21.72%였다.14</p>
<p>ICML 2020의 핵심 주제는 **신뢰 가능한 머신러닝(Trustworthy Machine Learning)**이었다. 적대적 공격에 대한 방어력을 높이는 연구, 노이즈가 포함된 레이블로부터 강건하게 학습하는 방법론, 모델의 공정성, 투명성, 설명가능성을 다루는 연구들이 대거 발표되었다.15 이는 AI 기술이 사회 전반에 미치는 영향력이 커짐에 따라, 그 기술의 안전성과 책임에 대한 학문적 고민이 깊어지고 있음을 시사한다. 또한, <strong>자기지도 학습의 부상</strong>이 뚜렷하게 나타났다. 특히 ‘A Simple Framework for Contrastive Learning of Visual Representations’(SimCLR) 논문이 발표되며, 레이블 없이 대규모 데이터셋을 활용하여 지도 학습에 필적하는 표현을 학습할 수 있는 가능성을 열었다.14 이와 함께, PAC-Bayesian 분석과 같은 학습 이론 연구와 Atari 벤치마크에서 인간 수준을 뛰어넘는 강화학습 에이전트 ’Agent57’의 발표 등, 이론과 실제를 아우르는 다채로운 연구 성과가 공존했다.15</p>
<h3>2.3  Robotics: Science and Systems (RSS) 2020</h3>
<p>7월 12일부터 16일까지 가상으로 개최된 RSS 2020은 총 321편의 논문이 제출되어 103편이 채택, 32%의 비교적 높은 채택률을 보였다.18 학회는 모든 채택 논문과 발표 영상을 무료로 공개하여 연구 교류의 장벽을 낮추었다.1</p>
<p>가장 큰 주목을 받은 주제는 <strong>학습 기반 제어 및 Sim-to-Real</strong>이었다. ’Deep Drone Acrobatics’와 같이 시뮬레이션에서 학습한 제어 정책을 실제 로봇으로 성공적으로 이전하는 연구가 대표적이다.6 이는 복잡하고 위험한 작업을 현실 세계에서 직접 시도하지 않고도 안전하고 효율적으로 학습할 수 있는 길을 열어주었다는 점에서 큰 의미를 가진다.</p>
<p><strong>조작(Manipulation)</strong> 분야 역시 활발한 연구가 이루어졌다. 천과 같은 비강체(non-rigid) 물체 조작, 케이블 조작, 그리고 다수의 접촉이 발생하는(contact-rich) 환경에서의 조작 등, 예측하기 어려운 복잡한 물체를 다루는 연구들이 발표되었다.19 이는 로봇이 구조화되지 않은 실생활 환경에서 더욱 다양한 작업을 수행하기 위한 핵심적인 진전이다. 더불어,</p>
<p><strong>다개체 로봇 시스템(Multi-Robot Systems)</strong> 분야에서는 여러 로봇에게 효율적으로 작업을 할당하거나, 통신을 유지하며 공동의 목표를 달성하는 등, 단일 로봇의 한계를 넘어선 협력적 임무 수행 기술이 발전했다.19</p>
<p>이 시기 세 학회에서 각각 두각을 나타낸 SimCLR(ICML), NeRF(arXiv 및 CVPR 커뮤니티), 그리고 Deep Drone Acrobatics(RSS)는 표면적으로는 각각 표현 학습, 3D 렌더링, 로봇 제어라는 다른 문제를 다루고 있다. 그러나 이들 연구의 기저에는 ’관찰로부터의 학습(Learning from Observation)’이라는 공통된 철학이 흐르고 있다. SimCLR는 레이블 없는 이미지 데이터 자체의 내재적 구조, 즉 서로 다른 증강(augmentation) 버전 간의 관계를 ’관찰’하여 표현을 학습한다.21 NeRF는 여러 시점에서 촬영된 2D 이미지들을 ’관찰’하여 3D 공간의 연속적인 표현을 학습한다.4 ’Deep Drone Acrobatics’의 제어 정책은 시뮬레이션 내에서 완벽한 제어를 수행하는 전문가 컨트롤러의 궤적을 ’관찰’하여 학습한다.6 이 세 연구는 모두 전통적인 수동적 파이프라인(특징 공학, 메시 모델링, 고전 제어 이론)을 데이터 기반의 종단간(end-to-end) 신경망 최적화로 대체했다는 공통점을 가진다. 이는 AI와 로보틱스가 원시 데이터로부터 직접적으로 복잡한 세계 모델과 행동을 추론하는 방향으로 나아가고 있음을 시사하는 중요한 변곡점이라 할 수 있다.</p>
<h2>3.  시각적 표현 학습의 패러다임 전환: SimCLR</h2>
<h3>3.1  SimCLR의 핵심 방법론: 단순한 대조 학습 프레임워크</h3>
<p>SimCLR(A Simple Framework for Contrastive Learning of Visual Representations)은 이전에 제안된 복잡한 대조적 자기지도 학습 알고리즘들을 단순화한 프레임워크로, 특별한 네트워크 구조나 대규모 메모리 뱅크 없이도 당시 최고 수준의 성능을 달성했다.2 이 방법론의 핵심 아이디어는 동일한 이미지로부터 생성된 서로 다른 증강(augmented) 버전, 즉 ’positive pair’의 표현은 잠재 공간(latent space)에서 가깝게 만들고, 서로 다른 이미지에서 비롯된 ’negative pairs’의 표현과는 멀어지도록 모델을 학습시키는 것이다.21</p>
<p>SimCLR 프레임워크는 네 가지 핵심 구성 요소로 이루어진다.</p>
<ol>
<li><strong>확률적 데이터 증강 (Stochastic Data Augmentation):</strong> SimCLR 성공의 가장 중요한 요소로 평가된다. 단일 변환 기법이 아닌, 여러 증강 기법의 ’조합’이 효과적인 예측 작업을 정의하는 데 결정적인 역할을 한다는 점을 실험적으로 증명했다.2 특히</li>
</ol>
<p><strong>무작위 자르기(random cropping)와 색상 왜곡(color distortion)의 조합</strong>은 최고 성능을 달성하는 데 필수적이었다.28 이러한 강력한 증강은 모델이 색상 히스토그램과 같은 피상적이고 쉬운 단서에 의존하는 것을 방지하고, 객체의 형태와 같은 더 본질적이고 일반화 성능이 높은 표현을 학습하도록 강제하는 효과를 낳는다.28</p>
<ol start="2">
<li>
<p><strong>기본 인코더 (Base Encoder):</strong> 증강된 이미지로부터 표현 벡터 <span class="math math-inline">h</span>를 추출하는 신경망으로, 논문에서는 주로 ResNet 아키텍처를 사용했다.27 흥미로운 발견은, 네트워크의 깊이와 너비를 증가시킬 때 지도 학습보다 자기지도 학습에서 성능 향상의 폭이 더 컸다는 점이다. 이는 대규모 비지도 데이터가 더 큰 모델의 잠재력을 끌어내는 데 효과적일 수 있음을 시사한다.27</p>
</li>
<li>
<p><strong>프로젝션 헤드 (Projection Head):</strong> 인코더의 출력인 표현 벡터 <span class="math math-inline">h</span>를 대조 손실(contrastive loss)이 계산되는 더 낮은 차원의 잠재 공간으로 매핑하는 작은 신경망(MLP)이다.2 표현 벡터</p>
</li>
</ol>
<p><span class="math math-inline">h</span>와 대조 손실 사이에 <strong>학습 가능한 비선형 변환</strong>을 도입하는 것이 최종적으로 학습되는 표현의 질을 크게 향상시킨다는 점을 발견했다.2 프로젝션 헤드는 대조 학습 과제에만 특화되어 증강에 불변적인 특징(예: 색상 정보)을 일부러 버리도록 학습될 수 있다. 따라서 실제 다운스트림 작업(예: 이미지 분류)에는 프로젝션 헤드를 통과하기 전의 풍부한 정보가 담긴 표현 <span class="math math-inline">h</span>를 사용하는 것이 더 효과적이다.27</p>
<ol start="4">
<li><strong>대조 손실 함수 (Contrastive Loss Function):</strong> NT-Xent(Normalized Temperature-scaled Cross-Entropy) 손실 함수를 사용하여 positive pair는 당기고 negative pair는 밀어내는 역할을 수행한다.</li>
</ol>
<p>SimCLR의 가장 큰 기여는 특정 아키텍처나 복잡한 손실 함수가 아니라, ’어떤 변환에도 불구하고 이미지의 본질은 무엇인가?’라는 근본적인 질문을 데이터 증강의 조합을 통해 효과적으로 ’문제화’했다는 점에 있다. 기존의 자기지도 학습은 회전 각도 맞추기, 직소 퍼즐 풀기 등 사람이 설계한 휴리스틱한 ’가짜 문제(pretext task)’에 의존하는 경우가 많았다.29 반면, SimCLR는 무작위 자르기와 색상 왜곡의 조합을 통해 모델이 ’위치, 크기, 색감의 변화에도 불구하고 동일한 객체임을 인식’하는, 보다 근본적이고 일반화 성능이 높은 문제를 풀도록 강제했다.28 이는 ’좋은 표현’이란 ’좋은 문제’를 풀 때 나온다는 철학을 실증적으로 증명한 것으로, 문제 자체를 잘 설계하는 것(여기서는 데이터 증강 전략)이 모델 아키텍처를 복잡하게 만드는 것보다 더 중요할 수 있음을 보여주었다.</p>
<h3>3.2  NT-Xent 손실 함수 분석</h3>
<p>SimCLR에서 사용된 NT-Xent(Normalized Temperature-scaled Cross-Entropy) 손실 함수는 대조 학습의 핵심 메커니즘을 구현한다. 주어진 positive pair <span class="math math-inline">(i, j)</span>에 대해, 크기 <span class="math math-inline">N</span>의 미니배치 내에 존재하는 다른 모든 <span class="math math-inline">2(N-1)</span>개의 증강된 샘플들을 negative 샘플로 간주한다.22 손실 함수는 positive pair 간의 유사도는 높이고, anchor 샘플과 모든 negative 샘플 간의 유사도는 낮추도록 설계되었다.</p>
<p>수학적으로 positive pair <span class="math math-inline">(i, j)</span>에 대한 손실 <span class="math math-inline">\ell_{i,j}</span>는 다음과 같이 정의된다.2</p>
<p><span class="math math-display">
\ell_{i,j} = -\log \frac{\exp(\text{sim}(z_i, z_j) / \tau)}{\sum_{k=1}^{2N} \mathbbm{1}_{[k \neq i]} \exp(\text{sim}(z_i, z_k) / \tau)}
</span><br />
여기서 <span class="math math-inline">sim(u, v)</span>는 두 벡터 <span class="math math-inline">u</span>와 <span class="math math-inline">v</span>의 L2 정규화 후 내적, 즉 코사인 유사도(<span class="math math-inline">u^T v / \|u\| \|v\|</span>)를 의미한다. <span class="math math-inline">z</span>는 프로젝션 헤드의 출력이며, <span class="math math-inline">\tau</span>는 온도(temperature) 하이퍼파라미터이다.2</p>
<p>이 손실 함수의 각 구성 요소는 중요한 역할을 수행한다.</p>
<ul>
<li>
<p><strong>코사인 유사도 및 L2 정규화:</strong> 표현 벡터를 단위 구(hypersphere) 상에 투영함으로써, 모델이 벡터의 크기가 아닌 방향에 집중하여 학습하게 한다. 이는 학습을 안정화하고 더 나은 품질의 표현을 얻는 데 기여하는 것으로 나타났다.27</p>
</li>
<li>
<p><strong>온도(<span class="math math-inline">\tau</span>):</strong> 소프트맥스 함수의 분포를 조절하여 학습의 난이도를 제어한다. 낮은 온도(<span class="math math-inline">\tau</span>) 값은 유사도 점수 간의 차이를 증폭시켜 모델이 어려운 negative 샘플(positive 샘플과 유사한 negative 샘플)에 더 집중하도록 유도한다. 이는 학습에 도움이 될 수 있지만, 너무 낮은 값은 학습을 불안정하게 만들 수 있어 적절한 값을 찾는 것이 중요하다.27</p>
</li>
<li>
<p><strong>대규모 배치 크기:</strong> SimCLR는 별도의 메모리 뱅크 없이 현재 미니배치 내의 샘플들을 negative로 사용한다. 따라서 배치 크기가 클수록 더 많고 다양한 negative 샘플을 한 번에 비교할 수 있게 되어 학습 효과가 크게 향상된다.27 이는 SimCLR가 높은 성능을 달성한 핵심 요인이었지만, 동시에 막대한 계산 자원을 요구하는 한계점으로 지적되기도 한다.37</p>
</li>
</ul>
<h3>3.3  실험 결과 및 학계에 미친 영향</h3>
<p>SimCLR은 그 단순성에도 불구하고 당시 자기지도 및 준지도 학습 분야에서 전례 없는 성능을 기록하며 큰 파장을 일으켰다.</p>
<p>ImageNet 데이터셋을 사용한 실험 결과는 SimCLR의 우수성을 명확히 보여준다. **선형 평가(Linear Evaluation)**는 사전 학습된 인코더의 가중치를 고정한 채, 그 위에 선형 분류기만 추가로 학습하여 표현의 질을 평가하는 방식이다. 이 평가에서 SimCLR로 학습된 ResNet-50 모델은 ImageNet Top-1 정확도 76.5%를 달성했다. 이는 당시 최고 성능을 큰 폭으로 경신한 것이며, 동일한 구조의 모델을 지도 학습으로 처음부터 학습시킨 경우와 필적하는 놀라운 결과였다.2</p>
<p><strong>준지도 학습(Semi-supervised Learning)</strong> 환경에서는 SimCLR의 효율성이 더욱 빛을 발했다. 전체 ImageNet 레이블의 단 1%만을 사용하여 모델 전체를 미세 조정(fine-tuning)했을 때, Top-5 정확도 85.8%를 달성했다.3 이는 극소수의 레이블만으로도 매우 높은 성능을 낼 수 있음을 증명한 것으로, 레이블링 비용이 비싼 현실 세계 문제에 자기지도 학습이 강력한 해결책이 될 수 있음을 시사했다.</p>
<table><thead><tr><th>방법 (Method)</th><th>사용 레이블 (Labels Used)</th><th>Top-1 정확도 (%)</th><th>Top-5 정확도 (%)</th></tr></thead><tbody>
<tr><td>지도 학습 (Supervised ResNet-50)</td><td>100%</td><td>~76.5</td><td>-</td></tr>
<tr><td>SimCLR (선형 평가)</td><td>0%</td><td>76.5</td><td>93.2</td></tr>
<tr><td>SimCLR (미세 조정)</td><td>1%</td><td>63.0</td><td>85.8</td></tr>
</tbody></table>
<p>SimCLR의 성공은 학계와 산업계에 막대한 영향을 미쳤다. 학계에서는 SimCLR의 계산 비용 문제를 해결하기 위한 MoCo, negative 샘플 없이 학습하는 BYOL, 클러스터링 기반의 SwAV 등 다양한 후속 연구들이 폭발적으로 증가했다.38 이로써 자기지도 학습은 레이블링 비용 문제를 해결할 수 있는 강력한 대안으로 부상하며 머신러닝의 주류 연구 분야로 자리매김했다.21 더 나아가, SimCLR과 같은 방법론은 웹 규모의 비정형 데이터로 거대 모델을 사전 학습한 후, 다양한 다운스트림 작업에 전이 학습하는 ‘파운데이션 모델’ 패러다임의 이론적, 실증적 기반을 마련했다.39 산업계에서도 구글이 의료 영상 분석과 같이 레이블이 부족한 전문 분야에 SimCLR 기반의 사전 학습 모델을 활용하는 등, 그 실용적 가치를 입증하기 시작했다.41</p>
<h2>4.  3D 장면 표현의 혁신: NeRF</h2>
<h3>4.1  NeRF의 5D 연속 장면 표현</h3>
<p>NeRF(Neural Radiance Fields for View Synthesis)는 여러 다른 시점에서 촬영된 희소한 2D 이미지 세트만을 사용하여, 해당 장면의 새로운 시점 이미지를 매우 사실적으로 합성하는 혁신적인 방법론이다.4 기존의 3D 표현 방식인 복셀 그리드(voxel grid)나 메시(mesh)가 이산적인 기하학적 구조를 사용하는 것과 달리, NeRF는 장면을</p>
<p><strong>연속적인 5D 함수</strong>로 표현함으로써 저장 공간의 한계를 극복하고 전례 없는 수준의 디테일을 구현했다.23</p>
<p>NeRF가 모델링하는 5D 함수는 3D 공간 상의 한 점의 좌표 <span class="math math-inline">(x, y, z)</span>와 그 점을 바라보는 2D 시선 방향 <span class="math math-inline">(\theta, \phi)</span>을 입력으로 받는다. 그리고 그 결과로 해당 지점의 **볼륨 밀도(volume density, <span class="math math-inline">\sigma</span>)**와 **시선 방향에 따라 달라지는 색상(view-dependent color, <span class="math math-inline">c</span>)**을 출력한다.4 이 연속적인 5D 함수는 별도의 합성곱 층(convolutional layer)이 없는 단순한 심층 신경망, 즉 **MLP(Multi-Layer Perceptron)**를 통해 근사된다.23</p>
<p>NeRF의 MLP 아키텍처는 물리적으로 타당한 장면 표현을 학습하기 위해 독특하게 설계되었다. 볼륨 밀도 <span class="math math-inline">\sigma</span>는 객체의 형태를 결정하는 물리량으로, 보는 방향과 관계없이 특정 위치에 고유해야 한다. 반면, 색상 <span class="math math-inline">c</span>는 표면의 재질에 따라 보는 각도에 따라 하이라이트(specular reflection) 등이 변할 수 있다. 이를 반영하여 NeRF의 MLP는 볼륨 밀도 <span class="math math-inline">\sigma</span>를 오직 위치 <span class="math math-inline">x</span>만의 함수로 예측하고, 색상 <span class="math math-inline">c</span>는 위치 <span class="math math-inline">x</span>와 시선 방향 <span class="math math-inline">d</span> 모두의 함수로 예측하도록 구조화되었다. 이러한 설계는 **다중 시점 일관성(multi-view consistency)**을 자연스럽게 강제하여, 더욱 사실적인 3D 장면 표현을 학습하게 한다.23</p>
<h3>4.2  미분 가능한 볼륨 렌더링과 최적화 기법</h3>
<p>NeRF는 고전 컴퓨터 그래픽스의 볼륨 렌더링(volume rendering) 기법을 차용하여 2D 이미지를 생성한다. 특정 픽셀의 색상을 결정하기 위해, 카메라 중심에서 해당 픽셀을 통과하는 가상의 광선(ray)을 쏜다. 이 광선을 따라 여러 점들을 샘플링하고, 각 샘플링된 점에서의 색상(<span class="math math-inline">c</span>)과 밀도(<span class="math math-inline">\sigma</span>) 값을 MLP를 통해 얻은 후, 이를 광선의 경로를 따라 적분(accumulate)하여 최종 픽셀 색상을 계산한다.23</p>
<p>광선 <span class="math math-inline">r</span>에 대한 최종 색상 <span class="math math-inline">C(r)</span>을 계산하는 볼륨 렌더링 방정식은 다음과 같은 적분식으로 표현되며, 실제 구현에서는 이를 이산적인 합으로 근사하여 계산한다.48</p>
<p><span class="math math-display">
C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t)) \mathbf{c}(\mathbf{r}(t), \mathbf{d}) dt, \quad \text{where } T(t) = \exp\left(-\int_{t_n}^{t} \sigma(\mathbf{r}(s)) ds\right)
</span><br />
이 방정식에서 <span class="math math-inline">T(t)</span>는 광선이 <span class="math math-inline">t_n</span>부터 <span class="math math-inline">t</span>까지 진행하는 동안 다른 입자에 부딪히지 않고 살아남을 확률, 즉 투과율(transmittance)을 의미한다. 전체 렌더링 과정이 미분 가능(differentiable)하기 때문에, 렌더링된 이미지와 실제 촬영된 이미지 간의 오차(단순한 L2 손실)를 계산하고, 이를 역전파하여 MLP의 가중치를 종단간(end-to-end) 방식으로 최적화할 수 있다.4</p>
<p>하지만 단순한 MLP는 고주파 정보를 잘 학습하지 못하는 경향(spectral bias)이 있어, 장면의 미세한 디테일을 표현하지 못하고 흐릿한 결과물을 생성하는 한계가 있다.46 NeRF는 이를 해결하기 위해 두 가지 핵심적인 최적화 기법을 도입했다.</p>
<ol>
<li>
<p><strong>위치 인코딩 (Positional Encoding):</strong> 입력 좌표 <span class="math math-inline">(x, y, z, \theta, \phi)</span>를 직접 MLP에 입력하는 대신, 이를 일련의 삼각함수를 통해 고차원 공간으로 매핑하는 위치 인코딩을 적용한다. 이는 푸리에 특징 매핑(Fourier feature mapping)의 일종으로, MLP가 고주파 함수를 훨씬 효과적으로 학습할 수 있도록 돕는다.23 위치 인코딩 함수 <span class="math math-inline">\gamma(p)</span>는 다음과 같이 정의된다.45<br />
<span class="math math-display">
\gamma(p) = (\sin(2^0\pi p), \cos(2^0\pi p), \cdots, \sin(2^{L-1}\pi p), \cos(2^{L-1}\pi p))
</span></p>
</li>
<li>
<p><strong>계층적 볼륨 샘플링 (Hierarchical Volume Sampling):</strong> 광선을 따라 균일하게 점들을 샘플링하는 것은 비효율적이다. NeRF는 ’coarse’와 ’fine’이라는 두 개의 네트워크를 동시에 학습시킨다. 먼저, 비교적 적은 수의 샘플로 학습하는 coarse 네트워크를 통해 광선을 따라 밀도가 높은 영역, 즉 객체가 존재할 가능성이 높은 구간을 대략적으로 예측한다. 그 다음, 이 예측된 분포를 바탕으로 해당 중요 구간에 더 많은 샘플을 집중적으로 할당하여 fine 네트워크를 학습시킨다. 이 계층적 샘플링 전략은 렌더링 품질을 높이면서도 계산 효율성을 크게 개선한다.42</p>
</li>
</ol>
<p>NeRF의 혁신은 복잡한 3D 장면을 명시적인 기하학적 요소(메시, 복셀)로 표현하려는 기존의 접근법에서 벗어난 데 있다. 전통적인 3D 표현 방식은 해상도가 높아질수록 메모리 요구량이 기하급수적으로 증가하는 확장성 문제를 가지며, 반투명 효과나 복잡한 표면 디테일을 표현하기 어렵다.23 반면 NeRF는 MLP라는 매우 간결한 구조를 사용하여 장면을 연속적인 함수로 ’암시적(implicit)’으로 인코딩한다. 이 방식은 이론적으로 무한한 해상도를 가지며, 메모리 사용량은 장면의 복잡도와 무관하게 네트워크 크기에만 의존한다.23 MLP가 좌표를 입력받아 밀도와 색상을 출력하는 방식은 장면의 기하학적 정보와 외형 정보를 하나의 통합된 함수로 학습하게 한다. 이 ‘암시적 표현’ 방식이 반사, 굴절 등 복잡한 빛의 상호작용을 자연스럽게 포착하여 기존 방식으로는 불가능했던 수준의 사실감을 달성하게 한 것이다.4 이는 3D 비전 분야에서 ’어떻게 표현할 것인가’의 문제를 ’어떻게 학습할 것인가’의 문제로 전환시킨 중요한 전환점이었으며, 이후 다양한 ‘뉴럴 필드(Neural Fields)’ 연구의 시발점이 되었다.51</p>
<h3>4.3  성능 및 응용 분야</h3>
<p>NeRF는 발표 당시 기존의 최신(state-of-the-art) 방법들을 PSNR(Peak Signal-to-Noise Ratio), SSIM(Structural Similarity Index)과 같은 정량적 평가 지표에서 큰 차이로 능가했다.4 특히 복잡한 기하학적 구조와 반사 및 하이라이트와 같은 시선 의존적 외형(view-dependent appearance)을 가진 장면에 대해 전례 없는 수준의 사실적인 뷰 합성을 달성하며 기술적 우수성을 입증했다.</p>
<table><thead><tr><th>방법 (Method)</th><th>Chair</th><th>Drums</th><th>Ficus</th><th>Hotdog</th><th>Lego</th><th>Materials</th><th>Mic</th><th>Ship</th><th>평균 (Average)</th></tr></thead><tbody>
<tr><td><strong>PSNR (dB) ↑</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>NeRF</td><td>33.00</td><td>25.01</td><td>30.13</td><td>36.18</td><td>32.54</td><td>29.62</td><td>32.91</td><td>28.65</td><td>31.01</td></tr>
<tr><td>NeRF-Pytorch</td><td>33.31</td><td>25.14</td><td>30.28</td><td>36.52</td><td>31.80</td><td>29.25</td><td>32.50</td><td>28.54</td><td>30.92</td></tr>
<tr><td>MobileR2L (Ours)</td><td>33.66</td><td>25.05</td><td>29.80</td><td>36.84</td><td>32.18</td><td>30.54</td><td>34.37</td><td>28.75</td><td>31.34</td></tr>
<tr><td><strong>SSIM ↑</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>NeRF</td><td>0.967</td><td>0.925</td><td>0.964</td><td>0.974</td><td>0.961</td><td>0.949</td><td>0.980</td><td>0.856</td><td>0.947</td></tr>
<tr><td>NeRF-Pytorch</td><td>0.998</td><td>0.985</td><td>0.996</td><td>0.998</td><td>0.991</td><td>0.989</td><td>0.996</td><td>0.980</td><td>0.991</td></tr>
<tr><td>MobileR2L (Ours)</td><td>0.998</td><td>0.986</td><td>0.996</td><td>0.998</td><td>0.992</td><td>0.992</td><td>0.997</td><td>0.982</td><td>0.993</td></tr>
</tbody></table>
<p>NeRF의 등장은 다양한 산업 분야에 혁신적인 응용 가능성을 제시했다.</p>
<ul>
<li>
<p><strong>콘텐츠 제작 및 엔터테인먼트:</strong> 영화, 게임, 광고 등에서 사실적인 가상 환경과 디지털 에셋을 제작하는 데 활용될 수 있다.44 특히 NVIDIA가 발표한 Instant NeRF는 렌더링 속도를 수천 배까지 가속하여 실시간 상호작용을 가능하게 만들었고, 이는 3D 콘텐츠 제작의 패러다임을 바꾸고 있다.57</p>
</li>
<li>
<p><strong>가상/증강 현실(VR/AR):</strong> 현실 세계를 고품질 3D 모델로 캡처하여 사용자에게 몰입감 높은 가상 경험을 제공하는 데 핵심적인 기술로 자리 잡았다.42</p>
</li>
<li>
<p><strong>로보틱스:</strong> 로봇이 주변 환경에 대한 정확하고 상세한 3D 모델을 구축하여 자율 주행, 물체 조작, 시뮬레이션 등에 활용할 수 있다. 특히 기존 방법으로는 어려웠던 투명하거나 반사율이 높은 객체를 인식하고 조작하는 데 강점을 보인다.49</p>
</li>
</ul>
<p>NeRF의 성공은 렌더링 속도 개선(Instant-NGP, Plenoxels) 53, 도시 규모의 대규모 환경으로의 확장(Block-NeRF) 58, 시간에 따라 변화하는 동적 장면 처리(D-NeRF) 50, 그리고 적은 수의 이미지로 학습하는 기술(Few-shot NeRF) 53 등 폭넓은 후속 연구를 촉발시키며, 3D 비전 분야의 가장 활발한 연구 주제 중 하나로 자리매김했다.</p>
<h2>5.  로보틱스 분야의 주요 발전: 자율 드론 곡예 비행</h2>
<h3>5.1  시뮬레이션 기반 학습과 Sim-to-Real 전이</h3>
<p>쿼드콥터의 곡예 비행은 높은 추력과 극한의 각가속도를 요구하는 매우 어려운 제어 문제로, 전문 조종사에게도 상당한 훈련이 필요하다.6 기존의 자율 비행 시스템들은 종종 외부 모션 캡처 시스템이나 외부 컴퓨팅 자원에 의존하여 이러한 한계를 극복하고자 했다.61 ‘Deep Drone Acrobatics’ 연구는 이러한 제약을 극복하고 온보드 센서와 컴퓨팅만으로 고난도 곡예 비행을 가능하게 하는 새로운 접근법을 제시했다.</p>
<p>이 연구의 핵심 접근법은 **특권적 학습(Privileged Learning)**이다. 제어 정책(student)은 <strong>전적으로 시뮬레이션 환경에서만 학습</strong>된다.6 학습 과정에서 정책은 실제 비행 시에는 접근할 수 없는 <strong>특권적 정보(privileged information)</strong>, 즉 시뮬레이터의 완벽한 상태 정보(ground-truth state)에 접근 가능한 최적 컨트롤러(expert)의 비행 시연(demonstration)을 모방하여 학습한다.6 이 방식은 실제 로봇을 파손시킬 위험 없이 안전하게 학습을 진행할 수 있고, 인간 전문가의 시연이 필요 없으며, 심지어 인간 조종사에게도 어려운 기동을 학습할 수 있다는 명확한 장점을 가진다.6</p>
<p>가장 중요한 성과는 시뮬레이션에서 학습된 정책이 실제 데이터에 대한 어떠한 추가적인 미세 조정(fine-tuning) 없이도 <strong>직접 실제 쿼드콥터에 배포되어 성공적으로 작동</strong>했다는 점이다. 이는 **제로샷 Sim-to-Real 전이(Zero-shot Sim-to-Real Transfer)**의 성공적인 사례로, 시뮬레이션 기반 로봇 학습의 실용성을 한 단계 끌어올린 중요한 성취이다.6</p>
<h3>5.2  센서 데이터 추상화 및 제어 정책</h3>
<p>Sim-to-Real 전이의 가장 큰 난제는 시뮬레이션 환경과 현실 세계 간의 차이, 즉 ’현실 격차(reality gap)’이다. 특히 시각 정보의 경우, 시뮬레이터의 렌더링 결과와 실제 카메라 이미지 사이에는 조명, 질감, 노이즈 등에서 큰 차이가 존재한다. 이 연구는 이러한 시각적 격차를 해소하기 위해, 원본 카메라 이미지(raw image)를 직접 제어 정책의 입력으로 사용하지 않는 영리한 전략을 채택했다. 대신, 고전적인 컴퓨터 비전 기법을 사용하여 이미지로부터 <strong>특징점(feature points)과 같은 추상화된 시각 정보</strong>를 추출하고, 이를 정책의 입력으로 사용했다.6 이러한 <strong>센서 데이터 추상화(sensor data abstraction)</strong> 전략은 시뮬레이션과 현실의 시각적 차이에 덜 민감한, 보다 강건한 표현을 제공함으로써 Sim-to-Real 격차를 줄이는 데 핵심적인 역할을 수행했다.6</p>
<p>제어 정책은 신경망으로 표현되며, 서로 다른 주파수로 작동하는 카메라와 IMU(관성 측정 장치) 센서 데이터를 효과적으로 처리하기 위해 <strong>비동기식(asynchronous) 네트워크</strong> 아키텍처로 설계되었다.6 이 네트워크는 온보드 센서(카메라 이미지 <span class="math math-inline">I[k]</span>, 관성 측정값 <span class="math math-inline">\phi[k]</span>)로부터 추출된 정보의 이력(history)을 입력받아, 드론을 직접 제어하는 저수준 명령인 **질량 정규화된 총 추력(mass-normalized collective thrust, <span class="math math-inline">c</span>)과 동체 좌표계 기준 각속도(body rates, <span class="math math-inline">\omega</span>)**를 직접 출력(regress)한다.6</p>
<p>’Deep Drone Acrobatics’의 성공은 Sim-to-Real 문제에 대한 중요한 관점의 전환을 시사한다. 즉, 현실 세계를 완벽하게 모사하려는 시도보다, 현실과 시뮬레이션의 ‘차이’ 자체에 강건한 표현(abstraction)과 학습 전략을 찾는 것이 더 효과적일 수 있다는 것이다. Sim-to-Real의 전통적인 접근법 중 하나는 시뮬레이터의 물리 엔진과 렌더링을 최대한 현실과 가깝게 만드는 것(system identification)이지만, 이는 매우 어렵고 완벽할 수 없다.62 또 다른 접근법인 도메인 랜덤화(domain randomization)는 시뮬레이션의 다양한 파라미터를 무작위로 바꿔 모델이 변화에 강건해지도록 학습시킨다.63 이 연구는 제3의 길을 제시했다. 즉, 시뮬레이션과 현실 양쪽에서 모두 안정적으로 추출 가능한 ’추상화된 특징(abstracted features)’을 학습의 입력으로 사용하는 것이다.6 원본 픽셀값은 조명, 질감 등에서 Sim-Real 격차가 크지만, 특징점의 기하학적 관계는 상대적으로 일관성을 유지한다. 이는 ’현실의 모든 복잡성’을 모델링하려는 시도에서 벗어나, 제어에 필요한 ’핵심 정보’가 무엇인지를 정의하고, 그 정보가 Sim-Real 간에 불변하도록 ’표현 공간’을 설계하는 것이 중요함을 보여준다.</p>
<h3>5.3  실험 결과 및 의의</h3>
<p>제안된 방법론을 통해, 실제 쿼드콥터는 **파워 루프(Power Loop), 배럴 롤(Barrel Roll), 매티 플립(Matty Flip)**과 같은 인간 전문가에게도 도전적인 고난도 곡예 비행을 성공적으로 수행했다. 이 과정에서 드론은 최대 <strong>3g</strong>에 달하는 높은 가속도를 견뎌냈다.6</p>
<p>이 연구는 온보드 센싱과 컴퓨팅 자원만으로 고속, 고가속도의 복잡한 기동을 수행한 최초의 종단간(end-to-end) 센서모터 정책 학습 사례라는 점에서 기술적 성취가 크다.6 이는 민첩한 로봇 제어 분야에서 학습 기반 접근법의 엄청난 잠재력을 입증했으며, 특히 안전하고 효율적인 Sim-to-Real 전이 방법론을 구체적으로 제시했다는 점에서 학문적, 실용적 의의를 가진다. 이 연구는 이후 드론 레이싱, 복잡한 환경에서의 자율 비행 등 더욱 도전적인 과제에 대한 Sim-to-Real 연구를 촉진하는 중요한 계기가 되었다.64</p>
<h2>6.  arXiv를 통해 본 2020년 6월의 연구 흐름</h2>
<h3>6.1  컴퓨터 비전 (cs.CV) 분야 동향</h3>
<p>2020년 6월 한 달 동안 arXiv의 컴퓨터 비전(cs.CV) 분야에는 총 1,397편의 논문이 등록되었다.66 이는 같은 해 CVPR 전체 채택 논문 수와 맞먹는 방대한 양으로, 공식적인 학회 발표 외에도 수많은 연구가 실시간으로 공유되는 활발한 연구 생태계를 보여준다. 등록된 논문들 중에는 CVPR 2020에서 채택된 연구들이 다수 포함되어 있어, 학회 발표와 arXiv를 통한 사전 공개가 거의 동시에 이루어지는 빠른 연구 속도전 양상을 확인할 수 있다.66 연구 주제는 의료 영상 분석, 얼굴 및 제스처 인식, 로보틱스 비전 등 다양한 응용 분야를 포괄했다.66</p>
<p>특히 이 시기에는 기술 발전의 이면에 대한 중요한 성찰이 제기되었다. Vinay Prabhu 등이 발표한 ‘Large scale vision datasets: common practices and consequences’(<span class="math math-inline">arXiv:2006.16923</span>)는 ImageNet, Tiny Images와 같은 대규모 비전 데이터셋에 내재된 심각한 문제들을 공론화했다.69 이 연구는 데이터 수집 과정에서의 동의 문제, 사생활 침해, 인종 및 성별 편향, 심지어 검증 가능한 포르노그래피 이미지의 포함과 같은 윤리적 문제들을 구체적인 사례와 함께 지적했다. 저자들은 데이터셋 구축 과정에 대한 제도적 검토 위원회(Institutional Review Board, IRB)의 의무화를 주장하며, AI 연구 커뮤니티에 데이터 윤리에 대한 중요한 경각심을 불러일으켰다.</p>
<h3>6.2  로보틱스 (cs.RO) 분야 동향</h3>
<p>같은 기간 로보틱스(cs.RO) 분야에는 총 314편의 논문이 등록되었다.70 이 논문들에서도 당시 연구의 핵심 화두를 엿볼 수 있다. RSS 2020에 채택된 다수의 논문이 포함되어 있었으며, 이는 <strong>Sim-to-Real 및 학습 기반 제어</strong>가 로보틱스 연구의 주류로 자리 잡고 있음을 재확인시켜 주었다.70 강인한 보행 로봇 제어, 자율 주행 시나리오에서의 보행자 의도 예측, 물리 기반 촉각 신호 해석 등 다양한 문제에 학습 기반 접근법이 활발하게 적용되었다.70</p>
<p>또한, 불확실한 환경에서 로봇의 위치와 자세를 강건하게 추정하기 위한 센서 융합 기술도 지속적으로 발전했다. 다중 IMU와 다중 카메라를 결합하여 정확도를 높인 시각-관성 항법 시스템(Visual-Inertial Navigation System, VINS) 연구가 그 대표적인 예이다.67</p>
<p><strong>인간-로봇 상호작용(Human-Robot Interaction, HRI)</strong> 분야 역시 중요한 연구 주제였다. 인간의 물리적 교정과 같은 암시적 피드백으로부터 로봇이 보상 함수를 학습하거나, 자연어 명령을 이해하여 물체를 가져오는 등, 인간과 로봇이 보다 직관적이고 자연스럽게 소통하고 협력하기 위한 연구가 활발히 진행되었다.71</p>
<p>2020년 6월의 연구 지형은 SimCLR, NeRF와 같이 경이로운 성능 향상을 이룬 기술적 돌파구와 동시에, 대규모 데이터셋의 윤리적 문제를 지적하는 비판적 성찰이 공존하는 모습을 보여준다.69 이는 기술의 성숙도가 높아짐에 따라 그 사회적 책임에 대한 논의 역시 필연적으로 수반됨을 시사한다. SimCLR과 같은 자기지도 학습 모델의 성공은 웹에서 무차별적으로 수집된 방대한 이미지 데이터에 기반하며, NeRF 역시 수십, 수백 장의 사진을 입력으로 요구한다.4 이러한 데이터 수집 관행은 데이터 주체의 동의 없이 이미지가 수집되거나, 이로 인해 사회적 편향이 증폭되고 사생활이 침해될 수 있는 위험을 내포하고 있다.</p>
<p><span class="math math-inline">arXiv:2006.16923</span> 논문은 바로 이러한 문제를 정면으로 다루며, 기술적 성취의 이면에는 반드시 해결해야 할 윤리적, 사회적 과제가 존재한다는 인식을 AI 커뮤니티에 확산시키는 계기가 되었다.69 따라서 2020년 6월은 AI 커뮤니티가 ’어떻게 더 좋은 성능을 낼 것인가’라는 질문뿐만 아니라, ’어떻게 더 책임감 있게 연구할 것인가’라는 질문에 본격적으로 직면하기 시작한 중요한 시점이라 평가할 수 있다.</p>
<h2>7. 결론</h2>
<p>2020년 6월은 AI 및 로보틱스 분야에서 여러 패러다임 전환을 이끈 중요한 시기였다. 본 보고서에서 심층 분석한 세 가지 핵심 연구는 각각의 영역에서 새로운 지평을 열었다. SimCLR는 레이블에 대한 의존도를 극복하고 대규모 비지도 데이터를 활용한 표현 학습의 새로운 가능성을 제시했다. NeRF는 3D 비전과 그래픽스의 경계를 허무는 암시적 뉴럴 표현의 시대를 열었으며, 사실적인 3D 콘텐츠 생성의 문턱을 낮추었다. 로보틱스 분야에서는 ’Deep Drone Acrobatics’가 Sim-to-Real 문제에 대한 효과적인 해법을 제시하며, 시뮬레이션 기반 학습을 통해 복잡한 동적 제어 문제를 해결할 수 있음을 입증했다.</p>
<p>이들 연구는 공통적으로 ’관찰로부터의 학습’이라는 철학을 공유하며, 데이터 기반의 종단간 학습이 특징 공학, 명시적 모델링, 고전 제어 이론 등 전통적인 파이프라인을 대체하는 거대한 흐름을 명확히 보여주었다. 이는 AI 연구가 더욱 데이터 중심적이고, 더 적은 인간의 개입으로 더 복잡한 문제를 해결하는 방향으로 나아가고 있음을 의미한다. 동시에, 기술의 급격한 발전에 따른 데이터 윤리와 사회적 책임에 대한 논의가 본격적으로 시작된 시점이기도 하다. 대규모 데이터셋의 윤리적 문제에 대한 비판적 성찰은 기술의 성능 향상만큼이나 책임감 있는 연구의 중요성을 부각시켰다.</p>
<p>이 시기에 제시된 아이디어들은 이후 파운데이션 모델, 뉴럴 필드, 학습 기반 로봇 제어 등 현재 AI 및 로보틱스 연구를 주도하는 핵심 분야들의 직접적인 뿌리가 되었다. 2020년 6월의 성과와 성찰은 앞으로의 연구가 나아가야 할 방향, 즉 더 높은 성능, 효율성, 일반화 능력과 함께 더 높은 수준의 안전성, 신뢰성, 그리고 윤리성을 동시에 확보해야 하는 과제를 명확히 제시하고 있다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Robotics: Science and Systems July 12-16, 2020, https://roboticsconference.org/2020/</li>
<li>A Simple Framework for Contrastive Learning of Visual Representations, https://proceedings.mlr.press/v119/chen20j/chen20j.pdf</li>
<li>A Simple Framework for Contrastive Learning of Visual Representations - arXiv, https://arxiv.org/abs/2002.05709</li>
<li>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis, https://neuralfields.cs.brown.edu/paper_33.html</li>
<li>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis - ResearchGate, https://www.researchgate.net/publication/340049640_NeRF_Representing_Scenes_as_Neural_Radiance_Fields_for_View_Synthesis</li>
<li>Deep Drone Acrobatics - arXiv, http://arxiv.org/pdf/2006.05768</li>
<li>Drohnen lernen selbstständig akrobatische Manöver - UZH News - Universität Zürich, https://www.news.uzh.ch/en/articles/2020/Quadrokopter-Akrobatics.html</li>
<li>CVPR 2020 Accepted Paper List - Paper Copilot, https://papercopilot.com/paper-list/cvpr-paper-list/cvpr-2020-paper-list/</li>
<li>Assessing Image Quality Issues for Real-World Problems - CVPR 2020 Open Access Repository, https://openaccess.thecvf.com/content_CVPR_2020/html/Chiu_Assessing_Image_Quality_Issues_for_Real-World_Problems_CVPR_2020_paper.html</li>
<li>CVPR 2020 Open Access Repository, https://openaccess.thecvf.com/CVPR2020.py</li>
<li>CVPR 2020 Workshops - CVF Open Access - The Computer Vision Foundation, https://openaccess.thecvf.com/CVPR2020_workshops/menu.py</li>
<li>2020 CVPR Accepted Papers - Matt Deitke, https://mattdeitke.com/CVPR-2020/</li>
<li>Learning Multiview 3D Point Cloud Registration - CVPR 2020 Open Access Repository, https://openaccess.thecvf.com/content_CVPR_2020/html/Gojcic_Learning_Multiview_3D_Point_Cloud_Registration_CVPR_2020_paper.html</li>
<li>ICML 2020 Accepted Paper List - Paper Copilot, https://papercopilot.com/paper-list/icml-paper-list/icml-2020-paper-list/</li>
<li>Eighteen papers were accepted at ICML 2020 | Center for Advanced Intelligence Project, https://aip.riken.jp/news/icml2020/</li>
<li>ICML 2020 Papers - ICML 2025, https://icml.cc/virtual/2020/papers.html</li>
<li>ICML 2020 Poster Schedule, https://icml.cc/Conferences/2020/PosterSchedule</li>
<li>Conferences - RSS Foundation, https://roboticsfoundation.org/conferences/</li>
<li>Accepted Papers · Robotics: Science and Systems, https://roboticsconference.org/2020/program/papers.html</li>
<li>RSS 2020 Accepted Paper List, https://papercopilot.com/paper-list/rss-paper-list/rss-2020-paper-list/</li>
<li>SimCLR Explained: The ELI5 Guide for Engineers - Lightly, https://www.lightly.ai/blog/simclr</li>
<li>[Representation Learning] The SimCLR Family - Youngdo Lee, https://leeyngdo.github.io/blog/representation-learning/2023-01-15-simple-framework-for-contrastive-learning/</li>
<li>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis, https://www.cs.jhu.edu/~misha/ReadingSeminar/Papers/Mildenhall21.pdf</li>
<li>A Simple Framework for Contrastive Learning of Visual Representations - ResearchGate, https://www.researchgate.net/publication/339251894_A_Simple_Framework_for_Contrastive_Learning_of_Visual_Representations</li>
<li>[2002.05709] A Simple Framework for Contrastive Learning of Visual Representations - ar5iv, https://ar5iv.labs.arxiv.org/html/2002.05709</li>
<li>[D] The Illustrated SimCLR Framework (Blog) : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/fdclqc/d_the_illustrated_simclr_framework_blog/</li>
<li>A Simple Framework for Contrastive Learning of Visual Representations | Elias Z. Wang | AI Researcher &amp; PhD Candidate at Stanford, https://eliaszwang.com/paper-reviews/simple-framework-contrastive-learning/</li>
<li>Advancing Self-Supervised and Semi-Supervised Learning with …, https://research.google/blog/advancing-self-supervised-and-semi-supervised-learning-with-simclr/</li>
<li>SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, https://icml.cc/media/icml-2020/Slides/6762.pdf</li>
<li>Google Open Sources SimCLR, A Framework for Self-Supervised and Semi-Supervised Image Training - KDnuggets, https://www.kdnuggets.com/2020/04/google-open-sources-simclr-self-supervised-semi-supervised-image-training.html</li>
<li>Tutorial 17: Self-Supervised Contrastive Learning with SimCLR — UvA DL Notebooks v1.2 documentation, https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial17/SimCLR.html</li>
<li>Contrastive Learning - SimCLR and BYOL (With Code Example) - LearnOpenCV, https://learnopencv.com/contrastive-learning-simclr-and-byol-with-code-example/</li>
<li>NT-Xent Loss for Contrastive Learning - Emergent Mind, https://www.emergentmind.com/topics/normalized-temperature-scaled-cross-entropy-loss-nt-xent</li>
<li>NT-Xent Loss - A Quick Overview - Dilith Jayakody, https://dilithjay.com/blog/nt-xent-loss-explained</li>
<li>Contrastive Learning Loss: NT-Xent &amp; InfoNCE | by Frederik vom Lehn - Medium, https://medium.com/self-supervised-learning/nt-xent-loss-normalized-temperature-scaled-cross-entropy-loss-ea5a1ede7c40</li>
<li>The Bad Batches: Enhancing Self-Supervised Learning in Image Classification Through Representative Batch Curation - arXiv, https://arxiv.org/html/2403.19579v1</li>
<li>SimCLR with less compute — MoCo-V2 in PyTorch | Analytics Vidhya - Aditya Rastogi, https://aditya-rastogi.medium.com/simclr-with-less-computational-constraints-moco-v2-in-pytorch-3d8f3a8f8bf2</li>
<li>Day 4 — Popular Frameworks in Contrastive Learning | by Deepali …, https://medium.com/@deepsiya10/day-4-popular-frameworks-in-contrastive-learning-26543ffa3fcf</li>
<li>Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses - arXiv, https://arxiv.org/html/2509.10620v1</li>
<li>[2509.10620] Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses - arXiv, https://www.arxiv.org/abs/2509.10620</li>
<li>Self-Supervised Learning and Its Applications - neptune.ai, https://neptune.ai/blog/self-supervised-learning</li>
<li>NeRF: Revolutionizing 3D Scene Reconstruction with Neural Radiance Fields - Medium, https://medium.com/@jimcanary/nerf-revolutionizing-3d-scene-reconstruction-with-neural-radiance-fields-1c28df282857</li>
<li>BeyondPixels: A Comprehensive Review of the Evolution of Neural Radiance Fields - arXiv, https://arxiv.org/html/2306.03000v3</li>
<li>What is NeRF? - Neural Radiance Fields Explained - AWS - Updated 2025, https://aws.amazon.com/what-is/neural-radiance-fields/</li>
<li>NeRF: Representing Scenes as Neural Radiance Fields for View …, https://arxiv.org/abs/2003.08934</li>
<li>Notes on NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis by Mildenhall et al | by M(A)C | Medium, https://medium.com/@chengmu/notes-on-nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis-by-mildenhall-et-al-fd88f715fe77</li>
<li>Unlocking the Potential of NeRF for Photorealistic Image Synthesis - OpenCV.ai, https://www.opencv.ai/blog/nerf-short-review</li>
<li>Volume Rendering Digest (for NeRF), https://theialab.ca/pubs/tagliasacchi2022volume.pdf</li>
<li>Neural radiance field - Wikipedia, https://en.wikipedia.org/wiki/Neural_radiance_field</li>
<li>NeRF and follow-up works - César Díaz Blanco, https://blancocd.com/notes/NeRF.pdf</li>
<li>Neural Radiance Fields for the Real World: A Survey - arXiv, https://arxiv.org/html/2501.13104v1</li>
<li>Deep Review and Analysis of Recent NeRFs - Now Publishers, https://www.nowpublishers.com/article/OpenAccessDownload/SIP-2022-0062</li>
<li>NeRF: Neural Radiance Field in 3D Vision: A Comprehensive Review - arXiv, https://arxiv.org/html/2210.00379v6</li>
<li>Neural Radiance Fields (NeRF): A Breakthrough in 3D … - Tooliqa, https://www.tooli.qa/insights/neural-radiance-fields-nerf-a-breakthrough-in-3d-reconstruction</li>
<li>Neural Radiance Fields - GeeksforGeeks, https://www.geeksforgeeks.org/artificial-intelligence/neural-radiance-fields/</li>
<li>Transform Images Into 3D Scenes With Instant NeRF - NVIDIA Blog, https://blogs.nvidia.com/blog/ai-decoded-instant-nerf/</li>
<li>Block-NeRF - Waymo, https://waymo.com/research/block-nerf/</li>
<li>Adapting neural radiance fields (NeRFs) to dynamic scenes - Amazon Science, https://www.amazon.science/blog/adapting-neural-radiance-fields-nerfs-to-dynamic-scenes</li>
<li>AIM 2024 Sparse Neural Rendering Challenge: Methods and Results - arXiv, https://arxiv.org/html/2409.15045v1</li>
<li>(PDF) Deep Drone Acrobatics - ResearchGate, https://www.researchgate.net/publication/342093628_Deep_Drone_Acrobatics</li>
<li>Quantifying the Sim-To-Real Gap in UAV Disturbance Rejection - DROPS, https://drops.dagstuhl.de/storage/01oasics/oasics-vol125-dx2024/OASIcs.DX.2024.16/OASIcs.DX.2024.16.pdf</li>
<li>SIM2REAL: How to Reduce the Reality Gap in Robotics - Reinforcement Learning Path, https://www.reinforcementlearningpath.com/sim2real/</li>
<li>Sim-to-Real Deep Reinforcement Learning for Safe End-to-End Planning of Aerial Robots, https://www.mdpi.com/2218-6581/11/5/109</li>
<li>Zero-Shot Sim-to-Real Visual Quadrotor Control with Hard Constraints - arXiv, https://arxiv.org/html/2503.02198v1</li>
<li>Computer Vision and Pattern Recognition Jun 2020 - arXiv, https://arxiv.org/list/cs.CV/2020-06</li>
<li>Robotics Jun 2020 - arXiv, http://arxiv.org/list/cs.RO/2020-06?skip=150&amp;show=25</li>
<li>Computer Science Jun 2020 - arXiv, https://arxiv.org/list/cs/2020-06</li>
<li>[2006.16923] Large image datasets: A pyrrhic win for computer vision? - arXiv, https://arxiv.org/abs/2006.16923</li>
<li>Robotics Jun 2020 - arXiv, http://arxiv.org/list/cs.RO/2020-06?skip=25&amp;show=500</li>
<li>Robotics Jun 2020 - arXiv, http://arxiv.org/list/cs.RO/2020-06?skip=125&amp;show=100</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>