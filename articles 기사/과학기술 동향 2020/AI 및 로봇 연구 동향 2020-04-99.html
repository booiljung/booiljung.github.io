<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2020년 4월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2020년 4월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2020년 AI 및 로봇 연구 동향</a> / <span>2020년 4월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2020년 4월 AI 및 로봇 연구 동향</h1>
<h2>1. 서론: 가상 학회의 부상과 연구의 지속성</h2>
<p>2020년 초반은 전 세계적인 팬데믹으로 인해 학술 교류의 패러다임이 급변한 시기였다. AAAI 2020은 2월에 오프라인으로 성공적으로 개최되었으나 1, 4월을 기점으로 IEEE International Conference on Robotics and Automation (ICRA) 2020과 같은 주요 학회들이 가상(virtual) 형태로 전환되었다.2 이러한 변화는 연구 발표 및 논의의 장을 물리적 공간에서 디지털 플랫폼으로 이동시켰으며, 연구자들이 자신의 최신 성과를 신속하게 공유하는 통로로서 arXiv와 같은 사전 공개(pre-print) 서버의 역할을 더욱 부각시켰다.3 2020년 4월은 학계가 필연적으로 디지털 우선의 교류 모델을 전면적으로 수용한 중요한 변곡점을 나타낸다.</p>
<p>본 보고서는 이러한 특수한 상황 속에서도 중단 없이 이어진 2020년 4월의 인공지능(AI) 및 로봇 공학 분야 핵심 연구들을 심층적으로 분석한다. 특히 컴퓨터 비전 분야에서 중요한 이정표가 된 YOLOv4의 등장, International Conference on Learning Representations (ICLR) 2020에서 발표된 대규모 모델의 효율화 및 지능형 에이전트 관련 핵심 연구, 그리고 소프트 및 군집 로보틱스와 같은 첨단 로봇 공학 분야의 최신 동향을 면밀히 검토하여 해당 시점의 기술적 성취와 미래 방향성을 조망한다.</p>
<h2>2.  컴퓨터 비전의 재도약: YOLOv4와 실시간 객체 탐지의 최적화</h2>
<h3>2.1  YOLOv4의 등장과 아키텍처 분석</h3>
<p>2020년 4월 23일, Alexey Bochkovskiy, Chien-Yao Wang, Hong-Yuan Mark Liao는 arXiv를 통해 “YOLOv4: Optimal Speed and Accuracy of Object Detection“을 발표하며 실시간 객체 탐지 분야에 새로운 기준을 제시했다.5 이 연구는 기존 YOLOv3가 가졌던 정확도의 한계를 극복하고, 실시간 처리 속도와 탐지 정확도 사이의 최적의 균형점을 찾는 것을 목표로 했다.7</p>
<p>YOLOv4의 아키텍처는 크게 세 부분으로 구성된다. 첫째, <strong>Backbone</strong>으로는 CSPDarknet53을 채택했다.7 이는 기존 Darknet53 구조에 Cross-Stage Partial (CSP) 연결을 적용한 것으로, 특징 추출 과정에서 연산량을 효과적으로 줄이면서도 높은 수준의 특징 표현 능력은 유지하도록 설계되었다.7 둘째, <strong>Neck</strong> 부분에서는 다양한 크기의 특징 맵을 효과적으로 융합하기 위해 Spatial Pyramid Pooling (SPP) 블록과 Path Aggregation Network (PANet) 구조를 결합했다.7 SPP 블록은 고정된 크기의 특징 벡터를 생성하여 다양한 입력 이미지 크기에 대응하고, PANet은 상향식(bottom-up) 경로와 하향식(top-down) 경로를 모두 활용하여 저수준 특징과 고수준 의미 정보를 동시에 강화한다. 셋째, <strong>Head</strong>는 YOLOv3의 구조를 계승하여 최종적으로 Bounding Box의 위치와 객체의 클래스를 예측하는 역할을 수행한다.7</p>
<h3>2.2  성능 향상을 위한 핵심 전략: Bag of Freebies (BoF)와 Bag of Specials (BoS)</h3>
<p>YOLOv4의 가장 큰 특징은 단 하나의 혁신적인 아이디어를 제시한 것이 아니라, 기존에 제안되었던 수많은 기법들을 체계적으로 실험하고 최적으로 조합하여 성능을 극대화했다는 점이다. 저자들은 이러한 기법들을 두 가지 범주로 명확하게 분류하여 접근했다: ’Bag of Freebies (BoF)’와 ‘Bag of Specials (BoS)’.7</p>
<p>**Bag of Freebies (BoF)**는 추론(inference) 시에는 비용을 증가시키지 않으면서 모델의 정확도를 향상시키는 학습 전략 및 기법들을 의미한다. 이는 주로 데이터 증강과 손실 함수 개선에 초점을 맞춘다. 대표적인 예로는 4개의 다른 이미지를 하나의 이미지처럼 합쳐 학습시키는 <strong>Mosaic 데이터 증강</strong> 기법이 있다.9 이 방법은 모델이 일반적인 문맥에서 벗어난 객체를 탐지하는 능력을 키우고, 특히 작은 객체에 대한 탐지 성능을 향상시키는 효과를 가져왔다.7 또한, 모델의 강건성(robustness)을 높이기 위해 **Self-Adversarial Training (SAT)**을 도입했으며 12, Bounding Box 회귀의 정확도를 개선하기 위해 후술할 <strong>CIoU 손실 함수</strong>를 채택했다.5</p>
<p>**Bag of Specials (BoS)**는 추론 비용이 약간 증가하지만 그보다 훨씬 큰 폭으로 정확도를 향상시키는 플러그인 모듈이나 후처리 기법들을 포함한다. Backbone에는 <strong>Mish 활성화 함수</strong>, <strong>Cross-Stage Partial (CSP) 연결</strong>, <strong>Multi-input Weighted Residual Connections (MiWRC)</strong> 등을 적용하여 특징 추출 능력을 극대화했다.7 Neck 부분에는 <strong>SPP 블록</strong>과 <strong>PANet</strong>을 도입하여 특징 융합을 강화했으며, 후처리 단계에서는 **DIoU-NMS (Distance-IoU Non-Maximum Suppression)**를 사용하여 겹치는 객체들을 보다 정교하게 필터링했다.10</p>
<h3>2.3  CIoU 손실 함수의 수학적 원리</h3>
<p>Bounding Box 회귀 손실 함수는 객체 탐지 모델의 위치 정확도를 결정하는 핵심 요소다. YOLOv3까지는 주로 좌표값의 차이를 계산하는 L2 손실을 사용했으나, 이는 Bounding Box의 크기나 IoU(Intersection over Union)와 직접적인 관련이 적다는 한계가 있었다. YOLOv4는 이러한 문제를 해결하기 위해 IoU 기반 손실 함수의 발전된 형태인 <strong>CIoU (Complete IoU) Loss</strong>를 채택했다.9</p>
<p>CIoU는 기존의 GIoU (Generalized IoU)나 DIoU (Distance IoU)에서 한 단계 더 나아가, Bounding Box 간의 세 가지 핵심적인 기하학적 요소를 모두 고려한다: <strong>(1) 중첩 영역 (Overlap area), (2) 중심점 거리 (Central point distance), (3) 종횡비 일관성 (Aspect ratio consistency)</strong>. 이를 통해 예측된 Bounding Box가 실제(Ground Truth) Box와 겹치지 않는 경우에도 효과적인 그래디언트를 제공하며, 예측 박스가 실제 박스의 형태와 위치에 더 빠르고 안정적으로 수렴하도록 유도한다.</p>
<p>CIoU 손실 함수(<span class="math math-inline">L_{CIoU}</span>)는 다음과 같이 정의된다:</p>
<p><span class="math math-display">
L_{CIoU} = 1 - IoU + \frac{\rho^2(b, b^{gt})}{c^2} + \alpha v
</span><br />
여기서 <span class="math math-inline">IoU</span>는 예측 박스와 실제 박스 간의 Intersection over Union 값이다. 두 번째 항은 중심점 거리 페널티로, <span class="math math-inline">\rho(b, b^{gt})</span>는 두 박스의 중심점 <span class="math math-inline">b</span>와 <span class="math math-inline">b^{gt}</span> 간의 유클리드 거리를, <span class="math math-inline">c</span>는 두 박스를 모두 포함하는 가장 작은 볼록 박스(convex hull)의 대각선 길이를 나타낸다. 이 항은 두 박스가 멀리 떨어져 있을수록 손실을 증가시킨다.</p>
<p>세 번째 항은 종횡비 일관성을 측정하는 페널티다. 여기서 <span class="math math-inline">v</span>는 다음과 같이 정의된다:</p>
<p><span class="math math-display">
v = \frac{4}{\pi^2} \left( \arctan \frac{w^{gt}}{h^{gt}} - \arctan \frac{w}{h} \right)^2
</span><br />
<span class="math math-inline">w, h</span>와 <span class="math math-inline">w^{gt}, h^{gt}</span>는 각각 예측 박스와 실제 박스의 너비와 높이를 의미한다. 이 항은 두 박스의 종횡비가 유사할수록 0에 가까워진다. <span class="math math-inline">\alpha</span>는 <span class="math math-inline">v</span>의 중요도를 조절하는 양의 가중치 파라미터로, 다음과 같이 정의된다:</p>
<p><span class="math math-display">
\alpha = \frac{v}{(1 - IoU) + v}
</span><br />
이 가중치는 IoU가 높은, 즉 잘 맞는 예제에 대해서는 종횡비의 중요도를 낮추고, 잘 맞지 않는 예제에 대해서는 종횡비 일관성을 더 강하게 고려하도록 동적으로 조절하는 역할을 한다.10</p>
<h3>2.4  실험 결과 및 영향</h3>
<p>YOLOv4는 MS COCO 데이터셋에서 AP(Average Precision) 43.5%, AP50 65.7%의 높은 정확도를 달성하며, Tesla V100 GPU 환경에서 약 65 FPS의 실시간 처리 속도를 기록했다.5 이는 당시 비슷한 정확도를 보였던 EfficientDet 모델과 비교했을 때 약 2배 빠른 속도로, 실시간 객체 탐지 분야에서 속도와 정확도 간의 새로운 SOTA(State-of-the-Art)를 달성했음을 의미한다.11</p>
<table><thead><tr><th>모델 (Model)</th><th>Backbone</th><th>AP (MS COCO)</th><th>AP50 (MS COCO)</th><th>FPS (Tesla V100)</th></tr></thead><tbody>
<tr><td>YOLOv3</td><td>Darknet-53</td><td>33.0%</td><td>55.3%</td><td>~62</td></tr>
<tr><td>EfficientDet-D3</td><td>EfficientNet-B3</td><td>43.0%</td><td>61.9%</td><td>~31</td></tr>
<tr><td><strong>YOLOv4</strong></td><td><strong>CSPDarknet53</strong></td><td><strong>43.5%</strong></td><td><strong>65.7%</strong></td><td><strong>~65</strong></td></tr>
</tbody></table>
<p>YOLOv4의 성공은 객체 탐지 연구의 패러다임에 중요한 변화를 가져왔다. 이는 단일의 혁신적인 아키텍처를 추구하는 것에서 벗어나, 보다 전체론적인 ‘시스템 통합’ 접근 방식의 가치를 입증했다. YOLOv4는 세심한 공학적 노력, 광범위한 실험, 그리고 수십 개의 검증된 기법들을 지능적으로 조합함으로써 단일 요소의 혁신보다 우수한 결과를 낳을 수 있음을 보여주었다. 이 연구의 가장 큰 영향 중 하나는 단일 GPU 환경에서도 누구나 고성능 모델을 쉽게 학습하고 사용할 수 있도록 접근성을 크게 높였다는 점이다.11 이는 고성능 객체 탐지 기술을 대규모 자원을 보유한 산업 연구소의 전유물에서 벗어나게 하여 더 넓은 연구 커뮤니티에 힘을 실어주었다. 결과적으로 YOLOv4는 새로운 모델을 제공했을 뿐만 아니라, 이후의 연구들이 데이터 증강, 손실 함수, 활성화 함수 등 전체 학습 파이프라인을 최적화하는 데 더 많은 관심을 기울이게 하는 더 나은 ’모델 구축 템플릿’을 제시했다.7</p>
<h2>3.  대규모 모델의 효율화: ICLR 2020 핵심 연구 심층 분석</h2>
<p>ICLR 2020은 AI 모델의 규모가 기하급수적으로 커지는 추세 속에서, 이러한 대규모 시스템을 더 효율적으로 만들고 실용성을 높이는 연구들이 두각을 나타냈다. 특히 자연어 처리, 강화학습, 로보틱스 분야에서 발표된 주요 논문들은 각각 메모리, 컴퓨팅 자원, 학습 데이터의 효율성을 극대화하는 독창적인 해결책을 제시했다.</p>
<table><thead><tr><th>논문 제목 (Paper Title)</th><th>저자 (Authors)</th><th>해결 과제 (Problem)</th><th>핵심 기여 (Key Contribution)</th></tr></thead><tbody>
<tr><td>Compressive Transformers for Long-Range Sequence Modelling</td><td>Rae, J. W., et al.</td><td>Transformer의 고정된 컨텍스트 길이로 인한 장기 의존성 모델링 한계</td><td>과거의 은닉 상태를 폐기하는 대신 압축하여 계층적 메모리(세분화된 단기, 거친 장기)를 구성</td></tr>
<tr><td>SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference</td><td>Espeholt, L., et al.</td><td>기존 분산 강화학습의 비효율적인 자원 사용 및 높은 실험 비용</td><td>추론(Inference)을 액터(CPU)에서 분리하여 학습기(GPU/TPU)에서 중앙 집중식으로 처리하는 아키텍처</td></tr>
<tr><td>Learning To Explore Using Active Neural SLAM</td><td>Chaplot, D. S., et al.</td><td>End-to-end 강화학습 기반 자율 탐색의 낮은 샘플 효율성 및 일반화 성능</td><td>학습 기반 모듈(SLAM, Policy)과 고전적 경로 계획기를 결합한 모듈러, 계층적 접근 방식</td></tr>
</tbody></table>
<h3>3.1  장기 의존성 문제 해결을 위한 접근: 압축 트랜스포머 (Compressive Transformer)</h3>
<p>ICLR 2020에서 발표된 “Compressive Transformers for Long-Range Sequence Modelling“은 표준 Transformer 아키텍처가 긴 시퀀스를 처리할 때 직면하는 계산 및 메모리 비용 문제를 해결하고자 했다.19 기존의 Transformer-XL 모델은 고정된 크기의 메모리를 슬라이딩 윈도우 방식으로 사용하여 이전 세그먼트의 정보를 재사용함으로써 컨텍스트 길이를 확장했다.20 하지만 이 방식 역시 메모리 크기가 제한되어 있어 가장 오래된 정보는 결국 폐기될 수밖에 없었다.</p>
<p>Compressive Transformer는 여기서 한 단계 더 나아가, 가장 오래된 메모리를 버리는 대신 이를 **압축(compress)**하여 별도의 ’압축 메모리(compressed memory)’에 저장하는 혁신적인 방식을 제안했다.22 이로써 모델은 두 종류의 메모리, 즉 최근의 정보를 상세하게 담고 있는 ’세분화된 단기 기억’과 오래된 정보를 요약하여 담고 있는 ’거친 장기 기억’을 모두 활용하는 계층적 메모리 구조를 갖게 된다. 어텐션 메커니즘은 이 두 메모리 모두에 접근하여 훨씬 더 넓은 범위의 컨텍스트를 참조할 수 있게 된다.24 연구팀은 메모리 압축 방식으로 1D Convolution, 풀링(pooling), 그리고 과거 어텐션 가중치에 기반한 선택 등 다양한 기법을 실험하여 그 효과를 검증했다.22 이 모델은 WikiText-103과 Enwik8 벤치마크에서 각각 17.1 ppl(perplexity), 0.97 bpc(bits per character)를 달성하며 당시 SOTA를 경신했다.19</p>
<h3>3.2  분산 강화학습의 확장성과 효율성: SEED RL 아키텍처</h3>
<p>“SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference“는 대규모 분산 강화학습의 효율성을 극대화하기 위한 새로운 아키텍처를 제시했다.26 IMPALA나 Ape-X와 같은 기존의 액터-러너(Actor-Learner) 구조에서는 수많은 액터(주로 CPU에서 실행)가 환경과 상호작용하며 자체적으로 모델 추론까지 수행했다.27 이러한 방식은 두 가지 주요 비효율성을 야기했다. 첫째, CPU는 신경망 추론 연산에 비효율적이다. 둘째, 모든 액터가 최신 모델 파라미터를 학습기로부터 주기적으로 받아와야 하므로 높은 네트워크 대역폭을 요구한다.27</p>
<p>SEED RL은 이 문제를 해결하기 위해 <strong>모델 추론 기능을 중앙 학습기(Learner)로 집중</strong>시키는 과감한 구조 변경을 제안했다. 이 아키텍처에서 액터는 오직 환경과의 상호작용(관측값 수집 및 액션 수행)만 담당하고, 추론이 필요할 때마다 관측값을 학습기로 전송한다. 그러면 학습기에 연결된 가속기(GPU/TPU)가 여러 액터로부터 온 요청을 일괄적으로 처리하여 추론을 수행하고, 그 결과를 다시 액터에게 반환한다.27 이 구조는 최신 가속기의 강력한 병렬 연산 능력을 최대한 활용하여 초당 수백만 프레임의 경이적인 학습 속도를 가능하게 했으며, 실험에 소요되는 총 컴퓨팅 비용을 기존 방식 대비 40%에서 최대 80%까지 절감하는 성과를 거두었다.27</p>
<p>Compressive Transformer와 SEED RL은 각각 자연어 처리와 강화학습이라는 다른 분야에서 출발했지만, 2020년 4월의 중요한 기술적 흐름인 **‘계산 요소의 분리 및 전문화’**라는 공통된 원칙을 공유한다. Compressive Transformer는 단일했던 메모리를 ’단기(고해상도)’와 ’장기(압축)’로 분리하여 각 시간적 역할에 맞게 전문화했다. 마찬가지로 SEED RL은 ’환경 상호작용(액터)’과 ’신경망 추론(러너)’이라는 두 가지 작업을 분리하여, 각각의 작업에 가장 적합한 하드웨어(CPU와 GPU/TPU)에 할당했다. 두 연구 모두 모델과 데이터 규모가 커짐에 따라 단일 구성 요소가 병목 현상을 일으킨다는 문제에 직면했고, 그 해결책으로 시스템을 전문화된 하위 시스템으로 분해하는 방식을 채택했다. 이는 AI의 여러 하위 분야에서 공통적으로 나타나는 중요한 아키텍처 설계 원칙의 등장을 시사한다.</p>
<h2>4.  지능형 로보틱스의 진화: 탐색, 제어, 그리고 새로운 형태</h2>
<h3>4.1  학습과 고전의 융합, 자율 탐색: Active Neural SLAM</h3>
<p>ICLR 2020에서 발표된 “Learning To Explore Using Active Neural SLAM“은 미지의 3D 환경을 자율적으로 탐색하는 로봇을 위한 새로운 패러다임을 제시했다.29 기존의 순수 End-to-End 강화학습 기반 탐색 방식은 로봇이 취할 수 있는 모든 행동의 조합으로 인해 탐색 공간이 너무 방대하여 학습에 막대한 양의 데이터가 필요한, 즉 샘플 효율성이 매우 낮은 고질적인 문제를 겪어왔다.</p>
<p>Active Neural SLAM은 이 문제를 해결하기 위해 시스템을 세 개의 핵심 모듈로 분해하는 <strong>모듈러 및 계층적 접근법</strong>을 채택했다.</p>
<ol>
<li>
<p><strong>Neural SLAM 모듈</strong>: 학습 기반의 이 모듈은 로봇의 RGB-D 카메라 입력과 모션 센서 데이터를 받아 실시간으로 환경 지도(map)를 생성하고 지도상에서 로봇의 현재 위치(pose)를 추정한다.</p>
</li>
<li>
<p><strong>Global Policy</strong>: 이 강화학습 기반 정책은 Neural SLAM 모듈이 생성한 지도 전체를 입력으로 받아, 탐색을 가장 효율적으로 수행할 수 있는 장기적인 목표 지점(long-term goal)을 결정한다.</p>
</li>
<li>
<p><strong>Local Policy</strong>: 이 모방학습 기반 정책은 현재 로봇의 시각적 관측 정보와 단기 목표(short-term goal)를 바탕으로 ‘전진’, ‘좌회전’ 등 실제 로봇을 움직이는 저수준(low-level) 제어 명령을 출력한다.30</p>
</li>
</ol>
<p>이 시스템의 독창성은 Global Policy가 설정한 장기 목표까지의 최적 경로를 계산하는 데 A*나 Dijkstra와 같은 고전적인 경로 계획 알고리즘을 활용한다는 점이다.30 이처럼 데이터 기반 학습의 유연성(SLAM, Policy)과 고전 알고리즘의 안정성 및 효율성(Path Planning)을 결합함으로써, 순수 End-to-End 방식 대비 월등히 높은 샘플 효율성과 새로운 환경에 대한 일반화 성능을 달성했음을 실험적으로 입증했다.32</p>
<h3>4.2  로봇 제어의 정밀화: 모델 오차 감소를 위한 문맥 인식 비용 함수 설계</h3>
<p>ICRA 2020에 채택된 “Context-aware cost shaping to reduce the impact of model error in safe, receding horizon control” 연구는 모델 예측 제어(Model Predictive Control, MPC)의 현실적인 한계를 다룬다.34 MPC는 미래 상태를 예측하고 제어를 최적화하는 강력한 기법이지만, 예측에 사용되는 동역학 모델이 실제 시스템과 오차가 있을 경우 성능이 저하된다.35 특히, 실시간 제어를 위해 계산 효율성이 높은 단순화된 모델을 사용할 수밖에 없는 로봇 공학에서 이 문제는 더욱 두드러진다.</p>
<p>이 연구는 로봇이 동일한 경로를 반복적으로 주행하는 시나리오에 착안하여 독창적인 해결책을 제시한다. 과거 주행 데이터로부터 <strong>모델이 예측한 비용과 실제 발생한 비용 간의 오차를 학습</strong>하는 것이다. 그리고 이 학습된 ’비용 보정 모델’을 MPC의 기존 비용 함수에 추가하여(Cost Shaping), 컨트롤러가 모델의 예측이 부정확할 것으로 예상되는 특정 상태나 속도 영역에서 보다 보수적이고 안정적인 행동을 취하도록 유도한다.35 이 ‘문맥 인식’ 접근법은 온라인 모델 학습과 비용 함수 학습을 결합하여, 변화하는 실제 환경 조건 속에서 로봇 제어의 강건성과 성능을 크게 향상시킬 수 있음을 보여주었다.</p>
<h3>4.3  미래 로보틱스의 단면: 소프트 및 군집 로보틱스 연구 동향</h3>
<p>2020년 4월, 학술지 <em>Frontiers in Robotics and AI</em> 등을 통해 발표된 논문들은 미래 로봇 기술의 중요한 방향성을 제시했다.</p>
<p><strong>소프트 로보틱스</strong>는 기존의 단단한(rigid) 로봇과 달리 유연하고 변형 가능한 소재를 사용하여 제작되며, 생물체의 움직임과 구조를 모방하는 생체 모방(bio-inspiration)이 핵심적인 디자인 원리로 작용한다.37 이 시기의 주요 연구 주제는 유전체 탄성 액추에이터(DEA)나 3D 프린팅 센서와 같은 새로운 소프트 액추에이터 및 센서 개발, 연성 재료의 고질적인 문제인 내구성 해결, 그리고 복잡하고 비선형적인 변형을 정밀하게 제어하기 위한 학습 기반 모델링 기법 등이었다.38 이러한 연구들은 향후 로봇이 안전한 인간-로봇 상호작용, 비침습적 수술, 섬세한 농작물 수확 등 새로운 응용 분야로 확장될 가능성을 열어주었다.40</p>
<p><strong>군집 로보틱스</strong>는 다수의 로봇이 중앙 통제 없이 분산된 상호작용을 통해 협력적으로 문제를 해결하는 패러다임을 연구한다.41 2020년 4월에 발표된 한 중요한 리뷰 논문은 학술적 연구에서 제안되는 이상적인 완전 분산 알고리즘과 실제 산업 현장에서 사용되는 기술 간의 상당한 괴리가 존재함을 지적했다.42 산업계에서는 창발적 행동(emergent behavior)의 예측 불가능성, 통신 인프라의 한계 등의 현실적인 문제로 인해 여전히 중앙 집중식 제어에 크게 의존하고 있었다.42 이는 군집 로보틱스가 실제 환경에 널리 적용되기 위해 해결해야 할 핵심 과제가 알고리즘의 이론적 정교함뿐만 아니라, 예측 가능성, 안정성, 통신 효율성과 같은 실용적인 측면에 있음을 명확히 보여준다.</p>
<p>2020년 4월의 다양한 로봇 공학 연구들은 분야의 성숙을 알리는 중요한 신호를 보낸다. Active Neural SLAM은 순수 종단간 학습을 거부하고 더 강건한 하이브리드 모델을 채택했으며, MPC 연구는 실제 모델의 필연적인 오차를 직접 모델링하고 보상했다. 군집 및 소프트 로보틱스 리뷰는 산업적 채택을 가로막는 병목 현상을 명시적으로 분석했다. 이 모든 연구의 공통된 실마리는 ’현실과의 간극 메우기’로, 연구의 초점이 “로봇이 무엇을 할 수 있는가?“에서 “어떻게 하면 로봇이 실험실 밖에서도 일관되고 안전하게 작동하게 할 수 있는가?“로 실용적으로 전환되고 있음을 보여준다.</p>
<h2>5. 결론: 2020년 4월 연구가 제시하는 미래 방향성</h2>
<p>2020년 4월은 AI 및 로봇 공학 연구가 팬데믹이라는 외부적 도전에도 불구하고 중요한 내적 진전을 이룬 시기였다. 본 보고서에서 분석한 주요 연구들은 향후 기술 발전의 세 가지 핵심적인 방향성을 제시한다.</p>
<p>첫째, <strong>기존 기술의 창의적 통합과 공학적 최적화의 중요성</strong>이다. YOLOv4는 완전히 새로운 발견이 아닌, 수많은 기존 기법들의 체계적이고 공학적인 통합을 통해 SOTA를 달성하며 실용주의적 연구의 가치를 명확히 입증했다. 이는 미래의 혁신이 반드시 새로운 이론의 발견에서만 비롯되는 것이 아니라, 기존 요소들을 어떻게 조합하고 최적화하는지에 따라 결정될 수 있음을 보여준다.</p>
<p>둘째, <strong>대규모 시스템의 효율화 및 민주화</strong>이다. Compressive Transformer와 SEED RL은 각각 메모리와 컴퓨팅 자원의 사용을 최적화하는 방안을 제시했다. 이는 더욱 크고 강력한 AI 모델을 더 적은 비용과 자원으로 운용할 수 있는 길을 열어, 첨단 기술에 대한 접근성을 높이고 연구의 저변을 확대하는 데 기여한다.</p>
<p>셋째, <strong>학습 기반 접근법과 고전 이론의 시너지</strong>이다. Active Neural SLAM과 문맥 인식 MPC 연구는 순수 학습 기반 접근법이 가진 샘플 효율성이나 안정성의 한계를 인식하고, 데이터 기반 학습의 유연성과 고전적 제어 및 계획 이론의 안정성을 결합하는 하이브리드 접근법의 우수성을 보여주었다.</p>
<p>이러한 경향들은 향후 AI 및 로보틱스 기술이 더욱 강력해지는 동시에, 더 넓은 분야에서 실용적으로 적용될 수 있는 견고한 기반을 마련하고 있음을 시사한다. 2020년 4월의 연구들은 불확실한 환경 속에서도 기술 발전이 어떻게 지속되고, 현실의 문제를 해결하기 위해 스스로를 재정비하는지를 보여주는 중요한 사례로 기록될 것이다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>
<p>AAAI 2020 Conference - The Association for the Advancement of Artificial Intelligence, https://aaai.org/conference/aaai/aaai-20/</p>
</li>
<li>
<p>2020 IEEE International Conference on Robotics and Automation - ICRA 2020, https://ewh.ieee.org/soc/ras/conf/fullysponsored/icra/ICRA2020/www.icra2020.org/index.html</p>
</li>
<li>
<p>Robotics Apr 2020 - arXiv, http://arxiv.org/list/cs.RO/2020-04?skip=0&amp;show=500</p>
</li>
<li>
<p>arXiv.org e-Print archive, https://arxiv.org/</p>
</li>
<li>
<p>(Open Access) YOLOv4: Optimal Speed and Accuracy of Object Detection (2020) | Alexey Bochkovskiy | 12689 Citations - SciSpace, https://scispace.com/papers/yolov4-optimal-speed-and-accuracy-of-object-detection-mzs6seakj0</p>
</li>
<li>
<p>Y.M. (2020) Yolov4 Optimal Speed and Accuracy of Object Detection. arXiv preprint arXiv 2004.10934. - References - Scientific Research Publishing, https://www.scirp.org/reference/referencespapers?referenceid=3308955</p>
</li>
<li>
<p>A Study on Object Detection Performance of YOLOv4 for Autonomous Driving of Tram, https://www.mdpi.com/1424-8220/22/22/9026</p>
</li>
<li>
<p>A Real-Time Object Detector for Autonomous Vehicles Based on YOLOv4 - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC8683201/</p>
</li>
<li>
<p>What is YOLOv4? A Detailed Breakdown. - Roboflow Blog, https://blog.roboflow.com/a-thorough-breakdown-of-yolov4/</p>
</li>
<li>
<p>Review of YOLOv4 Architecture. Paper, Original Code, PyTorch Code - Cenk Bircanoglu, https://cenk-bircanoglu.medium.com/review-of-yolov4-architecture-f488ec32c1c4</p>
</li>
<li>
<p>(PDF) YOLOv4: Optimal Speed and Accuracy of Object Detection - ResearchGate, https://www.researchgate.net/publication/340883401_YOLOv4_Optimal_Speed_and_Accuracy_of_Object_Detection</p>
</li>
<li>
<p>Accelerating Object Detection with YOLOv4 for Real-Time Applications - arXiv, https://arxiv.org/html/2410.16320v1</p>
</li>
<li>
<p>A Comprehensive Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS - MDPI, https://www.mdpi.com/2504-4990/5/4/83</p>
</li>
<li>
<p>YOLOv4 Explained | CIOU Loss, CSPDarknet53, SPP, PANet | Everything about it, https://www.youtube.com/watch?v=b148nt9P8J0</p>
</li>
<li>
<p>YOLO Loss Function Part 1: SIoU and Focal Loss - LearnOpenCV, https://learnopencv.com/yolo-loss-function-siou-focal-loss/</p>
</li>
<li>
<p>YOLOv4: A Breakthrough in Real-Time Object Detection - arXiv, https://arxiv.org/html/2502.04161v1</p>
</li>
<li>
<p>DRIVING-SCENE IMAGE CLASSIFICATION USING DEEP LEARNING NETWORKS: YOLOV4 ALGORITHM - DiVA portal, https://www.diva-portal.org/smash/get/diva2:1689752/FULLTEXT01.pdf</p>
</li>
<li>
<p>Research on Road Object Detection Model Based on YOLOv4 of Autonomous Vehicle, https://www.researchgate.net/publication/377289181_Research_on_Road_Object_Detection_Model_Based_on_YOLOv4_of_Autonomous_Vehicle</p>
</li>
<li>
<p>Compressive Transformers for Long-Range Sequence Modelling | Request PDF, https://www.researchgate.net/publication/337241959_Compressive_Transformers_for_Long-Range_Sequence_Modelling</p>
</li>
<li>
<p>Transformer-XL: Attentive Language Models beyond a Fixed-Length Context - ACL Anthology, https://aclanthology.org/P19-1285.pdf</p>
</li>
<li>
<p>The Transformer Architecture with Hybrid Models | by Bijit Ghosh - Medium, https://medium.com/@bijit211987/the-transformer-architecture-with-hybrid-models-eca885e12056</p>
</li>
<li>
<p>Compressive Transformers for Long-Range Sequence Modelling - OpenReview, https://openreview.net/forum?id=SylKikSYDH</p>
</li>
<li>
<p>[D] What happened to Compressive Transformers? : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/qoyyy7/d_what_happened_to_compressive_transformers/</p>
</li>
<li>
<p>Hugging Face Reads, Feb. 2021 - Long-range Transformers, https://huggingface.co/blog/long-range-transformers</p>
</li>
<li>
<p>COMPRESSIVE TRANSFORMERS FOR LONG-RANGE SEQUENCE MODELLING - OpenReview, https://openreview.net/pdf?id=SylKikSYDH</p>
</li>
<li>
<p>[1910.06591] SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference - arXiv, https://arxiv.org/abs/1910.06591</p>
</li>
<li>
<p>SEED RL: SCALABLE AND EFFICIENT DEEP-RL WITH ACCELERATED CENTRAL INFERENCE - OpenReview, https://openreview.net/pdf?id=rkgvXlrKwH</p>
</li>
<li>
<p>Massively Scaling Reinforcement Learning with SEED RL - Google Research, https://research.google/blog/massively-scaling-reinforcement-learning-with-seed-rl/</p>
</li>
<li>
<p>Learning To Explore Using Active Neural SLAM | OpenReview, https://openreview.net/forum?id=HklXn1BKDH</p>
</li>
<li>
<p>Learning to Explore using Active Neural SLAM - CMU ML Blog, https://blog.ml.cmu.edu/2020/06/19/learning-to-explore-using-active-neural-slam/</p>
</li>
<li>
<p>Pytorch code for ICLR-20 Paper “Learning to Explore using Active Neural SLAM” - GitHub, https://github.com/devendrachaplot/Neural-SLAM</p>
</li>
<li>
<p>Learning to explore using active neural SLAM - ΑΙhub - AI Hub, https://aihub.org/2020/06/24/learning-to-explore-using-active-neural-slam/</p>
</li>
<li>
<p>LEARNING TO EXPLORE USING ACTIVE NEURAL SLAM - OpenReview, https://openreview.net/pdf/071ce51856763401b2c0898d6fdbdbe3f0800d03.pdf</p>
</li>
<li>
<p>ICRA 2020 | Dynamic Systems Lab | Prof. Angela Schoellig, https://www.dynsyslab.org/icra-2020/</p>
</li>
<li>
<p>Context-aware Cost Shaping to Reduce the Impact of Model Error in …, https://www.dynsyslab.org/wp-content/papercite-data/pdf/mckinnon-ral20.pdf</p>
</li>
<li>
<p>Context-aware Cost Shaping to Reduce the Impact of Model Error in Receding Horizon Control@ICRA2020 - YouTube, https://www.youtube.com/watch?v=xrgcO2-A9bo</p>
</li>
<li>
<p>Editorial: Current Advances in Soft Robotics - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2020.00056/epub</p>
</li>
<li>
<p>Current Advances in Soft Robotics: Best Papers From RoboSoft 2018, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2020.00056/full</p>
</li>
<li>
<p>Soft Robot Design, Manufacturing, and Operation Challenges: A Review - MDPI, https://www.mdpi.com/2504-4494/8/2/79</p>
</li>
<li>
<p>Advanced Design of Soft Robots with Artificial Intelligence - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC11176285/</p>
</li>
<li>
<p>Recent trends in robot learning and evolution for swarm robotics - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2023.1134841/full</p>
</li>
<li>
<p>Swarm Robotic Behaviors and Current Applications - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2020.00036/full</p>
</li>
<li></li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>