<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2020년 7월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2020년 7월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2020년 AI 및 로봇 연구 동향</a> / <span>2020년 7월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2020년 7월 AI 및 로봇 연구 동향</h1>
<h2>1.  2020년 7월, AI 패러다임의 변곡점</h2>
<p>2020년 7월은 인공지능(AI) 및 로봇 공학 분야의 역사에서 단순한 한 달을 넘어, 여러 독립적이고 강력한 기술적 흐름이 한데 모여 패러다임의 전환을 예고한 결정적 변곡점으로 기록된다. 이 시기는 세 가지 핵심적인 축을 중심으로 전개되었다. 첫째, 거대 언어 모델(Large Language Model, LLM)의 잠재력이 ’규모의 가설(scaling hypothesis)’을 통해 폭발적으로 입증되었다. 둘째, 세계 최고 수준의 학술대회에서는 딥러닝의 이론적 깊이가 한층 성숙했음을 증명하는 연구들이 쏟아져 나왔다. 셋째, 실험실의 연구 대상에 머물던 고등 로봇 기술이 실질적인 산업 도구로 상용화되는 가시적인 전환이 이루어졌다.</p>
<p>이러한 동향은 한국전자통신연구원(ETRI)이 발표한 ‘2020년 AI 7대 트렌드’ 보고서의 전망과 정확히 일치하며, 당시의 기술적 맥락을 이해하는 중요한 틀을 제공한다.1 보고서에서 제시된 ‘R&amp;D 혁신지능’, ‘창작지능의 진화’, 그리고 ’AI 호문쿨루스(Homunculus)’라는 개념은 2020년 7월의 주요 사건들을 관통하는 핵심 주제다. 예를 들어, OpenAI의 GPT-3 등장은 단순한 모방을 넘어 인간의 창작 활동을 넘보는 ’창작지능의 진화’를 상징했다. 또한, 세계 최고 권위의 머신러닝 학회인 ICML(International Conference on Machine Learning) 2020에서 발표된 심도 있는 이론 연구들은 AI가 인간 연구자의 사고방식을 바꾸고 R&amp;D 생산성을 향상시키는 ’R&amp;D 혁신지능’의 가능성을 보여주었다. 마지막으로, 보스턴 다이내믹스의 로봇 ’스팟(Spot)’의 상용화는 AI가 물리적 실체를 통해 외부 환경과 상호작용하며 지능을 발전시키는 ‘AI 호문쿨루스’ 개념의 현실화를 의미했다.2</p>
<p>이 현상들은 개별적인 사건이 아니라, 컴퓨팅 성능의 기하급수적 증가, 데이터 가용성의 폭발, 그리고 트랜스포머(Transformer)와 같은 foundational 알고리즘의 성숙이라는 근본적인 ’환경 조건’이 임계점에 도달했음을 시사한다. 이는 마치 생물학적 진화에서 다양한 종이 폭발적으로 등장한 ’캄브리아기 대폭발’에 비견될 수 있다. 2020년 7월은 AI 분야에서 거대 생성 모델, 이론적으로 엄밀한 최적화 기법, 물리적으로 구현된 지능이라는 각기 다른 ’종(species)’이 동시다발적으로 출현하며 AI 생태계의 다양성과 복잡성을 극적으로 확장시킨 순간이었다.</p>
<p>본 보고서는 2020년 7월에 발생한 이 세 가지 핵심적인 사건—OpenAI의 GPT-3 발표, ICML 2020의 주요 연구 성과, 그리고 보스턴 다이내믹스 ’스팟’의 산업적 적용—을 심층적으로 해부하고, 이들 간의 상호 연관성을 분석하며, AI 및 로봇 공학의 미래 궤도에 남긴 집합적 유산을 평가하는 것을 목표로 한다.</p>
<h2>2.  거대 언어 모델의 서막: GPT-3의 등장과 그 파급력</h2>
<p>2020년 6월과 7월에 걸쳐 공개된 OpenAI의 GPT-3는 단순한 모델 업데이트가 아니었다. 이는 ’더 많은 데이터와 더 큰 모델이 더 나은 성능을 낳는다’는 ’규모의 가설’이 극적으로 증명된 순간이었으며, AI 연구의 방향을 근본적으로 바꾼 사건이었다. GPT-3는 전례 없는 규모, 그로 인해 발현된 새로운 능력, 그리고 이를 가능하게 한 기술적 기반을 통해 거대 언어 모델 시대를 본격적으로 열었다.</p>
<h3>2.1  기술적 명세와 규모의 도약</h3>
<p>GPT-3의 가장 두드러진 특징은 압도적인 규모였다. 이 모델은 이전 버전인 GPT-2의 15억 개 파라미터와 비교하여 100배 이상 증가한 1,750억 개의 파라미터로 구성되었다.3 이는 당시 마이크로소프트가 발표했던 가장 큰 모델인 ’튜링 NLG(Turing NLG)’의 170억 개 파라미터를 10배 이상 뛰어넘는 수치로, AI 모델의 규모에 대한 기존의 통념을 완전히 깨뜨렸다.3</p>
<p>이러한 규모의 도약은 방대한 훈련 데이터와 막대한 컴퓨팅 자원이 결합된 결과였다. GPT-3는 Common Crawl, WebText2, 서적, 위키피디아 등 거의 1조 개에 달하는 단어로 구성된 데이터셋으로 훈련되었다.4 이 거대한 모델과 데이터를 처리하기 위해 마이크로소프트가 제공한 대규모 슈퍼컴퓨팅 클러스터가 사용되었으며, 훈련에는 수많은 NVIDIA V100 GPU가 동원되었다.3 OpenAI는 이 과정에서 cuDNN으로 가속화된 PyTorch 딥러닝 프레임워크를 활용하여 훈련 효율을 극대화했다.3 이처럼 GPT-3의 탄생은 단순히 알고리즘의 혁신이 아니라, 하드웨어, 소프트웨어, 데이터가 결합된 대규모 엔지니어링의 승리였다. 이는 AI 연구의 최전선이 더 이상 개인 연구자나 소규모 팀의 영역이 아니라, 막대한 자본과 인프라를 동원할 수 있는 거대 기술 기업의 각축장이 되었음을 명확히 보여주었다.</p>
<table><thead><tr><th>특성 (Feature)</th><th>GPT-2</th><th>GPT-3</th><th>의의 (Significance)</th></tr></thead><tbody>
<tr><td>파라미터 수 (Parameter Count)</td><td>15억 (1.5 Billion)</td><td>1,750억 (175 Billion)</td><td>100배 이상의 규모 증가, ‘규모의 가설’ 입증</td></tr>
<tr><td>아키텍처 (Architecture)</td><td>트랜스포머 (Transformer)</td><td>트랜스포머 (더 크고 깊음)</td><td>기존 아키텍처의 확장성을 통해 성능 극대화</td></tr>
<tr><td>훈련 데이터 크기 (Training Data Size)</td><td>약 40 GB (WebText)</td><td>약 45 TB (다중 소스 혼합)</td><td>웹 전체에 가까운 방대한 데이터로 일반화 능력 확보</td></tr>
<tr><td>핵심 능력 (Key Capability)</td><td>제로샷 일반화 (Zero-shot generalization)</td><td>퓨샷 인컨텍스트 학습 (Few-shot in-context learning)</td><td>파인튜닝 없이 새로운 태스크 수행 패러다임 제시</td></tr>
</tbody></table>
<h3>2.2  패러다임 전환: Few-Shot Learning</h3>
<p>GPT-3가 가져온 가장 혁신적인 변화는 ‘퓨샷 학습(Few-Shot Learning)’ 능력의 입증이었다. 기존의 AI 모델들은 특정 과업(task)을 수행하기 위해 해당 과업에 특화된 대규모 데이터셋으로 모델의 가중치를 업데이트하는 ‘파인튜닝(fine-tuning)’ 과정을 거쳐야 했다. 그러나 GPT-3는 파인튜닝 없이, 단지 몇 개의 예시(example)나 지시사항을 프롬프트(prompt)에 제시하는 것만으로도 새로운 과업을 수행하는 능력을 보여주었다.4</p>
<p>이러한 ‘인컨텍스트 학습(In-Context Learning)’ 능력은 세 가지 형태로 나타났다.</p>
<ol>
<li>
<p><strong>Zero-shot:</strong> 과업에 대한 설명만 제공하고 예시 없이도 과업을 수행한다.</p>
</li>
<li>
<p><strong>One-shot:</strong> 단 하나의 예시를 제공하여 과업을 수행한다.</p>
</li>
<li>
<p><strong>Few-shot:</strong> 몇 개의 예시를 제공하여 과업을 수행한다.</p>
</li>
</ol>
<p>GPT-3는 번역, 질의응답, 심지어 간단한 3자리 산수 연산이나 문맥에 맞는 새로운 단어 사용과 같은 즉석 추론이 필요한 과업에서도 강력한 퓨샷 성능을 보였다.3 이는 AI 모델과의 상호작용 방식을 근본적으로 바꾸는 계기가 되었다. 더 이상 모델을 ’훈련’시키는 전문가가 아니더라도, 자연어 프롬프트를 통해 모델의 행동을 ’프로그래밍’하는 것이 가능해졌기 때문이다. 이 개념은 훗날 ’프롬프트 엔지니어링(prompt engineering)’이라는 새로운 분야의 탄생으로 이어졌다. 모델의 가중치를 변경하는 대신, 추론 시점에 제공되는 컨텍스트를 조작하여 원하는 결과를 유도하는 방식은 AI의 접근성과 활용성을 극적으로 높이는 결과를 낳았다.</p>
<h3>2.3  초기 반응과 비판적 고찰</h3>
<p>GPT-3의 등장은 AI 커뮤니티에 엄청난 충격을 주었다. 연구자들은 “그것이 할 수 있는 일들에 완전히 압도되었다“고 표현했으며, 수주 내에 사용자들은 GPT-3를 이용해 시, 프로그램 코드, 노래, 웹사이트 등을 생성하며 그 잠재력을 탐색하기 시작했다.5 인간 평가자들이 사람이 쓴 기사와 구별하기 어려운 뉴스 기사를 생성하는 능력은 AI가 창의적인 영역까지 넘볼 수 있다는 가능성을 대중에게 각인시켰다.3</p>
<p>그러나 모든 반응이 긍정적이었던 것은 아니다. 일부 비판가들은 GPT-3가 근본적인 아키텍처 측면에서는 GPT-2와 거의 동일하며, “단지 트랜스포머를 더 크게 만들면 놀라운 일이 일어난다“는 기존의 발견을 더 큰 숫자로 재확인했을 뿐, 새로운 과학적 지식을 추가하지는 않았다고 평가했다.4 또한, 460만 달러 이상으로 추정되는 막대한 훈련 비용과, 모델을 오픈소스로 공개하는 대신 유료 API를 통해 접근을 제한한 OpenAI의 결정은 AI 연구의 상업화와 진입 장벽에 대한 논쟁을 불러일으켰다.6 이러한 비판에도 불구하고, GPT-3가 AI 연구의 방향을 ’규모’로 집중시키고, 후속 거대 언어 모델 경쟁의 서막을 열었다는 점은 부인할 수 없는 사실이다.</p>
<h2>3.  ICML 2020: 기계학습 연구의 최전선</h2>
<p>2020년 7월, 가상으로 개최된 ICML은 산업계의 규모 경쟁과 병행하여 학계에서 이루어지고 있는 이론적 깊이와 알고리즘 혁신의 현주소를 명확히 보여준 장이었다. 이 학회에서 발표된 연구들, 특히 최우수 논문상(Outstanding Paper Award)을 수상한 논문들은 최적화, 표현 학습, 그리고 핵심 아키텍처의 새로운 영역 적용과 같은 근본적인 문제들을 어떻게 다루고 있는지를 심도 있게 조명했다.</p>
<h3>3.1  최우수 논문상(Outstanding Paper Awards) 심층 분석</h3>
<p>ICML 2020에서는 단 0.04%의 채택률을 기록한 두 편의 논문이 최우수 논문으로 선정되어, 당시 연구의 정점을 보여주었다.8 이 두 연구는 각각 계산 이미징과 표현 학습이라는 다른 영역에 속해 있었지만, 기존의 난제를 정교한 수학적, 방법론적 융합을 통해 해결했다는 공통점을 가졌다.</p>
<p><strong>1. “Tuning-free Plug-and-Play Proximal Algorithm for Inverse Imaging Problems”</strong></p>
<p>이 연구는 의료 영상 복원(예: MRI)이나 위상 복원(phase retrieval)과 같은 ’역 이미징 문제(inverse imaging problems)’를 해결하는 데 있어 오랜 골칫거리였던 파라미터 튜닝 문제를 해결했다.8</p>
<ul>
<li>
<p><strong>핵심 문제:</strong> PnP(Plug-and-Play) 프레임워크는 ADMM과 같은 전통적인 최적화 알고리즘에 딥러닝 기반의 강력한 노이즈 제거기(denoiser)를 결합하여 뛰어난 성능을 보이는 기법이다.9 하지만 이 프레임워크의 성능은 페널티 파라미터(<span class="math math-inline">\mu</span>), 노이즈 제거 강도(<span class="math math-inline">\sigma</span>), 그리고 종료 시점(<span class="math math-inline">\tau</span>)과 같은 내부 하이퍼파라미터에 극도로 민감하다. 최적의 파라미터 값은 이미지마다, 노이즈 수준마다 다르기 때문에, 이를 수동으로 조정하는 것은 매우 비효율적이고 어려운 작업이었다.8</p>
</li>
<li>
<p><strong>제안된 해결책:</strong> 연구팀은 이 파라미터 선택 문제를 순차적 의사결정 문제로 재정의하고, 이를 해결하기 위해 강화학습(Reinforcement Learning, RL)을 도입했다.8 즉, 최적화 과정의 각 단계에서 현재 상태(state)를 입력받아 최적의 파라미터(action)를 출력하는 ’정책 네트워크(policy network)’를 학습시킨 것이다. 이 정책 네트워크는 모델 기반(model-based) 및 모델 프리(model-free) 강화학습을 혼합한 효율적인 알고리즘을 통해 학습되어, 각기 다른 상태에 맞는 파라미터를 동적으로 자동 결정할 수 있게 했다.8</p>
</li>
<li>
<p><strong>주요 의의:</strong> 이 연구는 최적화, 딥러닝, 강화학습이라는 서로 다른 머신러닝 분야를 창의적으로 융합하여 계산 이미징 분야의 실질적이고 오래된 문제를 해결한 대표적인 사례다. 이는 머신러닝 연구자들이 수동으로 수행하던 직관적인 튜닝 작업을 자동화하는 ’연구자 자동화(Automating the Researcher)’라는 더 큰 흐름을 보여준다.</p>
</li>
</ul>
<p><strong>2. “On Learning Sets of Symmetric Elements”</strong></p>
<p>이 논문은 비정렬 집합(unordered sets) 데이터로부터 학습하는 근본적인 문제를 다루되, 집합의 각 원소가 이미지나 그래프처럼 자체적인 대칭성(symmetry)을 가질 때 이를 어떻게 효과적으로 처리할지에 대한 원칙적인 접근법을 제시했다.11</p>
<ul>
<li>
<p><strong>핵심 문제:</strong> 기존의 집합 학습 모델(예: Deep Sets)은 원소의 순서에 무관한 순열 불변성(permutation invariance)에 초점을 맞추었다. 그러나 이미지 집합의 경우, 각 이미지는 이동(translation)에 대해 등변성(equivariance)을 갖는 등 고유한 대칭 구조를 지닌다. 일반적인 접근법인 ’샴 네트워크(Siamese network)’는 각 원소를 독립적으로 처리한 후 마지막에 정보를 합치기 때문에, 원소들 간의 저수준 상호작용 정보를 초기에 잃어버릴 수 있다는 한계가 있었다.13</p>
</li>
<li>
<p><strong>제안된 해결책:</strong> 연구팀은 ‘대칭적 원소를 위한 딥셋(Deep Sets for Symmetric elements, DSS)’ 레이어라는 새로운 구조를 제안했다. DSS 레이어는 수학적으로 (1) 원소 순서 재배열에 대한 등변성(순열 등변성)과 (2) 원소 자체의 고유 대칭성(예: 이미지의 이동 등변성)을 <strong>동시에</strong> 만족하는 선형 레이어의 공간을 완벽하게 특정한다.12 이 구조의 핵심은 정보가 마지막이 아닌</p>
</li>
</ul>
<p><strong>모든 레이어에서</strong> 원소들 간에 공유되어야 한다는 점이다. 예를 들어, 이미지 집합에 대한 DSS 레이어는 각 이미지에 독립적으로 적용되는 컨볼루션(<span class="math math-inline">L_1</span>) 부분과 모든 이미지의 합에 적용되는 컨볼루션(<span class="math math-inline">L_2</span>) 부분의 합으로 구성된다.</p>
<ul>
<li><strong>주요 의의:</strong> 이 연구는 대칭성을 가진 원소로 구성된 집합 학습을 위한 통일되고 원칙적인 프레임워크를 제공했다. 또한, DSS 네트워크가 샴 네트워크보다 표현력이 수학적으로 더 높으며, 불변 및 등변 함수에 대한 보편 근사 정리(universal approximation theorems)를 만족함을 증명하여 이론적 토대를 공고히 했다.14 이는 알려진 도메인 지식(대칭성)을 아키텍처에 명시적으로 통합하는 것이 얼마나 강력한지를 보여주는 사례다.</li>
</ul>
<table><thead><tr><th>논문 (Paper)</th><th>핵심 문제 (Core Problem)</th><th>제안된 해결책 (Proposed Solution)</th><th>주요 의의 (Significance)</th></tr></thead><tbody>
<tr><td>Tuning-free PnP…</td><td>역 이미징 문제 해결 시 PnP 알고리즘의 수동 파라미터 튜닝</td><td>강화학습 기반 정책 네트워크를 통해 파라미터를 동적으로 자동 결정</td><td>최적화, 딥러닝, 강화학습을 융합하여 연구자의 수동 튜닝 작업을 자동화</td></tr>
<tr><td>On Learning Sets of Symmetric Elements</td><td>고유 대칭성을 가진 원소들로 구성된 비정렬 집합 학습</td><td>순열 및 원소 대칭성에 대해 동시 등변인 DSS(Deep Sets for Symmetric elements) 레이어 제안</td><td>집합 학습을 위한 통일된 이론적 프레임워크를 제공하고, 샴 네트워크보다 표현력이 높음을 증명</td></tr>
<tr><td>Generative Pretraining from Pixels (iGPT)</td><td>비지도 방식을 통한 강력한 시각 표현 학습</td><td>이미지를 1D 픽셀 시퀀스로 취급하고, 대규모 트랜스포머로 자기회귀적 생성 사전 학습 수행</td><td>2D 구조에 대한 사전 지식 없이도 강력한 시각 표현 학습이 가능함을 입증, ViT의 선구자 역할</td></tr>
</tbody></table>
<h3>3.2  주목할 만한 연구 및 동향</h3>
<p>최우수 논문상 외에도 ICML 2020에서는 AI 연구의 미래 방향을 제시하는 중요한 연구들이 다수 발표되었다.</p>
<p><strong>1. “Generative Pretraining from Pixels (iGPT)”</strong></p>
<p>최우수 논문 가작(Honorable Mention)으로 선정된 이 연구는 자연어 처리 분야를 지배하던 트랜스포머 아키텍처를 컴퓨터 비전 분야에 성공적으로 이식한 기념비적인 논문이다.11</p>
<ul>
<li><strong>핵심 아이디어:</strong> 이 연구의 접근 방식은 급진적일 정도로 단순했다. 이미지의 2차원 공간 구조에 대한 사전 지식(예: 컨볼루션)을 전혀 사용하지 않고, 이미지를 단순히 픽셀의 1차원 시퀀스로 취급했다. 그리고 GPT-2 규모의 거대한 트랜스포머 모델을 사용하여, 마치 문장에서 다음 단어를 예측하듯이, 이미지에서 다음 픽셀을 예측하도록 자기회귀적(auto-regressive)으로 훈련시켰다.17 연구팀은 자기회귀적 예측(</li>
</ul>
<p><span class="math math-inline">L_{AR}</span>)과 BERT 스타일의 마스크된 픽셀 예측(<span class="math math-inline">L_{BERT}</span>) 두 가지 목표를 모두 실험했다.18</p>
<ul>
<li><strong>결과와 영향:</strong> 놀랍게도, 레이블 없는 저해상도 이미지넷 데이터로 사전 학습한 이 모델은 다운스트림 과업에서 강력한 성능을 보였다. 특히 CIFAR-10 분류 문제에서 선형 평가(linear probe)만으로 96.3%의 정확도를 달성하여, 당시 지도 학습으로 훈련된 강력한 CNN 모델인 Wide ResNet을 능가했다.18 이 결과는 대규모의 일반적인 아키텍처(트랜스포머)가 충분한 데이터와 컴퓨팅을 통해 도메인 특화된 구조(CNN) 없이도 해당 도메인의 구조를 스스로 학습할 수 있음을 시사했다. 이 연구는 이후 Vision Transformer(ViT)의 등장에 직접적인 영감을 주며, 컴퓨터 비전 분야에서 트랜스포머 아키텍처의 시대를 여는 데 결정적인 역할을 했다.</li>
</ul>
<p><strong>2. 기타 주요 동향</strong></p>
<ul>
<li>
<p><strong>다국어 일반화 (Cross-Lingual Generalization):</strong> 40개 언어와 9개 과업에 걸쳐 다국어 모델의 일반화 성능을 평가하는 <strong>XTREME 벤치마크</strong>의 등장은 진정으로 전 세계적으로 통용될 수 있는 AI 시스템을 구축하려는 연구 커뮤니티의 노력을 반영했다.17</p>
</li>
<li>
<p><strong>물리 및 시뮬레이션 (Physics and Simulation):</strong> **“Learning to Simulate Complex Physics with Graph Networks”**와 같은 연구는 그래프 신경망(GNN)을 사용하여 복잡한 물리 시스템을 학습하고 시뮬레이션하는 가능성을 보여주었다. 이는 AI가 디지털 세계를 넘어 물리적 세계의 법칙을 이해하고 예측하는 방향으로 나아가고 있음을 나타내는 중요한 흐름이었다.17</p>
</li>
<li>
<p><strong>Test of Time Award:</strong> <strong>“Gaussian Process Optimization in the Bandit Setting”</strong> 논문에 수여된 ’시간의 시험상’은 베이즈 최적화(Bayesian Optimization)가 GPT-3와 같은 거대 모델의 하이퍼파라미터 튜닝에 미친 장기적인 영향을 인정하는 것이었다.11 이는 복잡한 AI 시스템을 효율적으로 훈련시키는 기반 기술의 중요성을 다시 한번 확인시켜 주었다.</p>
</li>
</ul>
<p>ICML 2020은 AI 연구의 두 가지 상반되면서도 상호 보완적인 철학이 공존하며 발전하고 있음을 명확히 보여주었다. iGPT의 성공은 충분한 규모의 범용 아키텍처가 데이터로부터 모든 것을 학습할 수 있다는 ’규모의 길’을 제시했다. 반면, DSS 논문의 성공은 도메인 지식을 수학적으로 정교하게 아키텍처에 통합하는 ’특화의 길’이 여전히 강력하고 효율적임을 입증했다. 이 두 가지 접근 방식의 공존은 이후 AI 연구의 다양성과 풍부함을 이끄는 원동력이 되었다.</p>
<h2>4.  로보틱스 연구의 진일보: 학계와 산업의 연결</h2>
<p>2020년 7월은 로봇 공학 분야에서도 학술적 성취와 산업적 이정표가 동시에 나타난 중요한 시기였다. 한편에서는 IEEE Robotics and Automation Letters (RA-L)와 같은 최고 수준의 학술지에서 로봇 인식 및 조작의 한계를 넓히는 전문화된 연구들이 발표되었고, 다른 한편에서는 수십 년간 연구의 상징이었던 보스턴 다이내믹스의 ’스팟’이 상용 제품으로 출시되어 실제 산업 현장에 투입되기 시작했다. 이 두 흐름은 연구가 현실로 이어지는 파이프라인이 본격적으로 가동되고 있음을 보여주었다.</p>
<h3>4.1  학계의 최전선: IEEE Robotics and Automation Letters</h3>
<p>IEEE RA-L 2020년 7월호에 게재되고 수상한 논문들은 로봇이 비정형 환경에서 마주하는 근본적인 난제들을 해결하기 위한 창의적인 접근법들을 제시했다.20</p>
<ul>
<li>
<p><strong>“LIT: Light-Field Inference of Transparency for Refractive Object Localization”:</strong> 이 연구는 로봇 인식 분야의 오랜 난제인 ‘투명한 물체’ 인식을 다루었다. 유리잔이나 플라스틱 용기 같은 투명 물체는 빛을 굴절시키고 왜곡시켜 기존의 컴퓨터 비전 알고리즘을 무력화시킨다. 이 논문은 빛의 강도뿐만 아니라 방향 정보까지 포착하는 ‘라이트 필드(light-field)’ 센싱 기술을 활용하여, 빛의 왜곡 패턴 자체를 분석함으로써 투명 물체의 위치와 자세를 정확하게 추정하는 새로운 방법론(LIT)을 제안했다. 이 연구는 로봇이 인간의 일상 환경에서 흔히 접하는 물체들을 더 잘 다룰 수 있게 하는 핵심 기술을 제시했다는 점에서 큰 의미를 가진다.21</p>
</li>
<li>
<p><strong>“EGAD! An Evolved Grasping Analysis Dataset for Diversity and Reproducibility in Robotic Manipulation”:</strong> 로봇 파지(grasping) 알고리즘의 성능을 객관적으로 평가하는 것은 매우 어려운 문제다. 기존 데이터셋은 물체의 종류가 제한적이거나, 특정 형태에 편중되어 있었다. 이 연구는 진화 알고리즘(evolutionary algorithms)을 사용하여 기하학적 복잡도와 파지 난이도가 매우 다양한 2,000개 이상의 3D 물체 데이터셋(EGAD)을 자동으로 생성했다. 이를 통해 연구자들은 자신들의 파지 알고리즘을 훨씬 더 다양하고 도전적인 환경에서 훈련하고 공정하게 벤치마킹할 수 있게 되었다. 이는 로봇 조작 연구의 재현성과 신뢰도를 높이는 데 크게 기여했다.23</p>
</li>
<li>
<p><strong>“Inverted and Inclined Climbing Using Capillary Adhesion in a Quadrupedal Insect-Scale Robot”:</strong> 이 연구는 곤충과 같은 작은 생명체에서 영감을 받아, 1.4g에 불과한 초소형 4족 보행 로봇이 천장과 같은 거꾸로 된 표면을 기어오를 수 있는 새로운 부착 메커니즘을 개발했다. 연구팀은 물의 ’모세관 현상(capillary adhesion)’을 이용하여 접착과 미끄러짐이 동시에 가능한 47mg의 초경량 접착 패드를 설계했다. 이는 제한된 센서와 제어 능력만을 가진 소형 로봇에게 적합한 수동적이고 안정적인 부착 방식으로, 재난 현장 탐사나 시설 점검 등에서 활용될 수 있는 새로운 형태의 로봇 개발 가능성을 열었다.25</p>
</li>
</ul>
<h3>4.2  산업 현장으로의 도약: 보스턴 다이내믹스 ‘스팟’</h3>
<p>학계의 전문화된 연구와 동시에, 2020년 6월과 7월은 보스턴 다이내믹스가 ’스팟’의 일반 상용 판매를 시작하며 첨단 로봇 기술이 연구실을 나와 산업 현장으로 진입하는 역사적인 순간을 기록했다.26 특히 포드 자동차 공장에서의 파일럿 프로그램은 스팟의 실질적인 가치를 명확하게 보여주는 대표적인 사례가 되었다.</p>
<p><strong>1. 상용화 선언</strong></p>
<p>보스턴 다이내믹스는 소수의 파트너에게만 제공되던 ’얼리 어답터 프로그램’을 종료하고, 미국 내 기업들을 대상으로 스팟을 온라인으로 판매하기 시작했다.26 이는 다이나믹한 보행 능력을 갖춘 4족 로봇이 연구용 프로토타입을 넘어, 실제 산업 환경에서 가치를 창출할 수 있는 성숙한 제품이 되었음을 공식적으로 선언한 것이었다.</p>
<p><strong>2. 포드 자동차 도입 사례 심층 분석</strong></p>
<p>포드 자동차는 미시간에 위치한 밴 다이크 변속기 공장의 효율성 증대를 위해 스팟 로봇 2대(“플러피“와 “스팟“이라는 별칭)를 임대하여 파일럿 프로그램을 시작했다.28</p>
<ul>
<li>
<p><strong>도입 목적:</strong> 주된 목적은 공장 설비를 재배치(retooling)하기 전에 최신 상태의 3D 컴퓨터 지원 설계(CAD) 모델을 확보하는 것이었다. 공장은 수년에 걸쳐 크고 작은 변경이 이루어지지만, 이러한 내용이 설계 도면에 즉시 반영되지 않는 경우가 많다. 스팟은 레이저 스캐너와 고해상도 카메라를 탑재하고 공장 내부를 자율적으로 이동하며 정밀한 3D 데이터를 수집하는 임무를 맡았다.30</p>
</li>
<li>
<p><strong>활용 방식:</strong> 기존의 수동 방식은 작업자가 삼각대를 들고 공장 곳곳을 이동하며 한 지점에서 5분씩 기다려 스캔하는 방식으로, 전체 공장을 스캔하는 데 2주가 소요되었다.30 반면, 스팟은 360도 인식 능력과 뛰어난 기동성을 바탕으로 계단, 철망 바닥, 30도 경사면 등 인간 작업자나 기존의 바퀴형 로봇(“스카우터(Scouter)“로 불림)이 접근하기 어려운 좁고 복잡한 공간까지 자율적으로 이동하며 데이터를 수집했다.32 스팟의 4족 보행 능력은 바퀴형 로봇이 갈 수 없는 비정형적인 공장 환경을 탐색하는 데 결정적인 역할을 했다.</p>
</li>
<li>
<p><strong>기대 효과:</strong> 포드는 스팟 도입을 통해 상당한 시간 및 비용 절감 효과를 기대했다. 기존에 2주가 걸리던 스캔 작업을 절반인 1주 만에 완료할 수 있었으며, 한 시설당 거의 30만 달러에 달했던 비용을 “일부(a fraction)” 수준으로 대폭 절감할 수 있을 것으로 전망했다.29 이는 첨단 로봇 도입에 대한 명확한 투자 수익률(ROI)을 제시한 사례로, 다른 제조업체들의 로봇 도입을 가속화하는 계기가 되었다.</p>
</li>
</ul>
<table><thead><tr><th>항목 (Item)</th><th>기존 방식 (Manual Process)</th><th>‘스팟’ 도입 방식 (Spot-Enabled Process)</th><th>기대 효과 (Expected Outcome)</th></tr></thead><tbody>
<tr><td><strong>과업 (Task)</strong></td><td>공장 3D 레이저 스캐닝</td><td>공장 3D 레이저 스캐닝</td><td>고정밀 최신 3D CAD 모델 확보</td></tr>
<tr><td><strong>소요 시간 (Time Required)</strong></td><td>2주 (2 weeks)</td><td>1주 (1 week)</td><td>50% 시간 단축</td></tr>
<tr><td><strong>비용 (Cost)</strong></td><td>약 30만 달러 (Approx. $300,000)</td><td>“일부 비용 (Fraction of the cost)”</td><td>대폭적인 비용 절감</td></tr>
<tr><td><strong>접근성 (Accessibility)</strong></td><td>인간 작업자의 접근 가능한 영역으로 제한</td><td>계단, 좁은 공간, 위험 구역 접근 가능</td><td>데이터 수집 범위의 획기적 확장</td></tr>
<tr><td><strong>핵심 구현 기술 (Key Enabling Tech.)</strong></td><td>삼각대 및 고정형 레이저 스캐너</td><td>자율 4족 보행 및 동적 장애물 회피</td><td>비정형 산업 환경에서의 완전 자율화</td></tr>
</tbody></table>
<p>포드의 사례는 4족 보행 로봇의 핵심 가치가 단순히 신기한 움직임이 아니라, 바퀴형 로봇이나 인간이 접근하기 어려운 ’비정형 환경’을 극복하는 능력에 있음을 명확히 보여주었다. 이는 수십 년간 이어진 동적 보행 로봇 연구가 마침내 산업적 가치를 창출하는 단계에 이르렀음을 의미한다. 더 나아가, 스팟과 같은 상용 플랫폼의 등장은 학계의 첨단 인식 및 조작 알고리즘(예: IEEE RA-L에서 발표된 연구들)이 탑재되어 실제 환경에서 테스트되고 상용화될 수 있는 ’몸체(body)’를 제공함으로써, 학계와 산업계가 서로의 발전을 가속하는 선순환 구조의 시작을 알렸다.</p>
<h2>5.  결론: 2020년 7월이 남긴 유산과 미래 전망</h2>
<p>2020년 7월은 AI 및 로봇 공학 분야에서 여러 중요한 흐름이 교차하고 증폭되며 미래의 기술 지형을 예고한 결정적인 시기였다. 이 시기에 나타난 세 가지 핵심 축—거대 모델의 ‘규모’, 학술 연구의 ‘깊이’, 그리고 로봇 공학의 ‘구현’—은 개별적인 성과를 넘어, 서로에게 영향을 미치며 AI 발전의 새로운 방향성을 제시했다.</p>
<h3>5.1  3대 축의 융합과 시너지</h3>
<p>GPT-3를 통해 입증된 ’규모의 법칙’은 더 크고 방대한 모델이 새로운 능력을 창발시킨다는 사실을 명확히 했다. 그러나 이러한 거대 모델을 효율적으로 훈련하고 최적화하기 위해서는 ICML 2020에서 발표된 것과 같은 정교한 알고리즘과 깊이 있는 이론적 이해가 필수적이다. 예를 들어, 베이즈 최적화와 같은 기술은 거대 모델의 하이퍼파라미터 탐색 공간을 효율적으로 탐색하는 데 결정적인 역할을 한다.</p>
<p>동시에, 보스턴 다이내믹스 ’스팟’의 상용화는 AI가 디지털 공간을 넘어 물리적 세계와 상호작용하는 ’구현된 지능(Embodied AI)’의 중요성을 부각시켰다. 이러한 물리적 로봇이 복잡한 실제 환경을 이해하고 상호작용하기 위해서는 iGPT와 같은 연구에서 시작된 강력한 시각-언어 이해 능력이 필수적으로 요구될 것이다. 결국, GPT-3가 보여준 방대한 ’세계 지식’과 스팟이 보여준 ’물리적 상호작용 능력’의 결합은 미래 AI 에이전트가 나아갈 방향을 암시한다.</p>
<h3>5.2  미래 연구 방향에 대한 제언</h3>
<p>2020년 7월의 사건들은 이후 수년간의 AI 및 로봇 공학 연구 아젠다를 설정하는 데 지대한 영향을 미쳤다.</p>
<ul>
<li>
<p><strong>규모의 길 (The Path of Scale):</strong> GPT-3의 성공은 ’더 큰 것이 더 낫다’는 패러다임을 확립하며, 파운데이션 모델(Foundation Model) 시대를 열었다. 이후 수년간 연구계와 산업계는 컴퓨팅 자원을 총동원하여 수천억, 수조 개의 파라미터를 가진 모델을 개발하는 데 집중하게 되었다.</p>
</li>
<li>
<p><strong>효율성과 이론의 길 (The Path of Efficiency and Theory):</strong> ICML의 연구들은 모든 문제가 무한한 데이터와 컴퓨팅으로 해결될 수 없음을 상기시켰다. 데이터가 희소하거나, 안전성이 중요하거나, 해석 가능성이 요구되는 과학 및 의료 분야에서는 DSS 논문처럼 도메인 지식(예: 대칭성)을 아키텍처에 통합하여 데이터 효율성과 신뢰성을 높이는 연구가 계속해서 중요하게 다뤄질 것이다.</p>
</li>
<li>
<p><strong>구현의 길 (The Path of Embodiment):</strong> 스팟의 상업적 성공은 고도로 동적인 로봇이 산업 현장에서 실질적인 가치를 창출할 수 있음을 증명했다. 이는 비정형 환경에서의 강인한 이동, 정교한 조작, 그리고 인간과의 안전한 상호작용에 대한 연구를 더욱 촉진시키는 계기가 되었다.</p>
</li>
</ul>
<h3>5.3  최종 요약</h3>
<p>결론적으로, 2020년 7월은 AI 혁명이 단일한 기술적 돌파구가 아닌, 다각적이고 복합적인 양상으로 가속화되고 있음을 명확하게 보여준 한 달이었다. 거대 언어 모델이 지능의 ’규모’를 재정의했고, 학계는 그 기반을 다지는 ’깊이’를 더했으며, 로봇 공학은 이를 현실 세계로 이끌어내는 ’구현’의 가능성을 입증했다. 이 세 가지 축이 서로 맞물려 돌아가기 시작한 이 시점은, 이후 우리가 목격하게 될 생성 AI의 폭발적 성장과 물리적 AI 에이전트의 부상을 예고하는 진정한 서막이었다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>ETRI, ‘2020년 AI 7대 트렌드’ 발표 - 로봇신문, https://www.irobotnews.com/news/articleView.html?idxno=19383</li>
<li>2020년 AI 7대 트렌드는… AI 시대는 미vs중 양극체제 - 한국무역협회, https://kita.net/cmmrcInfo/cmmrcNews/cmmrcNews/cmmrcNewsDetail.do?pageIndex=1&amp;nIndex=56442&amp;sSiteid=1</li>
<li>OpenAI Presents GPT-3, a 175 Billion Parameters Language Model …, https://developer.nvidia.com/blog/openai-presents-gpt-3-a-175-billion-parameters-language-model/</li>
<li>GPT-3 A Hitchhiker’s Guide - Lambda, https://lambda.ai/blog/gpt-3</li>
<li>What Are Foundation Models? - NVIDIA Blog, https://blogs.nvidia.com/blog/what-are-foundation-models/</li>
<li>OpenAI Opens GPT-3 for Everyone | Towards Data Science, https://towardsdatascience.com/openai-opens-gpt-3-for-everyone-fb7fed309f6/</li>
<li>The GPT-3 economy - TechTalks, https://bdtechtalks.com/2020/09/21/gpt-3-economy-business-model/</li>
<li>ICML 2020 Outstanding Paper Award | Cambridge Image Analysis - DAMTP, http://www.damtp.cam.ac.uk/research/cia/icml-2020-outstanding-paper-award</li>
<li>Tuning-free Plug-and-Play Proximal Algorithm for Inverse … - arXiv, https://arxiv.org/pdf/2002.09611</li>
<li>Stochastic Deep Restoration Priors for Imaging Inverse Problems - ICML 2025, https://icml.cc/virtual/2025/poster/45624</li>
<li>ICML 2020 Awards, https://icml.cc/virtual/2020/awards_detail</li>
<li>On Learning Sets of Symmetric Elements, http://proceedings.mlr.press/v119/maron20a/maron20a.pdf</li>
<li>On Learning Sets of Symmetric Elements (Extended Abstract) - IJCAI, https://www.ijcai.org/proceedings/2021/0653.pdf</li>
<li>On Learning Sets of Symmetric Elements - arXiv, https://arxiv.org/pdf/2002.08599</li>
<li>[2002.08599] On Learning Sets of Symmetric Elements - arXiv, https://arxiv.org/abs/2002.08599</li>
<li>Awards: Test of Time and Best Papers - ICML 2025, https://icml.cc/Conferences/2020/Awards</li>
<li>Highlights from ICML 2020 | Lewis Tunstall’s Blog, https://lewtun.github.io/blog/research/conference/2020/07/31/icml2020.html</li>
<li>Generative Pretraining from Pixels - OpenAI, https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf</li>
<li>Generative Pretraining From Pixels - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v119/chen20s.html</li>
<li>2021 IEEE RAS Publications best paper awards - Robohub, https://robohub.org/2021-ieee-ras-publications-best-paper-awards/</li>
<li>LIT: Light-field Inference of Transparency for Refractive Object Localization - Google Sites, https://sites.google.com/umich.edu/prolit</li>
<li>Best paper award for a robot that can see and move transparent objects, https://cse.engin.umich.edu/stories/best-paper-award-for-a-robot-that-can-see-and-move-transparent-objects</li>
<li>EGAD! An Evolved Grasping Analysis Dataset for diversity and reproducibility in robotic manipulation - Monash University, https://research.monash.edu/en/publications/egad-an-evolved-grasping-analysis-dataset-for-diversity-and-repro</li>
<li>(PDF) EGAD! an Evolved Grasping Analysis Dataset for diversity and reproducibility in robotic manipulation - ResearchGate, https://www.researchgate.net/publication/339675264_EGAD_an_Evolved_Grasping_Analysis_Dataset_for_diversity_and_reproducibility_in_robotic_manipulation</li>
<li>Inverted and Inclined Climbing Using Capillary Adhesion in a Quadrupedal Insect-Scale Robot | Request PDF - ResearchGate, https://www.researchgate.net/publication/342326227_Inverted_and_Inclined_Climbing_Using_Capillary_Adhesion_in_a_Quadrupedal_Insect-Scale_Robot</li>
<li>Boston Dynamics Launches Commercial Sales of Spot® Robot, https://bostondynamics.com/news/boston-dynamics-launches-commercial-sales-of-spot-robot/</li>
<li>Boston Dynamics - The Robot Report Robotics and Intelligent Systems Search, https://search.therobotreport.com/listings/boston-dynamics/</li>
<li>Ford is using Boston Dynamics robots to survey its plant in Michigan - TNW, https://thenextweb.com/news/ford-is-using-boston-dynamics-robots-to-survey-its-plant-in-michigan</li>
<li>Boston Dynamics Spot robots helping Ford survey Michigan plant, https://www.therobotreport.com/boston-dynamics-spot-robots-ford-michigan-plant/</li>
<li>Ford experimenting with four-legged robots to laser-scan plant …, https://www.greencarcongress.com/2020/07/20200728-ford.html</li>
<li>Meet Fluffy and Spot: Ford’s New Four-Legged Robots | 2020-08-12 - Assembly Magazine, https://www.assemblymag.com/articles/95827-meet-fluffy-and-spot-fords-new-four-legged-robots</li>
<li>No Bones About It: Ford Experiments with Four-Legged Robots, to Scout Factories, Saving Time, Money, https://media.ford.com/content/fordmedia/feu/en/news/2020/07/27/no-bones-about-it-ford-experiments-with-four-legged-robots.html</li>
<li>Ford Experimenting with 4-legged Boston Dynamics robots to 3D Scout factories, https://highways.today/2020/07/28/ford-robots-3d-scout-factories/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>