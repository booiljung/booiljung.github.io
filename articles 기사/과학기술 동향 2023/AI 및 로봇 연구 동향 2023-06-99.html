<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2023년 6월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2023년 6월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2023년 AI 및 로봇 연구 동향</a> / <span>2023년 6월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2023년 6월 AI 및 로봇 연구 동향</h1>
<h2>1. 서론: 2023년 6월, AI 연구의 변곡점</h2>
<p>2023년 6월은 인공지능(AI) 및 로보틱스 연구 분야에서 중요한 변곡점으로 기록될 만한 시점이다. 이 시기는 컴퓨터 비전 분야의 최고 권위 학회인 IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) 2023이 성황리에 개최되고, 로보틱스 분야의 양대 산맥 중 하나인 IEEE International Conference on Robotics and Automation (ICRA) 2023이 마무리되는 시점과 맞물려, 한 해의 기술적 성취와 향후 연구 방향을 가늠할 수 있는 풍성한 논의의 장을 제공했다.1 특히 CVPR 2023은 9,155편이라는 기록적인 수의 논문이 제출되어 작년 대비 12%의 증가율을 보이는 등, AI 연구 커뮤니티의 폭발적인 성장세와 연구의 심도를 명확히 보여주었다.4</p>
<p>본 보고서는 2023년 6월을 기점으로 발표된 주요 연구들을 <strong>통합(Integration)</strong>, <strong>구성성(Compositionality)</strong>, <strong>개인화(Personalization)</strong>, **지능형 제어(Intelligent Control)**라는 네 가지 핵심 키워드를 통해 심층적으로 분석한다. 한편에서는 대규모 언어 모델(Large Language Models, LLM)의 강력한 추론 능력을 활용하여 기존의 전문화된 AI 모델들을 마치 프로그래밍하듯 조합하는 새로운 ‘구성적’ 패러다임이 부상했으며, 다른 한편에서는 자율주행과 같이 극도로 복잡한 시스템을 단일 통합 네트워크로 설계하려는 ‘통합적’ 접근법이 학문적 성취의 정점에 올랐다. 이는 AI 시스템을 설계하는 근본적인 철학에 있어 중요한 분기점이 도래했음을 시사한다.</p>
<p>이러한 학문적 성취의 배경에는 AI 연구 생태계의 구조적인 변화가 자리 잡고 있다. CVPR 2023에 제출된 논문 수의 기록적인 증가는 단순히 학문적 관심의 양적 팽창을 넘어선다.4 이는 강력한 기반 모델(Foundation Models)이 널리 보급되면서 연구의 진입 장벽이 낮아지고, 소수의 연구자나 기관도 기존의 거대 모델을 활용하여 고차원적인 연구를 수행할 수 있게 되었음을 의미한다. 동시에, SenseTime과 같은 산업계 연구소들이 학계의 최전선에서 경쟁하며 최고의 학문적 성과를 내는 현상 7은 산업계의 막대한 자본과 데이터가 연구 개발의 속도를 기하급수적으로 끌어올리는 ‘가속화의 선순환’ 구조가 공고히 형성되었음을 보여준다. 즉, 기반 모델의 발전이 연구 생산성을 향상시키고, 이는 더 많은 연구 인력의 유입과 산업계의 투자를 유치하며, 다시 더 발전된 기반 모델의 개발로 이어지는 피드백 루프가 현재 AI 연구의 폭발적 성장을 주도하는 핵심 동력으로 작용하고 있다. 이러한 배경 속에서 2023년 6월의 연구들은 AI가 순수 학문의 영역을 넘어 하나의 거대한 ’산업’으로 변모하고 있으며, 그 발전 속도가 스스로를 가속하는 새로운 단계에 진입했음을 명확히 보여준다.</p>
<h2>2.  CVPR 2023 - 컴퓨터 비전의 새로운 지평</h2>
<h3>2.1  학회 개요 및 통계</h3>
<p>CVPR 2023은 2023년 6월 18일부터 22일까지 캐나다 밴쿠버 컨벤션 센터에서 개최되었다.1 컴퓨터 비전, 머신러닝, 인공지능 산업의 전 세계 전문가들이 모이는 최대 규모의 학술 행사 중 하나로, 올해 역시 전례 없는 규모와 열기 속에서 진행되었다.</p>
<p>이번 학회에는 총 9,155편의 방대한 논문이 제출되었으며, 엄격한 동료 심사 과정을 거쳐 최종적으로 2,359편의 논문이 채택되어 25.8%의 채택률을 기록했다.4 이는 해당 분야의 높은 학문적 기준과 치열한 경쟁을 여실히 보여주는 수치이다. 채택된 논문 중에서도 특히 뛰어난 연구로 인정받은 상위 10%에 해당하는 235편의 논문이 하이라이트(Highlight)로 선정되었으며, 그중에서도 단 12편의 논문(전체 제출 논문의 0.13%)만이 최우수 논문(Best Paper) 후보로 지명되어 학계의 큰 주목을 받았다.4 이처럼 소수의 논문에 주어지는 영예는 해당 연구들이 컴퓨터 비전 분야에 미치는 영향력과 독창성을 방증한다.</p>
<h3>2.2 [표 1] CVPR 2023 주요 수상 논문 요약</h3>
<p>다음 표는 CVPR 2023에서 주요 상을 수상한 논문들의 핵심 내용을 요약한 것이다. 이 논문들은 각각 컴퓨터 비전 분야의 중요한 문제들에 대해 혁신적인 접근법을 제시하며, 앞으로의 연구 방향에 큰 영향을 미칠 것으로 평가된다.</p>
<table><thead><tr><th>수상 부문</th><th>논문 제목</th><th>핵심 기여</th><th>저자 및 소속</th></tr></thead><tbody>
<tr><td><strong>Best Paper</strong></td><td>Visual Programming: Compositional visual reasoning without training</td><td>LLM을 이용해 시각적 추론 프로그램을 생성하는 신경-심볼릭 접근법</td><td>Tanmay Gupta, Aniruddha Kembhavi (Allen Institute for AI)</td></tr>
<tr><td><strong>Best Paper</strong></td><td>Planning-oriented Autonomous Driving</td><td>인식-예측-계획을 통합한 ‘계획 지향적’ 자율주행 프레임워크(UniAD)</td><td>Yihan Hu et al. (Shanghai AI Lab, SenseTime, Wuhan Univ.)</td></tr>
<tr><td><strong>Best Student Paper</strong></td><td>3D Registration with Maximal Cliques</td><td>극대 클릭(Maximal Clique)을 활용한 강인한 3D 포인트 클라우드 정합</td><td>Xiyu Zhang et al. (Northwestern Polytechnical University)</td></tr>
<tr><td><strong>Honorable Mention</strong></td><td>DynIBaR: Neural Dynamic Image-Based Rendering</td><td>동적 장면의 고품질 뷰 합성을 위한 신경망 이미지 기반 렌더링</td><td>Zhengqi Li et al. (Google Research, Cornell Univ.)</td></tr>
<tr><td><strong>Honorable Mention (Student)</strong></td><td>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</td><td>소수 이미지로 특정 대상을 학습시키는 개인화된 확산 모델 파인튜닝</td><td>Nataniel Ruiz et al. (Google Research, Boston Univ.)</td></tr>
</tbody></table>
<h3>2.3  최우수 논문 심층 분석 (1): 신경-심볼릭 접근법의 도약, ‘Visual Programming’</h3>
<p>CVPR 2023 최우수 논문 중 하나로 선정된 ’Visual Programming: Compositional visual reasoning without training’은 AI 시스템 설계에 대한 기존의 통념을 깨는 혁신적인 패러다임을 제시했다.11 이 연구의 핵심은 특정 시각적 과업을 해결하기 위해 거대한 단일 모델을 훈련시키는 대신, 대규모 언어 모델(LLM)을 ’프로그래머’로 활용하여 문제 해결을 위한 일련의 절차, 즉 ’시각 프로그램(visual program)’을 동적으로 생성하는 데 있다.</p>
<p>핵심 개념 및 방법론</p>
<p>VisProg이라 명명된 이 신경-심볼릭(neuro-symbolic) 시스템은 자연어로 된 지시(예: “이미지에서 빅토리어스에 출연한 여배우를 찾고, 그녀를 흑백으로 만드세요”)를 입력받는다.14 그러면 GPT-3와 같은 LLM이 이 지시를 해석하여, 문제를 해결하기 위한 단계별 파이썬 유사 코드를 생성한다. 이 코드의 각 라인은 사전에 정의된 다양한 ’모듈’을 호출하는 명령어에 해당한다. 이 모듈들은 특정 기능을 수행하는 전문가 시스템으로, 객체 탐지를 위한 OWL-ViT, 이미지 분할을 위한 MaskFormer, 시각 질의응답을 위한 ViLT와 같은 최신 딥러닝 모델부터, OpenCV 라이브러리에 내장된 이미지 처리 함수, 혹은 간단한 산술/논리 연산자까지 광범위하게 포함된다.15</p>
<p>이 접근법의 가장 큰 기술적 의의는 특정 과업을 위한 별도의 훈련 과정이나 데이터셋 구축이 전혀 필요 없다는 점이다. 시스템은 LLM이 소수의 예시(instruction-program 쌍)를 보고 새로운 문제에 대한 프로그램을 생성해내는 ‘인-컨텍스트 학습(in-context learning)’ 능력에 전적으로 의존한다.14 예를 들어, LLM에게 몇 가지 질문과 그에 해당하는 시각 프로그램 예제를 프롬프트로 제공하면, LLM은 그 패턴을 학습하여 완전히 새로운 질문에 대해서도 적절한 프로그램을 생성해낼 수 있다. 또한, 생성된 프로그램 코드 자체가 문제 해결의 전 과정을 명확하게 보여주기 때문에, 시스템의 결정 과정을 사용자가 쉽게 이해하고 검증할 수 있다. 이는 AI의 ‘블랙박스’ 문제를 해결하는 설명가능 AI(XAI) 측면에서 매우 중요한 장점을 제공한다.15 VisProg은 이러한 유연성을 바탕으로 구성적 시각 질의응답(compositional VQA), 제로샷 이미지 쌍 추론, 사실 지식 기반 객체 태깅, 언어 기반 이미지 편집 등 서로 다른 성격의 4가지 과업에서 그 성능을 성공적으로 입증했다.14</p>
<p>패러다임 전환의 신호탄</p>
<p>’Visual Programming’의 등장은 AI 시스템 설계 철학의 근본적인 패러다임 전환을 예고한다. 지금까지의 주류였던 ‘엔드-투-엔드(end-to-end)’ 방식은 특정 과업을 위해 입력부터 출력까지 한 번에 처리하는 거대한 단일 모델(monolithic model)을 구축하는 데 집중했다. 이는 특정 분야에서 높은 성능을 보였지만, 새로운 과업에 적용하기 위해서는 막대한 데이터와 컴퓨팅 자원을 투입하여 모델을 재훈련하거나 파인튜닝해야 하는 확장성의 한계를 지니고 있었다.</p>
<p>반면, ’Visual Programming’이 제시하는 ‘구성적(compositional)’ 방식은 이러한 한계를 극복할 새로운 가능성을 보여준다. 이 패러다임에서 LLM은 시스템의 ‘두뇌’ 또는 ’범용 추론 엔진’의 역할을 수행하며, 다양한 ’전문가 모듈’들을 필요에 따라 동적으로 조립하고 지휘한다. 이는 마치 숙련된 프로그래머가 여러 라이브러리를 <span class="math math-inline">import</span>하여 복잡한 소프트웨어를 개발하는 과정과 유사하다. 이러한 접근법이 성공적으로 확장될 경우, AI 개발의 무게중심은 ’개별 모델의 훈련’에서 ’고품질 모듈 라이브러리 구축’과 LLM이 이해할 수 있도록 과업을 정교하게 기술하는 ’고수준 프롬프트 엔지니어링’으로 이동할 수 있다. 이는 AI 개발의 민첩성과 유연성을 극대화하여, 지금까지는 비용과 데이터의 문제로 접근하기 어려웠던 무수히 많은 ‘롱테일(long-tail)’ 문제들을 해결할 수 있는 새로운 길을 열어줄 잠재력을 지닌다.14</p>
<h3>2.4  최우수 논문 심층 분석 (2): 통합 자율주행 프레임워크, ‘Planning-oriented Autonomous Driving’</h3>
<p>’Visual Programming’과 함께 CVPR 2023 최우수 논문의 영예를 안은 ’Planning-oriented Autonomous Driving’은 정반대의 철학을 통해 자율주행이라는 극도로 복잡한 실제 세계의 문제를 해결하는 새로운 방향을 제시했다.7 이 연구는 개별 모듈의 조합이 아닌, 시스템 전체를 하나의 유기적인 통합체로 설계하는 접근법의 강력함을 입증했다.</p>
<p>핵심 철학 및 방법론</p>
<p>연구팀은 기존 자율주행 시스템의 표준적인 파이프라인, 즉 ’인식(Perception) → 예측(Prediction) → 계획(Planning)’으로 이어지는 순차적 모듈 구조의 근본적인 문제점을 지적한다. 각 모듈이 독립적으로 최적화될 때, 상위 모듈에서 발생한 작은 오차가 하위 모듈로 전파되며 증폭되는 ‘누적 오류(accumulative errors)’ 문제와 모듈 간 정보 전달 과정에서 발생하는 ‘정보 손실’ 문제가 발생하여 시스템 전체의 안정성을 저해한다는 것이다.19</p>
<p>이러한 문제에 대한 해법으로, 연구팀은 **‘계획 지향적(planning-oriented)’**이라는 새로운 설계 철학을 제안한다. 이는 시스템의 최종 목표인 ’안전하고 효율적인 주행 계획’을 달성하기 위해, 인식과 예측을 포함한 모든 상위 모듈들이 계획 모듈에 가장 유용한 정보를 제공하는 방향으로 함께 최적화되어야 한다는 개념이다.19 이 철학을 구현한 프레임워크가 바로 **UniAD(Unified Autonomous Driving)**이다.</p>
<p>UniAD는 다중 카메라 이미지를 입력받아 조감도(Bird’s-Eye-View, BEV) 특징 공간으로 변환한 뒤, 이 공간 위에서 5가지 핵심 과업을 단일 통합 신경망 내에서 처리한다: 1) 객체 탐지 및 추적(TrackFormer), 2) 차선 등 지도 정보 생성(MapFormer), 3) 다른 차량의 미래 경로 예측(MotionFormer), 4) 미래의 공간 점유 확률 예측(OccFormer), 5) 자차의 주행 경로 계획(Planner).19 이 모든 모듈을 연결하는 핵심적인 기술 요소는</p>
<p><strong>‘쿼리(Query)’</strong> 기반 인터페이스다. 각 과업의 결과물은 단순한 좌표값이나 경계 상자(bounding box)가 아닌, 풍부한 맥락 정보를 담은 고차원 특징 벡터인 ‘쿼리’ 형태로 다음 모듈에 전달된다. 이 쿼리는 파이프라인을 거치면서 점진적으로 정제되고 풍부해지므로, 모듈 간 정보 손실을 최소화하고 유기적인 상호작용을 가능하게 한다.19</p>
<p>성과 및 의의</p>
<p>UniAD는 자율주행 분야의 대표적인 벤치마크인 nuScenes 데이터셋에서 기존의 모든 방법론을 압도하는 성능을 기록했다. 특히, 최종 목표인 주행 계획 오차를 28% 감소시켰으며, 다중 객체 추적 정확도는 20%, 차선 인식 정확도는 30% 향상시키는 등 모든 세부 과업에서 기존 최고 성능(State-of-the-Art, SOTA)을 큰 폭으로 경신했다.7 이는 ’계획 지향적 통합 설계’라는 철학의 실질적인 우월성을 명확히 입증한 결과다.</p>
<p>UniAD의 성공은 복잡한 로봇 시스템 설계에 있어 ’시스템 수준 최적화(system-level optimization)’의 중요성을 강력하게 역설한다. 개별 부품의 성능을 각각 최고로 만드는 것보다, 전체 시스템의 최종 목표에 맞춰 모든 부품을 조화롭게 함께 최적화하는 것이 더 나은 결과를 가져온다는 점을 실증적으로 보여준 것이다. 예를 들어, 전통적인 방식에서는 인식 모듈이 평가 지표인 mAP(mean Average Precision)를 높이는 데 집중하지만, 그 과정에서 계획에 치명적인 영향을 미칠 수 있는 작은 장애물을 놓칠 수 있다. 반면 UniAD에서는 최종적인 ’계획 손실(planning loss)’이 모든 상위 모듈에 역전파되어 함께 훈련되므로, 인식 모듈은 단순히 mAP 점수가 높은 객체가 아니라 ‘안전한 계획에 중요한’ 객체를 탐지하도록 자연스럽게 학습된다. 이처럼 최종 목표에 기반한 통합적 최적화 철학은 자율주행을 넘어, 수술 로봇, 공장 자동화 시스템 등 다수의 하위 시스템이 정교하게 상호작용해야 하는 모든 복잡한 로봇 공학 문제에 적용될 수 있는 핵심적인 설계 원칙을 제시한다. CVPR이라는 컴퓨터 비전 학회에서 로보틱스 성격이 강한 이 논문이 최고상을 수상한 것은, 비전 기술이 더 이상 독립된 인식 문제에 머무르지 않고, 실제 세계의 복잡한 시스템과 어떻게 통합되어야 하는지에 대한 커뮤니티의 깊은 고민과 방향성을 반영하는 상징적인 사건이라 할 수 있다.</p>
<h3>2.5  주목할 만한 수상 논문 분석</h3>
<p>최우수 논문들 외에도, CVPR 2023에서는 AI 기술의 새로운 가능성을 연 주목할 만한 연구들이 다수 발표되었다. 특히 개인화, 동적 장면 이해, 3D 기하학 등 핵심 분야에서 중요한 진전을 이룬 논문들이 우수 논문으로 선정되었다.</p>
<h4>2.5.1 ‘DreamBooth’ (Honorable Mention, Student)</h4>
<p>’DreamBooth’는 대규모 텍스트-이미지 생성 모델을 ’개인화’하는 획기적인 방법을 제시하여 큰 주목을 받았다.11 이 기술의 핵심은 단 3~5장의 적은 이미지 만으로 특정 피사체(예: 사용자의 반려견, 특정 스타일의 가방)의 고유한 시각적 특징을 모델에 주입하는 것이다.23 학습이 완료되면, 사용자는</p>
<p><span class="math math-inline">[V]</span>와 같은 고유 식별자를 사용하여 “해변에 있는 <span class="math math-inline">[V]</span> 강아지”(<span class="math math-inline">a [V] dog on a beach</span>)와 같은 텍스트 프롬프트를 통해 해당 피사체가 포함된 완전히 새로운 장면의 이미지를 자유자재로 생성할 수 있다.23</p>
<p>이 기술의 핵심적인 난제는 특정 피사체를 학습하는 과정에서 모델이 기존에 가지고 있던 일반적인 개념(예: ’강아지’라는 클래스 전체)에 대한 지식을 잃어버리는 ‘언어 표류(language drift)’ 또는 ‘과적합(overfitting)’ 현상이다. DreamBooth는 이를 해결하기 위해 독창적인 **‘사전 보존 손실(Prior Preservation Loss)’**이라는 정규화 기법을 제안했다.23 이 기법은 특정 피사체 이미지(예: 나의 반려견 사진)로 모델을 파인튜닝하는 동시에, 사전 훈련된 모델</p>
<p><strong>자신</strong>이 생성한 일반 클래스 이미지(예: “a photo of a dog” 프롬프트로 생성한 다양한 강아지 이미지)를 함께 학습 데이터로 사용하는 방식이다. 이를 통해 모델은 특정 인스턴스의 고유한 특징을 배우면서도, 해당 클래스의 일반적인 다양성을 잊지 않도록 강제된다.</p>
<p>DreamBooth의 전체 손실 함수는 다음과 같은 수식으로 표현된다 24:</p>
<p><span class="math math-display">
\mathbb{E}_{\mathbf{x},\mathbf{c},\epsilon,\epsilon&#39;,t} \left[ w_t \left\| \hat{\mathbf{x}}_\theta(\alpha_t \mathbf{x} + \sigma_t \epsilon, \mathbf{c}) - \mathbf{x} \right\|_2^2 + \lambda w_{t&#39;} \left\| \hat{\mathbf{x}}_\theta(\alpha_{t&#39;} \mathbf{x}_\text{pr} + \sigma_{t&#39;} \epsilon&#39;, \mathbf{c}_\text{pr}) - \mathbf{x}_\text{pr} \right\|_2^2 \right]
</span><br />
여기서 첫 번째 항은 입력된 특정 피사체 이미지 <span class="math math-inline">\mathbf{x}</span>와 그에 해당하는 프롬프트 <span class="math math-inline">\mathbf{c}</span>에 대한 표준적인 확산 모델의 재구성 손실이다. 두 번째 항이 바로 ’사전 보존 손실’에 해당하며, <span class="math math-inline">\mathbf{x}_\text{pr}</span>은 사전 훈련된 모델이 생성한 일반 클래스 이미지를, <span class="math math-inline">\mathbf{c}_\text{pr}</span>은 일반 클래스 프롬프트(예: “a photo of dog”)를 의미한다. 하이퍼파라미터 <span class="math math-inline">\lambda</span>는 이 두 손실 항 사이의 상대적 중요도를 조절하는 역할을 한다. 이 독창적인 손실 함수 설계를 통해 DreamBooth는 높은 충실도(fidelity)로 피사체를 재현하면서도, 다양한 새로운 맥락에 자연스럽게 합성하는 능력을 갖추게 되었다.</p>
<h4>2.5.2 ‘DynIBaR’ (Honorable Mention)</h4>
<p>’DynIBaR: Neural Dynamic Image-Based Rendering’는 단일 모노큘러 비디오(예: 스마트폰으로 촬영한 영상)로부터 복잡한 동적 장면의 자유로운 시점(free-viewpoint) 영상을 합성하는 문제에 대한 새로운 해법을 제시했다.11 기존의 동적 장면 합성 기술, 특히 NeRF(Neural Radiance Fields) 기반의 방법들은 카메라가 복잡하게 움직이거나 영상의 길이가 길어질 경우, 결과물이 흐릿해지거나 왜곡되는 한계를 보였다.27</p>
<p>DynIBaR은 이러한 한계를 극복하기 위해, 장면의 모든 정보를 거대한 신경망(MLP)의 가중치에 압축하여 저장하는 NeRF 방식에서 벗어났다. 대신, 고전적인 이미지 기반 렌더링(Image-Based Rendering, IBR) 아이디어를 현대적인 신경망 기법과 결합한 ‘체적 이미지 기반 렌더링(volumetric image-based rendering)’ 프레임워크를 채택했다.27 이 방법은 새로운 시점의 이미지를 생성할 때, 해당 지점과 가까운 입력 비디오 프레임들의 지역적 특징(local features)들을 가져와 집계(aggregate)하는 방식으로 동작한다. 이때 핵심은 장면 내 객체의 움직임을 정확하게 추정하고 이를 보상하여(scene motion-aware manner), 동적인 장면임에도 불구하고 정적인 장면처럼 특징들을 정합하고 합성하는 것이다. 이 접근법을 통해 DynIBaR은 긴 비디오나 자유로운 카메라 움직임이 포함된 ‘일상적인(in-the-wild)’ 영상에 대해서도 이전 기술들보다 훨씬 선명하고 사실적인 고품질 뷰 합성 결과를 보여주었다.26</p>
<h4>2.5.3 ‘3D Registration with Maximal Cliques’ (Best Student Paper)</h4>
<p>’3D Registration with Maximal Cliques’는 3D 컴퓨터 비전의 근본적인 문제 중 하나인 ’포인트 클라우드 정합(point cloud registration)’에 대해, 그래프 이론에 기반한 강인하고 효율적인 알고리즘을 제안하여 최우수 학생 논문상을 수상했다.12 3D 정합은 서로 다른 시점에서 스캔된 두 개의 3D 포인트 클라우드 데이터(예: 자율주행차의 라이다 스캔 데이터)를 하나의 좌표계로 정확하게 정렬하는 기술이다. 이 과정의 가장 큰 난관은 특징점 매칭 과정에서 필연적으로 발생하는 다수의 잘못된 대응점, 즉 ’아웃라이어(outliers)’를 효과적으로 제거하고 올바른 변환 관계를 찾아내는 것이다.</p>
<p>기존의 그래프 기반 접근법 중 하나는 대응점들 간의 기하학적 일관성을 노드와 엣지로 표현하는 ’호환성 그래프(compatibility graph)’를 구축한 뒤, 이 그래프 내에서 모든 노드가 서로 연결된 가장 큰 부분 그래프, 즉 ’최대 클릭(maximum clique)’을 찾는 방식이었다. 최대 클릭은 가장 큰 규모의 상호 일관적인 대응점 집합을 의미하므로, 이를 기반으로 변환을 추정하면 강인한 결과를 얻을 수 있다. 하지만 이 논문은 ’최대 클릭’이 단 하나만 존재하며 너무 엄격한 제약 조건이라는 점에 주목했다. 대신, 다른 노드를 추가하면 더 이상 클릭이 아니게 되는 부분 집합인 ’극대 클릭(maximal cliques)’을 여러 개 탐색하는 방식을 제안했다.30 이는 최대 클릭에 비해 더 느슨한 제약 조건으로, 하나의 거대한 일치 집합 대신 여러 개의 신뢰도 높은 ’지역적 일치 집합(local consensus sets)’을 찾아낼 수 있게 한다. 이렇게 찾아낸 다수의 극대 클릭들로부터 여러 변환 가설(hypotheses)을 생성하고, 그중 가장 우수한 가설을 최종 결과로 선택함으로써, 아웃라이어 비율이 매우 높은 도전적인 상황에서도 기존 방법들보다 훨씬 정확하고 안정적인 정합 성능을 달성했다.30 특히, 이 기법은 딥러닝 기반 특징점 추출 방법과 결합되었을 때 3DMatch 및 3DLoMatch 벤치마크에서 각각 95.7%, 78.9%의 높은 정합 성공률을 기록하며 SOTA 성능을 달성했다.30</p>
<h2>3.  ICRA 2023 - 로보틱스 지능의 최전선</h2>
<h3>3.1  학회 개요</h3>
<p>IEEE International Conference on Robotics and Automation (ICRA) 2023은 2023년 5월 29일부터 6월 2일까지 영국 런던의 ExCeL London에서 개최되었다.2 ICRA는 로보틱스 분야에서 가장 권위 있고 영향력 있는 학회 중 하나로, 전 세계 연구자들이 모여 로봇 공학의 이론적 기반부터 실제 산업 및 사회에 적용되는 응용 기술에 이르기까지 광범위한 주제에 대한 최신 연구 성과를 공유하고 토론하는 장이다. 올해 학회 역시 다리 로봇 제어, 다중 로봇 협업, 인간-로봇 상호작용 등 로보틱스 지능의 최전선을 다루는 혁신적인 연구들이 다수 발표되었다.</p>
<h3>3.2 [표 2] ICRA 2023 주요 수상 논문 요약</h3>
<p>다음 표는 ICRA 2023에서 가장 주목받은 최우수 논문 및 주요 부문별 우수 논문들을 정리한 것이다. 이 연구들은 로봇 제어, 계획, 동역학 분야에서 현재 마주한 핵심적인 난제들을 해결하기 위한 창의적인 접근법을 제시하고 있다.</p>
<table><thead><tr><th>수상 부문</th><th>논문 제목</th><th>핵심 기여</th><th>저자 및 소속</th></tr></thead><tbody>
<tr><td><strong>Outstanding Paper</strong></td><td>Distributed Data-Driven Predictive Control for Multi-Agent Collaborative Legged Locomotion</td><td>데이터 기반 예측 제어(DDPC)를 이용한 다중 에이전트 다리 로봇의 분산 협업 보행</td><td>Randall T. Fawcett et al. (Virginia Tech, Caltech)</td></tr>
<tr><td><strong>Outstanding Student Paper</strong></td><td>Robust Locomotion on Legged Robots through Planning on Motion Primitive Graphs</td><td>모션 프리미티브 그래프 상에서의 계획을 통한 강인한 다리 로봇 보행</td><td>Wyatt Ubellacker, Aaron Ames (Caltech)</td></tr>
<tr><td><strong>Outstanding Dynamics and Control Paper</strong></td><td>Nonlinear Model Predictive Control of a 3D Hopping Robot: Leveraging Lie Group Integrators for Dynamically Stable Behaviors</td><td>리 군(Lie Group) 적분기를 활용한 3D 호핑 로봇의 동적 안정성 제어</td><td>Noel Csomay-Shanklin et al. (Caltech)</td></tr>
</tbody></table>
<h3>3.3  최우수 논문 심층 분석: ‘Distributed Data-Driven Predictive Control for Multi-Agent Collaborative Legged Locomotion’</h3>
<p>ICRA 2023 최우수 논문(Outstanding Paper)으로 선정된 이 연구는 여러 대의 다리 로봇이 물리적으로 연결되어 무거운 물체를 함께 운반하는 것과 같은 협업 작업을 수행할 때 발생하는 고차원적인 제어 문제를 해결하기 위한 새로운 프레임워크를 제시했다.34</p>
<p>핵심 문제 및 접근법</p>
<p>다수의 로봇이 단단히 연결(holonomically constrained)되면, 각 로봇의 움직임은 다른 로봇에게 직접적인 힘을 가하게 된다. 로봇의 수가 늘어날수록 이러한 상호작용 동역학은 기하급수적으로 복잡해져, 전통적인 물리 기반 모델링 방식으로는 실시간 제어에 필요한 정확하고 효율적인 모델을 수립하기가 거의 불가능해진다.36</p>
<p>이 연구는 이러한 모델링의 한계를 극복하기 위해, 고전적인 제어 이론과 최신 데이터 기반 방법론을 독창적으로 결합했다. 연구의 핵심은 <strong>데이터 기반 예측 제어(Data-Driven Predictive Control, DDPC)</strong> 기법의 도입이다.36 DDPC는 시스템의 복잡한 미분방정식을 직접 유도하는 대신, 시스템을 운영하며 수집한 실제 입출력 데이터의 시계열(이를 Hankel 행렬 형태로 구성)을 기반으로 시스템의 미래 행동을 직접 예측한다. 이 연구에서는 행동 시스템 이론(behavioral systems theory)을 활용하여, 이 데이터 기반 모델을 예측 제어(Predictive Control) 프레임워크에 통합했다.36 이를 통해, 모델링하기 가장 어려운 로봇 간의 상호작용력을 데이터로부터 학습하여 보상하면서도, 예측 제어의 장점인 미래 상태 예측 및 제약 조건 처리가 가능해졌다.</p>
<p>또한, 이 연구는 계산 효율성을 위해 <strong>분산 제어(distributed control)</strong> 아키텍처를 채택했다. 모든 로봇의 상태를 중앙에서 한 번에 계산하고 제어 명령을 내리는 중앙 집중식 방식은 로봇 수가 늘어남에 따라 계산량이 폭발적으로 증가하여 실시간 적용이 어렵다. 분산 제어 방식에서는 각 로봇(에이전트)이 자신의 상태를 기반으로 지역적인 계산을 수행하고, 이웃 로봇과의 통신을 통해 정보를 교환하며 전체 시스템의 목표를 달성한다. 이를 통해 전체 계산 부담을 각 로봇에 분산시켜 실시간 제어의 확장성을 확보했다.35</p>
<p>성과 및 의의</p>
<p>연구팀은 시뮬레이션 환경에서 최대 5대의 로봇, 그리고 실제 Unitree A1 사족보행 로봇 3대를 물리적으로 연결한 하드웨어 실험을 통해 제안된 방법론의 우수성을 입증했다.36 실험에서 로봇들은 예측하지 못한 외부의 힘(push disturbances), 고르지 않은 지형, 추가적인 탑재물과 같은 다양한 불확실성 속에서도 안정적인 협업 보행을 성공적으로 수행했다.</p>
<p>이 연구의 수상은 로보틱스 분야의 중요한 기술적 흐름을 상징한다. 그것은 순수한 딥러닝 기반의 엔드-투-엔드 제어나 전통적인 모델 기반 제어라는 양극단의 접근법에서 벗어나, 두 패러다임의 장점을 결합하는 ‘하이브리드’ 방식의 실효성과 중요성이 부각되고 있음을 보여준다. 이 연구는 모델 예측 제어(MPC)라는 고전 제어 이론의 강력하고 체계적인 프레임워크를 유지하면서, 그 안에서 모델링이 가장 까다롭고 불확실성이 큰 부분(즉, 로봇 간의 상호작용 동역학)만을 데이터 기반 방식으로 대체하는 실용적인 접근을 취했다. 이는 “이미 잘 알고 있는 물리 법칙은 최대한 활용하고, 모델링하기 어려운 미지의 영역만 데이터로부터 배우자“는 효율적인 공학적 철학을 구현한 것이다. 이러한 하이브리드 접근법은 이론적 안정성을 보장하면서도 실제 세계의 복잡성과 불확실성에 강인하게 대응할 수 있는 현실적인 해법을 제시하며, 향후 복잡한 로봇 시스템 제어 연구의 중요한 방향이 될 것임을 시사한다.</p>
<h3>3.4  주요 수상 논문 분석</h3>
<p>ICRA 2023에서는 다리 로봇의 강인한 보행과 동적 제어 능력의 한계를 확장하는 중요한 연구들이 다수 수상의 영예를 안았다.</p>
<h4>3.4.1 ‘Robust Locomotion on Legged Robots through Planning on Motion Primitive Graphs’ (Outstanding Student Paper)</h4>
<p>이 연구는 다리 로봇이 예측 불가능하고 동적인 환경에서 강인하게 보행할 수 있는 새로운 계획(planning) 프레임워크를 제시하여 최우수 학생 논문상을 수상했다.34 이 접근법의 핵심은 로봇이 수행할 수 있는 다양한 기본 동작들, 예를 들어 ‘제자리 걷기’, ‘빠르게 전진하기’, ‘옆으로 이동하기’ 등을 **‘모션 프리미티브(motion primitive)’**라는 원자적인 단위로 정의하는 것에서 시작한다.40</p>
<p>연구팀은 이러한 모션 프리미티브들을 노드(node)로, 그리고 한 프리미티브에서 다른 프리미티브로 안전하게 전환(transition)할 수 있는 조건을 엣지(edge)로 표현하는 **‘모션 프리미티브 그래프(motion primitive graph)’**를 구축했다. 이 그래프는 로봇의 동적 능력에 대한 일종의 추상화된 지도를 제공한다. 로봇이 보행하는 도중 외부로부터 예기치 않은 충격을 받거나 미끄러운 바닥을 만나는 등 교란(disturbance)이 발생하면, 시스템은 현재 로봇의 동역학적 상태(예: 속도, 자세)를 평가한다. 그리고 이 상태에서 현재의 모션 프리미티브를 유지하는 것이 불안정하다고 판단되면, 모션 프리미티브 그래프 상에서 현재 상태로부터 도달 가능한 가장 안정적인 다음 모션 프리미티브로의 경로를 실시간으로 탐색한다.40 예를 들어, 빠르게 달리다가(모션 프리미티브 A) 갑자기 강한 측면 충격을 받으면, 넘어지는 대신 즉시 자세를 낮추고 옆으로 걷는(모션 프리미티브 B) 동작으로 전환하여 안정성을 회복하는 식이다. 이처럼 고수준의 이산적 계획(그래프 탐색)과 저수준의 연속적 제어(프리미티브 실행)를 결합함으로써, 로봇은 다양한 돌발 상황에 유연하고 강인하게 대처하는 전체론적인(holistic) 강인성을 확보할 수 있다. 연구팀은 실제 사족보행 로봇을 이용한 다양한 실험을 통해 이 프레임워크의 효과를 성공적으로 입증했다.40</p>
<h4>3.4.2 ‘Nonlinear Model Predictive Control of a 3D Hopping Robot’ (Outstanding Dynamics and Control Paper)</h4>
<p>이 연구는 로봇 공학의 고전적이면서도 매우 도전적인 과제인 ’3D 호핑 로봇의 안정적인 제어’를 비선형 모델 예측 제어(Nonlinear Model Predictive Control, NMPC)를 통해 성공적으로 구현하여 동역학 및 제어 부문 최우수 논문상을 수상했다.34 호핑 로봇의 제어는 공중에 떠 있는 대부분의 시간 동안 제어가 불가능한 ‘과소 작동(underactuated)’ 상태와, 지면에 닿는 극히 짧은 순간에만 제어 입력을 가할 수 있다는 극단적인 하이브리드 동역학 특성 때문에 매우 어렵다.41</p>
<p>이 연구의 핵심적인 기술적 기여는 로봇의 3차원 회전 자세(attitude)를 NMPC의 예측 모델 내에서 수학적으로 일관되고 안정적으로 다루는 방법을 제시한 데 있다. 일반적으로 오일러 각(Euler angles)과 같은 3차원 회전 표현 방식은 특정 자세에서 발생하는 ’짐벌 락(gimbal lock)’과 같은 특이점(singularity) 문제를 가지고 있어, 큰 각도의 회전이 포함된 동적인 움직임을 제어할 때 수치적 불안정성을 야기할 수 있다. 연구팀은 이러한 문제를 근본적으로 해결하기 위해, 회전 공간의 기하학적 구조를 올바르게 표현하는 수학적 도구인 <strong>리 군(Lie Group)</strong> 이론을 도입했다. 구체적으로, NMPC가 미래 상태를 예측하기 위해 동역학 모델을 수치적으로 적분하는 과정에 **‘리 군 적분기(Lie group integrators)’**를 적용함으로써, 특이점 문제없이 기하학적으로 일관된 방식으로 로봇의 회전 운동을 정확하게 예측하고 최적화할 수 있게 되었다.41 이 접근법을 통해 연구팀은 자체 개발한 새로운 3D 호핑 로봇의 안정적인 제자리 호핑을 실제 하드웨어에서 성공적으로 시연했으며, 시뮬레이션 상에서는 궤적 추종 및 공중제비와 같은 고도의 동적 기동까지 가능함을 보여주었다.41</p>
<h2>4.  산업계의 혁신: AI의 실용화와 미래 과제</h2>
<p>2023년 6월과 그 직후, 학계의 연구 진전과 더불어 산업계, 특히 AI 분야를 선도하는 기업들로부터 AI의 실용화와 미래 방향성에 대한 중요한 발표가 이어졌다. OpenAI는 LLM의 응용 가능성을 극적으로 확장하는 기술적 이정표를 제시했으며, 동시에 AI 기술이 인류에게 미칠 장기적인 위험에 대한 근본적인 고민과 대응책을 발표하며 기술 리더십의 양면을 보여주었다.</p>
<h3>4.1  OpenAI의 API 혁신: 함수 호출(Function Calling) 기능</h3>
<p>2023년 6월 13일, OpenAI는 <span class="math math-inline">gpt-4-0613</span> 및 <span class="math math-inline">gpt-3.5-turbo-0613</span> 모델을 포함한 API의 대규모 업데이트를 발표했으며, 그중 가장 핵심적인 기능은 **‘함수 호출(Function Calling)’**이었다.44 이 기능은 LLM이 단순한 텍스트 생성 도구를 넘어, 외부 소프트웨어 및 서비스와 안정적으로 상호작용할 수 있는 강력한 ’애플리케이션 백엔드’로 진화하는 결정적인 계기를 마련했다.</p>
<p>기술적 개념 및 의의</p>
<p>함수 호출 기능의 핵심은 개발자가 API를 통해 LLM과 소통할 때, 자신이 정의한 외부 함수들(예: send_email, get_current_weather)의 명세(signature)를 JSON 스키마 형식으로 모델에 전달할 수 있다는 점이다. 그러면 모델은 사용자의 자연어 입력을 분석하여, 그 의도를 수행하기 위해 어떤 함수를 어떤 인자(argument)와 함께 호출해야 하는지를 지능적으로 판단하고, 그 결과를 미리 약속된 구조화된 JSON 객체 형태로 반환한다.44 예를 들어, 사용자가 “보스턴의 현재 날씨는 어떤가요?“라고 질문하면, 모델은 <code>{"name": "get_current_weather", "arguments": {"location": "Boston, MA"}}</code> 와 같은 JSON을 출력한다. 개발자는 이 출력을 파싱하여 실제 날씨 API를 호출하고, 그 결과를 다시 모델에 전달하여 사용자에게 자연어 답변을 생성하게 할 수 있다.</p>
<p>이 기능이 도입되기 이전에는, 개발자들이 LLM의 출력을 특정 형식(예: JSON)으로 유도하기 위해 복잡하고 불안정한 프롬프트 엔지니어링에 의존해야 했다. 모델이 항상 약속된 형식으로 출력한다는 보장이 없었기 때문에, 출력 결과를 파싱하는 과정에서 잦은 오류가 발생하여 신뢰성 있는 애플리케이션을 구축하기 어려웠다. 함수 호출 기능은 이러한 ’유도’의 과정을 모델 자체에 내장된 ’정의된 약속(contract)’으로 전환시켰다. 모델 자체가 함수 호출의 필요성을 감지하고 정확한 형식의 JSON을 출력하도록 파인튜닝되었기 때문에 44, LLM과 외부 도구 간의 연동이 훨씬 더 안정적이고 예측 가능해졌다. 이와 함께, <code>gpt-3.5-turbo</code> 모델에 16,000 토큰의 컨텍스트 길이를 지원하는 버전이 추가되고, 임베딩 모델과 <code>gpt-3.5-turbo</code> 모델의 가격이 각각 75%, 25% 인하되는 등 44 접근성 또한 크게 향상되었다.</p>
<p>AI 에이전트 개발의 대중화</p>
<p>‘함수 호출’ 기능의 등장은 LLM 기반 AI 에이전트 개발 생태계에 아이폰의 ’SDK(Software Development Kit)’가 등장한 것과 같은 파급력을 지닌다. 아이폰이 공식 SDK를 배포함으로써 수많은 개발자들이 안정적이고 표준화된 방식으로 앱을 개발하고 거대한 생태계를 구축할 수 있었던 것처럼, 함수 호출 기능은 개발자들이 LLM을 핵심 추론 엔진으로 사용하여 날씨 조회, 이메일 전송, 데이터베이스 쿼리, 온라인 쇼핑 등 구체적인 기능을 수행하는 수많은 ’AI 네이티브 애플리케이션’을 만들 수 있는 표준화된 ’개발 키트’를 제공한 셈이다. 이는 LLM을 활용한 애플리케이션 개발의 복잡성과 불확실성을 극적으로 낮추는 핵심적인 인프라 기술이다. 이를 통해 더 많은 개발자들이 LLM의 강력한 능력을 자신의 서비스에 손쉽게 통합할 수 있게 되면서, 이전에 상상하기 어려웠던 다양한 형태의 AI 에이전트와 서비스가 폭발적으로 등장할 수 있는 기술적 토대가 마련되었다.</p>
<h3>4.2  AI 안전성의 새로운 도전: OpenAI ‘초지능 정렬(Superalignment)’</h3>
<p>기술의 실용적 확장에 대한 발표와 더불어, OpenAI는 2023년 7월 5일, AI 기술의 장기적인 위험성에 대한 심도 깊은 고민과 대응을 담은 ‘초지능 정렬(Superalignment)’ 팀의 발족을 발표했다.45 이는 6월의 기술적 진보와 맞물려, AI 개발의 최전선에서 기술의 힘과 책임에 대한 논의가 어떻게 진행되고 있는지를 보여주는 중요한 사건이다.</p>
<p>문제 인식과 접근 방식</p>
<p>OpenAI는 이 발표를 통해, 인간보다 훨씬 뛰어난 지능을 가진 가상의 ’초지능(Superintelligence)’이 향후 10년 안에 등장할 수 있다고 예측하며, 이러한 시스템이 인류의 의도에 반하여 통제 불가능한 행동을 할 위험을 공식적으로 제기했다.45 특히, 현재 ChatGPT와 같은 모델을 정렬(align)하는 데 사용되는 핵심 기술인 ’인간 피드백 기반 강화학습(Reinforcement Learning from Human Feedback, RLHF)’은 인간이 AI의 행동을 평가하고 감독할 수 있다는 것을 전제로 한다. 하지만 초지능은 인간의 이해와 감독 능력을 훨씬 뛰어넘을 것이기 때문에, RLHF와 같은 현재의 정렬 기법은 초지능에 대해서는 확장될 수 없다는 근본적인 한계를 명확히 인정했다.45</p>
<p>이러한 전례 없는 난제를 해결하기 위해, OpenAI는 공동 창업자이자 수석 과학자인 일리야 수츠케버(Ilya Sutskever)와 정렬 팀 헤드인 얀 라이케(Jan Leike)의 공동 주도 하에 새로운 ‘초지능 정렬’ 팀을 구성했다. 이 팀의 목표는 ’인간 수준의 자동화된 정렬 연구원(human-level automated alignment researcher)’을 구축하는 것이다. 이는 본질적으로 AI를 사용하여 또 다른 AI, 특히 인간보다 더 지능적인 AI의 정렬 문제를 연구하고 해결하는 ‘재귀적(recursive)’ 접근법을 의미한다. OpenAI는 이 야심찬 목표를 위해, 향후 4년간 확보한 컴퓨팅 자원의 20%라는 막대한 양을 이 문제에 투입할 것이라고 약속했다.45</p>
<p>철학에서 공학으로의 전환</p>
<p>‘초지능 정렬’ 팀의 발족과 막대한 자원 투입은 AI 안전성에 대한 논의가 더 이상 일부 연구자들의 철학적, 추상적 담론의 영역에 머무르지 않고, AI 분야를 선도하는 기업의 최우선 순위에 놓인 시급하고 구체적인 ’공학적 문제’로 전환되었음을 알리는 강력한 선언이다. 이는 AI 기술의 개발자 스스로가 “우리가 만드는 기술이 통제 불가능해질 수 있으며, 현재의 방법으로는 그 위험을 막을 수 없다“고 공식적으로 인정한 중대한 사건이다. ’10년 내’라는 구체적인 시간표와 ’컴퓨팅 자원 20%’라는 실질적인 투자는 이 문제가 먼 미래의 공상과학적 위험이 아니라, 당장 해결책을 찾아야 하는 현실적인 기술적 난제임을 명백히 한다. 또한, ’AI로 AI를 정렬한다’는 접근법은 문제의 난이도가 이미 인간의 개별적인 감독 능력을 넘어설 것임을 전제하고 있으며, 이는 향후 AI 안전성 연구의 방향이 ’인간의 감독 능력 강화’에서 ’신뢰할 수 있는 자동화된 감독 시스템 개발’로 전환되어야 함을 시사하는 중요한 이정표다.</p>
<h2>5.  ArXiv를 통해 본 6월의 연구 흐름</h2>
<p>주요 학회 발표 외에도, 2023년 6월 한 달간 사전 논문 공개 플랫폼인 ArXiv의 로보틱스(cs.RO) 섹션에는 526편의 논문이 등재되는 등 46, 학계의 최신 연구 흐름을 엿볼 수 있는 다양한 연구들이 활발하게 공유되었다. 이들 연구는 자율 탐사, 정보 수집, 생성 모델의 활용, 인간-로봇 상호작용 등 로보틱스의 핵심 분야 전반에 걸쳐 있었다.46</p>
<h3>5.1  자율 탐사 및 강인한 SLAM</h3>
<p>미지의 환경을 스스로 탐사하며 지도를 작성하는 것은 자율 로봇의 가장 근본적인 능력 중 하나다. 이와 관련하여, 6월 ArXiv에는 현실 세계의 다양한 제약 조건 하에서 로봇의 자율성을 구현하려는 실용적인 연구들이 다수 발표되었다. 한 연구에서는 저가형 로봇이나 스마트폰처럼 시야각(Field-of-View, FoV)이 좁은 센서를 사용하는 환경에서 동시적 위치 추정 및 지도 작성(Simultaneous Localization and Mapping, SLAM) 시스템의 안정성을 유지하기 위한 새로운 탐사 전략을 제안했다. 이 전략은 시각적 정보량이 풍부한 특정 지점을 ’등대(lighthouse)’로 지정하고, 탐사 중 주기적으로 이 등대를 재방문하여 누적된 위치 오차를 보정함으로써, 특징이 부족한 환경에서도 강인하게 지도를 작성하는 것을 목표로 한다.47 또 다른 연구는 다중 로봇 시스템이 GPS와 같은 전역적 위치 정보 없이, 로봇 간의 통신 신호 강도(RSSI) 등을 이용해 상대적 위치를 지속적으로 보정하면서, 동시에 가우시안 프로세스(Gaussian Processes, GP)를 이용해 미탐사 영역에 대한 정보 획득을 최대화하는 통합적인 탐사 및 측위(Simultaneous Exploration and Localization, SEAL) 기법을 제안했다.48</p>
<h3>5.2  효율적인 정보 수집 및 모델링</h3>
<p>로봇이 제한된 자원(시간, 배터리)을 사용하여 환경에 대한 가장 유용한 정보를 효율적으로 수집하는 ‘로봇 정보 수집(Robotic Information Gathering, RIG)’ 문제 역시 활발히 연구되었다. 특히, 환경 모델링의 정확도를 높이기 위한 새로운 기법이 주목받았다. 한 연구에서는 환경의 특성이 위치에 따라 변하는 ’비정상성(non-stationarity)’을 효과적으로 모델링하기 위해, 가우시안 프로세스의 커널 함수를 데이터로부터 적응적으로 학습하는 ’주의 집중 커널(Attentive Kernel)’을 제안했다.49 이 기법을 적용한 자율 수상정(ASV)은 수심이나 수온의 변화가 급격한 지역을 우선적으로 탐사하여, 적은 데이터로도 환경의 중요한 특징을 효율적으로 모델링할 수 있음을 실험적으로 보였다.46</p>
<h3>5.3  생성 모델과 시뮬레이션을 활용한 로봇 학습</h3>
<p>로봇 공학 분야의 고질적인 문제인 데이터 부족 문제를 해결하기 위해, 생성 모델(Generative Models)과 시뮬레이션을 활용하려는 시도가 두드러졌다. 특히, 최근 이미지 생성 분야에서 놀라운 성공을 거둔 확산 모델(Diffusion Models)을 로봇 학습에 적용하는 연구가 활발하다. 확산 모델은 복잡하고 다봉적인(multi-modal) 데이터 분포를 모델링하는 데 강점을 보여, 여러 개의 유효한 해가 존재할 수 있는 로봇의 궤적 계획(trajectory planning)이나 파지(grasping) 학습 문제에 유망한 접근법으로 떠오르고 있다.50 또한, 시뮬레이션 환경에서 강화학습을 통해 제어 정책을 학습한 뒤, 이를 실제 로봇에 성공적으로 이전하는 ‘Sim2Real’ 연구는 저비용으로 고성능 로봇 제어기를 개발할 수 있는 가능성을 열어주고 있다. 한 연구에서는 저가의 RC카를 이용하여, 시뮬레이션에서 학습한 드리프트 주행, 험지 주행, 시각 기반 항법 정책을 별도의 추가 학습 없이 실제 RC카에서 성공적으로 구현하는 사례를 보여주었다.52</p>
<h3>5.4  인간-로봇 상호작용 및 내재된 AI(Embodied AI)</h3>
<p>대규모 언어 모델(LLM)의 발전은 로봇이 인간과 더 자연스럽게 상호작용할 수 있는 새로운 가능성을 열었다. 인간의 말과 함께 나타나는 제스처를 생성하는 로봇 챗 시스템에 대한 연구 46 등은 LLM을 로봇의 ’사회적 지능’과 결합하려는 시도의 일환이다. 더 나아가, 로봇이나 스마트 안경처럼 물리적인 실체(embodiment)를 가지고 현실 세계와 상호작용하며 학습하는 ’내재된 AI(Embodied AI)’가 미래 AI 발전의 중요한 패러다임으로 부상하고 있다. 이러한 에이전트는 단순히 분리된 데이터를 학습하는 것을 넘어, 실제 환경과의 연속적인 상호작용을 통해 세상을 이해하고 예측하는 ’세계 모델(world model)’을 내재적으로 구축할 수 있을 것으로 기대된다.53 이는 AI가 인간과 같은 방식으로 세상을 배우고 이해하는 데 한 걸음 더 다가가는 중요한 연구 방향이다.</p>
<h2>6. 결론: 2023년 6월 연구 동향의 종합적 의의 및 향후 전망</h2>
<p>2023년 6월에 발표된 AI 및 로보틱스 분야의 주요 연구들은 표면적으로는 각기 다른 문제를 다루고 있지만, 그 기저에는 AI 시스템을 설계하는 두 가지 강력하고 뚜렷한 패러다임이 부상하고 있음을 보여준다. 이 두 가지 흐름은 향후 AI 기술의 발전 방향을 이해하는 중요한 렌즈를 제공한다.</p>
<p>첫 번째는 **‘구성적(Compositional) 패러다임’**이다. CVPR 최우수 논문인 ’Visual Programming’과 OpenAI의 ‘함수 호출’ 기능이 이 패러다임의 대표적인 사례다. 이 접근법의 핵심 철학은, 강력한 추론 능력을 지닌 대규모 언어 모델(LLM)을 시스템의 중앙 처리 장치(CPU) 또는 ’범용 지휘자’로 간주하는 것이다. 그리고 객체 탐지 모델, 이미지 편집 API, 데이터베이스 쿼리 엔진 등 고도로 전문화된 ’모듈’들을 마치 라이브러리처럼 활용하여, LLM이 이들을 동적으로 호출하고 조합함으로써 복잡한 과업을 해결한다. 이 방식은 마치 프로그래머가 다양한 함수와 API를 엮어 새로운 소프트웨어를 만드는 것과 유사하다. 이 패러다임의 가장 큰 장점은 시스템의 유연성, 확장성, 그리고 해석 가능성을 극대화한다는 점이다. 새로운 기능이 필요할 때마다 거대한 모델 전체를 재훈련할 필요 없이, 해당 기능을 수행하는 새로운 모듈을 추가하고 LLM에게 그 사용법을 알려주기만 하면 된다.</p>
<p>두 번째는 **‘통합적(Integrative) 패러다임’**이다. 또 다른 CVPR 최우수 논문인 ’Planning-oriented Autonomous Driving(UniAD)’과 ICRA 최우수 논문인 ‘Distributed Data-Driven Predictive Control’ 연구가 이를 상징한다. 이 접근법은 자율주행이나 다중 로봇 협업과 같이, 시스템의 여러 구성요소들이 서로 긴밀하게 얽혀 있고 안전성(safety)과 강인성(robustness)이 최우선시되는 문제를 다룬다. 이 철학은 시스템을 개별 모듈의 단순한 합으로 보지 않고, 하나의 거대한 ’통합 유기체’로 간주한다. 따라서 시스템의 최종 목표(예: 안전한 주행)를 달성하기 위해, 인식, 예측, 제어 등 모든 하위 구성요소들을 하나의 프레임워크 안에서 ‘엔드-투-엔드(end-to-end)’ 또는 시스템 전반에 걸쳐 함께 최적화한다. 이를 통해 모듈 간의 정보 손실을 최소화하고 상호작용을 최적화하여, 개별 모듈의 성능 합을 뛰어넘는 전체 시스템의 성능과 안정성을 달성하는 것을 목표로 한다.</p>
<p>이 두 패러다임은 서로 대립하거나 배타적인 관계가 아니라, 해결하고자 하는 문제의 성격에 따라 선택되는 상호 보완적인 접근법으로 이해해야 한다. ‘구성적’ 접근법은 정의되지 않은 무한에 가까운 개방형 과업(open-ended tasks), 예를 들어 “어제 찍은 사진 중에서 가장 재미있는 것을 골라 친구에게 농담과 함께 보내줘“와 같은 인간의 창의적이고 복합적인 요구사항을 처리하는 데 강력한 잠재력을 보인다. 반면, ‘통합적’ 접근법은 자율주행이나 수술 로봇과 같이, 모든 예외 상황에 대해 예측 가능하고 신뢰할 수 있는 반응이 요구되는 고도로 구조화되고 안전이 최우선시되는 과업(highly structured, safety-critical tasks)에 필수적이다.</p>
<p>미래의 가장 진보된 AI 시스템은 아마도 이 두 가지 철학을 모두 채택하는 계층적 구조를 가질 가능성이 높다. 예를 들어, 미래의 자율주행 차량은 차량의 주행 자체를 책임지는 핵심 제어 시스템은 UniAD와 같은 고도로 최적화된 ‘통합’ 모델이 담당하여 최대한의 안전성과 안정성을 확보할 것이다. 그리고 그 상위 레벨에서, 운전자와 상호작용하고, 목적지를 추천하며, 차량 내 엔터테인먼트를 제어하는 인포테인먼트 시스템은 ‘함수 호출’ 기능을 사용하는 ‘구성적’ AI 에이전트가 담당하여 무한한 유연성과 확장성을 제공할 것이다. 이처럼 2023년 6월은 AI 시스템 설계에 있어 이 두 가지 거대한 흐름이 명확하게 그 모습을 드러낸 중요한 시점이었다. 앞으로 이 두 패러다임이 각자의 영역에서 어떻게 발전하고, 때로는 서로 경쟁하며, 궁극적으로 어떻게 융합하여 더 고도화된 지능 시스템을 만들어 나갈지를 지켜보는 것이 AI 기술의 미래를 예측하는 핵심적인 관전 포인트가 될 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>2023 Dates and Deadlines - CVPR - The Computer Vision Foundation, https://cvpr.thecvf.com/Conferences/2023/Dates</li>
<li>Awesome Robotics Conferences and Schools List, https://torydebra.github.io/AwesomeRoboticsConferencesAndSchoolsList/conf.html</li>
<li>2023 IEEE International Conference on Robotics and Automation (ICRA 2023) (Table of Contents) - Proceedings.com, https://www.proceedings.com/content/069/069566webtoc.pdf</li>
<li>CVPR 2023 Accepted Papers, https://cvpr.thecvf.com/Conferences/2023/AcceptedPapers</li>
<li>CVPR 2023 News Gallery - The Computer Vision Foundation, https://cvpr.thecvf.com/Conferences/2023/NewsGallery</li>
<li>CVPR 2023 Plenary Panels to Expound on History and Future of AI; Highlight Creativity in Computer Vision and Language, https://cvpr.thecvf.com/Conferences/2023/PlenaryPanelAdvisory</li>
<li>SenseTime Wins the Best Paper Award at CVPR 2023 For Groundbreaking Perception-Decision Integrated Autonomous Driving Foundation Model, https://www.sensetime.com/en/news-detail/51166787</li>
<li>Exhibitor Information - CVPR - The Computer Vision Foundation, https://cvpr.thecvf.com/Conferences/2023/ExhibitorInformation</li>
<li>Computer Vision and Pattern Recognition (CVPR), 2023 | ServiceNow Research, https://www.servicenow.com/research/event/2023-cvpr.html</li>
<li>CVPR 2023 Call for Papers - The Computer Vision Foundation, https://cvpr.thecvf.com/Conferences/2023/CallForPapers</li>
<li>CVPR 2023 Wraps with Record Paper Submissions and Continued Growth Trajectory, https://cvpr.thecvf.com/Conferences/2023/ClosingStatement</li>
<li>CVPR 2023 Awards - The Computer Vision Foundation, https://cvpr.thecvf.com/Conferences/2023/Awards</li>
<li>CVPR Paper Awards - IEEE Computer Society Technical Committee on Pattern Analysis and Machine Intelligence, https://tc.computer.org/tcpami/awards/cvpr-paper-awards/</li>
<li>Visual Programming: Compositional visual reasoning without training, https://www.computer.org/csdl/proceedings-article/cvpr/2023/012900o4953/1POQlWl6vuw</li>
<li>CVPR Poster Visual Programming: Compositional Visual Reasoning Without Training, https://cvpr.thecvf.com/virtual/2023/poster/22652</li>
<li>Visual Programming: Compositional Visual … - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf</li>
<li>Visual Programming: Compositional visual reasoning without training - Semantic Scholar, https://www.semanticscholar.org/paper/Visual-Programming%3A-Compositional-visual-reasoning-Gupta-Kembhavi/af1c871282ec122869d03f5420ef5d9143358a91</li>
<li>Reviewing the CVPR 2023 best paper winners: A Balanced Examination of UniAD and VISPROG | by Kiran Bhattacharyya | Medium, https://medium.com/@bhattacharyyakiran12/reviewing-the-cvpr-2023-best-paper-winners-a-balanced-examination-of-uniad-and-visprog-be3daaa36b16</li>
<li>Planning-Oriented Autonomous Driving - CVF Open Access - The …, https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Planning-Oriented_Autonomous_Driving_CVPR_2023_paper.pdf</li>
<li>Planning-oriented Autonomous Driving - IEEE Computer Society, https://www.computer.org/csdl/proceedings-article/cvpr/2023/012900r7853/1POTrq0pwlO</li>
<li>Insights into Top Paper Nominee, “Planning-oriented Autonomous Driving” - CVPR, https://cvpr.thecvf.com/Conferences/2023/AuthorQAAutonomous</li>
<li>CVPR Honors Best Paper Recipients - The Computer Vision Foundation, https://cvpr.thecvf.com/Conferences/2023/Blog22June</li>
<li>DreamBooth, https://dreambooth.github.io/</li>
<li>DreamBooth: Fine Tuning Text-to-Image … - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.pdf</li>
<li>How to rewrite DreamBooth loss in terms of ϵ-prediction? - Cross Validated, https://stats.stackexchange.com/questions/601782/how-to-rewrite-dreambooth-loss-in-terms-of-epsilon-prediction</li>
<li>DynIBaR: Neural Dynamic Image-Based Rendering, https://dynibar.github.io/</li>
<li>DynIBaR: Neural Dynamic Image-Based Rendering - IEEE Computer Society, https://www.computer.org/csdl/proceedings-article/cvpr/2023/1.29E277/1POShj1rlDy</li>
<li>[PDF] DynIBaR: Neural Dynamic Image-Based Rendering | Semantic Scholar, https://www.semanticscholar.org/paper/7129623909b2a944f0d486bf2b9dd7e242552b83</li>
<li>CVPR Poster 3D Registration With Maximal Cliques - The Computer Vision Foundation, https://cvpr.thecvf.com/virtual/2023/poster/22705</li>
<li>3D Registration With Maximal Cliques - CVF Open Access - The …, https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_3D_Registration_With_Maximal_Cliques_CVPR_2023_paper.pdf</li>
<li>MAC: Maximal Cliques for 3D Registration - ResearchGate, https://www.researchgate.net/publication/383091490_MAC_Maximal_Cliques_for_3D_Registration</li>
<li>[CVPR 2023, BSP] 3D Registration with Maximal Cliques - GitHub, https://github.com/zhangxy0517/3D-Registration-with-Maximal-Cliques</li>
<li>Conference Proceedings : 29th May-2nd June 2023, ExCeL London - Google Books, https://books.google.com/books/about/2023_IEEE_International_Conference_on_Ro.html?id=U34V0AEACAAJ</li>
<li>#ICRA2023 awards finalists and winners - ΑΙhub - AI Hub, https://aihub.org/2023/06/12/icra2023-awards-finalists-and-winners/</li>
<li>Robotics paper wins the outstanding paper award of the 2023 IEEE ICRA, https://me.vt.edu/news/briefs/hamed-fawcett-best-paper-ieee-icra-2023.html</li>
<li>Distributed Data-Driven Predictive Control for Multi-Agent Collaborative Legged Locomotion, https://authors.library.caltech.edu/records/fbgka-5wn20/latest</li>
<li>Distributed Data-Driven Predictive Control for Multi-Agent Collaborative Legged Locomotion, https://www.semanticscholar.org/paper/Distributed-Data-Driven-Predictive-Control-for-Fawcett-Amanzadeh/077fcca7b074061cd04c034137c96fbc9041d9d6</li>
<li>(PDF) Distributed Data-Driven Predictive Control for Multi-Agent Collaborative Legged Locomotion - ResearchGate, https://www.researchgate.net/publication/365371909_Distributed_Data-Driven_Predictive_Control_for_Multi-Agent_Collaborative_Legged_Locomotion</li>
<li>ICRA 2023 Special - Weekly Robotics #250, https://www.weeklyrobotics.com/weekly-robotics-250</li>
<li>Robust Locomotion on Legged Robots through Planning on Motion …, https://arxiv.org/pdf/2209.07503</li>
<li>Nonlinear Model Predictive Control of a 3D Hopping Robot …, https://authors.library.caltech.edu/records/8gvvt-tg512/latest</li>
<li>Nonlinear Model Predictive Control of a 3D Hopping Robot … - 专知, https://zhuanzhi.ai/paper/797881474872d5eda3c09aa53a8e3245</li>
<li>Nonlinear Model Predictive Control of a 3D Hopping Robot: Leveraging Lie Group Integrators for Dynamically Stable Behaviors | Request PDF - ResearchGate, https://www.researchgate.net/publication/372113562_Nonlinear_Model_Predictive_Control_of_a_3D_Hopping_Robot_Leveraging_Lie_Group_Integrators_for_Dynamically_Stable_Behaviors</li>
<li>Function calling and other API updates | OpenAI, https://openai.com/index/function-calling-and-other-api-updates/</li>
<li>Introducing Superalignment - OpenAI, https://openai.com/index/introducing-superalignment/</li>
<li>Robotics Jun 2023 - arXiv, https://arxiv.org/list/cs.RO/2023-06</li>
<li>Lighthouses and Global Graph Stabilization: Active SLAM for Low-compute, Narrow-FoV Robots - arXiv, https://arxiv.org/pdf/2306.10463</li>
<li>arXiv:2306.12623v1 [cs.RO] 22 Jun 2023, https://arxiv.org/pdf/2306.12623</li>
<li>Adaptive Robotic Information Gathering via Non-Stationary Gaussian Processes - arXiv, https://arxiv.org/pdf/2306.01263</li>
<li>Generative Artificial Intelligence in Robotic Manipulation: A Survey - arXiv, https://arxiv.org/html/2503.03464v1</li>
<li>Diffusion Models for Robotic Manipulation: A Survey - arXiv, https://arxiv.org/html/2504.08438v1</li>
<li>Wheeled Lab: Modern Sim2Real for Low-cost, Open-source Wheeled Robotics - arXiv, https://arxiv.org/html/2502.07380v2</li>
<li>Embodied AI Agents: Modeling the World - arXiv, https://arxiv.org/html/2506.22355v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>