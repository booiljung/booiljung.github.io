<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2023년 7월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2023년 7월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2023년 AI 및 로봇 연구 동향</a> / <span>2023년 7월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2023년 7월 AI 및 로봇 연구 동향</h1>
<h2>1. 서론: AI 기술 민주화와 연구의 심화</h2>
<p>2023년 7월은 인공지능(AI) 기술의 접근성 확대와 학문적 연구의 심화가 동시에 일어난 중요한 변곡점으로 기록된다. 산업계에서는 Meta의 Llama 2 공개가 고성능 대규모 언어 모델(Large Language Model, LLM)의 민주화를 가속했으며, 이는 이전까지 소수 빅테크 기업의 전유물로 여겨졌던 최첨단 AI 기술을 중소기업 및 개인 개발자에게까지 확산시키는 기폭제가 되었다.1 이러한 기술 확산의 이면에서는 책임과 규제에 대한 사회적 논의 또한 본격화되었다. 미국 백악관이 주도한 주요 AI 기업 7개사와의 자발적 안전 협약 체결은 기술 발전의 속도와 사회적 안전망 구축 사이의 균형점을 찾으려는 노력이 시작되었음을 시사한다.1</p>
<p>학계에서는 자연어 처리, 머신러닝, 로보틱스 분야의 최상위 국제 학회인 ACL(Association for Computational Linguistics), ICML(International Conference on Machine Learning), RSS(Robotics: Science and Systems)가 연이어 개최되며 연구의 깊이를 더했다. 이들 학회에서는 단순히 모델의 성능을 높이는 것을 넘어, AI 모델의 내부 작동 원리를 규명하려는 해석 가능성(interpretability) 연구와 LLM이 가진 방대한 지능을 물리적 세계로 확장하려는 로보틱스 응용 연구에서 중대한 진전이 발표되었다.</p>
<p>본 보고서는 2023년 7월 한 달간 관찰된 AI 및 로봇 분야의 핵심 동향을 산업, 학계, 그리고 응용 연구의 다각적 관점에서 심층적으로 분석한다. Llama 2로 대표되는 개방형 AI 생태계의 변화, 주요 학회에서 발표된 핵심 연구들의 기술적 의의, 그리고 로보틱스 분야의 패러다임 전환을 이끄는 새로운 방법론들을 상세히 탐구함으로써, 해당 시점의 기술적 성취를 조명하고 미래 연구 방향에 대한 통찰을 제공하고자 한다.</p>
<h2>2.  거대 언어 모델(LLM)의 새로운 지평: 개방, 경쟁, 그리고 책임</h2>
<p>2023년 7월은 LLM 분야에서 개방성, 경쟁, 그리고 책임이라는 세 가지 키워드가 두드러진 시기였다. Meta의 Llama 2 출시는 오픈소스 생태계에 전례 없는 활력을 불어넣었고, Anthropic의 Claude 2 등장은 기술 경쟁을 심화시켰다. 동시에, AI 기술의 사회적 영향력에 대한 인식이 높아지면서 안전한 개발을 위한 규제 논의가 수면 위로 부상했다.</p>
<h3>2.1  Meta Llama 2: 오픈소스 AI 생태계의 판도를 바꾸다</h3>
<p>7월 AI 기술 분야에서 가장 중요한 사건은 Meta의 Llama 2 출시였다. 이는 단순한 모델 공개를 넘어, AI 개발 생태계의 지형을 근본적으로 바꾸는 전략적 행보로 평가된다.</p>
<h4>2.1.1 기술적 개요</h4>
<p>Llama 2는 70억(7B)에서 700억(70B) 개의 매개변수를 갖는 차세대 오픈소스 LLM 제품군이다. 이전 버전인 Llama 1에 비해 40% 더 많은 2조 개의 토큰으로 사전 훈련되었으며, 특히 대화형 작업에 최적화된 성능을 보인다. 가장 큰 특징은 학술 연구 목적뿐만 아니라 상업적 용도로도 무료로 사용할 수 있다는 점이다.1 이는 고성능 LLM에 대한 접근성을 획기적으로 확대하여, 자본과 컴퓨팅 자원이 부족한 중소기업, 스타트업, 개인 개발자들도 자체 AI 서비스를 구축할 수 있는 길을 열어주었다.</p>
<h4>2.1.2 생태계에 미친 영향</h4>
<p>Llama 2의 등장은 OpenAI의 GPT 시리즈와 같은 폐쇄형(closed-source) API 기반 모델이 주도하던 시장에 강력한 대안을 제시했다. 개발자들은 모델을 직접 내려받아 특정 도메인에 맞게 미세 조정(fine-tuning)하거나 내부 구조를 수정하는 등 완전한 제어권을 가질 수 있게 되었다. 실제로 Llama 2가 공개된 직후, 오픈소스 커뮤니티는 모델의 기본 컨텍스트 창(context window) 크기를 4,096 토큰에서 32,768 토큰으로 확장하는 등 발 빠른 개선을 통해 집단 지성의 힘을 증명했다.2</p>
<p>이러한 Meta의 결정은 단순한 기술 공개 이상의 의미를 지닌다. 이는 AI 시장의 지배 구조를 재편하려는 정교한 전략적 움직임으로 해석될 수 있다. OpenAI와 Google이 API 사용료를 통해 막대한 수익을 창출하는 폐쇄형 모델 생태계를 구축한 상황에서, 후발주자인 Meta는 비대칭적 전략을 선택했다. 고성능 모델을 상업적으로 이용 가능한 오픈소스로 공개함으로써, 기반 모델(foundation model) 자체의 가치를 희석시키고(commoditization), 경쟁의 축을 모델 소유에서 모델 활용 및 응용 개발로 이동시키려는 의도이다. 이는 API 사용료가 주 수입원인 경쟁사들의 비즈니스 모델을 약화시키는 동시에, Meta의 광고 및 메타버스 사업과 연계된 AI 생태계를 확장하는 데 유리한 환경을 조성한다. 또한, Microsoft Azure와 같은 주요 클라우드 플랫폼과의 파트너십을 통해 Llama 2의 배포를 가속한 것은 1, Microsoft가 OpenAI의 독점적 파트너라는 인식을 희석시키고 자사 중심의 새로운 생태계를 구축하려는 다층적 전략의 일환으로 볼 수 있다.</p>
<h3>2.2  경쟁의 심화: Anthropic Claude 2와 시장 구도</h3>
<p>Meta가 오픈소스 진영에서 파장을 일으키는 동안, 폐쇄형 모델 시장에서도 경쟁은 더욱 치열해졌다. AI 안전성 연구에 중점을 둔 스타트업 Anthropic이 출시한 Claude 2는 LLM 시장의 기술적 다양성을 확보하고 경쟁 구도를 다각화하는 중요한 역할을 했다.</p>
<p>Claude 2는 여러 표준 벤치마크 테스트에서 OpenAI의 GPT-4에 필적하는 우수한 성능을 보였으며, 특히 최대 10만 토큰에 달하는 방대한 양의 컨텍스트를 한 번에 처리하는 능력과 유해한 출력을 생성하지 않도록 설계된 안전성 측면에서 강점을 보였다.1 Claude 2의 등장은 LLM 시장이 특정 기업의 독점 구도가 아닌, 각기 다른 강점을 가진 여러 강력한 플레이어들이 경쟁하는 과점(oligopoly) 형태로 발전하고 있음을 명확히 보여주었다. 출시 직후 Jasper와 같은 AI 카피라이팅 도구에 즉시 통합되는 등 2, 실제 상용 서비스에도 빠르게 적용되며 시장에서의 영향력을 확대했다.</p>
<h3>2.3  AI 안전과 규제: 백악관의 자발적 협약</h3>
<p>AI 기술의 발전 속도가 빨라지고 사회적 영향력이 커짐에 따라, 그 위험성을 통제하고 책임 있는 개발을 보장하기 위한 제도적 장치 마련의 필요성이 대두되었다. 2023년 7월, 미국 백악관은 OpenAI, Google, Meta, Anthropic 등 7개의 주요 AI 기업과 함께 AI 기술의 안전한 개발을 위한 ’자발적 협약(Voluntary Commitments)’을 체결했다.1</p>
<p>이 협약은 다음과 같은 핵심 내용을 담고 있다:</p>
<ul>
<li><strong>출시 전 안전성 테스트:</strong> 모델을 대중에게 공개하기 전에 독립적인 전문가를 통해 사이버 보안, 생화학적 위협, 사회적 편향 등 잠재적 위험을 테스트한다.</li>
<li><strong>정보 공유:</strong> AI 기술의 위험과 한계에 대한 정보를 산업계, 정부, 학계와 투명하게 공유한다.</li>
<li><strong>워터마킹 기술 도입:</strong> AI가 생성한 콘텐츠임을 식별할 수 있는 워터마킹 기술을 개발하고 도입하여, 허위 정보 확산을 방지한다.</li>
</ul>
<p>이러한 움직임은 학계에서 제기된 문제의식과도 맥을 같이 한다. 예를 들어, 7월 arXiv에 발표된 “Frontier AI Regulation: Managing Emerging Risks to Public Safety“라는 논문은 최첨단 ‘프론티어 AI’ 모델이 예기치 않은 위험한 능력을 발현할 수 있으며, 일단 배포되면 오용을 막기 어렵다는 점을 지적했다. 이 논문은 이러한 위험에 대응하기 위해 표준 설정, 모델 등록 및 보고 의무와 같은 체계적인 규제가 필요하다고 주장했다.3</p>
<p>그러나 기업들의 자발적 협약 참여는 순수한 윤리적 동기 외에, 더 강력하고 강제적인 정부 규제를 선제적으로 방어하고 규제의 방향을 자신들에게 유리하게 설정하려는 전략적 행보로도 해석될 수 있다. 유럽연합(EU)의 AI 법안(AI Act)과 같이 전 세계적으로 AI에 대한 법적 규제 움직임이 가시화되는 상황에서 1 기업들은 막대한 컴플라이언스 비용과 혁신 속도 저하를 우려할 수밖에 없다. 따라서 기업들이 먼저 ‘자발적으로’ 안전 장치를 마련하고 있음을 보여줌으로써, “업계가 책임감 있게 행동하고 있으니 정부의 과도한 개입은 불필요하다“는 메시지를 전달하는 것이다. 이는 규제 논의의 주도권을 정부로부터 업계로 가져와, 향후 법적 표준이 자신들의 기술 로드맵과 비즈니스 모델에 더 유리한 방향으로 설정되도록 영향력을 행사하려는 시도일 수 있다. 결국 백악관 협약은 AI 안전을 위한 중요한 진전인 동시에, 규제의 미래를 둘러싼 산업계와 정부 간의 복잡한 힘겨루기가 본격적으로 시작되었음을 알리는 신호탄이라 할 수 있다.</p>
<h2>3.  AI 모델의 내면을 향한 탐구: 주요 학회 발표 심층 분석</h2>
<p>2023년 7월은 ACL과 ICML이라는 두 최상위 AI 학회가 개최되면서, 학문적 연구 성과가 집중적으로 발표된 시기였다. 이들 학회에서는 LLM과 생성 모델의 성능 향상을 넘어, 모델의 내부 작동 방식을 이해하고 그 한계를 명확히 하려는 ’해석 가능성’과 ’지식 탐색’에 대한 깊이 있는 연구들이 주목받았다. 특히 최우수 및 우수 논문으로 선정된 연구들은 해당 분야의 가장 중요한 질문들에 답하며 새로운 연구 방향을 제시했다.</p>
<table><thead><tr><th>학회</th><th>수상 구분</th><th>논문 제목</th><th>저자 및 소속기관</th></tr></thead><tbody>
<tr><td>ACL 2023</td><td>Best Paper</td><td>Do Androids Laugh at Electric Sheep? Humor “Understanding” Benchmarks from The New Yorker Caption Contest</td><td>Jack Hessel (Allen Institute for AI), Ana Marasovic (University of Utah), Jena D. Hwang (Allen Institute for AI), Lillian Lee (Cornell University), Jeff Da (University of Washington), Rowan Zellers (OpenAI), Robert Mankoff (Air Mail and Cartoon Collections), Yejin Choi (Allen Institute for AI, University of Washington)</td></tr>
<tr><td>ACL 2023</td><td>Best Paper</td><td>What the DAAM: Interpreting Stable Diffusion Using Cross Attention</td><td>Raphael Tang (Comcast Applied AI), Linqing Liu (UCL), Akshat Pandey (Comcast Applied AI), Zhiying Jiang (University of Waterloo), Gefei Yang (Comcast Applied AI), Karun Kumar (Comcast Applied AI), Pontus Stenetorp (UCL), Jimmy Lin (University of Waterloo), Ferhan Ture (Comcast Applied AI)</td></tr>
<tr><td>ACL 2023</td><td>Outstanding Paper</td><td>Do PLMs Know and Understand Ontological Knowledge?</td><td>Weiqi Wu, Chengyue Jiang, Yong Jiang, Pengjun Xie, Kewei Tu (ShanghaiTech University, Alibaba DAMO Academy)</td></tr>
<tr><td>ICML 2023</td><td>Outstanding Paper</td><td>A Watermark for Large Language Models</td><td>John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein (University of Maryland)</td></tr>
<tr><td>ICML 2023</td><td>Outstanding Paper</td><td>Learning-Rate-Free Learning by D-Adaptation</td><td>Aaron Defazio (FAIR), Konstantin Mishchenko (Samsung AI Center)</td></tr>
</tbody></table>
<h3>3.1  ACL 2023: 언어 모델의 지식과 추론 능력 탐색</h3>
<p>자연어 처리 분야의 최고 권위 학회인 ACL 2023에서는 언어 모델이 가진 지식의 본질과 한계를 탐구하고, 블랙박스처럼 여겨졌던 생성 모델의 내부를 들여다보려는 시도들이 큰 주목을 받았다.</p>
<h4>3.1.1  What the DAAM: 확산 모델의 어텐션 해석</h4>
<p>ACL 2023 최우수 논문상을 수상한 “What the DAAM: Interpreting Stable Diffusion Using Cross Attention“은 Stable Diffusion과 같은 텍스트-이미지 확산 모델(diffusion model)의 내부 작동 방식을 해석하기 위한 독창적인 방법론인 DAAM(Diffusion Attentive Attribution Maps)을 제안했다.4</p>
<p>핵심 기여 및 방법론 심층 분석</p>
<p>DAAM의 목표는 사용자가 입력한 텍스트 프롬프트의 각 단어가 최종적으로 생성된 이미지의 어떤 픽셀 영역에 영향을 미쳤는지를 시각적으로 보여주는 것이다. 이를 위해 연구팀은 이미지 생성 과정에서 노이즈를 제거하는 U-Net 구조 내의 ‘교차 어텐션(cross-attention)’ 메커니즘에 주목했다. 교차 어텐션은 텍스트 임베딩과 이미지의 잠재 표현(latent representation)을 연결하는 역할을 하는데, 이때 계산되는 어텐션 점수(attention score)가 바로 ’어떤 단어가 어떤 이미지 영역에 주목했는가’에 대한 정보를 담고 있다.</p>
<p>DAAM은 이 어텐션 점수들을 denoising 과정의 모든 시간 단계(t), 모든 어텐션 헤드(l), 그리고 모든 레이어(i)에 걸쳐 추출한 뒤, 이를 공간적으로 업스케일링하고 시간-공간적으로 합산하여 최종적인 단어별 귀속 맵(attribution map) D_k^R을 생성한다. 업스케일링에는 바이큐빅 보간(bicubic interpolation)이 사용되며, 최종 맵은 다음 수식을 통해 계산된다 5:<br />
<span class="math math-display">
D^{R}_{k} [x, y] := \sum_{i, j, l} \tilde{F}^{(i)\downarrow}_{t_j, k, l}[x, y] + \tilde{F}^{(i)\uparrow}_{t_j, k, l}[x, y]
</span><br />
주요 발견과 그 이면의 의미</p>
<p>이 방법론을 통해 저자들은 Stable Diffusion의 중요한 실패 모드를 정량적으로 밝혀냈다. 첫째, ’a giraffe and a zebra’와 같이 의미적으로 유사하지만 구별되는 두 개념(cohyponyms, 공동 하위어)이 프롬프트에 함께 주어질 경우, 모델은 두 개념을 혼동하여 하나의 이미지만 생성하거나 두 개념이 뒤섞인 기이한 이미지를 생성하는 경향을 보였다. DAAM 분석 결과, 이 경우 ’giraffe’와 ’zebra’의 어텐션 맵이 이미지의 동일한 영역에서 높게 나타나, 모델이 두 개념을 공간적으로 분리하지 못했음이 드러났다.5</p>
<p>둘째, ’a red car’에서 ’red’와 같은 기술적 형용사는 단순히 ’car’라는 객체에만 영향을 미치는 것이 아니라, 배경을 포함한 이미지 전체의 분위기나 스타일에 영향을 미치는 ‘피처 얽힘(feature entanglement)’ 현상이 관찰되었다.5</p>
<p>이러한 ‘피처 얽힘’ 현상은 단순한 결함을 넘어, 현재 생성 모델들이 진정한 의미의 ’구성적 일반화(compositional generalization)’를 달성하는 데 근본적인 한계가 있음을 시사한다. 인간은 ’빨간색’이라는 속성과 ’자동차’라는 객체를 독립적인 개념으로 이해하고, 이 둘을 조합하여 ’빨간 자동차’라는 새로운 복합 개념을 즉시 구성할 수 있다. 이것이 바로 구성성(compositionality)의 핵심이다. 그러나 DAAM 분석 결과에 따르면, 모델은 ‘빨간색’ + ’자동차’의 조합으로 이미지를 생성하는 것이 아니라, 훈련 데이터에서 본 수많은 ’빨간색을 띤 사물’과 ’자동차’의 이미지를 통계적으로 뒤섞어 ‘전반적으로 붉은 톤을 띠는 자동차스러운’ 장면을 생성하는 것에 가깝다. 즉, 속성과 객체가 분리되지 않고 얽혀 있는 것이다. 이는 모델이 “파란색 코끼리가 초록색 사과를 들고 있는 모습“과 같이 복잡하고 새로운 조합의 프롬프트를 정확하게 생성하는 데 어려움을 겪는 이유를 설명해준다. 따라서 피처 얽힘은 모델의 제어 가능성과 신뢰성을 저해하는 핵심 장애물이며, 이를 해결하는 것이 차세대 생성 모델 연구의 중요한 과제가 될 것임을 보여준다.</p>
<h4>3.1.2  Do PLMs Know and Understand Ontological Knowledge?: 온톨로지 지식 탐색</h4>
<p>ACL 2023 우수 논문상을 수상한 이 연구는 사전 훈련된 언어 모델(PLM)이 단순히 ’파리의 수도는 프랑스’와 같은 단편적인 사실적 지식을 넘어, 개념, 속성, 그리고 이들 간의 계층 관계 등 구조화된 ’온톨로지 지식(ontological knowledge)’을 얼마나 잘 기억하고(memorizing), 이를 바탕으로 논리적 추론을 수행할 수 있는지(understanding)를 체계적으로 탐색했다.6</p>
<p>실험 설계 및 방법론</p>
<p>연구팀은 온톨로지 지식을 탐색하기 위해 두 가지 주요 과제를 설계했다.</p>
<ol>
<li><strong>기억(Memorization) 과제:</strong> (1) 개체의 유형(예: ‘리오넬 메시는 사람이다’), (2) 클래스 간 계층 관계(예: ‘사람은 동물의 하위 클래스다’), (3) 속성 간 계층 관계, (4) 속성의 정의역 제약(예: ’스포츠팀의 멤버’라는 속성의 주체는 ’사람’이어야 한다), (5) 속성의 치역 제약 등 5가지 유형의 온톨로지적 사실에 대해 모델이 정답을 맞히는지 평가했다.7</li>
<li><strong>이해(Understanding) 과제:</strong> RDF 스키마의 추론 규칙에 기반하여, 주어진 전제로부터 논리적으로 새로운 사실을 추론할 수 있는지 평가했다. 예를 들어, “리오넬 메시는 아르헨티나 축구팀의 멤버이다“라는 사실과 “스포츠팀의 멤버라는 속성의 주체는 사람이다“라는 제약 조건이 주어졌을 때, “리오넬 메시는 사람이다“라고 추론할 수 있는지를 테스트했다.7</li>
</ol>
<p>성능 비교 분석</p>
<p>실험 결과, BERT와 RoBERTa와 같은 인코더 전용 모델들은 어느 정도 온톨로지 지식을 내부에 인코딩하고 있었으나, 그 정확도는 제한적이었으며 특히 추론 능력에서 한계를 보였다.6 반면, 디코더 전용 모델인 ChatGPT는 기억과 추론 두 측면 모두에서 주목할 만한 성능 향상을 보였지만, 여전히 완벽과는 거리가 멀었다.6</p>
<p>특히 흥미로운 발견은 모델들이 추론에 필요한 전제가 프롬프트에 명시적으로 주어진 경우(Explicitly Given)보다, 모델이 사전 훈련 과정에서 내재적으로 학습한 지식을 활용해야 하는 경우(Implicitly Given)에 성능이 크게 저하되었다는 점이다.7 이는 모델이 지식을 인간처럼 구조화된 논리 체계로 저장하고 활용하는 것이 아니라, 텍스트의 표면적 패턴과 통계적 연관성에 크게 의존하고 있음을 시사한다.</p>
<p>이러한 결과는 LLM의 ’이해’라는 개념에 대해 근본적인 질문을 던진다. ChatGPT가 BERT/RoBERTa보다 온톨로지 추론을 더 잘하는 것이 진정한 논리적 이해 능력이 향상되었다기보다는, 방대한 대화형 데이터와 인간 피드백을 통한 강화학습(RLHF)을 통해 ‘논리적으로 보이는’ 텍스트 패턴을 더 정교하게 모방하게 된 결과일 수 있다. 즉, “A는 B의 하위 클래스이고, X가 A의 인스턴스이면, X는 B의 인스턴스이다“와 같은 논리적 설명 패턴 자체를 통계적으로 학습했을 가능성이 높다. 이는 모델이 ’추론하는 척’을 더 잘하게 된 것일 수 있으며, 한 번도 보지 못한 복잡한 논리적 연쇄나 역설적 상황에 직면했을 때는 여전히 취약할 수 있음을 의미한다. 결론적으로, 이 연구는 현재의 LLM이 진정한 의미의 기호적 추론(symbolic reasoning)을 수행하기보다는, 거대한 통계적 상관관계 데이터베이스에 가깝다는 가설을 강력하게 지지한다.</p>
<h3>3.2  ICML 2023: 머신러닝의 근본적 문제 해결</h3>
<p>머신러닝 분야의 최고 권위 학회인 ICML 2023에서는 AI 기술의 신뢰성과 효율성에 관한 근본적인 문제들을 해결하려는 연구들이 우수 논문으로 선정되었다.</p>
<h4>3.2.1  A Watermark for Large Language Models: AI 생성물 식별</h4>
<p>ICML 2023 우수 논문상을 수상한 이 연구는 LLM이 생성한 텍스트에 인간은 인지할 수 없지만 알고리즘적으로는 탐지 가능한 ’워터마크’를 삽입하는 정교한 프레임워크를 제안했다.9 이는 AI가 생성한 허위 정보, 스팸, 학술적 표절 등을 식별하는 데 중요한 기술적 기반을 제공한다.</p>
<p>워터마크 임베딩 및 탐지 메커니즘</p>
<p>이 방법론의 핵심은 텍스트 생성 과정에 미세한 통계적 편향을 주입하는 것이다.</p>
<ul>
<li><strong>임베딩:</strong> 텍스트를 생성할 때, 바로 이전 토큰을 시드(seed)로 사용하여 해시 함수를 통해 전체 어휘(vocabulary)의 일부(예: 50%)를 무작위로 ’그린 리스트(green list)’로 지정한다. 그리고 다음 토큰을 샘플링하기 직전, 이 그린 리스트에 속한 토큰들의 로짓(logit) 값에 작은 양수 \delta를 더해준다. 이 과정은 그린 리스트 토큰이 선택될 확률을 미세하게 높이지만, 텍스트의 전반적인 품질에는 거의 영향을 미치지 않는다.10</li>
<li><strong>탐지:</strong> 탐지하고자 하는 텍스트가 주어지면, 각 토큰에 대해 동일한 해시 함수를 적용하여 해당 토큰이 생성될 당시에 그린 리스트에 속했는지 여부를 확인한다. 그 후, 텍스트 전체에서 그린 리스트에 속한 토큰의 개수를 센다. 워터마크가 없는 자연어 텍스트라면 이 개수는 특정 비율(예: 50%)을 중심으로 이항분포를 따를 것이다. 관찰된 그린 토큰의 수가 이 기대치에서 통계적으로 유의미하게 벗어나는지를 검증하기 위해 일표본 Z-검정(one-proportion z-test)을 사용한다. Z-점수는 다음과 같이 계산된다 10:</li>
</ul>
<p><span class="math math-display">
z = \frac{|s|_G - \gamma T}{\sqrt{\gamma(1-\gamma)T}}
</span></p>
<p>여기서 <span class="math math-inline">|s|_G</span>는 관찰된 그린 토큰의 수, <span class="math math-inline">T</span>는 전체 토큰 수, <span class="math math-inline">\gamma</span>는 그린 리스트의 비율(예: 0.5)이다. Z-점수가 특정 임계값(예: 4.0)을 초과하면, 해당 텍스트가 워터마크가 삽입된, 즉 AI에 의해 생성되었을 확률이 매우 높다고 판단할 수 있다. 이 Z-점수는 해석 가능한 p-value로 변환되어 통계적 신뢰도를 제공한다.10</p>
<p>이 워터마킹 기술의 발표는 AI 생성 콘텐츠의 출처를 둘러싼 ’공격’과 ‘방어’ 기술의 새로운 군비 경쟁의 시작을 의미한다. 이 논문은 AI의 오용을 막기 위한 강력한 ‘방어’ 기술을 제시했지만, 기술이 공개됨에 따라 이를 무력화하려는 ‘공격’ 기술의 등장은 필연적이다. 예를 들어, 텍스트의 의미는 유지하면서 표현을 미세하게 바꾸는 재작성(paraphrasing)을 통해 워터마크가 남긴 통계적 편향을 제거하려는 시도가 있을 수 있다.11 이러한 공격이 성공하면, 더 정교하고 강건한(robust) 워터마킹 기술이 필요해지고, 이는 다시 더 발전된 공격 기술을 유발하는 순환적 군비 경쟁으로 이어질 것이다. 따라서 이 논문은 단순히 하나의 기술을 제안한 것을 넘어, AI 안전성 분야에서 ’탐지 및 회피(detection and evasion)’라는 새로운 연구 및 산업 영역을 본격적으로 열었다고 평가할 수 있다.</p>
<h2>4.  로보틱스의 패러다임 전환: 언어, 모방, 그리고 최적 제어</h2>
<p>2023년 7월 로보틱스 분야에서는 이론과 실제 응용을 아우르는 획기적인 연구들이 발표되었다. 특히 최고 권위의 로보틱스 학회인 RSS(Robotics: Science and Systems)에서는 로봇의 탐색 및 학습 능력의 한계를 확장하는 연구들이 최우수 논문으로 선정되며, 로봇 지능의 새로운 패러다임을 제시했다. 또한, Google DeepMind는 LLM의 지식을 물리적 세계로 이전하는 혁신적인 모델을 공개하며 학계와 산업계에 큰 반향을 일으켰다.</p>
<table><thead><tr><th>학회</th><th>수상 구분</th><th>논문 제목</th><th>저자 및 소속기관</th></tr></thead><tbody>
<tr><td>RSS 2023</td><td>Best Paper</td><td>Time Optimal Ergodic Search</td><td>Dayi E Dong, Henry P Berger, Ian Abraham (Yale University)</td></tr>
<tr><td>RSS 2023</td><td>Best Student Paper</td><td>Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations</td><td>Siddhant Haldar, Jyothish Pari, Anant Rai, Lerrel Pinto (New York University)</td></tr>
</tbody></table>
<h3>4.1  RSS 2023: 로봇 지능의 한계 확장</h3>
<p>RSS 2023에서는 로봇의 자율적 행동 능력을 근본적으로 향상시키는 두 가지 상반된 접근법이 주목받았다. 하나는 수학적으로 엄밀한 최적 제어 이론에 기반한 탐색 방법론이고, 다른 하나는 소량의 인간 시연 데이터로부터 효율적으로 기술을 학습하는 데이터 기반 모방 학습 방법론이다.</p>
<h4>4.1.1  Time Optimal Ergodic Search: 시간 최적 탐색 문제</h4>
<p>RSS 2023 최우수 논문상을 수상한 “Time Optimal Ergodic Search“는 로봇 탐색 분야의 패러다임을 전환하는 중요한 이론적 진전을 이루었다.12 기존의 에르고딕 탐색(ergodic search) 연구가 ’고정된 시간 내에 얼마나 효과적으로 탐색하는가’에 초점을 맞춘 반면, 이 연구는 ’주어진 탐색 품질을 만족하면서 얼마나 빨리 임무를 완료할 수 있는가’라는 새로운 문제를 ’시간 최적 에르고딕 탐색’으로 공식화했다.13</p>
<p>수학적 공식화 및 해법</p>
<p>이 문제는 목적 함수가 총 비행 시간 <span class="math math-inline">t_f</span>를 최소화하는 최적 제어 문제로 정의된다:<br />
<span class="math math-display">
\min_{u(t), t_f} t_f
</span><br />
이때 로봇은 자신의 동역학 <span class="math math-inline">\dot{x}(t) = f(x(t), u(t))</span>를 따라 움직여야 하며, 동시에 탐색의 품질을 나타내는 에르고딕 메트릭 <span class="math math-inline">E(x(t), \phi)</span>가 사전에 정의된 임계값 <span class="math math-inline">\epsilon</span> 이하가 되어야 한다는 부등식 제약조건 <span class="math math-inline">E(x(t), \phi) \le \epsilon</span>을 만족해야 한다.13 에르고딕 메트릭은 로봇 궤적의 시간 평균 통계(즉, 각 영역에 머문 시간의 분포)와 목표 정보 분포(즉, 탐색해야 할 중요도의 분포) 간의 차이를 푸리에 공간에서 측정하는 척도이다.13</p>
<p>연구팀은 이 문제에 대해 Pontryagin의 최적성 원리를 적용하여 최적해의 필요조건을 분석적으로 유도했으며, 수치적으로는 직접 전사(direct transcription) 최적화 기법을 사용하여 실제 로봇이 따라갈 수 있는 궤적을 계산했다.13</p>
<p>이 연구는 로봇 탐색 분야의 패러다임을 ’가능한 경로 찾기(path feasibility)’에서 ’보장된 성능 하에 최적 경로 찾기(performance-guaranteed optimality)’로 한 단계 끌어올렸다. 전통적인 로봇 경로 계획은 “장애물을 피해 목표 지점까지 가는 경로를 찾아라“와 같이 경로의 존재 여부나 기하학적 제약 충족에 중점을 두었다. 이후 등장한 에르고딕 탐색은 “정보가 많은 곳에서 더 많은 시간을 보내라“는 확률 분포 기반의 커버리지 개념을 도입하여 탐색의 ’질’을 정량화했다. 이 논문은 여기에 ’시간’이라는 최적화 변수를 추가하여, “탐색의 질(\epsilon)을 보장하면서 가장 빨리 끝내는 방법은 무엇인가?“라는 한 차원 높은 질문에 답할 수 있는 프레임워크를 제공했다. 이는 수색 및 구조 13와 같이 시간이 절대적으로 중요한 실제 응용에서 로봇이 단순히 임무를 수행하는 것을 넘어, ‘가장 효율적으로’ 수행할 수 있게 만드는 핵심적인 이론적 토대를 마련한 것이다.</p>
<h4>4.1.2  Teach a Robot to FISH: 1분 시연으로 다재다능한 모방 학습</h4>
<p>RSS 2023 최우수 학생 논문상을 수상한 “Teach a Robot to FISH“는 데이터 효율성과 강건성이라는 두 마리 토끼를 잡은 새로운 모방 학습 프레임워크 FISH(Fast Imitation of Skills from Humans)를 제안했다.15 이 연구의 핵심 기여는 단 1분 분량의 소수(1~3개) 전문가 시연 데이터만으로도, 이전에 보지 못한 새로운 환경 변화에 강건하게 대처할 수 있는 시각 기반 로봇 기술을 학습할 수 있다는 점이다.16</p>
<p>2단계 학습 과정과 Optimal Transport 기반 보상</p>
<p>FISH의 혁신은 모방 학습(Imitation Learning, IL)과 강화 학습(Reinforcement Learning, RL)의 장점을 결합한 2단계 학습 과정에 있다.</p>
<ol>
<li><strong>오프라인 학습:</strong> 먼저, 소수의 시연 데이터를 이용해 행동 복제(behavioral cloning) 방식으로 부정확하지만 유용한 초기 ’기본 정책(base policy)’을 학습한다.18</li>
<li><strong>온라인 적응:</strong> 그 후, 실제 환경에서 로봇을 실행하며 기본 정책의 출력에 미세 조정을 더하는 ’잔여 정책(residual policy)’을 강화학습으로 훈련한다. 이때 강화학습에 필수적인 보상 신호(reward signal)를 인간이 수동으로 설계하는 대신, ’Optimal Transport(OT)’라는 수학적 도구를 이용해 자동으로 생성한다.18</li>
</ol>
<p>OT 기반 보상 계산은 로봇이 현재 실행 중인 궤적의 시각적 관찰 시퀀스(일련의 카메라 이미지)와 전문가 시연 궤적의 시각적 관찰 시퀀스 간의 ’유사도’를 측정하는 방식으로 작동한다. 두 시퀀스가 시각적으로 더 유사할수록 높은 보상을 부여하여, 로봇이 점차 전문가의 행동을 모방하도록 유도한다. 이 방식은 별도의 상태 추정이나 보상 함수 설계 없이 순수 시각 정보만으로 학습을 가능하게 한다.16</p>
<p>FISH의 핵심 혁신은 IL과 RL의 고질적인 단점을 서로 보완하는 연결고리를 ’자동 보상 생성’이라는 메커니즘으로 구현한 데 있다. IL은 데이터 효율적이지만 시연 데이터에 없는 상황에 대한 일반화 성능이 낮다는 ‘분포 이동(distributional shift)’ 문제가 있다. 반면, RL은 온라인 적응을 통해 일반화 성능을 높일 수 있지만, 모든 작업에 대해 의미 있는 보상 함수를 수동으로 설계하는 것이 매우 어렵다는 ‘보상 공학(reward engineering)’ 문제가 있다. FISH는 IL로 학습한 불안정한 정책을 시작점으로 삼고, RL로 온라인 적응을 하되, 필요한 보상을 ’시연과의 시각적 유사도’라는 직관적인 기준으로 자동 생성함으로써 이 두 문제를 동시에 해결한다. 이는 ‘데이터는 적게 쓰면서(IL의 장점), 새로운 상황에 적응하고(RL의 장점), 보상 설계는 자동으로(OT의 기여)’ 하는 효과적인 접근법으로, 로봇 학습의 실용성을 한 단계 끌어올린 중요한 방법론적 진전이다.</p>
<p>다재다능성 입증</p>
<p>이 방법론의 효과는 다양한 로봇 플랫폼과 작업에서 입증되었다. 연구팀은 xArm(로봇 팔), Allegro Hand(다지 로봇 핸드), Stretch(이동 로봇) 등 서로 다른 형태, 동역학, 카메라 구성을 가진 3개의 로봇 플랫폼과 9개의 다양한 작업(문 열기, 카드 밀기, 베이글 뒤집기 등)에서 FISH를 평가했다. 그 결과, 단 1~3개의 시연과 20분 이내의 온라인 학습만으로 평균 93%의 높은 성공률을 달성하며 그 다재다능성을 입증했다.16</p>
<h3>4.2  웹 지식의 물리적 구현: RT-2 모델 심층 분석</h3>
<p>2023년 7월 말, Google DeepMind는 웹 스케일의 방대한 시각-언어 지식을 로봇 제어에 직접 접목하는 새로운 패러다임인 VLA(Vision-Language-Action) 모델, RT-2를 발표하며 로보틱스 커뮤니티에 큰 충격을 주었다.19</p>
<p>핵심 개념 및 방법론: 행동의 언어화(Action as Language)</p>
<p>RT-2의 핵심 아이디어는 로봇의 연속적인 행동(예: 7차원 벡터로 표현되는 로봇 팔의 위치, 회전, 그리퍼 상태)을 이산적인 텍스트 토큰으로 변환하는 것이다. 예를 들어, ``과 같은 행동 벡터를 하나의 텍스트 문자열처럼 취급한다. 이 간단하지만 강력한 아이디어를 통해, 로봇의 학습 데이터 형식인 (이미지, 언어명령) -&gt; (행동)을 VLM의 표준적인 학습 프레임워크인 (이미지, 텍스트) -&gt; (텍스트)에 완벽하게 통합할 수 있다.19 즉, 로봇 제어 문제를 언어 모델이 가장 잘하는 ‘다음 토큰 예측’ 문제로 치환한 것이다.</p>
<p>결과: 발현적 능력(Emergent Capabilities)</p>
<p>이 접근법을 통해 RT-2는 로봇 훈련 데이터에는 전혀 등장하지 않았던 추상적인 개념을 이해하고 그에 맞춰 행동하는 놀라운 ’발현적 능력’을 보여주었다.</p>
<ul>
<li><strong>의미론적 이해:</strong> “멸종된 동물을 집어라“라는 명령에, 훈련 데이터에 없었던 공룡 장난감을 정확히 인식하고 집었다.19</li>
<li><strong>기호적 추론:</strong> “2 더하기 1의 합계가 있는 곳으로 바나나를 옮겨라“라는 명령에, 숫자 ‘3’ 아이콘 위로 바나나를 옮겼다.20</li>
<li><strong>상식 추론:</strong> “피곤한 사람에게 줄 음료를 가져와라“라는 명령에, 여러 음료수 중 에너지 드링크를 선택했다.19</li>
</ul>
<p>이러한 결과는 웹 데이터로부터 학습한 방대한 시각-의미 지식이 별도의 변환 과정 없이 로봇 행동으로 직접 이전될 수 있음을 보여준다. RT-2의 진정한 의의는 성능 향상을 넘어, 로봇 공학의 근본적인 문제였던 ’기호적 추상화(symbolic abstraction)’와 ‘저수준 제어(low-level control)’ 사이의 간극을 언어 모델을 통해 메우는 새로운 패러다임을 제시했다는 점이다. 전통적으로 로봇 시스템은 “무엇을 할지“를 기호적으로 계획하는 상위 레벨의 ’작업 계획기(Task Planner)’와 “어떻게 움직일지“를 연속적인 값으로 계산하는 하위 레벨의 ’동작 계획기/제어기(Motion Planner/Controller)’라는 별개의 모듈로 구성되며, 이 둘을 연결하는 것은 매우 어려운 문제였다. RT-2는 이 두 단계를 하나의 엔드투엔드 모델로 통합한다. 모델은 이미지와 명령을 보고 “사과를 집어야겠다“는 내부적 추론과 “그리퍼를 x,y,z 좌표로 움직여야겠다“는 저수준 제어 명령을 ’행동 토큰’의 형태로 동시에 생성한다. ‘행동을 언어처럼’ 다루는 이 아이디어는 로봇 제어를 ’물리 법칙의 문제’에서 ’시퀀스 변환의 문제’로 재정의하며, 진정한 의미의 ’범용 로봇(Generalist Robot)’을 향한 가장 유망한 경로 중 하나를 제시한 기념비적인 연구이다.</p>
<h2>5.  AI 연구의 최전선: 다중모달리티와 응용</h2>
<p>2023년 7월에는 LLM과 로보틱스 외에도, 다양한 데이터 양식을 통합적으로 처리하는 다중모달 AI와 특정 응용 분야에서 AI의 활용도를 높이는 기술들이 주목받았다. 특히 의료 분야에서의 범용 AI 개발과 이미지 생성 AI의 상호작용성 강화가 두드러졌다.</p>
<h3>5.1  범용 생의학 AI를 향하여: Med-PaLM M</h3>
<p>Google 연구팀은 7월 arXiv를 통해 발표한 “Towards Generalist Biomedical AI” 논문에서 단 하나의 모델로 다양한 형태의 생의학 데이터를 통합적으로 처리하고 해석할 수 있는 대규모 다중모달 생성 모델, Med-PaLM Multimodal(Med-PaLM M)을 제안했다.22</p>
<p>핵심 기여 및 성능</p>
<p>이 연구의 핵심은 ’전문화(specialization)’가 아닌 ’일반화(generalization)’에 있다. 기존의 의료 AI가 특정 질병 진단이나 특정 데이터(예: X-ray) 분석에 특화된 개별 모델들을 개발하는 데 초점을 맞췄다면, Med-PaLM M은 텍스트(임상 노트), 의료 영상(유방촬영술, 피부과 이미지, X-ray 등), 그리고 유전체학 데이터까지 아우르는 다양한 양식을 단 하나의 범용 모델(generalist model)로 처리한다.</p>
<p>연구팀은 모델의 성능을 평가하기 위해 의료 질의응답, 유방촬영술 및 피부과 이미지 해석, 방사선 보고서 생성 및 요약, 유전체 변이 호출 등 14개의 다양한 생의학 태스크를 포함하는 새로운 벤치마크 ’MultiMedBench’를 구축했다. Med-PaLM M은 이 모든 태스크에서 기존의 전문가 모델(specialist models)과 경쟁하거나 이를 능가하는 성능을 보였다. 특히, 246개의 후향적 흉부 X-ray 보고서를 평가한 실험에서, 방사선 전문의들은 최대 40.5%의 경우에 Med-PaLM M이 생성한 보고서를 인간 방사선 전문의가 작성한 보고서보다 선호한다고 평가하여 잠재적인 임상적 유용성을 시사했다.22</p>
<p>이 연구는 특정 작업에 특화된 여러 개의 AI 모델을 개발하는 대신, 다양한 의료 데이터를 통합적으로 이해하고 추론할 수 있는 단일 ’범용 생의학 AI’의 개발 가능성을 입증한 중요한 이정표이다.</p>
<h3>5.2  기타 주요 기술 발전 동향</h3>
<p>7월에는 사용자의 창의성을 지원하고 AI의 활용 범위를 넓히는 다양한 응용 기술들도 발표되었다.2</p>
<ul>
<li><strong>OpenAI Code Interpreter:</strong> ChatGPT 플러그인으로 제공된 Code Interpreter는 사용자가 자연어 명령을 통해 데이터를 업로드하고, 이를 분석, 정제, 시각화하며, 예측 모델링까지 수행할 수 있는 강력한 도구이다. 이는 데이터 과학자나 개발자가 아닌 일반 사용자도 복잡한 데이터 작업을 수행할 수 있게 하여 AI의 접근성을 한 단계 높였다.</li>
<li><strong>Stability.ai Stable Doodle &amp; Midjourney Panning:</strong> 이미지 생성 AI 분야에서는 사용자의 제어 가능성과 상호작용성을 높이는 기술들이 등장했다. Stability.ai의 Stable Doodle은 사용자가 그린 간단한 스케치를 이미지 생성 프롬프트의 일부로 사용하여, 사용자의 의도를 더 정확하게 반영하는 이미지를 만들 수 있게 했다. 한편, Midjourney는 ‘패닝(Panning)’ 기능을 도입하여, 생성된 이미지의 상하좌우 경계를 자연스럽게 확장하며 더 넓은 장면을 창조하거나 구도를 변경할 수 있는 새로운 창작 방식을 제공했다.</li>
<li><strong>Meta CM3Leon:</strong> Meta가 발표한 CM3Leon(카멜레온)은 텍스트에서 이미지로, 그리고 이미지에서 텍스트로의 양방향 생성이 모두 가능한 다재다능한 이미지 생성 모델이다. 특히, 기존 이미지의 특정 영역을 지정하여 “이 부분에 선글라스를 씌워줘“와 같은 명령으로 수정하는 ‘인페인팅(inpainting)’ 및 ‘편집(editing)’ 기능에서 높은 성능을 보여, 이미지 생성 AI가 단순 생성을 넘어 창의적인 편집 도구로 발전할 수 있는 가능성을 제시했다.</li>
</ul>
<h2>6. 결론: 통합과 지능의 심화</h2>
<p>2023년 7월의 AI 및 로봇 분야 연구 동향은 두 가지 핵심적인 흐름, 즉 ’통합(Integration)’과 ’심화(Deepening)’로 요약할 수 있다. 이 두 흐름은 상호작용하며 기술 발전의 새로운 국면을 열고 있다.</p>
<p>첫째, **‘통합’**의 흐름은 AI 기술이 개별적인 도구를 넘어, 더 크고 복잡한 시스템의 일부로 융합되고 있음을 보여준다. Meta의 Llama 2와 같은 고성능 오픈소스 모델의 확산은 다양한 상용 서비스와 AI 기술의 통합을 가속화하는 산업적 기반을 마련했다. 학문적으로는 이러한 통합이 더욱 근본적인 차원에서 이루어졌다. Google DeepMind의 RT-2는 웹 스케일의 방대한 언어 및 시각 지식을 로봇의 물리적 행동과 통합하여, 추상적 지능과 구체적 제어 사이의 오랜 간극을 메우는 새로운 패러다임을 제시했다. 마찬가지로, Med-PaLM M은 텍스트, 영상, 유전체학 등 파편화되어 있던 생의학 데이터를 단일 모델로 통합하여, 기존의 전문화된 AI의 한계를 뛰어넘는 범용 의료 지능의 가능성을 보여주었다.</p>
<p>둘째, **‘심화’**의 흐름은 AI 연구가 단순히 성능 지표를 높이는 경쟁을 넘어, 모델의 본질을 이해하려는 방향으로 나아가고 있음을 의미한다. ACL과 ICML에서 발표된 주요 연구들은 AI 모델이 무엇을 할 수 있는지(capability)를 넘어, 그것이 어떻게 작동하고(interpretability), 무엇을 알고 있으며(knowledge probing), 어떻게 그 출력을 신뢰할 수 있는지(safety &amp; provenance)에 대한 근본적인 질문에 답하려 했다. DAAM을 통한 확산 모델의 내부 시각화, PLM의 온톨로지 지식 보유 여부 탐색, LLM 출력물에 대한 워터마킹 기술 개발 등은 모두 이러한 연구의 심화를 보여주는 대표적인 사례이다.</p>
<p>이러한 통합과 심화의 흐름은 AI 연구가 더 이상 더 큰 모델을 만드는 규모의 경쟁에만 머무르지 않고, 모델이 가진 지능을 실제 세계와 안전하고 효과적으로 연결하며, 그 내부 작동 원리를 깊이 있게 이해하고 제어하는 방향으로 성숙하고 있음을 시사한다. 향후 연구는 VLA 모델의 강건성과 데이터 효율성을 확보하는 문제, LLM의 통계적 패턴 매칭을 넘어 진정한 논리적 추론 능력을 부여하는 문제, 그리고 AI가 생성하는 정보의 신뢰성을 보장하기 위한 기술적, 제도적 장치를 마련하는 문제에 더욱 집중될 것으로 전망된다. 2023년 7월은 이러한 미래를 향한 중요한 발걸음이 내디뎌진 시기로 기억될 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>2023년 7월 AI 뉴스 동향 총정리 - 알쓸AI잡 - 티스토리, https://ai-mentor.tistory.com/m/9</li>
<li>The month in AI marketing news round-up: July 2023 edition - BioStrata, https://www.biostratamarketing.com/blog/the-month-in-ai-marketing-news-round-up-july-edition</li>
<li>[2307.03718] Frontier AI Regulation: Managing Emerging Risks to Public Safety - arXiv, https://arxiv.org/abs/2307.03718</li>
<li>Best Papers - ACL 2023, https://2023.aclweb.org/program/best_papers/</li>
<li>What the DAAM: Interpreting Stable Diffusion Using … - ACL Anthology, https://aclanthology.org/2023.acl-long.310.pdf</li>
<li>SIST paper wins ACL 2023 Outstanding Paper Award, https://www.shanghaitech.edu.cn/eng/2023/1017/c1685a1083748/page.htm</li>
<li>Do PLMs Know and Understand Ontological … - ACL Anthology, https://aclanthology.org/2023.acl-long.173.pdf</li>
<li>Do PLMs Know And Understand Ontological Knowledge?, https://faculty.sist.shanghaitech.edu.cn/faculty/tukw/acl23probe-poster.pdf</li>
<li>2023 Awards - ICML 2025, https://icml.cc/Conferences/2023/Awards</li>
<li>A Watermark for Large Language Models - Proceedings of Machine …, https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf</li>
<li>Optimizing Adaptive Attacks against Watermarks for Language Models - ICML 2025, https://icml.cc/virtual/2025/poster/46148</li>
<li>Best Paper Award - RSS Foundation, https://roboticsfoundation.org/awards/best-paper-award/</li>
<li>Time Optimal Ergodic Search - Robotics, https://www.roboticsproceedings.org/rss19/p082.pdf</li>
<li>(PDF) Time Optimal Ergodic Search - ResearchGate, https://www.researchgate.net/publication/370938234_Time_Optimal_Ergodic_Search</li>
<li>Best Student Paper Award - RSS Foundation, https://roboticsfoundation.org/awards/best-student-paper-award/</li>
<li>Teach a Robot to FISH: Versatile Imitation from One … - Robotics, https://www.roboticsproceedings.org/rss19/p009.pdf</li>
<li>Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations - arXiv, https://arxiv.org/abs/2303.01497</li>
<li>(PDF) Teach a Robot to FISH: Versatile Imitation from One Minute of …, https://www.researchgate.net/publication/372806690_Teach_a_Robot_to_FISH_Versatile_Imitation_from_One_Minute_of_Demonstrations</li>
<li>RT-2: Vision-Language-Action Models Transfer Web … - arXiv, https://arxiv.org/abs/2307.15818</li>
<li>RT-2: New model translates vision and language into action - Google DeepMind, https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/</li>
<li>Vision-Language-Action Models: RT-2, https://robotics-transformer2.github.io/</li>
<li>Towards Generalist Biomedical AI - arXiv, https://arxiv.org/abs/2307.14334</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>