<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2023년 5월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2023년 5월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2023년 AI 및 로봇 연구 동향</a> / <span>2023년 5월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2023년 5월 AI 및 로봇 연구 동향</h1>
<h2>1. 서론: 2023년 5월 AI 연구 지형 개관</h2>
<p>2023년 5월은 인공지능(AI) 및 로봇 공학 분야에서 중요한 변곡점으로 기록될 만한 시기였다. 이 기간에 발표된 연구들은 추상적인 콘텐츠 생성에 집중하던 생성형 AI의 막강한 능력이 물리적 세계와 상호작용하는 로봇 공학 및 체화된 지능(embodied intelligence)의 영역과 본격적으로 융합되기 시작했음을 명확히 보여주었다. 본 보고서는 이 시기의 연구 지형을 두 가지 지배적인 패러다임, 즉 2D 이미지를 넘어 다차원적 세계를 창조하는 생성형 모델의 폭발적인 발전과, 물리적 세계를 인식하고 행동하도록 설계된 자율 시스템의 고도화를 중심으로 심층 분석한다.1</p>
<p>이러한 기술적 혁신은 주요 학술 대회를 통해 세상에 알려졌다. 5월 초에 개최된 학습 표현에 관한 국제 학회(ICLR, International Conference on Learning Representations)와 5월 말에 시작된 로봇 공학 및 자동화에 관한 국제 학회(ICRA, International Conference on Robotics and Automation)는 이 시기의 혁신을 주도한 핵심적인 장이었다.3 또한, 6월에 예정된 컴퓨터 비전 및 패턴 인식 학회(CVPR, Computer Vision and Pattern Recognition)에 채택되어 사전 공개된 영향력 있는 논문들 역시 5월의 기술 담론을 형성하는 데 중요한 역할을 했다.5</p>
<p>학계의 이러한 발전은 고립된 현상이 아니었다. 전 세계적인 노동력 부족, 지정학적 불확실성에 따른 공급망 회복탄력성 확보를 위한 리쇼어링(reshoring) 추세, 그리고 에너지 비용 상승에 따른 효율성 증대 요구 등 시급한 산업계의 현안들이 연구실에서 탄생하는 신기술에 대한 강력한 수요를 창출하고 있었다.6 이는 본 보고서에서 다루는 연구들이 단순한 학문적 탐구를 넘어, 상업적 및 사회적으로 지대한 중요성을 지니고 있음을 시사한다.</p>
<p>이 시기 연구의 정수를 파악하기 위해, ICLR 2023에서 최우수 논문상(Outstanding Paper Award)을 수상한 연구들을 먼저 살펴볼 필요가 있다. 이 논문들은 해당 월의 가장 중요한 연구 방향을 압축적으로 보여주는 지표 역할을 한다. 아래 표는 ICLR 2023의 수상 논문들을 요약한 것으로, 이들이 다루는 주제—소수샷 학습, 그래프 신경망 이론, 텍스트-3D 변환, 그리고 자율 항법—는 본 보고서 전체를 관통하는 핵심적인 기술 흐름을 예고한다.9</p>
<p>2023년 5월의 주요 연구 주제들은 개별적으로 존재하지 않고, 디지털 지능과 물리적 응용 사이의 강력한 피드백 루프를 형성하고 있었다. 생성형 AI는 로봇 공학을 위한 새로운 시뮬레이션 및 설계 도구를 제공하고, 반대로 로봇 공학의 난제들(예: 정교한 조작, 복잡한 환경에서의 항법)은 AI 모델의 성능을 평가하고 발전시키는 새로운 기준점이 되었다. 예를 들어, 리쇼어링과 같은 산업계의 동향은 더 유연하고 신속하게 배치할 수 있는 자동화 시스템을 요구하는데, 이는 적은 데이터로 새로운 작업을 학습하는 소수샷 학습(few-shot learning)이나 복잡한 공간을 탐색하는 체화된 AI 연구와 직접적으로 연결된다. <em>DreamFusion</em>과 같은 생성형 모델은 물리적 세계를 시뮬레이션하고 설계하는 도구를 창출하며, ICRA에서 발표된 로봇 공학 연구나 도요타의 발표와 같은 산업계의 움직임은 AI 기반 정책을 현실 세계에 적용하는 과정을 보여준다.12 이는 디지털 창조가 물리적 행동에 정보를 제공하고, 물리적 세계의 도전 과제가 다시 디지털 혁신을 촉진하는 선순환 구조를 형성하고 있음을 의미한다.</p>
<table><thead><tr><th>수상 유형</th><th>논문 제목</th><th>주요 기여 내용</th><th>출처</th></tr></thead><tbody>
<tr><td>최우수 논문</td><td>Universal Few-shot Learning of Dense Prediction Tasks with Visual Token Matching</td><td>조밀 예측 작업(dense prediction task)을 위한 소수샷 학습에 시각적 토큰 매칭(VTM)이라는 통일된 접근법을 제안하여, 최소한의 레이블링된 데이터로 모델이 새로운 작업을 학습할 수 있게 함.</td><td>9</td></tr>
<tr><td>최우수 논문</td><td>Rethinking the Expressive Power of GNNs via Graph Biconnectivity</td><td>표준 그래프 신경망(GNN)이 그래프 이중연결성(biconnectivity)을 포착하는 데 가지는 한계를 규명하고, 이를 극복하기 위한 더 강력한 아키텍처(Graphormer-GD)를 제안함.</td><td>9</td></tr>
<tr><td>최우수 논문</td><td>DreamFusion: Text-to-3D using 2D Diffusion</td><td>사전 훈련된 2D 확산 모델을 사용하여 3D 훈련 데이터 없이 텍스트 프롬프트로부터 고품질 3D 모델을 생성하는 점수 증류 샘플링(SDS) 기법을 도입함.</td><td>9</td></tr>
<tr><td>최우수 논문</td><td>Emergence of Maps in the Memories of Blind Navigation Agents</td><td>명시적인 매핑 모듈이 없는 AI 에이전트가 항법을 위해 내부적으로 공간 지도를 암묵적으로 구축하고 활용함을 입증하여, 지도 없는 항법에 대한 기존 가설에 도전함.</td><td>9</td></tr>
</tbody></table>
<h2>2. 생성형 AI의 패러다임 전환: 2D를 넘어 3D 세계의 창조로</h2>
<p>2023년 5월은 생성형 AI 기술이 2차원 이미지 합성을 넘어, 제어 가능하고 다차원적인 복합 콘텐츠를 창조하는 단계로 성숙했음을 알리는 중요한 시점이었다. 이 시기를 정의하는 세 가지 핵심 연구인 <em>DreamFusion</em>, <em>DreamBooth</em>, 그리고 <em>InstructPix2Pix</em>는 단순한 품질 향상을 넘어 생성형 AI의 제어 가능성과 응용 범위를 극적으로 확장했다. 이 연구들은 생성형 AI가 단순한 이미지 생성기를 넘어, 정밀한 제어가 가능한 창의적, 공학적 도구로 진화하고 있음을 보여주었다.</p>
<h3>2.1 텍스트-3D 합성의 새로운 지평: DreamFusion 분석</h3>
<p>ICLR 2023에서 최우수 논문상을 수상한 <em>DreamFusion</em>은 텍스트-3D 합성 분야에 새로운 지평을 열었다.9 이 연구의 핵심 혁신은 대규모 3D 데이터셋의 부재라는 근본적인 한계를 독창적인 방식으로 우회한 데 있다.</p>
<p><em>DreamFusion</em>은 Imagen과 같이 방대한 텍스트-이미지 쌍으로 사전 훈련된 2D 확산 모델을 3D 모델 생성을 위한 강력한 사전 지식(prior)으로 활용했다.14 이는 3D 데이터 없이 2D 모델의 지식만으로 3차원 세계를 창조할 수 있음을 입증한 것이다.</p>
<p>이 방법론의 중심에는 점수 증류 샘플링(Score Distillation Sampling, SDS)이라는 새로운 손실 함수가 있다. SDS는 2D 확산 모델을 일종의 ’전지적 비평가’로 활용하여 3D 표현(Neural Radiance Field, NeRF)을 최적화하는 기법이다.15 최적화 과정은 다음과 같이 진행된다. 먼저, 무작위로 초기화된 3D 모델(NeRF)을 임의의 시점에서 렌더링하여 2D 이미지를 생성한다. 그 다음, 사전 훈련된 2D 확산 모델을 사용하여 이 렌더링된 이미지가 주어진 텍스트 프롬프트와 얼마나 잘 부합하는지를 평가한다. 확산 모델은 단순히 ‘좋다’ 또는 ’나쁘다’를 평가하는 것을 넘어, 이미지가 더 사실적으로 개선되기 위해 어느 방향으로 수정되어야 하는지에 대한 ‘점수(score)’, 즉 그래디언트를 제공한다. 이 그래디언트는 3D 모델의 파라미터를 업데이트하는 데 사용되며, 이 과정을 반복함으로써 2D 확산 모델이 가진 방대한 시각적 지식이 3D 모델로 ’증류(distilled)’된다.15</p>
<p>SDS 손실 함수의 그래디언트는 수학적으로 다음과 같이 정의된다. 이 수식은 3D 모델 파라미터 <span class="math math-inline">θ</span>에 대한 손실 함수의 변화율을 나타낸다.<br />
<span class="math math-display">
\nabla_{\theta} L_{SDS}(\phi, x=g(\theta)) = \mathbb{E}_{t, \epsilon}\left[w(t)(\epsilon_{\phi}(x_t; y) - \epsilon)\frac{\partial x}{\partial \theta}\right]
</span><br />
여기서 각 항은 다음과 같은 의미를 가진다.</p>
<ul>
<li><span class="math math-inline">θ</span>: 최적화하고자 하는 3D 모델(NeRF)의 파라미터.</li>
<li><span class="math math-inline">g(θ)</span>: 파라미터 <span class="math math-inline">θ</span>를 입력받아 2D 이미지를 출력하는 미분 가능한 렌더러.</li>
<li><span class="math math-inline">x = g(θ)</span>: 렌더링된 2D 이미지.</li>
<li><span class="math math-inline">y</span>: 사용자가 입력한 텍스트 프롬프트.</li>
<li><span class="math math-inline">x_t</span>: 시간 <span class="math math-inline">t</span>에서 노이즈 <span class="math math-inline">ϵ</span>가 추가된 이미지.</li>
<li><span class="math math-inline">\epsilon_{\phi}(x_t; y)</span>: 텍스트 프롬프트 <span class="math math-inline">y</span>가 주어졌을 때, 노이즈가 낀 이미지 <span class="math math-inline">x_t</span>에서 노이즈를 예측하는 2D 확산 모델.</li>
<li><span class="math math-inline">w(t)</span>: 시간에 따라 가중치를 조절하는 함수.</li>
</ul>
<p>이 그래디언트 업데이트의 핵심은 렌더링된 이미지 <span class="math math-inline">x</span>가 2D 확산 모델의 확률 분포 하에서 더 높은 가능도(likelihood)를 가지도록, 즉 더 사실적이고 프롬프트에 부합하도록 <span class="math math-inline">θ</span>를 조정하는 것이다.17 이 과정을 통해 3D 데이터 없이도 텍스트 설명만으로 고품질의 3D 에셋을 생성하는 것이 가능해졌다.</p>
<h3>2.2 모델 개인화와 명령어 기반 편집 기술</h3>
<p>생성형 AI의 발전은 단순히 새로운 콘텐츠를 만드는 것을 넘어, 기존 콘텐츠를 사용자의 의도에 맞게 정밀하게 수정하고 개인화하는 방향으로 나아갔다. CVPR 2023에서 발표된 <em>DreamBooth</em>와 <em>InstructPix2Pix</em>는 이러한 흐름을 주도한 대표적인 연구로, 각각 ’주체 주도 생성’과 ’명령어 기반 편집’이라는 새로운 제어 패러다임을 제시했다.18</p>
<p><em>DreamBooth</em>는 텍스트-이미지 모델을 ’개인화’하는 기술에 초점을 맞춘다.19 이 기술의 핵심은 단 3~5장의 특정 피사체 이미지(예: 특정 반려견 사진)를 사용하여 사전 훈련된 거대 확산 모델을 미세 조정(fine-tuning)하는 것이다. 이 과정을 통해 모델은 해당 피사체를 [V]와 같은 고유 식별자와 결합하는 법을 학습한다.19 학습이 완료되면, 사용자는 “해변에 있는 [V] 강아지“와 같은 프롬프트에 이 고유 식별자를 삽입하여, 원본 이미지에는 없었던 새로운 배경, 포즈, 조명 환경에서 해당 피사체가 포함된 완전히 새로운 이미지를 생성할 수 있다.</p>
<p><em>DreamBooth</em>의 성공 비결 중 하나는 ’클래스-특정 사전 보존 손실(class-specific prior preservation loss)’이라는 독창적인 규제 기법에 있다. 미세 조정 과정에서 모델이 특정 피사체에 과적합되어 ’강아지’라는 일반적인 개념 자체를 잊어버리는 ‘언어 표류(language drift)’ 현상이 발생할 수 있다. 이를 방지하기 위해, <em>DreamBooth</em>는 특정 피사체([V] 강아지)를 학습하는 동시에, 모델이 자체적으로 생성한 일반적인 클래스(“강아지”)의 이미지를 함께 학습 데이터로 사용한다. 이 이중 학습 구조는 모델이 특정 피사체의 고유한 특징을 학습하면서도, ’강아지’라는 광범위한 개념에 대한 사전 지식을 유지하도록 강제하여 생성 품질과 다양성을 모두 확보한다.20</p>
<p>반면, <em>InstructPix2Pix</em>는 대규모의 인간 주석 데이터셋 없이 이미지 편집 모델을 훈련하는 새로운 방법을 제시했다.22 이 연구의 독창성은 GPT-3와 같은 거대 언어 모델(LLM)과 Stable Diffusion과 같은 텍스트-이미지 모델의 능력을 결합하여 훈련 데이터를 자동으로 생성하는 파이프라인을 구축한 데 있다.22 먼저, GPT-3를 사용하여 기존 이미지 캡션(예: “해바라기 밭에 있는 남자의 사진”)을 바탕으로 편집 명령어(“해바라기를 장미로 바꿔줘”)와 편집 후 캡션(“장미 밭에 있는 남자의 사진”)을 대량으로 생성한다. 그 다음, Prompt-to-Prompt라는 기술과 Stable Diffusion을 활용하여 이 캡션 쌍에 해당하는 ’편집 전’과 ‘편집 후’ 이미지 쌍을 생성한다. 이렇게 구축된 수십만 개의 합성 데이터셋을 사용하여, InstructPix2Pix 모델은 이미지와 편집 명령어를 입력받아 편집된 이미지를 출력하도록 훈련된다.26</p>
<p>이 두 연구를 비교하면 생성형 AI의 제어 패러다임이 어떻게 진화하고 있는지를 명확히 알 수 있다. <em>DreamBooth</em>는 모델의 어휘 사전에 새로운 ‘명사’(피사체)를 주입하는 기술에 가깝다. 사용자는 몇 장의 이미지를 제공함으로써 모델이 새로운 개념을 배우게 한다. 반면, <em>InstructPix2Pix</em>는 모델에게 이미지 조작과 관련된 ‘동사’(행동)를 이해하도록 가르치는 기술이다. 사용자는 자연어 명령을 통해 모델이 수행할 작업을 지시한다. 이는 초기 모델들이 프롬프트 엔지니어링에 의존했던 것에서 한 단계 나아가, 사용자가 보다 직관적이고 정밀하게 생성 과정을 제어할 수 있게 되었음을 의미한다. “사실적인 이미지를 생성할 수 있는가?“라는 질문에서 “내가 원하는 바로 그 이미지를 생성하고, 정밀하게 수정하며, 심지어 다른 차원(3D)으로 만들어낼 수 있는가?“라는 질문으로 연구의 초점이 이동한 것이다.</p>
<h2>3. 체화된 지능의 발전: 로봇의 인식, 탐색, 그리고 제어</h2>
<p>생성형 AI가 디지털 콘텐츠 제작의 패러다임을 바꾸는 동안, 물리적 세계에서는 AI를 통해 로봇이 환경을 더 잘 이해하고, 탐색하며, 상호작용하는 체화된 지능 연구가 큰 진전을 이루었다. 2023년 5월의 연구들은 인식, 계획, 제어 기술이 융합되면서 로봇이 더욱 복잡하고 예측 불가능한 환경에서 자율적으로 임무를 수행할 수 있는 가능성을 열었다. 특히, 로봇의 행동 생성을 조건부 생성 문제로 간주하고, 이를 이미지 및 텍스트 생성에서 혁명을 일으킨 확산 모델과 같은 부류의 모델로 해결하려는 새로운 패러다임이 부상했다. 이는 이전에 분리되어 있던 AI 하위 분야들이 의미 있는 방식으로 수렴하고 있음을 보여주는 중요한 신호다.</p>
<h3>3.1 공간 인식 및 자율 항법 기술의 최전선</h3>
<p>자율 시스템의 핵심 능력 중 하나는 자신의 위치를 파악하고 목표 지점까지 경로를 찾는 항법(navigation) 기술이다. 이 분야에서 ICLR 2023 최우수 논문으로 선정된 ‘맹인 항법 에이전트의 기억 속 지도 출현’ 연구는 중요한 이론적 통찰을 제공했다.9 이 연구는 시각 정보 없이 오직 이전의 움직임과 접촉 정보에만 의존하는 ‘맹인’ 에이전트가 명시적인 지도 생성 모듈 없이도 복잡한 환경을 성공적으로 탐색할 수 있음을 보였다. 더 나아가, 분석 결과 이 에이전트의 기억 장치(LSTM) 내에서 환경의 구조를 나타내는 지도와 같은 공간적 표현이 암묵적으로 형성되었음을 입증했다. 이는 오랫동안 대립해 온 ’지도 기반 항법’과 ’지도 없는 항법’이라는 이분법적 접근 방식에 도전하며, 외부 환경에 대한 표상을 명시적으로 설계하지 않아도 범용 학습 아키텍처를 통해 복잡한 공간 추론 능력이 자발적으로 나타날 수 있음을 시사한다.9</p>
<p>이러한 이론적 발전과 함께, 실제 환경에서 로봇의 항법 정확도를 높이기 위한 실용적인 연구도 활발히 진행되었다. <em>BoW3D</em> 논문은 3D 라이다(LiDAR) SLAM(동시적 위치 추정 및 지도 작성) 시스템의 오랜 난제였던 실시간 루프 클로저(loop closure) 문제를 해결하기 위한 새로운 접근법을 제시했다.27 루프 클로저는 로봇이 이전에 방문했던 장소를 다시 인식하여 누적된 위치 오차를 보정하는 핵심적인 기능이다.</p>
<p><em>BoW3D</em>는 시각 SLAM 분야에서 성공적으로 사용된 ‘Bag of Words(BoW)’ 개념을 3D 포인트 클라우드 데이터에 맞게 변형했다. 구체적으로, LinK3D라는 효율적인 3D 특징점 기술자를 사용하여 포인트 클라우드에서 추출한 특징들을 단어(word)로 변환하고, 이를 기반으로 장소를 표현하는 벡터를 생성한다. 이를 통해 로봇은 현재 관측하는 장소가 과거에 방문했던 곳과 유사한지를 매우 빠르게 판단하고, 6자유도(6-DoF) 전체에 대한 정밀한 위치 보정을 실시간으로 수행할 수 있다. 이는 장시간 동안 대규모 환경에서 로봇이 정확한 지도를 유지하며 자율 주행하는 데 필수적인 기술이다.27</p>
<h3>3.2 확산 정책을 통한 로봇의 고난이도 기술 학습</h3>
<p>로봇 제어 분야에서는 도요타 연구소(TRI)가 생성형 AI 기법에 기반한 ’확산 정책(Diffusion Policy)’을 사용하여 로봇에게 새로운 행동을 가르치는 획기적인 연구를 발표하며 큰 주목을 받았다.12 이 연구는 2장에서 논의된 생성형 모델의 발전이 어떻게 로봇 제어라는 물리적 영역으로 직접 확장될 수 있는지를 명확하게 보여주는 사례다.</p>
<p>확산 정책은 관측(observation)으로부터 행동(action)으로의 매핑, 즉 정책(policy)을 학습하는 과정을 확산 모델의 생성 과정으로 취급한다. 기존의 강화학습이 수많은 시행착오를 통해 정책을 학습하거나, 고전적인 제어 이론이 환경에 대한 정밀한 수학적 모델을 요구했던 것과 달리, 이 접근법은 전문가가 직접 로봇을 조종하여 보여주는 소수의 시연(demonstration) 데이터만으로도 복잡한 기술을 학습할 수 있다. 학습된 정책은 액체 따르기, 변형 가능한 물체 조작하기 등 전통적인 방법으로는 매우 다루기 어려웠던 고난이도 작업을 부드럽고 안정적으로 수행하는 행동 시퀀스를 생성해낸다.12</p>
<p>더 나아가, TRI는 이 기술의 장기적인 비전을 제시하기 위해 ’거대 행동 모델(Large Behavior Models, LBMs)’이라는 용어를 도입했다. 이는 자연어 처리 분야의 ’거대 언어 모델(Large Language Models, LLMs)’에 직접적으로 대응하는 개념으로, 로봇 공학 분야의 전략적 전환을 예고한다. LBM의 핵심 아이디어는 다양한 작업에 대한 방대한 데이터를 사전 학습한 하나의 거대 모델이 새로운 작업에 대해서도 제로샷(zero-shot) 또는 소수샷(few-shot) 방식으로 빠르게 일반화할 수 있다는 것이다. 이는 특정 작업마다 개별적인 모델을 개발해야 했던 기존의 패러다임을 넘어서, 로봇 행동을 위한 일종의 ’기초 모델(foundation model)’을 구축하려는 시도다. 이러한 모델이 실현된다면, 로봇을 새로운 작업에 투입하는 데 필요한 시간과 전문성을 극적으로 줄일 수 있다. 예를 들어, 공장 작업자가 새로운 부품 조립 작업을 몇 번 시연하기만 하면, LBM이 이를 즉시 학습하여 자동화하는 미래를 상상할 수 있다. 이는 서론에서 언급된 산업계의 유연성 및 신속한 배치 요구에 직접적으로 부응하는 기술적 해결책이다.6</p>
<h2>4. 로봇 시스템 및 하드웨어 혁신: ICRA 2023 주요 연구 동향</h2>
<p>AI 알고리즘의 눈부신 발전은 그 지능을 담아낼 수 있는 더욱 유능한 물리적 ’몸체’가 있을 때 비로소 완성된다. 2023년 5월 말에 개최된 ICRA는 알고리즘의 진보와 병행하여 로봇 하드웨어 및 시스템 분야에서도 중요한 혁신이 이루어지고 있음을 보여주는 장이었다. 이 학회에서는 재료, 구동기, 메커니즘 설계 등 로봇 공학의 근간을 이루는 다양한 연구들이 발표되었으며, 이는 AI의 지능을 현실 세계에서 효과적으로 발현시키기 위한 필수적인 기반 기술들이다.3 로봇 공학 분야의 발전은 두 개의 트랙으로 진행되고 있다. 하나는 LBM과 같이 더 똑똑한 ’두뇌’를 만드는 고수준 AI 연구이며, 다른 하나는 재료, 메커니즘, 기전공학 분야에서 더 유능한 ’몸체’를 만드는 기초 연구다. 진정한 돌파구는 이 두 분야가 교차하는 지점에서 발생한다.</p>
<h3>4.1 소프트 로보틱스와 생체모방 메커니즘</h3>
<p>ICRA 2023에서는 기존의 단단하고 정형화된 로봇의 한계를 극복하기 위한 소프트 로보틱스 및 생체모방 로봇 연구가 두드러졌다. 이러한 연구들은 로봇의 물리적 특성 자체가 제어와 환경 적응에 기여하는 ‘체화된 지능’ 개념을 구체화한다.33</p>
<p>핵심 부품 수준에서는 옥스포드 대학에서 발표한 ’조정 가능한 특성을 가진 3D 프린팅 공압 구동 이안정성 밸브’와 같은 연구가 주목받았다.30 이 밸브는 외부 전력 공급 없이도 두 가지 안정된 상태를 유지할 수 있어, 소프트 로봇의 에너지 효율을 높이고 제어 시스템을 단순화하는 데 기여할 수 있다. 이는 소프트 그리퍼나 인공 근육의 미세하고 효율적인 제어를 가능하게 하는 기초 기술이다.</p>
<p>생물체의 움직임에서 영감을 얻은 새로운 이동 메커니즘 연구도 활발했다. ’다양한 직경의 파이프 내부를 기어가기 위한 텐세그리티 기반 자벌레형 로봇’은 장력과 압축력을 이용하는 텐세그리티 구조를 활용하여 좁고 가변적인 환경을 탐색하는 능력을 보여주었다.30 또한, 칭화대학에서 개발한 ’다양한 지형 탐사를 위한 저압 미세유체 구동기를 갖춘 무선 노래기 로봇’은 수많은 다리를 유기적으로 제어하여 복잡한 지형을 안정적으로 이동하는 새로운 가능성을 제시했다.30 공압 인공 근육(PAM)으로 구동되는 뱀 로봇의 에너지 효율을 분석한 연구는 장시간 야외 임무 수행과 같은 실용적인 응용을 위해 로봇의 에너지 소비를 최적화하려는 노력을 보여준다.34 이러한 연구들은 생물학적 원리를 공학적으로 구현하여 로봇이 기존에는 접근하기 어려웠던 복잡하고 비정형적인 환경에서 임무를 수행할 수 있도록 하는 데 초점을 맞추고 있다.</p>
<h3>4.2 휴머노이드 로봇 및 고등 메커니즘 설계</h3>
<p>소프트 로보틱스와 함께, 인간과 유사한 형태와 능력을 갖춘 휴머노이드 로봇 및 이를 구성하는 고등 메커니즘에 대한 연구도 꾸준히 진행되었다. 이는 인간의 작업 환경에 로봇을 자연스럽게 통합하기 위한 노력의 일환이다.</p>
<p>ICRA 2023에서는 로봇의 효율성과 성능을 근본적으로 개선하기 위한 구체적인 공학적 혁신들이 발표되었다. 한국기계연구원의 ‘로봇 팔의 필요 토크 최소화를 위한 기어 유닛 및 스프링 기반의 진보된 2자유도 평형 유지 메커니즘’ 연구는 중력의 영향을 상쇄하여 로봇 팔을 움직이는 데 필요한 에너지를 획기적으로 줄이는 기술을 선보였다.30 이는 특히 배터리로 작동하는 모바일 로봇이나 협동 로봇의 작동 시간을 늘리는 데 결정적인 역할을 할 수 있다. 또한, ’2자유도 토러스 휠로 구동되는 손바닥 크기의 전방향 이동 로봇’은 좁은 공간에서도 자유롭게 움직일 수 있는 새로운 소형 이동 플랫폼을 제시하며, 물류 및 서비스 로봇의 이동성을 한 단계 끌어올릴 잠재력을 보여주었다.30</p>
<p>이러한 개별 부품 및 메커니즘 연구는 더 큰 그림, 즉 휴머노이드 로봇 개발이라는 산업계의 거대한 흐름과 맞물려 있다. 2023년 5월, Figure AI가 7천만 달러 규모의 시리즈 A 투자를 유치하고, Agility Robotics가 연간 1만 대의 로봇 생산을 위한 공장 건설 계획을 발표하는 등 휴머노이드 분야에 대한 산업계의 기대와 투자가 최고조에 달했다.35 ICRA에서 발표된 효율적인 팔, 정교한 손, 새로운 이동 메커니즘에 대한 기초 연구들은 이러한 야심찬 휴머노이드 프로젝트의 실현 가능성을 높이는 핵심적인 밑거름이 된다. 결국, 고도화된 AI ’두뇌’가 내리는 복잡한 명령을 물리적으로 정확하고 효율적으로 수행할 수 있는 ’몸체’의 발전 없이는 진정한 의미의 휴머노이드 로봇은 구현될 수 없다.</p>
<h2>5. 결론: 주요 연구 성과의 통합적 의의와 향후 전망</h2>
<p>2023년 5월은 AI 및 로봇 공학 분야에서 생성형 AI의 추상적 능력과 체화된 지능의 물리적 구현이 본격적으로 수렴하기 시작한 중요한 시기로 평가된다. <em>DreamFusion</em>과 <em>InstructPix2Pix</em> 같은 연구를 통해 복잡한 다차원 데이터를 생성하고 제어하는 능력이 입증되었고, 이는 도요타의 확산 정책 및 LBM 개념에서 볼 수 있듯이 복잡한 물리적 행동을 생성하고 제어하는 능력으로 이어지고 있다. 이 두 흐름의 융합은 단순히 개별 기술의 발전을 넘어, AI가 디지털 세계를 넘어 물리적 세계와 상호작용하고 변화시키는 능력을 갖추게 되었음을 의미한다.</p>
<p>이 시기의 연구 성과들은 다음과 같은 미래 전망을 제시한다.</p>
<p>단기적으로(1~2년 내), 생성형 모델을 로봇 공학의 다양한 문제에 적용하는 연구가 폭발적으로 증가할 것이다. 조작(manipulation)을 넘어, 작업 계획 수립, 인간-로봇 상호작용 시나리오 설계, 고충실도 시뮬레이션 환경 자동 생성 등 로봇 개발의 전 과정에 생성형 AI가 깊숙이 통합될 것이다. 산업계에서 주목받고 있는 ‘노코드(no-code)’ 또는 ‘로우코드(low-code)’ 로봇 프로그래밍 플랫폼은 이러한 기초 모델을 기반으로 더욱 강력하고 직관적인 기능을 제공하게 될 것이다.6</p>
<p>중기적으로(3~5년 내), ’거대 행동 모델(LBMs)’이 물류 및 제조 현장에 처음으로 실질적으로 배치될 것으로 예상된다. 이를 통해 로봇은 전문가의 개입을 최소화하면서 새로운 제품 라인이나 작업 흐름에 신속하게 재배치될 수 있게 된다. ICRA에서 발표된 하드웨어 기술의 발전에 힘입어, 휴머노이드 로봇은 이러한 LBM을 탑재하고 인간 작업자와 같은 공간에서 복잡한 작업을 수행하는 주요 플랫폼이 될 것이다.35</p>
<p>장기적으로(5년 이상), 궁극적인 비전은 다중 모달리티로 세계를 인식하고, 이에 대해 추론하며, 물리적으로 행동할 수 있는 통합된 AI 시스템의 구축이다. 2023년 5월에 발표된 연구들—암묵적인 지도 형성에서부터 생성적 제어 정책에 이르기까지—은 이러한 범용 인공지능(AGI), 특히 물리적 세계와 상호작용하는 체화된 AGI라는 장기적 목표를 향한 핵심적인 구성 요소들을 마련한 것으로 볼 수 있다.</p>
<p>이러한 기술적 진보는 순수한 과학적 탐구를 넘어, 고령화 사회, 노동력 부족, 그리고 더 회복력 있고 지속 가능한 제조업의 필요성과 같은 중대한 사회적 과제에 대한 직접적인 해결책을 제시한다.6 2023년 5월의 연구 성과들은 차세대 자동화와 인간-기계 협업의 시대를 여는 기술적 토대를 마련했으며, 그 영향력은 앞으로 수년에 걸쳐 산업과 사회 전반에 깊이 각인될 것이다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>(PDF) Artificial Intelligence, Machine Learning and Deep Learning in Advanced Robotics, A Review - ResearchGate, https://www.researchgate.net/publication/369818981_Artificial_Intelligence_Machine_Learning_and_Deep_Learning_in_Advanced_Robotics_A_Review</li>
<li>Intelligent Robotics—A Systematic Review of Emerging Technologies and Trends - MDPI, https://www.mdpi.com/2079-9292/13/3/542</li>
<li>Conference Proceedings : 29th May-2nd June 2023, ExCeL London - Google Books, https://books.google.com/books/about/2023_IEEE_International_Conference_on_Ro.html?id=U34V0AEACAAJ</li>
<li>2023 - Call For Papers - ICLR 2026, https://iclr.cc/Conferences/2023/CallForPapers</li>
<li>CVPR 2023 Open Access Repository, https://openaccess.thecvf.com/CVPR2023</li>
<li>Top 5 Robot Trends 2023 - International Federation of Robotics, https://ifr.org/ifr-press-releases/news/top-5-robot-trends-2023</li>
<li>ABB predicts top robotics trends for 2023 | News center, https://new.abb.com/news/detail/100312/prsrl-abb-predicts-top-robotics-trends-for-2023</li>
<li>Top U.S. Robotics Trends for 2023 | Supply &amp; Demand Chain Executive, https://www.sdcexec.com/software-technology/robotics/article/22863555/abb-top-us-robotics-trends-for-2023</li>
<li>ICLR 2023 Awards, https://iclr.cc/virtual/2023/awards_detail</li>
<li>Eleventh ICLR Unveils Outstanding Paper Award Winners ICLR 2023 is the first major AI and deep learning conference gathering in Kigali, Africa, https://www.iclr.cc/media/Press/ICLR_2023_Press_Release.pdf</li>
<li>ICLR 2023 announces the Outstanding Paper Award recipients: an overview of the four selected papers - MLWires, https://www.mlwires.com/iclr-2023-announces-the-outstanding-paper-award-recipients-an-overview-of-the-four-selected-papers/</li>
<li>Toyota Research Institute Unveils Breakthrough in Teaching Robots New Behaviors, https://pressroom.toyota.com/toyota-research-institute-unveils-breakthrough-in-teaching-robots-new-behaviors/</li>
<li>Outstanding Paper Award, International Conference on Learning Representations (ICLR), 2023 | Erik Wijmans, https://wijmans.xyz/award/eom-best-paper/</li>
<li>[2209.14988] DreamFusion: Text-to-3D using 2D Diffusion - ar5iv, https://ar5iv.labs.arxiv.org/html/2209.14988</li>
<li>DreamFusion: Text-to-3D using 2D Diffusion, https://dreamfusion3d.github.io/</li>
<li>dreamfusion: text-to-3d using 2d diffusion - arXiv, https://arxiv.org/pdf/2209.14988</li>
<li>DreamFusion: Text-to-3D using 2D Diffusion, https://arxiv.org/abs/2209.14988</li>
<li>SkalskiP/top-cvpr-2023-papers: This repository is a curated … - GitHub, https://github.com/SkalskiP/top-cvpr-2023-papers</li>
<li>DreamBooth, https://dreambooth.github.io/</li>
<li>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for …, https://arxiv.org/abs/2208.12242</li>
<li>arXiv:2208.12242v2 [cs.CV] 15 Mar 2023, https://arxiv.org/pdf/2208.12242</li>
<li>[2211.09800] InstructPix2Pix: Learning to Follow Image Editing Instructions - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/2211.09800</li>
<li>InstructPix2Pix - Tim Brooks, https://www.timothybrooks.com/instruct-pix2pix</li>
<li>InstructPix2Pix - everyday series, https://newsletter.everydayseries.com/instructpix2pix/</li>
<li>arXiv:2211.09800v2 [cs.CV] 18 Jan 2023, https://3dvar.com/Brooks2022InstructPix2Pix.pdf</li>
<li>arXiv:2211.09800v2 [cs.CV] 18 Jan 2023, https://arxiv.org/pdf/2211.09800</li>
<li>BoW3D: Bag of Words for Real-Time Loop Closing in 3D LiDAR SLAM - ResearchGate, https://www.researchgate.net/publication/365310694_BoW3D_Bag_of_Words_for_Real-Time_Loop_Closing_in_3D_LiDAR_SLAM</li>
<li>(PDF) BoW3D: Bag of Words for Real-time Loop Closing in 3D LiDAR SLAM, https://www.researchgate.net/publication/362728568_BoW3D_Bag_of_Words_for_Real-time_Loop_Closing_in_3D_LiDAR_SLAM</li>
<li>BoW3D: Bag of Words for Real-Time Loop Closing in 3D LiDAR SLAM - J-Global, https://jglobal.jst.go.jp/en/detail?JGLOBAL_ID=202302214192383056</li>
<li>2023 IEEE International Conference on Robotics and Automation (ICRA 2023) (Table of Contents) - Proceedings.com, https://www.proceedings.com/content/069/069566webtoc.pdf</li>
<li>ryanbgriffiths/ICRA2023PaperList: ICRA2023 Paper List - GitHub, https://github.com/ryanbgriffiths/ICRA2023PaperList</li>
<li>ROBOTICS AND AUTOMATION. IEEE INTERNATIONAL CONFERENCE. 2023. (ICRA 2023) (13 VOLS) - proceedings.com, https://www.proceedings.com/69566.html</li>
<li>Perspective for soft robotics: The field’s past and future - Heriot-Watt Research Portal, https://researchportal.hw.ac.uk/en/publications/perspective-for-soft-robotics-the-fields-past-and-future</li>
<li>Robotics, Volume 12, Issue 3 (June 2023) – 28 articles, https://www.mdpi.com/2218-6581/12/3</li>
<li>The 5 biggest robotics industry trends of 2023 - The Robot Report, https://www.therobotreport.com/the-biggest-robotics-industry-trends-in-2023/</li>
<li>McKinsey technology trends outlook 2025, https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-top-trends-in-tech</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>