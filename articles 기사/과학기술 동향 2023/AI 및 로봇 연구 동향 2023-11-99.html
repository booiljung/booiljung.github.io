<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2023년 11월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2023년 11월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2023년 AI 및 로봇 연구 동향</a> / <span>2023년 11월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2023년 11월 AI 및 로봇 연구 동향</h1>
<h2>1.  2023년 11월, 물리 세계로 확장되는 AI의 지평</h2>
<p>2023년 11월은 인공지능, 특히 대규모 파운데이션 모델(Foundation Models)이 언어와 이미지라는 디지털 영역을 넘어 로보틱스, 재료 과학 등 복잡한 물리 세계의 문제를 해결하는 핵심 도구로 자리매김하기 시작한 변곡점으로 기록된다. 이 시기의 연구들은 단순히 기존 AI 기술을 물리적 과제에 ’적용’하는 단계를 넘어섰다. 대신, 데이터 부족, 일반화의 어려움, 안전성과 같은 로보틱스 및 과학 분야의 고질적인 난제를 해결하기 위한 근본적으로 새로운 패러다임을 제시했다. 과거 로보틱스 연구가 특정 과업을 위해 막대한 양의 데이터를 사용하여 처음부터 모델을 학습시키는 방식에 의존했다면, 2023년 11월의 연구 흐름은 거대한 사전 학습 모델이 보유한 일반화되고 풍부한 지식 표현을 출발점으로 삼는 방향으로 명확히 전환되었다. 이는 ’무에서 유를 창조하는 학습’에서, 방대한 기존 지식을 물리 세계에 맞게 ‘증류하고, 적응시키며, 현실에 기반(grounding)하게 하는’ 방식으로의 패러다임 전환을 의미한다.</p>
<p>이러한 변화의 중심에는 제7회 로봇 학습 컨퍼런스(CoRL 2023)가 있었다. 이 학회에서는 파운데이션 모델을 활용하여 로봇 조작(manipulation)과 상호작용의 한계를 돌파하려는 시도들이 대거 발표되었다.1 동시에 산업계 연구소들은 학계의 흐름을 뛰어넘는 거대한 이정표를 세웠다. Google DeepMind는 AI 기반 신소재 발견 프로젝트 GNoME을 통해 과학적 발견의 속도를 근본적으로 바꾸는 가능성을 입증했으며 5, Meta AI는 AI가 실제 세계에서 인간의 복잡한 기술을 학습하는 데 초석이 될 Ego-Exo4D 데이터셋을 공개하며 새로운 연구의 장을 열었다.7</p>
<p>본 보고서는 이러한 핵심 이정표들을 중심으로 2023년 11월의 주요 연구 성과를 심층적으로 분석한다. 먼저 CoRL 2023의 주요 수상작과 핵심 동향을 통해 학계의 최전선을 조망하고, 이어 Google DeepMind와 Meta AI의 기념비적 연구를 통해 산업계가 이끄는 기술 혁명의 본질을 파헤친다. 마지막으로, 더 넓은 관점에서 주요 기술 동향과 사회적 맥락을 고찰하며 미래를 전망한다.</p>
<h2>2.  언어와 행동의 융합: CoRL 2023 주요 연구 심층 분석</h2>
<p>2023년 11월 6일부터 9일까지 미국 애틀랜타에서 개최된 제7회 로봇 학습 컨퍼런스(CoRL)는 로보틱스와 기계 학습의 교차점에서 가장 중요한 학술 행사 중 하나로, 해당 분야의 최신 연구 동향을 명확히 보여주었다.1 이 해에 발표된 연구들은 특히 대규모 언어 모델(LLM)과 비전 파운데이션 모델을 활용하여 로봇 조작, 장기 과제 수행(Long-Horizon Tasks), 그리고 인간-로봇 상호작용(HRI)의 난제를 해결하려는 경향을 뚜렷하게 드러냈다.</p>
<h3>2.1  최우수 논문 심층 분석: 로봇 조작과 상호작용의 새로운 패러다임</h3>
<p>CoRL 2023의 수상작들은 파운데이션 모델을 로봇 공학의 핵심 난제에 적용하는 새로운 방법론을 제시하며 학계의 주목을 받았다. 이 논문들은 단순히 모델을 적용하는 것을 넘어, 데이터 효율성, 일반화 능력, 그리고 안전성을 획기적으로 개선하는 방향을 제시했다.</p>
<h4>2.1.1  최우수 논문상: <em>Distilled Feature Fields Enable Few-Shot Manipulation</em></h4>
<p>이 연구는 단 한 번의 시연(One-Shot Demonstration)만으로도 새로운 물체와 환경에 대해 정교한 조작(Dexterous Manipulation) 기술을 일반화할 수 있는 ‘희소 관점 특징 증류(Sparse-View Feature Distillation)’ 방법론, 즉 SparseDFF를 제안했다.11 이는 대규모 2D 비전 모델(예: DINO)이 학습한 풍부한 시맨틱 특징을 3D 공간으로 ’증류(distill)’하여, 객체 간의 의미론적 대응 관계(Semantic Correspondence)를 설정하는 DFF(Distilled Feature Field) 개념에 기반한다.</p>
<p><strong>기술적 분석: DFF와 Contrastive Loss</strong></p>
<p>DFF는 3D 공간상의 각 점 <span class="math math-inline">p</span>에 대해, 사전 학습된 2D 비전 모델로부터 추출된 고차원 특징 벡터 <span class="math math-inline">f(p)</span>를 매핑하는 함수다. 이를 통해 장면을 단순한 기하학적 정보가 아닌, ‘손잡이’, ’가장자리’와 같은 시맨틱 정보로 표현할 수 있게 된다.15 SparseDFF의 핵심 혁신은 기존 DFF 연구가 조밀한 다중 시점 이미지를 요구했던 반면, 단 몇 개의 RGBD 이미지(Sparse Views)만으로도 시점 일관성(View-consistent)을 갖는 3D DFF를 생성한다는 점이다.12</p>
<p>시점 일관성을 확보하기 위해, 연구진은 서로 다른 두 시점 <span class="math math-inline">i</span>와 <span class="math math-inline">j</span>에서 관찰된 동일한 3D 점 <span class="math math-inline">p</span>에 투영된 특징 <span class="math math-inline">f_i(p)</span>와 <span class="math math-inline">f_j(p)</span> 사이의 유사도는 높이고, 다른 점 <span class="math math-inline">q</span>의 특징 <span class="math math-inline">f_j(q)</span>와의 유사도는 낮추는 대조 손실(Contrastive Loss)을 사용했다. 이 손실 함수는 다음과 같이 표현된다.</p>
<p><span class="math math-display">
\mathcal{L}_{\text{contrastive}} = -\log \frac{\exp(\text{sim}(f_i(p), f_j(p)) / \tau)}{\sum_{q \in \mathcal{N}(p)} \exp(\text{sim}(f_i(p), f_j(q)) / \tau)}
</span><br />
여기서 <span class="math math-inline">\text{sim}</span>은 코사인 유사도, <span class="math math-inline">\tau</span>는 온도 파라미터, <span class="math math-inline">\mathcal{N}(p)</span>는 점 <span class="math math-inline">p</span> 주변의 부정적 샘플(negative samples) 집합을 의미한다. 이 손실 함수는 서로 다른 각도에서 보더라도 동일한 지점은 동일한 특징을 갖도록 모델을 강제하여, 희소한 정보만으로도 강건한 3D 특징 필드를 구축하게 한다.</p>
<p>이렇게 생성된 DFF를 바탕으로, 시연 장면(Source)과 목표 장면(Target)에서 각각 로봇 손 표면에 샘플링된 점들의 특징 벡터 차이를 최소화하는 에너지 함수를 최적화하여 목표 자세를 찾는다.15 에너지 함수는 다음과 같다.</p>
<p><span class="math math-display">
E(\theta) = \sum_{p \in \mathcal{H}(\theta)} \| f_{\text{target}}(p) - f_{\text{source}}(p_{\text{demo}}) \|_2^2
</span><br />
여기서 <span class="math math-inline">\mathcal{H}(\theta)</span>는 파라미터 <span class="math math-inline">\theta</span>로 결정되는 로봇 손의 표면 점 집합이며, 이 최적화를 통해 로봇은 시연에서 보여준 ’의미론적’으로 동일한 방식으로 물체를 파지하게 된다.</p>
<h4>2.1.2  최우수 시스템 논문상: <em>RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools</em></h4>
<p>이 연구는 칼, 롤러 등 다양한 도구를 사용하여 반죽과 같은 탄소성(Elasto-Plastic) 물체를 조작하는 ’만두 만들기’와 같은 장기 과업을 성공적으로 수행하는 로봇 시스템 RoboCook을 개발했다.11 이는 그래프 신경망(GNN) 기반의 동역학 모델과 자기 지도 학습(Self-supervised Learning) 정책을 결합하여, 복잡한 도구-객체 상호작용을 모델링하고 계획하는 능력을 보여준다.</p>
<p><strong>기술적 분석: GNN 동역학 모델과 손실 함수</strong></p>
<p>RoboCook은 반죽과 도구를 수많은 입자(Particle)들의 집합으로 표현하고, 이들 간의 상호작용을 GNN으로 모델링하여 다음 상태를 예측한다. GNN은 각 입자를 노드로, 주변 입자들을 엣지로 간주하여 메시지를 전달하고 상태를 업데이트함으로써, 도구의 종류나 형태에 상관없이 일반화된 물리적 상호작용을 학습할 수 있다.18</p>
<p>모델 학습에는 예측된 입자 구름(<span class="math math-inline">\hat{O}_t</span>)과 실제 관측된 입자 구름(<span class="math math-inline">O_t</span>) 간의 기하학적 차이를 측정하는 손실 함수가 사용된다. 연구진은 주로 Chamfer Distance (CD)와 Earth Mover’s Distance (EMD)의 가중 합을 사용했다.19</p>
<p>Chamfer Distance는 두 점 구름 사이의 평균적인 최근접점 거리를 측정한다.</p>
<p><span class="math math-display">
L_{CD}(O_t, \hat{O}_t) = \sum_{x \in O_t} \min_{y \in \hat{O}_t} \|x - y\|_2^2 + \sum_{y \in \hat{O}_t} \min_{x \in O_t} \|x - y\|_2^2
</span><br />
Earth Mover’s Distance는 한 점 구름을 다른 점 구름으로 변환하는 데 필요한 최소 ’이동 비용’을 계산하여, 점들의 분포까지 고려하는 더 정밀한 척도다.</p>
<p><span class="math math-display">
L_{EMD}(O_t, \hat{O}_t) = \min_{\mu: O_t \to \hat{O}_t} \sum_{x \in O_t} \|x - \mu(x)\|_2
</span><br />
이 두 손실 함수를 결합함으로써 모델은 변형 가능한 객체의 복잡한 동역학을 높은 정확도로 학습할 수 있었다.</p>
<h4>2.1.3  최우수 학생 논문상: <em>Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners</em></h4>
<p>이 논문은 LLM 기반 로봇 플래너가 자신의 계획에 대한 불확실성을 스스로 인지하고, 필요할 때 인간에게 명확한 질문을 통해 도움을 요청하는 프레임워크 ’KnowNo’를 제안했다.11 이는 LLM의 가장 큰 약점 중 하나인 ‘자신감 있는 환각(Confident Hallucination)’ 문제를 완화하고, 안전하고 신뢰성 있는 인간-로봇 협업을 가능하게 하는 중요한 진전이다.</p>
<p>이 연구는 LLM을 로봇에 적용하는 초기 단계의 “계획을 생성할 수 있는가?“라는 질문에서 한 걸음 더 나아가 “생성된 계획을 신뢰할 수 있는가?“라는 더 깊고 실용적인 질문에 답하려는 시도다. 로봇 커뮤니티가 LLM의 잠재력에 대한 초기 흥분을 넘어, 실제 물리적 세계에서 안전하게 작동하기 위해 필요한 근본적인 신뢰성 문제를 해결하기 시작했음을 보여주는 명백한 증거다.</p>
<p><strong>기술적 분석: Conformal Prediction</strong></p>
<p>KnowNo 프레임워크는 통계적 강건성을 보장하는 Conformal Prediction 이론에 기반한다.11 LLM이 “음료수를 가져와“와 같은 모호한 명령에 대해 ‘콜라’, ‘레모네이드’, ’물’과 같은 여러 가능한 다음 행동 옵션들을 생성하면, KnowNo는 각 옵션에 대한 신뢰도 점수를 계산한다. 이 점수 분포를 바탕으로, 사전에 정의된 오류율(예: 5%) 하에서 통계적으로 정답을 포함할 확률이 보장되는 ’신뢰 집합(Confidence Set)’을 구성한다.</p>
<p>만약 이 신뢰 집합에 두 개 이상의 행동 옵션이 포함된다면(예: {콜라, 레모네이드}), 이는 로봇이 어떤 행동을 선택해야 할지 통계적으로 유의미하게 불확실하다는 의미로 해석된다. 이때 로봇은 단순히 가장 확률이 높은 옵션을 선택하는 대신, “콜라와 레모네이드 중 어떤 음료를 가져올까요?“와 같이 구체적인 질문을 생성하여 인간에게 개입을 요청한다. 이 접근법은 LLM의 출력을 맹목적으로 신뢰하는 대신, 불확실성을 정량화하고 이를 바탕으로 인간과의 협업을 유도하는 안전장치를 마련한 것이다.11</p>
<table><thead><tr><th>수상 부문 (Award)</th><th>논문 제목 (Paper Title)</th><th>핵심 기여 (Key Contribution)</th><th>주 저자 및 소속 (Lead Authors &amp; Affiliation)</th></tr></thead><tbody>
<tr><td><strong>최우수 논문상 (Best Paper)</strong></td><td>Distilled Feature Fields Enable Few-Shot Manipulation</td><td>대규모 2D 비전 모델의 특징을 3D 공간으로 증류하여, 단 한 번의 시연으로 정교한 조작 기술을 일반화하는 SparseDFF 프레임워크 제안.</td><td>William Shen, Ge Yang, et al. (MIT)</td></tr>
<tr><td><strong>최우수 시스템 논문상 (Best Systems Paper)</strong></td><td>RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools</td><td>GNN 동역학 모델과 자기 지도 학습을 결합하여, 다양한 도구를 사용한 탄소성 물체의 장기 복합 조작(예: 만두 만들기)을 성공적으로 수행.</td><td>Haochen Shi, Huazhe Xu, et al. (Stanford, Tsinghua)</td></tr>
<tr><td><strong>최우수 학생 논문상 (Best Student Paper)</strong></td><td>Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners</td><td>LLM 플래너의 불확실성을 통계적으로 정량화하고, 필요 시 인간에게 도움을 요청하여 신뢰성과 안전성을 높이는 KnowNo 프레임워크 제안.</td><td>Allen Z. Ren, Anushri Dixit, et al. (Princeton, Google)</td></tr>
</tbody></table>
<h3>2.2  구두 발표 및 워크숍 핵심 동향</h3>
<p>수상작 외에도 CoRL 2023에서는 AI와 로보틱스의 융합을 가속화하는 여러 중요한 연구 흐름이 관찰되었다.</p>
<ul>
<li>
<p><strong>LLM의 로봇 적용 심화:</strong> LLM을 단순히 명령어 번역기로 사용하는 것을 넘어, 로봇의 인지 및 계획 능력의 핵심부로 통합하려는 연구가 다수 발표되었다. ’SayPlan’은 LLM을 3D 장면 그래프(Scene Graph)와 결합하여 물리적 환경에 기반한 확장 가능한 작업 계획을 생성했으며 3, ’VoxPoser’는 LLM이 직접 조작 궤적을 생성하는 대신, 복잡한 3D 가치 맵(Value Map)을 합성하는 파이썬 코드를 생성하게 함으로써 LLM의 추상적 추론 능력을 활용했다.3 이는 LLM의 출력을 물리적 제약 조건과 결합하여 신뢰성을 높이려는 시도로 볼 수 있다.</p>
</li>
<li>
<p><strong>센서모터 사전학습(Sensorimotor Pre-training):</strong> ‘RPT’ 연구는 대규모 비지도 센서모터 데이터(카메라 이미지, 로봇 관절 상태, 수행 행동) 시퀀스의 일부를 마스킹하고 이를 예측하도록 Transformer 모델을 학습시키는 방식을 제안했다.21 이는 자연어 처리에서 BERT와 같은 모델이 사용한 사전학습 패러다임을 로봇 공학에 적용한 것으로, 이렇게 사전 학습된 모델은 물리 세계에 대한 내재적 모델을 학습하게 된다. 그 결과, 특정 과업에 대한 미세조정(Fine-tuning) 시 성능과 데이터 효율성을 크게 향상시켜, 로봇 학습의 확장성을 위한 중요한 방향을 제시했다.</p>
</li>
<li>
<p><strong>소프트 로보틱스(Soft Robotics):</strong> 연성 로봇의 고유한 복잡성과 제어의 어려움을 해결하기 위한 워크숍이 개최되었다.22 특히, 연성체의 복잡한 동역학을 시뮬레이션하고 이를 통해 제어 정책을 학습하기 위한 미분 가능한 시뮬레이션(Differentiable Simulation)과 같은 기법들이 활발히 논의되며, 이 분야의 발전을 위한 새로운 도구들이 모색되었다.</p>
</li>
</ul>
<h2>3.  파운데이션 모델이 이끄는 과학적 발견의 혁명</h2>
<p>2023년 11월 말, Google DeepMind와 Meta AI는 각각 재료 과학과 멀티모달 인식 분야에서 AI의 역할을 재정의하는 기념비적인 연구를 발표했다. 이들의 발표는 AI가 특정 엔지니어링 문제를 해결하는 도구를 넘어, 기초 과학의 발견 프로세스 자체를 가속화하고 인간의 능력을 확장하는 새로운 패러다임을 열고 있음을 명확히 보여주었다.</p>
<h3>3.1  Google DeepMind의 GNoME: AI가 발견한 800년 분량의 신소재</h3>
<p>11월 29일, 세계적인 학술지 Nature에 발표된 “Scaling deep learning for materials discovery” 논문은 GNoME(Graph Networks for Materials Exploration)이라는 딥러닝 모델을 통해 220만 개의 새로운 결정 구조를 예측하고, 그중 38만 개 이상이 열역학적으로 안정적임을 확인했다고 밝혔다.5 이는 인류가 지금까지 발견한 안정적인 무기 결정체 4만 8천여 개의 수를 거의 10배 가까이 늘린 것으로, AI가 인간의 화학적 직관을 뛰어넘어 광대한 미지의 재료 공간을 체계적으로 탐색할 수 있음을 증명한 사건이다.</p>
<p><strong>기술적 분석: GNN 모델과 활성 학습(Active Learning) 루프</strong></p>
<p>GNoME의 성공은 두 가지 핵심 기술의 결합에 있다: 그래프 신경망(GNN)과 대규모 활성 학습(Active Learning)이다.</p>
<ul>
<li>
<p><strong>GNN 모델 구조:</strong> GNoME는 원자를 노드(Node), 원자 간의 결합을 엣지(Edge)로 하는 그래프 구조로 결정체를 표현한다. GNN은 이러한 그래프 입력을 받아 원자 간의 상호작용을 메시지 전달(Message Passing) 방식으로 반복적으로 계산하여, 전체 시스템의 에너지, 즉 안정성을 예측한다.24 이 구조는 원자의 종류와 위치가 변해도 동일한 물리 법칙이 적용되는 결정 구조의 특성(대칭성)을 효과적으로 학습하는 데 최적화되어 있다.</p>
</li>
<li>
<p><strong>활성 학습 프로세스:</strong> GNoME의 진정한 힘은 방대한 계산 자원을 활용한 활성 학습 루프에서 나온다. 이 과정은 다음과 같은 4단계로 구성된다.</p>
</li>
</ul>
<ol>
<li>
<p><strong>생성 (Generate):</strong> 기존에 알려진 결정 구조를 변형하거나(Structural Pipeline), 무작위 화학식으로부터(Compositional Pipeline) 수백만 개의 새로운 후보 물질 구조를 대량으로 생성한다.25</p>
</li>
<li>
<p><strong>예측 (Predict):</strong> 현재 학습된 GNN 모델을 사용하여 이 후보 물질들의 안정성을 빠르고 저렴하게 예측하고, 안정적일 가능성이 높은 소수의 후보군을 선별한다.</p>
</li>
<li>
<p><strong>검증 (Verify):</strong> 선별된 후보군에 대해서만 계산 비용이 매우 높은 양자역학 기반 시뮬레이션(Density Functional Theory, DFT)을 수행하여 실제 안정성을 정밀하게 검증한다.25</p>
</li>
<li>
<p><strong>재학습 (Retrain):</strong> DFT를 통해 검증된 새로운 데이터를 ’정답’으로 간주하여 학습 데이터셋에 추가하고, 이를 바탕으로 GNN 모델을 다시 학습시킨다.</p>
</li>
</ol>
<p>이 과정을 반복하면서 모델의 예측 정확도는 점진적으로 향상되었고(초기 50%대에서 최종 80% 이상으로), 더 효율적으로 새로운 안정적 물질을 발견하는 선순환 구조가 만들어졌다.25</p>
<p><strong>잠재적 응용 분야 및 파급 효과</strong></p>
<p>GNoME이 발견한 38만 개의 안정적 신소재 중에는 52,000개의 그래핀 유사 층상 화합물(초전도체 개발 후보)과 528개의 리튬 이온 전도체(차세대 배터리 성능 향상 후보) 등이 포함되어 있다.25 이는 청정 에너지, 고효율 컴퓨팅 등 미래 기술 개발에 필수적인 원천 물질을 대거 확보했음을 의미한다.</p>
<p>더 나아가, 버클리 국립 연구소의 A-Lab 프로젝트는 GNoME이 예측한 물질의 합성법을 AI가 자율적으로 문헌에서 찾아내고 로봇 팔을 이용해 실제로 합성하는 데 성공했다.5 이는 ’AI 기반 예측 → 로봇 기반 자율 합성’이라는 과학 연구의 완전 자동화 루프가 현실화되었음을 보여준다. 이 패러다임은 신물질 개발 주기를 수년에서 수일 단위로 단축시킬 잠재력을 가지며, 과학 연구의 방법론 자체를 근본적으로 변화시킬 것이다.</p>
<h3>3.2  Meta AI의 Ego-Exo4D: 인간의 기술을 이해하는 AI의 눈</h3>
<p>11월 30일, Meta AI는 15개 대학과의 대규모 컨소시엄을 통해 구축한 Ego-Exo4D 데이터셋을 발표했다.7 이 데이터셋은 한 사람이 웨어러블 카메라를 착용하고 촬영한 1인칭(Egocentric) 비디오와, 주변에 설치된 여러 대의 카메라로 동시에 촬영한 3인칭(Exocentric) 비디오를 포함하는 세계 최대 규모의 멀티뷰, 멀티모달 데이터셋이다.31 총 1,400시간이 넘는 분량으로, 요리, 운동, 악기 연주, 자전거 수리 등 다양한 ’숙련된 인간 활동(Skilled Human Activity)’을 담고 있다.</p>
<p><strong>데이터셋의 구성과 특징</strong></p>
<p>Ego-Exo4D의 가치는 단순히 데이터의 양에만 있지 않다. AI가 인간의 복잡한 기술을 깊이 있게 이해하기 위해 필수적인 세 가지 핵심 요소를 제공한다는 점에서 그 의의가 크다.</p>
<ul>
<li>
<p><strong>1인칭-3인칭 동시성:</strong> 이 데이터셋의 가장 큰 특징은 1인칭 시점(‘내가 보는 것’)과 3인칭 시점(‘남이 나를 보는 것’)을 완벽하게 동기화하여 제공한다는 점이다.30 이는 AI가 ’관찰을 통한 학습(Learning from Observation)’을 수행할 때 겪는 근본적인 문제, 즉 ‘행위자-관찰자 시점 변환(Actor-Observer Translation)’ 문제를 해결하는 데 결정적인 데이터를 제공한다.32 로봇이 인간의 시연 영상을 보고 동작을 따라 하려면, 3인칭으로 관찰한 동작을 자신의 1인칭 시점으로 변환해야 하는데, 이 데이터셋은 바로 그 변환 관계를 직접 학습할 수 있는 기회를 제공한다.</p>
</li>
<li>
<p><strong>멀티모달리티:</strong> RGB 비디오 외에도 7채널 오디오, 시선 추적(Eye Gaze), 관성 측정 장치(IMU), 3D 포인트 클라우드, 카메라 포즈 등 풍부한 모달리티를 포함한다.31 이는 “손이 어디에 있는가“를 넘어 “무엇을 보고, 어떤 소리를 들으며, 어떻게 움직이는가“와 같은 복합적인 맥락을 AI가 학습할 수 있게 한다.</p>
</li>
<li>
<p><strong>전문가 수준의 언어 주석:</strong> 데이터셋에는 세 종류의 언어 주석이 포함되어 있다. 참가자가 자신의 행동을 설명하는 나레이션, 제3자의 객관적 행동 묘사, 그리고 가장 독창적인 요소로, 해당 분야 전문가(코치, 교사 등)가 영상 속 인물의 수행 능력을 비평하고 개선점을 지적하는 ’전문가 코멘터리(Expert Commentary)’가 그것이다.32 이 코멘터리는 AI가 단순히 행동을 인식하는 것을 넘어, 행동의 ’질’과 ’숙련도’를 평가하고 이해하는 고차원적인 능력을 학습할 수 있는 전례 없는 데이터를 제공한다.</p>
</li>
</ul>
<p><strong>벤치마크 과제와 미래 응용</strong></p>
<p>Ego-Exo4D는 네 가지 핵심 벤치마크 과제를 제시한다: (1) 핵심 단계 인식(Keystep Recognition), (2) 숙련도 평가(Proficiency Estimation), (3) 시점 간 관계 추론(Relation), (4) 3D 자세 추정(Pose Estimation).33 이 데이터셋을 통해 개발된 AI 기술은 미래에 다양한 형태로 응용될 수 있다. 예를 들어, AR 스마트 글래스를 착용한 사용자에게 실시간으로 가상 코칭을 제공하거나, 인간의 시연 비디오를 한 번 보고 정교한 조작 기술을 즉시 학습하는 로봇, 또는 사용자의 기술 수준을 정확히 이해하고 맞춤형 피드백을 제공하는 차세대 소셜 플랫폼 등으로 발전할 수 있다.7</p>
<table><thead><tr><th>항목 (Metric)</th><th>수치 (Value)</th><th>비고 (Notes)</th></tr></thead><tbody>
<tr><td><strong>총 비디오 시간 (Total Video Hours)</strong></td><td>1,422 시간</td><td>1인칭(Ego)과 3인칭(Exo) 비디오 포함 7</td></tr>
<tr><td><strong>참가자 수 (Participants)</strong></td><td>800+ 명</td><td>740명 31 또는 839명 37 등 자료에 따라 약간의 차이 존재</td></tr>
<tr><td><strong>촬영 도시 (Cities)</strong></td><td>13 개 도시</td><td>미국, 일본, 콜롬비아, 인도 등 전 세계적으로 수집 7</td></tr>
<tr><td><strong>촬영 시나리오 (Scenarios)</strong></td><td>8개 주요 분야</td><td>요리, 음악, 축구, 헬스, 농구, 댄스, 자전거 수리, 암벽 등반 33</td></tr>
<tr><td><strong>주요 모달리티 (Key Modalities)</strong></td><td>멀티뷰 비디오, 7채널 오디오, 시선, IMU, 3D 포인트 클라우드, 카메라 포즈, 3종 언어 주석</td><td>Meta의 Aria 글래스를 통해 수집된 풍부한 센서 데이터 33</td></tr>
</tbody></table>
<h2>4.  AI 및 로봇 커뮤니티의 주요 연구 흐름</h2>
<p>2023년 11월에는 특정 컨퍼런스나 대규모 발표 외에도, AI와 로봇 기술의 근본적인 발전을 이끄는 중요한 연구 흐름들이 관찰되었다. 특히 인간과 로봇의 소통 방식을 혁신하는 자연어 처리 기술의 진전과, LLM 자체의 성능 고도화 및 AI 기술의 사회적 책임에 대한 논의가 두드러졌다. 이러한 흐름들은 개별 연구 성과를 넘어, 신뢰할 수 있는 물리적 AI 시스템을 구축하기 위한 다각적인 노력을 보여준다.</p>
<h3>4.1  인간-로봇 상호작용의 고도화: 자연어 통신의 진전</h3>
<p><strong>브라운 대학의 Lang2LTL 연구</strong></p>
<p>이 시기에 주목받은 브라운 대학의 Lang2LTL 연구는 인간과 로봇 간의 소통에서 신뢰성과 정확성을 확보하기 위한 중요한 이정표를 제시했다.</p>
<ul>
<li>
<p><strong>핵심 아이디어:</strong> 이 연구의 핵심은 “은행에 들른 후에 메인 스트리트의 가게로 가되, CVS는 피해서 가“와 같이 복잡하고 순서가 중요한 자연어 명령을 로봇이 명확하게 이해하고 수행할 수 있도록, 이를 선형 시간 논리(Linear Temporal Logic, LTL)라는 수학적으로 엄밀한 형식 언어로 자동 번역하는 시스템 Lang2LTL을 개발한 것이다.39</p>
</li>
<li>
<p><strong>LTL의 중요성:</strong> LTL은 <span class="math math-inline">F</span> (Finally, 언젠가), <span class="math math-inline">G</span> (Globally, 항상), <span class="math math-inline">U</span> (Until, ~할 때까지), <span class="math math-inline">X</span> (Next, 다음에)와 같은 시간 연산자를 사용하여 복잡한 시간적, 순서적 제약 조건을 모호함 없이 표현할 수 있다.41 예를 들어, “A에 방문한 후 B에 방문하라“는 LTL 수식</p>
</li>
</ul>
<p><span class="math math-inline">F(A \land F B)</span>로 표현될 수 있다. 이는 “언젠가 A에 도달하고, 그 이후 언젠가 B에 도달한다“는 의미를 명확히 한다. 자연어의 모호성을 제거하고 로봇의 행동을 수학적으로 검증 가능하게 만든다는 점에서 큰 의의가 있다.</p>
<ul>
<li><strong>LLM의 역할:</strong> Lang2LTL은 LLM을 사용하여 자연어 문장에서 ‘은행’, ’메인 스트리트의 가게’와 같은 핵심 장소(Named Entity)를 인식하고, ‘…후에’, ’…피해서’와 같은 문장 구조를 분석하여 LTL 형식으로 변환하는 역할을 수행한다. 이 연구는 방대한 훈련 데이터 없이도 LLM의 few-shot 학습 능력을 활용하여 21개 도시의 시뮬레이션 환경에서 80% 이상의 높은 번역 정확도를 달성했음을 보여주었다.40 이는 LLM의 유연한 언어 이해 능력과 형식 검증(Formal Verification)이 가능한 LTL의 엄밀함을 결합하여, 로봇 명령 시스템의 신뢰성과 표현력을 한 단계 끌어올린 성공적인 사례다.</li>
</ul>
<h3>4.2  산업계 기술 동향과 AI 안전에 대한 고찰</h3>
<p>학계의 연구와 더불어, 산업계에서는 AI 모델의 성능을 고도화하고 이를 안전하게 배포하기 위한 기술적, 정책적 노력이 동시에 이루어졌다.</p>
<ul>
<li>
<p><strong>주요 LLM 업데이트:</strong></p>
</li>
<li>
<p><strong>OpenAI의 GPT-4 Turbo</strong>는 128k 토큰이라는 방대한 컨텍스트 창을 지원하여, 이전 모델보다 훨씬 더 긴 문서나 대화를 한 번에 처리하고 복잡한 추론을 수행할 수 있게 되었다.43</p>
</li>
<li>
<p><strong>Anthropic의 Claude 2.1</strong>은 200k 토큰 컨텍스트 창을 제공하며 GPT-4 Turbo를 능가했고, 특히 환각 현상을 이전 모델 대비 절반으로 줄였다고 발표했다. 이는 모델의 신뢰성을 높이는 중요한 개선점으로, 앞서 언급된 KnowNo 연구와 같이 모델의 신뢰성 확보가 중요한 과제로 부상하고 있음을 보여준다.43</p>
</li>
<li>
<p><strong>AWS는 Amazon Q</strong>라는 기업용 특화 챗봇과, 다양한 파운데이션 모델을 쉽게 활용할 수 있는 플랫폼 <strong>Bedrock</strong>을 강화하며 AI의 산업 적용을 가속화했다.43</p>
</li>
<li>
<p><strong>하드웨어 및 국가 전략:</strong></p>
</li>
<li>
<p><strong>중국의 휴머노이드 로봇 양산 계획:</strong> 중국 공업정보화부(MIIT)는 2025년까지 휴머노이드 로봇을 대량 생산하겠다는 야심 찬 국가 전략을 발표했다. 이는 휴머노이드를 스마트폰에 버금가는 차세대 파괴적 기술로 인식하고, 하드웨어 제조 역량을 바탕으로 미래 로봇 시장의 주도권을 확보하려는 의지를 보여준다.39</p>
</li>
<li>
<p><strong>AI 안전 서밋 (AI Safety Summit):</strong></p>
</li>
<li>
<p>11월 1-2일, 영국 블레츨리 파크에서 세계 최초로 AI 안전 서밋이 개최되었다. 이 회의는 프론티어 AI의 잠재적 위험성을 논의하고, 이를 완화하기 위한 국제적 공조 체계를 마련하기 위해 열렸다.44</p>
</li>
<li>
<p>회의 결과, 주요 AI 선도국들은 ’블레츨리 선언(Bletchley Declaration)’에 서명하며 AI 안전에 대한 공동의 책임을 확인했다. 또한 주요 AI 기업들은 자사의 안전 정책을 투명하게 공개하기로 약속했다. 이는 기술 발전과 함께 윤리 및 안전 규범을 정립하려는 국제 사회의 노력이 본격화되었음을 시사한다.</p>
</li>
</ul>
<p>이러한 개별적인 움직임들을 종합해 보면, 신뢰할 수 있는 물리적 AI를 구축하기 위한 다각적인 접근 방식이 부상하고 있음을 알 수 있다. 첫째, <strong>기술적 안전성 및 검증 가능성</strong>이다. Lang2LTL이나 KnowNo와 같은 연구는 AI의 행동을 수학적으로 명확하게 정의하거나 통계적으로 불확실성을 관리함으로써 기술적 신뢰도를 높인다. 둘째, **데이터 기반 현실성 및 기반(Grounding)**이다. Meta의 Ego-Exo4D는 AI가 실제 세계에 대한 풍부하고 현실적인 데이터를 통해 학습함으로써, 비현실적인 환각을 줄이고 행동을 현실에 기반하게 만든다. 셋째, <strong>정책 및 거버넌스</strong>다. AI 안전 서밋은 기술 개발이 사회적 합의와 윤리적 틀 안에서 이루어지도록 하는 상위 수준의 규제 및 협력 체계를 구축하려는 노력이다. 이 세 가지 축은 서로를 보완하며, 안전하고 유용한 물리적 AI의 실현을 위해 동시에 발전해야 할 필수적인 요소들이다.</p>
<h2>5.  결론: 종합 및 미래 전망</h2>
<p>2023년 11월은 인공지능 연구의 흐름이 중요한 전환점을 맞이했음을 명확히 보여준 시기였다. 파운데이션 모델은 더 이상 디지털 정보 처리의 도구에 머무르지 않고, 로보틱스와 과학 발견이라는 두 개의 거대한 물리적 세계와 본격적으로 융합되기 시작했다.</p>
<p><strong>2023년 11월의 종합적 의의</strong></p>
<p>CoRL 2023에서는 파운데이션 모델이 가진 방대한 사전 지식을 로봇의 행동으로 ’증류(Distill)’하고, 그 불확실성을 인간과의 상호작용을 통해 ’정렬(Align)’하여 로봇의 데이터 효율성과 신뢰성을 동시에 높이는 연구가 주류를 이루었다. 이는 로봇 공학의 오랜 난제였던 일반화와 안전성 문제를 해결할 새로운 실마리를 제공했다. 동시에, Google DeepMind의 GNoME과 Meta AI의 Ego-Exo4D는 AI가 인간의 지능을 보조하는 것을 넘어, 방대한 가설 공간을 탐색하여 새로운 과학적 발견을 이끌고(GNoME), 풍부한 실제 세계 데이터를 통해 인간의 복잡하고 숙련된 기술을 학습할 수 있는(Ego-Exo4D) 새로운 경로를 열었다. 이 두 프로젝트는 AI가 인류의 지식 경계를 확장하고 인간의 능력을 증강시키는 강력한 파트너가 될 수 있음을 입증했다.</p>
<p><strong>미래 연구 방향 및 과제</strong></p>
<p>이러한 눈부신 성과들은 동시에 미래 연구를 위한 명확한 과제들을 제시한다.</p>
<ul>
<li>
<p><strong>통합과 검증:</strong> LLM의 고차원적 추론 능력, 비전 모델의 시맨틱 이해 능력, 그리고 로봇의 물리적 실행 능력을 하나의 통합된 아키텍처 안에서 어떻게 유기적으로 결합할 것인가가 첫 번째 과제다. 더 나아가, 이렇게 생성된 로봇 행동의 안전성과 신뢰성을 어떻게 수학적으로, 그리고 형식적으로 검증할 것인가가 핵심 과제로 남는다. Lang2LTL과 KnowNo 연구는 이러한 방향성의 중요한 시작점이다.</p>
</li>
<li>
<p><strong>데이터의 선순환:</strong> Ego-Exo4D와 같은 고품질 데이터셋 구축과, RPT와 같은 센서모터 사전학습 모델 개발은 서로를 보완하며 발전해야 한다. 실제 로봇이 수집한 방대한 데이터로 강력한 사전학습 모델을 만들고, 더 똑똑해진 모델이 다시 더 유용하고 의미 있는 데이터를 효율적으로 수집하는 ‘데이터 선순환(Data Flywheel)’ 구조를 구축하는 것이 로봇 지능의 확장성을 위해 필수적이다.</p>
</li>
<li>
<p><strong>시뮬레이션과 현실의 간극:</strong> RoboCook과 같은 연구는 실제 세계 데이터의 중요성을 강조하지만, GNoME의 성공은 대규모 고정밀 시뮬레이션(DFT 계산)의 압도적인 힘을 보여준다. 물리적으로 매우 정확하면서도 미분 가능한 시뮬레이터(Differentiable Simulator)의 발전은, 실제 세계 실험의 높은 비용과 위험을 줄이면서도 AI 모델의 성능을 극대화하는 데 필수적인 열쇠가 될 것이다.</p>
</li>
</ul>
<p><strong>최종 전망</strong></p>
<p>2023년 11월의 성과들은 AI가 디지털 정보를 처리하는 추상적 지능을 넘어, 물리 세계를 이해하고, 상호작용하며, 심지어 그 법칙을 발견하는 ’물리적 지능(Physical Intelligence)’으로 진화하고 있음을 예고한다. 이 거대한 흐름은 향후 수십 년간 과학 연구의 방법론, 산업 자동화의 수준, 그리고 우리 일상의 모습을 근본적으로 변화시킬 것이다. 이제 인류는 AI라는 강력한 도구를 손에 쥐고, 물질과 행동의 세계에 대한 이해를 새로운 차원으로 끌어올릴 준비를 하고 있다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>CoRL 2023, https://www.corl2023.org/</li>
<li>CoRL 2023 – Research Impact &amp; Leadership - Georgia Institute of Technology, https://sites.gatech.edu/research/corl-2023/</li>
<li>Papers — CoRL 2023, https://www.corl2023.org/papers</li>
<li>Explore Papers - CoRL 2023, https://www.corl2023.org/explore-papers</li>
<li>Google DeepMind Adds Nearly 400000 New Compounds to Berkeley Lab’s Materials Project, https://newscenter.lbl.gov/2023/11/29/google-deepmind-new-compounds-materials-project/</li>
<li>NATURE. Scaling deep learning for materials discovery. Google Deepmind. 29 NOV. - blog.biocomm.ai, https://blog.biocomm.ai/2023/11/29/nature-scaling-deep-learning-for-materials-discovery-google-deepmind-29-nov/</li>
<li>Introducing Ego-Exo4D: A foundational dataset for research on …, https://ai.meta.com/blog/ego-exo4d-video-learning-perception/</li>
<li>A Decade of Advancing the State-of-the-Art in AI Through Open Research - About Meta, https://about.fb.com/news/2023/11/decade-of-advancing-ai-through-open-research/</li>
<li>2023 Conference on Robot Learning - MassRobotics, https://www.massrobotics.org/event/2023-conference-on-robot-learning/</li>
<li>Conference on Robot Learning (CoRL), 2023 | ServiceNow AI Research, https://www.servicenow.com/research/event/2023-corl.html</li>
<li>Awards — CoRL 2023, https://www.corl2023.org/awards</li>
<li>Daily Papers - Hugging Face, https://huggingface.co/papers?q=SparseD</li>
<li>SpareDFF - Qianxu Wang, https://qianxu.wang/SparseDFF/</li>
<li>Professor Yixin Zhu’s research team from Institute for Artificial Intelligence, Peking University, has made significant progress in demonstrating strong generalization in dexterous manipulation for a class of objects., https://www.ai.pku.edu.cn/en/info/1191/2139.htm</li>
<li>SparseDFF: Sparse-View Feature Distillation for One-Shot Dexterous Manipulation - arXiv, https://arxiv.org/html/2310.16838v2</li>
<li>SparseDFF: Sparse-View Feature Distillation for One-Shot Dexterous Manipulation - arXiv, https://arxiv.org/html/2310.16838v1</li>
<li>helloqxwang/SparseDFF - GitHub, https://github.com/helloqxwang/SparseDFF</li>
<li>RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools, https://hshi74.github.io/robocook/</li>
<li>RoboCook: Long-Horizon Elasto-Plastic Object … - Jiajun Wu, https://jiajunwu.com/papers/robocook_corl.pdf</li>
<li>[2307.05973] VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models - arXiv, https://arxiv.org/abs/2307.05973</li>
<li>CoRL 2023 Conference | OpenReview, https://openreview.net/group?id=robot-learning.org/CoRL/2023/Conference</li>
<li>CoRL 2023 - Workshop, https://sites.google.com/view/corl-2023-soft-robots-ws</li>
<li>Recent Breakthrough in AI-Driven Materials Science: Tech Giants Introduce Groundbreaking Models - arXiv, https://arxiv.org/pdf/2402.05799</li>
<li>Millions of new materials discovered with deep learning - UC Berkeley Law, https://www.law.berkeley.edu/wp-content/uploads/2024/02/Millions-of-new-materials-discovered-with-deep-learning-Google-DeepMind.pdf</li>
<li>Millions of new materials discovered with deep learning - Google …, https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/</li>
<li>[PDF] Scaling deep learning for materials discovery - Semantic Scholar, https://www.semanticscholar.org/paper/4e08141db0f2aa01afe903d312011c7d3d7acc46</li>
<li>Scaling deep learning for materials discovery - PubMed, https://pubmed.ncbi.nlm.nih.gov/38030720/</li>
<li>Using Deep Learning for Materials Discovery | E2E Networks Blog, https://www.e2enetworks.com/blog/using-deep-learning-for-materials-discovery</li>
<li>(PDF) Scaling deep learning for materials discovery - ResearchGate, https://www.researchgate.net/publication/376043855_Scaling_deep_learning_for_materials_discovery</li>
<li>Ego-Exo4D Dataset | Project Aria Docs, https://facebookresearch.github.io/projectaria_tools/docs/open_datasets/ego-exo4d</li>
<li>Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives - arXiv, https://arxiv.org/html/2311.18259v3</li>
<li>Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Grauman_Ego-Exo4D_Understanding_Skilled_Human_Activity_from_First-_and_Third-Person_Perspectives_CVPR_2024_paper.pdf</li>
<li>Ego-Exo4D, https://ego-exo4d-data.org/</li>
<li>The Ego-Exo4D Dataset - Antonino Furnari, https://antoninofurnari.github.io/research/egoexo/</li>
<li>Ego-Exo4D Project gives AI training a human touch | UNC-Chapel Hill, https://www.unc.edu/discover/ego-exo4d-project-gives-ai-training-a-human-touch/</li>
<li>Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives - Md Mohaiminul Islam, https://md-mohaiminul.github.io/publication/ego-exo4d/</li>
<li>Understanding Skilled Human Activity from First- and Third-Person Perspectives - Ego-Exo4D, https://ego-exo4d-data.org/paper/ego-exo4d.pdf</li>
<li>Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives - ResearchGate, https://www.researchgate.net/publication/384205548_Ego-Exo4D_Understanding_Skilled_Human_Activity_from_First-_and_Third-Person_Perspectives</li>
<li>Top 10 robotics stories of November 2023 - The Robot Report, https://www.therobotreport.com/top-10-robotic-stories-of-november-2023/</li>
<li>Brown researchers simplify human-to-robot communication with …, https://www.therobotreport.com/brown-university-researchers-simplify-human-to-robot-communication-llms/</li>
<li>Lang2LTL: Translating Natural Language Commands to Temporal Specification with Large Language Models - Humans To Robots Laboratory, https://h2r.cs.brown.edu/wp-content/uploads/liu2022langltl.pdf</li>
<li>Lang2LTL-2: Grounding Spatiotemporal Navigation Commands Using Large Language and Vision-Language Models - Brown Computer Science, https://cs.brown.edu/~gdk/pubs/lang2ltl-2.pdf</li>
<li>November 2023 AI news roundup: What’s new in the world of AI? - Pluralsight, https://www.pluralsight.com/resources/blog/ai-and-data/ai-this-month-november-2023</li>
<li>AI Safety Summit 2023 - GOV.UK, https://www.gov.uk/government/topical-events/ai-safety-summit-2023</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>