<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2024년 11월 AI 및 로봇 주요 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2024년 11월 AI 및 로봇 주요 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2024년 AI 및 로봇 연구 동향</a> / <span>2024년 11월 AI 및 로봇 주요 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2024년 11월 AI 및 로봇 주요 연구 동향</h1>
<h2>1.  2024년 11월 AI 및 로봇공학 연구 동향 개관</h2>
<p>2024년 11월은 인공지능(AI) 및 로봇공학 분야에서 중요한 패러다임의 전환을 목격한 시기로 기록될 것이다. 지난 수년간 이 분야의 발전은 주로 모델과 데이터의 규모를 확장하는 ’규모의 경제’에 의해 주도되어 왔다. 그러나 2024년 11월에 발표된 주요 연구들은 이러한 경향에서 벗어나, 제한된 자원으로 최대의 성능을 이끌어내는 ’효율성의 경제’로의 전환이 본격화되었음을 명확히 보여준다. 본 보고서는 이 시기에 발표된 핵심 연구들을 심층적으로 분석하여, 기술적 성취의 의미를 해석하고 미래 연구 방향에 대한 통찰을 제공하고자 한다.</p>
<p>이러한 패러다임 전환의 중심에는 파운데이션 모델(Foundation Models)의 활용 방식이 자리 잡고 있다. 시각-언어 모델(VLM)을 행동으로 연결하는 시각-언어-행동(VLA) 모델 연구는 이제 성숙기에 접어들었으며, 특히 <code>OpenVLA</code>의 등장은 폐쇄적인 기술 생태계를 개방형 혁신으로 이끄는 기폭제가 될 것으로 평가된다.1</p>
<p><code>OpenVLA</code>는 거대 모델인 Google의 RT-2-X보다 7배나 적은 파라미터로 더 높은 성능을 달성함으로써, 모델의 크기보다 아키텍처와 데이터의 질이 중요할 수 있음을 시사했다.2</p>
<p>데이터 효율성 측면에서는, 값비싼 인-도메인(in-domain) 데이터 수집의 한계를 극복하려는 노력이 구체적인 성과로 나타났다. CoRL 2024에서 구두 발표로 선정된 <code>RAM(Retrieval-Based Affordance Transfer)</code> 프레임워크는 로봇 데이터, 인간-객체 상호작용(HOI) 데이터 등 이종(heterogeneous)의 방대한 외부 데이터를 ’어포던스 메모리’로 구축하고, 이를 검색하여 새로운 작업에 제로샷(zero-shot)으로 적용하는 혁신적인 접근법을 제시했다.3 이는 로봇 학습이 더 이상 고립된 데이터셋에 의존하지 않고, 웹에 존재하는 방대한 시각적, 행동적 지식을 활용할 수 있는 가능성을 열었다.</p>
<p>아키텍처 수준에서는 범용 트랜스포머를 로봇의 특수성에 맞게 변형하려는 시도가 눈에 띄는 성공을 거두었다. <code>Body Transformer(BoT)</code>는 로봇의 물리적 구조(embodiment), 즉 신체 연결 정보를 그래프로 모델링하고 이를 어텐션 마스크에 직접 통합함으로써, 표준 트랜스포머 대비 연산 효율성과 성능을 동시에 향상시키는 방법을 제안했다.5 한편,</p>
<p><code>PoliFormer</code>는 대규모 병렬 시뮬레이션 환경에서 온-폴리시 강화학습을 통해 트랜스포머 정책을 극한까지 훈련시켜, 시뮬레이션에서 학습한 에이전트가 별도의 미세조정 없이 현실 세계에서 높은 항해 성능을 보이는 ’sim-to-real’의 새로운 가능성을 입증했다.6</p>
<p>이러한 학계의 이론적 발전은 Google, Meta와 같은 주요 산업 연구소들의 전략적 방향과 맞물려 더욱 큰 의미를 갖는다. Google DeepMind는 <code>AlphaFold 3</code>의 모델 코드를 공개하고 ’AI for Science Forum’을 개최하며 AI를 인류의 기초 과학 난제 해결을 위한 핵심 도구로 자리매김하려는 비전을 구체화했다.8 반면, Meta AI는</p>
<p><code>Meta Sparsh</code>와 <code>Digit 360</code> 등 인간과 유사한 고해상도 촉각 센서 기술을 대거 발표하며, 로봇이 물리 세계와 상호작용하는 방식의 근본적인 혁신을 추구하고 있음을 보여주었다.10 이는 AGI(범용인공지능)를 향한 경로에 대해 각 기업이 서로 다른 철학적, 전략적 베팅을 하고 있음을 시사한다.</p>
<p>결론적으로, 2024년 11월의 연구들은 개별적인 성과를 넘어 ’효율성’이라는 공통된 목표 아래 유기적으로 연결되어 있다. <code>OpenVLA</code>의 파라미터 효율성, <code>RAM</code>의 데이터 효율성, <code>Body Transformer</code>의 연산 효율성은 모두 로봇 AI 기술의 실용화 및 대중화를 가로막는 핵심 장벽을 해결하려는 시도이다. 이러한 ’효율성으로의 전환’은 단순히 기술적 진보를 넘어, 향후 로봇 AI 연구의 평가 기준 자체를 바꾸는 중요한 변곡점이 될 것이다. 이제 성능의 절대적 수치뿐만 아니라, ’얼마나 적은 자원(데이터, 컴퓨팅, 파라미터)으로 그 성능을 달성했는가’가 연구의 가치를 판단하는 핵심 척도로 부상할 것이다. 이는 거대 자본 없이도 창의적인 아이디어로 기여할 수 있는 학계와 중소 연구팀의 역할을 재조명하며, 로봇 연구 생태계의 다양성과 건강성을 증진시키는 계기가 될 수 있다.</p>
<h2>2.  주요 학술대회 동향: Conference on Robot Learning (CoRL) 2024 심층 분석</h2>
<p>2024년 11월 AI 및 로봇공학 분야의 학술적 논의를 주도한 가장 중요한 행사는 단연 Conference on Robot Learning (CoRL) 2024였다. 로봇과 기계 학습의 교차점에 초점을 맞춘 이 학회는 해당 분야에서 가장 권위 있는 국제 학회 중 하나로 인정받고 있다.</p>
<h3>2.1 CoRL 2024 개요</h3>
<p>CoRL 2024는 2024년 11월 6일부터 9일까지 독일 뮌헨에서 개최되었다.12 이 학회는 이번 달에 발표된 가장 영향력 있고 혁신적인 연구들의 집결지 역할을 했다. 학회의 프로그램은 구두 발표, 포스터 세션, 그리고 특정 주제를 심도 있게 다루는 워크숍으로 구성되었다. 특히 ’가정용 로봇을 위한 평생 학습 워크숍(Workshop on Lifelong Learning for Home Robots)’과 같은 세션들은 학계의 현재 관심사가 어디에 집중되어 있는지를 명확히 보여주었다.14 평생 학습(Lifelong Learning), 시뮬레이션-현실 전이(Sim-to-Real Transfer), 그리고 인간-로봇 상호작용(Human-Robot Interaction)과 같은 주제들이 주요 의제로 부상하며 활발한 논의가 이루어졌다.14</p>
<h3>2.2 Outstanding Paper Award 수상 연구 분석</h3>
<p>CoRL 2024의 최우수 논문(Outstanding Paper Award) 선정 결과는 현재 로봇 학습 연구의 최전선이 어디인지를 명확하게 보여주는 지표이다. 수상작들은 각기 다른 문제 영역을 다루고 있지만, ’일반화(Generalization)’라는 하나의 거대한 목표를 향해 수렴하는 경향을 보였다.</p>
<ul>
<li>
<p><strong>“PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators”</strong>: 이 연구는 대규모 강화학습과 트랜스포머 아키텍처를 결합하여 실내 항해(indoor navigation) 문제에서 전례 없는 성능을 달성했다. 가장 주목할 만한 성과는 시뮬레이션에서만 훈련된 정책이 별도의 적응 과정 없이 실제 로봇(LoCoBot, Stretch RE-1)에 성공적으로 배포되어 높은 성능을 보였다는 점이다. 이는 ’환경과 로봇 형태(embodiment)’에 대한 일반화 능력을 입증한 것으로, 비용과 시간이 많이 소요되는 실제 로봇 훈련의 필요성을 크게 줄일 수 있는 ’sim-to-real’의 새로운 가능성을 열었다는 점에서 높은 평가를 받았다.7 (본 연구에 대한 상세 분석은 V장에서 이어진다.)</p>
</li>
<li>
<p><strong>“One Model to Drift Them All: Physics-Informed Conditional Diffusion Model for Driving at the Limits”</strong>: 이 논문은 자율주행의 극한 영역인 ‘드리프트’ 제어에 도전했다. 물리 법칙에 대한 사전 지식을 통합한 조건부 확산 모델(conditional diffusion model)을 사용하여, 단 하나의 모델로 Toyota Supra와 Lexus LC 500이라는 서로 다른 두 차종과 다양한 도로 조건(마른 노면, 젖은 노면)에서 안정적인 드리프트를 성공시켰다. 이는 ’차량 모델과 도로 조건’에 대한 뛰어난 일반화 성능을 보여준 사례로, 모델의 독창성과 실제적 유용성 측면에서 큰 주목을 받았다.16</p>
</li>
<li>
<p><strong>기타 주요 수상작</strong>: 이 외에도 여러 연구들이 각자의 분야에서 중요한 기여를 했다. “ReMix: Optimizing Data Mixtures for Large Scale Imitation Learning“은 대규모 모방 학습에서 데이터 혼합 비율을 최적화하는 방법을 제시했고, “Equivariant Diffusion Policy“는 기하학적 대칭성(등변성, equivariance)을 확산 정책 모델에 통합하여 데이터 효율성과 일반화 성능을 높였다. 또한 “HumanPlus: Humanoid Shadowing and Imitation from Humans“는 인간의 동작을 실시간으로 모방하여 휴머노이드 로봇을 제어하는 기술을, “OpenVLA: An Open-Source Vision-Language-Action Model“는 강력한 성능의 VLA 모델을 오픈소스로 공개하여 연구 생태계에 기여한 공로를 인정받았다.16</p>
</li>
</ul>
<p>이러한 수상 결과는 로봇 학습 연구가 특정 작업(task-specific)이나 특정 환경에 국한된 솔루션을 넘어, 하나의 잘 훈련된 모델로 다양한 상황과 조건에 강건하게 대처하는 ’범용성(generality)’을 확보하는 단계로 본격적으로 진입했음을 보여준다. 이는 자연어 처리 분야에서 GPT와 같은 파운데이션 모델이 등장하며 나타난 패러다임 전환과 유사한 흐름이다. 로봇공학 분야에서도 특정 도메인의 데이터를 대규모로 학습하여 ’로봇 파운데이션 모델’을 구축하고, 이를 다양한 하위 작업에 적용하려는 시도가 CoRL 2024를 기점으로 본격화되고 있음을 알 수 있다.</p>
<p>이러한 변화는 향후 로봇 연구의 방향성에 중요한 시사점을 던진다. 미래의 로봇 연구는 단순히 새로운 알고리즘이나 아키텍처를 발명하는 것을 넘어, ’어떤 데이터를, 어떻게 큐레이션하여, 어떤 종류의 파운데이션 모델을 훈련할 것인가’에 대한 고민이 더욱 중요해질 것이다. 이는 Open X-Embodiment와 같은 대규모 공개 데이터셋의 가치를 더욱 부각시키며, 데이터 엔지니어링과 고품질 시뮬레이션 기술이 로봇 AI 연구의 핵심 경쟁력으로 자리 잡게 될 것임을 예고한다.</p>
<h3>2.3 기타 11월 개최 학회</h3>
<p>CoRL 외에도 11월에는 여러 중요한 AI 및 로봇 관련 학술 행사가 개최되었다.</p>
<ul>
<li>
<p><strong>International Conference on Robotics and Automation Engineering (ICRAE 2024)</strong>: 11월 8일부터 10일까지 싱가포르에서 개최된 이 학회는 로봇 시스템, 자동화 공학, 지능형 제어 기술 분야의 최신 연구 성과를 공유하는 장을 마련했다.20</p>
</li>
<li>
<p><strong>International Conference on Robotics and Mechatronics (ICROM 2024)</strong>: 11월 23일부터 25일까지 일본 도쿄에서 열렸으며, 로봇공학과 메카트로닉스의 융합 기술에 대한 논의가 이루어졌다.20</p>
</li>
<li>
<p><strong>4th International Conference on AI ML, Data Science and Robotics</strong>: 11월 28일부터 29일까지 포르투갈에서 하이브리드(온/오프라인 병행) 형태로 개최되었다. 이 행사는 AI, 머신러닝, 데이터 과학, 로보틱스 등 다양한 분야의 전문가들이 모여 융합 연구의 현재와 미래를 논의하는 기회를 제공했다.21</p>
</li>
</ul>
<p>아래 표는 2024년 11월에 개최된 주요 AI 및 로봇공학 관련 학술대회를 요약한 것이다.</p>
<table><thead><tr><th>학회명 (Conference Name)</th><th>개최 기간 (Dates)</th><th>개최 장소 (Location)</th><th>주요 특징 및 발표 내용 (Key Features &amp; Announcements)</th></tr></thead><tbody>
<tr><td>CoRL 2024</td><td>11월 6-9일</td><td>독일, 뮌헨</td><td>로봇 학습 분야 최상위 학회. 일반화, Sim-to-Real, 파운데이션 모델 활용이 핵심 주제. PoliFormer, OpenVLA, RAM 등 주요 연구 발표.</td></tr>
<tr><td>ICRAE 2024</td><td>11월 8-10일</td><td>싱가포르</td><td>로봇 자동화 공학 및 지능형 제어 기술에 중점. 산업계 적용 사례 및 특정 기술 분야 발전 논의.</td></tr>
<tr><td>ICROM 2024</td><td>11월 23-25일</td><td>일본, 도쿄</td><td>로봇공학과 메카트로닉스 융합 분야. 하드웨어와 소프트웨어 통합 기술 발표.</td></tr>
<tr><td>4th Intl. Conf. on AI ML, Data Science and Robotics</td><td>11월 28-29일</td><td>포르투갈, 포르티망 (하이브리드)</td><td>AI, ML, 로보틱스 등 융합 연구 분야. 학계와 산업계 전문가들의 네트워킹 및 아이디어 교류.</td></tr>
</tbody></table>
<p>이 표를 통해 11월의 학술 지형에서 CoRL 2024가 학문적 깊이와 영향력 측면에서 다른 행사들을 압도했음을 명확히 알 수 있다. 이는 이어지는 심층 분석이 CoRL에서 발표된 연구들에 집중하는 이유를 정당화하며, 보고서의 전체적인 내러티브 흐름을 설정하는 역할을 한다.</p>
<h2>3.  심층 분석 1: OpenVLA - 개방형 시각-언어-행동 모델의 등장과 파급 효과</h2>
<p>2024년 11월 로봇공학 커뮤니티에 가장 큰 파장을 일으킨 사건 중 하나는 <code>OpenVLA(Open-Source Vision-Language-Action Model)</code>의 등장이었다. 이는 단순히 또 하나의 고성능 모델이 발표된 것을 넘어, 로봇 학습 연구의 생태계 자체를 변화시킬 잠재력을 지닌 중요한 이정표로 평가된다.</p>
<h3>3.1 OpenVLA 개요</h3>
<p><code>OpenVLA</code>는 Stanford, UC Berkeley, Google DeepMind, Toyota Research Institute 등 세계 유수의 연구 기관들이 협력하여 개발한 70억(7B) 파라미터 규모의 시각-언어-행동(VLA) 모델이다.1 이 모델의 가장 큰 의의는 이름에서 알 수 있듯이 ’오픈소스’라는 점에 있다. 이전까지 Google의 RT-2, RT-X와 같은 고성능 VLA 모델들은 대부분 내부적으로만 연구되거나 제한적으로만 접근이 허용된 ‘폐쇄적(closed)’ 모델이었다. 반면,</p>
<p><code>OpenVLA</code>는 모델 가중치와 훈련 코드를 전면 공개하는 MIT 라이선스를 채택함으로써, 전 세계 누구나 이 강력한 모델에 접근하고, 이를 기반으로 새로운 연구를 수행할 수 있는 길을 열었다.1</p>
<h3>3.2 기술적 아키텍처 분석</h3>
<p><code>OpenVLA</code>는 기존의 강력한 사전 훈련된 모델들을 효과적으로 결합하여 높은 성능과 효율성을 달성했다.</p>
<ul>
<li>
<p><strong>백본(Backbone)</strong>: 모델의 핵심은 Llama 2 7B 언어 모델이다. 언어적 이해와 추론 능력을 담당하는 이 언어 모델에 시각 정보를 결합하는 구조를 가진다.1</p>
</li>
<li>
<p><strong>시각 인코더(Visual Encoder)</strong>: <code>OpenVLA</code>의 독창적인 부분은 시각 정보를 처리하는 방식에 있다. 단일 시각 모델에 의존하는 대신, 서로 다른 강점을 가진 두 개의 사전 훈련된 시각 모델, 즉 DINOv2와 SigLIP의 특징(feature)을 융합하여 사용한다. DINOv2는 객체의 기하학적, 구조적 특징을 잘 포착하는 것으로 알려져 있으며, SigLIP은 이미지와 텍스트 간의 의미론적 관계를 잘 이해한다. 이 두 모델의 특징을 결합함으로써 <code>OpenVLA</code>는 더 풍부하고 다각적인 시각적 이해 능력을 확보하게 된다.1</p>
</li>
<li>
<p><strong>작동 방식</strong>: <code>OpenVLA</code>는 사용자의 언어 지시(예: “빨간색 컵을 분홍색 접시 위로 옮겨줘”)와 로봇의 카메라로부터 입력된 현재 작업 공간의 이미지를 동시에 입력받는다. 시각 인코더가 이미지에서 시각적 특징을 추출하고, 이는 언어 지시와 함께 Llama 2 모델에 전달된다. 그러면 모델은 이 정보를 바탕으로 로봇이 수행해야 할 다음 행동을 예측하여 출력한다. 이 행동은 로봇 팔 끝(엔드 이펙터)의 7차원 자유도(7-DOF)에 대한 미세한 변화량, 즉 델타 값(<code>(x, y, z, roll, pitch, yaw, gripper)</code>)의 형태로 생성된다.22</p>
</li>
<li>
<p><strong>효율성을 위한 디자인</strong>: 개발 과정에서 연구팀은 224x224 픽셀과 384x384 픽셀의 두 가지 이미지 해상도를 비교했다. 흥미롭게도, 더 높은 해상도가 VLM 벤치마크에서는 종종 성능 향상으로 이어지지만, VLA 작업에서는 유의미한 성능 차이를 보이지 않았다. 반면, 384x384 해상도는 훈련 시간이 3배 더 소요되었다. 이에 연구팀은 효율성을 극대화하기 위해 224x224 픽셀 해상도를 최종적으로 채택했다.2</p>
</li>
</ul>
<h3>3.3 훈련 데이터 및 방법론</h3>
<p><code>OpenVLA</code>의 강력한 일반화 성능은 방대하고 다양한 데이터셋에 기반한다.</p>
<ul>
<li>
<p><strong>데이터셋</strong>: 모델 훈련에는 <code>Open X-Embodiment</code>라는 대규모 공개 데이터셋에서 선별된 97만 개의 실제 로봇 조작 궤적(trajectory) 데이터가 사용되었다.1 이 데이터셋은 수십 종류의 서로 다른 로봇 팔, 수백 가지의 작업, 수천 개의 다양한 환경에서 수집된 데이터를 포함하고 있어, 모델이 특정 로봇이나 환경에 과적합되지 않고 범용적인 조작 능력을 학습하는 데 결정적인 역할을 했다.</p>
</li>
<li>
<p><strong>훈련 과정</strong>: 훈련은 64개의 NVIDIA A100 GPU로 구성된 클러스터에서 15일 동안 진행되었다. 이처럼 대규모 모델을 효율적으로 훈련하기 위해 PyTorch FSDP(Fully Sharded Data Parallel)와 Flash-Attention과 같은 최신 분산 훈련 및 어텐션 최적화 기술이 활용되었다.23</p>
</li>
</ul>
<h3>3.4 성능 평가 및 비교 분석</h3>
<p><code>OpenVLA</code>는 다양한 평가 시나리오에서 기존 모델들을 능가하는 인상적인 성능을 보여주었다.</p>
<ul>
<li><strong>제로샷(Zero-Shot) 성능</strong>: <code>OpenVLA</code>는 훈련 데이터에 포함된 로봇(예: WidowX 로봇)과 환경에 대해서는 별도의 추가 학습 없이도 즉시 뛰어난 제어 성능을 보였다. 이는 기존의 범용 로봇 정책인 RT-1-X나 Octo를 능가하는 수준이다.24 특히, 70억 개의 파라미터를 가진</li>
</ul>
<p><code>OpenVLA</code>가 550억 개 파라미터의 거대 비공개 모델인 Google의 RT-2-X와 비교했을 때, 29개의 공통 평가 작업에서 평균적으로 16.5% 더 높은 절대 성공률을 기록했다는 점은 매우 놀랍다. 이는 모델의 크기가 성능을 결정하는 유일한 요소가 아니며, 아키텍처 설계와 데이터의 다양성이 얼마나 중요한지를 명확히 보여주는 결과이다.1</p>
<ul>
<li>
<p><strong>미세조정(Fine-tuning) 성능</strong>: <code>OpenVLA</code>의 진정한 강점은 새로운 환경이나 작업에 대한 빠른 적응 능력에 있다. 소량의 시연 데이터만으로도 새로운 로봇이나 작업을 위한 미세조정이 효율적으로 가능하다. 특히 여러 객체가 있고 복잡한 언어 지시를 이해해야 하는 다중 작업 환경에서는, 처음부터 학습하는 Diffusion Policy와 같은 생성 모델 기반의 모방 학습 방법보다 20.4% 더 높은 성공률을 보였다. 이는 <code>OpenVLA</code>가 Open X-Embodiment 데이터셋을 통해 사전 학습한 범용적인 시각-언어-행동 지식이 새로운 작업 학습에 강력한 기반(prior)으로 작용하기 때문이다.1</p>
</li>
<li>
<p><strong>파라미터 효율적 미세조정(PEFT)</strong>: <code>OpenVLA</code>는 LoRA(Low-Rank Adaptation)와 같은 최신 PEFT 기법을 완벽하게 지원한다. 실험 결과, 전체 모델 파라미터의 단 1.4%에 해당하는 부분만 미세조정해도 전체 모델을 미세조정하는 것과 동등한 성능을 달성할 수 있었다. 이는 고가의 GPU 장비가 없는 연구실이나 개인 개발자도 소비자용 GPU를 사용하여 <code>OpenVLA</code>를 자신만의 작업에 맞게 커스터마이징할 수 있음을 의미하며, 기술의 접근성을 획기적으로 높이는 중요한 성과이다.23</p>
</li>
</ul>
<p>아래 표는 <code>OpenVLA</code>를 기존의 주요 VLA 모델들과 정량적으로 비교한 것이다.</p>
<table><thead><tr><th>모델명 (Model Name)</th><th>파라미터 수 (Parameters)</th><th>기반 VLM (Base VLM)</th><th>훈련 데이터 (Training Data)</th><th>공개 여부 (Open Source?)</th><th>주요 성능 지표 (Key Performance Metric)</th></tr></thead><tbody>
<tr><td>RT-1-X</td><td>55B</td><td>PaLM-E</td><td>Robotics Transformer X</td><td>아니오</td><td>기준 모델</td></tr>
<tr><td>Octo</td><td>8B</td><td>-</td><td>Open X-Embodiment (Magic Soup)</td><td>예</td><td>RT-1-X 대비 성능 개선</td></tr>
<tr><td>RT-2-X</td><td>55B</td><td>PaLM-E</td><td>Webli + Robotics Data</td><td>아니오</td><td>RT-1-X 대비 16% 성능 향상</td></tr>
<tr><td><strong>OpenVLA</strong></td><td><strong>7B</strong></td><td><strong>Llama 2</strong></td><td><strong>Open X-Embodiment (Magic Soup++)</strong></td><td><strong>예</strong></td><td><strong>RT-2-X 대비 16.5% 성능 향상</strong></td></tr>
</tbody></table>
<p>이 표는 <code>OpenVLA</code>의 혁신성을 명확하게 보여준다. <code>OpenVLA</code>는 RT-2-X보다 파라미터 수가 약 1/7에 불과함에도 불구하고 더 높은 성능을 달성했으며, 동시에 모델과 코드를 완전히 공개했다. 이는 <code>OpenVLA</code>가 기술적 우위뿐만 아니라, 로봇 연구 커뮤니티 전체에 실질적인 가치를 제공하고 있음을 강조한다.</p>
<p><code>OpenVLA</code>의 등장은 VLA 연구 분야의 ‘아이폰 모멘트’ 또는 컴퓨터 비전 분야의 ’ImageNet/AlexNet 모멘트’에 비견될 수 있다. 과거 소수의 거대 기업만이 접근할 수 있었던 고성능 VLA 기술이, 이제는 HuggingFace를 통한 쉬운 접근성, LoRA를 통한 저비용 미세조정, REST API를 통한 간편한 배포 등 잘 갖춰진 개발자 생태계와 함께 공개되었다. 이러한 기술적 장벽의 해소는 전 세계의 수많은 연구자들이 <code>OpenVLA</code>라는 표준 플랫폼 위에서 이전에는 불가능했던 다양하고 창의적인 아이디어(새로운 로봇, 새로운 작업, 새로운 상호작용 방식)를 실험하고 구현하는 것을 가능하게 할 것이다. 이는 향후 VLA 연구의 경쟁 구도가 ’누가 더 큰 모델을 만드는가’에서 ’누가 <code>OpenVLA</code>를 더 창의적으로 활용하고 미세조정하는가’로 전환될 것임을 시사한다. 즉, 연구의 무게 중심이 ’모델 아키텍처’에서 ’데이터와 애플리케이션’으로 이동하게 되며, 이는 로봇공학 기술의 상업화와 실용화를 크게 앞당기는 결정적인 촉매제가 될 것이다.</p>
<h2>4.  심층 분석 2: RAM - 검색 기반 어포던스 전이를 통한 제로샷 조작 기술</h2>
<p>로봇이 인간처럼 다양한 도구와 객체를 능숙하게 다루기 위해서는 방대한 양의 경험 데이터가 필요하다. 그러나 실제 로봇을 이용해 이러한 데이터를 수집하는 것은 엄청난 비용과 시간을 요구하며, 이는 로봇 학습의 가장 큰 병목 중 하나로 지적되어 왔다. CoRL 2024에서 구두 발표(Oral Presentation)로 선정된 <code>RAM(Retrieval-Based Affordance Transfer)</code> 프레임워크는 이러한 데이터 부족 문제를 정면으로 돌파하는 새로운 해법을 제시한다.25</p>
<h3>4.1 RAM 프레임워크 개요</h3>
<p><code>RAM</code>은 특정 작업에 대한 시연 데이터가 전혀 없는 상태, 즉 제로샷(zero-shot) 환경에서 로봇이 복잡한 조작 작업을 수행할 수 있도록 하는 혁신적인 프레임워크이다.3</p>
<p><code>RAM</code>의 핵심 철학은 ’처음부터 모든 것을 학습하지 말고, 세상에 존재하는 방대한 지식을 빌려 쓰자’는 것이다. 여기서 지식이란 로봇 시연 영상, 사람이 물건을 다루는 영상(HOI, Human-Object Interaction), 심지어 인터넷에 떠도는 이미지나 만화 영상까지 포함하는 광범위한 시각적 데이터를 의미한다.3</p>
<p><code>RAM</code>은 이러한 이종(heterogeneous)의 ‘아웃-오브-도메인(out-of-domain)’ 데이터로부터 객체와 상호작용하는 방법에 대한 본질적인 정보, 즉 ’어포던스(affordance, 행동유도성)’를 추출하고, 이를 현재 로봇이 마주한 새로운 작업에 창의적으로 ’전이(transfer)’하여 사용한다.28</p>
<p>이 접근법은 로봇 학습에서 ‘지식의 표현’ 방식을 근본적으로 바꾸는 시도이다. 기존의 엔드-투-엔드(end-to-end) 딥러닝 모델들이 모든 지식을 거대한 신경망의 암시적인 가중치(implicit weights) 안에 압축하여 저장하려 했다면, <code>RAM</code>은 지식을 ’명시적인 외부 메모리(explicit external memory)’의 형태로 구축하고, 필요할 때마다 가장 관련 있는 지식을 ’검색(retrieve)’하여 사용하는 하이브리드 방식을 채택한다. 이는 최근 자연어 처리 분야에서 LLM의 한계를 보완하기 위해 각광받는 RAG(Retrieval-Augmented Generation) 기술과 철학적으로 맥을 같이 한다. RAG가 외부 문서 데이터베이스를 검색하여 LLM의 환각(hallucination)을 줄이고 최신 정보를 반영하는 것처럼, <code>RAM</code>은 외부 행동 데이터베이스를 검색하여 로봇의 제로샷 일반화 성능을 높이고 데이터 부족 문제를 해결한다. 이런 관점에서 <code>RAM</code>은 ’로봇공학을 위한 RAG(RAG for Robotics)’의 성공적인 첫 사례로 평가될 수 있다.</p>
<h3>4.2 핵심 방법론: 검색 및 전이(Retrieve-and-Transfer) 파이프라인</h3>
<p><code>RAM</code>의 작동 방식은 크게 네 단계로 구성된 체계적인 파이프라인을 따른다.</p>
<ol>
<li>
<p><strong>어포던스 메모리 구축 (Affordance Memory Construction)</strong>: 파이프라인의 첫 단계는 다양한 데이터 소스로부터 지식을 추출하여 재사용 가능한 형태로 저장하는 것이다. <code>RAM</code>은 로봇 데이터셋, HOI 데이터셋, 인터넷 이미지 등에서 ‘어디를(where to act)’ 상호작용 지점과 ‘어떻게(how to act)’ 상호작용 방식을 나타내는 2D 어포던스 정보를 추출한다. 예를 들어, ’문을 연다’는 작업에 대해 문손잡이의 위치와 손잡이를 잡고 당기는 궤적 등을 2D 이미지 상의 정보로 표현하여 대규모 ’어포던스 메모리’에 저장한다.4</p>
</li>
<li>
<p><strong>계층적 검색 (Hierarchical Retrieval)</strong>: 로봇에게 새로운 작업 지시(예: “서랍을 열어라”)와 현재 장면을 담은 RGBD 이미지가 주어지면, <code>RAM</code>은 어포던스 메모리에서 가장 적합한 과거 경험을 찾아낸다. 이 검색 과정은 효율성을 위해 3단계의 계층적 구조를 가진다. 먼저 언어 지시를 기반으로 의미적으로 관련된 작업들을 필터링하고, 그 다음 전체 이미지의 시각적 유사성을 비교하며, 마지막으로 객체 수준의 세부적인 특징을 비교하여 가장 유사한 단 하나의 시연 사례를 최종적으로 선택한다.4</p>
</li>
<li>
<p><strong>2D 어포던스 전이 (2D Affordance Transfer)</strong>: 가장 유사한 시연 사례(소스 도메인 <code>S</code>)가 선택되면, 그 사례에 담긴 2D 어포던스 정보를 현재 로봇이 보고 있는 장면(타겟 도메인 <code>T</code>)으로 옮겨와야 한다. <code>RAM</code>은 DINO와 같은 강력한 시각 파운데이션 모델(VFM)을 활용하여 소스 이미지와 타겟 이미지 간의 픽셀 단위 대응 관계(pixel-wise correspondence)를 찾아낸다. 이를 통해 소스 이미지의 서랍 손잡이에 해당하는 픽셀 영역을 타겟 이미지의 서랍 손잡이 픽셀 영역으로 정확하게 매핑하고, 관련된 상호작용 궤적 정보 또한 함께 전이시킨다.4</p>
</li>
<li>
<p><strong>3D 어포던스 리프팅 (3D Affordance Lifting)</strong>: 마지막으로, 타겟 이미지 위에 전이된 2D 어포던스 정보는 로봇이 실제로 실행할 수 있는 3D 공간의 행동으로 변환되어야 한다. 이 과정을 ’리프팅(lifting)’이라고 부른다. <code>RAM</code>은 RGBD 카메라로부터 얻은 깊이(depth) 정보를 활용하여 2D 픽셀 좌표를 3D 공간 좌표로 변환한다. 그리고 샘플링 기반의 최적화 기법을 통해 전이된 궤적을 따르면서 물리적으로 가장 실현 가능한 3D 로봇 팔 궤적, 즉 최종적인 3D 어포던스(<code>A_3D</code>)를 생성한다. 이 3D 어포던스는 로봇의 모션 플래너에 의해 직접 실행될 수 있다.4</p>
</li>
</ol>
<h3>4.3 실험 및 확장성</h3>
<p><code>RAM</code>은 시뮬레이션과 실제 로봇 실험 모두에서 기존의 제로샷 조작 연구들을 큰 차이로 능가하는 성능을 입증했다.3 서랍 열기, 전자레인지 문 열기, 병 집기 등 다양한 일상적인 작업을 어떠한 사전 시연 없이도 성공적으로 수행해냈다.27</p>
<p><code>RAM</code>의 프레임워크는 다양한 흥미로운 응용으로 쉽게 확장될 수 있다.</p>
<ul>
<li>
<p><strong>원샷 시각 모방 (One-Shot Visual Imitation)</strong>: 검색 파이프라인을 사용하는 대신, 사용자가 특정 시연 영상(예: 사람이 휴지를 뽑는 영상)을 직접 제공하면, <code>RAM</code>은 이를 즉시 모방하여 로봇이 동일한 작업을 수행하게 할 수 있다. 심지어 ’톰과 제리’와 같은 만화 영상에서 캐릭터가 서랍을 여는 장면을 보고 로봇이 실제 서랍을 여는 것을 모방하는 데 성공했는데, 이는 시각 파운데이션 모델의 놀라운 일반화 능력 덕분에 현실과 만화라는 거대한 도메인 격차를 극복할 수 있음을 보여준다.27</p>
</li>
<li>
<p><strong>LLM/VLM 통합</strong>: “부엌을 정리해줘“와 같은 복잡하고 장기적인(long-horizon) 지시가 주어지면, LLM/VLM이 이를 “1. 냉장고 문을 연다. 2. 우유를 꺼낸다. 3. 식탁 위에 놓는다“와 같은 작은 단위 작업으로 분해하고, <code>RAM</code>이 각 단위 작업을 순차적으로 수행하는 방식으로 쉽게 통합될 수 있다.27</p>
</li>
</ul>
<p>이러한 <code>RAM</code>의 접근법은 로봇 지능의 미래에 대한 중요한 통찰을 제공한다. 로봇 지능이 거대한 단일 모델에 모든 것을 의존하는 형태가 아니라, 방대한 외부 지식 베이스와 능동적으로 상호작용하는 형태로 발전할 가능성을 제시한 것이다. 이는 앞으로의 로봇 연구가 더 나은 어포던스 표현법을 개발하고, 웹상의 비디오, 이미지, 텍스트 등 세상에 존재하는 모든 형태의 데이터로부터 행동 지식을 자동으로 추출하여 거대한 ’로봇 행동 지식 베이스’를 구축하는 방향으로 나아갈 수 있음을 시사한다. 궁극적으로 이는 로봇 커뮤니티를 위한 위키피디아나 지식 그래프처럼, 로봇의 지능을 집단적으로 발전시키기 위한 거대한 공공 지식 인프라 구축의 필요성을 제기한다.</p>
<h2>5.  심층 분석 3: PoliFormer 및 Body Transformer - 로봇 제어를 위한 트랜스포머 아키텍처의 진화</h2>
<p>트랜스포머 아키텍처는 자연어 처리와 컴퓨터 비전 분야에 혁명을 일으킨 후, 로봇공학 분야에서도 그 영향력을 빠르게 확장하고 있다. 하지만 표준 트랜스포머는 본래 순차적인 텍스트 데이터를 처리하기 위해 설계되었기 때문에, 물리적 신체(embodiment)를 가지고 시공간과 상호작용하는 로봇의 고유한 특성을 완벽하게 반영하지는 못한다는 문제의식이 꾸준히 제기되어 왔다.5 2024년 11월에 발표된 <code>PoliFormer</code>와 <code>Body Transformer</code>는 이러한 한계를 극복하고, 트랜스포머를 로봇 제어 문제에 더욱 효과적으로 적용하기 위한 두 가지 상보적인 진화 방향을 제시하는 대표적인 사례이다.</p>
<h3>5.1 PoliFormer: 대규모 강화학습을 통한 항해 전문가</h3>
<p><code>PoliFormer</code>는 ‘스케일 업(scale up)’ 전략, 즉 방대한 데이터와 컴퓨팅 자원을 활용하여 모델의 성능 한계를 돌파하는 접근법의 성공을 보여준다. 이 연구의 핵심 기여는 대규모 병렬 시뮬레이션 환경에서 온-폴리시(on-policy) 강화학습을 통해 트랜스포머 기반 정책을 수억 번의 상호작용 데이터로 훈련시켰다는 점이다. 그 결과, 시뮬레이션에서만 학습했음에도 불구하고 실제 로봇에 배포했을 때 별도의 미세조정 없이도(zero-shot sim-to-real transfer) 기존의 어떤 방법보다 뛰어난 항해 성능을 달성했다.6</p>
<h4>5.1.1 아키텍처 분석</h4>
<p><code>PoliFormer</code>는 항해 작업의 특성을 고려하여 설계된 완전한 트랜스포머 기반 아키텍처이다.</p>
<ul>
<li>
<p><strong>시각 인코더 (Vision Encoder)</strong>: 에이전트의 시각 입력(RGB 이미지)을 처리하기 위해, 사전 훈련된 강력한 시각 파운데이션 모델인 DINOv2 ViT를 사용한다. 훈련 중 이 인코더의 가중치는 고정(frozen)되는데, 이는 시뮬레이션 환경의 시각적 특징에 과적합되는 것을 방지하고 현실 세계로의 일반화 성능을 높이는 데 결정적인 역할을 한다.32</p>
</li>
<li>
<p><strong>상태 인코더 (State Encoder)</strong>: DINOv2로부터 추출된 시각 특징과 목표 정보(예: “사과를 찾아라“는 텍스트 지시)를 입력받아, 트랜스포머 인코더를 통해 현재 상황을 종합적으로 요약한 상태 벡터 <code>s</code>를 생성한다.32</p>
</li>
<li>
<p><strong>인과적 디코더 (Causal Decoder)</strong>: <code>PoliFormer</code> 아키텍처의 핵심적인 부분으로, GPT와 같은 언어 모델에서 사용되는 인과적 트랜스포머 디코더 구조를 채택했다. 이 디코더는 현재 시점 <code>t</code>까지의 상태 벡터 시퀀스 <code>{s_0, s_1,..., s_t}</code>를 입력받아, 과거의 경험을 바탕으로 장기적인 기억과 추론을 수행한다. 이를 통해 에이전트는 복잡한 환경에서 길을 잃지 않고 체계적인 탐색을 할 수 있다. 특히, 훈련 및 추론 속도를 높이기 위해 KV-캐시(Key-Value cache) 최적화 기법을 적용했다. 이는 이전 타임스텝의 어텐션 계산 결과를 재사용하여, 롤아웃 시 매 스텝마다 전체 시퀀스를 다시 계산하는 비효율을 제거한다. 그 결과, 연산 복잡도를 시퀀스 길이에 대한 제곱 시간(<code>$O(t^2)$</code>)에서 선형 시간(<code>$O(t)$</code>)으로 획기적으로 줄여 대규모 훈련을 가능하게 했다.32</p>
</li>
</ul>
<h4>5.1.2 성능</h4>
<p><code>PoliFormer</code>는 다양한 실내 항해 벤치마크에서 압도적인 성능을 기록했다. 특히, 복잡한 집안일을 모사한 Chores-𝕊 벤치마크에서는 85.5%의 성공률을 달성하여, 기존 최고 성능(SOTA) 모델 대비 28.5%라는 경이적인 절대 성능 향상을 보여주었다.6</p>
<h3>5.2 Body Transformer (BoT): 로봇의 몸을 아키텍처에 새기다</h3>
<p><code>PoliFormer</code>가 ‘양적’ 접근법의 극치를 보여준다면, <code>Body Transformer(BoT)</code>는 ‘질적’ 접근법, 즉 아키텍처 자체에 도메인 지식을 주입하여 효율성을 극대화하는 ‘구조적 편향 주입(structure injection)’ 전략의 성공 사례이다. <code>BoT</code>의 핵심 아이디어는 로봇의 물리적 신체 구조(embodiment)가 그 자체로 유용한 정보이며, 이를 정책 네트워크의 귀납적 편향(inductive bias)으로 활용해야 한다는 것이다.5</p>
<h4>5.2.1 아키텍처 분석</h4>
<p><code>BoT</code>는 로봇의 신체 구조를 트랜스포머의 어텐션 메커니즘에 직접 통합한다.</p>
<ul>
<li>
<p><strong>그래프 표현 (Graph Representation)</strong>: 먼저 로봇의 신체를 그래프로 모델링한다. 각 관절의 센서(예: 각도 센서)와 액추에이터(모터)를 그래프의 노드(node)로 정의하고, 이들을 연결하는 물리적인 링크(link)를 엣지(edge)로 정의한다.5</p>
</li>
<li>
<p><strong>마스크드 어텐션 (Masked Attention)</strong>: <code>BoT</code>의 핵심은 표준 트랜스포머의 ‘완전 연결된(fully connected)’ 어텐션을 ’마스크드 어텐션’으로 대체하는 것이다. 어텐션 마스크 행렬 <code>$M$</code>을 로봇 신체 그래프의 인접 행렬 <code>$A$</code>와 단위 행렬 <code>$I_n$</code>의 합, 즉 <code>$M = I_n + A$</code>로 정의한다. 이 마스크는 셀프-어텐션 계산 시 적용되어, 각 노드(관절)가 오직 자기 자신과 물리적으로 직접 연결된 이웃 노드들의 정보에만 어텐션(attend)을 수행하도록 강제한다.5</p>
</li>
<li>
<p><strong>계층적 정보 흐름</strong>: 이러한 마스크드 어텐션 레이어를 여러 층으로 쌓으면, 정보는 자연스럽게 로봇의 신체 구조를 따라 전파된다. 첫 번째 레이어에서는 각 관절이 이웃 관절의 정보만을 처리하여 지역적인(local) 계산을 수행한다. 레이어가 깊어짐에 따라 정보가 점차 멀리 퍼져나가, 상위 레이어에서는 로봇 전체의 상태를 고려하는 전역적인(global) 추론이 가능해진다. 이는 외부 자극에 대한 동물의 운동 반응이 자극 위치와 공간적으로 상관관계가 있다는 생물학적 사실과도 일치한다.5</p>
</li>
</ul>
<h4>5.2.2 성능</h4>
<p><code>BoT</code>는 모방 학습과 강화 학습 시나리오 모두에서 표준 트랜스포머와 전통적인 MLP(Multi-Layer Perceptron) 아키텍처를 능가하는 성능을 보였다. 특히, <code>BoT</code>의 마스크드 어텐션은 대부분의 요소가 0인 희소 행렬(sparse matrix) 연산이므로, 이론적으로 엄청난 연산 효율성 향상을 가져올 수 있다. 연구팀은 현재 딥러닝 라이브러리들이 이러한 희소 어텐션을 제대로 지원하지 않는 한계를 지적하며, 자체적으로 CPU 기반의 맞춤형 구현을 통해 최대 206%의 런타임 속도 향상과 부동소수점 연산(FLOPs) 감소를 실험적으로 입증했다.29</p>
<p><code>PoliFormer</code>와 <code>Body Transformer</code>는 트랜스포머를 로봇공학에 성공적으로 적용하기 위한 두 가지 중요한 방향성을 제시한다. <code>PoliFormer</code>는 막대한 데이터와 컴퓨팅 파워를 통해 일반화의 한계를 돌파하는 ‘스케일 업’ 접근법의 유효성을 증명했으며, <code>Body Transformer</code>는 아키텍처 자체에 로봇의 물리적 구조라는 도메인 지식을 녹여내어 데이터와 연산 효율성을 추구하는 ’구조적 편향 주입’의 힘을 보여주었다.</p>
<p>이 두 접근법은 서로 배타적인 것이 아니라 상호 보완적이다. 미래의 가장 강력한 로봇 정책 아키텍처는 이 둘을 결합한 형태일 가능성이 높다. 즉, <code>Body Transformer</code>와 같이 로봇의 물리적 구조를 반영하여 본질적으로 효율적인 아키텍처를 기반으로 삼고, 그 위에서 <code>PoliFormer</code>처럼 대규모의 다양한 상호작용 데이터를 통해 강화학습을 수행하는 방식이다. <code>PoliFormer</code>가 ‘무엇을(what)’ 학습해야 하는지에 대한 해답(대규모 상호작용 데이터)을 제시했다면, <code>Body Transformer</code>는 ‘어떻게(how)’ 더 효율적으로 학습할 수 있는지에 대한 아키텍처 수준의 해답을 제시한 셈이다.</p>
<p>궁극적으로 이 두 연구의 결합은 ’범용 체화된 트랜스포머(General-purpose Embodied Transformer)’라는 미래 아키텍처의 청사진을 제시한다. 이러한 미래의 아키텍처는 다양한 로봇의 기구학적, 동역학적 정보를 그래프 형태로 입력받아 동적으로 어텐션 마스크를 생성하고, 이를 통해 어떤 로봇이든 즉시 제어할 수 있는 고도로 일반화된 정책을 학습할 수 있을 것이다. 이는 로봇 하드웨어의 복잡성과 다양성을 소프트웨어가 효과적으로 흡수하는, 진정한 의미의 ‘소프트웨어 정의 로봇(Software-Defined Robotics)’ 시대를 여는 핵심 기술이 될 수 있다.</p>
<h2>6.  주요 산업 연구소 발표 동향 및 전략적 함의</h2>
<p>2024년 11월은 학계의 혁신적인 연구 발표뿐만 아니라, AI 분야를 선도하는 거대 산업 연구소들이 각자의 장기적인 비전과 전략적 방향을 구체적인 결과물로 제시했다는 점에서도 매우 중요한 시기였다. Google/DeepMind, Meta AI, OpenAI의 발표들은 이들이 범용인공지능(AGI)이라는 궁극적인 목표를 향해 각기 다른 경로를 탐색하고 있음을 명확히 보여준다.</p>
<h3>6.1 Google / DeepMind: AI를 통한 과학 혁명의 가속화</h3>
<p>Google은 AI 기술을 상업적 응용을 넘어 인류의 가장 근본적인 지식의 지평을 넓히는 데 사용하려는 ‘AI for Science’ 비전을 꾸준히 추구해왔으며, 11월에는 이 비전을 상징하는 중요한 발표들이 이어졌다.</p>
<ul>
<li>
<p><strong>AlphaFold 3 모델 코드 공개</strong>: 11월 11일, Google DeepMind는 학술적 연구 목적으로 <code>AlphaFold 3</code>의 모델 코드와 가중치를 공개했다.8 2020년에 단백질 구조 예측 문제를 해결하며 생명 과학 분야에 혁명을 일으킨 AlphaFold 2의 후속 버전인 <code>AlphaFold 3</code>는, 단백질을 넘어 DNA, RNA, 약물 분자(ligand) 등 거의 모든 생명 분자 간의 상호작용 구조를 전례 없는 정확도로 예측한다.8 이 발표는 <code>AlphaFold</code> 개발의 주역인 Demis Hassabis와 John Jumper가 2024년 노벨 화학상 수상자로 선정된 직후에 이루어져, AI가 인류의 과학적 발견에 기여하는 가장 상징적인 사례로서의 위상을 더욱 공고히 했다.37 이는 신약 개발, 유전병 연구, 신소재 개발 등 다양한 분야의 연구를 극적으로 가속화할 것으로 기대된다.</p>
</li>
<li>
<p><strong>AI for Science Forum 개최</strong>: Google DeepMind는 세계에서 가장 오래된 과학 학술 단체인 영국 왕립학회(Royal Society)와 공동으로 제1회 ’AI for Science Forum’을 개최했다.9 이 포럼에는 전 세계의 저명한 과학자, 정책 입안자, 산업계 리더들이 모여 AI가 어떻게 과학적 발견의 패러다임을 바꾸고 있는지, 그리고 인류가 직면한 기후 변화, 질병과 같은 거대 난제를 해결하는 데 어떻게 기여할 수 있는지를 논의했다.39 이는 AI 기술 리더십을 활용하여 과학계의 논의를 주도하고, AI를 인류 발전에 기여하는 긍정적인 힘으로 포지셔닝하려는 Google의 전략적 의도를 보여준다.</p>
</li>
<li>
<p><strong>기타 기초 과학 분야 적용</strong>: 이 외에도 Google은 양자 컴퓨터에서 발생하는 오류를 AI를 이용해 높은 정확도로 식별하고 수정하는 디코더인 <code>AlphaQubit</code>을 발표하는 등, AI 기술을 물리학, 화학, 수학 등 다양한 기초 과학 분야의 난제 해결에 적용하려는 노력을 지속하고 있음을 보여주었다.39</p>
</li>
</ul>
<h3>6.2 Meta AI: 인간과 같은 촉각을 향한 도전</h3>
<p>Meta AI는 가상 세계(메타버스)와 현실 세계를 잇는 기술의 핵심이 물리적 상호작용 능력에 있다고 보고, 특히 인간의 감각 중 가장 복잡하고 재현하기 어려운 ‘촉각(touch)’ 연구에 집중하는 모습을 보였다.</p>
<ul>
<li>
<p><strong>로보틱스 혁신 기술 발표 (11월 4일)</strong>: Meta AI는 로봇이 물리 세계와 더 자연스럽고 정교하게 상호작용할 수 있도록 하는 일련의 촉각 센싱 및 로봇 손 기술을 대거 공개했다.10 이는 로봇이 단순히 ‘보는’ 것을 넘어 ‘느끼고’ 상호작용하는 시대로의 전환을 예고한다.</p>
</li>
<li>
<p><strong>Meta Sparsh &amp; Digit 360</strong>: 발표의 핵심은 두 가지 혁신적인 기술이다. <code>Meta Sparsh</code>는 다양한 종류의 시각 기반 촉각 센서에 범용적으로 적용될 수 있는 인코더 모델로, 자기지도학습(self-supervised learning)을 통해 별도의 레이블링 없이도 촉각 정보를 의미 있는 표현으로 변환한다.11</p>
</li>
</ul>
<p><code>Digit 360</code>은 한 걸음 더 나아가, 800만 개 이상의 미세 감지 단위(taxel)를 가진 인공 손가락 끝 센서로, 압력, 질감, 미세한 형상 변화 등을 인간의 손가락 수준으로 감지할 수 있다.10</p>
<ul>
<li><strong>생태계 구축 전략</strong>: Meta는 이러한 첨단 기술을 내부적으로만 보유하는 대신, 적극적인 외부 파트너십을 통해 연구 생태계를 구축하려는 전략을 명확히 했다. 촉각 센서 전문 기업인 GelSight와의 협력을 통해 <code>Digit 360</code>을 상용화하고, 로봇 핸드 제조사인 원익로보틱스(Wonik Robotics)와 협력하여 <code>Digit 360</code> 센서가 통합된 차세대 ’Allegro Hand’를 개발하여 2025년부터 연구자들에게 보급할 계획을 밝혔다.11 이는 촉각 센싱 연구의 표준 플랫폼을 선점하고, 관련 분야의 발전을 주도하려는 전략적 포석으로 해석된다.</li>
</ul>
<h3>6.3 OpenAI: 체화된 AGI를 향한 재시동</h3>
<p>한때 로보틱스 팀을 해체하며 물리적 세계와의 상호작용 연구에서 한발 물러서는 듯했던 OpenAI는 11월을 기점으로 이 분야에 다시 공격적으로 진입하려는 움직임을 보였다.</p>
<ul>
<li>
<p><strong>로보틱스 팀 재건 및 인력 영입</strong>: OpenAI는 11월, Meta에서 증강현실(AR) 글래스 개발을 이끌었던 하드웨어 전문가 Caitlin Kalinowski를 하드웨어 디렉터로 영입했다.43 이는 OpenAI가 단순히 소프트웨어 모델 개발을 넘어, 자체적인 로봇 하드웨어 개발까지 염두에 두고 있음을 시사하는 중요한 신호이다. 관련 분야의 인력 채용 공고 역시 활발하게 이루어지고 있으며, 이는 로보틱스 팀을 다시 핵심 부서로 재건하고 있음을 보여준다.43</p>
</li>
<li>
<p><strong>전략적 방향 전환</strong>: 이러한 움직임은 OpenAI가 범용인공지능(AGI) 달성을 위해 언어 모델과 같은 디지털 지능만으로는 불충분하며, 물리 세계와 상호작용하고 경험을 쌓는 ’체화된 에이전트(embodied agent)’가 필수적이라는 전략적 판단을 내렸음을 의미한다. Caitlin Kalinowski는 OpenAI가 자체 맞춤형 센서를 탑재한 로봇을 개발하고 있다고 언급했으며, 이는 AI 모델과 하드웨어의 긴밀한 수직적 통합을 통해 성능을 극대화하려는 의도로 분석된다.43</p>
</li>
</ul>
<p>이들 세 거대 연구소의 11월 발표는 AGI를 향한 각기 다른 철학과 경로를 극명하게 보여준다. <strong>Google</strong>은 지능의 본질을 우주의 법칙을 이해하고 새로운 지식을 창조하는 ‘과학적 발견’ 능력에서 찾고 있다. 이들에게 AI는 인류 지성사 전체를 가속하는 도구이다. <strong>Meta</strong>는 지능이 환경과의 풍부한 ’상호작용’을 통해 발달한다고 본다. 시각만으로는 불완전하며, 촉각과 같은 고차원적인 감각 데이터가 있어야만 세상에 대한 깊은 이해와 섬세한 조작이 가능하다는 철학이다. 이는 궁극적으로 메타버스와 현실 세계를 매끄럽게 연결하려는 그들의 비전과도 맞닿아 있다. 마지막으로 <strong>OpenAI</strong>는 지능이 궁극적으로 물리 세계에서 목표를 달성하는 ’행동’으로 증명되어야 한다고 믿는다. 이를 위해서는 소프트웨어(AI 모델)와 하드웨어(로봇)가 분리될 수 없으며, 이 둘이 긴밀하게 통합된 범용 에이전트(예: 휴머노이드 로봇)를 만드는 것이 AGI로 가는 가장 빠른 길이라고 판단하는 것으로 보인다.</p>
<p>이 세 가지 서로 다른 접근법은 향후 수년간 AI 및 로봇공학 분야의 주요 연구 의제와 기술 경쟁의 축을 형성할 것이다. 이들의 경쟁과 때로는 협력을 통해, ’디지털 지능’의 시대를 넘어 물리 세계와 직접 상호작용하는 ’체화된 지능(Embodied Intelligence)’의 시대가 본격적으로 열릴 것이며, 어떤 접근법이 더 빠르고 효과적으로 AGI에 도달하는지에 따라 미래 기술의 패권이 결정될 수 있다.</p>
<h2>7.  결론: 2024년 11월 연구 동향 종합 및 향후 전망</h2>
<p>2024년 11월은 AI 및 로봇공학 분야가 양적 팽창의 시기를 지나 질적 성숙의 단계로 접어들고 있음을 알리는 중요한 변곡점이었다. 본 보고서에서 심층적으로 분석한 바와 같이, 이 시기의 연구들은 ‘효율성’, ‘일반화’, ’체화’라는 세 가지 핵심 키워드를 중심으로 주목할 만한 진전을 이루었다. 학계와 산업계의 연구들이 서로 다른 문제를 다루는 것처럼 보이지만, 그 기저에는 가상 세계의 지능을 물리 세계의 유용한 행동으로 변환하려는 공통된 목표가 자리하고 있다.</p>
<h3>7.1 월 연구 동향 요약 및 통합적 해석</h3>
<p>이번 달 연구 동향의 가장 큰 특징은 ’효율성’에 대한 전방위적인 추구였다. CoRL 2024를 중심으로 발표된 <code>OpenVLA</code>, <code>RAM</code>, <code>PoliFormer</code>, <code>Body Transformer</code>는 각각 파라미터, 데이터, 연산, 아키텍처 측면에서 효율성을 극대화하는 방법을 제시했다. 이러한 학계의 ‘효율성’ 추구는 단순히 학문적 유행이 아니라, 산업계의 ’상용화’라는 현실적인 요구와 직접적으로 연결된다. 더 적은 데이터로 훈련하고, 더 저렴한 하드웨어에서 추론하며, 더 다양한 상황에 적용될 수 있는 모델만이 실제 산업 현장과 일상생활에 보급될 수 있기 때문이다.</p>
<p>’일반화’는 효율성과 함께 이번 달 연구들을 관통하는 또 다른 핵심 축이었다. <code>PoliFormer</code>는 시뮬레이션 환경에서 현실 세계로, <code>One Model to Drift Them All</code>은 한 차종에서 다른 차종으로, <code>OpenVLA</code>와 <code>RAM</code>은 훈련 데이터에 없던 새로운 작업으로 지식을 일반화하는 데 성공했다. 이는 로봇 학습이 특정 조건에서만 작동하는 ’좁은 지능’을 넘어, 다양한 변화에 강건하게 대처하는 ’넓은 지능’으로 나아가고 있음을 보여준다. 이러한 학계의 일반화 연구는 Google, Meta, OpenAI와 같은 산업계가 추구하는 ‘범용 로봇’ 및 ’AGI’라는 장기적 목표를 달성하기 위한 필수적인 디딤돌이다.</p>
<p>마지막으로, ’체화(Embodiment)’의 중요성이 그 어느 때보다 강조되었다. <code>Body Transformer</code>는 로봇의 물리적 구조를 AI 아키텍처에 내재화하는 직접적인 접근을 취했으며, Meta의 촉각 센서 연구는 로봇이 세상을 인식하는 방식을 근본적으로 바꾸려는 시도이다. OpenAI의 로보틱스 팀 재건 역시 디지털 공간에 머물던 AI를 물리적 실체에 담아내려는 강한 의지를 보여준다. 이는 지능이 신체와 환경과의 상호작용 속에서 발현되고 발전한다는 ‘체화된 인지(Embodied Cognition)’ 이론이 AI 연구의 주류 패러다임으로 자리 잡고 있음을 시사한다.</p>
<h3>7.2 향후 연구 전망</h3>
<p>2024년 11월의 연구 동향을 바탕으로, 향후 AI 및 로봇공학 분야는 다음과 같은 방향으로 발전할 것으로 전망된다.</p>
<ol>
<li>
<p><strong>하이브리드 모델의 부상</strong>: <code>RAM</code>과 같은 검색 기반 접근법과 <code>OpenVLA</code>와 같은 엔드-투-엔드 학습 접근법은 각기 다른 장단점을 가진다. 검색 기반 모델은 해석 가능성과 데이터 확장성이 높지만, 실시간 반응성이 떨어질 수 있다. 반면, 엔드-투-엔드 모델은 반응 속도가 빠르지만, 학습 데이터에 없는 상황에 대한 일반화가 어렵고 ’블랙박스’처럼 작동하는 경향이 있다. 향후 연구는 이 두 가지 패러다임의 장점을 결합하여, 평소에는 빠른 엔드-투-엔드 모델로 작동하다가 불확실하거나 새로운 상황에 직면하면 외부 지식 베이스를 검색하여 안정성을 높이는 하이브리드 아키텍처 개발에 집중될 것이다.</p>
</li>
<li>
<p><strong>다중 모달리티(Multi-modality)의 심화</strong>: 현재 주류를 이루는 시각-언어(vision-language) 모델을 넘어, 더 풍부한 감각 정보를 통합하려는 노력이 가속화될 것이다. Meta의 촉각 센서 연구가 그 신호탄이며, 소리를 통해 물체의 재질이나 상태를 파악하고, 힘/토크 센서를 통해 상호작용의 물리적 역학을 이해하는 등, 시각, 언어, 촉각, 청각, 고유수용성 감각 등 모든 감각 정보를 융합하는 진정한 의미의 다중 모달리티 모델이 로봇 지능의 핵심 연구 주제가 될 것이다.</p>
</li>
<li>
<p><strong>Sim-to-Real의 완전 자동화</strong>: <code>PoliFormer</code>의 성공은 대규모 시뮬레이션이 실제 로봇 정책을 학습하는 데 매우 효과적인 플랫폼임을 입증했다. 향후에는 단순히 기존 시뮬레이션 환경을 사용하는 것을 넘어, 현실 세계와의 차이(reality gap)를 최소화하기 위해 절차적으로 무한히 다양한 환경과 작업을 자동으로 생성하고(procedural content generation), 강화학습 에이전트가 스스로 어려운 커리큘럼을 만들어 학습하며(automatic curriculum learning), 학습된 정책을 현실 세계에 배포하고 피드백을 받아 다시 시뮬레이터를 개선하는 완전 자동화된 ‘Sim-to-Real-to-Sim’ 루프 구축 연구가 활발해질 것이다.</p>
</li>
<li>
<p><strong>데이터 생태계 경쟁</strong>: <code>OpenVLA</code>가 Open X-Embodiment 데이터셋의 힘을 입증했듯이, 앞으로는 고품질의 대규모 공개 데이터셋을 확보하고 공유하는 플랫폼이 기술 발전의 속도를 결정하는 핵심 인프라가 될 것이다. 이는 단순히 데이터를 모으는 것을 넘어, 다양한 로봇 하드웨어, 센서 모달리티, 작업 시나리오를 포괄하고, 데이터에 대한 표준화된 주석과 평가 프로토콜을 제공하는 데이터 생태계 구축 경쟁으로 이어질 것이다.</p>
</li>
</ol>
<h3>7.3 최종 결론</h3>
<p>2024년 11월은 AI와 로봇이 가상 세계에서 축적한 방대한 지능을 물리 세계의 의미 있는 행동으로 변환하는 과정에서 겪던 성장통을 극복하고, 보다 성숙하고 실용적인 단계로 나아가는 중요한 전환점이었다. ’더 크게’를 외치던 시대에서 ’더 효율적으로, 더 똑똑하게’를 추구하는 시대로의 변화가 시작되었다. 앞으로의 발전은 개별 기술의 단편적인 혁신을 넘어, 이들을 어떻게 유기적으로 통합하고, 어떤 데이터로 학습시키며, 인류에게 가장 중요한 어떤 문제에 적용할 것인가에 대한 생태계 전반의 깊은 고민과 지혜에 의해 결정될 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>OpenVLA: An Open-Source Vision-Language-Action Model, https://proceedings.mlr.press/v270/kim25c.html</li>
<li>OpenVLA: An Open-Source Vision-Language-Action Model - arXiv, https://arxiv.org/html/2406.09246v3</li>
<li>RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v270/kuang25a.html</li>
<li>RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation, https://semrob.github.io/docs/rss_semrob2024_cr_paper17.pdf</li>
<li>Body Transformer: Leveraging Robot Embodiment for Policy Learning - OpenReview, https://openreview.net/pdf/3a536f0b2d899536f4948c0cb43072f15cc663fe.pdf</li>
<li>PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators - arXiv, https://arxiv.org/html/2406.20083v1</li>
<li>PoliFormer: Embodied Navigation, On-Policy RL, Transformer Policy, https://poliformer.allen.ai/</li>
<li>AlphaFold 3 predicts the structure and interactions of all of life’s molecules - Google Blog, https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/</li>
<li>The AI for Science Forum: A new era of discovery - Google Blog, https://blog.google/technology/ai/ai-science-forum-2024/</li>
<li>Meta AI Research: SAM 2.1 &amp; CoTracker3 | Ultralytics HUB, https://www.ultralytics.com/blog/ai-research-updates-from-meta-fair-sam-2-1-and-cotracker3</li>
<li>Meta Develops Next-Generation AI Robots with Human-like Sense of Touch - Mike Kalil, https://mikekalil.com/blog/meta-robots-touch-perception/</li>
<li>Conference on Robot Learning (CoRL), 2024 | ServiceNow AI Research, https://www.servicenow.com/research/event/2024-corl.html</li>
<li>CoRL 2024, https://2024.corl.org/</li>
<li>Lifelong Learning for Home Robots: CoRL 2024, https://llhomerobots.github.io/</li>
<li>LangRob @ CoRL 2024 - Google Sites, https://sites.google.com/view/langrob-corl24/</li>
<li>Awards - CoRL 2024, https://2024.corl.org/program/awards</li>
<li>One Model to Drift Them All: Physics-Informed Conditional Diffusion …, https://proceedings.mlr.press/v270/djeumou25a.html</li>
<li>One Model to Drift Them All: Physics-Informed Conditional Diffusion Model for Driving at the Limits - GitHub, https://raw.githubusercontent.com/mlresearch/v270/main/assets/djeumou25a/djeumou25a.pdf</li>
<li>RSS 2021 - AI Best Paper Awards, https://aibestpape.rs/venue/?id=RSS</li>
<li>List of Top Robotics Conferences in 2024-2025 in London UK Europe US, Asia, China and Japan - IoT Magazine, https://iotworldmagazine.com/2024/10/01/2473/list-of-top-robotics-conferences-in-2024-2025-in-london-uk-europe-us-asia-china-and-japan</li>
<li>AI ML, Data Science &amp; Robotics Conferences 2024, https://aiml.events/events/ai-ml-data-science-robotics-conferences-2024</li>
<li>openvla/openvla-7b - Hugging Face, https://huggingface.co/openvla/openvla-7b</li>
<li>OpenVLA: An open-source vision-language-action model for robotic manipulation. - GitHub, https://github.com/openvla/openvla</li>
<li>OpenVLA: An Open-Source Vision-Language-Action Model, https://openvla.github.io/</li>
<li>USC at the Conference of Robot Learning (CoRL) 2024, https://viterbischool.usc.edu/news/2024/11/usc-at-the-conference-of-robot-learning-corl-2024/</li>
<li>yxKryptonite/RAM_code: Official implementation of RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation - GitHub, https://github.com/yxKryptonite/RAM_code</li>
<li>RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation - Yuxuan Kuang, https://yuxuank.com/RAM/</li>
<li>RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation - arXiv, https://arxiv.org/html/2407.04689v1</li>
<li>Body Transformer: Leveraging Robot Embodiment for Policy Learning - arXiv, https://arxiv.org/html/2408.06316v1</li>
<li>PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators | Request PDF - ResearchGate, https://www.researchgate.net/publication/381851256_PoliFormer_Scaling_On-Policy_RL_with_Transformers_Results_in_Masterful_Navigators</li>
<li>PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators - GitHub, https://raw.githubusercontent.com/mlresearch/v270/main/assets/zeng25a/zeng25a.pdf</li>
<li>[Literature Review] PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators - Moonlight, https://www.themoonlight.io/en/review/poliformer-scaling-on-policy-rl-with-transformers-results-in-masterful-navigators</li>
<li>CoRL.2024 | Cool Papers - Immersive Paper Discovery, https://papers.cool/venue/CoRL.2024</li>
<li>Body Transformer: Leveraging Robot Embodiment for Policy Learning - Paper Detail, https://deeplearn.org/arxiv/517077/body-transformer:-leveraging-robot-embodiment-for-policy-learning</li>
<li>Body Transformer: Leveraging Robot Embodiment for Policy Learning - YouTube, https://www.youtube.com/watch?v=tKcR4Oc5N7I</li>
<li>AlphaFold - Google DeepMind, https://deepmind.google/science/alphafold/</li>
<li>AlphaFold - Wikipedia, https://en.wikipedia.org/wiki/AlphaFold</li>
<li>predicting protein structure with alphafold - Imperial blogs, https://blogs.imperial.ac.uk/molecular-science-engineering/2024/11/05/predicting-protein-structure-with-alphafold/</li>
<li>7 pieces of AI news we announced in November - The Keyword, https://blog.google/technology/ai/google-ai-updates-november-2024/</li>
<li>Deep Minds: Reflections from the AI for Science Forum - TL;DR, https://www.digital-science.com/blog/2024/11/ai-for-science-forum/</li>
<li>Meta Unveils Robotics Innovations with Human-Like Touch - ASO World, https://marketingtrending.asoworld.com/en/discover/meta-unveils-robotics-innovations-with-human-like-touch/</li>
<li>Embodied AI in Action: Breakthroughs in Tactile Sensing | Dive!, https://go-dive.net/embodied-ai-in-action-breakthroughs-in-tactile-sensing/</li>
<li>OpenAI’s Bold Plans to Revolutionize Robotics - Kenility, https://www.kenility.com/blog/ai-news/openai-robotics</li>
<li>OpenAI Accelerates Robotics Push for AGI Amid Rival Competition - WebProNews, https://www.webpronews.com/openai-accelerates-robotics-push-for-agi-amid-rival-competition/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>