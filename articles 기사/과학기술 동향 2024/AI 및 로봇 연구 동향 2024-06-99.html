<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2024년 6월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2024년 6월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2024년 AI 및 로봇 연구 동향</a> / <span>2024년 6월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2024년 6월 AI 및 로봇 연구 동향</h1>
<h2>1.  2024년 6월 AI 및 로봇 연구 동향 개관</h2>
<p>2024년 6월은 인공지능(AI) 및 로봇 공학 분야의 연구 패러다임이 중요한 전환점을 맞이한 시기로 기록될 것이다. 이 시기에 발표된 주요 연구들은 정적인 데이터 생성과 패턴 인식을 넘어, 동적이고 상호작용이 가능하며 물리적 법칙에 순응하는 세계를 모델링하려는 기술적 지향점을 명확히 드러냈다. 컴퓨터 비전, 기계 학습, 로보틱스 분야의 세계 최고 권위 학회들에서 발표된 성과들은 생성 AI의 표현력이 한 차원 높아지고, 3D 비전 기술이 실용적 완성도를 더하며, 대규모 언어 모델(LLM)이 단순한 정보 처리 도구를 넘어 복잡한 작업을 자율적으로 수행하는 ’에이전트’로 진화하고 있음을 입증했다. 특히 이러한 개별 기술들의 발전은 물리 세계에 대한 깊은 이해와 상호작용을 목표로 하는 ’체화된 AI(Embodied AI)’라는 거대한 흐름으로 수렴하는 양상을 보였다.</p>
<p>본 보고서는 2024년 6월을 기점으로 발표된 AI 및 로봇 분야의 핵심적인 학술적 성과들을 심층적으로 분석하여, 해당 월의 기술적 성취를 종합하고 미래 연구 개발의 방향성을 전망하는 것을 목표로 한다. 이를 위해 다음과 같은 주요 기술 동향에 주목한다.</p>
<p>첫째, <strong>생성 모델의 동역학 및 상호작용성 강화</strong>이다. 컴퓨터 비전 분야 최고 학회인 CVPR 2024에서는 단일 정지 이미지로부터 바람에 흔들리는 나무와 같은 자연스러운 움직임을 담은 동영상을 생성하는 연구가 최고 논문으로 선정되었다.1 이는 생성 모델이 단순히 외형을 모방하는 것을 넘어, 내재된 물리적 동역학을 학습하고 시뮬레이션하는 단계로 나아가고 있음을 보여준다. 또한, 생성된 이미지의 결함을 단순히 ’좋다/나쁘다’로 평가하는 것을 넘어, 어느 영역이 어떤 이유로 잘못되었는지를 상세히 지적하는 인간의 ’풍부한 피드백’을 학습하여 생성 품질을 제어하는 연구 역시 최고 논문으로 공동 선정되며, 인간과 AI의 상호작용이 더욱 정교해지고 있음을 시사했다.1</p>
<p>둘째, <strong>3D 재구성 및 렌더링 기술의 고도화</strong>이다. 실시간 고품질 렌더링으로 각광받는 3D 가우시안 스플래팅(3D Gaussian Splatting) 기술의 근본적인 문제였던 앨리어싱(aliasing) 현상을 해결한 연구가 CVPR 2024 최고 학생 논문상을 수상했다.1 이는 렌더링 품질을 다양한 시점과 거리에서도 일관되게 유지함으로써, 가상현실(VR) 및 증강현실(AR)과 같은 몰입형 경험의 현실감을 한 단계 끌어올리는 중요한 기술적 진보다.</p>
<p>셋째, <strong>LLM의 추론 능력 및 에이전트화</strong>이다. 기계 학습 분야 최고 학회인 ICML 2024에서는 LLM의 출력을 특정 제약 조건에 맞게 유도하기 위한 정교한 확률적 추론 기법이 제시되었으며, 이는 AI의 행동을 보다 신뢰성 있게 제어할 수 있는 수학적 토대를 마련했다.3 더 나아가, LLM이 전문 3D 모델링 소프트웨어인 Blender를 프로그래밍 방식으로 제어하여 텍스트 설명만으로 복잡한 3D 장면을 자율적으로 구축하는 에이전트 프레임워크가 발표되었다.3 이는 LLM이 단순한 텍스트 생성기를 넘어, 복잡한 디지털 도구를 사용하는 창의적 작업의 주체로 부상하고 있음을 의미한다.</p>
<p>마지막으로, <strong>로보틱스와 AI의 융합 가속화</strong>이다. 로보틱스 분야 최고 학회인 RSS 2024 및 주요 사전 공개 연구들은 AI 기술이 로봇의 지능을 근본적으로 향상시키고 있음을 명확히 보여주었다. 로봇의 탐사 전략을 최적화하는 데 있어 단일 최적 경로가 아닌, 다양한 가능성을 고려하는 확률적 추론 기법이 도입되었고 6, 인간의 동작을 VR과 같은 접근성 높은 장비로 모방하여 휴머노이드 로봇의 복잡한 전신 제어 기술을 학습시키는 연구는 로봇 학습의 데이터 수집 병목 현상을 해결할 실마리를 제공했다.8</p>
<p>이처럼 2024년 6월의 연구들은 AI가 디지털 세계를 더욱 현실감 있게 창조하고, 나아가 물리적 세계에서 인간과 유사한 방식으로 인지하고 행동하는 지능체로 발전해 나가는 중요한 이정표를 제시했다. 본 보고서는 이러한 핵심 연구들을 하나씩 심층 분석함으로써 현재 기술의 최전선을 조망하고, 다가올 미래의 기술 패러다임을 예측하고자 한다.</p>
<h2>2.  컴퓨터 비전의 새로운 지평: CVPR 2024 주요 연구 심층 분석</h2>
<p>2024년 6월 미국 시애틀에서 개최된 컴퓨터 비전 및 패턴 인식 학회(CVPR, Conference on Computer Vision and Pattern Recognition)는 11,500편이 넘는 방대한 양의 논문이 제출되며 명실상부 해당 분야 최고의 학술적 권위를 다시 한번 입증했다.2 이 학회에서 발표되고 수상한 연구들은 컴퓨터 비전 분야의 현재와 미래를 가늠하는 가장 중요한 지표다. 올해의 주요 수상작들은 공통적으로 ’정적인 2D 이미지 표현을 넘어, 동적이고 상호작용이 가능하며, 다차원적인 피드백을 이해하는 3D 세계의 모델링’이라는 명확한 방향성을 제시했다. 본 장에서는 CVPR 2024의 최고 논문(Best Paper) 및 최고 학생 논문(Best Student Paper)으로 선정된 연구들을 중심으로, 컴퓨터 비전 기술의 새로운 지평을 심층적으로 분석한다.</p>
<h3>2.1 Table 1: CVPR 2024 주요 수상 논문 요약</h3>
<p>CVPR 2024에서 가장 주목받은 연구들의 핵심 내용을 요약하면 다음과 같다. 이 표는 각 연구가 어떤 학술적 기여로 최고의 인정을 받았는지 한눈에 파악할 수 있도록 하며, 이어지는 상세 분석의 길잡이 역할을 한다.</p>
<table><thead><tr><th><strong>상 (Award)</strong></th><th><strong>논문 제목 (Paper Title)</strong></th><th><strong>핵심 기여 (Key Contribution)</strong></th></tr></thead><tbody>
<tr><td>Best Paper</td><td>Generative Image Dynamics</td><td>단일 이미지로부터 자연스러운 진동 동역학을 모델링하여 동영상 및 인터랙티브 시뮬레이션 생성 1</td></tr>
<tr><td>Best Paper</td><td>Rich Human Feedback for Text-to-Image Generation</td><td>이미지 영역 및 텍스트 키워드에 대한 상세한 인간 피드백 데이터셋(RichHF-18K) 및 예측 모델(RAHF) 제안 1</td></tr>
<tr><td>Best Student Paper</td><td>Mip-Splatting: Alias-free 3D Gaussian Splatting</td><td>3D 스무딩 필터와 2D Mip 필터를 도입하여 3D 가우시안 스플래팅의 앨리어싱 문제를 해결 1</td></tr>
<tr><td>Honorable Mention</td><td>pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction</td><td>이미지 쌍으로부터 확장 가능한 3D 재구성 방법론 제시 1</td></tr>
<tr><td>Honorable Mention</td><td>Objects as Volumes: A Stochastic Geometry View of Opaque Solids</td><td>불투명 고체에 대한 확률적 기하학 관점의 새로운 볼륨 렌더링 이론 제시 1</td></tr>
</tbody></table>
<h3>2.2  정적 이미지에 생명을 불어넣다: 생성적 이미지 동역학 (Breathing Life into Static Images: Generative Image Dynamics)</h3>
<h4>2.2.1 연구 개요 및 문제 정의</h4>
<p>Google Research의 Zhengqi Li 등이 발표하여 CVPR 2024 최고 논문상을 수상한 “Generative Image Dynamics“는 단 한 장의 정지 이미지로부터 사실적이고 자연스러운 애니메이션을 생성하는 혁신적인 접근법을 제시했다.1 기존의 이미지 애니메이션 기술들은 사용자가 직접 움직일 영역을 지정해야 하거나, 생성되는 움직임이 제한적이고 부자연스러운 한계를 가지고 있었다. 이 연구는 이러한 문제를 해결하기 위해, 실제 비디오에서 관찰되는 자연 현상, 특히 바람에 흔들리는 나무, 꽃, 촛불, 옷가지 등과 같은 ’진동 동역학(oscillatory dynamics)’에 대한 일반적인 사전 지식, 즉 이미지 공간 사전확률(image-space prior)을 데이터로부터 직접 학습하는 방식을 택했다.13 목표는 별도의 사용자 입력 없이도 이미지에 내재된 잠재적인 움직임을 스스로 파악하고 이를 생생한 동영상으로 구현하는 것이다.</p>
<h4>2.2.2 핵심 방법론: 스펙트럴 볼륨과 확산 모델</h4>
<p>이 연구의 핵심은 움직임을 시간의 흐름에 따른 픽셀의 변화로 보는 대신, 움직임을 구성하는 다양한 주파수 성분의 조합으로 해석하는 데 있다.</p>
<ul>
<li><strong>푸리에 변환 기반 움직임 표현:</strong> 연구팀은 실제 비디오 클립에서 각 픽셀의 움직임 궤적(motion trajectories)을 추출한 뒤, 이를 푸리에 변환(Fourier Transform)하여 시간 도메인에서 주파수 도메인으로 변환한다. 이렇게 변환된 주파수 및 위상 정보는 ’스펙트럴 볼륨(spectral volumes)’이라는 3차원 데이터 구조로 인코딩된다.11 이 표현 방식은 복잡하고 장기적인 움직임을 시간에 따라 프레임 단위로 저장하는 것보다 훨씬 압축적이고 효율적으로 표현할 수 있는 장점을 가진다.</li>
<li><strong>확산 모델을 이용한 스펙트럴 볼륨 예측:</strong> 모델은 단일 정지 이미지를 입력받아, 해당 장면에 가장 잘 어울리는 스펙트럴 볼륨을 예측하는 작업을 수행한다. 이 예측 과정에는 최근 생성 모델의 주류로 자리 잡은 확산 모델(diffusion model)이 사용된다.13 특히, 이 모델은 ’주파수 조정 확산 샘플링(frequency-coordinated diffusion sampling process)’이라는 독특한 방식을 사용한다. 이는 전체적인 큰 움직임을 결정하는 저주파 성분부터 세밀한 떨림을 표현하는 고주파 성분까지 순차적으로, 그리고 일관성을 유지하며 생성하도록 유도하여, 최종적으로 자연스러운 움직임을 만들어낸다.14</li>
<li><strong>이미지 기반 렌더링:</strong> 확산 모델이 예측한 스펙트럴 볼륨은 역 푸리에 변환을 통해 전체 비디오 시퀀스에 걸쳐 각 픽셀이 어떻게 움직여야 하는지를 정의하는 ’모션 텍스처(motion texture)’로 변환된다.13 최종적으로, 이미지 기반 렌더링 모듈이 원본 정지 이미지의 픽셀들을 이 모션 텍스처에 따라 매 프레임마다 이동시켜 자연스럽게 움직이는 동영상을 완성한다.</li>
</ul>
<h4>2.2.3 주요 결과 및 응용</h4>
<p>이러한 방법론을 통해 얻어진 결과는 단순히 이미지를 움직이는 것을 넘어, 새로운 형태의 상호작용을 가능하게 한다.</p>
<ul>
<li><strong>이미지-비디오 변환:</strong> 어떤 정지 이미지라도 입력하면, 그럴듯한 움직임을 포함하는, 끊김 없이 자연스럽게 반복되는(seamlessly looping) 비디오로 자동 변환할 수 있다.2 이는 웹사이트 배경, 디지털 사이니지, 또는 가상 회의에서 사용하는 동적 배경화면 등 다양한 분야에 즉시 적용될 수 있는 잠재력을 지닌다.10</li>
<li><strong>인터랙티브 동역학:</strong> 이 연구의 가장 혁신적인 부분은 생성된 움직임 모델을 상호작용에 활용하는 것이다. 예측된 스펙트럴 볼륨은 단순히 움직임을 기록한 데이터가 아니라, 해당 객체가 어떻게 진동하고 반응할 수 있는지에 대한 물리적 특성을 담고 있는 ’이미지 공간 모달 기저(image-space modal bases)’로 해석될 수 있다.2 이를 통해 사용자가 마우스로 이미지 속의 꽃을 클릭하여 잡아당겼다가 놓으면, 마치 실제 꽃처럼 탄성을 가지고 흔들리다가 멈추는 사실적인 물리적 반응을 실시간으로 시뮬레이션할 수 있다.10</li>
</ul>
<p>이 연구는 생성 AI의 목표가 단순히 그럴듯한 ’외형’을 만드는 것에서 그럴듯한 ’물리적 행동’을 학습하고 생성하는 것으로 확장되고 있음을 보여주는 중요한 사례다. 기존의 비디오 생성 모델들이 주로 시각적 일관성에 초점을 맞추었다면, 이 연구는 물리 시뮬레이션 없이 순수하게 데이터로부터 자연의 동역학 원리를 학습했다는 점에서 근본적인 차이를 보인다. 물리학과 공학에서 객체의 진동 특성을 분석하는 데 사용되는 ’모달 해석’이라는 고전적인 개념이 데이터 기반 모델의 표현 방식으로 자연스럽게 도출되었다는 점은 주목할 만하다. 이는 생성 모델이 단순히 픽셀을 나열하는 것이 아니라, 세계가 작동하는 방식에 대한 의미 있는 압축된 표현을 학습하고 있음을 시사한다. 궁극적으로 이 연구는 데이터 기반 생성 모델과 물리 기반 시뮬레이션 사이의 간극을 메우는 중요한 다리를 놓았으며, 단순한 비디오 생성을 넘어, 단 한 장의 사진으로부터 상호작용이 가능한 현실적인 디지털 트윈을 생성하는 ’생성적 인터랙티브 현실(Generative Interactive Reality)’의 미래를 향한 중요한 첫걸음이라 평가할 수 있다.</p>
<h3>2.3  인간과 생성 모델의 상호작용 고도화: 풍부한 인간 피드백 (Advancing Human-Generative Model Interaction: Rich Human Feedback for Text-to-Image Generation)</h3>
<h4>2.3.1 연구 개요 및 문제 정의</h4>
<p>“Generative Image Dynamics“와 함께 CVPR 2024 최고 논문상을 공동 수상한 Google Research의 Youwei Liang 등의 “Rich Human Feedback for Text-to-Image Generation“은 Text-to-Image(T2I) 모델의 품질을 향상시키기 위한 인간-AI 상호작용의 새로운 패러다임을 제시했다.1 T2I 모델은 놀라운 발전을 이루었지만, 생성된 이미지에는 여전히 기이한 형태의 아티팩트(예: 손가락이 6개인 손), 물리적으로 불가능한 장면, 또는 입력된 텍스트 프롬프트와 내용이 일치하지 않는 문제들이 빈번하게 발생한다.18 기존에는 대규모 언어 모델의 성능 향상에 크게 기여한 RLHF(인간 피드백 기반 강화 학습) 기법을 T2I 모델에 적용하려는 시도가 있었으나, 이는 주로 생성된 이미지에 대해 ‘A가 B보다 낫다’ 또는 ’이 이미지는 5점 만점에 3점이다’와 같은 단일 점수 형태의 피드백에 의존했다. 이러한 단순한 피드백은 이미지의 어떤 부분이, 어떤 이유로 잘못되었는지에 대한 구체적인 정보를 제공하지 못해 모델을 효과적으로 개선하는 데 한계가 있었다.20</p>
<h4>2.3.2 핵심 방법론</h4>
<p>이 연구는 이러한 한계를 극복하기 위해, 훨씬 더 구체적이고 다차원적인, 말 그대로 ‘풍부한(rich)’ 인간 피드백을 수집하고 이를 학습하는 시스템을 제안한다.</p>
<ul>
<li><strong>RichHF-18K 데이터셋 구축:</strong> 이 연구의 첫 번째 핵심 기여는 18,000개의 생성 이미지에 대해 상세한 피드백을 수집한 최초의 데이터셋 ’RichHF-18K’를 구축한 것이다.18 이 데이터셋은 다음과 같은 세 가지 종류의 정교한 주석(annotation)을 포함한다:</li>
</ul>
<ol>
<li><strong>이미지 영역 주석:</strong> 평가자가 이미지 위에서 직접 문제 영역을 지정한다. 예를 들어, 형태가 이상한 객체나 물리적으로 비현실적인 부분은 ‘비현실성(implausibility)’ 영역으로, ’파란색 자동차’라는 프롬프트에 빨간색 자동차가 생성된 경우 해당 자동차는 ‘불일치(misalignment)’ 영역으로 표시된다. 이 정보는 히트맵 형태로 변환되어 어느 픽셀 영역에 문제가 있는지를 시각적으로 나타낸다.18</li>
<li><strong>불일치 키워드 주석:</strong> 평가자가 텍스트 프롬프트 내에서 이미지에 제대로 반영되지 않았거나(misrepresented) 아예 누락된(missing) 단어를 직접 클릭하여 지정한다. 이는 이미지-텍스트 간 불일치의 원인을 텍스트 수준에서 명확히 pinpoint하는 역할을 한다.18</li>
<li><strong>세분화된 점수:</strong> 기존의 단일 점수와 달리, 이미지를 4가지 구체적인 기준으로 나누어 5점 척도로 평가한다: 현실성(Plausibility), 심미성(Aesthetics), 텍스트-이미지 일치도(Text-Image Alignment), 그리고 전반적인 품질(Overall Quality).21</li>
</ol>
<ul>
<li><strong>RAHF (Rich Automatic Human Feedback) 모델:</strong> 두 번째 핵심 기여는 RichHF-18K 데이터셋을 이용해 인간의 풍부한 피드백을 자동으로 예측하도록 훈련된 멀티모달 트랜스포머 모델 ’RAHF’를 개발한 것이다.18 RAHF 모델은 ViT(Vision Transformer)와 T5 언어 모델을 결합한 구조를 가지며, 이미지와 텍스트 프롬프트를 함께 입력받아 위에서 설명한 세 종류의 풍부한 피드백(문제 영역 히트맵, 불일치 키워드, 세분화된 점수)을 모두 자동으로 출력한다.21</li>
</ul>
<h4>2.3.3 주요 결과 및 응용</h4>
<p>RAHF 모델이 예측한 풍부한 피드백은 T2I 모델의 성능을 실질적으로 개선하는 데 매우 유용하게 사용될 수 있다.</p>
<ol>
<li><strong>데이터 선별 및 미세조정:</strong> RAHF가 예측한 높은 품질 점수를 받은 이미지만을 선별하여 생성 모델(예: Google의 Muse)을 추가로 미세조정(fine-tuning)하는 데 사용함으로써, 모델이 더 고품질의 이미지를 생성하도록 유도할 수 있다.18</li>
<li><strong>선택적 인페인팅(Inpainting):</strong> RAHF가 예측한 문제 영역 히트맵을 마스크(mask)로 활용하여, 이미지의 결함이 있는 부분만 선택적으로 다시 생성(inpaint)함으로써 전체적인 이미지 품질을 효율적으로 개선할 수 있다.18</li>
</ol>
<p>이 연구는 모델 정렬(alignment) 기술의 중요한 진화를 보여준다. 기존의 RLHF가 모델을 인간의 일반적인 ’선호도’에 맞추는 ’선호도 튜닝(Preference Tuning)’이었다면, 이 새로운 접근법은 모델이 저지른 구체적이고, 국소화되었으며, 설명 가능한 ’오류’를 직접 교정하도록 유도하는 ’진단적 미세조정(Diagnostic Fine-tuning)’의 시대를 열었다. 이는 마치 학생에게 단순히 성적(단일 점수)을 주는 것과, 에세이의 특정 문법 오류나 논리적 비약을 지적하며 상세한 첨삭 지도를 해주는 것의 차이와 같다. 후자가 학습에 훨씬 효과적인 것은 자명하다. 이와 같은 ‘진단적’ 데이터는 단순한 선호도 최적화(DPO 등)를 넘어서는 새로운 차원의 정렬 기법을 가능하게 한다. 결함이 있는 영역에 대한 마스크 인페인팅, 잘못 표현된 키워드에 기반한 타겟된 네거티브 프롬프팅 등, 모델 개선을 위한 훨씬 더 효율적이고 해석 가능한 피드백 루프를 구축할 수 있게 된 것이다. 이는 미래의 AI 정렬 기술이 단순히 선호도를 포착하는 것을 넘어, 자신의 출력물에 대한 구조화된 비평을 이해하고 그에 따라 행동하는 시스템을 구축하는 방향으로 나아갈 것임을 암시한다.</p>
<h3>2.4  앨리어싱 없는 3D 재구성: Mip-Splatting (Alias-Free 3D Reconstruction: Mip-Splatting)</h3>
<h4>2.4.1 연구 개요 및 문제 정의</h4>
<p>3D 가우시안 스플래팅(3DGS)은 여러 장의 2D 이미지로부터 3D 장면을 복원하고, 이를 실시간에 가까운 속도로 렌더링할 수 있는 능력 덕분에 새로운 뷰 합성(Novel View Synthesis) 분야에 혁명을 일으켰다.24 그러나 이 기술은 근본적인 한계를 가지고 있었다. 카메라를 장면에 가깝게 가져가거나(줌인) 멀리 떨어뜨릴 때(줌아웃)처럼 렌더링 시의 샘플링 비율이 훈련 시와 달라지면, 이미지에 심각한 시각적 결함, 즉 아티팩트가 발생하는 문제였다. 이는 신호 처리의 기본 원리인 나이퀴스트 샘플링 정리(Nyquist’s sampling theorem)를 위반하여 발생하는 앨리어싱(aliasing) 현상 때문으로, 줌인 시에는 디테일이 깨지는 고주파 노이즈가 발생하고 줌아웃 시에는 이미지가 부자연스럽게 두꺼워지거나 얇아지는 팽창(dilation) 및 침식(erosion) 아티팩트가 나타났다.24 CVPR 2024 최고 학생 논문상을 수상한 “Mip-Splatting“은 바로 이 문제를 해결하기 위해 제안되었다.</p>
<h4>2.4.2 핵심 방법론: 안티앨리어싱을 위한 이중 필터링 전략</h4>
<p>Mip-Splatting은 앨리어싱 문제를 해결하기 위해 3D 공간과 2D 공간 양쪽에서 작동하는 독창적인 이중 필터링 전략을 도입했다.</p>
<p><strong>1. 3D 스무딩 필터 (3D Smoothing Filter):</strong></p>
<ul>
<li>
<p><strong>원리:</strong> 이 필터는 “3D 장면에서 재구성할 수 있는 디테일의 한계(최고 주파수)는 훈련에 사용된 원본 이미지들의 해상도(샘플링 레이트)에 의해 결정된다“는 통찰에 기반한다.24 장면을 구성하는 수백만 개의 3D 가우시안 각각에 대해, 모든 훈련용 카메라 뷰 중에서 해당 가우시안을 가장 높은 해상도로 포착할 수 있는 뷰를 찾아 ’최대 샘플링 주파수’를 결정한다.25</p>
</li>
<li>
<p><strong>수식 및 적용:</strong> 이 최대 샘플링 주파수 ν^k를 사용하여 각 3D 가우시안 Gk의 크기와 형태를 나타내는 공분산 행렬 Σk를 조절한다. 구체적으로, 가우시안 저역 통과 필터(Gaussian low-pass filter)를 적용하는데, 이는 두 가우시안의 컨볼루션(convolution) 연산이 또 다른 가우시안이 되는 속성을 이용해 효율적으로 계산된다. 정규화된 새로운 공분산 Σ~k는 다음과 같이 계산된다 24:<br />
<span class="math math-display">
\tilde{\Sigma}_k = \Sigma_k + s^2 \left( \frac{1}{\hat{\nu}_k} \right)^2 \mathbf{I}
</span><br />
여기서 s는 필터의 강도를 조절하는 하이퍼파라미터이고, <span class="math math-inline">\mathbf{I}</span>는 단위 행렬이다. 이 수식은 각 가우시안이 훈련 데이터가 허용하는 한계를 넘어 과도하게 작아지는(즉, 너무 높은 주파수 성분을 갖게 되는) 것을 방지하는 정규화(regularization) 역할을 한다. 결과적으로 줌인 시 발생하던 고주파수 아티팩트가 효과적으로 제거된다.25</p>
</li>
</ul>
<p><strong>2. 2D Mip 필터 (2D Mip Filter):</strong></p>
<ul>
<li><strong>원리:</strong> 기존 3DGS는 3D 가우시안을 2D 화면에 투영한 후, 고정된 크기의 2D 팽창(dilation) 필터를 적용했다. 이 방식은 줌아웃 시 투영된 가우시안이 픽셀보다 작아지면서 앨리어싱을 유발하고, 부자연스러운 팽창 아티팩트를 만드는 원인이었다.25 Mip-Splatting은 이를 대체하기 위해 컴퓨터 그래픽스의 고전적인 안티앨리어싱 기법인 ’밉매핑(mipmapping)’에서 영감을 받은 2D Mip 필터를 도입한다. 이 필터는 실제 카메라 센서가 빛을 받아들일 때 발생하는 2D 박스 필터(box filter) 효과를 모사하도록 설계되었다.25</li>
<li><strong>적용:</strong> 2D Mip 필터는 렌더링될 이미지의 각 픽셀이 차지하는 영역 전체에 걸쳐 투영된 가우시안의 영향을 정확하게 적분하여 색상을 계산하는 것을 근사한다. 이는 렌더링 해상도에 따라 필터의 크기가 동적으로 조절됨을 의미하며, 투영된 가우시안이 아무리 작아져도 항상 픽셀 영역에 맞게 적절히 블러(blur) 처리되어 앨리어싱을 방지한다. 이를 통해 줌아웃 시 발생하는 시각적 결함들을 효과적으로 완화한다.25</li>
</ul>
<h4>2.4.3 주요 결과 및 의의</h4>
<p>Mip-Splatting은 이중 필터링 전략을 통해 3DGS가 스케일 변화에 따라 겪던 팽창, 침식, 앨리어싱 아티팩트를 성공적으로 제거했다. 그 결과, 사용자가 장면에 가까이 다가가거나 멀어지는 등 시점을 자유롭게 변경하더라도 시각적으로 일관되고 선명한 고품질 렌더링 결과를 얻을 수 있게 되었다.27</p>
<p>이 연구는 단순히 이미지 품질을 점진적으로 개선한 것을 넘어, 3D 가우시안 스플래팅을 가상현실(VR) 및 증강현실(AR)과 같은 실용적인 몰입형 애플리케이션에 적용할 수 있도록 만든 핵심적인 기술적 돌파구로 평가받는다. 3DGS의 실시간 렌더링 속도는 VR/AR의 필수 요건이지만 24, 기존 기술의 시각적 불안정성은 자유로운 사용자 움직임이 핵심인 VR/AR 환경에서는 치명적인 단점이었다. 사용자가 고개를 돌리거나 가상 공간을 걸어 다닐 때마다 객체가 부풀어 오르거나, 얇아지거나, 자글거리는 현상이 발생한다면 몰입 경험은 완전히 깨져버릴 것이다. Mip-Splatting은 신호 처리의 근본적인 문제인 앨리어싱을 해결함으로써, 3DGS를 고정된 경로로 감상하는 ‘영상’ 제작용 기술에서 사용자가 자유롭게 탐험할 수 있는 ‘인터랙티브 공간’ 구축용 기술로 격상시켰다. 이는 메타버스, 디지털 트윈, 공간 컴퓨팅 등 사용자의 자유로운 상호작용 속에서도 시각적 일관성이 반드시 보장되어야 하는 차세대 3D 콘텐츠 제작의 장벽을 크게 낮춘 중요한 진전이다.26</p>
<h2>3.  기계 학습의 근본적 진보: ICML 2024 핵심 연구 조명</h2>
<p>2024년 7월 오스트리아 비엔나에서 개최된 국제 기계 학습 학회(ICML, International Conference on Machine Learning)는 9,653편의 논문이 제출되고 그 중 약 30.5%인 2,944편이 채택되는 등, 기계 학습 분야의 최신 이론과 혁신적인 응용을 총망라하는 최고 권위의 학술 행사로서의 위상을 공고히 했다.33 올해 ICML에서 발표된 연구들은 특히 대규모 언어 모델(LLM)의 능력을 근본적으로 확장하고 제어하는 방법에 대한 깊이 있는 탐구가 두드러졌다. 본 장에서는 LLM의 생성 과정을 수학적으로 정교하게 제어하는 새로운 확률적 추론 기법과, LLM을 단순한 언어 생성 도구가 아닌 복잡한 창의적 소프트웨어의 ’사용자’로 격상시키는 에이전트 기술에 초점을 맞춘 두 가지 핵심 연구를 심층적으로 조명한다.</p>
<h3>3.1  언어 모델의 확률적 추론 능력 강화: Twisted Sequential Monte Carlo (Enhancing Probabilistic Inference in Language Models: Twisted SMC)</h3>
<h4>3.1.1 연구 개요 및 문제 정의</h4>
<p>Stephen Zhao 등이 발표한 “Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo“는 LLM의 행동을 제어하고 정렬(align)하는 다양한 기법들, 예를 들어 인간 피드백 기반 강화 학습(RLHF), 프롬프트 엔지니어링, 특정 제약 조건 하의 텍스트 생성 등을 하나의 통일된 프레임워크로 바라보는 새로운 시각을 제시했다.4 이 연구는 이러한 모든 작업을 ‘주어진 보상 함수나 제약 조건을 만족시키는 텍스트 시퀀스를 생성하는, 비정규화된 목표 확률 분포(unnormalized target distribution)로부터 샘플링하는’ 문제로 수학적으로 재정의한다.36 이는 LLM이 단순히 통계적으로 가장 가능성 높은 다음 단어를 예측하는 것을 넘어, 사용자가 원하는 특정 목표(예: 유용하면서도 무해한 답변 생성)를 만족하는 복잡한 시퀀스를 생성하도록 유도하는 근본적인 문제에 대한 해법을 모색하는 것이다.</p>
<h4>3.1.2 핵심 방법론: Twisted Sequential Monte Carlo (SMC)</h4>
<p>이 문제를 해결하기 위해, 연구팀은 통계학과 신호 처리 분야에서 복잡한 확률 분포를 다루기 위해 오랫동안 사용되어 온 순차 몬테카를로(Sequential Monte Carlo, SMC) 기법을 LLM에 적용하고 이를 독창적으로 발전시켰다.</p>
<ul>
<li>
<p><strong>SMC의 기본 원리:</strong> SMC는 복잡한 전체 시퀀스를 한 번에 샘플링하는 대신, 텍스트를 한 토큰씩 순차적으로 생성하는 과정으로 문제를 분해한다. 이 과정에서 여러 개의 가능한 시퀀스 후보, 즉 ’입자(particle)’들을 동시에 생성하고 추적한다. 각 단계에서 각 입자(부분 시퀀스)가 목표 분포에 얼마나 부합하는지를 평가하여 ’가중치(weight)’를 부여하고, 가중치가 높은 유망한 입자들은 복제하고 낮은 입자들은 제거하는 ‘리샘플링’ 과정을 통해 탐색의 효율성을 높인다.</p>
</li>
<li>
<p><strong>’Twist’의 도입:</strong> 표준 SMC는 미래를 예측하지 못하고 현재 단계의 확률만을 고려하기 때문에, 시퀀스 후반부에 가서야 낮은 보상을 받는 ’막다른 길’로 들어서는 비효율성을 겪을 수 있다. 이 연구의 핵심 혁신은 이러한 한계를 극복하기 위해 ’트위스트 함수(twist functions)’라는 개념을 도입한 것이다.34 트위스트 함수는 각 생성 단계에서, 현재까지 생성된 부분 시퀀스를 바탕으로 앞으로 생성될 나머지 시퀀스가 최종적으로 받게 될 ’미래의 예상 가치(expected future value of the potential)’를 추정하는 역할을 하는 학습된 함수다.36</p>
</li>
<li>
<p><strong>수식적 표현 및 작동 원리:</strong> 목표 분포 <span class="math math-inline">\sigma(s_{1:T})</span>는 LLM의 기본 분포 <span class="math math-inline">p_0(s_{1:T})</span>와 사용자가 정의한 제약 조건 또는 보상을 나타내는 잠재 함수 <span class="math math-inline">\phi(s_{1:T})</span>의 곱에 비례한다:<br />
<span class="math math-display">
\sigma(s_{1:T}) \propto p_0(s_{1:T})\phi(s_{1:T})
</span><br />
.37 Twisted SMC는 각 중간 단계 <span class="math math-inline">t</span>에서 트위스트 함수 <span class="math math-inline">\psi_t(s_{1:t})</span>를 사용하여 제안 분포를 ’비틀어(twist)’준다. 최적의 트위스트 함수 <span class="math math-inline">\psi_t^*(s_{1:t})</span>는 미래에 생성될 시퀀스에 대한 잠재 함수의 기댓값으로 정의된다:<br />
<span class="math math-display">
\psi_t^*(s_{1:t}) = \sum_{s_{t+1:T}} p_0(s_{t+1:T} | s_{1:t}) \phi(s_{1:T})
</span><br />
.37 이 트위스트 함수를 통해 SMC 알고리즘은 현재 단계에서부터 미래에 높은 보상을 받을 것으로 예상되는 유망한 부분 시퀀스(입자)에 더 높은 가중치를 부여하게 된다. 이는 전체 탐색 과정을 목표 분포에 더 가깝게 유도하여 샘플링 효율을 극대화하는 효과를 낳는다.36</p>
</li>
</ul>
<h4>3.1.3 주요 응용 및 결과</h4>
<p>이러한 수학적으로 정교한 프레임워크는 LLM의 행동을 제어하는 다양한 실제 문제에 성공적으로 적용될 수 있다.</p>
<ul>
<li><strong>유해성 콘텐츠 생성 방지 (Automated Red-teaming):</strong> 모델이 유해하거나 바람직하지 않은 출력을 생성하도록 의도적으로 유도하여, 모델의 잠재적 취약점을 사전에 식별하고 방어하는 데 사용될 수 있다.34</li>
<li><strong>제어된 텍스트 생성:</strong> ’긍정적인 어조의 영화 리뷰를 작성하라’와 같이 특정 감성(sentiment)을 지닌 텍스트를 생성하거나, 문장 중간의 빈칸을 문맥에 맞게 채우는 인필링(infilling)과 같은 정교한 제어 작업을 높은 품질로 수행할 수 있다.34</li>
</ul>
<p>이 연구는 LLM의 정렬 및 제어 방식에 대한 중요한 패러다임 전환을 의미한다. 기존의 프롬프트 엔지니어링이나 경험적인 디코딩 전략과 같은 ‘기교적’ 접근법에서 벗어나, LLM의 행동 제어를 ’확률적 추론’이라는 엄밀한 수학적 문제로 정립했다는 점에서 큰 의의가 있다. 통계학의 고전적이면서도 강력한 도구인 SMC를 LLM 생성 과정에 적용함으로써, 모델의 행동을 제어하는 과정에 이론적 견고함을 부여한 것이다. 이는 AI 안전성과 정렬 분야가 단순한 ’프롬프트의 기술’에서 ’확률적 추론의 과학’으로 발전하고 있음을 보여주는 상징적인 사례다. 이러한 접근법은 향후 AI의 행동에 대한 수학적 보증을 제공하고, 더 신뢰성 있고 예측 가능한 AI 시스템을 구축하는 데 핵심적인 역할을 할 것으로 기대된다.</p>
<h3>3.2  텍스트에서 3D 세계로: LLM 에이전트 SceneCraft (From Text to 3D Worlds: The LLM Agent SceneCraft)</h3>
<h4>3.2.1 연구 개요 및 문제 정의</h4>
<p>Ziniu Hu 등이 발표한 “SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code“는 텍스트 설명만으로 복잡한 3D 장면을 자동으로 생성하는 LLM 기반 ’에이전트’를 제안하여 ICML 2024에서 큰 주목을 받았다.3 건축 시각화, 게임 개발, 가상현실(VR) 콘텐츠 제작 등 다양한 산업에서 3D 장면 생성은 필수적이지만, 이는 Blender와 같은 전문 소프트웨어에 대한 높은 숙련도를 요구하는 매우 노동 집약적이고 어려운 작업이다.38 기존의 text-to-3D 생성 기술들은 주로 단일 객체를 만들거나 몇 개의 객체로 구성된 소규모 장면에 국한되는 한계가 있었다. SceneCraft는 이러한 한계를 넘어, 수십 개에서 많게는 백여 개의 3D 에셋들이 복잡한 공간적 관계를 이루며 배치된 대규모 장면을 생성하는 것을 목표로 한다.5 이는 단순한 3D 모델 생성을 넘어, 장면 전체의 구성과 레이아웃에 대한 깊은 이해와 계획, 그리고 전문 도구를 프로그래밍 방식으로 제어하는 고도의 ‘에이전트’ 능력을 요구한다.</p>
<h4>3.2.2 핵심 방법론: 이중 루프 최적화 파이프라인</h4>
<p>SceneCraft의 핵심은 인간 아티스트의 작업 방식을 모방한 ’생성-평가-수정’의 반복적인 이중 루프 최적화 파이프라인이다.5</p>
<p><strong>1. 내부 루프: 장면별 레이아웃 최적화 (Inner-Loop: Per-Scene Layout Optimization):</strong></p>
<ul>
<li><strong>장면 그래프 생성:</strong> 사용자가 “폐허가 된 도시의 골목길, 쓰레기통과 부서진 자동차들이 널려 있다“와 같은 텍스트를 입력하면, LLM 플래너는 먼저 장면에 필요한 에셋 목록(‘쓰레기통’, ‘자동차’ 등)을 만들고, 이들 간의 공간적 관계(‘쓰레기통은 벽 옆에 있다’, ‘자동차들은 서로 겹치지 않게 나란히 있다’ 등)를 정의하는 추상적인 ’장면 그래프(scene graph)’를 생성한다.5 이 그래프는 장면 설계의 청사진 역할을 한다.</li>
<li><strong>코드 생성:</strong> 다음으로, LLM은 이 장면 그래프를 바탕으로 전문 3D 모델링 소프트웨어인 Blender에서 직접 실행 가능한 Python 스크립트를 작성한다. 이 스크립트는 ’벽 옆에 있다’와 같은 추상적인 관계를 에셋의 구체적인 좌표(x,y,z), 회전값, 크기 등을 결정하는 수치적 제약 조건으로 변환하는 역할을 한다.5</li>
<li><strong>시각적 피드백 및 반복 개선:</strong> 생성된 스크립트를 Blender에서 실행하여 3D 장면을 렌더링하고 그 결과 이미지를 얻는다. 그 후, GPT-4V와 같은 강력한 비전-언어 모델(VLM)이 이 렌더링된 이미지를 ’눈’으로 보고, 원래의 텍스트 설명과 불일치하는 부분이나 부자연스러운 배치를 찾아낸다. 예를 들어, “자동차가 공중에 떠 있습니다. 바닥에 닿도록 위치를 수정해야 합니다“와 같은 구체적인 피드백을 생성한다. 이 피드백을 받은 LLM은 Python 코드를 다시 수정하고, 이 과정을 반복하며 장면의 완성도를 점진적으로 높여나간다.5</li>
</ul>
<p><strong>2. 외부 루프: 라이브러리 학습을 통한 자기 개선 (Outer-Loop: Self-Improvement via Library Learning):</strong></p>
<ul>
<li>SceneCraft는 여러 다른 장면을 생성하는 과정에서 반복적으로 사용되는 유용한 코드 패턴을 스스로 학습한다. 예를 들어, ’여러 객체를 원형으로 배치하는 함수’나 ’특정 에셋을 무작위로 흩뿌리는 함수’와 같이 자주 사용되는 코드 조각들을 식별하여 재사용 가능한 ’라이브러리 함수’로 컴파일한다.38 이 ‘라이브러리 학습’ 메커니즘 덕분에, SceneCraft는 LLM 자체의 파라미터를 미세조정하는 막대한 비용 없이도, 더 많은 장면을 생성할수록 경험을 통해 문제 해결 능력을 지속적으로 향상시킬 수 있다.</li>
</ul>
<h4>3.2.3 주요 결과 및 의의</h4>
<p>실험 결과, SceneCraft는 기존의 다른 LLM 기반 3D 생성 에이전트(BlenderGPT)와 비교했을 때, 생성된 장면의 의미적 일치도를 평가하는 CLIP 점수에서 40% 이상의 괄목할 만한 향상을 보였으며, 주어진 공간 제약 조건을 만족시키는 정확도에서도 압도적으로 높은 성능을 달성했다.5</p>
<p>SceneCraft의 등장은 AI의 역할에 대한 근본적인 패러다임 전환을 상징한다. 이는 AI가 단순히 콘텐츠를 ’생성’하는 것을 넘어, 고도로 전문화된 도구를 능숙하게 ’사용’하는 주체로 진화하고 있음을 보여준다. 텍스트나 이미지를 직접 생성하는 대신, Blender라는 강력하고 정밀한 전문 소프트웨어를 프로그래밍 방식으로 제어함으로써, SceneCraft는 LLM의 추론 및 언어 이해 능력과 전문 도구의 강력한 성능을 결합하는 최상의 시너지를 만들어낸다. 이러한 ‘에이전트’ 접근 방식은 3D 모델링이라는 매우 복잡하고 전문적인 창작 분야를 비전문가도 자연어만으로 접근할 수 있게 만드는 ’기술의 민주화’를 이끄는 강력한 동력이 될 것이다. 더 나아가, 이 연구에서 제시된 ’의도 해석 → 코드 생성 → 실행 및 피드백 → 코드 수정’의 이중 루프 프레임워크는 3D 생성을 넘어, CAD 소프트웨어를 이용한 공학 설계, 디지털 오디오 워크스테이션(DAW)을 이용한 음악 제작, 영상 편집 등 평가 가능한 결과물이 존재하는 모든 복잡한 디지털 창작 활동을 자동화하는 일반적인 청사진을 제시한다. 이는 ‘Text-to-X’ 시대를 넘어, 사용자가 최종 결과물을 자연어로 묘사하면 AI 에이전트가 필요한 모든 도구를 조율하여 이를 실현하는 ‘Intent-to-Product’ 시스템의 미래를 앞당기는 중요한 연구라 할 수 있다.</p>
<h2>4.  로보틱스의 현재와 미래: RSS 2024 및 주요 로봇 연구</h2>
<p>2024년 7월 네덜란드 델프트 공과대학교에서 개최된 ‘로보틱스: 과학과 시스템’(RSS, Robotics: Science and Systems) 학회는 로봇 공학의 이론과 실제를 아우르는 핵심적인 연구 성과들이 발표되는 장이었다.42 이 시기를 전후하여 발표된 로봇 연구들은 AI 기술, 특히 확률적 추론, 강화 학습 등과의 깊은 융합을 통해 로봇의 자율성, 적응성, 그리고 학습 능력을 한 차원 높은 수준으로 끌어올리는 데 집중하고 있음을 보여주었다. 본 장에서는 불확실한 환경에서 로봇의 행동 전략을 최적화하는 새로운 이론적 접근법과, 인간과 가장 유사한 형태를 지닌 휴머노이드 로봇의 제어 및 학습 분야에서 이룩한 실질적인 기술적 진전을 심층적으로 다룬다.</p>
<h3>4.1  효율적 탐사 전략 최적화: Stein 변분 에르고딕 탐색 (Optimizing Efficient Exploration Strategies: Stein Variational Ergodic Search)</h3>
<h4>4.1.1 연구 개요 및 문제 정의</h4>
<p>Darrick Lee 등이 발표하여 RSS 2024에서 구두 발표된 “Stein Variational Ergodic Search“는 로봇이 미지의 연속적인 공간을 효율적으로 탐사(exploration)하고 커버(coverage)하는 근본적인 문제에 대한 새로운 해법을 제시했다.6 로봇이 동적으로 변화하는 복잡한 환경(예: 움직이는 장애물이 있는 공간)을 탐사할 때, 잠재적으로 무한에 가까운 탐사 경로 옵션에 직면하게 된다. 기존의 많은 경로 계획 알고리즘은 특정 비용 함수를 최소화하는 단 하나의 ‘최적’ 경로를 찾는 데 집중했다. 그러나 이러한 단일 최적 경로는 환경이 예기치 않게 변할 경우 무용지물이 되거나 비효율적으로 변할 수 있다. 따라서 이 연구는 단일 최적 경로가 아닌, 목적을 달성할 수 있는 다양한 ‘좋은’ 전략들의 분포 전체를 효율적으로 찾아내는 것을 목표로 한다.7</p>
<h4>4.1.2 핵심 방법론: 확률적 추론으로서의 탐색</h4>
<p>이 연구는 탐사 문제를 결정론적 최적화 문제가 아닌, 확률적 추론 문제로 재구성하는 혁신적인 접근을 취했다.</p>
<ul>
<li><strong>에르고딕 탐색(Ergodic Search)의 확률적 재해석:</strong> 에르고딕 이론은 시스템(여기서는 로봇)이 충분히 긴 시간 동안 움직일 때, 그 궤적이 공간의 각 영역을 방문하는 시간의 비율이 특정 확률 분포를 따르도록 하는 제어 이론이다. 이 연구는 최적의 단일 에르고딕 궤적을 찾는 문제를, ’좋은 에르고딕 궤적들의 사후 확률 분포(posterior distribution over ergodic trajectories)’를 추정하는 베이즈 추론 문제로 재구성했다.6 즉, ’가장 좋은 경로 하나’를 찾는 대신, ’좋은 경로일 확률이 높은 경로들의 집합’을 찾는 것이다.</li>
<li><strong>Stein 변분 방법(Stein Variational Methods)의 적용:</strong> 이렇게 정의된 복잡한 궤적 분포를 근사하기 위해, 연구팀은 비모수적 변분 추론 기법인 Stein 변분 방법을 활용했다.6 이 방법은 여러 개의 ’입자(particle)’를 사용하는데, 여기서 각 입자는 하나의 완전한 탐사 궤적에 해당한다. Stein 변분 방법은 이 입자들을 동시에 병렬적으로 최적화하면서, 입자들 사이에 서로를 밀어내는 힘을 가하여 모든 입자가 단 하나의 최적해로 붕괴(mode collapse)하는 것을 막고, 분포의 여러 다른 봉우리(mode), 즉 서로 다른 다양한 최적 해들로 자연스럽게 수렴하도록 유도한다.7 이는 계산적으로 효율적이면서도 최종적으로 얻어지는 해의 다양성을 보장하는 강력한 장점을 제공한다.46</li>
</ul>
<h4>4.1.3 주요 결과 및 응용</h4>
<p>제안된 방법론은 시뮬레이션과 실제 드론을 이용한 실험을 통해 그 효과를 성공적으로 입증했다.</p>
<ul>
<li><strong>다양한 탐사 전략 생성:</strong> 제안된 방법은 장애물이 있는 복잡한 환경에서, 동일한 탐사 목적을 달성하는 여러 개의 질적으로 다른 경로들(예: 장애물 왼쪽으로 우회하는 경로, 오른쪽으로 우회하는 경로 등)을 동시에 효율적으로 찾아낼 수 있음을 보여주었다.7</li>
<li><strong>온라인 적응:</strong> 이 방법론을 모델 예측 제어(Model Predictive Control, MPC) 프레임워크에 통합하여, 탐사 도중 예기치 않은 동적 장애물이 나타났을 때 로봇이 실시간으로 현재 경로를 폐기하고 미리 계산된 다른 유효한 경로로 전환하는 등 환경 변화에 강건하게 적응하는 능력을 보여주었다.6</li>
</ul>
<p>이 연구는 지능형 로봇 시스템의 행동 계획 패러다임에 중요한 전환을 시사한다. 고전적인 로봇 공학이 주로 단일하고 결정론적인 ‘최적’ 해를 찾는 데 집중했다면, 이 연구는 불확실한 실제 세계에서의 강건성(robustness)과 적응성(adaptability)을 위해 ‘좋은 행동들의 분포’ 자체를 추론하고 활용하는 방향으로 나아가고 있음을 보여준다. 이는 ’경로 계획(path planning)’에서 ’행동 분포 계획(behavioral distribution planning)’으로의 개념적 도약이다. 로봇이 단지 ’하나의 방법’을 아는 것이 아니라, ’여러 좋은 방법들’을 확률적 분포의 형태로 파악하고 있게 되면, 예기치 않은 상황에 직면했을 때 하나의 계획(mode)이 막히더라도 분포 내의 다른 확률 높은 계획으로 자연스럽게 전환할 수 있다. 이러한 내재적 이중성(redundancy)은 인간이나 다른 동적 개체와 함께 작동해야 하는 미래의 자율 시스템이 안전하고 효과적으로 임무를 수행하는 데 있어 필수적인 능력이다. 즉, ’플랜 B’를 갖는 것이 사치가 아닌 필수인 현실 세계에서, 이 연구는 그 ’플랜 B’뿐만 아니라 C, D까지 체계적으로 찾아내는 수학적 토대를 마련한 셈이다.</p>
<h3>4.2  인간을 닮은 기계의 진화: 휴머노이드 원격 조작 및 학습 (The Evolution of Human-like Machines: Humanoid Teleoperation and Learning)</h3>
<h4>4.2.1 연구 개요 및 문제 정의</h4>
<p>2024년 6월 arXiv에 공개된 Tairan He 등의 연구 “OmniH2O: Universal and Dexterous Human-to-Humanoid Whole-Body Teleoperation and Learning“은 인간의 전신 움직임을 실시간으로 모방하여 휴머노이드 로봇을 제어하는 학습 기반 원격 조작(teleoperation) 및 자율 시스템을 선보였다.8 휴머노이드 로봇은 인간 환경에서 인간을 대신하거나 협력하여 작업을 수행할 수 있는 엄청난 잠재력을 지녔지만, 그 잠재력을 실현하기 위해서는 두 발로 안정적으로 걷는 보행(locomotion)과 두 팔과 손으로 물체를 정교하게 다루는 조작(manipulation)을 동시에 수행하는 고도의 ‘전신 제어(whole-body control)’ 기술이 필수적이다. 그러나 이는 로봇의 높은 자유도와 복잡한 동역학 때문에 매우 어려운 문제로 남아있으며, 기존 연구들은 값비싼 모션 캡처 장비에 의존하거나 대규모 학습 데이터를 확보하는 데 어려움을 겪어왔다.9 OmniH2O는 이러한 한계를 극복하고, 보다 접근하기 쉬운 방식으로 강건한 전신 제어 정책을 학습시키는 것을 목표로 한다.</p>
<h4>4.2.2 핵심 방법론: OmniH2O 시스템</h4>
<p>OmniH2O 시스템은 데이터 수집의 확장성과 제어 정책의 강건성을 동시에 확보하기 위해 다각적인 접근법을 취한다.</p>
<ul>
<li>
<p><strong>범용 제어 인터페이스 (Universal Control Interface):</strong> 이 시스템의 가장 큰 특징은 인간의 동작을 로봇의 모터 명령으로 직접 변환하지 않고, ’운동학적 자세(kinematic pose)’라는 추상적인 중간 표현을 사용하는 것이다.9 이는 특정 입력 장치에 종속되지 않는 범용 인터페이스 역할을 한다. 덕분에 고가의 모션 캡처 장비뿐만 아니라, 상용 VR 헤드셋과 컨트롤러, 심지어 단일 RGB 카메라로 사람의 자세를 추정하거나, GPT-4o와 같은 언어 모델이 생성한 자세 지시까지 동일한 제어 정책에 입력으로 사용할 수 있는 뛰어난 유연성과 확장성을 확보했다.9</p>
</li>
<li>
<p><strong>RL 기반 Sim-to-Real 파이프라인:</strong> 실제 로봇에 안전하고 효과적으로 적용 가능한 제어 정책을 학습시키기 위해, 시뮬레이션-현실 전이(sim-to-real) 파이프라인을 정교하게 설계했다.9</p>
</li>
<li>
<p><strong>교사-학생 증류(Teacher-Student Distillation):</strong> 먼저, 시뮬레이션 환경에서만 접근 가능한 모든 정보(privileged information, 예: 로봇 각 신체 부위의 정확한 속도와 위치)를 활용하여 이상적인 움직임을 생성하는 ’교사 정책’을 강화 학습으로 훈련시킨다. 그 후, 실제 로봇에서 수집 가능한 제한된 센서 정보(예: 관절 각도, 관성 측정 장치(IMU) 센서 값)만을 입력으로 사용하는 ’학생 정책’이 이 교사 정책의 행동을 모방하도록 학습(증류, distillation)시킨다. 이 과정을 통해 학생 정책은 현실 세계의 불완전한 정보 속에서도 교사의 최적에 가까운 행동을 수행하는 능력을 내재화하게 된다.9</p>
</li>
<li>
<p><strong>데이터 증강:</strong> AMASS와 같은 대규모 공개 인간 동작 데이터셋을 휴머노이드의 신체 구조에 맞게 변환(리타겟팅)하여 학습 데이터로 활용한다. 특히, 로봇이 불필요하게 발을 계속 움직이는 ‘탭 댄스’ 현상을 줄이고 안정성을 높이기 위해, ’가만히 서 있기’나 ’웅크리기’와 같은 정적인 자세 데이터를 훈련 데이터에 의도적으로 추가하여 증강했다.9</p>
</li>
<li>
<p><strong>OmniH2O-6 데이터셋 공개:</strong> 이 연구 과정에서 수집된 ‘OmniH2O-6’ 데이터셋을 공개했다. 이는 6가지의 다양한 일상 작업을 포함하는 최초의 휴머노이드 전신 제어 데이터셋으로, 원격 조작을 통해 수집되었다. 각 데이터 포인트는 1인칭 시점의 RGBD 영상, 조작자의 제어 입력, 그리고 그에 따른 로봇의 전체 모터 동작 기록으로 구성되어 있어, 향후 모방 학습 및 로봇 파운데이션 모델 연구에 귀중한 자산이 될 것으로 기대된다.8</p>
</li>
</ul>
<h4>4.2.3 주요 결과 및 한계</h4>
<p>OmniH2O 시스템은 실제 휴머노이드 로봇을 통해 테니스 라켓 휘두르기, 물건 집어 옮기기, 그림 그리기 등 다양한 동적 및 정적 작업을 성공적으로 수행함을 보였다.9 하지만 동료 연구자들의 검토 과정에서 몇 가지 한계점도 지적되었다. 이전 연구인 H2O에 비해 성능이 얼마나, 왜 향상되었는지에 대한 비교 분석이 부족하다는 점, 그리고 로봇이 여전히 불필요하게 발을 계속 움직이는 ’탭 댄스’와 같은 불안정한 행동을 보인다는 점, 시연된 작업들이 매우 높은 정밀도를 요구하지는 않는다는 점 등이 그것이다.48 이는 휴머노이드 제어 기술이 아직 가야 할 길이 멀다는 것을 보여주며, 에너지 효율, 속도, 하드웨어 및 소프트웨어의 성숙도 등 해결해야 할 근본적인 과제들이 남아있음을 시사한다.50</p>
<p>그럼에도 불구하고, OmniH2O의 가장 중요한 기여는 제어 알고리즘 자체의 성능 개선을 넘어, 휴머노이드 로봇 연구의 가장 큰 병목 현상인 ’데이터 부족 문제’를 해결할 실마리를 제공했다는 데 있다. LLM이나 비전 모델이 인터넷 규모의 방대한 데이터 위에서 발전한 것과 달리, 로봇 공학, 특히 휴머노이드 분야는 고품질의 대규모 상호작용 데이터를 수집하는 것이 극도로 어렵고 비용이 많이 든다.49 OmniH2O가 제안한 ’범용 제어 인터페이스’는 고가의 전문 장비 없이도 상용 VR 기기 등을 통해 누구나 잠재적으로 휴머노이드의 시연 데이터를 생성할 수 있는 길을 열었다. 이는 로봇 학습을 위한 ’데이터 수집 엔진’을 구축한 것과 같으며, 더 다양한 사람들이 더 다양한 작업을 수행하는 데이터를 제공함으로써 로봇 기술의 일반화와 강건성을 획기적으로 높일 수 있는 ‘데이터 플라이휠’ 효과를 촉발할 수 있다. 알고리즘 자체보다 데이터 수집 인프라에 초점을 맞춘 이러한 접근 방식은, 특수 목적의 로봇 기술을 범용 목적의 체화된 인공지능으로 발전시키는 데 있어 장기적으로 가장 중요한 열쇠가 될 것이다.</p>
<h2>5.  결론: 주요 연구 동향 종합 및 향후 전망</h2>
<p>2024년 6월에 발표된 AI 및 로봇 분야의 주요 연구들은 개별적인 기술적 성취를 넘어, ’디지털 세계의 창조’와 ’물리적 세계와의 상호작용’이라는 두 가지 거대한 축을 중심으로 기술이 깊이 있게 융합되고 있음을 명확히 보여주었다. 본 보고서에서 심층 분석한 핵심 연구들은 AI가 단순한 패턴 인식 및 정보 처리 도구에서 벗어나, 세계를 이해하고, 생성하며, 그 안에서 목적을 가지고 행동하는 ’지능적 주체(Intelligent Agent)’로 진화하고 있음을 시사하는 중요한 변곡점을 형성했다.</p>
<p><strong>2024년 6월 연구 동향 종합</strong></p>
<ul>
<li><strong>생성 모델의 물리적 현실성 내재화:</strong> ’Generative Image Dynamics’와 ‘Mip-Splatting’ 연구는 생성 모델과 3D 비전 기술이 단순히 시각적으로 그럴듯한 결과물을 만드는 수준을 넘어서고 있음을 보여준다. ’Generative Image Dynamics’는 데이터로부터 직접 물리적 동역학(진동)을 학습했으며, ’Mip-Splatting’은 렌더링 과정의 물리적 원리(샘플링 이론)를 알고리즘에 통합하여 결과물의 완성도를 높였다. 이는 AI가 생성하는 가상 세계가 외형뿐만 아니라 작동 방식까지 현실을 모방하며 그 현실성을 극대화하는 방향으로 나아가고 있음을 의미한다.</li>
<li><strong>AI의 에이전트화와 기술의 민주화:</strong> ’SceneCraft’와 ’Rich Human Feedback for Text-to-Image Generation’은 AI가 인간의 파트너로서 새로운 역할을 수행하기 시작했음을 보여준다. ’SceneCraft’는 LLM이 인간 전문가의 복잡한 도구(Blender)를 대신 사용하는 ’에이전트’로 기능하며 고도의 창작 활동을 자동화했다. ’Rich Human Feedback’은 AI가 단순한 칭찬이나 비난을 넘어, 구체적이고 진단적인 비평을 이해하고 이를 통해 학습하는 고도화된 학습 능력을 보여주었다. 이 두 연구는 공통적으로 고도의 전문 기술을 비전문가도 자연어를 통해 쉽게 활용할 수 있도록 하는 ’기술의 민주화’에 크게 기여할 잠재력을 지닌다.</li>
<li><strong>로보틱스의 지능화와 데이터 병목 현상 해결:</strong> ’Stein Variational Ergodic Search’와 ’OmniH2O’는 로봇 공학의 오랜 난제에 대한 AI 기반의 해법을 제시했다. ’Stein Variational Ergodic Search’는 로봇이 불확실한 환경에서 단일 최적해가 아닌 다양한 가능성을 고려하는 확률론적 사고를 통해 행동의 강건성을 높이는 길을 열었다. ’OmniH2O’는 접근성 높은 인터페이스를 통해 휴머노이드 로봇의 학습 데이터 수집이라는 가장 큰 병목 현상을 해결할 실마리를 제공함으로써, 범용 로봇 지능의 실현을 앞당겼다.</li>
</ul>
<p><strong>향후 전망 및 과제</strong></p>
<p>이러한 연구 동향을 바탕으로, 향후 AI 및 로봇 분야는 다음과 같은 방향으로 발전하며 새로운 과제에 직면할 것으로 전망된다.</p>
<ul>
<li><strong>통합 월드 모델(Unified World Models)의 부상:</strong> 현재 개별적으로 발전하고 있는 시각, 언어, 물리적 상호작용 모델들은 결국 하나의 거대한 파운데이션 모델로 통합될 것이다. ’SceneCraft’가 생성한 가상 3D 환경에서 ’OmniH2O’와 같은 로봇 에이전트가 언어적 지시를 받아 작업을 수행하며 학습하는 시나리오가 머지않아 현실화될 것이다. 이는 진정한 의미의 ‘월드 모델’ 구축을 향한 중요한 단계가 될 것이다.</li>
<li><strong>Sim-to-Real의 지속적인 도전:</strong> ’OmniH2O’의 사례에서 드러났듯이, 시뮬레이션에서 학습된 AI 정책을 현실 세계의 로봇으로 성공적으로 이전하는 ‘Sim-to-Real’ 문제는 여전히 가장 큰 기술적 난관 중 하나다. 현실 세계의 복잡한 접촉, 마찰, 변형 등 예측 불가능한 물리 현상을 정확히 모델링하고, 시뮬레이션과 현실의 미세한 차이를 극복하는 기술은 앞으로도 핵심 연구 주제로 남을 것이다.</li>
<li><strong>안전성과 신뢰성 확보의 중요성 증대:</strong> AI 에이전트가 복잡한 소프트웨어를 제어하고 물리적 세계에서 직접 행동함에 따라, 그 행동의 예측 불가능성으로 인한 잠재적 위험을 통제하는 것이 무엇보다 중요해진다. ’Twisted SMC’와 같이 AI의 행동을 수학적으로 엄밀하게 제어하려는 시도와, ’Rich Human Feedback’처럼 AI의 결정을 인간이 이해하고 교정할 수 있도록 하는 해석 가능한 정렬(interpretable alignment) 방법론에 대한 연구는 기술의 발전과 함께 더욱 중요해질 것이다.</li>
</ul>
<p>결론적으로, 2024년 6월의 주요 연구들은 AI가 이제 세계에 대한 수동적인 해석자를 넘어, 디지털과 물리적 현실을 넘나들며 능동적으로 창조하고 행동하는 주체로 거듭나고 있음을 보여주었다. 이러한 변화는 무한한 가능성을 열어주지만, 동시에 그에 상응하는 기술적, 윤리적 책임에 대한 깊은 고찰을 요구하고 있다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>CVPR Paper Awards - IEEE Computer Society Technical Committee on Pattern Analysis and Machine Intelligence, https://tc.computer.org/tcpami/awards/cvpr-paper-awards/</li>
<li>Best papers at CVPR: they are kind of fun! - fxguide, https://www.fxguide.com/quicktakes/best-papers-cvpr-and-they-are-kind-of-fun/</li>
<li>ICML 2024 Conference | OpenReview, https://openreview.net/group?id=ICML.cc/2024/Conference</li>
<li>Downloads 2024 - ICML 2025, https://icml.cc/Downloads/2024</li>
<li>SceneCraft: An LLM Agent for Synthesizing 3D Scenes as Blender Code - GitHub, https://raw.githubusercontent.com/mlresearch/v235/main/assets/hu24g/hu24g.pdf</li>
<li>Science and Systems XX - Online Proceedings - Robotics, https://roboticsproceedings.org/rss20/p001.html</li>
<li>Stein Variational Ergodic Search - Robotics, https://www.roboticsproceedings.org/rss20/p001.pdf</li>
<li>OmniH2O: Universal and Dexterous Human-to- Humanoid Whole-Body Teleoperation and Learning - arXiv, <a href="https://arxiv.org/pdf/2406.08858">https://arxiv.org/pdf/2406.08858?</a></li>
<li>OmniH2O: Universal and Dexterous Human-to … - OpenReview, https://openreview.net/pdf/99227b3200eabc2df5631fb46f3f38f18c842517.pdf</li>
<li>Generative Image Dynamics - RSIP Vision, <a href="https://www.rsipvision.com/ComputerVisionNews-2024July/Computer%20Vision%20News.pdf">https://www.rsipvision.com/ComputerVisionNews-2024July/Computer%20Vision%20News.pdf</a></li>
<li>Top papers from CVPR 2024 : A Comprehensive Overview | by Djohra IBERRAKEN, https://medium.com/@djohraiberraken/top-papers-from-cvpr-2024-comprehensive-overview-7cd32398fc41</li>
<li>Li Generative Image Dynamics CVPR 2024 Paper | PDF - Scribd, https://www.scribd.com/document/771680688/Li-Generative-Image-Dynamics-CVPR-2024-paper</li>
<li>Generative Image Dynamics - CVPR 2024 Open Access Repository, https://openaccess.thecvf.com/content/CVPR2024/html/Li_Generative_Image_Dynamics_CVPR_2024_paper.html</li>
<li>Generative Image Dynamics, https://generative-dynamics.github.io/</li>
<li>[2309.07906] Generative Image Dynamics - arXiv, https://arxiv.org/abs/2309.07906</li>
<li>CVPR 2024: Overview and Key Papers - LearnOpenCV, https://learnopencv.com/cvpr2024/</li>
<li>Generative Image Dynamics - arXiv, https://arxiv.org/html/2309.07906v3</li>
<li>Rich Human Feedback for Text-to-Image Generation - arXiv, https://arxiv.org/html/2312.10240v2</li>
<li>Rich Human Feedback for Text-to-Image … - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_Rich_Human_Feedback_for_Text-to-Image_Generation_CVPR_2024_paper.pdf</li>
<li>Rich Human Feedback for Text-to-Image Generation - ResearchGate, https://www.researchgate.net/publication/384236898_Rich_Human_Feedback_for_Text-to-Image_Generation</li>
<li>[CVPR 2024] Rich Human Feedback for Text-to-Image Generation 논문리뷰, https://aipaper.tistory.com/m/166</li>
<li>Beyond Image Preferences - Rich Human Feedback for Text-to-Image Generation, https://huggingface.co/blog/RapidataAI/beyond-image-preferences</li>
<li>youweiliang/RichHF: Code for CVPR’24 best paper: Rich Human Feedback for Text-to-Image Generation (https://arxiv.org/pdf/2312.10240) - GitHub, https://github.com/youweiliang/RichHF</li>
<li>Mip-Splatting: Alias-free 3D Gaussian Splatting Paper Explained | by Lathika - Medium, https://medium.com/@lathwath5/mip-splating-alias-free-3d-gaussian-splatting-paper-explained-3cfb0a4748db</li>
<li>CVPR Poster Mip-Splatting: Alias-free 3D Gaussian Splatting, https://cvpr.thecvf.com/virtual/2024/poster/29873</li>
<li>Trends and Techniques in 3D Reconstruction and Rendering: A Survey with Emphasis on Gaussian Splatting - MDPI, https://www.mdpi.com/1424-8220/25/12/3626</li>
<li>Mip-Splatting - Zehao Yu, https://niujinshuchong.github.io/mip-splatting/</li>
<li>Mip-Splatting: Alias-free 3D Gaussian Splatting - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_Mip-Splatting_Alias-free_3D_Gaussian_Splatting_CVPR_2024_paper.pdf</li>
<li>Mip-Splatting: Alias-Free 3D Gaussian Splatting - IEEE Computer Society, https://www.computer.org/csdl/proceedings-article/cvpr/2024/530000t447/20hMmalUjok</li>
<li>[CVPR’24 Best Student Paper] Mip-Splatting: Alias-free 3D Gaussian Splatting - GitHub, https://github.com/autonomousvision/mip-splatting</li>
<li>Splats Change Everything: Why a once-obscure technology is taking the 3D graphics world, and Niantic, by storm, https://nianticlabs.com/news/splats-change-everything</li>
<li>CVPR Poster Hardware-Rasterized Ray-Based Gaussian Splatting, https://cvpr.thecvf.com/virtual/2025/poster/32765</li>
<li>ICML 2024 Accepted Paper List - Paper Copilot, https://papercopilot.com/paper-list/icml-paper-list/icml-2024-paper-list/</li>
<li>Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo - arXiv, https://arxiv.org/html/2404.17546v1</li>
<li>Probabilistic Inference in Language Models via Twisted Sequential …, https://proceedings.mlr.press/v235/zhao24c.html</li>
<li>Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo - GitHub, https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhao24c/zhao24c.pdf</li>
<li>Probabilistic Inference in Language Models via Twisted … - arXiv, https://arxiv.org/pdf/2404.17546</li>
<li>SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code - arXiv, https://arxiv.org/html/2403.01248v1</li>
<li>[2403.01248] SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code - arXiv, https://arxiv.org/abs/2403.01248</li>
<li>SceneCraft: Layout-Guided 3D Scene Generation - NIPS, https://proceedings.neurips.cc/paper_files/paper/2024/file/953d276d037e701fcd97dbb34ebb2394-Paper-Conference.pdf</li>
<li>[Literature Review] SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code, https://www.themoonlight.io/en/review/scenecraft-an-llm-agent-for-synthesizing-3d-scene-as-blender-code</li>
<li>Robotics: Science and Systems - Online Proceedings, https://www.roboticsproceedings.org/</li>
<li>Robotics: Science and Systems Jul 15 – Jul 19, 2024 Delft, Netherlands, https://roboticsconference.org/2024/</li>
<li>Stein Variational Ergodic Search - Emergent Mind, https://www.emergentmind.com/articles/2406.11767</li>
<li>Poster rooms - Robotics: Science and Systems, https://roboticsconference.org/2024/program/program/</li>
<li>Stein Variational Ergodic Search - ResearchGate, https://www.researchgate.net/publication/383892175_Stein_Variational_Ergodic_Search</li>
<li>OmniH2O: Universal and Dexterous Human-to-Humanoid Whole-Body Teleoperation and Learning | Request PDF - ResearchGate, https://www.researchgate.net/publication/381404447_OmniH2O_Universal_and_Dexterous_Human-to-Humanoid_Whole-Body_Teleoperation_and_Learning</li>
<li>OmniH2O: Universal and Dexterous Human-to-Humanoid Whole-Body Teleoperation and Learning | OpenReview, <a href="https://openreview.net/forum?id=oL1WEZQal8&amp;noteId=oL1WEZQal8">https://openreview.net/forum?id=oL1WEZQal8¬eId=oL1WEZQal8</a></li>
<li>Humanoid Locomotion and Manipulation: Current Progress and Challenges in Control, Planning, and Learning *co-corresponding authors - arXiv, https://arxiv.org/html/2501.02116v1</li>
<li>The value and limitations of humanoid robots in the warehouse of the future - Supply Chain Management Review, https://www.scmr.com/article/the-value-and-limitations-of-humanoid-robots-in-the-warehouse-of-the-future</li>
<li>On why humanoid robotics will be a very difficult market : r/RealTesla - Reddit, https://www.reddit.com/r/RealTesla/comments/1lnzwnw/on_why_humanoid_robotics_will_be_a_very_difficult/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>