<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2024년 12월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2024년 12월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2024년 AI 및 로봇 연구 동향</a> / <span>2024년 12월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2024년 12월 AI 및 로봇 연구 동향</h1>
<h2>1. 서론</h2>
<p>2024년 12월은 인공지능 및 로봇 공학 분야에서 중요한 변곡점을 기록한 시기이다. 제38회 NeurIPS(신경정보처리시스템학회)와 2024년 CoRL(로봇 학습 학회)을 중심으로 발표된 연구들은 기존 패러다임의 한계를 극복하고 새로운 방향성을 제시했다.1 본 보고서는 이들 학회와 arXiv에 공개된 주요 연구들을 심층적으로 분석하여, 해당 월의 핵심 기술 동향과 그 학술적, 산업적 함의를 도출하고자 한다.</p>
<p>2024년 12월의 연구 지형은 세 가지 주요 패러다임으로 요약될 수 있다. 첫째, <strong>생성 모델의 재정의</strong>이다. 확산 모델의 지배력에 도전하는 새로운 자기회귀 모델(VAR)의 등장과, 기존 모델의 성능을 극한으로 끌어올리는 정교한 유도(guidance) 기법(Autoguidance)이 발표되었다. 이는 생성 품질뿐만 아니라 추론 효율성과 제어 가능성에 대한 연구가 심화되고 있음을 시사한다. 둘째, <strong>체화된 AI를 위한 파운데이션 모델</strong>의 부상이다. 로봇 공학 분야에서는 대규모 시각-언어 모델(VLM)을 기반으로 한 범용 정책(generalist policy) 모델(PoliFormer, OpenVLA)이 주류로 부상하였다. 이는 로봇 학습이 개별 과제 해결에서 벗어나, 사전 훈련된 거대 모델을 특정 로봇 및 환경에 효율적으로 적응시키는 파인튜닝(fine-tuning) 패러다임으로 전환되고 있음을 명확히 보여준다. 셋째, <strong>AI의 효율성 및 사회적 정렬에 대한 심층 탐구</strong>이다. 모델의 규모가 기하급수적으로 증가함에 따라, 훈련 데이터의 질적 가치를 재평가하고(RHO-1), 고차원 연산의 비용을 획기적으로 절감하며(STDE), AI 모델이 다양한 문화적, 개인적 가치와 정렬되어야 한다는 문제의식(PRISM)이 주요 연구 주제로 부상하였다. 이는 AI 기술의 지속 가능성과 사회적 수용성에 대한 고민이 학계의 중심으로 이동하고 있음을 의미한다.</p>
<h2>2.  제38회 신경정보처리시스템학회(NeurIPS 2024) 주요 발표 분석</h2>
<p>NeurIPS 2024는 15,671편의 논문이 제출되어 4,037편이 채택(채택률 25.76%)되는 등, 여전히 AI 분야에서 가장 영향력 있는 학회로서의 위상을 공고히 하였다.5 본 섹션에서는 학회에서 가장 주목받은 최우수 논문상, 차점자 논문, 그리고 데이터셋 및 벤치마크 트랙의 수상작들을 중심으로 최신 연구 동향을 심층 분석한다.</p>
<table><thead><tr><th>Award Category</th><th>Paper Title</th><th>Core Contribution</th><th>Key Authors</th></tr></thead><tbody>
<tr><td>Best Paper</td><td>Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</td><td>확산 모델을 능가하는 성능과 효율성을 달성한 새로운 계층적 자기회귀 이미지 생성 패러다임 ‘VAR’ 제시 6</td><td>Keyu Tian, Yi Jiang, et al.</td></tr>
<tr><td>Best Paper</td><td>Stochastic Taylor Derivative Estimator: Efficient Amortization for Arbitrary Differential Operators</td><td>고차원, 고차 미분 연산의 비용을 획기적으로 줄여 백만 차원 PDE 문제 해결을 가능하게 한 ‘STDE’ 방법론 개발 6</td><td>Zekun Shi, Kenji Kawaguchi, et al.</td></tr>
<tr><td>Best Paper Runner-up</td><td>Not All Tokens Are What You Need for Pretraining</td><td>‘초과 손실’ 개념을 도입하여 LLM 사전 훈련의 데이터 효율성과 성능을 극대화하는 ‘선택적 언어 모델링(SLM)’ 제안 6</td><td>Zhenghao Lin, Yeyun Gong, et al.</td></tr>
<tr><td>Best Paper Runner-up</td><td>Guiding a Diffusion Model with a Bad Version of Itself</td><td>이미지 품질과 다양성을 분리하여 제어하는 새로운 ‘자기유도(Autoguidance)’ 기법을 통해 확산 모델의 생성 능력을 향상 6</td><td>Tero Karras, Miika Aittala, et al.</td></tr>
<tr><td>Datasets &amp; Benchmarks Best Paper</td><td>The PRISM Alignment Dataset</td><td>75개국 1,500명의 피드백을 수집하여 LLM의 주관적, 다문화적 정렬 연구를 위한 ‘PRISM’ 데이터셋 구축 6</td><td>Hannah Rose Kirk, Scott A. Hale, et al.</td></tr>
</tbody></table>
<h3>2.1  최우수 논문상(Best Paper Award) 심층 분석</h3>
<h4>2.1.1  Visual Autoregressive Modeling (VAR): 생성 모델의 새로운 지평</h4>
<p><strong>연구 목표 및 배경:</strong> 기존 자기회귀(AR) 모델은 텍스트 생성에서 큰 성공을 거두었으나, 이미지에 적용 시 2D 공간 구조를 무시하고 1D 래스터 스캔 순서로 토큰을 예측하여 성능과 효율성에 한계가 있었다.8 한편, 확산 모델(Diffusion Model)은 높은 이미지 품질을 달성했지만, 다단계 추론 과정으로 인한 속도 저하 문제가 있었다. VAR은 이러한 한계를 극복하고 AR 모델의 잠재력을 최대한 활용하여 확산 모델을 능가하는 것을 목표로 한다.8</p>
<p>핵심 방법론: ‘Next-Scale Prediction’</p>
<p>VAR은 이미지를 저해상도에서 고해상도로 점진적으로 생성하는 ‘Next-Scale Prediction’ 패러다임을 도입한다.6 이는 인간이 이미지를 인식하는 계층적 방식, 즉 전체적인 구조를 먼저 파악한 후 세부적인 묘사로 들어가는 과정과 유사하다.13</p>
<ul>
<li>
<p><strong>Multi-scale VQVAE:</strong> 먼저, 이미지는 다중 스케일 VQVAE 인코더를 통해 여러 해상도의 토큰 맵(<span class="math math-inline">r_1, r_2,..., r_K</span>)으로 변환된다. 이 과정에서 이미지는 계층적인 이산 표현(discrete representation)을 갖게 된다.8</p>
</li>
<li>
<p><strong>Autoregressive Transformer:</strong> 그 후, VAR 트랜스포머는 가장 낮은 해상도의 토큰 맵 <span class="math math-inline">r_1</span>부터 시작하여, 이전 단계의 모든 저해상도 맵들을 조건으로 다음 단계의 고해상도 맵을 예측한다. 자기회귀 확률은 다음과 같이 공식화된다 13:<br />
<span class="math math-display">
p(r_1, r_2,..., r_K) = \prod_{k=1}^{K} p(r_k \vert r_1,..., r_{k-1})
</span></p>
</li>
</ul>
<p><strong>주요 결과 및 함의:</strong></p>
<ul>
<li><strong>성능:</strong> ImageNet 256x256 벤치마크에서 기존 AR 모델의 FID(Fréchet Inception Distance)를 18.65에서 1.73으로 획기적으로 개선했으며, 추론 속도는 약 20배 향상시켰다.14 또한, 대표적인 확산 모델인 DiT(Diffusion Transformer)를 이미지 품질, 추론 속도, 데이터 효율성 등 다방면에서 능가함을 입증했다.6</li>
<li><strong>확장 법칙(Scaling Laws):</strong> 대규모 언어 모델(LLM)에서 관찰된 것과 유사한 명확한 멱법칙(power-law) 확장 법칙을 보여주었다 (선형 상관 계수 ≈ -0.998). 이는 모델 크기를 키움에 따라 성능 향상을 예측 가능하게 하여, 시각 생성 모델 연구에 체계적인 접근을 가능하게 한다.6</li>
<li><strong>제로샷 일반화:</strong> 이미지 인페인팅, 아웃페인팅, 편집 등 별도의 훈련 없이 제로샷으로 다운스트림 작업을 수행하는 능력을 보였다.14</li>
</ul>
<h4>2.1.2  Stochastic Taylor Derivative Estimator (STDE): 고차원 미분 연산의 혁신</h4>
<p><strong>문제 정의:</strong> 물리 정보 신경망(PINNs) 등 과학 계산 분야에서는 손실 함수에 고차원(high-dimensional, <span class="math math-inline">d</span>) 및 고차(high-order, <span class="math math-inline">k</span>) 미분 연산자가 포함되는 경우가 많다. 기존의 역전파(back-propagation) 방식은 미분 텐서 크기가 <span class="math math-inline">O(d^k)</span>로, 계산 그래프 크기가 <span class="math math-inline">O(2^{k-1}L)</span>로 폭발하여 대규모 문제에 적용하기 어려웠다.6</p>
<p><strong>핵심 원리:</strong> STDE는 테일러 모드 자동 미분(Taylor-mode AD)과 무작위화(randomization)를 결합하여 이 문제를 해결한다. 핵심 아이디어는 임의의 미분 연산자를 무작위로 샘플링된 제트(jet, 방향 도함수를 나타내는 벡터)의 기댓값으로 근사하는 것이다.18 STDE는 다변수 함수의 고차 미분 텐서 전체를 계산하는 대신, 적절히 구성된 입력 탄젠트(tangent)를 사용하여 단변수 고차 AD를 통해 텐서의 특정 수축(contraction)을 효율적으로 계산한다.17 이를 통해 미분 텐서를 명시적으로 저장하지 않고도 미분 연산자를 확률적으로 추정할 수 있어 메모리 및 계산 복잡도를 획기적으로 줄인다.18</p>
<p><strong>주요 결과 및 함의:</strong></p>
<ul>
<li><strong>성능:</strong> PINNs에 적용 시, 기존 1차 AD 기반 무작위화 방식 대비 1000배 이상의 속도 향상과 30배 이상의 메모리 절감 효과를 보였다.10</li>
<li><strong>초고차원 문제 해결:</strong> 단일 NVIDIA A100 GPU를 사용하여 100만 차원(1-million-dimensional) 편미분방정식(PDE)을 8분 만에 해결하는 성과를 달성했다.9 이는 기존 방법으로는 불가능했던 규모의 문제 해결 가능성을 연 것이다. 공식 구현은 GitHub을 통해 공개되었다.21</li>
</ul>
<p>NeurIPS 2024 최우수 논문상이 VAR과 STDE라는, 성격이 매우 다른 두 연구에 공동 수여된 것은 AI 분야의 발전을 상징적으로 보여준다.5 VAR은 인간의 시각 인지 과정(계층적, coarse-to-fine)을 모방한 직관적 아이디어에서 출발하여, 기존 생성 모델의 패러다임을 바꾸고 LLM과 유사한 ’확장 법칙’이라는 경험적 법칙을 발견했다.13 이는 경험적 성공과 일반화를 중시하는 연구 흐름을 대표한다. 반면, STDE는 고차 미분이라는 근본적인 수학적 문제에 천착하여, 테일러 모드 AD와 확률적 추정이라는 엄밀한 수학적 도구를 통해 기존의 계산적 병목 현상을 이론적으로 해결하고, 이를 통해 초고차원 물리 문제 해결이라는 실질적 돌파구를 열었다.10 이는 이론적 기반과 계산 효율성을 중시하는 연구 흐름을 대표한다. 이 두 논문의 동시 수상은 AI 분야가 복잡한 현상을 모방하고 확장하는 ‘경험적/직관적’ 접근과, 기반이 되는 수학 및 계산 과학의 한계를 돌파하는 ‘이론적/엄밀한’ 접근이라는 두 가지 축을 중심으로 균형 있게 발전하고 있음을 나타낸다. 한쪽은 ‘무엇을’ 할 수 있는가의 경계를 넓히고, 다른 한쪽은 ‘어떻게’ 더 효율적이고 정확하게 할 수 있는가의 깊이를 더하며 AI 연구의 성숙도를 보여주고 있다.</p>
<h3>2.2  주목할 만한 차점자 논문(Runner-Up Papers) 및 기여</h3>
<h4>2.2.1  Not All Tokens Are What You Need for Pretraining: 선택적 언어 모델링(SLM)의 부상</h4>
<p><strong>핵심 아이디어:</strong> 기존 LLM 사전 훈련이 모든 토큰에 동일한 손실을 적용하는 것의 비효율성을 지적하며, “모든 토큰이 훈련에 동등하게 중요하지는 않다“고 주장한다.11</p>
<p><strong>방법론: ‘초과 손실(Excess Loss)’ 기반 토큰 선택</strong></p>
<ol>
<li>고품질 데이터로 ’참조 모델(Reference Model)’을 먼저 훈련한다.24</li>
<li>사전 훈련 데이터의 각 토큰에 대해, 현재 훈련 중인 모델의 손실과 참조 모델의 손실 간의 차이, 즉 ’초과 손실’을 계산한다.24</li>
<li>초과 손실이 높은, 즉 ‘배울 가치가 있는’ 토큰에 대해서만 손실을 계산하고 역전파를 수행한다. 이는 너무 쉽거나(이미 학습됨) 너무 어려운(노이즈일 가능성이 높음) 토큰에 대한 낭비적인 계산을 줄인다.22</li>
</ol>
<p><strong>결과:</strong> 이 방법론을 적용한 RHO-1 모델은 MATH 데이터셋에서 DeepSeekMath와 유사한 성능을 단 3%의 사전 훈련 토큰만으로 달성했다.11 이는 데이터 효율성을 극적으로 향상시킨 결과이다. 다만, OpenReview 토론에서는 이 방법론이 ICML 2022에서 발표된 ’RhoLoss’와 본질적으로 동일하다는 지적이 제기되었으며, 계산량 절감 효과가 과장되었을 수 있다는 비판도 있었다.26 이러한 학술적 논의는 해당 기술의 독창성과 실효성을 평가하는 데 있어 중요한 고려사항이다.</p>
<h4>2.2.2  Guiding a Diffusion Model with a Bad Version of Itself: 자기유도(Autoguidance)의 발견</h4>
<p><strong>문제 제기:</strong> 널리 사용되는 분류기 없는 유도(Classifier-Free Guidance, CFG)는 프롬프트 정렬과 이미지 품질을 향상시키지만, 생성 결과의 다양성을 감소시키는 부작용이 있으며, 이 두 효과를 분리하여 제어하기 어렵다.12</p>
<p><strong>핵심 아이디어:</strong> 조건부 모델을 유도하기 위해 ‘조건 없는(unconditional)’ 모델을 사용하는 대신, ‘더 작거나 덜 훈련된(a bad version)’ 자기 자신을 사용한다.6 이 ’나쁜 버전’의 모델 예측으로부터 멀어지는 방향으로 샘플링을 유도하는 것이다.</p>
<p><strong>결과 및 함의:</strong> 이 ‘자기유도(Autoguidance)’ 방식은 이미지 품질 향상 효과는 유지하면서도 다양성 감소 문제를 해결하여, 두 요소를 분리하여 제어할 수 있는 가능성을 열었다.27 ImageNet 생성에서 FID 점수를 1.25(512x512)까지 낮추며 새로운 최고 성능(SOTA)을 기록했다.29 이는 확산 모델의 제어 가능성을 한 단계 높인 중요한 연구이다.</p>
<p>RHO-1(SLM)과 Autoguidance 연구는 AI 분야의 패러다임이 ’규모의 경제’에서 ’효율의 경제’로 전환되고 있음을 시사한다. 지난 몇 년간 AI 연구는 모델과 데이터의 크기를 키우는 데 집중해왔다. 그러나 RHO-1은 무작정 많은 토큰을 학습하는 것이 아니라 ‘어떤’ 토큰을 학습할 것인가에 집중하여, 동일한 성능을 훨씬 적은 계산량과 데이터로 달성하게 함으로써 훈련의 ’효율성’을 극대화한다.23 마찬가지로, Autoguidance는 CFG의 부작용을 해결하여 추가적인 모델 훈련 없이 샘플링 과정 자체를 ’효율화’하고 제어의 정밀도를 높인다.27 이 두 연구는 AI 기술이 단순히 리소스를 투입하여 규모를 키우는 단계를 넘어, 주어진 리소스를 가장 효율적으로 사용하여 최상의 결과를 얻는 방향으로 발전하고 있음을 보여준다. 이는 AI 기술의 산업적 적용과 지속 가능성을 위해 필수적인 단계이며, 향후 데이터 큐레이션, 훈련 알고리즘, 샘플링 전략에 대한 연구가 더욱 중요해질 것임을 예고한다.</p>
<h3>2.3  데이터셋 및 벤치마크 트랙: PRISM 정렬 데이터셋</h3>
<p><strong>연구 동기:</strong> 대규모 언어 모델(LLM)을 인간의 가치와 정렬(align)시키는 연구는 활발하지만, ‘누구의(who)’ 가치에 정렬시킬 것인가에 대한 문제는 간과되어 왔다. 기존 데이터셋은 주로 특정 지역(예: 미국)의 인구통계학적 특성에 편중되어 있었다.6</p>
<p><strong>PRISM 데이터셋의 기여:</strong></p>
<ul>
<li>75개국 1,500명의 다양한 인구통계학적 배경을 가진 참가자들의 피드백을 수집했다.6</li>
<li>참가자의 사회인구학적 정보, 명시적 선호도, 그리고 21개 LLM과의 실제 대화에서 나타난 맥락적 선호도를 연결하여, 개인화되고 다문화적인 정렬 연구를 가능하게 했다.6</li>
<li>가치 판단이 개입되는 논쟁적인 주제에 초점을 맞춰, 사람들 간 및 문화 간 의견 불일치를 연구할 수 있는 기반을 마련했다.6</li>
</ul>
<p><strong>함의:</strong> PRISM 데이터셋은 LLM 정렬 연구가 단일한 정답을 찾는 것을 넘어, ’다원적 정렬(Pluralistic Alignment)’이라는 새로운 방향으로 나아가야 함을 실증적으로 보여준다.31 이는 AI 윤리 및 거버넌스 논의에 중요한 실증적 데이터를 제공한다.</p>
<h3>2.4  시간의 시험상(Test of Time Award): GAN과 Seq2Seq의 유산</h3>
<p><strong>선정 논문:</strong> 10년 전인 NeurIPS 2014에서 발표된 “Generative Adversarial Nets” (Ian Goodfellow et al.)와 “Sequence to Sequence Learning with Neural Networks” (Ilya Sutskever et al.)가 공동 수상했다.32</p>
<p><strong>학술적 유산:</strong></p>
<ul>
<li><strong>GAN:</strong> 생성 모델링 분야의 혁명을 일으켰으며, 지난 10년간 수많은 후속 연구에 영감을 주었다. 현재 확산 모델이 주류이지만, 적대적 학습의 개념은 여전히 다양한 분야에서 활용되고 있다.32</li>
<li><strong>Seq2Seq:</strong> 인코더-디코더 아키텍처를 정립하였고, 이는 이후 어텐션 메커니즘과 결합하여 오늘날 트랜스포머 및 LLM의 근간이 되었다. 현대 AI 패러다임 전환의 초석을 다진 연구로 평가된다.32</li>
</ul>
<p><strong>함의:</strong> 이 두 논문의 수상은 현재 AI 기술의 폭발적 성장이 과거의 근본적인 아이디어에 깊이 뿌리내리고 있음을 상기시킨다. 이는 기초 연구의 장기적인 중요성을 강조하는 사례이다.</p>
<h2>3.  2024 로봇 학습 학회(CoRL 2024) 핵심 연구 동향</h2>
<p>2024년 11월 6일부터 9일까지 독일 뮌헨에서 개최된 CoRL 2024는 로봇 공학과 기계 학습의 교차점에서 가장 중요한 연구들이 발표되는 장이다.4 올해 학회는 특히 대규모 사전 훈련 모델을 로봇 제어에 접목하는 VLA(Vision-Language-Action) 모델과 관련된 연구가 두드러졌다.</p>
<table><thead><tr><th>Award Category</th><th>Paper Title</th><th>Architecture / Method</th><th>Key Contribution</th></tr></thead><tbody>
<tr><td>Outstanding Paper</td><td>PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators</td><td>ViT Encoder + Causal Transformer Decoder / On-Policy RL</td><td>대규모 시뮬레이션 훈련을 통해 실제 로봇에서 제로샷으로 높은 네비게이션 성공률(CHORES-S 85.5%) 달성 34</td></tr>
<tr><td>Outstanding Paper Finalist</td><td>OpenVLA: An Open-Source Vision-Language-Action Model</td><td>Llama 2 + DINOv2/SigLIP Fusion / Imitation Learning</td><td>97만 개의 실제 로봇 데이터로 훈련된 최초의 고성능 오픈소스 7B VLA 모델. RT-2-X 대비 적은 파라미터로 우수한 성능 34</td></tr>
<tr><td>N/A</td><td>GenDP: 3D Semantic Fields for Category-Level Generalizable Diffusion Policy</td><td>Diffusion Policy + 3D Semantic Fields</td><td>확산 정책의 일반화 성능을 크게 향상(평균 성공률 20%→93%)시켜 보지 못한 카테고리의 객체 조작을 가능하게 함 39</td></tr>
<tr><td>N/A</td><td>Robotic Control via Embodied Chain-of-Thought Reasoning</td><td>LLM-based Chain-of-Thought</td><td>로봇이 행동 계획을 수립할 때 내적 독백(inner monologue)과 같은 연쇄적 사고를 통해 복잡한 작업을 해결하는 능력 입증 3</td></tr>
</tbody></table>
<h3>3.1  최우수 논문: PoliFormer - 대규모 온폴리시 강화학습과 트랜스포머의 결합</h3>
<p><strong>아키텍처:</strong> 사전 훈련된 ViT(DINOv2)를 비전 인코더로, Causal Transformer를 디코더로 사용하여 장기 기억과 추론 능력을 갖춘 정책 모델을 구현했다.36</p>
<p><strong>훈련 방법론:</strong> 수억 번의 상호작용에 달하는 대규모 온폴리시(On-policy) 강화학습을 시뮬레이션 환경(ProcTHOR 등)에서 수행했다. 병렬화된 롤아웃을 통해 높은 처리량으로 효율적인 훈련을 달성했다.36</p>
<p><strong>핵심 성과: 제로샷 Sim-to-Real 전이</strong></p>
<ul>
<li>순수하게 시뮬레이션에서만 훈련했음에도 불구하고, 별도의 파인튜닝 없이 실제 로봇(LoCoBot, Stretch RE-1) 환경에 성공적으로 일반화되었다.35</li>
<li>CHORES-S 벤치마크에서 85.5%의 전례 없는 성공률을 기록하며, 기존 SOTA 대비 28.5%의 절대적 성능 향상을 이루었다.35</li>
</ul>
<p><strong>함의:</strong> PoliFormer의 성공은 강화학습 기반 로봇 정책도 충분한 규모의 데이터와 컴퓨팅, 그리고 적절한 아키텍처가 결합되면 모방학습 기반 모델처럼 강력한 일반화 성능을 확보할 수 있음을 증명했다.</p>
<h3>3.2  시각-언어-행동(VLA) 모델의 부상: OpenVLA</h3>
<p><strong>연구 동기:</strong> 기존의 강력한 VLA 모델(예: RT-2-X)은 대부분 비공개(closed-source)로, 학계의 접근성과 연구 확산을 저해했다.44</p>
<p><strong>아키텍처 및 데이터:</strong> Llama 2 언어 모델을 기반으로, DINOv2와 SigLIP의 시각 특징을 융합하는 듀얼 인코더 방식을 채택했다. 97만 개의 실제 로봇 시연 데이터셋으로 훈련된 7B 파라미터 모델이다.38</p>
<p><strong>성능 및 효율성:</strong></p>
<ul>
<li>55B 파라미터의 RT-2-X 모델보다 7배 적은 파라미터로, 29개 다중 작업 벤치마크에서 16.5% 더 높은 절대 성공률을 달성했다.38</li>
<li>LoRA와 같은 파라미터 효율적 파인튜닝(PEFT) 기법을 통해 소비자용 GPU에서도 새로운 작업에 대한 미세조정이 가능함을 보여, VLA 모델의 접근성을 크게 높였다.45</li>
</ul>
<p><strong>함의:</strong> OpenVLA는 고성능 VLA 모델을 오픈소스로 공개함으로써, 로봇 공학 연구의 민주화에 기여하고 VLA 기반 연구의 표준 플랫폼이 될 잠재력을 가진다.44</p>
<p>PoliFormer와 OpenVLA의 성공은 로봇 학습의 핵심 패러다임이 ’처음부터 학습(from scratch)’에서 ’사전 훈련 후 파인튜닝(pre-train and fine-tune)’으로 명백히 전환되었음을 선언하는 것과 같다. 전통적인 로봇 학습은 특정 작업을 위해 특정 로봇에서 데이터를 수집하고 정책을 훈련하는 방식이 주를 이루었으나, 이는 데이터 수집 비용이 높고 일반화가 어려웠다. NLP와 컴퓨터 비전 분야에서 ‘Pre-train, Fine-tune’ 패러다임이 압도적인 성공을 거두면서, 대규모 VLM의 등장은 이 패러다임을 로봇 행동 생성에까지 확장할 수 있는 기술적 기반을 제공했다. 이제 로봇 공학의 핵심 과제는 (1) 다양하고 방대한 로봇 상호작용 데이터를 어떻게 수집하고 공유할 것인가, (2) 거대한 VLA 모델을 특정 로봇과 작업에 어떻게 효율적으로 파인튜닝할 것인가, (3) 시뮬레이션과 실제 세계 간의 간극을 어떻게 최소화할 것인가로 재정의되고 있다. 이는 로봇 연구의 진입 장벽을 낮추는 동시에, 대규모 데이터와 컴퓨팅 자원을 보유한 소수 그룹의 영향력을 강화하는 이중적인 효과를 낳을 수 있다.</p>
<h3>3.3  로봇 조작 및 상호작용의 새로운 패러다임</h3>
<ul>
<li><strong>확산 정책의 일반화(Generalizing Diffusion Policies):</strong> GenDP는 확산 정책에 3D 시맨틱 필드를 결합하여, 훈련 데이터에 없던 새로운 카테고리의 객체에 대한 조작 성공률을 20%에서 93%로 극적으로 향상시켰다.39 이는 기하학적, 의미론적 이해가 로봇 조작의 일반화에 필수적임을 보여준다.</li>
<li><strong>체화된 연쇄적 사고(Embodied Chain-of-Thought):</strong> 로봇이 복잡한 지시를 수행하기 전에 “내적 독백“처럼 행동 계획을 언어적으로 생성하고 수정하는 연구가 발표되었다.3 이는 LLM의 추론 능력을 로봇의 물리적 행동 계획에 직접적으로 연결하려는 시도이다.</li>
<li><strong>촉각과 시각의 융합(Visuotactile Learning):</strong> 물체가 시야에서 가려진 상황에서도 촉각 정보를 활용하여 상태를 추정하고 비파지(non-prehensile) 조작을 수행하는 연구가 발표되었다.33 이는 복잡하고 불확실한 환경에서의 로봇 조작 능력을 향상시키는 데 중요한 기여를 한다.</li>
</ul>
<h2>4.  arXiv를 통해 본 2024년 12월 AI 및 로봇 연구의 확장</h2>
<p>학회 발표 외에도, 12월 arXiv에는 향후 연구 방향을 가늠할 수 있는 중요한 기술 보고서와 선행 연구들이 다수 공개되었다.46</p>
<h3>4.1  거대 언어 모델의 진화: DeepSeek-V3 기술 보고서</h3>
<p><strong>개요:</strong> DeepSeek-AI가 발표한 671B 파라미터의 전문가 혼합(MoE) 모델로, 각 토큰 당 37B 파라미터만 활성화하여 효율성을 높였다.50</p>
<p><strong>기술적 특징:</strong></p>
<ul>
<li>14.8조 개의 방대한 토큰으로 사전 훈련되었다.50</li>
<li>로드 밸런싱을 위한 별도의 보조 손실(auxiliary loss)이 없는 새로운 MoE 훈련 전략을 도입했다.50</li>
<li>FP8 혼합 정밀도 훈련을 대규모 모델에 성공적으로 적용하여 훈련 속도와 메모리 효율성을 개선했다.50</li>
</ul>
<p><strong>성능:</strong> MMLU-Pro, GPQA 등 주요 벤치마크에서 다른 오픈소스 모델을 능가하고, GPT-4o, Claude-3.5-Sonnet 등 최신 비공개 모델과 필적하는 성능을 보였다.50</p>
<p><strong>함의:</strong> MoE 아키텍처와 혁신적인 훈련 기법을 통해, 모델의 성능과 훈련 효율성을 동시에 달성할 수 있음을 보여주었다. 이는 LLM 경쟁이 파라미터 크기뿐만 아니라 아키텍처와 훈련 방법론의 효율성 경쟁으로 심화되고 있음을 시사한다.</p>
<table><thead><tr><th>Benchmark</th><th>DeepSeek-V3 (671B MoE)</th><th>GPT-4o</th><th>Claude-3.5-Sonnet</th></tr></thead><tbody>
<tr><td>MMLU-Pro</td><td>75.9</td><td>Comparable</td><td>Comparable</td></tr>
<tr><td>GPQA</td><td>59.1</td><td>Comparable</td><td>Comparable</td></tr>
<tr><td>MATH-500</td><td>State-of-the-art</td><td>Outperformed</td><td>Outperformed</td></tr>
<tr><td>LiveCodeBench</td><td>Top-performing</td><td>N/A</td><td>N/A</td></tr>
<tr><td>Chinese SimpleQA</td><td>Superior</td><td>Surpassed</td><td>Surpassed</td></tr>
</tbody></table>
<h3>4.2  AI의 사회적 및 환경적 영향에 대한 정량적 접근</h3>
<ul>
<li><strong>공중 보건 영향 정량화:</strong> AI 모델의 생애주기(반도체 제조부터 데이터센터 운영까지)에서 발생하는 미세먼지 등 대기 오염 물질이 공중 보건에 미치는 영향을 계량화하는 방법론이 제시되었다. Llama3.1 규모의 모델 훈련이 로스앤젤레스-뉴욕 간 자동차 왕복 1만 회 이상의 대기 오염을 유발할 수 있다고 분석했다.51</li>
<li><strong>환경 영향의 리바운드 효과:</strong> AI 모델의 에너지 효율성을 높이려는 노력이 오히려 더 큰 모델의 개발을 촉진하여, 총 환경 영향은 기하급수적으로 증가하는 ’리바운드 효과’가 발생하고 있음을 지적한 연구가 발표되었다. 이는 효율성 개선만으로는 AI의 환경 발자국을 줄일 수 없음을 시사한다.52</li>
<li><strong>함의:</strong> AI의 성능 향상에 대한 연구와 더불어, 그 사회적, 환경적 비용을 정량적으로 측정하고 완화하려는 연구가 본격화되고 있다. 이는 AI 기술의 지속 가능한 발전을 위해 반드시 고려해야 할 중요한 연구 방향이다.</li>
</ul>
<h2>5. 결론</h2>
<p>2024년 12월의 AI 및 로봇 공학 연구는 ‘규모’, ‘효율’, ‘일반화’, ’정렬’이라는 네 가지 키워드로 요약된다. VAR과 DeepSeek-V3는 각각 시각과 언어 모델의 ’규모’와 성능의 한계를 밀어붙였고, STDE와 RHO-1은 그 이면에 있는 계산 및 데이터 ‘효율성’ 문제를 정면으로 다루었다. 로봇 공학 분야에서는 PoliFormer와 OpenVLA가 대규모 사전 훈련 모델을 통해 전례 없는 ‘일반화’ 능력을 선보이며 새로운 패러다임을 열었다. 마지막으로, PRISM 데이터셋과 AI의 환경 영향 분석 연구는 기술이 인간 사회 및 환경과 어떻게 ’정렬’되어야 하는지에 대한 깊은 성찰을 요구하고 있다.</p>
<p>이러한 연구 흐름은 향후 AI 분야가 더욱 정교한 아키텍처, 데이터 중심의 최적화, 그리고 물리 세계와의 상호작용을 통한 체화된 지능 구현에 집중할 것임을 예고한다. 동시에, 기술의 사회적 책임과 지속 가능성에 대한 논의는 AI 연구 개발의 필수적인 일부로 자리 잡을 것이다. 본 보고서에서 분석한 연구들은 이러한 미래를 향한 중요한 이정표가 될 것이다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>NeurIPS 2024 Call for Papers, https://neurips.cc/Conferences/2024/CallForPapers</li>
<li>2024 Conference - NeurIPS 2025, https://neurips.cc/Conferences/2024</li>
<li>CoRL 2024 Conference | OpenReview, https://openreview.net/group?id=robot-learning.org/CoRL/2024/Conference</li>
<li>CoRL 2024, https://2024.corl.org/</li>
<li>5 Top Papers of NeurIPS 2024 That You Must Read - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2024/12/neurips-best-paper/</li>
<li>NeurIPS 2024 Awards, https://neurips.cc/virtual/2024/awards_detail</li>
<li>Announcing the NeurIPS 2024 Best Paper Awards, https://blog.neurips.cc/2024/12/10/announcing-the-neurips-2024-best-paper-awards/</li>
<li>Visual Autoregressive Modeling : The NeurIPS 2024 Best Paper Revolutionizing Image Generation | by Daniel Park | Medium, https://boxworld.medium.com/visual-autoregressive-modeling-the-neurips-2024-best-paper-revolutionizing-image-generation-52d8810df5f6</li>
<li>NUS Presidential Young Professor Kenji Kawaguchi wins Best Paper Award at NeurIPS 2024, https://news.nus.edu.sg/prof-kenji-kawaguchi-wins-best-paper-award-neurips-2024/</li>
<li>(PDF) Stochastic Taylor Derivative Estimator: Efficient amortization for arbitrary differential operators - ResearchGate, https://www.researchgate.net/publication/386374952_Stochastic_Taylor_Derivative_Estimator_Efficient_amortization_for_arbitrary_differential_operators</li>
<li>NeurIPS 2024 Not All Tokens Are What You Need for Pretraining Oral, https://neurips.cc/virtual/2024/oral/98004</li>
<li>Guiding a Diffusion Model with a Bad Version of Itself - arXiv, https://arxiv.org/html/2406.02507v1</li>
<li>Visual Autoregressive Modeling: Scalable Image Generation … - NIPS, https://proceedings.neurips.cc/paper_files/paper/2024/file/9a24e284b187f662681440ba15c416fb-Paper-Conference.pdf</li>
<li>NeurIPS Poster Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction, https://neurips.cc/virtual/2024/poster/94115</li>
<li>Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction, https://openreview.net/forum?id=gojL67CfS8</li>
<li>[2404.02905] Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction - arXiv, https://arxiv.org/abs/2404.02905</li>
<li>[2412.00088] Stochastic Taylor Derivative Estimator: Efficient amortization for arbitrary differential operators - arXiv, https://arxiv.org/abs/2412.00088</li>
<li>Stochastic Taylor Derivative Estimator: Efficient amortization for …, https://arxiv.org/pdf/2412.00088</li>
<li>Stochastic Taylor Derivative Estimator for High-Order Differential Operators 2 | UPB, https://upb.ro/wp-content/uploads/2025/07/SCSS_2025_Paper_Alex_Deonise.pdf</li>
<li>NeurIPS Poster Stochastic Taylor Derivative Estimator: Efficient amortization for arbitrary differential operators, https://neurips.cc/virtual/2024/poster/95741</li>
<li>sail-sg/stde: Official implementation of Stochastic Taylor Derivative Estimator (STDE) NeurIPS2024 - GitHub, https://github.com/sail-sg/stde</li>
<li>Rho-1: Not All Tokens Are What You Need - arXiv, https://arxiv.org/html/2404.07965v4</li>
<li>NeurIPS 2024 Papers from Microsoft Research Asia, https://www.microsoft.com/en-us/research/articles/msra-neurips-2024-papers/</li>
<li>Not All Tokens Are What You Need for Pretraining - OpenReview, https://proceedings.neurips.cc/paper_files/paper/2024/file/3322a9a72a1707de14badd5e552ff466-Paper-Conference.pdf</li>
<li>[Paper-club sessions] Rho-1: Not All Tokens Are What You Need - CloudWalk, https://www.cloudwalk.io/ai/paper-club-sessions-rho-1-not-all-tokens-are-what-you-need</li>
<li>Not All Tokens Are What You Need for Pretraining - OpenReview, https://openreview.net/forum?id=0NMzBwqaAJ</li>
<li>Guiding a Diffusion Model with a Bad Version of Itself | OpenReview, https://openreview.net/forum?id=bg6fVPVs3s</li>
<li>Guiding a Diffusion Model with a Bad Version of Itself - ResearchGate, https://www.researchgate.net/publication/381157859_Guiding_a_Diffusion_Model_with_a_Bad_Version_of_Itself</li>
<li>Guiding a Diffusion Model with a Bad Version of Itself - Aalto University’s research portal, https://research.aalto.fi/en/publications/guiding-a-diffusion-model-with-a-bad-version-of-itself</li>
<li>New diffusion paper : Guiding a Diffusion Model with a Bad Version of Itself - Reddit, https://www.reddit.com/r/StableDiffusion/comments/1hebcri/new_diffusion_paper_guiding_a_diffusion_model/</li>
<li>Rada Mihalcea and Zhijing Jin win two Best Paper Awards at NeurIPS 2024, https://cse.engin.umich.edu/stories/rada-mihalcea-and-zhijing-jin-win-two-best-paper-awards-at-neurips-2024</li>
<li>Announcing the NeurIPS 2024 Test of Time Paper Awards, https://blog.neurips.cc/2024/11/27/announcing-the-neurips-2024-test-of-time-paper-awards/</li>
<li>Paper accepted at 2024 Conference on Robot Learning (CoRL) - School of Informatics, https://informatics.ed.ac.uk/slmc/news-and-events/paper-accepted-at-2024-conference-on-robot-learning-corl</li>
<li>Awards - CoRL 2024, https://2024.corl.org/program/awards</li>
<li>PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators, https://openreview.net/forum?id=KdVLK0Wo5z</li>
<li>PoliFormer: Scaling On-Policy RL with Transformers Results in …, https://proceedings.mlr.press/v270/zeng25a.html</li>
<li>[PDF] OpenVLA: An Open-Source Vision-Language-Action Model - Semantic Scholar, https://www.semanticscholar.org/paper/OpenVLA%3A-An-Open-Source-Vision-Language-Action-Kim-Pertsch/8f9ceb5ffad8e7a066dfc9d9aaa5153b714740ee</li>
<li>(PDF) OpenVLA: An Open-Source Vision-Language-Action Model - ResearchGate, https://www.researchgate.net/publication/381404911_OpenVLA_An_Open-Source_Vision-Language-Action_Model</li>
<li>CoRL 2024 Publication Round-Up - RAI Institute, https://rai-inst.com/resources/blog/corl-round-up/</li>
<li>CoRL 2024 Accepted Paper List - Paper Copilot, https://papercopilot.com/paper-list/corl-paper-list/corl-2024-paper-list/</li>
<li>PoliFormer: Transformer-Based On-Policy RL - Emergent Mind, https://www.emergentmind.com/papers/2406.20083</li>
<li>[Literature Review] PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators - Moonlight, https://www.themoonlight.io/en/review/poliformer-scaling-on-policy-rl-with-transformers-results-in-masterful-navigators</li>
<li>PoliFormer: Embodied Navigation, On-Policy RL, Transformer Policy, https://poliformer.allen.ai/</li>
<li>OpenVLA: An Open-Source Vision-Language-Action Model | alphaXiv, https://www.alphaxiv.org/overview/2406.09246v3</li>
<li>OpenVLA: An Open-Source Vision-Language-Action Model, https://proceedings.mlr.press/v270/kim25c.html</li>
<li>Robotics Dec 2024 - arXiv, https://www.arxiv.org/list/cs.RO/2024-12?skip=150&amp;show=50</li>
<li>Robotics Dec 2024 - arXiv, https://www.arxiv.org/list/cs.RO/2024-12?skip=25&amp;show=2000</li>
<li>Robotics Dec 2024 - arXiv, https://www.arxiv.org/list/cs.RO/2024-12?skip=350&amp;show=50</li>
<li>Artificial Intelligence 2024 - arXiv, https://arxiv.org/list/cs.AI/2024</li>
<li>DeepSeek-V3 Technical Report, https://arxiv.org/abs/2412.19437</li>
<li>[2412.06288] The Unpaid Toll: Quantifying the Public Health Impact of AI - arXiv, https://arxiv.org/abs/2412.06288</li>
<li>[2412.17376] How Green Can AI Be? A Study of Trends in Machine Learning Environmental Impacts - arXiv, https://arxiv.org/abs/2412.17376</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>