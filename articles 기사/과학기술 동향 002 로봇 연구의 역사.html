<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:위대한 로봇 연구</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>위대한 로봇 연구</h1>
                    <nav class="breadcrumbs"><a href="../index.html">Home</a> / <a href="index.html">기사 (Articles)</a> / <span>위대한 로봇 연구</span></nav>
                </div>
            </header>
            <article>
                <h1>위대한 로봇 연구</h1>
<h2>1.  현대 로봇공학의 창세기 (1960년대–1980년대) — 물리적 및 인지적 기반 구축</h2>
<p>이 섹션에서는 로봇공학의 근간이 되는 두 가지 핵심 기둥을 확립한다. 하나는 물리적인 작업을 수행하는 능력(조작)이고, 다른 하나는 세계를 인식하고, 추론하며, 행동하는 능력(AI 기반 이동성)이다. 이러한 개념이 실현 가능함을 증명하고 모든 후속 연구의 발판을 마련한 최초의 시스템들을 상세히 다룰 것이다.</p>
<h3>1.1  산업의 여명: 유니메이트(Unimate, 1961)와 자동화 제조의 탄생</h3>
<p><strong>핵심 기여:</strong> 1961년 제너럴 모터스(General Motors) 공장에 최초의 산업용 로봇인 유니메이트(Unimate)가 설치된 사건은 산업용 로봇 산업의 탄생을 알리는 신호탄이었다.1</p>
<p><strong>기술적 세부사항:</strong> 이 보고서는 조지 데볼(George Devol)이 1954년에 출원한 “프로그램된 물품 이송(Programmed Article Transfer)” 특허를 상세히 다룬다. 이 특허는 학습 가능하고 디지털로 제어되는 매니퓰레이터 팔의 개념을 도입했다.1 유니메이트는 유압 액추에이터, 프로그래밍된 단계를 저장하기 위한 자기 드럼 메모리를 특징으로 했으며, 초기 기능은 인간 작업자에게 위험한 작업인 뜨겁게 달궈진 다이캐스팅 금속 부품을 다루고 용접하는 것이었다.2 발명가 조지 데볼과 기업가 조셉 엥겔버거(Joseph Engelberger)의 협력은 세계 최초의 로봇 회사인 유니메이션(Unimation)의 설립으로 이어졌다.1</p>
<p><strong>영향력 분석:</strong> 유니메이트의 데뷔는 단순한 기술 시연을 넘어선 패러다임의 전환이었다. 이는 자동화된 기계가 반복적이고 위험한 노동을 신뢰성 있게 수행할 수 있음을 증명했으며, 제조업의 지형을 근본적으로 바꾸고 수십 년간 로봇공학의 경제적 동인을 확립했다. 또한, *투나잇 쇼(The Tonight Show)*에 출연한 것은 로봇공학의 개념을 대중에게 소개하는 초기적이면서도 중요한 순간이었다.2</p>
<h3>1.2  기계 속의 정신: 셰이키(Shakey the Robot, 1966-1972)와 인식, 계획, 행동의 통합</h3>
<p><strong>핵심 기여:</strong> 셰이키는 자신의 행동과 환경에 대해 추론하고 논리적 계획을 물리적 실행과 통합한 최초의 이동 로봇이었다.4 이는 스탠퍼드 연구소(SRI)에서 수행된 기념비적인 프로젝트로, 현대 자율 시스템의 기반을 마련했다.</p>
<p><strong>기술적 세부사항:</strong> 이 보고서는 셰이키의 아키텍처를 상세히 설명한다. 이는 TV 카메라, 촉각 센서, 그리고 대형 원격 컴퓨터(초기 SDS-940, 후기 PDP-10)와 무선으로 연결된 이동 플랫폼으로 구성되었다.7 이 프로젝트에서 탄생한 두 가지 중요한 소프트웨어적 기여는 다음과 같다.</p>
<ul>
<li>
<p><em><em>A</em> 탐색 알고리즘:</em>* 환경 내에서 효율적인 경로(“경유지”)를 계획하기 위해 개발된 정교한 경로 탐색 알고리즘으로, 오늘날 내비게이션 및 계획 수립에 여전히 핵심적인 기술이다.4</p>
</li>
<li>
<p><strong>STRIPS (Stanford Research Institute Problem Solver):</strong> “플랫폼에서 블록을 밀어내라“와 같은 고수준의 명령을 실행 가능한 일련의 행동으로 분해할 수 있는 계획 시스템으로, 자동화된 추론 분야에서 큰 발전을 이루었다.7</p>
</li>
</ul>
<p><strong>영향력 분석:</strong> 셰이키는 수십 년 동안 AI와 로봇공학을 지배했던 “인식-계획-행동(sense-plan-act)” 패러다임의 탄생을 상징한다. 이는 기계가 세계에 대한 내부 모델을 유지하고, 그 모델을 사용하여 목표에 대해 추론하며, 이를 달성하기 위한 계획을 실행할 수 있다는 아이디어를 최초로 물리적으로 구현한 것이었다. 이 프로젝트의 결과물, 특히 A*와 STRIPS는 AI 및 로봇공학 교육 과정의 기초 요소가 되었다.</p>
<h3>1.3  자율주행으로의 긴 여정: 스탠퍼드 카트(The Stanford Cart, 1979)와 최초의 비전 기반 내비게이션</h3>
<p><strong>핵심 기여:</strong> 1979년 스탠퍼드 카트가 오직 스테레오 비전만을 사용하여 의자로 가득 찬 방을 성공적으로 자율 주행한 것은 기념비적인 성과였으며, 오늘날 자율주행 자동차의 직계 조상으로 여겨진다.10</p>
<p><strong>기술적 세부사항:</strong> 이 보고서는 카트의 진화 과정을 추적한다. 초기에는 지구-달 간의 2.6초 신호 지연을 시뮬레이션하며 달 탐사 로버의 원격 조종을 연구하는 목적이었으나, 최종적으로는 자율 주행 차량으로 발전했다.10 1979년의 성공 과정은 당시의 엄청난 계산적 한계를 보여준다. 카트는 1미터를 이동한 후 10-15분 동안 멈춰 서서 스테레오 이미지를 캡처하고, 주변 환경의 3D 모델을 구축하며, 다음 움직임을 계획했다. 이 과정을 반복하여 20미터 길이의 방을 횡단하는 데 총 5시간이 걸렸다.10 이 성과는 그의 박사 학위 논문의 일부로서 핵심 연구원인 한스 모라벡(Hans Moravec)의 공으로 인정받는다.11</p>
<p><strong>영향력 분석:</strong> 스탠퍼드 카트의 느리지만 성공적인 여정은 비전 기반 자율 주행의 중요한 개념 증명(proof-of-concept)이었다. 이는 로봇이 바닥의 선이나 외부 비콘 없이도 복잡하고 비정형적인 환경을 주행할 수 있음을 보여주었다. 이 연구는 CMU의 내브랩(Navlab) 프로젝트와 수십 년 후의 DARPA 그랜드 챌린지를 포함한 후속 자율주행 차량 연구에 직접적인 영감을 주었다.11</p>
<p>초기 로봇공학의 역사는 근본적인 분리를 보여준다. 유니메이트는 산업 자동화에 초점을 맞춘 강력하고 정밀하지만 지능이 없는 조작 장치, 즉 “몸체“를 대표했다. 반면, 셰이키와 스탠퍼드 카트는 물리적으로는 덜 유능했지만 인식, 추론, 자율 행동이라는 AI의 과제를 구현한 “정신“을 상징했다. 유니메이트는 구조화된 환경에서 프로그래밍된 반복적인 물리적 작업을 수행했다.1 셰이키는 단순화되었지만 비구조적인 세계에서 추론하고, 계획하며, 주행했다.7 이들의 능력을 비교해 보면, 유니메이트는 힘과 신뢰성을 가졌지만 지능은 없었고, 셰이키는 초보적인 지능을 가졌지만 느리고 물리적으로 단순했다. 이 두 프로젝트는 로봇공학의 두 가지 핵심적이면서도 초기에 분리되었던 과제, 즉 산업용 조작(“힘”)과 인공지능(“두뇌”)을 정의했다. 보스턴 다이내믹스의 아틀라스와 같은 현대의 범용 로봇에 대한 탐구는 이 두 갈래의 시작점을 하나의 시스템으로 통합하려는 궁극적인 노력의 표현이다.14</p>
<p>이러한 초기 시스템들의 한계는 거의 전적으로 당시 사용 가능했던 컴퓨팅 하드웨어에 의해 결정되었다. 셰이키는 원격 메인프레임을 필요로 했고 8, 스탠퍼드 카트가 1미터 이동마다 15분의 처리 시간을 필요로 했던 것은 계산적 제약의 직접적인 결과였다.10 이는 중요한 인과 관계를 확립한다. 즉, 컴퓨팅 파워의 기하급수적인 성장(무어의 법칙)은 로봇 자율성 발전의 주요 동력이었다. 셰이키와 카트가 사용했던 크고, 원격이며, 느린 컴퓨터를 7 스탠리(차량 내 6개의 펜티엄 M 블레이드 컴퓨터)나 15 보스(10개의 코어2듀오 프로세서)와 같은 현대 시스템과 비교하면 16, 후자는 훨씬 더 복잡한 인식과 계획을 실시간으로 수행한다. 로봇 지능의 역사는 컴퓨터 하드웨어의 역사와 분리될 수 없다. 로봇공학의 혁신은 단지 알고리즘적인 것만이 아니라, 이전에는 다루기 어려웠던 알고리즘을 실용적으로 만드는 충분한 계산 능력이 등장함으로써 가능해지는 경우가 많다.</p>
<h2>2.  알고리즘의 기반 (1980년대–2000년대) — 이론적 도구의 구축</h2>
<p>이 섹션에서는 선구적인 시스템에서 벗어나, 현대 로봇공학을 위한 이론적 언어와 도구를 창조한 중요한 학술 논문들로 초점을 옮긴다. 이러한 “발표“들은 동작 계획 및 위치 측정과 같은 근본적인 문제에 대해 확장 가능하고 견고한 해결책을 가능하게 한 수학적, 개념적 프레임워크를 제공했다.</p>
<h3>2.1  활동 무대의 정의: “공간 계획: 형상 공간 접근법 (Spatial Planning: A Configuration Space Approach)” (로자노-페레즈, 1983)</h3>
<p><strong>핵심 기여:</strong> 이 논문은 형상 공간(Configuration Space, C-space)이라는 패러다임을 전환하는 개념을 도입했다.17 이는 다관절 로봇의 동작을 계획하는 복잡한 문제를 고차원 공간에서 단일 점의 경로를 찾는 더 간단한 문제로 변환시켰다.</p>
<p><strong>기술적 세부사항:</strong> 이 보고서는 C-space의 작동 원리를 설명한다. 다수의 자유도(DOF)를 가진 로봇은 각 차원이 하나의 자유도에 해당하는 공간에서 단일 점으로 표현된다. 물리적 세계의 장애물은 이 새로운 공간에서 “C-장애물“로 매핑된다. 따라서 동작 계획 문제는 시작 형상에서 목표 형상까지 충돌 없는 경로를 찾는 문제로 축소된다.17 이 추상화는 혁명적이었다.</p>
<p><strong>영향력 분석:</strong> C-space 공식화는 로봇공학 역사상 가장 중요한 지적 기여 중 하나로 평가받는다. 이는 모든 로봇에 적용 가능한 동작 계획을 위한 통일된 수학적 프레임워크를 제공했으며, PRM과 RRT를 포함한 거의 모든 현대 계획 알고리즘의 기반으로 남아 있다.17</p>
<h3>2.2  동적 보행의 정복: “균형 잡는 다리 로봇 (Legged Robots That Balance)” (레이버트, 1986)</h3>
<p><strong>핵심 기여:</strong> 마크 레이버트(Marc Raibert)의 MIT 레그 랩(Leg Lab)에서 나온 이 책과 관련 연구는 다리 로봇의 동적 균형 및 보행에 대한 과학적 기반을 확립했다.19 이는 느리고 정적인 안정성에서 능동적이고 동적인 제어로 초점을 전환시켰다.</p>
<p><strong>기술적 세부사항:</strong> 레이버트의 핵심 제어 철학은 달리기의 복잡한 문제를 (1) 뛰는 높이 제어, (2) 전진 속도 제어, (3) 몸체 자세 제어라는 세 가지 더 간단하고 분리된 부분으로 분해하는 것이었다.19 이 책은 분석, 시뮬레이션, 그리고 실제 한 발로 뛰는 기계를 통해 이 단순화된 제어 모델이 놀라울 정도로 안정적이고 견고한 동적 보행을 만들어낼 수 있음을 보여주었다.21</p>
<p><strong>영향력 분석:</strong> 레이버트의 연구는 모든 현대 동적 다리 로봇의 기초가 되었다. 이는 복잡한 동물과 같은 움직임이 비교적 간단한 제어 원리로 달성될 수 있음을 증명했다. 이 연구는 보스턴 다이내믹스의 설립과 빅독(BigDog), 아틀라스(Atlas), 스팟(Spot)과 같은 상징적인 로봇 개발의 직접적인 전조가 되었다.20 레이버트의 로봇 중 두 대가 로봇 명예의 전당에 헌액된 것은 이 연구의 영향력을 증명한다.25</p>
<h3>2.3  가능성의 탐색: 샘플링 기반 동작 계획의 부상</h3>
<h4>2.3.1 a. “고차원 형상 공간에서의 경로 계획을 위한 확률적 로드맵 (Probabilistic Roadmaps for Path Planning in High-Dimensional Configuration Spaces)” (카브라키, 라톰, 외, 1996)</h4>
<p><strong>핵심 기여:</strong> 완전한 방법이 실패하는 고차원 C-space에서 동작 계획 문제를 해결하기 위한 실용적이고 효율적인 방법인 확률적 로드맵(Probabilistic Roadmap, PRM) 알고리즘을 도입했다.17</p>
<p><strong>기술적 세부사항:</strong> 이 보고서는 PRM의 두 단계 접근법을 설명한다. 첫 번째는 무작위로 충돌 없는 형상을 샘플링하고 간단한 지역 플래너를 사용하여 이를 연결하여 그래프(“로드맵”)를 구축하는 “학습” 단계이다. 두 번째는 시작 및 목표 형상을 로드맵에 연결한 다음 그래프를 탐색하여 경로를 찾는 “쿼리” 단계이다.26 이 접근법은 완전성을 확률적 완전성 및 실용적 효율성과 맞바꾸었다.</p>
<p><strong>영향력 분석:</strong> PRM은 많은 자유도를 가진 로봇의 동작 계획을 실용적으로 만든 혁신이었다. RRT와 함께, 이는 복잡한 문제에 대한 샘플링 기반 계획을 지배적인 패러다임으로 확립했다.17</p>
<h4>2.3.2 b. “RRT-Connect: 단일 쿼리 경로 계획을 위한 효율적인 접근법 (RRT-Connect: An Efficient Approach to Single-Query Path Planning)” (쿠프너 &amp; 라발, 2000)</h4>
<p><strong>핵심 기여:</strong> 단일 쿼리 계획 문제에 매우 효과적이고 널리 사용되는 알고리즘이 된 급속 탐색 랜덤 트리(Rapidly-exploring Random Tree, RRT), 특히 RRT-Connect 변형을 도입했다.17</p>
<p><strong>기술적 세부사항:</strong> PRM의 사전 계산 단계와 달리, RRT는 시작 형상에서부터 점진적으로 경로 트리를 구축하며, C-space의 미탐사 영역을 탐색하도록 편향된다. RRT-Connect는 시작과 목표에서 동시에 두 개의 트리를 성장시키고 이들을 연결하려고 시도함으로써 효율성을 향상시킨다.17</p>
<p><strong>영향력 분석:</strong> RRT는 온라인 및 단일 쿼리 시나리오에서 최적은 아니더라도 실행 가능한 경로를 신속하게 찾는 단순성과 효과성으로 인해 선호되는 알고리즘이 되었다.</p>
<h3>2.4  미지의 세계 탐험: SLAM의 부상</h3>
<p><strong>핵심 기여:</strong> 동시적 위치 추정 및 지도 작성(Simultaneous Localization and Mapping, SLAM)은 로봇이 미지의 환경 지도를 구축하면서 동시에 그 지도 내에서 자신의 위치를 추적하는 근본적인 문제이다.</p>
<h4>2.4.1 a. “MonoSLAM: 실시간 단일 카메라 SLAM (MonoSLAM: Real-Time Single Camera SLAM)” (데이비슨 외, 2007)</h4>
<p><strong>기술적 세부사항:</strong> 이 보고서는 MonoSLAM을 단일 카메라만을 사용하는 최초의 실시간 SLAM 알고리즘으로 설명한다.29 이는 확장 칼만 필터(EKF)를 사용하여 환경 내 특징점의 3D 위치와 카메라의 6자유도 자세를 실시간으로 공동 추정했다.</p>
<p><strong>영향력 분석:</strong> MonoSLAM은 단일 카메라라는 최소한의 센서 구성으로 SLAM이 가능하다는 것을 증명한 기념비적인 성과였으며, 휴대폰, 드론, AR/VR에서의 응용 가능성을 열었다.</p>
<h4>2.4.2 b. “ORB-SLAM: 다재다능하고 정확한 단안 SLAM 시스템 (ORB-SLAM: A Versatile and Accurate Monocular SLAM System)” (무르-아르탈 외, 2015)</h4>
<p><strong>기술적 세부사항:</strong> ORB-SLAM이 이전 연구(예: PTAM의 추적과 매핑 분리)를 기반으로 하여 다양한 환경에서 작동하는 견고한 특징점 기반 시스템을 어떻게 만들었는지 설명한다.30 이 시스템은 추적, 매핑, 재지역화, 루프 폐쇄 등 모든 SLAM 작업에 빠르고 견고한 동일한 ORB 특징점을 독특하게 사용했다. 키프레임과 맵 포인트를 위한 “적자생존” 전략은 간결하고 효율적인 맵을 생성했다.31</p>
<p><strong>영향력 분석:</strong> ORB-SLAM(및 그 후속작인 ORB-SLAM2/3)은 가장 영향력 있고 널리 사용되는 오픈 소스 SLAM 시스템 중 하나가 되었으며, 수많은 연구 프로젝트와 상용 제품의 벤치마크 및 기반 역할을 했다.34 공개적으로 배포된 것은 커뮤니티에 대한 주요 기여였다.</p>
<h3>2.5  최적성의 추구: “최적 동작 계획을 위한 샘플링 기반 알고리즘 (Sampling-based Algorithms for Optimal Motion Planning)” (카라만 &amp; 프라촐리, 2011) - RSS 2022 시간의 시험상</h3>
<p><strong>핵심 기여:</strong> 이 논문은 샘플링 기반 계획 분야를 근본적으로 바꾸었다. 이는 RRT와 같은 표준 알고리즘이 점근적으로 최적이 아님(즉, 무한한 시간이 주어져도 최상의 경로를 찾지 못함)을 증명하고, 증명 가능하게 최적인 새로운 알고리즘 RRT<em>와 PRM</em>를 도입했다.17</p>
<p><strong>기술적 세부사항:</strong> 이 보고서는 RRT*의 핵심 아이디어를 설명한다. 트리를 성장시키는 것 외에도, 알고리즘은 “재배선” 단계를 포함한다. 새로운 노드가 추가될 때, 이 노드가 기존 노드들로 가는 더 짧은 경로를 제공할 수 있는지 주변을 확인하고, 그 반대도 마찬가지로 점진적으로 트리 내 경로의 품질을 향상시킨다.38</p>
<p><strong>영향력 분석:</strong> 이 논문은 분수령이 된 순간이었다. 이는 샘플링 기반 방법의 효율성과 점근적 최적성 보장을 동시에 가질 수 있다는 아이디어를 도입했다. RRT*와 그 변형들은 현대 동작 계획의 초석이 되었으며, 특히 경로 품질(예: 길이, 부드러움, 에너지)이 중요한 응용 분야에서 그렇다. RSS 시간의 시험상 수상은 그 깊고 지속적인 영향을 공식적으로 인정한 것이다.41</p>
<p>이 시대의 주요 알고리즘적 혁신은 더 복잡한 시스템을 구축하는 것이 아니라, 올바른 추상화(C-space)나 원칙에 입각한 단순화(레이버트의 3부분 제어, PRM/RRT의 확률적 완화)를 찾는 데 있었다. 로자노-페레즈의 C-space는 복잡성을 추가하는 대신 추상화하여 기하학적 문제를 경로 탐색 문제로 전환했다.17 레이버트의 제어 방식은 다리 로봇의 전체적이고 복잡한 동역학을 무시하고 간단하고 효과적인 분리된 모델에 집중했다.19 PRM/RRT는 경로가 존재할 경우 반드시 찾는다는 보장(완전성)을 포기하고, 훨씬 더 실용적인 보장인 결국에는 찾을 것이라는 보장(확률적 완전성)을 택했다.17 이러한 영향력 있는 연구들은 로봇공학 연구에서 중요한 교훈을 가르쳐준다. 복잡한 문제를 해결하는 열쇠는 종종 더 많은 무차별 대입을 적용하는 것보다 문제에 대한 더 우아하고 단순화된 표현을 찾는 데 있다는 것이다.</p>
<p>또한, 이론과 실제 사이에는 명확한 피드백 루프가 존재한다. 고차원 자유도를 가진 C-space에서 완전한 플래너의 계산적 비실용성이 PRM과 RRT의 개발을 촉발했다. RRT가 최적이 아닌 울퉁불퉁한 경로를 생성한다는 관찰은 카라만과 프라촐리가 그 이론적 속성을 조사하고 최적의 RRT<em>를 개발하게 만들었다. 이는 실제적 필요가 이론적 발전을 동기 부여하고, 이는 다시 더 나은 실제 시스템을 가능하게 하는 건강하고 생산적인 순환을 보여준다. 예를 들어, 10자유도 팔의 동작 계획이 기존 알고리즘으로는 너무 느리다는 문제가 제기되자, 카브라키 등은 실용적인 샘플링 기반 해결책인 PRM을 제안했다.26 그러나 PRM/RRT가 경로를 빨리 찾지만 종종 들쭉날쭉하고 비효율적이라는 새로운 문제가 발생했다. 이에 대한 해결책으로 카라만과 프라촐리는 이 비최적성을 분석하고 최적 경로로의 수렴을 보장하는 RRT</em>를 제안했다.39 이러한 발전 과정은 이 분야가 직선적으로 발전하는 것이 아니라, 반응적인 루프 속에서 진보함을 보여준다. 각 주요 알고리즘 “발표“는 종종 이전 세대의 인지된 단점에 대한 직접적인 응답이다.</p>
<h2>3.  학습 혁명과 현대의 탁월함 (2010년대–현재)</h2>
<p>이 섹션에서는 현대 로봇공학에서 가장 중요한 패러다임 전환, 즉 딥러닝의 발전에 힘입어 데이터 기반 및 학습 기반 접근 방식으로의 전환을 상세히 다룬다. 이러한 아이디어를 로봇공학에 도입한 기초 논문들을 검토하고, 현재 기술 수준의 지표로서 최신 수상 논문들을 살펴볼 것이다.</p>
<h3>3.1  픽셀로부터 행동 학습하기: “심층 강화 학습으로 아타리 게임하기 (Playing Atari with Deep Reinforcement Learning)” (므니 외, 2013)</h3>
<p><strong>핵심 기여:</strong> 이 딥마인드(DeepMind) 논문은 엄밀히 말해 로봇공학 논문은 아니지만, 지대한 영향을 미쳤다. 이는 심층 강화 학습(RL)을 사용하여 고차원 감각 입력(원시 픽셀)으로부터 직접 복잡한 제어 정책을 성공적으로 학습한 최초의 알고리즘인 심층 Q-네트워크(Deep Q-Network, DQN)를 제시했다.42</p>
<p><strong>기술적 세부사항:</strong> 이 보고서는 DQN의 핵심 구성 요소를 설명한다. 즉, 최적의 행동-가치 함수(Q-함수)를 근사하기 위해 심층 컨볼루션 신경망(CNN)을 사용하고, 훈련 데이터의 시간적 상관관계를 깨뜨려 학습 과정을 안정화시키기 위해 “경험 재현(experience replay)” 버퍼를 사용하는 것이다.42 이 시스템은 게임 규칙에 대한 사전 지식 없이 오직 픽셀 데이터와 점수만으로 아타리 게임을 초인적인 수준으로 플레이하는 법을 학습했다.</p>
<p><strong>영향력 분석:</strong> DQN은 제어 문제에서 “종단간(end-to-end)” 학습을 위한 강력한 청사진을 제공했다. 이는 단일 범용 알고리즘이 수작업으로 설계된 특징 없이도 다양한 작업에 걸쳐 효과적인 전략을 학습할 수 있음을 보여주었다. 이 철학은 조작, 보행, 내비게이션과 같은 로봇공학 작업에 널리 채택되어, 로봇공학에서 심층 강화 학습의 현대 시대를 열었다.46</p>
<h3>3.2  파지법 학습하기: “로봇 파지를 감지하기 위한 심층 학습 (Deep Learning for Detecting Robotic Grasps)” (렌츠, 리, 삭세나, 2013) - RSS 2023 시간의 시험상</h3>
<p><strong>핵심 기여:</strong> 이 논문은 심층 학습을 핵심 로봇공학 문제, 즉 RGB-D 이미지에서 로봇 그리퍼의 안정적인 파지 지점을 식별하는 문제에 적용한 최초이자 가장 영향력 있는 논문 중 하나였다.51</p>
<p><strong>기술적 세부사항:</strong> 이 논문의 방법은 두 개의 심층 네트워크를 사용하는 2단계 계단식 탐지 시스템을 포함한다. 더 빠르고 작은 네트워크가 먼저 방대한 수의 가능한 파지 후보를 추려내고, 더 느리지만 정확한 네트워크가 가장 유망한 후보들을 재평가한다.54 이로 인해 접근 방식이 계산적으로 실현 가능해졌다. 또한 다중 모드 데이터(RGB + 깊이)를 처리하기 위한 새로운 방법을 도입했다.53</p>
<p><strong>영향력 분석:</strong> 이 연구는 심층 학습이 파지 감지를 위해 수작업으로 설계된 특징에 의존했던 이전의 최첨단 방법들을 능가할 수 있음을 보여주었다. 이는 로봇 파지 및 조작을 위해 데이터 기반 방법을 사용하는 현재의 지배적인 접근 방식의 토대를 마련했다. 2023년 RSS 시간의 시험상 수상은 이 분야에 대한 지속적이고 중요한 영향을 확인시켜 준다.41 코드와 데이터셋의 공개는 그 영향력을 더욱 증폭시켰다.56</p>
<h3>3.3  인식을 행동으로 변환하기: “이미지를 지도로 변환하기 (Translating Images into Maps)” (사하 외, 2022) - ICRA 2022 우수 논문</h3>
<p><strong>핵심 기여:</strong> 이 논문은 전방 카메라 이미지를 자율 주행을 위한 하향식 조감도(BEV) 지도로 직접 변환하는 새롭고 매우 효과적인 순간 매핑 방법을 도입했다.57</p>
<p><strong>기술적 세부사항:</strong> 핵심 아이디어는 이 변환을 번역 문제로 취급하는 것이다. 이 논문은 이미지의 수직 “스캔라인“을 BEV 지도의 해당 극좌표 “광선“으로 변환하는 새로운 트랜스포머 네트워크 아키텍처를 사용한다.57 이 물리적으로 기반을 둔 공식화는 네트워크가 복잡한 투영 기하학을 암묵적으로 효율적으로 학습할 수 있게 한다.</p>
<p><strong>영향력 분석:</strong> 이 연구는 자율 주행에 필수적인 기능인 카메라 전용 BEV 인식 분야의 최첨단을 대표한다. nuScenes 및 Argoverse와 같은 주요 데이터셋에서 상당한 성능 향상을 달성함으로써 57, 강력한 기하학적 사전 지식을 가진 로봇공학 문제에 현대 딥러닝 아키텍처(트랜스포머)를 적용했을 때의 힘을 보여준다. ICRA 우수 논문으로 인정받은 것은 인식 커뮤니티에 즉각적인 영향을 미쳤음을 강조한다.60</p>
<h3>3.4  현대적 영향력의 연대기: 최근 수상 논문 분석</h3>
<p><strong>핵심 기여:</strong> 이 하위 섹션에서는 최첨단 연구 및 새로운 동향의 현황을 제공하기 위해 ICRA 60 및 IROS 64에서 최근 “최우수 논문” 상을 수상한 논문들을 선별하여 분석한다.</p>
<p><strong>분석 예시:</strong></p>
<ul>
<li>
<p><strong>IROS 2022 최우수 논문: “SpeedFolding: 의류의 효율적인 양손 접기 학습 (SpeedFolding: Learning Efficient Bimanual Folding of Garments)” (아비갈 외)</strong>.67 이 논문은 변형 가능한 물체 조작이라는 악명 높은 어려운 문제를 다룬다. 그 기여는 일련의 행동 원시 함수에 대한 그리퍼 자세를 예측하는 새로운 신경망을 사용하여 이전 연구보다 5-10배 빠르게 옷을 접는 법을 배우는 양손 시스템이다.69 이는 복잡하고 장기적인 조작 작업을 학습하는 경향을 보여준다.</p>
</li>
<li>
<p><strong>IROS 2023 최우수 논문: “인식-인지 MPC를 통한 드론의 자율 전력선 검사 (Autonomous Power Line Inspection with Drones via Perception-Aware MPC)” (싱 외)</strong>.65 이 연구는 인식과 제어의 긴밀한 결합을 보여준다. 이는 장애물을 피하면서 전력선의 <em>가시성을 극대화</em>하도록 명시적으로 행동을 계획하는 모델 예측 제어기(MPC)를 사용하며, 이는 “능동적 인식“의 핵심 원칙이다.71 이는 수동적 인식에서 능동적이고 목표 지향적인 정보 수집으로의 전환을 나타낸다.</p>
</li>
<li>
<p><strong>ICRA 2025 최우수 학생 논문: “1만 대 로봇 배치: 평생 다중 에이전트 경로 탐색을 위한 확장 가능한 모방 학습 (Deploying Ten Thousand Robots: Scalable Imitation Learning for Lifelong Multi-Agent Path Finding)” (장 외)</strong>.63 이 논문은 다중 로봇 시스템의 확장성이라는 중요한 과제를 다루며, 모방 학습을 사용하여 대규모 로봇 무리의 경로 탐색을 처리한다. 이는 많은 고전적인 방법으로는 다루기 힘든 문제이다.</p>
</li>
</ul>
<p><strong>영향력 분석:</strong> 이러한 수상 논문들을 검토함으로써 복잡한 조작을 위한 학습, 긴밀한 인식-행동 루프, 확장 가능한 다중 로봇 시스템과 같은 현재의 연구 최전선을 식별할 수 있다. 이 상들은 로봇공학 커뮤니티가 현재 가장 영향력 있고 혁신적이라고 여기는 것에 대한 동료 심사를 거친 신호 역할을 한다.</p>
<p>학습 혁명은 원시 센서 데이터(픽셀, 라이다 포인트)에서 행동(모터 토크, 조향각)으로 직접 학습하는 “종단간” 시스템으로의 철학적 전환으로 특징지어진다. DQN에 의해 정신적으로 개척된 이 접근 방식은 인식, 모델링, 계획, 제어를 위한 별도의 수작업 모듈로 구성된 전통적인 파이프라인을 우회한다. 이 전환은 단일의 크고 최적화된 모델이 인간이 설계한 모듈식 시스템보다 더 나은 표현과 정책을 발견할 수 있다는 가설에 의해 주도된다. DQN 논문의 핵심 아이디어는 픽셀에서 행동으로의 학습이었고 42, 파지 감지 논문은 수작업 특징을 피하고 RGB-D 이미지에서 파지 사각형으로 바로 학습했으며 52, “이미지를 지도로 변환하기” 논문은 단일 네트워크에서 이미지로부터 BEV 지도를 생성했다.57 이 세 가지 모두 다단계의 인간 설계 파이프라인을 학습된 함수로 대체했다. 이러한 경향은 명시적인 3D 모델이나 의미론적 장면 그래프와 같은 중간 표현이 최적이 아닐 수 있으며, 심층 네트워크가 내부적으로 더 효과적인 잠재 표현을 학습할 수 있다는 믿음을 시사한다.</p>
<p>또한, ORB-SLAM이나 렌츠 등의 파지 감지 논문과 같은 연구의 영향력은 저자들이 코드와 데이터셋을 공개함으로써 대폭 증폭되었다.31 이는 로봇공학 연구에서 중요한 문화적 변화를 나타낸다. 오픈소싱은 직접적인 복제, 벤치마킹, 그리고 이전 연구를 기반으로 한 구축을 가능하게 하여 전체 분야의 혁신 속도를 극적으로 가속화한다. 논문의 영향은 더 이상 그 아이디어뿐만 아니라, 커뮤니티에 제공하는 도구에 의해서도 결정된다. ORB-SLAM 초록에서 소스 코드를 공개한다는 명시적인 언급(“커뮤니티의 이익을 위해, 우리는 소스 코드를 공개합니다”)과 31 심층 파지 논문의 공개 GitHub 저장소의 존재는 56 이를 잘 보여준다. 알고리즘이 설명되었지만 쉽게 접근할 수 없었던 이전 시대와는 대조적이다. 21세기 현대 로봇공학에서 오픈 소스 소프트웨어의 부상은 지식 전달과 영향력의 주요 메커니즘이 되었다. 논문의 인용 횟수는 이제 영향력의 척도로서 GitHub의 스타와 포크 수에 의해 보완된다.</p>
<h4>3.4.1 표 1: RSS 시간의 시험상 수상작 — 지속적인 영향력의 공식적 척도</h4>
<table><thead><tr><th>연도</th><th>논문 제목</th><th>저자</th><th>지속적인 기여 요약 (초록에서 종합)</th><th>출처 ID</th></tr></thead><tbody>
<tr><td>2023</td><td>Deep Learning for Detecting Robotic Grasps</td><td>Lenz, Lee, Saxena</td><td>RGB-D 데이터로부터 로봇 파지를 감지하기 위해 딥러닝을 사용하는 것을 개척하여 현대 데이터 기반 조작의 기초를 확립했다.</td><td>41</td></tr>
<tr><td>2022</td><td>Incremental Sampling-based Algorithms for Optimal Motion Planning</td><td>Karaman, Frazzoli</td><td>RRT가 최적이 아님을 증명하고, 샘플링 기반 동작 계획을 혁신한 증명 가능한 점근적 최적 알고리즘 RRT*를 도입했다.</td><td>17</td></tr>
<tr><td>2021</td><td>SARSOP: Efficient Point-Based POMDP Planning…</td><td>Kurniawati, Hsu, Lee</td><td>불확실성 하에서의 계획(POMDP)을 위한 매우 효율적인 알고리즘을 개발하여 더 넓은 범위의 로봇 작업에 실용적으로 적용할 수 있게 만들었다.</td><td>41</td></tr>
<tr><td>2020</td><td>Square Root SAM</td><td>Dellaert, Kaess</td><td>제곱근 인수분해를 사용한 평활화 및 매핑(SAM)을 EKF 기반 SLAM보다 빠르고, 더 정확하며, 더 견고한 대안으로 제시했다.</td><td>41</td></tr>
</tbody></table>
<h3>3.5  랜드마크 시스템과 그랜드 챌린지 (2000년대–현재) — 진보와 대중 인식의 촉매제</h3>
<p>이 섹션에서는 로봇공학 연구의 물리적 발현에 초점을 맞춘다. 즉, 기술적 한계를 뛰어넘었을 뿐만 아니라, 이 분야에 대한 대중의 이해와 투자를 형성한 획기적인 시스템과 세간의 이목을 끈 대회들을 다룬다.</p>
<h3>3.6  산업을 일으킨 사막 경주: DARPA 그랜드 챌린지 (2004-2007)</h3>
<p><strong>핵심 기여:</strong> 자율주행 차량의 개발을 촉진하고 현대 자율주행차 산업을 직접적으로 이끈 일련의 상금 기반 대회였다.74</p>
<p><strong>기술적 세부사항:</strong></p>
<ul>
<li>
<p><strong>2004 그랜드 챌린지:</strong> 142마일의 사막 경주로, 어떤 차량도 완주하지 못했으며, 최고 성적을 낸 차량(CMU의 샌드스톰)은 단 7.3마일만 주행했다. 이 “실패“는 문제의 엄청난 어려움을 부각시키는 데 결정적이었다.75</p>
</li>
<li>
<p><strong>2005 그랜드 챌린지:</strong> 불과 18개월 후, 다섯 대의 차량이 132마일 코스를 완주했다. 우승자인 **스탠퍼드의 “스탠리”**는 폭스바겐 투아렉 기반의 하드웨어(지붕에 장착된 SICK 레이저 및 컬러 카메라 등)와 적응적으로 지형을 분류하기 위해 머신러닝에 크게 의존한 소프트웨어 아키텍처를 갖추고 있었다.15 세바스찬 스런(Sebastian Thrun)의 리더십과 비전이 주목받았다.80</p>
</li>
<li>
<p><strong>2007 어반 챌린지:</strong> 이 대회는 문제를 모의 도시 환경으로 옮겨, 로봇이 교통 법규를 준수하고, 합류하며, 다른 차량과 상호작용하도록 요구했다. 우승자인 **CMU의 “보스”**는 쉐보레 타호 기반의 하드웨어(벨로다인 라이다를 포함한 360도 센서 스위트)와 정교한 3계층 계획 아키텍처(임무, 행동, 동작 계획)를 갖추고 있었다.16</p>
</li>
</ul>
<p><strong>영향력 분석:</strong> 그랜드 챌린지는 현대사에서 기술 개발을 장려한 가장 성공적인 사례 중 하나일 것이다. 이 대회는 구글, 우버 및 주요 자동차 회사의 자율주행 프로그램을 이끌게 될 세대의 전문가들을 배출했다. 이 행사는 자율주행의 실현 가능성을 증명하고, 이를 틈새 학문 연구에서 주요 산업적 노력으로 전환시켰다.</p>
<h4>3.6.1 표 2: DARPA 그랜드 챌린지의 주요 이정표</h4>
<table><thead><tr><th>챌린지</th><th>날짜</th><th>장소</th><th>목표</th><th>우승팀 및 차량</th><th>핵심 기술 도약</th><th>출처 ID</th></tr></thead><tbody>
<tr><td>그랜드 챌린지</td><td>2004년 3월</td><td>모하비 사막</td><td>142마일 오프로드 코스</td><td>없음 (CMU의 샌드스톰이 가장 멀리 감)</td><td>극도의 어려움 증명; 기준선 설정.</td><td>75</td></tr>
<tr><td>그랜드 챌린지</td><td>2005년 10월</td><td>모하비 사막</td><td>132마일 오프로드 코스</td><td>스탠퍼드 레이싱 팀 (“스탠리”)</td><td>최초의 장거리 자율주행 성공; 머신러닝 기반 인식.</td><td>74</td></tr>
<tr><td>어반 챌린지</td><td>2007년 11월</td><td>캘리포니아 빅터빌</td><td>교통이 있는 60마일 도시 코스</td><td>타르탄 레이싱 (“보스”)</td><td>교통 속에서 교통 법규를 준수하며 자율 주행.</td><td>16</td></tr>
</tbody></table>
<h3>3.7  휴머노이드 대사: 혼다 아시모(ASIMO)의 공개 데뷔와 진화 (2000)</h3>
<p><strong>핵심 기여:</strong> 2000년 아시모의 공개 데뷔는 휴머노이드 로봇공학에서 양자 도약을 의미했으며, 전례 없는 동적 보행, 달리기, 그리고 인간과의 상호작용 능력을 선보이며 전 세계의 상상력을 사로잡았다.86</p>
<p><strong>기술적 세부사항:</strong> 이 보고서는 혼다의 초기 E-시리즈 및 P-시리즈 로봇에서부터 아시모의 계보를 추적한다.86 2000년 버전의 핵심 기술인 “i-WALK” 지능형 보행 기술은 부드러운 회전과 안정성을 가능하게 했으며, 이족 보행 로봇의 주요 과제인 계단 오르기 능력도 갖추고 있었다.86 2002년 뉴욕 증권 거래소에서의 미국 데뷔는 주요 홍보의 순간으로 언급될 것이다.86</p>
<p><strong>영향력 분석:</strong> 아시모는 기술 시연 이상의 문화적 아이콘이었다. 10년 이상 동안, 이는 첨단 로봇공학의 대중적 얼굴이었으며, 한 세대의 학생들과 연구자들에게 영감을 주었다. 상용 제품은 아니었지만, 연구 플랫폼이자 로봇공학의 “친선 대사“로서의 역할은 휴머노이드 로봇의 잠재력을 보여주는 데 엄청난 영향을 미쳤다.90</p>
<h3>3.8  수술 로봇공학의 여명: 다빈치 수술 시스템의 상용 출시 (2000)</h3>
<p><strong>핵심 기여:</strong> 2000년 다빈치 수술 시스템의 FDA 승인 및 상용 출시는 로봇 보조 최소 침습 수술 분야를 창출했으며, 전 세계 수술실을 변화시켰다.92</p>
<p><strong>기술적 세부사항:</strong> 이 보고서는 시스템의 아키텍처를 설명한다. 3D 고화질 비전과 마스터 컨트롤이 있는 외과의사 콘솔, 그리고 환자 몸 안에서 작은 기구들의 더 작고 정밀한 움직임으로 외과의사의 손 움직임을 변환하는 여러 로봇 팔이 있는 환자 측 카트로 구성된다.93 손 떨림을 걸러내고 향상된 손재주(기구 손목의 7자유도)를 제공하는 시스템의 능력이 강조될 것이다.95 원격 전장 수술을 위한 SRI 및 DARPA 연구에서 기술의 기원을 찾을 수 있다.95</p>
<p><strong>영향력 분석:</strong> 다빈치 시스템은 지금까지 만들어진 가장 상업적으로 성공하고 영향력 있는 로봇 시스템 중 하나이다. 이는 전립선 절제술 및 자궁 절제술과 같은 복잡한 수술에 대해 환자의 흉터를 줄이고 회복 시간을 단축시키는 수백만 건의 최소 침습 시술을 가능하게 했다.93 이는 수십억 달러 규모의 산업을 창출했으며 수술 로봇공학 분야에서 지배적인 플랫폼으로 남아 있다.</p>
<h3>3.9  바이럴이 된 로봇들: 보스턴 다이내믹스 판테온과 그들의 문화적 및 연구적 영향</h3>
<p><strong>핵심 기여:</strong> 마크 레이버트의 리더십 하에 보스턴 다이내믹스는 지금까지 만들어진 가장 동적으로 유능한 로봇 시리즈를 제작했으며, 이들의 바이럴 비디오는 대중의 인식과 로봇공학 연구의 방향 모두에 깊은 영향을 미쳤다.20</p>
<p><strong>기술적 세부사항 및 데뷔 비디오:</strong></p>
<ul>
<li>
<p><strong>빅독 (BigDog, 데뷔 비디오 2008년 3월):</strong> 이 사족 보행 짐꾼 로봇이 거친 지형을 횡단하고, 얼음 위에서 미끄러지지만 회복하며, 강력한 발차기를 견뎌내는 비디오는 전례 없는 수준의 동적 안정성과 견고성을 보여주었다.98 이는 레이버트의 균형 원리를 직접 적용한 것이었다.</p>
</li>
<li>
<p><strong>아틀라스 (Atlas, 2013년 7월 11일 공개):</strong> 최초의 아틀라스는 DARPA 로보틱스 챌린지를 위해 개발된 유압식 휴머노이드였다.14 이후 버전이 파쿠르, 백플립, 춤을 추는 비디오는 이전에는 공상 과학으로만 여겨졌던 인간 수준의 민첩성을 보여주었다.14</p>
</li>
<li>
<p><strong>스팟 (Spot, 이전 스팟미니, 데뷔 비디오 2016년 6월):</strong> 더 작고, 조용하며, 완전 전기식인 스팟은 더 실용적이고 다재다능한 플랫폼으로 소개되었다.102 2020년 상용 출시는 세계에서 가장 진보된 동적 사족 보행 로봇을 산업 검사 및 연구용으로 사용할 수 있게 만든 주요 이정표였다.106 스팟미니를 선보인 마크 레이버트의 2017년 TED 강연은 주요 공개 발표였다.20</p>
</li>
</ul>
<p><strong>영향력 분석:</strong> 보스턴 다이내믹스 비디오는 진정한 의미의 “발표“이다. 이는 동적 보행 및 제어 분야의 최첨단 기술을 강력하고 부인할 수 없는 시연으로 보여준다. 이들은 수많은 연구자들에게 영감을 주었고, 가능한 것의 새로운 기준을 설정했으며, 이 분야가 동물의 우아함과 견고함으로 세계를 탐색할 수 있는 로봇을 만드는 과제에 직면하게 만들었다.</p>
<h3>3.10  하늘의 민주화: DJI 팬텀 1 (2013년 1월 7일)</h3>
<p><strong>핵심 기여:</strong> DJI 팬텀 1의 출시는 항공 로봇공학의 분수령이었다. 이는 GPS 기반 안정성과 사용자 친화적인 제어를 결합한 최초의 저렴하고 즉시 비행 가능한 쿼드콥터 중 하나로, 항공 사진 및 비디오 촬영을 소비자와 전문가 모두에게 접근 가능하게 만들었다.111</p>
<p><strong>기술적 세부사항:</strong> 이 보고서는 팬텀 1의 주요 특징인 독특한 흰색 외관, GPS 기반 위치 고정(당시 안정성을 위한 주요 기능), 그리고 고프로(GoPro) 카메라용 마운트를 설명한다.111 통합된 패키지에서 이러한 기능의 조합은 혁명적이었다.</p>
<p><strong>영향력 분석:</strong> 팬텀 시리즈는 사실상 소비자 및 프로슈머 드론 시장을 창출했다. 그 영향은 단일 제품을 훨씬 뛰어넘는다. 이는 항공 감지를 민주화하여 영화 촬영, 부동산, 건설, 농업 및 공공 안전 분야에서 새로운 비즈니스 모델을 창출했다. 이는 현재까지 로봇 기술의 가장 중요한 상용화 사례 중 하나를 대표한다.</p>
<p>DARPA 그랜드 챌린지는 연구 개발을 가속화하는 강력한 새로운 모델을 보여주었다. 대담하지만 명확한 목표를 설정하고, 공통의 벤치마크를 제공하며, 상당한 상금을 제공함으로써 DARPA는 수년간 정체되었던 문제를 18개월 만에 해결하는 치열한 경쟁과 협력을 촉진했다. 2004년 챌린지의 극적인 실패는 77 오히려 집중된 커뮤니티와 해결해야 할 명확한 문제들을 만들어냈다.74 2005년과 2007년 챌린지에서는 여러 팀이 성공하면서 빠르고 기하급수적인 진전이 이루어졌다.74 이 팀들로부터 나온 인재와 기술은 상업용 자율주행 산업의 씨앗이 되었다. 이러한 경쟁적이고 가시성이 높은 형식은 특정하고 야심찬 기술 목표에 대해 전통적이고 분리된 연구 자금 지원 모델을 능가할 수 있는 강력한 촉매제이다.</p>
<p>한편, 보스턴 다이내믹스는 로봇공학 연구가 전파되는 방식을 근본적으로 바꾸었다. 전례 없는 능력을 보여주는 세심하게 제작된 비디오들은 일반적인 학술 논문을 훨씬 뛰어넘는 도달 범위를 가진 새로운 형태의 “발표“가 되었다. 2008년 빅독 발차기 비디오나 100 아틀라스 백플립 비디오에 대한 대중의 반응은 기술 시연을 넘어선 문화적 사건이었다. 이러한 비디오들은 로봇의 능력(동적 안정성, 민첩성)을 어떤 학술 논문보다 더 효과적으로, 그리고 더 넓은 청중에게 전달한다. 이는 대중의 흥미를 유발하고, 자금 지원 기관의 주목을 끌며, 다른 연구자들이 유사한 수준의 성능을 추구하도록 영감을 주거나 압박하는 피드백 루프를 생성한다. 21세기에 “바이럴 시연 비디오“는 과학적 커뮤니케이션 및 발표의 합법적이고 매우 영향력 있는 형태가 되었으며, 연구 의제를 설정하는 데 있어 중요한 논문만큼이나 영향력이 있다고 주장할 수 있다.</p>
<h2>4. 결론: 종합 및 미래 궤적</h2>
<h3>4.1 진화적 흐름의 종합</h3>
<p>이 보고서는 로봇공학의 진화 과정을 추적했다. 셰이키와 초기 산업용 팔의 결정론적이고 모델 기반의 세계에서 시작하여, 불확실성을 다루기 위한 확률론적 방법(SLAM, PRM)의 도입을 거쳐, 현재의 종단간 학습이 지배하는 시대에 이르렀다. 이러한 여정은 한 패러다임의 한계가 다음 패러다임의 창조를 직접적으로 영감을 주는 진보의 순환적 특성을 보여준다. 초기에는 “몸체”(유니메이트)와 “정신”(셰이키)이 분리되어 발전했다. 하나는 산업 현장의 힘과 정밀성을 추구했고, 다른 하나는 인공지능의 인식과 추론을 탐구했다. 이후 C-space와 같은 수학적 추상화와 PRM/RRT와 같은 알고리즘적 혁신은 로봇이 복잡한 환경에서 움직일 수 있는 이론적 도구를 제공했다. 이 시기는 이론이 실제의 한계를 극복하고, 다시 실제가 이론의 새로운 방향을 제시하는 건강한 피드백 루프를 통해 특징지어졌다. 최근 10년은 딥러닝 혁명으로 정의된다. DQN이 제시한 “픽셀에서 행동으로“의 철학은 로봇공학의 거의 모든 측면에 스며들어, 수작업으로 설계된 파이프라인을 데이터로부터 직접 학습하는 종단간 시스템으로 대체했다. DARPA 그랜드 챌린지와 같은 촉매적 사건과 보스턴 다이내믹스의 바이럴 시연은 이러한 기술적 도약을 대중의 인식 속으로 밀어 넣었고, 학문적 추구에서 산업적 현실로의 전환을 가속화했다.</p>
<h3>4.2 미래 궤적에 대한 전문가 전망</h3>
<p>최근 수상 논문과 새로운 동향 분석에 근거하여, 차세대 영향력 있는 혁신이 일어날 가능성이 높은 분야에 대한 전문가적 견해는 다음과 같다.</p>
<ul>
<li>
<p><strong>조작의 일반화:</strong> 단일 작업을 학습하는 것에서 벗어나, 비정형적인 인간 환경에서 광범위한 조작 기술을 수행할 수 있는 로봇을 만드는 방향으로 나아갈 것이다. 이는 “SpeedFolding“과 같은 연구를 기반으로 한다.</p>
</li>
<li>
<p><strong>언어 기반 로봇공학:</strong> 대규모 언어 모델(LLM)을 로봇 시스템과 통합하여 자연어 지시를 따르고 추론할 수 있게 하는 것이다. 이는 “자연어 대화에서 동적 내비게이션 목표 추출“과 같은 연구를 확장한다.</p>
</li>
<li>
<p><strong>견고한 Sim-to-Real 전환:</strong> 시뮬레이션에서 학습된 정책을 물리적 세계로 이전하는 지속적인 과제를 해결하는 것은 안전하고 샘플 효율적인 강화 학습을 위한 핵심적인 요소이다.</p>
</li>
<li>
<p><strong>인지 아키텍처:</strong> 반응적인 정책을 넘어, 더 심오한 추론, 계획, 기억 능력을 갖춘 로봇을 구축하는 방향으로 나아갈 것이다. 이는 아마도 셰이키 프로젝트의 원래 목표 중 일부로의 회귀를 의미할 수 있지만, 현대의 학습 기반 도구로 무장하고 있다.</p>
</li>
</ul>
<p>결론적으로, 로봇공학의 역사는 추상화, 경쟁, 그리고 계산 능력의 끊임없는 진보에 의해 주도된 여정이다. 미래의 혁신은 이러한 흐름을 계속 이어가되, 기계가 우리의 세계를 인식하고, 상호작용하며, 학습하는 방식에 대한 근본적인 이해를 심화시키는 데이터의 힘을 더욱 활용할 것이다.</p>
<h2>5. 참고 자료</h2>
<ol>
<li>NIHF Inductee George Devol Invented the Industrial Robot, https://www.invent.org/inductees/george-devol</li>
<li>Unimate - Wikipedia, https://en.wikipedia.org/wiki/Unimate</li>
<li>Unimate - The Robot Hall of Fame, http://www.robothalloffame.org/inductees/03inductees/unimate.html</li>
<li>Groundbreaking Robots from History You Need to Know - ASME, https://www.asme.org/topics-resources/content/5-most-influential-robots-in-history</li>
<li>George Devol Invents Unimate, the First Industrial Robot - History of Information, https://www.historyofinformation.com/detail.php?entryid=4071</li>
<li>The most influential people in robotics today - Telefónica, https://www.telefonica.com/en/communication-room/blog/most-influential-people-robotics-today/</li>
<li>Shakey - CHM Revolution - Computer History Museum, https://www.computerhistory.org/revolution/artificial-intelligence-robotics/13/289</li>
<li>Shakey the Robot - Stanford AI Lab, https://ai.stanford.edu/~nilsson/OnlinePubs-Nils/shakey-the-robot.pdf</li>
<li>History of Robots and Robotics | Origins of Robots - Robotnik, https://robotnik.eu/history-of-robots-and-robotics/</li>
<li>Stanford’s robotics legacy | Stanford Report, https://news.stanford.edu/stories/2019/01/stanfords-robotics-legacy</li>
<li>Stanford Cart - Watson, http://watson.latech.edu/book/intelligence/intelligenceOverview5b4.html</li>
<li>Robot Research - CHM Revolution - Computer History Museum, https://www.computerhistory.org/revolution/artificial-intelligence-robotics/13/293</li>
<li>Stanford Cart - CHM Revolution - Computer History Museum, https://www.computerhistory.org/revolution/artificial-intelligence-robotics/13/293/1277</li>
<li>Atlas (robot) - Wikipedia, https://en.wikipedia.org/wiki/Atlas_(robot)</li>
<li>Stanford Racing Team’s Entry In The 2005 DARPA Grand Challenge, https://cs.stanford.edu/people/dstavens/darpa/Stanford.pdf</li>
<li>Autonomous Driving in Traffic: Boss and the Urban Challenge, https://www.cmu.edu/traffic21/pdfs/aimag2009_urmson-compressed.pdf</li>
<li>Source of seminal papers in robotics, https://robotics.stackexchange.com/questions/15409/source-of-seminal-papers-in-robotics</li>
<li>Manipulation Techniques in Robotics: A Comprehensive Review and Future Directions Abstract - OSF, https://osf.io/b5f6x/download</li>
<li>Dynamically Stable Legged Locomotion - CMU Robotics Institute, https://www.ri.cmu.edu/pub_files/pub3/raibert_marc_h_1983_1/raibert_marc_h_1983_1.pdf</li>
<li>Marc Raibert - Expert Keynote and Motivational Speakers, https://www.chartwellspeakers.com/speaker/marc-raibert/</li>
<li>Legged Robots That Balance | Semantic Scholar, https://www.semanticscholar.org/paper/Legged-Robots-That-Balance-Raibert-Tello/a5366f4d0e17dce1cdb59ddcd90e806ef8741fbc</li>
<li>Legged Robots That Balance (Artificial Intelligence) - Raibert, Marc …, https://www.abebooks.com/9780262181174/Legged-Robots-Balance-Artificial-Intelligence-0262181177/plp</li>
<li>Legged Robots That Balance book by Marc H. Raibert - ThriftBooks, https://www.thriftbooks.com/w/legged-robots-that-balance_marc-h-raibert/20034673/</li>
<li>Marc Raibert | Keynote Speaker | AAE Speakers Bureau, https://www.aaespeakers.com/keynote-speakers/marc-raibert</li>
<li>CISE/Hariri Distinguished Speaker Marc Raibert, Executive Director, AI Institute; Founder, Boston Dynamics, https://www.bu.edu/hic/cise-hariri-distinguished-speaker-marc-raibertboston-dynamics/</li>
<li>Probabilistic Roadmaps for Path Planning in High-Dimensional Configuration Spaces, https://www.researchgate.net/publication/3298646_Probabilistic_Roadmaps_for_Path_Planning_in_High-Dimensional_Configuration_Spaces</li>
<li>Probabilistic Roadmaps for Path Planning in High-Dimensional …, https://www.cs.cmu.edu/~motionplanning/papers/sbp_papers/PRM/prmbasic_01.pdf</li>
<li>Probabilistic Roadmaps for Path Planning in High-Dimensional Configuration Spaces - DSpace, https://dspace.library.uu.nl/bitstream/1874/17328/1/kavraki_94_probabilistic.pdf</li>
<li>A Comprehensive Survey of Visual SLAM Algorithms - MDPI, https://www.mdpi.com/2218-6581/11/1/24</li>
<li>Mur-Artal, R., Montiel, J.M.M. and Tardos, J.D. (2015) ORB-SLAM A Versatile and Accurate Monocular SLAM System. IEEE Transactions on Robotics, 31, 1147-1163. - References - Scientific Research Publishing, https://www.scirp.org/reference/referencespapers?referenceid=2606503</li>
<li>(PDF) ORB-SLAM: a versatile and accurate monocular SLAM system - ResearchGate, https://www.researchgate.net/publication/271823237_ORB-SLAM_a_versatile_and_accurate_monocular_SLAM_system</li>
<li>[1502.00956] ORB-SLAM: a Versatile and Accurate Monocular SLAM System - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/1502.00956</li>
<li>ORB-SLAM: A Versatile and Accurate Monocular SLAM System - SciSpace, https://scispace.com/papers/orb-slam-a-versatile-and-accurate-monocular-slam-system-t5f1c0a74e</li>
<li>Key papers to catch up on the last 5 years of state-of-the-art SLAM, localization, state estimation, and sensor fusion : r/robotics - Reddit, https://www.reddit.com/r/robotics/comments/1l1qw2s/key_papers_to_catch_up_on_the_last_5_years_of/</li>
<li>raulmur/ORB_SLAM: A Versatile and Accurate Monocular SLAM - GitHub, https://github.com/raulmur/ORB_SLAM</li>
<li>[1502.00956] ORB-SLAM: a Versatile and Accurate Monocular SLAM System - arXiv, https://arxiv.org/abs/1502.00956</li>
<li>Incremental Sampling-based Algorithms for Optimal Motion Planning | Request PDF, https://www.researchgate.net/publication/45915538_Incremental_Sampling-based_Algorithms_for_Optimal_Motion_Planning</li>
<li>Sampling-based algorithms for optimal motion planning - DSpace@MIT, https://dspace.mit.edu/handle/1721.1/81442</li>
<li>[1005.0416] Incremental Sampling-based Algorithms for Optimal Motion Planning - arXiv, https://arxiv.org/abs/1005.0416</li>
<li>Sampling-based Algorithms for Optimal Motion Planning - Computer Science &amp; Engineering, https://www.cse.lehigh.edu/~trink/Courses/RoboticsII/reading/karaman_sampling-based-optimal-motion-planning.pdf</li>
<li>Test of Time Award - RSS Foundation, https://roboticsfoundation.org/awards/test-of-time-award/</li>
<li>Playing Atari with Deep Reinforcement Learning - Notes by Lex Toumbourou, https://notesbylex.com/playing-atari-with-deep-reinforcement-learning</li>
<li>(PDF) Playing Atari with Deep Reinforcement Learning (2013) | Volodymyr Mnih | 10532 Citations - SciSpace, https://scispace.com/papers/playing-atari-with-deep-reinforcement-learning-2f9cvplh8c?citations_page=77</li>
<li>Playing Atari with Deep Reinforcement Learning, https://arxiv.org/abs/1312.5602</li>
<li>Playing Atari with Deep Reinforcement Learning - ResearchGate, https://www.researchgate.net/publication/259367763_Playing_Atari_with_Deep_Reinforcement_Learning</li>
<li>Reinforcement Learning in Robotics: A Survey, https://www.ri.cmu.edu/pub_files/2013/7/Kober_IJRR_2013.pdf</li>
<li>(PDF) Reinforcement Learning in Robotics: A Survey - ResearchGate, https://www.researchgate.net/publication/258140920_Reinforcement_Learning_in_Robotics_A_Survey</li>
<li>Must read papers for Reinforcement Learning : r/reinforcementlearning - Reddit, https://www.reddit.com/r/reinforcementlearning/comments/1is773d/must_read_papers_for_reinforcement_learning/</li>
<li>Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes - AAAI Publications, https://ojs.aaai.org/index.php/AAAI/article/view/35095/37250</li>
<li>Learning-based legged locomotion; state of the art and future perspectives - arXiv, https://arxiv.org/html/2406.01152v2</li>
<li>Honglak Lee receives RSS 2023 Test-of-Time Award - Computer Science and Engineering, https://cse.engin.umich.edu/stories/honglak-lee-receives-rss-2023-test-of-time-award</li>
<li>[1301.3592] Deep Learning for Detecting Robotic Grasps - arXiv, https://arxiv.org/abs/1301.3592</li>
<li>(PDF) Deep Learning for Detecting Robotic Grasps - ResearchGate, https://www.researchgate.net/publication/319770233_Deep_Learning_for_Detecting_Robotic_Grasps</li>
<li>Ian Lenz - Semantic Scholar, https://www.semanticscholar.org/author/Ian-Lenz/1966934</li>
<li>Deep Learning for Detecting Robotic Grasps, https://www.roboticsproceedings.org/rss09/p12.pdf</li>
<li>lude-ma/deep_grasping: Code for Deep Learning for Detecting Robotic Grasps, Ian Lenz, Honglak Lee, Ashutosh Saxena. In Robotics: Science and Systems (RSS), 2013. - GitHub, https://github.com/lude-ma/deep_grasping</li>
<li>Translating Images into Maps, https://arxiv.org/pdf/2110.00966</li>
<li>Translating Images into Maps (Extended Abstract) - Surrey Open Research repository, https://openresearch.surrey.ac.uk/view/pdfCoverPage?instCode=44SUR_INST&amp;filePid=13187330840002346&amp;download=true</li>
<li>Translating Images into Maps (Extended Abstract) - IJCAI, https://www.ijcai.org/proceedings/2023/0725.pdf</li>
<li>#ICRA2022 awards finalists and winners - Robohub, https://robohub.org/icra2022-awards-finalists-and-winners/</li>
<li>IEEE International Conference on Robotics and Automation (ICRA), https://robotics.ee/author/ieee-international-conference-on-robotics-and-auto/</li>
<li>Awards and Finalists - IEEE ICRA 2025, https://2025.ieee-icra.org/program/awards-and-finalists/</li>
<li>IEEE Robotics and Automation Society (RAS) Announces Award-Winning Papers and Demonstrations at the IEEE International Conference on Robotics and Automation (ICRA) - ICRA 2025, https://2025.ieee-icra.org/announcements/ieee-robotics-and-automation-society-ras-announces-award-winning-papers-and-demonstrations-at-the-ieee-international-conference-on-robotics-and-automation-icra/</li>
<li>IROS RoboCup Best Paper Award, https://www.robocup.org/iros_robocup_best_paper_award</li>
<li>IROS 2023 Award Winners - IROS 2023, https://2023.ieee-iros.org/iros-2023-award-winners/</li>
<li>Best Paper Award - IROS 2019 - Macau, https://www.iros2019.org/awards</li>
<li>Award Winners - IROS 2022, https://iros2022.org/2022/10/30/award-winners/</li>
<li>Research Papers - UC Berkeley’s AUTOLab, https://autolab.berkeley.edu/publications.shtml</li>
<li>SpeedFolding - Learning Efficient Bimanual Folding of Garments, https://pantor.github.io/speedfolding/</li>
<li>SpeedFolding: Learning Efficient Bimanual Folding of Garments - arXiv, https://arxiv.org/pdf/2208.10552</li>
<li>Autonomous Power Line Inspection with Drones via Perception-Aware MPC - zora.uzh.ch, https://www.zora.uzh.ch/entities/publication/2f9fbf0d-78bc-4790-b0d7-baabbac9877b</li>
<li>Autonomous Power Line Inspection with Drones via Perception-Aware MPC - ResearchGate, https://www.researchgate.net/publication/369759852_Autonomous_Power_Line_Inspection_with_Drones_via_Perception-Aware_MPC</li>
<li>Autonomous Power Line Inspection with Drones via Perception-Aware MPC, https://ippc-iros23.github.io/papers/xing.pdf</li>
<li>The DARPA Grand Challenge: Ten Years Later, https://www.darpa.mil/news/2014/grand-challenge-ten-years-later</li>
<li>DARPA Grand Challenge - Wikipedia, https://en.wikipedia.org/wiki/DARPA_Grand_Challenge</li>
<li>The Drive for Autonomous Vehicles: The DARPA Grand Challenge - HeroX, https://www.herox.com/blog/159-the-drive-for-autonomous-vehicles-the-darpa-grand</li>
<li>DARPA Grand Challenge (2004) - Wikipedia, https://en.wikipedia.org/wiki/DARPA_Grand_Challenge_(2004)</li>
<li>Autonomous Automobile Trajectory Tracking for Off-Road Driving: Controller Design, Experimental Validation and Racing - Stanford AI Lab, https://ai.stanford.edu/~gabeh/papers/hoffmann_stanley_control07.pdf</li>
<li>Stanley: The robot that won the DARPA Grand Challenge - ResearchGate, https://www.researchgate.net/publication/260870018_Stanley_The_robot_that_won_the_DARPA_Grand_Challenge</li>
<li>NOVA | The Great Robot Race | Cars That Drive Themselves - PBS, https://www.pbs.org/wgbh/nova/darpa/cars.html</li>
<li>Winning the DARPA Grand Challenge [Robotics] - Microsoft Research, https://www.microsoft.com/en-us/research/video/winning-the-darpa-grand-challenge-robotics/</li>
<li>Sebastian Thrun - the International Multisensory Research Forum, https://imrf.info/2006/viewabstract.php%3Fid=87.html</li>
<li>Autonomous Driving in Traffic: Boss and the Urban Challenge | AI Magazine, https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2238</li>
<li>Autonomous driving in urban environments: Boss and the Urban Challenge, https://www.ri.cmu.edu/publications/autonomous-driving-in-urban-environments-boss-and-the-urban-challenge/</li>
<li>Autonomous driving in urban environments: Boss and the Urban Challenge - Carnegie Mellon University, https://www.cmu.edu/traffic21/pdfs/urmson_christopher_2008_1.pdf</li>
<li>Honda’s Advanced Humanoid Robot ‘ASIMO’ Makes U.S. Debut, https://hondanews.com/en-US/releases/release-6d3f77ac0cb7c24f66e07a004c34c752-hondas-advanced-humanoid-robot-asimo-makes-u-s-debut</li>
<li>ASIMO - Wikipedia, https://en.wikipedia.org/wiki/ASIMO</li>
<li>Bring back ASIMO? What Happened to Honda’s Pioneering Humanoid Robot? - YouTube, https://www.youtube.com/watch?v=yAI_mk1u_-Y</li>
<li>Honda’s ASIMO Robot to be Demonstrated at Kennedy Center, https://hondanews.com/en-US/honda-corporate/releases/release-e9d4c6628d512e58ae6b38004c34bd89-hondas-asimo-robot-to-be-demonstrated-at-kennedy-center</li>
<li>ASIMO Makes Public Debut in Europe | Honda Global Corporate Website, https://global.honda/en/newsroom/worldnews/2003/c030909.html</li>
<li>Dead: Honda Asimo - Jalopnik, https://www.jalopnik.com/dead-asimo-1827216710/</li>
<li>Company History | Robotic Assisted Surgery | Intuitive, https://www.intuitive.com/en-us/about-us/company/history</li>
<li>da Vinci Surgical System - Wikipedia, https://en.wikipedia.org/wiki/Da_Vinci_Surgical_System</li>
<li>DaVinci Robot Demonstration Draws Crowd at UM Hospital - University of Miami, http://newsletter.miami.edu/med-archives/web/med/april2008/story03.html</li>
<li>Robot Surgery - the da Vinci Robot - Science Museum Blog, https://blog.sciencemuseum.org.uk/robot-surgery-the-da-vinci-robot/</li>
<li>Intuitive Surgical - Wikipedia, https://en.wikipedia.org/wiki/Intuitive_Surgical</li>
<li>The History of Robotic-Assisted Surgery, https://www.generalsurgerynews.com/Opinion/Article/09-21/The-History-of-Robotic-Assisted-Surgery/64651</li>
<li>BigDog - Wikipedia, https://en.wikipedia.org/wiki/BigDog</li>
<li>Boston Dynamics Big Dog (new video March 2008) - YouTube, https://www.youtube.com/watch?v=W1czBcnX1Ww</li>
<li>Boston Dynamics – More Canine Than Canine - Coilhouse, https://coilhouse.net/2008/03/boston-dynamics-more-canine-than-canine/</li>
<li>A Complete Review Of Boston Dynamics’ Atlas Robot - Brian D. Colwell, https://briandcolwell.com/a-complete-review-of-boston-dynamics-atlas-robot/</li>
<li>Google’s New Robot Can Do The Dishes - CBS Philadelphia, https://www.cbsnews.com/philadelphia/news/googles-new-robot-can-do-the-dishes/</li>
<li>Boston Dynamics Introduces SpotMini - Born to Engineer, https://www.borntoengineer.com/boston-dynamics-introduces-spotmini</li>
<li>Introducing Spot (previously SpotMini) - YouTube, https://www.youtube.com/watch?v=tf7IEVTDjng</li>
<li>US robot designed to tackle household chores - Press TV, https://www.presstv.co.uk/Detail/2016/06/28/472624/Boston-Dynamics-Spot-Min-robot</li>
<li>About the Spot Robot - Boston Dynamics Support Center, https://support.bostondynamics.com/s/article/About-the-Spot-Robot-72005</li>
<li>Spot | Boston Dynamics, https://bostondynamics.com/products/spot/</li>
<li>About | Boston Dynamics, https://bostondynamics.com/about/</li>
<li>Marc Raibert | Speaker | TED, https://www.ted.com/speakers/marc_raibert</li>
<li>Our robotic overlords: The talks of Session 2 of TED2017 | TED Blog, https://blog.ted.com/our-robotic-overlords-the-talks-of-session-2-of-ted2017/</li>
<li>DJI Phantom - Wikipedia, https://en.wikipedia.org/wiki/DJI_Phantom</li>
<li>RIP the DJI Phantom, the drone that started it all – and got me into aerial photography, https://www.techradar.com/cameras/drones/rip-the-dji-phantom-the-drone-that-started-it-all-and-got-me-into-aerial-photography</li>
<li>History of DJI: Redefining Aerial Photography, https://aboutphotography.blog/blog/history-of-dji</li>
<li>VIDEO: Boston Dynamics Unveils Upgraded BigDog Robotic Mule - TechEBlog, https://www.techeblog.com/video-boston-dynamics-unveils-upgraded-bigdog-robotic-mule/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>