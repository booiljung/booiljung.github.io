<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2021년 5월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2021년 5월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2021년 AI 및 로봇 연구 동향</a> / <span>2021년 5월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2021년 5월 AI 및 로봇 연구 동향</h1>
<h2>1.  서론</h2>
<p>2021년은 데이터가 ’디지털 경제의 원유’로 확고히 자리 잡고, 이를 처리하는 엔진으로서의 인공지능(AI)과 고속도로 역할을 하는 5G 기술의 융합이 전 산업의 지능화를 가속하는 변곡점이었다.1 이러한 거시적 기술 배경 속에서 AI 및 로봇 공학 연구는 새로운 국면을 맞이했다. 당시 AI 기술 수준은 미국이 세계 최고 기술 보유국의 지위를 유지하며 타 주요국과의 격차를 벌리는 양상을 보였는데 2, 이러한 배경은 International Conference on Learning Representations (ICLR), Conference on Computer Vision and Pattern Recognition (CVPR), International Conference on Robotics and Automation (ICRA)와 같은 최상위 학회에서 발표되는 연구들이 곧 기술 패권의 향방을 가늠하는 척도임을 시사했다.</p>
<p>이러한 산업적, 기술적 요구는 학계의 연구 방향에 직접적인 영향을 미쳤다. 데이터 기반의 지능형 애플리케이션에 대한 수요가 폭증함에 따라, 레이블이 없는 방대한 데이터를 효율적으로 학습할 수 있는 비지도 학습 방법론의 필요성이 대두되었다. 또한, 5G 기술이 촉발한 실감형 디지털 경험에 대한 요구는 제어 가능한 3D 콘텐츠 생성 기술의 발전을 촉진했다. 동시에, 서비스 경제의 성장은 인간과 안전하게 상호작용할 수 있는 실용적인 로봇 기술의 개발을 요구했다.</p>
<p>2021년 5월을 전후하여 개최된 ICLR(표현 학습), CVPR(컴퓨터 비전), ICRA(로봇 공학)는 각 분야의 가장 중요한 연구 성과가 공개되는 장이었다. 본 보고서는 이들 학회에서 발표된 핵심 연구 세 가지를 심층적으로 분석하여 당시 기술 발전의 최전선을 조망하고자 한다. 첫째, ICLR 2021에서 발표된 Prototypical Contrastive Learning (PCL)을 통해 데이터의 내재적 의미 구조를 학습하려는 비지도 표현 학습의 진화를 분석한다. 둘째, CVPR 2021의 최대 화두였던 3D 비전 분야에서, 제어 가능한 3D 장면 생성을 가능하게 한 GIRAFFE 모델을 탐구한다. 마지막으로, ICRA 2021에서 최우수 논문상을 수상한 평면형 천 공압 인공 근육(ffPAM)을 통해 웨어러블 로봇 기술의 발전을 조망한다.</p>
<h2>2.  인공지능: 자기지도 표현 학습과 3차원 비전의 심화</h2>
<h3>2.1  ICLR 2021: 비지도 표현 학습의 패러다임 전환</h3>
<p>2021년 ICLR에서는 표현 학습(Representation Learning)이 핵심 주제 중 하나로 다뤄졌다.3 특히, 레이블이 없는 방대한 데이터로부터 유의미한 특징(representation)을 추출하는 자기지도학습(Self-Supervised Learning, SSL)이 강화학습, 컴퓨터 비전 등 다양한 분야에서 주목받았다. 이는 로봇 제어 문제와 같이 픽셀 이미지와 같은 고차원 입력을 직접 처리해야 하는 상황에서, 사전 학습된 특징 추출기의 성능이 전체 시스템의 성패를 좌우하기 때문이다.4 SSL은 ImageNet과 같은 특정 데이터셋에 대한 의존도를 줄이고, 주어진 데이터 자체에서 감독 신호(supervisory signal)를 찾아내어 보다 일반화된 표현을 학습하는 것을 목표로 한다.4</p>
<h4>2.1.1 심층 분석 1: Prototypical Contrastive Learning (PCL)의 원리 및 의의</h4>
<p>기존의 SimCLR, MoCo 등 당시 최고 수준의 자기지도학습 방법론은 ’Instance-wise Contrastive Learning’에 기반했다. 이 방식은 동일 이미지의 다른 증강(augmentation) 버전은 ’positive’로 간주하여 임베딩 공간에서 가깝게 만들고, 그 외 다른 모든 이미지는 ’negative’로 간주하여 멀어지게 학습하는 방식이다.6 그러나 이 접근법은 근본적인 한계를 내포하고 있었다. 예를 들어, ‘고양이 A’ 이미지와 ‘고양이 B’ 이미지는 인간이 보기에 의미론적으로 매우 유사하지만, instance-wise 프레임워크에서는 서로 다른 개체(instance)이므로 ‘negative’ 쌍으로 취급되어 임베딩 공간에서 서로 밀어내도록 학습된다.7 이는 모델이 색상이나 질감 같은 데이터의 저수준 단서(low-level cues)에만 의존하게 만들고, 데이터셋 전체에 걸친 전역적인 의미 구조(global semantic structure)를 포착하지 못하게 하는 “의미론적 충돌(semantic collision)” 문제를 야기했다.5</p>
<p>PCL은 이러한 한계를 극복하기 위해 Contrastive Learning과 클러스터링을 결합하는 혁신적인 접근법을 제시했다.11 핵심 아이디어는 임베딩 공간에서 의미적으로 유사한 샘플들을 클러스터링하고, 각 클러스터의 중심점인 **프로토타입(Prototype)**을 해당 의미 그룹의 대표자로 사용하는 것이다.7 이를 통해 학습 과정은 개별 샘플의 임베딩이 자신이 속한 클러스터의 프로토타입과 가까워지도록 유도함으로써, instance-level의 지역적 유사성뿐만 아니라 semantic-level의 전역적 구조를 함께 학습하게 된다.5 이는 단순히 ’이 이미지는 저 이미지가 아니다’라는 수준을 넘어, ’이 고양이 이미지는 저 고양이 이미지와 개념적으로 유사하다’는 더 높은 수준의 추상화를 학습하는 패러다임의 전환을 의미했다.</p>
<p>PCL은 이 과정을 통계적으로 엄밀한 Expectation-Maximization (EM) 알고리즘 프레임워크로 공식화했다.5</p>
<ul>
<li>
<p><strong>E-step (Expectation):</strong> 현재 네트워크가 생성한 임베딩 공간에서 k-means와 같은 클러스터링 알고리즘을 수행하여 각 데이터 포인트가 어떤 프로토타입에 속할지에 대한 확률 분포를 추정한다. 즉, 프로토타입을 찾는 단계다.7</p>
</li>
<li>
<p><strong>M-step (Maximization):</strong> E-step에서 할당된 프로토타입을 일종의 가상 레이블(pseudo-label)처럼 사용하여, 네트워크 파라미터를 업데이트한다. 이 과정은 PCL이 제안한 <strong>ProtoNCE Loss</strong>를 최소화함으로써 수행된다.7</p>
</li>
</ul>
<p>ProtoNCE Loss는 기존 InfoNCE Loss를 일반화한 형태로 7, 실제 PCL에서 사용된 손실 함수는 두 가지 핵심 요소의 결합으로 이루어져 있다.12</p>
<ol>
<li>
<p><strong>Instance-wise Contrastive Loss (<span class="math math-inline">\mathcal{L}_{\text{InfoNCE}}</span>):</strong> 기존 MoCo 등에서 사용된 손실 함수와 동일하며, 각 샘플이 자신의 다른 증강 버전(positive)과는 가까워지고 다른 모든 샘플(negatives)과는 멀어지도록 하여 지역적 평활성(local smoothness)을 보존하는 역할을 한다.10 수식은 다음과 같다.</p>
<p><span class="math math-display">
\mathcal{L}_{\text{InfoNCE}} = - \sum_{i=1}^{n} \log \frac{\exp(v_i \cdot v_i&#39; / \tau)}{\sum_{j=0}^{r} \exp(v_i \cdot v_j&#39; / \tau)}
</span></p>
</li>
</ol>
<p>여기서 <span class="math math-inline">v_i</span>는 앵커(anchor) 샘플의 임베딩, <span class="math math-inline">v_i&#39;</span>는 positive 샘플의 임베딩, <span class="math math-inline">v_j&#39;</span>는 <span class="math math-inline">r</span>개의 negative 샘플과 1개의 positive 샘플 임베딩 집합, <span class="math math-inline">\tau</span>는 온도 파라미터(temperature parameter)이다.</p>
<ol start="2">
<li>
<p><strong>Prototypical Contrastive Loss (<span class="math math-inline">\mathcal{L}_{\text{ProtoNCE}}</span>):</strong> PCL의 핵심으로, 샘플의 임베딩 <span class="math math-inline">v_i</span>가 자신이 할당된 프로토타입 <span class="math math-inline">c_s</span>와는 가까워지고, 다른 프로토타입 <span class="math math-inline">c_t</span>와는 멀어지도록 학습한다.</p>
<p><span class="math math-display">
\mathcal{L}_{\text{ProtoNCE}} = - \sum_{i=1}^{n} \log \frac{\exp(v_i \cdot c_s / \phi_s)}{\sum_{t=1}^{k} \exp(v_i \cdot c_t / \phi_t)}
</span></p>
</li>
</ol>
<p>여기서 <span class="math math-inline">c_s</span>는 <span class="math math-inline">v_i</span>에 할당된 positive 프로토타입, <span class="math math-inline">{c_t}_{t=1}^k</span>는 모든 <span class="math math-inline">k</span>개의 프로토타입 집합이다. <span class="math math-inline">\phi</span>는 각 클러스터의 집중도(concentration)를 나타내는 파라미터로, 클러스터 내 샘플들의 분포가 얼마나 밀집되어 있는지를 동적으로 추정하여 학습에 반영한다.7</p>
<p>최종 손실 함수는 이 두 항의 가중합 <span class="math math-inline">\mathcal{L}_{\text{PCL}} = \mathcal{L}_{\text{InfoNCE}} + \lambda \mathcal{L}_{\text{ProtoNCE}}</span>로 표현된다. 학습 초기에는 아직 클러스터가 불안정하므로 <span class="math math-inline">\mathcal{L}_{\text{InfoNCE}}</span> 항이 표현 학습을 주도하고, 학습이 진행되어 클러스터가 의미 있는 그룹을 형성하게 되면 <span class="math math-inline">\mathcal{L}_{\text{ProtoNCE}}</span> 항이 전역적인 의미 구조를 학습하는 데 더 중요해지는 상호보완적 관계를 가진다.12</p>
<h4>2.1.2 Table 1: PCL 성능 비교</h4>
<p>PCL의 효과를 객관적으로 입증하기 위해, 당시의 대표적인 SSL 방법론인 MoCo v2, SimCLR 등과의 정량적 성능 비교는 필수적이다. 특히 ImageNet 데이터셋에 대한 선형 분류(linear classification) 성능은 비지도 학습된 표현의 품질을 평가하는 표준 척도이며, 전이 학습(transfer learning) 성능은 표현의 일반화 능력을 보여준다.5</p>
<table><thead><tr><th>모델 (Model)</th><th>Backbone</th><th>학습 Epochs</th><th>ImageNet Top-1 정확도 (%)</th><th>VOC07 Transfer Learning (mAP)</th></tr></thead><tbody>
<tr><td>MoCo v2 12</td><td>ResNet-50</td><td>200</td><td>67.5</td><td>-</td></tr>
<tr><td>SimCLR</td><td>ResNet-50</td><td>200</td><td>66.5</td><td>-</td></tr>
<tr><td><strong>PCL (ours)</strong> 12</td><td><strong>ResNet-50</strong></td><td><strong>200</strong></td><td><strong>67.6</strong></td><td><strong>83.2</strong></td></tr>
<tr><td>Supervised</td><td>ResNet-50</td><td>100</td><td>76.5</td><td>83.9</td></tr>
</tbody></table>
<h3>2.2  CVPR 2021: 제어 가능한 3D 장면 생성의 서막</h3>
<p>CVPR 2021은 3D 컴퓨터 비전이 학회의 핵심 주제로 부상한 해였다. 총 1660편의 채택 논문 중 200편 이상이 3D 비전 관련 연구였으며, 이는 딥러닝, 이미지 합성 등 다른 분야를 압도하는 수치였다.15 이는 단순히 새로운 시점을 합성하는 것을 넘어, 3D 세계를 이해하고, 분해하며, 제어 가능한 방식으로 생성하려는 연구 커뮤니티의 강력한 의지를 반영했다.</p>
<h4>2.2.1 심층 분석 2: GIRAFFE - Compositional Generative Neural Feature Fields</h4>
<p>StyleGAN과 같은 기존의 고품질 이미지 생성 모델들은 2D 공간에서 작동하여 3차원 세계의 구성적(compositional) 본질을 제대로 반영하지 못하는 한계가 있었다. 예를 들어, 생성된 이미지에서 자동차의 위치를 조금 옮기려고 하면, 자동차와는 무관한 배경까지 함께 왜곡되는 ‘얽힘(entanglement)’ 문제가 발생했다.16 이는 제어 메커니즘이 모델에 내재된 것이 아니라, 학습된 잠재 공간(latent space)에서 사후에 발견해야 하는 방식이기 때문이다.</p>
<p>GIRAFFE는 이러한 문제를 해결하기 위해, 씬(scene)을 여러 개의 구성요소(배경, 객체 등)로 분해하고, 각 요소를 **생성적 신경 특징 필드(Generative Neural Feature Fields)**라는 3D 표현으로 모델링하는 방식을 제안했다.16 이는 NeRF(Neural Radiance Fields)의 아이디어를 생성 모델에 적용하고, 이를 여러 객체로 확장한 것이다. 이 접근법을 통해 GIRAFFE는 별도의 레이블이나 감독 정보 없이(unsupervised) 이미지 컬렉션만으로 배경과 객체를 분리하고, 각 객체의 형상(shape), 외형(appearance), 3D 공간상의 포즈(pose)를 독립적으로 제어할 수 있게 되었다.18</p>
<p>GIRAFFE의 진정한 혁신은 복잡한 3D 장면 표현(feature fields)과 최종 픽셀 생성(neural renderer)을 효율적인 중간 표현(저해상도 특징 이미지)으로 연결하여 전략적으로 분리한 데 있다. 이 아키텍처는 높은 품질, 제어 가능성, 그리고 빠른 속도를 동시에 달성하는 열쇠가 되었다. 기존의 3D 인식 모델들이 3D 공간에서 고해상도 RGB 이미지를 직접 렌더링하여 계산 비용이 매우 높았던 반면 17, GIRAFFE는 렌더링 대상을 저차원의 추상적 표현인 특징 맵으로 변경하여 계산량을 획기적으로 줄였다. 최종 고품질 픽셀을 생성하는 작업은 이 분야에 고도로 최적화된 2D 신경망(신경 렌더러)에 위임했다.18 이러한 역할 분담은 제어 가능성을 높이는 부수적인 효과를 낳았다. 3D Volume Rendering 부분은 장면의 구조, 배치, 대략적인 특징에만 집중하고, 2D 신경 렌더러는 질감, 조명 등 세부적인 표현을 담당하게 되면서, 3D 포즈 조작이 씬의 구조만 변경하고 텍스처는 그 위에 자연스럽게 다시 그려지도록 만들어 2D GAN의 얽힘 문제를 근본적으로 해결했다.</p>
<p>GIRAFFE의 렌더링 파이프라인은 다음과 같은 두 단계로 구성된다.18</p>
<ol>
<li>
<p><strong>Volume Rendering to a Feature Image:</strong> 먼저, 각 객체와 배경의 Feature Field를 3D 공간상에서 합성한 후, 고전적인 Volume Rendering 기법을 사용하여 이를 렌더링한다. 그러나 최종 RGB 이미지가 아닌, **저해상도의 특징 이미지(low-resolution feature image)**를 생성한다.18 Volume Rendering의 원리는 특정 시점의 카메라 광선(ray)을 따라 샘플링된 3D 포인트들의 색상(<span class="math math-inline">c</span>)과 밀도(<span class="math math-inline">\sigma</span>)를 적분하여 최종 픽셀 색상을 결정하는 것이다.22 이산화된 형태의 렌더링 방정식은 다음과 같다.</p>
<p><span class="math math-display">
\hat{C}(\mathbf{r}) = \sum_{i=1}^{N} T_i (1 - \exp(-\sigma_i \delta_i)) \mathbf{c}_i, \quad \text{where} \quad T_i = \exp\left(-\sum_{j=1}^{i-1} \sigma_j \delta_j\right)
</span></p>
</li>
</ol>
<p>여기서 <span class="math math-inline">\hat{C}(\mathbf{r})</span>는 광선 <span class="math math-inline">\mathbf{r}</span>에 대한 예상 색상, <span class="math math-inline">T_i</span>는 투과율(transmittance), <span class="math math-inline">\sigma_i</span>와 <span class="math math-inline">\mathbf{c}_i</span>는 <span class="math math-inline">i</span>번째 샘플의 밀도와 색상, <span class="math math-inline">\delta_i</span>는 샘플 간의 거리이다. GIRAFFE에서는 <span class="math math-inline">\mathbf{c}_i</span>가 RGB가 아닌 고차원 특징 벡터(feature vector)가 된다.</p>
<ol start="2">
<li><strong>2D Neural Rendering:</strong> Volume Rendering을 통해 생성된 저해상도 특징 이미지를 입력받아, 2D CNN 기반의 **신경 렌더러(Neural Renderer)**가 최종적인 고해상도, 고품질 RGB 이미지로 변환한다.17 신경 렌더링은 딥러닝 모델을 활용하여 3D 모델로부터 2D 이미지를 생성하는 과정을 의미하며, 전통적인 렌더링 파이프라인의 일부를 학습 가능한 신경망으로 대체하여 사실성과 제어 가능성을 높이는 기술이다.24</li>
</ol>
<h4>2.2.2 Table 2: GIRAFFE와 2D GAN 비교</h4>
<p>GIRAFFE의 핵심 기여는 ’제어 가능성’의 획기적인 향상에 있다. 따라서 기존 2D GAN과의 비교는 생성된 이미지의 품질(FID Score)과 같은 정량적 지표뿐만 아니라, 객체의 포즈, 외형, 배경 등을 독립적으로 제어할 수 있는지에 대한 정성적 평가를 반드시 포함해야 한다.16</p>
<table><thead><tr><th>구분 (Category)</th><th>2D-GAN Baseline</th><th>GIRAFFE (ours)</th><th>분석 (Analysis)</th></tr></thead><tbody>
<tr><td><strong>정량 지표</strong></td><td></td><td></td><td></td></tr>
<tr><td>FID Score (낮을수록 좋음)</td><td>높음 17</td><td><strong>낮음</strong> 17</td><td>3D 구조를 이해하여 더 사실적인 이미지 생성</td></tr>
<tr><td>렌더링 시간 (ms)</td><td>-</td><td><strong>5.9</strong> (vs. GRAF 1595.0) 17</td><td>하이브리드 렌더링 파이프라인으로 압도적인 속도 향상</td></tr>
<tr><td><strong>정성 지표 (제어 가능성)</strong></td><td></td><td></td><td></td></tr>
<tr><td>객체 이동 (Translation)</td><td>배경 왜곡 발생 16</td><td><strong>독립적 제어 가능</strong> 17</td><td>전경과 배경의 완벽한 분리(disentanglement) 달성</td></tr>
<tr><td>객체 회전 (Rotation)</td><td>불가능 또는 비현실적</td><td><strong>3D 일관성 유지</strong> 17</td><td>3D 표현을 통해 모든 시점에서 일관된 외형 유지</td></tr>
<tr><td>객체 외형 변경</td><td>다른 객체/배경에 영향</td><td><strong>독립적 제어 가능</strong> 16</td><td>객체별 외형 코드를 분리하여 학습</td></tr>
<tr><td>카메라 포즈 변경</td><td>불가능</td><td><strong>가능</strong> 16</td><td>3D 씬을 렌더링하므로 자유로운 시점 변경 가능</td></tr>
<tr><td>객체 추가/제거</td><td>불가능</td><td><strong>테스트 시점에 가능</strong> 16</td><td>구성적 표현 덕분에 훈련 데이터에 없던 씬 구성 가능</td></tr>
</tbody></table>
<h2>3.  로봇 공학: 인간 중심의 소프트 로봇과 자율 시스템</h2>
<h3>3.1  ICRA 2021: 필드 로보틱스부터 인간-로봇 상호작용까지</h3>
<p>로봇 공학 분야 최고 권위의 학회인 ICRA 2021에서는 항공 로보틱스(Aerial Robotics), 매니퓰레이션(Manipulation), SLAM, 인간-로봇 상호작용(HRI), 필드 로보틱스(Field Robotics), 소프트 로보틱스(Soft Robotics) 등 매우 광범위한 주제의 연구들이 발표되었다.25 이는 로봇 기술이 전통적인 산업 현장을 넘어 일상 생활, 의료, 탐사 등 다양한 영역으로 확장되고 있음을 보여준다.26 특히 ICRA 2021에서는 국내 대학 및 연구기관의 활약이 두드러졌는데, KAIST, 서울대 등 주요 대학 연구실에서 다리형 로봇, 매니퓰레이션, 인식, SLAM, 햅틱 등 여러 핵심 분야에 걸쳐 다수의 논문을 발표하며 세계적인 연구 경쟁력을 입증하였다.28</p>
<h4>3.1.1 심층 분석 3: 소프트 웨어러블 로봇을 위한 평면형 천 공압 인공 근육 (ffPAM)</h4>
<p>McKibben 타입으로 대표되는 기존 공압 인공 근육(Pneumatic Artificial Muscle, PAM)은 높은 힘 대 무게비를 가지는 장점에도 불구하고, 부피가 크고(bulky) 응답 속도가 느리다는 고질적인 문제점을 안고 있었다.29 이는 특히 인체에 밀착되어야 하는 웨어러블 로봇에 적용하기 어려운 제약 조건으로 작용했다.</p>
<p>KAIST 김정 교수 연구팀이 개발하고 ICRA 2021 서비스 로봇 부문 최우수 논문상(Best Paper Award in Service Robotics)을 수상한 ffPAM은 이러한 한계를 극복하기 위해 두 가지 핵심적인 혁신을 도입했다.31</p>
<ol>
<li>
<p><strong>소재의 혁신:</strong> 기존의 고무 튜브와 꼰 메쉬 슬리브 구조 대신, 얇고 유연한 **특수 천 소재(thin fabric)**를 기본 재료로 사용했다.29</p>
</li>
<li>
<p><strong>구조의 혁신:</strong> 원통형이 아닌 **컴팩트한 평면형 디자인(compact flat design)**을 고안하여, 작동하지 않을 때의 부피를 최소화하고 의복에 은닉 형태로 내장할 수 있도록 설계했다.30</p>
</li>
</ol>
<p>ffPAM의 성공은 현대 로봇 공학의 중요한 경향을 보여준다. 즉, 혁신은 더 이상 알고리즘과 제어 이론에만 국한되지 않으며, 재료 과학과 폼 팩터 설계에 의해 주도되고 있다는 점이다. 기존 PAM이 웨어러블 응용 분야에서 겪었던 물리적 한계는 새로운 제어 알고리즘이 아닌, 재료(고무/메쉬에서 천으로)와 형태(원통형에서 평면형으로)를 근본적으로 재고함으로써 해결되었다. 이러한 재료 및 설계 선택은 높은 힘 대 무게비, 빠른 응답 속도, 얇은 프로파일과 같은 성능 지표의 직접적인 원인이 되었다. 이는 웨어러블 서비스 로봇이라는 새로운 응용 분야의 문을 여는 계기가 되었으며, 미래의 인간 중심 로봇(보조 기기, 외골격, 협업 로봇) 개발이 지능형 재료와 새로운 물리적 구조를 제어 시스템과 함께 공동으로 설계하는 방향으로 나아갈 것임을 시사한다. 재료 자체가 연산과 기능의 일부가 되어 하드웨어와 소프트웨어의 경계를 허무는 것이다.</p>
<p>ffPAM은 다양한 실험을 통해 기존 PAM 대비 월등한 실용적 성능을 입증했다.29 172 kPa의 압력에서</p>
<p><strong>118.0 N</strong>의 높은 힘을 발휘하며, 무게는 <strong>34.8 g</strong>에 불과했다. 수축 응답 시간은 <strong>0.0318초</strong>에 불과하며, **4 Hz 이상의 대역폭(bandwidth)**을 가져 빠른 움직임에 대응할 수 있었다. 또한 낮은 이력 오차(<strong>6.2%</strong>)를 보이며, <strong>10,000회</strong>의 반복 작동에도 최대 수축 길이를 유지하는 높은 내구성을 입증했다. 더 나아가, 천 기반의 부드러운 수축 센서를 내장하여 별도의 외부 센서 없이 구동기 자체의 수축 길이를 측정할 수 있는 기능까지 통합했다.29</p>
<h4>3.1.2 Table 3: ffPAM 핵심 성능 지표</h4>
<p>ffPAM 연구의 핵심은 정량적으로 측정된 뛰어난 성능 지표에 있다. 이 데이터들을 표로 정리하면 연구의 기여도를 직관적으로 파악하고 기존 기술과 쉽게 비교할 수 있으며, 이 논문이 최우수 논문상을 수상한 이유를 명확히 보여준다.29</p>
<table><thead><tr><th>성능 지표 (Performance Metric)</th><th>측정값 (Value)</th><th>비고 (Notes)</th></tr></thead><tbody>
<tr><td>최대 힘 (Maximum Force)</td><td>118.0 N ± 0.1 N</td><td>@ 172 kPa</td></tr>
<tr><td>평균 최대 수축률 (Avg. Max. Contraction Ratio)</td><td>23.84% ± 0.06%</td><td></td></tr>
<tr><td>최대 이력 오차 (Max. Hysteresis Error)</td><td>6.2%</td><td></td></tr>
<tr><td>응답 시간 (Response Time for Contraction)</td><td>0.0318 s ± 0.0012 s</td><td></td></tr>
<tr><td>대역폭 (Bandwidth)</td><td>&gt; 4 Hz</td><td></td></tr>
<tr><td>내구성 (Durability)</td><td>10,000 cycles</td><td>최대 수축 길이 유지</td></tr>
<tr><td>총 무게 (Total Weight)</td><td>34.8 g</td><td></td></tr>
</tbody></table>
<h2>4.  종합 및 전망</h2>
<p>2021년 5월의 주요 연구 동향을 종합해 보면, AI와 로봇 공학 분야에서 각각 ’추상화 및 제어’와 ’실용성 및 인간 중심 설계’라는 뚜렷한 방향성이 나타난다.</p>
<ul>
<li>
<p><strong>AI: 추상화와 제어 가능성을 향한 도약:</strong> AI 분야에서는 레이블 없는 데이터로부터 ’무엇이 다른가’를 넘어 ’무엇이 같은가’를 학습하려는 **의미론적 구조화(PCL)**와, 2D 픽셀 생성을 넘어 3D 세계의 구성요소를 이해하고 독립적으로 제어하려는 **구성적 생성(GIRAFFE)**이라는 두 가지 큰 흐름이 관측되었다. 이는 AI가 저수준 패턴 인식을 넘어 고수준의 추상적 표현과 제어 가능한 세계 모델을 구축하는 방향으로 심화되고 있음을 의미한다.</p>
</li>
<li>
<p><strong>로봇 공학: 실용성과 인간 중심 설계의 확산:</strong> 로봇 공학 분야에서는 알고리즘의 정교화와 더불어, 실제 환경, 특히 인간과의 상호작용 환경에서 로봇이 효과적으로 사용되기 위한 **소재 및 설계의 혁신(ffPAM)**이 핵심적인 연구 주제로 부상했다. 이는 로봇 기술이 실험실을 벗어나 서비스, 의료, 재활 등 인간의 삶에 직접적인 영향을 미치는 단계로 진입하고 있음을 보여준다.34</p>
</li>
</ul>
<p>이러한 연구들은 미래 기술 및 산업에 중대한 영향을 미칠 것으로 전망된다. PCL과 같은 고품질 비지도 표현 학습 기술은 대규모 데이터는 존재하지만 레이블링 비용이 막대한 의료, 금융, 자율주행 분야에서 AI 모델의 성능과 적용 범위를 획기적으로 확장할 잠재력을 가진다. GIRAFFE로 대표되는 제어 가능한 3D 생성 모델은 메타버스, 디지털 트윈, 가상현실(VR/AR) 콘텐츠 제작의 패러다임을 바꿀 것이며, 현실과 유사한 시뮬레이션 환경을 자동으로 구축하여 강화학습 에이전트의 훈련을 가속화하는 데에도 핵심적인 역할을 할 것이다. 마지막으로, ffPAM과 같은 혁신적인 소프트 액추에이터는 서비스 로봇 시장의 성장을 견인할 핵심 기술이 될 것이다.34 특히 고령화 사회에 대응하기 위한 재활 및 보조 로봇, 일상 생활 지원 로봇, 그리고 안전한 산업용 협업 로봇 분야에서 그 활용이 폭발적으로 증가할 것으로 예상된다.</p>
<p>결론적으로, 2021년 5월의 주요 연구들은 AI가 데이터의 깊은 의미를 이해하고 가상 세계를 창조하는 능력의 지평을 넓히는 동시에, 로봇 공학은 그 지능을 인간 친화적인 물리적 형태로 구현하여 현실 세계의 문제를 해결하는 방향으로 나아가고 있음을 명확히 보여주었다. 이러한 두 분야의 상호 발전은 향후 수년 간 기술 혁신의 가장 강력한 동력이 될 것이다.</p>
<h2>5. 참고 자료</h2>
<ol>
<li>2021년 DNA 분야별 국내 디지털 혁신 전망 - 소프트웨어정책연구소, <a href="https://spri.kr/posts/view/23140?code=data_all&amp;study_type&amp;board_type=industry_trend">https://spri.kr/posts/view/23140?code=data_all&amp;study_type=&amp;board_type=industry_trend</a></li>
<li>의결하였다. 세 가지 핵심 기술은 AI-반도체, 첨단바이오, 퀀텀 - 소프트웨어정책연구소, https://spri.kr/posts?code=industry_trend</li>
<li>2021 Conference - ICLR 2026, https://iclr.cc/Conferences/2021</li>
<li>[ICLR 2021] 1편: 현실 문제를 풀기 위한 강화학습 연구 - LG AI …, https://www.lgresearch.ai/blog/view?seq=118</li>
<li>Prototypical Contrastive Learning of Unsupervised Representations | Request PDF, https://www.researchgate.net/publication/341310757_Prototypical_Contrastive_Learning_of_Unsupervised_Representations</li>
<li>Contrastive Representation Learning | Lil’Log, https://lilianweng.github.io/posts/2021-05-31-contrastive/</li>
<li>PCL - Self Supervised Learning 논문 리뷰 - FFighting, https://ffighting.net/deep-learning-paper-review/self-supervised-learning/pcl/</li>
<li>Prototypical contrastive learning of unsupervised representations - InK@SMU.edu.sg, https://ink.library.smu.edu.sg/context/sis_research/article/9996/viewcontent/2021_ICLR_PCL.pdf</li>
<li>Prototypical Graph Contrastive Learning, <a href="https://www.atailab.cn/seminar2022Fall/pdf/2022_IEEE_Prototypical%20Graph%20Contrastive%20Learning.pdf">https://www.atailab.cn/seminar2022Fall/pdf/2022_IEEE_Prototypical%20Graph%20Contrastive%20Learning.pdf</a></li>
<li>Prototypical Contrastive Learning of Unsupervised … - SciSpace, https://scispace.com/pdf/prototypical-contrastive-learning-of-unsupervised-2xvu9u59l8.pdf</li>
<li>[2005.04966] Prototypical Contrastive Learning of Unsupervised Representations - ar5iv, https://ar5iv.labs.arxiv.org/html/2005.04966</li>
<li>Prototypical Contrastive Learning of Unsupervised Representations - OpenReview, https://openreview.net/forum?id=KmykpuSrjcq</li>
<li>[PDF] Rethinking Prototypical Contrastive Learning through Alignment, Uniformity and Correlation | Semantic Scholar, https://www.semanticscholar.org/paper/18312bd0309c836e0b69fe80c34789e8a9a2ae8d</li>
<li>Learning Label Hierarchy with Supervised Contrastive Learning - ACL Anthology, https://aclanthology.org/2024.findings-eacl.108.pdf</li>
<li>CVPR 2021: An Overview | Yassine - Blog | Yassine, https://yassouali.github.io/ml-blog/cvpr2021/</li>
<li>GIRAFFE: Representing Scenes as Compositional Generative …, https://m-niemeyer.github.io/project-pages/giraffe/</li>
<li>GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields - Stanford Computer Graphics Laboratory, <a href="https://graphics.stanford.edu/courses/cs348n-22-winter/LectureSlides/FinalSlides/GIRAFFE_%20Representing%20Scenes%20as%20Compositional%20Generative%20Neural%20Feature%20Fields.pdf">https://graphics.stanford.edu/courses/cs348n-22-winter/LectureSlides/FinalSlides/GIRAFFE_%20Representing%20Scenes%20as%20Compositional%20Generative%20Neural%20Feature%20Fields.pdf</a></li>
<li>GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields, http://cyvy.de/uploads/news/attachment/419/GIRAFFE2021CVPR.pdf</li>
<li>GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields - arXiv, https://arxiv.org/abs/2011.12100</li>
<li>Generative Neural Scene Representations for 3D-Aware Image Synthesis - Yen-Chen Lin, https://yenchenlin.me/3D-representation-reading/assets/Michael.pdf</li>
<li>GIRAFFE HD: A High-Resolution 3D-Aware Generative Model - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Xue_GIRAFFE_HD_A_High-Resolution_3D-Aware_Generative_Model_CVPR_2022_paper.pdf</li>
<li>Volume Rendering Digest (for NeRF), https://theialab.ca/pubs/tagliasacchi2022volume.pdf</li>
<li>NeRF: Neural Radiance Fields - Matthew Tancik, https://www.matthewtancik.com/nerf</li>
<li>Neural Rendering and Its Hardware Acceleration: A Review - arXiv, https://arxiv.org/html/2402.00028v1</li>
<li>dectrfov/ICRA2021PaperList: ICRA 2021 paper list - GitHub, https://github.com/dectrfov/ICRA2021PaperList</li>
<li>ICRA 2020 논문을 통해 본 로봇분야 연구동향 - JKSPE, https://jkspe.kspe.or.kr/xml/29080/29080.pdf</li>
<li>IEEE International Conference on Robotics and Automation, ICRA 2021, Xi’an, China, May 30 - June 5, 2021 - researchr publication, https://researchr.org/publication/icra-2021</li>
<li>ICRA(IEEE International Conference on - Robotics and Automation) - 제어로봇시스템학회, <a href="http://icros.org/Newsletter/202207/8.%ED%95%99%EC%88%A0%ED%96%89%EC%82%AC%EC%B0%B8%EA%B4%80%EA%B8%B0_%EC%9C%A4%EC%A0%95%EC%9B%90.pdf">http://icros.org/Newsletter/202207/8.%ED%95%99%EC%88%A0%ED%96%89%EC%82%AC%EC%B0%B8%EA%B4%80%EA%B8%B0_%EC%9C%A4%EC%A0%95%EC%9B%90.pdf</a></li>
<li>Compact Flat Fabric Pneumatic Artificial Muscle (ffPAM) for Soft Wearable Robotic Devices, https://www.researchgate.net/publication/349600093_Compact_Flat_Fabric_Pneumatic_Artificial_Muscle_ffPAM_for_Soft_Wearable_Robotic_Devices</li>
<li>A soft wearable robotic device for active knee motions using flat pneumatic artificial muscles, https://www.researchgate.net/publication/286679960_A_soft_wearable_robotic_device_for_active_knee_motions_using_flat_pneumatic_artificial_muscles</li>
<li>김우종 박사과정 학생, IEEE ICRA 2021 Best … - KAIST 기계공학과, https://me.kaist.ac.kr/news/news_020100.html?bmain=view&amp;uid=327</li>
<li>X-crossing pneumatic artificial muscles - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC10511197/</li>
<li>[IEEE RA-L &amp; ICRA 2021] [Best Paper Award] Compact Flat Fabric …, https://www.youtube.com/watch?v=CH12lWJOkiw</li>
<li>서비스 로봇, <a href="https://repository.kisti.re.kr/bitstream/10580/17886/3/ASTI%20MARKET%20INSIGHT%20032(0712).pdf">https://repository.kisti.re.kr/bitstream/10580/17886/3/ASTI%20MARKET%20INSIGHT%20032%280712%29.pdf</a></li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>