<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2021년 10월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2021년 10월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2021년 AI 및 로봇 연구 동향</a> / <span>2021년 10월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2021년 10월 AI 및 로봇 연구 동향</h1>
<h2>1. 서론</h2>
<p>2021년 하반기는 인공지능(AI) 분야가 기존 모델의 한계를 명확히 인식하고, 효율성, 일반화 성능, 그리고 확장성에 대한 근본적인 해결책을 모색하던 중요한 시점이었다. 이 시기는 단순히 모델의 크기를 키우는 스케일링 경쟁에서 벗어나, 아키텍처 자체의 혁신을 통해 성능과 효율을 동시에 달성하려는 패러다임 전환의 서막을 열었다.1 AI 기술이 다양한 산업 분야로 확산됨에 따라, 특정 작업에 과도하게 특화된 모델의 비효율성과 실제 세계의 복잡성을 다루지 못하는 한계가 부각되었다. 이에 따라 연구 커뮤니티는 보다 범용적이고, 에너지 효율적이며, 다양한 데이터를 통합적으로 이해할 수 있는 차세대 AI 시스템에 대한 고민을 시작했다.3</p>
<p>본 보고서는 2021년 10월에 발표된 세 가지 핵심적인 기술적 돌파구를 중심으로 AI 및 로봇 분야의 연구 동향을 심층 분석한다. 첫째, 컴퓨터 비전 분야의 최고 학회인 ICCV 2021에서 발표된 주요 연구를 통해 Transformer 아키텍처가 어떻게 기존의 한계를 극복하고 컴퓨터 비전 분야의 범용 백본(general-purpose backbone)으로 자리매김하게 되었는지를 분석한다. 둘째, 로봇 학습 컨퍼런스인 CoRL 2021의 성과를 통해 물리적 세계와의 복잡한 상호작용, 특히 모델링이 어려운 변형 가능 객체 조작 문제에서 동적 제어(dynamic control)와 자기 지도 학습(self-supervised learning)이 어떠한 가능성을 입증했는지 탐구한다. 마지막으로, 기존 AI 모델의 근본적인 한계(단일 작업, 단일 모달리티, 밀집 연산)를 극복하기 위한 새로운 아키텍처 비전을 제시한 구글의 Pathways를 조명한다.</p>
<p>각 장에서는 주요 연구의 기술적 배경, 핵심 방법론, 실험적 성과를 상세히 기술하고, 이들이 갖는 학술적, 산업적 함의를 다각도로 분석할 것이다. 이를 통해 2021년 10월이 AI 기술 발전에 있어 어떠한 변곡점이었는지를 명확히 제시하는 것을 목표로 한다. 이 시기의 연구들은 현재 AI 기술의 근간을 이루는 핵심 아이디어들의 출발점으로서, 그 중요성은 아무리 강조해도 지나치지 않다.</p>
<h2>2.  컴퓨터 비전의 새로운 지평: ICCV 2021 주요 연구</h2>
<p>2021년 10월에 개최된 컴퓨터 비전 분야 최고 권위의 학회인 ICCV (International Conference on Computer Vision)에서는 분야의 패러다임을 바꿀 만한 혁신적인 연구들이 다수 발표되었다. 특히 Vision Transformer의 실용성을 획기적으로 개선한 Swin Transformer와 신경망 렌더링의 품질을 한 단계 끌어올린 Mip-NeRF는 각각 최고 논문상과 Honorable Mention을 수상하며 학계의 큰 주목을 받았다.5 이 두 연구는 Transformer 아키텍처의 비전 분야 적용 확대와 3D 비전의 사실성 제고라는 두 가지 중요한 흐름을 상징하는 기념비적인 성과로 평가된다.</p>
<h3>2.1  Swin Transformer: 계층적 구조와 이동 윈도우를 통한 Vision Transformer의 혁신</h3>
<p><strong>연구 배경: Vision Transformer(ViT)의 등장과 그 한계</strong></p>
<p>컴퓨터 비전 분야는 오랜 기간 CNN(Convolutional Neural Networks)이 지배적인 아키텍처로 자리 잡고 있었다. AlexNet의 등장 이후, CNN은 더 깊고, 더 넓고, 더 정교한 형태로 발전하며 이미지 인식 분야의 발전을 견인해왔다.6 그러나 자연어 처리(NLP) 분야에서 압도적인 성공을 거둔 Transformer 아키텍처가 ViT(Vision Transformer)라는 이름으로 이미지 인식 문제에 적용되면서 새로운 가능성이 열렸다.7 ViT는 이미지를 여러 개의 패치(patch)로 분할하고 이를 시퀀스 데이터처럼 처리하여, CNN의 지역적 수용장(local receptive field)이라는 제약에서 벗어나 이미지 전역의 장거리 의존성(long-range dependency)을 효과적으로 모델링할 수 있음을 보여주었다.</p>
<p>하지만 ViT는 범용 비전 백본으로 사용되기에는 몇 가지 명백한 한계를 가지고 있었다. 첫째, ViT는 입력 이미지의 해상도에 관계없이 항상 고정된 해상도의 특징 맵만을 생성했다. 이는 다양한 스케일의 객체를 다루어야 하는 객체 탐지(object detection)나 픽셀 단위의 조밀한 예측(dense prediction)이 필요한 시맨틱 분할(semantic segmentation)과 같은 작업에 적용하기 어렵게 만들었다.6 둘째, ViT의 핵심인 자기 어텐션(self-attention) 메커니즘은 연산량이 이미지 패치 수의 제곱에 비례하는 <span class="math math-inline">O(N^2)</span>의 연산 복잡도를 가졌다.10 이로 인해 고해상도 이미지를 처리할 때 엄청난 계산 비용과 메모리가 요구되었고, 이는 실용적인 적용에 있어 근본적인 제약으로 작용했다.</p>
<p><strong>Swin Transformer의 핵심 방법론</strong></p>
<p>Swin Transformer는 ViT의 이러한 한계를 극복하고 Transformer를 범용 비전 백본으로 활용하기 위해 두 가지 핵심적인 아이디어를 제안했다. 바로 ’계층적 특징 맵’과 ’이동 윈도우 기반 자기 어텐션’이다. 이 접근법은 CNN의 장점인 계층적 구조와 지역성(locality)을 Transformer에 효과적으로 융합시킨 것으로 평가받는다.</p>
<ul>
<li>
<p><strong>계층적 특징 맵 (Hierarchical Feature Maps):</strong> Swin Transformer는 CNN과 유사하게 계층적인 구조를 채택했다. 먼저 입력 이미지를 작은 크기(예: 4x4 픽셀)의 겹치지 않는 패치로 분할한다. 이후 모델의 깊이가 깊어짐에 따라 ‘패치 병합(Patch Merging)’ 레이어를 통해 인접한 2x2 패치 그룹을 하나의 패치로 병합한다.10 이 과정에서 특징 맵의 해상도는 절반으로 줄어들고 채널 수는 두 배로 늘어난다. 이러한 계층적 구조는 다양한 스케일의 특징을 추출할 수 있게 하여, FPN(Feature Pyramid Networks)이나 U-Net과 같은 기존의 조밀한 예측 기법들과 쉽게 결합될 수 있는 유연성을 제공한다.6 이는 고정된 해상도만 처리하던 ViT와의 가장 큰 차별점이다.</p>
</li>
<li>
<p><strong>이동 윈도우 기반 자기 어텐션 (Shifted Window based Self-Attention, W-MSA &amp; SW-MSA):</strong> ViT의 이차 복잡도 문제를 해결하기 위해, Swin Transformer는 전역 어텐션(global attention)을 포기하고 지역적인(local) 어텐션을 도입했다. 먼저 특징 맵을 겹치지 않는 여러 개의 윈도우(window)로 분할하고, 각 윈도우 내부에서만 자기 어텐션을 계산한다(Windowed MSA, W-MSA).6 각 윈도우 내의 패치 수는 고정되어 있으므로, 전체 연산 복잡도는 이미지 크기에 선형적으로 비례하게 된다. 하지만 이 방식은 윈도우 간의 정보 교류를 차단하여 모델의 표현력을 저하시키는 문제를 야기한다. 이 문제를 해결하기 위해 Swin Transformer는 연속된 두 개의 Transformer 블록에서 윈도우 파티션을 이동시키는 ‘이동 윈도우(Shifted Window)’ 기법을 제안했다(Shifted Window MSA, SW-MSA).9 두 번째 블록에서는 윈도우를 윈도우 크기의 절반만큼 이동시켜, 이전 레이어의 윈도우 경계를 넘나드는 새로운 윈도우를 형성한다. 이를 통해 인접 윈도우 간의 정보 교류가 효율적으로 이루어지게 하여, 지역 어텐션의 한계를 극복하면서도 선형적인 연산 복잡도를 유지하는 데 성공했다.</p>
</li>
</ul>
<p><strong>연산 복잡도 분석</strong></p>
<p>Swin Transformer의 효율성은 표준 Multi-head Self-Attention (MSA)과 Windowed MSA (W-MSA)의 연산 복잡도 비교를 통해 명확히 드러난다. 이미지 패치의 총 개수가 <span class="math math-inline">h \times w</span>이고 각 패치의 특징 벡터 차원이 <span class="math math-inline">C</span>일 때, 두 방식의 연산 복잡도는 다음과 같이 계산된다.13</p>
<ul>
<li><strong>표준 MSA의 연산 복잡도:</strong></li>
</ul>
<ol>
<li>
<p>쿼리(Q), 키(K), 밸류(V) 행렬 생성: 각 패치에 대해 <span class="math math-inline">C \times C</span> 차원의 가중치 행렬을 곱하므로, <span class="math math-inline">3 \times (hw) \times C^2</span>의 연산이 필요하다.</p>
</li>
<li>
<p>어텐션 스코어 계산 (<span class="math math-inline">QK^T</span>): <span class="math math-inline">(hw \times C)</span>와 <span class="math math-inline">(C \times hw)</span> 행렬의 곱이므로, <span class="math math-inline">(hw)^2 C</span>의 연산이 필요하다.</p>
</li>
<li>
<p>어텐션 가중치 적용: <span class="math math-inline">(hw \times hw)</span> 차원의 어텐션 맵과 <span class="math math-inline">(hw \times C)</span> 차원의 V 행렬을 곱하므로, <span class="math math-inline">(hw)^2 C</span>의 연산이 필요하다.</p>
</li>
<li>
<p>최종 출력 프로젝션: <span class="math math-inline">(hw \times C)</span> 결과에 <span class="math math-inline">C \times C</span> 차원의 가중치 행렬을 곱하므로, <span class="math math-inline">(hw)C^2</span>의 연산이 필요하다.</p>
</li>
</ol>
<p>이를 모두 합산한 최종 연산 복잡도는 다음과 같다.</p>
<p><span class="math math-display">
  \Omega(\text{MSA}) = 4hwC^2 + 2(hw)^2C
</span></p>
<ul>
<li>W-MSA의 연산 복잡도:</li>
</ul>
<p><span class="math math-inline">M \times M</span> 크기의 윈도우 내부에서 MSA를 계산한다. 각 윈도우 내 패치 수는 <span class="math math-inline">M^2</span>이다.</p>
<ol>
<li>
<p>단일 윈도우 내 MSA 연산 복잡도는 위 식에서 <span class="math math-inline">hw</span>를 <span class="math math-inline">M^2</span>으로 대체하여 <span class="math math-inline">4M^2C^2 + 2(M^2)^2C = 4M^2C^2 + 2M^4C</span>가 된다.</p>
</li>
<li>
<p>전체 특징 맵에는 총 <span class="math math-inline">(hw/M^2)</span>개의 윈도우가 존재한다.</p>
</li>
</ol>
<p>따라서 전체 연산 복잡도는 단일 윈도우의 복잡도에 윈도우 개수를 곱한 값이다.</p>
<pre><code>$$
\Omega(\text{W-MSA}) = \frac{hw}{M^2} \times (4M^2C^2 + 2M^4C) = 4hwC^2 + 2M^2hwC
</code></pre>
<p>$$</p>
<p>분석 결과, 표준 MSA의 복잡도는 패치 수 <span class="math math-inline">hw</span>에 대해 이차(quadratic) 함수인 반면, W-MSA의 복잡도는 <span class="math math-inline">M</span>이 고정된 상수일 때 <span class="math math-inline">hw</span>에 대해 선형(linear) 함수임을 명확히 알 수 있다. 이 선형 복잡도 덕분에 Swin Transformer는 고해상도 이미지를 효율적으로 처리할 수 있게 되었다.</p>
<p><strong>실험 결과 및 의의</strong></p>
<p>Swin Transformer는 ImageNet-1K 이미지 분류, COCO 객체 탐지, ADE20K 시맨틱 분할 등 주요 컴퓨터 비전 벤치마크에서 당시의 최고 성능(SOTA) 모델들을 큰 차이로 능가하는 압도적인 성능을 보였다.6 특히 COCO 객체 탐지에서는 기존 SOTA 대비 +2.7 box AP, ADE20K 시맨틱 분할에서는 +3.2 mIoU의 성능 향상을 기록했다.12 이러한 성과를 인정받아 ICCV 2021에서 최고 논문상인 Marr Prize를 수상했으며 5, Transformer 아키텍처가 CNN을 대체할 수 있는 강력한 범용 비전 백본이 될 수 있음을 명확히 입증했다.</p>
<p>Swin Transformer의 성공은 단순히 ViT의 연산량 문제를 해결한 것을 넘어선다. 이는 CNN의 핵심 설계 원리인 ’계층적 구조’와 ’지역성’을 Transformer 아키텍처에 성공적으로 융합시킨 ’하이브리드 패러다임’의 승리였다. ViT가 Transformer의 전역적 관계 모델링 능력을 비전 분야에 도입했지만, CNN이 수십 년간 발전시켜 온 ’지역적 상관관계’와 ’스케일 불변성’이라는 강력한 귀납적 편향(inductive bias)을 포기한 대가로 대규모 데이터셋에 대한 의존성과 조밀한 예측 작업에서의 비효율성을 감수해야 했다. Swin Transformer는 ’윈도우’라는 개념으로 지역성을, ’패치 병합’이라는 개념으로 계층 구조를 다시 도입했다. 그리고 ’이동 윈도우’는 지역 연산의 한계를 극복하면서도 전역 어텐션의 비효율성을 피하는 절묘한 공학적 해법이었다. 결론적으로, Swin Transformer는 Transformer의 표현력과 CNN의 효율성 및 구조적 장점을 모두 취함으로써, ’어느 한 쪽이 우월하다’는 논쟁을 종식시키고 두 아키텍처의 철학을 통합하는 것이 실용적인 해결책임을 증명했다. 이는 향후 비전 아키텍처 연구가 두 패러다임의 장점을 결합하는 방향으로 나아갈 것임을 강력하게 시사한 사건이었다.</p>
<table><thead><tr><th>모델</th><th>파라미터 수 (M)</th><th>FLOPs (G)</th><th>ImageNet-1K Top-1 Acc. (%)</th><th>COCO test-dev Box AP</th><th>COCO test-dev Mask AP</th><th>ADE20K val mIoU</th></tr></thead><tbody>
<tr><td>ResNeXt-101</td><td>89</td><td>16.5</td><td>82.3</td><td>48.1</td><td>41.5</td><td>46.1</td></tr>
<tr><td>DeiT-B</td><td>86</td><td>17.5</td><td>83.1</td><td>51.2</td><td>45.0</td><td>49.8</td></tr>
<tr><td>ViT-B/16</td><td>86</td><td>17.6</td><td>84.2</td><td>-</td><td>-</td><td>-</td></tr>
<tr><td><strong>Swin-T</strong></td><td><strong>29</strong></td><td><strong>4.5</strong></td><td><strong>81.3</strong></td><td><strong>51.9</strong></td><td><strong>45.0</strong></td><td><strong>47.6</strong></td></tr>
<tr><td><strong>Swin-S</strong></td><td><strong>50</strong></td><td><strong>8.7</strong></td><td><strong>83.0</strong></td><td><strong>54.5</strong></td><td><strong>47.1</strong></td><td><strong>51.8</strong></td></tr>
<tr><td><strong>Swin-B</strong></td><td><strong>88</strong></td><td><strong>15.4</strong></td><td><strong>83.5</strong></td><td><strong>55.5</strong></td><td><strong>47.9</strong></td><td><strong>52.6</strong></td></tr>
</tbody></table>
<h3>2.2  Mip-NeRF: 앨리어싱을 해결한 다중 스케일 신경망 렌더링</h3>
<p><strong>연구 배경: NeRF의 성공과 앨리어싱 문제</strong></p>
<p>NeRF(Neural Radiance Fields)는 다수의 2D 이미지로부터 3D 장면을 학습하여 새로운 시점의 이미지를 매우 사실적으로 렌더링하는 획기적인 방법을 제시했다.15 NeRF는 3D 공간상의 좌표와 시선 방향을 입력받아 해당 지점의 밀도(density)와 색상(color)을 출력하는 MLP(Multi-Layer Perceptron)를 사용하여 장면을 연속적인 함수로 표현한다. 이 방식은 복잡한 기하학과 미세한 텍스처를 놀랍도록 정교하게 재현해내며 3D 비전 분야에 큰 반향을 일으켰다.</p>
<p>하지만 NeRF는 렌더링 과정에서 픽셀당 하나의 무한히 얇은 광선(ray)만을 샘플링한다는 근본적인 한계를 가지고 있었다. 이로 인해 학습 이미지와 렌더링할 이미지의 카메라 거리가 달라지는 등, 장면을 다중 해상도(multi-resolution)로 관찰해야 하는 상황에서 심각한 시각적 결함이 발생했다. 가까운 거리에서 렌더링하면 이미지가 과도하게 흐릿해지고(blur), 먼 거리에서 렌더링하면 계단 현상이나 패턴 왜곡과 같은 앨리어싱(aliasing) 아티팩트가 나타났다.17 이는 NeRF의 실용성을 저해하는 주요한 문제점이었다.</p>
<p><strong>Mip-NeRF의 핵심 방법론</strong></p>
<p>Mip-NeRF는 NeRF의 앨리어싱 문제를 해결하기 위해 컴퓨터 그래픽스의 고전적인 안티앨리어싱 기법인 ’밉맵핑(mipmapping)’에서 영감을 얻었다. 핵심 아이디어는 무한히 얇은 광선 대신, 픽셀이 3D 공간에서 차지하는 부피를 가진 ‘원뿔 절두체(conical frustum)’ 영역 전체를 고려하여 렌더링하는 것이다.17</p>
<ul>
<li>
<p><strong>다중 스케일 표현과 원뿔 절두체 (Multiscale Representation &amp; Conical Frustums):</strong> Mip-NeRF는 각 픽셀에 해당하는 광선을 원뿔로 확장하고, 광선을 따라 샘플링하는 각 구간을 원뿔 절두체로 모델링한다. 즉, 단일 점이 아닌 특정 부피를 가진 영역의 정보를 통합하여 렌더링에 사용한다. 이는 카메라와의 거리에 따라 픽셀이 덮는 3D 공간의 크기가 달라지는 것을 자연스럽게 반영하는 방식이다.15 멀리 있는 객체를 볼 때 픽셀은 넓은 영역을 대표하므로 저주파 정보(흐릿한 평균값)를 사용하고, 가까이 있는 객체를 볼 때는 좁은 영역을 대표하므로 고주파 정보(선명한 디테일)를 사용하여 앨리어싱을 방지한다.</p>
</li>
<li>
<p><strong>통합 위치 인코딩 (Integrated Positional Encoding, IPE):</strong> 이 아이디어를 신경망에 적용하기 위해, Mip-NeRF는 ’통합 위치 인코딩(IPE)’이라는 새로운 특징 표현 방식을 제안했다. 기존 NeRF의 위치 인코딩(PE)이 단일 3D 좌표 <span class="math math-inline">\mathbf{x}</span>를 고주파 특징 벡터로 변환했다면, IPE는 원뿔 절두체와 같은 3D 공간상의 ‘영역’ 전체를 인코딩하도록 일반화한 것이다.17 IPE는 먼저 해당 영역을 다변량 가우시안(multivariate Gaussian) 분포로 근사한다. 그 다음, 이 가우시안 분포 내 모든 점에 대한 위치 인코딩(PE)의 기댓값(expected value)을 폐쇄형(closed-form) 수식으로 계산하여 최종 특징 벡터를 생성한다.17</p>
</li>
</ul>
<p><strong>관련 수식 분석</strong></p>
<p>IPE의 수학적 원리는 기존 PE와의 비교를 통해 명확히 이해할 수 있다.</p>
<ul>
<li>
<p><strong>표준 위치 인코딩 (PE):</strong> 3D 좌표 <span class="math math-inline">\mathbf{x}</span>를 <span class="math math-inline">L</span>개의 주파수를 가진 사인, 코사인 함수에 통과시켜 특징 벡터 <span class="math math-inline">\gamma(\mathbf{x})</span>를 생성한다.</p>
<p><span class="math math-display">
\gamma(\mathbf{x}) = (\sin(2^0 \pi \mathbf{x}), \cos(2^0 \pi \mathbf{x}), \dots, \sin(2^{L-1} \pi \mathbf{x}), \cos(2^{L-1} \pi \mathbf{x}))
</span></p>
</li>
<li>
<p><strong>통합 위치 인코딩 (IPE):</strong> 평균 <span class="math math-inline">\mu</span>와 분산 <span class="math math-inline">\sigma^2</span>를 갖는 1차원 가우시안 영역 <span class="math math-inline">\mathcal{N}(\mu, \sigma^2)</span>에 대한 <span class="math math-inline">\gamma(x)</span>의 기댓값을 계산한다.21 각 주파수 성분</p>
</li>
</ul>
<p><span class="math math-inline">l</span>에 대한 기댓값은 다음과 같이 유도된다.</p>
<p><span class="math math-display">
  \mathbb{E}_{x \sim \mathcal{N}(\mu, \sigma^2)}[\sin(2^l \pi x)] = \exp(-\frac{1}{2}(2^l \pi)^2 \sigma^2) \sin(2^l \pi \mu)
</span></p>
<p><span class="math math-display">
  \mathbb{E}_{x \sim \mathcal{N}(\mu, \sigma^2)}[\cos(2^l \pi x)] = \exp(-\frac{1}{2}(2^l \pi)^2 \sigma^2) \cos(2^l \pi \mu)
</span></p>
<p>이 수식은 IPE의 안티앨리어싱 효과를 명확하게 보여준다. 영역의 분산 <span class="math math-inline">\sigma^2</span>이 클수록 (즉, 카메라가 멀리 있어 픽셀이 넓은 영역을 덮을수록), 지수 항 <span class="math math-inline">\exp(-\dots)</span>이 빠르게 0에 가까워진다. 이 효과는 주파수가 높은(즉, <span class="math math-inline">l</span>이 큰) 성분일수록 더욱 두드러진다. 결과적으로, 넓은 영역에 대해서는 고주파 성분이 자연스럽게 0으로 감쇠되어 저주파 신호만 MLP에 입력되고, 이는 앨리어싱을 방지하는 사전 필터링(pre-filtering) 효과를 낳는다. 반대로 영역이 매우 좁아 <span class="math math-inline">\sigma^2 \to 0</span>이 되면, 지수 항은 1이 되어 IPE는 기존 PE와 동일해진다.</p>
<p><strong>실험 결과 및 의의</strong></p>
<p>Mip-NeRF는 실험을 통해 그 효과를 압도적으로 입증했다. 기존 NeRF 데이터셋에서 평균 오류율을 17% 감소시켰고, 다중 스케일 렌더링 문제를 의도적으로 부각시킨 자체 제작 데이터셋에서는 오류율을 무려 60%나 줄였다.15 놀랍게도 이러한 성능 향상은 추가적인 계산 비용 없이 이루어졌으며, 오히려 NeRF보다 7% 더 빠르고 모델 크기는 절반에 불과했다.19 이 성과를 인정받아 Mip-NeRF는 ICCV 2021에서 Best Paper Honorable Mention을 수상했다.5</p>
<p>Mip-NeRF의 등장은 신경망 렌더링 연구가 단순한 ‘블랙박스’ 모델링을 넘어, 샘플링 이론과 신호 처리라는 고전적이고 원칙적인 프레임워크 내에서 분석되고 개선될 수 있음을 보여준 중요한 사건이었다. 초기 NeRF가 “MLP가 어떻게든 3D 장면을 학습한다“는 마법 같은 결과에 집중했다면, Mip-NeRF는 다중 해상도 문제를 ’샘플링 주파수가 나이퀴스트 한계를 초과하는 앨리어싱’이라는 신호 처리의 관점으로 재정의했다. 그리고 그 해결책으로 제시된 원뿔 절두체 렌더링과 IPE는 각각 콘 트레이싱(cone tracing)과 사전 필터링(pre-filtering)이라는 고전 그래픽스 기법의 ’신경망 버전’이었다. 이는 NeRF를 ’신기한 데모’에서 ’원칙에 기반한 렌더링 파이프라인’으로 격상시켰으며, 분야의 성숙도를 한 단계 높이고 후속 연구들이 렌더링의 근본적인 원리를 탐구하도록 방향을 제시했다.</p>
<table><thead><tr><th>평가 지표</th><th>NeRF</th><th>NeRF w/ Supersampling (4x)</th><th><strong>Mip-NeRF</strong></th></tr></thead><tbody>
<tr><td>PSNR (↑)</td><td>26.54</td><td>29.11</td><td><strong>33.07</strong></td></tr>
<tr><td>SSIM (↑)</td><td>0.881</td><td>0.915</td><td><strong>0.961</strong></td></tr>
<tr><td>LPIPS (↓)</td><td>0.201</td><td>0.138</td><td><strong>0.063</strong></td></tr>
</tbody></table>
<h2>3.  로봇 학습의 진화: CoRL 2021 주요 연구</h2>
<p>2021년 10월 말부터 11월 초에 걸쳐 진행된 로봇 학습 컨퍼런스(Conference on Robot Learning, CoRL)에서는 로봇이 복잡하고 비정형적인 실제 환경과 상호작용하는 능력을 한 단계 끌어올리는 중요한 연구들이 발표되었다. 특히, 변형 가능한 객체 조작이라는 오랜 난제를 해결하기 위해 동적 제어와 자기 지도 학습을 결합한 FlingBot 연구는 Best System Paper Award를 수상하며 큰 주목을 받았다.22 이 장에서는 FlingBot을 중심으로 로봇 학습의 최신 동향을 분석하고, 미래 연구 방향을 제시한 Blue Sky 논문들의 함의를 살펴본다.</p>
<h3>3.1  FlingBot: 동적 조작을 통한 변형 가능 객체 제어의 돌파구</h3>
<p><strong>연구 배경: 변형 가능 객체 조작의 어려움</strong></p>
<p>옷, 담요, 밧줄과 같은 변형 가능 객체(deformable objects)는 로봇 조작 분야에서 가장 어려운 문제 중 하나로 여겨진다. 강체(rigid body)와 달리, 이러한 객체들은 무한에 가까운 자유도(degrees of freedom)를 가지며, 그 상태를 정확히 표현하고 예측하기가 매우 어렵다.24 또한, 구겨진 상태에서는 자기 폐색(self-occlusion)이 심각하여 객체의 전체적인 형태나 주요 특징점(예: 옷의 소매 끝)을 파악하기 힘들다.26</p>
<p>이러한 어려움 때문에 기존 연구들은 대부분 ‘준정적(quasi-static)’ 접근법에 의존했다. 이는 로봇이 매우 느리고 정밀한 단일 팔 동작(예: 한 지점을 집어서 다른 곳으로 옮기기)을 반복하여 점진적으로 객체를 조작하는 방식이다.22 이 방식은 제어는 용이하지만, 심하게 구겨진 천을 완전히 펼치기까지 수십, 수백 번의 상호작용이 필요하여 매우 비효율적이다. 더 큰 문제는, 로봇의 물리적 도달 범위(reach range)보다 큰 객체는 원천적으로 다룰 수 없다는 명백한 한계가 있었다.24</p>
<p><strong>FlingBot의 핵심 방법론</strong></p>
<p>FlingBot은 이러한 준정적 접근법의 한계를 정면으로 돌파하기 위해, 인간이 구겨진 천을 펼 때 본능적으로 사용하는 ‘동적(dynamic)’ 행위에 주목했다.</p>
<ul>
<li>
<p><strong>동적 내던지기 (Dynamic Flinging):</strong> FlingBot은 인간이 양손으로 천의 양 끝을 잡고 한 번에 ’휙’하고 내던져서 펼치는 행위에서 영감을 얻었다. 이를 모방하여 양팔 로봇을 위한 ’집기-늘리기-내던지기(pick-stretch-fling)’라는 기본 동작(primitive)을 정의했다.22 이 고속의 동적 동작은 천의 질량과 관성을 적극적으로 활용하여, 단 한 번의 상호작용으로도 천을 넓게 펼칠 수 있게 한다. 이는 로봇의 도달 범위를 효과적으로 확장시켜, 팔이 닿지 않는 곳까지 천을 펼쳐 놓는 것을 가능하게 한다.24</p>
</li>
<li>
<p><strong>자기 지도 학습 프레임워크 (Self-supervised Learning Framework):</strong> 천의 복잡한 동역학을 사전에 정확히 모델링하는 것은 거의 불가능하다. FlingBot은 이 문제를 ’학습’을 통해 해결했다. 전문가의 시연 데이터나 정답 레이블 없이, 로봇이 스스로 수많은 시행착오를 통해 최적의 행동을 학습하는 자기 지도 학습 프레임워크를 구축했다.28</p>
</li>
<li>
<p><strong>보상 신호 (Supervision Signal):</strong> 학습의 목표는 ’천을 최대한 넓게 펼치는 것’으로 설정되었다. 이를 위해 시스템은 행동을 수행하기 전과 후의 천이 작업 공간에서 차지하는 면적(coverage)을 상단에 설치된 RGB 카메라로 촬영하여 비교한다. 이 면적의 ’변화량(delta-coverage)’이 바로 보상 신호(reward signal)가 된다.26 즉, 로봇은 자신의 행동이 천을 더 넓게 펼치는 데 기여했는지를 스스로 평가하고, 더 큰 보상을 주는 행동을 강화하는 방향으로 학습한다.</p>
</li>
<li>
<p><strong>가치 네트워크 (Value Network):</strong> FlingBot의 정책은 현재의 시각적 관측(카메라 이미지)을 입력으로 받아, 가능한 모든 양팔 파지 지점(grasp points) 조합에 대한 가치(예상되는 커버리지 증가량)를 예측하는 가치 네트워크(value network)로 구현된다.26 이 네트워크는 완전한 컨볼루션 신경망(FCN) 형태로, 입력 이미지와 동일한 해상도의 가치 맵을 출력한다. 정책은 이 가치 맵에서 가장 높은 값을 갖는 행동을 탐욕적으로(greedily) 선택하여 수행한다. 이 모든 과정은 시뮬레이션 환경에서 수만 번의 상호작용을 통해 자동으로 학습된다.</p>
</li>
</ul>
<p><strong>실험 결과 및 의의</strong></p>
<p>FlingBot의 성능은 압도적이었다. 시뮬레이션에서 처음 보는 직사각형 천을 단 3번의 행동으로 평균 80% 이상 펼쳤으며, 이는 준정적 방식보다 2배 이상 높은 효율이다.26 특히 로봇 팔의 도달 범위를 넘어서는 큰 천도 성공적으로 다루었고, 직사각형 천으로만 학습했음에도 불구하고 실제 티셔츠에도 추가 학습 없이 성공적으로 일반화되는 놀라운 능력을 보였다.22 실제 로봇을 이용한 실험에서는 준정적 Pick &amp; Place 방식보다 4배 이상 높은 커버리지 증가율을 기록하며, 시뮬레이션에서 학습된 정책의 현실 적용 가능성을 입증했다.22 이러한 성과를 바탕으로 FlingBot은 CoRL 2021에서 Best System Paper Award를 수상했다.23</p>
<p>FlingBot의 성공은 로봇 학습 분야에 중요한 시사점을 던진다. 이는 로봇이 ’정밀한 모델링’의 한계를 ’효과적인 상호작용 학습’으로 넘어설 수 있음을 보여준 대표적인 사례다. 천의 복잡한 물리 현상을 완벽하게 시뮬레이션하고 예측하려는 고전적 접근 대신, ’천을 넓게 편다’는 명확한 목표를 달성하기 위한 ‘충분히 좋은’ 동적 전략을 데이터로부터 직접 배우는 것이 더 실용적이고 강력할 수 있다는 새로운 방향을 제시한 것이다. 이는 ’이해하고 제어한다(understand and control)’는 전통적인 로봇 공학 패러다임에서 ’상호작용하고 학습한다(interact and learn)’는 새로운 패러다임으로의 전환을 상징한다. 특히 FlingBot이 보상 함수를 ’커버리지 증가’라는 매우 단순하고 측정 가능한 지표로 설정한 것은, 복잡한 물리 모델 없이도 시스템이 스스로 유용한 행동을 발견하도록 유도하는 학습 기반 접근법의 정수를 보여준다. 이는 모델링이 극도로 어려운 실제 세계의 문제들을 해결하는 데 있어 중요한 철학적 전환을 의미한다.</p>
<table><thead><tr><th>평가 지표</th><th>준정적 Pick &amp; Place Baseline</th><th><strong>FlingBot</strong></th></tr></thead><tbody>
<tr><td>1회 행동 후 평균 커버리지 (%)</td><td>18.2</td><td><strong>45.7</strong></td></tr>
<tr><td>3회 행동 후 평균 커버리지 (%)</td><td>35.1</td><td><strong>82.3</strong></td></tr>
<tr><td>80% 커버리지 도달까지 평균 행동 횟수</td><td>&gt; 10</td><td><strong>2.8</strong></td></tr>
</tbody></table>
<h3>3.2  미래 로봇 학습을 위한 청사진: Blue Sky 논문 분석</h3>
<p>CoRL 2021에서는 당장의 성능 개선을 넘어 미래 연구 방향을 제시하는 ‘Blue Sky’ 트랙의 논문들도 주목받았다.23 이 논문들은 로봇 학습이 나아가야 할 장기적인 비전을 제시했다는 점에서 의미가 크다.</p>
<ul>
<li>
<p><strong>분산된 로봇 데이터 공유 및 가치 평가:</strong> 다수의 로봇(예: 자율주행차 군집, 가정용 로봇)이 각자 수집한 방대한 데이터를 중앙 서버 없이, 프라이버시를 보호하면서 효율적으로 공유하고 거래하는 탈중앙화 프레임워크를 제안했다.30 특히, 한 로봇에게는 흔한 데이터가 다른 환경의 로봇에게는 매우 희귀하고 중요한 ‘분포 외(out-of-distribution)’ 데이터일 수 있다. 이 시스템은 각 로봇이 다른 로봇에게 가치가 있을 만한 데이터를 스스로 예측하고 우선적으로 공유하도록 하여, 전체 로봇 군집의 강건성(robustness)을 집단적으로 향상시키는 것을 목표로 한다.</p>
</li>
<li>
<p><strong>인과적 그래픽 모델의 활용:</strong> 현재의 로봇 학습은 대부분 데이터의 ’상관관계’에 기반한다. 이로 인해 학습 환경과 조금이라도 다른 새로운 상황에 직면했을 때 예측하지 못한 오류를 범하기 쉽다. 이에 대한 해결책으로, 로봇이 단순히 현상을 모방하는 것을 넘어 환경의 ’인과 관계’를 명시적으로 이해하고 추론할 수 있도록 ’인과적 그래픽 모델(causal graphical models)’을 도입할 것을 제안했다.30 이를 통해 로봇은 자신의 행동이 어떤 결과를 초래할지 더 깊이 예측하고, 전례 없는 상황에서도 더 합리적인 의사결정을 내릴 수 있게 될 것이다.</p>
</li>
</ul>
<h2>4.  차세대 AI 아키텍처의 서막</h2>
<p>2021년 10월, 구글은 특정 연구 결과 발표를 넘어, 미래 AI 시스템이 나아가야 할 방향에 대한 거시적인 비전과 아키텍처 철학을 담은 ’Pathways’를 공개했다.2 이는 당시 AI 모델들이 직면한 근본적인 한계를 극복하고, 더 일반적이고 효율적인 지능을 구현하기 위한 청사진을 제시했다는 점에서 학계와 산업계에 큰 파장을 일으켰다.</p>
<h3>4.1  Google Pathways: 멀티태스크, 멀티모달, 희소 활성화의 비전</h3>
<p><strong>등장 배경: 기존 AI 모델의 근본적 한계</strong></p>
<p>Pathways가 제안될 당시의 AI 모델들은 놀라운 성능을 보여주었지만, 세 가지 근본적인 한계를 안고 있었다.2</p>
<ol>
<li>
<p><strong>단일 작업 (Single-Task):</strong> 대부분의 모델은 이미지 분류, 기계 번역 등 오직 하나의 특정 작업을 수행하도록 개별적으로 훈련되었다. 이는 수많은 모델을 각각 개발하고 유지해야 하는 비효율성을 낳았고, 여러 작업 간에 지식이 공유되거나 전이되기 어려웠다.</p>
</li>
<li>
<p><strong>단일 모달리티 (Single-Modality):</strong> 모델들은 텍스트, 이미지, 음성 등 한 가지 종류의 데이터(modality)만을 처리하도록 설계되었다. 이는 인간이 여러 감각을 통해 세상을 종합적으로 이해하는 방식과는 거리가 멀었다.</p>
</li>
<li>
<p><strong>밀집 활성화 (Dense Activation):</strong> 모델의 모든 파라미터(뉴런)가 입력 데이터의 종류나 난이도에 관계없이 모든 계산에 참여하는 ‘밀집(dense)’ 구조를 가졌다. 이는 매우 간단한 작업을 처리할 때도 거대한 네트워크 전체를 활성화시켜야 하므로 막대한 에너지 비효율을 초래했다.</p>
</li>
</ol>
<p>구글은 이러한 한계들이 AI가 인류가 직면한 기후 변화, 질병과 같은 복잡하고 다면적인 문제를 해결하는 데 걸림돌이 된다고 진단하고, 이를 총체적으로 극복하기 위한 새로운 아키텍처로 Pathways를 제안했다.2</p>
<p><strong>Pathways의 세 가지 핵심 비전</strong></p>
<p>Pathways는 기존 모델의 한계를 극복하기 위해 세 가지 핵심적인 비전을 제시했다. 이는 인간의 뇌가 정보를 처리하는 방식에서 영감을 얻은 것이다.31</p>
<ul>
<li>
<p><strong>멀티태스크 (Multitask):</strong> Pathways는 수천, 수백만 개의 다양한 작업을 동시에 학습하고 수행할 수 있는 거대한 단일 모델을 지향한다.2 이는 마치 뇌의 여러 영역이 다양한 능력을 갖추고 필요에 따라 협력하는 것과 같다. 이 단일 모델은 여러 작업에서 얻은 지식을 상호 활용하여 새로운 작업을 더 빠르고 효율적으로 학습할 수 있다(few-shot learning). 예를 들어, 이미지를 이해하는 능력을 학습한 모델은 지도를 읽는 것과 같은 공간 추론 작업을 더 쉽게 배울 수 있다.</p>
</li>
<li>
<p><strong>멀티모달 (Multimodal):</strong> Pathways는 텍스트, 이미지, 음성 등 여러 종류의 데이터를 동시에 이해하고 처리하는 것을 목표로 한다.2 예를 들어, 모델이 ’표범’이라는 단어를 읽거나, 누군가 ’표범’이라고 말하는 소리를 듣거나, 표범이 달리는 영상을 볼 때, 모델 내부에서는 이 모든 다른 형태의 입력이 ’표범’이라는 동일한 개념(concept of a leopard)을 활성화시킨다. 이를 통해 모델은 더 깊고 통합적인 이해가 가능해지며, 편향이나 실수에 덜 취약해진다.</p>
</li>
<li>
<p><strong>희소 활성화 (Sparse Activation):</strong> Pathways의 가장 혁신적인 아이디어는 ’희소 활성화’이다. 거대한 단일 모델을 구축하되, 특정 작업이 주어졌을 때 전체 네트워크가 아닌, 해당 작업과 가장 관련성이 높은 일부 경로(pathway)만 선택적으로 활성화하여 계산을 수행한다.2 이는 인간의 뇌가 특정 과제(예: 글쓰기, 운동)를 수행할 때 관련 영역만 활성화되는 것과 유사한 원리다. 모델은 어떤 경로가 어떤 작업에 가장 적합한지를 동적으로 학습한다. 이를 통해 모델의 잠재적 용량(capacity)은 극대화하면서도, 실제 연산 속도와 에너지 효율은 획기적으로 개선할 수 있다.31</p>
</li>
</ul>
<p><strong>아키텍처 구성 요소 및 구현</strong></p>
<p>Pathways는 단순한 모델이 아닌, 대규모 분산 학습을 위한 전체 시스템이다. 이 시스템은 수천에서 수만 개의 가속기(TPU)를 효율적으로 관리하고 작업을 조율하도록 설계되었다.32 주요 구성 요소는 다음과 같다.</p>
<ul>
<li>
<p><strong>Pathways Resource Manager:</strong> 전체 가속기 리소스를 관리하고 사용자 작업에 할당을 조율하는 중앙 제어부.</p>
</li>
<li>
<p><strong>Pathways Client:</strong> 사용자 프로그램(예: JAX 코드)의 진입점 역할을 하며, 리소스 관리자와 협력하여 컴파일된 프로그램을 어디서 실행할지 결정.</p>
</li>
<li>
<p><strong>Pathways Worker:</strong> 가속기 머신(TPU VM)에서 실행되는 프로세스로, 실제 계산을 수행.</p>
</li>
</ul>
<p>희소 활성화 개념은 GShard, Switch Transformer와 같은 구글의 선행 연구에서 이미 그 가능성을 입증했다. 이 모델들은 전문가 혼합(Mixture-of-Experts, MoE) 구조를 통해 입력에 따라 다른 전문가(부분 신경망)를 활성화하는 방식으로 희소성을 구현했다. 실험 결과, 이 모델들은 유사한 크기의 밀집 모델 대비 1/10 미만의 에너지를 소비하면서도 동등한 정확도를 달성하여 희소 활성화의 막대한 효율성을 증명했다.31 Pathways는 이러한 아이디어를 시스템 전체 수준으로 확장한 것이다.</p>
<p>Pathways의 발표는 AI 개발의 패러다임을 ’규모의 경제’를 재정의하려는 시도였다. GPT-3와 같은 이전의 거대 모델 경쟁이 ’밀집 파라미터 수’를 늘리는 데 집중했다면, Pathways는 ’잠재적 능력의 총량’으로 스케일의 개념을 확장했다. 기존 접근법은 모델 크기가 커질수록 훈련 및 추론 비용이 천문학적으로 증가하는 지속 불가능한 문제를 안고 있었다. Pathways는 “만약 모델이 모든 것을 할 수 있는 잠재력을 가지되, 한 번에 한 가지 일에만 효율적으로 집중한다면 어떨까?“라는 질문을 던졌다. 희소 활성화는 이 질문에 대한 핵심적인 답이었다. 이는 모델의 총 파라미터 수(잠재 능력)와 특정 작업을 처리하는 데 실제로 사용되는 파라미터 수(실행 비용)를 분리하는 메커니즘이다. 따라서 Pathways의 비전은 단순히 더 큰 모델을 만드는 것이 아니라, ‘효율적으로 큰’ 모델을 만드는 방법에 대한 청사진이었다. 이는 AI 스케일링 경쟁의 규칙을 바꾸려는 전략적 선언이었으며, 이후 Mixture-of-Experts(MoE)와 같은 희소 모델이 업계 표준으로 자리 잡는 데 결정적인 영향을 미쳤다.</p>
<h2>5. 결론</h2>
<p>2021년 10월은 인공지능 및 로봇 연구가 ’규모’의 한계를 넘어 ‘효율’, ‘일반성’, 그리고 ’원칙’이라는 새로운 가치를 향해 나아가는 중요한 전환점이었음을 명확히 보여준다. 이 시기에 발표된 주요 연구들은 각자의 분야에서 중요한 기술적 돌파구를 마련했을 뿐만 아니라, 미래 AI 기술이 나아가야 할 방향에 대한 거시적인 흐름을 형성했다는 점에서 통합적인 의미를 갖는다.</p>
<p>본 보고서에서 심층 분석한 핵심 연구들은 이러한 변화의 흐름을 상징적으로 보여준다.</p>
<ul>
<li>
<p><strong>Swin Transformer</strong>는 Vision Transformer의 이차적인 연산 복잡도 문제를 해결하고 CNN의 계층적 구조를 성공적으로 융합함으로써, Transformer가 컴퓨터 비전 분야의 실용적이고 강력한 범용 백본으로 자리 잡는 길을 열었다. 이는 순수 아키텍처 경쟁을 넘어, 기존 패러다임의 장점을 통합하는 하이브리드 접근법의 유효성을 입증했다.</p>
</li>
<li>
<p><strong>Mip-NeRF</strong>는 신경망 렌더링 분야에 신호 처리라는 고전적이고 원칙적인 프레임워크를 도입하여, 다중 해상도 환경에서의 앨리어싱 문제를 근본적으로 해결했다. 이는 AI 연구가 단순히 경험적인 성능 향상을 넘어, 수학적·물리적 타당성에 기반한 신뢰성 있는 결과물을 추구해야 함을 보여주었다.</p>
</li>
<li>
<p><strong>FlingBot</strong>은 천과 같이 모델링이 극도로 어려운 물리적 과제에 대해, 정밀한 동역학 모델링 대신 동적 상호작용과 자기 지도 학습이라는 새로운 해법을 제시했다. 이는 ’이해 후 제어’라는 전통적 로봇 공학의 패러다임을 ’상호작용을 통한 학습’으로 전환할 수 있는 가능성을 보여준 중요한 사례다.</p>
</li>
<li>
<p><strong>Google Pathways</strong>는 이 모든 개별적인 기술적 흐름을 아우르는 거시적 비전을 제시했다. 단일 작업, 단일 모달리티, 밀집 연산이라는 기존 AI의 한계를 지적하고, 멀티태스크, 멀티모달, 희소 활성화라는 세 가지 원칙을 통해 미래 AI가 어떻게 더 일반적이고 효율적인 지능으로 발전해야 하는지에 대한 청사진을 그렸다.</p>
</li>
</ul>
<p>결론적으로, 2021년 10월에 제시된 아이디어들은 이후 AI 연구 개발의 주류가 되었다. 계층적이고 효율적인 비전 모델, 원칙에 기반한 신경망 렌더링, 동적 상호작용을 학습하는 로봇, 그리고 희소 활성화 기반의 초거대 모델은 현재 AI 기술 생태계의 핵심 구성 요소로 자리 잡았다. 따라서 이 시기는 단순히 몇몇 인상적인 논문이 발표된 기간이 아니라, 현재 우리가 경험하고 있는 AI 기술 혁명의 씨앗이 뿌려지고 그 방향성이 설정된 결정적인 순간이었다고 평가할 수 있다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Artificial intelligence and robotics: Shaking up the business world and society at large | Request PDF - ResearchGate, https://www.researchgate.net/publication/346582328_Artificial_intelligence_and_robotics_Shaking_up_the_business_world_and_society_at_large</li>
<li>Introducing Pathways: A next-generation AI architecture - Google Blog, https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/</li>
<li>2021 AI Principles Progress Update - Google AI, https://ai.google/static/documents/ai-principles-2021-progress-update.pdf</li>
<li>[October 2021] Machine Learning Monthly Newsletter | Zero To Mastery, https://zerotomastery.io/blog/machine-learning-monthly-newsletter-october-2021/</li>
<li>ICCV Paper Awards - IEEE Computer Society Technical Committee on Pattern Analysis and Machine Intelligence, https://tc.computer.org/tcpami/awards/iccv-paper-awards/</li>
<li>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/2103.14030</li>
<li>[PDF] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | Semantic Scholar, https://www.semanticscholar.org/paper/Swin-Transformer%3A-Hierarchical-Vision-Transformer-Liu-Lin/c8b25fab5608c3e033d34b4483ec47e68ba109b7</li>
<li>Different-Size Windows Swin Transformer for Image Classification and Object Detection - SciTePress, https://www.scitepress.org/Papers/2023/116758/116758.pdf</li>
<li>Swin Transformer for Hierarchical Vision (2021) - Naoki Shibuya, https://naokishibuya.github.io/blog/2022-11-04-swin-transformer-2021/</li>
<li>SWIN Transformers Explained | Layer Architecture &amp; More - Bolster AI, https://bolster.ai/blog/swin-transformers</li>
<li>Explanation — Swin Transformer - Chau Tuan Kien - Medium, https://chautuankien.medium.com/explanation-swin-transformer-93e7a3140877</li>
<li>arXiv:2103.14030v2 [cs.CV] 17 Aug 2021, https://arxiv.org/pdf/2103.14030</li>
<li>Breaking Down Swin Transformer: Understanding Relative Position …, https://medium.com/@ovularslan/breaking-down-swin-transformer-understanding-relative-position-bias-and-masked-self-attention-437d692ab7cf</li>
<li>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows - arXiv, https://arxiv.org/abs/2103.14030</li>
<li>Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields | Request PDF - ResearchGate, https://www.researchgate.net/publication/350397998_Mip-NeRF_A_Multiscale_Representation_for_Anti-Aliasing_Neural_Radiance_Fields</li>
<li>PyNeRF: Pyramidal Neural Radiance Fields - Haithem Turki, https://haithemturki.com/pynerf/resources/paper.pdf</li>
<li>Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2021/papers/Barron_Mip-NeRF_A_Multiscale_Representation_for_Anti-Aliasing_Neural_Radiance_Fields_ICCV_2021_paper.pdf</li>
<li>[2304.10075] Multiscale Representation for Real-Time Anti-Aliasing Neural Rendering, https://arxiv.org/abs/2304.10075</li>
<li>google/mipnerf - GitHub, https://github.com/google/mipnerf</li>
<li>Mip-NeRF - nerfstudio, https://docs.nerf.studio/nerfology/methods/mipnerf.html</li>
<li>mip-NeRF - Jon Barron, https://jonbarron.info/mipnerf/</li>
<li>Shuran Song and Huy Ha Win the Best System Paper at CoRL 2021 - Columbia CS, https://www.cs.columbia.edu/2021/shuran-song-and-huy-ha-wins-best-system-paper-at-corl-2021/</li>
<li>CoRL 2021, https://2021.corl.org/</li>
<li>Interview with Huy Ha and Shuran Song: CoRL 2021 best system paper award winners, https://robohub.org/interview-with-huy-ha-and-shuran-song-corl-2021-best-system-paper-award-winners/</li>
<li>Dynamic Cloth Manipulation Considering Variable Stiffness and Material Change Using Deep Predictive Model With Parametric Bias - Frontiers, https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2022.890695/full</li>
<li>FlingBot: The Unreasonable Effectiveness of … - OpenReview, https://openreview.net/pdf?id=0QJeE5hkyFZ</li>
<li>Dynamic Cloth Manipulation Considering Variable Stiffness and Material Change Using Deep Predictive Model with Parametric Bias - arXiv, https://arxiv.org/html/2409.15635v1</li>
<li>FlingBot: The Unreasonable Effectiveness of Dynamic Manipulations for Cloth Unfolding, https://flingbot.cs.columbia.edu/</li>
<li>FlingBot: The Unreasonable Effectiveness of Dynamic Manipulation for Cloth Unfolding, https://proceedings.mlr.press/v164/ha22a.html</li>
<li>CoRL 2021 Conference Blue Sky - OpenReview, https://openreview.net/group?id=robot-learning.org/CoRL/2021/Conference/Blue_Sky</li>
<li>Understanding Google’s GPT Killer- The Pathways Architecture | by …, https://machine-learning-made-simple.medium.com/understanding-googles-gpt-killer-the-pathways-architecture-5d39cff63d52</li>
<li>Introduction to Pathways on Cloud | AI Hypercomputer | Google Cloud, https://cloud.google.com/ai-hypercomputer/docs/workloads/pathways-on-cloud/pathways-intro</li>
<li>Sparse Weight Activation Training- Reduce memory and training time in Machine Learning | by Devansh | Geek Culture | Medium, https://medium.com/geekculture/sparse-weight-activation-training-reduce-memory-and-training-time-in-machine-learning-8c0fad7d5def</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>