<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2021년 6월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2021년 6월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2021년 AI 및 로봇 연구 동향</a> / <span>2021년 6월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2021년 6월 AI 및 로봇 연구 동향</h1>
<h2>1.  2021년 중반 AI 연구의 지형도</h2>
<p>2021년 6월은 인공지능(AI) 연구 커뮤니티가 전 세계적 팬데믹으로 인한 가상 학회 환경에 완전히 적응하며, 특정 기술적 흐름들이 수렴하고 폭발적인 성장을 이루는 중요한 변곡점이었다.1 이 시기는 단순히 기존 연구를 개선하는 것을 넘어, AI의 근본적인 패러다임을 전환하려는 시도들이 가시적인 성과로 나타나기 시작한 때이다. 본 보고서는 이 시기에 발표된 주요 AI 및 로보틱스 연구를 심층적으로 분석하여, 당시의 핵심적인 기술적 성취를 조명하고 이것이 현재와 미래의 연구 지형에 어떠한 영향을 미쳤는지 고찰하고자 한다.</p>
<p>당시 AI 연구를 관통하는 지배적인 주제는 세 가지로 요약할 수 있다. 첫째, <strong>트랜스포머 아키텍처의 패권 확장</strong>이다. 자연어 처리(NLP) 분야를 평정한 트랜스포머가 컴퓨터 비전 분야의 핵심 백본(backbone)으로 자리매김하려는 시도가 정점에 달했다. 이는 Vision Transformer(ViT)의 단순 적용을 넘어, 비전 태스크가 가진 고유한 특성인 계층성(hierarchy)과 지역성(locality)을 아키텍처 설계에 적극적으로 반영하는 방향으로 진화했다.</p>
<p>둘째, <strong>생성 모델의 질적 도약</strong>이다. 기존의 생성적 적대 신경망(GAN)이 2차원 이미지의 사실성을 높이는 데 주력했다면, 이 시기에는 제어 가능성(controllability)과 3차원 공간에 대한 이해(3D-awareness)를 결합한 새로운 모델들이 등장했다. 이는 단순히 보기 좋은 이미지를 생성하는 것을 넘어, 사용자가 의도한 대로 장면을 구성하고 조작할 수 있는 디지털 콘텐츠 제작의 새로운 가능성을 열었다.</p>
<p>셋째, <strong>범용 에이전트(Generally Capable Agents)를 향한 탐색</strong>이다. 특정 태스크를 해결하는 데 특화된 ’좁은 AI’의 한계를 넘어, 명시적으로 정의되지 않은 다양한 환경과 문제에 스스로 적응하고 해결책을 찾아내는 범용 AI 에이전트 개발을 위한 기초 연구가 산업계 대형 연구소들을 중심으로 활발히 진행되었다.</p>
<p>본 보고서는 2021년 6월 AI 연구의 정점이었던 컴퓨터 비전 및 패턴 인식 학회(CVPR 2021)의 주요 발표를 시작으로, 로보틱스, 자연어 처리, 그리고 주요 산업계 연구소의 동향을 순차적으로 분석한다. 각 연구에 대해서는 핵심 방법론을 기술적으로 상세히 설명하고, 해당 연구가 갖는 심층적 의의와 학계 및 산업계에 미친 파급 효과를 다각도로 분석할 것이다.</p>
<h2>2.  컴퓨터 비전의 지평 확장: CVPR 2021 주요 연구 동향</h2>
<p>2021년 6월 19일부터 25일까지 가상으로 개최된 IEEE/CVF 컴퓨터 비전 및 패턴 인식 학회(CVPR 2021)는 해당 월의 가장 중요하고 영향력 있는 학술 행사였다.1 7,000건이 넘는 논문이 제출되어 그중 1,600편이 발표되었으며, 이는 컴퓨터 비전 분야의 폭발적인 연구 열기와 빠른 기술 발전을 명확히 보여주는 지표였다.2 이 학회에서는 트랜스포머의 비전 분야 적용, 3D 인식 생성 모델, 자기지도학습 등 다양한 주제가 다루어졌으며, 그중에서도 특히 주목받은 연구들은 이후 AI 기술의 방향성을 결정하는 중요한 이정표가 되었다.</p>
<h3>2.1  제어 가능한 3D 장면 합성의 새로운 패러다임: GIRAFFE</h3>
<p>CVPR 2021에서 최우수 논문상(Best Paper Award)을 수상한 “GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields“는 2D 이미지 생성 모델의 근본적인 한계를 극복하고, 3D 공간에 대한 깊은 이해를 바탕으로 장면을 ’구성적(compositional)’으로 생성하고 제어하는 새로운 패러다임을 제시했다.2 기존의 StyleGAN과 같은 2D GAN 모델들이 이미지 전체를 하나의 잠재 벡터(latent vector)로부터 생성하여 객체의 위치나 자세를 개별적으로 제어하기 어려웠던 반면, GIRAFFE는 장면을 구성하는 개별 요소들을 3D 공간에서 분리하여 모델링함으로써 전례 없는 수준의 제어 가능성을 확보했다.</p>
<h4>2.1.1 핵심 방법론: Compositional Generative Neural Feature Fields</h4>
<p>GIRAFFE의 핵심 아이디어는 장면을 (<span class="math math-inline">(N-1)</span>개의 전경(foreground) 객체와 1개의 배경(background)으로 구성된 ’신경 특징 필드(Neural Feature Fields)’의 조합으로 표현하는 것이다.5 각 객체는 독립적인 형상(shape)과 외형(appearance) 잠재 코드를 가지며, 이 코드들은 다층 퍼셉트론(MLP)으로 구성된 생성기를 통해 3D 특징 필드 <span class="math math-inline">f_i</span>와 밀도 필드 <span class="math math-inline">\sigma_i</span>를 생성한다. 이 필드들은 3D 공간 좌표 <span class="math math-inline">x</span>를 입력받아 해당 위치의 특징 벡터와 밀도를 출력하는 연속적인 함수 <span class="math math-inline">(f_i, \sigma_i) = G_i(z_{\text{shape},i}, z_{\text{app},i}; x)</span>로 정의된다.6</p>
<p>이렇게 생성된 각 객체의 3D 필드는 개별적인 3D 변환(회전, 이동) 행렬 <span class="math math-inline">T_i</span>를 통해 가상 3D 공간에 배치된다. 최종적으로 렌더링할 때는 특정 광선(ray)을 따라 모든 객체의 특징 필드와 밀도 필드를 합산(summation)하여 전체 장면을 구성한다.6 이 ‘합성을 통한 표현(representation-by-composition)’ 방식은 3D 세계의 물리적 구성 원리를 모델에 내재한 강력한 귀납적 편향(inductive bias)으로 작용하여, 모델이 별도의 레이블 없이도 장면의 구성 요소를 자연스럽게 분리(disentangle)하도록 유도한다.</p>
<h4>2.1.2 신경 렌더링 파이프라인</h4>
<p>GIRAFFE는 사실적인 이미지를 효율적으로 생성하기 위해 2단계 신경 렌더링 파이프라인을 도입했다. 먼저, 구성된 3D 장면의 특징 필드를 볼륨 렌더링(volume rendering) 기법을 사용하여 상대적으로 낮은 해상도(예: 16x16)의 2D 특징 이미지(feature image)로 렌더링한다.7 이 단계는 3D 정보를 2D 평면에 투영하면서도 각 픽셀에 풍부한 특징 정보를 담도록 한다. 그 후, 이 저해상도 특징 이미지를 합성곱 신경망(CNN) 기반의 신경 렌더러(Neural Renderer)에 입력하여 최종적인 고해상도 RGB 이미지로 변환한다.6 이 2단계 접근법은 NeRF(Neural Radiance Fields)와 같이 픽셀 단위로 광선을 추적하여 렌더링하는 방식에 비해 렌더링 속도를 획기적으로 개선하면서도 높은 품질의 이미지를 생성하는 핵심적인 역할을 했다.6</p>
<p>이러한 설계는 생성 모델 연구의 흐름을 바꾸는 중요한 계기가 되었다. GIRAFFE의 성공은 2D 픽셀 공간에서만 작동하던 생성 모델들이 3D 세계의 구조를 명시적으로 모델링할 때 비로소 진정한 제어 가능성과 일반화 능력을 얻을 수 있음을 증명했다. 이는 이후 3D GAN, Generative Radiance Fields 등 3D-aware 생성 모델 연구의 폭발적인 증가를 촉발하는 기폭제가 되었다.</p>
<p><strong>표 1: CVPR 2021 주요 수상 논문</strong></p>
<table><thead><tr><th>수상 부문 (Award Category)</th><th>논문 제목 (Paper Title)</th><th>저자 (Authors)</th><th>핵심 기여 (Core Contribution)</th></tr></thead><tbody>
<tr><td><strong>최우수 논문상 (Best Paper)</strong></td><td>GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields</td><td>Michael Niemeyer, Andreas Geiger</td><td>3D 기반의 구성적 표현을 통해 감독 없이 객체 분리 및 제어 가능한 사실적 이미지 합성을 달성함.</td></tr>
<tr><td><strong>최우수 학생 논문상 (Best Student Paper)</strong></td><td>Task Programming: Learning Data Efficient Behavior Representations</td><td>Jennifer J. Sun, Ann Kennedy, et al.</td><td>과제 프로그램을 통해 적은 데이터로도 효율적인 행동 표현 학습 방법을 제안함.</td></tr>
<tr><td><strong>우수 논문 (Honorable Mention)</strong></td><td>Exploring Simple Siamese Representation Learning</td><td>Xinlei Chen, Kaiming He</td><td>네거티브 샘플 없이 샴(Siamese) 네트워크를 이용한 간단하면서도 효과적인 자기지도 표현 학습 프레임워크(SimSiam)를 제안함.</td></tr>
<tr><td><strong>우수 논문 (Honorable Mention)</strong></td><td>Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling</td><td>Jie Lei, Linjie Li, et al.</td><td>비디오-언어 학습에서 희소 샘플링(sparse sampling)을 통해 효율성과 성능을 동시에 개선함.</td></tr>
</tbody></table>
<p>자료: 2</p>
<h3>2.2  비전 트랜스포머의 진화: Swin Transformer</h3>
<p>Vision Transformer(ViT)가 이미지 분류에서 CNN의 성능을 뛰어넘는 가능성을 보여주었지만, 이미지 크기에 대해 이차적($O(N^2)`)으로 증가하는 연산 복잡도 문제는 치명적인 한계였다. 이로 인해 고해상도 이미지를 다루는 객체 탐지(object detection)나 시맨틱 분할(semantic segmentation)과 같은 조밀한 예측(dense prediction) 태스크에 ViT를 직접 적용하는 것은 비효율적이었다.8 마이크로소프트 연구진이 발표한 “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows“는 이러한 한계를 극복하고 트랜스포머를 범용적인 비전 백본으로 사용하기 위해 제안된 혁신적인 아키텍처다.3</p>
<h4>2.2.1 핵심 방법론 1: 계층적 특징 맵과 패치 병합</h4>
<p>Swin Transformer는 CNN의 핵심적인 장점인 계층적 특징 맵(hierarchical feature maps) 구조를 트랜스포머에 도입했다.8 입력 이미지를 작은 크기의 패치(예: 4x4 픽셀)로 분할하여 초기 토큰을 생성한 후, 여러 단계의 ‘패치 병합(Patch Merging)’ 레이어를 통과시킨다. 패치 병합 레이어는 인접한 2x2 패치 그룹의 특징 벡터들을 연결(concatenate)하여 공간 해상도를 절반으로 줄이는 대신 채널 차원을 4배로 늘리고, 이후 선형 레이어를 통해 채널 차원을 2배로 조정한다. 이 과정을 반복함으로써 모델은 깊은 레이어로 갈수록 더 넓은 영역의 정보를 요약하는 저해상도-고차원 특징 맵을 생성하게 된다. 이러한 계층적 구조는 다양한 크기의 객체를 인식해야 하는 비전 태스크에 매우 효과적이며, FPN(Feature Pyramid Networks)과 같은 기존의 비전 프레임워크와 손쉽게 통합될 수 있는 기반을 제공한다.8</p>
<h4>2.2.2 핵심 방법론 2: Shifted Window 기반 Self-Attention (W-MSA &amp; SW-MSA)</h4>
<p>연산 복잡도 문제를 해결하기 위해 Swin Transformer는 전역적인(global) 자기 주의(self-attention) 대신, 이미지를 겹치지 않는 여러 개의 로컬 윈도우(local windows)로 분할하고 각 윈도우 내부에서만 자기 주의를 계산하는 **Window-based Multi-head Self-Attention (W-MSA)**을 사용한다.8 윈도우 크기를 고정하면 연산량은 이미지 크기에 대해 선형적(<span class="math math-inline">O(N)</span>)으로 감소하여 고해상도 이미지 처리가 가능해진다.</p>
<p>그러나 W-MSA만 사용하면 윈도우 간의 정보 교류가 단절되는 문제가 발생한다. 이를 해결하기 위해 Swin Transformer는 연속된 두 개의 트랜스포머 블록에서 윈도우 파티션을 주기적으로 이동시키는 **Shifted Window-based Multi-head Self-Attention (SW-MSA)**을 도입했다.8 예를 들어, l$번째 레이어에서 W-MSA를 수행했다면, <span class="math math-inline">l+1</span>번째 레이어에서는 윈도우를 윈도우 크기의 절반만큼 대각선 방향으로 이동시킨다. 이렇게 이동된 윈도우는 이전 레이어의 윈도우 경계를 가로지르기 때문에, 결과적으로 윈도우 간의 정보 흐름이 발생하여 전역적인 정보를 모델링하는 효과를 얻게 된다. 이 두 모듈을 번갈아 사용하는 것이 Swin Transformer의 핵심이다.</p>
<p><span class="math math-display">
\begin{aligned}
\hat{\mathbf{z}}^l &amp;= \text{W-MSA}(\text{LN}(\mathbf{z}^{l-1})) + \mathbf{z}^{l-1}, \\
\mathbf{z}^l &amp;= \text{MLP}(\text{LN}(\hat{\mathbf{z}}^l)) + \hat{\mathbf{z}}^l, \\
\hat{\mathbf{z}}^{l+1} &amp;= \text{SW-MSA}(\text{LN}(\mathbf{z}^l)) + \mathbf{z}^l, \\
\mathbf{z}^{l+1} &amp;= \text{MLP}(\text{LN}(\hat{\mathbf{z}}^{l+1})) + \hat{\mathbf{z}}^{l+1}
\end{aligned}
</span></p>
<h4>2.2.3 상대적 위치 편향</h4>
<p>자기 주의 연산 시, 절대 위치 임베딩 대신 상대적 위치 편향(relative position bias) <span class="math math-inline">B</span>를 주의 점수(attention score)에 더하는 방식을 사용한다. 이는 윈도우 내 패치들 간의 상대적인 위치 관계를 모델링하며, 이동 불변성(translation invariance)을 높이는 데 기여한다. 수식은 다음과 같다.</p>
<p><span class="math math-display">
\text{Attention}(Q, K, V) = \text{SoftMax}(\frac{QK^T}{\sqrt{d}} + B)V
</span><br />
Swin Transformer의 등장은 트랜스포머가 단순히 CNN의 대안이 아니라, 비전 분야 전반에 걸쳐 더 우수한 성능과 효율성을 제공하는 ’표준 아키텍처’가 될 수 있음을 입증한 사건이었다. 지난 10년간 비전 분야를 지배해 온 CNN의 시대가 저물고, 트랜스포머가 새로운 패권 아키텍처로 부상할 수 있음을 예고하는 강력한 신호탄이었다.</p>
<p><strong>표 2: Swin Transformer와 주요 비전 모델 성능 비교</strong></p>
<table><thead><tr><th>모델 (Model)</th><th>ImageNet-1K Top-1 Acc. (%)</th><th>COCO test-dev box AP</th><th>COCO test-dev mask AP</th><th>ADE20K val mIoU</th></tr></thead><tbody>
<tr><td>ViT-B/16</td><td>84.2</td><td>-</td><td>-</td><td>-</td></tr>
<tr><td>DeiT-B</td><td>83.1</td><td>-</td><td>-</td><td>-</td></tr>
<tr><td>ResNeXt101-64x4d</td><td>82.8</td><td>48.3</td><td>41.7</td><td>45.3</td></tr>
<tr><td><strong>Swin-B</strong></td><td><strong>86.4</strong></td><td><strong>51.9</strong></td><td><strong>45.0</strong></td><td><strong>49.5</strong></td></tr>
<tr><td><strong>Swin-L</strong></td><td><strong>87.3</strong></td><td><strong>52.2</strong></td><td><strong>45.5</strong></td><td><strong>51.0</strong></td></tr>
</tbody></table>
<p>자료: 8</p>
<h3>2.3  텍스트-이미지 생성의 도약: DALL-E와 Zero-Shot 생성 모델</h3>
<p>2021년 초에 발표되었지만 CVPR, ICML 등 여름 학회 시즌에 본격적으로 논의되며 학계에 큰 충격을 준 OpenAI의 “Zero-Shot Text-to-Image Generation” (DALL-E)는 텍스트 설명만으로 매우 창의적이고 복잡한 이미지를 생성하는 능력을 선보였다.3 “아보카도 모양의 안락의자“나 “아코디언으로 만들어진 맥“과 같이 현실에 존재하지 않는 개념을 시각화하는 능력은 AI가 단순히 데이터를 암기하는 것을 넘어, 개념을 조합하고 추론하는 단계로 나아가고 있음을 보여주었다.</p>
<h4>2.3.1 핵심 방법론: 2단계 학습 프레임워크</h4>
<p>DALL-E의 접근법은 극도로 단순하면서도 강력하다. 텍스트와 이미지를 별개의 데이터로 취급하지 않고, 모두 이산적인 토큰(token)으로 변환하여 하나의 긴 시퀀스로 취급하고, 이를 거대한 트랜스포머 모델로 학습시키는 것이다. 이 과정은 두 단계로 나뉜다.</p>
<ol>
<li>
<p><strong>1단계: 이산적 VAE(dVAE)를 이용한 이미지 토큰화:</strong> 256x256 픽셀의 RGB 이미지를 직접 모델링하는 것은 엄청난 계산량을 요구한다. 이를 해결하기 위해 먼저 dVAE(discrete Variational Autoencoder)를 학습시켜 이미지를 32x32 그리드의 이산적인 이미지 토큰으로 압축한다. 각 토큰은 8192개의 코드북 벡터 중 하나를 가리키는 인덱스로 표현된다.14 이 과정은 이미지의 본질적인 시각적 개념은 유지하면서도, 모델이 다루어야 할 데이터의 차원을 192배나 줄여 트랜스포머가 효율적으로 학습할 수 있도록 한다.</p>
</li>
<li>
<p><strong>2단계: 자기회귀 트랜스포머를 이용한 사전 확률 학습:</strong> 텍스트(최대 256개의 BPE 인코딩 토큰)와 1단계에서 얻은 이미지 토큰(1024개)을 하나의 시퀀스로 연결한다. 그 후, 120억 개의 파라미터를 가진 거대한 자기회귀(autoregressive) 트랜스포머를 학습시켜 이 통합 시퀀스의 결합 확률 분포 <span class="math math-inline">p(\text{text}, \text{image\_tokens})</span>를 모델링한다.14 즉, 주어진 텍스트 토큰들과 이전에 생성된 이미지 토큰들을 바탕으로 다음 이미지 토큰을 순차적으로 예측하도록 학습한다.</p>
</li>
</ol>
<h4>2.3.2 증거 하한(ELBO) 기반 학습</h4>
<p>전체 학습 과정은 이미지 <span class="math math-inline">x</span>, 캡션 <span class="math math-inline">y</span>, 이미지 토큰 <span class="math math-inline">z</span>에 대한 결합 우도(joint likelihood)의 증거 하한(Evidence Lower Bound, ELBO)을 최대화하는 것으로 공식화할 수 있다.16</p>
<p><span class="math math-display">
\ln p_{\theta,\psi}(x, y) \geqslant E_{z\sim q_{\phi}(z \vert x)} \left( \ln p_{\theta}(x \vert y, z) - \beta D_{KL}(q_{\phi}(y, z \vert x), p_{\psi}(y, z)) \right)
</span><br />
여기서 <span class="math math-inline">q_\phi</span>는 dVAE 인코더가 생성하는 이미지 토큰의 분포, <span class="math math-inline">p_\theta</span>는 dVAE 디코더가 토큰으로부터 이미지를 복원할 확률, 그리고 <span class="math math-inline">p_\psi</span>는 트랜스포머가 모델링하는 텍스트와 이미지 토큰의 결합 사전 확률(joint prior)을 나타낸다.</p>
<p>DALL-E의 성공은 AI 연구의 방향성에 중요한 시사점을 던졌다. 복잡하고 정교한 모델 아키텍처나 보조 손실 함수 없이도, ‘확장 가능한(scalable)’ 아키텍처와 ’대규모 데이터’라는 두 가지 요소만으로 서로 다른 양식(modality)의 데이터를 통합하고 전례 없는 일반화 능력을 달성할 수 있음을 증명한 것이다. 이는 GPT-3가 NLP에서 보여준 ’규모의 법칙(scaling laws)’이 비전과 언어를 아우르는 멀티모달 영역에서도 동일하게 적용될 수 있음을 시사하며, 이후 등장할 거대 언어 모델(LLM) 및 거대 멀티모달 모델(LMM) 시대의 서막을 열었다.</p>
<h3>2.4  기타 주목할 만한 CVPR 2021 발표 연구</h3>
<p>CVPR 2021에서는 앞서 소개한 연구들 외에도 다양한 분야에서 중요한 진전이 있었다. 최우수 학생 논문상(Best Student Paper Award)을 수상한 **“Task Programming: Learning Data Efficient Behavior Representations”**는 소량의 데이터만으로도 효율적으로 행동 표현을 학습하는 방법을 제시하여, 데이터 효율성(data efficiency)이라는 머신러닝의 오랜 과제를 다루었다.2</p>
<p>또한, 우수 논문(Honorable Mentions)으로 선정된 연구들도 주목할 만하다. Kaiming He 연구팀의 **“Exploring Simple Siamese Representation Learning” (SimSiam)**은 대조 학습(contrastive learning)에서 필수적이라고 여겨졌던 네거티브 페어(negative pair) 없이도 효과적인 표현 학습이 가능함을 보여주어 자기지도학습 연구에 새로운 방향을 제시했다.4 **“Open-Vocabulary Object Detection Using Captions”**는 기존 객체 탐지 모델들이 고정된 클래스 집합에 묶여 있던 한계를 넘어, 이미지 캡션 데이터를 활용하여 이전에 보지 못한 새로운 객체까지 탐지하는 ’개방 어휘 객체 탐지’라는 실용적인 패러다임을 제안했다.18 마지막으로 **“Vx2Text: End-to-End Learning of Video-Based Text Generation”**은 비디오, 텍스트, 음성 등 다양한 모달리티의 입력을 하나의 통합된 프레임워크 내에서 처리하여 텍스트를 생성하는 기술을 선보이며 멀티모달 AI의 가능성을 확장했다.18</p>
<h2>3.  로보틱스 분야의 탐색 및 자율성 증진 연구</h2>
<p>2021년 중반 로보틱스 분야는 AI, 특히 딥러닝 및 강화학습 기술과의 융합이 가속화되면서 로봇의 자율성(autonomy) 수준을 한 단계 끌어올리는 연구들이 큰 주목을 받았다. 그중에서도 미지의 대규모 환경을 빠르고 효율적으로 탐색하는 기술은 재난 구조, 행성 탐사, 시설 점검 등 다양한 실제 응용 분야의 성공을 좌우하는 핵심 과제로 부상했다. 이 시기 로보틱스 분야의 가장 권위 있는 학회 중 하나인 Robotics: Science and Systems (RSS) 2021에서 발표된 연구들은 이러한 흐름을 잘 보여준다.19</p>
<h3>3.1  복잡한 3D 환경 탐색의 효율성 극대화: TARE 프레임워크</h3>
<p>RSS 2021에서 최우수 논문상(Best Paper Award)과 최우수 시스템 논문상(Best System Paper Award)을 동시에 수상하는 영예를 안은 “TARE: A Hierarchical Framework for Efficiently Exploring Complex 3D Environments“는 대규모의 복잡한 3D 환경을 자율적으로 탐색하는 문제에 대한 혁신적인 해결책을 제시했다.19 기존의 탐색 방법들은 단일한 고해상도 맵 표현에 의존하여 환경이 커질수록 계산량이 폭발적으로 증가하는 확장성 문제에 직면했다. TARE는 이러한 문제를 해결하기 위해 인간의 공간 인지 방식과 유사한 계층적 접근법을 도입했다.</p>
<h4>3.1.1 핵심 방법론: 계층적 계획 프레임워크</h4>
<p>TARE의 핵심은 탐색 문제를 ‘가까운 곳은 상세하게, 먼 곳은 개략적으로’ 계획하는 두 개의 계층으로 분리한 것이다.22</p>
<ul>
<li>
<p><strong>글로벌(상위) 레벨:</strong> 이 레벨에서는 로봇으로부터 멀리 떨어진 광역 공간에 대한 계획을 담당한다. 전체 환경을 ‘희소한(sparse)’ 그래프 형태로 추상화하여 표현하고, 이를 기반으로 전체적인 탐색 방향을 결정하는 ‘거친(coarse)’ 경로를 신속하게 계획한다. 이 단계에서는 계산 속도를 확보하기 위해 세부적인 장애물 정보 등을 의도적으로 희생하는 대신, 장기적인 탐색 효율성을 극대화하는 데 집중한다.22</p>
</li>
<li>
<p><strong>로컬(하위) 레벨:</strong> 이 레벨은 로봇 주변의 근접 공간에 대한 정밀한 계획을 수행한다. 로봇 센서로부터 입력되는 데이터를 바탕으로 ‘밀집된(dense)’ 복셀 맵(voxel map)을 실시간으로 구축하고, 이를 이용해 장애물을 회피하며 안전하게 이동할 수 있는 ‘상세한(detailed)’ 경로를 계산한다. 이 단계는 로봇의 단기적인 안전성과 즉각적인 움직임을 보장하는 역할을 한다.22</p>
</li>
</ul>
<p>최종적으로 로봇이 따라가는 탐색 경로는 로컬 계획 범위의 경계 지점에서 이 두 계층의 경로를 자연스럽게 연결하여 생성된다. 이를 통해 TARE는 장기적인 탐색 효율성과 단기적인 이동 안전성이라는 두 마리 토끼를 동시에 잡을 수 있었다.22</p>
<p>TARE의 성공은 복잡하고 대규모인 실제 세계의 로보틱스 문제를 해결하기 위해서는 ’계층적 추상화(hierarchical abstraction)’가 필수적인 설계 원리임을 명확히 보여주었다. 모든 정보를 가장 상세한 수준에서 한 번에 처리하려는 기존의 접근 방식이 가진 본질적인 한계를 지적하고, 문제의 스케일에 따라 표현의 수준(representation granularity)을 동적으로 조절하는 것이 확장 가능한 자율성의 핵심임을 증명한 것이다. 이는 탐색 문제를 넘어 장기적인 조작(long-horizon manipulation), 다개체 로봇 협업 등 다른 고차원 로보틱스 문제에도 적용될 수 있는 강력한 패러다임을 제시했다.</p>
<h3>3.2  2021년 중반 로보틱스 학계의 주요 동향</h3>
<p>TARE 외에도 2021년 중반 로보틱스 분야에서는 다양한 연구가 활발히 진행되었다. 인간-로봇 상호작용(HRI) 및 사회적 로봇(Social Robotics) 분야에서는 로봇이 인간의 감정과 의도를 파악하고, 신뢰와 사회적 수용성을 높이는 방법에 대한 연구가 중요한 주제로 다루어졌다.23 이는 로봇이 단순히 작업을 수행하는 도구를 넘어, 인간 사회의 일원으로 통합되기 위한 필수적인 연구 방향이다.</p>
<p>한편, Robotics Summit &amp; Expo와 같은 산업 중심의 행사에서는 물류, 제조, 헬스케어 등 다양한 산업 현장에 즉시 적용 가능한 상용 로봇 기술들이 대거 소개되었다.24 이는 로봇 기술이 더 이상 연구실 수준에 머무르지 않고, 실제 산업 현장에서 구체적인 가치를 창출하는 단계로 빠르게 진입하고 있음을 보여주었다. 자율 주행 분야 역시 nuScenes, Waymo Open Dataset과 같은 대규모 멀티모달 데이터셋의 공개에 힘입어 인식, 예측, 계획 기술이 지속적으로 발전하고 있었다.25</p>
<h2>4.  자연어 처리 및 산업계 연구소의 최신 동향</h2>
<p>2021년 6월은 8월에 개최될 자연어 처리 분야 최고 권위 학회인 ACL 2021을 앞두고 관련 연구들이 사전 공개 저장소인 arXiv를 통해 대거 공개되던 시기였다.26 학계가 특정 벤치마크의 성능을 높이는 데 집중하는 동안, Google DeepMind, Meta AI와 같은 거대 산업계 연구소들은 학계의 단기적인 성능 경쟁을 넘어 AI의 근본적인 능력과 사회적 책임을 탐구하는 장기적인 연구 결과를 발표하며 기술의 방향성을 제시했다.</p>
<h3>4.1  ACL 2021을 향한 주요 NLP 연구 발표</h3>
<p>2021년 6월 arXiv의 cs.CL (Computation and Language) 카테고리에는 ACL 2021 본 학회 및 관련 워크숍(Findings of ACL, BioNLP, TextGraphs 등)에 채택된 논문들이 다수 공개되었다.26 이 논문들을 통해 당시 NLP 연구의 몇 가지 주요 흐름을 파악할 수 있다.</p>
<ul>
<li>
<p><strong>저자원 및 토착 언어 처리:</strong> 영어 중심의 NLP 기술 개발에서 벗어나, 데이터가 부족한 소수 언어 및 사멸 위기에 처한 토착 언어에 대한 NLP 기술을 개발하고 언어적 불평등을 해소하려는 노력이 중요한 연구 주제로 부상했다. 이는 AI 기술의 포용성을 높이는 데 기여했다.27</p>
</li>
<li>
<p><strong>대화형 AI 및 요약:</strong> 챗봇, 가상 비서 등 대화형 AI의 성능을 높이기 위해 대화의 복잡한 맥락을 이해하고 핵심 정보를 요약하는 기술이 주목받았다. 특히 사용자의 특정 요구에 따라 요약의 내용이나 길이를 제어하는 ‘제어 가능한 요약(controllable summarization)’ 연구가 활발히 진행되었다.29</p>
</li>
<li>
<p><strong>인과 추론과 NLP의 결합:</strong> 기존 NLP 모델들이 데이터의 상관관계에 의존하여 발생하는 가짜 연관(spurious correlation) 문제를 해결하기 위해, 인과 추론(causal inference)의 개념을 도입하려는 초기 연구들이 등장했다. 이는 모델의 강건성(robustness)과 설명 가능성(explainability)을 높이는 근본적인 접근법으로 평가받았다.29</p>
</li>
</ul>
<h3>4.2  산업계 연구소의 주요 발표: DeepMind와 Meta AI</h3>
<p>산업계 연구소들은 막대한 컴퓨팅 자원과 데이터를 바탕으로 학계가 시도하기 어려운 장기적이고 근본적인 연구를 수행하며 AI의 경계를 넓혔다.</p>
<ul>
<li>
<p><strong>DeepMind: 열린 학습과 범용 에이전트:</strong> DeepMind는 특정 게임이나 태스크에 과적합되는 기존 강화학습의 한계를 지적하며, “Open-Ended Learning Leads to Generally Capable Agents“라는 선구적인 연구를 발표했다.30 이 연구는 절차적으로 무한히 다양한 3D 게임 환경이 생성되는 XLand라는 가상 세계를 구축했다. 에이전트는 이 안에서 고정된 최종 목표 없이, 지속적으로 생성되는 새로운 과제들을 해결하며 스스로 학습하고 발전한다. 이는 학습 과정 자체가 끊임없이 새로운 목표를 만들어내는 ’열린 학습(open-ended learning)’이라는 새로운 패러다임을 제시한 것으로, 진정한 의미의 범용 인공지능(AGI)을 향한 중요한 첫걸음으로 평가받았다.30</p>
</li>
<li>
<p><strong>Meta AI (구 Facebook AI): 생태계 구축과 사회적 책임:</strong> Meta AI는 자사의 핵심 AI 프레임워크인 PyTorch가 페이스북의 수십억 사용자를 위한 서비스에서 매일 수조 건의 추론 연산을 수행하고 있음을 강조하며, 프레임워크의 지속적인 발전을 알렸다.31 특히 모바일 기기에서 AI 모델을 직접 실행하여 사용자 경험과 개인정보 보호를 강화하는 온디바이스 AI(on-device AI)를 중요한 미래 방향으로 제시했다.31 또한, 101개 언어에 대한 다국어 기계 번역 성능을 공정하게 평가할 수 있는 “FLORES-101” 벤치마크를 공개하여 저자원 언어 연구 커뮤니티에 크게 기여했다.32 이와 더불어, 온라인상의 허위 정보와 사회적 양극화 문제 해결을 위한 연구 지원 프로그램을 발표하며 AI 기술의 사회적 책임을 다하려는 노력을 보여주었다.33</p>
</li>
</ul>
<p>이러한 동향은 AI 연구 생태계가 두 개의 중요한 축으로 움직이고 있음을 보여준다. 학계는 측정 가능하고 명확하게 정의된 문제들을 정교하게 파고들며 점진적인 기술 발전을 이끄는 역할을 한다. 반면, 막대한 자원과 장기적인 비전을 가진 산업계 연구소들은 AI의 근본적인 한계를 돌파하고 기술 패러다임 자체를 바꾸려는 ‘고위험 고수익’ 연구를 주도하고 있다. 이 두 축의 건강한 상호작용과 경쟁이 AI 분야 전체의 발전을 견인하는 핵심 동력이다.</p>
<h2>5.  결론: 2021년 6월 AI 연구의 종합적 의의 및 향후 전망</h2>
<p>2021년 6월은 AI 기술이 여러 중요한 방향으로 뚜렷한 질적 도약을 이룬 시기로 기록된다. 이 시기에 발표된 연구들은 개별적인 성과를 넘어, AI 연구의 패러다임을 전환하고 이후 기술 발전의 방향성을 제시하는 중요한 이정표 역할을 했다.</p>
<p>본 보고서에서 분석한 핵심적인 진전을 종합하면 다음과 같다.</p>
<ul>
<li>
<p><strong>2D에서 3D로 (From 2D to 3D):</strong> GIRAFFE가 보여주었듯, 컴퓨터 비전 기술은 2D 픽셀 패턴을 인식하는 수준을 넘어, 우리가 사는 3D 세계의 구조를 이해하고 이를 바탕으로 가상 세계를 생성 및 조작하는 방향으로 나아가기 시작했다. 이는 메타버스와 디지털 트윈 기술의 핵심 기반이 되었다.</p>
</li>
<li>
<p><strong>특화에서 범용으로 (From Specialized to General-Purpose):</strong> Swin Transformer는 트랜스포머 아키텍처가 이미지 분류, 객체 탐지, 분할 등 다양한 비전 태스크를 아우르는 강력한 범용 백본이 될 수 있음을 입증했다. 이는 여러 태스크에 걸쳐 단일 모델을 사용하는 파운데이션 모델(foundation model)의 등장을 예고했다.</p>
</li>
<li>
<p><strong>감독에서 확장 가능한 자기지도/제로샷으로 (From Supervised to Scalable Self-Supervised/Zero-Shot):</strong> DALL-E는 특정 태스크에 대한 레이블링 데이터 없이도, 인터넷 규모의 방대한 데이터와 거대 모델의 힘만으로 전례 없는 일반화 및 창작 능력을 달성할 수 있음을 보여주었다. 이는 데이터 수집 및 레이블링 비용의 한계를 극복하는 새로운 길을 열었다.</p>
</li>
<li>
<p><strong>고정된 목표에서 열린 학습으로 (From Fixed-Goal to Open-Ended):</strong> DeepMind의 연구는 AI가 더 이상 인간이 설정한 고정된 문제를 푸는 수동적인 도구가 아니라, 스스로 문제를 발견하고 학습하며 영원히 발전할 수 있는 능동적인 주체가 될 수 있다는 가능성을 탐색하기 시작했음을 알렸다.</p>
</li>
</ul>
<p>이 시기의 연구들은 이후 우리가 목격하게 될 거대 멀티모달 모델(LMM), 생성형 AI의 폭발적인 성장, 그리고 로보틱스와 AI의 심층적 융합으로 이어지는 직접적인 기술적 토대가 되었다. ‘제어 가능성’, ‘효율성’, ‘범용성’, 그리고 ’확장성’이라는 키워드는 2021년 6월을 기점으로 AI 연구의 핵심 화두로 자리 잡았으며, 이는 현재까지도 AI 기술이 지향하는 가장 중요한 가치로 남아있다. 결론적으로, 본 보고서에서 분석한 2021년 6월의 주요 연구들은 AI가 다음 단계로 진화하는 데 있어 결정적인 전환점을 마련한 역사적인 성과였다고 평가할 수 있다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>The most important AI conferences 2021 (updated) - AMAI GmbH, https://www.am.ai/en/insights/ai-conferences-2021</li>
<li>CVPR 2021 Conference Report | IEEE Computer Society, https://www.computer.org/publications/tech-news/events/cvpr-2021-recap/</li>
<li>louisfb01/top-10-cv-papers-2021: A curated list of the top … - GitHub, https://github.com/louisfb01/top-10-cv-papers-2021</li>
<li>CVPR’21 PAPER AWARDS | CVPR 2021, https://cvpr2021.thecvf.com/node/329</li>
<li>GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields, https://m-niemeyer.github.io/project-pages/giraffe/</li>
<li>GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields - Stanford Computer Graphics Laboratory, <a href="https://graphics.stanford.edu/courses/cs348n-22-winter/LectureSlides/FinalSlides/GIRAFFE_%20Representing%20Scenes%20as%20Compositional%20Generative%20Neural%20Feature%20Fields.pdf">https://graphics.stanford.edu/courses/cs348n-22-winter/LectureSlides/FinalSlides/GIRAFFE_%20Representing%20Scenes%20as%20Compositional%20Generative%20Neural%20Feature%20Fields.pdf</a></li>
<li>GIRAFFE: Representing Scenes As … - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2021/papers/Niemeyer_GIRAFFE_Representing_Scenes_As_Compositional_Generative_Neural_Feature_Fields_CVPR_2021_paper.pdf</li>
<li>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/2103.14030</li>
<li>Swin Transformer: Hierarchical Vision Transformer using Shifted …, https://arxiv.org/pdf/2103.14030</li>
<li>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows - arXiv, https://arxiv.org/abs/2103.14030</li>
<li>[PDF] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | Semantic Scholar, https://www.semanticscholar.org/paper/Swin-Transformer%3A-Hierarchical-Vision-Transformer-Liu-Lin/c8b25fab5608c3e033d34b4483ec47e68ba109b7</li>
<li>This is an official implementation for “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows”. - GitHub, https://github.com/microsoft/Swin-Transformer</li>
<li>Zero-Shot Text-to-Image Generation - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v139/ramesh21a.html</li>
<li>OpenAI’s DALL·E: Text-to-Image Generation Explained - Louis-François Bouchard, https://www.louisbouchard.ai/openais-dall-e-text-to-image-generation-explained/</li>
<li>[2102.12092] Zero-Shot Text-to-Image Generation - arXiv, https://arxiv.org/abs/2102.12092</li>
<li>Zero-Shot Text-to-Image Generation - arXiv, https://www.arxiv.org/pdf/2102.12092</li>
<li>Zero-Shot Text-to-Image Generation - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v139/ramesh21a/ramesh21a.pdf</li>
<li>9 Papers From CS Researchers Accepted to CVPR 2021 | Department of Computer Science, Columbia University, https://www.cs.columbia.edu/2021/cvpr-2021/</li>
<li>AI Best Paper Awards, https://aibestpape.rs/venue/?id=RSS</li>
<li>Best Paper Award - RSS Foundation, https://roboticsfoundation.org/awards/best-paper-award/</li>
<li>Awards - Robotics: Science and Systems, https://roboticsconference.org/2021/program/awards/</li>
<li>TARE Planner - Autonomous Exploration Development Environment, https://www.cmu-exploration.com/tare-planner</li>
<li>Call: 17th International Conference on Social Robotics (ICSR 2025) – ISPR, https://ispr.info/2025/03/31/call-17th-international-conference-on-social-robotics-icsr-2025/</li>
<li>Robotics Summit &amp; Expo, https://www.roboticssummit.com/</li>
<li>Most Influential CVPR Papers (2021-02) – Resources - Paper Digest, https://www.paperdigest.org/2021/02/most-influential-cvpr-papers/</li>
<li>Computation and Language Jun 2021 - arXiv, http://arxiv.org/list/cs.CL/2021-06?skip=0&amp;show=250</li>
<li>NLP Progress in Indigenous Latin American Languages - arXiv, https://arxiv.org/html/2404.05365v2</li>
<li>Peer-Reviewed Conference Papers - Ting-Hao ‘Kenneth’ Huang, https://crowd.ist.psu.edu/publication.html</li>
<li>The 2021 Conference on Empirical Methods in Natural Language Processing, https://aclanthology.org/events/emnlp-2021/</li>
<li>Generally capable agents emerge from open-ended play - Google DeepMind, https://deepmind.google/discover/blog/generally-capable-agents-emerge-from-open-ended-play/</li>
<li>PyTorch builds the future of AI and machine learning at Facebook - AI at Meta, https://ai.meta.com/blog/pytorch-builds-the-future-of-ai-and-machine-learning-at-facebook/</li>
<li>Meta AI Publications, <a href="https://ai.meta.com/results/?page=6&amp;content_types%5B0%5D=publication">https://ai.meta.com/results/?page=6&amp;content_types%5B0%5D=publication</a></li>
<li>Announcing the 2021 recipients of research awards in misinformation and polarization, https://research.facebook.com/blog/2021/9/announcing-the-2021-recipients-of-research-awards-in-misinformation-and-polarization/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>