<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2021년 7월 AI 및 로봇 연구 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2021년 7월 AI 및 로봇 연구 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">2021년 AI 및 로봇 연구 동향</a> / <span>2021년 7월 AI 및 로봇 연구 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2021년 7월 AI 및 로봇 연구 동향</h1>
<h2>1.  2021년 7월 AI 및 로봇 연구의 패러다임 전환</h2>
<p>2021년 7월은 인공지능(AI) 및 로봇 공학 분야의 역사에서 단순한 기술적 진보를 넘어, 연구 개발의 근본적인 패러다임이 전환되는 중대한 변곡점으로 기록된다. 이 시기에 발표된 연구들은 개별적인 성과를 뛰어넘어, 물리 세계의 시뮬레이션, 거대 과학 문제의 해결, 그리고 인간의 창의적 활동 지원이라는 세 가지 핵심 축에서 AI의 역할과 가능성을 재정의했다. 특히 ‘미분 가능한 물리(Differentiable Physics)’, ‘거대 과학 문제 해결(Solving Grand Scientific Challenges)’, 그리고 ’생성 AI의 대중화(Democratization of Generative AI)’라는 키워드는 당시의 기술적 성취를 압축적으로 보여준다.1</p>
<p>이 시기의 가장 두드러진 흐름 중 하나는 ’모든 것의 미분 가능성(Differentiable Everything)’이라는 개념의 부상이었다. 전통적으로 로봇 공학은 기계적 설계와 제어 시스템 프로그래밍이라는 분리된 단계를 거쳤고, AI 이론은 고정된 신경망 내의 파라미터 최적화에 집중했다. 그러나 2021년 7월, 세계 최고 수준의 학회에서 발표된 핵심 연구들은 이러한 경계를 허물었다. 로봇의 물리적 절단 과정을 미분 가능하게 만든 DiSECt, 로봇의 물리적 ‘형태’ 자체를 최적화 가능한 변수로 취급한 MIT의 접촉 인지 로봇 설계, 그리고 학습 과정 자체를 더욱 효율적으로 최적화하는 방법을 제시한 Persistent Evolution Strategies (PES) 등은 모두 경사 하강법(gradient-based optimization)이라는 통일된 프레임워크 아래 물리 세계와 학습 과정을 통합하려는 시도였다. 이는 고정된 시스템 내에서 파라미터를 최적화하던 기존 방식에서 벗어나, 시스템의 물리적 형태와 학습 알고리즘 자체를 포함한 전체 시스템을 동시에 최적화하는 새로운 시대를 예고했다. 이러한 접근법은 특정 작업에 고도로 전문화되고 효율적인 로봇의 자동 설계를 가능하게 하며, 길고 반복적인 수동 설계-제작-테스트 주기를 하나의 계산적 최적화 문제로 전환시킬 잠재력을 보여주었다.</p>
<p>본 보고서는 이러한 패러다임 전환의 중심에 있었던 제38회 국제 머신러닝 학회(ICML 2021)와 2021 로보틱스: 과학 및 시스템 학회(RSS 2021)의 핵심 연구들을 심층적으로 분석한다. 더 나아가, 딥마인드(DeepMind)의 알파폴드 2(AlphaFold 2), OpenAI와 GitHub의 코파일럿(Copilot), 그리고 MIT 컴퓨터 과학 및 인공지능 연구소(CSAIL)의 주요 발표들을 종합하여, 2021년 7월의 기술적 성취가 현재와 미래의 AI 및 로봇 공학에 미치는 심대한 영향을 체계적으로 조망하고자 한다.</p>
<h2>2.  제38회 국제 머신러닝 학회(ICML 2021): 최적화 및 학습 이론의 진보</h2>
<h3>2.1  학회 주요 동향 및 수상 연구 개관</h3>
<p>제38회 국제 머신러닝 학회(The 38th International Conference on Machine Learning, ICML 2021)는 2021년 7월 18일부터 24일까지 전면 가상(Virtual Only) 형식으로 개최되었다.5 이 학회는 머신러닝 분야의 가장 권위 있는 학술대회 중 하나로, 당시 학계의 연구 동향을 파악할 수 있는 중요한 지표가 되었다. ICML 2021에서는 특히 최적화 이론(optimization theory), 자기 지도 학습(self-supervised learning), 그리고 모델의 강건성(robustness)과 같은 머신러닝의 근본적인 문제들에 대한 깊이 있는 탐구가 주를 이루었다. 이는 AI 모델의 성능을 단순히 높이는 것을 넘어, 학습 과정 자체의 효율성과 안정성을 확보하고, 레이블이 없는 데이터로부터 유의미한 표현을 학습하는 능력의 중요성이 부각되고 있음을 시사한다. 학회에서 수여하는 최우수 논문(Outstanding Paper) 및 우수 논문(Honorable Mention) 목록은 이러한 연구 경향을 명확하게 반영하며, 당시 연구 커뮤니티가 가장 중요하게 평가한 학문적 기여를 보여준다.8</p>
<table><thead><tr><th>수상 구분</th><th>논문 제목</th><th>저자</th></tr></thead><tbody>
<tr><td><strong>최우수 논문 (Outstanding Paper)</strong></td><td>Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies</td><td>Paul Vicol, Luke Metz, Jascha Sohl-Dickstein</td></tr>
<tr><td><strong>우수 논문 (Honorable Mention)</strong></td><td>Optimal Complexity in Decentralized Training</td><td>Yucheng Lu, Christopher De Sa</td></tr>
<tr><td><strong>우수 논문 (Honorable Mention)</strong></td><td>Understanding self-supervised learning dynamics without contrastive pairs</td><td>Yuandong Tian, Xinlei Chen, Surya Ganguli</td></tr>
<tr><td><strong>우수 논문 (Honorable Mention)</strong></td><td>Solving high-dimensional parabolic PDEs using the tensor train format</td><td>Lorenz Richter, Leon Sallandt, Nikolas Nüsken</td></tr>
<tr><td><strong>우수 논문 (Honorable Mention)</strong></td><td>Oops I Took A Gradient: Scalable Sampling for Discrete Distributions</td><td>Will Grathwohl, Kevin Swersky, Milad Hashemi, David Duvenaud, Chris Maddison</td></tr>
</tbody></table>
<p>표 1: ICML 2021 주요 수상 논문. 이 표는 ICML 2021에서 학문적 기여도가 가장 높다고 인정받은 연구들을 요약한 것이다.8</p>
<h3>2.2  최우수 논문 심층 분석: Persistent Evolution Strategies (PES)</h3>
<p>ICML 2021 최우수 논문으로 선정된 “Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies“는 순환 신경망(RNN) 학습, 학습된 옵티마이저(learned optimizer) 훈련, 하이퍼파라미터 튜닝 등 광범위한 문제에 내재된 ’펼쳐진 계산 그래프(Unrolled Computation Graph)’의 최적화라는 근본적인 난제를 다루었다.8 이 연구는 기존 방법들이 가졌던 편향, 메모리, 속도의 트레이드오프 관계를 극복하는 새로운 그래디언트 추정 기법인 PES(Persistent Evolution Strategies)를 제안했다.</p>
<h4>2.2.1  Unrolled Computation Graph 최적화의 난제와 기존 접근법의 한계</h4>
<p>Unrolled Computation Graph는 상태가 이전 상태와 파라미터에 의존하여 순차적으로 업데이트되는 계산 과정을 의미한다.9 이러한 구조는 시간적 의존성을 모델링하는 데 필수적이지만, 전체 시퀀스에 걸쳐 파라미터의 영향을 추적하여 그래디언트를 계산하는 것은 매우 어렵다. 이 문제를 해결하기 위한 기존의 대표적인 접근법들은 명확한 한계를 가지고 있었다.</p>
<ul>
<li>
<p><strong>BPTT (Backpropagation Through Time):</strong> 가장 고전적인 방법으로, 전체 시퀀스를 메모리에 펼쳐놓고 마지막 단계부터 시작하여 역전파를 수행한다. 이 방식은 정확한 그래디언트를 계산할 수 있지만, unroll 길이가 길어질수록 메모리 사용량이 선형적으로 증가하고, 전체 시퀀스가 끝날 때까지 단 한 번의 파라미터 업데이트만 가능하여 학습 속도가 매우 느리다는 치명적인 단점이 있다.9</p>
</li>
<li>
<p><strong>Truncated BPTT:</strong> BPTT의 메모리 및 속도 문제를 완화하기 위해 전체 시퀀스를 짧은 조각으로 잘라 각 조각에 대해서만 BPTT를 수행한다. 이는 실용적인 대안이지만, 시퀀스 조각 사이의 그래디언트 흐름을 인위적으로 차단하기 때문에 계산된 그래디언트 추정치에 편향(bias)이 발생하여 최적의 해에 수렴하지 못할 수 있다.</p>
</li>
<li>
<p><strong>RTRL (Real-Time Recurrent Learning):</strong> BPTT와 달리 순방향으로 그래디언트를 계산하고 누적하는 방식이다. 매 시간 단계마다 파라미터 업데이트가 가능하고 편향이 없다는 장점이 있지만, 그래디언트 정보를 저장하기 위한 메모리 및 계산 요구량이 파라미터 개수의 제곱에 비례하여 증가하므로 대규모 신경망에는 사실상 적용이 불가능하다.9</p>
</li>
</ul>
<p>이러한 기존 방법들의 한계는 머신러닝 분야가 더 복잡하고 장기적인 의존성을 갖는 문제를 해결하는 데 있어 중요한 병목으로 작용했다. 특히, 옵티마이저 자체를 학습시키거나(Learned Optimizer), 학습 과정을 제어하는 하이퍼파라미터를 자동으로 튜닝하는 ‘메타-러닝(meta-learning)’ 분야에서는 학습 과정 전체를 하나의 긴 unrolled graph로 보아야 하므로 이 문제가 더욱 심각했다. 이는 단순히 특정 작업을 위한 모델 아키텍처를 개발하는 단계를 넘어, ’학습 과정 자체를 학습’하는 더 높은 수준의 추상화로 나아가는 데 있어 근본적인 해결책이 필요함을 시사했다.</p>
<table><thead><tr><th>기법</th><th>편향 (Bias)</th><th>메모리 사용량</th><th>업데이트 지연</th><th>분산 (Variance)</th><th>핵심 아이디어</th></tr></thead><tbody>
<tr><td><strong>BPTT</strong></td><td>없음</td><td>높음 (<code>O(T)</code>)</td><td>큼 (전체 시퀀스 후 1회)</td><td>낮음</td><td>전체 시퀀스 역전파</td></tr>
<tr><td><strong>Truncated BPTT</strong></td><td>있음</td><td>낮음 (<code>O(K)</code>)</td><td>작음 (부분 시퀀스 후 1회)</td><td>중간</td><td>잘린 시퀀스 역전파</td></tr>
<tr><td><strong>RTRL</strong></td><td>없음</td><td>매우 높음 (<code>O(N^2)</code>)</td><td>없음 (매 스텝 업데이트)</td><td>낮음</td><td>순방향 그래디언트 누적</td></tr>
<tr><td><strong>PES</strong></td><td>없음</td><td>낮음</td><td>없음 (매 스텝 업데이트)</td><td>합리적 수준</td><td>누적 섭동을 이용한 ES 기반 추정</td></tr>
</tbody></table>
<p>표 2: Unrolled Computation Graph 그래디언트 추정 기법 비교. T는 전체 시퀀스 길이, K는 잘린 시퀀스 길이, N은 파라미터 개수를 의미한다. PES는 기존 방법들의 장점을 결합하고 단점을 극복한 혁신적인 접근법임을 알 수 있다.8</p>
<h4>2.2.2  PES의 수학적 원리: 편향 없는 그래디언트 추정을 위한 누적 섭동</h4>
<p>PES는 기존 방법들의 딜레마를 해결하기 위해 진화 전략(Evolution Strategies, ES)이라는 제로차(zeroth-order) 최적화 기법을 독창적으로 활용한다. ES는 그래디언트를 직접 계산하는 대신, 파라미터에 무작위적인 섭동(perturbation)을 가하고 그로 인한 손실 값의 변화를 측정하여 그래디언트 방향을 추정하는 방식이다.10 바닐라 ES의 그래디언트 추정량은 다음과 같이 표현된다:</p>
<p><span class="math math-display">
g_{ES} = \frac{1}{N\sigma^2} \sum_{i=1}^{N} \epsilon^{(i)}L(\theta + \epsilon^{(i)})
</span><br />
여기서 <span class="math math-inline">\epsilon^{(i)}</span>는 평균이 0이고 분산이 <span class="math math-inline">\sigma^2</span>인 정규분포에서 샘플링된 섭동 벡터이며, <span class="math math-inline">L(\theta)</span>는 목적 함수이다. ES는 목적 함수가 미분 불가능하거나 손실 표면이 매우 복잡할 때 유용하며, 특히 손실 표면을 가우시안 커널로 평활화(smoothing)한 효과를 주어 최적화를 안정시키는 장점이 있다.10</p>
<p>PES의 핵심 아이디어는 Truncated BPTT처럼 시퀀스를 짧게 자르되, 각 조각에서 발생하는 편향을 제거하기 위해 섭동을 ’누적’하는 것이다.8 즉, 각 부분 unroll을 시작할 때마다 섭동을 새로 샘플링하는 것이 아니라, 이전까지의 모든 섭동의 합을 현재 파라미터의 ’누적 섭동’으로 간주하고 여기에 새로운 섭동을 더한다. 이를 통해 현재 시간 스텝의 손실이 과거의 모든 파라미터 변화(섭동)에 어떻게 영향을 받았는지를 올바르게 추적할 수 있다.</p>
<p>PES 그래디언트 추정량의 핵심적인 수학적 원리는 다음과 같이 분해하여 이해할 수 있다. 시간 스텝 <span class="math math-inline">t</span>에서의 손실 <span class="math math-inline">L_t</span>에 대한 그래디언트 기여분 <span class="math math-inline">g_{PES,t,\epsilon}</span>는, 시간 스텝 1부터 <span class="math math-inline">t</span>까지 누적된 섭동 <span class="math math-inline">\xi_t</span>와 해당 스텝의 손실 값의 곱으로 근사된다 9:</p>
<p><span class="math math-display">
g_{PES,t,\epsilon} = \frac{1}{\sigma^2} \xi_t L_t (\theta_1 + \epsilon_1,..., \theta_t + \epsilon_t) \quad \text{where} \quad \xi_t = \sum_{\tau=1}^{t} \epsilon_\tau
</span><br />
전체 그래디언트 <span class="math math-inline">g_{PES}</span>는 모든 시간 스텝에 대한 기여분의 합으로 계산된다:</p>
<p><span class="math math-display">
g_{PES} = \sum_{t=1}^{T} g_{PES,t,\epsilon}
</span><br />
이 누적 섭동 <span class="math math-inline">\xi_t</span> 항이 바로 PES가 편향을 제거하는 핵심 메커니즘이다. 각 스텝의 손실 <span class="math math-inline">L_t</span>는 그 시점까지의 모든 파라미터 <span class="math math-inline">\theta_1,..., \theta_t</span>에 의존하므로, <span class="math math-inline">L_t</span>에 대한 그래디언트를 계산할 때는 과거의 모든 파라미터 변화가 미친 영향을 고려해야 한다. PES는 이를 섭동의 누적 합으로 자연스럽게 모델링하여, 시퀀스를 짧게 잘라 계산하면서도 전체 시퀀스에 대한 편향 없는 그래디언트 추정치를 얻을 수 있게 한다.</p>
<h4>2.2.3  연구의 의의 및 메타-러닝 분야에의 기여</h4>
<p>PES의 제안은 메타-러닝 분야에 중요한 돌파구를 마련했다. 이 방법론은 빠른 파라미터 업데이트(매 스텝 가능), 낮은 메모리 사용량, 편향 없는 그래디언트, 그리고 합리적인 수준의 분산이라는 네 가지 핵심 요건을 동시에 만족시켰다.8 이는 기존 방법들이 특정 요건을 만족시키기 위해 다른 요건을 희생해야 했던 트레이드오프 관계를 극복한 것이다.</p>
<p>특히 학습된 옵티마이저나 신경망 아키텍처 탐색(Neural Architecture Search), 하이퍼파라미터 최적화와 같은 메타-러닝 문제들은 매우 긴 unroll 길이를 요구하여 기존 방법으로는 실용적인 학습이 어려웠다.10 PES는 이러한 장기 의존성 문제들을 훨씬 효율적이고 안정적으로 다룰 수 있는 강력한 도구를 제공함으로써, AI가 스스로 학습 방법을 학습하는 ‘메타-러닝’ 연구를 한 단계 발전시키는 데 크게 기여했다. 이는 AI 개발의 자동화를 향한 중요한 진전으로 평가받는다.</p>
<h2>3.  2021 로보틱스: 과학 및 시스템 학회(RSS 2021): 물리적 세계와의 상호작용 지능</h2>
<h3>3.1  학회 주요 동향 및 수상 연구 개관</h3>
<p>2021 로보틱스: 과학 및 시스템 학회(Robotics: Science and Systems, RSS 2021)는 2021년 7월 12일부터 16일까지 가상으로 개최되었다.12 RSS는 로봇 공학 분야의 최상위 학회 중 하나로, 로봇이 물리적 세계를 인식하고, 상호작용하며, 작업을 수행하는 데 필요한 근본적인 과학 및 시스템 기술에 초점을 맞춘다. RSS 2021에서는 특히 효율적인 3D 환경 탐사, 절단이나 접촉과 같은 복잡한 물리적 상호작용의 정밀한 모델링, 그리고 시뮬레이션과 현실 세계 간의 간극(reality gap)을 줄이기 위한 미분 가능한 프레임워크(differentiable framework)의 도입이 핵심적인 연구 주제로 부상했다. 이는 로봇 공학이 단순히 정적인 환경에서의 작업을 넘어, 동적이고 불확실한 실제 환경과 능동적으로 상호작용하는 지능을 구현하는 방향으로 나아가고 있음을 보여준다.</p>
<table><thead><tr><th>수상 구분</th><th>논문 제목</th><th>저자</th></tr></thead><tbody>
<tr><td><strong>최우수 논문 / 세스 텔러 기념 최우수 시스템 논문</strong></td><td>TARE: A Hierarchical Framework for Efficiently Exploring Complex 3D Environments</td><td>Chao Cao, Hongbiao Zhu, Howie Choset, Ji Zhang</td></tr>
<tr><td><strong>최우수 학생 논문</strong></td><td>DiSECt: A Differentiable Simulation Engine for Autonomous Robotic Cutting</td><td>Eric Heiden, Miles Macklin, Yashraj S Narang, Dieter Fox, Animesh Garg, Fabio Ramos</td></tr>
<tr><td><strong>최우수 학생 논문</strong></td><td>Learning Riemannian Manifolds for Geodesic Motion Skills</td><td>Hadi Beik-mohammadi, Soren Hauberg, Georgios Arvanitidis, Gerhard Neumann, Leonel Rozo</td></tr>
</tbody></table>
<p>표 3: RSS 2021 주요 수상 논문. 이 표는 RSS 2021에서 로봇 공학 분야의 발전에 가장 크게 기여했다고 평가된 연구들을 보여준다.13</p>
<h3>3.2  최우수 논문 심층 분석: TARE - 계층적 3D 환경 탐사 프레임워크</h3>
<p>RSS 2021 최우수 논문으로 선정된 “TARE: A Hierarchical Framework for Efficiently Exploring Complex 3D Environments“는 로봇이 미지의 복잡한 3D 환경을 자율적으로 탐사하는 문제에 대한 획기적인 해결책을 제시했다.14</p>
<h4>3.2.1  TARE의 계층적 방법론: 로컬-글로벌 계획의 결합</h4>
<p>TARE의 핵심은 탐사 문제를 두 개의 계층으로 나누어 처리하는 데 있다. 이는 인간이 낯선 환경을 탐색할 때, 당장 눈앞의 공간은 자세히 살피되 멀리 있는 곳은 대략적인 방향만 설정하는 것과 유사한 직관에 기반한다.16</p>
<ul>
<li>
<p><strong>로컬 레벨 (Local Level):</strong> 로봇 주변의 일정 범위(local planning horizon) 내에서는 환경에 대한 고밀도(dense) 데이터를 유지한다. 이 상세한 정보를 바탕으로, 로봇이 실제로 고속 주행할 수 있도록 운동역학적 제약(kinodynamic constraints)을 만족하는 정밀한 경로를 계산한다. 즉, ’지금 당장 어떻게 움직일 것인가’에 대한 최적의 해를 구하는 단계이다.16</p>
</li>
<li>
<p><strong>글로벌 레벨 (Global Level):</strong> 로봇으로부터 멀리 떨어진 영역에 대해서는 데이터를 희소(sparse)하게 유지하고, 전체적인 탐사 방향을 결정하기 위한 대략적인(coarse) 경로를 계산한다. 이는 계산 효율성을 극대화하기 위해 원거리 정보의 세부사항을 과감히 희생하는 전략이다. 이 글로벌 경로는 아직 탐사되지 않은 유망한 지역으로 로봇을 안내하는 역할을 한다.16</p>
</li>
</ul>
<p>TARE는 이 두 계층에서 계산된 경로를 로컬 계획 범위의 경계에서 자연스럽게 연결하여 최종 탐사 경로를 생성한다. 이 프레임워크의 목표는 아직 탐사되지 않은 모든 표면(<span class="math math-inline">\bar{S}</span>)을 커버하면서 전체 경로의 길이(<span class="math math-inline">T^*</span>)를 최소화하는 것이다.16 이는 단순히 다음 단계의 정보 이득을 최대로 하는 근시안적 결정이 아니라, 탐사 임무 전체의 효율성을 최적화하는 접근법이다.</p>
<h4>3.2.2  성능 분석 및 기존 탐사 전략과의 비교 우위</h4>
<p>TARE의 계층적 접근법은 성능 면에서 놀라운 결과를 보여주었다. 실제 로봇을 이용한 실내 및 실외 환경 탐사 실험에서, TARE는 기존 최첨단(State-of-the-art, SOTA) 방법론과 비교하여 탐사 효율성(초당 평균 탐사 부피)을 80% 이상 향상시키면서도, 알고리즘 실행에 필요한 계산량은 50% 미만으로 줄이는 데 성공했다.16</p>
<p>이러한 성능 향상의 핵심 요인은 기존 탐사 전략과의 근본적인 철학 차이에 있다. 많은 프론티어 기반(frontier-based) 또는 정보 이득 기반(information gain-based) 탐사 방법들은 다음 이동 지점을 선택할 때 가장 많은 정보를 얻을 수 있는 곳으로 향하는 탐욕적(greedy) 전략을 사용한다.16 이러한 방식은 단기적으로는 효율적일 수 있으나, 전체적으로는 비효율적인 경로를 생성하거나 특정 지역에 갇히는 문제를 야기할 수 있다. 반면, TARE는 로컬과 글로벌 계획을 통합하여 전체 탐사 경로의 효율성을 최적화함으로써, 더 빠르고 완전한 탐사를 가능하게 했다.16</p>
<h3>3.3  최우수 학생 논문 심층 분석: DiSECt - 미분 가능한 절단 시뮬레이션</h3>
<p>RSS 2021 최우수 학생 논문으로 선정된 “DiSECt: A Differentiable Simulation Engine for Autonomous Robotic Cutting“은 로봇 공학이 물리적 상호작용을 모델링하는 방식에 있어 중요한 진전을 이루었다.13 이 연구는 수술, 식품 가공 등 다양한 분야에서 필수적인 연성 재료(soft material) 절단 작업을 위한 최초의 미분 가능한(differentiable) 시뮬레이터를 제안했다.</p>
<h4>3.3.1  연성 재료 절단 시뮬레이션의 핵심 기술</h4>
<p>DiSECt는 복잡한 절단 과정을 정밀하게 모델링하기 위해 여러 기술을 융합했다.20</p>
<ul>
<li>
<p><strong>유한 요소법 (Finite Element Method, FEM):</strong> 연성 재료가 힘을 받았을 때 어떻게 변형되는지를 시뮬레이션하기 위한 표준적인 물리 기반 모델링 기법이다.20</p>
</li>
<li>
<p><strong>부호 거리 함수 (Signed Distance Fields, SDF):</strong> 칼과 같은 절단 도구와 연성 재료 간의 접촉을 불연속적인 충돌 이벤트가 아닌, 연속적인(continuous) 함수로 모델링한다. 이를 통해 접촉으로 인한 힘의 변화를 미분 가능하게 표현할 수 있다.20</p>
</li>
<li>
<p><strong>연속 손상 모델 (Continuous Damage Model):</strong> DiSECt의 가장 핵심적인 혁신으로, 재료가 잘려나가는 ‘균열(crack)’ 현상을 미분 가능하게 모델링한 것이다. 연구진은 절단이 일어날 것으로 예상되는 평면의 양쪽에 가상의 스프링들을 삽입했다. 칼이 재료에 힘을 가하면, 이 접촉력에 비례하여 해당 위치의 스프링 강성이 점차 약해져 결국 0이 된다. 이 과정을 통해 불연속적인 파괴 현상을 연속적이고 미분 가능한 과정으로 근사하여 시뮬레이션에 통합할 수 있었다.20</p>
</li>
</ul>
<h4>3.3.2  ’미분 가능성’의 활용: 경사 하강법을 통한 최적화</h4>
<p>DiSECt의 진정한 강력함은 시뮬레이터 전체가 ’미분 가능’하다는 사실에서 비롯된다. 이는 시뮬레이션 결과(예: 칼에 가해지는 힘)에 대해 시뮬레이션의 입력 파라미터(예: 재료의 강성, 칼의 이동 경로)로의 그래디언트(gradient), 즉 미분값을 계산할 수 있음을 의미한다. 이 그래디언트 정보는 경사 하강법(gradient descent)과 같은 강력한 최적화 알고리즘을 적용할 수 있게 한다.20</p>
<ul>
<li>
<p><strong>시뮬레이션 파라미터 보정 (Calibration):</strong> 실제 로봇이 감자나 사과를 자를 때 측정한 힘과 변형 데이터를 바탕으로, 시뮬레이션이 실제 현상과 가장 유사한 결과를 내도록 재료의 강성, 마찰 계수 등 수백 개의 시뮬레이션 파라미터를 자동으로 최적화(보정)할 수 있다. 이는 시뮬레이션과 현실 간의 간극을 효과적으로 줄여준다.20</p>
</li>
<li>
<p><strong>제어 정책 최적화 (Control Optimization):</strong> ’절단에 필요한 힘을 최소화하라’와 같은 목적 함수를 설정하고, 이 목적 함수를 최소화하는 칼의 이동 경로를 그래디언트 기반 최적화를 통해 자동으로 찾아낼 수 있다. 연구에서는 이를 통해 단순히 아래로 누르는 동작보다 에너지를 15% 덜 사용하는, 인간의 ’톱질’과 유사한 효율적인 절단 동작이 자발적으로 생성됨을 보여주었다.20</p>
</li>
</ul>
<p>DiSECt는 물리 시뮬레이션을 단순한 예측 도구에서, 로봇의 행동과 물리적 파라미터를 최적화할 수 있는 능동적인 도구로 격상시켰다.</p>
<h3>3.4  MIT CSAIL의 접촉 인지 로봇 설계: 형태와 제어의 통합 최적화</h3>
<p>같은 RSS 2021 학회에서 발표된 MIT CSAIL의 “An End-to-End Differentiable Framework for Contact-Aware Robot Design” 연구는 DiSECt가 보여준 미분 가능한 물리 시뮬레이션의 개념을 로봇 설계 자체로 확장했다.25 이 연구는 로봇의 물리적 형태(morphology)와 제어 방식을 분리하여 설계하는 기존의 패러다임이 비효율적임을 지적하고, 이 둘을 하나의 최적화 문제로 통합하는 종단간(end-to-end) 미분 가능 프레임워크를 제안했다.28</p>
<p>이 프레임워크의 핵심 요소는 다음과 같다:</p>
<ul>
<li>
<p><strong>변형 기반 파라미터화 (Deformation-based Parameterization):</strong> 컴퓨터 그래픽스 분야의 ‘케이지 기반 변형(Cage-based deformation)’ 기법을 도입하여, 로봇 손가락과 같이 복잡하고 유기적인 형태를 소수의 파라미터로 표현했다. 중요한 점은 이 형태 표현 자체가 미분 가능하다는 것이다. 즉, 로봇의 ’모양’을 최적화 알고리즘이 직접 수정할 수 있는 변수로 만든 것이다.26</p>
</li>
<li>
<p><strong>미분 가능한 강체 시뮬레이터:</strong> DiSECt와 유사하게, 접촉이 많은 시나리오를 처리할 수 있는 미분 가능한 시뮬레이터를 사용하여 특정 작업(예: 물건 집기, 조립)에 대한 성능을 평가한다. 이 시뮬레이터는 로봇의 형태 파라미터와 제어 파라미터 모두에 대한 그래디언트를 계산할 수 있다.26</p>
</li>
</ul>
<p>이 연구는 ’모든 것의 미분 가능성’이라는 패러다임의 정점을 보여주는 사례이다. 로봇의 제어 방식뿐만 아니라, 그 물리적 형태까지도 주어진 작업에 최적화되도록 그래디언트 기반으로 자동 설계할 수 있는 길을 열었다. 예를 들어, 특정 물체를 안정적으로 잡기 위해 손가락의 길이, 두께, 곡률 등이 자동으로 최적화될 수 있다. 이는 로봇 설계 과정을 근본적으로 바꾸고, 인간의 직관을 뛰어넘는 고성능 맞춤형 로봇의 등장을 예고하는 중요한 연구이다.</p>
<h2>4.  산업 및 학계 연구소의 혁신적 발표: 과학과 개발의 미래를 열다</h2>
<p>2021년 7월은 학회뿐만 아니라, 세계적인 AI 연구소들에서도 미래 기술의 방향을 제시하는 중대한 발표가 이어진 시기였다. 딥마인드는 생물학 분야의 50년 난제를 해결하며 AI의 과학적 도구로서의 가능성을 입증했고, OpenAI와 GitHub은 인간 개발자의 창의적 작업을 보조하는 새로운 협업 모델을 제시했다. MIT CSAIL은 인간과 로봇의 물리적 상호작용에 대한 안전 패러다임을 재정의하며 미래 보조 로봇 기술의 청사진을 그렸다.</p>
<h3>4.1  딥마인드의 AlphaFold 2: 생물학 50년 난제 해결과 그 파급 효과</h3>
<p>2020년 말 CASP14 대회에서 압도적인 성능을 선보이며 학계를 놀라게 했던 딥마인드의 AlphaFold 2는 2021년 7월 15일, 그 기술적 세부사항을 담은 논문을 세계적인 과학 저널 ’Nature’에 발표하고 관련 소스 코드를 전격 공개함으로써 다시 한번 세계의 주목을 받았다.30 이는 단백질의 아미노산 서열만으로 3차원 구조를 예측하는 ’단백질 접힘 문제(protein folding problem)’라는 생물학계의 50년 된 난제를 해결한 것으로 평가받는다.34</p>
<h4>4.1.1  신경망 아키텍처 분석: Evoformer와 Structure Module</h4>
<p>AlphaFold 2의 성공은 기존의 접근법을 완전히 뛰어넘는 혁신적인 신경망 아키텍처에 기인한다.</p>
<ul>
<li>
<p><strong>입력 처리:</strong> 단일 아미노산 서열을 입력받아, 거대한 단백질 서열 데이터베이스를 검색하여 진화적으로 관련된 서열들의 묶음인 다중 서열 정렬(Multiple Sequence Alignment, MSA)과, 구조가 이미 알려진 유사 단백질인 구조적 템플릿(template)을 생성한다. 이 MSA와 템플릿이 네트워크의 핵심 입력이 된다.36 MSA는 단백질의 특정 위치에 있는 아미노산이 다른 아미노산과 함께 변이하는 공진화(co-evolution) 정보를 담고 있으며, 이는 3차원 구조에서 서로 가까이 위치할 가능성이 높다는 강력한 신호가 된다.</p>
</li>
<li>
<p><strong>Evoformer:</strong> AlphaFold 2의 심장부로, MSA 표현(서열의 진화적 정보)과 쌍(pair) 표현(아미노산 잔기 쌍 간의 거리 정보) 사이의 정보를 반복적으로 교환하며 정제하는 48개의 블록으로 구성된 핵심 모듈이다.38 이 과정에서 자연어 처리 분야에서 혁명을 일으킨 트랜스포머(Transformer)의 어텐션(attention) 메커니즘을 독창적으로 활용한다. 이를 통해 서열상 멀리 떨어져 있지만 3차원 공간에서는 가까운 잔기들 간의 상호작용과 같은 장거리 의존성(long-range dependency)을 효과적으로 학습한다. 이는 기존의 컨볼루션 신경망(CNN)이 국소적인 정보 처리에 집중했던 것과는 근본적으로 다른 접근법이다.33</p>
</li>
<li>
<p><strong>Structure Module:</strong> Evoformer를 통해 고도로 정제된 표현을 입력받아, 이를 실제 원자 단위의 3차원 좌표 구조로 변환하는 모듈이다. 이 모듈은 단백질 구조가 가져야 할 물리적, 기하학적 제약(예: 원자 간의 결합 길이, 각도)을 내재적으로 활용하여, 회전 및 이동에 불변하는(equivariant) 방식으로 최종 구조를 직접 생성한다.33</p>
</li>
<li>
<p><strong>Recycling:</strong> 예측된 최종 구조와 중간 표현들을 다시 Evoformer의 입력으로 되돌려 전체 예측 과정을 3회 반복한다. 이 ‘재활용’ 과정은 예측의 정확도를 점진적으로 개선하는 역할을 한다.33</p>
</li>
</ul>
<h4>4.1.2  AlphaFold DB 공개: 구조 생물학의 민주화</h4>
<p>딥마인드는 기술 공개에 그치지 않고, 유럽 분자생물학 연구소(EMBL-EBI)와 협력하여 인간의 전체 단백질체(proteome) 약 2만 개를 포함, 총 35만 개 이상의 단백질 3차원 구조 예측 결과를 담은 ’AlphaFold 단백질 구조 데이터베이스(AlphaFold DB)’를 구축하고 2021년 7월에 이를 전 세계 연구자들에게 무료로 공개했다.42</p>
<p>이 사건은 AI가 과학 연구의 패러다임을 어떻게 바꿀 수 있는지를 보여주는 상징적인 사례가 되었다. 이전까지 단백질 하나의 구조를 규명하기 위해서는 X선 결정학이나 저온전자현미경 같은 고가의 장비를 이용해 수개월에서 수년의 시간이 소요되었다.34 그러나 AlphaFold DB의 등장으로, 연구자들은 이제 아미노산 서열만 알면 수초 내에 매우 정확한 3차원 구조 예측 결과를 얻을 수 있게 되었다. 이는 마치 현미경이나 입자 가속기와 같은 새로운 ’과학적 도구’가 발명된 것과 같은 효과를 낳았다.</p>
<p>AlphaFold는 더 이상 데이터를 분석하는 보조 도구가 아니라, 이전에는 얻기 매우 어렵거나 불가능했던 새로운 과학 데이터를 생성하는 핵심적인 ’연구 장비’로서의 역할을 하게 된 것이다. 전 세계의 생물학자, 의학자, 신약 개발자들은 이 데이터베이스를 ’단백질 구조를 위한 구글 검색’처럼 활용하여 새로운 가설을 세우고, 실험 방향을 설정하며, 질병의 원인을 규명하고 신약을 설계하는 연구 속도를 극적으로 가속화하고 있다.45 AlphaFold의 성공은 재료 과학, 기후 모델링, 핵융합 에너지 등 다른 거대 과학 난제들에도 AI를 적용하여 돌파구를 마련할 수 있다는 청사진을 제시하며, AI 주도 과학 발견의 새로운 시대를 열었다.</p>
<h3>4.2  OpenAI와 GitHub의 Copilot: AI 페어 프로그래머의 등장</h3>
<p>2021년 6월 29일, GitHub은 OpenAI와의 협력을 통해 AI가 개발자와 함께 코딩하는 ’페어 프로그래머’인 GitHub Copilot의 기술 프리뷰를 발표하며 소프트웨어 개발 패러다임에 큰 파장을 일으켰다.46</p>
<h4>4.2.1  기반 모델 OpenAI Codex와 그 작동 방식</h4>
<p>GitHub Copilot의 핵심 엔진은 OpenAI가 개발한 <strong>Codex</strong> 모델이다. Codex는 GPT-3를 기반으로 하되, GitHub에 공개된 수십억 줄의 소스 코드를 포함한 방대한 양의 코드로 추가 학습(미세 조정)하여 코드 생성 및 이해 능력에 특화된 거대 언어 모델(LLM)이다.47</p>
<p>Copilot은 개발자가 사용하는 코드 에디터(Visual Studio Code 등)의 확장 프로그램 형태로 작동한다. 개발자가 코드에 주석으로 원하는 기능(예: “Sort a list of users by name”)을 작성하거나, 함수 이름을 선언하면, Copilot이 그 문맥을 이해하여 해당 기능을 수행하는 코드 블록 전체를 회색 텍스트로 제안한다. 개발자는 이 제안을 수락하거나, 수정하거나, 다른 제안을 요청할 수 있다. 이는 단순히 코드를 자동 완성하는 수준을 넘어, 개발자의 의도를 파악하고 논리적인 코드 구조를 생성하는 능동적인 작업 보조에 가깝다.47</p>
<h4>4.2.2  생산성 향상과 저작권, 코드 품질에 대한 다각적 고찰</h4>
<p>GitHub Copilot의 등장은 개발자 커뮤니티에 기대와 우려를 동시에 안겨주었다.</p>
<ul>
<li>
<p><strong>긍정적 측면 (생산성 향상):</strong> 가장 큰 장점은 개발 생산성의 비약적인 향상이다. 반복적으로 작성해야 하는 상용구(boilerplate) 코드, 테스트 케이스, 간단한 유틸리티 함수 등을 AI가 대신 작성해 줌으로써 개발자는 더 복잡하고 창의적인 문제 해결에 집중할 수 있다. 또한, 익숙하지 않은 프로그래밍 언어나 라이브러리를 사용할 때 좋은 예제를 빠르게 얻는 학습 도구로도 활용될 수 있다.47</p>
</li>
<li>
<p><strong>부정적 측면 및 논쟁:</strong></p>
</li>
<li>
<p><strong>저작권 및 라이선스 문제:</strong> Copilot이 촉발한 가장 큰 논쟁은 저작권 문제였다. Codex 모델은 GPL, MIT 등 다양한 라이선스를 가진 공개 소스 코드를 학습 데이터로 사용했다. 이로 인해 Copilot이 생성한 코드가 원본 코드의 저작권을 침해하거나, 특정 라이선스의 의무(예: 소스 코드 공개)를 사용자에게 전가할 수 있다는 심각한 법적, 윤리적 문제가 제기되었다. GitHub 측은 학습 과정이 ’공정 이용(fair use)’에 해당한다고 주장했지만, 이는 법적으로 명확히 판결된 바 없는 새로운 영역이었기에 격렬한 논쟁을 불러일으켰다.46</p>
</li>
<li>
<p><strong>코드 품질 및 보안:</strong> AI가 생성한 코드가 항상 완벽하지는 않다. 미묘한 버그나 보안 취약점을 포함할 수 있으며, 개발자가 이를 비판적으로 검토하지 않고 무분별하게 수용할 경우, 오히려 소프트웨어의 전체적인 품질과 안정성을 저해할 수 있다는 우려가 제기되었다. “버그가 생각의 속도보다 더 빠르게 전파될 것“이라는 경고는 이러한 위험을 단적으로 보여준다.47</p>
</li>
<li>
<p><strong>개발자 역량 저하 가능성:</strong> 특히 초급 개발자들이 문제 해결을 위해 깊이 고민하는 과정 대신 AI가 제안하는 ’정답’에 의존하게 될 경우, 장기적으로 프로그래밍의 근본적인 원리를 이해하고 문제 해결 능력을 기르는 데 방해가 될 수 있다는 교육적 우려도 존재한다.</p>
</li>
</ul>
<p>GitHub Copilot은 생성 AI가 인간의 전문적이고 창의적인 작업을 어떻게 변화시킬 수 있는지를 보여주는 첫 번째 대규모 실험이었으며, 기술적 가능성과 함께 해결해야 할 사회적, 법적, 윤리적 과제를 수면 위로 끌어올린 중요한 사건이었다.</p>
<h3>4.3  MIT CSAIL의 보조 로봇 연구: ‘안전한 충돌’ 개념을 통한 인간-로봇 상호작용의 재정의</h3>
<p>2021년 7월 14일, MIT CSAIL 연구진은 거동이 불편한 사람들이 옷을 입는 것을 돕는 보조 로봇에 관한 연구를 발표하며, 인간과 로봇의 물리적 상호작용(Physical Human-Robot Interaction, pHRI)에 대한 기존의 안전 패러다임을 근본적으로 재검토할 필요성을 제기했다.25</p>
<p>산업용 로봇의 안전 규범은 인간과 로봇의 작업 공간을 분리하거나, 접촉이 감지되면 즉시 작동을 멈추는 ’충돌 회피(collision avoidance)’에 기반을 두고 있다. 그러나 옷 입히기와 같은 보조 작업에서는 로봇이 인간의 몸에 지속적으로, 그리고 가깝게 접촉하는 것이 필수적이다. 이러한 상황에서 기존의 ‘충돌 회피’ 패러다임을 그대로 적용하면, 로봇은 인간에게 가까이 다가가기만 해도 작업을 멈추게 되어 임무 수행 자체가 불가능해지는 딜레마에 빠진다.54</p>
<p>이러한 근본적인 한계를 극복하기 위해, MIT 연구진은 안전의 정의를 새롭게 제안했다. 바로 <strong>‘안전한 충돌(safe impacts)’</strong> 이라는 개념이다. 이는 모든 물리적 접촉을 회피의 대상으로 보는 대신, 인간에게 해를 끼치지 않는 가벼운 접촉이나 충격은 ’안전’의 범주 내에서 허용하는 것이다.51 안전의 기준을 기하학적인 ’거리’에서 물리적인 ’힘’으로 전환한 이 개념적 도약은, 인간과 로봇이 물리적으로 협력해야 하는 미래의 보조 로봇 및 개인용 로봇 기술 개발에 있어 핵심적인 원칙을 제공한다.</p>
<p>이 새로운 안전 패러다임을 구현하기 위해, 연구진은 불확실하고 예측하기 어려운 인간의 움직임을 모델링하고, ‘안전한 충돌’ 제약 조건을 만족시키면서 작업 효율성을 극대화하는 로봇의 경로를 실시간으로 계획하는 <strong>모델 예측 제어(Model Predictive Control, MPC)</strong> 알고리즘을 개발했다.54 이 연구는 산업 현장의 안전 모델을 넘어, 가정이나 병원과 같은 비정형 환경에서 인간과 로봇이 안전하고 효과적으로 협력하기 위한 새로운 길을 제시했다는 점에서 큰 의의를 가진다.</p>
<h2>5.  결론: 2021년 7월이 제시하는 AI와 로봇 공학의 미래</h2>
<p>2021년 7월 한 달 동안 발표된 연구들은 인공지능과 로봇 공학 분야가 중대한 전환점을 맞이했음을 명백히 보여주었다. 이 시기의 성과들은 개별 기술의 발전을 넘어, AI가 현실 세계와 상호작용하고, 과학적 발견을 주도하며, 인간의 지적 활동에 깊숙이 관여하는 방식에 대한 근본적인 변화를 예고했다.</p>
<p>첫째, <strong>’미분 가능한 세상(Differentiable World)’으로의 전환이 가속화되었다.</strong> ICML의 PES는 학습 과정 자체를 최적화하는 길을 열었고, RSS에서 발표된 DiSECt와 MIT의 접촉 인지 로봇 설계 연구는 각각 절단이라는 물리 현상과 로봇의 물리적 형태 자체를 미분 가능한 최적화의 대상으로 만들었다. 이는 설계, 제어, 학습이라는 기존의 분리된 영역들을 ’경사 하강법’이라는 통일된 언어로 통합하는 거대한 흐름을 형성했다. 이 패러다임은 앞으로 특정 작업에 고도로 최적화된 로봇과 AI 시스템을 자동으로 설계하는 기술의 기반이 될 것이며, 인간의 직관을 뛰어넘는 새로운 형태와 기능의 등장을 촉진할 것이다.</p>
<p>둘째, <strong>AI는 분석 도구를 넘어 ’과학적 발견의 동반자’로 자리매김했다.</strong> 딥마인드의 AlphaFold 2와 AlphaFold DB의 공개는 AI가 수십 년간 풀리지 않던 거대 과학 난제를 해결할 수 있음을 증명했다. 이는 AI가 단순히 데이터를 분석하고 패턴을 찾는 역할을 넘어, 새로운 과학적 가설을 생성하고 실험을 이끄는 핵심적인 연구 도구가 될 수 있음을 보여준 상징적인 사건이다. 앞으로 재료 과학, 기후 변화, 신약 개발 등 다양한 분야에서 AI를 활용한 과학적 발견이 폭발적으로 증가할 것으로 예상된다.</p>
<p>셋째, <strong>생성 AI와 인간의 협업 모델이 본격적으로 대두되었으며, 이는 기술적 과제와 함께 사회적, 윤리적 숙고를 요구한다.</strong> GitHub Copilot은 AI가 인간의 코딩 작업을 보조하는 강력한 도구가 될 수 있음을 보여주었지만, 동시에 저작권, 코드의 신뢰성, 개발자 역량 등 복잡한 문제를 수면 위로 끌어올렸다. 또한, MIT의 보조 로봇 연구는 인간과 로봇의 물리적 공존을 위해 ’안전’의 개념을 재정의해야 함을 역설했다. 이는 AI 기술이 실험실을 넘어 사회 전반에 통합되기 위해서는 기술적 완성도뿐만 아니라, 법적, 윤리적, 사회적 합의를 형성하는 과정이 필수적임을 시사한다.</p>
<p>결론적으로, 2021년 7월은 AI 이론의 깊이를 더하고(PES), 로봇이 물리 세계를 이해하고 상호작용하는 방식을 근본적으로 바꾸며(TARE, DiSECt), AI가 인류의 과학적 난제를 해결하고(AlphaFold 2) 인간의 창의적 작업을 보조하는(Copilot) 새로운 시대를 여는 중요한 이정표였다. 앞으로 AI 및 로봇 공학 분야는 ’미분 가능한 세상’의 개념을 더욱 확장하고, AI 기반 과학 탐구를 가속화하며, 생성 AI와 인간의 협업에 대한 사회적, 기술적 과제를 해결하는 방향으로 나아갈 것이며, 2021년 7월의 성과들은 그 여정의 중요한 초석이 될 것이다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>America’s AI Action Plan - The White House, https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf</li>
<li>Augmented Reality Meets Artificial Intelligence in Robotics: A Systematic Review - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2021.724798/full</li>
<li>The month in AI marketing news round-up: July 2023 edition - BioStrata, https://www.biostratamarketing.com/blog/the-month-in-ai-marketing-news-round-up-july-edition</li>
<li>Artificial Intelligence Index Report 2021 - arXiv, https://arxiv.org/pdf/2103.06312</li>
<li>2021 Conference - ICML 2025, https://icml.cc/Conferences/2021/index.html</li>
<li>2021 Dates and Deadlines - ICML 2025, https://icml.cc/Conferences/2021/Dates</li>
<li>ICML 2021 Call for Papers, https://icml.cc/Conferences/2021/CallForPapers</li>
<li>ICML 2021 Awards, https://icml.cc/virtual/2021/awards_detail</li>
<li>Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies (Extended Abstract) - IJCAI, https://www.ijcai.org/proceedings/2022/0750.pdf</li>
<li>Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v139/vicol21a/vicol21a.pdf</li>
<li>Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies (Extended Abstract) | Request PDF - ResearchGate, https://www.researchgate.net/publication/362049176_Unbiased_Gradient_Estimation_in_Unrolled_Computation_Graphs_with_Persistent_Evolution_Strategies_Extended_Abstract</li>
<li>Robotics: Science and Systems (RSS), 2021 | ServiceNow AI Research, https://www.servicenow.com/research/event/2021-rss.html</li>
<li>Animesh Garg and collaborators win Best Student Paper Award at RSS’21 | Mathematical &amp; Computational Sciences - University of Toronto Mississauga, https://www.utm.utoronto.ca/math-cs-stats/news/animesh-garg-and-collaborators-win-best-student-paper-award-rss-21</li>
<li>Awards - Robotics: Science and Systems, https://roboticsconference.org/2021/program/awards/</li>
<li>RSS 2021 - AI Best Paper Awards, https://aibestpape.rs/venue/?id=RSS</li>
<li>TARE: A Hierarchical Framework for Efficiently Exploring Complex 3D Environments, http://biorobotics.ri.cmu.edu/papers/paperUploads/RSS_2021.pdf</li>
<li>TARE: A Hierarchical Framework for Efficiently Exploring Complex 3D Environments, https://roboticsconference.org/2021/program/papers/018/index.html</li>
<li>TARE: A Hierarchical Framework for Efficiently Exploring Complex 3D Environments, https://www.ri.cmu.edu/publications/tare-a-hierarchical-framework-for-efficiently-exploring-complex-3d-environments/</li>
<li>TARE Planner - Autonomous Exploration Development Environment, https://www.cmu-exploration.com/tare-planner</li>
<li>DiSECt - Differentiable Cutting Simulator, https://diff-cutting-sim.github.io/</li>
<li>DiSECt: A Differentiable Simulation Engine for Autonomous Robotic Cutting - arXiv, https://arxiv.org/abs/2105.12244</li>
<li>DiSECt: A Differentiable Simulation Engine for … - Robotics, https://www.roboticsproceedings.org/rss17/p067.pdf</li>
<li>NVlabs/DiSECt: Differentiable Cutting Simulator - GitHub, https://github.com/NVlabs/DiSECt</li>
<li>DiSECt: A Differentiable Simulation Engine for Autonomous Robotic Cutting - Eric Heiden, https://eric-heiden.com/publication/2021-disect-rss/poster.pdf</li>
<li>Computer Science and Artificial Intelligence Laboratory (CSAIL) | MIT News | Massachusetts Institute of Technology, https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail?page=30</li>
<li>Novel Contact-Aware Robot Design Helps Optimize Complex and Organic Shapes, https://www.hackster.io/news/novel-contact-aware-robot-design-helps-optimize-complex-and-organic-shapes-b990c0419887</li>
<li>An End-to-End Differentiable Framework for Contact … - Robotics, https://www.roboticsproceedings.org/rss17/p008.pdf</li>
<li>An End-to-End Differentiable Framework for Contact-Aware Robot Design - MIT, https://diffhand.csail.mit.edu/</li>
<li>Contact-aware robot design | MIT News | Massachusetts Institute of Technology, https://news.mit.edu/2021/contact-aware-robot-design-0719</li>
<li>AlphaFold - Wikipedia, https://en.wikipedia.org/wiki/AlphaFold</li>
<li>Highly accurate protein structure prediction with AlphaFold - PubMed, https://pubmed.ncbi.nlm.nih.gov/34265844/</li>
<li>(PDF) Highly accurate protein structure prediction with AlphaFold - ResearchGate, https://www.researchgate.net/publication/353275939_Highly_accurate_protein_structure_prediction_with_AlphaFold</li>
<li>DeepMind’s AlphaFold 2 reveal: Convolutions are out, attention is in | ZDNET, https://www.zdnet.com/article/deepminds-alphafold-2-reveal-what-we-learned-and-didnt-learn/</li>
<li>AlphaFold: a solution to a 50-year-old grand challenge in biology - Google DeepMind, https://deepmind.google/discover/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology/</li>
<li>AlphaFold 2 - SciSoc, https://scisoc.com/alphafold-2/</li>
<li>AlphaFold 2 is here: what’s behind the structure prediction miracle | Oxford Protein Informatics Group, https://www.blopig.com/blog/2021/07/alphafold-2-is-here-whats-behind-the-structure-prediction-miracle/</li>
<li>AlphaFold2: A high-level overview | AlphaFold - EMBL-EBI, https://www.ebi.ac.uk/training/online/courses/alphafold/inputs-and-outputs/a-high-level-overview/</li>
<li>AlphaFold Architecture - UV &lt;-&gt; Bio, https://www.uvio.bio/alphafold-architecture/</li>
<li>Using AlphaFold2 Evoformer for protein function prediction | by AI-Advance | Medium, https://medium.com/@lifengyi_6964/using-alphafold2-evoformer-for-protein-function-prediction-1ae4afa5ddbf</li>
<li>Understanding AlphaFold · GitHub, https://gist.github.com/MikeyBeez/abd09b5510b5a08722da4f7cd9eeefaf</li>
<li>AlphaFold 2: Why It Works and Its Implications for Understanding the Relationships of Protein Sequence, Structure, and Function - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC8592092/</li>
<li>Google DeepMind - Wikipedia, https://en.wikipedia.org/wiki/Google_DeepMind</li>
<li>DeepMind and EMBL release database of predicted protein structures - ΑΙhub - AI Hub, https://aihub.org/2021/07/23/deepmind-and-embl-release-database-of-predicted-protein-structures/</li>
<li>AlphaFold Protein Structure Database, https://alphafold.ebi.ac.uk/</li>
<li>AlphaFold reveals the structure of the protein universe - Google DeepMind, https://deepmind.google/discover/blog/alphafold-reveals-the-structure-of-the-protein-universe/</li>
<li>GitHub Copilot - Wikipedia, https://en.wikipedia.org/wiki/GitHub_Copilot</li>
<li>GitHub Previews Copilot, an OpenAI-Powered Coding Assistant …, https://www.infoq.com/news/2021/07/github-copilot-pair-programmming/</li>
<li>Under the hood: Exploring the AI models powering GitHub Copilot, https://github.blog/ai-and-ml/github-copilot/under-the-hood-exploring-the-ai-models-powering-github-copilot/</li>
<li>Codex vs Claude Code - PromptLayer Blog, https://blog.promptlayer.com/codex-vs-claude-code/</li>
<li>GitHub Copilot · Your AI pair programmer, https://github.com/features/copilot</li>
<li>MIT’s robot could help people with limited mobility dress themselves - Primetel, https://primetel.com.cy/mit-s-robot-could-help-people-with-limited-mobility-dress-themselves-1972</li>
<li>MIT Researchers Develop Robot That Can Help Those with Limited Mobility Get Dressed, https://www.techeblog.com/mit-robot-dressed-limited-mobility/</li>
<li>Delicate robot is learning to help people get dressed - Mashable, https://mashable.com/video/mit-csail-assistive-robot-clothes</li>
<li>Safe Dressing, https://safe-dressing.github.io/</li>
<li>This Robot Can Help People To Dress Themselves | Ubergizmo, https://www.ubergizmo.com/2021/07/robot-help-people-dress-themselves/</li>
<li>Shen Li, https://shenlirobot.github.io/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>