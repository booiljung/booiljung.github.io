<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:1.2. 로봇 vs AI 커뮤니티의 기대치 차이</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>1.2. 로봇 vs AI 커뮤니티의 기대치 차이</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">실전 논문 작성 가이드</a> / <span>1.2. 로봇 vs AI 커뮤니티의 기대치 차이</span></nav>
                </div>
            </header>
            <article>
                <h1>1.2. 로봇 vs AI 커뮤니티의 기대치 차이</h1>
<p>학술적 글쓰기, 특히 논문 작성이라는 전쟁터에서 승리하기 위해서는 적을 알고 나를 아는 것이 필수적이다. 여기서 ’적’이란 다름 아닌 내가 투고할 학회의 리뷰어들과 그들이 속한 커뮤니티의 암묵적인 합의(Consensus)를 의미한다. 인공지능(AI)과 로봇 공학(Robotics)은 겉보기에는 형제처럼 가까운 분야로 보이며, 최근에는 ’Embodied AI(체화된 인공지능)’나 ’Robot Learning(로봇 러닝)’이라는 키워드 아래 급격하게 융합되고 있다. 그러나 연구의 가치를 판단하는 잣대, 즉 ’기여(Contribution)’를 정의하는 철학적 배경과 ’성공적인 연구’를 판별하는 방법론에 있어서는 여전히 심연과 같은 차이가 존재한다.1</p>
<p>이 챕터에서는 단순히 “AI 학회는 이론을 좋아하고 로봇 학회는 실험을 좋아한다“는 식의 표면적인 구분을 넘어, 두 커뮤니티가 연구를 바라보는 근본적인 시각차를 해부한다. 당신의 연구가 탁월함에도 불구하고 거절(Rejection)당하는 비극은 대부분 연구의 품질 때문이 아니라, ’번지수’를 잘못 찾았거나 해당 번지수의 거주민들이 사용하는 언어로 포장하지 못했기 때문에 발생한다.3 우리는 AI 커뮤니티를 주로 컴퓨터 비전(Computer Vision)과 머신러닝(Machine Learning)을 다루는 집단(예: CVPR, NeurIPS, ICML)으로, 로봇 커뮤니티를 물리적 시스템과 그 제어를 다루는 집단(예: ICRA, IROS, RSS)으로 정의하고 논의를 전개할 것이다. 이 두 집단의 기대치 차이를 명확히 이해하는 것은 연구의 방향성을 설정하는 초기 단계부터, 논문의 서론을 구성하고 실험을 설계하는 마지막 단계까지 모든 의사결정의 나침반이 될 것이다.</p>
<h2>1.  기여점(Contribution)에 대한 인식론적 충돌</h2>
<p>두 커뮤니티의 가장 큰 차이는 무엇을 ’과학적 기여’로 인정하느냐에 있다. 이는 각 분야가 발전해 온 역사적 궤적과 밀접하게 연관되어 있다. AI 분야, 특히 딥러닝 기반의 커뮤니티는 데이터라는 디지털 자원 위에서 통계적 패턴을 발견하는 데 집중해 온 반면, 로봇 공학은 물리 세계의 불확실성과 싸우며 시스템을 제어하는 기계/전자 공학적 뿌리를 가지고 있다.</p>
<h3>1.1  “새로운 아키텍처” vs “작동하는 시스템”</h3>
<p>AI 커뮤니티(NeurIPS, CVPR 등)에서 가장 전형적이고 강력한 기여점은 ‘새로운 모델 아키텍처(Novel Architecture)’ 혹은 ’새로운 손실 함수(Novel Loss Function)’의 제안이다. 이들은 기존의 방법론이 해결하지 못했던 데이터 분포의 특성을 수학적 혹은 구조적 혁신을 통해 해결해내는 것을 최우선 가치로 둔다.4 따라서 AI 논문은 방법론(Methodology) 섹션에서 수식의 독창성과 네트워크 구조의 신규성을 증명하는 데 상당한 지면을 할애한다. 만약 기존에 존재하는 모듈들을 조합(Combination)하여 성능을 올렸다면, 그 조합의 논리적 필연성이 수학적으로, 혹은 대규모 실험적으로 매우 강력하게 뒷받침되지 않는 이상 “단순한 엔지니어링(Just Engineering)” 혹은 “기존 방법의 응용(Incremental Application)“이라 폄하받기 십상이다.5 리뷰어들은 “Why represents a novel contribution?“이라는 질문을 끊임없이 던지며, 기존 레고 블록을 단순히 조립한 행위 자체에는 큰 점수를 주지 않는다.</p>
<p>반면, 로봇 커뮤니티(ICRA, IROS, RSS)는 ‘시스템의 통합(System Integration)’ 그 자체를 매우 중요한 과학적 기여로 인정하는 경향이 강하다.6 로봇 공학에서 ’작동한다(It works)’는 명제는 단순히 코드상에서 에러 없이 돌아간다는 의미가 아니다. 센서의 노이즈, 액추에이터의 비선형성, 통신의 지연 시간, 배터리의 전압 강하 등 불확실성이 가득한 물리 세계(Physical World)에서, 이질적인 컴포넌트들이 유기적으로 결합되어 목적한 태스크를 완수해낸다는 것은 그 자체로 엄청난 난이도를 내포하기 때문이다.8 특히 RSS(Robotics: Science and Systems)와 같은 탑티어 학회는 “Systems“라는 단어를 학회명에 포함시킬 정도로, 기존 기술의 창의적 통합을 통해 전례 없는 로봇 행동을 구현해낸 연구를 높게 평가한다.3</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>AI 커뮤니티 (NeurIPS, CVPR, ICML)</strong></th><th><strong>로봇 커뮤니티 (ICRA, IROS, RSS)</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 기여 (Core Contribution)</strong></td><td>알고리즘의 수학적 신규성, 아키텍처의 독창성</td><td>물리적 시스템의 통합, 실제 환경에서의 작동</td></tr>
<tr><td><strong>’Engineering’의 뉘앙스</strong></td><td>부정적 (“Novelty 부족”, “단순 구현”)</td><td>긍정적/중립적 (“복잡성 해결”, “실현 가능성 증명”)</td></tr>
<tr><td><strong>결합(Integration) 연구</strong></td><td>이론적 통찰 없이는 거절 가능성 높음</td><td>실제 환경에서 난제를 해결했다면 높은 가치 부여</td></tr>
<tr><td><strong>선호하는 증거 (Evidence)</strong></td><td>벤치마크 데이터셋의 숫자 (Accuracy, F1)</td><td>실제 로봇 데모 비디오, 성공률 (Success Rate)</td></tr>
<tr><td><strong>재현성 (Reproducibility)</strong></td><td>코드 및 데이터 공개가 필수적</td><td>하드웨어 의존성으로 인해 상대적으로 관대함</td></tr>
</tbody></table>
<p>이러한 차이는 연구자가 논문의 ’Introduction’을 작성할 때 결정적인 영향을 미친다. AI 학회에 투고할 때는 “우리는 A와 B를 결합했다“라고 쓰기보다 “우리는 A와 B의 결합이 왜 이론적으로 최적해에 가까운지를 증명하는 새로운 프레임워크 C를 제안한다“라고 포장해야 한다. 반면 로봇 학회에서는 “우리는 A와 B를 결합하여 실제 환경에서 동작하는 최초의 시스템을 구축했고, 이를 통해 기존에 불가능했던 X 태스크를 수행했다“라고 쓰는 것이 훨씬 설득력 있다.3</p>
<h3>1.2  일반화(Generalization)의 두 가지 의미</h3>
<p>’일반화’라는 단어는 모든 과학 연구의 성배와도 같지만, 두 커뮤니티에서 통용되는 의미는 미묘하게, 때로는 완전히 다르다.</p>
<p>AI 커뮤니티에서 일반화는 주로 통계적 의미를 갖는다. 즉, “학습 데이터(Training Set)와 동일한 분포(I.I.D)를 가지지만 본 적 없는 테스트 데이터(Test Set)에 대한 성능“을 의미한다.11 최근에는 ‘Out-of-Distribution(OOD)’ 일반화나 ‘Zero-shot’ 능력이 화두가 되고 있지만, 이 역시 여전히 디지털 데이터셋 내에서의 분포 이동(Distribution Shift)을 다루는 경우가 많다.12 예를 들어, “강아지 사진으로 학습해서 고양이 사진을 분류할 수 있는가?” 혹은 “낮에 찍은 사진으로 학습해서 밤에 찍은 사진을 인식할 수 있는가?“가 AI 연구자들의 일반화 관심사다.</p>
<p>로봇 커뮤니티에서 일반화는 “물리적 환경의 무작위적 변화에 대한 강건성(Robustness)“을 의미한다.13 시뮬레이션이나 학습 데이터에는 존재하지 않았던 ‘Real-world Nuisance(현실의 골칫거리들)’—예를 들어 조명 조건의 급격한 변화, 바닥 마찰 계수의 미세한 차이, 센서 데이터의 누락, 모터의 열화, 예상치 못한 사람의 개입 등을 극복하고 로봇이 멈추지 않고(Fail-safe) 동작하는지가 진정한 일반화의 척도가 된다.15</p>
<p>물체를 집는(Grasping) 연구를 예로 들어보자.</p>
<ul>
<li><strong>AI 연구자의 질문:</strong> “ImageNet으로 프리트레인된 모델을 사용했을 때, 학습 데이터셋에 없던 새로운 카테고리의 물체(Unseen Object Categories) 이미지를 입력받아도 그립 포인트를 정확히 예측하는가?” (Visual Generalization) 17</li>
<li><strong>로봇 연구자의 질문:</strong> “로봇 팔의 카메라 위치가 1cm 틀어졌거나, 물체가 기름이 묻어 미끄럽거나, 조명이 역광이어서 센서 값이 튀는 상황에서도 실제로 물체를 들어 올리는 데 성공했는가?” (Physical Generalization) 15</li>
</ul>
<p>따라서 로봇 논문에서는 단순한 추론 정확도(Inference Accuracy)보다는 ’실행 성공률(Task Success Rate)’이 훨씬 중요한 지표로 다루어진다. “모델이 99%의 정확도로 파지 위치를 예측했다“고 해도, 실제 로봇이 가서 헛손질을 하거나 물체를 떨어뜨린다면 로봇 커뮤니티에서 그 연구는 실패한 것으로 간주된다. 이것이 바로 로봇 연구자들이 “End-to-End” 평가와 “Closed-loop” 제어를 선호하는 이유이기도 하다.20</p>
<h2>2.  평가 방법론(Evaluation Methodology)의 심연</h2>
<p>두 커뮤니티의 기대치 차이가 가장 극명하게 드러나는 곳은 바로 ‘실험 결과(Results)’ 섹션이다. 무엇을 보여주어야 리뷰어가 고개를 끄덕일 것인가?</p>
<h3>2.1  SOTA(State-of-the-Art) 테이블의 권위 vs 무용론</h3>
<p>AI 논문, 특히 컴퓨터 비전 분야의 논문에서 가장 중요한 “한 장의 그림“을 꼽으라면 단연코 SOTA 비교 테이블일 것이다. COCO, ImageNet, ADE20K와 같은 표준화된 벤치마크 데이터셋에서 소수점 둘째 자리까지의 정확도 경쟁은 논문의 승패를 가르는 핵심 요소다.4 리뷰어들은 “왜 SOTA 모델인 X와 비교하지 않았는가?” 혹은 “성능 향상 폭이 미미하다(Marginal)“라는 이유로 가차 없이 리젝을 놓기도 한다.23 이는 데이터셋이 고정되어 있고(Static), 평가 지표가 명확하기(Clear Metric) 때문에 가능한 문화다. AI 연구자들에게 데이터셋은 곧 실험실이자 심판관이다.</p>
<p>반면 로봇 커뮤니티에서는 이러한 SOTA 테이블의 권위가 상대적으로 낮으며, 때로는 무용론까지 제기된다.3 물론 로봇 분야에서도 KITTI나 EuRoC 같은 데이터셋이 존재하지만(특히 SLAM 분야), 대부분의 로봇 제어(Control)나 조작(Manipulation) 연구는 표준화된 벤치마크를 적용하기가 극도로 어렵다.25 로봇 하드웨어의 기구학적 특성(자유도, 링크 길이)이 다르고, 실험실마다 환경(책상의 높이, 조명, 사용하는 물체)이 제각각이기 때문이다. “내 로봇에서 90% 성공률을 보였다“는 말이 “네 로봇에서도 90% 성공할 것“을 보장하지 않는다.</p>
<p>따라서 로봇 논문의 리뷰어들은 수치적 우월성보다는 실험 설계의 타당성에 집중한다:</p>
<ul>
<li>“이 실험이 현실 세계의 복잡성을 충분히 반영하고 있는가?”</li>
<li>“비교 대상이 된 베이스라인(Baseline) 알고리즘을 공정하게 튜닝했는가?”</li>
<li>“단순히 성공 횟수만 센 것이 아니라, 실패한 케이스(Failure Mode)에 대한 분석이 있는가?” 13</li>
</ul>
<h3>2.2  비디오 증거의 필수성: “백문이 불여일견”</h3>
<p>로봇 학회(ICRA/IROS)에서 ’비디오 첨부(Video Attachment)’는 선택이 아닌 필수 사항에 가깝다.28 텍스트와 그래프로 아무리 훌륭한 성능을 주장해도, 실제 로봇이 부드럽게 움직이는 영상이 없다면 리뷰어들은 본능적으로 의심한다. “시뮬레이션에서는 잘 되겠지, 하지만 실제로는?“이라는 의심은 로봇 연구자들이 넘어야 할 가장 큰 산이다. 로봇 커뮤니티에서 비디오는 단순한 보조 자료가 아니라, 연구의 진실성을 증명하는 ’1차 사료’다.</p>
<p>반면 CVPR과 같은 AI 학회에서는 비디오가 있으면 좋지만(Supplementary), 없다고 해서 결정적인 결격 사유가 되지는 않는다. 그들에게는 코드를 공개하거나 재현 가능한 수치를 제시하는 것이 더 중요하다.28 그러나 최근 로봇 관련 연구가 AI 학회에 투고될 때는 비디오의 중요성이 덩달아 높아지고 있는 추세다.</p>
<h3>2.3  시뮬레이션 실험의 위상 변화: “Pure Simulation“의 딜레마</h3>
<p>“시뮬레이션 실험만으로 논문이 통과될 수 있는가?“는 로봇 연구를 시작하는 대학원생들이 가장 많이 묻는 질문 중 하나다. 이에 대한 답은 학회마다, 그리고 연구의 성격마다 다르다.</p>
<ul>
<li><strong>AI 커뮤니티:</strong> 강화학습(RL) 분야를 제외하고는, 컴퓨터 비전이나 NLP 연구에서 ’시뮬레이션’이라는 개념 자체가 모호하다. 그들은 이미 디지털 데이터셋이라는 가상 공간에서 작업하기 때문이다. 하지만 구체적으로 ’시뮬레이션된 이미지(Synthetic Data)’를 사용하여 학습하고 테스트한 논문들은 최근 ‘Data-Centric AI’ 트렌드와 함께 점차 인정받는 추세다. 다만, 이때의 초점은 “합성 데이터가 실제 데이터를 얼마나 대체할 수 있는가“에 맞춰져 있다.30</li>
<li><strong>로봇 커뮤니티:</strong> 전통적으로 로봇 학회에서 ‘Pure Simulation(순수 시뮬레이션)’ 논문은 리젝 1순위 후보였다.25 “Simulation is doomed to succeed(시뮬레이션은 성공할 수밖에 없다)“라는 격언이 있을 정도로, 물리 엔진의 단순화된 모델링 위에서의 성공은 현실 세계에서의 성공을 보장하지 않는다고 믿기 때문이다.</li>
<li><strong>변화의 바람:</strong> 하지만 최근 트렌드는 변하고 있다. Sim-to-Real 기술(Domain Randomization 등)이 발전하고, Isaac Sim이나 MuJoCo와 같은 고정밀 물리 엔진이 보급되면서 시뮬레이션 결과만으로도 받아들여지는 경우가 늘고 있다.13</li>
<li><strong>조건부 허용:</strong> 단, 조건이 붙는다. 순수 시뮬레이션 논문이 로봇 학회에서 통과되려면, 시뮬레이션 환경이 극도로 정교하거나(High-fidelity), 제안하는 알고리즘이 하드웨어 제약상 실제 실험이 불가능한 경우(예: 수천 대의 군집 로봇, 위험한 재난 환경, 우주 로봇)여야 한다.33 그렇지 않다면, 최소한 “왜 실제 로봇 실험을 하지 않았는가“에 대한 매우 타당한 방어 논리가 필요하다. 특히 CoRL과 같은 학회는 시뮬레이션 연구에 상대적으로 개방적이지만, 여전히 Sim-to-Real에 대한 고찰을 요구한다.34</li>
</ul>
<h3>2.4  시스템 식별(System Identification) vs 도메인 적응(Domain Adaptation)</h3>
<p>같은 문제를 다루면서도 용어와 접근법이 다르다. 시뮬레이션과 현실의 차이를 줄이는 문제를 AI 커뮤니티는 주로 ‘Domain Adaptation(도메인 적응)’ 혹은 ’Transfer Learning(전이 학습)’의 관점에서 바라본다. 즉, 데이터의 분포(Distribution)를 맞추는 통계적 접근을 취한다.35</p>
<p>반면 로봇 커뮤니티는 이를 ‘System Identification(시스템 식별)’ 혹은 ’Model Calibration’의 문제로 접근하는 경향이 있다.13 즉, 시뮬레이터의 물리 파라미터(마찰력, 질량, 관성 모멘트)를 현실과 일치시키거나, 현실의 동역학 모델 오차를 추정하는 제어 이론적 접근을 선호한다. 최근에는 이 두 접근이 융합되어 ’Sim-to-Real RL’이라는 거대한 흐름을 만들고 있지만, 논문을 작성할 때 어떤 용어를 선택하고 어떤 선행 연구를 인용하느냐에 따라 리뷰어에게 주는 인상이 달라진다. AI 학회에 낼 때는 “Visual Domain Gap“을 강조하고, 로봇 학회에 낼 때는 “Dynamics Mismatch“를 강조하는 것이 전략적으로 유리하다.14</p>
<h2>3.  학회별 성향 분석 및 투고 전략 (Venue Profiling)</h2>
<p>연구의 성격을 파악하고 가장 적합한 학회를 선정하는 것은 연구의 ’Fit’을 맞추는 핵심 과정이다. 단순히 Impact Factor나 h5-index만 보고 투고처를 정하는 것은 위험하다. 각 학회는 고유의 ’부족 문화(Tribal Culture)’를 가지고 있다.</p>
<h3>3.1  CVPR / NeurIPS / ICCV: “새로움의 미학”</h3>
<p>이곳은 전 세계에서 투고량이 가장 많고 경쟁이 치열한 곳이다.36 AI 연구의 최전선이자, 트렌드를 주도하는 곳이다.</p>
<ul>
<li><strong>핵심 가치:</strong> Novelty(참신함), SOTA(최고 성능), Theoretical Grounding(이론적 토대).</li>
<li><strong>선호하는 논문:</strong></li>
<li>기존에 없던 새로운 문제를 정의하거나(Task Novelty),</li>
<li>기존 아키텍처의 비효율성을 수학적으로 우아하게 해결하거나(Methodological Novelty),</li>
<li>압도적인 규모의 데이터셋이나 벤치마크를 제안하는 논문.24</li>
<li><strong>주의점:</strong> “로봇에 적용해봤더니 잘 되더라“는 식의 응용 논문은 여기서 환영받지 못한다. 로봇은 단지 애플리케이션의 하나일 뿐, 연구의 본질(Core Contribution)이 로봇 시스템 그 자체에 있다면 리뷰어들은 “이 논문은 로봇 학회(IROS/ICRA)로 가야 한다“고 지적하며 ‘Out of Scope’ 판정을 내릴 것이다.4</li>
<li><strong>특이점:</strong> 최근 NeurIPS는 ‘Datasets and Benchmarks’ 트랙을 신설하여 데이터셋 논문의 위상을 높이고 있으며, 이는 로봇 분야에서도 시뮬레이션 벤치마크나 대규모 데이터셋(Open X-Embodiment 등)을 발표할 수 있는 좋은 창구가 되고 있다.36</li>
</ul>
<h3>3.2  ICRA / IROS: “로봇 공학의 올림픽”</h3>
<p>로봇 분야의 양대 산맥으로, 가장 많은 로봇 연구자가 모이는 곳이다.41 하드웨어부터 소프트웨어까지 모든 스펙트럼을 포괄한다.</p>
<ul>
<li><strong>핵심 가치:</strong> Practicality(실용성), Real-world Validation(실검증), Robustness(강건성).</li>
<li><strong>선호하는 논문:</strong></li>
<li>하드웨어 제약을 고려한 알고리즘 최적화 (예: 임베디드 보드에서의 실시간성 확보),</li>
<li>새로운 메커니즘과 제어기의 통합,</li>
<li>복잡한 비정형 환경(야외, 수중, 재난 현장)에서의 내비게이션 및 조작 성공 사례.</li>
<li><strong>주의점:</strong> 순수 이론이나 시뮬레이션만으로 구성된 논문은 방어가 어렵다. “그래서 이게 실제 로봇에서 돌아가?“라는 질문에 답해야 한다. 또한, 수학적으로 덜 화려하더라도 시스템적으로 완성도가 높으면 억셉될 확률이 높다.31</li>
<li><strong>리뷰어 성향:</strong> “Method가 너무 뻔하다“는 비판보다는 “실험 조건이 너무 단순하다(Toy Example)“는 비판을 더 두려워해야 한다. 리뷰어들은 실제 하드웨어를 다뤄본 경험이 많아 실험의 허점을 기가 막히게 찾아낸다.</li>
</ul>
<h3>3.3  RSS (Robotics: Science and Systems): “엄격한 소수정예”</h3>
<p>로봇 학회 중 가장 억셉률이 낮고(싱글 트랙), 엄격한 기준을 가진 학회다.6 로봇 공학의 ‘과학적’ 측면과 ‘시스템적’ 측면 모두에서 최고 수준을 요구한다.</p>
<ul>
<li><strong>핵심 가치:</strong> Deep Technical Insight + Rigorous System Validation.</li>
<li><strong>선호하는 논문:</strong> 단순히 “돌아가는 시스템“을 넘어, 왜 그렇게 시스템을 설계해야만 했는지에 대한 깊은 통찰(Insight)과 과학적 분석(Scientific Rigor)이 담긴 논문. 크리스 하우저(Kris Hauser)의 “Putting the Systems Back into RSS” 가이드라인은 시스템 논문이 갖춰야 할 덕목(단순 구현이 아닌 일반화 가능한 교훈)을 잘 설명하고 있다.3</li>
<li><strong>전략:</strong> 실험 섹션에서 단순히 성공률만 보여주는 것이 아니라, 시스템의 각 요소가 전체 성능에 미치는 영향을 분석하는 ’Ablation Study’를 매우 깊이 있게 수행해야 한다. “왜 잘되는지”, “언제 실패하는지“를 완벽하게 장악해야 한다.</li>
</ul>
<h3>3.4  CoRL (Conference on Robot Learning): “교집합의 최전선”</h3>
<p>AI와 로봇의 접점인 ’로봇 러닝’을 위해 태어난 학회로, 최근 급부상하고 있다.34 로봇에 머신러닝을 적용하는 연구들이 주를 이룬다.</p>
<ul>
<li><strong>핵심 가치:</strong> Learning for Physical Systems.</li>
<li><strong>특징:</strong> AI 학회의 리뷰어와 로봇 학회의 리뷰어가 섞여 있다. 이는 양날의 검이다. 로봇 리뷰어는 “실제 실험 부족“을, AI 리뷰어는 “학습 알고리즘의 신규성 부족“을 공격할 수 있다.</li>
<li><strong>투고 요건:</strong> 명시적으로 “물리적 로봇(Physical Robot)“에 대한 적용 가능성을 증명해야 한다고 가이드라인에 명시되어 있다.34 순수 시뮬레이션 논문도 받아주지만, Sim-to-Real에 대한 논의가 필수적이다.46 “Learning” 요소가 없으면(순수 제어 이론 등) 리젝될 수 있다.</li>
</ul>
<h2>4.  리뷰 프로세스에서 발생하는 충돌과 대응 전략 (Handling Rebuttals)</h2>
<p>논문을 투고하고 나면 리뷰어와의 싸움이 시작된다. 이때 각 커뮤니티의 ’리뷰어 페르소나’를 이해하는 것이 반박문(Rebuttal) 작성의 핵심이다. 서로 다른 언어를 쓰는 두 집단 사이에서 발생하는 오해를 푸는 과정이다.</p>
<h3>4.1  “Just Engineering” 공격에 대처하기</h3>
<p>AI 백그라운드를 가진 리뷰어가 로봇 시스템 논문을 심사할 때 가장 흔하게 나오는 거절 사유가 바로 “This is just engineering effort(이건 그저 엔지니어링 노력일 뿐이다)“라는 코멘트다.3 이는 알고리즘적 참신함이 부족하고, 기존 컴포넌트를 조립(Assembly)한 것에 불과하다는 폄하의 의미를 담고 있다.</p>
<p><strong>대응 전략:</strong></p>
<ul>
<li><strong>프레이밍의 전환:</strong> “단순 조립“이 아니라, “상충하는 제약 조건(Contradictory Constraints) 하에서의 최적화“임을 강조해야 한다. 예를 들어, “비전 모듈과 제어 모듈을 단순히 연결했다“가 아니라, “비전 모듈의 지연 시간(Latency)이 제어 안정성(Stability)에 미치는 부정적 영향을 분석하고, 이를 보상하기 위해 비동기적(Asynchronous) 파이프라인을 설계했다“라고 서술해야 한다.</li>
<li><strong>문제의 난이도 강조:</strong> 해결하고자 하는 시스템적 문제가 얼마나 복잡하고 어려운 것인지(예: 불확실성, 노이즈, 하드웨어 한계)를 서론에서 강력하게 어필해야 한다. “누구나 할 수 있는 엔지니어링“이 아님을 보여줘야 한다.</li>
<li><strong>일반화 가능한 교훈(Generalizable Lessons) 도출:</strong> 이 시스템 구현을 통해 얻은 지식이 이 로봇뿐만 아니라 다른 로봇 시스템에도 적용될 수 있음을 보여주어야 한다.3 시스템 논문의 가치는 ’Specific Implementation’이 아니라 ’Generalizable Insight’에 있다.</li>
</ul>
<h3>4.2  “Comparison with SOTA” 공격에 대처하기</h3>
<p>반대로 로봇 백그라운드의 연구자가 AI 학회에 냈을 때, 혹은 로봇 학회에서도 AI 성향의 리뷰어를 만났을 때 듣는 말은 “Why didn’t you compare with?“이다. 로봇 실험은 하드웨어 셋업을 바꾸는 것이 매우 비용이 많이 들고(시간적, 물리적), 타 연구자의 하드웨어 환경(센서 위치, 로봇 종류, 작업대 높이 등)을 100% 재현하는 것이 불가능에 가깝다.</p>
<p><strong>대응 전략:</strong></p>
<ul>
<li><strong>재현 불가능성(Irreproducibility)의 인정 및 대안 제시:</strong> “해당 SOTA 논문의 하드웨어 셋업(예: 특수 제작된 그리퍼)이 우리와 달라 직접적인 1:1 비교는 불가능하다“고 정중히 명시하되, “대신 시뮬레이션 환경에서 동일한 조건으로 비교 실험을 수행했다“거나 “해당 알고리즘을 우리의 하드웨어에 맞게 포팅(Porting)하여 최선의 베이스라인을 구축했다“는 노력을 보여줘야 한다.</li>
<li><strong>Task-Level 비교:</strong> 알고리즘 단위의 성능 비교가 어렵다면, 시스템 전체의 태스크 성공률이나 수행 속도 관점에서 비교우위를 점하고 있음을 강조한다. “알고리즘 X보다 정확도는 낮지만, 전체 시스템의 사이클 타임은 20% 빠르다“는 식의 주장이 유효하다.</li>
</ul>
<h3>4.3  “Open Source Code” 요구에 대한 대응</h3>
<p>AI 커뮤니티(특히 NeurIPS/ICLR)는 코드 공개(Reproducibility)에 매우 엄격하다. 코드를 공개하지 않으면 점수가 깎이는 문화가 정착되어 있다.4 로봇 연구의 경우, 하드웨어 드라이버나 독점적인(Proprietary) 미들웨어, 혹은 상용 로봇의 라이선스 문제로 코드 전체 공개가 어려운 경우가 많다.</p>
<p><strong>대응 전략:</strong></p>
<ul>
<li>가능한 범위 내(예: 핵심 알고리즘 파트, 시뮬레이션 환경 설정 파일)에서 최대한 공개하겠다는 의지를 보이고, 하드웨어 종속적인 부분은 추상화(Abstraction)하여 설명했음을 어필해야 한다. “Anonymous GitHub” 링크를 제출 시점에 포함시키는 것이 좋다. “하드웨어가 없어서 코드를 돌려볼 수 없다“는 비판에 대해서는, “시뮬레이션 환경(Docker Container 등)을 제공하여 코드 검증을 가능하게 했다“고 방어하는 것이 최선이다.</li>
</ul>
<h2>5.  최신 트렌드: 경계의 붕괴와 새로운 표준</h2>
<p>최근 몇 년간 ‘Open X-Embodiment’ 프로젝트나 구글의 RT(Robotics Transformer) 시리즈와 같은 연구들은 로봇 데이터도 AI 데이터처럼 대규모로 모으고 학습시키는 것이 가능함을 보여주었다.17 이는 로봇 연구의 평가 기준을 다시 한번 뒤흔들고 있다.</p>
<h3>5.1  파운데이션 모델(Foundation Models)의 등장</h3>
<p>LLM(거대언어모델)과 VLM(비전-언어 모델)이 로봇의 인지 및 계획(Planning) 단계에 적용되면서, 로봇 논문에서도 “GPT-4를 썼더니 되더라“는 식의 접근이 늘어나고 있다.48</p>
<ul>
<li><strong>기대치의 변화:</strong> 이제 단순한 픽 앤 플레이스(Pick-and-Place)는 더 이상 연구 주제가 되기 어렵다. “자연어 명령을 이해하고, 상식(Common Sense)을 활용하여, 본 적 없는 환경에서 복잡한 시퀀스의 작업을 수행하는 것“이 새로운 표준(Standard)이 되고 있다.</li>
<li><strong>평가의 변화:</strong> 정량적 성공률뿐만 아니라, “얼마나 다양한 지시어(Instructions)를 이해하는가?”, “얼마나 긴 호라이즌(Long-horizon)의 계획을 세울 수 있는가?“가 중요한 평가 지표로 등장했다. 로봇의 ’지능’을 평가하는 척도가 ’정확한 제어’에서 ’유연한 적응’으로 이동하고 있다.12</li>
</ul>
<h3>5.2  데이터셋 논문의 재평가</h3>
<p>과거에는 로봇 분야에서 데이터셋 논문이 큰 주목을 받지 못했으나(하드웨어 의존성 때문에), 범용 로봇 모델(Generalist Robot Policies) 연구가 활발해지면서 양질의 로봇 데이터를 수집하고 정제하는 연구 자체가 큰 기여로 인정받기 시작했다. ICRA 2024에서 ‘Open X-Embodiment’ 논문이 베스트 페이퍼를 수상한 것은 이러한 시대적 변화를 상징한다.47 이는 로봇 커뮤니티가 이제 “나만의 실험실“을 넘어 “공유된 데이터” 위에서의 과학을 지향하기 시작했음을 시사한다. 이제 로봇 연구자들도 “데이터 기여(Data Contribution)“를 당당하게 주요 기여점으로 내세울 수 있게 되었다.</p>
<h3>5.3 [요약 및 비교] 주요 학회별 평가 기준 매트릭스</h3>
<p>아래 표는 주요 학회들의 평가 기준과 투고 시 강조해야 할 전략적 포인트를 요약한 것이다. 이를 통해 연구의 포지셔닝을 더욱 명확히 할 수 있다.</p>
<table><thead><tr><th><strong>구분</strong></th><th><strong>CVPR / ICCV / ECCV</strong></th><th><strong>NeurIPS / ICML</strong></th><th><strong>ICRA / IROS</strong></th><th><strong>RSS (Robotics: Science and Systems)</strong></th><th><strong>CoRL (Conf. on Robot Learning)</strong></th></tr></thead><tbody>
<tr><td><strong>주요 분야</strong></td><td>컴퓨터 비전, 패턴 인식</td><td>머신러닝 이론 및 응용</td><td>로봇 공학 전반 (제어, 인지, 메커니즘)</td><td>로봇 과학 및 시스템 (이론+시스템)</td><td>로봇 학습 (AI + Robotics)</td></tr>
<tr><td><strong>평가 1순위</strong></td><td><strong>SOTA 성능</strong>, 벤치마크 결과, 새로운 Task 제안</td><td><strong>알고리즘적 신규성</strong>, 이론적 증명, 범용성</td><td><strong>실제 작동 여부(It works)</strong>, 실험적 검증, 실용성</td><td><strong>통찰력(Insight)</strong>, 수학적 깊이, 시스템적 완성도</td><td><strong>학습(Learning)의 역할</strong>, Sim-to-Real, 데이터 효율성</td></tr>
<tr><td><strong>실험 요구사항</strong></td><td>표준 데이터셋(COCO, ImageNet 등) 필수</td><td>Toy Problem도 허용되나 이론적 깊이 필요</td><td><strong>하드웨어 실험 권장(사실상 필수)</strong>, 비디오 첨부</td><td>고도로 정교한 실험 설계 및 분석 (Ablation)</td><td>물리 로봇 실험 권장 (시뮬레이션만으론 방어 어려움)</td></tr>
<tr><td><strong>방법론 서술</strong></td><td>아키텍처 다이어그램, 수식의 독창성 강조</td><td>수학적 유도 과정, 수렴성 증명 등 강조</td><td>시스템 블록 다이어그램, 하드웨어 구현 디테일</td><td>알고리즘과 시스템의 유기적 결합 논리</td><td>학습 파이프라인, 데이터 수집 및 처리 과정</td></tr>
<tr><td><strong>리뷰어 성향</strong></td><td>성능 수치에 민감, “이전 모델 X와 비교했나?”</td><td>이론적 결함에 민감, “수학적으로 말이 되나?”</td><td>현실성(Reality Gap)에 민감, “가정이 너무 단순하지 않나?”</td><td>논리적 비약에 민감, “이게 왜 작동하는지 아는가?”</td><td>융합적 시각, “ML로서 새롭고 로봇으로서 되는가?”</td></tr>
<tr><td><strong>‘Engineering’</strong></td><td>부정적 (“Novelty가 없다”)</td><td>부정적 (“Application paper다”)</td><td><strong>긍정적</strong> (“훌륭한 시스템 통합이다”)</td><td>중립적 (단순 구현은 X, 시스템적 발견은 O)</td><td>중립적 (학습을 위한 엔지니어링은 인정)</td></tr>
<tr><td><strong>비디오</strong></td><td>Supplementary (선택)</td><td>Supplementary (선택)</td><td><strong>필수 요소 (영상 퀄리티 중요)</strong></td><td>중요 요소</td><td>중요 요소</td></tr>
<tr><td><strong>투고 전략</strong></td><td>비전 기반 로봇 인지(Perception) 연구에 적합</td><td>강화학습 이론, 최적화 이론 연구에 적합</td><td>SLAM, 제어, 내비게이션, 하드웨어 연구에 적합</td><td>깊이 있는 로봇 이론/시스템 분석 연구에 적합</td><td>End-to-End 로봇 조작, 강화학습 응용 연구에 적합</td></tr>
</tbody></table>
<h2>6. 결론: 이중 언어 구사자(Bilingual)가 되라</h2>
<p>로봇과 AI 커뮤니티의 기대치 차이는 단순한 ’선호도’의 문제가 아니라, 각 학문이 발전해 온 역사와 해결하고자 하는 문제의 본질(가상 세계의 정보 처리 vs 물리 세계의 상호작용)에서 기인한다. 성공적인 연구자가 되기 위해서는 이 두 가지 언어를 모두 구사할 줄 아는 ’이중 언어 구사자(Bilingual)’가 되어야 한다.</p>
<ol>
<li><strong>AI 학회에 낼 때:</strong> 로봇은 당신의 완벽한 알고리즘을 증명하기 위한 ’가장 어렵고 멋진 테스트베드’임을 강조하라. 복잡한 시스템 구현 디테일은 부록(Appendix)으로 빼고, 수학적 모델링과 데이터 효율성, 학습의 안정성에 집중하라.</li>
<li><strong>로봇 학회에 낼 때:</strong> 알고리즘은 당신의 로봇이 현실 세계에서 생존하기 위한 ’도구’임을 강조하라. 수학적 우아함보다는 시스템의 견고함, 실험의 광범위함, 그리고 실제 세계에서의 유용성(Utility)을 전면에 내세워라.</li>
<li><strong>경계를 넘을 때:</strong> CoRL과 같은 융합 학회를 타겟팅할 때는, 시뮬레이션과 현실 실험, 이론적 신규성과 시스템적 통합의 균형(Balance)을 맞추는 데 각별한 노력을 기울여라.</li>
</ol>
<p>결국, 좋은 논문은 타겟 독자(Reviewer)가 가려워하는 부분을 정확히 긁어주는 논문이다. 내가 속한 분야가 아니라, 내 논문을 읽을 사람들의 문법으로 글을 쓰는 것, 그것이 “쓰기 전에 이기는(Pre-writing Phase)” 가장 확실한 전략이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>What Is The Difference Between AI and Robotics? | Bernard Marr, https://bernardmarr.com/what-is-the-difference-between-ai-and-robotics/</li>
<li>Robotics vs. AI: Key differences (&amp; how they work together) - Standard Bots, https://standardbots.com/blog/ai-and-robotics-whats-the-difference-and-how-do-they-work-together</li>
<li>Putting the Systems Back into RSS: Recommendations for Reviewers and Authors, https://hauser-kris.medium.com/putting-the-systems-back-into-rss-recommendations-for-reviewers-and-authors-d28c33fa17e4</li>
<li>CVPR 2026 Reviewer Training Material - The Computer Vision Foundation, https://cvpr.thecvf.com/Conferences/2026/ReviewerTrainingMaterial</li>
<li>[D] Too many AI researchers think real-world problems are not relevant - Reddit, https://www.reddit.com/r/MachineLearning/comments/ieo3aw/d_too_many_ai_researchers_think_realworld/</li>
<li>Author Information - Robotics: Science and Systems, https://roboticsconference.org/information/authorinfo/</li>
<li>An Integrated Robot System Architecture, <a href="https://www.cs.jhu.edu/~rht/RHT%20Papers/1983/Integrated%20Robot%20System.pdf">https://www.cs.jhu.edu/~rht/RHT%20Papers/1983/Integrated%20Robot%20System.pdf</a></li>
<li>Implementing best practices for systems integration and distributed software development in service robotics - Fraunhofer-Publica, https://publica.fraunhofer.de/bitstreams/c7918b5c-2e88-4ee8-9e33-b2ec39241f59/download</li>
<li>Integrating Lean Principles into Lean Robotics Systems for Enhanced Production Processes, https://www.mdpi.com/2079-8954/13/2/106</li>
<li>Research and practice on system integration of shock absorber welding robot workstation, https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13445/134450J/Research-and-practice-on-system-integration-of-shock-absorber-welding/10.1117/12.3054413.full</li>
<li>AI and cognitive sciences: can AIs be endowed with a human-like ability to generalize?, https://hellofuture.orange.com/en/ai-and-cognitive-sciences-can-ais-be-endowed-with-a-human-like-ability-to-generalize/</li>
<li>AI model could boost robot intelligence via object recognition | Stanford Report, https://news.stanford.edu/stories/2025/10/ai-model-functional-correspondence-tools-robot-autonomy</li>
<li>Robot Learning From Randomized Simulations: A Review - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2022.799893/full</li>
<li>Evaluating Real-World Robot Manipulation Policies in Simulation - arXiv, https://arxiv.org/html/2405.05941v1</li>
<li>Disambiguate Gripper State in Grasp-Based Tasks: Pseudo-Tactile as Feedback Enables Pure Simulation Learning - arXiv, https://arxiv.org/html/2503.23835v1</li>
<li>Meta: Advanced AI Research | Boston Dynamics, https://bostondynamics.com/case-studies/advanced-ai-adding-capabilities-to-spot-through-research/</li>
<li>What Matters in Learning from Large-Scale Datasets for Robot Manipulation - OpenReview, https://openreview.net/forum?id=LqhorpRLIm</li>
<li>Building Generalist Robot Policy from Pre-trained Visual Representations - OpenReview, https://openreview.net/forum?id=9GKMCecZ7c</li>
<li>(PDF) Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models - ResearchGate, https://www.researchgate.net/publication/387183701_Towards_Generalist_Robot_Policies_What_Matters_in_Building_Vision-Language-Action_Models</li>
<li>An evaluation of closed-loop control options for continuum manipulators - IEEE Xplore, https://ieeexplore.ieee.org/document/6224735/</li>
<li>Comparison Study between Open-Loop and Closed-Loop Identification for Industrial Hydraulics Actuator System - ijmerr, https://www.ijmerr.com/2024/IJMERR-V13N5-516.pdf</li>
<li>An evaluation of closed-loop control options for continuum manipulators - ResearchGate, https://www.researchgate.net/publication/239763468_An_evaluation_of_closed-loop_control_options_for_continuum_manipulators</li>
<li>CVPR 2025 Reviewer Guidelines - The Computer Vision Foundation, https://cvpr.thecvf.com/Conferences/2025/ReviewerGuidelines</li>
<li>Datasets Benchmarks 2024 - NeurIPS 2025, https://neurips.cc/virtual/2024/events/datasets-benchmarks-2024</li>
<li>GR-AttNet: Robotic grasping with lightweight spatial attention mechanism - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC12677535/</li>
<li>Visual SLAM: What Are the Current Trends and What to Expect? - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC9735432/</li>
<li>Author Information - Robotics: Science and Systems, https://roboticsconference.org/2023/information/authorinfo/</li>
<li>ICRA 2026 Papers - Submission closed, https://2026.ieee-icra.org/contribute/icra-2026-papers/</li>
<li>Visual SLAM for Automated Driving: Exploring the Applications of Deep Learning - CVF Open Access, https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w9/Milz_Visual_SLAM_for_CVPR_2018_paper.pdf</li>
<li>Sim-on-Wheels: Physical World in the Loop Simulation for Self-Driving - IEEE Xplore, https://ieeexplore.ieee.org/ielaam/7083369/10287676/10287403-aam.pdf</li>
<li>Simulation-Based Internal Models for Safer Robots - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2017.00074/full</li>
<li>On the use of simulation in robotics: Opportunities, challenges, and suggestions for moving forward - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC7817170/</li>
<li>Self-assembly and Self-repair during Motion with Modular Robots - White Rose Research Online, https://eprints.whiterose.ac.uk/id/eprint/186796/7/electronics_11_01595.pdf</li>
<li>Instruction for Reviews - CoRL 2025, https://www.corl.org/contributions/instruction-for-reviews</li>
<li>A Robotic Benchmark For Continual Reinforcement Learning, https://proceedings.neurips.cc/paper_files/paper/2021/file/ef8446f35513a8d6aa2308357a268a7e-Paper.pdf</li>
<li>Reflecting on the 2025 Review Process from the Datasets and Benchmarks Chairs, https://blog.neurips.cc/2025/09/30/reflecting-on-the-2025-review-process-from-the-datasets-and-benchmarks-chairs/</li>
<li>NeurIPS 2025 Decision Storm: When Full Scores Still Mean Rejection | CSPaper Forum, https://forum.cspaper.org/topic/161/neurips-2025-decision-storm-when-full-scores-still-mean-rejection</li>
<li>NeurIPS 2025: A Guide to Key Papers, Trends &amp; Stats | IntuitionLabs, https://intuitionlabs.ai/articles/neurips-2025-conference-summary-trends</li>
<li>arXiv:2310.08864v3 [cs.RO] 17 Oct 2023 - Open X-Embodiment: Robotic Learning Datasets and RT-X Models, https://robotics-transformer-x.github.io/paper.pdf</li>
<li>Open X-Embodiment: Robotic Learning Datasets and RT-X Models - arXiv, https://arxiv.org/html/2310.08864v6</li>
<li>Publishing in journals or conferences : r/AskAcademia - Reddit, https://www.reddit.com/r/AskAcademia/comments/tqwdzf/publishing_in_journals_or_conferences/</li>
<li>Publishing a computer vision work at ICRA or IROS? - robotics - Reddit, https://www.reddit.com/r/robotics/comments/15o4kqg/publishing_a_computer_vision_work_at_icra_or_iros/</li>
<li>The 2022 Robotics: Science and Systems Conference - Seita’s Place, https://danieltakeshi.github.io/2022/12/11/rss-2022/</li>
<li>[D] AAAI/IJCAI for ML and related papers : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/sbtsrf/d_aaaiijcai_for_ml_and_related_papers/</li>
<li>Instruction for Authors - CoRL 2025, https://www.corl.org/contributions/instruction-for-authors</li>
<li>Beyond Pick-and-Place: Tackling Robotic Stacking of Diverse Shapes | OpenReview, https://openreview.net/forum?id=U0Q8CrtBJxJ</li>
<li>Robotic Learning Datasets and RT-X Models : Open X-Embodiment Collaboration&gt;0, https://asu.elsevierpure.com/en/publications/open-x-embodiment-robotic-learning-datasets-and-rt-x-models-open-/</li>
<li>Real-World Robot Applications of Foundation Models: A Review - arXiv, https://arxiv.org/html/2402.05741v2</li>
<li>Real-world robot applications of foundation models: a review - Taylor &amp; Francis Online, https://www.tandfonline.com/doi/full/10.1080/01691864.2024.2408593</li>
<li>ICRA 2024 Best Paper - Henrik I Christensen, https://hichristensen.com/post/icra-2024-best-paper/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>