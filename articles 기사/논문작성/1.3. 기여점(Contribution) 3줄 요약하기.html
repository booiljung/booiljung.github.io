<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:1.3. 기여점(Contribution) 3줄 요약하기</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>1.3. 기여점(Contribution) 3줄 요약하기</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">실전 논문 작성 가이드</a> / <span>1.3. 기여점(Contribution) 3줄 요약하기</span></nav>
                </div>
            </header>
            <article>
                <h1>1.3. 기여점(Contribution) 3줄 요약하기</h1>
<p>논문 작성의 거대한 여정에서 가장 결정적이고도 고통스러운 순간을 꼽으라면, 단연코 자신의 연구 성과를 단 몇 줄의 문장으로 압축해야 하는 순간일 것이다. 수개월, 길게는 수년에 걸쳐 수행한 방대한 실험과 복잡한 수식, 그리고 수없이 많은 시행착오의 결과물을 단 세 개의 ’불릿 포인트(Bullet Point)’로 요약하는 행위는 단순한 텍스트 줄이기가 아니다. 이는 연구의 본질을 관통하는 철학적 정제 과정이며, 동시에 리뷰어(Reviewer)와 독자를 설득하기 위한 가장 정교한 ’세일즈 피치(Sales Pitch)’이다.</p>
<p>이 장에서는 논문의 서론(Introduction) 마지막에 위치하여 논문의 생사를 가르는 ’기여점(Contribution)’을 작성하는 방법을 심도 있게 다룬다. 특히 인공지능(AI)과 로봇 공학(Robotics)이라는 두 거대 분야가 요구하는 서로 다른 기대치를 분석하고, 이를 충족시키기 위한 구체적인 작문 전략, 단어 선택, 그리고 논리 구조를 해부한다.</p>
<h2>1.  기여점의 전략적 중요성: 왜 3줄인가?</h2>
<h3>1.1  리뷰어의 인지 부하와 생존 본능</h3>
<p>현대 학술계, 특히 CVPR, NeurIPS, ICRA와 같은 탑티어(Top-tier) 컨퍼런스는 매년 기하급수적으로 증가하는 투고량에 신음하고 있다. 예를 들어 CVPR이나 NeurIPS의 경우 매년 1만 편 이상의 논문이 투고되며, 리뷰어 한 명이 담당해야 할 논문의 수는 5편에서 10편에 이른다.1 이러한 상황에서 리뷰어는 극심한 ‘인지 부하(Cognitive Load)’ 상태에 놓이게 된다. 그들은 논문을 처음부터 끝까지 정독하며 숨겨진 가치를 찾아주는 탐험가가 아니다. 그들은 제한된 시간 내에 논문의 결함(Flaw)을 찾아내고, 게재 거절(Rejection) 혹은 승인(Acceptance)의 명분을 만들어야 하는 평가자들이다.</p>
<p>이때 리뷰어의 시선이 초록(Abstract)을 훑은 직후 가장 먼저 향하는 곳이 바로 서론의 마지막, 즉 ‘기여점(Contribution)’ 섹션이다. 이곳은 리뷰어에게 “이 논문은 이러이러한 새로운 가치가 있으니 읽어볼 만하다“라는 이정표를 제시하는 곳이다. 만약 이 3줄 요약이 모호하거나, 기존 연구와의 차별점이 불분명하거나, 기술적 깊이가 얕아 보인다면, 리뷰어는 본문을 읽기도 전에 이미 마음속으로 ‘Weak Reject’ 버튼에 손을 올리게 된다. 따라서 기여점 작성은 논문의 첫인상을 결정짓는 것을 넘어, 논문의 생존 확률을 좌우하는 가장 전략적인 텍스트이다.4</p>
<h3>1.2  3단 구성의 인지적 효율성</h3>
<p>학계에서 불문율처럼 통용되는 ’3개의 기여점’은 우연의 산물이 아니다. 이는 인간이 정보를 처리하고 기억하는 방식에 최적화된 구조이다. 심리학적으로 정보의 단위(Chunk)가 3개를 넘어갈 때 단기 기억의 효율이 급격히 떨어진다는 점을 고려할 때, 3줄 요약은 다음과 같은 논리적 완결성을 갖춰야 한다.</p>
<ol>
<li><strong>무엇이 새로운가? (Novelty):</strong> 기존 연구의 한계를 지적하고 이를 극복하는 새로운 아이디어의 제안.</li>
<li><strong>어떻게 해결했는가? (Methodology):</strong> 제안된 아이디어를 실현하는 핵심 기술적 메커니즘 혹은 아키텍처.</li>
<li><strong>무엇이 좋아졌는가? (Impact/Evidence):</strong> 제안된 방법이 가져오는 정량적 성능 향상, 효율성 증대, 혹은 새로운 가능성의 확장.</li>
</ol>
<p>이 3단 구조는 리뷰어가 리뷰 폼(Review Form)의 ‘Summary’ 및 ‘Strengths’ 항목을 작성할 때 그대로 복사해서 붙여넣기(Copy &amp; Paste) 하기 좋은 형태를 제공한다. 리뷰어가 내 논문을 요약하기 쉽게 만들어주는 것, 그것이 바로 리뷰어를 내 편으로 만드는 첫걸음이다.6</p>
<h2>2.  분야별 기대치 분석: AI vs 로봇</h2>
<p>AI 커뮤니티(CVPR, NeurIPS, ICLR 등)와 로봇 커뮤니티(ICRA, IROS, RSS 등)는 기술적 뿌리를 공유하면서도, ’훌륭한 기여(Good Contribution)’를 정의하는 기준에서 미묘하지만 결정적인 차이를 보인다. 자신의 연구가 속한 분야의 ’언어’와 ’가치관’을 정확히 파악하고 포지셔닝하는 것이 필수적이다.7</p>
<h3>2.1  인공지능(AI) 분야: CVPR, NeurIPS, ICCV</h3>
<p>AI 분야는 ’데이터’와 ’알고리즘’의 싸움터다. 이곳에서의 기여는 주로 수학적 독창성, 아키텍처의 효율성, 그리고 벤치마크 성능의 갱신(State-of-the-Art, SOTA)에 집중된다.9</p>
<h4>2.1.1  알고리즘적 신규성 (Algorithmic Novelty)</h4>
<p>새로운 손실 함수(Loss Function)를 유도하거나, 기존 최적화 기법의 수렴 속도를 개선하거나, 어텐션 메커니즘(Attention Mechanism)의 연산 복잡도를 줄이는 등의 기여가 환영받는다. 이때 중요한 것은 단순히 “성능이 올랐다“가 아니라, “왜(Why)” 성능이 올랐는지에 대한 이론적 근거나 구조적 통찰(Insight)을 제시하는 것이다.</p>
<ul>
<li><strong>작성 포인트:</strong> 수학적 우아함, 일반화 가능성(Generalization), 재현 가능성(Reproducibility).</li>
<li><strong>예시:</strong> “우리는 비볼록(Non-convex) 최적화 문제에서의 수렴성을 보장하는 새로운 경사 하강법 스케줄러를 제안한다.”</li>
</ul>
<h4>2.1.2  아키텍처 혁신 (Architectural Innovation)</h4>
<p>기존의 CNN이나 Transformer 구조를 변경하여 파라미터 효율성을 높이거나, 새로운 모달리티(Modality) 융합 방식을 제안하는 경우다. 최근에는 거대 언어 모델(LLM)이나 파운데이션 모델(Foundation Model)과 관련된 아키텍처 수정이 주요 트렌드다.3</p>
<ul>
<li><strong>작성 포인트:</strong> 모델의 구조적 특징, 파라미터 수 대비 성능, 추론 속도(Inference Time).</li>
<li><strong>예시:</strong> “우리는 계층적 토큰 병합(Hierarchical Token Merging)을 통해 Vision Transformer의 연산량을 40% 절감하면서도 정확도를 유지하는 아키텍처를 제안한다.”</li>
</ul>
<h4>2.1.3  데이터셋 및 벤치마크 (Dataset &amp; Benchmark)</h4>
<p>단순한 모델 제안을 넘어, 분야 전체가 도전할 수 있는 새로운 문제 정의와 이를 평가할 수 있는 대규모 데이터셋을 공개하는 것은 매우 강력한 기여로 인정받는다. 특히 NeurIPS의 경우 ’Datasets and Benchmarks Track’을 별도로 운영할 정도로 이를 중시한다.9</p>
<ul>
<li><strong>작성 포인트:</strong> 데이터의 규모(Scale), 다양성(Diversity), 주석(Annotation)의 품질, 그리고 공개 여부.</li>
<li><strong>예시:</strong> “우리는 100만 장 규모의 다중 모달 의료 영상 데이터셋을 구축하고, 이를 위한 새로운 편향(Bias) 평가 지표를 제안하여 커뮤니티에 공개한다.”</li>
</ul>
<h3>2.2  로봇 공학(Robotics) 분야: ICRA, IROS, RSS</h3>
<p>로봇 분야는 ’물리적 세계(Physical World)’와의 상호작용을 다룬다. 따라서 AI 분야에서 중시하는 순수 알고리즘 성능보다는 시스템의 통합(Integration), 실제 환경에서의 강건성(Robustness), 그리고 하드웨어 제약 사항(Constraints) 극복 여부가 핵심 기여가 된다.12</p>
<h4>2.2.1  시스템 통합 및 완성도 (System Integration)</h4>
<p>로봇 연구에서는 개별 컴포넌트(인식, 계획, 제어)가 제각기 훌륭하더라도, 이를 하나의 시스템으로 합쳤을 때 실제 작동하지 않으면 의미가 없다. 기존에 존재하는 알고리즘들을 독창적으로 결합하여 이전에 불가능했던 복잡한 태스크(예: 설거지, 재난 현장 구조)를 수행해내는 ’시스템 논문(System Paper)’은 RSS와 같은 학회에서 높은 가치를 인정받는다.12</p>
<ul>
<li><strong>작성 포인트:</strong> 시스템 아키텍처의 설계 철학, 컴포넌트 간의 유기적 연결, 실시간성(Real-time) 확보.</li>
<li><strong>예시:</strong> “우리는 시각, 촉각, 고유수용성 감각을 융합하여 미지의 물체를 파지하고 조작할 수 있는 종단간(End-to-End) 로봇 시스템을 구축하였다.”</li>
</ul>
<h4>2.2.2  현실 세계 검증 (Real-world Validation)</h4>
<p>시뮬레이션(Sim)에서만 작동하는 알고리즘은 로봇 학회에서 ‘반쪽짜리’ 취급을 받기 쉽다. 실제 로봇(Real Robot)을 이용하여, 조명 변화, 센서 노이즈, 기구학적 오차 등 예측 불가능한 현실 요소들을 극복하고 작동함을 증명해야 한다.14 Sim-to-Real 기술이나 실제 하드웨어 실험 결과는 필수적인 기여 요소다.</p>
<ul>
<li><strong>작성 포인트:</strong> 실험의 난이도, 환경의 다양성, 실패 케이스 분석, 장시간 자율 구동 성공률.</li>
<li><strong>예시:</strong> “우리는 쿼드루펠(4족 보행) 로봇을 이용하여 험지, 계단, 미끄러운 바닥 등 다양한 실제 환경에서 2시간 이상의 자율 보행 실험을 통해 제안된 제어기의 강건성을 입증했다.”</li>
</ul>
<h4>2.2.3  하드웨어-소프트웨어 공진화 (HW-SW Co-design)</h4>
<p>특정 알고리즘을 돌리기 위해 하드웨어를 최적화하거나, 반대로 하드웨어의 특성을 이용해 알고리즘을 단순화하는 접근은 로봇 분야만의 독특한 기여다. 임베디드 프로세서의 한계를 극복하기 위한 경량화나, 소프트 로봇(Soft Robot)의 기구학적 특성을 이용한 제어 등이 이에 해당한다.</p>
<ul>
<li><strong>작성 포인트:</strong> 계산 자원의 제약 해결, 에너지 효율성, 기구학적 이점 활용.</li>
</ul>
<h3>2.3  요약: 분야별 기여점 비교표</h3>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>인공지능 (AI: CVPR, NeurIPS)</strong></th><th><strong>로봇 공학 (Robotics: ICRA, RSS)</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 가치</strong></td><td>Novelty, SOTA Performance, Theory</td><td>Robustness, Integration, Real-world Utility</td></tr>
<tr><td><strong>선호하는 단어</strong></td><td>Architecture, Loss, Convergence, Accuracy</td><td>System, Deployment, Hardware, Robustness</td></tr>
<tr><td><strong>검증 방식</strong></td><td>Standard Benchmark (ImageNet, COCO)</td><td>Real Robot Experiment, Sim-to-Real</td></tr>
<tr><td><strong>재현성 요구</strong></td><td>Code Release, Model Checkpoint</td><td>Hardware Design(CAD), System Setup Details</td></tr>
<tr><td><strong>기여의 형태</strong></td><td>“새로운 수학적 모델을 만들었다”</td><td>“실제 세상에서 작동하게 만들었다”</td></tr>
</tbody></table>
<h2>3.  기여점 작성의 3단계 공식 (The 3-Step Formula)</h2>
<p>기여점을 작성할 때는 막연히 “우리는 열심히 했다“가 아니라, 철저히 논리적이고 구조적인 접근이 필요하다. 다음의 3단계 공식을 따라 작성해 보자.</p>
<h3>3.1  [1단계] Hook: 문제의 재정의와 해결책의 제안 (Novelty)</h3>
<p>첫 번째 불릿 포인트는 독자의 주의를 끄는 ’훅(Hook)’이다. 기존 연구들이 해결하지 못한 난제(Gap)를 명확히 지적하고, 이를 해결하기 위한 당신만의 ’새로운 무기’를 선언해야 한다.4</p>
<ul>
<li><strong>필수 요소:</strong></li>
<li>해결하고자 하는 구체적인 문제 (Specific Problem)</li>
<li>제안하는 핵심 아이디어의 이름 (Proposed Method Name)</li>
<li>기존 방법과의 차별성 (Differentiation)</li>
<li><strong>나쁜 예:</strong> “우리는 물체 인식을 위한 새로운 딥러닝 모델을 제안한다.” (너무 밋밋함)</li>
<li><strong>좋은 예 (AI):</strong> “우리는 텍스트-이미지 생성 모델의 고질적인 문제인 ‘물체 환각(Object Hallucination)’ 현상을 해결하기 위해, 레이어별 기능적 역할을 분석하여 디코딩 과정을 제어하는 새로운 ‘Layer-Aware Decoding’ 전략을 제안한다.” 16</li>
<li><strong>좋은 예 (Robot):</strong> “우리는 시각 정보가 부재한 연기 속 환경에서도 로봇이 안전하게 주행할 수 있도록, 레이더와 열화상 카메라를 적응적으로 융합하는 ‘Multi-Spectral Navigation’ 프레임워크를 제안한다.”</li>
</ul>
<h3>3.2  [2단계] Enabler: 기술적 깊이와 방법론의 구체화 (Methodology)</h3>
<p>두 번째 포인트는 첫 번째에서 제안한 해결책이 ‘어떻게(How)’ 작동하는지를 설명하는 ’기술적 증거(Enabler)’다. 단순히 “트랜스포머를 썼다“가 아니라, 트랜스포머를 <em>어떻게 변형해서</em> 문제를 해결했는지를 보여주어야 한다. 기술적 용어(Technical Terms)를 적절히 배치하여 전문성을 드러내라.9</p>
<ul>
<li><strong>필수 요소:</strong></li>
<li>핵심 알고리즘/모듈의 구체적 명칭</li>
<li>작동 원리의 핵심 (Key Mechanism)</li>
<li>이론적 근거 혹은 설계 철학</li>
<li><strong>나쁜 예:</strong> “우리는 어텐션 메커니즘을 사용하여 성능을 높였다.” (누구나 할 수 있는 말)</li>
<li><strong>좋은 예 (AI):</strong> “제안된 모델은 ’스펙트럼 불안정성(Spectral Instability)’을 완화하기 위해 주파수 도메인에서의 정규화 기법을 도입하였으며, 이를 통해 기존 트랜스포머 대비 3배 긴 시퀀스에서도 안정적인 학습이 가능함을 이론적으로 증명한다.” 16</li>
<li><strong>좋은 예 (Robot):</strong> “우리의 시스템은 ’분산 힘 인지 접촉 표현(Distributed Force-aware Contact Representation)’을 도입하여, 비정형 물체와의 상호작용 시 발생하는 복잡한 변형을 실시간으로 추적하고 보정한다.” 17</li>
</ul>
<h3>3.3  [3단계] Evidence: 성능 입증과 커뮤니티 기여 (Impact)</h3>
<p>마지막 세 번째 포인트는 제안한 방법이 실제로 유효함을 증명하는 ’증거(Evidence)’와 학계에 남기는 ’유산(Legacy)’이다. 수치적인 성능 향상, 실험의 규모, 그리고 코드나 데이터 공개 여부를 명시하여 신뢰를 확보한다.18</p>
<ul>
<li><strong>필수 요소:</strong></li>
<li>정량적 성능 지표 (Quantitative Metrics)</li>
<li>비교 대상 (SOTA Comparison)</li>
<li>실험의 다양성 및 규모 (Scale of Experiments)</li>
<li>오픈 소스 기여 (Open Source Promise)</li>
<li><strong>나쁜 예:</strong> “실험 결과 우리 모델이 기존 모델보다 좋았다.” (주관적임)</li>
<li><strong>좋은 예 (AI):</strong> “COCO와 LVIS 벤치마크에서의 광범위한 실험을 통해, 제안된 모델이 SOTA 대비 mAP를 2.5% 향상시키면서도 추론 속도는 30% 빠름을 입증하였다. 또한, 재현성을 위해 코드와 사전 학습된 모델을 공개한다.” 20</li>
<li><strong>좋은 예 (Robot):</strong> “우리는 3가지 다른 로봇 플랫폼과 500회 이상의 실제 파지 실험을 통해 시스템의 일반화 성능을 검증하였으며, 시뮬레이션 훈련만으로도 실제 환경 성공률 95%를 달성하는 Sim-to-Real 효율성을 확인하였다.” 21</li>
</ul>
<h2>4.  ’강한 동사(Strong Verbs)’와 문장 다듬기 기술</h2>
<p>기여점을 작성할 때는 수동적이거나 자신감 없는 표현을 피하고, 연구의 주도성을 보여주는 ’강한 동사’를 사용해야 한다. 동사의 선택은 논문의 인상을 좌우한다.22</p>
<h3>4.1  피해야 할 약한 동사 (Weak Verbs)</h3>
<ul>
<li><strong>Study (연구하다):</strong> 너무 일반적이다. 연구는 당연히 하는 것이다. 무엇을 ’발견’했거나 ’제안’했는지가 중요하다.</li>
<li><strong>Try / Attempt (시도하다):</strong> 시도만 하고 실패했을 수도 있다는 뉘앙스를 준다. 성공했음을 암시해야 한다.</li>
<li><strong>Discuss (논의하다):</strong> 기여점보다는 Discussion 섹션에 어울린다. 기여점에서는 ’결론’을 제시해야 한다.</li>
<li><strong>Use / Utilize (사용하다):</strong> 도구를 쓴 것 자체는 기여가 아니다. 도구를 통해 무엇을 ’달성’했는지를 적어야 한다.</li>
</ul>
<h3>4.2  상황별 최적의 강한 동사 (Action Verbs)</h3>
<table><thead><tr><th><strong>상황 (Context)</strong></th><th><strong>추천 동사 (Verbs)</strong></th><th><strong>문장 예시 (Snippet)</strong></th></tr></thead><tbody>
<tr><td><strong>새로운 개념/방법 제안</strong></td><td><strong>Propose, Introduce, Present, Formulate</strong></td><td>“We <strong>formulate</strong> the generalization problem inherent in visual RL…” 21</td></tr>
<tr><td><strong>문제 해결 및 극복</strong></td><td><strong>Address, Tackle, Overcome, Alleviate, Mitigate</strong></td><td>“This paper <strong>addresses</strong> the transductive zero-shot challenge…” 23</td></tr>
<tr><td><strong>성능/효과 입증</strong></td><td><strong>Demonstrate, Validate, Verify, Outperform, Surpass</strong></td><td>“Extensive results <strong>demonstrate</strong> consistent improvements…” 24</td></tr>
<tr><td><strong>상세한 분석/규명</strong></td><td><strong>Analyze, Reveal, Identify, Elucidate</strong></td><td>“We <strong>identify</strong> a key limitation of existing decoding-time approaches…” 16</td></tr>
<tr><td><strong>시스템/데이터 구축</strong></td><td><strong>Construct, Establish, Curate, Release</strong></td><td>“We <strong>introduce</strong> RefHuman, a large-scale dataset that substantially extends…” 25</td></tr>
<tr><td><strong>통합 및 연결</strong></td><td><strong>Integrate, Bridge, Unified</strong></td><td>“We propose a <strong>unified</strong> view to analyze existing diffusion-based DA methods…” 24</td></tr>
</tbody></table>
<h3>4.3  형용사의 구체화 (Quantifying Adjectives)</h3>
<p>“좋은 성능(Good performance)”, “상당한 향상(Significant improvement)“과 같은 모호한 형용사는 리뷰어의 의구심을 자아낸다. 이를 구체적인 수치나 명확한 범위로 대체하라.26</p>
<ul>
<li><em>Vague:</em> “High accuracy” -&gt; <em>Specific:</em> “State-of-the-art accuracy of 85.4%”</li>
<li><em>Vague:</em> “Fast speed” -&gt; <em>Specific:</em> “Real-time performance at 60 FPS”</li>
<li><em>Vague:</em> “Scalable method” -&gt; <em>Specific:</em> “Linear computational complexity O(N)”</li>
<li><em>Vague:</em> “Various environments” -&gt; <em>Specific:</em> “Across 5 distinct indoor and outdoor scenarios”</li>
</ul>
<h2>5.  실전 예시: 논문 기여점의 진화 과정 (Iterative Refinement)</h2>
<p>초보자가 쓴 초안이 전문가의 손길을 거쳐 어떻게 탑티어 학회용 기여점으로 변모하는지 그 과정을 추적해 본다.</p>
<h3>5.1 Case 1: 로봇 주행 (Navigation) 논문</h3>
<ul>
<li><strong>초안 (Draft):</strong></li>
<li>우리는 로봇 주행을 위해 새로운 강화학습 알고리즘을 만들었다.</li>
<li>이 알고리즘은 카메라와 라이다 센서를 같이 사용한다.</li>
<li>실험해보니 기존 방법보다 더 잘 작동했다.</li>
<li><em>(평가: 너무 평범하고, 구체적인 정보가 없으며, ’잘 작동했다’는 근거가 부족함.)</em></li>
<li><strong>1차 수정 (Revision 1):</strong></li>
<li>우리는 복잡한 환경에서의 주행을 위한 멀티모달 강화학습 프레임워크를 제안한다.</li>
<li>카메라 이미지와 라이다 포인트 클라우드를 융합하는 어텐션 모듈을 개발했다.</li>
<li>다양한 장애물이 있는 시뮬레이션 환경에서 충돌 횟수를 줄였다.</li>
<li><em>(평가: 조금 나아졌으나, 여전히 ’복잡한 환경’이나 ’어텐션 모듈’의 특성이 모호함.)</em></li>
<li><strong>최종 완성 (Final Polish - Expert Level):</strong></li>
<li><strong>(1) Novelty:</strong> 우리는 시각적 폐색(Occlusion)이 빈번한 동적 환경에서 강건한 주행을 보장하기 위해, 센서 간의 불확실성을 상호 보완하는 <strong>‘Uncertainty-Aware Multi-Modal Fusion’ 프레임워크를 제안한다 (Propose)</strong>.</li>
<li><strong>(2) Methodology:</strong> 제안된 시스템은 **‘Cross-Attention Transformer’**를 도입하여, 라이다의 기하학적 정보와 카메라의 의미론적 정보를 적응적으로 가중 결합함으로써 센서 결측(Sensor Dropout) 상황에서도 <strong>연속적인 경로 계획을 가능하게 한다 (Enable)</strong>.</li>
<li><strong>(3) Impact:</strong> 10km 이상의 실제 도심 주행 실험을 통해, 제안된 방법이 단일 센서 기반 방식 대비 <strong>충돌률을 60% 감소</strong>시키고 <strong>주행 성공률 98%를 달성</strong>함을 <strong>입증하였다 (Demonstrate)</strong>.</li>
</ul>
<h3>5.2 Case 2: 이미지 생성 (Generative Model) 논문</h3>
<ul>
<li><strong>초안 (Draft):</strong></li>
<li>텍스트로 이미지를 만드는 새로운 모델을 제안한다.</li>
<li>기존 모델보다 더 사실적인 이미지를 만든다.</li>
<li>FID 점수가 낮게 나왔다.</li>
<li><strong>최종 완성 (Final Polish):</strong></li>
<li><strong>(1) Novelty:</strong> 우리는 텍스트-이미지 생성 모델의 제어 가능성(Controllability)을 극대화하기 위해, 인간의 피드백을 직접적인 학습 신호로 사용하는 <strong>‘Rich Human Feedback (RHF)’ 기반의 새로운 미세 조정(Fine-tuning) 전략을 소개한다 (Introduce)</strong>. 20</li>
<li><strong>(2) Methodology:</strong> 단순한 이진 선택(Binary Choice)을 넘어 이미지의 특정 영역에 대한 구체적인 피드백을 반영할 수 있는 **‘Region-Specific Reward Model’**을 설계하였으며, 이를 통해 생성된 이미지의 <strong>공간적 정합성을 획기적으로 개선하였다</strong>.</li>
<li><strong>(3) Impact:</strong> 제안된 모델은 FID 점수 5.2를 기록하여 <strong>SOTA를 갱신</strong>했을 뿐만 아니라, 사용자 선호도 평가(User Study)에서 기존 모델 대비 <strong>70% 더 높은 선호도</strong>를 기록하였다. 또한, 연구 활성화를 위해 <strong>3만 건의 RHF 데이터셋을 공개한다</strong>.</li>
</ul>
<h2>6.  리뷰어의 마음을 읽는 기여점 체크리스트</h2>
<p>기여점을 다 썼다면, 잠시 저자의 모자를 벗고 까칠한 리뷰어의 가면을 쓰고 다음 질문들을 던져보라. 이 체크리스트를 통과하지 못하면 아직 제출할 준비가 되지 않은 것이다.4</p>
<h3>6.1  자기 점검 질문 (Self-Reflection Questions)</h3>
<ol>
<li><strong>“So what?” 테스트:</strong> “그래서 이게 왜 중요한데?“라는 질문에 답할 수 있는가? 단순히 어렵고 복잡한 수식을 풀었다는 것이 중요한 게 아니다. 그것이 어떤 문제를 해결했는지가 중요하다.</li>
<li><strong>“Incremental” 함정:</strong> 기존 연구(A)에 약간의 수정(B)만 가한 “A+B” 논문으로 보이지 않는가? 단순히 모듈 하나를 추가한 것이 아니라, 그 추가가 왜 필수적이었는지 ’통찰(Insight)’을 강조해야 한다.</li>
<li><strong>재현성(Reproducibility) 보장:</strong> 리뷰어가 “이거 진짜 되는 거 맞아?“라고 의심할 때, 코드 공개나 상세한 실험 설정으로 안심시킬 수 있는가? 9</li>
<li><strong>과장 광고(Overclaim) 주의:</strong> “인간 수준의 지능(Human-level intelligence)“이나 “모든 문제를 해결했다(Solved all problems)“와 같은 표현은 금물이다. 연구의 범위(Scope)와 한계(Limitation)를 명확히 하는 것이 오히려 신뢰를 높인다.</li>
<li><strong>타겟 적합성:</strong> 로봇 학회에 내면서 시뮬레이션 결과만 3줄 채우진 않았는가? 혹은 AI 학회에 내면서 이론적 깊이 없이 시스템 구현만 자랑하진 않았는가?</li>
</ol>
<h3>6.2  형식적 점검 (Formatting Check)</h3>
<ul>
<li><input disabled="" type="checkbox"/>
불릿 포인트는 3개~5개 사이인가? (3개가 가장 이상적이다) 18</li>
<li><input disabled="" type="checkbox"/>
각 포인트는 완전한 문장(Complete Sentence)으로 구성되었는가?</li>
<li><input disabled="" type="checkbox"/>
각 포인트의 길이는 2~3줄 이내로 적당한가? (너무 길면 가독성이 떨어진다)</li>
<li><input disabled="" type="checkbox"/>
문장의 시작이 강한 동사(Propose, Present, Introduce)로 시작되거나, 주어(We, Our method)가 명확한가?</li>
</ul>
<h2>7.  결론: 기여점은 ’약속’이다</h2>
<p>기여점 3줄 요약은 논문의 본문으로 들어가기 전, 저자가 리뷰어와 독자에게 건네는 **“이 논문을 읽으면 당신은 이러한 새로운 지식을 얻게 될 것입니다”**라는 엄중한 약속이다. 이 약속은 명확해야 하고(Clear), 검증 가능해야 하며(Verifiable), 무엇보다 매력적이어야 한다(Compelling).</p>
<p>화려한 미사여구로 포장된 빈약한 약속보다는, 투박하더라도 구체적인 수치와 명확한 논리로 무장된 단단한 약속이 학계에서는 더 큰 울림을 준다. 당신이 작성 중인 그 3줄이, 누군가의 연구 방향을 바꾸고, 새로운 기술의 표준이 될 수도 있음을 기억하라. 쓰기 전에 이겨놓고 시작하는 것, 그것이 바로 기여점 작성의 본질이다. 이제, 당신의 연구를 가장 빛나게 할 3개의 문장을 다듬어 보자.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>2025 Author Guidelines - CVPR - The Computer Vision Foundation, https://cvpr.thecvf.com/Conferences/2025/AuthorGuidelines</li>
<li>hzwer/WritingAIPaper: Writing AI Conference Papers: A Handbook for Beginners - GitHub, https://github.com/hzwer/WritingAIPaper</li>
<li>Announcing the NeurIPS 2024 Best Paper Awards, https://blog.neurips.cc/2024/12/10/announcing-the-neurips-2024-best-paper-awards/</li>
<li>How to Write a Research Paper | A Beginner’s Guide - Scribbr, https://www.scribbr.com/category/research-paper/</li>
<li>How to write the significance of a study? | CW Authors, https://www.cwauthors.com/article/writing-the-significance-of-a-study</li>
<li>Are itemized lists discouraged in scientific papers? - Academia Stack Exchange, https://academia.stackexchange.com/questions/19794/are-itemized-lists-discouraged-in-scientific-papers</li>
<li>Robotics vs. AI: Key differences (&amp; how they work together) - Standard Bots, https://standardbots.com/blog/ai-and-robotics-whats-the-difference-and-how-do-they-work-together</li>
<li>Differences between robotics and Artificial Intelligence - Telefónica, https://www.telefonica.com/en/communication-room/blog/difference-robotics-ai/</li>
<li>NeurIPS Paper Checklist Guidelines, https://neurips.cc/public/guides/PaperChecklist</li>
<li>Formatting Instructions For NeurIPS 2025 - arXiv, https://arxiv.org/html/2505.10292v1</li>
<li>NeurIPS 2024 Awards, https://neurips.cc/virtual/2024/awards_detail</li>
<li>Putting the Systems Back into RSS: Recommendations for Reviewers and Authors, https://hauser-kris.medium.com/putting-the-systems-back-into-rss-recommendations-for-reviewers-and-authors-d28c33fa17e4</li>
<li>Perspectives on the Future of Robot Modularity, Interoperability, and Standards - ARIA, https://www.aria.org.uk/media/skbgnqaq/the-future-of-robotics-modularity-and-interoperability.pdf</li>
<li>An Integrated System for Autonomous Robotics Manipulation, https://www.ri.cmu.edu/pub_files/2012/10/IROS12_1059_FI.pdf</li>
<li>This Single Statement Generates Great Robotic Research - Robotiq’s blog, https://blog.robotiq.com/this-single-statement-generates-great-robotics-research</li>
<li>LISA: A Layer-wise Integration and Suppression Approach for Hallucination Mitigation in Multimodal Large Language Models - arXiv, https://arxiv.org/html/2507.19110v2</li>
<li>Dynamic Reconstruction of Hand-Object Interaction with Distributed Force-aware Contact Representation, https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_Dynamic_Reconstruction_of_Hand-Object_Interaction_with_Distributed_Force-aware_Contact_Representation_ICCV_2025_paper.pdf</li>
<li>Research Article template and guidance.docx - Elsevier, <a href="https://legacyfileshare.elsevier.com/promis_misc/Research%20Article%20template%20and%20guidance.docx">https://legacyfileshare.elsevier.com/promis_misc/Research%20Article%20template%20and%20guidance.docx</a></li>
<li>Ten common statistical mistakes to watch out for when writing or reviewing a manuscript, https://pmc.ncbi.nlm.nih.gov/articles/PMC6785265/</li>
<li>CVPR 2024 Announces Best Paper Award Winners - The Computer Vision Foundation, https://cvpr.thecvf.com/Conferences/2024/News/Awards</li>
<li>DeGuV: Depth-Guided Visual Reinforcement Learning for Generalization and Interpretability in Manipulation - arXiv, https://arxiv.org/html/2509.04970v1</li>
<li>Action Verbs to Use on Your Resume | Career Services | University of Colorado Boulder, https://www.colorado.edu/career/job-searching/resumes-and-cover-letters/resumes/action-verbs-use-your-resume</li>
<li>CVPR 2024 Awards - The Computer Vision Foundation, https://cvpr.thecvf.com/virtual/2024/awards_detail</li>
<li>Inversion Circle Interpolation: Diffusion-based Image Augmentation for Data-scarce Classification - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Inversion_Circle_Interpolation_Diffusion-based_Image_Augmentation_for_Data-scarce_Classification_CVPR_2025_paper.pdf</li>
<li>Referring Human Pose and Mask Estimation in the Wild - NIPS papers, https://proceedings.neurips.cc/paper_files/paper/2024/file/4f1150c8c5f49af270555ad0c7db76d0-Paper-Conference.pdf</li>
<li>10 Common Academic Writing Mistakes and How to Avoid Them - ArchivEd, https://www.archivedacademy.com.au/writing/common-mistakes-to-avoid</li>
<li>The most common mistakes in writing a scientific manuscript, https://eikipub.com/index.php/learning-resources/the-most-common-mistakes-in-writing-a-scientific-manuscript</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>