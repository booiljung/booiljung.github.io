<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:8.1. 보충 자료 (Supplementary Material)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>8.1. 보충 자료 (Supplementary Material)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">기사 (Articles)</a> / <a href="index.html">실전 논문 작성 가이드</a> / <span>8.1. 보충 자료 (Supplementary Material)</span></nav>
                </div>
            </header>
            <article>
                <h1>8.1. 보충 자료 (Supplementary Material)</h1>
<p>학술 출판의 세계에서 본문(Main Paper)이 빙산의 일각이라면, **보충 자료(Supplementary Material)**는 수면 아래 거대한 지지 기반이다. 8페이지 내외의 제한된 지면에 연구의 모든 정수를 담아내는 것은 물리적으로 불가능하며, 현대의 인공지능(AI)과 로봇 공학 연구는 그 복잡성으로 인해 본문만으로는 재현성(Reproducibility)을 담보하기 어렵다. 과거에는 보충 자료가 단순히 지면이 부족해 밀려난 잉여 데이터의 저장소로 여겨졌으나, 현재는 논문의 합격(Acceptance)을 결정짓는 <strong>제2의 본문</strong>이자, 연구의 진실성을 입증하는 <strong>검증의 장</strong>으로 위상이 격상되었다.</p>
<p>특히 NeurIPS, ICML, CVPR, ICRA와 같은 탑 티어(Top-tier) 학회에서는 보충 자료의 품질이 리뷰어의 최종 판단에 결정적인 영향을 미친다. 본 장에서는 보충 자료를 단순히 부록으로 취급하는 안일한 태도를 버리고, 전략적 무기로 활용하는 방법을 심층적으로 분석한다. 이론적 증명의 완결성 확보부터 코드의 익명화 전략, 시각적 임팩트를 주는 영상 제작, 그리고 연구의 수명을 연장하는 프로젝트 페이지 구축까지, 보충 자료의 A to Z를 다룬다.</p>
<h2>1.  보충 자료의 전략적 위상과 철학: 왜 중요한가?</h2>
<p>연구자가 보충 자료 작성에 공을 들여야 하는 이유는 단순히 학회의 요구사항을 충족시키기 위함이 아니다. 이는 리뷰어와의 심리 게임에서 우위를 점하고, 출판 이후 커뮤니티 내에서 연구의 영향력을 극대화하기 위한 생존 전략이다.</p>
<h3>1.1  리뷰어의 의심을 잠재우는 방패 (Defense Mechanism)</h3>
<p>현대 학회의 리뷰 과정은 가혹하다. 리뷰어들은 제한된 시간 내에 논문의 결함을 찾아내야 한다는 압박을 받는다. 이때 본문에서 충분히 설명되지 않은 수식의 유도 과정이나, 특정 하이퍼파라미터 설정에 대한 근거가 부족할 경우, 리뷰어는 이를 공격의 빌미로 삼는다. 보충 자료는 이러한 공격을 선제적으로 방어하는 역할을 수행한다.</p>
<ul>
<li><strong>증명의 완결성:</strong> 본문에서 “지면 관계상 증명은 생략한다(Proof is omitted due to space constraint)“라는 문장은 리뷰어에게 좌절감을 준다. 보충 자료에 엄밀한 수학적 증명을 수록하고 본문에서 이를 정확히 포인팅함으로써, 저자는 이론적 엄밀함을 포기하지 않았음을 증명해야 한다.1</li>
<li><strong>부정적 결과의 투명한 공개:</strong> 모든 실험이 SOTA(State-of-the-Art)를 경신할 수는 없다. 본문에서는 제안하는 방법론의 우수성을 강조하더라도, 보충 자료에서는 실패 케이스(Failure Cases)나 성능이 떨어지는 조건(Edge Cases)을 솔직하게 공개해야 한다. 이는 역설적으로 연구의 진정성을 높이고, 리뷰어에게 “이 저자들은 본인 연구의 한계를 정확히 파악하고 있으며, 숨기지 않는다“는 신뢰를 심어준다.1</li>
</ul>
<h3>1.2  재현성 위기(Replication Crisis)의 극복</h3>
<p>AI 연구, 특히 딥러닝 분야는 재현성 위기를 겪고 있다. 수많은 논문이 기발한 아이디어를 제시하지만, 코드를 공개하지 않거나 공개된 코드조차 실행되지 않는 경우가 허다하다. NeurIPS는 2019년부터 코드 제출 정책을 강화했고, 재현성 체크리스트(Reproducibility Checklist) 작성을 의무화했다. 로봇 공학에서는 시뮬레이션 영상과 실제 실험(Real-world Experiment) 영상이 재현성을 입증하는 결정적 증거가 된다.4 보충 자료는 “나의 연구는 언제, 어디서, 누구에 의해서든 검증될 수 있다“는 자신감의 표현이어야 한다.</p>
<h3>1.3  페이지 제한의 전략적 우회로 (Expanding the Canvas)</h3>
<p>CVPR(8페이지), NeurIPS(9페이지), ICRA(6페이지) 등 대부분의 학회는 엄격한 페이지 제한을 둔다. 이 좁은 공간에 서론, 관련 연구, 방법론, 실험을 모두 욱여넣다 보면 필연적으로 설명이 부실해지거나 시각 자료가 작아질 수밖에 없다. 보충 자료는 이러한 물리적 제약을 뛰어넘는 무한한 캔버스다.</p>
<ul>
<li><strong>전략적 분배:</strong> 본문에는 가장 임팩트 있는 ’Teaser Figure’와 핵심 결과 그래프만을 싣고, 보충 자료에는 다양한 데이터셋에 대한 추가 실험 결과, 절제 연구(Ablation Study)의 세부 테이블, 네트워크 아키텍처의 상세 다이어그램을 배치하는 전략이 필요하다.8</li>
<li><strong>롱테일 독자를 위한 배려:</strong> 일반 독자는 본문의 결론만 보고 넘어가겠지만, 후속 연구를 진행할 전문가는 데이터 전처리 방식이나 미세한 하이퍼파라미터 설정이 궁금할 것이다. 보충 자료는 이러한 ’헤비 유저’를 만족시키는 심화 매뉴얼 역할을 한다.</li>
</ul>
<h2>2.  기술적 부록(Technical Appendix)의 해부학: 텍스트 그 이상의 가치</h2>
<p>기술적 부록은 보충 자료의 가장 전통적이면서도 중요한 형태다. 대개 PDF 형식으로 제출되며, 본문과 동일한 LaTeX 템플릿을 사용하여 시각적 일관성을 유지하는 것이 기본이다. 그러나 단순히 본문에서 넘친 내용을 복사해서 붙여넣는 것이 아니라, 독자적인 완결성을 갖춘 문서로 기획되어야 한다.</p>
<h3>2.1  이론적 증명 (Theoretical Proofs): 우아함과 엄밀함 사이</h3>
<p>이론 논문이나 수식적 배경이 중요한 모델링 논문의 경우, 증명 과정은 본문의 흐름을 끊는 주범이 되기도 한다. 따라서 본문에는 **정리(Theorem)**와 **보조정리(Lemma)**의 명제(Statement)와 직관적인 설명(Sketch of Proof)만을 기술하고, 엄밀한 수식 전개는 보충 자료로 넘기는 것이 정석이다.</p>
<ul>
<li><strong>구조화된 증명:</strong> 보충 자료 내에서도 본문의 정리 번호를 그대로 유지하거나, 명확히 매핑되도록 작성해야 한다. 예를 들어 본문의 “Theorem 1“에 대한 증명은 보충 자료에서 “Proof of Theorem 1“이라는 명시적인 섹션 제목 하에 기술되어야 한다. 단순히 “Appendix A“라고만 적혀 있으면 리뷰어는 해당 증명을 찾기 위해 문서를 뒤져야 한다.</li>
<li><strong>단계별 유도 (Step-by-step Derivation):</strong> 본문에서는 지면상 생략했던 중간 유도 과정을 보충 자료에서는 친절하게 풀어써야 한다. 리뷰어는 수식의 점프(Leap)가 있는 곳에서 오류를 의심한다. 보충 자료는 이러한 점프를 메워주는 가교 역할을 해야 한다. 필요한 경우 보조 정리(Lemma)를 추가로 정의하여 증명의 논리적 완결성을 높여야 한다.1</li>
</ul>
<h3>2.2  추가 실험 및 절제 연구 (Extended Experiments &amp; Ablations)</h3>
<p>본문에는 제안하는 방법론이 가장 잘 작동하는 메인 데이터셋에 대한 결과(SOTA 비교 등)를 싣는 것이 일반적이다. 그러나 리뷰어는 “이 방법이 다른 데이터셋이나 조건에서도 통하는가?“를 묻는다. 이를 방어하기 위해 보충 자료에는 다음과 같은 데이터가 포함되어야 한다.</p>
<ul>
<li><strong>다양한 벤치마크:</strong> 본문이 ImageNet 결과에 집중했다면, 보충 자료에는 CIFAR-100, COCO, Places365 등 다른 도메인의 데이터셋에 대한 결과를 수록하여 범용성(Generalization)을 입증한다.</li>
<li><strong>상세한 절제 연구:</strong> 모델의 특정 컴포넌트(예: Loss function의 항, 레이어의 깊이, 활성화 함수 종류, Batch Normalization 유무)를 하나씩 제거하거나 변경했을 때의 성능 변화를 보여주는 테이블은 분량이 많아 본문에 다 싣기 어렵다. 이러한 상세 분석 테이블은 보충 자료의 단골 손님이며, 방법론의 설계 근거(Design Choice)를 뒷받침하는 강력한 증거가 된다.</li>
<li><strong>정성적 결과(Qualitative Results)의 갤러리화:</strong> 본문의 그림 크기 제한으로 보여주지 못한 다양한 시각화 결과(생성된 이미지, 로봇의 경로 계획 시각화, 어텐션 맵 등)를 갤러리 형태로 구성한다. 특히 “Random Samples“를 포함시켜 체리 피킹(Cherry-picking) 의혹을 해소해야 한다.11</li>
</ul>
<h3>2.3  하이퍼파라미터 및 재현 상세 (Hyperparameters &amp; Reproducibility)</h3>
<p>“구현 세부 사항(Implementation Details)” 섹션은 본문에도 존재하지만, 텍스트로 요약된 경우가 많다. 보충 자료에는 실험에 사용된 모든 값을 표(Table) 형태로 정리하여 제공해야 한다. 이는 향후 연구자들이 당신의 연구를 베이스라인으로 삼을 때 가장 많이 참고하는 섹션이 된다.</p>
<table><thead><tr><th><strong>파라미터 카테고리</strong></th><th><strong>포함해야 할 세부 항목</strong></th></tr></thead><tbody>
<tr><td><strong>학습 설정</strong></td><td>Learning rate, Batch size, Optimizer(<span class="math math-inline">\beta_1, \beta_2, \epsilon</span>), Weight decay, Epoch 수, LR Schedule(Warmup, Decay steps)</td></tr>
<tr><td><strong>하드웨어 환경</strong></td><td>GPU 모델명(예: NVIDIA A100 80GB), 사용된 GPU 개수, CPU 사양, 학습 소요 시간(Training hours)</td></tr>
<tr><td><strong>데이터 전처리</strong></td><td>이미지 리사이징(Crop, Resize, Interpolation method), 정규화(Mean/Std 값), Augmentation 기법 및 확률</td></tr>
<tr><td><strong>라이브러리 버전</strong></td><td>PyTorch/TensorFlow 버전, CUDA 버전, 주요 종속성 패키지(Numpy, Pandas 등)의 버전</td></tr>
<tr><td><strong>랜덤 시드</strong></td><td>실험에 사용된 고정 시드(Seed) 값, 여러 번 실행(Run)했을 경우의 평균 및 표준 편차</td></tr>
</tbody></table>
<p>이러한 세부 사항은 1에서 강조하는 것처럼 재현성을 위한 필수 요소다.</p>
<h3>2.4  LaTeX 작성 테크닉: 교차 참조와 목차의 마법</h3>
<p>보충 자료를 별도의 PDF로 제출할 때 가장 기술적으로 까다로운 문제는 본문(Main Paper)과의 교차 참조(Cross-referencing)다. 본문의 “Figure 3“을 보충 자료에서 언급하거나, 반대로 본문에서 “See Appendix A“라고 참조할 때, 하드코딩(직접 숫자 입력)은 피해야 한다. 수정 과정에서 번호가 바뀌면 모든 숫자가 틀어지는 재앙이 발생하기 때문이다.</p>
<h4>2.4.1 <code>xr</code> 패키지 활용법 (The Cross-Referencing Standard)</h4>
<p>LaTeX의 <code>xr</code> (external references) 패키지는 서로 다른 문서 간의 레이블(Label)을 참조할 수 있게 해주는 표준 도구다. 이를 능숙하게 다루는 것은 프로페셔널한 논문 작성의 기본이다.</p>
<p><strong>Step 1: Main Paper (<code>main.tex</code>) 설정</strong></p>
<pre><code>\usepackage{xr}
\externaldocument{supplementary} % supplementary.aux 파일을 참조한다고 선언
</code></pre>
<p><strong>Step 2: Supplementary Material (<code>supplementary.tex</code>) 설정</strong></p>
<pre><code>\usepackage{xr}
\externaldocument{main} % main.aux 파일을 참조한다고 선언
</code></pre>
<p>Step 3: 컴파일 순서의 중요성</p>
<p>xr 패키지는 .aux 파일을 읽어서 참조를 해결한다. 따라서 컴파일 순서가 매우 중요하다.</p>
<ol>
<li><code>main.tex</code>를 컴파일한다 (이때 <code>main.aux</code> 생성).</li>
<li><code>supplementary.tex</code>를 컴파일한다 (이때 <code>supplementary.aux</code> 생성 및 <code>main.aux</code> 참조).</li>
<li>다시 <code>main.tex</code>를 컴파일한다 (최종적으로 <code>supplementary.aux</code> 참조).</li>
</ol>
<p>Overleaf와 같은 클라우드 환경에서는 이를 자동화하기 위해 <code>latexmkrc</code> 파일을 설정하거나, 커스텀 커맨드를 정의하여 의존성 파일이 자동으로 로드되도록 해야 한다.14</p>
<h4>2.4.2 부록 전용 목차 (Mini Table of Contents)</h4>
<p>보충 자료가 20페이지를 넘어가는 경우, 독자가 원하는 정보를 빠르게 찾을 수 있도록 별도의 목차(Table of Contents)를 제공하는 것이 예의다. <code>minitoc</code> 패키지나 <code>etoc</code> 패키지를 활용하면 보충 자료의 첫 페이지에 해당 문서만의 목차를 생성할 수 있다.</p>
<pre><code>\usepackage{minitoc}
...
\begin{document}
...
\appendix
\chapter{Supplementary Material}
\minitoc % 부록 전용 목차 생성
...
\end{document}
</code></pre>
<p>이러한 세심한 배려가 리뷰어의 가독성을 높이고, “이 논문은 정리가 잘 되어 있다“는 긍정적인 인상을 무의식중에 심어준다.18</p>
<h2>3.  코드 제출: 익명성과 실행 가능성의 균형</h2>
<p>NeurIPS, ICML, ICLR 등 Top-tier AI 학회는 코드 제출을 ’강력 권장’하며, 사실상 합격을 위한 필수 조건으로 간주하는 추세다. CVPR과 같은 비전 학회나 로봇 학회에서도 코드 제출은 논문의 신뢰도 점수(Confidence Score)를 크게 높인다. 하지만 리뷰 기간 중에는 <strong>이중맹검(Double-blind)</strong> 원칙을 철저히 지켜야 하므로, 코드 제출 시 각별한 주의가 필요하다.4</p>
<h3>3.1  익명화(Anonymization) 전략: 데스크 리젝트를 피하는 법</h3>
<p>코드 제출 시 저자의 신원이 노출되면 리뷰도 받지 못하고 ‘데스크 리젝트(Desk Reject)’ 당할 수 있다. 다음은 필수적으로 체크해야 할 익명화 항목들이다.</p>
<ul>
<li><strong>소스 코드 헤더 및 주석:</strong> 파일 상단에 습관적으로 적는 <code>Author: Hong Gil Dong</code>, <code>Copyright (c) 2025 KAIST</code>, <code>Contact: gildong@kaist.ac.kr</code> 등의 문구를 전수 조사하여 제거해야 한다. <code>sed</code> 명령어나 IDE의 전체 검색 기능을 활용하라.</li>
<li><strong>절대 경로(Absolute Path):</strong> 코드 내에 <code>/home/gildong/project/data</code>와 같이 사용자 이름이 포함된 절대 경로가 하드코딩되어 있는지 검사한다. 모든 경로는 상대 경로(Relative Path)로 변경하거나, 설정 파일(<code>config.yaml</code>)로 분리하여 사용자가 직접 지정할 수 있게 해야 한다.</li>
<li><strong>Git 히스토리 (.git 폴더):</strong> 가장 흔한 실수다. 프로젝트 폴더를 통째로 압축하면서 <code>.git</code> 폴더까지 포함하는 경우, 리뷰어는 <code>git log</code> 명령어 한 번으로 저자의 이름, 이메일, 커밋 날짜를 모두 볼 수 있다. 제출용 코드는 <code>.git</code> 폴더를 제외한 ’Clean Snapshot’이어야 한다.</li>
<li><strong>URL 및 하이퍼링크:</strong> 코드 내의 주석이나 <code>README.md</code>에 저자의 이전 연구나 랩 홈페이지, 개인 블로그 링크가 포함되어 있는지 확인한다. 필요하다면 ``로 대체한다.22</li>
</ul>
<h3>3.2  제출 방식: ZIP vs 익명 저장소</h3>
<ul>
<li><strong>ZIP 파일 업로드:</strong> 학회 제출 시스템(CMT, OpenReview)에 보충 자료로 업로드하는 가장 전통적이고 안전한 방식이다. 하지만 용량 제한(보통 100MB)이 있어 대용량 데이터나 모델 가중치(Weights)를 포함하기 어렵다는 단점이 있다.8</li>
<li><strong>익명 GitHub (Anonymous GitHub):</strong> 최근 많이 사용되는 방식이다. <code>anonymous.4open.science</code>와 같은 서드파티 서비스를 이용하거나, GitHub 리포지토리를 익명화하여 보여주는 도구를 활용한다. OpenReview 등에서는 익명 리포지토리 링크를 허용하는 추세다. 단, 학회 규정을 꼼꼼히 확인해야 한다. 일부 학회는 외부 링크를 통한 코드 제출을 금지하고 오직 시스템 업로드만 허용하기도 한다. 링크는 반드시 ’읽기 전용’이어야 하며, 제출 마감 이후 수정되지 않았음을 보장해야 한다.22</li>
</ul>
<h3>3.3  완벽한 README.md 작성법: 실행되지 않는 코드는 쓰레기다</h3>
<p>리뷰어는 코드를 한 줄 한 줄 분석할 시간이 없다. 그들이 확인하고 싶은 것은 “이 코드가 실제로 돌아가는가?“이다. 따라서 <code>README.md</code> 파일은 코드를 실행하기 위한 완벽한 가이드가 되어야 한다.</p>
<ul>
<li><strong>Dependencies:</strong> <code>requirements.txt</code> 또는 <code>environment.yml</code> (Conda) 파일을 제공하여 의존성 패키지를 한 번에 설치할 수 있게 한다. 더 나아가 <code>Dockerfile</code>을 제공하면 OS 환경 차이로 인한 실행 오류를 원천 차단할 수 있어 높은 평가를 받는다.</li>
<li><strong>One-Liner Execution:</strong> 복잡한 설정 없이 <code>python main.py --config configs/test.yaml</code>과 같이 한 줄의 명령어로 주요 실험을 재현할 수 있도록 스크립트를 준비한다. “데이터셋을 다운받아 A 폴더에 넣고, 전처리 스크립트를 돌린 뒤…“와 같은 복잡한 과정은 스크립트(<code>run_demo.sh</code>)로 자동화해야 한다.</li>
<li><strong>Pre-trained Models:</strong> 학습에 며칠이 걸리는 모델이라면, 미리 학습된 가중치(Pre-trained weights)를 다운로드할 수 있는 익명 링크(Google Drive, Dropbox 등 - <em>단, 계정 정보가 노출되지 않도록 주의</em>)를 제공하여 리뷰어가 추론(Inference) 결과만이라도 즉시 확인할 수 있게 배려한다.</li>
<li><strong>Toy Data:</strong> 전체 데이터셋을 다운로드하기 어렵다면, 적은 용량의 샘플 데이터(Toy Dataset)를 포함시켜 코드가 정상적으로 작동함을 보여준다. 이는 코드가 ’실행 가능하다’는 것을 입증하는 가장 확실한 방법이다.1</li>
</ul>
<h2>4.  멀티미디어 자료: 로봇과 비전 연구의 승부처</h2>
<p>텍스트와 정지된 이미지만으로는 로봇의 동적인 움직임(Dynamics)이나 비전 모델의 시간적 변화(Temporal Consistency)를 설명하는 데 한계가 있다. ICRA, IROS와 같은 로봇 학회에서는 비디오 제출이 거의 의무이며, CVPR, ECCV 등에서도 비디오는 논문의 매력을 어필하는 가장 효과적인 수단이다.</p>
<h3>4.1  학회별 비디오 가이드라인 분석 및 전략</h3>
<p>학회마다 요구하는 비디오의 성격이 다르므로 맞춤형 전략이 필요하다.</p>
<table><thead><tr><th><strong>구분</strong></th><th><strong>ICRA / IROS (Robotics)</strong></th><th><strong>CVPR / NeurIPS (AI &amp; Vision)</strong></th></tr></thead><tbody>
<tr><td><strong>필수 여부</strong></td><td>사실상 필수 (Mandatory)</td><td>권장 (Highly Recommended)</td></tr>
<tr><td><strong>길이 제한</strong></td><td>1분 ~ 3분 (엄격함)</td><td>5분 이내 (상대적으로 유동적)</td></tr>
<tr><td><strong>핵심 콘텐츠</strong></td><td><strong>Real-world Experiments</strong>, 센서 데이터</td><td><strong>Qualitative Results</strong>, 데모 시연</td></tr>
<tr><td><strong>파일 규격</strong></td><td>10MB ~ 20MB 제한 (매우 작음)</td><td>100MB ~ (여유 있음)</td></tr>
<tr><td><strong>평가 포인트</strong></td><td>동작의 신뢰성, 강건함(Robustness)</td><td>시각적 품질, 아이디어의 직관성</td></tr>
</tbody></table>
<ul>
<li><strong>ICRA/IROS:</strong> 로봇 학회에서 시뮬레이션 영상만 제출하는 것은 위험하다. “Real robot experiment is missing“이라는 리뷰를 피하기 위해 실제 하드웨어 구동 영상을 반드시 포함해야 한다. 파일 용량 제한이 매우 작으므로, 압축 효율이 좋은 코덱을 사용하고 불필요한 장면은 과감히 삭제해야 한다.25</li>
<li><strong>CVPR/NeurIPS:</strong> 화려한 시각화가 중요하다. 생성형 AI라면 고해상도 생성 과정을, 자율주행이라면 인식 결과를 영상에 오버레이(Overlay)하여 보여준다. 최근에는 5분 이내의 발표 형식 영상을 요구하기도 한다.21</li>
</ul>
<h3>4.2  스토리텔링이 있는 비디오 구성: 논문의 예고편이 아닌 다큐멘터리</h3>
<p>비디오는 단순히 실험 장면을 나열한 영상 앨범이 아니다. 논문의 논리를 시각적으로 요약한 ’다큐멘터리’여야 한다.</p>
<ol>
<li><strong>도입부 (The Hook - 0:00~0:15):</strong> 첫 15초가 승부처다. 연구가 해결하고자 하는 문제 상황(Problem Definition)을 명확히 보여주고, 제안하는 로봇/모델이 이를 어떻게 해결하는지 하이라이트 컷으로 보여준다.</li>
<li><strong>방법론 시각화 (Method Visualization - 0:15~0:45):</strong> 복잡한 아키텍처 다이어그램을 애니메이션으로 보여주면 이해도가 급상승한다. 데이터의 흐름, 로봇의 인지-판단-제어 루프를 도식화하여 영상에 오버레이한다.</li>
<li><strong>비교 실험 (Comparisons - 0:45~1:30):</strong> 화면 분할(Split Screen)을 적극 활용한다. 왼쪽에는 기존 방법(Baseline)이 실패하거나 넘어지는 모습을, 오른쪽에는 제안하는 방법(Ours)이 성공적으로 주행하는 모습을 동시에 보여주어 성능 차이를 직관적으로 각인시킨다.29</li>
<li><strong>실패 케이스 분석 (Failure Cases - 1:30~End):</strong> 영상 후반부에 모델이 실패하는 상황을 솔직하게 보여주고, 그 원인을 자막으로 분석하면 리뷰어에게 깊은 신뢰를 줄 수 있다. 이는 “우리는 한계를 알고 있다“는 메시지를 준다.</li>
</ol>
<h3>4.3  오디오 전략: 내레이션(Narration) vs 자막(Subtitles)</h3>
<ul>
<li><strong>자막 중심(Subtitle-first):</strong> 학회장은 시끄럽거나 소리를 켤 수 없는 환경일 수 있다. 따라서 소리 없이 영상만 봐도 내용을 이해할 수 있도록 자막을 충실히 달아야 한다. 핵심 키워드와 간결한 문장으로 상황을 설명한다. 글자는 모바일에서도 보일 만큼 커야 하며, 흰색 글씨에 검은색 테두리(Stroke)를 넣어 가독성을 확보한다.</li>
<li><strong>내레이션 활용:</strong> 가능하다면 전문적인 톤의 영어 내레이션을 입히는 것이 좋다. 최근에는 ElevenLabs, OpenAI Voice 등 고품질 AI TTS(Text-to-Speech) 기술이 발달하여 원어민 수준의 내레이션을 쉽게 생성할 수 있다. 내레이션은 시각적 정보와 청각적 정보를 동시에 전달하여 정보 습득 효율을 높인다. 자막과 내레이션을 병행하는 것이 최상의 전략이다.31</li>
</ul>
<h3>4.4  기술적 품질 관리 (Technical Standards)</h3>
<ul>
<li><strong>코덱과 호환성:</strong> 호환성이 가장 좋은 <strong>MP4 컨테이너</strong>와 <strong>H.264 코덱</strong>을 사용한다. 최신 H.265(HEVC) 코덱은 압축률은 좋지만 구형 브라우저나 OS에서 재생되지 않을 수 있다.</li>
<li><strong>비트레이트(Bitrate) 조절:</strong> 파일 용량을 맞추기 위해 해상도를 줄이기보다는 비트레이트를 조절하는 것이 낫다. 동적인 장면이 많다면 가변 비트레이트(VBR)를, 정적인 장면(슬라이드 등)이 많다면 낮은 비트레이트를 사용하여 용량을 절약한다. <code>ffmpeg</code>와 같은 도구를 사용하여 화질 손상을 최소화하면서 용량을 줄이는 기술을 익혀야 한다.</li>
<li><em>Tip:</em> <code>ffmpeg -i input.mp4 -vcodec libx264 -crf 23 output.mp4</code> 명령어를 사용하면 적절한 화질과 용량의 균형을 맞출 수 있다.</li>
</ul>
<h2>5.  프로젝트 페이지와 인터랙티브 데모: 출판 그 이후를 위하여</h2>
<p>논문이 출판된 이후(Post-submission), 혹은 카메라 레디(Camera Ready) 단계에서 보충 자료의 확장판으로 ’프로젝트 페이지’를 개설하는 것은 이제 표준적인 관례가 되었다. 이는 연구의 가시성(Visibility)을 높이고 인용 수(Citation)를 늘리는 데 결정적인 역할을 한다. 정적인 PDF는 검색엔진 최적화(SEO)에 불리하지만, 잘 만들어진 웹페이지는 구글 검색 상단에 노출될 확률이 높다.34</p>
<h3>5.1  프로젝트 페이지 구축 (Project Page Construction)</h3>
<p>웹 기반의 프로젝트 페이지는 비디오, 인터랙티브 뷰어, 고해상도 이미지, 코드 링크, BibTeX 등을 한곳에 모아 보여주는 허브 역할을 한다.</p>
<ul>
<li><strong>Nerfies 템플릿의 표준화:</strong> 구글 리서치의 Keunhong Park 등이 공개한 ‘Nerfies’ 프로젝트 페이지 템플릿은 AI/비전 분야의 비공식 표준(De facto standard)이 되었다. 깔끔한 디자인, 반응형 레이아웃, 그리고 무엇보다 <strong>비디오 비교 슬라이더(Video Comparison Slider)</strong> 기능을 제공하여, 마우스 드래그만으로 Before/After를 비교할 수 있게 해준다. GitHub에서 소스를 포크(Fork)하여 자신의 내용만 채워 넣으면 전문가 수준의 페이지를 10분 만에 만들 수 있다.36</li>
<li><strong>GitHub Pages 호스팅:</strong> 별도의 서버 호스팅 비용 없이 GitHub Pages를 이용해 무료로 페이지를 호스팅할 수 있다. <code>username.github.io/project-name</code> 형태의 URL은 접근성이 좋고 유지보수가 쉽다. Jekyll이나 Hugo 같은 정적 사이트 생성기(Static Site Generator)를 활용하면 관리가 더욱 용이하다.39</li>
</ul>
<h3>5.2  Hugging Face Spaces &amp; Web Demos</h3>
<p>최근 AI 연구, 특히 생성형 AI나 LLM 연구에서는 사용자가 직접 모델을 테스트해 볼 수 있는 ’Web Demo’가 강력한 마케팅 수단이다. 백문이 불여일견, 사용자가 자신의 데이터를 넣어보고 결과를 확인하는 순간 연구의 파급력은 폭발한다.</p>
<ul>
<li><strong>Hugging Face Spaces:</strong> Gradio나 Streamlit과 같은 파이썬 기반 웹 프레임워크를 사용하면 HTML/JS 지식 없이도 몇 줄의 코드로 데모 앱을 만들 수 있다. 이를 Hugging Face Spaces에 배포하면 전 세계 연구자들이 웹 브라우저 상에서 GPU 자원을 활용해 모델을 돌려볼 수 있다. 이는 논문의 접근성을 기하급수적으로 높인다.13</li>
<li><strong>Google Colab:</strong> GPU가 필요한 무거운 모델의 경우, 미리 세팅된 Google Colab 노트북 링크를 제공하는 것도 좋은 방법이다. 사용자가 “Run All” 버튼만 누르면 환경 설정부터 추론까지 원스톱으로 경험하게 하는 것은 최고의 사용자 경험(UX)이다.</li>
</ul>
<h3>5.3  데모 트랙(Demo Track) 활용</h3>
<p>AAAI, IJCAI, ACL 등 주요 학회는 별도의 ’데모 트랙’을 운영한다. 본 논문과는 별도로 시스템의 시연에 초점을 맞춘 짧은 논문(2~4페이지)과 비디오를 제출하여 선정되면, 학회장에서 부스를 배정받아 연구자들에게 직접 시연할 기회를 얻는다. 이는 네트워킹과 피드백 수집, 그리고 기업 리크루터들에게 자신을 어필하는 데 매우 유리하다.44</p>
<h2>6.  심층 분석: 학회별 보충 자료 전략 매트릭스</h2>
<p>성공적인 투고를 위해서는 타겟 학회의 성향에 맞춰 보충 자료 전략을 커스터마이징해야 한다. 다음은 주요 AI 및 로봇 학회의 보충 자료 특성을 비교 분석한 매트릭스다.</p>
<table><thead><tr><th><strong>특징</strong></th><th><strong>NeurIPS / ICML / ICLR</strong></th><th><strong>CVPR / ICCV / ECCV</strong></th><th><strong>ICRA / IROS</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 가치</strong></td><td><strong>이론적 엄밀성 &amp; 재현성</strong></td><td><strong>시각적 결과 &amp; 비교</strong></td><td><strong>실세계 적용 &amp; 시스템 완성도</strong></td></tr>
<tr><td><strong>필수 제출물</strong></td><td>Paper Checklist, 코드 (권장)</td><td>추가 정성적 결과 (PDF)</td><td><strong>데모 비디오 (필수)</strong></td></tr>
<tr><td><strong>PDF 부록</strong></td><td>증명(Proofs) 필수, 본문 뒤 병합 가능</td><td>별도 파일 제출 선호</td><td>본문 뒤 병합 또는 별도 제출</td></tr>
<tr><td><strong>코드 제출</strong></td><td>익명화 필수, 채점 비중 높음</td><td>익명화 필수, 권장 사항</td><td>권장하나 비디오가 더 중요</td></tr>
<tr><td><strong>비디오 스타일</strong></td><td>이론 설명, 토이 데이터 데모</td><td>화려한 시각화, 비교 슬라이더</td><td><strong>실제 로봇 구동, 하드웨어 셋업</strong></td></tr>
<tr><td><strong>Sim-to-Real</strong></td><td>상대적으로 덜 중요</td><td>중요하나 시각적 품질 우선</td><td><strong>핵심 평가 요소 (Evidence)</strong></td></tr>
<tr><td><strong>데이터셋</strong></td><td>라이선스, 윤리 검토, Datasheet</td><td>데이터셋 공개 여부 중요</td><td>하드웨어 설계도(CAD) 공개 시 가산점</td></tr>
</tbody></table>
<h3>6.1  NeurIPS/ICML 전략: “Checklist is King”</h3>
<p>NeurIPS는 ‘Paper Checklist’ 작성을 의무화하고 있으며, 이에 대한 답변이 보충 자료와 일치해야 한다. 예를 들어 체크리스트에서 “코드 제출: 예“라고 답했다면, 반드시 코드가 포함되어야 한다. 또한 “이론적 결과: 예“라고 답했다면 전체 증명이 부록에 있어야 한다. 리뷰어 가이드라인은 체크리스트 항목을 하나하나 검증하도록 되어 있으므로, 보충 자료는 이 체크리스트의 ‘증거 자료집’ 역할을 해야 한다.1</p>
<h3>6.2  CVPR 전략: “Visuals First”</h3>
<p>CVPR 리뷰어들은 수많은 논문을 검토하므로 시각적으로 눈길을 끄는 것이 중요하다. 보충 자료 PDF의 첫 페이지에도 본문의 Teaser와 같은 강력한 시각 자료를 배치하라. 또한, 100MB의 용량 제한을 꽉 채워서라도 고화질의 결과 이미지를 다수 포함시켜 “체리 피킹(Cherry-picking, 좋은 결과만 골라 보여주기)“이 아님을 증명해야 한다.8</p>
<h3>6.3  ICRA/IROS 전략: “Real World Proof”</h3>
<p>로봇 학회에서 시뮬레이션 결과만 있는 논문은 “Real robot experiment is missing“이라는 리뷰를 받기 십상이다. 보충 자료 비디오를 통해 실제 환경에서의 동작, 센서 노이즈 처리, 외란(Disturbance)에 대한 강건함 등을 보여주어야 한다. 만약 실제 로봇 실험이 불가능하다면, 시뮬레이션 환경이 현실을 얼마나 정밀하게 모사했는지(Sim-to-Real Gap 최소화 노력)를 입증하는 자료가 필수적이다.6</p>
<h2>7.  보충 자료 제출 전 최종 체크리스트</h2>
<p>제출 버튼을 누르기 전, 다음 항목들을 반드시 점검하라. 사소한 실수가 치명적인 결과(Desk Reject)로 이어질 수 있다.</p>
<p><strong>익명성 (Anonymity)</strong></p>
<ul>
<li><input disabled="" type="checkbox"/>
PDF 부록 파일명에 저자 이름이 포함되지 않았는가? (예: <code>supp_kim.pdf</code> -&gt; <code>supp.pdf</code>)</li>
<li><input disabled="" type="checkbox"/>
코드 내 주석, 라이선스 파일, 설정 파일 경로에 이름이나 소속 기관명이 없는가?</li>
<li><input disabled="" type="checkbox"/>
비디오 내레이션에서 “We from KAIST…“와 같이 소속을 언급하지 않았는가?</li>
<li><input disabled="" type="checkbox"/>
비디오 화면 구석에 랩 로고나 학교 마크가 찍혀 있지 않은가?</li>
<li><input disabled="" type="checkbox"/>
프로젝트 페이지나 데모 링크가 익명화된 URL인가? (Github 개인 계정 링크 금지)</li>
</ul>
<p><strong>기술적 완성도 (Technical Integrity)</strong></p>
<ul>
<li><input disabled="" type="checkbox"/>
본문과 보충 자료의 수식 번호, 그림 번호, 참고문헌 번호가 정확히 일치하는가? (교차 참조 확인)</li>
<li><input disabled="" type="checkbox"/>
PDF 파일이 열리는지, 폰트가 깨지지 않는지 다른 컴퓨터에서 확인했는가?</li>
<li><input disabled="" type="checkbox"/>
비디오 파일이 일반적인 플레이어(VLC, QuickTime 등)에서 문제없이 재생되는가?</li>
<li><input disabled="" type="checkbox"/>
코드 압축 파일 내에 불필요한 대용량 파일(빌드 부산물, <code>__pycache__</code>, <code>.git</code>)이 포함되지 않았는가?</li>
<li><input disabled="" type="checkbox"/>
제공된 설치 스크립트(<code>install.sh</code> 등)가 빈 가상환경(Virtual Environment)에서 에러 없이 작동하는가?</li>
</ul>
<p><strong>내용적 일관성 (Content Consistency)</strong></p>
<ul>
<li><input disabled="" type="checkbox"/>
본문에서 “See Supplementary Material“이라고 언급한 내용이 실제로 존재하는가?</li>
<li><input disabled="" type="checkbox"/>
보충 자료의 기호(Notation) 표기법이 본문과 동일한가?</li>
<li><input disabled="" type="checkbox"/>
추가 실험 결과가 본문의 주장과 모순되지 않는가? (혹은 모순된다면 그 이유를 설명했는가?)</li>
<li><input disabled="" type="checkbox"/>
데이터셋을 공개하는 경우, 개인정보 보호(Privacy) 및 윤리적 문제(Ethics)를 해결했는가?</li>
</ul>
<h2>8.  맺음말: 보충 자료는 저자의 성의(Sincerity)다</h2>
<p>보충 자료를 작성하는 일은 고되고 지루하다. 마감 직전까지 본문을 수정하느라 보충 자료는 뒷전이 되기 쉽다. 그러나 명심해야 할 것은, 리뷰어는 보충 자료의 품질을 통해 <strong>저자의 연구에 대한 태도</strong>를 평가한다는 사실이다. 엉성한 코드, 깨진 수식, 재생되지 않는 비디오는 “이 연구는 아직 덜 다듬어졌다“는 신호를 준다. 반면, 깔끔하게 정리된 증명, 한 번에 실행되는 코드, 친절한 내레이션이 담긴 비디오는 “이 연구자는 자신의 결과를 독자에게 완벽하게 전달하기 위해 최선을 다했다“는 강력한 긍정 신호를 보낸다.</p>
<p>경쟁이 치열한 톱 티어 학회일수록 합격의 당락은 ’한 끝 차이’로 결정된다. 그 한 끝을 채우는 것이 바로 보충 자료다. 본문이 당신의 아이디어를 <strong>파는(Sell)</strong> 공간이라면, 보충 자료는 그 아이디어를 **증명(Prove)**하고 **서비스(Service)**하는 공간이다. 독자와 리뷰어를 위한 최고의 서비스를 제공하라. 그것이 당신의 논문을 ’Accept’로 이끄는 가장 확실한 투자다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>NeurIPS Paper Checklist Guidelines, https://neurips.cc/public/guides/PaperChecklist</li>
<li>Formatting Instructions For NeurIPS 2024 - arXiv, https://arxiv.org/html/2506.18027v1</li>
<li>Understanding Domain Randomization for Sim-to-real Transfer - OpenReview, https://openreview.net/forum?id=T8vZHIRTrY</li>
<li>CVPR 2025 Suggested Practices for Authors - The Computer Vision Foundation, https://cvpr.thecvf.com/Conferences/2025/AuthorSuggestedPractices</li>
<li>Paper Writing Best Practices - ICML 2026, https://icml.cc/Conferences/2022/BestPractices</li>
<li>Sim-to-real via latent prediction: Transferring visual non-prehensile manipulation policies, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2022.1067502/full</li>
<li>One-shot sim-to-real transfer policy for robotic assembly via reinforcement learning with visual demonstration | Robotica | Cambridge Core, https://www.cambridge.org/core/journals/robotica/article/oneshot-simtoreal-transfer-policy-for-robotic-assembly-via-reinforcement-learning-with-visual-demonstration/FC22E58B7B0876F0E5F151A229E241FD</li>
<li>NeurIPS 2024 Call for Papers, https://neurips.cc/Conferences/2024/CallForPapers</li>
<li>Call for ICRA 2025 Papers - Now Accepting Submissions, https://2025.ieee-icra.org/announcements/paper-submissions/</li>
<li>Call for Papers Workshops,Tutorials and Competitions - IROS 2024, <a href="https://iros2024-abudhabi.org/assets/Call%20for%20Papers%20Workshops,%20Tutorials%20and%20Competitions.pdf">https://iros2024-abudhabi.org/assets/Call%20for%20Papers%20Workshops,%20Tutorials%20and%20Competitions.pdf</a></li>
<li>Table of Contents inside an Appendices Environment - TeX - LaTeX Stack Exchange, https://tex.stackexchange.com/questions/70294/table-of-contents-inside-an-appendices-environment</li>
<li>(PDF) DESIGNING SUPPLEMENTARY MATERIALS FOR TEACHING-LEARNING OF WRITING EXPLANATORY TEXT BASED ON A GENRE-BASED APPROACH - ResearchGate, https://www.researchgate.net/publication/370064414_DESIGNING_SUPPLEMENTARY_MATERIALS_FOR_TEACHING-LEARNING_OF_WRITING_EXPLANATORY_TEXT_BASED_ON_A_GENRE-BASED_APPROACH</li>
<li>Formatting Instructions For NeurIPS 2025 - arXiv, https://arxiv.org/html/2505.21032v1</li>
<li>Cross referencing with the xr package in Overleaf, https://www.overleaf.com/learn/how-to/Cross_referencing_with_the_xr_package_in_Overleaf</li>
<li>How to reference figures that are in the supplementary content in main text? - TeX, https://tex.stackexchange.com/questions/397810/how-to-reference-figures-that-are-in-the-supplementary-content-in-main-text</li>
<li>arXiv: post supplementary files using external document and xr - LaTeX Stack Exchange, https://tex.stackexchange.com/questions/399065/arxiv-post-supplementary-files-using-external-document-and-xr</li>
<li>Cross Referencing with the xr Package in Overleaf (and custom Section Names) - TeX, https://tex.stackexchange.com/questions/693337/cross-referencing-with-the-xr-package-in-overleaf-and-custom-section-names</li>
<li>Insert subappendices in minitoc - TeX - LaTeX Stack Exchange, https://tex.stackexchange.com/questions/334747/insert-subappendices-in-minitoc</li>
<li>Make a table of contents specific to appendix (article class) and display “Appendices” as a section in main TOC - LaTeX Stack Exchange, https://tex.stackexchange.com/questions/686939/make-a-table-of-contents-specific-to-appendix-article-class-and-display-appen</li>
<li>Paper Information / Code Submission Policy - NeurIPS 2025, https://neurips.cc/public/guides/CodeSubmissionPolicy</li>
<li>AUTHOR GUIDELINES - CVPR 2022 - The Computer Vision Foundation, https://cvpr2022.thecvf.com/author-guidelines</li>
<li>How to create an anonymous repository for double-blind peer review? #175306 - GitHub, https://github.com/orgs/community/discussions/175306</li>
<li>ICML 2026 Author Instructions, https://icml.cc/Conferences/2026/AuthorInstructions</li>
<li>[D] Best practice for providing code during review : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/1nni5ld/d_best_practice_for_providing_code_during_review/</li>
<li>Authors Presentation Materials Instructions - IEEE ICRA 2025, https://2025.ieee-icra.org/contribute/authors-presentation-materials-instructions/</li>
<li>Uploading presentation video to InfoVaya - IROS | Khalifa University, https://iros2024-abudhabi.org/uploading-presentation</li>
<li>Call for Papers - IROS 2024, <a href="http://iros2024-abudhabi.org/assets/Call%20for%20papers.pdf?v=3.0">http://iros2024-abudhabi.org/assets/Call%20for%20papers.pdf?v=3.0</a></li>
<li>Supplementary Material - AAAI, https://aaai.org/conference/aaai/aaai-23/supplementary-material/</li>
<li>Structuring Robotics Conference Papers - YouTube, https://www.youtube.com/watch?v=C8ZlyA-CMJI</li>
<li>Understanding Video Narratives Through Dense Captioning with Linguistic Modules, Contextual Semantics, and Caption Selection - MDPI, https://www.mdpi.com/2673-2688/6/8/166</li>
<li>Voice-over or subtitles: which is better for video translation? – Scriptis, https://www.scriptis.com/voiceover-or-subtitles/</li>
<li>Voice Over: Learn to Do It Like a Pro | TechSmith, https://www.techsmith.com/blog/voice-over/</li>
<li>(PDF) Subtitles vs. narration: The acquisition of information from visual-verbal and audio-verbal channels when watching a television documentary - ResearchGate, https://www.researchgate.net/publication/313369238_Subtitles_vs_narration_The_acquisition_of_information_from_visual-verbal_and_audio-verbal_channels_when_watching_a_television_documentary</li>
<li>Research Paper Structure - UCSD Psychology, https://psychology.ucsd.edu/undergraduate-program/undergraduate-resources/academic-writing-resources/writing-research-papers/research-paper-structure.html</li>
<li>Supplementary Materials in Research: 5 Tips for Authors - Paperpal, https://paperpal.com/blog/researcher-resources/phd-pointers/supplementary-materials-in-research-5-tips-for-authors</li>
<li>ripl/nerfies-template: Nerfies template for project page (website) and code - GitHub, https://github.com/ripl/nerfies-template</li>
<li>nerfies/nerfies.github.io, https://github.com/nerfies/nerfies.github.io</li>
<li>Project Page - Tutorials, https://tutorial.j3soon.com/research/project-page/</li>
<li>eliahuhorwitz/Academic-project-page-template: A project page template for academic papers. Demo at https://eliahuhorwitz.github.io/Academic-project-page-template - GitHub, https://github.com/eliahuhorwitz/Academic-project-page-template</li>
<li>alshedivat/al-folio: A beautiful, simple, clean, and responsive Jekyll theme for academics - GitHub, https://github.com/alshedivat/al-folio</li>
<li>NeurIPS 2025 Datasets and Benchmarks FAQ, https://neurips.cc/Conferences/2025/DatasetsBenchmarks-FAQ</li>
<li>[NeurIPS 2024] Depth Anything V2. A More Capable Foundation Model for Monocular Depth Estimation - GitHub, https://github.com/DepthAnything/Depth-Anything-V2</li>
<li>NeurIPS 2025 - a Hugging Face Space by ai-conferences, https://huggingface.co/spaces/ai-conferences/NeurIPS2025</li>
<li>Demonstration Program: Call for Demonstrations - Association for the Advancement of Artificial Intelligence (AAAI), https://aaai.org/conference/aaai/aaai-26/demonstration-call/</li>
<li>Call for Papers – Demonstrations Track - IJCAI 2025, https://2025.ijcai.org/call-for-papers-demonstrations/</li>
<li>Call for Demo Papers - CIKM 2025, https://cikm2025.org/calls/demo-papers</li>
<li>System Demonstrations - ACL 2025, https://2025.aclweb.org/calls/system_demonstration/</li>
<li>Demonstrations Track - ICDE 2026, https://icde2026.github.io/cf-demonstrations.html</li>
<li>2024 Reviewer Guidelines - NeurIPS 2025, https://neurips.cc/Conferences/2024/ReviewerGuidelines</li>
<li>Latest CVPR 2025 clarification and Q&amp;A - The Computer Vision Foundation, https://cvpr.thecvf.com/Conferences/2025/Clarification</li>
<li>Towards bridging the gap: Systematic sim-to-real transfer for diverse legged robots - arXiv, https://arxiv.org/html/2509.06342v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>