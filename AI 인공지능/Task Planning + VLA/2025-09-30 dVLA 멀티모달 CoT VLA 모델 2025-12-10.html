<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:dVLA 멀티모달 CoT VLA 모델 (2025-09-30)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>dVLA 멀티모달 CoT VLA 모델 (2025-09-30)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">작업 계획 (Task Planning) + VLA</a> / <span>dVLA 멀티모달 CoT VLA 모델 (2025-09-30)</span></nav>
                </div>
            </header>
            <article>
                <h1>dVLA 멀티모달 CoT VLA 모델 (2025-09-30)</h1>
<p>2025-12-10, G30DR</p>
<pre><code class="language-mermaid">graph TD
    subgraph "1: 전통적 로봇 시스템 (Modular Pipeline)"
        A1["인식 (Perception)"] --&gt;|"정보 손실 가능성"| B1["계획 (Planning)"]
        B1 --&gt;|"오류 전파 (Error Propagation)"| C1["제어 (Control)"]
        D1["구조적 한계"] -.-&gt;|"비정형 환경 적응 어려움"| A1
    end

    subgraph "2: 기존 VLA 모델 (Autoregressive)"
        A2["비전 + 언어 입력"] --&gt; B2["트랜스포머 (Transformer)"]
        B2 --&gt;|"순차적 토큰 예측"| C2["행동 생성"]
        D2["한계점"] -.-&gt;|"느린 추론 (Drift 현상)"| B2
        D2 -.-&gt;|"이산-연속 데이터 충돌"| B2
    end

    subgraph "3: dVLA (Discrete Diffusion)"
        A3["비전/언어/행동 통합 입력"] --&gt; B3["이산 확산 모델 (Discrete Diffusion)"]
        B3 --&gt;|"동시적 궤적 생성 (Non-Autoregressive)"| C3["정밀 행동 제어"]
        D3["핵심 혁신"] -.-&gt;|"멀티모달 CoT (추론 능력)"| B3
        D3 -.-&gt;|"단일 이산 공간 통합"| B3
    end

    C1 -.-&gt;|"패러다임 전환"| A2
    C2 -.-&gt;|"한계 극복"| A3
</code></pre>
<h2>1.  서론: 로봇 공학의 패러다임 전환과 범용 모델의 필요성</h2>
<p>인공지능(AI)과 로봇 공학의 융합은 지난 수십 년간 학계와 산업계의 가장 중요한 화두 중 하나였다. 초기의 로봇 시스템은 인지(Perception), 계획(Planning), 제어(Control)가 각각 독립적인 모듈로 분리된 파이프라인 구조를 가졌다. 이러한 접근 방식은 각 모듈을 개별적으로 최적화할 수 있다는 장점이 있었으나, 모듈 간의 정보 손실과 오류 전파(Error Propagation)라는 구조적 한계를 안고 있었다. 특히 비정형화된 실제 환경(Unstructured Real-world Environment)에서 로봇이 복잡한 작업을 수행하기 위해서는 시각적 정보와 언어적 지시, 그리고 물리적 행동을 유기적으로 결합할 수 있는 통합된 지능이 필수적이다.</p>
<p>최근 대규모 언어 모델(LLM)과 멀티모달 모델(LMM)의 비약적인 발전은 로봇 공학 분야에 ‘비전-언어-행동(Vision-Language-Action, VLA)’ 모델이라는 새로운 패러다임을 제시하였다. VLA 모델은 방대한 인터넷 데이터와 로봇 주행 데이터를 학습하여, 시각적 입력과 자연어 명령을 받아 로봇의 행동을 직접 생성하는 엔드-투-엔드(End-to-End) 학습을 지향한다.1 구글의 RT-2나 OpenVLA와 같은 선구적인 모델들은 로봇이 이전에 보지 못한 물체를 인식하거나 추상적인 명령을 수행할 수 있는 가능성을 보여주었다.2</p>
<p>그러나 기존의 VLA 모델들은 대부분 트랜스포머(Transformer) 기반의 자기회귀(Autoregressive) 생성 방식을 채택하고 있어 근본적인 한계에 직면해 있다. 첫째, 자기회귀 모델은 토큰을 순차적으로 예측하기 때문에 추론 시간이 길어질수록 초기 오류가 누적되는 드리프트(Drift) 현상에 취약하다. 둘째, 텍스트와 같은 이산적(Discrete) 데이터와 로봇 팔의 관절 각도와 같은 연속적(Continuous) 데이터 간의 분포 차이는 학습 과정에서 그래디언트 충돌(Gradient Conflict)을 유발하여 최적화를 어렵게 만든다.3 셋째, 로봇이 복잡한 작업을 수행하기 위해서는 단순한 반사적 행동 생성을 넘어, 작업의 단계를 계획하고 미래를 예측하는 고차원적인 추론 능력이 요구되지만, 기존 모델들은 이를 명시적으로 구현하는 데 어려움을 겪었다.</p>
<p>이러한 배경 속에서 등장한 **dVLA(Diffusion Vision-Language-Action Model)**는 기존의 자기회귀 방식이 아닌 <strong>이산 확산(Discrete Diffusion)</strong> 방식을 도입하여 VLA 모델의 새로운 지평을 열었다. dVLA는 시각, 언어, 행동 데이터를 단일한 이산 공간으로 통합하고, 이를 확산 기반의 노이즈 제거 목표(Denoising Objective)로 학습함으로써 모달리티 간의 장벽을 허물었다. 특히 인간의 사고 과정과 유사한 <strong>멀티모달 생각의 사슬(Multimodal Chain-of-Thought, CoT)</strong> 메커니즘을 도입하여, 로봇이 행동을 수행하기 전에 시각적 목표와 텍스트 계획을 먼저 생성하도록 설계되었다.3</p>
<p>본 보고서는 arXiv:2509.25681 논문 및 관련 기술 문서를 바탕으로 dVLA의 기술적 배경, 아키텍처, 학습 방법론, 그리고 실험적 성능을 포괄적이고 심층적으로 분석한다. 특히 dVLA가 제시하는 ’통합된 이산 확산’과 ’멀티모달 CoT’가 로봇의 일반화 능력과 추론 능력을 어떻게 혁신적으로 향상시켰는지에 대해 면밀히 고찰한다.</p>
<pre><code class="language-mermaid">graph TD
    subgraph "자기회귀 모델 (Autoregressive)"
        T1["시점 t의 예측"] --&gt;|"오류 발생 (Small Error)"| T2["시점 t+1의 예측"]
        T2 --&gt;|"오류 누적 (Accumulated Error)"| T3["시점 t+2의 예측"]
        T3 --&gt;|"심각한 궤적 이탈 (Drift)"| FAIL["작업 실패"]
    end

    subgraph "dVLA (Diffusion)"
        NOISE["노이즈 섞인 전체 궤적"] --&gt;|"전역적 수정 (Global Refinement)"| STEP1["디노이징 스텝 1"]
        STEP1 --&gt;|"전체 경로 동시 최적화"| STEP2["디노이징 스텝 n"]
        STEP2 --&gt;|"일관성 유지"| SUCCESS["안정적 전체 궤적 생성"]
    end

    style FAIL fill:#ffcccc,stroke:#ff0000
    style SUCCESS fill:#ccffcc,stroke:#00aa00
</code></pre>
<h2>2.  이론적 배경 및 기존 연구의 한계</h2>
<h3>2.1  비전-언어-행동(VLA) 모델의 발전과 정체</h3>
<p>VLA 모델은 대규모 비전-언어 모델(VLM)을 로봇 제어 영역으로 확장한 개념이다. 초기의 VLA 모델들은 VLM의 강력한 의미론적 이해(Semantic Understanding) 능력을 로봇의 행동 생성에 전이(Transfer)시키는 데 주력했다. 예를 들어, RT-2는 이미지를 텍스트 토큰으로 변환하고, 이를 언어 모델에 입력하여 로봇의 행동을 텍스트 토큰 형태로 출력하는 방식을 사용했다.</p>
<p>하지만 이러한 접근 방식은 본질적으로 서로 다른 성격의 데이터를 억지로 하나의 틀에 끼워 맞추는 형태였다. 텍스트는 불연속적이고 상징적인 정보를 담고 있는 반면, 로봇의 행동은 물리적 법칙에 따르는 연속적인 궤적(Trajectory)이다. 이를 단순히 텍스트 토큰으로 이산화(Discretization)할 경우, 정밀한 제어가 어려워지거나 정보의 손실이 발생할 수 있다.4 또한, 언어 생성과 행동 생성이라는 두 가지 목표를 동시에 학습시킬 때 발생하는 최적화 목표의 상충(Objective Mismatch)은 모델의 성능을 제한하는 주요 원인이 되었다.</p>
<pre><code class="language-mermaid">graph TB
    subgraph "기존 VLA의 문제점"
        L_TEXT["텍스트 손실 함수 (이산적)"]
        L_ACT["행동 회귀 손실 함수 (연속적)"]
        OPT["최적화 과정"]
        CONFLICT["그래디언트 충돌 (Gradient Conflict)&lt;br/&gt;분포 차이로 인한 학습 불안정"]
        
        L_TEXT --&gt; OPT
        L_ACT --&gt; OPT
        OPT --&gt; CONFLICT
    end

    subgraph "dVLA의 해결책"
        TOKEN_ALL["모든 데이터의 토큰화&lt;br/&gt;(이미지, 텍스트, 행동)"]
        UNIFIED["단일 이산 공간 (Unified Discrete Space)"]
        OBJ["단일 목표 함수 (Unified Objective)&lt;br/&gt;Next-Token Prediction / Masked Recovery"]
        SMOOTH["안정적인 최적화 (Stable Optimization)"]

        TOKEN_ALL --&gt; UNIFIED
        UNIFIED --&gt; OBJ
        OBJ --&gt; SMOOTH
    end
    
    CONFLICT -.-&gt;|"해결"| SMOOTH
</code></pre>
<h3>2.2  확산 모델(Diffusion Model)의 부상과 로봇 공학적 적용</h3>
<p>생성형 AI 분야에서 확산 모델은 데이터의 분포를 학습하여 노이즈로부터 고품질의 샘플을 생성하는 능력으로 주목받았다. 로봇 공학에서도 Diffusion Policy와 같은 연구들이 등장하여, 복잡하고 다봉(Multimodal)적인 행동 분포를 효과적으로 모델링할 수 있음을 입증했다.1 확산 모델은 전체 행동 궤적을 한 번에 생성할 수 있어 자기회귀 모델의 단점인 오차 누적 문제를 완화할 수 있으며, 고차원 행동 공간에서도 안정적인 학습이 가능하다.</p>
<p>그러나 기존의 확산 기반 로봇 정책(Policy)들은 대부분 시각적 이해나 언어적 추론 능력보다는 저수준의 행동 생성(Low-level Action Generation)에 초점을 맞추었다. 즉, “어떻게 움직일 것인가“는 잘 해결했지만, “무엇을 왜 해야 하는가“에 대한 고차원적인 지능은 부족했다. 또한, 연속적인 확산 모델은 텍스트와 같은 이산적인 데이터와 결합하기 까다로워, 진정한 의미의 멀티모달 통합을 이루기에는 한계가 있었다.6</p>
<h3>2.3  dVLA의 접근: 통합과 추론의 결합</h3>
<p>dVLA는 이러한 문제들을 해결하기 위해 <strong>이산 확산(Discrete Diffusion)</strong> 프레임워크를 채택했다. 이는 연속적인 공간이 아닌 이산적인 토큰 공간에서 확산 과정을 수행하는 것으로, 텍스트와 이미지, 행동을 모두 동일한 형태의 이산 토큰으로 변환하여 처리할 수 있게 해준다. 이를 통해 dVLA는 시각적 인식, 언어적 추론, 행동 생성을 <strong>단일한 확률적 목표(Unified Probabilistic Objective)</strong> 하에 통합하였다.3</p>
<p>더 나아가 dVLA는 로봇에게 ’생각하는 능력’을 부여하기 위해 <strong>멀티모달 CoT</strong>를 도입했다. 이는 대규모 언어 모델(LLM)에서 복잡한 문제를 단계별로 풀어나가는 ‘Chain-of-Thought’ 기법을 로봇 제어에 적용한 것이다. dVLA는 행동을 생성하기 전에 (1) 미래의 시각적 하위 목표(Visual Subgoal)를 상상하고, (2) 구체적인 텍스트 계획(Textual Plan)을 수립하는 과정을 거친다. 이는 로봇의 행동에 인과성을 부여하고, 장기적인 작업 수행 능력을 획기적으로 향상시키는 핵심 기제가 된다.5</p>
<h2>3.  dVLA 시스템 아키텍처 심층 분석</h2>
<p>dVLA의 아키텍처는 이질적인 모달리티를 통합하기 위한 정교한 토큰화 전략과, 이를 처리하는 이산 확산 백본, 그리고 추론 효율성을 위한 가속화 모듈로 구성된다.</p>
<h3>3.1  모달리티별 이산 토큰화 전략 (Modality-Specific Discrete Tokenization)</h3>
<p>dVLA는 모든 입력 데이터를 이산 토큰으로 변환하여 공통의 어휘 사전(Vocabulary) 위에서 처리한다. 이를 위해 각 모달리티의 특성에 최적화된 토크나이저(Tokenizer)를 사용한다.4</p>
<pre><code class="language-mermaid">graph LR
    title["통합 어휘 공간 (Unified Vocabulary Space) 구조"]
    
    ROOT["통합 어휘 사전 (Total: 136,704 Tokens)"]
    
    subgraph "언어 (Language) - 지배적 비율"
        LANG["LLaDA Tokenizer"]
        L_SIZE["126,464 토큰"]
        L_ROLE["명령, 추론, 일반 상식 표현"]
    end

    subgraph "비전 (Vision)"
        VIS["MAGVIT-v2"]
        V_SIZE["8,192 토큰"]
        V_ROLE["이미지 구조 및 의미 정보"]
    end

    subgraph "행동 (Action) - 고압축"
        ACT["FAST Tokenizer"]
        A_SIZE["2,048 토큰"]
        A_ROLE["정밀한 물리적 움직임"]
    end

    LANG --&gt; ROOT
    VIS --&gt; ROOT
    ACT --&gt; ROOT
    
    L_SIZE --- LANG
    V_SIZE --- VIS
    A_SIZE --- ACT
</code></pre>
<h4>3.1.1  비전 (Vision): MAGVIT-v2</h4>
<p>이미지 데이터는 픽셀 단위의 연속적인 정보로 구성되어 있다. dVLA는 이를 효율적인 이산 토큰으로 변환하기 위해 <strong>MAGVIT-v2</strong>를 사용한다. MAGVIT-v2는 벡터 양자화(Vector Quantization) 기법을 활용하여 이미지를 의미론적 정보를 담은 코드북(Codebook)의 인덱스로 매핑한다.</p>
<ul>
<li><strong>압축 효율:</strong> 256x256 해상도의 이미지를 256개의 토큰으로 압축(압축률 16)하거나, 512x512 이미지를 1024개의 토큰으로 변환한다.8</li>
<li><strong>코드북 크기:</strong> 8,192개의 시각적 어휘를 사용하여 이미지의 다양한 특징을 표현한다.</li>
<li><strong>역할:</strong> 이를 통해 확산 모델은 픽셀 단위의 세부 사항보다는 이미지의 구조적이고 의미론적인 정보에 집중하여 학습할 수 있게 된다.</li>
</ul>
<h4>3.1.2  언어 (Language): LLaDA Tokenizer</h4>
<p>텍스트 처리를 위해서는 LLaDA(Large Language Diffusion Architecture)의 토크나이저를 채택했다.</p>
<ul>
<li><strong>어휘 크기:</strong> 126,464개의 방대한 어휘를 지원하여 복잡한 자연어 명령과 추론 과정을 정교하게 표현한다.3</li>
<li><strong>역할:</strong> 사용자의 명령(Instruction)과 모델 내부의 사고 과정(Thought Process)을 이산적인 텍스트 토큰으로 변환한다.</li>
</ul>
<h4>3.1.3  행동 (Action): FAST Tokenizer</h4>
<p>연속적인 로봇의 행동 데이터를 이산 토큰으로 변환하는 것은 VLA 모델의 가장 큰 난제 중 하나이다. dVLA는 <strong>FAST(Frequency-Aware Spatial-Temporal) Tokenizer</strong>를 도입하여 이 문제를 해결했다.8</p>
<ul>
<li><strong>이산 코사인 변환(DCT):</strong> 연속적인 행동 궤적(Trajectory)을 시간 도메인에서 주파수 도메인으로 변환한다. 이는 신호의 에너지를 저주파 성분에 집중시키는 효과(Energy Compaction)가 있어 데이터 압축 효율을 높인다.</li>
<li><strong>바이트 페어 인코딩(BPE):</strong> DCT 계수들을 양자화한 후, 텍스트 압축에 주로 쓰이는 BPE 알고리즘을 적용하여 빈번하게 등장하는 행동 패턴을 하나의 토큰으로 묶는다.</li>
<li><strong>어휘 크기:</strong> 최종적으로 2,048개의 행동 어휘를 생성한다.3</li>
<li><strong>의의:</strong> 단순한 선형 양자화(Linear Quantization) 방식에 비해 훨씬 적은 수의 토큰으로 복잡하고 정밀한 행동을 표현할 수 있게 해주며, 이는 추론 속도 향상과 장기 의존성 학습에 결정적인 기여를 한다.</li>
</ul>
<p>이 세 가지 토크나이저를 통해 생성된 토큰들은 총 136,704개(기존 LLaDA 어휘 + 확장된 행동/비전 어휘)의 통합 어휘 공간을 형성한다.3 이를 통해 dVLA는 텍스트, 이미지, 행동을 구별 없이 하나의 시퀀스로 처리할 수 있게 된다.</p>
<pre><code class="language-mermaid">graph TD
    subgraph "입력 (Input)"
        RAW["연속적 행동 궤적 (Continuous Action Trajectory)&lt;br/&gt;(Time Domain)"]
    end

    subgraph "1단계: 주파수 변환"
        DCT["이산 코사인 변환 (DCT)"]
        FREQ["주파수 도메인 (Frequency Domain)"]
        ENERGY["에너지 집중 (Energy Compaction)&lt;br/&gt;저주파 성분 보존"]
    end

    subgraph "2단계: 토큰화"
        QUANT["양자화 (Quantization)"]
        BPE["바이트 페어 인코딩 (BPE)"]
        GROUP["빈번한 패턴 그룹화"]
    end

    subgraph "출력 (Output)"
        TOKENS["행동 토큰 (Action Tokens)&lt;br/&gt;어휘 크기: 2,048"]
    end

    RAW --&gt; DCT
    DCT --&gt; FREQ
    FREQ --&gt; ENERGY
    ENERGY --&gt; QUANT
    QUANT --&gt; BPE
    BPE --&gt; GROUP
    GROUP --&gt; TOKENS
</code></pre>
<h3>3.2  통합된 이산 확산 모델링 (Unified Discrete Diffusion Modeling)</h3>
<p>dVLA의 학습은 <strong>MMaDA(Multimodal Large Diffusion Language Models)</strong> 아키텍처를 기반으로 한다.8 모델은 입력 시퀀스 <span class="math math-inline">x</span> 내의 일부 토큰을 무작위로 마스킹()하고, 마스킹되지 않은 나머지 정보를 바탕으로 이를 복원하는 방식으로 학습된다.</p>
<p>입력 시퀀스 <span class="math math-inline">x</span>의 구성은 다음과 같다:</p>
<p><span class="math math-display">
x = \{o, l, s, o_{goal}, r, a_{chunk}\}
</span><br />
여기서 <span class="math math-inline">o</span>는 현재 관측 이미지, <span class="math math-inline">l</span>은 언어 지시, <span class="math math-inline">s</span>는 로봇의 상태(관절 각도 등), <span class="math math-inline">o_{goal}</span>은 예측된 미래의 시각적 하위 목표, <span class="math math-inline">r</span>은 텍스트 추론(CoT), <span class="math math-inline">a_{chunk}</span>는 수행할 행동 청크를 의미한다.8</p>
<p>학습 목표 함수 <span class="math math-inline">L_{unify}(\theta)</span>는 다음과 같이 정의된다:</p>
<p><span class="math math-display">
L_{unify}(\theta) = -E_{t, x_0, x_t} \left[ \log p_\theta(x_0^i | x_t) \right]
</span></p>
<ul>
<li><span class="math math-inline">x_0</span>: 마스킹되지 않은 원본 데이터 (Ground Truth)</li>
<li><span class="math math-inline">t</span>: 노이즈 레벨을 나타내는 타임스텝 ($t \in $)</li>
<li><span class="math math-inline">x_t</span>: 전방 확산 과정(Forward Process)을 통해 일부가 마스킹된 데이터</li>
<li><span class="math math-inline">\mathbb{I}</span>: 인디케이터 함수 (마스킹된 위치에서만 손실을 계산)</li>
</ul>
<p>이 수식은 모델이 주어진 문맥(Context)을 바탕으로 누락된 정보를 채워 넣도록 유도한다. 예를 들어, 현재 이미지(<span class="math math-inline">o</span>)와 명령(<span class="math math-inline">l</span>)이 주어졌을 때 미래의 행동(<span class="math math-inline">a_{chunk}</span>)을 예측하거나, 반대로 행동이 주어졌을 때 그 결과 이미지(<span class="math math-inline">o_{goal}</span>)를 예측하는 식이다. 이러한 양방향성(Bidirectionality)은 모델이 모달리티 간의 인과관계를 깊이 있게 이해하도록 돕는다.</p>
<pre><code class="language-mermaid">graph TD
    subgraph "컨텍스트 (Context / Prefix)"
        O["Visual Obs (o)&lt;br/&gt;(현재 이미지)"]
        L["Instruction (l)&lt;br/&gt;(텍스트 명령)"]
        S["Robot State (s)&lt;br/&gt;(관절 상태)"]
    end

    subgraph "생성 목표 (Generative Targets / CoT)"
        G["Visual Goal (o_goal)&lt;br/&gt;(미래 이미지 예측)"]
        R["Reasoning (r)&lt;br/&gt;(텍스트 사고 과정)"]
        A["Action Chunk (a_chunk)&lt;br/&gt;(행동 토큰들)"]
    end

    O --&gt; L --&gt; S --&gt; G --&gt; R --&gt; A
    
    style O fill:#e1f5fe
    style L fill:#e1f5fe
    style S fill:#e1f5fe
    style G fill:#fff9c4,stroke:#fbc02d
    style R fill:#fff9c4,stroke:#fbc02d
    style A fill:#ffccbc,stroke:#ff5722
</code></pre>
<pre><code class="language-mermaid">graph TD
    subgraph "입력 모달리티 (Input Modalities)"
        V["이미지 (Vision)"]
        L["언어 명령 (Language)"]
        A["로봇 행동 (Action)"]
    end

    subgraph "토크나이저 (Tokenizers)"
        VP["MAGVIT-v2"]
        LP["LLaDA Tokenizer"]
        AP["FAST Tokenizer"]
    end

    subgraph "처리 과정 (Process)"
        V_P["벡터 양자화 (Vector Quantization)&lt;br/&gt;코드북 매핑"]
        L_P["텍스트 토큰화"]
        A_P1["DCT (주파수 변환)"] --&gt; A_P2["양자화 &amp; BPE"]
    end

    subgraph "통합 공간 (Unified Output)"
        VOCAB["통합 어휘 사전 (Unified Vocabulary)&lt;br/&gt;총 136,704개 토큰"]
        TOKENS["이산 토큰 시퀀스"]
    end

    V --&gt; VP --&gt; V_P --&gt; VOCAB
    L --&gt; LP --&gt; L_P --&gt; VOCAB
    A --&gt; AP --&gt; A_P1
    A_P2 --&gt; VOCAB
    
    VOCAB --&gt; TOKENS
</code></pre>
<pre><code class="language-mermaid">graph TD
   
    subgraph "입력 시퀀스 구성 (Unified Sequence x)"
        O["현재 관측&lt;br&gt;(o)"]
        L["언어 지시&lt;br&gt;(l)"]
        S["로봇 상태&lt;br&gt;(s)"]
        G["미래 시각 목표&lt;br&gt;(o_goal)"]
        R["텍스트 추론/CoT&lt;br&gt;(r)"]
        AC["행동 청크&lt;br&gt;(a_chunk)"]
    end

    subgraph "확산 학습 과정 (Diffusion Training)"
        MASK["무작위 마스킹&lt;br&gt;(Random Masking)"]
        NOISE["노이즈 레벨 t 주입"]
        MODEL["이산 확산 모델&lt;br&gt;(MMaDA Backbone)"]
        DENOISE["디노이징&lt;br&gt;(Denoising Objective)"]
    end

    subgraph "학습 목표 (Objective)"
        LOSS["L_unify:&lt;br&gt;마스킹된 정보 복원&lt;br/&gt;(양방향 문맥 이해)"]
    end

    O &amp; L &amp; S &amp; G &amp; R &amp; AC --&gt; MASK
    MASK --&gt; NOISE --&gt; MODEL
    MODEL --&gt; DENOISE --&gt; LOSS
    
    style LOSS stroke:#f00,stroke-width:2px
</code></pre>
<pre><code class="language-mermaid">graph LR
    title["마스킹 기반 양방향 생성 메커니즘"]

    subgraph "시나리오 A: 행동 생성 (Action Generation)"
        IN_A1["입력: 현재 이미지 (o)"]
        IN_A2["입력: 언어 명령 (l)"]
        MASK_A["마스킹: 행동 청크 (MASK)"]
        PROCESS_A["이산 확산 복원"]
        OUT_A["출력: 물리적 행동 (a_chunk)"]
        
        IN_A1 &amp; IN_A2 &amp; MASK_A --&gt; PROCESS_A --&gt; OUT_A
    end

    subgraph "시나리오 B: 결과 상상 (Outcome Prediction)"
        IN_B1["입력: 현재 이미지 (o)"]
        IN_B2["입력: 특정 행동 (a)"]
        MASK_B["마스킹: 미래 목표 (MASK)"]
        PROCESS_B["이산 확산 복원"]
        OUT_B["출력: 예상되는 미래 이미지 (o_goal)"]

        IN_B1 &amp; IN_B2 &amp; MASK_B --&gt; PROCESS_B --&gt; OUT_B
    end
</code></pre>
<h3>3.3  멀티모달 생각의 사슬 (Multimodal CoT) 메커니즘</h3>
<p>dVLA의 가장 독창적인 특징은 행동 생성 이전에 명시적인 추론 단계를 거친다는 점이다.3</p>
<ol>
<li><strong>시각적 하위 목표 (Visual Subgoal Generation):</strong> 모델은 현재 상태에서 약 5~50 프레임 뒤에 나타날 것으로 예상되는 이미지를 생성한다. 이는 로봇이 자신의 행동 결과를 시각적으로 시뮬레이션하는 과정이다. 예를 들어, “물건을 집으라“는 명령을 받으면 로봇 팔이 물건에 접근한 상태의 이미지를 먼저 생성한다.</li>
<li><strong>텍스트 추론 (Textual Reasoning):</strong> “파란색 큐브를 집기 위해 그리퍼를 아래로 이동한다“와 같이 구체적인 행동 지침을 텍스트로 생성한다. 이는 복잡한 작업을 논리적인 단계로 분해(Decomposition)하는 역할을 한다.</li>
<li><strong>근거 있는 행동 생성 (Grounded Action Generation):</strong> 위에서 생성된 시각적, 언어적 단서를 바탕으로 실제 물리적 행동 토큰을 생성한다.</li>
</ol>
<p>이러한 CoT 과정은 로봇이 단순히 입력에 반응하는 것이 아니라, 계획하고 예측하며 행동하는 ‘시스템 2(System 2)’ 사고를 가능하게 한다.2</p>
<pre><code class="language-mermaid">sequenceDiagram
    autonumber
    participant Env as 환경/사용자
    participant VLM as dVLA&lt;br&gt;(인지/추론)
    participant Action as dVLA&lt;br&gt;(행동 생성)
    participant Robot as "로봇&lt;br&gt;하드웨어"

    Note over Env, Robot: 멀티모달 CoT 추론 과정

    Env-&gt;&gt;VLM: 1. 현재 이미지 + "파란 큐브 집어라" (명령)
    
    rect rgb(240, 248, 255)
        Note right of VLM: 내부 사고 과정&lt;br&gt;(Internal Reasoning)
        VLM-&gt;&gt;VLM: 2. 시각적 하위 목표 상상&lt;br&gt;(Visual Subgoal)&lt;br/&gt;"물체를 잡은 미래 이미지 생성"
        VLM-&gt;&gt;VLM: 3. 텍스트 계획 수립&lt;br&gt;(Textual Plan)&lt;br/&gt;"그리퍼 이동 -&gt; 하강 -&gt; 파지"
    end
    
    VLM-&gt;&gt;Action: 4. 사고 결과&lt;br&gt;(이미지+텍스트) 전달
    Action-&gt;&gt;Action: 5. 근거 있는 행동 토큰 생성&lt;br&gt;(Grounded Action)
    Action-&gt;&gt;Robot: 6. 행동 청크&lt;br&gt;(Action Chunk) 전송
    Robot-&gt;&gt;Env: 7. 물리적 작업 수행
</code></pre>
<pre><code class="language-mermaid">graph TD
    title["System 1 (기존) vs System 2 (dVLA) 프로세스 비교"]

    subgraph "System 1: 반사적 처리 (Reflexive)"
        INPUT1["자극 (입력)"] --&gt; DIRECT["직관적 매핑 (Direct Mapping)"]
        DIRECT --&gt; ACT1["즉각적 행동"]
        LIMIT1["한계: 복잡한 논리 부재"]
        ACT1 -.-&gt; LIMIT1
    end

    subgraph "System 2: 숙고적 처리 (Deliberate) - dVLA"
        INPUT2["자극 (입력)"] --&gt; PLAN["계획 수립 (Planning)"]
        PLAN --&gt; SIM["시뮬레이션 (Visual CoT)"]
        SIM --&gt; REASON["논리적 추론 (Textual CoT)"]
        REASON --&gt; ACT2["근거 있는 행동"]
        MERIT2["장점: 장기적 목표 달성"]
        ACT2 -.-&gt; MERIT2
    end

    style DIRECT fill:#eee,stroke:#999
    style PLAN fill:#d4f1f4,stroke:#005f73
    style SIM fill:#d4f1f4,stroke:#005f73
    style REASON fill:#d4f1f4,stroke:#005f73
</code></pre>
<h2>4.  실시간 제어를 위한 추론 가속화 전략</h2>
<p>확산 모델은 일반적으로 수십 번의 반복적인 노이즈 제거(Denoising) 과정을 거치기 때문에 추론 속도가 느리다는 단점이 있다. 실시간성이 생명인 로봇 제어에서 이는 치명적일 수 있다. dVLA는 이를 극복하기 위해 두 가지 가속화 전략을 도입했다.3</p>
<pre><code class="language-mermaid">graph TD

    START["시작: 노이즈 토큰 초기화 (x_T)"] --&gt; CONDITION["조건 입력 (이미지/명령)"]
    
    subgraph "반복적 디노이징 (Iterative Denoising)"
        LOOP_START{"Loop: t = T to 1"}
        
        CHECK_PREFIX{"토큰이 접두사(Prefix)인가?"}
        
        YES_PATH["Yes: KV 캐시 로드"]
        NO_PATH["No: 어텐션 연산 수행"]
        
        UPDATE["토큰 업데이트 (x_t-1)"]
        CACHE_SAVE["새로운 KV 캐시 저장"]
        
        LOOP_START --&gt; CHECK_PREFIX
        CHECK_PREFIX --&gt;|"이미지/명령 부분"| YES_PATH
        CHECK_PREFIX --&gt;|"생성 중인 부분"| NO_PATH
        
        YES_PATH --&gt; UPDATE
        NO_PATH --&gt; CACHE_SAVE --&gt; UPDATE
        UPDATE --&gt; LOOP_START
    end

    LOOP_START --&gt;|"t=0 도달"| FINISH["최종 행동 시퀀스 출력"]
</code></pre>
<h3>4.1  접두사 어텐션 마스크 (Prefix Attention Mask)</h3>
<p>추론 과정에서 이미 확정된 정보(예: 입력 이미지, 명령)나 이전에 생성된 토큰들에 대해서는 불필요한 연산을 줄이는 기법이다. 일반적으로 양방향 어텐션(Bidirectional Attention)은 모든 토큰 쌍 간의 관계를 계산해야 하므로 비용이 많이 든다. dVLA는 ’접두사(Prefix)’로 간주되는 부분에 대해서는 어텐션 패턴을 고정하거나 최적화하여, 모델이 생성해야 할 새로운 정보에 집중하도록 유도한다. 이는 계산 복잡도를 줄이면서도 문맥 정보를 효과적으로 유지하게 한다.</p>
<h3>4.2  확산 모델을 위한 KV 캐싱 (KV Caching)</h3>
<p>LLM에서 널리 쓰이는 KV(Key-Value) 캐싱을 확산 모델에 적용하였다. 반복적인 디노이징 단계에서 변하지 않는 입력(Conditioning) 부분에 대한 Key와 Value 값을 캐시에 저장해두고 재사용함으로써 중복 연산을 제거한다. 확산 모델의 특성상 매 스텝마다 입력이 조금씩 변하지만, 조건부 생성(Conditional Generation)의 경우 조건부 정보는 고정되어 있다는 점을 활용한 것이다.</p>
<p>이 두 전략을 통해 dVLA는 추론 속도를 기존 대비 약 2배 향상시켰다. LIBERO 벤치마크에서는 약 <strong>2.9Hz</strong>, 실제 로봇 제어에서는 <strong>3Hz</strong>의 속도를 달성하여, 실시간 반응이 가능한 수준의 성능을 확보했다.4</p>
<pre><code class="language-mermaid">graph TD
    subgraph "문제 상황"
        SLOW["느린 확산 추론 속도"]
        ITER["반복적 디노이징 필요"]
    end

    subgraph "해결책 1: 접두사 최적화"
        ATTN["접두사 어텐션 마스크 (Prefix Attention Mask)"]
        FIXED["고정된 컨텍스트 (이미지/명령)"]
        CALC1["불필요한 양방향 연산 제거"]
        ATTN --&gt; FIXED --&gt; CALC1
    end

    subgraph "해결책 2: 캐싱 전략"
        KV["KV 캐싱 (KV Caching)"]
        COND["변하지 않는 조건부 입력"]
        CALC2["중복 연산 방지 (값 재사용)"]
        KV --&gt; COND --&gt; CALC2
    end

    subgraph "최종 결과"
        FAST["추론 속도 2배 향상"]
        FREQ["3Hz 실시간 제어 달성"]
    end

    SLOW --&gt; ATTN
    SLOW --&gt; KV
    CALC1 --&gt; FAST
    CALC2 --&gt; FAST
    FAST --&gt; FREQ
</code></pre>
<h2>5.  실험 및 성능 평가</h2>
<p>dVLA의 성능은 표준화된 시뮬레이션 벤치마크(LIBERO)와 실제 로봇 하드웨어(Franka Emika Panda)를 이용한 실험을 통해 검증되었다. 비교 대상으로는 대표적인 연속 행동 정책 모델인 Diffusion Policy, GR00T와 이산 행동 정책 모델인 OpenVLA 등이 포함되었다.</p>
<h3>5.1  LIBERO 벤치마크 결과 (Simulation)</h3>
<p>LIBERO는 로봇 학습 모델의 일반화 능력과 장기 작업 수행 능력을 평가하기 위한 도전적인 벤치마크이다. dVLA는 이 벤치마크에서 기존의 최첨단(SOTA) 모델들을 압도하는 성능을 보여주었다.</p>
<p><strong>[표 1] LIBERO 벤치마크 모델별 평균 성공률 비교</strong> 4</p>
<table><thead><tr><th><strong>모델 (Model)</strong></th><th><strong>정책 유형 (Policy Type)</strong></th><th><strong>성공률 (Success Rate)</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>Diffusion Policy (Chi et al.)</strong></td><td>Continuous</td><td>~35.0%</td><td>베이스라인</td></tr>
<tr><td><strong>OpenVLA (Kim et al.)</strong></td><td>Discrete</td><td>33.3%</td><td>7B 파라미터 대규모 모델</td></tr>
<tr><td><strong>GR00T (Bjorck et al.)</strong></td><td>Continuous</td><td>44.2%</td><td>-</td></tr>
<tr><td><strong>Vanilla dVLA (No CoT)</strong></td><td>Discrete Diffusion</td><td>52.5% ~ 55.8%</td><td>CoT 없이도 우수한 성능</td></tr>
<tr><td><strong>dVLA (Ours)</strong></td><td><strong>Discrete Diffusion</strong></td><td><strong>96.4%</strong></td><td><strong>SOTA 달성</strong></td></tr>
</tbody></table>
<ul>
<li><strong>압도적인 성능 격차:</strong> dVLA는 96.4%라는 경이적인 성공률을 기록하며, 30~40%대에 머물던 기존 모델들과의 격차를 벌렸다. 이는 dVLA의 접근 방식이 로봇 제어 문제에 매우 효과적임을 시사한다.</li>
<li><strong>CoT의 효과:</strong> Vanilla dVLA(55.8%)와 dVLA(96.4%)의 차이는 멀티모달 CoT가 단순히 부가적인 기능이 아니라 성능 향상의 핵심 요인임을 증명한다.</li>
</ul>
<pre><code class="language-mermaid">graph TD
    title["LIBERO 벤치마크 성공률 비교"]

    subgraph "비교 모델군"
        DP["Diffusion Policy (Continuous)&lt;br/&gt;성공률: ~35.0%"]
        OV["OpenVLA (Discrete AR)&lt;br/&gt;성공률: 33.3%"]
        GR["GR00T (Continuous)&lt;br/&gt;성공률: 44.2%"]
    end

    subgraph "dVLA 모델군"
        dVLA_V["Vanilla dVLA (No CoT)&lt;br/&gt;성공률: 55.8%"]
        dVLA_Full["dVLA (With CoT)&lt;br/&gt;성공률: 96.4%"]
    end

    DP --- Compare((성능 격차))
    OV --- Compare
    GR --- Compare
    
    Compare ==&gt;|"CoT 및 통합 확산의 효과"| dVLA_Full
    dVLA_V -.-&gt;|"CoT 추가 시"| dVLA_Full

    style dVLA_Full fill:#d4f1f4,stroke:#005f73,stroke-width:4px
    style DP fill:#f4d4d4
    style OV fill:#f4d4d4
</code></pre>
<h3>5.2  실제 로봇 실험 (Real-World Evaluation)</h3>
<p>시뮬레이션을 넘어 실제 물리 환경에서의 성능을 검증하기 위해 Franka 로봇을 사용하여 ‘물건 분류(Bin Picking)’, ‘컵 걸기(Hang Cups)’, ‘물건 옮기기(Pick &amp; Place)’ 등의 작업을 수행했다.</p>
<ul>
<li><strong>복잡한 환경 적응력:</strong> 특히 여러 물체가 무질서하게 섞여 있는 Bin Picking 작업에서 dVLA는 시각적 CoT를 통해 정확한 파지 위치를 예측하고 장애물을 회피하는 능력을 보여주었다.</li>
<li><strong>성공률:</strong> 실제 환경에서도 평균 **65%**의 성공률을 기록하여, GR00T(60%), Diffusion Policy/OpenVLA(35%)를 상회했다.3 특히 OpenVLA와 같은 기존 모델들은 복잡한 시각적 배경에서 물체를 인식하는 데 어려움을 겪었으나, dVLA는 강건한 성능을 유지했다.</li>
</ul>
<pre><code class="language-mermaid">graph TD
    title["작업 복잡도에 따른 모델 대응력 비교"]

    subgraph "단순 작업 (Pick &amp; Place)"
        TASK1["정형화된 환경"]
        RES1["대부분의 모델이 수행 가능"]
    end

    subgraph "고난이도 작업 (Bin Picking)"
        TASK2["비정형 환경 (무질서한 물체)"]
        OBS["장애물 및 가림 현상 발생"]
        
        FAIL_GRP["기존 모델 (OpenVLA 등)"]
        SUCC_GRP["dVLA"]
        
        REASON_F["시각적 인식 실패&lt;br/&gt;정밀 제어 부족"]
        REASON_S["Visual CoT로 위치 예측&lt;br/&gt;장애물 회피 경로 생성"]
    end

    TASK1 --&gt; RES1
    TASK2 --&gt; OBS
    OBS --&gt; FAIL_GRP
    OBS --&gt; SUCC_GRP
    
    FAIL_GRP --&gt; REASON_F
    SUCC_GRP --&gt; REASON_S

    style SUCC_GRP fill:#d4f1f4,stroke:#005f73,stroke-width:3px
</code></pre>
<h3>5.3  절제 연구 (Ablation Study)</h3>
<p>연구진은 CoT의 구성 요소가 성능에 미치는 영향을 분석했다.</p>
<ul>
<li><strong>Visual CoT Only:</strong> 미래 이미지만 예측할 경우에도 성능 향상이 있었으며, 이는 물리적 상호작용의 결과를 예측하는 것이 중요함을 보여준다.</li>
<li><strong>Textual CoT Only:</strong> 텍스트 계획만 생성할 경우, 논리적인 단계 구분에는 도움이 되지만 미세한 물리적 제어에는 한계가 있었다.</li>
<li><strong>Combined:</strong> 두 가지를 모두 사용했을 때 시너지 효과가 발생하여 최고의 성능을 기록했다.3</li>
</ul>
<pre><code class="language-mermaid">graph TD
    subgraph "옵션 A: 텍스트 CoT만 사용 (Text Only)"
        TXT["텍스트 계획 생성"] --&gt; LOGIC["논리적 단계 구분 성공"]
        LOGIC --&gt; LIMIT_A["미세 물리 제어 실패"]
        LIMIT_A --&gt; PERF_A["성능: 중간"]
    end

    subgraph "옵션 B: 비전 CoT만 사용 (Visual Only)"
        IMG["미래 이미지 예측"] --&gt; PHYS["물리적 상호작용 이해"]
        PHYS --&gt; LIMIT_B["장기 계획 부족"]
        LIMIT_B --&gt; PERF_B["성능: 우수"]
    end

    subgraph "옵션 C: 통합 CoT (Combined) - dVLA 채택"
        BOTH["텍스트 + 비전 동시 생성"] --&gt; SYNERGY["시너지 효과 발생"]
        SYNERGY --&gt; ADV["논리적 계획 + 정밀 제어"]
        ADV --&gt; PERF_C["성능: 최상 (SOTA)"]
    end

    PERF_A -.-&gt; PERF_C
    PERF_B -.-&gt; PERF_C
</code></pre>
<h2>6.  논의 및 시사점</h2>
<h3>6.1  ’통합’이 가져온 혁신</h3>
<p>dVLA의 가장 큰 기여는 서로 다른 모달리티를 개별적으로 처리하던 관행을 깨고, 이를 하나의 수학적 프레임워크(이산 확산)로 통합했다는 점이다. 이는 데이터 간의 경계를 허물고, 로봇이 텍스트의 논리와 이미지의 공간 정보를 동시에 활용하여 행동을 생성할 수 있게 만들었다. 특히 그래디언트 충돌 문제를 해결함으로써 학습의 안정성을 크게 높였다.6</p>
<h3>6.2  설명 가능한 로봇 지능</h3>
<p>멀티모달 CoT는 로봇의 내부 의사결정 과정을 인간이 이해할 수 있는 형태(이미지, 텍스트)로 보여준다. 이는 로봇이 왜 실패했는지, 어떤 의도로 행동했는지를 파악할 수 있게 해주어 디버깅과 신뢰성 확보에 큰 도움이 된다. dVLA는 로봇에게 ’자기 성찰’의 가능성을 열어주었다.</p>
<h3>6.3  남겨진 과제와 미래</h3>
<p>비록 dVLA가 혁신적인 성과를 거두었지만, 여전히 개선의 여지는 있다. 예를 들어, 행동 디코딩(Action Decoding) 과정에서 발생하는 미세한 오류가 실패의 주 원인으로 지목되기도 했다.6 또한 3Hz의 추론 속도는 많이 개선된 것이지만, 고속 동작이 필요한 작업에서는 여전히 부족할 수 있다. 향후 연구는 더 효율적인 토크나이저 개발과 경량화된 모델 구조 탐색, 그리고 더 긴 시계열을 다루는 장기 기억 능력 강화에 집중될 것으로 보인다.</p>
<h2>7.  결론</h2>
<p>dVLA는 확산 기반의 생성 모델과 로봇 제어 이론을 성공적으로 결합한 사례로서, VLA 연구의 새로운 이정표를 세웠다. 시각, 언어, 행동을 아우르는 <strong>통합된 이산 확산 프레임워크</strong>와 로봇의 추론 능력을 극대화한 <strong>멀티모달 생각의 사슬</strong>은 로봇이 복잡하고 비정형화된 현실 세계에서 어떻게 지능적으로 행동해야 하는지에 대한 강력한 해답을 제시한다.</p>
<p>LIBERO 벤치마크에서의 압도적인 성능과 실제 환경에서의 강건함은 dVLA가 단순한 실험적 모델을 넘어 실용적인 로봇 파운데이션 모델(Robot Foundation Model)로 발전할 수 있는 잠재력을 입증한다. dVLA가 열어젖힌 ’생각하고 상상하며 행동하는 로봇’의 시대는 향후 제조업, 서비스업, 가정용 로봇 등 다양한 분야에서 혁신적인 변화를 가져올 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression | Semantic Scholar, https://www.semanticscholar.org/paper/Diffusion-VLA%3A-Scaling-Robot-Foundation-Models-via-Wen-Zhu/4e9ed96d566dc0d2f8c1f6f8cad9d0c5eedcb6a5</li>
<li>[2509.25681] dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought - arXiv, https://arxiv.org/abs/2509.25681</li>
<li>dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought - arXiv, https://arxiv.org/html/2509.25681v1</li>
<li>dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought, https://www.researchgate.net/publication/396048175_dVLA_Diffusion_Vision-Language-Action_Model_with_Multimodal_Chain-of-Thought</li>
<li>[Literature Review] dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought - Moonlight, https://www.themoonlight.io/en/review/dvla-diffusion-vision-language-action-model-with-multimodal-chain-of-thought</li>
<li>dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought | OpenReview, https://openreview.net/forum?id=2rxgospB5s</li>
<li>dVLA: Diffusion Vision-LANGUA - Pangram Labs, https://www.pangram.com/history/95e7a15b-2d35-41c4-8e2b-201e1c50deae</li>
<li>dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought - arXiv, <a href="https://arxiv.org/pdf/2509.25681">https://arxiv.org/pdf/2509.25681?</a></li>
<li>dVLA: Diffusion Vision-Language-Action Model with Multimodal, https://www.alphaxiv.org/overview/2509.25681</li>
<li>A Survey on Efficient Vision-Language-Action Models - arXiv, https://arxiv.org/html/2510.24795</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>