<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:FVV 자유시점 비디오를 위한 깊이 인식 장면 표현 방식</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>FVV 자유시점 비디오를 위한 깊이 인식 장면 표현 방식</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">장면인식 인공지능 (Scene Recognition AIs)</a> / <span>FVV 자유시점 비디오를 위한 깊이 인식 장면 표현 방식</span></nav>
                </div>
            </header>
            <article>
                <h1>FVV 자유시점 비디오를 위한 깊이 인식 장면 표현 방식</h1>
<p>MPI, NeRF, 3D 가우시안 스플래팅 그리고 “DiAD” 개념을 중심으로</p>
<h2>1.  자유시점 비디오(FVV)와 몰입형 현실감 구현을 위한 탐구</h2>
<h3>1.1  FVV 패러다임의 정의</h3>
<p>자유시점 비디오(Free-Viewpoint Video, FVV)는 시청자가 수동적으로 콘텐츠를 소비하는 전통적인 2D 비디오나 입체 3D 비디오의 한계를 넘어, 장면을 바라보는 시점과 각도를 능동적으로 자유롭게 선택할 수 있게 하는 혁신적인 미디어 경험을 의미한다.1 FVV는 단순히 기술적 발전을 넘어 스포츠 중계, 엔터테인먼트, 교육, 광고 등 다양한 분야에서 상호작용적 스토리텔링과 데이터 탐색을 위한 새로운 매체로 자리매김하고 있다.1 시청자는 더 이상 콘텐츠 제작자가 정해준 시점에 얽매이지 않고, 마치 실제 공간에 있는 것처럼 원하는 각도에서 역동적인 장면을 탐색할 수 있는 완전한 자유를 얻게 된다.1 이러한 상호작용성은 사용자 참여도를 극대화하고, 전례 없는 수준의 몰입감을 제공하는 핵심 요소이다.3</p>
<h3>1.2  엔드-투-엔드 FVV 시스템 아키텍처</h3>
<p>FVV 시스템은 일반적으로 여러 단계의 파이프라인으로 구성되며, 각 단계는 고유한 기술적 과제를 안고 있다. 이 아키텍처를 이해하는 것은 현대적 장면 표현 기술의 필요성을 파악하는 데 필수적이다.</p>
<ul>
<li><strong>획득(Acquisition):</strong> FVV의 첫 단계는 다수의 카메라를 이용해 장면을 동시에 포착하는 것이다. 카메라 배열은 주로 대상을 중심으로 안쪽을 향하는(inward-looking) 방식과 바깥 세상을 향하는(outward-looking) 방식으로 나뉜다.4 특히 스포츠 이벤트와 같이 역동적인 피사체를 촬영할 때는 모든 카메라가 밀리초(millisecond) 수준에서 정밀하게 동기화되어야 한다.1 과거에는 이러한 동기화와 각 카메라의 내외부 파라미터를 정확히 계산하는 캘리브레이션 과정이 매우 복잡하고 비용이 많이 들어 FVV 기술의 대중화를 가로막는 주요 장벽으로 작용했다.2</li>
<li><strong>처리(Processing):</strong> 이 단계에서는 획득된 여러 개의 2D 비디오 스트림을 일관성 있는 3D 장면 표현으로 변환한다.3 이는 FVV 파이프라인의 핵심 기술이며, 본 안내서의 주요 논의 대상이 되는 장면 표현 방식이 바로 이 단계에서 결정된다.</li>
<li><strong>전송 및 압축(Transmission &amp; Compression):</strong> 3D 장면 데이터는 기존 2D 비디오에 비해 용량이 훨씬 크기 때문에 효율적인 전송 및 저장을 위한 압축 기술이 필수적이다. 이를 위해 MPEG과 같은 표준화 기구에서는 다시점 비디오 코딩(Multi-view Video Coding, MVC) 등의 표준을 개발하여 데이터 전송률을 낮추고 있다.3</li>
<li><strong>렌더링 및 디스플레이(Rendering &amp; Display):</strong> 마지막으로, 압축 해제된 3D 장면 데이터는 사용자의 시점 요청에 따라 새로운 뷰의 이미지로 합성(렌더링)된다. 이렇게 생성된 영상은 모바일 기기, 웹 플레이어, 가상현실(VR) 헤드셋 등 다양한 디스플레이 장치를 통해 사용자에게 제공된다.1</li>
</ul>
<h3>1.3  핵심 문제: 장면 표현 방식</h3>
<p>FVV 시스템의 품질, 성능, 그리고 실현 가능성을 결정하는 가장 중요한 요소는 바로 3D 장면을 컴퓨터가 이해할 수 있는 데이터로 어떻게 표현하는가에 달려있다. 전통적으로는 3D 모델의 표면을 삼각형의 집합으로 표현하는 ’메시(mesh)’와 같은 명시적(explicit) 표현 방식이 주를 이루었다. 반면, 최근에는 장면을 신경망의 가중치와 같은 암시적(implicit) 함수로 표현하는 방식이 등장하며 FVV 기술의 새로운 지평을 열었다.</p>
<p>이러한 기술적 진화의 배경에는 FVV 콘텐츠 제작의 ’민주화’라는 거대한 흐름이 존재한다. 초기 FVV 시스템은 ’매트릭스’의 ‘불릿 타임’ 효과처럼 막대한 자본과 전문 인력이 투입되는 전문 스튜디오 환경에서나 가능했다.1 복잡한 하드웨어 설치와 까다로운 캘리브레이션 과정은 일반 사용자의 접근을 원천적으로 차단하는 높은 진입 장벽이었다.4 그러나 NeRF, 3DGS와 같은 최신 기술들은 이러한 부담을 하드웨어에서 소프트웨어, 즉 컴퓨팅 파워로 이전시켰다. 이제는 스마트폰으로 촬영한 영상만으로도 고품질의 3D 콘텐츠를 생성할 수 있게 되면서 5, FVV 기술은 전문가의 영역을 넘어 이커머스, 소셜 미디어, 개인 창작 등 다양한 분야로 확산될 잠재력을 갖게 되었다. 결국 FVV 기술의 발전사는 단순히 화질을 개선하는 것을 넘어, 3D 콘텐츠 제작의 장벽을 허물고 누구나 몰입형 미디어를 만들고 즐길 수 있도록 하는 과정 그 자체라 할 수 있다.</p>
<h2>2.  기초적인 장면 표현 방식: 명시적 기하학에서 계층적 깊이까지</h2>
<h3>2.1  초기 패러다임: 메시와 포인트 기반 렌더링</h3>
<p>현대적인 신경망 기반 표현 방식을 논하기에 앞서, 그 토대가 된 고전적인 접근법을 이해하는 것은 필수적이다.</p>
<ul>
<li><strong>동적 텍스처 메시(Dynamic Textured Meshes):</strong> 컴퓨터 그래픽스 분야에서 가장 전통적이고 기본적인 표현 방식은 장면의 표면을 작은 삼각형들의 집합, 즉 3D 메시로 모델링하는 것이다. 동적 장면을 표현하기 위해서는 각 비디오 프레임마다 메시를 생성하고, 시간의 흐름에 따라 이 메시들이 일관성을 유지하도록 추적(tracking)해야 한다.6 이 방식은 렌더링이 빠르고 직관적이라는 장점이 있지만, 사람이 손뼉을 치거나 옷자락이 펄럭이는 것처럼 객체의 위상(topology)이 변하는 상황을 자연스럽게 처리하기 매우 어렵다는 근본적인 한계를 가진다.6</li>
<li><strong>동적 포인트 샘플(Dynamic Point Samples):</strong> 메시의 위상 변화 문제를 해결하기 위한 대안으로 장면을 3D 포인트의 집합으로 표현하는 방식이 제안되었다.10 각 포인트는 위치, 색상, 법선 벡터(표면의 방향) 등의 속성을 가지며, 장면은 이 포인트들의 밀집된 구름으로 표현된다. 이 방식은 포인트의 추가 및 삭제가 자유롭기 때문에 위상 변화를 암시적으로 손쉽게 처리할 수 있다. 이는 훗날 3D 가우시안 스플래팅(3DGS)의 기본 아이디어로 이어지는 중요한 개념적 선구자 역할을 했다.</li>
</ul>
<h3>2.2  핵심적인 디딤돌: 다중 평면 이미지(MPI)</h3>
<p>다중 평면 이미지(Multiplane Image, MPI)는 고전적인 기하학 표현과 현대적인 신경망 렌더링 사이의 중요한 개념적 다리 역할을 하는 기술이다. MPI는 장면을 명시적인 깊이 정보를 가진 여러 개의 계층으로 분해하여 표현한다.</p>
<ul>
<li><strong>MPI 구조와 이론:</strong> MPI는 특정 카메라의 절두체(view frustum) 내에 서로 다른 깊이 값에 위치한 여러 개의 평행한 평면들로 장면을 표현한다.11 각 평면은 색상 정보를 담은 RGB 채널과 투명도 정보를 담은 알파(</li>
</ul>
<p>α) 채널을 가진 이미지(RGBα 이미지)이다. 이 알파 채널은 객체의 부드러운 가장자리나 반투명한 효과를 표현하는 데 결정적인 역할을 한다.</p>
<ul>
<li>
<p><strong>MPI로부터의 렌더링:</strong> 새로운 시점의 이미지를 생성하기 위해, MPI의 각 평면 이미지는 목표 시점에 맞게 호모그래피 변환(homographic warping)을 통해 투영된다. 그 후, 가장 멀리 있는 평면부터 가장 가까운 평면 순서로 알파 블렌딩(alpha compositing)을 통해 차례대로 합성하여 최종 이미지를 만들어낸다.11</p>
</li>
<li>
<p><strong>학습 기반 MPI 예측:</strong> MPI의 진정한 잠재력은 딥러닝과의 결합에서 발현되었다. 컨볼루션 신경망(CNN)과 같은 딥러닝 모델을 사용하여 한두 장의 입력 이미지로부터 MPI를 직접 예측하도록 학습시킬 수 있다.11 이때 학습은 예측된 MPI로 렌더링한 이미지와 실제 정답 이미지 간의 차이를 최소화하는 방향으로 진행된다.</p>
</li>
<li>
<p><strong>강점과 한계:</strong> MPI는 입력 이미지에서는 가려져 보이지 않던 부분(disocclusion)을 자연스럽게 채우거나, 금속 표면의 반사와 같은 비-람베르트(non-Lambertian) 효과를 어느 정도 표현할 수 있다는 강력한 장점을 가진다.11 하지만 평면들이 카메라 시점에 고정되어 있고 깊이가 이산적으로 샘플링되기 때문에, 시점을 크게 벗어나면 계단 현상과 같은 시각적 오류(discretization artifact)가 발생할 수 있다. 이러한 한계를 극복하기 위해 장면에 맞춰 평면의 위치와 방향을 적응적으로 조절하는 구조적 MPI(Structural MPI, S-MPI)와 같은 연구도 제안되었다.15</p>
</li>
</ul>
<p>MPI를 이해하는 것은 단순히 과거 기술을 복습하는 것을 넘어, NeRF의 구조적 배경을 파악하는 데 필수적이다. MPI는 이미지로부터 학습 가능하고, 미분 가능한(differentiable) 체적(volumetric) 표현이라는 개념을 도입했다. 이는 NeRF의 핵심 철학과 정확히 일치한다. 고전적인 메시 표현은 기하학이 완전히 명시적이지만, 이를 실제 영상으로부터 생성하고 애니메이션하는 것은 매우 어려운 문제였다. MPI는 복잡한 표면을 직접 모델링하는 대신, 장면의 부피를 단순화된 여러 계층으로 근사하는 학습 기반의 접근법을 취했다. 결정적으로, MPI의 렌더링 과정(평면 변환 및 알파 블렌딩)은 미분이 가능하여 11, 신경망이 렌더링된 결과와 정답 이미지를 비교하는 것만으로 종단간(end-to-end) 학습을 할 수 있게 만들었다. NeRF는 바로 이 MPI의 계층적이고 이산적인 구조를 연속적이고 암시적인 함수(MLP)로 일반화한 것이다. 즉, MPI는 NeRF가 등장하기 위한 핵심 구성 요소인 ’체적 표현’과 ’미분 가능한 렌더러’를 분리하여 그 가능성을 처음으로 입증한 기술이라고 할 수 있다.</p>
<h2>3.  신경망 암시적 표현의 혁명: 뉴럴 래디언스 필드(NeRF)</h2>
<h3>3.1  NeRF 패러다임: 연속적인 체적 함수</h3>
<p>뉴럴 래디언스 필드(Neural Radiance Fields, NeRF)는 정적인 3D 장면을 하나의 연속적인 5차원 함수, 즉 <span class="math math-inline">F(x, y, z, \theta, \phi) \rightarrow (c, \sigma)</span>로 표현하는 혁신적인 아이디어를 제시했다.16 이 함수는 간단한 다층 퍼셉트론(Multi-Layer Perceptron, MLP) 신경망으로 구현된다. 함수의 입력은 3차원 공간 좌표(<span class="math math-inline">x, y, z</span>)와 2차원 시선 방향(<span class="math math-inline">\theta, \phi</span>)이며, 출력은 해당 지점에서 해당 방향으로 방출되는 빛의 색상(RGB 값, c)과 그 지점의 밀도(σ)이다. 밀도는 빛이 그 지점을 통과할 때 얼마나 흡수되거나 산란되는지를 나타내는 값으로, 불투명도와 유사한 개념이다.</p>
<h3>3.2  NeRF 파이프라인: 광선에서 픽셀까지</h3>
<p>NeRF가 새로운 시점의 이미지를 생성하는 과정은 고전적인 컴퓨터 그래픽스 기술과 딥러닝의 절묘한 결합으로 이루어진다.</p>
<ul>
<li><strong>광선 행진 및 샘플링(Ray Marching and Sampling):</strong> 렌더링하고자 하는 가상 카메라의 각 픽셀에 대해, 카메라 중심에서 픽셀을 향해 나아가는 하나의 광선(ray)을 설정한다. 그리고 이 광선을 따라 여러 개의 3D 지점들을 샘플링한다.18</li>
<li><strong>체적 렌더링 방정식(Volumetric Rendering Equation):</strong> 샘플링된 각 지점에 대해, 5D 좌표(위치+방향)를 MLP에 입력하여 색상(c)과 밀도(σ) 값을 얻는다. 그 후, 고전적인 체적 렌더링 방정식을 사용하여 광선을 따라 얻어진 모든 색상과 밀도 값들을 적분(accumulation)하여 최종 픽셀 색상을 계산한다.16 이 전 과정이 미분 가능하다는 점이 NeRF 학습의 핵심이다.</li>
<li><strong>위치 인코딩(Positional Encoding):</strong> 단순한 MLP는 저주파 신호, 즉 부드러운 변화를 학습하는 데 편향되어 있어, 이미지의 세밀한 디테일을 표현하기 어렵다. 이 문제를 해결하기 위해 NeRF는 위치 인코딩이라는 기법을 사용한다. 이는 저차원의 입력 좌표(x,y,z,θ,ϕ)를 푸리에 변환과 유사한 방식으로 고차원의 특징 벡터로 매핑하는 과정이다. 이를 통해 MLP가 사진처럼 사실적인 고주파 디테일을 학습할 수 있게 된다.17</li>
</ul>
<h3>3.3  능력, 응용 분야 및 내재적 한계</h3>
<ul>
<li><strong>강점:</strong> NeRF는 기존 기술들을 압도하는 수준의 사진 현실적인(photorealistic) 결과물을 생성한다. 특히 유리나 금속의 반사, 투명도, 굴절과 같이 시선 방향에 따라 모습이 크게 변하는 복잡한 광학 현상을 매우 정교하게 표현할 수 있다.17 또한, 장면 전체가 MLP의 가중치라는 작은 크기의 데이터로 압축되어 저장 효율성이 높다.23</li>
<li><strong>응용 분야:</strong> NeRF의 등장은 가상/증강현실(VR/AR), 로보틱스, 3D 콘텐츠 생성, 문화유산 디지털 복원 등 수많은 분야에 지대한 영향을 미쳤다.9</li>
<li><strong>약점:</strong> NeRF의 가장 큰 단점은 막대한 계산 비용이다. 하나의 장면을 학습하는 데 수 시간에서 수 일이 걸리며, 렌더링 속도 또한 매우 느리다. 이는 픽셀 하나를 렌더링하기 위해 MLP를 수백 번씩 호출해야 하기 때문으로, 실시간 상호작용이 필수적인 응용 분야에는 부적합하다.22 또한 거울과 같은 복잡한 반사체의 경우, 가상 이미지를 실제 기하학적 구조로 오인하여 부정확한 깊이를 추정하고 흐릿한 결과물을 생성하기도 한다.21</li>
</ul>
<p>NeRF의 성공은 단순히 MLP라는 신경망 덕분이 아니라, 고전적인 렌더링 기법인 ’체적 렌더링’과 현대 딥러닝을 영리하게 결합한 시너지 효과에 기인한다. NeRF는 신경망이 3D 데이터를 직접 보지 않고도, 오직 2D 이미지와 카메라 포즈 정보만을 이용해 3D 공간을 학습할 수 있다는 것을 증명했다. 이 과정은 일종의 ‘분석에 의한 합성(analysis-by-synthesis)’ 패러다임으로 볼 수 있다. 즉, 새로운 2D 뷰를 생성하는 문제의 해답을 찾기 위해, 먼저 모델(MLP)을 통해 뷰를 합성해보고, 그 결과를 실제 정답 뷰와 비교하여 모델을 개선해나가는 방식이다. NeRF의 저자들은 고전적인 체적 렌더링 방정식이 본질적으로 미분 가능하다는 사실에 주목했다.17 따라서 이 미분 가능한 렌더링 파이프라인의 중심에 MLP를 배치할 수 있었다. MLP는 공간상의 임의의 지점에 대한 색상과 밀도를 예측하는 역할을 담당하고 18, 학습 과정에서 렌더링된 픽셀과 실제 픽셀 간의 오차는 미분 가능한 렌더링 방정식을 거슬러 올라가 MLP의 가중치를 업데이트하는 데 사용된다. 결국 NeRF의 혁신은 새로운 신경망 구조를 발명한 것이 아니라, 뷰 합성 문제를 고전적이고 미분 가능한 렌더링 파이프라인을 통해 종단간으로 최적화할 수 있는 문제로 재정의한 데 있다. 이 통찰은 이후 모든 신경망 렌더링 기술의 발전에 지대한 영향을 미쳤다.</p>
<h2>4.  명시적 래스터화의 혁명: 3D 가우시안 스플래팅(3DGS)</h2>
<h3>4.1  3DGS 패러다임: 명시적 프리미티브로의 회귀</h3>
<p>3D 가우시안 스플래팅(3D Gaussian Splatting, 3DGS)은 NeRF의 극심한 성능 한계에 대한 직접적인 해답으로 등장했다. 3DGS는 장면을 신경망으로 표현하는 대신, 수백만 개의 3D 가우시안(Gaussian)이라는 명시적인 기본 단위(primitive)의 집합으로 표현한다.29 각 가우시안은 3D 위치(중심점), 3D 공분산(모양과 회전), 색상(시선 의존 효과를 위해 구면 조화 함수(Spherical Harmonics)로 표현), 그리고 불투명도(α)라는 파라미터들로 정의된다.30</p>
<h3>4.2  3DGS 파이프라인: SfM에서 실시간 렌더링까지</h3>
<ul>
<li><strong>초기화(Initialization):</strong> 3DGS 파이프라인은 입력 이미지들로부터 SfM(Structure from Motion) 기술(주로 COLMAP 라이브러리 사용)을 통해 희소한 3D 포인트 클라우드를 생성하는 것으로 시작한다.29 이 SfM 포인트 클라우드의 각 점이 하나의 초기 3D 가우시안이 된다.</li>
<li><strong>미분 가능한 래스터화(Differentiable Rasterization):</strong> 3DGS의 핵심 기술은 3D 가우시안들을 2D 이미지 평면에 효율적으로 투영하고 합성하는 고속 타일 기반 래스터라이저이다.29 이 래스터라이저는 3D 가우시안을 2D 타원으로 투영한 후, 앞에서부터 순서대로 블렌딩하여 최종 이미지를 생성한다. 이 과정이 미분 가능하도록 설계되어, NeRF와 마찬가지로 경사 하강법을 통한 최적화가 가능하다.</li>
<li><strong>최적화 및 적응적 밀도 제어(Optimization and Adaptive Densification):</strong> 가우시안들의 파라미터들은 확률적 경사 하강법(Stochastic Gradient Descent)을 통해 최적화된다. 이 과정에서 가장 중요한 부분은 ’적응적 밀도 제어’이다. 주기적으로, 너무 큰 가우시안은 더 작은 가우시안들로 분할(split)하고, 디테일이 부족한 영역의 작은 가우시안은 복제(clone)하며, 거의 투명해져 불필요해진 가우시안은 제거(prune)한다. 이 과정을 통해 모델은 넓고 균일한 영역과 세밀한 디테일을 모두 효율적으로 표현할 수 있게 된다.30</li>
</ul>
<h3>4.3  능력, 응용 분야 및 내재적 한계</h3>
<ul>
<li><strong>강점:</strong> 3DGS의 가장 큰 장점은 압도적인 성능이다. NeRF와 동등하거나 그 이상의 시각적 품질을 유지하면서도, 1080p 해상도에서 초당 30프레임 이상의 실시간 렌더링 속도를 달성하고 학습 시간 또한 수 분에서 수 시간 단위로 극적으로 단축시켰다.21</li>
<li><strong>약점:</strong> 명시적 표현 방식이기 때문에 수백만 개의 가우시안 파라미터를 모두 저장해야 하므로 메모리 사용량이 클 수 있다.22 또한, 빈 공간에 작은 가우시안이 떠다니는 것처럼 보이는 ‘플로터(floater)’ 현상이나, 가까이 확대했을 때 표면이 아닌 점들의 집합처럼 보이는 등 시각적 오류가 발생할 수 있다.35 NeRF와 같은 연속 함수가 아니기 때문에 깨끗한 메시 표면을 추출하거나 장면의 조명을 바꾸는 등의 작업이 상대적으로 더 어렵지만, 이를 해결하기 위한 연구가 활발히 진행 중이다.37 장면 전체가 하나의 거대한 포인트 클라우드처럼 표현되기 때문에 개별 객체와의 상호작용도 어렵다.22</li>
</ul>
<p>3DGS는 NeRF에 대한 반발이 아니라, 그로부터 직접적으로 진화한 기술로 보아야 한다. 3DGS는 NeRF의 핵심 최적화 원리인 ’분석에 의한 합성’을 그대로 유지하되, NeRF의 가장 큰 병목이었던 느린 MLP를 GPU의 고도로 최적화된 래스터화 파이프라인에 친화적인 빠르고 명시적인 프리미티브로 대체했다. NeRF의 병목은 모든 광선의 모든 샘플링 지점에 대해 MLP를 반복적으로 호출하는 데 있었다.39 이를 해결하기 위해 3DGS 개발자들은 래스터화라는 고전적인 그래픽스 기술에 주목했다. 전통적인 래스터화는 삼각형을 사용하지만, 이미지로부터 고품질의 삼각형 메시를 만드는 것 자체가 어려운 문제였다. 대신 그들은 ’가우시안’이라는 새로운 프리미티브를 제안했다. 가우시안은 딱딱한 표면의 삼각형과 달리 부드러운 체적을 가진 ’얼룩(splat)’과 같아서, 불확실성을 내포한 이미지 기반 복원에 더 자연스럽게 부합했다.29 그리고 가장 결정적인 혁신은 바로 이 가우시안을 렌더링하는 과정을 미분 가능하게 만든 것이었다.31 이를 통해 NeRF와 동일한 최적화 루프를 사용하면서도, MLP 가중치 대신 명시적인 가우시안들의 파라미터(위치, 색상, 모양 등)를 직접 업데이트할 수 있게 되었다. 이는 성능 문제를 단번에 해결한 발상의 전환이었다.</p>
<h2>5.  움직임의 도전: 동적 장면 렌더링</h2>
<h3>5.1  FVV의 4번째 차원</h3>
<p>진정한 의미의 FVV를 위해서는 시간에 따라 변화하는 동적 장면을 모델링하는 것이 필수적이다. 초기 연구들은 비강체 메시 추적(non-rigid mesh tracking)과 같은 기법을 사용하여 움직임을 표현하려 시도했다.6 그러나 신경망 기반 표현 방식의 등장은 동적 장면 렌더링에 새로운 가능성을 열었다.</p>
<h3>5.2  NeRF의 동적 장면으로의 확장 (Dynamic NeRF)</h3>
<p>NeRF를 동적 장면에 적용하려는 시도들은 주로 MLP에 시간(t)을 추가적인 입력으로 넣거나, 각 시간대별로 장면이 어떻게 변형되는지를 학습하는 변형 필드(deformation field)를 도입하는 방식으로 이루어졌다. 하지만 단일 MLP가 장면의 기하학, 외형, 그리고 시간의 흐름에 따른 움직임까지 모두 학습해야 하는 복잡하고 얽힌 문제 때문에 한계가 있었다.</p>
<h3>5.3  최전선: 동적 3D 가우시안 스플래팅</h3>
<p>반면, 3DGS는 동적 장면 모델링 분야에서 매우 빠른 발전을 보이고 있다. 이는 3DGS의 명시적인 표현 방식이 움직임을 모델링하는 데 본질적으로 더 직관적이고 적합하기 때문이다.</p>
<ul>
<li><strong>영속적 가우시안 추적(Tracking Persistent Gaussians):</strong> “Dynamic 3D Gaussians“와 같은 연구들은 가우시안들이 시간이 지남에 따라 움직이고 회전하도록 하되, 그 고유의 속성(색상, 크기, 불투명도)은 변하지 않도록(persistent) 제약을 가하는 방식을 제안했다. 여기에 국소적 강체 제약(local-rigidity constraint)을 추가하여 주변 가우시안들과의 상대적 위치 관계를 유지하도록 함으로써, 별도의 추적 정보 없이도 최적화 과정에서 자연스럽게 빽빽한 6자유도(6-DOF) 추적이 가능해졌다.40</li>
<li><strong>4D 및 하이브리드 표현:</strong> 가우시안을 4차원(x,y,z,t)으로 확장하여 시간 축을 직접 모델링하는 접근법도 등장했다. 더 나아가, 하이브리드 3D-4D 가우시안 스플래팅(Hybrid 3D-4DGS)과 같은 진보된 방법은 장면을 분석하여 움직이지 않는 정적 영역은 효율적인 3D 가우시안으로, 움직임이 있는 동적 영역만 복잡한 4D 가우시안으로 표현하는 적응적 전략을 사용한다. 이를 통해 불필요한 파라미터를 크게 줄여 메모리 사용량과 학습 속도를 획기적으로 개선했다.41</li>
<li><strong>무한한 환경에서의 동적 장면:</strong> 자율 주행 데이터셋인 Waymo나 KITTI와 같이 광활하고 경계가 없는 야외 환경에서의 동적 장면 렌더링은 또 다른 차원의 어려움을 수반한다. Gaussian-UDSR과 같은 솔루션은 이러한 대규모 동적 환경을 효율적으로 재구성하고 실시간으로 렌더링하기 위해 특별히 설계되었다.42</li>
<li><strong>SLAM과의 통합:</strong> 3DGS는 SLAM(Simultaneous Localization and Mapping) 시스템과의 융합에서도 큰 잠재력을 보여준다. 3DGS-SLAM 시스템은 비전, LiDAR 등 다양한 센서 데이터를 실시간으로 처리하여 카메라의 위치를 추적함과 동시에 주변 환경을 3D 가우시안 맵으로 재구성한다. 이 과정에서 움직이는 객체를 효과적으로 분리하고 추적하여 동적 환경에서도 강건한 성능을 발휘한다.43</li>
</ul>
<p>3DGS의 명시적인 특성은 NeRF의 암시적인 단일체(monolithic) MLP에 비해 동역학을 모델링하는 데 근본적으로 더 유리하다. NeRF의 움직임을 모델링하려면 단일 MLP가 장면의 모든 것을 한 번에 학습해야 하는 어려운 문제에 직면한다. 반면, 3DGS에서는 장면이 이미 수백만 개의 개별 프리미티브로 분해되어 있으므로, 동역학 모델링은 “이 개별 프리미티브들이 어떻게 움직이는가?“라는 훨씬 직관적인 문제로 환원될 수 있다.40 각 가우시안에 움직임 벡터를 할당하거나 변형을 학습시키는 것이 훨씬 직접적이다. 이러한 명시성은 하이브리드 3D-4DGS 접근법처럼 정적인 영역의 가우시안에서는 시간 차원을 간단히 ‘끄는’ 식의 효율적인 모델링을 가능하게 한다.41 이는 단일 연속 함수인 NeRF에서는 적용하기 어려운 개념이다. 또한, 명시적인 포인트들은 SLAM 시스템의 LiDAR 포인트 클라우드와 같은 다른 명시적 데이터 소스와 더 쉽게 통합될 수 있다.43 결론적으로, 3DGS가 장면을 명시적 프리미티브로 분해한 것은 동역학 모델링 문제를 근본적으로 단순화하여 NeRF의 전체론적 접근 방식보다 더 직접적이고 제어 가능하며 효율적인 솔루션을 가능하게 했다.</p>
<h2>6.  비교 분석: 다각적 평가</h2>
<h3>6.1  기술별 정면 비교</h3>
<p>FVV를 위한 핵심 장면 표현 기술들인 MPI, NeRF, 3DGS는 각각 뚜렷한 장단점을 가지며, 특정 응용 분야에 따라 그 유용성이 달라진다. 이들의 기술적 특성을 다각도로 비교하면 다음과 같다.</p>
<table><thead><tr><th><strong>기준</strong></th><th><strong>다중 평면 이미지 (MPI)</strong></th><th><strong>뉴럴 래디언스 필드 (NeRF)</strong></th><th><strong>3D 가우시안 스플래팅 (3DGS)</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 표현 방식</strong></td><td>준-명시적 (계층적 RGBα 평면)</td><td>완전 암시적 (MLP를 통한 연속 5D 함수)</td><td>완전 명시적 (이산적 3D 가우시안 프리미티브)</td></tr>
<tr><td><strong>렌더링 메커니즘</strong></td><td>미분 가능한 변환 및 합성</td><td>미분 가능한 체적 광선 행진</td><td>미분 가능한 타일 기반 래스터화</td></tr>
<tr><td><strong>렌더링 속도</strong></td><td>실시간</td><td>매우 느림 (오프라인)</td><td>실시간 (&gt;30fps)</td></tr>
<tr><td><strong>학습 시간</strong></td><td>빠름 ~ 보통</td><td>매우 느림 (수 시간/수 일)</td><td>빠름 (수 분/수 시간)</td></tr>
<tr><td><strong>시각적 품질</strong></td><td>양호하나, 계층/이산화 오류 발생 가능</td><td>최첨단 수준의 사진 현실감</td><td>최첨단 수준, 거의 사진과 흡사</td></tr>
<tr><td><strong>시선 의존 효과</strong></td><td>제한적인 비-람베르트 효과 표현</td><td>탁월함 (반사, 투명도)</td><td>양호함 (구면 조화 함수 사용), 거울 등 복잡한 효과에는 한계</td></tr>
<tr><td><strong>메모리 사용량</strong></td><td>보통 (계층 수/해상도에 의존)</td><td>작음 (MLP 가중치는 압축적)</td><td>큼 (수백만 개의 전체 공분산 행렬 저장)</td></tr>
<tr><td><strong>편집/상호작용성</strong></td><td>낮음 (계층이 뷰 절두체에 종속)</td><td>매우 낮음 (암시적 함수는 블랙박스)</td><td>보통 (프리미티브가 명시적이어서 조작 가능)</td></tr>
<tr><td><strong>동적 장면 처리</strong></td><td>어려움 (프레임별 MPI 필요)</td><td>어려움 (복잡한 확장 필요)</td><td>다수의 효과적인 접근법으로 빠르게 발전 중</td></tr>
<tr><td><strong>깊이 표현</strong></td><td>명시적, 이산적 (평면 깊이)</td><td>암시적, 연속적 (밀도 σ로부터 파생)</td><td>명시적으로 저장 안 됨, 렌더링 시 정렬 키로만 사용</td></tr>
<tr><td><strong>데이터 소스</strong></td><td>11</td><td>17</td><td>21</td></tr>
</tbody></table>
<p>이 표는 각 기술의 철학적 차이(암시적 vs. 명시적), 성능적 트레이드오프(속도 vs. 메모리), 그리고 기능적 차이(시선 의존성 vs. 편집 용이성)를 한눈에 파악할 수 있게 해준다. 이러한 구조화된 비교는 특정 응용에 어떤 기술을 사용할지 정보에 입각한 결정을 내리고, 가상의 “DiAD” 모델이 추구해야 할 방향을 이해하는 기초가 된다.</p>
<h3>6.2  시각적 오류 및 실패 모드 분석</h3>
<p>각 기술은 고유한 형태의 시각적 오류를 생성하는 경향이 있다.</p>
<ul>
<li><strong>NeRF:</strong> 입력 뷰가 부족한 영역에서는 기하학적 제약이 약해져 흐릿하거나 안개처럼 ‘떠다니는’ 오류(floating artifact)가 발생할 수 있다.21 또한, 데이터가 충분하지 않으면 날카로운 기하학적 모서리를 완벽하게 표현하지 못하는 경향이 있다.</li>
<li><strong>3DGS:</strong> 빈 공간에 잘못 위치한 작은 가우시안들이 먼지처럼 떠다니는 ‘플로터’ 오류가 대표적이다.36 또한, 최적화 중 가우시안이 추가될 때 갑자기 나타나는 ‘팝핑(popping)’ 현상이나, 지나치게 확대하면 개별 가우시안이 보여 ’포인트 클라우드’처럼 보일 수 있다.35 빽빽한 장면에서는 가려짐(occlusion) 문제가 발생하기도 한다.8</li>
</ul>
<p>두 방법 모두 초기 SfM 단계에서 추정된 카메라 포즈의 정확도에 매우 민감하다. 부정확한 캘리브레이션은 재구성 품질을 저하하는 주요 원인이다.8</p>
<h3>6.3  성능에 대한 상반된 보고와 미묘한 차이</h3>
<p>일부 사용자 경험 보고에서는 3DGS의 처리 시간이 NeRF보다 오히려 더 오래 걸렸다는 상반된 결과가 나타나기도 한다.44 이는 3DGS 알고리즘 자체의 근본적인 결함이라기보다는, 특정 구현체의 세부 사항, 하드웨어 환경(예: CUDA 가속을 통한 고속 정렬 기능의 부재) 31, 또는 전체 파이프라인에서 SfM 단계가 차지하는 시간 등 복합적인 요인에 기인할 가능성이 높다. 이는 실제 현장에서의 성능이 단순한 학술적 벤치마크보다 더 복잡한 변수에 의해 좌우됨을 시사한다.</p>
<p>이러한 비교 분석을 통해 우리는 하나의 ‘최고의’ 모델은 존재하지 않으며, <strong>렌더링 속도</strong>, <strong>모델 편집 용이성</strong>, 그리고 <strong>사진 현실적인 빛 전파(light transport) 표현</strong> 사이에 근본적인 ’삼중고(trilemma)’가 존재함을 알 수 있다. NeRF는 복잡한 반사와 같은 빛 전파 표현에 탁월하지만 17, 이로 인해 렌더링이 느리고 편집이 거의 불가능하다.22 3DGS는 렌더링 속도와 편집 용이성에서 뛰어나지만 29, 구면 조화 함수를 통한 시선 의존 효과 표현은 근사치이며 NeRF가 자연스럽게 처리하는 복잡한 빛 전파를 완벽히 재현하는 데는 한계가 있다.35 고전적인 메시는 편집이 매우 용이하고 렌더링도 빠르지만, 학습 기반 방법들이 달성한 사진 현실감을 이미지로부터 직접 얻어내기는 어렵다. 따라서 사용자와 연구자는 이 삼중고의 어떤 꼭짓점을 우선시할지 선택해야 한다. 실시간성, 완전한 편집 가능성, 완벽한 사진 현실감을 모두 갖춘 FVV의 ’성배’는 아직 존재하지 않으며, 바로 이 해결되지 않은 긴장감이 미래 연구와 “DiAD“와 같은 새로운 모델을 구상하게 하는 주된 동력이다.</p>
<h2>7.  종합 및 미래 전망: “DiAD” 개념에 대한 비판적 고찰</h2>
<h3>7.1  “DiAD“의 해체: “깊이 인식“의 중요성</h3>
<p>본 안내서의 분석을 종합하여 사용자의 질의인 “DiAD 모델에 대한 고찰“에 답하고자 한다. “DiAD“라는 용어에서 가장 중요한 단서는 “깊이 인식(Depth-aware)“일 가능성이 높다. 이는 깊이 정보가 각 기술 패러다임에서 어떻게 다루어지는지를 재검토할 필요성을 제기한다.</p>
<ul>
<li><strong>MPI:</strong> 깊이는 명시적이고 이산적인 평면들의 위치로 정의된다.</li>
<li><strong>NeRF:</strong> 깊이는 체적 밀도 필드(σ)로부터 암시적으로 파생되는 부차적인 속성이다.</li>
<li><strong>3DGS:</strong> 깊이는 렌더링 시 가우시안들을 정렬하기 위한 일시적인 키(key) 값일 뿐, 표현 자체에 내재된 속성이 아니다.</li>
</ul>
<p>이러한 관점에서 볼 때, “DiAD“는 깊이 정보가 렌더링의 부산물이 아니라, 장면 표현 내에서 제어 가능하고 핵심적인 역할을 하는 ’일급 시민(first-class citizen)’으로 취급되는 시스템을 지향할 것으로 추론할 수 있다.</p>
<h3>7.2  “DiAD” 아키텍처 가설</h3>
<p>기존 방법들의 강점을 결합하고 약점을 보완하는 차세대 “DiAD” 모델의 아키텍처로 다음과 같은 몇 가지 가설을 제시할 수 있다.</p>
<ul>
<li><strong>가설 A (하이브리드 암시적-명시적 모델):</strong> NeRF와 유사한 MLP가 색상과 밀도를 직접 예측하는 대신, 3D 가우시안의 파라미터(위치, 공분산, 색상 등)를 예측하는 모델이다. 이는 암시적 모델의 강력한 학습 능력과 명시적 래스터화의 실시간 렌더링 성능을 결합하는 방식이다.</li>
<li><strong>가설 B (명시적 깊이 유도 3DGS):</strong> 3DGS를 최적화하면서 동시에 해당 장면에 대한 깊이 맵(depth map)을 명시적으로 학습하고, 이 깊이 정보를 3DGS의 정규화(regularization)에 사용하는 모델이다. 이 깊이 맵은 가려짐(occlusion)을 더 정확하게 처리하고, 플로터 오류를 효과적으로 제거하며, 특히 입력 뷰가 희소한 환경에서 가우시안의 밀도 제어를 더 정교하게 유도하는 데 사용될 수 있다.39 이는 3DGS의 알려진 약점을 직접적으로 보완하는 전략이다.</li>
<li><strong>가설 C (모호성 해결을 위한 비용 필터링):</strong> 이 가설은 산업용 이상 탐지 분야에서 영감을 받은 독창적인 접근법이다.45 스테레오 매칭 분야의 비용 필터링(cost filtering) 개념을 차용하여, “DiAD” 모델이 먼저 노이즈가 많고 모호한 3D 비용 볼륨(cost volume)을 구성하도록 한다. 이 비용 볼륨은 각 깊이에서 기하학적 구조가 존재할 확률을 나타낸다. 그 후, 별도의 필터링 네트워크가 이 비용 볼륨을 정제하여 노이즈를 억제하고 더 깨끗하고 강건한 기하학적 골격을 생성한다. 이렇게 정제된 기하학 정보는 최종 렌더링 단계(NeRF 또는 3DGS)의 강력한 사전 정보(prior)로 사용되어, 희소하거나 노이즈가 많은 입력으로부터의 재구성 품질을 획기적으로 향상시킬 수 있다.</li>
</ul>
<h3>7.3  FVV의 미래 궤적</h3>
<p>FVV 기술의 미래는 암시적 표현과 명시적 표현의 지속적인 융합을 향해 나아갈 것이다. 즉, 학습된 사전 정보와 신경망 구성 요소를 활용하되, 최종적으로는 래스터화에 친화적인 명시적 프레임워크 내에서 작동하는 하이브리드 형태가 주를 이룰 것으로 예측된다. NeRF의 약점은 기하학과 외형이 MLP 가중치 안에 얽혀 있어 느리고 편집이 어렵다는 점이다. 3DGS의 약점은 기하학이 단순히 점들의 집합이라 실제 표면에 대한 이해가 부족하고 오류를 유발할 수 있다는 점이다. MPI는 명시적인 기하학 구조를 가졌지만 너무 경직되었다.</p>
<p>“DiAD” 모델은 논리적으로 기하학(깊이)과 외형(색상)을 분리(decouple)하려는 시도일 것이다. 먼저 장면의 기하학적 구조를 강건하게 추정한 다음, 그 명시적인 구조를 기반으로 고품질의 실시간 렌더링을 수행하는 것이다. 이는 장면의 깊이를 정확히 알면 NeRF의 광선 행진을 효율적으로 멈추거나, 3DGS의 가려진 가우시안을 훨씬 효과적으로 제거할 수 있기 때문이다. 궁극적인 목표는 우리의 4차원 시공간을 완벽하게 상호작용 가능하고, 사진처럼 사실적이며, 실시간으로 렌더링되고, 쉽게 편집할 수 있는 단일 표현 방식으로 담아내는 것이다. “DiAD“는 이러한 목표를 향한 중요한 진화의 한 단계가 될 것이다.</p>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>