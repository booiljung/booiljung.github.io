<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:신경망 오디오 코덱과 토큰화</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>신경망 오디오 코덱과 토큰화</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">오디오 (Audio) 트랜스포머</a> / <span>신경망 오디오 코덱과 토큰화</span></nav>
                </div>
            </header>
            <article>
                <h1>신경망 오디오 코덱과 토큰화</h1>
<p><strong>초록</strong></p>
<p>본 안내서는 전통적인 오디오 압축 방식에서 신경망 기반 오디오 코덱으로의 패러다임 전환을 심층적으로 분석한다. 특히, 신경망 코덱이 구현하는 ‘오디오 토큰화(Audio Tokenization)’ 개념이 제로샷 음성합성(TTS), 음악 생성 등 생성형 오디오 AI 기술의 폭발적인 발전을 이끈 핵심 기반임을 조명한다. SoundStream, EnCodec, VALL-E와 같은 주요 아키텍처의 작동 원리와 혁신을 상세히 기술하고, ’이산 표현 불일치(Discrete Representation Inconsistency, DRI)’와 같은 핵심 기술적 난제를 탐구한다. 나아가, 이 기술이 가져오는 비즈니스 기회와 함께 음성 복제, 딥페이크, 저작권 등 중대한 사회적, 법적, 윤리적 함의를 고찰하며, 차세대 오디오 AI의 미래 연구 방향을 제시함으로써 해당 기술 분야에 대한 포괄적이고 통찰력 있는 분석을 제공한다.</p>
<h2>1. 서론</h2>
<p>전통적인 오디오 코덱은 수십 년간 디지털 오디오의 저장과 전송을 지배해왔다. MP3, AAC와 같은 코덱은 인간의 청각 심리 모델에 기반하여 인지적으로 덜 중요한 정보를 제거함으로써 효율적인 압축을 달성했다.1 이러한 손실 압축 방식은 파일 크기를 획기적으로 줄여 음악 스트리밍과 디지털 통신의 대중화를 이끌었다.3 그러나 이들의 수작업으로 설계된 신호 처리 파이프라인은 생성형 인공지능(AI) 시대에 이르러 명백한 한계에 부딪혔다. AI 모델, 특히 대규모 언어 모델(LLM)이 요구하는 풍부하고 구조화된 데이터 표현을 제공하지 못하기 때문이다. 전통적 코덱의 출력은 AI가 직접 다루기 어려운 연속적인 파형(waveform)에 불과했다.</p>
<p>이러한 배경에서 종단간(end-to-end) 방식으로 훈련되는 신경망 오디오 코덱(Neural Audio Codec)이 등장하며 새로운 패러다임을 열었다.4 신경망 코덱은 데이터 기반 학습을 통해 스스로 오디오를 압축하고 복원하는 최적의 방법을 찾는다. 이 과정에서 기존 코덱을 압도하는 초저비트레이트(ultra-low bitrate) 압축 성능을 달성했을 뿐만 아니라 6, 더 중요한 혁신을 가져왔다. 바로 ’오디오 토큰화(Audio Tokenization)’이다.</p>
<p>오디오 토큰화는 연속적인 오디오 파형을 마치 문장이 단어의 나열로 표현되듯, 이산적인(discrete) 토큰의 시퀀스로 변환하는 과정이다.8 이는 오디오 처리 문제를 기존의 신호 회귀(signal regression) 문제에서 조건부 언어 모델링(conditional language modeling) 문제로 재정의하는 근본적인 전환을 의미한다.10 이로써 지난 10년간 비약적으로 발전해 온 자연어 처리(NLP) 분야의 강력한 기술, 특히 트랜스포머(Transformer) 아키텍처와 LLM을 오디오 생성 및 이해 작업에 직접 적용할 수 있는 길이 열렸다.12 오디오가 비로소 AI가 ’읽고 쓸 수 있는 언어’가 된 것이다.</p>
<p>본 안내서는 신경망 오디오 코덱과 토큰화 기술의 모든 측면을 심층적으로 고찰한다. 제1장에서는 신경망 코덱의 기본 작동 원리를 해부하고, 제2장에서는 SoundStream, EnCodec, VALL-E 등 산업을 선도하는 주요 아키텍처들을 심층 분석한다. 제3장에서는 이 기술이 열어젖힌 제로샷 TTS, 음악 생성, 실시간 통신 등 혁신적인 응용 분야를 탐색한다. 제4장에서는 이산 표현 불일치(DRI)와 같은 첨단 기술적 난제와 연구 동향을 다루고, 제5장에서는 GPU, TPU 등 하드웨어 최적화와 비즈니스 케이스를 분석한다. 제6장에서는 음성 복제와 딥페이크로 야기되는 윤리적, 법적 딜레마를 고찰하며, 마지막으로 제7장에서는 차세대 오디오 AI의 미래를 전망하며 안내서를 마무리한다.</p>
<hr />
<h2>2.  신경망 오디오 코덱의 작동 원리</h2>
<p>신경망 오디오 코덱의 핵심은 원본 오디오를 정보 손실을 최소화하면서 압축된 이산적 표현으로 변환하고, 다시 이 표현으로부터 원본에 가까운 오디오를 복원하는 것이다. 이 과정은 인코더, 양자화기, 디코더라는 세 가지 주요 구성 요소의 유기적인 상호작용을 통해 이루어진다.</p>
<h3>2.1  핵심 아키텍처: 인코더, 양자화기, 디코더</h3>
<p>신경망 코덱의 기본 구조는 오토인코더(Autoencoder)와 유사하다. 인코더가 데이터를 압축된 잠재 공간(latent space)으로 보내고, 디코더가 이를 다시 원본 데이터 형태로 복원하는 구조이다.13</p>
<p>**인코더(Encoder)**는 원시 오디오 파형을 입력받아 특징을 추출하고 압축하는 역할을 한다. 주로 1D 컨볼루션 신경망(CNN) 블록들을 여러 층으로 쌓아 구성되며, 입력 신호를 점진적으로 다운샘플링(downsampling)하여 시간적 해상도는 낮추고 채널의 깊이는 늘리는 방식으로 저차원의 잠재 표현(latent representation)을 생성한다.4 이 과정은 오디오의 핵심적인 음향적, 의미적 특징을 응축하는 과정으로 볼 수 있다. 예를 들어, SoundStream 모델의 인코더는 여러 개의 1D 컨볼루션 블록을 통과하며 입력 데이터를 압축한다.4</p>
<p>**양자화기(Quantizer)**는 신경망 코덱의 심장부로, 토큰화를 직접 수행하는 모듈이다. 인코더로부터 전달받은 연속적인(continuous) 잠재 표현을 ’코드북(codebook)’이라는 학습된 벡터 집합을 참조하여 이산적인(discrete) 인덱스, 즉 토큰으로 변환한다.4 이 단계에서 정보의 손실 압축이 발생하며, 연속적인 신호가 유한한 개수의 토큰으로 표현된다. 이 토큰 시퀀스가 바로 후속 생성 모델의 입력이 된다.</p>
<p>**디코더(Decoder)**는 인코더와 대칭적인 구조를 가지며, 양자화기를 통해 얻은 이산적인 토큰 시퀀스를 다시 연속적인 오디오 파형으로 복원하는 역할을 한다.4 디코더는 업샘플링(upsampling) 과정을 통해 잠재 표현의 시간적 해상도를 점차 높여 원본 오디오의 샘플링 레이트에 맞춰 파형을 생성한다.15 디코더의 성능은 최종적으로 생성되는 오디오의 품질을 결정하는 핵심 요소이다.</p>
<h3>2.2  압축의 핵심: 벡터 양자화(VQ)와 잔차 벡터 양자화(RVQ)</h3>
<p>양자화 과정의 효율성과 표현력은 코덱의 성능을 좌우한다. 초기의 단순한 접근 방식은 곧 한계에 부딪혔고, 이를 극복하기 위한 혁신적인 기법이 등장했다.</p>
<p>**벡터 양자화(Vector Quantization, VQ)**는 인코더가 출력한 잠재 공간의 벡터를 코드북 내에서 가장 가까운 벡터(codebook vector)로 대체하는 가장 기본적인 양자화 방식이다. 하지만 이 방식은 심각한 한계를 가진다. 예를 들어, 24kHz 샘플링 레이트의 오디오를 6kbps 비트레이트로 압축한다고 가정하면, 각 프레임당 약 80비트를 할당할 수 있다. 이를 단일 코드북으로 표현하려면 <span class="math math-inline">2^{80}</span>개라는 천문학적인 크기의 코드북이 필요하며, 이는 현실적으로 구현 및 학습이 불가능하다.4</p>
<p>**잔차 벡터 양자화(Residual Vector Quantization, RVQ)**는 이러한 한계를 극복하기 위해 제안된 혁신적인 기술로, SoundStream, EnCodec 등 거의 모든 현대 신경망 코덱의 표준으로 자리 잡았다.14 RVQ는 여러 개의 양자화기를 계층적으로(cascade) 연결하는 방식이다. 첫 번째 양자화기가 원본 신호를 양자화하면, 두 번째 양자화기는 원본 신호와 첫 번째 양자화 결과 사이의 ’잔차(residual error)’를 다시 양자화한다. 이 과정을 반복함으로써, 각 단계의 양자화기는 이전 단계에서 포착하지 못한 미세한 정보를 점진적으로 보정해 나간다. 예를 들어, 각각 1024개의 벡터를 가진 8개의 코드북을 사용하면, 총</p>
<p>1024×8개의 벡터만으로도 10248(=280)가지의 표현력을 달성할 수 있다.4 이 계층적 접근 방식은 관리 가능한 코드북 크기로도 매우 높은 표현력을 구현할 수 있게 하여, 고품질과 저비트레이트라는 두 마리 토끼를 동시에 잡는 핵심 열쇠가 되었다.</p>
<h3>2.3  고품질 학습을 위한 손실 함수: 재구성 손실과 적대적 손실</h3>
<p>신경망 코덱이 원본과 구별하기 어려운 고품질 오디오를 생성하기 위해서는 정교하게 설계된 손실 함수(loss function)를 통한 학습이 필수적이다.</p>
<p>**재구성 손실(Reconstruction Loss)**은 원본 오디오와 디코더가 복원한 오디오 간의 차이를 직접적으로 측정하는 손실이다. 단순히 파형의 유사도만 측정하면(예: L1 손실) 소리가 뭉개지거나 흐릿해지는 문제가 발생할 수 있다. 이를 보완하기 위해 시간 영역(time-domain) 손실과 주파수 영역(frequency-domain) 손실을 함께 사용한다. 특히 다중 스케일 단시간 푸리에 변환(Multi-scale STFT) 손실은 다양한 해상도에서 스펙트로그램을 비교함으로써 오디오의 시간적, 주파수적 구조를 모두 보존하는 데 효과적이다.15</p>
<p>**적대적 손실(Adversarial Loss)**은 재구성 손실만으로는 포착하기 어려운 ’지각적 현실감(perceptual realism)’을 확보하기 위해 도입된 생성적 적대 신경망(GAN) 기반의 학습 방식이다.4 이 방식에서는 ’판별자(discriminator)’라는 또 다른 신경망이 동원된다. 판별자는 원본 오디오와 코덱이 생성한 오디오를 구별하도록 학습된다. 동시에 코덱(생성자)은 판별자를 속여 자신이 생성한 오디오가 진짜처럼 들리도록 학습된다.15 이 경쟁적인 학습 과정은 코덱이 단순히 수학적으로 유사한 파형이 아니라, 통계적으로 실제 오디오와 유사한 특성을 가진, 즉 인간이 듣기에 자연스러운 소리를 생성하도록 강제한다. 이를 통해 미세한 잡음(artifact)을 줄이고 선명도를 높이는 데 큰 효과를 볼 수 있다. 판별자 역시 다양한 시간 스케일이나 해상도에서 작동하여 여러 종류의 잡음을 효과적으로 포착한다.4</p>
<p>이처럼 신경망 코덱의 아키텍처는 단순한 압축 문제를 넘어, 생성 모델에 적합한 토큰 공간을 창출하는 이중의 목표를 동시에 해결하도록 진화해왔다. 압축 효율을 높이기 위한 해결책(RVQ)이 의도치 않게 생성 AI 문제에 완벽한 입력(토큰 시퀀스)을 제공하는 결과로 이어진 것은 이 기술의 발전 과정에서 나타난 가장 흥미로운 지점 중 하나이다.</p>
<hr />
<h2>3.  주요 신경망 코덱 아키텍처 심층 분석</h2>
<p>신경망 오디오 코덱 분야는 Google, Meta, Microsoft와 같은 거대 기술 기업들의 주도 하에 빠르게 발전해왔으며, 학계와 오픈소스 커뮤니티 또한 중요한 혁신을 이끌고 있다. 각 진영은 서로 다른 목표와 접근 방식을 통해 기술의 지평을 넓혀왔다.</p>
<h3>3.1  Google의 선구적 연구: SoundStream, Lyra, AudioLM</h3>
<p>Google은 신경망 코덱 분야의 초기 연구를 선도하며 SoundStream, Lyra, AudioLM 등 기념비적인 모델들을 발표했다.</p>
<p><strong>SoundStream</strong>은 음성, 음악, 일반 오디오를 모두 처리할 수 있는 범용(universal) 신경망 코덱의 기틀을 마련한 모델이다.4 컨볼루션 오토인코더와 RVQ, 그리고 다중 스케일 및 STFT 기반 판별자를 활용한 GAN 학습 방식을 결합한 구조를 처음으로 제시했다.4 특히 ‘양자화기 드롭아웃(Quantizer Dropout)’ 기술은 훈련 중에 RVQ의 일부 계층을 무작위로 비활성화함으로써, 단일 모델이 추가적인 학습 없이 다양한 비트레이트에 대응할 수 있게 하는 혁신적인 아이디어였다.4</p>
<p><strong>Lyra</strong>는 매우 낮은 비트레이트의 <em>음성</em> 통신에 특화된 코덱이다.19</p>
<ul>
<li><strong>Lyra v1</strong>은 전통적인 특징 추출기(로그 멜 스펙트로그램)를 사용하되, 디코더를 WaveRNN 기반의 생성 모델로 대체하여 3kbps라는 초저비트레이트에서 높은 음질을 달성했다.21 하지만 약 90ms에 달하는 높은 지연 시간(latency)이 단점이었다.20</li>
<li><strong>Lyra v2</strong>는 아키텍처를 SoundStream 기반의 오토인코더로 전면 교체하여 성능과 효율을 극적으로 개선했다. 지연 시간을 20ms로 크게 줄였으며 20, 3.2kbps의 Lyra v2가 10kbps의 전통적인 Opus 코덱과 맞먹는 음질을 보여주어 막대한 대역폭 절감 효과를 입증했다.22</li>
</ul>
<p><strong>AudioLM</strong>은 오디오 생성을 본격적인 언어 모델링 문제로 접근한 프레임워크로, 토큰화의 잠재력을 최대한으로 보여주었다.9 이 모델의 핵심은 두 종류의 토큰을 사용하는 <em>하이브리드 토큰화(hybrid tokenization)</em> 방식이다.25</p>
<ul>
<li>
<p><strong>시맨틱 토큰(Semantic Tokens):</strong> w2v-BERT와 같은 자기지도학습(self-supervised learning) 모델에서 추출하며, “무엇을” 말하는지와 같은 장기적이고 고차원적인 의미 구조를 포착한다.9</p>
</li>
<li>
<p><strong>어쿠스틱 토큰(Acoustic Tokens):</strong> SoundStream을 통해 생성되며, 화자의 목소리 톤, 음향 환경 등 “어떻게” 들리는지에 대한 미세한 음향 정보를 담는다.9</p>
</li>
</ul>
<p>AudioLM은 계층적 트랜스포머 모델을 사용하여 먼저 시맨틱 토큰을 예측하고, 그 다음 예측된 시맨틱 토큰을 조건으로 어쿠스틱 토큰을 예측한다. 이를 통해 장기적인 의미의 일관성과 높은 음향 품질을 동시에 달성했다.26</p>
<h3>3.2  Meta의 오픈소스 혁신: EnCodec과 AudioCraft 생태계</h3>
<p>Meta는 강력한 성능의 코덱을 오픈소스로 공개하고 이를 기반으로 한 생성 모델 생태계를 구축함으로써 기술 민주화에 크게 기여했다.</p>
<p><strong>EnCodec</strong>은 SoundStream 아키텍처를 계승하고 발전시킨 고품질 실시간 신경망 코덱이다.14 단일 다중 스케일 스펙트로그램 판별자를 사용하여 훈련 과정을 단순화하고 안정성을 높였다.15 EnCodec은 최첨단 성능을 입증했는데, 3kbps 모델이 6kbps Opus는 물론 12kbps Lyra v2보다도 우수한 주관적 음질 평가(MUSHRA 점수)를 기록하기도 했다.6</p>
<p><strong>AudioCraft</strong>는 EnCodec을 핵심 토크나이저로 사용하는 Meta의 통합 오디오 생성 프레임워크다.29 이 생태계는 다음을 포함한다:</p>
<ul>
<li>
<p><strong>MusicGen:</strong> 텍스트 프롬프트나 멜로디 정보(크로마그램)를 바탕으로 EnCodec 토큰을 예측하여 고품질 음악을 생성하는 텍스트-음악(Text-to-Music) 모델이다.31</p>
</li>
<li>
<p><strong>AudioGen:</strong> 동일한 원리를 적용하여 텍스트로부터 음향 효과를 생성하는 텍스트-음향(Text-to-Sound) 모델로, 영화, 게임 등 다양한 분야에 활용될 수 있다.29</p>
</li>
</ul>
<p>AudioCraft는 단일화되고 간결한 코드베이스로 공개되어, 오디오 생성 분야의 연구 개발 장벽을 크게 낮추는 데 기여했다.29</p>
<h3>3.3  Microsoft의 언어 모델링 접근: VALL-E와 제로샷 TTS</h3>
<p>Microsoft는 오디오 토큰화를 텍스트-음성 합성(TTS)에 접목하여 제로샷(zero-shot) 음성 복제라는 새로운 지평을 열었다.</p>
<p><strong>VALL-E</strong>는 TTS를 조건부 언어 모델링 과제로 재정의한 기념비적인 모델이다.10 이 모델의 파이프라인은 <code>음소(phoneme) --&gt;&gt; 이산 코드(discrete code) --&gt;&gt; 파형(waveform)</code> 순서로 진행된다. VALL-E는 EnCodec과 같은 기성 신경망 코덱이 생성한 이산 코드를 직접 예측 대상으로 삼는다.</p>
<p>VALL-E의 핵심 혁신은 TTS에서의 *인컨텍스트 러닝(in-context learning)*이다. 처음 보는 화자의 3초짜리 음성 녹음 파일을 ’어쿠스틱 프롬프트(acoustic prompt)’로 제공하면, VALL-E는 해당 화자의 목소리, 감정, 음향 환경까지 그대로 보존하여 고품질의 개인화된 음성을 합성할 수 있다.10 이는 별도의 훈련 없이 즉석에서 목소리를 복제하는 ‘제로샷 음성 복제(zero-shot voice cloning)’ 기술이다.</p>
<p><strong>VALL-E 2</strong>는 원본 모델의 안정성과 효율성 문제를 개선한 후속 버전이다. ‘반복 인지 샘플링(repetition-aware sampling)’ 기법을 도입하여 모델이 무한 루프에 빠지는 현상을 방지하고, ’그룹 토큰 모델링(grouped token modeling)’으로 유효 시퀀스 길이를 줄여 추론 속도를 높였다. 이를 통해 일부 벤치마크에서 인간과 동등한 수준(human-parity)의 성능을 달성했다.35</p>
<h3>3.4  학계와 오픈소스의 발전: HiFi-Codec, MUFFIN, StreamCodec</h3>
<p>학계와 오픈소스 커뮤니티는 거대 기업들이 간과하기 쉬운 문제들을 해결하고 새로운 방향을 제시하며 기술 생태계를 풍성하게 만들고 있다.</p>
<p><strong>HiFi-Codec</strong>은 대규모 데이터와 컴퓨팅 자원이 필요한 코덱 훈련의 어려움을 해결하고자 개발되었다.36 새로운 ‘그룹-잔차 벡터 양자화(Group-Residual Vector Quantization, GRVQ)’ 기술을 도입하여 단 4개의 코드북만으로도 EnCodec을 능가하는 성능을 달성했다. 이는 후속 생성 모델의 토큰 시퀀스 길이를 크게 줄여 모델의 부담을 덜어주는 중요한 성과다.36 또한, EnCodec, SoundStream, HiFi-Codec의 훈련 코드를 포함하는 <code>AcademiCodec</code> 툴킷을 공개하여 오픈소스 연구에 크게 기여했다.36</p>
<p><strong>MUFFIN</strong>은 ’신경 심리음향 코덱(Neural Psychoacoustic Codec, NPC)’이라는 개념을 처음으로 제시한 모델이다.2 이 모델은 기존의 심리음향 원리를 신경망 아키텍처에 직접 통합했다. 핵심 기술인 ’다중 대역 스펙트럼 RVQ(Multi-Band Spectral RVQ, MBS-RVQ)’는 인간의 청각 시스템처럼 주파수 대역별 지각적 중요도에 따라 비트를 차등 할당한다.2 이를 통해 12.5Hz라는 극단적인 압축률에서도 중요한 음향 디테일을 보존하며 고품질을 유지한다.38</p>
<p><strong>StreamCodec</strong>은 실시간 통신(Real-Time Communication, RTC)을 위해 특별히 설계된 코덱이다.40 미래 정보를 참조하지 않는 완전 인과적(fully causal) 구조를 채택하고, 변형 이산 코사인 변환(MDCT) 영역에서 작동하여 낮은 지연 시간을 구현했다. 핵심 혁신인 ’잔차 스칼라-벡터 양자화기(Residual Scalar-Vector Quantizer, RSVQ)’는 대략적인 특징은 단순한 스칼라 양자화로, 세밀한 디테일은 벡터 양자화로 처리하여 효율과 품질의 균형을 맞췄다. 그 결과, 7M의 가벼운 파라미터로 일반 CPU에서 20ms의 고정 지연 시간을 달성하여 실제 실시간 통신 환경에 매우 적합하다.40</p>
<h3>3.5 표 2.1: 주요 신경망 오디오 코덱 아키텍처 비교</h3>
<table><thead><tr><th>모델명 (주요 기관)</th><th>핵심 아키텍처</th><th>주요 혁신 / 차별점</th><th>주 응용 분야</th><th>주요 성능 지표</th><th>관련 자료</th></tr></thead><tbody>
<tr><td><strong>SoundStream</strong> (Google)</td><td>Conv-오토인코더 + RVQ</td><td>가변 비트레이트를 위한 양자화기 드롭아웃; STFT 기반 판별자</td><td>범용 오디오 압축</td><td>3kbps가 12kbps Opus 성능 상회. 스마트폰 CPU에서 실행 가능.</td><td>4</td></tr>
<tr><td><strong>Lyra v2</strong> (Google)</td><td>SoundStream 기반</td><td>초저비트레이트 음성에 최적화; 낮은 지연시간 (20ms)</td><td>실시간 음성 통신</td><td>3.2kbps 음질이 10kbps Opus와 동급; Pixel 6에서 실시간보다 35배 빠름.</td><td>19</td></tr>
<tr><td><strong>EnCodec</strong> (Meta)</td><td>SoundStream 기반</td><td>단순화된 단일 다중 스케일 스펙트로그램 판별자; 오픈소스 공개</td><td>범용 압축; AudioCraft의 기반</td><td>3kbps가 6kbps Opus 및 12kbps Lyra v2 성능 상회.</td><td>6</td></tr>
<tr><td><strong>VALL-E</strong> (Microsoft)</td><td>코덱 토큰 기반 언어 모델</td><td>3초 프롬프트 기반 인컨텍스트 러닝으로 제로샷 음성 복제</td><td>제로샷 텍스트-음성 합성 (TTS)</td><td>감정/환경을 보존하는 개인화된 음성 합성.</td><td>10</td></tr>
<tr><td><strong>MUFFIN</strong> (학계)</td><td>Conv-오토인코더 + MBS-RVQ</td><td>신경 심리음향 코딩(NPC); 다중 대역 스펙트럼 RVQ</td><td>고품질, 고압축 오디오</td><td>SOTA 12.5Hz 압축률; 화자/콘텐츠 정보 분리.</td><td>2</td></tr>
<tr><td><strong>StreamCodec</strong> (학계)</td><td>인과적 MDCT 기반 + RSVQ</td><td>잔차 스칼라-벡터 양자화기(RSVQ); 낮은 지연시간을 위한 완전 인과적 구조</td><td>실시간 통신 (RTC)</td><td>20ms 고정 지연시간; CPU에서 실시간보다 약 20배 빠른 속도 (7M 파라미터).</td><td>40</td></tr>
<tr><td><strong>HiFi-Codec</strong> (학계)</td><td>Conv-오토인코더 + GRVQ</td><td>그룹-잔차 VQ (GRVQ)로 코드북 수 감소 (4개)</td><td>생성 모델 기반 기술 (예: VALL-E)</td><td>더 적은 코드북으로 Encodec 성능 상회, LLM 부담 감소.</td><td>36</td></tr>
</tbody></table>
<h2>4.  오디오 토큰화의 응용: 생성형 AI의 새로운 지평</h2>
<p>오디오 토큰화는 오디오를 AI가 이해하고 생성할 수 있는 ’언어’로 변환함으로써, 이전에는 상상하기 어려웠던 다양한 응용 분야를 현실로 만들고 있다. 이 기술의 핵심은 ’토큰’이라는 보편적 인터페이스를 통해 음성, 음악, 음향 효과 등 서로 다른 오디오 양식을 동일한 생성 모델링 프레임워크 안에서 다룰 수 있게 했다는 점이다.</p>
<h3>4.1  텍스트-음성 합성(TTS): VALL-E 패러다임과 제로샷 음성 복제</h3>
<p>TTS 기술은 오디오 토큰화로 인해 근본적인 변화를 맞이했다. 과거의 TTS 시스템은 녹음된 음성 조각들을 이어 붙이는 연접 합성(concatenative synthesis) 방식에 의존했다.43 이후 멜 스펙트로그램과 같은 중간 표현을 예측하고 이를 보코더(vocoder)로 파형으로 변환하는 방식으로 발전했지만, VALL-E의 등장은 새로운 패러다임을 제시했다.</p>
<p>VALL-E는 멜 스펙트로그램 대신 코덱 토큰을 직접 예측 대상으로 삼는다.10 이는 언어 모델이 고수준의 순서 예측에만 집중하게 하고, 풍부한 음향 디테일의 복원은 사전 학습된 코덱의 디코더에 맡기는 효율적인 분업 구조를 만든다. 이 접근법의 가장 큰 성과는 ’인컨텍스트 러닝’을 통한 제로샷 음성 복제다. 단 3초의 음성 샘플을 프롬프트로 제공하는 것만으로, 모델은 해당 목소리의 톤, 감정, 녹음 환경까지 모방하여 새로운 문장을 발화할 수 있다.10</p>
<p>이러한 기술은 오디오북 제작, 가상 비서, 내비게이션 시스템 등 기존 TTS 응용 분야의 품질을 획기적으로 향상시키는 동시에 34, 사용자가 자신의 목소리로 콘텐츠를 만드는 등 새로운 개인화 서비스를 가능하게 한다.45 KT AI Studio나 odiro.ai와 같은 상용 서비스들은 이미 이러한 AI 기반 TTS 기술을 활용하여 비용 효율적이고 빠른 오디오 콘텐츠 제작 솔루션을 제공하고 있다.44</p>
<h3>4.2  텍스트-음악 및 음향 생성: MusicGen과 AudioGen</h3>
<p>오디오 토큰화의 범용성은 음성을 넘어 음악과 음향 효과 생성으로 확장되었다. Meta의 AudioCraft 프레임워크는 범용 코덱인 EnCodec이 복잡한 음악과 환경음까지 효과적으로 토큰화할 수 있음을 보여주었다.29</p>
<p><strong>MusicGen</strong>은 음악 생성을 자기회귀 언어 모델링(autoregressive language modeling) 문제로 취급한다. 텍스트 설명이나 멜로디 정보(크로마그램)를 조건으로 EnCodec 토큰 시퀀스를 예측하여 고품질의 음악을 만들어낸다.31 특히, 효율적인 코드북 인터리빙(interleaving) 패턴을 사용하여 주어진 길이의 오디오를 생성하는 데 필요한 자기회귀 단계 수를 줄임으로써, 더 긴 길이의 음악을 더 빠르게 생성할 수 있게 되었다.31 또한, 확산 모델(diffusion model) 기반의 디코더(MBD-EnCodec)를 사용하면 계산 비용은 증가하지만 더 적은 잡음으로 높은 품질의 음악을 생성할 수 있다.32</p>
<p><strong>AudioGen</strong>은 동일한 원리를 음향 효과 생성에 적용한 모델이다. 텍스트 프롬프트로부터 개 짖는 소리, 자동차 경적 소리 등 특정 음향 효과를 생성하며, 이는 영화, 비디오 게임 개발, 가상 환경 구축에 매우 유용하다.29</p>
<h3>4.3  실시간 통신(RTC): 지연시간, 품질, 대역폭의 트레이드오프</h3>
<p>실시간 음성 및 영상 통신은 낮은 지연시간, 높은 품질, 효율적인 대역폭 사용이라는 상충하는 요구사항을 동시에 만족시켜야 하는 매우 까다로운 분야다.46 신경망 코덱은 이 분야에서 큰 잠재력을 보여주고 있다.</p>
<p>Lyra v2나 SoundStream과 같은 신경망 코덱은 Opus나 EVS 같은 기존의 고성능 통신 코덱보다 훨씬 낮은 비트레이트에서 동등하거나 더 나은 품질을 제공한다.6 예를 들어, Lyra v2는 Opus가 사용하는 대역폭의 50-60%만으로 비슷한 수준의 음질을 달성한다.22 이렇게 확보된 대역폭의 여유는 불안정한 네트워크 환경에서 패킷 손실에 대응하기 위해 중복 패킷을 전송하는 등 통신의 안정성을 높이는 데 사용될 수 있다.46</p>
<p>하지만 RTC 적용에는 ’인과성(causality)’이라는 큰 장벽이 존재한다. EnCodec과 같은 고성능 코덱들은 종종 미래의 오디오 프레임을 참조하는 비인과적(non-causal) 구조를 채택하는데, 이는 필연적으로 지연시간을 발생시킨다.49 이를 해결하기 위해 StreamCodec과 같이 완전 인과적으로 설계된 모델이 등장했다. 이 모델은 오직 과거와 현재의 정보만을 사용하여 20ms 수준의 낮은 고정 지연시간을 달성함으로써 진정한 실시간 상호작용을 가능하게 한다.40</p>
<p>결론적으로, 오디오 토큰화는 다양한 오디오 양식을 ’토큰’이라는 단일한 인터페이스로 통합하는 강력한 추상화를 제공한다. 코덱이 오디오를 보편적인 ’언어’로 번역하면, 트랜스포머 기반의 언어 모델은 그 언어의 문법과 통계적 패턴을 학습한다. 따라서 음성, 음악, 음향 효과 생성은 결국 어떤 데이터셋으로 학습하고 어떤 조건(텍스트, 음소, 멜로디)을 부여하는가의 문제로 귀결된다. 이러한 모듈성과 보편성이야말로 오디오 토큰화 기반 아키텍처가 가진 혁신적이고 확장 가능한 힘의 원천이다.</p>
<h2>5.  기술적 난제와 첨단 연구 동향</h2>
<p>신경망 오디오 코덱 기술은 비약적인 발전을 이루었지만, 여전히 해결해야 할 기술적 난제들이 존재한다. 이러한 문제들을 해결하기 위한 노력은 최첨단 연구의 핵심 동력이 되고 있으며, 기술의 완성도를 높이는 방향으로 나아가고 있다.</p>
<h3>5.1  잡음(Artifact) 문제: 분석과 완화</h3>
<p>신경망 코덱은 특히 낮은 비트레이트에서 전통적인 코덱과는 다른 종류의 독특한 잡음을 발생시킬 수 있다.</p>
<p><strong>잡음의 특성:</strong> GAN 기반 학습 과정에서 발생하는 주기적이거나 음조가 있는 잡음(tonal/periodic artifacts), 소리의 선명도 부족, 심지어는 모델이 ’환각’을 일으켜 만들어내는 기묘한 소리 등이 포함될 수 있다.14 Descript Audio Codec(DAC)과 같은 모델은 이러한 잡음 문제를 해결하는 데 중점을 두고 설계되었다.50</p>
<p><strong>탐지 및 평가:</strong> 이러한 잡음을 정량적으로 측정하는 것은 매우 어렵다. MUSHRA와 같은 주관적 청취 평가 방식이 황금 표준으로 여겨지지만, 시간과 비용이 많이 소요된다는 단점이 있다.6 따라서 깨끗한 원본 오디오 없이도 웅웅거림(hum), 쉬익거림(hiss), 왜곡(distortion) 등의 잡음을 자동으로 탐지할 수 있는 객관적인 비참조(no-reference) 모델에 대한 연구가 활발히 진행되고 있다.51</p>
<p><strong>완화 전략:</strong> 잡음을 줄이기 위해 다양한 전략이 시도되고 있다.</p>
<ul>
<li><strong>아키텍처 개선:</strong> 다중 스케일 스펙트로그램 판별자와 같이 개선된 판별자 구조를 사용하면 훈련 과정에서 잡음을 더 효과적으로 억제할 수 있다.15</li>
<li><strong>주기적 활성화 함수:</strong> Snake와 같은 주기적인 편향(inductive bias)을 가진 활성화 함수를 도입하면 음조 및 주기성 관련 잡음을 줄여 더 깨끗한 오디오를 생성하는 데 도움이 된다.14</li>
<li><strong>확산 모델 기반 디코더:</strong> 코덱의 잠재 벡터를 디코딩하는 데 확산 모델을 사용하면(예: MBD-EnCodec) 더 적은 잡음으로 고품질 샘플을 생성할 수 있지만, 계산 비용이 증가하는 단점이 있다.32</li>
</ul>
<h3>5.2  이산 표현 불일치(DRI): 오디오 언어 모델의 근본적인 도전</h3>
<p>오디오 토큰화의 가장 심각한 문제 중 하나는 ‘이산 표현 불일치(Discrete Representation Inconsistency, DRI)’ 현상이다.</p>
<p><strong>핵심 문제:</strong> 텍스트에서 ’사과’라는 단어가 항상 동일한 토큰 시퀀스로 변환되는 것과 달리, 오디오에서는 인간이 지각하기에 동일한 소리라도 미세한 입력 변화나 문맥에 따라 여러 개의 서로 다른 토큰 시퀀스로 변환될 수 있다.52 이것이 DRI 문제의 본질이다.</p>
<p><strong>생성 모델에 미치는 영향:</strong> 이러한 ‘다대일(many-to-one)’ 매핑은 VALL-E와 같은 자기회귀 언어 모델에 큰 혼란을 준다. 모델은 다음 토큰을 예측하도록 훈련되는데, 동일한 소리에 대해 여러 개의 ‘정답’ 토큰 시퀀스가 존재하면 예측 목표가 불분명해진다. 이는 결국 생성 과정에서 단어 누락, 반복, 발음 오류 등 불안정한 결과를 초래하는 주요 원인이 된다.52</p>
<p><strong>제안된 해결책:</strong> 최근 연구들은 코덱의 훈련 목표 함수에 ’일관성 손실(consistency loss)’을 추가하여 이 문제를 직접적으로 해결하려 하고 있다.</p>
<ul>
<li>
<p><strong>슬라이스 일관성(Slice-Consistency):</strong> 오디오의 일부를 무작위로 잘라낸다. 코덱이 이 잘라낸 조각으로부터 생성한 토큰 표현이, 전체 오디오에서 생성된 토큰 시퀀스의 해당 부분과 일치하도록 강제한다.52</p>
</li>
<li>
<p><strong>섭동 일관성(Perturbation-Consistency):</strong> 원본 오디오와 여기에 미세한 노이즈를 가한 오디오에 대해 코덱이 동일한 토큰 시퀀스를 출력하도록 훈련한다.52</p>
</li>
</ul>
<p>이러한 방법들은 토큰의 일관성을 크게 향상시켜, 후속 TTS 작업에서 단어 오류율(WER)을 낮추고 화자 유사도를 높이는 효과가 있음이 입증되었다.52</p>
<h3>5.3  재구성을 넘어: 소스 분리와 심리음향 모델링</h3>
<p>코덱의 역할을 단순한 압축 및 재구성을 넘어, 보다 지능적인 정보 처리로 확장하려는 연구도 활발하다.</p>
<p><strong>소스 분리 코덱(SD-Codec):</strong> 대부분의 범용 코덱은 음성, 음악 등 모든 종류의 오디오를 구분 없이 단일 잠재 공간에 압축한다. 이는 제어 가능성과 해석 가능성을 떨어뜨리는 원인이 된다.54 SD-Codec은 오디오 코딩과 소스 분리를 결합한 새로운 접근법을 제시한다.55 이 모델은 음성, 음악, 음향 효과 등 각 도메인에 특화된 여러 개의 양자화기를 사용한다. 혼합된 오디오가 입력되면, 모델은 각 소스를 분리하여 해당 양자화기로 라우팅하도록 학습된다. 이를 통해 전체 오디오 믹스뿐만 아니라 개별 소스 트랙도 선택적으로 재구성할 수 있어, 제어 가능성과 해석 가능성을 크게 높인다.55</p>
<p><strong>신경 심리음향 코딩(MUFFIN):</strong> 이 접근법은 인간의 청각 심리 모델을 신경망 아키텍처에 명시적으로 통합한다.2 MUFFIN의 MBS-RVQ 모듈은 MP3와 유사하게, 음성 명료도에 중요한 중간 주파수 대역에 더 많은 비트를 할당하는 등 주파수 대역의 지각적 중요도에 따라 비트를 차등적으로 할당한다.2 이는 인간의 청각 원리를 종단간 학습 프레임워크 내에서 구현한 것으로, 극단적인 압축률에서도 뛰어난 음질을 유지하는 비결이다.38</p>
<h3>5.4  성능 표준화: Codec-SUPERB 벤치마크의 역할</h3>
<p>수많은 신경망 코덱 모델이 등장하면서, 이들을 공정하게 비교 평가하는 것이 중요한 과제가 되었다. 각 모델이 서로 다른 데이터셋, 메트릭, 실험 환경에서 평가되어 직접적인 비교가 어렵기 때문이다.57</p>
<p><strong>Codec-SUPERB</strong>는 이러한 문제를 해결하기 위해 설계된 포괄적인 벤치마크이다.60 이 벤치마크는 두 가지 주요 관점에서 모델을 평가한다:</p>
<ul>
<li><strong>신호 레벨 메트릭:</strong> 다양한 음성, 음악, 일반 오디오 데이터셋에 대한 재구성 품질을 측정한다.58</li>
<li><strong>응용 레벨 메트릭:</strong> 코덱이 생성한 토큰이 후속 과제에 필요한 정보를 얼마나 잘 보존하는지 평가한다. 평가 항목에는 내용(음성인식), 화자 정체성(화자 인증), 운율 등 부가 정보(감정 인식) 등이 포함된다.57</li>
</ul>
<p>Codec-SUPERB는 오픈소스 코드베이스와 공개 리더보드를 통해 커뮤니티의 협력을 장려하고, 투명하고 엄격한 비교 프레임워크를 제공함으로써 연구 개발을 가속화하는 중요한 역할을 하고 있다.12</p>
<h3>5.5 표 4.1: 신경망 오디오 코덱의 주요 기술적 난제와 해결 방안</h3>
<table><thead><tr><th>난제</th><th>문제 설명</th><th>제안된 해결책 / 연구 방향</th><th>주요 모델 / 논문</th><th>관련 자료</th></tr></thead><tbody>
<tr><td><strong>오디오 잡음(Artifacts)</strong></td><td>GAN 기반 훈련이 전통적 코덱에 없는 비현실적인 음조/주기성 잡음을 유발할 수 있음.</td><td>개선된 판별자, 주기적 활성화 함수(Snake), 확산 모델 기반 디코더.</td><td>EnCodec, Improved RVQGAN, MBD-EnCodec</td><td>14</td></tr>
<tr><td><strong>이산 표현 불일치 (DRI)</strong></td><td>지각적으로 동일한 오디오가 다른 토큰 시퀀스로 표현되어 후속 언어 모델에 혼란을 주고 생성 오류를 야기함.</td><td>코덱 훈련 목표에 일관성 손실 항 추가 (슬라이스 일관성, 섭동 일관성).</td><td>Liu et al. (2024)</td><td>52</td></tr>
<tr><td><strong>제어/해석 가능성 부족</strong></td><td>범용 코덱이 모든 소리를 단일 잠재 공간에 혼합하여 제어 및 해석이 어려움.</td><td>소스 분리와 코딩을 동시에 학습하는 도메인 특화 양자화기 사용.</td><td>SD-Codec</td><td>54</td></tr>
<tr><td><strong>비효율적 비트 할당</strong></td><td>표준 코덱이 모든 주파수 성분을 동등하게 처리하여 지각적으로 비효율적임.</td><td>인간의 청각 민감도에 따라 주파수 대역별로 비트를 차등 할당하는 심리음향 원리 통합.</td><td>MUFFIN</td><td>2</td></tr>
<tr><td><strong>비인과적 모델의 높은 지연시간</strong></td><td>다수의 고성능 모델이 미래 프레임을 참조하여 실시간 상호작용에 부적합함.</td><td>완전 인과적 아키텍처 설계, 주로 스펙트럼(MDCT) 도메인에서 작동.</td><td>StreamCodec, Lyra v2</td><td>40</td></tr>
<tr><td><strong>벤치마킹 파편화</strong></td><td>다양한 평가 데이터셋, 메트릭, 조건으로 인해 모델 간 공정한 비교가 어려움.</td><td>신호 및 응용 레벨 작업을 위한 통일된 코드베이스와 리더보드를 갖춘 표준화된 공개 벤치마크.</td><td>Codec-SUPERB</td><td>12</td></tr>
</tbody></table>
<h2>6.  시스템 및 하드웨어 최적화: 이론에서 구현까지</h2>
<p>신경망 오디오 코덱의 잠재력을 최대한 발휘하기 위해서는 알고리즘의 혁신뿐만 아니라, 이를 효율적으로 실행할 수 있는 시스템 및 하드웨어 수준의 최적화가 필수적이다. 클라우드 기반의 대규모 학습부터 사용자의 기기에서 실시간으로 작동하는 온디바이스 추론에 이르기까지, 다양한 환경에 맞는 최적화 전략이 요구된다.</p>
<h3>6.1  GPU와 TPU 가속: NVIDIA와 Google을 통한 성능 극대화</h3>
<p>대규모 신경망 모델의 훈련과 배포는 고성능 병렬 컴퓨팅 하드웨어에 크게 의존한다.</p>
<p><strong>NVIDIA CUDA</strong>는 이 분야에서 가장 지배적인 플랫폼으로, 신경망 코덱 개발의 모든 단계에서 핵심적인 역할을 한다.</p>
<ul>
<li><strong>CUDA-X 라이브러리:</strong> cuDNN(딥러닝 연산 라이브러리)이나 TensorRT(추론 최적화 런타임)와 같은 고수준 라이브러리는 코덱 모델의 성능을 극대화하는 데 필수적이다.62</li>
<li><strong>NVIDIA NeMo:</strong> NVIDIA의 대화형 AI 툴킷인 NeMo는 EnCodec을 포함한 다양한 오디오 코덱의 사전 구성된 모델과 훈련 스크립트를 제공하여, 연구자들이 복잡한 설정 없이도 손쉽게 모델을 훈련하고 미세 조정할 수 있도록 지원한다.63</li>
<li><strong>고급 최적화:</strong> 극도의 성능이 요구되는 경우, 개발자는 CUDA C++나 심지어 PTX 어셈블리어를 직접 작성하여 연산을 융합(fuse)하고 하드웨어 활용률을 최대화할 수 있다.64 또한, 메모리 접근 패턴을 최적화(coalescing)하고 고정 메모리(pinned memory)를 사용하는 것은 데이터 전송 병목 현상을 줄이는 데 매우 중요하다.65</li>
</ul>
<p>**Google TPU(Tensor Processing Unit)**는 Google이 AI 워크로드를 위해 특별히 설계한 맞춤형 반도체(ASIC)이다.66</p>
<ul>
<li><strong>아키텍처:</strong> TPU는 딥러닝의 핵심 연산인 대규모 행렬 곱셈에 최적화되어 있으며, 많은 AI 작업에서 GPU 대비 뛰어난 전력 대비 성능(performance-per-watt)을 보여준다.67</li>
<li><strong>확장성:</strong> TPU Pod는 수천 개의 칩을 고속으로 연결하여 대규모 분산 학습 및 추론을 가능하게 한다. 이는 신경망 코덱 개발에 필요한 방대한 데이터셋과 거대 모델을 처리하는 데 필수적이다.66 최신 세대인 v5p, Trillium, Ironwood는 생성 모델의 대규모 훈련 및 추론을 위해 특별히 설계되었다.67</li>
<li><strong>온디바이스/엣지:</strong> Edge TPU는 저전력 온디바이스 추론을 위해 설계되어, 소형 기기에서도 실시간 오디오 분류와 같은 작업을 가능하게 한다.70</li>
</ul>
<h3>6.2  온디바이스 인텔리전스: Apple의 Core ML과 Neural Engine의 역할</h3>
<p>사용자 경험과 개인 정보 보호를 위해, AI 모델을 클라우드가 아닌 사용자 기기에서 직접 실행하는 온디바이스 인텔리전스의 중요성이 커지고 있다.</p>
<p><strong>Core ML 프레임워크:</strong> Apple의 Core ML은 iOS, macOS 등 자사 운영체제에 머신러닝 모델을 쉽게 통합할 수 있도록 지원하는 프레임워크이다.71 이는 모델을 CPU, GPU, 그리고 특화된 Neural Engine에서 효율적으로 실행되도록 최적화한다.71</p>
<p><strong>Neural Engine:</strong> Apple Silicon에 탑재된 Neural Engine은 신경망 연산을 저전력으로 빠르게 가속하기 위해 설계된 전용 하드웨어이다. Core ML은 모델의 연산 그래프를 분석하여 적절한 부분을 Neural Engine에 자동으로 할당함으로써 성능을 극대화한다.71</p>
<p><strong>개인 정보 보호와 반응성:</strong> 온디바이스 처리의 가장 큰 장점은 사용자의 민감한 오디오 데이터가 기기 밖으로 나가지 않아 개인 정보가 보호된다는 점이다. 또한, 네트워크 연결 없이도 앱이 빠르게 반응하여 사용자 경험을 향상시킨다.71 Core ML은 PyTorch나 TensorFlow로 훈련된 모델을 변환하고, Xcode 내에서 온디바이스 성능을 직접 프로파일링할 수 있는 강력한 도구를 제공한다.73</p>
<h3>6.3  비즈니스 케이스: 대역폭, 인코딩 비용, 그리고 체감 품질(QoE)</h3>
<p>신경망 코덱의 도입은 기술적 우수성을 넘어 명확한 비즈니스 가치를 창출한다.</p>
<p><strong>대역폭 절감:</strong> 가장 직접적인 경제적 이점이다. Lyra v2가 Opus 대비 50-60%의 대역폭만으로 동등 이상의 품질을 제공하는 것처럼 22, 효율적인 신경망 코덱은 스트리밍 서비스, VoIP 제공업체, 콘텐츠 전송 네트워크(CDN)의 데이터 전송 비용을 직접적으로 절감시킨다.46</p>
<p><strong>인코딩 비용:</strong> 이는 중요한 상충 관계(trade-off)이다. 신경망 코덱은 대역폭을 절약하는 대신, 인코딩 과정에서 더 많은 계산 자원을 필요로 할 수 있다. 이는 초기 도입 비용이나 지속적인 처리 비용의 증가로 이어질 수 있다.50 따라서 새로운 코덱을 도입하기 전에는</p>
<p><code>인코딩 비용 / 단위 시간당 대역폭 절감액</code>과 같은 손익분기점 분석이 필수적이다.74</p>
<p><strong>체감 품질(Quality of Experience, QoE):</strong> 대역폭이 제한된 모바일 네트워크 환경에서 더 효율적인 코덱은 사용자가 버퍼링 없이 더 높은 품질의 오디오를 경험하게 한다. 이는 사용자 만족도를 높이는 핵심적인 경쟁 우위 요소가 된다.46 실시간 통신에서는 더 선명하고 끊김 없는 통화를 의미한다.</p>
<p><strong>신규 시장 창출:</strong> Lyra와 같은 초저비트레이트 코덱은 네트워크 인프라가 매우 열악한 지역에서도 음성 통신을 가능하게 하여, 새로운 시장과 응용 분야를 열 수 있는 잠재력을 가지고 있다.19</p>
<p>결론적으로, 신경망 코덱 생태계는 클라우드에서 훈련되는 거대하고 범용적인 모델과, 엣지 디바이스에서 효율적으로 실행되도록 특화된 모델이라는 두 가지 방향으로 발전하고 있다. 최첨단 연구와 모델 개발은 NVIDIA GPU나 Google TPU와 같은 대규모 클라우드 인프라에서 이루어지지만, 실제 사용자에게 서비스를 제공하기 위해서는 Apple의 Neural Engine과 같은 온디바이스 하드웨어에 최적화된 경량 모델이 필수적이다. 유튜브와 같은 스트리밍 플랫폼과 Webex와 같은 실시간 통신 앱의 비즈니스 요구사항과 기술적 제약이 다르듯, 미래의 코덱 아키텍처는 이러한 다양한 요구에 맞춰 분화되고 발전해 나갈 것이다.</p>
<h2>7.  윤리적, 법적, 사회적 고찰</h2>
<p>신경망 오디오 코덱, 특히 이를 기반으로 한 생성형 AI 기술은 인류에게 큰 혜택을 줄 잠재력을 지녔지만, 동시에 오용될 경우 심각한 사회적 문제를 야기할 수 있는 양날의 검이다. 이 기술을 책임감 있게 발전시키기 위해서는 기술적 논의를 넘어 윤리적, 법적, 사회적 차원의 깊이 있는 고찰이 반드시 수반되어야 한다.</p>
<h3>7.1  양날의 검: 음성 복제, 딥페이크, 그리고 허위 정보</h3>
<p>VALL-E와 같이 단 몇 초의 음성 샘플만으로 특정인의 목소리를 완벽에 가깝게 복제하는 기술은 심각한 위협을 내포하고 있다.10 이러한 기술은 ’보이스피싱’이나 ’조부모 사기(Grandparent scams)’와 같은 금융 사기, 특정 인물을 사칭한 명예훼손, 선거 개입을 위한 정치적 허위 정보 유포 등에 악용될 수 있다.11 특히 저렴한 비용으로 누구나 쉽게 접근할 수 있는 음성 복제 도구들이 확산되면서 그 위험성은 더욱 커지고 있다.78</p>
<p>이에 따라 기술 개발자에게는 강력한 윤리적 책임이 요구된다.</p>
<ul>
<li>
<p><strong>데이터 출처의 투명성:</strong> AI 음성을 훈련시키는 데 사용되는 데이터는 반드시 해당 성우나 개인의 명시적인 동의를 얻어야 한다.80</p>
</li>
<li>
<p><strong>통제된 배포:</strong> 개발된 AI 음성이 계약된 승인된 채널 외에서는 사용될 수 없도록 기술적인 안전장치를 마련해야 한다. 단순히 ’서비스 이용 약관’에 금지 조항을 두는 것만으로는 불충분하며, 기술 자체에 오용 방지 메커니즘이 내장되어야 한다.80</p>
</li>
</ul>
<p>Microsoft와 같은 주요 연구 기관들은 이러한 위험성을 인지하고, 자신들의 연구가 사용자의 동의를 전제로 수행되었음을 명시하며 기술 오용 가능성에 대한 윤리적 성명을 발표하고 있다.10</p>
<h3>7.2  법적 지형 탐색: 저작권, 퍼블리시티권, 그리고 규제</h3>
<p>기술의 발전 속도를 법과 제도가 따라가지 못하면서 새로운 법적 공백과 논쟁이 발생하고 있다.</p>
<p><strong>저작권법:</strong></p>
<ul>
<li><strong>인간 저작물 원칙:</strong> 미국 저작권법은 인간이 창작한 저작물만을 보호한다. 따라서 의미 있는 인간의 창의적 개입 없이 AI가 전적으로 생성한 음악이나 오디오는 저작권 보호 대상이 아니며, 퍼블릭 도메인으로 간주된다.82</li>
<li><strong>AI 보조 저작물:</strong> 반면, 인간이 AI를 도구로 사용하여 창작 활동을 한 경우(예: 가사를 쓰고 AI로 반주를 생성하거나, AI 생성물을 인간이 상당 부분 수정하는 경우), 인간이 창작한 부분에 대해서는 저작권이 인정될 수 있다. 이 경우, 기존의 2차적 저작물(derivative works)에 대한 법리가 적용된다.83</li>
</ul>
<p><strong>퍼블리시티권(Right of Publicity, RoP):</strong> 개인의 목소리나 초상을 상업적으로 무단 사용하는 것을 막는 가장 직접적인 법적 권리이다.79</p>
<ul>
<li><strong>주(州) 단위 입법:</strong> 미국에는 연방 차원의 퍼블리시티권 법이 없어 주마다 다른 법률이 적용되는 파편화된 상태이다.86 최근 각 주에서는 AI 생성 딥페이크와 음성 복제를 명시적으로 규제하기 위해 기존 법을 개정하거나 새로운 법을 제정하고 있다.</li>
<li><strong>ELVIS 법 (테네시주):</strong> 2024년 제정된 이 법은 AI를 사용한 개인의 목소리 모방을 명시적으로 금지하고, 이를 생성, 배포하는 자는 물론 관련 기술을 제공하는 도구 제작자에게까지 책임을 묻는 획기적인 법안이다.87</li>
<li><strong>연방법 제정 움직임:</strong> NO FAKES 법안, No AI FRAUD 법안 등 개인의 디지털 복제물에 대한 연방 차원의 보호를 제공하려는 법안들이 발의되어 논의 중이다.86</li>
</ul>
<p><strong>규제 기관의 역할:</strong> 미국 연방거래위원회(FTC)는 AI를 이용한 사칭 행위에 대한 규제 방안을 모색하고 있으며, 미국 저작권청은 디지털 복제물에 대한 연구를 통해 향후 입법 방향을 제시할 안내서를 준비하고 있다.86</p>
<h3>7.3  더 넓은 사회적 함의: 신경 인터페이스 시대의 프라이버시</h3>
<p>이 기술의 장기적인 발전 궤적은 뇌-컴퓨터 인터페이스(BCI)를 통한 직접적인 뇌-음성 변환 기술로 향하고 있다.89 이는 ’정신적 프라이버시(mental privacy)’라는 근본적인 윤리적 질문을 제기한다. 만약 기술이 개인이 외부로 표현하려 의도하지 않은 내면의 말(covert speech)이나 사적인 생각까지 해독할 수 있게 된다면, 이는 자유롭게 사적인 숙고를 할 수 있는 ’인지적 자유(cognitive liberty)’를 침해할 수 있다.90</p>
<p><strong>진정성과 통제:</strong> 사용자가 의도한 바를, 의도했을 때에만 정확하게 표현할 수 있는 ’사용자 통제권’은 핵심적인 윤리적 고려사항이다. 모델의 예측 오류나 외부의 간섭으로 인해 사용자의 의도와 다른 말이 생성되고 그것이 사용자의 발언으로 오인된다면, 이는 개인의 자율성과 책임 소재에 심각한 문제를 야기할 수 있다.89</p>
<p><strong>정체성의 본질:</strong> 목소리는 개인의 정체성을 구성하는 핵심 요소이다. 이를 완벽하게 복제하고 개인과 분리할 수 있게 되면서, 우리는 타인의 발언에 대한 신뢰와 진정성에 대한 근본적인 도전에 직면하게 되었다.88</p>
<p>이러한 기술적, 법적, 윤리적 문제들은 기술의 발전과 사회적 수용 사이에 존재하는 ’거버넌스 갭(governance gap)’을 명확히 보여준다. VALL-E와 같은 파괴적 기술이 등장하면, 사회는 먼저 기존의 법(저작권, 퍼블리시티권)을 적용하려 시도한다. 그러나 기존 법의 한계가 드러나면, 악용 사례가 증가하는 불확실성의 시기를 거쳐, 결국 ELVIS 법과 같이 AI 기술을 직접 겨냥한 새로운 법률이 만들어지는 반응적 주기가 나타난다. 현재의 음성 복제 논쟁은 향후 신경 인터페이스와 같은 더 발전된 AI 기술에서 반복될 문제들의 예고편이며, 혁신과 오용 가능성 사이의 긴장을 조율하려는 사회와 법 제도의 끊임없는 노력이 요구될 것이다.</p>
<h2>8.  미래 전망: 차세대 오디오 AI를 향하여</h2>
<p>신경망 오디오 코덱과 토큰화 기술은 현재도 빠르게 진화하고 있으며, 학계와 산업계의 최신 연구는 차세대 오디오 AI의 청사진을 제시하고 있다. 현재의 기술적 난제를 해결하려는 노력과 함께, AI의 근본적인 작동 방식에 대한 새로운 비전이 등장하고 있다.</p>
<h3>8.1  최고 수준 학회에서 본 최신 연구 동향 (ICASSP, NeurIPS, ICML)</h3>
<p>세계 최고 수준의 AI 및 신호 처리 학회는 미래 연구의 방향을 가늠할 수 있는 중요한 지표이다.</p>
<ul>
<li><strong>ICASSP 2025 (음향, 음성, 신호 처리 국제 학회):</strong> 오디오 및 신호 처리 분야의 핵심 연구가 발표되는 곳이다. 주요 주제로는 코덱 내에서의 소스 분리 학습 93, 실시간 응용을 위한 견고성 및 효율성 향상 40, 요약 임베딩을 활용한</li>
</ul>
<p><code>Music2Latent2</code>와 같은 새로운 압축 기법 94, 음성 변환 및 복원을 위한 확산 모델의 적용 94, 그리고 CodecFake+와 같은 대규모 데이터셋을 이용한 딥페이크 탐지 기술 96 등이 있다.</p>
<ul>
<li>
<p><strong>NeurIPS 2024 (신경정보처리시스템학회):</strong> 머신러닝 분야의 최고 권위 학회로, 생성형 오디오에 특화된 워크숍이 주목받고 있다. ‘Audio Imagination’ 워크숍은 오디오 생성의 고유한 과제와 해결책을 집중적으로 다루며 97, ‘머신러닝과 압축’ 워크숍은 두 분야의 깊은 상호 관계를 탐구하며 신경망 압축의 이론적 한계와 생성 모델의 역할을 논의한다.99 “SNAC: 다중 스케일 신경망 오디오 코덱“과 같은 논문은 핵심 코덱 아키텍처의 지속적인 개선을 보여준다.98</p>
</li>
<li>
<p><strong>ICML 2025 (국제 머신러닝 학회):</strong> 머신러닝의 근본 원리에 초점을 맞춘다. 잡음 감소 연구는 생성 모델이 만들어내는 ’생성적 잡음’에만 집중하도록 모델을 제약하는 방향으로 나아가고 있으며, 이는 오디오 분야에도 적용될 수 있는 원리이다.100 또한 생성 모델의 내부 표현과 학습 동역학에 대한 깊은 이해는 더 견고하고 해석 가능한 오디오 모델 설계의 기반이 될 것이다.101</p>
</li>
</ul>
<h3>8.2  비전의 최전선: 월드 모델과 JEPA(Joint Embedding Predictive Architecture)</h3>
<p>Meta의 수석 AI 과학자이자 딥러닝의 대가인 얀 르쿤(Yann LeCun)은 현재의 자기회귀 모델(LLM 포함)이 가진 근본적인 한계를 지적하며 새로운 비전을 제시한다.</p>
<ul>
<li><strong>자기회귀 모델의 한계에 대한 비판:</strong> 르쿤은 LLM과 같은 자기회귀 모델이 진정한 의미에서 세계를 이해하지 못하며, 추론이나 계획 능력이 부족하고, 오류가 누적되는 경향이 있다고 주장한다.103</li>
<li><strong>월드 모델(World Models)의 필요성:</strong> 그는 진정한 지능은 세상이 어떻게 작동하는지에 대한 내적인 ’월드 모델’을 학습함으로써 얻어지며, 이는 텍스트보다 훨씬 풍부한 정보를 담고 있는 관찰(예: 비디오)을 통해 주로 학습된다고 역설한다.103</li>
<li><strong>JEPA(Joint Embedding Predictive Architecture):</strong> 르쿤이 이러한 월드 모델을 구축하기 위해 제안한 아키텍처이다.105</li>
<li><strong>비생성적 접근:</strong> 픽셀이나 샘플 단위로 모든 디테일을 복원하려는 GAN이나 VAE와 달리, JEPA는 추상적인 표현 공간에서 작동한다. 입력의 일부(예: 이미지의 패치, 비디오의 일부)를 가린 뒤, 주변 문맥의 표현으로부터 가려진 부분의 ’표현’을 예측하도록 학습한다.106</li>
<li><strong>예측 가능한 것에 집중:</strong> 추상 공간에서 작동함으로써, JEPA는 예측 불가능한 세부 사항(예: 나뭇잎의 정확한 패턴)은 무시하고 세상의 고차원적이고 예측 가능한 의미 구조를 학습하는 데 집중할 수 있다. 이는 훨씬 효율적이고 견고한 학습으로 이어진다.106</li>
<li><strong>오디오에의 적용:</strong> 현재는 이미지(I-JEPA)와 비디오(V-JEPA)에 적용되었지만, 이 원리는 오디오에 직접 적용될 수 있다. ’A-JEPA’는 미래 오디오 세그먼트의 추상적 표현을 예측함으로써, 청각 세계에 대한 예측 모델을 형성하게 될 것이다. 이는 단순히 토큰 시퀀스를 모델링하는 것을 넘어, 소리를 발생시키는 세상의 근본적인 동역학을 모델링하는 단계로의 도약을 의미한다.</li>
</ul>
<h3>8.3  미래 연구를 위한 종합 및 제언</h3>
<p>미래의 오디오 AI 연구는 여러 방향으로 수렴하고 발전할 것이다.</p>
<ul>
<li><strong>하이브리드 모델:</strong> MUFFIN의 심리음향 원리, SD-Codec의 소스 분리, DRI 완화를 위한 견고한 훈련 방법론 등을 결합한 하이브리드 모델이 유망하다.</li>
<li><strong>범용 표현:</strong> 진정으로 보편적인 오디오 표현을 찾기 위한 연구는 계속될 것이다. Masked Modeling Duo(M2D)와 같은 프레임워크는 다양한 후속 작업에 활용될 수 있는 사전 훈련 모델을 목표로 한다.107</li>
<li><strong>효율성과 확장성:</strong> 모델이 커짐에 따라 효율적인 아키텍처, 양자화, 하드웨어-소프트웨어 공동 설계에 대한 연구의 중요성은 더욱 커질 것이다.</li>
<li><strong>견고성과 신뢰:</strong> DRI 문제 해결, 더 나은 잡음 탐지, 그리고 강력한 딥페이크 탐지 모델 개발은 기술이 신뢰를 얻고 책임감 있게 배포되기 위한 필수 과제이다.</li>
<li><strong>다중 모달리티로의 확장:</strong> 궁극적인 목표는 AudioGPT나 새로운 다중 모달 Lyra MLLM과 같이, 텍스트, 이미지, 오디오, 비디오를 넘나들며 추론할 수 있는 거대 다중 모달 모델에 오디오 토큰을 완벽하게 통합하는 것이다.108</li>
</ul>
<p>이러한 연구 동향은 신경망 오디오 처리 분야가 단순한 ’모방(mimicry)’에서 진정한 ’이해(understanding)’로 나아가고 있음을 시사한다. 현재의 생성 모델은 정교한 모방 기계에 가깝다. 이들은 오디오 파형의 통계적 특성이나 토큰 시퀀스의 패턴을 모방한다. DRI와 같은 문제는 이러한 피상적인 이해의 한계를 보여주는 증상이다. 르쿤이 제시하는 JEPA와 같은 비전은 다음 단계로의 도약을 의미한다. 즉, 다음 토큰을 예측하는 것이 아니라, 추상적인 개념 공간에서 세상의 ’결과’를 예측하는 모델을 만드는 것이다. 이는 훨씬 더 야심 차고 어려운 목표이지만, 진정으로 지능적인 방식으로 추론하고, 계획하고, 세상과 상호작용할 수 있는 AI로 나아가는 길이다.</p>
<h2>9. 결론</h2>
<p>신경망 오디오 코덱과 이를 통한 오디오 토큰화는 디지털 오디오 처리 분야에 혁명적인 변화를 가져왔다. 전통적인 심리음향 모델 기반 코덱의 한계를 넘어, 데이터 기반의 종단간 학습을 통해 전례 없는 압축 효율과 품질을 달성했으며, 더 나아가 오디오를 언어 모델이 처리할 수 있는 이산적인 토큰 시퀀스로 변환하는 새로운 패러다임을 열었다. 이로써 음성, 음악, 음향 등 모든 종류의 오디오를 NLP의 강력한 도구로 분석하고 생성할 수 있게 되었으며, 이는 VALL-E의 제로샷 음성 복제, MusicGen의 텍스트-음악 생성과 같은 혁신적인 응용을 탄생시켰다.</p>
<p>그러나 이 기술은 이산 표현 불일치(DRI)와 같은 근본적인 기술적 난제와 함께, 딥페이크, 저작권, 프라이버시 침해 등 심각한 윤리적, 법적 과제를 안고 있다. 최첨단 연구는 이러한 문제들을 해결하기 위해 일관성 손실 도입, 소스 분리, 심리음향 원리 통합 등 다각적인 노력을 기울이고 있으며, Codec-SUPERB와 같은 표준화된 벤치마크는 공정하고 투명한 기술 경쟁을 촉진하고 있다.</p>
<p>미래를 내다볼 때, 신경망 오디오 기술은 단순히 정교한 소리를 ’모방’하는 단계를 넘어, 소리가 발생하는 물리적 세계를 ’이해’하고 예측하는 방향으로 나아가고 있다. JEPA와 같은 차세대 아키텍처는 오디오를 더 큰 다중 모달 지능 시스템의 핵심 구성 요소로 통합하여, AI가 세상을 더 깊이 이해하고 상호작용하는 데 기여할 것이다. 결국, 오디오 토큰화는 단순한 압축 기술을 넘어, 기계가 인간의 가장 원초적인 소통 방식인 ’소리’를 이해하고 구사하게 만드는, 생성형 AI 시대의 핵심 기반 기술로 자리매김하고 있다. 이 기술의 책임감 있는 발전과 사회적 합의를 형성해 나가는 것이 우리 앞에 놓인 중요한 과제이다.</p>
<h2>10. 참고 자료</h2>
<ol>
<li>Codec, 코텍이란? - 간단한 개발관련 내용, accessed July 16, 2025, https://joochang.tistory.com/120</li>
<li>Multi-band Frequency Reconstruction for Neural … - alphaXiv, accessed July 16, 2025, https://www.alphaxiv.org/ko/overview/2505.07235v1</li>
<li>Audio Codecs: From Common Choices to the Best Pick - Tencent MPS, accessed July 16, 2025, https://mps.live/blog/details/audio-codecs-from-common-to-best</li>
<li>SoundStream: An End-to-End Neural Audio Codec - Ostin X - 티스토리, accessed July 16, 2025, https://ostin.tistory.com/205</li>
<li>(PDF) Neural Audio Codec - ResearchGate, accessed July 16, 2025, https://www.researchgate.net/publication/391981877_Neural_Audio_Codec</li>
<li>[Paper 리뷰] EnCodec: High-Fidelity Neural Audio Compression, accessed July 16, 2025, https://randomsampling.tistory.com/210</li>
<li>[2107.03312] SoundStream: An End-to-End Neural Audio Codec - arXiv, accessed July 16, 2025, https://arxiv.org/abs/2107.03312</li>
<li>뉴럴 오디오 코덱과 코덱 기반 언어모델 |, accessed July 16, 2025, https://ncsoft.github.io/ncresearch/597b26f272d3ebc8b4c070563e2a0969503009a9</li>
<li>AudioLM, accessed July 16, 2025, https://google-research.github.io/seanet/audiolm/examples/</li>
<li>Vall E - Microsoft, accessed July 16, 2025, https://www.microsoft.com/en-us/research/project/vall-e-x/vall-e/</li>
<li>VALL-E - Microsoft, accessed July 16, 2025, https://www.microsoft.com/en-us/research/project/vall-e-x/</li>
<li>Codec SUPERB, accessed July 16, 2025, https://codecsuperb.github.io/</li>
<li>신경망과 딥 러닝 비교 - 인공 지능 분야 간의 차이점 - AWS, accessed July 16, 2025, https://aws.amazon.com/ko/compare/the-difference-between-deep-learning-and-neural-networks/</li>
<li>High-Fidelity Audio Compression with Improved RVQGAN - arXiv, accessed July 16, 2025, https://arxiv.org/pdf/2306.06546</li>
<li>High Fidelity Neural Audio Compression - OpenReview, accessed July 16, 2025, https://openreview.net/pdf?id=ivCd8z8zR2</li>
<li>High Fidelity Neural Audio Compression (EnCodec) - Ostin X - 티스토리, accessed July 16, 2025, https://ostin.tistory.com/206</li>
<li>NeurIPS Poster High-Fidelity Audio Compression with Improved RVQGAN, accessed July 16, 2025, https://neurips.cc/virtual/2023/poster/70342</li>
<li>SoundStream: An End-to-End Neural Audio Codec - Google Research, accessed July 16, 2025, https://research.google/blog/soundstream-an-end-to-end-neural-audio-codec/</li>
<li>google/lyra: A Very Low-Bitrate Codec for Speech Compression - GitHub, accessed July 16, 2025, https://github.com/google/lyra</li>
<li>Lyra (codec) - Wikipedia, accessed July 16, 2025, https://en.wikipedia.org/wiki/Lyra_(codec)</li>
<li>Lyra: A New Very Low-Bitrate Codec for Speech Compression - Google Research, accessed July 16, 2025, https://research.google/blog/lyra-a-new-very-low-bitrate-codec-for-speech-compression/</li>
<li>Google Lyra goes to open source (C++) - HydrogenAudio, accessed July 16, 2025, https://hydrogenaudio.org/index.php/topic,120810.0.html</li>
<li>Lyra V2 open-source audio codec gets faster, higher quality and compatible with more platforms - CNX Software, accessed July 16, 2025, https://www.cnx-software.com/2022/10/03/lyra-v2-open-source-audio-codec-gets-faster-higher-quality-and-compatible-with-more-platforms/</li>
<li>AudioLM: a Language Modeling Approach to Audio Generation - arXiv, accessed July 16, 2025, https://arxiv.org/pdf/2209.03143</li>
<li>AudioLM: a Language Modeling Approach to Audio Generation - arXiv, accessed July 16, 2025, https://arxiv.org/abs/2209.03143</li>
<li>AudioLM: a Language Modeling Approach to Audio Generation - Google Research, accessed July 16, 2025, https://research.google/blog/audiolm-a-language-modeling-approach-to-audio-generation/</li>
<li>a Language Modeling Approach to Audio Generation | audiolm-google-torch, accessed July 16, 2025, https://carlosholivan.github.io/audiolm-google-torch/</li>
<li>[2210.13438] High Fidelity Neural Audio Compression - arXiv, accessed July 16, 2025, https://arxiv.org/abs/2210.13438</li>
<li>AudioCraft - Meta AI, accessed July 16, 2025, https://ai.meta.com/resources/models-and-libraries/audiocraft/</li>
<li>AudioCraft: A simple one-stop shop for audio modeling - Meta AI, accessed July 16, 2025, https://ai.meta.com/blog/audiocraft-musicgen-audiogen-encodec-generative-ai-audio/</li>
<li>MusicGen - Open Laboratory, accessed July 16, 2025, https://openlaboratory.ai/models/musicgen</li>
<li>MusicGen: Simple and Controllable Music Generation - AudioCraft, accessed July 16, 2025, https://audiocraft.metademolab.com/musicgen.html</li>
<li>Discover: Exploring Meta’s MusicGen - AI Music Generator - RebelCorp, accessed July 16, 2025, https://rebelcorp.in/blog/revolutionizing-music-creation-explore-meta-s-musicgen-the-ai-powered-music-generator</li>
<li>Text-to-Audio 모델 12選, accessed July 16, 2025, https://turingpost.co.kr/p/text-to-audio-models</li>
<li>VALL-E 2: Enhancing the robustness and naturalness of text-to-speech models - Microsoft, accessed July 16, 2025, https://www.microsoft.com/en-us/research/articles/vall-e-2-enhancing-the-robustness-and-naturalness-of-text-to-speech-models/</li>
<li>HiFi-Codec: Group-residual Vector quantization for High Fidelity Audio Codec - Bohrium, accessed July 16, 2025, https://bohrium.dp.tech/paper/arxiv/bfd114dae4e013161bd852bde09e636ce14cfb6c1b74bc71cf8f152b9307ce55</li>
<li>AcademiCodec: An Open Source Audio Codec Model for Academic Research - GitHub, accessed July 16, 2025, https://github.com/yangdongchao/AcademiCodec</li>
<li>Multi-band Frequency Reconstruction for Neural Psychoacoustic Coding - ResearchGate, accessed July 16, 2025, https://www.researchgate.net/publication/391676766_Multi-band_Frequency_Reconstruction_for_Neural_Psychoacoustic_Coding</li>
<li>Multi-band Frequency Reconstruction for Neural Psychoacoustic Coding | OpenReview, accessed July 16, 2025, <a href="https://openreview.net/forum?id=fXqUGWCdjf&amp;noteId=71OTE3OXEa">https://openreview.net/forum?id=fXqUGWCdjf¬eId=71OTE3OXEa</a></li>
<li>A Streamable Neural Audio Codec with Residual Scalar-Vector Quantization for Real-Time Communication | Request PDF - ResearchGate, accessed July 16, 2025, https://www.researchgate.net/publication/390639361_A_Streamable_Neural_Audio_Codec_with_Residual_Scalar-Vector_Quantization_for_Real-Time_Communication</li>
<li>[2504.06561] A Streamable Neural Audio Codec with Residual Scalar-Vector Quantization for Real-Time Communication - arXiv, accessed July 16, 2025, https://arxiv.org/abs/2504.06561</li>
<li>A Streamable Neural Audio Codec with Residual Scalar-Vector Quantization for Real-Time Communication - arXiv, accessed July 16, 2025, https://arxiv.org/html/2504.06561v1</li>
<li>음성 합성 - 위키백과, 우리 모두의 백과사전, accessed July 16, 2025, <a href="https://ko.wikipedia.org/wiki/%EC%9D%8C%EC%84%B1_%ED%95%A9%EC%84%B1">https://ko.wikipedia.org/wiki/%EC%9D%8C%EC%84%B1_%ED%95%A9%EC%84%B1</a></li>
<li>TTS 오디오북 제작하기 : 오디로 블로그, accessed July 16, 2025, https://www.odiro.ai/blog/?bmode=view&amp;idx=19790767</li>
<li>KT AI STUDIO TTS 변환, 내 목소리로 컨텐츠 생성해 볼 수 있을까? - YouTube, accessed July 16, 2025, https://m.youtube.com/watch?v=WB-JiroH1NE</li>
<li>Webex AI Codec, accessed July 16, 2025, https://www.webex.com/content/dam/www/us/en/whitepaper/ai-codec-whitepaper-cm-6165.pdf</li>
<li>Open Source Audio Codec Lyra V2 Is Now Compatible With More Platforms, accessed July 16, 2025, https://www.opensourceforu.com/2022/10/open-source-audio-codec-lyra-v2-is-now-compatible-with-more-platforms/</li>
<li>Neural encoding enables more-efficient recovery of lost audio packets - Amazon Science, accessed July 16, 2025, https://www.amazon.science/blog/neural-encoding-enables-more-efficient-recovery-of-lost-audio-packets</li>
<li>Speech quality evaluation of neural audio codecs - ISCA Archive, accessed July 16, 2025, https://www.isca-archive.org/interspeech_2024/muller24c_interspeech.pdf</li>
<li>Neural Audio Codecs: The Future of Sound Compression - Abyssmedia, accessed July 16, 2025, https://www.abyssmedia.com/audioconverter/neural-audio-codecs-overview.shtml</li>
<li>A no-reference model for detecting audio artifacts using pretrained audio neural networks - Amazon Science, accessed July 16, 2025, https://www.amazon.science/publications/a-no-reference-model-for-detecting-audio-artifacts-using-pretrained-audio-neural-networks</li>
<li>Analyzing and Mitigating Inconsistency in Discrete Audio Tokens for Neural Codec Language Models | OpenReview, accessed July 16, 2025, https://openreview.net/forum?id=KFLtFSOtdj</li>
<li>Analyzing and Mitigating Inconsistency in Discrete Audio Tokens for Neural Codec Language Models - arXiv, accessed July 16, 2025, https://arxiv.org/html/2409.19283v2</li>
<li>Learning Source Disentanglement in Neural Audio Codec This work was funded by the European Union (ERC, HI-Audio, 101052978). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority - arXiv, accessed July 16, 2025, https://arxiv.org/html/2409.11228v1</li>
<li>Learning Source Disentanglement in Neural Audio Codec, accessed July 16, 2025, https://arxiv.org/pdf/2409.11228</li>
<li>Multi-band Frequency Reconstruction for Neural Psychoacoustic Coding - arXiv, accessed July 16, 2025, https://arxiv.org/html/2505.07235v1</li>
<li>[2402.13071] Codec-SUPERB: An In-Depth Analysis of Sound Codec Models - arXiv, accessed July 16, 2025, https://arxiv.org/abs/2402.13071</li>
<li>Codec-SUPERB: An In-Depth Analysis of Sound Codec Models - ACL Anthology, accessed July 16, 2025, https://aclanthology.org/2024.findings-acl.616.pdf</li>
<li>Codec-Superb @ SLT 2024: A Lightweight Benchmark For Neural Audio… - OpenReview, accessed July 16, 2025, https://openreview.net/forum?id=Ns0tgxwVU3</li>
<li>SUPERB: Speech processing Universal PERformance Benchmark, accessed July 16, 2025, https://superbbenchmark.github.io/</li>
<li>voidful/Codec-SUPERB: Audio Codec Speech processing Universal PERformance Benchmark - GitHub, accessed July 16, 2025, https://github.com/voidful/Codec-SUPERB</li>
<li>CUDA-X GPU-Accelerated Libraries - NVIDIA Developer, accessed July 16, 2025, https://developer.nvidia.com/gpu-accelerated-libraries</li>
<li>NeMo/tutorials/tts/Audio_Codec_Training.ipynb at main - GitHub, accessed July 16, 2025, https://github.com/NVIDIA/NeMo/blob/main/tutorials/tts/Audio_Codec_Training.ipynb</li>
<li>Advanced NVIDIA CUDA Kernel Optimization Techniques: Handwritten PTX, accessed July 16, 2025, https://developer.nvidia.com/blog/advanced-nvidia-cuda-kernel-optimization-techniques-handwritten-ptx/</li>
<li>Optimizing Apple Lossless Audio Codec Algorithm using NVIDIA CUDA Architecture, accessed July 16, 2025, https://www.researchgate.net/publication/324786530_Optimizing_Apple_Lossless_Audio_Codec_Algorithm_using_NVIDIA_CUDA_Architecture</li>
<li>What Is a Tensor Processing Unit (TPU)? - Built In, accessed July 16, 2025, https://builtin.com/articles/tensor-processing-unit-tpu</li>
<li>Tensor Processing Units (TPUs) - Google Cloud, accessed July 16, 2025, https://cloud.google.com/tpu</li>
<li>GPU and TPU Comparative Analysis Report | by ByteBridge - Medium, accessed July 16, 2025, https://bytebridge.medium.com/gpu-and-tpu-comparative-analysis-report-a5268e4f0d2a</li>
<li>Ironwood: The first Google TPU for the age of inference, accessed July 16, 2025, https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/</li>
<li>Models - Audio classification - Coral, accessed July 16, 2025, https://coral.ai/models/audio-classification/</li>
<li>Core ML - Machine Learning - Apple Developer, accessed July 16, 2025, https://developer.apple.com/machine-learning/core-ml/</li>
<li>Core ML | Apple Developer Documentation, accessed July 16, 2025, https://developer.apple.com/documentation/coreml</li>
<li>Tune your Core ML models - WWDC21 - Videos - Apple Developer, accessed July 16, 2025, https://developer.apple.com/videos/play/wwdc2021/10038/</li>
<li>Saving Streaming Costs: Adding a New Codec, accessed July 16, 2025, https://streaminglearningcenter.com/codecs/saving-streaming-costs-adding-a-new-codec.html</li>
<li>세계의 오디오 코덱 시장 조사 안내서 : 산업 분석, 규모, 점유율, 성장, 동향, 예측(2025-2033년), accessed July 16, 2025, https://www.giikorea.co.kr/report/vmr1672196-global-audio-codec-market-research-report-industry.html</li>
<li>오디오 코덱 시장 규모, 산업 안내서 및 예측 2032 - Market Research Future, accessed July 16, 2025, https://www.marketresearchfuture.com/ko/reports/audio-codec-market-32761</li>
<li>Codec2: a whole Podcast on a Floppy Disk - Auphonic Blog, accessed July 16, 2025, https://auphonic.com/blog/2018/06/01/codec2-podcast-on-floppy-disk/</li>
<li>AI Voice Cloning: Do These 6 Companies Do Enough to Prevent Misuse? - Consumer Reports, accessed July 16, 2025, https://innovation.consumerreports.org/AI-Voice-Cloning-Report-.pdf</li>
<li>Understanding Voice Cloning: The Laws and Your Rights - - National Security Law Firm, accessed July 16, 2025, http://www.nationalsecuritylawfirm.com/understanding-voice-cloning-the-laws-and-your-rights/</li>
<li>Ethical AI at ReadSpeaker: Best Practices for the AI Voice Industry, accessed July 16, 2025, https://www.readspeaker.com/blog/ethical-ai/</li>
<li>VALL-E: Vall E R - Microsoft, accessed July 16, 2025, https://www.microsoft.com/en-us/research/project/vall-e-x/vall-e-r/</li>
<li>BREAKING NEWS: The END of AI MUSIC! US Copyright Office Says You Can’t Copyright it, accessed July 16, 2025, https://www.youtube.com/watch?v=JcCeZUL5iLs&amp;pp=0gcJCfwAo7VqN5tD</li>
<li>Can AI-generated content be copyrighted? Here’s what a new report from the US Copyright Office says… - Music Business Worldwide, accessed July 16, 2025, https://www.musicbusinessworldwide.com/can-ai-generated-content-be-copyrighted-heres-what-a-new-report-from-the-us-copyright-office-says1/</li>
<li>Copyright Law in the Age of AI: Navigating Authorship, Infringement, and Creative Rights - New York State Bar Association, accessed July 16, 2025, https://nysba.org/copyright-law-in-the-age-of-ai-navigating-authorship-infringement-and-creative-rights/</li>
<li>AI Filmmaking &amp; AI Music Gain Copyright Recognition in the USA: What Creators Need to Know, accessed July 16, 2025, https://www.soundverse.ai/blog/article/ai-filmmaking-and-ai-music-gain-copyright-recognition-in-the-usa</li>
<li>AI “voice cloning” in the crosshairs of legislators, regulators, civil litigants - CCH, accessed July 16, 2025, https://business.cch.com/ipld/SP_AI-voice-cloning_6-20-2024_locked.pdf</li>
<li>ROP on the Rise: Right of Publicity Claims Will Rise as States Address AI Generated Deepfakes and Voice Cloning | News &amp; Events - Clark Hill, accessed July 16, 2025, https://www.clarkhill.com/news-events/news/rop-on-the-rise-right-of-publicity-claims-will-rise-as-states-address-ai-generated-deepfakes-and-voice-cloning/</li>
<li>What’s in a Voice: How the Law is Being Used to Combat Deepfakes - Sunstein LLP, accessed July 16, 2025, https://www.sunsteinlaw.com/publications/whats-in-a-voice-deepfakes</li>
<li>Neuroprosthetic Speech: The Ethical Significance of Accuracy, Control and Pragmatics, accessed July 16, 2025, https://www.cambridge.org/core/journals/cambridge-quarterly-of-healthcare-ethics/article/neuroprosthetic-speech-the-ethical-significance-of-accuracy-control-and-pragmatics/45DFF4E8AEE69A7D376082DEAFA146C1</li>
<li>Brain Recording, Mind-Reading, and Neurotechnology: Ethical Issues from Consumer Devices to Brain-Based Speech Decoding - ResearchGate, accessed July 16, 2025, https://www.researchgate.net/publication/341060379_Brain_Recording_Mind-Reading_and_Neurotechnology_Ethical_Issues_from_Consumer_Devices_to_Brain-Based_Speech_Decoding</li>
<li>Brain Recording, Mind-Reading, and Neurotechnology: Ethical Issues from Consumer Devices to Brain-Based Speech Decoding - PubMed Central, accessed July 16, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC7417394/</li>
<li>The ethical significance of user-control in AI-driven speech-BCIs: a narrative review - PMC, accessed July 16, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC11240287/</li>
<li>Hi! PARIS Papers Accepted at ICASSP &amp; ICLR 2025, accessed July 16, 2025, https://www.hi-paris.fr/2025/04/25/hi-paris-papers-accepted-at-icassp-iclr-2025/</li>
<li>ICASSP 2025 - Sony Group Portal, accessed July 16, 2025, https://www.sony.com/en/SonyInfo/technology/Conference/ICASSP2025/</li>
<li>International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2025, accessed July 16, 2025, https://machinelearning.apple.com/updates/apple-at-icassp-2025</li>
<li>CodecFake+: A Large-Scale Neural Audio Codec-Based Deepfake Speech Dataset - arXiv, accessed July 16, 2025, https://arxiv.org/html/2501.08238v2</li>
<li>NeurIPS 2024 Workshops, accessed July 16, 2025, https://neurips.cc/virtual/2024/events/workshop</li>
<li>Audio Imagination: NeurIPS 2024 Workshop - OpenReview, accessed July 16, 2025, https://openreview.net/group?id=NeurIPS.cc/2024/Workshop/Audio_Imagination</li>
<li>Machine Learning and Compression Workshop @ NeurIPS 2024, accessed July 16, 2025, https://www.itsoc.org/event/machine-learning-and-compression-workshop-neurips-2024</li>
<li>Paper Digest: ICML 2025 Papers &amp; Highlights, accessed July 16, 2025, https://www.paperdigest.org/2025/06/icml-2025-papers-highlights/</li>
<li>ICML 2025 Orals, accessed July 16, 2025, https://icml.cc/virtual/2025/events/oral</li>
<li>ICML 2025 Tuesday 07/15, accessed July 16, 2025, https://icml.cc/virtual/2025/day/7/15</li>
<li>Transcript for Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI &amp; the Future of AI | Lex Fridman Podcast #416, accessed July 16, 2025, https://lexfridman.com/yann-lecun-3-transcript/</li>
<li>What do you think about Yann Lecun’s controversial opinions about ML? [D] - Reddit, accessed July 16, 2025, https://www.reddit.com/r/MachineLearning/comments/19534v6/what_do_you_think_about_yann_lecuns_controversial/</li>
<li>Critical review of LeCun’s Introductory JEPA paper | Medium - Malcolm Lett, accessed July 16, 2025, https://malcolmlett.medium.com/critical-review-of-lecuns-introductory-jepa-paper-fabe5783134e</li>
<li>V-JEPA: The next step toward Yann LeCun’s vision of advanced machine intelligence (AMI), accessed July 16, 2025, https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/</li>
<li>NTT’s 22 papers accepted for ICASSP2025, the world’s largest international conference on signal processing technology | Topics - NTT Group, accessed July 16, 2025, https://group.ntt/en/topics/2025/03/31/icassp2025.html</li>
<li>Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition - arXiv, accessed July 16, 2025, https://arxiv.org/html/2412.09501v1</li>
<li>Rongjiehuang - GitHub, accessed July 16, 2025, https://github.com/Rongjiehuang</li>
<li>[ Embedded ] 03. 오디오코덱의 이해와 역사, accessed July 16, 2025, <a href="https://coder-in-war.tistory.com/entry/Embedded-03-%EC%98%A4%EB%94%94%EC%98%A4%EC%BD%94%EB%8D%B1%EC%9D%98-%EC%9D%B4%ED%95%B4%EC%99%80-%EC%97%AD%EC%82%AC">https://coder-in-war.tistory.com/entry/Embedded-03-%EC%98%A4%EB%94%94%EC%98%A4%EC%BD%94%EB%8D%B1%EC%9D%98-%EC%9D%B4%ED%95%B4%EC%99%80-%EC%97%AD%EC%82%AC</a></li>
<li>[논문 리뷰] DAC-JAX: A JAX Implementation of the Descript Audio Codec, accessed July 16, 2025, https://www.themoonlight.io/ko/review/dac-jax-a-jax-implementation-of-the-descript-audio-codec</li>
<li>[Paper 리뷰] SuperCodec: A Neural Speech Codec with Selective Back-Projection Network, accessed July 16, 2025, https://randomsampling.tistory.com/197</li>
<li>AAC - 나무위키, accessed July 16, 2025, https://namu.wiki/w/AAC</li>
<li>Opus(오디오 코덱) - 나무위키, accessed July 16, 2025, <a href="https://namu.wiki/w/Opus(%EC%98%A4%EB%94%94%EC%98%A4%20%EC%BD%94%EB%8D%B1)">https://namu.wiki/w/Opus(%EC%98%A4%EB%94%94%EC%98%A4%20%EC%BD%94%EB%8D%B1)</a></li>
<li>음성 생성 (텍스트 음성 변환) | Gemini API | Google AI for Developers, accessed July 16, 2025, https://ai.google.dev/gemini-api/docs/speech-generation?hl=ko</li>
<li>OuteTTS, 350M 규모의 영문 전용 TTS 모델 - 읽을거리&amp;정보공유 - 파이토치 한국 사용자 모임, accessed July 16, 2025, https://discuss.pytorch.kr/t/outetts-350m-tts/5453</li>
<li>IP 오디오 코덱 시장 규모, 공유 및 산업 동향 분석 2033 - Market Research Intellect, accessed July 16, 2025, https://www.marketresearchintellect.com/ko/product/ip-audio-codec-market/</li>
<li>오디오 코덱 및 압축 기술 - 오디오 데이터 압축 원리, 손실/무손실 압축 방식, 다양한 오디오 코덱의 특성, 비트레이트, 스트리밍 기술 및 품질 최적화 방법을 학습합니다. | Flashcards World, accessed July 16, 2025, https://flashcards.world/flashcards/sets/5f628801-5ebc-4e6e-8df7-c86c294e6e7d/</li>
<li>오디오 코덱 시장 점유율, 성장 기회 2024-2032 - Global Market Insights, accessed July 16, 2025, https://www.gminsights.com/ko/industry-analysis/audio-codec-market</li>
<li>차세대 실감방송서비스를 위한 MPEG-H 3D Audio 표준화 동향, accessed July 16, 2025, https://koreascience.kr/article/JAKO201508449473705.pdf</li>
<li>wavtokenizer: an efficient acoustic discrete codec tokenizer for audio language modeling, accessed July 16, 2025, https://arxiv.org/html/2408.16532v1</li>
<li>Bringing Interpretability to Neural Audio Codecs - arXiv, accessed July 16, 2025, https://arxiv.org/html/2506.04492v1</li>
<li>Repairing Artifacts in Neural Activity Recordings Using Low-Rank Matrix Estimation - PMC, accessed July 16, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC10220667/</li>
<li>Neural Speech and Audio Coding 1. Introduction - Minje Kim’s Home, accessed July 16, 2025, https://minjekim.com/wp-content/uploads/spm2024_mkim.pdf</li>
<li>Universal Semantic Disentangled Privacy-preserving Speech Representation Learning, accessed July 16, 2025, https://arxiv.org/html/2505.13085v1</li>
<li>Audio and Speech Processing Jan 2025 - arXiv, accessed July 16, 2025, https://www.arxiv.org/list/eess.AS/2025-01?skip=0&amp;show=250</li>
<li>NeurIPS 2024 Papers, accessed July 16, 2025, https://nips.cc/virtual/2024/papers.html</li>
<li>[2506.04492] Bringing Interpretability to Neural Audio Codecs - arXiv, accessed July 16, 2025, https://arxiv.org/abs/2506.04492</li>
<li>Codec-SUPERB: An In-Depth Analysis of Sound Codec Models - arXiv, accessed July 16, 2025, https://arxiv.org/html/2402.13071v1</li>
<li>[2409.19283] Analyzing and Mitigating Inconsistency in Discrete Audio Tokens for Neural Codec Language Models - arXiv, accessed July 16, 2025, https://arxiv.org/abs/2409.19283</li>
<li>ICML 2025 Papers, accessed July 16, 2025, https://icml.cc/virtual/2025/papers.html</li>
<li>FLAM: Frame-Wise Language-Audio Modeling - ICML 2025, accessed July 16, 2025, https://icml.cc/virtual/2025/poster/46310</li>
<li>Codec 2 - Wikipedia, accessed July 16, 2025, https://en.wikipedia.org/wiki/Codec_2</li>
<li>HE-AAC, HE-AAC v2 - Fraunhofer-Institut für Integrierte Schaltungen IIS, accessed July 16, 2025, https://www.iis.fraunhofer.de/en/ff/amm/broadcast-streaming/heaac.html</li>
<li>Now Available: Codec 2.0 - Modern Degradation Revamped - Lese Audio Technologies, accessed July 16, 2025, https://lese.io/blog/now-available-codec-2/</li>
<li>Releases / google/lyra - GitHub, accessed July 16, 2025, https://github.com/google/lyra/releases</li>
<li>Lyra 1.3.2 Free Download - Google’s Speech Codec, accessed July 16, 2025, https://www.free-codecs.com/download/lyra.htm</li>
<li>MDCTCodec: A Lightweight MDCT-based Neural Audio Codec, accessed July 16, 2025, https://www.aimodels.fyi/papers/arxiv/mdctcodec-lightweight-mdct-based-neural-audio-codec</li>
<li>[2405.06621] On Streaming Codes for Simultaneously Correcting Burst and Random Erasures - arXiv, accessed July 16, 2025, https://arxiv.org/abs/2405.06621</li>
<li>Xiao-Hang Jiang - Papers With Code, accessed July 16, 2025, https://paperswithcode.com/author/xiao-hang-jiang</li>
<li>Xiao-Hang Jiang - DBLP, accessed July 16, 2025, https://dblp.org/pid/369/3334</li>
<li>[2505.07235] Multi-band Frequency Reconstruction for Neural Psychoacoustic Coding, accessed July 16, 2025, https://arxiv.org/abs/2505.07235</li>
<li>Speech Token Prediction via Compressed-to-fine Language Modeling - arXiv, accessed July 16, 2025, https://arxiv.org/html/2505.24496v1</li>
<li>Wenrui Liu - CatalyzeX, accessed July 16, 2025, <a href="https://www.catalyzex.com/author/Wenrui%20Liu">https://www.catalyzex.com/author/Wenrui%20Liu</a></li>
<li>Electrical Engineering and Systems Science Sep 2024 - arXiv, accessed July 16, 2025, http://arxiv.org/list/eess/2024-09?skip=1075&amp;show=2000</li>
<li>‪Wenrui Liu‬ - ‪Google 学术搜索‬, accessed July 16, 2025, https://scholar.google.com/citations?user=KmaNaj4AAAAJ&amp;hl=zh-CN</li>
<li>Codec-Superb @ SLT 2024: A Lightweight Benchmark For Neural Audio Codec Models | Request PDF - ResearchGate, accessed July 16, 2025, https://www.researchgate.net/publication/388096206_Codec-Superb_SLT_2024_A_Lightweight_Benchmark_For_Neural_Audio_Codec_Models</li>
<li>Can I Be Protected Against Myself? Artificial Intelligence and Voice Replication, accessed July 16, 2025, https://jolt.richmond.edu/2024/05/17/can-i-be-protected-against-myself-artificial-intelligence-and-voice-replication/</li>
<li>Codec-SUPERB - Hugging Face, accessed July 16, 2025, https://huggingface.co/Codec-SUPERB/datasets</li>
<li>Codec-SUPERB: An In-Depth Analysis of Sound Codec Models - arXiv, accessed July 16, 2025, https://arxiv.org/html/2402.13071v3</li>
<li>The open source code for LLM-Codec - GitHub, accessed July 16, 2025, https://github.com/yangdongchao/LLM-Codec</li>
<li>Dongchao Yang yangdongchao - GitHub, accessed July 16, 2025, https://github.com/yangdongchao</li>
<li>0417keito/SpeechTokenizer_trainer: Trainer of Speech Tokenizer(https://arxiv.org/abs/2308.16692) - GitHub, accessed July 16, 2025, https://github.com/0417keito/SpeechTokenizer_trainer</li>
<li>HILCodec: High-Fidelity and Lightweight Neural Audio Codec - arXiv, accessed July 16, 2025, https://arxiv.org/html/2405.04752v2</li>
<li>Core ML Models - Machine Learning - Apple Developer, accessed July 16, 2025, https://developer.apple.com/machine-learning/models/</li>
<li>Machine Learning &amp; AI - Apple Developer, accessed July 16, 2025, https://developer.apple.com/machine-learning/</li>
<li>Community list of startups working with AI in audio and music technology - GitHub, accessed July 16, 2025, https://github.com/csteinmetz1/ai-audio-startups</li>
<li>OpenAI Startup Fund’s Portfolio Company Improves RVQGAN: 90x Compression of 44.1 KHz Audio at 8kbps Bandwidth - Synced Review, accessed July 16, 2025, https://syncedreview.com/2023/06/21/openai-startup-funds-portfolio-company-improves-rvqgan-90x-compression-of-44-1-khz-audio-at-8kbps-bandwidth/</li>
<li>arXiv:2303.08005v1 [eess.AS] 14 Mar 2023, accessed July 16, 2025, https://arxiv.org/pdf/2303.08005</li>
<li>A Comprehensive Survey on Knowledge Distillation - OpenReview, accessed July 16, 2025, https://openreview.net/pdf/6a970f574d37071e1bbf61c5616567c5029bf63f.pdf</li>
<li>Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural audio… - OpenReview, accessed July 16, 2025, https://openreview.net/forum?id=V1ebluMI6f</li>
<li>HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis | OpenReview, accessed July 16, 2025, https://openreview.net/forum?id=868masI331</li>
<li>Yann LeCun’s Home Page, accessed July 16, 2025, http://yann.lecun.com/</li>
<li>Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI &amp; the Future of AI | Lex Fridman Podcast #416 - YouTube, accessed July 16, 2025, https://www.youtube.com/watch?v=5t1vTLU7s40</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>