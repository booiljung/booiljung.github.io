<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:PointRend (2020)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>PointRend (2020)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">이미지 분할 (Image Segmentations)</a> / <span>PointRend (2020)</span></nav>
                </div>
            </header>
            <article>
                <h1>PointRend (2020)</h1>
<h2>1.  PointRend의 개념적 기반: 렌더링으로서의 이미지 분할</h2>
<h3>1.1  기존 정규 그리드(Regular Grid) 방식의 근본적 한계</h3>
<p>현대 이미지 분할(Image Segmentation) 기술의 주류를 이루는 합성곱 신경망(Convolutional Neural Network, CNN) 기반 모델들은 근본적으로 정규 그리드(regular grid) 위에서 연산을 수행하도록 설계되었다.1 입력 이미지가 픽셀의 정규 그리드인 것처럼, 모델 내부의 특징 맵(feature map)과 최종적으로 출력되는 레이블 맵(label map) 역시 모두 정규 그리드 형태를 가진다.2 이러한 구조는 연산의 편의성을 제공하지만, 이미지 분할이라는 과제의 본질적 특성과 충돌하며 몇 가지 근본적인 한계를 드러낸다.</p>
<p>가장 핵심적인 문제는 계산 자원의 비효율적 분배이다. 이미지 분할에서 예측해야 할 레이블 맵은 대부분의 영역에서 평탄한(smooth) 특성을 보인다. 즉, 객체의 내부 영역에 속한 인접 픽셀들은 대부분 동일한 레이블을 공유한다. 반면, 객체와 객체, 또는 객체와 배경이 만나는 경계(boundary) 영역은 레이블이 급격하게 변하는 고주파(high-frequency) 특성을 띤다. 정규 그리드 방식은 이러한 공간적 정보량의 불균일성을 고려하지 않고 모든 픽셀에 동일한 계산량을 할당한다. 그 결과, 정보 밀도가 낮은 객체 내부의 평탄한 영역에서는 불필요한 계산을 반복하는 과잉 샘플링(oversampling)이 발생하고, 정작 정교한 예측이 필요한 복잡한 경계 영역에서는 세부 정보를 충분히 포착하지 못하는 과소 샘플링(undersampling) 문제가 발생한다.3</p>
<p>이러한 과소/과잉 샘플링의 이중적 문제는 분할 마스크의 품질 저하로 직결된다. 과소 샘플링된 경계는 흐릿하고 뭉툭하게 표현되며(blurry contours), 이는 흔히 ’계단 현상(staircasing effect)’으로 관찰된다.5 이 문제를 완화하기 위해 많은 모델들은 계산량과 성능 사이에서 타협점을 찾는다. 예를 들어, 시맨틱 분할(semantic segmentation)에서는 입력 해상도의 1/8 크기로, 인스턴스 분할(instance segmentation)에서는 28x28과 같은 고정된 저해상도로 예측을 수행한 뒤, 이를 다시 원본 크기로 확대하는 방식을 사용한다.1 이는 계산 효율성을 확보하기 위한 어쩔 수 없는 선택이지만, 근본적으로 경계의 정교함을 희생하는 결과를 초래한다.</p>
<h3>1.2  컴퓨터 그래픽스 렌더링 기법과의 유추</h3>
<p>PointRend는 이러한 정규 그리드 방식의 한계를 극복하기 위해, 이미지 분할이라는 문제를 고전 컴퓨터 그래픽스(computer graphics)의 렌더링(rendering) 문제로 재해석하는 혁신적인 관점을 제시한다.1 컴퓨터 그래픽스에서 3차원 모델을 2차원 이미지 평면에 고품질로 렌더링할 때, 모든 픽셀에 대해 동일한 계산을 수행하는 것은 매우 비효율적이다. 대신, 변화가 심한 영역, 예를 들어 객체의 실루엣이나 텍스처가 복잡한 부분에는 더 많은 계산을 집중하고, 색상이 일정한 평탄한 영역은 소수의 샘플링 포인트 값들을 보간(interpolation)하여 처리하는 적응형 세분화(adaptive subdivision) 기법이 오랫동안 사용되어 왔다.1</p>
<p>PointRend는 바로 이 아이디어에서 영감을 얻었다. 이미지 분할의 레이블 맵 역시 객체 내부에서는 평탄하고 경계에서는 급격하게 변하는 특성을 공유하므로, 렌더링과 동일한 원칙을 적용할 수 있다는 것이다. 이에 따라 PointRend는 전체 픽셀 그리드에 대해 조밀한 예측(dense prediction)을 수행하는 대신, 분할 결과가 불확실할 가능성이 높은, 즉 경계에 위치할 확률이 높은 소수의 지점들을 ‘적응적으로(adaptively)’ 선택하여 레이블을 계산하는 비균일 샘플링(non-uniform sampling) 전략을 채택한다.2</p>
<h3>1.3  이미지 분할을 렌더링 문제로 재정의: 핵심 통찰</h3>
<p>PointRend의 핵심 통찰은 이미지 분할을 “기저에 존재하는 연속적인 어떤 표현(underlying continuous representation)으로부터, 이산적인(discrete) 고해상도 레이블 맵을 ’렌더링’하는 과정“으로 재정의한 데에 있다.9 이는 분할 문제를 단순히 수많은 픽셀을 각각 분류하는 작업의 집합으로 보는 관점에서 벗어나, 이미지 평면 위의 임의의 실수 좌표 지점(point)에서의 값(레이블)을 추론하는 문제로 바라보는 근본적인 관점의 전환을 의미한다.1</p>
<p>이러한 접근법은 계산 자원을 가장 필요한 곳에 집중적으로 배분함으로써, 기존 방식으로는 메모리나 연산량의 한계로 인해 실용적으로 구현하기 어려웠던 고해상도의 정교한 분할 맵 생성을 가능하게 만드는 결정적인 열쇠가 되었다.2 PointRend의 진정한 혁신은 새로운 종류의 레이어나 복잡한 네트워크 구조를 제안한 것이 아니라, ’계산의 공간적 분배’에 대한 철학을 바꾼 것에 있다. 기존 CNN 모델들이 이미지의 모든 픽셀이 동등한 정보량을 가진다는 암묵적 가정하에 연산 밀도를 균일하게 적용했다면, PointRend는 이미지의 정보량이 공간적으로 불균일하다는 사실을 정면으로 마주한다. 객체 경계와 같이 정보 밀도가 높은 곳에 계산을 집중하고, 내부 평면과 같이 정보 밀도가 낮은 곳은 효율적으로 처리하는 ‘적응형 계산(adaptive computation)’ 패러다임을 분할 문제에 성공적으로 도입한 것이다. ’렌더링’이라는 유추는 이러한 패러다임을 구현하기 위한 강력하고 직관적인 프레임워크를 제공했다.</p>
<h2>2.  핵심 아키텍처 분석</h2>
<p>PointRend 모듈은 추상적으로 CNN 특징 맵을 입력받아, 선택된 특정 지점들에서 고해상도 예측을 출력하는 일반적인 구조를 가진다.1 이 과정은 크게 세 가지 핵심 구성 요소로 나뉜다: 1) 적응형 포인트 샘플링 전략, 2) 포인트별 특징 표현, 3) 포인트 헤드.1</p>
<h3>2.1  적응형 포인트 샘플링 전략</h3>
<p>PointRend의 효율성과 성능을 좌우하는 가장 중요한 요소는 계산이 필요한 지점을 지능적으로 선택하는 샘플링 전략이다.10 이 전략은 모델이 이미 잘 예측하고 있는 평탄한 영역에 대한 불필요한 계산을 피하고, 예측이 어려운 경계 영역에 계산 자원을 집중시킨다. 이 전략은 추론(inference) 단계와 학습(training) 단계에서 서로 다른 목표를 달성하기 위해 다르게 적용된다.3</p>
<h4>2.1.1  추론 단계: 반복적 세분화</h4>
<p>추론 단계의 목표는 이미 학습된 지식을 바탕으로 가장 불확실한 부분을 정밀하게 다듬어 최종 마스크의 품질을 극대화하는 것이다. 이를 위해 결정론적(deterministic)이고 반복적인 세분화 방식을 사용한다.3 이 과정은 저해상도(예: 7x7)의 거친 예측에서 시작하여 목표 해상도(예: 224x224)에 도달할 때까지 점진적으로 해상도를 높여나간다.10 각 단계는 다음과 같이 구성된다.</p>
<ol>
<li><strong>업샘플링</strong>: 현재의 저해상도 예측 맵을 쌍선형 보간법(bilinear interpolation)을 사용하여 2배로 업샘플링한다.3</li>
<li><strong>불확실한 포인트 선택</strong>: 업샘플링된 맵에서 가장 불확실한(uncertain) <code>N</code>개의 포인트를 선택한다. 이진 분할 마스크의 경우, 이는 예측 확률값이 0.5에 가장 가까운 지점들을 의미하며, 이들은 대부분 객체의 경계에 위치할 가능성이 높다.3</li>
<li><strong>포인트별 예측 및 업데이트</strong>: 선택된 <code>N</code>개의 포인트에 대해서만 포인트별 특징 표현을 추출하고, 포인트 헤드(MLP)를 통해 새로운 레이블을 예측한다. 이 예측값으로 해당 포인트의 값을 업데이트한다.</li>
<li><strong>반복</strong>: 위 과정을 목표 해상도에 도달할 때까지 반복한다.9</li>
</ol>
<p>이러한 반복적 세분화 전략은 매우 효율적이다. <code>M0 x M0</code> 해상도에서 시작하여 <code>M x M</code> 목표 해상도에 도달하기 위해 필요한 총 포인트 예측 횟수는 모든 픽셀을 계산하는 <span class="math math-inline">M^2</span>에 비해 훨씬 적은 <span class="math math-inline">N \log_2(M/M_0)</span>에 비례한다.9 예를 들어, 7x7 해상도에서 시작하여 224x224 해상도로 마스크를 생성할 경우, 전체 픽셀을 계산하는 방식에 비해 약 15배의 연산량 감소 효과를 얻을 수 있다.10</p>
<h4>2.1.2  학습 단계: 비반복적 확률 샘플링</h4>
<p>학습 단계의 목표는 모델이 경계 영역을 효과적으로 ’탐색’하고 학습하도록 유도하는 것이다. 이를 위해 추론과 달리 비반복적인 확률 기반 샘플링 방식을 사용하며, 이는 효율적인 탐색(exploration)과 정밀한 활용(exploitation) 사이의 균형을 맞추도록 설계되었다.9 이 전략은 세 가지 원칙을 결합하여 <code>N</code>개의 학습 포인트를 선정한다.</p>
<ol>
<li><strong>과잉 생성 (Overgeneration)</strong>: 먼저 균일 분포(uniform distribution)에서 <code>kN</code>개의 포인트를 무작위로 샘플링한다 (<code>k &gt; 1</code>, 논문에서는 <code>k=3</code>을 사용). 이는 이후 단계에서 선택할 충분하고 다양한 후보군을 확보하기 위한 과정이다.3</li>
<li><strong>중요도 샘플링 (Importance Sampling)</strong>: <code>kN</code>개의 후보 포인트 중에서, 모델의 거친 예측(coarse prediction)을 기반으로 가장 불확실한 <code>βN</code>개의 포인트를 선택한다 (<code>β</code>는 0과 1 사이의 값으로, 논문에서는 <code>β=0.75</code>를 사용). 이 과정은 모델이 학습하기 어려운 경계 영역에 집중하도록 유도하여 학습 효율을 높이는, 즉 ’활용’의 역할을 한다.3</li>
<li><strong>커버리지 (Coverage)</strong>: 나머지 <code>(1-β)N</code>개의 포인트는 다시 전체 영역에서 균일 분포로 샘플링한다. 이는 모델이 중요도 높은 경계 영역에만 과적합(overfitting)되는 것을 방지하고, 평탄한 영역을 포함한 전반적인 커버리지를 보장하여 모델의 일반화 성능을 유지하는, 즉 ’탐색’의 역할을 한다.3</li>
</ol>
<p>최종적으로 선택된 <code>N</code>개의 포인트에 대해서만 예측이 수행되고 손실(loss)이 계산되어 가중치 업데이트에 사용된다.9 이처럼 학습과 추론에서 각기 다른 목표에 맞춰 샘플링 전략을 이원화한 것은 PointRend가 고도로 설계된 시스템임을 보여주는 중요한 지점이다.</p>
<h3>2.2  포인트별 특징 표현</h3>
<p>적응적으로 선택된 각 포인트에 대해 정확한 예측을 수행하기 위해서는 풍부한 정보를 담은 특징 벡터가 필요하다. PointRend는 이를 위해 두 가지 종류의 특징, 즉 ’세분화된 특징’과 ’거친 예측 특징’을 결합하는 방식을 사용한다.1 이는 현대 분할 모델에서 고질적으로 나타나는 ’저수준 디테일’과 ‘고수준 시맨틱’ 정보 간의 상충 관계(trade-off)를 개별 포인트 수준에서 해결하려는 정교한 접근법이다.</p>
<h4>2.2.1  세분화된 특징 (Fine-grained Features)</h4>
<p>세분화된 특징의 주된 목적은 마스크의 세밀한 디테일, 즉 정교한 경계를 표현하는 데 필요한 저수준의 공간 정보를 제공하는 것이다.13 CNN 아키텍처에서 얕은 층의 특징 맵은 높은 공간 해상도를 유지하고 있어 픽셀 수준의 정확한 위치와 텍스처 정보를 풍부하게 담고 있다. PointRend는 이러한 특징 맵(예: ResNet의 <code>res2</code> 또는 FPN의 <code>P2</code> 출력)에서 특징을 추출한다.3</p>
<p>샘플링된 포인트의 좌표는 실수 값(<code>(x, y)</code>)이므로, 정수 그리드에 위치한 특징 맵에서 정확한 특징 벡터를 얻기 위해 쌍선형 보간법(bilinear interpolation)이 사용된다. 이 방법은 해당 포인트 주변의 가장 가까운 4개 픽셀의 특징 벡터 값을 거리에 따라 가중 평균하여, 서브 픽셀(sub-pixel) 수준의 정밀한 특징 표현을 계산한다.10 특징 맵 <span class="math math-inline">f</span>와 실수 좌표 <span class="math math-inline">(x, y)</span>가 주어졌을 때, 보간된 특징 <span class="math math-inline">f(x, y)</span>는 다음과 같이 근사적으로 계산된다.<br />
<span class="math math-display">
f(x, y) \approx \sum_{i=0}^{1}\sum_{j=0}^{1} w(x_i, x) w(y_j, y) f(x_i, y_j)
</span><br />
여기서 <span class="math math-inline">(x_i, y_j)</span>는 <span class="math math-inline">(x, y)</span>를 둘러싼 4개의 정수 그리드 좌표이며, <span class="math math-inline">w(a, b) = 1 - \vert a - b \vert</span>는 선형 보간 가중치를 나타낸다.14</p>
<h4>2.2.2  거친 예측 특징 (Coarse Prediction Features)</h4>
<p>세분화된 특징만으로는 두 가지 중요한 정보가 부족하다. 첫째, 이 특징들은 지역적인 시각 정보만을 담고 있어, 해당 포인트가 어떤 객체에 속하는지에 대한 전체적인 문맥이나 의미론적(semantic) 정보를 충분히 제공하지 못한다. 둘째, 인스턴스 분할의 경우, 동일한 좌표에 여러 객체의 경계 상자(bounding box)가 겹칠 수 있는데, 세분화된 특징만으로는 이들을 구분할 수 없다. 즉, ’영역별 정보(region-specific information)’가 부재한다.9</p>
<p>이러한 한계를 보완하기 위해 PointRend는 ’거친 예측 특징’을 사용한다. 이는 기존 분할 모델의 헤드에서 생성된 저해상도 예측 결과를 의미한다. 예를 들어, Mask R-CNN의 경우 7x7 크기의 마스크 헤드 출력이 이에 해당한다.3 이 저해상도 예측 맵은 이미 전체 객체에 대한 고수준의 시맨틱 정보를 압축적으로 담고 있으며, 각 채널은 특정 클래스에 대한 예측 확률을 나타낸다. 세분화된 특징과 마찬가지로, 쌍선형 보간법을 통해 해당 포인트의 위치에서 <code>K</code>-차원(클래스 개수) 예측 벡터를 추출하여 특징으로 사용한다.1</p>
<p>이 두 종류의 특징, 즉 저수준의 정교한 공간 정보를 담은 ’세분화된 특징’과 고수준의 의미론적 및 영역별 정보를 담은 ’거친 예측 특징’을 연결(concatenation)함으로써, 각 포인트는 예측에 필요한 풍부하고 다각적인 정보를 담은 최종 특징 표현을 갖게 된다.10</p>
<h3>2.3  포인트 헤드: MLP 기반 예측</h3>
<p>최종적으로 구성된 포인트별 특징 표현은 ’포인트 헤드(Point Head)’라고 불리는 간단한 신경망에 입력되어 해당 포인트의 최종 레이블을 예측한다.1 포인트 헤드는 일반적으로 다층 퍼셉트론(Multi-Layer Perceptron, MLP)으로 구현된다.10</p>
<p>논문에서 제안된 구조는 3개의 은닉층(hidden layer)을 가지며, 각 층은 256개의 채널(뉴런)으로 구성된 경량 MLP이다.10 이 MLP는 모든 포인트와 모든 관심 영역(Region of Interest, RoI)에 걸쳐 가중치를 공유(weight sharing)하는 방식으로 설계되었다. 이는 파라미터 수를 크게 줄여 모델을 효율적으로 만들고, 각 포인트의 예측이 위치에 무관하게 동일한 방식으로 처리되도록 보장한다.13 포인트 헤드는 각 포인트를 독립적으로 분류하는 역할을 수행하며, 인스턴스 분할의 경우 이진 교차 엔트로피(binary cross-entropy), 시맨틱 분할의 경우 다중 클래스 교차 엔트로피(multi-class cross-entropy)와 같은 표준 분할 손실 함수를 통해 end-to-end 방식으로 학습된다.13</p>
<h2>3.  기존 분할 프레임워크와의 통합</h2>
<p>PointRend의 가장 큰 장점 중 하나는 특정 모델에 종속되지 않고 다양한 기존 분할 프레임워크에 유연하게 통합될 수 있는 모듈(module)이라는 점이다.1 이는 연구자들이나 개발자들이 기존의 잘 구축된 모델 아키텍처를 크게 변경하지 않고도, PointRend를 “플러그 앤 플레이(plug-and-play)” 방식으로 추가하여 분할 품질을 향상시킬 수 있음을 의미한다.16</p>
<h3>3.1  인스턴스 분할: Mask R-CNN과의 결합</h3>
<p>PointRend는 인스턴스 분할의 대표적인 모델인 Mask R-CNN과 결합될 때 가장 극적인 효과를 보여준다.9 통합은 Mask R-CNN의 표준 마스크 헤드(mask head)를 PointRend 모듈로 대체하는 방식으로 이루어진다.2</p>
<p>기존 Mask R-CNN의 마스크 헤드는 일반적으로 4개의 3x3 합성곱 레이어 스택으로 구성되며, RoIAlign을 통해 추출된 14x14 크기의 특징 맵을 입력받아 최종적으로 28x28 해상도의 저해상도 마스크를 출력한다.2 이 저해상도 마스크를 업샘플링하는 과정에서 경계의 세부 정보가 손실되고 계단 현상이 발생하는 문제가 있었다.</p>
<p>PointRend는 이 컨볼루션 스택을 완전히 대체한다. 대신, 훨씬 가벼운 헤드(예: 2개의 컨볼루션 레이어)를 사용하여 7x7과 같은 매우 낮은 해상도의 ’거친 예측 특징’만을 생성한다.1 그 후, 이 7x7 예측과 FPN(Feature Pyramid Network)의 고해상도 특징 맵(일반적으로 가장 해상도가 높은 <code>P2</code>)을 입력으로 받아, 앞서 설명한 반복적 세분화 과정을 수행한다. 이 과정을 통해 최종적으로 224x224와 같이 훨씬 더 높은 해상도의 정교한 마스크를 ’렌더링’해낸다.1</p>
<p>이러한 통합 방식은 ’무엇을 분할할 것인가(What to segment)’와 ’어떻게 정교하게 분할할 것인가(How to segment precisely)’라는 두 가지 문제를 명확히 분리하는 설계 철학을 보여준다. Mask R-CNN의 나머지 부분(백본, RPN, 박스 헤드)은 객체를 탐지하고 대략적인 위치를 잡는 ’무엇’의 역할을 수행하고, PointRend 모듈은 그 결과를 받아 정교한 마스크를 생성하는 ’어떻게’의 역할을 전문적으로 담당한다. 이는 분할의 ‘정제(refinement)’ 단계를 독립적인 전문 작업으로 격상시키는 계기가 되었으며, 이후 경계 정제를 전문으로 하는 다양한 후속 연구들의 이론적 기반을 마련했다.</p>
<h3>3.2  시맨틱 분할: FCN 계열 모델과의 결합</h3>
<p>PointRend의 유연성은 인스턴스 분할에만 국한되지 않는다. DeepLabv3나 SemanticFPN과 같은 FCN(Fully Convolutional Network) 기반의 시맨틱 분할 모델에도 성공적으로 적용될 수 있다.1</p>
<p>FCN 기반 모델들은 일반적으로 입력 이미지보다 8배 또는 16배 축소된 저해상도 특징 맵에서 최종 픽셀별 클래스 예측을 수행한다. 그리고 마지막 단계에서 간단한 쌍선형 보간법을 사용하여 이 저해상도 예측 맵을 원본 이미지 크기로 한 번에 업샘플링한다.1 이 과정에서 특히 가늘고 긴 구조물(thin structures)이나 복잡한 경계의 세부 정보가 손실되는 문제가 발생한다.</p>
<p>PointRend는 바로 이 마지막 업샘플링 단계를 대체한다. FCN 모델이 출력한 저해상도 예측 맵을 ’거친 예측 특징’으로, 그리고 CNN 백본의 초기 레이어에서 나온 고해상도 특징 맵을 ’세분화된 특징’으로 활용한다. 이를 바탕으로 전체 이미지에 대해 포인트 샘플링과 반복적 세분화 과정을 적용하여 최종 고해상도 시맨틱 맵을 생성한다. 이 방식을 통해, 기존 모델들이 어려움을 겪었던 전봇대, 가로등, 자전거 바퀴살과 같은 얇고 긴 구조물의 분할 품질을 눈에 띄게 향상시킬 수 있다.16</p>
<h2>4.  성능 평가: 정량적 및 정성적 분석</h2>
<p>PointRend는 주요 이미지 분할 벤치마크에서 기존의 강력한 베이스라인 모델들을 능가하는 유의미한 성능 향상을 정량적 및 정성적으로 입증했다.1</p>
<h3>4.1  정량적 성과: COCO 및 Cityscapes 벤치마크</h3>
<p>PointRend의 효과는 COCO 데이터셋을 사용한 인스턴스 분할과 Cityscapes 데이터셋을 사용한 시맨틱 분할에서 정량적으로 검증되었다.2</p>
<p><strong>테이블 1: COCO 데이터셋 인스턴스 분할 성능 (Mask AP)</strong></p>
<p>아래 표는 COCO <code>val2017</code> 데이터셋에서 PointRend를 적용한 Mask R-CNN과 표준 Mask R-CNN의 성능을 비교한 것이다. Mask AP(Average Precision)는 분할 마스크의 품질을 평가하는 핵심 지표이다.</p>
<table><thead><tr><th>Mask Head</th><th>Backbone</th><th>Schedule</th><th>Output Resolution</th><th>Mask AP</th></tr></thead><tbody>
<tr><td>Standard (4x conv)</td><td>R50-FPN</td><td>1x</td><td>28x28</td><td>34.8</td></tr>
<tr><td>PointRend</td><td>R50-FPN</td><td>1x</td><td>224x224</td><td>36.2 (+1.4)</td></tr>
<tr><td>Standard (4x conv)</td><td>R101-FPN</td><td>3x</td><td>28x28</td><td>38.8</td></tr>
<tr><td>PointRend</td><td>R101-FPN</td><td>3x</td><td>224x224</td><td>40.1 (+1.3)</td></tr>
<tr><td>Standard (4x conv)</td><td>X101-FPN</td><td>3x</td><td>28x28</td><td>39.5</td></tr>
<tr><td>PointRend</td><td>X101-FPN</td><td>3x</td><td>224x224</td><td>41.1 (+1.6)</td></tr>
</tbody></table>
<p>데이터 출처:.21 성능 수치는 보고된 값들 중 대표적인 값을 기준으로 함.</p>
<p>표에서 볼 수 있듯이, PointRend는 다양한 백본 아키텍처(ResNet-50, ResNet-101, ResNeXt-101)와 학습 스케줄에 걸쳐 일관되게 표준 마스크 헤드보다 높은 Mask AP를 달성했다. 이는 PointRend가 특정 조건에만 국한되지 않는 견고하고 일반적인 성능 향상을 제공함을 보여준다.</p>
<p><strong>테이블 2: Cityscapes 데이터셋 시맨틱 분할 성능 (mIoU)</strong></p>
<p>시맨틱 분할 성능은 Cityscapes <code>val</code> 데이터셋에서 mIoU(mean Intersection over Union)를 통해 평가되었다. 특히, PointRend의 핵심 주장인 ’경계 품질 개선’을 직접적으로 검증하기 위해, 객체 경계 주변의 좁은 띠(trimap) 영역에서의 mIoU를 추가로 측정했다.</p>
<table><thead><tr><th>Method</th><th>Backbone</th><th>mIoU</th><th>Trimap mIoU (8px)</th></tr></thead><tbody>
<tr><td>DeeplabV3</td><td>ResNet-101</td><td>77.2</td><td>42.4</td></tr>
<tr><td>DeeplabV3 + PointRend</td><td>ResNet-101</td><td>78.4 (+1.2)</td><td>47.3 (+4.9)</td></tr>
<tr><td>SemanticFPN</td><td>ResNet-101</td><td>77.7</td><td>47.0</td></tr>
<tr><td>SemanticFPN + PointRend</td><td>ResNet-101</td><td>78.6 (+0.9)</td><td>48.6 (+1.6)</td></tr>
</tbody></table>
<p>데이터 출처:.22</p>
<p>이 결과는 매우 중요한 점을 시사한다. 전체 이미지에 대한 mIoU 증가는 약 1점 내외로 나타나지만, 8픽셀 폭의 좁은 경계 영역(trimap)에서는 mIoU가 최대 4.9점까지 크게 향상되었다. 이는 IoU 기반의 표준 평가 지표가 픽셀 수가 훨씬 많은 객체 내부에 편향되어 있어 경계 품질의 개선을 제대로 반영하지 못하는 한계를 가지고 있음을 보여준다.2 PointRend의 진정한 기여는 표준 메트릭 상의 수치보다 훨씬 크며, 이는 ’좋은 분할이란 무엇인가’에 대한 평가 기준 자체에 대한 논의를 촉발시키는 계기가 되었다.</p>
<h3>4.2  정성적 개선: 선명한 객체 경계 생성</h3>
<p>정성적 결과는 PointRend의 가치를 가장 직관적으로 보여준다. 기존 방법들이 과도하게 평활화(over-smoothed)하여 세부 정보를 잃어버렸던 영역에서, PointRend는 매우 선명하고(crisp) 정교한 객체 경계를 성공적으로 복원한다.1</p>
<p>특히 사람의 손가락 사이, 동물의 발, 가느다란 가구 다리, 자전거 바퀴살과 같이 복잡하고 세밀한 구조를 가진 객체에서 그 효과가 극명하게 드러난다.5 Mask R-CNN의 28x28 저해상도 출력에서 나타나는 뭉툭하고 각진 경계와 PointRend가 생성한 224x224 고해상도 출력의 부드럽고 정교한 경계를 시각적으로 비교하면, 그 차이를 명확히 확인할 수 있다.2</p>
<h3>4.3  계산 효율성 분석</h3>
<p>PointRend의 또 다른 핵심적인 기여는 높은 품질의 마스크를 매우 효율적인 방식으로 생성한다는 점이다.5 고품질 마스크를 위해 필수적인 고해상도 예측을 기존의 조밀한(dense) CNN 방식으로 수행할 경우, 연산량과 메모리 요구량이 기하급수적으로 증가하여 실용성이 떨어진다.1</p>
<p>PointRend는 적응형 샘플링 전략을 통해 이 문제를 해결한다. 224x224 해상도의 마스크를 생성하는 경우, 모든 픽셀을 계산하는 표준 4-컨볼루션 스택 마스크 헤드와 비교했을 때, PointRend는 부동소수점 연산(FLOPs)과 메모리 사용량 측면에서 30분의 1 미만의 비용만을 소모한다.10</p>
<p>이러한 효율성은 단순히 ’빠르다’는 부가적인 이점을 넘어, ’고품질’을 달성하기 위한 전제 조건으로서의 의미를 가진다. 효율성이 없었다면 고해상도 예측 자체가 실용적으로 불가능했을 것이므로, PointRend의 효율성과 품질은 분리할 수 없는 동전의 양면과 같다. 이 덕분에 메모리나 계산 자원의 제약이 심한 환경에서도 고해상도 출력을 실용적으로 생성할 수 있는 길이 열렸다.5</p>
<h2>5.  응용 분야 및 확장 모델</h2>
<p>PointRend가 제시한 적응형 샘플링과 경계 정제라는 강력한 아이디어는 일반적인 객체 분할을 넘어 다양한 전문 분야로 빠르게 확산되었으며, 동시에 원본 모델의 한계를 극복하기 위한 여러 확장 모델의 등장을 촉진했다.</p>
<h3>5.1  주요 응용 사례: 의료, 위성, 산업 분야</h3>
<p>PointRend의 정교한 경계 표현 능력은 특히 복잡하고 불분명한 경계를 다루어야 하는 전문 도메인에서 큰 가치를 발휘했다.</p>
<ul>
<li><strong>의료 영상 분할 (Medical Image Segmentation)</strong>: 의료 영상은 본질적으로 경계가 모호하고 구조가 복잡한 경우가 많다. PointRend는 현미경 이미지에서의 세포 분할 27, 내시경 이미지에서의 용종(polyp) 탐지 29, MRI 이미지에서의 종양 분할 등에서 U-Net과 같은 기존 의료 영상 분할 모델과 결합되어 분할의 정밀도를 크게 향상시키는 데 사용되었다. 나아가 PointRend의 ‘분할을 렌더링으로 보는’ 철학은 MORSE와 같은 새로운 의료 영상 렌더링 기반 분할 프레임워크에 직접적인 영감을 주었다.12</li>
<li><strong>위성 및 항공 이미지 분석 (Satellite and Aerial Image Analysis)</strong>: 고해상도 원격 탐사 이미지에서 도시 계획, 재난 관리 등에 필수적인 건물 외곽선을 정밀하게 추출하는 작업에 성공적으로 적용되었다. PointRend를 기반으로 건물 크기에 따라 파라미터를 조절하는 AP-PointRend와 같은 모델이 개발되어 이 분야의 정확도를 한 단계 끌어올렸다.30</li>
<li><strong>산업 분야 (Industrial Applications)</strong>: 반도체 제조 공정에서 웨이퍼의 미세 결함을 정밀하게 식별하고 분할하는 것은 수율 관리에 매우 중요하다. SEMI-PointRend와 같은 연구에서는 PointRend를 적용하여 기존 Mask R-CNN보다 훨씬 우수한 결함 분할 성능을 달성했다.33</li>
</ul>
<p>이 외에도 빽빽하게 가려진 객체가 등장하는 비디오에서의 인스턴스 추적 35, 지질학적 분석을 위한 암석 균열 탐지 35 등 다양한 분야에서 PointRend의 아이디어가 활발히 응용되고 있다.</p>
<h3>5.2  한계점 및 개선 방안: AP-PointRend</h3>
<p>원본 PointRend는 뛰어난 성능에도 불구하고, 특정 조건에서는 한계를 보였다. 특히 위성 이미지와 같이 동일 이미지 내에 객체의 크기 편차가 극심한 경우, 고정된 샘플링 파라미터(<code>N</code>, <code>k</code>)를 사용하는 방식은 문제가 되었다. 작은 건물에 최적화된 파라미터는 거대한 건물의 복잡한 경계를 제대로 표현하지 못했고, 반대로 큰 건물에 맞추면 작은 건물에서 비효율이 발생했다. 또한, 이 과정에서 의미 없는 작은 이산적 패치(discrete patches)들이 생성되는 부작용도 관찰되었다.30</p>
<p>이러한 문제를 해결하기 위해 **AP-PointRend (Adaptive Parameter-PointRend)**가 제안되었다.30 AP-PointRend는 두 가지 핵심적인 개선점을 도입했다.</p>
<ol>
<li><strong>적응형 파라미터 선택 (Adaptive Parameter Selection)</strong>: 탐지된 객체의 크기(예: 경계 둘레)를 기반으로 반복 횟수와 샘플링할 포인트 수를 동적으로 조절한다. 이를 통해 모델은 객체의 스케일에 맞춰 최적의 계산량을 할당하여, 대규모 객체의 분할 정밀도를 크게 향상시켰다.</li>
<li><strong>정규화 제약 조건 (Regularization Constraints)</strong>: 반복적인 정제 과정에서 일정 크기 임계값 이하의 작은 패치들을 제거하는 정규화 단계를 추가했다. 이를 통해 이산적 패치 문제를 효과적으로 해결하고, 최종 마스크의 시각적 품질과 정확도를 높였다.</li>
</ol>
<h3>5.3  약지도 학습으로의 확장: Implicit PointRend</h3>
<p>이미지 분할 연구의 또 다른 주요 과제는 막대한 레이블링 비용이다. 픽셀 단위의 정교한 마스크를 제작하는 것은 시간과 비용이 많이 소요되므로, 소수의 포인트 클릭만으로 모델을 학습시키는 약지도 학습(weakly-supervised learning)에 대한 관심이 높아졌다.36</p>
<p>그러나 원본 PointRend를 포인트 기반 지도 학습에 직접 적용했을 때, 한 가지 문제점이 발견되었다. PointRend의 구조에 포함된 저해상도(7x7) 거친 마스크 헤드는 희소한(sparse) 포인트 레이블만으로는 안정적인 학습 신호를 받기 어려워, 오히려 전체 모델의 성능을 저하시키는 요인으로 작용했다.36</p>
<p>이러한 문제에 대한 해결책으로 <strong>Implicit PointRend</strong>가 제안되었다.36 Implicit PointRend는 포인트 기반 지도 학습에 최적화하기 위해 다음과 같이 구조를 변경했다.</p>
<ol>
<li><strong>거친 마스크 헤드 제거</strong>: 불안정한 학습 신호의 원인이었던 저해상도 거친 마스크 헤드와 그에 해당하는 손실 함수를 과감히 제거했다.</li>
<li><strong>암시적 함수 (Implicit Function) 도입</strong>: 대신, 각 객체에 대해 포인트 헤드(MLP)의 파라미터를 직접 예측하는 구조를 채택했다. 이 MLP는 특정 포인트의 좌표와 해당 위치의 이미지 특징을 입력받아 직접 마스크 값을 예측하는 ‘암시적 함수’ 역할을 한다.</li>
<li><strong>단일 손실 함수 사용</strong>: 두 개의 분리된 마스크 손실 함수 대신, 최종 포인트 레벨 예측에 대한 단일 손실 함수만을 사용하여 전체 구조를 단순화하고 포인트 기반 지도 신호가 모델에 직접적으로 전달되도록 했다.</li>
</ol>
<p>AP-PointRend와 Implicit PointRend의 등장은 PointRend가 단일 고정 모델을 넘어, 특정 문제(스케일 변화, 레이블링 비용)에 맞춰 유연하게 변형될 수 있는 ’적응형 샘플링 프레임워크’로서의 가치를 지니고 있음을 명확히 보여준다. 이는 PointRend의 핵심 아이디어가 특정 구현에 묶여 있는 것이 아니라, 다양한 문제 도메인과 학습 조건에 맞춰 변주될 수 있는 강력하고 일반적인 원리임을 증명한다.</p>
<h2>6.  후속 연구 및 패러다임 변화 속에서의 PointRend</h2>
<p>PointRend는 이미지 분할, 특히 고품질 경계 정제 분야 연구의 중요한 기폭제 역할을 했다. 그 영향력은 직접적인 후속 연구뿐만 아니라, 이후 등장한 새로운 아키텍처 패러다임과의 관계 속에서도 찾아볼 수 있다.</p>
<h3>6.1  경계 정제 기법의 발전: RefineMask와 Mask Transfiner</h3>
<p>PointRend의 성공은 분할 마스크의 품질을 더욱 향상시키기 위한 다양한 정제(refinement) 기법 연구를 촉진했다.39</p>
<ul>
<li><strong>RefineMask</strong>: PointRend가 각 포인트를 독립적으로 처리하기 때문에 주변의 문맥(context) 정보를 충분히 활용하지 못한다는 한계를 지적하며 등장했다.40 RefineMask는 다단계(multi-stage) CNN 기반의 정제 방식을 제안한다. ’Semantic Fusion Module (SFM)’을 통해 고해상도 시맨틱 특징과 인스턴스 특징을 융합하고, ‘Boundary-Aware Refinement (BAR)’ 전략으로 명시적으로 경계 영역에 집중한다.41 이는 PointRend의 MLP 기반 독립적 포인트 처리 방식에서 한 단계 나아가, CNN을 통해 지역적 문맥 정보를 적극적으로 활용하는 방식으로 발전했음을 의미한다.</li>
<li><strong>Mask Transfiner</strong>: 여기서 더 나아가 Transformer 아키텍처를 정제 과정에 도입했다. Mask Transfiner는 에러가 발생하기 쉬운 희소한(sparse) 영역을 쿼드트리(quadtree)라는 계층적 자료구조로 효율적으로 표현한다. 그리고 Transformer의 핵심인 어텐션(attention) 메커니즘을 이용해 이들 희소한 포인트들 간의 장거리 의존성(long-range dependency)을 모델링하여 오류를 병렬적으로 수정한다.42 이는 PointRend의 독립적인 포인트 처리나 RefineMask의 지역적 문맥 처리를 넘어, 전역적인 관계성까지 고려하는 정제 방식으로의 진화를 보여준다.</li>
</ul>
<p>이러한 연구들의 흐름은 PointRend가 제시한 ’경계 정제’라는 문제의식을 바탕으로, ’어떻게 더 나은 문맥 정보를 활용할 것인가’라는 질문에 대해 MLP에서 CNN, 그리고 Transformer로 이어지는 기술적 발전을 이끌었음을 보여준다.</p>
<h3>6.2  파운데이션 모델의 등장: SAM과의 비교</h3>
<p>2023년 발표된 Segment Anything Model (SAM)은 분할 분야에 근본적인 패러다임의 전환을 가져왔다.43 PointRend와 SAM은 모두 ’포인트’를 핵심적인 요소로 사용하지만, 그 역할과 철학, 그리고 기술적 기반은 완전히 다르다. 이 둘을 비교하는 것은 지난 몇 년간 이미지 분할 분야에서 일어난 패러다임의 변화를 상징적으로 보여준다.</p>
<h4>6.2.1  작업 패러다임의 근본적 차이</h4>
<p>가장 큰 차이는 모델이 지향하는 목표에 있다. PointRend는 주어진 특정 태스크(인스턴스 또는 시맨틱 분할) 내에서 최고의 성능을 내기 위해 학습된 <strong>‘전문가(specialist)’ 모델</strong>이다. 그 목표는 주어진 클래스에 대해 가장 정확하고 고품질의 마스크를 생성하는 것이다.1</p>
<p>반면, SAM은 특정 태스크나 클래스에 국한되지 않는 **‘일반화(generalist)’ 파운데이션 모델(foundation model)**이다. SAM의 목표는 사용자가 제공하는 다양한 형태의 프롬프트(prompt)에 따라, 이전에 본 적 없는 객체라도 분할해낼 수 있는 ‘제로샷(zero-shot)’ 일반화 능력을 갖추는 것이다.46 이는 모델의 역할을 정해진 정답을 찾는 ‘인식(Recognition)’ 시스템에서, 사용자의 의도를 이해하고 그에 맞는 결과를 생성하는 ‘상호작용(Interaction)’ 시스템으로 전환시켰다.</p>
<h4>6.2.2  ’포인트’의 역할 비교</h4>
<p>이러한 패러다임의 차이는 ’포인트’의 역할에서 극명하게 드러난다.</p>
<ul>
<li><strong>PointRend의 포인트</strong>: 모델 **‘내부’**에서 생성되는 <strong>계산의 단위</strong>이다. 모델이 스스로 예측이 불확실한 지점을 ’선택’하여 계산을 집중하기 위한 내부적인 알고리즘의 일부이다.9 사용자는 이 과정에 개입하지 않는다.</li>
<li><strong>SAM의 포인트</strong>: 사용자 또는 다른 시스템이 **‘외부’**에서 제공하는 **‘프롬프트(prompt)’**이다. 분할하고자 하는 대상을 지정하기 위한 사용자 상호작용의 수단이며, 긍정적(foreground) 또는 부정적(background) 지점을 나타낸다.47</li>
</ul>
<p>즉, PointRend의 포인트는 모델의 ’판단’의 결과물인 반면, SAM의 포인트는 외부의 ’의도’를 전달하는 입력인 것이다.</p>
<h4>6.2.3  아키텍처 비교</h4>
<p>두 모델은 기반이 되는 아키텍처 또한 완전히 다르다.</p>
<ul>
<li><strong>PointRend</strong>: CNN 기반 아키텍처(예: Mask R-CNN)의 일부를 대체하거나 보강하는 경량 MLP 기반의 <strong>모듈</strong>이다.1</li>
<li><strong>SAM</strong>: 거대한 Vision Transformer (ViT) 기반의 이미지 인코더, 다양한 프롬프트를 처리하는 프롬프트 인코더, 그리고 실시간 상호작용을 위한 경량 마스크 디코더로 구성된, 완전히 새로운 개념의 <strong>독립적인 대규모 아키텍처</strong>이다.51</li>
</ul>
<h4>6.2.4  품질과 일반화의 상충: HQ-SAM의 등장</h4>
<p>SAM은 놀라운 제로샷 일반화 능력을 보여주었지만, 그 대가로 마스크 경계의 정교함은 PointRend와 같은 전문가 모델에 비해 떨어지는 경향을 보였다.53 이는 일반화 성능과 특정 태스크에서의 최고 품질 사이에 존재하는 상충 관계를 보여준다.</p>
<p>이러한 간극을 메우기 위해 **HQ-SAM (High-Quality SAM)**이 제안되었다.53 HQ-SAM은 SAM의 강력한 일반화 능력과 프롬프트 기반 상호작용을 그대로 유지하면서, PointRend와 유사한 아이디어를 통해 경계 품질을 향상시킨다. 구체적으로, SAM의 ViT 인코더의 초기 레이어(저수준, 지역적 정보)와 최종 레이어(고수준, 전역적 정보)의 특징을 융합하고(Global-local Feature Fusion), 고품질 마스크 예측을 전담하는 새로운 ’HQ-Output Token’을 마스크 디코더에 주입한다.58</p>
<p>흥미롭게도, HQ-SAM의 ‘Global-local Feature Fusion’ 아이디어는 PointRend가 ’세분화된 특징(저수준)’과 ’거친 예측 특징(고수준)’을 결합한 핵심 원리와 정확히 일치한다. 이는 아키텍처가 CNN에서 Transformer로 진화했음에도 불구하고, ’정확한 경계를 위해서는 저수준의 공간 정보와 고수준의 시맨틱 정보가 모두 필요하다’는 PointRend의 근본적인 통찰이 여전히 유효하며, 새로운 패러다임 속에서 재해석되고 있음을 보여주는 강력한 증거이다.</p>
<h2>7.  결론: PointRend의 유산과 미래 전망</h2>
<h3>7.1  PointRend의 핵심 기여 요약</h3>
<p>PointRend는 이미지 분할 분야에 몇 가지 중요한 기여를 남겼다. 첫째, 이미지 분할을 컴퓨터 그래픽스의 렌더링 문제로 재해석함으로써, 계산 효율성과 경계 품질이라는 두 마리 토끼를 동시에 잡는 새로운 패러다임의 가능성을 열었다. 둘째, ’적응형 포인트 샘플링’이라는 강력하고 유연한 프레임워크를 제시하여, 이후 다양한 모델과 문제에 적용되고 변형될 수 있는 풍부한 연구의 토대를 마련했다. 셋째, 기존의 강력한 분할 모델에 손쉽게 통합되어 성능을 극대화할 수 있는 ‘플러그 앤 플레이’ 모듈화 접근법의 성공적인 사례를 보여주었다.</p>
<h3>7.2  분할 기술 발전에서의 위치와 영향</h3>
<p>PointRend는 고품질 인스턴스 분할, 특히 경계 정제(boundary refinement)라는 세부 연구 분야의 중요한 기폭제 역할을 했다. RefineMask, Mask Transfiner와 같은 직접적인 후속 연구들은 PointRend가 제기한 문제의식을 바탕으로, 더 정교한 문맥 정보를 활용하는 방향으로 기술을 발전시켰다. 또한, ‘포인트 기반’ 접근법의 효율성과 잠재력을 입증함으로써, Mask2Former와 같은 후속 Transformer 기반 모델들이 학습 효율을 높이기 위해 포인트 기반 손실 계산을 차용하거나, Implicit PointRend와 같이 약지도 학습으로의 길을 여는 등 새로운 연구 방향을 제시하는 데 기여했다.</p>
<h3>7.3  미래 전망: 파운데이션 모델 시대의 경계 정제</h3>
<p>SAM과 같은 파운데이션 모델의 등장은 분할 분야의 지형을 바꾸어 놓았다. 이제는 단일 태스크에서의 최고 성능보다는, 다양한 태스크와 데이터에 대한 일반화 능력이 더 중요한 가치로 부상하고 있다. 이러한 변화 속에서 PointRend의 유산은 새로운 방식으로 계승될 것으로 전망된다.</p>
<p>미래의 분할 시스템은 SAM과 같은 파운데이션 모델이 범용적인 초기 분할을 수행하고, PointRend의 철학을 계승한 HQ-SAM, SAM-Refiner 61와 같은 전문가 모듈이 그 결과를 사용자의 요구에 맞게 정교하게 다듬는 하이브리드 형태로 발전할 가능성이 높다. 즉, ’일반화’와 ’정밀화’의 역할 분담이 이루어지는 것이다.</p>
<p>또한, PointRend의 핵심 철학인 ’적응형 계산’은 비단 이미지 분할에만 국한되지 않는다. 계산 비용이 많이 드는 비디오 처리, 3차원 포인트 클라우드 분석, 의료 볼륨 데이터 처리 등 다른 조밀한 예측(dense prediction) 과제들에서도, 정보 밀도가 높은 영역에 계산 자원을 집중하는 PointRend의 접근법은 앞으로도 오랫동안 유효한 영감을 제공할 것이다. PointRend는 특정 모델의 이름을 넘어, 효율적이고 정교한 예측을 위한 중요한 원칙으로 컴퓨터 비전 분야에 깊이 각인되었다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>PointRend: Image Segmentation As Rendering - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2020/papers/Kirillov_PointRend_Image_Segmentation_As_Rendering_CVPR_2020_paper.pdf</li>
<li>(PDF) PointRend: Image Segmentation As Rendering (2020) | Alexander Kirillov - SciSpace, https://scispace.com/papers/pointrend-image-segmentation-as-rendering-nsw2r3do7l</li>
<li>PointRend: Image Segmentation as Rendering | Learning-Deep-Learning, https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/pointrend.html</li>
<li>PointRend: Image Segmentation as Rendering - 5cents, https://hammer-wang.github.io/5cents/representation-learning/pointrend/</li>
<li>Using a classical rendering technique for state of the art image segmentation - AI at Meta, https://ai.meta.com/blog/using-a-classical-rendering-technique-to-push-state-of-the-art-for-image-segmentation/</li>
<li>Facebook PointRend: Rendering Image Segmentation | by Synced - Medium, https://medium.com/syncedreview/facebook-pointrend-rendering-image-segmentation-f3936d50e7f1</li>
<li>[1912.08193] PointRend: Image Segmentation as Rendering - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/1912.08193</li>
<li>Inaccurate masks with Mask-RCNN: Stairs effect and sudden stops - Stack Overflow, https://stackoverflow.com/questions/62810854/inaccurate-masks-with-mask-rcnn-stairs-effect-and-sudden-stops</li>
<li>PointRend: Image Segmentation as Rendering, https://weiyc.github.io/seminar/PointRend_qiqi.pdf</li>
<li>PointRend: Image Segmentation as Rendering | SERP AI, https://serp.ai/posts/pointrend/</li>
<li>Pointrend: Image Segmentation As Rendering: Alexander Kirillov Kaiming He Yuxin Wu Ross Girshick | PDF - Scribd, https://www.scribd.com/document/470657078/833-talk</li>
<li>Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC11151725/</li>
<li>arXiv:1912.08193v2 [cs.CV] 16 Feb 2020, http://arxiv.org/pdf/1912.08193</li>
<li>Bilinear interpolation - Wikipedia, https://en.wikipedia.org/wiki/Bilinear_interpolation</li>
<li>Detectron2 FPN + PointRend Model for amazing Satellite Image Segmentation | by Affine, https://affine.medium.com/detectron2-fpn-pointrend-model-for-amazing-satellite-image-segmentation-183456063e15</li>
<li>PointRend: Image Segmentation As Rendering - YouTube, https://www.youtube.com/watch?v=yvNZGDZC3F8</li>
<li>How Mask R-CNN Works? | ArcGIS API for Python - Esri Developer, https://developers.arcgis.com/python/latest/guide/how-maskrcnn-works/</li>
<li>Mask R-CNN + PointRend structure. | Download Scientific Diagram - ResearchGate, https://www.researchgate.net/figure/Mask-R-CNN-PointRend-structure_fig6_349191977</li>
<li>Facebook PointRend: Rendering Image Segmentation - Synced Review, https://syncedreview.com/2019/12/26/facebook-pointrend-rendering-image-segmentation/</li>
<li>[1912.08193] PointRend: Image Segmentation as Rendering - arXiv, https://arxiv.org/abs/1912.08193</li>
<li>BRefine: Achieving High-Quality Instance Segmentation - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC9459926/</li>
<li>Trending Papers - Hugging Face, https://paperswithcode.com/lib/detectron2</li>
<li>detectron2/projects/PointRend … - Gitlab @ TU Graz, https://gitlab.tugraz.at/2EB2687AA157E3CC/logs_segm_thesis/-/tree/83d67c7fe68a0038a4ac4f9536da6467f27c1de8/detectron2/projects/PointRend</li>
<li>Supplementary materials: PointRend: Image Segmentation as Rendering - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Kirillov_PointRend_Image_Segmentation_CVPR_2020_supplemental.pdf</li>
<li>PointRend: Image Segmentation As Rendering - Papertalk, https://papertalk.org/papertalks/15599</li>
<li>PointRend: Image Segmentation As Rendering | Research - AI at Meta, https://ai.meta.com/research/publications/pointrend-image-segmentation-as-rendering/</li>
<li>IAUNet: Instance-Aware U-Net - arXiv, https://arxiv.org/html/2508.01928v1</li>
<li>(PDF) An improved approach for automated cervical cell segmentation with PointRend, https://www.researchgate.net/publication/381582089_An_improved_approach_for_automated_cervical_cell_segmentation_with_PointRend</li>
<li>HCMUS at MediaEval2021: PointRend with Attention Fusion Refinement for Polyps Segmentation - CEUR-WS, https://ceur-ws.org/Vol-3181/paper71.pdf</li>
<li>AP-PointRend: An Improved Network for Building Extraction via High-Resolution Remote Sensing Images - MDPI, https://www.mdpi.com/2072-4292/17/9/1481</li>
<li>(PDF) AP-PointRend: An Improved Network for Building Extraction …, https://www.researchgate.net/publication/391009077_AP-PointRend_An_Improved_Network_for_Building_Extraction_via_High-Resolution_Remote_Sensing_Images</li>
<li>Enhanced building footprint extraction from satellite imagery using Mask R-CNN and PointRend - ResearchGate, https://www.researchgate.net/publication/384510615_Enhanced_building_footprint_extraction_from_satellite_imagery_using_Mask_R-CNN_and_PointRend</li>
<li>PointRend: Image Segmentation As Rendering - Semantic Scholar, https://www.semanticscholar.org/paper/PointRend%3A-Image-Segmentation-As-Rendering-Kirillov-Wu/47c92620b4b966cb320a8f8ea5b8dfc8065e8fa4</li>
<li>SEMI-PointRend: improved semiconductor wafer defect classification and segmentation as rendering - SPIE Digital Library, https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12496/1249608/SEMI-PointRend–improved-semiconductor-wafer-defect-classification-and-segmentation/10.1117/12.2657555.short</li>
<li>PointRend Segmentation for a Densely Occluded Moving Object in a Video - ResearchGate, https://www.researchgate.net/publication/354110025_PointRend_Segmentation_for_a_Densely_Occluded_Moving_Object_in_a_Video</li>
<li>Pointly-Supervised Instance Segmentation - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Cheng_Pointly-Supervised_Instance_Segmentation_CVPR_2022_paper.pdf</li>
<li>[2104.06404] Pointly-Supervised Instance Segmentation - arXiv, https://arxiv.org/abs/2104.06404</li>
<li>Pointly-Supervised Instance Segmentation CVPR 2022 - Bowen Cheng, https://bowenc0221.github.io/point-sup/</li>
<li>Dual-Path Enhanced YOLO11 for Lightweight Instance Segmentation with Attention and Efficient Convolution - MDPI, https://www.mdpi.com/2079-9292/14/17/3389</li>
<li>EffSeg: Efficient Fine-Grained Instance Segmentation using Structure-Preserving Sparsity - arXiv, https://arxiv.org/html/2307.01545</li>
<li>RefineMask: Towards High-Quality Instance … - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_RefineMask_Towards_High-Quality_Instance_Segmentation_With_Fine-Grained_Features_CVPR_2021_paper.pdf</li>
<li>Mask Transfiner for High-Quality Instance … - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Ke_Mask_Transfiner_for_High-Quality_Instance_Segmentation_CVPR_2022_paper.pdf</li>
<li>Which is the current state of the art in image segmentation? - Milvus, https://milvus.io/ai-quick-reference/which-is-the-current-state-of-the-art-in-image-segmentation</li>
<li>[2304.02643] Segment Anything - arXiv, https://arxiv.org/abs/2304.02643</li>
<li>Segment Anything: A Paradigm Shift in Image Segmentation - YouTube, https://www.youtube.com/watch?v=tq_Fg3-km4g</li>
<li>facebookresearch/segment-anything: The repository provides code for running inference with the SegmentAnything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model. - GitHub, https://github.com/facebookresearch/segment-anything</li>
<li>Meta AI’s Segment Anything Model (SAM) Explained: The Ultimate Guide - Encord, https://encord.com/blog/segment-anything-model-explained/</li>
<li>Segment Anything Model (SAM) - Ultralytics YOLO Docs, https://docs.ultralytics.com/models/sam/</li>
<li>Semantic-aware SAM for Point-Prompted Instance Segmentation - arXiv, https://arxiv.org/html/2312.15895v2</li>
<li>Semantic-aware SAM for Point-Prompted Instance Segmentation - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Wei_Semantic-aware_SAM_for_Point-Prompted_Instance_Segmentation_CVPR_2024_paper.pdf</li>
<li>Segment Anything Model (SAM): Intro, Use Cases, V7 Tutorial - V7 Labs, https://www.v7labs.com/blog/segment-anything-model-sam</li>
<li>Segment Anything – A Foundation Model for Image Segmentation - LearnOpenCV, https://learnopencv.com/segment-anything/</li>
<li>[2306.01567] Segment Anything in High Quality - arXiv, https://arxiv.org/abs/2306.01567</li>
<li>Promoting Segment Anything Model towards Highly Accurate Dichotomous Image Segmentation - arXiv, https://arxiv.org/html/2401.00248v3</li>
<li>Promoting Segment Anything Model towards Highly Accurate Dichotomous Image Segmentation - arXiv, https://arxiv.org/html/2401.00248v2</li>
<li>[2306.01567] Segment Anything in High Quality - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/2306.01567</li>
<li>SysCV/sam-hq: Segment Anything in High Quality [NeurIPS 2023] - GitHub, https://github.com/SysCV/sam-hq</li>
<li>Segment Anything in High Quality, https://proceedings.neurips.cc/paper_files/paper/2023/file/5f828e38160f31935cfe9f67503ad17c-Paper-Conference.pdf</li>
<li>SAM-HQ - Hugging Face, https://huggingface.co/docs/transformers/model_doc/sam_hq</li>
<li>Exploring HQ-SAM - DebuggerCafe, https://debuggercafe.com/exploring-hq-sam/</li>
<li>SAMRefiner: Taming Segment Anything Model for Universal Mask Refinement, https://openreview.net/forum?id=JlDx2xp01W</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>