<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:UPSNet - 단일 네트워크 기반 통합적 파놉틱 분할 (2019-01-12)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>UPSNet - 단일 네트워크 기반 통합적 파놉틱 분할 (2019-01-12)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">이미지 분할 (Image Segmentations)</a> / <span>UPSNet - 단일 네트워크 기반 통합적 파놉틱 분할 (2019-01-12)</span></nav>
                </div>
            </header>
            <article>
                <h1>UPSNet - 단일 네트워크 기반 통합적 파놉틱 분할 (2019-01-12)</h1>
<h2>1.  파놉틱 분할의 등장과 통합적 접근의 필요성</h2>
<p>컴퓨터 비전 분야는 딥러닝 기술의 발전에 힘입어 이미지 내 객체를 인식하고 분할하는 능력에서 비약적인 성장을 이루었다. 특히, 이미지의 모든 픽셀에 특정 클래스 레이블을 할당하는 시맨틱 분할(Semantic Segmentation)과 동일 클래스 내 개별 객체 인스턴스를 구별하여 분할하는 인스턴스 분할(Instance Segmentation)은 각기 다른 방향으로 고도화되었다.1 그러나 이러한 개별적인 발전은 실제 세계의 복잡한 장면을 온전히 이해하는 데 한계를 드러냈다. 자율 주행 자동차나 지능형 로봇과 같은 시스템은 단순히 ’도로’와 ’자동차’를 구분하는 것을 넘어, 여러 대의 자동차를 개별적으로 인식하고 동시에 ’하늘’이나 ’보도’와 같은 배경 요소까지 포괄적으로 이해해야 하기 때문이다.2 이러한 요구에 부응하여, 2018년 Kirillov 연구팀은 두 분할 과제를 통합한 새로운 패러다임인 ’파놉틱 분할(Panoptic Segmentation, PS)’을 제안하였다.4</p>
<h3>1.1  시맨틱 분할과 인스턴스 분할의 통합: 파놉틱 분할의 정의</h3>
<p>파놉틱 분할은 전통적으로 분리되어 있던 시맨틱 분할과 인스턴스 분할을 하나의 통일된 과제로 정의한다.4 시맨틱 분할이 이미지 내 모든 픽셀에 ‘자동차’, ‘사람’, ’도로’와 같은 의미론적 클래스 레이블을 부여하는 데 초점을 맞춘다면, 인스턴스 분할은 ‘자동차_1’, ’자동차_2’와 같이 동일 클래스에 속하는 개별 객체들을 탐지하고 각각의 영역을 분할하는 것을 목표로 한다.1 파놉틱 분할의 궁극적인 목표는 이 두 가지 정보를 결합하여, 이미지 내 모든 픽셀에 대해 유일한(unique) 시맨틱 레이블과 인스턴스 ID를 동시에 할당하는 것이다.4 이를 통해 풍부하고 완전하며, 논리적으로 일관된 장면 분할 결과를 생성하여, 개별 과제만으로는 달성할 수 없었던 총체적인(holistic) 장면 이해를 가능하게 한다.8</p>
<p>이러한 통합적 접근의 명칭인 ’파놉틱(panoptic)’은 그리스어 ’pan(모두)’과 ’optic(보다)’의 합성어로, ’한눈에 모든 것을 본다’는 의미를 내포한다.6 이는 파놉틱 분할이 추구하는 장면의 전역적이고 통일된 시각을 상징적으로 보여준다. 과거에는 두 과제를 별개의 모델로 처리한 후 그 결과를 단순히 합치는 ‘후기 융합(late fusion)’ 방식이 시도되었으나, 이는 필연적으로 두 모델의 예측이 충돌하는 영역에서 불일치를 야기하고, 두 개의 무거운 모델을 독립적으로 실행해야 하는 계산 비효율성을 동반했다.1 파놉틱 분할은 과제 자체를 통합적으로 정의함으로써, 이러한 근본적인 문제들을 해결하고 새로운 아키텍처 설계를 촉진하는 계기가 되었다. 특히, 파놉틱 분할은 모든 픽셀이 단 하나의 <span class="math math-inline">(시맨틱 레이블, 인스턴스 ID)</span> 쌍에만 속해야 한다는 ‘비중첩(non-overlapping)’ 제약 조건을 명시적으로 요구하는데 5, 이는 단순한 후처리 방식으로는 해결하기 어려운 문제이며, 아키텍처 수준에서의 근본적인 통합이 필요함을 시사한다.</p>
<h3>1.2  ’Things’와 ’Stuff’의 개념적 구분과 분할 과제의 복잡성</h3>
<p>파놉틱 분할은 이미지의 구성 요소를 두 가지 주요 범주, 즉 ’Things’와 ’Stuff’로 개념화하여 접근한다.1 이 구분은 각 요소의 본질적인 특성과 이를 분할하는 데 적합한 기술적 접근법의 차이를 반영한다.</p>
<ul>
<li><strong>Things</strong>: 명확한 기하학적 형태를 가지며 개수를 셀 수 있는(countable) 객체들을 의미한다. 자동차, 사람, 동물 등이 여기에 해당하며, 각 인스턴스를 개별적으로 식별하고 분리해야 하므로 인스턴스 분할 과제와 밀접하게 연관된다.6</li>
<li><strong>Stuff</strong>: 형태가 정해져 있지 않고(amorphous) 개수를 세기 어려운(uncountable) 배경 영역을 지칭한다. 하늘, 도로, 잔디, 벽과 같은 영역이 대표적이며, 주로 질감이나 재질과 같은 특성으로 식별되므로 시맨틱 분할 과제에 더 적합하다.1</li>
</ul>
<p>파놉틱 분할의 핵심적인 복잡성은 이 두 가지 이질적인 범주를 단일 프레임워크 내에서 일관되게 처리해야 한다는 점에 있다. 기존의 인스턴스 분할 모델들은 주로 경계 상자(bounding box) 제안에 기반하므로 ‘Stuff’ 클래스를 자연스럽게 무시하는 경향이 있었고, 시맨틱 분할 모델들은 동일 클래스의 ‘Things’ 인스턴스들을 하나의 덩어리로 취급했다.1 파놉틱 분할은 모델이 이미지의 모든 픽셀에 대해 유일하고 중첩되지 않는 레이블을 할당하도록 강제함으로써, 이러한 두 가지 상이한 인식 모드를 조화시키고 그 사이에서 발생하는 충돌을 해결하도록 요구한다.1 결과적으로, 모델은 특정 픽셀이 ’세 번째 자동차’의 일부인지, 아니면 ’도로’의 일부인지를 명확하게 결정해야만 한다.</p>
<h3>1.3  통합 평가 지표: 파놉틱 품질(Panoptic Quality, PQ)의 도입</h3>
<p>새로운 과제를 정립하고 연구를 활성화하기 위해서는 공정하고 포괄적인 평가 지표가 필수적이다. Kirillov 연구팀은 파놉틱 분할의 성능을 측정하기 위해 ’파놉틱 품질(Panoptic Quality, PQ)’이라는 새로운 지표를 함께 제안했다.4 적절한 통합 지표의 부재는 과거 유사한 장면 이해(scene parsing) 과제들이 활성화되지 못했던 주요 원인 중 하나였으며, PQ는 이러한 공백을 메우는 중요한 역할을 했다.4</p>
<p>PQ는 ’Things’와 ‘Stuff’ 클래스 모두에 대해 분할 성능을 해석 가능하고 통일된 방식으로 측정하도록 설계되었다.4 이 지표는 두 가지 핵심 요소의 곱으로 정의된다: 분할 품질(Segmentation Quality, SQ)과 인식 품질(Recognition Quality, RQ).12<br />
<span class="math math-display">
PQ = \underbrace{\frac{\sum_{(p, g) \in TP} IoU(p, g)}{\vert TP \vert}}_{\text{Segmentation Quality (SQ)}} \times \underbrace{\frac{\vert TP \vert}{\vert TP \vert + \frac{1}{2}\vert FP \vert + \frac{1}{2}\vert FN \vert}}_{\text{Recognition Quality (RQ)}}
</span></p>
<ul>
<li><strong>분할 품질 (SQ)</strong>: 올바르게 예측된 세그먼트(True Positives, TP)들에 대해, 예측 세그먼트(<span class="math math-inline">p</span>)와 실제 정답 세그먼트(<span class="math math-inline">g</span>) 간의 평균 IoU(Intersection over Union)를 계산한다. 이는 예측된 마스크가 얼마나 정교하게 실제 객체의 경계와 일치하는지를 측정한다.7</li>
<li><strong>인식 품질 (RQ)</strong>: 탐지(detection) 문제에서 널리 사용되는 F1 점수와 유사한 개념으로, 참 긍정(TP), 거짓 긍정(False Positives, FP), 거짓 부정(False Negatives, FN)의 수를 기반으로 계산된다. 이는 모델이 객체를 얼마나 정확하게 탐지하고 분류하는지를 평가한다.12</li>
</ul>
<p>PQ는 분할의 정교함(SQ)과 탐지의 정확성(RQ) 사이의 균형을 하나의 값으로 명쾌하게 나타낸다. 예를 들어, 객체의 위치는 정확히 찾았지만 마스크의 경계가 부정확하면 SQ가 낮아지고, 반대로 마스크는 완벽하지만 엉뚱한 객체를 탐지하면 RQ가 낮아져 결국 PQ 점수가 모두 낮아진다. 이처럼 PQ는 파놉틱 분할의 두 가지 핵심 측면을 모두 반영하는 강력한 지표로 자리매김했으며, 이후 모델들의 성능을 엄밀하게 비교하고 발전 방향을 제시하는 기준이 되었다.</p>
<h2>2.  UPSNet: 통합된 파놉틱 분할 네트워크 심층 분석</h2>
<p>파놉틱 분할이라는 새로운 과제가 정립되면서, 이를 효과적으로 해결하기 위한 새로운 아키텍처에 대한 요구가 증대되었다. 이러한 배경 속에서 2019년 CVPR에서 발표된 UPSNet(Unified Panoptic Segmentation Network)은 초기 파놉틱 분할 연구의 방향을 제시한 선구적인 모델 중 하나이다.13 UPSNet은 분리된 두 개의 모델을 사용하는 기존의 접근 방식에서 벗어나, 단일 통합 네트워크를 통해 파놉틱 분할을 엔드-투-엔드(end-to-end) 방식으로 해결하는 혁신을 선보였다.</p>
<h3>2.1  핵심 철학: 분리된 모델에서 단일 통합 네트워크로의 전환</h3>
<p>UPSNet의 핵심 철학은 ’통합(unification)’이다.1 이전의 기준선(baseline) 모델들은 시맨틱 분할을 위해 PSPNet과 같은 모델을, 인스턴스 분할을 위해 Mask R-CNN과 같은 모델을 각각 독립적으로 실행한 후, 휴리스틱(heuristic) 규칙에 기반하여 결과를 병합하는 방식을 취했다.1 이러한 접근법은 두 가지 근본적인 문제를 안고 있었다. 첫째, 두 개의 대형 네트워크를 순차적으로 실행해야 하므로 계산 비용이 매우 높고 추론 속도가 느렸다. 둘째, 두 태스크가 서로의 정보를 활용하지 못하고 독립적으로 학습되기 때문에, 두 태스크 간의 잠재적인 시너지 효과를 기대할 수 없으며 예측 결과 간의 불일치가 빈번하게 발생했다.8</p>
<p>UPSNet은 이러한 문제들을 해결하기 위해 단일 백본(backbone) 네트워크를 기반으로 시맨틱 분할과 인스턴스 분할을 위한 특징 표현(feature representation)을 공유하는 구조를 제안했다.1 이는 중복되는 특징 추출 과정을 제거하여 계산 효율성을 극대화하고, 전체 시스템이 하나의 목표 아래 엔드-투-엔드로 함께 학습될 수 있도록 하여 두 하위 과제 간의 상호 보완적인 학습을 유도한다. 이처럼 UPSNet은 파놉틱 분할을 위한 우아하고 효율적인 통합 솔루션을 제시하며, 분리된 모델의 한계를 구조적으로 극복하고자 했다.</p>
<h3>2.2  네트워크 아키텍처 상세</h3>
<p>UPSNet은 공유 백본 네트워크 위에 각각의 하위 과제에 특화된 두 개의 헤드(head)와, 이 둘의 결과를 최종적으로 통합하는 파놉틱 헤드를 얹은 구조로 설계되었다. 이는 효율성을 위한 특징 공유와 전문성을 위한 헤드 분리의 장점을 결합한 영리한 접근 방식이다.</p>
<h4>2.2.1  백본 네트워크: 특징 추출을 위한 ResNet-FPN 구조</h4>
<p>UPSNet의 근간을 이루는 백본 네트워크는 특징 피라미드 네트워크(Feature Pyramid Network, FPN)가 결합된 ResNet(Residual Network) 아키텍처를 사용한다.12 이는 당시 인스턴스 분할 분야의 최고 성능 모델이었던 Mask R-CNN에서 그 효과가 입증된 강력한 조합이다.12</p>
<p>ResNet은 깊은 신경망에서 발생할 수 있는 기울기 소실(gradient vanishing) 문제를 잔차 학습(residual learning)을 통해 해결하여 매우 깊은 네트워크의 학습을 가능하게 한다. FPN은 ResNet의 여러 계층에서 추출된 특징 맵을 활용하여 다중 스케일(multi-scale) 특징 피라미드를 구축한다. 구체적으로, FPN은 저수준(low-level) 특징 맵(높은 해상도, 약한 시맨틱 정보)과 고수준(high-level) 특징 맵(낮은 해상도, 강한 시맨틱 정보)을 상향식(bottom-up) 및 하향식(top-down) 경로를 통해 결합하여, 각기 다른 스케일에서 풍부한 시맨틱 정보를 담고 있는 특징 맵 세트(예: P2, P3, P4, P5)를 생성한다.12 이렇게 생성된 다중 스케일 특징은 이미지 내에 존재하는 다양한 크기의 객체와 영역을 효과적으로 인식하고 분할하는 데 결정적인 역할을 하며, 이후 시맨틱 헤드와 인스턴스 헤드 모두에 입력으로 제공된다.</p>
<h4>2.2.2  시맨틱 분할 헤드: Deformable Convolution을 활용한 전경·배경 분할</h4>
<p>시맨틱 분할 헤드는 인스턴스 구별 없이 모든 픽셀에 대해 시맨틱 클래스를 예측하는 역할을 담당한다.12 이 헤드는 FPN으로부터 P2, P3, P4, P5 특징 맵을 입력받아 ’Stuff’와 ’Things’를 포함한 모든 클래스의 분할을 수행한다.</p>
<p>이 헤드의 핵심적인 구조적 특징은 Deformable Convolution(가변형 컨볼루션)의 사용이다.1 일반적인 컨볼루션 연산이 고정된 격자 형태의 커널을 사용하는 것과 달리, Deformable Convolution은 입력 특징 맵에 따라 샘플링 위치를 동적으로 학습하여 기하학적 변형에 더 강인한 특징을 추출할 수 있다. 이는 형태가 비정형적이고 경계가 불분명한 ‘Stuff’ 클래스(하늘, 도로 등)를 분할하는 데 특히 효과적이다.</p>
<p>시맨틱 헤드의 처리 과정은 다음과 같다. 먼저, FPN의 각 스케일별 특징 맵(P2-P5)이 독립적으로 Deformable Convolution 네트워크를 통과한다. 그 후, 처리된 특징 맵들은 모두 공통된 스케일(원본 이미지의 1/4 크기)로 업샘플링(upsampling)된 후 채널(channel) 축을 따라 연결(concatenate)된다. 마지막으로, 연결된 특징 맵에 1x1 컨볼루션과 소프트맥스(softmax) 함수를 적용하여 최종적인 픽셀별 시맨틱 예측 로짓(logit)을 생성한다.12 이처럼 UPSNet은 ‘Stuff’ 분할에 특화된 강력한 구조를 시맨틱 헤드에 도입하여 전체 성능을 끌어올렸다.</p>
<h4>2.2.3  인스턴스 분할 헤드: Mask R-CNN 기반 객체 탐지 및 마스크 생성</h4>
<p>인스턴스 분할 헤드는 ‘Things’ 클래스에 속하는 개별 객체 인스턴스를 탐지하고 분할하는 임무를 수행한다. 이 헤드는 Mask R-CNN의 헤드 구조를 거의 그대로 차용했다.1</p>
<p>Mask R-CNN은 2단계(two-stage) 접근법을 따른다. 첫 번째 단계에서는 영역 제안 네트워크(Region Proposal Network, RPN)가 FPN 특징 맵 위에서 객체가 존재할 가능성이 높은 후보 영역(Region of Interest, ROI)들을 제안한다.6 두 번째 단계에서는 각 ROI에 대해 RoIAlign과 같은 기법을 사용하여 고정된 크기의 특징 벡터를 추출한 후, 이를 세 개의 병렬적인 브랜치(branch)로 전달한다. 이 세 브랜치는 각각 ROI의 클래스를 분류(classification)하고, 경계 상자의 위치를 정교하게 조정(bounding box regression)하며, 픽셀 단위의 이진 마스크(binary mask)를 생성하는 역할을 수행한다.12</p>
<p>UPSNet의 인스턴스 헤드는 이 검증된 구조를 활용하여, 이미지 내의 셀 수 있는 ‘Things’ 객체들에 대한 인스턴스 수준의 정확한 정보를 생성한다. 이 헤드에서 생성된 클래스, 경계 상자, 마스크 정보는 이후 파놉틱 헤드에서 시맨틱 헤드의 정보와 통합되는 핵심적인 재료가 된다.</p>
<h3>2.3  파놉틱 헤드: 결과 통합과 충돌 해결을 위한 혁신적 메커니즘</h3>
<p>UPSNet의 가장 독창적이고 핵심적인 기여는 바로 파놉틱 헤드(Panoptic Head)에 있다. 이 모듈은 시맨틱 헤드와 인스턴스 헤드라는 두 이질적인 정보 소스를 지능적으로 융합하여 최종적인 파놉틱 분할 결과를 생성하는 역할을 한다. 특히, 이 헤드는 단순히 두 결과를 합치는 것을 넘어, 그 과정에서 발생하는 필연적인 충돌과 모호성을 해결하기 위한 정교한 메커니즘을 포함하고 있다.</p>
<h4>2.3.1  매개변수 없는(Parameter-Free) 로짓(Logit) 통합 방식</h4>
<p>파놉틱 헤드의 가장 큰 특징 중 하나는 학습 가능한 매개변수가 없는(parameter-free) 구조라는 점이다.1 이는 파놉틱 헤드가 추가적인 학습 부담 없이 결정론적인(deterministic) 규칙에 따라 두 헤드의 출력을 결합함을 의미한다. 이로 인해 모델이 가볍고 효율적이며, 다양한 백본 네트워크에 쉽게 적용될 수 있다.</p>
<p>통합 과정은 두 헤드에서 나온 픽셀별 로짓(logit, 소프트맥스 함수에 입력되기 전의 값)을 기반으로 이루어진다. 파놉틱 헤드는 최종적으로 <span class="math math-inline">(N_{stuff} + N_{inst})</span>개의 채널을 갖는 로짓 텐서 <span class="math math-inline">Z</span>를 생성하는 것을 목표로 한다. 여기서 <span class="math math-inline">N_{stuff}</span>는 ‘Stuff’ 클래스의 수이고, <span class="math math-inline">N_{inst}</span>는 해당 이미지에서 탐지된 ‘Things’ 인스턴스의 수이다.</p>
<ol>
<li><strong>Stuff 로짓 처리</strong>: 시맨틱 헤드에서 나온 로짓 <span class="math math-inline">X</span> 중 ‘Stuff’ 클래스에 해당하는 부분(<span class="math math-inline">X_{stuff}</span>)은 텐서 <span class="math math-inline">Z</span>의 첫 <span class="math math-inline">N_stuff</span>개 채널에 그대로 복사된다.1</li>
<li><strong>Instance 로짓 생성</strong>: 탐지된 각 인스턴스 <span class="math math-inline">i</span>에 대해, 최종 로짓은 시맨틱 헤드의 정보와 인스턴스 헤드의 정보를 결합하여 생성된다.</li>
</ol>
<ul>
<li>먼저, 시맨틱 헤드의 ‘Things’ 로짓(<span class="math math-inline">X_{thing}</span>)에서 해당 인스턴스의 예측 클래스(<span class="math math-inline">C_i</span>)에 해당하는 채널을 선택한다. 그리고 이 인스턴스의 경계 상자(<span class="math math-inline">B_i</span>) 내부 영역의 값만 남기고 나머지는 0으로 마스킹하여 <span class="math math-inline">X_{mask_i}</span>를 만든다.</li>
<li>다음으로, 인스턴스 헤드에서 나온 마스크 로짓 <span class="math math-inline">Y_i</span>를 원본 이미지 스케일로 업샘플링하고 경계 상자 외부를 0으로 채워 <span class="math math-inline">Y_{mask_i}</span>를 만든다.</li>
<li>최종적으로 인스턴스 <span class="math math-inline">i</span>에 대한 로짓은 두 로짓의 합으로 계산된다: <span class="math math-inline">Z_{N_{stuff}+i} = X_{mask_i} + Y_{mask_i}</span>.1</li>
</ul>
<ol start="3">
<li><strong>최종 예측</strong>: 이렇게 완성된 텐서 <span class="math math-inline">Z</span>의 채널 축에 대해 픽셀별로 소프트맥스 함수를 적용한다. 각 픽셀에서 가장 높은 값을 갖는 채널의 인덱스가 해당 픽셀의 최종 클래스(‘Stuff’ 클래스 중 하나 또는 특정 인스턴스 ID)로 결정된다.1</li>
</ol>
<h4>2.3.2  ‘Unknown’ 클래스의 도입과 오분류 문제 완화 전략</h4>
<p>UPSNet의 또 다른 핵심 혁신은 ’unknown(알 수 없음)’이라는 추가적인 클래스를 파놉틱 헤드에 도입한 것이다.1 이 메커니즘은 두 헤드의 예측이 충돌하거나 모호할 때 발생하는 오분류 문제를 완화하기 위해 고안된 정교한 전략이다.</p>
<p>그 동기는 PQ 평가 지표의 특성과 깊은 관련이 있다. 예를 들어, 모델이 ’보행자’를 ’자전거’로 잘못 예측했다고 가정해보자. 이 경우, ‘보행자’ 클래스에서는 거짓 부정(FN)이 1 증가하고, ‘자전거’ 클래스에서는 거짓 긍정(FP)이 1 증가한다. 이는 RQ 점수를 계산할 때 이중으로 불이익을 주어 PQ 점수를 크게 떨어뜨린다.1 만약 이처럼 잘못된 예측이 불가피한 상황에서 해당 픽셀을 ’unknown’으로 예측할 수 있다면, 평가 시 ‘unknown’ 픽셀은 무시되므로 ‘보행자’ 클래스의 FN만 1 증가하고 ‘자전거’ 클래스의 FP는 발생하지 않는다. 결과적으로 PQ 점수의 하락 폭을 완화할 수 있다.</p>
<p>‘unknown’ 클래스의 로짓 <span class="math math-inline">Z_{unknown}</span>은 다음과 같이 계산된다:<br />
<span class="math math-display">
Z_{unknown} = \max(X_{thing}) - \max(X_{mask})
</span><br />
여기서 <span class="math math-inline">X_{mask}</span>는 모든 인스턴스의 <span class="math math-inline">X_{mask_i}</span>를 채널 축으로 연결한 텐서이다.1 이 수식의 직관적인 의미는 다음과 같다. 어떤 픽셀에 대해 시맨틱 헤드가 전반적으로 ‘Things’ 클래스일 확률이 높다고 예측했지만(<span class="math math-inline">\max(X_{thing})</span> 값이 큼), 인스턴스 헤드가 제안한 어떤 특정 인스턴스에도 강하게 속하지 않는다면(<span class="math math-inline">\max(X_{mask})</span> 값이 작음), 이는 인스턴스 탐지기가 해당 객체를 놓쳤을(FN) 가능성이 높다는 신호이다. 이러한 불일치가 발생할 때 <span class="math math-inline">Z_{unknown}</span> 값이 커지게 되어, 해당 픽셀이 ’unknown’으로 분류될 확률이 높아진다.</p>
<p>이 메커니즘을 학습시키기 위해, 학습 과정에서는 실제 정답 마스크의 일부(논문에서는 30%)를 무작위로 샘플링하여 ‘unknown’ 클래스의 정답으로 설정하는 전략을 사용한다.1 이는 모델이 예측 불일치 상황을 ’unknown’으로 대응하도록 명시적으로 가르치는 역할을 한다.</p>
<h4>2.3.3  추론 시 인스턴스 간 중첩(Overlap) 해결 기법</h4>
<p>파놉틱 분할의 정의에 따라 최종 출력물에서는 인스턴스 간의 중첩이 허용되지 않는다. 그러나 Mask R-CNN 기반의 인스턴스 헤드는 각 ROI를 독립적으로 처리하기 때문에 예측된 마스크들이 서로 겹칠 수 있다. UPSNet은 추론(inference) 단계에서 이러한 중첩을 해결하기 위한 마스크 가지치기(mask pruning) 과정을 수행한다.1</p>
<ol>
<li><strong>초기 필터링</strong>: 먼저, 예측된 모든 인스턴스들의 경계 상자에 대해 클래스와 무관하게(class-agnostic) 비최대 억제(Non-Maximum Suppression, NMS)를 적용하여 심하게 겹치는 박스들을 제거한다. 이후, 클래스 예측 확률이 일정 임계값(예: 0.6) 이상인 인스턴스들만 남긴다.</li>
<li><strong>순차적 마스크 병합</strong>: 남은 인스턴스들을 예측 확률이 높은 순서대로 정렬한다. 그리고 빈 캔버스(원본 이미지와 동일한 크기)에 마스크를 하나씩 ‘그려나간다’.</li>
<li><strong>중첩 처리</strong>: 새로운 마스크를 캔버스에 추가할 때, 이미 캔버스에 그려진 마스크들과의 중첩 영역을 계산한다. 만약 중첩 영역이 현재 마스크 면적의 특정 비율(예: 30%)을 초과하면, 이 마스크는 중복 예측으로 간주하여 버린다. 그렇지 않은 경우, 기존 마스크와 겹치지 않는 부분만 캔버스에 복사한다.1</li>
</ol>
<p>이러한 휴리스틱 기반의 후처리 과정은 UPSNet이 파놉틱 분할의 비중첩 제약 조건을 만족하는 유효한 결과를 생성하기 위한 필수적인 장치이다. 이는 통합 모델 내에서도 여전히 하위 모듈들의 특성에서 비롯되는 불일치를 해결하기 위한 절차적 논리가 필요함을 보여준다. 이처럼 UPSNet의 아키텍처는 효율성을 위한 통합과 전문성을 위한 분리, 그리고 그 사이의 충돌을 해결하기 위한 지능적 융합 메커니즘이 정교하게 결합된 결과물이다. 이는 당시의 분리된 접근법과 이후 등장할 완전 통합형 트랜스포머 모델 사이의 중요한 기술적 가교 역할을 수행했다.</p>
<h2>3.  실험 및 성능 평가</h2>
<p>UPSNet의 우수성을 입증하기 위해, 연구팀은 당시 파놉틱 분할 분야의 표준 벤치마크로 자리 잡은 대규모 데이터셋에서 광범위한 실험을 수행했다. 성능 평가는 정량적 지표 비교뿐만 아니라, 추론 속도와 같은 효율성 측면과 실제 분할 결과를 시각적으로 분석하는 정성적 평가를 모두 포함하여 다각도로 이루어졌다.</p>
<h3>3.1  평가 데이터셋: COCO 및 Cityscapes 벤치마크</h3>
<p>UPSNet의 성능은 두 개의 주요 데이터셋, 즉 COCO와 Cityscapes에서 집중적으로 평가되었다.1</p>
<ul>
<li><strong>COCO (Common Objects in Context)</strong>: 일상적인 장면에서 발견되는 다양한 객체들을 포함하는 대규모 데이터셋이다. 광범위한 ’Things’와 ‘Stuff’ 클래스를 포함하고 있어, 모델의 일반적인 파놉틱 분할 성능을 측정하는 데 적합하다.6</li>
<li><strong>Cityscapes</strong>: 도시 거리 풍경 이미지에 특화된 데이터셋으로, 자율 주행 연구에 필수적인 벤치마크이다. 자동차, 보행자, 도로, 건물 등 정교하게 주석 처리된 픽셀 단위 레이블을 제공한다.9</li>
</ul>
<p>이 두 데이터셋은 파놉틱 분할 모델을 학습하고 평가하는 데 필요한 시맨틱 및 인스턴스 주석을 모두 갖추고 있어, 당시 연구의 표준으로 사용되었다.5</p>
<h3>3.2  정량적 성능 분석: 주요 모델과의 성능 비교</h3>
<p>UPSNet은 발표 당시 경쟁 모델들을 능가하는 최고 수준(state-of-the-art)의 성능을 달성했다.1 성능은 PQ, SQ, RQ를 중심으로 ’Things’와 ‘Stuff’ 각각에 대한 PQ 값(PQ_Th, PQ_St)과, 전통적인 시맨틱 분할 지표(mIoU) 및 인스턴스 분할 지표(AP)를 함께 측정하여 종합적으로 분석되었다.1</p>
<p>아래 표는 COCO와 Cityscapes 검증(validation) 데이터셋에서의 주요 성능 비교 결과를 요약한 것이다.</p>
<p><strong>표 1: COCO val2017 데이터셋 성능 비교</strong></p>
<table><thead><tr><th>Models</th><th>PQ</th><th>SQ</th><th>RQ</th><th>PQ_{Th}</th><th>PQ_{St}</th><th>mIoU</th><th>AP</th></tr></thead><tbody>
<tr><td>MR-CNN-PSP</td><td>41.8</td><td>78.4</td><td>51.3</td><td>47.8</td><td>32.8</td><td>53.9</td><td>34.2</td></tr>
<tr><td><strong>UPSNet</strong></td><td><strong>42.5</strong></td><td><strong>78.0</strong></td><td><strong>52.4</strong></td><td><strong>48.5</strong></td><td><strong>33.4</strong></td><td><strong>54.3</strong></td><td><strong>34.3</strong></td></tr>
<tr><td><strong>UPSNet-101-DCN (test-dev)</strong></td><td><strong>46.6</strong></td><td><strong>80.5</strong></td><td><strong>56.9</strong></td><td><strong>53.2</strong></td><td><strong>36.7</strong></td><td>-</td><td>-</td></tr>
</tbody></table>
<p>주: MR-CNN-PSP는 Mask R-CNN과 PSPNet을 결합한 기준선 모델이다. UPSNet-101-DCN은 더 깊은 ResNet-101 백본과 Deformable Convolution을 사용한 버전의 test-dev 결과이다.1</p>
<p>COCO 데이터셋 결과에서 UPSNet은 기준선 모델인 MR-CNN-PSP에 비해 전반적인 PQ 점수에서 우위를 보였다. 특히 주목할 점은, 분할의 정교함을 나타내는 SQ는 소폭 감소했으나, 객체 인식의 정확성을 나타내는 RQ가 51.3에서 52.4로 유의미하게 향상되었다는 것이다. 이는 UPSNet의 통합 구조와 파놉틱 헤드의 충돌 해결 메커니즘이 단순히 마스크 경계를 다듬는 것보다, 객체를 정확히 탐지하고 분류하는(즉, FP와 FN을 줄이는) 데 더 큰 기여를 했음을 시사한다. 이러한 경향은 ‘unknown’ 클래스를 통해 오분류(FP)를 억제하고 미탐지(FN)로 전환하려는 설계 의도와 정확히 일치한다.</p>
<p><strong>표 2: Cityscapes 데이터셋 성능 비교</strong></p>
<table><thead><tr><th>Models</th><th>PQ</th><th>SQ</th><th>RQ</th><th>PQ_{Th}</th><th>PQ_{St}</th><th>mIoU</th><th>AP</th></tr></thead><tbody>
<tr><td>MR-CNN-PSP</td><td>58.0</td><td>79.2</td><td>71.8</td><td>52.3</td><td>62.2</td><td>75.2</td><td>32.8</td></tr>
<tr><td>TASCNet</td><td>55.9</td><td>-</td><td>-</td><td>50.5</td><td>59.8</td><td>-</td><td>-</td></tr>
<tr><td><strong>UPSNet</strong></td><td><strong>59.3</strong></td><td><strong>79.7</strong></td><td><strong>73.0</strong></td><td><strong>54.6</strong></td><td><strong>62.7</strong></td><td><strong>75.2</strong></td><td><strong>33.3</strong></td></tr>
<tr><td><strong>UPSNet-COCO</strong></td><td><strong>60.5</strong></td><td><strong>80.9</strong></td><td><strong>73.5</strong></td><td><strong>57.0</strong></td><td><strong>63.0</strong></td><td><strong>77.8</strong></td><td><strong>37.8</strong></td></tr>
<tr><td><strong>UPSNet-101-M-COCO</strong></td><td><strong>61.8</strong></td><td><strong>81.3</strong></td><td><strong>74.8</strong></td><td><strong>57.6</strong></td><td><strong>64.8</strong></td><td><strong>79.2</strong></td><td><strong>39.0</strong></td></tr>
</tbody></table>
<p>주: TASCNet은 동시대의 경쟁 모델이다. ’-COCO’는 COCO 데이터셋으로 사전 학습한 모델, ’-101’은 ResNet-101 백본, ’-M’은 다중 스케일 테스트를 의미한다.1</p>
<p>Cityscapes 데이터셋에서도 유사한 경향이 나타났다. 기본 UPSNet 모델은 MR-CNN-PSP와 TASCNet을 모두 능가했으며, 특히 RQ 점수가 71.8에서 73.0으로 크게 향상되었다. 또한, 이 표는 모델 성능에 영향을 미치는 다른 요인들에 대한 중요한 정보를 제공한다. COCO 데이터셋으로 사전 학습했을 때(‘UPSNet-COCO’), 더 깊은 ResNet-101 백본을 사용하고 다중 스케일 테스트를 적용했을 때(‘UPSNet-101-M-COCO’) 성능이 순차적으로 향상되는 것을 볼 수 있다. 이는 대규모 외부 데이터셋을 통한 전이 학습(transfer learning)과 모델 용량 증대, 그리고 테스트 시의 앙상블 기법이 파놉틱 분할 성능에 긍정적인 영향을 미침을 보여준다.</p>
<h3>3.3  효율성 분석: 추론 속도 및 모델 크기 비교</h3>
<p>UPSNet의 가장 큰 장점 중 하나는 정확도뿐만 아니라 효율성에도 있었다. 단일 공유 백본 구조는 중복 계산을 제거하여 추론 속도와 모델 크기 면에서 상당한 이점을 가져왔다.1</p>
<ul>
<li><strong>추론 속도</strong>: Cityscapes 데이터셋(1024x2048 해상도)에서, 기준선 모델인 MR-CNN-PSP가 추론에 약 3배 더 많은 시간을 소요하는 반면, UPSNet은 훨씬 빠른 속도를 기록했다.1 이미지 크기가 커질수록 이러한 속도 차이는 더욱 두드러졌다. 실시간 처리가 중요한 자율 주행과 같은 실제 응용 분야에서 이러한 속도 향상은 매우 중요한 의미를 갖는다.</li>
<li><strong>모델 크기</strong>: COCO 데이터셋 기준으로, UPSNet의 파라미터 수는 46.1M(백만)인 반면, MR-CNN-PSP는 91.6M으로 거의 두 배에 달했다.1 이는 모델을 저장하고 배포하는 데 필요한 리소스를 절반으로 줄여, 하드웨어 제약이 있는 환경에서의 적용 가능성을 높인다.</li>
</ul>
<p>이러한 효율성 향상은 단순히 모델을 경량화한 결과가 아니라, 아키텍처 자체의 합리적인 설계, 즉 특징 공유를 통해 얻어진 것이라는 점에서 그 가치가 크다. UPSNet은 성능 저하 없이, 오히려 성능을 향상시키면서 효율성을 달성했다.</p>
<h3>3.4  정성적 결과 분석: 시각적 분할 예시를 통한 강점과 약점 고찰</h3>
<p>정량적 지표와 더불어, 실제 이미지에 대한 분할 결과를 시각적으로 분석하는 정성적 평가는 모델의 동작 방식과 장단점을 직관적으로 이해하는 데 도움을 준다. 논문에 제시된 시각적 예시들은 UPSNet의 강점을 명확히 보여준다.12</p>
<p>기준선 모델의 경우, 시맨틱 분할 결과와 인스턴스 분할 결과가 충돌하는 영역에서 예측을 포기하고 이를 ‘unknown’ 클래스(검은색 영역)로 처리하는 경향이 두드러졌다. 이는 두 독립적인 모델의 예측을 기계적으로 병합할 때 발생하는 한계를 보여준다. 반면, UPSNet은 이러한 충돌 상황을 훨씬 더 효과적으로 해결하여 더 깨끗하고 일관된 분할 맵을 생성했다.1 이는 파놉틱 헤드가 두 헤드의 정보를 유기적으로 결합하고, 모호한 경우 ‘unknown’ 클래스를 국소적으로 활용하여 전체적인 분할 품질을 유지하는 능력이 뛰어남을 시각적으로 증명한다. COCO, Cityscapes, 그리고 Uber의 내부 데이터셋에 대한 다양한 예시들은 복잡한 장면에서도 UPSNet이 강건하게 동작함을 보여주었다.12</p>
<h2>4.  UPSNet의 영향력과 파놉틱 분할 기술의 발전</h2>
<p>UPSNet은 파놉틱 분할 분야의 초석을 다진 중요한 모델로서, 학계와 산업계에 상당한 영향을 미쳤다. 이 모델이 제시한 통합적 접근 방식은 이후 연구의 흐름을 바꾸었으며, 동시에 그 한계점은 차세대 기술의 등장을 촉진하는 계기가 되었다.</p>
<h3>4.1  UPSNet이 남긴 학술적 기여와 후속 연구에 미친 영향</h3>
<p>UPSNet의 가장 큰 학술적 기여는 파놉틱 분할이라는 새로운 과제를 해결하기 위해 <strong>단일 통합 네트워크의 실효성과 우수성을 최초로 입증</strong>했다는 점이다.1 이는 단순히 두 개의 기존 모델을 이어 붙이는 방식보다, 특징 추출 단계를 공유하고 전체를 엔드-투-엔드로 학습시키는 것이 성능과 효율성 양면에서 모두 우월하다는 패러다임을 제시했다.</p>
<p>이러한 접근 방식은 후속 연구에 직접적인 영향을 미쳤다. 예를 들어, UPSNet의 기본 골격을 유지하면서 특징 추출 네트워크를 재귀적 특징 피라미드(recursive feature pyramid)로 개선하거나, 객체 간 가림(occlusion) 문제를 처리하기 위한 별도의 모듈을 추가하는 연구들이 등장했다.18 이는 UPSNet이 강력하고 확장 가능한 베이스라인으로서의 역할을 했음을 의미한다. 또한, 파라미터가 없는 파놉틱 헤드와 PQ 지표에 최적화된 ‘unknown’ 클래스라는 독창적인 아이디어는 후속 모델들이 결과를 융합하고 충돌을 해결하는 방식에 대한 새로운 영감을 제공했다. UPSNet은 파놉틱 분할 연구를 ’어떻게 두 결과를 합칠 것인가’의 문제에서 ’어떻게 처음부터 통합적으로 예측할 것인가’의 문제로 전환시키는 데 결정적인 역할을 했다.</p>
<h3>4.2  2019년 이후의 패러다임 변화: Transformer 기반 모델의 부상</h3>
<p>UPSNet이 통합 아키텍처의 문을 열었지만, 그 구조는 여전히 과도기적 특성을 지니고 있었다. 즉, 시맨틱 분할을 위한 상향식(bottom-up) 픽셀 단위 예측과 인스턴스 분할을 위한 하향식(top-down) 영역 제안 방식이라는 두 가지 이질적인 접근법을 내부에 공존시키고, 이를 파놉틱 헤드라는 사후 조정 장치로 봉합하는 형태였다. 이러한 구조적 긴장감은 필연적으로 복잡한 휴리스틱과 충돌 해결 메커니즘을 요구했다.</p>
<p>2019년 이후, 컴퓨터 비전 분야에는 트랜스포머(Transformer) 아키텍처가 도입되면서 또 한 번의 패러다임 전환이 일어났다.19 DETR(DEtection TRansformer)을 시작으로, Panoptic SegFormer, MaskFormer, Mask2Former와 같은 모델들은 파놉틱 분할을 근본적으로 다른 방식으로 접근했다.21</p>
<p>이들 모델의 핵심은 분할 문제를 <strong>직접적인 집합 예측(direct set prediction) 문제</strong>로 재정의한 것이다. 이들은 소수의 학습 가능한 ’쿼리(query)’를 사용하여 이미지 전체의 특징과 상호작용(attention)하고, 각 쿼리가 하나의 객체 인스턴스(‘Thing’) 또는 배경 영역(‘Stuff’)의 마스크와 클래스를 직접 예측하도록 한다.24 이 방식은 다음과 같은 장점을 가진다.</p>
<ul>
<li><strong>진정한 통합</strong>: RPN과 같은 영역 제안 단계나 별도의 시맨틱/인스턴스 헤드가 필요 없다. 단일 메커니즘(쿼리 기반 예측)이 ’Things’와 ’Stuff’를 모두 처리하므로 구조가 훨씬 더 간결하고 통합적이다.</li>
<li><strong>휴리스틱 제거</strong>: NMS나 복잡한 마스크 병합 규칙과 같은 후처리 과정이 대부분 제거된다. 모델이 학습을 통해 중복되지 않는 전체 분할 맵을 직접 생성하도록 유도된다.</li>
<li><strong>높은 성능</strong>: Mask2Former와 같은 모델들은 UPSNet을 포함한 이전 세대 CNN 기반 모델들의 성능을 크게 뛰어넘으며 새로운 SOTA를 달성했다.22</li>
</ul>
<p>결론적으로, UPSNet이 ’분리된 모델’에서 ’통합된 헤드를 가진 단일 모델’로의 전환을 이끌었다면, 트랜스포머 기반 모델들은 ’통합된 헤드’마저 필요 없는 ’완전 통합 예측 메커니즘’으로의 진화를 이끌었다. UPSNet은 그 시대의 문제를 해결한 중요한 전환기적 아키텍처였으며, 그 한계는 자연스럽게 다음 세대 기술의 목표가 되었다.</p>
<h3>4.3  현재의 도전 과제와 미래 전망</h3>
<p>트랜스포머 기반 모델들의 등장으로 파놉틱 분할의 정확도는 크게 향상되었지만, 실제 세계에 이 기술을 안정적으로 배포하기까지는 여전히 많은 도전 과제가 남아있다.</p>
<ul>
<li><strong>실시간 성능(Real-Time Performance)</strong>: 자율 주행과 같이 즉각적인 반응이 요구되는 응용 분야에서는 수십 밀리초 내에 추론이 완료되어야 한다. 많은 SOTA 모델들은 여전히 이 기준을 충족시키지 못하며, 정확도와 속도 사이의 균형을 맞추는 것은 계속되는 연구 주제이다.6</li>
<li><strong>강건성(Robustness)</strong>: 대부분의 모델은 맑은 날씨와 좋은 조명 조건에서 촬영된 데이터셋으로 학습된다. 이로 인해 비, 안개, 눈, 야간과 같은 악천후나 열악한 조명 조건에서는 성능이 급격히 저하된다.26 이러한 도메인 격차(domain gap)를 극복하기 위한 도메인 적응(domain adaptation) 및 강건성 향상 연구가 활발히 진행 중이다.29</li>
<li><strong>개방형 세계 시나리오(Open-World Scenarios)</strong>: 현재 모델들은 학습 데이터에 존재하는 미리 정의된 ‘닫힌(closed)’ 클래스 집합에 대해서만 예측할 수 있다. 하지만 실제 환경에서는 학습 시 보지 못했던 새로운 객체나 예상치 못한 상황이 끊임없이 발생한다. 이러한 ’미지(unknown)’의 대상을 인식하고 안전하게 대처하는 능력은 자율 시스템의 안전에 직결되는 문제이다. 이를 해결하기 위한 개방형 세계(open-world) 및 개방형 어휘(open-vocabulary) 파놉틱 분할 연구가 차세대 핵심 연구 분야로 부상하고 있다.31</li>
<li><strong>주석 비용(Annotation Cost)</strong>: 파놉틱 분할은 픽셀 단위의 시맨틱 레이블과 인스턴스 ID를 모두 요구하므로, 고품질의 학습 데이터를 구축하는 데 막대한 시간과 비용이 소요된다. 이러한 부담을 줄이기 위해 바운딩 박스나 이미지 태그와 같은 약한 형태의 레이블을 활용하는 약지도 학습(weakly-supervised learning) 및 소량의 정밀 레이블만 사용하는 준지도 학습(semi-supervised learning) 방법론이 연구되고 있다.34</li>
</ul>
<p>미래의 파놉틱 분할 기술은 단순히 깨끗한 벤치마크 데이터셋에서의 PQ 점수를 높이는 것을 넘어, 예측 불가능하고 동적으로 변화하는 실제 환경 속에서 신뢰할 수 있고 안전하게 동작하는 것을 목표로 발전해 나갈 것이다.</p>
<h2>5.  결론</h2>
<p>UPSNet은 파놉틱 분할이라는 새로운 컴퓨터 비전 과제의 초창기에 등장하여, 통합적 장면 이해를 향한 중요한 이정표를 세운 모델이다. 이 모델은 분리된 네트워크를 사용하던 기존의 접근법이 가진 비효율성과 불일치 문제를 정면으로 해결하고자 했으며, 그 해법으로 단일 공유 백본 기반의 통합 아키텍처를 제시했다.</p>
<p>UPSNet의 핵심적인 기여는 다음과 같이 요약할 수 있다. 첫째, 특징 추출 과정을 공유함으로써 계산 효율성을 극대화하고, 엔드-투-엔드 학습을 통해 시맨틱 분할과 인스턴스 분할 간의 시너지를 유도할 수 있음을 입증했다. 둘째, 학습 가능한 파라미터 없이 두 헤드의 출력을 지능적으로 융합하는 혁신적인 파놉틱 헤드를 설계했다. 셋째, 파놉틱 품질(PQ)이라는 평가 지표의 특성을 깊이 이해하고, 오분류 패널티를 완화하기 위한 ‘unknown’ 클래스라는 정교한 메커니즘을 도입하여 실질적인 성능 향상을 이끌어냈다. 그 결과, UPSNet은 발표 당시 경쟁 모델들을 압도하는 SOTA 성능을 훨씬 빠른 추론 속도로 달성하며, 통합적 접근 방식의 명백한 우월성을 증명했다.1</p>
<p>역사적 관점에서 UPSNet은 파놉틱 분할 기술 발전의 중요한 ’전환기적 아키텍처’로 평가될 수 있다. 이는 완전히 분리된 모델이라는 과거의 패러다임과, 쿼리 기반의 완전 통합 모델이라는 미래의 패러다임 사이를 잇는 견고한 다리 역할을 했다. UPSNet은 당시의 가장 시급한 문제였던 효율성과 결과 융합의 문제를 성공적으로 해결했지만, 그 구조 내에 여전히 남아있던 하향식(top-down)과 상향식(bottom-up) 접근법의 이질성은 차세대 모델들이 해결해야 할 과제로 남겼다.</p>
<p>결론적으로, UPSNet은 총체적 장면 이해(holistic scene understanding)라는 컴퓨터 비전의 오랜 목표를 향한 여정에서 중요한 진전을 이룬 기념비적인 연구이다. 이 모델은 파놉틱 분할이라는 새로운 과제에 대한 실용적이고 강력한 해결책을 제시했으며, 이후 등장할 더욱 정교하고 통합된 모델들이 나아갈 길을 밝혔다. UPSNet을 통해 우리는 시맨틱 분할과 인스턴스 분할의 통합이 단순히 가능한 것을 넘어, 정확도와 효율성 모두를 위한 필연적인 방향임을 확인할 수 있었다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>UPSNet: A Unified Panoptic Segmentation Network - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2019/papers/Xiong_UPSNet_A_Unified_Panoptic_Segmentation_Network_CVPR_2019_paper.pdf</li>
<li>Panoptic Segmentation: A Review - arXiv, https://arxiv.org/pdf/2111.10250</li>
<li>Panoptic Perception for Autonomous Driving: A Survey - arXiv, https://arxiv.org/html/2408.15388v1</li>
<li>[1801.00868] Panoptic Segmentation - arXiv, https://arxiv.org/abs/1801.00868</li>
<li>Panoptic Segmentation - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2019/papers/Kirillov_Panoptic_Segmentation_CVPR_2019_paper.pdf</li>
<li>Understanding Panoptic Segmentation Basics - Viso Suite, https://viso.ai/deep-learning/panoptic-segmentation/</li>
<li>Panoptic Segmentation: Unifying Semantic and Instance Segmentation - DigitalOcean, https://www.digitalocean.com/community/tutorials/panoptic-segmentation</li>
<li>Panoptic Segmentation: Definition, Datasets &amp; Tutorial [2024] - V7 Labs, https://www.v7labs.com/blog/panoptic-segmentation-guide</li>
<li>Guide to Panoptic Segmentation - Encord, https://encord.com/blog/panoptic-segmentation-guide/</li>
<li>Panoptic Segmentation - Cloudinary, https://cloudinary.com/glossary/panoptic-segmentation</li>
<li>www.v7labs.com, <a href="https://www.v7labs.com/blog/panoptic-segmentation-guide#:~:text=Panoptic%20segmentation%20helps%20classify%20objects,%2C%20cars%2C%20animals%2C%20etc.">https://www.v7labs.com/blog/panoptic-segmentation-guide#:~:text=Panoptic%20segmentation%20helps%20classify%20objects,%2C%20cars%2C%20animals%2C%20etc.</a></li>
<li>Review — UPSNet: A Unified Panoptic Segmentation Network | by Sik-Ho Tsang | Medium, https://sh-tsang.medium.com/review-upsnet-a-unified-panoptic-segmentation-network-3754561fe497</li>
<li>uber-research/UPSNet: UPSNet: A Unified Panoptic Segmentation Network - GitHub, https://github.com/uber-research/UPSNet</li>
<li>UPSNet: A Unified Panoptic Segmentation Network - CVPR 2019 Open Access Repository, https://openaccess.thecvf.com/content_CVPR_2019/html/Xiong_UPSNet_A_Unified_Panoptic_Segmentation_Network_CVPR_2019_paper.html</li>
<li>What is Mask R-CNN? The Ultimate Guide. - Roboflow Blog, https://blog.roboflow.com/mask-rcnn/</li>
<li>(PDF) UPSNet: A Unified Panoptic Segmentation Network - ResearchGate, https://www.researchgate.net/publication/330382643_UPSNet_A_Unified_Panoptic_Segmentation_Network</li>
<li>Mask R-CNN - Everything explained - Picsellia, https://www.picsellia.com/post/mask-r-cnn-everything-explained</li>
<li>Improvement of panoptic segmentation method for urban road - OpenReview, https://openreview.net/forum?id=16fJWPOK0m</li>
<li>Cascade contour-enhanced panoptic segmentation for robotic vision perception - Frontiers, https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2024.1489021/full</li>
<li>What is Panoptic Segmentation? Challenges &amp; How It Works - Deepchecks, https://www.deepchecks.com/glossary/panoptic-segmentation/</li>
<li>Delving Deeper into Panoptic Segmentation with Transformers - arXiv, https://arxiv.org/pdf/2109.03814</li>
<li>Mask2Former - Hugging Face, https://huggingface.co/docs/transformers/model_doc/mask2former</li>
<li>[2109.03814] Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with Transformers - arXiv, https://arxiv.org/abs/2109.03814</li>
<li>MP-Former: Mask-Piloted Transformer for Image Segmentation - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_MP-Former_Mask-Piloted_Transformer_for_Image_Segmentation_CVPR_2023_paper.pdf</li>
<li>Real-Time Panoptic Segmentation From Dense Detections - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2020/papers/Hou_Real-Time_Panoptic_Segmentation_From_Dense_Detections_CVPR_2020_paper.pdf</li>
<li>Benchmarking the Robustness of Panoptic Segmentation for Automated Driving - arXiv, https://arxiv.org/html/2402.15469v1</li>
<li>Unifying Panoptic Segmentation for Autonomous Driving | Request PDF - ResearchGate, https://www.researchgate.net/publication/363906446_Unifying_Panoptic_Segmentation_for_Autonomous_Driving</li>
<li>End-to-End Video Semantic Segmentation in Adverse Weather using Fusion Blocks and Temporal-Spatial Teacher-Student Learning - NIPS, https://proceedings.neurips.cc/paper_files/paper/2024/file/fed6d142d12b2f8c031615cc8fd50893-Paper-Conference.pdf</li>
<li>ACDC: The Adverse Conditions Dataset with Correspondences for Robust Semantic Driving Scene Perception - ETH Zürich, https://people.ee.ethz.ch/~csakarid/ACDC_v2/ACDC_The_Adverse_Conditions_Dataset_with_Correspondences_for_Robust_Semantic_Driving_Scene_Perception-Sakaridis+Wang+Li+Zurbruegg+Jadon+Abbeloos+Olmeda_Reino+Van_Gool+Dai-arXiv_2024.pdf</li>
<li>Language-Guided Instance-Aware Domain-Adaptive Panoptic Segmentation - arXiv, https://arxiv.org/html/2404.03799v1</li>
<li>[2412.12740] Open-World Panoptic Segmentation - arXiv, https://arxiv.org/abs/2412.12740</li>
<li>Open-World Panoptic Segmentation - arXiv, https://arxiv.org/html/2412.12740v1</li>
<li>EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation - arXiv, https://arxiv.org/html/2412.08628v2</li>
<li>Weakly- and Semi-Supervised Panoptic Segmentation | Qizhu Li | 李淇竹, https://qizhuli.github.io/publication/weakly-supervised-panoptic-segmentation/</li>
<li>Weakly- and Semi-Supervised, Non-Overlapping Instance Segmentation of Things and Stuff - CVF Open Access, https://openaccess.thecvf.com/content_ECCV_2018/papers/Anurag_Arnab_Weakly-_and_Semi-Supervised_ECCV_2018_paper.pdf</li>
<li>Pointly-Supervised Panoptic Segmentation - European Computer Vision Association, https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136900318.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>