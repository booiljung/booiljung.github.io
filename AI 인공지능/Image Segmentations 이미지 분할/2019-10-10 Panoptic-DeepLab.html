<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Panoptic-DeepLab (2019-10-10)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Panoptic-DeepLab (2019-10-10)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">이미지 분할 (Image Segmentations)</a> / <span>Panoptic-DeepLab (2019-10-10)</span></nav>
                </div>
            </header>
            <article>
                <h1>Panoptic-DeepLab (2019-10-10)</h1>
<h2>1.  통합적 장면 이해를 향한 여정, 파놉틱 분할</h2>
<p>컴퓨터 비전 분야의 궁극적인 목표 중 하나는 인간의 시각 시스템처럼 주어진 장면을 총체적으로 이해하는 것이다. 이러한 목표를 달성하기 위한 핵심 과업으로, 2018년 Alexander Kirillov 등에 의해 ’파놉틱 분할(Panoptic Segmentation)’이라는 새로운 패러다임이 제안되었다.1 파놉틱 분할은 단순히 기술적인 발전을 넘어, 기존의 분절된 인식 체계를 통합하여 보다 완전한 장면 이해로 나아가려는 컴퓨터 비전 연구의 철학적 전환을 상징한다. 초기 컴퓨터 비전 연구가 분류, 탐지, 분할 등 개별 과업에 집중했던 것과 달리, 파놉틱 분할은 장면의 모든 픽셀에 대해 의미와 개체를 동시에 부여함으로써, 인공지능이 세상을 보다 인간과 유사한 방식으로, 즉 총체적이고 맥락적으로 인식하도록 하는 중요한 이정표를 제시했다.</p>
<h3>1.1  파놉틱 분할의 정의와 목표</h3>
<p>파놉틱 분할은 그 이름이 ’모든 것’을 의미하는 그리스어 ’pan’과 ’시각’을 의미하는 ’optic’의 합성어인 것에서 알 수 있듯이, 시야에 들어오는 모든 것을 분할하는 과업을 지향한다.1 기술적으로 이는 기존 컴퓨터 비전의 두 가지 핵심 분할 과업인 의미론적 분할(semantic segmentation)과 인스턴스 분할(instance segmentation)을 하나의 통일된 프레임워크로 통합하는 것을 목표로 한다.4</p>
<p>의미론적 분할은 이미지의 모든 픽셀에 ‘자동차’, ‘도로’, ’하늘’과 같은 사전 정의된 클래스 레이블을 할당한다.6 이를 통해 장면의 전반적인 의미적 구성을 파악할 수 있지만, ’자동차 1’과 ’자동차 2’처럼 동일한 클래스에 속하는 개별 객체 인스턴스를 구분하지 못하는 명백한 한계를 가진다.6 반면, 인스턴스 분할은 이미지 내에 존재하는 각 객체 인스턴스를 개별적으로 탐지하고, 픽셀 단위로 정확하게 분할하는 데 초점을 맞춘다.5 이는 객체의 개수를 세거나 개별 객체를 추적하는 데 유용하지만, ’도로’나 ’하늘’과 같은 배경 영역(stuff)은 고려하지 않는다.7</p>
<p>파놉틱 분할은 이러한 상호 보완적인 두 과업을 통합하여, 이미지의 모든 픽셀에 의미론적 레이블과 고유한 인스턴스 ID를 동시에 할당한다.4 이로써 장면 내의 모든 구성 요소를 모호함 없이, 완전하고 일관된 방식으로 표현할 수 있게 된다. 예를 들어, 파놉틱 분할 결과물은 특정 픽셀이 ’자동차’라는 의미를 가질 뿐만 아니라, 그것이 장면 내의 ’3번 자동차’에 속한다는 정보까지 함께 제공한다.</p>
<h3>1.2  “Things“와 “Stuff”: 파놉틱 분할의 핵심 개념</h3>
<p>파놉틱 분할은 장면을 구성하는 요소를 근본적으로 다른 두 가지 범주, 즉 “Things“와 “Stuff“로 구분하여 이해한다.3 이 개념적 구분은 두 분할 과업을 통합하는 데 있어 핵심적인 역할을 한다.</p>
<ul>
<li><strong>Things:</strong> 명확한 형태와 경계를 가지며, 개별적으로 셀 수 있는(countable) 객체 인스턴스를 지칭한다. 자동차, 사람, 동물 등이 여기에 해당하며, 전통적으로 인스턴스 분할의 주된 대상이 되어왔다.1</li>
<li><strong>Stuff:</strong> 하늘, 도로, 잔디, 벽과 같이 비정형적이고 뚜렷한 경계가 없으며, 개별 인스턴스로 구분하기 어려운 배경 영역을 의미한다. 이는 주로 텍스처나 재질로 식별되며, 의미론적 분할을 통해 처리된다.1</li>
</ul>
<p>기존의 분할 방식들은 이 두 범주를 독립적으로 다루거나 어느 한쪽에만 집중함으로써 정보의 격차를 남겼다. 파놉틱 분할은 “Things“와 “Stuff“를 하나의 통일된 출력으로 표현함으로써, 장면의 모든 구성 요소를 빠짐없이 설명하고, 이를 통해 진정한 의미의 포괄적인 장면 이해(holistic scene understanding)를 가능하게 한다.10</p>
<h3>1.3  파놉틱 분할의 중요성과 응용 분야</h3>
<p>파놉틱 분할이 제공하는 통합적이고 상세한 장면 정보는 다양한 실제 응용 분야에서 필수적이다. 특히 자율 주행 시스템은 이러한 정보의 가장 중요한 수혜자 중 하나다.6 자율 주행 차량은 주행 가능한 영역인 ‘도로’(stuff)를 인식하는 동시에, 주변의 다른 ’차량’이나 ‘보행자’(things)를 개별적으로 식별하고 그 움직임을 예측해야만 안전한 의사결정을 내릴 수 있다.12 파놉틱 분할은 이 모든 정보를 단일 프레임워크 내에서 제공함으로써, 시스템의 복잡도를 낮추고 인식의 정확성과 일관성을 높인다.</p>
<p>이 외에도 파놉틱 분할은 증강 현실(AR)에서 가상 객체를 실제 환경과 자연스럽게 상호작용하도록 배치하거나 6, 의료 영상에서 종양(thing)과 주변 조직(stuff)을 정밀하게 구분하여 진단 및 치료 계획 수립을 돕는 등 6, 다양한 분야에서 그 중요성이 커지고 있다. 로보틱스 분야에서도 로봇이 복잡한 환경을 탐색하고 객체와 상호작용하기 위해 장면의 모든 구성 요소를 이해하는 것이 필수적이다.6 이처럼 파놉틱 분할은 단순한 학술적 과업을 넘어, 인공지능 시스템이 실제 세계와 더 정교하게 상호작용하기 위한 핵심 기반 기술로 자리매김하고 있다.</p>
<h2>2.  Panoptic-DeepLab의 제안: 단순하고 강력한 상향식(Bottom-Up) 접근법</h2>
<p>Panoptic-DeepLab이 등장하기 전, 파놉틱 분할 분야는 주로 하향식(top-down) 접근법이 주도하고 있었다. 이 방식들은 높은 정확도를 보였지만, 구조적 복잡성과 느린 추론 속도라는 한계를 안고 있었다. 이러한 배경 속에서, Bowen Cheng 연구팀은 CVPR 2020에서 “Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation“이라는 논문을 통해 상향식 접근법의 새로운 가능성을 제시했다.13 이 모델의 등장은 단순히 또 하나의 고성능 모델을 추가한 것을 넘어, ‘정확도’ 일변도의 연구 방향에 ’효율성’과 ’단순성’이라는 중요한 가치를 다시금 상기시키는 계기가 되었다. 이는 상향식 접근법에 대한 재평가를 이끌어냈으며, 특히 실시간 처리가 중요한 산업 응용 분야에서 파놉틱 분할의 실용적 채택 가능성을 활짝 열어주었다.</p>
<h3>2.1  상향식(Bottom-Up) vs. 하향식(Top-Down) 접근법</h3>
<p>파놉틱 분할을 수행하는 방법론은 크게 하향식과 상향식으로 나뉜다. 이 두 접근법은 문제를 해결하는 순서와 방식에서 근본적인 차이를 보인다.</p>
<ul>
<li><strong>하향식 (Top-Down) 접근법:</strong> 이 방식은 ‘탐지 후 분할(detect-then-segment)’ 전략을 따른다. 대부분 Mask R-CNN과 같은 객체 탐지기를 기반으로 하며, 먼저 이미지 내에서 ‘thing’ 클래스에 해당하는 모든 객체 인스턴스에 대한 제안(proposal)이나 경계 상자(bounding box)를 생성한다.15 그 후, 각 제안 영역 내에서 픽셀 단위의 마스크를 예측하여 인스턴스 분할을 수행한다. 의미론적 분할은 별도의 분기(branch)에서 수행된 후, 후처리 과정에서 인스턴스 마스크와 병합된다. 이 2단계(two-stage) 방식은 각 단계가 전문화되어 있어 높은 정확도를 달성하기에 유리하지만, 별도의 제안 생성 단계가 필요하여 계산적으로 비효율적이고 추론 속도가 느린 경향이 있다.3</li>
<li><strong>상향식 (Bottom-Up) 접근법:</strong> Panoptic-DeepLab이 채택한 이 방식은 제안(proposal) 없이 단일 샷(single-shot)으로 문제를 해결한다. 먼저 이미지의 모든 픽셀에 대해 의미론적 레이블을 예측한다. 그 다음, ‘thing’ 클래스로 예측된 픽셀들을 특정 기준(예: 기하학적 중심점)에 따라 그룹화하여 개별 인스턴스를 형성한다.15 이 방식은 전체 과정을 단일 네트워크에서 한 번에 처리하므로 개념적으로 더 간단하고 잠재적으로 훨씬 빠르다는 장점을 가진다.18</li>
</ul>
<h3>2.2  Panoptic-DeepLab의 핵심 철학: Simple, Strong, and Fast</h3>
<p>Panoptic-DeepLab의 개발 목표는 논문 제목에 명시된 바와 같이 ‘단순하고(Simple), 강력하며(Strong), 빠른(Fast)’ 상향식 기준 모델(baseline)을 수립하는 것이었다.14 이는 당시의 지배적인 통념에 도전하는 것이었다. Panoptic-DeepLab이 제안될 당시, 상향식 접근법은 하향식에 비해 성능이 떨어진다는 인식이 널리 퍼져 있었다.15</p>
<ul>
<li><strong>Simple:</strong> Panoptic-DeepLab은 복잡한 다단계 파이프라인 대신, 의미론적 분할과 인스턴스 중심 예측이라는 두 개의 평행한 분기를 갖는 단일 네트워크 구조를 제안했다. 이는 이해하고 구현하기에 상대적으로 용이하다.</li>
<li><strong>Strong:</strong> 단순함에도 불구하고, Panoptic-DeepLab은 발표 당시 가장 경쟁이 치열한 벤치마크 중 하나인 Cityscapes에서 의미론적 분할(mIoU), 인스턴스 분할(AP), 파놉틱 분할(PQ) 세 가지 부문 모두에서 1위를 차지하며 SOTA(State-of-the-Art)를 달성했다.15 이는 상향식 접근법이 정확도 측면에서도 하향식에 필적하거나 능가할 수 있음을 최초로 입증한 사례였다.</li>
<li><strong>Fast:</strong> MobileNetV3와 같은 경량 백본과 결합했을 때, 고해상도 이미지(<code>1025×2049</code>)에 대해 초당 15.8 프레임(FPS)이라는 거의 실시간에 가까운 추론 속도를 달성했다.14 이는 자율 주행과 같이 빠른 응답성이 요구되는 응용 분야에 파놉틱 분할을 적용할 수 있는 현실적인 가능성을 보여주었다.</li>
</ul>
<p>이처럼 Panoptic-DeepLab은 ‘단순성’, ‘정확도’, ’속도’라는 세 가지 가치를 성공적으로 조화시켰다. 이는 단순히 하나의 우수한 모델을 제시한 것을 넘어, 상향식 접근법이라는 대안적 연구 방향의 잠재력을 증명하고, 후속 연구들이 따라야 할 명확한 벤치마크를 제시함으로써 전체 연구 생태계에 큰 영향을 미쳤다. 복잡한 후속 연구들은 최소한 Panoptic-DeepLab이 제시한 성능-효율 트레이드오프를 넘어서야만 그 가치를 인정받을 수 있게 된 것이다.</p>
<h2>3.  아키텍처 심층 분석: 듀얼 헤드(Dual-Head) 설계의 원리</h2>
<p>Panoptic-DeepLab의 아키텍처는 ’단순하고 강력하다’는 철학을 구조적으로 구현한 결과물이다. 그 핵심에는 공유 인코더를 기반으로 의미론적 분할과 인스턴스 분할을 위한 두 개의 독립적인 경로, 즉 ‘듀얼(dual)’ 또는 ‘분리된(decoupled)’ 헤드를 두는 설계가 있다.15 이 구조는 두 과업이 서로 밀접하게 연관되어 있지만, 최적의 특징 표현을 위해서는 각기 다른 정보가 필요하다는 통찰에 기반한다. 공유 인코더로 공통의 기반을 다진 후, 각 과업에 특화된 경로로 필요한 정보를 정제하게 함으로써, 과업 간의 잠재적 간섭(interference)을 최소화하고 전체 성능을 극대화하는 매우 효율적인 다중 과업 학습(multi-task learning) 전략을 보여준다.</p>
<h3>3.1  공유 인코더 백본</h3>
<p>네트워크의 기반이 되는 인코더 백본은 입력 이미지로부터 저수준 및 고수준 특징을 추출하는 역할을 한다. 이 백본은 의미론적 분할과 인스턴스 분할 두 과업 모두를 위해 공유되어 계산 효율성을 높인다.15 Panoptic-DeepLab은 DeepLab 계열 모델의 특징적인 설계를 계승하여, 백본의 마지막 블록에 atrous convolution(팽창 컨볼루션)을 적용한다.15 이를 통해 표준적인 CNN에서 발생하는 공간 해상도의 급격한 감소를 막고, 더 조밀한(denser) 고해상도 특징 맵을 추출할 수 있다. 이는 픽셀 단위의 정교한 예측이 요구되는 분할 과업에 매우 유리하다. 또한, Panoptic-DeepLab은 ResNet, Xception, HRNet, MobileNetV3 등 다양한 백본 네트워크를 지원하여, 사용자가 특정 응용 분야의 요구에 맞춰 성능과 효율성 사이의 균형을 선택할 수 있도록 설계되었다.13</p>
<h3>3.2  핵심 혁신: 듀얼 ASPP 및 듀얼 디코더</h3>
<p>Panoptic-DeepLab 아키텍처의 가장 핵심적인 혁신은 인코더에서 추출된 공유 특징 맵을 두 개의 독립적인 경로로 분기하여 처리하는 듀얼 헤드 구조에 있다.15 이는 의미론적 분할(주로 ‘stuff’ 처리)과 인스턴스 분할(주로 ‘thing’ 처리)이 서로 다른 종류의 문맥 정보와 공간적 정교함을 요구한다는 가정에 근거한다.</p>
<ul>
<li><strong>듀얼 ASPP (Atrous Spatial Pyramid Pooling):</strong> 공유 인코더의 출력은 의미론적 분할 분기와 인스턴스 분할 분기를 위한 두 개의 별도 ASPP 모듈로 입력된다.15 ASPP는 서로 다른 팽창률(dilation rate)을 가진 여러 개의 병렬 atrous convolution을 사용하여 다양한 스케일의 문맥 정보를 효과적으로 포착하는 모듈이다. ASPP를 분리함으로써, 의미론적 분할 분기는 넓은 영역의 텍스처와 전역적 문맥을 파악하는 데 집중하고, 인스턴스 분할 분기는 객체의 경계와 형태를 파악하는 데 더 적합한 문맥 정보를 독립적으로 학습할 수 있다.</li>
<li><strong>듀얼 디코더:</strong> 각 ASPP 모듈의 출력은 다시 각 과업에 특화된 경량 디코더로 전달된다.15 이 디코더의 역할은 저수준 특징 맵과 고수준 특징 맵을 결합하여 점진적으로 공간 해상도를 복원하고, 최종 예측 맵을 생성하는 것이다. Panoptic-DeepLab은 DeepLabV3+의 디코더 구조를 개선하여 사용하며, 각 업샘플링 단계에서 계산 효율성이 높은 깊이별 분리 가능 컨볼루션(depthwise-separable convolution)을 적용한다.13 디코더를 분리함으로써, 각 과업은 자신의 목표에 가장 적합한 방식으로 공간 정보를 복원하고 특징을 정제할 수 있다.</li>
</ul>
<p>이러한 ‘느슨한 결합(loosely coupled)’ 방식은 모든 파라미터를 공유하는 것(hard parameter sharing)과 두 개의 완전히 독립된 네트워크를 사용하는 것 사이의 현명한 절충안이다. 이는 Panoptic-DeepLab이 ’단순함’을 유지하면서도 ‘강력한’ 성능을 낼 수 있었던 구조적 비결이며, 효율적인 다중 과업 학습 아키텍처 설계의 좋은 선례를 남겼다.</p>
<table><thead><tr><th>구성 요소 (Component)</th><th>역할 (Role)</th><th>핵심 기술 (Key Technology)</th></tr></thead><tbody>
<tr><td>인코더 백본 (Encoder Backbone)</td><td>의미론적/인스턴스 분할을 위한 공통 특징 추출</td><td>Atrous Convolution, 다양한 백본 지원 (ResNet, HRNet 등)</td></tr>
<tr><td>듀얼 ASPP (Dual ASPP)</td><td>각 과업에 특화된 다중 스케일 문맥 정보 포착</td><td>Atrous Spatial Pyramid Pooling</td></tr>
<tr><td>듀얼 디코더 (Dual Decoder)</td><td>각 과업에 최적화된 방식으로 공간 해상도 복원</td><td>점진적 업샘플링, Depthwise-Separable Convolution</td></tr>
<tr><td>의미론적 분할 헤드 (Semantic Head)</td><td>모든 픽셀에 ‘thing’ 및 ‘stuff’ 클래스 레이블 예측</td><td>1x1 Convolution</td></tr>
<tr><td>인스턴스 분할 헤드 (Instance Head)</td><td>‘thing’ 객체의 중심 히트맵 및 중심 오프셋 벡터 예측</td><td>1x1 Convolution</td></tr>
</tbody></table>
<h3>3.3  태스크별 예측 헤드</h3>
<p>듀얼 디코더를 통과한 특징 맵은 최종적으로 두 개의 독립적인 예측 헤드로 이어진다. 각 헤드는 특정 과업을 위한 최종 출력을 생성한다.15</p>
<ul>
<li><strong>의미론적 분할 헤드 (Semantic Segmentation Head):</strong> 이 헤드는 이미지의 모든 픽셀에 대해 ’thing’과 ‘stuff’ 클래스를 포함한 전체 의미론적 클래스 중 하나를 예측하는 분류 작업을 수행한다. 출력은 이미지와 동일한 해상도를 가지는 의미론적 분할 맵이다.15</li>
<li><strong>인스턴스 분할 헤드 (Instance Segmentation Head):</strong> 이 헤드는 클래스와 무관하게(class-agnostic) ‘thing’ 객체들을 분리하는 데 필요한 정보를 예측한다. 이를 위해 두 가지 출력을 동시에 생성한다 15:</li>
</ul>
<ol>
<li><strong>인스턴스 중심 예측 (Instance Center Prediction):</strong> 각 ‘thing’ 인스턴스의 질량 중심(center of mass)이 존재할 확률을 나타내는 2D 히트맵(heatmap)을 예측한다. 이 히트맵은 각 인스턴스의 중심 위치에서 높은 활성화 값을 갖는 봉우리(peak) 형태를 띤다.</li>
<li><strong>중심 오프셋 회귀 (Center Offset Regression):</strong> 각 ‘thing’ 픽셀에 대해, 해당 픽셀이 속한 인스턴스의 중심점까지의 2D 변위 벡터(offset vector)를 예측한다. 이 오프셋 정보는 후처리 과정에서 각 픽셀을 올바른 인스턴스 중심으로 그룹화하는 데 사용된다.</li>
</ol>
<h2>4.  학습 전략: 손실 함수와 최적화</h2>
<p>Panoptic-DeepLab은 세 가지 개별적인 손실 함수의 가중합으로 구성된 단일 목적 함수를 통해 종단간(end-to-end) 방식으로 학습된다.15 이 손실 함수의 설계는 모델이 각 하위 과업을 효과적으로 학습하도록 유도하며, 특히 학습하기 어려운 예제에 더 집중하는 전략을 일관되게 보여준다. 이는 제한된 학습 자원을 가장 정보량이 많은 픽셀과 영역에 집중시켜, 효율적으로 전체 성능을 끌어올리려는 영리한 최적화 전략이다.</p>
<h3>4.1  전체 손실 함수</h3>
<p>네트워크의 전체 손실 함수 <span class="math math-inline">L</span>은 의미론적 분할 손실(<span class="math math-inline">L_{sem}</span>), 인스턴스 중심 히트맵 손실(<span class="math math-inline">L_{heatmap}</span>), 그리고 중심 오프셋 회귀 손실(<span class="math math-inline">L_{offset}</span>)의 선형 결합으로 정의된다. 각 손실 항에는 하이퍼파라미터인 가중치 <span class="math math-inline">\lambda</span>가 곱해져 각 손실의 상대적 중요도를 조절한다.15<br />
<span class="math math-display">
L = \lambda_{sem}L_{sem} + \lambda_{heatmap}L_{heatmap} + \lambda_{offset}L_{offset}
</span></p>
<h3>4.2  의미론적 분할 손실 (<span class="math math-inline">L_{sem}</span>)</h3>
<p>의미론적 분할을 위해서는 가중 부트스트랩 교차 엔트로피 손실(weighted bootstrapped cross entropy loss)이 사용된다.15 표준 교차 엔트로피 손실은 모든 픽셀을 동등하게 취급하지만, 이 방식은 학습 과정에서 모델이 예측하기 어려워하는 ’어려운 픽셀(hard pixels)’에 더 높은 가중치를 부여하여 학습을 집중시킨다.</p>
<p>특히 Panoptic-DeepLab은 작은 객체에 대한 분할 성능을 향상시키기 위해 추가적인 가중치 전략을 사용한다. 인스턴스의 면적이 <span class="math math-inline">64 \times 64</span> 픽셀보다 작은 경우, 해당 인스턴스에 속한 픽셀들의 손실 가중치 <span class="math math-inline">\lambda_{sem}</span>을 3으로 설정하고, 그 외의 모든 픽셀에는 1을 부여한다.15 이는 모델이 학습 과정에서 자연스럽게 손실 기여도가 높은 큰 객체에 편향되는 경향을 교정하려는 명시적인 시도다. 작은 객체는 픽셀 수가 적어 전체 손실에 미치는 영향이 미미할 수 있으므로, 이 가중치를 통해 모델이 작은 객체를 무시하지 않고 학습하도록 강제한다.</p>
<h3>4.3  인스턴스 중심 히트맵 손실 (<span class="math math-inline">L_{heatmap}</span>)</h3>
<p>인스턴스 중심 예측을 학습시키기 위해, 먼저 실제(ground-truth) 인스턴스 중심 위치를 2D 히트맵으로 변환한다. 각 중심점은 표준편차 8픽셀을 갖는 2D 가우시안 분포로 인코딩되어, 중심에서 가장 높은 값을 갖고 주변으로 갈수록 점차 감소하는 부드러운 봉우리 형태를 만든다.15</p>
<p>네트워크가 예측한 히트맵과 이렇게 생성된 실제 히트맵 간의 평균 제곱 오차(Mean Squared Error, MSE)를 손실 함수로 사용한다.15 이는 예측 히트맵이 실제 히트맵의 가우시안 형태를 모방하도록 유도하며, 결과적으로 모델이 각 인스턴스의 중심을 명확하고 뾰족한 단일 지점으로 예측하도록 강제하는 효과가 있다. 다른 손실들과의 스케일 균형을 맞추기 위해, 이 손실 항의 가중치 <span class="math math-inline">\lambda_{heatmap}</span>은 200이라는 비교적 큰 값으로 설정된다.15</p>
<h3>4.4  중심 오프셋 회귀 손실 (<span class="math math-inline">L_{offset}</span>)</h3>
<p>중심 오프셋 회귀는 ‘thing’ 클래스에 속하는 픽셀에 대해서만 계산된다.15 이는 오프셋이라는 개념 자체가 무의미한 ‘stuff’ 배경 영역에 대해 네트워크가 불필요한 값을 학습하느라 계산 자원을 낭비하는 것을 방지한다.</p>
<p>손실 함수로는 예측된 오프셋 벡터와 실제 오프셋 벡터 간의 L1 손실이 사용된다.15 L1 손실은 MSE(L2 손실)에 비해 이상치(outlier)에 덜 민감하여, 일부 픽셀에서 오프셋 예측이 크게 빗나가더라도 전체 학습 과정이 불안정해지는 것을 막아준다. 이 손실 항의 가중치 <span class="math math-inline">\lambda_{offset}</span>은 0.01로 설정된다.15</p>
<h2>5.  추론 및 후처리: 분할 결과의 융합</h2>
<p>Panoptic-DeepLab은 학습된 네트워크를 통해 세 가지 예측(의미론적 분할, 인스턴스 중심 히트맵, 중심 오프셋)을 생성한 후, 매우 간단하고 효율적인 후처리 과정을 거쳐 최종 파놉틱 분할 결과를 도출한다. 이 후처리 과정은 모델의 ‘Simple and Fast’ 철학을 끝까지 유지하는 핵심적인 설계 요소다. 복잡한 휴리스틱이나 반복적인 최적화 과정 없이, 간단한 기하학적 그룹핑과 투표만으로 두 개의 독립적인 예측을 일관성 있는 최종 출력으로 통합한다. 이는 모델의 실용성을 크게 높이며, 특히 후처리 시간이 네트워크 추론 시간보다 길어지는 병목 현상을 방지했다는 점에서 주목할 만하다.13</p>
<h3>5.1  인스턴스 중심점 탐지</h3>
<p>추론 단계의 첫 번째 과정은 예측된 인스턴스 중심 히트맵에서 개별 인스턴스를 대표하는 중심점들을 식별하는 것이다. 이를 위해 히트맵에 비최대 억제(Non-Maximum Suppression, NMS)를 적용한다.15 NMS는 본질적으로 히트맵에 최대 풀링(max pooling)을 적용하고, 풀링 전후에 값이 변하지 않는 위치, 즉 국소적 최댓값(local maxima)만을 유지하는 것과 같다. 이 과정을 통해 주변보다 확연히 높은 활성화 값을 갖는 위치들만 남게 된다. 마지막으로, 신뢰도가 낮은 예측을 제거하기 위해 임계값(예: 0.1)을 적용하고, 신뢰도 점수가 가장 높은 상위 <span class="math math-inline">k</span>개(예: 200개)의 위치만을 최종 인스턴스 중심점 후보 목록 <span class="math math-inline">C_k = \{(i_k, j_k)\}</span>로 선택한다.15</p>
<h3>5.2  픽셀 그룹화</h3>
<p>다음으로, 의미론적 분할 예측을 통해 ‘thing’ 클래스로 분류된 각 픽셀을 이전에 탐지된 인스턴스 중심점 중 하나에 할당한다. 픽셀 <span class="math math-inline">(i, j)</span>가 주어졌을 때, 오프셋 회귀 헤드는 해당 픽셀로부터 소속 인스턴스의 중심까지의 2D 변위 벡터 <span class="math math-inline">O(i, j)</span>를 예측한다. 이 오프셋 벡터를 현재 픽셀 위치에 더하면, 예상되는 인스턴스 중심 위치 <span class="math math-inline">((i, j) + O(i, j))</span>를 계산할 수 있다.15</p>
<p>이제 이 예상 중심 위치에서 유클리드 거리가 가장 가까운 인스턴스 중심점 후보 <span class="math math-inline">C_k</span>를 찾아, 해당 픽셀 <span class="math math-inline">(i, j)</span>를 <span class="math math-inline">k</span>번째 인스턴스에 할당한다. 이 그룹화 과정은 다음의 최근접 이웃 탐색 수식으로 요약될 수 있다 15:<br />
<span class="math math-display">
\hat{k}_{i,j} = \operatorname{argmin}_k \Vert C_k - ((i, j) + O(i, j)) \Vert_2
</span><br />
여기서 <span class="math math-inline">\hat{k}_{i,j}</span>는 픽셀 <span class="math math-inline">(i, j)</span>에 할당된 예측 인스턴스 ID이다. 이 과정을 모든 ‘thing’ 픽셀에 대해 반복하면, 클래스 정보는 없지만 각 픽셀이 어떤 인스턴스에 속하는지를 나타내는 클래스 불가지론적(class-agnostic) 인스턴스 분할 맵이 생성된다. ’stuff’로 예측된 픽셀들은 이 과정에서 제외된다.15</p>
<h3>5.3  최종 융합: 과반수 투표</h3>
<p>마지막 단계는 지금까지 생성된 두 개의 독립적인 결과, 즉 (1) 모든 픽셀에 대한 의미론적 레이블 맵과 (2) ‘thing’ 픽셀에 대한 인스턴스 ID 맵을 하나의 일관된 파놉틱 분할 맵으로 융합하는 것이다.</p>
<p>Panoptic-DeepLab은 이를 위해 매우 효율적인 ‘과반수 투표(majority voting)’ 방식을 사용한다.15 각 예측된 인스턴스 마스크(예: 인스턴스 ID가 <span class="math math-inline">k</span>인 모든 픽셀의 집합)에 대해, 해당 마스크 내에 포함된 픽셀들의 의미론적 레이블 예측값을 조사한다. 그리고 그중 가장 빈번하게 나타난(과반수를 차지한) 의미론적 클래스를 해당 인스턴스 <span class="math math-inline">k</span>의 최종 클래스로 결정한다. 예를 들어, 1번 인스턴스 마스크 내 픽셀들의 90%가 ’자동차’로 예측되었다면, 1번 인스턴스는 ’자동차’로 최종 분류된다.</p>
<p>이 과반수 투표 연산은 본질적으로 각 인스턴스 마스크별로 클래스 레이블의 히스토그램을 계산하는 것과 같으므로, GPU에서 매우 효율적으로 병렬 처리가 가능하다. 논문에 따르면 <span class="math math-inline">1025 \times 2049</span> 해상도의 이미지에 대해 이 병합 과정은 단 3ms밖에 소요되지 않아, 전체 추론 속도에 거의 영향을 미치지 않는다.15</p>
<h2>6.  성능 평가 및 벤치마크 분석</h2>
<p>모델의 성능은 객관적인 벤치마크 데이터셋을 통해 평가된다. Panoptic-DeepLab은 특히 도시 풍경 데이터셋인 Cityscapes에서 압도적인 성능을 보이며 그 우수성을 입증했지만, 더 다양하고 복잡한 COCO 데이터셋에서는 다른 양상을 보였다. 이 두 벤치마크에서의 성능 차이는 모델의 특성을 깊이 있게 이해하는 중요한 단서를 제공한다. 이는 상향식 접근법이 구조화된 장면에 얼마나 강점을 보이는지, 그리고 복잡한 장면에 대한 일반화에는 어떤 한계를 갖는지를 명확히 보여주는 증거다.</p>
<h3>6.1  Cityscapes 벤치마크</h3>
<p>Cityscapes는 자율 주행 시나리오의 도시 거리 풍경 이미지로 구성된 데이터셋으로, 파놉틱 분할 연구에서 가장 중요한 벤치마크 중 하나로 꼽힌다.20 이 데이터셋은 도로, 건물, 하늘과 같은 ’stuff’와 자동차, 보행자, 자전거와 같은 ’thing’이 비교적 명확하게 구분되고 구조화된 장면을 담고 있다.</p>
<p>Panoptic-DeepLab은 발표 당시 Cityscapes 테스트 세트에서 세 가지 주요 평가 지표 모두에서 1위를 차지하며 SOTA(State-of-the-Art) 성능을 기록하는 기염을 토했다.14 이는 상향식 접근법의 잠재력을 시장에 각인시킨 결정적인 성과였다. 단일 모델, 그리고 각 태스크에 대한 별도의 미세 조정 없이 달성한 공식 기록은 다음과 같다 14:</p>
<ul>
<li><strong>파놉틱 품질 (Panoptic Quality, PQ):</strong> 65.5%</li>
<li><strong>평균 정밀도 (Average Precision, AP):</strong> 39.0%</li>
<li><strong>평균 IoU (mean Intersection-over-Union, mIoU):</strong> 84.2%</li>
</ul>
<p>또한, HRNet-w48과 같은 더 강력한 백본을 사용했을 때는 PQ 63.4%를, MobileNetV3와 같은 경량 백본을 사용했을 때도 54.1%의 경쟁력 있는 PQ를 달성하며 다양한 요구사항에 대응할 수 있는 유연성을 보여주었다.13</p>
<h3>6.2  COCO 벤치마크</h3>
<p>COCO(Common Objects in Context)는 80개의 ‘thing’ 클래스와 53개의 ‘stuff’ 클래스를 포함하며, 일상생활의 다양한 장면을 담고 있어 Cityscapes보다 훨씬 더 어렵고 일반적인 성능을 측정하는 벤치마크다.21 객체의 종류, 크기, 개수, 배치, 그리고 서로 겹치는 정도가 매우 다양하고 예측 불가능하다.24</p>
<p>COCO test-dev 세트에서 Panoptic-DeepLab은 여러 하향식 접근법과 대등한(on par) 수준의 성능을 보이며 상향식 방법의 가능성을 다시 한번 입증했다.15 Papers with Code 리더보드에 따르면, Xception-71 백본을 사용한 Panoptic-DeepLab 모델은 **PQ 41.4%**를 기록했으며, ’thing’에 대한 품질(<span class="math math-inline">PQ_{th}</span>)은 45.1%, ’stuff’에 대한 품질(<span class="math math-inline">PQ_{st}</span>)은 35.9%였다.25 이후 SWideRNet이라는 더 강력한 백본을 사용한 개선 버전은 **PQ 46.5%**까지 성능을 끌어올렸다.25</p>
<h3>6.3  성능 비교 분석</h3>
<p>Panoptic-DeepLab의 성능을 현재의 기술 수준에서 평가하기 위해, 동시대의 다른 접근법 및 최신 Transformer 기반 모델들과의 성능을 비교하는 것은 매우 중요하다. 아래 표는 COCO test-dev 벤치마크에서의 주요 모델들 성능을 요약한 것이다.</p>
<table><thead><tr><th>모델 (Model)</th><th>백본 (Backbone)</th><th>PQ (%)</th><th><span class="math math-inline">PQ_{th}</span> (%)</th><th><span class="math math-inline">PQ_{st}</span> (%)</th></tr></thead><tbody>
<tr><td>Panoptic FPN</td><td>-</td><td>40.9</td><td>48.3</td><td>29.7</td></tr>
<tr><td>UPSNet</td><td>ResNet-101-FPN</td><td>46.6</td><td>53.2</td><td>36.7</td></tr>
<tr><td><strong>Panoptic-DeepLab</strong></td><td>Xception-71</td><td>41.4</td><td>45.1</td><td>35.9</td></tr>
<tr><td><strong>Panoptic-DeepLab</strong></td><td>SWideRNet</td><td>46.5</td><td>52.0</td><td>38.2</td></tr>
<tr><td>kMaX-DeepLab</td><td>-</td><td>58.5</td><td>64.8</td><td>49.0</td></tr>
<tr><td>Mask2Former</td><td>Swin-L</td><td>58.3</td><td>65.1</td><td>48.1</td></tr>
</tbody></table>
<p>표의 데이터는 25에서 취합됨.</p>
<p>이 표는 몇 가지 중요한 사실을 보여준다. 첫째, Panoptic-DeepLab은 동시대의 하향식 모델인 UPSNet과 유사한 성능을 보이며 상향식 접근법의 경쟁력을 입증했다. 둘째, 최신 Transformer 기반 모델인 kMaX-DeepLab과 Mask2Former는 Panoptic-DeepLab 대비 PQ 점수에서 10%p 이상의 압도적인 성능 우위를 보인다.</p>
<p>이러한 성능 차이는 데이터셋의 특성과 모델 아키텍처의 근본적인 차이에서 기인한다. Cityscapes와 같이 구조화된 장면에서는 픽셀을 중심으로 그룹핑하는 Panoptic-DeepLab의 방식이 매우 효과적이다. 객체들이 비교적 잘 분리되어 있고 배경과 명확히 구분되기 때문이다. 그러나 COCO처럼 수많은 객체가 복잡하게 얽혀있고(occlusion), 같은 종류의 객체가 떼로 몰려있는(dense scenes) 장면에서는, 픽셀 단위의 지역적인 오프셋 정보만으로는 정확한 인스턴스를 구분하기 어렵다. 이는 상향식 방법의 고질적인 한계로 지적된다.17</p>
<p>반면, Mask2Former와 같은 최신 모델들은 ’object query’라는 개념을 사용하여 각 인스턴스를 전역적인 문맥 속에서 독립적으로 예측한다.27 이 방식은 이미지 전체의 정보를 활용하여 각 객체를 분리해내므로, 복잡하고 중첩이 심한 COCO와 같은 장면에 훨씬 더 강건한 성능을 보인다. 결국, 두 벤치마크에서의 성능 차이는 단순한 수치 비교를 넘어, 각 아키텍처 패러다임이 어떤 종류의 문제에 더 적합한지를 보여주는 질적인 차이를 드러낸다. Panoptic-DeepLab은 ’구조화된 장면’에 최적화된 효율적인 솔루션이었고, COCO는 그 한계를 드러내며 새로운 패러다임(Transformer)의 등장을 촉진하는 계기가 되었다.</p>
<h2>7.  종합 평가: Panoptic-DeepLab의 강점, 한계, 그리고 영향</h2>
<p>Panoptic-DeepLab은 파놉틱 분할의 역사에서 중요한 위치를 차지하는 모델이다. 이 모델은 단순히 한때 SOTA를 기록했던 수많은 모델 중 하나가 아니라, 특정 기술적 패러다임의 정점을 보여주고 다음 세대 기술의 필요성을 역설적으로 증명한 ’시대적 전환점’으로서의 의미를 갖는다. Panoptic-DeepLab의 강점과 한계를 종합적으로 평가하고, 그것이 학계와 산업계에 미친 영향을 분석하는 것은 현재의 파놉틱 분hal 기술을 이해하는 데 필수적이다.</p>
<h3>7.1  강점</h3>
<ul>
<li><strong>단순성과 효율성 (Simplicity and Efficiency):</strong> Panoptic-DeepLab의 가장 큰 미덕은 그 구조적 단순함과 이로 인한 높은 효율성이다. 복잡한 객체 제안 생성 단계가 없는 단일 샷(single-shot) 상향식 모델로서, 개념적으로 이해하고 구현하기가 상대적으로 용이하다.15 이러한 구조적 간결함은 빠른 추론 속도로 직결되어, MobileNetV3 백본 사용 시 고해상도 이미지에서도 거의 실시간 처리가 가능함을 보여주었다.15 이는 파놉틱 분할 기술이 연구실을 넘어 실제 산업 현장에 적용될 수 있다는 가능성을 제시한 중요한 성과다.</li>
<li><strong>강력한 성능의 기준 모델 (Strong Performance Baseline):</strong> Panoptic-DeepLab은 단순함에도 불구하고, 발표 당시 Cityscapes와 같은 주요 벤치마크에서 기존의 복잡한 하향식 모델들을 모두 능가하는 SOTA 성능을 달성했다.14 이는 ’상향식 접근법은 정확도가 떨어진다’는 당시의 통념을 깨뜨린 충격적인 결과였으며, 상향식 방법론의 잠재력을 재평가하게 만드는 계기가 되었다. 이로써 Panoptic-DeepLab은 이후 등장하는 모든 파놉틱 분할 모델들이 비교해야 할 강력하고 효율적인 기준 모델(strong baseline)로 확고히 자리 잡았다.</li>
<li><strong>통합된 아키텍처 (Unified Architecture):</strong> 단일 네트워크 내에서 의미론적 분할과 인스턴스 분할을 위한 특징을 공유하고, 각 과업에 특화된 헤드를 통해 동시에 예측을 수행하는 구조는 매우 효율적이다.15 두 과업을 완전히 별개로 처리한 후 결과를 합치는 방식에 비해, 특징 학습 과정에서 상호 보완적인 정보를 활용할 수 있으며, 종단간 학습을 통해 전체 파이프라인을 한 번에 최적화할 수 있다는 장점이 있다.</li>
</ul>
<h3>7.2  한계</h3>
<ul>
<li><strong>밀집 및 중첩 객체 처리의 어려움 (Difficulty with Dense and Occluded Objects):</strong> Panoptic-DeepLab이 채택한 픽셀 그룹핑 방식은 상향식 접근법의 고질적인 한계를 그대로 물려받았다. 객체들이 서로를 가리거나(occlusion) 빽빽하게 모여 있는(dense scenes) 복잡한 상황에서는, 각 픽셀을 올바른 인스턴스 중심으로 할당하는 것이 매우 어려워진다. 이로 인해 서로 다른 인스턴스들이 하나의 덩어리로 잘못 합쳐지는 ‘인스턴스 유착(instance adhesion)’ 현상이 발생하기 쉽다.17 이러한 한계는 특히 COCO 벤치마크에서 최신 모델들과의 성능 격차로 명확히 드러났다.</li>
<li><strong>최신 아키텍처 대비 성능 한계 (Performance Gap with Newer Architectures):</strong> COCO 벤치마크 성능 비교에서 확인했듯이, Transformer 아키텍처를 기반으로 한 쿼리 중심 모델들(예: Mask2Former, kMaX-DeepLab)은 Panoptic-DeepLab보다 월등한 성능을 보인다.25 Transformer의 셀프 어텐션 메커니즘은 이미지 전체의 전역적인 문맥을 효과적으로 포착할 수 있어, 지역적인 정보에 주로 의존하는 CNN 기반의 Panoptic-DeepLab보다 복잡한 객체 간의 관계를 파악하고 분리하는 데 훨씬 유리하다.</li>
<li><strong>구현 및 재현의 어려움 (Implementation and Reproducibility Challenges):</strong> Panoptic-DeepLab의 공식 PyTorch 구현이 제공되기는 했지만 13, 일부 재현성 연구에서는 논문의 설명이 다소 고수준으로 기술되어 있어 상당한 사전 도메인 지식이 요구되며, 모델을 처음부터 학습시키기 위해서는 최소 4-8개의 고성능 GPU와 같은 상당한 컴퓨팅 자원이 필요하다는 점이 한계로 지적되기도 했다.29</li>
</ul>
<h3>7.3  학계 및 산업계에 미친 영향</h3>
<p>Panoptic-DeepLab의 유산은 현재 SOTA 모델이 아니라는 사실에도 불구하고 매우 중요하고 광범위하다. 이 모델은 ’성능의 정점’이 아니라 ’효율적인 패러다임의 정점’을 대표하며, 기술 발전의 중요한 다리 역할을 수행했다.</p>
<ul>
<li><strong>상향식 연구 활성화:</strong> Panoptic-DeepLab의 압도적인 성공은 한동안 주춤했던 상향식 파놉틱 분할에 대한 연구를 다시 활성화시키는 기폭제가 되었다.15 이 모델이 제시한 ’인스턴스 중심 예측 + 오프셋 회귀’라는 간결한 프레임워크는 이후 많은 상향식 모델들의 기본 구조로 채택되거나 변형되어 사용되었다.</li>
<li><strong>DeepLab 계열의 확장:</strong> Panoptic-DeepLab은 의미론적 분할 분야에서 막강한 영향력을 가졌던 DeepLab 브랜드를 파놉틱 분할이라는 새로운 영역으로 성공적으로 확장시킨 사례다. 이는 DeepLab 프레임워크의 유연성과 확장성을 입증했으며, 이후 CMT-DeepLab 28, kMaX-DeepLab 25, Axial-DeepLab 16 등 DeepLab의 이름을 계승한 혁신적인 후속 연구들이 등장하는 기반을 마련했다.</li>
<li><strong>비디오로의 확장:</strong> Panoptic-DeepLab의 견고하고 효율적인 설계는 정지 이미지를 넘어 비디오 영역으로 확장되는 데 중요한 발판이 되었다. SOTA 비디오 파놉틱 분할 모델인 ViP-DeepLab과 VPS-Transformer는 모두 Panoptic-DeepLab을 기본 아키텍처로 채택하고, 여기에 시간적 일관성을 모델링하기 위한 모듈을 추가하는 방식으로 개발되었다.16 이는 Panoptic-DeepLab의 설계 원리가 시공간적 차원으로 확장될 수 있는 강력한 잠재력을 가졌음을 보여주는 명백한 증거다.</li>
</ul>
<p>결론적으로, Panoptic-DeepLab이 드러낸 명확한 한계점(예: 밀집 객체 처리의 어려움)은 역설적으로 연구 커뮤니티에게 “이 문제를 해결하기 위해서는 지역적인 컨볼루션 연산을 넘어선, 더 전역적인 상호작용을 모델링할 수 있는 새로운 메커니즘이 필요하다“는 강력한 신호를 보냈다. 그리고 그 해답은 Transformer 아키텍처에서 나왔다. 이처럼 Panoptic-DeepLab은 CNN 기반 상향식 접근법이라는 한 시대를 성공적으로 마무리하고, 다음 시대가 풀어야 할 숙제를 명확히 정의해준 ’촉매’로서의 역할을 충실히 수행했다고 평가할 수 있다.</p>
<h2>8.  결론: 상향식 파놉틱 분할의 초석을 다지다</h2>
<p>Panoptic-DeepLab은 컴퓨터 비전의 통합적 장면 이해라는 목표를 향한 여정에서 중요한 이정표를 세운 모델이다. 이 모델은 ‘단순하고(Simple), 강력하며(Strong), 빠른(Fast)’ 상향식 접근법이라는 명확한 철학을 바탕으로, 당시 파놉틱 분할 분야를 지배하던 복잡하고 느린 하향식 방법론에 대한 강력한 대안을 제시했다.14</p>
<p>듀얼 ASPP 및 듀얼 디코더로 대표되는 독창적인 듀얼 헤드 아키텍처는 의미론적 분할과 인스턴스 분할이라는 두 하위 과업의 특성을 모두 존중하면서 효율적으로 통합하는 방법을 보여주었다.15 또한, 기하학적 그룹핑과 과반수 투표에 기반한 지극히 단순하고 빠른 후처리 과정은 네트워크의 효율성을 최종 결과물까지 일관되게 유지하는 영리한 설계였다.15 이러한 혁신을 통해 Panoptic-DeepLab은 Cityscapes 벤치마크에서 SOTA를 달성하며 상향식 접근법의 성능적 가능성을 입증했고, 더 어려운 COCO 벤치마크에서도 경쟁력 있는 성능을 보이며 파놉틱 분할 연구를 위한 견고한 기준 모델을 수립했다.15</p>
<p>물론, 기술의 발전과 함께 Transformer 기반의 쿼리 중심 모델들이 등장하면서 Panoptic-DeepLab은 최고 성능의 자리에서 내려왔다. 특히 복잡하고 중첩이 심한 장면에서의 성능 한계는 명확하다. 그러나 이 모델의 가치는 단순히 특정 시점의 성능 수치에 국한되지 않는다. Panoptic-DeepLab은 효율성과 단순성이라는 실용적 가치를 파놉틱 분할 연구의 중심에 다시 가져왔으며, 그 견고한 설계는 비디오 파놉틱 분할과 같은 새로운 분야로 연구가 확장되는 데 결정적인 기반을 제공했다.16</p>
<p>결론적으로, Panoptic-DeepLab은 한 시대의 기술적 정점을 보여줌과 동시에 다음 시대로의 전환을 촉진한, 파놉틱 분할 분야의 발전에 있어 빼놓을 수 없는 초석(cornerstone)과 같은 모델로 평가받아야 마땅하다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>Understanding Panoptic Segmentation Basics - Viso Suite, https://viso.ai/deep-learning/panoptic-segmentation/</li>
<li>[1801.00868] Panoptic Segmentation - arXiv, https://arxiv.org/abs/1801.00868</li>
<li>Panoptic Segmentation: Definition, Datasets &amp; Tutorial [2024] - V7 Labs, https://www.v7labs.com/blog/panoptic-segmentation-guide</li>
<li>Guide to Panoptic Segmentation - Encord, https://encord.com/blog/panoptic-segmentation-guide/</li>
<li>Panoptic Segmentation: Unifying Semantic and Instance Segmentation - DigitalOcean, https://www.digitalocean.com/community/tutorials/panoptic-segmentation</li>
<li>Panoptic Segmentation - Cloudinary, https://cloudinary.com/glossary/panoptic-segmentation</li>
<li>What is Panoptic Segmentation? - AI Stack Exchange, https://ai.stackexchange.com/questions/40310/what-is-panoptic-segmentation</li>
<li>Semantic vs. Instance vs. Panoptic Segmentation - PyImageSearch, https://pyimagesearch.com/2022/06/29/semantic-vs-instance-vs-panoptic-segmentation/</li>
<li>www.v7labs.com, <a href="https://www.v7labs.com/blog/panoptic-segmentation-guide#:~:text=Panoptic%20segmentation%20helps%20classify%20objects,%2C%20cars%2C%20animals%2C%20etc.">https://www.v7labs.com/blog/panoptic-segmentation-guide#:~:text=Panoptic%20segmentation%20helps%20classify%20objects,%2C%20cars%2C%20animals%2C%20etc.</a></li>
<li>[1812.01192] Learning to Fuse Things and Stuff - arXiv, https://arxiv.org/abs/1812.01192</li>
<li>Things and stuff or how remote sensing could benefit from panoptic segmentation, https://softwaremill.com/things-and-stuff-or-how-remote-sensing-could-benefit-from-panoptic-segmentation/</li>
<li>Benchmarking the Robustness of Panoptic Segmentation for Automated Driving - arXiv, https://arxiv.org/html/2402.15469v1</li>
<li>bowenc0221/panoptic-deeplab: This is Pytorch re-implementation of our CVPR 2020 paper “Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation” (https://arxiv.org/abs/1911.10194) - GitHub, https://github.com/bowenc0221/panoptic-deeplab</li>
<li>[1911.10194] Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation - arXiv, https://arxiv.org/abs/1911.10194</li>
<li>A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation, https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_Panoptic-DeepLab_A_Simple_Strong_and_Fast_Baseline_for_Bottom-Up_Panoptic_CVPR_2020_paper.pdf</li>
<li>arXiv:2210.03546v1 [cs.CV] 7 Oct 2022, https://arxiv.org/pdf/2210.03546</li>
<li>Panoptic Segmentation Method Based on Feature Fusion and Edge Guidance - MDPI, https://www.mdpi.com/2076-3417/15/9/5152</li>
<li>Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation - Google Research, https://research.google/pubs/panoptic-deeplab-a-simple-strong-and-fast-baseline-for-bottom-up-panoptic-segmentation/</li>
<li>Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation | Request PDF - ResearchGate, https://www.researchgate.net/publication/337485015_Panoptic-DeepLab_A_Simple_Strong_and_Fast_Baseline_for_Bottom-Up_Panoptic_Segmentation</li>
<li>Benchmark Suite - Cityscapes Dataset, https://www.cityscapes-dataset.com/benchmarks/</li>
<li>High Quality Panoptic Segmentation Datasets - maadaa.ai, https://maadaa.ai/Blog/BlogDetail/High-Quality-Panoptic-Segmentation-Datasets</li>
<li>[1910.04751] Panoptic-DeepLab - arXiv, https://arxiv.org/abs/1910.04751</li>
<li>A Supplementary Material, https://proceedings.neurips.cc/paper/2021/file/83a368f54768f506b833130584455df4-Supplemental.pdf</li>
<li>Benchmarking Object Detectors with COCO: A New Path Forward - arXiv, https://arxiv.org/html/2403.18819v1</li>
<li>COCO test-dev Benchmark (Panoptic Segmentation) | Papers With Code, https://paperswithcode.com/sota/panoptic-segmentation-on-coco-test-dev?p=attention-guided-unified-network-for-panoptic</li>
<li>Panoptic Image Segmentation Method Based on Dynamic Instance Query - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC12074491/</li>
<li>Panoptic Image Segmentation Method Based on Dynamic Instance Query - MDPI, https://www.mdpi.com/1424-8220/25/9/2919</li>
<li>CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation - arXiv, https://arxiv.org/abs/2206.08948</li>
<li>[Re] Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation | OpenReview, https://openreview.net/forum?id=rYUxW6fXnRK</li>
<li>Cityscapes-VPS Benchmark (Video Panoptic Segmentation) - Papers With Code, https://paperswithcode.com/sota/video-panoptic-segmentation-on-cityscapes-vps</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>