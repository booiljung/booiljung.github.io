<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:SAM 2 - Segment Anything in Images and Videos</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>SAM 2 - Segment Anything in Images and Videos</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">이미지 분할 (Image Segmentations)</a> / <span>SAM 2 - Segment Anything in Images and Videos</span></nav>
                </div>
            </header>
            <article>
                <h1>SAM 2 - Segment Anything in Images and Videos</h1>
<p>2025-10-12</p>
<h2>1.  이미지 분할을 넘어, 시공간 분할의 새로운 패러다임</h2>
<h3>1.1 배경: Segment Anything Model (SAM)의 의의와 한계</h3>
<p>2023년 발표된 Segment Anything Model (SAM)은 컴퓨터 비전 분야, 특히 이미지 분할(image segmentation) 영역에서 하나의 분기점을 제시했다.1 SAM의 핵심적인 기여는 특정 데이터셋이나 객체 범주에 대한 추가적인 학습 없이, 사용자의 간단한 프롬프트(prompt) — 점(point), 경계 상자(bounding box), 마스크(mask) 등 — 에 반응하여 이미지 내의 어떤 객체든 정교하게 분할해내는 제로샷 일반화(zero-shot generalization) 능력을 입증한 데 있다.1 1100만 개의 이미지와 11억 개의 마스크로 구성된 방대한 SA-1B 데이터셋으로 학습된 SAM은, 이전에 보지 못했던 객체와 이미지에 대해서도 놀라운 분할 성능을 보여주며 ’파운데이션 모델(foundation model)’의 가능성을 시각 영역으로 확장했다.3</p>
<p>그러나 SAM의 혁신적인 능력은 본질적으로 정적인(static) 2차원 이미지에 국한되었다.2 현실 세계의 시각 정보는 대부분 시간적 차원을 포함하는 동적인 비디오 형태로 존재한다. 비디오 데이터는 객체의 복잡한 움직임, 형태 변화, 다른 객체에 의한 가려짐(occlusion), 조명 조건의 급격한 변화 등 이미지 분할에서는 나타나지 않는 고유한 도전 과제들을 내포한다.2 SAM은 이러한 시간적 연속성과 동적 변화를 처리할 수 있는 내재적 메커니즘을 갖추고 있지 않았기에, 증강/가상현실(AR/VR), 로보틱스, 자율 주행, 비디오 편집 등 동영상 이해가 필수적인 핵심 응용 분야로의 직접적인 확장에 명백한 한계를 보였다.2</p>
<h3>1.2 SAM 2의 등장: 통합적 시각 분할을 향한 비전</h3>
<p>이러한 배경 속에서 등장한 <code>SAM 2: Segment Anything in Images and Videos</code>는 SAM의 근본적인 한계를 극복하고, 이미지와 비디오라는 두 가지 다른 양식(modality)을 하나의 통합된 프레임워크 안에서 해결하려는 야심 찬 비전을 제시한다.4 SAM 2의 핵심 목표는 SAM이 보여준 “무엇이든 분할하는(segment anything)” 능력을 시간 축으로 확장하여, 시공간적 연속성을 가진 객체를 일관되게 추적하고 분할하는 것이다.2</p>
<p>이를 위해 연구진은 기술적 구현에 앞서 문제 자체를 재정의하는 개념적 전환을 시도했다. 기존의 접근법이 “이미지 분할 모델을 어떻게 비디오에 맞게 수정할 것인가?“에 초점을 맞췄다면, SAM 2는 “이미지와 비디오를 모두 포괄하는 일반적인 분할 문제는 무엇인가?“라는 더 근본적인 질문에서 출발한다. 그 해답은 이미지를 “단일 프레임으로 구성된 비디오“로 간주하는 패러다임의 전환이었다.2 이 관점에서 이미지 분할은 비디오 분할의 특수한 경우(프레임 수=1)가 되며, 두 문제를 별개의 것으로 취급할 필요 없이 하나의 일반화된 ‘프롬프트 기반 시각 분할(promptable visual segmentation)’ 문제로 통합할 수 있게 된다. 이러한 재정의는 복잡한 시간 처리 모듈을 추가하는 대신, 기존 SAM 아키텍처에 ’메모리’라는 최소한의 요소를 더하여 비디오를 자연스럽게 처리할 수 있는 우아하고 효율적인 경로를 열어주었다.</p>
<h3>1.3 핵심 기여 요약</h3>
<p>SAM 2 연구의 핵심적인 기여는 다음 세 가지로 요약할 수 있다.</p>
<ol>
<li>
<p><strong>통합 모델 아키텍처:</strong> 실시간 비디오 처리를 위해 스트리밍 메모리(streaming memory)를 갖춘 간단한 트랜스포머(transformer) 구조를 제안했다.2 이 아키텍처는 비디오 프레임을 순차적으로 처리하면서도 과거의 중요한 정보를 효율적으로 참조하여 시간적 일관성을 유지한다. 메모리가 비어있을 때는 이미지 분할 모델처럼, 메모리가 채워지면 비디오 분할 모델처럼 작동하여 두 작업을 완벽하게 통합한다.2</p>
</li>
<li>
<p><strong>데이터 중심 접근법:</strong> 고품질 비디오 분할 데이터의 부재를 해결하기 위해, 모델과 인간 어노테이터가 상호작용하며 데이터를 생성하고 모델을 개선하는 ‘모델-인-더-루프(model-in-the-loop)’ 방식의 데이터 엔진을 구축했다.4 이를 통해 현재까지 공개된 데이터셋 중 가장 큰 규모인 SA-V(Segment Anything Video) 데이터셋을 수집했으며, 이는 파운데이션 모델 학습의 새로운 방법론을 제시한다.2</p>
</li>
<li>
<p><strong>획기적인 성능 향상:</strong> SA-V 데이터셋으로 학습된 SAM 2는 다양한 평가에서 압도적인 성능을 입증했다. 이미지 분할에서는 기존 SAM보다 더 높은 정확도를 보이면서 6배 빠른 추론 속도를 달성했으며, 비디오 분할에서는 기존 최신 기술 대비 3배 적은 사용자 상호작용으로 더 우수한 정확도를 기록했다.2 이는 SAM 2가 학술적 성과를 넘어 실용적인 도구로서의 가치를 지니고 있음을 시사한다.</p>
</li>
</ol>
<p>결론적으로 SAM 2는 단순한 모델 업그레이드를 넘어, 시각 분할 문제에 대한 접근 방식을 근본적으로 바꾼 연구라 할 수 있다. 문제 정의의 일반화를 통해 아키텍처의 우아함을 확보하고, 데이터와 모델의 선순환을 통해 성능을 극대화함으로써, 이미지와 비디오를 아우르는 통합적 시각 지능의 새로운 장을 열었다.</p>
<h2>2.  데이터 중심 접근법: 데이터 엔진과 SA-V 데이터셋의 구축</h2>
<h3>2.1 데이터 엔진의 필요성: 고품질 비디오 분할 데이터의 부재</h3>
<p>파운데이션 모델의 성공은 대규모의 고품질 학습 데이터에 크게 의존한다. 그러나 SAM 2 연구진이 비디오 분할로 영역을 확장하고자 했을 때, 그들은 심각한 데이터 부족 문제에 직면했다.2 기존의 비디오 분할 데이터셋들은 여러 가지 근본적인 한계를 가지고 있었다. 첫째, 데이터의 규모가 절대적으로 부족했다. 둘째, 대부분의 데이터셋이 특정 객체 카테고리(예: 사람, 자동차)에 한정되어 있어 ‘무엇이든 분할하는’ 범용 모델을 학습시키기에는 다양성이 부족했다. 셋째, 어노테이션의粒度가 거칠어, 객체의 미세한 일부(parts)나 하위 부분(subparts)에 대한 정교한 마스크 정보가 거의 없었다.2</p>
<p>이러한 데이터를 수동으로 구축하는 것은 엄청난 비용과 시간을 요구하는 비현실적인 작업이다.4 이 문제를 해결하기 위해 SAM 2 연구진은 SAM 프로젝트의 성공 핵심 요인 중 하나였던 ‘모델-인-더-루프’ 데이터 엔진 개념을 비디오 영역으로 확장하여 적용했다.1 데이터 엔진의 핵심 철학은 데이터 수집을 모델 학습과 분리된 선형적 과정으로 보지 않고, 모델과 데이터가 서로의 성장을 돕는 순환적이고 공진화하는(co-evolutionary) 과정으로 설계하는 것이다.</p>
<h3>2.2 SAM 2 데이터 엔진의 작동 원리 및 단계별 진화</h3>
<p>SAM 2 데이터 엔진은 모델과 인간 어노테이터 간의 긴밀한 상호작용을 통해 데이터를 효율적으로 수집하고, 수집된 데이터로 모델을 점진적으로 개선하는 선순환 구조를 가진다.4 이 과정은 모델의 성능이 향상될수록 데이터 수집 효율이 기하급수적으로 증가하는 특징을 보인다. 데이터 엔진은 총 3단계에 걸쳐 진화했다.7</p>
<ul>
<li>
<p><strong>Phase 1: SAM per frame:</strong> 프로젝트 초기에는 비디오를 처리할 수 있는 모델이 없었기 때문에, 기존의 이미지 분할 모델인 SAM을 각 비디오 프레임에 독립적으로 적용하여 마스크의 초안을 생성했다. 어노테이터는 각 프레임의 마스크를 개별적으로 검토하고 수정해야 했다. 이 방식은 시간적 일관성을 보장하기 어렵고, 프레임마다 반복적인 수정 작업이 필요하여 매우 비효율적이었다.</p>
</li>
<li>
<p><strong>Phase 2: SAM + SAM 2 Mask:</strong> 이 단계에서는 초기 SAM 2 프로토타입이 데이터 엔진에 도입되었다. 이 모델은 시간적 전파(temporal propagation) 기능을 갖추고 있어, 어노테이터가 특정 프레임에서 객체의 마스크를 지정하면 모델이 이를 비디오의 다른 프레임으로 자동으로 전파했다. 어노테이터의 역할은 모델이 생성한 전파 결과를 검토하고 오류가 발생한 부분만 수정하는 것으로 축소되었다. 이는 Phase 1에 비해 어노테이션 효율을 크게 향상시켰다.</p>
</li>
<li>
<p><strong>Phase 3: SAM 2:</strong> 최종적으로, 충분한 데이터로 학습된 완성도 높은 SAM 2 모델이 데이터 엔진에 완전히 통합되었다. 이 모델은 훨씬 더 정확한 시간적 전파 능력을 갖추고 있어, 단 한 번의 클릭이나 최소한의 상호작용만으로도 비디오 전체에 걸쳐 매우 높은 품질의 시간적으로 일관된 마스크(이를 논문에서는 ’마스크릿(masklet)’이라 칭함)를 생성할 수 있었다. 이 단계에서 데이터 수집 효율은 극대화되었으며, 최종 데이터 엔진은 유사한 품질을 목표로 하는 기존 모델 지원 어노테이션 방식보다 8.4배 빠른 속도를 달성했다.2</p>
</li>
</ul>
<p>특히 이 데이터 엔진은 단순히 데이터를 많이 수집하는 것을 넘어, ‘어렵고 유용한’ 데이터를 선별적으로 수집하도록 설계되었다. 즉, 현재 모델이 분할에 어려움을 겪는 객체(예: 크기가 작은 객체, 빠르게 움직이는 객체, 가려졌다가 다시 나타나는 객체)에 어노테이션 작업을 집중함으로써 모델의 약점을 효과적으로 보완하는 데이터를 효율적으로 확보했다.2 이는 모델과 데이터의 공동 진화를 위한 정교한 프레임워크로서, 향후 대규모 파운데이션 모델 개발의 표준 방법론이 될 잠재력을 보여준다.</p>
<h3>2.3 SA-V (Segment Anything Video) 데이터셋 분석</h3>
<p>이러한 데이터 엔진을 통해 최종적으로 구축된 것이 바로 SA-V(Segment Anything Video) 데이터셋이다.7 SA-V 데이터셋의 규모와 특징은 다음과 같다.</p>
<ul>
<li>
<p><strong>압도적인 규모:</strong> SA-V는 50,900개의 비디오에 걸쳐 총 3,550만 개의 마스크로 구성되어 있다.2 이는 기존에 존재하던 어떤 비디오 분할 데이터셋보다도 마스크 수 기준으로 53배 이상 많은 양으로, 비디오 분할 연구를 위한 전례 없는 규모의 리소스를 제공한다.2</p>
</li>
<li>
<p><strong>높은 다양성:</strong> SA-V는 특정 객체 카테고리에 국한되지 않고 “유효한 경계를 가진 모든 객체“를 어노테이션 대상으로 삼았다.2 이로 인해 객체뿐만 아니라 객체의 부분(parts), 하위 부분(subparts)까지 포함하는 매우 다양한 분할 데이터를 확보할 수 있었다. 또한, 지리적으로도 다양한 지역에서 촬영된 비디오를 포함하여 데이터의 편향을 최소화했다.2</p>
</li>
<li>
<p><strong>투명성:</strong> 연구진은 데이터셋 카드(Dataset Card)를 통해 데이터의 구성, 수집 과정, 전처리 방식, 의도된 사용 목적, 배포 및 유지보수 계획 등을 상세히 공개하여 연구의 투명성과 재현성을 높였다.7</p>
</li>
</ul>
<p>결론적으로, SA-V 데이터셋의 진정한 가치는 단순히 그 엄청난 규모에만 있는 것이 아니다. 최첨단 모델과의 상호작용을 통해 모델에게 가장 학습 가치가 높은, 즉 ‘어렵고 유용한’ 데이터가 체계적으로 수집되었다는 점에 더 큰 의의가 있다. 이는 정적인 데이터셋으로 한 번 학습하고 끝나는 기존의 패러다임을 넘어, 모델과 데이터셋이 함께 성장하는 새로운 연구 개발 패러다임을 제시한 것이다.</p>
<h2>3.  SAM 2 아키텍처 심층 분석: 스트리밍 메모리를 통한 시간의 이해</h2>
<h3>3.1 전체 구조: SAM의 자연스러운 확장</h3>
<p>SAM 2의 아키텍처는 복잡성을 더하기보다는 기존 SAM의 우아함을 유지하면서 비디오라는 새로운 차원을 포용하는 방향으로 설계되었다. 그 핵심은 SAM의 기본 3요소인 이미지 인코더, 프롬프트 인코더, 마스크 디코더 구조를 그대로 계승하는 것이다.1 SAM 2가 SAM과 구별되는 가장 큰 특징은 비디오의 시간적 정보를 처리하기 위해 <strong>메모리 어텐션(Memory Attention)</strong> 모듈과 **메모리 뱅크(Memory Bank)**라는 새로운 구성 요소를 도입한 점이다.2</p>
<p>전체 시스템은 비디오 프레임을 순차적으로 하나씩 입력받아 처리하는 스트리밍 아키텍처(streaming architecture)를 기반으로 한다.2 이는 임의의 길이를 가진 비디오를 실시간으로 처리할 수 있게 해주며, 사용자의 상호작용에 즉각적으로 반응하는 데 필수적이다. 이 구조는 앞서 언급한 “이미지는 단일 프레임 비디오“라는 개념적 일반화를 아키텍처 수준에서 구현한 것으로, SAM에 대한 매우 자연스러운 확장이라 할 수 있다.</p>
<h3>3.2 핵심 구성 요소 상세 분석</h3>
<h4>3.2.1 이미지 인코더 (Image Encoder)</h4>
<ul>
<li>
<p>SAM 2는 이미지 인코더로 MAE(Masked Autoencoders) 방식으로 사전 학습된 Hiera 모델을 사용한다.2 Hiera 인코더는 계층적(hierarchical) 구조를 가지고 있어, 이미지의 저수준 특징부터 고수준 의미 정보까지 다중 스케일의 특징을 효율적으로 추출할 수 있다는 장점이 있다.2</p>
</li>
<li>
<p>비디오 처리 시, 이미지 인코더는 각 프레임에 대해 단 한 번만 실행되어 해당 프레임의 시각적 정보를 담은 특징 임베딩(토큰) 시퀀스를 생성한다. 이 시점의 임베딩은 아직 시간적 문맥을 포함하지 않은, 순수한 공간적 정보만을 담고 있다.2</p>
</li>
</ul>
<h4>3.2.2 메모리 어텐션 (Memory Attention) 및 메모리 뱅크 (Memory Bank)</h4>
<p>이 두 요소는 SAM 2 아키텍처의 심장부라 할 수 있으며, 시간의 흐름을 이해하고 시간적 일관성을 유지하는 핵심적인 역할을 수행한다.</p>
<ul>
<li>
<p><strong>메모리 뱅크 (Memory Bank):</strong> 일종의 장기 기억 저장소로, 이전에 처리된 프레임들에서 추출된 객체에 대한 핵심 정보를 저장한다. 여기에는 과거 프레임의 예측 마스크, 사용자가 제공한 프롬프트, 그리고 해당 프레임의 특징 임베딩 등이 포함될 수 있다.2</p>
</li>
<li>
<p><strong>메모리 어텐션 (Memory Attention):</strong> 현재 프레임의 특징(공간 정보)을 메모리 뱅크에 저장된 과거의 정보(시간 정보)와 결합하여, 시간적 문맥이 풍부한 새로운 특징 표현을 만들어내는 역할을 한다.2 이 과정은 트랜스포머의 교차 어텐션(cross-attention) 메커니즘을 통해 이루어진다. 현재 프레임의 각 특징 토큰이 ’쿼리(Query)’가 되어, 메모리 뱅크에 저장된 과거 정보들의 ‘키(Key)’ 및 ’값(Value)’과 상호작용한다. 직관적으로 이는 “과거 프레임들에서 추적하던 바로 그 객체가 현재 프레임에서는 어떤 모습으로 어디에 위치하는가?“라는 질문에 답하는 과정과 같다.2</p>
</li>
</ul>
<p>이러한 스트리밍 메모리 아키텍처는 기존의 비디오 처리 방식들과 비교했을 때 뚜렷한 장점을 가진다. 여러 프레임을 묶어서 한 번에 처리하는 슬라이딩 윈도우 방식은 계산 비용이 높고 윈도우 크기를 벗어나는 장기적인 의존성을 포착하기 어렵다. 이전 프레임의 정보를 순차적으로 다음 프레임에 전달하는 RNN/LSTM 기반 방식은 그래디언트 소실 문제에 취약하고 병렬 처리가 어렵다는 단점이 있다. 반면, SAM 2의 메모리 어텐션은 현재 프레임이 메모리 뱅크에 저장된 과거의 모든 중요한 순간들과 직접적으로 ’소통’할 수 있게 해준다. 덕분에 객체가 수십, 수백 프레임 동안 가려졌다가 다시 나타나는 것과 같은 매우 긴 시간 간격의 의존성도 효과적으로 처리할 수 있으며, 동시에 스트리밍 방식으로 실시간 처리가 가능하다.6</p>
<h4>3.2.3 프롬프트 인코더 및 마스크 디코더 (Prompt Encoder and Mask Decoder)</h4>
<ul>
<li>
<p>이 부분은 기존 SAM의 구조와 거의 동일하게 작동한다.1 프롬프트 인코더는 사용자가 제공한 프롬프트(클릭, 상자 등)를 신경망이 이해할 수 있는 임베딩으로 변환한다.</p>
</li>
<li>
<p>마스크 디코더는 경량 트랜스포머 구조로, 메모리 어텐션을 통해 시간적 문맥이 보강된 프레임 특징과 프롬프트 임베딩을 입력받아 최종적인 분할 마스크를 예측한다.2</p>
</li>
<li>
<p>SAM 2의 상호작용성은 이 구조에서 비롯된다. 만약 사용자가 특정 프레임에서 모델의 예측이 틀렸다고 판단하여 새로운 수정 프롬프트(예: 잘못 분할된 영역에 부정적 클릭)를 제공하면, 이 새로운 정보는 즉시 메모리 뱅크를 업데이트하는 데 사용된다. 업데이트된 메모리는 후속 프레임들의 예측에 곧바로 반영되어, 단 한 번의 수정으로 이후의 모든 예측이 개선되는 효과를 낳는다.2 이것이 SAM 2가 뛰어난 상호작용 효율성을 보이는 구조적 이유다.</p>
</li>
</ul>
<h3>3.3 손실 함수 및 학습 과정</h3>
<p>SAM 2는 데이터 엔진을 통해 수집된 대규모 SA-V 데이터셋과 내부적으로 라이선스를 확보한 비디오 데이터를 혼합하여 학습되었다.2 논문에 구체적인 손실 함수가 명시되지는 않았지만, 일반적으로 분할 모델에서 널리 사용되는 Dice Loss와 Focal Loss의 조합이 사용되었을 것으로 추정된다.4 전체 학습 과정은 대규모 데이터셋을 활용한 사전 학습(pre-training) 단계와, 이미지 및 비디오 분할 작업 모두에 대해 모델 전체를 미세 조정(full training)하는 단계로 구성되었을 가능성이 높다.7</p>
<h2>4.  성능 평가 및 분석: 다중 도메인에서의 제로샷 일반화 능력 검증</h2>
<h3>4.1 평가 방법론: 포괄적인 제로샷 벤치마킹</h3>
<p>SAM 2의 핵심 가치는 학습 과정에서 접하지 못한 새로운 시각적 도메인, 객체, 비디오에 대해 얼마나 뛰어난 성능을 보이는지, 즉 제로샷 일반화(zero-shot generalization) 능력에 있다.4 연구진은 이 능력을 엄격하게 검증하기 위해 매우 광범위하고 포괄적인 벤치마크 스위트를 구축하여 평가를 수행했다. 평가에는 비디오 분할 관련 17개 데이터셋과 이미지 분할 관련 37개 데이터셋이 포함되었으며, 이는 기존 연구들에서 볼 수 없었던 대규모 평가이다.2</p>
<p>특히, 평가 데이터셋은 일반적인 비디오나 이미지뿐만 아니라, 로봇 수술 장면, 현미경 세포 이미지, 자율 주행 데이터, 어안 렌즈(fish-eye) 이미지, 수중 사진, 회화 등 극도로 다양한 도메인을 포괄하도록 구성되었다.2 이는 SAM 2가 특정 분야에 특화된 모델이 아닌, 범용적으로 적용 가능한 파운데이션 모델임을 입증하기 위한 것이다.</p>
<h3>4.2 비디오 분할 성능 분석</h3>
<h4>4.2.1 Promptable Video Segmentation (PVS)</h4>
<p>PVS는 사용자가 비디오의 특정 프레임에 프롬프트를 제공하면 모델이 해당 객체를 비디오 전체에 걸쳐 분할하고 추적하는 대화형(interactive) 작업이다. 이 작업에서 SAM 2는 기존의 강력한 베이스라인(예: SAM의 프레임별 예측을 XMem++이나 Cutie와 같은 비디오 전파 모델과 결합한 방식)들과 비교되었다. 평가 결과, SAM 2는 기존 접근법들보다 **3배 적은 사용자 상호작용(클릭 횟수)**만으로도 더 높은 분할 정확도(mIoU, mean Intersection over Union)를 달성했다.2 이는 SAM 2의 스트리밍 메모리 아키텍처가 사용자의 의도를 더 빠르고 정확하게 파악하고, 이를 시간 축으로 일관되게 전파하는 능력이 뛰어남을 정량적으로 보여준다. 실용적인 관점에서 이는 비디오 어노테이션이나 편집 작업에 소요되는 시간과 노력을 획기적으로 줄일 수 있음을 의미한다.</p>
<h4>4.2.2 Semi-supervised Video Object Segmentation (VOS)</h4>
<p>VOS는 비디오의 첫 번째 프레임에만 객체의 정확한 마스크가 주어지고, 나머지 프레임에서는 모델이 스스로 객체를 추적해야 하는 더 어려운 작업이다. 이 전통적인 VOS 벤치마크에서도 SAM 2는 여러 평가 설정 하에서 기존의 최신 기술(State-Of-The-Art, SOTA) 모델들을 능가하는 성능을 보였다.2 이는 SAM 2가 대화형 능력뿐만 아니라, 한 번 주어진 정보를 바탕으로 오랫동안 안정적으로 객체를 추적하는 능력 또한 갖추고 있음을 증명한다.</p>
<h4>4.2.3 공정성 평가 (Fairness Evaluation)</h4>
<p>연구진은 모델의 사회적 영향을 고려하여 공정성 평가도 수행했다. 비디오에 등장하는 인물의 인지된 성별(perceived gender)이나 연령 그룹에 따라 분할 성능에 유의미한 차이가 있는지 분석한 결과, 그룹 간 성능 격차가 거의 없는 것으로 나타났다.2 이는 SAM 2가 특정 인구 집단에 편향되지 않고 공정하게 작동할 가능성이 높음을 시사하는 긍정적인 결과다.</p>
<h3>4.3 이미지 분할 성능 분석</h3>
<p>SAM 2는 비디오 분할 모델이지만, “이미지는 단일 프레임 비디오“라는 설계 철학에 따라 이미지 분할에서도 강력한 성능을 보인다. 기존 SAM과의 직접적인 비교를 통해 그 우수성이 입증되었다.</p>
<p>평가 결과, SAM 2는 기존 SAM(ViT-H 백본 모델 기준)보다 **더 높은 정확도(1-click mIoU 기준)**를 달성하면서도, 추론 속도는 <strong>6배나 빠른 것</strong>으로 나타났다.2 예를 들어, Hiera-B+를 백본으로 사용하는 SAM 2 모델은 NVIDIA A100 GPU에서 초당 130.1 프레임(FPS)을 처리하는 반면, SAM(ViT-H)은 21.7 FPS에 그쳤다.2 이러한 압도적인 속도 향상은 더 효율적인 Hiera 이미지 인코더와 전체 아키텍처의 최적화 덕분이다.</p>
<p>이는 SAM 2가 기존 모델들이 흔히 겪는 ’정확도-속도’의 트레이드오프(trade-off) 관계를 극복했음을 보여준다. 단순히 더 정확해진 것을 넘어, 훨씬 더 효율적으로 작동함으로써 실시간 상호작용이 필수적인 애플리케이션에서의 적용 가능성을 크게 확장시켰다. 이러한 성능 향상은 SAM 2가 단순한 기능 추가가 아닌, 아키텍처 수준의 근본적인 진보를 이루었음을 방증한다.</p>
<h3>4.4 핵심 성능 비교표</h3>
<p>다음 표들은 SAM 2의 핵심적인 성능 주장을 정량적으로 요약하여 보여준다.</p>
<p><strong>표 1: 제로샷 이미지 분할 성능 비교 (SAM vs. SAM 2)</strong></p>
<table><thead><tr><th>모델</th><th>백본</th><th>학습 데이터</th><th>1-click mIoU (SA-23)</th><th>FPS</th></tr></thead><tbody>
<tr><td>SAM</td><td>ViT-H</td><td>SA-1B</td><td>(베이스라인)</td><td>21.7</td></tr>
<tr><td>SAM 2</td><td>Hiera-B+</td><td>SA-1B</td><td>베이스라인 상회</td><td><strong>130.1</strong></td></tr>
<tr><td>SAM 2</td><td>Hiera-L</td><td>SA-1B</td><td>베이스라인 + 1.0</td><td>43.1</td></tr>
<tr><td>SAM 2</td><td>Hiera-L</td><td>SA-1B + SA-V + Internal</td><td><strong>베이스라인 + 1.5</strong></td><td>43.1</td></tr>
</tbody></table>
<p>주: mIoU 수치는 논문의 상대적 성능 기술 2에 기반하여 개념적으로 표현됨. FPS는 A100 GPU 기준.</p>
<p><strong>표 2: 대화형 비디오 분할(PVS) 성능 비교</strong></p>
<table><thead><tr><th>방법</th><th>1-click mIoU (14개 비디오 데이터셋)</th><th>5-click mIoU</th><th>상호작용 효율성</th></tr></thead><tbody>
<tr><td>SAM+XMem++</td><td>(베이스라인)</td><td>(베이스라인)</td><td>1x</td></tr>
<tr><td>SAM+Cutie</td><td>(베이스라인)</td><td>(베이스라인)</td><td>1x</td></tr>
<tr><td>SAM 2 (Hiera-B+)</td><td><strong>69.6</strong></td><td>성능 추가 향상</td><td><strong>~3x</strong></td></tr>
<tr><td>SAM 2 (Hiera-L)</td><td><strong>71.1</strong></td><td>성능 추가 향상</td><td><strong>~3x</strong></td></tr>
</tbody></table>
<p>주: mIoU 수치는 2에서 직접 인용. 상호작용 효율성은 2의 “3배 적은 상호작용” 주장에 기반함.</p>
<h3>4.5 애블레이션 연구 (Ablation Studies)</h3>
<p>연구진은 제안된 모델과 데이터의 각 요소가 성능에 미치는 영향을 분석하기 위해 심층적인 애블레이션 연구를 수행했다.7 데이터 측면에서는 SA-1B(이미지)와 SA-V(비디오) 데이터의 혼합 비율을 조절하며 학습시킨 결과, 두 종류의 데이터를 함께 사용했을 때 이미지와 비디오 분할 양쪽에서 모두 최고의 성능을 보임을 확인했다. 이는 서로 다른 양식의 데이터가 모델의 일반화 성능에 상호 보완적으로 기여함을 보여준다. 아키텍처 측면에서는 메모리 모듈의 유무, 메모리 크기 변화 등에 따른 성능 변화를 분석하여, 제안된 스트리밍 메모리 아키텍처가 시간적 일관성을 유지하는 데 결정적인 역할을 함을 실험적으로 입증했다.7</p>
<h2>5.  SAM 2의 영향력과 파생 연구 생태계</h2>
<h3>5.1 파운데이션 모델로서의 역할</h3>
<p>SAM 2는 특정 다운스트림 작업을 해결하기 위한 ’완제품’이라기보다는, 다양한 시각 기반 응용 분야의 근간이 되는 강력하고 범용적인 ’엔진’으로서 설계되었다.4 Meta AI는 모델 체크포인트, 대규모 SA-V 데이터셋, 그리고 관련 코드를 모두 허용적인 라이선스 하에 오픈소스로 공개함으로써 2, 전 세계의 학계와 산업계 연구자들이 이를 자유롭게 활용하고 그 위에 새로운 기술을 구축할 수 있는 건강한 연구 생태계를 조성했다.</p>
<p>이러한 개방적인 접근 방식은 SAM 2의 영향력을 모델 자체의 성능을 넘어, 다른 연구를 촉발하고 가속하는 ‘촉매’ 역할로 확장시켰다. SAM 2가 “어떤 객체든 정교하게 분할할 수 있다“는 강력하지만 일반적인 도구를 제공하자, 각 전문 분야의 연구자들은 이 도구를 자신의 특정 문제에 맞게 ’특화’하고 ’자동화’하는 방향으로 활발한 후속 연구를 진행하기 시작했다. 이는 성공적인 파운데이션 모델이 기술 생태계를 어떻게 형성하고 발전시키는지를 보여주는 교과서적인 사례가 되었다.</p>
<h3>5.2 주요 파생 연구 및 응용 분야</h3>
<p>SAM 2가 공개된 이후, 그 잠재력을 활용하려는 다양한 파생 연구들이 빠르게 등장했다.</p>
<h4>5.2.1 의료 영상 분석 (Medical Image Analysis)</h4>
<p>의료 영상 분석은 SAM 2의 파급력이 가장 두드러지게 나타나는 분야 중 하나다.</p>
<ul>
<li>
<p><strong>Medical SAM 2 (MedSAM-2):</strong> 이 연구는 CT나 MRI와 같이 여러 2D 슬라이스(slice)로 구성된 3D 의료 영상을 일종의 ’비디오’로 간주하는 창의적인 접근법을 제시했다.9 SAM 2의 비디오 객체 추적 파이프라인을 그대로 활용하여, 인접한 슬라이스 간의 해부학적 연속성을 시간적 연속성처럼 처리한다. 이를 통해 3D 볼륨 전체에 걸쳐 종양이나 장기의 경계를 훨씬 더 일관되고 정확하게 분할할 수 있게 되었다. 특히, <code>self-sorting memory bank</code>라는 새로운 메커니즘을 제안하여 슬라이스의 순서와 관계없이 가장 유용한 정보를 동적으로 선택함으로써 성능을 극대화했다.9</p>
</li>
<li>
<p><strong>Biomedical SAM 2 (BioSAM-2):</strong> 범용 데이터로 학습된 SAM 2는 세포나 병변과 같이 자연 이미지와 특성이 다른 생의학(biomedical) 이미지에서는 성능적 한계를 보일 수 있다. BioSAM-2는 이러한 한계를 극복하기 위해 SAM 2를 대규모 생의학 데이터에 맞게 최적화하고 미세 조정한 모델이다.10 그 결과, 특정 질병이나 장기 분할을 위해 개별적으로 개발된 기존의 전문화된 의료 분할 모델들과 대등하거나 오히려 더 뛰어난 성능을 보이는 강력한 범용 의료 분할 파운데이션 모델을 탄생시켰다.10</p>
</li>
</ul>
<h4>5.2.2 자동 비디오 객체 분할 (Automatic Video Object Segmentation)</h4>
<p>SAM 2의 가장 명확한 한계 중 하나는 분할을 시작하기 위해 사용자의 명시적인 프롬프트가 필요하다는 점이다. 이 한계를 극복하려는 연구도 활발히 진행 중이다.</p>
<ul>
<li><strong>EntitySAM:</strong> 이 연구는 SAM 2를 프롬프트 없이 작동하도록 확장하는 것을 목표로 한다.11 이를 위해 쿼리 기반 객체 탐지(query-based entity discovery) 메커니즘을 SAM 2 아키텍처에 통합했다. 그 결과, EntitySAM은 별도의 프롬프트 없이도 비디오에 처음 등장하는 모든 유의미한 객체(entity, 사물과 배경 요소를 모두 포함)를 자동으로 발견하고, 비디오가 끝날 때까지 일관되게 추적 및 분할하는 ’제로샷 비디오 개체 분할(Video Entity Segmentation)’이라는 새로운 작업을 성공적으로 수행한다.11</li>
</ul>
<h4>5.2.3 원격 탐사 및 로보틱스 (Remote Sensing and Robotics)</h4>
<p>SAM 2의 강력한 분할 능력은 다른 전문 분야에서도 새로운 가능성을 열고 있다.</p>
<ul>
<li>
<p><strong>SAM2Former:</strong> 이 모델은 SAM 2의 정교한 분할 능력을 UNet과 유사한 트랜스포머 아키텍처와 결합하여, 위성이나 항공기에서 촬영된 원격 탐사 이미지의 의미론적 분할(semantic segmentation) 문제를 해결한다.4 원격 탐사 이미지는 객체의 크기가 매우 다양하고 특정 클래스가 데이터의 대부분을 차지하는 클래스 불균형 문제가 심각한데, SAM2Former는 SAM 2의 강력한 사전 지식을 활용하여 이러한 문제를 효과적으로 완화하고 분할 성능을 향상시킨다.4</p>
</li>
<li>
<p><strong>기타 응용:</strong> 이 외에도 로봇 수술 중 수술 도구나 장기를 정밀하게 추적하고 4, 현미경 이미지에서 특정 미생물 군집을 자동으로 계수하는 등 4, 인간의 정밀한 시각적 인식이 필요한 다양한 전문 분야에서 SAM 2를 기반으로 한 응용 연구가 활발히 이루어지고 있다.</p>
</li>
</ul>
<p>이러한 파생 연구들은 공통적인 패턴을 보인다. 즉, ‘객체의 경계를 찾는’ 저수준 분할 작업은 SAM 2에 맡기고, 연구자들은 ‘어떤 객체를 찾아야 하는가’ 또는 ’분할된 객체를 어떻게 활용할 것인가’와 같은 더 고수준의, 도메인 특화적인 문제 해결에 집중하는 것이다. 이는 SAM 2가 제공한 것이 완성된 솔루션이 아니라, 다양한 응용 솔루션을 만들 수 있는 고성능의 모듈화된 ’엔진’임을 명확히 보여준다.</p>
<h2>6.  종합 분석 및 전망: 시각 지능의 미래를 향하여</h2>
<h3>6.1 SAM 2 연구의 핵심 성과 요약</h3>
<p>SAM 2 연구는 컴퓨터 비전 분야에 다층적인 기여를 남겼으며, 그 핵심 성과는 기술, 데이터, 그리고 실용성의 세 가지 차원에서 요약할 수 있다.</p>
<ul>
<li>
<p><strong>기술적 성과:</strong> 이미지와 비디오라는 이질적인 두 데이터를 “프레임 시퀀스“라는 하나의 개념으로 통합하고, 이를 스트리밍 메모리 아키텍처를 통해 성공적으로 구현했다.2 이는 실시간 상호작용 능력과 장기적인 시간적 일관성이라는 두 마리 토끼를 동시에 잡은 중요한 아키텍처 혁신이다.</p>
</li>
<li>
<p><strong>데이터적 성과:</strong> 모델과 데이터의 선순환 구조를 핵심으로 하는 데이터 엔진을 통해, 비디오 분할 연구의 지평을 근본적으로 넓힌 대규모 SA-V 데이터셋을 구축하고 이를 전 세계 연구 커뮤니티에 공개했다.2 이는 향후 파운데이션 모델 개발의 새로운 표준을 제시한 데이터 중심 접근법의 성공 사례다.</p>
</li>
<li>
<p><strong>실용적 성과:</strong> 이미지 분할에서는 기존 SOTA 모델 대비 압도적인 속도 향상을, 비디오 분할에서는 인간의 노력을 획기적으로 줄이는 효율성을 입증했다.2 이는 시각 분할 기술이 더 이상 연구실 수준의 기술에 머무르지 않고, 실제 산업 현장의 워크플로우에 통합될 수 있는 실용성의 문턱을 넘었음을 시사한다.</p>
</li>
</ul>
<h3>6.2 내재된 한계 및 향후 연구 과제</h3>
<p>SAM 2는 기념비적인 성과를 이루었지만, 동시에 여러 내재적 한계와 향후 연구 과제를 남겼다.</p>
<ul>
<li>
<p><strong>프롬프트 의존성:</strong> SAM 2는 여전히 작업을 시작하기 위해 인간의 프롬프트를 필요로 한다. 완전 자동화된 시각 시스템을 구축하기 위해서는 <code>EntitySAM</code>과 같은 연구를 더욱 발전시켜, 비디오 내의 중요한 객체를 스스로 탐지하고 분할을 시작하는 능력이 필수적으로 통합되어야 한다.11</p>
</li>
<li>
<p><strong>의미론적 이해의 부재:</strong> SAM 2는 객체의 경계가 ‘어디에’ 있는지는 놀라울 정도로 잘 알지만, 그 객체가 ’무엇’인지는 이해하지 못한다. 즉, 픽셀 수준의 분할은 수행하지만 의미론적(semantic) 이해는 결여되어 있다. 분할된 마스크에 ‘고양이’, ’자동차’와 같은 의미론적 레이블을 부여하는 능력, 즉 의미론적 분할, 인스턴스 분할, 파놉틱 분할과의 깊이 있는 통합이 다음 단계의 중요한 과제다.11</p>
</li>
<li>
<p><strong>고차원적 데이터로의 확장:</strong> 3D 의료 영상과 같은 분야로의 확장이 성공적으로 시도되고는 있으나 9, 이는 여러 2D 슬라이스의 집합이라는 특수한 경우다. 자율주행의 라이다(LiDAR) 데이터와 같은 3D 포인트 클라우드, 혹은 과학 시뮬레이션에서 생성되는 복잡한 볼륨 데이터 등 더 일반적이고 고차원적인 시각 데이터에 대한 분할 능력으로 확장하는 것은 여전히 도전적인 연구 주제로 남아있다.</p>
</li>
</ul>
<h3>6.3 미래 전망: ’분할’을 넘어 ’이해’로</h3>
<p>SAM 2가 연 미래는 ’분할’이라는 단계를 넘어, 더 깊은 수준의 시각적 ’이해’와 ’상호작용’으로 나아갈 것이다.</p>
<ul>
<li>
<p><strong>생성 모델과의 결합:</strong> SAM 2의 정교한 분할 마스크는 비디오 생성 모델의 입력으로 활용되어, 전례 없는 수준의 비디오 편집을 가능하게 할 것이다. 예를 들어, 비디오에서 특정 인물이나 객체만을 자연스럽게 제거하거나, 특정 객체의 스타일만을 변경하는 등의 작업이 가능해질 것이다.6</p>
</li>
<li>
<p><strong>대형 언어 모델(LLM)과의 통합:</strong> 분할된 객체에 대한 단순한 레이블링을 넘어, 그 객체의 종류, 상태, 기능, 주변 객체와의 관계 등을 자연어로 설명하는 고차원적인 이해를 위해 대형 언어 모델과의 결합이 가속화될 것이다.8 이는 시각적 ’인식(perception)’을 넘어 시각적 ’이해(understanding)’로 도약하는 핵심적인 단계가 될 것이다.</p>
</li>
</ul>
<p>궁극적으로 SAM 시리즈의 발전 방향은 ’Segment Anything’을 넘어 ‘Understand Anything’, 그리고 ’Interact with Anything’으로 진화할 것이다. 이는 인간과 자연스럽게 소통하고 물리적 세계와 상호작용하는 더 높은 수준의 범용 인공지능을 향한 중요한 이정표가 될 것이다.</p>
<p>SAM 2의 가장 심오한 기여는 시각 지능 연구의 ’레이어(layer)’를 효과적으로 분리했다는 점에 있다. 즉, “객체가 어디에 있는가?“라는 저수준(low-level) 인식 문제를 거의 해결에 가까운 수준으로 끌어올림으로써, 이제 연구 커뮤니티가 “그것이 무엇인가?”, “왜 거기에 있는가?”, “그것과 어떻게 상호작용해야 하는가?“와 같은 더 고수준(high-level)의 인지 문제에 연구 역량을 집중할 수 있는 견고한 기반을 마련한 것이다. 복잡한 AI 시스템을 구축하는 데 있어 이러한 ’모듈화’와 ’역할 분리’는 매우 효율적인 접근 방식이다. SAM 2는 시각 지능 스택(stack)에서 가장 신뢰할 수 있는 기반 레이어 중 하나를 제공함으로써, 그 위에 쌓아 올릴 수 있는 무한한 응용 연구의 가능성을 열었다. 이것이 SAM 2가 갖는 장기적인 영향력의 핵심이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Segment Anything | Meta AI, https://segment-anything.com/</li>
<li>SAM 2: SEGMENT ANYTHING IN IMAGES AND … - OpenReview, https://openreview.net/pdf?id=Ha6RTeWMd0</li>
<li>facebookresearch/segment-anything: The repository provides code for running inference with the SegmentAnything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model. - GitHub, https://github.com/facebookresearch/segment-anything</li>
<li>SAM 2: Segment Anything in Images and Videos - ResearchGate, https://www.researchgate.net/publication/382797270_SAM_2_Segment_Anything_in_Images_and_Videos</li>
<li>SAM 2: Segment Anything in Images and Videos | Research - AI at Meta, https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/</li>
<li>Introducing Meta Segment Anything Model 2 (SAM 2), https://ai.meta.com/sam2/</li>
<li>SAM 2: Segment Anything in Images and Videos - arXiv, https://arxiv.org/html/2408.00714v1</li>
<li>[PDF] SAM 2: Segment Anything in Images and Videos | Semantic …, https://www.semanticscholar.org/paper/SAM-2%3A-Segment-Anything-in-Images-and-Videos-Ravi-Gabeur/92a09cdfc19f3f582d89c28c1b4f386299cc69e1</li>
<li>Medical SAM 2: Segment medical images as video via Segment Anything Model 2 - arXiv, https://arxiv.org/abs/2408.00874</li>
<li>[2408.03286] Biomedical SAM 2: Segment Anything in Biomedical Images and Videos - arXiv, https://arxiv.org/abs/2408.03286</li>
<li>EntitySAM: Segment Everything in Video - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2025/papers/Ye_EntitySAM_Segment_Everything_in_Video_CVPR_2025_paper.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>