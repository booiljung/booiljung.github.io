<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Hybrid Task Cascade (2019) Instance Segmentation의 상호보완적 융합</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Hybrid Task Cascade (2019) Instance Segmentation의 상호보완적 융합</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">이미지 분할 (Image Segmentations)</a> / <span>Hybrid Task Cascade (2019) Instance Segmentation의 상호보완적 융합</span></nav>
                </div>
            </header>
            <article>
                <h1>Hybrid Task Cascade (2019) Instance Segmentation의 상호보완적 융합</h1>
<h2>1.  Instance Segmentation의 발전과 Hybrid Task Cascade의 등장</h2>
<h3>1.1 Instance Segmentation의 정의와 중요성</h3>
<p>Instance segmentation은 컴퓨터 비전 분야의 근본적인 과업 중 하나로, 이미지 내에 존재하는 모든 객체의 클래스를 예측하고, 각 객체 인스턴스를 픽셀 수준에서 정확하게 분할하는 것을 목표로 한다.1 이는 단순히 객체의 위치를 경계 상자(bounding box)로 찾는 객체 탐지(object detection)나, 동일 클래스에 속하는 모든 픽셀을 하나의 의미 단위로 묶는 의미론적 분할(semantic segmentation)을 넘어서는 고차원적인 시각 이해 능력을 요구한다.1</p>
<p>이 기술의 중요성은 자율 주행, 지능형 영상 감시, 정밀 의료 영상 분석, 위성 이미지 해석 등 다양한 실제 응용 분야에서 두드러진다.1 예를 들어, 자율 주행 차량은 도로 위의 다른 차량, 보행자, 자전거 등을 개별 인스턴스로 정확히 인식하고 그 형태를 파악해야만 안전한 주행이 가능하다.3 이러한 실제 환경은 객체의 심한 변형, 다른 객체에 의한 가려짐(occlusion), 급격한 스케일 변화, 그리고 복잡하게 얽힌 배경(cluttered background) 등 수많은 난제를 포함하고 있어, 이에 강건한 분할 모델의 개발이 필수적이다.1</p>
<h3>1.2 초기 접근법의 한계: Mask R-CNN과 Cascade R-CNN의 단순 결합</h3>
<p>이러한 문제에 대응하기 위해 제안된 Cascade R-CNN은 객체 탐지 분야에서 큰 성공을 거둔 아키텍처이다.6 이 모델은 다단계에 걸쳐 점진적으로 더 엄격한 Intersection over Union (IoU) 임계값을 적용하여 후보 영역(proposal)을 정제함으로써, 높은 품질의 탐지 결과를 달성했다.1</p>
<p>그러나 이 강력한 캐스케이드 구조를 인스턴스 분할에 단순하게 적용하는 시도, 즉 각 단계에 Mask R-CNN의 마스크 예측 브랜치를 추가한 ’Cascade Mask R-CNN’은 기대에 미치지 못하는 제한적인 성능 향상만을 보였다.1 구체적으로, Cascade Mask R-CNN은 바운딩 박스 예측 성능(bbox AP)은 3.5%라는 큰 폭으로 향상시킨 반면, 마스크 분할 성능(mask AP)은 1.2% 향상에 그쳤다.1 이 성능 불균형의 근본적인 원인은 아키텍처 내 정보 흐름의 병목 현상에 있었다. 후속 단계의 마스크 브랜치는 단지 이전 단계에서 더 정확하게 정제된 바운딩 박스의 위치 정보로부터 간접적인 이득을 얻을 뿐, 마스크 예측 능력 자체를 점진적으로 개선할 수 있는 직접적인 정보 전달 경로가 부재했던 것이다.1 기존 접근법은 ’더 좋은 박스가 더 좋은 마스크를 만든다’는 단방향적 인과관계에 의존했으며, 이로 인해 마스크 분할 성능의 잠재력을 완전히 끌어내지 못했다.</p>
<h3>1.3 HTC의 핵심 철학: 탐지(Detection)와 분할(Segmentation)의 상호 보완적 관계 활용</h3>
<p>Hybrid Task Cascade (HTC)는 바로 이 한계를 극복하기 위해 제안되었다. HTC의 핵심 철학은 탐지와 분할이 독립적인 과업이 아니라, 서로의 성능을 높여줄 수 있는 상호 보완적(reciprocal) 관계에 있다는 인식에서 출발한다.1 이는 인스턴스 분할 캐스케이드의 문제 자체를 재정의하는 접근법이었다. 즉, ’더 좋은 박스가 더 좋은 마스크를 만들고, 동시에 더 좋은 마스크와 주변 컨텍스트 정보가 더 좋은 박스를 만든다’는 양방향적, 순환적 관계를 아키텍처에 직접 구현하고자 했다.</p>
<p>이러한 철학을 바탕으로 HTC는 기존 모델과 차별화되는 두 가지 핵심 전략을 도입했다 1:</p>
<ol>
<li><strong>작업 간 엮임 (Interweaving Tasks)</strong>: 탐지와 분할 작업을 각 단계에서 독립적으로 처리하는 대신, 두 작업을 긴밀하게 엮어(interweave) 하나의 공동 다단계 처리(joint multi-stage processing) 파이프라인으로 통합했다.</li>
<li><strong>공간적 맥락 활용 (Leveraging Spatial Context)</strong>: 복잡한 배경 속에서 전경 객체를 명확히 구분하기 위해, 이미지 전체의 공간적 맥락을 제공하는 완전 컨볼루션(fully convolutional) 기반의 의미론적 분할(semantic segmentation) 브랜치를 추가했다.</li>
</ol>
<p>이러한 혁신적인 설계를 통해 HTC는 각 정제 단계에서 탐지, 분할, 그리고 전역 컨텍스트라는 상호 보완적인 정보들을 유기적으로 통합함으로써, 점진적으로 더욱 판별력 있는 특징(discriminative features)을 학습할 수 있게 되었다.3 결과적으로 HTC는 독립적인 작업들의 순차적 정제라는 기존의 패러다임을 ’상호 의존적인 작업들의 통합적, 동시적 정제’라는 새로운 패러다임으로 전환시켰다.</p>
<h2>2.  Hybrid Task Cascade 아키텍처 심층 분석</h2>
<p>HTC 아키텍처의 정수는 정보 흐름을 단일 경로에서 다중 경로로 다각화하고, 이를 통해 각기 다른 작업들이 서로를 강화하는 선순환 구조를 구축한 데 있다. 기존 모델이 선형적인 파이프라인에 가까웠다면, HTC는 여러 정보 소스가 유기적으로 융합되고 순환하며 점진적으로 정제되는 ’정보 융합 네트워크’로 설계되었다.</p>
<h3>2.1 다단계 캐스케이드 구조</h3>
<p>HTC는 Cascade R-CNN의 기본 골격을 계승하여 총 3개의 단계(stage)로 구성된다. 각 단계는 점진적으로 높아지는 IoU 임계값(예: 0.5, 0.6, 0.7)으로 훈련되어, 특정 품질 수준의 RoI(Region of Interest)에 특화된 탐지기와 분할기를 학습한다.14 이 방식은 쉬운 샘플은 초기 단계에서 처리하고 어려운 샘플은 후기 단계에서 집중적으로 다루게 함으로써, 전체적인 학습 효율과 최종 예측의 정확도를 높인다.1 전체 프로세스는 RPN(Region Proposal Network)이 생성한 초기 후보 영역을 입력으로 받아 첫 번째 단계에서 시작되며, 각 단계의 출력은 다음 단계의 입력으로 전달되어 점진적인 정제가 이루어진다.14</p>
<h3>2.2 핵심 개선 사항</h3>
<p>HTC는 기존 Cascade Mask R-CNN의 한계를 극복하기 위해 정보 흐름을 근본적으로 재설계한 세 가지 핵심 요소를 도입했다. 이는 각각 동일 단계 내에서의 횡적 흐름, 단계 간의 종적 흐름, 그리고 모델 전체에 걸친 전역적 흐름을 담당한다.</p>
<h4>2.2.1 교차 실행 (Interleaved Execution)</h4>
<p>기존 모델에서는 각 단계에서 박스 헤드와 마스크 헤드가 이전 단계의 박스 예측을 공통 입력으로 받아 병렬적으로 처리되었다.2 이는 한 단계 내에서 박스 예측의 개선이 마스크 예측에 즉각적으로 반영되지 못하는 정보 지연을 야기했다.1</p>
<p>HTC는 이 문제를 해결하기 위해 ‘교차 실행’ 방식을 도입했다. 이는 <span class="math math-inline">t</span>번째 단계의 박스 헤드(<span class="math math-inline">B_t</span>)가 먼저 위치를 정제하고, 그 업데이트된 박스 예측 결과를 동일 단계의 마스크 헤드(<span class="math math-inline">M_t</span>)가 즉시 입력으로 사용하도록 처리 순서를 직렬화한 것이다.1 이를 수식으로 표현하면 다음과 같다. 먼저 <span class="math math-inline">t-1</span> 단계의 박스 예측 <span class="math math-inline">r_{t-1}</span>과 백본 특징 <span class="math math-inline">x</span>를 입력받아 현재 단계의 박스 예측 <span class="math math-inline">r_t</span>를 계산한다.<br />
<span class="math math-display">
r_t = B_t(P(x, r_{t-1}))
</span><br />
그 다음, 병렬적으로 처리하는 대신 이 새로 계산된 <span class="math math-inline">r_t</span>를 이용해 마스크 예측 <span class="math math-inline">m_t</span>를 수행한다.<br />
<span class="math math-display">
m_t = M_t(P(x, r_t))
</span><br />
여기서 <span class="math math-inline">P</span>는 RoIAlign과 같은 풀링 연산자이다.2 이러한 횡적 정보 흐름은 마스크 브랜치가 항상 가장 최신의, 가장 정확한 위치 정보 위에서 작동하도록 보장하여 분할의 정밀도를 향상시킨다.</p>
<h4>2.2.2 마스크 정보 흐름 (Mask Information Flow)</h4>
<p>Cascade Mask R-CNN의 또 다른 주요 한계는 마스크 브랜치 간의 정보 단절이었다. 각 단계의 마스크 예측은 독립적으로 수행되어, 이전 단계에서 학습된 유용한 마스크 특징들이 소실되었다.2</p>
<p>HTC는 이 문제를 해결하기 위해 단계 간 마스크 브랜치를 직접 연결하는 ‘마스크 정보 흐름’ 경로를 설계했다.1 구체적으로, <span class="math math-inline">t-1</span> 단계 마스크 헤드의 중간 특징 표현(<span class="math math-inline">\bar{m}_{t-1}</span>)을 현재 <span class="math math-inline">t</span> 단계 마스크 헤드의 입력 RoI 특징(<span class="math math-inline">x_{mask, t}</span>)과 융합(fusion)하여 마스크 예측에 활용한다. 이 융합은 주로 요소별 합(element-wise sum) 연산을 통해 구현된다.1 이를 수식으로 나타내면 다음과 같다.<br />
<span class="math math-display">
m_t = M_t(F(x_{mask, t}, G_t(\bar{m}_{t-1})))
</span><br />
여기서 <span class="math math-inline">F</span>는 융합 함수이며, <span class="math math-inline">G_t</span>는 채널 차원을 맞추기 위한 <code>1x1</code> 컨볼루션과 같은 변환 계층이다.16 이 종적 정보 흐름은 단순히 정제된 박스 위에서 마스크를 매번 새로 예측하는 수준을 넘어, 마스크 형태 자체를 점진적으로 정제하고 다듬어 나가는 것을 가능하게 한다.16</p>
<h4>2.2.3 의미론적 분할 브랜치 (Semantic Segmentation Branch)</h4>
<p>객체와 배경의 구분이 모호한 복잡한 장면에서 정확한 분할을 위해서는 객체 주변의 전역적인 컨텍스트 정보가 매우 중요하다. HTC는 이러한 전역적 정보 흐름을 위해 추가적인 ’의미론적 분할 브랜치’를 도입했다.1</p>
<p>이 브랜치는 FPN(Feature Pyramid Network)의 다중 스케일 출력(P2-P5)을 입력으로 받아, 이를 융합하여 풍부한 컨텍스트 정보를 담은 단일 의미론적 특징 맵(semantic feature map)을 생성한다.1 이 과정은 완전 컨볼루션 네트워크(FCN) 구조로 이루어지며, 전체 이미지에 대한 픽셀 단위 분류를 학습한다.1</p>
<p>생성된 의미론적 특징 맵은 각 캐스케이드 단계의 박스 헤드와 마스크 헤드에 모두 주입된다. RoIAlign을 통해 해당 RoI 위치의 의미론적 특징을 추출하고, 이를 기존의 RoI 특징과 요소별 합으로 결합하는 방식이다.1 이 전역적 정보 흐름은 강력한 공간적 단서(spatial cues)를 제공하여, 특히 가려지거나 배경과 유사하여 분간하기 어려운 객체의 탐지 및 분할 정확도를 크게 향상시키는 역할을 한다.1</p>
<h3>2.3 전체 손실 함수</h3>
<p>HTC의 학습은 다중 작업, 다단계 학습 프레임워크를 통해 종단간(end-to-end)으로 이루어진다. 전체 손실 함수는 각 단계의 바운딩 박스 손실과 마스크 손실, 그리고 의미론적 분할 브랜치의 손실을 가중합하여 정의된다.1</p>
<p>전체 손실 <span class="math math-inline">L</span>은 다음과 같은 수식으로 표현된다 1:<br />
<span class="math math-display">
L = \sum_{t=1}^{T} \alpha_t (L_{bbox}^t + L_{mask}^t) + \beta L_{seg}
</span><br />
각 구성 요소는 다음과 같다:</p>
<ul>
<li><span class="math math-inline">L_{bbox}^t</span>: <code>t</code>번째 단계의 바운딩 박스 손실로, 분류 손실(<span class="math math-inline">L_{cls}</span>)과 회귀 손실(<span class="math math-inline">L_{reg}</span>)의 합으로 구성된다.</li>
<li><span class="math math-inline">L_{mask}^t</span>: <code>t</code>번째 단계의 마스크 손실로, 픽셀 단위의 이진 교차 엔트로피(Binary Cross-Entropy)를 사용한다.</li>
<li><span class="math math-inline">L_{seg}</span>: 의미론적 분할 브랜치의 손실로, 전체 이미지에 대한 픽셀 단위 교차 엔트로피(Cross-Entropy)를 사용한다.</li>
<li><span class="math math-inline">T</span>: 전체 캐스케이드 단계의 수로, 일반적으로 3이 사용된다.</li>
<li><span class="math math-inline">\alpha_t, \beta</span>: 각 손실 항의 중요도를 조절하는 가중치 계수이다. 논문에서는 후속 단계로 갈수록 가중치를 줄이는 방식을 채택하여 <span class="math math-inline">\alpha_t = [1, 0.5, 0.25]</span>를 사용했으며, 의미론적 분할의 가중치 <span class="math math-inline">\beta</span>는 1로 설정했다.1</li>
</ul>
<h2>3.  성능 평가 및 Ablation Study</h2>
<h3>3.1 COCO 데이터셋 성능 분석</h3>
<p>HTC는 제안 당시 가장 널리 사용되는 벤치마크인 COCO(Common Objects in Context) 데이터셋에서 기존의 최첨단(SOTA) 모델들을 압도하는 뛰어난 성능을 입증했다. 특별한 추가 기법(bells and whistles) 없이 ResNet-101-FPN 백본을 사용한 기본 HTC 모델만으로도, 강력한 베이스라인인 Cascade Mask R-CNN 대비 1.5%의 mask AP(Average Precision) 향상을 달성했다.1 또한, 이전 SOTA 모델인 Mask R-CNN과 비교해서는 2.6% 더 높은 mask AP를 기록하며 그 효과를 명확히 보여주었다.1</p>
<p>더 나아가, Deformable Convolution, 다중 스케일 훈련 및 테스트, 모델 앙상블과 같은 일반적인 성능 향상 기법들을 결합했을 때, HTC는 COCO 2017 <code>test-dev</code> 데이터셋에서 49.0의 mask AP를 달성했다. 이는 2017년 COCO 챌린지 우승 모델의 성능을 2.3% 상회하는 기록이다.1 최종적으로, 이 시스템은 COCO 2018 챌린지 Object Detection Task에서 48.6 mask AP를 기록하며 1위를 차지하는 쾌거를 이루었다.9</p>
<p>아래 표 1은 HTC와 주요 베이스라인 모델들의 성능을 COCO <code>minival</code> 데이터셋에서 직접 비교한 결과이다. HTC가 바운딩 박스와 마스크 예측 양쪽 모두에서 일관된 성능 향상을 보였음을 확인할 수 있다.</p>
<table><thead><tr><th>모델</th><th>백본</th><th>box AP</th><th>mask AP</th></tr></thead><tbody>
<tr><td>Mask R-CNN</td><td>ResNet-101-FPN</td><td>42.0</td><td>37.8</td></tr>
<tr><td>Cascade Mask R-CNN</td><td>ResNet-101-FPN</td><td>45.5</td><td>39.2</td></tr>
<tr><td><strong>HTC</strong></td><td><strong>ResNet-101-FPN</strong></td><td><strong>46.6</strong></td><td><strong>40.6</strong></td></tr>
</tbody></table>
<h3>3.2 Ablation Study: 구성 요소별 기여도 분석</h3>
<p>HTC의 성공이 어떤 설계 요소에서 비롯되었는지 정량적으로 파악하기 위해 수행된 Ablation Study는 각 혁신적인 아이디어의 기여도를 명확하게 보여준다.1 이 분석은 각 구성 요소를 점진적으로 추가하면서 성능 변화를 측정하는 방식으로 진행되었으며, 단순히 작업 순서를 바꾸는 것보다 새로운 정보(이전 마스크 특징, 전역 컨텍스트)를 명시적으로 주입하는 것이 성능 향상에 더 결정적인 영향을 미쳤음을 시사한다.</p>
<ul>
<li><strong>Interleaved Execution (교차 실행)</strong>: 박스와 마스크 헤드의 실행 순서를 병렬에서 직렬로 변경한 이 개선은 mask AP를 0.2% 향상시켰다. 이는 상대적으로 작은 향상이지만, 마스크 브랜치가 항상 최신 위치 정보를 활용하게 하는 구조적 최적화의 긍정적인 효과를 입증한다.1</li>
<li><strong>Mask Information Flow (마스크 정보 흐름)</strong>: 이전 단계의 마스크 특징을 현재 단계로 전달하는 이 새로운 정보 경로는 mask AP를 추가로 0.6% 향상시켰다. 이는 정보의 ’내용’을 풍부하게 만드는 것이 성능에 더 큰 영향을 미침을 보여주며, 마스크 예측 자체의 점진적 정제가 효과적임을 증명한다.1</li>
<li><strong>Semantic Segmentation Branch (의미론적 분할 브랜치)</strong>: 전역 공간 컨텍스트를 제공하는 이 브랜치는 mask AP를 다시 0.6% 향상시키는 가장 큰 기여를 했다. 이는 복잡한 장면 이해에 있어 지역적인 RoI 특징만으로는 부족하며, 전역적인 정보가 필수적임을 강력하게 시사한다.1</li>
</ul>
<p>아래 표 2는 이러한 Ablation Study 결과를 요약한 것이다. 각 구성 요소가 추가될 때마다 mask AP뿐만 아니라 다른 세부 지표들(AP50, AP75 등)도 꾸준히 향상되는 것을 통해, HTC의 설계가 다각도에서 모델의 성능을 견고하게 만들었음을 알 수 있다.</p>
<table><thead><tr><th>구성 요소</th><th>box AP</th><th>mask AP</th><th>AP50</th><th>AP75</th><th>APs</th><th>APm</th><th>APl</th></tr></thead><tbody>
<tr><td>Baseline (Cascade Mask R-CNN)</td><td>42.5</td><td>36.5</td><td>57.9</td><td>39.4</td><td>18.9</td><td>39.5</td><td>50.8</td></tr>
<tr><td>+ Interleaved Execution</td><td>42.5</td><td>36.7</td><td>57.7</td><td>39.4</td><td>18.9</td><td>39.7</td><td>50.8</td></tr>
<tr><td>+ Mask Information Flow</td><td>42.5</td><td>37.4</td><td>58.1</td><td>40.3</td><td>19.6</td><td>40.3</td><td>51.5</td></tr>
<tr><td>+ Semantic Segmentation (Full HTC)</td><td>43.2</td><td>38.0</td><td>59.4</td><td>40.7</td><td>20.3</td><td>40.9</td><td>52.3</td></tr>
</tbody></table>
<h2>4.  최신 기술과의 비교: HTC와 Transformer 기반 모델</h2>
<h3>4.1 패러다임의 전환: RoI-based vs. Query-based</h3>
<p>HTC는 CNN 기반의 <strong>RoI-based</strong> 접근법이 도달할 수 있는 성능의 정점을 보여준 모델이다. 이 패러다임은 RPN(Region Proposal Network)을 통해 수천 개의 후보 영역(RoI)을 생성하고, 각 RoI에 대해 개별적으로 특징을 추출하여 분류, 회귀, 분할을 순차적으로 수행하는 방식에 기반한다.19 이 방식은 높은 정확도를 달성했지만, NMS(Non-Maximum Suppression)와 같은 후처리 과정이 필수적이며, 수많은 후보 영역을 처리하는 데서 오는 계산 비효율성이라는 본질적인 한계를 안고 있었다.</p>
<p>이후 DETR(DEtection TRansformer)의 등장은 인스턴스 분할 분야에 패러다임 전환을 가져왔다. <strong>Query-based</strong> 모델로 불리는 이 새로운 접근법은 소수의 학습 가능한 ’객체 쿼리(object query)’를 사용하여 이미지 전체의 특징과 직접 상호작용(attention)함으로써, 종단간(end-to-end) 방식으로 객체를 예측한다.20 이 방식은 NMS와 같은 복잡한 후처리 과정을 제거했을 뿐만 아니라, 어텐션 메커니즘을 통해 이미지 내 객체들 간의 전역적인 관계를 효과적으로 모델링하는 데 강력한 이점을 보였다.22</p>
<p>이러한 패러다임의 변화는 단순히 계산 효율성을 높이는 것을 넘어, 모델이 이미지를 이해하는 방식 자체를 근본적으로 바꾸었다. HTC가 수많은 ‘공간적 가설’(RoI)을 정제하는 방식이라면, Transformer 모델은 소수의 ‘개념적 가설’(query)을 이미지 전체의 맥락 속에서 정제하는 방식이다. 이 접근법의 차이가 장거리 의존성(long-range dependency) 포착 능력의 차이로 이어졌고, 이는 최신 SOTA 경쟁에서 Transformer 기반 모델들이 우위를 점하게 된 핵심적인 이유가 되었다. 아래 표 3은 두 패러다임의 주요 특징을 비교한 것이다.</p>
<table><thead><tr><th>구분</th><th>RoI-based (예: HTC)</th><th>Query-based (예: Mask DINO)</th></tr></thead><tbody>
<tr><td><strong>핵심 아이디어</strong></td><td>후보 영역 제안 후 정제 (Propose-and-Refine)</td><td>직접 집합 예측 (Direct Set Prediction)</td></tr>
<tr><td><strong>제안 메커니즘</strong></td><td>밀집된 앵커/RoI 생성 (RPN)</td><td>소수의 학습 가능한 객체 쿼리</td></tr>
<tr><td><strong>중복 제거</strong></td><td>NMS (Non-Maximum Suppression) 후처리</td><td>불필요 (End-to-End)</td></tr>
<tr><td><strong>전역 컨텍스트</strong></td><td>별도 모듈(FPN, Semantic Branch)로 제한적 활용</td><td>Transformer의 Self-Attention으로 내재적 모델링</td></tr>
<tr><td><strong>장점</strong></td><td>높은 정확도, 성숙한 기술</td><td>End-to-End 학습, 전역 관계 모델링에 강함</td></tr>
<tr><td><strong>단점</strong></td><td>복잡한 파이프라인, NMS 의존, 계산 비효율</td><td>학습 수렴이 느리고, 작은 객체 탐지에 어려움 (초기 모델)</td></tr>
</tbody></table>
<h3>4.2 주요 SOTA 모델 분석</h3>
<p>HTC 이후 등장한 최첨단 모델들은 대부분 Transformer 아키텍처를 기반으로 하며, query-based 패러다임의 잠재력을 극대화하는 방향으로 발전했다.</p>
<ul>
<li><strong>Mask2Former</strong>: 이 모델은 ‘Universal’ 분할 아키텍처를 지향하며, 인스턴스, 의미론적, 파놉틱 분할을 단일 모델로 처리할 수 있다.24 핵심 혁신은 <strong>Masked Attention</strong>으로, 예측된 마스크의 전경 영역 내에서만 cross-attention을 수행하여 배경 노이즈의 간섭을 차단하고 학습을 가속화한다.24 또한, 다중 스케일 특징을 효과적으로 활용하는 Transformer decoder를 통해 작은 객체에 대한 분할 성능을 크게 개선했다.27</li>
<li><strong>Mask DINO</strong>: 강력한 객체 탐지 모델인 DINO를 기반으로, 마스크 예측 브랜치를 통합하여 탐지와 분할을 하나의 통일된 프레임워크에서 공동으로 학습시킨다.29 이 모델은 DINO의 핵심 기술들(anchor-guided attention, query selection, denoising training 등)을 그대로 계승하면서, 탐지 작업을 통해 사전 학습된 강력한 객체 표현력을 분할 작업으로 자연스럽게 전이시킨다.29 이를 통해 탐지와 분할이 서로의 성능을 끌어올리는 시너지 효과를 극대화했다.</li>
</ul>
<h3>4.3 COCO Leaderboard 성능 비교</h3>
<p>아래 표 4는 COCO <code>test-dev</code> 데이터셋의 인스턴스 분할 리더보드를 기준으로 HTC와 최신 Transformer 기반 SOTA 모델들의 성능을 비교한 것이다. HTC는 Swin Transformer와 같은 강력한 백본과 결합된 HTC++ 버전에서 여전히 높은 경쟁력을 보여주지만, Co-DETR, Mask DINO와 같은 최신 query-based 모델들이 더 높은 성능을 달성하며 기술적 진보를 이끌고 있음을 확인할 수 있다.</p>
<table><thead><tr><th>모델</th><th>백본</th><th>mask AP</th><th>AP50</th><th>AP75</th><th>APs</th><th>APm</th><th>APl</th></tr></thead><tbody>
<tr><td>HTC 1</td><td>ResNeXt-101-64x4d</td><td>42.3</td><td>64.3</td><td>46.1</td><td>23.9</td><td>45.6</td><td>56.9</td></tr>
<tr><td>Swin-L (HTC++) 32</td><td>Swin-L</td><td>51.1</td><td>74.7</td><td>56.2</td><td>34.0</td><td>54.2</td><td>64.9</td></tr>
<tr><td>CBNetV2 (HTC) 34</td><td>Dual-Swin-L</td><td>52.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr>
<tr><td>Mask2Former 34</td><td>Swin-L</td><td>50.5</td><td>74.9</td><td>54.9</td><td>29.1</td><td>53.8</td><td>71.2</td></tr>
<tr><td>Mask DINO 29</td><td>Swin-L</td><td>54.7</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr>
<tr><td><strong>Co-DETR</strong> 34</td><td>Swin-L</td><td><strong>57.1</strong></td><td><strong>80.2</strong></td><td><strong>63.4</strong></td><td><strong>41.6</strong></td><td><strong>60.1</strong></td><td><strong>72.0</strong></td></tr>
</tbody></table>
<h2>5.  응용 분야 및 확장 모델</h2>
<p>HTC의 강력한 성능은 다양한 실제 문제 해결에 적용되었으며, 그 과정에서 드러난 한계는 특정 도메인에 특화된 확장 모델의 개발로 이어졌다. 이는 범용 모델과 전문가 모델 사이의 상호작용을 통한 기술 발전을 보여주는 좋은 사례이다.</p>
<h3>5.1 실세계 응용 사례</h3>
<ul>
<li><strong>원격 탐사 및 위성 이미지 분석</strong>: HTC는 고해상도 항공 및 위성 이미지에서 다양한 크기와 형태를 가진 건물을 개별 인스턴스로 정확하게 추출하는 데 성공적으로 활용되었다.4 또한, 기상 조건에 관계없이 영상을 획득할 수 있는 SAR(합성 개구 레이더) 이미지에서 선박을 탐지하고 분할하는 데 적용되어, 해상 감시 및 관리에 기여했다.37</li>
<li><strong>의료 영상 분석</strong>: 정밀함이 요구되는 의료 영상 분야에서 HTC 아키텍처는 중요한 역할을 수행했다. 병리학 슬라이드 이미지에서 수많은 세포핵(nuclei)을 개별적으로 분할하고 유형별로 분류하는 작업에 적용되어 암 진단 및 예후 예측의 자동화를 도왔다.39 또한, 혈액암의 일종인 다발성 골수종 세포를 핵과 세포질 단위로 정밀하게 분할하여 질병의 정량적 분석을 가능하게 했다.40</li>
<li><strong>자율 주행 및 실시간 인식</strong>: HTC의 원 논문에서부터 자율 주행은 핵심 응용 분야로 제시되었다.2 복잡한 도로 환경에서 차량, 보행자, 차선 등 다양한 객체를 픽셀 수준에서 정확하게 인식하는 능력은 안전한 자율 주행 시스템을 구축하기 위한 필수적인 기반 기술이다.5 최근에는 HTC와 고해상도 네트워크(HRNet)를 결합하여 6D 객체 자세 추정의 정확도를 높이는 연구도 진행되고 있다.41</li>
</ul>
<h3>5.2 HTC의 한계와 개선 방향</h3>
<p>HTC는 뛰어난 성능에도 불구하고 몇 가지 내재적인 한계를 가지고 있다.</p>
<ul>
<li><strong>계산 복잡도 및 실시간 처리</strong>: 다단계 캐스케이드 구조와 여러 개의 병렬 브랜치(박스, 마스크, 시맨틱)는 상당한 계산량을 요구한다. 이로 인해 추론 속도가 느려져, 자율 주행이나 실시간 영상 분석과 같이 즉각적인 반응이 필수적인 응용 분야에 직접 적용하기에는 어려움이 따른다.43</li>
<li><strong>CNN 기반 아키텍처의 내재적 한계</strong>: HTC는 CNN을 기반으로 하므로, 컨볼루션 연산의 고유한 특성인 지역적 수용장(local receptive field)의 한계를 벗어나기 어렵다.21 이는 이미지 내 멀리 떨어진 객체 간의 복잡한 관계나 전역적인 컨텍스트를 완벽하게 이해하는 데 제약을 준다.46</li>
<li><strong>특정 도메인에 대한 일반화</strong>: COCO와 같은 일반적인 데이터셋에 최적화된 HTC는 SAR 이미지의 고유한 노이즈 특성이나 의료 영상의 미세한 세포 구조와 같은 특정 도메인의 문제에 직접 적용될 때 최적의 성능을 발휘하지 못할 수 있다.37 이는 각 도메인의 특성을 반영한 아키텍처의 수정 및 미세 조정이 필요함을 의미한다.</li>
</ul>
<h3>5.3 파생 및 확장 모델</h3>
<p>이러한 한계를 극복하고 특정 응용 분야의 요구사항을 충족시키기 위해 HTC를 기반으로 한 여러 확장 모델이 제안되었다.</p>
<ul>
<li><strong>HTC+</strong>: SAR 선박 탐지라는 특정 과업에 HTC를 최적화한 모델이다. SAR 이미지의 특성(예: 작은 선박의 높은 빈도, 다양한 선박 형태, 고유한 노이즈)에 대응하기 위해, 다중 해상도 특징 추출 네트워크(MRFEN), 향상된 FPN(EFPN), 의미-유도 앵커 학습(SGAALN) 등 7가지의 정교한 기술을 추가했다. 그 결과, 원본 HTC 대비 SAR 선박 데이터셋에서 box AP와 mask AP를 각각 6.7%, 5.0%라는 큰 폭으로 향상시켰다.37</li>
<li><strong>NuHTC</strong>: 병리학 이미지 내 세포핵 분할 및 분류를 위해 HTC를 개선한 모델이다. 세포가 밀집되고 겹쳐 있는 환경에 대응하기 위해, Watershed 알고리즘에서 영감을 받은 제안 네트워크(WSPN)를 추가하여 더 정확한 초기 후보 영역을 생성한다. 또한, 고수준의 전역 특징과 저수준의 의미 특징을 효과적으로 결합하는 하이브리드 특징 추출기(HFE)를 도입하여 클래스 내 분산을 줄이고 분류 정확도를 높였다.39</li>
</ul>
<p>이러한 확장 모델들의 등장은 HTC가 강력한 범용 프레임워크로서의 역할을 넘어, 특정 전문 분야의 문제를 해결하기 위한 견고한 ’기본 틀’로도 기능할 수 있음을 보여준다. 이는 AI 모델 개발에서 범용성과 특화 사이의 상호 보완적인 발전 양상을 잘 나타내는 사례이다.</p>
<h2>6.  결론: HTC의 유산과 미래 전망</h2>
<h3>6.1 HTC의 기술적 기여 요약</h3>
<p>Hybrid Task Cascade는 인스턴스 분할 분야의 발전에 있어 중요한 이정표를 세운 모델이다. HTC의 가장 큰 기술적 기여는 단순히 성능 수치를 끌어올린 것을 넘어, 문제 해결을 위한 새로운 접근 방식을 제시했다는 데 있다.</p>
<p>첫째, HTC는 탐지와 분할이라는 두 과업이 서로의 성능을 향상시킬 수 있는 상호 보완적 관계에 있음을 통찰하고, 이를 ’하이브리드 다중 작업 정제’라는 아키텍처로 성공적으로 구현했다. 이는 독립적인 작업들의 단순한 캐스케이드를 넘어서는 새로운 패러다임이었다.</p>
<p>둘째, Interleaved Execution, Mask Information Flow, Semantic Segmentation Branch라는 세 가지 구체적이고 창의적인 설계를 통해 모델 내 정보 흐름을 극대화했다. 이는 성능 병목 현상을 더 큰 모델이나 더 많은 데이터에 의존하기보다, ’더 나은 정보 흐름을 위한 구조적 설계’로 해결할 수 있음을 입증한 사례이다. 이러한 접근법은 아키텍처 엔지니어링의 중요성을 다시 한번 상기시키며, 딥러닝 연구의 중요한 방법론을 제시했다.</p>
<p>결과적으로 HTC는 CNN 기반 2-stage 인스턴스 분할 모델의 기술적 정점을 보여주었으며, 후속 연구들에게 다중 작업 학습과 캐스케이드 구조를 효과적으로 결합하는 방법에 대한 중요한 청사진을 제공했다.</p>
<h3>6.2 현재 SOTA 모델에 미친 영향과 향후 연구 방향</h3>
<p>HTC가 제시한 철학과 아이디어는 현재의 최첨단 모델들에도 그 유산을 남겼다. HTC가 강조한 ’탐지와 분할의 시너지’라는 개념은 Mask DINO와 같은 최신 통합 프레임워크에서도 계승되고 있다. Mask DINO가 탐지와 분할을 하나의 Transformer decoder 내에서 공동으로 학습시켜 상호 이득을 얻는 방식은, 비록 구현 기술은 다르지만 HTC의 핵심 철학과 맥을 같이 한다.29</p>
<p>한편, HTC가 가졌던 전역적 컨텍스트 처리와 계산 효율성의 한계는 자연스럽게 Transformer 기반 Query-based 모델의 등장을 촉진하는 계기가 되었다. 이는 RoI-based 패러다임에서 Query-based 패러다임으로의 기술적 진화가 필연적인 과정이었음을 보여준다.</p>
<p>향후 인스턴스 분할 연구는 여러 방향으로 전개될 것으로 전망된다. 첫째, HTC와 같은 정교한 CNN 구조의 장점(강력한 지역 특징 추출 능력)과 Transformer의 장점(효과적인 전역 관계 모델링)을 결합하는 하이브리드 아키텍처에 대한 탐구가 계속될 것이다. 둘째, HTC+나 NuHTC의 사례에서 보듯, 특정 도메인의 고유한 문제를 해결하기 위해 범용 아키텍처를 어떻게 효과적으로 ’특화’시킬 것인지에 대한 연구가 더욱 중요해질 것이다. 마지막으로, 대규모 데이터셋과 자기 지도 학습(self-supervised learning)을 통해 사전 학습된 강력한 범용 백본 위에 HTC와 같은 정교한 태스크 헤드를 결합하여, 데이터와 아키텍처의 이점을 모두 취하는 방향으로의 발전도 기대된다. HTC의 유산은 이처럼 끊임없이 진화하는 인스턴스 분할 기술의 역사 속에서 중요한 길잡이 역할을 계속할 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Hybrid Task Cascade for Instance Segmentation - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Hybrid_Task_Cascade_for_Instance_Segmentation_CVPR_2019_paper.pdf</li>
<li>Hybrid Task Cascade for Instance Segmentation - ResearchGate, https://www.researchgate.net/publication/338513481_Hybrid_Task_Cascade_for_Instance_Segmentation</li>
<li>Hybrid Task Cascade for Instance Segmentation, https://staff.ie.cuhk.edu.hk/~ccloy/files/cvpr_2019_hybrid.pdf</li>
<li>Hybrid Task Cascade-Based Building Extraction Method in Remote Sensing Imagery - MDPI, https://www.mdpi.com/2072-4292/15/20/4907</li>
<li>A Review of Vision-Based Multi-Task Perception Research Methods for Autonomous Vehicles - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC12030850/</li>
<li>Cascade R-CNN: Delving into High Quality Object Detection - Statistical Visual Computing Lab, http://www.svcl.ucsd.edu/publications/conference/2018/cvpr/cascade-rcnn.pdf</li>
<li>[1906.09756] Cascade R-CNN: High Quality Object Detection and Instance Segmentation - arXiv, https://arxiv.org/abs/1906.09756</li>
<li>Cascade R-CNN: High Quality Object Detection and Instance Segmentation - arXiv, https://arxiv.org/pdf/1906.09756</li>
<li>laihuihui/htc - GitHub, https://github.com/laihuihui/htc</li>
<li>[1901.07518] Hybrid Task Cascade for Instance Segmentation - arXiv, https://arxiv.org/abs/1901.07518</li>
<li>Hybrid Task Cascade for Instance Segmentation | Latest Papers | HyperAI, https://hyper.ai/en/papers/1901.07518</li>
<li>[PDF] Hybrid Task Cascade for Instance Segmentation - Semantic Scholar, https://www.semanticscholar.org/paper/Hybrid-Task-Cascade-for-Instance-Segmentation-Chen-Pang/21248bcc81539e7cd1ef83b3b184768603f6f247</li>
<li>What is: Hybrid Task Cascade? - Viet-Anh on Software, https://www.vietanh.dev/glossary/htc</li>
<li>Exploiting Concepts of Instance Segmentation to Boost Detection in Challenging Environments - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC9144693/</li>
<li>Building Instance Segmentation and Boundary Regularization from High-Resolution Remote Sensing Images, https://ris.utwente.nl/ws/portalfiles/portal/282852954/Zhao_2020_Building_instance_segmentation_and_.pdf</li>
<li>Brief Review — HTC: Hybrid Task Cascade for Instance Segmentation | by Sik-Ho Tsang, https://sh-tsang.medium.com/brief-review-htc-hybrid-task-cascade-for-instance-segmentation-a17dac3035f1</li>
<li>HybridTask Cascade - Computer Vision Wiki - CloudFactory, https://wiki.cloudfactory.com/docs/mp-wiki/model-architectures/hybridtask-cascade</li>
<li>Explained architecture of Hybrid Task Cascade Network. It utilizes… | Download Scientific Diagram - ResearchGate, https://www.researchgate.net/figure/Explained-architecture-of-Hybrid-Task-Cascade-Network-It-utilizes-three-box-and-mask_fig1_353984532</li>
<li>What Is Instance Segmentation? - IBM, https://www.ibm.com/think/topics/instance-segmentation</li>
<li>FastInst: A Simple Query-Based Model for Real-Time Instance Segmentation - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/He_FastInst_A_Simple_Query-Based_Model_for_Real-Time_Instance_Segmentation_CVPR_2023_paper.pdf</li>
<li>SCTS: Instance Segmentation of Single Cells Using a Transformer-Based Semantic-Aware Model and Space-Filling Augmentation, https://openaccess.thecvf.com/content/WACV2023/papers/Zhou_SCTS_Instance_Segmentation_of_Single_Cells_Using_a_Transformer-Based_Semantic-Aware_WACV_2023_paper.pdf</li>
<li>SOIT: Segmenting Objects with Instance-Aware Transformers - The Association for the Advancement of Artificial Intelligence, https://cdn.aaai.org/ojs/20227/20227-13-24240-1-2-20220628.pdf</li>
<li>Masked-attention Mask Transformer for Universal Image Segmentation CVPR 2022 - Bowen Cheng, https://bowenc0221.github.io/mask2former/</li>
<li>What is Mask2Former? The Ultimate Guide. - Roboflow Blog, https://blog.roboflow.com/what-is-mask2former/</li>
<li>Mask2former Swin Base Coco Instance · Models - Dataloop, https://dataloop.ai/library/model/facebook_mask2former-swin-base-coco-instance/</li>
<li>Mask2Former: Hands-on Tutorial Guide, https://www.labellerr.com/blog/mask2former-hands-on-tutorial-guide/</li>
<li>Mask2Former for Semantic, Instance, and Panoptic Segmentation - DebuggerCafe, https://debuggercafe.com/mask2former/</li>
<li>Mask DINO: Towards a Unified Transformer-Based Framework for Object Detection and Segmentation - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Mask_DINO_Towards_a_Unified_Transformer-Based_Framework_for_Object_Detection_CVPR_2023_paper.pdf</li>
<li>[2206.02777] Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation - ar5iv, https://ar5iv.labs.arxiv.org/html/2206.02777</li>
<li>[2206.02777] Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation - arXiv, https://arxiv.org/abs/2206.02777</li>
<li>arXiv:2103.14030v2 [cs.CV] 17 Aug 2021, https://arxiv.org/pdf/2103.14030</li>
<li>Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf</li>
<li>COCO test-dev Benchmark (Instance Segmentation) | Papers With Code, https://paperswithcode.com/sota/instance-segmentation-on-coco?p=yolact-real-time-instance-segmentation</li>
<li>Building Instance Extraction Method Based on Improved Hybrid Task Cascade - University of Waterloo, https://uwaterloo.ca/geospatial-intelligence/sites/default/files/uploads/files/2022_liu_chen_wei_wang_goncalves_marcato_li_grsl.pdf</li>
<li>Hybrid Task Cascade-Based Building Extraction Method in Remote Sensing Imagery, https://www.researchgate.net/publication/374648532_Hybrid_Task_Cascade-Based_Building_Extraction_Method_in_Remote_Sensing_Imagery</li>
<li>HTC+ for SAR Ship Instance Segmentation - MDPI, https://www.mdpi.com/2072-4292/14/10/2395</li>
<li>(PDF) SAR Ship Instance Segmentation Based on Hybrid Task Cascade - ResearchGate, https://www.researchgate.net/publication/357967383_SAR_Ship_Instance_Segmentation_Based_on_Hybrid_Task_Cascade</li>
<li>NuHTC: A hybrid task cascade for nuclei instance segmentation and classification - PubMed, https://pubmed.ncbi.nlm.nih.gov/40294567/</li>
<li>Instance Segmentation of Multiple Myeloma Cells via Hybrid Task Cascade | OpenReview, https://openreview.net/forum?id=T1ZK_GYtdbn</li>
<li>[2502.03877] Advanced Object Detection and Pose Estimation with Hybrid Task Cascade and High-Resolution Networks - arXiv, https://arxiv.org/abs/2502.03877</li>
<li>Advanced Object Detection and Pose Estimation with Hybrid Task Cascade and High-Resolution Networks - arXiv, https://arxiv.org/html/2502.03877v1</li>
<li>Instance segmentation convolutional neural network based on multi-scale attention mechanism | PLOS One, https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0263134</li>
<li>(PDF) Real-Time Applications of Hybrid Task Cascade: Challenges and Solutions, https://www.researchgate.net/publication/389322326_Real-Time_Applications_of_Hybrid_Task_Cascade_Challenges_and_Solutions</li>
<li>What are the limitations of CNN in computer vision? - Milvus, https://milvus.io/ai-quick-reference/what-are-the-limitations-of-cnn-in-computer-vision</li>
<li>Semantic segmentation: Complete guide [Updated 2024] - SuperAnnotate, https://www.superannotate.com/blog/guide-to-semantic-segmentation</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>