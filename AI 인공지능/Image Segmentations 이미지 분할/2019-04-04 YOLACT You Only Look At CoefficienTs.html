<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:YOLACT (You Only Look At CoefficienTs, 2019-04-04)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>YOLACT (You Only Look At CoefficienTs, 2019-04-04)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">이미지 분할 (Image Segmentations)</a> / <span>YOLACT (You Only Look At CoefficienTs, 2019-04-04)</span></nav>
                </div>
            </header>
            <article>
                <h1>YOLACT (You Only Look At CoefficienTs, 2019-04-04)</h1>
<h2>1.  실시간 인스턴스 분할의 패러다임 전환</h2>
<h3>1.1  인스턴스 분할의 과제: 정확도와 속도의 상충 관계</h3>
<p>컴퓨터 비전 분야에서 인스턴스 분할(Instance Segmentation)은 각 픽셀을 특정 객체 인스턴스에 할당하는 근본적이면서도 매우 도전적인 과제이다.1 이는 단순히 객체의 종류를 식별하는 의미론적 분할(Semantic Segmentation)을 넘어, 동일한 클래스에 속하는 여러 객체를 개별적으로 구별해야 하는 고차원적인 작업을 포함한다.1 역사적으로 이 분야는 Mask R-CNN과 같은 2단계(two-stage) 방법론이 지배해왔다.1 이러한 모델들은 먼저 관심 영역(Region of Interest, ROI)을 제안하고, 각 ROI에 대해 분류와 마스크 예측을 순차적으로 수행하는 방식으로 높은 정확도를 달성했다.3</p>
<p>하지만 이러한 높은 정확도는 상당한 계산 비용을 수반했다. 2단계 방법론의 순차적 특성은 추론 속도에 심각한 병목 현상을 야기했으며, 대부분의 모델은 초당 10프레임(FPS) 미만의 성능을 보여 실시간 응용 분야에 적용하기 어려웠다.1 자율 주행, 로보틱스, 실시간 비디오 분석과 같은 분야에서는 최소 20-30 FPS의 처리 속도가 요구되므로 1, 기존의 고정확도 모델들은 실용적인 해결책이 되지 못했다. 이로 인해 정확도를 유지하면서도 실시간 성능을 달성할 수 있는 새로운 아키텍처에 대한 연구 필요성이 절실히 대두되었다.</p>
<h3>1.2  YOLACT의 제안: 병렬 처리를 통한 실시간 성능 달성</h3>
<p>이러한 배경 속에서 2019년 Daniel Bolya 등에 의해 제안된 YOLACT(You Only Look At CoefficienTs)는 실시간 인스턴스 분할 분야의 새로운 지평을 열었다.6 YOLACT는 경쟁력 있는 정확도를 유지하면서 실시간 성능을 달성한 최초의 완전 컨볼루션(fully-convolutional) 모델이다.4 단일 Titan Xp GPU 환경에서 MS COCO 데이터셋에 대해 29.8 mAP(mean Average Precision)를 33.5 FPS의 속도로 달성하였는데, 이는 이전의 어떤 접근 방식보다 월등히 빠른 속도였다.3 이 결과는 단 하나의 GPU로 학습하여 얻은 것으로, 모델의 효율성을 입증한다.4</p>
<p>YOLACT의 핵심 혁신은 복잡한 인스턴스 분할 작업을 두 개의 단순하고 독립적인 서브태스크로 분리하여 병렬로 처리하는 데 있다.6 첫 번째 태스크는 이미지 전체에 걸쳐 적용되는 일반적인 ‘프로토타입 마스크(prototype masks)’ 집합을 생성하는 것이고, 두 번째 태스크는 각 객체 인스턴스에 대한 ’마스크 계수(mask coefficients)’를 예측하는 것이다.4 이 두 결과물은 최종적으로 매우 가벼운 조합 단계를 통해 각 인스턴스의 최종 마스크를 생성한다. 이러한 병렬 구조는 기존 방법론의 순차적 병목을 근본적으로 해결하며, YOLACT가 실시간 성능을 달성할 수 있었던 핵심적인 이유이다.3 YOLACT의 등장은 단순히 속도를 점진적으로 개선한 것을 넘어, ’실시간 인스턴스 분할’의 기준을 새롭게 정립했다. 30 FPS라는 목표를 현실적인 기준으로 만들었고, 이후의 연구들이 속도를 정확도와 함께 핵심적인 평가 지표로 고려하도록 하는 촉매제가 되었다.</p>
<h2>2.  YOLACT의 핵심 원리: 마스크 생성의 분리</h2>
<h3>2.1  특징 지역화(Feature Localization) 단계의 제거</h3>
<p>Mask R-CNN과 같은 2단계 방법론의 핵심 연산 중 하나는 RoIAlign과 같은 ‘특징 지역화’ 또는 ‘재풀링(re-pooling)’ 단계이다.3 이 과정은 특징 맵(feature map)에서 특정 경계 상자(bounding box) 제안에 해당하는 영역의 특징을 잘라내어 고정된 크기로 조정한 후, 마스크 예측 헤드에 입력으로 전달한다.4 이 과정은 본질적으로 순차적이다. 즉, 객체의 위치를 먼저 예측해야만 해당 위치의 특징을 풀링할 수 있다.3 이러한 순차적 의존성은 모델의 병렬화를 방해하고 속도를 저하시키는 주된 원인이었다.4</p>
<p>YOLACT는 이 특징 지역화 단계를 과감히 제거함으로써 패러다임을 전환했다.3 프로토타입 마스크는 특정 ROI에 국한되지 않고 이미지 전체에 대해 생성된다. 이로써 마스크 생성 과정이 비지역적(non-local) 특성을 갖게 되며, 순차적 병목 현상에서 완전히 자유로워졌다.</p>
<h3>2.2  두 개의 병렬 서브태스크: 프로토타입 마스크와 마스크 계수</h3>
<p>YOLACT는 인스턴스 분할을 두 개의 병렬적인 서브태스크로 분해하여 처리한다.6</p>
<ul>
<li><strong>프로토타입 마스크 (Prototype Masks):</strong> 네트워크는 고정된 개수, <span class="math math-inline">k</span>개의 범용 프로토타입 마스크를 생성한다. 이 마스크들은 특정 객체 인스턴스나 클래스에 종속되지 않으며, 이미지 내에 존재하는 다양한 형태, 경계, 공간적 패턴들을 학습한 일종의 ‘기저(basis)’ 또는 ‘사전(dictionary)’ 역할을 한다.7</li>
<li><strong>마스크 계수 (Mask Coefficients):</strong> 프로토타입 생성과 동시에, 객체 탐지 브랜치에 연결된 예측 헤드는 탐지된 각 인스턴스(앵커)에 대해 <span class="math math-inline">k</span>차원의 마스크 계수 벡터를 예측한다.8 이 벡터는 해당 인스턴스를 표현하기 위해 프로토타입 사전을 어떻게 조합해야 하는지를 인코딩한다.</li>
</ul>
<p>이러한 분리 전략은 계산적으로 무거운 프로토타입 생성을 이미지 전체에 대해 한 번만 수행하고, 각 인스턴스에 대해서는 가벼운 계수 벡터 예측만을 수행하도록 하여 전체적인 효율을 극대화한다.3 이는 마치 복잡한 이미지를 소수의 기본 벡터(프로토타입)와 그 조합 계수로 표현하는 분산 표현(distributed representation) 학습과 유사하다. 프로토타입의 개수</p>
<p><span class="math math-inline">k</span>가 클래스의 개수 <span class="math math-inline">c</span>와 무관하기 때문에, 프로토타입은 여러 클래스에 걸쳐 공유되며, 이는 더 압축적이고 일반화 성능이 높은 표현 학습으로 이어진다.3 이 방식은 복잡한 시각적 과제를 일반적인 ‘기저 공간’ 학습과 각 인스턴스를 해당 공간으로 매핑하는 간단한 함수 학습으로 분해할 수 있음을 시사하며, 이는 다른 밀집 예측(dense prediction) 작업에도 적용될 수 있는 강력한 설계 원리이다.</p>
<h3>2.3  완전 컨볼루션 네트워크의 내재적 위치 분별 능력</h3>
<p>명시적인 지역화 단계 없이 어떻게 완전 컨볼루션 네트워크가 서로 다른, 심지어 겹쳐 있는 인스턴스들을 구별할 수 있는지에 대한 의문이 제기될 수 있다. 해답은 모델의 ’내재적 동작(emergent behavior)’에 있다. YOLACT는 별도의 지시 없이 스스로 인스턴스를 지역화하는 방법을 학습한다.3 학습 과정에서 프로토타입들은 위치에 따라 다르게 활성화되는, 즉 위치 변동성(translation variance)을 갖도록 진화한다.9</p>
<p>분석 결과, 프로토타입들은 종종 이미지의 특정 공간적 분할(예: ‘좌상단 영역’, ‘중앙’, ‘오른쪽 경계’)을 표현하도록 학습되는 경향을 보인다. 네트워크는 이러한 공간적 맵들을 선형적으로 조합함으로써(예: 한 프로토타입에서 다른 프로토타입을 빼는 방식) 동일 클래스에 속하는 객체일지라도 특정 인스턴스만을 효과적으로 분리해낼 수 있다.9</p>
<h2>3.  아키텍처 심층 분석</h2>
<h3>3.1  백본 네트워크와 특징 피라미드 네트워크(FPN)</h3>
<p>YOLACT는 ResNet-101과 같은 표준 단일 단계(one-stage) 객체 탐지기 위에 구축되며, 특징 피라미드 네트워크(Feature Pyramid Network, FPN)와 결합된다.4 이를 통해 강력한 사전 학습된 특징 추출기를 활용할 수 있다. FPN은 다양한 스케일의 객체를 효과적으로 처리하기 위해 여러 수준의 의미 정보가 풍부한 특징 맵 피라미드를 생성하는 데 결정적인 역할을 한다.8 YOLACT는 원본 FPN을 일부 수정하여 P3부터 P7까지의 특징 레벨을 생성하여 사용한다.11</p>
<h3>3.2  프로토넷(Protonet): <code>k</code>개의 프로토타입 마스크 생성</h3>
<p>프로토넷은 <span class="math math-inline">k</span>개의 프로토타입 마스크 생성을 전담하는 독립된 브랜치이다.8 이 브랜치는 완전 컨볼루션 네트워크(FCN) 구조를 가지며, FPN의 가장 깊으면서도 해상도가 높은 특징 맵인 P3를 입력으로 받는다.11 이러한 선택은 의도적인 설계로, P3는 강한 의미 정보와 세밀한 공간 정보를 모두 포함하고 있어 고품질 마스크 생성에 가장 이상적이다.11 프로토넷은 P3 특징 맵을 입력 이미지의 1/4 크기로 업샘플링하여 최종 프로토타입을 생성하며, 마지막 레이어는 각 프로토타입에 해당하는 <span class="math math-inline">k</span>개의 채널을 가진다.9 활성화 함수로는 ReLU가 사용되어 출력이 비제한적(unbounded)이 되도록 함으로써, 네트워크가 특정 패턴에 대해 높은 확신도를 가질 수 있게 한다.9</p>
<h3>3.3  예측 헤드(Prediction Head): 클래스, 경계 상자, 그리고 마스크 계수 예측</h3>
<p>예측 헤드는 프로토넷과 병렬로 동작하며, 각 FPN 레벨(P3-P7)에 작은 예측 네트워크들이 부착된 형태이다.8 각 특징 맵의 모든 앵커 위치에 대해, 예측 헤드는 세 가지 예측을 동시에 수행한다 1:</p>
<ol>
<li><span class="math math-inline">c</span>개의 클래스 신뢰도 점수</li>
<li><span class="math math-inline">4</span>개의 경계 상자 회귀 오프셋</li>
<li><span class="math math-inline">k</span>개의 마스크 계수</li>
</ol>
<p>예측된 <span class="math math-inline">k</span>개의 마스크 계수에는 <code>tanh</code> 활성화 함수가 적용된다. 이는 출력을 -1과 1 사이로 제한하여 학습을 안정시키고, 프로토타입들을 더하거나 빼는 방식의 조합에 적합한 값을 제공한다.11</p>
<p>이러한 아키텍처 설계에는 정교한 비대칭성이 존재한다. 공간적으로 풍부한 마스크를 생성하는 프로토넷은 고해상도의 P3 특징 맵에만 연결되는 반면, 다양한 크기의 객체 탐지를 담당하는 예측 헤드는 모든 피라미드 레벨(P3-P7)에 연결된다. 이는 각 서브태스크의 요구사항에 맞춰 특징 소스를 최적화한 결과이다. 마스크 생성은 세밀한 표현을 위해 고해상도 특징이 필요하지만, 객체 탐지는 작은 객체부터 큰 객체까지 모두 처리하기 위해 다중 스케일 특징이 필수적이다. 이처럼 각 브랜치의 역할을 분리하고 그에 맞는 최적의 특징을 공급하는 것은 효율적인 다중 작업(multi-task) 딥러닝 모델을 설계하는 데 있어 중요한 원칙을 보여준다.</p>
<h2>4.  최종 마스크 조합 및 추론 과정</h2>
<h3>4.1  선형 결합 메커니즘의 수학적 원리</h3>
<p>최종 마스크 조합 단계는 프로토타입과 계수의 선형 결합으로, 매우 단순하면서도 효율적으로 구현된다.7</p>
<p><span class="math math-inline">k</span>개의 프로토타입 마스크로 구성된 <span class="math math-inline">h \times w \times k</span> 크기의 행렬을 <span class="math math-inline">P</span>라 하고, 필터링을 통과한 <span class="math math-inline">n</span>개 인스턴스에 대한 마스크 계수 행렬을 <span class="math math-inline">n \times k</span> 크기의 <span class="math math-inline">C</span>라고 하자. 최종적으로 <span class="math math-inline">n</span>개의 인스턴스 마스크 <span class="math math-inline">M</span> (크기 <span class="math math-inline">h \times w</span>)은 단일 행렬 곱셈과 시그모이드(sigmoid) 함수를 통해 생성된다.11 시그모이드 함수는 최종 값을 0과 1 사이로 매핑하여 마스크 확률 값으로 변환한다.</p>
<p>수학식은 다음과 같다:<br />
<span class="math math-display">
M = \sigma(P C^T)
</span><br />
이 연산은 현대 GPU에서 극도로 빠르게 수행될 수 있으며, 마스크 생성과 조합을 포함한 전체 마스크 브랜치는 약 5ms의 오버헤드만을 추가할 뿐이다.4</p>
<h3>4.2  후처리: Fast NMS를 통한 중복 탐지 제거</h3>
<p>마스크가 생성된 후, 중복된 탐지를 제거하기 위해 비-최대 억제(Non-Maximum Suppression, NMS)가 적용된다. 하지만 전통적인 NMS는 순차적으로 동작하여 속도가 느릴 수 있다.11 YOLACT는 이를 해결하기 위해 <strong>Fast NMS</strong>라는 병렬화된 NMS 버전을 도입했다.4 Fast NMS는 성능 저하를 최소화하면서 약 12ms의 속도 향상을 가져온다.6</p>
<p>Fast NMS는 각 클래스에 대해 신뢰도 점수가 가장 높은 상위 <span class="math math-inline">n</span>개의 탐지를 대상으로, 먼저 쌍별(pairwise) IoU(Intersection over Union) 행렬을 계산한다. 그 후, 특정 임계값 이상의 IoU를 가지면서 자신보다 더 높은 점수를 가진 탐지가 있는 경우, 해당 탐지를 동시에 제거하는 방식으로 병렬 처리를 수행한다.11 마지막으로, 살아남은 마스크들은 예측된 경계 상자에 의해 잘라내어져(cropping) 최종 인스턴스 분할 결과를 형성한다.9</p>
<h2>5.  모델 학습 및 최적화 전략</h2>
<h3>5.1  종합 손실 함수: <span class="math math-inline">L_{cls}</span>, <span class="math math-inline">L_{box}</span>, <span class="math math-inline">L_{mask}</span>의 통합</h3>
<p>YOLACT는 세 가지 손실 함수의 가중합으로 구성된 종합 손실 함수(composite loss function)를 사용하여 종단간(end-to-end)으로 학습된다.8</p>
<p>총 손실 <span class="math math-inline">L</span>은 다음과 같이 정의된다:<br />
<span class="math math-display">
L = L_{cls} + \alpha L_{box} + \beta L_{mask}
</span><br />
여기서 각 항은 다음과 같다:</p>
<ul>
<li><span class="math math-inline">L_{cls}</span>: 분류 손실 (Classification Loss), 일반적으로 다중 클래스 분류를 위한 소프트맥스 손실을 사용한다.</li>
<li><span class="math math-inline">L_{box}</span>: 경계 상자 회귀 손실 (Box Regression Loss), 일반적으로 Smooth L1 손실을 사용한다.</li>
<li><span class="math math-inline">L_{mask}</span>: 마스크 손실 (Mask Loss), 조합된 최종 마스크 <span class="math math-inline">M</span>과 실제 정답 마스크 <span class="math math-inline">M_{gt}</span> 간의 픽셀 단위 이진 교차 엔트로피(Binary Cross-Entropy, BCE) 손실이다.9</li>
</ul>
<p>논문에서 사용된 가중치 <span class="math math-inline">\alpha</span>와 <span class="math math-inline">\beta</span>는 각각 1.5와 6.125이다.11 학습 시, 마스크 손실은 실제 정답 경계 상자 영역 내에서만 계산되며, 해당 영역의 면적으로 나누어진다. 이는 작은 객체들이 전체 손실에 의미 있게 기여하도록 보장하는 역할을 한다.11</p>
<h3>5.2  의미론적 분할 손실을 통한 성능 향상</h3>
<p>추론 시간에는 비용을 추가하지 않으면서 특징의 표현력을 높이기 위해, 학습 과정 중에 보조적인 의미론적 분할 손실(semantic segmentation loss)이 추가된다.11 가장 큰 특징 맵인 P3에 간단한 1x1 컨볼루션 레이어를 부착하여, 모든 <span class="math math-inline">c</span>개 클래스에 대한 의미론적 분할 맵을 예측하도록 한다. 이 브랜치는 추론 시에는 완전히 제거되므로 속도에 전혀 영향을 미치지 않는다. 이 간단한 추가 장치만으로도 프로토넷이 사용하는 P3 특징의 의미론적 표현력이 강화되어, 최종 mAP를 0.4점 향상시키는 효과를 가져온다.11</p>
<p>여기서 주목할 점은 프로토타입 자체에 직접적으로 적용되는 손실 함수가 없다는 것이다.9 프로토넷은 오직 최종적으로 조합된 마스크의 손실(<span class="math math-inline">L_{mask}</span>)을 통해서만 간접적으로 감독된다. 이는 네트워크가 프로토타입의 형태를 자유롭게 학습할 수 있도록 허용하며, 그 결과로 앞서 언급한 ’내재적 지역화’와 같은 효과적인 동작이 발현될 수 있는 기반이 된다. 만약 프로토타입이 특정 클래스의 의미론적 분할과 같은 명시적인 목표를 갖도록 강제되었다면, 인스턴스를 구별하는 데 필요한 공간적 분할 능력과 같은 추상적인 표현을 학습하기 어려웠을 것이다. 이는 최종 출력에 대한 손실만으로도 모델이 최적의 중간 표현을 스스로 구성할 수 있음을 보여주는 강력한 사례이다.</p>
<h2>6.  성능 분석 및 벤치마크</h2>
<h3>6.1  MS COCO 데이터셋 성능 평가</h3>
<p>YOLACT의 성능은 실시간 처리 능력과 경쟁력 있는 정확도의 균형을 통해 입증된다. MS COCO 데이터셋은 이러한 성능을 평가하는 표준 벤치마크로 사용된다. 아래 표는 YOLACT와 그 변형 모델들을 Mask R-CNN과 같은 기존의 고정확도 모델 및 다른 실시간 모델들과 비교한 결과이다.</p>
<p><strong>표 1: MS COCO <code>val2017</code> 벤치마크 비교</strong></p>
<table><thead><tr><th>모델</th><th>백본</th><th>AP (mask)</th><th>AP (box)</th><th>AP_S</th><th>AP_M</th><th>AP_L</th><th>FPS (Titan Xp)</th><th>FPS (RTX 3090)</th><th></th></tr></thead><tbody>
<tr><td>Mask R-CNN</td><td>ResNet-101-FPN</td><td>35.7</td><td>39.6</td><td>18.6</td><td>39.3</td><td>48.9</td><td>~12 FPS</td><td>23.2</td><td></td></tr>
<tr><td>YOLACT-550</td><td>ResNet-101-FPN</td><td>29.8</td><td>32.5</td><td>10.1</td><td>32.3</td><td>50.2</td><td><strong>33.5</strong></td><td>41.0</td><td></td></tr>
<tr><td>YOLACT++</td><td>ResNet-50-FPN</td><td>34.1</td><td>36.6</td><td>13.5</td><td>37.1</td><td>51.1</td><td><strong>33.5</strong></td><td>-</td><td></td></tr>
<tr><td>CenterMask</td><td>V-99-eSE-FPN</td><td>36.7</td><td>40.9</td><td>17.2</td><td>39.8</td><td>53.4</td><td>35.7</td><td>45.2</td><td></td></tr>
<tr><td>SOLOv2</td><td>ResNet-50-DCN</td><td>36.4</td><td>-</td><td>13.1</td><td>40.1</td><td>57.8</td><td>~29 FPS</td><td>37.0</td><td></td></tr>
<tr><td>YolactEdge</td><td>ResNet-101-FPN</td><td>29.8</td><td>32.5</td><td>10.1</td><td>32.3</td><td>50.2</td><td>~80 FPS*</td><td>53.5 / 177.3*</td><td></td></tr>
</tbody></table>
<p>데이터는 5에서 종합됨. FPS 값은 하드웨어 및 배치 크기에 따라 달라질 수 있음.</p>
<p><code>*</code>는 TensorRT 최적화를 의미함.</p>
<p>표에서 볼 수 있듯이, YOLACT-550은 Mask R-CNN보다 약 3배 빠른 속도를 보이면서도 준수한 정확도를 유지한다. 특히 YOLACT++는 속도를 거의 그대로 유지하면서 정확도를 34.1 mAP까지 끌어올려 속도와 정확도 간의 상충 관계를 크게 개선했다. YolactEdge는 TensorRT 최적화를 통해 압도적인 추론 속도를 보여주어 엣지 디바이스에서의 실시간 처리에 대한 가능성을 입증했다.</p>
<h3>6.2  정성적 평가: 타 모델과의 마스크 품질 비교</h3>
<p>YOLACT는 특히 큰 객체에 대해 Mask R-CNN이나 FCIS와 같은 다른 모델들보다 눈에 띄게 높은 품질의 마스크를 생성한다.4 이는 특징 재풀링 단계에서 발생하는 정보 손실 및 정렬 오류가 없기 때문이다.3 프로토타입이 전체 특징 맵을 기반으로 생성되므로, 더 많은 공간적 세부 정보를 보존할 수 있어 객체의 경계를 더 깔끔하고 정확하게 표현한다.4 또한, 비디오 데이터에 적용했을 때, 연속된 프레임 간에 프로토타입이 안정적으로 유지되는 경향이 있어 별도의 처리 없이도 뛰어난 시간적 일관성(temporal stability)을 보인다.4</p>
<h3>6.3  어블레이션 연구: 설계 선택의 영향 분석</h3>
<p>YOLACT의 각 설계 요소가 전체 성능에 미치는 영향을 분석하기 위해 다양한 어블레이션 연구(ablation studies)가 수행되었다.</p>
<ul>
<li><strong>프로토타입 개수:</strong> 프로토타입의 개수 <span class="math math-inline">k</span>를 조절하며 속도와 정확도 간의 상충 관계를 분석했다. <span class="math math-inline">k</span>가 증가할수록 표현력이 높아져 정확도가 상승하지만, 계산량이 늘어나는 경향이 관찰되었다.</li>
<li><strong>Fast NMS:</strong> Fast NMS는 기존 NMS 대비 약 12ms의 속도 향상을 가져오면서도 성능 저하는 미미한 수준임을 입증했다.4</li>
<li><strong>의미론적 분할 손실:</strong> 학습 중에만 사용되는 이 보조 손실은 추론 속도에 영향을 주지 않으면서 mAP를 0.4점 향상시키는 효과적인 정규화 기법임이 확인되었다.11</li>
<li><strong>주의 집중 메커니즘:</strong> 후속 연구에서는 YOLACT의 FPN에 PSA(Pixel Self-Attention), CBAM과 같은 주의 집중 모듈을 통합하여 특징 표현을 강화함으로써 추가적인 성능 향상을 달성할 수 있음을 보여주었다.17</li>
</ul>
<h2>7.  YOLACT의 진화: YOLACT++와 YolactEdge</h2>
<p>YOLACT의 기본 아키텍처는 단일 성공 모델에 그치지 않고, 다양한 요구사항에 맞춰 확장 및 최적화될 수 있는 유연한 플랫폼으로서의 가능성을 입증했다. YOLACT++는 정확도 향상에, YolactEdge는 엣지 디바이스에서의 효율성에 초점을 맞춰 진화했다.</p>
<h3>7.1  YOLACT++: 정확도 향상을 위한 개선 사항</h3>
<p>YOLACT++는 원본 YOLACT의 속도를 거의 저하시키지 않으면서 마스크 정확도를 크게 향상시킨 후속 모델이다.18 MS COCO 데이터셋에서 34.1 mAP를 33.5 FPS로 달성하며, 실시간 모델의 정확도 기준을 한 단계 끌어올렸다.15 주요 개선 사항은 다음과 같다.</p>
<ul>
<li><strong>변형 가능한 컨볼루션 (Deformable Convolutions, DCNv2):</strong> ResNet 백본의 C3-C5 단계에 있는 마지막 3x3 컨볼루션을 DCNv2로 대체했다. DCN은 고정된 격자 형태가 아닌, 객체의 실제 형태에 맞춰 동적으로 샘플링 위치를 학습하므로 다양한 크기, 회전, 종횡비를 가진 객체에 더 효과적으로 대응할 수 있다. 이 개선으로 8ms의 속도 저하가 발생했지만, 마스크 mAP는 1.8점이나 상승했다.18</li>
<li><strong>최적화된 예측 헤드:</strong> 앵커의 종횡비를 기존의 [1, 1/2, 2]에서 [1, 1/2, 2, 1/3, 3]으로 확장하고 스케일을 최적화하여 경계 상자 예측의 재현율(recall)을 높였다.18</li>
<li><strong>빠른 마스크 재평가 네트워크 (Fast Mask Re-scoring Network):</strong> 분류 신뢰도 점수와 실제 마스크 품질 간의 불일치 문제를 해결하기 위해 도입되었다. 이 네트워크는 생성된 마스크를 입력받아 실제 정답과의 IoU를 예측한다. 예측된 IoU 점수를 기존의 분류 점수와 곱하여 최종 신뢰도를 재조정함으로써, 품질이 낮은 마스크의 순위를 낮추고 전반적인 AP를 향상시킨다. 이 추가 브랜치는 6개의 레이어로 구성된 작은 FCN이며, 단 1.2ms의 오버헤드만으로 mAP를 1.0점 향상시키는 높은 효율을 보였다.18</li>
</ul>
<p><strong>표 2: YOLACT++ 개선 사항에 대한 어블레이션 연구</strong></p>
<table><thead><tr><th>구성</th><th>마스크 mAP</th><th>속도 (FPS on Titan Xp)</th><th>속도 오버헤드 (ms)</th><th></th></tr></thead><tbody>
<tr><td>YOLACT (기본)</td><td>31.2</td><td>34.5</td><td>-</td><td></td></tr>
<tr><td>+ 변형 가능한 컨볼루션 (DCNv2)</td><td>33.0 (+1.8)</td><td>26.5</td><td>+8.0</td><td></td></tr>
<tr><td>+ 빠른 마스크 재평가</td><td>32.2 (+1.0)</td><td>33.3</td><td>+1.2</td><td></td></tr>
<tr><td><strong>YOLACT++ (모든 개선 적용)</strong></td><td><strong>34.1</strong></td><td><strong>33.5</strong></td><td><strong>~1.2</strong> (최적화 구현)</td><td></td></tr>
</tbody></table>
<p>데이터는 15에서 종합됨. 기본 mAP는 논문마다 약간의 차이가 있으며, 값은 YOLACT++ 논문에 보고된 향상 폭을 나타냄.</p>
<h3>7.2  YolactEdge: 엣지 디바이스를 위한 실시간 분할</h3>
<p>YolactEdge는 YOLACT를 NVIDIA Jetson AGX Xavier와 같은 저전력, 저사양 엣지 디바이스에 배포하기 위해 최적화한 버전이다.5 Jetson AGX Xavier에서 최대 30.8 FPS, RTX 2080 Ti에서는 172.7 FPS라는 놀라운 속도를 달성하며, 엣지 환경에서 기존 방법들보다 3~5배 빠른 성능을 보였다.5 최적화는 두 가지 방향으로 이루어졌다.</p>
<ol>
<li><strong>시스템 수준 최적화:</strong> NVIDIA의 TensorRT 추론 엔진을 적용하여 모델을 양자화(quantization)하고 연산을 최적화했다. 이를 통해 정확도 저하를 최소화하면서 모델 크기를 줄이고 계산 속도를 대폭 향상시켰다.5</li>
<li><strong>알고리즘 수준 최적화:</strong> 비디오 처리를 위해 새로운 **특징 워핑 모듈(feature warping module)**을 도입했다. 이는 프레임 간의 시간적 중복성(temporal redundancy)을 활용하여, 이전 프레임의 특징을 현재 프레임으로 변환하고 전파한다. 이를 통해 계산 비용이 큰 백본 네트워크의 실행 빈도를 줄여 전체적인 처리 속도를 높였다.5</li>
</ol>
<p>이러한 진화 과정은 YOLACT의 ’프로토타입 + 계수’라는 핵심 아이디어가 얼마나 견고하고 확장 가능한지를 보여준다. YOLACT++는 정확도 향상을 위한 모듈을 추가했고, YolactEdge는 배포 효율성을 위한 최적화를 적용했다. 이는 YOLACT가 실시간 밀집 예측 모델을 구축하기 위한 훌륭한 청사진을 제공했음을 의미한다.</p>
<h2>8.  결론 및 전망</h2>
<h3>8.1  YOLACT의 기여와 영향 요약</h3>
<p>YOLACT는 인스턴스 분할 분야에서 정확도와 속도 사이의 오랜 상충 관계를 성공적으로 해결한 기념비적인 모델이다. 분할 작업을 프로토타입 생성과 계수 예측이라는 두 개의 병렬 태스크로 분해하고, 특징 재풀링이라는 순차적 병목을 제거함으로써 실시간 애플리케이션을 위한 새로운 성능 기준을 수립했다. 이 혁신적인 접근 방식은 단순하고 빠르면서도 고품질의 마스크를 생성했으며, YOLACT++와 YolactEdge와 같은 후속 연구를 위한 견고한 플랫폼 역할을 했다.</p>
<h3>8.2  실시간 인스턴스 분할 분야의 향후 연구 방향</h3>
<p>YOLACT 이후, 실시간 인스턴스 분할 분야는 지속적으로 발전하고 있다. SOLO와 같은 앵커 프리(anchor-free) 방식이나 Mask2Former와 같은 트랜스포머 기반 모델들이 등장하며 정확도와 효율성의 한계를 더욱 확장하고 있다.16 향후 연구는 단일 단계(one-shot) 분할의 품질을 더욱 향상시키고, NMS와 같은 복잡한 후처리 과정에 대한 의존도를 줄이며, 모바일 및 엣지 디바이스 배포를 위한 더욱 경량화된 아키텍처 개발에 집중될 것으로 보인다. YOLACT의 병렬 설계 원리를 새로운 아키텍처 요소와 결합하려는 시도 또한 계속될 것이다. 또한, 인스턴스 분할을 넘어 다른 밀집 예측 과제에 YOLACT와 유사한 원리를 적용하는 것은 여전히 유망한 연구 분야로 남아 있다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>RailYolact - A Yolact Focused on edge for Real-Time Rail Segmentation - arXiv, https://arxiv.org/html/2410.09612v1</li>
<li>[PDF] YOLACT: Real-Time Instance Segmentation - Semantic Scholar, https://www.semanticscholar.org/paper/YOLACT%3A-Real-Time-Instance-Segmentation-Bolya-Zhou/653d92ca61d77a906eabb880f40cac12f6f1dc12</li>
<li>Real-time Instance Segmentation | Yolact – Weights &amp; Biases - Wandb, https://wandb.ai/jackbailin/Yolact/reports/YOLACT-Real-time-Instance-Segmentation–Vmlldzo0MTA2NTI</li>
<li>YOLACT Real-time Instance Segmentation - Computer Science | UC Davis Engineering, https://web.cs.ucdavis.edu/~yjlee/projects/iccv2019_yolact.pdf</li>
<li>YolactEdge: Real-time Instance Segmentation on the Edge - arXiv, https://arxiv.org/pdf/2012.12259</li>
<li>[1904.02689] YOLACT: Real-time Instance Segmentation - arXiv, https://arxiv.org/abs/1904.02689</li>
<li>Real-time Instance Segmentation | Yolact – Weights &amp; Biases - Wandb, https://wandb.ai/jackbailin/Yolact/reports/YOLACT-Real-time-Instance-Segmentation–Vmlldzo0MTA1NzY</li>
<li>YOLACT Explained: Revolutionizing Real-Time Instance Segmentation - Ikomia, https://www.ikomia.ai/blog/yolact-instance-segmentation-revolution</li>
<li>YOLACT (Real time Instance Segmentation) | by Anmol Dua - Medium, https://medium.com/@anmoldua/yolact-important-points-125268f475d0</li>
<li>YOLACT - Semantic Scholar, https://pdfs.semanticscholar.org/d81a/b0929ba7c3b60064e4348073ec20157b8406.pdf</li>
<li>Brief Review — YOLACT: Real-time Instance Segmentation | by Sik …, https://sh-tsang.medium.com/brief-review-yolact-real-time-instance-segmentation-66f48f9c5be0</li>
<li>Architecture of YOLACT++ model. It uses ResNet-101 with FPN as the… - ResearchGate, https://www.researchgate.net/figure/Architecture-of-YOLACT-model-It-uses-ResNet-101-with-FPN-as-the-feature-backbone-and_fig2_354056614</li>
<li>[CV2019/PaperSummary] YOLACT :Real-time Instance Segmentation | by abhigoku10, https://abhigoku10.medium.com/cv2019-papersummary-yolact-real-time-instance-segmentation-e62fa721957f</li>
<li>YOLACT: Real-Time Instance Segmentation - CVF Open Access, https://openaccess.thecvf.com/content_ICCV_2019/papers/Bolya_YOLACT_Real-Time_Instance_Segmentation_ICCV_2019_paper.pdf</li>
<li>[1912.06218] YOLACT++: Better Real-time Instance Segmentation - arXiv, https://arxiv.org/abs/1912.06218</li>
<li>Benchmarking Deep Learning Models for Instance Segmentation, https://www.mdpi.com/2076-3417/12/17/8856</li>
<li>Ablation study of different attention block with UNet and LinkNet encoder - ResearchGate, https://www.researchgate.net/figure/Ablation-study-of-different-attention-block-with-UNet-and-LinkNet-encoder_tbl1_369238754</li>
<li>Brief Review — YOLACT++ Better Real-Time Instance Segmentation | by Sik-Ho Tsang, https://sh-tsang.medium.com/brief-review-yolact-better-real-time-instance-segmentation-52c16f83517b</li>
<li>YOLACT++: New Method For Instance Segmentation In Real-Time, https://neurohive.io/en/news/yolact-new-method-for-instance-segmentation-in-real-time/</li>
<li>[2012.12259] YolactEdge: Real-time Instance Segmentation on the Edge - arXiv, https://arxiv.org/abs/2012.12259</li>
<li>COCO test-dev Benchmark (Instance Segmentation) | Papers With Code, https://paperswithcode.com/sota/instance-segmentation-on-coco?p=yolact-real-time-instance-segmentation</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>