<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:영상 분할 (Image Segmentation) 모델</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>영상 분할 (Image Segmentation) 모델</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">이미지 분할 (Image Segmentations)</a> / <span>영상 분할 (Image Segmentation) 모델</span></nav>
                </div>
            </header>
            <article>
                <h1>영상 분할 (Image Segmentation) 모델</h1>
<h2>1.  영상 분할의 정의와 중요성</h2>
<h3>1.1  영상 분할(Image Segmentation)의 개념 정의</h3>
<p>영상 분할(Image Segmentation)은 디지털 이미지를 구성하는 픽셀들을 특정 기준에 따라 그룹화하여, 의미 있는 여러 개의 세그먼트(segment) 또는 객체(object)로 분할하는 컴퓨터 비전의 핵심 과제이다.1 이는 단순히 이미지를 나누는 것을 넘어, 이미지 내 모든 픽셀에 특정 클래스 레이블(class label)을 할당함으로써 이미지의 표현을 단순화하고, 보다 의미 있고 분석하기 쉬운 형태로 변환하는 것을 궁극적인 목표로 삼는다.3 이 과정을 통해 생성된 결과물은 일반적으로 각 픽셀의 소속 클래스를 색상이나 값으로 표현하는 분할 마스크(segmentation mask) 형태를 띤다.3</p>
<h3>1.2  컴퓨터 비전 계층에서의 영상 분할의 역할</h3>
<p>컴퓨터 비전 기술의 발전은 이미지에 대한 이해 수준을 점차 심화시키는 방향으로 이루어져 왔다. 이 계층 구조에서 영상 분할은 이미지 분류(Image Classification)와 객체 탐지(Object Detection)를 넘어서는 가장 고차원적인 이미지 이해 작업으로 자리매김한다.3</p>
<ul>
<li><strong>이미지 분류 (Image Classification):</strong> “이미지 전체에 무엇이 있는가?“라는 질문에 답하며, 이미지 한 장에 대해 ‘자동차’ 또는 ’고양이’와 같은 단일 레이블을 할당한다.3 이는 이미지의 전반적인 내용을 파악하는 가장 기본적인 단계이다.</li>
<li><strong>객체 탐지 (Object Detection):</strong> “객체가 어디에 있는가?“라는 질문으로 나아가, 이미지 내에 존재하는 객체들의 위치를 경계 상자(bounding box)로 표시하고 각 객체의 클래스를 분류한다.3 이는 객체의 존재 유무와 대략적인 위치 정보를 제공한다.</li>
<li><strong>영상 분할 (Image Segmentation):</strong> “객체의 정확한 형태와 경계는 무엇인가?“라는 가장 정밀한 질문에 답한다. 경계 상자만으로는 알 수 없는 객체의 정확한 윤곽선을 픽셀 수준에서 식별함으로써, 이미지 내 모든 구성 요소의 형태와 공간적 관계에 대한 가장 상세하고 풍부한 정보를 제공한다.6</li>
</ul>
<p>이러한 기술의 발전 과정은 인공지능 시스템이 요구하는 정보의 세분화 수준이 점차 높아지고 있음을 시사한다. 초기 컴퓨터 비전은 이미지 전체를 하나의 단위로 인식하는 분류 작업에 집중했다.3 그러나 자율주행차나 의료 영상 분석과 같은 실용적인 응용 분야에서는 객체의 위치 정보가 필수적이 되면서 경계 상자를 활용하는 객체 탐지 기술이 중요해졌다.3 하지만 경계 상자는 객체의 정확한 모양이나 점유 영역을 표현하지 못하는 명백한 한계를 가진다. 예를 들어, 자율주행차가 주행 가능한 도로 영역을 정확히 판단하거나 6, 의료 영상에서 종양의 정밀한 크기와 형태를 측정하기 위해서는 5 픽셀 단위의 세밀한 정보가 반드시 필요하다. 이러한 현실 세계의 복잡한 문제 해결 요구가 영상 분할 기술의 발전을 필연적으로 이끌었다. 따라서 영상 분할은 단순한 기술적 진보를 넘어, 복잡한 환경과 정밀하게 상호작용해야 하는 고도화된 인공지능 시스템의 근본적인 요구사항에서 비롯된 필연적인 발전 단계라 할 수 있다.</p>
<h3>1.3  영상 분할의 핵심 목표: 이미지의 의미론적 이해</h3>
<p>영상 분할의 핵심 목표는 단순히 이미지를 영역별로 나누는 것을 넘어, 이미지에 대한 깊이 있는 의미론적 이해(semantic understanding)를 달성하는 데 있다. 각 픽셀이 어떤 의미를 가진 클래스에 속하는지를 식별함으로써, 기계는 이미지 속 객체들의 관계와 장면(scene)의 전체적인 맥락을 파악할 수 있게 된다.2 예를 들어, 도로 장면 이미지에서 ‘자동차’, ‘보행자’, ‘도로’, ‘신호등’ 픽셀을 각각 분할하고 나면, “자동차가 도로 위에 있고 보행자 근처에 있다“는 식의 공간적, 의미론적 관계 추론이 가능해진다. 이처럼 장면(scene)에 대한 완전한 이해를 가능하게 하는 영상 분할은 자율 주행, 의료 영상 분석, 로봇 공학 등 인간 수준의 정밀한 판단과 상호작용이 요구되는 다양한 첨단 응용 분야에서 없어서는 안 될 근간 기술로 평가받는다.1</p>
<h2>2.  영상 분할의 핵심 원리와 방법론</h2>
<p>영상 분할 기술은 크게 전통적인 이미지 처리 기법에 기반한 접근법과 데이터로부터 특징을 학습하는 딥러닝 기반 접근법으로 나눌 수 있다. 이 두 패러다임은 문제 해결 방식에서 근본적인 차이를 보인다.</p>
<h3>2.1  전통적 접근법: 유사성(Similarity)과 불연속성(Discontinuity) 기반</h3>
<p>전통적인 영상 분할 기법은 이미지 픽셀들이 갖는 고유한 특성을 분석하여 영역을 구분하는 두 가지 근본적인 원리, 즉 ’유사성’과 ’불연속성’에 기반한다.7 이는 사람이 사물을 인지할 때 비슷한 색이나 질감을 가진 영역을 하나의 덩어리로 보거나, 색이 급격히 변하는 지점을 경계로 인식하는 방식과 유사하다.</p>
<h4>2.1.1  유사성 기반 기법</h4>
<p>유사성 기반 기법은 픽셀들이 색상, 밝기, 질감 등에서 유사한 특성을 공유하는 영역으로 그룹화하는 방식이다.7</p>
<ul>
<li><strong>임계값 기반 분할 (Thresholding):</strong> 가장 간단하고 직관적인 방법으로, 픽셀의 강도(intensity) 값을 미리 정해진 임계값(threshold)과 비교하여 이미지를 전경(foreground)과 배경(background) 등 두 개 이상의 영역으로 나눈다.5 이미지 전체에 단일 임계값을 적용하는 전역 임계값(Global Thresholding) 방식과 이미지의 각기 다른 부분에 별도의 임계값을 적용하여 조명 변화에 대응하는 적응형 임계값(Adaptive Thresholding) 방식이 있다.8</li>
<li><strong>영역 기반 분할 (Region-based Segmentation):</strong> 이 기법은 인접한 픽셀들 간의 유사성을 기준으로 영역을 확장하거나 병합한다. 대표적으로 특정 ‘시드(seed)’ 픽셀에서 시작하여 유사한 특성을 가진 이웃 픽셀들을 점진적으로 병합해 나가는 영역 확장(Region Growing) 방식이 있다.3 반대로, 전체 이미지를 작은 영역으로 반복적으로 분할한 뒤, 인접하고 유사한 영역들을 다시 병합하는 분할 및 병합(Split and Merge) 기법도 사용된다.7</li>
<li><strong>클러스터링 기반 분할 (Clustering-based Segmentation):</strong> K-평균(K-means)이나 평균 이동(Mean-shift)과 같은 비지도 학습 알고리즘을 활용하여 픽셀들을 특징 공간(feature space)에서 몇 개의 군집(cluster)으로 그룹화하는 방식이다.5 각 군집은 하나의 분할된 세그먼트에 해당한다.</li>
</ul>
<h4>2.1.2  불연속성 기반 기법</h4>
<p>불연속성 기반 기법은 픽셀 값의 급격한 변화, 즉 엣지(edge)를 감지하여 객체의 경계를 찾는 데 중점을 둔다.7</p>
<ul>
<li><strong>엣지 기반 분할 (Edge-based Segmentation):</strong> 이미지에서 픽셀의 밝기나 색상 값이 급격하게 변하는 지점은 객체의 경계일 가능성이 높다. Sobel, Canny, Laplacian of Gaussian (LoG)과 같은 엣지 탐지 연산자를 사용하여 이러한 경계선을 찾고, 찾아낸 경계선들을 연결하여 닫힌 영역을 형성함으로써 객체를 분할한다.7</li>
</ul>
<h3>2.2  딥러닝 기반 접근법: 특징 학습의 패러다임 전환</h3>
<p>전통적인 기법들은 사람이 직접 특징(예: 색상, 질감)을 정의하고 이를 기반으로 하는 수학적 모델에 의존하기 때문에 계산적으로 효율적이고 구현이 비교적 간단하다는 장점이 있다.7 그러나 조명 변화가 심하거나, 객체 간의 경계가 불분명하고, 질감이 복잡한 실제 환경의 이미지에서는 원하는 수준의 정확도를 달성하기 어렵다는 명백한 한계를 가진다.2</p>
<p>이러한 한계는 딥러닝, 특히 합성곱 신경망(CNN)의 등장으로 극복되었다. 딥러닝 기반 접근법의 핵심은 데이터로부터 직접 계층적인 특징(hierarchical features)을 자동으로 학습한다는 점이다.5 사람이 규칙을 설계하는 대신, 모델이 수많은 예제 데이터를 통해 스스로 이미지 분할에 유용한 특징이 무엇인지를 학습한다. 이 과정에서 영상 분할 문제를 해결하는 방식은 ’규칙 기반’에서 ’데이터 기반 학습’으로 근본적인 패러다임 전환을 맞이하게 되었다.</p>
<p>대부분의 딥러닝 기반 분할 모델은 <strong>인코더-디코더(Encoder-Decoder) 구조</strong>를 핵심 골격으로 채택한다.7</p>
<ul>
<li><strong>인코더 (Encoder):</strong> 입력 이미지로부터 점진적으로 공간적 차원(해상도)을 줄여나가면서, 이미지의 전반적인 맥락과 의미론적 정보(semantic information)를 담은 고차원 특징 맵(feature map)을 추출한다.</li>
<li><strong>디코더 (Decoder):</strong> 인코더에서 추출된 압축된 특징 맵을 다시 원래 이미지의 해상도로 점진적으로 확대(upsampling)한다. 이 과정에서 세부적인 공간 정보를 복원하여 최종적으로 픽셀 단위의 정밀한 분할 마스크를 생성한다.</li>
</ul>
<p>이러한 패러다임의 전환은 영상 분할 기술의 성능을 비약적으로 향상시켰다. 전통적 방법이 이미지 처리 ‘규칙’(예: ‘밝기 차이가 크면 경계다’)을 외부에서 명시적으로 주입하는 방식이었다면, 딥러닝은 대규모 데이터를 통해 ‘지식’(예: ‘이러한 픽셀 패턴의 조합은 자동차의 경계일 가능성이 높다’)을 모델 파라미터 내부에 스스로 내재화하는 방식으로 문제를 해결한다. 이는 영상 분할 문제를 ’해석’의 영역에서 ’학습’의 영역으로 옮겨온 중요한 철학적 변화이며, 오늘날 영상 분할 기술 발전의 핵심 동력이 되고 있다.</p>
<h2>3.  영상 분할 태스크의 분류</h2>
<p>영상 분할은 목표하는 결과물의 종류와 정보의 세분화 수준에 따라 크게 세 가지 주요 태스크로 분류된다: Semantic Segmentation, Instance Segmentation, 그리고 이 둘을 통합한 Panoptic Segmentation. 이러한 태스크의 발전 과정은 컴퓨터가 인간처럼 시각적 장면을 ‘완전하게’ 해석하도록 요구하는 방향으로 진행되어 왔으며, 이는 단순히 기술적 분류를 넘어 인공지능이 달성해야 할 목표 수준의 진화를 의미한다.</p>
<h3>3.1  Semantic Segmentation: 픽셀 단위의 의미론적 분류</h3>
<p>Semantic Segmentation(의미론적 분할)은 이미지 내의 모든 픽셀을 미리 정의된 특정 클래스(예: 자동차, 사람, 도로, 하늘)로 분류하는 것을 목표로 한다.4 이 태스크의 핵심 특징은 같은 클래스에 속하는 여러 객체 인스턴스(instance)를 구분하지 않고 모두 동일한 클래스로 취급한다는 점이다.12 예를 들어, 한 이미지 안에 여러 대의 자동차가 존재하더라도, Semantic Segmentation의 결과물에서는 이 모든 자동차 픽셀이 단순히 ’자동차’라는 단일 클래스로 레이블링된 하나의 영역으로 나타난다.12 이는 “이미지가 어떤 종류의 영역들로 구성되어 있는가?“라는 질문에 답하며, 장면의 전반적인 구성을 이해하는 데 중점을 둔다.</p>
<h3>3.2  Instance Segmentation: 개별 객체 인스턴스의 식별</h3>
<p>Instance Segmentation(인스턴스 분할)은 Semantic Segmentation에서 한 단계 더 나아가, 같은 클래스에 속하더라도 각기 다른 객체 인스턴스를 개별적으로 식별하고 분할하는 것을 목표로 한다.4 예를 들어, 이미지에 자동차가 세 대 있다면, 이들을 각각 ‘car_1’, ‘car_2’, ’car_3’과 같이 고유하게 구분하여 각각의 분할 마스크를 생성한다.12 이는 “이미지 안에 어떤 개별 객체들이 존재하는가?“라는 질문에 답하며, 객체의 개수를 세거나(counting), 특정 객체를 추적(tracking)하는 등의 응용 분야에서 필수적이다.15 자율주행차가 도로 위의 자동차들을 하나의 덩어리가 아닌 ’개별 자동차’로 인식해야 하는 필요성이 바로 이러한 Instance Segmentation 기술의 발전을 이끌었다.</p>
<h3>3.3  Panoptic Segmentation: Semantic과 Instance의 통합</h3>
<p>Panoptic Segmentation(파놉틱 분할)은 Semantic Segmentation과 Instance Segmentation을 결합한 가장 포괄적이고 진보된 형태의 분할 방식이다.4 이 태스크의 목표는 이미지 내 모든 픽셀에 대해 의미론적 레이블(semantic label)과 인스턴스 ID(instance ID)를 동시에 할당하는 것이다.15 즉, 모든 픽셀은 자신이 어떤 클래스에 속하는지에 대한 정보와, 만약 그것이 개별적으로 식별 가능한 객체라면 몇 번째 인스턴스인지에 대한 정보를 모두 갖게 된다. 결과적으로 Panoptic Segmentation의 출력물은 분할된 영역들이 서로 겹치지 않으며(non-overlapping) 이미지 전체를 빈틈없이 포괄하는(collectively cover the entire image) 특징을 가진다.4 이는 “이미지 내 모든 픽셀은 어떤 종류의 영역에 속하며, 만약 그것이 객체라면 몇 번째 개체인가?“라는 최종적인 질문에 답함으로써, 기계의 시각적 인식을 인간의 포괄적인 시야(panoptic view) 수준으로 끌어올리려는 시도이다.</p>
<h3>3.4  ’Things’와 ‘Stuff’ 개념을 통한 심층 비교</h3>
<p>이 세 가지 분할 태스크의 차이점을 보다 명확하게 이해하기 위해 ’Things’와 ’Stuff’라는 개념을 도입할 수 있다.14</p>
<ul>
<li><strong>Things:</strong> 사람, 자동차, 동물과 같이 명확한 형태를 가지며 셀 수 있는(countable) 객체를 의미한다.</li>
<li><strong>Stuff:</strong> 하늘, 도로, 잔디와 같이 형태가 일정하지 않고 셀 수 없는(uncountable) 배경 영역이나 질감을 의미한다.</li>
</ul>
<p>이 개념을 통해 각 분할 태스크를 다음과 같이 재정의할 수 있다.</p>
<ul>
<li><strong>Semantic Segmentation:</strong> ’Things’와 ’Stuff’를 모두 클래스 단위로 구분한다. 모든 ‘Things’(예: 자동차들)는 하나의 클래스로 통합하여 처리한다.14</li>
<li><strong>Instance Segmentation:</strong> 주로 ’Things’에 집중하여 각 인스턴스를 개별적으로 분리한다. 배경에 해당하는 ’Stuff’는 일반적으로 분할 대상에서 제외하거나 중요하게 다루지 않는다.14</li>
<li><strong>Panoptic Segmentation:</strong> ’Things’는 Instance Segmentation처럼 개별 인스턴스 단위로 분할하고, ’Stuff’는 Semantic Segmentation처럼 클래스 단위로 분할한다. 이를 통해 이미지 전체에 대한 통일된(unified) 종합적인 이해를 제공한다.12</li>
</ul>
<p>이러한 분류는 영상 분할 기술이 단순히 픽셀을 그룹화하는 것을 넘어, 컴퓨터가 시각 세계를 얼마나 완전하고 구조적으로 이해할 수 있는지를 보여주는 척도이다.</p>
<p><strong>Table 1: 영상 분할 유형 비교</strong></p>
<table><thead><tr><th>특징</th><th>Semantic Segmentation</th><th>Instance Segmentation</th><th>Panoptic Segmentation</th></tr></thead><tbody>
<tr><td><strong>주요 목표</strong></td><td>픽셀 단위 클래스 분류</td><td>개별 객체 인스턴스 분리</td><td>포괄적인 장면 분할</td></tr>
<tr><td><strong>결과물</strong></td><td>클래스 맵 (Class Map)</td><td>각 객체별 마스크 집합</td><td>통합된 클래스 및 인스턴스 맵</td></tr>
<tr><td><strong>‘Things’ 처리</strong></td><td>클래스로 통합</td><td>개별 인스턴스로 분리</td><td>개별 인스턴스로 분리</td></tr>
<tr><td><strong>‘Stuff’ 처리</strong></td><td>클래스로 분할</td><td>일반적으로 무시</td><td>클래스로 분할</td></tr>
<tr><td><strong>인스턴스 구분</strong></td><td>불가능</td><td>가능</td><td>가능</td></tr>
<tr><td><strong>결과물 중첩</strong></td><td>불가능</td><td>가능 (마스크 간)</td><td>불가능</td></tr>
</tbody></table>
<p>이 표는 각 분할 방식의 정의와 특징을 추상적으로 이해하는 것을 넘어, ’Things’와 ’Stuff’라는 구체적인 개념을 기준으로 실제 결과물이 어떻게 달라지는지를 명확하게 보여준다. 이는 특정 응용 분야에 어떤 분할 기술이 적합한지 판단하는 실용적인 기준을 제공한다. 예를 들어, 주행 가능 영역만 파악하면 되는 경우 Semantic, 개별 차량의 추적이 중요하면 Instance, 그리고 장면 전체의 완전한 이해가 필요하면 Panoptic을 선택해야 함을 직관적으로 알 수 있게 한다.</p>
<h2>4.  딥러닝 기반 영상 분할의 핵심 아키텍처</h2>
<p>딥러닝 기반 영상 분할 기술의 발전은 혁신적인 신경망 아키텍처의 등장과 함께 이루어져 왔다. 이들 아키텍처는 단순히 성능이 좋은 모델을 넘어, 영상 분할이 마주한 근본적인 난제들, 즉 ‘정보 손실’, ‘정렬 오류’, ‘스케일 다양성’ 등에 대한 각기 다른 철학과 독창적인 해결책을 제시했다는 점에서 기념비적인 의미를 갖는다.</p>
<h3>4.1  U-Net: 의료 영상 분할의 혁신</h3>
<p>U-Net은 2015년 Olaf Ronneberger 연구팀이 의료 영상 분할을 목표로 개발한 완전 합성곱 신경망(Fully Convolutional Network, FCN) 기반 모델이다.10 적은 양의 데이터로도 매우 정밀한 분할이 가능하여 의료 영상 분야에 혁신을 가져왔으며, 이후 다양한 분야로 그 영향력을 확장했다.19</p>
<p>U-Net의 구조는 이름처럼 ’U’자 형태를 띠며, 크게 수축 경로(Contracting Path)와 확장 경로(Expansive Path)로 구성된다.10</p>
<ul>
<li><strong>Contracting Path (인코더):</strong> 이 경로는 일반적인 CNN 구조와 유사하다. 반복적인 3x3 Convolution 연산과 Rectified Linear Unit (ReLU) 활성화 함수, 그리고 2x2 Max Pooling 연산을 통해 입력 이미지의 특징을 추출하고 압축한다.10 이 과정에서 특징 맵의 공간 해상도는 점차 감소하는 대신, 특징 채널의 수는 두 배씩 증가한다. 이를 통해 이미지의 전반적인 맥락(context) 정보, 즉 ’무엇이 있는지’에 대한 의미론적 정보를 효과적으로 포착한다.7</li>
<li><strong>Expansive Path (디코더):</strong> 이 경로는 압축된 특징 맵의 해상도를 다시 점진적으로 복원하여 정밀한 지역화(localization) 정보, 즉 ’어디에 있는지’에 대한 정보를 생성한다. 이는 업샘플링(Upsampling) 또는 전치 합성곱(Transposed Convolution) 연산을 통해 특징 맵의 크기를 키우고 채널 수를 줄이는 방식으로 이루어진다.10</li>
<li><strong>Skip Connection (생략 연결):</strong> U-Net의 가장 핵심적인 혁신은 바로 이 생략 연결에 있다. 인코더의 각 단계에서 생성된 고해상도의 특징 맵을 디코더의 동일한 해상도를 가진 단계의 특징 맵과 직접 결합(concatenate)한다.10 딥러닝 인코더는 특징을 추출하는 과정에서 필연적으로 세밀한 공간 정보(예: 객체의 정확한 경계)를 잃어버리게 된다. Skip Connection은 이 ‘정보 손실’ 문제를 정면으로 해결한다. 즉, “과거의 고해상도 정보를 현재의 저해상도 정보와 결합하여 잃어버린 것을 복원하자“는 철학을 구현한 것이다. 이를 통해 디코더는 업샘플링 과정에서 희미해진 세부 정보를 효과적으로 복원할 수 있게 되어, 매우 정교하고 날카로운 분할 경계를 예측할 수 있다.7</li>
</ul>
<p>U-Net의 성공 이후, UNet++ 20, 3D U-Net 10 등 다양한 변형 모델이 제안되었으며, 의료 영상뿐만 아니라 위성 영상 분석, 자율 주행 등 광범위한 분야에서 표준적인 분할 아키텍처 중 하나로 자리 잡았다.19</p>
<h3>4.2  Mask R-CNN: Instance Segmentation의 표준</h3>
<p>Mask R-CNN은 2017년 Kaiming He 연구팀이 제안한 모델로, 객체 탐지 분야의 표준 모델인 Faster R-CNN을 기반으로 Instance Segmentation을 수행하는 강력하고 유연한 프레임워크이다.22</p>
<p>Mask R-CNN은 Faster R-CNN의 2단계 구조를 그대로 계승하고 확장한다.22</p>
<ol>
<li><strong>1단계 (Region Proposal Network, RPN):</strong> 입력 이미지에서 객체가 있을 법한 후보 영역(Region of Interest, RoI)을 제안한다.</li>
<li><strong>2단계 (Prediction Heads):</strong> 각 RoI에 대해 클래스 분류(classification)와 경계 상자 회귀(bounding box regression)를 수행한다. Mask R-CNN은 여기에 **마스크 브랜치(mask branch)**를 병렬적으로 추가했다. 이 마스크 브랜치는 각 RoI에 대해 픽셀 단위의 이진 마스크(binary mask)를 예측하는 작은 FCN으로 구성된다.23</li>
</ol>
<p>Mask R-CNN의 성공에는 <strong>RoIAlign</strong>이라는 핵심적인 기술이 기여했다. 기존 Faster R-CNN의 RoIPool 연산은 RoI의 특징을 추출할 때 소수점 좌표를 버리는 거친 공간 양자화(quantization)를 수행했다.23 이로 인해 원본 이미지의 픽셀과 추출된 특징 맵의 픽셀 간에 미세한 ’정렬 오류(misalignment)’가 발생했다. 이러한 불일치는 경계 상자를 예측하는 데는 큰 문제가 되지 않았지만, 픽셀 단위의 정밀함이 요구되는 마스크 예측에는 치명적인 성능 저하를 유발했다. Mask R-CNN은 “정확한 분할은 정확한 정렬에서 시작된다“는 철학을 바탕으로, 이 문제를 해결하기 위해 RoIAlign을 제안했다.23 RoIAlign은 양자화를 완전히 제거하고, 대신 이중 선형 보간법(bilinear interpolation)을 사용하여 RoI 내의 정확한 위치에서 특징 값을 샘플링한다.24 이를 통해 픽셀-투-픽셀 정렬(pixel-to-pixel alignment)을 완벽하게 유지함으로써 마스크 예측의 정확도를 획기적으로 향상시켰다.25</p>
<h3>4.3  DeepLab 계열: 다중 스케일 컨텍스트 정보의 활용</h3>
<p>DeepLab은 Google에서 개발한 Semantic Segmentation 모델 시리즈로, 실제 이미지에 다양한 크기의 객체들이 공존하는 ‘스케일 다양성(scale diversity)’ 문제를 효과적으로 해결하는 데 중점을 둔다.26</p>
<p>DeepLab의 핵심 기술은 다음과 같다.</p>
<ul>
<li><strong>Atrous (Dilated) Convolution:</strong> 우리말로는 ‘팽창 합성곱’ 또는 ’아트러스 컨볼루션’으로 불린다. 이는 표준 컨볼루션 필터의 커널(kernel) 내부에 간격을 두어(구멍을 뚫어) 적용하는 방식이다.28 파라미터 수나 계산량을 늘리지 않으면서도 필터가 한 번에 바라보는 영역, 즉 수용 영역(receptive field)을 기하급수적으로 확장할 수 있다. 이를 통해 모델은 더 넓은 영역의 컨텍스트 정보를 포착할 수 있으며, 동시에 기존의 Max Pooling이나 스트라이딩(striding)으로 인해 발생하는 특징 맵의 해상도 손실 문제를 피할 수 있다.27</li>
<li><strong>Atrous Spatial Pyramid Pooling (ASPP):</strong> “하나의 모델이 여러 개의 눈(다양한 수용 영역)을 동시에 가져야 한다“는 철학을 구현한 모듈이다. 동일한 입력 특징 맵에 대해 서로 다른 팽창률(dilation rate)을 가진 Atrous Convolution을 여러 개 병렬로 적용한 후, 그 결과들을 모두 융합한다.26 이를 통해 단일 모델 내에서 다양한 스케일의 객체와 컨텍스트 정보를 동시에 효과적으로 포착할 수 있어, 분할 성능을 크게 향상시킨다.28</li>
<li><strong>인코더-디코더 구조 (DeepLabv3+):</strong> 최신 버전인 DeepLabv3+는 DeepLabv3의 강력한 인코더(ASPP 포함)에 간단하면서도 효과적인 디코더 모듈을 결합한 구조를 채택했다.28 인코더가 추출한 풍부한 의미론적 정보와, 네트워크 초기 단계의 저수준(low-level) 특징이 가진 세밀한 공간 정보를 융합하여, 특히 객체 경계 주변의 분할 결과를 더욱 정교하게 다듬는다.28</li>
</ul>
<p>이 세 가지 대표적인 아키텍처는 각각 영상 분할의 핵심 난제에 대한 깊은 통찰과 그에 기반한 독창적인 해결책을 제시하며 기술 발전을 이끌어왔다.</p>
<h2>5.  모델 성능 평가 및 최적화</h2>
<p>영상 분할 모델의 성능을 객관적으로 평가하고, 학습 과정을 올바른 방향으로 이끌기 위해서는 적절한 평가 지표(evaluation metric)와 손실 함수(loss function)를 선택하는 것이 매우 중요하다. 어떤 지표와 손실 함수를 선택하느냐는 해당 분할 태스크에서 ’무엇을 중요하게 여기는가’를 수학적으로 정의하는 행위이며, 특히 클래스 불균형과 같은 현실적인 문제 상황에서 그 중요성이 더욱 부각된다.</p>
<h3>5.1  주요 평가 지표</h3>
<p>분할 모델의 성능을 정량적으로 측정하는 주요 지표는 다음과 같다.30</p>
<ul>
<li>
<p><strong>Pixel Accuracy (픽셀 정확도):</strong> 전체 픽셀 중에서 올바르게 분류된 픽셀의 비율을 나타내는 가장 간단한 지표이다.30</p>
<p><span class="math math-display">
\text{Pixel Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
</span></p>
</li>
</ul>
<p>여기서 TP(True Positive), TN(True Negative), FP(False Positive), FN(False Negative)은 각각 진양성, 진음성, 위양성, 위음성 픽셀의 수를 의미한다. 이 지표는 계산이 직관적이지만, 클래스 불균형(class imbalance) 문제에 매우 취약하다는 치명적인 단점이 있다.30 예를 들어, 이미지의 99%가 배경(음성 클래스)이고 1%만이 관심 객체(양성 클래스)인 의료 영상에서, 모델이 모든 픽셀을 배경으로 예측하더라도 99%의 높은 정확도를 기록하게 된다. 이는 모델이 정작 중요한 객체를 전혀 탐지하지 못했음에도 불구하고 성능이 좋은 것처럼 보이는 심각한 오해를 불러일으킬 수 있다.32</p>
<ul>
<li>
<p><strong>IoU (Intersection over Union) / Jaccard Index:</strong> 예측된 분할 영역과 실제 정답(Ground Truth) 영역 사이의 중첩도를 측정하는 지표로, Semantic Segmentation 분야에서 가장 널리 사용되는 표준 지표 중 하나이다.30 두 영역의 교집합(Intersection) 넓이를 합집합(Union) 넓이로 나눈 값으로 계산된다.</p>
<p><span class="math math-display">
\text{IoU} = \frac{\text{Area of Overlap}}{\text{Area of Union}} = \frac{TP}{TP + FP + FN}
</span></p>
</li>
</ul>
<p>IoU 값은 0과 1 사이이며, 1에 가까울수록 예측이 정답과 완벽하게 일치함을 의미한다.30 이 지표는 TN을 계산에서 제외하므로 클래스 불균형 문제에 강건하며, 객체의 경계를 얼마나 정확하게 예측했는지를 종합적으로 평가하는 데 효과적이다.</p>
<ul>
<li>
<p><strong>Dice Coefficient (다이스 계수) / F1 Score:</strong> IoU와 유사하게 예측 영역과 실제 영역의 유사도를 측정하는 지표로, 특히 의료 영상 분야에서 널리 사용된다.30 두 영역의 교집합 넓이를 두 배 한 값을 두 영역의 넓이 합으로 나눈 값으로 계산된다. 이는 정밀도(Precision)와 재현율(Recall)의 조화 평균인 F1 Score와 동일한 형태를 가진다.33</p>
<p><span class="math math-display">
\text{Dice Coefficient} = \frac{2 \times \text{Area of Overlap}}{\text{Total Area}} = \frac{2 \times TP}{2 \times TP + FP + FN}
</span></p>
</li>
</ul>
<p>Dice 계수 역시 0과 1 사이의 값을 가지며, 1에 가까울수록 성능이 우수함을 나타낸다.30 IoU와 매우 유사한 특성을 보이지만, 수학적 관계상 IoU에 비해 잘못된 예측(FP, FN)에 대해 덜 엄격한 페널티를 부과하는 경향이 있어, 작은 객체 분할 평가에 더 유리할 수 있다.32</p>
<ul>
<li>
<p><strong>Panoptic Quality (PQ):</strong> Panoptic Segmentation 성능을 평가하기 위해 특별히 고안된 지표이다.14 이는 분할 품질(Segmentation Quality, SQ)과 인식 품질(Recognition Quality, RQ)이라는 두 가지 요소의 곱으로 정의된다.</p>
<p><span class="math math-display">
PQ = \underbrace{\frac{\sum_{(p,g) \in TP} \text{IoU}(p,g)}{|TP|}}_{\text{Segmentation Quality (SQ)}} \times \underbrace{\frac{|TP|}{|TP| + \frac{1}{2}|FP| + \frac{1}{2}|FN|}}_{\text{Recognition Quality (RQ)}}
</span></p>
</li>
</ul>
<p>SQ는 올바르게 매칭된 예측 세그먼트와 정답 세그먼트 간의 평균 IoU를 측정하여 분할의 질적인 측면을 평가하고, RQ는 F1 Score와 유사하게 객체 탐지의 정확도(TP, FP, FN)를 평가한다.14 PQ는 ’Things’와 ‘Stuff’ 클래스 모두에 대한 성능을 하나의 단일 지표로 종합적으로 평가할 수 있다는 장점이 있다.</p>
<p><strong>Table 2: 주요 평가 지표 요약</strong></p>
<table><thead><tr><th>평가 지표</th><th>정의</th><th>LaTeX 수식 (TP, FP, FN, TN 기반)</th><th>주요 특징 및 활용 사례</th></tr></thead><tbody>
<tr><td><strong>Pixel Accuracy</strong></td><td>전체 픽셀 중 정확히 예측된 픽셀의 비율</td><td><code>$$\frac{TP + TN}{TP + TN + FP + FN}$$</code></td><td>계산이 간단하지만 클래스 불균형에 매우 취약함.</td></tr>
<tr><td><strong>IoU (Jaccard Index)</strong></td><td>실제와 예측의 합집합 대비 교집합 비율</td><td><code>$$\frac{TP}{TP + FP + FN}$$</code></td><td>Semantic Segmentation의 표준 지표, 경계 정확도에 민감함.</td></tr>
<tr><td><strong>Dice Coefficient (F1 Score)</strong></td><td>실제와 예측의 평균 넓이 대비 교집합의 2배 비율</td><td><code>$$\frac{2 \cdot TP}{2 \cdot TP + FP + FN}$$</code></td><td>의료 영상에서 선호, IoU보다 작은 객체에 상대적으로 관대함.</td></tr>
</tbody></table>
<h3>5.2. 핵심 손실 함수</h3>
<p>손실 함수는 모델의 예측이 실제 정답과 얼마나 다른지를 측정하여, 이 차이를 최소화하는 방향으로 모델의 가중치를 업데이트하도록 유도하는 역할을 한다.36</p>
<ul>
<li><strong>Cross-Entropy (CE) Loss:</strong> 영상 분할을 픽셀 단위 다중 클래스 분류 문제로 간주하여, 각 픽셀의 예측 확률 분포와 실제 레이블(one-hot vector) 간의 차이를 측정하는 분포 기반 손실 함수이다.37 이진 분류의 경우 Binary Cross-Entropy (BCE) Loss가 사용된다.</li>
</ul>
<p><span class="math math-display">
L_{BCE}(y, \hat{y}) = - (y \log(\hat{y}) + (1-y) \log(1-\hat{y}))
  </span></p>
<p>CE Loss는 이론적으로 안정적이고 널리 사용되지만, 각 픽셀의 오차를 동등하게 취급하기 때문에 클래스 불균형 문제에 민감하다.39 즉, 다수 클래스(예: 배경)의 픽셀들이 손실 값의 대부분을 차지하게 되어 모델이 소수 클래스(예: 작은 객체)를 제대로 학습하지 못하고 배경에 편향될 수 있다.39 이를 해결하기 위해 클래스별로 가중치를 부여하는 Weighted CE Loss나, 분류하기 어려운 예제에 더 집중하도록 하는 Focal Loss 등이 제안되었다.38</p>
<ul>
<li>
<p><strong>Dice Loss:</strong> 평가 지표인 Dice Coefficient를 직접 최적화하도록 설계된 영역 기반 손실 함수이다.37</p>
<p><code>1 - Dice Coefficient</code>로 정의되며, 예측 영역과 실제 영역 간의 겹침을 최대화하는 방향으로 학습을 직접적으로 유도한다.38</p>
</li>
</ul>
<p><span class="math math-display">
L_{Dice}(y, \hat{p}) = 1 - \frac{2y\hat{p} + \epsilon}{y + \hat{p} + \epsilon}
  </span></p>
<p>(여기서 <span class="math math-inline">\epsilon</span>은 분모가 0이 되는 것을 방지하기 위한 작은 상수이다.)</p>
<p>Dice Loss는 손실 계산 시 TN을 고려하지 않으므로, 배경이 압도적으로 많은 클래스 불균형 상황에서 CE Loss보다 훨씬 강건한 성능을 보인다.39 이 때문에 전경 객체가 이미지의 극히 일부만을 차지하는 의료 영상 분할과 같은 분야에서 특히 효과적이다.36</p>
<p>결론적으로, 평가 지표와 손실 함수의 선택은 해결하고자 하는 문제의 본질을 정확하게 반영해야 한다. ’소수의 중요한 전경 객체를 정확히 분할하는 것’이 핵심인 문제에서는, 배경 픽셀의 영향을 배제하고 영역 겹침에 집중하는 IoU, Dice Coefficient와 같은 지표와 Dice Loss와 같은 손실 함수를 사용하는 것이 훨씬 합리적인 접근 방식이다.</p>
<h2>6. 영상 분할의 주요 응용 분야</h2>
<p>영상 분할 기술은 픽셀 단위의 정밀한 분석을 통해 다양한 산업 분야에서 혁신을 주도하고 있다. 특히 의료, 자율 주행, 위성 이미지 분석 분야에서는 인간의 능력을 보조하거나 뛰어넘는 핵심 기술로 자리 잡았다. 각 응용 분야의 고유한 특성과 요구사항은 영상 분할 모델의 아키텍처와 평가 방식의 진화를 이끌어왔다.</p>
<h3>6.1. 의료 영상 분석: 정밀 진단 및 치료 계획 지원</h3>
<p>의료 영상 분야에서 영상 분할은 MRI, CT, X-ray, 초음파 등 다양한 영상으로부터 특정 해부학적 구조나 병변을 정밀하게 분리하여 정량적 분석을 가능하게 하는 필수적인 기술이다.3</p>
<ul>
<li><strong>종양 탐지 및 분석:</strong> 암 진단 과정에서 종양의 정확한 위치, 크기, 모양, 부피를 3차원적으로 측정하는 데 활용된다.5 이를 통해 암의 병기를 정확히 결정하고, 방사선 치료나 항암 치료 후 종양 크기 변화를 추적하여 치료 반응을 객관적으로 평가할 수 있다.42</li>
<li><strong>장기 분할 및 수술 계획:</strong> 수술 전, 뇌, 심장, 간, 신장 등 특정 장기의 경계를 정밀하게 분할하여 3D 모델을 생성한다.5 외과의사는 이 모델을 통해 수술 접근 경로를 계획하고, 절제 범위를 시뮬레이션하며, 수술 중 발생할 수 있는 위험을 최소화할 수 있다.4 또한, 방사선 치료 시 종양에는 최대의 방사선을 조사하면서 주변 정상 장기는 보호하기 위한 정밀한 치료 계획 수립에 결정적인 역할을 한다.44</li>
<li><strong>세포 및 조직 분석:</strong> 현미경 이미지에서 개별 세포나 특정 조직을 분할하여 정량화한다.11 이는 병리학적 진단, 신약 개발 과정에서의 세포 반응 연구, 기초 생물학 연구 등에서 중요한 데이터를 제공한다.</li>
</ul>
<p>의료 영상 분야는 매우 적은 양의 데이터로 극도로 정밀한 경계를 찾아야 하는 특성을 가진다.18 이러한 ’정밀성’에 대한 요구는 U-Net의 Skip Connection과 같은 아키텍처적 혁신을 이끌었다. 또한, 관심 병변이 전체 이미지에서 차지하는 비율이 극히 작은 극심한 클래스 불균형 문제 31를 해결하기 위해, 전경 객체에 집중하는 Dice Score와 Dice Loss가 표준적인 평가 및 최적화 방식으로 자리 잡게 되었다. 이처럼 도메인의 특수성이 기술적 해결책을 직접적으로 유도한 대표적인 사례이다.</p>
<h3>6.2. 자율 주행: 실시간 환경 인식의 핵심 기술</h3>
<p>자율주행차는 인간 운전자처럼 주변 환경을 완벽하게 이해하고 실시간으로 안전한 주행 결정을 내려야 한다. 영상 분할은 이를 위한 핵심적인 시각 인지(visual perception) 기술이다.6</p>
<ul>
<li><strong>주행 가능 영역 및 차선 인식:</strong> 도로, 인도, 연석, 갓길 등을 픽셀 수준에서 정밀하게 구분하여 차량이 안전하게 주행할 수 있는 경로(drivable area)를 실시간으로 파악한다.6 이는 객체의 대략적인 위치만 알려주는 경계 상자 방식으로는 불가능한, 매우 정밀한 정보이다.</li>
<li><strong>객체 인식 및 분할:</strong> 도로 위의 다른 차량, 보행자, 자전거, 오토바이뿐만 아니라 교통 표지판, 신호등과 같은 중요 객체들을 픽셀 단위로 정확하게 분할한다.6 이를 통해 각 객체의 정확한 형태, 크기, 이동 방향을 예측하고, 충돌 회피, 차선 변경, 속도 조절 등 복잡한 주행 전략을 수립하는 데 활용한다.</li>
<li><strong>다중 센서 융합:</strong> 자율 주행의 안전성과 강건성을 높이기 위해, RGB 카메라뿐만 아니라 깊이 정보를 제공하는 LiDAR, 악천후에도 강한 RADAR 등 다양한 센서로부터 얻은 데이터를 융합하여 분할 정확도를 높이는 연구가 활발히 진행되고 있다.50</li>
</ul>
<p>자율 주행 분야에서는 모델의 정확도만큼이나 추론 속도(inference speed)가 생명과 직결되는 중요한 요소이다.49 아무리 정확한 모델이라도 실시간으로 주변 환경 변화에 대응하지 못하면 무용지물이기 때문이다. 이러한 ’실시간성’에 대한 강력한 요구는 DeepLab 계열에서 연산 효율이 높은 depthwise separable convolution을 도입하거나 28, 전반적으로 모델을 경량화하고 하드웨어 가속에 최적화된 아키텍처를 개발하는 연구를 촉진하는 핵심 동력이 되었다.</p>
<h3>6.3. 위성 이미지 분석: 환경 모니터링 및 도시 계획</h3>
<p>인공위성과 항공기에서 촬영한 광범위한 지역의 이미지로부터 유의미한 정보를 대규모로 추출하는 데 영상 분할 기술이 핵심적으로 사용된다.51</p>
<ul>
<li><strong>토지 피복 분류 (Land Cover Classification):</strong> 위성 이미지를 숲, 수역, 도시, 농경지, 초원 등 다양한 토지 피복 유형으로 픽셀 단위로 분할하여 정밀한 지도를 제작한다.34 이는 국토 관리, 환경 정책 수립, 자원 탐사 등의 기초 자료로 활용된다.</li>
<li><strong>변화 탐지 (Change Detection):</strong> 서로 다른 시점에 촬영된 위성 이미지를 각각 분할하고 그 결과를 비교하여 시간의 흐름에 따른 지표면의 변화를 탐지한다.51 이를 통해 도시의 무분별한 확장, 대규모 삼림 벌채, 해수면 상승으로 인한 해안선 변화, 빙하 면적 감소 등 지구 환경 변화를 정량적으로 모니터링하고 분석할 수 있다. 또한, 지진이나 홍수 발생 전후의 이미지를 비교하여 피해 지역의 범위를 신속하게 파악하고 재난 대응 계획을 수립하는 데에도 활용된다.34</li>
<li><strong>객체 추출 및 관리:</strong> 이미지에서 건물, 도로망, 선박, 항공기, 태양광 패널 등 특정 인공 객체를 정밀하게 추출한다.53 이는 도시 계획, 사회 기반 시설 관리, 불법 건축물 단속, 국방 및 정보 분석 등 다양한 분야에 응용된다.</li>
</ul>
<p>위성 이미지는 분석해야 할 영역이 매우 넓고 54, 분석 대상 객체의 스케일이 개별 건물부터 거대한 숲까지 매우 다양하다는 51 특징을 가진다. 이러한 ‘대규모 및 다중 스케일’ 특성은 이미지를 작은 블록 단위로 나누어 처리하는 기법이나 55, DeepLab의 ASPP와 같이 다중 스케일 컨텍스트를 효과적으로 처리하는 아키텍처의 활용을 장려한다. 이처럼 영상 분할 기술은 진공 상태에서 발전하는 것이 아니라, 각 응용 분야가 제기하는 고유한 제약 조건과 요구사항을 해결하는 과정 속에서 진화해왔다.</p>
<h2>7. 영상 분할의 최신 연구 동향 및 미래 전망</h2>
<p>영상 분할 기술은 여전히 빠르게 발전하고 있으며, 현재 연구 커뮤니티는 기존 딥러닝 방법론의 근본적인 한계를 극복하기 위한 새로운 패러다임을 모색하고 있다. 이러한 노력은 크게 ‘효율성(데이터)’, ‘상호작용성(인간)’, ’일반화(모델)’라는 세 가지 키워드로 요약될 수 있으며, 이는 각각 데이터, 인간, 모델의 관점에서 현재 기술의 병목 현상을 해결하려는 시도이다.</p>
<h3>7.1. Unsupervised Semantic Segmentation (USS): 레이블링 비용 절감</h3>
<p>기존의 지도 학습(supervised learning) 기반 분할 모델은 픽셀 단위로 정밀하게 레이블링된 대규모 데이터셋을 필요로 한다. 그러나 이러한 데이터셋을 구축하는 데는 막대한 비용과 시간이 소요되는 근본적인 한계가 있다. Unsupervised Semantic Segmentation(비지도 의미론적 분할)은 이러한 데이터 의존성 문제를 해결하고 모델 학습의 ’효율성’을 극대화하기 위한 대안으로 부상하고 있다.56 “레이블 없이도 의미를 이해할 수 있는가?“라는 질문을 통해, 이 분야의 연구는 별도의 정답 레이블 없이도 이미지 내에서 의미론적으로 일관된 영역을 자동으로 그룹화하는 것을 목표로 한다.</p>
<p>최근에는 Vision Transformer(ViT)와 같은 자기지도학습(self-supervised learning)으로 사전 학습된 모델이 제공하는 풍부한 특징(feature) 표현을 활용하는 접근 방식이 주목받고 있다.56 CVPR 2024에서 발표된 EAGLE과 같은 연구는 단순히 패치 수준의 특징을 넘어, 객체 중심(object-centric) 표현을 학습함으로써 복잡한 구조를 가진 객체에 대한 분할 성능을 한 단계 끌어올리려는 시도를 보여준다.56</p>
<h3>7.2. Interactive Segmentation: 인간-AI 상호작용</h3>
<p>완벽하게 자동화된 모델을 만드는 것이 현실적으로 어렵다면, 인간의 직관과 지식을 최소한의 노력으로 모델에 주입하여 협력하는 것이 효과적인 대안이 될 수 있다. Interactive Segmentation(상호작용형 분할)은 이러한 ’상호작용성’에 초점을 맞춘다.57 “인간과 모델이 어떻게 가장 효율적으로 협력할 수 있는가?“라는 질문에 답하며, 사용자가 클릭(click), 경계 상자(box), 스크리블(scribble) 등 간단한 상호작용(prompt)을 제공하면 모델이 실시간으로 사용자가 원하는 객체를 정밀하게 분할해주는 기술이다.</p>
<p>이 분야는 Meta AI가 발표한 **Segment Anything Model (SAM)**의 등장으로 패러다임 전환을 맞이했다.58 SAM은 1100만 개의 이미지와 10억 개 이상의 방대한 마스크 데이터로 학습된 파운데이션 모델(Foundation Model)로, 특정 객체에 대한 사전 지식 없이도 다양한 프롬프트에 대해 뛰어난 제로샷(zero-shot) 분할 성능을 보여주며 상호작용형 분할의 가능성을 폭발적으로 확장시켰다.7</p>
<p>그러나 SAM과 같은 범용(Generalist) 모델은 다양한 상황에 대응할 수 있는 유연성과 빠른 속도에 강점이 있지만, 특정 도메인에 고도로 특화된 전문가(Specialist) 모델에 비해 분할 결과의 품질이 다소 떨어지는 한계가 있다.57 CVPR 2024에서 제안된 SegNext와 같은 최신 연구들은 이 두 모델의 장점을 결합하여, 낮은 지연 시간(Low Latency), 높은 품질(High Quality), 다양한 프롬프트 지원(Diverse Prompts)을 모두 만족시키는 차세대 상호작용형 분할 모델을 개발하는 방향으로 나아가고 있다.57</p>
<h3>7.3. 아키텍처의 진화: 하이브리드와 앙상블</h3>
<p>특정 데이터셋에 과적합된 ‘전문가’ 모델을 넘어, 다양한 상황과 객체에 유연하게 대처할 수 있는 ’일반화’된 모델에 대한 요구가 커지고 있다. 이를 위해 단일 아키텍처의 한계를 극복하려는 시도들이 활발히 이루어지고 있다.</p>
<p>최근 연구 동향은 각기 다른 장점을 가진 아키텍처들을 결합하는 하이브리드(hybrid) 모델에 주목한다. 예를 들어, CNN은 이미지의 지역적인 특징(local feature)과 질감을 포착하는 데 탁월하고, Vision Transformer는 이미지 전반에 걸친 장거리 의존성, 즉 전역적인 관계(global dependency)를 파악하는 데 강점을 가진다.59 이 두 아키텍처의 모듈을 결합한 하이브리드 모델은 두 가지 장점을 모두 취하여 더 강건하고 높은 성능을 달성할 수 있다.59</p>
<p>또 다른 접근 방식은 각 아키텍처(CNN, Transformer, MLP-Mixer 등)의 고유한 구조적 무결성을 유지하며 독립적으로 학습시킨 후, 그 예측 결과를 결합하는 앙상블(ensemble) 기법이다.59 이는 각 모델이 세상을 바라보는 서로 다른 ’관점’을 활용하여 상보적인 효과를 극대화하려는 전략으로, 모델의 일반화 성능을 높이는 데 기여할 수 있다.</p>
<p>이 세 가지 최신 연구 트렌드는 서로 분리된 것이 아니라, 더 적은 비용으로(효율성), 더 유연하게(상호작용성), 더 광범위한 문제에(일반화) 적용할 수 있는 차세대 영상 분할 기술을 향한 유기적인 움직임으로 해석할 수 있다.</p>
<h2>8. 결론: 영상 분할 기술의 현재와 미래</h2>
<h3>8.1. 핵심 내용 요약 및 기술적 의의 정리</h3>
<p>본 안내서는 영상 분할 기술의 근본적인 정의에서부터 시작하여, 전통적 기법과 딥러닝 기반 방법론의 패러다임 전환을 거쳐, Semantic, Instance, Panoptic으로 세분화되는 태스크의 발전 과정을 체계적으로 분석했다. 또한, U-Net, Mask R-CNN, DeepLab과 같은 기념비적인 아키텍처들이 영상 분할의 핵심 난제를 어떻게 해결했는지 그 철학을 탐구했으며, IoU, Dice Coefficient 등의 평가 지표와 Cross-Entropy, Dice Loss 등의 손실 함수가 특정 문제 상황에서 어떻게 합리적으로 선택되는지를 살펴보았다. 마지막으로 의료, 자율 주행, 위성 이미지 분석 등 주요 응용 분야에서 영상 분할이 어떻게 실질적인 가치를 창출하는지, 그리고 비지도 학습, 상호작용형 분할, 하이브리드 아키텍처 등 최신 연구 동향이 기술의 미래를 어떻게 그려나가고 있는지를 조망했다.</p>
<p>결론적으로, 영상 분할 기술은 단순한 픽셀 분류 작업을 넘어, 컴퓨터가 시각 세계를 구조적이고 의미론적으로 이해하게 하는 핵심적인 기술로 자리매김했다. 이는 기계가 인간처럼 보고, 해석하고, 상호작용하는 고차원적인 인공지능을 구현하기 위한 필수적인 초석이라 할 수 있다.</p>
<h3>8.2. 향후 발전 방향 및 도전 과제 제시</h3>
<p>영상 분할 기술은 눈부신 발전을 이루었지만, 여전히 해결해야 할 도전 과제들이 남아 있으며, 이는 곧 미래 연구의 방향을 제시한다.</p>
<ul>
<li><strong>3D, 비디오, 다중 모달 데이터로의 확장:</strong> 현재의 연구는 주로 2D 정지 이미지에 집중되어 있다. 의료 분야의 3D 볼륨 데이터, 자율 주행의 연속적인 비디오 스트림, 그리고 RGB, 깊이, 열화상 등 여러 종류의 센서 데이터를 통합하는 다중 모달(multi-modal) 분할 기술은 앞으로 더욱 중요해질 것이다.</li>
<li><strong>실시간 처리를 위한 모델 효율성 증대:</strong> 자율 주행, 로보틱스, 증강 현실과 같이 즉각적인 반응이 요구되는 응용 분야를 위해, 높은 정확도를 유지하면서도 연산량을 줄이고 추론 속도를 극대화하는 모델 경량화 및 최적화 연구는 지속적으로 필요하다.</li>
<li><strong>신뢰성 및 설명가능성 확보:</strong> 의료나 자율 주행과 같이 인간의 생명과 안전에 직결되는 분야에서는 모델의 예측 결과를 맹목적으로 신뢰할 수 없다. 설명가능 AI(Explainable AI, XAI) 기술을 접목하여 모델이 왜 특정 영역을 그렇게 분할했는지에 대한 근거를 제시하고, 예측의 불확실성(uncertainty)을 정량화하여 모델의 신뢰도를 높이는 연구가 필수적이다.</li>
<li><strong>파운데이션 모델의 효과적인 활용:</strong> SAM과 같은 거대 파운데이션 모델의 등장은 새로운 가능성을 열었지만, 이를 특정 전문 도메인(예: 의료, 위성)의 데이터에 효과적으로 적응(fine-tuning)시키고, 그 과정에서 발생할 수 있는 편향이나 성능 저하 문제를 해결하는 방법론에 대한 깊이 있는 연구가 요구된다.</li>
<li><strong>데이터 프라이버시와 편향성 문제:</strong> 특히 의료 데이터와 같이 민감한 정보를 다룰 때, 데이터 프라이버시를 보호하면서 모델을 학습시킬 수 있는 연합 학습(Federated Learning) 등의 기술이 중요해질 것이다. 또한, 학습 데이터에 내재된 인종적, 지역적 편향이 모델의 공정성에 미치는 영향을 분석하고 이를 완화하려는 노력도 중요한 과제로 남는다.</li>
</ul>
<p>영상 분할 기술은 앞으로도 인공지능의 ’눈’으로서, 기계가 현실 세계를 더욱 깊이 이해하고 인간과 상호작용하는 방식을 혁신해 나갈 것이며, 이러한 도전 과제들을 해결하는 과정에서 기술의 지평은 더욱 넓어질 것이다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>encord.com, 8월 31, 2025에 액세스, <a href="https://encord.com/blog/image-segmentation-for-computer-vision-best-practice-guide/#:~:text=Image%20segmentation%20in%20computer%20vision%20is%20crucial%2C%20where%20the%20goal,%2C%20medical%20imaging%2C%20and%20robotics.">https://encord.com/blog/image-segmentation-for-computer-vision-best-practice-guide/#:~:text=Image%20segmentation%20in%20computer%20vision%20is%20crucial%2C%20where%20the%20goal,%2C%20medical%20imaging%2C%20and%20robotics.</a></li>
<li>Techniques and Challenges of Image Segmentation: A Review - MDPI, 8월 31, 2025에 액세스, https://www.mdpi.com/2079-9292/12/5/1199</li>
<li>What Is Image Segmentation? | IBM, 8월 31, 2025에 액세스, https://www.ibm.com/think/topics/image-segmentation</li>
<li>Image segmentation - Wikipedia, 8월 31, 2025에 액세스, https://en.wikipedia.org/wiki/Image_segmentation</li>
<li>What Is Image Segmentation? - MATLAB &amp; Simulink - MathWorks, 8월 31, 2025에 액세스, https://www.mathworks.com/discovery/image-segmentation.html</li>
<li>Image segmentation: what, why &amp; how in self-driving cars - Labellerr, 8월 31, 2025에 액세스, https://www.labellerr.com/blog/image-segmentation-in-self-driving-cars/</li>
<li>Image Segmentation in Computer Vision [Updated 2024] | Encord, 8월 31, 2025에 액세스, https://encord.com/blog/image-segmentation-for-computer-vision-best-practice-guide/</li>
<li>Image Segmentation Approaches and Techniques in Computer Vision - GeeksforGeeks, 8월 31, 2025에 액세스, https://www.geeksforgeeks.org/computer-vision/image-segmentation-approaches-and-techniques-in-computer-vision/</li>
<li>5 Types of Image Segmentation Techniques in Vision Inspection - Averroes AI, 8월 31, 2025에 액세스, https://averroes.ai/blog/types-of-image-segmentation-techniques</li>
<li>U-Net - Wikipedia, 8월 31, 2025에 액세스, https://en.wikipedia.org/wiki/U-Net</li>
<li>What are the types of image segmentation? - Milvus, 8월 31, 2025에 액세스, https://milvus.io/ai-quick-reference/what-are-the-types-of-image-segmentation</li>
<li>Understanding Panoptic Segmentation Basics - Viso Suite, 8월 31, 2025에 액세스, https://viso.ai/deep-learning/panoptic-segmentation/</li>
<li>Guide to Panoptic Segmentation - Encord, 8월 31, 2025에 액세스, https://encord.com/blog/panoptic-segmentation-guide/</li>
<li>Semantic vs. Instance vs. Panoptic Segmentation - PyImageSearch, 8월 31, 2025에 액세스, https://pyimagesearch.com/2022/06/29/semantic-vs-instance-vs-panoptic-segmentation/</li>
<li>Semantic vs Instance vs Panoptic: Which Image Segmentation Technique To Choose?, 8월 31, 2025에 액세스, https://www.labellerr.com/blog/semantic-vs-instance-vs-panoptic-which-image-segmentation-technique-to-choose/</li>
<li>What is Panoptic Segmentation? - GeeksforGeeks, 8월 31, 2025에 액세스, https://www.geeksforgeeks.org/computer-vision/what-is-panoptic-segmentation/</li>
<li>Panoptic Segmentation: Unifying Semantic and Instance Segmentation - DigitalOcean, 8월 31, 2025에 액세스, https://www.digitalocean.com/community/tutorials/panoptic-segmentation</li>
<li>U-Net: Convolutional Networks for Biomedical Image … - arXiv, 8월 31, 2025에 액세스, https://arxiv.org/abs/1505.04597</li>
<li>U-Net: A Versatile Deep Learning Architecture for Image Segmentation | by Alexquesada, 8월 31, 2025에 액세스, https://medium.com/@alexquesada22/u-net-a-versatile-deep-learning-architecture-for-image-segmentation-2a85b52d71f6</li>
<li>UNet++: A Nested U-Net Architecture for Medical Image Segmentation - PMC, 8월 31, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC7329239/</li>
<li>[1807.10165] UNet++: A Nested U-Net Architecture for Medical Image Segmentation - arXiv, 8월 31, 2025에 액세스, https://arxiv.org/abs/1807.10165</li>
<li>[1703.06870] Mask R-CNN - ar5iv - arXiv, 8월 31, 2025에 액세스, https://ar5iv.labs.arxiv.org/html/1703.06870</li>
<li>Mask R-CNN, 8월 31, 2025에 액세스, https://arxiv.org/abs/1703.06870</li>
<li>How Mask R-CNN Works? | ArcGIS API for Python - Esri Developer, 8월 31, 2025에 액세스, https://developers.arcgis.com/python/latest/guide/how-maskrcnn-works/</li>
<li>arXiv:1703.06870v3 [cs.CV] 24 Jan 2018, 8월 31, 2025에 액세스, https://arxiv.org/pdf/1703.06870</li>
<li>Google DeepLab V3 for Image Semantic Segmentation - GitHub, 8월 31, 2025에 액세스, https://github.com/leimao/DeepLab-V3</li>
<li>DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs - Johns Hopkins Computer Science, 8월 31, 2025에 액세스, https://www.cs.jhu.edu/~ayuille/JHUcourses/VisionAsBayesianInference2022/9/DeepLabJayChen.pdf</li>
<li>Encoder-Decoder with Atrous Separable Convolution for Semantic …, 8월 31, 2025에 액세스, https://arxiv.org/abs/1802.02611</li>
<li>Enhancing DeepLabV3+ to Fuse Aerial and Satellite Images for Semantic Segmentation This project has been funded by the Ministry of Europe and Foreign Affairs (MEAE), the Ministry of Higher Education, Research (MESR) and the Ministry of Higher Education, Scientific Research and Innovation (MESRSI), under the framework of the Franco-Moroccan bilateral program - arXiv, 8월 31, 2025에 액세스, https://arxiv.org/html/2503.22909v1</li>
<li>What are different evaluation metrics used to evaluate image …, 8월 31, 2025에 액세스, https://www.geeksforgeeks.org/computer-vision/what-are-different-evaluation-metrics-used-to-evaluate-image-segmentation-models/</li>
<li>Towards a guideline for evaluation metrics in medical image segmentation - PMC, 8월 31, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC9208116/</li>
<li>towards a guideline for evaluation metrics in medical image segmentation - arXiv, 8월 31, 2025에 액세스, https://arxiv.org/pdf/2202.05273</li>
<li>All the segmentation metrics! - Kaggle, 8월 31, 2025에 액세스, https://www.kaggle.com/code/yassinealouini/all-the-segmentation-metrics</li>
<li>A brief introduction to satellite image segmentation with neural networks | by Robin Cole, 8월 31, 2025에 액세스, https://medium.com/@robmarkcole/a-brief-introduction-to-satellite-image-segmentation-with-neural-networks-33ea732d5bce</li>
<li>Statistical Validation of Image Segmentation Quality Based on a Spatial Overlap Index: Scientific Reports - PMC - PubMed Central, 8월 31, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC1415224/</li>
<li>Common Loss Functions in Modern Machine Vision - UnitX, 8월 31, 2025에 액세스, https://www.unitxlabs.com/resources/loss-function-machine-vision-system-common-loss-functions/</li>
<li>Unified Focal loss: Generalising Dice and cross entropy-based losses to handle class imbalanced medical image segmentation - PMC, 8월 31, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC8785124/</li>
<li>A survey of loss functions for semantic segmentation - arXiv, 8월 31, 2025에 액세스, https://arxiv.org/pdf/2006.14822</li>
<li>Loss functions for semantic segmentation - Grzegorz Chlebus blog, 8월 31, 2025에 액세스, https://gchlebus.github.io/2018/02/18/semantic-segmentation-loss-functions.html</li>
<li>The Combined Focal Cross Entropy and Dice Loss Function for Segmentation of Protein Secondary Structures from Cryo-EM 3D Density maps - PMC, 8월 31, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC12225904/</li>
<li>Instance segmentation loss functions - SoftwareMill, 8월 31, 2025에 액세스, https://softwaremill.com/instance-segmentation-loss-functions/</li>
<li>What Is Medical Image Segmentation and How Does It Work …, 8월 31, 2025에 액세스, https://www.synopsys.com/glossary/what-is-medical-image-segmentation.html</li>
<li>Medical Image Segmentation: A Comprehensive Review of Deep Learning-Based Methods, 8월 31, 2025에 액세스, https://www.mdpi.com/2379-139X/11/5/52</li>
<li>An Extensive Overview: 3 types of image segmentation, 8월 31, 2025에 액세스, https://www.gdsonline.tech/3-types-of-image-segmentation/</li>
<li>www.synopsys.com, 8월 31, 2025에 액세스, <a href="https://www.synopsys.com/glossary/what-is-medical-image-segmentation.html#:~:text=For%20certain%20procedures%2C%20such%20as,soft%20tissues%20to%20be%20isolated.">https://www.synopsys.com/glossary/what-is-medical-image-segmentation.html#:~:text=For%20certain%20procedures%2C%20such%20as,soft%20tissues%20to%20be%20isolated.</a></li>
<li>Advances in Medical Image Segmentation: A Comprehensive Review of Traditional, Deep Learning and Hybrid Approaches - MDPI, 8월 31, 2025에 액세스, https://www.mdpi.com/2306-5354/11/10/1034</li>
<li>Semantic segmentation of autonomous driving scenes based on multi-scale adaptive attention mechanism - Frontiers, 8월 31, 2025에 액세스, https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2023.1291674/full</li>
<li>Driving Forward:Semantic Segmentation in Autonomous Vehicles | Keylabs, 8월 31, 2025에 액세스, https://keylabs.ai/blog/driving-forward-semantic-segmentation-in-autonomous-vehicles/</li>
<li>Real-Time Semantic Image Segmentation with Deep Learning for Autonomous Driving: A Survey - MDPI, 8월 31, 2025에 액세스, https://www.mdpi.com/2076-3417/11/19/8802</li>
<li>Multimodal Semantic Segmentation in Autonomous Driving: A Review of Current Approaches and Future Perspectives - MDPI, 8월 31, 2025에 액세스, https://www.mdpi.com/2227-7080/10/4/90</li>
<li>Image Segmentation In Earth Observation - August 6, 2025, 8월 31, 2025에 액세스, https://mapscaping.com/image-segmentation-in-earth-observation/</li>
<li>Applying Semantic Segmentation in Aerial Imagery Analysis - Keymakr, 8월 31, 2025에 액세스, https://keymakr.com/blog/applying-semantic-segmentation-in-aerial-imagery-analysis/</li>
<li>Simple segmentation of geospatial images - Robin’s Blog, 8월 31, 2025에 액세스, https://blog.rtwilson.com/simple-segmentation-of-geospatial-images/</li>
<li>satellite-image-deep-learning/techniques - GitHub, 8월 31, 2025에 액세스, https://github.com/satellite-image-deep-learning/techniques</li>
<li>What Is Medical Image Segmentation? - MATLAB &amp; Simulink - MathWorks, 8월 31, 2025에 액세스, https://www.mathworks.com/discovery/medical-image-segmentation.html</li>
<li>EAGLE: Eigen Aggregation Learning for Object … - CVF Open Access, 8월 31, 2025에 액세스, https://openaccess.thecvf.com/content/CVPR2024/html/Kim_EAGLE_Eigen_Aggregation_Learning_for_Object-Centric_Unsupervised_Semantic_Segmentation_CVPR_2024_paper.html</li>
<li>Rethinking Interactive Image Segmentation with … - CVF Open Access, 8월 31, 2025에 액세스, https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Rethinking_Interactive_Image_Segmentation_with_Low_Latency_High_Quality_and_CVPR_2024_paper.html</li>
<li>arXiv:2404.00741v1 [cs.CV] 31 Mar 2024, 8월 31, 2025에 액세스, <a href="https://arxiv.org/pdf/2404.00741">https://arxiv.org/pdf/2404.00741?</a></li>
<li>arXiv:2504.09076v1 [cs.CV] 12 Apr 2025, 8월 31, 2025에 액세스, https://arxiv.org/pdf/2504.09076</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>