<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Mask DINO (2022-06-06) 객체 탐지 및 세그멘테이션을 위한 통합 Transformer 기반 프레임워크</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Mask DINO (2022-06-06) 객체 탐지 및 세그멘테이션을 위한 통합 Transformer 기반 프레임워크</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">이미지 분할 (Image Segmentations)</a> / <span>Mask DINO (2022-06-06) 객체 탐지 및 세그멘테이션을 위한 통합 Transformer 기반 프레임워크</span></nav>
                </div>
            </header>
            <article>
                <h1>Mask DINO (2022-06-06) 객체 탐지 및 세그멘테이션을 위한 통합 Transformer 기반 프레임워크</h1>
<h2>1. 서론</h2>
<p>컴퓨터 비전 분야에서 객체 탐지(Object Detection)와 이미지 세그멘테이션(Image Segmentation)은 오랜 기간 독립적으로 발전해 온 두 가지 핵심 과제이다. 객체 탐지는 이미지 내에서 특정 객체의 위치를 경계 상자(bounding box)로 식별하고 분류하는 반면 1, 이미지 세그멘테이션은 픽셀 단위로 객체를 구분하고 분류하는 데 주력한다. 이러한 작업들은 그 목적과 기술적 요구사항의 차이로 인해 각기 다른 전문 아키텍처를 기반으로 성장해 왔다.2 예를 들어, 전통적인 객체 탐지 모델인 Faster R-CNN은 인스턴스 세그멘테이션을 위한 마스크 분기를 추가한 Mask R-CNN으로 확장되었고, 이는 각 작업을 위한 개별적인 최적화의 한계를 내포하였다.</p>
<p>한편, Transformer 기반의 종단간(end-to-end) 모델인 DETR(DEtection TRansformer)은 수작업으로 설계된 앵커(anchor)나 비최대 억제(Non-Maximum Suppression)와 같은 모듈을 제거하며 객체 탐지 분야에 혁신을 가져왔다.2 DETR은 객체 탐지와 팬옵틱 세그멘테이션을 모두 처리할 수 있는 가능성을 보였으나, 그 성능은 여전히 각 분야의 전문 모델에 미치지 못하는 한계가 존재했다.1 이러한 배경 속에서 Mask DINO는 객체 탐지 및 모든 유형의 세그멘테이션(인스턴스, 팬옵틱, 시맨틱) 작업을 단일 아키텍처에서 최고 수준으로 수행하는 통합 프레임워크를 개발하는 것을 목표로 등장하였다.1 Mask DINO는 기존의 DETR 계열 모델들이 지닌 한계를 극복하고, 단일 모델로 여러 비전 작업을 효과적으로 해결하는 새로운 패러다임을 제시하였다.</p>
<h2>2.  Mask DINO의 핵심 개념 및 설계 목표</h2>
<h3>2.1  정의 및 DINO의 한계 극복 전략</h3>
<p>Mask DINO는 DINO(DETR with Improved Denoising Anchor Boxes) 프레임워크를 확장하여 모든 이미지 세그멘테이션 작업을 지원하는 마스크 예측 분기를 추가한 통합 모델이다.1 DINO는 DETR의 다양한 개선 사항을 통합하여 객체 탐지 분야에서 새로운 최첨단(SOTA) 결과를 달성하였으나 2, 영역 수준의 회귀(region-level regression)에 최적화된 모델이어서 픽셀 수준의 정렬(pixel-level alignment)에는 적합하지 않은 근본적인 문제가 있었다.2 이러한 이유로, DINO의 쿼리는 풍부한 위치 정보와 높은 수준의 의미 정보(semantic)를 인코딩하도록 설계되었지만, 픽셀 수준의 저수준 피처와 상호 작용하도록 설계되지는 않았다.8</p>
<p>Mask DINO는 이 근본적인 문제를 해결하기 위해, 모든 세그멘테이션 작업을 쿼리 기반의 마스크 분류 문제로 통일하는 <code>Mask2Former</code>의 핵심 아이디어를 채택하였다.4 Mask DINO는 DINO 아키텍처의 단순한 변형을 시도하는 대신, 이미 세그멘테이션 분야에서 성공을 거둔 최신 기술을 병렬 분기(parallel branch)로 결합하는 실용적이고 효과적인 전략을 선택하였다. 이러한 접근 방식은 Mask DINO의 기술적 독창성이 부족하다는 비판의 원인이 될 수 있지만 9, 단일 프레임워크 내에서 객체 탐지와 세그멘테이션을 최고 수준으로 통합하는 강력한 확장성을 제공한다.2</p>
<h3>2.2  주요 목표</h3>
<p>Mask DINO의 설계는 다음 세 가지 주요 목표를 달성하는 데 초점을 맞추었다.</p>
<ol>
<li>
<p><strong>단일 아키텍처로 모든 주요 컴퓨터 비전 작업을 통합한다:</strong> 객체 탐지, 인스턴스 세그멘테이션, 팬옵틱 세그멘테이션, 시맨틱 세그멘테이션을 하나의 프레임워크로 처리함으로써 알고리즘 개발을 단순화하고 효율성을 높인다.3</p>
</li>
<li>
<p><strong>작업 간의 상호 협력 및 시너지를 입증한다:</strong> 객체 탐지 및 세그멘테이션 작업이 공유된 아키텍처 설계와 훈련 방법을 통해 서로에게 도움이 될 수 있음을 보인다. 특히, 탐지 작업이 배경 “stuff” 카테고리의 분할과 같은 세그멘테이션 성능에 긍정적인 영향을 미칠 수 있음을 보여준다.1</p>
</li>
<li>
<p><strong>대규모 사전 훈련의 이점을 활용한다:</strong> 대규모 객체 탐지 데이터셋(예: Objects365)에 대한 사전 훈련이 세그멘테이션 작업의 성능을 향상시키는 데 효과적임을 보인다.1</p>
</li>
</ol>
<h2>3.  Mask DINO의 아키텍처 및 핵심 방법론</h2>
<p>Mask DINO는 DINO의 기본 구조를 계승하며, 여기에 마스크 예측을 위한 분기를 병렬로 추가하였다. 이는 원본 DINO 모델의 모든 개선 사항, 즉 앵커 박스 유도 교차 주의(anchor box-guided cross attention), 쿼리 선택, 노이즈 제거 훈련 등을 자연스럽게 상속받는 구조이다.2</p>
<h3>3.1  세그멘테이션 분기 (Segmentation Branch)</h3>
<p>Mask DINO의 마스크 예측 분기는 DINO의 쿼리 임베딩을 재사용하여 동작한다.1 이 과정은 다음의 주요 단계를 거친다.</p>
<ol>
<li>
<p><strong>픽셀 임베딩 맵 구성:</strong> 백본(ResNet 또는 Swin)에서 얻은 저해상도 피처 맵(<span class="math math-inline">C_b</span>)과 Transformer 인코더에서 얻은 고해상도 피처 맵(<span class="math math-inline">C_e</span>)을 융합하여 고해상도 픽셀 임베딩 맵을 생성한다.3 이는 DINO가 부족했던 픽셀 수준 정렬을 가능하게 한다.2</p>
</li>
<li>
<p><strong>마스크 예측:</strong> 디코더의 각 콘텐츠 쿼리 임베딩(<span class="math math-inline">q_c</span>)과 생성된 픽셀 임베딩 맵을 내적하여 바이너리 마스크 세트를 예측한다.1</p>
</li>
</ol>
<p>이러한 마스크 예측 과정은 다음의 수식으로 표현된다.</p>
<p><span class="math math-display">
m = q_c \otimes \mathcal{M} (\mathcal{T} (C_b) + \mathcal{F} (C_e))
</span><br />
여기서 $ m $은 출력 마스크, $ \mathcal{M} $은 세그멘테이션 헤드, $ \mathcal{T} $는 채널 차원을 Transformer hidden 차원에 매핑하는 컨볼루션 레이어, $ \mathcal{F} $는 $ C_e $의 2배 업샘플링을 수행하는 보간 함수이다. 이 분기는 개념적으로 단순하여 DINO 프레임워크에 쉽게 구현될 수 있다.2</p>
<h3>3.2  통합 훈련 기법 (Unified Training Techniques)</h3>
<p>Mask DINO는 탐지 및 세그멘테이션 작업 간의 시너지를 극대화하기 위해 다음과 같은 훈련 기법을 도입하였다.</p>
<ul>
<li>
<p><strong>마스크에 대한 통합 Denoising 훈련:</strong> <code>DN-DETR</code>에서 도입된 노이즈 제거 훈련을 세그멘테이션 작업으로 확장한다. 모델은 노이즈가 있는 마스크에 대해 원본 마스크를 예측하는 노이즈 제거 작업으로 학습된다. 이는 박스를 노이즈가 있는 마스크로 간주하고, 이를 복원하는 방식으로 마스크 예측을 훈련시키는 것을 포함한다. 이 기법은 훈련을 가속화하고 성능을 향상시키는 효과가 있다.4</p>
</li>
<li>
<p><strong>하이브리드 이분 매칭:</strong> 객체 탐지와 세그멘테이션을 위한 두 개의 병렬 예측 헤드가 서로 일치하지 않는 박스-마스크 쌍을 예측하는 문제를 해결하기 위해, 매칭 비용에 마스크 예측 손실을 추가한다.4 이는 하나의 쿼리에 대해 보다 정확하고 일관된 매칭 결과를 유도한다. 최종 매칭 비용은 다음과 같이 정의된다.</p>
</li>
</ul>
<p><span class="math math-display">
\lambda_\textrm{cls}\mathcal{L}_\textrm{cls}+ \lambda_\textrm{box}\mathcal{L}_\textrm{box}+ \lambda_\textrm{mask}\mathcal{L}_\textrm{mask}
</span></p>
<p>여기서 $ \mathcal{L}<em>\textrm{cls} $, $ \mathcal{L}</em>\textrm{box} $, $ \mathcal{L}_\textrm{mask} $는 각각 분류 손실, 박스 손실, 마스크 손실이며, $ \lambda $는 각 손실에 대한 가중치이다.8 팬옵틱 세그멘테이션의 경우, “stuff” 카테고리에 대한 박스 손실과 매칭은 제거하여 학습 효율을 높였다.8</p>
<h2>4.  성능 평가 및 벤치마크 분석</h2>
<p>Mask DINO는 다양한 벤치마크에서 기존의 전문 모델들을 크게 능가하는 최첨단 성능을 달성하였다.1 특히, ResNet-50 백본을 사용한 동일한 설정에서, Mask DINO는 DINO 및 Mask2Former와 비교하여 탐지 및 세그멘테이션 성능 모두에서 유의미한 향상을 보였다.2</p>
<p>다음 표는 Mask DINO와 주요 경쟁 모델들의 성능을 COCO 및 ADE20K 데이터셋에서 비교 분석한 결과이다.</p>
<h4>4.0.1 Mask DINO와 주요 모델들의 벤치마크 성능 비교</h4>
<table><thead><tr><th>모델</th><th>백본</th><th>COCO 인스턴스 AP</th><th>COCO 팬옵틱 PQ</th><th>ADE20K 시맨틱 mIoU</th><th>COCO 박스 AP</th></tr></thead><tbody>
<tr><td>Mask DINO</td><td>SwinL</td><td>54.5</td><td>59.4</td><td>60.8</td><td>59.0</td></tr>
<tr><td>Mask DINO</td><td>ResNet-50</td><td>+2.6 AP over Mask2Former</td><td>+1.1 PQ over Mask2Former</td><td>+1.5 mIoU over Mask2Former</td><td>+0.8 AP over DINO</td></tr>
<tr><td>Mask2Former</td><td>ResNet-50</td><td>-</td><td>-</td><td>-</td><td>-</td></tr>
<tr><td>DINO</td><td>ResNet-50</td><td>-</td><td>-</td><td>-</td><td>-</td></tr>
</tbody></table>
<p>이러한 결과는 Mask DINO가 단일 통합 프레임워크로 객체 탐지(DINO 대비 박스 AP +0.8)와 세그멘테이션(Mask2Former 대비 인스턴스 AP +2.6) 성능을 동시에 향상시켰음을 명확히 입증한다.2 또한, 10억 개 미만의 매개변수를 가진 모델 중에서는 COCO 인스턴스 세그멘테이션(54.5 AP), 팬옵틱 세그멘테이션(59.4 PQ), ADE20K 시맨틱 세그멘테이션(60.8 mIoU)에서 현재까지 최고의 결과를 달성하였다.1 이는 탐지 및 세그멘테이션 작업 간의 효과적인 협력을 통해 모델의 궁극적인 성능 상한이 확장될 수 있음을 보여주는 강력한 증거이다.</p>
<h2>5.  Mask DINO의 주요 기여점 및 한계점</h2>
<h3>5.1  주요 기여점</h3>
<ul>
<li>
<p><strong>통합 프레임워크 구축:</strong> 객체 탐지 및 모든 세그멘테이션 작업을 단일 Transformer 기반 아키텍처로 통합함으로써 알고리즘 개발을 단순화하고, 다양한 비전 과제를 해결하는 범용 모델의 가능성을 제시하였다.1</p>
</li>
<li>
<p><strong>작업 간 상호 협력 입증:</strong> 탐지 작업과 세그멘테이션 작업이 공유된 구조와 훈련 방법을 통해 서로에게 긍정적인 영향을 미칠 수 있음을 입증하였다. 특히, 탐지 데이터에 대한 사전 훈련이 세그멘테이션 성능을, 심지어 배경 “stuff” 카테고리의 분할 정확도를 크게 향상시킬 수 있음을 보였다.1</p>
</li>
<li>
<p><strong>대규모 사전 학습의 이점 활용:</strong> 대규모 탐지 데이터셋에 대한 사전 훈련이 모든 세그멘테이션 작업의 성능을 크게 향상시킬 수 있음을 실험적으로 증명하였다.1</p>
</li>
</ul>
<h3>5.2  한계점 및 후속 연구</h3>
<p>Mask DINO는 여러 혁신을 도입했음에도 불구하고, 다음과 같은 한계점들이 후속 연구를 통해 지적되었다.</p>
<ul>
<li><strong>훈련 안정성 및 기술적 독창성:</strong> 훈련 과정에서 손실(loss)이 일시적으로 급락하는 <code>dip</code> 현상과 같은 훈련 안정성 문제가 지적되었다.9 또한</li>
</ul>
<p><code>Mask2Former</code>의 핵심 아이디어를 차용했기 때문에 기술적 참신성이 다소 부족하다는 비판이 제기되었다.9 복잡한 배경 정보로 인해 경계가 모호해지거나 작은 객체에 대한 세그멘테이션 정확도가 낮아지는 문제도 보고되었다.12</p>
<ul>
<li>
<p><strong>탐지-세그멘테이션 불균형:</strong> 후속 연구인 DI-MaskDINO는 Mask DINO의 변환기 디코더 초기 계층에서 객체 탐지 성능이 인스턴스 세그멘테이션 성능보다 뒤처지는 현상을 발견하였다.13 이러한 성능 불균형은 탐지 작업이 객체의 경계 상자를 나타내는 4차원 벡터로 희소하게(sparsely) 감독되는 반면, 세그멘테이션은 모든 픽셀에 대한 마스크로 조밀하게(densely) 감독되기 때문에 발생한다. 이 불균형이 모델의 최종 성능 상한을 제한하는 요인으로 작용함을 DI-MaskDINO 연구는 입증하였다.13</p>
</li>
<li>
<p><strong>훈련 효율성 논란:</strong> Mask DINO 논문은 모델이 “효율적이고 확장 가능하다“고 주장하였으나 1, 실제 훈련에는 막대한 컴퓨팅 자원이 소요된다. ResNet-50 백본 모델을 8개의 GPU로 훈련하는 데 GPU당 약 15G의 메모리가 필요하며 50 에포크에 3일이 소요된다.15 특히 Swin-L 백본은 GPU당 60G의 메모리를 요구한다.15</p>
</li>
</ul>
<p><code>Mask Frozen-DETR</code>과 같은 일부 후속 연구는 Mask DINO가 약 1600 GPU 시간을 소요하는 반면, 자신들의 방법은 140 GPU 시간으로 더 높은 성능을 달성한다고 주장함으로써 Mask DINO의 훈련 효율성에 근본적인 의문을 제기하였다.16</p>
<h2>6.  실제 구현 및 응용</h2>
<p>Mask DINO의 공식 구현체는 <code>Detectron2</code>를 기반으로 하며 GitHub에 공개되어 있다.2 이는 연구자들이 모델을 재현하고 활용하는 데 용이성을 제공한다.</p>
<p>이 모델은 다양한 응용 분야에 적용될 수 있다. 특히, 실내 환경에서의 <code>팬옵틱 세그멘테이션</code>과 같은 로봇 지각(robot perception) 분야에서 탁월한 성능을 보였다.17 이는 모델이 명암 대비와 같은 다양한 조건에서도 객체를 정확하게 감지하고 분류하는 강인한 능력을 지녔음을 의미한다. 이러한 기능은 자율주행, 로봇 보조, 공간 매핑(SLAM)과 같은 실제 응용 프로그램에서 중요한 역할을 할 수 있다.17</p>
<h2>7.  결론 및 향후 전망</h2>
<p>Mask DINO는 Transformer 기반 모델을 활용하여 객체 탐지 및 모든 세그멘테이션 작업을 효과적으로 통합한 기념비적인 모델이다. 이 모델은 단일 프레임워크로 여러 비전 작업을 해결하는 새로운 패러다임을 제시하며, COCO 및 ADE20K와 같은 주요 벤치마크에서 기존의 전문 모델들을 능가하는 최첨단 성능으로 그 가능성을 입증하였다.</p>
<p>그러나 Mask DINO의 성공은 동시에 후속 연구의 새로운 방향을 제시하였다. DI-MaskDINO 논문이 지적한 <code>탐지-세그멘테이션 불균형</code>과 같은 내부적 한계는 모델의 성능 상한을 제약하는 요인으로 작용한다. 또한, 개념적 단순성과는 달리 실제 훈련에 막대한 컴퓨팅 자원이 소모되는 문제도 효율성 측면에서 개선이 필요함을 보여준다. Mask DINO는 이러한 한계를 해결하기 위한 후속 연구의 출발점이 되었으며, 이를 통해 통합 프레임워크의 범용성과 실용성은 더욱 높아질 것으로 예상된다. Mask DINO는 단순한 SOTA 모델을 넘어, 컴퓨터 비전 분야의 통합 연구를 가속화하는 중요한 이정표로 평가받는다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation - IEEE Computer Society, https://www.computer.org/csdl/proceedings-article/cvpr/2023/012900d041/1POQ0uIisBG</li>
<li>Mask DINO: Towards a Unified Transformer … - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Mask_DINO_Towards_a_Unified_Transformer-Based_Framework_for_Object_Detection_CVPR_2023_paper.pdf</li>
<li>CVPR Poster Mask DINO: Towards a Unified Transformer-Based Framework for Object Detection and Segmentation, https://cvpr.thecvf.com/virtual/2023/poster/22154</li>
<li>Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation 논문 리뷰 - Ostin X - 티스토리, https://ostin.tistory.com/86</li>
<li>마스크 디노: 객체 탐지 및 세그멘테이션을 위한 통합된 트랜스포머 기반 프레임워크로의 도전, https://hyper.ai/kr/papers/2206.02777</li>
<li>Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation - AI-Powered arXiv Paper Summarization, https://www.summarizepaper.com/en/arxiv-id/2206.02777v1/</li>
<li>Serve MaskDINO - Supervisely Ecosystem, https://ecosystem.supervisely.com/apps/maskdino/serve</li>
<li>[논문리뷰] Mask DINO: Towards A Unified Transformer-based …, <a href="https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/mask-dino/">https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/mask-dino/</a></li>
<li>[Paper Review] DINO: Emerging Properties in Self-Supervised Vision Transformers (ICCV’21) - 유진’s 공부로그 - 티스토리, https://daebaq27.tistory.com/121</li>
<li>[R] Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation - Reddit, https://www.reddit.com/r/MachineLearning/comments/12p7ms3/r_mask_dino_towards_a_unified_transformerbased/?tl=ko</li>
<li>ar5iv.labs.arxiv.org, <a href="https://ar5iv.labs.arxiv.org/html/2206.02777#:~:text=Under%20the%20same%20setting%20with,%2C%20and%20ADE20K%20semantic%20segmentation">https://ar5iv.labs.arxiv.org/html/2206.02777#:~:text=Under%20the%20same%20setting%20with,%2C%20and%20ADE20K%20semantic%20segmentation).</a>.)</li>
<li>An Improved Small Target Segmentation Model Based on Mask Dino - MDPI, https://www.mdpi.com/2076-3417/15/4/1832</li>
<li>NeurIPS Poster DI-MaskDINO: A Joint Object Detection and Instance …, https://nips.cc/virtual/2024/poster/93369</li>
<li>DI-MaskDINO: A Joint Object Detection and Instance Segmentation Model - NIPS, https://proceedings.neurips.cc/paper_files/paper/2024/file/6f1346bac8b02f76a631400e2799b24b-Paper-Conference.pdf</li>
<li>IDEA-Research/MaskDINO: [CVPR 2023] Official … - GitHub, https://github.com/IDEA-Research/MaskDINO</li>
<li>MASK FROZEN-DETR: HIGH QUALITY INSTANCE SEGMENTATION WITH ONE GPU - OpenReview, https://openreview.net/pdf/18d835746d5ab69304458d0463b91f36cfd62dba.pdf</li>
<li>Panoptic Segmentation for Indoor Environments using MaskDINO: An Experiment on the Impact of Contrast - ddd-UAB, https://ddd.uab.cat/pub/elcvia/elcvia_a2025v24n1/elcvia_a2025v24n1p1.pdf</li>
<li>A Panoptic Segmentation for Indoor Environments using MaskDINO: An Experiment on the Impact of Contrast | ELCVIA Electronic Letters on Computer Vision and Image Analysis, https://elcvia.cvc.uab.cat/article/view/1861</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>