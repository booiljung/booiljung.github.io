<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Panoptic-FPN (2019-01-08)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Panoptic-FPN (2019-01-08)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">이미지 분할 (Image Segmentations)</a> / <span>Panoptic-FPN (2019-01-08)</span></nav>
                </div>
            </header>
            <article>
                <h1>Panoptic-FPN (2019-01-08)</h1>
<h2>1. 서론</h2>
<p>컴퓨터 비전 분야에서 이미지 세그멘테이션(Image Segmentation)은 장면의 모든 픽셀을 이해하려는 목표를 향해 발전해왔다. 초기 연구는 주로 두 가지 독립적인 방향으로 진행되었다. 첫 번째는 **시맨틱 세그멘테이션(Semantic Segmentation)**으로, 이미지의 모든 픽셀에 ‘도로’, ‘하늘’, ’건물’과 같은 클래스 레이블을 할당하는 과업이다.1 이는 장면의 전반적인 의미론적 구성을 파악하는 데 중점을 둔다. 두 번째는 **인스턴스 세그멘테이션(Instance Segmentation)**으로, ‘자동차_1’, ‘자동차_2’, ’사람_1’과 같이 개별 객체 인스턴스를 탐지하고 각각의 영역을 정밀하게 분할하는 과업이다.3 이 두 과업은 서로 다른 데이터셋, 평가 지표, 그리고 모델 아키텍처를 사용하여 개별적으로 연구되어 왔으며, 이는 시각적 장면에 대한 통합적이고 일관된 이해를 저해하는 파편화된 접근법이라는 한계를 지녔다.2</p>
<p>이러한 배경 속에서 Kirillov et al.은 **파놉틱 세그멘테이션(Panoptic Segmentation, PS)**이라는 새로운 패러다임을 제안하였다.2 ’모든 것을 한눈에 본다’는 의미의 ’panoptic’에서 유래한 이 과업은, 이미지의 모든 픽셀에 시맨틱 레이블과 인스턴스 ID를 동시에 할당함으로써 시맨틱 세그멘테이션과 인스턴스 세그멘테이션을 하나의 통합된 프레임워크로 결합한다.2 이로써 배경(stuff)과 객체(things)를 아우르는 포괄적이고 전체론적인 장면 이해가 가능해졌다.4</p>
<p>본 안내서에서 심층적으로 분석할 <strong>Panoptic-FPN</strong>은 바로 이 파놉틱 세그멘테이션 과업을 위해 제안된 최초의 강력한 단일 네트워크 기반 모델이다.7 이 모델의 핵심적인 기여는 복잡한 신규 아키텍처를 처음부터 설계하는 대신, 당시 인스턴스 세그멘테이션 분야에서 압도적인 성공을 거둔 Mask R-CNN 프레임워크를 최소한으로 확장하는 우아한 접근법을 채택했다는 점에 있다.8 Panoptic-FPN은 공유된 특징 피라미드 네트워크(Feature Pyramid Network, FPN) 백본 위에 인스턴스 세그멘테이션 브랜치와 시맨틱 세그멘테이션 브랜치를 병렬로 구성함으로써, 두 과업 간의 연산을 효율적으로 공유하는 아키텍처적 통합을 최초로 증명하였다.7</p>
<p>따라서 본 안내서는 Panoptic-FPN에 대한 포괄적이고 비판적인 분석을 제공하는 것을 목표로 한다. 이를 위해 파놉틱 세그멘테이션의 근본 원리를 탐구하고, Panoptic-FPN의 아키텍처와 작동 메커니즘을 상세히 해부하며, 당대의 경쟁 모델들과의 성능을 정량적으로 비교 평가할 것이다. 최종적으로는 이 모델이 남긴 기술적 유산과 명백한 한계를 조명함으로써, 컴퓨터 비전 분야에서 통합적 장면 이해 연구의 새로운 장을 연 촉매제로서의 역할을 평가하고자 한다. Panoptic-FPN의 진정한 의의는 최고 성능의 모델이 되는 것이 아니라, 이질적으로 여겨졌던 두 세그멘테이션 과업 간의 연산 공유가 가능할 뿐만 아니라 매우 효과적이라는 사실을 입증한 *개념 증명(proof of concept)*을 제시했다는 점이다. 이전의 통합 시도들이 별개의 네트워크 결과를 후처리로 병합하는 수준에 머물렀던 반면 8, Panoptic-FPN은 단일 네트워크 기반의 통합 설계라는 새로운 철학을 제시하며 연구의 방향을 근본적으로 전환시켰다.</p>
<h2>2. 파놉틱 세그멘테이션의 개념과 평가</h2>
<h3>2.1 과업의 정의: Things와 Stuff</h3>
<p>파놉틱 세그멘테이션은 공식적으로 이미지 내 모든 픽셀에 <code>(l, z)</code> 형태의 튜플을 할당하는 과업으로 정의된다. 여기서 <span class="math math-inline">l</span>은 시맨틱 레이블(예: ‘자동차’, ‘도로’)을, <span class="math math-inline">z</span>는 인스턴스 ID를 의미한다.2 이 과업의 핵심은 시각적 장면을 두 가지 기본 범주, 즉 ’Things’와 ’Stuff’로 구분하는 데 있다.</p>
<ul>
<li><strong>Things</strong>: 자동차, 사람, 나무와 같이 명확한 경계를 가지며 개수를 셀 수 있는 객체들을 의미한다.1 이들은 전통적인 인스턴스 세그멘테이션의 대상이 되며, 각 인스턴스는 고유한 ID(<span class="math math-inline">z &gt; 0</span>)를 부여받아 서로 구분된다.</li>
<li><strong>Stuff</strong>: 하늘, 도로, 잔디와 같이 비정형적이고 셀 수 없는 배경 영역을 의미한다.1 이들은 시맨틱 세그멘테이션의 대상이 되며, 개별 인스턴스의 개념이 없으므로 인스턴스 ID는 무시된다 (<span class="math math-inline">z=0</span>).</li>
</ul>
<p>이러한 ’Things’와 ’Stuff’의 이분법적 구분은 파놉틱 세그멘테이션 과업의 이중적 성격을 규정하며, 이를 해결하기 위한 모델 아키텍처가 두 가지 측면을 모두 고려해야 함을 시사한다.2</p>
<h3>2.2 평가 지표: Panoptic Quality (PQ)</h3>
<p>파놉틱 세그멘테이션 과업과 함께 제안된 Panoptic Quality (PQ)는 이 통합 과업의 성능을 단일 지표로 측정하기 위해 고안된 핵심적인 평가 척도이다.2</p>
<h4>2.2.1 PQ의 필요성</h4>
<p>기존의 평가 지표들은 파놉틱 세그멘테이션에 적용하기에 부적합했다. 인스턴스 세그멘테이션에서 널리 사용되는 mAP(mean Average Precision)는 예측 결과에 대한 신뢰도 점수(confidence score)를 기반으로 순위를 매겨 평가를 진행한다. 하지만 시맨틱 세그멘테이션의 출력물은 픽셀 단위의 클래스 예측 값으로, 이러한 신뢰도 점수를 자연스럽게 생성하지 않는다.3 따라서 두 과업을 아우르는 새로운 통합 지표의 필요성이 대두되었고, PQ가 그 해결책으로 제시되었다.</p>
<h4>2.2.2 PQ의 공식과 분해</h4>
<p>PQ는 예측된 세그먼트(<span class="math math-inline">p</span>)와 실제 정답 세그먼트(<span class="math math-inline">g</span>) 간의 매칭을 기반으로 계산된다. 먼저, Intersection over Union (IoU) 값이 특정 임계값(일반적으로 0.5)을 초과하는 예측-정답 쌍을 찾아 True Positives (TP)로 정의한다. 매칭되지 못한 예측은 False Positives (FP), 매칭되지 못한 정답은 False Negatives (FN)가 된다.3 이를 바탕으로 PQ는 다음과 같이 정의된다.<br />
<span class="math math-display">
PQ = \frac{\sum_{(p,g) \in TP} \text{IoU}(p,g)}{\vert TP \vert + \frac{1}{2}\vert FP \vert + \frac{1}{2}\vert FN \vert}
</span><br />
이 공식은 직관적인 이해를 위해 두 가지 구성 요소, 즉 분할 품질(Segmentation Quality, SQ)과 인식 품질(Recognition Quality, RQ)의 곱으로 분해될 수 있다.1<br />
<span class="math math-display">
PQ = \underbrace{\frac{\sum_{(p,g) \in TP} \text{IoU}(p,g)}{\vert TP \vert}}_{\text{Segmentation Quality (SQ)}} \times \underbrace{\frac{\vert TP \vert}{\vert TP \vert + \frac{1}{2}\vert FP \vert + \frac{1}{2}\vert FN \vert}}_{\text{Recognition Quality (RQ)}}
</span></p>
<ul>
<li><strong>분할 품질 (Segmentation Quality, SQ)</strong>: 이 지표는 올바르게 탐지된 인스턴스(TP)들에 대해서만 평균 IoU를 계산한다. 즉, 모델이 ‘찾은’ 객체들을 ‘얼마나 정확하게’ 분할했는지를 측정하며, 놓치거나 잘못 탐지한 객체는 평가에서 제외한다.1 SQ가 높다는 것은 모델이 정교한 마스크를 생성하는 능력이 뛰어남을 의미한다.</li>
<li><strong>인식 품질 (Recognition Quality, RQ)</strong>: 이 지표는 객체 탐지의 F1 점수(F1 Score)와 동일하다. 분모의 형태는 정밀도(Precision)와 재현율(Recall)의 조화 평균인 F1 점수의 공식 <span class="math math-inline">2 \cdot TP / (2 \cdot TP + FP + FN)</span>을 재구성한 것이다. 따라서 RQ는 모델이 FP와 FN을 얼마나 잘 억제하며 객체를 ‘얼마나 정확하게’ 인식했는지를 측정한다.1 RQ가 높다는 것은 모델의 객체 탐지 능력이 우수함을 의미한다.</li>
</ul>
<p>PQ를 SQ와 RQ로 분해하는 것은 단순히 해석의 용이성을 넘어 강력한 진단 도구로서의 역할을 한다. 예를 들어, SQ는 높지만 RQ가 낮은 모델은 객체의 경계는 정교하게 그리지만 정작 객체를 잘 찾아내지 못하는 문제를 가지고 있음을 시사한다. 반대로, RQ는 높지만 SQ가 낮은 모델은 객체는 잘 찾지만 분할 마스크가 부정확하다는 것을 의미한다. 이처럼 PQ의 분해는 연구자들이 모델의 특정 아키텍처적 약점을 정확히 진단하고 개선 방향을 설정하는 데 중요한 단서를 제공한다.</p>
<h2>3. 특징 피라미드 네트워크 (Feature Pyramid Network, FPN)</h2>
<p>Panoptic-FPN의 근간을 이루는 특징 피라미드 네트워크(FPN)는 다양한 크기의 객체를 효과적으로 탐지하기 위해 설계된 핵심적인 특징 추출기이다.</p>
<h3>3.1 다중 스케일 문제</h3>
<p>컴퓨터 비전 모델이 직면하는 근본적인 문제 중 하나는 이미지 내에 객체들이 매우 다양한 크기로 존재한다는 점이다.12 멀리 있는 객체는 작게 보이고, 가까이 있는 객체는 크게 보이며, 객체 자체의 크기도 다양하다. 전통적인 해결책 중 하나인 이미지 피라미드(Image Pyramids)는 입력 이미지의 크기를 여러 단계로 조절하여 각각에 대해 특징을 추출하는 방식이지만, 이는 막대한 연산 비용을 초래하여 실용성이 떨어진다.12 반면, 심층 신경망(CNN)의 마지막 레이어에서 추출된 단일 스케일 특징 맵을 사용하는 방식은 연산 효율은 높지만 공간 해상도가 낮아 작은 객체를 탐지하는 데 실패하는 경향이 있다.12</p>
<h3>3.2 FPN 아키텍처</h3>
<p>FPN은 이러한 다중 스케일 문제를 해결하기 위해 고안된 우아한 아키텍처로, 상향식 경로, 하향식 경로, 그리고 측면 연결이라는 세 가지 핵심 요소로 구성된다.12</p>
<ul>
<li><strong>상향식 경로 (Bottom-up Pathway)</strong>: 이는 ResNet과 같은 백본 CNN의 일반적인 순전파(feed-forward) 과정에 해당한다.14 네트워크의 깊이가 깊어질수록(상위 레이어로 갈수록) 특징 맵의 공간 해상도는 점차 감소하고, 대신 더 추상적이고 의미론적으로 강한(semantically strong) 특징이 추출된다.12 이 과정에서 자연스럽게 <span class="math math-inline">C_2, C_3, C_4, C_5</span>와 같은 계층적 특징 맵들이 생성된다.</li>
<li><strong>하향식 경로 (Top-down Pathway)</strong>: 이 경로는 상향식 경로의 최상위 레이어에서부터 시작하여, 의미론적으로는 풍부하지만 공간적으로는 거친 특징 맵을 점진적으로 업샘플링(upsampling)하여 고해상도 특징 맵을 재구성한다.12 업샘플링은 일반적으로 최근접 이웃 보간법(nearest-neighbor interpolation)을 사용하여 불필요한 아티팩트 생성을 최소화한다.14</li>
<li><strong>측면 연결 (Lateral Connections)</strong>: 이는 FPN의 가장 핵심적인 부분으로, 상향식 경로와 하향식 경로를 융합하는 역할을 한다.12 하향식 경로에서 업샘플링된 특징 맵은 상향식 경로의 동일한 공간 해상도를 갖는 특징 맵과 결합된다. 이때, 상향식 특징 맵은 채널 차원을 맞추기 위해 <span class="math math-inline">1 \times 1</span> 컨볼루션 연산을 거친 후, 하향식 특징 맵과 원소별 덧셈(element-wise addition)을 통해 병합된다.15 이 과정을 통해 고해상도 특징 맵에 심층 레이어의 강력한 의미 정보가 주입된다.</li>
</ul>
<p>최종적으로, 병합된 각 특징 맵에 <span class="math math-inline">3 \times 3</span> 컨볼루션 필터를 적용하여 업샘플링으로 인한 에일리어싱(aliasing) 효과를 완화하고, 최종적인 특징 피라미드 <span class="math math-inline">P_2, P_3, P_4, P_5</span>를 생성한다.14</p>
<h3>3.3 파놉틱 세그멘테이션에서의 FPN의 역할</h3>
<p>FPN은 단순히 좋은 성능을 내는 백본을 넘어, 파놉틱 세그멘테이션 과업의 이중적 본질과 철학적으로 완벽하게 부합하는 구조를 제공한다. 파놉틱 세그멘테이션은 ’Things’의 정확한 경계를 위한 <strong>공간적 정밀성</strong>과 ‘Stuff’ 영역의 분류를 위한 <strong>의미론적 추상성</strong>을 동시에 요구한다. CNN의 얕은 레이어는 공간 해상도가 높아 경계 파악에 유리하지만 의미 정보가 부족하고, 깊은 레이어는 의미 정보는 풍부하지만 해상도가 낮아 정밀한 위치 파악에 불리하다.12 FPN의 하향식 경로와 측면 연결은 바로 이 두 가지 상충되는 특성을 조화시키는 역할을 한다. 깊은 레이어의 풍부한 의미 정보를 얕은 레이어의 고해상도 특징 맵에 주입함으로써, FPN은 모든 피라미드 레벨에서 의미론적으로 강하면서도 공간적으로 정밀한 특징 맵을 생성한다.12 이는 ’Things’와 ’Stuff’를 동시에 처리해야 하는 파놉틱 세그멘테이션 모델에게 이상적인 특징 표현을 제공하며, 과업별 헤드가 적용되기도 전에 이미 핵심적인 특징 표현 문제를 해결해주는 역할을 한다.</p>
<h2>4. Panoptic-FPN: 구조와 작동 원리</h2>
<p>Panoptic-FPN은 공유 FPN 백본 위에 인스턴스 세그멘테이션과 시맨틱 세그멘테이션을 위한 두 개의 병렬 브랜치를 구축한 통합 네트워크이다.7 이 모델의 핵심은 기존의 강력한 인스턴스 세그멘테이션 모델인 Mask R-CNN을 최소한으로 수정하여 시맨틱 세그멘테이션 기능을 추가했다는 점이며, FPN을 통한 특징 공유는 높은 연산 효율성을 보장한다.8</p>
<h3>4.1 전체 아키텍처 및 브랜치별 분석</h3>
<h4>4.1.1 인스턴스 세그멘테이션 브랜치 (Instance Segmentation Branch)</h4>
<p>이 브랜치는 표준 Mask R-CNN의 헤드 부분과 거의 동일한 구조를 가진다.8</p>
<ol>
<li><strong>영역 제안 네트워크 (Region Proposal Network, RPN)</strong>: FPN이 생성한 다중 스케일 특징 맵(<span class="math math-inline">P_2</span>부터 <span class="math math-inline">P_6</span>) 위에서 슬라이딩 윈도우 방식으로 작동하여 객체가 있을 법한 영역, 즉 제안(proposal)들을 생성한다.14</li>
<li><strong>RoIAlign</strong>: 생성된 각 제안 영역에 대해, 해당 제안의 크기에 맞는 적절한 FPN 레벨에서 고정된 크기의 특징 맵을 추출한다.</li>
<li><strong>예측 헤드</strong>: 추출된 특징 맵은 후속 완전 연결 계층(fully connected layers)으로 전달되어 각 제안에 대한 클래스 분류(예: ‘자동차’, ‘사람’)와 경계 상자(bounding box) 좌표 회귀를 수행한다.</li>
<li><strong>마스크 헤드</strong>: 동시에, RoIAlign을 통해 추출된 특징 맵은 FCN(Fully Convolutional Network) 기반의 마스크 헤드로 입력되어 각 인스턴스에 대한 픽셀 단위의 이진(binary) 마스크를 예측한다.</li>
</ol>
<h4>4.1.2 시맨틱 세그멘테이션 브랜치 (Semantic Segmentation Branch)</h4>
<p>이 브랜치는 Mask R-CNN에 새롭게 추가된 부분으로, ‘Stuff’ 영역 분할을 담당한다.7</p>
<ol>
<li><strong>특징 맵 입력</strong>: FPN의 특징 맵들(<span class="math math-inline">P_2</span>부터 <span class="math math-inline">P_5</span>)을 입력으로 받는다.</li>
<li><strong>업샘플링 및 융합</strong>: 각기 다른 해상도의 특징 맵들을 모두 동일한 해상도(일반적으로 <span class="math math-inline">P_2</span>와 동일한 입력 이미지의 1/4 크기)로 업샘플링한다.</li>
<li><strong>특징 통합</strong>: 업샘플링된 모든 특징 맵들을 합산하여 단일 특징 맵으로 통합한다.</li>
<li><strong>최종 예측</strong>: 이 통합된 특징 맵을 몇 개의 컨볼루션 레이어와 최종 소프트맥스(softmax) 레이어로 구성된 예측 헤드에 통과시켜, 모든 클래스(’Things’와 ‘Stuff’ 포함)에 대한 픽셀 단위의 시맨틱 세그멘테이션 맵을 생성한다.</li>
</ol>
<h3>4.2 학습: 다중 과업 손실 함수</h3>
<p>Panoptic-FPN은 두 브랜치의 손실을 가중합하여 전체 네트워크를 학습시킨다. 총 손실 함수 <span class="math math-inline">L</span>은 다음과 같이 정의된다.8<br />
<span class="math math-display">
L = \lambda_i (L_c + L_b + L_m) + \lambda_s L_s
</span></p>
<ul>
<li><span class="math math-inline">L_c, L_b, L_m</span>: 각각 인스턴스 브랜치의 분류 손실, 경계 상자 회귀 손실, 마스크 손실을 의미하며, 이는 표준 Mask R-CNN의 손실 함수와 동일하다. 각 손실은 샘플링된 RoI의 수 등으로 정규화된다.8</li>
<li><span class="math math-inline">L_s</span>: 시맨틱 브랜치의 손실로, 일반적으로 픽셀 단위 교차 엔트로피(per-pixel cross-entropy) 손실을 사용하며, 레이블이 있는 전체 픽셀 수로 정규화된다.8</li>
<li><span class="math math-inline">\lambda_i, \lambda_s</span>: 두 과업의 손실 가중치이다. 인스턴스 손실과 시맨틱 손실은 서로 다른 척도와 정규화 방식을 가지므로, 단순 합산(<span class="math math-inline">\lambda_i=1, \lambda_s=1</span>)은 어느 한쪽 과업의 성능 저하를 야기할 수 있다. 따라서 이 가중치들은 두 과업 간의 균형을 맞추기 위해 신중하게 조정되는 하이퍼파라미터이며, 이를 통해 단일 네트워크가 두 과업 모두에서 높은 정확도를 달성할 수 있게 된다.8</li>
</ul>
<h3>4.3 추론: 결과 병합 과정</h3>
<p>Panoptic-FPN의 추론 과정에서 가장 특징적인 부분은 두 브랜치의 독립적인 예측 결과를 하나의 일관된 파놉틱 세그멘테ATION 출력으로 병합하는 후처리 단계이다. 이 과정은 학습이 불가능한(non-learnable) 경험적 규칙(heuristic)에 기반하며, 모델의 중요한 특성이자 한계점으로 작용한다.8</p>
<ol>
<li><strong>인스턴스 간 중첩 해결</strong>: 인스턴스 브랜치에서 예측된 여러 마스크가 서로 겹치는 경우, 각 인스턴스의 분류 신뢰도 점수(classification confidence score)를 기준으로 우선순위를 정한다. 더 높은 신뢰도를 가진 인스턴스가 해당 픽셀을 차지한다.8</li>
<li><strong>인스턴스와 시맨틱 간 중첩 해결</strong>: 인스턴스 마스크와 시맨틱 브랜치가 예측한 ‘Stuff’ 영역이 겹치는 경우, 항상 인스턴스 마스크가 우선권을 갖는다. 즉, ’Things’가 ’Stuff’보다 우선시된다.8</li>
<li><strong>정리 단계</strong>: 최종 출력의 정합성을 높이기 위해, 특정 면적 임계값보다 작은 크기의 자잘한 ‘Stuff’ 세그먼트들은 제거된다.8</li>
</ol>
<p>이러한 경험적 병합 과정은 Panoptic-FPN의 ’아킬레스건’으로 평가될 수 있다. 이 방식은 간단하고 직관적이지만, 미분 불가능한 규칙에 의존하기 때문에 진정한 의미의 종단간(end-to-end) 최적화를 방해한다. 즉, 네트워크는 개별 과업의 손실(<span class="math math-inline">L_i, L_s</span>)을 통해 학습되지만, 최종 평가는 병합된 파놉틱 출력(PQ)으로 이루어지므로 학습 목표와 평가 목표 사이에 불일치가 발생한다. 이러한 구조적 한계는 후속 연구들이 UPSNet의 학습 가능한 ’파놉틱 헤드’와 같이 미분 가능한 병합 모듈을 도입하여 종단간 학습을 구현하도록 직접적인 동기를 부여했다.10</p>
<h2>5. 성능 분석 및 비교</h2>
<p>Panoptic-FPN의 성능을 정량적으로 평가하고, 당대의 주요 모델들과 비교하여 그 위치를 가늠하기 위해 대표적인 벤치마크 데이터셋인 COCO와 Cityscapes에서의 결과를 분석한다.</p>
<h3>5.1 COCO test-dev 벤치마크 성능 비교</h3>
<p>COCO 데이터셋은 다양한 객체 클래스와 복잡한 ‘실세계(in-the-wild)’ 장면을 포함하고 있어, 모델의 일반화 성능을 평가하는 표준 벤치마크로 사용된다.17 아래 표는 COCO test-dev 데이터셋에서 Panoptic-FPN과 주요 경쟁 모델들의 성능을 비교한 것이다.</p>
<table><thead><tr><th>모델</th><th>PQ</th><th>PQ\textsuperscript{Th} (Things)</th><th>PQ\textsuperscript{St} (Stuff)</th><th>백본</th><th>출처</th></tr></thead><tbody>
<tr><td>Panoptic FPN</td><td>40.9</td><td>48.3</td><td>29.7</td><td>-</td><td>19</td></tr>
<tr><td>UPSNet</td><td>46.6</td><td>53.2</td><td>36.7</td><td>ResNet-101-FPN</td><td>19</td></tr>
<tr><td>SOGNet</td><td>47.8</td><td>-</td><td>-</td><td>ResNet-101-FPN</td><td>19</td></tr>
<tr><td>Panoptic-DeepLab</td><td>41.4</td><td>45.1</td><td>35.9</td><td>Xception-71</td><td>19</td></tr>
</tbody></table>
<h3>5.2 Cityscapes 벤치마크 성능 비교</h3>
<p>Cityscapes 데이터셋은 자율 주행과 같은 응용 분야에 필수적인 도시 거리 장면에 특화된 벤치마크이다.17 구조화된 환경과 특정 클래스 분포는 COCO와는 다른 종류의 도전 과제를 제시한다. 아래 표는 Cityscapes 검증(validation) 데이터셋에서의 성능을 비교한 것이다.</p>
<table><thead><tr><th>모델</th><th>PQ</th><th>AP (Things)</th><th>mIoU (Stuff)</th><th>백본 / 비고</th><th>출처</th></tr></thead><tbody>
<tr><td>Panoptic FPN</td><td>58.1</td><td>33.0</td><td>75.7</td><td>ResNet-50-FPN</td><td>21</td></tr>
<tr><td>UPSNet</td><td>60.5</td><td>-</td><td>-</td><td>ResNet-50-FPN, COCO 사전학습</td><td>10</td></tr>
<tr><td>Panoptic-DeepLab</td><td>63.0</td><td>35.3</td><td>80.5</td><td>X-71, 단일 스케일</td><td>21</td></tr>
<tr><td>Panoptic-DeepLab</td><td>67.0</td><td>42.5</td><td>83.1</td><td>X-71, 다중 스케일, 추가 데이터</td><td>21</td></tr>
</tbody></table>
<h3>5.3 분석 및 고찰</h3>
<p>위 성능 비교표는 파놉틱 세그멘테이션 분야의 초기 발전 과정을 명확하게 보여준다.</p>
<ul>
<li><strong>강력한 베이스라인으로서의 Panoptic-FPN</strong>: Panoptic-FPN은 두 벤치마크 모두에서 준수한 성능을 기록하며, 간단하면서도 효과적인 베이스라인으로서의 가치를 입증했다.7 특히 Cityscapes에서의 높은 PQ 점수는 후속 연구들이 넘어야 할 견고한 기준점을 설정했다.</li>
<li><strong>종단간 모델의 부상</strong>: UPSNet은 두 데이터셋 모두에서 Panoptic-FPN을 일관되게 능가하는 성능을 보인다. 이는 경험적 규칙에 기반한 병합 과정을 학습 가능한 통합 파놉틱 헤드로 대체한 아키텍처적 혁신에 기인한다.10 종단간 공동 최적화를 통해 객체 간의 충돌을 더 효과적으로 해결함으로써 전반적인 PQ 성능을 향상시켰다.</li>
<li><strong>상향식 접근법의 잠재력</strong>: Panoptic-DeepLab은 특히 Cityscapes에서 당시 최고 수준(state-of-the-art)의 성능을 달성하며 상향식(bottom-up) 접근법의 강력한 잠재력을 증명했다.21 제안 기반(proposal-based)의 하향식(top-down) 방식인 Panoptic-FPN과 달리, Panoptic-DeepLab의 클래스에 무관한(class-agnostic) 인스턴스 중심점 예측과 효율적인 병합 과정은 더 빠르고 간단하면서도 우수한 결과를 낳았다.21 이는 Panoptic-FPN이 기반을 둔 Mask R-CNN 패러다임의 지배력에 도전하는 중요한 전환점이 되었다.</li>
</ul>
<p>결론적으로, 성능 데이터는 하나의 명확한 서사를 제시한다. Panoptic-FPN이 무대를 마련하자, UPSNet은 하향식 접근법을 종단간 학습으로 정교화했으며, Panoptic-DeepLab은 강력한 대안 패러다임을 제시하며 경쟁을 심화시켰다. 이는 한 모델의 한계가 다음 모델의 혁신을 직접적으로 촉발하는 건강하고 역동적인 연구 생태계를 보여주는 증거이다.</p>
<h2>6. 결론: Panoptic-FPN의 의의와 한계</h2>
<p>Panoptic-FPN은 파놉틱 세그멘테이션이라는 새로운 연구 분야의 문을 연 선구적인 모델로서 컴퓨터 비전 역사에 중요한 족적을 남겼다. 이 모델의 기술적 유산은 그 자체의 성능을 넘어, 후속 연구에 미친 영향력과 제기한 문제의식에서 찾아야 한다.</p>
<h3>6.1 주요 기여와 의의</h3>
<p>Panoptic-FPN의 핵심적인 기여는 다음과 같이 요약할 수 있다.</p>
<ol>
<li><strong>최초의 통합 네트워크 베이스라인 제시</strong>: 파편화되어 있던 시맨틱 및 인스턴스 세그멘테이션을 통합하는 파놉틱 세그멘테이션 과업에 대해, 최초로 간단하면서도 강력한 단일 네트워크 기반의 베이스라인을 제시했다.7 이는 추상적인 개념이었던 통합적 장면 이해를 구체적인 모델 아키텍처로 구현한 첫 사례였다.</li>
<li><strong>특징 공유의 효율성 입증</strong>: 공유 FPN 백본을 통해 두 개의 이질적인 과업이 효과적으로 연산을 공유할 수 있음을 증명했다. 이는 연산 및 메모리 효율성을 크게 향상시키는 아키텍처 설계 원칙을 확립했으며, 이후 등장하는 대부분의 통합 모델에 영향을 미쳤다.8</li>
<li><strong>연구 커뮤니티의 진입 장벽 완화</strong>: 당시 가장 성공적이고 널리 사용되던 Mask R-CNN 프레임워크를 기반으로 구축됨으로써, 많은 연구자들이 새로운 파놉틱 세그멘테이션 과업에 쉽게 접근하고 실험할 수 있는 발판을 마련했다.</li>
</ol>
<h3>6.2 명백한 한계와 후속 연구에의 영향</h3>
<p>동시에 Panoptic-FPN은 명확한 한계를 지니고 있었으며, 바로 이 한계점들이 후속 연구의 방향성을 결정하는 중요한 이정표가 되었다.</p>
<ol>
<li><strong>경험적 규칙 기반의 병합</strong>: 모델의 가장 큰 한계는 미분 불가능한 경험적 규칙에 의존하여 두 브랜치의 결과를 병합하는 후처리 단계였다.8 이 비학습적(non-learnable) 모듈은 진정한 의미의 종단간 학습을 가로막았으며, 복잡한 객체 중첩 상황을 해결하는 데 최적의 성능을 보장하지 못했다. 이 문제는 UPSNet과 같은 모델이 학습 가능한 ’파놉틱 헤드’를 도입하는 직접적인 계기가 되었다.</li>
<li><strong>하향식 접근법의 내재적 비효율성</strong>: 제안 기반의 하향식 모델로서, Panoptic-FPN은 Mask R-CNN 계열이 가진 복잡성과 잠재적 비효율성을 그대로 계승했다. 이는 Panoptic-DeepLab과 같은 상향식 모델이 더 빠르고 단순한 구조로 더 나은 성능을 달성하며 새로운 연구 방향을 제시하는 배경이 되었다.21</li>
</ol>
<h3>6.3 최종 평가</h3>
<p>결론적으로, Panoptic-FPN은 파놉틱 세그멘테이션의 최종 해결책이 아니라, 그 서사의 중요한 첫 장을 연 모델로 기억된다. 이 모델은 공동의 연구 기반과 강력한 성능 기준을 성공적으로 확립했으며, ’결과 병합 문제’와 같은 명확한 도전 과제를 제시함으로써 UPSNet, Panoptic-DeepLab, 그리고 더 나아가 트랜스포머 기반 모델에 이르기까지 23 더 정교하고 진정한 의미의 종단간 통합 아키텍처 개발을 촉진하는 결정적인 촉매 역할을 수행했다. Panoptic-FPN의 가장 큰 유산은 분야의 발전을 가속화했다는 점이며, 그 역할은 매우 성공적이었다고 평가할 수 있다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Panoptic Segmentation: How It Works in 2025 - Label Your Data, https://labelyourdata.com/articles/data-annotation/panoptic-segmentation</li>
<li>Panoptic Segmentation - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2019/papers/Kirillov_Panoptic_Segmentation_CVPR_2019_paper.pdf</li>
<li>Panoptic quality (PQ), segmentation quality (SQ) and recognition quality (RQ), https://iq.opengenus.org/pq-sq-rq/</li>
<li>An End-To-End Network for Panoptic Segmentation - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_An_End-To-End_Network_for_Panoptic_Segmentation_CVPR_2019_paper.pdf</li>
<li>[1801.00868] Panoptic Segmentation - arXiv, https://arxiv.org/abs/1801.00868</li>
<li>Panoptic Segmentation: A Review - arXiv, https://arxiv.org/pdf/2111.10250</li>
<li>[1901.02446] Panoptic Feature Pyramid Networks - arXiv, https://arxiv.org/abs/1901.02446</li>
<li>Panoptic Feature Pyramid Networks - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2019/papers/Kirillov_Panoptic_Feature_Pyramid_Networks_CVPR_2019_paper.pdf</li>
<li>[PDF] Panoptic Feature Pyramid Networks - Semantic Scholar, https://www.semanticscholar.org/paper/Panoptic-Feature-Pyramid-Networks-Kirillov-Girshick/a84906dbd4d6640f918d0b6ed2a7313dda0d55f1</li>
<li>UPSNet: A Unified Panoptic Segmentation … - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2019/papers/Xiong_UPSNet_A_Unified_Panoptic_Segmentation_Network_CVPR_2019_paper.pdf</li>
<li>Panoptic Segmentation — The Panoptic Quality Metric. | by Daniel Mechea | Medium, https://medium.com/@danielmechea/panoptic-segmentation-the-panoptic-quality-metric-d69a6c3ace30</li>
<li>Feature Pyramid Networks: Multi-Scale Feature Fusion | ML &amp; CV Consultant - Abhik Sarkar, https://www.abhik.xyz/concepts/deep-learning/feature-pyramid-networks</li>
<li>Feature Pyramid Network (FPN) - GeeksforGeeks, https://www.geeksforgeeks.org/computer-vision/feature-pyramid-network-fpn/</li>
<li>Understanding Feature Pyramid Networks for object detection (FPN) | by Jonathan Hui, https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c</li>
<li>Feature Pyramid Network for Multi-Scale Detection - Ruman - Medium, https://rumn.medium.com/feature-pyramid-network-for-multi-scale-detection-f573a889c7b1</li>
<li>[1901.03784] UPSNet: A Unified Panoptic Segmentation Network - arXiv, https://arxiv.org/abs/1901.03784</li>
<li>High Quality Panoptic Segmentation Datasets - maadaa.ai, https://maadaa.ai/Blog/BlogDetail/High-Quality-Panoptic-Segmentation-Datasets</li>
<li>COCO - Common Objects in Context, https://cocodataset.org/</li>
<li>Papers with code · GitHub, https://paperswithcode.com/sota/panoptic-segmentation-on-coco-test-dev?p=attention-guided-unified-network-for-panoptic</li>
<li>Cascade contour-enhanced panoptic segmentation for robotic vision perception - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC11532083/</li>
<li>A Simple, Strong, and Fast Baseline for Bottom … - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_Panoptic-DeepLab_A_Simple_Strong_and_Fast_Baseline_for_Bottom-Up_Panoptic_CVPR_2020_paper.pdf</li>
<li>Panoptic-DeepLab: Fast Bottom-Up Segmentation - Emergent Mind, https://www.emergentmind.com/articles/1911.10194</li>
<li>Delving Deeper into Panoptic Segmentation with Transformers - arXiv, https://arxiv.org/pdf/2109.03814</li>
<li>Fully Convolutional Networks for Panoptic Segmentation - Jiaya Jia, https://jiaya.me/file/papers/fcn_panoptic_seg_cvpr21.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>