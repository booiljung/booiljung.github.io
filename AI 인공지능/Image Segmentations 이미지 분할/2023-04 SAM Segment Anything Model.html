<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Segment Anything Model (SAM, 프롬프트 기반 분할, 2023-04)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Segment Anything Model (SAM, 프롬프트 기반 분할, 2023-04)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">이미지 분할 (Image Segmentations)</a> / <span>Segment Anything Model (SAM, 프롬프트 기반 분할, 2023-04)</span></nav>
                </div>
            </header>
            <article>
                <h1>Segment Anything Model (SAM, 프롬프트 기반 분할, 2023-04)</h1>
<h2>1.  이미지 분할의 새로운 지평을 연 파운데이션 모델</h2>
<p>컴퓨터 비전의 핵심 과업인 이미지 분할(Image Segmentation)은 오랜 기간 특정 도메인과 데이터셋에 종속적인 한계를 보여왔다. 이는 모델의 일반화 성능을 저해하고, 새로운 과업에 적용하기 위해 막대한 양의 수동 레이블링 데이터와 재학습 비용을 요구하는 근본적인 문제점을 야기했다.1 이러한 전통적인 분할 모델의 한계를 극복하고 범용적인 시각 지능을 구현하기 위한 파운데이션 모델의 필요성이 대두되었다.</p>
<p>Meta AI가 제시한 ‘Segment Anything’ 프로젝트는 이러한 배경 속에서 등장했으며, 특정 과업을 해결하는 모델을 넘어 ’프롬프트 기반 분할(Promptable Segmentation)’이라는 새로운 과업 자체를 정의함으로써 기존의 패러다임을 근본적으로 전환했다.2 이는 사용자가 점(point), 상자(box), 마스크(mask), 텍스트(text) 등 다양한 형태의 프롬프트를 제공하면, 모델이 해당 프롬프트에 상응하는 객체의 마스크를 실시간으로 생성하는 것을 목표로 한다.5 이 접근법의 진정한 혁신은 분할의 대상을 모델이 사전 정의된 클래스 내에서 결정하는 것이 아니라, 사용자가 실시간으로 지정하도록 함으로써 상호작용성과 범용성을 극대화한 데 있다. 이는 마치 자연어 처리 분야에서 특정 과업(번역, 요약)에 특화된 모델이 GPT와 같은 범용 언어 모델로 진화한 것과 유사한 흐름을 보여주며, 분할 모델의 응용 범위를 특정 도메인에서 벗어나 범용적인 ’시각적 도구’로 확장하는 기반을 마련했다.4</p>
<p>SAM의 가장 큰 특징은 11억 개 이상의 마스크로 구성된 SA-1B 데이터셋을 통해 학습하여, 사전 학습되지 않은 새로운 객체나 이미지 분포에 대해서도 추가 학습 없이(zero-shot) 높은 분할 성능을 보인다는 점이다.2 이러한 제로샷 일반화(Zero-shot Generalization) 능력은 많은 경우 기존의 완전 지도 학습(fully supervised) 모델의 성능을 상회하기도 하여 2, 컴퓨터 비전 분야에서 파운데이션 모델의 가능성을 입증한 중요한 성과로 평가받는다.4 SAM의 성공은 단순히 뛰어난 모델 아키텍처만으로 이루어진 것이 아니다. 효율적인 모델이 있었기에 ’데이터 엔진’이라는 대규모 데이터 수집 파이프라인이 현실적으로 작동할 수 있었고, 이 데이터 엔진을 통해 수집된 방대한 데이터가 다시 모델의 강력한 제로샷 성능을 뒷받침하는, 모델과 데이터의 공진화(Co-evolution)가 성공의 핵심 동력이었다.6</p>
<p>본 안내서는 SAM의 아키텍처, 데이터 구축 철학, 학습 방법론을 심층적으로 분석하고, 객관적인 성능 평가와 산업적 활용 사례를 제시한다. 나아가 SAM의 기술적 한계와 이를 극복하기 위한 후속 연구(HQ-SAM, SAM 2)까지 포괄적으로 다룸으로써, SAM 패밀리에 대한 완전하고 깊이 있는 이해를 제공하고자 한다.11</p>
<h2>2.  SAM의 핵심 아키텍처 해부</h2>
<p>SAM은 이미지 인코더(Image Encoder), 프롬프트 인코더(Prompt Encoder), 마스크 디코더(Mask Decoder)라는 세 가지 핵심 모듈로 구성된 단순하면서도 효율적인 아키텍처를 채택했다.13 이 구조의 핵심 설계 철학은 계산량의 비대칭적 분배에 있다. 무거운 연산을 담당하는 이미지 인코더는 이미지당 한 번만 수행하여 고품질의 시각적 특징을 추출하고, 사용자와 직접 상호작용하는 프롬프트 인코더와 마스크 디코더는 극도로 가볍게 설계하여 실시간에 가까운 반응성을 구현했다. 이러한 ‘전처리-후처리’ 분리 구조는 SAM이 연구용 모델을 넘어 실제 응용 프로그램에서 사용될 수 있게 만든 결정적인 요인이다.6</p>
<h3>2.1  이미지 인코더 (Image Encoder)</h3>
<p>이미지 인코더는 MAE(Masked Autoencoder) 방식으로 사전 학습된 대규모 Vision Transformer (ViT-H)를 백본으로 사용한다.10 입력 이미지를 받아 <span class="math math-inline">1024x1024</span> 해상도로 처리한 후, 최종적으로 <span class="math math-inline">64x64</span> 크기의 256채널 이미지 임베딩을 생성한다.10 이 과정은 이미지당 단 한 번만 수행되므로, 사용자가 프롬프트를 변경할 때마다 반복적인 연산을 수행할 필요가 없어 전체 시스템의 효율성을 극대화한다.6 ViT-H 기반 인코더는 약 6억 3200만 개의 파라미터를 가지며, 이는 SAM 전체 파라미터의 대부분을 차지한다. 이 거대한 인코더는 모델이 이미지로부터 풍부하고 일반화된 시각적 표현을 학습하는 능력의 원천이 된다.6</p>
<h3>2.2  프롬프트 인코더 (Prompt Encoder)</h3>
<p>프롬프트 인코더는 점, 상자, 마스크, 텍스트 등 다양한 형태의 사용자 프롬프트를 256차원의 벡터 임베딩으로 변환하여 마스크 디코더가 이해할 수 있는 형태로 제공하는 역할을 한다.13 프롬프트는 희소(sparse) 프롬프트와 밀집(dense) 프롬프트로 나뉘어 처리된다.</p>
<ul>
<li><strong>희소 프롬프트 (Sparse Prompts) 인코딩</strong>:</li>
<li><strong>점(Points):</strong> 각 점의 위치 정보는 푸리에 특징(Fourier features)을 이용한 위치 인코딩(positional encoding)으로 변환된다. 여기에 해당 점이 객체의 일부임을 나타내는 전경(foreground)인지, 배경(background)인지를 구분하는 학습된 임베딩이 더해져 최종 임베딩이 생성된다.4</li>
<li><strong>상자(Boxes):</strong> 상자의 좌측 상단(top-left)과 우측 하단(bottom-right) 모서리를 각각의 점으로 간주하여 인코딩한다. 각 모서리의 위치 인코딩에 ‘top-left’ 또는 ’bottom-right’를 의미하는 학습된 임베딩을 더하여 표현한다.14</li>
<li><strong>텍스트(Text):</strong> 공개된 모델에는 포함되지 않았으나, 연구 단계에서는 CLIP의 텍스트 인코더를 사용하여 텍스트 프롬프트의 가능성을 탐색했다.6</li>
<li><strong>밀집 프롬프트 (Dense Prompts) 인코딩</strong>:</li>
<li><strong>마스크(Masks):</strong> 입력 마스크는 이미지와 공간적 차원이 동일하므로, 다운샘플링 후 컨볼루션(Convolution) 연산을 통해 임베딩을 추출한다. 이 마스크 임베딩은 이미지 임베딩과 원소별 덧셈(element-wise sum) 방식으로 결합되어 디코더에 전달된다.14</li>
</ul>
<h3>2.3  마스크 디코더 (Mask Decoder)</h3>
<p>마스크 디코더는 이미지 임베딩과 프롬프트 임베딩을 입력받아 최종 출력 마스크를 예측하는 경량 Transformer 디코더이다.6 단 2개의 디코더 레이어로 구성되어 있으며, 총 파라미터 수가 400만 개에 불과할 정도로 매우 효율적으로 설계되었다. 덕분에 웹 브라우저 환경의 CPU에서도 프롬프트당 약 50ms라는 빠른 속도로 실행이 가능하다.6</p>
<p>디코더는 프롬프트 토큰과 이미지 임베딩 간의 양방향 교차 어텐션(cross-attention) 메커니즘을 통해 정보를 교환하고 업데이트한다.19 이 과정을 통해 프롬프트에 담긴 사용자 의도를 이미지 전체 컨텍스트에 전파하고, 반대로 이미지의 시각적 정보를 프롬프트 토큰에 반영하여 정교한 마스크를 생성한다.</p>
<p>특히 이 디코더는 프롬프트의 모호성을 오류가 아닌 자연스러운 특성으로 수용하도록 설계되었다. 예를 들어, 사용자가 인형의 눈을 클릭했을 때, 이는 눈, 얼굴, 또는 인형 전체를 의미할 수 있다. 기존 모델들은 이러한 모호성을 노이즈로 처리하려 했지만, SAM은 이를 해결하기 위해 3개의 유효한 마스크(예: 전체-whole, 부분-part, 하위부분-subpart)를 동시에 출력한다.4 각 마스크에는 신뢰도 점수(IoU 예측값)가 함께 제공되어, 사용자가 여러 유효한 해석 중 가장 적절한 결과를 선택할 수 있도록 한다.4 이는 모델이 하나의 ’정답’을 강요하는 대신 여러 ’가능성’을 제시하는 방식으로, 보다 유연하고 사용자 친화적인 AI 시스템의 설계 방향을 보여준다.</p>
<h2>3.  데이터 엔진: 11억 개 마스크 데이터셋 SA-1B의 구축 비결</h2>
<p>파운데이션 모델을 학습시키기 위해서는 방대하고 다양한 데이터가 필수적이지만, 기존의 분할 데이터셋은 규모와 다양성 측면에서 명백한 한계를 가지고 있었다.14 이 문제를 해결하기 위해 Meta AI는 ’데이터 엔진(Data Engine)’이라는 독자적인 데이터 수집 파이프라인을 구축했다.14</p>
<p>데이터 엔진의 핵심 철학은 모델이 데이터 수집 과정에 직접 참여하는 ‘모델-인-더-루프(Model-in-the-loop)’ 방식이다.6 이는 초기 모델을 사용하여 데이터 주석 작업을 보조하고, 이렇게 수집된 데이터로 모델을 다시 학습시켜 성능을 향상시킨 후, 개선된 모델을 다시 데이터 수집에 활용하는 선순환 구조를 의미한다.6 이러한 접근법은 AI 모델 개발의 패러다임을 ’데이터가 모델을 만든다’에서 ’모델이 데이터를 만들고, 그 데이터가 다시 모델을 성장시킨다’는 자기증식적(self-augmenting) 사이클로 전환시켰으며, 이는 SAM 프로젝트의 핵심적인 기술적 자산이라 할 수 있다.</p>
<p>데이터 구축 과정은 점진적으로 자동화 수준을 높이는 3단계로 진행되었다.</p>
<ol>
<li><strong>1단계: 모델 보조 수동 주석 (Assisted-manual Stage):</strong> 초기에는 전문 주석가들이 SAM 기반의 대화형 분할 도구를 사용하여 객체의 전경/배경 점을 클릭하는 방식으로 마스크를 생성했다. 이 단계는 전통적인 대화형 분할과 유사하며, 초기 고품질 데이터를 확보하는 데 중점을 두었다.14</li>
<li><strong>2단계: 반자동 주석 (Semi-automatic Stage):</strong> 모델이 일정 수준 이상 학습되면, 먼저 이미지 내의 잠재적 객체들을 자동으로 탐지하여 마스크를 제안했다. 주석가는 모델이 제안한 마스크를 검토 및 수정하거나, 누락된 객체에 대해 추가로 주석을 다는 방식으로 작업 효율을 극적으로 높였다.14</li>
<li><strong>3단계: 완전 자동 주석 (Fully-automatic Stage):</strong> 마지막으로, 모델 성능이 충분히 높아지면 사람의 개입 없이 모델이 스스로 이미지 내의 모든 객체에 대한 마스크를 생성했다.14 이미지에 32x32 격자점을 프롬프트로 입력하여 가능한 모든 객체를 분할하고, 이 중 신뢰도가 높은 마스크와 중복되지 않는 마스크만을 필터링하여 최종 데이터셋을 구축했다. SA-1B 데이터셋의 마스크 중 99% 이상이 이 완전 자동화 단계를 통해 생성되었다.24</li>
</ol>
<p>이렇게 구축된 SA-1B 데이터셋은 1,100만 개의 고해상도 이미지와 11억 개 이상의 고품질 마스크로 구성되어, 기존 분할 데이터셋을 규모 면에서 압도한다.2 또한, 전 세계 다양한 국가와 지역에서 수집된 이미지를 포함하여 지리적 다양성을 확보했으며, 성별, 연령, 인종 등 인구통계학적 편향성을 분석하여 모델이 특정 그룹에 치우치지 않고 공정한 성능을 보이도록 설계되었다 (RAI Analysis).14 자동 생성되었음에도 불구하고, 전문 주석가들이 검수한 결과와 비교했을 때 IoU(Intersection over Union) 90% 이상을 달성하는 마스크가 전체의 94%에 이를 정도로 높은 품질을 자랑한다.14</p>
<p>SA-1B 데이터셋의 또 다른 중요한 특징은 마스크에 ‘고양이’, ’자동차’와 같은 의미론적 레이블(semantic label)이 없다는 점이다.4 오직 ’이것이 하나의 객체다’라는 정보만 존재한다. 이러한 ‘클래스 없는(class-agnostic)’ 접근 방식은 모델이 특정 객체 클래스에 과적합되는 것을 방지하고, ’객체성(objectness)’이라는 보다 근본적이고 일반적인 시각적 개념을 학습하도록 유도했다. 이것이 바로 SAM이 사전 학습되지 않은 다양한 객체에 대해 강력한 제로샷 일반화 성능을 보이는 근본적인 이유이다. 즉, 의미론적 정보를 의도적으로 배제함으로써 오히려 시각적 일반성을 획득한 역설적인 결과를 낳은 것이다.</p>
<h2>4.  모델 학습 및 최적화 전략</h2>
<p>SAM의 학습 방식은 실제 사용자와의 상호작용을 모방하도록 설계되었다. 정적인 데이터셋을 한 번에 학습하는 것이 아니라, 사용자가 점을 추가하며 마스크를 수정해나가는 ‘과정’ 자체를 시뮬레이션한다.19 학습 과정에서는 먼저 Ground Truth 마스크에서 무작위로 점이나 상자 프롬프트를 샘플링하여 모델에 입력한다. 모델이 마스크를 예측하면, 예측 결과와 Ground Truth 간의 오류 영역(false positive 또는 false negative)에서 다음 프롬프트를 샘플링하여 다시 입력하는 과정을 반복한다. 이러한 방식은 모델이 단순히 정답 마스크를 암기하는 것을 넘어, 불완전한 예측에서 어떻게 개선해 나가야 하는지에 대한 ’전략’을 학습하게 만든다. 이는 실제 추론 환경과 학습 환경 간의 불일치를 줄여, 실제 사용 시나리오에서 모델이 더 강건하고 효과적으로 작동하도록 하는 핵심적인 역할을 한다.</p>
<h3>4.1  손실 함수 (Loss Function)</h3>
<p>SAM의 마스크 예측 손실은 Focal Loss와 Dice Loss의 선형 결합(linear combination)으로 정의된다.19<br />
<span class="math math-display">
\mathcal{L}_{\text{mask}} = \lambda_{\text{focal}}\mathcal{L}_{\text{focal}} + \lambda_{\text{dice}}\mathcal{L}_{\text{dice}}
</span><br />
여기서 <span class="math math-inline">\mathcal{L}_{\text{focal}}</span>은 픽셀 단위의 Focal Loss, <span class="math math-inline">\mathcal{L}_{\text{dice}}</span>는 영역 기반의 Dice Loss를 의미한다. SAM에서는 가중치를 <span class="math math-inline">\lambda_{\text{focal}} = 20</span>, <span class="math math-inline">\lambda_{\text{dice}} = 1</span>로 설정하여 Focal Loss에 훨씬 큰 비중을 두었다.19</p>
<ul>
<li><strong>Focal Loss</strong>: 표준적인 교차 엔트로피(Cross-Entropy) 손실의 변형으로, 분류하기 쉬운 샘플(예: 넓은 배경)의 손실 비중은 낮추고, 분류하기 어려운 샘플(예: 객체의 경계)에 더 집중하도록 설계되었다.28 이는 분할 마스크에서 흔히 발생하는 전경과 배경의 극심한 클래스 불균형 문제를 완화하는 데 매우 효과적이다.20</li>
<li><strong>Dice Loss</strong>: 예측 마스크와 실제 마스크 간의 중첩(overlap)을 직접적으로 측정하는 IoU와 유사한 지표이다.30 F1 점수를 손실 함수로 변환한 형태로, 클래스 불균형에 강건하며 분할 영역의 형태적 유사성을 최적화하는 데 유리하다.29</li>
</ul>
<p>Focal Loss와 Dice Loss를 20:1이라는 비대칭적인 비율로 결합한 것은 중요한 설계 결정이다. Dice Loss가 영역의 전반적인 중첩을 다루는 반면, Focal Loss는 개별 픽셀, 특히 경계와 같이 어려운 픽셀의 정확한 분류에 중점을 둔다. 20:1 가중치는 개발자들이 단순히 ‘대략적으로 맞는’ 영역을 찾는 것보다, ’픽셀 수준에서 정확한 경계를 찾는 것’을 훨씬 더 중요하게 여겼음을 시사한다. 이는 고품질 마스크 생성을 위한 전략적 선택이며, SAM이 종종 매우 정교한 경계를 생성하는 이유를 설명해 준다.</p>
<h3>4.2  IoU 예측 및 모호성 처리 학습</h3>
<p>마스크 디코더는 예측된 마스크의 품질을 자체적으로 평가하기 위해 IoU 예측 헤드를 별도로 가진다.4 이 헤드는 예측된 마스크와 실제 마스크 간의 IoU 값을 예측하도록 학습되며, 손실 함수로는 평균 제곱 오차(Mean Squared Error, MSE)가 사용된다. 이 손실은 마스크 손실에 더해져 전체 모델을 최적화하는 데 사용된다.19</p>
<p>또한, 단일 프롬프트에 대해 3개의 마스크가 출력되는 모호한 상황을 학습할 때, 3개의 예측 마스크 중 Ground Truth와 가장 손실이 낮은(가장 유사한) 단 하나의 마스크로부터만 역전파(backpropagation)를 수행한다.19 이는 모델이 여러 유효한 해석 중 가장 합리적인 하나에 집중하도록 유도하는 효율적인 학습 전략이다.</p>
<h2>5.  제로샷 성능 평가 및 벤치마크 분석</h2>
<p>SAM의 핵심 성능은 특정 데이터셋에 대한 미세조정(fine-tuning) 없이, 사전 학습된 모델을 그대로 사용하여 다양한 분할 벤치마크에서 성능을 측정하는 ‘제로샷 전이(Zero-shot Transfer)’ 능력으로 평가된다.2 이는 모델의 일반화 성능을 가늠하는 가장 중요한 척도이다.</p>
<p>SAM은 에지 검출, 객체 제안 생성, 인스턴스 분할 등 다양한 다운스트림 과업에서 강력한 제로샷 성능을 입증했다.7 주요 벤치마크에서의 정량적 성능은 다음과 같다.</p>
<ul>
<li><strong>LVIS v1.0 val (Zero-Shot Instance Segmentation):</strong> ViTDet-H를 프롬프트 생성기로 사용했을 때, SAM은 AP(Average Precision) 44.7을 기록하여 매우 경쟁력 있는 성능을 보였다.31</li>
<li><strong>COCO val (Zero-Shot Instance Segmentation):</strong> SAM(ViT-H)은 COCO 데이터셋에서 AP 48.5를 기록했다.32</li>
<li><strong>SA-1B 자체 평가:</strong> SA-1B 벤치마크에서 SAM은 AP 38.9를 기록했다.33</li>
</ul>
<p>많은 경우, SAM의 제로샷 성능은 해당 과업을 위해 완전히 지도 학습된(fully supervised) 기존의 SOTA(State-of-the-Art) 모델들과 경쟁력이 있거나 심지어 능가하는 결과를 보였다.2 이는 “제로샷 성능이 좋다“는 주장이 단순한 가능성이 아니라, 정량적으로 측정되고 검증된 성능임을 명확히 보여준다. 특히, 특정 데이터셋을 위해 처음부터 학습된 전문 모델들과 대등하거나 우월한 성능을 보인다는 사실은, 대규모의 범용 데이터로 학습된 단일 모델이 수많은 개별 전문 모델을 대체할 수 있다는 파운데이션 모델의 핵심 가설을 강력하게 뒷받침한다.</p>
<p>정성적 평가에서도 SAM의 우수성이 드러났다. COCO 및 LVIS 데이터셋에 대한 분석 결과, SAM이 생성한 마스크는 기존 SOTA 모델인 ViTDet보다 경계가 더 명확하고 세밀하여 질적으로 우수한 경우가 많음이 관찰되었다.14</p>
<p>한편, SAM(ViT-H)은 높은 성능을 보이지만 막대한 계산 자원을 요구한다.9 이에 대한 대응으로 MobileSAM, FastSAM, EfficientSAM과 같은 경량화 모델들이 등장했다.8 이 모델들은 성능을 일부 희생하는 대신 추론 속도와 모델 크기를 극적으로 줄여 실용성을 높였다. 특히 EfficientSAM과 같은 모델은 다른 경량화 모델보다 훨씬 적은 성능 저하로 높은 효율을 달성했는데 35, 이는 단순히 모델을 압축하는 것을 넘어, SAM의 핵심 기능을 유지하며 효율화하는 새로운 학습 전략이 가능함을 시사한다. 이는 SAM 생태계가 고성능 연구용 모델과 실용적인 배포용 모델로 분화하며 발전할 수 있음을 보여준다.</p>
<h3>5.1 표 1: 주요 벤치마크에서의 제로샷 인스턴스 분할 성능 비교</h3>
<table><thead><tr><th>Model</th><th>Backbone</th><th>Dataset</th><th>AP (Average Precision)</th><th>AP_small</th><th>AP_medium</th><th>AP_large</th><th>Source</th></tr></thead><tbody>
<tr><td>SAM (ViTDet prompt)</td><td>ViT-H</td><td>LVIS v1.0 val</td><td>44.7</td><td>-</td><td>-</td><td>-</td><td>31</td></tr>
<tr><td>SAM (baseline)</td><td>ViT-H</td><td>COCO val</td><td>48.5</td><td>-</td><td>-</td><td>-</td><td>32</td></tr>
<tr><td>HQ-SAM</td><td>ViT-H</td><td>COCO val</td><td>49.5</td><td>-</td><td>-</td><td>-</td><td>32</td></tr>
<tr><td>EfficientSAM-S</td><td>-</td><td>COCO val</td><td>44.4</td><td>-</td><td>-</td><td>-</td><td>35</td></tr>
<tr><td>SAM</td><td>ViT-H</td><td>SA-1B</td><td>38.9</td><td>20.0</td><td>59.9</td><td>82.8</td><td>33</td></tr>
</tbody></table>
<h2>6.  산업 응용 분야 및 활용 사례</h2>
<p>SAM의 프롬프트 기반 설계는 다른 시스템과의 유연한 통합을 가능하게 하여 다양한 산업 분야에 적용될 잠재력을 가진다.6 예를 들어, 객체 탐지 모델이 생성한 경계 상자를 SAM의 프롬프트로 사용하여 즉시 정교한 인스턴스 분할을 수행할 수 있다.8 그러나 산업 현장에서 SAM 단독으로 모든 문제를 해결하기는 어렵다. SAM은 객체를 분할할 뿐, 그것이 ’자동차’인지 ’사람’인지 알려주지 않기 때문이다.37 따라서 실제 응용에서는 객체 탐지 모델(예: Grounding DINO)과 SAM을 결합하여 ‘탐지 후 분할’ 파이프라인을 구성하는 방식이 일반적이다.32 이는 SAM의 역할이 엔드-투-엔드 솔루션 제공자가 아니라, 다른 AI 모델과 결합하여 전체 시스템의 성능을 한 단계 끌어올리는 강력한 ‘분할 전문 모듈’ 또는 ’구성요소’에 있음을 시사한다.</p>
<ul>
<li><strong>데이터 어노테이션 자동화:</strong> SAM의 가장 직접적이고 강력한 활용 사례는 데이터 레이블링 자동화이다. 기존의 수동 폴리곤 생성 방식과 비교하여, SAM을 활용하면 클릭 몇 번으로 고품질의 분할 마스크를 생성할 수 있어 어노테이션 속도와 효율을 극적으로 향상시킬 수 있다.7</li>
<li><strong>콘텐츠 제작 및 이미지 편집:</strong> 사용자가 클릭 한 번으로 이미지 내 특정 객체를 정교하게 분리하거나 배경을 제거하는 데 활용될 수 있다.6 또한, 분할된 영역에 선택적으로 스타일을 적용하는 ’분할 스타일 전이(Segmented Style Transfer)’와 같은 창의적인 도구 개발에 기여하며 38, Stable Diffusion과 같은 생성 모델과 결합하여 특정 객체만 수정하는 인페인팅(Inpainting) 작업의 정밀도를 높일 수 있다.37</li>
<li><strong>의료 영상 분석:</strong> CT, MRI 등 의료 영상에서 장기, 종양, 혈관 등을 분할하는 데 사용될 수 있다.11 일반 이미지(natural images)와 분포가 매우 다른 의료 영상과 같은 특수 도메인에서는 제로샷 성능의 한계가 명확히 나타난다.17 그러나 적은 양의 도메인 특화 데이터로 미세조정(fine-tuning)할 경우 성능이 극적으로 향상된다는 점도 보고되었다.41 이는 파운데이션 모델의 활용 전략이 ’제로샷으로 모든 것을 해결’하는 접근법과 ’처음부터 새로 학습’하는 전통적인 접근법 사이의 균형점에 있음을 보여준다. 즉, ‘파운데이션 모델을 강력한 초기 가중치로 삼아, 최소한의 데이터와 비용으로 특정 도메인에 신속하게 적응시키는(domain adaptation)’ 전략이 가장 현실적이고 효과적인 활용 방안이 될 것이다.</li>
<li><strong>자율 주행 및 지능형 교통 시스템:</strong> 차량, 보행자, 차선, 교통 표지판 등 도로 위의 다양한 객체를 실시간으로 분할하여 주변 환경을 정밀하게 인식하는 데 활용될 수 있다.11 이는 자율주행차의 의사결정 정확도와 안전성을 높이는 데 기여한다.</li>
<li><strong>산업 자동화 및 결함 검출:</strong> 제조 공정에서 제품 표면의 스크래치, 균열 등 미세한 결함을 탐지하고 분할하는 데 적용될 수 있다.17 다만, 저대비 환경이나 매우 작고 불규칙한 형태의 결함에 대해서는 제로샷 성능의 한계가 보고되고 있다.17</li>
<li><strong>증강 현실 (AR) / 가상 현실 (VR):</strong> 사용자의 시선(gaze)을 프롬프트로 사용하여 현실 세계의 객체를 선택하고 상호작용하는 등, 보다 직관적이고 몰입감 있는 AR/VR 경험을 구현하는 데 활용될 수 있다.6</li>
</ul>
<h2>7.  기술적 한계와 극복을 위한 노력</h2>
<p>SAM은 혁신적인 성과에도 불구하고 몇 가지 내재적인 한계를 가지고 있다. 이러한 한계를 극복하기 위한 후속 연구들이 활발히 진행되고 있다.</p>
<h3>7.1  원본 SAM의 내재적 한계</h3>
<ul>
<li><strong>거친 마스크 경계 및 미세 구조 분할 실패:</strong> SAM은 11억 개의 자동으로 생성된 마스크로 학습되었기 때문에, 마스크의 품질이 완벽하지 않다. 이로 인해 종종 객체의 경계가 거칠게 예측되거나, 연기, 머리카락, 동물의 털, 그물 등 가늘고 복잡한 구조(thin object structures)를 제대로 분할하지 못하거나 무시하는 경향이 있다.9</li>
<li><strong>의미론적 정보 부재:</strong> SAM은 ’객체성’에 기반하여 분할을 수행하므로, 분할된 마스크가 어떤 객체인지에 대한 의미론적 정보(semantic information)를 이해하지 못한다.47 이는 클래스별 분할이 필요한 작업에 직접 적용하기 어렵게 만든다.</li>
<li><strong>성능과 효율성의 상충:</strong> ViT-H 기반의 SAM은 높은 성능을 제공하지만, 막대한 계산량과 메모리를 요구하여 실시간 응용이나 모바일 기기 배포에 어려움이 있다.9</li>
</ul>
<h3>7.2  한계 극복을 위한 연구: HQ-SAM</h3>
<p>HQ-SAM은 SAM의 장점인 프롬프트 기반 설계, 효율성, 제로샷 일반화 능력을 그대로 유지하면서, 고품질의 정교한 마스크를 생성하는 것을 목표로 개발되었다.9 이는 거대한 파운데이션 모델을 처음부터 재학습하는 대신, 최소한의 ‘패치’ 또는 ‘애드온(add-on)’ 모듈을 추가하여 특정 약점을 보완하는 효율적인 업그레이드 전략을 제시한다.</p>
<ul>
<li><strong>핵심 아이디어</strong>:</li>
</ul>
<ol>
<li><strong>고품질 출력 토큰 (HQ-Output Token):</strong> 기존 SAM의 마스크 디코더에 새로운 학습 가능한 ’HQ-Output Token’을 주입한다. 이 토큰은 기존 출력 토큰과 별개로, 오직 고품질 마스크 예측에만 특화되도록 학습된다.32</li>
<li><strong>전역-지역 특징 융합 (Global-local Feature Fusion):</strong> 마스크 디코더의 특징뿐만 아니라, 이미지 인코더(ViT)의 초기 및 최종 레이어 특징을 융합하여 마스크의 세부적인 경계 정보를 보강한다.</li>
</ol>
<ul>
<li><strong>학습 및 성능:</strong> SAM의 파라미터 대부분을 동결하고 추가된 일부 모듈(전체 파라미터의 0.5% 미만)만 학습시켜 효율성을 높였다. 44,000개의 고품질 마스크로 구성된 HQSeg-44K 데이터셋에서 단 4시간 만에 학습이 가능하며 9, 결과적으로 COCO 벤치마크에서 SAM의 AP를 48.5에서 49.5로 향상시키는 등 원본 SAM보다 일관되게 우수한 마스크 품질을 보여준다.32</li>
</ul>
<p>HQ-SAM의 접근 방식은 파운데이션 모델의 발전 방향에 중요한 시사점을 던진다. SAM이 11억 개의 ’적당한 품질’의 마스크로 ’일반성’을 학습했다면, HQ-SAM은 4만 4천 개의 ’매우 높은 품질’의 마스크로 ’정교함’을 학습했다.9 이는 파운데이션 모델의 발전이 초기에는 데이터의 ’양’을 통해 범용성을 확보하고, 그 이후에는 특정 목적을 위해 선별된 고품질 데이터로 모델을 미세 조정하거나 확장하는, 즉 ’양’에서 ’질’로 초점이 이동할 수 있음을 보여준다.</p>
<h2>8.  차세대 모델 SAM 2: 비디오 분할로의 확장</h2>
<p>원본 SAM은 정적 2D 이미지를 위해 설계되어, 비디오와 같은 시간적 차원을 가진 데이터를 직접 처리하는 데 한계가 있었다.11 SAM 2는 이러한 한계를 극복하고 이미지와 비디오 분할을 단일 통합 모델로 해결하는 것을 목표로 개발되었다.49</p>
<h3>8.1  통합 아키텍처와 작동 방식</h3>
<p>SAM 2는 이미지와 비디오를 별개의 모델로 처리하는 대신, 이미지를 ’단일 프레임 비디오’로 간주하여 동일한 아키텍처 내에서 처리한다.5 사용자가 비디오의 특정 프레임에 프롬프트를 제공하면, SAM 2는 해당 프레임의 마스크를 즉시 생성하고, 이 정보를 비디오 전체로 전파(propagate)하여 모든 프레임에 대한 분할 마스크(masklet)를 생성한다.50</p>
<p>원본 SAM에서의 상호작용이 단일 이미지 내에서 마스크를 ’수정’하는 정적인 개념이었다면, SAM 2에서의 상호작용은 비디오 타임라인의 어느 지점에서든 프롬프트를 추가하면 그 정보가 전체 비디오로 전파되어 마스크 트랙 전체를 ’개선’하는 동적인 개념으로 진화했다.50 이는 사용자의 개입이 시공간 전체에 영향을 미치는 보다 강력하고 효율적인 상호작용 모델을 구현한 것으로, 비디오 편집이나 어노테이션과 같은 실제 작업의 효율성을 극적으로 높일 수 있는 잠재력을 가진다.</p>
<h3>8.2  핵심 기술 혁신: 스트리밍 메모리 메커니즘</h3>
<p>SAM 2 아키텍처의 핵심은 시간적 일관성을 유지하기 위한 ’스트리밍 메모리 메커니즘(Streaming Memory Mechanism)’이다.11 이는 메모리 인코더, 메모리 뱅크, 메모리 어텐션 모듈로 구성된다.53 모델이 각 프레임을 순차적으로 처리하면서, 타겟 객체의 시각적 특징과 이전 프레임의 마스크 정보를 메모리 뱅크에 저장한다.52 다음 프레임을 처리할 때, 메모리 어텐션 모듈이 현재 프레임의 특징과 메모리 뱅크에 저장된 과거 정보를 비교하고 정렬하여, 객체가 가려지거나(occlusion) 다시 나타나도 일관된 추적과 분할을 유지하도록 돕는다.51 이는 Transformer 아키텍처의 어텐션 메커니즘이 본질적으로 ’쿼리(현재 프레임)’와 ‘키/밸류(과거 프레임 메모리)’ 간의 관계를 모델링하는 데 매우 적합하기에 가능한 접근법이다. 즉, SAM 2는 Transformer의 본질적 특성을 활용하여 공간적 분할 문제를 시공간적 일관성 추적 문제로 자연스럽게 확장하는 우아한 해결책을 제시했다.</p>
<h3>8.3  SAM 대비 성능 향상</h3>
<p>SAM 2는 원본 SAM 대비 여러 측면에서 상당한 성능 향상을 이루었다.</p>
<ul>
<li><strong>정확도:</strong> 이미지 분할에서는 원본 SAM보다 더 높은 정확도를 달성했으며, 비디오 분할에서는 기존 SOTA 모델들을 능가하는 성능을 보였다.49</li>
<li><strong>속도 및 효율성:</strong> 이미지 분할 추론 속도는 SAM보다 6배 빨라졌으며, 비디오 분할 작업에 필요한 사용자 상호작용(클릭 횟수 등)은 기존 방식 대비 3배 감소했다.49 실시간 처리가 가능한 약 44 FPS의 속도를 보여준다.51</li>
<li><strong>데이터:</strong> SAM 2 학습을 위해 새로운 모델-인-더-루프 데이터 엔진을 구축하여, 현재까지 가장 큰 비디오 분할 데이터셋인 SA-V를 수집하고 공개했다.5</li>
</ul>
<h2>9.  결론: SAM이 제시하는 비전 AI의 미래</h2>
<p>SAM, HQ-SAM, SAM 2로 이어지는 SAM 패밀리는 컴퓨터 비전 분야에 세 가지 중요한 기여를 했다. 첫째, ’프롬프트 기반 분할’이라는 새로운 패러다임을 제시하여 모델의 범용성과 상호작용성을 극대화했다. 둘째, ‘모델-인-더-루프’ 데이터 엔진을 통해 대규모 고품질 데이터셋 구축의 새로운 방법론을 정립했다. 셋째, 이미지 분할을 넘어 비디오 분할까지 아우르는 통합 파운데이션 모델의 가능성을 입증했다.1</p>
<p>SAM의 등장은 향후 비전 AI 연구에 여러 방향성을 제시한다. 첫째, 의료, 산업 등 특수 도메인에서의 성능을 높이기 위한 효율적인 도메인 적응 기법 연구가 더욱 중요해질 것이다.1 둘째, 현재의 시각적 프롬프트를 넘어, 자연어 설명을 더 정교하게 이해하고 분할에 반영하는 다중 모달(multi-modal) 파운데이션 모델로의 발전이 기대된다. 마지막으로, 실시간 응용 및 온디바이스(on-device) AI를 위해, 성능 저하를 최소화하면서 모델을 경량화하는 연구가 지속적으로 요구될 것이다.1</p>
<p>결론적으로, SAM은 AI 모델이 특정 작업을 수동적으로 수행하는 ’도구’에서 벗어나, 인간의 의도를 이해하고 시각 세계와 능동적으로 상호작용하는 ’파트너’가 될 수 있는 가능성을 보여주었다. SAM의 발전은 향후 더 일반적이고, 지능적이며, 상호작용적인 비전 AI 시스템의 초석이 될 것이다.</p>
<h2>10. 참고 자료</h2>
<ol>
<li>(PDF) Segment Anything: A Review - ResearchGate, https://www.researchgate.net/publication/385734885_Segment_Anything_A_Review</li>
<li>Segment Anything | Research - AI at Meta, https://ai.meta.com/research/publications/segment-anything/</li>
<li>[2304.02643] Segment Anything - arXiv, https://arxiv.org/abs/2304.02643</li>
<li>Segment Anything (SAM) — Object Recognition Lecture - HdM Stuttgart, https://maucher.pages.mi.hdm-stuttgart.de/orbook/deeplearning/SAM.html</li>
<li>facebookresearch/segment-anything: The repository provides code for running inference with the SegmentAnything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model. - GitHub, https://github.com/facebookresearch/segment-anything</li>
<li>Segment Anything | Meta AI, https://segment-anything.com/</li>
<li>Segment Anything Model (SAM) - Ultralytics YOLO Docs, https://docs.ultralytics.com/ko/models/sam/</li>
<li>Segment Anything Model (SAM) - Ultralytics YOLO Docs, https://docs.ultralytics.com/models/sam/</li>
<li>Segment Anything in High Quality, https://proceedings.neurips.cc/paper_files/paper/2023/file/5f828e38160f31935cfe9f67503ad17c-Paper-Conference.pdf</li>
<li>Performance Evaluation of Segment Anything Model with Variational Prompting for Application to Non-Visible Spectrum Imagery - arXiv, https://arxiv.org/html/2404.12285v1</li>
<li>A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering - arXiv, https://arxiv.org/html/2306.06211v4</li>
<li>[2306.06211] A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering - arXiv, https://arxiv.org/abs/2306.06211</li>
<li>Segment Anything Model (SAM) - The Complete Guide - Viso Suite, https://viso.ai/deep-learning/segment-anything-model-sam-explained/</li>
<li>[논문 리뷰] Segment Anything (SAM) - Luna AI Blog, https://lunaleee.github.io/posts/SAM/</li>
<li>[논문리뷰] Segment Anything in High Quality (HQ-SAM) - 전생했더니 인공지능이었던 건에 대하여, <a href="https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/hq-sam/">https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/hq-sam/</a></li>
<li>Segment Anything Model (SAM): Intro, Use Cases, V7 Tutorial - V7 Labs, https://www.v7labs.com/blog/segment-anything-model-sam</li>
<li>(PDF) Segment Anything Model for Industrial Vision: A Comprehensive Evaluation with a New Metric, a New Dataset, and a Toolbox - ResearchGate, https://www.researchgate.net/publication/379951503_Segment_Anything_Model_for_Industrial_Vision_A_Comprehensive_Evaluation_with_a_New_Metric_a_New_Dataset_and_a_Toolbox</li>
<li>AI’s New Breakthrough: Segment Anything Model (SAM) - AIFT, https://hkaift.com/ais-new-breakthrough-segment-anything-model-sam/</li>
<li>Supplementary material: Segment Anything - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2023/supplemental/Kirillov_Segment_Anything_ICCV_2023_supplemental.pdf</li>
<li>SAM: Segment Anything Model - Medium, https://medium.com/@hasfatauil12/sam-segment-anything-model-d4f541165f6b</li>
<li>SAM(Segment Anything) - Jordano - 티스토리, https://jordano-jackson.tistory.com/121</li>
<li>Introducing Meta Segment Anything Model 2 (SAM 2), https://ai.meta.com/sam2/</li>
<li>[2025-1] 김경훈 - SAM (Segment Anything Model), https://blog.outta.ai/44</li>
<li>[논문 리뷰] Segment Anything - velog, <a href="https://velog.io/@barley_15/%EB%85%BC%EB%AC%B8-%EC%A0%95%EB%A6%AC-Segment-Anything">https://velog.io/@barley_15/%EB%85%BC%EB%AC%B8-%EC%A0%95%EB%A6%AC-Segment-Anything</a></li>
<li>[논문 리뷰] Segment Anything - 러닝머신의 머신러닝, https://dhk1349.tistory.com/23</li>
<li>Meta AI의 SAM(Segment Anything Model) 리뷰 - cherish-j devlog - 티스토리, https://cherish-j.tistory.com/142</li>
<li>[Paper Review] Segment Anything Model (SAM) 자세한 논문 리뷰 …, <a href="https://2na-97.tistory.com/entry/Paper-Review-Segment-Anything-Model-SAM-%EC%9E%90%EC%84%B8%ED%95%9C-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Meta%EC%9D%98-Segment-Anything-%EC%84%A4%EB%AA%85">https://2na-97.tistory.com/entry/Paper-Review-Segment-Anything-Model-SAM-%EC%9E%90%EC%84%B8%ED%95%9C-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Meta%EC%9D%98-Segment-Anything-%EC%84%A4%EB%AA%85</a></li>
<li>Instance segmentation loss functions - SoftwareMill, https://softwaremill.com/instance-segmentation-loss-functions/</li>
<li>Multiclass segmentation for different loss functions(Dice loss, Focal loss, Total loss = (Summation of Dice and focal loss)) in Tensorflow | by Momojit Biswas | Medium, https://medium.com/@mb16biswas/multiclass-segmentation-for-different-loss-functions-dice-loss-focal-loss-total-loss-summation-455178517cea</li>
<li>Understanding Loss Functions for Deep Learning Segmentation Models - Medium, https://medium.com/@devanshipratiher/understanding-loss-functions-for-deep-learning-segmentation-models-30187836b30a</li>
<li>Segment Anything | Papers With Code, https://paperswithcode.com/paper/segment-anything</li>
<li>Segment Anything in High Quality | OpenReview, <a href="https://openreview.net/forum?id=RA7ND878XP&amp;noteId=8UFWRyRXj2">https://openreview.net/forum?id=RA7ND878XP¬eId=8UFWRyRXj2</a></li>
<li>SA-1B Benchmark (Segmentation) - Papers With Code, https://paperswithcode.com/sota/segmentation-on-sa-1b?p=segment-anything-without-supervision</li>
<li>SqueezeSAM: User-Friendly Mobile Interactive Segmentation - arXiv, https://arxiv.org/html/2312.06736v3</li>
<li>EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything - arXiv, https://arxiv.org/html/2312.00863v1</li>
<li>EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Xiong_EfficientSAM_Leveraged_Masked_Image_Pretraining_for_Efficient_Segment_Anything_CVPR_2024_paper.pdf</li>
<li>Top 5 Use Cases for Segment Anything Model (SAM) - Roboflow Blog, https://blog.roboflow.com/sam-use-cases/</li>
<li>SAMStyler: Enhancing Visual Creativity with Neural Style Transfer and Segment Anything Model (SAM) - ResearchGate, https://www.researchgate.net/publication/373907846_SAMStyler_Enhancing_Visual_Creativity_with_Neural_Style_Transfer_and_Segment_Anything_Model_SAM</li>
<li>Stable Diffusion Inpainting with SAM: A Comprehensive Guide - Ikomia, https://www.ikomia.ai/blog/stable-diffusion-inpainting-with-segment-anything-model-sam-using-the-ikomia-api</li>
<li>Segment Anything Model for Medical Image Segmentation: Current Applications and Future Directions - arXiv, https://arxiv.org/html/2401.03495v1</li>
<li>Generalist Vision Foundation Models for Medical Imaging: A Case Study of Segment Anything Model on Zero-Shot Medical Segmentation - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC10252742/</li>
<li>Segment Anything Is Not Always Perfect: An Investigation of SAM on Different Real-world Applications[1] - GitHub Pages, <a href="https://vision-based-industrial-inspection.github.io/cvpr-2023/pdf/Segment%20Anything%20Is%20Not%20Always%20Perfect%20An%20Investigation%20of%20SAM%20on%20Different%20Real-world%20Applications.pdf">https://vision-based-industrial-inspection.github.io/cvpr-2023/pdf/Segment%20Anything%20Is%20Not%20Always%20Perfect%20An%20Investigation%20of%20SAM%20on%20Different%20Real-world%20Applications.pdf</a></li>
<li>SAM for Road Object Segmentation: Promising but Challenging - MDPI, https://www.mdpi.com/2313-433X/11/6/189</li>
<li>image segmentExploring SAM (Segment Anything Model): The Future of Image Segmentation | by Saba Hesaraki | Medium, https://medium.com/@saba99/exploring-sam-segment-anything-model-the-future-of-image-segmentation-2f326cf6ec1a</li>
<li>SAID: Segment All Industrial Defects with Scene Prompts - MDPI, https://www.mdpi.com/1424-8220/25/16/4929</li>
<li>arXiv:2306.01567v2 [cs.CV] 23 Oct 2023, http://arxiv.org/pdf/2306.01567</li>
<li>Boosting Segment Anything Model Towards Open-Vocabulary Learning - arXiv, https://arxiv.org/html/2312.03628v2</li>
<li>Using Segment Anything Model 2 for Zero-Shot 3D Segmentation of Abdominal Organs in Computed Tomography Scans to Adapt Video Tracking Capabilities for 3D Medical Imaging: Algorithm Development and Validation - JMIR AI, https://ai.jmir.org/2025/1/e72109/</li>
<li>[2408.00714] SAM 2: Segment Anything in Images and Videos - arXiv, https://arxiv.org/abs/2408.00714</li>
<li>SAM 2: Segment Anything in Images and Videos - JunHan’s AI Factory - 티스토리, https://junhan-ai.tistory.com/227</li>
<li>SAM 2: Segment Anything Model 2 - Ultralytics YOLO Docs, https://docs.ultralytics.com/models/sam-2/</li>
<li>Update: Expanding access to Meta Segment Anything 2.1 on Amazon SageMaker JumpStart, https://ai.meta.com/blog/segment-anything-2/</li>
<li>Segmentation Simplified: A Deep Dive into Meta’s SAM 2 - Labellerr, https://www.labellerr.com/blog/sam-2/</li>
<li>SAM2: 이미지와 비디오 모두에 적용 가능한 Segment Anything Model (feat. Meta), https://discuss.pytorch.kr/t/sam2-segment-anything-model-feat-meta/4949</li>
<li>Segment Anything Model 2 (SAM 2) &amp; SA-V Dataset from Meta AI - Encord, https://encord.com/blog/segment-anything-model-2-sam-2/</li>
<li>SAM 2: Segment Anything Model 2 - Ultralytics YOLO 문서, https://docs.ultralytics.com/ko/models/sam-2/</li>
<li>SAM 2, 이미지부터 비디오까지 ’모두를 분할(Segment Anything)’한다! - YouTube, https://www.youtube.com/watch?v=TkYZgoPzxQQ</li>
<li>From SAM to SAM 2: Exploring Improvements in Meta’s Segment Anything Model - arXiv, https://arxiv.org/html/2408.06305v1</li>
<li>SAM 2: Segment Anything in Images and Videos - OpenReview, https://openreview.net/forum?id=Ha6RTeWMd0</li>
<li>Segment anything model for medical image segmentation: Current applications and future directions - PubMed, https://pubmed.ncbi.nlm.nih.gov/38422961/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>