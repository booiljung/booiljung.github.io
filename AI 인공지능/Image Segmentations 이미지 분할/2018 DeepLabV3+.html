<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:DeepLabV3+ (2018) Atrous Separable Convolution을 이용한 인코더-디코더 구조의 Semantic Segmentation</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>DeepLabV3+ (2018) Atrous Separable Convolution을 이용한 인코더-디코더 구조의 Semantic Segmentation</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">이미지 분할 (Image Segmentations)</a> / <span>DeepLabV3+ (2018) Atrous Separable Convolution을 이용한 인코더-디코더 구조의 Semantic Segmentation</span></nav>
                </div>
            </header>
            <article>
                <h1>DeepLabV3+ (2018) Atrous Separable Convolution을 이용한 인코더-디코더 구조의 Semantic Segmentation</h1>
<h2>1. 서론</h2>
<p>컴퓨터 비전 분야에서 Semantic Segmentation은 이미지 내 모든 픽셀에 대해 특정 클래스 레이블을 할당하는 고밀도 예측(dense prediction) 작업이다.1 이는 단순히 이미지에 어떤 객체가 존재하는지를 판별하는 이미지 분류(image classification)를 넘어, 각 객체의 정확한 위치와 형태를 픽셀 단위로 식별하는 것을 목표로 한다.1 자율 주행 자동차의 도로 환경 인식, 의료 영상에서의 종양 영역 분할, 위성 이미지 분석을 통한 지형 분류 등 정밀한 시각적 이해가 요구되는 다양한 응용 분야에서 핵심적인 기술로 자리 잡았다.3</p>
<p>초기 딥러닝 기반의 Semantic Segmentation 모델들은 Fully Convolutional Networks (FCN)의 등장과 함께 큰 발전을 이루었으나, 두 가지 근본적인 문제에 직면했다. 첫째, 심층 컨볼루션 신경망(Deep Convolutional Neural Networks, DCNNs)의 반복적인 풀링(pooling) 및 스트라이드(stride) 연산으로 인해 특징 맵(feature map)의 공간적 해상도가 점차 감소하여 정교한 경계 예측이 어렵다는 점이다.5 둘째, 이미지 내에 다양한 크기로 존재하는 객체들을 효과적으로 인식하기 위해 다중 스케일(multi-scale) 정보를 처리해야 한다는 점이다.5</p>
<p>이러한 문제들을 해결하기 위해 구글 연구팀은 DeepLab 시리즈를 발표하며 Semantic Segmentation 분야의 발전을 선도했다. DeepLabV1은 Atrous Convolution(Dilated Convolution)을 도입하여 공간 해상도 손실 없이 수용 영역(receptive field)을 확장하는 방법을 제시했으며, 후처리 과정으로 Fully Connected Conditional Random Field (CRF)를 사용하여 경계선 디테일을 복원했다.7 DeepLabV2는 다양한 비율의 Atrous Convolution을 병렬로 사용하는 Atrous Spatial Pyramid Pooling (ASPP) 모듈을 제안하여 다중 스케일 문제를 체계적으로 해결하고자 했다.8 DeepLabV3는 ASPP 모듈에 이미지 레벨 특징(image-level features)과 배치 정규화(Batch Normalization)를 추가하여 성능을 개선하고, CRF 후처리를 제거하여 완전한 종단 간(end-to-end) 학습이 가능한 모델을 구현했다.5</p>
<p>본 안내서는 DeepLab 시리즈의 최종 진화형이라 할 수 있는 DeepLabV3+에 대한 심층적인 기술 분석을 제공하는 것을 목표로 한다. DeepLabV3+는 DeepLabV3의 강력한 특징 추출 능력을 인코더(encoder)로 활용하고, 여기에 간단하면서도 효과적인 디코더(decoder) 모듈을 결합하여 객체 경계 부분의 분할 정확도를 획기적으로 개선했다.10 또한, 연산 효율성을 극대화하기 위해 Depthwise Separable Convolution을 ASPP와 디코더 모듈에 전면적으로 적용하여 더 빠르고 강력한 네트워크를 완성했다.10</p>
<p>본 안내서는 먼저 Semantic Segmentation의 기본 개념과 DeepLab 시리즈의 발전 과정을 개괄하여 DeepLabV3+가 등장하게 된 기술적 배경을 설명한다. 이어서 DeepLabV3+의 핵심인 인코더-디코더 구조를 상세히 해부하고, Atrous Convolution, ASPP, Depthwise Separable Convolution, 그리고 Xception 백본 등 핵심 기술 요소들을 수학적 원리와 함께 심도 있게 분석한다. PASCAL VOC 2012 및 Cityscapes와 같은 주요 벤치마크 데이터셋에서의 성능 평가 결과를 통해 모델의 우수성을 검증하고, 자율 주행 및 의료 영상과 같은 실제 응용 분야에서의 활용 사례와 한계점을 고찰한다. 마지막으로, 최신 기술 동향인 트랜스포머(Transformer) 기반 모델들과의 비교 분석을 통해 DeepLabV3+의 현재적 위상과 기술적 유산을 조망하며 마무리한다.</p>
<h2>2.  Semantic Segmentation과 DeepLab 프레임워크의 기원</h2>
<p>DeepLabV3+의 혁신성을 온전히 이해하기 위해서는 먼저 Semantic Segmentation이라는 과업의 본질과, 이 문제를 해결하기 위해 DeepLab 시리즈가 거쳐온 기술적 진화 과정을 파악해야 한다. 이 장에서는 Semantic Segmentation의 목표와 핵심 과제를 정의하고, DeepLabV1부터 V3까지 각 버전이 이전 버전의 한계를 어떻게 극복하며 발전해왔는지 연대기적으로 추적한다. 이러한 분석은 DeepLabV3+ 아키텍처가 탄생하게 된 필연적인 논리적 경로를 제시할 것이다.</p>
<h3>2.1  고밀도 예측 과업의 정의: Semantic Segmentation의 목표와 도전 과제</h3>
<h4>2.1.1  핵심 정의 및 관련 과업과의 비교</h4>
<p>Semantic Segmentation은 컴퓨터 비전의 한 분야로, 이미지 내 모든 픽셀에 대해 미리 정의된 클래스(예: 사람, 자동차, 하늘) 중 하나의 레이블을 할당하는 것을 목표로 하는 고밀도 예측 과업이다.1 이 과업의 결과물은 원본 이미지와 동일한 해상도를 가지는 세그멘테이션 맵(segmentation map)이며, 각 픽셀은 해당 클래스에 따라 특정 색상으로 표현된다.1 이는 이미지에 ‘무엇이’ 있는지를 알려주는 이미지 분류와 달리, 각 클래스가 ‘어디에’ 위치하는지 정확한 공간 정보를 제공한다는 점에서 근본적인 차이가 있다.1</p>
<p>Semantic Segmentation은 다른 이미지 분할 과업들과 명확히 구분된다. <strong>Instance Segmentation</strong>은 같은 클래스에 속하더라도 개별 객체 인스턴스(instance)를 구별한다. 예를 들어, 이미지에 여러 대의 자동차가 있다면 Semantic Segmentation은 모든 자동차 픽셀을 ’자동차’라는 단일 클래스로 레이블링하지만, Instance Segmentation은 ‘자동차1’, ’자동차2’와 같이 각 자동차를 개별적으로 분할한다.1</p>
<p><strong>Panoptic Segmentation</strong>은 이 두 가지를 결합한 가장 포괄적인 형태의 분할로, 이미지의 모든 픽셀에 대해 Semantic 레이블(예: 자동차, 도로)과 Instance ID(예: 자동차1, 자동차2)를 동시에 할당한다.4 DeepLabV3+가 해결하고자 하는 과업은 이 중 Semantic Segmentation으로, 동일 클래스의 객체들을 하나의 의미론적 덩어리(stuff)로 취급하는 특징을 가진다.12</p>
<h4>2.1.2  근본적인 도전 과제</h4>
<p>딥러닝 기반 Semantic Segmentation 모델들은 두 가지 상충하는 요구사항 사이에서 균형을 찾아야 하는 근본적인 도전에 직면한다.</p>
<ol>
<li><strong>공간 해상도 손실 (Loss of Spatial Resolution)</strong>: 표준적인 컨볼루션 신경망(CNN)은 이미지 분류 과업에서 높은 성능을 달성하기 위해 연속적인 풀링(pooling) 또는 스트라이드 컨볼루션(strided convolution)을 사용하여 특징 맵의 공간적 차원을 점진적으로 축소한다. 이러한 다운샘플링(downsampling) 과정은 모델이 더 넓은 영역의 문맥 정보를 학습하고 위치 변화에 불변하는(invariant) 특징을 추출하는 데 도움을 주지만, Semantic Segmentation에서는 치명적인 단점으로 작용한다.5 다운샘플링은 픽셀 수준의 정교한 위치 정보를 소실시켜, 최종적으로 생성되는 세그멘테이션 맵의 경계가 불분명하고 뭉개지는 결과를 초래한다.</li>
<li><strong>다중 스케일 객체의 존재 (Existence of Objects at Multiple Scales)</strong>: 현실 세계의 이미지는 동일한 클래스의 객체라도 카메라와의 거리나 객체 자체의 크기 차이로 인해 매우 다양한 스케일로 나타난다.5 예를 들어, 도로 장면 이미지에는 가까이 있는 큰 자동차와 멀리 있는 작은 자동차가 공존한다. 따라서 효과적인 Segmentation 모델은 작은 객체의 디테일을 놓치지 않으면서도 큰 객체를 인식할 수 있는 넓은 문맥을 파악할 수 있도록, 다중 스케일 정보를 효과적으로 처리할 수 있는 능력을 갖추어야 한다.</li>
</ol>
<h3>2.2  DeepLab의 계보: 핵심 기여에 대한 연대기적 개요</h3>
<p>DeepLab 시리즈의 발전 과정은 앞서 언급된 두 가지 도전 과제를 해결하기 위한 연속적인 노력의 결과물이다. 각 버전은 이전 버전의 한계를 명확히 인식하고 이를 개선하기 위한 새로운 아이디어를 도입했으며, 이러한 점진적 혁신의 과정은 DeepLabV3+의 설계 철학을 이해하는 데 필수적인 배경을 제공한다.</p>
<ul>
<li><strong>DeepLabV1 (2014/2015)</strong>: 시리즈의 기반을 마련한 모델로, 두 가지 핵심 개념을 처음으로 제시했다.8</li>
<li><strong>Atrous Convolution</strong>: 공간 해상도 손실 문제에 대응하기 위한 핵심 혁신 기술이다. 프랑스어로 ’구멍이 있는’을 의미하는 ’à trous’에서 유래한 이 기법은, 기존 DCNN의 마지막 풀링 계층에서 스트라이드를 제거하여 특징 맵의 해상도를 유지하는 대신, 이후의 컨볼루션 필터 내부에 ’구멍(0)’을 삽입하여 필터의 유효 수용 영역을 확장한다.7 이를 통해 파라미터 수나 연산량 증가 없이 더 넓은 문맥 정보를 포착하면서도 고밀도의 특징 맵을 생성할 수 있었다.6 이는 DeepLab 시리즈 전체를 관통하는 가장 중요한 아이디어이다.</li>
<li><strong>Fully Connected Conditional Random Field (CRF)</strong>: DCNN이 생성한 조악한(coarse) 세그멘테이션 맵을 정제하기 위한 후처리 단계로 도입되었다.7 CRF는 DCNN의 높은 수준의 의미론적 예측 결과와 픽셀의 색상, 위치와 같은 낮은 수준의 이미지 정보를 결합하여, 객체의 경계를 선명하게 복원하는 역할을 수행했다.15 이는 DCNN의 약점인 경계 지역화(localization) 정확도 문제를 보완하기 위한 효과적인 해결책이었다.</li>
<li><strong>DeepLabV2 (2016)</strong>: V1을 기반으로 다중 스케일 객체 문제를 보다 체계적으로 다루기 위한 구조를 도입했다.8</li>
<li><strong>Atrous Spatial Pyramid Pooling (ASPP)</strong>: V2의 핵심 기여는 ASPP 모듈의 제안이다. ASPP는 동일한 특징 맵에 대해 서로 다른 확장률(dilation rate)을 가진 여러 개의 Atrous Convolution을 병렬적으로 적용한 후, 그 결과를 통합하는 방식이다.6 이를 통해 모델은 다양한 스케일의 문맥 정보를 동시에 포착할 수 있게 되어, 이미지 내 여러 크기의 객체에 대해 강건한 분할 성능을 보일 수 있었다.8 이는 이전의 일부 연구에서 사용되던 다중 스케일 이미지 입력 방식보다 훨씬 효율적이고 통합된 해결책이었다.</li>
<li><strong>DeepLabV3 (2017)</strong>: ASPP 모듈을 개선하고, CRF 후처리에 의존하지 않는 완전한 종단 간 학습 모델을 구현하는 데 초점을 맞췄다.8</li>
<li><strong>개선된 ASPP (Improved ASPP)</strong>: 기존 ASPP 모듈에 전역 평균 풀링(global average pooling)을 통해 얻은 이미지 레벨 특징을 추가하여 전역적인 문맥(global context) 정보를 포착하도록 개선했다.6 또한, 각 병렬 브랜치에 배치 정규화(Batch Normalization)를 도입하여 학습 안정성과 성능을 향상시켰다.8 이러한 개선은 매우 큰 확장률을 사용할 경우 3x3 필터가 사실상 1x1 필터처럼 동작하여 장거리 정보 포착에 실패하는 ASPP의 잠재적 문제점을 해결하는 데 기여했다.19</li>
<li><strong>CRF 제거</strong>: DeepLabV3의 가장 중요한 진보는 CRF 후처리를 제거한 것이다. 개선된 ASPP와 배치 정규화의 효과적인 사용 등 아키텍처 자체의 발전으로 인해 DCNN이 생성하는 세그멘테이션 맵의 품질이 충분히 높아졌고, 더 이상 연산 비용이 높고 종단 간 학습이 불가능한 CRF에 의존할 필요가 없어졌다.5 이로써 DeepLabV3는 온전히 딥러닝 구성 요소만으로 이루어진, 더 빠르고 효율적인 종단 간 학습 시스템으로 거듭났다.</li>
</ul>
<p>이러한 발전 과정은 단순히 성능을 개선하기 위한 무작위적인 시도의 나열이 아니라, 명확한 문제 인식과 그에 대한 논리적인 해결책 제시의 연속이었다. CNN의 다운샘플링으로 인한 ’의미론적 정보’와 ‘공간적 정밀도’ 간의 상충 관계라는 핵심 문제를 해결하기 위해, V1은 Atrous Convolution이라는 해법을 제시했지만 CRF라는 외부적 보완책이 필요했다. V2는 ASPP를 통해 다중 스케일 문제를 내부적으로 해결하려 했지만 여전히 CRF에 의존했다. 마침내 V3는 ASPP를 강화하여 CRF라는 외부적 의존성을 완전히 제거하고, 신경망 내부에서 모든 문제를 해결하는 통합된 프레임워크를 완성했다. 이처럼 각 단계는 이전 단계의 병목 현상(성능, 속도, 학습 가능성)을 식별하고 이를 더 우아하고 학습 가능한 신경망 구성 요소로 대체하는, 딥러닝 공학의 전형적인 발전 양상을 보여준다. DeepLabV3+는 이러한 진화의 정점에 서 있는 아키텍처이다.</p>
<table><thead><tr><th>모델 버전</th><th>발표 연도</th><th>핵심 혁신 기술</th><th>주요 백본 네트워크</th><th>PASCAL VOC 2012 Test mIoU (%)</th></tr></thead><tbody>
<tr><td>DeepLabV1</td><td>2014</td><td>Atrous Convolution, Fully Connected CRF</td><td>VGG-16</td><td>71.6 16</td></tr>
<tr><td>DeepLabV2</td><td>2016</td><td>Atrous Spatial Pyramid Pooling (ASPP)</td><td>ResNet-101</td><td>79.7 14</td></tr>
<tr><td>DeepLabV3</td><td>2017</td><td>개선된 ASPP (이미지 레벨 특징, 배치 정규화), CRF 제거</td><td>ResNet-101</td><td>86.9 5</td></tr>
<tr><td>DeepLabV3+</td><td>2018</td><td>인코더-디코더 구조, Depthwise Separable Convolution</td><td>Xception</td><td>89.0 5</td></tr>
</tbody></table>
<h2>3.  DeepLabV3+의 아키텍처 해부</h2>
<p>DeepLabV3+는 이전 버전인 DeepLabV3의 강력한 특징 추출 능력과 정교한 경계 복원을 위한 디코더 구조를 결합하여 Semantic Segmentation의 정확도를 한 단계 끌어올렸다. 이 장에서는 DeepLabV3+의 핵심 설계 철학인 인코더-디코더 패러다임을 중심으로, 각 구성 요소가 어떻게 유기적으로 작동하여 고품질의 세그멘테이션 맵을 생성하는지 상세히 분석한다.</p>
<h3>3.1  인코더-디코더 패러다임: 상호 보완적 관계</h3>
<p>Semantic Segmentation 네트워크 설계의 핵심은 의미론적 정보(semantic information)와 공간적 정보(spatial information) 간의 균형을 맞추는 데 있다. 네트워크의 깊은 계층으로 갈수록 특징 맵의 채널은 깊어지고 수용 영역은 넓어져, 이미지의 전반적인 문맥과 객체의 클래스를 이해하는 데 필요한 풍부한 의미론적 정보를 담게 된다. 하지만 이 과정에서 반복적인 다운샘플링으로 인해 공간 해상도가 크게 낮아져 객체의 정확한 경계와 같은 세밀한 공간적 정보는 소실된다.5 반대로 얕은 계층의 특징 맵은 높은 공간 해상도를 유지하여 디테일한 정보를 보존하지만, 수용 영역이 좁아 의미론적 이해도는 낮다.</p>
<p>인코더-디코더 아키텍처는 이러한 상충 관계를 해결하기 위한 효과적인 구조이다.5</p>
<ul>
<li><strong>인코더(Encoder)</strong>: 입력 이미지로부터 점진적으로 공간 해상도를 줄여나가며 고수준의 의미론적 특징을 추출하는 역할을 한다.</li>
<li><strong>디코더(Decoder)</strong>: 인코더가 추출한 저해상도의 의미론적 특징 맵을 다시 점진적으로 업샘플링(upsampling)하여 원본 이미지 크기의 세그멘테이션 맵으로 복원한다. 이 과정에서 인코더의 얕은 계층으로부터 ’스킵 연결(skip connection)’을 통해 고해상도의 공간적 정보를 전달받아, 의미론적 정보와 공간적 정보를 융합함으로써 정교한 경계 복원을 가능하게 한다.</li>
</ul>
<p>DeepLabV3+는 이러한 패러다임을 채택하여, 이미 검증된 고성능 모델인 <strong>DeepLabV3 전체를 강력한 인코더로 사용한다</strong>.5 DeepLabV3 인코더는 Atrous Convolution과 개선된 ASPP 모듈을 통해 다중 스케일 문맥 정보를 효율적으로 포착하여 의미론적으로 매우 풍부한 특징 맵을 생성한다. 이때 생성되는 특징 맵은 일반적으로 출력 스트라이드(Output Stride, OS) 8 또는 16으로, 원본 이미지보다 8배 또는 16배 작은 공간 해상도를 가진다.5</p>
<p>DeepLabV3+의 중요한 장점 중 하나는 Atrous Convolution을 통해 인코더 특징 맵의 해상도를 유연하게 제어할 수 있다는 점이다.10 OS를 8로 설정하면 더 조밀한 특징 맵을 얻어 정확도를 높일 수 있고, OS를 16으로 설정하면 연산량을 줄여 속도를 향상시킬 수 있다. 이러한 정확도와 속도 간의 트레이드오프(trade-off) 조절 능력은 고정된 구조를 가진 다른 인코더-디코더 모델에서는 찾아보기 힘든 유연성을 제공한다.10</p>
<h3>3.2  디코더 모듈: 공간적 디테일의 복원</h3>
<p>DeepLabV3가 최종 예측을 위해 단순히 특징 맵을 16배 쌍선형 보간(bilinear interpolation)했던 것과 달리, DeepLabV3+는 세밀한 경계 복원을 위해 의도적으로 단순하면서도 매우 효과적인 디코더 모듈을 설계했다.10 이 디코더의 작동 방식은 다음과 같은 단계로 이루어진다.</p>
<ol>
<li><strong>인코더 특징 업샘플링</strong>: 인코더의 최종 출력(ASPP 모듈을 통과한 특징 맵)을 먼저 4배 쌍선형 보간하여 공간 해상도를 높인다.5 예를 들어, OS=16인 경우 입력 특징 맵은 OS=4 수준의 해상도로 커진다.</li>
<li><strong>저수준 특징과의 연결</strong>: 백본 네트워크의 초기 계층(예: ResNet의 <code>conv2</code> 블록 출력)에서 저수준 특징(low-level features)을 가져온다. 이 특징들은 공간적으로는 풍부하지만(OS=4로 해상도가 높음) 의미론적으로는 빈약하다.5</li>
<li><strong>채널 수 감소</strong>: 저수준 특징은 일반적으로 채널 수가 매우 많기 때문에(예: 256 또는 512), 이를 그대로 사용할 경우 의미론적으로 더 중요한 인코더 특징의 영향력을 약화시킬 수 있다. 이 문제를 해결하기 위해 저수준 특징에 1x1 컨볼루션을 적용하여 채널 수를 대폭 줄인다(예: 48개로).10 이는 두 특징 간의 균형을 맞추고 학습을 안정화하는 매우 중요한 단계이다.</li>
<li><strong>특징 융합(Concatenation)</strong>: 4배 업샘플링된 인코더 특징과 채널 수가 감소된 저수준 특징을 채널 축을 따라 결합(concatenate)한다.5</li>
<li><strong>특징 정제</strong>: 결합된 특징 맵에 몇 개의 3x3 컨볼루션을 적용하여 융합된 정보를 정제하고 두 특징 간의 상호작용을 학습한다.10</li>
<li><strong>최종 업샘플링</strong>: 마지막으로, 정제된 특징 맵을 다시 4배 쌍선형 보간하여 최종적으로 원본 이미지와 동일한 해상도의 세그멘테이션 맵을 생성한다.5</li>
</ol>
<p>이러한 디코더 설계는 고수준의 의미론적 정보와 저수준의 공간적 정보를 효과적으로 융합하는 ’정보의 균형’을 맞추는 데 초점을 맞춘다. 특히 스킵 연결에 포함된 1x1 컨볼루션은 정보의 ‘조절기’ 역할을 수행한다. 만약 채널 수가 많은 저수준 특징을 그대로 결합하면, 역전파 과정에서 의미론적으로 덜 중요한 저수준 특징이 그래디언트 업데이트를 지배하게 될 위험이 있다. 1x1 컨볼루션은 저수준 공간 정보의 ’볼륨’을 학습 가능한 방식으로 조절하여, 의미론적 정보와의 융합이 보다 안정적이고 효과적으로 이루어지도록 보장한다. 이처럼 특징 채널의 균형을 맞추는 원리는 고성능 융합 기반 네트워크를 구축하는 데 있어 미묘하지만 결정적인 설계 디테일이다.</p>
<h3>3.3  V3+ 맥락에서의 Atrous Spatial Pyramid Pooling (ASPP)</h3>
<p>ASPP 모듈은 DeepLabV3+ 인코더의 핵심 구성 요소로서 그 중요성을 그대로 유지한다. 이 모듈은 백본 네트워크를 통해 추출된 특징 맵을 입력받아 다중 스케일 정보를 추출하는 역할을 담당한다. V3에서 개선된 ASPP의 구조는 다음과 같다.5</p>
<ul>
<li>1x1 컨볼루션 브랜치 1개</li>
<li>서로 다른 확장률(예: OS=16일 때 <code>rate = (6, 12, 18)</code>)을 가진 3x3 Atrous Convolution 브랜치 3개</li>
<li>이미지 전체의 문맥 정보를 포착하기 위한 이미지 풀링(Image Pooling) 브랜치 1개</li>
</ul>
<p>이미지 풀링 브랜치는 입력 특징 맵에 전역 평균 풀링을 적용한 후, 1x1 컨볼루션을 통해 채널 수를 조절하고, 다시 원래 특징 맵 크기로 업샘플링하여 전역적인 문맥 정보를 제공한다.19 이 5개의 브랜치에서 나온 결과들은 모두 채널 축을 따라 결합된 후, 최종적으로 1x1 컨볼루션을 거쳐 인코더의 최종 출력 특징 맵을 생성한다. 이 특징 맵이 디코더로 전달되어 저수준 특징과 융합되는 것이다. ASPP는 모델이 다양한 크기의 객체에 강건하게 대응할 수 있도록 보장하며, 이미지 풀링은 국소적인 정보에만 매몰되지 않고 전역적인 장면의 이해를 바탕으로 예측을 수행하도록 돕는다.</p>
<h2>4.  핵심 방법론과 연산 혁신</h2>
<p>DeepLabV3+의 뛰어난 성능과 효율성은 몇 가지 핵심적인 기술에 기반한다. 이 장에서는 모델의 근간을 이루는 Atrous Convolution, Depthwise Separable Convolution, 그리고 이 기술들을 효과적으로 활용하는 Xception 백본 아키텍처에 대해 심층적으로 분석한다. 각 기술의 원리를 수학적 수식과 시각적 자료를 통해 명확히 설명하고, 이들이 어떻게 결합하여 Semantic Segmentation 과업에서 시너지를 발휘하는지 탐구한다.</p>
<h3>4.1  Atrous (Dilated) Convolution: 고밀도 특징 추출의 엔진</h3>
<p>Atrous Convolution, 또는 Dilated Convolution으로도 알려진 이 기법은 컨볼루션 필터의 파라미터 수나 연산량을 늘리지 않으면서 수용 영역(receptive field)을 효과적으로 확장하는 방법이다.6 이는 필터의 가중치들 사이에 <code>r-1</code>개의 0(zero)을 삽입함으로써 구현되며, 여기서 <code>r</code>은 확장률(dilation rate)을 의미한다.6 <code>r=1</code>일 때 Atrous Convolution은 표준 컨볼루션과 동일하다.</p>
<p>1차원 신호 <span class="math math-inline">x[i]</span>와 길이 <span class="math math-inline">K</span>의 필터 <span class="math math-inline">w[k]</span>에 대한 Atrous Convolution의 출력 <span class="math math-inline">y[i]</span>는 다음과 같이 정의할 수 있다.<br />
<span class="math math-display">
y[i] = \sum_{k=1}^{K} x[i + r \cdot k] \cdot w[k]
</span><br />
이 수식은 확장률 <span class="math math-inline">r</span>이 입력 신호에서 샘플링하는 간격을 어떻게 넓히는지를 명확하게 보여준다. 2차원 이미지에 대해서도 동일한 원리가 적용된다.</p>
<p>Atrous Convolution이 Semantic Segmentation에 기여하는 핵심적인 이점은, DCNN에서 공간 해상도를 유지하면서도 넓은 문맥 정보를 집약할 수 있게 해준다는 점이다.6 기존 CNN이 수용 영역을 넓히기 위해 풀링이나 스트라이드를 사용하여 특징 맵을 다운샘플링했던 것과 달리, Atrous Convolution은 스트라이드를 1로 고정한 채 확장률을 높임으로써 특징 맵의 크기를 그대로 유지하면서 더 넓은 영역의 정보를 고려할 수 있다. 이는 정교한 경계 예측에 필수적인 공간적 디테일을 보존하는 데 결정적인 역할을 한다.</p>
<h3>4.2  Depthwise Separable Convolution: 효율성의 열쇠</h3>
<p>Depthwise Separable Convolution은 표준 컨볼루션 연산을 두 개의 더 단순한 연산으로 분해하여 연산 비용과 파라미터 수를 획기적으로 줄이는 기법이다.10 이는 공간적 상관관계(spatial correlation)와 채널 간 상관관계(cross-channel correlation)를 분리하여 처리한다는 아이디어에 기반한다.23</p>
<ol>
<li><strong>Depthwise Convolution (채널별 컨볼루션)</strong>: 이 단계에서는 입력 특징 맵의 각 채널에 대해 독립적으로 공간적 필터링을 수행한다. 입력 채널 수가 <code>M</code>개라면, <code>Dk x Dk x 1</code> 크기의 필터 <code>M</code>개를 사용하여 각 채널별로 컨볼루션을 적용한다.25 이 연산은 오직 공간적 특징만을 추출하며, 채널 간의 정보 교환은 일어나지 않는다.</li>
<li><strong>Pointwise Convolution (점별 컨볼루션)</strong>: Depthwise Convolution의 출력에 1x1 컨볼루션을 적용한다. 이 단계에서는 <code>1 x 1 x M</code> 크기의 필터 <code>N</code>개를 사용하여 <code>M</code>개의 채널에 걸쳐 있는 정보들을 선형적으로 결합하여 <code>N</code>개의 새로운 출력 채널을 생성한다.25 이 연산은 공간적 필터링 없이 오직 채널 간의 정보 혼합만을 담당한다.</li>
</ol>
<h4>4.2.1  연산 비용 분석</h4>
<p>이러한 분해는 상당한 연산 효율성 향상을 가져온다. <code>Dp x Dp</code> 크기의 출력 특징 맵을 생성할 때, 각 연산의 곱셈 연산량(FLOPs)은 다음과 같다.</p>
<ul>
<li>
<p><strong>표준 컨볼루션 (Standard Convolution)</strong>:</p>
<p><span class="math math-display">
Cost_{std} = D_k \times D_k \times M \times N \times D_p \times D_p
</span></p>
</li>
<li>
<p><strong>Depthwise Separable Convolution</strong>:</p>
<p><span class="math math-display">
Cost_{sep} = (D_k \times D_k \times M \times D_p \times D_p) + (M \times N \times D_p \times D_p)
</span><br />
(Depthwise 연산량) + (Pointwise 연산량)</p>
</li>
</ul>
<p>두 연산 비용의 비율을 계산하면 다음과 같다.25</p>
<p><span class="math math-display">
\frac{Cost_{sep}}{Cost_{std}} = \frac{D_k^2 \cdot M \cdot D_p^2 + M \cdot N \cdot D_p^2}{D_k^2 \cdot M \cdot N \cdot D_p^2} = \frac{1}{N} + \frac{1}{D_k^2}
</span><br />
이 공식은 Depthwise Separable Convolution의 효율성이 출력 채널 수 <code>N</code>과 커널 크기 <code>Dk</code>에 따라 결정됨을 보여준다. 예를 들어, 3x3 커널(<code>Dk=3</code>)을 사용하고 출력 채널 수가 256개(<code>N=256</code>)라면, 연산량은 표준 컨볼루션의 약 1/9 수준으로 감소한다.</p>
<table><thead><tr><th>연산 유형</th><th>파라미터 수 공식</th><th>FLOPs 공식</th><th>예시 (입력: 128x128x256, 커널: 3x3, 출력 채널: 512)</th></tr></thead><tbody>
<tr><td>표준 컨볼루션</td><td><span class="math math-inline">D_k \times D_k \times M \times N</span></td><td><span class="math math-inline">D_k^2 \cdot M \cdot N \cdot D_p^2</span></td><td>파라미터: 1,179,648 / FLOPs: ~19.3 GFLOPs</td></tr>
<tr><td>Depthwise Separable</td><td><span class="math math-inline">(D_k \times D_k \times M) + (M \times N)</span></td><td><span class="math math-inline">M \cdot D_p^2 \cdot (D_k^2 + N)</span></td><td>파라미터: 133,120 / FLOPs: ~2.2 GFLOPs</td></tr>
</tbody></table>
<p>DeepLabV3+에서는 이 두 개념을 결합한 <strong>Atrous Separable Convolution</strong>을 사용한다.5 이는 Depthwise Convolution 단계에 확장률을 적용하는 것으로, 모델이 고밀도 특징을 추출하면서도 동시에 연산 효율성을 유지할 수 있게 하는 핵심 기술이다.</p>
<h3>4.3  수정된 Aligned Xception 백본</h3>
<p>DeepLabV3+의 뛰어난 성능은 강력하고 효율적인 백본 네트워크의 선택에 크게 힘입었다. 저자들은 기존의 ResNet 대신, Depthwise Separable Convolution을 기반으로 설계된 <strong>Xception</strong>(“Extreme Inception”) 아키텍처를 수정하여 사용했다.5</p>
<p>Xception은 공간적 상관관계와 채널 간 상관관계를 완전히 분리할 수 있다는 가설 아래, 거의 모든 컨볼루션 계층을 Depthwise Separable Convolution으로 구성한 모델이다.24 DeepLabV3+에서는 Semantic Segmentation 과업에 적합하도록 원본 Xception을 다음과 같이 수정했다.5</p>
<ol>
<li><strong>최대 풀링(Max Pooling) 제거</strong>: 원본 Xception의 모든 최대 풀링 연산을 스트라이드가 있는 Depthwise Separable Convolution으로 대체했다. 이를 통해 임의의 해상도로 특징 맵을 추출하는 것이 가능해졌으며, 이는 Atrous Separable Convolution을 적용하는 데 필수적이다.</li>
<li><strong>추가적인 정규화 및 활성화</strong>: MobileNet과 유사하게, 모든 3x3 Depthwise Convolution 뒤에 배치 정규화(Batch Normalization)와 ReLU 활성화 함수를 추가하여 학습을 안정화하고 성능을 향상시켰다.</li>
</ol>
<p>Xception 백본의 채택은 단순한 교체를 넘어선 패러다임의 전환을 의미했다. 기존의 강력하지만 무거운 ResNet 대신, 연산 효율이 뛰어난 분해된 컨볼루션(factorized convolution) 기반의 아키텍처가 최상위 성능을 달성할 수 있음을 입증한 것이다. 이는 Depthwise Separable Convolution이 단순히 모바일 환경을 위한 경량화 기법이 아니라, 고성능 모델을 설계하는 보편적인 원리가 될 수 있음을 보여준 중요한 사례이다. Atrous Convolution의 의미론적 풍부함과 Separable Convolution의 연산 효율성이 결합된 Xception 백본은 DeepLabV3+가 정확도와 속도 두 마리 토끼를 모두 잡을 수 있게 한 결정적인 요소였다.</p>
<h2>5.  실험적 검증 및 성능 벤치마크</h2>
<p>아키텍처의 우수성은 결국 정량적인 실험 결과를 통해 입증된다. 이 장에서는 DeepLabV3+가 Semantic Segmentation 분야의 표준 벤치마크인 PASCAL VOC 2012와 Cityscapes 데이터셋에서 달성한 성능을 분석한다. 또한, 모델의 각 구성 요소가 성능 향상에 얼마나 기여했는지를 파악하기 위해 저자들이 수행한 제거 연구(ablation study) 결과를 심도 있게 검토한다.</p>
<h3>5.1  PASCAL VOC 2012에서의 성능</h3>
<p>PASCAL VOC 2012 데이터셋은 20개의 전경 객체 클래스와 1개의 배경 클래스로 구성된 Semantic Segmentation의 고전적인 벤치마크이다.28 최고 성능을 달성하기 위해, 일반적으로 10,582개의 학습 이미지를 포함하는 증강된 학습셋(<code>trainaug</code>)이 사용된다.28</p>
<p>DeepLabV3+는 이 데이터셋에서 당시 최고 수준(State-Of-The-Art, SOTA)의 성능을 기록하며 그 효과를 입증했다. 성능 평가의 핵심 지표는 mIoU(mean Intersection over Union)로, 예측된 세그멘테이션 영역과 실제 영역 간의 중첩도를 클래스별로 계산하여 평균 낸 값이다.</p>
<ul>
<li><strong>ResNet-101 백본 사용 시</strong>: 87.8% mIoU 달성 5</li>
<li><strong>수정된 Aligned Xception 백본 사용 시</strong>: <strong>89.0% mIoU</strong> 달성 5</li>
</ul>
<p>특히 Xception 백본을 사용했을 때 달성한 89.0%의 mIoU는 후처리 과정 없이 단일 모델로 달성한 놀라운 성과였으며, 이는 DeepLabV3+의 아키텍처 설계가 매우 효과적임을 보여준다. 정성적인 결과에서도 이전 모델들에 비해 객체의 경계를 훨씬 더 선명하고 정확하게 분할하는 모습을 보여주었다.30</p>
<h3>5.2  Cityscapes에서의 성능</h3>
<p>Cityscapes 데이터셋은 자율 주행 연구를 위해 제작된 대규모 데이터셋으로, 19개의 클래스로 구성된 복잡한 도시 거리 풍경 이미지를 포함한다.32 1024x2048의 고해상도 이미지는 모델에게 상당한 계산적, 기술적 도전을 제기한다.</p>
<p>DeepLabV3+는 이처럼 까다로운 벤치마크에서도 뛰어난 성능을 보였다.</p>
<ul>
<li><strong>Xception 백본 사용 시</strong>: Cityscapes 테스트셋에서 <strong>82.1% mIoU</strong>를 달성하며 SOTA 성능을 기록했다.5 이 결과는 DeepLabV3+가 실제 자율 주행 환경과 같이 복잡하고 정밀도가 요구되는 시나리오에서도 매우 효과적으로 작동할 수 있음을 시사한다.</li>
</ul>
<table><thead><tr><th>데이터셋</th><th>백본 네트워크</th><th>사전 학습 데이터</th><th>출력 스트라이드(OS)</th><th>mIoU (%)</th></tr></thead><tbody>
<tr><td>PASCAL VOC 2012 test</td><td>ResNet-101</td><td>ImageNet</td><td>16</td><td>87.8 5</td></tr>
<tr><td>PASCAL VOC 2012 test</td><td>Xception-65</td><td>ImageNet</td><td>16</td><td>88.5 (논문 표기 기준)</td></tr>
<tr><td>PASCAL VOC 2012 test</td><td>Xception-65</td><td>ImageNet + JFT</td><td>16</td><td><strong>89.0</strong> 5</td></tr>
<tr><td>Cityscapes test</td><td>Xception-71</td><td>ImageNet</td><td>16</td><td>81.3 (논문 표기 기준)</td></tr>
<tr><td>Cityscapes test</td><td>Xception-71</td><td>ImageNet + COCO</td><td>16</td><td><strong>82.1</strong> 5</td></tr>
</tbody></table>
<h3>5.3  제거 연구: 성능 향상의 원인 분석</h3>
<p>DeepLabV3+의 성능 향상이 단일 요소가 아닌 여러 구성 요소의 시너지 효과에 기인함을 이해하는 것이 중요하다. 원본 논문에 제시된 제거 연구는 각 설계 결정의 타당성을 뒷받침하는 강력한 증거를 제공한다.5</p>
<ul>
<li>
<p><strong>디코더 모듈의 중요성</strong>: 디코더의 유무에 따른 성능 차이는 디코더 모듈의 효과를 명확히 보여준다. PASCAL VOC 검증셋(val set)에서 OS=16으로 추론했을 때,</p>
</li>
<li>
<p>ResNet-101 기반 DeepLabV3에 디코더를 추가하자 mIoU가 77.21%에서 78.85%로 약 1.6%p 상승했다.30</p>
</li>
<li>
<p>Xception 기반 모델에 디코더를 추가했을 때는 mIoU가 81.34%에서 82.55%로 상승했다.</p>
</li>
</ul>
<p>이는 디코더가 인코더의 종류와 무관하게 저수준 특징을 활용하여 경계를 정제하는 데 일관되게 기여함을 의미한다.</p>
<ul>
<li>
<p><strong>Xception 백본의 효과</strong>: 백본 네트워크를 ResNet-101에서 수정된 Xception으로 교체하는 것만으로도 상당한 성능 향상이 있었다. 디코더가 없는 상태에서 백본을 교체하자 mIoU가 79.17%에서 81.34%로 2%p 이상 크게 상승했다.5 이는 Xception 백본이 ResNet보다 Semantic Segmentation 과업에 더 효율적이고 강력한 특징 표현을 학습한다는 것을 입증한다.</p>
</li>
<li>
<p><strong>Atrous Separable Convolution의 기여</strong>: 논문에서는 Atrous Separable Convolution을 ASPP와 디코더 모듈에 적용함으로써, 연산량을 크게 줄이면서도 성능 저하 없이, 오히려 일부 경우 성능이 향상됨을 보였다.10</p>
</li>
</ul>
<p>이러한 분석을 통해 DeepLabV3+의 성공이 단순히 더 좋은 백본을 사용했기 때문이 아니라, 시스템 전체의 시너지 효과 덕분임이 드러난다. Xception 백본은 그 자체로도 우수하지만, 그 진정한 가치는 Atrous Separable Convolution의 적용을 가능하게 하여 인코더가 매우 효율적으로 풍부한 다중 스케일 특징을 추출할 수 있게 한 데 있다. 이렇게 생성된 고품질의 의미론적 특징 맵을 효과적인 디코더가 이어받아 정제함으로써, 전체 시스템은 각 부분의 합보다 더 큰 성능 향상을 이루어냈다. 이는 DeepLabV3+가 개별 부품의 최적화를 넘어, 전체 시스템의 유기적 설계를 통해 SOTA 성능을 달성했음을 보여주는 사례이다.</p>
<h2>6.  특정 도메인 응용 및 적응</h2>
<p>DeepLabV3+의 강력한 성능과 유연한 아키텍처는 학술적 벤치마크를 넘어 다양한 실제 산업 분야로의 확장을 가능하게 했다. 특히, 높은 정밀도와 신뢰성이 요구되는 자율 주행 및 의료 영상 분야에서 DeepLabV3+는 핵심적인 기술로 채택되고 변형되어 왔다. 이 장에서는 이 두 분야에서의 구체적인 응용 사례를 살펴보고, 실시간 처리가 필수적인 환경을 위해 모델을 경량화하려는 연구 동향을 분석한다.</p>
<h3>6.1  자율 주행: 강건한 장면 인식의 구현</h3>
<p>자율 주행 시스템에서 Semantic Segmentation은 차량 주변 환경을 픽셀 단위로 완벽하게 이해하기 위한 필수적인 인식 기술이다.33 도로나 차선, 보행자, 다른 차량, 교통 표지판 등을 정확히 분할하는 능력은 안전한 경로 계획과 의사 결정의 기반이 된다.3</p>
<p>DeepLabV3+는 Cityscapes와 같은 도시 풍경 데이터셋에서 입증된 높은 정확도 덕분에 자율 주행 분야에서 널리 활용되고 있다.36 구체적인 응용 사례는 다음과 같다.</p>
<ul>
<li><strong>차선 및 주행 가능 영역 검출</strong>: 차량이 주행해야 할 차선과 안전하게 이동할 수 있는 영역을 정밀하게 분할하여 경로 유지를 돕는다.34</li>
<li><strong>객체 인식 및 위험 평가</strong>: 보행자, 자전거, 다른 차량 등의 위치와 형태를 정확히 파악하여 충돌 위험을 예측하고 회피 기동을 계획하는 데 사용된다.</li>
<li><strong>교통 신호 및 표지판 인식</strong>: 정지 신호, 속도 제한 표지판 등을 분할하여 교통 법규를 준수하는 주행을 가능하게 한다.</li>
</ul>
<p>하지만 실제 도로 환경은 궂은 날씨, 야간이나 저조도 환경, 희미하거나 손상된 차선, 복잡한 배경 등 예측 불가능한 변수가 많다.33 이러한 ’야생 환경(in the wild)’에서의 강건성을 확보하기 위해, 원본 DeepLabV3+를 기반으로 특정 상황에 맞게 성능을 개선하려는 다양한 연구가 활발히 진행되고 있다.</p>
<h3>6.2  의료 영상 분석: 진단 능력의 향상</h3>
<p>의료 영상 분석 분야에서 Semantic Segmentation은 의사의 진단을 보조하고 치료 계획을 수립하는 데 결정적인 역할을 한다. MRI, CT, X-ray 등의 영상에서 특정 장기, 조직, 또는 종양과 같은 병변 영역을 자동으로 정밀하게 분할하는 것은 수동 작업에 비해 시간과 노력을 크게 절감시키고 객관성과 일관성을 높여준다.4</p>
<p>DeepLabV3+는 의료 영상의 복잡하고 미세한 구조를 분할하는 데 뛰어난 성능을 보여 다양한 분야에 적용되었다.</p>
<ul>
<li><strong>뇌종양 분할</strong>: 다중 모달리티(multi-modal) MRI 영상에서 전체 종양(whole tumor), 종양 핵심부(tumor core), 강화 종양(enhancing tumor) 등 세부적인 영역을 분할하여 종양의 특성을 분석하고 수술 계획을 세우는 데 활용된다.41</li>
<li><strong>COVID-19 폐 병변 분할</strong>: 흉부 CT 영상에서 COVID-19 감염으로 인한 폐 병변 영역을 분할하여 감염 정도를 정량적으로 평가하고 질병의 경과를 추적하는 데 사용된다.42</li>
<li><strong>치과 영상 분석</strong>: 파노라마 방사선 사진에서 치근단 병소(apical lesion)를 자동으로 검출하고 분할하여 진단의 정확도를 높인다.44</li>
</ul>
<p>의료 영상은 일반적인 자연 이미지와는 다른 특성을 가진다. 병변의 크기와 형태가 매우 다양하고, 정상 조직과의 경계가 불분명한 경우가 많다. 이러한 특성에 대응하기 위해 DeepLabV3+ 아키텍처를 수정하는 연구가 다수 진행되었다. 예를 들어, 작고 복잡한 형태의 병변을 더 잘 포착하기 위해 ASPP 모듈의 Atrous rate를 기존보다 작은 값으로 조정하여, 넓은 문맥보다는 국소적인 디테일에 더 집중하도록 모델을 변형하는 접근 방식이 효과적임이 입증되었다.42</p>
<h3>6.3  실시간 처리를 위한 경량화 수정</h3>
<p>Xception 백본을 사용하는 DeepLabV3+는 높은 정확도를 자랑하지만, 상당한 연산 자원을 요구하기 때문에 차량 내 임베디드 시스템이나 모바일 헬스케어 기기와 같이 자원이 제한된 엣지 디바이스(edge device)에 배포하기에는 부담이 될 수 있다.45 따라서 실시간 처리가 요구되는 응용 분야를 위해 모델을 경량화하려는 노력이 활발히 이루어지고 있다.</p>
<p>가장 일반적인 전략은 백본 네트워크를 더 가벼운 모델로 교체하는 것이다. <strong>MobileNetV2</strong>는 모바일 환경을 위해 특별히 설계된 경량 네트워크로, 역 잔차(inverted residual) 및 선형 병목(linear bottleneck) 구조를 사용하여 파라미터 수와 연산량을 크게 줄였다.47 DeepLabV3+의 백본을 Xception에서 MobileNetV2로 교체하면 모델의 크기가 대폭 감소하여 추론 속도(FPS, Frames Per Second)를 크게 향상시킬 수 있다.35</p>
<p>물론 이러한 경량화는 일반적으로 약간의 정확도 저하를 동반한다.46 따라서 많은 연구에서는 MobileNetV2 백본을 사용하면서도, 어텐션 메커니즘(attention mechanism)과 같은 추가적인 모듈을 도입하여 손실된 정확도를 일부 회복하려는 시도를 하고 있다.46 이를 통해 속도와 정확도 사이의 최적의 균형점을 찾는 것이 실시간 Semantic Segmentation 모델 연구의 주요 과제 중 하나이다.</p>
<p>이처럼 DeepLabV3+가 자율 주행과 의료 영상이라는 상이한 도메인에서 성공적으로 적용되고, 실시간 처리를 위해 유연하게 변형될 수 있다는 사실은 이 아키텍처가 가진 놀라운 ’구조적 유연성(architectural plasticity)’을 보여준다. Atrous Convolution, ASPP, 인코더-디코더 구조와 같은 핵심 구성 요소들은 특정 데이터 유형에 과도하게 최적화된 것이 아니라, 고밀도 예측을 위한 보편적인 설계 원칙을 담고 있다. 따라서 백본 네트워크를 교체하거나 ASPP의 파라미터를 조정하는 것만으로도 각 응용 분야의 고유한 제약 조건(객체 스케일, 연산 예산)에 맞춰 효과적으로 모델을 적응시킬 수 있다. DeepLabV3+는 단일 모델이 아니라, 다양한 문제에 맞게 변형하고 확장할 수 있는 강력하고 유연한 ’프레임워크’로서 그 가치를 입증하고 있다.</p>
<h2>7.  비판적 평가: 한계와 후속 개선 연구</h2>
<p>어떠한 모델도 완벽할 수 없으며, DeepLabV3+ 역시 몇 가지 내재적인 한계를 가지고 있다. 이러한 한계는 후속 연구자들에게 새로운 도전 과제를 제시했으며, 이를 극복하기 위한 다양한 개선 방안들이 제안되었다. 이 장에서는 DeepLabV3+의 구조적 한계를 비판적으로 분석하고, 특히 어텐션 메커니즘의 도입을 중심으로 이루어진 주요 개선 연구들을 살펴본다.</p>
<h3>7.1  내재된 아키텍처의 한계</h3>
<p>DeepLabV3+는 이전 모델들에 비해 크게 발전했지만, 다음과 같은 한계점들이 지적된다.</p>
<ul>
<li><strong>경계 분할의 부정확성</strong>: 디코더 모듈이 저수준 특징을 활용하여 경계를 정제함에도 불구하고, 완벽하게 선명한 객체 경계를 생성하는 데에는 여전히 어려움이 있다. 특히 복잡한 형태를 가진 객체의 경우, 거친 의미론적 맵과 저수준 특징의 융합 과정에서 여전히 경계가 약간 흐릿하거나 부정확하게 예측될 수 있다.51</li>
<li><strong>작은 객체 분할의 어려움</strong>: ASPP 모듈이 다중 스케일 처리를 돕지만, 크기가 매우 작은 객체는 인코더의 초기 다운샘플링 과정에서 특징 정보가 완전히 소실될 수 있다.52 이 경우, 디코더가 복원할 정보 자체가 없기 때문에 작은 객체를 놓치거나 부정확하게 분할하는 문제가 발생한다.</li>
<li><strong>공간적 관계 정보의 손실</strong>: 모델이 기본적으로 픽셀 단위 분류(per-pixel classification)에 의존하기 때문에, 픽셀들 간의 전역적인 공간 관계나 구조적 정보를 명시적으로 모델링하는 데 한계가 있다.51 이로 인해 때때로 문맥적으로 비합리적인 예측(예: 건물 한가운데에 ‘도로’ 픽셀이 나타나는 경우)이 발생할 수 있다.</li>
<li><strong>까다로운 조명 조건에서의 성능 저하</strong>: 표준 DeepLabV3+ 모델은 저조도나 역광과 같은 비정상적인 조명 조건의 이미지에서 성능이 저하될 수 있다. 이러한 환경에서는 특징 추출기(백본 네트워크)가 강건한 특징을 추출하는 데 어려움을 겪기 때문이다.51</li>
</ul>
<h3>7.2  어텐션 메커니즘의 통합: 중요한 것에 집중하기</h3>
<p>이러한 한계들을 극복하기 위한 가장 유망한 접근법 중 하나는 어텐션 메커니즘(attention mechanism)의 도입이다. 어텐션 모듈은 신경망이 이미지의 특정 영역이나 특징 맵의 특정 채널에 더 ’집중’하도록 학습하여, 제한된 자원을 효율적으로 사용하고 중요한 정보에 가중치를 부여하는 역할을 한다.53 이는 특징 추출 과정을 데이터에 따라 동적으로 조절함으로써 DeepLabV3+의 정적인 필터링 방식의 단점을 보완한다.</p>
<ul>
<li><strong>채널 어텐션 (Channel Attention)</strong>: Squeeze-and-Excitation (SE) 모듈이 대표적인 예이다. 이 메커니즘은 특징 맵의 여러 채널들 간의 상호 의존성을 모델링하여, 각 채널의 중요도를 계산하고 이를 가중치로 적용한다.46 이를 통해 정보량이 많은 채널은 강조하고, 상대적으로 덜 중요한 채널은 억제하여 특징 표현의 질을 높인다. 특히 작은 객체나 세밀한 특징을 구분하는 데 효과적인 것으로 알려져 있다.</li>
<li><strong>공간 어텐션 (Spatial Attention)</strong>: 특징 맵 내에서 공간적으로 어떤 위치가 더 중요한지에 대한 ’어텐션 맵’을 생성한다. 이 맵을 원래 특징 맵에 곱해줌으로써, 모델이 객체의 주요 부분이나 경계와 같은 핵심 영역에 더 집중하도록 유도한다.</li>
<li><strong>하이브리드 어텐션 (Hybrid Attention)</strong>: Convolutional Block Attention Module (CBAM)은 채널 어텐션과 공간 어텐션을 순차적으로 결합하여 두 가지 측면에서 모두 특징을 정제하는 방식이다.38 CBAM은 DeepLabV3+ 아키텍처의 다양한 위치(백본, ASPP 이후 등)에 통합되어 핵심 특징에 대한 집중력을 높이고 분할 정확도를 개선하는 연구에 널리 사용되었다.</li>
</ul>
<p>이러한 어텐션 모듈의 도입은 DeepLabV3+의 성능을 점진적으로 개선하는 실용적인 접근법으로 자리 잡았다. 기존의 강력한 컨볼루션 기반 아키텍처의 골격은 유지하면서, 어텐션이라는 동적이고 내용 기반의(content-aware) 모듈을 ’부가 기능(bolt-on)’처럼 장착하는 것이다. 특징 추출의 주된 역할은 여전히 컨볼루션이 담당하지만, 어텐션은 추출된 특징을 한 번 더 정제하고 재조정하는 역할을 수행한다. 이는 전체 아키텍처를 처음부터 재설계하는 대신, 검증된 프레임워크의 특정 약점을 외과적으로 보완하여 성능을 향상시키는 효율적인 연구 개발 전략을 보여준다.</p>
<h3>7.3  기타 아키텍처 개선 연구</h3>
<p>어텐션 외에도 DeepLabV3+를 개선하기 위한 다양한 아키텍처 수정이 제안되었다.</p>
<ul>
<li><strong>ASPP 모듈 수정</strong>: 일부 연구에서는 ASPP 모듈 자체를 수정했다. 예를 들어, 연산량을 줄이기 위해 3x3 Atrous Convolution을 두 개의 1D 컨볼루션으로 분해하거나 56, 모듈 내부에 더 조밀한 연결(dense connection)을 추가하여 특징 전파를 개선하는(예: DenseDDSSPP) 방식이 있다.57</li>
<li><strong>디코더 강화</strong>: DeepLabV3+의 간단한 디코더를 넘어서, 쌍선형 보간 대신 학습 가능한 업샘플링 방식인 전치 컨볼루션(transposed convolution)을 사용하거나 58, 더 복잡한 융합 블록을 설계하여 디코더의 성능을 강화하려는 시도도 있었다.</li>
<li><strong>다중 손실 함수 학습 (Multi-Loss Training)</strong>: 네트워크의 중간 지점에 보조적인 손실 함수(auxiliary loss)를 추가하여, 최종 출력뿐만 아니라 중간 특징 맵도 정답에 가깝게 학습되도록 강제하는 방식이다.52 이는 그래디언트 흐름을 원활하게 하고, 더 깊은 계층까지 효과적으로 학습이 전파되도록 돕는다.</li>
</ul>
<p>이러한 후속 연구들은 DeepLabV3+가 제공한 강력한 기반 위에서, 특정 약점을 보완하고 성능을 극한까지 끌어올리려는 커뮤니티의 지속적인 노력을 보여준다.</p>
<h2>8.  비교 전망: 트랜스포머 시대의 DeepLabV3+</h2>
<p>최근 컴퓨터 비전 분야, 특히 Semantic Segmentation은 트랜스포머(Transformer) 아키텍처의 등장으로 인해 또 한 번의 패러다임 전환을 맞이했다. 이 장에서는 DeepLabV3+를 컨볼루션 신경망(CNN) 시대의 정점으로 평가하고, 새로운 강자로 부상한 트랜스포머 기반 모델들과의 비교를 통해 그 기술적 강점과 약점, 그리고 현재적 위상을 분석한다.</p>
<h3>8.1  CNN 대 트랜스포머: 패러다임의 전환</h3>
<ul>
<li><strong>CNN의 귀납적 편향 (Inductive Bias)</strong>: DeepLabV3+를 포함한 CNN 기반 모델들은 두 가지 강력한 귀납적 편향을 내재하고 있다. 첫째는 **지역성(locality)**으로, 컨볼루션 필터가 이미지의 작은 인접 영역만을 처리한다는 점이다. 둘째는 **병진 등변성(translation equivariance)**으로, 필터가 이미지 내 위치에 상관없이 동일한 특징을 감지한다는 점이다.59 이러한 편향 덕분에 CNN은 적은 데이터로도 효율적인 학습이 가능하지만, 커널의 유한한 수용 영역으로 인해 이미지 전체에 걸친 장거리 의존성(long-range dependency)을 모델링하는 데에는 본질적인 한계를 가진다.61 Atrous Convolution은 이를 완화하지만, 완벽히 해결하지는 못한다.</li>
<li><strong>트랜스포머의 전역적 문맥 (Global Context)</strong>: 반면, Vision Transformer (ViT)는 이미지를 여러 개의 패치(patch)로 분할하고, 이를 시퀀스 데이터처럼 취급한다. 그리고 셀프 어텐션(self-attention) 메커니즘을 통해 모든 패치 쌍 간의 관계를 직접적으로 계산한다.61 이 덕분에 트랜스포머는 네트워크의 첫 계층부터 이미지 전체를 아우르는 전역적인 수용 영역을 가지며, 장거리 문맥 정보를 포착하는 데 매우 탁월한 능력을 보인다.62 하지만 CNN의 내재된 편향이 없어, 일반적으로 우수한 성능을 내기 위해서는 방대한 양의 데이터셋을 통한 사전 학습이 요구된다.65</li>
</ul>
<p><strong>SegFormer</strong> 61와 <strong>Mask2Former</strong> 67와 같은 최신 트랜스포머 기반 Segmentation 모델들은 계층적 구조와 효율적인 어텐션 방식을 도입하여 기존 CNN 기반 모델들의 성능을 여러 벤치마크에서 뛰어넘으며 새로운 SOTA를 기록하고 있다.</p>
<h3>8.2  하이브리드 모델의 부상: 두 세계의 장점 결합</h3>
<p>CNN과 트랜스포머가 서로 상보적인 강점을 가지고 있다는 인식 하에, 두 아키텍처를 결합하려는 하이브리드 모델들이 새로운 주류로 부상하고 있다.65</p>
<ul>
<li><strong>TransUNet</strong>: 의료 영상 분야에서 두각을 나타낸 모델로, U-Net 구조에 트랜스포머를 결합했다. 초기 계층에서는 CNN을 사용하여 효율적으로 지역적인 특징을 추출하고, 네트워크의 병목(bottleneck) 구간에서 이 특징 맵을 패치화하여 트랜스포머 인코더에 입력한다. 이를 통해 전역적인 관계를 모델링한 후, 다시 CNN 기반의 디코더와 스킵 연결을 통해 세밀한 공간 정보를 복원한다.70 즉, 지역적 특징 추출은 CNN에, 전역적 문맥 모델링은 트랜스포머에 맡기는 역할 분담 구조이다.</li>
<li><strong>CMT (Convolutional Neural Networks Meet Vision Transformers)</strong>: CNN과 트랜스포머 구성 요소를 더 긴밀하게 융합한 구조이다. CMT는 트랜스포머 블록 내부에 컨볼루션 연산을 포함시켜, 각 블록이 지역적 정보와 전역적 정보를 동시에 효율적으로 처리하도록 설계했다.72</li>
<li><strong>MobileViT</strong>: 경량화에 초점을 맞춘 하이브리드 모델로, CNN을 통해 초기 특징을 추출한 후, ’MobileViT 블록’이라는 특수한 모듈을 통해 트랜스포머를 마치 정교한 비지역적(non-local) 컨볼루션처럼 사용한다.63</li>
</ul>
<p>이러한 하이브리드 모델의 등장은 Semantic Segmentation의 미래가 ’CNN 대 트랜스포머’의 양자택일이 아니라, 두 패러다임의 지능적인 ’융합’에 있음을 시사한다. CNN의 효율적인 지역 특징 추출 능력과 트랜스포머의 강력한 전역 문맥 이해 능력을 어떻게 효과적으로 결합할 것인가가 현재 SOTA 아키텍처 설계의 핵심 과제가 되었다.</p>
<h3>8.3  결론적 분석: DeepLabV3+의 지속적인 유효성</h3>
<p>트랜스포머와 하이브리드 모델이 새로운 SOTA를 경신하고 있는 현재에도, DeepLabV3+는 여전히 중요한 의미를 지닌다.</p>
<ul>
<li><strong>강력한 성능 기준점 (Performance Baseline)</strong>: DeepLabV3+는 수많은 후속 연구에서 성능 비교를 위한 강력하고 신뢰할 수 있는 기준 모델(baseline)로 꾸준히 사용되고 있다.18 새로운 아키텍처의 우수성을 주장하기 위해서는 DeepLabV3+의 성능을 뛰어넘는 것이 일종의 관문처럼 여겨진다.</li>
<li><strong>실용적 이점</strong>: 많은 트랜스포머 모델이 방대한 데이터와 컴퓨팅 자원을 요구하는 반면, 잘 튜닝된 CNN 기반의 DeepLabV3+나 그 경량화 버전은 제한된 데이터나 연산 환경에서 더 실용적이고 강건한 선택지가 될 수 있다.65</li>
<li><strong>기술적 유산과 영향력</strong>: DeepLab 시리즈가 대중화하고 정립한 핵심 기술들, 특히 Atrous Convolution과 Spatial Pyramid Pooling은 이제 Semantic Segmentation 분야의 표준적인 기법으로 자리 잡았으며, 최신 하이브리드 아키텍처에도 그 아이디어가 녹아들어 있다.75 DeepLabV3+의 유산은 단순히 그 자체의 성능에 국한되지 않고, 후대의 고밀도 예측 네트워크 설계에 미친 심대한 영향력에 있다.</li>
</ul>
<p>결론적으로, DeepLabV3+는 순수 CNN 기반 Semantic Segmentation 아키텍처의 정점을 대표하는 모델이다. 이 모델이 보여준 한계는 역설적으로 CNN의 지역적 편향을 극복하고 전역적 문맥을 통합하려는 트랜스포머 및 하이브리드 모델의 등장을 촉진하는 계기가 되었다. 따라서 DeepLabV3+는 한 시대의 최고 성능 모델이자, 다음 시대로의 전환을 이끈 중요한 촉매제로서 컴퓨터 비전의 역사에 기록될 것이다.</p>
<h2>9. 결론</h2>
<p>본 안내서는 DeepLabV3+ 아키텍처에 대한 포괄적이고 심층적인 분석을 제공했다. Semantic Segmentation의 근본적인 도전 과제인 공간 해상도 손실과 다중 스케일 객체 처리 문제를 해결하기 위해 DeepLab 시리즈가 거쳐온 논리적인 진화 과정을 추적함으로써, DeepLabV3+가 이전 세대 모델들의 성과를 계승하고 한계를 극복하며 탄생한 필연적인 결과물임을 밝혔다.</p>
<p>DeepLabV3+의 핵심은 <strong>강력한 인코더와 효과적인 디코더의 결합</strong>에 있다. DeepLabV3를 인코더로 채택하여 Atrous Convolution과 개선된 ASPP 모듈을 통해 풍부한 다중 스케일 의미론적 정보를 추출하고, 여기에 저수준 특징을 정교하게 융합하는 간단한 디코더를 추가함으로써 객체 경계의 정밀도를 획기적으로 향상시켰다. 이 과정에서 저수준 특징의 채널 수를 1x1 컨볼루션으로 조절하여 의미론적 정보와 공간적 정보 간의 균형을 맞추는 설계는, 고성능 융합 기반 네트워크 구축에 대한 중요한 원칙을 제시했다.</p>
<p>또한, <strong>Depthwise Separable Convolution의 전면적인 도입</strong>은 DeepLabV3+의 또 다른 중요한 혁신이다. 수정된 Xception 백본과 Atrous Separable Convolution의 결합을 통해, 모델은 연산 효율성을 극대화하면서도 SOTA 수준의 정확도를 달성했다. 이는 분해된 컨볼루션(factorized convolution)이 단순한 경량화 기법을 넘어 최상위 성능 모델의 핵심 구성 요소가 될 수 있음을 입증한 사례로, 이후 효율적인 네트워크 설계에 큰 영향을 미쳤다.</p>
<p>PASCAL VOC 2012와 Cityscapes 벤치마크에서의 SOTA 성능 기록은 이러한 아키텍처 설계의 우수성을 실험적으로 증명했다. 나아가 자율 주행, 의료 영상 등 다양한 응용 분야에서 성공적으로 적용되고, 실시간 처리를 위해 경량 백본으로 유연하게 변형되는 모습은 DeepLabV3+가 단일 모델을 넘어 하나의 강력한 ’프레임워크’로서 기능함을 보여주었다.</p>
<p>물론 DeepLabV3+ 역시 완벽하지 않으며, 경계 처리나 작은 객체 분할 등에서 내재적 한계를 가진다. 이러한 한계를 보완하기 위해 어텐션 메커니즘을 통합하는 등 수많은 후속 연구가 이어졌으며, 이는 기존의 강력한 프레임워크를 점진적으로 개선해 나가는 실용적인 연구 개발의 방향성을 제시했다.</p>
<p>현재 Semantic Segmentation 분야는 트랜스포머의 등장과 함께 CNN의 지역적 강점과 트랜스포머의 전역적 강점을 결합하는 하이브리드 아키텍처 시대로 접어들고 있다. 이러한 변화의 흐름 속에서 DeepLabV3+는 순수 CNN 기반 접근법이 도달할 수 있었던 기술적 정점을 상징하는 동시에, 그 한계를 통해 새로운 패러다임의 필요성을 역설하는 중요한 이정표로 자리매김하고 있다. DeepLab 시리즈가 개척한 Atrous Convolution과 ASPP와 같은 핵심 아이디어들은 여전히 현대 아키텍처의 중요한 구성 요소로 남아 있으며, DeepLabV3+는 앞으로도 오랫동안 Semantic Segmentation 분야의 중요한 기준점이자 기술적 유산으로 평가될 것이다.</p>
<h2>10. 참고 자료</h2>
<ol>
<li>What Is Semantic Segmentation? - IBM, https://www.ibm.com/think/topics/semantic-segmentation</li>
<li>What Is Semantic Segmentation? How It Works - Roboflow Blog, https://blog.roboflow.com/what-is-semantic-segmentation/</li>
<li>Semantic Segmentation Explained | Meaning, CCN and Semantics - Quantanite, https://www.quantanite.com/blog/semantic-segmentation-explained-meaning-ccn-and-semantics/</li>
<li>What is Semantic Segmentation? | 2025 Guide, https://www.gdsonline.tech/what-is-semantic-segmentation/</li>
<li>DeepLabv3 &amp; DeepLabv3+ The Ultimate PyTorch Guide - LearnOpenCV, https://learnopencv.com/deeplabv3-ultimate-guide/</li>
<li>How DeepLabV3 Works | ArcGIS API for Python - Esri Developer, https://developers.arcgis.com/python/latest/guide/how-deeplabv3-works/</li>
<li>DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs - Johns Hopkins Computer Science, https://www.cs.jhu.edu/~ayuille/JHUcourses/VisionAsBayesianInference2022/9/DeepLabJayChen.pdf</li>
<li>Deeplab series : Semantic image segmentation - GeeksforGeeks, https://www.geeksforgeeks.org/computer-vision/deeplab-series-semantic-image-segmentation/</li>
<li>DeepLabV3 Guide: Key to Image Segmentation - Ikomia, https://www.ikomia.ai/blog/understanding-deeplabv3-image-segmentation</li>
<li>[1802.02611] Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation - arXiv, https://arxiv.org/abs/1802.02611</li>
<li>arXiv:1802.02611v3 [cs.CV] 22 Aug 2018, http://arxiv.org/pdf/1802.02611</li>
<li>What Is Image Segmentation? | IBM, https://www.ibm.com/think/topics/image-segmentation</li>
<li>Witnessing the Progression in Semantic Segmentation: DeepLab Series from V1 to V3+, https://medium.com/data-science/witnessing-the-progression-in-semantic-segmentation-deeplab-series-from-v1-to-v3-4f1dd0899e6e</li>
<li>[1606.00915] DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs - arXiv, https://arxiv.org/abs/1606.00915</li>
<li>(ICLR 2015) Semantic image segmentation with deep convolutional nets and fully connected CRFs | Alex Hex, <a href="https://alexhex7.github.io/2019/11/16/Semantic%20image%20segmentation%20with%20deep%20convolutional%20nets%20and%20fully%20connected%20CRFs/">https://alexhex7.github.io/2019/11/16/Semantic%20image%20segmentation%20with%20deep%20convolutional%20nets%20and%20fully%20connected%20CRFs/</a></li>
<li>[1412.7062] Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs - arXiv, https://arxiv.org/abs/1412.7062</li>
<li>open-cv/deeplab-v2 - GitHub, https://github.com/open-cv/deeplab-v2</li>
<li>A Guide to Using DeepLabV3 for Semantic Segmentation | Datature Blog, https://datature.io/blog/a-guide-to-using-deeplabv3-for-semantic-segmentation</li>
<li>Rethinking Atrous Convolution for Semantic Image Segmentation, https://arxiv.org/abs/1706.05587</li>
<li>Review: DeepLabv3 — Atrous Convolution (Semantic Segmentation) - Medium, https://medium.com/data-science/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74</li>
<li>Atrous convolution for generating CAM - Kaggle, https://www.kaggle.com/code/mehanat96/atrous-convolution-for-generating-cam</li>
<li>Depthwise Separable Convolutions in PyTorch - Marc Päpper, https://www.paepper.com/blog/posts/depthwise-separable-convolutions-in-pytorch/</li>
<li>Fundamental doubts regarding Depthwise Separable Convolutions : r/computervision, https://www.reddit.com/r/computervision/comments/d25m9o/fundamental_doubts_regarding_depthwise_separable/</li>
<li>Xception: Deep Learning’s Leap Beyond Inception | by Dong-Keon Kim - Medium, https://medium.com/@kdk199604/xception-deep-learnings-leap-beyond-inception-05a708c205f9</li>
<li>Depth wise Separable Convolutional Neural Networks - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/depth-wise-separable-convolutional-neural-networks/</li>
<li>Depthwise separable convolutions for machine learning - Eli Bendersky’s website, https://eli.thegreenplace.net/2018/depthwise-separable-convolutions-for-machine-learning/</li>
<li>Xception - GeeksforGeeks, https://www.geeksforgeeks.org/computer-vision/xception/</li>
<li>PascalVOC <code>trainaug</code> and <code>test</code> dataset - PyTorch Forums, https://discuss.pytorch.org/t/pascalvoc-trainaug-and-test-dataset/143705</li>
<li>PASCAL VOC 2012 DATASET - Kaggle, https://www.kaggle.com/datasets/gopalbhattrai/pascal-voc-2012-dataset</li>
<li>
<ol>
<li>The results on the PASCAL VOC 2012 test set. They are shown in Tab. 1. With DeepLabv3 and DeepLabv3+, the, https://proceedings.neurips.cc/paper/2019/file/a67c8c9a961b4182688768dd9ba015fe-AuthorFeedback.pdf</li>
</ol>
</li>
<li>Visual results on PASCAL VOC 2012. From 1st to 4th column are images - ResearchGate, https://www.researchgate.net/figure/sual-results-on-PASCAL-VOC-2012-From-1st-to-4th-column-are-images-ground-truth-results_fig1_342929572</li>
<li>Cityscapes test Benchmark (Real-Time Semantic Segmentation) - Papers With Code, https://paperswithcode.com/sota/real-time-semantic-segmentation-on-cityscapes</li>
<li>DeepLabv3+ Semantic Segmentation Algorithm for Autonomous Driving Incorporating SE-FPN - ResearchGate, https://www.researchgate.net/publication/393623803_DeepLabv3_Semantic_Segmentation_Algorithm_for_Autonomous_Driving_Incorporating_SE-FPN</li>
<li>Advances in Deep Learning for Autonomous Vehicle Perception: A Comprehensive Review - BonViewPress, https://ojs.bonviewpress.com/index.php/JCCE/article/download/5836/1495</li>
<li>A Semantic Segmentation Method for Road Scene Images Based on Improved DeeplabV3+ Network - The Science and Information (SAI) Organization, https://thesai.org/Downloads/Volume15No8/Paper_83-A_Semantic_Segmentation_Method_for_Road_Scene_Images.pdf</li>
<li>Deep Lab V3 | PDF | Image Segmentation | Deep Learning - Scribd, https://www.scribd.com/document/693331455/DeepLabV3</li>
<li>An Improved Deeplabv3+ Model for Semantic Segmentation of Urban Environments Targeting Autonomous Driving | INTERNATIONAL JOURNAL OF COMPUTERS COMMUNICATIONS &amp; CONTROL, https://univagora.ro/jour/index.php/ijccc/article/view/5879</li>
<li>Enhancing Suburban Lane Detection Through Improved DeepLabV3+ Semantic Segmentation - MDPI, https://www.mdpi.com/2079-9292/14/14/2865</li>
<li>Highlighting the Advanced Capabilities and the Computational Efficiency of DeepLabV3+ in Medical Image Segmentation: An Ablation Study - MDPI, https://www.mdpi.com/2673-7426/5/1/10</li>
<li>The DeepLabV3+ Algorithm Combined With the ResNeXt Network for Medical Image Segmentation - ResearchGate, https://www.researchgate.net/publication/389047191_The_DeepLabV3_Algorithm_Combined_With_the_ResNeXt_Network_for_Medical_Image_Segmentation</li>
<li>Segmentation of Brain Tumors Using DeepLabv3+ - Semantic Scholar, https://www.semanticscholar.org/paper/Segmentation-of-Brain-Tumors-Using-DeepLabv3%2B-Choudhury-Vanguri/877aa1ab656e586529e9ea8f9978f987a5f02f62</li>
<li>A modified DeepLabV3+ based semantic segmentation of chest computed tomography images for COVID‐19 lung infections - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC9349869/</li>
<li>A modified DeepLabV3+ based semantic segmentation of chest computed tomography images for COVID-19 lung infections - PubMed, https://pubmed.ncbi.nlm.nih.gov/35941930/</li>
<li>DeepLabv3 + method for detecting and segmenting apical lesions on panoramic radiography - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC11785705/</li>
<li>Too Much Memory Issue with Semantic Image Segmentation NN (DeepLabV3+) - Stack Overflow, https://stackoverflow.com/questions/54805568/too-much-memory-issue-with-semantic-image-segmentation-nn-deeplabv3</li>
<li>(PDF) MFA-Deeplabv3+: an improved lightweight semantic …, https://www.researchgate.net/publication/394983975_MFA-Deeplabv3_an_improved_lightweight_semantic_segmentation_algorithm_based_on_Deeplabv3</li>
<li>Semantic Segmentation Using DeepLabv3+ Model for Fabric Defect Detection, https://wujns.edpsciences.org/articles/wujns/full_html/2022/06/wujns-1007-1202-2022-06-0539-11/wujns-1007-1202-2022-06-0539-11.html</li>
<li>Improved DeepLabV3+ Network Beacon Spot Capture Methods - MDPI, https://www.mdpi.com/2304-6732/11/5/451</li>
<li>Real-Time Image Semantic Segmentation Based on Improved DeepLabv3+ Network - MDPI, https://www.mdpi.com/2504-2289/9/6/152</li>
<li>Improved DeepLabV3+ for UAV-Based Highway Lane Line Segmentation - MDPI, https://www.mdpi.com/2071-1050/17/16/7317</li>
<li>Unified DeepLabV3+ for Semi-Dark Image Semantic Segmentation - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC9324997/</li>
<li>An improved Deeplabv3+ semantic segmentation algorithm with multiple loss constraints - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC8769336/</li>
<li>Deeplabv3+ semantic segmentation model based on feature cross attention mechanism, https://www.researchgate.net/publication/347237574_Deeplabv3_semantic_segmentation_model_based_on_feature_cross_attention_mechanism</li>
<li>Attention Deeplabv3+: Multi-level Context Attention Mechanism for Skin Lesion Segmentation - OpenReview, https://openreview.net/pdf?id=oMQOHIS1JWy</li>
<li>Improving the Deeplabv3+ Model with Attention Mechanisms … - MDPI, https://www.mdpi.com/2227-7390/10/15/2597</li>
<li>An Improved SAR Image Semantic Segmentation Deeplabv3+ Network Based on the Feature Post-Processing Module - MDPI, https://www.mdpi.com/2072-4292/15/8/2153</li>
<li>Automated Road Extraction from Satellite Imagery Integrating Dense Depthwise Dilated Separable Spatial Pyramid Pooling with DeepLabV3+ - MDPI, https://www.mdpi.com/2076-3417/15/3/1027</li>
<li>Enhancing DeepLabV3+ to Fuse Aerial and Satellite Images for Semantic Segmentation This project has been funded by the Ministry of Europe and Foreign Affairs (MEAE), the Ministry of Higher Education, Research (MESR) and the Ministry of Higher Education, Scientific Research and Innovation (MESRSI), under the framework of the Franco-Moroccan bilateral program - arXiv, https://arxiv.org/html/2503.22909v1</li>
<li>Accuracy Improvement of Cell Image Segmentation Using Feedback Former - arXiv, https://arxiv.org/html/2408.12974v2</li>
<li>CTH-Net: A CNN and Transformer hybrid network for skin lesion segmentation - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC10957498/</li>
<li>Surg-SegFormer: A Dual Transformer-Based Model for Holistic Surgical Scene Segmentation - arXiv, https://arxiv.org/html/2507.04304</li>
<li>Image Segmentation with transformers: An Overview, Challenges and Future - arXiv, https://www.arxiv.org/pdf/2501.09372</li>
<li>Navigating Efficiency of MobileViT through Gaussian Process on Global Architecture Factors, https://arxiv.org/html/2406.04820v1</li>
<li>Comparing the Decision-Making Mechanisms by Transformers and CNNs via Explanation Methods - arXiv, https://arxiv.org/html/2212.06872v4</li>
<li>EHCTNet: Enhanced Hybrid of CNN and Transformer Network for Remote Sensing Image Change Detection - arXiv, https://arxiv.org/html/2501.01238v1</li>
<li>SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers - arXiv, http://arxiv.org/pdf/2105.15203</li>
<li>Masked-Attention Mask Transformer for Universal Image Segmentation - CVPR 2022 Open Access Repository - The Computer Vision Foundation, https://openaccess.thecvf.com/content/CVPR2022/html/Cheng_Masked-Attention_Mask_Transformer_for_Universal_Image_Segmentation_CVPR_2022_paper.html</li>
<li>PEM: Prototype-based Efficient MaskFormer for Image Segmentation - arXiv, https://arxiv.org/html/2402.19422v1</li>
<li>BEFUnet: A Hybrid CNN-Transformer Architecture for Precise Medical Image Segmentation, https://arxiv.org/html/2402.08793v1</li>
<li>TransUNet: Transformers Make Strong Encoders for Medical Image …, https://www.cs.jhu.edu/~alanlab/Pubs21/chen2021transunet.pdf</li>
<li>TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation, https://www.researchgate.net/publication/349125041_TransUNet_Transformers_Make_Strong_Encoders_for_Medical_Image_Segmentation</li>
<li>CMT: Convolutional Neural Networks Meet … - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_CMT_Convolutional_Neural_Networks_Meet_Vision_Transformers_CVPR_2022_paper.pdf</li>
<li>CMT: Convolutional Neural Networks Meet Vision Transformers | Request PDF, https://www.researchgate.net/publication/353233884_CMT_Convolutional_Neural_Networks_Meet_Vision_Transformers</li>
<li>MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer - arXiv, https://arxiv.org/abs/2110.02178</li>
<li>Transformer and CNN Hybrid Deep Neural Network for Semantic Segmentation of Very-High-Resolution Remote Sensing Imagery - OpenRS, http://openrs.whu.edu.cn/md/papers/2022/2022-TGRS-Transformer_and_CNN_Hybrid_Deep_Neural_Network_for_Semantic_Segmentation_of_Very-High-Resolution_Remote_Sensing_Imagery.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>