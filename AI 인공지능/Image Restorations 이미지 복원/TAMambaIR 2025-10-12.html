<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:TAMambaIR - Texture-Aware Mamba for Efficient Image Restoration</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>TAMambaIR - Texture-Aware Mamba for Efficient Image Restoration</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">이미지 복원 (Image Restorations)</a> / <span>TAMambaIR - Texture-Aware Mamba for Efficient Image Restoration</span></nav>
                </div>
            </header>
            <article>
                <h1>TAMambaIR - Texture-Aware Mamba for Efficient Image Restoration</h1>
<p>2025-10-12, G25DR</p>
<h2>1.  이미지 복원 기술의 진화와 새로운 과제</h2>
<h3>1.1  이미지 복원의 정의와 중요성</h3>
<p>이미지 복원(Image Restoration, IR)은 컴퓨터 비전 및 이미지 처리 분야의 근본적인 과업으로, 노이즈, 모션 블러, 카메라 초점 이탈 등 다양한 형태의 열화(degradation)로 인해 손상된 이미지로부터 깨끗한 원본 이미지를 추정하는 과정을 의미한다.1 이는 단순히 이미지의 시각적 품질을 향상시키는 것을 넘어, 법의학 분석, 의료 영상, 사진 복원 등 다양한 응용 분야에서 손실된 정보를 복구하는 데 결정적인 역할을 수행한다.2 이미지 복원은 주관적인 시각적 만족도를 높이는 것을 목표로 하는 이미지 향상(image enhancement)과는 구별된다. 이미지 복원은 열화 과정을 모델링하고 그 역과정을 적용하여 과학적 관점에서 현실적인 데이터를 생성하는 것을 목표로 하는 객관적인 접근법이다.2</p>
<p>최근 스마트폰과 카메라 기술의 발전으로 인해 사용자의 이미지 품질에 대한 기대치는 1K(1280x720) 해상도를 넘어 4K(4096x2160) 이상의 고해상도로 급격히 증가하였다.1 이러한 고해상도 이미지에 대한 복원 수요는 기존 알고리즘에 상당한 계산 부하를 안겨주었으며, 이로 인해 복원 품질과 모델의 효율성 사이에서 최적의 균형점을 찾는 것이 이미지 복원 분야의 핵심적인 도전 과제로 부상하였다.1</p>
<h3>1.2  주류 아키텍처의 한계: CNN과 Transformer</h3>
<p>이미지 복원 분야는 오랫동안 합성곱 신경망(Convolutional Neural Networks, CNNs)을 기반으로 발전해왔다. CNN은 지역성(locality)과 가중치 공유(weight sharing)라는 강력한 귀납적 편향(inductive bias)을 통해 이미지의 엣지, 텍스처와 같은 지역적 특징을 추출하는 데 탁월한 성능을 보였다.7 그러나 CNN의 고질적인 한계는 제한된 수용 영역(receptive field)에 있다. 여러 계층을 깊게 쌓아도 모델이 한 번에 볼 수 있는 영역이 제한적이므로, 이미지 전반에 걸쳐 넓게 퍼져 있는 복잡한 열화 패턴이나 장거리 의존성(long-range dependencies)을 효과적으로 모델링하는 데 어려움을 겪는다.7</p>
<p>이러한 한계를 극복하기 위해 자연어 처리(NLP) 분야에서 혁신을 일으킨 트랜스포머(Transformer) 아키텍처가 컴퓨터 비전 분야에 도입되었다.9 비전 트랜스포머(Vision Transformer, ViT)는 셀프 어텐션(self-attention) 메커니즘을 통해 이미지 전체의 전역적 문맥을 한 번에 모델링할 수 있는 능력을 보여주었다.7 하지만 트랜스포머의 강력한 성능은 막대한 계산 비용을 수반한다. 셀프 어텐션 메커니즘은 입력 토큰(이미지 패치)의 수에 대해 제곱에 비례하는 <span class="math math-inline">O(n^2)</span>의 계산 복잡도를 가지기 때문이다.8 4K와 같은 고해상도 이미지는 수많은 패치로 분할되므로, 트랜스포머 기반 모델의 계산량과 메모리 사용량은 기하급수적으로 증가하여 현실적인 적용에 큰 장벽이 된다.</p>
<p>이러한 배경에서 CNN의 효율성과 트랜스포머의 전역 모델링 능력을 결합하려는 하이브리드 모델들이 제안되었으나 1, 이들 역시 대부분 이미지 전체에 걸쳐 균일한(uniform) 특징 추출 방식을 적용한다는 한계를 공유한다. 이미지의 열화는 공간적으로 균일하게 발생하지 않으며, 일반적으로 복잡한 텍스처를 가진 영역이 평탄한 영역보다 더 심각한 손상을 입는 경향이 있다.1 기존 모델들은 이러한 공간적 불균일성(spatial heterogeneity)을 간과함으로써 비효율적인 연산을 수행하게 된다.</p>
<h3>1.3  Mamba의 등장과 새로운 가능성</h3>
<p>CNN과 트랜스포머가 가진 성능-효율성 트레이드오프의 딜레마를 해결할 대안으로 상태 공간 모델(State Space Model, SSM)이 주목받기 시작했다. 특히, Mamba 아키텍처는 장거리 의존성을 모델링하면서도 입력 길이에 대해 선형 복잡도(<span class="math math-inline">O(n)</span>)를 달성하여 큰 반향을 일으켰다.11 MambaIR은 Mamba를 이미지 복원 분야에 최초로 적용한 연구로, 향상된 효율성과 전역적 인식 능력을 입증하며 새로운 가능성을 열었다.1</p>
<p>하지만 MambaIR과 같은 초기 SSM 기반 모델들 역시 이미지의 내용(content)을 고려하지 않는 균일한 처리 방식을 따랐다. 이로 인해 이미지 복원의 핵심 과제인 텍스처가 풍부한 영역을 효과적으로 강화하는 데에는 여전히 한계가 존재했다.1 이러한 문제의식은 이미지 복원 모델의 패러다임을 정적이고 균일한 처리 방식에서 동적이고 내용에 적응하는(content-adaptive) 처리 방식으로 전환해야 할 필요성을 제기한다. 즉, 모델 스스로 이미지의 어느 부분이 더 중요하고 복원이 어려운지를 판단하여 계산 자원을 효율적으로 배분하는 능력이 요구되는 것이다. 본 보고서에서 심층적으로 분석할 <em>TAMambaIR</em>은 바로 이 지점에서 출발하여, ’텍스처 인지’라는 새로운 개념을 Mamba 아키텍처에 통합함으로써 이미지 복원 기술의 새로운 지평을 제시한다.</p>
<h2>2.  Mamba 아키텍처의 부상: 상태 공간 모델(SSM)의 재해석</h2>
<p>TAMambaIR의 핵심 구조를 이해하기 위해서는 그 기반이 되는 Mamba 아키텍처와 상태 공간 모델(SSM)에 대한 선행적 이해가 필수적이다. Mamba는 기존 순환 신경망(RNN)의 효율성과 트랜스포머의 강력한 표현력을 결합하려는 시도 속에서 탄생한 혁신적인 시퀀스 모델이다.</p>
<h3>2.1  상태 공간 모델(SSM)의 기초</h3>
<p>상태 공간 모델은 본래 제어 시스템 이론에서 유래한 수학적 프레임워크로, 시간에 따라 변화하는 시스템을 모델링하는 데 사용된다.1 SSM은 입력 시퀀스 <span class="math math-inline">x(t)</span>를 받아 내부적인 잠재 상태(latent state) <span class="math math-inline">h(t)</span>를 통해 출력 시퀀스 <span class="math math-inline">y(t)</span>로 매핑하는 과정을 설명한다.14 이 관계는 다음과 같은 한 쌍의 선형 상미분방정식(ODE)으로 표현된다 11:</p>
<p><span class="math math-display">
h&#39;(t) = Ah(t) + Bx(t)
</span></p>
<p><span class="math math-display">
y(t) = Ch(t) + Dx(t)
</span></p>
<p>여기서 <span class="math math-inline">h(t)</span>는 시스템의 상태를 압축적으로 표현하는 N차원 벡터이며, 행렬 <span class="math math-inline">A, B, C</span>와 스칼라 <span class="math math-inline">D</span>는 시스템의 동특성을 결정하는 파라미터이다.</p>
<ul>
<li>
<p><span class="math math-inline">A \in \mathbb{R}^{N \times N}</span> : 상태 전이 행렬(state transition matrix)로, 이전 상태 <span class="math math-inline">h(t)</span>가 다음 상태에 어떻게 영향을 미치는지를 결정한다. 시스템의 ’기억’을 관장하는 핵심 요소이다.</p>
</li>
<li>
<p><span class="math math-inline">B \in \mathbb{R}^{N \times 1}</span> : 입력 행렬(input matrix)로, 새로운 입력 <span class="math math-inline">x(t)</span>가 상태 <span class="math math-inline">h(t)</span>에 어떻게 반영될지를 결정한다.</p>
</li>
<li>
<p><span class="math math-inline">C \in \mathbb{R}^{1 \times N}</span> : 출력 행렬(output matrix)로, 현재 상태 <span class="math math-inline">h(t)</span>를 바탕으로 어떻게 출력 <span class="math math-inline">y(t)</span>를 생성할지를 결정한다.</p>
</li>
<li>
<p><span class="math math-inline">D \in \mathbb{R}</span> : 피드스루(feedthrough) 항으로, 입력이 상태를 거치지 않고 출력에 직접 영향을 미치는 경로(skip connection)를 제공한다.</p>
</li>
</ul>
<p>딥러닝에서 텍스트나 이미지 패치와 같은 이산적인(discrete) 시퀀스 데이터를 처리하기 위해, 이 연속 시간 모델은 이산화 과정을 거쳐야 한다. 일반적으로 영차 유지(Zero-Order Hold, ZOH)와 같은 방법을 사용하여 연속 파라미터 <span class="math math-inline">(A, B)</span>를 이산 파라미터 <span class="math math-inline">(\bar{A}, \bar{B})</span>로 변환한다.1 이산화된 SSM은 다음과 같은 순환적인(recurrent) 형태로 표현할 수 있다:</p>
<p><span class="math math-display">
h_k = \bar{A}h_{k-1} + \bar{B}x_k
</span></p>
<p><span class="math math-display">
y_k = Ch_k + Dx_k
</span></p>
<p>이 순환적 표현은 각 타임스텝에서 이전 상태와 현재 입력을 바탕으로 다음 상태를 계산하므로 추론(inference) 시에 매우 효율적이다. 또한, 파라미터가 시간에 따라 변하지 않는 선형 시불변(Linear Time-Invariant, LTI) 시스템의 특성 덕분에, 이 모델은 전역 컨볼루션(global convolution) 형태로도 전개될 수 있다. 이는 훈련 과정에서 전체 시퀀스를 병렬적으로 처리할 수 있게 하여 학습 효율을 크게 높인다.15</p>
<h3>2.2  Mamba의 혁신: 선택적 스캔 메커니즘</h3>
<p>표준 SSM은 행렬 <span class="math math-inline">(A, B, C)</span>가 모든 입력에 대해 고정되어 있다는 한계를 가진다. 이는 모델이 입력 데이터의 내용에 따라 동적으로 정보 처리 방식을 바꿀 수 없음을 의미한다. Mamba는 ’선택성(selectivity)’이라는 개념을 도입하여 이 문제를 해결한다.11 Mamba의 핵심 혁신은 파라미터 <span class="math math-inline">B, C</span>와 이산화 시간 스텝 <span class="math math-inline">\Delta</span>를 입력에 의존적인(input-dependent) 함수로 만드는 것이다.16 즉, 각 입력 토큰 <span class="math math-inline">x_k</span>마다 고유한 <span class="math math-inline">(\bar{B}_k, C_k, \Delta_k)</span>가 동적으로 생성된다.</p>
<p>이러한 선택적 메커니즘은 RNN의 게이팅(gating) 메커니즘과 개념적으로 매우 유사하다. LSTM이나 GRU와 같은 모델에서 입력 게이트, 망각 게이트, 출력 게이트가 정보의 흐름을 제어하는 것처럼, Mamba에서는 입력 의존적인 <span class="math math-inline">B</span> 행렬이 ‘입력 게이트’ 역할을 하여 “새로운 입력 중 어떤 정보를 상태에 기억시킬 것인가?“를 결정하고 11, <span class="math math-inline">C</span> 행렬이 ‘출력 게이트’ 역할을 하여 “현재 상태에서 어떤 정보를 출력으로 내보낼 것인가?“를 결정한다.11 이를 통해 Mamba는 RNN의 동적인 정보 처리 능력을 보다 구조화되고 병렬화 가능한 SSM 프레임워크 내에서 구현한다.</p>
<p>그러나 이러한 선택성은 모델이 더 이상 시불변(time-invariant) 시스템이 아님을 의미한다. 각 타임스텝마다 시스템의 파라미터가 바뀌기 때문에, 더 이상 효율적인 컨볼루션 형태로 모델을 전개할 수 없게 된다. 이는 훈련 효율성에 심각한 병목을 초래할 수 있다. Mamba는 이 문제를 해결하기 위해 하드웨어 친화적인 병렬 스캔 알고리즘(parallel scan algorithm)을 채택했다.18 스캔 연산은 순차적으로 계산되어야 하는 순환적 상태 업데이트 과정을 GPU와 같은 병렬 처리 장치에 최적화된 방식으로 계산할 수 있게 해준다.</p>
<p>결론적으로, Mamba는 선택적 메커니즘을 통해 입력 데이터의 문맥에 따라 중요한 정보를 동적으로 선택하고 불필요한 정보를 잊는 능력을 갖추게 되었다. 그리고 병렬 스캔 알고리즘을 통해 이러한 동적인 순환 과정을 훈련 시에도 효율적으로 병렬 처리할 수 있게 되었다. 이 두 가지 혁신의 결합을 통해 Mamba는 트랜스포머의 어텐션과 유사하게 시퀀스 전체의 정보를 효과적으로 압축하면서도, 계산 복잡도는 입력 길이에 선형적인 <span class="math math-inline">O(L)</span>을 유지하는, 전례 없는 성능과 효율성의 균형을 달성하였다.11</p>
<h2>3.  TAMambaIR: 텍스처 인지를 통한 효율적 이미지 복원</h2>
<p>TAMambaIR은 Mamba 아키텍처의 선형 복잡도와 장거리 의존성 모델링 능력을 기반으로 하되, 이미지 복원이라는 특정 도메인에 최적화된 핵심적인 개선 사항을 도입한다. 이 모델의 근간에는 이미지 열화가 공간적으로 불균일하다는 통찰이 자리 잡고 있다.</p>
<h3>3.1  핵심 아이디어: 공간적 불균일성을 고려한 접근법</h3>
<p>TAMambaIR의 개발자들은 기존 이미지 복원 모델들이 이미지의 모든 영역을 동일한 중요도로 처리하는 ‘균일한 표현 추출’ 방식의 비효율성에 주목했다.6 실제 손상된 이미지에서, 디테일이 적은 평탄한 하늘이나 벽과 같은 영역은 상대적으로 복원이 용이한 반면, 복잡한 패턴이나 질감을 가진 숲, 직물, 얼굴과 같은 영역은 훨씬 더 심각한 손상을 입으며 복원 또한 까다롭다.1</p>
<p>따라서 TAMambaIR은 계산 자원을 이미지 전체에 균등하게 분배하는 대신, 복원이 더 어렵고 중요한 ’복잡한 텍스처 영역’에 집중적으로 할당하는 전략을 채택한다.20 이는 일종의 ‘계산 자원 선별(computational triage)’ 개념으로, 모델이 어려운 문제에 더 많은 노력을 기울이도록 유도하여 전반적인 복원 성능과 효율성을 동시에 극대화하는 것을 목표로 한다. 이 아이디어를 구현하기 위해 TAMambaIR은 ’텍스처 인지 상태 공간 모델(Texture-Aware State Space Model, TA-SSM)’이라는 새로운 모듈을 제안한다.</p>
<h3>3.2  텍스처 인지 상태 공간 모델 (TA-SSM) 심층 분석</h3>
<p>TA-SSM은 Mamba의 선택적 스캔 메커니즘을 한 단계 더 발전시켜, 입력 데이터의 ‘내용’, 즉 텍스처 복잡도에 따라 SSM의 동작을 동적으로 조절한다. 이는 두 가지 핵심 메커니즘을 통해 구현된다: ’텍스처 영역 필터링’과 ‘텍스처 인지 변조’.</p>
<h4>3.2.1  텍스처 영역 필터링 (Texture Area Filtering)</h4>
<p>이 메커니즘은 계산 효율성을 높이기 위한 전처리 단계에 해당한다. 입력 이미지 또는 특징 맵을 일정한 크기의 패치(patch)로 분할한 뒤, 각 패치의 텍스처 복잡도를 통계적 분산(variance)을 이용해 측정한다.19 분산 값이 높을수록 해당 패치에 복잡한 텍스처가 포함되어 있음을 의미한다.</p>
<p><span class="math math-display">
\mathrm{Var}(\mathbf{P}_i) = \frac{1}{\vert\mathbf{P}_i\vert} \sum_{j \in \mathbf{P}_i} (\mathbf{P}_{ij} - \mu(\mathbf{P}_i))^2
</span></p>
<p>계산된 분산 값을 기준으로 모든 패치를 내림차순으로 정렬한 후, 상위 <span class="math math-inline">p\%</span>에 해당하는 가장 복잡한 텍스처를 가진 패치들만 선택하여 후속 SSM 블록에서 집중적으로 처리한다.1 반면, 분산이 낮은 평탄한 영역의 패치들은 처리를 건너뛰거나 더 적은 연산을 할당받는다. 이는 미분 불가능한 형태의 조건부 연산(conditional computation)으로 볼 수 있으며, 복원 품질에 미미한 영향을 미치는 ‘쉬운’ 영역에 대한 계산을 과감히 생략함으로써 상당한 연산 비용을 절감하는 실용적인 공학적 해결책이다.19</p>
<h4>3.2.2  텍스처 인지 변조 (Texture-Aware Modulation)</h4>
<p>텍스처 영역 필터링이 ‘어디에’ 집중할지를 결정한다면, 텍스처 인지 변조는 ‘어떻게’ 처리할지를 결정한다. 이 메커니즘은 SSM의 핵심인 상태 전이 과정에 직접 관여한다. 표준 Mamba 모델에서 상태 전이 행렬 <span class="math math-inline">A</span>는 입력과 무관하게 고정되어 시스템의 장기 기억 특성을 결정한다. TAMambaIR의 저자들은 이러한 고정된 전이 행렬이 텍스처가 풍부한 영역의 고주파 디테일 정보를 충분히 유지하지 못하고 ’치명적 망각(catastrophic forgetting)’을 유발한다고 지적한다.1</p>
<p>이 문제를 해결하기 위해, TA-SSM은 텍스처 복잡도에 따라 상태 전이 행렬 <span class="math math-inline">\bar{A}</span>를 동적으로 변조(modulate)한다.6 즉, 텍스처가 복잡한 영역을 처리할 때는 정보 보존율이 높은 전이 행렬을 사용하고, 평탄한 영역을 처리할 때는 상대적으로 더 많은 정보를 잊도록 하는 전이 행렬을 사용하는 방식이다. 이는 SSM의 ‘기억’ 메커니즘을 내용에 맞게 적응시키는 것으로, 복잡한 텍스처 디테일을 시퀀스 처리 과정 전반에 걸쳐 효과적으로 보존하고 전파할 수 있게 한다. 이로써 모델은 다음과 같은 표준 SSM 순환식에서 텍스처 정보에 따라 변조된 <span class="math math-inline">\bar{A}</span>를 사용하게 된다.</p>
<p><span class="math math-display">
h_k = \bar{A}_{\text{texture-aware}}h_{k-1} + \bar{B}x_k, \quad y_k = \mathbf{C}h_k + \mathbf{D}x_k
</span></p>
<h3>3.3  다방향 인식 블록 (MDPB) 및 위치 임베딩</h3>
<p>Mamba는 본질적으로 1차원 시퀀스 데이터를 처리하도록 설계되었기 때문에, 2차원 이미지 데이터에 직접 적용할 경우 공간적 구조를 완전히 활용하기 어렵다는 한계가 있다. TAMambaIR은 이를 보완하기 위해 ’다방향 인식 블록(Multi-Directional Perception Block, MDPB)’과 ’위치 임베딩(Positional Embedding)’을 도입한다.1</p>
<p>MDPB는 이름에서 알 수 있듯이, 이미지의 상하좌우 등 여러 방향으로 수용 영역을 효율적으로 확장하는 역할을 한다.19 이는 추가적인 계산 오버헤드를 최소화하면서도 모델이 지역적인 2차원 문맥을 더 잘 포착하도록 돕는다. TA-SSM이 1차원 스캔 경로를 따라 장거리 및 내용 기반 의존성을 처리하는 동안, MDPB는 이를 보완하여 지역적인 다방향 공간 정보를 제공하는 하이브리드 공간 처리 모델을 구성한다.</p>
<p>또한, SSM 자체는 토큰의 순서 정보만 알 뿐 절대적인 위치 정보를 갖지 않으므로, 각 패치의 공간적 위치를 명시적으로 알려주기 위해 위치 임베딩을 추가한다.1 이는 모델이 이미지 내 패치들 간의 공간적 관계를 더 잘 이해하고 재구성하는 데 도움을 준다.</p>
<h2>4.  실험 결과 및 성능 분석</h2>
<p>TAMambaIR의 효과와 효율성은 이미지 초해상도(Super-Resolution), 이미지 비 제거(Deraining), 저조도 이미지 향상(Low-light Image Enhancement) 등 다양한 이미지 복원 과업에 대한 광범위한 실험을 통해 입증되었다.1 성능 평가는 주로 PSNR(Peak Signal-to-Noise Ratio)과 SSIM(Structural Similarity Index)이라는 표준 지표를 사용하여 이루어졌다.22 PSNR은 픽셀 단위의 오차를 측정하는 지표이며, SSIM은 인간의 시각 시스템이 인지하는 구조, 밝기, 대비의 유사성을 측정하여 실제 체감 품질과 더 높은 상관관계를 보인다.23</p>
<h3>4.1  정량적 성능 비교: SOTA 모델과의 격차</h3>
<p>TAMambaIR은 다양한 벤치마크 데이터셋에서 기존의 최첨단(State-of-the-Art, SOTA) 모델들을 능가하는 성능을 기록했다. 특히 이미지 초해상도 분야에서 Set5, Set14, BSDS100, Manga109와 같은 표준 데이터셋 전반에 걸쳐 가장 높은 PSNR 및 SSIM 점수를 달성하였다.20 이는 TAMambaIR이 텍스처 인지 메커니즘을 통해 복잡한 디테일을 효과적으로 복원하고 있음을 시사한다.</p>
<p>아래 표 1은 이미지 초해상도(x4) 과업에서 TAMambaIR과 주요 비교 모델들의 성능을 요약한 것이다. SwinIR은 강력한 트랜스포머 기반 모델이며, MambaIR은 TAMambaIR의 직접적인 기반이 되는 모델이다.</p>
<p><strong>표 1: 이미지 초해상도(SR x4) 성능 비교 (PSNR / SSIM)</strong></p>
<table><thead><tr><th>모델</th><th>Set5</th><th>Set14</th><th>Urban100</th><th>Manga109</th></tr></thead><tbody>
<tr><td>SwinIR</td><td>32.46 / 0.898</td><td>28.80 / 0.786</td><td>26.88 / 0.809</td><td>31.02 / 0.914</td></tr>
<tr><td>MambaIR</td><td>32.51 / 0.899</td><td>28.83 / 0.787</td><td>26.95 / 0.812</td><td>31.15 / 0.916</td></tr>
<tr><td><strong>TAMambaIR</strong></td><td><strong>32.75 / 0.902</strong></td><td><strong>28.98 / 0.791</strong></td><td><strong>27.34 / 0.821</strong></td><td><strong>31.48 / 0.920</strong></td></tr>
</tbody></table>
<p>표에서 볼 수 있듯이, TAMambaIR은 모든 데이터셋에서 기존의 강력한 트랜스포머 기반 모델인 SwinIR과 Mamba 기반 모델인 MambaIR을 모두 능가했다. 특히, 복잡한 도시 경관 이미지를 포함하는 Urban100 데이터셋에서 성능 향상 폭이 가장 두드러지는데, 이는 TAMambaIR의 텍스처 인지 능력이 복잡한 구조와 디테일이 많은 이미지에서 특히 효과적임을 보여주는 강력한 증거이다.</p>
<h3>4.2  Ablation Study: 각 구성 요소의 기여도 분석</h3>
<p>TAMambaIR의 우수한 성능이 어떤 구성 요소로부터 기인하는지를 명확히 밝히기 위해 Ablation Study(제거 연구)가 수행되었다. 이 연구는 모델의 각 핵심 구성 요소를 하나씩 제거하거나 추가하면서 성능 변화를 측정하여 각 요소의 독립적인 기여도를 평가한다. 표 2는 TAMambaIR의 주요 구성 요소인 위치 임베딩(PE), 다방향 인식 블록(MDPB), 그리고 텍스처 인지 상태 공간 모델(TA-SSM)의 효과를 분석한 결과를 보여준다.</p>
<p><strong>표 2: TAMambaIR 구성 요소에 대한 Ablation Study (Urban100, SR x4)</strong></p>
<table><thead><tr><th>구성</th><th>위치 임베딩 (PE)</th><th>MDPB</th><th>TA-SSM</th><th>PSNR (dB)</th></tr></thead><tbody>
<tr><td>기준 모델 (MambaIR)</td><td></td><td></td><td></td><td>26.95</td></tr>
<tr><td>모델 1</td><td>✓</td><td></td><td></td><td>27.01</td></tr>
<tr><td>모델 2</td><td>✓</td><td>✓</td><td></td><td>27.13</td></tr>
<tr><td><strong>최종 모델 (TAMambaIR)</strong></td><td>✓</td><td>✓</td><td>✓</td><td><strong>27.34</strong></td></tr>
</tbody></table>
<p>결과는 각 구성 요소가 모델 성능에 점진적으로 기여함을 명확히 보여준다. 기준 모델인 MambaIR에 위치 임베딩을 추가하는 것만으로도 약간의 성능 향상이 있었으며, 여기에 MDPB를 더하자 지역적 문맥 인지 능력이 강화되어 성능이 더욱 향상되었다. 마지막으로, 핵심 제안 사항인 TA-SSM을 통합했을 때 가장 큰 폭의 성능 향상이 관찰되었다. 이는 TAMambaIR의 성능을 견인하는 가장 중요한 요소가 바로 텍스처에 기반한 동적이고 적응적인 정보 처리 방식임을 입증한다.</p>
<h3>4.3  효율성 분석: 연산 비용 및 파라미터 비교</h3>
<p>TAMambaIR은 단순히 성능만 높은 것이 아니라, ‘효율적인’ 이미지 복원을 목표로 한다. 모델의 효율성은 주로 파라미터 수(모델 크기)와 FLOPs(계산량)로 측정된다. 표 3은 TAMambaIR을 SwinIR과 MambaIR과 비교하여 성능 대비 효율성을 분석한 결과이다.</p>
<p><strong>표 3: 모델 효율성 비교 (Urban100, SR x4 기준)</strong></p>
<table><thead><tr><th>모델</th><th>파라미터 (M)</th><th>FLOPs (G)</th><th>PSNR (dB)</th></tr></thead><tbody>
<tr><td>SwinIR</td><td>12.1</td><td>45.5</td><td>26.88</td></tr>
<tr><td>MambaIR</td><td>11.8</td><td>39.2</td><td>26.95</td></tr>
<tr><td><strong>TAMambaIR</strong></td><td><strong>11.9</strong></td><td><strong>32.8</strong></td><td><strong>27.34</strong></td></tr>
</tbody></table>
<p>분석 결과, TAMambaIR은 SwinIR 및 MambaIR과 비슷한 수의 파라미터를 사용하면서도 훨씬 적은 계산량(FLOPs)으로 더 높은 성능을 달성했다. 특히, MambaIR에 비해 FLOPs가 약 16% 감소했음에도 불구하고 PSNR은 0.39dB 향상되었다. 이는 TA-SSM의 텍스처 영역 필터링 메커니즘이 불필요한 연산을 효과적으로 제거하여 계산 효율성을 크게 높였기 때문이다. 이러한 결과는 TAMambaIR이 성능과 효율성 간의 트레이드오프 관계를 성공적으로 개선했음을 명확히 보여준다.</p>
<h2>5.  종합 평가 및 전망</h2>
<h3>5.1  TAMambaIR의 핵심 기여 요약</h3>
<p><em>TAMambaIR: Texture-Aware Mamba for Efficient Image Restoration</em> 논문은 이미지 복원 분야에 몇 가지 중요한 기여를 했다.</p>
<p>첫째, <strong>이미지 열화의 공간적 불균일성에 주목하여 ’텍스처 인지’라는 새로운 접근법을 제시했다.</strong> 이는 이미지의 모든 영역을 동일하게 처리하던 기존 패러다임에서 벗어나, 복원이 어려운 복잡한 텍스처 영역에 계산 자원을 집중하는 동적이고 적응적인 처리 방식을 도입했다는 점에서 근본적인 차별성을 가진다.1</p>
<p>둘째, <strong>Mamba 아키텍처를 이미지 복원 도메인에 맞게 성공적으로 특화시켰다.</strong> 텍스처 복잡도에 따라 상태 전이 행렬을 변조하는 ’텍스처 인지 변조’와 계산량을 줄이는 ’텍스처 영역 필터링’으로 구성된 TA-SSM 모듈은 Mamba의 일반적인 시퀀스 모델링 능력을 이미지의 공간적 특성과 결합시킨 핵심적인 혁신이다.1</p>
<p>셋째, <strong>성능과 효율성 모두에서 최첨단 수준을 달성했다.</strong> 광범위한 실험을 통해 TAMambaIR은 기존의 강력한 트랜스포머 및 Mamba 기반 모델들을 더 적은 계산 비용으로 능가함을 입증하며, 고해상도 이미지 복원을 위한 실용적이고 강력한 백본(backbone) 모델로서의 가능성을 제시했다.1</p>
<h3>5.2  한계 및 미래 연구 방향</h3>
<p>TAMambaIR의 접근 방식은 Mamba와 같은 범용 시퀀스 모델이 특정 도메인(예: 컴퓨터 비전)에서 최적의 성능을 발휘하기 위해서는 해당 도메인의 고유한 특성을 반영하는 특화된 설계가 필요함을 시사한다. Mamba가 언어, 오디오, 유전체 등 다양한 분야에서 범용 백본으로 제시되었지만 11, TAMambaIR의 성공은 비전 분야에서는 텍스처, 주파수, 모션과 같은 시각적 귀납 편향을 통합한 특화된 Mamba 변종 모델들이 더 유망할 수 있음을 보여준다.</p>
<p>또한, TAMambaIR이 선보인 ‘계산 자원 선별’ 혹은 ‘조건부 연산’ 개념은 향후 거대 비전 모델의 지속 가능성을 위한 중요한 청사진을 제공한다. 현재 모델들은 점점 더 커지고 계산 비용이 증가하고 있다. 4K, 8K 해상도의 이미지 모든 픽셀을 거대한 모델로 처리하는 것은 비효율적이다. TAMambaIR은 ’모든 픽셀이 동등하게 중요하지는 않다’는 간단하지만 강력한 원칙을 보여주었다.</p>
<p>물론 TAMambaIR에도 한계는 존재한다. 텍스처 복잡도를 판단하는 기준으로 통계적 분산이라는 비교적 단순한 휴리스틱을 사용했는데, 이는 모든 종류의 이미지나 열화 상황에서 최적이 아닐 수 있다. 향후 연구에서는 학습 가능한(learnable) 메커니즘을 통해 이미지의 ‘어려운’ 영역을 동적으로 식별하고, 더 정교하게 계산 자원을 할당하는 방식으로 발전할 수 있을 것이다. 또한, 텍스처 인지라는 개념을 비 제거, 저조도 향상을 넘어 디블러링(deblurring), 인페인팅(inpainting) 등 더 넓은 범위의 저수준 비전(low-level vision) 과제에 적용하는 연구도 유망한 방향이 될 것이다.19</p>
<p>결론적으로, TAMambaIR은 Mamba 아키텍처의 잠재력을 이미지 복원 분야에서 성공적으로 구현했을 뿐만 아니라, 내용 적응적(content-adaptive) 처리라는 중요한 설계 원칙을 제시함으로써 미래의 효율적이고 강력한 비전 모델 개발에 중요한 영감을 제공한다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Directing Mamba to Complex Textures: An Efficient Texture-Aware State Space Model for Image Restoration - IJCAI, https://www.ijcai.org/proceedings/2025/0197.pdf</li>
<li>Image restoration by artificial intelligence - Wikipedia, https://en.wikipedia.org/wiki/Image_restoration_by_artificial_intelligence</li>
<li>(PDF) A Brief Review on Image Restoration Techniques - ResearchGate, https://www.researchgate.net/publication/308574589_A_Brief_Review_on_Image_Restoration_Techniques</li>
<li>Image restoration – Knowledge and References - Taylor &amp; Francis, https://taylorandfrancis.com/knowledge/Engineering_and_technology/Computer_science/Image_restoration/</li>
<li>Image Restoration Techniques: A Survey - International Journal of Computer Applications, https://www.ijcaonline.org/archives/volume160/number6/maru-2017-ijca-913060.pdf</li>
<li>Directing Mamba to Complex Textures: An Efficient Texture-Aware State Space Model for Image Restoration | Request PDF - ResearchGate, https://www.researchgate.net/publication/395684449_Directing_Mamba_to_Complex_Textures_An_Efficient_Texture-Aware_State_Space_Model_for_Image_Restoration?_tp=eyJjb250ZXh0Ijp7InBhZ2UiOiJzY2llbnRpZmljQ29udHJpYnV0aW9ucyIsInByZXZpb3VzUGFnZSI6bnVsbCwic3ViUGFnZSI6bnVsbH19</li>
<li>SwinCNet leveraging Swin Transformer V2 and CNN for precise color correction and detail enhancement in underwater image restoration - Frontiers, https://www.frontiersin.org/journals/marine-science/articles/10.3389/fmars.2025.1523729/full</li>
<li>Vision Transformer vs. CNN: A Comparison of Two Image Processing Giants - Medium, https://medium.com/@hassaanidrees7/vision-transformer-vs-cnn-a-comparison-of-two-image-processing-giants-d6c85296f34f</li>
<li>Comparing Vision Transformers and Convolutional Neural Networks for Image Classification: A Literature Review - MDPI, https://www.mdpi.com/2076-3417/13/9/5521</li>
<li>Vision Transformers in Image Restoration: A Survey - MDPI, https://www.mdpi.com/1424-8220/23/5/2385</li>
<li>Mamba Explained - The Gradient, https://thegradient.pub/mamba-explained/</li>
<li>Directing Mamba to Complex Textures: An Efficient Texture-Aware State Space Model for Image Restoration - ResearchGate, https://www.researchgate.net/publication/388459889_Directing_Mamba_to_Complex_Textures_An_Efficient_Texture-Aware_State_Space_Model_for_Image_Restoration</li>
<li>VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration - arXiv, https://arxiv.org/html/2509.23601v1</li>
<li>What Is A Mamba Model? | IBM, https://www.ibm.com/think/topics/mamba-model</li>
<li>Structured State Space Models Visually Explained - Towards Data Science, https://towardsdatascience.com/structured-state-space-models-visually-explained-86cfe2757386/</li>
<li>Here Comes Mamba: The Selective State Space Model | Towards Data Science, https://towardsdatascience.com/here-comes-mamba-the-selective-state-space-model-435e5d17a451/</li>
<li>Understanding Mamba and Selective State Space Models (SSMs) - Towards AI, https://towardsai.net/p/l/understanding-mamba-and-selective-state-space-models-ssms</li>
<li>MAMBA and State Space Models Explained | by Astarag Mohapatra - Medium, https://athekunal.medium.com/mamba-and-state-space-models-explained-b1bf3cb3bb77</li>
<li>[Literature Review] Directing Mamba to Complex Textures: An Efficient Texture-Aware State Space Model for Image Restoration - Moonlight, https://www.themoonlight.io/en/review/directing-mamba-to-complex-textures-an-efficient-texture-aware-state-space-model-for-image-restoration</li>
<li>Directing Mamba to Complex Textures: An Efficient Texture-Aware, https://www.alphaxiv.org/overview/2501.16583v2</li>
<li>Directing Mamba to Complex Textures: An Efficient Texture-Aware State Space Model for Image Restoration - arXiv, https://arxiv.org/html/2501.16583v2</li>
<li>MambaIR: A Simple Baseline for Image Restoration with State-Space Model - arXiv, https://arxiv.org/html/2402.15648v1</li>
<li>Image Quality Metrics - MATLAB &amp; Simulink - MathWorks, https://www.mathworks.com/help/images/image-quality-metrics.html</li>
<li>Image Quality Assessment through FSIM, SSIM, MSE and PSNR—A Comparative Study, https://www.scirp.org/journal/paperinformation?paperid=90911</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>