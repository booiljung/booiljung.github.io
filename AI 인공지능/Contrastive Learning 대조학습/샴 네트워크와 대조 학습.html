<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:샴 네트워크와 대조 학습 (유사성 기반 표현 학습)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>샴 네트워크와 대조 학습 (유사성 기반 표현 학습)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">대조 학습(Contrastive Learning)</a> / <span>샴 네트워크와 대조 학습 (유사성 기반 표현 학습)</span></nav>
                </div>
            </header>
            <article>
                <h1>샴 네트워크와 대조 학습 (유사성 기반 표현 학습)</h1>
<h2>1.  유사성 학습의 패러다임</h2>
<h3>1.1 배경: 표현 학습(Representation Learning)의 부상</h3>
<p>딥러닝의 발전은 데이터로부터 유의미한 특징(feature) 또는 표현(representation)을 자동으로 학습하는 능력에 그 핵심을 둔다.1 과거 머신러닝 시스템이 전문가의 도메인 지식에 의존한 수작업 특징 공학(hand-crafted feature engineering)에 많은 노력을 투입해야 했던 것과 달리, 현대 딥러닝은 데이터 자체에서 패턴을 발견하고 계층적인 특징을 추출하는 패러다임으로 전환되었다.3 특히 지도 학습(Supervised Learning) 환경에서, 대규모 레이블 데이터셋은 모델이 복잡한 표현을 학습하는 데 결정적인 역할을 했다. 그러나 이러한 접근법은 막대한 양의 데이터를 수집하고 정확하게 레이블링하는 데 상당한 비용과 시간을 요구하는 본질적인 한계를 내포한다.4 이러한 한계는 레이블 없이 데이터 자체의 구조를 활용하여 표현을 학습하는 자기지도학습(Self-Supervised Learning)과 같은 새로운 패러다임의 등장을 촉진하는 계기가 되었다.2</p>
<h3>1.2 문제 정의: 분류가 아닌 ‘유사성’ 학습</h3>
<p>전통적인 딥러닝 분류(classification) 모델은 입력 데이터를 사전 정의된 고정된 수의 클래스 중 하나로 매핑하고, 최종적으로 각 클래스에 대한 확률 분포를 출력한다. 이러한 모델은 학습 데이터가 풍부할 때 강력한 성능을 보이지만, 몇 가지 근본적인 문제에 직면한다. 새로운 클래스가 추가될 경우, 모델 전체를 재학습해야 하는 경직성을 가지며, 클래스당 데이터 샘플이 부족한 상황에서는 일반화 성능이 급격히 저하된다.1</p>
<p>이러한 문제를 해결하기 위한 대안적 접근법이 바로 ’유사성 척도 학습(Similarity Metric Learning)’이다.7 이 패러다임은 모델이 입력 데이터를 특정 클래스로 분류하는 대신, 두 입력 데이터 간의 ‘관계’, 즉 의미론적 ’거리’나 ’유사도’를 직접 학습하도록 설계된다.9 이는 모델의 학습 목표를 ’절대적 분류’에서 ’상대적 비교’로 전환시킨다. “이 이미지는 고양이인가?“라는 질문 대신 “이 두 이미지는 같은 고양이인가?” 혹은 “이 두 문장은 같은 의미인가?“라는 질문에 답하는 능력을 학습하는 것이다. 이 미묘하지만 근본적인 관점의 변화는 모델이 각 클래스의 보편적 특징을 암기하는 대신, 입력 간의 미세한 차이를 구별하는 능력을 기르도록 유도한다. 그 결과, 클래스당 샘플이 단 하나만 존재하는 극단적인 상황, 즉 원샷 학습(One-shot learning) 환경에서도 효과적으로 작동할 수 있는 잠재력을 갖게 된다.4</p>
<h3>1.3 핵심 개념 소개: 샴 네트워크와 대조 학습</h3>
<p>유사성 척도 학습을 구현하기 위한 대표적인 두 가지 핵심 요소가 바로 샴 네트워크(Siamese Network)와 대조 학습(Contrastive Learning)이다.</p>
<ul>
<li><strong>샴 네트워크(Siamese Network):</strong> 두 개 이상의 동일한 서브네트워크가 모든 가중치를 공유하는 독특한 신경망 구조를 의미한다. 이는 서로 다른 입력에 대해 일관되고 편향 없는 특징을 추출하기 위한 <strong>구조적 해법</strong>을 제공한다.7</li>
<li><strong>대조 학습(Contrastive Learning):</strong> 임베딩 공간(embedding space) 내에서 의미적으로 유사한 데이터 쌍(positive pair)은 서로 가깝게 끌어당기고, 관련 없는 데이터 쌍(negative pair)은 멀리 밀어내는 방식으로 표현을 학습하는 <strong>학습 철학 및 방법론</strong>이다. 이는 레이블 없이 데이터 자체의 내재적 관계를 학습하는 강력한 프레임워크를 제공한다.12</li>
</ul>
<p>이 두 개념은 상호 보완적인 관계를 맺는다. 샴 네트워크가 유사성 비교를 위한 안정적인 ’틀’을 제공한다면, 대조 학습은 그 틀 안에서 ’무엇을 어떻게 학습할 것인가’에 대한 구체적인 지침, 즉 손실 함수와 학습 원리를 제공한다. 본 안내서는 이 두 기술의 원리, 결합 방식, 그리고 현대 딥러닝 프레임워크에서의 구현을 심층적으로 고찰하고자 한다.</p>
<h2>2.  샴 네트워크(Siamese Network)의 구조와 원리</h2>
<h3>2.1  핵심 구조: 가중치 공유(Weight Sharing)</h3>
<p>샴 네트워크는 그 이름이 ’샴쌍둥이’에서 유래한 것처럼, 구조적으로 동일한 두 개 이상의 서브네트워크로 구성된다.1 이 아키텍처의 가장 중요하고 본질적인 특징은 모든 서브네트워크가 동일한 구조를 가질 뿐만 아니라, 모든 파라미터, 즉 **가중치를 완벽하게 공유(weight sharing)**한다는 점이다.1 이는 개념적으로는 여러 개의 네트워크가 있는 것처럼 보이지만, 실제로는 단 하나의 파라미터 셋을 가진 네트워크를 여러 입력에 독립적으로 적용하는 것과 같다.1</p>
<p>가중치 공유는 다음과 같은 명확한 목적을 가진다.</p>
<ul>
<li><strong>일관된 특징 추출(Consistent Feature Extraction):</strong> 두 개의 서로 다른 입력을 비교하기 위해서는 두 입력을 동일한 ’잣대’로 평가해야 한다. 가중치 공유는 동일한 변환 함수 <span class="math math-inline">f(·)</span>를 각 입력에 적용하여, 비교 가능한 의미론적 공간으로 매핑된 특징 벡터(임베딩)를 추출하는 것을 보장한다.11 만약 두 서브네트워크가 서로 다른 가중치를 학습한다면, 동일한 객체의 이미지를 입력하더라도 완전히 다른 임베딩이 생성될 수 있어 의미 있는 거리 계산이 불가능해진다.16</li>
<li><strong>대칭성(Symmetry) 보장:</strong> 거리 척도(metric)가 만족해야 할 기본적인 속성 중 하나는 대칭성, 즉 거리 함수 <span class="math math-inline">d(A, B)</span>는 <span class="math math-inline">d(B, A)</span>와 같아야 한다는 것이다. 가중치를 공유하지 않으면 입력 <span class="math math-inline">x_1</span>과 <span class="math math-inline">x_2</span>의 순서를 바꾸었을 때 <span class="math math-inline">d(f_1(x_1), f_2(x_2))</span>와 <span class="math math-inline">d(f_1(x_2), f_2(x_1))</span>이 달라질 수 있다. 가중치 공유는 이러한 비대칭성 문제를 원천적으로 방지하여, 입력 순서에 관계없이 일관된 유사도 점수를 출력하도록 보장한다.8</li>
<li><strong>파라미터 효율성:</strong> 두 개의 네트워크를 사용함에도 불구하고 학습해야 할 파라미터의 수는 단일 네트워크와 동일하다. 이는 모델의 복잡도를 낮추어 과적합(overfitting)의 위험을 줄이고, 제한된 데이터에서도 더 나은 일반화 성능을 달성하는 데 기여한다.14</li>
</ul>
<h3>2.2  작동 방식: 임베딩과 거리 측정</h3>
<p>샴 네트워크의 작동 과정은 세 단계로 명확하게 구분할 수 있다.</p>
<ol>
<li><strong>입력:</strong> 비교하고자 하는 두 개의 독립적인 데이터 <span class="math math-inline">x_1</span>과 <span class="math math-inline">x_2</span>가 네트워크에 입력으로 제공된다.7</li>
<li><strong>특징 추출 (인코딩):</strong> 각 입력은 가중치를 공유하는 동일한 서브네트워크(인코더) <span class="math math-inline">f(·)</span>를 통과한다. 이 인코더는 입력 데이터를 저차원의 밀집된 특징 벡터(dense feature vector), 즉 임베딩 <span class="math math-inline">h_1 = f(x_1)</span>과 <span class="math math-inline">h_2 = f(x_2)</span>로 변환(인코딩)한다.7 이미지 데이터의 경우 이 인코더는 주로 합성곱 신경망(CNN)으로 구성되며, 텍스트 데이터의 경우 순환 신경망(RNN)이나 트랜스포머(Transformer)가 사용될 수 있다.3</li>
<li><strong>유사도 측정:</strong> 추출된 두 임베딩 벡터 <span class="math math-inline">h_1</span>과 <span class="math math-inline">h_2</span> 사이의 거리를 계산하여 두 입력 간의 유사도를 정량화한다. 일반적으로 사용되는 거리 척도는 다음과 같다.</li>
</ol>
<ul>
<li><strong>유클리드 거리(Euclidean Distance):</strong> <span class="math math-inline">D_W = \|f(x_1) - f(x_2)\|_2</span>. 두 벡터 간의 L2-norm으로, 임베딩 공간에서의 직선 거리를 의미한다. 거리가 작을수록 두 입력은 더 유사하다고 판단한다.7</li>
<li><strong>코사인 유사도(Cosine Similarity):</strong> <span class="math math-inline">\text{sim}(h_1, h_2) = (h_1 \cdot h_2) / (\|h_1\| \|h_2\|)</span>. 두 벡터가 이루는 각도의 코사인 값으로, 방향성의 유사성을 측정한다. 값이 1에 가까울수록 유사도가 높다.7</li>
</ul>
<p>최종적으로 샴 네트워크는 특정 클래스에 대한 확률 분포가 아닌, 두 입력 간의 ‘거리’ 또는 ’유사도 점수’를 출력한다.9 이 점수를 기반으로 두 입력이 동일한 클래스에 속하는지 여부를 판별하게 된다. 이는 샴 네트워크가 단순한 특징 추출기를 넘어, 특정 태스크에 최적화된 ’의미적 거리(semantic distance)’를 정의하고 계산하는 함수 자체를 데이터로부터 학습하는 강력한 프레임워크임을 시사한다. 기존의 k-NN과 같은 알고리즘이 유클리드 거리와 같은 ‘고정된’ 거리 함수를 사용하는 반면, 샴 네트워크는 <span class="math math-inline">D_W(x_1, x_2) = \|f_W(x_1) - f_W(x_2)\|_2</span> 라는, 가중치 <span class="math math-inline">W</span>에 의해 파라미터화된 ‘학습 가능한 비선형 거리 함수’ 그 자체로 볼 수 있다.18</p>
<h3>2.3  주요 응용 분야: One-Shot &amp; Few-Shot Learning</h3>
<p>샴 네트워크의 구조적 특성은 데이터가 희소한 환경, 특히 원샷(One-shot) 및 퓨샷(Few-shot) 학습에서 강력한 이점을 제공한다.</p>
<ul>
<li><strong>One-Shot Learning의 필요성:</strong> 대부분의 딥러닝 모델은 클래스당 수백, 수천 개의 학습 데이터를 요구하지만, 얼굴 인식, 서명 인증, 희귀 질병 진단과 같이 실제 세계의 많은 문제에서는 클래스당 단 하나의 또는 극소수의 샘플만 가용한 경우가 많다.4 원샷 학습은 이러한 제약 하에서 모델이 새로운 클래스를 인식할 수 있도록 하는 것을 목표로 한다.10</li>
<li><strong>샴 네트워크의 역할:</strong> 일반적인 분류 모델과 달리, 샴 네트워크는 새로운 클래스가 추가되어도 모델 전체를 재학습할 필요가 없다. 대신, 새로운 데이터의 임베딩을 계산한 후, 이를 기존에 등록된 데이터(gallery set)의 임베딩들과 비교하여 가장 거리가 가까운 클래스로 분류하는 방식을 사용한다.4 이러한 유연성 덕분에 샴 네트워크는 원샷 및 퓨샷 학습 시나리오에 이상적인 아키텍처로 평가받는다.10</li>
<li><strong>대표적 응용 사례:</strong></li>
<li><strong>얼굴 인식/인증(Face Recognition/Verification):</strong> 데이터베이스에 등록된 인물의 얼굴 이미지 임베딩과 현재 입력된 얼굴 이미지의 임베딩 간 거리를 계산하여 동일인 여부를 판단한다. 출입 통제 시스템이나 모바일 기기 잠금 해제 등에 널리 사용된다.7</li>
<li><strong>서명 인증(Signature Verification):</strong> 진본 서명과 위조 서명을 구별하는 데 사용된다. 모델은 진본 서명 쌍 간의 거리는 가깝게, 진본과 위조 서명 간의 거리는 멀게 학습한다.7</li>
<li><strong>자연어 처리(NLP):</strong> 두 문장이 의미적으로 얼마나 유사한지 측정하거나, Q&amp;A 시스템에서 중복 질문을 탐지하는 등의 태스크에 활용된다.7</li>
</ul>
<h2>3.  대조 학습(Contrastive Learning)의 철학</h2>
<h3>3.1  핵심 원리: 유사한 것은 당기고, 다른 것은 밀어내기</h3>
<p>대조 학습의 기본 철학은 인간이 세상을 인식하고 학습하는 방식과 매우 유사하다. 예를 들어, 우리는 ’수달’이라는 동물을 한 번도 본 적이 없더라도, 여러 동물 사진들을 보면서 어떤 사진들이 같은 종의 동물을 담고 있는지, 또 어떤 사진들이 다른 종의 동물을 담고 있는지 구별할 수 있다.13 이는 각 동물의 정확한 명칭(레이블)을 아는 것과는 별개의 능력으로, ’비교’를 통해 유사성과 차이점을 학습하는 과정이다.</p>
<p>대조 학습은 이러한 직관을 수학적 프레임워크로 구현한다. 학습의 기본 단위는 개별 데이터 샘플이 아닌, 데이터 ’쌍(pair)’이다.12</p>
<ul>
<li><strong>긍정 쌍(Positive Pair):</strong> 의미적으로 유사하거나 동일한 출처를 가진 데이터 쌍이다. 예를 들어, 동일한 이미지에 서로 다른 데이터 증강(augmentation)을 적용하여 생성된 두 개의 이미지가 이에 해당한다.6</li>
<li><strong>부정 쌍(Negative Pair):</strong> 의미적으로 관련이 없거나 서로 다른 출처를 가진 데이터 쌍이다. 예를 들어, 데이터셋에서 무작위로 추출한 두 개의 서로 다른 이미지가 이에 해당한다.12</li>
</ul>
<p>대조 학습의 목표는 인코더를 통해 데이터를 매핑한 임베딩 공간에서, 긍정 쌍에 속하는 두 샘플의 임베딩 간 거리는 최소화하고(attraction), 부정 쌍에 속하는 두 샘플의 임베딩 간 거리는 최대화(repulsion)하는 것이다.13 이 과정을 통해 모델은 레이블 정보 없이도 데이터의 의미론적 구조를 스스로 파악하게 된다.</p>
<h3>3.2  자기지도학습(Self-Supervised Learning)에서의 활용</h3>
<p>대조 학습은 레이블이 없는 방대한 데이터로부터 유용한 표현을 학습하는 자기지도학습(Self-Supervised Learning)의 핵심 패러다임으로 자리 잡았다.2 자기지도학습에서는 외부에서 제공되는 레이블 대신, 데이터 자체를 감독 신호(supervisory signal)로 활용하기 위해 ’프리텍스트 태스크(pretext task)’를 설계한다. 대조 학습에서는 데이터 증강(Data Augmentation)이 이 역할을 수행한다.</p>
<p>하나의 원본 이미지 <span class="math math-inline">x</span>에 서로 다른 랜덤한 데이터 증강 함수 <span class="math math-inline">t</span>와 <span class="math math-inline">t&#39;</span>를 적용하여 두 개의 ‘뷰(view)’ <span class="math math-inline">x_i = t(x)</span>와 <span class="math math-inline">x_j = t&#39;(x)</span>를 생성한다. 이 두 뷰는 픽셀 수준에서는 다르지만 의미론적으로는 동일한 객체를 나타내므로, 강력한 긍정 쌍을 형성한다.13 이때 사용되는 데이터 증강 기법은 다음과 같이 다양하다.</p>
<ul>
<li><strong>기하학적 변환:</strong> 랜덤 크롭 및 리사이즈(Random Cropping and Resizing), 수평 뒤집기(Horizontal Flipping), 회전(Rotation).2</li>
<li><strong>외형적 변환:</strong> 색상 왜곡(Color Jittering), 그레이스케일 변환(Grayscale), 가우시안 블러(Gaussian Blur).13</li>
</ul>
<p>이러한 증강 기법들을 조합하여 강력하게 적용하는 것이 매우 중요하다. 이는 모델이 색상 히스토그램이나 특정 텍스처와 같은 피상적인 저수준 특징에 의존하여 문제를 쉽게 푸는 ’꼼수(shortcut)’를 방지하고, 객체의 형태와 구조 등 더 추상적이고 강건한(robust) 의미론적 특징을 학습하도록 강제하는 효과가 있다.30</p>
<h3>3.3  좋은 표현의 조건: 정렬(Alignment)과 균일성(Uniformity)</h3>
<p>대조 학습이 왜 효과적인 표현을 학습하는지에 대한 이론적 분석은 Wang and Isola (2020)의 연구에서 제시된 두 가지 속성, 즉 정렬(Alignment)과 균일성(Uniformity)로 설명될 수 있다.32</p>
<ul>
<li><strong>정렬(Alignment):</strong> 긍정 쌍의 임베딩이 서로 가까워야 한다는 속성이다. 즉, 의미적으로 유사한 샘플들은 임베딩 공간상에서 서로 가깝게 위치해야 한다. 이는 표현의 판별력을 높이는 데 기여한다.</li>
<li><strong>균일성(Uniformity):</strong> 데이터셋 전체 샘플들의 임베딩이 단위 초구(unit hypersphere) 상에 가능한 한 균일하게 분포해야 한다는 속성이다. 이는 임베딩 공간을 최대한 활용하여 정보 손실을 최소화하고 표현의 다양성을 보존하는 데 중요하다.</li>
</ul>
<p>이 두 가지 목표는 서로 상충될 수 있다. 예를 들어, 모든 샘플을 하나의 점으로 매핑하면 정렬은 완벽하게 만족되지만, 모든 정보가 손실되어 표현으로서의 가치가 사라지는 ’표현 붕괴(representation collapse)’가 발생한다.33 반대로, 너무 균일하게만 분포시키면 같은 클래스 내의 군집 구조가 파괴되어 판별력이 저하될 수 있다.</p>
<p>대조 학습의 성공은 이 두 힘 사이의 절묘한 균형을 맞추는 메커니즘에 있다. 긍정 쌍을 끌어당기는 힘은 ’정렬’을 촉진하고, 부정 쌍을 밀어내는 힘은 임베딩이 한곳에 뭉치지 않고 공간 전체로 퍼지도록 하여 ’균일성’을 촉진한다. 결과적으로, 대조 손실 함수는 암묵적으로 이 두 가지 속성을 동시에 최적화함으로써, 클래스 내에서는 높은 응집도를 가지면서도 전체적으로는 풍부한 정보를 담고 있는 이상적인 임베딩 공간을 형성하게 된다.32</p>
<h2>4.  샴 네트워크와 대조 학습의 결합: 손실 함수 심층 분석</h2>
<p>샴 네트워크 아키텍처는 대조 학습의 철학을 구현하기 위한 이상적인 구조이며, 이 둘의 결합은 특정 손실 함수를 통해 완성된다. 이 손실 함수들은 임베딩 공간에서 샘플 간의 거리를 조절하여 네트워크의 가중치를 업데이트하는 역할을 수행한다.</p>
<h3>4.1  대조 손실 (Contrastive Loss)</h3>
<p>대조 손실은 샴 네트워크를 위해 제안된 초기의 대표적인 손실 함수로, Hadsell, Chopra, LeCun (2006)의 논문 “Dimensionality Reduction by Learning an Invariant Mapping“에서 소개되었다.18 이 함수의 목표는 명확하다: 유사한 쌍(Positive Pair)의 임베딩 간 거리는 0에 가깝게 만들고, 다른 쌍(Negative Pair)의 거리는 사용자가 정의한 특정 경계값, 즉 마진(margin) <span class="math math-inline">m</span> 이상이 되도록 강제하는 것이다.35</p>
<p>수식은 다음과 같이 정의된다.<br />
<span class="math math-display">
L(W, Y, \vec{X_1}, \vec{X_2}) = (1-Y) \frac{1}{2} (D_W)^2 + (Y) \frac{1}{2} \{\max(0, m - D_W)\}^2
</span></p>
<ul>
<li><span class="math math-inline">Y</span>: 두 입력 <span class="math math-inline">\vec{X_1}</span>과 <span class="math math-inline">\vec{X_2}</span>가 유사한 쌍이면 0, 다른 쌍이면 1의 값을 갖는 레이블이다.37 (일부 구현에서는 반대로 정의되기도 하므로 주의가 필요하다.9)</li>
<li><span class="math math-inline">D_W</span>: 가중치 <span class="math math-inline">W</span>로 파라미터화된 네트워크 <span class="math math-inline">G_W</span>의 출력, 즉 두 임베딩 간의 유클리드 거리 <span class="math math-inline">\|G_W(\vec{X_1}) - G_W(\vec{X_2})\|_2</span>를 의미한다.18</li>
<li><span class="math math-inline">m</span>: 다른 쌍이 최소한 이 거리만큼은 떨어져 있도록 보장하는 양수 값의 하이퍼파라미터이다.5</li>
</ul>
<p>이 손실 함수의 작동 원리는 두 경우로 나뉜다.</p>
<ul>
<li><strong>유사한 쌍 (Y=0):</strong> 두 번째 항이 0이 되면서 손실은 <span class="math math-inline">(1/2) \cdot (D_W)^2</span>가 된다. 이 손실을 최소화하기 위해 네트워크는 거리 <span class="math math-inline">D_W</span>를 0에 가깝게 만들도록 학습된다. 이는 마치 두 물체를 연결한 용수철이 수축하여 서로를 끌어당기는 것과 같다.36</li>
<li><strong>다른 쌍 (Y=1):</strong> 첫 번째 항이 0이 되면서 손실은 <span class="math math-inline">(1/2) \cdot \{\max(0, m - D_W)\}^2</span>가 된다. 만약 두 임베딩 간의 거리 <span class="math math-inline">D_W</span>가 이미 마진 <span class="math math-inline">m</span>보다 크다면, <span class="math math-inline">\max</span> 함수 안의 값은 음수가 되어 손실은 0이 된다. 이 경우, 두 샘플은 이미 충분히 멀리 떨어져 있으므로 네트워크는 가중치를 업데이트하지 않는다. 그러나 만약 <span class="math math-inline">D_W</span>가 <span class="math math-inline">m</span>보다 작다면, 양수의 손실이 발생하며, 네트워크는 <span class="math math-inline">D_W</span>를 <span class="math math-inline">m</span>까지 증가시키는 방향으로 학습된다. 이는 두 물체 사이에 최소 <span class="math math-inline">m</span>의 거리를 유지하려는 척력처럼 작용한다.28</li>
</ul>
<h3>4.2  삼중항 손실 (Triplet Loss)</h3>
<p>삼중항 손실은 FaceNet (Schroff et al., 2015) 논문에서 제안되어 얼굴 인식 분야에서 큰 성공을 거두며 널리 알려졌다.5 대조 손실이 데이터 ’쌍(pair)’을 다루는 반면, 삼중항 손실은 세 개의 데이터, 즉 ’삼중항(triplet)’을 사용하여 더 정교한 상대적 거리 관계를 학습한다.</p>
<p>학습의 기본 단위는 다음과 같이 구성된다.</p>
<ul>
<li><strong>앵커 (Anchor, A):</strong> 기준이 되는 샘플.</li>
<li><strong>긍정 (Positive, P):</strong> 앵커와 같은 클래스에 속하는 샘플.</li>
<li><strong>부정 (Negative, N):</strong> 앵커와 다른 클래스에 속하는 샘플.15</li>
</ul>
<p>삼중항 손실의 목표는 앵커와 긍정 샘플 간의 거리 <span class="math math-inline">d(A, P)</span>가 앵커와 부정 샘플 간의 거리 <span class="math math-inline">d(A, N)</span>보다 항상 특정 마진 <span class="math math-inline">\alpha</span>만큼 더 작도록 만드는 것이다.15</p>
<p>수식은 다음과 같다.<br />
<span class="math math-display">
L(A, P, N) = \max \left( \|f(A) - f(P)\|^2 - \|f(A) - f(N)\|^2 + \alpha, 0 \right)
</span></p>
<ul>
<li><span class="math math-inline">f(·)</span>: 인코더 네트워크.</li>
<li><span class="math math-inline">\alpha</span>: 양수 값의 마진 하이퍼파라미터.15</li>
</ul>
<p>이 손실 함수는 <span class="math math-inline">\|f(A) - f(P)\|^2 + \alpha &lt; \|f(A) - f(N)\|^2</span> 조건을 만족시키도록 네트워크를 학습시킨다. 만약 이 조건이 이미 만족되어 부정 샘플이 긍정 샘플보다 마진 <span class="math math-inline">\alpha</span> 이상으로 충분히 멀리 떨어져 있다면, <span class="math math-inline">\max</span> 함수 안의 값은 음수가 되어 최종 손실은 0이 되고, 가중치 업데이트는 발생하지 않는다. 학습의 효율성을 높이기 위해서는 무작위로 삼중항을 구성하기보다, 학습하기 어려운 샘플, 즉 <span class="math math-inline">d(A, P)</span>가 큰 ‘어려운 긍정(hard positive)’ 샘플이나 <span class="math math-inline">d(A, N)</span>이 작은 ‘어려운 부정(hard negative)’ 샘플을 의도적으로 선택하는 <strong>하드 샘플 마이닝(Hard Sample Mining)</strong> 전략이 매우 중요하다. 너무 쉬운 삼중항만으로는 모델이 충분히 학습되지 않을 수 있기 때문이다.40</p>
<h3>4.3  NT-Xent (Normalized Temperature-scaled Cross-Entropy) 손실</h3>
<p>NT-Xent 손실은 SimCLR (Chen et al., 2020) 프레임워크에서 제안된 현대적인 대조 손실 함수이다.2 이 손실 함수는 대규모 배치(batch)를 활용하여 학습 신호를 극대화하는 데 초점을 맞춘다.</p>
<p>핵심 아이디어는 배치 내에서 하나의 긍정 쌍(예: 동일 이미지의 두 가지 다른 증강 뷰)을 제외한 나머지 모든 샘플을 부정 샘플로 간주하는 것이다. 긍정 쌍 <span class="math math-inline">(i, j)</span>에 대한 손실은 다음과 같이 정의된다.<br />
<span class="math math-display">
\ell_{i,j} = -\log \frac{\exp(\text{sim}(z_i, z_j)/\tau)}{\sum_{k=1}^{2N} \mathbf{1}_{[k \neq i]} \exp(\text{sim}(z_i, z_k)/\tau)}
</span></p>
<ul>
<li><span class="math math-inline">z_i, z_j</span>: 긍정 쌍의 임베딩.</li>
<li><span class="math math-inline">\text{sim}(u, v)</span>: 두 벡터 <span class="math math-inline">u, v</span> 간의 코사인 유사도 <span class="math math-inline">u^T v / \|u\| \|v\|</span>.43</li>
<li><span class="math math-inline">\tau</span>: 온도(temperature) 하이퍼파라미터. 유사도 분포의 첨예도를 조절한다. <span class="math math-inline">\tau</span>가 낮을수록 모델은 어려운 부정 샘플에 더 집중하게 된다.45</li>
<li><span class="math math-inline">2N</span>: 크기 <span class="math math-inline">N</span>의 미니배치에서 각 이미지마다 2개의 증강 뷰를 생성하여 총 <span class="math math-inline">2N</span>개의 샘플이 있음을 의미한다. 분모는 <span class="math math-inline">i</span>번째 샘플과 자기 자신을 제외한 <span class="math math-inline">2N-1</span>개의 다른 모든 샘플과의 유사도를 합산한다.</li>
</ul>
<p>이 수식의 형태는 본질적으로 Softmax 함수와 Cross-Entropy 손실의 결합이다. 즉, <span class="math math-inline">2N-1</span>개의 후보 중에서 긍정 쌍인 <span class="math math-inline">j</span>를 올바르게 ’분류’하는 문제로 재정의한 것과 같다.44 이 방식은 한 번의 업데이트에 수천 개의 부정 샘플을 활용할 수 있게 하여 매우 풍부한 대조 정보를 제공하며, 이는 SimCLR의 뛰어난 성능을 이끈 핵심 요소이다.30</p>
<p>이들 손실 함수의 발전 과정은 ’대조 정보를 얼마나 효율적으로, 그리고 풍부하게 활용할 것인가’에 대한 고민의 역사로 볼 수 있다. 대조 손실은 한 번에 하나의 쌍만 고려하여 국소적인 정보를 활용하는 반면, 삼중항 손실은 긍정 및 부정 쌍을 동시에 고려하여 상대적인 정보를 추가한다. NT-Xent 손실은 이를 극단적으로 확장하여 배치 내 모든 샘플을 대조 정보로 활용함으로써 학습 신호의 밀도를 극대화한다. 이는 대조 학습의 성능이 양질의, 그리고 다량의 부정 샘플 확보에 크게 의존함을 명확히 보여준다.</p>
<h3>4.4 Table 1: 손실 함수 비교 분석</h3>
<table><thead><tr><th>항목 (Item)</th><th>대조 손실 (Contrastive Loss)</th><th>삼중항 손실 (Triplet Loss)</th><th>NT-Xent 손실</th></tr></thead><tbody>
<tr><td><strong>입력 단위</strong></td><td>데이터 쌍 (Pair)</td><td>데이터 삼중항 (Triplet)</td><td>데이터 배치 (Batch)</td></tr>
<tr><td><strong>핵심 목표</strong></td><td>절대적 거리 마진 확보</td><td>상대적 거리 순서(랭킹) 학습</td><td>배치 내 Positive Pair 분류</td></tr>
<tr><td><strong>수식</strong></td><td><span class="math math-inline">(1-Y)D^2 + Y\{\max(0, m-D)\}^2</span></td><td>$\max(</td><td>f_A-f_P</td></tr>
<tr><td><strong>장점</strong></td><td>구현이 간단하고 직관적임.</td><td>상대적 순위를 학습하여 더 정교한 임베딩 공간 형성 가능.</td><td>대규모 Negative 샘플을 활용하여 학습 신호가 풍부하고 성능이 우수함.</td></tr>
<tr><td><strong>단점</strong></td><td>단순한 이진적 구별에 그침. 마진 <span class="math math-inline">m</span> 설정이 중요함.</td><td>Triplet 구성 및 샘플 마이닝 전략이 성능에 큰 영향을 미침. 계산 복잡도가 높음.47</td><td>매우 큰 배치 크기를 요구하여 메모리 및 계산 자원 소모가 큼.2</td></tr>
</tbody></table>
<h2>5.  현대적 대조 학습 프레임워크 고찰</h2>
<p>샴 네트워크와 대조 손실의 기본 아이디어를 바탕으로, 현대 자기지도학습 연구는 더 효율적이고 강력한 표현을 학습하기 위한 다양한 프레임워크를 제시했다. 그중 SimCLR과 MoCo는 가장 대표적이며, 대조 학습의 발전에 큰 영향을 미쳤다.</p>
<h3>5.1  SimCLR (A Simple Framework for Contrastive Learning)</h3>
<p>SimCLR은 Google 연구팀이 제안한 프레임워크로, 메모리 뱅크나 복잡한 아키텍처 없이도 기존의 자기지도학습 방법론들을 뛰어넘는 성능을 달성하여 큰 주목을 받았다.2 SimCLR의 성공은 세 가지 핵심 구성 요소의 시너지에 기인한다.</p>
<ol>
<li>
<p><strong>강력한 데이터 증강 조합:</strong> SimCLR는 성능의 핵심 요인으로 데이터 증강의 중요성을 강조했다. 특히, 랜덤 크롭(Random Cropping)과 색상 왜곡(Color Distortion)을 조합하여 사용하는 것이 모델이 유의미한 표현을 학습하는 데 결정적인 역할을 함을 실험적으로 증명했다.30</p>
</li>
<li>
<p><strong>프로젝션 헤드 (Projection Head):</strong> 인코더(예: ResNet)를 통해 추출된 표현 벡터 <span class="math math-inline">h</span>를 직접 대조 손실 계산에 사용하지 않고, 그 위에 작은 비선형 MLP(Multi-Layer Perceptron)인 ‘프로젝션 헤드’ <span class="math math-inline">g(·)</span>를 추가한다. 대조 손실은 이 프로젝션 헤드를 통과한 최종 출력 <span class="math math-inline">z = g(h)</span>에 적용된다.2 흥미로운 점은, 자기지도학습이 완료된 후 이 프로젝션 헤드는 버리고, 다운스트림 태스크(예: 이미지 분류)에는 인코더의 출력인 <span class="math math-inline">h</span>를 특징 벡터로 사용한다는 것이다.2 이는 프로젝션 헤드가 학습 과정에서 데이터 증강에 대한 불변성(invariance)을 학습하는 부담을 전담하고, 그 덕분에 인코더의 표현</p>
</li>
</ol>
<p><span class="math math-inline">h</span>는 색상, 질감, 방향 등 다운스트림 태스크에 유용할 수 있는 더 풍부하고 일반적인 정보를 보존할 수 있게 하는 효과적인 분업 메커니즘으로 작용한다.31</p>
<ol start="3">
<li><strong>대규모 배치와 NT-Xent 손실:</strong> SimCLR은 NT-Xent 손실을 효과적으로 활용하기 위해 4096 또는 8192와 같은 매우 큰 배치 크기를 사용한다. 이는 한 번의 학습 단계에서 수천 개의 풍부한 부정 샘플을 제공하여 모델이 임베딩 공간의 전역적인 구조를 더 잘 학습하도록 돕는다.30</li>
</ol>
<h3>5.2  MoCo (Momentum Contrast)</h3>
<p>MoCo는 Facebook AI Research(FAIR)에서 제안한 프레임워크로, SimCLR의 가장 큰 실용적 한계인 ‘대규모 배치 요구 사항’ 문제를 독창적인 방식으로 해결했다.52</p>
<ol>
<li><strong>동적 딕셔너리로서의 큐(Queue as a Dynamic Dictionary):</strong> MoCo는 대규모 부정 샘플을 확보하기 위해 GPU 메모리에 모든 샘플을 올리는 대신, 이전 미니배치들에서 인코딩된 키(key) 벡터들을 저장하는 큐(Queue) 자료구조를 사용한다. 매 학습 단계마다 현재 미니배치의 키 벡터들은 큐에 추가(enqueue)되고, 가장 오래된 배치의 키 벡터들은 제거(dequeue)된다. 이 방식을 통해 배치 크기와 부정 샘플의 수를 분리(decouple)하여, 작은 배치 크기를 사용하면서도 수만 개의 부정 샘플을 일관되게 활용할 수 있게 된다.52</li>
<li><strong>모멘텀 인코더(Momentum Encoder):</strong> 큐를 사용할 때 발생하는 잠재적인 문제는 큐에 저장된 키들이 서로 다른 시점의 인코더에 의해 생성되어 일관성이 떨어진다는 점이다. MoCo는 이 문제를 해결하기 위해 ’모멘텀 인코더’라는 개념을 도입했다. 쿼리(query)를 인코딩하는 쿼리 인코더 <span class="math math-inline">f_q</span>는 역전파를 통해 일반적인 방식으로 학습되는 반면, 키(key)를 인코딩하는 키 인코더 <span class="math math-inline">f_k</span>는 역전파를 받지 않는다. 대신, 키 인코더의 가중치 <span class="math math-inline">\theta_k</span>는 쿼리 인코더의 가중치 <span class="math math-inline">\theta_q</span>를 사용하여 다음과 같은 모멘텀 업데이트 방식으로 점진적으로 갱신된다: <span class="math math-inline">\theta_k \leftarrow m \cdot \theta_k + (1-m) \cdot \theta_q</span>. 여기서 모멘텀 계수 <span class="math math-inline">m</span>은 0.999와 같이 매우 큰 값을 사용하여 키 인코더가 매우 느리고 부드럽게 변하도록 한다.52 이 메커니즘 덕분에 큐에 저장된 키들의 표현이 시간에 걸쳐 높은 일관성을 유지할 수 있어 안정적인 학습이 가능하다.53</li>
</ol>
<p>결국 SimCLR과 MoCo는 ’일관성 있는 대규모 부정 샘플 확보’라는 동일한 목표를 서로 다른 철학으로 달성한다. SimCLR은 막대한 계산 자원을 투입하여 모든 부정 샘플을 ‘동시에’ 처리함으로써 완벽한 일관성을 보장하는 ’공간적 해결책’을 택했다. 반면, MoCo는 큐와 모멘텀 인코더라는 독창적인 장치를 통해 약간의 근사를 감수하며 효율성을 극대화하는 ’시간적 해결책’을 제시했다. 이들의 차이는 단순히 기술적 디테일을 넘어, 동일한 문제에 대한 근본적으로 다른 공학적 접근 방식을 보여준다.</p>
<h3>5.3 Table 2: SimCLR vs. MoCo 비교</h3>
<table><thead><tr><th>항목 (Item)</th><th>SimCLR</th><th>MoCo</th></tr></thead><tbody>
<tr><td><strong>Negative 샘플 처리</strong></td><td>현재 배치 내의 다른 샘플들을 활용 (In-batch negatives)</td><td>이전 배치들의 임베딩을 저장하는 동적 큐(Queue) 활용</td></tr>
<tr><td><strong>인코더 구조</strong></td><td>단일 인코더 + 프로젝션 헤드</td><td>쿼리 인코더 + 모멘텀 키 인코더</td></tr>
<tr><td><strong>자원 요구사항</strong></td><td>매우 큰 배치 크기 필요 (GPU 메모리 집약적)</td><td>작은 배치 크기로도 대규모 Negative 샘플 활용 가능 (메모리 효율적)</td></tr>
<tr><td><strong>핵심 혁신</strong></td><td>데이터 증강과 프로젝션 헤드의 중요성 입증</td><td>모멘텀 인코더를 통한 일관성 있는 키(Key) 생성 및 큐를 이용한 효율적 Negative 샘플링</td></tr>
</tbody></table>
<h2>6.  학습의 실제적 과제와 미묘한 지점들</h2>
<p>샴 네트워크와 대조 학습 프레임워크는 강력한 성능을 제공하지만, 성공적인 학습을 위해서는 몇 가지 실제적인 과제와 미묘한 지점들을 고려해야 한다.</p>
<h3>6.1  하이퍼파라미터 설정의 민감성</h3>
<ul>
<li><strong>마진(Margin) 선택:</strong> 대조 손실과 삼중항 손실에서 마진(<span class="math math-inline">m</span>, <span class="math math-inline">\alpha</span>) 값은 학습 결과에 큰 영향을 미치는 중요한 하이퍼파라미터이다. 마진이 너무 작으면 다른 클래스의 샘플들을 충분히 밀어내지 못해 판별력이 낮은 임베딩이 학습될 수 있다. 반대로 마진이 너무 크면 모델이 수렴하기 어려워지고 학습 시간이 길어질 수 있다.57 최적의 마진 값은 데이터셋의 특성, 임베딩 공간의 차원, 사용된 거리 척도(예: 정규화 여부) 등에 따라 달라지므로, 신중한 실험을 통한 튜닝이 필수적이다.58</li>
<li><strong>온도(Temperature) <span class="math math-inline">\tau</span>:</strong> NT-Xent 손실에서 온도 <span class="math math-inline">\tau</span>는 유사도 점수 분포의 형태를 조절하여 학습 동역학을 결정한다. <span class="math math-inline">\tau</span> 값이 낮을수록 Softmax 분포가 뾰족해져 모델이 어려운 부정 샘플(hard negatives)에 더 집중하게 되지만, 이는 그래디언트 폭주(exploding gradients)를 유발하여 학습을 불안정하게 만들 수 있다. 반면 <span class="math math-inline">\tau</span> 값이 높으면 손실 함수가 더 부드러워져 학습이 안정되지만, 모든 부정 샘플을 유사하게 취급하여 판별력 학습이 약화될 수 있다.44</li>
</ul>
<h3>6.2  계산 복잡도와 자원 제약</h3>
<p>샴 네트워크는 가중치 공유 덕분에 파라미터 효율성이 높지만, 학습 데이터 구성 방식 때문에 훈련 시간이 길어질 수 있다. 데이터셋의 모든 가능한 쌍(pair)이나 삼중항(triplet)을 고려하면 학습 데이터의 수가 원본 데이터 크기의 제곱 또는 세제곱에 비례하여 폭발적으로 증가하기 때문이다.21</p>
<p>또한, SimCLR과 같은 현대적 방법론은 수천 단위의 배치 크기를 전제로 하므로, 단일 GPU 환경에서는 메모리 부족으로 인해 구현이 거의 불가능하다. 이러한 자원 제약을 극복하기 위해서는 다중 GPU를 활용한 분산 학습(distributed training)이나, 여러 미니배치의 그래디언트를 누적하여 한 번에 업데이트하는 그래디언트 누적(gradient accumulation)과 같은 고급 기술이 요구된다.2</p>
<h3>6.3  출력의 해석과 활용</h3>
<p>샴 네트워크 및 대조 학습 기반 모델의 최종 출력은 전통적인 분류 모델처럼 각 클래스에 대한 확률 분포가 아니다. 대신, 입력 쌍 간의 ‘거리’ 또는 ’유사도 점수’라는 상대적인 값을 제공한다.9 이는 직접적인 분류 확률이 필요한 애플리케이션에서는 단점으로 작용할 수 있다.9 실제 시스템에 적용하기 위해서는, 검증 데이터셋을 사용하여 이 거리 값에 대한 최적의 임계값(threshold)을 결정해야 한다. 예를 들어, 얼굴 인증 시스템에서는 계산된 거리가 특정 임계값보다 작으면 ‘동일인’, 크면 ’다른 사람’으로 판별하는 식의 후처리 과정이 필요하다.38</p>
<p>이러한 과제들 속에서, 성공적인 대조 학습의 핵심은 아키텍처나 손실 함수 자체만큼이나 ’어떻게 효과적으로 학습에 도움이 되는 샘플을 지속적으로 공급할 것인가’에 달려있다는 점을 인식하는 것이 중요하다. 손실 함수들은 이미 잘 구별되는 ‘쉬운’ 샘플 쌍에 대해서는 거의 0에 가까운 손실을 발생시켜 그래디언트가 흐르지 않게 설계되어 있다.28 즉, 학습은 오직 모델이 현재 구별하기 어려워하는 ‘어려운’ 샘플들을 통해서만 이루어진다. 따라서 무작위 샘플링 전략이 대부분 ‘쉬운’ 쌍이나 삼중항만 제공한다면, 모델은 대부분의 시간 동안 아무것도 배우지 못하고 학습이 정체될 것이다.41 이런 관점에서 SimCLR의 대규모 배치나 MoCo의 큐는 단순히 부정 샘플의 ’양’을 늘리는 것을 넘어, 배치나 큐 안에 ‘어려운’ 부정 샘플이 포함될 ’확률’을 높이는 효과적인 샘플링 전략으로 해석할 수 있다. 이는 샘플링 전략이 단순한 구현 이슈가 아니라, 대조 학습의 성공을 좌우하는 핵심적인 알고리즘적 과제임을 시사한다.</p>
<h2>7.  결론: 종합적 고찰 및 전망</h2>
<h3>7.1  샴 네트워크와 대조 학습의 시너지 요약</h3>
<p>샴 네트워크와 대조 학습은 현대 딥러닝에서 유사성 기반 표현 학습의 근간을 이루는 두 축으로서, 강력한 시너지를 발휘한다.</p>
<ul>
<li><strong>샴 네트워크</strong>는 ’일관된 비교’를 위한 <strong>구조적 토대</strong>를 제공한다. 가중치 공유라는 단순하지만 강력한 제약을 통해, 입력 데이터의 순서나 개별적 특성에 흔들리지 않고 의미 있는 유사성 측정이 가능한 안정적인 임베딩 공간을 구축할 수 있는 기반을 마련한다.</li>
<li><strong>대조 학습</strong>은 이 구조 위에서 ’무엇을 학습할 것인가’에 대한 <strong>철학과 방법론</strong>을 제공한다. 긍정/부정 쌍이라는 직관적인 개념과 이를 구현하는 정교한 손실 함수들을 통해, 모델이 명시적인 레이블 없이도 데이터의 내재적 구조를 파악하고 풍부한 의미를 담은 표현을 학습하도록 이끈다.</li>
</ul>
<p>이 둘의 결합은 클래스당 데이터가 극히 적은 원샷 학습 문제부터, 레이블이 전혀 없는 방대한 데이터셋을 활용하는 대규모 자기지도학습에 이르기까지, 다양한 문제 상황에서 딥러닝의 적용 범위를 획기적으로 확장시켰다.</p>
<h3>7.2  현재 연구 동향 및 향후 전망</h3>
<p>샴 네트워크와 대조 학습 분야는 여전히 활발하게 연구가 진행 중이며, 몇 가지 주요한 방향으로 발전하고 있다.</p>
<ul>
<li><strong>거짓 부정(False Negative) 문제 해결:</strong> 현재 배치나 큐에서 무작위로 부정 샘플을 선택하는 방식은, 실제로는 같은 클래스에 속하지만 다른 샘플로 취급되는 ‘거짓 부정’ 샘플이 포함될 위험이 있다. 이는 모델이 같은 클래스의 샘플들을 서로 밀어내도록 학습시켜 성능을 저해하는 요인이 된다. 이러한 거짓 부정 샘플의 영향을 완화하거나 제거하기 위한 연구가 활발히 진행되고 있다.</li>
<li><strong>대조 학습의 이론적 이해 심화:</strong> 대조 학습이 왜 효과적인지에 대한 이론적 분석, 예를 들어 정보 이론적 관점에서의 상호 정보량 최대화나 정렬/균일성 관점에서의 분석 등이 더욱 깊이 연구되고 있다.32 이러한 이론적 기반은 향후 더 효율적이고 안정적인 손실 함수와 학습 전략을 개발하는 데 중요한 단초를 제공할 것이다.</li>
<li><strong>다양한 도메인으로의 확장:</strong> 컴퓨터 비전 분야에서 시작된 대조 학습 방법론은 자연어 처리, 그래프 신경망, 오디오 및 음성 처리, 추천 시스템 등 다양한 데이터 도메인으로 성공적으로 확장되고 있다.46 각 도메인의 고유한 특성에 맞는 새로운 데이터 증강 기법과 프리텍스트 태스크가 개발되면서 그 적용 범위는 계속해서 넓어질 것이다.</li>
</ul>
<p>궁극적으로, 샴 네트워크의 구조적 아이디어와 대조 학습의 철학은 더 적은 인간의 감독으로 더 일반적이고 강건한 인공지능을 구축하려는 거대한 목표에 핵심적인 기여를 계속할 것이다. 이는 미래 AI 기술이 소수의 데이터만으로도 새로운 개념을 학습하고, 다양한 태스크에 유연하게 적응하는 능력을 갖추는 데 중요한 역할을 할 것으로 전망된다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>[AI] Siamese Neural Networks (샴 네트워크) 개념 이해하기, https://milkclub.tistory.com/96</li>
<li>SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, https://www.geeksforgeeks.org/deep-learning/simclr-a-simple-framework-for-contrastive-learning-of-visual-representations/</li>
<li>(PDF) Siamese-Network Based Signature Verification using Self Supervised Learning, https://www.researchgate.net/publication/372633368_Siamese-Network_Based_Signature_Verification_using_Self_Supervised_Learning</li>
<li>[딥러닝]Siamese Network(샴 네트워크)란? - Meaningful AI, https://meaningful96.github.io/deeplearning/simese/</li>
<li>Siamese Network (샴네트워크) - Philemon 1:21 - 티스토리, https://esther-eun27.tistory.com/16</li>
<li>Rethinking Positive Pairs in Contrastive Learning - arXiv, https://arxiv.org/html/2410.18200v1</li>
<li>Siamese network - Memorize - 티스토리, https://jik9210.tistory.com/29</li>
<li>What is the motivation for making Siamese networks share weights? - Quora, https://www.quora.com/What-is-the-motivation-for-making-Siamese-networks-share-weights</li>
<li>Understanding Siamese Networks: A Comprehensive Introduction - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2023/08/introduction-and-implementation-of-siamese-networks/</li>
<li>Siamese Nets: A Breakthrough in One-shot Image Recognition - Medium, https://medium.com/@kdk199604/siamese-nets-a-breakthrough-in-one-shot-image-recognition-53aa4a4fa5db</li>
<li>What is a Siamese network in deep learning? - Milvus, https://milvus.io/ai-quick-reference/what-is-a-siamese-network-in-deep-learning</li>
<li>itpe.jackerlab.com, <a href="https://itpe.jackerlab.com/entry/Contrastive-Learning-%EB%8C%80%EC%A1%B0%ED%95%99%EC%8A%B5#:~:text=1.%20%EA%B0%9C%EB%85%90%20%EB%B0%8F%20%EC%A0%95%EC%9D%98,%EB%AC%B4%EA%B4%80%ED%95%9C%20%EB%91%90%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%ED%8F%AC%EC%9D%B8%ED%8A%B8">https://itpe.jackerlab.com/entry/Contrastive-Learning-%EB%8C%80%EC%A1%B0%ED%95%99%EC%8A%B5#:~:text=1.%20%EA%B0%9C%EB%85%90%20%EB%B0%8F%20%EC%A0%95%EC%9D%98,%EB%AC%B4%EA%B4%80%ED%95%9C%20%EB%91%90%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%ED%8F%AC%EC%9D%B8%ED%8A%B8</a></li>
<li>The Beginner’s Guide to Contrastive Learning - V7 Labs, https://www.v7labs.com/blog/contrastive-learning-guide</li>
<li>Siamese Neural Network in Deep Learning - GeeksforGeeks, https://www.geeksforgeeks.org/nlp/siamese-neural-network-in-deep-learning/</li>
<li>[Pytorch] Siamese network를 이용하여 나의 외모를 점검해보자_1탄, https://kau-deeperent.tistory.com/145</li>
<li>Siamese networks: Why does the network to be duplicated? - Stack Overflow, https://stackoverflow.com/questions/48693224/siamese-networks-why-does-the-network-to-be-duplicated</li>
<li>Siamese Networks for Computer Vision - Number Analytics, https://www.numberanalytics.com/blog/ultimate-guide-siamese-networks-computer-vision</li>
<li>Dimensionality Reduction by Learning an Invariant Mapping - ResearchGate, https://www.researchgate.net/profile/Yann_Lecun/publication/4246277_Dimensionality_Reduction_by_Learning_an_Invariant_Mapping/links/00b7d514af9f25ecca000000/Dimensionality-Reduction-by-Learning-an-Invariant-Mapping.pdf</li>
<li>(PDF) Dimensionality Reduction by Learning an Invariant Mapping - ResearchGate, https://www.researchgate.net/publication/4246277_Dimensionality_Reduction_by_Learning_an_Invariant_Mapping</li>
<li>Siamese Neural Networks for One-shot Image Recognition - CMU School of Computer Science, https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf</li>
<li>An Introduction to Siamese Networks | Built In, https://builtin.com/machine-learning/siamese-network</li>
<li>Face Recognition with Siamese Network - Kaggle, https://www.kaggle.com/code/tatianakushniruk/face-recognition-with-siamese-network</li>
<li>SIGNATURE VERIFICATION USING A “SIAMESE” TIME DELAY NEURAL NETWORK | International Journal of Pattern Recognition and Artificial Intelligence - World Scientific Publishing, https://www.worldscientific.com/doi/abs/10.1142/S0218001493000339</li>
<li>Automated Signature Verification Using Siamese Network - DataDrivenInvestor, https://medium.datadriveninvestor.com/automated-signature-verification-using-siamese-network-87f3a2f55f2e</li>
<li>What’s the intuition behind contrastive learning or approach? - Cross Validated, https://stats.stackexchange.com/questions/451827/whats-the-intuition-behind-contrastive-learning-or-approach</li>
<li>Contrastive Learning (대조학습), <a href="https://itpe.jackerlab.com/entry/Contrastive-Learning-%EB%8C%80%EC%A1%B0%ED%95%99%EC%8A%B5">https://itpe.jackerlab.com/entry/Contrastive-Learning-%EB%8C%80%EC%A1%B0%ED%95%99%EC%8A%B5</a></li>
<li>Self-supervised learning (자기지도학습)과 Contrastive learning (대조학습): 개념과 방법론 톺아보기 - 휴블로그 - 티스토리, https://sanghyu.tistory.com/184</li>
<li>[딥러닝]Contrastive Learning(대조 학습)이란? - Meaningful AI, https://meaningful96.github.io/deeplearning/contrastivelarning/</li>
<li>Full Guide to Contrastive Learning | Encord, https://encord.com/blog/guide-to-contrastive-learning/</li>
<li>A Simple Framework for Contrastive Learning of Visual Representations, https://proceedings.mlr.press/v119/chen20j/chen20j.pdf</li>
<li>Paper explained — A Simple Framework for Contrastive Learning of Visual Representations [simCLR] | by Nazim Bendib | Medium, https://medium.com/@nazimbendib/paper-explained-a-simple-framework-for-contrastive-learning-of-visual-representations-simclr-217ae5f13af2</li>
<li>Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere, https://proceedings.mlr.press/v119/wang20k/wang20k.pdf</li>
<li>Understanding and Improving the Role of Projection Head in Self-Supervised Learning - arXiv, https://arxiv.org/pdf/2212.11491</li>
<li>Contrastive Learning - Deepnote, https://deepnote.com/app/svpino/Contrastive-Learning-7a0ec6e4-6a1d-43c7-9d30-aad8ad367334</li>
<li>Learning a Similarity Metric Discriminatively, with Application to Face Verification - Department of Computer Science, University of Toronto, http://www.cs.utoronto.ca/~hinton/csc2535_06/readings/chopra-05.pdf</li>
<li>Exploring Siamese Networks for Image Similarity using Contrastive Loss - Medium, https://medium.com/@hayagriva99999/exploring-siamese-networks-for-image-similarity-using-contrastive-loss-f5d5ae5a0cc6</li>
<li>[논문 리뷰] Dimensionality Reduction by Learning an Invariant Mapping (CVPR 2006), <a href="https://sumim.tistory.com/entry/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Dimensionality-Reduction-by-Learning-an-Invariant-Mapping-CVPR-2006">https://sumim.tistory.com/entry/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Dimensionality-Reduction-by-Learning-an-Invariant-Mapping-CVPR-2006</a></li>
<li>Contrastive Loss for Siamese Networks with Keras and TensorFlow - PyImageSearch, https://pyimagesearch.com/2021/01/18/contrastive-loss-for-siamese-networks-with-keras-and-tensorflow/</li>
<li>A Comprehensive Guide to Siamese Neural Networks | by Rinki Nag - Medium, https://medium.com/@rinkinag24/a-comprehensive-guide-to-siamese-neural-networks-3358658c0513</li>
<li>Triplet Loss - Advanced Intro - Qdrant, https://qdrant.tech/articles/triplet-loss/</li>
<li>샴 네트워크(Siamese Network)와 삼중항 손실 (Triplet loss) - Aero-Machine Learning, <a href="https://metar.tistory.com/entry/%EC%83%B4-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%ACSiamese-Network%EC%99%80-%EC%82%BC%EC%A4%91%ED%95%AD-%EC%86%90%EC%8B%A4-Triplet-loss">https://metar.tistory.com/entry/%EC%83%B4-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%ACSiamese-Network%EC%99%80-%EC%82%BC%EC%A4%91%ED%95%AD-%EC%86%90%EC%8B%A4-Triplet-loss</a></li>
<li>A Simple Framework for Contrastive Learning of Visual Representations - arXiv, https://arxiv.org/abs/2002.05709</li>
<li>[Representation Learning] The SimCLR Family - Youngdo Lee, https://leeyngdo.github.io/blog/representation-learning/2023-01-15-simple-framework-for-contrastive-learning/</li>
<li>NT-Xent Loss - A Quick Overview | Dilith Jayakody, https://dilithjay.com/blog/nt-xent-loss-explained</li>
<li>Contrastive Learning. Contrastive Learning 은 한국어로 대조 학습이다… | by ChengKang Tan | Medium, https://medium.com/@hichengkang/contrastive-learning-2bb36d9fe5d8</li>
<li>NT-Xent Loss for Contrastive Learning - Emergent Mind, https://www.emergentmind.com/topics/normalized-temperature-scaled-cross-entropy-loss-nt-xent</li>
<li>[VISION] 비전 시스템을 위한 딥러닝(11) - 시각 임베딩 (visual embedding), <a href="https://lsoovmee-rhino.tistory.com/entry/VISION-%EB%B9%84%EC%A0%84-%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9D%84-%EC%9C%84%ED%95%9C-%EB%94%A5%EB%9F%AC%EB%8B%9D11-%EC%8B%9C%EA%B0%81-%EC%9E%84%EB%B2%A0%EB%94%A9-visual-embedding">https://lsoovmee-rhino.tistory.com/entry/VISION-%EB%B9%84%EC%A0%84-%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9D%84-%EC%9C%84%ED%95%9C-%EB%94%A5%EB%9F%AC%EB%8B%9D11-%EC%8B%9C%EA%B0%81-%EC%9E%84%EB%B2%A0%EB%94%A9-visual-embedding</a></li>
<li>[D] Contrastive Learning (SimCLR, MoCo) vs. Non-Contrastive Pretext Tasks (Rotation, Inpainting): When/Why Does One Approach Dominate? : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/1k0fbvq/d_contrastive_learning_simclr_moco_vs/</li>
<li>The Illustrated SimCLR Framework - Amit Chaudhary, https://amitness.com/posts/simclr</li>
<li>Paper explained: A Simple Framework for Contrastive Learning of Visual Representations, https://towardsdatascience.com/paper-explained-a-simple-framework-for-contrastive-learning-of-visual-representations-6a2a63bfa703/</li>
<li>What are the differences between SimCLR and MoCo, two popular contrastive learning frameworks? - Milvus, https://milvus.io/ai-quick-reference/what-are-the-differences-between-simclr-and-moco-two-popular-contrastive-learning-frameworks</li>
<li>Easily Explained: Momentum Contrast for Unsupervised Visual Representation Learning | by Hey Amit | Medium, https://medium.com/@heyamit10/easily-explained-momentum-contrast-for-unsupervised-visual-representation-learning-10bf51e08fb1</li>
<li>Momentum Contrast for Unsupervised Visual Representation Learning - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf</li>
<li>[1911.05722] Momentum Contrast for Unsupervised Visual Representation Learning - arXiv, https://arxiv.org/abs/1911.05722</li>
<li>Momentum Contrastive Learning with Enhanced Negative Sampling and Hard Negative Filtering - arXiv, https://arxiv.org/html/2501.16360v1</li>
<li>Momentum Contrast for Unsupervised Visual Representation Learning. - 5cents, https://hammer-wang.github.io/5cents/representation-learning/moco/</li>
<li>Training a Siamese Network with Caffe - Stack Overflow, https://stackoverflow.com/questions/43074206/training-a-siamese-network-with-caffe</li>
<li>Choosing a margin for contrastive loss in a siamese network - Stack Overflow, https://stackoverflow.com/questions/54892607/choosing-a-margin-for-contrastive-loss-in-a-siamese-network</li>
<li>A Gentle Introduction to Siamese Neural Networks Architecture - ProjectPro, https://www.projectpro.io/article/siamese-neural-networks/718</li>
<li>Simple and Asymmetric Graph Contrastive Learning without Augmentations - OpenReview, <a href="https://openreview.net/forum?id=UK8mA3DRnb&amp;noteId=0POENE2MzY">https://openreview.net/forum?id=UK8mA3DRnb¬eId=0POENE2MzY</a></li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>