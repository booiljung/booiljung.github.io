<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:EGoT (Embodied Graph-of-Thought, 체화된 사고 그래프, 2025-11-03)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>EGoT (Embodied Graph-of-Thought, 체화된 사고 그래프, 2025-11-03)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">작업 계획 (Task Planning)</a> / <span>EGoT (Embodied Graph-of-Thought, 체화된 사고 그래프, 2025-11-03)</span></nav>
                </div>
            </header>
            <article>
                <h1>EGoT (Embodied Graph-of-Thought, 체화된 사고 그래프, 2025-11-03)</h1>
<p>2025-12-07, G30DR</p>
<h2>1.  서론: 구현된 인공지능의 진화와 협업의 난제</h2>
<p>인공지능(AI) 기술의 발전은 텍스트와 이미지를 처리하는 수동적인 정보 처리 시스템을 넘어, 물리적 세계와 직접 상호작용하며 과업을 수행하는 구현된 지능(Embodied Intelligence)으로 진화하고 있다. 특히 거대 언어 모델(LLM)의 추론 능력과 로봇 제어 기술이 결합된 비전-언어-행동(Vision-Language-Action, VLA) 모델의 등장은 로봇 공학의 패러다임을 근본적으로 변화시키고 있다.1 RT-2나 OpenVLA와 같은 최신 VLA 모델들은 대규모 인터넷 데이터셋을 통해 학습된 일반 상식과 추론 능력을 바탕으로, 복잡한 언어 명령을 이해하고 이를 구체적인 로봇의 행동 토큰(action token)으로 변환하는 데 성공했다.3 이는 로봇이 사전에 정의되지 않은 환경에서도 범용적인 작업을 수행할 수 있는 가능성을 열어주었다.</p>
<p>그러나 이러한 단일 로봇 중심의 성공에도 불구하고, 다중 로봇(multi-robot) 시스템이나 양팔 로봇(bimanual robot)과 같은 다중 신체(multi-embodiment) 환경으로의 확장은 여전히 심각한 기술적 장벽에 부딪히고 있다.3 기존의 자귀회귀적(autoregressive) VLA 모델들은 본질적으로 순차적인 데이터 처리에 특화되어 있어, 동시 다발적으로 발생하는 다중 에이전트 간의 상호작용과 역할 분담을 효과적으로 모델링하지 못한다. 예를 들어, 두 개의 로봇 팔이 협력하여 무거운 물체를 들어 올리거나, 한 팔이 물체를 고정하고 다른 팔이 조작을 가하는 상황에서 기존 모델들은 각 팔의 독립적인 역할과 타이밍을 동기화하는 데 실패하거나, 구조적으로 유효하지 않은 행동 시퀀스를 생성하는 경향을 보인다.3</p>
<p>이러한 배경 속에서 등장한 ‘신체 전이 학습(Embodiment Transfer Learning, ET-VLA)’ 프레임워크와 그 핵심 기술인 ’구현된 사고 그래프(Embodied Graph-of-Thought, EGoT)’는 다중 신체 제어의 난제를 해결할 획기적인 접근법으로 주목받고 있다. 본 보고서는 EGoT의 이론적 토대와 기술적 아키텍처, 그리고 이를 통한 로봇 협업 능력의 향상 효과를 심도 있게 분석한다. 특히, 기존의 선형적 사고 사슬(Chain-of-Thought) 방식이 가진 한계를 극복하고, 그래프 기반의 구조적 사고가 어떻게 로봇의 물리적 협업을 가능케 하는지에 대해 면밀히 고찰한다.</p>
<pre><code class="language-mermaid">graph TD
    subgraph "AI Evolution"
        A["Traditional AI"] -- "Passive Processing" --&gt; B["Text &amp; Image Processing"]
        B -- "Vision-Language-Action (VLA)" --&gt; C["Embodied Intelligence"]
        C -- "Action Token Generation" --&gt; D["Single Robot Control"]
    end

    subgraph "Current Challenge"
        D -- "Expansion Attempt" --&gt; E{"Multi-Embodiment Environment"}
        E -- "Bimanual / Multi-robot" --&gt; F["Technical Barriers"]
        F -- "Lack of Synchronization" --&gt; G["Failure in Collaboration"]
    end

    subgraph "Solution"
        G -- "Proposed Framework" --&gt; H["ET-VLA &amp; EGoT"]
    end

    style C fill:#e1f5fe,stroke:#01579b
    style F fill:#ffebee,stroke:#b71c1c
    style H fill:#e8f5e9,stroke:#1b5e20
</code></pre>
<pre><code class="language-mermaid">sequenceDiagram
    participant User as "User Command"
    participant RobotL as "Left Arm"
    participant RobotR as "Right Arm"
    participant Object as "Heavy Object"

    User-&gt;&gt;RobotL: "Lift the object together"
    User-&gt;&gt;RobotR: "Lift the object together"
    
    Note over RobotL, RobotR: "Standard VLA (Autoregressive)"
    
    RobotL-&gt;&gt;Object: "Grasps Object"
    RobotL-&gt;&gt;Object: "Lifts Immediately (No Sync)"
    
    Note right of RobotR: "Robot R is still approaching"
    
    Object--&gt;&gt;RobotL: "Too heavy for one arm -&gt; Slips/Falls"
    RobotR-&gt;&gt;Object: "Grasps empty air (Late)"
    
    Note over RobotL, RobotR: "Result: Collaboration Failure"
</code></pre>
<h2>2.  VLA 모델의 한계와 다중 신체 환경의 복잡성 분석</h2>
<pre><code class="language-mermaid">graph TD

    subgraph "Limitation 2: Linear Chain-of-Thought"
        CoT["Linear CoT"] -- "Sequential Logic" --&gt; Step1["Action A -&gt; Action B"]
        Step1 -- "Fails to represent" --&gt; Concurrency{"Concurrency &amp; Dependency"}
        Concurrency -- "Robot A waits for Robot B" --&gt; Failure["Synchronization Failure"]
    end

    subgraph "Limitation 1: Autoregressive Bias"
        Data["Single Arm Data (OXE, Droid)"] --&gt; Training["Pretraining VLA"]
        Training -- "Single Subject Bias" --&gt; Inference["Inference on Multi-Robot"]
        Inference -- "Treats as One Giant Subject" --&gt; Error1["Wrong Token Count"]
        Inference -- "No Entity Distinction" --&gt; Error2["Physical Conflict"]
    end

    style Error1 fill:#ffccbc
    style Error2 fill:#ffccbc
    style Failure fill:#ffccbc
</code></pre>
<h3>2.1  자귀회귀적 모델의 순차성 편향과 데이터 의존성</h3>
<p>현대의 VLA 모델은 대부분 트랜스포머(Transformer) 아키텍처에 기반을 두고 있으며, 이는 입력된 비전 및 언어 토큰에 이어지는 행동 토큰을 확률적으로 예측하는 방식으로 작동한다.3 이러한 방식은 텍스트 생성이나 단일 로봇의 순차적 작업 수행에는 효과적이지만, 병렬적 처리가 필수적인 다중 로봇 시스템에는 치명적인 한계를 드러낸다. 연구에 따르면, 현재의 VLA 모델들은 사전 학습 데이터(pretraining data)에 극도로 의존적인 경향을 보인다.3</p>
<p>문제는 현재 가용한 오픈 소스 로봇 데이터셋(예: OXE, Droid 등)의 대다수가 단일 로봇 팔(unimanual)의 조작 데이터로 구성되어 있다는 점이다.3 모델이 학습 과정에서 ’하나의 주체’가 작업을 수행하는 데이터만 접했기 때문에, 이를 다중 로봇 환경에 그대로 전이(transfer)할 경우 모델은 두 로봇을 별개의 주체로 인식하지 못하고 하나의 거대한, 그러나 조화롭지 못한 단일 주체처럼 제어하려 든다. 이는 결과적으로 각 로봇에게 할당되어야 할 행동 토큰의 개수를 잘못 예측하거나, 두 로봇의 행동이 서로 충돌하는 물리적 모순을 초래한다.</p>
<pre><code class="language-mermaid">graph TD
    subgraph "Training Phase"
        Data["Open Source Data (OXE, Droid)"]
        Nature["Unimanual (Single Arm) Data"]
        Data -- "Dominant Input" --&gt; Nature
        Nature -- "Train VLA" --&gt; Model["Pretrained VLA Model"]
    end

    subgraph "Inference Error Mechanism"
        Model -- "Transfer to Dual-Arm" --&gt; Perception["Perception"]
        Perception -- "Misinterpretation" --&gt; Hallucination["Perceives as One Giant Body"]
        
        Hallucination -- "Result 1" --&gt; TokenErr["Incorrect Action Token Count"]
        Hallucination -- "Result 2" --&gt; Conflict["Physical Contradiction (Self-Collision)"]
    end

    style Hallucination fill:#ffccbc,stroke:#d84315
</code></pre>
<h3>2.2  선형적 사고 사슬(Chain-of-Thought)의 구조적 결함</h3>
<p>거대 언어 모델의 추론 능력을 로봇 계획(planning)에 적용하려는 시도로 ‘사고 사슬(Chain-of-Thought, CoT)’ 기법이 널리 활용되어 왔다. CoT-VLA와 같은 연구들은 복잡한 작업을 일련의 하위 목표(sub-goals)로 분해하여 로봇의 행동을 유도한다.3 그러나 CoT는 본질적으로 선형적인(linear) 구조를 가진다. “A를 하고, 그 다음 B를 한다“는 식의 사고는 인과관계가 명확한 단일 작업에는 적합하지만, “로봇 A가 물체를 잡고 있는 동안, 로봇 B가 뚜껑을 연다“와 같은 동시성(concurrency)과 상호 의존성(interdependency)이 존재하는 협업 작업에는 부적합하다.</p>
<p>선형적 계획법은 다중 에이전트 간의 시간적 동기화(synchronization)와 공간적 조정(coordination)을 표현하는 데 한계가 있다. 예를 들어, 병렬로 진행되던 두 작업이 특정 시점에서 만나야 하거나, 한 로봇의 작업 실패가 다른 로봇의 대기 상태를 유발해야 하는 상황을 선형적 텍스트나 시퀀스로는 효율적으로 기술할 수 없다. 따라서 다중 신체 제어를 위해서는 단순한 시퀀스를 넘어선 고차원적인 위상 구조, 즉 그래프(Graph) 기반의 접근이 필수적으로 요구된다.</p>
<pre><code class="language-mermaid">sequenceDiagram
    participant Planner as "Linear CoT Planner"
    participant RobotA as "Robot A (Hold)"
    participant RobotB as "Robot B (Open Lid)"
    
    Note over Planner: "Linear Plan: 1. Hold -&gt; 2. Open"
    
    Planner-&gt;&gt;RobotA: "Step 1: Grasp Object"
    RobotA--&gt;&gt;Planner: "Done"
    
    Planner-&gt;&gt;RobotB: "Step 2: Open Lid"
    
    Note right of RobotB: "Constraint: Object must be held FIRMLY"
    
    rect rgb(255, 200, 200)
    Note over RobotA, RobotB: "CoT Failure: Temporal Gap"
    RobotA-&gt;&gt;RobotA: "Wait for next command (Loosens grip?)"
    RobotB-&gt;&gt;RobotB: "Tries to open (Object moves)"
    end
    
    RobotB--&gt;&gt;Planner: "Action Failed"
    Planner-&gt;&gt;Planner: "Confusion / Halt"
</code></pre>
<h2>3.  ET-VLA 프레임워크: 다중 신체 전이를 위한 통합 아키텍처</h2>
<p>ET-VLA(Embodiment Transfer Learning for Vision-Language-Action Models)는 단일 로봇 데이터로 학습된 VLA 모델을 다중 로봇 환경으로 효율적으로 전이시키기 위해 제안된 포괄적인 프레임워크이다. 이 프레임워크는 크게 두 가지 핵심 기술적 기둥, 즉 ’합성 지속 사전 학습(Synthetic Continued Pretraining, SCP)’과 ’구현된 사고 그래프(Embodied Graph-of-Thought, EGoT)’로 구성된다.3 이 두 기술은 상호 보완적으로 작동하여 모델의 하위 레벨 제어 능력과 상위 레벨 계획 능력을 동시에 향상시킨다.</p>
<pre><code class="language-mermaid">graph LR

subgraph "Foundation Layer: SCP"
    SCP["Synthetic Continued Pretraining (SCP)"]
    Function1["Warm-up with Synthetic Data"]
    Function2["Learn Hardware Interface"]
    Function3["Correct Token Count Learning"]

    SCP -- "Installs" --&gt; Driver["Hardware Driver Role"]
    SCP --- Function1
    SCP --- Function2
    SCP --- Function3
end

subgraph "High-Level Layer: EGoT"
    EGoT["Embodied Graph-of-Thought (EGoT)"]
    Role1["Complex Task Planning"]
    Role2["Collaboration Logic"]
    Role3["Graph-based Control"]

    EGoT -- "Installs" --&gt; OS["OS / Task Manager Role"]
    EGoT --- Role1
    EGoT --- Role2
    EGoT --- Role3
end

Driver -- "Provides Basic Control" --&gt; OS
OS -- "Executes Complex Mission" --&gt; Action["Intelligent Action"]


style SCP fill:#fff9c4,stroke:#fbc02d
style EGoT fill:#d1c4e9,stroke:#512da8
</code></pre>
<h3>3.1  합성 지속 사전 학습(SCP): 행동 토큰의 정렬과 비용 효율화</h3>
<p>새로운 신체(embodiment)에 모델을 적응시키는 가장 직관적인 방법은 해당 신체로 수집된 대량의 데이터를 사용하여 미세 조정(fine-tuning)하는 것이다. 그러나 실제 다중 로봇 시스템을 운용하여 인간의 시연 데이터(human demonstration)를 수집하는 것은 막대한 비용과 시간이 소요되는 작업이다. SCP는 이러한 데이터 수집의 병목 현상을 해결하기 위해 합성 데이터(synthetic data)를 활용한 ‘웜업(warm-up)’ 전략을 채택한다.2</p>
<p>SCP의 핵심 목표는 모델이 새로운 신체의 물리적 특성과 제어 인터페이스에 익숙해지도록 하는 것이다. 구체적으로 SCP는 다음과 같은 두 가지 중요한 기능을 수행한다.</p>
<p>첫째, 비용 절감 및 데이터 효율성 증대이다. 시뮬레이션이나 생성 모델을 통해 만들어진 합성 데이터를 활용함으로써, 실제 로봇을 구동하지 않고도 모델을 초기 학습시킬 수 있다. 이는 데이터 수집 비용을 획기적으로 낮출 뿐만 아니라, 희귀한 상황에 대한 데이터를 생성하여 모델의 강건성을 높이는 데 기여한다.3</p>
<p>둘째, 정확한 행동 및 토큰 수 학습이다. 다중 로봇 제어에서는 각 로봇에게 전달되어야 할 제어 신호(토큰)의 길이와 형식이 단일 로봇과는 다르다. SCP는 모델이 이러한 구조적 차이를 인지하고, 올바른 개수의 행동 토큰을 생성하도록 유도한다. 이는 자귀회귀적 모델이 흔히 범하는 토큰 생성 오류를 미연에 방지하는 기초 훈련 과정이라 할 수 있다.3 실제로 RLBench 벤치마크에서 BridgeData V2로 훈련된 OpenVLA 모델에 SCP를 적용한 결과, 성공률이 6.49%에서 37.66%로 급증한 사례는 SCP가 단순한 보조 수단이 아니라 모델의 기본 성능을 보장하는 필수 요소임을 시사한다.3</p>
<pre><code class="language-mermaid">graph TD
    subgraph "Cost Problem"
        Real["Real Robot Operation"] -- "Expensive/Slow" --&gt; BottleNeck["Data Collection Bottleneck"]
    end

    subgraph "SCP Solution"
        Sim["Simulation / Gen-Model"] -- "Generates" --&gt; SynthData["Synthetic Data"]
        SynthData -- "Diverse Scenarios" --&gt; Rare["Rare/Edge Cases"]
        
        SynthData -- "Warm-up Training" --&gt; VLA["VLA Model"]
        
        VLA -- "Learn Interface" --&gt; Token["Correct Token Structure"]
        VLA -- "Learn Physics" --&gt; Dynamics["Basic Control Dynamics"]
    end

    subgraph "Outcome"
        Dynamics -- "Zero-shot / Few-shot" --&gt; RealWorld["Real World Adaptation"]
    end

    style BottleNeck fill:#ffcdd2
    style SynthData fill:#e1bee7
    style RealWorld fill:#c8e6c9
</code></pre>
<h3>3.2  아키텍처 내에서의 상호작용</h3>
<p>ET-VLA 프레임워크 내에서 SCP와 EGoT는 계층적(hierarchical)으로 기능한다. SCP가 모델의 ’하드웨어 드라이버’를 설치하는 과정이라면, EGoT는 그 위에 설치되는 ‘운영 체제’ 또는 ’작업 관리자’의 역할을 수행한다. SCP를 통해 물리적 제어의 기초를 다진 모델은 EGoT를 통해 비로소 복잡한 협업 논리를 이해하고 실행할 수 있게 된다. 이 두 구성 요소의 결합은 VLA 모델이 단순한 명령 추종자를 넘어, 상황을 판단하고 조율하는 지능형 에이전트로 거듭나게 한다.5</p>
<pre><code class="language-mermaid">graph TD
    subgraph "ET-VLA System Stack"
        direction BT
        
        subgraph "Hardware Layer"
            HW["Physical Robots (Bimanual / Multi-agent)"]
        end
        
        subgraph "SCP Layer (Driver)"
            Driver["SCP: Hardware Driver"]
            Func1["Identify Embodiment Physics"]
            Func2["Align Token Counts"]
            
            HW --&gt; Driver
            Driver --- Func1
            Driver --- Func2
        end
        
        subgraph "EGoT Layer (OS)"
            OS["EGoT: Operating System"]
            Logic1["Task Scheduler (Graph)"]
            Logic2["Conflict Manager"]
            
            Driver --&gt; OS
            OS --- Logic1
            OS --- Logic2
        end
        
        subgraph "Application Layer"
            App["Complex Collaborative Tasks"]
            OS --&gt; App
        end
    end
    
    style Driver fill:#fff9c4,stroke:#fbc02d
    style OS fill:#d1c4e9,stroke:#512da8
</code></pre>
<h2>4.  구현된 사고 그래프(EGoT)의 기술적 심층 분석</h2>
<p>EGoT는 ET-VLA 프레임워크의 두뇌에 해당하며, 복잡한 작업을 실행 가능한 그래프(actionable graph)로 변환하여 각 로봇에게 역할을 할당하고 순서를 제어한다. 이 기술은 VLA 모델이 각 신체의 기능적 역할(functionalities and roles)을 명확히 구분할 수 있도록 돕는다.6</p>
<pre><code class="language-mermaid">graph TD
    subgraph "EGoT Structure"
        Input["Vision &amp; Language Input"] --&gt; GraphGen["Generate EGoT Graph"]
        
        subgraph "Node Types"
            ST["Sub-task Node: Unit Action"]
            EN["End Node: Robot Finish"]
            CN["Complete Node: Mission Success"]
        end
        
        GraphGen -- "Decomposes into" --&gt; ST
        ST -- "Sequence Ends" --&gt; EN
        EN -- "All Agents Done" --&gt; CN
        
        subgraph "Edge Dependencies"
            Causal["Causal Dependency"] -- "Physical Necessity" --&gt; ST
            Sync["Collaborative Synchronization"] -- "Timing Coordination" --&gt; ST
        end
    end

    subgraph "Execution"
        ST -- "Generate" --&gt; Token["Discrete Action Tokens"]
        Token -- "Real-time" --&gt; Interpretation["Interpretability &amp; Debugging"]
    end

    style ST fill:#b2dfdb
    style EN fill:#ffecb3
    style CN fill:#c8e6c9
</code></pre>
<h3>4.1  노드(Nodes)의 유형학 및 의미론적 정의</h3>
<p>EGoT에서 그래프의 노드는 단순한 작업의 나열이 아니라, 실행 상태와 주체를 명확히 하는 제어점(control points)으로 기능한다. 연구 문헌에 따르면, EGoT는 작업의 정교한 제어를 위해 다음과 같은 특수 노드 유형들을 정의하고 있다.3</p>
<table><thead><tr><th><strong>노드 유형 (Node Type)</strong></th><th><strong>정의 및 기능 (Definition &amp; Function)</strong></th><th><strong>시스템 내 역할 및 중요성 (Role &amp; Significance)</strong></th></tr></thead><tbody>
<tr><td><strong>Sub-task Node</strong></td><td>개별 로봇이 수행해야 할 구체적인 단위 작업 (예: “왼팔로 사과 집기”).</td><td>작업을 물리적으로 실행 가능한 단위로 분해하며, 특정 신체(embodiment)에 바인딩된다.</td></tr>
<tr><td><strong>End Node</strong></td><td>특정 로봇에게 할당된 일련의 작업 시퀀스가 완료되었음을 표시.</td><td>해당 로봇의 활동 종료를 명시적으로 선언하여, 시스템 리소스를 해제하거나 다음 명령 대기 상태로 전환.</td></tr>
<tr><td><strong>Complete Node</strong></td><td>전체 워크플로우의 성공적인 완료를 의미하는 그래프의 종단점(Terminal Node).</td><td>모든 에이전트의 작업이 성공적으로 수행되었고, 협업 목표가 달성되었음을 최종 검증.</td></tr>
</tbody></table>
<p>이러한 노드의 세분화는 VLA 모델이 “작업이 끝났다“는 추상적인 개념을 “로봇 A의 작업 종료“와 “전체 미션의 완료“로 구체적으로 구분하게 해준다. 이는 다중 로봇 시스템에서 흔히 발생하는 ’조기 종료(early termination)’나 ‘무한 대기(deadlock)’ 문제를 방지하는 데 결정적인 역할을 한다.</p>
<pre><code class="language-mermaid">mindmap
  root(("EGoT Task Decomposition"))
    Command("User: 'Make Coffee'")
      SubTask("Sub-task Nodes")
        Action1("Left: Grasp Cup")
        Action2("Right: Press Button")
        Action3("Left: Place Cup")
      EndSignals("End Nodes")
        End1("Left Arm Task Done")
        End2("Right Arm Task Done")
      Terminal("Complete Node")
        Success("Mission Success Verified")
        Resource("Release System Resources")
</code></pre>
<h3>4.2  엣지(Edges)와 의존성 위상수학</h3>
<p>EGoT의 그래프 구조에서 엣지(Edge)는 노드 간의 논리적, 물리적 의존성(dependency)을 나타낸다. 이는 방향성 비순환 그래프(DAG) 형태를 띠며, 작업의 흐름을 제어한다.3 엣지는 단순한 선후 관계를 넘어 다음과 같은 복합적인 관계를 표현한다.</p>
<ul>
<li>
<p><strong>인과적 의존성(Causal Dependency):</strong> “물체를 잡아야(Node A) 들어 올릴 수 있다(Node B)“와 같은 물리적 필연성을 강제한다.</p>
</li>
<li>
<p>협업적 동기화(Collaborative Synchronization): “로봇 A가 문을 열 때까지(Node X), 로봇 B는 대기해야 한다(Node Y)“와 같은 에이전트 간의 타이밍을 조율한다.</p>
</li>
</ul>
<p>이러한 엣지 구조는 VLA 모델이 사전 학습 데이터에 내재된 단일 로봇의 선형적 바이어스를 극복하고, 다중 로봇 간의 입체적인 협업 관계를 이해하도록 돕는다.3</p>
<pre><code class="language-mermaid">graph TD
    Start(("Start"))

    subgraph "Causal Dependency (Physical Logic)"
        NodeA["Node A: Grasp Object"]
        NodeB["Node B: Lift Object"]
        NodeA -- "Must happen before" --&gt; NodeB
    end

    subgraph "Collaborative Synchronization (Timing)"
        NodeX["Robot A: Unlock Door"]
        NodeY["Robot B: Wait at Door"]
        NodeZ["Robot B: Open Door"]
        
        NodeX -- "Signal: Unlocked" --&gt; Sync{{"Synchronization Point"}}
        NodeY -- "Signal: Ready" --&gt; Sync
        Sync -- "Condition Met" --&gt; NodeZ
    end

    Start --&gt; NodeA
    Start --&gt; NodeX
    Start --&gt; NodeY

    style Sync fill:#fff9c4,stroke:#fbc02d,stroke-width:2px
</code></pre>
<h3>4.3  해석 가능한 실시간 행동 생성 및 추론 과정</h3>
<p>EGoT는 모델의 내부 연산 과정을 사람이 이해할 수 있는 형태로 시각화하는 ’해석 가능성(Interpretability)’을 제공한다.3 기존의 블랙박스 모델과 달리, EGoT는 현재 모델이 그래프상의 어느 노드를 실행 중이며, 어떤 의존성을 대기하고 있는지를 명확히 보여준다. 이는 ’실시간 행동 해석기(Real-Time Action Interpreter)’로서 기능하며, 디버깅과 신뢰성 확보에 중요한 이점을 제공한다.</p>
<p>추론 과정(Inference Process)에서 모델은 시각적 관찰(visual observation)과 언어 명령(language instruction)을 입력받아 EGoT 그래프를 생성하고, 이를 순회하며 각 단계에 맞는 이산적 행동 토큰(discrete action tokens)을 생성한다.3 이 과정에서 그래프는 동적인 가이드라인 역할을 하며, 모델이 문맥을 잃지 않고 긴 호흡의 작업을 수행할 수 있도록 지지한다.</p>
<pre><code class="language-mermaid">graph TD
    Input["Input: Video + Text"] --&gt; Obs["Visual Observation"]
    
    subgraph "EGoT Inference Cycle"
        Obs --&gt; Graph["Generate/Update EGoT Graph"]
        Graph -- "Traverse Nodes" --&gt; Current["Identify Current Node"]
        Current -- "Check Dependencies" --&gt; Valid["Valid Action?"]
        
        Valid -- "Yes" --&gt; TokenGen["Discrete Action Token Generation"]
        Valid -- "No (Wait)" --&gt; Pause["Hold / Wait State"]
    end
    
    TokenGen --&gt; Robot["Robot Execution"]
    Robot -- "World State Change" --&gt; Obs
    
    subgraph "Interpretability"
        Graph -.-&gt; Visualize["Visualizer: Show Active Node"]
        Visualize -.-&gt; Debug["Human Debugging"]
    end

    style TokenGen fill:#b3e5fc
    style Pause fill:#ffecb3
</code></pre>
<h3>4.4  EGoT와 기존 방법론(CoT, ReAct)의 비교 우위</h3>
<p>EGoT의 우수성은 기존의 최신 방법론들과의 비교를 통해 더욱 명확해진다.</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>Chain-of-Thought (CoT) / CoT-VLA</strong></th><th><strong>Embodied Graph-of-Thought (EGoT)</strong></th></tr></thead><tbody>
<tr><td><strong>기본 구조</strong></td><td>선형적 시퀀스 (Linear Sequence)</td><td>방향성 비순환 그래프 (DAG)</td></tr>
<tr><td><strong>추론 방식</strong></td><td>자귀회귀적, 단계별(Step-by-step) 생성</td><td>구조적, 병렬적(Parallel) 관계 모델링</td></tr>
<tr><td><strong>다중 에이전트 대응</strong></td><td>에이전트 간 역할 구분이 모호하며, 병렬 작업 표현 불가</td><td>노드를 통해 각 신체(embodiment)의 역할과 기능을 명확히 구분</td></tr>
<tr><td><strong>동기화 능력</strong></td><td>시점 간 의존성 표현의 한계로 협업 타이밍을 놓치기 쉬움</td><td>엣지를 통해 명시적인 동기화 및 의존성 제약 조건 설정 가능</td></tr>
<tr><td><strong>주요 적용 대상</strong></td><td>단일 로봇 팔의 복잡한 추론 작업</td><td>양팔 로봇, 다중 로봇 협업 시스템</td></tr>
</tbody></table>
<p>표에서 알 수 있듯이, EGoT는 구조적 차원에서 CoT의 한계를 극복하고 다중 신체 제어에 최적화된 형태를 띠고 있다. 이는 단순한 성능 향상을 넘어, 복잡계(complex system)로서의 로봇 작업을 모델링하는 새로운 표준을 제시한다.</p>
<pre><code class="language-mermaid">graph TD
    subgraph "Chain-of-Thought (Linear)"
        A1["Start"] --&gt; A2["Step 1"]
        A2 --&gt; A3["Step 2"]
        A3 --&gt; A4["Step 3"]
        A4 --&gt; A5["End"]
        style A1 stroke-dasharray: 5 5
    end

    subgraph "Embodied Graph-of-Thought (DAG)"
        B1["Start"] --&gt; B2["Robot A: Task 1"]
        B1 --&gt; B3["Robot B: Task 1"]
        B2 -- "Wait for B" --&gt; B4["Sync Point"]
        B3 --&gt; B4
        B4 --&gt; B5["Robot A: Task 2"]
        B4 --&gt; B6["Robot B: Task 2"]
        B5 --&gt; B7["Complete"]
        B6 --&gt; B7
        style B4 fill:#f8bbd0,stroke:#c2185b,stroke-width:2px
    end

    A5 -.-&gt; B1
    linkStyle 8 stroke-width:0px,fill:none
</code></pre>
<h2>5.  실험적 검증: 시뮬레이션 벤치마크 분석</h2>
<p>ET-VLA와 EGoT의 성능은 다양한 시뮬레이션 환경에서의 엄격한 테스트를 통해 검증되었다. 연구진은 VirtualHome, ALFRED, RLBench 등 대표적인 로봇 학습 벤치마크를 활용하여 모델의 범용성과 강건성을 평가하였다.1</p>
<pre><code class="language-mermaid">graph TD
    subgraph "Baseline Performance"
        OpenVLA["Standard OpenVLA"]
        BridgeData["Trained on BridgeData V2"]
        Result1["Success Rate: 6.49%"]
        
        OpenVLA --- BridgeData --&gt; Result1
    end

    subgraph "ET-VLA Performance"
        SCP_App["Apply SCP (Warm-up)"]
        EGoT_App["Apply EGoT (Structure)"]
        Result2["Success Rate: 37.66%"]
        
        Result1 -- "Add SCP &amp; EGoT" --&gt; SCP_App
        SCP_App --&gt; EGoT_App --&gt; Result2
    end
    
    subgraph "Impact"
        Validation["Validated on VirtualHome/ALFRED"]
        Robust["2x Success vs ReAct"]
        
        Result2 -- "Also" --&gt; Validation --&gt; Robust
    end

    style Result1 fill:#ffccbc
    style Result2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:4px
</code></pre>
<h3>5.1  VirtualHome 및 ALFRED: 장기 과제 수행 능력</h3>
<p>VirtualHome과 ALFRED는 가정 환경을 모사한 시뮬레이터로, 커피 타기나 청소하기와 같은 일상적이지만 긴 시퀀스가 필요한 작업(long-horizon tasks)을 다룬다. 이러한 환경은 로봇이 단순히 물체를 집는 것을 넘어, 환경과 상호작용하며 상태를 변화시켜야 하므로 높은 수준의 계획 능력을 요구한다.</p>
<p>실험 결과, ET-VLA는 다양한 지시어의 규모와 유형, 그리고 비정상성(non-stationarity)을 포함하는 시나리오에서 탁월한 견고함을 보여주었다.1 특히 목표 성공률(goal success rate)과 실행 효율성(execution efficiency) 측면에서 ReAct와 같은 기존의 LLM 기반 작업 계획 방식들을 일관되게 능가하였다.1 일부 실험(WAH-NL 데이터셋 등)에서는 베이스라인 대비 거의 두 배에 가까운 성공률(예: ReAct 31% vs. 제안 모델 61%)을 기록하기도 했다.6 이는 EGoT가 복잡한 지시를 논리적인 그래프로 분해하여 수행함으로써, 실행 도중 발생할 수 있는 오류를 최소화하고 효율적인 경로를 생성함을 입증한다.</p>
<pre><code class="language-mermaid">graph TD
    Input["Task: 'Clean the living room'"] --&gt; EGoT_Graph["Generate Complex Graph"]
    
    subgraph "Execution Loop"
        Node1["Sub-task: Pick up trash"] --&gt; Check1{"Success?"}
        
        Check1 -- "Yes" --&gt; Node2["Sub-task: Open Bin"]
        Check1 -- "No (Dropped)" --&gt; Retry["Local Recovery: Re-grasp"]
        Retry --&gt; Check1
        
        Node2 --&gt; Check2{"Success?"}
        Check2 -- "Yes" --&gt; Node3["Sub-task: Throw in"]
        
        Node3 --&gt; Terminal["Goal Reached"]
    end
    
    subgraph "Baseline Comparison"
        ReAct["ReAct Method"] -- "Linear Execution" --&gt; Error["Context Lost after 10+ steps"]
        Error --&gt; Fail["Mission Fail (31%)"]
    end
    
    Terminal -- "High Success Rate (61%)" --&gt; Result["Robust Performance"]
    
    style Retry fill:#b3e5fc,stroke:#0277bd
    style Fail fill:#ffcdd2
</code></pre>
<h3>5.2  RLBench: 정밀 조작과 협업의 검증</h3>
<p>RLBench는 로봇 조작 기술을 평가하는 데 널리 사용되는 벤치마크이다. 특히 이번 연구에서는 기존의 단일 로봇 작업을 확장하여 양팔 로봇 조작(bimanual manipulation)을 다루는 ‘RLBench2’ 환경을 구축하거나 활용한 것으로 파악된다.5 이 환경은 두 팔의 정밀한 좌표 제어와 협응을 요구한다.</p>
<p>앞서 언급했듯이, SCP가 적용되지 않은 OpenVLA 모델은 6.49%라는 매우 저조한 성공률을 보였으나, SCP와 EGoT가 적용된 모델은 37.66% 이상의 성공률을 기록하며 비약적인 성능 향상을 입증하였다.3 이는 시뮬레이션상의 물리 엔진과 상호작용하는 과정에서 EGoT가 제공하는 구조적 가이던스가 얼마나 중요한지를 보여주는 정량적 지표이다.</p>
<h2>6.  실제 로봇(Real-World) 환경에서의 실증 연구</h2>
<p>시뮬레이션 결과만으로는 물리적 세계의 불확실성과 하드웨어적 제약을 완벽히 반영할 수 없다. 따라서 연구진은 실제 양팔 로봇 시스템을 구축하여 EGoT의 효용성을 최종 검증하였다.6</p>
<pre><code class="language-mermaid">graph TD
    subgraph "Real-World Experiment Setup"
        Tasks["6 Real-world Tasks"] -- "Cooking, Assembly, Cleanup" --&gt; Exec
        Embodiments["3 Different Robot Bodies"] -- "Prevents Overfitting" --&gt; Exec
        Exec["Execution"]
    end

    subgraph "Performance Comparison"
        Base["Baseline (OpenVLA)"] -- "Fails on Noise/Conflict" --&gt; LowPerf["Low Success"]
        Ours["ET-VLA (Ours)"] -- "Maintains Role/Sync" --&gt; HighPerf["High Success"]
        
        LowPerf -- "Improvement" --&gt; Diff["+53.2% Performance Gain"] --&gt; HighPerf
    end

    Exec --&gt; Base
    Exec --&gt; Ours
    
    style Diff fill:#fff9c4,stroke:#fbc02d,stroke-dasharray: 5 5
</code></pre>
<h3>6.1  실험 설정 및 방법론</h3>
<p>실험은 세 가지 서로 다른 형태의 양팔 로봇 신체(embodiments)를 대상으로 진행되었다. 이는 모델이 특정 하드웨어 구성에 과적합(overfitting)되지 않고, 다양한 신체 구조로 전이될 수 있는지를 확인하기 위함이다. 실험 대상 작업은 요리 보조, 물체 조립, 정리 정돈 등 두 팔의 협업이 필수적인 6가지의 실제 세계 작업(real-world tasks)으로 구성되었다.4</p>
<h3>6.2  성능 평가 및 비교 결과</h3>
<p>실제 로봇 실험 결과는 시뮬레이션 결과보다 더욱 고무적이다. ET-VLA 모델은 베이스라인인 OpenVLA 대비 평균 53.2% 이상의 성능 향상을 기록하였다.2</p>
<p>이러한 압도적인 성능 차이는 EGoT가 실제 환경의 노이즈와 변수 속에서도 로봇 간의 역할을 명확히 유지시켜 주었기 때문이다. 예를 들어, 한 팔이 물체를 놓쳤을 때 그래프 구조상 다음 단계로 넘어가지 않고 대기하거나 재시도하는 유연성을 보였을 것으로 추론된다. 이는 OpenVLA가 실패 상황에서도 맹목적으로 다음 토큰을 생성하여 작업을 망치는 것과 대조적이다. 즉, EGoT는 로봇에게 ’상황 판단 능력’을 부여하여 실전성을 극대화하였다.</p>
<pre><code class="language-mermaid">graph TD
    Event{{"Event: Robot A drops object"}}

    subgraph "Baseline: OpenVLA"
        Event -- "Blindly continues" --&gt; NextToken["Generate Next Token"]
        NextToken -- "Predicts assuming success" --&gt; ActionFail["Action on empty air"]
        ActionFail --&gt; Disaster["Task Failure / Chaos"]
    end

    subgraph "Ours: EGoT"
        Event -- "Visual Feedback" --&gt; GraphCheck["Check Node Status"]
        GraphCheck -- "Condition NOT Met" --&gt; Logic{{"Decision Logic"}}
        
        Logic -- "Retry" --&gt; ReGrasp["Re-execute Grasp Node"]
        Logic -- "Wait" --&gt; RobotB_Wait["Robot B Holds Position"]
        
        ReGrasp --&gt; Success["Resume Workflow"]
    end

    style Disaster fill:#ef9a9a
    style Success fill:#a5d6a7
</code></pre>
<h2>7.  관련 연구 생태계 및 기술적 위상</h2>
<p>EGoT는 고립된 기술이 아니라, 로봇 공학의 최신 트렌드인 검색 증강 생성(RAG) 및 효율적 토큰화 기술들과 궤를 같이하며 발전하고 있다.</p>
<pre><code class="language-mermaid">graph TD
    subgraph "Robotic AI Ecosystem"
        Center["Enhanced Embodied Intelligence"]
        
        subgraph "Reasoning Engine"
            EGoT["EGoT"] -- "Decompose &amp; Assemble" --&gt; Logic["Structural Logic"]
        end
        
        subgraph "Memory Engine"
            RAEA["RAEA (Retrieval-Augmented)"] -- "Retrieve &amp; Reference" --&gt; Memory["External Experience"]
        end
        
        subgraph "Efficiency Engine"
            Tiny["TinyVLA / FAST"] -- "Token Efficiency" --&gt; Speed["Fast Inference"]
        end
        
        Logic --&gt; Center
        Memory --&gt; Center
        Speed --&gt; Center
        
        EGoT -. "Complementary" .- RAEA
        EGoT -. "Aligns with" .- Tiny
    end
    
    style Center fill:#e0f7fa,stroke:#006064
</code></pre>
<h3>7.1  검색 증강 구현 에이전트(RAEA)와의 비교</h3>
<p>EGoT와 유사한 시기에 연구된 ’검색 증강 구현 에이전트(Retrieval-Augmented Embodied Agents, RAEA)’는 외부의 정책 메모리 뱅크(policy memory bank)를 활용하여 로봇의 성능을 높이는 접근법이다.8 RAEA가 과거의 성공적인 경험이나 유사한 시나리오를 ’검색(Retrieve)’하여 현재의 문제 해결에 참고하는 방식이라면, EGoT는 현재 주어진 문제를 구조적으로 ’분해(Decompose)’하고 논리적으로 ’조립(Assemble)’하는 추론 중심의 방식이다.</p>
<p>흥미로운 점은 두 기술이 상호 배타적이지 않다는 것이다. 일부 문헌에서는 두 기술이 구현된 지능의 성능을 높이는 상호 보완적인 기술로 언급된다.1 RAEA가 데이터의 부족을 외부 기억으로 채운다면, EGoT는 제어의 복잡성을 구조적 사고로 해결한다. 향후 이 두 기술이 결합된다면, ’기억’과 ’추론’을 모두 갖춘 더욱 강력한 로봇 지능이 탄생할 수 있을 것이다.</p>
<pre><code class="language-mermaid">graph TD
    User["User Instruction"] --&gt; Split{{"System Approach"}}
    
    subgraph "RAEA (Memory-based)"
        Split -- "Retrieve" --&gt; DB[("Policy Memory Bank")]
        DB -- "Search Similar Past" --&gt; Match["Retrieve Successful Trajectory"]
        Match -- "Imitate/Adapt" --&gt; Action1["Action Generation"]
    end
    
    subgraph "EGoT (Reasoning-based)"
        Split -- "Decompose" --&gt; Reason["Structure Logic"]
        Reason -- "Construct Graph" --&gt; DAG["Dependency Graph (DAG)"]
        DAG -- "Solve Constraints" --&gt; Action2["Action Generation"]
    end
    
    subgraph "Future Integration"
        Action1 -.-&gt; Hybrid["Cognitive Architecture"]
        Action2 -.-&gt; Hybrid
        Hybrid -- "Memory + Reasoning" --&gt; Super["Super-Embodied Intelligence"]
    end
    
    style DB fill:#e0f7fa,stroke:#006064
    style DAG fill:#e1bee7,stroke:#4a148c
</code></pre>
<h3>7.2  효율성과 경량화 연구와의 연계</h3>
<p>TinyVLA나 FAST와 같은 연구들은 거대한 VLA 모델을 경량화하고 추론 속도를 높이는 데 주력한다.9 EGoT 또한 명확한 작업 그래프를 통해 불필요한 탐색 공간을 줄이고 효율적인 계획을 수립한다는 점에서 이러한 ‘효율성 추구’ 트렌드에 부합한다. 특히 SCP를 통해 모델이 불필요하게 긴 토큰을 생성하지 않도록 학습시키는 것은 전체적인 시스템의 연산 효율을 높이는 데 기여한다.</p>
<h2>8.  결론 및 향후 전망: 협업 로봇 지능의 미래</h2>
<p>본 보고서는 ET-VLA 프레임워크와 그 핵심인 EGoT 기술이 다중 신체 로봇 제어의 난제를 해결하는 데 있어 어떠한 기여를 했는지 종합적으로 분석하였다.</p>
<p>첫째, 구조적 혁신이다. EGoT는 기존의 선형적 사고 사슬을 그래프 기반의 다차원적 사고로 확장함으로써, 다중 로봇 간의 복잡한 상호작용과 협업을 수학적으로 모델링하는 데 성공하였다.</p>
<p>둘째, 전이 학습의 실용성 입증이다. SCP와 결합된 EGoT는 값비싼 데이터 수집 없이도 단일 로봇 데이터를 다중 로봇 시스템으로 효과적으로 전이시킬 수 있음을 증명하였으며, 이는 로봇 AI의 상용화 속도를 가속화할 것이다.</p>
<p>셋째, 실증적 성능 우위이다. 시뮬레이션과 실제 환경을 아우르는 광범위한 실험에서 OpenVLA 대비 53.2%라는 압도적인 성능 향상을 보인 것은 EGoT가 단순한 이론적 제안을 넘어 실질적인 솔루션임을 확인시켜 준다.</p>
<p>향후 연구는 현재의 양팔 로봇을 넘어, 드론과 지상 로봇의 협업이나 이기종(heterogeneous) 로봇 군집 제어와 같은 더욱 복잡한 시스템으로 EGoT를 확장하는 방향으로 나아갈 것이다. 또한, 연구진이 관련 코드를 오픈 소스로 공개할 예정임에 따라5, 전 세계 연구자들에 의해 EGoT를 기반으로 한 다양한 파생 연구와 응용 사례가 폭발적으로 증가할 것으로 전망된다. EGoT는 로봇이 인간의 언어를 이해하고, 물리적 세계에서 지능적으로 협업하는 미래를 앞당기는 핵심적인 인지 아키텍처(Cognitive Architecture)로 자리매김할 것이다.</p>
<pre><code class="language-mermaid">graph TD
    subgraph "Heterogeneous System"
        Drone["Agent A: Drone (Aerial View)"]
        Dog["Agent B: Robot Dog (Ground Action)"]
    end
    
    subgraph "EGoT Extension"
        Global["Global Graph Planner"]
        
        Global -- "Assign: Scout" --&gt; NodeA["Node: Scan Area"]
        Global -- "Assign: Fetch" --&gt; NodeB["Node: Go to Target"]
        
        NodeA -- "Edge: Visual Data" --&gt; Sync{{"Data Sync"}}
        Sync -- "Coordinates" --&gt; NodeB
    end
    
    Drone -.-&gt; NodeA
    Dog -.-&gt; NodeB
    
    NodeA --&gt; CompleteA["Drone Land"]
    NodeB --&gt; CompleteB["Item Delivered"]
    
    style Global fill:#c8e6c9,stroke:#1b5e20
    style Sync fill:#ffecb3,stroke:#ff6f00
</code></pre>
<h2>9. 참고 자료</h2>
<ol>
<li>Retrieval-Augmented Embodied Agents | Request PDF - ResearchGate, https://www.researchgate.net/publication/385342326_Retrieval-Augmented_Embodied_Agents</li>
<li>π₀: A Vision-Language-Action Flow Model for General Robot Control - ResearchGate, https://www.researchgate.net/publication/395364425_p_A_Vision-Language-Action_Flow_Model_for_General_Robot_Control</li>
<li>Embodiment Transfer Learning for Vision-Language-Action Models - arXiv, https://arxiv.org/html/2511.01224v1</li>
<li>CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models, https://www.researchgate.net/publication/394511717_CoT-VLA_Visual_Chain-of-Thought_Reasoning_for_Vision-Language-Action_Models</li>
<li>Embodiment Transfer Learning for Vision-Language-Action Models - ResearchGate, https://www.researchgate.net/publication/397232748_Embodiment_Transfer_Learning_for_Vision-Language-Action_Models</li>
<li>Code as Policies: Language Model Programs for Embodied Control - ResearchGate, https://www.researchgate.net/publication/372122437_Code_as_Policies_Language_Model_Programs_for_Embodied_Control</li>
<li>RLBench: The Robot Learning Benchmark &amp; Learning Environment - ResearchGate, https://www.researchgate.net/publication/339347623_RLBench_The_Robot_Learning_Benchmark_Learning_Environment</li>
<li>Retrieval-Augmented Embodied Agents - arXiv, https://arxiv.org/html/2404.11699v1</li>
<li>TinyVLA: Toward Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation | Request PDF - ResearchGate, https://www.researchgate.net/publication/389282468_TinyVLA_Towards_Fast_Data-Efficient_Vision-Language-Action_Models_for_Robotic_Manipulation</li>
<li>FAST: Efficient Action Tokenization for Vision-Language-Action Models | Request PDF - ResearchGate, https://www.researchgate.net/publication/395364524_FAST_Efficient_Action_Tokenization_for_Vision-Language-Action_Models</li>
<li>[2511.01224] Embodiment Transfer Learning for Vision-Language-Action Models - arXiv, https://arxiv.org/abs/2511.01224</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>