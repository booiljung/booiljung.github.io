<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:CoT (Chain-of-Thought, 연쇄 사고 추론, 2022-01-28)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>CoT (Chain-of-Thought, 연쇄 사고 추론, 2022-01-28)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">작업 계획 (Task Planning)</a> / <span>CoT (Chain-of-Thought, 연쇄 사고 추론, 2022-01-28)</span></nav>
                </div>
            </header>
            <article>
                <h1>CoT (Chain-of-Thought, 연쇄 사고 추론, 2022-01-28)</h1>
<h2>1.  대규모 언어 모델 추론의 패러다임 전환</h2>
<p>대규모 언어 모델(Large Language Models, LLM)의 발전 초기, 모델의 능력은 주로 방대한 텍스트 데이터로부터 학습한 패턴을 기반으로 유창한 문장을 생성하는 데 집중되어 있었다. 이러한 모델들은 정보 검색, 요약, 번역과 같은 과제에서 인상적인 성능을 보였으나, 여러 단계의 논리적 사고를 요구하는 복잡한 추론(multi-step reasoning) 문제에 직면했을 때는 명백한 한계를 드러냈다.1</p>
<p>표준 프롬프팅(Standard Prompting) 방식, 즉 모델에게 질문과 정답 쌍의 예시 몇 개를 제공하고 유사한 문제의 답을 직접적으로 요구하는 접근법은 이러한 한계를 극복하지 못했다. 예를 들어, 여러 계산 단계가 포함된 산술 문제나 복잡한 인과관계를 파악해야 하는 상식 추론 문제에서, 모델은 중간 과정을 생략하고 직관에 의존하여 답변을 생성하려는 경향을 보였다. 이로 인해 논리적 비약이나 사실적 오류가 빈번하게 발생했으며, 이는 모델이 단순히 정답에 도달하는 데 필요한 계산을 수행하는 데 어려움을 겪는다는 것을 보여주었다.3</p>
<p>이러한 배경 속에서 ’연쇄 사고 추론(Chain-of-Thought, CoT)’이라는 새로운 패러다임이 등장했다. CoT는 LLM에게 최종적인 답을 즉시 요구하는 대신, 문제 해결에 이르는 과정을 단계별로 상세히 설명하도록 유도하는 혁신적인 프롬프트 엔지니어링 기법이다.3 이 접근법은 모델이 복잡한 문제를 더 작고 관리 가능한 하위 문제로 분해하고, 각 단계를 순차적으로 해결하도록 함으로써 추론의 정확성과 신뢰도를 획기적으로 향상시켰다. CoT의 등장은 LLM 연구의 초점을 모델이 단순히 ’무엇을 아는가(knowledge)’에서 ’어떻게 생각하는가(reasoning process)’로 전환시키는 결정적인 계기가 되었다. 이는 LLM을 수동적인 정보 검색 도구가 아닌, 능동적인 문제 해결 파트너로 인식하게 만든 패러다임의 전환을 의미한다.</p>
<p>본 보고서는 연쇄 사고 추론의 기본 원리와 작동 메커니즘을 심층적으로 탐구하는 것을 목표로 한다. CoT의 개념적 기원과 그 효과가 모델의 규모에 따라 ’창발적 능력(emergent ability)’으로 발현되는 현상을 분석하고, 다양한 구현 전략을 살펴본다. 나아가 산술, 상식, 기호 추론 등 주요 과제에서의 성능을 정량적으로 분석하며, 동시에 환각(hallucination), 계산 비용 증가 등 CoT가 가진 명백한 한계와 최신 모델에서의 효용성 변화를 비판적으로 고찰한다. 마지막으로, CoT의 한계를 극복하기 위해 제안된 자기 일관성(Self-Consistency), 사고의 나무(Tree-of-Thoughts), 그리고 사고의 그래프(Graph-of-Thoughts)와 같은 후속 연구로 이어지는 지적 계보를 체계적으로 탐구함으로써, LLM 추론 기술의 현재를 진단하고 미래 발전 방향을 조망하고자 한다.</p>
<h2>2.  연쇄 사고 추론(CoT)의 기본 원리</h2>
<pre><code class="language-mermaid">graph LR
    subgraph "표준 프롬프팅 (Standard Prompting)"
        A1["질문 (Input)"] --&gt; B1["모델의 직관/패턴 매칭"]
        B1 --&gt; C1["최종 정답 (Output)"]
        style B1 fill:#ffcccc,stroke:#333,stroke-width:2px
    end

    subgraph "연쇄 사고 추론 (Chain-of-Thought)"
        A2["질문 (Input)"] --&gt; B2["문제 분해 (Decomposition)"]
        B2 --&gt; C2["중간 추론 단계 1 (Reasoning Step 1)"]
        C2 --&gt; D2["중간 추론 단계 2 (Reasoning Step 2)"]
        D2 --&gt; E2["논리적 연결 확인"]
        E2 --&gt; F2["최종 정답 (Output)"]
        style C2 fill:#ccffcc,stroke:#333,stroke-width:2px
        style D2 fill:#ccffcc,stroke:#333,stroke-width:2px
    end
</code></pre>
<h3>2.1 CoT의 정의 및 작동 메커니즘</h3>
<p>연쇄 사고 추론(Chain-of-Thought, CoT)은 대규모 언어 모델이 복잡한 문제에 대해 최종 답변을 도출하기 전에, 문제 해결에 이르는 일련의 중간 추론 단계(intermediate reasoning steps)를 자연어로 명시적으로 생성하도록 유도하는 프롬프트 엔지니어링 기법으로 정의된다.3 이 기법의 핵심은 모델이 인간의 문제 해결 방식과 유사하게 사고 과정을 외현화하도록 만드는 데 있다.</p>
<p>CoT의 작동 메커니즘은 크게 두 가지 핵심 원리에 기반한다.</p>
<ol>
<li>
<p><strong>문제 분해(Problem Decomposition)</strong>: CoT는 LLM이 하나의 복잡하고 거대한 문제를 여러 개의 더 작고 관리하기 쉬운 하위 단계로 나누도록 유도한다. 이는 인간이 어려운 과제에 직면했을 때, 전체 문제를 한 번에 해결하려 하지 않고 논리적인 순서에 따라 단계를 나누어 접근하는 인지 과정을 모방한 것이다.2 각 단계는 이전 단계의 결과를 바탕으로 진행되므로, 모델은 전체 문제의 복잡성에 압도되지 않고 개별 하위 문제에 계산 자원을 집중할 수 있다.</p>
</li>
<li>
<p><strong>단계별 처리(Step-by-Step Processing)</strong>: 분해된 각 하위 문제는 순차적으로 처리된다. 여기서 중요한 점은 한 단계의 출력이 다음 단계의 입력을 위한 풍부한 컨텍스트(context)가 된다는 것이다.4 예를 들어, 다단계 수학 문제를 풀 때, 첫 번째 단계에서 계산된 중간값은 텍스트 형태로 생성되어 프롬프트의 일부가 되고, 모델은 이 값을 참조하여 다음 단계의 계산을 수행한다. 이 과정을 통해 논리적 흐름이 일관되게 유지되며, 중간 계산 오류로 인한 최종 결과의 실패 가능성이 크게 줄어든다.</p>
</li>
</ol>
<p>트랜스포머(Transformer) 아키텍처에 기반한 LLM은 본질적으로 다음 토큰을 예측하는 자기회귀(auto-regressive) 모델로서, 복잡한 다단계 추론을 순전히 내부의 잠재 공간(latent space)에서만 처리하는 데에는 구조적 한계를 가진다.10 인간이 복잡한 암산을 할 때 종이에 중간 계산 과정을 적어두어 인지적 부담을 더는 것과 유사하게, CoT는 모델이 자신의 ’생각’을 컨텍스트 창에 명시적으로 ’기록’하도록 강제한다.11 이렇게 생성된 중간 추론 단계는 다음 토큰 예측을 위한 강력하고 명시적인 컨텍스트로 작용한다. 이는 사실상 LLM의 제한된 ’작업 기억(working memory)’을 프롬프트의 컨텍스트 창으로 확장시키는 효과적인 전략으로 기능하며, 모델이 더 긴 의존성을 가진 복잡한 문제를 해결할 수 있는 근본적인 이유가 된다.</p>
<h3>2.2 표준 프롬프팅과의 비교</h3>
<p>CoT의 혁신성은 기존의 표준 프롬프팅 방식과 비교할 때 더욱 명확해진다.</p>
<ul>
<li>
<p><strong>표준 프롬프팅 (Standard Prompting)</strong>: 이 방식은 모델에게 <code>[질문] -&gt; [정답]</code> 형식으로 구성된 입출력 쌍 예시(few-shot exemplars)를 제공한다. 모델은 이 예시들을 통해 과제의 형식을 학습하지만, 추론 과정 자체를 학습하지는 않는다. 따라서 새로운 질문에 대해 모델은 중간 논리 단계를 생략하고 직관적으로, 혹은 패턴 매칭에 의존하여 답변을 생성하려 한다.5 이는 간단한 문제에는 효과적일 수 있으나, 다단계 추론이 필요한 문제에서는 실패할 확률이 높다.</p>
</li>
<li>
<p><strong>CoT 프롬프팅 (Chain-of-Thought Prompting)</strong>: 반면, CoT 프롬프팅은 <code>[질문] -&gt; [사고의 연쇄] -&gt; [정답]</code>이라는 세 부분으로 구성된 예시를 제공한다. 모델은 질문에 답하기 전에 왜, 그리고 어떻게 그런 결론에 도달했는지를 보여주는 ’사고의 연쇄’를 생성하는 패턴을 학습한다.12 이 명시적인 추론 과정은 모델이 자신의 논리를 검토하고, 이를 바탕으로 최종 답변을 도출하도록 안내하는 역할을 한다.</p>
</li>
</ul>
<p>이 차이는 다음의 구체적인 예시를 통해 극명하게 드러난다.5</p>
<p>질문:</p>
<p>로저는 테니스공 5개를 가지고 있다. 그는 테니스공 3개가 들어있는 캔을 2개 더 샀다. 그는 지금 총 몇 개의 테니스공을 가지고 있는가?</p>
<p>표준 프롬프팅 예시:</p>
<p>Q: 로저는 테니스공 5개를 가지고 있다. 그는 테니스공 3개가 들어있는 캔을 2개 더 샀다. 그는 지금 총 몇 개의 테니스공을 가지고 있는가?</p>
<p>A: 정답은 11이다.</p>
<p>CoT 프롬프팅 예시:</p>
<p>Q: 로저는 테니스공 5개를 가지고 있다. 그는 테니스공 3개가 들어있는 캔을 2개 더 샀다. 그는 지금 총 몇 개의 테니스공을 가지고 있는가?</p>
<p>A: 로저는 처음에 5개의 공을 가지고 있었다. 3개의 공이 들어있는 캔 2개는 총 6개의 공이다. 5 + 6 = 11이다. 정답은 11이다.</p>
<p>표준 프롬프팅은 모델이 ‘5’, ‘3’, ’2’라는 숫자들을 보고 직관적으로 답을 추측하게 만드는 반면, CoT 프롬프팅은 문제를 ’추가로 구매한 공의 수 계산’과 ’기존 공의 수와 더하기’라는 두 단계로 분해하도록 명확하게 안내한다. 이처럼 CoT는 단순한 입력-출력 매핑을 넘어, 문제 해결을 위한 절차적 지식(procedural knowledge)을 모델이 따르도록 유도하는 강력한 메커니즘을 제공한다.</p>
<pre><code class="language-mermaid">graph TD
    Q["질문: 로저는 공 5개 보유. 3개입 캔 2개 추가 구매. 총 개수는?"] --&gt; Step1["사고 단계 1: 추가 구매량 계산"]
    Step1 --&gt; Logic1["'3개 들어있는 캔 2개' -&gt; 3 * 2 = 6"]
    Logic1 --&gt; Context["컨텍스트에 중간 결과(6) 저장"]
    Context --&gt; Step2["사고 단계 2: 전체 합계 계산"]
    Step2 --&gt; Logic2["'기존 5개 + 추가 6개' -&gt; 5 + 6 = 11"]
    Logic2 --&gt; A["최종 답변: 정답은 11이다."]
    
    style Logic1 fill:#e1f5fe,stroke:#01579b
    style Logic2 fill:#e1f5fe,stroke:#01579b
    style A fill:#fff9c4,stroke:#fbc02d
</code></pre>
<h2>3.  CoT의 기원과 창발적 능력(Emergent Ability)</h2>
<h3>3.1 Wei et al. (2022)의 독창적 연구</h3>
<p>연쇄 사고 추론(CoT)의 개념을 학술적으로 정립하고 그 효과를 체계적으로 입증한 연구는 Google Research의 Jason Wei 등이 2022년에 발표한 논문 “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models“이다.5 이 논문은 프롬프트 엔지니어링 분야의 기념비적인 연구로 평가받으며, LLM의 추론 능력을 이끌어내는 새로운 방법론을 제시했다. 연구팀은 LLM이 복잡한 문제를 해결할 때 중간 추론 단계를 생성하도록 유도하는 것이 최종 답변의 정확도를 극적으로 향상시킨다는 사실을 다양한 벤치마크 실험을 통해 증명했다. 이 연구는 CoT가 단순한 프롬프팅 기법을 넘어, LLM의 잠재된 추론 능력을 활성화시키는 핵심적인 열쇠임을 보여주었다.</p>
<h3>3.2 모델 규모(Scale)와 추론 능력의 상관관계</h3>
<p>Wei 등의 연구에서 가장 중요한 발견 중 하나는 CoT의 효과가 모델의 규모(scale), 즉 파라미터 수에 따라 질적으로 달라진다는 점이다. 실험 결과, 상대적으로 작은 모델에서는 CoT 프롬프팅이 기대만큼의 성능 향상을 보이지 않거나, 심지어는 표준 프롬프팅보다 성능을 저하시키는 경우도 관찰되었다.1 작은 모델들은 CoT를 적용했을 때 겉보기에는 유창하지만 실제로는 논리적 결함이 많거나 비약이 심한 추론 과정을 생성하는 경향이 있었다. 이는 해당 모델들이 복잡한 추론 구조를 이해하고 생성할 만큼 충분한 용량(capacity)이나 학습이 이루어지지 않았음을 시사한다.</p>
<h3>3.3 창발적 속성(Emergent Ability)으로서의 CoT</h3>
<p>CoT의 추론 능력 향상 효과는 특정 규모 이상의 거대 모델(논문에서는 약 1000억 개 파라미터 이상을 기준으로 제시)에서만 뚜렷하고 일관되게 나타났는데, 연구팀은 이를 ’창발적 속성(Emergent Ability)’이라고 명명했다.3 창발적 속성이란, 시스템의 규모나 복잡성이 특정 임계점을 돌파할 때 이전에는 존재하지 않았던 새로운 능력이 비선형적으로 발현되는 현상을 의미한다. CoT의 경우가 바로 이에 해당한다.</p>
<p>모델 규모가 임계점을 넘어서면, CoT 프롬프트를 통해 이전에는 해결 불가능했던 복잡한 추론 과제를 해결하는 능력이 갑자기 나타난다. 이는 모델이 대규모 데이터셋을 학습하는 과정에서 단순히 개별적인 사실 관계를 암기하는 것을 넘어, 문제 해결에 필요한 추상적인 추론 패턴이나 알고리즘 자체를 내재적으로 학습했기 때문으로 해석된다.16 CoT 프롬프트는 이렇게 잠재되어 있던 추론 능력을 활성화시키는 일종의 ‘트리거’ 역할을 하는 것이다.</p>
<p>이러한 현상은 스케일링 곡선(scaling curve)을 통해 시각적으로 확인할 수 있다. 표준 프롬프팅의 경우, 모델의 크기가 증가함에 따라 성능이 완만하고 예측 가능하게 향상되는 경향을 보인다. 반면, CoT 프롬프팅을 적용했을 때는 특정 모델 규모 지점에서 성능이 급격하게 치솟는 ’상전이(phase transition)’와 같은 모습을 보인다.16</p>
<p>CoT가 ’창발적 능력’이라는 사실은 LLM의 발전이 단순히 기존 능력의 양적(quantitative) 향상에 그치는 것이 아니라, 특정 규모에서 질적(qualitative)인 변화를 겪으며 완전히 새로운 능력을 획득할 수 있음을 보여주는 강력한 증거이다. 이는 LLM 개발에 있어 ’규모의 법칙(scaling laws)’이 단순한 성능 예측을 넘어, 새로운 지능의 발현을 예측하는 중요한 단서가 될 수 있음을 시사하며, 인공 일반 지능(AGI) 연구에도 깊은 함의를 던진다.</p>
<h2>4.  CoT 프롬프팅의 유형 및 구현 전략</h2>
<p>연쇄 사고 추론(CoT)은 그 기본 원리를 바탕으로 다양한 구현 전략으로 발전해왔다. 각 전략은 프롬프트 구성 방식과 인간의 개입 정도에 따라 특징이 나뉘며, 이는 프롬프트 엔지니어링의 과제가 ’최적의 예시를 수동으로 제작하는 것’에서 ’모델이 스스로 최적의 추론 과정을 생성하도록 유도하는 일반적인 메커니즘을 설계하는 것’으로 진화해왔음을 보여준다.</p>
<pre><code class="language-mermaid">graph TD
    subgraph "Few-Shot CoT"
        F1["사람이 작성한 질문-사고-정답 예시 준비"] --&gt; F2["프롬프트에 예시 삽입 (In-context Learning)"]
        F2 --&gt; F3["모델이 예시의 추론 패턴 모방"]
    end

    subgraph "Zero-Shot CoT"
        Z1["질문 입력"] --&gt; Z2["트리거 문구 추가: '단계별로 생각해보자'"]
        Z2 --&gt; Z3["내재된 단계별 설명 패턴 활성화"]
    end

    subgraph "Auto-CoT"
        A1["질문 데이터셋 클러스터링"] --&gt; A2["대표 질문 샘플링"]
        A2 --&gt; A3["Zero-Shot으로 추론 과정 자동 생성"]
        A3 --&gt; A4["생성된 예시로 Few-Shot 프롬프트 구성"]
    end
</code></pre>
<h3>4.1 Few-Shot CoT</h3>
<p>Few-Shot CoT는 Wei et al. (2022)의 원 논문에서 제안된 가장 기본적인 형태의 CoT 구현 방식이다. 이 전략은 프롬프트 내에 2개에서 8개 사이의 완전한 <code>[질문]-[사고의 연쇄]-[정답]</code> 형식의 예시(exemplar)를 포함시킨다.5 모델은 제공된 예시들을 ’문맥 내 학습(in-context learning)’의 형태로 받아들여, 그 안에 담긴 추론의 구조와 패턴을 학습한다. 그리고 새로운 질문이 주어졌을 때, 학습한 패턴을 모방하여 유사한 방식으로 단계별 추론 과정을 생성하고 최종 답변을 도출한다.11</p>
<p>이 방식의 성공은 제공되는 예시의 수와 품질에 크게 의존한다. 잘 구성된 소수의 예시만으로도 모델의 추론 능력을 효과적으로 이끌어낼 수 있지만, 예시를 수동으로 제작해야 하는 번거로움이 있으며, 특정 유형의 예시에 편향될 경우 모델의 일반화 성능이 저하될 수 있다는 단점이 있다.</p>
<h3>4.2 Zero-Shot CoT</h3>
<p>Few-Shot CoT의 예시 제작 부담을 해결하기 위해 Kojima et al. (2022)은 Zero-Shot CoT라는 획기적으로 간단한 방법을 제안했다.14 이 방식은 별도의 추론 예시를 프롬프트에 포함시키지 않고, 사용자의 질문 바로 뒤에 “단계별로 생각해보자 (Let’s think step by step)“와 같은 간단한 지시 문구를 추가하는 것만으로 CoT 추론을 유도한다.1</p>
<p>이 단순한 문구가 효과를 발휘하는 이유는, LLM이 방대한 텍스트 데이터를 사전 훈련하는 과정에서 문제에 대한 단계별 설명이나 풀이 과정을 제시하는 수많은 문서를 학습했기 때문이다. “단계별로 생각해보자“는 지시는 모델 내부에 이미 잠재된 ‘단계별 설명’ 패턴을 활성화시키는 일종의 ‘트리거’ 역할을 한다.20 이 방법은 준비가 매우 간편하여 누구나 쉽게 CoT를 시도해볼 수 있다는 장점이 있지만, 생성되는 추론의 품질이나 일관성이 Few-Shot CoT에 비해 다소 불안정할 수 있다.7 이 발견은 CoT 능력이 특정 예시에만 의존하는 것이 아니라, LLM 내부에 이미 일반화된 능력으로 존재하며, 특정 ’마법의 단어’를 통해 그 동작 모드를 활성화할 수 있음을 보여준 중요한 사례이다.</p>
<h3>4.3 Auto-CoT (Automatic CoT)</h3>
<p>Auto-CoT는 Few-Shot CoT의 안정성과 Zero-Shot CoT의 편리함을 결합하려는 시도에서 탄생한 자동화된 프롬프팅 전략이다.20 이 기법의 핵심은 CoT 예시를 만드는 과정을 인간이 아닌 LLM 자신에게 맡기는 것이다.</p>
<p>Auto-CoT의 프로세스는 일반적으로 두 단계로 구성된다 7:</p>
<ol>
<li>
<p><strong>질문 클러스터링 (Question Clustering)</strong>: 먼저, 주어진 문제 데이터셋의 질문들을 의미적 유사성에 따라 여러 개의 클러스터로 그룹화한다. 이는 생성될 예시들이 특정 유형에 치우치지 않고 다양한 추론 패턴을 대표하도록 하기 위함이다.</p>
</li>
<li>
<p><strong>데모 샘플링 및 생성 (Demonstration Sampling and Generation)</strong>: 각 클러스터에서 대표적인 질문을 하나씩 샘플링한 후, Zero-Shot CoT(“단계별로 생각해보자”)를 사용하여 해당 질문에 대한 추론 과정과 답변을 LLM이 자동으로 생성하도록 한다. 이렇게 생성된 <code>[질문]-[사고의 연쇄]-[정답]</code> 쌍들이 최종적으로 Few-Shot CoT 프롬프트를 구성하는 예시로 활용된다.</p>
</li>
</ol>
<p>Auto-CoT는 CoT 예시 생성 과정을 자동화함으로써 인간의 노력을 극적으로 줄이고, 다양한 문제 도메인에 쉽게 적용할 수 있는 확장성을 제공한다. 이는 프롬프트 엔지니어링이 특정 문제 해결법을 직접 주입하는 ’지식 주입(knowledge injection)’에서, 모델이 스스로 문제 해결법을 찾도록 유도하는 ’메타-러닝(meta-learning)’의 방향으로 발전하고 있음을 보여주는 대표적인 사례이다.</p>
<h2>5.  주요 추론 과제별 성능 분석</h2>
<p>연쇄 사고 추론(CoT)의 효과는 특정 유형의 과제, 특히 순차적이고 논리적인 절차를 요구하는 문제에서 두드러지게 나타난다. 이는 CoT가 단순한 텍스트 생성을 넘어, LLM이 내부적으로 추상적인 문제 해결 절차를 시뮬레이션하도록 돕기 때문이다. 주요 추론 과제인 산술, 상식, 기호 추론 영역에서 CoT가 표준 프롬프팅 대비 어떤 성능 향상을 보이는지 구체적인 벤치마크 결과를 통해 분석한다.</p>
<h3>5.1 산술 추론 (Arithmetic Reasoning)</h3>
<p>산술 추론, 특히 여러 단계의 계산이 필요한 수학 단어 문제(math word problems)는 CoT의 효과가 가장 극적으로 나타나는 영역이다. GSM8K, SVAMP, MAWPS와 같은 표준 벤치마크에서 CoT는 기존 방법론들을 압도하는 성능을 기록했다.5</p>
<p>가장 대표적인 사례는 GSM8K 벤치마크에 대한 PaLM 540B 모델의 실험이다. 표준 프롬프팅을 사용했을 때 PaLM 540B의 문제 해결률은 18%에 불과했다. 그러나 단 8개의 CoT 예시를 포함한 프롬프트를 사용하자 해결률은 58%로 폭발적으로 증가했다.5 이는 단순히 성능이 향상된 것을 넘어, 검증기(verifier)를 사용하여 별도의 파인튜닝을 거친 GPT-3 175B 모델의 최고 성능(55%)마저 능가하는 결과였다.12 이러한 성공은 CoT가 복잡한 계산 문제를 논리적인 단계로 정확하게 분해하고, 각 단계에서 도출된 중간 결과값을 다음 계산에 오류 없이 전달하는 데 매우 효과적임을 증명한다.15</p>
<p>아래 표는 GSM8K 벤치마크에서 CoT의 압도적인 성능을 요약하여 보여준다.</p>
<table><thead><tr><th>모델</th><th>프롬프팅 방식</th><th>해결률 (%)</th></tr></thead><tbody>
<tr><td>GPT-3 175B (Finetuned)</td><td>Verifier</td><td>55%</td></tr>
<tr><td>PaLM 540B</td><td>Standard Prompting</td><td>18%</td></tr>
<tr><td>PaLM 540B</td><td>Chain-of-Thought</td><td>58%</td></tr>
</tbody></table>
<p>표 1: GSM8K 벤치마크 성능 비교. CoT 프롬프팅을 적용한 PaLM 540B 모델이 추가적인 파인튜닝 없이도 기존 최고 성능을 경신했음을 보여준다.12</p>
<h3>5.2 상식 추론 (Commonsense Reasoning)</h3>
<p>상식 추론은 세상에 대한 암묵적인 지식을 바탕으로 주어진 상황을 이해하고 논리적인 결론을 도출하는 능력을 평가한다. CSQA(CommonsenseQA), StrategyQA와 같은 벤치마크에서 CoT는 모델의 규모가 커질수록 표준 프롬프팅보다 일관되게 더 나은 성능을 보였다.13</p>
<p>특히 주목할 만한 결과는 ‘스포츠 경기 상황 이해(Sports Understanding)’ 과제에서 나왔다. 이 과제는 복잡한 경기 규칙과 상황의 변화를 이해하고 다음 상황을 예측하는 능력을 요구한다. PaLM 540B 모델과 CoT 프롬프팅을 결합했을 때, 모델은 95%의 정확도를 기록했는데, 이는 해당 분야에 대한 지식을 가진 인간 스포츠 애호가의 평균 정확도(84%)를 뛰어넘는 놀라운 결과였다.16 이는 CoT가 단순히 계산 능력뿐만 아니라, 복잡한 시나리오의 인과관계를 추적하고 논리적으로 추론하는 고차원적인 능력까지 향상시킬 수 있음을 시사한다.</p>
<h3>5.3 기호 추론 (Symbolic Reasoning)</h3>
<p>기호 추론은 단어의 의미가 아닌, 주어진 규칙에 따라 기호나 문자를 조작하는 능력을 평가하는 과제이다. ’마지막 글자 이어붙이기(Last letter concatenation)’나 ’동전 뒤집기(Coin flip)’와 같은 추상적인 규칙 기반 과제에서 CoT는 표준 프롬프팅이 실패하는 지점에서 탁월한 성능을 보였다.13</p>
<p>표준 프롬프팅은 이러한 과제에서 단어의 의미론적 정보에 이끌려 규칙을 무시하는 경향이 있다. 그러나 CoT는 “첫 번째 단어의 마지막 글자는 ’n’이다. 두 번째 단어의 마지막 글자는 ’d’이다. 따라서 결과는 ’nd’이다.“와 같이, 문제를 명시적인 절차로 분해하여 모델이 기호 조작 연산에 집중하도록 만든다.</p>
<p>더욱 중요한 것은 CoT의 일반화 능력이다. 훈련 예시보다 더 많은 단계(예: 더 많은 단어의 마지막 글자를 이어붙이는)를 요구하는 도메인 외(Out-of-Domain, OOD) 테스트에서, 표준 프롬프팅은 거의 완벽하게 실패했다. 반면, CoT를 적용한 모델은 문제의 복잡성이 증가함에 따라 성능이 자연스럽게 확장되는 성공적인 스케일링 곡선을 보였다.13 이는 CoT가 특정 예시를 암기하는 것이 아니라, 문제 해결을 위한 일반화된 ’알고리즘’을 학습하고 있음을 강력하게 시사한다. CoT의 효과는 단순히 ’더 많은 텍스트를 생성’하는 것에서 오는 것이 아니라, 텍스트 생성을 통해 LLM이 본래 취약했던 ’절차적’이고 ‘기호적인’ 연산을 수행할 수 있는 ’실행 환경(execution environment)’을 제공하는 데 그 본질이 있다.</p>
<h2>6.  CoT의 한계와 비판적 고찰</h2>
<p>연쇄 사고 추론(CoT)은 LLM의 추론 능력에 있어 중요한 돌파구를 마련했지만, 만병통치약은 아니다. 실제 적용 과정에서 여러 한계와 문제점들이 드러났으며, 특히 모델 기술이 발전함에 따라 그 효용성에 대한 비판적 재검토가 이루어지고 있다. CoT는 특정 세대의 LLM이 가진 ’추론 능력 부족’이라는 문제를 해결하기 위한 ’과도기적 기술’의 성격을 띠며, 모델 자체가 진화함에 따라 그 가치가 변화하고 있다.</p>
<h3>6.1 환각(Hallucination) 문제</h3>
<p>CoT의 가장 심각한 문제 중 하나는 ‘환각’, 즉 사실과 다르거나 논리적으로 결함이 있는 내용을 그럴듯하게 생성하는 경향이다. CoT가 생성하는 추론 과정은 단계별로 구성되어 매우 유창하고 논리적으로 보이기 때문에, 사용자는 그 안에 포함된 미묘한 오류를 인지하기 어렵다. 이는 ’설득력 있는 오류(convincing but incorrect reasoning)’를 만들어내어 잘못된 정보에 대한 신뢰를 높이는 위험을 초래한다.10</p>
<p>최근 연구는 CoT가 환각 문제에 대해 ’이중적 효과(double-edged effect)’를 가진다고 지적한다. 한편으로 CoT는 모델이 단계별로 생각하게 함으로써 전반적인 환각 발생 빈도를 줄이는 긍정적인 역할을 한다. 하지만 다른 한편으로, 일단 환각이 발생하면 그럴듯한 추론 과정이라는 ‘위장’ 때문에 탐지하기가 훨씬 더 어려워진다. 모델이 높은 확신도를 가지고 오류를 생성하기 때문에, 자동화된 환각 탐지 시스템마저도 이를 걸러내기 어렵게 만든다.23</p>
<h3>6.2 계산 비용 및 지연 시간(Latency)</h3>
<p>CoT는 본질적으로 더 많은 텍스트, 즉 더 많은 토큰을 생성하는 것을 전제로 한다. 중간 추론 과정을 상세히 서술해야 하므로, 최종 답변만 생성하는 표준 프롬프팅에 비해 API 호출 비용과 응답 시간이 크게 증가한다.6 예를 들어, 일부 연구에서는 CoT 요청이 직접적인 요청보다 35%에서 최대 600% 더 오래 걸리는 것으로 나타났다.24 이러한 높은 지연 시간은 실시간 채팅이나 빠른 상호작용이 요구되는 애플리케이션에서는 CoT를 적용하기 어렵게 만드는 실질적인 장벽이 된다.</p>
<h3>6.3 지식 의존성 및 외부 정보 통합의 어려움</h3>
<p>CoT를 통해 생성되는 추론의 품질은 전적으로 모델이 사전 훈련 과정에서 학습한 내부 지식에 의존한다. 따라서 모델의 학습 데이터가 오래되었거나, 특정 분야에 대한 정보가 부족하거나, 편향된 정보를 포함하고 있을 경우, CoT는 이러한 결함을 그대로 증폭시켜 부정확한 추론을 생성할 수 있다.21</p>
<p>또한, CoT 프레임워크는 외부 데이터베이스나 실시간 API와 같은 외부 정보 소스를 동적으로 통합하는 메커니즘이 내재되어 있지 않다. 이로 인해 최신 정보나 검증된 사실 확인이 중요한 금융, 의료, 법률과 같은 전문 분야에서는 CoT 기반 추론의 신뢰성에 한계가 있다.21</p>
<h3>6.4 최신 모델에서의 효용성 감소</h3>
<p>가장 최근의 비판은 CoT의 근본적인 가치에 대한 것이다. Ethan Mollick 등이 2024년 6월에 발표한 연구에 따르면, GPT-4o나 Claude 3.5 Sonnet과 같이 고도의 추론 능력이 이미 내재된 최신 ’추론 모델(Reasoning Models)’에서는 CoT 프롬프팅으로 인한 정확도 향상이 미미하거나, 오히려 성능이 감소하는 경향이 관찰되었다.24</p>
<p>이러한 최신 모델들은 별도의 지시 없이도 기본적으로 CoT와 유사한 내부적인 추론 과정을 수행하는 것으로 보인다. 따라서 이들에게 “단계별로 생각하라“고 명시적으로 지시하는 것은 오히려 자연스러운 사고 과정을 방해하고 불필요한 계산 오버헤드만 발생시키는 결과를 낳을 수 있다. 반면, 추론 기능이 상대적으로 약한 ’비추론 모델(Non-reasoning Models)’에서는 CoT가 여전히 성능 향상에 기여하지만, 답변의 일관성이 떨어지는 부작용이 나타날 수 있음이 지적되었다.24 이는 CoT의 효용성이 보편적인 것이 아니라 모델의 종류와 세대에 따라 달라지는 상대적인 것임을 시사한다.</p>
<p>아래 표는 CoT 프롬프팅에 대한 균형 잡힌 시각을 제공하기 위해 장단점과 최신 동향을 종합적으로 정리한 것이다.</p>
<table><thead><tr><th>구분</th><th>상세 내용</th><th>관련 연구/현상</th></tr></thead><tbody>
<tr><td><strong>장점</strong></td><td>복잡한 문제 분해 및 정확도 향상</td><td>Wei et al. (2022)의 GSM8K 결과 12</td></tr>
<tr><td></td><td>추론 과정의 해석 가능성 제공</td><td>디버깅 및 모델 행동 이해에 용이 5</td></tr>
<tr><td></td><td>일반화 능력 (산술, 상식, 기호)</td><td>다양한 도메인에 적용 가능 13</td></tr>
<tr><td><strong>단점</strong></td><td>환각 및 설득력 있는 오류 생성</td><td>논리적 외관으로 오류 탐지 어려움 10</td></tr>
<tr><td></td><td>높은 계산 비용 및 지연 시간</td><td>실시간 응용 프로그램에 부적합 6</td></tr>
<tr><td></td><td>내부 지식 의존 및 외부 정보 통합 한계</td><td>최신 정보 반영 어려움 21</td></tr>
<tr><td><strong>최신 동향</strong></td><td>최신 ’추론 모델’에서 효용성 감소</td><td>Mollick et al. (2024) 연구, 정확도 향상 미미 24</td></tr>
</tbody></table>
<p><em>표 2: CoT 프롬프팅의 장단점 및 최신 동향. CoT의 혁신적 장점과 함께 명백한 한계, 그리고 기술 발전에 따른 가치 변화를 보여준다.</em></p>
<h2>7.  CoT의 확장: 자기 일관성(Self-Consistency)</h2>
<p>연쇄 사고 추론(CoT)은 LLM이 단일한 추론 경로를 따라 문제를 해결하도록 유도한다. 이 방식은 탐욕적 디코딩(greedy decoding)과 유사하게, 각 단계에서 가장 확률이 높은 다음 토큰을 선택하여 하나의 결정적인 경로를 생성한다.26 그러나 이 접근법은 치명적인 약점을 내포하고 있다. 만약 추론 초기에 사소한 실수가 발생하거나 잘못된 방향으로 첫걸음을 내디디면, 그 오류는 후속 단계에 연쇄적으로 영향을 미쳐 최종적으로 완전히 틀린 결론에 도달하게 된다. 즉, 단일 경로 추론은 견고성(robustness)이 부족하다.</p>
<pre><code class="language-mermaid">graph TD
    Input["복잡한 질문"] --&gt; PathGen["다중 추론 경로 생성 (Temperature &gt; 0)"]
    
    PathGen --&gt; Path1["추론 경로 A (중간 실수)"]
    PathGen --&gt; Path2["추론 경로 B (정상 추론)"]
    PathGen --&gt; Path3["추론 경로 C (다른 방법)"]
    
    Path1 --&gt; Ans1["답변: 42 (오답)"]
    Path2 --&gt; Ans2["답변: 45 (정답)"]
    Path3 --&gt; Ans3["답변: 45 (정답)"]
    
    Ans1 --&gt; Vote["최종 답변 집계 (Aggregation)"]
    Ans2 --&gt; Vote
    Ans3 --&gt; Vote
    
    Vote --&gt; Majority["다수결 선택 (Majority Voting)"]
    Majority --&gt; Final["최종 결정: 45 (견고성 확보)"]

    style Final fill:#dcedc8,stroke:#33691e
</code></pre>
<h3>7.1 자기 일관성의 개념과 작동 원리</h3>
<p>이러한 단일 경로 추론의 한계를 극복하기 위해 제안된 기법이 바로 ’자기 일관성(Self-Consistency)’이다. 이 기법은 복잡한 문제에는 여러 가지 다른 해결 경로가 존재할 수 있다는 직관에 기반한다.28 자기 일관성은 LLM 추론을 ’단일 전문가의 독단적인 결정’에서 ‘다양한 전문가들로 구성된 위원회의 합의’ 모델로 전환시킨다. 이는 개별 추론 경로의 논리적 완결성 자체보다는, 다양한 접근 방식에도 불구하고 일관되게 수렴하는 ’결론의 강건함’을 더 신뢰하는 실용적인 접근법이다.</p>
<p>자기 일관성의 작동 메커니즘은 다음과 같은 단계로 이루어진다 28:</p>
<ol>
<li>
<p><strong>다중 추론 경로 생성 (Sampling Diverse Reasoning Paths)</strong>: 먼저, CoT 프롬프트를 기반으로 하되, 모델의 디코딩 과정에 무작위성을 부여한다. 이는 일반적으로 <code>temperature</code> 파라미터 값을 0보다 큰 값(예: 0.7)으로 설정하여 구현된다.26 이를 통해 모델은 동일한 프롬프트에 대해서도 매번 다른 토큰을 샘플링하게 되어, 결과적으로 여러 개의 서로 다른, 다양한 추론 경로를 생성한다.</p>
</li>
<li>
<p><strong>최종 답변 집계 (Aggregating Final Answers)</strong>: 생성된 여러 개의 독립적인 추론 경로들로부터 각각의 최종 답변만을 추출한다. 이 단계에서는 중간 추론 과정의 논리적 타당성은 일단 고려하지 않고, 오직 결론에만 집중한다.</p>
</li>
<li>
<p><strong>다수결 선택 (Majority Voting)</strong>: 마지막으로, 추출된 최종 답변들 중에서 가장 빈번하게 등장한(가장 일관된) 답변을 최종 결과로 선택한다. 이는 일종의 앙상블(ensemble) 기법과 유사하게, 다수의 추론이 지지하는 결론이 가장 신뢰할 수 있다는 원리를 이용하는 것이다.</p>
</li>
</ol>
<h3>7.2 견고성 및 정확성 향상</h3>
<p>예를 들어, 어떤 수학 문제에 대해 자기 일관성을 적용하여 5개의 추론 경로를 생성했다고 가정해보자.</p>
<ul>
<li>
<p>추론 경로 1: 계산 실수로 인해 최종 답 <strong>42</strong> 도출</p>
</li>
<li>
<p>추론 경로 2: 다른 접근법을 사용해 최종 답 <strong>45</strong> 도출</p>
</li>
<li>
<p>추론 경로 3: 또 다른 계산 실수로 인해 최종 답 <strong>43</strong> 도출</p>
</li>
<li>
<p>추론 경로 4: 경로 2와 유사한 논리로 최종 답 <strong>45</strong> 도출</p>
</li>
<li>
<p>추론 경로 5: 새로운 접근법을 사용했으나 최종 답 <strong>45</strong> 도출</p>
</li>
</ul>
<p>이 경우, 최종 답변은 {42, 45, 43, 45, 45}가 된다. 다수결 원칙에 따라, 가장 빈번하게 등장한 <strong>45</strong>가 최종 답변으로 채택된다. 이 방식은 단일 추론 경로에서 발생할 수 있는 우연한 실수를 효과적으로 보정하고, 모델이 다양한 각도에서 문제를 고려했을 때 가장 높은 확률로 도달하는 결론을 채택함으로써 결과의 견고성과 정확성을 크게 향상시킨다.27 자기 일관성은 특히 정답이 명확하게 정해져 있는 산술 문제나 상식 추론 과제에서 큰 효과를 발휘한다.</p>
<h2>8.  CoT의 일반화 I: 사고의 나무(Tree-of-Thoughts, ToT)</h2>
<p>CoT와 자기 일관성은 LLM의 추론 능력을 크게 향상시켰지만, 근본적으로 선형적인(linear) 사고의 틀을 벗어나지 못한다. 추론 과정은 시작부터 끝까지 한 방향으로만 진행되며, 중간에 실수가 발생했음을 인지하더라도 이전 단계로 돌아가 결정을 수정하거나(backtracking), 여러 가능한 대안을 동시에 탐색하는 것이 불가능하다.31 이는 계획, 탐색, 전략적 의사결정이 필수적인 복잡한 문제 해결에는 부적합하다.</p>
<pre><code class="language-mermaid">graph TD

subgraph "Tree-of-Thoughts (비선형/계층)"
        T1["생각 A (Root)"] --&gt; T2["생각 B1"]
        T1 --&gt; T3["생각 B2"]
        T2 --&gt; T4["생각 C1 (유망함)"]
        T2 --&gt; T5["생각 C2 (가지치기/Pruning)"]
        T3 --&gt; T6["생각 C3"]
        style T5 stroke-dasharray: 5 5
    end

	subgraph "Chain-of-Thought (선형)"
        C1["생각 A"] --&gt; C2["생각 B"] --&gt; C3["생각 C"]
    end
    
    style C1 fill:#f5f5f5
    style T1 fill:#e3f2fd
</code></pre>
<h3>8.1 ToT의 개념: 추론을 나무 구조로 확장</h3>
<p>이러한 선형적 사고의 한계를 극복하기 위해 제안된 프레임워크가 바로 ’사고의 나무(Tree-of-Thoughts, ToT)’이다. ToT는 LLM의 추론 과정을 비선형적인 ‘나무(Tree)’ 구조로 일반화한다.31 이 구조에서 각 노드(node)는 문제 해결의 중간 상태를 나타내는 ’사고(thought)’를 의미하며, 각 노드는 여러 개의 자식 노드를 가질 수 있다. 이는 특정 단계에서 하나의 다음 단계만 고려하는 것이 아니라, 여러 개의 가능한 대안적 ’사고’를 동시에 탐색할 수 있음을 의미한다.</p>
<p>ToT는 LLM을 단순한 ’자기회귀적 텍스트 생성기’에서 ’의도적인 문제 해결 에이전트(deliberate problem-solving agent)’로 격상시킨다. 이는 고전적인 인공지능(AI)의 탐색 알고리즘과 현대 LLM의 강력한 언어 생성 및 평가 능력을 결합한 혁신적인 하이브리드 접근법이다.</p>
<h3>8.2 ToT의 핵심 메커니즘</h3>
<p>ToT 프레임워크는 다음과 같은 핵심적인 4단계 과정을 통해 작동한다 31:</p>
<ol>
<li>
<p><strong>분해 (Decomposition)</strong>: 먼저, 주어진 문제를 여러 개의 중간 단계로 분해한다. 이는 CoT와 유사하지만, 각 단계가 여러 갈래로 나뉠 수 있다는 점에서 차이가 있다.</p>
</li>
<li>
<p><strong>사고 생성 (Thought Generation)</strong>: 현재 상태를 나타내는 각 노드에서, LLM을 사용하여 다음 단계로 나아갈 수 있는 잠재적인 ’사고’들을 여러 개 생성한다. 예를 들어, ‘24 게임’(주어진 숫자 4개와 사칙연산을 이용해 24를 만드는 게임)에서 현재 <code>(1, 2, 3, 4)</code>라는 숫자가 있다면, <code>1+2=3</code>, <code>4-3=1</code> 등 여러 가능한 다음 연산을 생성하는 것이다.</p>
</li>
<li>
<p><strong>상태 평가 (State Evaluation)</strong>: 생성된 각 ’사고’가 최종 목표 달성에 얼마나 유망한지를 LLM 스스로 평가하도록 한다. 이 평가는 휴리스틱(heuristic)으로 작용하여 탐색의 방향을 결정한다. 평가는 간단한 점수(예: 1-10점)일 수도 있고, “확실함/어쩌면/불가능함(sure/maybe/impossible)“과 같은 범주형 분류일 수도 있다.31 이 자기 성찰(self-reflection) 능력은 ToT의 핵심적인 부분이다.</p>
</li>
<li>
<p><strong>탐색 (Search)</strong>: 마지막으로, 너비 우선 탐색(BFS)이나 깊이 우선 탐색(DFS)과 같은 고전적인 트리 탐색 알고리즘을 사용하여 ’사고의 나무’를 체계적으로 탐색한다.31 이 과정에서 유망하지 않은 경로는 가지치기(pruning)하고, 가장 유망한 경로를 따라 탐색을 계속 진행한다.</p>
</li>
</ol>
<h3>8.3 ToT의 장점: 예측과 백트래킹</h3>
<p>이러한 메커니즘은 CoT가 가지지 못했던 두 가지 강력한 능력을 LLM에 부여한다:</p>
<ul>
<li>
<p><strong>전략적 예측 (Lookahead)</strong>: 여러 단계 앞을 미리 내다보며 각 선택이 미래에 미칠 영향을 평가하고, 이를 바탕으로 현재의 최적의 결정을 내릴 수 있다. 이는 단기적인 최적해가 아닌 전역적인 최적해를 찾는 데 매우 중요하다.</p>
</li>
<li>
<p><strong>백트래킹 (Backtracking)</strong>: 탐색 과정에서 특정 경로가 막다른 길이라고 판단되면, 탐색을 중단하고 이전의 분기점으로 돌아가 다른 대안적인 경로를 탐색할 수 있다.32 이는 초기의 잘못된 결정으로 인해 전체 문제 해결에 실패하는 것을 방지하고, 훨씬 더 넓은 해결 공간을 탐색할 수 있게 해준다.</p>
</li>
</ul>
<p>이러한 능력 덕분에 ToT는 24 게임, 미니 크로스워드, 창의적 글쓰기와 같이 정답이 하나로 정해져 있지 않거나 복잡한 계획이 필요한 문제에서 CoT 대비 월등한 성능을 보인다.31 이는 LLM의 추론이 직관에 의존하는 ’빠른 사고(System 1)’에서, 의도적인 계획과 평가를 포함하는 ’느린 사고(System 2)’로 한 단계 진화했음을 의미한다.</p>
<h2>9.  CoT의 일반화 II: 사고의 그래프(Graph-of-Thoughts, GoT)</h2>
<p>사고의 나무(ToT)는 LLM의 추론 구조를 선형에서 비선형적인 트리 구조로 확장하며 계획과 탐색 능력을 부여했다. 그러나 트리 구조 역시 근본적인 한계를 가지고 있다. 트리에서는 각 노드가 단 하나의 부모 노드만을 가질 수 있기 때문에, 서로 다른 추론 경로(트리의 다른 가지)에서 나온 유망한 아이디어나 중간 결과물들을 하나로 융합(merge)하는 것이 구조적으로 불가능하다.35 이는 인간의 사고 과정에서 흔히 일어나는 ’통합적 사고’나 ’아이디어의 시너지’를 모델링하지 못하는 한계로 작용한다.</p>
<pre><code class="language-mermaid">graph TD

	subgraph "Graph-of-Thoughts (네트워크)"
        G1["생각 A"] --&gt; G2["생각 B"]
        G3["생각 C"] --&gt; G2
        G2 --&gt; G4["생각 D (결합/Aggregation)"]
        G4 --&gt; G4["피드백 루프 (자기 개선)"]
        G4 --&gt; G5["최종 해결책"]
    end
    
    subgraph "Chain-of-Thought (선형)"
        C1["생각 A"] --&gt; C2["생각 B"] --&gt; C3["생각 C"]
    end
    
    style C1 fill:#f5f5f5
    style G1 fill:#f3e5f5
</code></pre>
<h3>9.1 GoT의 개념: 추론을 그래프 구조로 일반화</h3>
<p>이러한 트리 구조의 한계를 극복하고 추론의 유연성을 극대화하기 위해 제안된 프레임워크가 바로 ’사고의 그래프(Graph-of-Thoughts, GoT)’이다. GoT는 LLM의 추론 과정을 임의의 ‘그래프(Graph)’ 구조로 모델링한다.35 이 프레임워크에서 개별 ’사고’는 그래프의 정점(vertex)이 되고, 사고들 간의 논리적 의존성이나 파생 관계는 방향성이 있는 간선(edge)으로 표현된다.</p>
<p>GoT는 인간의 복잡한 사고 과정, 특히 여러 아이디어가 서로 영향을 주고받으며 새로운 아이디어를 탄생시키는 ’연상(association)’과 ‘통합(synthesis)’ 과정을 가장 근접하게 모방하려는 시도이다. 인간의 뇌가 선형적이거나 엄격한 계층 구조보다는 복잡하게 연결된 뉴런 네트워크처럼 작동하는 것과 같이, GoT는 LLM 추론에 이러한 네트워크 구조의 자유도를 부여한다.</p>
<h3>9.2 GoT의 핵심 장점</h3>
<p>그래프 구조는 트리 구조가 가지지 못했던 강력한 능력들을 가능하게 한다 35:</p>
<ol>
<li>
<p><strong>사고의 결합 (Aggregation)</strong>: GoT의 가장 큰 장점은 여러 부모 노드로부터 나온 사고들을 하나의 자식 노드로 통합할 수 있다는 점이다. 이는 그래프에서 하나의 정점이 여러 개의 들어오는 간선(incoming edges)을 가질 수 있음을 의미한다. 예를 들어, 어떤 문제에 대해 한쪽 경로에서는 ’A’라는 장점을 가진 해결책이, 다른 경로에서는 ’B’라는 장점을 가진 해결책이 도출되었을 때, 이 둘을 결합하여 ’A’와 ’B’의 장점을 모두 가지는 새로운 해결책을 생성할 수 있다. 이는 창의적인 문제 해결에서 발생하는 시너지 효과를 모델링하는 데 매우 강력하다.</p>
</li>
<li>
<p><strong>피드백 루프 (Feedback Loops)</strong>: GoT는 순환(cycle) 구조를 허용한다. 이는 특정 사고를 개선하기 위해 자기 자신을 다시 참조하는 피드백 루프를 만들 수 있음을 의미한다. 예를 들어, 생성된 글의 초안을 평가하고, 그 평가 결과를 바탕으로 다시 초안을 수정하는 ‘반복적인 개선(iterative refinement)’ 과정을 자연스럽게 모델링할 수 있다.</p>
</li>
<li>
<p><strong>최고 수준의 유연성</strong>: GoT는 가장 일반적인 추론 프레임워크로서, 기존의 추론 기법들을 모두 특수한 경우로 포함한다. CoT는 하나의 경로를 가진 선형 그래프로, ToT는 분기만 있고 합쳐지지 않는 비순환 그래프(DAG)의 일종인 트리로 볼 수 있다.35 따라서 GoT는 문제의 특성에 따라 CoT, ToT와 같은 단순한 구조를 사용하거나, 필요에 따라 더 복잡한 그래프 구조를 동적으로 구성할 수 있는 최고의 유연성을 제공한다.</p>
</li>
</ol>
<h3>9.3 GoT 프레임워크의 공식적 정의</h3>
<p>GoT 프레임워크는 공식적으로 네 가지 요소의 튜플(tuple) <code>(G, T, E, R)</code>로 모델링될 수 있다 35:</p>
<ul>
<li>
<p><code>G (Graph)</code>: 현재까지의 모든 ’사고’들과 그들 사이의 관계를 나타내는 방향성 그래프이다.</p>
</li>
<li>
<p><code>T (Transformations)</code>: 그래프를 수정하고 확장하는 연산들의 집합이다. 여기에는 새로운 사고를 생성하는 연산, 여러 사고를 하나로 결합하는 연산, 특정 사고를 개선하는 연산 등이 포함된다.</p>
</li>
<li>
<p><code>E (Evaluator)</code>: 그래프 내의 각 ’사고’가 얼마나 유망한지를 평가하는 평가자 함수이다.</p>
</li>
<li>
<p><code>R (Ranker)</code>: 평가된 사고들의 순위를 매겨 다음 변환(transformation)을 적용할 대상을 선택하는 랭킹 함수이다.</p>
</li>
</ul>
<p>이러한 프레임워크는 LLM을 단순한 추론기를 넘어, 복잡한 지식 네트워크를 스스로 구축하고, 탐색하며, 새로운 지식을 창출하는 ’지식 생성기(knowledge creator)’로 발전시킬 잠재력을 보여준다. 아래 표는 CoT에서 GoT에 이르기까지 추론 프롬프팅 기법의 진화 과정을 한눈에 비교하여 보여준다.</p>
<table><thead><tr><th>기법</th><th>추론 구조</th><th>핵심 아이디어</th><th>주요 능력</th><th>한계점</th></tr></thead><tbody>
<tr><td><strong>Standard Prompting</strong></td><td>점 (Point)</td><td>직접적인 답변 생성</td><td>빠른 응답</td><td>다단계 추론 불가</td></tr>
<tr><td><strong>Chain-of-Thought (CoT)</strong></td><td>선 (Line)</td><td>단계별 추론 과정 생성</td><td>문제 분해, 해석 가능성</td><td>백트래킹 불가, 단일 경로</td></tr>
<tr><td><strong>Self-Consistency</strong></td><td>다중 선 (Multiple Lines)</td><td>다수결 기반의 강건성 확보</td><td>오류 보정, 신뢰도 향상</td><td>계산 비용 증가</td></tr>
<tr><td><strong>Tree-of-Thoughts (ToT)</strong></td><td>나무 (Tree)</td><td>대안 탐색 및 평가</td><td>계획, 백트래킹, 예측</td><td>경로 융합 불가</td></tr>
<tr><td><strong>Graph-of-Thoughts (GoT)</strong></td><td>그래프 (Graph)</td><td>임의의 사고 연결 및 융합</td><td>아이디어 통합, 피드백 루프</td><td>구현 복잡성, 제어 난이도</td></tr>
</tbody></table>
<p><em>표 3: 추론 프롬프팅 기법의 진화 비교. 각 기법의 추론 구조, 핵심 아이디어, 주요 능력 및 한계를 요약하여 기술의 발전 계보를 보여준다.</em></p>
<h2>10.  결론: 연쇄 사고 추론의 현재와 미래</h2>
<p>연쇄 사고 추론(Chain-of-Thought, CoT)은 대규모 언어 모델(LLM)의 역사에서 하나의 분기점을 형성한 기념비적인 연구였다. CoT는 LLM이 방대한 지식을 저장하는 것을 넘어, 그 지식을 활용하여 복잡한 다단계 추론을 수행할 수 있는 잠재력을 가졌음을 명확히 보여주었다. 이는 프롬프트 엔지니어링이라는 새로운 분야를 개척하고, 모델의 내부 가중치를 변경하지 않고도 잠재된 능력을 최대한으로 이끌어내는 새로운 방법론의 시대를 열었다.</p>
<p>현재 CoT는 여전히 많은 상황에서, 특히 추론 능력이 상대적으로 부족한 모델이나 고도로 구조화된 절차적 문제 해결에 있어 유용한 기법으로 활용되고 있다. 그러나 GPT-4o와 같이 추론 능력이 고도로 내재화된 최신 ’추론 모델’의 등장으로 인해, CoT의 보편적 가치는 재평가되고 있다. 이제는 CoT를 모든 문제에 적용하는 만능 해결책으로 보기보다는, ‘어떤 모델에, 어떤 과제에, 어떻게’ 사용해야 하는지에 대한 더 섬세하고 비판적인 이해가 요구되는 시점이다.</p>
<p>CoT와 그 후속 연구들(Self-Consistency, ToT, GoT)이 제시한 방향성은 LLM의 미래 연구에 중요한 청사진을 제공한다. 향후 연구는 다음과 같은 방향으로 심화될 것으로 예측된다.</p>
<p>첫째, <strong>구조화된 추론과 외부 도구의 결합</strong>이 가속화될 것이다. GoT와 같은 정교한 내부 추론 프레임워크는 외부 지식 베이스를 참조하는 검색 증강 생성(RAG), 정확한 계산을 수행하는 코드 인터프리터, 외부 세계와 상호작용하는 API와 같은 외부 도구들과 긴밀하게 결합될 것이다. 이는 LLM의 고질적인 문제인 환각을 줄이고, 사실에 기반한 신뢰도 높은 추론을 수행하는 데 필수적이다.</p>
<p>둘째, <strong>추론 과정의 자동 검증 및 수정</strong> 메커니즘이 핵심 연구 주제가 될 것이다. LLM이 생성한 추론 경로의 논리적 타당성, 사실적 정확성, 그리고 내부적 일관성을 자동으로 검증하고, 오류가 발견되었을 때 스스로 수정하는 능력을 개발하는 것은 LLM을 신뢰할 수 있는 파트너로 만드는 데 중요한 과제이다.</p>
<p>셋째, <strong>잠재 공간에서의 효율적인 추론</strong>에 대한 탐구가 이루어질 것이다. 현재의 CoT 계열 기술들은 추론 과정을 명시적인 텍스트로 풀어내는 방식을 취하고 있어 계산 비용이 높다. 미래에는 모델이 대부분의 추론을 효율적인 잠재 공간(latent space) 내에서 수행하고, 사용자가 요구하거나 시스템이 필요하다고 판단할 경우에만 그 과정을 설명하는 방식으로 발전할 수 있다.10</p>
<p>CoT에서 시작하여 GoT에 이르는 연구의 여정은 궁극적으로 ’인공 일반 지능(AGI)’의 핵심 요소인 ’자율적인 문제 해결 능력’을 구현하려는 거대한 목표를 향하고 있다. 이 연구들은 LLM을 인간의 지시를 수동적으로 따르는 도구를 넘어, 스스로 문제를 정의하고, 계획을 세우며, 다양한 전략을 탐색하고, 결과를 검증하며, 인간과 협력하여 복잡한 문제를 해결하고 새로운 지식을 창출하는 ’지적 파트너’로 만들기 위한 과정에 있다. 연쇄 사고 추론은 이 위대한 여정의 가능성을 연 중요한 첫걸음으로 기록될 것이다.</p>
<h2>11. 참고 자료</h2>
<ol>
<li>Comprehensive Guide to Chain-of-Thought Prompting - Mercity AI, https://www.mercity.ai/blog-post/guide-to-chain-of-thought-prompting</li>
<li>CoT(사고 사슬) 프롬프트란 무엇입니까? 예시 및 이점 - Unite.AI, https://www.unite.ai/ko/what-is-chain-of-thought-cot-prompting-examples-benefits/</li>
<li>What is chain of thought (CoT) prompting? | IBM, https://www.ibm.com/think/topics/chain-of-thoughts</li>
<li>Chain of Thought Prompting - .NET - Microsoft Learn, https://learn.microsoft.com/en-us/dotnet/ai/conceptual/chain-of-thought-prompting</li>
<li>Chain-of-Thought Prompting Elicits Reasoning in Large … - arXiv, https://arxiv.org/pdf/2201.11903</li>
<li>[LLM][프롬프트엔지니어링] CoT(Chain of Thought) - logN^블 - 티스토리, https://dwin.tistory.com/170</li>
<li>Chain of Thought Prompting Guide - PromptHub, https://www.prompthub.us/blog/chain-of-thought-prompting-guide</li>
<li>생각의 연쇄 프롬프트란 무엇인가요? - Botpress, https://botpress.com/ko/blog/chain-of-thought</li>
<li>How Chain of Thought (CoT) Prompting Helps LLMs Reason More Like Humans | Splunk, https://www.splunk.com/en_us/blog/learn/chain-of-thought-cot-prompting.html</li>
<li>New paper reveals Chain-of-Thought reasoning of LLMs a mirage : r/LocalLLaMA - Reddit, https://www.reddit.com/r/LocalLLaMA/comments/1mkza1b/new_paper_reveals_chainofthought_reasoning_of/</li>
<li>Chain-of-Thought Prompting: Step-by-Step Reasoning with LLMs | DataCamp, https://www.datacamp.com/tutorial/chain-of-thought-prompting</li>
<li>Chain-of-Thought Prompting Elicits Reasoning in … - OpenReview, https://openreview.net/pdf?id=_VjQlMeSB_J</li>
<li>Review — Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, https://sh-tsang.medium.com/review-chain-of-thought-prompting-elicits-reasoning-in-large-language-models-6bdc0541f702</li>
<li>Timothyxxx/Chain-of-ThoughtsPapers: A trend starts from “Chain of Thought Prompting Elicits Reasoning in Large Language Models”. - GitHub, https://github.com/Timothyxxx/Chain-of-ThoughtsPapers</li>
<li>Chain-of-Thought Prompting: A Comprehensive Analysis of Reasoning Techniques in Large Language Models | by Pier-Jean Malandrino | Scub-Lab, https://lab.scub.net/chain-of-thought-prompting-a-comprehensive-analysis-of-reasoning-techniques-in-large-language-b67fdd2eb72a</li>
<li>Language Models Perform Reasoning via Chain of Thought, https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/</li>
<li>Paper Summary: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, https://queirozf.com/entries/paper-summary-chain-of-thought-prompting-elicits-reasoning-in-large-language-models</li>
<li>Chain-of-Thought Prompting | Prompt Engineering Guide, https://www.promptingguide.ai/techniques/cot</li>
<li>Aligning Large and Small Language Models via Chain-of-Thought Reasoning - ACL Anthology, https://aclanthology.org/2024.eacl-long.109.pdf</li>
<li>Automatic Chain of Thought Prompting in Large Language Models - OpenReview, https://openreview.net/forum?id=5NTt8GFjUHkr</li>
<li>Topic #11: 더 강력한 추론 성능을 위한 ‘Chain-of-Knowledge’ - 튜링포스트코리아, https://turingpost.co.kr/p/chain-of-knowledge-reasoning</li>
<li>How to teach chain of thought reasoning to your LLM | Invisible Blog, https://invisibletech.ai/blog/how-to-teach-chain-of-thought-reasoning-to-your-llm</li>
<li>Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation - arXiv, https://arxiv.org/html/2506.17088v1</li>
<li>Technical Report: The Decreasing Value of Chain of Thought in …, https://gail.wharton.upenn.edu/research-and-insights/tech-report-chain-of-thought/</li>
<li>gail.wharton.upenn.edu, <a href="https://gail.wharton.upenn.edu/research-and-insights/tech-report-chain-of-thought/#:~:text=Chain-of-Thought%20prompting%20is,justify%20the%20increased%20response%20time.">https://gail.wharton.upenn.edu/research-and-insights/tech-report-chain-of-thought/#:~:text=Chain%2Dof%2DThought%20prompting%20is,justify%20the%20increased%20response%20time.</a></li>
<li>Advanced Prompt Engineering — Self-Consistency, Tree-of-Thoughts, RAG | by Sulbha Jain, https://medium.com/@sulbha.jindal/advanced-prompt-engineering-self-consistency-tree-of-thoughts-rag-17a2d2c8fb79</li>
<li>Master Prompting Techniques: Self-Consistency Prompting, https://promptengineering.org/self-consistency-prompting/</li>
<li>Self-Consistency and Universal Self-Consistency Prompting, https://www.prompthub.us/blog/self-consistency-and-universal-self-consistency-prompting</li>
<li>Self-Consistency in Prompt Engineering - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2024/07/self-consistency-in-prompt-engineering/</li>
<li>Self Consistency - Prompt Engineering Masterclass - YouTube, https://www.youtube.com/watch?v=SMk4syMMdRk</li>
<li>Tree of Thoughts (ToT) | Prompt Engineering Guide, https://www.promptingguide.ai/techniques/tot</li>
<li>Tree of Thoughts: Deliberate Problem Solving with Large Language Models - OpenReview, https://openreview.net/forum?id=5Xc1ecxO1h</li>
<li>What is Tree Of Thoughts Prompting? - IBM, https://www.ibm.com/think/topics/tree-of-thoughts</li>
<li>princeton-nlp/tree-of-thought-llm: [NeurIPS 2023] Tree of Thoughts: Deliberate Problem Solving with Large Language Models - GitHub, https://github.com/princeton-nlp/tree-of-thought-llm</li>
<li>Graph of Thoughts: Solving Elaborate Problems with Large …, https://ojs.aaai.org/index.php/AAAI/article/view/29720/31236</li>
<li>Graph of Thoughts: Solving Elaborate Problems with Large Language Models - arXiv, https://arxiv.org/abs/2308.09687</li>
<li>Official Implementation of “Graph of Thoughts: Solving Elaborate Problems with Large Language Models” - GitHub, https://github.com/spcl/graph-of-thoughts</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>