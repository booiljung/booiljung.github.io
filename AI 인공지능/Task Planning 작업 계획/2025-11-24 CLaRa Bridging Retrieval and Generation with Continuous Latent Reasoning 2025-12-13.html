<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:CLaRa (Bridging Retrieval and Generation with Continuous Latent Reasoning, 2025-11-24)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>CLaRa (Bridging Retrieval and Generation with Continuous Latent Reasoning, 2025-11-24)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">작업 계획 (Task Planning)</a> / <span>CLaRa (Bridging Retrieval and Generation with Continuous Latent Reasoning, 2025-11-24)</span></nav>
                </div>
            </header>
            <article>
                <h1>CLaRa (Bridging Retrieval and Generation with Continuous Latent Reasoning, 2025-11-24)</h1>
<p>2025-12-13, G30DR</p>
<h2>1.  서론: RAG 패러다임의 구조적 한계와 진화의 필연성</h2>
<p>인공지능 연구, 특히 자연어 처리(NLP) 분야에서 대규모 언어 모델(Large Language Models, LLM)의 등장은 지식의 저장과 활용 방식에 근본적인 변화를 가져왔다. 그러나 LLM이 가진 고질적인 문제인 환각(Hallucination) 현상과 지식의 최신성(Knowledge Obsolescence) 한계를 극복하기 위해 검색 증강 생성(Retrieval-Augmented Generation, RAG)이라는 패러다임이 표준으로 자리 잡았다.1 RAG는 외부의 거대한 지식베이스(Knowledge Base)에서 관련 문서를 검색하여 모델의 입력으로 제공함으로써, 모델이 학습되지 않은 정보에 대해서도 사실에 입각한 답변을 생성할 수 있도록 돕는다.</p>
<p>하지만 현재 주류를 이루고 있는 RAG 시스템은 근본적인 구조적 결함을 안고 있다. 이는 ’검색(Retrieval)’과 ’생성(Generation)’이라는 두 개의 핵심 모듈이 서로 다른 목적 함수를 가지고 독립적으로 최적화된다는 점이다. 일반적으로 검색기(Retriever)는 쿼리와 문서 간의 표면적 유사도(Surface-level similarity)나 임베딩 공간에서의 거리를 기준으로 문서를 선택하도록 훈련되는 반면, 생성기(Generator)는 주어진 텍스트를 바탕으로 답변을 생성하는 언어 모델링(Language Modeling) 작업에 집중한다.1 이 두 과정 사이의 단절은 ’최적화의 불연속성(Disjoint Optimization)’을 초래한다. 즉, 생성기가 답변을 도출하는 과정에서 특정 문서가 도움이 되었는지, 혹은 방해가 되었는지에 대한 피드백(Gradient)이 검색기로 전달되지 못한다.</p>
<p>더욱이, 이러한 분절된 구조는 심각한 비효율성을 야기한다. 밀집 검색기(Dense Retriever)는 문서를 벡터 공간으로 인코딩하여 처리하지만, 생성기는 여전히 원시 텍스트(Raw Text) 형태의 입력을 요구한다. 이로 인해 검색된 문서를 다시 텍스트로 디코딩하고, 이를 생성기가 다시 인코딩하는 중복 연산이 발생하며, 이는 긴 문맥(Long Context)을 처리할 때 막대한 계산 비용과 메모리 오버헤드로 이어진다.1 이러한 ’표현 공간의 불일치(Inefficiency in Representation Spaces)’는 실시간 애플리케이션이나 온디바이스(On-device) 환경에서의 RAG 도입을 저해하는 주요 요인으로 작용한다.</p>
<p>본 보고서는 이러한 RAG의 난제를 해결하기 위해 Apple과 에든버러 대학교(University of Edinburgh) 연구진이 제안한 <strong>CLaRa(Continuous Latent Reasoning)</strong> 프레임워크를 심층적으로 분석한다.2 CLaRa는 문서를 사전에 고도로 압축된 ’연속 잠재 표현(Continuous Latent Representations)’으로 변환하고, 검색과 생성을 이 공유된 잠재 공간 내에서 단일 프로세스로 통합함으로써 미분 가능한(Differentiable) 최적화를 실현한다.2 본고에서는 CLaRa의 기술적 아키텍처, 특히 **SCP(Salient Compressor Pretraining)**와 <strong>미분 가능한 Top-k 선택(Differentiable Top-k Selection)</strong> 메커니즘을 상세히 해부하고, 이를 통해 달성한 압축 효율성과 추론 성능의 향상을 정량적, 정성적으로 평가한다. 또한, 이 기술이 갖는 산업적 함의와 향후 RAG 시스템의 발전 방향에 대해 논의한다.</p>
<h2>2.  기존 RAG 아키텍처의 한계와 도전 과제</h2>
<p>CLaRa의 혁신성을 온전히 이해하기 위해서는 기존 RAG 시스템이 직면한 기술적 장벽을 구체적으로 파악할 필요가 있다. 기존 시스템들은 주로 DPR(Dense Passage Retrieval)과 같은 검색기와 BART, T5, 혹은 GPT 계열의 생성기를 결합하는 형태를 취해왔다.4</p>
<h3>2.1  최적화의 단절 (Disjoint Optimization)</h3>
<p>전통적인 RAG 파이프라인에서 문서 선택은 이산적(Discrete)인 과정이다. 검색기는 수백만 개의 문서 중에서 상위 <span class="math math-inline">k</span>개를 선택(Hard Selection)하여 생성기에 전달한다. 이 ’선택’이라는 행위는 미분 불가능한(Non-differentiable) 연산이기에, 생성 단계에서 계산된 손실 함수(Loss Function)의 그래디언트가 검색기로 역전파(Backpropagation)될 수 없다.1</p>
<p>이로 인해 발생하는 문제는 다음과 같다:</p>
<ul>
<li><strong>피드백 루프의 부재:</strong> 생성기가 정답을 맞추는 데 결정적인 역할을 한 문서에 대해 검색기에게 “이 문서는 유용했다“는 신호를 보낼 수 없다. 반대로, 검색기가 관련성 있어 보이는 문서를 가져왔지만 실제로는 정답 생성에 도움이 되지 않는 경우(예: 키워드는 같지만 맥락이 다른 문서)에도 이를 수정할 방법이 요원하다.</li>
<li><strong>목적 함수의 불일치:</strong> 검색기는 ’질문과의 유사도’를 최대화하려 하고, 생성기는 ’정답의 우도(Likelihood)’를 최대화하려 한다. 이 두 목표가 항상 일치하는 것은 아니며, 결과적으로 검색기는 생성에 필요한 ’추론적 단서’보다는 단순 키워드 매칭에 편향된 문서를 추출하게 된다.</li>
</ul>
<h3>2.2  표현 공간의 비효율성 (Inefficiency in Representation Spaces)</h3>
<p>RAG 시스템의 두 번째 문제는 데이터 표현 방식의 이원화다.</p>
<ul>
<li><strong>검색 단계:</strong> 문서를 밀집 벡터(Dense Vector)로 압축하여 저장하고 검색한다.</li>
<li><strong>생성 단계:</strong> 검색된 문서의 인덱스를 통해 다시 원본 텍스트(Raw Text)를 불러오고, 이를 생성기의 토크나이저(Tokenizer)로 다시 처리한다.</li>
</ul>
<p>이 과정에서 발생하는 비효율은 명확하다.</p>
<ol>
<li><strong>중복 인코딩:</strong> 문서는 검색용 인코더와 생성용 인코더에 의해 두 번 처리된다.</li>
<li><strong>문맥 윈도우 낭비:</strong> 생성기는 사람이 읽을 수 있는 텍스트를 입력으로 받기 때문에, 긴 문서를 처리할 때 문맥 윈도우(Context Window)를 빠르게 소진한다. 이는 “Lost in the Middle” 현상을 유발하거나, 하드웨어 비용을 기하급수적으로 증가시키는 원인이 된다.3</li>
<li><strong>정보 밀도의 저하:</strong> 원본 텍스트에는 답변 생성에 불필요한 조사, 관사, 구문적 구조 등이 포함되어 있다. 이는 제한된 생성기의 입력 용량을 낭비하는 결과를 낳는다.</li>
</ol>
<h3>2.3  이전 연구들과의 비교 (REALM, RAG-Token vs. CLaRa)</h3>
<p>REALM(Retrieval-Augmented Language Model Pre-training)과 같은 초기 연구들은 검색기를 사전 학습 단계에서 업데이트하려는 시도를 했으나, 여전히 전체 문서를 비동기적으로 재색인(Re-indexing)해야 하는 엄청난 계산 비용 문제에 부딪혔다.4 또한 RAG-Token이나 RAG-Sequence 모델들은 검색된 문서를 잠재 변수(Latent Variable)로 취급하여 주변화(Marginalization)를 시도했으나, 여전히 텍스트 기반의 인터페이스를 유지함으로써 미분 가능한 엔드투엔드(End-to-End) 학습의 이점을 완전히 살리지 못했다.</p>
<p>CLaRa는 이러한 한계를 극복하기 위해 ‘문서’ 자체를 텍스트가 아닌 ’연속적인 벡터(Continuous Vector)’로 취급하고, 이 벡터 위에서 검색과 생성을 모두 수행하는 급진적인 접근 방식을 취한다.1</p>
<h2>3.  CLaRa 아키텍처 심층 분석: 연속 잠재 추론</h2>
<p>CLaRa의 핵심 철학은 **“검색과 생성의 경계를 허물고, 단일한 잠재 공간에서 추론을 수행한다”**는 것이다. 이를 구현하기 위해 CLaRa는 크게 두 단계의 학습 과정을 거친다. 첫 번째는 문서를 의미론적으로 압축하는 <strong>SCP(Salient Compressor Pretraining)</strong> 단계이며, 두 번째는 압축된 표현 위에서 검색과 생성을 공동으로 최적화하는 <strong>Joint Training</strong> 단계이다.2</p>
<h3>3.1  공유된 연속 표현 (Shared Continuous Representations)</h3>
<p>CLaRa 아키텍처의 기반은 문서 <span class="math math-inline">D</span>를 고정된 길이의 컴팩트한 벡터 <span class="math math-inline">M</span>으로 변환하는 것이다. 기존의 RAG가 문서의 임베딩(검색용)과 텍스트(생성용)를 별도로 관리했던 것과 달리, CLaRa는 단 하나의 벡터 <span class="math math-inline">M</span>만을 유지한다.</p>
<ul>
<li><strong>메모리 토큰 (Memory Tokens):</strong> 문서는 수천 개의 토큰으로 구성되지만, CLaRa는 이를 <span class="math math-inline">K</span>개의 ‘메모리 토큰’ (예: 16개, 32개, 128개 등)으로 압축한다. 이 메모리 토큰들은 문서의 핵심적인 의미와 추론에 필요한 논리적 구조를 담고 있는 고밀도 벡터이다.1</li>
<li><strong>이중 용도 (Dual Purpose):</strong> 이 압축된 벡터 <span class="math math-inline">M</span>은 검색 단계에서는 쿼리와의 유사도를 계산하는 Key로 사용되고, 생성 단계에서는 LLM의 프롬프트(Soft Prompt)로 직접 입력되어 답변 생성의 문맥(Context) 역할을 한다. 즉, 생성기는 텍스트를 읽는 것이 아니라, 이 압축된 ’사고의 흔적(Latent Thought)’을 읽어들인다.</li>
</ul>
<h3>3.2  쿼리 추론기 (Query Reasoner) <span class="math math-inline">\theta_{qr}</span></h3>
<p>CLaRa는 사용자의 질문을 처리하기 위해 단순한 인코더 대신 **쿼리 추론기(Query Reasoner)**를 도입한다. 쿼리 추론기는 질문을 문서와 동일한 잠재 공간으로 매핑하는 역할을 수행한다. 그러나 단순한 매핑을 넘어, 쿼리 추론기는 “답변을 생성하기 위해 어떤 정보가 필요한가“를 예측하도록 훈련된다.</p>
<ul>
<li><strong>Logit Lens 분석의 통찰:</strong> 연구진이 학습된 쿼리 추론기의 출력 벡터를 분석(Logit Lens 기법 활용)한 결과, 원래 질문 텍스트에는 존재하지 않는 단어들이 벡터 공간에 포함되어 있음이 발견되었다.1 예를 들어, 질문이 암시적인 관계를 묻는 경우, 쿼리 추론기는 그 관계를 해소하는 데 필요한 중간 단계의 키워드나 개념을 스스로 생성하여 검색 쿼리에 포함시킨다. 이는 CLaRa가 단순한 키워드 매칭을 넘어 **잠재적 추론(Latent Reasoning)**을 수행하고 있음을 강력하게 시사한다.</li>
</ul>
<h3>3.3  생성기 (Generator) <span class="math math-inline">\theta_g</span></h3>
<p>생성기는 쿼리 추론기가 전달한 질문 벡터와 검색된 문서 벡터들(<span class="math math-inline">M^{(k)}</span>)을 입력받아 최종 답변을 생성한다. 이때 생성기는 원본 텍스트를 전혀 보지 못한다. 오직 압축된 벡터만을 보고 답변을 생성해야 하므로, 압축 과정에서 정보의 손실이 없어야 하며, 생성기는 압축된 표현을 해석(Decipher)하는 능력을 갖추어야 한다. 이를 위해 CLaRa는 SCP라는 독창적인 사전 학습 방식을 도입하였다.</p>
<h2>4.  SCP (Salient Compressor Pretraining): 정보의 소화와 의미론적 압축</h2>
<p>CLaRa의 성공 여부는 “긴 문서를 얼마나 효과적으로 압축할 수 있는가“에 달려 있다. 기존의 오토인코더(Autoencoder) 방식은 입력된 토큰을 하나하나 복원(Reconstruction)하는 것을 목표로 했기 때문에, 문장의 구문(Syntax)이나 불필요한 조사와 같은 비본질적인 정보까지 압축하려다 용량을 낭비하는 경향이 있었다.1 이를 극복하기 위해 CLaRa 연구진은 **SCP(Salient Compressor Pretraining)**를 제안한다.</p>
<h3>4.1  인지적 영감: 소화(Digestion) 원리</h3>
<p>SCP는 인간의 인지 과정인 ’소화(Digestion)’에서 영감을 받았다. 우리가 책을 읽고 내용을 기억할 때, 문장의 토시 하나하나를 기억하는 것이 아니라 그 안에 담긴 ’개념’과 ’논리’를 추출하여 저장하고, 구체적인 문장 형태는 잊어버리는 것과 같다.3 SCP는 모델이 문서를 ’기억’하게 하는 대신, 문서를 ’이해’하고 ’핵심을 요약’하게 만든다.</p>
<h3>4.2  가이드된 데이터 합성 파이프라인 (Guided Data Synthesis Pipeline)</h3>
<p>SCP는 압축기(Compressor)가 문서의 표면적 형태가 아닌 본질적 의미(Salient Information)에 집중하도록 하기 위해, 대규모 언어 모델(Qwen-32B 등)을 활용하여 합성 데이터를 생성한다. 이 데이터는 압축기가 수행해야 할 세 가지 과제로 구성된다.7</p>
<h4>4.2.1  단순 QA (Simple QA)</h4>
<ul>
<li><strong>목표:</strong> 문서 내의 세밀한 사실 정보(Fine-grained factual retention)를 보존한다.</li>
<li><strong>메커니즘:</strong> 모델은 문서에서 추출 가능한 단일 사실(Atomic Fact)을 묻고 답하는 쌍을 생성한다. 중복을 피하기 위해 이전에 생성된 질문과 겹치지 않는 새로운 사실을 찾도록 유도된다.</li>
<li><strong>예시:</strong></li>
<li>Q: “Trichocladus crinitus는 어떤 식물 과에 속하는가?”</li>
<li>A: “Hamamelidaceae (조록나무과)에 속한다.”</li>
</ul>
<h4>4.2.2  복합 QA (Complex QA)</h4>
<ul>
<li><strong>목표:</strong> 단순 사실의 나열을 넘어, 정보 간의 관계를 파악하고 추론하는 능력(Relational Reasoning)을 배양한다.</li>
<li><strong>메커니즘:</strong> 문서 내에 흩어진 여러 사실을 연결해야만 답할 수 있는 질문을 생성한다. 이는 압축된 벡터가 단편적인 정보뿐만 아니라 정보 간의 논리적 연결 고리까지 포함하도록 강제한다.</li>
<li><strong>예시:</strong></li>
<li>Q: “2014년 10월 2일 Newport County에 입단하여 10월 4일 데뷔한 선수는?”</li>
<li>A: “James Loveridge이다.”</li>
</ul>
<h4>4.2.3  바꿔쓰기 (Paraphrase)</h4>
<ul>
<li><strong>목표:</strong> 표현의 다양성을 학습하고, 표면적인 텍스트 형태와 무관한 의미적 불변성(Semantic Invariance)을 확보한다.</li>
<li><strong>메커니즘:</strong> 원본 문서의 문장 구조를 재배열하거나 다른 단어로 표현하되, 핵심 의미는 유지하는 텍스트를 생성하게 한다. 이를 통해 압축기는 특정 단어의 존재 여부가 아니라 그 단어가 가리키는 의미를 인코딩하게 된다.</li>
</ul>
<h3>4.3  검증 및 반복 (Verification and Loop)</h3>
<p>합성된 데이터의 품질은 압축 성능에 직결되므로, SCP는 엄격한 검증 절차를 포함한다. LLM은 생성된 QA 쌍들이 원본 문서의 핵심 정보를 빠짐없이 포괄하고 있는지 확인한다. 만약 누락된 정보가 발견되면, 해당 부분을 커버하는 새로운 질문을 생성하도록 요청하며, 이 과정은 최대 10회까지 반복된다.7 이를 통해 SCP는 문서의 모든 중요한 정보가 압축 벡터에 ’소화’되도록 보장한다.</p>
<h3>4.4  훈련 목적 함수 (Training Objective)</h3>
<p>SCP 단계에서 압축기는 생성된 QA 쌍과 바꿔쓰기 데이터를 바탕으로 학습된다. 이때 압축된 벡터 <span class="math math-inline">M</span>을 입력으로 받아 정답을 생성하는 Cross-Entropy Loss와 함께, 원본 문서의 전체적인 의미 공간을 유지하기 위한 보조 손실 함수가 사용될 수 있다.</p>
<h2>5.  결합 최적화 (Joint Optimization) 및 미분 가능한 검색</h2>
<p>SCP를 통해 문서를 효과적으로 압축할 수 있게 되었다면, 다음 단계는 이 압축된 표현을 기반으로 검색과 생성을 연결하는 것이다. CLaRa는 이 단계에서 **미분 가능한 Top-k 선택(Differentiable Top-k Selection)**을 도입하여 기존 RAG의 최적화 단절 문제를 해결한다.</p>
<h3>5.1  그래디언트 흐름의 문제와 해결</h3>
<p>앞서 언급했듯, 일반적인 <span class="math math-inline">k</span>개의 문서를 선택하는 작업은 미분 불가능하다. 즉, <span class="math math-inline">argmax</span>나 <span class="math math-inline">top-k</span> 연산은 그래디언트를 0으로 만들거나 정의되지 않게 만든다. CLaRa는 이를 우회하기 위해 <strong>Straight-Through (ST) Estimator</strong>를 활용한다.1</p>
<ul>
<li>
<p><strong>학습 단계 (Training):</strong> 검색된 문서들의 분포를 Softmax를 통해 연속적인 확률 분포로 근사한다. 이를 통해 생성기의 손실(Loss)이 검색기의 점수(Score) 계산 파트까지 역전파될 수 있는 ‘소프트(Soft)’ 경로를 만든다.</p>
</li>
<li>
<p><strong>추론 단계 (Inference):</strong> 실제 문서를 선택해야 하므로, 가장 확률이 높은 <span class="math math-inline">k</span>개의 문서를 선택하는 ‘하드(Hard)’ 방식을 사용한다.</p>
</li>
<li>
<p>ST Estimator의 수식:<br />
<span class="math math-display">
Z = Z_{hard} + (Z_{soft} - SG(Z_{soft}))
</span><br />
여기서 <span class="math math-inline">SG</span>는 Stop-Gradient 연산자이다. 이 수식의 의미는 순전파(Forward pass) 시에는 <span class="math math-inline">Z_{hard}</span>의 값(이산적 선택)을 사용하되, 역전파(Backward pass) 시에는 <span class="math math-inline">Z_{soft}</span>의 기울기(연속적 미분값)를 사용하겠다는 것이다. 이를 통해 모델은 추론 시에는 명확한 문서를 선택하면서도, 학습 시에는 미분 가능한 특성을 유지할 수 있다.7</p>
</li>
</ul>
<h3>5.2  단일 언어 모델링 손실 (Unified Language Modeling Loss)</h3>
<p>CLaRa는 검색기의 성능을 높이기 위해 별도의 검색 레이블(Relevance Labels)을 필요로 하지 않는다. 대신, 최종 답변 생성의 정확도를 높이는 방향으로 검색기를 유도한다.<br />
<span class="math math-display">
L_{CLaRa}(\theta_{qr}, \theta_g) = - \sum_{t=1}^{|R^*|} \log p_{\theta_g}(a^*_t | Q, M^{(k)}, a^*_{&lt;t}) + \lambda L_{MSE}
</span></p>
<ul>
<li><strong><span class="math math-inline">L_{CE}</span> (Cross-Entropy):</strong> 정답 답변 <span class="math math-inline">a^*</span>를 생성할 확률을 최대화한다. 이 손실은 생성기 <span class="math math-inline">\theta_g</span>뿐만 아니라, 미분 가능한 검색 경로를 통해 쿼리 추론기 <span class="math math-inline">\theta_{qr}</span>까지 전파된다. 즉, 쿼리 추론기는 “생성기가 정답을 더 잘 맞추게 하는 문서“를 찾아내도록 학습된다.1</li>
<li><strong><span class="math math-inline">L_{MSE}</span> (Mean Squared Error):</strong> 압축된 벡터가 원본 문서의 의미론적 공간에서 너무 멀어지지 않도록 제어하는 정규화(Regularization) 항이다. 이는 벡터가 과도하게 Task-specific하게 변형되는 것을 방지하고 범용성을 유지하게 돕는다.1</li>
</ul>
<h2>6.  실험 결과 및 성능 평가</h2>
<p>CLaRa의 성능을 검증하기 위해 연구진은 Natural Questions (NQ), HotpotQA, MuSiQue, 2WikiMultiHopQA 등 다양한 벤치마크 데이터셋을 활용하여 광범위한 실험을 수행하였다.7</p>
<h3>6.1  실험 설정 (Experimental Setup)</h3>
<ul>
<li><strong>데이터셋:</strong> 단일 홉(Single-hop) 질문을 위한 NQ, 멀티 홉(Multi-hop) 추론을 위한 HotpotQA, MuSiQue, 2WikiMultiHopQA.</li>
<li><strong>베이스라인:</strong> BM25(희소 검색), BGE(밀집 검색)를 사용하는 기본 RAG, 그리고 최신 압축 기반 RAG 모델인 AutoCompressor, PISCO, LLMLingua-2 등.</li>
<li><strong>평가 지표:</strong> 검색 성능을 위한 Recall@k, 생성 품질을 위한 Exact Match (EM), F1 Score.</li>
<li><strong>압축률 (Compression Ratio, CR):</strong> 4x, 16x, 32x, 64x, 128x 등 다양한 설정에서 테스트.</li>
</ul>
<h3>6.2  검색 및 압축 효율성 (Compression Efficiency vs. Retrieval)</h3>
<p>CLaRa는 극단적인 압축률에서도 놀라운 성능 유지력을 보여주었다. 특히 SCP가 적용된 모델은 단순 텍스트 기반 검색이나 다른 압축 방법론을 일관되게 상회했다.</p>
<p><strong>표 1. 다양한 압축률에 따른 Mistral-7B 기반 모델의 검색 Recall 성능 비교</strong> 7</p>
<table><thead><tr><th><strong>모델 (Model)</strong></th><th><strong>압축률 (CR)</strong></th><th><strong>NQ</strong></th><th><strong>HotpotQA</strong></th><th><strong>MuSiQue</strong></th><th><strong>2Wiki</strong></th><th><strong>평균 (Average)</strong></th></tr></thead><tbody>
<tr><td>BM25 (Text)</td><td>1x</td><td>6.57</td><td>21.89</td><td>-</td><td>-</td><td>-</td></tr>
<tr><td>Mistral-7B (w/o BGE)</td><td>1x</td><td>35.01</td><td>27.55</td><td>5.38</td><td>38.45</td><td>26.60</td></tr>
<tr><td>Mistral-7B (w/ BGE)</td><td>1x</td><td>54.58</td><td>42.94</td><td>8.94</td><td>44.24</td><td>37.67</td></tr>
<tr><td><strong>CLaRa (SCP-Mistral)</strong></td><td><strong>4x</strong></td><td><strong>57.05</strong></td><td><strong>45.09</strong></td><td><strong>10.34</strong></td><td><strong>46.94</strong></td><td><strong>39.86</strong></td></tr>
<tr><td>LLMLingua-2</td><td>4x</td><td>47.53</td><td>37.05</td><td>9.02</td><td>44.35</td><td>34.49</td></tr>
<tr><td><strong>CLaRa (SCP-Mistral)</strong></td><td><strong>16x</strong></td><td>55.56</td><td>43.72</td><td>10.55</td><td>46.00</td><td>38.96</td></tr>
<tr><td>PISCO</td><td>16x</td><td>54.39</td><td>41.94</td><td>10.09</td><td>44.88</td><td>37.83</td></tr>
<tr><td><strong>CLaRa (SCP-Mistral)</strong></td><td><strong>128x</strong></td><td>53.36</td><td>41.37</td><td>10.26</td><td>46.40</td><td>37.85</td></tr>
<tr><td>XRAG</td><td>128x</td><td>32.35</td><td>25.16</td><td>3.64</td><td>28.79</td><td>22.48</td></tr>
</tbody></table>
<p>위 표에서 주목할 점은 다음과 같다:</p>
<ol>
<li><strong>압축이 성능을 향상시킨다:</strong> CLaRa(4x)는 원문 전체를 사용하는 BGE 검색(1x)보다 평균 2.19% 포인트 더 높은 Recall을 기록했다. 이는 SCP가 문서 내의 노이즈를 제거하고 핵심 정보만을 정제했기 때문에, 검색기가 더 명확한 신호를 포착할 수 있었음을 의미한다.7</li>
<li><strong>압축 내성 (Robustness):</strong> 압축률을 4x에서 128x로 32배나 높였음에도 불구하고, 평균 성능 저하는 약 2% 포인트에 불과했다(39.86 -&gt; 37.85). 반면, 경쟁 모델인 XRAG는 128x 압축 시 성능이 급격히 하락하여 22.48에 그쳤다. 이는 CLaRa의 벡터 표현이 정보 밀도가 극도로 높음을 증명한다.</li>
</ol>
<h3>6.3  생성 품질 (Generation Quality)</h3>
<p>단순히 관련 문서를 잘 찾는 것뿐만 아니라, 최종 답변의 정확도에서도 CLaRa는 우수성을 입증했다.</p>
<ul>
<li><strong>Oracle 설정:</strong> 정답 문서가 검색 후보군에 포함된 이상적인 상황(Oracle)에서 CLaRa는 NQ와 HotpotQA 데이터셋에 대해 F1 점수 75% 이상을 기록하였다.9 이는 압축된 벡터가 원본 텍스트를 대체하여 생성기의 입력으로 사용되기에 충분한 정보를 담고 있음을 보여준다.</li>
<li><strong>End-to-End 성능:</strong> 실제 검색과 생성을 결합한 시나리오에서도 CLaRa는 텍스트 기반의 Fine-tuned 베이스라인 모델들을 상회하는 성과를 보였다.2 특히 멀티 홉 추론이 필요한 HotpotQA에서 4x 압축 시 96.21%의 Recall을 달성했다는 점은9, 복잡한 추론 과정에서 잠재 벡터 간의 연결성이 효과적으로 작동했음을 시사한다.</li>
</ul>
<h2>7.  논의: RAG의 미래와 산업적 함의</h2>
<p>CLaRa 프레임워크는 RAG 기술의 발전 방향에 있어 몇 가지 중요한 시사점을 던진다.</p>
<h3>7.1  텍스트 중심에서 의미 중심으로의 전환</h3>
<p>지금까지의 RAG는 ’텍스트’를 매개체로 삼았다. 하지만 CLaRa는 기계가 처리하기 가장 효율적인 형태인 ’연속 벡터(Continuous Vector)’를 매개체로 삼는다. 이는 인간과 기계의 인터페이스(입출력)는 텍스트로 하되, 기계 내부의 사고 과정(Processing)은 전적으로 잠재 공간에서 이루어지는 것이 효율적이라는 점을 보여준다. SCP의 ‘소화(Digestion)’ 개념은 미래의 LLM이 방대한 지식을 텍스트 그대로 저장하는 것이 아니라, 추상화된 형태로 내부 메모리에 통합하는 방향으로 진화할 것임을 예고한다.3</p>
<h3>7.2  온디바이스(On-device) AI를 위한 돌파구</h3>
<p>본 연구가 Apple 연구진에 의해 주도되었다는 점은 매우 중요하다.2 스마트폰이나 노트북과 같은 엣지 디바이스는 메모리와 배터리 제약이 심하다. CLaRa는 16배에서 최대 128배의 압축을 통해, 제한된 메모리 내에서 기존보다 훨씬 더 많은 문맥(Long Context)을 다룰 수 있게 해준다. 예를 들어, 128배 압축을 적용하면 1GB의 메모리로 128GB 분량의 텍스트 문맥을 처리하는 효과를 낼 수 있다. 이는 개인화된 AI 비서가 사용자의 로컬 데이터를 서버로 전송하지 않고도 기기 자체에서 효율적으로 검색하고 추론할 수 있는 가능성을 열어준다.10</p>
<h3>7.3  데이터 효율적인 RAG 구축</h3>
<p>CLaRa는 관련성 레이블이 없는 데이터셋에서도 생성 손실(Generation Loss)만으로 검색기를 학습시킬 수 있다. 이는 기업이 사내 문서를 RAG에 도입할 때, 별도의 고비용 검색 데이터셋 구축 과정 없이 문서 자체만으로 고성능 검색기를 최적화할 수 있음을 의미한다. SCP를 위한 합성 데이터 생성 비용이 들지만, 이는 한 번 구축하면 영구적으로 사용할 수 있는 상각 비용(Amortized Cost)이므로 장기적으로는 경제적이다.</p>
<h3>7.4  잠재적 한계 및 향후 과제</h3>
<p>물론 CLaRa에도 한계는 존재한다. 잠재 공간에서의 검색은 사람이 해석하기 어렵다(Black Box). 검색된 벡터가 구체적으로 어떤 내용을 담고 있는지 텍스트로 복원해보기 전까지는 알 수 없기 때문에, 설명 가능한 AI(XAI) 측면에서는 불리할 수 있다. 또한, SCP 학습을 위해서는 고성능의 교사 모델(Teacher Model, 예: Qwen-32B)을 통한 데이터 합성이 선행되어야 하므로, 초기 구축 단계에서의 컴퓨팅 리소스 요구량이 적지 않다. 향후 연구는 이러한 해석 가능성을 높이고, SCP 학습 과정을 더욱 경량화하는 방향으로 진행되어야 할 것이다.</p>
<h2>8.  결론</h2>
<p>CLaRa(Continuous Latent Reasoning)는 검색 증강 생성(RAG) 시스템이 직면한 구조적 비효율과 최적화의 단절 문제를 근본적으로 해결하기 위해 제안된 혁신적인 프레임워크이다. 문서의 표면적 텍스트를 고밀도의 잠재 벡터로 변환하는 SCP 기술은 인간의 ‘소화’ 과정을 모방하여 정보의 본질만을 추출하며, 이를 통해 극적인 메모리 효율성과 추론 성능 향상을 동시에 달성했다. 또한, 미분 가능한 Top-k 선택을 통한 결합 학습은 검색기와 생성기가 서로를 돕는 선순환 구조를 만들어냈다.</p>
<p>실험 결과는 CLaRa가 단순한 압축 기술을 넘어, RAG 시스템의 작동 원리를 ’키워드 매칭’에서 ’잠재적 추론’으로 진화시켰음을 보여준다. 특히 128배의 압축률에서도 유지되는 강력한 성능은 향후 모바일 및 엣지 컴퓨팅 환경에서의 AI 활용 가능성을 크게 확장시켰다. CLaRa는 LLM이 외부 지식을 활용하는 방식에 있어 ’압축 네이티브(Compression-Native)’라는 새로운 표준을 제시하며, 이는 차세대 인공지능 시스템 설계에 있어 필수적인 참조 모델이 될 것이다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>Bridging Retrieval and Generation with Continuous Latent Reasoning, https://arxiv.org/pdf/2511.18659</li>
<li>CLaRa: Bridging Retrieval and Generation with Continuous Latent …, https://www.researchgate.net/publication/397934183_CLaRa_Bridging_Retrieval_and_Generation_with_Continuous_Latent_Reasoning</li>
<li>The RAG Illusion: Why “Grafting” Memory Is No Longer Enough, https://dzone.com/articles/rag-illusion-grafting-memory-limitations</li>
<li>Improving the Domain Adaptation of Retrieval Augmented Generation, https://www.scribd.com/document/739453497/Improving-the-Domain-Adaptation-of-Retrieval-Augmented-Generation-RAG-Models-for-Open-Domain-Question-Answering</li>
<li>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf</li>
<li>apple/ml-clara - GitHub, https://github.com/apple/ml-clara</li>
<li>Bridging Retrieval and Generation with Continuous Latent Reasoning, https://arxiv.org/html/2511.18659v1</li>
<li>Bridging Retrieval and Generation with Continuous Latent Reasoning, https://chatpaper.com/paper/212726</li>
<li>CLaRa: Bridging Retrieval and Generation with Continuous Latent …, https://www.alphaxiv.org/overview/2511.18659v2</li>
<li>Stop Worshipping The Vector Database: How To Build RAG …, https://abvcreative.medium.com/stop-worshipping-the-vector-database-how-to-build-rag-systems-that-actually-work-830c29e723c9</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>