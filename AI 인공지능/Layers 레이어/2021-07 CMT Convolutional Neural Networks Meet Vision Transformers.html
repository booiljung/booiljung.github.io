<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:CMT (Convolutional Neural Networks Meet Vision Transformers)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>CMT (Convolutional Neural Networks Meet Vision Transformers)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">딥러닝 레이어 (Layers)</a> / <span>CMT (Convolutional Neural Networks Meet Vision Transformers)</span></nav>
                </div>
            </header>
            <article>
                <h1>CMT (Convolutional Neural Networks Meet Vision Transformers)</h1>
<h2>1.  컴퓨터 비전 아키텍처의 진화</h2>
<h3>1.1  컨볼루션 신경망(CNN)의 시대: 귀납적 편향의 힘과 한계</h3>
<p>2010년대 컴퓨터 비전 분야의 발전은 컨볼루션 신경망(Convolutional Neural Networks, CNNs)의 부흥과 함께했다. CNN의 성공은 이미지 데이터의 본질적 특성을 효과적으로 활용하는 강력한 ’귀납적 편향(inductive bias)’에 기인한다.1 CNN의 핵심 구성 요소인 지역적 수용장(local receptive fields), 가중치 공유(weight sharing), 그리고 공간적 계층 구조(spatial hierarchy)는 모델이 이미지 내에서 공간적으로 인접한 픽셀들 간의 관계로부터 시작하여 점차 복잡하고 추상적인 특징을 학습하도록 유도한다.3 특히, 객체의 위치가 변하더라도 동일한 특징을 인식할 수 있는 변환 등변성(translation equivariance)은 객체 탐지와 같은 핵심 비전 과제에서 CNN을 매우 효율적이고 강력한 도구로 만들었다.2</p>
<p>그러나 이러한 설계는 명확한 한계를 내포하고 있었다. 작은 크기의 컨볼루션 커널을 반복적으로 쌓아 특징을 추출하는 방식은 본질적으로 지역적인 연산에 의존한다. 이로 인해 모델의 수용장은 점진적으로만 확장될 수 있으며, 이미지 내에서 멀리 떨어진 픽셀이나 객체 간의 장거리 의존성(long-range dependencies) 및 전역적 맥락(global context)을 포착하는 데 구조적인 어려움을 겪는다.3 이는 이미지의 전체적인 구조나 부분들 간의 복잡한 관계를 이해하는 능력을 제한하는 근본적인 약점으로 작용했다.</p>
<h3>1.2  Vision Transformer(ViT)의 등장: 글로벌 컨텍스트 모델링의 새로운 지평</h3>
<p>이러한 배경 속에서 자연어 처리(NLP) 분야를 석권한 트랜스포머(Transformer) 아키텍처를 비전 분야에 적용한 Vision Transformer(ViT)가 등장하며 새로운 패러다임을 제시했다.5 ViT는 이미지를 고정된 크기의 여러 패치(patch)로 분할하고, 이를 1차원 벡터 시퀀스로 변환한 뒤 트랜스포머 인코더에 입력한다. 이 구조의 핵심은 셀프 어텐션(self-attention) 메커니즘으로, 모든 패치 쌍 간의 상호작용 가중치를 동적으로 계산하여 각 패치가 이미지 전체의 맥락 속에서 어떤 의미를 갖는지 학습한다.1</p>
<p>ViT의 가장 큰 강점은 셀프 어텐션을 통해 단일 레이어 내에서 이미지 전체를 아우르는 진정한 의미의 전역적 수용장을 확보할 수 있다는 점이다. 이는 CNN이 여러 레이어를 거쳐야만 간접적으로 얻을 수 있었던 장거리 의존성을 훨씬 효과적으로 모델링할 수 있게 해주었다.1 하지만 ViT는 CNN이 가졌던 강력한 귀납적 편향이 부재하여, 이미지의 공간적 구조에 대한 사전 지식 없이 모든 것을 데이터로부터 학습해야 했다. 이로 인해 상대적으로 작은 데이터셋에서는 성능이 저하되는 ‘데이터 갈증(data-hungry)’ 현상이 나타났으며, 대규모 데이터셋에서의 사전 훈련이 필수적이었다.3 또한, 셀프 어텐션의 계산 복잡도는 입력 시퀀스의 길이(즉, 패치의 수)에 제곱으로 비례하는 <span class="math math-inline">O(N^2C)</span>의 특성을 가져, 고해상도 이미지를 처리할 때 막대한 계산 비용과 메모리를 요구하는 실용적인 한계를 보였다.1</p>
<h3>1.3  하이브리드 모델의 필요성: 두 패러다임의 교차점</h3>
<p>ViT의 등장은 역설적으로 CNN이 가진 귀납적 편향의 가치를 재조명하는 계기가 되었다. ViT가 대규모 데이터셋에서는 CNN을 능가하는 성능을 보였지만 5, 중소 규모 데이터셋에서는 고전하는 모습은 이미지 데이터에 내재된 공간적 구조를 효율적으로 학습하는 능력이 얼마나 중요한지를 명확히 보여주었다. 이는 단순히 ’ViT가 더 우수하다’는 1차원적 결론을 넘어, ’어떤 조건에서 어떤 아키텍처가 더 효과적인가’라는 근본적인 질문을 제기했다.</p>
<p>이 질문에 대한 해답을 찾는 과정에서, 두 아키텍처의 장점을 결합하려는 하이브리드 모델의 등장은 필연적인 수순이었다. CNN과 ViT는 서로의 단점을 보완할 수 있는 이상적인 상호 보완 관계에 있었다.8 CNN을 통해 초기 레이어에서 지역적 특징과 공간적 계층 구조를 효율적으로 추출하고, 이렇게 정제된 특징 표현을 트랜스포머가 이어받아 전역적 맥락과 장거리 관계를 모델링하는 접근법은 논리적 타당성을 지녔다.4 CMT(Convolutional Neural Networks Meet Vision Transformers)는 바로 이러한 철학을 정교하게 구현하여, 두 패러다임의 시너지를 극대화한 대표적인 하이브리드 아키텍처이다.10</p>
<h2>2.  CMT: CNN과 Transformer의 시너지적 결합</h2>
<h3>2.1  설계 철학: 지역적 특징과 전역적 의존성의 조화</h3>
<p>CMT의 핵심 설계 철학은 CNN의 효율적인 지역적 특징 모델링 능력과 트랜스포머의 강력한 장거리 의존성 포착 능력을 유기적으로 결합하는 데 있다. 이를 통해 정확도와 계산 효율성 측면에서 기존의 순수 CNN 및 순수 트랜스포머 기반 모델들을 모두 능가하는 새로운 하이브리드 네트워크를 개발하는 것을 목표로 삼았다.4 CMT는 단순히 두 아키텍처를 순차적으로 연결하는 수준을 넘어, 트랜스포머의 구조적 약점을 CNN의 요소로 적극적으로 보완하고, 동시에 CNN의 전역적 수용장 한계를 트랜스포머로 극복하는 상호 보완적 설계를 추구했다.</p>
<h3>2.2  CMT가 해결하고자 하는 ViT의 근본적 문제점 분석</h3>
<p>CMT의 아키텍처는 ViT가 가진 근본적인 문제점들을 체계적으로 해결하기 위한 공학적 해법의 집합체로 볼 수 있다. CMT 연구진은 ViT의 약점을 다음과 같이 세 가지로 명확하게 정의하고, 각 문제에 대한 직접적인 해결책을 아키텍처에 통합했다.4</p>
<ol>
<li><strong>2D 구조 및 지역 정보 무시:</strong> ViT는 이미지를 1차원 토큰 시퀀스로 취급함으로써 각 패치가 지닌 고유한 2D 공간 구조와 그 내부의 미세한 지역적 관계 정보를 효과적으로 활용하지 못한다.</li>
<li><strong>고정된 패치 크기로 인한 다중 스케일 특징 추출의 어려움:</strong> ViT는 전체 이미지에 대해 고정된 크기의 패치를 사용하므로, 다양한 크기의 객체를 인식해야 하는 객체 탐지나 의미론적 분할과 같은 밀집 예측(dense prediction) 작업에 필수적인 다중 스케일 특징 맵(multi-scale feature map)을 생성하기 어렵다.</li>
<li><strong>고해상도 이미지 처리의 비효율성:</strong> 셀프 어텐션의 <span class="math math-inline">O(N^2C)</span> 계산 복잡도는 이미지 해상도가 증가함에 따라 계산량과 메모리 요구량을 기하급수적으로 증가시켜, 고해상도 이미지를 다루는 실제 비전 응용 프로그램에서의 사용을 어렵게 만든다.</li>
</ol>
<p>CMT의 각 설계 요소는 이러한 문제점들을 명확하게 겨냥하고 있다. 예를 들어, ’CMT Stem’은 문제 1을 해결하기 위해 입력단에서부터 컨볼루션을 활용하며, 계층적 구조와 ’Patch Aggregation’은 문제 2에 대한 해답을 제시한다. 또한, CMT 블록 내의 ‘Lightweight Multi-head Self-attention(LMHSA)’ 모듈은 문제 3을 직접적으로 완화하기 위해 설계되었다. 이처럼 CMT는 이론적 우아함보다는 기존 모델의 명확한 병목 현상을 분석하고 이를 효율적으로 개선하는 실용적인 공학적 접근법을 통해 탄생했다.</p>
<h2>3.  CMT 아키텍처 심층 분석</h2>
<h3>3.1  전체 구조: 계층적 설계를 통한 다중 스케일 표현 학습</h3>
<p>CMT는 ResNet과 같은 현대적인 CNN의 성공적인 설계 원칙인 계층적 구조를 적극적으로 채택했다.12 입력 이미지는 초기 처리 단계인 ’CMT Stem’을 거친 후, 총 4개의 스테이지(Stage)를 순차적으로 통과한다. 각 스테이지를 거치면서 특징 맵의 공간적 해상도는 점진적으로 감소하고 채널 차원은 증가하는 피라미드 형태의 구조를 띤다.</p>
<p>이러한 계층적 설계 덕분에 CMT는 입력 이미지에 대해 각각 <span class="math math-inline">H/4 \times W/4</span>, <span class="math math-inline">H/8 \times W/8</span>, <span class="math math-inline">H/16 \times W/16</span>, <span class="math math-inline">H/32 \times W/32</span>의 해상도를 갖는 4개의 다중 스케일 특징 맵을 자연스럽게 생성할 수 있다. 이는 고정된 해상도의 특징 맵만을 사용하는 표준 ViT와 근본적으로 다른 점으로, 다양한 크기의 객체를 다루어야 하는 객체 탐지, 인스턴스 분할 등의 다운스트림 태스크에 모델을 쉽게 적용할 수 있게 하는 핵심적인 장점이다. 이로 인해 CMT는 단순한 이미지 분류기를 넘어 다재다능한 백본(versatile backbone) 아키텍처로서의 가치를 지닌다.4 예를 들어 CMT-S 모델은 4개의 스테이지에 각각 2, 2, 16, 2개의 CMT 블록을 포함하는 구조로 설계되었다.12</p>
<h3>3.2  입력 처리 단계: CMT Stem 및 Patch Aggregation의 역할</h3>
<p><strong>CMT Stem:</strong> ViT가 이미지를 겹치지 않는 패치로 분할하고 선형 투영하는 단순한 방식은 패치 내부의 풍부한 지역적 정보를 손실시키는 문제를 야기한다. CMT는 이 문제를 해결하기 위해 입력단에 컨볼루션 기반의 ’CMT Stem’을 도입했다.12 CMT Stem은 3개의 3x3 컨볼루션 레이어로 구성되며, 초기 레이어에서 스트라이드(stride) 2를 사용하여 입력 이미지의 해상도를 1/4로 줄이는 동시에, 컨볼루션 연산을 통해 저수준의 미세한 특징(fine-grained features)을 효과적으로 추출한다.</p>
<p><strong>Patch Aggregation:</strong> 각 스테이지의 시작 부분에는 ‘Patch Aggregation’ 모듈이 위치한다. 이 모듈은 스트라이드 2를 갖는 2x2 컨볼루션 연산을 통해 이전 스테이지에서 전달된 특징 맵의 공간 해상도를 절반으로 줄이고(다운샘플링), 동시에 채널 차원을 두 배로 확장하는 역할을 수행한다.12 이 과정은 모델이 점진적으로 더 넓은 영역의 정보를 통합하고 더 추상적인 특징을 학습하도록 유도하며, 계산 효율성을 유지하면서 계층적 표현을 생성하는 핵심 메커니즘으로 작동한다.</p>
<h3>3.3  핵심 혁신: CMT 블록의 세부 구성 요소</h3>
<p>CMT 아키텍처의 심장부는 바로 ’CMT 블록’이다. 각 스테이지는 여러 개의 CMT 블록을 순차적으로 쌓아 구성되며, 이 블록 내부에서 지역적 정보와 전역적 정보의 정교한 융합이 이루어진다. CMT 블록은 Local Perception Unit (LPU), Lightweight Multi-head Self-attention (LMHSA), Inverted Residual Feed-forward Network (IRFFN)라는 세 가지 핵심 모듈로 구성된다.4</p>
<h4>3.3.1  Local Perception Unit (LPU): 컨볼루션을 통한 지역 정보 강화</h4>
<p>LPU는 ViT에서 사용되는 절대 위치 인코딩(absolute position encoding)이 이미지의 변환 불변성(translation-invariance)을 해치고, 패치 내부의 지역적 구조 정보를 무시하는 문제를 완화하기 위해 제안되었다.12 LPU는 3x3 깊이별 컨볼루션(Depth-wise Convolution, DWConv)과 잔차 연결(residual connection)로 구성된 매우 간단하면서도 효과적인 구조를 가진다. 이 모듈은 셀프 어텐션 연산에 들어가기 전에 입력 특징에 컨볼루션 연산을 적용함으로써, 모델이 지역적인 공간 정보를 명시적으로 학습하도록 돕는다. 수학적 정의는 다음과 같다.<br />
<span class="math math-display">
LPU(X) = DWConv(X) + X
</span></p>
<h4>3.3.2  Lightweight Multi-head Self-attention (LMHSA): 효율적인 글로벌 관계 모델링</h4>
<p>LMHSA는 표준 셀프 어텐션의 막대한 계산량을 줄여 고해상도 특징 맵을 효율적으로 처리하기 위해 고안된 핵심 모듈이다.12 핵심 아이디어는 쿼리(Query, Q)는 그대로 두되, 어텐션 연산에 사용될 키(Key, K)와 값(Value, V)의 공간적 크기를</p>
<p><span class="math math-inline">k \times k</span> 스트라이드 깊이별 컨볼루션을 통해 미리 축소하는 것이다. 예를 들어, 스트라이드 <span class="math math-inline">k=2</span>를 사용하면 K와 V의 토큰 수가 1/4로 줄어들어, 어텐션 맵 계산량이 크게 감소한다. 또한, 학습 가능한 상대적 위치 편향(relative position bias) <span class="math math-inline">B</span>를 어텐션 스코어에 더하여, 패치 간의 상대적인 위치 정보를 보다 유연하게 모델링한다. 이 편향 값은 추후 다른 해상도를 사용하는 다운스트림 태스크에 전이 학습 시, 간단한 이중입방 보간법(bicubic interpolation)을 통해 크기를 조절할 수 있어 유연성이 높다.4 LMHSA의 연산은 다음과 같이 정의된다.<br />
<span class="math math-display">
LightAttn(Q, K, V) = Softmax(Q K&#39;^{T} / \sqrt{d_k} + B)V&#39;
</span><br />
여기서 <span class="math math-inline">K&#39; = DWConv(K)</span> 이고 <span class="math math-inline">V&#39; = DWConv(V)</span> 이다.</p>
<h4>3.3.3  Inverted Residual Feed-forward Network (IRFFN): 표현력 증대를 위한 구조</h4>
<p>IRFFN은 ViT의 표준 FFN(Feed-forward Network)을 개선한 구조로, MobileNetV2와 같은 효율적인 CNN에서 널리 사용되는 역 잔차 블록(Inverted Residual Block)의 설계에서 영감을 받았다.4 표준 FFN이 두 개의 선형 레이어로 구성된 것과 달리, IRFFN은 채널을 확장하는 1x1 컨볼루션, 3x3 깊이별 컨볼루션, 그리고 다시 채널을 축소하는 1x1 투영 컨볼루션으로 구성된다. 중간에 깊이별 컨볼루션을 삽입함으로써 매우 적은 추가 계산 비용으로 지역적 정보를 한 번 더 포착하고 모델의 표현력을 향상시킨다. 또한, 잔차 연결의 위치를 변경하여 그래디언트 흐름을 개선하고 더 나은 성능을 달성했다. IRFFN의 구조는 다음과 같이 표현될 수 있다.<br />
<span class="math math-display">
IRFFN(X) = Conv_{proj}(F(Conv_{expand}(X)))
</span><br />
여기서 <span class="math math-inline">F(X) = DWConv(X) + X</span> 이다.</p>
<h3>3.4  스케일링 전략: CMT 모델군(Ti, XS, S, B, L)의 구성</h3>
<p>CMT는 단일 모델이 아니라, 다양한 계산 자원 및 성능 요구사항에 대응할 수 있도록 설계된 모델군(family of models)이다. 연구진은 기본 아키텍처의 깊이(각 스테이지의 블록 수)와 너비(채널 차원)를 체계적으로 조절하는 스케일링 전략을 통해, 가장 작은 CMT-Ti (Tiny)부터 CMT-XS (Extra Small), CMT-S (Small), CMT-B (Base), 그리고 가장 큰 CMT-L (Large)에 이르는 5가지 크기의 모델을 제안했다.12 이를 통해 사용자는 자신의 응용 분야에 맞는 최적의 정확도-효율성 트레이드오프를 갖는 모델을 선택할 수 있다.</p>
<table><thead><tr><th>구성 요소 (Component)</th><th>주요 역할 (Core Role)</th><th>수학적 정의 (Mathematical Definition)</th></tr></thead><tbody>
<tr><td>LPU</td><td>깊이별 컨볼루션을 통해 지역적/공간적 정보 강화</td><td><span class="math math-inline">LPU(X) = DWConv(X) + X</span></td></tr>
<tr><td>LMHSA</td><td>K, V 다운샘플링으로 셀프 어텐션 계산 효율화</td><td><span class="math math-inline">LightAttn(Q, K, V) = Softmax(Q K&#39;^{T} / \sqrt{d_k} + B)V&#39;</span></td></tr>
<tr><td>IRFFN</td><td>역 잔차 구조와 깊이별 컨볼루션을 활용한 표현력 증대</td><td><span class="math math-inline">IRFFN(X) = Conv_{proj}(F(Conv_{expand}(X)))</span></td></tr>
</tbody></table>
<h2>4.  실험적 성능 평가 및 분석</h2>
<h3>4.1  ImageNet 분류 벤치마크 성능</h3>
<p>CMT 아키텍처의 성능은 컴퓨터 비전 분야의 표준 벤치마크인 ImageNet-1K 데이터셋에서 철저히 검증되었다. 실험 결과, CMT 모델군은 기존의 CNN 및 트랜스포머 기반 모델들을 뛰어넘는 우수한 정확도-효율성 트레이드오프를 달성했다. 특히, 주력 모델인 CMT-S는 단 4.0B FLOPs의 계산량으로 83.5%의 높은 Top-1 정확도를 기록했으며, 더 큰 모델인 CMT-B는 9.3B FLOPs로 84.5%의 정확도를 달성했다.4</p>
<p>아래 표는 주요 경쟁 모델들과 CMT 모델군의 성능을 비교한 것이다. CMT-S는 유사한 계산량을 가진 Swin-T (4.5B FLOPs, 81.3%)나 EfficientNet-B4 (4.2B FLOPs, 82.9%)보다 높은 정확도를 보이면서도, 파라미터 수는 경쟁력 있는 수준을 유지했다. 이는 CMT의 하이브리드 설계가 단순히 정확도를 높이는 것을 넘어, 계산 자원을 매우 효율적으로 사용함을 입증한다.</p>
<table><thead><tr><th>모델</th><th>Top-1 정확도 (%)</th><th>파라미터 수 (M)</th><th>FLOPs (B)</th></tr></thead><tbody>
<tr><td>ResNet-50</td><td>76.2</td><td>26</td><td>4.1</td></tr>
<tr><td>RegNetY-4GF</td><td>80.0</td><td>21</td><td>4.0</td></tr>
<tr><td>DeiT-S</td><td>79.8</td><td>22</td><td>4.6</td></tr>
<tr><td>PVT-M</td><td>81.2</td><td>44</td><td>6.7</td></tr>
<tr><td>Swin-T</td><td>81.3</td><td>29</td><td>4.5</td></tr>
<tr><td>EfficientNet-B4</td><td>82.9</td><td>19</td><td>4.2</td></tr>
<tr><td><strong>CMT-Ti</strong></td><td><strong>79.1</strong></td><td><strong>9.5</strong></td><td><strong>0.6</strong></td></tr>
<tr><td><strong>CMT-XS</strong></td><td><strong>81.8</strong></td><td><strong>15.2</strong></td><td><strong>1.5</strong></td></tr>
<tr><td><strong>CMT-S</strong></td><td><strong>83.5</strong></td><td><strong>25.1</strong></td><td><strong>4.0</strong></td></tr>
<tr><td><strong>CMT-B</strong></td><td><strong>84.5</strong></td><td><strong>45.7</strong></td><td><strong>9.3</strong></td></tr>
</tbody></table>
<h3>4.2  다운스트림 태스크로의 전이 학습 능력</h3>
<p>CMT의 진정한 가치는 이미지 분류를 넘어 다양한 다운스트림 태스크에서도 강력한 성능을 발휘하는 다재다능한 백본으로서의 역할에서 드러난다.</p>
<p><strong>객체 탐지 및 인스턴스 분할:</strong> COCO val2017 데이터셋에서 CMT-S를 백본으로 사용한 RetinaNet과 Mask R-CNN은 뛰어난 성능을 보였다. RetinaNet의 경우, CMT-S 백본은 44.3%의 mAP를 달성하여, 더 많은 계산량을 요구하는 PVT-M 기반 모델을 3.9%p 차이로 능가했다.4 이는 CMT의 계층적 구조가 생성하는 다중 스케일 특징 맵이 밀집 예측 태스크에 매우 효과적임을 시사한다.13</p>
<p><strong>기타 분류 데이터셋:</strong> CIFAR-10, CIFAR-100, Flowers, Oxford-IIIT Pets 등 다양한 이미지 분류 데이터셋에 대한 전이 학습 실험에서도 CMT는 일관되게 높은 성능을 보이며 뛰어난 일반화 능력을 입증했다.4 예를 들어, CMT-S는 CIFAR-10에서 99.2%, Flowers 데이터셋에서 98.7%라는 매우 높은 정확도를 기록하여, ImageNet에서 학습된 특징이 다른 도메인의 데이터에도 효과적으로 전이됨을 보여주었다.</p>
<h3>4.3  효율성 분석: 정확도, 파라미터, FLOPs의 트레이드오프</h3>
<p>CMT의 가장 중요한 공헌은 단순히 높은 수준의 정확도를 달성한 것이 아니라, ’경쟁력 있는 정확도를 훨씬 높은 효율성으로 달성했다’는 점에 있다. 당시 SOTA 모델들은 성능을 극대화하기 위해 모델 크기와 계산량을 기하급수적으로 늘리는 경향이 있었지만, CMT는 정교한 아키텍처 설계를 통해 이러한 트렌드를 역행했다.</p>
<p>정확도-FLOPs 그래프를 분석해 보면, CMT 모델군은 기존의 CNN 및 트랜스포머 모델들보다 우상단에 위치하며, 더 나은 파레토 최적 곡선(Pareto frontier)을 형성한다.12 구체적으로 CMT-S는 DeiT-S와 비교했을 때, FLOPs가 약 14% 더 적음에도 불구하고 Top-1 정확도는 3.7%p 더 높다. 또한, 고도로 최적화된 CNN인 EfficientNet-B4와 비교해도 더 적은 FLOPs로 0.6%p 더 높은 정확도를 달성했다.4 이러한 결과는 CMT의 하이브리드 아키텍처가 성능과 효율성 사이의 균형을 탁월하게 맞추었음을 정량적으로 보여준다. 이러한 효율성은 모델을 실제 엣지 디바이스나 리소스가 제한된 환경에 배포하는 데 있어 중요한 실용적 가치를 지니며, 이는 AI 기술의 접근성을 높이고 지속 가능성에 기여하는 중요한 진전이라 할 수 있다.</p>
<h2>5.  CMT의 의의와 비판적 고찰</h2>
<h3>5.1  하이브리드 접근법의 유효성과 CMT의 공헌</h3>
<p>CMT의 성공은 CNN의 강력한 귀납적 편향과 트랜스포머의 전역적 모델링 능력을 결합하는 하이브리드 접근법의 잠재력을 명확하게 입증한 사례이다. CoaT(Co-Scale Conv-Attentional Image Transformers)나 CeiT(Convolution-enhanced image Transformer)와 같은 동시대의 다른 하이브리드 모델들과 함께, CMT는 비전 아키텍처 연구의 새로운 흐름을 형성했다.15</p>
<p>CMT는 단순히 두 아키텍처를 결합하는 개념적 제안에 그치지 않고, LMHSA와 IRFFN과 같이 계산 효율성을 극대화하는 구체적이고 실용적인 모듈을 제안함으로써 하이브리드 설계에 대한 명확한 청사진을 제시했다는 점에서 중요한 공헌을 했다. 이는 후속 연구들이 CNN과 트랜스포머의 시너지를 탐구하는 데 있어 중요한 기술적 기반을 제공했다.</p>
<h3>5.2  철학적 대비: 순수 CNN의 반격, ConvNeXt와의 비교 분석</h3>
<p>CMT와 거의 동시에 발표된 ConvNeXt는 비전 아키텍처 연구에 또 다른 중요한 관점을 제시했다.2 ConvNeXt는 Swin Transformer와 같은 현대적 ViT를 훈련시키는 데 사용된 향상된 훈련 기법(AdamW 옵티마이저, 대규모 데이터 증강 등)과 매크로 디자인(스테이지 비율 조정, ‘패치화’ 스템 등)을 순수 CNN인 ResNet에 점진적으로 적용하는 방식을 택했다. 놀랍게도, 이 연구는 셀프 어텐션 메커니즘을 전혀 사용하지 않고도 순수 컨볼루션 네트워크가 트랜스포머급 성능에 도달할 수 있음을 보였다.</p>
<p>CMT와 ConvNeXt의 철학은 근본적으로 다르다. CMT가 ’ViT의 약점을 CNN으로 보완’하는 하이브리드 접근법이라면, ConvNeXt는 ’ViT의 성공 요인(어텐션 제외)을 CNN에 이식’하는 접근법이다. ConvNeXt의 성공은 비전 모델의 성능 향상이 과연 셀프 어텐션 메커니즘 자체의 힘 때문인지, 아니면 그와 함께 도입된 현대적인 훈련 방식과 거시적 설계 덕분인지에 대한 근본적인 질문을 던졌다. 이 두 모델의 동시 등장은 비전 커뮤니티로 하여금 성능 향상의 원동력을 다각적으로 재평가하게 만들었다. 이는 ’트랜스포머 혁명’이 단순히 셀프 어텐션이라는 단일 기술에 의한 것이 아니라, 훈련 방법론과 아키텍처 설계 패러다임의 총체적인 변화였음을 시사한다. CMT와 ConvNeXt는 이 복잡한 현상을 해부하는 데 도움을 준 두 개의 중요한 실험으로 볼 수 있다.</p>
<h3>5.3  CMT 아키텍처의 내재적 한계와 향후 연구 방향</h3>
<p>CMT는 혁신적인 아키텍처임에도 불구하고 몇 가지 내재적 한계를 가지며, 이는 향후 연구의 방향을 제시한다.</p>
<p><strong>한계점:</strong></p>
<ul>
<li><strong>구조적 복잡성:</strong> CMT는 LPU, LMHSA, IRFFN 등 여러 특수 모듈을 정교하게 결합한 구조이다. 이는 ConvNeXt와 같은 순수 CNN 아키텍처에 비해 상대적으로 복잡하며, 구현 및 유지보수의 난이도를 높일 수 있다.17</li>
<li><strong>여전히 남은 계산량:</strong> LMHSA를 통해 셀프 어텐션의 계산량을 획기적으로 줄였지만, 근본적으로 어텐션 연산을 포함하고 있기 때문에 극도로 경량화된 모델이 요구되는 모바일이나 엣지 컴퓨팅 환경에서는 여전히 부담이 될 수 있다.18</li>
<li><strong>해석의 어려움:</strong> CNN과 트랜스포머라는 이질적인 두 메커니즘이 복잡하게 얽혀 있는 하이브리드 모델은, 각 구성 요소가 최종 예측에 어떻게 기여하는지 직관적으로 해석하기가 순수 아키텍처보다 더 어려울 수 있다.8</li>
</ul>
<p><strong>향후 연구 방향:</strong></p>
<ul>
<li><strong>동적 아키텍처:</strong> 입력 이미지의 내용이나 복잡도에 따라 컨볼루션 연산과 어텐션 연산의 비중을 동적으로 조절하여 계산 효율성을 더욱 최적화하는 연구가 가능하다.9</li>
<li><strong>새로운 융합 메커니즘:</strong> 현재의 직렬 또는 병렬적 결합 방식을 넘어, 두 아키텍처의 특징을 더욱 깊은 수준에서 융합하는 새로운 메커니즘에 대한 탐구가 필요하다.</li>
<li><strong>특수 도메인으로의 확장 및 최적화:</strong> CMT와 같은 하이브리드 모델의 원리를 의료 영상 분할 20, 3D 객체 탐지 23, 위성 이미지 분석 등 특정 도메인의 데이터 특성에 맞게 변형하고 최적화하는 연구가 활발히 진행될 수 있다.</li>
</ul>
<h2>6.  결론</h2>
<p>CMT(Convolutional Neural Networks Meet Vision Transformers)는 CNN의 효율적인 지역 정보 처리 능력과 트랜스포머의 강력한 전역적 컨텍스트 모델링 능력을 성공적으로 융합한 혁신적인 하이브리드 아키텍처이다. 이를 통해 당대 최고의 정확도-효율성 트레이드오프를 달성하며, 비전 아키텍처 연구의 새로운 가능성을 열었다. CMT의 계층적 구조, Local Perception Unit (LPU), Lightweight Multi-head Self-attention (LMHSA), Inverted Residual Feed-forward Network (IRFFN)와 같은 핵심 설계 원리들은 Vision Transformer가 가진 명확한 단점들을 체계적으로 해결하기 위한 정교한 공학적 해법이었다.</p>
<p>동시에, ConvNeXt의 등장은 비전 아키텍처의 성능을 결정하는 요인이 단순히 어텐션 메커니즘의 유무가 아니라, 훈련 방법론과 거시적 설계를 포함한 총체적인 접근 방식에 있음을 시사하며 중요한 학술적 논의를 촉발시켰다.</p>
<p>결론적으로, CMT는 하이브리드 모델의 잠재력을 최전선에서 입증한 중요한 이정표이다. 이는 CNN과 트랜스포머가 대립하는 관계가 아닌, 서로의 강점을 극대화할 수 있는 상호 보완적 파트너가 될 수 있음을 보여주었다. ConvNeXt와 함께, CMT는 향후 비전 아키텍처 연구가 나아갈 방향에 대한 깊은 통찰을 제공하는 모델로서 그 가치를 인정받고 있다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Vision Transformers vs CNNs at the Edge, https://www.edge-ai-vision.com/2024/03/vision-transformers-vs-cnns-at-the-edge/</li>
<li>A ConvNet for the 2020s, https://arxiv.org/pdf/2201.03545</li>
<li>Vision Transformers vs. Convolutional Neural Networks (CNNs) - GeeksforGeeks, https://www.geeksforgeeks.org/deep-learning/vision-transformers-vs-convolutional-neural-networks-cnns/</li>
<li>CMT: Convolutional Neural Networks Meet Vision Transformers - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_CMT_Convolutional_Neural_Networks_Meet_Vision_Transformers_CVPR_2022_paper.pdf</li>
<li>Vision Transformers vs. Convolutional Neural Networks | by Fahim Rustamy, PhD | Medium, https://medium.com/@faheemrustamy/vision-transformers-vs-convolutional-neural-networks-5fe8f9e18efc</li>
<li>A comparative study between vision transformers and CNNs in digital pathology - arXiv, https://arxiv.org/pdf/2206.00389</li>
<li>[D] Have transformers won in Computer Vision? : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/1hzn0gg/d_have_transformers_won_in_computer_vision/</li>
<li>A Hybrid Fully Convolutional CNN-Transformer Model for Inherently Interpretable Medical Image Classification - arXiv, https://arxiv.org/html/2504.08481v1</li>
<li>CNNs vs. Transformers: Image Recognition Models Explained - FlyPix AI, https://flypix.ai/blog/image-recognition-models-cnns/</li>
<li>CMT: Convolutional Neural Networks Meet Vision Transformers | Request PDF, https://www.researchgate.net/publication/353233884_CMT_Convolutional_Neural_Networks_Meet_Vision_Transformers</li>
<li>[2107.06263] CMT: Convolutional Neural Networks Meet Vision Transformers - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/2107.06263</li>
<li>[2107.06263] CMT: Convolutional Neural Networks Meet Vision Transformers - arXiv, https://arxiv.org/abs/2107.06263</li>
<li>Review — CMT: Convolutional Neural Networks Meet Vision Transformers - Sik-Ho Tsang, https://sh-tsang.medium.com/review-cmt-convolutional-neural-networks-meet-vision-transformers-1538c84a332a</li>
<li>CMT: Convolutional Neural Networks Meet Vision Transformers (Supplementary Material) - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/supplemental/Guo_CMT_Convolutional_Neural_CVPR_2022_supplemental.pdf</li>
<li>Co-Scale Conv-Attentional Image Transformers - arXiv, https://arxiv.org/pdf/2104.06399</li>
<li>[PDF] CMT: Convolutional Neural Networks Meet Vision Transformers | Semantic Scholar, https://www.semanticscholar.org/paper/CMT%3A-Convolutional-Neural-Networks-Meet-Vision-Guo-Han/0b036cd5dfc49d835d0c759c8ca31d89f2410e65</li>
<li>A Benchmark Study of Hybrid CNN-Transformer Architectures in Vision-Language Tasks - Emerging Publishing Society, https://emergingpub.com/index.php/sr/article/download/78/66/256</li>
<li>Small Vision-Language Models: A Survey on Compact Architectures and Techniques - arXiv, https://arxiv.org/html/2503.10665v1</li>
<li>Comparison of CNN, transformer, and hybrid models for defect detection… | Download Scientific Diagram - ResearchGate, https://www.researchgate.net/figure/Comparison-of-CNN-transformer-and-hybrid-models-for-defect-detection-in-civil_tbl1_387592787</li>
<li>BEFUnet: A Hybrid CNN-Transformer Architecture for Precise Medical Image Segmentation, https://www.semanticscholar.org/paper/BEFUnet%3A-A-Hybrid-CNN-Transformer-Architecture-for-Manzari-Kaleybar/8eec70f1671532df1c7cc006b3d8156e595d06b2</li>
<li>H2Former: An Efficient Hierarchical Hybrid Transformer for Medical Image Segmentation - A*STAR OAR, https://oar.a-star.edu.sg/storage/y/y3npzo5jmq/h2former.pdf</li>
<li>Systematic Review of Hybrid Vision Transformer Architectures for Radiological Image Analysis - PubMed, https://pubmed.ncbi.nlm.nih.gov/39871042/</li>
<li>Cross Modal Transformer: Towards Fast and Robust 3D Object Detection - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2023/papers/Yan_Cross_Modal_Transformer_Towards_Fast_and_Robust_3D_Object_Detection_ICCV_2023_paper.pdf</li>
<li>Cross Modal Transformer: Towards Fast and Robust 3D Object Detection - arXiv, https://arxiv.org/abs/2301.01283</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>