<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:FPN (Feature Pyramid Network, 2016)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>FPN (Feature Pyramid Network, 2016)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">딥러닝 레이어 (Layers)</a> / <span>FPN (Feature Pyramid Network, 2016)</span></nav>
                </div>
            </header>
            <article>
                <h1>FPN (Feature Pyramid Network, 2016)</h1>
<h2>1.  서론: 다중 스케일 인식 문제와 FPN의 등장</h2>
<p>컴퓨터 비전 분야의 핵심 목표 중 하나는 인간의 시각 시스템처럼 이미지 내 객체를 정확하게 인식하고 이해하는 것이다. 이 과정에서 가장 근본적이고 어려운 과제 중 하나는 객체의 ‘스케일(scale)’ 변화에 강건한 모델을 구축하는 것이다. 동일한 종류의 객체라도 카메라와의 거리, 촬영 각도, 이미지 해상도 등 다양한 요인에 의해 전혀 다른 크기로 나타나기 때문이다.1 이러한 스케일 불변성(Scale Invariance) 문제는 객체 탐지(Object Detection), 분할(Segmentation) 등 고수준 비전 태스크의 성능을 좌우하는 결정적인 요소로 작용해왔다. Feature Pyramid Network (FPN)는 이 오랜 난제를 해결하기 위해 등장한 혁신적인 아키텍처로, 기존 접근법들의 한계를 명확히 인식하고 그에 대한 우아한 공학적 해법을 제시했다. 본 고찰은 FPN의 등장 배경부터 시작하여 그 구조적 특징, 핵심 응용 사례, 그리고 후속 연구를 통한 발전 과정을 심층적으로 분석하고자 한다.</p>
<h3>1.1  컴퓨터 비전의 근본 과제: 스케일 불변성 (Scale Invariance)</h3>
<p>실세계의 시각 정보는 본질적으로 다중 스케일 특성을 갖는다. 예를 들어, 도시 풍경 이미지에는 거대한 빌딩부터 아주 작은 행인까지 다양한 크기의 객체가 공존한다. 인식 모델이 이러한 스케일 변화에 효과적으로 대응하지 못한다면, 특정 크기의 객체만 탐지하거나 동일한 객체를 다른 크기에서 인식하지 못하는 문제가 발생한다. 따라서 스케일 변화에 관계없이 일관된 인식 성능을 유지하는 능력, 즉 스케일 불변성은 고성능 컴퓨터 비전 시스템이 갖추어야 할 필수적인 속성이다.3 이 문제를 해결하기 위한 노력은 FPN 등장 이전부터 다양한 형태로 이루어져 왔으며, 각 접근법은 뚜렷한 장점과 함께 치명적인 한계를 안고 있었다.</p>
<h3>1.2  FPN 이전의 다중 스케일 접근법과 그 한계</h3>
<p>FPN의 혁신성을 이해하기 위해서는 이전 세대의 다중 스케일 접근법들이 직면했던 기술적 딜레마를 먼저 살펴볼 필요가 있다. 이들은 주로 정확도와 계산 효율성 사이의 상충 관계(trade-off) 속에서 각기 다른 선택을 했다.</p>
<h4>1.2.1  Featurized Image Pyramid (특징화된 이미지 피라미드)</h4>
<p>가장 직관적이고 이론적으로 강력한 방법은 입력 이미지 자체를 여러 해상도로 리사이징하여 ’이미지 피라미드’를 만드는 것이다. 이렇게 생성된 각기 다른 스케일의 이미지들은 동일한 합성곱 신경망(CNN)에 독립적으로 입력되어 특징을 추출한다.4 이 방식은 객체의 스케일 변화를 피라미드의 레벨 이동으로 상쇄시키기 때문에, 모델이 다양한 크기의 객체를 효과적으로 학습하고 탐지할 수 있도록 한다.2 실제로 이 접근법은 스케일 불변성 확보에 있어 매우 높은 성능을 보여주었다.4</p>
<p>하지만 이 방식의 가장 큰 문제는 ’계산적 비현실성’에 있다. 이미지 피라미드의 각 레벨마다 전체 네트워크 연산을 독립적으로 수행해야 하므로, 피라미드 레벨 수에 비례하여 연산량과 메모리 사용량이 폭발적으로 증가한다.4 이는 모델의 훈련 시간을 감당하기 어려울 정도로 늘리고, 추론(inference) 시에는 극심한 속도 저하를 유발하여 실시간 응용을 거의 불가능하게 만들었다.6 따라서 이론적 성능에도 불구하고 실제 시스템에서는 거의 사용되지 못했다.2</p>
<h4>1.2.2  Single Feature Map (단일 특징 맵)</h4>
<p>이미지 피라미드의 계산 비용 문제를 해결하기 위한 극단적인 대안은 CNN의 가장 깊은 계층에서 생성된 단 하나의 특징 맵만을 사용하는 것이다. 이 특징 맵은 입력 이미지 전체의 정보를 압축하고 있으며, 가장 높은 수준의 의미론적 정보(semantically strong features)를 담고 있다. YOLOv1과 같은 초기 단일 단계(one-stage) 탐지기들이 이러한 접근법을 채택했다.7</p>
<p>이 방식의 장점은 명확하다. 단 한 번의 순전파(forward pass) 연산만으로 예측을 수행하므로 속도가 매우 빠르다.9 그러나 이는 다중 스케일 정보를 완전히 포기하는 대가로 얻은 속도였다. 네트워크의 깊은 층을 통과하면서 고해상도 정보가 모두 소실되므로, 작은 객체에 대한 탐지 성능이 현저히 저하되고 객체의 정밀한 위치를 파악하는 데 어려움을 겪는다.1 결국, 속도를 위해 정확도를 희생하는 방식으로, 많은 응용 분야에서 요구하는 성능 기준을 충족시키지 못했다.</p>
<h4>1.2.3  Pyramidal Feature Hierarchy (피라미드 특징 계층)</h4>
<p>이미지 피라미드의 성능과 단일 특징 맵의 효율성 사이에서 절충안을 찾으려는 시도로, CNN이 순전파 과정에서 자연스럽게 생성하는 계층적 특징 맵들을 활용하는 방식이 등장했다. CNN은 각 컨볼루션 블록을 통과할 때마다 스트라이드(stride)나 풀링(pooling) 연산에 의해 특징 맵의 공간 해상도가 점차 감소하는 피라미드 형태의 구조를 내재하고 있다. SSD(Single Shot Detector)는 이러한 ‘인-네트워크(in-network)’ 특징 계층을 활용한 대표적인 모델이다.2</p>
<p>이 방식은 단일 입력 이미지에 대한 한 번의 순전파만으로 다중 스케일 특징 맵을 얻을 수 있어 연산 효율성이 높다는 장점을 가진다. 하지만 SSD는 중요한 한계를 드러냈다. SSD의 설계자들은 네트워크의 얕은 층에서 추출된 저수준 특징(low-level features)은 의미론적 정보가 부족하여 객체 인식에 오히려 해가 된다고 판단했다. 따라서 VGG 네트워크를 기준으로 conv4_3 이후의 깊은 층들만을 예측에 사용하고, 그 뒤에 새로운 컨볼루션 레이어를 추가하여 피라미드를 구성했다.2 이 결정은 두 가지 문제를 야기했다. 첫째, 고해상도를 가진 얕은 층의 특징 맵을 사용하지 않음으로써, 작은 객체 탐지에 매우 유용한 정보를 활용할 기회를 놓쳤다.2 둘째, 각 예측이 서로 다른 깊이의 특징 맵에서 독립적으로 이루어지기 때문에, 깊은 층의 ’강한 의미론적 정보’와 얕은 층의 ’풍부한 공간 정보’가 서로 분리되는 ‘의미론적 간극(semantic gap)’ 문제가 발생했다.2</p>
<h3>1.3  FPN의 제안: 효율성과 성능의 통합</h3>
<p>FPN은 바로 이러한 이전 접근법들의 실패에 대한 직접적인 응답으로 탄생했다. FPN의 설계는 다음과 같은 질문에서 출발했다: “이미지 피라미드의 강력한 다중 스케일 표현 능력을, SSD와 같은 단일 순전파 방식의 효율성으로 달성할 수 없을까? 그리고 SSD가 해결하지 못한 의미론적 간극 문제를 어떻게 극복할 수 있을까?”</p>
<p>FPN은 이 질문에 대한 해답으로, CNN의 내재적 특징 피라미드를 활용하되, 이를 수동적으로 선택하는 것이 아니라 능동적으로 재구성하는 새로운 아키텍처를 제안했다. 핵심 아이디어는 “거의 추가 비용 없이(marginal extra cost)” 모든 스케일의 특징 맵이 강력한 의미론적 정보를 갖도록 만드는 것이었다.2 이를 위해 FPN은 하향식 경로(top-down pathway)와 측면 연결(lateral connections)이라는 독창적인 구조를 도입하여, 깊은 층의 의미 정보를 얕은 층으로 전파하고 융합했다.</p>
<p>이러한 접근 방식은 특징 추출기의 역할을 재정의하는 패러다임의 전환을 가져왔다. FPN 이전의 모델들이 백본 네트워크를 주로 분류를 위한 표현 학습기로 간주하고 그 결과물 중 일부를 선택하여 사용했다면, FPN은 백본의 전체 계층 구조를 하나의 유기적인 시스템으로 보고, 이를 재가공하여 모든 스케일에서 고품질의 특징을 생성하는 ’특징 생성기’로 탈바꿈시켰다. 이는 이후 컴퓨터 비전 모델 설계에 있어 백본(backbone)과 헤드(head) 사이에 FPN과 같은 ‘넥(neck)’ 구조를 두는 표준 아키텍처 패턴을 확립하는 계기가 되었다.</p>
<table><thead><tr><th>방식</th><th>핵심 아이디어</th><th>장점</th><th>단점</th></tr></thead><tbody>
<tr><td><strong>Featurized Image Pyramid</strong></td><td>입력 이미지 자체를 여러 스케일로 리사이징하여 각각 CNN에 입력</td><td>- 스케일 불변성 확보에 가장 효과적 - 다양한 크기 객체에 대한 높은 탐지 성능</td><td>- 막대한 연산량과 메모리 요구 - 극도로 느린 훈련 및 추론 속도 - 실시간 적용 불가능</td></tr>
<tr><td><strong>Single Feature Map</strong></td><td>CNN의 마지막 계층에서 나온 단일 고수준 특징 맵만 사용</td><td>- 매우 빠른 연산 속도 - 구조적 단순함</td><td>- 다중 스케일 정보 부재 - 작은 객체 탐지 성능 매우 저하 - 정밀한 위치 정보 소실</td></tr>
<tr><td><strong>Pyramidal Feature Hierarchy (SSD)</strong></td><td>CNN의 순전파 과정에서 자연스럽게 생성되는 계층적 특징 맵 활용</td><td>- 단일 순전파로 다중 스케일 특징 획득 - 이미지 피라미드 대비 높은 효율성</td><td>- 저수준 특징 맵을 사용하지 않아 작은 객체 탐지에 한계 - 층간 의미론적 간극(semantic gap) 존재</td></tr>
</tbody></table>
<h2>2.  FPN 아키텍처의 해부</h2>
<p>FPN의 구조는 그 설계 철학인 ’의미론적 정보와 공간 정보의 융합’을 구현하기 위해 세 가지 핵심 요소로 구성된다: 상향식 경로(Bottom-up Pathway), 하향식 경로(Top-down Pathway), 그리고 이 둘을 잇는 측면 연결(Lateral Connections). 이 세 요소는 유기적으로 결합하여 FPN의 강력한 다중 스케일 특징 표현을 만들어낸다.</p>
<h3>2.1  핵심 철학: 의미론적 정보와 공간 정보의 융합</h3>
<p>FPN의 근본적인 설계 원리는 CNN의 서로 다른 계층이 가진 정보의 상보적인 특성을 활용하는 것이다. 네트워크의 깊은 층으로 갈수록 특징 맵의 공간 해상도는 낮아지지만, 더 넓은 수용장(receptive field)을 통해 객체의 정체성과 같은 추상적이고 의미론적으로 강한 정보를 포착한다.7 반대로, 얕은 층은 공간 해상도가 높아 객체의 정확한 위치, 경계선, 질감 등 정밀한 공간 정보를 풍부하게 담고 있지만, 의미론적으로는 약하다.2 FPN은 이 두 종류의 정보를 효과적으로 결합하여, 모든 스케일의 특징 맵이 높은 공간 해상도와 강한 의미론적 정보를 동시에 갖도록 만드는 것을 목표로 한다.1</p>
<h3>2.2  상향식 경로 (Bottom-up Pathway)</h3>
<p>상향식 경로는 FPN의 기반을 형성하며, 사실상 백본 네트워크(주로 ResNet이 사용됨)의 표준적인 순전파(feedforward) 계산 과정을 의미한다.10 입력 이미지는 이 경로를 따라 여러 컨볼루션 블록을 통과하면서 계층적인 특징으로 변환된다.</p>
<ul>
<li>
<p><strong>과정:</strong> 각 컨볼루션 스테이지(stage)를 거치면서 특징 맵의 공간 해상도는 일반적으로 2배씩 감소하고(scaling step of 2), 채널 수는 증가한다.7 이 과정에서 특징은 이미지의 원초적인 요소(모서리, 색상 패치 등)를 감지하는 저수준 표현에서, 점차 복잡한 패턴과 객체의 부분을 인식하는 고수준 표현으로 추상화된다.10</p>
</li>
<li>
<p><strong>출력:</strong> FPN은 각 스테이지의 마지막 레이어 출력을 피라미드 구축을 위한 원재료로 사용한다. 스테이지는 동일한 공간 해상도를 출력하는 레이어들의 집합으로 정의된다.7 ResNet을 백본으로 사용하는 경우, 각 잔차 블록(conv2, conv3, conv4, conv5)의 최종 출력이 선택되며, 이들을 각각 <span class="math math-inline">C_2, C_3, C_4, C_5</span>로 명명한다.10 이 특징 맵들은 입력 이미지 대비 각각 {4, 8, 16, 32} 픽셀의 스트라이드(stride)를 갖는다. 일반적으로 conv1의 출력은 메모리 점유가 매우 크기 때문에 피라미드 구축에서 제외된다.12 이 <span class="math math-inline">C_i</span>들이 바로 하향식 경로에서 사용될 공간 정보의 원천이다.</p>
</li>
</ul>
<h3>2.3  하향식 경로 (Top-down Pathway)와 측면 연결 (Lateral Connections)</h3>
<p>하향식 경로와 측면 연결은 FPN 아키텍처의 핵심 혁신이며, 분리해서 생각할 수 없는 상호보완적인 관계이다. 이 두 구조가 결합하여 의미론적 간극을 메우고 강력한 특징 피라미드를 완성한다.</p>
<ul>
<li><strong>하향식 경로의 목적:</strong> 이 경로는 가장 상위 레벨, 즉 공간적으로 가장 작지만 의미론적으로 가장 강한 특징 맵(<span class="math math-inline">C_5</span>에서 파생)에서 시작한다. 그리고 업샘플링(upsampling) 연산을 통해 점진적으로 공간 해상도를 복원하면서 아래 계층으로 내려온다.7 이 과정의 본질은 고수준의 의미론적 정보를 저해상도에서 고해상도 맵으로 ’전파(propagate)’하는 것이다.10</li>
<li><strong>측면 연결의 목적:</strong> 하향식 경로를 통해 업샘플링된 특징 맵은 의미론적으로는 풍부하지만, 여러 번의 다운샘플링과 업샘플링을 거치면서 위치 정보가 다소 부정확해지거나 흐려질 수 있다. 측면 연결은 이러한 정보 손실을 보정하는 역할을 한다. 각 하향식 레벨에서, 측면 연결은 상향식 경로의 동일한 공간 해상도를 가진 특징 맵(<span class="math math-inline">C_i</span>)을 가져와 융합한다. 이 <span class="math math-inline">C_i</span> 맵은 샘플링 횟수가 적어 훨씬 더 정밀하고 정확한 위치 정보를 담고 있기 때문이다.2</li>
<li><strong>융합 과정 (Iterative Process):</strong> FPN의 특징 융합은 최상위 레벨에서 시작하여 한 단계씩 아래로 진행되는 반복적인 과정이다.</li>
</ul>
<ol>
<li><strong>시작 (<span class="math math-inline">P_5</span> 생성):</strong> 가장 먼저, 상향식 경로의 최상위 특징 맵인 <span class="math math-inline">C_5</span>에 1x1 합성곱을 적용하여 채널 수를 FPN의 목표 채널 수 <code>d</code>(논문에서는 256으로 설정)로 줄인다. 이것이 하향식 경로의 시작점이자 피라미드의 최상위 레벨인 <span class="math math-inline">P_5</span>의 기반이 된다.10</li>
<li><strong>반복 (<span class="math math-inline">P_4, P_3, P_2</span> 생성):</strong> 상위 레벨의 피라미드 맵 <span class="math math-inline">P_{i+1}</span>을 2배 업샘플링한다. 이를 위해 계산적으로 효율적인 최근접 이웃 보간법(nearest neighbor interpolation)이 주로 사용된다.10</li>
<li>동시에, 측면 연결을 통해 상향식 경로의 해당 레벨 특징 맵 <span class="math math-inline">C_i</span>를 가져온다. 이 <span class="math math-inline">C_i</span>에도 1x1 합성곱을 적용하여 채널 수를 <code>d</code>로 통일시킨다. 이 1x1 합성곱은 단순히 채널 수를 맞추는 역할뿐만 아니라, 하향식 정보와 융합하기에 적합하도록 특징 표현을 조정하는 역할도 수행한다.1</li>
<li>업샘플링된 <span class="math math-inline">P_{i+1}</span>과 채널이 조정된 <span class="math math-inline">C_i</span>를 요소별 덧셈(element-wise addition)으로 병합한다. 이 덧셈 연산이 바로 의미론적 정보와 공간 정보가 만나는 지점이다.11</li>
<li><strong>후처리:</strong> 마지막으로, 병합된 특징 맵에 3x3 합성곱을 적용하여 최종 피라미드 맵 <span class="math math-inline">P_i</span>를 생성한다. 이 3x3 합성곱은 두 가지 중요한 역할을 한다. 첫째, 업샘플링 과정에서 발생할 수 있는 격자무늬 같은 앨리어싱(aliasing) 효과를 완화하여 특징 맵을 부드럽게 만든다.10 둘째, 각기 다른 출처에서 온 특징들을 자연스럽게 섞어주어 융합된 특징 표현을 정제(refine)한다.</li>
<li>이 과정은 가장 높은 해상도의 피라미드 레벨인 <span class="math math-inline">P_2</span>가 생성될 때까지 반복된다. 최종적으로, 입력 이미지 대비 {4, 8, 16, 32}배의 스트라이드를 가지며 모두 동일한 채널 수 <span class="math math-inline">d</span>를 갖는 특징 맵 세트 <span class="math math-inline">\{P_2, P_3, P_4, P_5\}</span>가 완성된다.</li>
</ol>
<p>FPN 아키텍처의 아름다움은 그 단순성에 있다. 복잡한 어텐션 메커니즘이나 게이트 없이, 1x1 합성곱, 업샘플링, 덧셈, 3x3 합성곱이라는 기본적인 연산들의 조합만으로 강력한 특징 융합을 구현했다. 논문 저자들은 더 복잡한 연결 모듈을 사용해도 성능 향상이 미미했다고 밝혔는데, 이는 FPN의 성공이 복잡한 연산이 아닌 ‘구조적 아이디어’ 자체의 힘에 있음을 시사한다.14 이러한 설계의 단순함과 강건함은 FPN이 특정 백본이나 태스크에 국한되지 않고 범용적인 특징 추출기로서 널리 채택될 수 있었던 핵심 요인이다.</p>
<h3>2.4  수학적 표현</h3>
<p>FPN의 하향식 경로와 측면 연결을 통한 융합 과정은 다음과 같은 재귀적 수식으로 간결하게 표현할 수 있다.</p>
<p>먼저, 피라미드의 최상위 레벨 <span class="math math-inline">P_5</span>는 <span class="math math-inline">C_5</span>로부터 직접 생성된다. 후처리를 위한 3x3 합성곱을 포함하면 다음과 같다.</p>
<p><span class="math math-display">
P_5 = \text{Conv}_{3 \times 3}(\text{Conv}_{1 \times 1}(C_5))
</span><br />
그 다음, <span class="math math-inline">i \in \{4, 3, 2\}</span>에 대해 하위 레벨 피라미드 맵 <span class="math math-inline">P_i</span>는 상위 레벨 맵 <span class="math math-inline">P_{i+1}</span>과 상향식 맵 <span class="math math-inline">C_i</span>를 융합하여 생성된다.</p>
<p><span class="math math-display">
P_i = \text{Conv}_{3 \times 3}(\text{Upsample}_{2 \times}(P_{i+1}) + \text{Conv}_{1 \times 1}(C_i))
</span><br />
여기서 <code>Conv</code>는 합성곱 연산을, <code>Upsample</code>은 2배 업샘플링 연산을, <code>+</code>는 요소별 덧셈을 의미한다. 이 수식을 통해 모든 피라미드 레벨 <span class="math math-inline">P_i</span>는 상위 레벨의 의미론적 정보(<span class="math math-inline">P_{i+1}</span>)와 현재 레벨의 공간 정보(<span class="math math-inline">C_i</span>)를 모두 포함하게 된다. 또한, 모든 <span class="math math-inline">P_i</span>는 동일한 채널 수 <code>d=256</code>을 갖도록 설계되어, 후속 태스크 헤드(head)가 파라미터를 공유하며 모든 스케일에서 일관되게 작동할 수 있는 기반을 마련한다.10</p>
<h2>3.  FPN의 주요 응용 사례 분석</h2>
<p>FPN은 이론적 우수성을 넘어, 실제 컴퓨터 비전 분야의 여러 핵심 태스크에서 SOTA(State-of-the-Art) 모델의 성능을 획기적으로 향상시키는 핵심 부품으로 자리매김했다. FPN은 독립적인 모델이라기보다는, 기존의 강력한 모델들과 결합하여 그 성능을 극대화하는 ’특징 추출 백본’으로서의 역할을 수행한다. 본 섹션에서는 객체 탐지, 인스턴스 분할, 그리고 의미론적 분할 분야에서 FPN이 어떻게 통합되고 작동하는지 구체적인 사례를 통해 심층적으로 분석한다.</p>
<h3>3.1  객체 탐지 (Object Detection)</h3>
<p>객체 탐지는 FPN이 처음 제안되고 그 효과를 입증한 가장 대표적인 분야이다. FPN은 2단계(two-stage) 탐지기와 1단계(one-stage) 탐지기 모두에 성공적으로 통합되었다.</p>
<h4>3.1.1  Faster R-CNN과의 통합</h4>
<p>FPN은 Faster R-CNN과 같은 지역 기반 탐지기의 특징 추출부를 대체하는 방식으로 작동한다.10 기존 Faster R-CNN이 백본 네트워크의 단일 스케일 특징 맵(예: VGG의 conv5_3) 위에서만 작동했던 것과 달리, FPN 기반 Faster R-CNN은 다중 스케일 예측을 수행한다.</p>
<ul>
<li>
<p><strong>RPN (Region Proposal Network) 적용:</strong> FPN 기반 RPN은 <span class="math math-inline">\{ P_2, P_3, P_4, P_5 \}</span>의 모든 피라미드 레벨에 걸쳐 독립적으로 작동한다. 각 레벨에는 해당 스케일에 맞는 단일 크기의 앵커 박스 세트(예: <span class="math math-inline">P_2</span>에는 <span class="math math-inline">32^2</span>, <span class="math math-inline">P_3</span>에는 <span class="math math-inline">64^2</span> 등)가 할당된다.15 이는 기존 RPN처럼 여러 스케일과 종횡비의 앵커를 한 레벨에서 모두 사용할 필요 없이, 각 피라미드 레벨이 특정 스케일의 객체를 전담하도록 하여 자연스럽게 다중 스케일 객체 제안을 생성하게 한다.10</p>
</li>
<li>
<p><strong>RoI 할당 전략:</strong> RPN이 생성한 수많은 RoI(Region of Interest)들은 그 크기에 따라 가장 적절한 피라미드 레벨에 할당되어 특징을 추출한다. 너비 <code>w</code>와 높이 <code>h</code>를 가진 RoI는 다음의 경험적 수식을 통해 목표 레벨 <code>k</code>가 결정된다 16:</p>
<p><span class="math math-display">
k = \lfloor k_0 + \log_2(\sqrt{wh} / 224) \rfloor
</span><br />
여기서 <span class="math math-inline">k_0</span>는 기준 레벨(보통 <span class="math math-inline">P_4</span>에 해당하는 4)이며, 224는 ImageNet 사전 학습 시 사용된 표준 입력 이미지 크기이다. 이 수식의 직관적인 의미는, <span class="math math-inline">224 \times 224</span> 크기의 RoI는 기준 레벨인 <span class="math math-inline">P_4</span>에 할당되고, 이보다 크기가 2배 커지면 한 단계 위인 <span class="math math-inline">P_5</span>에, 크기가 절반으로 작아지면 한 단계 아래인 <span class="math math-inline">P_3</span>에 할당되는 식이다. 이 전략은 큰 객체는 더 넓은 맥락을 보는 저해상도 맵에서, 작은 객체는 더 세밀한 디테일을 보는 고해상도 맵에서 특징을 추출하도록 하여, 각 RoI가 최적의 정보를 활용하게 한다.</p>
</li>
<li>
<p><strong>성능 향상:</strong> FPN을 Faster R-CNN에 적용한 결과는 놀라웠다. COCO 데이터셋에서 강력한 단일 스케일 ResNet 기반 모델 대비, AP(Average Precision)는 2.3점, AR(Average Recall)은 8.0점이나 향상되었다. 특히 작은 객체에 대한 성능 개선이 두드러졌다.2 더욱 인상적인 것은, 추가적인 연산이 거의 없음에도 불구하고 추론 속도가 이미지당 0.148초로, 기존 단일 스케일 모델(0.32초)보다 오히려 2배 이상 빨라졌다는 점이다. 이는 FPN이 더 효율적인 예측을 가능하게 했기 때문이다.10</p>
</li>
</ul>
<h4>3.1.2  RetinaNet: 조밀한 객체 탐지를 위한 FPN 활용</h4>
<p>FPN은 1단계 탐지기인 RetinaNet의 핵심 구성 요소이기도 하다. RetinaNet은 FPN과 Focal Loss라는 두 가지 혁신을 결합하여, 1단계 탐지기의 속도와 2단계 탐지기의 정확도를 모두 달성하고자 했다.5</p>
<ul>
<li><strong>아키텍처:</strong> RetinaNet은 ResNet-FPN 백본 위에, 파라미터를 공유하는 두 개의 작은 FCN(Fully Convolutional Network) 서브네트워크를 추가한 구조이다. 하나는 객체 분류를 위한 서브넷이고, 다른 하나는 바운딩 박스 회귀를 위한 서브넷이다.5</li>
<li><strong>FPN의 역할:</strong> RetinaNet에서 FPN은 더 넓은 범위의 스케일을 다루기 위해 확장된 피라미드 <span class="math math-inline">\{P_3, P_4, P_5, P_6, P_7\}</span>을 생성한다. <span class="math math-inline">P_3</span>부터 <span class="math math-inline">P_5</span>까지는 ResNet 백본의 <span class="math math-inline">C_3</span>~<span class="math math-inline">C_5</span> 출력으로부터 표준적인 FPN 방식으로 생성된다. 추가적인 레벨인 <span class="math math-inline">P_6</span>는 <span class="math math-inline">P_5</span>에 스트라이드 2를 갖는 3x3 합성곱을 적용하여 얻고, <span class="math math-inline">P_7</span>은 다시 <span class="math math-inline">P_6</span>에 동일한 연산을 적용하여 얻는다.17 이렇게 더 낮은 해상도의 특징 맵을 추가함으로써 매우 큰 객체까지 효과적으로 탐지할 수 있게 된다.</li>
<li><strong>서브네트워크 연동:</strong> 확장된 피라미드의 각 레벨(<span class="math math-inline">P_3</span>~<span class="math math-inline">P_7</span>)은 두 개의 서브네트워크에 독립적으로 입력된다. 이 서브네트워크들은 모든 피라미드 레벨에 걸쳐 파라미터를 공유한다. 이는 FPN이 생성한 모든 레벨의 특징 맵이 유사한 수준의 강력한 의미론적 정보를 가지고 있기에 가능한 설계이다. 각 레벨에서 분류 서브넷은 앵커 박스별로 객체의 존재 확률을 예측하고, 회귀 서브넷은 앵커 박스로부터 실제 객체 박스까지의 오프셋을 예측한다.5</li>
</ul>
<p>FPN은 RetinaNet이 조밀하게 배치된 수많은 앵커 박스에 대해 효율적으로 다중 스케일 예측을 수행할 수 있는 기반을 제공했으며, 이는 특히 항공 이미지나 고밀도 군중 장면처럼 작고 조밀한 객체가 많은 시나리오에서 뛰어난 성능을 발휘하는 원동력이 되었다.5</p>
<h3>3.2  인스턴스 분할 (Instance Segmentation): Mask R-CNN</h3>
<p>인스턴스 분할은 객체를 탐지할 뿐만 아니라, 각 객체 인스턴스를 픽셀 단위로 정확하게 분할하는 더 어려운 태스크이다. 이 분야의 표준 모델인 Mask R-CNN은 FPN을 백본으로 채택하여 성능을 한 단계 끌어올렸다.21</p>
<ul>
<li>
<p><strong>아키텍처:</strong> Mask R-CNN은 FPN 기반 Faster R-CNN 아키텍처에, 각 RoI에 대한 픽셀 단위 마스크를 예측하는 세 번째 브랜치(mask head)를 추가한 구조이다.23</p>
</li>
<li>
<p><strong>FPN의 역할:</strong> FPN은 객체 탐지에서와 마찬가지로, 다양한 크기의 객체에 대해 고품질의 다중 스케일 특징을 제공한다. RoI 할당 전략 또한 Faster R-CNN과 동일한 수식을 사용한다. FPN 덕분에 Mask R-CNN은 이미지 내의 작은 객체와 큰 객체 모두에 대해 일관되게 높은 분할 성능을 유지할 수 있다.24</p>
</li>
<li>
<p><strong>RoIAlign과의 시너지:</strong> Mask R-CNN의 또 다른 핵심 요소는 RoIAlign이다. RoIAlign은 기존의 RoIPooling에서 발생하는 좌표 양자화(quantization) 오류를 쌍선형 보간법(bilinear interpolation)을 통해 해결함으로써, RoI의 특징을 원본 특징 맵에서 정확하게 추출할 수 있게 한다.24 FPN과 RoIAlign의 결합은 특히 강력한 시너지를 발휘한다. FPN이 제공하는 고해상도 특징 맵(<span class="math math-inline">P_2</span>)은 작은 객체에 대한 RoIAlign의 성능을 극대화한다. 고해상도 맵 위에서 RoIAlign을 수행하면, 작은 객체의 미세한 경계 정보가 손실 없이 추출되어 매우 정교하고 정확한 분할 마스크를 생성할 수 있다. FPN이 없었다면, 저해상도 맵에서 추출된 부정확한 특징으로 인해 작은 객체의 마스크 품질은 크게 저하되었을 것이다.23</p>
</li>
</ul>
<h3>3.3  의미론적 및 파노ፕ틱 분할 (Semantic &amp; Panoptic Segmentation)</h3>
<p>FPN의 활용 범위는 객체 단위의 인식을 넘어, 이미지의 모든 픽셀을 분류하는 분할 태스크로 확장되었다.</p>
<ul>
<li><strong>FPN for Semantic Segmentation:</strong> 의미론적 분할은 이미지의 각 픽셀을 ‘하늘’, ‘도로’, ‘사람’ 등 미리 정의된 클래스로 분류하는 작업이다. FPN은 이 태스크에서 인코더-디코더 구조의 디코더 역할을 효과적으로 수행할 수 있다.26 일반적인 접근법은 FPN이 생성한 다중 스케일 특징 맵</li>
</ul>
<p><span class="math math-inline">\{P_2, P_3, P_4, P_5\}</span>를 모두 공통된 해상도(예: 가장 높은 해상도인 <span class="math math-inline">P_2</span>의 1/4 스케일)로 업샘플링한 후, 이들을 채널 축으로 연결(concatenate)하거나 요소별로 더하여 융합하는 것이다.27 이렇게 융합된 특징 맵은 다양한 스케일의 풍부한 컨텍스트 정보를 담고 있어, 최종적으로 픽셀 단위 분류를 수행하는 데 사용된다. 이를 통해 분할 경계의 정확도를 높이고, 크고 작은 구조물을 모두 잘 인식할 수 있게 된다.28</p>
<ul>
<li>
<p><strong>Panoptic FPN:</strong> 파노ፕ틱 분할은 인스턴스 분할(‘things’, 셀 수 있는 객체)과 의미론적 분할(‘stuff’, 배경 영역)을 하나의 통합된 프레임워크에서 동시에 수행하는 새로운 태스크이다. Panoptic FPN은 이 문제를 해결하기 위해 제안된 강력하고 효율적인 모델이다.29</p>
</li>
<li>
<p><strong>구조:</strong> Panoptic FPN은 Mask R-CNN 아키텍처를 기반으로 한다. 핵심 아이디어는 <strong>단일 FPN 백본을 공유</strong>하여, 그 위에 인스턴스 분할을 위한 헤드(기존 Mask R-CNN의 박스 및 마스크 헤드)와 의미론적 분할을 위한 별도의 헤드를 동시에 얹는 것이다.30</p>
</li>
<li>
<p><strong>의미론적 분할 브랜치:</strong> 이 새로운 브랜치는 FPN의 모든 레벨(<span class="math math-inline">P_2</span>~<span class="math math-inline">P_5</span>)에서 특징을 입력받는다. 각 레벨의 특징 맵은 일련의 (3x3 Conv -&gt; Group Norm -&gt; ReLU -&gt; 2x Upsample) 블록을 통해 점진적으로 1/4 스케일까지 업샘플링된다. 이렇게 모두 동일한 1/4 스케일이 된 특징 맵들은 요소별 덧셈으로 융합된다. 마지막으로 1x1 Conv와 4배 업샘플링을 거쳐 원본 이미지 해상도의 픽셀 단위 예측을 생성한다.30</p>
</li>
<li>
<p><strong>효율성:</strong> 두 가지 태스크를 위해 두 개의 분리된 네트워크를 사용하는 대신, 단일 FPN 백본을 공유함으로써 거의 절반의 연산량으로 두 작업을 동시에 수행하면서도 각 태스크에서 높은 성능을 유지할 수 있다. 이는 FPN이 다양한 태스크에 필요한 특징을 효과적으로 제공하는 범용적인 표현을 학습함을 보여준다.30</p>
</li>
</ul>
<p>이러한 응용 사례들은 FPN이 단순한 성능 향상 도구를 넘어, 백본과 태스크별 헤드 사이의 <strong>‘표준화된 다중 스케일 인터페이스’</strong> 역할을 수행했음을 보여준다. 개발자들은 백본의 복잡한 내부 계층 구조를 직접 다룰 필요 없이, <span class="math math-inline">\{P_2, P_3, P_4, P_5\}</span>라는 의미론적으로 풍부하고 일관된 다중 스케일 API를 통해 손쉽게 강력한 모델을 설계할 수 있게 되었다. 이러한 모듈화와 표준화는 FPN을 다양한 아키텍처에 ‘플러그 앤 플레이’ 방식으로 쉽게 통합할 수 있게 만든 핵심 성공 요인이자, 컴퓨터 비전 분야 전체의 발전을 가속화한 중요한 기여이다.</p>
<h2>4.  FPN의 장점과 본질적 한계</h2>
<p>FPN은 다중 스케일 객체 인식 분야에 큰 획을 그었으며, 그 성공은 명확한 장점들에 기반한다. 하지만 동시에 그 아키텍처에는 후속 연구들이 파고들 본질적인 한계점 또한 내재되어 있었다. FPN의 장점과 한계를 균형 있게 이해하는 것은 특징 피라미드 네트워크의 발전 계보를 파악하는 데 필수적이다.</p>
<h3>4.1  FPN의 핵심 장점</h3>
<p>FPN이 현대 컴퓨터 비전 아키텍처의 표준 구성 요소로 자리 잡을 수 있었던 이유는 다음과 같은 강력한 장점들 때문이다.</p>
<ul>
<li><strong>높은 성능과 정확도:</strong> FPN의 가장 큰 기여는 성능 향상이다. 네트워크의 얕은 층이 가진 고해상도의 정밀한 공간 정보와 깊은 층이 가진 고수준의 강한 의미론적 정보를 효과적으로 융합함으로써, 이전 모델들이 어려움을 겪었던 다중 스케일 객체 인식 문제를 해결했다.2 특히, 고해상도 특징 맵에 의미 정보를 부여함으로써 작은 객체에 대한 탐지 및 분할 성능을 전례 없는 수준으로 끌어올렸다.7</li>
<li><strong>연산 및 메모리 효율성:</strong> FPN은 이미지 피라미드 방식과 달리, 백본 네트워크가 단일 순전파 과정에서 생성한 특징 맵을 재활용한다. 하향식 경로와 측면 연결에 추가되는 연산은 대부분 1x1 및 3x3 합성곱으로 구성되어 있어 매우 가볍다. 그 결과, 이미지 피라미드 방식에 비해 극히 적은 추가 연산량과 메모리만으로 강력한 다중 스케일 표현을 구축할 수 있다.2 이는 제한된 컴퓨팅 자원 내에서도 고성능 모델의 훈련과 배포를 가능하게 하는 결정적인 장점이다.</li>
<li><strong>범용성 및 유연성:</strong> FPN은 특정 백본 아키텍처나 다운스트림 태스크에 종속되지 않는 범용 특징 추출기로 설계되었다. ResNet, VGG, DenseNet 등 다양한 백본 네트워크 위에 구축될 수 있으며, 그 출력은 객체 탐지, 인스턴스 분할, 의미론적 분할 등 광범위한 컴퓨터 비전 문제에 쉽게 적용될 수 있다.11 이러한 유연성은 FPN이 학계와 산업계 전반에 걸쳐 빠르게 확산되는 원동력이 되었다.</li>
<li><strong>End-to-End 학습 가능성:</strong> FPN을 포함한 전체 네트워크는 단일 손실 함수 아래에서 종단간(end-to-end) 학습이 가능하다. 이는 이미지 피라미드처럼 각 스케일을 별도로 처리하거나, 다단계 파이프라인을 구성할 필요 없이, 역전파 알고리즘을 통해 모든 파라미터를 한 번에 최적화할 수 있음을 의미한다. 이는 훈련 과정을 크게 단순화하고 효율성을 높인다.2</li>
</ul>
<h3>4.2  FPN의 본질적 한계</h3>
<p>모든 혁신적인 기술과 마찬가지로, FPN 역시 그 설계 철학에서 비롯된 몇 가지 본질적인 한계를 가지고 있다. 이러한 한계들은 이후 PANet, BiFPN 등 더 발전된 아키텍처의 등장 배경이 되었다.</p>
<ul>
<li>
<p><strong>단방향 정보 흐름 (One-way Information Flow):</strong> FPN의 핵심적인 정보 흐름은 상위 레벨(저해상도, 고수준 의미)에서 하위 레벨(고해상도, 저수준 의미)로 향하는 하향식 경로에 집중되어 있다. 이 구조는 고수준의 의미 정보를 아래로 전파하는 데는 효과적이지만, 반대 방향의 정보 흐름, 즉 얕은 층(<span class="math math-inline">C_2, C_3</span>)이 가진 정밀한 위치 정보가 상위 피라미드 레벨(<span class="math math-inline">P_4, P_5</span>)로 충분히 전달되는 데에는 한계가 있다. 이는 정보의 병목 현상을 유발하여, 고수준 특징 맵의 위치 정확도를 저해할 수 있다.32</p>
</li>
<li>
<p><strong>최상위 특징 맵의 정보 부족:</strong> 하향식 경로의 시작점인 피라미드 최상위 레벨(<span class="math math-inline">P_5</span>)은 오직 상향식 경로의 최상위 맵(<span class="math math-inline">C_5</span>)으로부터만 정보를 받는다. 다른 하위 레벨들(<span class="math math-inline">P_2, P_3, P_4</span>)이 상위 레벨과 현재 레벨의 정보를 모두 융합하는 혜택을 받는 것과 대조적이다. 이로 인해 <span class="math math-inline">P_5</span>는 다른 레벨에 비해 상대적으로 정보가 부족할 수 있으며, 이는 특히 매우 큰 객체를 인식하는 데 불리하게 작용할 수 있다.33 이 문제는 “최상위 계층의 저주(Curse of the Top Layer)“라고도 볼 수 있는데, 만약 백본 네트워크가</p>
</li>
</ul>
<p><span class="math math-inline">C_5</span>에서 객체에 대한 충분한 정보를 포착하지 못하면, 이 정보 손실이 하향식 경로를 따라 모든 하위 피라미드 레벨로 전파되어 전체 성능에 악영향을 미치는 구조적 취약점을 내포한다.</p>
<ul>
<li>
<p><strong>단순한 융합 방식의 한계:</strong> FPN은 서로 다른 의미론적 수준과 공간적 해상도를 가진 특징 맵(<span class="math math-inline">C_i</span>와 업샘플링된 <span class="math math-inline">P_{i+1}</span>)을 단순한 요소별 덧셈(element-wise addition)으로 융합한다. 이 방식은 모든 입력 정보가 동등한 중요도를 갖는다고 가정한다. 하지만 실제로는 특정 스케일의 정보가 다른 스케일의 정보보다 더 중요할 수 있다. 이처럼 정보의 중요도를 고려하지 않는 단순한 융합 방식은 최적이 아닐 수 있으며, 더 정교하고 적응적인 융합 메커니즘의 필요성을 시사한다.33</p>
</li>
<li>
<p><strong>아키텍처의 경험적 설계:</strong> FPN의 하향식 경로와 측면 연결 구조는 연구자의 깊은 통찰과 직관에 의해 설계되었지만, 이것이 모든 데이터셋이나 백본에 대해 최적의 토폴로지라는 보장은 없다. 즉, FPN의 범용성은 특정 문제에 대한 최적성을 희생한 결과일 수 있다. 더 복잡하거나 다른 형태의 연결 구조가 특정 태스크에서 더 나은 성능을 보일 가능성은 항상 존재한다. 이 ’일반성 vs 최적성’의 긴장 관계가 바로 NAS-FPN과 같은 후속 연구를 촉발한 근본적인 동기가 되었다.33</p>
</li>
</ul>
<p>결론적으로, FPN은 효율성과 성능 사이의 균형을 맞춘 획기적인 솔루션이었지만, 정보 흐름의 방향성, 융합 방식의 단순성, 그리고 설계의 고정성이라는 측면에서 개선의 여지를 남겼다. 이러한 한계점들은 이후 특징 피라미드 네트워크 연구가 나아갈 방향을 제시하는 중요한 이정표가 되었다.</p>
<h2>5.  FPN의 진화: 후속 연구 동향</h2>
<p>FPN이 제시한 특징 피라미드라는 패러다임은 수많은 후속 연구를 촉발시켰다. 이 연구들은 FPN의 본질적 한계를 극복하고, 더 효율적이거나 더 강력한 특징 융합 네트워크를 구축하는 것을 목표로 했다. 이 과정은 FPN의 아이디어를 계승, 비판, 통합하며 발전하는 변증법적 양상을 띤다. 본 섹션에서는 FPN 이후 등장한 대표적인 모델인 PANet, NAS-FPN, BiFPN을 중심으로 그 발전 계보를 추적한다.</p>
<h3>5.1  PANet (Path Aggregation Network): 양방향 정보 흐름의 도입</h3>
<ul>
<li>
<p><strong>문제의식:</strong> PANet은 FPN의 가장 큰 한계로 지적된 ‘단방향 정보 흐름’ 문제에 주목했다. FPN의 하향식 경로는 고수준의 의미 정보를 하위 계층으로 잘 전달하지만, 저수준 계층의 정밀한 위치 정보(localization information)가 상위 계층으로 전파되는 경로는 차단되어 있다. 이로 인해 고수준 특징 맵에서 객체의 위치를 예측할 때 정확도가 떨어질 수 있다고 보았다.32</p>
</li>
<li>
<p><strong>핵심 개선점:</strong> PANet은 이 문제를 해결하기 위해 FPN의 구조 위에 **추가적인 상향식 경로(Bottom-up Path Augmentation)**를 도입했다. 이 새로운 경로는 FPN이 최종적으로 생성한 특징 피라미드 <span class="math math-inline">\{P_2, P_3, P_4, P_5\}</span>를 다시 아래에서 위로, 즉 <span class="math math-inline">P_2</span>부터 <span class="math math-inline">P_5</span> 방향으로 정보를 융합해 나간다.13 구체적으로,</p>
</li>
</ul>
<p><span class="math math-inline">P_2</span>를 다운샘플링하여 <span class="math math-inline">P_3</span>와 융합하고, 그 결과를 다시 다운샘플링하여 <span class="math math-inline">P_4</span>와 융합하는 과정을 반복한다. 이 과정을 통해 가장 높은 해상도를 가진 <span class="math math-inline">P_2</span>의 정밀한 공간 정보가 상위 피라미드 레벨까지 효과적으로 전파되어, 모든 스케일의 특징 맵이 강한 의미 정보와 정확한 공간 정보를 모두 갖게 된다.32</p>
<ul>
<li><strong>결과:</strong> PANet은 FPN의 하향식 경로에 상향식 경로를 추가함으로써 정보 흐름을 양방향으로 만들었다. 이 간단하면서도 효과적인 개선을 통해 FPN의 정보 병목 현상을 완화하고, 특히 인스턴스 분할과 같이 정밀한 위치 정보가 중요한 태스크에서 성능을 향상시켰다.</li>
</ul>
<h3>5.2  NAS-FPN (Neural Architecture Search FPN): 최적의 경로 탐색</h3>
<ul>
<li><strong>문제의식:</strong> FPN과 PANet의 구조는 모두 인간 연구자의 직관과 경험에 기반한 수동 설계의 산물이다. NAS-FPN은 이러한 고정된 토폴로지가 과연 최적인지에 대한 근본적인 의문을 제기했다. 어쩌면 인간이 생각하지 못한 더 효율적이고 강력한 특징 융합 경로가 존재할 수 있다는 가설에서 출발했다.33</li>
<li><strong>핵심 개선점:</strong> NAS-FPN은 신경망 아키텍처 탐색(Neural Architecture Search, NAS) 기술을 FPN 설계에 도입했다. 미리 정의된 탐색 공간(search space) 안에서 다양한 특징 맵들을 어떻게 연결하고 융합할지에 대한 수많은 후보 구조(토폴로지)를 생성한다. 그리고 강화 학습(reinforcement learning)이나 진화 알고리즘(evolutionary algorithm)과 같은 탐색 전략을 사용하여 각 후보 구조의 성능을 평가하고, 점진적으로 더 나은 구조를 찾아 나간다.13 이 과정을 통해 데이터로부터 최적의 FPN 아키텍처를 자동으로 발견하고자 했다.</li>
<li><strong>장단점:</strong> NAS-FPN은 실제로 인간이 설계한 FPN이나 PANet을 능가하는 불규칙하고 복잡한 형태의 고성능 아키텍처를 발견하는 데 성공했다. 하지만 이는 막대한 계산 비용을 치른 결과였다. 최적의 구조를 탐색하는 과정은 수백에서 수천 GPU-days에 달하는 컴퓨팅 자원을 요구한다.33 또한, 이렇게 찾아낸 아키텍처는 특정 데이터셋(예: COCO)과 특정 백본(예: ResNet)에 과적합될 경향이 있어, 다른 데이터셋이나 백본으로 변경할 경우 다시 값비싼 탐색 과정을 거쳐야 하는 범용성 부족 문제를 안고 있다.33</li>
</ul>
<h3>5.3  BiFPN (Bidirectional FPN): 효율성과 성능의 새로운 균형</h3>
<p>BiFPN은 EfficientDet 논문에서 제안되었으며, FPN 진화의 변증법적 종합(synthesis)으로 평가받는다. BiFPN은 PANet의 효과적인 양방향 흐름과 NAS-FPN의 최적화 필요성을 모두 수용하되, 이를 훨씬 더 효율적이고 일반화 가능한 방식으로 해결했다.</p>
<ul>
<li><strong>핵심 개선점 1 (효율적인 양방향 연결):</strong> BiFPN은 PANet의 양방향 구조 아이디어를 계승했지만, 그 구조를 대폭 간소화했다. 예를 들어, 입력 연결이 하나뿐인 노드는 특징 융합에 거의 기여하지 않는다고 보고 과감히 제거했다. 이를 통해 더 적은 파라미터와 연산으로 유사한 양방향 정보 흐름을 구현했다.32 또한, 이 간소화된 양방향 블록을 하나의 단위 모듈로 취급하고, 이를 여러 번 반복적으로 쌓는 방식을 제안했다. 이는 더 깊은 수준의 특징 융합(high-level feature fusion)을 가능하게 하여 모델의 표현력을 더욱 강화한다.32</li>
<li><strong>핵심 개선점 2 (가중치 기반 특징 융합, Weighted Feature Fusion):</strong> BiFPN의 가장 중요한 혁신은 특징 융합 방식 자체에 있다. FPN과 PANet이 서로 다른 해상도에서 온 특징 맵들을 단순 덧셈으로 동일하게 취급했던 것과 달리, BiFPN은 각 입력 특징 맵의 중요도가 다를 수 있다는 점에 착안했다. 따라서 융합 시 각 입력 특징 맵에 학습 가능한 스칼라 가중치(learnable weight)를 부여하여 가중합(weighted sum)을 계산한다.33 이를 통해 네트워크는 훈련 과정에서 어떤 스케일의 정보에 더 집중해야 할지를 스스로 학습하여, 훨씬 더 효과적이고 적응적으로 특징을 융합할 수 있게 된다. 이 가중치 정규화 방식은 ’Fast normalized fusion’으로 구현되어 학습 안정성을 보장한다.32</li>
<li><strong>결과:</strong> BiFPN은 효율성과 성능이라는 두 마리 토끼를 모두 잡았다. PANet의 양방향 흐름을 더 효율적으로 구현하고, NAS-FPN이 추구했던 최적화를 ’가중치’라는 학습 가능한 파라미터를 통해 훨씬 저렴한 비용으로 달성했다. 이러한 혁신 덕분에 BiFPN을 채택한 EfficientDet은 적은 연산량으로 SOTA 수준의 정확도를 달성하며, 효율적인 객체 탐지 모델의 새로운 표준을 제시했다.35 BiFPN의 등장은 FPN 연구의 초점이 ’어떤 노드를 연결할 것인가(토폴로지)’에서 ’연결된 노드들을 어떻게 융합할 것인가(융합 메커니즘)’로 전환되는 중요한 계기가 되었다.</li>
</ul>
<table><thead><tr><th>모델</th><th>핵심 개선점</th><th>정보 흐름</th><th>특징 융합 방식</th><th>장점 / 단점</th></tr></thead><tbody>
<tr><td><strong>FPN</strong></td><td>하향식 경로와 측면 연결 도입</td><td>단방향 (Top-down)</td><td>요소별 덧셈</td><td><strong>장점:</strong> 효율성, 범용성, 준수한 성능 <strong>단점:</strong> 단방향 정보 흐름, 단순 융합</td></tr>
<tr><td><strong>PANet</strong></td><td>추가적인 상향식 경로 도입</td><td>양방향 (Top-down + Bottom-up)</td><td>요소별 덧셈</td><td><strong>장점:</strong> 위치 정보 강화, 성능 향상 <strong>단점:</strong> FPN 대비 연산량 증가</td></tr>
<tr><td><strong>NAS-FPN</strong></td><td>NAS를 통한 최적의 토폴로지 자동 탐색</td><td>비정형적, 데이터 기반</td><td>요소별 덧셈 또는 연결</td><td><strong>장점:</strong> 잠재적 최고 성능 달성 <strong>단점:</strong> 막대한 탐색 비용, 낮은 범용성</td></tr>
<tr><td><strong>BiFPN</strong></td><td>효율적 양방향 연결 + 가중치 기반 융합</td><td>반복적인 양방향</td><td>가중합 (Weighted Sum)</td><td><strong>장점:</strong> 높은 효율성, SOTA급 성능, 적응적 융합 <strong>단점:</strong> FPN 대비 구조적 복잡성 소폭 증가</td></tr>
</tbody></table>
<h2>6.  결론: FPN의 유산과 미래 전망</h2>
<p>Feature Pyramid Network (FPN)는 컴퓨터 비전, 특히 객체 인식 분야의 발전에 있어 하나의 이정표를 세운 기술이다. FPN은 단순히 기존 모델의 성능을 소폭 개선한 것이 아니라, 다중 스케일 문제를 접근하는 방식에 대한 근본적인 패러다임 전환을 이끌어냈다. 본 고찰은 FPN의 등장 배경, 아키텍처, 응용, 그리고 그 이후의 발전 과정을 심층적으로 분석함으로써 FPN이 남긴 유산과 미래의 가능성을 조망하고자 했다.</p>
<h3>6.1  FPN의 유산: 다중 스케일 모델링의 표준 확립</h3>
<p>FPN의 가장 큰 유산은 효율적이면서도 강력한 다중 스케일 특징 처리의 ’표준’을 확립했다는 점이다. FPN 이전, 연구자들은 이미지 피라미드의 엄청난 비용과 단일 특징 맵의 낮은 성능 사이에서 어려운 선택을 해야 했다. FPN은 CNN의 내재적 계층 구조를 재활용하고 재구성하는 우아한 해법을 통해 이 딜레마를 해결했으며, 이는 오늘날 거의 모든 고성능 객체 탐지 및 분할 모델의 기본 구성 요소가 되었다.</p>
<p>또한, FPN은 현대적인 컴퓨터 비전 모델 아키텍처를 ‘백본-넥-헤드(Backbone-Neck-Head)’ 구조로 개념화하는 데 결정적인 역할을 했다. 여기서 FPN은 특징 추출을 담당하는 백본과 최종 예측을 수행하는 헤드 사이에서 특징을 정제하고 융합하는 ’넥(Neck)’의 역할을 완벽하게 수행했다. 이 모듈화된 접근 방식은 모델 설계의 유연성과 확장성을 크게 높였다.</p>
<p>마지막으로, FPN의 성공은 ’특징 융합(feature fusion)’이라는 연구 분야 자체를 학계의 주요 주제로 부상시키는 기폭제가 되었다. FPN이 제시한 단순한 덧셈 방식의 한계를 극복하기 위해 PANet, BiFPN 등 더 정교하고 지능적인 융합 메커니즘에 대한 연구가 활발하게 이어졌으며, 이는 어텐션 메커니즘과 트랜스포머의 도입으로까지 이어지고 있다.</p>
<h3>6.2  미래 전망: 특징 피라미드의 다음 단계</h3>
<p>FPN의 기본 철학, 즉 ’계층적 특징의 재활용과 융합’은 앞으로도 오랫동안 유효할 것이지만, 그 구현 방식은 계속해서 진화할 것이다. 향후 특징 피라미드 네트워크 연구는 다음과 같은 방향으로 나아갈 것으로 전망된다.</p>
<ul>
<li><strong>동적 및 적응형 피라미드:</strong> 현재의 FPN 계열 모델들은 대부분 고정된 토폴로지와 융합 방식을 사용한다. 미래의 아키텍처는 입력 이미지의 내용이나 특정 태스크의 요구에 따라 피라미드의 연결 구조나 융합 가중치를 동적으로 조절하는 방향으로 발전할 것이다. 이미 트랜스포머를 활용하여 특징 맵 간의 전역적인 관계를 모델링하려는 시도들이 나타나고 있으며, 이는 더 유연하고 강력한 적응형 피라미드로 이어질 것이다.36</li>
<li><strong>경량화 및 효율성 극대화:</strong> 스마트폰, 드론, IoT 기기 등 엣지 디바이스에서의 실시간 AI 수요가 증가함에 따라, 성능을 최대한 유지하면서도 연산량과 메모리 사용량을 최소화한 초경량 FPN 구조에 대한 연구가 더욱 중요해질 것이다. 지식 증류(knowledge distillation), 신경망 가지치기(pruning), 양자화(quantization) 등의 기술이 FPN 경량화에 적극적으로 활용될 것이다.</li>
<li><strong>3D 및 비디오로의 확장:</strong> FPN의 성공은 주로 2D 정적 이미지에 국한되어 있었다. 자율 주행, 로보틱스, 증강 현실 등의 분야가 발전함에 따라, 3D 포인트 클라우드 데이터나 비디오 시퀀스에 대한 특징 피라미드 구축이 중요한 연구 주제가 될 것이다. 이는 공간적 스케일뿐만 아니라 시간적 스케일까지 고려하는 시공간적(spatio-temporal) 특징 피라미드의 형태로 발전할 가능성이 높다.</li>
</ul>
<p>결론적으로, FPN은 다중 스케일 인식 문제에 대한 강력하고 실용적인 해법을 제시함으로써 한 시대를 정의했다. 그 아이디어는 수많은 후속 연구에 영감을 주었으며, 컴퓨터 비전 기술의 발전을 한 단계 앞당겼다. 비록 새로운 아키텍처들이 계속해서 등장하겠지만, FPN이 확립한 ’효율적인 특징 계층 융합’이라는 핵심 원리는 미래의 컴퓨터 비전 모델을 설계하는 데 있어 변치 않는 지침으로 남을 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Feature Pyramid Network (FPN) - GeeksforGeeks, https://www.geeksforgeeks.org/computer-vision/feature-pyramid-network-fpn/</li>
<li>Feature Pyramid Networks for Object Detection - CVF Open Access, https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf</li>
<li>FPN Explained — Feature Pyramid Network | by Amit Yadav - Medium, https://medium.com/@amit25173/fpn-explained-feature-pyramid-network-7c0f65ea8f8b</li>
<li>FPN : Feature Pyramid Net 논문 리뷰 - beomseok_Oh - 티스토리, https://oh2279.tistory.com/162</li>
<li>How RetinaNet works? | ArcGIS API for Python | Esri Developer, https://developers.arcgis.com/python/latest/guide/how-retinanet-works/</li>
<li>FPN 논문(Feature Pyramid Networks for Object Detection) 리뷰 - 약초의 숲으로 놀러오세요, https://herbwood.tistory.com/18</li>
<li>Feature Pyramid Network[FPN] - DEVELOPER - 티스토리, https://ctkim.tistory.com/entry/Feature-Pyramid-NetworkFPN</li>
<li>[논문 읽기] Feature Pyramid Net, FPN(2017) 리뷰 - 딥러닝 공부방 - 티스토리, https://deep-learning-study.tistory.com/491</li>
<li>FPN (Feature Pyramid Network) 요약 - 컴퓨터와 수학, 몽상 조금 - 티스토리, https://skyil.tistory.com/207</li>
<li>Understanding Feature Pyramid Networks for object detection (FPN …, https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c</li>
<li>The Ultimate Guide to FPNs - Number Analytics, https://www.numberanalytics.com/blog/ultimate-guide-to-fpns</li>
<li>Feature Pyramid Networks for Object Detection, https://arxiv.org/abs/1612.03144</li>
<li>Feature Pyramid Network and its types | by Abhishek Jain - Medium, https://medium.com/@abhishekjainindore24/feature-pyramid-network-and-its-types-7c277f8bbe42</li>
<li>Feature Pyramid Networks (FPN). FPN was proposed in 2017 in a …, https://medium.com/@dpatidar687/feature-pyramid-networks-fpn-4cb1994f99d</li>
<li>Papers Explained 21: Feature Pyramid Network | by Ritvik Rastogi | DAIR.AI - Medium, https://medium.com/dair-ai/papers-explained-21-feature-pyramid-network-6baebcb7e4b8</li>
<li>갈아먹는 Object Detection [7] Feature Pyramid Network - 퍼스트펭귄 코딩스쿨, https://blog.firstpenguine.school/44</li>
<li>RetinaNet : Advanced Computer Vision - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2022/09/retinanet-advanced-computer-vision/</li>
<li>How we used RetinaNet for dense shape detection in live imagery - Mantra Labs, https://www.mantralabsglobal.com/blog/better-dense-shape-detection-in-live-imagery-with-retinanet/</li>
<li>Feature Pyramid Network Receptive Field - Stack Overflow, https://stackoverflow.com/questions/51786949/feature-pyramid-network-receptive-field</li>
<li>The intuition behind RetinaNet. The end goal of this blog post is to… | by Prakash Jay | Medium, https://medium.com/@14prakash/the-intuition-behind-retinanet-eb636755607d</li>
<li>matterport/Mask_RCNN: Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow - GitHub, https://github.com/matterport/Mask_RCNN</li>
<li>Ultimate Guide to Mask R-CNN: Architecture and Applications - Ikomia, https://www.ikomia.ai/blog/ultimate-guide-mask-rcnn-object-detection-segmentation</li>
<li>Mask-Refined R-CNN: A Network for Refining Object Details in Instance Segmentation, https://www.mdpi.com/1424-8220/20/4/1010</li>
<li>What is Mask R-CNN? | SKY ENGINE AI, https://www.skyengine.ai/blog/what-is-mask-r-cnn</li>
<li>Implementing Mask R-CNN: Advanced Object Detection and Segmentation - Voxel51, https://voxel51.com/blog/implementing-mask-r-cnn-advanced-object-detection-and-segmentation</li>
<li>FPN - Computer Vision Wiki - CloudFactory, https://wiki.cloudfactory.com/docs/mp-wiki/model-architectures/fpn</li>
<li>Feature Pyramid Network for Multi-Class Land … - CVF Open Access, https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w4/Seferbekov_Feature_Pyramid_Network_CVPR_2018_paper.pdf</li>
<li>A2-FPN for semantic segmentation of fine-resolution remotely sensed images, https://nora.nerc.ac.uk/id/eprint/532410/1/N532410JA.pdf</li>
<li>[1901.02446] Panoptic Feature Pyramid Networks - arXiv, https://arxiv.org/abs/1901.02446</li>
<li>Review: Semantic FPN, Panoptic FPN — Panoptic Feature Pyramid …, https://sh-tsang.medium.com/review-semantic-fpn-panoptic-fpn-panoptic-feature-pyramid-networks-8771f502bb99</li>
<li>Mastering Feature Pyramid Networks - Number Analytics, https://www.numberanalytics.com/blog/mastering-feature-pyramid-networks</li>
<li>[논문 읽기] EfficientDet(2020), Scalable and Efficient Object Detection, https://deep-learning-study.tistory.com/627</li>
<li>Week 8/9/10 - Object Detection - BiFPN , NASFPN, AugFPN - 미미로그, https://memesoo99.tistory.com/72</li>
<li>A Novel Pyramid Network with Feature Fusion and Disentanglement for Object Detection, https://pmc.ncbi.nlm.nih.gov/articles/PMC7987438/</li>
<li>[Object Detection] EfficientDet: Scalable and Efficient Object Detection - 시나브로_개발자 성장기 - 티스토리, https://developer-lionhong.tistory.com/175</li>
<li>HA-FPN: Hierarchical Attention Feature Pyramid Network for Object Detection - MDPI, https://www.mdpi.com/1424-8220/23/9/4508</li>
<li>[논문리뷰] Exploring Plain Vision Transformer Backbones for Object Detection - LCY, <a href="https://lcyking.tistory.com/entry/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Exploring-Plain-Vision-Transformer-Backbones-for-Object-Detection">https://lcyking.tistory.com/entry/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Exploring-Plain-Vision-Transformer-Backbones-for-Object-Detection</a></li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>