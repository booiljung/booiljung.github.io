<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:최대 풀링(Max Pooling) 계층</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>최대 풀링(Max Pooling) 계층</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">딥러닝 레이어 (Layers)</a> / <span>최대 풀링(Max Pooling) 계층</span></nav>
                </div>
            </header>
            <article>
                <h1>최대 풀링(Max Pooling) 계층</h1>
<h2>1.  서론 - 풀링의 개념과 필요성</h2>
<h3>1.1  합성곱 신경망(CNN)의 계층적 특징 추출 구조</h3>
<p>합성곱 신경망(Convolutional Neural Network, CNN)은 시각적 데이터 처리에 있어 탁월한 성능을 보이는 심층 신경망의 한 종류로, 그 구조는 여러 유형의 계층이 중첩된 형태를 띤다.1 기본적으로 CNN은 입력층(Input Layer), 합성곱 계층(Convolutional Layer), 풀링 계층(Pooling Layer), 그리고 완전 연결 계층(Fully Connected Layer)으로 구성된다.1 정보는 이 계층들을 순차적으로 통과하며 점차 저수준의 특징에서 고수준의 추상적인 특징으로 변환된다.</p>
<p>이 과정의 핵심은 합성곱 계층이다. 이 계층은 학습 가능한 필터(Filter) 또는 커널(Kernel)을 사용하여 입력 데이터(예: 이미지) 위를 이동(slide)하면서 합성곱 연산을 수행한다.1 이 연산을 통해 필터는 입력으로부터 엣지, 질감, 색상과 같은 지역적이고 기본적인 특징(local feature)을 감지하고, 그 결과를 특징 맵(Feature Map) 또는 활성화 맵(Activation Map)이라는 형태로 출력한다.3 각 필터는 서로 다른 종류의 특징을 감지하도록 학습되므로, 하나의 합성곱 계층은 다수의 특징 맵을 생성하여 입력에 대한 풍부한 표현을 구축한다.</p>
<h3>1.2  특징 맵의 공간적 차원과 정보 과부하 문제</h3>
<p>CNN 아키텍처가 깊어짐에 따라 합성곱 계층은 반복적으로 사용된다. 이 과정에서 특징 맵의 채널(channel) 수는 일반적으로 증가하여 더 복잡하고 다양한 특징을 표현하게 된다. 그러나 만약 특징 맵의 공간적 차원(너비와 높이)이 계속 유지되거나 아주 서서히 감소한다면, 네트워크는 심각한 문제에 직면하게 된다. 후속 계층으로 전달되는 데이터의 양이 기하급수적으로 증가하여 막대한 계산 비용과 메모리 사용량을 유발하기 때문이다.2</p>
<p>이러한 계산적 비효율성은 모델의 훈련 시간을 늘리고 더 많은 하드웨어 자원을 요구하는 직접적인 문제로 이어진다. 더 나아가, 이는 모델의 일반화 성능에도 악영향을 미칠 수 있다. 지나치게 큰 특징 맵은 모델이 훈련 데이터에 존재하는 미세하고 불필요한 세부 정보나 노이즈까지 학습하도록 유도할 수 있다. 이는 모델이 훈련 데이터에는 완벽하게 작동하지만, 이전에 보지 못한 새로운 데이터에 대해서는 성능이 저하되는 과적합(Overfitting) 현상을 초래하는 주요 원인이 된다.1</p>
<h3>1.3  풀링 계층의 역할과 목적</h3>
<p>풀링 계층은 바로 이러한 정보 과부하와 과적합 문제를 해결하기 위해 도입된 구조적 장치이다. 풀링은 합성곱 계층 다음에 주기적으로 삽입되어, 특징 맵의 공간적 차원을 의도적으로 축소하는 비선형 서브샘플링(subsampling) 또는 다운샘플링(downsampling) 연산을 수행한다.4 이 과정은 학습 가능한 파라미터 없이, 사전에 정의된 간단한 규칙에 따라 이루어진다.10 풀링 계층의 핵심 목적은 다음과 같이 세 가지로 요약할 수 있다.</p>
<ol>
<li><strong>계산 효율성 증대</strong>: 특징 맵의 너비와 높이를 줄임으로써 후속 계층에서 처리해야 할 파라미터의 수와 연산량을 극적으로 감소시킨다. 이는 모델의 훈련 속도를 높이고 메모리 요구량을 줄여 더 깊고 복잡한 네트워크를 설계할 수 있게 한다.2</li>
<li><strong>과적합 방지</strong>: 공간적 정밀도를 낮추고 특징 맵의 정보를 요약함으로써, 모델이 훈련 데이터의 사소한 변동이나 노이즈에 과도하게 민감해지는 것을 방지한다. 이는 일종의 정규화(regularization) 효과를 제공하여 모델의 일반화 성능을 향상시킨다.1</li>
<li><strong>특징의 불변성(Invariance) 획득</strong>: 풀링은 특징의 정확한 위치보다는 그 존재 여부에 더 집중하도록 만든다. 이로 인해 입력 데이터에 작은 이동(translation), 회전(rotation), 크기 변화(scale) 등이 발생하더라도 모델이 강인한(robust) 예측을 할 수 있도록 돕는다.5</li>
</ol>
<p>표면적으로 풀링은 데이터의 크기를 줄이는 단순한 압축 과정처럼 보일 수 있다. 그러나 그 본질은 ’전략적 정보 폐기’에 가깝다. 예를 들어, 가장 널리 사용되는 최대 풀링(Max Pooling)은 특정 영역에서 가장 강한 활성화 신호 하나만을 남기고 나머지 정보는 의도적으로 모두 버린다.3 이는 때로는 전체 정보의 75%를 폐기하는 과감한 결정이다.8 이처럼 풀링 과정에서 발생하는 정보 손실은 오류나 부작용이 아니라, 불필요한 세부 정보를 제거하고 가장 핵심적인 특징만을 추출하여 표현을 더욱 추상적이고 강인하게 만들기 위한 핵심적인 기능이다.</p>
<h2>2.  최대 풀링의 작동 원리 및 수학적 기초</h2>
<h3>2.1  최대 풀링 연산의 정의 및 메커니즘</h3>
<p>최대 풀링(Max Pooling)은 풀링 연산의 가장 대표적인 형태로, 그 이름에서 알 수 있듯이 ’최대값’을 선택하는 방식으로 동작한다. 구체적으로, 최대 풀링은 입력 특징 맵을 특정 크기의 윈도우(window) 또는 필터(filter)로 순회하며, 각 윈도우가 덮는 영역 내에서 가장 큰 활성화 값을 추출하여 새로운, 축소된 특징 맵을 생성하는 연산으로 정의된다.3</p>
<p>이 과정은 입력 특징 맵이 여러 개의 채널로 구성되어 있을 경우, 각 채널에 대해 독립적으로 수행된다. 즉, 한 채널의 풀링 연산은 다른 채널에 영향을 주지 않는다. 결과적으로 최대 풀링 연산을 거친 후에도 특징 맵의 채널 수는 변하지 않는다.10 연산 메커니즘은 다음과 같은 단계로 이루어진다.</p>
<ol>
<li><strong>윈도우 정의</strong>: 연산을 수행할 윈도우의 크기와 이동 간격(스트라이드)을 정의한다.</li>
<li><strong>윈도우 슬라이딩</strong>: 정의된 윈도우를 입력 특징 맵의 좌측 상단부터 시작하여 스트라이드만큼 이동시키며 전체 영역을 순회한다.</li>
<li><strong>최대값 추출</strong>: 각 윈도우 위치에서 윈도우 내에 포함된 모든 픽셀 값 중 가장 큰 값을 찾는다.</li>
<li><strong>출력 맵 생성</strong>: 추출된 최대값을 출력 특징 맵의 해당 위치에 기록한다. 이 과정을 모든 윈도우 위치에 대해 반복하여 최종 출력 맵을 완성한다.</li>
</ol>
<h3>2.2  핵심 하이퍼파라미터</h3>
<p>최대 풀링 연산의 동작 방식은 학습을 통해 결정되는 것이 아니라, 사용자가 사전에 정의하는 몇 가지 하이퍼파라미터(hyperparameter)에 의해 결정된다.</p>
<ul>
<li>
<p><strong>풀링 윈도우 크기 (Pooling Window Size, <code>f</code> 또는 <code>P</code>)</strong>: 최대값을 추출할 국소 영역의 크기를 의미한다. 일반적으로 너비와 높이가 같은 정사각형 형태인 <code>2x2</code> 또는 <code>3x3</code> 크기가 널리 사용된다.3 윈도우 크기가 클수록 더 공격적인 다운샘플링이 일어나며 공간적 정보 손실이 커질 수 있지만, 계산량 감소 효과는 더 크다.7</p>
</li>
<li>
<p><strong>스트라이드 (Stride, <code>s</code> 또는 <code>S</code>)</strong>: 윈도우가 한 번의 연산 후 다음 위치로 이동하는 픽셀의 수를 결정한다. 예를 들어 스트라이드가 2이면 윈도우는 두 픽셀씩 건너뛰며 이동한다.3 일반적으로 윈도우 크기와 동일한 값(예:</p>
</li>
</ul>
<p><code>2x2</code> 윈도우에 스트라이드 2)을 설정하여 윈도우가 서로 겹치지 않도록(non-overlapping) 하는 경우가 많다. 스트라이드가 클수록 출력 특징 맵의 크기는 더 작아진다.1</p>
<ul>
<li><strong>패딩 (Padding)</strong>: 입력 특징 맵의 가장자리에 특정 값(주로 0)을 추가하여 입력의 크기를 조절하는 기법이다. 패딩을 사용하면 풀링 연산 후 출력 크기를 원하는 대로 제어하거나, 입력의 경계 부분에 있는 정보가 연산에서 소외되지 않도록 보존하는 데 도움이 될 수 있다.17</li>
</ul>
<h3>2.3  수학적 공식화</h3>
<p>최대 풀링 연산과 그로 인한 출력 차원의 변화는 수학적으로 명확하게 공식화할 수 있다.</p>
<ul>
<li>최대 풀링 연산:</li>
</ul>
<p>입력 특징 맵을 X, 출력 특징 맵을 Y라 할 때, X의 k번째 채널에 대한 Y의 (i,j) 위치의 값은 다음과 같이 정의된다. 이 수식은 출력 위치 <span class="math math-inline">(i, j)</span>에 해당하는 입력 맵 상의 풀링 영역 Ri,j 내에서 최대값을 찾는 과정을 나타낸다.14<br />
<span class="math math-display">
  Y_{i,j,k} = \max_{(m,n) \in R_{i,j}} X_{m,n,k}
</span></p>
<ul>
<li>출력 특징 맵의 차원 계산:</li>
</ul>
<p>입력 특징 맵의 높이와 너비를 각각 Hin, <span class="math math-inline">W_{in}</span>이라 하고, 풀링 윈도우의 크기를 f×f, 스트라이드를 s, 양쪽에 적용된 패딩의 크기를 p라고 할 때, 출력 특징 맵의 높이 <span class="math math-inline">H_{out}</span>과 너비 <span class="math math-inline">W_{out}</span>은 다음 공식으로 계산된다. 이 공식은 CNN 아키텍처 설계 시 각 계층을 통과하면서 특징 맵의 크기가 어떻게 변하는지 예측하는 데 필수적이다.14<br />
<span class="math math-display">
  H_{out} = \lfloor \frac{H_{in} + 2p - f}{s} \rfloor + 1
</span></p>
<p><span class="math math-display">
  W_{out} = \lfloor \frac{W_{in} + 2p - f}{s} \rfloor + 1
</span></p>
<p>여기서 ⌊⋅⌋는 바닥 함수(floor function)로, 소수점 이하를 버리는 연산을 의미한다.</p>
<h3>2.4  수치 예제를 통한 연산 과정의 시각화</h3>
<p>추상적인 개념을 구체적으로 이해하기 위해, 간단한 4×4 크기의 입력 특징 맵에 <code>2x2</code> 윈도우와 스트라이드 2를 적용하는 최대 풀링 과정을 단계별로 살펴보자.3</p>
<ul>
<li>
<p><strong>입력 특징 맵</strong>:</p>
<pre><code> 9  12   8   4
18   7  10   6
 3   5  15   1
 2   1  11  16
</code></pre>
</li>
</ul>
<pre><code>
- **연산 과정**:

1. **첫 번째 윈도우 (좌상단)**: 2×2 영역 `{9, 12, 18, 7}`에서 최대값은 `18`이다.
2. **두 번째 윈도우 (우상단)**: 스트라이드 2만큼 우측으로 이동한 2×2 영역 `{8, 4, 10, 6}`에서 최대값은 `10`이다.
3. **세 번째 윈도우 (좌하단)**: 다시 좌측 끝으로 돌아와 스트라이드 2만큼 아래로 이동한 2×2 영역 `{3, 5, 2, 1}`에서 최대값은 `5`이다.
4. **네 번째 윈도우 (우하단)**: 스트라이드 2만큼 우측으로 이동한 2×2 영역 `{15, 1, 11, 16}`에서 최대값은 `16`이다.

- **최종 출력 특징 맵**:

</code></pre>
<p>18  10<br />
5  16</p>
<pre><code>
이 예제를 통해 4×4 입력이 최대 풀링을 거쳐 2×2 출력으로 축소되는 과정을 명확히 확인할 수 있다.

중요한 점은 최대 풀링 계층이 합성곱 계층과 달리 학습 가능한 파라미터를 전혀 가지고 있지 않다는 것이다.10 그 역할은 데이터로부터 특정 지식을 '학습'하는 것이 아니라, 하이퍼파라미터에 의해 정의된 고정된 규칙에 따라 특징 맵에 '구조적 변환'을 가하는 것이다. 따라서 최대 풀링은 지식 습득이 아닌, 이미 추출된 특징을 정제하고 요약하여 더 유용하고 다루기 쉬운 형태로 만드는 '구조적 연산자'로 이해해야 한다. 역전파 과정에서는 각 윈도우에서 선택된 최대값의 위치로만 그래디언트가 전달되며, 나머지 위치로는 그래디언트가 흐르지 않는다.20

## 3.  최대 풀링의 핵심 기능과 이론적 효과


최대 풀링은 단순히 특징 맵의 크기를 줄이는 것을 넘어, CNN 모델의 성능과 특성에 중대한 영향을 미치는 두 가지 핵심적인 이론적 효과를 가진다: 바로 공간적 불변성 확보와 과적합 억제다.

### 3.1  공간적 불변성(Spatial Invariance) 확보


공간적 불변성이란 입력 데이터 내에서 객체나 특징의 위치가 조금 변하더라도, 모델의 인식 결과는 크게 변하지 않는 성질을 의미한다. 최대 풀링은 이러한 불변성을 근사적으로 획득하는 데 중요한 역할을 한다.

- **이동 불변성(Translation Invariance)의 원리**: 최대 풀링의 핵심은 특징의 정확한 픽셀 단위 위치(pixel-perfect location)가 아니라, 특정 풀링 영역 내에서의 '존재' 자체에 집중한다는 점이다. 예를 들어, `2x2` 풀링 윈도우 내에서 어떤 강한 특징(높은 활성화 값)이 한 픽셀 옆으로 이동하더라도, 그 값이 여전히 해당 윈도우 내에서 최대값이라면 풀링의 결과는 전혀 변하지 않는다.8 이처럼 최대 풀링은 출력 값이 입력의 작은 이동에 둔감해지도록 만들어, 모델이 객체의 본질적인 패턴을 위치 변화에 상관없이 학습하도록 돕는다.15
- **강인성(Robustness) 획득 과정**: 입력 이미지가 1픽셀만큼 이동했다고 가정해보자. 합성곱 계층의 출력인 특징 맵의 모든 값은 변하게 된다. 그러나 이 특징 맵에 최대 풀링을 적용하면, 많은 윈도우에서 최대값의 위치만 바뀔 뿐 값 자체는 변하지 않을 수 있다. 결과적으로 최대 풀링을 거친 출력 맵의 값들 중 상당수는 변하지 않게 되어, 전체 네트워크가 입력의 미세한 변화에 대해 더 강인한 반응을 보이게 된다.1 이는 모델이 객체의 세부적인 위치보다는 '이 영역에 눈이 있다' 또는 '저 영역에 바퀴가 있다'와 같이 존재 여부에 기반한 추상적인 학습을 하도록 유도한다.
- **불변성의 한계**: 그러나 최대 풀링이 제공하는 불변성은 완벽하지 않으며, '지역적(local)'이고 '근사적(approximate)'이라는 한계를 가진다. 불변성은 풀링 윈도우의 크기 내에서 발생하는 작은 이동에 대해서만 효과적이다. 특징이 풀링 윈도우의 경계를 넘어 다른 윈도우로 이동할 만큼 크게 변위된다면, 풀링의 출력은 완전히 달라질 수 있다.9 따라서 최대 풀링은 완벽한 불변성을 보장하는 것이 아니라, 지역적인 수준에서 불변성을 점진적으로 구축해나가는 역할을 한다.

### 3.2  과적합(Overfitting) 억제를 통한 일반화 성능 향상


과적합은 모델이 훈련 데이터에 지나치게 최적화되어 새로운 데이터에 대한 예측 성능이 떨어지는 현상이다. 최대 풀링은 여러 메커니즘을 통해 과적합을 억제하고 모델의 일반화 능력을 향상시킨다.

- **차원 축소와 모델 복잡도 감소**: 가장 직접적인 기여는 공간적 차원 축소를 통한 모델의 복잡도 감소다. 예를 들어, `2x2` 윈도우와 스트라이드 2를 사용하는 최대 풀링은 특징 맵의 크기를 1/4로 줄인다. 이는 후속 계층, 특히 파라미터 수가 많은 완전 연결 계층으로 전달되는 입력의 크기를 크게 줄여, 전체 네트워크가 학습해야 할 파라미터의 수를 감소시킨다. 파라미터 수가 적을수록 모델이 훈련 데이터의 노이즈까지 암기할 가능성이 줄어든다.1
- **정규화(Regularization) 효과**: 최대 풀링은 본질적으로 가장 강한 신호만을 선택하고 나머지 약한 신호들은 무시하는 '정보 병목(information bottleneck)'을 생성한다. 이 과정은 모델이 생존에 필수적인, 즉 가장 핵심적이고 일반화 가능한 특징에만 집중하도록 강제하는 일종의 정규화 기법으로 작용한다.7 덜 중요한 활성화 값들을 버림으로써, 모델은 사소한 변동에 덜 민감해지고 데이터의 근본적인 구조를 학습하는 데 더 집중하게 된다.
- **노이즈 억제**: 특징 맵에 존재하는 약한 활성화 값들은 종종 유의미한 정보가 아닌 노이즈일 수 있다. 최대 풀링은 이러한 값들을 효과적으로 필터링하여 제거한다. 윈도우 내에서 가장 두드러진 특징만을 다음 계층으로 전달함으로써, 노이즈가 네트워크를 통해 전파되는 것을 억제하고 모델의 예측을 더 안정적으로 만드는 효과가 있다.3

이러한 불변성 획득과 정보 손실은 동전의 양면과 같은 관계에 있다. 최대 풀링은 이동 불변성을 제공하여 분류(Classification)와 같은 작업에서 모델의 성능을 향상시키는 데 크게 기여한다.8 그러나 이 과정에서 필연적으로 발생하는 공간적 정보의 손실은 특징의 '정확한 위치'가 중요한 작업에서는 문제가 될 수 있다. 예를 들어, 이미지 내에서 객체의 정확한 경계를 찾는 객체 탐지(Object Detection)나 이미지 분할(Segmentation) 같은 회귀(Regression) 문제에서는 최대 풀링으로 인한 위치 정보의 손실이 성능 저하의 원인이 될 수 있다.22 위치 정보가 완전히 소실되는 것은 아니며, 가장 강한 활성화의 위치가 다운샘플링된 좌표계로 변환되어 '대략적인 위치' 정보는 보존된다.22 하지만 미세한 공간적 관계나 세밀한 텍스처 정보는 손실될 수밖에 없다.7 이 때문에 현대의 객체 탐지 모델들은 최대 풀링을 매우 신중하게 사용하거나, 관심 영역 풀링(Region of Interest Pooling, RoI Pooling)과 같이 위치 정보를 더 잘 보존하도록 설계된 변형된 풀링 기법을 도입하기도 한다.24

## 4.  주요 풀링 기법과의 비교 분석


최대 풀링은 가장 널리 알려진 풀링 기법이지만, 유일한 선택지는 아니다. 문제의 특성과 목표에 따라 다른 풀링 기법들이 더 적합할 수 있다. 여기서는 최대 풀링을 평균 풀링, 전역 풀링과 비교하여 각각의 특징과 장단점을 분석한다.

### 4.1  평균 풀링(Average Pooling)과의 비교


평균 풀링은 최대 풀링과 함께 가장 기본적인 풀링 연산 중 하나로, 연산 방식에서 근본적인 차이를 보인다.

- **연산 방식의 차이**: 최대 풀링이 풀링 윈도우 내에서 가장 큰 값을 선택하는 반면 7, 평균 풀링은 윈도우 내에 있는 모든 활성화 값의 산술 평균을 계산하여 출력으로 사용한다.7
- **특징 표현의 차이**:
- **최대 풀링(Max Pooling)**: 가장 두드러진(prominent) 특징, 즉 가장 강한 신호를 보존하는 데 집중한다. 이는 이미지의 엣지, 코너, 밝은 점과 같이 날카로운 특징을 강조하는 경향이 있다. 이러한 특성 덕분에 대부분의 이미지 인식 과제에서 평균 풀링보다 더 나은 성능을 보이는 경우가 많다.7
- **평균 풀링(Average Pooling)**: 윈도우 내의 모든 특징을 동등하게 고려하여 종합한다. 이로 인해 특징 맵이 전반적으로 부드러워지는(smoothing) 효과가 있으며, 보다 일반화된 표현을 생성한다. 전체적인 컨텍스트나 배경 정보를 보존하는 것이 중요할 때 유용할 수 있지만, 반대로 가장 중요한 핵심 특징을 희석시켜 약화시킬 위험도 존재한다.7
- **사용 사례**: 예를 들어, MNIST 손글씨 숫자 데이터셋처럼 배경이 검은색이고 객체(숫자)가 흰색으로 표현된 경우, 가장 밝은 픽셀, 즉 최대값을 선택하는 최대 풀링이 직관적으로 더 유리하다.25 반면, 평균 풀링은 특징의 전반적인 분포가 중요하거나, 노이즈가 많은 환경에서 특징을 부드럽게 만들어 안정성을 높이고자 할 때 고려될 수 있다.26

### 4.2  전역 풀링(Global Pooling)과의 비교


전역 풀링은 지역적인 윈도우를 사용하는 대신, 특징 맵 전체를 하나의 풀링 영역으로 간주하는 방식이다.

- **정의**: 전역 풀링은 각 채널별로 특징 맵 전체에 대해 단일 연산을 적용하여, 채널 수만큼의 스칼라 값을 생성한다. 즉, H×W×C 크기의 특징 맵을 1×1×C 크기의 벡터로 변환한다.2
- **전역 최대 풀링 (Global Max Pooling, GMP)**: 각 채널의 특징 맵 전체에서 가장 큰 활성화 값 하나를 선택한다.24
- **전역 평균 풀링 (Global Average Pooling, GAP)**: 각 채널의 특징 맵 전체 값들의 평균을 계산한다. ResNet, GoogLeNet 등 현대적인 CNN 아키텍처에서 널리 채택되어 표준적인 기법으로 자리 잡았다.6
- **역할 및 장점**: 전역 풀링은 주로 CNN 아키텍처의 마지막 부분, 즉 합성곱 계층들과 분류기(classifier)를 연결하는 단계에서 사용된다. 전통적인 아키텍처(예: AlexNet, VGG)에서는 이 부분에 파라미터 수가 매우 많은 완전 연결 계층(Fully-Connected Layer)을 사용했지만, 전역 풀링은 이를 대체하여 다음과 같은 중요한 장점을 제공한다.28
1. **과적합 방지**: 완전 연결 계층에 필요한 막대한 수의 파라미터를 제거함으로써 모델의 복잡도를 크게 낮추고 과적합을 효과적으로 방지한다.28
2. **입력 크기 유연성**: 완전 연결 계층은 고정된 크기의 입력을 요구하지만, 전역 풀링을 사용하면 입력 이미지의 크기에 제약이 없는 완전 합성곱 네트워크(Fully Convolutional Network) 설계를 용이하게 한다.6
3. **해석 가능성 향상**: 각 특징 맵이 최종 분류에 기여하는 정도를 더 직접적으로 연결해주어, 어떤 특징이 특정 클래스를 예측하는 데 중요한 역할을 하는지 시각화하는 Class Activation Mapping(CAM)과 같은 기법의 기반이 된다.

### 4.3  비교 분석 요약 테이블


다음 표는 세 가지 주요 풀링 기법의 핵심적인 특징을 요약하여 비교한 것이다. 이러한 비교는 특정 문제에 가장 적합한 풀링 전략을 선택하는 데 있어 구조적인 의사결정을 돕는다.

| **기준**           | **최대 풀링 (Max Pooling)**                                 | **평균 풀링 (Average Pooling)**                              | **전역 풀링 (Global Pooling)**                             |
| ------------------ | ----------------------------------------------------------- | ------------------------------------------------------------ | ---------------------------------------------------------- |
| **연산 방식**      | 풀링 영역 내 **최대값** 선택                                | 풀링 영역 내 모든 값의 **평균** 계산                         | 특징 맵 **전체**에 대한 단일 값(최대 또는 평균) 계산       |
| **정보 보존**      | 가장 강한 신호(salient feature) 보존, 나머지 정보는 손실 17 | 영역 내 모든 정보를 종합하나, 특징이 희석될 수 있음 7        | 공간 정보를 완전히 제거하고 채널별 통계 정보만 남김 17     |
| **주요 장점**      | 강한 특징 강조, 이동 불변성 제공, 우수한 성능 7             | 전반적인 특징 분포 반영, 부드러운 특징 표현 7                | 파라미터 수 대폭 감소, 과적합 방지, 완전 연결 계층 대체 28 |
| **주요 단점**      | 정보 손실이 큼, 노이즈에 민감할 수 있음 7                   | 강한 특징을 약화시킬 수 있음 13                              | 공간 정보의 완전한 손실 30                                 |
| **주요 사용 사례** | 대부분의 CNN에서 표준 다운샘플링 기법으로 사용 1            | 특징의 전반적 분포가 중요하거나, 부드러운 표현이 필요할 때 7 | CNN의 분류기(classifier) 부분에서 완전 연결 계층을 대체 6  |

## 5.  최대 풀링의 한계와 진보된 풀링 기법들


최대 풀링은 단순함과 효과성으로 인해 CNN의 표준 구성 요소로 자리 잡았지만, 그 작동 방식에는 근본적인 한계가 존재한다. 이러한 한계를 극복하기 위해 다양한 진보된 풀링 기법들이 제안되었다.

### 5.1  최대 풀링의 내재적 한계: 가혹한 정보 손실


최대 풀링의 가장 큰 한계는 "승자독식(winner-takes-all)" 방식으로 인한 가혹한 정보 손실이다. 풀링 윈도우 내에서 가장 강한 활성화 값 단 하나만을 선택하고 나머지 모든 값들을 완전히 버리기 때문에, 그 영역 내에 존재할 수 있는 유용한 부가 정보나 미세한 텍스처 패턴들이 소실된다.2 예를 들어, 한 윈도우 내에 두 번째로 강한 활성화 값이 존재하더라도, 최대 풀링은 이를 전혀 고려하지 않는다. 이러한 정보 손실은 특히 객체의 세부적인 구분이 중요하거나 복잡한 배경 속에서 여러 특징을 동시에 고려해야 하는 과제에서 모델의 성능을 저해하는 요인이 될 수 있다.

### 5.2  확률적 풀링 (Stochastic Pooling)


확률적 풀링은 최대 풀링의 결정론적인 선택 방식을 확률적인 샘플링으로 대체하여 정보 손실 문제를 완화하고자 하는 시도이다.

- **개념**: 풀링 윈도우 내에서 항상 최대값을 선택하는 대신, 각 활성화 값의 크기에 비례하는 확률로 하나의 활성화를 무작위로 샘플링한다. 즉, 활성화 값이 클수록 선택될 확률이 높지만, 작은 값을 가진 활성화도 선택될 기회를 가진다.31

- **수학적 표현**: 풀링 영역 Rj 내의 각 활성화 값 ai에 대해, 해당 활성화가 선택될 확률 pi는 영역 내 모든 활성화 값의 합으로 정규화하여 계산된다.
  $$
  p_i = \frac{a_i}{\sum_{k \in R_j} a_k}
$$
이후, 이렇게 계산된 다항 분포(multinomial distribution) p에 따라 하나의 활성화 값을 샘플링하여 출력 sj로 사용한다.32

- **효과**:

1. **강력한 정규화**: 훈련 과정에서 동일한 입력에 대해서도 매번 다른 활성화가 샘플링될 수 있으므로, 모델이 특정 특징에 과도하게 의존하는 것을 방지한다. 이는 드롭아웃(Dropout)과 유사한 정규화 효과를 제공하여 일반화 성능을 향상시킨다.31
2. **정보 보존과 특징 탐색의 균형**: 최대 풀링(가장 강한 특징만 보존)과 평균 풀링(모든 특징을 희석) 사이의 효과적인 절충안을 제공한다. 약한 신호도 선택될 가능성을 열어둠으로써, 최대 풀링이 버릴 수 있는 유용한 정보를 보존할 기회를 만든다.31

### 5.3  분수 최대 풀링 (Fractional Max-Pooling, FMP)


분수 최대 풀링은 다운샘플링 비율의 경직성을 해결하려는 독창적인 접근법이다.

- **개념**: 기존의 풀링이 공간적 차원을 2배, 3배와 같은 정수 배율로 축소하는 것과 달리, FMP는 1&lt;α&lt;2 사이의 비정수(non-integer), 즉 분수 배율로 차원을 축소할 수 있게 한다.34
- **작동 방식**: FMP는 고정된 격자(grid)를 사용하는 대신, 목표 축소 비율 α에 맞춰 풀링 영역의 크기와 위치를 무작위적(random) 또는 준-무작위적(pseudo-random) 방식으로 생성한다. 이로 인해 훈련 시 매번 다른 다운샘플링 격자가 적용되며, 이는 풀링 과정 자체에 확률적 요소를 도입한다. 여기서 무작위성은 풀링 영역 내에서의 선택이 아닌, 풀링 영역 자체의 생성 방식에 있다는 점에서 확률적 풀링과 구별된다.34
- **효과**:
1. **점진적이고 유연한 차원 축소**: 기존의 `2x2` 풀링보다 더 완만하게 특징 맵의 크기를 줄일 수 있다. 이는 네트워크의 깊이를 더 세밀하게 조절하고 더 많은 풀링 계층을 쌓을 수 있게 하여, 다양한 스케일의 특징을 학습할 기회를 늘려준다.35
2. **강력한 정규화**: 풀링 영역의 무작위성은 일종의 데이터 증강(data augmentation)과 유사한 효과를 내어 과적합을 매우 효과적으로 억제한다. FMP를 제안한 논문에서는 드롭아웃과 같은 다른 정규화 기법 없이도 CIFAR-100 데이터셋에서 당시 최고 수준(state-of-the-art)의 성능을 달성했다고 보고했다.34

### 5.4  기타 변형 풀링 기법


- **Lp 풀링**: 최대 풀링(p→∞)과 평균 풀링(p=1)을 Lp 노름(Lp norm)이라는 단일 프레임워크로 일반화한 기법이다. p 값을 조절하여 두 풀링 방식 사이의 특성을 조절할 수 있다.5
- **혼합 풀링 (Mixed Pooling)**: 최대 풀링과 평균 풀링의 결과를 가중합하여 사용하는 방식이다. 이 가중치는 고정된 하이퍼파라미터일 수도 있고, 학습을 통해 데이터에 맞게 최적화될 수도 있다.24

이러한 진보된 풀링 기법들은 최대 풀링의 두 가지 근본적인 한계, 즉 '결정론'과 '경직성'을 극복하려는 시도로 볼 수 있다. 확률적 풀링은 항상 최대값만 선택하는 '결정론'을 확률적 샘플링으로 대체하여 유연성을 부여한다. 분수 최대 풀링은 `2x2`와 같은 고정된 격자에 따라 다운샘플링하는 '경직성'을 무작위적인 격자 생성으로 대체한다. 두 기법 모두 '무작위성(randomness)' 또는 '확률(stochasticity)'이라는 도구를 사용하여 기존의 고정된 규칙을 완화하고, 풀링 연산에 더 많은 다양성을 부여함으로써 모델이 더 강인하고 일반화된 특징을 학습하도록 유도한다. 이는 현대 딥러닝 아키텍처 설계의 중요한 연구 방향을 제시한다.

## 6.  결론 - 최대 풀링의 현대적 의의와 미래


### 6.1  최대 풀링의 핵심 기여 요약


최대 풀링은 합성곱 신경망(CNN)의 역사에서 빼놓을 수 없는 핵심적인 구성 요소이다. 그 기여는 단순히 기술적인 측면을 넘어, 딥러닝 기반 컴퓨터 비전 분야의 발전을 가능하게 한 근간으로 평가받아야 한다. 첫째, 최대 풀링은 심층 네트워크의 계산적 타당성(computational feasibility)을 확보했다. 특징 맵의 차원을 효과적으로 줄여, 제한된 컴퓨팅 자원 환경에서도 깊은 네트워크의 훈련을 가능하게 했다. 둘째, 과적합을 억제하는 강력한 정규화 장치로서 기능했다. 정보 병목을 통해 모델이 가장 본질적인 특징에 집중하도록 유도하여 일반화 성능을 크게 향상시켰다. 셋째, 기본적인 공간적 불변성을 제공함으로써 모델이 입력 데이터의 작은 변화에 강인하게 반응하도록 만들었다. 이러한 단순함과 강력한 효과성 덕분에 최대 풀링은 오늘날에도 여전히 수많은 CNN 아키텍처에서 기본적인 다운샘플링 도구로 널리 활용되고 있다.

### 6.2  최신 CNN 아키텍처에서의 역할 변화


그러나 딥러닝 기술이 발전함에 따라 최대 풀링의 역할과 위상에도 변화가 나타나고 있다. 특히 두 가지 경향이 두드러진다.

- **스트라이드 합성곱(Strided Convolutions)의 부상**: ResNet과 같은 현대적인 아키텍처에서는 별도의 풀링 계층을 사용하는 대신, 스트라이드(stride)가 2 이상인 합성곱 계층을 사용하여 다운샘플링과 특징 추출을 동시에 수행하는 방식이 선호되고 있다.26 이 접근법은 다운샘플링 과정을 학습 가능한 파라미터(필터 가중치)를 통해 수행하므로, 모델이 데이터로부터 정보 손실을 최소화하는 최적의 다운샘플링 방식을 스스로 학습할 수 있다는 장점이 있다. 이는 최대 풀링의 비학습적이고 고정된 규칙으로 인한 정보 손실을 줄이려는 시도로 해석할 수 있다.
- **전역 평균 풀링(Global Average Pooling)의 표준화**: 네트워크의 후반부, 즉 분류기로 넘어가기 직전 단계에서는 전통적인 완전 연결 계층 대신 전역 평균 풀링(GAP)을 사용하는 것이 거의 표준으로 자리 잡았다.6 GAP는 파라미터 수를 극적으로 줄여 과적합을 방지하고, 입력 이미지 크기에 유연한 모델을 설계할 수 있게 하는 등 여러 장점을 제공하여 현대 CNN 아키텍처의 효율성과 성능을 한 단계 끌어올렸다.

### 6.3  풀링 기법의 발전 방향과 미래


최대 풀링에서 시작된 풀링의 개념은 이제 단순한 다운샘플링을 넘어 더욱 지능적이고 유연한 방향으로 진화하고 있다. 미래의 풀링 연구는 다음과 같은 방향으로 나아갈 것으로 전망된다.

- **적응형 풀링(Adaptive Pooling)**: 입력 데이터의 특성이나 컨텍스트에 따라 풀링의 방식이나 강도를 동적으로 조절하는 기법이다. 예를 들어, 중요한 정보가 밀집된 영역에서는 약한 풀링을, 배경과 같이 정보가 적은 영역에서는 강한 풀링을 적용하는 방식이다.
- **학습 가능한 풀링(Learnable Pooling)**: 풀링 연산 자체를 학습의 대상으로 삼는 접근법이다. 최대 풀링이나 평균 풀링과 같은 고정된 규칙 대신, 어떤 정보를 보존하고 어떤 정보를 버릴지를 네트워크가 역전파를 통해 직접 학습하도록 설계하는 것이다.

결론적으로, 최대 풀링은 CNN 발전의 초석을 다진 중요한 기술이며, 그 핵심 아이디어인 '가장 중요한 특징을 선택하여 표현을 요약한다'는 개념은 여전히 유효하다. 비록 최신 아키텍처에서는 그 역할이 일부 대체되거나 변형되고 있지만, 정보 압축과 불변성 획득이라는 풀링의 근본적인 필요성은 사라지지 않을 것이다. 따라서 최대 풀링의 원리를 이해하는 것은 현재와 미래의 딥러닝 아키텍처를 깊이 있게 파악하는 데 필수적이며, 그 개념은 앞으로도 다양한 형태로 진화하며 딥러닝 기술의 중요한 한 축으로 남을 것이다.

## 7. 참고 자료


1. Introduction to Convolution Neural Network - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/introduction-convolution-neural-network/
2. A Comparison of Pooling Methods for Convolutional Neural Networks - MDPI, https://www.mdpi.com/2076-3417/12/17/8643
3. What is a max pooling layer in CNN? - Educative.io, https://www.educative.io/answers/what-is-a-max-pooling-layer-in-cnn
4. Max Pooling in Convolutional Neural Networks explained - deeplizard, https://deeplizard.com/learn/video/ZjM_XQa5s6s
5. Pooling Layers in CNN - Giskard, https://www.giskard.ai/glossary/pooling-layers-in-cnn
6. How much pooling should you do in a CNN? : r/learnmachinelearning - Reddit, https://www.reddit.com/r/learnmachinelearning/comments/t3ja2k/how_much_pooling_should_you_do_in_a_cnn/
7. CNN | Introduction to Pooling Layer - GeeksforGeeks, https://www.geeksforgeeks.org/deep-learning/cnn-introduction-to-pooling-layer/
8. Convolutional Neural Networks (CNN): Step 2 - Max Pooling - SuperDataScience, https://www.superdatascience.com/blogs/convolutional-neural-networks-cnn-step-2-max-pooling
9. 9.3 Pooling - CEDAR, [http://www.cedar.buffalo.edu/~srihari/CSE676/9.3%20Pooling.pdf](http://www.cedar.buffalo.edu/~srihari/CSE676/9.3%20Pooling.pdf)
10. Max Pooling in Convolutional Neural Network - YouTube, https://www.youtube.com/watch?v=zg_AA3fZpE0
11. 7.5. Pooling — Dive into Deep Learning 1.0.3 documentation, https://d2l.ai/chapter_convolutional-neural-networks/pooling.html
12. www.sapien.io, [https://www.sapien.io/glossary/definition/pooling#:~:text=Max%20pooling%20is%20one%20of,making%20the%20network%20more%20efficient.](https://www.sapien.io/glossary/definition/pooling#:~:text=Max%20pooling%20is%20one%20of,making%20the%20network%20more%20efficient.)
13. Advantages and disadvantages of different pooling approach in CNN. - ResearchGate, https://www.researchgate.net/figure/Advantages-and-disadvantages-of-different-pooling-approach-in-CNN_tbl1_363090585
14. MAX POOLING. The pooling operation involves sliding… | by DhanushKumar - Medium, https://medium.com/@danushidk507/max-pooling-ef545993b6e4
15. Max Pooling: A Fundamental Concept in Deep Learning, https://www.alooba.com/skills/concepts/deep-learning/max-pooling/
16. Neural Networks: Pooling Layers | Baeldung on Computer Science, https://www.baeldung.com/cs/neural-networks-pooling-layers
17. Max Pooling: A Deep Dive - Number Analytics, https://www.numberanalytics.com/blog/max-pooling-deep-dive
18. How to calculate the output size after convolving and pooling to the input image, https://stackoverflow.com/questions/44193270/how-to-calculate-the-output-size-after-convolving-and-pooling-to-the-input-image
19. Convolution and Max Pooling - Colby Computer Science, https://cs.colby.edu/courses/F19/cs343/lectures/lecture11/Lecture11Slides.pdf
20. Questions_ About Max Pooling - DeepLearning.AI, https://community.deeplearning.ai/t/questions-about-max-pooling/589625
21. Max Pooling, Why use it and its advantages. | by Prashant Dixit | Geek Culture - Medium, https://medium.com/geekculture/max-pooling-why-use-it-and-its-advantages-5807a0190459
22. Why don't max pooling layers break CNNs performance in solving regression problems?, https://stackoverflow.com/questions/51942071/why-dont-max-pooling-layers-break-cnns-performance-in-solving-regression-proble
23. A Comprehensive Exploration of Pooling in Neural Networks, https://blog.paperspace.com/a-comprehensive-exploration-of-pooling-in-neural-networks/
24. Pooling layer - Wikipedia, https://en.wikipedia.org/wiki/Pooling_layer
25. Maxpooling vs minpooling vs average pooling | by Madhushree Basavarajaiah - Medium, https://medium.com/@bdhuma/which-pooling-method-is-better-maxpooling-vs-minpooling-vs-average-pooling-95fb03f45a9
26. Max Pooling vs Average Pooling for residual/skip connections - Cross Validated, https://stats.stackexchange.com/questions/530044/max-pooling-vs-average-pooling-for-residual-skip-connections
27. What is global max pooling layer and what is its advantage over maxpooling layer?, https://stats.stackexchange.com/questions/257321/what-is-global-max-pooling-layer-and-what-is-its-advantage-over-maxpooling-layer
28. Don't Use Flatten() - Global Pooling for CNNs with TensorFlow and Keras - Stack Abuse, https://stackabuse.com/dont-use-flatten-global-pooling-for-cnns-with-tensorflow-and-keras/
29. Activation Functions, Global Average Pooling, Softmax, Negative Likelihood Loss - Medium, https://medium.com/@anilaknb/activation-functions-global-average-pooling-softmax-negative-likelihood-loss-86fb50232459
30. Are CNNs exactly translation invariant with global max/average pooling layers?, https://ai.stackexchange.com/questions/39598/are-cnns-exactly-translation-invariant-with-global-max-average-pooling-layers
31. Stochastic Pooling for Regularization of Deep Convolutional Neural Networks | Request PDF - ResearchGate, https://www.researchgate.net/publication/234131103_Stochastic_Pooling_for_Regularization_of_Deep_Convolutional_NeuralNetworks
32. Stochastic Pooling for Regularization of Deep Convolutional Neural Networks - arXiv, https://arxiv.org/pdf/1301.3557
33. Stochastic Pooling for Regularization of Deep Convolutional Neural Networks - arXiv, https://arxiv.org/abs/1301.3557
34. [1412.6071] Fractional Max-Pooling - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/1412.6071
35. arXiv:1412.6071v3 [cs.CV] 2 Mar 2015, https://www.arxiv.org/pdf/1412.6071v3
36. arXiv:1412.6071v4 [cs.CV] 12 May 2015, https://arxiv.org/pdf/1412.6071
37. [1412.6071] Fractional Max-Pooling - arXiv, https://arxiv.org/abs/1412.6071
38. hybrid of max pooling and average pooling - Stack Overflow, https://stackoverflow.com/questions/45563438/hybrid-of-max-pooling-and-average-pooling
</code></pre>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>