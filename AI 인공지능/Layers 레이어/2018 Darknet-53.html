<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Darknet-53 아키텍처에 대한 심층 고찰</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Darknet-53 아키텍처에 대한 심층 고찰</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">딥러닝 레이어 (Layers)</a> / <span>Darknet-53 아키텍처에 대한 심층 고찰</span></nav>
                </div>
            </header>
            <article>
                <h1>Darknet-53 아키텍처에 대한 심층 고찰</h1>
<h2>1.  서론: YOLOv3와 새로운 백본의 등장</h2>
<p>객체 탐지 분야에서 You Only Look Once (YOLO) 패러다임의 등장은 실시간 처리의 새로운 지평을 열었다. YOLOv1은 객체 탐지를 경계 상자(bounding box) 좌표와 클래스 확률을 예측하는 단일 회귀 문제로 재정의함으로써, 당시 주류였던 2단계(two-stage) 탐지기들의 복잡한 파이프라인을 단일 신경망으로 통합했다.1 이 접근법은 압도적인 추론 속도를 달성했으나, 정확도, 특히 작은 객체에 대한 탐지 성능에서는 한계를 보였다. 후속 모델인 YOLOv2(YOLO9000)는 Darknet-19라는 더 깊고 효율적인 백본 네트워크를 도입하고 배치 정규화(Batch Normalization), 앵커 박스(anchor box) 등의 기법을 적용하여 성능을 개선했다.2 그럼에도 불구하고, YOLO 계열은 여전히 정확도 측면에서 Faster R-CNN과 같은 2단계 탐지기들과의 격차를 완전히 좁히지 못했다.</p>
<p>2018년, Joseph Redmon과 Ali Farhadi는 “YOLOv3: An Incremental Improvement“라는 논문을 통해 YOLO의 세 번째 버전을 공개했다.5 논문의 제목이 시사하듯, YOLOv3는 이전 모델의 철학을 뒤엎는 혁명적 변화보다는 기존 구조의 약점을 보완하고 실용적인 개선을 통해 완성도를 높이는 데 초점을 맞추었다.8 YOLOv3의 핵심 목표는 속도라는 강점을 유지하면서 정확도를 당대 최고 수준으로 끌어올리는 것이었다. 이를 달성하기 위해 다중 스케일 예측(multi-scale prediction)과 같은 새로운 기법이 도입되었으며, 이러한 복합적인 작업을 효과적으로 지원하기 위해서는 이전의 Darknet-19보다 훨씬 더 깊고 강력한 표현력을 지닌 특징 추출기(feature extractor)가 필수적이었다. 이러한 배경 속에서 YOLOv3의 심장부 역할을 수행할 새로운 백본 아키텍처, Darknet-53이 탄생했다.4</p>
<p>Darknet-53의 도입은 YOLO의 개발 철학이 중요한 전환점을 맞이했음을 의미한다. 초기 YOLO 모델들이 속도를 최우선 가치로 내세웠다면, YOLOv3는 정확도와의 균형을 추구하는 방향으로 나아갔다. 당시 최고 수준의 정확도를 자랑하던 RetinaNet과 같은 모델들과 경쟁하기 위해서는 7, 속도만으로는 부족했다. ResNet-101/152와 견줄 만한 깊이를 가진 Darknet-53의 등장은 3, YOLO가 더 이상 속도에만 의존하는 틈새 모델이 아니라, 속도-정확도 트레이드오프 곡선에서 가장 이상적인 지점, 즉 ’스위트 스폿(sweet spot)’을 점유하려는 주류 아키텍처로 진화하고 있음을 보여주는 명백한 증거였다.4 이 전략적 변화는 YOLO가 이후 객체 탐지 분야의 표준으로 자리매김하는 데 결정적인 역할을 했다.</p>
<h2>2.  Darknet-53의 설계 철학과 구조적 특징</h2>
<h3>2.1  설계 목표: 정확도와 효율성의 공존</h3>
<p>Darknet-53의 설계 철학은 두 가지 핵심 목표를 중심으로 구축되었다. 첫째는 당대 최고의 이미지 분류기로 인정받던 ResNet(Residual Network)의 장점을 수용하여 깊은 네트워크를 안정적으로 학습시키는 것이고, 둘째는 ResNet보다 더 높은 연산 효율성을 달성하여 실시간 객체 탐지라는 YOLO의 정체성을 유지하는 것이었다.</p>
<p>ResNet은 잔차 연결(residual connection)이라는 혁신적인 구조를 통해 100층이 넘는 매우 깊은 네트워크의 학습을 가능하게 했으며, 이는 네트워크 깊이가 깊어질수록 성능이 저하되는 퇴화(degradation) 문제와 기울기 소실(vanishing gradient) 문제를 효과적으로 해결했다.5 Darknet-53은 이 잔차 연결의 아이디어를 적극적으로 채택하여, 53층이라는 깊은 구조를 안정적으로 구축하고 풍부한 계층적 특징을 추출할 수 있는 기반을 마련했다.4</p>
<p>그러나 Darknet-53은 단순히 ResNet을 모방하는 데 그치지 않았다. 실시간 탐지를 위해서는 추론 속도(FPS)가 핵심 지표이므로, ResNet보다 더 적은 연산량(FLOPs)으로 동등하거나 그 이상의 성능을 내는 것을 목표로 삼았다. 이는 Darknet-53이 단순한 정확도 경쟁을 넘어, ’연산 효율성’이라는 척도에서 새로운 기준을 제시하고자 했음을 보여준다.3</p>
<h3>2.2  핵심 아키텍처 심층 분석</h3>
<p>Darknet-53은 이름에서 알 수 있듯이 53개의 학습 가능한 레이어로 구성된다. 더 정확히는 52개의 컨볼루션 레이어와 분류를 위한 마지막 1개의 완전 연결 레이어(fully connected layer)를 포함한다.13 전체 구조는 초기 특징 추출을 위한 컨볼루션 레이어 그룹과 5개의 주요 잔차 블록 그룹으로 나눌 수 있다.4</p>
<p>이러한 구조는 단순히 분류 성능을 높이기 위한 설계가 아니다. 많은 동시대 탐지기들이 ImageNet 분류를 위해 사전 학습된 VGG나 ResNet을 가져와 탐지 작업에 맞게 미세 조정(fine-tuning)하는 방식을 따랐지만, Darknet-53의 구조적 선택들은 처음부터 객체 탐지를 염두에 둔 결과물이다. 분류기의 목표는 공간 정보를 점진적으로 제거하여 최종적으로 공간 불변성(spatial invariance)을 갖는 단일 클래스 레이블을 만드는 것이다. 반면, 탐지기는 분류와 동시에 정확한 위치 정보를 예측해야 하므로 공간적 정밀성을 최대한 보존해야 한다. Darknet-53의 설계는 이러한 탐지 중심적 철학을 명확히 보여준다. 예를 들어, 5단계의 다운샘플링 구조는 YOLOv3의 세 가지 탐지 헤드가 필요로 하는 특정 해상도(스트라이드 8, 16, 32)의 특징 맵을 정확하게 생성하도록 맞춤 설계되었다.14 따라서 Darknet-53은 우수한 분류기이면서 동시에, 다중 스케일 탐지를 위한 특징 피라미드 생성기로서의 역할에 최적화된 아키텍처라 할 수 있다.</p>
<h4>2.2.1 Table 1: Darknet-53 상세 레이어 구성</h4>
<p>아래 표는 Darknet-53의 전체 레이어 구성을 단계별로 요약한 것이다. 각 잔차 블록(Residual Block)은 채널 수를 줄이는 1x1 컨볼루션과 특징을 추출하는 3x3 컨볼루션으로 구성된 후, 입력과 더해지는 구조를 반복한다.</p>
<table><thead><tr><th>레이어 유형</th><th>반복 횟수</th><th>커널 크기 / 스트라이드</th><th>필터 수</th><th>출력 크기 (416x416 입력 기준)</th></tr></thead><tbody>
<tr><td>Convolutional</td><td>1</td><td>3x3 / 1</td><td>32</td><td>416 x 416 x 32</td></tr>
<tr><td>Convolutional</td><td>1</td><td>3x3 / 2</td><td>64</td><td>208 x 208 x 64</td></tr>
<tr><td>Residual Block</td><td>1</td><td>1x1, 3x3</td><td>64</td><td>208 x 208 x 64</td></tr>
<tr><td>Convolutional</td><td>1</td><td>3x3 / 2</td><td>128</td><td>104 x 104 x 128</td></tr>
<tr><td>Residual Block</td><td>2</td><td>1x1, 3x3</td><td>128</td><td>104 x 104 x 128</td></tr>
<tr><td>Convolutional</td><td>1</td><td>3x3 / 2</td><td>256</td><td>52 x 52 x 256</td></tr>
<tr><td>Residual Block</td><td>8</td><td>1x1, 3x3</td><td>256</td><td>52 x 52 x 256</td></tr>
<tr><td>Convolutional</td><td>1</td><td>3x3 / 2</td><td>512</td><td>26 x 26 x 512</td></tr>
<tr><td>Residual Block</td><td>8</td><td>1x1, 3x3</td><td>512</td><td>26 x 26 x 512</td></tr>
<tr><td>Convolutional</td><td>1</td><td>3x3 / 2</td><td>1024</td><td>13 x 13 x 1024</td></tr>
<tr><td>Residual Block</td><td>4</td><td>1x1, 3x3</td><td>1024</td><td>13 x 13 x 1024</td></tr>
<tr><td>Average Pooling</td><td>1</td><td>-</td><td>-</td><td>1 x 1 x 1024</td></tr>
<tr><td>Fully Connected</td><td>1</td><td>-</td><td>1000</td><td>1000</td></tr>
</tbody></table>
<p>참고: 위 표는 GeeksforGeeks의 자료를 기반으로 재구성되었으며, ImageNet 분류 작업을 위한 전체 구조를 나타낸다.4 YOLOv3에서는 Average Pooling과 Fully Connected 레이어가 제거되고 탐지 헤드가 추가된다.</p>
<h4>2.2.2 잔차 연결 (Residual Connection)</h4>
<p>잔차 연결은 Darknet-53이 53층의 깊이를 효과적으로 학습할 수 있게 하는 핵심 기술이다. 잔차 블록은 입력 x가 여러 레이어로 구성된 비선형 변환 함수 <span class="math math-inline">\mathcal{F}</span>를 통과한 출력 <span class="math math-inline">\mathcal{F}(x)</span>에 원래의 입력 x를 요소별로 더하는(element-wise addition) 스킵 연결(skip connection) 구조를 가진다.15 이를 수식으로 표현하면 다음과 같다.<br />
<span class="math math-display">
y = \mathcal{F}(x, \{W_i\}) + x
</span><br />
여기서 y는 잔차 블록의 최종 출력이고, <span class="math math-inline">{W_i}</span>는 레이어의 가중치 집합을 의미한다. 이 구조는 역전파 과정에서 기울기가 <span class="math math-inline">\mathcal{F}</span>를 거치는 경로 외에, 입력 x를 통해 직접 전달되는 추가적인 경로를 제공한다. 덕분에 네트워크가 아무리 깊어져도 기울기가 소실되지 않고 초기 레이어까지 효과적으로 전파될 수 있다. 또한, 만약 특정 레이어 블록이 항등 매핑(identity mapping, 즉 y=x)을 학습하는 것이 최적이라면, 네트워크는 가중치들을 복잡하게 조정할 필요 없이 <span class="math math-inline">\mathcal{F}(x)</span>를 0에 가깝게 만드는 방향으로 학습하면 된다. 이는 깊은 네트워크에서 불필요한 레이어가 오히려 성능을 저하시키는 퇴화(degradation) 문제를 방지하는 데 결정적인 역할을 한다.15</p>
<h4>2.2.3 스트라이드 컨볼루션 (Strided Convolution)을 통한 다운샘플링</h4>
<p>기존의 많은 CNN 아키텍처들은 특징 맵의 공간적 해상도를 줄이기 위해 맥스 풀링(Max-Pooling) 레이어를 사용했다. 하지만 Darknet-53은 이를 사용하지 않고, 대신 스트라이드(stride)가 2인 컨볼루션 레이어를 통해 다운샘플링을 수행한다.18</p>
<p>이러한 선택은 특징 보존 측면에서 중요한 이점을 가진다. 맥스 풀링은 특정 커널 영역 내에서 가장 큰 활성화 값 하나만을 남기고 나머지 정보는 버리는 비가역적 연산이다. 이 과정에서 객체의 정밀한 위치나 형태와 관련된 저수준 공간 정보(low-level spatial feature)가 손실될 위험이 크다.19 특히 작은 객체를 탐지해야 하는 경우, 이러한 정보 손실은 치명적일 수 있다. 반면, 스트라이드가 2인 컨볼루션은 학습 가능한 필터를 사용하여 전체 입력 영역에 대한 연산을 수행하면서 공간 차원을 축소한다. 이는 네트워크가 데이터의 특성에 맞게 가장 효율적인 다운샘플링 방법을 스스로 학습하게 하므로, 풀링으로 인한 정보 손실을 최소화하고 더 풍부한 특징 표현을 유지할 수 있게 한다.19</p>
<h4>2.2.4 Leaky ReLU 활성화 함수</h4>
<p>활성화 함수로는 표준 ReLU 대신 Leaky ReLU가 채택되었다. 표준 ReLU 함수 <span class="math math-inline">f(x) = \max(0, x)</span>는 입력값이 음수일 때 출력이 항상 0이 되므로, 해당 뉴런의 기울기 역시 0이 되어 더 이상 가중치 업데이트가 일어나지 않는 ‘죽은 ReLU(Dying ReLU)’ 문제를 야기할 수 있다.22</p>
<p>Leaky ReLU는 이러한 문제를 해결하기 위해 입력이 음수일 때 0 대신 아주 작은 양의 기울기를 부여한다. 이를 수식으로 표현하면 다음과 같다.24<br />
<span class="math math-display">
f(x) = \begin{cases} x, &amp; \text{if } x \ge 0 \\ \alpha x, &amp; \text{otherwise} \end{cases}
</span><br />
여기서 α는 0.01과 같이 미리 정의된 작은 상수이다. 이 작은 기울기 덕분에 음수 입력을 받는 뉴런도 계속해서 기울기를 역전파할 수 있으며, 모든 뉴런이 학습 과정에 지속적으로 참여하게 되어 학습의 안정성과 수렴 속도를 높이는 데 기여한다.4</p>
<h2>3.  성능 및 효율성 심층 평가</h2>
<p>Darknet-53의 진정한 가치는 단순히 깊은 구조를 가졌다는 점이 아니라, 그 깊이를 매우 높은 연산 효율성으로 구현했다는 데 있다. 이는 ImageNet 이미지 분류 데이터셋에서의 성능 벤치마크를 통해 명확히 입증된다.</p>
<h3>3.1  ImageNet 분류 성능 벤치마크</h3>
<p>Darknet-53의 특징 추출 성능을 객관적으로 평가하기 위해, Joseph Redmon은 논문에서 ImageNet 데이터셋에 대한 분류 정확도와 추론 속도를 이전 백본인 Darknet-19 및 당대의 표준 아키텍처인 ResNet-101, ResNet-152와 비교했다.</p>
<h4>3.1.1 Table 2: 백본 네트워크 성능 비교</h4>
<p>아래 표는 각 네트워크의 ImageNet 분류 성능을 주요 지표별로 정리한 것이다. Top-1 정확도는 가장 확률이 높은 예측이 정답인 비율, Top-5 정확도는 상위 5개 예측 안에 정답이 포함된 비율을 의미한다. BFLOPs는 수십억 단위의 부동소수점 연산 횟수로, 모델의 계산 복잡도를 나타낸다.</p>
<table><thead><tr><th>백본 아키텍처</th><th>Top-1 정확도 (%)</th><th>Top-5 정확도 (%)</th><th>추론 속도 (FPS)</th><th>연산량 (BFLOPs)</th></tr></thead><tbody>
<tr><td>Darknet-19</td><td>74.1</td><td>91.8</td><td>-</td><td>5.58</td></tr>
<tr><td>ResNet-101</td><td>77.1</td><td>93.5</td><td>57</td><td>15.6</td></tr>
<tr><td>ResNet-152</td><td>77.8</td><td>93.8</td><td>41</td><td>22.1</td></tr>
<tr><td><strong>Darknet-53</strong></td><td><strong>77.2</strong></td><td><strong>93.8</strong></td><td><strong>78</strong></td><td><strong>14.5</strong></td></tr>
</tbody></table>
<p>참고: 정확도 및 FPS 수치는 “YOLOv3: An Incremental Improvement” 논문 및 관련 분석 자료를 기반으로 하며 3, 연산량은 유사한 분석 자료를 참조하여 구성했다. GPU 환경(Titan X) 및 입력 크기(256x256)에 따라 값은 달라질 수 있다.</p>
<h3>3.2  효율성 분석: 연산량 대비 성능</h3>
<p>위 표의 데이터는 Darknet-53의 설계 목표가 성공적으로 달성되었음을 명확하게 보여준다.</p>
<p>첫째, <strong>속도</strong> 측면에서 Darknet-53은 압도적인 우위를 보인다. 78 FPS의 추론 속도는 57 FPS의 ResNet-101보다 약 1.37배, 41 FPS의 ResNet-152보다는 약 1.9배 빠른 수치이다.3 이는 실시간 객체 탐지라는 목표에 완벽하게 부합하는 성능이다.</p>
<p>둘째, <strong>정확도</strong> 측면에서 Darknet-53은 훨씬 더 깊고 복잡한 ResNet-152와 대등한 수준의 성능(Top-5 정확도 93.8%)을 달성했다. 이는 속도를 위해 정확도를 희생한 것이 아니라, 두 마리 토끼를 모두 잡았음을 의미한다.9</p>
<p>이러한 성공의 핵심은 <strong>연산 효율성</strong>에 있다. Darknet-53의 연산량은 14.5 BFLOPs로, 22.1 BFLOPs를 요구하는 ResNet-152보다 현저히 낮다. 즉, Darknet-53은 더 적은 계산 자원을 사용하면서도 동등한 수준의 정확도를 달성한 것이다. 이는 “연산 당 성능(performance per FLOP)“이라는 지표에서 기존 아키텍처들을 능가했음을 보여준다.</p>
<p>이러한 탁월한 효율성은 단순히 개별 기술(잔차 연결, Leaky ReLU 등)의 합으로만 설명되지 않는다. 이는 네트워크의 깊이와 너비(채널 수) 사이의 절묘한 균형을 찾아낸 결과로 해석할 수 있다. Darknet-19처럼 너무 얕으면 복잡한 특징을 학습할 표현력이 부족하고, ResNet-152처럼 너무 깊으면 연산량이 급증하여 실시간성에 제약을 받는다. Darknet-53의 53층 구조는 잔차 연결 덕분에 충분히 깊어 풍부한 계층적 특징을 학습할 수 있으면서도, 각 잔차 블록 내의 1x1 컨볼루션을 통해 채널 수를 효과적으로 제어하여 연산량이 과도하게 증가하는 것을 막았다. 결국 Darknet-53은 실시간 객체 탐지라는 제약 조건 하에서 특징 학습 능력을 극대화하는, 공학적으로 최적화된 ’스위트 스폿’을 찾아낸 아키텍처라고 평가할 수 있다.</p>
<h2>4.  YOLOv3에서의 역할: 다중 스케일 특징 추출기</h2>
<p>Darknet-53은 단독으로 사용되는 분류기를 넘어, YOLOv3의 다중 스케일 탐지 전략을 구현하는 핵심 엔진으로서의 역할을 수행한다. YOLOv3의 가장 큰 개선점 중 하나는 서로 다른 크기의 객체를 효과적으로 탐지하기 위해 세 가지 다른 스케일에서 예측을 수행한다는 것이다.14</p>
<h3>4.1  특징 피라미드 구조</h3>
<p>YOLOv3는 FPN(Feature Pyramid Network)과 유사한 개념을 차용하여 다중 스케일 특징을 융합한다.2 이 과정에서 Darknet-53은 계층적인 특징 맵을 생성하는 역할을 담당한다.</p>
<ol>
<li><strong>특징 맵 추출:</strong> Darknet-53은 입력 이미지를 순차적으로 처리하며 5번의 다운샘플링을 수행한다. YOLOv3는 이 중 마지막 3개의 다운샘플링 단계에서 생성된 특징 맵을 예측에 사용한다. 구체적으로 입력 이미지가 416x416일 때, 스트라이드가 각각 8, 16, 32인 지점에서 52x52, 26x26, 13x13 크기의 특징 맵을 추출한다.14</li>
<li><strong>특징 융합:</strong> 가장 깊은 레이어에서 추출된 13x13 특징 맵은 풍부한 의미론적 정보(semantic information)를 담고 있어 큰 객체를 탐지하는 데 유리하다. YOLOv3는 이 특징 맵을 2배 업샘플링(upsampling)한 후, 그 이전 단계에서 추출한 26x26 특징 맵과 채널 축을 기준으로 연결(concatenation)한다. 이 융합된 특징 맵은 고수준의 의미 정보와 더불어 상대적으로 높은 해상도의 공간 정보를 모두 갖게 되어 중간 크기 객체 탐지에 사용된다. 이 과정은 한 번 더 반복되어, 26x26 융합 특징 맵을 다시 2배 업샘플링하여 52x52 특징 맵과 연결하고, 이는 작은 객체 탐지에 활용된다.14</li>
</ol>
<p>이러한 상향식(top-down) 특징 융합 방식을 통해, 얕은 레이어의 세밀한(fine-grained) 특징과 깊은 레이어의 추상적인 특징이 결합되어 모든 스케일의 탐지 헤드가 풍부한 정보를 바탕으로 예측을 수행할 수 있게 된다.</p>
<h3>4.2  작은 객체 탐지 성능 향상에의 기여</h3>
<p>이전 YOLO 버전의 가장 큰 약점은 작은 객체 탐지 성능이었다. Darknet-53을 기반으로 한 다중 스케일 예측 구조는 이 문제를 정면으로 해결했다. 52x52와 같이 상대적으로 고해상도의 특징 맵을 예측에 직접 활용함으로써, 이전에는 특징이 소실되어 탐지하기 어려웠던 작은 객체들을 효과적으로 포착할 수 있게 되었다.3 이 개선 덕분에 YOLOv3는 작은 객체에 대한 평균 정밀도(AP_small)를 크게 향상시켰고, 이는 SSD와 같은 다른 경쟁적인 단일 단계 탐지기들과 대등한 성능을 내는 중요한 발판이 되었다.31</p>
<h2>5.  유산(Legacy): 후속 모델에 미친 영향과 한계</h2>
<p>Darknet-53은 YOLOv3의 성공을 이끈 핵심 동력이었지만, 그 자체로 완벽한 아키텍처는 아니었다. 후속 연구들은 Darknet-53의 한계를 분석하고 이를 개선하는 방향으로 발전했으며, 이는 Darknet-53이 현대 객체 탐지 아키텍처의 중요한 유산을 남겼음을 방증한다.</p>
<h3>5.1  CSPDarknet53으로의 진화</h3>
<p>Darknet-53은 연산 효율성 측면에서 큰 성공을 거두었지만, 그 구조는 여전히 최적화의 여지를 남기고 있었다. 특히 잔차 연결 구조는 기울기 흐름을 원활하게 했지만, 연산 과정에서의 중복성과 메모리 대역폭 측면에서 비효율성을 내포했다. 잔차 블록의 <code>F(x) + x</code> 연산은 <code>F(x)</code>가 계산되는 동안 원본 입력 <code>x</code>를 메모리에 유지해야 하므로, 메모리 사용량과 데이터 이동 비용을 증가시킨다. FPN 구조의 상향식 특징 융합 역시 초기 레이어의 거대한 특징 맵을 메모리에 유지해야 하므로 비슷한 부담을 야기한다. 이러한 문제는 단순 FLOPs 계산만으로는 드러나지 않는 ’보이지 않는 비용’이며, 실제 하드웨어에서의 성능 병목 현상을 유발할 수 있다.</p>
<p>이러한 한계를 극복하기 위해 YOLOv4에서는 CSPNet(Cross Stage Partial Network) 개념을 Darknet-53에 접목한 <strong>CSPDarknet53</strong>을 새로운 백본으로 채택했다.32 CSPNet의 핵심 아이디어는 각 단계의 입력 특징 맵을 두 개의 경로로 분할하는 것이다. 한 경로는 기존의 잔차 블록들을 통과하고, 다른 경로는 별도의 변환 없이 바로 다음 단계로 전달된다. 두 경로의 출력은 마지막에 다시 합쳐진다. 이 설계를 통해 전체 네트워크의 연산량을 약 20% 절감하면서도, 기울기 흐름을 더욱 다양화하고 풍부하게 만들어 정확도를 유지하거나 향상시킬 수 있었다.33 CSPDarknet53은 연산량 감소뿐만 아니라, 데이터 흐름을 더 규칙적이고 효율적으로 만들어 메모리 대역폭 문제를 완화하고 하드웨어 가속에 더 친화적인 구조를 제공했다. 이는 Darknet-53의 한계를 정확히 파악하고 이를 개선한 논리적 진화의 결과물이다.</p>
<h3>5.2  Darknet-53의 영향력과 현재 위치</h3>
<p>Darknet-53은 YOLOv3의 폭발적인 성공을 통해 ‘정확하면서도 빠른’ 백본 아키텍처의 새로운 표준을 제시했다. 이는 후속 객체 탐지 모델 개발에 지대한 영향을 미쳤다. YOLOv4, YOLOX, YOLOv5 등 수많은 후속 YOLO 계열 모델들은 CSPDarknet53과 같이 Darknet-53의 기본 설계 철학을 계승하고 발전시키는 형태로 진화했다.36</p>
<p>결론적으로, Darknet-53은 단순히 YOLOv3의 부속품이 아니라, 현대 객체 탐지기 백본 설계의 패러다임에 영향을 미친 중요한 아키텍처이다. ResNet의 깊이와 표현력을 실시간 탐지 환경에 맞게 효율적으로 재해석한 Darknet-53의 성공은, 이후 수많은 연구가 정확도와 속도의 균형점을 찾는 데 중요한 기준점이 되었다. 비록 CSPDarknet53과 같은 더 발전된 형태로 대체되었지만, 그 설계 원칙과 철학은 여전히 현대 객체 탐지 모델들의 근간을 이루고 있다.</p>
<h2>6.  결론: Darknet-53의 종합적 평가</h2>
<p>Darknet-53은 YOLOv3의 핵심 구성 요소이자, 2018년 당시 객체 탐지 아키텍처 설계에 있어 중요한 이정표를 세운 모델이다. 본 고찰을 통해 분석한 Darknet-53의 핵심 기여와 의의는 다음과 같이 요약할 수 있다.</p>
<p>첫째, Darknet-53은 <strong>정확도와 효율성의 성공적인 공존</strong>을 입증했다. ResNet으로부터 잔차 연결이라는 핵심 아이디어를 차용하여 53층의 깊은 네트워크를 안정적으로 학습할 수 있는 기반을 마련하면서도, 스트라이드 컨볼루션을 통한 학습 가능한 다운샘플링과 효율적인 잔차 블록 설계를 통해 ResNet-152와 대등한 정확도를 훨씬 적은 연산량과 빠른 속도로 달성했다. 이는 실시간 객체 탐지 모델이 더 이상 정확도를 크게 희생할 필요가 없음을 보여준 중요한 사례이다.</p>
<p>둘째, Darknet-53은 YOLOv3의 <strong>다중 스케일 탐지 능력을 실현하는 중추적인 역할</strong>을 수행했다. 계층적으로 깊어지는 구조를 통해 다양한 해상도의 특징 맵을 생성하고, 이를 FPN 스타일의 특징 융합 메커니즘과 결합함으로써 이전 YOLO 버전의 고질적인 문제였던 작은 객체 탐지 성능을 획기적으로 개선했다. 이는 YOLO가 더 넓은 범위의 실제 응용 분야에 적용될 수 있는 범용성을 갖추게 된 결정적인 계기가 되었다.</p>
<p>셋째, Darknet-53은 <strong>후속 객체 탐지 모델의 발전에 지대한 영향</strong>을 미쳤다. YOLOv4의 CSPDarknet53을 비롯한 수많은 후속 모델들이 Darknet-53의 설계 철학을 계승하고 이를 개선하는 방향으로 발전했다. 이는 Darknet-53이 제시한 ’효율적인 심층 네트워크’라는 개념이 객체 탐지 분야의 중요한 연구 방향으로 자리 잡았음을 의미한다.</p>
<p>결론적으로, Darknet-53은 단순한 하나의 백본 아키텍처를 넘어, 실시간 객체 탐지 분야의 기술적 한계를 한 단계 끌어올린 혁신적인 결과물이다. 속도와 정확도라는 두 가지 상충되는 목표 사이에서 최적의 균형점을 찾아낸 Darknet-53의 설계 원칙은, 오늘날에도 여전히 효율적인 심층 신경망을 설계하고자 하는 연구자들에게 중요한 영감과 교훈을 제공하는 이정표로 남아있다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>[1506.02640] You Only Look Once: Unified, Real-Time Object Detection - arXiv, https://arxiv.org/abs/1506.02640</li>
<li>Improved YOLO-V3 with DenseNet for Multi-Scale Remote Sensing Target Detection - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC7435986/</li>
<li>YOLOv3: A Huge Improvement. YOLO : You Only Look Once by …, https://sonawaneanand.medium.com/yolo3-a-huge-improvement-2bc4e6fc44c5</li>
<li>Darknet 53 - GeeksforGeeks, https://www.geeksforgeeks.org/computer-vision/darknet-53/</li>
<li>[PDF] YOLOv3: An Incremental Improvement - Semantic Scholar, https://www.semanticscholar.org/paper/YOLOv3%3A-An-Incremental-Improvement-Redmon-Farhadi/ebc96892b9bcbf007be9a1d7844e4b09fde9d961</li>
<li>‪Joseph Redmon‬ - ‪Google Scholar‬, https://scholar.google.com/citations?user=TDk_NfkAAAAJ&amp;hl=en</li>
<li>[1804.02767] YOLOv3: An Incremental Improvement - arXiv, https://arxiv.org/abs/1804.02767</li>
<li>Yolov3: An Incremental Improvement: Joseph Redmon, Ali Farhadi | PDF | Accuracy And Precision | Statistical Classification - Scribd, https://www.scribd.com/document/386736130/YOLOv3</li>
<li>YOLOv3: Real-Time Object Detection Algorithm (Guide) - Viso Suite, https://viso.ai/deep-learning/yolov3-overview/</li>
<li>Digging deep into YOLO V3 - A hands-on guide Part 1 | Towards Data Science, https://towardsdatascience.com/digging-deep-into-yolo-v3-a-hands-on-guide-part-1-78681f2c7e29/</li>
<li>AI-Powered Paper Summarization about the arXiv paper 1804.02767v1, https://www.summarizepaper.com/en/arxiv-id/1804.02767v1/</li>
<li>Darknet-53 architecture adopted by YOLOv3 (from [13]). - ResearchGate, https://www.researchgate.net/figure/Darknet-53-architecture-adopted-by-YOLOv3-from-13_fig4_336646881</li>
<li>What is the real architecture(layers) of YOLOv3? - Stack Overflow, https://stackoverflow.com/questions/60851999/what-is-the-real-architecturelayers-of-yolov3</li>
<li>The Application of Improved YOLO V3 in Multi-Scale Target Detection, https://www.mdpi.com/2076-3417/9/18/3775</li>
<li>What is Residual Connection? - Towards Data Science, https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55/</li>
<li>What is Residual Connection? | Towards Data Science, https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55</li>
<li>8.6. Residual Networks (ResNet) and ResNeXt - Dive into Deep Learning, http://d2l.ai/chapter_convolutional-modern/resnet.html</li>
<li>【ML Paper】Explanation of all of YOLO series Part 11 - Zenn, https://zenn.dev/yuto_mo/articles/14a87a0db17dfa</li>
<li>All you need to know about YOLO v3 (You Only Look Once) - DEV …, https://dev.to/afrozchakure/all-you-need-to-know-about-yolo-v3-you-only-look-once-e4m</li>
<li>You Only Look Once (YOLO) - - NeuralCeption -, https://www.neuralception.com/objectdetection-yolo/</li>
<li>[D] Comparison of architectures with max-pooling layers and convolutional layers only (with stride &gt; 1) : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/jriap0/d_comparison_of_architectures_with_maxpooling/</li>
<li>Activation functions: ReLU vs. Leaky ReLU | by Srikari Rallabandi - Medium, https://medium.com/mlearning-ai/activation-functions-relu-vs-leaky-relu-b8272dc0b1be</li>
<li>What are some use cases where leaky ReLu is preferred to ReLu? - Quora, https://www.quora.com/What-are-some-use-cases-where-leaky-ReLu-is-preferred-to-ReLu</li>
<li>machine-learning-articles/using-leaky-relu-with-keras.md at main - GitHub, https://github.com/christianversloot/machine-learning-articles/blob/main/using-leaky-relu-with-keras.md</li>
<li>Activation Functions in Deep Learning (Sigmoid, ReLU, LReLU, PReLU, RReLU, ELU, Softmax) - Lipman’s Artificial Intelligence Directory, https://laid.delanover.com/activation-functions-in-deep-learning-sigmoid-relu-lrelu-prelu-rrelu-elu-softmax/</li>
<li>ReLU vs Leaky ReLU vs ELU with pros and cons - Data Science Stack Exchange, https://datascience.stackexchange.com/questions/102483/relu-vs-leaky-relu-vs-elu-with-pros-and-cons</li>
<li>YOLOv3, and YOLOv3u - Ultralytics YOLO Docs, https://docs.ultralytics.com/models/yolov3/</li>
<li>The Ultimate Guide to YOLOv3 Architecture - ProjectPro, https://www.projectpro.io/article/yolov3-architecture/836</li>
<li>Architecture of YOLOv3 - OpenGenus IQ, https://iq.opengenus.org/architecture-of-yolov3/</li>
<li>YOLO V3 Explained - Medium, https://medium.com/data-science/yolo-v3-explained-ff5b850390f</li>
<li>YOLOv3: An Incremental Improvement | Request PDF - ResearchGate, https://www.researchgate.net/publication/324387691_YOLOv3_An_Incremental_Improvement</li>
<li>Review of YOLOv4 Architecture. Paper, Original Code, PyTorch Code - Cenk Bircanoglu, https://cenk-bircanoglu.medium.com/review-of-yolov4-architecture-f488ec32c1c4</li>
<li>YOLOv4: The Evolution of Object Detection with CSPDarknet53 - SERP AI, https://serp.ai/posts/cspdarknet53/</li>
<li>Comparison of CSPDarkNet53, CSPResNeXt-50, and EfficientNet-B0 Backbones on YOLO V4 as Object Detector, <a href="http://download.garuda.kemdikbud.go.id/article.php?article=2999669&amp;val=21071&amp;title=Comparison+of+CSPDarkNet53+CSPResNeXt-50+and+EfficientNet-B0+Backbones+on+YOLO+V4+as+Object+Detector">http://download.garuda.kemdikbud.go.id/article.php?article=2999669&amp;val=21071&amp;title=Comparison%20of%20CSPDarkNet53%20CSPResNeXt-50%20and%20EfficientNet-B0%20Backbones%20on%20YOLO%20V4%20as%20Object%20Detector</a></li>
<li>YOLOv4 — Version 3: Proposed Workflow - Medium, https://medium.com/visionwizard/yolov4-version-3-proposed-workflow-e4fa175b902</li>
<li>What is YOLOv6? A Deep Insight into the Object Detection Model - arXiv, https://arxiv.org/html/2412.13006v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>