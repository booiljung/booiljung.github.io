<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:완전 컨볼루션 네트워크(Fully Convolutional Network, FCN, 2015)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>완전 컨볼루션 네트워크(Fully Convolutional Network, FCN, 2015)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">딥러닝 레이어 (Layers)</a> / <span>완전 컨볼루션 네트워크(Fully Convolutional Network, FCN, 2015)</span></nav>
                </div>
            </header>
            <article>
                <h1>완전 컨볼루션 네트워크(Fully Convolutional Network, FCN, 2015)</h1>
<h2>1.  시맨틱 분할의 패러다임 전환, 완전 컨볼루션 네트워크</h2>
<h3>1.1  시맨틱 분할 문제의 정의와 도전 과제</h3>
<p>시맨틱 분할(Semantic Segmentation)은 컴퓨터 비전의 핵심 과제 중 하나로, 이미지 내 모든 픽셀에 대해 해당 픽셀이 속하는 객체 클래스(class)를 할당하는 것을 목표로 한다.1 이는 이미지 전체에 대해 단 하나의 레이블을 예측하는 이미지 분류(image classification)나 객체의 위치를 경계 상자(bounding box)로 표현하는 객체 탐지(object detection)보다 훨씬 더 정교한 이해를 요구하는 밀집 예측(dense prediction) 문제이다.3</p>
<p>시맨틱 분할은 근본적으로 ’무엇(what)’과 ’어디(where)’라는 두 가지 정보 사이의 상충 관계(trade-off)를 해결해야 하는 도전을 내포한다.4 ’무엇’은 이미지의 전역적인 문맥(global context)을 통해 파악되는 의미론적 정보(semantic information)이며, ’어디’는 객체의 정확한 경계와 위치를 나타내는 지역적인 공간 정보(spatial information)이다. 깊은 신경망일수록 더 넓은 수용 영역(receptive field)을 통해 추상적이고 의미론적인 특징을 잘 추출하지만, 반복적인 다운샘플링(downsampling) 과정에서 정교한 공간 정보는 점차 소실된다. 반대로 얕은 신경망은 공간적 해상도는 높게 유지하지만, 의미론적 문맥을 파악하는 데 한계가 있다. FCN 이전의 방법론들은 이 두 가지 정보를 효과적으로 통합하는 데 어려움을 겪었다.</p>
<h3>1.2  FCN 이전 시대: 패치 기반 CNN 접근법의 한계</h3>
<p>FCN이 등장하기 이전, 시맨틱 분할에 대한 지배적인 접근법은 기존의 이미지 분류용 합성곱 신경망(Convolutional Neural Network, CNN)을 패치(patch) 단위로 적용하는 것이었다.4 이 방식은 분할하고자 하는 각 픽셀 주변에 작은 이미지 조각(patch)을 추출하고, 이 패치를 AlexNet이나 VGG와 같은 분류 모델에 입력하여 중심 픽셀의 클래스를 예측하는 과정을 반복한다.</p>
<p>그러나 이러한 패치 기반 접근법은 몇 가지 치명적인 한계를 가지고 있었다.</p>
<ol>
<li><strong>계산 비효율성</strong>: 이미지의 모든 픽셀에 대해 독립적으로 CNN을 실행하는 것은 엄청난 계산 중복을 야기한다. 인접한 픽셀들의 패치는 대부분의 영역을 공유함에도 불구하고, 매번 동일한 컨볼루션 연산을 반복해야 했다. 이로 인해 단일 이미지를 처리하는 데 수 분에서 수 시간이 소요되어 실시간 적용이 불가능했다.6</li>
<li><strong>제한된 문맥 정보 활용</strong>: 각 패치는 독립적으로 처리되므로, 네트워크는 패치 크기를 넘어서는 전역적인 문맥 정보를 활용하기 어렵다.7 이는 객체 간의 관계나 전체적인 장면 구조를 이해하는 데 필수적인 정보를 놓치게 만들어, 특히 크기가 크거나 모호한 객체를 분할할 때 성능 저하의 원인이 되었다.</li>
<li><strong>복잡한 후처리 의존성</strong>: 패치 단위 예측 결과는 공간적 일관성이 부족하여 종종 노이즈가 많고 부자연스러운 분할 맵을 생성했다. 이를 보정하기 위해 조건부 무작위장(Conditional Random Fields, CRFs)이나 초픽셀(superpixel) 기반의 정제와 같은 복잡한 후처리 과정이 필수적으로 요구되었다.4 이는 전체 파이프라인을 복잡하게 만들고, 종단 간(end-to-end) 학습을 방해하는 요인이었다.</li>
</ol>
<h3>1.3  FCN의 혁신: 종단 간(End-to-End), 픽셀 대 픽셀(Pixels-to-Pixels) 학습의 서막</h3>
<p>2015년, Long, Shelhamer, Darrell은 “Fully Convolutional Networks for Semantic Segmentation“이라는 논문을 통해 이러한 한계를 근본적으로 해결하는 새로운 패러다임을 제시했다.4 FCN은 시맨틱 분할을 위해 최초로 신경망을 ‘종단 간(end-to-end), 픽셀 대 픽셀(pixels-to-pixels)’ 방식으로 학습시킨 모델이다.9</p>
<p>FCN의 핵심 아이디어는 기존 분류용 CNN의 구조를 변형하여, 임의의 크기(arbitrary size) 입력을 받아 그에 상응하는 공간적 차원을 가진 출력(heatmap)을 생성하는 ‘완전 컨볼루션(fully convolutional)’ 네트워크를 구축하는 것이었다.4 이는 고정된 크기의 입력만을 처리할 수 있었던 기존 CNN의 한계를 극복하고, 이미지 전체를 단 한 번의 순전파(forward pass)로 처리하여 모든 픽셀에 대한 예측을 동시에 수행할 수 있게 만들었다. 이러한 접근 방식은 단순한 효율성 향상을 넘어, 시맨틱 분할을 파편화된 여러 단계의 문제에서 통합된 단일 학습 문제로 재정의하는 철학적 전환을 이끌었다. FCN은 분할 과제에 최적화된 특징 표현을 데이터로부터 직접 학습할 수 있는 길을 열었고, 이는 이후 시맨틱 분할 연구의 폭발적인 성장을 촉발하는 계기가 되었다.</p>
<h3>1.4  전통적 CNN vs. FCN: 근본적 구조 비교</h3>
<p>FCN이 가져온 구조적 혁신을 명확히 이해하기 위해 전통적인 분류용 CNN과 FCN을 비교하면 다음과 같다. 전통적인 CNN은 특징 추출을 위한 컨볼루션 및 풀링 계층과 분류를 위한 완전 연결(Fully Connected, FC) 계층으로 구성된다.5 특히 FC 계층은 입력 특징 맵을 1차원 벡터로 평탄화(flatten)하여 공간 정보를 소실시키고, 고정된 크기의 출력(클래스 점수 벡터)을 생성한다. 이 때문에 입력 이미지의 크기 또한 고정되어야만 한다.12</p>
<p>반면, FCN은 이러한 FC 계층을 모두 컨볼루션 계층으로 대체한다.13 이 ’완전 컨볼루션화(convolutionalization)’를 통해 네트워크는 입력부터 출력까지 특징 맵의 2차원 공간 구조를 그대로 유지할 수 있다. 결과적으로 FCN은 고정된 벡터 대신, 각 픽셀 위치가 입력 이미지의 해당 위치에 대응되는 조밀한 예측 맵(dense prediction map) 또는 히트맵(heatmap)을 출력할 수 있게 된다.14</p>
<table><thead><tr><th>특징 (Feature)</th><th>전통적 CNN (분류용)</th><th>완전 컨볼루션 네트워크 (FCN, 분할용)</th></tr></thead><tbody>
<tr><td><strong>주요 목표</strong></td><td>이미지 단위 분류 (Image-level Classification)</td><td>픽셀 단위 분류 (Pixel-level Classification)</td></tr>
<tr><td><strong>최종 계층</strong></td><td>완전 연결 계층 (Fully-Connected Layers)</td><td>컨볼루션 계층 (Convolutional Layers)</td></tr>
<tr><td><strong>입력 크기 요구사항</strong></td><td>고정 (Fixed-size)</td><td>임의 (Arbitrary-size)</td></tr>
<tr><td><strong>출력 형태</strong></td><td>1차원 클래스 점수 벡터 (1D Class Score Vector)</td><td>2차원 예측 맵 (2D Prediction Map)</td></tr>
<tr><td><strong>공간 정보 처리</strong></td><td>최종 단계에서 공간 정보 소실</td><td>네트워크 전체에서 공간 정보 유지</td></tr>
<tr><td><strong>학습 패러다임</strong></td><td>이미지 대 레이블 (Image-to-Label)</td><td>픽셀 대 픽셀 (Pixels-to-Pixels)</td></tr>
</tbody></table>
<p><strong>표 1: 전통적 CNN과 FCN의 구조적 및 기능적 비교</strong></p>
<h2>2.  FCN의 핵심 원리: 완전 컨볼루션화 (Convolutionalization)</h2>
<p>FCN의 가장 근본적인 혁신은 기존의 분류용 CNN을 시맨틱 분할에 적합한 형태로 변형시킨 ’완전 컨볼루션화(convolutionalization)’에 있다. 이는 완전 연결(FC) 계층을 기능적으로 동일한 컨볼루션 계층으로 재해석하고 대체하는 과정이다. 이 변환을 통해 네트워크는 임의 크기의 입력을 처리하고, 공간 정보를 담은 히트맵을 효율적으로 생성하는 능력을 갖추게 된다.</p>
<h3>2.1  완전 연결 계층의 재해석: 공간을 압축하는 컨볼루션</h3>
<p>수학적으로 FC 계층은 입력 특징 맵을 1차원 벡터로 평탄화한 뒤, 가중치 행렬과 곱셈 연산을 수행하는 것으로 정의된다. 그러나 이 연산은 특정 형태의 컨볼루션 연산과 동일하게 해석될 수 있다. 입력 특징 맵의 공간적 차원(<code>W</code> x <code>H</code>)과 동일한 크기의 커널(<code>F</code>=<code>W</code>, <code>F</code>=<code>H</code>)을 가진 컨볼루션 계층은 <code>stride=1</code>, <code>padding=0</code> 조건에서 정확히 <code>1x1</code> 크기의 출력을 생성한다.15</p>
<p>예를 들어, VGG-16 네트워크에서 <code>pool5</code> 계층의 출력인 <code>7x7x512</code> 크기의 특징 맵을 처리하는 첫 번째 FC 계층(<code>fc6</code>)을 생각해보자. 이 계층은 4096개의 뉴런을 가지며, 각 뉴런은 입력 특징 맵의 모든 값(<code>7*7*512</code>개)과 연결된다. 이는 4096개의 <code>7x7x512</code> 크기 필터를 가진 컨볼루션 연산으로 완벽하게 대체될 수 있다. 이 컨볼루션 연산의 결과는 <code>1x1x4096</code> 크기의 특징 맵이 되며, 이는 원래 FC 계층의 출력 벡터와 정보적으로 동일하다.15 이후의 FC 계층들(<code>fc7</code>, <code>fc8</code>)은 <code>1x1xN</code> 형태의 입력을 처리하므로, <code>1x1</code> 컨볼루션으로 자연스럽게 변환될 수 있다.17</p>
<p>이러한 재해석은 분류 네트워크가 본질적으로 조밀한 특징 맵을 학습하지만, FC 계층이라는 구조적 병목 현상 때문에 풍부한 공간 정보가 마지막 단계에서 파괴된다는 사실을 드러낸다. FCN의 기여는 이 병목을 제거함으로써, 네트워크의 특징 계층 구조에 이미 내재된 공간 정보를 ’해방’시킨 것이다. 이는 이미지넷(ImageNet)과 같은 대규모 분류 데이터셋으로 학습된 특징 표현이 단순한 추상적 개념이 아니라, 공간적으로 의미가 있으며 픽셀 단위의 이해가 필요한 과제에 직접적으로 전이될 수 있음을 증명했다. FCN은 분할 모델을 처음부터 학습시킬 필요 없이, 기존 분류 모델의 마지막 계층을 재해석하는 것만으로도 강력한 성능을 얻을 수 있다는 전이 학습의 새로운 지평을 열었다.</p>
<h3>2.2  컨볼루션화의 과정: VGG-16을 FCN으로</h3>
<p>실제 VGG-16과 같은 분류 네트워크를 FCN으로 변환하는 과정은 다음과 같다.12</p>
<ol>
<li><strong>분류 계층 제거</strong>: 최종적으로 1000개의 클래스에 대한 확률을 출력하는 마지막 분류 계층(softmax layer)을 제거한다.</li>
<li><strong>FC 계층의 컨볼루션 변환</strong>: VGG-16의 3개 FC 계층(<code>fc6</code>, <code>fc7</code>, <code>fc8</code>)을 컨볼루션 계층으로 변환한다.</li>
</ol>
<ul>
<li><code>fc6</code>: <code>7x7x512</code> 입력을 받아 <code>1x1x4096</code> 출력을 생성하므로, <code>7x7</code> 크기의 필터 4096개를 가진 컨볼루션 계층으로 변환된다.</li>
<li><code>fc7</code>: <code>1x1x4096</code> 입력을 받아 <code>1x1x4096</code> 출력을 생성하므로, <code>1x1</code> 크기의 필터 4096개를 가진 컨볼루션 계층으로 변환된다.</li>
<li><code>fc8</code>: <code>1x1x4096</code> 입력을 받아 <code>1x1x1000</code> 출력을 생성하므로, <code>1x1</code> 크기의 필터 1000개를 가진 컨볼루션 계층으로 변환된다.</li>
</ul>
<ol start="3">
<li><strong>픽셀 단위 예측 계층 추가</strong>: 마지막으로, 분할하고자 하는 클래스의 수(<code>C</code>)만큼의 필터를 가진 <code>1x1</code> 컨볼루션 계층을 추가한다. 예를 들어, PASCAL VOC 데이터셋(배경 포함 21개 클래스)의 경우, 21개의 필터를 가진 <code>1x1</code> 컨볼루션 계층이 추가되어 각 픽셀 위치에서 21개 클래스에 대한 점수(score)를 출력하는 히트맵을 생성한다.6</li>
</ol>
<h3>2.3  임의 크기 입력 처리와 히트맵 생성의 효율성</h3>
<p>컨볼루션화의 가장 큰 장점은 네트워크가 임의 크기의 입력 이미지를 처리할 수 있게 된다는 점이다.6 모든 연산이 위치에 무관하게 적용되는 컨볼루션으로 구성되어 있으므로, 더 이상 고정된 입력 크기에 제약을 받지 않는다.</p>
<p>고정된 크기의 이미지를 입력했을 때 <code>1x1xC</code> 크기의 벡터를 출력하던 네트워크가, 더 큰 이미지를 입력받으면 공간적 차원을 가진 <code>W'xH'xC</code> 크기의 히트맵을 출력하게 된다.6 이 히트맵의 각 셀 (i, j)는 원본 이미지의 특정 수용 영역(receptive field)에 대한 예측 결과를 담고 있다.6</p>
<p>이 방식은 계산 효율성을 극적으로 향상시킨다. 수백만 개의 중첩된 패치에 대해 독립적으로 네트워크를 실행하는 대신, 단 한 번의 순전파(forward pass)로 이미지 전체에 대한 예측을 동시에 계산한다. 중첩되는 영역에 대한 계산이 공유되기 때문에, 추론 속도는 기존 패치 기반 방식에 비해 수십 배에서 수백 배까지 빨라진다.6 이 효율성 덕분에 시맨틱 분할의 실시간 적용 가능성이 열리게 되었다.</p>
<h2>3.  공간 정보 복원을 위한 업샘플링: 전치 컨볼루션 (Transposed Convolution)</h2>
<p>FCN의 인코더(encoder) 부분은 VGG와 같은 분류 네트워크를 기반으로 하여, 반복적인 풀링(pooling) 연산을 통해 특징 맵의 공간 해상도를 점차 줄여나간다. 이 과정은 더 넓은 문맥 정보를 포착하고 의미론적 특징을 추출하는 데 필수적이지만, 최종적으로는 원본 이미지보다 훨씬 작은(예: 32배 축소된) 특징 맵을 생성한다.7 픽셀 단위의 조밀한 예측을 위해서는 이 거친(coarse) 특징 맵을 다시 원본 이미지 크기로 확대하는 업샘플링(upsampling) 과정이 필요하다.1 FCN은 이 문제를 해결하기 위해 ’전치 컨볼루션(Transposed Convolution)’이라는 학습 가능한 업샘플링 기법을 도입했다.</p>
<h3>3.1  다운샘플링의 문제: 공간 해상도 손실</h3>
<p>CNN의 컨볼루션과 풀링 계층은 특징을 추출하고 계산량을 줄이기 위해 입력의 공간적 차원을 축소한다. 특히 최대 풀링(max-pooling)과 같은 연산은 지역적 특징의 위치 정보를 일부 소실시키며, 이 과정은 비가역적이다. 따라서 인코더의 마지막 계층에 남은 특징 맵은 ’무엇’에 대한 정보는 풍부하지만 ’어디’에 대한 정보는 매우 희박한 상태가 된다. 단순한 보간법(interpolation)만으로는 이 손실된 세부 정보를 완벽하게 복원하기 어렵다.</p>
<h3>3.2  전치 컨볼루션의 작동 원리</h3>
<p>전치 컨볼루션은 종종 ’디컨볼루션(deconvolution)’으로 잘못 불리기도 하지만, 수학적으로 엄밀한 의미의 역컨볼루션과는 다르다.20 이는 작은 크기의 특징 맵을 입력받아 더 큰 크기의 특징 맵을 출력하는, 학습 가능한 업샘플링 연산이다.1</p>
<p>전치 컨볼루션의 작동 원리는 일반적인 컨볼루션의 연결 관계를 뒤집는 것으로 시각화할 수 있다. 일반 컨볼루션이 입력의 여러 픽셀(‘many’)을 하나의 출력 픽셀(‘one’)로 매핑한다면, 전치 컨볼루션은 입력의 한 픽셀(‘one’)을 출력의 여러 픽셀(‘many’)에 영향을 미치도록 매핑한다. 구체적인 연산 과정은 다음과 같다 22:</p>
<ol>
<li>입력 특징 맵의 각 픽셀 값을 컨볼루션 커널 전체와 스칼라 곱셈한다.</li>
<li>결과로 생성된 작은 행렬들을 출력 그리드 상에 특정 보폭(stride)에 맞춰 배치한다.</li>
<li>배치된 행렬들이 겹치는 부분은 해당 위치의 값들을 모두 더하여 최종 출력 값을 결정한다.</li>
</ol>
<p>이러한 ‘one-to-many’ 방식의 연산을 통해 입력보다 공간적으로 더 큰 출력 특징 맵이 생성된다. ’전치(transposed)’라는 이름은 이 연산을 행렬 곱셈으로 표현했을 때, 일반 컨볼루션의 순전파에 사용되는 행렬의 전치 행렬(<code>C^T</code>)을 사용하는 것과 같기 때문에 붙여졌다. 이는 일반 컨볼루션의 역전파(backpropagation) 과정에서 기울기를 계산하는 방식과 동일하다.22</p>
<h3>3.3  학습 가능한 업샘플링의 장점</h3>
<p>최근접 이웃 보간법(nearest-neighbor interpolation)이나 쌍선형 보간법(bilinear interpolation)과 같은 고정된 업샘플링 방법과 비교했을 때, 전치 컨볼루션의 가장 큰 장점은 <strong>커널의 가중치가 학습 가능한 파라미터</strong>라는 점이다.3</p>
<p>이는 네트워크가 주어진 데이터와 과제에 가장 적합한 업샘플링 방식을 스스로 학습할 수 있음을 의미한다. 고정된 보간법이 주변 픽셀 값을 단순히 평균 내거나 복사하는 데 그치는 반면, 전치 컨볼루션은 다운샘플링 과정에서 손실된 세부적인 경계나 질감 정보를 더 지능적으로 복원하는 방법을 학습할 수 있다. FCN 논문에서는 실제로 전치 컨볼루션 계층의 가중치를 쌍선형 보간법으로 초기화한 후, 전체 네트워크 학습 과정에서 미세 조정(fine-tuning)되도록 하여 성능을 향상시켰다.26</p>
<p>이러한 학습 가능한 업샘플링 방식의 채택은 FCN의 종단 간 학습 철학을 강화하는 중요한 설계 결정이었다. 업샘플링을 단순한 후처리 단계가 아닌, 특징 학습 계층 구조의 일부로 취급함으로써, FCN은 인코더-디코더 구조 전체가 유기적으로 학습되는 새로운 아키텍처의 표준을 제시했다. 이 아이디어는 이후 U-Net과 같은 대칭적 인코더-디코더 모델과, 저차원 잠재 공간에서 복잡한 데이터를 생성하는 생성 모델(Generative Models)의 학습 가능한 디코더 설계에 지대한 영향을 미쳤다.</p>
<h2>4.  정밀도 향상을 위한 다중 스케일 융합: 스킵 아키텍처 (Skip Architecture)</h2>
<p>단순히 인코더의 마지막 계층만을 업샘플링하는 방식은 의미론적으로는 정확할 수 있으나, 심각한 공간 정보 손실로 인해 객체의 경계가 매우 흐릿하게 나타나는 문제를 야기한다. FCN은 이 문제를 해결하기 위해 네트워크의 역사에 길이 남을 중요한 기여 중 하나인 ’스킵 아키텍처(skip architecture)’를 제안했다.</p>
<h3>4.1  의미론(Semantics)과 위치(Location)의 상충 관계</h3>
<p>깊은 CNN 아키텍처의 본질적인 특성은 계층이 깊어질수록 의미론적 정보(‘what’)는 풍부해지지만, 공간적 해상도(‘where’)는 급격히 낮아진다는 점이다.4 VGG와 같은 네트워크의 마지막 컨볼루션 계층은 고양이의 존재 여부는 잘 알지만, 고양이의 정확한 윤곽선 정보는 거의 잃어버린 상태다. 반면, 초기 계층들은 색상, 에지, 질감과 같은 저수준의 시각적 특징을 높은 해상도로 유지하고 있지만, 이것이 고양이의 일부인지 아닌지를 판단할 의미론적 문맥은 부족하다.</p>
<p>가장 깊은 계층의 특징 맵만을 업샘플링하는 FCN-32s 모델의 결과물이 의미적으로는 옳지만 경계가 뭉툭하게 나타나는 것은 바로 이 때문이다.8 정교한 분할을 위해서는 이 두 종류의 정보를 효과적으로 결합할 방법이 필요했다.</p>
<h3>4.2  스킵 연결의 도입과 작동 방식</h3>
<p>스킵 연결(skip connection)은 이러한 상충 관계를 해결하기 위한 FCN의 핵심적인 해법이다.10 이는 ResNet의 짧은 잔차 연결(short skip connection)과는 구별되는, 인코더와 디코더를 가로지르는 긴 연결(long skip connection)이다.29</p>
<p>스킵 아키텍처의 작동 방식은 깊은 계층의 거친 의미 정보(coarse semantic information)와 얕은 계층의 정교한 외형 정보(fine appearance information)를 융합하는 것이다.9 구체적인 과정은 다음과 같다.</p>
<ol>
<li>깊은 계층(예: <code>conv7</code>의 출력)의 특징 맵을 전치 컨볼루션을 통해 2배 업샘플링한다.</li>
<li>더 얕은 계층(예: <code>pool4</code>)에서 나온, 공간 해상도가 더 높은 특징 맵을 가져온다.</li>
<li>이 두 특징 맵의 공간적 차원을 맞춘 후, 픽셀별 덧셈(element-wise summation)을 통해 융합한다.19</li>
<li>융합된 새로운 특징 맵을 다시 업샘플링하여 최종 예측에 사용한다.</li>
</ol>
<h3>4.3  분할 경계 정교화에 미치는 영향</h3>
<p>스킵 연결을 통해 얕은 계층의 고해상도 특징 맵이 디코더에 직접 전달됨으로써, 디코더는 깊은 계층의 의미론적 예측을 기반으로 객체의 경계를 정교하게 다듬을 수 있는 위치 정보를 얻게 된다.28 즉, 깊은 계층이 “여기에 자동차가 있다“고 알려주면, 얕은 계층은 “자동차의 윤곽선은 여기에 있다“는 세부 정보를 제공하는 방식이다.</p>
<p>이러한 다중 스케일 정보의 융합은 ’what’과 ’where’의 긴장 관계를 직접적으로 해소하며, 분할 결과의 정확도를 극적으로 향상시킨다. 스킵 연결이 없는 모델과 비교했을 때, 스킵 연결을 추가한 모델의 출력은 객체의 경계가 훨씬 더 선명하고 정확하게 표현된다.19</p>
<p>스킵 연결의 도입은 순차적인 계층 처리 모델만으로는 고수준의 추상화와 저수준의 정밀도를 동시에 달성하기 어렵다는 점을 인정한 것이다. 이는 풀링과 같은 연산이 비가역적이며, 한 번 손실된 공간 정보는 아무리 강력한 디코더를 사용하더라도 완벽히 복원될 수 없음을 시사한다. 스킵 연결은 이 손실된 정보가 깊은 계층을 우회하여 디코더로 직접 전달되는 ‘지름길’ 역할을 한다. 이 다중 스케일 융합 원칙은 FCN의 가장 영향력 있는 유산 중 하나로, U-Net의 대칭적 구조, 객체 탐지에서의 특징 피라미드 네트워크(FPN), 그리고 모든 계층을 연결하는 DenseNet과 같은 후속 연구들의 핵심적인 설계 철학으로 자리 잡았다.29</p>
<h2>5.  FCN 아키텍처의 진화: FCN-32s, 16s, 8s</h2>
<p>FCN 논문은 스킵 아키텍처의 효과를 입증하기 위해 점진적으로 정교해지는 세 가지 변형 모델(FCN-32s, FCN-16s, FCN-8s)을 제시했다. 이 모델들은 스킵 연결을 어떻게, 그리고 얼마나 많이 활용하는지에 따라 구조와 성능에서 차이를 보인다. 이들의 비교 분석은 다중 스케일 정보 융합의 중요성을 명확하게 보여주는 사례 연구라 할 수 있다.</p>
<h3>5.1  FCN-32s: 기본 단일 스트림 아키텍처</h3>
<ul>
<li><strong>구조</strong>: FCN-32s는 가장 단순한 형태의 FCN이다. VGG-16과 같은 백본 네트워크의 마지막 컨볼루션 계층(원본 이미지 대비 32배 다운샘플링됨)에서 나온 특징 맵에 <code>1x1</code> 컨볼루션을 적용하여 클래스 점수를 계산한다. 그 후, 단 한 번의 32배 업샘플링(전치 컨볼루션)을 통해 최종 분할 맵을 생성한다.19</li>
<li><strong>특징</strong>: 이 모델은 스킵 연결을 전혀 사용하지 않으며, 오직 가장 깊은 계층의 정보만을 활용한다. 이 때문에 ‘단일 스트림(single stream)’ 아키텍처라고도 불린다.7</li>
<li><strong>한계</strong>: 가장 추상적이고 의미론적인 정보만을 사용하기 때문에, 생성된 분할 맵은 공간적으로 가장 거칠고 객체의 경계가 매우 불분명하다.12</li>
</ul>
<h3>5.2  FCN-16s: 첫 번째 스킵 연결의 도입</h3>
<ul>
<li><strong>구조</strong>: FCN-16s는 FCN-32s의 한계를 개선하기 위해 하나의 스킵 연결을 추가한 모델이다.</li>
</ul>
<ol>
<li>마지막 계층(stride 32)의 출력을 2배 업샘플링하여 stride 16 수준의 특징 맵을 만든다.</li>
<li>이와 별도로, 백본 네트워크의 <code>pool4</code> 계층(stride 16)에서 나온 특징 맵에 <code>1x1</code> 컨볼루션을 적용하여 동일한 stride 수준의 클래스 예측을 생성한다.</li>
<li>이 두 개의 stride 16 특징 맵을 픽셀별 덧셈으로 융합한다.</li>
<li>최종적으로 융합된 특징 맵을 16배 업샘플링하여 원본 이미지 크기로 복원한다.19</li>
</ol>
<ul>
<li><strong>특징</strong>: ‘두 개 스트림(two stream)’ 아키텍처로, 깊은 계층의 의미 정보와 <code>pool4</code> 계층의 비교적 정교한 공간 정보를 결합한다.7</li>
<li><strong>성능</strong>: <code>pool4</code> 계층의 정보를 활용함으로써 FCN-32s에 비해 훨씬 더 세밀하고 선명한 객체 경계를 예측할 수 있다.19</li>
</ul>
<h3>5.3  FCN-8s: 다중 스킵 연결을 통한 정밀도 극대화</h3>
<ul>
<li><strong>구조</strong>: FCN-8s는 가장 정교하고 성능이 우수한 모델로, 두 개의 스킵 연결을 사용한다.</li>
</ul>
<ol>
<li>FCN-16s에서 융합된 stride 16 특징 맵을 다시 2배 업샘플링하여 stride 8 수준으로 만든다.</li>
<li>백본 네트워크의 <code>pool3</code> 계층(stride 8)에서 나온 특징 맵에 <code>1x1</code> 컨볼루션을 적용한다.</li>
<li>이 두 개의 stride 8 특징 맵을 융합한다.</li>
<li>최종 융합된 특징 맵을 8배 업샘플링하여 최종 분할 맵을 생성한다.12</li>
</ol>
<ul>
<li><strong>특징</strong>: ‘세 개 스트림(three stream)’ 아키텍처로, 가장 깊은 계층, 중간 계층(<code>pool4</code>), 그리고 더 얕은 계층(<code>pool3</code>)의 정보를 순차적으로 융합한다.7</li>
<li><strong>성능</strong>: 가장 얕은 계층인 <code>pool3</code>의 정밀한 공간 정보까지 활용하므로, 세 모델 중 가장 정확한 경계 예측 성능을 보여준다.19</li>
</ul>
<table><thead><tr><th>모델 (Model)</th><th>예측 보폭 (Stride)</th><th>업샘플링 전략</th><th>사용된 스킵 연결</th><th>정보 소스</th><th>상대적 성능 (경계 정밀도)</th></tr></thead><tbody>
<tr><td><strong>FCN-32s</strong></td><td>32</td><td>단일 단계 32배 업샘플링</td><td>없음</td><td><code>conv7</code></td><td>가장 거침 (Coarsest)</td></tr>
<tr><td><strong>FCN-16s</strong></td><td>16</td><td>2단계 업샘플링 (2x, 16x)</td><td><code>pool4</code></td><td><code>conv7</code> + <code>pool4</code></td><td>중간 (Finer)</td></tr>
<tr><td><strong>FCN-8s</strong></td><td>8</td><td>3단계 업샘플링 (2x, 2x, 8x)</td><td><code>pool4</code>, <code>pool3</code></td><td><code>conv7</code> + <code>pool4</code> + <code>pool3</code></td><td>가장 정교함 (Finest)</td></tr>
</tbody></table>
<p><strong>표 2: FCN 변형 모델(32s, 16s, 8s)의 구조 및 성능 비교</strong></p>
<p>이러한 점진적인 개선 과정은 스킵 연결이 단순히 부가적인 장치가 아니라, FCN 아키텍처의 성능을 결정짓는 핵심 요소임을 명확히 보여준다.</p>
<h2>6.  종단 간 학습과 픽셀 단위 손실 함수</h2>
<p>FCN의 성공은 아키텍처의 혁신뿐만 아니라, 이를 효과적으로 학습시킬 수 있는 훈련 방법론에도 기인한다. FCN은 이미지 전체를 한 번에 처리하고, 모든 픽셀에 대해 손실(loss)을 계산하여 종단 간 학습을 수행한다.</p>
<h3>6.1  전체 이미지 학습의 효율성</h3>
<p>FCN은 패치 단위가 아닌 전체 이미지를 입력으로 받아 학습을 진행한다. 이는 계산적으로 매우 효율적인데, 그 이유는 손실 함수의 특성에서 찾을 수 있다. FCN의 손실 함수는 최종 출력 맵의 모든 공간적 위치(모든 픽셀)에 대한 손실의 총합으로 정의된다.6 수학적으로 표현하면 다음과 같다.<br />
<span class="math math-display">
L(x; \theta) = \sum_{i,j} l(x_{ij}; \theta)
</span><br />
여기서 <span class="math math-inline">L(x; \theta)</span>는 이미지 전체에 대한 총 손실, <span class="math math-inline">l(x_{ij}; \theta)</span>는 위치 (i, j)에서의 픽셀 단위 손실을 의미한다. 총 손실의 기울기(gradient)는 각 픽셀 단위 손실의 기울기의 합과 같다. 따라서 확률적 경사 하강법(SGD)을 사용하여 전체 이미지에 대한 손실을 최소화하는 것은, 출력 맵의 모든 픽셀 위치에 해당하는 수용 영역들을 하나의 거대한 미니배치(minibatch)로 취급하여 학습하는 것과 동일한 효과를 낸다.6 이는 중첩된 수용 영역에 대한 순전파 및 역전파 계산을 공유하게 하여 학습 과정을 극적으로 가속화한다.</p>
<h3>6.2  픽셀 단위 교차 엔트로피 손실 함수</h3>
<p>FCN에서 사용하는 구체적인 손실 함수는 ’픽셀 단위 다항 로지스틱 손실(per-pixel multinomial logistic loss)’이며, 이는 일반적으로 ‘픽셀 단위 교차 엔트로피(pixel-wise cross-entropy)’ 손실과 동일하다.6 다중 클래스 분류 문제에 사용되는 표준적인 교차 엔트로피 손실을 각 픽셀에 독립적으로 적용하는 것이다. 수식은 다음과 같다.35<br />
<span class="math math-display">
L_{ce} = - \frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{ic} \log(p_{ic})
</span><br />
각 항의 의미는 다음과 같다.</p>
<ul>
<li><span class="math math-inline">N</span>: 이미지의 전체 픽셀 수.</li>
<li><span class="math math-inline">C</span>: 분할할 전체 클래스의 수 (배경 포함).</li>
<li><span class="math math-inline">y_{ic}</span>: 이진 지시자(binary indicator). 픽셀 <span class="math math-inline">i</span>의 실제 정답 클래스가 <span class="math math-inline">c</span>이면 1, 아니면 0의 값을 가진다.</li>
<li><span class="math math-inline">p_{ic}</span>: 네트워크의 최종 소프트맥스(softmax) 계층에서 출력된 확률. 픽셀 <span class="math math-inline">i</span>가 클래스 <span class="math math-inline">c</span>에 속할 것으로 예측된 확률을 의미한다.</li>
</ul>
<p>이 손실 함수는 각 픽셀의 예측이 정답과 얼마나 다른지를 독립적으로 측정하고 페널티를 부과한다. 네트워크는 이 손실을 최소화하는 과정에서 이미지의 모든 위치에 대해 올바른 클래스 레이블을 예측하도록 학습된다.</p>
<p>이처럼 단순하고 합산 가능한 픽셀 단위 손실 함수의 선택은 FCN 아키텍처의 철학과 일치한다. 이는 각 픽셀을 독립적인 분류 문제로 취급함으로써 병렬 처리와 계산 효율성을 극대화했다. 그러나 바로 이 점이 FCN의 근본적인 한계로 이어진다. 이 손실 함수는 픽셀 간의 관계나 구조적 일관성을 전혀 고려하지 않는다. 예를 들어, ‘자동차’ 클래스 한가운데에 ‘하늘’ 클래스로 예측된 픽셀이 하나 있더라도, 다른 픽셀들이 모두 정확하다면 전체 손실에 미치는 영향은 미미하다. 이 때문에 FCN의 초기 출력은 종종 공간적으로 일관되지 않은 노이즈를 포함했으며, CRF와 같은 후처리가 여전히 유용하게 사용되었다. 이러한 한계는 이후 연구에서 Dice 손실(클래스 불균형 문제 완화), Lovasz-Softmax 손실(IoU 직접 최적화), 경계 손실(Boundary Loss) 등 픽셀 간의 관계를 고려하는 더 정교한 손실 함수들이 개발되는 직접적인 동기가 되었다.36</p>
<h2>7.  FCN의 한계와 유산: 후속 연구로의 확장</h2>
<p>FCN은 시맨틱 분할 분야에 혁명을 일으켰지만, 완벽한 해결책은 아니었다. FCN이 남긴 몇 가지 명확한 한계점들은 역설적으로 후속 연구들이 나아갈 방향을 제시하는 이정표가 되었다. U-Net, SegNet, DeepLab과 같은 차세대 분할 모델들은 FCN의 유산을 계승하면서도 그 한계를 극복하기 위한 독창적인 아이디어들을 제시하며 분야를 한 단계 더 발전시켰다.</p>
<h3>7.1  FCN의 내재적 한계</h3>
<ol>
<li><strong>흐릿한 경계 (Blurry Boundaries)</strong>: FCN의 가장 큰 한계는 분할 경계가 흐릿하고 정교하지 못하다는 점이다. 이는 인코더의 반복적인 풀링 연산으로 인한 극심한 공간 해상도 손실에 근본적인 원인이 있다. 스킵 연결을 통해 일부 정보가 보완되기는 하지만, 여전히 매우 거친 특징 맵으로부터 최종 예측을 복원해야 하므로 미세한 경계 디테일을 살리는 데에는 한계가 명확했다.8</li>
<li><strong>인스턴스 미인식 (Instance-Unaware)</strong>: FCN은 시맨틱 분할 모델로서, 픽셀을 오직 클래스(‘자동차’)로만 분류할 뿐, 개별 객체(‘1번 자동차’, ‘2번 자동차’)를 구분하지 못한다. 따라서 이미지에 동일한 클래스의 객체 두 개가 인접해 있을 경우, FCN은 이를 하나의 거대한 ‘자동차’ 덩어리로 분할한다.39 이는 시맨틱 분할이라는 과제 자체의 정의에 따른 한계이기도 하다.</li>
<li><strong>작은 객체 분할의 어려움</strong>: 심한 다운샘플링 비율(예: 32배)은 이미지 내의 작은 객체들이 깊은 계층의 특징 맵에서 완전히 사라지게 만들 수 있다. 정보 자체가 소실되므로, 네트워크가 이러한 작은 객체들을 올바르게 분할하는 것은 거의 불가능에 가깝다.37</li>
</ol>
<h3>7.2  FCN의 유산과 후속 모델의 혁신</h3>
<p>FCN의 한계점들은 다음 세대 모델들의 핵심 혁신과 직접적으로 연결된다.</p>
<ul>
<li>
<p><strong>U-Net</strong>: 흐릿한 경계와 정보 손실 문제를 해결하는 데 집중했다.</p>
</li>
<li>
<p><strong>혁신</strong>: U-Net은 FCN의 인코더-디코더 구조를 완벽한 <strong>대칭 형태</strong>로 발전시키고, <strong>훨씬 더 많은 스킵 연결</strong>을 도입했다.32 결정적으로, U-Net의 스킵 연결은 픽셀별 덧셈이 아닌</p>
</li>
</ul>
<p><strong>채널별 결합(concatenation)</strong> 방식을 사용한다. 이는 얕은 계층의 특징 맵을 손실 없이 그대로 디코더에 전달하여 훨씬 풍부한 정보를 제공하며, 이를 통해 매우 정교한 경계 복원이 가능해졌다. 특히 적은 양의 데이터로도 높은 성능을 보여 의료 영상 분석 분야에서 표준 모델로 자리 잡았다.41</p>
<ul>
<li>
<p><strong>SegNet</strong>: 업샘플링 과정의 효율성과 정밀도를 개선하는 데 초점을 맞췄다.</p>
</li>
<li>
<p><strong>혁신</strong>: SegNet은 디코더에서 업샘플링을 수행할 때, 해당 인코더 계층의 **최대 풀링 인덱스(max-pooling indices)**를 재사용하는 독창적인 방식을 제안했다.43 이는 특징 값을 원래의 위치에 정확히 되돌려 놓는 비선형 업샘플링 방식으로, 경계와 같은 고주파수 디테일을 보존하는 데 매우 효과적이다. 또한, 스킵 연결처럼 전체 특징 맵을 전달하는 방식보다 메모리 측면에서 훨씬 효율적이라는 장점이 있다.43</p>
</li>
<li>
<p><strong>DeepLab</strong>: 정보 손실의 근본 원인인 풀링 자체를 재고했다.</p>
</li>
<li>
<p><strong>혁신</strong>: DeepLab은 <strong>‘Atrous 컨볼루션(Atrous Convolution)’</strong> 또는 ’Dilated Convolution’이라 불리는 기술을 핵심적으로 사용했다.46 Atrous 컨볼루션은 커널 내부에 간격(hole)을 두어, 파라미터 수를 늘리지 않으면서도 수용 영역을 확장하는 기술이다. DeepLab은 후반부의 풀링 계층을 Atrous 컨볼루션으로 대체함으로써, 공간 해상도를 거의 줄이지 않고도 깊은 계층에서 넓은 문맥 정보를 포착할 수 있게 했다. 이는 FCN처럼 손실된 정보를 나중에 복원하려는 시도가 아닌, 애초에 정보 손실을 최소화하는 보다 근본적인 해결책을 제시한 것이다.48</p>
</li>
</ul>
<table><thead><tr><th>모델 (Model)</th><th>해결하고자 한 FCN 한계</th><th>핵심 혁신</th><th>업샘플링 방식</th><th>스킵 연결 전략</th><th>주요 장점</th></tr></thead><tbody>
<tr><td><strong>FCN (Baseline)</strong></td><td>-</td><td>완전 컨볼루션화, 스킵 아키텍처</td><td>전치 컨볼루션</td><td>픽셀별 덧셈</td><td>종단 간 학습, 임의 크기 입력</td></tr>
<tr><td><strong>U-Net</strong></td><td>흐릿한 경계, 정보 손실</td><td>대칭적 인코더-디코더</td><td>전치 컨볼루션</td><td>채널별 결합</td><td>정교한 경계 복원, 소량 데이터 강점</td></tr>
<tr><td><strong>SegNet</strong></td><td>업샘플링 정밀도, 메모리 효율</td><td>풀링 인덱스 활용</td><td>풀링 인덱스 기반 업샘플링</td><td>없음 (인덱스 전송)</td><td>메모리 효율적, 경계 디테일 보존</td></tr>
<tr><td><strong>DeepLab</strong></td><td>공간 해상도 손실, 제한된 문맥</td><td>Atrous 컨볼루션, ASPP</td><td>쌍선형 보간법</td><td>없음 (고해상도 특징 유지)</td><td>해상도 유지하며 수용 영역 확장</td></tr>
</tbody></table>
<p><strong>표 3: FCN과 주요 후속 모델의 혁신 비교</strong></p>
<p>이처럼 FCN은 단순히 하나의 성공적인 모델을 넘어, 후속 연구들이 해결해야 할 명확한 문제들을 정의하고 다양한 해결책을 모색하게 만든 ’문제 제기자’로서의 역할을 수행했다. 현대 시맨틱 분할 모델들은 모두 FCN이 닦아놓은 길 위에서 그 한계를 넘어서기 위한 노력의 결실이라 할 수 있다.</p>
<h2>8.  결론: 시맨틱 분할의 초석으로서의 FCN</h2>
<p>완전 컨볼루션 네트워크(FCN)는 시맨틱 분할 분야의 역사에서 단순한 하나의 모델이 아닌, 연구의 방향 자체를 바꾼 분수령으로 기록된다. FCN의 등장은 파편화되고 비효율적이었던 기존의 접근법에 종언을 고하고, 딥러닝 기반 밀집 예측 문제 해결의 현대적 기틀을 마련했다.</p>
<h3>8.1  FCN의 핵심 기여 요약</h3>
<p>FCN의 기여는 세 가지 핵심 혁신으로 요약할 수 있다.</p>
<ol>
<li><strong>종단 간, 픽셀 대 픽셀 학습 패러다임의 확립</strong>: FCN은 복잡한 전후처리 파이프라인을 제거하고, 이미지 입력부터 최종 분할 맵 출력까지 전체 과정을 단일 네트워크 안에서 미분 가능하게 만들어 종단 간 학습을 실현했다. 이는 분할 과제에 최적화된 특징 표현을 데이터로부터 직접 학습하는 시대를 열었다.7</li>
<li><strong>분류 네트워크의 완전 컨볼루션화</strong>: 기존 분류용 CNN의 완전 연결 계층을 컨볼루션 계층으로 대체함으로써, 임의 크기의 입력을 처리하고 공간 정보를 보존하는 조밀한 예측을 가능하게 했다. 이는 사전 학습된 강력한 분류 모델을 분할 과제에 효과적으로 전이 학습할 수 있는 길을 열었다.9</li>
<li><strong>스킵 아키텍처를 통한 다중 스케일 융합</strong>: 깊은 계층의 의미론적 정보와 얕은 계층의 공간적 정보를 결합하는 스킵 연결을 도입하여, ’무엇’과 ’어디’의 상충 관계를 해결하고 분할의 정밀도를 크게 향상시켰다. 이 원리는 현대의 거의 모든 고성능 분할 모델에 계승되었다.7</li>
</ol>
<h3>8.2  현대 분할 아키텍처에 남겨진 FCN의 DNA</h3>
<p>FCN이 제시한 기본 청사진은 오늘날 시맨틱 분할 아키텍처의 근간을 이루고 있다. U-Net, SegNet, DeepLab을 비롯한 수많은 후속 모델들은 본질적으로 ‘완전 컨볼루션’ 네트워크이며, 특징을 추출하는 인코더와 공간 해상도를 복원하는 디코더, 그리고 이 둘을 잇는 스킵 연결의 변형된 형태를 채택하고 있다.50 FCN은 현대 분할 모델들이 사용하는 기본적인 ’문법’을 창시했으며, 이후의 연구들은 이 문법을 바탕으로 더 정교하고 효율적인 ’문장’을 만들어내는 과정이었다고 볼 수 있다.</p>
<h3>8.3  전망과 미래 방향</h3>
<p>물론, 시맨틱 분할 분야는 FCN의 시대를 넘어 트랜스포머(Transformer) 기반 모델이나 거대 언어 모델과 결합된 파운데이션 모델(Foundation Models)의 시대로 나아가고 있다.38 이러한 새로운 아키텍처들은 전역적인 문맥을 더 효과적으로 모델링하며 놀라운 성능을 보여주고 있다. 하지만 이들 모델조차도 종종 컨볼루션 네트워크를 백본으로 사용하거나, FCN이 개척한 다중 스케일 융합의 원리를 다른 형태로 구현하고 있다.</p>
<p>결론적으로, FCN은 시맨틱 분할을 딥러닝의 주류 문제로 끌어올린 선구자적 모델이다. FCN이 제시한 원칙과 구조는 수많은 후속 연구에 영감을 주었으며, 그 영향력은 기술의 구체적인 구현 방식이 변화하는 와중에도 여전히 분야의 저변에 깊이 뿌리내리고 있다. FCN은 시맨틱 분할이라는 거대한 건축물의 가장 단단한 초석으로 기억될 것이다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>13.11. Fully Convolutional Networks — Dive into Deep Learning 0.17.6 documentation, https://classic.d2l.ai/chapter_computer-vision/fcn.html</li>
<li>Loss Functions in the Era of Semantic Segmentation: A Survey and Outlook - arXiv, https://arxiv.org/html/2312.05391v1</li>
<li>Introduction to Semantic Segmentation using Convolutional Neural Networks - Machine Learning, http://machinelearning.math.rs/Radovic-SemanticSegmentation.pdf</li>
<li>Fully convolutional networks for semantic segmentation - IEEE Computer Society, https://www.computer.org/csdl/proceedings-article/cvpr/2015/07298965/12OmNy49sME</li>
<li>Convolutional neural network - Wikipedia, https://en.wikipedia.org/wiki/Convolutional_neural_network</li>
<li>Fully Convolutional Networks for Semantic Segmentation, http://arxiv.org/pdf/1411.4038</li>
<li>Role of Fully Convolutional Networks in Semantic Segmentation, https://www.analyticsvidhya.com/blog/2024/07/fully-convolutional-networks-for-semantic-segmentation/</li>
<li>Semantic Segmentation With Boundary Neural … - CVF Open Access, https://openaccess.thecvf.com/content_cvpr_2016/papers/Bertasius_Semantic_Segmentation_With_CVPR_2016_paper.pdf</li>
<li>[1411.4038] Fully Convolutional Networks for Semantic Segmentation - arXiv, https://arxiv.org/abs/1411.4038</li>
<li>[1605.06211] Fully Convolutional Networks for Semantic Segmentation - arXiv, https://arxiv.org/abs/1605.06211</li>
<li>What are Convolutional Neural Networks? - IBM, https://www.ibm.com/think/topics/convolutional-neural-networks</li>
<li>Fully Convolutional Networks | TJHSST Machine Learning Club, https://tjmachinelearning.com/lectures/1718/fcn/</li>
<li>Fully Convolutional Network for the Semantic Segmentation of Medical Images: A Survey, https://pmc.ncbi.nlm.nih.gov/articles/PMC9689961/</li>
<li>machine learning - What is a fully convolution network? - Artificial …, https://ai.stackexchange.com/questions/21810/what-is-a-fully-convolution-network</li>
<li>How are 1x1 convolutions the same as a fully connected layer?, https://datascience.stackexchange.com/questions/12830/how-are-1x1-convolutions-the-same-as-a-fully-connected-layer</li>
<li>How to convert fully connected layer into convolutional layer …, https://stats.stackexchange.com/questions/263349/how-to-convert-fully-connected-layer-into-convolutional-layer</li>
<li>How do fully convolutional networks fare against regular convolutional networks? - Reddit, https://www.reddit.com/r/MachineLearning/comments/4xpjb6/how_do_fully_convolutional_networks_fare_against/</li>
<li>Fully Convolutional Networks for Semantic Segmentation, https://cqf.io/EESM5900V/lectures/Lecture8.pdf</li>
<li>JustinHeaton/fully-convolutional-networks: Visualized the … - GitHub, https://github.com/JustinHeaton/fully-convolutional-networks</li>
<li>Upsampling and Transposed Convolutions Layers | by Surya Teja Menta | Medium, https://medium.com/@suryatejamenta/upsampler-and-convolution-transpose-layers-7e4346c05bb6</li>
<li>In CNN, are upsampling and transpose convolution the same? - Stats Stackexchange, https://stats.stackexchange.com/questions/252810/in-cnn-are-upsampling-and-transpose-convolution-the-same</li>
<li>14.10. Transposed Convolution — Dive into Deep Learning 1.0.3 documentation, https://d2l.ai/chapter_computer-vision/transposed-conv.html</li>
<li>Understand Transposed Convolutions | Towards Data Science, https://towardsdatascience.com/understand-transposed-convolutions-and-build-your-own-transposed-convolution-layer-from-scratch-4f5d97b2967/</li>
<li>Transposed Convolutions Explained: A Fast 8-Minute Explanation | Computer Vision, https://m.youtube.com/watch?v=xoAv6D05j7g&amp;pp=0gcJCccJAYcqIYzv</li>
<li>Transposed Convolutions for “smart” upsampling of images and arrays - Reddit, https://www.reddit.com/r/learnmachinelearning/comments/wuxgn4/transposed_convolutions_for_smart_upsampling_of/</li>
<li>FCN for Image Semantic Segmentation | MindSpore 2.2 Tutorials, https://www.mindspore.cn/tutorials/application/en/r2.2/cv/fcn8s.html</li>
<li>[1511.02674] Semantic Segmentation with Boundary Neural Fields - arXiv, https://arxiv.org/abs/1511.02674</li>
<li>The architecture of Skip-connection Convolutional Neural Network for… - ResearchGate, https://www.researchgate.net/figure/The-architecture-of-Skip-connection-Convolutional-Neural-Network-for-crowd-counting-In_fig4_323365231</li>
<li>What are Skip Connections in Deep Learning? - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2021/08/all-you-need-to-know-about-skip-connections/</li>
<li>Development of Skip Connection in Deep Neural Networks for Computer Vision and Medical Image Analysis: A Survey - arXiv, https://arxiv.org/html/2405.01725v1</li>
<li>The Importance of Skip Connections in Biomedical Image Segmentation - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/1608.04117</li>
<li>Can you explain the difference between U-Net and FCN for image segmentation?, <a href="https://massedcompute.com/faq-answers/?question=Can+you+explain+the+difference+between+U-Net+and+FCN+for+image+segmentation?">https://massedcompute.com/faq-answers/?question=Can+you+explain+the+difference+between+U-Net+and+FCN+for+image+segmentation%3F</a></li>
<li>Fully Convolutional Networks for Semantic Segmentation by Jonathan Long*, Evan Shelhamer*, and Trevor Darrell. CVPR 2015 and PAMI 2016. - GitHub, https://github.com/shelhamer/fcn.berkeleyvision.org</li>
<li>A comparative study on fully convolutional networks—FCN-8, FCN-16, and FCN-32 | Request PDF - ResearchGate, https://www.researchgate.net/publication/358702473_A_comparative_study_on_fully_convolutional_networks-FCN-8_FCN-16_and_FCN-32</li>
<li>Understanding Loss Functions for Deep Learning Segmentation …, https://medium.com/@devanshipratiher/understanding-loss-functions-for-deep-learning-segmentation-models-30187836b30a</li>
<li>Instance segmentation loss functions - SoftwareMill, https://softwaremill.com/instance-segmentation-loss-functions/</li>
<li>Examples of problem in instance segmentation. In the first line, a… - ResearchGate, https://www.researchgate.net/figure/Examples-of-problem-in-instance-segmentation-In-the-first-line-a-large-number-of_fig1_353655257</li>
<li>A Deep Learning Framework for Boundary-Aware Semantic Segmentation - arXiv, https://arxiv.org/pdf/2503.22050</li>
<li>Guide to Image Segmentation in Computer Vision: Best Practices - Encord, https://encord.com/blog/image-segmentation-for-computer-vision-best-practice-guide/</li>
<li>Supporting Fully Convolutional Networks (and U-Net) for Image …, https://datature.io/blog/supporting-fully-convolutional-networks-and-u-net-for-image-segmentation</li>
<li>FCN and U-Net | Adil’s Notes, https://adilsarsenov.dev/posts/fcn_unet/</li>
<li>Comparative Performance Analysis of Deep Convolutional Neural Network for Gastrointestinal Polyp Image Segmentation - IJISET, https://ijiset.com/vol8/v8s4/IJISET_V8_I04_17.pdf</li>
<li>SegNet: A Deep Convolutional Encoder-Decoder Architecture for …, https://www.geeksforgeeks.org/computer-vision/segnet-a-deep-convolutional-encoder-decoder-architecture-for-image-segmentation/</li>
<li>[1511.00561] SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation - arXiv, https://arxiv.org/abs/1511.00561</li>
<li>FEATURE EXTRACTION FROM SATELLITE IMAGES USING SEGNET AND FULLY CONVOLUTIONAL NETWORKS (FCN) - TUFUAB, https://www.tufuab.org.tr/uploads/files/articles/feature-extraction-from-satellite-images-using-segnet-and-fully-convolutional-networks-fcn-2193.pdf</li>
<li>Comparative Analysis of Popular Semantic Segmentation Architectures: U-Net, DeepLab, and FCN - ResearchGate, https://www.researchgate.net/publication/394470887_Comparative_Analysis_of_Popular_Semantic_Segmentation_Architectures_U-Net_DeepLab_and_FCN</li>
<li>A Comparison and Strategy of Semantic Segmentation on Remote Sensing Images - ar5iv, https://ar5iv.labs.arxiv.org/html/1905.10231</li>
<li>A Comprehensive Guide on Atrous Convolution in CNNs, https://www.analyticsvidhya.com/blog/2023/12/a-comprehensive-guide-on-atrous-convolution-in-cnns/</li>
<li>DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs - Johns Hopkins Computer Science, https://www.cs.jhu.edu/~ayuille/JHUcourses/VisionAsBayesianInference2022/9/DeepLabJayChen.pdf</li>
<li>A review of semantic segmentation using deep neural networks - ResearchGate, https://www.researchgate.net/publication/321283063_A_review_of_semantic_segmentation_using_deep_neural_networks</li>
<li>SDFCNv2: An Improved FCN Framework for Remote Sensing Images Semantic Segmentation - MDPI, https://www.mdpi.com/2072-4292/13/23/4902</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>