<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:InceptionV3-LSTM 심층 신경망을 통한 시각적 특징 추출과 시퀀스 모델링</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>InceptionV3-LSTM 심층 신경망을 통한 시각적 특징 추출과 시퀀스 모델링</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">딥러닝 레이어 (Layers)</a> / <span>InceptionV3-LSTM 심층 신경망을 통한 시각적 특징 추출과 시퀀스 모델링</span></nav>
                </div>
            </header>
            <article>
                <h1>InceptionV3-LSTM 심층 신경망을 통한 시각적 특징 추출과 시퀀스 모델링</h1>
<p>2025-12-13, G30DR</p>
<h2>1.  서론</h2>
<p>인공지능 연구의 최전선에서 컴퓨터 비전(Computer Vision)과 자연어 처리(Natural Language Processing, NLP)의 융합은 기계가 세상을 ‘보고’ 동시에 ’이해’하여 언어로 표현하는 능력을 부여하는 핵심 과제로 자리 잡았다. 과거의 이미지 인식 기술이 단순히 사물의 종류를 분류(Classification)하거나 위치를 탐지(Detection)하는 데 그쳤다면, 현대의 멀티모달(Multi-modal) 학습 모델은 이미지 내의 객체 간 상호작용, 행위의 흐름, 그리고 전반적인 맥락(Context)을 파악하여 자연어 문장으로 서술하거나 비디오 데이터의 시간적 의미를 해석하는 단계로 진화하였다. 이러한 기술적 도약을 가능하게 한 중심에는 심층 합성곱 신경망(Deep Convolutional Neural Networks, CNN)과 순환 신경망(Recurrent Neural Networks, RNN)의 결합이 존재한다.</p>
<p>본 보고서에서는 시각적 데이터의 공간적 특징(Spatial Feature)을 추출하는 데 탁월한 성능을 보이는 <strong>InceptionV3</strong> 모델과, 시계열 데이터의 시간적 의존성(Temporal Dependency)을 학습하는 데 특화된 <strong>LSTM(Long Short-Term Memory)</strong> 네트워크를 결합한 <strong>InceptionV3-LSTM</strong> 하이브리드 아키텍처를 심층적으로 분석한다. 이 모델은 정지 이미지에 대한 캡션 생성(Image Captioning)부터 동영상 내의 행동 인식(Action Recognition), 농작물 생육 상태 예측, 의료 영상 분석에 이르기까지 광범위한 분야에서 높은 성과를 입증하였다.1</p>
<p>특히 본 연구 보고서는 단순한 모델의 구조적 결합을 넘어, Inception 모듈의 내부 작동 원리, LSTM의 게이트 메커니즘, 그리고 이 두 모델을 효과적으로 융합하기 위한 ‘병합(Merge)’ 및 ‘주입(Inject)’ 아키텍처의 차이점을 상세히 규명한다. 또한, 모델 학습 과정에서 수렴 속도를 높이기 위한 교사 강요(Teacher Forcing) 기법, 시공간 특징 학습을 위한 TimeDistributed 레이어의 활용, 그리고 모델 성능을 정량적으로 검증하기 위한 다양한 평가지표(BLEU, METEOR, ROUGE 등)에 대해 포괄적으로 다룬다. 이를 통해 InceptionV3-LSTM 모델이 현대 딥러닝 응용 분야에서 갖는 기술적 의의와 실질적인 구현 전략을 제시한다.</p>
<h2>2.  InceptionV3: 효율적인 공간 특징 추출기</h2>
<h3>2.1  Inception 아키텍처의 진화와 철학</h3>
<p>InceptionV3는 2014년 구글이 발표한 GoogLeNet(Inception-v1)의 설계를 계승하고 발전시킨 모델로, 심층 신경망의 연산 효율성과 정확도를 동시에 확보하기 위해 설계되었다. 당시 VGGNet과 같은 모델들이 레이어의 깊이(Depth)를 늘리는 데 집중하여 파라미터 수가 급격히 증가하고 연산 비용이 높아지는 문제를 겪었던 반면, Inception 아키텍처는 네트워크의 너비(Width)를 확장하는 전략을 취했다. 이는 하나의 레이어 내에서 다양한 크기(1x1, 3x3, 5x5)의 합성곱 필터를 병렬로 적용함으로써, 이미지 내의 다양한 스케일의 특징을 동시에 포착하는 ’Inception 모듈’을 통해 구현된다.4</p>
<p>InceptionV3의 가장 큰 특징은 <strong>합성곱의 분해(Factorization of Convolutions)</strong> 기법을 적극적으로 도입했다는 점이다. 큰 크기의 필터는 연산 비용이 높으므로, InceptionV3는 5x5 합성곱을 두 개의 연속된 3x3 합성곱으로 대체하거나, nxn 합성곱을 1xn과 nx1의 비대칭 합성곱(Asymmetric Convolutions)으로 분해하였다. 예를 들어, 7x7 합성곱을 1x7과 7x1 합성곱의 연속으로 처리함으로써, 수용 영역(Receptive Field)의 크기는 유지하면서도 파라미터 수와 연산량을 획기적으로 줄이는 데 성공하였다. 이러한 설계 덕분에 InceptionV3는 VGG나 ResNet 계열 모델에 비해 상대적으로 적은 메모리(약 96MB)를 사용하면서도 더 깊은 네트워크 구조를 가질 수 있게 되었다.6</p>
<h3>2.2  특징 추출(Feature Extraction) 메커니즘</h3>
<p>InceptionV3-LSTM 아키텍처에서 InceptionV3는 이미지를 분류하는 분류기(Classifier)가 아닌, 시각적 정보를 고차원 벡터로 압축하는 인코더(Encoder) 역할을 수행한다. 일반적으로 ImageNet 데이터셋(약 1,000개 클래스, 120만 개 이미지)으로 사전 학습된(Pre-trained) 가중치를 사용하며, 이를 전이 학습(Transfer Learning)의 형태로 활용한다.1</p>
<p>구체적인 특징 추출 과정은 다음과 같다. 먼저 입력 이미지는 모델이 요구하는 <code>299x299x3</code> (가로, 세로, RGB 채널) 크기로 전처리되고, 픽셀 값은 -1에서 1 사이로 정규화된다.7 이미지가 네트워크를 통과하면서 각 레이어는 점진적으로 추상화된 특징을 추출한다. 초기 레이어에서는 에지(Edge), 질감(Texture)과 같은 저수준 특징(Low-level Features)을, 깊은 레이어에서는 객체의 부분이나 전체 형태와 같은 고수준 특징(High-level Features)을 학습한다.2</p>
<p>캡셔닝이나 비디오 분류를 위해서는 모델의 최상단에 위치한 완전 연결 계층(Fully Connected Layer)과 Softmax 분류기를 제거하고, 마지막 합성곱 블록의 출력이나 <strong>전역 평균 풀링(Global Average Pooling, GAP)</strong> 레이어의 출력을 사용한다. GAP 레이어는 각 특징 맵(Feature Map)의 공간적 정보를 평균화하여 1차원 벡터로 변환하는데, InceptionV3의 경우 일반적으로 <code>2048</code> 차원의 벡터가 출력된다. 이 벡터는 이미지의 모든 시각적 정보를 압축적으로 담고 있는 ‘임베딩(Embedding)’ 역할을 하며, 이후 LSTM 네트워크의 입력으로 전달된다.7</p>
<table><thead><tr><th><strong>단계</strong></th><th><strong>레이어/블록 명칭</strong></th><th><strong>출력 텐서 형태 (Batch, H, W, Channels)</strong></th><th><strong>역할 및 특징</strong></th></tr></thead><tbody>
<tr><td><strong>입력</strong></td><td>Input Layer</td><td>(None, 299, 299, 3)</td><td>전처리된 RGB 이미지 수용</td></tr>
<tr><td><strong>특징 추출</strong></td><td>Conv2D &amp; Inception Blocks</td><td>(None, 8, 8, 2048)</td><td>다중 스케일 특징 및 비선형 패턴 학습</td></tr>
<tr><td><strong>압축</strong></td><td>Global Average Pooling</td><td>(None, 2048)</td><td>공간 차원 축소 및 특징 벡터화</td></tr>
<tr><td><strong>출력</strong></td><td>Feature Vector</td><td>(None, 2048)</td><td>LSTM에 전달될 최종 시각 임베딩</td></tr>
</tbody></table>
<p>이러한 특징 추출 방식은 비디오 분류 작업에서도 동일하게 적용되는데, 비디오의 경우 각 프레임마다 독립적으로 InceptionV3를 통과시켜 시퀀스 형태의 특징 벡터 <code>(Batch, Sequence_Length, 2048)</code>를 생성하게 된다. 연구에 따르면, InceptionV3의 특정 중간 블록(Block-02, Block-05)에서 추출한 저수준 및 중수준 특징을 함께 활용할 경우 스포츠 비디오 분류와 같이 역동적인 움직임이 많은 데이터에서 더 높은 정확도를 보인다는 결과도 보고되었다.2</p>
<h2>3.  LSTM: 시계열 데이터의 문맥과 흐름 학습</h2>
<h3>3.1  순환 신경망(RNN)의 한계 극복</h3>
<p>이미지 캡셔닝에서 단어의 순서를 결정하거나 비디오에서 행동의 흐름을 파악하기 위해서는 이전 정보를 기억하고 이를 현재의 판단에 활용할 수 있는 시퀀스 모델링 능력이 필수적이다. 기본적인 순환 신경망(RNN)은 이를 위해 은닉 상태(Hidden State)를 순환시키지만, 시퀀스의 길이가 길어질수록 초기 입력 정보가 희석되거나 역전파 과정에서 기울기가 사라지는 ‘기울기 소실(Vanishing Gradient)’ 문제에 취약하다. 이는 긴 문장이나 긴 비디오 클립을 처리하는 데 치명적인 단점으로 작용한다.4</p>
<p>**LSTM(Long Short-Term Memory)**은 이러한 단기 기억 문제를 해결하기 위해 고안된 특수한 형태의 RNN이다. LSTM의 핵심은 정보를 장기간 보존하거나 필요에 따라 삭제할 수 있는 **셀 상태(Cell State, <span class="math math-inline">C_t</span>)**라는 고속도로와 같은 통로를 별도로 두었다는 점이다. 이 셀 상태는 정보가 큰 변형 없이 흐를 수 있도록 보장하며, 세 가지 <strong>게이트(Gate)</strong> 구조를 통해 정보의 흐름을 정교하게 제어한다.7</p>
<h3>3.2  LSTM 게이트의 작동 원리 및 역할</h3>
<p>InceptionV3에서 추출된 시각 정보와 텍스트 시퀀스 정보는 LSTM 내부의 게이트를 통해 다음과 같이 처리된다.</p>
<ol>
<li><strong>망각 게이트(Forget Gate):</strong> “과거의 정보 중 무엇을 잊을 것인가?“를 결정한다. 이전 시점의 은닉 상태(<span class="math math-inline">h_{t-1}</span>)와 현재의 입력(<span class="math math-inline">x_t</span>)을 받아 시그모이드(Sigmoid) 함수를 통해 0과 1 사이의 값을 출력한다. 값이 0에 가까우면 해당 정보를 삭제하고, 1에 가까우면 유지한다. 예를 들어, 이미지 캡셔닝 중 문장의 주어가 ’남자’에서 ’여자’로 바뀌는 시점이 오면, 이전 주어에 대한 성별 정보를 잊어버리도록 작동할 수 있다.11</li>
<li><strong>입력 게이트(Input Gate):</strong> “새로운 정보 중 무엇을 저장할 것인가?“를 결정한다. 현재 입력(<span class="math math-inline">x_t</span>)과 이전 상태(<span class="math math-inline">h_{t-1}</span>)를 분석하여 셀 상태(<span class="math math-inline">C_t</span>)에 업데이트할 중요 정보를 선별한다. 비디오 분석에서 새로운 프레임에 중요한 동작(예: 공을 차는 순간)이 포착되면, 입력 게이트가 활성화되어 해당 정보를 셀 상태에 기록한다.11</li>
<li><strong>출력 게이트(Output Gate):</strong> “현재 상태를 바탕으로 무엇을 출력할 것인가?“를 결정한다. 갱신된 셀 상태(<span class="math math-inline">C_t</span>)를 바탕으로 현재 시점의 은닉 상태(<span class="math math-inline">h_t</span>)를 계산하며, 이 값은 다음 단어를 예측하는 Softmax 레이어로 전달되거나 다음 시점의 LSTM 셀로 전달된다.4</li>
</ol>
<p>이러한 메커니즘을 통해 LSTM은 “주어-동사-목적어“와 같은 언어의 장기적인 문법적 의존성을 학습하거나, 비디오 프레임 간의 시간적 인과관계를 파악하여 “달리고 있다“와 “넘어졌다” 사이의 행동 변화를 인지할 수 있게 된다.</p>
<h2>4.  이미지 캡셔닝을 위한 통합 아키텍처 설계</h2>
<p>이미지 캡셔닝 모델은 이미지를 입력받아 이를 설명하는 자연어 문장을 생성하는 것을 목표로 한다. 이를 위해 InceptionV3(인코더)와 LSTM(디코더)을 결합하는 방식은 크게 <strong>주입(Inject)</strong> 아키텍처와 <strong>병합(Merge)</strong> 아키텍처로 나뉜다. 이 두 방식은 이미지 정보를 LSTM에 언제, 어떻게 제공하느냐에 따라 구분되며 모델의 성능에 중요한 영향을 미친다.</p>
<h3>4.1  주입(Inject) 아키텍처와 병합(Merge) 아키텍처의 비교</h3>
<h4>4.1.1  주입(Inject) 아키텍처</h4>
<p>주입 아키텍처는 이미지 특징 벡터를 LSTM의 **초기 상태(Initial State)**로 설정하거나, 시퀀스의 첫 번째 입력으로 제공하는 방식이다. 즉, 이미지가 문장 생성의 ‘씨앗(Seed)’ 역할을 한다.</p>
<ul>
<li><strong>작동 방식:</strong> LSTM은 첫 번째 타임스텝에서만 이미지를 보고, 이후 타임스텝부터는 이전에 생성된 단어만을 입력으로 받아 다음 단어를 생성한다.</li>
<li><strong>한계:</strong> 문장의 길이가 길어질수록 초기에 입력된 이미지 정보가 LSTM의 순환 과정에서 점차 희석될(Fading Memory) 위험이 있다. 따라서 긴 문장 생성 시 이미지와 무관한 텍스트가 생성될 가능성이 존재한다.12</li>
</ul>
<h4>4.1.2  병합(Merge) 아키텍처</h4>
<p>현재 대부분의 고성능 이미지 캡셔닝 모델(예: 1)에서 채택하는 방식이다. 이미지 특징과 텍스트 시퀀스를 독립적인 레이어에서 처리한 후, LSTM의 입력 단계가 아닌 중간 단계에서 결합한다.</p>
<ul>
<li><strong>작동 방식:</strong></li>
</ul>
<ol>
<li><strong>이미지 처리 경로:</strong> InceptionV3에서 추출된 2048차원 벡터는 고밀도 층(Dense Layer)을 거쳐 텍스트 임베딩과 동일한 차원(예: 256 또는 512)으로 압축된다. 이후 <code>RepeatVector</code> 등을 통해 시퀀스 길이만큼 복제되거나, 결합 층에서 브로드캐스팅된다.</li>
<li><strong>텍스트 처리 경로:</strong> 캡션 텍스트는 임베딩 층(Embedding Layer)을 거쳐 밀집 벡터로 변환되고, LSTM 레이어를 통과하여 시퀀스 특징을 추출한다.</li>
<li><strong>결합(Fusion):</strong> 이미지 경로의 출력과 텍스트 경로(LSTM)의 출력을 <strong>더하기(Add)</strong> 또는 <strong>연결(Concatenate)</strong> 연산을 통해 병합한다.</li>
<li><strong>추론:</strong> 병합된 벡터는 다시 고밀도 층과 Softmax 활성화 함수를 거쳐 전체 어휘 사전(Vocabulary)에 대한 확률 분포를 출력하고, 이를 통해 다음 단어를 예측한다.</li>
</ol>
<ul>
<li><strong>장점:</strong> LSTM이 온전히 언어적 문맥 학습에 집중할 수 있으며, 이미지 정보가 시퀀스 생성의 매 단계마다 보조 정보로 제공되므로 긴 문장에서도 이미지의 내용을 잃지 않고 반영할 수 있다.13</li>
</ul>
<h3>4.2  레이어 결합 방식: Add vs. Concatenate</h3>
<p>Keras와 같은 프레임워크 구현 시, 두 경로를 합치는 방식으로 <code>Add</code>와 <code>Concatenate</code> 층이 사용된다.</p>
<ul>
<li><strong>Add:</strong> 두 텐서의 요소를 원소별로 더한다. 이 경우 두 텐서의 차원(Dimension)이 정확히 일치해야 한다. 이는 ResNet의 스킵 연결(Skip Connection)과 유사하게 정보의 ’수정’이나 ’보정’의 의미를 갖는다. 파라미터 수가 늘어나지 않는 장점이 있다.15</li>
<li><strong>Concatenate:</strong> 두 텐서를 채널 방향으로 이어 붙인다. 출력 텐서의 차원은 두 입력 차원의 합이 되며, 이후 레이어의 파라미터 수가 증가한다. 두 정보가 서로 독립적인 성격을 가질 때(예: 시각 정보와 언어 정보) 단순히 정보를 나열하여 다음 레이어가 관계를 학습하도록 유도하는 방식이다.14 이미지 캡셔닝의 병합 아키텍처에서는 두 방식 모두 사용되나, 정보의 보존 측면에서 Concatenate 후 Dense Layer를 두는 방식이 널리 쓰인다.</li>
</ul>
<h2>5.  비디오 행동 인식 및 시공간 모델링</h2>
<p>비디오 데이터는 정지 이미지의 집합이므로, 시간 축(Time Axis)이 추가된 4차원 텐서 <code>(Batch, Time, Height, Width, Channel)</code>를 처리해야 한다. InceptionV3-LSTM은 비디오 내의 행동 인식(Action Recognition) 및 이상 징후 탐지에 강력한 성능을 발휘한다.</p>
<h3>5.1  TimeDistributed 레이어를 통한 프레임별 특징 추출</h3>
<p>비디오를 처리할 때 가장 핵심적인 기술은 <strong>TimeDistributed</strong> 래퍼(Wrapper)의 활용이다. 일반적인 2D CNN(InceptionV3)은 3차원 이미지 텐서(H, W, C)만을 입력으로 받을 수 있다. TimeDistributed 레이어는 입력된 비디오 텐서의 시간 차원(Time)을 유지한 채, 각 타임스텝(프레임)마다 동일한 InceptionV3 모델을 적용한다.17</p>
<ul>
<li><strong>작동 메커니즘:</strong> 입력 <code>(Batch, 10, 299, 299, 3)</code> (10 프레임의 비디오)가 들어오면, TimeDistributed 레이어는 이를 10개의 독립적인 이미지로 간주하여 InceptionV3에 통과시킨다. 이때 가중치는 10번의 연산 모두에서 공유된다.</li>
<li><strong>출력 변환:</strong> 각 프레임에서 추출된 특징 벡터들이 다시 시간 순서대로 결합되어 <code>(Batch, 10, 2048)</code> 형태의 시퀀스 텐서가 생성된다. 이 텐서는 이제 “시간에 따른 시각적 특징의 변화“를 나타내며, LSTM의 입력으로 사용되기에 완벽한 형태가 된다.18</li>
</ul>
<h3>5.2  최적 프레임 샘플링 전략 (FPS 및 시퀀스 길이)</h3>
<p>비디오의 모든 프레임(보통 초당 30~60프레임)을 처리하는 것은 막대한 연산 비용을 초래하며, 인접한 프레임 간의 정보는 매우 중복적이어서 오히려 학습을 방해할 수 있다. 따라서 적절한 프레임 샘플링 전략이 필수적이다.</p>
<p>연구 결과에 따르면, 전체 프레임을 사용하는 것보다 <strong>1 FPS(초당 1프레임)</strong> 정도로 다운샘플링하여 사용하는 것이 계산 효율성을 극대화하면서도 분류 정확도를 유지하거나 심지어 향상시키는 것으로 나타났다. 예를 들어, 한 연구에서는 1 FPS 설정이 0.5 FPS보다 높은 F1-Score(94.3% vs 91.1%)를 기록하며 최적의 성능을 보였다.20 시퀀스 길이(Sequence Length)의 경우, 스포츠 비디오 분석 등에서는 <strong>20~40 프레임</strong> 정도의 윈도우 크기가 동작의 시작과 끝을 포괄하기에 적절한 것으로 보고된다. 시퀀스가 너무 길면 LSTM의 기억 용량 한계를 초과할 수 있고, 너무 짧으면 행동의 맥락을 파악하기 어렵다.2</p>
<h2>6.  학습 방법론 및 최적화 전략</h2>
<h3>6.1  교사 강요 (Teacher Forcing)</h3>
<p>InceptionV3-LSTM과 같은 시퀀스 생성 모델을 학습시킬 때 <strong>교사 강요(Teacher Forcing)</strong> 기법은 수렴 속도를 높이는 데 결정적인 역할을 한다.</p>
<ul>
<li><strong>개념:</strong> 디코더(LSTM)가 시점 <span class="math math-inline">t</span>에서 단어를 예측할 때, 입력으로 이전 시점 <span class="math math-inline">t-1</span>에서 모델이 예측한 단어가 아닌 <strong>실제 정답(Ground Truth) 단어</strong>를 넣어주는 방식이다.</li>
<li><strong>필요성:</strong> 학습 초기 모델은 무작위한 단어를 내뱉을 확률이 높다. 만약 모델이 잘못 예측한 단어를 다음 시점의 입력으로 사용하면, 오류가 누적되어 이후의 모든 예측이 엉망이 되고 학습이 매우 더디게 진행된다.22</li>
<li><strong>노출 편향(Exposure Bias):</strong> 교사 강요를 사용하면 학습 때는 정답을 보며 쉽게 배우지만, 실제 추론(Test/Inference) 단계에서는 정답을 알 수 없으므로 자신의 예측값을 사용해야 한다. 이로 인해 학습 환경과 테스트 환경 간의 괴리가 발생하여 성능이 저하되는 현상을 노출 편향이라 한다. 이를 완화하기 위해 학습이 진행될수록 교사 강요의 비율을 점진적으로 줄이는 ‘Scheduled Sampling’ 기법이 제안되기도 한다.23</li>
</ul>
<h3>6.2  손실 함수 및 최적화 기법</h3>
<ul>
<li><strong>손실 함수(Loss Function):</strong> 이미지 캡셔닝은 본질적으로 매 시점마다 어휘 사전 내의 단어 중 하나를 맞히는 다중 클래스 분류 문제이다. 따라서 **범주형 교차 엔트로피(Categorical Cross-Entropy)**가 표준 손실 함수로 사용된다. 비디오 분류 역시 행동 클래스를 예측하므로 동일한 손실 함수를 사용한다.1</li>
<li><strong>최적화(Optimizer):</strong> 희소한 기울기 문제에 강하고 학습률을 적응적으로 조절하는 <strong>Adam Optimizer</strong>가 가장 널리 사용된다. 때로는 과적합 방지를 위해 그래디언트 클리핑(Gradient Clipping)을 적용하여 기울기 폭주를 막기도 한다.1</li>
</ul>
<h3>6.3  데이터 전처리 및 증강</h3>
<p>InceptionV3-LSTM 모델의 성능은 데이터의 질에 크게 좌우된다.</p>
<ul>
<li><strong>이미지:</strong> InceptionV3의 입력에 맞춰 299x299 크기로 리사이징하고, ImageNet 학습 데이터의 분포에 맞춰 픽셀 값을 정규화한다. 데이터 부족을 해결하기 위해 회전, 이동, 좌우 반전 등의 증강(Augmentation)을 적용한다.3</li>
<li><strong>텍스트:</strong> 캡션 텍스트는 구두점을 제거하고 소문자로 변환한 뒤 토큰화한다. 어휘 사전의 크기를 빈도수 기반으로 제한하여 모델의 복잡도를 줄이고, <code>&lt;unk&gt;</code>(Unknown) 토큰으로 희귀 단어를 처리한다.26</li>
</ul>
<h2>7.  어텐션 메커니즘(Attention Mechanism)의 도입</h2>
<p>기본적인 InceptionV3-LSTM 모델은 이미지를 하나의 고정된 벡터로 압축하기 때문에, 이미지 내의 세부적인 공간 정보가 손실될 수 있다. 이를 극복하기 위해 <strong>어텐션 메커니즘</strong>이 도입되었다. 어텐션은 디코더가 단어를 생성하는 매 시점마다 이미지의 <strong>어느 영역</strong>을 집중해서 볼지를 동적으로 결정한다.4</p>
<ul>
<li>
<p><strong>Soft Attention:</strong> 이미지의 모든 영역에 대해 0~1 사이의 가중치를 부여하고, 가중합(Weighted Sum)을 통해 문맥 벡터를 생성한다. 미분 가능하여 역전파 학습이 용이하다.</p>
</li>
<li>
<p>Hard Attention: 특정 시점에 이미지의 한 영역만을 선택(Sampling)하여 본다. 확률적 과정이므로 강화학습 등의 기법이 필요할 수 있다.</p>
</li>
</ul>
<p>대부분의 이미지 캡셔닝 연구에서는 Soft Attention을 적용하여, InceptionV3의 마지막 풀링 레이어 이전의 공간 특징 맵(Spatial Feature Map, 예: 8x8x2048)을 활용한다. 이는 모델이 “공을 차다“라는 단어를 생성할 때 이미지의 ‘공’ 부분과 ‘발’ 부분에 높은 가중치를 두는 것을 가능하게 하여, 모델의 **해석 가능성(Interpretability)**을 크게 향상시킨다.27</p>
<h2>8.  성능 평가 지표 및 실험 결과 분석</h2>
<h3>8.1  자연어 생성 평가지표</h3>
<p>이미지 캡셔닝 모델의 성능은 생성된 문장이 인간이 작성한 정답 캡션과 얼마나 유사한지를 측정하는 지표로 평가된다.</p>
<ul>
<li><strong>BLEU (Bilingual Evaluation Understudy):</strong> 생성 문장과 정답 문장 간의 n-gram(단어의 연속) 일치도를 측정한다. BLEU-1은 단어 단위의 정확도, BLEU-4는 4개 단어 연속의 일치도를 의미한다. 가장 보편적이지만 문장의 의미보다는 어휘 일치에 집중하는 경향이 있다.7</li>
<li><strong>METEOR:</strong> 동의어 매칭과 어순 변경까지 고려하여 BLEU의 단점을 보완한 지표이다.</li>
<li><strong>CIDEr (Consensus-based Image Description Evaluation):</strong> 이미지 캡셔닝에 특화된 지표로, 자주 등장하는 단어(a, the 등)의 가중치를 낮추고(TF-IDF), 캡션 집단과의 합의(Consensus)를 측정한다.29</li>
<li><strong>ROUGE:</strong> 주로 텍스트 요약 평가에 쓰이며, 재현율(Recall)을 중시하여 정답 문장의 단어가 얼마나 많이 포함되었는지를 측정한다.27</li>
</ul>
<h3>8.2  비디오 분류 평가지표 및 성과</h3>
<p>비디오 행동 인식 모델은 <strong>정확도(Accuracy)</strong>, <strong>정밀도(Precision)</strong>, <strong>재현율(Recall)</strong>, <strong>F1-Score</strong>를 통해 평가된다. 연구 사례를 보면, UCF Sports 데이터셋에 대해 InceptionV3-LSTM 모델은 약 <strong>89.5%의 정확도</strong>와 <strong>90%의 정밀도</strong>를 달성하였으며, 이는 기존의 수작업 특징 추출 방식이나 단순 CNN 모델을 상회하는 결과이다.2 또한, 유채꽃 수확 시기 예측 연구에서는 RGB 이미지만을 사용하여 <strong>96%의 인식률</strong>을 달성, 농업 분야에서의 실용성을 입증하였다.3</p>
<h2>9.  응용 분야 사례 연구</h2>
<p>InceptionV3-LSTM 모델의 강력한 시공간 해석 능력은 다양한 산업 분야로 확장되고 있다.</p>
<ol>
<li><strong>스포츠 경기 분석:</strong> 축구, 농구 등의 경기 영상에서 골, 파울, 드리블과 같은 특정 이벤트를 자동으로 탐지하고 분류한다. InceptionV3는 선수와 공의 위치적 특징을, LSTM은 시간적 움직임을 분석하여 빠른 경기 흐름 속에서도 높은 정확도로 이벤트를 포착한다.2</li>
<li><strong>스마트 농업:</strong> 작물의 생육 상태를 모니터링하기 위해 드론이나 CCTV 영상을 분석한다. 바람에 흔들리는 작물의 움직임이나 시간에 따른 색상 변화(예: 유채꽃의 개화)를 시계열로 분석하여 최적의 수확 시기를 예측하거나 질병을 조기에 진단한다.3</li>
<li><strong>인간 행동 인식(HAR) 및 헬스케어:</strong> 웨어러블 센서 데이터나 CCTV 영상을 통해 낙상 감지, 운동 자세 교정, 수화 인식 등을 수행한다. 특히 InceptionV3 구조를 1차원 센서 데이터 처리에 맞게 변형하거나, MediaPipe와 같은 손동작 추적 모델과 결합하여 수화 번역의 정확도를 높이는 연구가 진행되고 있다.5</li>
</ol>
<h2>10.  결론</h2>
<p>본 보고서에서 심층 분석한 <strong>InceptionV3-LSTM</strong> 모델은 컴퓨터 비전의 공간적 특징 추출 능력과 자연어 처리의 시퀀스 모델링 능력을 성공적으로 융합한 딥러닝 아키텍처의 모범 사례이다. InceptionV3의 효율적인 연산 구조는 이미지 내의 복잡한 시각 정보를 손실 없이 압축하며, LSTM의 게이트 메커니즘은 시간적 흐름과 문맥을 파악하여 정교한 캡션 생성과 행동 인식을 가능하게 한다.</p>
<p>특히 ’병합 아키텍처’의 도입과 ’어텐션 메커니즘’의 결합은 긴 시퀀스에서도 정보의 소실을 막고 모델의 해석 가능성을 높이는 데 기여하였다. 비록 최근 Transformer 기반의 거대 모델들이 등장하고 있으나, InceptionV3-LSTM은 여전히 데이터 효율성, 구현의 용이성, 그리고 검증된 성능을 바탕으로 엣지 디바이스나 실시간 분석이 필요한 산업 현장에서 강력한 경쟁력을 유지하고 있다. 향후 이 모델은 경량화 기술 및 강화학습 기반의 어텐션 기법과 결합되어 더욱 정교하고 효율적인 멀티모달 지능형 시스템으로 발전할 것으로 전망된다.</p>
<h2>11. 참고 자료</h2>
<ol>
<li>khaled87ahmed/Image-Captioning-InceptionV3-LSTM … - GitHub, https://github.com/khaled87ahmed/Image-Captioning-InceptionV3-LSTM</li>
<li>Automatic Sports Video Classification Using CNN-LSTM Approach, https://ceur-ws.org/Vol-3694/paper4.pdf</li>
<li>(PDF) InceptionV3-LSTM: A Deep Learning Net for the Intelligent …, https://www.researchgate.net/publication/365935588_InceptionV3-LSTM_A_Deep_Learning_Net_for_the_Intelligent_Prediction_of_Rapeseed_Harvest_Time</li>
<li>Exploring Enhanced Image-to-Text Generation using Inception v3 …, https://dergipark.org.tr/tr/download/article-file/3320935</li>
<li>MediaPipe, Inception-v3 and LSTM-Based Enhanced Deep … - MDPI, https://www.mdpi.com/2079-9292/13/16/3233</li>
<li>Image Captioning Generation Using Inception V3 and Attention …, https://iasj.rdd.edu.iq/journals/uploads/2024/12/20/f429c9f690b18a542c88f691608e0b4a.pdf</li>
<li>Image Caption Generator: Leveraging LSTM and BLSTM over …, https://medium.com/@shivamnegi92/image-caption-generator-leveraging-lstm-and-blstm-over-inception-v3-ecdde22ff8e4</li>
<li>InceptionV3 - Keras, https://keras.io/api/applications/inceptionv3/</li>
<li>Automatic Sports Video Classification Using CNN-LSTM Approach, https://www.researchgate.net/publication/381740998_Automatic_Sports_Video_Classification_Using_CNN-LSTM_Approach</li>
<li>InceptionV3 Feature Extraction - Kaggle, https://www.kaggle.com/code/hebamohamed/inceptionv3-feature-extraction</li>
<li>Image Captioning using ResNet and LSTM, https://learnopencv.com/image-captioning/</li>
<li>Two-Tier LSTM Model for Image Caption Generation, https://oaji.net/pdf.html?n=2021/3603-1624925420.pdf</li>
<li>Image Captioning Approach for Household Environment Visual …, http://ijistech.org/ijistech/index.php/ijistech/article/download/135/pdf</li>
<li>Create Your Own Image Caption Generator using Keras!, https://www.analyticsvidhya.com/blog/2020/11/create-your-own-image-caption-generator-using-keras/</li>
<li>Add vs Concatenate layer in Keras - Stack Overflow, https://stackoverflow.com/questions/70400558/add-vs-concatenate-layer-in-keras</li>
<li>When to “add” layers and when to “concatenate” in neural networks?, https://stats.stackexchange.com/questions/361018/when-to-add-layers-and-when-to-concatenate-in-neural-networks</li>
<li>Visualize features into a CNN+LSTM for video classification #82, https://github.com/philipperemy/keract/issues/82</li>
<li>TimeDistributed layer - Keras, https://keras.io/api/layers/recurrent_layers/time_distributed/</li>
<li>How to Use the TimeDistributed Layer in Keras, https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/</li>
<li>arXiv:2504.09738v1 [cs.CV] 13 Apr 2025, https://www.arxiv.org/pdf/2504.09738</li>
<li>Continuous video classification with TensorFlow, Inception and …, https://blog.coast.ai/continuous-video-classification-with-tensorflow-inception-and-recurrent-nets-250ba9ff6b85</li>
<li>12월 13, 2025에 액세스, [https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning#:<sub>:text=Teacher%20Forcing%20is%20when%20we,faster%20convergence%20of%20the%20model.](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning#:</sub>:text=Teacher Forcing is when we, <a href="https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning#:~:text=Teacher%20Forcing%20is%20when%20we,faster%20convergence%20of%20the%20model.">https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning#:~:text=Teacher%20Forcing%20is%20when%20we,faster%20convergence%20of%20the%20model.</a></li>
<li>What is Teacher Forcing?. A common technique in training… - Medium, https://medium.com/data-science/what-is-teacher-forcing-3da6217fed1c</li>
<li>sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning - GitHub, https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning</li>
<li>Loss functions for LSTM model. - ResearchGate, https://www.researchgate.net/figure/Loss-functions-for-LSTM-model_fig5_343468272</li>
<li>InceptionV3 &amp; LSTM ImageCaptionGenerator - Part 2 - Kaggle, https://www.kaggle.com/code/shweta2407/inceptionv3-lstm-imagecaptiongenerator-part-2</li>
<li>Exploring Enhanced Image-to-Text Generation using Inception v3 …, https://dergipark.org.tr/en/download/article-file/3320935</li>
<li>Inception-LSTM Human Motion Recognition with Channel Attention …, https://pmc.ncbi.nlm.nih.gov/articles/PMC9208947/</li>
<li>CNN-LSTM-BASED DEEP LEARNING FOR … - New Science, https://periodicos.newsciencepubl.com/arace/article/download/1339/4716/13926</li>
<li>How we used Modified Inception V3 CNN for Video Processing …, https://medium.com/@Apriorit/using-modified-inception-v3-cnn-for-video-processing-and-video-classification-b0c145baa1fc</li>
<li>The accuracy and loss of the inceptionv3 -LSTM model during the…, https://www.researchgate.net/figure/The-accuracy-and-loss-of-the-inceptionv3-LSTM-model-during-the-training-process-a-the_fig5_343046108</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>