<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:PANet (Path Aggregation Network)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>PANet (Path Aggregation Network)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">딥러닝 레이어 (Layers)</a> / <span>PANet (Path Aggregation Network)</span></nav>
                </div>
            </header>
            <article>
                <h1>PANet (Path Aggregation Network)</h1>
<h2>1. 서론</h2>
<h3>1.1 객체 탐지에서의 다중 스케일 문제의 중요</h3>
<p>자연스러운 이미지 내에 존재하는 객체들은 그 크기가 매우 다양하며, 이러한 광범위한 스케일 변동성(scale variation)은 컴퓨터 비전 분야, 특히 객체 탐지(object detection)에서 해결해야 할 근본적인 난제 중 하나로 꼽힌다.1 동일한 클래스의 객체라도 카메라와의 거리, 시점, 이미지 해상도 등에 따라 픽셀 단위에서 차지하는 크기가 수십 배에서 수백 배까지 차이 날 수 있다. 이러한 스케일 차이는 모델이 일관된 특징을 학습하는 것을 방해하며, 특히 이미지에서 차지하는 영역이 매우 작은 소형 객체(small object)의 탐지 성능을 저하시키는 주요 원인으로 작용한다.3</p>
<p>이 문제를 해결하기 위한 고전적이면서도 직관적인 접근법은 이미지 피라미드(Image Pyramid)를 활용하는 것이다.5 이 방식은 입력 이미지 자체를 다양한 배율로 리사이징(resizing)하여 여러 스케일의 이미지 복사본을 만들고, 각 스케일의 이미지에 대해 독립적으로 특징 추출기와 탐지기를 적용한다. 이를 통해 모델은 특정 스케일에 최적화된 상태에서 객체를 탐지할 수 있게 되어 스케일 불변성(scale-invariance)을 확보할 수 있다.5 그러나 이미지 피라미드 방식은 다수의 리사이징된 이미지를 개별적으로 처리해야 하므로 막대한 계산 비용과 메모리 요구량을 수반한다. 이로 인해 추론 속도가 현저히 느려질 뿐만 아니라, 전체 파이프라인을 하나의 네트워크에서 종단간(end-to-end)으로 학습시키는 것이 현실적으로 불가능에 가깝다는 치명적인 한계를 지닌다.3</p>
<h3>1.2 특징 피라미드 네트워크 (FPN)의 등장과 패러다임 전환</h3>
<p>이미지 피라미드의 비효율성을 극복하기 위한 대안으로, 2017년 Lin 등이 제안한 특징 피라미드 네트워크(Feature Pyramid Network, FPN)는 객체 탐지 분야의 패러다임을 전환하는 중요한 계기가 되었다.5 FPN의 핵심 아이디어는 입력 이미지가 아닌, 심층 컨볼루션 신경망(Deep Convolutional Neural Network, CNN)이 특징 추출 과정에서 자연스럽게 생성하는 내부의 특징 맵 계층(feature map hierarchy)을 활용하는 것이다. 즉, 단일 스케일의 고해상도 이미지를 한 번만 네트워크에 입력하면, CNN의 깊이가 깊어짐에 따라 공간 해상도는 낮아지고 의미론적 정보(semantic information)는 풍부해지는 다중 스케일의 특징 피라미드가 내부적으로 생성된다는 점에 착안했다.1 이는 연산의 대상을 입력 공간(input space)에서 특징 공간(feature space)으로 옮겨온 근본적인 발상의 전환이었다.</p>
<p>FPN은 이러한 내재적 특징 계층을 효과적으로 활용하기 위해 세 가지 핵심 요소로 구성된다 1:</p>
<ol>
<li><strong>상향식 경로 (Bottom-up pathway)</strong>: 입력 이미지로부터 특징을 추출하는 일반적인 CNN 백본 네트워크(예: ResNet)를 의미한다. 이 경로를 따라가며 특징 맵의 공간 해상도는 점차 감소하고, 채널 수는 증가하며, 더 추상적이고 의미론적인 정보가 추출된다.</li>
<li><strong>하향식 경로 (Top-down pathway)</strong>: 상향식 경로의 최상위 계층, 즉 가장 의미론적으로 풍부하지만 공간 해상도가 낮은 특징 맵에서부터 시작하여 업샘플링(upsampling)을 통해 점차 해상도를 높여나가는 경로이다.</li>
<li><strong>측면 연결 (Lateral connections)</strong>: 하향식 경로에서 업샘플링된 특징 맵을 상향식 경로의 동일한 공간 해상도를 갖는 특징 맵과 융합(fusion)하는 연결이다. 일반적으로 <span class="math math-inline">1 \times 1</span> 컨볼루션을 통해 채널 차원을 맞춘 후, 요소별 덧셈(element-wise addition)을 통해 두 특징 맵을 결합한다.</li>
</ol>
<p>이 구조를 통해 FPN은 추가적인 계산 비용을 최소화하면서도, 모든 스케일의 특징 맵이 고수준의 풍부한 의미 정보를 공유하게 만들어 객체 탐지 성능을 획기적으로 향상시켰다.</p>
<h3>1.3  FPN의 내재적 한계: 단방향 정보 흐름</h3>
<p>FPN은 다중 스케일 문제에 대한 매우 효율적인 해결책을 제시했지만, 그 구조에는 정보 흐름의 방향성과 관련된 내재적 한계가 존재했다. FPN의 정보 융합은 본질적으로 고수준의 의미 정보를 저수준의 특징 맵으로 전파하는 <strong>하향식(top-down) 단방향</strong>으로만 이루어진다.7 이로 인해, 네트워크의 초기 계층(low-level feature maps)에 풍부하게 존재하는 엣지(edge), 코너(corner), 텍스처(texture)와 같은 정밀한 공간적/위치적 정보(localization signals)가 최상위 특징 맵까지 도달하는 경로가 매우 길어지는 정보 병목 현상이 발생한다.7</p>
<p>예를 들어 ResNet과 같은 깊은 백본 네트워크에서 가장 낮은 수준의 특징 맵(e.g., <span class="math math-inline">C_2</span>)에 담긴 정보가 최상위 예측 계층(e.g., <span class="math math-inline">P_5</span>)에 영향을 미치기 위해서는 100개가 넘는 컨볼루션 계층을 순차적으로 통과해야만 한다.8 이처럼 긴 정보 전달 경로는 초기 계층의 정밀한 위치 정보가 수많은 비선형 변환을 거치면서 희석되거나 소실될 위험을 내포한다. 이는 객체의 경계를 정확하게 예측해야 하는 인스턴스 분할(instance segmentation)이나 작은 객체의 위치를 정밀하게 특정해야 하는 상황에서 성능의 한계로 작용할 수 있다. 바로 이 FPN의 단방향 정보 흐름 문제를 해결하고, 저수준의 위치 정보를 고수준으로 효율적으로 전달할 새로운 경로를 구축하는 것이 Path Aggregation Network (PANet)가 해결하고자 하는 핵심 과제였다.7</p>
<h2>2. Path Aggregation Network (PANet)의 설계 철학과 핵심 구조</h2>
<h3>2.1  양방향 정보 흐름을 통한 특징 계층 강화</h3>
<p>Path Aggregation Network (PANet)의 설계 철학은 FPN의 단방향 정보 흐름 구조를 보완하여, 특징 피라미드 내에서 정보가 양방향으로 원활하게 흐르도록 만드는 데 있다.10 FPN이 고수준의 의미 정보를 하향식으로 전파하는 데 집중했다면, PANet은 여기에 더해 저수준의 정밀한 위치 정보를 상향식으로 전달하는 새로운 경로를 추가함으로써 정보 흐름의 균형을 맞추고자 했다.7 이 양방향 구조는 모든 스케일의 특징 맵이 고수준의 의미 정보와 저수준의 위치 정보를 모두 충분히 활용할 수 있도록 하여, 전체 특징 계층의 표현력을 극대화하는 것을 목표로 한다.</p>
<p>이러한 철학을 구현하기 위해 PANet은 FPN 구조 위에 **상향식 경로 증강 (Bottom-up Path Augmentation)**이라는 혁신적인 모듈을 추가하여, 정보가 아래에서 위로 흐르는 명시적인 경로를 구축했다.7 이와 더불어, 각 객체 제안(proposal)이 모든 스케일의 정보를 효과적으로 활용하도록 **적응형 특징 풀링 (Adaptive Feature Pooling)**을 도입하고, 마스크 예측의 정밀도를 높이기 위해 <strong>완전 연결 계층 융합 (Fully-connected Fusion)</strong> 기법을 제안했다. 이 세 가지 핵심 구성 요소는 ’네트워크 내 정보 활용의 극대화’라는 하나의 통일된 목표 아래 유기적으로 결합된다.</p>
<h3>2.2  상향식 경로 증강 (Bottom-up Path Augmentation)</h3>
<p>상향식 경로 증강은 PANet의 가장 핵심적인 구조적 혁신으로, FPN에서 간과되었던 저수준 특징의 상향 전파 문제를 직접적으로 해결한다.</p>
<ul>
<li>
<p><strong>동기</strong>: 앞서 지적했듯이, FPN의 백본 네트워크에서 저수준 특징이 최상위 특징에 도달하기까지의 경로는 100개 이상의 계층을 통과하는 매우 긴 경로(long path)이다.7 이로 인해 정보가 소실될 가능성이 크다. PANet은 이 긴 경로를 10개 미만의 계층으로 구성된 명시적인 “지름길(shortcut)“로 대체하여, 저수준의 정밀한 위치 정보가 손실 없이 상위 계층으로 빠르게 전달되도록 하고자 했다.8</p>
</li>
<li>
<p><strong>구조</strong>: 이 모듈은 FPN이 생성한 특징 피라미드 {<span class="math math-inline">P_2, P_3, P_4, P_5</span>}<code>를 입력으로 받아, 새로운 특징 피라미드 </code>{<span class="math math-inline">N_2, N_3, N_4, N_5</span>}`를 생성하는 방식으로 동작한다.7 이 과정은 가장 낮은 레벨인</p>
</li>
</ul>
<p><span class="math math-inline">P_2</span>에서 시작하여 점진적으로 상위 레벨로 진행된다.</p>
<ol>
<li>가장 낮은 레벨의 새로운 특징 맵 <span class="math math-inline">N_2</span>는 별도의 처리 없이 <span class="math math-inline">P_2</span>와 동일하게 정의된다.</li>
<li>그 다음 레벨인 <span class="math math-inline">N_3</span>를 생성하기 위해, 먼저 <span class="math math-inline">N_2</span>를 stride가 2인 <span class="math math-inline">3 \times 3</span> 컨볼루션 연산을 통해 공간적으로 다운샘플링하여 <span class="math math-inline">P_3</span>와 동일한 해상도로 맞춘다.</li>
<li>이렇게 다운샘플링된 맵은 측면 연결(lateral connection)을 통해 <span class="math math-inline">P_3</span>와 요소별 덧셈(element-wise addition)으로 융합된다.</li>
<li>마지막으로, 융합된 특징 맵에 다시 <span class="math math-inline">3 \times 3</span> 컨볼루션 연산을 적용하여 최종적으로 <span class="math math-inline">N_3</span>를 생성한다.</li>
<li>이 과정은 <span class="math math-inline">N_4</span>, <span class="math math-inline">N_5</span>를 생성하기 위해 순차적으로 반복되며, <span class="math math-inline">N_i</span>와 <span class="math math-inline">P_{i+1}</span>을 융합하여 <span class="math math-inline">N_{i+1}</span>을 만드는 구조를 가진다.7</li>
</ol>
<ul>
<li><strong>효과</strong>: 이 짧고 효율적인 상향식 경로를 통해, 이미지의 엣지, 텍스처 등 객체의 정확한 위치를 파악하는 데 결정적인 저수준 정보가 최소한의 변형만을 거쳐 상위 계층의 특징 맵에 직접적으로 융합된다. 결과적으로, 전체 특징 계층의 위치 결정 능력(localization capability)이 크게 강화되어, 보다 정밀한 객체 탐지와 분할이 가능해진다.7</li>
</ul>
<h3>2.3  적응형 특징 풀링 (Adaptive Feature Pooling)</h3>
<p>적응형 특징 풀링은 FPN의 또 다른 한계, 즉 객체 제안(proposal)에 특징을 할당하는 방식의 비효율성을 해결하기 위해 고안되었다.</p>
<ul>
<li><strong>동기</strong>: FPN에서는 제안의 크기(예: 면적)에 따라 특정 특징 레벨 하나에만 휴리스틱하게 할당하는 방식을 사용한다.7 예를 들어, 작은 제안은 고해상도 특징 맵(</li>
</ul>
<p><span class="math math-inline">P_2</span>)에, 큰 제안은 저해상도 특징 맵(<span class="math math-inline">P_5</span>)에 할당된다. 이 방식은 두 가지 문제를 야기한다. 첫째, 크기가 아주 약간 다른 두 제안이 서로 다른 레벨에 할당되어 일관성 없는 특징을 부여받을 수 있다. 둘째, 각 제안이 다른 레벨에 존재하는 유용한 정보를 활용할 기회를 원천적으로 차단한다. 예를 들어, 작은 객체를 인식하는 데에도 주변의 넓은 문맥 정보를 담고 있는 고수준 특징이 도움이 될 수 있으며, 반대로 큰 객체의 경계를 정밀하게 파악하는 데에는 저수준의 세부 정보가 유용할 수 있다.7</p>
<ul>
<li><strong>구조</strong>: 이러한 인위적인 제약을 없애기 위해, PANet은 각 제안에 대해 상향식 경로 증강을 통해 생성된 모든 특징 레벨 <code>{$N_2, N_3, N_4, N_5$}</code>로부터 특징을 추출하여 종합적으로 활용하는 방식을 제안한다.7</li>
</ul>
<ol>
<li>하나의 객체 제안이 주어지면, 이 제안 영역을 모든 특징 레벨 <code>{$N_2, N_3, N_4, N_5$}</code>에 매핑한다.</li>
<li>각 레벨에서 ROIAlign과 같은 풀링 기법을 사용하여 고정된 크기의 특징 그리드(feature grid)를 추출한다.</li>
<li>이렇게 각 레벨에서 추출된 여러 개의 특징 그리드를 하나의 융합 연산(fusion operation), 예를 들어 요소별 최댓값(element-wise max)이나 합계(sum)를 통해 단일 특징 그리드로 통합한다.7</li>
</ol>
<ul>
<li><strong>효과</strong>: 이 방식을 통해 모든 객체 제안은 자신의 크기와 무관하게 모든 스케일의 정보를 종합적으로 활용할 수 있게 된다. 이는 FPN에서 끊어졌던 ’제안’과 ‘전체 특징 피라미드’ 사이의 정보 경로를 복원하는 효과를 가져온다. PANet 논문에서 수행된 분석에 따르면, 실제로 작은 제안은 예측에 필요한 정보의 약 70%를 고수준 특징에서, 큰 제안은 약 50% 이상을 저수준 특징에서 가져오는 것으로 나타나, 이 구조의 타당성을 실험적으로 입증했다.7</li>
</ul>
<h3>2.4  완전 연결 계층 융합 (Fully-connected Fusion for Mask Prediction)</h3>
<p>이 구성 요소는 특히 인스턴스 분할(instance segmentation) 작업의 마스크 예측 성능을 향상시키기 위해 설계되었다.</p>
<ul>
<li><strong>동기</strong>: Mask R-CNN과 같은 기존 모델의 마스크 예측 헤드는 주로 FCN(Fully Convolutional Network)으로 구성된다. FCN은 각 픽셀에 대해 독립적인 예측을 수행하며 공간 정보를 잘 보존하는 장점이 있지만, 예측이 지역적인 수용장(local receptive field)에 크게 의존한다는 한계가 있다. 반면, FC(Fully-connected) 계층은 제안 영역 전체의 정보를 종합하여 전역적인(global) 예측을 수행하며, 각 공간 위치에 대해 다른 파라미터를 학습하므로 위치에 민감한(location-sensitive) 특징을 포착할 수 있다.7 이 두 방식은 상호 보완적인 특성을 지닌다.</li>
<li><strong>구조</strong>: PANet은 기존 FCN 기반 마스크 브랜치에 FC 계층을 활용하는 경량의 보조 브랜치(complementary branch)를 추가하여 두 방식의 장점을 모두 취한다.</li>
</ul>
<ol>
<li>메인 경로는 기존과 같이 여러 개의 컨볼루션 레이어로 구성된 FCN이 클래스별 마스크를 예측한다.</li>
<li>이와 별개로, FCN의 중간 특징 맵에서 새로운 경로가 분기된다. 이 경로는 두 개의 <span class="math math-inline">3 \times 3</span> 컨볼루션과 하나의 FC 계층으로 구성된 짧은 경로이다.</li>
<li>이 FC 계층은 계산 효율성과 일반화 성능을 높이기 위해 클래스와 무관하게 단순히 전경(foreground)과 배경(background)만을 구분하는 마스크를 예측하도록 설계된다.</li>
<li>최종적으로, FCN 경로에서 예측된 클래스별 마스크와 FC 경로에서 예측된 전경/배경 마스크를 요소별 덧셈(element-wise addition)으로 융합하여 최종 마스크 예측 결과를 생성한다.7</li>
</ol>
<ul>
<li><strong>효과</strong>: 지역적인 픽셀 단위 정보(FCN)와 전역적인 제안 단위 정보(FC)를 결합함으로써 정보의 다양성을 높이고, 서로 다른 관점의 예측을 융합하여 더욱 정교하고 품질 높은 마스크를 생성할 수 있게 된다.7</li>
</ul>
<h2>3. 실험적 성능 분석 및 평가</h2>
<h3>3.1  COCO 데이터셋 기반 성능 평가</h3>
<p>PANet의 효과는 당시 가장 권위 있는 객체 탐지 및 분할 벤치마크인 COCO 데이터셋을 통해 성공적으로 입증되었다. PANet은 COCO 2017 Challenge에서 인스턴스 분할(Instance Segmentation) 부문 1위, 객체 탐지(Object Detection) 부문 2위를 차지하며 제안된 아키텍처의 SOTA(State-Of-The-Art) 성능을 증명했다.11</p>
<p>아래 표 1은 COCO <code>test-dev</code> 데이터셋에서 PANet과 당시의 주요 모델들을 비교한 인스턴스 분할 성능을 보여준다. 비교 대상은 FPN을 사용하는 Mask R-CNN과 COCO 2016 챌린지 우승 모델이다. 이 표에서 주목할 점은, 상대적으로 가벼운 ResNet-50을 백본으로 사용한 PANet이 더 무겁고 강력한 ResNet-101 백본을 사용한 Mask R-CNN의 성능(AP 35.7)을 상회하는 36.6의 AP(Average Precision)를 달성했다는 것이다.7 이는 단순히 백본의 성능을 높이는 것보다 넥(neck) 부분의 아키텍처를 개선하는 것이 얼마나 효과적인지를 명확히 보여주는 결과이다. 더 강력한 ResNeXt-101 백본을 사용했을 때, PANet은 40.0 AP라는 높은 성능을 기록했으며, 다중 스케일 학습(multi-scale training)을 적용했을 때는 42.0 AP까지 성능이 향상되어 당시 최고 수준의 성능을 달성했다.</p>
<table><thead><tr><th>방법</th><th>백본</th><th>AP</th><th>AP50</th><th>AP75</th><th>APS</th><th>APM</th><th>APL</th></tr></thead><tbody>
<tr><td>Champion 2016</td><td>6×ResNet-101</td><td>37.6</td><td>59.9</td><td>40.4</td><td>17.1</td><td>41.0</td><td>56.0</td></tr>
<tr><td>Mask R-CNN + FPN</td><td>ResNet-101</td><td>35.7</td><td>58.0</td><td>37.8</td><td>15.5</td><td>38.1</td><td>52.4</td></tr>
<tr><td>Mask R-CNN + FPN</td><td>ResNeXt-101</td><td>37.1</td><td>60.0</td><td>39.4</td><td>16.9</td><td>39.9</td><td>53.5</td></tr>
<tr><td>PANet</td><td>ResNet-50</td><td>36.6</td><td>58.0</td><td>39.3</td><td>16.3</td><td>38.1</td><td>53.1</td></tr>
<tr><td>PANet [ms-train]</td><td>ResNet-50</td><td>38.2</td><td>60.2</td><td>41.4</td><td>19.1</td><td>41.1</td><td>52.6</td></tr>
<tr><td>PANet</td><td>ResNeXt-101</td><td>40.0</td><td>62.8</td><td>43.1</td><td>18.8</td><td>42.3</td><td>57.2</td></tr>
<tr><td>PANet [ms-train]</td><td>ResNeXt-101</td><td>42.0</td><td>65.1</td><td>45.7</td><td>22.4</td><td>44.7</td><td>58.1</td></tr>
</tbody></table>
<h3>3.2  구성 요소별 기여도 분석</h3>
<p>PANet의 우수한 성능이 어떤 구성 요소로부터 기인하는지를 명확히 파악하기 위해, 논문에서는 체계적인 제거 연구(ablation study)를 수행했다. 이 연구는 재구현된 Mask R-CNN 베이스라인(Reimplemented Baseline, RBL)에 PANet의 핵심 구성 요소들을 순차적으로 추가하며 성능 변화를 측정하는 방식으로 진행되었다.9</p>
<p>실험 결과, 각 구성 요소가 독립적으로도 유의미한 성능 향상에 기여함이 밝혀졌다.</p>
<ul>
<li><strong>상향식 경로 증강 (BPA)</strong>: 베이스라인에 BPA를 추가하자 마스크 AP와 박스 AP가 각각 0.6, 0.9 포인트 이상 일관되게 향상되었다. 이는 저수준의 위치 정보를 상위 계층으로 전달하는 ’지름길’이 실제로 모델의 정밀도를 높이는 데 효과적임을 증명한다.</li>
<li><strong>적응형 특징 풀링 (AFP)</strong>: BPA의 적용 여부와 관계없이 AFP를 추가했을 때 성능이 꾸준히 향상되었다. 이는 제안별로 모든 스케일의 특징을 종합적으로 활용하는 것이 FPN의 휴리스틱한 할당 방식보다 우수함을 보여준다.</li>
<li><strong>완전 연결 계층 융합 (FF)</strong>: 마스크 예측에 FF를 적용했을 때, 마스크 AP가 0.7 포인트 향상되었다. 이는 FCN의 지역적 정보와 FC의 전역적 정보를 융합하는 방식이 마스크 품질 개선에 기여함을 나타낸다.</li>
</ul>
<p>이러한 개별적인 성능 향상 외에도, Multi-GPU synchronized Batch Normalization (MBN)과 같은 학습 기법이 모델의 수렴을 도와 약 1.5 AP의 추가적인 성능 향상을 가져왔으며, PANet의 모든 구성 요소를 통합했을 때 베이스라인 대비 총 1.9 AP의 성능이 추가로 향상되었다.9 이 결과들은 PANet의 성능이 단일 아이디어에 의존하는 것이 아니라, 정보 흐름을 개선하기 위한 여러 구조적 장치들이 시너지를 내어 만들어낸 결과임을 명확히 보여준다.</p>
<h2>4. 후속 연구에 미친 영향과 구조적 발전</h2>
<p>PANet이 제시한 양방향 특징 융합 패러다임은 학문적 성과에 그치지 않고, 이후 등장하는 수많은 객체 탐지 모델의 아키텍처 설계에 지대한 영향을 미쳤다. 특히, PANet의 구조는 FPN을 대체하며 현대 객체 탐지 모델의 ‘넥(Neck)’ 부분의 사실상 표준(de facto standard)으로 자리 잡았다.12 PANet의 영향력은 크게 두 가지 흐름으로 나타났는데, 하나는 PANet 구조를 적극적으로 채택하고 특정 목적에 맞게 최적화하는 ’실용적 채택’의 흐름이며, 다른 하나는 PANet의 개념을 비판적으로 재해석하고 더 효율적인 구조로 발전시키는 ’학문적 진화’의 흐름이다.</p>
<h3>4.1  YOLO 계열 모델의 표준 넥(Neck) 구조로의 정착</h3>
<p>실용적 채택의 가장 대표적인 사례는 실시간 객체 탐지 분야를 선도하는 YOLO(You Only Look Once) 계열 모델에서 찾아볼 수 있다.</p>
<ul>
<li><strong>YOLOv4</strong>: YOLOv3는 넥 구조로 FPN을 사용했지만, YOLOv4는 성능 향상을 위해 이를 PANet으로 교체했다.14 CSPDarknet53이라는 강력한 백본과 YOLOv3 헤드 사이를 PANet이 연결하면서, 저수준의 정밀한 특징과 고수준의 의미론적 특징이 효과적으로 융합될 수 있었다.16 이 구조적 변경은 YOLOv3 대비 전반적인 탐지 정확도를 높이는 데 결정적인 역할을 했으며, 특히 FPN의 정보 병목 현상으로 인해 어려움을 겪었던 작은 객체 탐지 성능을 크게 개선하는 데 기여했다.17</li>
<li><strong>YOLOv5 및 후속 모델</strong>: YOLOv4의 성공 이후, YOLOv5, YOLOv8 등 후속 YOLO 모델들 역시 PANet의 양방향 정보 흐름 구조를 기본 골격으로 계승했다.19 이들은 PANet의 핵심 아이디어를 유지하면서도, CSP(Cross Stage Partial) 구조를 PANet의 일부 컨볼루션 블록에 통합하는 등의 변형을 통해 파라미터 수와 계산량을 줄여 효율성을 더욱 높이는 방향으로 최적화를 진행했다.13 이는 PANet의 구조가 성능뿐만 아니라 다른 모듈과 결합하여 변형 및 확장하기에도 용이한 유연성을 갖추고 있음을 보여주는 사례이다.</li>
</ul>
<h3>4.2  EfficientDet의 BiFPN으로의 진화</h3>
<p>학문적 진화의 흐름은 Google 연구팀이 발표한 EfficientDet 모델에서 제안된 BiFPN(Bi-directional Feature Pyramid Network)에서 명확하게 나타난다. BiFPN은 PANet의 양방향 정보 흐름이 매우 효과적이라는 점을 인정하면서도, 그 구조와 융합 방식에 몇 가지 비효율성이 존재한다고 보고 이를 개선하는 데 초점을 맞췄다.22</p>
<p>BiFPN이 PANet에 비해 개선한 핵심 사항은 다음과 같다 23:</p>
<ol>
<li><strong>가중치 기반 특징 융합 (Weighted Feature Fusion)</strong>: PANet은 서로 다른 레벨의 특징 맵을 융합할 때 단순한 요소별 덧셈을 사용한다. 이는 모든 입력 특징이 동일한 중요도를 갖는다고 가정하는 것과 같다. BiFPN은 이 가정이 비합리적이라고 보고, 각 입력 특징에 학습 가능한 가중치(learnable weights)를 부여하는 <strong>가중치 기반 융합</strong> 방식을 도입했다. 이를 통해 네트워크가 스스로 각 스케일 정보의 기여도를 학습하여 더 중요한 특징에 높은 가중치를 부여함으로써 융합의 효율과 성능을 극대화했다.25</li>
<li><strong>교차 스케일 연결 최적화 (Optimized Cross-Scale Connections)</strong>: BiFPN은 PANet의 연결 구조를 분석하여 정보 융합에 기여도가 낮은 노드를 제거했다. 구체적으로, 입력 경로가 하나뿐인 노드(단순히 특징을 전달만 하는 노드)를 과감히 삭제하여 계산 복잡도를 줄였다. 동시에, 동일한 레벨의 입력 노드에서 출력 노드로 바로 이어지는 추가적인 지름길(skip connection)을 도입하여, 더 적은 비용으로 더 많은 특징을 융합할 수 있도록 구조를 최적화했다.24</li>
<li><strong>반복 가능한 모듈 구조 (Repeatable Module)</strong>: PANet은 하나의 하향식 경로와 하나의 상향식 경로로 구성된 단일 패스(single-pass) 구조이다. 반면, BiFPN은 전체 양방향 경로 자체를 하나의 ‘레이어’ 또는 ’블록’으로 정의하고, 이를 여러 번 반복하여 쌓을(stack) 수 있도록 설계했다. 이를 통해 더 깊은 수준의 고차원 특징 융합을 가능하게 하여 모델의 표현력을 더욱 강화했다.23</li>
</ol>
<p>이러한 진화 과정을 아래 표 2에서 요약하여 비교할 수 있다.</p>
<table><thead><tr><th>항목</th><th>FPN</th><th>PANet</th><th>BiFPN</th></tr></thead><tbody>
<tr><td><strong>정보 흐름 방향</strong></td><td>하향식 (Top-down) 단방향</td><td>양방향 (하향식 + 상향식)</td><td>최적화된 양방향 (Optimized Bi-directional)</td></tr>
<tr><td><strong>핵심 아이디어</strong></td><td>고수준 의미 정보의 저수준 전파</td><td>저수준 위치 정보의 상향 전파 경로 추가</td><td>가중치 기반 융합 및 반복적 특징 융합</td></tr>
<tr><td><strong>특징 융합 방식</strong></td><td>요소별 덧셈 (Element-wise Sum)</td><td>요소별 덧셈 (Element-wise Sum)</td><td>빠른 정규화 가중합 (Fast Normalized Weighted Sum)</td></tr>
<tr><td><strong>주요 연결</strong></td><td>측면 연결 (Lateral Connection)</td><td>측면 연결 + 상향식 경로 연결</td><td>교차 스케일 가중 연결 + 동일 레벨 지름길</td></tr>
<tr><td><strong>구조적 특징</strong></td><td>단일 하향식 패스</td><td>단일 하향식 + 상향식 패스</td><td>반복 가능한 양방향 블록 구조</td></tr>
</tbody></table>
<h2>5. 결론: PANet의 기여와 의의</h2>
<h3>5.1  PANet의 핵심 기여 요약</h3>
<p>PANet은 객체 탐지 아키텍처의 역사에서 중요한 전환점을 제시한 모델로, 그 기여는 크게 두 가지로 요약할 수 있다.</p>
<p>첫째, PANet은 FPN이 제시한 특징 피라미드 개념의 내재적 한계, 즉 단방향 정보 흐름 문제를 명확히 지적하고, <strong>상향식 경로 증강</strong>이라는 구체적인 해결책을 통해 <strong>양방향 정보 흐름</strong>이라는 새로운 패러다임을 확립했다. 이는 저수준의 정밀한 위치 정보가 고수준의 의미 정보와 동등하게 중요하며, 이 두 정보가 네트워크 내에서 원활하게 교환되어야 최적의 성능을 달성할 수 있음을 입증한 것이다.</p>
<p>둘째, <strong>적응형 특징 풀링</strong>을 통해 객체 제안의 크기와 무관하게 모든 스케일의 특징 정보를 종합적으로 활용하는 방식을 제안함으로써, 특징 피라미드의 정보 활용도를 극대화했다. 이는 FPN의 휴리스틱한 규칙 기반 할당 방식에서 벗어나, 데이터 기반으로 네트워크가 스스로 최적의 정보를 선택하고 융합하도록 유도한 중요한 진전이었다.</p>
<h3>5.2  현대 컴퓨터 비전 아키텍처에 남긴 유산</h3>
<p>PANet의 영향력은 발표 이후 현재까지도 이어지고 있으며, 현대 컴퓨터 비전 아키텍처에 깊은 유산을 남겼다. PANet이 제안한 양방향 구조는 YOLO 계열을 필두로 수많은 고성능 객체 탐지 모델에서 ‘넥’ 부분의 기본 설계 철학으로 채택되며, 특징 융합 네트워크 연구의 중요한 이정표가 되었다.</p>
<p>더 나아가 PANet의 성공은 단순히 백본 네트워크를 깊고 크게 만드는 것만이 능사가 아니라는 점을 명확히 보여주었다. 백본에서 추출된 다중 스케일 특징을 ’어떻게 효과적으로 융합하고 전파하는가’가 모델의 최종 성능에 지대한 영향을 미친다는 사실을 각인시킨 것이다. 이는 후속 연구들이 백본, 넥, 헤드를 아우르는 전체적인 네트워크 아키텍처의 균형과 최적화에 더욱 집중하게 만드는 중요한 계기가 되었다.</p>
<p>결론적으로, PANet은 FPN의 개념을 한 단계 발전시키고, BiFPN과 같은 더욱 정교하고 효율적인 후속 아키텍처의 등장을 촉발한 핵심적인 연결고리이다. 이는 현대의 고효율, 고성능 객체 탐지 네트워크의 기틀을 마련한 선구적인 아키텍처로서 그 의의를 높게 평가받아야 한다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Feature Pyramid Network (FPN) - GeeksforGeeks, https://www.geeksforgeeks.org/computer-vision/feature-pyramid-network-fpn/</li>
<li>(PDF) Adaptive Feature Pyramid Networks for Object Detection - ResearchGate, https://www.researchgate.net/publication/353470279_Adaptive_Feature_Pyramid_Networks_for_Object_Detection</li>
<li>Understanding Feature Pyramid Networks for object detection (FPN) | by Jonathan Hui, https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c</li>
<li>FPN Explained — Feature Pyramid Network | by Amit Yadav - Medium, https://medium.com/@amit25173/fpn-explained-feature-pyramid-network-7c0f65ea8f8b</li>
<li>Feature Pyramid Networks for Object Detection - CVF Open Access, https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf</li>
<li>Feature Pyramid Networks for Object Detection | Facebook AI Research - AI at Meta, https://ai.meta.com/research/publications/feature-pyramid-networks-for-object-detection/</li>
<li>Path Aggregation Network for Instance … - CVF Open Access, https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Path_Aggregation_Network_CVPR_2018_paper.pdf</li>
<li>Architecture of path aggregation network (PANet). - ResearchGate, https://www.researchgate.net/figure/Architecture-of-path-aggregation-network-PANet_fig2_363281339</li>
<li>Reading: PANet — Path Aggregation Network, 1st Place in COCO 2017 Challenge (Instance Segmentation) - Becoming Human: Artificial Intelligence Magazine, https://becominghuman.ai/reading-panet-path-aggregation-network-1st-place-in-coco-2017-challenge-instance-segmentation-fe4c985cad1b</li>
<li>[PDF] Path Aggregation Network for Instance Segmentation - Semantic Scholar, https://www.semanticscholar.org/paper/Path-Aggregation-Network-for-Instance-Segmentation-Liu-Qi/5cc22f65bf4e5c0aa61f3139da832d3a946e15cf</li>
<li>[1803.01534] Path Aggregation Network for Instance Segmentation - arXiv, https://arxiv.org/abs/1803.01534</li>
<li>Path Aggregation Network for Instance Segmentation | Request PDF - ResearchGate, https://www.researchgate.net/publication/323571076_Path_Aggregation_Network_for_Instance_Segmentation</li>
<li>Ultralytics YOLOv5 Architecture, https://docs.ultralytics.com/yolov5/tutorials/architecture_description/</li>
<li>Comparison of the proposed YOLOv4 and other state-of-the-art object… - ResearchGate, https://www.researchgate.net/figure/Comparison-of-the-proposed-YOLOv4-and-other-state-of-the-art-object-detectors-YOLOv4_fig1_340883401</li>
<li>YOLOv4: High-Speed and Precise Object Detection - Ultralytics Docs, https://docs.ultralytics.com/models/yolov4/</li>
<li>Overall structure of YOLOv4, including CSPDarknet (backbone), SPPnet - ResearchGate, https://www.researchgate.net/figure/Overall-structure-of-YOLOv4-including-CSPDarknet-backbone-SPPnet-PANet-and-3-YOLO_fig2_344919620</li>
<li>YOLOv1 to YOLOv11: A Comprehensive Survey of Real-Time Object Detection Innovations and Challenges - arXiv, https://arxiv.org/html/2508.02067v1</li>
<li>Evaluating YOLOv4 and YOLOv5 for Enhanced Object Detection in UAV-Based Surveillance, https://www.mdpi.com/2227-9717/13/1/254</li>
<li>YOLOv8 Explained: Understanding Object Detection from Scratch | by Mélissa Colin, https://medium.com/@melissa.colin/yolov8-explained-understanding-object-detection-from-scratch-763479652312</li>
<li>What is YOLOv8: An In-Depth Exploration of the Internal Features of the Next-Generation Object Detector - arXiv, https://arxiv.org/html/2408.15857v1</li>
<li>A novel algorithm for small object detection based on YOLOv4 - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC10280595/</li>
<li>YOLO-ReT: Towards High Accuracy Real-Time Object Detection on Edge GPUs - CVF Open Access, https://openaccess.thecvf.com/content/WACV2022/papers/Ganesh_YOLO-ReT_Towards_High_Accuracy_Real-Time_Object_Detection_on_Edge_GPUs_WACV_2022_paper.pdf</li>
<li>EfficientDet: Scalable and Efficient Object Detection | by Aakash Nain | Medium, https://medium.com/@nainaakash012/efficientdet-scalable-and-efficient-object-detection-ea05ccd28427</li>
<li>Review — EfficientDet: Scalable and Efficient Object Detection | by Sik-Ho Tsang - Medium, https://medium.com/codex/review-efficientdet-scalable-and-efficient-object-detection-ed9ebc70f873</li>
<li>[19.11] EfficientDet - DOCSAID, https://docsaid.org/en/papers/feature-fusion/bifpn/</li>
<li>EfficientDet: Scalable and Efficient Object Detection - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2020/papers/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.pdf</li>
<li>Multi-Scale Residual Aggregation Feature Pyramid Network for Object Detection - MDPI, https://www.mdpi.com/2079-9292/12/1/93</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>