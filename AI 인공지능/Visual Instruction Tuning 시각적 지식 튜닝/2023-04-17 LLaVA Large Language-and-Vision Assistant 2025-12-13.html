<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:LLaVA Large Language-and-Vision Assistant (2023-04-17)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>LLaVA Large Language-and-Vision Assistant (2023-04-17)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">시각적 지시 튜닝 (Visual Instruction Tuning)</a> / <span>LLaVA Large Language-and-Vision Assistant (2023-04-17)</span></nav>
                </div>
            </header>
            <article>
                <h1>LLaVA Large Language-and-Vision Assistant (2023-04-17)</h1>
<p>2025-12-13, G30DR</p>
<h2>1.  서론: 인공지능의 멀티모달 확장과 오픈소스의 반격</h2>
<p>인공지능 연구의 흐름은 텍스트라는 단일 양식(Modality)을 처리하는 대규모 언어 모델(LLM)의 시대를 지나, 시각, 청각 등 다양한 감각 정보를 통합적으로 이해하고 추론하는 멀티모달 대규모 언어 모델(MLLM, Multimodal Large Language Models)의 시대로 급격히 이동하고 있다. GPT-4V(Vision)와 Google의 Gemini와 같은 선도적인 독점 모델들이 보여준 혁신적인 능력은 텍스트와 이미지의 경계를 허물며 인공지능이 인간과 유사한 방식으로 세상을 인지할 수 있는 가능성을 열었다. 그러나 이러한 거대 상용 모델들은 닫힌 생태계 안에서 운영되며, 그 내부 구조와 학습 방법론은 ’블랙박스’로 남아 있어 학계와 연구 커뮤니티의 접근을 제한하는 한계를 지니고 있었다. 이러한 배경 속에서 등장한 <strong>LLaVA(Large Language-and-Vision Assistant)</strong> 프로젝트는 오픈소스 진영에서 멀티모달 연구의 새로운 지평을 연 기념비적인 작업으로 평가받는다.1</p>
<p>LLaVA는 단순히 이미지를 보고 객체를 분류하는 기존의 컴퓨터 비전(Computer Vision) 모델과는 근본적으로 다른 접근 방식을 취한다. 이 모델은 사용자의 복잡하고 다양한 언어적 지시(Instruction)를 이해하고, 그 맥락 안에서 시각 정보를 해석하여 논리적인 답변을 생성하는 ’시각적 지시 튜닝(Visual Instruction Tuning)’이라는 개념을 정립하였다.1 이는 텍스트 도메인에서 InstructGPT나 ChatGPT가 보여준 ‘지시 따르기(Instruction Following)’ 능력을 시각 도메인으로 확장한 것으로, 사용자가 마치 사람과 대화하듯 이미지에 대해 질문하고, 토론하고, 복잡한 추론을 요청할 수 있게 만든다.</p>
<p>본 보고서는 LLaVA 모델의 초기 아키텍처 설계부터 LLaVA-1.5를 거쳐 최신 LLaVA-NeXT(v1.6)에 이르는 기술적 진화 과정을 포괄적이고 심층적으로 분석한다. 특히 LLaVA가 제시한 혁신적인 데이터 생성 방법론, 즉 텍스트 전용 GPT-4를 활용하여 멀티모달 학습 데이터를 구축한 파이프라인과, 사전 학습된 비전 인코더와 LLM을 효율적으로 연결하는 아키텍처 전략을 상세히 고찰한다. 또한, 다양한 벤치마크를 통해 증명된 모델의 성능과 그 한계, 그리고 이를 극복하기 위한 최신 연구 동향과 미래 전망까지 아우르는 백서 수준의 분석을 제공하는 것을 목적으로 한다.</p>
<h2>2.  이론적 배경 및 기반 기술</h2>
<p>LLaVA의 성공을 이해하기 위해서는 그 기반이 되는 두 가지 핵심 기술, 즉 비전 인코더로서의 CLIP과 언어 모델로서의 Vicuna, 그리고 이들을 결합하는 멀티모달 학습의 이론적 배경을 이해해야 한다. LLaVA는 바닥부터 새로운 모델을 학습시키는 것이 아니라, 이미 강력한 성능이 검증된 모듈들을 효과적으로 ’연결’하고 ’조율’하는 전략을 취함으로써 효율성을 극대화했다.</p>
<h3>2.1  비전 인코더: CLIP과 시각적 특징의 추출</h3>
<p>LLaVA는 시각적 입력을 처리하기 위해 OpenAI가 개발한 <strong>CLIP(Contrastive Language-Image Pre-training)</strong> 모델의 비전 인코더를 활용한다. CLIP은 인터넷상의 방대한 이미지-텍스트 쌍(4억 개 이상)을 사용하여 대조 학습(Contrastive Learning)을 수행한 모델이다. 대조 학습의 핵심은 이미지와 그에 대응하는 텍스트 캡션이 임베딩 공간상에서 서로 가깝게 위치하도록, 그리고 대응하지 않는 쌍은 서로 멀어지도록 학습하는 것이다.</p>
<p>LLaVA-v1에서는 CLIP의 ViT-L/14(Vision Transformer Large, 패치 크기 14x14) 모델을 비전 백본(Backbone)으로 사용한다.1 ViT(Vision Transformer)는 이미지를 작은 패치 단위로 나누어 트랜스포머의 입력으로 사용하는데, 이 과정에서 이미지의 전역적(Global)인 문맥 정보와 지역적(Local)인 특징을 동시에 포착할 수 있다. LLaVA는 이 사전 학습된 CLIP의 시각적 특징(Visual Features)을 그대로 활용하며, 초기 학습 단계에서는 이 인코더의 가중치(Weights)를 동결(Freeze)하여 시각적 표현 능력이 손상되는 것을 방지한다. 이는 LLaVA가 적은 양의 데이터로도 빠르게 수렴하고, 일반적인 객체 인식 능력을 유지할 수 있는 근간이 된다.3</p>
<h3>2.2  대규모 언어 모델 백본: Vicuna</h3>
<p>시각적 특징을 입력받아 인간의 언어로 해석하고 답변을 생성하는 두뇌 역할은 <strong>Vicuna</strong> 모델이 담당한다. Vicuna는 Meta의 LLaMA 모델을 기반으로 하여, ShareGPT 등에서 수집된 사용자-ChatGPT 간의 대화 데이터로 파인 튜닝(Fine-tuning)된 모델이다. Vicuna는 당시 오픈소스 모델 중에서는 GPT-4에 가장 근접한 대화 능력과 추론 능력을 보여주었으며, 특히 사용자의 의도를 파악하고 자연스러운 대화 톤을 유지하는 데 강점이 있었다.1</p>
<p>LLaVA는 Vicuna를 LLM 백본으로 채택함으로써, 모델이 단순히 이미지 태깅이나 짧은 캡셔닝에 머무르지 않고, 복잡한 문장 구조를 이해하고 논리적인 서술이 가능하도록 설계되었다. LLaVA 연구진은 7B(70억 파라미터)와 13B(130억 파라미터) 크기의 Vicuna 모델을 주로 사용하였으며, 이는 연구용 GPU 환경에서도 학습 및 추론이 가능한 적절한 크기였다.</p>
<h3>2.3  시각적 지시 튜닝 (Visual Instruction Tuning)의 개념</h3>
<p>기존의 멀티모달 모델 연구는 주로 이미지-캡션 쌍을 대량으로 학습하여 이미지와 텍스트의 정렬(Alignment)을 맞추는 데 집중했다. 그러나 이러한 방식은 모델이 이미지를 설명할 수는 있어도, “이 이미지 속의 남자가 들고 있는 도구의 위험성은 무엇인가?“와 같은 복잡한 지시나 추론 요구에는 제대로 대응하지 못하는 한계가 있었다.</p>
<p>LLaVA가 제안한 <strong>시각적 지시 튜닝</strong>은 언어 모델의 ‘지시 튜닝(Instruction Tuning)’ 방법론을 시각 도메인으로 확장한 것이다. 즉, 모델에게 단순한 이미지-텍스트 쌍을 보여주는 것이 아니라, 이미지와 함께 “명령(Instruction)“을 입력으로 주고, 그 명령에 따른 적절한 “응답(Response)“을 출력하도록 학습시키는 것이다.1 이를 통해 모델은 시각 정보를 바탕으로 사용자의 의도에 맞는 다양한 작업을 수행할 수 있는 범용적인 능력을 획득하게 된다. 이는 전문적인 작업(Task-specific) 모델에서 범용(General-purpose) 어시스턴트로의 진화를 의미한다.</p>
<h2>3.  LLaVA v1: 아키텍처 및 데이터 생성의 혁신</h2>
<p>LLaVA의 첫 번째 버전(v1)은 복잡한 구조를 지양하고, 가장 단순하면서도 효과적인 아키텍처를 통해 시각적 지시 튜닝의 가능성을 입증하는 데 주력했다. 특히 데이터 부족 문제를 해결하기 위해 고안된 데이터 생성 파이프라인은 이 연구의 가장 독창적인 기여 중 하나이다.</p>
<h3>3.1  아키텍처 설계: 단순함의 미학</h3>
<p>LLaVA v1의 아키텍처는 매우 직관적이다. 핵심은 사전 학습된 비전 인코더(CLIP)와 대규모 언어 모델(Vicuna)을 연결하는 **투영 레이어(Projection Layer)**에 있다.</p>
<p>입력 이미지 <span class="math math-inline">X_v</span>가 주어졌을 때, 비전 인코더는 이를 시각적 특징 벡터 <span class="math math-inline">Z_v</span>로 변환한다. 이 <span class="math math-inline">Z_v</span>는 비전 모델의 임베딩 공간에 존재하므로, 언어 모델이 이를 직접 이해할 수는 없다. 따라서 LLaVA는 학습 가능한 투영 행렬 <span class="math math-inline">W</span>를 도입하여 시각적 특징을 언어 임베딩 공간 <span class="math math-inline">H_v</span>로 선형 변환(Linear Projection)한다.<br />
<span class="math math-display">
H_v = W \cdot Z_v
</span><br />
이렇게 변환된 시각적 토큰 <span class="math math-inline">H_v</span>는 언어적 지시(Instruction) <span class="math math-inline">X_q</span>의 토큰들과 결합(Concatenation)되어 LLM의 입력으로 들어간다. LLM은 이 결합된 입력을 바탕으로 응답 <span class="math math-inline">X_a</span>를 생성한다.3 초기 LLaVA에서는 이 연결 부위로 단순한 <strong>선형 레이어(Linear Layer)</strong> 하나만을 사용했다. 이는 모델의 구조적 복잡성을 최소화하면서도, 강력한 두 사전 학습 모델의 능력을 결합하기에 충분함을 실험적으로 증명했다.</p>
<h3>3.2  GPT-4를 활용한 데이터 생성: 텍스트에서 시각으로</h3>
<p>LLaVA 프로젝트 시작 당시에는 시각적 지시 튜닝을 위한 대규모 데이터셋이 전무했다. 기존의 COCO나 CC3M 같은 데이터셋은 단순한 이미지-캡션 쌍으로만 구성되어 있어, 복잡한 대화나 추론을 학습시키기에는 부적합했다. 연구진은 이를 해결하기 위해 텍스트 전용 모델인 GPT-4를 활용하여 멀티모달 데이터를 생성하는 기발한 방법을 고안했다.1</p>
<p>핵심 아이디어는 이미지를 텍스트로 치환하여 GPT-4에 제공하는 것이다. 연구진은 COCO 데이터셋의 이미지를 (1) **캡션(Caption)**과 (2) 객체의 종류 및 위치를 나타내는 <strong>바운딩 박스(Bounding Box)</strong> 좌표 리스트라는 **심볼릭 표현(Symbolic Representation)**으로 변환했다. GPT-4는 이미지를 직접 볼 수는 없지만, 이 텍스트 정보를 바탕으로 이미지의 장면을 머릿속으로 재구성할 수 있다.</p>
<p>연구진은 GPT-4에게 다음과 같은 세 가지 유형의 데이터를 생성하도록 프롬프팅했다.6</p>
<ol>
<li><strong>대화(Conversation):</strong> 이미지 속 상황에 대해 사람과 AI가 주고받는 자연스러운 문답. 예를 들어, “이 사진 속의 사람들은 무엇을 하고 있나요?”, “오른쪽에 있는 물건은 무엇인가요?“와 같은 질문과 답변을 생성한다.</li>
<li><strong>상세 묘사(Detailed Description):</strong> 이미지의 전반적인 분위기, 객체들의 세밀한 특징, 배경 등을 매우 자세하고 길게 서술하도록 유도한다. 이는 모델이 이미지의 세부 정보를 놓치지 않고 포착하는 능력을 기르는 데 도움을 준다.</li>
<li><strong>복잡한 추론(Complex Reasoning):</strong> 단순한 사실 확인을 넘어, 이미지 속 상황의 인과 관계나 논리적 추론을 요구하는 질문을 생성한다. 예를 들어, “이 트럭이 싣고 있는 짐의 상태를 볼 때, 이 트럭은 어디로 가고 있을 가능성이 높은가?“와 같은 고차원적인 질문들이다.</li>
</ol>
<p>이 과정을 통해 총 158,000개(158K)의 고품질 멀티모달 지시 데이터셋(<strong>LLaVA-Instruct-158K</strong>)이 구축되었으며, 이는 LLaVA가 GPT-4 수준의 대화 능력을 모방하는 데 결정적인 역할을 했다.1</p>
<h3>3.3  2단계 훈련 전략 (Two-Stage Training Strategy)</h3>
<p>LLaVA의 학습은 효율성을 극대화하기 위해 두 단계로 나누어 진행된다.4</p>
<ul>
<li>1단계: 특징 정렬을 위한 사전 훈련 (Feature Alignment Pre-training):</li>
</ul>
<p>이 단계의 목표는 시각적 토크나이저(커넥터)가 이미지를 LLM이 이해할 수 있는 언어적 형태로 올바르게 변환하도록 가르치는 것이다. CC3M 데이터셋에서 필터링된 약 595,000개의 이미지-캡션 쌍을 사용한다. 이때 비전 인코더와 LLM의 가중치는 모두 고정(Freeze)하고, 오직 투영 레이어(Projection Layer) 만을 학습시킨다. 이를 통해 시각적 특징과 언어 임베딩 간의 기본적인 정렬(Alignment)이 이루어진다.3</p>
<ul>
<li>2단계: 종단간 파인 튜닝 (End-to-End Fine-tuning):</li>
</ul>
<p>실질적인 지시 따르기 능력을 학습하는 단계이다. 1단계에서 학습된 투영 레이어와 LLM 백본의 가중치를 모두 업데이트한다(비전 인코더는 여전히 고정). 앞서 생성한 158K의 LLaVA-Instruct 데이터셋을 사용하여 모델을 학습시키며, 이 과정을 통해 모델은 사용자의 질문 의도를 파악하고, 시각 정보를 근거로 논리적인 답변을 생성하는 능력을 갖추게 된다.2</p>
<h2>4.  LLaVA-1.5: 단순한 수정으로 이룬 비약적 성능 향상</h2>
<p>LLaVA v1의 성공 이후, 연구진은 2023년 하반기에 <strong>LLaVA-1.5</strong>를 발표했다. LLaVA-1.5는 구조적으로 매우 큰 변화를 주기보다는, 데이터 품질 향상, 해상도 증가, 커넥터 개선 등 몇 가지 “단순한 수정(Simple Modifications)“만으로 당시 11개의 벤치마크에서 SOTA(State-of-the-Art)를 달성하는 기염을 토했다.1</p>
<h3>4.1  MLP 커넥터의 도입과 표현력 강화</h3>
<p>LLaVA v1에서 사용된 단일 선형 레이어 커넥터는 효율적이었으나, 시각적 정보의 복잡한 비선형적 관계를 언어 모델에 전달하는 데에는 한계가 있었다. LLaVA-1.5는 이를 **2층 구조의 MLP(Multi-Layer Perceptron)**로 교체했다.5 MLP는 두 개의 선형 레이어 사이에 GELU 활성화 함수를 포함하는 구조로, 시각적 특징을 언어 임베딩 공간으로 매핑할 때 훨씬 더 풍부하고 유연한 표현력을 제공한다. 연구 결과, 이 단순한 변화만으로도 멀티모달 이해 능력이 유의미하게 향상됨이 확인되었다.10</p>
<h3>4.2  입력 해상도 증가와 OCR 성능 개선</h3>
<p>기존 LLaVA v1은 CLIP의 기본 해상도인 224x224 픽셀 이미지를 사용했다. 이는 일반적인 객체 인식에는 충분할 수 있으나, 이미지 내의 작은 글자를 읽어야 하는 OCR(광학 문자 인식) 작업이나 세밀한 디테일을 파악하는 데에는 턱없이 부족했다. LLaVA-1.5는 이를 개선하기 위해 <strong>CLIP-ViT-L-336px</strong> 모델을 도입하여 입력 해상도를 336x336으로 증가시켰다.11</p>
<p>해상도의 증가는 단순히 픽셀 수가 늘어나는 것을 넘어, 모델이 처리하는 시각적 토큰(Visual Token)의 수를 증가시킨다. 224 해상도에서는 256개의 토큰이 생성되던 것이, 336 해상도에서는 576개의 토큰으로 늘어났다(패치 크기 14x14 기준). 더 많은 토큰은 더 세밀한 시각적 정보를 LLM에 전달하게 되며, 이는 특히 문서 이해, 차트 분석, 작은 객체 탐지 성능을 획기적으로 끌어올리는 결과를 낳았다.9</p>
<h3>4.3  학술적 데이터의 통합: LLaVA-Mix-665K</h3>
<p>데이터 측면에서도 큰 변화가 있었다. GPT-4로 생성한 데이터는 대화 능력에는 탁월했지만, VQA(Visual Question Answering)와 같이 짧고 정확한 답변을 요구하는 학술적 벤치마크에서는 다소 장황하게 답변하는 경향이 있었다. 이를 보완하기 위해 LLaVA-1.5는 <strong>LLaVA-Mix-665K</strong>라는 새로운 데이터셋 혼합물을 구성했다.12</p>
<p>이 데이터셋은 기존의 158K 데이터에 더해 다음과 같은 데이터들을 통합했다.</p>
<ul>
<li><strong>VQAv2:</strong> 자연 이미지에 대한 질의응답 데이터.</li>
<li><strong>GQA:</strong> 장면 그래프(Scene Graph) 기반의 논리적 추론 및 공간 관계 데이터.</li>
<li><strong>OCRVQA / TextVQA:</strong> 이미지 내 텍스트 인식 및 독해 능력 강화를 위한 데이터.</li>
<li><strong>ShareGPT:</strong> 순수 텍스트 대화 데이터로, 모델의 언어적 일반화 능력을 유지.</li>
</ul>
<p>특히, 연구진은 답변 포맷을 제어하기 위해 <strong>응답 포맷 프롬프트(Response Formatting Prompts)</strong> 전략을 사용했다. 예를 들어, 단답형 답변이 필요한 경우 질문 뒤에 <code>Answer the question using a single word or phrase.</code>라는 지시어를 추가하여 학습시켰다. 이를 통해 하나의 모델로 서술형 대화와 단답형 퀴즈를 모두 유연하게 처리할 수 있게 되었다.14</p>
<h3>4.4  환각(Hallucination) 억제와 POPE 벤치마크</h3>
<p>멀티모달 모델의 고질적인 문제 중 하나는 이미지에 없는 객체를 있다고 대답하는 ‘환각’ 현상이다. LLaVA-1.5는 이를 해결하기 위해 학습 데이터에 “존재하지 않는 객체“에 대해 묻고 “없음“이라고 대답하는 데이터를 명시적으로 포함시켰다. 그 결과, 객체 존재 유무를 묻는 <strong>POPE(Polling on Object Existence)</strong> 벤치마크에서 매우 높은 점수(85.9%)를 기록하며 신뢰성을 입증했다.15</p>
<h2>5.  LLaVA-NeXT (v1.6): 고해상도와 논리적 추론의 완성</h2>
<p>2024년 1월, LLaVA 팀은 LLaVA-1.5의 성공을 바탕으로 성능을 더욱 극대화한 **LLaVA-NeXT (v1.6)**를 공개했다. “More Pixels, More Reasoning“이라는 슬로건은 이 모델의 지향점을 명확히 보여준다. LLaVA-NeXT는 모델의 크기를 키우고(Scaling up), 혁신적인 해상도 처리 기술을 도입하여 GPT-4V와 Gemini Pro와 같은 상용 모델들과 직접 경쟁할 수 있는 수준에 도달했다.17</p>
<h3>5.1  AnyRes: 동적 고해상도 처리 메커니즘</h3>
<p>LLaVA-NeXT의 가장 핵심적인 기술 혁신은 <strong>AnyRes(Any Resolution)</strong> 기술이다. 기존 모델들은 이미지를 고정된 해상도(예: 336x336)로 강제로 리사이징하거나 잘라내어(Crop) 처리했다. 이 과정에서 이미지의 비율이 왜곡되거나 중요한 정보가 손실되는 문제가 발생했다.</p>
<p>AnyRes는 <strong>그리드 분할(Grid Splitting)</strong> 방식을 통해 이를 해결한다.17</p>
<ol>
<li><strong>동적 분할:</strong> 입력 이미지의 종횡비(Aspect Ratio)와 해상도를 분석하여, 이미지를 여러 개의 패치(예: 336x336 크기)로 분할한다. 예를 들어, 세로로 긴 이미지는 1x2 그리드로, 가로로 긴 이미지는 2x1 그리드로, 고해상도 이미지는 2x2 그리드로 분할한다.</li>
<li><strong>개별 인코딩:</strong> 분할된 각 패치를 독립적으로 비전 인코더에 통과시켜 특징을 추출한다.</li>
<li><strong>글로벌 컨텍스트:</strong> 단순히 부분 이미지만 보면 전체적인 맥락을 놓칠 수 있으므로, 원본 이미지를 축소한 ’글로벌 썸네일(Global Thumbnail)’의 특징도 함께 추출한다.</li>
<li><strong>통합:</strong> 각 패치의 특징들과 글로벌 썸네일의 특징을 결합하여 LLM에 전달한다.</li>
</ol>
<p>이 방식을 통해 LLaVA-NeXT는 최대 **4배 더 많은 픽셀(672x672, 1344x336 등)**을 처리할 수 있게 되었다. 이는 빽빽한 문서의 작은 글씨를 읽거나, 복잡한 인포그래픽을 해석하는 데 있어 결정적인 성능 향상을 가져왔다.18</p>
<h3>5.2  모델 백본의 다양화 및 확장</h3>
<p>LLaVA-NeXT는 Vicuna에 국한되지 않고 다양한 최신 LLM을 백본으로 채택하여 성능을 확장했다.</p>
<ul>
<li><strong>Vicuna-1.5 (7B/13B):</strong> 기존의 안정적인 성능을 계승.</li>
<li><strong>Mistral-7B:</strong> 더 뛰어난 지시 따르기 능력과 논리적 추론 능력을 제공.</li>
<li><strong>Nous-Hermes-2-Yi-34B:</strong> 340억 파라미터의 대형 모델을 사용하여, 언어적 표현력과 상식 추론 능력을 대폭 강화.</li>
</ul>
<p>특히 34B 모델은 상용 모델인 Gemini Pro를 능가하는 벤치마크 결과를 보여주며, 오픈소스 모델의 잠재력을 과시했다.17</p>
<h3>5.3  비디오 및 멀티 이미지 이해 능력</h3>
<p>LLaVA-NeXT는 단일 이미지 처리를 넘어 비디오와 다중 이미지 처리로 영역을 확장했다. AnyRes 기술은 본질적으로 여러 장의 이미지 패치를 처리하는 방식이므로, 이를 비디오의 프레임 시퀀스에 적용하면 별도의 대규모 비디오 데이터 학습 없이도 강력한 비디오 이해(Zero-shot Video Understanding)가 가능하다.20 또한, <strong>인터리브(Interleave)</strong> 데이터 학습을 통해 텍스트와 이미지가 번갈아 나오는 블로그 게시물이나 복잡한 문서를 처리하는 능력도 갖추게 되었다.</p>
<h2>6.  성능 평가 및 벤치마크 상세 분석</h2>
<p>LLaVA 시리즈의 성능은 다양한 학술적 벤치마크를 통해 객관적으로 검증되었다. 각 벤치마크의 특성과 LLaVA의 성취를 상세히 분석한다.</p>
<h3>6.1  종합 벤치마크 비교</h3>
<p>다음은 LLaVA-1.5, LLaVA-NeXT, 그리고 주요 경쟁 모델들의 성능을 요약한 표이다.16</p>
<table><thead><tr><th><strong>벤치마크 (Metric)</strong></th><th><strong>설명</strong></th><th><strong>LLaVA-1.5 (13B)</strong></th><th><strong>LLaVA-NeXT (34B)</strong></th><th><strong>Gemini Pro</strong></th><th><strong>GPT-4V</strong></th></tr></thead><tbody>
<tr><td><strong>VQAv2</strong></td><td>일반적인 이미지 질의응답</td><td>80.0</td><td><strong>83.7</strong></td><td>71.2</td><td>77.2</td></tr>
<tr><td><strong>GQA</strong></td><td>공간/논리적 복합 추론</td><td>63.3</td><td><strong>67.1</strong></td><td>-</td><td>-</td></tr>
<tr><td><strong>ScienceQA</strong></td><td>과학 지식 및 멀티모달 추론</td><td>71.6</td><td><strong>81.8</strong></td><td>-</td><td>-</td></tr>
<tr><td><strong>TextVQA</strong></td><td>이미지 내 텍스트 인식 및 QA</td><td>61.3</td><td>69.5</td><td>74.6</td><td><strong>78.0</strong></td></tr>
<tr><td><strong>POPE</strong></td><td>객체 존재 유무 확인 (환각 평가)</td><td>85.9</td><td><strong>87.7</strong></td><td>-</td><td>-</td></tr>
<tr><td><strong>MM-Vet</strong></td><td>복합적 시각-언어 능력 통합 평가</td><td>35.4</td><td>57.4</td><td><strong>64.3</strong></td><td>-</td></tr>
<tr><td><strong>MMBench</strong></td><td>다지선다형 종합 평가</td><td>67.7</td><td><strong>79.3</strong></td><td>73.6</td><td>-</td></tr>
</tbody></table>
<h3>6.2  결과 해석 및 시사점</h3>
<ol>
<li><strong>일반 성능의 역전:</strong> LLaVA-NeXT-34B는 VQAv2, GQA, ScienceQA와 같은 전통적인 벤치마크에서 Gemini Pro와 GPT-4V를 상회하거나 대등한 점수를 기록했다. 이는 오픈소스 모델이 특정 도메인에서는 상용 모델을 충분히 대체할 수 있음을 강력하게 시사한다.</li>
<li><strong>OCR 성능의 격차:</strong> TextVQA에서 LLaVA-NeXT(69.5)는 전작 대비 크게 발전했으나, 여전히 GPT-4V(78.0)에는 미치지 못한다. 이는 상용 모델들이 보유한 압도적인 양의 독점 OCR 데이터와 모델 규모의 차이에서 기인하는 것으로 분석된다.</li>
<li><strong>복잡한 추론의 한계와 가능성:</strong> MM-Vet은 모델의 다양한 능력을 통합적으로 평가하는 까다로운 지표이다. 여기서 LLaVA-NeXT(57.4)는 Gemini Pro(64.3)에 뒤처지는데, 이는 아주 복잡한 상황에서의 뉘앙스 파악이나 초고난도 추론에서는 여전히 초대형 모델이 우위를 점하고 있음을 보여준다. 그러나 격차는 빠르게 좁혀지고 있다.</li>
<li><strong>데이터 효율성:</strong> LLaVA는 100만 개 미만의 데이터(약 665K~700K)로 학습되었음에도, 수십 억 개의 데이터로 학습된 모델들을 능가했다. 이는 “양보다 질“이라는 데이터 중심 AI(Data-Centric AI)의 철학이 멀티모달 분야에서도 유효함을 증명한다.</li>
</ol>
<h2>7.  배포 및 효율성: 하드웨어 최적화</h2>
<p>LLaVA는 누구나 사용할 수 있는 오픈소스 모델을 지향하므로, 실제 구동을 위한 하드웨어 효율성은 매우 중요한 요소이다.</p>
<h3>7.1  양자화(Quantization)와 VRAM 요구사항</h3>
<p>LLaVA는 FP16(16비트 부동소수점) 포맷을 기본으로 하지만, 커뮤니티의 노력으로 4비트 및 8비트 양자화 기술(bitsandbytes, GGUF, GPTQ)이 활발히 적용되고 있다. 이를 통해 고가의 서버급 GPU가 없는 개인 연구자나 개발자들도 소비자용 GPU에서 LLaVA를 구동할 수 있다.23</p>
<table><thead><tr><th><strong>모델</strong></th><th><strong>정밀도</strong></th><th><strong>최소 VRAM</strong></th><th><strong>권장 GPU 환경</strong></th></tr></thead><tbody>
<tr><td><strong>LLaVA-7B</strong></td><td>FP16</td><td>~14 GB</td><td>RTX 3090 / 4080 (16GB+)</td></tr>
<tr><td><strong>LLaVA-7B</strong></td><td>4-bit (GGUF)</td><td><strong>~6-8 GB</strong></td><td>RTX 3060 / 4060 / Laptop GPU</td></tr>
<tr><td><strong>LLaVA-13B</strong></td><td>FP16</td><td>~26 GB</td><td>RTX 3090 / 4090 / A100</td></tr>
<tr><td><strong>LLaVA-13B</strong></td><td>4-bit</td><td><strong>~10-12 GB</strong></td><td>RTX 3080 Ti / 4070</td></tr>
<tr><td><strong>LLaVA-34B</strong></td><td>4-bit</td><td><strong>~20-24 GB</strong></td><td>RTX 3090 / 4090 (24GB VRAM 필수)</td></tr>
</tbody></table>
<h3>7.2  추론 가속화 및 서빙 프레임워크</h3>
<p>LLaVA의 배포를 돕는 다양한 도구들이 존재한다.</p>
<ul>
<li><strong>SGLang:</strong> LLaVA 팀이 개발에 참여한 엔진으로, KV 캐시(Key-Value Cache) 공유 및 배치 처리 최적화를 통해 추론 속도를 획기적으로 높였다. 특히 비디오 처리와 같이 긴 컨텍스트가 필요한 작업에서 효율적이다.17</li>
<li><strong>llama.cpp:</strong> C++로 작성된 경량화 추론 엔진으로, Apple Silicon(Mac) 및 CPU 전용 환경에서도 놀라운 속도로 LLaVA를 구동할 수 있게 해준다.26</li>
<li><strong>Ollama:</strong> 복잡한 설정 없이 로컬에서 LLM/MLLM을 실행할 수 있는 도구로, LLaVA v1.5와 v1.6을 공식 지원하며 개발자들 사이에서 폭발적인 인기를 끌고 있다.</li>
</ul>
<h2>8.  LLaVA 생태계의 확장과 미래 전망</h2>
<p>LLaVA는 단일 모델을 넘어 하나의 거대한 생태계로 진화하고 있다. LLaVA의 아키텍처와 데이터 방법론을 기반으로 다양한 파생 연구들이 쏟아져 나오고 있다.</p>
<h3>8.1  특화 모델 및 파생 프로젝트</h3>
<ul>
<li><strong>LLaVA-Med:</strong> 생의학(Biomedical) 도메인에 특화된 모델로, PubMed 등의 의학 논문 이미지와 텍스트로 튜닝되어 의학적 질문에 대한 전문적인 답변을 제공한다.28</li>
<li><strong>TinyLLaVA / MobileVLM:</strong> 모바일 기기 탑재를 목표로 하는 소형 모델들로, 3B 이하의 파라미터로도 LLaVA의 핵심 기능을 수행하도록 경량화되었다.</li>
<li><strong>MoE-LLaVA:</strong> 전문가 혼합(Mixture of Experts) 아키텍처를 도입하여, 모델의 파라미터 수를 획기적으로 늘리면서도 실제 추론 시 연산량은 낮게 유지하는 효율적인 확장을 시도하고 있다.10</li>
<li><strong>LLaVA-Interactive / LLaVA-Plus:</strong> 단순히 보고 말하는 것을 넘어, 도구(Tool)를 사용하거나 GUI를 조작하고, 이미지를 생성 및 편집하는 에이전트(Agent)로서의 기능을 수행한다.11</li>
</ul>
<h3>8.2  한계점 및 극복 과제</h3>
<p>여전히 해결해야 할 과제들도 존재한다.</p>
<ul>
<li><strong>공간적 그라운딩(Spatial Grounding):</strong> “사과가 어디에 있는가?“라는 질문에 좌표로 정확히 답하는 능력은 전문적인 객체 탐지 모델에 비해 아직 부족하다.</li>
<li><strong>다국어 지원의 한계:</strong> 대부분의 학습 데이터가 영어로 구성되어 있어, 한국어를 포함한 비영어권 언어에 대한 멀티모달 이해도는 상대적으로 낮다. 이는 다국어 데이터셋 구축을 통해 해결해야 할 과제이다.</li>
<li><strong>고해상도의 비용:</strong> AnyRes 기술은 성능을 높이지만, 이미지 패치 수가 늘어날수록 연산 비용과 메모리 사용량이 급증한다. 이를 효율화하기 위한 토큰 압축 기술(Token Compression) 연구가 활발히 진행 중이다.30</li>
</ul>
<h2>9.  결론: 오픈소스 멀티모달 AI의 표준을 제시하다</h2>
<p>LLaVA 프로젝트는 텍스트 중심의 언어 모델에 시각적 “눈“을 달아주는 가장 효율적이고 효과적인 방법론을 제시했다. 복잡하고 무거운 아키텍처 대신, 검증된 모듈을 <strong>단순한 커넥터</strong>로 연결하고 <strong>고품질의 지시 튜닝 데이터</strong>에 집중하는 전략은, 적은 자원으로도 상용 모델(GPT-4V)에 버금가는 성능을 낼 수 있음을 전 세계에 입증했다.</p>
<p>LLaVA-1.5에서의 안정화, LLaVA-NeXT에서의 고해상도 및 비디오 확장으로 이어지는 진화 과정은 오픈소스 AI 생태계의 역동성을 상징적으로 보여준다. 이제 LLaVA는 단순한 챗봇을 넘어, 로보틱스, 자율 주행, 의료 AI, 개인화 에이전트 등 다양한 산업 분야에서 멀티모달 지능의 핵심 엔진으로 활용되고 있다. 앞으로도 LLaVA는 연구자와 개발자들에게 멀티모달 AI의 원리를 이해하고 새로운 응용 분야를 개척하는 데 없어서는 안 될 나침반이자 강력한 도구로 남을 것이다. 이 모델이 열어젖힌 가능성은 이제 막 시작되었을 뿐이다.</p>
<h2>10. 참고 자료</h2>
<ol>
<li>LLaVA, https://llava-vl.github.io/</li>
<li>[2304.08485] Visual Instruction Tuning - arXiv, https://arxiv.org/abs/2304.08485</li>
<li>Understanding Visual Instruction Tuning - Gradient Ascent, https://newsletter.artofsaience.com/p/understanding-visual-instruction</li>
<li>Visual Instruction Tuning | OpenReview, https://openreview.net/forum?id=w0H2xGHlkw</li>
<li>LLaVA: Large Language and Vision Assistant Explained | Encord, https://encord.com/blog/llava-large-language-vision-assistant/</li>
<li>LLaVa and Visual Instruction Tuning Explained - Zilliz blog, https://zilliz.com/blog/llava-visual-instruction-training</li>
<li>LLaVA and LLaVA-1.5. Paper Reviews | by Eleventh Hour Enthusiast | Medium, https://medium.com/@EleventhHourEnthusiast/llava-and-llava-1-5-1cf8be377245</li>
<li>Learning to Instruct for Visual Instruction Tuning - arXiv, https://arxiv.org/html/2503.22215v1</li>
<li>Improved Baselines with Visual Instruction Tuning - arXiv, https://arxiv.org/html/2310.03744v2</li>
<li>MoE-LLaVA: Mixture of Experts for Large Vision-Language Models - arXiv, https://arxiv.org/html/2401.15947v1</li>
<li>haotian-liu/LLaVA: [NeurIPS’23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond. - GitHub, https://github.com/haotian-liu/LLaVA</li>
<li>kaiyuyue/llava-1.5-665k-genqa-500k-instructions · Datasets at Hugging Face, https://huggingface.co/datasets/kaiyuyue/llava-1.5-665k-genqa-500k-instructions</li>
<li>llava_v1_5_mix665k.json · liuhaotian/LLaVA-Instruct-150K at main - Hugging Face, https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_v1_5_mix665k.json</li>
<li>LLaVA/docs/Evaluation.md at main · haotian-liu/LLaVA - GitHub, https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md</li>
<li>Improved Baselines with Visual Instruction Tuning - Haotian Liu, https://static.hliu.cc/files/llava/improved_llava.pdf</li>
<li>Comparing with LLaVA 1.6 Next · Issue #1 · thunlp/LLaVA-UHD - GitHub, https://github.com/thunlp/LLaVA-UHD/issues/1</li>
<li>LLaVA-NeXT: Improved reasoning, OCR, and world knowledge, https://llava-vl.github.io/blog/2024-01-30-llava-next/</li>
<li>Large Language and Vision Assistant (LLaVA) — v1.6 vs. v1.5 | by Sulaiman Shamasna, https://medium.com/@sulaiman.shamasna/large-language-and-vision-assistant-llava-v1-6-vs-v1-5-ede06b81ab48</li>
<li>LLaVA 1.6 released, 34B model beating Gemini Pro : r/LocalLLaMA - Reddit, https://www.reddit.com/r/LocalLLaMA/comments/1afc751/llava_16_released_34b_model_beating_gemini_pro/</li>
<li>LLaVA-NeXT: A Strong Zero-shot Video Understanding Model, https://llava-vl.github.io/blog/2024-04-30-llava-next-video/</li>
<li>MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models for Integrated Capabilities - arXiv, https://arxiv.org/html/2408.00765v1</li>
<li>[ICCVW 25] LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning - GitHub, https://github.com/aimagelab/LLaVA-MORE</li>
<li>LLaVA/README.md at main · haotian-liu/LLaVA - GitHub, https://github.com/haotian-liu/LLaVA/blob/main/README.md</li>
<li>Explanation of 7b vs 13b and VRAM requirements : r/LocalLLM - Reddit, https://www.reddit.com/r/LocalLLM/comments/17gwzcg/explanation_of_7b_vs_13b_and_vram_requirements/</li>
<li>LLaVA-NeXT - Hugging Face, https://huggingface.co/docs/transformers/v4.45.2/model_doc/llava_next</li>
<li>liuhaotian/llava-v1.6-mistral-7b · What kind of GPU need to run this model locally on-prem ?, https://huggingface.co/liuhaotian/llava-v1.6-mistral-7b/discussions/8</li>
<li>liuhaotian/llava-v1.6-34b · What kind of GPU need to run this model locally on-prem ?, https://huggingface.co/liuhaotian/llava-v1.6-34b/discussions/17</li>
<li>LLaVA-VL/LLaVA-NeXT - GitHub, https://github.com/LLaVA-VL/LLaVA-NeXT</li>
<li>LLaVA: Large Language and Vision Assistant - Microsoft Research, https://www.microsoft.com/en-us/research/project/llava-large-language-and-vision-assistant/</li>
<li>LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token, https://arxiv.org/html/2501.03895v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>