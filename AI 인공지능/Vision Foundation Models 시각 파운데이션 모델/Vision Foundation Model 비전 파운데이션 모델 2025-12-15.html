<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:비전 파운데이션 모델 (Vision Foundation Model)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>비전 파운데이션 모델 (Vision Foundation Model)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">비전 파운데이션 모델 (Vision Foundation Model)</a> / <span>비전 파운데이션 모델 (Vision Foundation Model)</span></nav>
                </div>
            </header>
            <article>
                <h1>비전 파운데이션 모델 (Vision Foundation Model)</h1>
<p>2025-12-15, G30DR</p>
<h2>1.  서론: 시각 지능의 새로운 지평</h2>
<p>인공지능(AI)의 역사에서 컴퓨터 비전(Computer Vision)은 오랫동안 인간의 시각적 인지 능력을 모방하고자 하는 도전적인 과제였다. 초기 규칙 기반(Rule-based) 시스템에서부터 2010년대 딥러닝 혁명을 이끈 합성곱 신경망(Convolutional Neural Networks, CNN)에 이르기까지, 이 분야는 특정 작업을 해결하기 위한 특화된 모델 개발에 집중해 왔다. 그러나 2020년대를 기점으로, 방대한 데이터와 연산 자원을 바탕으로 사전 훈련(Pre-training)된 후 다양한 하류 작업(Downstream Tasks)에 유연하게 적응하는 **비전 파운데이션 모델(Vision Foundation Model, VFM)**의 등장은 이 패러다임을 근본적으로 변화시켰다.1</p>
<p>비전 파운데이션 모델은 텍스트, 이미지, 비디오, 3D 공간 정보 등 다양한 모달리티(Modality)를 통합적으로 이해하고 처리할 수 있는 능력을 갖추고 있다. 이는 단순히 이미지 내의 객체를 분류하는 수준을 넘어, 장면의 의미론적 맥락을 파악하고, 보이지 않는 영역을 추론하며, 물리적 세계의 법칙을 내재화하는 방향으로 진화하고 있다. 특히 자연어 처리(NLP) 분야에서 트랜스포머(Transformer) 아키텍처가 거둔 성공은 비전 분야로 전이되어, Vision Transformer(ViT)라는 새로운 표준을 확립했다.3</p>
<p>본 보고서는 비전 파운데이션 모델의 정의와 역사적 배경을 시작으로, CLIP, DINOv2, SAM, 그리고 최신 Llama 3.2 Vision과 같은 핵심 모델들의 기술적 메커니즘을 심층 분석한다. 또한, Yann LeCun이 제안한 JEPA(Joint-Embedding Predictive Architecture)와 Fei-Fei Li가 주창하는 공간 지능(Spatial Intelligence) 개념이 어떻게 차세대 ’월드 모델(World Model)’로 구체화되고 있는지 탐구한다. 나아가 헬스케어, 로보틱스, 자율주행 등 실제 산업 현장에서의 구체적인 적용 사례와 그 파급 효과를 면밀히 조사하여, 이 기술이 가져올 미래 사회의 변화를 조망한다.</p>
<h2>2.  컴퓨터 비전 모델의 진화: 아키텍처와 학습 패러다임의 전환</h2>
<p>비전 파운데이션 모델의 부상을 이해하기 위해서는 지난 10년간 컴퓨터 비전 모델이 겪어온 아키텍처의 진화 과정을 면밀히 살펴볼 필요가 있다. 이는 ’귀납적 편향(Inductive Bias)’을 중시하던 설계 철학에서, 데이터의 양과 모델의 크기를 통해 일반화 성능을 확보하는 ‘확장성(Scalability)’ 중심의 철학으로의 이동을 의미한다.</p>
<h3>2.1  합성곱 신경망(CNN): 지역적 특징 추출의 시대</h3>
<p>2012년 AlexNet의 등장 이후, CNN은 컴퓨터 비전의 표준 아키텍처로 자리 잡았다. ResNet, VGG, EfficientNet 등은 이미지의 지역적 특징(Local Features)을 추출하는 데 탁월한 성능을 보였다. CNN은 이미지가 픽셀 간의 지역적 상관관계가 높다는 가정(Inductive Bias) 하에 설계되었으며, 이는 적은 데이터로도 효율적인 학습을 가능하게 했다.5 그러나 CNN은 수용 영역(Receptive Field)이 제한적이어서 이미지 전체의 전역적 문맥(Global Context)을 파악하기 위해서는 깊은 레이어를 쌓아야만 했고, 이는 모델의 복잡도를 증가시키는 원인이 되었다. 또한, 대규모 데이터셋으로 확장할 때 모델의 파라미터 수 대비 성능 향상 폭이 포화(Saturation)되는 한계가 관찰되었다.</p>
<h3>2.2  비전 트랜스포머(ViT): 전역적 문맥과 확장성의 확보</h3>
<p>2020년 Google Research가 발표한 Vision Transformer(ViT)는 NLP 분야의 트랜스포머 아키텍처를 비전 분야에 도입한 혁신적인 시도였다. ViT는 이미지를 픽셀 단위가 아닌 고정된 크기(예: 16x16)의 패치(Patch)로 분할하고, 이를 선형 투영(Linear Projection)을 통해 임베딩 벡터로 변환한다.3</p>
<h4>2.2.1  자기 주의 메커니즘(Self-Attention)의 역할</h4>
<p>ViT의 핵심은 다중 헤드 자기 주의(Multi-Head Self-Attention, MSA) 메커니즘이다. 이를 통해 모델은 첫 번째 레이어부터 이미지 내의 모든 패치 간의 상호작용을 계산할 수 있으며, 이는 CNN이 깊은 레이어를 거쳐야만 얻을 수 있었던 전역적 수용 영역(Global Receptive Field)을 즉각적으로 확보하게 한다.3 각 패치는 자신의 위치와 상관없이 다른 모든 패치와의 유사도(Attention Score)를 기반으로 정보를 집계(Aggregate)하므로, 이미지 내의 장거리 의존성(Long-range dependencies)을 효과적으로 모델링할 수 있다.</p>
<h4>2.2.2  귀납적 편향의 제거와 데이터 효율성</h4>
<p>ViT는 CNN이 가진 지역성(Locality)이나 평행 이동 불변성(Translation Invariance)과 같은 귀납적 편향을 제거했다. 이는 작은 데이터셋에서는 CNN보다 성능이 떨어질 수 있음을 의미하지만(Overfitting 위험), JFT-300M과 같은 초대규모 데이터셋으로 사전 훈련할 경우 CNN을 능가하는 성능을 발휘한다.4 즉, ViT는 데이터가 충분하다면 모델 구조의 제약 없이 데이터 자체의 패턴을 학습할 수 있는 높은 확장성을 제공하며, 이것이 파운데이션 모델의 백본(Backbone)으로 ViT가 선택되는 결정적인 이유이다.</p>
<h3>2.3  마스크드 오토인코더(MAE): 비지도 학습의 효율화</h3>
<p>지도 학습(Supervised Learning)의 한계인 라벨링 비용 문제를 해결하기 위해, He et al.(2021)은 마스크드 오토인코더(Masked Autoencoders, MAE)를 제안했다.9 MAE는 입력 이미지 패치의 상당 부분(예: 75%)을 무작위로 마스킹(Masking)하고, 인코더는 보이는 패치(Visible Patches)만을 처리하게 한다. 이후 경량화된 디코더(Decoder)가 마스킹된 패치를 복원하도록 훈련된다.</p>
<ul>
<li><strong>의미론적 추론 강제:</strong> 픽셀의 단순한 보간(Interpolation)으로는 75%나 가려진 이미지를 복원할 수 없다. 따라서 모델은 이미지의 전체적인 구조, 객체의 형태, 텍스처의 패턴 등 고수준의 의미론적 정보를 학습해야만 한다.9</li>
<li><strong>훈련 효율성:</strong> 인코더가 전체 이미지가 아닌 25%의 패치만 처리하므로, 훈련 속도가 비약적으로 빨라지고 메모리 사용량이 감소한다. 이는 더 큰 모델(ViT-Large, ViT-Huge)을 훈련시키는 것을 가능하게 했다.12 MAE의 성공은 이후 등장하는 비전 파운데이션 모델들의 학습 방법론에 지대한 영향을 미쳤다.</li>
</ul>
<h2>3.  핵심 비전 파운데이션 모델의 기술적 해부</h2>
<p>2021년부터 2025년까지 비전 파운데이션 모델은 폭발적인 성장을 거듭했다. 이 섹션에서는 현재 생태계를 구성하는 주요 모델들인 CLIP, SAM, DINOv2의 아키텍처, 학습 방법론, 그리고 성능 특성을 심층적으로 분석한다.</p>
<h3>3.1  CLIP (Contrastive Language-Image Pre-training): 멀티모달 이해의 시발점</h3>
<p>OpenAI가 2021년 공개한 CLIP은 비전 모델과 언어 모델을 결합하여, 시각 정보와 텍스트 정보 사이의 의미론적 연결을 학습한 최초의 성공적인 대규모 모델이다.14</p>
<h4>3.1.1  대조 학습(Contrastive Learning) 메커니즘</h4>
<p>CLIP은 이미지 인코더(ResNet-50 또는 ViT)와 텍스트 인코더(Transformer)라는 두 개의 독립적인 신경망으로 구성된다. 훈련 데이터는 인터넷에서 수집한 4억 개의 이미지-텍스트 쌍(WebImageText)이다. CLIP의 학습 목표는 배치(Batch) 내에서 N개의 이미지와 N개의 텍스트가 있을 때, 올바른 쌍(Positive Pair) <span class="math math-inline">N</span>개의 코사인 유사도는 최대화하고, 잘못된 쌍(Negative Pair) <span class="math math-inline">N^2 - N</span>개의 유사도는 최소화하는 것이다.16</p>
<p>이 과정에서 모델은 이미지를 설명하는 텍스트의 의미를 시각적 특징과 정렬(Alignment)하게 된다. 이는 기존의 분류 모델이 사전에 정의된 1,000개의 클래스(ImageNet 등) 중 하나를 예측하는 ‘닫힌 집합(Closed-set)’ 문제에 갇혀 있었던 것과 달리, 자연어의 풍부한 표현력을 빌려 어떠한 시각적 개념도 표현할 수 있는 ‘열린 집합(Open-set)’ 문제로 비전 태스크를 확장시켰다.</p>
<h4>3.1.2  제로샷(Zero-shot) 성능과 견고성</h4>
<p>CLIP의 가장 큰 혁신은 <strong>제로샷 전이(Zero-shot Transfer)</strong> 능력이다. 훈련 과정에서 한 번도 본 적 없는 클래스라 하더라도, “A photo of a {object}“와 같은 프롬프트를 통해 분류가 가능하다.</p>
<ul>
<li><strong>성능 지표:</strong> 초기 ViT-B/32 기반 CLIP은 ImageNet 제로샷 정확도 63.2%를 기록했다.18 이후 모델 크기를 키운 ViT-L/14 버전은 76.2%를 달성했으며 18, 오픈 소스 진영인 LAION의 OpenCLIP 프로젝트를 통해 훈련된 ViT-G/14 모델은 2023년 기준 80.1%의 정확도를 달성하며 지도 학습 모델인 ResNet-50의 성능을 뛰어넘었다.19</li>
<li><strong>인간 유사성:</strong> CLIP의 오류 패턴은 인간의 시각적 인지 오류와 높은 상관관계를 보인다. 이는 모델이 단순히 픽셀 패턴을 암기하는 것이 아니라, 인간과 유사한 방식으로 시각적 개념을 추상화하고 있음을 시사한다.16</li>
</ul>
<h4>3.1.3  한계와 극복: 위치 정보와 픽셀 정밀도</h4>
<p>CLIP은 이미지 전체의 전역적 특징(Global Feature)을 텍스트와 매핑하는 데 최적화되어 있어, 이미지 내 객체의 정확한 위치(Localization)나 픽셀 단위의 분할(Segmentation) 능력은 상대적으로 부족하다.21 이를 보완하기 위해 MaskCLIP과 같은 후속 연구들은 마스킹 기법과 자기 증류를 결합하여 지역적 의미 학습 능력을 강화하고 있다.22</p>
<h3>3.2  DINOv2 (Self-Distillation with No Labels): 순수 시각적 특징의 정점</h3>
<p>Meta AI가 개발한 DINOv2는 텍스트 라벨 없이 이미지 자체만으로 강력한 시각적 특징(Visual Features)을 학습하는 자기 지도 학습(SSL) 모델의 정점이다.23</p>
<h4>3.2.1  자기 증류(Self-Distillation)와 교사-학생 네트워크</h4>
<p>DINOv2는 <strong>DINO(Self-distillation with no labels)</strong> 방법론을 계승한다. 이는 교사(Teacher) 네트워크와 학생(Student) 네트워크가 동일한 아키텍처를 공유하되, 학생은 교사의 출력을 모사하도록 훈련되는 방식이다.11</p>
<ul>
<li><strong>다중 뷰 학습:</strong> 입력 이미지에 대해 서로 다른 크롭(Crop)과 증강(Augmentation)을 적용하여, 학생 네트워크는 이미지의 국소적인 부분(Local view)을 보고도 교사 네트워크가 보는 전역적인 뷰(Global view)와 일치하는 특징을 추출하도록 강제된다.23</li>
<li><strong>파라미터 업데이트:</strong> 교사 네트워크의 파라미터는 역전파(Backpropagation)로 업데이트되지 않고, 학생 네트워크 파라미터의 지수 이동 평균(Exponential Moving Average, EMA)으로 서서히 업데이트된다. 이는 학습의 안정성을 높이고 특징의 붕괴(Collapse)를 방지한다.</li>
</ul>
<h4>3.2.2  데이터 큐레이션과 확장</h4>
<p>DINOv2의 성공 요인 중 하나는 데이터의 양보다 질에 집중한 <strong>LVD-142M</strong> 데이터셋의 구축이다. Meta 연구팀은 유사 이미지 검색을 통해 정제된 데이터셋을 구축했으며, 이를 통해 11억 개의 파라미터를 가진 ViT-Giant 모델을 효과적으로 훈련시켰다.25</p>
<h4>3.2.3  성능: 기하학적 이해와 밀집 예측</h4>
<p>DINOv2는 텍스트 설명에 의존하지 않기 때문에, 텍스트로 묘사하기 힘든 시각적 정보(예: 깊이, 질감, 형태의 미세한 구조)를 포착하는 데 탁월하다.</p>
<ul>
<li><strong>벤치마크:</strong> ImageNet 선형 평가(Linear Evaluation)에서 ViT-Giant 모델은 87.1%의 정확도를 기록했다.26</li>
<li><strong>응용:</strong> 깊이 추정(Depth Estimation)이나 시멘틱 세그멘테이션과 같은 밀집 예측(Dense Prediction) 태스크에서 별도의 미세 조정 없이도 최첨단(SOTA) 성능을 발휘한다.27 특히 의료 영상과 같이 전문적인 지식이 필요하거나 라벨이 없는 데이터에 대해 강력한 적응력을 보여준다.29</li>
</ul>
<h3>3.3  SAM (Segment Anything Model) &amp; SAM 2: 분할의 보편화</h3>
<p>Meta가 2023년 공개한 SAM은 컴퓨터 비전의 난제 중 하나인 세그멘테이션(Segmentation)을 ‘프롬프트’ 방식으로 해결함으로써, 모델의 범용성을 극대화했다.14</p>
<h4>3.3.1  프롬프트 가능한 분할(Promptable Segmentation)</h4>
<p>SAM은 NLP의 프롬프트 엔지니어링 개념을 도입했다. 사용자가 이미지 내의 특정 지점을 클릭(Point)하거나, 박스(Box)를 치거나, 텍스트로 명령하면 모델은 해당 객체의 정밀한 마스크(Mask)를 즉시 생성한다.</p>
<ul>
<li><strong>SA-1B 데이터셋:</strong> SAM의 강력한 성능은 1,100만 장의 이미지와 11억 개의 고품질 마스크로 구성된 SA-1B 데이터셋에 기인한다. 이는 기존 최대 규모 데이터셋 대비 400배 이상 큰 규모이다.30</li>
<li><strong>모호성 처리:</strong> 사용자의 입력이 모호할 경우(예: 사람을 클릭했는지, 그 사람이 입은 셔츠를 클릭했는지 불분명할 때), SAM은 단일 예측 대신 3가지 수준(전체, 부분, 하위 부분)의 마스크를 동시에 출력하여 모호성을 해결한다.31</li>
</ul>
<h4>3.3.2  SAM 2: 시간적 차원으로의 확장 (2025년 최신 동향)</h4>
<p>2024년 말에서 2025년 초 공개된 SAM 2는 정지 이미지를 넘어 비디오 세그멘테이션으로 영역을 확장했다.32</p>
<ul>
<li><strong>메모리 메커니즘:</strong> SAM 2는 이전 프레임의 정보를 저장하고 참조할 수 있는 메모리 뱅크(Memory Bank)를 도입했다. 이를 통해 비디오 내에서 객체가 잠시 사라졌다 나타나거나(Occlusion), 형태가 변형되더라도 일관되게 추적(Tracking)할 수 있다.32</li>
<li><strong>통합 아키텍처:</strong> SAM 2는 이미지를 ’길이가 1인 비디오’로 간주하여, 이미지와 비디오 분할 태스크를 단일 모델로 통합했다. 이는 비디오 편집, 자율주행 데이터 라벨링, 시각 효과(VFX) 산업에 즉각적인 혁신을 가져오고 있다.34</li>
</ul>
<p>아래 표는 주요 비전 파운데이션 모델들의 특성을 비교 요약한 것이다.</p>
<table><thead><tr><th><strong>모델 명</strong></th><th><strong>개발사</strong></th><th><strong>핵심 학습 방식</strong></th><th><strong>주요 입력 데이터</strong></th><th><strong>강점 및 특징</strong></th><th><strong>주요 응용 분야</strong></th></tr></thead><tbody>
<tr><td><strong>CLIP</strong></td><td>OpenAI</td><td>Contrastive Learning</td><td>이미지-텍스트 쌍</td><td>제로샷 분류, 텍스트 기반 검색</td><td>이미지 검색, 생성형 AI(DALL-E)</td></tr>
<tr><td><strong>DINOv2</strong></td><td>Meta AI</td><td>Self-Distillation (SSL)</td><td>이미지 (Unlabeled)</td><td>기하학적 구조 이해, 밀집 특징 추출</td><td>깊이 추정, 의료 영상 분석</td></tr>
<tr><td><strong>SAM 2</strong></td><td>Meta AI</td><td>Promptable Segmentation</td><td>이미지/비디오, 마스크</td><td>제로샷 분할, 비디오 객체 추적</td><td>이미지/비디오 편집, 데이터 라벨링</td></tr>
<tr><td><strong>MAE</strong></td><td>Meta AI</td><td>Masked Autoencoding</td><td>이미지 (Unlabeled)</td><td>높은 데이터 효율성, 확장성</td><td>파운데이션 모델 사전 훈련</td></tr>
</tbody></table>
<h2>4.  멀티모달 파운데이션 모델(LMM)과 오픈 웨이트 혁명</h2>
<p>비전 모델이 단독으로 존재하던 시기를 지나, 2024년과 2025년은 대규모 언어 모델(LLM)과 비전 모델이 융합된 **대규모 멀티모달 모델(Large Multimodal Model, LMM)**의 전성시대이다.</p>
<h3>4.1  GPT-4V 및 GPT-4o: 독점적 고성능 모델</h3>
<p>OpenAI의 GPT-4V(Vision)와 GPT-4o는 텍스트, 오디오, 이미지를 단일 모델 내에서 처리하는 멀티모달 능력을 보여주었다. 이들은 CLIP 스타일의 비전 인코더를 통해 시각 정보를 임베딩하고, 이를 LLM의 입력 토큰과 결합하여 추론한다. 연구 결과에 따르면, GPT-4V는 안과 질환 진단 등 고도의 전문 지식이 필요한 영역에서도 텍스트 전용 모델이나 기존 비전 모델보다 우수한 성능을 보이며, 인간 전문가 수준의 추론 능력을 입증했다.35 특히 복잡한 의료 차트나 이미지를 해석하고 진단을 보조하는 데 있어 높은 정확도를 기록했다.</p>
<h3>4.2  Gemini 1.5 Pro: 장기 문맥(Long Context)의 이해</h3>
<p>Google의 Gemini 1.5 Pro는 최대 100만~1000만 토큰에 이르는 컨텍스트 윈도우를 지원한다. 이는 1시간 분량의 비디오나 11시간 분량의 오디오를 한 번에 입력받아 처리할 수 있는 능력이다.36 기존 모델들이 비디오를 짧은 클립으로 나누어 처리하던 것과 달리, Gemini 1.5 Pro는 비디오 전체의 서사를 이해하고, 특정 장면을 검색하거나 내용을 요약하는 데 있어 압도적인 성능을 보여준다.37 이는 감시 카메라 영상 분석이나 장편 영화의 콘텐츠 분석 등 비디오 이해(Video Understanding) 분야에 새로운 가능성을 열었다.</p>
<h3>4.3  Llama 3.2 Vision: 오픈 소스 생태계의 확장</h3>
<p>2024년 9월, Meta는 Llama 3.2를 공개하며 11B 및 90B 파라미터 규모의 비전 모델을 포함시켰다.38 이는 고성능 멀티모달 모델의 가중치(Weights)를 대중에게 공개했다는 점에서 중요한 이정표이다.</p>
<ul>
<li><strong>학습 데이터:</strong> Llama 3.2 Vision은 60억 쌍의 이미지-텍스트 데이터로 사전 훈련되었으며, 300만 개의 합성 데이터를 포함한 시각적 지시 튜닝(Visual Instruction Tuning)을 거쳤다.40</li>
<li><strong>접근성:</strong> 폐쇄형 모델(Closed models)과 달리, Llama 3.2는 연구자와 기업이 자체 서버에 모델을 설치하고 미세 조정할 수 있게 함으로써, 데이터 보안이 중요한 금융, 의료, 국방 분야에서의 도입을 가속화하고 있다. 또한 11B 모델은 상대적으로 낮은 사양의 GPU에서도 구동 가능하여 엣지 디바이스로의 확장을 가능케 한다.41</li>
</ul>
<h2>5.  차세대 프런티어: 월드 모델(World Models)과 공간 지능</h2>
<p>비전 파운데이션 모델의 궁극적인 목표는 단순히 이미지를 인식하는 것을 넘어, 물리적 세계를 시뮬레이션하고 이해하는 **월드 모델(World Model)**로의 진화이다.</p>
<h3>5.1  Yann LeCun의 JEPA: 내부 세계 모델의 구축</h3>
<p>Meta의 수석 AI 과학자 Yann LeCun은 현재의 생성형 AI(Generative AI)가 픽셀 단위의 확률적 생성에 의존하는 것을 비판하며, **JEPA(Joint-Embedding Predictive Architecture)**를 대안으로 제시했다.43</p>
<ul>
<li><strong>생성 대 예측(Generative vs Predictive):</strong> Sora와 같은 비디오 생성 모델은 다음 프레임의 모든 픽셀을 예측하려 시도한다. 그러나 현실 세계는 불확실성으로 가득 차 있어 모든 세부 사항을 예측하는 것은 불가능하며 비효율적이다. 반면 JEPA는 추상적인 특징 공간(Embedding Space)에서 미래의 상태를 예측한다.44</li>
<li><strong>학습 메커니즘:</strong> JEPA는 비디오나 이미지의 일부를 마스킹하고, 모델이 마스킹된 부분의 ’의미론적 정보’를 예측하도록 훈련된다. 이 과정에서 모델은 배경의 사소한 움직임(예: 흔들리는 나뭇잎)은 무시하고, 객체의 물리적 상호작용과 같은 중요한 정보에 집중하는 법을 배운다. 이는 AI가 “컵을 놓으면 떨어진다“와 같은 인과관계를 내재화하는 과정이다.44</li>
<li><strong>JETS와 헬스케어:</strong> JEPA 아키텍처는 비전뿐만 아니라 시계열 데이터 분석에도 적용된다. JETS 모델은 Apple Watch 데이터의 불규칙성과 결측치를 JEPA 방식으로 처리하여, 라벨링된 데이터가 15%에 불과한 상황에서도 높은 정확도로 질병을 예측했다.43</li>
</ul>
<h3>5.2  Fei-Fei Li의 월드랩스(World Labs)와 공간 지능</h3>
<p>ImageNet의 창시자 Fei-Fei Li 교수는 2024년 ’월드랩스(World Labs)’를 설립하고 **공간 지능(Spatial Intelligence)**이라는 개념을 주창했다.46</p>
<ul>
<li><strong>공간 지능의 정의:</strong> 공간 지능은 AI가 3D 공간을 인식하고, 추론하며, 그 안에서 행동할 수 있는 능력을 말한다. 이는 언어와 2D 이미지 처리에 능숙한 현재 AI의 한계를 넘어, 물리적 현실과의 상호작용을 가능하게 하는 핵심 고리이다.47</li>
<li><strong>대규모 월드 모델(LWM):</strong> 월드랩스는 3D 환경의 기하학적 구조, 조명, 물리 법칙을 이해하는 LWM을 개발하고 있다. 이는 가상 세계를 생성하고 탐색 가능하게(Navigable) 만들 뿐만 아니라, 로봇이 실제 세계에서 작업을 수행하기 전 시뮬레이션 환경에서 학습하는 데 필수적인 기반이 된다.46</li>
</ul>
<h3>5.3  생성형 시뮬레이션 vs 예측형 추론</h3>
<p>현재 월드 모델 연구는 Sora와 같은 생성형 시뮬레이터와 JEPA와 같은 예측형 추론 모델로 양분되어 있다. Sora는 시각적으로 그럴듯한 영상을 생성하지만 물리적 일관성이 부족할 수 있고, JEPA는 물리적 인과관계 파악에 강점이 있지만 시각적 생성 능력은 제한적이다.49 향후 연구는 이 두 접근법을 융합하여, 시각적 충실도와 물리적 정확성을 모두 갖춘 모델을 개발하는 방향으로 나아갈 것이다.</p>
<h2>6.  인프라 및 학습 경제학</h2>
<p>비전 파운데이션 모델의 발전은 막대한 컴퓨팅 자원과 데이터 인프라에 의존하고 있다.</p>
<h3>6.1  학습 비용과 효율성</h3>
<p>CLIP이나 Llama 3.2 Vision과 같은 모델을 훈련시키는 데는 수천 개의 GPU와 수백만 달러의 비용이 소요된다. Gartner에 따르면 GPU 시간당 비용은 8~25달러에 달하며, 이는 모델 개발의 진입 장벽으로 작용한다.50 따라서 MAE와 같이 데이터 효율성을 극대화하거나, DINOv2와 같이 학습 과정을 최적화하여 훈련 비용을 절감하려는 시도가 지속되고 있다. 또한, OpenAI와 Anthropic 등은 ‘훈련 크레딧’ 시스템을 도입하여 고객들이 모델 훈련에 필요한 컴퓨팅 파워를 유연하게 구매할 수 있도록 하고 있다.50</p>
<h3>6.2  데이터셋의 진화</h3>
<ul>
<li><strong>LAION-5B:</strong> 오픈 소스 커뮤니티가 구축한 58억 개의 이미지-텍스트 쌍 데이터셋으로, Stable Diffusion 등 다양한 모델의 훈련에 사용되었다.</li>
<li><strong>SA-1B:</strong> Meta가 공개한 1100만 이미지, 11억 마스크 데이터셋은 세그멘테이션 분야의 표준이 되었다.</li>
<li><strong>합성 데이터(Synthetic Data):</strong> 개인정보 보호 문제와 고품질 데이터 부족을 해결하기 위해, Llama 3.2 Vision 훈련에는 300만 개 이상의 합성 데이터가 활용되었다.40 이는 향후 모델 훈련에서 합성 데이터의 비중이 더욱 커질 것임을 예고한다.</li>
</ul>
<h2>7.  산업적 응용 및 사례 연구</h2>
<p>비전 파운데이션 모델은 연구실을 벗어나 다양한 산업 분야에서 실질적인 가치를 창출하고 있다.</p>
<h3>7.1  헬스케어 및 의료 정밀 진단</h3>
<p>의료 분야는 고품질의 라벨링된 데이터를 얻기 어렵고 비용이 많이 들기 때문에, 비지도 학습 기반의 파운데이션 모델이 가장 큰 파급력을 미치는 영역이다.</p>
<ul>
<li><strong>웨어러블 데이터 분석 (JETS):</strong> MIT 연구진이 개발한 JETS 모델은 16,522명의 Apple Watch 사용자로부터 수집된 300만 인-일(person-days) 데이터를 활용했다. 이 데이터의 85%는 라벨이 없는 상태였으나, JEPA 기반의 자기 지도 학습을 통해 심혈관 질환, 호흡기 질환 등을 예측하는 데 성공했다. 특히 심방조동 예측에서 AUROC 70.5%, 고혈압 예측에서 86.8%를 기록하며 임상적 유용성을 입증했다.43</li>
<li><strong>의료 영상 분할:</strong> DINOv2와 SAM은 CT나 MRI 영상에서 장기 영역을 자동으로 분할하는 데 사용된다. DINOv2는 8개 미만의 적은 샘플(Few-shot)만으로도 장기 분할에서 타 모델 대비 압도적인 성능을 보였다.51</li>
</ul>
<h3>7.2  로보틱스 및 자율 시스템 (VLA)</h3>
<p>로봇 공학에서 <strong>VLA(Vision-Language-Action)</strong> 모델은 로봇의 인지 능력을 혁신하고 있다.</p>
<ul>
<li><strong>자연어 명령 수행:</strong> Google의 RT-2와 같은 모델은 비전 파운데이션 모델을 로봇의 ’눈’으로 사용하여, “서랍을 열고 파란색 컵을 꺼내줘“와 같은 자연어 명령을 로봇의 제어 신호로 변환한다.52</li>
<li><strong>공간 인식:</strong> 월드랩스의 공간 지능 기술은 로봇이 미지의 환경에서 충돌 없이 이동하고, 객체의 물리적 특성(무게, 재질 등)을 시각적으로 추론하여 파지(Grasping) 동작을 최적화하는 데 기여할 것이다.46</li>
</ul>
<h3>7.3  자율주행 및 인프라 시뮬레이션</h3>
<ul>
<li><strong>월드 모델 기반 시뮬레이션:</strong> 자율주행차는 희귀한 사고 상황(Edge case)에 대한 데이터가 부족하다. 비전 파운데이션 모델 기반의 월드 모델은 사실적인 가상 환경에서 수만 가지의 사고 시나리오를 시뮬레이션하여 자율주행 AI를 안전하게 훈련시키는 데 사용된다.46</li>
<li><strong>도시 계획:</strong> 위성 이미지나 드론 영상에 VFM을 적용하여 도시의 인프라 변화, 재해 위험 지역 등을 자동으로 분석하고, 도시 성장을 시뮬레이션하는 데 활용된다.46</li>
</ul>
<h3>7.4  창작 및 엔터테인먼트</h3>
<ul>
<li><strong>이미지/비디오 편집:</strong> SAM 2는 영화 제작이나 영상 편집 과정에서 특정 객체(예: 배우, 소품)를 자동으로 추적하고 분리하는 ‘로토스꼬핑(Rotoscoping)’ 작업을 자동화하여 작업 시간을 획기적으로 단축시켰다.53</li>
<li><strong>몰입형 콘텐츠:</strong> 공간 지능 기술은 사용자가 직접 탐험하고 상호작용할 수 있는 3D 가상 현실(VR) 및 메타버스 환경을 구축하는 데 핵심적인 역할을 할 것이다.46</li>
</ul>
<h2>8.  향후 전망 및 결론</h2>
<p>2025년 현재, 비전 파운데이션 모델은 기술적 성숙기에 접어들고 있다. CLIP과 SAM이 보여준 범용성, DINOv2의 정밀함, 그리고 JEPA와 공간 지능이 제시하는 물리적 세계의 이해는 AI가 인간 수준의 시각 지능에 도달하기 위한 필수적인 단계들이다.</p>
<p>향후 2~3년 내에 우리는 다음과 같은 발전들을 목격하게 될 것이다.</p>
<ol>
<li><strong>4D 및 동적 세계의 완전한 이해:</strong> 정지 이미지를 넘어 시간의 흐름과 공간의 깊이를 통합적으로 이해하는 4D 월드 모델이 주류가 될 것이다.52</li>
<li><strong>에이전트형 AI(Agentic AI)의 확산:</strong> 단순히 정보를 처리하는 수동적인 도구를 넘어, 물리적 세계에서 목표를 설정하고 계획하며 행동하는 로봇 에이전트가 등장할 것이다.</li>
<li><strong>효율적인 엣지 AI:</strong> Llama 3.2 11B와 같은 경량화 모델의 발전으로, 고성능 비전 AI가 스마트폰, 로봇, 드론 등 엣지 디바이스에서 실시간으로 구동될 것이다.</li>
</ol>
<p>결론적으로, 비전 파운데이션 모델은 디지털 세계와 물리적 세계를 연결하는 가장 강력한 인터페이스로 자리 잡았다. 이 기술은 산업 자동화, 의료 서비스 개선, 그리고 인간의 창의성 확장에 기여할 것이며, 동시에 윤리적 AI 개발과 데이터 프라이버시에 대한 지속적인 논의를 요구할 것이다. 연구자와 산업계 리더들은 이러한 기술적 흐름을 예의 주시하고, 책임감 있는 혁신을 주도해야 할 시점이다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>Foundation Models Defining a New Era in Vision: A Survey and …, https://www.computer.org/csdl/journal/tp/2025/04/10834497/23mYUeDuDja</li>
<li>Exploring the Benefits of Vision Foundation Models for … - arXiv, https://arxiv.org/html/2406.09896v2</li>
<li>Interactive Look: Self-Attention in Vision Transformers - Abhik Sarkar, https://www.abhik.xyz/concepts/attention/self-attention-vit</li>
<li>11.8. Transformers for Vision - Dive into Deep Learning, https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html</li>
<li>History Of Computer Vision Models: From Pixels To Perception, https://clarion.ai/the-history-of-computer-vision-models-from-pixels-to-perception/</li>
<li>Vision Transformer (ViT) - Hugging Face, https://huggingface.co/docs/transformers/model_doc/vit</li>
<li>Vision Transformer (ViT) Architecture - GeeksforGeeks, https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/</li>
<li>Vision Transformer: What It Is &amp; How It Works [2024 Guide] - V7 Go, https://www.v7labs.com/blog/vision-transformer-guide</li>
<li>Self-Guided Masked Autoencoder | OpenReview, <a href="https://openreview.net/forum?id=7vXufiEzSy&amp;referrer=%5Bthe+profile+of+Junho+Lee%5D(/profile?id%3D~Junho_Lee2)">https://openreview.net/forum?id=7vXufiEzSy&amp;referrer=%5Bthe%20profile%20of%20Junho%20Lee%5D(%2Fprofile%3Fid%3D~Junho_Lee2)</a></li>
<li>Masked Autoencoders Are Scalable Vision Learners - arXiv, https://arxiv.org/abs/2111.06377</li>
<li>Self-Supervised Learning and Foundation Models - Inria, <a href="https://www-sop.inria.fr/members/Francois.Bremond/MSclass/deepLearningWinterSchool25/UCA_master/slides_win20/SSL%20and%20Foundation%20Model.pdf">https://www-sop.inria.fr/members/Francois.Bremond/MSclass/deepLearningWinterSchool25/UCA_master/slides_win20/SSL%20and%20Foundation%20Model.pdf</a></li>
<li>Understanding Masked Autoencoders From a Local Contrastive …, https://arxiv.org/html/2310.01994v2</li>
<li>Masked Autoencoders Are Scalable Vision Learners - ResearchGate, https://www.researchgate.net/publication/384694446_Masked_Autoencoders_Are_Scalable_Vision_Learners</li>
<li>CLIP, DINO, SAM, and GIT: The 2025 Computer Vision Stack - Medium, https://medium.com/@pranavprakash4777/clip-dino-sam-and-git-the-2025-computer-vision-stack-1c55ec5db42c</li>
<li>CLIP: Connecting text and images - OpenAI, https://openai.com/index/clip/</li>
<li>CLIP: The Zero-Shot Monster That Devours Fully Supervised Models …, https://medium.com/@akdemir_bahadir/clip-the-zero-shot-monster-that-devours-fully-supervised-models-years-later-still-worth-talking-91eed91fee5b</li>
<li>Learning Transferable Visual Models From Natural Language …, https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language.pdf</li>
<li>About the ImageNet zero-shot performance with the released models, https://github.com/openai/CLIP/issues/24</li>
<li>Reaching 80% zero-shot accuracy with OpenCLIP: ViT-G/14 trained …, https://laion.ai/blog/giant-openclip/</li>
<li>(PDF) CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot …, https://www.researchgate.net/publication/371909177_CLIPA-v2_Scaling_CLIP_Training_with_811_Zero-shot_ImageNet_Accuracy_within_a_10000_Budget_An_Extra_4000_Unlocks_818_Accuracy</li>
<li>Understanding the Differences Between CLIP, Grounding DINO, and …, https://medium.com/@hexiangnan/understanding-the-differences-between-clip-grounding-dino-and-sam-a-deep-dive-into-b0724d15d92c</li>
<li>MaskCLIP: Masked Self-Distillation Advances Contrastive Language …, https://liner.com/review/maskclip-masked-selfdistillation-advances-contrastive-languageimage-pretraining</li>
<li>Understanding DINOv2: Engineer’s Deep Dive - Lightly AI, https://www.lightly.ai/blog/dinov2</li>
<li>Paper Review: DINOv2: Learning Robust Visual Features without …, https://andlukyane.com/blog/paper-review-dinov2</li>
<li>DINOv2: Learning Robust Visual Features without Supervision - arXiv, https://arxiv.org/html/2304.07193v2</li>
<li>dinov2/MODEL_CARD.md at main - GitHub, https://github.com/facebookresearch/dinov2/blob/main/MODEL_CARD.md</li>
<li>Comparative Analysis of Pre-trained Deep Learning Models and …, https://arxiv.org/html/2501.12023v1</li>
<li>The Foundation Models Reshaping Computer Vision - Tenyks, https://www.tenyks.ai/blog/the-foundation-models-reshaping-computer-vision</li>
<li>Assessing the Performance of the DINOv2 Self-supervised Learning …, https://www.researchgate.net/publication/385823317_Assessing_the_Performance_of_the_DINOv2_Self-supervised_Learning_Vision_Transformer_Model_for_the_Segmentation_of_the_Left_Atrium_from_MRI_Images</li>
<li>Exploring Improvements in Meta’s Segment Anything Model - arXiv, https://arxiv.org/html/2408.06305v1</li>
<li>[2304.02643] Segment Anything - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/2304.02643</li>
<li>SAM 2 + GPT-4o: Cascading Foundation Models via Visual …, https://www.edge-ai-vision.com/2025/02/sam-2-gpt-4o-cascading-foundation-models-via-visual-prompting-part-1/</li>
<li>SAM 2: Segment Anything in Images and Videos - OpenReview, https://openreview.net/forum?id=Ha6RTeWMd0</li>
<li>SAM 2: Segment Anything Model 2 - Ultralytics YOLO Docs, https://docs.ultralytics.com/models/sam-2/</li>
<li>Performance of GPT-4V(ision) and GPT-4o in Ophthalmology, https://www.dovepress.com/image-recognition-performance-of-gpt-4vision-and-gpt-4o-in-ophthalmolo-peer-reviewed-fulltext-article-OPTH</li>
<li>Our next-generation model: Gemini 1.5 - Google Blog, https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/</li>
<li>Gemini 1.5: Unlocking multimodal understanding across millions of …, https://arxiv.org/pdf/2403.05530</li>
<li>Introducing Llama 3.2 models from Meta in Amazon Bedrock - AWS, https://aws.amazon.com/blogs/aws/introducing-llama-3-2-models-from-meta-in-amazon-bedrock-a-new-generation-of-multimodal-vision-and-lightweight-models/</li>
<li>Llama (language model) - Wikipedia, https://en.wikipedia.org/wiki/Llama_(language_model)</li>
<li>meta-llama/Llama-3.2-11B-Vision - Hugging Face, https://huggingface.co/meta-llama/Llama-3.2-11B-Vision</li>
<li>Llama 3.2 11B Vision Instruct vLLM Benchmarks - KubeAI, https://www.kubeai.org/benchmarks/llama-3.2-11b-vision/</li>
<li>llama3.2 - Ollama, https://ollama.com/library/llama3.2</li>
<li>How Apple Watch helped researchers to train AI model that can detect multiple medical conditions, https://timesofindia.indiatimes.com/technology/wearables/how-apple-watch-helped-researchers-to-train-ai-model-that-can-detect-multiple-medical-conditions/articleshow/125892478.cms</li>
<li>The Anatomy of JEPA: The Architecture Behind embedded … - Medium, https://medium.com/@frinktyler1445/the-anatomy-of-jepa-the-architecture-behind-embedded-predictive-representation-learning-994bfa0bffe0</li>
<li>Deep Dive into Yann LeCun’s JEPA - Rohit Bandaru, https://rohitbandaru.github.io/blog/JEPA-Deep-Dive/</li>
<li>AI’s next reach is world-building: spatial intelligence that can reconstruct and simulate 3D realities, https://m.economictimes.com/opinion/et-commentary/ais-next-reach-is-world-building-spatial-intelligence-that-can-reconstruct-and-simulate-3d-realities/articleshow/125918371.cms</li>
<li>Inside Fei-Fei Li’s Plan to Build AI-Powered Virtual Worlds | TIME, https://time.com/7339513/ai-fei-fei-li-virtual-worlds/</li>
<li>About - World Labs, https://www.worldlabs.ai/about</li>
<li>V-JEPA: The next step toward Yann LeCun’s vision of advanced …, https://www.reddit.com/r/MachineLearning/comments/1at7fib/vjepa_the_next_step_toward_yann_lecuns_vision_of/</li>
<li>Pricing AI Training vs Inference: Different Models for Different Phases, https://www.getmonetizely.com/articles/pricing-ai-training-vs-inference-different-models-for-different-phases</li>
<li>An Experimental Study of DINOv2 on Radiology Benchmarks - arXiv, https://arxiv.org/html/2312.02366v4</li>
<li>Unlocking the Future of Computer Vision: Our Journey at CVPR 2025, https://www.skyengine.ai/blog/unlocking-the-future-of-computer-vision-our-journey-at-cvpr-2025</li>
<li>‘Grassroots not an initiative of the ISPL, it is the base’: Core Committee member Minal Kale, https://timesofindia.indiatimes.com/sports/cricket/news/grassroots-not-an-initiative-of-the-ispl-it-is-the-base-core-committee-member-minal-kale/articleshow/125912370.cms</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>