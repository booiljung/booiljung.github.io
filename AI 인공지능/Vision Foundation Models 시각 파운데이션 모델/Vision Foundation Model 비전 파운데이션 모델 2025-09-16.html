<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:비전 파운데이션 모델 (Vision Foundation Model)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>비전 파운데이션 모델 (Vision Foundation Model)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">비전 파운데이션 모델 (Vision Foundation Model)</a> / <span>비전 파운데이션 모델 (Vision Foundation Model)</span></nav>
                </div>
            </header>
            <article>
                <h1>비전 파운데이션 모델 (Vision Foundation Model)</h1>
<p>2025-09-16, G25DR</p>
<h2>1.  컴퓨터 비전의 패러다임 전환</h2>
<p>컴퓨터 비전 분야는 인공지능(AI)의 발전과 함께 눈부신 성장을 거듭해왔다. 초기 패턴 인식 알고리즘에서부터 특정 작업을 위해 정교하게 설계된 심층 신경망에 이르기까지, 기계가 시각적 세계를 ‘보고’ 이해하도록 만드는 기술은 꾸준히 진화했다. 그러나 최근 몇 년간 이 분야는 점진적 개선을 넘어선 근본적인 패러다임 전환을 맞이하고 있다. 이 변화의 중심에는 ’비전 파운데이션 모델(Vision Foundation Model, VFM)’이 자리 잡고 있다. VFM은 단순히 더 크고 강력한 모델을 의미하는 것이 아니라, AI 개발과 활용의 철학 자체를 바꾸는 새로운 개념이다. 과거의 모델들이 특정 과업(예: 고양이 탐지, 얼굴 인식)을 위해 개별적으로 훈련되는 ’전문가’였다면, VFM은 방대한 시각적 데이터로부터 세상에 대한 보편적 이해를 학습한 ’일반 지능’에 가깝다. 이 범용적인 시각적 지능을 바탕으로, VFM은 최소한의 추가 학습만으로 광범위한 하위 작업에 신속하게 적응할 수 있는 능력을 갖춘다. 본 보고서는 컴퓨터 비전의 새로운 지평을 연 비전 파운데이션 모델의 정의와 핵심 속성을 명확히 하고, 전통적인 모델과의 근본적 차이점을 분석함으로써 이 기술적 변혁의 본질을 심도 있게 탐구하고자 한다.</p>
<h3>1.1  비전 파운데이션 모델(VFM)의 정의와 핵심 속성</h3>
<p>비전 파운데이션 모델(VFM)은 광범위한 데이터(일반적으로 대규모 자기지도학습을 통해)로 훈련되어 다양한 하위 작업(downstream tasks)에 적응할 수 있는 대규모 AI 모델로 정의된다.1 이 용어는 스탠퍼드 인간 중심 인공지능 연구소(Stanford Institute for Human-Centered Artificial Intelligence)에 의해 대중화되었으며, 자연어 처리(NLP) 분야에서 대규모 언어 모델(LLM)이 보여준 성공을 시각 데이터 영역으로 확장한 개념이다.2 VFM은 복잡한 시각적 데이터를 이해하고 처리하도록 설계된 범용 백본(backbone) 역할을 하며, 특히 생성형 AI를 포함한 수많은 응용 프로그램의 기반이 된다.4 VFM을 특징짓는 핵심 속성은 다음과 같다.</p>
<ul>
<li>
<p><strong>규모 확장성 (Scalability):</strong> VFM의 가장 두드러진 특징은 훈련에 사용되는 데이터와 모델의 엄청난 규모다. 이 모델들은 수억 개에서 수십억 개에 달하는 이미지 또는 이미지-텍스트 쌍으로 구성된 방대한 데이터셋으로 사전 훈련된다.2 예를 들어, 언어 모델인 GPT-3가 1750억 개의 매개변수를 가진 것처럼, VFM 역시 전례 없는 규모의 파라미터를 통해 복잡하고 미묘한 시각적 패턴을 학습한다.1 DINOv2, SAM과 같은 모델들이 이러한 규모의 대표적인 예시다.6</p>
</li>
<li>
<p><strong>일반화 (Generalization):</strong> VFM은 특정 작업에 과적합되지 않고, 보편적인 시각적 표현을 학습한다. 이 덕분에 사전 훈련 과정에서 보지 못했던 새로운 작업이나 데이터 도메인에 대해서도 강력한 성능을 발휘하는 ‘제로샷(zero-shot)’ 및 ‘퓨샷(few-shot)’ 전이 학습 능력을 갖춘다.6 이는 소량의 예시 데이터만으로, 혹은 전혀 없이도 새로운 문제를 해결할 수 있음을 의미하며, 모델의 활용 범위를 극적으로 넓힌다.6</p>
</li>
<li>
<p><strong>자기지도학습 (Self-Supervision):</strong> VFM 훈련의 핵심 동력은 자기지도학습(Self-Supervised Learning, SSL)이다. SSL은 레이블이 없는 대규모 데이터로부터 모델 스스로 감독 신호(supervisory signal)를 생성하여 학습하는 방식이다.1 예를 들어, 이미지의 일부를 가리고 나머지 부분을 이용해 가려진 부분을 예측하게 하거나(마스크 이미지 모델링), 동일한 이미지에 다른 변형을 가한 두 버전을 유사하게 임베딩하도록 학습한다(대조 학습). 이 방식은 수작업으로 레이블을 만드는 데 드는 막대한 비용과 시간을 절감시켜, 웹 스케일의 방대한 비정형 데이터를 훈련에 활용할 수 있게 한다.4</p>
</li>
<li>
<p><strong>적응성 및 프롬프트 가능성 (Adaptability &amp; Promptability):</strong> VFM은 단일 목적이 아닌, 다양한 하위 작업에 유연하게 적응(adapt)하도록 설계되었다. 사전 훈련된 모델의 가중치를 기반으로 특정 작업 데이터에 맞게 미세 조정(fine-tuning)하거나, 모델의 일부만 학습시키는 파라미터 효율적 튜닝(parameter-efficient tuning)을 통해 손쉽게 전문화할 수 있다.1 더 나아가, SAM(Segment Anything Model)과 같은 최신 VFM은 텍스트, 점, 경계 상자 등 다양한 형태의 ’프롬프트(prompt)’를 통해 사용자와 상호작용하며 실시간으로 원하는 작업을 수행하는 ’프롬프트 가능성’을 보여준다.6</p>
</li>
</ul>
<p>이러한 속성들은 VFM이 단순한 모델의 발전을 넘어 AI 개발 패러다임 자체를 변화시키고 있음을 시사한다. 전통적인 AI 개발이 각 문제에 맞는 모델을 처음부터 설계하고 훈련하는 ’모델 구축(model construction)’의 과정이었다면, VFM 시대의 개발은 이미 방대한 지식을 학습한 거대 모델을 특정 목적에 맞게 조정하고 활용하는 ’지식 적응(knowledge adaptation)’의 과정으로 전환되고 있다.5 개발자들의 핵심 역량은 이제 심층적인 아키텍처 설계에서 벗어나, 효과적인 프롬프트 엔지니어링, 효율적인 미세 조정 전략, 그리고 특정 도메인에 맞는 데이터 큐레이션 능력으로 옮겨가고 있다. 이는 마치 모든 작업에 맞는 개별 도구를 제작하던 시대에서, 다양한 부품(bit)을 교체하며 사용할 수 있는 만능 전동 공구를 활용하는 시대로의 전환과 같다. 이러한 변화는 강력한 AI 애플리케이션 개발의 진입 장벽을 낮추는 동시에, 파운데이션 모델 자체를 개발할 수 있는 소수의 거대 기관에 막대한 영향력을 집중시키는 경제적, 사회적 함의를 내포한다.7</p>
<h3>1.2  전통적 컴퓨터 비전 모델과의 근본적 차이점 분석</h3>
<p>VFM과 전통적인 컴퓨터 비전 모델의 차이는 단순히 규모나 성능의 차이를 넘어선다. 그 차이는 모델의 설계 철학, 학습 방식, 그리고 활용 범위에 걸쳐 근본적인 수준에서 나타난다.</p>
<p>첫째, **범용성 대 특수성(General vs. Narrow AI)**의 차이가 있다. 전통적인 모델은 본질적으로 ’좁은 AI(Narrow AI)’로, 대출 부도 예측 모델이 고객 상담 챗봇으로 기능할 수 없듯이, 설계된 특정 목적 외에는 사용될 수 없다.2 반면, VFM은 텍스트 합성, 이미지 조작, 코드 생성 등 광범위한 작업을 수행할 수 있는 ’범용 AI(General-Purpose AI)’의 특성을 지닌다.2 이들은 특정 작업이 아닌, 시각적 세계에 대한 포괄적인 이해를 목표로 학습되기 때문에 다양한 응용 분야의 ’기반(foundation)’이 될 수 있다.</p>
<p>둘째, <strong>아키텍처의 차이</strong>가 명확하다. 전통적인 컴퓨터 비전은 이미지의 지역적 특징(local receptive field)에 집중하는 합성곱 신경망(Convolutional Neural Networks, CNN)에 깊이 의존해왔다.7 CNN은 필터를 통해 이미지의 가장자리, 질감, 형태와 같은 저수준 특징부터 시작하여 점진적으로 복잡한 객체를 인식하는 계층적 구조를 가진다.12 반면, VFM은 주로 트랜스포머(Transformer) 아키텍처에 기반한다.6 트랜스포머는 ‘셀프 어텐션(self-attention)’ 메커니즘을 통해 이미지 전체의 맥락을 한 번에 처리하고, 이미지 내 멀리 떨어진 픽셀 간의 장거리 의존성(long-range dependency)을 효과적으로 포착한다.14 이는 전체적인 장면의 이해가 중요한 복잡한 시각적 추론 과제에서 결정적인 이점을 제공한다.</p>
<p>셋째, <strong>학습 방식과 데이터의 차이</strong>가 있다. 전통적인 모델은 대부분 지도 학습(supervised learning) 방식으로, 사람이 정성껏 레이블링한 비교적 작은 규모의 데이터셋으로 훈련되었다.8 이와 대조적으로, VFM은 웹에서 수집된 레이블 없는 방대한 데이터를 활용하는 자기지도학습(self-supervised learning)에 의존한다.1 이로 인해 VFM은 수작업 레이블링의 한계를 벗어나 사실상 무한에 가까운 데이터로부터 학습할 수 있으며, 이는 모델의 일반화 성능을 극대화하는 원동력이 된다.</p>
<p>마지막으로, <strong>개발 효율성</strong> 측면에서 큰 차이를 보인다. VFM은 사전 훈련을 통해 이미 시각적 세계에 대한 깊은 이해를 갖추고 있으므로, 개발자는 수작업 특징 공학(feature engineering)이나 대규모 레이블링 작업 없이도 신속하게 고성능 AI 애플리케이션을 개발할 수 있다.4 이는 제품 개발 주기를 단축하고 시장 출시 기간을 줄이는 실질적인 경제적 이점으로 이어진다.4</p>
<p>이러한 변화는 ’동질화(homogenization)’라는 양날의 검을 품고 있다. 소수의 지배적인 VFM이 수많은 애플리케이션의 기반이 되는 현상은 혁신을 위한 강력한 지렛대 역할을 하지만, 동시에 시스템적 위험을 내포한다.2 예를 들어, GPT-4가 ChatGPT, Bing Chat 등 다양한 서비스의 동력이 되듯, 하나의 VFM이 여러 비전 애플리케이션의 핵심 엔진이 될 수 있다.2 이는 효율적이지만, 만약 해당 파운데이션 모델에 내재된 결함, 편향, 또는 특정 취약점이 존재한다면, 그 문제는 그 위에 구축된 수천 개의 하위 애플리케이션으로 그대로 전파된다.16 이는 사회적 규모의 단일 장애점(single point of failure)을 만들어낼 수 있다. 특정 인구 집단에 대한 편향을 가진 VFM이 있다면, 그 편향은 해당 모델을 사용하는 모든 보안 시스템, 의료 진단 도구, 채용 솔루션에 스며들게 될 것이다.4 따라서 VFM으로의 전환은 애플리케이션 혁신을 가속화하는 반면, ’기반’의 다양성 부족이 진정한 아키텍처 혁신을 저해하고 시스템적 취약성을 키울 수 있다는 점을 경계해야 한다.</p>
<h2>2.  VFM의 진화: CNN에서 트랜스포머까지</h2>
<p>비전 파운데이션 모델의 등장은 어느 날 갑자기 이루어진 것이 아니다. 이는 수십 년에 걸친 컴퓨터 비전 연구의 흐름 속에서 이전 세대 아키텍처의 한계를 극복하려는 끊임없는 노력의 산물이다. 특히, 지역적 특징 추출에 최적화된 합성곱 신경망(CNN)의 시대에서 시작하여, 전역적 맥락 이해에 능한 트랜스포머(Transformer) 아키텍처로의 전환은 VFM의 탄생을 가능하게 한 결정적인 기술적 도약이었다. 이 장에서는 CNN의 황금기부터 트랜스포머가 시각 데이터 처리에 혁신을 가져오기까지의 역사적 궤적을 추적하며, 각 아키텍처의 핵심 원리와 그 한계가 어떻게 다음 세대의 혁신을 촉발했는지 분석한다.</p>
<h3>2.1  초기 컴퓨터 비전: 합성곱 신경망(CNN)의 시대</h3>
<p>현대 컴퓨터 비전의 초석을 다진 것은 합성곱 신경망(CNN)이었다. CNN의 개념적 뿌리는 1979년 일본 과학자 후쿠시마 쿠니히코(Kunihiko Fukushima)가 제안한 ’네오코그니트론(Neocognitron)’으로 거슬러 올라간다.11 네오코그니트론은 인간의 시각 피질 구조에서 영감을 받아 설계된 계층적 패턴 인식 신경망으로, 현대 CNN의 이론적 기틀을 마련했다.19</p>
<p>이 아이디어를 발전시켜 실용적인 모델을 구현한 것은 얀 르쿤(Yann LeCun)이었다. 그가 1989년에 개발한 LeNet-5는 손으로 쓴 숫자 인식을 성공적으로 수행하며, 합성곱(convolution)과 풀링(pooling) 계층을 반복적으로 쌓는 현대 CNN의 기본 구조를 정립했다.12 LeNet-5는 우편번호 자동 분류 시스템 등에 활용되며 CNN의 잠재력을 입증했다.13</p>
<p>CNN이 컴퓨터 비전의 주류로 부상한 결정적 계기는 2012년 이미지넷 대규모 시각 인식 챌린지(ILSVRC)에서 알렉스 크리제브스키(Alex Krizhevsky) 팀이 개발한 AlexNet이 압도적인 성능으로 우승한 사건이었다.12 AlexNet의 성공은 여러 혁신적 요소의 결합 덕분이었다. 첫째, GPU를 활용한 병렬 계산을 통해 이전보다 훨씬 깊고 큰 모델의 훈련이 가능해졌다.19 둘째, 활성화 함수로 ReLU(Rectified Linear Unit)를 사용하여 기울기 소실 문제를 완화하고 학습 속도를 높였다. 셋째, 드롭아웃(dropout)과 데이터 증강(data augmentation) 기법을 통해 과적합을 방지하고 모델의 일반화 성능을 향상시켰다.13</p>
<p>AlexNet의 성공은 컴퓨터 비전 분야에 딥러닝 혁명을 촉발했다. 이후 연구자들은 더 깊고 복잡한 CNN 아키텍처를 경쟁적으로 개발했다. 2014년에는 더 작은 크기의 필터를 여러 겹 쌓아 깊이를 늘린 VGGNet이 등장했고, 2015년에는 ’잔차 연결(residual connection)’이라는 혁신적인 개념을 도입한 ResNet이 등장하여 100층이 넘는 매우 깊은 네트워크의 학습을 가능하게 했다.13 객체 탐지 분야에서도 R-CNN, Fast R-CNN, Faster R-CNN과 같은 모델들이 연이어 발표되며 CNN 기반 방법론의 위력을 과시했다.12</p>
<p>그러나 CNN의 바로 그 강점이 VFM으로 확장되는 데 있어 가장 큰 장벽이 되었다. CNN의 핵심은 ’귀납적 편향(inductive bias)’에 있다. 즉, CNN은 시각적 패턴이 ’지역성(locality)’과 ’이동 불변성(translation invariance)’을 가진다는 강력한 사전 가정을 아키텍처에 내장하고 있다. 필터가 이미지의 작은 영역을 훑으며 특징을 추출하는 방식은 이러한 가정을 구현한 것이다.11 이 편향은 기본적인 이미지 분류처럼 지역적인 객체 정보를 인식하는 것만으로 충분한 작업에서는 매우 효율적으로 작동한다. 하지만 “고양이가 테이블 <em>아래에</em> 있는가?“와 같이 이미지 내 멀리 떨어진 요소들 간의 관계를 이해해야 하는 복잡한 추론에서는 한계를 드러낸다. CNN이 이러한 전역적 맥락을 파악하기 위해서는 수많은 계층을 거치며 점진적으로 수용 영역(receptive field)을 넓혀야 하는데, 이 과정은 비효율적이며 정보 손실을 유발할 수 있다. 자연어 처리 분야에서 트랜스포머가 문장 내 단어 간의 거리에 상관없이 관계를 모델링하는 셀프 어텐션을 통해 순환 신경망(RNN)의 지역성 편향을 극복하고 성공을 거둔 것처럼, 컴퓨터 비전에서도 CNN의 구조적 한계를 뛰어넘을 새로운 아키텍처의 필요성이 대두되었다. 이 필요성이 바로 비전 트랜스포머의 등장을 예고하는 신호탄이었다.</p>
<h3>2.2  비전 트랜스포머(ViT)의 등장과 구조적 혁신</h3>
<p>2020년 구글 연구팀이 발표한 논문 “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale“은 컴퓨터 비전 분야에 새로운 이정표를 세웠다.22 이 논문에서 제안된 비전 트랜스포머(Vision Transformer, ViT)는 자연어 처리(NLP)를 위해 설계된 순수 트랜스포머 아키텍처를 최소한의 수정만으로 이미지 분류 작업에 직접 적용하여, 대규모 데이터셋에서 CNN 기반 모델들을 능가하는 성능을 달성했다.22</p>
<p>ViT의 핵심적인 구조적 혁신은 이미지를 언어처럼 ’토큰의 시퀀스’로 취급하는 데 있다. 이 과정은 다음과 같은 단계로 이루어진다.14</p>
<ol>
<li>
<p><strong>패치 임베딩 (Patch Embedding):</strong> 입력 이미지를 바둑판처럼 일정한 크기(예: 16x16 픽셀)의 겹치지 않는 패치(patch)들로 분할한다. 이는 문장을 단어로 나누는 것과 유사한 과정이다.14</p>
</li>
<li>
<p><strong>선형 투영 (Linear Projection):</strong> 각 이미지 패치를 1차원 벡터로 펼친 후, 학습 가능한 선형 투영(linear projection)을 통해 고정된 차원의 벡터, 즉 ’패치 임베딩’으로 변환한다. 이는 NLP에서 단어를 임베딩 벡터로 변환하는 것과 동일한 역할을 한다.14</p>
</li>
<li>
<p><strong>위치 임베딩 (Position Embedding):</strong> 트랜스포머는 본질적으로 순서 정보를 처리하지 못하므로, 각 패치의 원래 공간적 위치 정보를 보존하기 위해 학습 가능한 ’위치 임베딩’을 패치 임베딩에 더해준다. 이 과정이 없다면 모델은 패치들의 순서를 구분할 수 없게 된다.24</p>
</li>
<li>
<p><strong>트랜스포머 인코더 (Transformer Encoder):</strong> 위치 정보가 추가된 패치 임베딩 시퀀스는 표준 트랜스포머 인코더로 입력된다. 인코더는 여러 개의 블록으로 구성되며, 각 블록은 멀티 헤드 셀프 어텐션(Multi-Head Self-Attention, MHSA) 계층과 피드포워드 신경망(Feed-Forward Network, FFN) 계층이 번갈아 나타나는 구조를 가진다.14</p>
</li>
</ol>
<p>ViT의 심장부인 셀프 어텐션 메커니즘은 각 패치를 처리할 때 다른 모든 패치와의 연관성을 계산하여 가중치를 부여한다. 이를 통해 모델은 첫 번째 계층에서부터 이미지 전체의 전역적인 관계를 파악할 수 있다.7 셀프 어텐션은 쿼리(Query), 키(Key), 밸류(Value)라는 세 가지 벡터를 사용하여 계산된다. 입력 임베딩 시퀀스 <span class="math math-inline">X</span>에 대해, 이들은 각각 다른 가중치 행렬 <span class="math math-inline">W_Q, W_K, W_V</span>를 곱하여 얻어진다. 어텐션 출력은 다음의 수식으로 계산된다.</p>
<p><span class="math math-display">
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
</span><br />
여기서 <span class="math math-inline">Q, K, V</span>는 각각 쿼리, 키, 밸류 행렬을 나타내고, <span class="math math-inline">d_k</span>는 키 벡터의 차원이다. 이 수식은 각 쿼리가 모든 키와 얼마나 유사한지를 계산하고( <span class="math math-inline">QK^T</span> ), 그 결과를 스케일링한 후 소프트맥스 함수를 통해 확률 분포(어텐션 가중치)로 변환한다. 마지막으로 이 가중치를 밸류에 곱하여 최종 출력을 얻는다.</p>
<p>ViT의 등장은 단순히 새로운 고성능 모델의 출현 이상의 의미를 가졌다. 이는 대규모 자기지도학습을 위한 촉매제 역할을 했다. 초기 실험 결과, ViT는 이미지넷과 같은 상대적으로 작은 데이터셋에서는 CNN보다 성능이 떨어졌지만, JFT-300M과 같은 수억 장 규모의 거대 데이터셋에서 사전 훈련했을 때 CNN을 능가하는 성능을 보였다.22 이는 ViT가 CNN보다 약한 귀납적 편향을 가지고 있음을 시사한다. 즉, ViT는 이미지의 지역성 같은 시각적 사전 지식을 내장하고 있지 않기 때문에, 이를 데이터로부터 직접 학습하기 위해 막대한 양의 데이터가 필요하다.</p>
<p>바로 이 지점에서 자기지도학습(SSL)과의 시너지가 발생했다. 대규모의 <em>레이블링된</em> 데이터셋을 구축하는 것은 엄청난 비용과 노력이 드는 병목 현상의 원인이었다.15 반면, SSL은 사실상 무한한 <em>레이블 없는</em> 데이터를 활용할 수 있는 길을 열어주었다.4 ViT의 데이터에 대한 갈증은 SSL의 확장성에 의해 충족되었고, SSL의 잠재력은 CNN의 지역적 편향이라는 족쇄에서 벗어난 ViT의 유연한 아키텍처를 통해 완전히 발현되었다. 이 완벽한 조합은 CLIP, DINO, MAE와 같은 후속 비전 파운데이션 모델들이 탄생할 수 있는 비옥한 토양을 마련했다.6</p>
<h2>3.  핵심 훈련 패러다임: 자기지도학습</h2>
<p>비전 파운데이션 모델의 강력한 일반화 능력은 그 훈련 방식에 근본적인 비밀이 있다. VFM은 대부분 ’자기지도학습(Self-Supervised Learning, SSL)’이라는 패러다임을 통해 훈련된다. SSL은 명시적인 정답 레이블 없이 데이터 자체의 구조적 정보를 활용하여 모델이 스스로 학습 목표, 즉 ’유사 레이블(pseudo-label)’을 생성하고 이를 해결하는 과정에서 유의미한 표현(representation)을 학습하는 방법론이다.8 이는 레이블링 비용의 한계에서 벗어나 웹 스케일의 방대한 비정형 데이터를 훈련에 활용할 수 있게 함으로써 VFM의 규모 확장을 가능하게 한 핵심 기술이다. 이 장에서는 VFM 훈련에 사용되는 대표적인 세 가지 SSL 패러다임인 대조 학습, 마스크 이미지 모델링, 그리고 자기 증류를 심층적으로 분석하고, 각 방법론의 작동 원리와 이론적 기반을 탐구한다.</p>
<h3>3.1  대조 학습 (Contrastive Learning): CLIP과 InfoNCE 손실 함수 심층 분석</h3>
<p>대조 학습(Contrastive Learning)은 ’비슷한 것은 가깝게, 다른 것은 멀게’라는 직관적인 원칙에 기반한 자기지도학습 방법론이다.31 모델은 임베딩 공간(embedding space) 내에서 의미적으로 유사한 ’긍정 쌍(positive pair)’의 표현은 서로 가깝게 끌어당기고, 의미적으로 다른 ’부정 쌍(negative pair)’의 표현은 서로 멀리 밀어내도록 훈련된다.31</p>
<p>이 패러다임에서 긍정 쌍과 부정 쌍을 어떻게 정의하느냐가 핵심이다. 순수 자기지도학습 환경(예: SimCLR)에서는 일반적으로 동일한 이미지에 서로 다른 데이터 증강(augmentation) 기법(예: 자르기, 색상 왜곡, 회전)을 적용하여 긍정 쌍을 생성한다.31 이때, 배치(batch) 내의 다른 모든 이미지는 부정 쌍으로 간주된다. 반면, OpenAI의 CLIP과 같은 다중모드(multimodal) 모델에서는 긍정 쌍을 (이미지, 해당 이미지에 대한 텍스트 캡션)으로 정의하고, 배치 내의 다른 모든 이미지-텍스트 조합을 부정 쌍으로 취급한다.34</p>
<p>대조 학습에서 가장 널리 사용되는 손실 함수는 InfoNCE(Noise Contrastive Estimation)이다.35 InfoNCE는 주어진 ‘쿼리(query)’ 임베딩(예: 특정 이미지의 표현)에 대해, 여러 ‘키(key)’ 임베딩 중에서 자신의 긍정 키(positive key)를 올바르게 식별하는 분류 문제로 학습을 재구성한다.37 쿼리 <span class="math math-inline">q</span>, 긍정 키 <span class="math math-inline">k_+</span>, 그리고 <span class="math math-inline">N-1</span>개의 부정 키 집합 <span class="math math-inline">\{k_i\}</span>가 주어졌을 때, InfoNCE 손실 함수는 다음과 같이 정의된다.</p>
<p><span class="math math-display">
\mathcal{L}_q = -\log \frac{\exp(\text{sim}(q, k_+) / \tau)}{\exp(\text{sim}(q, k_+) / \tau) + \sum_{i=1}^{N-1} \exp(\text{sim}(q, k_i) / \tau)}
</span><br />
여기서 <span class="math math-inline">\text{sim}(\cdot, \cdot)</span>은 두 벡터 간의 유사도를 측정하는 함수로, 주로 코사인 유사도(cosine similarity)가 사용된다. <span class="math math-inline">\tau</span>는 ‘온도(temperature)’ 하이퍼파라미터로, 로짓(logit) 값을 조절하여 확률 분포의 뾰족함(sharpness)을 제어한다. 온도가 낮을수록 모델은 어려운 부정 샘플에 더 집중하게 된다.36 이 손실 함수는 분자가 최대화되고 분모가 최소화되도록, 즉 긍정 쌍의 유사도는 높이고 부정 쌍의 유사도는 낮추도록 모델을 최적화한다.</p>
<p>대조 학습, 특히 대규모 배치를 사용하는 경우의 효과는 ’어려운 부정 샘플(hard negatives)’에 의해 생성되는 암묵적인 교육과정(implicit curriculum)에서 비롯된다. 훈련 초기에는 모델이 쿼리와 시각적으로 매우 다른 ‘쉬운 부정 샘플’(예: 쿼리가 고양이, 부정 샘플이 자동차)을 구별하는 법부터 배운다. 이 단계에서는 대부분의 부정 샘플에 대한 손실이 높게 나타난다. 훈련이 진행됨에 따라, 모델은 쉬운 부정 샘플과의 유사도를 매우 낮게 평가하게 되고, 손실 값은 이제 의미적으로는 다르지만 시각적으로는 유사한 ‘어려운 부정 샘플’(예: 쿼리가 고양이, 부정 샘플이 호랑이)에 의해 주로 결정된다. 모델은 이제 고양이와 자동차를 구별하는 것을 넘어, 고양이와 호랑이 사이의 미묘한 차이를 학습하도록 강제된다. 이처럼 점진적으로 어려운 예제에 집중하게 만드는 과정은 모델이 더 세밀하고 식별력 있는 특징을 학습하도록 유도한다. 대규모 배치는 이러한 어려운 부정 샘플을 마주할 확률을 높이기 때문에 대조 학습의 성능 향상에 결정적인 역할을 한다.38</p>
<h3>3.2  마스크 이미지 모델링 (Masked Image Modeling): MAE의 접근법</h3>
<p>마스크 이미지 모델링(Masked Image Modeling, MIM)은 NLP 분야의 BERT에서 사용된 마스크 언어 모델링(Masked Language Modeling, MLM)에서 영감을 받은 자기지도학습 기법이다.24 MIM의 기본 아이디어는 입력 이미지의 일부를 의도적으로 가리고(masking), 모델이 주변의 보이는 영역(context)을 이용하여 가려진 부분을 예측하거나 복원하도록 훈련하는 것이다.40</p>
<p>MIM의 성공을 위한 핵심 요소 중 하나는 ’마스킹 전략’이다. 특히, Masked Autoencoders (MAE) 논문에서는 이미지 패치의 75%라는 매우 높은 비율을 무작위로 마스킹하는 전략을 채택했다.25 이렇게 높은 마스킹 비율은 모델이 단순히 인접 픽셀의 정보를 복사하여 문제를 해결하는 것을 방지하고, 이미지의 전체적인 구조와 의미(semantics)를 이해해야만 복원이 가능하도록 만드는 어려운 과제를 제시한다.42 복원 대상은 연구에 따라 다양한데, MAE나 SimMIM처럼 원본 픽셀 값을 직접 예측하기도 하고, BEiT처럼 이산적인 시각적 토큰을 예측하거나, 잠재 공간의 특징을 복원하는 방식도 있다.24</p>
<p>MAE의 가장 중요한 구조적 혁신은 ‘비대칭 인코더-디코더(asymmetric encoder-decoder)’ 설계에 있다.28 이 구조에서, 파라미터 수가 많은 무거운 인코더는 마스킹되지 않은 소수의 <em>보이는</em> 패치(예: 전체의 25%)만을 입력으로 받아 처리한다. 그 후, 인코딩된 잠재 표현과 함께, 가려진 위치에 해당하는 학습 가능한 ’마스크 토큰’들이 매우 가벼운 디코더에 입력되어 전체 이미지를 복원한다.28 이 비대칭 구조는 인코더의 계산량을 극적으로 줄여(예: 75% 마스킹 시 4배 이상 감소) 사전 훈련 속도를 3배 이상 가속화하며, 이는 대규모 모델을 효율적으로 훈련시키는 데 결정적인 역할을 한다.25</p>
<p>MIM, 특히 높은 마스킹 비율을 사용하는 접근법의 성공은 언어와 시각 데이터 간의 ‘정보 밀도(information density)’ 차이를 해결하는 독창적인 방법으로 이해할 수 있다. 언어는 정보 밀도가 매우 높다. 문장에서 단어 하나만 빠져도 전체 의미가 크게 훼손될 수 있으며, 빠진 단어를 예측하기 위해서는 문법, 의미, 문맥에 대한 깊은 이해가 필요하다.24 반면, 이미지는 공간적 중복성(spatial redundancy)이 매우 높다. 인접한 픽셀들은 서로 강하게 상관되어 있기 때문에, 이미지의 15%(MLM의 일반적인 마스킹 비율) 정도를 가리는 것은 주변 픽셀 정보를 보간하는 것만으로도 쉽게 해결할 수 있는, 비교적 쉬운 문제다.42 이는 모델이 고수준의 의미적 추론을 하도록 유도하지 못한다. MAE가 발견한 핵심은 75%라는 극단적으로 높은 마스킹 비율을 통해 이미지에 인위적인 ’정보 희소성(information scarcity)’을 만들어내는 것이었다.28 이미지의 대부분이 사라지면, 모델은 더 이상 지역적 중복성에 의존할 수 없게 된다. 대신, 보이는 조각들을 바탕으로 이미지의 전체적인 형태, 즉 객체의 구조, 부분들의 관계, 장면의 구성을 추론해야만 거대한 빈 공간을 채울 수 있다. 이 과정은 모델이 이전에 언어 모델의 전유물로 여겨졌던 풍부하고 의미적인 표현을 학습하도록 강제하는 효과를 낳는다.</p>
<h3>3.3  자기 증류 (Self-Distillation): DINO의 교사-학생 프레임워크</h3>
<p>자기 증류(Self-Distillation)는 레이블 없이 지식 증류(knowledge distillation) 프레임워크를 활용하는 독특한 자기지도학습 방식이다. 이 분야의 대표적인 모델인 DINO(self-<strong>DI</strong>stillation with <strong>NO</strong> labels)는 ‘교사(teacher)’ 네트워크와 ‘학생(student)’ 네트워크라는 두 개의 모델을 사용하여 학습을 진행한다.29</p>
<p>DINO의 교사와 학생 네트워크는 동일한 아키텍처를 공유하지만, 파라미터 업데이트 방식이 다르다. 학생 네트워크의 가중치는 표준적인 역전파(backpropagation) 알고리즘을 통해 직접 업데이트된다. 반면, 교사 네트워크의 가중치는 학생 네트워크의 과거 가중치들의 지수 이동 평균(Exponential Moving Average, EMA)으로 부드럽게 업데이트된다.46 이 방식은 교사 모델을 학생 모델의 여러 시점 버전을 앙상블한 것처럼 만들어, 더 안정적이고 일관된 예측을 생성하게 한다.</p>
<p>훈련 과정에서, 동일한 입력 이미지에 대해 서로 다른 증강(예: 다른 크기와 위치의 잘라내기)이 적용된 두 개의 뷰(view)가 생성된다. 하나의 뷰는 학생 네트워크에, 다른 뷰는 교사 네트워크에 입력된다. 학생 네트워크의 목표는 자신의 출력 확률 분포가 교사 네트워크의 출력 확률 분포와 최대한 일치하도록, 즉 두 분포 간의 교차 엔트로피(cross-entropy) 손실을 최소화하도록 학습하는 것이다.29</p>
<p>이러한 프레임워크는 ’모델 붕괴(model collapse)’라는 잠재적 위험을 내포한다. 모델 붕괴는 네트워크가 모든 입력에 대해 동일한 상수(예: 균등 분포)를 출력하여 손실을 쉽게 최소화하려는 자명한 해(trivial solution)에 빠지는 현상이다. DINO는 이를 방지하기 위해 두 가지 핵심 기법을 사용한다. 첫째, ’센터링(centering)’은 교사 네트워크의 출력 특징 벡터에서 배치 단위의 평균을 빼주어 특정 차원이 지배적인 활성화를 갖는 것을 막는다.47 둘째, ’샤프닝(sharpening)’은 교사 네트워크의 소프트맥스 함수에 낮은 온도를 적용하여 출력 확률 분포를 더 뾰족하게 만든다. 이는 학생이 교사의 예측 중 더 확신에 찬 부분에 집중하도록 유도하여 학습을 돕는다.29 DINO의 후속 버전인 DINOv2는 이 접근법을 1억 4200만 장의 대규모 큐레이션 데이터셋으로 확장하고, iBOT에서 영감을 받은 패치 레벨 목적 함수와 SwAV의 Sinkhorn-Knopp 정규화 같은 개선된 알고리즘을 도입하여 성능을 한층 더 끌어올렸다.29</p>
<p>DINO의 자기 증류 프레임워크는 암묵적인 온라인 클러스터링(online clustering) 메커니즘으로 작동한다. 교사 네트워크는 안정적이고 느리게 진화하는 목표 표현을 제공하고, 학생 네트워크는 다른 시점(view)에서도 동일한 표현을 생성하도록 강제된다. 이는 증강 기법에 대한 불변성(invariance)을 학습하게 만든다. 예를 들어, 고양이의 머리 부분을 보든 몸 전체를 보든 모델은 ’고양이’라는 동일한 의미적 표현을 생성해야 한다. 샤프닝과 센터링 기법은 교사의 출력 분포가 자명하지 않고 특징 공간에 잘 퍼지도록 보장한다. 이 전체 과정은 각 이미지에 교사의 출력 벡터라는 부드러운 ’유사 레이블’을 할당하고, 학생이 이 레이블을 예측하도록 학습하는 것과 기능적으로 동일하다. 수백만 개의 이미지를 통해 이 과정을 반복하면서, 모델은 다양한 모습의 고양이들이 특징 공간 내에서 유사한 영역으로 매핑되어야 일관성 목표를 만족시킬 수 있다는 것을 스스로 학습하게 된다. 이러한 창발적 행동(emergent behavior)이 바로 클러스터링의 한 형태이며, DINO로 훈련된 ViT의 셀프 어텐션 맵이 별도의 분할 레이블 없이도 객체와 배경을 명확하게 구분해내는 현상을 통해 시각적으로 확인할 수 있다.48 이는 명시적인 부정 샘플링이나 픽셀 복원 목표 없이도 의미적 개념과 객체 부분을 발견하는 DINO의 우아하고 강력한 학습 능력을 보여준다.</p>
<h2>4.  주요 비전 파운데이션 모델 심층 분석</h2>
<p>자기지도학습 패러다임의 발전은 컴퓨터 비전 분야에 기념비적인 여러 파운데이션 모델의 등장을 이끌었다. 이 모델들은 각각 고유한 아키텍처와 훈련 전략을 통해 시각적 표현 학습의 새로운 가능성을 제시했으며, 특정 응용 분야에서 혁신을 주도하고 있다. 이 장에서는 VFM 생태계에서 가장 중요한 위치를 차지하는 네 가지 핵심 모델—CLIP, DINOv2, MAE, SAM—을 심층적으로 분석한다. 먼저, 이들 모델의 핵심적인 특징을 한눈에 비교할 수 있는 표를 제시한 후, 각 모델의 아키텍처, 훈련 목표, 그리고 기술적 기여를 상세히 탐구하여 VFM의 현주소를 조망한다.</p>
<table><thead><tr><th>모델 (Model)</th><th>핵심 아키텍처 (Core Architecture)</th><th>주요 훈련 패러다임 (Primary Training Paradigm)</th><th>핵심 기여 (Key Contribution)</th><th>대표 응용 분야 (Primary Applications)</th></tr></thead><tbody>
<tr><td><strong>CLIP</strong></td><td>Dual-Encoder (Image &amp; Text)</td><td>대조 학습 (Contrastive Learning)</td><td>자연어 감독을 통한 강력한 제로샷 시각 인식 (Powerful zero-shot visual recognition via natural language supervision)</td><td>제로샷 이미지 분류, 이미지-텍스트 검색 (Zero-shot classification, Image-text retrieval)</td></tr>
<tr><td><strong>DINOv2</strong></td><td>Vision Transformer (ViT)</td><td>자기 증류 (Self-Distillation) &amp; MIM</td><td>레이블 없이 고품질의 조밀한 시각적 특징 추출 (Extraction of high-quality, dense visual features without labels)</td><td>시맨틱 분할, 깊이 추정, 객체 탐지 (Semantic segmentation, Depth estimation, Object detection)</td></tr>
<tr><td><strong>MAE</strong></td><td>Asymmetric Encoder-Decoder (ViT)</td><td>마스크 이미지 모델링 (Masked Image Modeling)</td><td>대규모 ViT 모델의 확장 가능하고 효율적인 자기지도학습 (Scalable and efficient self-supervised learning for large ViT models)</td><td>전이 학습을 위한 특징 추출기 백본 (Feature extractor backbone for transfer learning)</td></tr>
<tr><td><strong>SAM</strong></td><td>Encoder-Decoder with Prompt Encoder</td><td>대규모 데이터 기반의 프롬프트 가능 분할 (Promptable segmentation on a massive dataset)</td><td>제로샷 일반화를 갖춘 프롬프트 기반 범용 이미지 분할 (Promptable, universal image segmentation with zero-shot generalization)</td><td>대화형 주석, 의료 영상 분할, 창의적 편집 (Interactive annotation, Medical image segmentation, Creative editing)</td></tr>
</tbody></table>
<h3>4.1  CLIP: 언어와 이미지의 연결</h3>
<p>CLIP(Contrastive Language-Image Pre-training)은 OpenAI가 2021년에 발표한 모델로, 이미지와 텍스트라는 두 가지 다른 양식(modality)을 연결하여 시각적 개념을 학습하는 획기적인 접근법을 제시했다.34</p>
<p><strong>아키텍처 및 훈련 목표:</strong> CLIP은 이미지 인코더(Image Encoder)와 텍스트 인코더(Text Encoder)로 구성된 이중 인코더(Dual-Encoder) 구조를 가진다.49 이미지 인코더로는 ViT나 ResNet이, 텍스트 인코더로는 트랜스포머가 사용된다.34 이 두 인코더는 각각 이미지와 텍스트를 입력받아, 동일한 차원의 다중모드 임베딩 공간(multimodal embedding space)으로 투영하는 역할을 한다.34</p>
<p>CLIP의 훈련은 웹에서 수집한 4억 개의 (이미지, 텍스트) 쌍 데이터를 기반으로 한 대조 학습 목표를 따른다.34 훈련 시, N개의 (이미지, 텍스트) 쌍으로 구성된 배치가 주어지면, 모델은 N×N개의 가능한 모든 조합 중에서 실제 올바른 N개의 쌍을 식별하도록 학습된다.34 구체적으로, 올바른 쌍의 이미지 임베딩과 텍스트 임베딩 간의 코사인 유사도는 최대화하고, 나머지</p>
<p>N2−N개의 잘못된 쌍의 유사도는 최소화한다.53 이 목표는 InfoNCE 손실 함수와 동일한 형태의 대칭적 교차 엔트로피 손실(symmetric cross-entropy loss)을 통해 달성된다.53</p>
<p><strong>핵심 능력 및 기여:</strong> CLIP의 가장 강력한 능력은 ’제로샷 분류(zero-shot classification)’이다.54 전통적인 분류 모델이 고정된 클래스 집합에 대해서만 예측할 수 있었던 반면, CLIP은 자연어 프롬프트를 통해 어떤 클래스든 분류할 수 있다. 예를 들어, 이미지를 ‘개’, ‘고양이’, ’자동차’로 분류하고 싶다면, “a photo of a dog”, “a photo of a cat”, “a photo of a car“와 같은 텍스트 프롬프트를 생성한다. 그 후, 각 프롬프트의 텍스트 임베딩과 주어진 이미지의 임베딩 간의 유사도를 계산하여 가장 높은 점수를 받은 프롬프트를 예측 결과로 선택한다.49</p>
<p>CLIP의 진정한 혁신은 기술적인 측면을 넘어 개념적인 차원에 있다. 이 모델은 웹에 존재하는 방대하고, 정제되지 않았으며, 비구조적인 인간의 언어적 지식을 직접 시각 모델로 ’증류(distill)’할 수 있음을 증명했다. 이는 ‘개’, ’고양이’와 같이 깨끗하고 이산적인 클래스 레이블에 의존하던 기존의 지도 학습 패러다임에서 완전히 벗어나, 보다 인간과 유사하고 유연한 방식으로 시각 모델과 상호작용할 수 있는 새로운 길을 열었다.</p>
<h3>4.2  DINO &amp; DINOv2: 레이블 없는 데이터로부터의 학습</h3>
<p>DINO와 그 후속작인 DINOv2는 Meta AI에서 개발한 모델로, 텍스트와 같은 외부 정보 없이 순수하게 시각 데이터 자체만으로 강력한 표현을 학습할 수 있음을 보여주었다.29</p>
<p><strong>아키텍처 및 훈련 목표:</strong> DINO 계열 모델은 단일 ViT를 백본으로 사용한다.29 훈련은 3.3절에서 설명한 자기 증류(self-distillation) 방식을 따른다. 즉, 지수 이동 평균으로 업데이트되는 안정적인 교사 네트워크의 출력을 학생 네트워크가 다른 증강된 뷰를 보고 따라 하도록 학습한다.47 DINOv2는 여기에 iBOT에서 영감을 받은 마스크 이미지 모델링 목표를 추가하고, 더 안정적인 학습을 위한 알고리즘 개선과 1억 4200만 장에 달하는 대규모 큐레이션 데이터셋을 활용하여 성능을 극대화했다.29</p>
<p><strong>핵심 능력 및 기여:</strong> DINOv2의 가장 큰 특징은 레이블 없이도 매우 높은 품질의 ‘조밀한(dense)’ 시각적 특징을 학습한다는 점이다.29 ’조밀하다’는 것은 이미지 전체에 대한 단일 표현(전역적 특징)뿐만 아니라, 각 패치(patch) 수준에서의 상세한 표현(지역적 특징)까지 학습한다는 의미이다. 이 조밀한 특징은 시맨틱 분할, 깊이 추정, 객체 탐지와 같이 픽셀 수준의 이해가 필요한 하위 작업에서 특히 강력한 성능을 발휘한다.29 놀랍게도, DINO로 훈련된 ViT의 셀프 어텐션 맵은 별도의 분할 훈련 없이도 이미지 내 객체의 경계를 명확하게 구분해내는 능력을 보여준다.48 DINOv2는 종종 미세 조정 없이, 사전 훈련된 특징 위에 간단한 선형 분류기(linear probe)를 추가하는 것만으로도 전문화된 지도 학습 모델에 필적하거나 이를 능가하는 성능을 달성한다.29</p>
<p>DINO/DINOv2의 성공은 순수한 시각적 자기지도학습 목표가 적절한 규모로 확장될 경우, CLIP과 같이 텍스트로부터 약한 감독 신호를 받는 방법론에 필적하거나 능가하는 특징을 생성할 수 있음을 입증했다는 점에서 중요한 의미를 갖는다. 이는 텍스트-이미지 쌍 데이터가 부족한 위성 영상이나 의료 영상과 같은 전문 분야에서 VFM을 활용할 수 있는 중요한 가능성을 열어주었다.58 즉, 풍부한 의미적 이해가 반드시 언어와 같은 외부 양식에 의존하지 않고, 시각적 구조 자체로부터 창발될 수 있음을 보여준 것이다.</p>
<h3>4.3  MAE: 비대칭 인코더-디코더와 효율적 학습</h3>
<p>MAE(Masked Autoencoders)는 Kaiming He 연구팀이 제안한 모델로, 마스크 이미지 모델링을 통해 대규모 ViT를 매우 효율적으로 사전 훈련하는 방법을 제시했다.25</p>
<p><strong>아키텍처 및 훈련 목표:</strong> MAE의 핵심은 3.2절에서 설명한 ‘비대칭 인코더-디코더’ 구조에 있다.44 무거운 ViT 인코더는 입력 이미지의 75%를 마스킹하고 남은 25%의 보이는 패치만을 처리한다. 그 후, 인코딩된 잠재 표현과 마스크 토큰이 훨씬 작고 가벼운 디코더에 전달되어 원본 이미지의 픽셀 값을 복원한다.28 훈련 목표는 복원된 패치와 원본 패치 간의 평균 제곱 오차(Mean Squared Error, MSE)를 최소화하는 것이다.45</p>
<p><strong>핵심 능력 및 기여:</strong> MAE의 가장 큰 기여는 사전 훈련 과정에서의 ’극적인 계산 효율성’이다.28 무거운 인코더가 전체 패치의 극히 일부만 처리하기 때문에, 훈련 시간을 3배 이상 단축시킬 수 있다. 이 효율성 덕분에 이전에는 막대한 계산 자원이 필요했던 ViT-Large나 ViT-Huge와 같은 거대 모델을 이미지넷-1K와 같은 표준 데이터셋만으로도 성공적으로 훈련하고, 최첨단(state-of-the-art) 성능을 달성하는 것이 가능해졌다.25</p>
<p>MAE의 혁신은 표현 학습(representation learning)에 있어 <em>인코딩</em>이 <em>디코딩</em>보다 훨씬 중요하다는 통찰을 실증적으로 보여준 데 있다. 표현 학습의 주된 역할을 하는 인코더와, 단순히 픽셀 복원이라는 사전 훈련 과제(pretext task)를 수행하는 디코더의 역할을 명확히 분리하고 디코더를 극도로 경량화함으로써, MAE는 엄청난 효율성 향상을 이루었다. 이는 전통적인 오토인코더 패러다임에 도전하는 발상이다. 즉, 픽셀 복원이라는 과제는 그 자체로 목적이 아니라, 인코더가 풍부한 잠재 표현을 학습하도록 강제하는 ’수단’에 불과하며, 디코더는 이 목적을 달성한 후에는 버려질 수 있는 단순한 유틸리티라는 점을 시사한다.</p>
<h3>4.4  SAM: 프롬프트 기반 분할의 혁신</h3>
<p>SAM(Segment Anything Model)은 Meta AI가 개발한 모델로, 이미지 분할(segmentation) 작업의 패러다임을 근본적으로 바꾸었다.9</p>
<p><strong>아키텍처 및 훈련 데이터:</strong> SAM은 세 가지 주요 구성 요소로 이루어져 있다: (1) 이미지당 한 번만 실행되는 무거운 ViT-H 이미지 인코더, (2) 점, 상자, 마스크와 같은 프롬프트를 빠르게 인코딩하는 프롬프트 인코더, (3) 이미지 임베딩과 프롬프트 임베딩을 결합하여 실시간으로 분할 마스크를 예측하는 가벼운 마스크 디코더.60</p>
<p>SAM의 경이로운 성능은 SA-1B라는 전례 없는 규모의 데이터셋에 기반한다. 이 데이터셋은 1100만 개의 이미지에 대해 10억 개 이상의 분할 마스크를 포함하고 있으며, ’데이터 엔진(data engine)’이라는 독특한 방식으로 구축되었다. 이 방식은 모델이 직접 데이터 생성 과정에 참여하는 인간-참여형(human-in-the-loop) 프로세스로, SAM을 사용하여 이미지에 주석을 달고, 그 데이터로 다시 SAM을 업데이트하는 과정을 반복하여 모델과 데이터셋을 동시에 개선했다.60</p>
<p><strong>핵심 능력 및 기여:</strong> SAM의 핵심 능력은 ’프롬프트 기반 제로샷 분할’이다.61 사용자가 이미지 위의 특정 지점을 클릭하거나 경계 상자를 그리는 등 상호작용적인 프롬프트를 제공하면, SAM은 해당 객체에 대한 분할 마스크를 즉시 생성한다.59 중요한 것은 이 과정이 특정 객체 클래스나 이미지 도메인에 대한 추가적인 훈련 없이도 가능하다는 점이다.9 후속 모델인 SAM 2는 스트리밍 메모리 아키텍처를 도입하여 이 능력을 비디오 영역으로 확장했다.9</p>
<p>SAM의 성공은 데이터 규모와 상호작용적 모델 설계의 힘을 증명한다. 이 모델은 분할의 목표를 “고정된 의미론적 클래스 집합을 예측하는 것“에서 “이 위치에 있는 그럴듯한 객체는 무엇인가?“로 재정의했다. 의미론적 레이블과 무관하게 방대한 양의 객체 마스크 데이터로 훈련함으로써, SAM은 “객체가 무엇인지에 대한 일반적인 개념“을 학습했다.62 이처럼 분할(segmentation) 작업을 분류(classification) 작업으로부터 분리한 것이 바로 SAM의 강력한 제로샷 일반화 능력의 원천이다.</p>
<h2>5.  VFM의 응용: 제로샷과 오픈 보캡의 시대</h2>
<p>비전 파운데이션 모델의 등장은 단순히 학문적 성과에 그치지 않고, 컴퓨터 비전 기술이 현실 세계의 문제들을 해결하는 방식에 실질적인 변화를 가져오고 있다. VFM의 강력한 일반화와 적응 능력은 기존의 인식 과제를 새로운 차원으로 끌어올렸으며, 이전에는 불가능하다고 여겨졌던 ‘오픈 보캡(open-vocabulary)’ 및 ‘제로샷(zero-shot)’ 시나리오를 현실화하고 있다. 이 장에서는 이미지 분류, 객체 탐지, 시맨틱 분할과 같은 전통적인 과제에서 VFM이 어떻게 활용되는지 살펴보고, 나아가 오픈 보캡 객체 탐지, 제로샷 시맨틱 분할과 같은 최첨단 응용 분야와 의료 영상 분석과 같은 전문 분야에서의 활용 사례를 통해 VFM의 광범위한 영향력을 조명한다.</p>
<h3>5.1  차세대 인식 과제: 이미지 분류, 객체 탐지, 시맨틱 분할</h3>
<p>이미지 분류, 객체 탐지, 시맨틱 분할은 컴퓨터 비전의 핵심적인 인식 과제들이다. VFM은 이러한 과제들을 해결하는 데 있어 강력한 백본(backbone) 역할을 수행하며, 개발 워크플로우를 근본적으로 변화시키고 있다.23</p>
<p>과거에는 각 과제를 위해 대규모의 레이블링된 데이터셋을 구축하고, 그에 맞는 모델을 처음부터 훈련시키는 것이 일반적이었다. 그러나 VFM의 등장으로 이러한 패러다임이 바뀌었다. 이제 개발자들은 방대한 데이터로 사전 훈련된 VFM을 기반으로, 상대적으로 적은 양의 특정 작업 데이터만으로 모델을 미세 조정(fine-tuning)하여 높은 성능을 달성할 수 있다.4 예를 들어, DINOv2와 같이 조밀한 특징(dense feature) 추출에 뛰어난 VFM은 시맨틱 분할과 같은 픽셀 단위 예측 작업에서 탁월한 성능을 보인다.29</p>
<p>VFM을 활용하는 가장 큰 이점은 전이 학습(transfer learning)의 효율성이다. 사전 훈련된 VFM을 고정된 특징 추출기로 사용하고 그 위에 간단한 예측 헤드(head)만 훈련시키거나, 전체 모델을 소량의 데이터로 미세 조정하는 것만으로도 충분히 높은 성능을 얻을 수 있다. 이는 데이터 레이블링에 드는 막대한 비용과 시간을 절감시켜, 고성능 컴퓨터 비전 기술에 대한 접근성을 높이고 기술의 민주화에 기여한다.4 결과적으로, VFM은 표준적인 컴퓨터 비전 과제의 경제성과 개발 워크플로우를 근본적으로 개선하고 있다.</p>
<h3>5.2  오픈 보캡 객체 탐지</h3>
<p>전통적인 객체 탐지 모델들은 훈련 데이터셋에 정의된 고정된 수의 클래스(예: ‘사람’, ‘자동차’, ‘개’)만 탐지할 수 있다는 치명적인 한계를 가졌다. 이는 현실 세계에서 마주치는 무한에 가까운 객체 종류를 처리하기에 부적합했다. 오픈 보캡 객체 탐지(Open-Vocabulary Object Detection, OVD)는 이러한 한계를 극복하기 위한 새로운 패러다임이다.67</p>
<p>OVD의 목표는 훈련 중에 보지 못했던 새로운 카테고리를 포함하여, 사용자가 추론 시점에 자연어 텍스트로 지정하는 임의의 객체를 탐지하는 것이다.67 이 기술을 가능하게 한 핵심 동력은 CLIP과 같은 비전-언어 모델(Vision-Language Models, VLM)이다.67 OVD 모델의 일반적인 작동 방식은 다음과 같다: 먼저, VFM 기반의 비전 인코더가 입력 이미지로부터 풍부한 시각적 특징을 추출한다. 다음으로, Faster R-CNN의 RPN(Region Proposal Network)과 같은 지역 제안 모듈이 이미지 내에서 객체가 있을 법한 후보 영역들을 제안한다. 마지막으로, CLIP의 텍스트 인코더를 사용하여 사용자가 입력한 텍스트(예: “빨간 스포츠카”)를 임베딩하고, 이 텍스트 임베딩과 각 후보 영역의 시각적 특징 간의 유사도를 계산하여 가장 일치하는 객체를 분류하고 위치를 특정한다.68</p>
<p>OVD는 AI 시스템이 ‘닫힌 세계(closed-world)’ 가정을 벗어나, 예측 불가능하고 새로운 객체가 계속해서 나타나는 ’열린 세계(open-world)’와 상호작용할 수 있게 만든다는 점에서 중요하다.67 이는 로보틱스, 자율 주행, 증강 현실 등 동적인 환경에서 작동해야 하는 AI 시스템의 유연성과 실용성을 극적으로 향상시키는 핵심 기술이다.</p>
<h3>5.3  제로샷 시맨틱 분할</h3>
<p>제로샷 시맨틱 분할(Zero-Shot Semantic Segmentation, ZSS)은 OVD보다 한 단계 더 나아간 도전적인 과제다. ZSS는 훈련 시에 보지 못했던 클래스에 대해 이미지의 모든 픽셀에 의미론적 레이블을 할당하는 것을 목표로 한다.69 이는 픽셀 수준의 조밀한 예측을 요구하기 때문에, 이미지 전체 수준의 텍스트로 훈련된 CLIP과 같은 VLM을 직접 적용하기 어렵다. VLM이 제공하는 높은 수준의 의미적 이해 능력과, 분할에 필요한 정교한 형태 묘사 능력 사이에 간극이 존재하기 때문이다.70</p>
<p>이 문제를 해결하기 위해 다양한 접근법이 연구되고 있다. 한 가지 방법은 생성 모델을 사용하여 보지 못한 클래스에 대한 시각적 특징을 합성하는 것이다.70 또 다른 방법은 CLIP과 같은 VLM을 활용하여 픽셀 수준의 시각적 특징과 텍스트 임베딩을 정렬(align)하는 것이다.69 최근 연구들은 VLM이 제공하는 의미적 풍부함과, DINO와 같은 자기지도학습 모델이 제공하는 형태 및 경계 포착 능력을 결합하려는 시도를 하고 있다.70 예를 들어, VLM으로 생성된 의미론적 예측과, 자기지도학습 특징으로 구성된 스펙트럼 분해(spectral decomposition) 결과를 융합하여 경계가 명확한 분할 맵을 생성하는 방식이다.70</p>
<p>ZSS 과제가 직면한 도전은 현재 VFM들이 가진 근본적인 긴장 관계, 즉 비전-언어 모델의 ’의미적 풍부함’과 자기지도 모델의 ‘공간적 정밀성’ 사이의 트레이드오프를 잘 보여준다. 미래의 조밀 예측(dense prediction) 기술은 이 두 패러다임의 장점을 효과적으로 융합하여, 의미를 깊이 이해하면서도 경계를 정교하게 구분할 수 있는 하이브리드 모델의 개발에 달려 있을 것이다.</p>
<h3>5.4  전문 분야 활용 사례: 의료 영상 분석을 중심으로</h3>
<p>VFM의 일반화 능력은 의료 영상 분석과 같은 고도로 전문화된 분야에서도 그 잠재력을 발휘하고 있다. CT, MRI, 병리 슬라이드 이미지 등에서 장기나 종양을 분할하는 작업은 질병 진단과 치료 계획 수립에 매우 중요하지만, 막대한 양의 전문적인 레이블링 데이터를 필요로 하는 어려운 과제다.64</p>
<p>VFM은 이러한 데이터 부족 문제를 해결할 유망한 대안으로 떠오르고 있다. 퓨샷 또는 제로샷 학습 능력을 통해 소량의 데이터만으로도 모델을 특정 의료 영상 작업에 적용할 수 있기 때문이다.73 하지만 여기에는 중요한 도전 과제가 존재한다. 대부분의 VFM이 훈련된 일반적인 자연 이미지와 의료 영상 사이에는 상당한 ’도메인 격차(domain gap)’가 존재한다.64 이로 인해 SAM과 같은 범용 VFM을 의료 영상에 직접 적용하면 만족스럽지 못한 결과를 얻는 경우가 많다.74</p>
<p>이 문제를 해결하기 위한 연구는 두 가지 방향으로 진행되고 있다. 첫 번째는 범용 VFM을 특정 의료 데이터셋으로 미세 조정하여 도메인에 적응시키는 것이다. 두 번째는 더 나아가, 대규모 의료 영상 데이터로 처음부터 사전 훈련된 ’의료 전문 VFM’을 개발하는 것이다. MedSAM, SAM-Med3D, RETFound와 같은 모델들이 이러한 노력의 결과물이며, 범용 모델을 미세 조정한 것보다 훨씬 우수한 성능을 보여준다.73</p>
<p>의료 영상 분야는 VFM 일반화 능력의 한계를 시험하는 중요한 시험대 역할을 한다. 이는 범용적인 세계 지식이 강력하기는 하지만 만병통치약은 아니라는 점을 분명히 보여준다. 전문 분야 AI의 미래는 아마도 2단계 접근법을 따를 것이다. 먼저, 거대한 범용 VFM이 세상에 대한 폭넓은 이해의 기반을 제공하고, 그 다음 이 모델을 특정 분야의 전문가 데이터로 지속적으로 사전 훈련하여 강력한 ’도메인 특화 파운데이션 모델’을 만드는 방식이다.</p>
<h2>6.  도전 과제와 윤리적 고찰</h2>
<p>비전 파운데이션 모델이 제시하는 혁신적인 가능성에도 불구하고, 이 기술의 발전과 보급에는 해결해야 할 중대한 기술적, 윤리적 과제들이 산재해 있다. VFM의 막대한 규모는 그 능력의 원천인 동시에 기술적 한계와 잠재적 위험의 근원이 되기도 한다. 또한, 사회의 데이터를 기반으로 학습하는 VFM은 현실 세계의 편향과 불평등을 그대로 반영하고 증폭시킬 수 있다. 이 장에서는 VFM이 직면한 기술적 한계와 윤리적 문제들을 비판적으로 검토하며, 책임 있는 AI 개발을 위해 필요한 고려사항들을 논의한다.</p>
<h3>6.1  기술적 한계: 계산 비용, 데이터 요구사항, 견고성</h3>
<p>VFM의 개발과 운영에는 몇 가지 본질적인 기술적 한계가 따른다.</p>
<p>첫째, <strong>엄청난 계산 비용</strong>이 가장 큰 장벽이다. 파운데이션 모델을 처음부터 훈련시키는 데는 수천 개의 고성능 GPU를 수 주에서 수개월 동안 가동해야 하며, 이는 수백만에서 수억 달러에 달하는 비용을 발생시킨다.1 이러한 막대한 자원 요구는 소수의 거대 기술 기업과 대규모 연구 기관만이 VFM 개발을 주도할 수 있게 하여, 기술력의 집중과 독점을 심화시킨다.18</p>
<p>둘째, <strong>방대한 데이터 요구사항</strong> 역시 중요한 문제다. VFM은 웹 스케일의 데이터를 필요로 하지만, 이러한 데이터를 수집하고 정제하는 것은 매우 어렵다.3 데이터의 품질, 다양성, 저작권 문제 등은 통제하기 어려운 변수이며, 현재 사용되는 대규모 데이터셋의 구성과 출처는 대부분 투명하게 공개되지 않고 있다.18</p>
<p>셋째, <strong>견고성 및 예측 불가능성</strong>의 문제가 있다. VFM은 훈련 데이터 분포에서 벗어난 입력(out-of-distribution data)에 취약할 수 있으며, 객체의 개수를 세거나 거리를 예측하는 등 추상적이고 체계적인 추론 능력에는 한계를 보인다.16 또한, 모델의 규모가 커지면서 명시적으로 프로그래밍되지 않은 새로운 능력이 나타나는 ‘창발적 속성(emergent properties)’ 때문에 모델의 행동을 예측하고 제어하기가 어렵다.16 이는 고장 모드를 이해하기 어렵게 만들고, 적대적 공격(adversarial attack)에 대한 취약성을 높여 공학적, 안전적 측면에서 심각한 도전 과제를 제기한다.18</p>
<p>결론적으로, VFM에 강력한 성능을 부여하는 바로 그 ’규모’가 역설적으로 가장 큰 약점이 된다. 이는 거대하고 중앙 집중화된 자원에 대한 의존성을 낳고, 엣지 디바이스와 같은 많은 실제 응용 환경에 배포하기에는 너무 큰 모델을 만들며, 그 실패 모드를 제대로 이해하기 어려운 복잡한 시스템을 탄생시킨다.16</p>
<h3>6.2  윤리적 문제: 데이터 편향, 개인정보 보호, 투명성</h3>
<p>VFM은 기술적 한계를 넘어 사회에 깊은 영향을 미칠 수 있는 윤리적 문제들을 안고 있다. 이는 단순한 기술적 결함이 아니라, VFM이 개발되고 배포되는 사회기술적 시스템과 복잡하게 얽혀 있다.</p>
<p>가장 심각한 문제는 **데이터 편향(data bias)**이다. VFM은 정제되지 않은 인터넷 데이터를 학습 자료로 삼기 때문에, 데이터에 내재된 인종, 성별, 문화, 직업 등에 대한 사회적 편견과 고정관념을 그대로 학습하고 증폭시킬 수 있다.34 예를 들어, 특정 인종이나 성별에 대한 부정적인 연관성을 학습한 모델은 의료 진단, 채용 심사, 보안 시스템 등에서 차별적이고 해로운 결과를 초래할 수 있다.4</p>
<p><strong>개인정보 보호(privacy)</strong> 역시 중요한 우려 사항이다. 웹에서 수집된 훈련 데이터에는 개인 식별 정보나 민감한 내용이 포함될 수 있으며, 모델이 이를 의도치 않게 ’기억’하여 출력 결과로 재생산할 위험이 있다.16 이는 심각한 개인정보 침해로 이어질 수 있으며, GDPR과 같은 데이터 보호 규정 준수에도 어려움을 야기한다.16</p>
<p>마지막으로, **투명성과 설명가능성(transparency and explainability)**의 부재가 문제된다. VFM은 수십억 개의 파라미터로 구성된 복잡한 시스템이기 때문에, 특정 결정을 내린 이유를 인간이 이해하기 매우 어렵다. 이러한 ‘블랙박스’ 특성은 모델의 예측을 신뢰하고 그 결과에 책임을 묻기 어렵게 만들며, 특히 의료, 금융, 법률과 같이 결정의 파급 효과가 큰 고위험 분야에서 VFM의 도입을 가로막는 주요 장벽이 된다.18</p>
<p>이러한 윤리적 문제들은 VFM이 단순한 기술적 도구가 아님을 보여준다. VFM은 사회의 데이터를 먹고 자라며, 그 데이터에 담긴 편견과 불평등을 비추는 거울 역할을 한다. 따라서 이 문제들을 해결하기 위해서는 알고리즘적 개선뿐만 아니라, 데이터 거버넌스, 규제, 개발자의 투명성 확보 노력 등 다학제적인 접근이 필수적이다.18</p>
<h2>7.  결론: 비전 파운데이션 모델의 미래 전망</h2>
<p>비전 파운데이션 모델은 컴퓨터 비전 분야에 전례 없는 변화를 가져왔으며, 그 발전은 이제 시작 단계에 있다. 지금까지의 논의를 종합해 볼 때, VFM의 미래는 단순히 더 큰 모델을 만드는 것을 넘어, 인간의 인식 능력에 더 가까워지고, 더 효율적이며, 더 안전하고 신뢰할 수 있는 방향으로 나아갈 것이다. 이 장에서는 VFM의 미래를 형성할 세 가지 핵심 연구 방향인 멀티모달리티로의 확장, 모델 압축 및 효율성 증대, 그리고 지속 및 연합 학습의 가능성을 조망하며 본 보고서를 마무리한다.</p>
<h3>7.1  멀티모달리티로의 확장</h3>
<p>VFM의 미래는 시각과 언어를 넘어 더 다양한 감각 정보를 통합하는 ’멀티모달리티(multimodality)’로 향하고 있다. 현재의 VFM이 주로 이미지와 텍스트를 다루는 반면, 미래의 파운데이션 모델은 오디오, 깊이 정보, 열화상, 시계열 데이터, 심지어 로봇의 행동 데이터까지 통합하여 처리하게 될 것이다.6</p>
<p>이러한 다중모드 통합은 AI가 세상을 보다 총체적으로 이해하고 상호작용하는 데 필수적이다.80 예를 들어, 시각 정보와 소리를 함께 이해하는 모델은 비디오 속 사건의 맥락을 훨씬 더 깊이 파악할 수 있으며, 시각과 촉각, 행동 데이터를 결합한 모델은 물리적 세계에서 더 정교한 작업을 수행하는 로봇을 구현할 수 있다. 이처럼 서로 다른 종류의 데이터 스트림을 공동으로 이해하고 추론하는 능력은 단일 양식에 국한된 모델로는 불가능한 복잡한 과제를 해결할 수 있게 한다. 멀티모달리티로의 확장은 결국 인간의 인지 방식과 유사하게, 다양한 감각을 통해 세상을 종합적으로 인식하는 AI 시스템을 향한 중요한 발걸음이 될 것이다.</p>
<h3>7.2  모델 압축 및 효율성 증대</h3>
<p>VFM의 규모가 기하급수적으로 커짐에 따라, 이 거대한 모델을 스마트폰, 자율주행차, IoT 기기와 같은 자원이 제한된 ‘엣지(edge)’ 환경에 배포하기 위한 효율성 증대 기술의 중요성이 날로 커지고 있다.6</p>
<p>이를 위한 핵심 연구 분야로는 다음과 같은 모델 압축(model compression) 기술이 있다.82</p>
<ul>
<li>
<p><strong>지식 증류 (Knowledge Distillation):</strong> 거대하고 복잡한 ‘교사’ 모델이 학습한 지식을 작고 가벼운 ‘학생’ 모델에게 전달하여, 성능 저하를 최소화하면서 모델 크기를 줄이는 기법이다.46</p>
</li>
<li>
<p><strong>양자화 (Quantization):</strong> 모델의 가중치를 표현하는 데 사용되는 부동소수점 숫자의 정밀도를 낮추어(예: 32비트에서 8비트로) 메모리 사용량과 계산량을 줄이는 기술이다.</p>
</li>
<li>
<p><strong>가지치기 (Pruning):</strong> 모델의 성능에 거의 영향을 미치지 않는 불필요한 가중치나 연결을 제거하여 네트워크를 희소(sparse)하게 만드는 방법이다.</p>
</li>
<li>
<p><strong>저계수 근사 (Low-Rank Approximation):</strong> 거대한 가중치 행렬을 더 작은 행렬들의 곱으로 분해하여 파라미터 수를 줄이는 기법이다.</p>
</li>
</ul>
<p>효율성은 단순히 실용적인 문제를 넘어, AI 기술의 민주화와 보편화를 위한 핵심적인 요소다. 강력한 AI 능력을 클라우드 데이터센터에서 벗어나 우리 주변의 모든 기기로 가져오는 것은, 실시간 저지연 성능이 필수적인 로보틱스, 증강 현실, 개인화된 헬스케어 등 새로운 응용 분야를 열어줄 것이다.</p>
<h3>7.3  지속 학습과 연합 학습의 가능성</h3>
<p>현재의 VFM은 대부분 한 번 대규모로 훈련된 후 정적인 상태로 배포되는 ‘일회성 훈련(train once, deploy forever)’ 모델이다. 그러나 현실 세계는 끊임없이 변화하므로, 미래의 AI 시스템은 이러한 변화에 적응할 수 있는 능력을 갖추어야 한다.</p>
<p>**지속 학습(Continual Learning)**은 모델이 새로운 데이터를 접했을 때, 과거에 학습한 지식을 잊지 않으면서 새로운 지식을 점진적으로 학습해 나가는 ‘평생 학습(lifelong learning)’ 능력을 목표로 한다.6 이는 모델이 항상 최신 정보를 유지하고, 변화하는 환경에 적응할 수 있게 해준다.</p>
<p>**연합 학습(Federated Learning)**은 데이터 프라이버시 문제를 해결할 유망한 대안이다.73 이 방식은 민감한 원본 데이터(예: 여러 병원의 의료 기록)를 중앙 서버로 옮기지 않고, 각 데이터가 위치한 로컬 환경에서 모델을 훈련시킨 후, 그 결과(예: 가중치 업데이트 값)만을 안전하게 취합하여 전체 모델을 개선한다.18 이는 개인정보를 보호하면서도 다양한 분산 데이터 소스로부터 학습할 수 있는 길을 열어준다.</p>
<p>지속 학습과 연합 학습은 VFM 훈련 패러다임의 미래를 대표한다. 이 기술들은 정적이고 중앙 집중적인 현재의 모델을 넘어, 끊임없이 적응하고, 개인화되며, 프라이버시를 존중하는 차세대 AI 시스템으로 나아가는 핵심 동력이 될 것이다. 분산된 개인 데이터로부터 지속적으로 학습하는 VFM은 오늘날의 모델보다 훨씬 더 견고하고, 현실 세계와 밀접하며, 신뢰할 수 있는 인공지능으로 진화할 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>The Full Guide to Foundation Models - Encord, https://encord.com/blog/foundation-models/</li>
<li>What is a foundation model? - Ada Lovelace Institute, https://www.adalovelaceinstitute.org/resource/foundation-models-explainer/</li>
<li>What Is a Foundation Model? How Does It Differ From Regular AI? - Kopius, https://kopiustech.com/insights-innovation/foundation-models/</li>
<li>Visual Foundation Models (VFMs) Explained - Encord, https://encord.com/blog/visual-foundation-models-vfms-explained/</li>
<li>What are Foundation Models? - Foundation Models in Generative AI Explained - AWS, https://aws.amazon.com/what-is/foundation-models/</li>
<li>Vision Foundation Models (VFMs) - Emergent Mind, https://www.emergentmind.com/topics/vision-foundation-models-vfms</li>
<li>Traditional AI vs Foundation Models - AI with Armand, https://newsletter.armand.so/p/traditional-ai-vs-foundation-models</li>
<li>What Is Self-Supervised Learning? - IBM, https://www.ibm.com/think/topics/self-supervised-learning</li>
<li>A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering - arXiv, https://arxiv.org/html/2306.06211v4</li>
<li>Foundation model - Wikipedia, https://en.wikipedia.org/wiki/Foundation_model</li>
<li>What is Computer Vision? - IBM, https://www.ibm.com/think/topics/computer-vision</li>
<li>CNN Variants for Computer Vision: History, Architecture, Application, Challenges and Future Scope - MDPI, https://www.mdpi.com/2079-9292/10/20/2470</li>
<li>Vision AI History: From Edge Detection to YOLOv8 - Ultralytics, https://www.ultralytics.com/blog/a-history-of-vision-models</li>
<li>Three things everyone should know about Vision Transformers - arXiv, https://arxiv.org/pdf/2203.09795</li>
<li>Self-supervised Learning in Computer Vision: A Review - ResearchGate, https://www.researchgate.net/publication/364415234_Self-supervised_Learning_in_Computer_Vision_A_Review</li>
<li>On the Opportunities and Risks of Foundation … - Stanford CRFM, https://crfm.stanford.edu/assets/report.pdf</li>
<li>How Have Foundation Models Redefined Computer Vision Using AI? - Encord, https://encord.com/blog/foundation-models-redefining-computer-vision/</li>
<li>Ethical framework for responsible foundational models in medical imaging - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC12128638/</li>
<li>80 Years of Computer Vision: From Early Concepts to State-of-the-Art AI - Network Optix, https://www.networkoptix.com/blog/2024/08/01/80-years-of-computer-vision-from-early-concepts-to-state-of-the-art-ai</li>
<li>History Of Computer Vision - Let’s Data Science, https://letsdatascience.com/learn/history/history-of-computer-vision/</li>
<li>History of computer vision contests won by deep CNNs on GPU - IDSIA, https://people.idsia.ch/~juergen/computer-vision-contests-won-by-gpu-cnns.html</li>
<li>arXiv:2010.11929v2 [cs.CV] 3 Jun 2021, https://arxiv.org/abs/2010.11929</li>
<li>A List of Foundation Models to Get the Most out of Your Data | Labelbox, https://labelbox.com/foundation-models/list-of-foundation-models/</li>
<li>A Brief History of Masked Image Modeling Family | by Vyhao | Medium, https://medium.com/@vyhao02/a-brief-history-of-masked-image-modeling-family-15ca1f21cc15</li>
<li>[D] (Paper Overview) MAE: Masked Autoencoders Are Scalable Vision Learners - Reddit, https://www.reddit.com/r/MachineLearning/comments/qt4y6g/d_paper_overview_mae_masked_autoencoders_are/</li>
<li>Visual Explanations of Vision Transformer Guided by Self-Attention - arXiv, https://arxiv.org/abs/2402.04563</li>
<li>Self-supervised Learning in Remote Sensing: A Review, https://elib.dlr.de/190040/1/2206.13188.pdf</li>
<li>Masked Autoencoders Are Scalable Vision Learners - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.pdf</li>
<li>Understanding DINOv2: Engineer’s Deep Dive - Lightly, https://www.lightly.ai/blog/dinov2</li>
<li>Self-Supervised Learning in Computer Vision: A Comprehensive Review, https://ijarsct.co.in/A27058.pdf</li>
<li>An Introduction to Contrastive Learning for Computer Vision - Lightly, https://www.lightly.ai/blog/contrastive-learning</li>
<li>Contrastive Learning: A Comprehensive Guide | by Juan C Olamendy | Medium, https://medium.com/@juanc.olamendy/contrastive-learning-a-comprehensive-guide-69bf23ca6b77</li>
<li>Full Guide to Contrastive Learning | Encord, https://encord.com/blog/guide-to-contrastive-learning/</li>
<li>Understanding OpenAI’s CLIP model | by Szymon Palucha - Medium, https://medium.com/@paluchasz/understanding-openais-clip-model-6b52bade3fa3</li>
<li>InfoNCE Loss for Attribute Embedding - Emergent Mind, https://www.emergentmind.com/topics/infonce-loss-for-attribute-embedding</li>
<li>What Is Noise Contrastive Estimation Loss? A Tutorial With Code …, https://wandb.ai/self-supervised-learning/index/reports/What-Is-Noise-Contrastive-Estimation-Loss-A-Tutorial-With-Code–Vmlldzo2NzY2OTY2</li>
<li>Explanation of Contrastive Predictive Coding - Ruihong Qiu, https://ruihongqiu.github.io/posts/2020/08/infonce/</li>
<li>Decoupled Contrastive Learning | Research - AI at Meta, https://ai.meta.com/research/publications/decoupled-contrastive-learning/</li>
<li>Revealing the Dark Secrets of Masked Image Modeling - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Revealing_the_Dark_Secrets_of_Masked_Image_Modeling_CVPR_2023_paper.pdf</li>
<li>[2408.06687] Masked Image Modeling: A Survey - arXiv, https://arxiv.org/abs/2408.06687</li>
<li>arxiv.org, <a href="https://arxiv.org/html/2408.06687v1#:~:text=The%20MIM%20task%20involves%20masking,visible%20part%20of%20the%20input.">https://arxiv.org/html/2408.06687v1#:~:text=The%20MIM%20task%20involves%20masking,visible%20part%20of%20the%20input.</a></li>
<li>SimMIM: A Simple Framework for Masked Image Modeling - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Xie_SimMIM_A_Simple_Framework_for_Masked_Image_Modeling_CVPR_2022_paper.pdf</li>
<li>Masked Image Modeling: A Survey - arXiv, https://arxiv.org/html/2408.06687v1</li>
<li>[2111.06377] Masked Autoencoders Are Scalable Vision Learners - arXiv, https://arxiv.org/abs/2111.06377</li>
<li>Masked Autoencoder: Scalable Self-Supervised Vision Representation Learning via … - Medium, https://medium.com/@kdk199604/masked-autoencoder-scalable-self-supervised-vision-representation-learning-via-autoencoder-e9d96fd65ac2</li>
<li>DINOv2: Exploring Self-Supervised Vision Transformers - Marvik - Blog, https://blog.marvik.ai/2023/05/16/dinov2-exploring-self-supervised-vision-transformers/</li>
<li>in DINO, how does knowledge distillation such as teacher vs. student help learn the general visual features of the images? - DEV Community, https://dev.to/henri_wang_d48b1e9bc1ea79/in-dino-how-does-knowledge-distillation-such-as-teacher-vs-student-help-learn-the-general-visual-b9d</li>
<li>DINOv2: Self-supervised Learning Model Explained - Encord, https://encord.com/blog/dinov2-self-supervised-learning-explained/</li>
<li>OpenAI CLIP Model Explained: An Engineer’s Guide - Lightly, https://www.lightly.ai/blog/clip-openai</li>
<li>Contrastive Language-Image Pre-training - Wikipedia, https://en.wikipedia.org/wiki/Contrastive_Language-Image_Pre-training</li>
<li>CLIP Model Overview : Unlocking the Power of Multimodal AI | Towards Data Science, https://towardsdatascience.com/clip-model-overview-unlocking-the-power-of-multimodal-ai/</li>
<li>[D] Paper Explained - OpenAI CLIP: ConnectingText and Images (Full Video Analysis), https://www.reddit.com/r/MachineLearning/comments/kvtcpt/d_paper_explained_openai_clip_connectingtext_and/</li>
<li>CLIP by OpenAI Explained - Kaggle, https://www.kaggle.com/code/pragyanbo/clip-by-openai-explained</li>
<li>CLIP: Connecting text and images - OpenAI, https://openai.com/index/clip/</li>
<li>CLIP (Contrastive Language-Image Pretraining) - GeeksforGeeks, https://www.geeksforgeeks.org/deep-learning/clip-contrastive-language-image-pretraining/</li>
<li>DINOv3: A Deep, Practical Overview of Meta’s New Self-Supervised Vision Backbone, https://medium.com/@hexiangnan/dinov3-a-deep-practical-overview-of-metas-new-self-supervised-vision-backbone-005311689dbf</li>
<li>[R] Dino v3: Self-supervised learning for vision at unprecedented scale - Reddit, https://www.reddit.com/r/MachineLearning/comments/1ms9d2u/r_dino_v3_selfsupervised_learning_for_vision_at/</li>
<li>DINOv3: Self-supervised learning for vision at unprecedented scale - AI at Meta, https://ai.meta.com/blog/dinov3-self-supervised-vision-model/</li>
<li>Meta AI’s Segment Anything Model (SAM) Explained: The Ultimate Guide - Encord, https://encord.com/blog/segment-anything-model-explained/</li>
<li>Master Image Segmentation with SAM’s Zero-Shot AI - Viso Suite, https://viso.ai/deep-learning/segment-anything-model-sam-explained/</li>
<li>Segment Anything | Meta AI, https://segment-anything.com/</li>
<li>Introducing Segment Anything: Working toward the first foundation model for image segmentation - AI at Meta, https://ai.meta.com/blog/segment-anything-foundation-model-image-segmentation/</li>
<li>Meta Segment Anything Model 2, https://ai.meta.com/sam2/</li>
<li>Vision Foundation Models in Medical Image Analysis: Advances and Challenges - arXiv, https://arxiv.org/html/2502.14584v2</li>
<li>CVPR Tutorial Recent Advances in Vision Foundation Models, https://cvpr.thecvf.com/virtual/2023/tutorial/18558</li>
<li>How to Benchmark Vision Foundation Models for Semantic Segmentation? - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024W/2WFM/papers/Kerssies_How_to_Benchmark_Vision_Foundation_Models_for_Semantic_Segmentation_CVPRW_2024_paper.pdf</li>
<li>The New Vision 1: Open-Vocabulary Object Detection - Polarix, https://polarixdata.com/nl/blog/the-new-vision-open-vocabulary-object-detection/</li>
<li>Open-Vocabulary Object Detection upon Frozen Vision and Language Models, https://openreview.net/forum?id=MIMwy4kh9lf</li>
<li>Annotation Free Semantic Segmentation with Vision Foundation Models - arXiv, https://arxiv.org/html/2403.09307v1</li>
<li>Delving Into Shape-Aware Zero-Shot Semantic … - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Delving_Into_Shape-Aware_Zero-Shot_Semantic_Segmentation_CVPR_2023_paper.pdf</li>
<li>Visual Foundation Models for Medical Image Analysis | NVIDIA Technical Blog, https://developer.nvidia.com/blog/visual-foundation-models-for-medical-image-analysis/</li>
<li>Triad: Vision Foundation Model for 3D Magnetic Resonance Imaging - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC11952655/</li>
<li>Foundation models in ophthalmology: opportunities and challenges - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC11620320/</li>
<li>Artificial intelligence in medical imaging: From task-specific models to large-scale foundation models - MedNexus, https://mednexus.org/doi/10.1097/CM9.0000000000003489</li>
<li>Foundation Models in Radiology: What, How, Why, and Why Not - RSNA Journals, https://pubs.rsna.org/doi/10.1148/radiol.240597</li>
<li>Navigating the Landscape of AI: Generative AI, Foundation Models, and Traditional AI Explained - Aicadium, https://aicadium.ai/navigating-the-landscape-of-ai-generative-ai-foundation-models-and-traditional-ai-explained/</li>
<li>Uncovering Bias in Foundation Models: Impact, Testing, Harm, and Mitigation - arXiv, https://arxiv.org/html/2501.10453v1</li>
<li>Rethinking the Bias of Foundation Model under Long-tailed Distribution - arXiv, https://arxiv.org/html/2501.15955v3</li>
<li>Foundation Models Defining a New Era in Vision: A Survey and Outlook, https://www.computer.org/csdl/journal/tp/2025/04/10834497/23mYUeDuDja</li>
<li>An In-Depth Survey of Multimodal Foundation Models and Their Challenges - Sciety, https://sciety.org/articles/activity/10.31224/4752?utm_source=sciety_labs_article_page</li>
<li>[2502.13130] Magma: A Foundation Model for Multimodal AI Agents - arXiv, https://arxiv.org/abs/2502.13130</li>
<li>Comprehensive Survey of Model Compression and Speed up for Vision Transformers_Chen et al_ | PDF - Scribd, https://www.scribd.com/document/828426614/Comprehensive-Survey-of-Model-Compression-and-Speed-up-for-Vision-Transformers-Chen-et-al</li>
<li>Knowledge Distillation: Compressing Large Models into Efficient Learners - Lightly, https://www.lightly.ai/blog/knowledge-distillation</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>