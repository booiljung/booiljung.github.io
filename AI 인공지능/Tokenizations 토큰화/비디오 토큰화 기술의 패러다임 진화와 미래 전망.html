<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:비디오 토큰화 기술의 패러다임 진화와 미래 전망</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>비디오 토큰화 기술의 패러다임 진화와 미래 전망</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">토큰화 (Tokenizations)</a> / <span>비디오 토큰화 기술의 패러다임 진화와 미래 전망</span></nav>
                </div>
            </header>
            <article>
                <h1>비디오 토큰화 기술의 패러다임 진화와 미래 전망</h1>
<h2>1. 서론</h2>
<p>현대 사회는 비디오 데이터의 기하급수적인 증가라는 전례 없는 현상에 직면해 있다. 소셜 미디어, 스트리밍 서비스, 자율 주행, 보안 감시 등 다양한 영역에서 생성되는 방대한 양의 비디오는 인공지능(AI) 시스템이 시각 세계를 이해하고 상호작용하기 위한 필수적인 정보 자원이다. 그러나 비디오는 본질적으로 고차원의 시공간적(spatio-temporal) 데이터 구조를 가지며, 프레임 간 높은 중복성과 막대한 계산량을 수반한다. 이러한 특성은 비디오를 기계가 직접 처리하고 학습하는 데 있어 근본적인 장벽으로 작용한다. 특히, 자연어 처리 분야에서 혁명을 일으킨 거대 언어 모델(LLM)의 등장은 이러한 시각적 데이터를 언어적 맥락과 통합하여 고차원적인 추론을 수행하려는 시도를 가속화했으며, 이는 비디오 데이터의 효율적인 표현 방식에 대한 요구를 더욱 증폭시켰다.</p>
<p>이러한 배경 속에서 ‘비디오 토큰화(Video Tokenization)’ 기술이 핵심적인 해결책으로 부상하였다. 비디오 토큰화는 원본 비디오의 방대한 픽셀 데이터를 저차원의 이산적인(discrete) 토큰 시퀀스로 변환하는 과정을 의미한다.1 이는 단순히 데이터를 압축하는 것을 넘어, 비디오에 내재된 공간적, 시간적 중복성을 제거하고, 핵심적인 의미론적 정보(semantic information)를 응축하여 LLM과 같은 후속 모델이 효율적으로 처리할 수 있도록 하는 핵심적인 전처리 과정이다. 즉, 비디오라는 ’외국어’를 기계가 이해할 수 있는 ’단어’의 나열로 번역하는 것과 같다.3</p>
<p>본 안내서는 비디오 토큰화 기술이 걸어온 진화의 궤적을 체계적으로 분석하고 미래를 조망하는 것을 목표로 한다. 기술 발전의 역사는 단선적인 개선의 과정이 아니라, 특정 문제에 대한 해결책이 새로운 한계를 낳고, 그 한계를 극복하기 위한 혁신이 다시 등장하는 역동적인 패러다임 전환의 연속이었다. 본 안내서는 이러한 진화 과정을 크게 세 가지 핵심 축, 즉 <strong>효율성(Efficiency)</strong>, <strong>충실도(Fidelity)</strong>, 그리고 **의미론적 표현력(Semantic Representation)**을 중심으로 추적한다. 첫째, Variational Autoencoder(VAE)와 3D 컨볼루션 신경망(3D CNN)에 기반한 초기 패러다임의 정립 과정을 살펴보고 그 성과와 명확한 한계를 분석한다. 둘째, 계산 비용과 토큰 수 절감이라는 ’효율성’의 압박 속에서 등장한 1D 토큰화, 마스킹, 동적 토큰화와 같은 혁신적 기법들을 탐구한다. 셋째, 효율성을 위해 희생되었던 ’충실도’와 ’시간적 일관성’을 복원하기 위한 확산 모델, 참조 기반 인코딩, 시공간 정보 분리와 같은 최신 기술 동향을 심도 있게 다룬다. 나아가, 기술의 지향점이 단순한 픽셀 재구성을 넘어 진정한 ’이해’로 나아가면서 부상한 객체 중심 표현(Object-Centric Representation)이라는 근본적인 패러다임 전환을 조명한다. 마지막으로, 생성된 토큰이 LLM과 어떻게 융합되어 활용되는지를 Flamingo와 BEiT-3 같은 대표적인 멀티모달 아키텍처를 통해 분석하고, 장편 비디오 처리, 시간적 드리프트, 범용 토크나이저 개발 등 미래의 핵심 도전 과제와 연구 방향을 제시하며 마무리한다.</p>
<h2>2.  비디오 토큰화의 서막 - VAE 기반 패러다임의 정립</h2>
<p>비디오 토큰화 기술의 여명기는 고차원의 연속적인 픽셀 데이터를 어떻게 하면 관리 가능하고 효율적인 형태로 변환할 수 있는가라는 근본적인 질문에서 시작되었다. 이 시기의 지배적인 패러다임은 생성 모델, 특히 Variational Autoencoder(VAE) 아키텍처를 중심으로 구축되었으며, 이는 이후 모든 발전의 토대가 되는 개념적 틀을 제공했다.</p>
<h3>2.1  기본 원리: 픽셀에서 잠재 공간으로</h3>
<p>비디오를 픽셀 수준에서 직접 조작하는 것은 엄청난 계산 비용을 요구한다.1 예를 들어, 단 몇 초 분량의 저해상도 비디오조차도 수백만 개의 픽셀 값으로 구성되며, 이를 자기회귀 모델(autoregressive model)과 같은 생성 모델에 직접 입력하는 것은 현실적으로 불가능에 가깝다. 이 문제를 해결하기 위해 연구자들은 원본 비디오를 훨씬 낮은 차원의 조밀한 잠재 표현(latent representation)으로 인코딩하는 토큰화의 근본적인 아이디어를 채택했다.1</p>
<p>이 패러다임의 핵심적인 사상은 비디오의 시각적 정보가 실제로는 내재적(intrinsic)으로 훨씬 낮은 차원의 매니폴드(manifold)에 존재한다는 가정에 기반한다. 즉, 비디오의 수많은 픽셀 값들 사이에는 높은 상관관계와 중복성이 존재하며, 본질적인 정보는 이보다 훨씬 간결하게 표현될 수 있다는 것이다. VAE는 이러한 아이디어를 구현하는 강력한 도구였다. VAE는 인코더(Encoder)와 디코더(Decoder)로 구성된 신경망으로, 인코더는 입력 데이터(비디오)를 받아 잠재 공간의 확률 분포(일반적으로 평균과 분산)로 압축하고, 디코더는 이 잠재 공간에서 샘플링된 벡터를 사용하여 원본 데이터와 유사한 데이터를 재구성하도록 학습된다.</p>
<p>특히, 비디오 토큰화에서는 Vector Quantized VAE(VQ-VAE) 5 및 그 변형들이 중요한 역할을 했다. VQ-VAE는 연속적인 잠재 공간 대신, 이산적인 코드들로 구성된 학습 가능한 ’코드북(codebook)’을 사용한다. 인코더가 생성한 연속적인 잠재 벡터는 코드북에서 가장 유사한 벡터(코드)로 강제 매핑(양자화)된다. 이 이산적인 코드가 바로 ’토큰’이며, 비디오는 이러한 토큰들의 시퀀스로 표현된다. 이 방식은 두 가지 주요 장점을 가진다. 첫째, 잠재 공간을 이산화함으로써 LLM과 같은 자기회귀 모델이 자연어 토큰을 처리하는 방식과 매우 유사하게 비디오 토큰을 처리할 수 있는 길을 열었다. 둘째, 후방 전파 시 그래디언트가 코드북을 통과할 수 있도록 ’Straight-Through Estimator’와 같은 기법을 사용하여 엔드-투-엔드(end-to-end) 학습이 가능하다.</p>
<h3>2.2  표준 아키텍처: 3D CNN과 VQ-GAN</h3>
<p>초기 비디오 토크나이저의 표준 아키텍처는 크게 세 부분으로 구성되었다: 3D CNN 기반 인코더, 벡터 양자화 계층, 그리고 결정론적 디코더.</p>
<p><strong>인코더:</strong> 비디오는 이미지의 연속이므로 공간적 정보와 시간적 정보를 동시에 처리해야 한다. 이를 위해 초기 모델들은 3D 컨볼루션 신경망(3D CNN)을 인코더의 핵심 구성 요소로 채택했다.1 2D CNN이 이미지의 높이와 너비 차원에서 특징을 추출하는 것과 달리, 3D CNN은 높이, 너비, 그리고 시간(프레임) 차원에서 3차원 필터를 적용하여 움직임과 시공간적 패턴을 직접적으로 포착할 수 있다. 인코더는 여러 층의 3D CNN과 다운샘플링(downsampling) 연산을 통해 입력 비디오를 점진적으로 압축하여 저차원의 시공간적 특징 맵을 생성한다.</p>
<p><strong>양자화:</strong> 인코더가 출력한 연속적인 특징 맵은 양자화 계층으로 전달된다. VQ-VAE의 원리에 따라, 특징 맵의 각 벡터는 학습된 코드북에서 유클리드 거리 기준으로 가장 가까운 코드 벡터로 대체된다.6 이 과정을 통해 비디오는 코드북의 인덱스, 즉 이산적인 토큰들의 그리드(grid)로 변환된다.</p>
<p><strong>디코더와 VQ-GAN:</strong> 양자화된 토큰 그리드는 디코더에 입력되어 원본 비디오를 재구성한다. 초기 디코더는 인코더와 대칭적인 구조로, 업샘플링(upsampling)과 3D CNN을 통해 토큰을 다시 픽셀 공간으로 복원했다. 그러나 VAE 기반 모델은 재구성 손실(Reconstruction Loss)과 잠재 분포를 정규화하는 KL 발산(KL Divergence) 항 사이의 균형을 맞춰야 하는 고질적인 문제를 안고 있었다.7 재구성 손실에만 집중하면 잠재 공간의 구조가 무너지고, KL 발산에 집중하면 재구성된 이미지가 흐릿해지는 경향이 있었다.</p>
<p>이 문제를 완화하고 재구성 품질, 특히 고주파 디테일을 향상시키기 위해 많은 모델들이 VQ-GAN 4의 아이디어를 차용했다. VQ-GAN은 디코더의 출력에 판별자(Discriminator)를 추가하여 적대적 학습(Adversarial Training)을 수행한다. 판별자는 디코더가 생성한 프레임과 실제 프레임을 구별하도록 학습하고, 디코더는 판별자를 속일 수 있을 만큼 사실적인 프레임을 생성하도록 학습한다. MAGVIT과 같은 대표적인 초기 비디오 토크나이저들은 이러한 VQ-GAN 구조를 비디오에 적용하여 재구성 충실도를 크게 높였다.8</p>
<h3>2.3  초기 패러다임의 성과와 한계</h3>
<p>VAE 기반의 초기 패러다임은 비디오를 이산적인 토큰 시퀀스로 변환함으로써, 비디오 생성 및 이해 작업을 LLM과 유사한 자기회귀적 시퀀스 모델링의 틀 안으로 가져왔다는 점에서 중요한 기술적 이정표를 세웠다.10 이는 비디오 분야에 대규모 생성 모델을 적용할 수 있는 가능성을 연 최초의 성공적인 시도였다. 이 접근법은 근본적으로 비디오를 압축해야 할 3차원 신호로 간주하는 ‘신호 처리’ 관점에 기반했다. 핵심 구성 요소인 3D CNN과 VAE는 구조화된 그리드 데이터로부터 특징을 추출하고 차원을 축소하는 데 최적화된 도구였으며, 평가 지표 역시 PSNR, SSIM과 같은 픽셀 수준의 충실도에 집중되었다. ’토큰’의 개념은 추상적인 개념이 아닌, 시공간적 픽셀 블록이라는 물리적 단위에 묶여 있었다.</p>
<p>그러나 이러한 관점과 아키텍처는 명확한 한계를 내포하고 있었다.</p>
<p>첫째, <strong>엄청난 계산적 부담</strong>이 가장 큰 문제였다. 3D CNN은 2D CNN에 비해 파라미터 수와 연산량이 기하급수적으로 많아 막대한 메모리와 계산 자원을 소모했다.1 이로 인해 긴 비디오를 처리하거나 모델의 크기를 키우는 데 심각한 제약이 따랐다.</p>
<p>둘째, <strong>정보 손실과 품질 저하</strong> 문제가 있었다. 고정된 패치 그리드 기반의 양자화 방식은 객체의 경계나 의미론적 단위를 고려하지 않고 비디오를 기계적으로 분할한다. 이 과정에서 미세한 텍스처, 복잡한 움직임, 또는 작은 객체와 같은 중요한 정보가 손실되기 쉬웠다. GAN 기반 학습이 이를 일부 보완했지만, 여전히 시각적 아티팩트(artifact)를 유발하거나 특정 텍스처를 과도하게 생성하는 등의 문제가 발생했다.5</p>
<p>셋째, <strong>학습의 불안정성</strong>이 있었다. GAN을 포함한 적대적 학습은 본질적으로 두 네트워크 간의 제로섬 게임(zero-sum game)으로, 안정적인 균형점을 찾기 어렵다. 하이퍼파라미터에 매우 민감하며, 학습이 불안정해지거나 모델이 특정 종류의 출력만 생성하는 모드 붕괴(mode collapse) 현상이 발생하기 쉬웠다.7</p>
<p>이러한 한계들은 비디오 토큰화 기술이 다음 단계로 나아가기 위해 반드시 해결해야 할 과제들을 명확히 제시했다. 계산 효율성을 높이고, 정보 손실을 최소화하며, 학습 과정을 안정화해야 한다는 요구는 자연스럽게 다음 세대 기술들의 연구 방향을 결정지었다.</p>
<h2>3.  효율성과 의미론적 압축을 향한 탐색</h2>
<p>1세대 VAE 기반 토크나이저가 남긴 ’높은 계산 비용’과 ’의미론적 정보 부족’이라는 과제는 기술 발전의 방향을 ’효율성’으로 이끌었다. 이 시기의 연구들은 단순히 원본을 잘 복원하는 것을 넘어, 얼마나 더 적은 토큰으로, 얼마나 더 의미 있는 정보를 담아낼 수 있는가에 집중했다. 이는 “가능한 한 무손실” 압축에서 “영리하게 손실을 감수하는” 압축으로의 철학적 전환을 의미했다. 이 패러다임의 기저에는 비디오 데이터의 모든 부분이 동등하게 중요하지 않으며, 중복되거나 덜 중요한 정보를 능동적으로 폐기함으로써 계산 효율성을 극대화할 수 있다는 인식이 자리 잡고 있었다. 이러한 변화는 특히 토큰 시퀀스의 길이에 이차적으로(<span class="math math-inline">O(N^2)</span>) 계산량이 증가하는 트랜스포머 기반 LLM이라는 주된 ’소비자’의 제약을 현실적으로 고려한 실용적인 대응이었다.</p>
<h3>3.1  토큰 차원 축소: 1D 토큰화의 등장 (TiTok)</h3>
<p>초기 토크나이저들은 비디오를 3차원(높이, 너비, 시간) 그리드로 처리했지만, 이미지조차도 2차원 그리드 구조가 인접 픽셀 간의 높은 공간적 중복성을 유발한다는 문제의식이 제기되었다.4 이 문제를 해결하기 위해 등장한 TiTok(Transformer-based 1-Dimensional Tokenizer)은 토큰화의 차원을 근본적으로 재고했다.4</p>
<p>TiTok은 이미지를 2D 잠재 그리드가 아닌, 1D 잠재 시퀀스로 직접 토큰화하는 혁신적인 접근법을 제안했다. 이 모델은 트랜스포머 아키텍처를 사용하여 이미지 패치 시퀀스를 입력받아, 훨씬 더 적은 수의 1D 잠재 토큰 시퀀스로 압축한다. 예를 들어, 256x256 크기의 이미지를 VQGAN 기반 모델이 256개 또는 1024개의 2D 토큰으로 변환하는 반면, TiTok은 단 32개의 1D 토큰으로도 합리적인 수준의 재구성이 가능함을 보였다.4</p>
<p>이러한 1D 토큰화는 두 가지 중요한 이점을 가져왔다. 첫째, <strong>표현의 극단적인 간결성</strong>이다. 토큰의 수를 획기적으로 줄임으로써 후속 모델의 계산 부담을 크게 경감시켰다. 둘째, <strong>유연한 의미론적 표현 학습</strong>이다. 2D 토큰이 특정 이미지 패치 위치에 강하게 묶여 있는 반면, 1D 토큰은 고정된 그리드 제약에서 벗어나 이미지 전체의 정보를 보다 유연하고 종합적으로 포착할 수 있다. 각 토큰은 특정 위치가 아닌, 더 높은 수준의 추상적이고 의미론적인 정보를 학습하게 되어 표현의 질을 높였다.4 이는 비디오 토큰화에도 적용될 수 있는 중요한 개념적 진보였다.</p>
<h3>3.2  토큰 희소화: 정보량에 따른 선택과 집중 (Masked Autoencoding)</h3>
<p>비디오의 모든 프레임, 모든 패치가 동등하게 중요하지 않다는 아이디어는 ’토큰 희소화(token sparsification)’라는 또 다른 효율성 향상 전략으로 이어졌다. 이 분야의 선구적인 연구는 Masked Autoencoder(MAE) 계열의 접근법으로, 입력의 상당 부분을 마스킹하고 보이는 부분만으로 마스킹된 부분을 예측하도록 모델을 학습시키는 방식이다.</p>
<p>VideoMAEv2는 이 아이디어를 비디오에 성공적으로 적용한 대표적인 모델이다.8 이 모델은 인코더에 입력되는 토큰의 일부(예: 80-90%)를 마스킹할 뿐만 아니라, 디코더가 재구성해야 할 목표 토큰의 일부도 선택적으로 마스킹하는 ‘이중 마스킹(Dual Masking)’ 전략을 사용했다. 즉, 모델은 전체 토큰 중 극히 일부(예: 15%)만 보고, 그중에서도 일부만을 재구성하는 훨씬 더 어려운 과제를 수행하도록 훈련된다.8</p>
<p>이러한 극단적인 희소화 전략은 놀랍게도 모델이 비디오의 핵심적인 시공간적 구조를 학습하는 데 매우 효과적이라는 것이 밝혀졌다. 더 적은 계산량으로 모델을 훈련시킬 수 있게 되었을 뿐만 아니라, 이전 모델들이 메모리 제약으로 다루기 어려웠던 긴 비디오 시퀀스(예: 16프레임에서 128프레임으로 확장)에 대한 사전 학습을 가능하게 했다. 긴 맥락을 학습한 모델은 다양한 다운스트림 작업에서 훨씬 뛰어난 성능을 보였다.8 이는 비디오 표현이 외형적 디테일의 총합이 아니라, 핵심적인 시공간적 관계를 통해 구성될 수 있음을 시사하며, 정보의 ’선택과 집중’이 효율성과 성능을 동시에 잡을 수 있는 열쇠임을 보여주었다.</p>
<h3>3.3  동적 토큰화: 내용에 따른 유연한 압축</h3>
<p>고정된 수의 토큰을 사용하거나 고정된 비율로 마스킹하는 전략은 비디오 내용의 동적인 변화를 고려하지 못한다는 한계가 있다. 정적인 배경만 있는 장면과 복잡한 객체들이 빠르게 움직이는 장면은 필요한 정보량이 명백히 다르다. 이러한 문제의식에서 출발한 것이 ’동적 토큰화(dynamic tokenization)’이다.</p>
<p><strong>ElasticTok</strong>은 비디오의 복잡도에 따라 프레임당 토큰 수를 유연하게 할당하는 적응형 토큰화 개념을 제시했다.11 이 모델은 학습 시 각 프레임의 토큰 인코딩 끝부분을 무작위 개수만큼 마스킹하여, 가변 길이의 토큰으로도 원본을 재구성하는 능력을 학습한다. 추론 시에는 재구성 오류가 특정 임계값 이하로 유지되는 한도 내에서 최대한 많은 토큰을 제거한다. 그 결과, 복잡하고 예측하기 어려운 장면에서는 더 많은 토큰을 할당하고, 단순하고 예측 가능한 장면에서는 더 적은 토큰을 할당하여 전체적인 토큰 수를 2배에서 5배까지 줄일 수 있었다.11</p>
<p><strong>Token Dynamics</strong>는 여기서 한 걸음 더 나아가 비디오 표현 자체를 근본적으로 재구성하는 혁신적인 프레임워크를 제안했다.2 이 모델은 비디오 정보를 두 가지 요소로 명시적으로 분리(disentangle)한다.</p>
<ol>
<li><strong>토큰 해시 테이블 (Token Hash Table):</strong> 비디오에 등장하는 고유한 시각적 패턴이나 객체의 부분들을 나타내는 토큰들의 집합이다. 이는 비디오의 ’무엇(what)’에 해당하는 정적인 외형 정보를 담고 있으며, 유사한 토큰들을 클러스터링하여 매우 간결하게 구성된다.2</li>
<li><strong>토큰 인덱스 키 맵 (Token Indices Key Map):</strong> 각 시공간적 위치에 어떤 ’해시 테이블’의 토큰이 나타나는지를 가리키는 인덱스 맵이다. 이는 객체들의 움직임, 즉 ’어떻게(how)’에 해당하는 동적인 시공간 정보를 담는다.2</li>
</ol>
<p>이러한 분리 구조는 엄청난 압축 효율을 가져온다. 외형 정보는 해시 테이블에 한 번만 저장되고, 키 맵은 이들의 동적인 배치 정보만을 기록하기 때문이다. Token Dynamics는 이 방식을 통해 원본 비디오 토큰 수의 단 0.07%만으로도 비디오를 표현하면서, 다운스트림 작업에서의 성능 저하를 최소화할 수 있음을 입증했다.2 이는 LLM의 제한된 컨텍스트 창이라는 현실적인 제약에 대한 가장 직접적이고 강력한 해결책 중 하나로, 효율성 중심의 패러다임이 도달한 정점을 보여준다. 이처럼 2세대 기술들은 비디오를 정보의 이질적인 집합체로 보고, 각 정보의 특성에 맞는 차별적인 압축 전략을 적용함으로써 토큰화 기술을 한 단계 성숙시켰다.</p>
<h2>4.  최신 기술 동향과 패러다임 전환</h2>
<p>효율성을 향한 2세대의 탐색은 필연적으로 새로운 질문을 낳았다. 극단적인 압축은 과연 재구성 충실도와 시간적 일관성을 해치지 않는가? 이 질문에 대한 해답을 찾는 과정에서 비디오 토큰화 기술은 다시 한번 패러다임 전환을 맞이했다. 최신 기술 동향은 단순히 더 작게 만드는 것을 넘어, ’어떻게 하면 손실된 품질을 복원하고, 비디오의 본질적인 특성을 더 잘 보존할 것인가’에 집중한다. 이는 이전 세대의 경험적이고 휴리스틱한 해결책에서 벗어나, 보다 근본적이고 원칙에 기반한(principled) 아키텍처를 추구하는 성숙한 단계로의 진입을 의미한다. GAN 학습의 불안정성을 안정적인 확산 모델로 대체하고, 모든 프레임을 동등하게 취급하던 가정에서 벗어나 참조-델타 메커니즘을 도입하며, 공간과 시간을 균일한 데이터로 보던 관점을 깨고 이들의 이질성을 인정하는 접근법들이 바로 그 증거다.</p>
<h3>4.1  확산 모델의 부상: 재구성 충실도의 새로운 기준 (CDT)</h3>
<p>2세대 모델들이 효율성을 추구하는 동안, 1세대 VQ-GAN 기반 모델들이 가졌던 재구성 품질의 한계와 학습 불안정성 문제는 여전히 중요한 과제로 남아있었다.7 특히 GAN 기반 디코더는 학습 과정이 매우 불안정하고, 하이퍼파라미터 튜닝이 어려우며, 특정 데이터 패턴만 생성하는 모드 붕괴(mode collapse) 현상에 취약했다.</p>
<p>이러한 문제에 대한 강력한 대안으로 ’확산 모델(Diffusion Model)’이 부상했다. 확산 모델은 데이터에 점진적으로 노이즈를 추가하는 순방향 프로세스와, 노이즈로부터 원본 데이터를 점진적으로 복원하는 역방향 프로세스를 학습한다. 이 과정은 GAN에 비해 훨씬 안정적이며, 고품질의 생성 결과를 만들어내는 것으로 잘 알려져 있다.</p>
<p>CDT(Conditioned Diffusion Tokenizer)는 이러한 확산 모델의 장점을 비디오 토크나이저의 디코더에 성공적으로 통합한 대표적인 모델이다.1 CDT의 아키텍처는 VAE 인코더와 조건부 확산 디코더로 구성된다.</p>
<ul>
<li><strong>인코더:</strong> 기존과 유사하게 3D CNN 등을 사용하여 입력 비디오를 조밀한 잠재 표현으로 압축한다.</li>
<li><strong>조건부 확산 디코더:</strong> 이 모델의 핵심 혁신으로, 순수한 노이즈에서 시작하여 인코더가 생성한 잠재 표현을 ’조건(condition)’으로 삼아 여러 단계의 노이즈 제거 과정을 거쳐 원본 비디오를 재구성한다.</li>
</ul>
<p>이 방식은 복잡한 적대적 학습 없이, 단순한 평균 제곱 오차(MSE) 기반의 확산 손실과 LPIPS와 같은 지각적 손실(perceptual loss)만으로도 학습이 가능하다.7 그 결과, 학습 과정이 매우 안정적이면서도 기존 GAN 기반 모델들을 능가하는 높은 재구성 충실도를 달성했다. 더욱이, DDIM(Denoising Diffusion Implicit Model)과 같은 샘플링 가속 기법을 활용하면, 단일 스텝 샘플링만으로도 SOTA 수준의 재구성 품질을 보여주어 효율성과 품질이라는 두 마리 토끼를 모두 잡는 데 성공했다.1 CDT의 등장은 비디오 토큰화에서 고품질 재구성을 위한 새로운 표준을 제시했다.</p>
<h3>4.2  시간적 일관성의 재정의: 참조 기반 토큰화 (RefTok)</h3>
<p>비디오의 가장 본질적인 특성 중 하나는 프레임 간의 높은 시간적 중복성과 연속성이다. 그러나 대부분의 기존 토크나이저들은 비디오를 일정한 길이의 클립(chunk)으로 나누고 각 클립을 독립적으로 처리함으로써, 클립과 클립 사이의 연속성이 깨지는 문제를 안고 있었다.5 이로 인해 양자화 과정에서 손실된 미세한 디테일(예: 사람의 얼굴, 텍스트, 옷의 패턴)이 프레임마다 미묘하게 변형되어 깜빡거리거나(flickering) 왜곡되는 현상이 발생했다.</p>
<p>RefTok(Reference-Based Tokenization)은 이러한 문제를 해결하기 위해 고전적인 비디오 코덱의 아이디어에서 영감을 얻은 혁신적인 개념, 즉 ’참조 프레임(reference frame)’을 도입했다.5 RefTok의 핵심 아이디어는 다음과 같다.</p>
<ol>
<li><strong>참조 프레임 지정:</strong> 비디오 시퀀스 내의 특정 프레임(예: 첫 프레임)을 ’참조 프레임’으로 지정한다. 이 참조 프레임은 양자화 과정을 거치지 않고, 그 정보가 거의 손실 없이 보존된다.14</li>
<li><strong>조건부 인코딩 및 디코딩:</strong> 나머지 ’대상 프레임(target frames)’들은 독립적으로 인코딩되는 것이 아니라, 이 고품질의 참조 프레임을 조건으로 하여 인코딩 및 디코딩된다. 즉, 모델은 대상 프레임의 전체 정보를 학습하는 대신, 참조 프레임과의 차이(delta) 또는 변화량에 해당하는 정보만을 효율적으로 학습하고 압축한다. RefTok은 이를 위해 인코더가 생성한 참조 프레임의 특징 표현을 양자화 병목 계층을 우회하여 디코더에 직접 전달한다.5</li>
</ol>
<p>이러한 참조 기반 방식은 막대한 이점을 가져왔다. 디코더가 대상 프레임을 재구성할 때 항상 선명한 ’원본’인 참조 프레임에 접근할 수 있으므로, 시간이 지나도 객체의 정체성, 얼굴 디테일, 텍스트의 가독성, 미세한 패턴 등이 비디오 전체에 걸쳐 매우 일관되게 유지된다.15 RefTok은 다양한 데이터셋에서 기존 SOTA 토크나이저들보다 재구성 품질 지표(PSNR, SSIM, LPIPS)를 평균 36.7%나 향상시키면서도 동일하거나 더 높은 압축률을 달성했음을 보였다.5 이는 비디오의 시간적 구조에 대한 깊은 이해를 바탕으로 토큰화의 패러다임을 한 단계 진전시킨 중요한 성과다.</p>
<h3>4.3  시공간 정보의 분리: 분리된 쿼리 오토인코더 (SweetTok)</h3>
<p>비디오는 공간적 정보(프레임 내의 외형, appearance)와 시간적 정보(프레임 간의 움직임, motion)라는 이질적인 두 종류의 정보로 구성된다. 공간 정보는 프레임 내에서 높은 중복성을 보이고, 시간 정보는 프레임 간에 높은 중복성을 보인다. 그러나 대부분의 토크나이저들은 이 둘을 구분하지 않고 균일한 3D 시공간 패치로 처리해왔다.</p>
<p>SweetTok(Semantic-aWarE spatial-tEmporal Tokenizer)은 이러한 관행에 의문을 제기하며, 공간 정보와 시간 정보를 명시적으로 분리하여 처리하는 것이 더 효율적이고 효과적이라는 아이디어를 제시했다.9 SweetTok의 핵심은 두 가지 혁신적인 구성 요소에 있다.</p>
<ol>
<li><strong>DQAE (Decoupled Query AutoEncoder):</strong> 이 아키텍처는 비디오 입력을 받아 ’공간적 쿼리(spatial query)’와 ’시간적 쿼리(temporal query)’라는 두 종류의 학습 가능한 쿼리를 사용하여 정보를 분리 압축한다. 공간적 쿼리는 비디오의 정적인 외형 정보를 포착하고, 시간적 쿼리는 동적인 움직임 정보를 포착하도록 특화된다.17 이렇게 분리된 쿼리들은 각각의 특성에 맞는 별도의 코드북을 통해 양자화된다.</li>
<li><strong>MLC (Motion-enhanced Language Codebook):</strong> SweetTok은 한 걸음 더 나아가, 코드북 자체도 정보의 특성에 맞게 설계했다. 외형과 움직임은 서로 다른 의미론적 특성을 가지므로, 이를 반영하여 움직임 정보에 더 특화된 의미론적 표현을 담은 코드북(MLC)을 사용한다.</li>
</ol>
<p>이러한 시공간 정보의 분리 접근법은 상당한 이점을 제공했다. 각 정보의 고유한 중복성 패턴에 맞춰 최적화된 압축을 수행함으로써, 높은 압축률을 달성하면서도 뛰어난 재구성 충실도를 유지하는 ’스윗 스팟(sweet spot)’을 찾을 수 있었다.18 SweetTok은 UCF-101 데이터셋에서 재구성 충실도(rFVD)를 42.8%나 개선했으며, 생성 작업 성능(gFVD)도 15.1% 향상시켰다.17 또한, 분리된 토큰들은 각각 외형과 움직임이라는 명확한 의미론적 정보를 담고 있어, LLM을 활용한 소수샷(few-shot) 인식과 같은 다운스트림 작업에서도 강력한 성능을 발휘했다.</p>
<h3>4.4  새로운 시퀀스 모델링: Mamba의 활용</h3>
<p>토크나이저 아키텍처의 또 다른 중요한 발전은 트랜스포머의 대안을 모색하는 과정에서 나타났다. 트랜스포머의 셀프 어텐션 메커니즘은 시퀀스 길이에 대해 이차적인 계산 복잡도(<span class="math math-inline">O(N^2)</span>)를 가져, 긴 비디오 시퀀스를 처리하는 데 근본적인 병목으로 작용했다.</p>
<p>이 문제에 대한 유력한 해결책으로 상태 공간 모델(State Space Model, SSM), 특히 Mamba가 부상했다.19 Mamba는 입력에 따라 동적으로 변하는 파라미터를 사용하는 선택적 SSM 구조를 통해, 시퀀스 길이에 대해 선형적인 계산 복잡도(<span class="math math-inline">O(N)</span>)를 가지면서도 장거리 의존성을 효과적으로 모델링할 수 있다.</p>
<p>STORM(Spatio-Temporal Mamba)과 같은 모델들은 토크나이저의 시간적 인코더에 Mamba를 적용하여 그 효율성과 성능을 입증했다.19 Mamba 기반 시간 인코더는 전체 비디오 시퀀스에 걸쳐 프레임 간의 동역학을 효율적으로 포착하고, 이 정보를 이미지 토큰에 통합하여 풍부한 시공간 표현을 생성한다. 이 방식은 긴 비디오 이해 능력을 크게 향상시키는 동시에, 계산 비용을 최대 8배까지 줄이고 추론 지연 시간을 2.4배에서 2.9배까지 단축시키는 결과를 가져왔다.19 NVIDIA에서 제안한 Mamba 기반 토크나이저 역시 기존 3D CNN 기반 접근법을 능가하는 SOTA 성능을 달성하며, Mamba가 차세대 비디오 토크나이저의 핵심 아키텍처가 될 잠재력을 보여주었다.6</p>
<p><strong>표 1: 비디오 토큰화 기술 패러다임별 주요 모델 및 특징 비교</strong></p>
<table><thead><tr><th>패러다임 (Paradigm)</th><th>주요 모델 (Key Models)</th><th>핵심 아이디어 (Core Idea)</th><th>장점 (Strengths)</th><th>한계 (Limitations)</th></tr></thead><tbody>
<tr><td><strong>1세대: VAE 기반</strong></td><td>MAGVIT 8, VQGAN 4</td><td>3D CNN 인코더와 GAN 기반 디코더를 결합한 VQ-VAE 구조</td><td>비디오를 이산 토큰으로 변환하는 선구적 접근법, 자기회귀 생성의 기반 마련</td><td>높은 계산 비용, 학습 불안정성, 시각적 아티팩트, 의미론적 정보 부족</td></tr>
<tr><td><strong>2세대: 효율성 중심</strong></td><td>TiTok 4, VideoMAEv2 8, ElasticTok 11, Token Dynamics 2</td><td>1D 토큰화, 마스크드 모델링, 적응형/동적 토큰 할당, 정보 분리</td><td>계산 효율성 극대화, 긴 비디오 학습 가능, LLM 친화적인 짧은 토큰 시퀀스</td><td>공격적인 압축으로 인한 정보 손실, 시간적 일관성 저하 가능성</td></tr>
<tr><td><strong>3세대: 충실도/일관성 중심</strong></td><td>CDT 1, RefTok 5, SweetTok 17, Mamba-based 19</td><td>확산 모델 디코더, 참조 프레임 기반 인코딩, 시공간 정보 분리, SSM</td><td>높은 재구성 품질, 안정적인 학습, 뛰어난 시간적 일관성, 효율적인 장거리 모델링</td><td>아키텍처 복잡도 증가, 특정 가정(예: 참조 프레임 가용성)에 의존</td></tr>
</tbody></table>
<h2>5.  객체 중심 표현과 진정한 의미 이해로의 도약</h2>
<p>지금까지 논의된 토큰화 기술들은 비디오를 효율적으로 압축하고 충실하게 재구성하는 데 큰 발전을 이루었다. 그러나 이들 대부분은 근본적으로 동일한 패러다임, 즉 비디오를 고정된 크기의 시공간적 패치(patch) 그리드로 분할하는 방식에 기반하고 있다. 이 패치 기반 표현은 계산적으로는 편리하지만, 인간이 세상을 인식하는 방식과는 근본적인 괴리가 있다. 우리는 세상을 픽셀의 격자로 보지 않고, 의미 있는 ’객체(object)’들의 집합과 그들의 상호작용으로 이해한다. 이러한 인식론적 간극을 메우고 기계가 진정한 의미 이해에 도달하게 하기 위한 노력이 바로 ’객체 중심 표현(Object-Centric Representation)’이라는 새로운 패러다임의 등장을 이끌었다. 이는 단순한 표현 방식의 개선을 넘어, 비디오 이해의 목표를 ’재구성(representation)’에서 ’추론(reasoning)’으로 전환시키는 중대한 도약이다.</p>
<h3>5.1  패치 기반 표현의 근본적 한계</h3>
<p>패치 기반 토큰화의 한계는 명확하다. 첫째, <strong>의미론적 경계의 무시</strong>다. 고정된 그리드는 객체의 실제 경계와 무관하게 비디오를 자른다. 이로 인해 하나의 객체가 여러 패치에 걸쳐 쪼개지거나, 하나의 패치 안에 여러 객체와 배경이 뒤섞이게 된다. 이는 모델이 ’객체’라는 일관된 단위를 학습하는 것을 방해한다.20</p>
<p>둘째, <strong>구성적 일반화(compositional generalization)의 부재</strong>다. 패치 기반 모델은 특정 장면의 픽셀 패턴을 암기하는 경향이 있다. 예를 들어, ‘컵이 테이블 위에 있는’ 장면을 학습했지만, ‘컵이 의자 위에 있는’ 새로운 장면을 보면 제대로 인식하지 못할 수 있다. 이는 ‘컵’, ‘테이블’, ’의자’라는 객체와 ’위에 있다’는 관계를 분리하여 학습하지 못하기 때문이다.21</p>
<p>셋째, <strong>배경과 전경의 혼동</strong>이다. 모델은 중요한 상호작용이 일어나는 전경 객체와 상대적으로 덜 중요한 배경을 구분하지 못하고 모든 픽셀에 동등한 주의를 기울이게 되어 비효율적인 학습을 초래한다. 이러한 한계들은 패치 기반 표현이 복잡한 상호작용을 이해하거나, 인과 관계를 추론하거나, 새로운 상황에 유연하게 대처하는 고차원적인 인지 과제를 수행하는 데 근본적인 장벽이 됨을 시사한다.</p>
<h3>5.2  객체 중심 학습(Object-Centric Learning)의 부상</h3>
<p>객체 중심 학습은 이러한 한계를 극복하기 위해 비디오를 픽셀 그리드가 아닌, 상호작용하는 분리된 객체들의 집합으로 표현하려는 패러다임 전환을 시도한다.20 이 접근법의 핵심 목표는 비지도적(unsupervised) 또는 약지도적(weakly-supervised) 방식으로 장면을 구성 요소(객체)로 자동 분해하고, 각 요소에 대한 독립적인 표현을 학습하는 것이다.</p>
<p>이 분야의 핵심적인 메커니즘 중 하나는 <strong>Slot Attention</strong>이다. 이 메커니즘은 학습 가능한 ’슬롯(slot)’이라는 벡터 집합을 사용한다. 각 슬롯은 장면에 있는 특정 객체나 개체 하나를 ’담당’하기 위해 경쟁한다. 어텐션 메커니즘을 통해 이미지 특징들이 각 슬롯에 할당되고, 각 슬롯은 자신에게 할당된 특징들을 바탕으로 해당 객체의 모양, 위치, 색상 등의 정보를 담은 표현으로 업데이트된다. 이 과정을 통해 전체 장면은 여러 개의 객체별 슬롯 표현으로 자연스럽게 분해된다.22</p>
<p>이러한 아이디어는 비디오로 확장되어, Slot-VPS와 같은 모델에서 구체화되었다.22 비디오 파놉틱 분할(Video Panoptic Segmentation)은 비디오 내의 모든 픽셀을 특정 객체 인스턴스(‘things’, 예: 자동차, 사람) 또는 배경 영역(‘stuff’, 예: 하늘, 도로)에 할당하는 복잡한 작업이다. 기존 방식들은 객체 탐지, 분할, 추적 등을 별도의 네트워크로 처리하고 후처리로 결합하는 복잡한 파이프라인을 가졌다. 반면, Slot-VPS는 객체 중심 학습을 적용하여 비디오 내의 모든 ‘파놉틱 객체’(things와 stuff 모두)를 ’파놉틱 슬롯’이라는 통일된 표현으로 나타낸다. 이 슬롯들은 비디오 전체를 보며 스스로를 업데이트하여 객체의 위치, 모양, 그리고 시간적 연관성을 한 번에 학습한다. 그 결과, 복잡한 후처리 없이도 엔드-투-엔드 방식으로 더 정확하고 효율적인 파놉틱 분할을 달성할 수 있었다.22</p>
<h3>5.3  사전 훈련된 모델의 활용: 객체 프롬프트</h3>
<p>객체 중심 표현을 학습하는 또 다른 강력한 접근법은 처음부터 모든 것을 학습하는 대신, 이미 방대한 시각-언어 데이터로 사전 훈련된 거대 모델(Foundation Model)의 지식을 활용하는 것이다. CLIP, GLIP과 같은 모델들은 이미지와 텍스트 사이의 풍부한 의미론적 연결을 이미 학습했기 때문에, 특정 객체에 대한 강력한 사전 지식을 가지고 있다.21</p>
<p>‘객체 프롬프트(Object Prompts)’ 기법은 이러한 사전 지식을 효율적으로 활용하는 방법론이다.21 이 기법의 핵심은 모델을 파인튜닝하는 대신, 텍스트 프롬프트를 통해 모델의 ’주의’를 원하는 방향으로 유도하는 것이다. 예를 들어, 요리 비디오에서 미래 행동을 예측하는 작업을 수행한다고 가정해보자. 이 작업에서는 ‘칼’, ‘도마’, ’양파’와 같은 객체들이 중요하다. 객체 프롬프트 기법은 다음과 같이 작동한다.</p>
<ol>
<li><strong>객체 어휘 정의:</strong> 해당 작업 도메인에서 중요한 객체들의 목록(어휘)을 정의한다. 이는 데이터셋의 레이블을 분석하거나, 단어 임베딩을 클러스터링하여 만들 수 있다.</li>
<li><strong>모델에 프롬프트 제공:</strong> “칼”, “도마”, “양파“와 같은 텍스트 프롬프트를 사전 훈련된 시각-언어 모델(예: GLIP)에 입력으로 제공한다.</li>
<li><strong>객체 중심 표현 추출:</strong> 모델은 이 프롬프트를 바탕으로 비디오 프레임 내에서 해당 텍스트에 해당하는 객체들의 위치를 찾고(grounding), 그 객체 영역에 해당하는 시각적 특징(visual descriptor)을 추출한다.</li>
</ol>
<p>이 방식은 별도의 레이블링이나 파인튜닝 없이도, 일반적인 모델로부터 특정 작업에 매우 유용한 객체 중심 표현을 ‘뽑아낼’ 수 있게 해준다. 실제로 이 기법을 적용했을 때, 장기 행동 예측과 같은 다운스트림 작업의 성능이 크게 향상되는 것이 확인되었다.21</p>
<h3>5.4  3D 장면 표현과의 연결</h3>
<p>객체 중심 표현의 잠재력은 2D 비디오를 넘어 3D 공간으로 확장될 때 더욱 분명해진다. 인간은 2D 이미지 시퀀스를 보면서도 그 기저에 있는 3D 세계를 자연스럽게 추론한다. 객체 중심 학습은 기계에게도 이러한 능력을 부여하는 열쇠가 될 수 있다.</p>
<p>DORSal과 같은 연구는 이 방향성을 보여주는 좋은 예다.23 이 모델은 OSRT(Object Scene Representation Transformer)라는 모델을 사용하여 여러 시점에서 촬영된 이미지를 입력받아, 장면을 구성하는 객체 단위의 슬롯 표현으로 인코딩한다. 이렇게 학습된 객체 슬롯들은 3D 공간에서의 객체의 위치, 모양, 외형 정보를 담고 있다. 이후, 확산 모델 기반의 디코더는 이 객체 슬롯들을 조건으로 받아 매우 사실적인 새로운 시점의 이미지를 렌더링할 수 있다.</p>
<p>더 중요한 것은, 이 표현이 ’편집 가능’하다는 점이다. 특정 객체에 해당하는 슬롯을 제거하면 렌더링된 이미지에서 해당 객체가 사라지고, 다른 장면에서 가져온 객체 슬롯을 추가하면 새로운 객체가 장면에 나타나는 등의 조작이 가능하다.23 이는 모델이 단순히 픽셀을 암기하는 것이 아니라, 장면의 구성 요소들을 독립적으로 이해하고 있음을 의미한다. 이처럼 객체 중심 표현은 비디오를 2D 픽셀의 나열이 아닌, 조작 가능한 3D 세계의 투영으로 모델링하는 ’월드 모델(World Model)’로 나아가는 중요한 첫걸음이다. 이는 단순한 토큰화를 넘어, 인과 관계 추론, 계획, 상상 등 고차원적인 인지 능력을 갖춘 AI를 향한 핵심 경로라 할 수 있다.</p>
<h2>6.  멀티모달 융합 - 토큰의 활용과 상호작용</h2>
<p>비디오 토큰화 기술이 아무리 정교하게 발전하여 효율적이고 의미 있는 토큰을 생성하더라도, 그 자체만으로는 완전한 비디오 이해를 달성할 수 없다. 시각 토큰은 비디오의 ’내용’을 담고 있지만, 그 의미를 해석하고, 질문에 답하며, 추론하는 고차원적인 인지 능력은 부족하기 때문이다. 이 누락된 조각을 채우는 것이 바로 거대 언어 모델(LLM)이다. LLM은 방대한 텍스트 데이터를 통해 축적한 풍부한 상식, 지식, 그리고 강력한 언어적 추론 능력을 갖추고 있다. 따라서, 비디오 토큰화의 궁극적인 목표는 시각적 세계와 언어적 세계를 연결하는 다리를 놓는 것이며, 이 과정의 핵심이 바로 ’멀티모달 융합(Multimodal Fusion)’이다.24</p>
<p>융합은 서로 다른 모달리티(시각, 언어)에서 온 정보를 어떻게 효과적으로 결합하여 시너지를 창출할 것인가에 대한 근본적인 질문이다.26 이 장에서는 멀티모달 융합 분야에서 가장 영향력 있는 두 가지 접근법, 즉 Flamingo와 BEiT-3를 사례 연구로 삼아, 이들이 각기 다른 철학과 아키텍처를 통해 어떻게 시각 토큰과 언어 토큰을 융합하는지 심층적으로 분석한다. 이들의 비교는 단순히 기술적 차이를 넘어, 거대 멀티모달 모델을 구축하는 두 가지 상이한 철학, 즉 ’모듈식 결합’과 ‘통합 아키텍처’ 간의 대립을 보여준다.</p>
<h3>6.1  융합의 필요성: 왜 비디오 토큰을 LLM과 결합하는가?</h3>
<p>비디오 토큰을 LLM과 결합해야 하는 이유는 명확하다. 예를 들어, “영상 속 남자가 방금 무엇을 선반에 올려놓았나요?“라는 질문에 답하기 위해서는, 단순히 영상 속 객체들을 인식하는 것을 넘어, ‘남자’, ‘선반’, ’올려놓다’라는 개념을 이해하고, 시공간적 관계를 추론하며, 그 결과를 자연어로 생성하는 능력이 필요하다. 시각 토큰은 전자의 ’인식’을, LLM은 후자의 ’이해와 추론’을 담당한다. 이 둘의 결합을 통해 비로소 다음과 같은 고차원적인 작업이 가능해진다.</p>
<ul>
<li><strong>시각 질의응답 (Visual Question Answering):</strong> 이미지나 비디오에 대한 질문에 텍스트로 답하기.24</li>
<li><strong>이미지/비디오 캡셔닝 (Captioning):</strong> 시각적 콘텐츠의 내용을 설명하는 문장 생성하기.</li>
<li><strong>시각 대화 (Visual Dialogue):</strong> 이미지나 비디오를 주제로 사용자와 자연스러운 대화 이어나가기.</li>
</ul>
<p>따라서 융합 아키텍처의 성공은 두 모달리티의 정보를 얼마나 손실 없이, 그리고 의미적으로 깊게 통합하여 상호 보완적인 관계를 구축하느냐에 달려있다.25</p>
<h3>6.2  사례 연구 1: Flamingo - 어댑터 기반의 점진적 융합</h3>
<p>DeepMind에서 개발한 Flamingo는 거대 멀티모달 모델을 구축하는 데 있어 매우 실용적이고 영향력 있는 접근법을 제시했다.27 Flamingo의 기본 철학은 ’최고의 전문가들을 영입하되, 그들의 방식을 존중하는 것’에 비유할 수 있다. 즉, 이미 각자의 분야에서 최고의 성능을 보이는 사전 훈련된 비전 인코더(예: CLIP-style 모델)와 LLM(예: Chinchilla)을 가져와, 이들의 파라미터는 대부분 ’동결(frozen)’시킨다. 그리고 이 두 거대한 모델 사이에 소수의 새로운 학습 가능한 ‘어댑터(adapter)’ 모듈을 삽입하여 두 세계를 연결한다.28 이 방식은 막대한 계산 비용이 드는 전체 모델의 재훈련을 피하면서도 효과적인 융합을 달성할 수 있다는 장점이 있다.</p>
<p>Flamingo의 융합 메커니즘은 두 가지 핵심 구성 요소로 이루어진다.</p>
<ol>
<li>
<p><strong>Perceiver Resampler:</strong> 이 모듈은 비전 인코더로부터 나온, 가변적인 크기와 개수를 가진 시각 특징 맵(visual feature map)을 입력받는다. 비디오의 경우, 프레임마다 수많은 특징 벡터가 생성될 수 있는데, 이를 그대로 LLM에 연결하면 계산량이 폭발적으로 증가한다. Perceiver Resampler는 어텐션 기반의 메커니즘을 사용하여 이 방대한 시각 정보를 고정된 개수(예: 64개)의 간결한 ’시각 토큰(visual token)’으로 압축하고 정렬하는 역할을 한다.27 이는 LLM이 처리해야 할 시각 정보의 양을 극적으로 줄여, 전체 시스템의 계산 효율성을 확보하는 결정적인 장치다.</p>
</li>
<li>
<p><strong>Gated Cross-Attention Dense Layers:</strong> 융합의 핵심적인 상호작용은 이 레이어에서 일어난다. Flamingo는 동결된 LLM의 기존 트랜스포머 블록들 사이에 새로운 교차 어텐션(Cross-Attention) 레이어를 끼워 넣는다.30 일반적인 셀프 어텐션이 텍스트 토큰들끼리만 상호작용하는 반면, 교차 어텐션은 텍스트 토큰이 다른 모달리티의 토큰을 ’참조’할 수 있게 해준다. 구체적으로, LLM이 특정 텍스트 토큰의 다음 단어를 예측할 때, 이 교차 어텐션 레이어를 통해 해당 텍스트 토큰 바로 앞에 제시되었던 이미지나 비디오로부터 추출된 시각 토큰들(Perceiver Resampler의 출력)을 함께 고려하게 된다. 이를 통해 언어적 맥락에 시각적 정보가 자연스럽게 통합된다.</p>
</li>
</ol>
<p>특히 Flamingo는 여기에 ‘게이트(Gate)’ 메커니즘을 추가했다.28 이 게이트는 학습 초기에는 닫혀있어(가중치가 0에 가깝게 초기화됨), 새로 추가된 교차 어텐션 레이어가 동결된 LLM의 안정적인 사전 학습 상태를 해치지 않도록 한다. 학습이 진행됨에 따라 모델이 시각 정보가 유용하다고 판단하면, 이 게이트가 점진적으로 열리면서 시각 정보의 영향력을 키워나간다. 이 점진적인 융합 방식은 학습의 안정성을 크게 높이는 핵심적인 설계 비결이다.</p>
<p>Flamingo의 모듈식 접근법은 단 32개의 예시만으로도 16개의 벤치마크 태스크에서 기존의 소수샷(few-shot) 학습 방법들을 크게 능가하는 성능을 보여주며 그 실용성과 효율성을 입증했다.27</p>
<h3>6.3  사례 연구 2: BEiT-3 - 통합 아키텍처 기반의 깊은 융합</h3>
<p>Microsoft에서 개발한 BEiT-3는 Flamingo와는 정반대의 철학을 가진다.3 BEiT-3의 목표는 각기 다른 전문가를 모으는 것이 아니라, 언어와 비전을 모두 이해할 수 있는 ‘만능 전문가’ 한 명을 키워내는 것에 가깝다. 이를 위해 BEiT-3는 분리된 모델을 연결하는 대신, ’Multiway Transformer’라는 단일 통합 백본 아키텍처를 사용하여 여러 모달리티를 처음부터 함께 처리하고 학습시킨다.32 이는 더 근본적이고 깊은 수준의 융합을 목표로 하는 야심찬 접근법이다.</p>
<p>BEiT-3의 핵심인 <strong>Multiway Transformer</strong> 아키텍처는 다음과 같이 구성된다.32</p>
<ol>
<li><strong>공유 셀프 어텐션 (Shared Self-Attention):</strong> 각 Multiway Transformer 블록의 중심에는 모든 모달리티의 토큰(이미지 토큰, 텍스트 토큰)이 함께 입력되는 공유 셀프 어텐션 모듈이 있다. 이 모듈에서 이미지 토큰과 텍스트 토큰은 서로 직접적으로 상호작용하며 어텐션을 계산한다. 이 과정을 통해 모델은 두 모달리티 간의 근본적인 정렬(alignment)과 깊은 의미론적 관계를 학습하게 된다.</li>
<li><strong>모달리티 전문가 (Modality Experts):</strong> 공유 셀프 어텐션을 거친 후, 각 토큰은 자신의 모달리티에 따라 특화된 ’전문가 피드포워드 네트워크(expert FFN)’로 라우팅된다. 즉, 이미지 토큰은 ‘비전 전문가’ FFN으로, 텍스트 토큰은 ‘언어 전문가’ FFN으로 보내져 각 모달리티 고유의 특성을 처리한다.</li>
<li><strong>융합 전문가 (Fusion Experts):</strong> 모델의 최상위 몇 개 레이어에는 비전 전문가와 언어 전문가 외에, 두 모달리티가 깊게 융합된 표현을 처리하기 위한 ‘비전-언어 융합 전문가’ FFN이 추가로 존재한다. 이는 두 모달리티의 정보가 충분히 섞인 후, 이를 바탕으로 한 고차원적인 멀티모달 추론을 수행하기 위해 설계되었다.</li>
</ol>
<p>BEiT-3는 이러한 통합 아키텍처 위에서 ’마스크된 데이터 모델링(Masked Data Modeling)’이라는 통일된 사전 학습 과제를 수행한다. 이미지의 일부 패치를 마스킹하고 예측하게 하고(Imglish), 텍스트의 일부 단어를 마스킹하고 예측하게 하며(English), 이미지-텍스트 쌍 데이터에서는 둘 모두를 마스킹하고 상호 참조하여 예측하게 한다(“parallel sentences”).3</p>
<p>이러한 깊은 융합 방식 덕분에 BEiT-3는 단일 모델임에도 불구하고, 파인튜닝을 통해 비전 전용 작업(객체 탐지, 이미지 분류), 언어 전용 작업, 그리고 비전-언어 융합 작업(VQA, 이미지 캡셔닝) 등 매우 넓은 범위의 다운스트림 태스크에서 SOTA 성능을 달성했다.33 이는 범용 인공지능을 향한 중요한 단계로 평가받는다.</p>
<p>Flamingo와 BEiT-3의 대조적인 접근법은 거대 멀티모달 모델 개발의 핵심적인 딜레마를 보여준다. Flamingo의 모듈식 접근은 개발이 빠르고 기존 자산을 최대한 활용할 수 있지만 융합의 깊이에 한계가 있을 수 있다. 반면, BEiT-3의 통합적 접근은 더 근본적인 융합을 이룰 잠재력이 있지만 막대한 학습 비용과 복잡성을 수반한다. 미래의 멀티모달 AI는 이 두 철학 사이의 장점들을 어떻게 결합하느냐에 따라 그 발전 방향이 결정될 것이다.</p>
<p><strong>표 2: 멀티모달 융합 아키텍처 비교</strong></p>
<table><thead><tr><th>구분</th><th>Flamingo</th><th>BEiT-3</th></tr></thead><tbody>
<tr><td><strong>기본 철학</strong></td><td><strong>모듈식 결합 (Modular Combination):</strong> 사전 훈련된 최고의 단일 모달 모델들을 ’동결’하고, 그 사이에 학습 가능한 ’어댑터’를 삽입하여 연결하는 실용적 접근.27</td><td><strong>통합 아키텍처 (Unified Architecture):</strong> 처음부터 여러 모달리티를 함께 처리할 수 있는 단일 ‘Multiway Transformer’ 백본을 설계하여 깊은 융합을 추구하는 근본적 접근.32</td></tr>
<tr><td><strong>비전/언어 모델 처리</strong></td><td><strong>사전 훈련 후 동결 (Pre-trained and Frozen):</strong> 강력한 비전 인코더와 LLM을 각각 독립적으로 사전 훈련한 후, 파라미터를 고정하고 재사용한다.28</td><td><strong>통합 학습 (Unified Training):</strong> 단일 Multiway Transformer 내에서 비전과 언어 표현을 함께 학습시킨다. 모달리티별 처리는 ’전문가 네트워크’가 담당한다.3</td></tr>
<tr><td><strong>융합 메커니즘</strong></td><td><strong>Perceiver Resampler + Gated Cross-Attention:</strong> Perceiver Resampler로 시각 토큰 수를 줄이고, LLM 내부에 삽입된 Gated Cross-Attention 레이어를 통해 언어 토큰이 시각 토큰을 참조한다.27</td><td><strong>Shared Self-Attention + Modality Experts:</strong> 모든 토큰이 공유 셀프 어텐션에서 상호작용하여 모달리티 간 정렬을 학습하고, 이후 각 모달리티 전문가 네트워크에서 개별적으로 처리된 후 다시 융합된다.32</td></tr>
<tr><td><strong>핵심 장점</strong></td><td><strong>효율적인 소수샷 학습 (Efficient Few-Shot Learning):</strong> 적은 수의 예시만으로도 새로운 작업에 빠르게 적응하며 높은 성능을 보인다. 개발 및 적응이 빠르다.28</td><td><strong>범용성과 깊은 융합 (Generality and Deep Fusion):</strong> 단일 모델로 비전, 언어, 비전-언어 등 다양한 다운스트림 작업에서 SOTA 성능을 달성한다. 더 근본적인 수준의 융합 가능성을 가진다.33</td></tr>
</tbody></table>
<h2>7.  미래 전망과 핵심 도전 과제</h2>
<p>비디오 토큰화 기술은 VAE 기반의 단순 압축에서 시작하여 효율성, 충실도, 의미론적 표현력을 아우르는 정교한 인코딩 기술로 눈부시게 발전해왔다. 그러나 기술이 성숙해짐에 따라 새로운 차원의 도전 과제들이 수면 위로 떠오르고 있다. 미래의 비디오 토큰화 기술은 단순히 현재의 성능을 개선하는 것을 넘어, 확장성, 시간적 강건성, 효율과 품질의 상충관계, 그리고 범용성이라는 근본적인 문제들을 해결해야 한다. 궁극적으로 이 기술의 지향점은 비디오를 ’표현’하는 것을 넘어, 그 안에 담긴 세계의 물리적, 인과적 법칙을 내재적으로 모델링하는 ’월드 모델(World Model)’로의 진화에 있다.</p>
<h3>7.1  확장성 (Scalability): 장편 비디오 처리</h3>
<p>현재까지 개발된 대부분의 비디오 토큰화 모델들은 수십 초에서 길어야 수 분 단위의 짧은 비디오 클립을 처리하는 데 초점을 맞추고 있다.8 하지만 현실 세계의 비디오 데이터는 영화, 강의, 스포츠 경기 등 수십 분에서 수 시간에 이르는 장편 콘텐츠가 대부분이다. 이러한 장시간 비디오를 효율적으로 토큰화하고, 그 안에서 장기적인 맥락과 복잡한 서사를 이해하는 것은 여전히 거대한 도전 과제로 남아있다.</p>
<p>이 문제를 해결하기 위해서는 여러 방향의 연구가 필요하다. 첫째, **계층적 토큰화(Hierarchical Tokenization)**이다. 비디오를 먼저 짧은 클립 단위로 토큰화한 후, 이 클립 토큰들을 다시 한번 더 높은 수준의 추상적인 토큰으로 묶어 전체 비디오의 구조를 계층적으로 표현하는 방식이다. 둘째, **적응형 샘플링(Adaptive Sampling)**이다. 비디오 전체를 균일하게 처리하는 대신, 내용의 변화가 거의 없는 부분은 건너뛰고, 중요한 사건이 발생하는 부분에만 집중하여 토큰을 생성하는 지능적인 샘플링 전략이 필요하다. 셋째, <strong>효율적인 장거리 의존성 모델의 발전</strong>이다. 트랜스포머의 계산 복잡도 문제를 해결할 수 있는 Mamba와 같은 선형 시간 복잡도 모델의 연구는 장편 비디오의 전체적인 맥락을 끊김 없이 처리하는 데 핵심적인 역할을 할 것이다.19</p>
<h3>7.2  시간적 드리프트 (Temporal Drift) 문제</h3>
<p>장편 비디오 처리의 또 다른 난관은 ‘시간적 드리프트’ 문제다.35 이는 긴 비디오 시퀀스에서 동일한 객체나 사람이 조명, 자세, 시점 변화, 또는 가려짐(occlusion) 등으로 인해 외형이 점차 변할 때, 모델이 이를 일관된 하나의 개체로 인식하지 못하고 표현이 점차 ’표류(drift)’하는 현상을 말한다. 예를 들어, 영화 초반에 등장한 인물의 토큰 표현이 후반부에는 완전히 다른 값으로 변해, 모델이 동일 인물임을 인지하지 못하게 될 수 있다.</p>
<p>RefTok과 같은 참조 기반 접근법은 단기적인 시간적 일관성을 유지하는 데 효과적인 해결책을 제시했다.5 그러나 참조 프레임과 현재 프레임 사이의 시간 간격이 매우 길어지거나, 외형 변화가 극심할 경우에는 그 효과가 감소할 수 있다. 이 문제를 근본적으로 해결하기 위해서는 객체의 핵심적인 정체성(identity)을 시간 변화에 불변하는(invariant) 형태로 추출하고, 변화하는 외형 정보를 별도의 속성으로 모델링하는 더욱 강건한 표현 학습 메커니즘이 필요하다. 객체 중심 학습과 3D 표현 연구가 이 방향으로 나아가는 중요한 단초를 제공한다.</p>
<h3>7.3  계산 효율성과 재구성 충실도의 상충관계 (Trade-off)</h3>
<p>계산 효율성과 재구성 충실도 사이의 상충관계는 비디오 토큰화의 영원한 딜레마다. 토큰 수를 줄여 LLM의 처리 부담을 덜어주면(효율성 증가), 필연적으로 원본 비디오에 담긴 정보의 일부가 손실되어 재구성 품질이나 다운스트림 작업 성능이 저하될 수 있다.2</p>
<p>미래 연구는 이 상충관계를 완화하는 데 집중될 것이다. 핵심은 ’무작위 압축’이 아닌 ’의미론적 압축’을 고도화하는 것이다. 즉, 버려도 되는 중복 정보와 반드시 보존해야 할 핵심 정보를 지능적으로 판단하는 능력을 모델에 부여하는 것이다. Token Dynamics의 정보 분리 접근법이나 SweetTok의 시공간 분리 접근법은 이러한 방향성의 좋은 예다.2 앞으로는 인간의 시지각 시스템이 어떻게 정보의 우선순위를 정하는지에 대한 인지과학적 연구 결과를 모델 설계에 반영하거나, 특정 다운스트림 작업의 목표에 맞춰 가장 중요한 정보가 무엇인지를 동적으로 판단하여 토큰을 생성하는 ‘작업 인식(task-aware)’ 토큰화 기술이 중요해질 것이다.</p>
<h3>7.4  범용 토크나이저를 향하여 (Towards a Unified Tokenizer)</h3>
<p>현재의 비디오 토크나이저는 종종 특정 다운스트림 작업을 염두에 두고 설계된다. 예를 들어, 비디오 생성을 위한 토크나이저는 재구성 충실도를 극대화하는 방향으로, 비디오 이해를 위한 토크나이저는 의미론적 정보를 잘 압축하는 방향으로 최적화된다.36 이는 각기 다른 작업을 수행하기 위해 여러 개의 특화된 토크나이저를 개발하고 유지해야 하는 비효율을 낳는다.</p>
<p>미래의 이상적인 목표는 생성, 이해, 편집 등 다양한 비디오 관련 작업을 모두 지원할 수 있는 단일 ’범용 토크나이저(Unified Tokenizer)’를 개발하는 것이다. SemHiTok과 같은 연구는 의미론적으로 안내되는 계층적 코드북을 사용하여 생성과 이해 작업 모두에서 경쟁력 있는 성능을 보이는 통합 토크나이저의 가능성을 제시했다.36 이러한 범용 토크나이저가 개발된다면, 이는 다양한 비디오 애플리케이션을 위한 강력하고 일관된 ‘기초 표현(foundation representation)’ 역할을 하게 될 것이며, 이는 모델 개발의 효율성과 확장성을 크게 향상시킬 것이다.</p>
<h3>7.5  월드 모델(World Model)로서의 발전</h3>
<p>궁극적으로 비디오 토큰화 기술이 나아가야 할 방향은 단순히 비디오의 픽셀을 압축하고 표현하는 것을 넘어, 비디오에 담긴 3차원 세계의 구조와 동역학을 내재적으로 학습하는 ’월드 모델’로 발전하는 것이다. 월드 모델은 장면의 기하학적 구조, 객체 간의 물리적 상호작용(예: 충돌, 중력), 그리고 행동의 인과 관계 등을 이해하고 시뮬레이션할 수 있는 내부 모델을 의미한다.</p>
<p>객체 중심 학습 20과 3D 장면 표현 23에 대한 최근 연구들은 이러한 방향으로 나아가는 명백한 초기 단계이다. 객체 단위로 장면을 분해하고, 각 객체의 3D 속성과 움직임을 모델링함으로써, 시스템은 단순한 패턴 인식을 넘어 ’세상이 어떻게 작동하는지’에 대한 근본적인 이해에 도달할 수 있다. 이러한 월드 모델 기반의 토큰화는 미래에 기계가 단순히 과거를 요약하는 것을 넘어, 미래를 예측하고, 가능한 행동의 결과를 시뮬레이션하며, 주어진 목표를 달성하기 위한 계획을 수립하는 등 진정으로 지능적인 행동을 수행하기 위한 필수적인 기반이 될 것이다.</p>
<h2>8. 결론</h2>
<p>비디오 토큰화 기술은 지난 몇 년간 괄목할 만한 진화를 거듭하며, 인공지능의 시각 세계 이해 방식에 근본적인 변화를 가져왔다. 초기의 VAE 및 3D CNN 기반 접근법은 고차원의 비디오 데이터를 이산적인 토큰 시퀀스로 변환하는 길을 열었으나, 막대한 계산 비용과 제한적인 표현력이라는 명확한 한계를 드러냈다. 이러한 한계는 기술 발전의 원동력이 되어, 효율성, 충실도, 그리고 의미론적 깊이라는 세 가지 핵심 축을 중심으로 역동적인 패러다임 전환을 이끌었다.</p>
<p>기술의 진화 과정은 문제 해결의 연속이었다. 계산 효율성에 대한 요구는 1D 토큰화, 마스크드 모델링, 동적 토큰화와 같은 혁신을 낳았고, 이로 인해 희생된 재구성 충실도와 시간적 일관성은 다시 확산 모델 기반 디코더와 참조 기반 인코딩이라는 새로운 해결책을 통해 복원되었다. 더 나아가, 비디오 정보의 이질성에 대한 깊은 통찰은 시공간 정보를 분리하여 처리하는 정교한 아키텍처의 등장을 촉진했다. 가장 중요한 변화는 패치 기반 표현의 근본적인 한계를 인식하고, 인간의 인지 방식과 유사하게 세상을 ’객체’의 집합으로 이해하려는 객체 중심 표현으로의 전환이다. 이 모든 과정은 비디오 토큰화가 단순한 압축 도구에서, 시각 세계의 구조와 동역학을 포착하는 정교한 의미론적 인코더로 자리매김했음을 보여준다.</p>
<p>미래의 비디오 토큰화 기술은 거대 언어 모델(LLM)과의 긴밀한 공생 관계 속에서 발전할 것이다. LLM의 계산적 제약은 토큰화 기술에 극도의 효율성을 요구하는 동시에, LLM의 추론 능력은 토큰에 더 높은 수준의 의미론적 정보를 담아낼 것을 요구한다. 따라서 미래 기술은 장편 비디오 처리의 확장성, 시간적 드리프트에 대한 강건성, 그리고 다양한 작업을 아우르는 범용성이라는 도전 과제를 해결하며, 효율성과 의미론적 표현력이라는 두 가지 목표를 동시에 달성하는 방향으로 나아갈 것이다.</p>
<p>궁극적으로, 비디오 토큰화 기술의 발전은 단순히 특정 기술 분야의 성장을 넘어, 기계가 인간처럼 시각 세계를 보고, 이해하며, 추론하는 범용 인공지능(AGI)을 향한 여정에서 핵심적인 초석이 될 것이다. 비디오를 이해 가능한 언어로 번역하는 이 기술은 미래 AI가 현실 세계와 상호작용하고, 예측하며, 창조하는 능력의 근간을 이룰 것이 자명하다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>Rethinking Video Tokenization: A Conditioned Diffusion-based Approach - arXiv, accessed August 14, 2025, https://arxiv.org/html/2503.03708v1</li>
<li>Towards Efficient and Dynamic Video Token Representation for Video Large Language Models - arXiv, accessed August 14, 2025, https://arxiv.org/html/2503.16980v2</li>
<li>Image as a Foreign Language: BEiT Pretraining for Vision and Vision-Language Tasks - Microsoft Research, accessed August 14, 2025, https://www.microsoft.com/en-us/research/publication/image-as-a-foreign-language-beit-pretraining-for-vision-and-vision-language-tasks/</li>
<li>NeurIPS Poster An Image is Worth 32 Tokens for Reconstruction and Generation, accessed August 14, 2025, https://neurips.cc/virtual/2024/poster/93338</li>
<li>RefTok: Reference-Based Tokenization for Video Generation - arXiv, accessed August 14, 2025, https://arxiv.org/html/2507.02862v1</li>
<li>MambaVideo for Discrete Video Tokenization with Channel-Split Quantization - Research at NVIDIA, accessed August 14, 2025, https://research.nvidia.com/labs/dir/mamba-tokenizer/</li>
<li>(PDF) Rethinking Video Tokenization: A Conditioned Diffusion-based Approach, accessed August 14, 2025, https://www.researchgate.net/publication/389617371_Rethinking_Video_Tokenization_A_Conditioned_Diffusion-based_Approach</li>
<li>Extending Video Masked Autoencoders to 128 frames - NIPS, accessed August 14, 2025, https://proceedings.neurips.cc/paper_files/paper/2024/file/dbe2cfe4767f3255160b73a36ae3162e-Paper-Conference.pdf</li>
<li>arXiv:2412.10443v3 [cs.CV] 11 Mar 2025, accessed August 14, 2025, <a href="https://arxiv.org/pdf/2412.10443">https://arxiv.org/pdf/2412.10443?</a></li>
<li>[2507.04559] MambaVideo for Discrete Video Tokenization with Channel-Split Quantization, accessed August 14, 2025, https://arxiv.org/abs/2507.04559</li>
<li>ElasticTok: Adaptive Tokenization for Image and Video | OpenReview, accessed August 14, 2025, https://openreview.net/forum?id=tFV5GrWOGm</li>
<li>Fugu-MT 論文翻訳(概要): Token Dynamics: Towards Efficient and, accessed August 14, 2025, https://fugumt.com/fugumt/paper_check/2503.16980v3</li>
<li>RefTok: Reference-Based Tokenization for Video Generation - arXiv, accessed August 14, 2025, <a href="https://arxiv.org/pdf/2507.02862">https://arxiv.org/pdf/2507.02862?</a></li>
<li>RefTok: Reference-Based Tokenization for Video Generation | AI Research Paper Details, accessed August 14, 2025, https://www.aimodels.fyi/papers/arxiv/reftok-reference-based-tokenization-video-generation</li>
<li>RefTok: Reference-Based Tokenization for Video Generation - ResearchGate, accessed August 14, 2025, https://www.researchgate.net/publication/393379243_RefTok_Reference-Based_Tokenization_for_Video_Generation</li>
<li>[2507.02862] RefTok: Reference-Based Tokenization for Video Generation - arXiv, accessed August 14, 2025, https://arxiv.org/abs/2507.02862</li>
<li>SweetTok: Semantic-Aware Spatial-Temporal Tokenizer for Compact Video Discretization, accessed August 14, 2025, https://arxiv.org/html/2412.10443v3</li>
<li>[2412.10443] SweetTok: Semantic-Aware Spatial-Temporal Tokenizer for Compact Video Discretization - arXiv, accessed August 14, 2025, https://arxiv.org/abs/2412.10443</li>
<li>[2503.04130] Token-Efficient Long Video Understanding for Multimodal LLMs - arXiv, accessed August 14, 2025, https://arxiv.org/abs/2503.04130</li>
<li>Efficient Object-centric Representation Learning with Pre-trained Geometric Prior - arXiv, accessed August 14, 2025, https://arxiv.org/html/2412.12331v1</li>
<li>Object-Centric Video Representation for Long … - CVF Open Access, accessed August 14, 2025, https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Object-Centric_Video_Representation_for_Long-Term_Action_Anticipation_WACV_2024_paper.pdf</li>
<li>Slot-VPS: Object-Centric Representation Learning for Video Panoptic Segmentation - CVF Open Access, accessed August 14, 2025, https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Slot-VPS_Object-Centric_Representation_Learning_for_Video_Panoptic_Segmentation_CVPR_2022_paper.pdf</li>
<li>DORSal: Diffusion for Object-centric Representations of Scenes $\textit{et al.} - OpenReview, accessed August 14, 2025, https://openreview.net/forum?id=3zvB14IF6D</li>
<li>Flamingo - Intuitively and Exhaustively Explained | Towards Data Science, accessed August 14, 2025, https://towardsdatascience.com/flamingo-intuitively-and-exhaustively-explained-bf745611238b/</li>
<li>Multimodal Alignment and Fusion: A Survey - arXiv, accessed August 14, 2025, https://arxiv.org/html/2411.17040v1</li>
<li>Multimodal Models and Fusion - A Complete Guide - Medium, accessed August 14, 2025, https://medium.com/@raj.pulapakura/multimodal-models-and-fusion-a-complete-guide-225ca91f6861</li>
<li>DeepMind Flamingo: A Visual Language Model for Few-Shot Learning - Wandb, accessed August 14, 2025, <a href="https://wandb.ai/gladiator/Flamingo%20VLM/reports/DeepMind-Flamingo-A-Visual-Language-Model-for-Few-Shot-Learning--VmlldzoyOTgzMDI2">https://wandb.ai/gladiator/Flamingo%20VLM/reports/DeepMind-Flamingo-A-Visual-Language-Model-for-Few-Shot-Learning–VmlldzoyOTgzMDI2</a></li>
<li>[22.04] Flamingo | DOCSAID, accessed August 14, 2025, https://docsaid.org/en/papers/multimodality/flamingo/</li>
<li>Guide to Vision-Language Models (VLMs) - Encord, accessed August 14, 2025, https://encord.com/blog/vision-language-models-guide/</li>
<li>Understanding DeepMind’s Flamingo Visual Language Models | by Szymon Palucha, accessed August 14, 2025, https://medium.com/@paluchasz/understanding-flamingo-visual-language-models-bea5eeb05268</li>
<li>LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models - arXiv, accessed August 14, 2025, https://arxiv.org/html/2502.02406v1</li>
<li>BEiT-3: Image as a Foreign Language: BEiT Pretraining for All …, accessed August 14, 2025, https://sh-tsang.medium.com/beit-3-image-as-a-foreign-language-beit-pretraining-for-all-vision-and-vision-language-tasks-67c5ddee412b</li>
<li>[2208.10442] Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks - arXiv, accessed August 14, 2025, https://arxiv.org/abs/2208.10442</li>
<li>Multimodal Pretraining with Microsoft’s BEiT-3 - YouTube, accessed August 14, 2025, https://www.youtube.com/watch?v=anNCXUywPOM</li>
<li>CVPR Poster Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks, accessed August 14, 2025, https://cvpr.thecvf.com/virtual/2025/poster/34606</li>
<li>SemHiTok: A Unified Image Tokenizer via Semantic-Guided Hierarchical Codebook for Multimodal Understanding and Generation - arXiv, accessed August 14, 2025, https://arxiv.org/html/2503.06764v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>