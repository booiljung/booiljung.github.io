<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:모달리티 비디오 토큰화 기술의 패러다임 진화와 미래 전망</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>모달리티 비디오 토큰화 기술의 패러다임 진화와 미래 전망</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">토큰화 (Tokenizations)</a> / <span>모달리티 비디오 토큰화 기술의 패러다임 진화와 미래 전망</span></nav>
                </div>
            </header>
            <article>
                <h1>모달리티 비디오 토큰화 기술의 패러다임 진화와 미래 전망</h1>
<h2>1.  시각적 세계를 이산적 언어로 번역하다</h2>
<h3>1.1  AI 시대의 토큰화: 자연어 처리를 넘어 시각 데이터로</h3>
<p>인공지능(AI) 모델, 특히 대규모 언어 모델(LLM)의 발전은 ’토큰화(Tokenization)’라는 근본적인 전처리 과정에 깊이 뿌리내리고 있다. 자연어 처리(NLP) 분야에서 토큰화는 연속적인 텍스트 스트림을 단어, 서브워드(subword), 혹은 문자 등 의미를 갖는 최소 단위인 ’토큰’으로 분할하는 과정을 의미한다.1 이 과정을 통해 비정형 텍스트 데이터는 기계가 학습하고 분석할 수 있는 정형화된 시퀀스로 변환된다.</p>
<p>최근 AI 기술의 발전은 토큰화의 개념을 텍스트의 영역 너머로 확장시키고 있다. 이미지, 오디오, 비디오와 같은 복잡하고 연속적인 멀티모달 데이터를 AI 모델이 소화할 수 있는 형태로 변환하기 위한 핵심 기술로서 토큰화가 재조명되고 있다.3 이는 각 모달리티의 고유한 특성을 반영하여 데이터를 근본적인 표현 단위(fundamental representative units)로 분해하는 과정이며, 모델이 인간의 언어와 문화를 소화 가능한 입력 특징으로 받아들이는 관문 역할을 한다.3</p>
<p>여기서 AI 분야의 토큰화는 데이터 보안 분야에서 사용되는 동명의 개념과 명확히 구분될 필요가 있다. 보안 분야에서 토큰화는 신용카드 번호나 개인 식별 정보와 같은 민감한 데이터를 수학적 관계가 없는 비민감 데이터, 즉 ’토큰’으로 대체하여 보안을 강화하는 기술을 지칭한다.6 반면, 본 안내서에서 다루는 AI 토큰화는 데이터를 모델이 이해하고 처리할 수 있는 기본 구성 요소로 분해하는 과정에 초점을 맞춘다.3 이처럼 비디오 토큰화는 단순히 데이터를 분할하는 전처리를 넘어, 연속적인 시각 세계를 AI가 이해하고 생성할 수 있는 이산적인 디지털 언어로 번역하는 근본적인 패러다임 전환으로 이해해야 한다. 이 번역의 정교함과 효율성이 후속되는 비디오 이해 및 생성 과업의 성패를 좌우하는 핵심 변수로 작용한다.</p>
<h3>1.2  비디오 토큰화의 정의와 핵심 과제: 압축, 표현, 그리고 생성</h3>
<p>비디오 토큰화는 ’연속적인 시공간 픽셀 스트림(spatio-temporal pixel stream)을 트랜스포머(Transformer)와 같은 기계 학습 모델이 처리할 수 있는 이산적인 시맨틱 토큰(semantic token) 시퀀스로 변환하는 과정’으로 정의할 수 있다.3 이 과정은 비디오 데이터가 가진 고유한 문제들을 해결하기 위한 세 가지 핵심 과제를 내포한다.</p>
<p>첫째, **압축(Compression)**이다. 비디오는 엄청난 양의 데이터를 포함하며, 특히 프레임 간에 정적인 배경과 같이 시공간적으로 중복되는 정보가 많다.10 효율적인 토큰화는 이러한 중복성을 제거하여 방대한 원본 데이터를 계산적으로 다루기 용이한 간결한 형태로 압축해야 한다.13</p>
<p>둘째, **표현(Representation)**이다. 압축 과정에서 비디오의 핵심적인 의미 정보가 손실되어서는 안 된다. 생성된 토큰은 원본 비디오의 공간적 정보(객체, 질감, 장면 구성)와 시간적 정보(움직임, 객체 간 상호작용, 동적 변화)를 충실하게 표현해야 한다.14 좋은 표현력은 비디오 분류나 행동 인식과 같은 이해(understanding) 과제의 성능을 결정짓는다.</p>
<p>셋째, **생성(Generation)**이다. 현대의 AI 모델은 단순히 데이터를 이해하는 것을 넘어 새로운 데이터를 생성하는 방향으로 발전하고 있다. 따라서 비디오 토큰은 비디오 이해뿐만 아니라, 생성 모델이 학습하기에 적합한 구조를 가져야 한다.16 특히 GPT와 같은 자기회귀(autoregressive) 모델은 이산적이고 순차적인 토큰 입력을 기반으로 작동하므로, 토큰화 과정은 생성 모델의 학습 용이성을 고려하여 설계되어야 한다.16</p>
<p>이러한 과제들은 비디오 토큰화 기술의 발전이 ‘이해’ 중심에서 ‘이해와 생성의 통합’ 중심으로 진화해왔음을 시사한다. 초기 비디오 분석 연구가 분류나 탐지와 같은 이해 과제에 집중했다면, 최근 LLM과 확산 모델의 성공은 고품질 비디오 생성이라는 새로운 목표를 제시했다.18 이로 인해, 단순히 비디오를 잘 표현하는 것을 넘어 생성 모델이 쉽게 학습하고 예측할 수 있는 ’좋은 토큰’을 만드는 것이 토크나이저의 핵심 역할로 부상했다.</p>
<h3>1.3  본 안내서의 구조와 전개 방향</h3>
<p>본 안내서는 비디오 토큰화 기술의 패러다임 진화 과정을 체계적으로 분석하고 미래 전망을 고찰하는 것을 목표로 한다. 제2장에서는 3D CNN 기반의 전통적인 특징 추출 방식에서 트랜스포머 기반의 엔드투엔드(end-to-end) 토큰화로 이어지는 역사적 발전 과정을 추적한다. 제3장에서는 현대 비디오 토큰화의 양대 산맥인 ’공간-시간 패치 기반 토큰화’와 ‘벡터 양자화 기반 토큰화’ 패러다임을 심층적으로 비교 분석한다. 제4장에서는 계산 효율성과 콘텐츠 적응성을 높이기 위한 최신 토큰 최적화 전략들을 살펴본다. 제5장에서는 계산 복잡성, 의미 보존, 장기 의존성 모델링이라는 기술적 난제들을 조명하고 이를 극복하기 위한 노력을 다룬다. 마지막으로 제6장과 제7장에서는 비디오 토큰화가 멀티모달 통합 모델, 월드 모델, 로보틱스 등 미래 AI 기술 지형에 미칠 심대한 영향을 전망하며 결론을 맺는다.</p>
<h2>2.  비디오 표현 학습의 패러다임 진화: 픽셀에서 토큰으로</h2>
<h3>2.1  초기 접근법: 3D CNN과 시공간 특징 추출</h3>
<p>비디오 표현 학습의 초기 연구는 이미지 인식 분야에서 성공을 거둔 2D 합성곱 신경망(CNN)을 시간 차원으로 확장하려는 시도에서 시작되었다.20 그 대표적인 아키텍처가 바로 3D CNN이다. 3D CNN은 3차원 컨볼루션 필터(예: <span class="math math-inline">3 \times 3 \times 3</span> 커널)를 사용하여 비디오의 공간적 차원(높이 H, 너비 W)과 시간적 차원(T)을 동시에 처리한다.21 이를 통해 이미지 프레임 내의 외형적 특징과 프레임 간의 지역적인(local) 움직임 패턴을 하나의 통합된 특징으로 학습할 수 있었다.22</p>
<p>C3D (Convolutional 3D)는 이러한 접근법을 대표하는 초기 모델로, 작은 3D 커널을 여러 층으로 쌓아 시공간 특징을 직접 학습했다.22 이후 등장한 I3D (Inflated 3D ConvNet)는 ImageNet 데이터셋으로 사전 학습된 강력한 2D CNN 모델의 가중치를 3D 커널로 ’팽창(inflating)’시키는 독창적인 방법을 제안했다.22 즉, 2D 필터(<span class="math math-inline">k \times k</span>)를 시간 축으로 <span class="math math-inline">t</span>번 반복하여 3D 필터(<span class="math math-inline">t \times k \times k</span>)를 만들고, 가중치를 <span class="math math-inline">t</span>로 나누어 초기화했다. 이 방식은 대규모 이미지 데이터셋에서 학습된 풍부한 공간적 특징 표현 능력을 비디오 모델로 효과적으로 전이시켜, 처음부터 학습하는 것보다 훨씬 뛰어난 성능을 달성했다.25</p>
<p>하지만 3D CNN은 본질적인 한계를 가지고 있었다. 컨볼루션 연산은 정의상 지역적인 수용장(receptive field) 내에서만 정보를 처리하기 때문에, 비디오의 전역적인(global) 맥락이나 수십 초에 걸쳐 발생하는 장기적인 시간적 의존성(long-range temporal dependency)을 포착하는 데 어려움이 있었다.21 이를 극복하기 위해 네트워크를 더 깊게 쌓거나 더 큰 커널을 사용해야 했지만, 이는 엄청난 계산 비용과 메모리 사용량, 그리고 최적화의 어려움을 초래했다.20 비디오 표현 학습의 역사는 이처럼 ’계산 효율성’과 ’장기 의존성 포착’이라는 두 상충하는 목표 사이의 균형점을 찾으려는 끊임없는 시도의 과정이었다.</p>
<h3>2.2  트랜스포머의 등장과 비전 모델의 혁신</h3>
<p>이러한 한계는 NLP 분야에서 혁명을 일으킨 트랜스포머(Transformer) 아키텍처의 등장으로 새로운 국면을 맞이했다.26 트랜스포머의 핵심은 ‘셀프 어텐션(Self-Attention)’ 메커니즘으로, 이는 시퀀스 내의 모든 요소(토큰) 쌍 간의 관계를 직접 계산하여 가중치를 부여하는 방식이다.28 덕분에 시퀀스의 길이에 상관없이 멀리 떨어진 요소 간의 의존성을 효과적으로 모델링할 수 있었다.</p>
<p>이 아이디어를 시각 데이터에 적용한 것이 바로 ViT (Vision Transformer)이다.29 ViT는 이미지를 일정한 크기의 겹치지 않는 패치(patch)들로 분할한 뒤, 각 패치를 하나의 토큰처럼 취급하여 트랜스포머 인코더에 입력했다.31 이는 CNN이 가진 지역성(locality)이나 평행이동 등변성(translation equivariance)과 같은 강한 귀납적 편향(inductive bias) 없이, 대규모 데이터로부터 직접 시각적 패턴과 관계를 학습할 수 있는 새로운 가능성을 열었다.</p>
<p>ViT의 성공은 자연스럽게 비디오로의 확장으로 이어졌다. TimeSformer는 ViT를 비디오에 적용한 대표적인 초기 모델로, 계산 효율성 문제를 해결하기 위해 ’분할 시공간 어텐션(Divided Space-Time Attention)’이라는 독창적인 방식을 제안했다.32 이는 모든 시공간 패치 간의 어텐션을 한 번에 계산하는 대신, 먼저 각 패치의 동일한 공간 위치에 있는 프레임들 간에 ’시간 어텐션’을 계산하고, 그 결과에 대해 각 프레임 내의 모든 패치 간에 ’공간 어텐션’을 순차적으로 적용하는 방식이다. 이 분리된 접근법 덕분에 TimeSformer는 3D CNN에 비해 훈련 속도가 약 3배 빠르고, 추론에 필요한 계산량은 1/10 미만으로 줄이면서도 최첨단 성능을 달성했다.32</p>
<p>ViViT (Video Vision Transformer)는 여기서 한 걸음 더 나아가 다양한 비디오 토큰화 전략과 모델링 방식을 체계적으로 탐구했다.31 특히, 공간(h, w)과 시간(t)을 동시에 포함하는 3D 패치인 ‘튜블렛(tubelet)’ 임베딩을 제안하여, 토큰화 초기 단계부터 시공간 정보를 효과적으로 융합하는 방법을 제시했다.29 이러한 연구들은 비디오를 ’이미지의 연속’이 아닌, NLP의 문장처럼 ’시공간 토큰의 시퀀스’라는 새로운 관점으로 재정의하는 계기가 되었다.</p>
<h3>2.3  패러다임 전환의 의의: 지역적 특징에서 전역적 관계 모델링으로</h3>
<p>3D CNN에서 비전 트랜스포머로의 전환은 비디오 표현 학습의 패러다임이 ’지역적 특징의 합성(local feature aggregation)’에서 ’전역적 관계의 모델링(global relation modeling)’으로 근본적으로 이동했음을 의미한다.21 CNN의 컨볼루션 연산은 지역성과 같은 강력한 사전 지식을 모델에 주입하여 데이터 효율성을 높이지만, 이는 동시에 장거리 상호작용을 포착하는 데 제약으로 작용한다.35</p>
<p>반면, 트랜스포머의 셀프 어텐션은 모든 토큰 쌍 간의 상호작용을 직접 계산함으로써, 비디오의 시작 부분에 나타난 객체가 마지막 부분의 사건에 미치는 영향과 같은 복잡하고 비지역적인(non-local) 관계를 효과적으로 학습할 수 있다.32 이는 복잡한 인간 행동을 인식하거나, 여러 단계로 구성된 사건의 인과 관계를 추론하는 등 고차원적인 비디오 이해 과제에 필수적인 능력이다.32 트랜스포머의 도입은 비디오를 텍스트와 유사한 시퀀스 데이터로 취급할 수 있게 만들었고, 이는 자기회귀 모델링이나 마스킹과 같은 NLP의 강력한 기법들을 비디오 분야에 직접 적용할 수 있는 길을 열었다. 이는 OpenAI의 Sora가 비디오를 ’시공간 패치’의 집합으로 간주하는 철학의 토대가 되었다.36</p>
<h2>3.  현대 비디오 토큰화의 핵심 패러다임 분석</h2>
<p>비전 트랜스포머의 등장 이후, 비디오를 어떻게 효과적인 ’토큰’으로 변환할 것인가에 대한 연구는 두 가지 주요 패러다임으로 수렴했다. 하나는 ViT의 아이디어를 시공간으로 확장한 ’패치 기반 토큰화’이며, 다른 하나는 연속적인 특징을 이산적인 코드로 변환하는 ’벡터 양자화 기반 토큰화’이다. 이 두 패러다임은 서로 다른 목표를 위해 발전했으며, 각각의 장단점은 비디오 이해와 생성이라는 두 가지 주요 응용 분야와 밀접하게 연관되어 있다.</p>
<h3>3.1  공간-시간 패치 기반 토큰화 (Spatio-temporal Patch-based Tokenization)</h3>
<p>이 방식은 ViT의 패치 분할 개념을 비디오의 시간 차원까지 확장한 직관적인 접근법이다. 비디오 볼륨(<span class="math math-inline">T \times H \times W \times C</span>)을 겹치지 않는 작은 시공간 단위로 분할하여, 이를 트랜스포머가 처리할 수 있는 토큰 시퀀스로 만든다.29</p>
<p>주요 방식으로는 두 가지가 있다. 첫째, **균일 프레임 샘플링(Uniform Frame Sampling)**은 비디오 클립에서 일정한 간격으로 프레임을 샘플링한 후, 각 프레임을 독립적으로 2D 패치로 분할하는 방식이다. 이렇게 생성된 패치 토큰들에 공간적 위치 정보와 시간적 위치 정보를 담은 임베딩을 더하여 시공간적 순서를 모델에 알려준다.29 둘째, **튜블렛 임베딩(Tubelet Embedding)**은 시간과 공간을 동시에 고려하여 비디오를 <span class="math math-inline">t \times h \times w</span> 크기의 3D ’튜블렛’으로 직접 분할한다. 각 튜블렛은 선형 투영(linear projection)을 통해 하나의 토큰으로 변환되며, 이 과정에서 지역적인 시공간 정보가 자연스럽게 융합된다.29</p>
<p>이 패러다임의 가장 큰 장점은 아키텍처가 직관적이고 구현이 비교적 간단하다는 것이다.38 또한, 양자화와 같은 정보 손실 과정이 없기 때문에 원본 비디오의 미세한 정보를 잘 보존하여 비디오 분류나 행동 인식과 같은 분석적(analytical) 과제에 강점을 보인다. 그러나 치명적인 단점은 정적인 배경이나 움직임이 거의 없는 장면에서도 동적인 장면과 동일한 수의 토큰을 생성한다는 점이다. 이는 정보의 중복성을 야기하며, 특히 긴 비디오를 처리할 때 토큰 시퀀스가 기하급수적으로 길어져 엄청난 계산 비효율성을 초래한다.10</p>
<h3>3.2  벡터 양자화 기반 토큰화 (Vector Quantization-based Tokenization)</h3>
<p>벡터 양자화(VQ) 기반 토큰화는 연속적인 시각적 특징 공간을 이산적인(discrete) 코드들의 집합, 즉 ’코드북(codebook)’을 사용하여 양자화하는 방식이다. 이는 비디오를 마치 한정된 ’단어’들의 조합으로 이루어진 문장처럼 표현하는 것과 같다.39 이 패러다임의 핵심은 VQ-VAE (Vector-Quantized Variational Autoencoder) 아키텍처에 있다.41</p>
<p>VQ-VAE는 크게 세 부분으로 구성된다 40:</p>
<ol>
<li><strong>인코더(Encoder):</strong> 입력 비디오를 저차원의 연속적인 특징 맵 <span class="math math-inline">z_e(x)</span>로 압축한다.</li>
<li><strong>벡터 양자화(Vector Quantization):</strong> 특징 맵 <span class="math math-inline">z_e(x)</span>의 각 벡터를 학습된 코드북 <span class="math math-inline">e</span>에서 유클리드 거리 기준으로 가장 가까운 코드북 벡터로 대체한다. 이 ’최근접 이웃 탐색(nearest-neighbor lookup)’을 통해 연속적인 특징이 이산적인 코드북 인덱스 시퀀스로 변환된다.43</li>
<li><strong>디코더(Decoder):</strong> 양자화된 특징 맵 <span class="math math-inline">z_q(x)</span>를 입력받아 원본 비디오를 복원한다.39</li>
</ol>
<p>VQ-VAE의 학습은 다음 세 가지 손실 함수의 조합으로 이루어진다 45:<br />
<span class="math math-display">
L = \underbrace{\log p(x|z_q(x))}_{\text{Reconstruction Loss}} + \underbrace{\beta ||\text{sg}[z_e(x)] - e||_2^2}_{\text{Codebook Loss}} + \underbrace{\gamma ||z_e(x) - \text{sg}[e]||_2^2}_{\text{Commitment Loss}}
</span><br />
여기서 <strong>재구성 손실</strong>은 디코더의 복원 능력을, <strong>코드북 손실</strong>은 코드북 벡터가 인코더 출력에 가깝도록, <strong>커밋먼트 손실</strong>은 인코더 출력이 선택된 코드북 벡터에 가깝도록 각 구성 요소를 학습시킨다. <code>sg</code>는 그래디언트 흐름을 차단하는 연산자이다.</p>
<p>초기 VQ-VAE는 픽셀 단위의 재구성 손실(예: MSE)에 집중하여 다소 흐릿한 결과물을 생성하는 경향이 있었다.46 VQ-GAN은 여기에 GAN(Generative Adversarial Network)의 판별자(discriminator)와 지각적 손실(perceptual loss)을 추가하여 이 문제를 해결했다.43 판별자는 재구성된 비디오가 실제 비디오처럼 보이도록 학습을 유도하고, 지각적 손실은 픽셀 값의 차이가 아닌 심층 신경망의 특징 공간에서의 차이를 최소화하여 인간이 보기에 더 자연스럽고 사실적인 결과를 생성하도록 돕는다.39 이처럼 VQ-GAN은 토크나이저의 목표를 ’정확한 복원’에서 ’그럴듯한 복원’으로 전환시켰고, 이는 후속 생성 모델의 품질을 비약적으로 향상시키는 결정적인 계기가 되었다.</p>
<p>VQ 기반 토크나이저의 가장 큰 장점은 비디오를 이산적인 토큰 시퀀스로 변환함으로써, GPT와 같은 자기회귀(AR) 언어 모델의 학습 패러다임을 비디오 생성에 직접 적용할 수 있는 길을 열었다는 점이다.17 이는 비디오 생성을 ‘다음 토큰 예측’ 문제로 치환시켜 LLM의 강력한 생성 능력을 시각 영역으로 확장하는 핵심적인 다리 역할을 한다.</p>
<h3>3.3  패러다임 비교 분석</h3>
<p>두 패러다임은 각각의 장단점이 명확하여, 적용하고자 하는 과업의 성격에 따라 선택이 달라진다. 패치 기반 토큰화는 원본 정보 보존에 유리하여 ’분석’과 ’이해’에 초점을 맞춘 연구에서 주로 사용되는 반면, VQ 기반 토큰화는 이산적 표현을 통해 LLM과의 결합이 용이하여 ‘생성’ 과제에서 압도적인 강점을 보인다.</p>
<table><thead><tr><th>특성</th><th>공간-시간 패치 기반 토큰화</th><th>벡터 양자화(VQ) 기반 토큰화</th></tr></thead><tbody>
<tr><td><strong>핵심 원리</strong></td><td>비디오를 시공간적 패치(튜블렛)로 분할하고, 각 패치를 연속적인 벡터 토큰으로 변환 29</td><td>비디오를 연속적 특징 공간으로 인코딩한 후, 학습된 이산적 코드북에서 가장 가까운 코드로 양자화 39</td></tr>
<tr><td><strong>토큰의 성격</strong></td><td>연속적인(Continuous) 임베딩 벡터</td><td>이산적인(Discrete) 코드북 인덱스</td></tr>
<tr><td><strong>주요 장점</strong></td><td>- 아키텍처가 직관적이고 구현이 용이함 - 원본 정보의 손실이 적음 (양자화 오류 없음)</td><td>- LLM/AR 모델과 직접 호환 가능- 높은 압축률- 생성 품질이 우수함 17</td></tr>
<tr><td><strong>주요 단점</strong></td><td>- 높은 계산 비용 및 메모리 요구량<br />- 세밀한 시공간 정보 손실<br />- 장기적인 시간적 관계 모델링의 한계</td><td>- 정보 손실 (Information Loss)<br />- 높은 계산 복잡성 (High Computational Complexity)<br />- 오류 전파 (Error Propagation)<br />- 코드북 설계의 어려움 (Codebook Design Difficulty)<br />- 제한적인 표현력 (Limited Representational Power)</td></tr>
<tr><td><strong>대표 모델</strong></td><td>TimeSformer 32, ViViT 31, MViT 49</td><td>VQ-VAE 41, VQ-GAN 44, MAGVIT 47, VideoPoet 18, LARP 16</td></tr>
<tr><td><strong>주요 응용 분야</strong></td><td>비디오 분류, 행동 인식 등 이해(Understanding) 과제</td><td>텍스트-비디오 생성, 비디오 압축, 비디오 편집 등 생성(Generation) 과제</td></tr>
</tbody></table>
<h2>4.  효율성과 적응성을 향한 진화: 토큰 최적화 전략</h2>
<p>트랜스포머 기반 비디오 모델의 성능이 입증되면서, 그 엄청난 계산 비용은 실용화를 가로막는 가장 큰 장벽이 되었다. 특히 긴 비디오를 처리할 때 발생하는 토큰의 폭발적인 증가는 이 문제를 더욱 심화시켰다. 이에 따라, 비디오 토큰화의 연구 방향은 단순히 비디오를 표현하는 것을 넘어, 어떻게 더 ’효율적’이고 ’적응적’으로 토큰을 생성하고 처리할 것인가로 이동했다. 이러한 노력은 정적인 압축에서 동적인 압축으로, 그리고 사후 처리 방식에서 내재적 설계 방식으로 진화하는 양상을 보인다.</p>
<h3>4.1  적응형 토큰화: 콘텐츠 복잡도에 따른 동적 자원 할당</h3>
<p>기존의 고정된 토큰화 방식은 비디오 내용의 동적인 변화를 전혀 고려하지 않는다. 예를 들어, 변화가 거의 없는 정적인 인터뷰 장면과 격렬한 액션 장면 모두에 동일한 수의 토큰을 할당하는 것은 극심한 비효율을 야기한다.50 적응형 토큰화는 이러한 문제를 해결하기 위해 비디오 콘텐츠의 복잡도에 따라 토큰 수를 동적으로 조절하는 것을 목표로 한다.</p>
<p><strong>ElasticTok</strong>은 이전 프레임 정보를 바탕으로 현재 프레임을 가변적인 수의 토큰으로 인코딩하는 방법을 제안한다.50 훈련 과정에서 각 프레임의 토큰 인코딩 끝부분을 무작위 길이로 마스킹하여, 모델이 더 적은 수의 토큰만으로도 정보를 효과적으로 표현하는 법을 학습하도록 강제한다. 추론 시에는 비디오의 복잡도에 따라 필요한 만큼의 토큰만 동적으로 할당함으로써, 기존 방식 대비 최대 2~5배 적은 토큰으로 긴 비디오를 효율적으로 표현할 수 있다.50</p>
<p><strong>CoordTok</strong>은 훈련 패러다임 자체를 바꿔 효율성을 극대화한다.13 3D 생성 모델에서 영감을 받아, 비디오 전체를 한 번에 재구성하는 대신, 비디오를 좌표 기반 표현(factorized triplane)으로 인코딩하고 무작위로 샘플링된 시공간 좌표(</p>
<p><span class="math math-inline">(x, y, t)</span>)에 해당하는 패치만 재구성하도록 훈련한다. 이 방식은 긴 비디오에 대한 토크나이저 훈련 비용을 획기적으로 줄여, 이전에는 불가능했던 수백 프레임 길이의 비디오에 대해서도 시간적 일관성을 효과적으로 학습할 수 있게 한다.13</p>
<p><strong>AdapTok</strong>은 블록 단위 마스킹 전략과 재구성 품질을 예측하는 스코어러(scorer)를 결합하여, 주어진 총 토큰 예산 내에서 콘텐츠에 맞춰 각 프레임에 할당할 토큰 수를 최적으로 결정한다. 이는 샘플별, 콘텐츠 인식적, 시간적으로 동적인 토큰 할당을 가능하게 하여 효율성을 높인다.51</p>
<h3>4.2  토큰 수 감축 전략: 중복성 제거와 핵심 정보 집중</h3>
<p>적응형 토큰화가 토큰 생성 단계에서 효율성을 추구한다면, 토큰 수 감축 전략은 이미 생성된 토큰 시퀀스에서 중복되거나 덜 중요한 토큰을 제거하여 계산량을 줄이는 데 초점을 맞춘다.</p>
<p>**토큰 병합(Token Merging)**은 유사하거나 중복되는 정보를 가진 토큰들을 하나의 토큰으로 합치는 기법이다. 초기 연구들은 이미지용 토큰 병합 기법(예: ToMe)을 비디오에 단순 적용했지만, **VTM (Video Token Merging)**은 한 단계 더 나아가 토큰 간의 유사성뿐만 아니라 각 토큰의 ’중요도(saliency)’를 함께 고려하여 병합 대상을 동적으로 결정하는 학습 가능한 방식을 제안했다.52 이를 통해 정보 손실을 최소화하면서 메모리 비용을 84%, 처리량을 약 6.89배 향상시키는 놀라운 성과를 보였다.52</p>
<p>**토큰 선택(Token Selection) 및 제거(Token Dropping)**는 중요도가 낮은 토큰을 아예 처리 과정에서 제외하는 방식이다. **STTS (Spatial-Temporal Token Selection)**는 경량의 스코어 네트워크를 트랜스포머 레이어 사이에 삽입하여 각 토큰의 중요도를 실시간으로 예측하고, 점수가 높은 상위 K개의 토큰만 선택하여 다음 레이어로 전달한다. 이 ’가지치기’를 통해 전체 계산량(GFLOPs)을 33% 이상 줄이면서도 정확도 하락은 0.7% 이내로 억제했다.54 한편, **RLT (Run-Length Tokenization)**는 데이터 압축 기법인 런-렝스 인코딩에서 영감을 얻어, 시간적으로 연속되는 동일하거나 매우 유사한 패치 ’런(run)’을 찾아 하나의 대표 토큰과 그 길이 정보로 압축한다. 이 방식은 별도의 학습 없이 콘텐츠에 따라 적응적으로 토큰 수를 줄일 수 있어 매우 효율적이며, 훈련 시간을 30% 단축하는 효과를 보였다.12</p>
<p>이러한 효율화 기술의 발전은 단순히 연산 속도를 높이는 양적 개선을 넘어, 이전에는 계산 비용 문제로 다루기 어려웠던 ‘장편 비디오(long-form video)’ 분석 및 생성이라는 새로운 응용 분야를 여는 질적 변화를 촉발했다. 효율적인 토큰화는 장기적인 스토리나 복잡한 사건을 이해하는 데 필수적인 긴 시간의 맥락을 모델이 처리할 수 있도록 만드는 핵심 동력이다.55</p>
<h3>4.3  효율적 비디오 토크나이저 모델 비교</h3>
<p>아래 표는 본 절에서 논의된 주요 효율적 비디오 토크나이저 모델들의 핵심 전략과 성과를 요약한 것이다. 각 모델은 서로 다른 접근법을 통해 비디오 처리의 효율성을 극대화하고자 하며, 이는 특정 응용 분야에 적합한 기술을 선택하는 데 중요한 지표를 제공한다.</p>
<table><thead><tr><th>모델명</th><th>핵심 전략</th><th>주요 장점 (효율성 측면)</th><th>성능 영향</th><th>관련 Snippet</th></tr></thead><tbody>
<tr><td><strong>ElasticTok</strong></td><td>적응형 토큰화 (Adaptive Tokenization)</td><td>콘텐츠 복잡도에 따라 토큰 수를 동적으로 조절, 불필요한 토큰 생성 방지</td><td>2-5배 적은 토큰으로 유사한 재구성 품질 달성</td><td>50</td></tr>
<tr><td><strong>CoordTok</strong></td><td>좌표 기반 재구성 (Coordinate-based Reconstruction)</td><td>긴 비디오를 전체 재구성 없이 부분 샘플링으로 훈련하여 훈련 비용 대폭 감소</td><td>128프레임 비디오를 기존 6144개 대비 1280개 토큰으로 표현</td><td>13</td></tr>
<tr><td><strong>VTM</strong></td><td>학습 가능한 토큰 병합 (Learnable Token Merging)</td><td>유사도와 중요도를 함께 고려하여 중복 토큰을 정보 손실 없이 병합</td><td>메모리 84% 감소, 처리량 6.89배 증가, 성능 유지 또는 향상</td><td>52</td></tr>
<tr><td><strong>STTS</strong></td><td>토큰 선택 (Token Selection)</td><td>경량 스코어러로 중요 토큰만 선별하여 처리</td><td>계산량(GFLOPs) 33% 감소, 정확도 하락 0.7% 이내</td><td>54</td></tr>
<tr><td><strong>RLT</strong></td><td>런-렝스 토큰화 (Run-Length Tokenization)</td><td>시간적으로 반복되는 패치를 단일 토큰으로 압축하여 중복성 제거</td><td>훈련 시간 30% 단축, 성능 유지</td><td>12</td></tr>
</tbody></table>
<h2>5.  비디오 토큰화의 기술적 난제와 극복 노력</h2>
<p>비디오 토큰화 기술은 괄목할 만한 발전을 이루었지만, 여전히 실용화를 위해 해결해야 할 근본적인 기술적 난제들이 존재한다. 계산 복잡성, 의미 보존, 그리고 장기 의존성 모델링이라는 세 가지 문제는 서로 긴밀하게 얽혀 있으며, 하나의 문제를 해결하려는 시도가 다른 문제를 악화시키는 ‘트릴레마(trilemma)’ 관계를 형성한다.</p>
<h3>5.1  계산 복잡성: 트랜스포머의 이차적 복잡도와 실시간 처리의 장벽</h3>
<p>가장 근본적인 문제는 트랜스포머의 셀프 어텐션 메커니즘이 가진 계산 복잡성이다. 어텐션 연산은 토큰 시퀀스의 길이(<span class="math math-inline">N</span>)에 대해 제곱에 비례하는(<span class="math math-inline">O(N^2)</span>) 계산량과 메모리를 요구한다.56 비디오는 프레임 수(<span class="math math-inline">T</span>), 높이(<span class="math math-inline">H</span>), 너비(<span class="math math-inline">W</span>)에 따라 토큰 수가 매우 많아지므로(<span class="math math-inline">N \propto T \times H \times W</span>), 이 이차적 복잡도는 고해상도 장편 비디오의 실시간 처리를 거의 불가능하게 만든다.57</p>
<p>이를 극복하기 위한 노력은 4장에서 논의된 토큰 수 자체를 줄이는 기법들 외에도, 어텐션 연산 자체를 근사화하는 방향으로도 진행되고 있다. 예를 들어, 어텐션 행렬을 저계급(low-rank) 행렬로 근사하거나 57, 어텐션 계산을 커널 함수(kernel function) 관점에서 재해석하여 스트리밍 비디오에서 프레임 간 계산을 효율적으로 재사용하는 <strong>TeSTra</strong>와 같은 연구가 대표적이다.57 또한, 실용적인 관점에서는 모델 아키텍처를 단순화하거나(예: 어텐션 헤드 수 감소) 입력 해상도를 동적으로 조절하여 추론 지연 시간을 목표 수준에 맞추려는 시도도 이루어지고 있다.58</p>
<h3>5.2  의미 보존의 딜레마: 압축률과 시맨틱 정보 간의 상충 관계</h3>
<p>비디오 토큰화는 본질적으로 손실 압축(lossy compression) 과정이다. 계산 복잡성을 줄이기 위해 압축률을 높일수록(즉, 토큰 수를 줄일수록) 원본 비디오가 담고 있던 미세한 시맨틱 정보가 손실될 위험이 커진다.50 특히, 생성 모델에서는 토큰 간의 의존성이 강하기 때문에 단 하나의 토큰 손실이 연쇄적으로 오류를 전파시켜 전체 비디오의 의미를 심각하게 왜곡할 수 있다.60</p>
<p>이 딜레마를 해결하기 위해 ’시맨틱 중심 압축’이라는 개념이 중요해진다. 단순히 픽셀 유사도에 기반해 토큰을 줄이는 것이 아니라, 내용적으로 중요한 정보를 보존하는 방향으로 압축이 이루어져야 한다. <strong>VTM</strong>은 토큰의 ’중요도(saliency)’를 학습하여 중요한 토큰은 병합 대상에서 제외함으로써 이를 구현했다.52 이미지 생성 분야의 <strong>SeQ-GAN</strong>은 VQ-VAE가 색상이나 질감 같은 저수준 디테일보다는 객체의 형태와 같은 ’시맨틱 특징’을 코드북에 우선적으로 압축하도록 유도하고, 디테일 복원은 디코더의 역할로 분리함으로써 생성 모델의 학습 효율과 품질을 높였다.39 또한, 통신 환경에서의 견고성(robustness)을 목표로 하는 <strong>ResiTok</strong>은 전송 과정에서 일부 토큰이 손실되더라도 전체적인 시각적 무결성을 최대한 유지할 수 있도록 ‘제로-아웃(zero-out)’ 훈련 전략을 사용하여 토크나이저의 복원력을 강화했다.61</p>
<h3>5.3  장기 의존성 문제: 시간적 일관성 유지를 위한 아키텍처 탐색</h3>
<p>긴 비디오에서 초기 프레임의 정보가 후반부의 내용에 영향을 미치는 ’장기 의존성(long-term dependency)’을 효과적으로 모델링하는 것은 비디오 이해와 생성 모두에서 매우 중요한 과제이다. 이는 전통적인 순환 신경망(RNN)이 기울기 소실(vanishing gradient) 문제로 인해 어려움을 겪었던 고질적인 문제이며, 트랜스포머 역시 시퀀스가 매우 길어지면 여전히 도전적인 과제로 남는다.62 시간적 일관성이 깨지면 생성된 비디오에서 객체가 갑자기 사라지거나 스타일이 바뀌는 등의 부자연스러운 결과가 나타난다.</p>
<p>이 문제 해결의 패러다임은 순환(Recurrence)에서 전역적 연결(Global Connectivity)로, 그리고 최근에는 압축된 상태(Compressed State)로 진화하고 있다. **LSTM(Long Short-Term Memory)**은 내부에 ’셀 상태(cell state)’와 세 개의 게이트(forget, input, output)를 두어 장기 정보를 선택적으로 기억하고 전달하는 순환 구조를 통해 이 문제를 해결하고자 했다.63</p>
<p><strong>트랜스포머</strong>는 셀프 어텐션을 통해 모든 시간 단계 간의 관계를 직접 모델링하는 전역적 연결 방식으로 접근했다.32 이는 매우 효과적이었지만 앞서 언급한 계산 복잡성 문제를 야기했다.</p>
<p>최근에는 이 둘의 장점을 결합하려는 시도로서 <strong>Mamba</strong>와 같은 상태 공간 모델(State Space Model, SSM)이 주목받고 있다. SSM은 시퀀스 정보를 효율적인 ’압축된 상태’로 요약하고, 이 상태를 RNN처럼 선형 시간 복잡도(<span class="math math-inline">O(N)</span>)로 업데이트하면서도 트랜스포머에 버금가는 표현력을 보여준다. <strong>STORM</strong>과 같은 모델은 Mamba 기반의 시간 프로젝터를 사용하여 시각 토큰에 풍부한 시간적 동역학 정보를 주입함으로써, 장기 의존성 포착 능력을 크게 향상시키면서도 효율적인 토큰 압축을 가능하게 했다.55 이는 장기 의존성, 계산 복잡성, 의미 보존이라는 트릴레마를 동시에 해결하려는 통합적인 접근법의 좋은 예시이다.</p>
<h2>6.  미래 전망: 통합 지능을 향한 비디오 토큰의 역할</h2>
<p>비디오 토큰화 기술의 발전은 단순히 비디오 처리 효율을 높이는 것을 넘어, 인공지능이 물리적 세계를 이해하고, 시뮬레이션하며, 상호작용하는 방식을 근본적으로 바꾸는 토대를 마련하고 있다. 미래의 비디오 토큰은 단순한 데이터 표현 단위를 넘어, AI의 ’물리 세계 언어(Language of the Physical World)’로 진화할 것이다.</p>
<h3>6.1  차세대 생성 모델의 초석: Sora와 같은 대규모 비디오 생성 모델의 기저</h3>
<p>OpenAI의 Sora는 비디오 생성 모델의 새로운 지평을 열었으며, 그 핵심에는 진보된 토큰화 철학이 자리 잡고 있다. Sora는 비디오를 ’시공간 패치(spacetime patches)’라는 통일된 표현으로 변환한다.36 이는 ViT의 패치 개념을 시공간으로 확장한 것으로, 다양한 해상도, 길이, 종횡비를 가진 비디오와 이미지를 동일한 아키텍처에서 일관되게 처리할 수 있게 하는 핵심 기술이다.</p>
<p>Sora는 VQ-VAE와 같은 이산적 토큰화 대신, 연속적인 잠재 공간(latent space)에서 작동하는 확산 트랜스포머(Diffusion Transformer) 아키텍처를 채택했다.36 먼저, 강력한 비디오 압축 네트워크를 통해 원본 비디오를 시공간적으로 압축된 잠재 공간으로 인코딩한다. 그 후, 이 잠재 공간의 시공간 패치에 노이즈를 추가하고, 텍스트 프롬프트를 조건으로 하여 원래의 ‘깨끗한’ 패치를 예측하도록 트랜스포머 모델을 훈련시킨다. 이는 비디오 토큰화가 반드시 이산적일 필요는 없으며, 생성 모델의 아키텍처(자기회귀 vs. 확산)에 따라 최적의 표현 방식이 달라질 수 있음을 시사한다. Sora의 성공은 ‘확장 가능한(scalable)’ 토큰화 방식이 고품질 장편 비디오 생성의 핵심임을 명확히 보여주었다.</p>
<h3>6.2  멀티모달 통합의 열쇠: 비디오, 오디오, 텍스트를 아우르는 공유 토큰 공간</h3>
<p>미래 AI의 궁극적인 목표 중 하나는 인간처럼 다양한 감각 정보를 통합적으로 이해하고 처리하는 것이다. 이를 위해 시각, 청각, 언어 등 서로 다른 모달리티를 별개의 모델로 처리하는 대신, 이를 공통의 ’토큰 어휘(token vocabulary)’로 변환하여 단일 대규모 모델이 처리하도록 하는 연구가 활발히 진행되고 있다.47</p>
<p>이를 구현하는 주요 전략은 두 가지로 나뉜다. 첫째는 <strong>모달리티별 토크나이저</strong>를 사용하는 방식이다. <strong>VideoPoet</strong>과 같은 모델은 비디오용 토크나이저(MAGVIT-v2)와 오디오용 토크나이저(SoundStream)를 각각 사용하여 각 모달리티를 이산 토큰으로 변환한 후, 이들의 어휘를 통합하여 단일 LLM에 입력한다.18 둘째는 <strong>공유 임베딩 공간</strong>을 학습하는 방식이다. <strong>VATT</strong>는 대조 학습(contrastive learning)을 통해 비디오, 오디오, 텍스트 임베딩을 동일한 벡터 공간 상에 정렬시켜, 모달리티 간의 의미적 유사성을 포착한다.56</p>
<p>이러한 통합 토큰 공간은 진정한 의미의 멀티모달 이해와 생성을 가능하게 한다. 예를 들어, 비디오 장면을 보고 그에 맞는 효과음을 생성하거나(video-to-audio), 음성 명령으로 비디오를 편집하는(audio-to-video-edit) 등 이전에는 불가능했던 복합적인 상호작용이 가능해진다.68 미래의 비디오 토크나이저는 단순히 비디오 재구성 능력만으로 평가받는 것이 아니라, 다른 모달리티와의 ‘정렬(alignment)’ 능력, 즉 통합 파운데이션 모델 전체의 성능에 얼마나 기여하는지로 그 가치가 결정될 것이다.70</p>
<h3>6.3  월드 모델과 물리적 세계의 시뮬레이션</h3>
<p>**월드 모델(World Model)**은 AI 에이전트가 특정 행동을 취했을 때 미래에 어떤 결과가 나타날지 예측(시뮬레이션)하는 데 사용하는 환경의 내적 모델이다.71 비디오 생성 모델은 물리 세계의 동역학, 즉 ’세상이 어떻게 작동하는가’를 데이터로부터 학습하기 때문에 강력한 월드 모델의 역할을 수행할 수 있다.72 Sora의 기술 안내서 제목이 “비디오 생성 모델은 세계 시뮬레이터(Video generation models as world simulators)“인 것은 이러한 비전을 명확히 보여준다.36</p>
<p>이러한 맥락에서 비디오 토큰은 월드 모델이 학습하고 예측하는 ’압축된 세계 상태(compressed world state)’를 나타내는 기본 단위가 된다. 비디오 토큰 시퀀스를 예측하는 것은 물리 세계의 다음 상태를 예측하는 것과 동일한 의미를 갖는다. 이는 특히 로보틱스 분야의 **모방 학습(Imitation Learning)**과 깊은 관련이 있다. **ACT(Action Chunking Transformer)**와 같은 모델은 인간의 시연 비디오(관찰)와 로봇 팔의 동작(행동)을 모두 토큰 시퀀스로 변환하여, 트랜스포머가 관찰로부터 행동을 예측하도록 학습한다.73 더 나아가</p>
<p><strong>WorldVLA</strong>와 같은 연구는 행동 모델(Vision-Language-Action)과 월드 모델을 단일 프레임워크로 통합하여, 행동 예측과 미래 상태 예측이 서로를 보강하며 학습하도록 설계했다.71 여기서 비디오 토큰화는 로봇이 세계를 인식하고 자신의 행동 결과를 예측하는 핵심 연결고리 역할을 한다.</p>
<h3>6.4  메타버스와 실시간 상호작용: 동적 가상 세계 구축을 위한 생성적 토대</h3>
<p>메타버스와 같은 광활하고 동적인 가상 세계를 구축하는 데 있어 가장 큰 병목은 콘텐츠 제작이다.76 생성 AI는 텍스트 프롬프트나 사용자 상호작용에 따라 실시간으로 가상 환경, NPC(Non-Player Character)의 행동, 인터랙티브한 스토리 등을 자동으로 생성함으로써 이 문제를 해결할 핵심 기술로 주목받고 있다.78</p>
<p>효율적인 비디오 토큰화 및 생성 기술은 메타버스 내에서 저지연으로 고품질의 동적 비디오 콘텐츠(예: 가상 TV 방송, 동적 광고, NPC 애니메이션)를 스트리밍하고 생성하는 기술적 토대를 제공한다.80 사용자의 행동에 실시간으로 반응하여 변화하는 환경, 사용자와 자연스럽게 상호작용하는 NPC 등은 모두 강력한 비디오 생성 모델을 필요로 한다. 비디오 토큰화는 이러한 동적 가상 세계를 구축하고 사용자에게 더욱 몰입감 있고 개인화된 경험을 제공하는 데 필수적인 기반 기술이 될 것이다.81</p>
<h2>7.  결론: 비디오 토큰화, 새로운 디지털 언어의 탄생</h2>
<h3>7.1  패러다임 진화의 핵심 동인 요약</h3>
<p>본 안내서에서 살펴본 바와 같이, 비디오 토큰화 기술은 지난 수년간 괄목할 만한 패러다임 전환을 겪었다. 이러한 진화는 세 가지 핵심적인 동인에 의해 추동되었다. 첫째, <strong>계산 효율성 증대</strong>의 압박은 3D CNN의 막대한 계산 비용을 극복하고 실시간 처리를 가능하게 하기 위해 트랜스포머 아키텍처와 다양한 토큰 감축 및 적응형 기법의 등장을 촉진했다. 둘째, 복잡한 사건과 행동을 이해하기 위한 <strong>장기 의존성 모델링</strong>의 필요성은 지역적 연산에 머물렀던 CNN을 넘어 모든 시점 간의 관계를 모델링할 수 있는 어텐션 기반 모델로의 전환을 이끌었다. 셋째, 비디오 이해를 넘어 생성을 목표로 하는 <strong>생성 모델과의 시너지 창출</strong> 요구는 LLM과 자연스럽게 결합할 수 있는 벡터 양자화 기반의 이산적 토큰화 패러다임을 주류로 부상시켰다. 이 세 가지 동인은 상호작용하며 비디오 토큰화 기술을 정적이고 고정된 방식에서 동적이고 적응적이며 생성 친화적인 방향으로 발전시켰다.</p>
<h3>7.2  미래 연구 방향과 기술적 제언</h3>
<p>비디오 토큰화는 이제 AI가 시각적 세계를 인식하고 상상하는 방식의 근간을 이루는 핵심 기술로 자리매김했다. 앞으로의 연구는 다음과 같은 방향으로 더욱 심화될 것으로 전망된다.</p>
<ul>
<li><strong>연속-이산 하이브리드 토큰화:</strong> 현재 패러다임은 확산 모델이 선호하는 연속적 잠재 공간과 자기회귀 모델에 적합한 이산적 토큰 공간으로 양분되어 있다. <strong>ACDiT</strong>와 같이 두 방식의 장점을 결합하여, 연속적 특징 블록을 자기회귀적으로 생성하는 하이브리드 접근법은 생성 품질과 구조적 제어 가능성을 모두 높일 수 있는 유망한 연구 방향이다.82</li>
<li><strong>인과적 및 물리 법칙 기반 토큰화:</strong> 현재의 토큰화는 대부분 데이터의 통계적 패턴에 기반하여 외형을 압축한다. 미래의 토크나이저는 비디오 내의 객체 간 상호작용, 물리적 법칙, 사건의 인과 관계 등 더 깊은 수준의 구조를 내재적으로 학습하여, ‘왜’ 그런 움직임이 나타나는지를 이해하는 방향으로 발전해야 한다. 이는 보다 일관성 있고 논리적인 비디오 생성 및 예측을 가능하게 할 것이다.</li>
<li><strong>실시간 및 온디바이스 최적화:</strong> 자율주행차, 로봇, AR/VR 기기 등 엣지 디바이스에서의 실시간 비디오 이해 및 생성 수요가 증가함에 따라, 모델 경량화, 양자화, 지식 증류 등을 통해 제한된 자원에서도 효율적으로 작동하는 토큰화 기술에 대한 연구가 더욱 중요해질 것이다.</li>
</ul>
<p>궁극적으로 비디오 토큰화 기술의 비전은 시각적 세계를 이해하고, 예측하며, 창조하는 범용 인공지능(AGI)의 ’눈’과 ’상상력’을 구현하는 데 있다. 이는 단순히 비디오를 처리하는 기술을 넘어, 기계가 인간과 같이 시각적 세계와 소통하고 상호작용하는 방식을 근본적으로 바꾸는 새로운 디지털 언어의 탄생을 예고한다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>What is tokenization? - GeeksforGeeks, https://www.geeksforgeeks.org/nlp/what-is-tokenization/</li>
<li>Tokenization in LLMs: How It Works - LXT, https://www.lxt.ai/ai-glossary/tokenization-in-llms/</li>
<li>What is Tokenization? - Moveworks, https://www.moveworks.com/us/en/resources/ai-terms-glossary/tokenization</li>
<li>Beyond Words: Exploring Tokenization Techniques for Sentiment Analysis in Multimodal Data | by Kenza hassanine | Medium, https://medium.com/@kenzahassanine1/beyond-words-exploring-tokenization-techniques-for-sentiment-analysis-in-multimodal-data-b577fea24a49</li>
<li>Tokenizers for Foundational models and LLMs | by Mehul Gupta | Data Science in Your Pocket | Jun, 2025 | Medium, https://medium.com/data-science-in-your-pocket/tokenizers-for-foundational-models-and-llms-6c864e3f060f</li>
<li>What is Tokenization? - IXOPAY, https://www.ixopay.com/blog/what-is-tokenization</li>
<li>Tokenization Explained: What Is Tokenization &amp; Why Use It? - Okta, https://www.okta.com/identity-101/tokenization/</li>
<li>What is tokenization? A primer on card tokenization - Mastercard, https://www.mastercard.com/us/en/news-and-trends/stories/2025/what-is-tokenization.html</li>
<li>Tokenization for Generative AI and RAG, Explained - YouTube, https://www.youtube.com/watch?v=T_1pdsC0x0o</li>
<li>VidTok: Rethinking Video Processing with Compact Tokenization | by Zilliz | Medium, https://medium.com/@zilliz_learn/vidtok-rethinking-video-processing-with-compact-tokenization-a2070a805a2d</li>
<li>VidTok introduces compact, efficient tokenization to enhance AI video processing - Microsoft, https://www.microsoft.com/en-us/research/blog/vidtok-introduces-compact-efficient-tokenization-to-enhance-ai-video-processing/</li>
<li>Don’t Look Twice: Faster Video Transformers with Run-Length Tokenization - NIPS, https://proceedings.neurips.cc/paper_files/paper/2024/file/3181db351fd3ced43cd589b0b572675d-Paper-Conference.pdf</li>
<li>Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction - arXiv, https://arxiv.org/html/2411.14762v1</li>
<li>[2405.06574] Deep video representation learning: a survey - arXiv, https://arxiv.org/abs/2405.06574</li>
<li>Deep Video Representation Learning: a Survey - arXiv, https://arxiv.org/html/2405.06574v1</li>
<li>LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior | OpenReview, https://openreview.net/forum?id=Wr3UuEx72f</li>
<li>Language Model Beats Diffusion - Tokenizer is key to visual …, https://openreview.net/forum?id=gzqrANCF4g</li>
<li>Tokenization In Large Language Model Video Generation | HackerNoon, https://hackernoon.com/tokenization-in-large-language-model-video-generation</li>
<li>Sora: Creating video from text - OpenAI, https://openai.com/index/sora/</li>
<li>EAR: Efficient action recognition with local-global temporal aggregation, https://web.pkusz.edu.cn/adsp/files/2022/01/EAR.pdf</li>
<li>A Framework Combining 3D CNN And Transformer For Video-Based Behavior Recognition, https://www.researchgate.net/publication/394190589_A_Framework_Combining_3D_CNN_And_Transformer_For_Video-Based_Behavior_Recognition</li>
<li>From Pixels to Embeddings: How Video AI Represents Visual Data - Zilliz Learn, https://zilliz.com/learn/from-pixels-to-embeddings-how-video-ai-represents-visual-data</li>
<li>Cross-Architecture Self-Supervised Video Representation Learning - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Cross-Architecture_Self-Supervised_Video_Representation_Learning_CVPR_2022_paper.pdf</li>
<li>Late Temporal Modeling in 3D CNN Architectures with BERT for Action Recognition | Request PDF - ResearchGate, https://www.researchgate.net/publication/348902167_Late_Temporal_Modeling_in_3D_CNN_Architectures_with_BERT_for_Action_Recognition</li>
<li>Multi-view and multi-scale behavior recognition algorithm based on attention mechanism, https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2023.1276208/full</li>
<li>arxiv.org, https://arxiv.org/html/2407.06162v2</li>
<li>Transformer (deep learning architecture) - Wikipedia, https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)</li>
<li>LLM Transformer Model Visually Explained - Polo Club of Data Science, https://poloclub.github.io/transformer-explainer/</li>
<li>Transformers in Video Processing (Part 1) - Hugging Face …, https://huggingface.co/learn/computer-vision-course/unit7/video-processing/transformers-based-models</li>
<li>Introduction to ViT (Vision Transformers): Everything You Need to Know - Lightly, https://www.lightly.ai/blog/vision-transformers-vit</li>
<li>Video Vision Transformer (ViViT) - GeeksforGeeks, https://www.geeksforgeeks.org/computer-vision/video-vision-transformer-vivit/</li>
<li>TimeSformer: A new architecture for video understanding - Meta AI, https://ai.meta.com/blog/timesformer-a-new-architecture-for-video-understanding/</li>
<li>TimeSformer: Is Space-Time Attention All You Need for Video Understanding? - Medium, https://medium.com/lunit/timesformer-is-space-time-attention-all-you-need-for-video-understanding-5668e84162f4</li>
<li>Video Vision Transformer - Keras, https://keras.io/examples/vision/vivit/</li>
<li>Convolutional Neural Networks or Vision Transformers: Who Will Win the Race for Action Recognitions in Visual Data?, https://pmc.ncbi.nlm.nih.gov/articles/PMC9862752/</li>
<li>Video generation models as world simulators | OpenAI, https://openai.com/index/video-generation-models-as-world-simulators/</li>
<li>Video Classification Using SpatioTemporal Transformers | by Emmanuel Olateju - Medium, https://medium.com/rectlabs/video-classification-using-spatiotemporal-transformers-17bad6b1d440</li>
<li>TimeSFormer: Efficient and Effective Video Understanding Without Convolutions - Medium, https://medium.com/@kdk199604/timesformer-efficient-and-effective-video-understanding-without-convolutions-249ea6316851</li>
<li>Rethinking the Objectives of Vector-Quantized Tokenizers for Image Synthesis - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Gu_Rethinking_the_Objectives_of_Vector-Quantized_Tokenizers_for_Image_Synthesis_CVPR_2024_paper.pdf</li>
<li>How Foundation Models Perceive the World with VQ-VAEs | by Aditya NG - Medium, https://adityang5.medium.com/how-foundation-models-perceive-the-world-with-vq-vaes-0f86115c38fa</li>
<li>Image and Video Tokenization with Binary Spherical Quantization - arXiv, https://arxiv.org/html/2406.07548v1</li>
<li>VQ-VAE EXPLAINER: Learn Vector-Quantized Variational Autoencoders with Interactive Visualization, https://xnought.github.io/files/vq_vae_explainer.pdf</li>
<li>Unsupervised Machine Learning - Department of Computer Science, https://www.cs.upc.edu/~bejar/URL/material/URL_Lecture_Notes.pdf</li>
<li>PQ-VAE: Learning Hierarchical Discrete Representations with Progressive Quantization - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024W/GCV/papers/Huang_PQ-VAE_Learning_Hierarchical_Discrete_Representations_with_Progressive_Quantization_CVPRW_2024_paper.pdf</li>
<li>Color-Conditioned Abstract Image Generation with Diffusion Models - POLITECNICO DI TORINO, https://webthesis.biblio.polito.it/30088/1/tesi.pdf</li>
<li>[D] What are the pros and cons of using a VAE to provide a latent space for generative modelling? (especially for images or video) - Reddit, https://www.reddit.com/r/MachineLearning/comments/1g0jpzq/d_what_are_the_pros_and_cons_of_using_a_vae_to/</li>
<li>Language Model Beats Diffusion - Tokenizer is Key to Visual Generation - arXiv, https://arxiv.org/html/2310.05737v3</li>
<li>Factorized Visual Tokenization and Generation - arXiv, https://arxiv.org/html/2411.16681v1</li>
<li>IntelPVT and Opt-STViT: Advances in Vision Transformers for Object Detection, Classification and Video Recognition - The Aquila Digital Community, https://aquila.usm.edu/cgi/viewcontent.cgi?article=3343&amp;context=dissertations</li>
<li>arxiv.org, https://arxiv.org/html/2410.08368v1</li>
<li>Learning Adaptive and Temporally Causal Video Tokenization in a 1D Latent Space - arXiv, https://arxiv.org/html/2505.17011v1</li>
<li>Video token merging for long-form video understanding - Amazon Science, https://www.amazon.science/publications/video-token-merging-for-long-form-video-understanding</li>
<li>Video Token Merging for Long Video Understanding - OpenReview, <a href="https://openreview.net/forum?id=wduRaBDRBS&amp;referrer=%5Bthe+profile+of+Jue+Wang%5D(/profile?id%3D~Jue_Wang8)">https://openreview.net/forum?id=wduRaBDRBS&amp;referrer=%5Bthe%20profile%20of%20Jue%20Wang%5D(%2Fprofile%3Fid%3D~Jue_Wang8)</a></li>
<li>Efficient Video Transformers with Spatial-Temporal Token Selection, https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136950068.pdf</li>
<li>Token-Efficient Long Video Understanding for Multimodal LLMs - Research at NVIDIA, https://research.nvidia.com/labs/lpr/storm/</li>
<li>VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text, https://papers.neurips.cc/paper_files/paper/2021/file/cb3213ada48302953cb0f166464ab356-Paper.pdf</li>
<li>Real-time Online Video Detection with Temporal Smoothing Transformers, https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136940478.pdf</li>
<li>Optimal Topology of Vision Transformer for Real-Time Video Action Recognition in an End-To-End Cloud Solution - MDPI, https://www.mdpi.com/2504-4990/5/4/67</li>
<li>LLM Tokens for Deep Learning - BytePlus, https://www.byteplus.com/en/topic/411515</li>
<li>Low-Complexity Semantic Packet Aggregation for Token Communication via Lookahead Search - arXiv, https://arxiv.org/html/2506.19451</li>
<li>ResiTok: A Resilient Tokenization-Enabled Framework for Ultra-Low-Rate and Robust Image Transmission - arXiv, https://arxiv.org/html/2505.01870v1</li>
<li>What is LSTM? Introduction to Long Short-Term Memory - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2021/03/introduction-to-long-short-term-memory-lstm/</li>
<li>Optimization For Long-Term Dependencies | PDF | Artificial Neural Network | Machine Learning - Scribd, https://www.scribd.com/document/781445598/Optimization-for-Long-Term-Dependencies</li>
<li>Mastering Deep LSTM Models to Solving Industry Challenges - SkillDux, https://skilldux.com/blogs/guide-to-develop-lstm-models-for-industry-problem-solutions</li>
<li>Token-Efficient Long Video Understanding for Multimodal LLMs - arXiv, https://arxiv.org/html/2503.04130v1</li>
<li>Sora | Prompt Engineering Guide, https://www.promptingguide.ai/models/sora</li>
<li>Audio Language Models and Multimodal Architecture | by Deepak Babu P R - Medium, https://medium.com/@prdeepak.babu/audio-language-models-and-multimodal-architecture-1cdd90f46fac</li>
<li>C3LLM: Conditional Multimodal Content Generation Using Large Language Models - arXiv, https://arxiv.org/html/2405.16136v1</li>
<li>Visual Language Models (VLM): A Deep Dive into the Future of Multimodal AI - Medium, https://medium.com/@Er.Devanshu/visual-language-models-vlm-a-deep-dive-into-the-future-of-multimodal-ai-71fcd7c640d6</li>
<li>QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation - arXiv, https://arxiv.org/html/2502.05178v1</li>
<li>(PDF) WorldVLA: Towards Autoregressive Action World Model - ResearchGate, https://www.researchgate.net/publication/393066305_WorldVLA_Towards_Autoregressive_Action_World_Model</li>
<li>arXiv:2409.03272v1 [cs.CV] 5 Sep 2024, <a href="https://arxiv.org/pdf/2409.03272">https://arxiv.org/pdf/2409.03272?</a></li>
<li>Revolutionizing Robot Behavior: How Transformers Elevate Imitation Learning with Action Chunking, https://www.trossenrobotics.com/post/revolutionizing-robot-behavior-how-transformers-elevate-imitation-learning-with-action-chunking</li>
<li>Ch. 21 - Imitation Learning - Underactuated Robotics, https://underactuated.mit.edu/imitation.html</li>
<li>WorldVLA: Towards Autoregressive Action World Model - arXiv, https://arxiv.org/html/2506.21539v1</li>
<li>A Comprehensive Survey on Generative AI for Metaverse: Enabling Immersive Experience, https://www.napier.ac.uk/research-and-innovation/research-search/outputs/a-comprehensive-survey-on-generative-ai-for-metaverse-enabling-immersive-experience</li>
<li>Generative AI Models and Extended Reality (XR) Content for the Metaverse - Digital Commons@Lindenwood University, https://digitalcommons.lindenwood.edu/cgi/viewcontent.cgi?article=1452&amp;context=faculty-research-papers</li>
<li>Large Model Empowered Metaverse: State-of-the-Art, Challenges and Opportunities - arXiv, https://arxiv.org/html/2502.10397v1</li>
<li>Exploring Generative AI in the Metaverse - SoluLab, https://www.solulab.com/generative-ai-in-metaverse/</li>
<li>Fusion of Mixture of Experts and Generative Artificial Intelligence in Mobile Edge Metaverse, https://arxiv.org/html/2404.03321v1</li>
<li>Generative AI in the Metaverse: New Frontiers in Virtual Design and Interaction, https://www.frontiersin.org/research-topics/66424/generative-ai-in-the-metaverse-new-frontiers-in-virtual-design-and-interaction</li>
<li>arXiv:2412.07720v2 [cs.CV] 13 Mar 2025, <a href="https://arxiv.org/pdf/2412.07720">https://arxiv.org/pdf/2412.07720?</a></li>
<li>ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion Transformer - arXiv, https://arxiv.org/html/2412.07720v2</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>