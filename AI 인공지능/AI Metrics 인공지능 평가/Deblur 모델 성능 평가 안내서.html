<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Deblur 모델 성능 평가 안내서</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Deblur 모델 성능 평가 안내서</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">인공지능 평가지표 (AI evaluation metrics)</a> / <span>Deblur 모델 성능 평가 안내서</span></nav>
                </div>
            </header>
            <article>
                <h1>Deblur 모델 성능 평가 안내서</h1>
<h2>1. 서론</h2>
<p>이미지 Deblurring은 흐릿한(blurred) 이미지로부터 선명한 원본 이미지를 복원하는 컴퓨터 비전의 핵심 분야이다.1 이러한 흐림 현상은 카메라 흔들림, 피사체의 빠른 움직임, 혹은 초점 이탈과 같은 다양한 원인으로 발생하며 4, 주어진 정보만으로는 유일한 정답을 찾기 어려운 ’잘못 설정된 역문제(ill-posed inverse problem)’로 정의된다.6 이로 인해 Deblurring은 기술적으로 매우 도전적인 과제로 남아있다.</p>
<p>과거에는 Wiener 필터나 Richardson-Lucy 알고리즘과 같은 전통적인 신호 처리 기법이 주를 이루었으나, 딥러닝 기술의 발전은 이 분야에 혁신을 가져왔다.7 합성곱 신경망(CNN), 생성적 적대 신경망(GAN), 트랜스포머(Transformer)를 거쳐 최근에는 확산 모델(Diffusion Model)에 이르기까지, 딥러닝 기반 접근법들은 기존의 한계를 뛰어넘는 괄목할 만한 성능 향상을 보여주었다.1</p>
<p>이러한 모델들의 급격한 발전은 역설적으로 성능 평가 방법론의 중요성을 부각시켰다. 초창기 모델들은 수학적으로 최적화된 단일 해를 찾는 것을 목표로 했기에, 픽셀 단위의 오차를 측정하는 지표만으로도 성능을 가늠할 수 있었다. 하지만 GAN이나 확산 모델과 같은 현대의 생성 모델들은 인간이 보기에 그럴듯한, 즉 ’지각적 타당성(perceptual plausibility)’이 높은 결과물을 생성하는 데 초점을 맞춘다.4 이러한 결과물은 픽셀 단위로 원본과 정확히 일치하지 않더라도 시각적으로 우수할 수 있으며, 이로 인해 기존 평가 지표와 실제 체감 성능 사이에 괴리가 발생한다.9 따라서 오늘날의 Deblur 모델 평가는 단순히 하나의 수치에 의존하는 것을 넘어, 정량적, 정성적, 그리고 사용된 데이터셋에 대한 다각적인 분석을 요구하는 복합적인 과정이 되었다.8 본 안내서는 이러한 복합적인 평가 체계를 구축하고 모델의 성능을 신뢰성 있게 분석하기 위한 종합적인 지침을 제공하는 것을 목표로 한다.</p>
<h2>2.  정량적 평가 지표: Full-Reference (FR) 방식</h2>
<p>Full-Reference(FR) 방식은 평가 과정에서 왜곡되지 않은 원본, 즉 Ground-Truth(GT) 이미지를 참조하여 복원된 이미지의 품질을 측정하는 방법론이다.12 이는 모델의 복원 정확도를 직접적으로 수치화할 수 있어 Deblurring 성능 평가의 가장 기본적이고 널리 사용되는 기준으로 자리 잡고 있다.</p>
<h3>2.1  Peak Signal-to-Noise Ratio (PSNR)</h3>
<p>PSNR은 신호(원본 이미지)가 가질 수 있는 최대 전력과 이를 왜곡시키는 노이즈(오차)의 전력 간의 비율을 데시벨(dB) 단위로 나타내는 지표이다.14 이 지표는 두 이미지 간의 평균 제곱 오차(Mean Squared Error, MSE)를 기반으로 계산되며, MSE가 0에 가까워질수록(즉, 두 이미지가 동일할수록) PSNR 값은 무한대에 수렴한다.9 따라서 높은 PSNR 값은 일반적으로 더 높은 품질의 복원을 의미한다.9</p>
<p>수학적 정의는 다음과 같다. 원본 이미지 <span class="math math-inline">f</span>와 복원 이미지 <span class="math math-inline">g</span>가 주어졌을 때, MSE와 PSNR은 아래와 같이 계산된다.</p>
<p><span class="math math-display">
MSE(f, g) = \frac{1}{MN} \sum_{i=1}^{M} \sum_{j=1}^{N} (f_{ij} - g_{ij})^2
</span></p>
<p><span class="math math-display">
PSNR(f, g) = 10 \log_{10} \left( \frac{MAX_I^2}{MSE(f, g)} \right)
</span></p>
<p>여기서 <span class="math math-inline">M</span>과 <span class="math math-inline">N</span>은 이미지의 가로 및 세로 픽셀 수이며, <span class="math math-inline">MAX_I</span>는 해당 이미지의 최대 픽셀 값이다(예: 8비트 이미지의 경우 255).9</p>
<p>PSNR의 가장 큰 장점은 개념이 직관적이고 계산이 매우 빠르다는 점이다.10 또한, 오랫동안 이미지 복원 분야의 표준 지표로 사용되어 왔기 때문에 과거 연구들과의 성능을 공정하게 비교하는 데 용이하다.10 하지만 PSNR은 픽셀 값의 절대적인 차이에만 의존한다는 명백한 한계를 가진다. 이로 인해 인간의 시각 시스템(Human Visual System, HVS)이 인지하는 구조적, 지각적 품질 저하를 제대로 반영하지 못한다.9 예를 들어, 이미지 전체에 미세한 밝기 변화가 생긴 경우와 특정 영역에 심각한 왜곡이 발생한 경우, 두 이미지의 시각적 품질은 현저히 다르지만 동일한 PSNR 값을 가질 수 있다.10</p>
<h3>2.2  Structural Similarity Index Measure (SSIM)</h3>
<p>SSIM은 PSNR의 한계를 극복하기 위해 제안된 지표로, 픽셀 단위의 절대 오차가 아닌 이미지의 구조적 정보 변화를 통해 품질 저하를 평가한다.18 이는 인간의 시각 시스템이 이미지의 구조적 정보 추출에 매우 민감하다는 관찰에 기반하며, 두 이미지 간의 휘도(Luminance), 대비(Contrast), 구조(Structure) 세 가지 요소를 비교하여 최종 유사도를 산출한다.9 SSIM 값은 -1과 1 사이의 값을 가지며, 1에 가까울수록 두 이미지가 시각적으로 더 유사함을 의미한다.18</p>
<p>SSIM의 세 가지 구성 요소와 최종 수식은 다음과 같다.</p>
<ul>
<li>
<p>휘도 비교: <span class="math math-inline">l(f, g) = \frac{2\mu_f \mu_g + C_1}{\mu_f^2 + \mu_g^2 + C_1}</span></p>
</li>
<li>
<p>대비 비교: <span class="math math-inline">c(f, g) = \frac{2\sigma_f \sigma_g + C_2}{\sigma_f^2 + \sigma_g^2 + C_2}</span></p>
</li>
<li>
<p>구조 비교: <span class="math math-inline">s(f, g) = \frac{\sigma_{fg} + C_3}{\sigma_f \sigma_g + C_3}</span></p>
</li>
</ul>
<p>최종 SSIM은 이 세 요소의 가중 곱으로 계산된다.</p>
<p><span class="math math-display">
SSIM(f, g) = [l(f, g)]^\alpha \cdot [c(f, g)]^\beta \cdot [s(f, g)]^\gamma
</span><br />
여기서 <span class="math math-inline">\mu</span>는 평균, <span class="math math-inline">\sigma</span>는 표준편차, <span class="math math-inline">\sigma_{fg}</span>는 공분산을 나타내며, <span class="math math-inline">C_1, C_2, C_3</span>는 분모가 0이 되는 것을 방지하는 안정화 상수이다.9 일반적으로 가중치 <span class="math math-inline">\alpha, \beta, \gamma</span>는 1로 설정된다.</p>
<p>SSIM은 PSNR보다 인간의 시각적 인지와 훨씬 높은 상관관계를 보이며, 특히 블러와 같이 이미지의 구조를 손상시키는 왜곡을 포착하는 데 효과적이다.10 그러나 SSIM 역시 단일 스케일에서만 계산되므로 이미지의 다양한 해상도에서의 구조적 특징을 모두 반영하기는 어렵다는 한계가 있다. 이를 보완하기 위해 여러 스케일에서 SSIM을 계산하여 평균을 내는 Multi-Scale SSIM (MS-SSIM)이나, 이미지의 그래디언트에 SSIM을 적용하여 블러 평가에 특화된 G-SSIM과 같은 변형 모델들이 제안되었다.15</p>
<p>PSNR과 SSIM은 각각의 단점에도 불구하고 대부분의 연구에서 함께 보고된다.20 이는 두 지표가 이미지 품질의 근본적으로 다른 측면을 측정하기 때문이다. PSNR은 픽셀 수준의 ’충실도(fidelity)’를, SSIM은 ’구조적 무결성(structural integrity)’을 평가한다. 예를 들어, 어떤 모델이 구조는 완벽하게 복원했지만 전체적으로 약간의 밝기 편향을 가진 결과물을 생성했다고 가정해보자. 이 경우, SSIM 점수는 높게 나오지만 모든 픽셀에서 오차가 발생하므로 PSNR 점수는 낮게 나올 것이다.10 반대로, 텍스처와 같은 미세 구조는 복원하지 못하고 밋밋한 표면으로 대체했지만 평균적인 색상과 밝기는 원본과 유사하게 맞춘 경우, PSNR은 비교적 높게 나올 수 있지만 SSIM 점수는 낮을 것이다. 이처럼 두 지표를 함께 제시함으로써, 모델이 신호 복원과 구조 보존 중 어느 쪽에서 강점 또는 약점을 보이는지 더 균형 잡힌 시각으로 파악할 수 있다.</p>
<h2>3.  정량적 평가 지표: 인간의 지각을 모사하는 방식</h2>
<p>전통적인 FR 지표들이 가진 수학적 한계를 극복하고 인간의 복잡한 시각 인지 과정을 더 정교하게 모사하려는 시도들이 이루어졌다. 이 중 가장 성공적이고 널리 채택된 방식은 딥러닝 네트워크의 내부 특징(feature)을 활용하는 것이다.</p>
<h3>3.1  Learned Perceptual Image Patch Similarity (LPIPS)</h3>
<p>LPIPS는 두 이미지 간의 지각적 유사성을 판단하기 위해 이미지 분류와 같은 고수준(high-level) 작업에 대해 사전 훈련된(pre-trained) 딥러닝 네트워크(예: AlexNet, VGG)를 일종의 ’특징 추출기’로 활용한다.22 작동 원리는 다음과 같다. 먼저, 비교하려는 두 이미지(원본과 복원된 이미지)를 네트워크에 각각 입력한다. 그 후, 네트워크의 여러 계층(layer)에서 활성화된 특징 맵(feature map)을 추출하고, 이들 간의 가중치가 적용된 L2 거리를 계산하여 최종 유사도 점수를 산출한다.22 이 점수가 낮을수록 두 이미지가 인간이 보기에 더 유사하다는 것을 의미한다.25</p>
<p>LPIPS의 성공은 매우 흥미로운 발견에 기반한다. 고양이, 자동차 등 객체를 인식하도록 훈련된 네트워크가 이미지의 의미론적(semantic) 정보뿐만 아니라, 인간의 지각과 밀접하게 관련된 텍스처, 모양, 엣지와 같은 저수준(low-level) 특징들을 이미 내부적으로 학습했다는 것이다.26 즉, 지각적 유사성은 특정 작업을 위해 별도로 학습시켜야 하는 능력이 아니라, 세상을 이해하도록 훈련된 심층 시각 표현(deep visual representations) 전반에 걸쳐 자연스럽게 나타나는 ’창발적 속성(emergent property)’이라는 점이다.29 객체를 정확히 인식하기 위해 네트워크는 조명 변화나 약간의 자세 변화와 같은 비본질적인 변화에는 둔감해지고, 객체의 구조를 정의하는 본질적인 특징에는 민감해지도록 학습한다. 이러한 ’구조에 대한 민감성’이 바로 우수한 지각적 평가 지표가 갖춰야 할 핵심 요건이다.</p>
<p>이러한 원리 덕분에 LPIPS는 PSNR이나 SSIM 등 기존의 모든 지표보다 인간의 실제 유사성 판단과 훨씬 높은 상관관계를 보인다.22 특히 GAN과 같이 사실적이지만 픽셀 단위로는 원본과 다를 수 있는 생성 모델의 결과물을 평가하는 데 매우 효과적이다. 나아가, 모델 훈련 과정에서 ’Perceptual Loss’로 활용되어 네트워크가 보다 시각적으로 만족스러운 이미지를 생성하도록 유도하는 데 널리 사용된다.22 다만, LPIPS는 인간이 인지하기 어려운 1픽셀 정도의 미세한 공간적 이동(spatial shift)이나 정렬 오류에 매우 민감하게 반응하여 점수가 크게 변동될 수 있다는 단점이 있으며 22, PSNR이나 SSIM에 비해 계산 비용이 높다.</p>
<h2>4.  정량적 평가 지표: No-Reference (NR) 방식</h2>
<h3>4.1  NR-IQA (Blind IQA)의 개념과 필요성</h3>
<p>실제 응용 환경에서는 평가의 기준이 되는 깨끗한 원본 이미지가 존재하지 않는 경우가 대부분이다.12 예를 들어, 사용자가 스마트폰으로 촬영한 흔들린 사진을 복원할 때, 우리는 ‘흔들리지 않았더라면 찍혔을’ 이상적인 사진을 가지고 있지 않다. 이러한 상황에서 모델의 성능을 평가하기 위해 개발된 기술이 바로 NR-IQA(No-Reference Image Quality Assessment), 또는 BIQA(Blind IQA)이다.13 이 기술은 참조 이미지 없이 주어진 이미지 자체의 통계적 특성이나 사전 학습된 모델을 통해 품질을 예측하며, Deblurring 알고리즘의 실용성을 검증하는 데 필수적이다.</p>
<h3>4.2  주요 NR Blur 평가 지표</h3>
<p>NR 지표는 크게 두 가지 접근 방식으로 나뉜다. 하나는 블러, 노이즈, 압축 아티팩트 등 특정 왜곡(distortion-specific)을 탐지하도록 특화된 것이고, 다른 하나는 다양한 유형의 왜곡을 포괄적으로 평가하는 범용(general-purpose) 모델이다.19</p>
<ul>
<li>
<p><strong>자연 영상 통계(Natural Image Statistics, NIS) 기반 지표:</strong> 이 접근법은 왜곡되지 않은 자연 영상이 특정 통계적 분포를 따른다는 가정에서 출발한다.</p>
</li>
<li>
<p><strong>BRISQUE (Blind/Referenceless Image Spatial Quality Evaluator):</strong> 평가할 이미지의 통계적 특징이 학습된 자연 영상의 분포에서 얼마나 벗어나는지를 측정하여 품질 점수를 계산한다. 이 모델은 알려진 왜곡이 있는 이미지 데이터베이스로 훈련되므로, 훈련에 사용된 유형의 왜곡을 평가하는 데는 효과적이지만 새로운 유형의 왜곡에는 취약할 수 있다.13</p>
</li>
<li>
<p><strong>NIQE (Natural Image Quality Evaluator):</strong> BRISQUE와 유사하게 NIS를 활용하지만, 인간의 주관적 품질 점수 없이 깨끗한 이미지 데이터베이스만으로 훈련된다(opinion-unaware). 따라서 특정 왜곡에 편향되지 않아 임의의 왜곡 유형에 대해 품질 측정이 가능하며 범용성이 더 높다.13</p>
</li>
<li>
<p><strong>Blur 특화 지표:</strong> 블러 현상의 물리적 특성을 직접적으로 이용한다.</p>
</li>
<li>
<p><strong>Edge-based Methods:</strong> 이미지의 선명도는 객체의 경계선, 즉 엣지(edge)의 날카로움과 밀접한 관련이 있다는 점에 착안한다. 블러가 발생하면 엣지가 넓고 완만해지므로, 이미지 내 엣지들의 평균적인 폭을 측정하여 블러의 정도를 추정하는 방식이다.35</p>
</li>
<li>
<p><strong>Re-blur 이론 기반 (NSSIM):</strong> 입력된 블러 이미지를 가우시안 필터 등을 이용해 의도적으로 한 번 더 블러링(re-blur)하여 ’유사 참조 이미지(pseudo-reference image)’를 생성한다. 그 후, 원본 블러 이미지와 re-blur된 이미지 간의 유사도(예: SSIM)를 측정하여 품질을 평가한다.19 이 아이디어는 이미 블러가 심한 이미지일수록 추가적인 블러링에 의한 시각적 변화가 적을 것이라는 직관적인 가정에 기반한다. 이 방식은 별도의 훈련 과정 없이 즉시 품질 평가가 가능하다는 큰 장점이 있다.19</p>
</li>
</ul>
<p>NR-IQA 분야에는 모든 상황에서 절대적으로 우수한 단일 지표는 존재하지 않는다. 이는 블러 현상 자체가 운동 블러, 초점 이탈 블러 등 다양한 원인에 의해 발생하며 각각 다른 통계적 특성을 가지기 때문이다.3 엣지 폭 기반 지표는 운동 블러 평가에는 효과적일 수 있지만, 이미지 전반의 고주파 디테일이 사라지는 초점 이탈 블러에는 덜 민감할 수 있다.36 BRISQUE와 같은 학습 기반 모델은 훈련 데이터에 포함되지 않은 새로운 유형의 아티팩트를 생성하는 모델을 평가할 때 신뢰도가 떨어진다.13 따라서 모델의 성능을 강건하게 평가하기 위해서는, 평가하려는 블러의 유형과 모델이 생성할 수 있는 잠재적 아티팩트를 고려하여 범용 지표와 블러 특화 지표를 함께 사용하는 것이 바람직하다.</p>
<h2>5.  정성적 평가</h2>
<h3>5.1  정량적 평가의 맹점과 정성적 평가의 중요성</h3>
<p>PSNR, SSIM, LPIPS와 같은 정량적 지표는 객관적인 수치를 제공하여 모델 간의 성능을 비교하는 데 유용하지만, 모델이 생성한 결과물의 시각적 완성도나 자연스러움을 완벽하게 대변하지는 못한다.11 특히 GAN 기반 모델들은 인간이 보기에 매우 사실적인 이미지를 생성하면서도, 픽셀 단위의 정확도를 중시하는 PSNR이나 SSIM 점수는 오히려 낮게 나오는 경우가 빈번하다.3 따라서 전문가의 면밀한 시각적 검토를 통한 정성적 평가는 모델의 실제 성능과 잠재적인 문제점을 파악하는 데 필수적인 과정이다.3</p>
<h3>5.2  주요 시각적 아티팩트(Artifact) 분석</h3>
<p>Deblurring 모델의 결과물을 정성적으로 평가할 때는 단순히 ’선명해졌는가’를 넘어, 복원 과정에서 발생하는 다양한 부작용, 즉 아티팩트를 체계적으로 분석해야 한다.</p>
<ul>
<li>
<p><strong>링잉 (Ringing):</strong> 이미지 내의 강한 엣지(예: 건물 윤곽선) 주변에 물결이나 후광처럼 나타나는 현상. 특히 전통적인 디컨볼루션(deconvolution) 기반 방법에서 자주 발생하며, 시각적으로 매우 거슬리는 아티팩트 중 하나이다.31</p>
</li>
<li>
<p><strong>노이즈 증폭 (Noise Amplification):</strong> Deblurring은 이미지의 고주파 성분을 복원하는 과정인데, 이 과정에서 이미지에 원래 존재하던 미세한 노이즈가 과도하게 증폭되어 결과물이 지저분해 보이는 현상이다.4</p>
</li>
<li>
<p><strong>잔존 블러 (Residual Blur):</strong> 블러가 완전히 제거되지 않고 일부가 희미하게 남아있는 상태. 모델의 복원 능력이 부족할 때 나타난다.31</p>
</li>
<li>
<p><strong>비현실적 텍스처 (Unrealistic Textures):</strong> 모델이 블러로 인해 손실된 미세한 디테일(예: 피부 질감, 옷감 무늬)을 복원하는 과정에서, 실제와 다른 부자연스럽거나 과장된 텍스처를 생성하는 경우. 특히 GAN 기반 모델에서 발생할 수 있다.</p>
</li>
<li>
<p><strong>구조적 왜곡 (Structural Distortion):</strong> 객체의 원래 형태나 직선이어야 할 부분이 휘어지는 등 이미지의 전반적인 구조가 왜곡되는 심각한 오류이다.</p>
</li>
</ul>
<h3>5.3  체계적인 정성적 평가 방법론</h3>
<p>정성적 평가는 주관적 판단에 의존하지만, 그 과정을 체계화하여 신뢰도를 높일 수 있다.</p>
<ul>
<li>
<p><strong>결과물 비교 시각화:</strong> 원본 블러 이미지, Ground-Truth, 그리고 비교하려는 여러 모델의 결과물을 나란히 배치하여 직접 비교하는 것이 가장 기본적인 방법이다.37 특히 텍스처 복원, 엣지 선명도, 아티팩트 유무 등을 세밀하게 관찰하기 위해 특정 영역을 크게 확대(zoom-in)하여 비교하는 것이 매우 중요하다.</p>
</li>
<li>
<p><strong>사용자 연구 (User Study):</strong> 다수의 피험자를 모집하여 어떤 모델의 결과물이 시각적으로 가장 우수한지 설문하는 방식이다.11 이는 가장 인간 중심적인 평가 방법으로, Amazon Mechanical Turk와 같은 크라우드소싱 플랫폼을 통해 대규모로 진행할 수 있다.11 일반적으로 두 개의 결과물을 나란히 보여주고 더 선호하는 쪽을 선택하게 하는 ‘쌍대 비교(paired comparison)’ 방식이 널리 사용되며, 수집된 선호도 데이터를 브래들리-테리(Bradley-Terry) 모델과 같은 통계 모델로 분석하여 전체 모델의 순위를 객관적으로 도출할 수 있다.11</p>
</li>
</ul>
<p>정성적 평가는 단순히 수치를 보완하는 것을 넘어, 연구의 방향을 결정하는 중요한 역할을 한다. 예를 들어, 한 사용자 연구에서는 피험자들이 노이즈나 약간의 잔존 블러보다 링잉 아티팩트를 훨씬 더 부정적으로 인식한다는 사실이 밝혀졌다.31 이는 모든 아티팩트가 인간의 지각에 동일한 영향을 미치지 않으며, ‘링잉 &gt; 노이즈 &gt; 잔존 블러’ 순으로 시각적 불쾌감이 크다는 ’지각적 가중치’가 존재함을 시사한다. 이러한 발견은 모델 개발자에게 약간의 선명도를 희생하더라도 링잉 아티팩트를 우선적으로 제거하는 방향으로 모델이나 손실 함수를 설계해야 한다는 구체적인 지침을 제공한다. 이처럼 정성적 평가는 모델의 숨겨진 단점을 발견하고 차세대 연구의 목표를 설정하는 핵심적인 과정이다.</p>
<h2>6.  평가 데이터셋의 이해와 선택</h2>
<p>Deblurring 모델의 성능은 어떤 데이터셋으로 훈련하고 평가했는지에 따라 크게 달라진다. 따라서 평가에 사용된 데이터셋의 특성을 정확히 이해하는 것은 모델의 성능을 올바르게 해석하는 데 매우 중요하다.</p>
<h3>6.1  합성 데이터셋(Synthetic Datasets) vs. 실제 데이터셋(Real-World Datasets)</h3>
<p>Deblurring 데이터셋은 생성 방식에 따라 크게 합성 데이터셋과 실제 데이터셋으로 나뉜다.</p>
<ul>
<li>
<p><strong>합성 데이터셋:</strong> 고속 카메라로 촬영한 연속된 선명한 비디오 프레임들을 수학적으로 평균 내어 블러 이미지를 인위적으로 생성하는 방식이다.40 이 방식의 가장 큰 장점은 Ground-Truth 이미지와 생성된 블러 이미지 간의 완벽한 픽셀 단위 정렬을 보장할 수 있다는 점이다.</p>
</li>
<li>
<p><strong>실제 데이터셋:</strong> 빔 스플리터(beam splitter)와 같은 특수 광학 장비를 사용하여 동일한 장면을 긴 셔터 속도(블러 이미지)와 매우 짧은 셔터 속도(선명한 이미지)로 동시에 촬영하여 데이터 쌍을 얻는다.20 이는 실제 카메라와 렌즈에서 발생하는 복합적인 왜곡을 그대로 담아낼 수 있다는 장점이 있다.</p>
</li>
</ul>
<p>이 두 방식의 차이로 인해 ’Sim-to-Real Gap’이라는 중요한 문제가 발생한다. 합성 데이터셋으로만 훈련된 모델은 실제 환경에서 촬영된 블러 이미지를 처리할 때 성능이 급격히 저하되는 경향을 보인다.37 이는 합성 블러가 실제 카메라의 미세한 흔들림, 렌즈의 광학적 왜곡, 저조도 환경에서의 센서 노이즈 등 현실 세계의 복합적인 물리적 요소를 완벽하게 모사하지 못하기 때문이다.40 따라서 모델의 실용성을 검증하기 위해서는 반드시 실제 데이터셋에서의 평가가 병행되어야 한다.</p>
<h3>6.2  주요 Deblurring 데이터셋 심층 비교</h3>
<p>모델을 평가하고 비교할 때 표준으로 사용되는 몇 가지 대표적인 데이터셋이 있다.</p>
<ul>
<li>
<p><strong>GoPro:</strong> 동적 장면 Deblurring을 위해 제안된 최초의 대규모 합성 데이터셋으로, 수많은 연구에서 훈련 데이터로 널리 사용되어 왔다.42 고속 카메라(240fps)로 촬영한 비디오 프레임을 평균하여 블러를 생성한다.21 그러나 주로 조명이 좋은 밝은 환경에서 촬영되었고, 블러 패턴이 다소 인위적이고 비현실적이라는 비판을 받기도 한다.37</p>
</li>
<li>
<p><strong>REDS (REalistic and Dynamic Scenes):</strong> 비디오 Deblurring 및 Super-Resolution을 위해 제안된 대규모 합성 데이터셋이다.41 GoPro보다 정교한 프레임 보간(interpolation) 기술을 도입하여 프레임 사이의 움직임을 더 부드럽게 연결함으로써 더 현실적인 모션 블러를 생성하고자 했다.41</p>
</li>
<li>
<p><strong>RealBlur:</strong> ’Sim-to-Real Gap’을 극복하기 위해 특수 제작된 광학 시스템으로 수집된 실제 데이터셋이다.20 특히 야간 거리나 실내와 같은 저조도 환경에서의 카메라 흔들림으로 인한 블러를 주로 다루어, 실제 사용 환경에서 마주할 수 있는 문제 상황을 매우 잘 반영한다.40 이 데이터셋은 카메라 내부 이미지 처리 파이프라인(ISP)을 거친 JPEG 이미지로 구성된</p>
</li>
</ul>
<p><strong>RealBlur-J</strong>와, 가공되지 않은 RAW 센서 데이터로부터 직접 처리된 <strong>RealBlur-R</strong> 두 가지 서브셋으로 나뉘어 제공된다.20 오늘날 모델의 일반화 성능을 평가하는 가장 중요한 벤치마크 중 하나로 인정받고 있다.</p>
<p>훈련 및 평가에 사용되는 데이터셋은 모델이 해결하도록 학습하는 ‘문제’ 자체를 암묵적으로 정의한다. 딥러닝 모델은 본질적으로 훈련 데이터의 분포를 학습하는 강력한 함수 근사기이다.1 GoPro 데이터셋으로 훈련된 모델은 ’밝은 대낮의 빠른 움직임으로 인한 블러’를 제거하는 문제에 대한 사전 지식(prior)을 학습한다. 반면, RealBlur 데이터셋은 ’어두운 환경에서의 손 떨림으로 인한 블러와 노이즈’라는 전혀 다른 문제 분포를 담고 있다.40 GoPro로 훈련된 모델을 RealBlur로 테스트했을 때 성능이 저하되는 것은, 모델이 훈련 데이터 분포를 벗어난(out-of-distribution) 샘플을 마주했기 때문이다. 이는 ’Deblurring’이 단일 문제가 아님을 명확히 보여준다. 따라서 어떤 모델이 ’최고 성능(SOTA)’이라고 주장할 때는, 어떤 데이터셋에서의 성능인지를 명시하는 것이 필수적이다.</p>
<table><thead><tr><th>데이터셋 (Dataset)</th><th>유형 (Type)</th><th>블러 생성 방식 (Blur Generation)</th><th>규모 (Scale)</th><th>해상도 (Resolution)</th><th>주요 특징 및 용도 (Key Features &amp; Use Case)</th></tr></thead><tbody>
<tr><td><strong>GoPro</strong></td><td>합성 (Synthetic)</td><td>고속 비디오 프레임 평균</td><td>~3,214 쌍 / 33 장면</td><td>1280×720</td><td>동적 장면에 대한 대규모 훈련 데이터, 밝은 환경 위주, 비현실적 블러 패턴 가능성</td></tr>
<tr><td><strong>REDS</strong></td><td>합성 (Synthetic)</td><td>고속 비디오 프레임 보간 및 평균</td><td>300개 시퀀스 (30,000 쌍)</td><td>1280×720</td><td>비디오 Deblurring/SR 벤치마크, GoPro 대비 정교한 블러 생성</td></tr>
<tr><td><strong>RealBlur</strong></td><td>실제 (Real-World)</td><td>빔 스플리터를 이용한 동시 촬영</td><td>~4,700 쌍 / 232 장면</td><td>가변적 (e.g., 1280×720)</td><td>실제 카메라 흔들림, 저조도 환경 반영, 모델의 일반화 성능(Sim-to-Real) 평가에 필수적</td></tr>
</tbody></table>
<h2>7.  종합적인 Deblur 모델 성능 평가 파이프라인</h2>
<p>신뢰도 높은 모델 평가를 위해서는 단일 지표나 데이터셋에 의존하지 않고, 연구 개발의 각 단계에 맞는 체계적인 프로토콜을 수립해야 한다. 이러한 파이프라인은 연구의 진행 과정을 자연스럽게 반영하며, 각 단계는 모델의 능력에 대해 점점 더 복잡하고 심층적인 질문에 답하는 과정이다.</p>
<h3>7.1  단계별 평가 프로토콜 제안</h3>
<ul>
<li>
<p><strong>1단계: 개발 및 디버깅 (Development &amp; Debugging)</strong></p>
</li>
<li>
<p><strong>목표:</strong> “모델이 무언가 학습하고 있는가?“라는 기본적인 질문에 답하며, 빠른 피드백을 통해 개발 초기 단계의 효율성을 극대화한다.</p>
</li>
<li>
<p><strong>방법:</strong> 모델을 훈련하고 검증하는 과정에서 PSNR, SSIM과 같이 계산이 매우 빠른 FR 지표를 실시간으로 모니터링한다.10 이를 통해 모델이 정상적으로 수렴하고 있는지, 주요 하이퍼파라미터 변경이 성능에 긍정적 혹은 부정적 영향을 미치는지 신속하게 확인할 수 있다. 이 단계에서는 주로 대규모의 잘 정렬된 합성 데이터셋(예: GoPro)을 활용하여 안정적인 학습 환경을 조성한다.21</p>
</li>
<li>
<p><strong>2단계: 심층 성능 검증 (In-depth Performance Validation)</strong></p>
</li>
<li>
<p><strong>목표:</strong> “모델이 ‘올바른’ 방향으로 학습하고 있으며, 학습한 내용을 새로운 환경에 일반화할 수 있는가?“를 검증한다.</p>
</li>
<li>
<p><strong>방법:</strong> 학습이 안정화된 모델에 대해 LPIPS를 추가로 측정하여, 픽셀 정확도를 넘어선 지각적 품질을 평가한다.22 또한, 모델의 강건성을 확인하기 위해 훈련에 사용되지 않은 다른 종류의 데이터셋에 대한 교차 검증을 수행한다. 예를 들어, GoPro 데이터셋으로 훈련한 모델을 RealBlur 데이터셋으로 테스트하여 ’Sim-to-Real Gap’의 크기를 정량적으로 측정하고 일반화 성능을 분석한다.37 이 단계에서 모델의 실제적인 한계와 강점이 드러나기 시작한다.</p>
</li>
<li>
<p><strong>3단계: 최종 벤치마킹 및 보고 (Final Benchmarking &amp; Reporting)</strong></p>
</li>
<li>
<p><strong>목표:</strong> “개발된 모델이 기존 최고 성능(SOTA) 모델들과 비교하여 어떤 점에서 우수하며, 그 기여는 무엇인가?“를 공정하고 종합적으로 입증한다.</p>
</li>
<li>
<p><strong>방법:</strong> GoPro, REDS, RealBlur-J/R 등 학계에서 인정된 모든 표준 벤치마크 데이터셋에 대해 PSNR, SSIM, LPIPS를 모두 측정하여 결과를 표로 명확하게 정리한다.20 각 데이터셋에서 얻은 대표적인 성공 사례와 실패 사례에 대한 심층적인 정성적 분석을 수행하고, 확대된 이미지 패치와 같은 시각 자료를 통해 주장을 뒷받침한다.37 Ground-Truth가 없는 실제 이미지에 대한 적용 결과를 보여주기 위해 NR-IQA 지표를 활용하거나, 신뢰도를 더욱 높이기 위해 사용자 연구를 설계하고 수행할 수도 있다.11</p>
</li>
</ul>
<table><thead><tr><th>지표 (Metric)</th><th>유형 (Type)</th><th>측정 대상 (Measures)</th><th>장점 (Advantages)</th><th>한계점 (Limitations)</th></tr></thead><tbody>
<tr><td><strong>PSNR</strong></td><td>Full-Reference (FR)</td><td>픽셀 단위의 절대 오차 (MSE 기반)</td><td>계산이 빠르고 간단함, 광범위하게 사용되어 비교 용이</td><td>인간의 시각적 인지와 상관관계 낮음, 구조적 왜곡 미반영</td></tr>
<tr><td><strong>SSIM</strong></td><td>Full-Reference (FR)</td><td>휘도, 대비, 구조의 유사성</td><td>PSNR보다 시각적 인지와 상관관계 높음, 구조적 정보 포착에 유리</td><td>단일 스케일에서만 작동, 미세한 텍스처 변화에 둔감할 수 있음</td></tr>
<tr><td><strong>LPIPS</strong></td><td>Full-Reference (FR), Perceptual</td><td>딥러닝 특징 공간에서의 거리</td><td>인간의 지각적 유사성 판단과 매우 높은 상관관계, 생성 모델 평가에 적합</td><td>계산 비용 높음, 미세한 공간적 정렬 오류에 민감</td></tr>
<tr><td><strong>BRISQUE/NIQE</strong></td><td>No-Reference (NR)</td><td>자연 영상 통계로부터의 편차</td><td>Ground-Truth 없이 품질 평가 가능, 실제 환경 적용에 필수적</td><td>특정 왜곡 유형에 편향될 수 있음 (BRISQUE), FR 지표보다 정확도 낮음</td></tr>
</tbody></table>
<h3>7.2  구현 참고: Python 라이브러리 활용</h3>
<p>이러한 평가 지표들은 널리 사용되는 Python 라이브러리를 통해 쉽게 구현할 수 있다.</p>
<ul>
<li>
<p><strong>PSNR 및 SSIM:</strong> <code>scikit-image</code>, <code>OpenCV</code> 라이브러리에서 관련 함수를 제공한다. 또한, <code>PyTorch</code> 환경에서는 <code>torchmetrics</code>나 <code>piq</code>와 같은 라이브러리를 사용하면 GPU 가속을 통해 효율적으로 계산할 수 있다.16</p>
</li>
<li>
<p><strong>LPIPS:</strong> 공식적으로 제공되는 <code>lpips</code> 패키지를 설치하여 간단하게 사용할 수 있다. 이 패키지는 사전 훈련된 AlexNet, VGG 등의 네트워크를 내장하고 있어 몇 줄의 코드만으로 LPIPS 점수를 계산할 수 있다.23</p>
</li>
</ul>
<h2>8. 결론</h2>
<p>본 안내서는 인공지능 Deblur 모델의 성능 평가가 단일 지표에 의존하는 단순한 과정이 아니라, 다차원적인 분석을 요구하는 복합적인 작업임을 강조했다. 성공적인 평가는 PSNR, SSIM과 같은 전통적인 FR 지표, LPIPS와 같은 인간의 지각을 모사하는 지표, Ground-Truth가 없는 실제 상황을 위한 NR 지표, 그리고 시각적 아티팩트를 분석하는 체계적인 정성적 평가를 모두 포함해야 한다. 또한, GoPro, REDS와 같은 합성 데이터셋과 RealBlur와 같은 실제 데이터셋의 본질적인 차이, 즉 ‘Sim-to-Real’ 격차의 중요성을 이해하고, 이를 바탕으로 모델의 일반화 성능을 검증하는 것이 필수적이다. 이러한 요소들을 종합하여 제시된 단계별 평가 파이프라인은 연구 개발 과정에서 모델의 성능을 신뢰성 있게 측정하고 보고하기 위한 실질적인 로드맵을 제공한다.</p>
<p>궁극적으로 Deblur 모델 평가의 지향점은 ’인간의 인지’와 얼마나 일치하는지로 귀결된다. 따라서 향후 연구는 더 정교하고 강건한 지각적 평가 지표를 개발하고, 다양한 실제 환경과 복합적인 왜곡을 포괄하는 대규모 실제 데이터셋을 구축하며, 의료 영상이나 자율 주행과 같이 특정 응용 분야에 특화된 평가 방법론을 정립하는 방향으로 나아가야 할 것이다. 실험실 환경에서의 높은 점수를 넘어, 예측 불가능한 실제 환경에서의 강건성(robustness)과 일반화 능력을 입증하는 것이 앞으로도 Deblur 연구의 가장 중요한 핵심 과제로 남을 것이다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>Deep Learning based Automated Image Deblurring - E3S Web of Conferences, https://www.e3s-conferences.org/articles/e3sconf/pdf/2023/67/e3sconf_icmpc2023_01052.pdf</li>
<li>(PDF) Deep Learning based Automated Image Deblurring - ResearchGate, https://www.researchgate.net/publication/374505481_Deep_Learning_based_Automated_Image_Deblurring</li>
<li>Image Deblurring using Deep Learning - IJRASET, https://www.ijraset.com/research-paper/image-deblurring-using-deep-learning</li>
<li>What is the image deblurring deep learning method? | by Saiwa - Medium, https://medium.com/@saiwadotai/what-is-the-image-deblurring-deep-learning-method-b40621406ae3</li>
<li>One-Step Diffusion Model for Image Motion-Deblurring - arXiv, https://arxiv.org/html/2503.06537v1</li>
<li>Let’s enhance: A deep learning approach to extreme deblurring of text images, https://www.aimsciences.org/article/doi/10.3934/ipi.2023019</li>
<li>A state-of-the-art review of image motion deblurring techniques in precision agriculture - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC10320030/</li>
<li>Deep Learning in Motion Deblurring: Current Status, Benchmarks and Future Prospects, https://arxiv.org/html/2401.05055v2</li>
<li>(PDF) Image quality metrics: PSNR vs. SSIM - ResearchGate, https://www.researchgate.net/publication/220931731_Image_quality_metrics_PSNR_vs_SSIM</li>
<li>Ways of cheating on popular objective metrics: blurring, noise, super-resolution and others, https://videoprocessing.ai/metrics/ways-of-cheating-on-popular-objective-metrics.html</li>
<li>A Comparative Study for Single Image Blind Deblurring - UC Merced, https://faculty.ucmerced.edu/mhyang/papers/cvpr16_deblurring_benchmark.pdf</li>
<li>Combined No-Reference Image Quality Metrics for Visual Quality Assessment Optimized for Remote Sensing Images - MDPI, https://www.mdpi.com/2076-3417/12/4/1986</li>
<li>Image Quality Metrics - MATLAB &amp; Simulink - MathWorks, https://www.mathworks.com/help/images/image-quality-metrics.html</li>
<li>(PDF) Comparison of image quality assessment: PSNR, HVS, SSIM, UIQI - ResearchGate, https://www.researchgate.net/publication/303517356_Comparison_of_image_quality_assessment_PSNR_HVS_SSIM_UIQI</li>
<li>A Review of the Image Quality Metrics used in Image Generative Models - Paperspace Blog, https://blog.paperspace.com/review-metrics-image-synthesis-models/</li>
<li>Peak Signal-to-Noise Ratio (PSNR) - Python - GeeksforGeeks, https://www.geeksforgeeks.org/python/python-peak-signal-to-noise-ratio-psnr/</li>
<li>Making Sense of PSNR, SSIM, VMAF - Visionular, https://visionular.ai/vmaf-ssim-psnr-quality-metrics/</li>
<li>Structural similarity index measure - Wikipedia, https://en.wikipedia.org/wiki/Structural_similarity_index_measure</li>
<li>No-Reference Blurred Image Quality Assessment by Structural Similarity Index - MDPI, https://www.mdpi.com/2076-3417/8/10/2003</li>
<li>RealBlur Dataset, https://cg.postech.ac.kr/research/realblur/</li>
<li>Real Image Deblurring Based on Implicit Degradation Representations and Reblur Estimation - MDPI, https://www.mdpi.com/2076-3417/13/13/7738</li>
<li>Shift-tolerant Perceptual Similarity Metric - European Computer Vision Association, https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136780089.pdf</li>
<li>richzhang/PerceptualSimilarity: LPIPS metric. pip install lpips - GitHub, https://github.com/richzhang/PerceptualSimilarity</li>
<li>Foundation Models Boost Low-Level Perceptual Similarity Metrics - PDXScholar, https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=1382&amp;context=compsci_fac</li>
<li>Learned Perceptual Image Patch Similarity (LPIPS) — PyTorch-Metrics 1.8.2 documentation, https://lightning.ai/docs/torchmetrics/stable/image/learned_perceptual_image_patch_similarity.html</li>
<li>The Unreasonable Effectiveness of Deep Features as a Perceptual Metric - CVF Open Access, https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_The_Unreasonable_Effectiveness_CVPR_2018_paper.pdf</li>
<li>Deep Perceptual Loss and Similarity - DiVA portal, https://www.diva-portal.org/smash/get/diva2:1752110/FULLTEXT01.pdf</li>
<li>Foundation Models Boost Low-Level Perceptual Similarity Metrics - arXiv, https://arxiv.org/html/2409.07650v2</li>
<li>The Unreasonable Effectiveness of Deep Features as a Perceptual Metric - Richard Zhang, https://richzhang.github.io/PerceptualSimilarity/</li>
<li>The Unreasonable Effectiveness of Deep Features as a Perceptual Metric - arXiv, https://arxiv.org/abs/1801.03924</li>
<li>A No-Reference Metric for Evaluating the Quality of Motion Deblurring - Princeton University, https://gfx.cs.princeton.edu/pubs/Liu_2013_ANM/sa13.pdf</li>
<li>A Survey on Image Quality Assessment: Insights, Analysis, and Future Outlook - arXiv, https://arxiv.org/html/2502.08540v1</li>
<li>No-Reference Quality Assessment of Blurred Images by Combining Hybrid Metrics | IIETA, https://www.iieta.org/journals/ts/paper/10.18280/ts.410435</li>
<li>Automatic no-reference image quality assessment - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC4947068/</li>
<li>NO-REFERENCE IMAGE BLUR ASSESSMENT USING MULTISCALE GRADIENT, https://live.ece.utexas.edu/publications/2009/mc_qomex09.pdf</li>
<li>(PDF) Blur-Specific No-Reference Image Quality Assessment: A Classification and Review of Representative Methods - ResearchGate, https://www.researchgate.net/publication/327752994_Blur-Specific_No-Reference_Image_Quality_Assessment_A_Classification_and_Review_of_Representative_Methods</li>
<li>Qualitative comparison of different deblurring methods on the… | Download Scientific Diagram - ResearchGate, https://www.researchgate.net/figure/Qualitative-comparison-of-different-deblurring-methods-on-the-RealBlur-J-and-RealBlur-R_fig5_345824632</li>
<li>DarkDeblur: Learning single-shot image deblurring in low-light condition - arXiv, https://arxiv.org/html/2503.02194v1</li>
<li>Two examples of qualitative evaluation on Real-world dataset [16] - ResearchGate, https://www.researchgate.net/figure/Two-examples-of-qualitative-evaluation-on-Real-world-dataset-16_fig5_353238023</li>
<li>Real-World Blur Dataset for Learning and Benchmarking Deblurring Algorithms - European Computer Vision Association, https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700188.pdf</li>
<li>NTIRE 2019 Challenge on Video Deblurring and Super-Resolution: Dataset and Study - CVF Open Access, https://openaccess.thecvf.com/content_CVPRW_2019/papers/NTIRE/Nah_NTIRE_2019_Challenge_on_Video_Deblurring_and_Super-Resolution_Dataset_and_CVPRW_2019_paper.pdf</li>
<li>Publications Datasets CV | Seungjun Nah, https://seungjunnah.github.io/Datasets/gopro.html</li>
<li>Publications Datasets CV | Seungjun Nah, https://seungjunnah.github.io/Datasets/reds.html</li>
<li>Real Blur - Kaggle, https://www.kaggle.com/datasets/vladparh/real-blur</li>
<li>A Curated List of Image Deblurring Datasets - Kaggle, https://www.kaggle.com/datasets/jishnuparayilshibu/a-curated-list-of-image-deblurring-datasets</li>
<li>rimchang/RSBlur: Repository for Realistic Blur Synthesis for Learning Image Deblurring - GitHub, https://github.com/rimchang/RSBlur</li>
<li>Jishnu8/DBlur-An-Image-Deblurring-Toolkit - GitHub, https://github.com/Jishnu8/DBlur-An-Image-Deblurring-Toolkit</li>
<li>PSNR and SSIM Metric: Python Implementation - CV Notes, https://cvnote.ddlee.cc/2019/09/12/psnr-ssim-python</li>
<li>jsh-me/psnr-ssim-tool: Peak signal-to-noise ratio and The structural similarity calculation tool, https://github.com/jsh-me/psnr-ssim-tool</li>
<li>skimage.metrics — skimage 0.25.2 documentation, https://scikit-image.org/docs/0.25.x/api/skimage.metrics.html</li>
<li>How i can measure pnsr for multiychannel iamges - PyTorch Forums, https://discuss.pytorch.org/t/how-i-can-measure-pnsr-for-multiychannel-iamges/126288</li>
<li>Peak Signal to Noise Ratio (PSNR) in Python for an Image, https://dsp.stackexchange.com/questions/38065/peak-signal-to-noise-ratio-psnr-in-python-for-an-image</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>