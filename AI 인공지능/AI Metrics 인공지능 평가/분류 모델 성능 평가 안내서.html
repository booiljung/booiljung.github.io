<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:분류 모델 성능 평가 안내서</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>분류 모델 성능 평가 안내서</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">인공지능 평가지표 (AI evaluation metrics)</a> / <span>분류 모델 성능 평가 안내서</span></nav>
                </div>
            </header>
            <article>
                <h1>분류 모델 성능 평가 안내서</h1>
<h2>1.  분류 모델 성능 평가의 본질</h2>
<h3>1.1  분류 모델의 정의와 역할</h3>
<p>분류(Classification) 모델은 지도 학습(Supervised Learning)의 핵심적인 유형으로, 입력된 데이터 포인트를 사전에 정의된 범주, 즉 클래스(Class)로 할당하는 예측 모델이다.1 모델은 레이블이 지정된 학습 데이터셋을 통해 각 클래스를 구별하는 고유한 특징(feature)을 학습하며, 이 학습된 지식을 바탕으로 이전에 보지 못한 새로운 데이터가 어떤 클래스에 속할지 예측하는 역할을 수행한다.2</p>
<p>분류 문제는 해결하고자 하는 과제에 따라 크게 두 가지로 나뉜다. 결과값이 ‘예/아니오’, ’스팸/정상’과 같이 두 개의 범주 중 하나인 경우를 **이진 분류(Binary Classification)**라 하고, ‘개/고양이/라마’, ’A/B/C 등급’처럼 세 개 이상의 범주로 나누는 경우를 **다중 클래스 분류(Multi-Class Classification)**라고 한다.2 이러한 분류 문제를 해결하기 위해 로지스틱 회귀(Logistic Regression), 서포트 벡터 머신(Support Vector Machine, SVM), 결정 트리(Decision Tree), 랜덤 포레스트(Random Forest), 신경망(Neural Network) 등 다양한 알고리즘이 개발되어 활용되고 있다.3</p>
<h3>1.2  성능 평가의 중요성</h3>
<p>머신러닝 모델을 구축하는 것만으로는 충분하지 않다. 구축된 모델이 얼마나 ‘좋은지’, 즉 예측이 얼마나 정확하고 신뢰할 수 있는지를 객관적으로 측정하는 성능 평가 과정은 모델 개발 생명주기에서 필수 불가결한 요소이다.6 성능 평가는 다음과 같은 핵심적인 이유로 중요하다.</p>
<ul>
<li>
<p><strong>신뢰성 있는 모델 선택:</strong> 현실의 문제를 해결하기 위해 여러 다른 알고리즘(예: 로지스틱 회귀, 랜덤 포레스트)으로 모델을 만들 수 있다. 성능 지표는 이러한 모델들의 우수성을 객관적인 수치로 비교하여, 주어진 과제에 가장 적합한 모델을 선정하는 합리적인 기준을 제공한다.7</p>
</li>
<li>
<p><strong>하이퍼파라미터 튜닝:</strong> 모델의 성능을 극대화하기 위해 내부 설정값인 하이퍼파라미터를 조정하는 과정이 필요하다. 성능 지표는 이 튜닝 과정에서 나침반 역할을 하여, 어떤 매개변수 조합이 최적의 결과를 내는지 알려준다.7</p>
</li>
<li>
<p><strong>비즈니스 영향 분석:</strong> 모델의 예측 오류는 비즈니스에 직접적인 손실이나 기회비용을 초래할 수 있다. 예를 들어, 의료 진단 모델이 암 환자를 정상으로 오진하는 경우(위음성, False Negative)의 비용과 스팸 필터가 중요한 메일을 스팸으로 분류하는 경우(위양성, False Positive)의 비용은 그 성격과 규모가 완전히 다르다. 적절한 평가 지표를 선택하고 분석함으로써 이러한 비즈니스 영향을 정량화하고 관리할 수 있다.7</p>
</li>
<li>
<p><strong>모델 드리프트 감지:</strong> 실제 운영 환경에 배포된 모델은 시간이 지남에 따라 데이터 분포가 변화하면서 성능이 점차 저하될 수 있다. 이를 ’모델 드리프트(Model Drift)’라 하는데, 성능 지표를 지속적으로 모니터링함으로써 이러한 성능 저하를 조기에 감지하고 모델을 재학습하거나 업데이트할 시점을 결정할 수 있다.7</p>
</li>
<li>
<p><strong>과적합(Overfitting) 방지:</strong> 모델이 학습 데이터에만 지나치게 최적화되어 새로운 데이터에 대해서는 예측 성능이 떨어지는 과적합 현상은 모델 일반화의 가장 큰 적이다. 성능 평가는 모델이 학습 데이터뿐만 아니라 보지 못한 데이터에 대해서도 얼마나 일관된 성능을 보이는지 측정하여 과적합을 방지하고 일반화 성능을 확보하는 데 필수적이다.9</p>
</li>
</ul>
<p>결론적으로, 머신러닝 모델 평가는 단순히 통계적 정확성을 측정하는 기술적 행위를 넘어선다. 이는 모델의 행동 양식과 오류 유형을 비즈니스의 가치 및 위험과 연결하는 전략적 과정이다. ’최고의 모델’이란 단일 지표 점수가 가장 높은 모델이 아니라, 특정 비즈니스 상황에서 발생하는 다양한 오류의 비용(cost of errors)을 가장 효과적으로 최소화하는 모델을 의미한다. 따라서 평가 지표의 선택은 “어떤 종류의 실수를 더 치명적으로 간주할 것인가?“라는 비즈니스 질문에 대한 답을 모델링의 언어로 번역하는 행위이며, 모델을 최종적인 비즈니스 목표에 정렬시키는 핵심적인 다리 역할을 한다.</p>
<h2>2.  평가의 초석: 혼동 행렬</h2>
<h3>2.1  혼동 행렬의 구조와 정의</h3>
<p>혼동 행렬(Confusion Matrix) 또는 오차 행렬(Error Matrix)은 분류 모델의 예측 성능을 시각적으로 표현하고 모든 성능 지표 계산의 기초를 제공하는 가장 기본적인 도구이다.11 이 행렬은 실제 값(Actual Values)과 모델의 예측 값(Predicted Values)을 행과 열로 하는 2x2 표(이진 분류의 경우)로 구성된다. 이름에서 알 수 있듯이, 이 표는 모델이 예측 과정에서 어떤 클래스들을 서로 ’혼동’하고 있는지를 명확하게 보여준다.10</p>
<h3>2.2  혼동 행렬의 4대 구성 요소</h3>
<p>이진 분류 문제에서 Positive 클래스(예: ‘병에 걸림’, ‘스팸 메일’)와 Negative 클래스(예: ‘정상’, ‘일반 메일’)가 있다고 가정할 때, 혼동 행렬은 다음과 같은 네 가지 핵심 요소로 구성된다.</p>
<ul>
<li>
<p><strong>진양성 (True Positive, TP):</strong> 실제 클래스가 Positive인 샘플을 모델이 Positive로 올바르게 예측한 경우의 수.11</p>
</li>
<li>
<p><strong>진음성 (True Negative, TN):</strong> 실제 클래스가 Negative인 샘플을 모델이 Negative로 올바르게 예측한 경우의 수.11</p>
</li>
<li>
<p><strong>위양성 (False Positive, FP):</strong> 실제 클래스가 Negative인 샘플을 모델이 Positive로 잘못 예측한 경우의 수. 이를 **1종 오류(Type I Error)**라고도 한다.11</p>
</li>
<li>
<p><strong>위음성 (False Negative, FN):</strong> 실제 클래스가 Positive인 샘플을 모델이 Negative로 잘못 예측한 경우의 수. 이를 **2종 오류(Type II Error)**라고도 한다.11</p>
</li>
</ul>
<p>여기서 TP와 TN은 모델이 정답을 맞힌 경우로, 행렬의 주 대각선에 위치한다. 반면 FP와 FN은 모델이 오류를 범한 경우로, 비대각선에 위치한다.15 이 네 가지 값을 통해 뒤따르는 모든 복잡한 성능 지표들이 계산된다.</p>
<table><thead><tr><th></th><th>예측: Positive (Predicted: Positive)</th><th>예측: Negative (Predicted: Negative)</th></tr></thead><tbody>
<tr><td><strong>실제: Positive (Actual: Positive)</strong></td><td>TP (진양성)</td><td>FN (위음성)</td></tr>
<tr><td><strong>실제: Negative (Actual: Negative)</strong></td><td>FP (위양성)</td><td>TN (진음성)</td></tr>
</tbody></table>
<h2>3.  핵심 성능 지표: 이진 분류를 중심으로</h2>
<p>혼동 행렬의 네 가지 구성 요소를 바탕으로, 모델의 성능을 다양한 관점에서 평가하는 핵심 지표들을 정의할 수 있다.</p>
<h3>3.1  정확도 (Accuracy): 직관성의 함정</h3>
<p><strong>정의:</strong> 정확도는 전체 샘플 중에서 모델이 올바르게 예측한(TP + TN) 샘플의 비율을 나타낸다. 가장 직관적이고 이해하기 쉬운 지표이다.11</p>
<p><strong>계산식:</strong></p>
<p><span class="math math-display">
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
</span><br />
<strong>한계점:</strong> 정확도의 가장 큰 맹점은 데이터의 클래스 분포가 불균형(imbalanced)할 때 발생한다.17 예를 들어, 전체 환자 중 1%만이 희귀병을 앓고 있는 데이터셋이 있다고 가정하자. 만약 모델이 모든 환자에 대해 ‘정상’(Negative)이라고만 예측한다면, 이 모델의 정확도는 99%에 달하게 된다. 수치상으로는 매우 뛰어난 모델처럼 보이지만, 실제로는 단 한 명의 환자도 찾아내지 못하는, 완전히 무용한 모델이다. 이처럼 정확도는 다수 클래스에 대한 예측 성능에 의해 결과가 좌우되어 소수 클래스에 대한 모델의 성능을 전혀 반영하지 못하는 심각한 왜곡을 일으킬 수 있다.11</p>
<h3>3.2  정밀도 (Precision): 예측의 신뢰도</h3>
<p><strong>정의:</strong> 정밀도는 모델이 Positive로 예측한 샘플들 중에서, 실제로 Positive였던 샘플의 비율을 의미한다. PPV(Positive Predictive Value)라고도 불린다.12 이 지표는 “모델이 ’Positive’라고 한 예측을 얼마나 믿을 수 있는가?“라는 질문에 답을 준다.</p>
<p><strong>계산식:</strong></p>
<p><span class="math math-display">
\text{Precision} = \frac{TP}{TP + FP}
</span><br />
<strong>중요성:</strong> 정밀도는 위양성(FP)으로 인한 비용이 클 때, 즉 실제 Negative를 Positive로 잘못 판단하는 것을 최소화해야 할 때 매우 중요한 지표가 된다.11</p>
<h3>3.3  재현율 (Recall): 실제 Positive 탐지 능력</h3>
<p><strong>정의:</strong> 재현율은 실제 Positive인 전체 샘플들 중에서, 모델이 Positive로 올바르게 예측해낸 샘플의 비율을 나타낸다. 민감도(Sensitivity) 또는 TPR(True Positive Rate)과 완전히 동일한 개념이다.12 이 지표는 “모델이 실제 Positive 샘플들을 얼마나 놓치지 않고 잘 찾아내는가?“라는 질문에 답을 준다.</p>
<p><strong>계산식:</strong><br />
<span class="math math-display">
\text{Recall (Sensitivity, TPR)} = \frac{TP}{TP + FN}
</span><br />
<strong>중요성:</strong> 재현율은 위음성(FN)으로 인한 비용이 클 때, 즉 실제 Positive를 Negative로 잘못 판단하는 것을 최소화해야 할 때 핵심적인 지표로 사용된다.11</p>
<h3>3.4  정밀도와 재현율의 상충 관계 (Precision-Recall Trade-off)</h3>
<p>정밀도와 재현율은 서로를 보완하는 동시에 상충 관계(Trade-off)에 놓여 있다.11 분류 모델은 내부적으로 각 샘플이 Positive일 확률을 계산하고, 이 확률이 특정 임계값(Threshold)을 넘으면 Positive로, 넘지 못하면 Negative로 최종 분류한다. 이 임계값을 조정함에 따라 정밀도와 재현율은 반대로 움직이는 경향을 보인다.</p>
<ul>
<li>
<p><strong>임계값을 높이면:</strong> 모델은 매우 확실한 경우에만 Positive로 예측하게 된다. 이로 인해 FP가 줄어들어 <strong>정밀도는 상승</strong>하지만, 일부 애매한 Positive 샘플을 Negative로 판단하게 되어 FN이 늘어나므로 <strong>재현율은 하락</strong>한다.</p>
</li>
<li>
<p><strong>임계값을 낮추면:</strong> 모델은 조금이라도 가능성이 있으면 Positive로 예측하게 된다. 이로 인해 FN이 줄어들어 <strong>재현율은 상승</strong>하지만, Negative 샘플을 Positive로 잘못 판단하는 FP가 늘어나므로 <strong>정밀도는 하락</strong>한다.</p>
</li>
</ul>
<p>따라서 어느 한 지표만 극단적으로 높은 모델은 바람직하지 않으며, 해결하고자 하는 문제의 특성에 맞춰 두 지표 간의 적절한 균형점을 찾는 것이 중요하다.11</p>
<h3>3.5  F1 점수 (F1-Score): 정밀도와 재현율의 조화 평균</h3>
<p><strong>정의:</strong> F1 점수는 정밀도와 재현율의 조화 평균(Harmonic Mean)이다. 산술 평균이 아닌 조화 평균을 사용하는 이유는 두 지표 중 어느 한쪽이라도 극단적으로 낮으면 F1 점수 역시 낮아지도록 하여, 두 지표가 모두 균형 있게 높을 때에만 높은 점수를 부여하기 위함이다.12</p>
<p><strong>계산식:</strong></p>
<p><span class="math math-display">
F_1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2 \times TP}{2 \times TP + FP + FN}
</span><br />
<strong>유용성:</strong> F1 점수는 클래스 불균형 상황에서 정확도보다 훨씬 더 신뢰할 수 있는 단일 성능 척도를 제공한다.17 또한, F-beta 점수(Fβ​)로 일반화하여 정밀도나 재현율에 특정 가중치를 부여할 수도 있다. 예를 들어, 재현율을 정밀도보다 2배 더 중요하게 생각한다면 F2​ 점수를, 정밀도를 재현율보다 더 중요하게 생각한다면 F0.5​ 점수를 사용할 수 있다.22</p>
<table><thead><tr><th>지표 (Metric)</th><th>계산식 (Formula)</th><th>핵심 질문 (Key Question)</th></tr></thead><tbody>
<tr><td>정확도 (Accuracy)</td><td><span class="math math-inline">(TP + TN) / (TP + TN + FP + FN)</span></td><td>전체 예측 중 얼마나 맞았는가?</td></tr>
<tr><td>정밀도 (Precision)</td><td><code>TP / (TP + FP)</code></td><td>Positive 예측이 얼마나 정확한가?</td></tr>
<tr><td>재현율 (Recall)</td><td><code>TP / (TP + FN)</code></td><td>실제 Positive를 얼마나 잘 찾아냈는가?</td></tr>
<tr><td>F1 점수 (F1-Score)</td><td><span class="math math-inline">2 \times (\text{Precision} \times \text{Recall}) / (\text{Precision} + \text{Recall})</span></td><td>정밀도와 재현율이 얼마나 균형을 이루는가?</td></tr>
</tbody></table>
<h2>4.  데이터 불균형 문제와 평가 전략</h2>
<h3>4.1  정확도의 착시 현상 심층 분석</h3>
<p>데이터 불균형은 특정 클래스의 데이터가 다른 클래스에 비해 현저히 많거나 적은 상태를 의미하며, 신용카드 사기 탐지, 제조 공정 불량 예측, 희귀 질병 진단 등 현실의 많은 문제에서 나타나는 보편적인 현상이다.28</p>
<p>이러한 상황에서 정확도는 ’착시 현상’을 일으킨다. 모델의 성능이 실제로는 다수 클래스(majority class)에 대한 예측 능력에 의해 지배되기 때문이다. 모델이 소수 클래스(minority class)를 완전히 무시하고 모든 예측을 다수 클래스로만 내놓더라도, 전체 데이터에서 다수 클래스가 차지하는 비율만큼의 높은 정확도를 기록하게 된다.11 이는 모델의 심각한 결함을 감추고, 마치 모델이 잘 작동하는 것처럼 보이게 만드는 ’정확도의 역설(Accuracy Paradox)’로 이어진다. 따라서 불균형 데이터를 다룰 때 정확도에만 의존하는 것은 매우 위험한 접근 방식이다.</p>
<h3>4.2  상황별 최적 지표 선택 가이드</h3>
<p>최적의 평가 지표를 선택하는 행위는 본질적으로 ’어떤 종류의 실수가 더 치명적인가?’라는 질문에 답하는 과정이다. 이는 단순히 기술적인 결정이 아니라, 문제의 도메인 지식과 비즈니스 로직에 깊이 뿌리내린 전략적 결정이다. 모든 모델은 위양성(FP) 또는 위음성(FN)의 실수를 범하며, 현실 세계의 문제에서 이 두 가지 실수 유형이 초래하는 결과, 즉 비용은 거의 항상 비대칭적이다. 재현율은 FN의 비용을 최소화하는 데, 정밀도는 FP의 비용을 최소화하는 데 초점을 맞춘다. 따라서, 모델 평가의 시작은 문제 자체의 비용 함수(cost function)를 이해하는 것에서 출발해야 한다.</p>
<h4>4.2.1 사례 1: 재현율(Recall)이 중요한 경우 - 의료 진단 (암 진단)</h4>
<ul>
<li>
<p><strong>상황:</strong> 암 환자를 진단하는 모델을 개발하는 경우 (Positive: 암, Negative: 정상).24</p>
</li>
<li>
<p><strong>위험 분석:</strong></p>
</li>
<li>
<p><strong>위음성 (False Negative, FN):</strong> 실제 암 환자를 ’정상’으로 오진하는 경우이다. 이 오류는 환자가 적절한 치료 시기를 놓치게 만들어 병이 악화되거나, 최악의 경우 생명을 잃게 할 수 있는 치명적인 결과를 초래한다.11</p>
</li>
<li>
<p><strong>위양성 (False Positive, FP):</strong> 건강한 사람을 ’암 환자’로 오진하는 경우이다. 이 경우, 대상자는 추가적인 정밀 검사를 받게 되어 시간적, 경제적, 정신적 비용이 발생하지만, 생명에 직접적인 위협이 되지는 않는다.31</p>
</li>
<li>
<p><strong>결론:</strong> 이 시나리오에서는 FN의 비용이 FP의 비용보다 압도적으로 높다. 따라서 모델은 단 한 명의 실제 암 환자라도 놓치지 않는 것을 최우선 목표로 삼아야 한다. 즉, <strong>재현율</strong>을 극대화하는 방향으로 모델을 평가하고 튜닝하는 것이 가장 합리적인 전략이다.7</p>
</li>
</ul>
<h4>4.2.2 사례 2: 정밀도(Precision)가 중요한 경우 - 스팸 메일 필터링</h4>
<ul>
<li>
<p><strong>상황:</strong> 수신되는 이메일이 스팸인지 아닌지를 분류하는 모델을 개발하는 경우 (Positive: 스팸, Negative: 정상 메일).32</p>
</li>
<li>
<p><strong>위험 분석:</strong></p>
</li>
<li>
<p><strong>위양성 (False Positive, FP):</strong> 중요한 정상 메일(예: 입사 제안, 긴급 업무 연락, 계약서)을 ’스팸’으로 잘못 분류하는 경우이다. 이 오류로 인해 사용자는 매우 중요한 정보를 놓쳐 심각한 기회비용이나 실질적인 손실을 입을 수 있다.22</p>
</li>
<li>
<p><strong>위음성 (False Negative, FN):</strong> 스팸 메일을 ’정상 메일’로 잘못 분류하여 받은 편지함으로 통과시키는 경우이다. 사용자가 불필요한 메일을 확인하고 삭제해야 하는 약간의 불편함이 발생하지만, 치명적인 손실로 이어질 가능성은 낮다.22</p>
</li>
<li>
<p><strong>결론:</strong> 이 시나리오에서는 FP의 비용이 FN의 비용보다 훨씬 높다. 따라서 모델은 정상 메일을 스팸으로 잘못 판단하는 실수를 최소화하는 것을 최우선 목표로 삼아야 한다. 즉, <strong>정밀도</strong>를 극대화하여 모델이 ’스팸’이라고 예측한 결과의 신뢰도를 높이는 것이 핵심 전략이다.11</p>
</li>
</ul>
<h2>5.  시각적 평가 도구와 곡선 아래 면적</h2>
<p>단일 숫자 지표는 모델 성능의 특정 단면만을 보여준다. 모델의 분류 결정 임계값(threshold) 변화에 따른 성능의 전체적인 동향을 파악하기 위해 시각적 평가 도구가 널리 사용된다.</p>
<h3>5.1  ROC 곡선과 AUC (Receiver Operating Characteristic Curve and AUC)</h3>
<p><strong>정의:</strong> ROC 곡선은 모든 가능한 분류 임계값에 대해 모델의 성능을 시각화한 그래프이다. 가로축(X축)은 위양성률(FPR), 세로축(Y축)은 진양성률(TPR, 재현율)을 나타낸다.37</p>
<ul>
<li>
<p>Y축: <strong>TPR (True Positive Rate)</strong> = <code>TP / (TP + FN)</code> (민감도, 재현율)</p>
</li>
<li>
<p>X축: <strong>FPR (False Positive Rate)</strong> = <code>FP / (FP + TN)</code> (1 - 특이도) 22</p>
</li>
</ul>
<p><strong>해석:</strong> 이상적인 모델은 FPR이 0이면서 TPR이 1인 지점, 즉 그래프의 좌상단(0, 1)에 최대한 가까운 곡선을 그린다.37 (0, 0)에서 (1, 1)을 잇는 대각선은 모델의 성능이 무작위 추측과 동일함을 의미한다.19</p>
<p><strong>AUC (Area Under the Curve):</strong> ROC 곡선 아래의 면적을 계산한 값으로, 0과 1 사이의 값을 가진다. AUC는 특정 임계값에 의존하지 않고 모델이 양성 샘플을 음성 샘플보다 더 높은 확률로 예측할 전반적인 능력을 나타내는 단일 수치이다.19 AUC 값이 1에 가까울수록 완벽한 모델이며, 0.5는 무작위 수준의 모델을 의미한다.19</p>
<p><strong>AUC 해석 기준:</strong> 19</p>
<ul>
<li>
<p>0.9 ~ 1.0: Excellent (매우 우수)</p>
</li>
<li>
<p>0.8 ~ 0.9: Good (우수)</p>
</li>
<li>
<p>0.7 ~ 0.8: Fair (보통)</p>
</li>
<li>
<p>0.5 ~ 0.7: Poor (성능 낮음)</p>
</li>
<li>
<p>&lt; 0.5: 무작위보다 나쁨 (예측 결과를 반대로 하면 성능이 향상될 수 있음) 19</p>
</li>
</ul>
<h3>5.2  정밀도-재현율(PR) 곡선과 PR AUC (Precision-Recall Curve and PR AUC)</h3>
<p><strong>정의:</strong> PR 곡선은 분류 임계값 변화에 따른 정밀도(Precision)와 재현율(Recall)의 관계를 시각화한 그래프이다.20</p>
<ul>
<li>
<p>Y축: <strong>Precision</strong></p>
</li>
<li>
<p>X축: <strong>Recall (TPR)</strong></p>
</li>
</ul>
<p><strong>해석:</strong> 이상적인 모델은 재현율이 높아져도 높은 정밀도를 유지하는, 즉 그래프의 우상단(1, 1)에 가까운 곡선을 그린다.44</p>
<p><strong>PR AUC (AUPRC):</strong> PR 곡선 아래의 면적으로, AP(Average Precision)라고도 불린다. 이 값 역시 1에 가까울수록 좋은 모델임을 의미한다.20</p>
<h3>5.3  ROC 곡선 vs. PR 곡선: 언제 무엇을 써야 하는가?</h3>
<p>ROC 곡선과 PR 곡선의 근본적인 차이는 가로축(FPR)과 세로축(Precision)의 ’분모’에서 비롯된다. 이 분모의 차이는 특히 불균형 데이터셋에서 두 곡선이 전달하는 성능에 대한 서사를 완전히 다르게 만든다.</p>
<ol>
<li>
<p>ROC 곡선의 x축인 FPR의 수식은 <span class="math math-inline">FP / (FP + TN)</span>이다. PR 곡선의 y축인 Precision의 수식은 <span class="math math-inline">TP / (TP + FP)</span>이다. 두 곡선 모두 TPR(Recall)을 공유하므로, 핵심적인 차이는 FPR과 Precision에서 발생한다.28</p>
</li>
<li>
<p>데이터가 불균형할 경우, 다수 클래스인 Negative 샘플의 수, 즉 TN(진음성)의 수는 소수 클래스인 Positive 샘플 수에 비해 압도적으로 많다.28</p>
</li>
<li>
<p>모델의 성능이 개선되어 FP(위양성)의 수가 줄어들더라도, FPR의 분모 <span class="math math-inline">(FP + TN)</span>은 거대한 TN의 값 때문에 거의 변하지 않는다. 결과적으로 FPR 값의 변화는 매우 둔감하게 나타나며, ROC 곡선 상에서는 실제 성능 개선이 미미하게 보이거나 실제보다 훨씬 좋게 보이는 ‘지나치게 낙관적인(optimistic)’ 그림을 그리게 된다.28</p>
</li>
<li>
<p>반면, Precision의 분모 <span class="math math-inline">(TP + FP)</span>는 오직 모델이 ’Positive라고 예측한 결과’에만 의존하며, 거대한 TN의 수와는 완전히 무관하다. 따라서 FP 값의 변화가 Precision 값에 직접적이고 민감하게 반영된다.28</p>
</li>
</ol>
<p>결론적으로, 소수 클래스 탐지 성능의 미세한 개선을 정밀하게 평가하고자 할 때, PR 곡선은 ROC 곡선보다 훨씬 더 유용하고 정직한 시각적 피드백을 제공한다. 따라서 <strong>클래스 불균형이 심한 데이터셋에서는 PR 곡선을 사용하는 것이 강력하게 권장된다</strong>.20</p>
<h2>6.  다중 클래스 분류 평가 지표의 확장</h2>
<h3>6.1  다중 클래스 평가의 복잡성</h3>
<p>클래스가 3개 이상인 다중 클래스 분류 문제에서는 이진 분류의 TP, TN, FP, FN 개념을 직접 적용하기 어렵다. 따라서 평가는 주로 각 클래스를 개별적인 Positive 클래스로 간주하고, 나머지 모든 클래스를 하나의 Negative 클래스로 묶는 ‘One-vs-Rest (OvR)’ 방식을 기반으로 이루어진다.25 이 방식을 통해 각 클래스별로 정밀도, 재현율, F1 점수 등을 계산할 수 있다. 그러나 모델의 전체 성능을 나타내는 단일 점수를 얻기 위해서는 이렇게 계산된 클래스별 점수들을 종합하는 ‘평균(averaging)’ 방식이 필요하다.</p>
<h3>6.2  Macro, Micro, Weighted 평균 방식 비교</h3>
<p>주요 평균 방식으로는 Macro, Micro, Weighted 세 가지가 있으며, 각각은 다른 철학을 바탕으로 성능을 집계한다.</p>
<ul>
<li>
<p><strong>Macro Average:</strong></p>
</li>
<li>
<p><strong>개념:</strong> 모든 클래스에 대해 개별적으로 성능 지표(예: F1 점수)를 계산한 뒤, 이들의 단순 산술 평균(가중치 없음)을 구하는 방식이다.47</p>
</li>
<li>
<p><strong>철학 (“One Class, One Vote”):</strong> 모든 클래스를 동등하게 중요하다고 간주한다. 이는 클래스별 샘플 수의 차이, 즉 클래스 불균형을 고려하지 않음을 의미한다. 따라서 샘플 수가 적은 소수 클래스에서 낮은 성능을 보이면 전체 평균 점수가 크게 하락하게 된다.47</p>
</li>
<li>
<p><strong>사용 시점:</strong> 모든 클래스의 예측 성능이 동등하게 중요할 때, 특히 소수 클래스의 성능 저하를 민감하게 감지하고 싶을 때 유용하다.47</p>
</li>
<li>
<p><strong>Weighted Average:</strong></p>
</li>
<li>
<p><strong>개념:</strong> 각 클래스별 성능 지표를 계산한 후, 각 클래스의 실제 샘플 수(support)에 비례하는 가중치를 부여하여 평균을 내는 방식이다.47</p>
</li>
<li>
<p><strong>철학 (“Proportional Importance”):</strong> 샘플 수가 많은 클래스의 성능을 더 중요하게 반영한다. 클래스 불균형을 점수에 직접 반영하는 방식이다.52</p>
</li>
<li>
<p><strong>사용 시점:</strong> 클래스 불균형이 존재하며, 다수 클래스에서의 성능이 비즈니스적으로 더 중요하다고 판단될 때 적합하다.47</p>
</li>
<li>
<p><strong>Micro Average:</strong></p>
</li>
<li>
<p><strong>개념:</strong> 클래스별로 지표를 계산한 후 평균을 내는 것이 아니라, 전체 데이터셋에 걸쳐 TP, FP, FN의 총합을 먼저 구한 뒤, 이 총합을 바탕으로 단일 성능 지표를 한 번에 계산하는 방식이다.47</p>
</li>
<li>
<p><strong>철학 (“One Sample, One Vote”):</strong> 클래스에 상관없이 모든 개별 샘플의 예측 결과를 동등하게 취급한다.</p>
</li>
<li>
<p><strong>중요한 특징:</strong> 다중 클래스 분류에서 Micro-averaged Precision, Micro-averaged Recall, Micro-averaged F1-score는 모두 **전체 정확도(Overall Accuracy)**와 정확히 동일한 값을 가진다.47 이 때문에</p>
</li>
</ul>
<p><code>scikit-learn</code>과 같은 라이브러리의 분류 성능 보고서에서는 Micro Average가 별도로 표시되지 않고 Accuracy로 통합되어 나타나는 경우가 많다.47</p>
<ul>
<li><strong>사용 시점:</strong> 데이터셋이 비교적 균형 잡혀 있거나, 클래스별 성능보다 개별 예측 하나하나의 성공 여부가 더 중요할 때 사용한다.47</li>
</ul>
<table><thead><tr><th>평균 방식 (Average)</th><th>계산 방법 (Calculation Method)</th><th>클래스 불균형 처리 (Imbalance Handling)</th><th>적합한 사용 사례 (Best Use Case)</th></tr></thead><tbody>
<tr><td><strong>Macro</strong></td><td>각 클래스별 F1 점수의 단순 산술 평균</td><td>불균형을 고려하지 않음. 모든 클래스를 동등하게 취급.</td><td>모든 클래스가 동등하게 중요할 때. 소수 클래스 성능 저하에 민감하게 반응해야 할 때.</td></tr>
<tr><td><strong>Weighted</strong></td><td>각 클래스별 F1 점수를 해당 클래스의 샘플 수로 가중 평균</td><td>불균형을 반영함. 다수 클래스에 더 큰 가중치를 부여.</td><td>클래스별 중요도가 샘플 수에 비례한다고 판단될 때. 다수 클래스의 성능이 더 중요할 때.</td></tr>
<tr><td><strong>Micro</strong></td><td>전체 TP, FP, FN의 합으로 단일 F1 점수 계산 (정확도와 동일)</td><td>각 샘플을 동등하게 취급하여 다수 클래스의 영향이 큼.</td><td>전체적인 예측 정확도가 가장 중요할 때. 클래스별 성능보다 개별 예측의 성공 여부가 중요할 때.</td></tr>
</tbody></table>
<h2>7.  신뢰성 높은 평가를 위한 검증 방법론</h2>
<p>모델의 성능을 단 한 번의 데이터 분할로 평가하는 것은 결과의 신뢰성을 떨어뜨릴 수 있다. 보다 견고하고 일반화된 성능을 측정하기 위한 체계적인 검증 방법론이 필요하다.</p>
<h3>7.1  데이터 분할의 원칙: 훈련, 검증, 테스트</h3>
<p>모델의 일반화 성능을 객관적으로 평가하기 위해 전체 데이터를 목적에 따라 세 부분으로 분할하는 것이 표준적인 접근 방식이다.55</p>
<ul>
<li>
<p><strong>훈련 데이터셋 (Training Set):</strong> 모델을 직접 학습시키는 데 사용되는 데이터. 모델은 이 데이터의 패턴과 관계를 학습한다.6</p>
</li>
<li>
<p><strong>검증 데이터셋 (Validation Set):</strong> 모델 학습 과정 중에 성능을 중간 평가하고, 최적의 하이퍼파라미터를 찾기 위한 기준으로 사용된다. 모델이 훈련 데이터에 과적합되는 것을 방지하는 모니터링 역할을 한다.6</p>
</li>
<li>
<p><strong>테스트 데이터셋 (Test Set):</strong> 모델 개발과 하이퍼파라미터 튜닝이 모두 완료된 후, 최종적으로 선택된 모델의 일반화 성능을 ‘단 한 번’ 평가하기 위해 사용된다. 이 데이터는 모델 개발 과정에서 완전히 격리되어야 하며, 여기서 얻은 성능 점수가 모델의 최종 성능으로 보고된다.6</p>
</li>
</ul>
<h3>7.2  교차 검증 (Cross-Validation): 데이터의 효율적 활용과 신뢰성 확보</h3>
<p><strong>필요성:</strong> 데이터를 훈련/검증/테스트 세트로 한 번만 나누어 평가할 경우, 특정 데이터 분할에 성능이 우연히 좋거나 나쁘게 나오는 ’운’에 의해 평가 결과가 좌우될 수 있다. 또한, 검증 세트를 반복적으로 사용하여 모델을 튜닝하면 모델이 해당 검증 세트에 과적합될 위험이 존재한다.9</p>
<p><strong>K-Fold 교차 검증 (K-Fold Cross-Validation):</strong> 이러한 문제를 해결하기 위한 대표적인 방법이다. 전체 훈련 데이터를 K개의 동일한 크기의 부분집합(fold)으로 나눈다. 그 후, K-1개의 fold를 훈련 데이터로 사용하여 모델을 학습시키고, 나머지 1개 fold를 검증 데이터로 사용하여 성능을 평가한다. 이 과정을 각 fold가 한 번씩 검증 세트가 되도록 총 K번 반복한다. 최종 성능은 K번의 검증 과정에서 얻은 성능 점수들의 평균으로 산출한다.9</p>
<ul>
<li>
<p><strong>장점:</strong> 모든 데이터를 훈련과 검증에 한 번씩 사용하므로 데이터 활용 효율이 매우 높다. 여러 번의 평가를 통해 평균을 내므로, 단일 평가보다 훨씬 안정적이고 신뢰할 수 있는 성능 추정치를 제공한다.55</p>
</li>
<li>
<p><strong>단점:</strong> 모델을 K번 훈련하고 평가해야 하므로, 단일 분할 방식에 비해 계산 비용이 높고 시간이 오래 걸린다.55</p>
</li>
<li>
<p><strong>다양한 기법:</strong> 이 외에도 클래스 불균형을 고려하여 각 fold의 클래스 비율을 원본 데이터와 동일하게 유지하는 <strong>계층적 K-Fold 교차 검증(Stratified K-Fold)</strong>, 데이터의 그룹 구조를 고려하는 <strong>Group K-Fold</strong>, 시계열 데이터의 순서를 유지하는 <strong>Time Series Split</strong> 등 다양한 변형 기법이 존재한다.55</p>
</li>
</ul>
<h2>8.  결론: 종합적 성능 평가를 위한 제언</h2>
<p>인공지능 분류 모델의 성능을 올바르게 평가하는 것은 성공적인 모델 개발의 핵심이다. 본 안내서에서 논의된 바와 같이, 단일 평가 지표, 특히 정확도에만 의존하는 것은 모델의 실제 성능을 심각하게 오해할 소지를 낳으며, 이는 클래스 불균형이 존재하는 현실 세계의 데이터셋에서 특히 위험하다.</p>
<p>성공적인 모델 평가는 기술적인 측정 이전에, 해결하고자 하는 문제의 본질(예: 의료 진단과 스팸 필터의 각기 다른 오류 비용)과 데이터의 고유한 특성(예: 클래스 분포)을 깊이 이해하는 것에서 출발해야 한다. 이를 바탕으로 다음과 같은 종합적인 접근 방식을 취해야 한다.</p>
<ol>
<li>
<p><strong>다각적 지표 활용:</strong> 혼동 행렬을 평가의 출발점으로 삼고, 정밀도, 재현율, F1 점수 등 다양한 지표를 종합적으로 검토하여 모델의 강점과 약점을 다각도에서 파악해야 한다.</p>
</li>
<li>
<p><strong>시각적 분석 병행:</strong> ROC 곡선과 PR 곡선과 같은 시각적 도구를 활용하여, 분류 임계값 변화에 따른 모델의 행동 양식을 전체적으로 분석하고 미묘한 성능 차이를 발견해야 한다. 특히 불균형 데이터셋에서는 PR 곡선이 제공하는 통찰에 주목해야 한다.</p>
</li>
<li>
<p><strong>견고한 검증 방법론 적용:</strong> 평가 결과의 우연성을 최소화하고 모델의 일반화 가능성을 확보하기 위해, K-Fold 교차 검증과 같은 엄격한 검증 방법론을 적용하는 것이 필수적이다.</p>
</li>
</ol>
<p>결론적으로, 진정한 의미의 모델 평가는 하나의 숫자에 의존하는 것이 아니라, 다양한 지표와 시각화, 그리고 견고한 검증 절차를 통해 얻어지는 다각적인 분석과 깊은 통찰의 결과물이다. 이러한 종합적인 접근만이 신뢰할 수 있고 비즈니스 가치에 부합하는 인공지능 모델을 탄생시킬 수 있다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>www.ibm.com, <a href="https://www.ibm.com/kr-ko/think/topics/classification-models#:~:text=%EB%B6%84%EB%A5%98%20%EB%AA%A8%EB%8D%B8%EC%9D%80%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%ED%8F%AC%EC%9D%B8%ED%8A%B8,%EB%AA%A8%EB%8D%B8%EB%A7%81%EC%9D%98%20%ED%95%9C%20%EC%9C%A0%ED%98%95%EC%9E%85%EB%8B%88%EB%8B%A4.">https://www.ibm.com/kr-ko/think/topics/classification-models#:~:text=%EB%B6%84%EB%A5%98%20%EB%AA%A8%EB%8D%B8%EC%9D%80%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%ED%8F%AC%EC%9D%B8%ED%8A%B8,%EB%AA%A8%EB%8D%B8%EB%A7%81%EC%9D%98%20%ED%95%9C%20%EC%9C%A0%ED%98%95%EC%9E%85%EB%8B%88%EB%8B%A4.</a></li>
<li>분류 모델이란 무엇인가요? | IBM, https://www.ibm.com/kr-ko/think/topics/classification-models</li>
<li>[머신러닝] 분류 (Classification) - TaeKyoung’s Study Blog - 티스토리, https://taekyounglee1224.tistory.com/40</li>
<li>[Machine Learning] Classification Model | 분류 모델 - Archive - 티스토리, https://dad-rock.tistory.com/714</li>
<li>ML 모델의 종류(classification) - 끄적거림 - 티스토리, <a href="https://bin-kkwon.tistory.com/entry/ML-%EB%AA%A8%EB%8D%B8%EC%9D%98-%EC%A2%85%EB%A5%98classification">https://bin-kkwon.tistory.com/entry/ML-%EB%AA%A8%EB%8D%B8%EC%9D%98-%EC%A2%85%EB%A5%98classification</a></li>
<li>Evaluation Metrics for Classification Models | by Shweta Goyal | Analytics Vidhya - Medium, https://medium.com/analytics-vidhya/evaluation-metrics-for-classification-models-e2f0d8009d69</li>
<li>머신러닝 성능 지표란? | 퓨어스토리지 - Pure Storage, https://www.purestorage.com/kr/knowledge/machine-learning-performance-metrics.html</li>
<li>모델 검증이란? 정의, 중요성, 방법, 머신러닝 모델 | appen 에펜, https://kr.appen.com/blog/model-validation/</li>
<li>K폴드 교차 검증이란? - 야뤼송 - 티스토리, https://yarisong.tistory.com/75</li>
<li>머신러닝 평가 지표 (Machine Learning Metrics ) - 한입만쥬 - 티스토리, <a href="https://joy-home.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%ED%8F%89%EA%B0%80-%EC%A7%80%ED%91%9C-Machine-Learning-Metrics">https://joy-home.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%ED%8F%89%EA%B0%80-%EC%A7%80%ED%91%9C-Machine-Learning-Metrics</a></li>
<li>성능평가지표 (Evaluation Metric) - research notes - 티스토리, https://forest62590.tistory.com/9</li>
<li>What is a confusion matrix? - IBM, https://www.ibm.com/think/topics/confusion-matrix</li>
<li>Confusion Matrix: How To Use It &amp; Interpret Results [Examples] - V7 Labs, https://www.v7labs.com/blog/confusion-matrix-guide</li>
<li>Confusion matrix - Wikipedia, https://en.wikipedia.org/wiki/Confusion_matrix</li>
<li>Confusion Matrix. A confusion matrix is a table that is… | by Nikita Malviya | Medium, https://medium.com/@nikitamalviya/confusion-matrix-870739a1ec31</li>
<li>What is A Confusion Matrix in Machine Learning? The Model Evaluation Tool Explained, https://www.datacamp.com/tutorial/what-is-a-confusion-matrix-in-machine-learning</li>
<li>분류 모델 성능 평가 지표 - Confusion Matrix란? :: 정확도(Accuracy …, <a href="https://leedakyeong.tistory.com/entry/%EB%B6%84%EB%A5%98-%EB%AA%A8%EB%8D%B8-%EC%84%B1%EB%8A%A5-%ED%8F%89%EA%B0%80-%EC%A7%80%ED%91%9C-Confusion-Matrix%EB%9E%80-%EC%A0%95%ED%99%95%EB%8F%84Accuracy-%EC%A0%95%EB%B0%80%EB%8F%84Precision-%EC%9E%AC%ED%98%84%EB%8F%84Recall-F1-Score">https://leedakyeong.tistory.com/entry/%EB%B6%84%EB%A5%98-%EB%AA%A8%EB%8D%B8-%EC%84%B1%EB%8A%A5-%ED%8F%89%EA%B0%80-%EC%A7%80%ED%91%9C-Confusion-Matrix%EB%9E%80-%EC%A0%95%ED%99%95%EB%8F%84Accuracy-%EC%A0%95%EB%B0%80%EB%8F%84Precision-%EC%9E%AC%ED%98%84%EB%8F%84Recall-F1-Score</a></li>
<li>분류 모델 성능 평가 지표(Accuracy, Precision, Recall, F1 score 등) - Note - 티스토리, https://white-joy.tistory.com/9</li>
<li>[딥러닝]Evaluation Metric(평가 지표) - (1) 분류 성능 지표 - Meaningful AI, https://meaningful96.github.io/deeplearning/evaluation_metric/</li>
<li>[ML] 분류 성능 지표: Precision(정밀도), Recall(재현율), F1-score - 이것저것 테크블로그, <a href="https://ai-com.tistory.com/entry/ML-%EB%B6%84%EB%A5%98-%EC%84%B1%EB%8A%A5-%EC%A7%80%ED%91%9C-Precision%EC%A0%95%EB%B0%80%EB%8F%84-Recall%EC%9E%AC%ED%98%84%EC%9C%A8">https://ai-com.tistory.com/entry/ML-%EB%B6%84%EB%A5%98-%EC%84%B1%EB%8A%A5-%EC%A7%80%ED%91%9C-Precision%EC%A0%95%EB%B0%80%EB%8F%84-Recall%EC%9E%AC%ED%98%84%EC%9C%A8</a></li>
<li>Evaluation Metrics in Machine Learning - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/metrics-for-machine-learning-model/</li>
<li>분류 성능 평가 지표 : 불균형 데이터에는 어떤 평가 지표가 좋을까? - 오차행렬 완벽 정리, <a href="https://eatchu.tistory.com/entry/%EB%B6%84%EB%A5%98-%EC%84%B1%EB%8A%A5-%ED%8F%89%EA%B0%80-%EC%A7%80%ED%91%9C-%EB%B6%88%EA%B7%A0%ED%98%95-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%90%EB%8A%94-%EC%96%B4%EB%96%A4-%ED%8F%89%EA%B0%80-%EC%A7%80%ED%91%9C%EA%B0%80-%EC%A2%8B%EC%9D%84%EA%B9%8C-%EC%98%A4%EC%B0%A8%ED%96%89%EB%A0%AC-%EC%99%84%EB%B2%BD-%EC%A0%95%EB%A6%AC">https://eatchu.tistory.com/entry/%EB%B6%84%EB%A5%98-%EC%84%B1%EB%8A%A5-%ED%8F%89%EA%B0%80-%EC%A7%80%ED%91%9C-%EB%B6%88%EA%B7%A0%ED%98%95-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%90%EB%8A%94-%EC%96%B4%EB%96%A4-%ED%8F%89%EA%B0%80-%EC%A7%80%ED%91%9C%EA%B0%80-%EC%A2%8B%EC%9D%84%EA%B9%8C-%EC%98%A4%EC%B0%A8%ED%96%89%EB%A0%AC-%EC%99%84%EB%B2%BD-%EC%A0%95%EB%A6%AC</a></li>
<li>Classification: Accuracy, recall, precision, and related metrics | Machine Learning, https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall</li>
<li>분류 모델 성능 지표 : Precision, Recall, F1-Score - Dream to Reality - 티스토리, https://dream2reality.tistory.com/9</li>
<li>F1 Score in Machine Learning - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/f1-score-in-machine-learning/</li>
<li>F1 점수: 머신러닝 모델의 성능을 평가하는 강력한 지표 - Part 2, <a href="https://mozenworld.tistory.com/entry/F1-%EC%A0%90%EC%88%98-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EB%AA%A8%EB%8D%B8%EC%9D%98-%EC%84%B1%EB%8A%A5%EC%9D%84-%ED%8F%89%EA%B0%80%ED%95%98%EB%8A%94-%EA%B0%95%EB%A0%A5%ED%95%9C-%EC%A7%80%ED%91%9C-Part-2">https://mozenworld.tistory.com/entry/F1-%EC%A0%90%EC%88%98-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EB%AA%A8%EB%8D%B8%EC%9D%98-%EC%84%B1%EB%8A%A5%EC%9D%84-%ED%8F%89%EA%B0%80%ED%95%98%EB%8A%94-%EA%B0%95%EB%A0%A5%ED%95%9C-%EC%A7%80%ED%91%9C-Part-2</a></li>
<li>[Python] 불균형 데이터에 대한 분류 모델 성과평가 지표 (performance metrics of binary classification for imbalanced data), https://rfriend.tistory.com/774</li>
<li>불균형(imbalanced) 데이터 모델링은 ROC curve를 사용을 추천하지 …, https://julie-tech.tistory.com/121</li>
<li>[튜토리얼2] 불균형 데이터 분류하기 - 개발 노트, <a href="https://limjun92.github.io/assets/TensorFlow%202.0%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC/8.%20%EA%B5%AC%EC%A1%B0%ED%99%94%EB%90%9C%20%EB%8D%B0%EC%9D%B4%ED%84%B0/%5B%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC2%5D%20%EB%B6%88%EA%B7%A0%ED%98%95%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EB%B6%84%EB%A5%98%ED%95%98%EA%B8%B0/">https://limjun92.github.io/assets/TensorFlow%202.0%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC/8.%20%EA%B5%AC%EC%A1%B0%ED%99%94%EB%90%9C%20%EB%8D%B0%EC%9D%B4%ED%84%B0/%5B%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC2%5D%20%EB%B6%88%EA%B7%A0%ED%98%95%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EB%B6%84%EB%A5%98%ED%95%98%EA%B8%B0/</a></li>
<li>[머신러닝] 7. 지도학습(분류): 평가 - velog, <a href="https://velog.io/@posisugar31/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5%ED%8F%89%EA%B0%80">https://velog.io/@posisugar31/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5%ED%8F%89%EA%B0%80</a></li>
<li>[AI/ML] Precision이 중요한 경우와 Recall이 중요한 경우 - CV DOODLE - 티스토리, https://mvje.tistory.com/200</li>
<li>Precision, Recall, F1 스코어 등의 모델 평가 방법 - Soon’s Blog, https://meme2515.github.io/machine_learning/performance_measurement/</li>
<li>머신러닝 분류 모델의 평가 지표 - 정확도, 정밀도, 재현율, 민감도, F1 …, https://zhining.tistory.com/143</li>
<li>분류 모형 평가하기: 오분류표와 핵심 평가 지표들 - Allen’s 데이터 맛집, <a href="https://allensdatablog.tistory.com/entry/%EC%9E%91%EC%84%B1%EC%A4%91-7">https://allensdatablog.tistory.com/entry/%EC%9E%91%EC%84%B1%EC%A4%91-7</a></li>
<li>자연어 처리 Task 모델 평가지표(Precision, Recall, F-measure…) - 아는 것의 미학, https://applepy.tistory.com/215</li>
<li>인공지능 평가지표와 데이터 불균형 - velog, <a href="https://velog.io/@gooook/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%ED%8F%89%EA%B0%80%EC%A7%80%ED%91%9C%EC%99%80-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%88%EA%B7%A0%ED%98%95">https://velog.io/@gooook/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%ED%8F%89%EA%B0%80%EC%A7%80%ED%91%9C%EC%99%80-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%88%EA%B7%A0%ED%98%95</a></li>
<li>ROC Curves and Precision-Recall Curves for Imbalanced Classification - MachineLearningMastery.com, https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/</li>
<li>Understanding AUC — ROC and Precision-Recall Curves | by Maria Gusarova - Medium, https://medium.com/@data.science.enthusiast/auc-roc-curve-ae9180eaf4f7</li>
<li>
<ol start="10">
<li>ROC Curve - 공부하려고 만든 블로그 - 티스토리, https://welcome-to-dewy-world.tistory.com/11</li>
</ol>
</li>
<li>ROC-AUC, PR-AUC 개념 비교 정리 - data-minggeul - 티스토리, https://data-minggeul.tistory.com/10</li>
<li>An Overview of Classification Model Metrics | by ML and DL Explained - Medium, https://medium.com/@ml_dl_explained/an-overview-of-classification-model-metrics-8e25432d36ea</li>
<li>Machine Learning - 분류: ROC 및 AUC - Google for Developers, https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc?hl=ko</li>
<li>Classification: ROC and AUC | Machine Learning - Google for Developers, https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc</li>
<li>The Relationship Between Precision-Recall and ROC Curves - cs.wisc.edu, https://ftp.cs.wisc.edu/machine-learning/shavlik-group/davis.icml06.pdf</li>
<li>F1 Score vs ROC AUC vs Accuracy vs PR AUC: Which Evaluation Metric Should You Choose? - Neptune.ai, https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc</li>
<li>불균형 데이터 분류 | TensorFlow Core, https://www.tensorflow.org/tutorials/structured_data/imbalanced_data?hl=ko</li>
<li>Micro, Macro &amp; Weighted Averages of F1 Score, Clearly Explained | Towards Data Science, https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f/</li>
<li>다중 클래스 분류 모델 채점 - Qlik Help, https://help.qlik.com/ko-KR/cloud-services/Subsystems/Hub/Content/Sense_Hub/AutoML/scoring-multiclass-classification.htm</li>
<li>머신러닝 다중 분류 모델 성능평가에서 f1, recall, roc_auc 사용하기 - magicode - 티스토리, https://magicode.tistory.com/38</li>
<li>Micro vs Macro vs Weighted average : r/learnmachinelearning - Reddit, https://www.reddit.com/r/learnmachinelearning/comments/qpjdjf/micro_vs_macro_vs_weighted_average/</li>
<li>Understanding Micro, Macro, and Weighted Averages for Scikit-Learn metrics in multi-class classification with example - Amir Masoud Sefidian, https://iamirmasoud.com/2022/06/19/understanding-micro-macro-and-weighted-averages-for-scikit-learn-metrics-in-multi-class-classification-with-example/</li>
<li>f1_score — scikit-learn 1.7.1 documentation, https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html</li>
<li>f1-score 종류와 의미 (macro, weighted, micro) - data-minggeul - 티스토리, https://data-minggeul.tistory.com/11</li>
<li>[평가지표 정리] F1-score, Macro f1-score, Micro f1-score - Godspeed - 티스토리, https://dhlee-study.tistory.com/2</li>
<li>교차검증(Cross-Validation) - 이게또오류 - 티스토리, https://aigaeddo.tistory.com/44</li>
<li>[python] 교차검증(cross validation/クロスバリデーション/交差検証)과 그 방법들, https://engineer-mole.tistory.com/65</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>