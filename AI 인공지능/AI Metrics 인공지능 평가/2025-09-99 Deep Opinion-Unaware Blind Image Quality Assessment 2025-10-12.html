<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Deep Opinion-Unaware Blind Image Quality Assessment</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Deep Opinion-Unaware Blind Image Quality Assessment</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">인공지능 평가지표 (AI evaluation metrics)</a> / <span>Deep Opinion-Unaware Blind Image Quality Assessment</span></nav>
                </div>
            </header>
            <article>
                <h1>Deep Opinion-Unaware Blind Image Quality Assessment</h1>
<p>2025-09-12, G25DR</p>
<h2>1.  서론</h2>
<h3>1.1  이미지 품질 평가(IQA)의 정의와 중요성</h3>
<p>디지털 이미지는 획득, 압축, 전송, 저장 등 생명주기의 모든 단계에서 필연적으로 다양한 왜곡에 노출된다.1 이러한 품질 저하는 이미지에 담긴 지각적 정보의 손실을 유발하며, 후속 이미지 처리 및 컴퓨터 비전 시스템의 신뢰도에 직접적인 영향을 미친다.1 예를 들어, 의료 영상의 품질은 진단의 정확성과 직결되며, 자율주행 시스템의 카메라 센서가 포착한 이미지의 선명도는 객체 인식 성능을 좌우한다.1 이처럼 소셜 미디어 피드부터 산업용 모니터링 시스템에 이르기까지, 이미지 품질은 현대 기술 사회의 다양한 응용 분야에서 핵심적인 역할을 담당한다.5 따라서 인간의 시각적 인지와 일관된 방식으로 이미지의 품질을 자동으로 정량화하는 이미지 품질 평가(Image Quality Assessment, IQA) 기술은 수많은 시스템의 성능을 보장하고 최적화하기 위한 필수적인 전처리 작업으로 간주된다.1</p>
<h3>1.2  객관적 IQA의 분류: FR, RR, NR-IQA</h3>
<p>객관적 IQA 기술은 평가 과정에서 왜곡되지 않은 원본, 즉 참조 이미지(pristine reference image)의 사용 가능 여부에 따라 세 가지 주요 범주로 분류된다.1</p>
<ul>
<li><strong>Full-Reference (FR) IQA:</strong> 이 방식은 왜곡된 이미지와 완전한 참조 이미지를 모두 사용하여 두 이미지 간의 차이를 측정함으로써 품질을 평가한다.9 평균 제곱 오차(MSE)나 구조적 유사성 지수(SSIM)와 같은 고전적인 지표들이 여기에 속하며, 참조 정보가 완전하기 때문에 가장 높은 정확도를 보이는 경향이 있다.10 그러나 실제 응용 환경에서는 원본 이미지를 확보하는 것이 불가능한 경우가 대부분이므로 적용이 제한적이다.1</li>
<li><strong>Reduced-Reference (RR) IQA:</strong> FR-IQA와 NR-IQA의 절충안으로, 완전한 참조 이미지 대신 원본으로부터 추출된 일부 통계적 특징(예: 텍스처, 에지 정보)만을 활용하여 품질을 평가한다.9 이는 전송 대역폭이나 저장 공간이 제한된 실시간 모니터링 시스템 등에서 유용하게 사용될 수 있다.10</li>
<li><strong>No-Reference (NR) IQA / Blind IQA (BIQA):</strong> 이 방식은 어떠한 참조 정보도 없이 왜곡된 이미지 자체의 픽셀 정보나 통계적 특성만을 분석하여 품질을 예측한다.4 참조 이미지가 필요 없기 때문에 가장 현실적이고 광범위한 적용 가능성을 가지며, ’블라인드 이미지 품질 평가(Blind Image Quality Assessment, BIQA)’라고도 불린다.1 본 보고서는 가장 도전적이면서도 실용적인 이 BIQA 패러다임에 초점을 맞춘다.</li>
</ul>
<h3>1.3  BIQA의 두 가지 패러다임: Opinion-Aware (OA) vs. Opinion-Unaware (OU)</h3>
<p>BIQA 방법론은 모델 훈련 과정에서 인간의 주관적 평가 점수를 활용하는지 여부에 따라 다시 두 가지 패러다임으로 나뉜다.13</p>
<ul>
<li><strong>Opinion-Aware (OA) BIQA:</strong> 이 접근법은 다수의 관찰자가 특정 이미지의 품질에 대해 매긴 주관적 점수의 평균인 ’평균 의견 점수(Mean Opinion Score, MOS)’를 정답 레이블로 사용한다.1 모델은 왜곡된 이미지의 시각적 특징과 MOS 간의 관계를 매핑하는 회귀 함수를 학습하는 지도 학습(supervised learning) 방식으로 훈련된다.1 딥러닝 기술의 발전과 함께 CNN 기반의 OA-BIQA 모델들이 높은 성능을 달성했으나, 근본적인 한계에 직면했다.1 MOS를 수집하는 주관적 평가는 통제된 환경에서 다수의 인원이 참여해야 하므로 막대한 시간과 비용이 소요된다.15 이로 인해 대부분의 IQA 데이터셋은 수천 장 규모에 불과하여, 수백만 개의 파라미터를 가진 딥러닝 모델을 훈련시키기에는 턱없이 부족하다.13 이러한 ’작은 데이터 문제(small-sample problem)’는 모델의 과적합(overfitting)을 유발하고, 훈련 데이터에 존재하지 않는 새로운 유형의 왜곡에 대한 일반화 성능을 심각하게 저하시킨다.1</li>
<li><strong>Opinion-Unaware (OU) BIQA:</strong> 이러한 OA 방식의 한계를 극복하기 위해 제안된 패러다임으로, 인간의 주관적 점수 없이 모델을 훈련하는 것을 목표로 한다.1 OU-BIQA는 값비싼 주관적 평가 데이터에 대한 의존성을 제거함으로써 데이터 주석 과정의 병목 현상을 해결한다.15 또한, 서로 다른 데이터셋 간에 존재하는 평가 환경 및 참여자 구성의 차이에서 비롯되는 주관성의 격차를 줄여, 모델의 일반화 성능과 강건성(robustness)을 향상시킬 수 있는 큰 잠재력을 가진다.14 OU-BIQA의 등장은 단순히 기술적 대안을 넘어, 딥러닝 시대의 ’데이터 병목 현상’이라는 근본적인 문제에 대한 컴퓨터 비전 분야의 한 가지 해법을 제시한다. 이는 고품질 레이블을 얻기 어려운 모든 분야에서 딥러닝을 적용하기 위한 ’학습 패러다임 전환’의 필요성을 보여주는 구체적인 사례이며, 약지도 학습(weakly supervised)이나 자기지도 학습(self-supervised)과 같은 현대 딥러닝의 주요 흐름과 맥을 같이한다.1</li>
</ul>
<h3>1.4  보고서의 구성</h3>
<p>본 보고서는 먼저 OU-BIQA의 이론적 기반이 되는 자연 영상 통계(NSS)를 고찰한다. 이후, 딥러닝 기술을 활용한 최신 OU-BIQA 방법론을 유사 레이블링, 순위 학습, 비지도/자기지도 학습, 하이브리드 아키텍처 등 네 가지 주요 갈래로 나누어 심층 분석한다. 또한, OU-BIQA 분야의 핵심 난제인 합성-실제 왜곡 간의 도메인 격차 문제와 그 해결 방안을 논의한다. 마지막으로, 주요 벤치마크 데이터셋과 평가 지표를 소개하고, SOTA 모델들의 성능을 종합적으로 비교 분석하며 결론 및 향후 연구 방향을 제시한다.</p>
<h2>2.  전통적 Opinion-Unaware BIQA의 원리: 자연 영상 통계</h2>
<h3>2.1  NSS 가설의 핵심</h3>
<p>딥러닝 이전 시대의 OU-BIQA 연구는 ’자연 영상 통계(Natural Scene Statistics, NSS)’라는 강력한 가설 위에 구축되었다.14 이 가설의 핵심은 왜곡되지 않은 자연 영상(natural scenes)은 이미지의 구체적인 내용(content)과 무관하게 보편적으로 공유되는 특정 통계적 규칙성(statistical regularities)을 가진다는 것이다.21 예를 들어, 자연 영상의 푸리에 변환 스펙트럼은 주파수 <code>$f$</code>에 대해 대략 <code>$1/f^{\gamma}$</code> 형태로 감소하는 경향을 보인다.21 인간의 시각 시스템(Human Visual System, HVS)은 오랜 진화와 발달 과정을 통해 이러한 ’자연스러움(naturalness)’에 대한 강력한 사전 지식을 내재화했으며, 이미지에 왜곡이 가해지면 이러한 통계적 규칙성이 깨지면서 시각적으로 ’부자연스러움(unnaturalness)’을 감지하게 된다.20 따라서, NSS 기반 BIQA는 왜곡된 이미지에서 나타나는 통계적 편차를 정량적으로 측정하여 품질 저하 정도를 예측하는 것을 목표로 한다.21</p>
<p>이러한 접근 방식은 통계학적 관점에서 ’정상성 모델링(Normality Modeling)’이라는 이상 탐지(anomaly detection) 프레임워크를 IQA 문제에 적용한 것으로 해석될 수 있다. 여기서 ‘정상(normal)’ 상태는 고품질 자연 영상이 공유하는 통계적 분포로 정의되며, 왜곡은 이 정상 분포로부터의 ‘편차(deviation)’ 또는 ’이상(anomaly)’으로 간주된다.21 즉, NSS 기반 OU-BIQA는 품질 평가 문제를 ’고품질 영상의 통계적 모델로부터 얼마나 벗어났는가’를 측정하는 문제로 재정의했으며, 이는 이후 딥러닝 기반의 재구성(reconstruction) 방법론의 철학적 토대를 마련했다.</p>
<h3>2.2  NSS 기반 주요 모델 및 특징 추출</h3>
<p>초기 NSS 기반 모델들은 주로 공간 영역(spatial domain)이나 웨이블릿(wavelet)과 같은 변환 영역(transformed domain)에서 다양한 통계적 특징들을 수작업(hand-crafted)으로 설계하여 추출했다.14</p>
<ul>
<li><strong>DIIVINE (Distortion Identification-based Image Verity and INtegrity Evaluation):</strong> 2단계 프레임워크를 제안한 선구적인 모델이다. 1단계에서는 왜곡된 이미지로부터 추출한 NSS 특징을 사용하여 해당 이미지에 어떤 유형의 왜곡(예: 블러, JPEG 압축, 노이즈)이 존재하는지를 먼저 식별한다. 2단계에서는 식별된 왜곡 유형에 특화된 품질 평가 모델을 적용하여 최종 점수를 산출한다.21</li>
<li><strong>BRISQUE (Blind/Referenceless Image Spatial Quality Evaluator):</strong> 공간 영역에서 직접 특징을 추출하는 효율적인 모델이다. 이미지의 휘도(luminance) 값에 대해 국부적으로 평균을 빼고 분산으로 나누는 ‘평균 차감 대비 정규화(Mean Subtracted Contrast Normalization, MSCN)’ 계수를 계산하고, 이 계수들의 분포를 모델링하여 품질 예측에 사용한다.23</li>
<li><strong>NIQE (Natural Image Quality Evaluator):</strong> 고품질의 자연 영상 패치들로부터 NSS 특징을 추출하여 기준이 되는 다변수 가우시안(Multivariate Gaussian, MVG) 모델을 미리 학습한다. 이후 평가 대상 이미지의 패치들로부터 동일한 NSS 특징을 추출하고, 이들의 MVG 모델과 기준 모델 간의 거리를 측정하여 품질 점수로 변환한다.12</li>
<li>이 외에도 웨이블릿 계수 간의 상호 의존성 변화를 측정한 <strong>TMIQ</strong> 20, DCT(Discrete Cosine Transform) 통계를 활용한 <strong>BLIINDS</strong> 17 등 다양한 모델들이 제안되었다.</li>
</ul>
<h3>2.3  NSS 방법론의 성과와 한계</h3>
<p>NSS 기반 방법론은 인간의 주관적 점수 없이도 특정 유형의 합성 왜곡에 대해 높은 예측 성능을 보이며 OU-BIQA의 가능성을 성공적으로 입증했다. 특히 NIQE와 같이 훈련 과정 없이(training-free) 작동하는 모델들은 매우 효율적이라는 장점을 가졌다.14</p>
<p>그러나 이러한 전통적인 접근법은 명확한 한계를 보였다. 가장 큰 문제는 실제 세계에서 발생하는 ‘야생의(in-the-wild)’ 왜곡에 대한 일반화 성능이 부족하다는 점이다.20 실제 왜곡은 카메라 센서 노이즈, 렌즈 수차, 열악한 조명 조건 등 여러 요인이 복합적으로 작용하여 발생하므로, 몇 가지 통계적 규칙성만으로는 정확하게 모델링하기 어렵다.20 또한, 특징 추출 과정이 특정 왜곡 유형을 가정하고 수작업으로 설계되었기 때문에, 새로운 유형의 왜곡에 유연하게 대처하는 확장성이 부족하며, 특징 표현 능력 자체에도 한계가 있었다.24 이러한 한계는 보다 강력하고 데이터 기반의 특징 학습이 가능한 딥러닝 방법론의 등장을 촉진하는 계기가 되었다.</p>
<h2>3.  딥러닝 기반 Opinion-Unaware BIQA 방법론 심층 분석</h2>
<p>딥러닝의 발전은 수작업 특징 추출의 한계를 극복하고, 대규모 데이터로부터 직접적으로 품질을 인식할 수 있는 표현(quality-aware representation)을 학습하는 시대를 열었다.20 딥러닝 기반 OU-BIQA 연구는 ’인간의 주관적 점수’라는 값비싼 정답지를 어떻게 대체할 것인가에 대한 다양한 아이디어를 중심으로 발전해왔다. 이는 크게 유사 레이블링, 순위 학습, 비지도/자기지도 학습, 하이브리드 아키텍처의 네 가지 흐름으로 분류할 수 있다. 이러한 발전 과정은 ’지식의 원천’을 어디서 확보하고 어떻게 활용하는지에 대한 탐구의 역사로 볼 수 있다. 초기에는 정확한 FR-IQA 모델을 지식의 원천으로 삼았고, 이후에는 통제된 왜곡 생성 프로세스 자체의 규칙을, 최근에는 ImageNet과 같은 대규모 비-IQA 데이터셋에 내재된 방대한 시각적 세계의 통계적 구조를 지식의 원천으로 활용하는 방향으로 진화하고 있다.</p>
<h3>3.1  유사 레이블링 및 다중 주석자 활용</h3>
<p>이 접근법의 핵심 아이디어는 인간 MOS를 직접 사용하는 대신, 성능이 검증된 FR-IQA 모델들을 ’알고리즘적 주석자(algorithmic annotators)’로 간주하여 대규모의 왜곡 이미지에 대한 유사 품질 점수(pseudo quality scores)나 순위 레이블을 자동으로 생성하고, 이를 딥러닝 모델의 훈련 데이터로 활용하는 것이다.13 이는 인간 전문가를 알고리즘 전문가로 대체하는 개념이지만, FR-IQA 모델 자체의 성능이 전체 시스템의 성능 상한선을 결정한다는 한계를 내포한다.</p>
<ul>
<li><strong>BLISS (Blind Learning of Image Quality using Synthetic Scores):</strong> FR-IQA 모델이 산출한 합성 점수(synthetic scores)를 정답 레이블로 사용하여 BIQA 모델을 훈련하는 방식을 최초로 제안한 연구 중 하나다.13</li>
<li>*<strong>Deep Opinion-Unaware BIQA model by learning and adapting from Multiple Annotators* (DUBMA):</strong> 이 모델은 한 단계 더 나아가 여러 개의 FR-IQA 모델을 다중 주석자로 활용한다.15 먼저 대규모의 왜곡 이미지 쌍을 생성한 뒤, 각 FR-IQA 모델이 이 쌍에 대해 상대적인 품질 순위를 매기도록 한다.20 이후 순위 학습(Learn-to-Rank, L2R) 접근법을 통해 여러 ‘잡음이 있는(noisy)’ 주석 결과로부터 공동으로 학습한다. 특히, 각 주석자(FR-IQA 모델)의 신뢰도를 별도의 파라미터로 두고 최대 가능도 추정(Maximum Likelihood Estimation, MLE)을 통해 암시적으로 추정함으로써, 신뢰도가 높은 주석자의 의견에 가중치를 부여하는 강건한 레이블 융합(robust label fusion) 메커니즘을 구현했다.20</li>
</ul>
<h3>3.2  순위 학습 기반 접근법</h3>
<p>순위 학습(Learning-to-Rank, L2R)은 이미지의 절대적인 품질 점수를 직접 예측하는 어려운 회귀(regression) 문제 대신, 두 이미지 중 어느 것의 품질이 더 나은지를 판별하는 상대적인 비교(pairwise comparison) 문제를 푸는 데 집중한다.25 상대적 순위 레이블은 “왜곡 강도 A가 B보다 약하면, 이미지 A의 품질은 B보다 좋다“와 같이 명확하고 확장 가능한 규칙을 통해 자동으로 대량 생성할 수 있어, 주관성의 영향을 덜 받고 데이터 구축이 용이하다는 장점이 있다.25</p>
<ul>
<li>*<strong>RankIQA: Learning from Rankings for No-Reference Image Quality Assessment*:</strong> 동일한 원본 이미지에 점진적으로 강도를 높여가며 왜곡을 적용함으로써, 품질 순위가 명확하게 알려진 이미지 집합을 자동으로 생성한다.25 이후 샴 네트워크(Siamese Network)를 사용하여 두 이미지를 입력받아 특징을 각각 추출하고, 이들의 품질 순위를 예측하도록 훈련한다. 이렇게 순위 학습을 통해 사전 훈련된 네트워크는 이미지 품질에 민감한 특징 표현을 학습하게 되며, 소량의 실제 IQA 데이터에 미세 조정(fine-tuning)하는 과정을 거쳐 절대적인 품질 점수를 예측하는 모델로 변환된다.25</li>
<li><strong>dipIQ (DIP inferred quality):</strong> 대규모 이미지 데이터베이스에서 내용이 유사하지만 품질 차이가 뚜렷한 ’품질 구별 가능 이미지 쌍(quality-discriminable image pairs, DIPs)’을 자동으로 대량 발굴한다.13 그리고 RankNet이라는 L2R 알고리즘을 사용하여 이 이미지 쌍들의 상대적 품질 순서를 학습하는 OU-BIQA 모델을 구축한다.28</li>
</ul>
<h3>3.3  비지도 및 자기지도 학습</h3>
<p>이 패러다임은 외부에서 제공되는 레이블 없이 데이터 자체의 내재적 구조를 활용하여 유용한 표현을 학습하는 것을 목표로 한다. 최근 OU-BIQA 분야에서 가장 활발히 연구되는 방향이다.</p>
<ul>
<li><strong>재구성 기반 방법론:</strong> 이 방법은 고품질(pristine) 이미지만을 사용하여 오토인코더(Autoencoder)나 변분 오토인코더(VAE)와 같은 생성 모델을 훈련시킨다.1 훈련된 모델은 고품질 이미지의 잠재 분포(latent distribution)를 학습하게 된다. 이후 왜곡된 이미지를 입력으로 넣었을 때 발생하는 높은 재구성 오류(reconstruction error)나, 학습된 정상 분포로부터의 통계적 편차(deviation)를 품질 저하의 척도로 활용한다.1 <em>Opinion Unaware Image Quality Assessment via Adversarial Convolutional Variational Autoencoder</em>는 이러한 아이디어를 적대적 학습과 결합하여 구현한 대표적인 모델이다.1</li>
<li><strong>대조 학습(Contrastive Learning) 기반 방법론:</strong> 명시적인 레이블 대신, 데이터 증강(data augmentation)을 통해 생성된 긍정적 쌍(positive pair)과 부정적 쌍(negative pair)을 이용하여 표현을 학습하는 자기지도 학습(self-supervised learning)의 한 갈래다.30 BIQA에서는 왜곡의 유형과 강도를 조절하여 긍정/부정 쌍을 정의하는 보조 작업(pretext task)을 설정한다. 예를 들어, 동일한 왜곡 이미지에서 추출된 다른 패치들은 품질이 유사하므로 긍정적 쌍으로, 동일한 원본 이미지에 서로 다른 왜곡을 적용한 패치들은 품질이 다르므로 부정적 쌍으로 간주할 수 있다.31</li>
<li>*<strong>CONTRIQUE (CONtrastive Image QUality Evaluator):*</strong> 왜곡 유형 및 강도 예측을 보조 작업으로 설정하고, 대조적 손실 함수를 사용하여 CNN 모델을 처음부터 훈련한다. 이 과정을 통해 학습된 특징 추출기는 이미지 품질 변화에 매우 민감한 표현을 학습하게 되며, 이후 간단한 선형 회귀 모델과 결합되어 최종 품질 점수를 예측하는 데 사용된다.23</li>
<li><strong>희소 표현(Sparse Representation) 기반 방법론:</strong></li>
<li>*<strong>UNIQUE (Unsupervised Image Quality Estimation):*</strong> 대규모의 레이블 없는 이미지 데이터베이스로부터 비지도 학습 방식으로 희소 표현(sparse representations)을 학습하는 선형 디코더(linear decoder)를 훈련한다.32 이는 인간 시각 시스템이 정보를 효율적으로 부호화하는 방식을 모방한 것이다. 품질 점수는 원본 이미지와 왜곡된 이미지의 희소 표현 벡터 간의 차이를 기반으로 계산된다.32</li>
</ul>
<h3>3.4  하이브리드 및 특화 아키텍처</h3>
<p>최근 연구들은 단일 패러다임에 의존하기보다, 여러 접근법의 장점을 결합하여 시너지를 창출하는 하이브리드 아키텍처를 제안하고 있다.</p>
<ul>
<li><strong>딥 피처와 통계 모델의 결합:</strong> 이 접근법은 딥러닝 모델의 강력한 일반 시각 특징 추출 능력과 전통적인 NSS 기반 통계 모델의 강건성을 결합한다.14</li>
<li>*<strong>Multi-Scale Deep Feature Statistics* (MDFS):</strong> ImageNet과 같은 대규모 데이터셋으로 사전 훈련된 비전 모델(예: ResNet)을 특징 추출기로 사용하여 다중 스케일 특징을 얻는다. 이후, 이 딥 피처(deep features)들의 통계적 분포를 전통적인 NIQE 방식과 유사하게 다변수 가우시안(MVG) 모델로 피팅한다. 최종 품질 점수는 평가 대상 이미지의 MVG 모델과 고품질 이미지들로 구축된 벤치마크 MVG 모델 간의 통계적 거리를 측정하여 계산된다.14 이는 NIQE의 아이디어를 딥러닝 특징 공간으로 확장한 현대적인 해석이라 할 수 있다.</li>
<li><strong>다중 스트림 아키텍처:</strong> 서로 다른 특성을 가진 왜곡 유형을 별도의 전문화된 네트워크 스트림에서 처리한 후, 그 결과를 지능적으로 통합하는 방식이다.2</li>
<li>*<strong>Deep Bilinear Convolutional Neural Network* (DB-CNN):</strong> 이 모델은 합성 왜곡과 실제 왜곡이라는 두 가지 시나리오를 명시적으로 분리하여 처리하는 2-스트림 구조를 가진다.2</li>
<li><strong>합성 왜곡 스트림 (S-CNN):</strong> 왜곡의 유형과 수준을 분류하도록 사전 훈련되어 합성 왜곡에 특화된 특징을 학습한다.</li>
<li><strong>실제 왜곡 스트림 (A-CNN):</strong> ImageNet으로 사전 훈련된 VGG-16 모델을 사용하여, 실제 이미지에 나타나는 복잡하고 일반적인 시각적 특징을 추출한다.</li>
<li>두 스트림에서 추출된 특징 벡터들은 **빌리니어 풀링(Bilinear Pooling)**을 통해 결합된다. 이는 두 특징 벡터의 외적(outer product)을 계산하는 것과 유사하며, 두 종류의 왜곡 특징 간의 모든 쌍별 상호작용을 풍부하게 모델링하여 강력한 통합 표현을 생성한다.2</li>
</ul>
<p>상당수의 최신 OU-BIQA 모델들은 ’의견-비인식’이라는 명칭에도 불구하고, 실제로는 ImageNet과 같이 거대한 규모의 인간 주석(객체 클래스 레이블)으로 사전 훈련된 모델에 암묵적으로 의존하고 있다.14 이는 ’완전한 비지도’라기보다는, 특정 작업(IQA)에 대한 주석은 없지만 다른 작업(분류)에 대한 방대한 주석으로부터 얻은 ’전이된 지식(transferred knowledge)’을 활용하는 것이다. 따라서 OU-BIQA의 성공은 IQA 데이터의 부족 문제를 대규모 일반 시각 데이터로 극복하는 전이 학습(transfer learning)의 성공적인 사례로도 평가될 수 있다.</p>
<table><thead><tr><th><strong>모델명 (원문)</strong></th><th><strong>발표 연도</strong></th><th><strong>핵심 원리 (학습 패러다임)</strong></th><th><strong>기반 기술</strong></th><th><strong>장점</strong></th><th><strong>단점/한계</strong></th></tr></thead><tbody>
<tr><td><strong>DUBMA</strong></td><td>2025</td><td>유사 레이블링 (다중 주석자)</td><td>L2R, Optimal Transport UDA</td><td>여러 FR-IQA 모델의 지식을 융합하고, 도메인 적응을 통해 실제 왜곡에 강건함 15</td><td>FR-IQA 모델들의 성능에 의존적이며, UDA 과정이 복잡할 수 있음</td></tr>
<tr><td><strong>RankIQA</strong></td><td>2017</td><td>순위 학습 (L2R)</td><td>Siamese Network</td><td>대규모 순위 데이터를 자동으로 생성하여 데이터 부족 문제 해결, 사전 훈련 후 미세 조정 방식 25</td><td>절대 점수 예측을 위해 추가적인 미세 조정 단계가 필요함</td></tr>
<tr><td><strong>dipIQ</strong></td><td>2019</td><td>순위 학습 (L2R)</td><td>RankNet</td><td>실제 데이터베이스에서 품질 차이가 나는 쌍을 자동으로 발굴하여 학습에 활용 13</td><td>이미지 쌍 발굴 알고리즘의 성능에 따라 모델 성능이 좌우될 수 있음</td></tr>
<tr><td><strong>CONTRIQUE</strong></td><td>2021</td><td>자기지도 학습 (대조 학습)</td><td>CNN, Contrastive Loss</td><td>레이블 없이 왜곡 유형/강도 예측 보조 작업을 통해 품질 인식 표현을 학습 23</td><td>효과적인 보조 작업 설계가 중요하며, 실제 왜곡의 다양성을 반영하기 어려울 수 있음</td></tr>
<tr><td><strong>UNIQUE</strong></td><td>2016</td><td>비지도 학습</td><td>Sparse Representation</td><td>레이블 없는 이미지로부터 희소 코딩을 통해 인간 시각 시스템과 유사한 특징을 학습 32</td><td>딥러닝 기반 모델에 비해 표현력에 한계가 있을 수 있음</td></tr>
<tr><td><strong>MDFS</strong></td><td>2024</td><td>하이브리드 (딥 피처 + 통계)</td><td>Pre-trained CNN + MVG</td><td>사전 훈련된 모델의 강력한 특징과 NSS 모델의 강건성을 결합, 훈련 데이터 불필요 14</td><td>사전 훈련된 모델의 특성에 성능이 크게 의존함</td></tr>
<tr><td><strong>DB-CNN</strong></td><td>2020</td><td>하이브리드 (다중 스트림)</td><td>Bilinear Pooling</td><td>합성 왜곡과 실제 왜곡을 별도의 스트림에서 전문적으로 처리하여 성능 극대화 2</td><td>아키텍처가 복잡하고, 두 스트림을 모두 훈련/관리해야 하는 부담이 있음</td></tr>
</tbody></table>
<h2>4.  핵심 과제: 합성-실제 왜곡 간의 도메인 격차</h2>
<h3>4.1  문제의 정의</h3>
<p>OU-BIQA 연구의 가장 큰 장점 중 하나는 합성 왜곡(synthetic distortions)을 통해 학습 데이터를 거의 무한히 생성할 수 있다는 점이다.20 그러나 이 강력한 장점은 생성된 데이터가 실제 데이터와 통계적 분포가 다를 경우 무의미해지는데, 이것이 바로 ‘합성-실제 도메인 격차(synthetic-to-authentic domain gap)’ 문제다.20</p>
<p>합성 왜곡은 가우시안 블러, JPEG 압축, 백색 노이즈 등 수학적으로 잘 정의되고 단일 유형으로 제어하기 쉬운 특성을 가진다.2 반면, 실제 왜곡(authentic distortions)은 스마트폰 카메라로 촬영된 사진처럼, 카메라 센서의 노이즈, 렌즈의 광학적 결함, 촬영 당시의 조명 조건, 후처리 알고리즘 등 예측 불가능하고 복합적인 여러 요인들의 상호작용으로 발생한다.15 이로 인해 합성 왜곡 데이터로만 훈련된 BIQA 모델을 실제 왜곡 이미지에 적용하면 성능이 급격히 저하되는 도메인 이동(domain shift) 현상이 발생한다.20 소셜 미디어, 자율주행, CCTV 등 BIQA 기술의 주요 실제 응용 분야는 모두 ‘실제 왜곡’ 환경이므로 5, 이 도메인 격차 문제는 OU-BIQA 기술이 실험실 수준의 연구를 넘어 실제 산업 현장에 적용되기 위해 반드시 해결해야 할 핵심적인 허들이다.</p>
<h3>4.2  해결을 위한 접근법</h3>
<p>이 도메인 격차 문제를 해결하기 위한 접근법들은 단순한 성능 향상 기법을 넘어, OU-BIQA 기술의 실용화를 위한 핵심적인 돌파구로 평가된다.</p>
<ul>
<li><strong>특화 아키텍처 (Specialized Architecture):</strong> DB-CNN 모델은 이 문제를 아키텍처 수준에서 해결하려는 시도다.2 합성 왜곡에 특화된 스트림(S-CNN)과 실제 왜곡에 강건한 일반 시각 특징을 추출하는 스트림(A-CNN)을 분리하여 각 도메인에 전문화된 표현을 학습한다. 이후 빌리니어 풀링을 통해 두 정보를 효과적으로 통합함으로써, 두 도메인 모두에서 높은 성능을 달성하고자 한다.3</li>
<li><strong>비지도 도메인 적응 (Unsupervised Domain Adaptation, UDA):</strong> 이 접근법은 레이블이 풍부한 소스 도메인(합성 왜곡)에서 학습한 모델을 레이블이 없는 타겟 도메인(실제 왜곡) 데이터에 적응시키는 것을 목표로 한다. DUBMA 모델이 제안한 UDA 전략이 대표적이다.15</li>
<li>DUBMA는 이상치에 강건한(outlier-robust) UDA를 위해 최적 수송(Optimal Transport, OT) 이론을 도입했다.15 OT는 한 확률 분포를 다른 확률 분포로 변환하는 데 필요한 최소 비용을 계산하는 수학적 프레임워크다. DUBMA는 이를 활용하여 소스 도메인과 타겟 도메인의 특징-레이블 결합 분포 간의 거리를 측정하고, 이 거리를 최소화하는 방향으로 모델을 업데이트한다.20 이를 통해 두 도메인 간의 통계적 분포 차이를 효과적으로 줄여, 합성 데이터로 학습된 모델이 실제 왜곡 데이터에 대해서도 높은 예측 정확도를 보이도록 조정한다.20</li>
</ul>
<h2>5.  성능 평가 및 비교 분석</h2>
<h3>5.1  주요 벤치마크 데이터셋</h3>
<p>OU-BIQA 모델의 성능을 객관적으로 평가하고 비교하기 위해서는 표준화된 벤치마크 데이터셋의 역할이 매우 중요하다. 이들 데이터셋은 크게 통제된 환경에서 인공적으로 왜곡을 생성한 ’합성 왜곡 데이터셋’과, 실제 촬영 환경에서 자연스럽게 발생한 왜곡을 포함하는 ’실제 왜곡 데이터셋’으로 나뉜다.</p>
<ul>
<li><strong>합성 왜곡 데이터셋:</strong></li>
<li><strong>LIVE IQA Database:</strong> 29개의 고품질 참조 이미지에 5가지 유형(JPEG2000, JPEG, 백색 노이즈, 가우시안 블러, 고속 페이딩 채널 오류)의 왜곡을 적용하여 총 779개의 왜곡 이미지를 포함한다. 초기 BIQA 연구의 표준 벤치마크로 널리 사용되었다.9</li>
<li><strong>TID2013:</strong> 25개의 참조 이미지에 24가지의 다양한 왜곡 유형을 5가지 수준으로 적용하여 총 3,000개의 왜곡 이미지를 구축했다. LIVE보다 왜곡 유형의 다양성이 훨씬 풍부하다.40</li>
<li><strong>CSIQ:</strong> 30개의 참조 이미지에 6가지 왜곡 유형(JPEG, JPEG2000, 전역 대비 감소, 핑크 가우시안 노이즈, 가우시안 블러)을 4~5가지 수준으로 적용하여 총 866개의 왜곡 이미지를 포함한다.42</li>
<li><strong>KADID-10k:</strong> 81개의 참조 이미지에 25가지 왜곡 유형을 적용하여 10,100여 개의 대규모 왜곡 이미지를 제공한다.11</li>
<li><strong>실제 왜곡 데이터셋:</strong></li>
<li><strong>LIVE In the Wild Image Quality Challenge Database (LIVE-C):</strong> 실제 모바일 기기로 촬영된 1,162개의 ‘야생’ 이미지를 포함한다. 인공적인 왜곡 없이, 촬영, 처리, 저장 과정에서 자연스럽게 발생한 복합적인 왜곡을 다룬다. 품질 평가는 아마존 메카니컬 터크(Amazon Mechanical Turk)를 이용한 대규모 크라우드소싱 방식으로 진행되었다.37</li>
<li><strong>KonIQ-10k:</strong> 10,073개의 다양한 실제 왜곡 이미지를 포함하는 대규모 데이터베이스로, ‘생태학적으로 유효한(ecologically valid)’ 평가를 목표로 구축되었다.18</li>
</ul>
<p>이 외에도 PIPAL, BIQ2021 등 GAN(Generative Adversarial Network) 기반 왜곡이나 최신 스마트폰으로 촬영된 이미지를 포함하는 새로운 데이터셋이 지속적으로 등장하며 BIQA 연구의 발전을 견인하고 있다.11</p>
<table><thead><tr><th><strong>데이터셋 이름</strong></th><th><strong>발표 연도</strong></th><th><strong>참조 이미지 수</strong></th><th><strong>왜곡 이미지 수</strong></th><th><strong>왜곡 유형 수</strong></th><th><strong>왜곡 종류</strong></th><th><strong>평가 방식</strong></th></tr></thead><tbody>
<tr><td><strong>LIVE</strong></td><td>2004</td><td>29</td><td>779</td><td>5</td><td>합성</td><td>실험실</td></tr>
<tr><td><strong>TID2013</strong></td><td>2013</td><td>25</td><td>3,000</td><td>24</td><td>합성</td><td>실험실</td></tr>
<tr><td><strong>CSIQ</strong></td><td>2009</td><td>30</td><td>866</td><td>6</td><td>합성</td><td>실험실</td></tr>
<tr><td><strong>KADID-10k</strong></td><td>2019</td><td>81</td><td>10,100</td><td>25</td><td>합성</td><td>-</td></tr>
<tr><td><strong>LIVE Challenge</strong></td><td>2015</td><td>0</td><td>1,162</td><td>-</td><td>실제</td><td>크라우드소싱</td></tr>
<tr><td><strong>KonIQ-10k</strong></td><td>2020</td><td>0</td><td>10,073</td><td>-</td><td>실제</td><td>크라우드소싱</td></tr>
</tbody></table>
<h3>5.2  평가 지표</h3>
<p>BIQA 모델이 예측한 품질 점수가 인간의 주관적 평가(MOS)와 얼마나 잘 일치하는지를 측정하기 위해 주로 두 가지 상관계수 지표가 사용된다.</p>
<ul>
<li><strong>Spearman Rank-Order Correlation Coefficient (SRCC):</strong> 예측 점수와 MOS의 순위(rank) 간의 상관관계를 측정한다. 예측 값의 절대적인 크기보다는 두 점수 간의 단조성(monotonicity)을 평가하는 데 중점을 둔다. 즉, ’MOS가 높은 이미지는 예측 점수도 높게 나오는가’와 같은 순서의 일치도를 본다.</li>
<li><strong>Pearson Linear Correlation Coefficient (PLCC):</strong> 예측 점수와 MOS 간의 선형 상관관계를 측정한다. SRCC와 달리 값의 크기 자체를 고려한다. 일반적으로 예측 점수에 비선형 로지스틱 함수를 적용하여 MOS 스케일과 맞춘 후 PLCC를 계산한다.</li>
</ul>
<p>두 지표 모두 -1에서 1 사이의 값을 가지며, 1에 가까울수록 모델의 예측이 인간의 평가와 강한 긍정적 상관관계를 가짐을 의미한다.</p>
<h3>5.3  SOTA OU-BIQA 모델 성능 종합 비교</h3>
<p>아래 표는 주요 벤치마크 데이터셋에 대한 전통적 및 딥러닝 기반 BIQA 모델들의 성능을 종합적으로 비교한 것이다. 특히 교차 데이터셋 평가는 모델의 일반화 성능을 가늠하는 중요한 척도다. 예를 들어, LIVE 데이터셋의 정보로 모델을 설정(또는 훈련)한 후, 한 번도 보지 못한 TID2013이나 CSIQ 데이터셋에서 테스트하는 방식이다.</p>
<p>IL-NIQE와 같은 전통적인 NSS 기반 OU-BIQA 모델은 특정 데이터셋에 대한 훈련 없이도 준수한 교차 데이터셋 성능을 보여주며 일반화 가능성을 입증했다. 반면, CORNIA와 같은 초기 딥러닝 기반 모델은 훈련 데이터셋(LIVE)에 과적합되어 다른 데이터셋(TID2013)에서는 성능이 저하되는 경향을 보였다. 최신 딥러닝 기반 OU-BIQA 모델들(예: DUBMA, MDFS)은 이러한 한계를 극복하고 합성 및 실제 왜곡 데이터셋 전반에 걸쳐 기존 SOTA 모델들을 능가하는 성능을 보고하고 있다.14</p>
<table><thead><tr><th><strong>방법</strong></th><th><strong>기반 원리</strong></th><th><strong>LIVE (SRCC ∣ PLCC)</strong></th><th><strong>CSIQ (SRCC ∣ PLCC)</strong></th><th><strong>TID2013 (SRCC ∣ PLCC)</strong></th><th><strong>MD1 (SRCC ∣ PLCC)</strong></th><th><strong>MD2 (SRCC ∣ PLCC)</strong></th></tr></thead><tbody>
<tr><td><strong>BIQI</strong></td><td>NSS</td><td>0.047 <span class="math math-inline">\vert</span> 0.311</td><td>0.010 <span class="math math-inline">\vert</span> 0.181</td><td>-</td><td>0.156 <span class="math math-inline">\vert</span> 0.175</td><td>0.332 <span class="math math-inline">\vert</span> 0.380</td></tr>
<tr><td><strong>BRISQUE</strong></td><td>NSS</td><td>0.088 <span class="math math-inline">\vert</span> 0.108</td><td>0.639 <span class="math math-inline">\vert</span> 0.728</td><td>-</td><td>0.625 <span class="math math-inline">\vert</span> 0.807</td><td>0.184 <span class="math math-inline">\vert</span> 0.591</td></tr>
<tr><td><strong>DIIVINE</strong></td><td>NSS</td><td>0.042 <span class="math math-inline">\vert</span> 0.093</td><td>0.146 <span class="math math-inline">\vert</span> 0.255</td><td>-</td><td>0.639 <span class="math math-inline">\vert</span> 0.669</td><td>0.252 <span class="math math-inline">\vert</span> 0.367</td></tr>
<tr><td><strong>CORNIA</strong></td><td>Deep Learning</td><td>0.097 <span class="math math-inline">\vert</span> 0.132</td><td><strong>0.656</strong> <span class="math math-inline">\vert</span> <strong>0.750</strong></td><td>-</td><td>0.772 <span class="math math-inline">\vert</span> 0.847</td><td>0.655 <span class="math math-inline">\vert</span> 0.719</td></tr>
<tr><td><strong>NIQE</strong></td><td>NSS (OU)</td><td><strong>0.906</strong> <span class="math math-inline">\vert</span> <strong>0.904</strong></td><td>0.627 <span class="math math-inline">\vert</span> 0.716</td><td>-</td><td><strong>0.871</strong> <span class="math math-inline">\vert</span> <strong>0.909</strong></td><td><strong>0.795</strong> <span class="math math-inline">\vert</span> <strong>0.848</strong></td></tr>
<tr><td><strong>IL-NIQE</strong></td><td>NSS (OU)</td><td><strong>0.898</strong> <span class="math math-inline">\vert</span> <strong>0.903</strong></td><td><strong>0.815</strong> <span class="math math-inline">\vert</span> <strong>0.854</strong></td><td>-</td><td><strong>0.891</strong> <span class="math math-inline">\vert</span> <strong>0.905</strong></td><td><strong>0.882</strong> <span class="math math-inline">\vert</span> <strong>0.897</strong></td></tr>
</tbody></table>
<p>주: 위 표는 TID2013 데이터셋으로 훈련(또는 설정)하고 다른 데이터셋에서 테스트한 교차 데이터셋 평가 결과임. 두꺼운 글씨는 각 데이터셋에서 상위 2개의 성능을 나타냄. 19</p>
<h2>6.  결론 및 향후 연구 방향</h2>
<h3>6.1  연구 동향 요약</h3>
<p>본 보고서에서 분석한 바와 같이, Deep Opinion-Unaware BIQA 연구는 인간의 주관적 평가 데이터에 대한 의존성을 제거하면서도 높은 예측 정확도와 일반화 성능을 달성하는 방향으로 괄목할 만한 발전을 이루었다. 연구의 흐름은 자연 영상의 통계적 규칙성을 활용하는 전통적인 NSS 접근법에서 시작하여, FR-IQA 모델을 활용한 유사 레이블링, 상대적 품질 순서를 학습하는 순위 학습, 그리고 최근에는 대규모 비-IQA 데이터와 사전 훈련된 모델의 지식을 적극적으로 활용하는 자기지도 학습 및 하이브리드 아키텍처로 진화했다. 특히, 합성 왜곡과 실제 왜곡 간의 도메인 격차를 해소하는 문제는 OU-BIQA 기술의 실용화를 위한 핵심 과제로 부상했으며, 도메인 적응(UDA)과 같은 고급 기계 학습 기법이 중요한 해결책으로 제시되고 있다.</p>
<h3>6.2  현재 기술의 한계</h3>
<p>이러한 발전에도 불구하고 현재의 OU-BIQA 기술은 여전히 몇 가지 한계를 가지고 있다.</p>
<ul>
<li><strong>복합 왜곡 및 콘텐츠 의존성:</strong> 대부분의 모델은 여전히 특정 유형의 왜곡에 대해 더 나은 성능을 보이는 경향이 있으며, 이미지의 시각적 내용(content)과 왜곡 유형 및 강도 간의 복잡한 비선형적 상호작용을 완벽하게 모델링하는 데에는 어려움이 있다.3</li>
<li><strong>해석 가능성(Interpretability) 부족:</strong> 대부분의 딥러닝 기반 BIQA 모델은 ’블랙박스’처럼 작동하여, 왜 특정 이미지의 품질을 낮게 평가했는지에 대한 구체적인 이유나 근거를 제시하지 못한다. 이는 시스템의 신뢰성을 확보하고 사용자가 결과를 바탕으로 이미지를 개선하는 데 걸림돌이 된다.</li>
<li><strong>계산 복잡도:</strong> 일부 SOTA 모델들은 높은 성능을 달성하기 위해 매우 깊고 복잡한 네트워크 구조를 사용하므로, 실시간 처리가 요구되는 모바일 기기나 스트리밍 서비스와 같은 응용 분야에 직접 적용하기에는 계산 비용이 너무 높을 수 있다.38</li>
</ul>
<h3>6.3  향후 연구 방</h3>
<p>이러한 한계를 극복하고 OU-BIQA 기술을 한 단계 더 발전시키기 위해 다음과 같은 연구 방향을 제안할 수 있다.</p>
<ul>
<li><strong>멀티모달(Multi-modal) BIQA:</strong> 인간은 이미지를 평가할 때 시각 정보뿐만 아니라, 이미지에 대한 설명(캡션)이나 관련 맥락 등 다양한 정보를 종합적으로 활용한다. 이미지의 시각적 특징과 함께 캡션과 같은 텍스트 정보 45 또는 비디오의 오디오 정보 6를 함께 입력으로 사용하여 인간의 종합적인 품질 인지 과정을 모사하는 멀티모달 BIQA 연구는 새로운 가능성을 열어줄 것이다.</li>
<li><strong>효율적인 자기지도 학습 패러다임:</strong> 현재의 자기지도 학습 방법들은 종종 대규모 데이터셋과 막대한 계산 자원을 필요로 한다. 더 적은 데이터와 계산 비용으로도 효과적인 품질 인식 표현을 학습할 수 있는 새로운 보조 작업(pretext task)과 손실 함수를 개발하는 연구가 필요하다.48</li>
<li><strong>특정 응용 분야(Domain-specific) BIQA:</strong> 범용 BIQA 모델을 넘어, 의료 영상 1, 스크린 콘텐츠 20, 저조도 환경 47, 위성 이미지 등 특정 도메인의 고유한 왜곡 특성과 품질 기준을 고려한 맞춤형 BIQA 모델 개발이 더욱 중요해질 것이다.</li>
<li><strong>인과관계 추론(Causal Inference)의 도입:</strong> 현재의 모델들은 왜곡 특징과 품질 점수 간의 ’상관관계’를 학습하는 데 그친다. 왜곡이 품질 저하를 유발하는 ’인과적 메커니즘’을 추론하고 학습하도록 모델을 설계한다면, 모델의 강건성과 설명력을 획기적으로 향상시킬 수 있을 것이다.</li>
</ul>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>