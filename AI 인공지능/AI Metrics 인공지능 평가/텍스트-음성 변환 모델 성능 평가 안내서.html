<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:텍스트-음성 변환 모델 성능 평가 안내서</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>텍스트-음성 변환 모델 성능 평가 안내서</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">인공지능 평가지표 (AI evaluation metrics)</a> / <span>텍스트-음성 변환 모델 성능 평가 안내서</span></nav>
                </div>
            </header>
            <article>
                <h1>텍스트-음성 변환 모델 성능 평가 안내서</h1>
<h2>1.  고품질 음성 합성을 위한 평가의 다차원적 프레임워크</h2>
<h3>1.1  TTS 기술의 발전과 평가의 중요성</h3>
<p>텍스트 음성 변환(Text-to-Speech, TTS) 기술은 단순히 서면 정보를 음성으로 변환하는 기능을 넘어, 인간과 기계의 상호작용을 근본적으로 혁신하는 핵심 동력으로 자리 잡았다. 초기 기계음에 가까웠던 합성 기술은 딥러닝과 신경망의 발전에 힘입어 챗봇, 인공지능 비서, 오디오북 제작, 그리고 장애인을 위한 접근성 도구에 이르기까지 광범위한 응용 분야에서 중추적인 역할을 수행하고 있다.1</p>
<p>기술의 성숙도가 높아짐에 따라, 합성음의 품질 목표 또한 비약적으로 상승했다. 과거에는 ’내용을 알아들을 수 있는 수준(intelligibility)’이 주된 목표였다면, 현대의 TTS 시스템은 ’실제 인간의 음성과 구별하기 어려운 수준(naturalness)’을 지향한다. 실제로 Tacotron2와 같은 선구적인 모델은 평균 의견 점수(Mean Opinion Score, MOS) 평가에서 실제 인간의 녹음 음성과 거의 차이가 없는 점수를 기록하며 기술적 변곡점을 제시하기도 했다.4 이처럼 합성음의 품질이 상향 평준화되면서, 모델 간의 미묘한 성능 차이를 정밀하게 측정하고, 특정 응용 분야에 대한 적합성을 객관적으로 판단하며, 나아가 모델의 약점을 진단하여 개선 방향을 설정하기 위한 체계적이고 다차원적인 평가 프레임워크의 중요성은 그 어느 때보다 부각되고 있다.5</p>
<p>이러한 평가 패러다임은 기술의 발전과 함께 진화해왔다. 초기 평가는 원본 녹음과 합성음 간의 신호 수준 왜곡을 최소화하는 ’신호 재현의 충실도(fidelity)’에 초점을 맞추었다.7 신경망 모델의 등장 이후, 평가는 ’인간 인지의 만족도(perceptual satisfaction)’를 측정하는 방향으로 전환되었으며, MOS가 핵심 기준으로 부상했다.9 최근 TTS 기술이 대화형 AI나 감정 표현과 같은 고차원적 영역으로 확장됨에 따라, 평가의 지향점은 다시 한번 ’문맥적 적절성(contextual appropriateness)’으로 이동하고 있다.11 이는 평가의 최종 목표가 더 이상 완벽한 음향적 복제가 아니라, 주어진 상황에서 가장 효과적인 ’소통’을 달성하는 것임을 시사한다.</p>
<h3>1.2  고품질 합성음의 핵심 구성 요소</h3>
<p>고품질 TTS 모델을 평가하는 것은 단일 점수로 우열을 가리는 과정이 아니다. 이는 인간 음성이 지닌 복합적인 특성을 모델이 얼마나 충실하게 재현하는지를 다각적으로 분석하는 과정이다. 성공적인 평가는 다음의 핵심 구성 요소들을 종합적으로 고려해야 한다.</p>
<ul>
<li>
<p><strong>자연성 (Naturalness):</strong> 합성음이 기계음처럼 부자연스럽거나 로봇 같지 않고, 실제 사람이 말하는 것처럼 얼마나 자연스럽고 편안하게 들리는지를 나타내는 척도이다. 이는 부드러운 음색 변화, 적절한 휴지(pause), 그리고 일관된 운율을 포함하는 종합적인 품질 요소다.9</p>
</li>
<li>
<p><strong>명료도 (Intelligibility):</strong> 합성음이 전달하고자 하는 내용이 청취자에게 얼마나 명확하고 정확하게 이해되는지를 의미한다. 발음의 정확성, 조음의 명확성 등이 주요 요인이며, 특히 소음이 있는 환경이나 네비게이션 안내와 같이 정보의 정확한 전달이 중요한 응용 분야에서 필수적이다.5</p>
</li>
<li>
<p><strong>운율 (Prosody):</strong> 개별 음소의 분절적(segmental) 특성을 넘어, 말의 리듬, 억양(intonation), 강세(stress), 속도(tempo) 등 초분절적(suprasegmental) 요소를 얼마나 적절하게 표현하는지를 평가한다. 자연스러운 운율은 문장의 의미를 명확히 하고 감정을 전달하는 데 결정적인 역할을 수행한다.15</p>
</li>
<li>
<p><strong>화자 유사도 (Speaker Similarity):</strong> 다화자(multi-speaker) TTS 또는 음성 복제(voice cloning) 모델의 핵심 평가 요소로, 생성된 음성이 목표로 하는 특정 화자의 음색, 억양, 말하는 스타일과 얼마나 유사한지를 측정한다. 이는 개인화된 AI 비서나 특정 인물의 목소리를 재현하는 콘텐츠 제작에 있어 매우 중요하다.18</p>
</li>
</ul>
<h3>1.3  주관적 평가와 객관적 평가의 상호보완적 관계</h3>
<p>TTS 모델의 성능을 종합적으로 평가하기 위해 연구자들은 크게 두 가지 접근법을 사용하며, 이 둘은 상호보완적인 관계를 형성한다.</p>
<ul>
<li>
<p><strong>주관적 평가 (Subjective Evaluation):</strong> 실제 인간 청취자가 합성음을 듣고 직접 품질에 대한 점수를 매기는 방식이다. 인간의 복합적인 인지 및 청각 시스템을 통해 평가가 이루어지므로, 합성음의 전반적인 품질을 판단하는 ’황금 표준(gold standard)’으로 간주된다.6 평균 의견 점수(MOS)가 가장 대표적인 주관적 평가 방법이다.</p>
</li>
<li>
<p><strong>객관적 평가 (Objective Evaluation):</strong> 합성된 음성 신호의 물리적, 음향적 특성을 수학적 알고리즘을 통해 정량적으로 측정하는 방식이다. 이 방법은 평가자의 주관이 배제되어 재현성이 높고, 평가 과정을 자동화할 수 있어 모델 개발 단계에서 빠르고 반복적인 성능 측정이 가능하다는 장점이 있다.18</p>
</li>
</ul>
<p>이 두 가지 방법은 서로를 대체하는 것이 아니라 각자의 강점과 약점을 보완한다. 주관적 평가는 최종 사용자의 경험을 가장 잘 대변하지만 시간과 비용이 많이 들고, 결과에 대한 원인을 구체적으로 진단하기 어렵다. 반면, 객관적 평가는 특정 음향적 측면(예: 스펙트럼, 피치)의 오류를 정밀하게 측정하여 모델 개선을 위한 구체적인 단서를 제공할 수 있지만, 인간이 인지하는 미묘한 품질 차이나 부자연스러움을 완벽하게 포착하지는 못한다.9 따라서 가장 이상적인 평가는 두 가지 방법을 병행하여, 객관적 지표로 모델의 기술적 성능을 빠르게 검증하고 주관적 평가를 통해 최종적인 품질과 사용자 만족도를 확인하는 것이다.</p>
<h2>2.  주관적 평가: 인간의 인지를 통한 품질 측정</h2>
<p>주관적 평가는 합성음의 품질을 인간의 청각적 인지를 통해 직접 측정하는 방식으로, 기계적인 측정만으로는 파악하기 어려운 자연스러움, 표현력 등 종합적인 측면을 평가하는 데 필수적이다. 이 중 평균 의견 점수(MOS)는 TTS 분야에서 가장 보편적으로 인정받는 황금 표준 평가 방법론이다.</p>
<h3>2.1  평균 의견 점수(MOS): 황금 표준의 이해</h3>
<p>MOS는 다수의 청취자가 주어진 음성 샘플을 듣고 품질에 대한 주관적인 점수를 부여하면, 이 점수들의 산술 평균을 계산하여 최종 점수를 도출하는 방식이다.9 이 점수는 합성음이 인간 청취자에게 얼마나 자연스럽고, 명확하며, 듣기 좋은지를 직접적으로 나타낸다.</p>
<ul>
<li><strong>평가 척도 (Rating Scale):</strong> 가장 널리 사용되는 척도는 1점(매우 나쁨)부터 5점(우수)까지의 5점 척도인 절대 범주 평가(Absolute Category Rating, ACR) 방식이다.22 평가의 세밀함을 높이기 위해 0.5점 단위의 9-scale 척도를 사용하기도 한다.18 평가 목적에 따라 ‘전반적인 품질(Overall Quality)’, ‘자연성(Naturalness)’, ‘명료도(Intelligibility)’ 등 구체적인 평가 항목을 질문할 수 있다.10</li>
</ul>
<p>표 1: 일반적인 MOS 5점 척도 (ACR) 22</p>
<table><thead><tr><th><strong>점수</strong></th><th><strong>평가</strong></th></tr></thead><tbody>
<tr><td>5</td><td>Excellent (우수)</td></tr>
<tr><td>4</td><td>Good (좋음)</td></tr>
<tr><td>3</td><td>Fair (보통)</td></tr>
<tr><td>2</td><td>Poor (나쁨)</td></tr>
<tr><td>1</td><td>Bad (매우 나쁨)</td></tr>
</tbody></table>
<ul>
<li>
<p><strong>MOS의 변형:</strong></p>
</li>
<li>
<p><strong>비교 평균 의견 점수 (Comparison MOS, CMOS):</strong> 두 개 이상의 시스템에서 생성된 음성을 쌍으로 제시하고, 청취자가 어느 쪽을 더 선호하는지 상대적으로 평가하는 방식이다. 일반적으로 -3점(A가 B보다 훨씬 나쁨)부터 +3점(A가 B보다 훨씬 좋음)까지의 7점 척도가 사용된다. 이 방법은 시스템 간의 미묘한 품질 차이를 감지하는 데 매우 효과적이다.24</p>
</li>
<li>
<p><strong>열화 평균 의견 점수 (Degradation MOS, DMOS):</strong> 원본 자연 음성을 명확한 참조(reference)로 먼저 들려준 후, 합성음이 원본에 비해 얼마나 품질이 저하되었는지를 평가하는 방식이다. 1점(열화가 인지되지 않음)부터 5점(열화가 매우 거슬림)까지의 척도를 사용하며, 고품질 시스템의 미세한 결함을 평가하는 데 유용하다.26</p>
</li>
</ul>
<p>MOS는 최종 사용자의 경험을 가장 직접적으로 반영한다는 점에서 타의 추종을 불허하는 가치를 지닌다. 그러나 이 방식은 ‘왜’ 품질이 낮은지에 대한 구체적인 진단 정보를 제공하지 못한다는 본질적인 한계를 가진다.21 예를 들어, MOS 점수가 낮게 나온 원인이 부정확한 발음 때문인지, 부자연스러운 억양 때문인지, 혹은 음향적 잡음 때문인지를 구별해주지 않는다. 또한, 대규모 청취자단을 모집하고 통제된 환경에서 평가를 진행하는 데에는 상당한 시간과 비용이 소요되어, 빠른 반복이 필수적인 딥러닝 모델 개발 사이클에 적용하기에는 비효율적이다.9 바로 이러한 ’진단 능력의 부재’와 ’효율성의 한계’가 빠르고 자동화된 측정이 가능한 객관적 평가 지표의 필요성을 강력하게 뒷받침한다.</p>
<h3>2.2  국제전기통신연합 표준(ITU-T P.800) 기반 평가 설계</h3>
<p>주관적 평가 결과의 신뢰도와 재현성을 확보하기 위해서는 국제적으로 표준화된 프로토콜을 따르는 것이 매우 중요하다. 국제전기통신연합 전기통신표준화부문(ITU-T)의 P.800 권고안은 음성 품질 평가를 위한 다양한 방법론을 제시하며, 이는 TTS 평가에도 널리 준용된다.9</p>
<ul>
<li>
<p><strong>절대 범주 평가 (Absolute Category Rating, ACR):</strong> 가장 기본적이고 널리 사용되는 MOS 평가 방식이다. 청취자는 한 번에 하나의 음성 샘플을 듣고, 다른 샘플과의 비교 없이 해당 샘플의 품질에 대해 절대적인 점수를 매긴다. 시스템의 전반적인 품질 수준을 평가하는 데 적합하다.28</p>
</li>
<li>
<p><strong>비교 범주 평가 (Comparison Category Rating, CCR):</strong> 청취자에게 기준 샘플(숨겨진 참조)을 포함한 한 쌍의 샘플을 차례로 들려주고, 두 번째 샘플이 첫 번째에 비해 얼마나 더 좋거나 나쁜지를 7점 척도로 평가하게 한다. 이 방식은 시스템 간의 미세한 품질 차이를 변별하는 데 매우 민감하게 작용한다.28</p>
</li>
<li>
<p><strong>열화 범주 평가 (Degradation Category Rating, DCR):</strong> 명시적으로 식별된 기준 샘플을 먼저 들려준 후, 평가 대상 샘플이 이 기준에 비해 얼마나 열화되었는지를 5점 척도로 평가한다. 주로 고품질 시스템 간의 미세한 결함을 평가할 때 유용하다.28</p>
</li>
</ul>
<p>또한, ITU-T P.800.1 권고안은 MOS 점수를 보고할 때 발생할 수 있는 혼동을 줄이기 위해, 평가 조건(예: 청취 전용), 평가 방식(주관적), 대역폭 등을 명시하는 표준화된 표기법을 사용할 것을 권장한다. 예를 들어, <code>MOS-LQS</code>는 ’주관적으로(Subjective) 측정된 청취 전용(Listening Quality) 점수’를 의미한다.29</p>
<h3>2.3  신뢰성 있는 주관적 평가 수행 방법론</h3>
<p>신뢰할 수 있는 주관적 평가 결과를 얻기 위해서는 청취자단 구성부터 평가 문장 설계, 실험 환경 통제에 이르기까지 전 과정을 세심하게 설계해야 한다.</p>
<ul>
<li>
<p><strong>청취자단 구성 (Listener Panel):</strong></p>
</li>
<li>
<p><strong>자격 및 규모:</strong> 일반적으로 정상적인 청력을 가진 해당 언어의 원어민으로 구성한다.24 통계적 유의성을 확보하기 위해 최소 20명 이상의 청취자를 모집하는 것이 권장된다.18 평가의 목적과 규모에 따라 10명 내외에서 100명 이상까지 다양하게 구성될 수 있다.24</p>
</li>
<li>
<p><strong>전문성:</strong> 평가 목적에 따라 일반 대중을 대표하는 비전문가(naïve listeners) 또는 음질 평가에 대한 훈련을 받은 전문가(expert listeners)를 선택할 수 있다. 전문가는 미세한 음질 차이를 일관되게 평가하는 능력이 뛰어나다.32</p>
</li>
<li>
<p><strong>신뢰도 관리:</strong> 아마존 미케니컬 터크(Amazon Mechanical Turk)와 같은 온라인 크라우드소싱 플랫폼을 활용할 경우, 평가에 성실히 참여하지 않는 응답자를 걸러내기 위한 장치가 필수적이다. 명확한 정답이 있는 주의력 확인 질문(attention check question)을 삽입하거나, 평가 시간이 비정상적으로 짧은 응답자를 제외하는 등의 방법을 사용해야 한다.24</p>
</li>
<li>
<p><strong>평가 문장 설계 (Test Sentences):</strong></p>
</li>
<li>
<p><strong>독립성:</strong> 평가에 사용되는 문장은 모델 학습에 사용되지 않은 데이터여야 한다.35 이는 모델이 단순히 학습 데이터를 암기한 것인지, 아니면 처음 보는 문장에 대해서도 일반화 성능을 보이는지를 확인하기 위함이다.</p>
</li>
<li>
<p><strong>길이 및 내용:</strong> 청취자의 기억력 부담을 줄이고 평가의 일관성을 유지하기 위해 문장의 길이는 10초 내외로 제한하는 것이 일반적이다.32 정치적으로 민감하거나 특정 감정을 유발할 수 있는 내용은 평가에 편향을 줄 수 있으므로 피해야 한다.32</p>
</li>
<li>
<p><strong>의미적으로 예측 불가능한 문장 (Semantically Unpredictable Sentences, SUS):</strong> SUS는 “코끼리가 파란색 신문을 날았다“와 같이 문법적으로는 완벽하지만 의미적으로는 예측이 불가능한 문장을 말한다.37 이러한 문장을 사용하면 청취자가 문맥을 통해 불분명하게 발음된 단어를 추측하는 것을 방지할 수 있다. 따라서 SUS는 합성음 자체의 순수한 명료도(intelligibility)를 측정하는 데 매우 효과적인 도구다.5</p>
</li>
<li>
<p><strong>실험 환경 및 절차:</strong></p>
</li>
<li>
<p><strong>환경 통제:</strong> 외부 소음이 차단된 조용한 환경에서, 고품질 헤드폰을 사용하여 모든 청취자에게 동일한 음량으로 음성 샘플을 제공해야 한다. 이는 평가 결과에 영향을 미칠 수 있는 외부 변수를 최소화하기 위함이다.9</p>
</li>
<li>
<p><strong>무작위화 (Randomization):</strong> 특정 샘플이 항상 먼저 제시되어 발생하는 순서 효과(order effect)를 방지하기 위해, 평가에 사용되는 문장과 시스템의 제시 순서는 각 청취자에게 무작위로 제공되어야 한다.41</p>
</li>
<li>
<p><strong>통계적 분석:</strong> 평가 결과를 제시할 때는 단순히 평균 점수만 보고하는 것을 넘어, 신뢰 구간(confidence interval)을 함께 표시하여 점수의 변동 범위를 보여주어야 한다. 또한, 시스템 간의 점수 차이가 통계적으로 유의미한지를 검증하기 위해 Wilcoxon 부호 순위 검정(Wilcoxon signed-rank test)이나 분산 분석(ANOVA)과 같은 통계적 유의성 검정을 수행해야 한다.28</p>
</li>
</ul>
<h2>3.  객관적 평가: 정량적 지표를 통한 자동화된 분석</h2>
<p>객관적 평가는 합성된 음성 신호를 수학적 알고리즘을 통해 분석하여 품질을 정량화하는 방법이다. 이는 주관적 평가에 비해 시간과 비용 효율성이 높고 결과의 재현성이 보장되므로, 모델 개발 과정에서 반복적인 성능 측정 및 진단에 필수적이다. 본 섹션에서는 스펙트럼 충실도, 운율 정확도, 명료도, 화자 유사도 등 핵심적인 평가 차원에 따라 널리 사용되는 객관적 지표들을 상세히 다룬다.</p>
<p>표 2: 주요 TTS 평가 지표 요약</p>
<table><thead><tr><th><strong>지표명</strong></th><th><strong>평가 차원</strong></th><th><strong>계산 방식 요약</strong></th><th><strong>장점</strong></th><th><strong>단점</strong></th></tr></thead><tbody>
<tr><td><strong>MOS/CMOS</strong></td><td>자연성, 명료도 등 종합 품질</td><td>인간 청취자의 5점/7점 척도 평가 평균</td><td>최종 사용자 경험을 가장 잘 반영 (황금 표준)</td><td>시간/비용 소모, 주관성, 진단 정보 부재</td></tr>
<tr><td><strong>MCD</strong></td><td>스펙트럼 충실도</td><td>참조-합성음 간 MFCC의 유클리드 거리</td><td>음색의 유사도를 정량화, 자동 계산 가능</td><td>인간의 청각 인지와 불일치 가능성, 운율 평가 불가</td></tr>
<tr><td><strong>F0 RMSE</strong></td><td>운율 정확도 (억양)</td><td>참조-합성음 간 F0 궤적의 평균 제곱근 오차</td><td>억양 패턴의 유사도를 정량적으로 측정</td><td>F0 추출 오류에 민감, 리듬/강세 평가 불가</td></tr>
<tr><td><strong>WER/CER</strong></td><td>명료도 (발음 정확성)</td><td>ASR을 이용한 전사 결과와 원문 텍스트 비교</td><td>내용 전달의 정확성을 자동화하여 측정</td><td>ASR 모델 성능에 크게 의존, 운율/감정 평가 불가</td></tr>
<tr><td><strong>코사인 유사도</strong></td><td>화자 유사도</td><td>화자 임베딩 벡터 간 코사인 각도 계산</td><td>목표 화자의 음색 재현도를 정량화</td><td>음색 외 말하기 스타일(예: 습관)은 반영 못함</td></tr>
<tr><td><strong>RTF</strong></td><td>효율성 (속도)</td><td>1초 음성 합성에 걸리는 시간</td><td>실시간 서비스 가능 여부를 직접적으로 판단</td><td>하드웨어 사양에 따라 결과가 달라짐</td></tr>
</tbody></table>
<h3>3.1  스펙트럼 충실도: 원음과의 음향적 거리 측정</h3>
<p>스펙트럼 충실도는 합성음의 음색과 음질이 원본 음성과 얼마나 유사한지를 음향 스펙트럼 수준에서 비교하는 지표이다.</p>
<ul>
<li>
<p><strong>멜-캡스트럼 왜곡 (Mel-Cepstral Distortion, MCD):</strong></p>
</li>
<li>
<p><strong>원리:</strong> MCD는 두 음성 신호 간의 스펙트럼 포락선(spectral envelope) 차이를 측정하는 가장 대표적인 객관적 지표다. 이는 인간의 청각 시스템이 주파수를 인지하는 방식과 유사한 멜 스케일(Mel scale)을 기반으로 계산된 멜-주파수 캡스트럼 계수(Mel-Frequency Cepstral Coefficients, MFCCs)를 사용한다. MCD 값이 낮을수록 두 음성의 스펙트럼 특성이 유사하며, 일반적으로 음질이 더 좋다고 판단한다.7</p>
</li>
<li>
<p><strong>수학적 정의:</strong> <code>k</code>번째 프레임에서의 MCD는 참조 음성(<code>ref</code>)과 합성 음성(<code>syn</code>)의 MFCC 벡터 간의 유클리드 거리(Euclidean distance)로 정의된다. 전체 발화에 대한 MCD는 모든 프레임의 <code>MCD(k)</code>를 평균하여 계산한다.8</p>
</li>
</ul>
<p>프레임별 MCD:</p>
<p><span class="math math-display">
MCD(k) = \alpha \sqrt{\sum_{i=s}^{D} (MC_{ref}(i, k) - MC_{syn}(i, k))^2}
</span><br />
전체 평균 MCD:</p>
<p><span class="math math-display">
MCD = \frac{1}{N} \sum_{k=1}^{N} MCD(k)
</span><br />
여기서 <span class="math math-inline">MC(i, k)</span>는<code> k</code>번째 프레임의<code> i</code>번째 MFCC 계수를,<code> D</code>는 사용된 계수의 총 개수(보통 13에서 24 사이)를 나타낸다.18<code> s</code>는 에너지에 해당하는 0번째 계수를 포함할지(<code>s=0</code>) 여부(<code>s=1</code>)를 결정한다. <span class="math math-inline">\alpha</span>는 결과를 데시벨(dB) 스케일로 변환하기 위해 사용되는 상수(<span class="math math-inline">\frac{10\sqrt{2}}{\ln 10}</span>)이다.43</p>
<ul>
<li><strong>동적 시간 왜곡 (Dynamic Time Warping, DTW):</strong></li>
<li><strong>역할:</strong> TTS 모델이 생성한 음성은 원본 녹음과 발화 속도에서 미세한 차이를 보여 길이가 다를 수 있다. 이 상태에서 프레임별로 직접 비교하면 부정확한 왜곡 값이 계산된다. DTW는 이러한 시간적 비일치를 해결하기 위해 비선형적으로 두 시퀀스를 정렬하여, 시간 축의 차이를 최소화하는 최적의 경로를 탐색한다. 이 과정을 통해 동일한 음소에 해당하는 프레임끼리 비교가 가능해져 MCD나 F0 RMSE와 같은 지표 계산의 정확도를 크게 향상시킨다.7</li>
</ul>
<h3>3.2  운율 정확도: 말의 리듬과 억양 평가</h3>
<p>운율은 말의 자연스러움과 표현력을 결정하는 핵심 요소이다. 객관적 평가는 주로 음성의 높낮이(pitch)와 길이(duration)를 중심으로 운율의 정확도를 측정한다.</p>
<ul>
<li>
<p><strong>기본 주파수 (F0) RMSE (Root Mean Square Error):</strong></p>
</li>
<li>
<p><strong>원리:</strong> 기본 주파수(F0)는 음성의 높낮이를 결정하는 물리량으로, 억양(intonation)의 변화를 나타낸다. F0 RMSE는 원본 음성과 합성 음성의 F0 궤적(contour) 간의 평균 제곱근 오차를 계산하여 억양 패턴의 유사성을 평가한다. 이 값이 낮을수록 합성음의 억양이 원본과 가깝다는 것을 의미한다.16</p>
</li>
<li>
<p><strong>수학적 정의:</strong> DTW를 통해 시간적으로 정렬된 두 F0 시퀀스 <span class="math math-inline">\mathbf{f0}_{ref}</span> 와 <span class="math math-inline">\mathbf{f0}_{syn}</span>에 대해 다음과 같이 계산된다.44<br />
<span class="math math-display">
  \text{RMSE}(\mathbf{f0}_{ref}, \mathbf{f0}_{syn}) = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (\mathbf{f0}_{ref}(i) - \mathbf{f0}_{syn}(i))^2}
</span><br />
여기서 <span class="math math-inline">N</span>은 목소리가 있는 유성음(voiced) 구간의 총 프레임 수를 의미한다.</p>
</li>
<li>
<p><strong>음소 길이 예측 오차 (Duration Prediction Error):</strong></p>
</li>
<li>
<p><strong>원리:</strong> FastSpeech와 같은 비-자기회귀(non-autoregressive) TTS 모델은 텍스트로부터 각 음소의 길이를 명시적으로 예측하는 ’duration predictor’를 포함한다.41 이 예측된 길이가 실제 음성의 음소 길이와 얼마나 차이 나는지를 측정함으로써, 모델이 말의 리듬과 속도를 얼마나 정확하게 모델링하는지 평가할 수 있다.</p>
</li>
<li>
<p><strong>계산:</strong> 일반적으로 원본 음성에 대해 강제 정렬(forced alignment)을 수행하여 실제 음소 길이를 추출한 후, 모델이 예측한 길이와의 평균 절대 오차(MAE) 또는 RMSE를 계산한다.</p>
</li>
</ul>
<h3>3.3  명료도: 내용 전달의 정확성 평가</h3>
<p>명료도는 합성된 음성의 내용이 얼마나 정확하게 전달되는지를 나타내는 지표이다. 이를 객관적으로 측정하기 위해 자동 음성 인식(ASR) 기술이 활용된다.</p>
<ul>
<li>
<p><strong>단어 오류율 (Word Error Rate, WER) 및 문자 오류율 (Character Error Rate, CER):</strong></p>
</li>
<li>
<p><strong>원리:</strong> 이 방법은 합성된 음성을 ASR 모델의 입력으로 사용하여 텍스트로 변환(transcription)한 후, 이 결과를 원본 입력 텍스트와 비교하여 얼마나 많은 오류가 발생했는지를 측정한다. WER은 합성음이 기계에 의해 얼마나 잘 이해되는지를 나타내는 지표로, 인간이 인지하는 명료도와 높은 상관관계를 보인다.15</p>
</li>
<li>
<p><strong>수학적 정의:</strong> 원본 텍스트와 ASR 출력 텍스트를 비교하여 발생한 세 가지 유형의 오류, 즉 대체(Substitution, <span class="math math-inline">S</span>), 삽입(Insertion, <span class="math math-inline">I</span>), 삭제(Deletion, <span class="math math-inline">D</span>)의 총 개수를 원본 텍스트의 총 단어(또는 문자) 수(<span class="math math-inline">N</span>)로 나누어 계산한다.49<br />
<span class="math math-display">
  WER = \frac{S + I + D}{N}
</span></p>
</li>
<li>
<p><strong>한계점:</strong> 이 평가 방법의 신뢰도는 전적으로 사용되는 ASR 모델의 성능에 의존한다. 만약 ASR 모델이 특정 억양, 소음 환경, 또는 드문 단어에 취약하다면, 이는 TTS 모델의 성능을 부정확하게 평가하는 결과로 이어질 수 있다. 때로는 ASR 모델이 내장된 언어 모델의 힘으로 불분명한 발음을 ’교정’하여 인식하는 경우도 있어, 순수한 음향적 명료도를 측정하는 데 한계가 있다.14</p>
</li>
</ul>
<h3>3.4  화자 유사도: 목표 음색의 재현성 평가</h3>
<p>다화자 TTS나 음성 복제 모델에서는 생성된 음성이 목표 화자의 고유한 음색을 얼마나 잘 재현하는지가 매우 중요한 평가 요소이다.</p>
<ul>
<li>
<p><strong>화자 임베딩과 코사인 유사도 (Speaker Embedding Cosine Similarity):</strong></p>
</li>
<li>
<p><strong>원리:</strong> 이 방법은 사전에 대규모 음성 데이터로 훈련된 화자 인식(speaker verification) 모델을 활용한다. 이 모델은 어떤 음성 입력이든 해당 화자의 고유한 음색 특징을 담은 고정 차원의 벡터, 즉 ’화자 임베딩(speaker embedding)’으로 변환하는 능력을 가지고 있다. 평가 시, 원본 참조 음성과 합성 음성에서 각각 화자 임베딩을 추출한 후, 두 벡터 간의 코사인 유사도를 계산한다. 이 값은 -1에서 1 사이의 값을 가지며, 1에 가까울수록 두 음성이 동일한 화자의 것으로 인식됨을 의미한다.18</p>
</li>
<li>
<p><strong>수학적 정의:</strong> 두 화자 임베딩 벡터 <span class="math math-inline">\vec{A}</span>와 <span class="math math-inline">\vec{B}</span>에 대한 코사인 유사도는 다음과 같이 정의된다.<br />
<span class="math math-display">
  \text{Similarity} = \cos(\theta) = \frac{\vec{A} \cdot \vec{B}}{\|\vec{A}\| \|\vec{B}\|}
</span></p>
</li>
<li>
<p><strong>활용:</strong> 이 지표는 특히 소량의 샘플만으로 새로운 화자의 목소리를 복제하는 제로샷(zero-shot) 또는 퓨샷(few-shot) TTS 모델의 성능을 평가하는 데 필수적이다.53</p>
</li>
</ul>
<p>각 객관적 지표는 TTS 품질의 특정 단면만을 보여주는 ’편향된 렌즈’와 같다. MCD는 스펙트럼의 평균적 유사도에 집중하지만, 인간이 민감하게 반응하는 순간적인 잡음이나 부자연스러운 운율을 놓칠 수 있다.8 F0 RMSE는 억양의 유사도를 측정하지만, 문맥에 따라 적절하게 변형된 억양이 원본과 다르더라도 더 자연스러울 수 있다는 점을 간과한다.16 WER은 명료도를 측정하지만, ASR 시스템의 한계에 발목이 잡히며 감정과 같은 비언어적 정보는 전혀 평가하지 못한다.14 따라서 신뢰성 있는 객관적 평가는 어느 한 지표에 의존하는 것이 아니라, 여러 지표를 종합적으로 살펴보고 그 결과를 주관적 평가와 교차 검증함으로써 모델의 강점과 약점을 입체적으로 파악하는 다각적인 접근을 요구한다.15</p>
<h2>4.  고급 평가 차원 및 실용적 고려사항</h2>
<p>기본적인 음질 평가를 넘어, 현대 TTS 시스템은 감정 표현, 다양한 말하기 스타일 구현, 실시간 서비스 제공 등 더욱 복잡하고 실용적인 요구사항에 직면해 있다. 따라서 평가는 이러한 고급 차원과 실제 응용 환경에서의 효율성을 반드시 고려해야 한다. 또한, 신뢰성 있는 평가를 위해서는 표준화된 데이터셋과 적절한 분석 도구를 활용하는 것이 필수적이다.</p>
<h3>4.1  표현력 평가: 감정과 스타일의 구현</h3>
<p>현대 TTS 기술의 중요한 연구 방향 중 하나는 단순히 중립적인 낭독체를 넘어, 인간의 말처럼 풍부한 감정과 다양한 스타일을 표현하는 것이다.</p>
<ul>
<li>
<p><strong>감정 표현 TTS 평가:</strong></p>
</li>
<li>
<p>기쁨, 슬픔, 분노와 같은 기본 감정뿐만 아니라 불안, 자신감 등 복합적인 감정을 표현하는 TTS 모델의 성능 평가는 점차 중요해지고 있다.11</p>
</li>
<li>
<p><strong>주관적 평가:</strong> 이 분야의 평가는 주로 주관적 방법에 의존한다. 청취자에게 합성음을 들려주고, 어떤 감정으로 인지되는지 선택하게 하는 감정 분류 정확도(emotion classification accuracy)를 측정하거나, 의도된 감정이 얼마나 강하게 표현되었는지를 척도로 평가하게 할 수 있다.56</p>
</li>
<li>
<p><strong>객관적 평가:</strong> 대안적으로, 감정이 레이블링된 대규모 음성 데이터로 훈련된 음성 감정 인식(Speech Emotion Recognition, SER) 모델을 사용하여 합성음의 감정을 자동으로 분류하고, 목표 감정과의 일치도를 측정하는 방법도 연구되고 있다.57</p>
</li>
<li>
<p><strong>스타일 전달 및 제어 가능성 평가:</strong></p>
</li>
<li>
<p>뉴스 앵커, 친근한 대화, 연설 등 특정 말하기 스타일을 얼마나 잘 모사하는지, 또는 참조 음성(reference audio)의 스타일을 얼마나 성공적으로 전달(style transfer)하는지를 평가한다.58</p>
</li>
<li>
<p>평가는 주로 주관적인 방식으로 이루어진다. 청취자에게 생성된 음성이 목표 스타일과 얼마나 부합하는지, 그리고 하나의 발화 내에서 스타일의 일관성이 얼마나 잘 유지되는지를 평가하도록 요청한다.</p>
</li>
</ul>
<h3>4.2  효율성 평가: 속도와 품질의 상충 관계</h3>
<p>실시간 대화형 서비스나 온디바이스(on-device) 환경에서는 합성음의 품질만큼이나 생성 속도가 중요한 성능 지표가 된다.</p>
<ul>
<li>
<p><strong>실시간 계수 (Real-Time Factor, RTF):</strong></p>
</li>
<li>
<p><strong>정의:</strong> RTF는 1초 분량의 음성을 합성하는 데 소요되는 실제 시간(초)을 나타내는 지표다. 예를 들어, RTF가 0.5라면 1초짜리 음성을 0.5초 만에 생성할 수 있다는 의미이며, RTF가 1보다 작아야 실시간 서비스가 가능하다.18</p>
</li>
<li>
<p><strong>측정:</strong> RTF는 특정 하드웨어 사양(예: NVIDIA T4 GPU, CPU)에서 측정되어야 의미가 있으며, 입력 텍스트의 길이에 따른 RTF 변화 추이를 분석하면 모델의 확장성을 평가할 수 있다.18</p>
</li>
<li>
<p><strong>모델 아키텍처와 효율성:</strong> TTS 모델의 아키텍처는 속도와 품질 간의 상충 관계(trade-off)를 결정하는 핵심 요인이다.</p>
</li>
</ul>
<table><thead><tr><th>표 3: 주요 TTS 모델 아키텍처 비교</th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td><strong>아키텍처</strong></td><td><strong>핵심 아이디어</strong></td><td><strong>주요 장점</strong></td><td><strong>주요 단점</strong></td><td><strong>대표 모델</strong></td></tr>
<tr><td><strong>RNN 기반 (자기회귀)</strong></td><td>Seq2Seq + Attention. 프레임 순차 생성</td><td>높은 자연성, 부드러운 운율</td><td>느린 추론 속도, Attention 실패 위험</td><td>Tacotron 2 59</td></tr>
<tr><td><strong>Transformer 기반 (비-자기회귀)</strong></td><td>Feed-Forward Transformer. 프레임 병렬 생성</td><td>매우 빠른 추론 속도, 안정적인 학습</td><td>상대적으로 낮은 운율 표현력 (초기 모델)</td><td>FastSpeech 2 46</td></tr>
<tr><td><strong>Flow/VAE/GAN 기반 (End-to-End)</strong></td><td>VAE, Normalizing Flow, GAN 결합. 텍스트→파형 직접 생성</td><td>높은 품질과 빠른 속도 동시 달성, End-to-End 학습</td><td>복잡한 모델 구조, 학습 난이도</td><td>VITS 61</td></tr>
<tr><td><strong>확산(Diffusion) 모델</strong></td><td>노이즈에서 음성으로 점진적 복원 (Denoising)</td><td>현존 최고 수준의 음성 품질 및 자연성</td><td>매우 느린 추론 속도 (다수의 스텝 필요)</td><td>Grad-TTS, DiffGAN-TTS 64</td></tr>
</tbody></table>
<h3>4.3  평가 생태계: 데이터셋과 도구</h3>
<p>신뢰성 있고 재현 가능한 평가를 위해서는 표준화된 벤치마크 데이터셋과 효율적인 분석 도구로 구성된 평가 생태계를 활용하는 것이 중요하다.</p>
<ul>
<li><strong>주요 벤치마크 데이터셋:</strong> 연구의 공정성과 비교 가능성을 위해 널리 인정받는 공개 데이터셋을 사용하는 것이 표준적인 관행이다.</li>
</ul>
<table><thead><tr><th>표 4: 주요 벤치마크 데이터셋 특성</th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td><strong>데이터셋</strong></td><td><strong>언어</strong></td><td><strong>화자 수</strong></td><td><strong>총 길이</strong></td><td><strong>샘플링 레이트</strong></td><td><strong>주요 특징 및 용도</strong></td></tr>
<tr><td><strong>LJSpeech</strong></td><td>영어</td><td>1 (여성)</td><td>~24시간</td><td>22,050 Hz</td><td>고품질, 단일 화자. 표준 벤치마크 36</td></tr>
<tr><td><strong>VCTK Corpus</strong></td><td>영어</td><td>110 (다양한 억양)</td><td>~44시간</td><td>48,000 Hz</td><td>다화자, 다억양. 다화자 TTS, 음성 변환 68</td></tr>
<tr><td><strong>KSS Dataset</strong></td><td>한국어</td><td>1 (여성)</td><td>~12시간</td><td>44,100 Hz</td><td>고품질, 단일 화자. 한국어 TTS 표준 벤치마크 70</td></tr>
</tbody></table>
<ul>
<li>
<p><strong>평가 자동화를 위한 도구:</strong></p>
</li>
<li>
<p><strong>음성 분석 소프트웨어:</strong> Praat과 WORLD Vocoder는 F0, 스펙트럼 포락선, 비주기성 등 정밀한 음향 특징을 추출하고 시각화하는 데 사용되는 강력한 도구다. 연구자들은 이를 통해 합성음의 미세한 음향적 특성을 심도 있게 분석할 수 있다.72</p>
</li>
<li>
<p><strong>Python 라이브러리:</strong></p>
</li>
<li>
<p><strong><code>librosa</code></strong>: 음성 신호 처리, 특징 추출(MFCC, 크로마 등), 시각화를 위한 가장 기본적인 Python 라이브러리다.75</p>
</li>
<li>
<p><strong><code>pyworld</code></strong>: WORLD Vocoder의 Python 래퍼(wrapper)로, 코드 내에서 손쉽게 F0, 스펙트럼 포락선(sp), 비주기성(ap)을 추출하고 다시 음성으로 합성하는 기능을 제공한다.74</p>
</li>
<li>
<p><strong><code>pypesq</code></strong>, <strong><code>speech_gen_eval</code></strong>, <strong><code>TTS-Evaluation</code></strong>: PESQ (Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective Intelligibility), WER, MCD 등 다양한 객관적 평가 지표를 편리하게 계산할 수 있도록 구현된 특화 라이브러리들이다.44</p>
</li>
</ul>
<h2>5.  비판적 고찰 및 미래 전망</h2>
<p>지금까지 TTS 평가에 널리 사용되는 주관적, 객관적 방법론들을 살펴보았다. 이러한 방법들은 TTS 기술 발전에 크게 기여했지만, 기술이 인간의 소통 방식을 더욱 정교하게 모방하려 함에 따라 그 내재적 한계 또한 명확해지고 있다. 본 섹션에서는 현행 평가 방법론을 비판적으로 고찰하고, 차세대 평가 기술의 동향을 탐색하며, 미래의 TTS 연구가 나아가야 할 방향을 제언한다.</p>
<h3>5.1  현행 TTS 평가 방법론의 내재적 한계</h3>
<ul>
<li>
<p><strong>문맥의 부재 (Lack of Context):</strong> 현재 대부분의 평가는 문맥과 무관한 단일 문장(isolated utterance) 단위로 이루어진다. 이는 대화의 흐름이나 문단 전체의 논리적 구조에 맞춰 억양, 감정, 말의 속도를 조절하는 능력을 제대로 평가하지 못하는 근본적인 한계를 가진다. 실제 대화 상황에서는 바로 앞선 발화가 다음 발화의 운율에 큰 영향을 미치지만, 현재의 평가 방식은 이러한 상호작용을 완전히 배제한다.12</p>
</li>
<li>
<p><strong>객관적 지표와 인간 인지의 불일치 (Cognitive Gap):</strong> MCD, F0 RMSE와 같은 객관적 지표는 계산이 빠르고 편리하지만, 인간이 실제로 인지하는 음질과 항상 일치하지는 않는다. 예를 들어, MCD 값이 낮더라도(스펙트럼이 유사하더라도) 위상 왜곡이나 미세한 잡음으로 인해 청취자는 부자연스러움을 느낄 수 있다. 반대로, MCD 값이 다소 높더라도 인간의 귀에는 자연스럽게 들리는 경우도 존재한다. 이러한 불일치는 객관적 지표가 인간의 복잡한 청각 인지 과정을 완벽히 모델링하지 못하기 때문에 발생한다.21</p>
</li>
<li>
<p><strong>주관적 평가의 편향성 및 모호성 (Subjectivity Bias):</strong> MOS는 황금 표준이지만, 평가 결과가 청취자의 개인적 선호도, 피로도, 문화적 배경, 심지어는 특정 화자에 대한 호감도에 따라 달라질 수 있다.9 또한, 평가자에게 ’자연성’을 평가하라고 요청하는 것과 ’전반적인 품질’을 평가하라고 요청하는 것은 미묘하게 다른 결과를 낳을 수 있으며, 많은 연구에서 이러한 평가 기준을 명확히 구분하지 않아 결과 해석에 혼란을 주기도 한다.10</p>
</li>
<li>
<p><strong>’정답 음성’의 부재 (Absence of a Single Ground Truth):</strong> 평가는 종종 원본 녹음 음성을 유일한 정답(ground truth)으로 간주하고, 합성음이 이와 얼마나 다른지를 측정하는 방식으로 이루어진다. 그러나 특정 텍스트를 발화하는 방식은 무수히 많으며, 원본 녹음은 그중 하나의 실현 예에 불과하다. 합성음이 원본과 운율이나 리듬이 다르다고 해서 반드시 품질이 낮은 것은 아니며, 상황에 따라서는 원본보다 더 명료하거나 듣기 편한 음성을 생성할 수도 있다. 따라서 원본과의 음향적 거리에만 의존하는 평가는 TTS 모델의 잠재력을 제한할 수 있다.78</p>
</li>
</ul>
<h3>5.2  차세대 평가 기술 동향</h3>
<p>이러한 한계를 극복하기 위해 TTS 평가 연구는 새로운 방향으로 나아가고 있다.</p>
<ul>
<li>
<p><strong>자동 MOS 예측 모델 (Automatic MOS Prediction):</strong> 대규모의 인간 평가 데이터(음성 샘플과 해당 MOS 점수 쌍)를 딥러닝 모델에 학습시켜, 새로운 음성 샘플이 주어졌을 때 MOS 점수를 자동으로 예측하는 기술이다. UTMOS, MOSNet과 같은 모델들이 대표적이며, 이는 주관적 평가의 높은 비용과 시간 문제를 해결하고 개발 과정에서 신속한 피드백을 제공할 수 있는 잠재력을 지닌다.34</p>
</li>
<li>
<p><strong>대화형 및 상호작용 기반 평가 (Interactive Evaluation):</strong> 정적인 음성 샘플을 평가하는 대신, 사용자가 TTS 기반의 대화형 에이전트와 실제로 상호작용하는 시나리오에서 평가를 진행한다. 이 방식은 과업 성공률(task success rate), 사용자 만족도, 대화의 자연스러움과 일관성 등을 종합적으로 측정하여, 실제 응용 환경에서의 모델 유용성을 보다 현실적으로 평가할 수 있다.12</p>
</li>
<li>
<p><strong>응용 분야 중심 평가 (Application-centered Evaluation):</strong> ’이상적인 음성’이라는 추상적인 기준에서 벗어나, 특정 응용 분야(예: 오디오북, 내비게이션, 교육용 콘텐츠)에서 요구되는 핵심 품질 요소를 중심으로 평가 프레임워크를 설계하는 접근법이다. 예를 들어, 오디오북 내레이션에서는 감정 표현력이, 차량 내비게이션 안내에서는 명료도가 가장 중요한 평가 기준이 될 수 있다. 이는 TTS 평가의 관점을 ’얼마나 인간과 똑같은가’에서 ’주어진 목적에 얼마나 적합한가’로 전환하는 것을 의미한다.78</p>
</li>
</ul>
<p>이러한 변화는 평가의 초점이 생성된 음성 자체의 ’음향적 속성(acoustic properties)’에서, 그 음성이 사용자와의 상호작용에서 얼마나 효과적으로 기능하는지를 나타내는 ’화용론적 기능(pragmatic functions)’으로 이동하고 있음을 보여준다. TTS가 단순한 ’텍스트 리더’를 넘어 진정한 ’대화 파트너’로 진화하기 위해서는, 평가 패러다임 역시 ’생성(generation)’의 완벽함이 아닌 ’소통(communication)’의 효과성을 측정하는 방향으로 나아가야 한다.</p>
<h3>5.3  종합적이고 신뢰성 있는 평가를 위한 제언</h3>
<p>결론적으로, Text-to-Voice 모델의 성능을 정확하고 의미 있게 평가하기 위해서는 다음과 같은 종합적인 접근이 요구된다.</p>
<ol>
<li>
<p><strong>다중 지표 보고 (Multi-metric Reporting):</strong> 단일 지표, 특히 MOS 점수에만 의존하는 관행에서 벗어나야 한다. 자연성(MOS), 스펙트럼 충실도(MCD), 운율 정확도(F0 RMSE), 명료도(WER), 화자 유사도(코사인 유사도), 효율성(RTF) 등 다양한 차원의 객관적 지표를 함께 보고하여 모델의 성능을 입체적으로 조명해야 한다.</p>
</li>
<li>
<p><strong>평가 조건의 투명성 확보 (Transparency in Methodology):</strong> 연구의 신뢰도와 재현성을 높이기 위해, 주관적 평가 시 사용된 질문, 척도의 정의, 청취자단의 인구통계학적 정보, 평가 환경, 음량 정규화 여부 등을 논문에 상세히 기술해야 한다.34</p>
</li>
<li>
<p><strong>정량적 분석을 넘어선 질적 분석 (Qualitative Error Analysis):</strong> 단순히 평균 점수를 비교하는 데 그치지 않고, 모델이 어떤 종류의 문장(예: 긴 문장, 특수기호가 포함된 문장)이나 특정 음운 환경에서 주로 실패하는지를 분석하는 오류 분석(error analysis)을 병행해야 한다. 이는 모델의 근본적인 약점을 파악하고 개선 방향을 설정하는 데 매우 중요한 통찰을 제공한다.18</p>
</li>
</ol>
<p>궁극적으로 TTS 평가는 하나의 정답을 찾는 과정이 아니라, ‘어떤 목적을 위해’, ‘어떤 측면에서’, ’얼마나 우수한가’를 종합적으로 규명하는 과정임을 인지해야 한다. 이러한 다차원적이고 비판적인 접근만이 더욱 자연스럽고 유용한 음성 합성 기술의 발전을 이끌 수 있을 것이다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Text-to-Speech Technology: Benefits, Applications &amp; Key Components - Lyzr AI, https://www.lyzr.ai/glossaries/text-to-speech/</li>
<li>산업 전반에 걸쳐 텍스트 음성 변환의 이점 | 샤이프 - Shaip, https://ko.shaip.com/blog/benefits-of-text-to-speech/</li>
<li>Text-to-Speech (TTS) 정리 - Jeongwooyeol’s Blog - 티스토리, https://jeongwooyeol0106.tistory.com/124</li>
<li>[논문들소개] Neural Text-to-Speech(TTS) - 음악과 오디오, 인공지능 - 티스토리, https://music-audio-ai.tistory.com/44</li>
<li>Text-to-Speech 101: The Ultimate Guide | by Felix Laumann, PhD | NeuralSpace | Medium, https://medium.com/neuralspace/text-to-speech-101-the-ultimate-guide-9a4b10e20fef</li>
<li>Text to Speech란 무엇인가요? - IBM, https://www.ibm.com/kr-ko/think/topics/text-to-speech</li>
<li>Comprehensive Guide to Text-to-Speech (TTS) Models - Inferless, https://www.inferless.com/learn/comparing-different-text-to-speech—tts–models-for-different-use-cases</li>
<li>[일상에 스며든 AI 음성 인식 서비스] Text-To-Speech (TTS) - ActionPower, <a href="https://actionpower.medium.com/%EC%9D%BC%EC%83%81%EC%97%90-%EC%8A%A4%EB%A9%B0%EB%93%A0-ai-%EC%9D%8C%EC%84%B1-%EC%9D%B8%EC%8B%9D-%EC%84%9C%EB%B9%84%EC%8A%A4-text-to-speech-tts-10828e315d93">https://actionpower.medium.com/%EC%9D%BC%EC%83%81%EC%97%90-%EC%8A%A4%EB%A9%B0%EB%93%A0-ai-%EC%9D%8C%EC%84%B1-%EC%9D%B8%EC%8B%9D-%EC%84%9C%EB%B9%84%EC%8A%A4-text-to-speech-tts-10828e315d93</a></li>
<li>Text to Speech (TTS) in Education: Everything You Need to Know - ReadSpeaker, https://www.readspeaker.com/blog/everything-you-need-to-know-about-text-to-speech-for-education/</li>
<li>Creating content for everyone: A guide to Text-to-Speech assistive technology, https://www.iubenda.com/en/help/183765-text-to-speech-assistive-technology</li>
<li>Evaluating text-to-speech synthesizers - ERIC, https://files.eric.ed.gov/fulltext/ED564181.pdf</li>
<li>Text-to-Speech Technology: What It Is and How It Works | Reading Rockets, https://www.readingrockets.org/topics/assistive-technology/articles/text-speech-technology-what-it-and-how-it-works</li>
<li>글소리 (TTS) - speechtools, https://speechtools.co.kr/tts/</li>
<li>한국어 TTS 시스템의 객관적인 성능평가를 위한 기초검토, https://koreascience.kr/article/CFKO200536035719535.pdf</li>
<li>A review on subjective and objective evaluation of synthetic speech - J-Stage, https://www.jstage.jst.go.jp/article/ast/45/4/45_e24.12/_article</li>
<li>What are common metrics for evaluating TTS quality? - Milvus, https://milvus.io/ai-quick-reference/what-are-common-metrics-for-evaluating-tts-quality</li>
<li>How Mean Opinion Score Ratings Improve Text-To-Speech Models - Way With Words, https://waywithwords.net/landing/mean-opinion-score-ratings-2/</li>
<li>A review on subjective and objective evaluation of synthetic speech, https://homepage.iis.sinica.edu.tw/papers/whm/new-7458-F.pdf</li>
<li>What are the standard evaluation metrics for TTS quality? - Zilliz Vector Database, https://zilliz.com/ai-faq/what-are-the-standard-evaluation-metrics-for-tts-quality</li>
<li>EmoSphere-TTS: Emotional Style and Intensity Modeling via Spherical Emotion Vector for Controllable Emotional Text-to-Speech - arXiv, https://arxiv.org/html/2406.07803v2</li>
<li>Voice Cloning: Modern Techniques for Speaker Identity Synthesis - LyricWinter, https://lyricwinter.com/blog/voice-cloning-evolution</li>
<li>How is Mean Opinion Score (MOS) used in TTS evaluation? - Milvus, https://milvus.io/ai-quick-reference/how-is-mean-opinion-score-mos-used-in-tts-evaluation</li>
<li>arXiv:2307.04517v2 [eess.AS] 10 Oct 2023, https://arxiv.org/pdf/2307.04517</li>
<li>Speech Quality Assessment, https://ecs.utdallas.edu/loizou/cimplants/quality_assessment_chapter.pdf</li>
<li>What metrics can be used to evaluate customized TTS output? - Milvus, https://milvus.io/ai-quick-reference/what-metrics-can-be-used-to-evaluate-customized-tts-output</li>
<li>Comparison of subjective evaluation and an objective evaluation metric for prosody in text-to-speech synthesis - ISCA Archive, https://www.isca-archive.org/ssw_1998/hirst98_ssw.pdf</li>
<li>MOS-FAD: IMPROVING FAKE AUDIO DETECTION VIA AUTOMATIC MEAN OPINION SCORE PREDICTION - arXiv, https://arxiv.org/html/2401.13249v2</li>
<li>Takaaki-Saeki/DiscreteSpeechMetrics: Reference-aware automatic speech evaluation toolkit - GitHub, https://github.com/Takaaki-Saeki/DiscreteSpeechMetrics</li>
<li>Mean opinion score - Wikipedia, https://en.wikipedia.org/wiki/Mean_opinion_score</li>
<li>What is a Mean Opinion Score (MOS)? | Twilio, https://www.twilio.com/docs/glossary/what-is-mean-opinion-score-mos</li>
<li>ITU-T P.800 methods - FORCE Technology, https://forcetechnology.com/-/media/force-technology-media/pdf-files/unnumbered/senselab/product-sheet-itu-t-p,-d-,800-methods.pdf</li>
<li>ITU-T Rec. P.800 (08/96) Methods for subjective determination of transmission quality, https://www.itu.int/rec/dologin_pub.asp?lang=e&amp;id=T-REC-P.800-199608-I!!PDF-E&amp;type=items</li>
<li>Good practices for evaluation of synthesized speech - arXiv, https://arxiv.org/html/2503.03250v1</li>
<li>Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech Evaluation - arXiv, https://arxiv.org/html/2411.12719v3</li>
<li>A critical analysis of MOS test methodology in TTS evaluation - OpenReview, https://openreview.net/pdf?id=bnVBXj-mOc</li>
<li>The evaluation of prosody in speech synthesis: a systematic review - Journals, https://journals-sol.sbc.org.br/index.php/jbcs/article/download/5468/3317</li>
<li>Automatic Evaluation of Speaker Similarity - ISCA Archive, https://www.isca-archive.org/interspeech_2022/deja22_interspeech.pdf</li>
<li>음성 품질 수치 (MOS) - Mint &amp; Latte_. - 티스토리, https://mintnlatte.tistory.com/437</li>
<li>SCOREQ: Speech Quality Assessment with Contrastive Regression - OpenReview, https://openreview.net/pdf?id=HDVsiUHQ1w</li>
<li>mel cepstral distortion (MCD) - learnius, https://learnius.com/slp/9+Speech+Synthesis/1+Fundamental+Concepts/3+Evaluation/mel+cepstral+distortion+(MCD)</li>
<li>mel-cepstral-distance - PyPI, https://pypi.org/project/mel-cepstral-distance/</li>
<li>arXiv:2104.00624v1 [eess.AS] 1 Apr 2021, https://arxiv.org/pdf/2104.00624</li>
<li>Performance of Objective Speech Quality Metrics on Languages Beyond Validation Data - arXiv, https://arxiv.org/pdf/2505.16616</li>
<li>Evaluation of objective measures for quality assessment of reverberant speech | Request PDF - ResearchGate, https://www.researchgate.net/publication/224246186_Evaluation_of_objective_measures_for_quality_assessment_of_reverberant_speech</li>
<li>WER - a Hugging Face Space by evaluate-metric, https://huggingface.co/spaces/evaluate-metric/wer</li>
<li>Word error rate - Wikipedia, https://en.wikipedia.org/wiki/Word_error_rate</li>
<li>음성 정확성 측정 및 개선 - Cloud Speech-to-Text, https://cloud.google.com/speech-to-text/docs/speech-accuracy?hl=ko</li>
<li>단어 오류율(WER)이란? - Speechify, https://speechify.com/ko/blog/what-is-word-error-rate-wer/</li>
<li>How to evaluate the accuracy of a speech recognition system? - Tencent Cloud, https://www.tencentcloud.com/techpedia/120295</li>
<li>Understanding Character Error Rate (CER) for AI Accuracy | Galileo, https://galileo.ai/blog/character-error-rate-cer-metric</li>
<li>hyeonsangjeon/computing-Korean-STT-error-rates: STT 한글 문장 인식기 출력 스크립트의 외자 오류율(CER), 단어 오류율(WER)을 계산하는 Python 함수 패키지 - GitHub, https://github.com/hyeonsangjeon/computing-Korean-STT-error-rates</li>
<li>The evaluation of prosody in speech synthesis: a systematic review - Journals, https://journals-sol.sbc.org.br/index.php/jbcs/article/view/5468</li>
<li>Merlin 툴킷을 이용한 한국어 TTS 시스템의 심층 신경망 구조 성능 비교, https://www.eksss.org/archive/view_article?pid=pss-11-2-57</li>
<li>Performance comparison of various deep neural network architectures using Merlin toolkit for a Korean TTS system, https://www.eksss.org/download/download_pdf?pid=pss-11-2-57</li>
<li>TTSDS - Text-to-Speech Distribution Score - arXiv, https://arxiv.org/html/2407.12707v1</li>
<li>The D-vector space visualization of the speaker embedding from speech… - ResearchGate, https://www.researchgate.net/figure/The-D-vector-space-visualization-of-the-speaker-embedding-from-speech-samples-produced-by_fig2_380298738</li>
<li>Voice Cloning: Comprehensive Survey - arXiv, https://arxiv.org/html/2505.00579v1</li>
<li>Analyzing and Improving Speaker Similarity Assessment for Speech Synthesis - arXiv, https://arxiv.org/html/2507.02176v1</li>
<li>Speaker Cosine Similarity for Various TTS Models using D-vector Speaker Representation., https://www.researchgate.net/figure/Speaker-Cosine-Similarity-for-Various-TTS-Models-using-D-vector-Speaker-Representation_fig2_357666976</li>
<li>EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting - arXiv, https://arxiv.org/html/2504.12867v1</li>
<li>arXiv:2404.01033v2 [eess.AS] 9 Apr 2024, https://arxiv.org/pdf/2404.01033</li>
<li>Optimizing Multilingual Text-To-Speech with Accents and Emotions - arXiv, https://arxiv.org/html/2506.16310v1</li>
<li>EME-TTS: Unlocking the Emphasis and Emotion Link in Speech Synthesis - arXiv, https://arxiv.org/html/2507.12015</li>
<li>EmoSSLSphere: Multilingual Emotional Speech Synthesis with Spherical Vectors and Discrete Speech Tokens - arXiv, https://arxiv.org/html/2508.11273v1</li>
<li>Evaluating Code-Switching Translation with Large Language Models - ACL Anthology, https://aclanthology.org/2024.lrec-main.565/</li>
<li>Enhancing Code-switched Text-to-Speech Synthesis Capability in Large Language Models with only Monolingual Corpora - arXiv, https://arxiv.org/html/2409.10969v2</li>
<li>DECM: Evaluating Bilingual ASR Performance on a Code-switching/mixing Benchmark - ACL Anthology, https://aclanthology.org/2024.lrec-main.400.pdf</li>
<li>VALL-E 2: Enhancing the robustness and naturalness of text-to-speech models - Microsoft, https://www.microsoft.com/en-us/research/articles/vall-e-2-enhancing-the-robustness-and-naturalness-of-text-to-speech-models/</li>
<li>Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment, https://arxiv.org/html/2406.17957v1</li>
<li>What benchmarks are available for comparing different TTS engines? - Milvus, https://milvus.io/ai-quick-reference/what-benchmarks-are-available-for-comparing-different-tts-engines</li>
<li>What are the standard evaluation metrics for TTS quality? - Milvus, https://milvus.io/ai-quick-reference/what-are-the-standard-evaluation-metrics-for-tts-quality</li>
<li>Measuring Speech Quality for Speech Enhancement - Picovoice, https://picovoice.ai/blog/speech-quality/</li>
<li>SVSNet: An End-to-end Speaker Voice Similarity Assessment Model - ResearchGate, https://www.researchgate.net/publication/358721295_SVSNet_An_End-to-end_Speaker_Voice_Similarity_Assessment_Model</li>
<li>Advancing Speech Quality Assessment Through Scientific Challenges and Open-source Activities - arXiv, https://arxiv.org/html/2508.00317v1</li>
<li>[논문 리뷰] MOS-Bench: Benchmarking Generalization Abilities of Subjective Speech Quality Assessment Models - Moonlight, https://www.themoonlight.io/ko/review/mos-bench-benchmarking-generalization-abilities-of-subjective-speech-quality-assessment-models</li>
<li>How to Evaluate Text-to-Speech Models for Voice AI Applications: Insights from Cartesia, https://www.coval.dev/blog/tts-benchmarks</li>
<li>What metrics can be used to evaluate customized TTS output? - Zilliz Vector Database, https://zilliz.com/ai-faq/what-metrics-can-be-used-to-evaluate-customized-tts-output</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>