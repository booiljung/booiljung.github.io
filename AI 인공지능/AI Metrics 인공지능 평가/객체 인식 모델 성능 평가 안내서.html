<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:객체 인식 모델 성능 평가 안내서</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>객체 인식 모델 성능 평가 안내서</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">인공지능 평가지표 (AI evaluation metrics)</a> / <span>객체 인식 모델 성능 평가 안내서</span></nav>
                </div>
            </header>
            <article>
                <h1>객체 인식 모델 성능 평가 안내서</h1>
<h2>1.  객체 인식 모델 성능 평가의 중요성</h2>
<p>객체 인식(Object Detection) 기술은 인공지능 시각 분야의 핵심 과제로, 이미지나 영상 속에서 특정 객체의 종류를 판별하는 분류(Classification) 문제와 그 객체의 정확한 위치를 경계 상자(Bounding Box)로 특정하는 위치 측정(Localization) 문제를 동시에 해결해야 한다.1 이처럼 객체 인식 모델은 “무엇인가?“와 “어디에 있는가?“라는 두 가지 질문에 모두 답해야 하므로, 모델의 성능을 평가하는 것 역시 이 두 가지 측면을 종합적으로 고려해야 한다.</p>
<p>단순히 “모델이 잘 작동한다“는 주관적이고 모호한 판단을 넘어, 정량적이고 객관적인 평가 지표는 모델의 성능을 정확히 파악하고 개선 방향을 설정하는 데 필수적이다. 잘 정의된 평가 지표는 다음과 같은 핵심적인 역할을 수행한다.1</p>
<ul>
<li>
<p><strong>성능 벤치마킹:</strong> 서로 다른 모델이나 알고리즘의 성능을 공정하고 일관된 기준으로 비교할 수 있게 하여, 특정 과업에 가장 적합한 모델을 선택하는 근거를 제공한다.</p>
</li>
<li>
<p><strong>오류 분석(Error Analysis):</strong> 정밀도(Precision), 재현율(Recall)과 같은 세부 지표를 분석함으로써 모델이 어떤 종류의 오류를 주로 범하는지 파악할 수 있다. 예를 들어, 모델이 존재하는 객체를 자주 놓치는지(낮은 재현율), 혹은 배경을 객체로 잘못 탐지하는지(낮은 정밀도) 등을 분석하여 모델 개선의 구체적인 방향을 설정할 수 있다.1</p>
</li>
<li>
<p><strong>신뢰성 확보:</strong> 자율주행, 의료 영상 분석, 보안 감시 등 실제 산업 현장에 모델을 적용하기 전, 요구되는 성능 수준을 충족하는지 객관적으로 검증하여 시스템의 신뢰성과 안전성을 보장한다.4 경영학의 대가 피터 드러커(Peter Drucker)가 남긴 “측정되는 것이 개선된다(What gets measured gets improved)“는 격언은 객체 인식 모델 개발 과정에서도 동일하게 적용된다.1</p>
</li>
</ul>
<p>더 나아가, 성능 평가 지표는 단순히 모델의 성적을 매기는 수동적인 도구에 그치지 않고, 연구 개발의 방향성을 제시하며 기술 혁신을 유도하는 능동적인 역할을 수행한다. 객체 인식 기술의 발전 과정을 살펴보면, 평가 기준의 변화가 모델 아키텍처의 진화를 직접적으로 견인해왔음을 알 수 있다. 초기 연구는 PASCAL VOC 데이터셋의 <code>mAP@0.5</code>와 같이 비교적 덜 엄격한 위치 정확도를 요구하는 지표를 표준으로 삼았다.6 이는 모델이 객체를 ‘찾는 것’ 자체에 중점을 두도록 유도했다. 그러나 기술이 성숙함에 따라 단순히 객체를 찾는 것을 넘어 ‘매우 정확하게’ 위치를 특정하는 능력의 중요성이 커졌고, 이에 COCO 벤치마크는 <code>mAP@[.5:.05:.95]</code>라는 훨씬 더 엄격한 평가 기준을 도입했다.8 이 새로운 기준은 연구 커뮤니티로 하여금 더 정교한 경계 상자 회귀(bounding box regression) 기술이나 고해상도 특징 맵(feature map)을 활용하는 FPN(Feature Pyramid Network)과 같은 진보된 아키텍처를 개발하도록 강력하게 동기 부여했다. 이처럼 평가 지표의 발전은 모델 아키텍처의 발전을 이끄는 인과 관계를 형성하며, 평가 기준이 까다로워질수록 모델은 그 기준을 충족시키기 위해 더욱 정교하게 진화한다.</p>
<h2>2.  평가의 기초: 기본 용어 정의</h2>
<p>객체 인식 모델의 성능을 평가하기 위해 사용되는 지표들을 이해하기 위해서는 먼저 몇 가지 기본 용어에 대한 명확한 정의가 필요하다.</p>
<h3>2.1 Ground Truth (실측값) vs. Prediction (예측값)</h3>
<ul>
<li>
<p><strong>Ground Truth:</strong> 사람이 직접 이미지에 레이블을 달아 생성한 ‘정답’ 데이터이다. 이미지 내에 존재하는 모든 객체에 대해 정확한 클래스 이름과 위치(경계 상자 좌표) 정보를 포함한다. 이는 모델 성능 평가의 절대적인 기준이 된다.2</p>
</li>
<li>
<p><strong>Prediction:</strong> 학습된 모델이 이미지를 입력받아 추론을 통해 출력한 결과물이다. 각 예측은 일반적으로 세 가지 요소로 구성된다: (1) 예측된 객체의 클래스, (2) 예측된 경계 상자의 좌표, 그리고 (3) 해당 예측에 대한 모델의 확신 수준을 나타내는 신뢰도 점수(Confidence Score)이다.2</p>
</li>
</ul>
<h3>2.2 신뢰도 점수 (Confidence Score)</h3>
<p>신뢰도 점수는 모델이 자신의 예측을 얼마나 ’확신’하는지를 0과 1 사이의 값으로 정량화한 수치이다.1 이 점수는 일반적으로 경계 상자 안에 객체가 존재할 확률(objectness score)과, 그 객체가 특정 클래스에 속할 확률(class score)의 곱으로 계산된다.1 신뢰도 점수는 모든 예측 결과에 순위를 매기는 기준으로 사용되며, 이 점수를 기준으로 임계값(threshold)을 조절함으로써 후술할 정밀도와 재현율의 관계를 조절하고 정밀도-재현율 곡선(Precision-Recall Curve)을 그리는 데 결정적인 역할을 한다.</p>
<h3>2.3 탐지 결과의 분류</h3>
<p>모델이 출력한 각각의 예측(Prediction)은 실측값(Ground Truth)과 비교하여 다음 네 가지 유형 중 하나로 분류된다. 이 분류 과정에서는 일반적으로 IoU(Intersection over Union) 임계값이 기준으로 사용된다.</p>
<ul>
<li>
<p><strong>True Positive (TP, 진양성):</strong> 모델이 객체를 올바르게 탐지한 경우이다. 구체적으로, 예측된 경계 상자가 대응하는 실측값 경계 상자와의 IoU가 미리 정해진 임계값 이상이며, 예측된 클래스 또한 올바를 때 TP로 간주된다.3</p>
</li>
<li>
<p><strong>False Positive (FP, 위양성):</strong> 모델이 잘못된 탐지를 한 경우이다. 이는 다음 세 가지 상황을 포함한다: (1) 객체가 없는 배경을 객체로 잘못 탐지한 경우, (2) 객체의 위치를 매우 부정확하게 예측하여 IoU가 임계값 미만인 경우, (3) 객체의 위치는 잘 맞췄으나 클래스를 잘못 분류한 경우이다.3</p>
</li>
<li>
<p><strong>False Negative (FN, 위음성):</strong> 모델이 이미지에 실제로 존재하는 객체를 탐지하지 못하고 놓친 경우이다. 이는 실측값은 존재하지만, 그에 대응하는 모델의 예측이 없는 상황을 의미한다.3</p>
</li>
<li>
<p><strong>True Negative (TN, 진음성):</strong> 객체 인식 분야에서는 일반적으로 고려되지 않는 개념이다. TN은 모델이 객체가 없는 배경 영역을 ’객체가 없음’으로 올바르게 판단한 경우를 의미하는데, 하나의 이미지 안에는 객체가 존재하지 않는 잠재적 경계 상자가 무한히 많기 때문에 이를 모두 TN으로 계산하는 것은 통계적으로 의미가 없다.3</p>
</li>
</ul>
<h2>3.  위치 정확도 측정: IoU (Intersection over Union)</h2>
<p>객체 인식에서 모델이 객체의 위치를 얼마나 정확하게 예측했는지를 평가하는 것은 분류 정확도만큼이나 중요하다. 이를 위한 핵심 척도가 바로 IoU(Intersection over Union)이다.</p>
<h3>3.1 IoU의 개념</h3>
<p>IoU는 예측된 경계 상자(Predicted Bounding Box)와 실측값 경계 상자(Ground Truth Bounding Box)가 얼마나 서로 겹치는지를 정량적으로 나타내는 지표이다.1 자카드 지수(Jaccard Index)라고도 불리며, 값의 범위는 0에서 1 사이다. 값이 0이면 두 상자가 전혀 겹치지 않음을, 1이면 두 상자가 완벽하게 일치함을 의미한다.10 IoU는 모델의 위치 특정(Localization) 성능을 측정하는 가장 기본적인 척도이다.13</p>
<h3>3.2 IoU의 수학적 정의</h3>
<p>IoU는 두 경계 상자의 교집합(Intersection) 영역의 넓이를 합집합(Union) 영역의 넓이로 나눈 값으로 계산된다.3</p>
<ul>
<li>
<p><strong>공식:</strong></p>
<p><span class="math math-display">
IoU = \frac{\text{Area of Overlap}}{\text{Area of Union}} = \frac{\text{Area}(B_p \cap B_{gt})}{\text{Area}(B_p \cup B_{gt})}
</span></p>
</li>
</ul>
<p>여기서 <span class="math math-inline">B_p</span>는 예측 박스를, <span class="math math-inline">B_{gt}</span>는 실측 박스를 의미한다. 합집합 영역은 두 박스의 넓이를 각각 더한 후, 중복으로 계산된 교집합 영역의 넓이를 한 번 빼주어 계산한다 (<span class="math math-inline">Area(B_p) + Area(B_{gt}) - Area(B_p \cap B_{gt})</span>).14</p>
<h3>3.3 IoU 임계값(Threshold)의 역할</h3>
<p>IoU 값 자체는 하나의 예측에 대한 위치 정확도를 나타내지만, 평가 과정에서는 주로 예측이 성공적인지(TP) 실패인지(FP)를 판정하는 ’문지기(gatekeeper)’로서의 역할을 수행한다.10 평가를 시작하기 전에 IoU 임계값(예: 0.5)을 먼저 설정한다. 그 후, 모델의 각 예측에 대해 실측값과의 IoU를 계산하여 이 값이 임계값보다 크거나 같으면 TP로, 작으면 FP로 분류한다.1 이 임계값은 애플리케이션의 요구사항에 따라 유연하게 조절될 수 있다. 예를 들어, 자율주행차의 차선 탐지와 같이 매우 정밀한 위치 특정이 필요하다면 임계값을 0.75 또는 그 이상으로 높게 설정할 수 있으며, 단순히 객체의 존재 유무를 확인하는 것이 중요하다면 0.3과 같이 낮게 설정할 수도 있다.10</p>
<p>여기서 IoU의 중요한 이중적 성격이 드러난다. IoU는 모델의 위치 예측 성능을 나타내는 최종 결과 지표(metric)인 동시에, 평가 과정 자체의 엄격함을 제어하는 하이퍼파라미터(hyperparameter)로서 기능한다. 동일한 모델이 동일한 예측 결과를 출력했더라도, 평가자가 IoU 임계값을 어떻게 설정하느냐에 따라 성능 점수는 크게 달라질 수 있다. 예를 들어, 한 예측의 IoU가 0.6이라고 가정해보자. IoU 임계값을 0.5로 설정하면 이 예측은 TP로 분류된다. 하지만 임계값을 0.7로 올리면 동일한 예측은 FP로 재분류된다. 이처럼 TP와 FP의 수가 바뀌면, 이를 기반으로 계산되는 정밀도, 재현율, AP, mAP 등 모든 후속 지표들이 연쇄적으로 변하게 된다.16 이는 ’성능’이라는 개념이 절대적이지 않으며, 어떤 기준으로 평가하는지에 따라 상대적일 수 있음을 시사한다. 따라서 모델의 성능을 논할 때는 단순히 “mAP가 75%이다“라고 말하는 것만으로는 불충분하며, “IoU 임계값 0.5에서 mAP가 75%이다”(즉, mAP@0.5)와 같이 평가 기준을 명확히 명시해야만 그 의미가 왜곡 없이 전달될 수 있다.</p>
<h2>4.  분류 성능 측정: 정밀도(Precision)와 재현율(Recall)</h2>
<p>IoU를 통해 각 예측이 TP 또는 FP로 분류되면, 이를 바탕으로 모델의 분류 성능을 평가할 수 있다. 이때 가장 널리 사용되는 지표가 정밀도(Precision)와 재현율(Recall)이다.</p>
<h3>4.1 정밀도 (Precision)</h3>
<p>정밀도는 모델이 ‘Positive’(즉, 특정 클래스의 객체)라고 예측한 것들 중에서 실제로 ’Positive’인 것의 비율을 의미한다. 이는 모델 예측의 ’질(quality)’을 측정하는 지표로 볼 수 있다.17</p>
<ul>
<li>
<p><strong>의미:</strong> 정밀도가 높다는 것은 모델이 “이것은 객체다“라고 예측했을 때, 그 예측이 맞을 확률이 높다는 것을 의미한다. 즉, FP(허위 경보, False Alarm)가 적다는 뜻이다.11 예를 들어, 보안 시스템에서 침입자가 아닌 사람을 침입자로 오인하는 경우(FP)는 큰 불편을 초래하므로 높은 정밀도가 요구된다.11</p>
</li>
<li>
<p><strong>공식:</strong></p>
<p><span class="math math-display">
\text{Precision} = \frac{TP}{TP + FP}
</span></p>
</li>
</ul>
<h3>4.2 재현율 (Recall)</h3>
<p>재현율은 실제 ’Positive’인 모든 객체들 중에서 모델이 ’Positive’라고 올바르게 예측한 것의 비율을 의미한다. 민감도(Sensitivity)라고도 불리며, 모델이 실제 객체들을 얼마나 빠짐없이 ‘찾아내는가’ 즉, 예측의 ’양(quantity)’을 측정하는 지표이다.17</p>
<ul>
<li>
<p><strong>의미:</strong> 재현율이 높다는 것은 모델이 탐지해야 할 객체들을 놓치는 경우(FN)가 적다는 것을 의미한다.3 예를 들어, 의료 영상에서 암 종양을 진단하는 모델의 경우, 실제 암 환자를 놓치는 것(FN)은 치명적인 결과를 초래할 수 있으므로 매우 높은 재현율이 필수적이다.4</p>
</li>
<li>
<p><strong>공식:</strong></p>
<p><span class="math math-display">
\text{Recall} = \frac{TP}{TP + FN}
</span></p>
</li>
</ul>
<h3>4.3 정밀도와 재현율의 상충 관계 (Trade-off)</h3>
<p>정밀도와 재현율은 일반적으로 상충 관계(trade-off)에 있다. 즉, 하나의 지표를 높이려고 하면 다른 지표가 낮아지는 경향이 있다.17 이 관계는 모델의 신뢰도 임계값을 조절함으로써 제어할 수 있다. 임계값을 높이면 모델은 매우 확실한 예측만 결과로 내놓게 되므로, FP가 줄어들어 정밀도는 높아지지만, 조금이라도 애매한 객체들은 놓치게 되어(FN 증가) 재현율은 낮아진다. 반대로 임계값을 낮추면 모델이 더 많은 객체를 탐지하게 되어 FN은 줄고 재현율은 높아지지만, 그 과정에서 FP도 함께 증가하여 정밀도는 낮아지게 된다.19</p>
<h3>4.4 F1 점수 (F1 Score)</h3>
<p>F1 점수는 정밀도와 재현율의 조화 평균(harmonic mean)으로, 두 지표를 하나의 값으로 결합하여 모델의 균형 잡힌 성능을 평가하고자 할 때 사용된다.3</p>
<ul>
<li>
<p><strong>의미:</strong> F1 점수는 정밀도와 재현율이 모두 균형 있게 높을 때 높은 값을 가진다. 산술 평균 대신 조화 평균을 사용하는 이유는 두 지표 중 어느 한쪽이라도 극단적으로 낮으면 F1 점수 또한 낮아지도록 하여, 한쪽에만 치우친 성능을 가진 모델에 페널티를 부여하기 위함이다.4 특히, 탐지하려는 객체의 수가 전체 이미지 영역에 비해 매우 적은 클래스 불균형 상황에서 정확도(Accuracy)보다 훨씬 유용한 평가 지표가 될 수 있다.23</p>
</li>
<li>
<p><strong>공식:</strong></p>
<p><span class="math math-display">
F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
</span></p>
</li>
</ul>
<h2>5.  종합 성능 시각화: 정밀도-재현율 곡선 (Precision-Recall Curve)</h2>
<p>정밀도와 재현율의 상충 관계 때문에, 특정 임계값에서의 단일 점수만으로는 모델의 전체적인 성능을 파악하기 어렵다. 정밀도-재현율 곡선(PR Curve)은 모든 가능한 임계값에 대한 모델의 성능 변화를 시각적으로 보여주는 강력한 도구이다.</p>
<h3>5.1 PR 곡선의 개념</h3>
<p>PR 곡선은 모델의 신뢰도 임계값을 0에서 1까지 연속적으로 변화시키면서, 각 임계값 지점에서의 재현율을 x축에, 정밀도를 y축에 표시하여 그린 그래프이다.15 이 곡선을 통해 모델이 다양한 재현율 수준에서 어느 정도의 정밀도를 유지할 수 있는지 한눈에 파악할 수 있다.</p>
<h3>5.2 PR 곡선 도출 과정</h3>
<p>PR 곡선을 그리는 과정은 다음과 같다.</p>
<ol>
<li>
<p>테스트 데이터셋에 대한 모델의 모든 예측 결과를 신뢰도 점수가 높은 순서대로 정렬한다.8</p>
</li>
<li>
<p>정렬된 목록을 맨 위(가장 신뢰도 높은 예측)부터 하나씩 순차적으로 확인한다. 각 예측에 대해 미리 설정된 IoU 임계값을 기준으로 실측값과 비교하여 TP 또는 FP로 판정한다.</p>
</li>
<li>
<p>각 순위(rank)까지의 누적 TP와 FP 수를 계산한다. 이를 바탕으로 해당 지점에서의 정밀도(<span class="math math-inline">TP / (TP + FP)</span>)와 재현율(<span class="math math-inline">TP / (\text{Total Ground Truths})</span>)을 계산한다.11</p>
</li>
<li>
<p>계산된 모든 (재현율, 정밀도) 쌍을 좌표평면에 점으로 찍고 이를 연결하여 PR 곡선을 완성한다.</p>
</li>
</ol>
<h3>5.3 PR 곡선 해석</h3>
<p>PR 곡선의 형태는 모델의 성능을 직관적으로 말해준다.</p>
<ul>
<li>
<p>이상적인 모델의 PR 곡선은 그래프의 우측 상단 모서리(재현율=1, 정밀도=1)에 최대한 가깝게 그려진다. 이는 모델이 모든 객체를 놓치지 않으면서(높은 재현율), 동시에 잘못된 탐지도 거의 하지 않음(높은 정밀도)을 의미한다.26</p>
</li>
<li>
<p>곡선 아래의 면적이 넓을수록 전반적인 성능이 우수하다고 평가할 수 있다. 두 모델의 PR 곡선을 비교했을 때, 한 곡선이 다른 곡선보다 전체적으로 더 위쪽(우측 상단)에 위치한다면, 그 모델의 성능이 더 뛰어나다고 볼 수 있다.15</p>
</li>
<li>
<p>PR 곡선은 특정 애플리케이션의 요구사항에 맞는 최적의 동작점(operating point), 즉 신뢰도 임계값을 선택하는 데 유용한 정보를 제공한다.21</p>
</li>
</ul>
<h2>6.  단일 클래스 성능의 표준: 평균 정밀도 (Average Precision, AP)</h2>
<p>PR 곡선은 모델 성능을 시각적으로 풍부하게 보여주지만, 여러 모델의 성능을 정량적으로 비교하기 위해서는 곡선 전체를 대표하는 단일 수치가 필요하다. 이 역할을 하는 것이 바로 평균 정밀도(Average Precision, AP)이다.</p>
<h3>6.1 AP의 개념</h3>
<p>AP는 특정 객체 클래스 하나에 대한 PR 곡선 아래의 면적(Area Under the Curve, AUC)을 계산한 값이다.15 즉, 0부터 1까지 모든 재현율 수준에 걸쳐 정밀도의 평균값을 구하는 것으로, 해당 클래스에 대한 모델의 종합적인 성능을 0과 1 사이의 단일 숫자로 요약해준다.20 AP 값이 높을수록 해당 클래스에 대한 모델의 탐지 성능이 우수함을 의미한다.3</p>
<h3>6.2 AP 계산 방법론</h3>
<p>실제 데이터로 그린 PR 곡선은 매끄러운 곡선이 아니라 계단 형태의 지그재그 모양으로 나타나기 때문에, 면적을 정확히 계산하기 위해 보간(Interpolation) 기법이 사용된다.</p>
<ul>
<li><strong>All-Point Interpolation (Area Under Curve 방식):</strong> 현재 가장 널리 사용되는 방식으로, PASCAL VOC 2010 이후 버전과 COCO 벤치마크에서 채택하고 있다.20 이 방법은 PR 곡선의 모든 변곡점(재현율이 변하는 모든 지점)을 고려하여 면적을 정밀하게 계산한다. 재현율이</li>
</ul>
<p><span class="math math-inline">R_{n-1}</span>에서 <span class="math math-inline">R_n</span>으로 증가했을 때의 정밀도 값 <span class="math math-inline">P_n</span>을 해당 구간의 높이로 보고, <span class="math math-inline">(R_n - R_{n-1}) \times P_n</span> 형태의 작은 사각형들의 넓이를 모두 더하는 방식으로 계산된다.27</p>
<ul>
<li>
<p><strong>공식:</strong></p>
<p><span class="math math-display">
  AP = \sum_{n} (R_n - R_{n-1}) P_n
</span></p>
</li>
<li>
<p><strong>11-Point Interpolation (PASCAL VOC 2010 이전 방식):</strong> 과거 PASCAL VOC 챌린지에서 사용되었던 근사 계산 방식이다.8 재현율 구간을 0, 0.1, 0.2,…, 1.0의 11개 지점으로 균등하게 나눈 뒤, 각 지점에서의 보간된 정밀도 값을 산술 평균하여 AP를 구한다.8</p>
</li>
<li>
<p><strong>보간 규칙:</strong> 특정 재현율 <span class="math math-inline">r</span>에서의 보간된 정밀도 <span class="math math-inline">p_{interp}(r)</span>은, 실제 재현율이 <span class="math math-inline">r</span> 이상인 모든 지점에서의 정밀도 값들 중 최댓값으로 정의된다. 즉, <span class="math math-inline">p_{interp}(r) = \max_{r&#39; : r&#39; \ge r} p(r&#39;)</span>이다. 이 규칙은 PR 곡선의 지그재그 패턴을 완만하게 만들어 단조 감소하는 형태로 바꾸는 효과가 있다.8</p>
</li>
<li>
<p><strong>공식:</strong></p>
<p><span class="math math-display">
  AP = \frac{1}{11} \sum_{r \in \{0, 0.1,..., 1\}} p_{interp}(r)
</span></p>
</li>
</ul>
<p>AP 계산에서 이러한 보간법을 사용하는 것은 단순히 면적을 구하는 수학적 편의성을 넘어, 평가의 안정성과 강건함(robustness)을 높이는 중요한 목적을 가진다. 모델의 예측 결과는 신뢰도 점수에 따라 정렬되는데, 만약 신뢰도 점수가 아주 근소한 차이를 보이는 두 예측(하나는 TP, 다른 하나는 FP)의 순서가 바뀌면 PR 곡선에 작은 ’지그재그(wiggle)’가 발생할 수 있다.8 만약 이 작은 변동이 AP 점수에 직접적인 영향을 미친다면, 모델의 본질적인 성능 변화가 없음에도 평가 점수가 흔들릴 수 있다. 보간법, 특히 최대 정밀도 값을 사용하는 규칙은 이러한 사소한 순위 변동에 평가 점수가 민감하게 반응하지 않도록 만들어준다. 즉, 보간법은 평가 지표 자체의 안정성을 확보하고, 서로 다른 모델 간의 성능 비교를 더 공정하고 신뢰할 수 있게 만드는 표준화 장치로서 기능한다.</p>
<h2>7.  모델 전체 성능의 척도: 평균 AP (mean Average Precision, mAP)</h2>
<p>AP가 단일 클래스에 대한 성능 척도라면, mAP(mean Average Precision)는 데이터셋에 존재하는 모든 클래스에 대한 성능을 종합하여 모델의 전체적인 성능을 나타내는 지표이다.</p>
<h3>7.1 mAP의 정의</h3>
<p>mAP는 데이터셋의 모든 객체 클래스에 대해 각각 AP를 계산한 후, 이 값들을 모두 산술 평균하여 구한다.5 이는 객체 인식 모델의 전반적인 성능을 평가하는 가장 보편적이고 핵심적인 단일 지표로 널리 사용된다.1</p>
<h3>7.2 주요 벤치마크별 mAP 계산 방식 비교</h3>
<p>mAP는 계산되는 방식, 특히 어떤 IoU 임계값을 사용하느냐에 따라 그 의미가 달라질 수 있다. 대표적인 두 가지 표준은 PASCAL VOC와 COCO이다.</p>
<ul>
<li>
<p><strong>PASCAL VOC 기준:</strong> PASCAL VOC 챌린지에서는 단일 IoU 임계값인 0.5를 기준으로 mAP를 계산한다. 따라서 여기서의 mAP는 종종 <strong>mAP@0.5</strong> 또는 <strong>AP50</strong>으로 명확하게 표기된다.7 이 방식은 예측된 경계 상자와 실측값이 50% 이상 겹치면 올바른 위치 예측으로 간주하므로, 비교적 너그럽게 위치 정확도를 평가하는 기준이라 할 수 있다.6</p>
</li>
<li>
<p><strong>COCO 기준:</strong> COCO 챌린지에서는 훨씬 더 엄격하고 종합적인 평가 방식을 사용한다. 가장 대표적인 COCO mAP 지표는 IoU 임계값을 0.5에서 시작하여 0.95까지 0.05 간격으로 변화시키면서(즉, 0.5, 0.55, 0.6,…, 0.95의 10개 임계값) 각 임계값에 대한 AP를 모두 계산한 후, 이를 모든 클래스와 모든 임계값에 대해 평균 낸 값이다. 이는 보통 <strong>mAP@[.5:.05:.95]</strong> 또는 논문 등에서 단순히 <strong>mAP</strong>로 표기된다.8</p>
</li>
<li>
<p><strong>두 방식의 차이가 시사하는 바:</strong> COCO의 평가 방식은 PASCAL VOC 방식보다 훨씬 더 까다롭다. 높은 IoU 임계값(예: 0.9)에서도 좋은 성능을 유지해야만 높은 최종 mAP 점수를 얻을 수 있기 때문이다. 따라서 COCO mAP는 모델이 단순히 객체를 탐지하는 것을 넘어, 얼마나 그 위치를 ‘정밀하게’ 예측하는지를 강력하게 평가하는 지표이다.6 결과적으로, COCO mAP 점수가 높은 모델은 일반적으로 위치 특정(Localization) 성능이 매우 뛰어나다고 해석할 수 있다.</p>
</li>
</ul>
<p>아래 표는 지금까지 설명한 핵심 성능 지표들을 요약한 것이다.</p>
<table><thead><tr><th>지표 (Metric)</th><th>공식 (Formula)</th><th>설명 (Description)</th></tr></thead><tbody>
<tr><td>IoU</td><td><span class="math math-inline">\frac{\text{Area}(B_p \cap B_{gt})}{\text{Area}(B_p \cup B_{gt})}</span></td><td>예측 박스와 실측 박스의 겹침 정도. 위치 정확도 측정.</td></tr>
<tr><td>정밀도 (Precision)</td><td><span class="math math-inline">\frac{TP}{TP + FP}</span></td><td>예측된 결과 중 실제 정답의 비율. 예측의 질(Quality) 평가.</td></tr>
<tr><td>재현율 (Recall)</td><td><span class="math math-inline">\frac{TP}{TP + FN}</span></td><td>실제 정답 중 모델이 맞춘 비율. 탐지의 양(Quantity) 평가.</td></tr>
<tr><td>F1 점수 (F1 Score)</td><td><span class="math math-inline">2 \cdot \frac{P \cdot R}{P + R}</span></td><td>정밀도와 재현율의 조화 평균. 두 지표의 균형 평가.</td></tr>
<tr><td>AP</td><td><span class="math math-inline">\sum_{n} (R_n - R_{n-1}) P_n</span></td><td>단일 클래스에 대한 PR 곡선 하단 면적. 클래스별 종합 성능.</td></tr>
<tr><td>mAP</td><td><span class="math math-inline">\frac{1}{N} \sum_{i=1}^{N} AP_i</span></td><td>모든 N개 클래스에 대한 AP의 평균. 모델의 전체적인 정확도.</td></tr>
</tbody></table>
<p>아래 표는 PASCAL VOC와 COCO의 mAP 평가 기준의 주요 차이점을 비교한 것이다.</p>
<table><thead><tr><th>특징 (Feature)</th><th>PASCAL VOC</th><th>COCO</th></tr></thead><tbody>
<tr><td><strong>주요 지표</strong></td><td>mAP@0.5</td><td>mAP@[.5:.05:.95]</td></tr>
<tr><td><strong>IoU 임계값</strong></td><td>0.5 (단일)</td><td>0.5, 0.55,…, 0.95 (10개 평균)</td></tr>
<tr><td><strong>평가 초점</strong></td><td>객체 탐지 (Detection)</td><td>정밀한 위치 특정 (Precise Localization)</td></tr>
<tr><td><strong>AP 계산법</strong></td><td>11-Point Interpolation (2010 이전), All-Point (이후)</td><td>All-Point Interpolation</td></tr>
<tr><td><strong>결과 해석</strong></td><td>IoU=0.5 기준에서의 평균 성능</td><td>다양한 수준의 위치 정확도를 종합적으로 고려한 평균 성능</td></tr>
</tbody></table>
<h2>8.  현실적 제약 조건: 속도와 효율성 평가</h2>
<p>실제 환경에서 객체 인식 모델을 배포할 때는 mAP로 대표되는 정확도뿐만 아니라, 추론 속도와 계산 효율성 또한 매우 중요한 고려사항이다. mAP 점수가 아무리 높아도 추론 속도가 너무 느리거나 과도한 컴퓨팅 자원을 요구한다면 실제 애플리케이션에 적용하기 어렵다. 따라서 정확도와 효율성 간의 균형을 맞추는 것이 중요하다.29</p>
<h3>8.1 추론 속도 지표</h3>
<ul>
<li>
<p><strong>FPS (Frames Per Second):</strong> 모델이 1초당 처리할 수 있는 이미지(프레임)의 수를 나타낸다. 실시간 영상 분석이나 CCTV 감시와 같은 애플리케이션에서 가장 직관적이고 중요한 속도 지표이다. FPS 값이 높을수록 더 원활한 실시간 처리가 가능하다.29</p>
</li>
<li>
<p><strong>지연 시간 (Latency):</strong> 단일 이미지를 모델에 입력한 순간부터 결과가 출력되기까지 걸리는 총 시간을 의미하며, 보통 밀리초(ms) 단위로 측정된다. 자율주행차나 로봇 제어와 같이 즉각적인 반응이 필수적인 시스템에서는 지연 시간을 최소화하는 것이 매우 중요하다. FPS와 지연 시간은 서로 역수 관계에 가깝다 (<span class="math math-inline">FPS \approx 1000 / Latency(ms)</span>).29</p>
</li>
</ul>
<h3>8.2 계산 복잡도 및 모델 크기 지표</h3>
<p>이 지표들은 특정 하드웨어에 대한 의존성 없이 모델 자체의 이론적인 복잡도를 평가한다.</p>
<ul>
<li>
<p><strong>FLOPs (Floating Point Operations):</strong> 모델이 단일 이미지를 추론하는 데 필요한 총 부동소수점 연산의 양을 나타낸다. 이는 모델의 이론적인 계산 복잡도를 측정하는 하드웨어 독립적인 지표이다. 일반적으로 GFLOPs (10^9 FLOPs) 단위로 표기된다.29 (주의: 모든 글자가 대문자인 FLOPS는 초당 부동소수점 연산 횟수를 의미하는 하드웨어 성능 단위이므로 구분해야 한다 34).</p>
</li>
<li>
<p><strong>MACs (Multiply-Accumulate Operations):</strong> 곱셈-누산 연산의 수를 의미한다. 딥러닝, 특히 CNN 연산의 대부분이 MAC 연산으로 이루어져 있어 계산량을 측정하는 데 자주 사용된다. 1개의 MAC 연산은 1개의 곱셈과 1개의 덧셈으로 구성되므로, 대략 2 FLOPs에 해당한다고 간주된다.35</p>
</li>
<li>
<p><strong>파라미터 수 (Parameters):</strong> 모델이 학습을 통해 얻은 가중치(weight)와 편향(bias)의 총 개수를 의미한다. 이는 모델의 크기를 나타내며, 모델을 저장하는 데 필요한 디스크 공간과 추론 시 필요한 메모리(RAM) 용량과 직접적인 관련이 있다.29</p>
</li>
</ul>
<h3>8.3 정확도와 속도의 상충 관계 분석</h3>
<p>일반적으로 모델의 복잡도(파라미터 수, FLOPs)가 높을수록 더 많은 특징을 학습할 수 있어 정확도(mAP)는 향상되지만, 그만큼 계산량이 많아져 속도(FPS)는 저하되는 상충 관계가 존재한다.29 이러한 관계는 객체 인식 모델의 발전사에서 뚜렷하게 나타난다.</p>
<ul>
<li>
<p><strong>사례 연구: YOLO vs. Faster R-CNN:</strong></p>
</li>
<li>
<p><strong>YOLO 계열:</strong> ’You Only Look Once’라는 이름처럼, 이미지를 한 번만 보고 객체의 위치와 클래스를 동시에 예측하는 1-stage detector이다. 구조가 단순하여 추론 속도가 매우 빠르기 때문에 실시간 처리에 강점을 보인다. 하지만 후보 영역을 먼저 추출하는 2-stage 방식에 비해 상대적으로 작은 객체를 탐지하거나 위치를 정밀하게 특정하는 데는 약점을 보일 수 있다.37</p>
</li>
<li>
<p><strong>Faster R-CNN 계열:</strong> Region Proposal Network(RPN)라는 별도의 네트워크를 통해 객체가 있을 법한 후보 영역을 먼저 제안하고, 이후 각 후보 영역에 대해 분류와 위치 보정을 수행하는 2-stage detector이다. 이처럼 두 단계에 걸쳐 정교하게 탐지를 수행하므로 정확도가 높고, 특히 작은 객체 탐지에 강하다. 그러나 구조가 복잡하고 계산량이 많아 YOLO 계열에 비해 속도가 현저히 느리다.30</p>
</li>
</ul>
<p>이러한 상충 관계는 결국 어떤 모델이 절대적으로 우수하다고 말할 수 없으며, 애플리케이션의 구체적인 요구사항(실시간성이 중요한가, 아니면 최고의 정확도가 중요한가)에 따라 최적의 모델을 선택해야 함을 시사한다.39</p>
<h2>9.  결론: 올바른 평가 지표 선택을 위한 종합적 고찰</h2>
<p>객체 인식 모델의 성능을 올바르게 평가하는 것은 성공적인 모델 개발과 배포의 핵심이다. 이 안내서에서 살펴본 다양한 지표들은 각각 모델 성능의 다른 측면을 조명하므로, 어느 하나에만 의존하기보다는 종합적인 관점에서 접근해야 한다.</p>
<p>최종적으로 어떤 평가 지표에 가중치를 둘 것인지는 해당 모델이 사용될 애플리케이션의 목적과 제약 조건에 따라 결정되어야 한다.</p>
<ul>
<li>
<p><strong>실시간 CCTV 감시 시스템</strong>에서는 수많은 영상 프레임을 지연 없이 처리하는 것이 중요하므로, mAP가 다소 낮더라도 FPS와 Latency가 더 중요한 평가 기준이 될 수 있다.39</p>
</li>
<li>
<p><strong>의료 영상 분석</strong>을 통한 질병 진단에서는 단 하나의 병변이라도 놓치는 것(FN)이 치명적일 수 있으므로, 재현율이 극도로 중요하며, 동시에 정상 조직을 병변으로 오진(FP)하지 않도록 높은 정밀도 또한 요구된다.4</p>
</li>
<li>
<p><strong>스마트폰이나 드론과 같은 엣지 디바이스</strong>에 모델을 배포해야 할 경우, 제한된 메모리와 연산 능력 때문에 모델의 크기(파라미터 수)와 계산량(FLOPs)이 성능을 결정하는 핵심 제약 조건이 된다.35</p>
</li>
</ul>
<p>결론적으로, 모델의 성능을 온전히 이해하기 위해서는 정확도(mAP), 속도(FPS, Latency), 효율성(FLOPs, Parameters) 지표를 함께 고려하는 다각적인 평가가 필수적이다. 모델의 성능을 하나의 점수로 보기보다는, ’정확도-속도’와 같은 다차원 공간 상의 한 지점으로 이해하고, 주어진 문제에 가장 적합한 균형점을 찾는 노력이 필요하다.30</p>
<p>객체 인식 기술의 궁극적인 목표는 정확도와 속도라는 두 마리 토끼를 모두 잡는, 즉 높은 mAP를 유지하면서도 실시간 추론이 가능한 효율적인 아키텍처를 개발하는 것이다. RT-DETR과 같이 NMS 후처리를 제거하여 속도와 정확도를 동시에 개선하려는 최신 연구들은 이러한 방향성을 잘 보여준다.38 이처럼 끊임없이 발전하는 기술의 성능을 올바르게 측정하고 다음 단계로 나아갈 방향을 제시하는 나침반으로서, 성능 평가 지표에 대한 깊이 있는 이해는 앞으로도 계속 중요할 것이다.</p>
<h2>10. 참고 자료</h2>
<ol>
<li>Key Object Detection Metrics for Computer Vision - Roboflow Blog, https://blog.roboflow.com/object-detection-metrics/</li>
<li>Object Detection: Key Metrics for Computer Vision Performance - Label Your Data, https://labelyourdata.com/articles/object-detection-metrics</li>
<li>Object Detection Metrics: A Complete Guide with mAP, IoU - Securade.ai, https://securade.ai/blog/technology/object-detection-metrics-guide.html</li>
<li>F1-Score: Definition, Formula &amp; Applications | Ultralytics, https://www.ultralytics.com/glossary/f1-score</li>
<li>Mean Average Precision (mAP) Explained - Ultralytics, https://www.ultralytics.com/glossary/mean-average-precision-map</li>
<li>Evaluation Metrics for Object detection algorithms | by Vijay Dubey …, https://medium.com/@vijayshankerdubey550/evaluation-metrics-for-object-detection-algorithms-b0d6489879f3</li>
<li>Clarification on map metric - TAO Toolkit - NVIDIA Developer Forums, https://forums.developer.nvidia.com/t/clarification-on-map-metric/160743</li>
<li>mAP (mean Average Precision) for Object Detection | by Jonathan …, https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173</li>
<li>Trouble understanding mAP@50 metric : r/computervision - Reddit, https://www.reddit.com/r/computervision/comments/162ss9x/trouble_understanding_map50_metric/</li>
<li>Understanding Intersection over Union for Model Accuracy - Viso Suite, https://viso.ai/computer-vision/intersection-over-union-iou/</li>
<li>Mean average precision (mAP) in object detection | SuperAnnotate, https://www.superannotate.com/blog/mean-average-precision-and-its-uses-in-object-detection</li>
<li>Intersection over Union. - DigiFarm, https://digifarm.io/blog/intersection-over-union</li>
<li>www.ultralytics.com, <a href="https://www.ultralytics.com/glossary/intersection-over-union-iou#:~:text=Intersection%20over%20Union%20(IoU)%20is%20a%20fundamental%20evaluation%20metric%20used,hand-labeled%2C%20correct%20outline.">https://www.ultralytics.com/glossary/intersection-over-union-iou#:~:text=Intersection%20over%20Union%20(IoU)%20is%20a%20fundamental%20evaluation%20metric%20used,hand%2Dlabeled%2C%20correct%20outline.</a></li>
<li>Intersection Over Union IoU in Object Detection Segmentation - LearnOpenCV, https://learnopencv.com/intersection-over-union-iou-in-object-detection-and-segmentation/</li>
<li>How Compute Accuracy For Object Detection works—ArcGIS Pro, https://pro.arcgis.com/en/pro-app/latest/tool-reference/image-analyst/how-compute-accuracy-for-object-detection-works.htm</li>
<li>Mean Average Precision (mAP) Explained: Everything You Need to Know - V7 Labs, https://www.v7labs.com/blog/mean-average-precision</li>
<li>Precision and recall - Wikipedia, https://en.wikipedia.org/wiki/Precision_and_recall</li>
<li>F1 Score in Machine Learning Explained - Encord, https://encord.com/blog/f1-score-in-machine-learning/</li>
<li>Understanding Precision vs. Recall in Model Evaluation - Viso Suite, https://viso.ai/computer-vision/precision-recall/</li>
<li>Metrics Matter: A Deep Dive into Object Detection Evaluation | by Henrique Vedoveli, https://medium.com/@henriquevedoveli/metrics-matter-a-deep-dive-into-object-detection-evaluation-ef01385ec62</li>
<li>Precision Recall Curves - Train in Data’s Blog, https://www.blog.trainindata.com/precision-recall-curves/</li>
<li>Classification: Accuracy, recall, precision, and related metrics | Machine Learning, https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall</li>
<li>What is F1 Score? A Computer Vision Guide. - Roboflow Blog, https://blog.roboflow.com/f1-score/</li>
<li>Precision-Recall Curve - ML - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/precision-recall-curve-ml/</li>
<li>Evaluating Object Detection Models Using Mean Average Precision (mAP) - DigitalOcean, https://www.digitalocean.com/community/tutorials/mean-average-precision</li>
<li>Precision-Recall Curves. Sometimes a curve is worth a thousand… | by Doug Steen, https://medium.com/@douglaspsteen/precision-recall-curves-d32e5b290248</li>
<li>Precision-Recall — scikit-learn 1.7.1 documentation, https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html</li>
<li>A Comprehensive Guide to Mean Average Precision - Lightly AI, https://www.lightly.ai/blog/mean-average-precision</li>
<li>Understanding Evaluation parameters for Object Detection Models — Flops, FPS, Latency, Params, Size, Memory, Storage, mAP, AP | by Nikita Malviya | Medium, https://medium.com/@nikitamalviya/evaluation-of-object-detection-models-flops-fps-latency-params-size-memory-storage-map-8dc9c7763cfe</li>
<li>Object detection: speed and accuracy comparison (Faster R-CNN, R-FCN, SSD, FPN, RetinaNet and YOLOv3) - Jonathan Hui - Medium, https://jonathan-hui.medium.com/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359</li>
<li>Evaluation of Inference Performance of Deep Learning Models for Real-Time Weed Detection in an Embedded Computer - MDPI, https://www.mdpi.com/1424-8220/24/2/514</li>
<li>YOLOv8 vs Faster R-CNN: A Comparative Analysis - Keylabs, https://keylabs.ai/blog/yolov8-vs-faster-r-cnn-a-comparative-analysis/</li>
<li>Complexity of CNN using MACC and FLOPS - Kaggle, https://www.kaggle.com/general/240788</li>
<li>What is FLOPS in field of deep learning? - Stack Overflow, https://stackoverflow.com/questions/58498651/what-is-flops-in-field-of-deep-learning</li>
<li>Calculate Computational Efficiency of Deep Learning Models with FLOPs and MACs, https://www.kdnuggets.com/2023/06/calculate-computational-efficiency-deep-learning-models-flops-macs.html</li>
<li>Flops counter for neural networks in pytorch framework - GitHub, https://github.com/sovrasov/flops-counter.pytorch</li>
<li>Comparative Analysis of YOLO and Faster R-CNN Models for Detecting Traffic Object - The Science and Information (SAI) Organization, https://thesai.org/Downloads/Volume16No3/Paper_42-Comparative_Analysis_of_YOLO_and_Faster_R_CNN_Models.pdf</li>
<li>DETRs Beat YOLOs on Real-time Object Detection - arXiv, https://arxiv.org/html/2304.08069v3</li>
<li>YOLOv8 vs. Faster R-CNN: Tradeoffs Between Speed and Accuracy - Patsnap Eureka, https://eureka.patsnap.com/article/yolov8-vs-faster-r-cnn-tradeoffs-between-speed-and-accuracy</li>
<li>Help Needed: Real-Time Small Object Detection at 30FPS+ : r/computervision - Reddit, https://www.reddit.com/r/computervision/comments/1l7opxh/help_needed_realtime_small_object_detection_at/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>