<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:영상 분할 모델 성능 평가 안내서</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>영상 분할 모델 성능 평가 안내서</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">인공지능 평가지표 (AI evaluation metrics)</a> / <span>영상 분할 모델 성능 평가 안내서</span></nav>
                </div>
            </header>
            <article>
                <h1>영상 분할 모델 성능 평가 안내서</h1>
<h2>1. 서론</h2>
<p>이미지 분할(Image Segmentation)은 컴퓨터 비전 분야의 근본적인 과업 중 하나로, 디지털 이미지를 의미 있는 여러 개의 세그먼트, 즉 픽셀 그룹으로 분할하는 과정을 의미한다.1 이는 단순히 이미지의 표현을 단순화하는 것을 넘어, 각 픽셀에 의미론적 정체성을 부여함으로써 장면(scene)에 대한 깊이 있는 이해를 가능하게 한다. 이러한 능력은 자율 주행 자동차의 주변 환경 인식, 의료 영상에서의 종양 및 장기 경계 식별, 위성 이미지 분석을 통한 지형 변화 감지 등 수많은 고차원 응용 분야에서 핵심적인 기술로 활용된다.3</p>
<p>모델의 성능 평가는 단순히 알고리즘에 점수를 매기는 행위를 초월한다. 이는 개발된 모델의 강점과 약점을 과학적으로 진단하고, 개선 방향을 설정하며, 다양한 모델 간의 객관적이고 공정한 비교를 가능하게 하는 필수적인 과정이다.3 특히, 신뢰할 수 있고 표준화된 평가 방법론은 연구 커뮤니티 전체의 발전을 위해 재현 가능하고 비교 가능한 연구 결과를 도출하는 초석이 된다.9</p>
<p>분할 모델의 성능을 총체적으로 이해하기 위해서는 정량적 평가와 정성적 평가의 통합적 접근이 반드시 필요하다. 정량적 평가는 Intersection over Union (IoU)이나 Dice 계수와 같이 수치화된 지표를 통해 모델의 성능을 객관적으로 측정하며, 통계적 비교와 자동화된 벤치마킹을 가능하게 한다.7 반면, 정성적 평가는 분할 결과를 시각적으로 분석하여 숫자만으로는 드러나지 않는 모델의 구체적인 행동 패턴, 체계적인 오류 유형, 그리고 미묘한 실패 사례들을 파악한다.7 높은 정량적 점수가 특정 유형의 치명적인 시각적 오류를 가릴 수 있으므로, 정성적 분석은 모델의 실제 적용 가능성을 가늠하고 직관적인 이해를 돕는 데 필수적이다.</p>
<p>이 안내서는 이러한 평가 지표들을 단순 나열하는 것을 넘어, 그 발전의 맥락을 함께 제시하고자 한다. 평가 지표의 역사는 분할 모델 발전의 역사와 그 궤를 같이하기 때문이다. 초기 모델들이 기본적인 영역 구분에도 어려움을 겪던 시기에는 픽셀 정확도(Pixel Accuracy)와 같은 단순하고 직관적인 지표로도 충분했다.12 모델이 점차 정교해지면서 클래스 불균형 문제를 더 잘 처리하고 영역의 중첩을 정확하게 측정할 필요성이 대두되었고, 이로 인해 IoU와 Dice 계수가 표준으로 자리 잡았다.4 최근의 최첨단(State-of-the-art) 모델들은 영역 자체는 거의 완벽하게 예측하지만, 객체의 세밀한 경계 표현 능력에서 성능 차이를 보인다. 기존의 영역 기반 IoU는 이러한 미세한 경계 품질 차이를 민감하게 측정하지 못하는 한계를 드러냈고 13, 이를 극복하기 위해 Boundary F1 Score, Boundary IoU와 같은 경계 기반 지표들이 새롭게 제안되었다.14 이처럼 평가 지표의 발전사는 단순히 ‘더 좋은’ 지표를 찾는 과정이 아니라, 인공지능 모델의 발전 단계에 따라 ’무엇을 중요하게 측정해야 하는가’라는 평가의 초점이 이동해 온 역사적 과정이다. 본 안내서는 이러한 진화의 흐름에 따라 분할 모델 평가에 필요한 모든 지식을 체계적으로 정리하고 심층적으로 분석할 것이다.</p>
<h2>2.  Segmentation Task의 이해</h2>
<p>성능 평가 지표를 논하기에 앞서, 평가의 대상이 되는 주요 분할 과업들의 정의와 목표를 명확히 이해하는 것이 선행되어야 한다. 어떤 분할 과업을 수행하는지에 따라 적합한 평가 지표와 방법론이 달라지기 때문이다.</p>
<h3>2.1  Semantic Segmentation (의미론적 분할)</h3>
<p>Semantic Segmentation은 이미지의 모든 픽셀을 미리 정의된 클래스(예: 자동차, 사람, 도로, 하늘 등) 중 하나로 분류하는 것을 목표로 하는 과업이다.1 이 과업의 핵심은 이미지 내 각 픽셀의 ’의미(semantic)’를 이해하는 데 있다. 결과물은 원본 이미지와 동일한 크기를 가지는 ’분할 맵(Segmentation Map)’이며, 이 맵의 각 픽셀 값은 해당 픽셀이 속하는 클래스 레이블을 나타낸다.16</p>
<p>그러나 Semantic Segmentation은 근본적인 한계를 가진다. 동일한 클래스에 속하는 여러 객체가 서로 인접하거나 겹쳐 있을 경우, 이를 개별적인 인스턴스로 구분하지 않고 하나의 거대한 영역으로 취급한다.6 예를 들어, 도로에 나란히 주차된 여러 대의 자동차는 개별 ‘자동차1’, ’자동차2’로 인식되지 않고, 단일 ‘자동차’ 영역으로 분할된다.</p>
<h3>2.2  Instance Segmentation (인스턴스 분할)</h3>
<p>Instance Segmentation은 Semantic Segmentation의 한계를 극복하기 위해 등장했다. 이 과업은 이미지 내에 존재하는 모든 ’객체 인스턴스(object instance)’를 개별적으로 탐지하고, 각각의 인스턴스를 픽셀 단위로 정확하게 분할하는 것을 목표로 한다.1 즉, “이 픽셀은 자동차 클래스다“를 넘어 “이 픽셀은 자동차 인스턴스 3번에 속한다“까지 구분해낸다.</p>
<p>출력 형식은 각 탐지된 인스턴스에 대한 고유한 식별자(instance ID)와 해당 인스턴스의 정확한 모양을 나타내는 이진 마스크(binary mask)의 집합이다. 이 과업은 동일한 클래스의 객체라도 서로 다른 인스턴스임을 명확히 구분하는 데 초점을 맞추며 6, 하늘, 도로, 벽과 같이 셀 수 없는 배경 영역(background regions)은 일반적으로 처리 대상에서 제외한다.</p>
<h3>2.3  Panoptic Segmentation (파놉틱 분할)</h3>
<p>Panoptic Segmentation은 Semantic Segmentation과 Instance Segmentation을 통합한 가장 포괄적이고 진보된 형태의 분할 과업이다.17 그리스어로 ’모든 것(pan)’과 ’보이는 것(optic)’의 합성어에서 유래했듯이, 이 과업의 목표는 장면에 대한 완전하고 통일된 이해를 제공하는 것이다.18 이를 위해 이미지의 모든 픽셀에 클래스 레이블과 (해당하는 경우) 인스턴스 ID를 동시에 할당한다.1</p>
<p>이 과업의 핵심은 ’Things’와 ’Stuff’라는 개념을 도입하여 장면을 체계적으로 해석하는 데 있다.</p>
<ul>
<li>
<p><strong>Things</strong>: 자동차, 사람, 동물처럼 셀 수 있고(countable) 명확한 경계를 가진 객체 인스턴스를 지칭한다. ’Things’에 해당하는 픽셀들에는 Instance Segmentation 방식이 적용되어, 각 인스턴스가 고유한 ID를 부여받는다.18</p>
</li>
<li>
<p><strong>Stuff</strong>: 하늘, 도로, 잔디처럼 경계가 불분명하고 셀 수 없는(uncountable) 배경 영역을 의미한다. ’Stuff’에 해당하는 픽셀들에는 Semantic Segmentation 방식이 적용되어 클래스 레이블만 할당되고 인스턴스 ID는 부여되지 않는다.18</p>
</li>
</ul>
<p>이러한 ’Things’와 ’Stuff’의 구분은 단순히 두 가지 분할 작업을 기계적으로 합친 것을 넘어선다. 이는 컴퓨터 비전이 현실 세계를 이해하는 방식에 있어 근본적인 발전을 의미한다. 초기 컴퓨터 비전 과업들은 주로 ‘객체(Things)’ 탐지에 집중했으며, 배경(‘Stuff’)은 종종 무시되거나 단일 ‘배경’ 클래스로 단순화되었다. 그러나 자율 주행차나 로봇이 실제 환경과 안전하게 상호작용하기 위해서는 ’도로’나 ’벽’과 같은 ’Stuff’를 이해하는 것이 ’사람’이나 ’차’를 인식하는 것만큼이나 중요하다. Panoptic Segmentation은 ’Things’와 ’Stuff’라는 이원적 개념을 도입함으로써, 장면의 모든 구성 요소를 체계적으로 다루는 최초의 프레임워크를 제공했다.18 이는 컴퓨터가 이미지를 단순히 ’객체들의 집합’으로 인식하는 것에서 벗어나, ’객체와 배경 요소들이 상호작용하는 통일된 장면’으로 인식하게 만드는 패러다임의 전환이라 할 수 있다. 이러한 개념적 진보는 자연스럽게 Panoptic Quality (PQ)와 같은 새로운 통합 평가 지표의 개발을 촉진했다.</p>
<hr />
<p><strong>Table 1: Segmentation Task 비교</strong></p>
<table><thead><tr><th>특징 (Feature)</th><th>Semantic Segmentation</th><th>Instance Segmentation</th><th>Panoptic Segmentation</th></tr></thead><tbody>
<tr><td><strong>주요 목표 (Primary Goal)</strong></td><td>모든 픽셀에 클래스 레이블 할당</td><td>모든 객체 인스턴스 탐지 및 분할</td><td>장면 전체에 대한 통일된 분할</td></tr>
<tr><td><strong>출력 형식 (Output Format)</strong></td><td>클래스 레이블 맵</td><td>(인스턴스 ID, 마스크) 집합</td><td>(클래스 레이블, 인스턴스 ID) 맵</td></tr>
<tr><td><strong>인스턴스 구분 (Instance Awareness)</strong></td><td>불가능 (X)</td><td>가능 (O)</td><td>가능 (O)</td></tr>
<tr><td><strong>처리 대상 (Target)</strong></td><td>모든 픽셀 (Things &amp; Stuff)</td><td>객체 인스턴스 (Things only)</td><td>모든 픽셀 (Things &amp; Stuff)</td></tr>
<tr><td><strong>핵심 질문 (Key Question)</strong></td><td>“이 픽셀은 어떤 클래스인가?”</td><td>“이 이미지에 어떤 객체들이 있고, 각각의 모양은 무엇인가?”</td><td>“이 픽셀은 어떤 클래스이며, 어떤 인스턴스에 속하는가?”</td></tr>
</tbody></table>
<hr />
<h2>3.  정량적 성능 평가 지표</h2>
<p>정량적 평가는 모델의 성능을 객관적인 수치로 표현하여, 알고리즘 간의 엄밀한 비교와 지속적인 개선을 가능하게 한다. 이 섹션에서는 분할 모델 평가에 사용되는 주요 정량적 지표들을 체계적으로 분류하고, 각각의 수학적 정의, 장단점, 그리고 적절한 사용 사례를 심층적으로 분석한다.</p>
<hr />
<p><strong>Table 2: 주요 정량적 평가 지표 요약</strong></p>
<table><thead><tr><th>지표 (Metric)</th><th>수식 (Formula)</th><th>범위 (Range)</th><th>장점 (Pros)</th><th>단점 (Cons)</th></tr></thead><tbody>
<tr><td><strong>Pixel Accuracy</strong></td><td><code>(TP+TN)/(TP+TN+FP+FN)</code></td><td></td><td>직관적, 계산 용이</td><td>클래스 불균형에 매우 취약</td></tr>
<tr><td><strong>IoU / Jaccard</strong></td><td><code>TP / (TP+FP+FN)</code></td><td></td><td>가장 표준적이고 널리 사용됨</td><td>큰 객체의 경계 오류에 둔감</td></tr>
<tr><td><strong>Dice / F1-Score</strong></td><td><code>2TP / (2TP+FP+FN)</code></td><td></td><td>작은 객체에 덜 민감, F1과 동일</td><td>IoU보다 과대/과소 분할 페널티 적음</td></tr>
<tr><td><strong>Boundary F1</strong></td><td><code>2*P*R / (P+R)</code> (경계 기반)</td><td></td><td>경계 품질 직접 평가, 인간 평가와 상관성 높음</td><td>거리 임계값 파라미터에 의존적</td></tr>
<tr><td><strong>Boundary IoU</strong></td><td>경계 밴드 내 IoU</td><td></td><td>대형 객체 경계 민감도 높음, 균형 잡힘</td><td>Mask IoU보다 계산 복잡</td></tr>
<tr><td><strong>mAP</strong></td><td>PR 곡선下面積의 클래스 평균</td><td></td><td>탐지+분할 동시 평가, 신뢰도 임계값 불변</td><td>계산 복잡, 직관적 해석 어려움</td></tr>
</tbody></table>
<hr />
<h3>3.1  픽셀 기반 영역 중첩 지표</h3>
<p>이 지표들은 예측된 분할 영역과 실제 정답(Ground Truth) 영역이 픽셀 단위에서 얼마나 일치하는지를 측정하는 가장 기본적인 방식이다.</p>
<h4>3.1.1  픽셀 정확도 (Pixel Accuracy)</h4>
<p>픽셀 정확도는 가장 직관적이고 계산이 간단한 지표로, 전체 픽셀 중에서 올바르게 분류된 픽셀의 비율을 나타낸다.12 픽셀 단위 예측을 네 가지 경우로 분류하여 계산한다:</p>
<ul>
<li>
<p><strong>True Positive (TP)</strong>: 실제 Positive 픽셀을 Positive로 올바르게 예측.</p>
</li>
<li>
<p><strong>True Negative (TN)</strong>: 실제 Negative 픽셀을 Negative로 올바르게 예측.</p>
</li>
<li>
<p><strong>False Positive (FP)</strong>: 실제 Negative 픽셀을 Positive로 잘못 예측 (Type I Error).</p>
</li>
<li>
<p><strong>False Negative (FN)</strong>: 실제 Positive 픽셀을 Negative로 잘못 예측 (Type II Error).</p>
</li>
</ul>
<p>이를 바탕으로 픽셀 정확도는 다음과 같이 정의된다.</p>
<p><span class="math math-display">
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
</span><br />
이 지표의 명백한 한계는 클래스 불균형(class imbalance) 문제에 극도로 취약하다는 점이다.12 예를 들어, 어떤 이미지의 95%가 ‘배경’ 클래스이고 5%만이 ‘객체’ 클래스라고 가정하자. 만약 모델이 모든 픽셀을 단순히 ’배경’으로만 예측하더라도, 이 모델의 픽셀 정확도는 95%에 달하게 된다. 이 수치는 겉보기에는 매우 높은 성능을 보이는 것 같지만, 실제로는 ‘객체’ 클래스를 단 하나도 탐지하지 못하는 쓸모없는 모델이다.12 이처럼 픽셀 정확도는 다수 클래스에 대한 성능을 과대평가하고 소수 클래스에 대한 성능을 제대로 반영하지 못하기 때문에, 현대의 분할 모델 평가에서는 주 지표로 거의 사용되지 않는다.</p>
<h4>3.1.2  Intersection over Union (IoU) / Jaccard Index</h4>
<p>Intersection over Union (IoU), 또는 Jaccard Index는 분할 모델 평가에서 가장 널리 사용되는 표준 지표 중 하나다.4 이름에서 알 수 있듯이, 예측 영역과 실제 영역의 교집합(Intersection) 크기를 합집합(Union) 크기로 나눈 값으로 정의된다.</p>
<p>수학적으로 두 가지 형태로 표현될 수 있다. 실제 마스크 영역을 A, 예측 마스크 영역을 B라고 할 때, 영역 기반 수식은 다음과 같다.</p>
<p><span class="math math-display">
\text{IoU}(A, B) = \frac{\vert A \cap B \vert}{\vert A \cup B \vert}
</span><br />
이를 픽셀 단위의 TP, FP, FN을 사용하여 표현하면 다음과 같다.5</p>
<p><span class="math math-display">
\text{IoU} = \frac{TP}{TP + FP + FN}
</span><br />
IoU 값은 0(전혀 겹치지 않음)과 1(완벽하게 일치) 사이의 값을 가지며, 1에 가까울수록 모델의 성능이 우수함을 의미한다.20 일반적으로 데이터셋에 여러 클래스가 존재하므로, 각 클래스에 대해 개별적으로 IoU를 계산한 후 이를 산술 평균하여 **mean IoU (mIoU)**를 구한다. mIoU는 데이터셋 전체에 대한 모델의 평균적인 분할 성능을 나타내는 대표적인 지표로 널리 사용된다.</p>
<h4>3.1.3  Dice 계수 (Dice Coefficient) / F1-Score</h4>
<p>Dice 계수(Dice Coefficient), 또는 Sørensen–Dice index는 예측 영역과 실제 영역 간의 유사도를 측정하는 지표로, 특히 의료 영상 분할 분야에서 IoU만큼이나 널리 사용된다.4 이 지표는 두 영역의 교집합 크기를 두 배 한 값을 두 영역의 전체 픽셀 수의 합으로 나누어 계산한다.</p>
<p>수식은 다음과 같다.19</p>
<p><span class="math math-display">
\text{Dice}(A, B) = \frac{2 \vert A \cap B \vert}{\vert A \vert + \vert B \vert}
</span><br />
Dice 계수는 분류 문제에서 널리 사용되는 F1-Score와 수학적으로 완전히 동일하다. 이를 TP, FP, FN으로 표현하면 F1-Score의 공식과 일치함을 알 수 있다.19</p>
<p><span class="math math-display">
\text{Dice} = \text{F1-Score} = \frac{2 TP}{2 TP + FP + FN}
</span><br />
Dice 계수와 IoU는 서로 변환이 가능할 정도로 밀접한 관계를 가지지만, 미묘한 특성 차이가 있다. Dice 계수는 IoU에 비해 작은 객체의 분할 오류에 대해 상대적으로 덜 민감하게 반응하는 경향이 있다. 다시 말해, IoU가 예측 영역이 실제 영역보다 크거나 작은 경우(과대/과소 분할)에 더 큰 페널티를 부과한다.9 이러한 특성 때문에 전체 이미지에서 차지하는 비율이 매우 작은 종양이나 병변 등을 분할하는 의료 영상 분석 분야에서 클래스 불균형 문제에 더 강건한 지표로 선호되는 경우가 많다.9</p>
<h3>3.2  경계 품질 평가 지표</h3>
<p>앞서 설명한 IoU와 같은 영역 기반 지표들은 분할 성능을 평가하는 데 매우 유용하지만, 한계점 또한 명확하다. 객체의 크기가 커질수록 내부(interior) 픽셀의 수는 면적에 비례하여 2차적으로 증가하는 반면, 경계(boundary) 픽셀의 수는 둘레에 비례하여 1차적으로만 증가한다. 이로 인해, 큰 객체의 경우 경계가 다소 부정확하고 뭉툭하게 예측되더라도, 압도적으로 많은 내부 픽셀을 정확히 맞혔다는 이유만으로 매우 높은 IoU 점수를 받을 수 있다.12 이는 인간의 시각적 평가와 상당한 괴리를 발생시키는 지점이며, 객체의 정확한 윤곽이 중요한 자율 주행이나 정밀한 수술 계획을 위한 의료 영상 분석과 같은 분야에서는 이러한 한계가 치명적일 수 있다.13 이러한 문제를 해결하기 위해 경계 품질을 직접적으로 평가하는 지표들이 제안되었다.</p>
<h4>3.2.1  Boundary F1 (BF) Score</h4>
<p>Boundary F1 (BF) Score는 예측된 경계와 실제 경계가 얼마나 잘 정합하는지를 F1-Score 개념을 이용하여 측정하는 지표다.14 두 경계 위에 있는 점들 사이의 거리가 특정 허용 오차 임계값(distance error tolerance) 이내일 때 ’일치’하는 것으로 간주하고, 이를 바탕으로 정밀도(Precision)와 재현율(Recall)을 계산한다.14</p>
<ul>
<li>
<p><strong>Precision</strong>: 예측된 경계 위의 점들 중에서 실제 경계와 충분히 가까운 거리에 있는 점들의 비율.</p>
</li>
<li>
<p><strong>Recall</strong>: 실제 경계 위의 점들 중에서 예측된 경계와 충분히 가까운 거리에 있는 점들의 비율.</p>
</li>
</ul>
<p>이 두 값을 이용하여 BF Score는 다음과 같이 계산된다.14</p>
<p><span class="math math-display">
\text{BF Score} = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
</span><br />
BF Score는 영역 전체가 아닌 경계에 집중함으로써, 인간의 질적 평가와 더 높은 상관관계를 보이는 경향이 있다.27</p>
<h4>3.2.2  Boundary IoU</h4>
<p>Boundary IoU는 BF Score보다 더 최근에 제안된 경계 평가 지표로, 기존 Mask IoU의 한계를 극복하기 위해 설계되었다. 이 지표는 마스크 전체 영역이 아닌, 예측 및 실제 경계선 주변의 좁은 밴드(narrow band) 내에서만 IoU를 계산하는 방식으로 작동한다.13</p>
<p>Boundary IoU는 다음과 같은 장점을 가진다.</p>
<ol>
<li>
<p><strong>대형 객체 민감도</strong>: Mask IoU가 놓치기 쉬운 대형 객체의 경계 품질 차이를 매우 민감하게 포착한다.13</p>
</li>
<li>
<p><strong>균형 잡힌 반응성</strong>: 작은 객체의 경계 오류에 대해 과도한 페널티를 부과하지 않아, 객체 크기 전반에 걸쳐 균형 잡힌 평가가 가능하다.15</p>
</li>
<li>
<p><strong>우수성</strong>: 예측/실제 마스크 쌍에 대한 대칭성, 스케일 변화에 대한 균형 잡힌 반응성 등에서 Trimap IoU나 F-measure와 같은 다른 경계 기반 지표들보다 더 바람직한 특성을 보인다.13</p>
</li>
</ol>
<p>이러한 우수성을 바탕으로, Boundary IoU 개념은 Instance Segmentation을 위한 <strong>Boundary AP (Average Precision)</strong>, Panoptic Segmentation을 위한 <strong>Boundary PQ (Panoptic Quality)</strong> 등 새로운 표준 평가 프로토콜로 확장되어, 경계 품질 개선을 정량적으로 추적하는 중요한 도구로 자리매김하고 있다.15</p>
<h3>3.3  인스턴스 분할 평가 지표</h3>
<p>Instance Segmentation은 이미지 내의 각 객체를 ’탐지(detection)’하고 그 모양을 ’분할(segmentation)’하는 두 가지 과업이 결합된 형태다. 따라서 평가는 개별 마스크(인스턴스) 단위로 이루어지며, 탐지 성능과 분할 품질을 동시에 고려하는 복합적인 지표가 사용된다.</p>
<h4>3.3.1  기초 개념: 정밀도(Precision), 재현율(Recall), F1-Score</h4>
<p>Instance Segmentation 평가를 위해서는 먼저 TP, FP, FN의 개념을 인스턴스 단위로 재정의해야 한다. 이는 예측된 마스크와 실제 마스크를 특정 IoU 임계값(일반적으로 0.5)을 기준으로 매칭하여 결정된다.28</p>
<ul>
<li>
<p><strong>True Positive (TP)</strong>: 예측된 마스크가 실제 마스크와 IoU 임계값 이상으로 겹치고, 클래스 레이블도 일치하는 경우.28</p>
</li>
<li>
<p><strong>False Positive (FP)</strong>: 예측된 마스크에 대응하는 실제 마스크가 없거나(IoU가 임계값 미만), 클래스가 틀린 경우.29</p>
</li>
<li>
<p><strong>False Negative (FN)</strong>: 실제 마스크가 존재하지만, 이에 대응하는 예측 마스크가 없는 경우.29</p>
</li>
</ul>
<p>이 재정의된 값들을 바탕으로 정밀도, 재현율, F1-Score를 계산할 수 있다.31</p>
<p><span class="math math-display">
\text{Precision} = \frac{TP}{TP + FP}
</span></p>
<p><span class="math math-display">
\text{Recall} = \frac{TP}{TP + FN}
</span></p>
<p><span class="math math-display">
\text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2 TP}{2 TP + FP + FN}
</span></p>
<h4>3.3.2  평균 정밀도 (Average Precision, AP) 및 mAP</h4>
<p>모델이 출력하는 예측값에는 보통 신뢰도 점수(confidence score)가 포함된다. 어떤 임계값을 사용하느냐에 따라 정밀도와 재현율은 트레이드오프 관계를 보인다. 단일 임계값에서의 성능은 모델의 전체적인 능력을 대표하기 어렵다. 평균 정밀도(Average Precision, AP)는 이러한 한계를 극복하기 위해, 신뢰도 점수 임계값을 0부터 1까지 변화시키면서 얻어지는 정밀도-재현율 곡선(Precision-Recall Curve)의 아래 면적(Area Under Curve)을 계산한 값이다. 이는 모델의 전반적인 탐지 및 분할 성능을 종합적으로 나타내는 지표다.28</p>
<p>AP의 계산 과정은 다음과 같다.</p>
<ol>
<li>
<p>특정 클래스에 대한 모든 예측 인스턴스를 신뢰도 점수가 높은 순으로 정렬한다.</p>
</li>
<li>
<p>정렬된 순서대로 각 예측이 (미리 정해진 IoU 임계값에 따라) TP인지 FP인지 판별한다.</p>
</li>
<li>
<p>각 지점에서 누적 TP와 FP를 기반으로 정밀도와 재현율 값을 계산한다.</p>
</li>
<li>
<p>계산된 (재현율, 정밀도) 점들을 이용하여 PR 곡선을 그린다.</p>
</li>
<li>
<p>이 PR 곡선 아래의 면적을 수치적으로 적분하여 해당 클래스의 AP를 얻는다.28</p>
</li>
</ol>
<p>**mAP (mean Average Precision)**는 데이터셋의 모든 클래스에 대해 AP를 각각 계산한 후, 이를 산술 평균한 값으로, 데이터셋 전체에 대한 모델의 최종 성능 지표로 사용된다.28</p>
<p>이러한 AP 계산 방식의 발전은 평가의 ’엄격함’과 ’안정성’을 동시에 확보하려는 시도를 반영한다. PASCAL VOC와 같은 초기 벤치마크는 IoU 임계값을 0.5라는 단일 값으로 고정하여 사용했다.33 이 방식은 직관적이지만, 모델이 0.51의 IoU를 달성하든 0.95의 IoU를 달성하든 동일한 TP로 취급하여, 위치 정확도가 훨씬 높은 모델의 우수성을 제대로 반영하지 못하는 문제점이 있었다.28 이러한 문제를 해결하기 위해 MS COCO 벤치마크는 IoU 임계값을 0.5부터 0.95까지 0.05 간격으로 변화시키며 각 임계값에서의 AP를 모두 계산한 후 평균을 내는 방식을 도입했다 (</p>
<p><code>AP@[.5:.05:.95]</code>).28 이 다중 임계값 방식은 모델이 단순히 ‘어느 정도 겹치는’ 수준을 넘어 ‘매우 정확하게 겹치도록’ 개발을 유도한다. 즉, 평가의 엄격함을 높이는 동시에, 특정 단일 임계값에 대한 민감도를 줄여 평가 결과의 안정성과 신뢰도를 높이는 효과를 가져왔으며, 이는 더 정교하고 신뢰성 있는 모델 개발을 촉진하는 중요한 변화로 평가받는다.</p>
<h2>4.  주요 벤치마크 데이터셋 및 평가 프로토콜</h2>
<p>학계와 산업계에서 개발된 분할 모델의 성능을 공정하게 비교하고 검증하기 위해 표준화된 벤치마크 데이터셋과 평가 프로토콜이 사용된다. 이 섹션에서는 가장 대표적인 데이터셋들과 그들의 공식 평가 방식을 소개한다.</p>
<h3>4.1  PASCAL VOC (Visual Object Classes)</h3>
<p>PASCAL VOC는 20개의 객체 클래스를 포함하며, 객체 탐지, 분할, 분류 등 다양한 컴퓨터 비전 과업의 초기 표준 벤치마크로 널리 사용되었다.33 데이터셋은 훈련(train), 검증(validation), 테스트(test) 세트로 나뉘며, 테스트 세트의 정답 레이블은 공개되지 않고 평가 서버를 통해 성능을 검증받는 방식으로 운영된다.33</p>
<p><strong>평가 프로토콜:</strong></p>
<ul>
<li>
<p><strong>주요 지표</strong>: Instance Segmentation (및 Object Detection) 성능 평가의 주요 지표는 **mAP (mean Average Precision)**이다.33</p>
</li>
<li>
<p><strong>IoU 임계값</strong>: PASCAL VOC의 가장 큰 특징은 <strong>0.5</strong>라는 단일 IoU 임계값을 사용한다는 점이다.33 즉, 예측 마스크와 실제 마스크의 IoU가 0.5 이상이면 TP로 간주된다. 따라서 PASCAL VOC의 mAP는 종종</p>
</li>
</ul>
<p><code>mAP@0.5</code>로 표기된다.</p>
<ul>
<li><strong>AP 계산 방식</strong>: 초기 VOC 2007 챌린지에서는 11개의 재현율 지점에서의 정밀도를 평균 내는 ‘11-point interpolation’ 방식을 사용했으나, VOC 2010 이후부터는 모든 재현율 지점을 고려하여 PR 곡선 아래 면적을 직접 계산하는 방식으로 변경되어 더 정확한 평가가 가능해졌다.28</li>
</ul>
<h3>4.2  MS COCO (Common Objects in Context)</h3>
<p>MS COCO는 80개의 객체 카테고리, 33만 장 이상의 이미지, 150만 개 이상의 객체 인스턴스를 포함하는 대규모 데이터셋이다.36 PASCAL VOC에 비해 이미지 당 평균 객체 수가 훨씬 많고, 작은 크기의 객체가 다수 포함되어 있으며, 객체들이 서로 가려지거나 복잡한 배경 속에 위치하는 등 더 현실적이고 도전적인 시나리오를 제공한다. 이러한 특성 때문에 MS COCO는 현대 분할 및 탐지 모델의 성능을 평가하는 사실상의 표준(de-facto standard)으로 자리 잡았다.37</p>
<p>평가 프로토콜:</p>
<p>MS COCO는 단일 mAP 점수만으로 모델을 평가하는 것의 한계를 인식하고, 모델의 성능을 다각적으로 분석하기 위해 총 12개의 공식 지표를 사용한다.39 이는 모델의 강점과 약점을 훨씬 더 세밀하게 파악할 수 있게 해준다.</p>
<p><strong>Table 3: MS COCO 공식 평가 지표 상세</strong></p>
<table><thead><tr><th>지표 (Metric)</th><th>정의 (Definition)</th><th>평가 목적 (Evaluation Purpose)</th></tr></thead><tbody>
<tr><td><strong>AP</strong></td><td>IoU 임계값을 0.5에서 0.95까지 0.05 간격으로 10단계로 변경하며 측정한 AP의 평균.</td><td><strong>(주요 챌린지 지표)</strong> 전반적인 성능. 위치 정확도(Localization)가 높은 모델에 가산점을 부여.</td></tr>
<tr><td><strong>AP⁵⁰</strong></td><td>IoU 임계값이 0.5일 때의 AP. (PASCAL VOC 방식)</td><td>일반적인 탐지 성능. 느슨한 기준에서의 성능을 측정.</td></tr>
<tr><td><strong>AP⁷⁵</strong></td><td>IoU 임계값이 0.75일 때의 AP.</td><td>더 정확한 위치 정확도를 요구하는 엄격한 기준에서의 성능.</td></tr>
<tr><td><strong>APˢ</strong></td><td>작은 객체(<code>area &lt; 32²</code>)에 대한 AP.</td><td>작은 객체에 대한 모델의 탐지 및 분할 능력 평가.</td></tr>
<tr><td><strong>APᴹ</strong></td><td>중간 크기 객체(<code>32² &lt; area &lt; 96²</code>)에 대한 AP.</td><td>중간 크기 객체에 대한 성능 평가.</td></tr>
<tr><td><strong>APᴸ</strong></td><td>큰 객체(<code>area &gt; 96²</code>)에 대한 AP.</td><td>큰 객체에 대한 성능 평가.</td></tr>
<tr><td><strong>AR¹</strong></td><td>이미지 당 탐지 개수가 1개일 때의 평균 재현율(Average Recall).</td><td>가장 자신 있는 단일 예측의 재현율 평가.</td></tr>
<tr><td><strong>AR¹⁰</strong></td><td>이미지 당 탐지 개수가 10개일 때의 평균 재현율.</td><td>상위 10개 예측에 대한 재현율 평가.</td></tr>
<tr><td><strong>AR¹⁰⁰</strong></td><td>이미지 당 탐지 개수가 100개일 때의 평균 재현율.</td><td>충분한 수의 예측을 허용했을 때의 최대 재현율 평가.</td></tr>
<tr><td><strong>ARˢ</strong></td><td>작은 객체를 놓치지 않고 얼마나 잘 찾아내는가 평가.</td><td>작은 객체에 대한 재현율 평가.</td></tr>
<tr><td><strong>ARᴹ</strong></td><td>중간 크기 객체(<code>32² &lt; area &lt; 96²</code>)에 대한 AR.</td><td>중간 크기 객체에 대한 재현율 평가.</td></tr>
<tr><td><strong>ARᴸ</strong></td><td>큰 객체(<code>area &gt; 96²</code>)에 대한 AR.</td><td>큰 객체에 대한 재현율 평가.</td></tr>
</tbody></table>
<h3>4.3  Cityscapes</h3>
<p>Cityscapes는 자율 주행 연구에 특화된 벤치마크 데이터셋이다. 50개의 다른 도시에서 수집된 고해상도(2048x1024) 도시 풍경 비디오 시퀀스로 구성되어 있으며, 30개 클래스(이 중 19개가 평가에 사용됨)에 대한 매우 정교한 픽셀 단위 주석을 제공한다.41</p>
<p>평가 프로토콜:</p>
<p>Cityscapes는 수행하는 과업에 따라 특화된 평가 지표를 사용한다.</p>
<ul>
<li>
<p><strong>Pixel-level Semantic Labeling</strong>: 클래스별 IoU (<code>IoUclass</code>)를 기본으로 하되, 큰 객체에 의해 점수가 왜곡되는 것을 방지하기 위해 인스턴스 크기를 고려하여 가중치를 부여한 <code>iIoU</code> (instance-level IoU)를 함께 평가한다.42</p>
</li>
<li>
<p><strong>Instance-level Semantic Labeling</strong>: MS COCO와 유사하게, 0.5에서 0.95까지 다양한 IoU 임계값에 대한 AP를 계산하여 평가한다.42</p>
</li>
<li>
<p><strong>Panoptic Labeling</strong>: Semantic Quality (SQ)와 Recognition Quality (RQ)를 결합한 <strong>Panoptic Quality (PQ)</strong> 지표를 사용하여 평가한다.</p>
</li>
</ul>
<h2>5.  정성적 평가 및 오류 분석</h2>
<p>정량적 지표가 ’모델이 전반적으로 얼마나 잘하는가’에 대한 거시적인 답을 제공한다면, 정성적 분석은 ’모델이 구체적으로 무엇을, 왜, 그리고 어떻게 틀리는가’에 대한 미시적이고 깊이 있는 통찰을 제공한다. 이는 모델의 한계를 명확히 이해하고 실질적인 개선 방향을 도출하기 위한 가장 중요한 과정이다.</p>
<h3>5.1  시각적 분석의 중요성</h3>
<p>숫자는 종종 전체 이야기를 말해주지 않는다. 예를 들어, 어떤 모델이 높은 mIoU 점수를 기록했더라도, 특정 클래스의 경계가 항상 흐릿하게 예측되거나, 이미지 내 작은 객체들을 지속적으로 놓치는 등의 체계적인 문제가 있을 수 있다.13 예측된 분할 마스크를 원본 이미지 및 실제 마스크와 시각적으로 겹쳐서 비교하는 것은, 이러한 수치 뒤에 숨겨진 모델의 실제 행동 패턴을 직관적으로 이해하는 가장 효과적이고 필수적인 방법이다.10</p>
<h3>5.2  주요 오류 유형 식별 및 진단 방법론</h3>
<p>단순히 실패 사례를 나열하는 것을 넘어, 체계적인 프레임워크에 따라 오류를 분류하고 그 원인을 추론하는 과정이 필요하다. 이는 모델의 실패 원인을 진단하고, 이를 해결하기 위한 구체적인 전략을 수립하는 데 도움을 준다.11</p>
<h4>5.2.1  경계 부정확성 (Boundary Inaccuracy)</h4>
<ul>
<li>
<p><strong>시각적 식별</strong>: 예측된 경계가 실제 객체의 윤곽보다 뭉툭하거나(blobby), 과도하게 매끄럽거나, 지글거리는 노이즈를 포함하는 경우를 확인한다. 특히 사람의 팔이나 동물의 다리와 같이 가늘고 긴 구조, 혹은 의자 다리 사이나 자전거 바퀴살과 같은 복잡한 구조의 경계를 정확하게 포착하지 못하는 경우가 대표적이다.13</p>
</li>
<li>
<p><strong>분석 및 진단</strong>: 이러한 현상은 모델의 유효 수용 영역(effective receptive field)이 너무 작아 국소적인 특징에만 집중했거나, 인코더-디코더 구조에서 업샘플링(upsampling) 과정 중 고주파수의 세밀한 공간 정보가 손실되었을 가능성을 시사한다.</p>
</li>
</ul>
<h4>5.2.2  잘못된 지역화 및 분류 오류 (Localization and Classification Errors)</h4>
<ul>
<li>
<p><strong>시각적 식별</strong>: 11</p>
</li>
<li>
<p><strong>잘못된 지역화 (Localization Error)</strong>: 객체의 클래스는 올바르게 예측했으나, 분할 마스크의 위치가 실제 위치에서 약간 벗어나 있거나, 크기가 실제보다 크거나 작게 예측된 경우.</p>
</li>
<li>
<p><strong>유사 객체와의 혼동 (Confusion with Similar Objects)</strong>: 시각적으로 유사한 다른 클래스로 잘못 분류하는 경우. 예를 들어, ’고양이’를 ’개’로, ’트럭’을 ’버스’로, 또는 ’자전거’를 ’오토바이’로 분할하는 사례.</p>
</li>
<li>
<p><strong>배경 오류 (Background Error)</strong>: 객체가 아닌 배경의 일부(예: 특정 패턴의 벽지, 그림자, 나뭇잎 무더기)를 특정 클래스의 객체로 잘못 분할하는 경우. 이는 대표적인 오탐(False Positive) 사례다.</p>
</li>
<li>
<p><strong>분석 및 진단</strong>: 클래스 간 혼동 오류는 모델이 두 클래스를 구분하는 결정적인 특징(discriminative features)을 충분히 학습하지 못했음을 의미한다. 배경 오류는 모델이 객체성(objectness)에 대한 전후 문맥(context)을 제대로 이해하지 못하고, 단순히 지역적인 텍스처나 색상에 과적합되었음을 나타낸다.</p>
</li>
</ul>
<h4>5.2.3  객체 속성에 따른 성능 저하</h4>
<ul>
<li>
<p><strong>시각적 식별</strong>:</p>
</li>
<li>
<p><strong>크기</strong>: 이미지 내에서 작은 객체들을 일관되게 탐지하지 못하고 누락하는 경우(미탐, False Negative).11</p>
</li>
<li>
<p><strong>가려짐 (Occlusion)</strong>: 다른 객체에 의해 일부가 가려진 객체의 보이지 않는 부분까지 전체 형태를 추론하지 못하고, 눈에 보이는 부분만 분할하는 경우.</p>
</li>
<li>
<p><strong>복잡성</strong>: 앞서 언급한 의자 다리, 자전거 바퀴살처럼 위상적으로 복잡하고 비연속적인 구조를 가진 객체의 세부 묘사에 실패하는 경우.</p>
</li>
<li>
<p><strong>분석 및 진단</strong>: 이러한 문제들은 모델이 다양한 스케일의 특징을 효과적으로 융합하지 못하거나, 부분적인 시각 정보로부터 전체적인 형태를 추론하는 능력이 부족함을 시사한다.</p>
</li>
</ul>
<p>이러한 정성적 오류 분석 과정은 단순히 모델을 ’평가’하는 수동적인 활동에 그치지 않는다. 이는 ‘데이터 중심 AI(Data-Centric AI)’ 접근법의 핵심적인 출발점이 된다. 모델의 성능이 정체될 때, 그 해결책은 더 복잡한 모델 아키텍처를 설계하는 것이 아닐 수 있다. 문제의 근본 원인은 데이터 자체의 품질이나 분포에 있을 가능성이 높다. 정성적 오류 분석은 모델이 어떤 종류의 데이터에 체계적으로 취약한지를 명확하게 보여준다. 예를 들어, ’흐린 날씨’나 ‘야간’ 이미지에서 유독 성능이 저하되는 패턴을 발견했다면, 이는 해당 조건의 학습 데이터가 부족하거나 레이블 품질이 낮다는 강력한 신호다. 이러한 분석을 통해 얻은 통찰은 어떤 데이터를 추가로 수집해야 하는지, 어떤 데이터의 레이블을 우선적으로 검토하고 개선해야 하는지 43, 그리고 어떤 종류의 데이터 증강(data augmentation) 기법이 효과적일지에 대한 구체적인 가이드라인을 제공한다. 결국, 정성적 평가는 데이터셋 자체를 개선하여 모델 성능의 근본적인 한계를 돌파하는 능동적인 과정의 시작인 셈이다.</p>
<h2>6. 결론</h2>
<p>인공지능 분할 모델의 성능을 정확하고 깊이 있게 평가하는 것은 신뢰할 수 있는 기술을 개발하고 발전시키는 데 있어 필수불가결한 요소다. 본 안내서는 분할 과업의 종류부터 시작하여 다양한 정량적 지표, 표준 벤치마크 프로토콜, 그리고 정성적 오류 분석 방법론에 이르기까지, 성능 평가에 대한 포괄적인 지식을 체계적으로 다루었다.</p>
<p><strong>핵심 평가 지표 선택 가이드라인</strong>은 다음과 같이 요약할 수 있다.</p>
<ul>
<li>
<p><strong>일반적인 Semantic Segmentation</strong>: <strong>mIoU</strong>를 주 지표로 사용하되, 의료 영상과 같이 특정 클래스의 크기가 매우 작은 불균형 데이터셋에서는 <strong>Dice 계수</strong>를 함께 고려하는 것이 바람직하다.</p>
</li>
<li>
<p><strong>경계 품질이 중요한 응용 분야</strong>: 자율 주행, 로보틱스, 의료 진단 등 객체의 정확한 윤곽이 중요한 경우, <strong>BF Score</strong> 또는 최신의 <strong>Boundary IoU</strong>를 반드시 함께 보고해야 한다. 이는 모델의 세밀한 표현 능력을 보여주는 핵심적인 보조 지표가 된다.</p>
</li>
<li>
<p><strong>Instance 및 Panoptic Segmentation</strong>: <strong>MS COCO의 12가지 공식 지표</strong>를 따르는 것이 현재의 표준이다. 특히 주요 지표인 <strong>AP</strong> (<code>AP@[.5:.05:.95]</code>)와 함께, 객체 크기별 성능(<code>APˢ/ᴹ/ᴸ</code>)과 재현율(<code>AR¹/¹⁰/¹⁰⁰</code>)을 심층적으로 분석하여 모델의 강점과 약점을 다각도로 파악해야 한다.</p>
</li>
</ul>
<p>궁극적으로 최고의 평가는 단일 숫자에 의존하지 않는다. 다수의 정량적 지표를 통해 성능을 다각도로 측정하고, 체계적인 정성적 분석을 통해 그 숫자들이 실제로 의미하는 바를 깊이 있게 해석하는 <strong>총체적 평가 접근법</strong>이 필수적이다.</p>
<p>미래의 분할 모델 평가는 더욱 정교하고 응용 지향적인 방향으로 발전할 것이다. wIoU(weighted IoU) 45나 Boundary DoU Loss 46와 같이 특정 측면을 더 잘 반영하려는 새로운 지표 연구가 계속되고 있으며, 라벨링 오류를 자동으로 탐지하거나 43 모델의 실패 원인을 자동으로 분석하려는 연구 또한 활발히 진행 중이다. 궁극적으로, 평가 지표는 의료 진단의 정확성 향상이나 자율 주행의 안전성 확보와 같이, 해당 기술이 적용되는 실제 응용 분야의 최종 목표와 직접적으로 연결되는 방향으로 진화할 것이다.48 연구자와 개발자는 이러한 흐름을 이해하고, 본 안내서에서 제시된 원칙들을 바탕으로 자신의 모델을 엄밀하고 종합적으로 평가함으로써 컴퓨터 비전 기술의 발전에 기여해야 할 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Guide to Image Segmentation in Computer Vision: Best Practices - Encord, https://encord.com/blog/image-segmentation-for-computer-vision-best-practice-guide/</li>
<li>Systematic review of image segmentation using complex networks - arXiv, https://arxiv.org/html/2401.02758v1</li>
<li>Selecting the Right Metric: A Detailed Study on Image Segmentation Evaluation | Request PDF - ResearchGate, https://www.researchgate.net/publication/386403542_Selecting_the_Right_Metric_A_Detailed_Study_on_Image_Segmentation_Evaluation</li>
<li>What are different evaluation metrics used to evaluate image segmentation models?, https://www.geeksforgeeks.org/computer-vision/what-are-different-evaluation-metrics-used-to-evaluate-image-segmentation-models/</li>
<li>Calculation Intersection Over Union (IoU) For Evaluating an Image …, https://www.geeksforgeeks.org/java/calculation-intersection-over-union-iou-for-evaluating-an-image-segmentation-model-using-java/</li>
<li>What Is Instance Segmentation? - IBM, https://www.ibm.com/think/topics/instance-segmentation</li>
<li>Significant full reference image segmentation evaluation: a survey in remote sensing field, https://search.proquest.com/openview/ee53c5880c33b8c8ce011c3ad01cba84/1?pq-origsite=gscholar&amp;cbl=54626</li>
<li>Image Filtering and Image Segmentation: An Insight of … - INSA Lyon, https://www.creatis.insa-lyon.fr/~grenier/wp-content/uploads/M6_CoursBoubakeur.pdf</li>
<li>towards a guideline for evaluation metrics in medical image segmentation - arXiv, https://arxiv.org/pdf/2202.05273</li>
<li>Semantic Segmentation Experiment Report | PASCAL-VOC2012-Sementic-Seg - Wandb, https://wandb.ai/khurramkhalil/PASCAL-VOC2012-Sementic-Seg/reports/Semantic-Segmentation-Experiment-Report–Vmlldzo2MTA3Njg2</li>
<li>Diagnosing Error in Object Detectors - Derek Hoiem, https://dhoiem.web.engr.illinois.edu/publications/eccv2012_detanalysis_derek.pdf</li>
<li>Semantic Segmentation | Supervisely, https://docs.supervisely.com/neural-networks/model-evaluation-benchmark/semantic-segmentation</li>
<li>Boundary IoU: Improving Object-Centric Image … - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2021/papers/Cheng_Boundary_IoU_Improving_Object-Centric_Image_Segmentation_Evaluation_CVPR_2021_paper.pdf</li>
<li>bfscore - Contour matching score for image segmentation - MATLAB, https://www.mathworks.com/help/images/ref/bfscore.html</li>
<li>Boundary IoU: Improving Object-Centric Image Segmentation Evaluation - ResearchGate, https://www.researchgate.net/publication/350512324_Boundary_IoU_Improving_Object-Centric_Image_Segmentation_Evaluation</li>
<li>What Is Semantic Segmentation? - IBM, https://www.ibm.com/think/topics/semantic-segmentation</li>
<li>en.wikipedia.org, <a href="https://en.wikipedia.org/wiki/Image_segmentation#:~:text=Panoptic%20segmentation%20combines%20both%20semantic,instances%20of%20the%20same%20class.">https://en.wikipedia.org/wiki/Image_segmentation#:~:text=Panoptic%20segmentation%20combines%20both%20semantic,instances%20of%20the%20same%20class.</a></li>
<li>Understanding Panoptic Segmentation Basics - Viso Suite, https://viso.ai/deep-learning/panoptic-segmentation/</li>
<li>All the segmentation metrics! - Kaggle, https://www.kaggle.com/code/yassinealouini/all-the-segmentation-metrics</li>
<li>Intersection over Union (IoU): Definition, Calculation, Code - V7 Labs, https://www.v7labs.com/blog/intersection-over-union-guide</li>
<li>Understanding Intersection over Union for Model Accuracy - Viso Suite, https://viso.ai/computer-vision/intersection-over-union-iou/</li>
<li>Intersection Over Union (IoU): From Theory to Practice - Lightly AI, https://www.lightly.ai/blog/intersection-over-union</li>
<li>Dice Coefficient! What is it?. Dice coefficient is a similarity metric …, https://lathashreeh.medium.com/dice-coefficient-what-is-it-ff090ec97bda</li>
<li>Calibrating the Dice Loss to Handle Neural Network Overconfidence for Biomedical Image Segmentation - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC10039156/</li>
<li>Understanding DICE COEFFICIENT - Kaggle, https://www.kaggle.com/code/yerramvarun/understanding-dice-coefficient</li>
<li>Understanding Evaluation Metrics in Medical Image Segmentation | by Nghi Huynh | Mastering Data Science | Medium, https://medium.com/mastering-data-science/understanding-evaluation-metrics-in-medical-image-segmentation-d289a373a3f</li>
<li>evaluateSemanticSegmentation - Evaluate semantic segmentation data set against ground truth - MATLAB - MathWorks, https://www.mathworks.com/help/vision/ref/evaluatesemanticsegmentation.html</li>
<li>Evaluation metrics for object detection and segmentation: mAP, https://kharshit.github.io/blog/2019/09/20/evaluation-metrics-for-object-detection-and-segmentation</li>
<li>Instance Segmentation | Supervisely, https://docs.supervisely.com/neural-networks/model-evaluation-benchmark/instance-segmentation</li>
<li>Evaluating image segmentation models. - Jeremy Jordan, https://www.jeremyjordan.me/evaluating-image-segmentation-models/</li>
<li>Classification: Accuracy, recall, precision, and related metrics …, https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall</li>
<li>F-score - Wikipedia, https://en.wikipedia.org/wiki/F-score</li>
<li>VOC Dataset - Ultralytics YOLO Docs, https://docs.ultralytics.com/datasets/detect/voc/</li>
<li>COCO Evaluation metrics explained - Picsellia, https://www.picsellia.com/post/coco-evaluation-metrics-explained</li>
<li>merve/pascal-voc · Datasets at Hugging Face, https://huggingface.co/datasets/merve/pascal-voc</li>
<li>COCO Dataset: All You Need to Know to Get Started - V7 Labs, https://www.v7labs.com/blog/coco-dataset-guide</li>
<li>Benchmarking Object Detectors with COCO: A New Path Forward - arXiv, https://arxiv.org/html/2403.18819v1</li>
<li>Confidence Score: The Forgotten Dimension of Object Detection Performance Evaluation - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC8271464/</li>
<li>README.md · rafaelpadilla/coco_metrics at a08c52915eea92ac57a4b42b92a900b44611089e - Hugging Face, https://huggingface.co/spaces/rafaelpadilla/coco_metrics/blob/a08c52915eea92ac57a4b42b92a900b44611089e/README.md</li>
<li>Most popular metrics used to evaluate object detection algorithms. - GitHub, https://github.com/rafaelpadilla/Object-Detection-Metrics</li>
<li>1 Dataset - arXiv, https://arxiv.org/html/2503.21854v1</li>
<li>Cityscapes Dataset – Semantic Understanding of Urban Street Scenes, https://www.cityscapes-dataset.com/</li>
<li>Estimating label quality and errors in semantic segmentation data via any model - arXiv, https://arxiv.org/abs/2307.05080</li>
<li>Estimating label quality and errors in semantic segmentation data via any model - arXiv, https://arxiv.org/pdf/2307.05080</li>
<li>arXiv:2107.09858v5 [cs.CV] 2 Jul 2024, https://arxiv.org/pdf/2107.09858</li>
<li>[2308.00220] Boundary Difference Over Union Loss For Medical Image Segmentation - arXiv, https://arxiv.org/abs/2308.00220</li>
<li>Automated Detection of Labeling Errors in Semantic Segmentation Datasets via Deep Learning and Unce - YouTube, https://www.youtube.com/watch?v=xIpvcEkVHi4</li>
<li>What Matters in Radiological Image Segmentation? Effect of Segmentation Errors on the Diagnostic Related Features, https://pmc.ncbi.nlm.nih.gov/articles/PMC10501981/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>