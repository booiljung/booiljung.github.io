<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:인공지능 평가 지표의 패러다임 전환</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>인공지능 평가 지표의 패러다임 전환</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">인공지능 평가지표 (AI evaluation metrics)</a> / <span>인공지능 평가 지표의 패러다임 전환</span></nav>
                </div>
            </header>
            <article>
                <h1>인공지능 평가 지표의 패러다임 전환</h1>
<h3>0.1 AI 평가, 정확도를 넘어선 새로운 지평</h3>
<p>인공지능(AI) 기술은 규칙 기반 시스템에서 출발하여 머신러닝, 딥러닝을 거쳐 이제는 초거대 언어 모델(Large Language Model, LLM)의 시대로 접어들었다.1 이러한 기술적 진화의 과정은 AI의 성능을 측정하고 가늠하는 ‘평가의 자’ 또한 근본적인 변화를 겪어야 함을 시사한다. 초기의 AI가 명확하게 정의된 특정 과업(task-specific)의 수행 능력에 초점을 맞추었다면, 현대의 생성형 AI는 인간의 언어를 이해하고 생성하며, 복잡한 추론을 수행하고, 심지어 창의적인 콘텐츠까지 만들어내는 범용적 능력을 지향하고 있다.3</p>
<p>이러한 AI 능력의 질적 확장은 기존의 정량적이고 단편적인 평가 지표로는 더 이상 AI의 진정한 가치와 잠재적 위험을 온전히 측정할 수 없다는 근본적인 문제를 제기한다. 정확도라는 단일 척도로는 모델의 편향성, 예상치 못한 입력에 대한 취약성, 혹은 그 결정 과정의 불투명성과 같은 중대한 이슈들을 포착할 수 없다. 따라서 본 안내서는 이러한 문제의식에서 출발하여, 전통적 평가 지표의 유효성과 한계를 명확히 하고, 정확도를 넘어 강건성(Robustness), 공정성(Fairness), 설명가능성(Explainability) 등 다차원적 가치를 포괄하는 새로운 평가 패러다임으로의 전환을 체계적으로 논증하고자 한다.5</p>
<p>본 안내서의 구성은 다음과 같다. 먼저 전통적 평가 지표들을 고찰하여 그 기반과 한계를 분석하고(II장), 생성형 AI의 등장이 가져온 평가의 딜레마를 심층적으로 탐구한다(III장). 이를 바탕으로 새로운 평가 패러다임의 핵심적인 축들을 상세히 정의하며(IV장), 이러한 새로운 패러다임을 구현하는 차세대 벤치마크와 혁신적인 평가 방법론들을 살펴본다(V장). 나아가 현실 세계의 적용 사례와 규제 동향을 통해 새로운 평가 패러다임의 실질적 함의를 도출하고(VI장), 마지막으로 미래 AI 평가가 나아가야 할 방향성을 제시하며 결론을 맺는다(VII장).</p>
<h2>1.  전통적 평가 지표의 명과 암</h2>
<p>전통적인 AI, 특히 머신러닝 모델의 성능은 주로 예측의 정확성에 초점을 맞춘 정량적 지표들을 통해 측정되어 왔다. 이러한 지표들은 모델의 유형과 과업의 성격에 따라 분류, 회귀, 자연어 생성 등 다양한 영역에서 발전해왔으며, AI 평가의 기틀을 마련했다.</p>
<h3>1.1  분류(Classification) 모델 평가: 정확성의 이면</h3>
<p>분류 모델의 평가는 예측 결과와 실제 값의 관계를 나타내는 혼동 행렬(Confusion Matrix)로부터 파생된 지표들에 크게 의존한다. 혼동 행렬은 True Positive (TP), True Negative (TN), False Positive (FP), False Negative (FN)의 네 가지 요소로 구성된다.8</p>
<ul>
<li>
<p><strong>정확도(Accuracy)</strong>: 전체 예측 중 올바르게 예측된 경우의 비율을 나타내는 가장 직관적인 지표다.9 하지만 데이터의 클래스 분포가 불균형할 경우, 모델의 성능을 심각하게 왜곡할 수 있다. 예를 들어, 99%가 정상이고 1%가 사기인 금융 거래 데이터에서 모델이 모든 거래를 ’정상’으로 예측해도 99%의 정확도를 기록하게 되어, 실제 중요한 소수 클래스에 대한 예측 능력은 전혀 평가하지 못하는 맹점을 가진다.8</p>
<p><span class="math math-display">
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
</span></p>
</li>
<li>
<p><strong>정밀도(Precision)</strong>: 모델이 ’Positive’로 예측한 것들 중 실제로 ’Positive’인 경우의 비율이다.12 이 지표는 거짓 양성(FP)으로 인한 비용이 클 때 특히 중요하다. 스팸 메일 필터가 정상 메일을 스팸으로 분류(FP)하면 중요한 정보를 놓치게 되므로 높은 정밀도가 요구된다.11</p>
<p><span class="math math-display">
Precision = \frac{TP}{TP + FP}
</span></p>
</li>
<li>
<p><strong>재현율(Recall)</strong>: 실제 ’Positive’인 것들 중 모델이 ’Positive’로 올바르게 예측한 경우의 비율이다.12 이 지표는 거짓 음성(FN)으로 인한 비용이 치명적일 때 중요하다. 암 진단 모델이 실제 암 환자를 정상으로 오진(FN)하는 경우의 심각성을 고려하면, 재현율은 의료 분야에서 매우 중요한 척도가 된다.8</p>
<p><span class="math math-display">
Recall = \frac{TP}{TP + FN}
</span></p>
</li>
<li>
<p><strong>F1 점수(F1 Score)</strong>: 정밀도와 재현율은 상충 관계(trade-off)에 있는 경우가 많다.9 F1 점수는 이 두 지표의 조화 평균으로, 둘 사이의 균형을 측정하는 데 사용된다. 어느 한쪽이 극단적으로 낮으면 F1 점수 역시 낮아지므로, 두 지표 모두를 적절히 고려해야 하는 상황에 유용하다.8</p>
<p><span class="math math-display">
F1 Score = 2 \times \frac{Precision \times Recall}{Precision + Recall} = \frac{2TP}{2TP + FP + FN}
</span></p>
</li>
<li>
<p><strong>AUC-ROC</strong>: ROC(Receiver Operating Characteristic) 곡선은 분류 모델의 임계값(threshold)을 변경해가며 TPR(재현율)과 FPR(False Positive Rate)의 변화를 그린 그래프다. AUC(Area Under the Curve)는 이 곡선 아래의 면적으로, 1에 가까울수록 모델이 양성 클래스와 음성 클래스를 완벽하게 구별함을 의미한다. AUC는 임계값에 구애받지 않고 모델의 전반적인 판별 능력을 평가할 수 있는 장점이 있다.13</p>
</li>
</ul>
<h3>1.2  회귀(Regression) 모델 평가: 오차 측정의 관점</h3>
<p>회귀 모델은 연속적인 값을 예측하며, 평가는 주로 예측값과 실제값 사이의 오차를 측정하는 방식으로 이루어진다.</p>
<ul>
<li>
<p><strong>평균 제곱 오차(Mean Squared Error, MSE)</strong>: 오차(잔차)를 제곱한 값의 평균이다. 오차를 제곱하기 때문에, 예측이 실제 값에서 크게 벗어난 이상치(outlier)에 대해 더 큰 페널티를 부여하는 특징이 있다.15</p>
<p><span class="math math-display">
MSE = \frac{1}{n}\sum_{i=1}^{n}(Y_i - \hat{Y_i})^2
</span></p>
</li>
</ul>
<p>여기서 <span class="math math-inline">n</span>은 데이터 포인트의 수, <span class="math math-inline">Y_i</span>는 실제 값, <span class="math math-inline">\hat{Y_i}</span>는 예측 값을 의미한다.17</p>
<ul>
<li>
<p><strong>평균 제곱근 오차(Root Mean Squared Error, RMSE)</strong>: MSE에 제곱근을 취한 값으로, 오차의 단위를 원래 데이터의 단위와 동일하게 만들어 해석을 더 직관적으로 만든다.13</p>
<p><span class="math math-display">
RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(Y_i - \hat{Y_i})^2}
</span></p>
</li>
</ul>
<h3>1.3  자연어 생성(NLG) 모델 평가: 문자열 유사도를 넘어서</h3>
<p>자연어 생성 모델, 특히 기계 번역이나 요약 모델의 평가는 생성된 텍스트와 정답 텍스트 간의 유사도를 측정하는 방식으로 발전해왔다.</p>
<ul>
<li>
<p><strong>BLEU (Bilingual Evaluation Understudy)</strong>: 기계 번역의 품질을 평가하기 위해 개발된 지표로, 생성된 문장과 하나 이상의 정답 문장 간의 n-gram(연속된 n개의 단어) 정밀도(precision)를 측정한다.18 생성된 문장이 정답 문장보다 지나치게 짧을 경우, 짧은 문장에 대한 벌점(Brevity Penalty, BP)을 부여하여 점수를 보정한다.18</p>
<p><span class="math math-display">
BLEU = BP \times \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)
</span></p>
</li>
<li>
<p><strong>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</strong>: 주로 텍스트 요약 모델 평가에 사용되며, BLEU와 반대로 n-gram의 재현율(recall)에 초점을 맞춘다. 즉, 정답 요약문에 포함된 중요한 정보가 생성된 요약문에 얼마나 포함되었는지를 측정한다.13</p>
</li>
</ul>
<p>이러한 전통적 지표들은 모두 ’정답(Ground Truth)’이 명확하게 존재하고, 모델의 목표는 그 정답을 얼마나 정확하게 맞추는가에 있다는 암묵적 가정을 공유한다.12 분류에서는 정해진 레이블, 회귀에서는 실제 값, 번역에서는 참조 번역문이 바로 그 ’정답’이다. 이는 AI를 ’정답을 찾는 기계’로 보는 관점을 반영한다. 하지만 창의적 글쓰기나 복잡한 문제 해결과 같이 정답이 여러 개이거나 명확히 존재하지 않는 생성형 AI의 과업에는 이 가정이 근본적으로 들어맞지 않으며, 이는 평가 패러다임의 전환이 불가피했던 핵심적인 이유가 된다.</p>
<p>더 나아가 어떤 평가 지표를 선택하느냐는 특정 가치를 우선시하는 행위와 같다. 예를 들어, 의료 진단에서 재현율을 중시하는 것은 ’한 명의 환자라도 놓쳐서는 안 된다’는 가치를, 정밀도를 중시하는 것은 ’불필요한 추가 검사로 인한 환자의 스트레스와 비용을 줄여야 한다’는 가치를 반영하는 것이다.11 이처럼 평가 지표는 단순히 숫자가 아니라, 해당 AI 시스템이 추구해야 할 목표와 사회적 가치를 반영하는 대리인(proxy) 역할을 수행한다. 따라서 새로운 평가 패러다임의 등장은 성능뿐만 아니라 공정성, 안전성 같은 더 복잡하고 중요한 사회적 가치를 어떻게 측정할 것인가의 문제로 귀결된다.</p>
<p><strong>표 1: 전통적 AI 평가 지표 요약</strong></p>
<table><thead><tr><th>지표 구분</th><th>지표명</th><th>수식 / 개념</th><th>주요 적용 분야</th><th>내재된 한계</th></tr></thead><tbody>
<tr><td><strong>분류</strong></td><td>정확도 (Accuracy)</td><td><span class="math math-inline">\frac{TP+TN}{TP+TN+FP+FN}</span></td><td>일반적인 분류 성능</td><td>클래스 불균형에 취약함 8</td></tr>
<tr><td></td><td>정밀도 (Precision)</td><td><span class="math math-inline">\frac{TP}{TP+FP}</span></td><td>스팸 필터, 오탐 비용이 큰 경우</td><td>실제 Positive를 놓치는 경우(FN)를 평가하지 못함 8</td></tr>
<tr><td></td><td>재현율 (Recall)</td><td><span class="math math-inline">\frac{TP}{TP+FN}</span></td><td>의료 진단, 미탐 비용이 큰 경우</td><td>정상 케이스를 오탐(FP)하는 경향을 평가하지 못함 8</td></tr>
<tr><td></td><td>F1 점수 (F1 Score)</td><td><span class="math math-inline">2 \times \frac{Precision \times Recall}{Precision + Recall}</span></td><td>정밀도와 재현율의 균형이 중요할 때</td><td>각 오류(FP, FN)의 중요도가 다를 경우 부적합 13</td></tr>
<tr><td><strong>회귀</strong></td><td>평균 제곱 오차 (MSE)</td><td><span class="math math-inline">\frac{1}{n}\sum(Y_i - \hat{Y_i})^2</span></td><td>일반적인 회귀 성능, 큰 오차에 민감</td><td>이상치에 과도하게 영향을 받으며, 단위 해석이 어려움 16</td></tr>
<tr><td></td><td>평균 제곱근 오차 (RMSE)</td><td><span class="math math-inline">\sqrt{MSE}</span></td><td>오차를 실제 값과 동일한 단위로 해석</td><td>MSE와 마찬가지로 이상치에 민감함 15</td></tr>
<tr><td><strong>자연어 생성</strong></td><td>BLEU</td><td>n-gram 정밀도 기반 유사도 측정</td><td>기계 번역</td><td>의미적 유사성, 유창성, 문법적 정확성 평가 불가 18</td></tr>
<tr><td></td><td>ROUGE</td><td>n-gram 재현율 기반 유사도 측정</td><td>텍스트 요약</td><td>BLEU와 유사하게 표면적 일치에 의존함 13</td></tr>
</tbody></table>
<h2>2.  패러다임 전환의 서막: 생성형 AI 시대의 평가 딜레마</h2>
<p>전통적 AI가 주어진 데이터 내에서 패턴을 학습하고 예측하는 데 집중했다면, LLM으로 대표되는 생성형 AI는 기존에 없던 새로운 콘텐츠를 창조하고, 복잡한 지시를 이해하며, 여러 단계에 걸친 추론을 수행하는 등 질적으로 다른 차원의 능력을 선보였다.3 이러한 능력의 재정의는 기존 평가 방식의 근본적인 부적절성(inadequacies)을 드러내는 계기가 되었다.</p>
<p>기존 벤치마크들은 대부분 표준화된 ’정답’을 가정하고 모델이 이를 얼마나 잘 재현하는지를 측정하는 시험 형식(exam-style)으로 구성되어 있다.5 이러한 접근법은 LLM이 가진 뉘앙스, 문화적 민감성, 맥락 이해 능력, 그리고 무엇보다 정답이 없는 문제에 대한 창의적 해결 능력과 같은 주관적이고 다면적인 품질을 평가하는 데 명백한 한계를 보인다.3 통제된 실험실 환경에서 높은 벤치마크 점수를 획득한 모델이, 예측 불가능하고 복잡한 상호작용이 발생하는 실제 세계에서는 형편없는 성능을 보이는 괴리가 빈번하게 발생한다.5 예를 들어, 기존 벤치마크는 단발적인 질의응답을 평가할 뿐, 여러 턴에 걸쳐 일관성을 유지해야 하는 동적인 대화 능력을 제대로 측정하기 어렵다.4</p>
<p>더욱 심각한 문제는 기존 성능 지표들이 모델이 생성하는 결과물의 안전성과 윤리적 측면을 완전히 간과한다는 점이다.3 높은 정확도나 BLEU 점수를 기록한 모델이 실제로는 유해하거나 편향된, 혹은 사회적으로 위험한 콘텐츠를 생성할 수 있는 가능성을 전혀 통제하지 못한다.5 또한, 모델들이 널리 알려진 벤치마크 데이터셋에 과적합(overfitting)되어 점수만 높이는 ‘벤치마크 해킹(gaming)’ 현상은 모델의 진정한 일반화 능력을 왜곡시켜 평가의 신뢰성을 근본적으로 훼손한다.5</p>
<p>생성형 AI의 등장은 단순히 더 나은 평가 ’방법’을 요구하는 것을 넘어, ’성능(performance)’이라는 개념 자체를 재정의하도록 강요하고 있다. 과거에 ’성능’이 ’정확도’와 거의 동의어였다면, 이제는 사용자의 복잡한 의도를 얼마나 잘 파악하고 유용한 결과물을 제공하는지를 나타내는 ‘유용성(helpfulness)’, 생성된 내용이 논리적으로 타당하고 일관되는지를 의미하는 ‘일관성(coherence)’, 그리고 새로운 아이디어를 제시하는 ’창의성(creativity)’과 같은 질적 차원까지 포괄하는 다차원적 개념으로 확장되고 있다.21 이는 AI를 정답을 찾는 단순한 도구(tool)에서 인간과 협력하는 파트너(collaborator)로 바라보는 관점의 변화를 반영하며, 평가 패러다임의 전환은 이러한 철학적 변화를 기술적으로 구현하려는 시도라고 볼 수 있다.</p>
<p>이러한 평가의 복잡성 증가는 AI 시스템이 사회에 깊숙이 통합되는 과정과 정비례한다. AI가 실험실을 나와 대출 심사, 채용, 의료 진단 등 사회의 핵심적인 의사결정 과정에 직접적으로 관여하게 되면서, 그 결정이 미치는 사회적, 윤리적 파급효과가 기술적 정확성만큼이나 중요해졌다.4 따라서 평가 지표 역시 기술적 성능을 넘어 공정성, 투명성, 책임성과 같은 사회적 가치를 측정해야 할 책임을 부여받게 된 것이다. 즉, AI 평가 패러다임의 복잡화는 AI 기술이 사회 시스템의 일부로 편입되면서 발생하는 필연적인 결과이며, 기술과 사회의 상호작용을 비추는 거울과 같다.</p>
<h2>3.  새로운 평가 패러다임의 다차원적 구성</h2>
<p>생성형 AI가 제기한 도전 과제에 대응하기 위해, AI 평가 패러다임은 정확도라는 단일 차원을 넘어 강건성, 공정성, 설명가능성, 효율성 등 다각적인 측면을 포괄하는 방향으로 진화하고 있다. 이는 AI 시스템의 신뢰성과 사회적 수용성을 확보하기 위한 필수적인 전환이다.</p>
<h3>3.1  강건성(Robustness): 예측 불가능성에 대한 저항력</h3>
<p>강건성이란 모델이 예상치 못한 입력 변화나 의도적인 적대적 공격(adversarial attacks)에도 불구하고 안정적인 성능과 예측을 유지하는 능력을 의미한다.6 이는 시스템의 신뢰성과 직결되는 핵심 요소로, 특히 자율주행차, 의료 진단, 금융 시스템과 같이 사소한 오류가 치명적인 결과를 초래할 수 있는 고위험(high-risk) AI 시스템에서 절대적으로 요구되는 특성이다.27</p>
<p>강건성을 평가하기 위한 방법론은 다양하다. 가장 기본적인 접근법은 입력 데이터에 미세한 노이즈를 추가하거나, 문장의 의미는 유지한 채 단어나 구문 구조를 바꾸는 등의 섭동(perturbation)을 가하여 모델의 성능 저하 정도를 측정하는 것이다.26 더 나아가, 최근 LLM을 대상으로 활발히 연구되는 평가 방식은 프롬프트 인젝션(prompt injection)이나 탈옥(jailbreak)과 같은 적대적 공격에 대한 방어 능력을 측정하는 것이다.28 이는 모델의 안전 가드레일을 우회하여 유해하거나 의도치 않은 응답을 생성하도록 유도하는 공격으로, 이에 대한 방어 능력은 ’적대적 강건성 점수’와 같이 0에서 1 사이의 값으로 정량화될 수 있다.28</p>
<h3>3.2  공정성(Fairness): 편향 없는 의사결정을 향하여</h3>
<p>공정성은 AI 모델의 예측이나 결정이 성별, 인종, 연령과 같은 민감한 인구통계학적 특성에 따라 특정 그룹에게 체계적으로 불이익을 주지 않도록 보장하는 것을 목표로 한다.6 ’공정성’은 단일하게 정의될 수 있는 개념이 아니며, 적용되는 사회적 맥락과 해결하고자 하는 문제에 따라 다양한 수학적 정의가 존재한다.31 주요 공정성 지표는 다음과 같다.</p>
<ul>
<li>
<p><strong>인구통계학적 동등성 (Demographic Parity)</strong>: 이 지표는 민감한 속성값에 관계없이 모든 그룹이 긍정적인 예측 결과를 받을 확률이 동일해야 한다는 ’결과의 평등’을 요구한다.33 예를 들어, 대출 승인 모델이 남성 지원자와 여성 지원자에게 동일한 비율로 대출을 승인해야 한다는 것이다.</p>
<p><span class="math math-display">
P(\hat{Y}=1 | A=a) = P(\hat{Y}=1 | A=b)
</span></p>
</li>
<li>
<p><strong>기회의 균등 (Equal Opportunity)</strong>: 이 지표는 실제로 자격이 있는(실제 값이 Positive, <span class="math math-inline">Y=1</span>) 사람들 사이에서, 모든 그룹이 긍정적인 예측을 받을 확률(즉, True Positive Rate)이 동일해야 한다는 ’기회의 평등’을 강조한다.33 대출 심사에서 실제로 상환 능력이 있는 지원자 그룹 내에서 남성과 여성이 동등한 비율로 대출 승인을 받아야 한다는 의미다.</p>
<p><span class="math math-display">
P(\hat{Y}=1 | Y=1, A=a) = P(\hat{Y}=1 | Y=1, A=b)
</span></p>
</li>
<li>
<p><strong>균등화된 승산 (Equalized Odds)</strong>: 기회의 균등을 확장한 개념으로, 자격이 있는 그룹(Y=1)과 자격이 없는 그룹(Y=0) 모두에서 TPR과 FPR(False Positive Rate)이 그룹 간에 동일할 것을 요구한다.29 이는 모델의 오류율이 모든 그룹에 걸쳐 동등하게 분포되어야 함을 의미한다.</p>
</li>
</ul>
<p>이러한 공정성 지표를 추구하는 과정에서 종종 ’공정성-정확성 상충관계(Fairness-Accuracy Trade-off)’라는 딜레마가 발생한다. 이는 특정 공정성 기준을 만족시키기 위해 모델을 보정하는 과정이 전체적인 예측 정확도를 감소시킬 수 있다는 문제다.36 그러나 이 상충관계는 필연적인 법칙이 아니며, 데이터 전처리 단계에서의 편향 제거, 알고리즘 학습 과정에서의 제약 조건 추가, 예측 결과 후처리 등 다양한 기술적 접근을 통해 완화하거나 최적의 균형점을 찾으려는 연구가 활발히 진행되고 있다.38</p>
<p><strong>표 2: AI 공정성 핵심 지표 비교</strong></p>
<table><thead><tr><th>지표명</th><th>핵심 개념</th><th>수학적 정의</th><th>장점</th><th>단점 및 고려사항</th></tr></thead><tbody>
<tr><td><strong>인구통계학적 동등성</strong></td><td>모든 그룹이 긍정적 결과를 받을 확률이 동일해야 함 (결과의 평등)</td><td><span class="math math-inline">P(\hat{Y}=1 \rvert A=a) = P(\hat{Y}=1 \rvert A=b)</span></td><td>직관적이고 이해하기 쉬움</td><td>실제 자격 여부를 고려하지 않아, 자격이 있는 그룹에 대한 역차별을 유발할 수 있음 33</td></tr>
<tr><td><strong>기회의 균등</strong></td><td>자격이 있는(Y=1) 개인들 사이에서 모든 그룹이 긍정적 예측을 받을 확률(TPR)이 동일해야 함 (기회의 평등)</td><td><span class="math math-inline">P(\hat{Y}=1 \rvert Y=1, A=a) = P(\hat{Y}=1 \rvert Y=1, A=b)</span></td><td>자격 요건을 갖춘 사람들에게 공평한 기회를 보장하는 데 중점을 둠</td><td>자격이 없는(Y=0) 사람들에 대한 오분류(FPR) 차이는 고려하지 않음 33</td></tr>
<tr><td><strong>균등화된 승산</strong></td><td>모든 그룹에 대해 TPR과 FPR이 모두 동일해야 함 (오류율의 평등)</td><td><span class="math math-inline">P(\hat{Y}=1 \rvert Y=1, A=a) = P(\hat{Y}=1 \rvert Y=1, A=b)</span> 및 <span class="math math-inline">P(\hat{Y}=1 \rvert Y=0, A=a) = P(\hat{Y}=1 \rvert Y=0, A=b)</span></td><td>가장 엄격한 기준 중 하나로, 모델의 오류가 특정 그룹에 집중되는 것을 방지함</td><td>세 가지 기준을 동시에 만족시키는 것은 수학적으로 불가능한 경우가 많음 40</td></tr>
</tbody></table>
<h3>3.3  설명가능성(Explainability, XAI): 블랙박스 해체와 신뢰 구축</h3>
<p>설명가능성이란 AI 모델이 내린 특정 결정이나 예측의 근거를 인간이 이해할 수 있는 형태로 제시하는 능력이다.22 복잡한 딥러닝 모델이 ’블랙박스’처럼 작동하여 그 내부 과정을 파악하기 어려운 상황에서, XAI는 모델의 투명성을 확보하고 사용자의 신뢰를 구축하는 데 결정적인 역할을 한다.43 또한, 모델의 잠재적 편향을 탐지하고 디버깅하며, 규제 요구사항을 준수하는 데 필수적이다.41</p>
<p>XAI의 평가 방법론은 크게 두 가지로 나뉜다. 첫째는 LIME(Local Interpretable Model-agnostic Explanations), SHAP(SHapley Additive exPlanations)과 같이 모델의 종류에 구애받지 않고 적용할 수 있는 모델-불특정(model-agnostic) 기법이다.46 이들은 특정 예측에 대해 입력 변수의 각 부분이 얼마나, 그리고 어떻게 기여했는지를 시각화하거나 정량적인 값으로 보여준다.48 둘째는 생성된 설명이 실제 사용자에게 얼마나 만족스러운지, 이해하기 쉬운지, 그리고 의사결정에 실질적인 도움이 되는지를 설문조사나 사용자 실험을 통해 직접 측정하는 인간 중심 평가(human-centered evaluation) 방식이다.43</p>
<h3>3.4  효율성(Efficiency)과 그 너머: 지속가능한 AI를 위한 척도</h3>
<p>새로운 패러다임에서 효율성은 단순히 응답 속도(latency)나 처리량(throughput)과 같은 전통적인 성능 지표를 넘어선다. 이제 효율성은 모델을 훈련하고 추론하는 데 소요되는 총체적인 비용과 자원을 포괄하는 개념으로 확장되었다.52 여기에는 API 호출 비용, 클라우드 인프라 비용과 같은 직접적인 금전적 비용뿐만 아니라, GPU/TPU와 같은 컴퓨팅 자원 사용률, 그리고 모델 운영에 필요한 에너지 소비량까지 포함된다.53 이러한 다각적인 효율성 평가는 AI 모델의 경제적, 환경적 지속가능성을 판단하는 중요한 척도가 된다.</p>
<p>이와 더불어, 창의성(creativity), 논리적 일관성(coherence), 유창성(fluency)과 같이 정량화하기 어려운 질적 측면들도 중요한 평가 요소로 부상하고 있다.23 이러한 주관적인 특성들은 전통적인 자동화 지표로는 측정이 불가능하기에, 인간 평가자가 직접 평가하거나, 강력한 LLM을 심판으로 활용하는 ’LLM-as-a-Judge’와 같은 새로운 방법론을 통해 평가가 이루어진다.21</p>
<p>이 새로운 지표들은 서로 독립적이지 않으며, 복잡한 상호의존성과 상충관계를 가진다. 예를 들어, 모델의 설명가능성을 높이기 위해 더 단순한 구조의 모델을 사용하면 정확성이나 강건성이 저하될 수 있다. 공정성을 강제하는 제약 조건은 모델의 예측 정확도를 떨어뜨릴 수 있으며(‘공정성-정확성 상충관계’), 적대적 공격에 대한 강건성을 높이기 위한 훈련(adversarial training)은 일반 데이터에 대한 성능을 저하시키고 더 많은 계산 자원을 요구하여 효율성을 떨어뜨릴 수 있다. 따라서 미래의 AI 평가는 단일 지표의 최적화가 아닌, 이러한 다차원적 지표들 간의 복잡한 관계를 이해하고, 특정 응용 분야의 요구사항에 맞게 최적의 균형점을 찾는 ‘다중 목표 최적화(Multi-objective Optimization)’ 문제로 진화할 것이다.</p>
<p>이러한 변화는 평가의 주체가 기술 그 자체에서 사회-기술 시스템으로 이동하고 있음을 시사한다. 전통적 평가는 모델 자체의 기술적 속성(정확도 등)에만 집중했다. 그러나 새로운 패러다임의 지표들, 특히 공정성과 설명가능성은 모델이 사용되는 사회적 맥락 없이는 정의되거나 측정될 수 없다. ’공정성’은 사회적 합의가 필요한 가치 판단의 문제이며, ’설명가능성’은 그 설명을 듣고 이해하는 ’인간’을 전제한다. 이는 평가의 대상이 순수한 기술적 산출물로서의 AI 모델에서, 그 기술이 인간 및 사회와 상호작용하며 영향을 주고받는 복합적인 ’사회-기술 시스템(Socio-technical System)’으로 확장되고 있음을 의미한다.</p>
<h2>4.  실증적 평가의 최전선: 차세대 벤치마크와 방법론</h2>
<p>새로운 평가 패러다임의 요구에 부응하기 위해, 연구 및 산업계에서는 기존의 단편적인 벤치마크를 넘어 AI의 다면적 능력을 종합적으로 측정하려는 차세대 벤치마크와 혁신적인 평가 방법론을 개발하고 있다.</p>
<h3>4.1  총체적 평가 프레임워크: HELM과 BIG-bench</h3>
<ul>
<li><strong>HELM (Holistic Evaluation of Language Models)</strong>: 스탠포드 CRFM(Center for Research on Foundation Models)이 개발한 HELM은 ’총체적 평가’를 기치로 내건다. 이 프레임워크는 정확도, 강건성, 공정성, 편향, 유해성, 효율성 등 7가지 핵심 지표를 42개의 다양한 실제 세계 시나리오에 걸쳐 종합적으로 평가한다.56 예를 들어, 대학원 수준의 과학 문제 해결 능력을 측정하는 ‘GPQA’ 시나리오를 통해 모델의 복잡한 추론 능력을 평가하는 식이다.57 이는 모델들을 단일 점수로 줄 세우는 경쟁에서 벗어나, 각 모델의 강점과 약점을 입체적으로 분석하는 다면적 프로필을 제공하려는 시도다.58</li>
<li><strong>BIG-bench (Beyond the Imitation Game Benchmark)</strong>: 구글 주도로 여러 기관이 협력하여 구축한 이 벤치마크는 200개가 넘는 방대하고 다양한 태스크를 통해 현재 LLM의 능력 한계를 탐색하고 미래의 잠재력을 예측하는 것을 목표로 한다.60 최근에는 기존 벤치마크 문제들이 최신 LLM에 의해 대부분 해결되는 ‘포화’ 현상이 나타나자, 훨씬 더 높은 난이도의 문제들로 구성된 ’BIG-Bench Extra Hard (BBEH)’를 선보이며 지속적으로 평가의 난이도를 높여가고 있다.62</li>
</ul>
<h3>4.2  특정 능력 심층 평가: HumanEval과 MT-Bench</h3>
<ul>
<li><strong>HumanEval</strong>: OpenAI가 개발한 HumanEval은 AI의 코드 생성 능력 평가에 특화된 벤치마크다. 164개의 Python 프로그래밍 문제에 대해, 모델이 생성한 코드가 숨겨진 단위 테스트를 통과하여 기능적으로 올바르게 작동하는지를 평가한다.63 특히</li>
</ul>
<p><code>pass@k</code>라는 독특한 지표를 사용하는데, 이는 모델이 <span class="math math-inline">k</span>개의 코드 샘플을 생성했을 때 그중 적어도 하나가 테스트를 통과할 확률을 측정한다. 이는 한 번에 완벽한 코드를 짜기보다 여러 번의 시도를 통해 올바른 코드를 찾아가는 현실적인 프로그래밍 상황을 모사한 것이다.</p>
<ul>
<li><strong>MT-Bench</strong>: 이 벤치마크는 LLM의 핵심 능력 중 하나인 대화 능력, 특히 여러 턴(multi-turn)에 걸쳐 맥락을 유지하고 일관된 대화를 이어가는 능력을 평가하기 위해 설계되었다. 80개의 고품질 다중 턴 질문 세트를 통해, 모델이 이전 대화 내용을 기억하고 사용자의 복잡한 지시 사항을 얼마나 잘 준수하는지를 심층적으로 평가한다.64</li>
</ul>
<h3>4.3  인간과 AI의 협력: HITL과 ‘LLM-as-a-Judge’</h3>
<ul>
<li><strong>Human-in-the-Loop (HITL)</strong>: 창의성, 문체의 적절성, 윤리적 판단 등 주관적이고 미묘한 품질을 평가하는 데 있어 인간의 역할은 여전히 대체 불가능하다. HITL은 AI 시스템의 훈련, 평가, 운영 과정 전반에 인간의 전문 지식과 피드백을 체계적으로 통합하는 접근 방식이다.65 인간 평가자는 모델의 편향을 탐지하고, 결과물의 신뢰성을 검증하며, AI가 학습하지 못한 세상의 상식과 가치를 주입하는 중요한 역할을 수행한다.67</li>
<li><strong>LLM-as-a-Judge</strong>: 인간 평가는 비용과 시간이 많이 소요된다는 한계가 있다. 이에 대한 대안으로 등장한 ’LLM-as-a-Judge’는 GPT-4와 같이 매우 강력한 성능을 지닌 LLM을 ’심판’으로 활용하여 다른 모델들의 출력물을 평가하는 혁신적인 방법론이다.21 MT-Bench와 같은 벤치마크에서 활발히 사용되며, 인간 평가에 비해 월등한 확장성과 비용 효율성을 자랑한다.71</li>
<li><strong>한계와 윤리적 과제</strong>: 이 방법론은 ’누가 심판을 심판할 것인가?’라는 근본적인 질문을 던진다. 심판 LLM 자체에 내재된 편향이 평가 결과의 공정성과 신뢰성을 심각하게 훼손할 수 있기 때문이다.73 연구에 따르면 심판 LLM은 먼저 제시된 답변을 선호하는 ‘위치 편향(position bias)’, 내용의 질과 무관하게 더 긴 답변에 높은 점수를 주는 ‘장황함 편향(verbosity bias)’, 그리고 자신과 유사한 스타일의 답변을 선호하는 ‘자기 향상 편향(self-enhancement bias)’ 등을 보이는 것으로 나타났다.74</li>
</ul>
<p>과거의 벤치마크가 모델들을 성적순으로 한 줄로 세우는 ’정적 시험’의 성격이 강했다면, HELM, BIG-bench와 같은 차세대 벤치마크는 다차원적 지표와 다양한 시나리오를 통해 모델의 강점과 약점을 입체적으로 보여주는 ’동적 진단 도구’로 진화하고 있다.56 이는 개발자에게는 모델을 개선할 방향을 제시하고, 사용자에게는 특정 작업에 가장 적합한 모델을 선택할 수 있는 상세한 ’사용 설명서’를 제공하는 역할을 한다. 즉, 평가의 목적이 단순히 ’순위 매기기’에서 모델에 대한 깊은 ’이해와 활용’으로 이동하고 있음을 보여준다.</p>
<p>또한, ’LLM-as-a-Judge’의 등장은 AI를 평가하기 위해 다시 AI를 사용하는 재귀적(recursive) 구조를 만들어냈다. 이는 평가 과정의 자동화와 확장을 가능하게 하는 혁신이지만, 동시에 심판 LLM의 편향이 평가받는 모델들에게 전파되고, 이 편향된 모델들이 다시 다음 세대 심판 LLM의 훈련 데이터가 되는 악순환의 위험을 내포한다.73 이러한 재귀적 구조의 위험성을 통제하고 평가의 신뢰성을 확보하기 위해, HITL을 통한 인간의 지속적인 개입과 감독, 그리고 심판 LLM 자체의 작동 방식에 대한 투명성 확보가 새로운 핵심 과제로 부상하고 있다.</p>
<p><strong>표 3: 차세대 AI 벤치마크 비교 분석</strong></p>
<table><thead><tr><th>벤치마크</th><th>핵심 목표</th><th>주요 시나리오 / 태스크</th><th>핵심 지표</th><th>측정하는 AI 능력</th></tr></thead><tbody>
<tr><td><strong>HELM</strong></td><td>모델의 다면적 능력을 ‘총체적으로’ 평가</td><td>42개 실제 시나리오 (요약, QA, 대화 등)</td><td>정확도, 강건성, 공정성, 유해성, 효율성 등 7가지</td><td>범용성, 신뢰성, 윤리성 56</td></tr>
<tr><td><strong>BIG-bench</strong></td><td>현재 LLM의 한계를 탐색하고 미래 능력을 예측</td><td>200+개의 창의적이고 도전적인 태스크</td><td>태스크별 맞춤형 지표</td><td>추론, 상식, 창의력, 미개척 능력 60</td></tr>
<tr><td><strong>HumanEval</strong></td><td>코드 생성 능력의 기능적 정확성 평가</td><td>164개 Python 프로그래밍 문제</td><td><code>pass@k</code> (기능적 정확성)</td><td>프로그래밍 및 알고리즘 구현 능력 63</td></tr>
<tr><td><strong>MT-Bench</strong></td><td>다중 턴 대화 및 지시 사항 준수 능력 평가</td><td>80개 다중 턴 질의응답 세트</td><td>LLM-as-a-Judge 기반 점수</td><td>대화의 일관성, 맥락 이해, 지시 이행 능력 64</td></tr>
</tbody></table>
<h2>5.  현실 세계의 적용과 도전 과제: 사례 연구 및 규제 동향</h2>
<p>새로운 평가 패러다임의 지표들은 단순히 학술적 논의에 그치지 않고, 현실 세계의 AI 시스템이 야기하는 구체적인 문제들을 분석하고 해결하는 데 실질적인 기준으로 작용하고 있다. 또한, 각국 정부와 국제기구는 이러한 평가 기준을 법적, 제도적 규제 프레임워크에 통합하려는 움직임을 보이고 있다.</p>
<h3>5.1  사례 연구: 평가 지표의 실제적 함의</h3>
<ul>
<li><strong>AI 편향: 대출 및 채용 시스템의 그림자</strong>: 공정성 평가는 AI가 사회적 불평등을 심화시킬 수 있는 영역에서 특히 중요하다. 과거의 차별적 관행이 포함된 데이터를 학습한 AI는 편향을 그대로 답습하거나 증폭시킬 수 있다. 실제로 미국의 일부 AI 대출 심사 시스템은 과거 특정 인종 그룹의 거주 지역에 대출을 거부했던 ‘레드라이닝(redlining)’ 관행과 유사한 패턴을 보여, 신용도가 비슷한 소수 인종 지원자에게 더 높은 거부율을 보인 사례가 보고되었다.76 또한, 아마존이 개발했던 AI 채용 도구는 과거 남성 중심의 이력서 데이터를 학습한 결과, 이력서에 ’여성(women’s)’과 같은 단어가 포함되면 감점하는 등 여성을 체계적으로 배제하여 결국 폐기되었다.78 이러한 사례들은 인구통계학적 동등성이나 기회의 균등과 같은 공정성 지표를 통해 시스템을 사전에 감사하고 보정하는 것이 얼마나 중요한지를 명확히 보여준다.79</li>
<li><strong>적대적 공격: 의료 AI 시스템의 치명적 취약성</strong>: 강건성 평가는 시스템의 오작동이 인간의 생명과 안전에 직결되는 분야에서 필수적이다. 한 연구에서는 의료 분야 LLM을 대상으로 한 적대적 공격의 위험성을 실증적으로 보여주었다. 공격자는 악의적으로 설계된 프롬프트를 주입하거나(prompt-based attack), 의도적으로 조작된 ’중독된 데이터’로 모델을 미세조정하여(fine-tuning with poisoned data), 모델이 정상적인 상황에서는 멀쩡하게 작동하다가 특정 조건에서 치명적인 오작동을 일으키도록 만들었다.81 그 결과, 모델은 COVID-19 백신 접종을 권장하는 대신 방해하고, 환자에게 위험한 약물 조합을 추천하며, 불필요한 고가의 진단 검사를 남발하도록 조작될 수 있었다.81 이는 강건성 평가가 단순한 기술적 검증을 넘어 사회 안전망을 지키는 중요한 과정임을 시사한다.</li>
<li><strong>XAI 적용: 유방암 진단 모델의 신뢰성 확보</strong>: 설명가능성은 복잡한 AI의 결정을 전문가가 신뢰하고 책임감 있게 활용하기 위한 전제 조건이다. 유방암 진단을 보조하는 CNN(Convolutional Neural Network) 모델에 XAI 기법인 LIME과 SHAP을 적용한 사례는 그 중요성을 잘 보여준다.47 이 기법들은 모델이 유방 촬영 이미지의 어떤 특정 영역(예: 종양으로 의심되는 미세석회화 군집)에 집중하여 ’악성’이라는 판단을 내렸는지를 히트맵 형태로 시각화해준다.47 이를 통해 방사선 전문의는 AI의 판단 근거가 의학적으로 타당한지 직접 검증할 수 있으며, AI가 비정상적인 아티팩트나 노이즈에 기반해 잘못된 결론을 내리는 것을 방지할 수 있다. 이는 AI를 ’블랙박스’가 아닌, 신뢰할 수 있는 진단 보조 도구로 만드는 데 결정적인 기여를 한다.48</li>
</ul>
<h3>5.2  규제 프레임워크의 부상: EU AI Act</h3>
<p>AI 평가의 중요성이 커지면서, 이는 기술 커뮤니티 내부의 논의를 넘어 법적, 제도적 규제의 영역으로 확장되고 있다. 그 대표적인 예가 세계 최초의 포괄적 AI 규제법인 EU AI Act이다.</p>
<ul>
<li><strong>고위험 AI 시스템의 의무사항</strong>: EU AI Act는 AI 시스템을 위험 수준에 따라 4단계로 분류하고, 채용, 의료, 핵심 인프라 등에 사용되는 ’고위험 AI 시스템(High-risk AI systems)’에 대해 엄격한 의무를 부과한다.25 이 법안의 제15조는 고위험 AI 시스템이 시스템의 전체 수명 주기에 걸쳐 적절한 수준의</li>
</ul>
<p><strong>정확성(accuracy)</strong>, <strong>강건성(robustness)</strong>, **사이버보안(cybersecurity)**을 달성하고 일관되게 수행해야 한다고 명시하고 있다.27 이는 새로운 평가 패러다임의 핵심 축들이 법적 요구사항으로 전환되었음을 의미한다.</p>
<ul>
<li><strong>투명성과 인간 감독</strong>: 또한, 고위험 AI 시스템은 고품질의 편향 없는 데이터셋을 사용하고, 모든 활동 기록을 로깅하여 추적 가능성을 보장하며, 시스템의 목적과 기능에 대한 상세한 기술 문서를 제공해야 한다.25 무엇보다 중요한 것은 시스템의 작동을 감독하고 위험을 최소화하기 위한 ‘적절한 인간 감독(human oversight)’ 체계를 갖추어야 한다는 점이다.86</li>
</ul>
<p>EU AI Act의 등장은 AI 평가가 더 이상 기술 개발자들의 선택 사항이나 연구자들의 학술적 목표가 아니라, 시장에 제품을 출시하고 서비스를 운영하기 위해 반드시 입증해야 하는 ’법적 준수(Compliance)’의 문제가 되었음을 명백히 보여준다.27 이는 AI 개발 라이프사이클 전반에 걸쳐 ‘평가 및 감사(Audit)’ 단계를 제도적으로 통합시켜야 함을 의미하며, 향후 AI 감사관, AI 윤리 책임자와 같은 새로운 유형의 전문직의 등장을 촉진할 것이다.</p>
<p>한편, ’공정성-정확성 상충관계’에 대한 논의는 중요한 시사점을 제공한다. 많은 연구에서 공정성을 높이는 조치가 정확도를 떨어뜨릴 수 있다는 기술적 상충관계가 언급되지만 36, 이는 주어진 데이터와 특정 모델의 한계 내에서의 현상일 뿐, 극복 불가능한 법칙은 아니다. ’모델 다양성(model multiplicity)’이라는 개념은 거의 동일한 정확도를 가지면서도 공정성 측면에서는 훨씬 더 나은 결과를 보이는 수많은 대체 모델이 존재할 수 있음을 시사한다.88 이는 결국 어떤 종류의 오류(예: 자격 있는 사람을 탈락시키는 것 vs. 자격 없는 사람을 통과시키는 것)를 우리 사회가 더 용납할 수 없는지에 대한 사회적 가치 판단의 문제로 귀결된다. 따라서 이 상충관계는 기술적 한계라기보다는, 더 나은 데이터를 수집하고, 더 다양한 모델을 탐색하며, 사회적 합의를 통해 최적의 균형점을 찾아나가야 할 ‘설계상의 선택’ 문제로 재정의될 수 있다.</p>
<h2>6.  결론: 미래 AI 평가의 방향성과 제언</h2>
<p>인공지능 기술의 발전은 평가의 개념을 근본적으로 바꾸어 놓았다. 본 안내서에서 분석한 바와 같이, AI 평가는 단일 정답을 얼마나 정확하게 맞추는지를 측정하는 정량적 ’성능 측정’에서, 예측 불가능한 실제 세계의 복잡성 속에서 AI가 얼마나 신뢰할 수 있고(강건성), 공평하며(공정성), 투명하고(설명가능성), 지속가능한지(효율성)를 종합적으로 진단하는 ’다차원적 가치 평가’로의 패러다임 전환을 겪고 있다.</p>
<p>미래의 AI 평가 방법론은 이러한 다차원적 가치를 더욱 정교하게 측정하는 방향으로 나아갈 것이다. 정적인 데이터셋에 기반한 일회성 평가를 넘어, 사용자와의 지속적인 상호작용 속에서 실시간으로 모델의 행동과 적응 능력을 평가하는 ’상호작용적 평가(interactive evaluation)’가 중요해질 것이다. 또한, 미리 정의된 테스트 케이스를 넘어, 절차적으로 새로운 테스트 시나리오를 무한히 생성하여 모델의 잠재적 취약점과 한계를 끊임없이 탐색하는 ‘절차적 생성(procedural generation)’ 방식이 강건성 및 안전성 평가의 표준으로 자리 잡을 가능성이 높다.6 궁극적으로 평가는 AI 모델이라는 기술적 산물에 국한되지 않고, 그 모델이 배포된 시스템, 상호작용하는 사용자, 그리고 영향을 받는 사회 전체를 아우르는 ‘사회-기술 시스템’ 관점에서 통합적으로 이루어져야 할 것이다.</p>
<p>이러한 패러다임 전환의 시대에, 연구 커뮤니티는 다음과 같은 과제에 집중해야 한다. 첫째, 새로운 평가 지표들을 측정하기 위한 표준화된 방법론과 재현 가능한 벤치마크 개발을 위해 학계와 산업계의 협력을 강화해야 한다. 둘째, 강건성, 공정성, 설명가능성 등 새로운 지표들 사이에 존재하는 복잡한 상충관계를 심도 있게 분석하고, 이를 시각화하여 의사결정자가 특정 응용 분야의 맥락에 맞는 최적의 균형점을 찾도록 돕는 도구와 프레임워크를 개발해야 한다.</p>
<p>마지막으로, 가장 중요한 것은 AI 평가가 더 이상 컴퓨터 과학자들만의 과제가 아님을 인식하는 것이다. AI의 결정이 사회 전반에 미치는 영향이 커짐에 따라, 기술적 평가와 윤리적, 법적, 사회적 영향 평가를 융합하는 학제간 연구가 필수적이다. 사회과학, 법학, 윤리학, 인지과학 등 다양한 분야의 전문가들과의 긴밀한 협력을 통해, 우리는 기술적으로 우수할 뿐만 아니라 인간의 가치와 조화를 이루는, 진정으로 신뢰할 수 있는 AI를 만들어 나갈 수 있을 것이다.6</p>
<h2>7. 참고 자료</h2>
<ol>
<li>최신 대형 언어 모델(LLM) 세부 기술 동향 분석 - Goover, https://seo.goover.ai/report/202505/go-public-report-ko-2aba1627-e909-4da9-8896-1829e5f54312-0-0.html</li>
<li>대규언어 모델(LLM)은 AI의 미래인가요? - MAKEBOT.AI, https://www.makebot.ai/blog/daegyueoneo-model-llm-eun-aiyi-miraeingayo</li>
<li>Generative AI vs. Traditional AI: Understand Key Differences | by AnalytixLabs - Medium, https://medium.com/@byanalytixlabs/generative-ai-vs-traditional-ai-understand-key-differences-ca2d3e37c45d</li>
<li>Clash Between Generative AI and Traditional AI - Global Skill Development Council, https://www.gsdcouncil.org/blogs/clash-between-generative-ai-and-traditional-ai</li>
<li>Inadequacies of Large Language Model Benchmarks in the … - arXiv, https://arxiv.org/pdf/2402.09880</li>
<li>arxiv.org, https://arxiv.org/html/2502.15620v1</li>
<li>대형 언어 모델과 LLM 평가: 최신 기술의 이해와 적용 - Goover, https://seo.goover.ai/report/202504/go-public-report-ko-94a71ec0-1a61-4c89-8b63-a753bdb667e4-0-0.html</li>
<li>Understanding F1 Score, Accuracy, ROC-AUC &amp; PR-AUC Metrics - Deepchecks, https://www.deepchecks.com/f1-score-accuracy-roc-auc-and-pr-auc-metrics-for-models/</li>
<li>Essential Math for Machine Learning: Confusion Matrix, Accuracy, Precision, Recall, F1-Score | by Dagang Wei | Medium, https://medium.com/@weidagang/demystifying-precision-and-recall-in-machine-learning-6f756a4c54ac</li>
<li>What is Accuracy, Precision, Recall and F1 Score? - Labelf AI, https://www.labelf.ai/blog/what-is-accuracy-precision-recall-and-f1-score</li>
<li>Evaluation Metrics for Machine Learning - Accuracy, Precision, Recall, and F1 Defined, https://wiki.pathmind.com/accuracy-precision-recall-f1</li>
<li>Precision and recall - Wikipedia, https://en.wikipedia.org/wiki/Precision_and_recall</li>
<li>AI Accuracy Metrics: Evaluating Model Performance | Galileo, https://galileo.ai/blog/accuracy-metrics-ai-evaluation</li>
<li>Evaluation Metrics in Machine Learning - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/metrics-for-machine-learning-model/</li>
<li>Mean Squared Error: Definition, Applications and Examples - Great Learning, https://www.mygreatlearning.com/blog/mean-square-error-explained/</li>
<li>Mean Square Error (MSE) | Machine Learning Glossary - Encord, https://encord.com/glossary/mean-square-error-mse/</li>
<li>Mean Squared Error - GeeksforGeeks, https://www.geeksforgeeks.org/maths/mean-squared-error/</li>
<li>Mastering BLEU Score in NLP - Number Analytics, https://www.numberanalytics.com/blog/ultimate-guide-bleu-score-nlp</li>
<li>NLP - BLEU Score for Evaluating Neural Machine Translation …, https://www.geeksforgeeks.org/nlp/nlp-bleu-score-for-evaluating-neural-machine-translation-python/</li>
<li>BLEU Score In NLP: What Is It &amp; How To Implement In Python - Spot Intelligence, https://spotintelligence.com/2024/08/13/bleu-score-in-nlp/</li>
<li>Define your evaluation metrics | Generative AI on Vertex AI - Google Cloud, https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval</li>
<li>LLM Evaluation in the Age of AI: What’s Changing? The Paradigm Shift in Measuring AI Model Performance - Magnimind Academy, https://magnimindacademy.com/blog/llm-evaluation-in-the-age-of-ai-whats-changing-the-paradigm-shift-in-measuring-ai-model-performance/</li>
<li>Assessing Creativity Across Multi-Step Intervention Using Generative AI Models, https://learning-analytics.info/index.php/JLA/article/view/8571</li>
<li>Measuring Content Coherence and Creativity in AI- Generated Text: Metrics and Human Evaluation | Request PDF - ResearchGate, https://www.researchgate.net/publication/390175322_Measuring_Content_Coherence_and_Creativity_in_AI-_Generated_Text_Metrics_and_Human_Evaluation</li>
<li>AI Act | Shaping Europe’s digital future - European Union, https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai</li>
<li>Chapter 0 Machine Learning Robustness: A Primer - arXiv, https://arxiv.org/html/2404.00897v1</li>
<li>Article 15: Accuracy, Robustness and Cybersecurity | EU Artificial …, https://artificialintelligenceact.eu/article/15/</li>
<li>Adversarial robustness evaluation metric - IBM, https://www.ibm.com/docs/en/watsonx/saas?topic=metrics-adversarial-robustness</li>
<li>Fairness Metrics in AI-Your Step-by-Step Guide to Equitable Systems - Shelf.io, https://shelf.io/blog/fairness-metrics-in-ai/</li>
<li>AI Evaluation Metrics - Bias &amp; Fairness - FRANKI T, https://www.francescatabor.com/articles/2025/7/10/ai-evaluation-metrics-bias-amp-fairness</li>
<li>AI fairness metrics - VerifyWise, https://verifywise.ai/lexicon/ai-fairness-metrics/</li>
<li>[2506.17035] Critical Appraisal of Fairness Metrics in Clinical Predictive AI - arXiv, https://arxiv.org/abs/2506.17035</li>
<li>Common fairness metrics - Fairlearn 0.13.0.dev0 documentation, https://fairlearn.org/main/user_guide/assessment/common_fairness_metrics.html</li>
<li>[변호사,AI를 말하다]④금융 AI 가이드라인, AI 성능과 공정성을 어떻게 담고 있나 - AI타임스, https://www.aitimes.com/news/articleView.html?idxno=139532</li>
<li>“금융분야 인공지능 활성화를 위한 가이드라인 등 마련 “, https://www.fsc.go.kr/comm/getFile?srvcId=BBSTY1&amp;upperNo=75743&amp;fileTy=ATTACH&amp;fileNo=5</li>
<li>Contextualizing the Accuracy-Fairness Trade-off in Algorithmic Prediction Outcomes - ScholarSpace, https://scholarspace.manoa.hawaii.edu/server/api/core/bitstreams/5c2296e7-7b1e-44a4-9b6d-f8a06e2e35ef/content</li>
<li>The Trade-Off Between Fairness and Accuracy in Algorithm Design …, https://anderson-review.ucla.edu/the-trade-off-between-fairness-and-accuracy-in-algorithm-design/</li>
<li>When and how do fairness-accuracy trade-offs occur?, https://wearepal.ai/blog/when-and-how-do-fairness-accuracy-trade-offs-occur</li>
<li>AI that’s fair and accurate - MIT CSAIL, https://www.csail.mit.edu/news/ai-thats-fair-and-accurate</li>
<li>Fairness-Accuracy Trade-Offs: A Causal Perspective - Elias Bareinboim, https://causalai.net/r107.pdf</li>
<li>What is Explainable AI (XAI)? | IBM, https://www.ibm.com/think/topics/explainable-ai</li>
<li>설명가능한 인공지능과 데이터과학의 국내 도입 방안 - 통계청, https://kostat.go.kr/boardDownload.es?bid=11891&amp;list_no=421255&amp;seq=1</li>
<li>A review of evaluation approaches for explainable AI with applications in cardiology - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC11315784/</li>
<li>설명 가능한 AI(XAI)란 무엇인가요? - IBM, https://www.ibm.com/kr-ko/think/topics/explainable-ai</li>
<li>Explainable AI Evaluation: A Top-Down Approach for Selecting Optimal Explanations for Black Box Models - MDPI, https://www.mdpi.com/2078-2489/15/1/4</li>
<li>설명 가능한 AI(2) XAI(eXplainable AI) 주요 방법론 - AHHA Labs - 아하랩스, https://ahha.ai/2024/07/09/xai_methods/</li>
<li>(PDF) LIME and SHAP Interpretability for Medical AI Systems, https://www.researchgate.net/publication/393651039_LIME_and_SHAP_Interpretability_for_Medical_AI_Systems</li>
<li>Interpretable AI for bio-medical applications - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC10074303/</li>
<li>Explainable AI for Forensic Analysis: A Comparative Study of SHAP and LIME in Intrusion Detection Models - MDPI, https://www.mdpi.com/2076-3417/15/13/7329</li>
<li>Human-centered evaluation of explainable AI applications: a systematic review - Frontiers, https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1456486/full</li>
<li>[LG Aimers] 설명가능한 AI - 3. Explainable AI(XAI) - 3, https://rahites.tistory.com/95</li>
<li>AI Metrics Evolution: Pioneering Change in Organizational Development, https://www.usaii.org/ai-insights/ai-metrics-evolution-pioneering-change-in-organizational-development</li>
<li>AI model performance metrics: In-depth guide - Nebius, https://nebius.com/blog/posts/ai-model-performance-metrics</li>
<li>KPIs for gen AI: Measuring your AI success | Google Cloud Blog, https://cloud.google.com/transform/gen-ai-kpis-measuring-ai-success-deep-dive</li>
<li>Observability in Generative AI with Azure AI Foundry - Microsoft Learn, https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/observability</li>
<li>Everything You Need to Know About HELM - The Stanford Holistic Evaluation of Language Models | by PrajnaAI | Jun, 2025, https://prajnaaiwisdom.medium.com/everything-you-need-to-know-about-helm-the-stanford-holistic-evaluation-of-language-models-f921b61160f3</li>
<li>HELM Capabilities - Stanford CRFM, https://crfm.stanford.edu/2025/03/20/helm-capabilities.html</li>
<li>HELM Dataset - Papers With Code, https://paperswithcode.com/dataset/helm</li>
<li>HELM Lite: Lightweight and Broad Capabilities Evaluation - Stanford CRFM, https://crfm.stanford.edu/2023/12/19/helm-lite.html</li>
<li>google/bigbench / Datasets at Hugging Face, https://huggingface.co/datasets/google/bigbench</li>
<li>google/BIG-bench: Beyond the Imitation Game … - GitHub, https://github.com/google/BIG-bench</li>
<li>BIG-Bench Extra Hard - arXiv, https://arxiv.org/html/2502.19187v1</li>
<li>HumanEval: When Machines Learned to Code | AI Programming …, https://www.runloop.ai/blog/humaneval-when-machines-learned-to-code</li>
<li>MT-Bench (Multi-turn Benchmark) - Klu, https://klu.ai/glossary/mt-bench-eval</li>
<li>What Is Human In The Loop | Google Cloud, https://cloud.google.com/discover/human-in-the-loop</li>
<li>Human-in-the-loop - Wikipedia, https://en.wikipedia.org/wiki/Human-in-the-loop</li>
<li>Designing Effective Human-in-the-Loop Systems for AI Evaluation | by Shaip | Medium, https://weareshaip.medium.com/designing-effective-human-in-the-loop-systems-for-ai-evaluation-e1a0588b1804</li>
<li>Human-in-the-loop or AI-in-the-loop? Automate or Collaborate? - arXiv, https://arxiv.org/html/2412.14232v1</li>
<li>LLM-as-a-judge: a complete guide to using LLMs for evaluations - Evidently AI, https://www.evidentlyai.com/llm-guide/llm-as-a-judge</li>
<li>Comprehensive Guide to LLM-as-a-Judge Evaluation - Galileo AI, https://galileo.ai/blog/llm-as-a-judge-guide-evaluation</li>
<li>Ojiyumm/MT_BENCH_RWKV - GitHub, https://github.com/ojiyumm/mt_bench_rwkv</li>
<li>[PDF] Judging LLM-as-a-judge with MT-Bench and Chatbot Arena | Semantic Scholar, https://www.semanticscholar.org/paper/Judging-LLM-as-a-judge-with-MT-Bench-and-Chatbot-Zheng-Chiang/a0a79dad89857a96f8f71b14238e5237cbfc4787</li>
<li>Blog Post: Can We Trust AI to Judge? Two Research Teams Explore …, https://ethics.nd.edu/news-and-events/news/blog-post-can-we-trust-ai-to-judge-two-research-teams-explore-the-opportunities-and-limitations-of-llm-as-a-judge/</li>
<li>arxiv.org, https://arxiv.org/html/2306.05685v4</li>
<li>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena | OpenReview, https://openreview.net/forum?id=uccHPGDlao</li>
<li>When Algorithms Judge Your Credit: Understanding AI Bias in …, https://www.accessiblelaw.untdallas.edu/post/when-algorithms-judge-your-credit-understanding-ai-bias-in-lending-decisions</li>
<li>Bias in Code: Algorithm Discrimination in Financial Systems, https://rfkhumanrights.org/our-voices/bias-in-code-algorithm-discrimination-in-financial-systems/</li>
<li>Algorithmic Bias, Financial Inclusion, and Gender - Women’s World Banking, https://www.womensworldbanking.org/wp-content/uploads/2021/02/2021_Algorithmic_Bias_Report.pdf</li>
<li>Fairness in AI-Driven Recruitment: Challenges, Metrics, Methods, and Future Directions, https://arxiv.org/html/2405.19699v3</li>
<li>Addressing Bias and Fairness in AI-Enabled Hiring and Financial Systems - ResearchGate, https://www.researchgate.net/publication/391013396_Addressing_Bias_and_Fairness_in_AI-Enabled_Hiring_and_Financial_Systems</li>
<li>Adversarial Attacks on Large Language Models in Medicine - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC11468488/</li>
<li>What Is Adversarial AI in Machine Learning? - Palo Alto Networks, https://www.paloaltonetworks.com/cyberpedia/what-are-adversarial-attacks-on-AI-Machine-Learning</li>
<li>Explainable AI for Retinoblastoma Diagnosis: Interpreting Deep Learning Models with LIME and SHAP - MDPI, https://www.mdpi.com/2075-4418/13/11/1932</li>
<li>Review on Explainable AI by using LIME and SHAP Models for Healthcare Domain, https://www.ijcaonline.org/archives/volume185/number45/shaikh-2023-ijca-923263.pdf</li>
<li>EU Artificial Intelligence Act | Up-to-date developments and analyses of the EU AI Act, https://artificialintelligenceact.eu/</li>
<li>EU AI Act: Summary &amp; Compliance Requirements - ModelOp, https://www.modelop.com/ai-governance/ai-regulations-standards/eu-ai-act</li>
<li>Key Issue 5: Transparency Obligations - EU AI Act, https://www.euaiact.com/key-issue/5</li>
<li>Debunking AI’s Supposed Fairness-accuracy Tradeoff - Technology Law - Jotwell, https://cyber.jotwell.com/debunking-ais-supposed-fairness-accuracy-tradeoff/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>