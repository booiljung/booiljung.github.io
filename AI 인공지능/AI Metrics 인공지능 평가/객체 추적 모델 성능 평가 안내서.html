<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:객체 추적 모델 성능 평가 안내서</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>객체 추적 모델 성능 평가 안내서</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">인공지능 평가지표 (AI evaluation metrics)</a> / <span>객체 추적 모델 성능 평가 안내서</span></nav>
                </div>
            </header>
            <article>
                <h1>객체 추적 모델 성능 평가 안내서</h1>
<h2>1.  객체 추적과 정량적 평가의 중요성</h2>
<h3>1.1  객체 추적(Object Tracking)의 정의와 목표</h3>
<p>객체 추적(Object Tracking)은 컴퓨터 비전의 핵심 연구 분야 중 하나로, 비디오 시퀀스와 같은 연속된 이미지 프레임 내에서 특정 객체 또는 다수 객체의 위치와 궤적을 시간의 흐름에 따라 추정하는 기술을 의미한다.1 이 기술의 궁극적인 목표는 객체의 동적인 상태 정보를 지속적으로 파악하는 것이다.</p>
<p>객체 추적 과정은 일반적으로 네 가지 주요 단계로 구성된다.3</p>
<ol>
<li>
<p><strong>대상 초기화 (Target Initialization)</strong>: 추적하고자 하는 관심 객체를 첫 프레임에서 정의하고, 보통 경계 상자(Bounding Box)로 그 위치를 지정한다.</p>
</li>
<li>
<p><strong>외형 모델링 (Appearance Modeling)</strong>: 조명 변화, 자세 변화, 가림(Occlusion) 등 다양한 환경 변화 속에서도 객체의 시각적 특성을 강건하게 표현하고 구별할 수 있는 모델을 구축한다.</p>
</li>
<li>
<p><strong>움직임 추정 (Motion Estimation)</strong>: 이전 프레임까지의 정보를 바탕으로 현재 프레임에서 객체가 존재할 가능성이 높은 영역을 예측한다.</p>
</li>
<li>
<p><strong>대상 위치 특정 (Target Positioning)</strong>: 움직임 추정으로 예측된 후보 영역 내에서 외형 모델을 사용하여 객체의 정확한 위치를 최종적으로 결정한다.</p>
</li>
</ol>
<p>이러한 객체 추적 기술은 자율주행 자동차의 주변 차량 및 보행자 인식, 지능형 영상 감시 시스템, 로보틱스의 환경 인지, 증강현실(AR) 및 인간-컴퓨터 상호작용(HCI) 등 수많은 고수준 컴퓨터 비전 과업의 기반이 되는 필수적인 기술로 자리 잡고 있다.1</p>
<h3>1.2  단일 객체 추적(SOT)과 다중 객체 추적(MOT)의 근본적 차이</h3>
<p>객체 추적은 추적하는 대상의 수에 따라 단일 객체 추적(Single Object Tracking, SOT)과 다중 객체 추적(Multiple Object Tracking, MOT)으로 명확히 구분된다. 두 분야는 단순히 대상의 수가 하나냐 여럿이냐의 차이를 넘어, 해결해야 할 문제의 본질과 기술적 접근 방식에서 근본적인 차이를 보인다.7</p>
<p>**단일 객체 추적(SOT)**은 비디오의 첫 프레임에서 지정된 단 하나의 객체를 끝까지 추적하는 것을 목표로 한다. SOT 연구의 핵심은 조명 변화, 객체의 크기 및 자세 변화, 빠른 움직임 등 대상의 외형이 급격하게 변하는 상황에서도 추적을 놓치지 않도록 정교한 외형 모델과 움직임 모델을 설계하는 데 집중된다.7 즉, SOT의 본질은 주어진 대상을 다시 찾아내는 ‘재탐색(re-localization)’ 문제에 가깝다.</p>
<p>반면, **다중 객체 추적(MOT)**은 영상 내에 존재하는 다수의 객체를 동시에 추적하는 과업이다. MOT는 SOT가 직면하는 모든 어려움을 포함할 뿐만 아니라, 다수의 객체로 인해 발생하는 다음과 같은 고유하고 복잡한 문제들을 추가로 해결해야 한다.7</p>
<ul>
<li>
<p><strong>객체 수의 변동성</strong>: 새로운 객체가 장면에 나타나거나(initialization) 기존 객체가 사라지는(termination) 상황을 동적으로 처리하고, 이에 맞춰 추적 궤적(track)을 생성하고 소멸시켜야 한다.</p>
</li>
<li>
<p><strong>정체성 유지 (Identity Maintenance)</strong>: MOT의 가장 핵심적이고 어려운 과제로, 유사한 외형을 가진 여러 객체들이 서로 교차하거나 가려지는 상황에서도 각 객체에 부여된 고유한 ID를 혼동 없이 일관되게 유지해야 한다.</p>
</li>
<li>
<p><strong>빈번한 가림 (Frequent Occlusions)</strong>: 다수의 객체가 서로를 가리거나 배경 구조물 뒤로 사라지는 일이 빈번하게 발생하며, 이로 인해 정보가 단절된 상황에서도 추적을 이어가야 한다.</p>
</li>
</ul>
<p>이러한 차이로 인해 SOT가 주로 단일 대상의 ‘재탐색’ 능력에 초점을 맞추는 반면, MOT는 프레임마다 탐지된 다수의 객체들을 기존의 추적 궤적과 올바르게 연결하는 ‘연관(association)’ 문제에 훨씬 큰 비중을 둔다.7</p>
<h3>1.3  표준화된 성능 평가의 필요성</h3>
<p>객체 추적 기술의 발전을 위해서는 알고리즘의 성능을 객관적이고 신뢰할 수 있는 방식으로 측정하는 정량적 평가가 필수적이다. 표준화된 성능 평가는 다음과 같은 중요한 역할을 수행한다.</p>
<ul>
<li>
<p><strong>객관적 비교의 기반</strong>: 모든 알고리즘을 동일한 데이터셋(Ground Truth), 공통된 평가 지표, 그리고 표준화된 테스트 절차 위에서 평가함으로써, 각 기술의 강점과 약점을 공정하고 과학적으로 비교할 수 있는 기반을 마련한다.5 과거에는 연구마다 사용하는 데이터셋이나 평가 방식이 달라 보고된 성능 지표들을 직접 비교하기 어렵거나 심지어 모순되는 경우가 많았다.8</p>
</li>
<li>
<p><strong>연구 방향 제시</strong>: 정량적 평가 결과를 심층적으로 분석하면 현재 기술 수준의 명확한 한계를 파악할 수 있다. 이는 비효율적인 접근법을 걸러내고, 향후 연구 개발이 나아가야 할 방향을 제시하는 나침반 역할을 한다.5</p>
</li>
<li>
<p><strong>신뢰성 및 재현성 확보</strong>: 표준화된 평가는 연구 결과의 신뢰도를 높이고, 다른 연구자들이 결과를 쉽게 재현하고 검증할 수 있도록 한다. 이는 학문적 발전을 가속화할 뿐만 아니라, 자율주행이나 로보틱스 분야의 안전 표준을 수립하는 등 산업적 신뢰도를 확보하는 데에도 필수적이다.5</p>
</li>
</ul>
<p>이러한 평가의 중요성은 문제 정의의 진화와도 깊은 관련이 있다. 초기의 객체 추적은 단순히 대상을 ‘찾는’ 문제에 가까웠으나, MOT의 복잡성이 부각되면서 문제는 ‘프레임마다 모든 객체를 탐지하고(Detection), 시간 축에 따라 올바른 ID로 연결하는(Association)’ 이중 과제로 재정의되었다.1 이러한 패러다임의 변화는 평가 지표의 발전을 촉진했고, 역으로 평가 지표의 한계는 문제 정의를 더욱 정교하게 만드는 상호작용을 통해 기술 발전을 이끌어왔다.</p>
<p>더 나아가, 정량적 평가는 단순히 알고리즘의 순위를 매기는 ’심판’의 역할을 넘어선다. 평가 지표를 세분화하여 분석하면(예: MOTA의 FP, FN, IDSW), 모델이 ‘무엇을’ 잘못하고 있는지 진단할 수 있는 ‘가이드’ 역할을 수행한다.11 예를 들어, 낮은 재현율(Recall)은 모델이 실제 객체를 자주 놓치고 있음을, 낮은 정밀도(Precision)는 있지도 않은 허상을 탐지하고 있음을 시사한다.11 이처럼 평가는 모델 개선을 위한 구체적인 단서를 제공하는 강력한 분석적 도구로 기능한다.</p>
<h2>2.  성능 평가의 기초: Ground Truth와 IoU</h2>
<h3>2.1  Ground Truth의 역할과 구성</h3>
<p>모든 객체 추적 모델의 성능 평가는 ‘정답’ 데이터와의 비교를 통해 이루어진다. 이 정답 데이터가 바로 <strong>Ground Truth</strong>이다. Ground Truth는 모델의 예측이 얼마나 정확한지를 판단하는 절대적인 기준으로, 평가의 신뢰성을 담보하는 가장 근본적인 요소이다.12</p>
<p>Ground Truth 데이터셋은 일반적으로 비디오의 모든 프레임에 대해, 영상에 존재하는 모든 관심 객체의 위치와 ID 정보를 포함한다. 각 객체의 위치는 보통 경계 상자(Bounding Box)의 좌표(예: 좌측 상단 x, y 좌표와 너비, 높이)로 표현되며, 각 객체에는 시퀀스 전체에서 변하지 않는 고유한 ID가 할당된다.2 이 데이터는 사람이 직접 수동으로 어노테이션 작업을 통해 생성되므로, 높은 정확성과 일관성을 유지하는 것이 매우 중요하다.</p>
<h3>2.2  핵심 측정 기준: IoU (Intersection over Union)</h3>
<p>모델이 예측한 결과와 Ground Truth를 비교하기 위한 가장 기본적인 측정 기준은 **IoU (Intersection over Union)**이다. 이는 ’Jaccard Index’라고도 불리며, 두 개의 경계 상자가 얼마나 겹치는지를 0과 1 사이의 값으로 정량화하는 지표이다.13</p>
<p>IoU는 모델이 예측한 경계 상자(Predicted Bounding Box)와 실제 정답 경계 상자(Ground Truth Bounding Box) 사이의 관계를 평가하는 데 사용된다. 계산 공식은 두 상자의 교집합(Intersection) 영역의 넓이를 두 상자의 합집합(Union) 영역의 넓이로 나누는 것이다.13</p>
<p>코드 스니펫</p>
<pre><code>$$\text{IoU}(A, B) = \frac{\vert A \cap B \vert}{\vert A \cup B \vert}$$
</code></pre>
<p>여기서 <code>$A$</code>는 예측 경계 상자, <code>$B$</code>는 Ground Truth 경계 상자를 나타낸다. IoU 값이 1에 가까울수록 두 상자가 거의 완벽하게 일치함을 의미하고, 0에 가까울수록 거의 겹치지 않음을 의미한다.</p>
<p>IoU 자체는 기하학적 겹침을 나타내는 연속적인 값이지만, 성능 평가의 맥락에서 그 진정한 역할은 ’임계값(threshold)’과 결합될 때 발현된다. 이 임계값은 “어느 정도 겹쳐야 ’맞았다’고 인정할 것인가?“라는 평가의 철학을 수치화한 것이다. 일반적으로 IoU 값을 특정 임계값(예: 0.5)과 비교하여, 모델의 각 예측이 성공적인지 아닌지를 판단하는 기준으로 사용된다.17</p>
<ul>
<li>
<p><strong>True Positive (TP)</strong>: 예측된 경계 상자가 Ground Truth 경계 상자와 매칭되고, 그 IoU 값이 설정된 임계값 이상일 경우. 즉, 모델이 객체를 ‘올바르게’ 탐지하고 위치도 ‘정확하게’ 예측한 경우다.</p>
</li>
<li>
<p><strong>False Positive (FP)</strong>: 예측된 경계 상자의 IoU 값이 임계값 미만이거나, 매칭되는 Ground Truth가 없는 경우. 즉, 모델이 객체의 위치를 부정확하게 예측했거나, 있지도 않은 객체를 탐지한 경우다.</p>
</li>
<li>
<p><strong>False Negative (FN)</strong>: Ground Truth 상자는 존재하지만, 그에 매칭되는 어떠한 예측 상자도 없는 경우. 즉, 모델이 실제 존재하는 객체를 놓친 경우다.</p>
</li>
</ul>
<p>이처럼 IoU는 단순한 겹침 측정 도구를 넘어, 평가의 엄격함을 조절하고 TP, FP, FN이라는 이산적인 판단을 내리는 근본적인 ’의사결정 메커니즘’으로 작용한다. 이후에 설명될 모든 복잡한 평가 지표들(mAP, MOTA, HOTA 등)은 바로 이 IoU 기반의 TP, FP, FN 정의를 가장 기본적인 구성 요소로 삼는다.</p>
<h2>3. 단일 객체 추적(SOT) 성능 평가</h2>
<p>단일 객체 추적(SOT)의 성능 평가는 주로 추적기가 주어진 단일 객체를 얼마나 정확하고 강건하게 추적하는지에 초점을 맞춘다. 이를 위해 OTB와 VOT라는 두 개의 대표적인 벤치마크가 서로 다른 평가 철학을 바탕으로 한 프로토콜을 제시해왔다.</p>
<h3>3.1. OTB 벤치마크 방식: One-Pass Evaluation (OPE)</h3>
<p>Object Tracking Benchmark (OTB)는 <strong>One-Pass Evaluation (OPE)</strong> 이라는 평가 프로토콜을 사용한다.8 이 방식은 추적기를 비디오 시퀀스의 첫 번째 프레임에서 Ground Truth로 초기화한 후, 추적이 중간에 실패하더라도 인위적인 재초기화 없이 끝까지 한 번에 실행하여 전체 성능을 평가한다.8 이는 추적기의 장기적인 강건성과 일시적 실패 후 스스로 복구하는 능력을 종합적으로 측정하려는 의도를 담고 있다.20 OTB는 주로 두 가지 시각화된 지표를 통해 성능을 보고한다.</p>
<ul>
<li>
<p><strong>정밀도 플롯 (Precision Plot)</strong>: 이 플롯은 위치 정확도를 평가하며, **중심 위치 오차(Center Location Error)**를 기반으로 한다. 중심 위치 오차는 예측된 경계 상자의 중심점과 Ground Truth 경계 상자의 중심점 사이의 유클리드 거리로 계산된다. 정밀도 플롯은 x축을 거리 임계값(threshold)으로, y축을 해당 임계값 이내의 오차를 보인 프레임의 비율(정밀도)로 하여 그래프를 그린다. 일반적으로 대표값으로 임계값이 20픽셀일 때의 정밀도 점수를 사용한다.8</p>
</li>
<li>
<p><strong>성공 플롯 (Success Plot)과 AUC (Area Under Curve)</strong>: 이 플롯은 위치와 크기를 모두 고려하는 <strong>중첩 점수(Overlap Score)</strong>, 즉 IoU를 기반으로 한다. 성공 플롯은 x축을 IoU 임계값(0에서 1까지)으로, y축을 해당 임계값 이상의 IoU를 기록한 프레임의 비율(성공률)로 하여 그래프를 그린다. <strong>AUC</strong>는 이 성공 플롯의 곡선 아래 면적을 계산한 값으로, 0과 1 사이의 단일 수치로 추적기의 전반적인 성능을 요약한다. AUC 점수가 높을수록 모든 임계값에 걸쳐 전반적으로 우수한 성능을 보임을 의미하며, 이는 평균 중첩 점수(Average Overlap Score)와 동일한 의미를 갖는다.8</p>
</li>
</ul>
<h3>3.2. VOT 챌린지 방식: 리셋(Reset) 기반 평가</h3>
<p>Visual Object Tracking (VOT) 챌린지는 전통적으로 <strong>리셋(Reset) 기반 평가</strong> 프로토콜을 채택해왔다.20 이 방식은 추적기가 추적에 실패했다고 판단되는 즉시(예: 예측과 Ground Truth의 IoU가 0이 되는 순간) 추적을 중단시키고, 몇 프레임 뒤에 Ground Truth 정보로 강제 재초기화하는 방식이다.23 이 프로토콜은 추적기가 실패를 얼마나 잘 회피하는지와, 실패하지 않는 동안 얼마나 정확하게 추적하는지를 분리하여 측정하는 데 목적이 있다. VOT는 주로 두 가지 핵심 지표로 성능을 평가한다.</p>
<ul>
<li>
<p><strong>정확도 (Accuracy)</strong>: 추적기가 실패하지 않고 성공적으로 타겟을 추적하고 있는 프레임들만을 대상으로 계산된 평균 IoU 값이다. 이 지표는 추적기가 얼마나 정밀하게 객체의 위치와 크기를 예측하는지를 나타낸다.20</p>
</li>
<li>
<p><strong>강건성 (Robustness)</strong>: 전체 시퀀스를 추적하는 동안 추적 실패가 발생한 횟수로 측정된다. 이 지표는 추적기가 얼마나 안정적으로, 실패 없이 오랫동안 추적을 유지하는지를 나타낸다. 실패 횟수가 적을수록 강건성이 높다고 평가된다.20</p>
</li>
</ul>
<p>이 두 가지 평가 프로토콜은 ’좋은 추적기란 무엇인가?’에 대한 서로 다른 철학적 관점을 반영한다. OTB의 OPE는 ‘실패하더라도 스스로 복구하여 끝까지 임무를 완수하는 능력’, 즉 실제 환경에서의 자율성과 장기적 강건성을 중시한다. 반면, VOT의 리셋 기반 방식은 ‘어떠한 상황에서도 절대 실패하지 않는 능력’, 즉 단기적 정확성과 실패 회피 능력을 극한까지 시험한다. 따라서 한 프로토콜에서 우수한 성능을 보인 추적기가 다른 프로토콜에서는 저조한 성능을 보일 수 있으며 20, 이는 추적 성능이 단일 차원으로 평가될 수 없음을 보여준다.</p>
<h3>3.3. 대표적인 SOT 벤치마크</h3>
<ul>
<li>
<p><strong>OTB (Object Tracking Benchmark)</strong>: OTB-2013(50개 시퀀스)과 OTB-2015(100개 시퀀스)로 구성된 초기 표준 벤치마크. 조명 변화, 가림, 배경 혼잡 등 11개의 도전적인 속성(challenge attributes)을 태깅하여 속성별 성능 분석이 용이하다.8</p>
</li>
<li>
<p><strong>VOT (Visual Object Tracking) Challenge</strong>: 2013년부터 매년 개최되는 경쟁 형식의 벤치마크로, 지속적으로 데이터셋을 갱신하고 평가 방식을 개선하여 SOT 연구 동향을 선도하고 있다. 리셋 기반 평가가 특징이었으나, 최근에는 OPE 방식 등 다양한 평가를 함께 제공한다.23</p>
</li>
<li>
<p><strong>LaSOT (Large-scale Single Object Tracking)</strong>: 장기 추적(Long-term tracking) 성능 평가에 중점을 둔 대규모 벤치마크. 평균 시퀀스 길이가 2,500 프레임 이상으로 매우 길어, 객체가 시야에서 완전히 사라졌다가 다시 나타나는 현실적인 시나리오를 다수 포함하고 있다.29</p>
</li>
</ul>
<h2>4. 다중 객체 추적(MOT) 성능 평가</h2>
<p>다중 객체 추적(MOT)은 SOT보다 훨씬 복잡한 과업으로, 단순히 객체를 찾는 것을 넘어 다수의 객체에 대한 탐지, 위치 추정, 그리고 가장 중요한 정체성(ID) 유지를 모두 평가해야 한다. 이를 위해 MOT 평가 지표는 탐지 오류와 연관 오류를 모두 고려하는 방향으로 발전해왔다.</p>
<h3>4.1. 고전적 표준: CLEAR MOT 지표</h3>
<p>CLEAR (Classification of Events, Activities, and Relationships) 워크숍에서 제안된 지표들은 오랫동안 MOT 성능 평가의 표준으로 사용되어 왔다.32</p>
<ul>
<li>
<p><strong>MOTA (Multiple Object Tracking Accuracy)</strong>: MOT 성능을 단일 수치로 나타내는 가장 대표적이고 널리 사용되는 지표이다. MOTA는 추적기가 범하는 세 가지 주요 오류인 <strong>False Positive (FP)</strong>, <strong>False Negative (FN)</strong>, 그리고 **ID Switch (IDSW)**의 총합을 전체 Ground Truth 객체 수로 정규화하여 계산한다.33</p>
<ul>
<li>
<p><strong>False Positives (FP)</strong>: 실제 객체가 없는 위치에 객체가 있다고 잘못 예측한 경우.32</p>
</li>
<li>
<p><strong>False Negatives (FN)</strong>: 실제로 존재하는 객체를 탐지하지 못하고 놓친 경우.32</p>
</li>
<li>
<p><strong>ID Switches (IDSW)</strong>: 이전에 특정 ID로 잘 추적하던 객체를 다른 ID로 잘못 할당한 경우.33</p>
</li>
</ul>
<p>MOTA의 계산 공식은 다음과 같다.</p>
<p>코드 스니펫</p>
</li>
</ul>
<pre><code>$$
MOTA = 1 - \frac{\sum_t (FN_t + FP_t + IDSW_t)}{\sum_t GT_t}
$$
</code></pre>
<p>여기서 <code>$FN_t$</code>, <code>$FP_t$</code>, <code>$IDSW_t$</code>는 각각 <code>$t$</code> 프레임에서의 오류 수이며, <code>$GT_t$</code>는 <code>$t$</code> 프레임에 존재하는 실제 객체의 수이다.32 MOTA 값은 1에 가까울수록 완벽한 추적 성능을 의미하며, 음수 값도 가질 수 있다.</p>
<ul>
<li>
<p><strong>MOTP (Multiple Object Tracking Precision)</strong>: 이 지표는 추적기의 위치 추정 정밀도를 측정한다. 올바르게 매칭된(True Positive) 예측 경계 상자와 Ground Truth 경계 상자 간의 평균적인 거리(예: 유클리드 거리) 또는 평균적인 불일치(예: 1 - IoU)를 나타낸다.34 MOTP는 ID가 올바른지와 무관하게, 일단 매칭된 객체들의 위치를 얼마나 정확하게 예측했는지만을 평가한다. 거리 기반 MOTP 공식은 다음과 같다.</p>
<p>코드 스니펫</p>
<pre><code>$$
MOTP = \frac{\sum_{t,i} d_{t,i}}{\sum_t c_t}
$$
</code></pre>
<p>여기서 <code>$d_{t,i}$</code>는 <code>$t$</code> 프레임에서 매칭된 <code>$i$</code>번째 객체 쌍의 거리이고, <code>$c_t$</code>는 <code>$t$</code> 프레임에서 매칭된 객체 쌍의 수이다.36</p>
</li>
</ul>
<h3>4.2. ID 유지 성능 중심: ID F1 Score (IDF1)</h3>
<p>MOTA는 계산이 직관적이지만, 탐지 오류(FP, FN)에 대한 페널티가 연관 오류(IDSW)보다 훨씬 커서 탐지 성능이 전체 점수를 좌우하는 ‘탐지 편향’ 문제가 있다. 이러한 한계를 보완하기 위해 <strong>IDF1 Score</strong>가 제안되었다. IDF1은 장기적인 관점에서 추적기가 각 객체의 정체성을 얼마나 일관되게 유지하는지를 평가하는 데 중점을 둔다.39</p>
<p>IDF1은 프레임 단위로 오류를 계산하는 MOTA와 달리, 비디오 전체 시퀀스에 걸쳐 예측된 궤적(predicted trajectories)과 실제 궤적(ground-truth trajectories) 간의 전역적인 최적의 일대일 매칭을 찾는다. 이 매칭 결과를 바탕으로, ID 관점에서의 정밀도(IDP)와 재현율(IDR)을 계산하고, 이 둘의 조화 평균을 통해 최종 IDF1 점수를 산출한다.39</p>
<ul>
<li>
<p><strong>IDTP (Identity True Positive)</strong>: 올바르게 식별된 탐지들의 총합.</p>
</li>
<li>
<p><strong>IDFP (Identity False Positive)</strong>: 잘못된 ID로 예측되었거나, Ground Truth 궤적과 매칭되지 않은 예측들의 총합.</p>
</li>
<li>
<p><strong>IDFN (Identity False Negative)</strong>: 어떤 예측 궤적과도 매칭되지 못한 Ground Truth 탐지들의 총합.</p>
</li>
</ul>
<p>각 지표의 계산 공식은 다음과 같다.35</p>
<p>코드 스니펫</p>
<pre><code>$$IDP = \frac{IDTP}{IDTP + IDFP}$$
</code></pre>
<p>코드 스니펫</p>
<pre><code>$$IDR = \frac{IDTP}{IDTP + IDFN}$$
</code></pre>
<p>코드 스니펫</p>
<pre><code>$$IDF1 = \frac{2 \times IDTP}{2 \times IDTP + IDFP + IDFN} = \frac{2 \times IDP \times IDR}{IDP + IDR}$$
</code></pre>
<h3>2.3  현대적 통합 표준: HOTA (Higher Order Tracking Accuracy)</h3>
<p>최근 MOT 평가의 새로운 표준으로 자리 잡은 <strong>HOTA</strong>는 MOTA의 탐지 편향과 IDF1의 연관 편향이라는 양극단의 한계를 모두 극복하기 위해 설계된 통합 지표이다.42 HOTA의 핵심 철학은 MOT 과업을</p>
<p><strong>탐지(Detection)</strong>, <strong>연관(Association)</strong>, **위치 정확도(Localization)**라는 세 가지 독립적인 하위 과업으로 명시적으로 분해하고, 이들의 성능을 균형 있게 결합하여 평가하는 것이다.42</p>
<p>HOTA 점수는 **탐지 정확도(Detection Accuracy, DetA)**와 **연관 정확도(Association Accuracy, AssA)**의 기하 평균으로 계산되어, 두 요소가 동등하게 중요하게 반영되도록 한다.42</p>
<p>코드 스니펫</p>
<pre><code>$$HOTA = \sqrt{DetA \times AssA}$$
</code></pre>
<ul>
<li>
<p><strong>DetA (Detection Accuracy)</strong>: 예측된 모든 탐지 객체들의 집합과 실제 모든 객체들의 집합이 얼마나 잘 정렬되는지를 Jaccard Index(IoU)와 유사한 방식으로 측정한다. 이는 전통적인 객체 탐지 평가와 유사하다.42</p>
</li>
<li>
<p><strong>AssA (Association Accuracy)</strong>: 올바르게 탐지된 객체들(TP)에 한해서, 이들의 예측된 궤적과 실제 궤적이 얼마나 일치하는지를 평균적으로 측정한다. 이는 순수한 연관 성능을 평가한다.42</p>
</li>
</ul>
<p>더 나아가 HOTA는 DetA를 탐지 재현율(DetRe)과 탐지 정밀도(DetPr)로, AssA를 연관 재현율(AssRe)과 연관 정밀도(AssPr)로 추가 분해할 수 있어, 추적기 성능에 대한 매우 상세하고 다각적인 분석을 가능하게 한다.42</p>
<p>이러한 MOT 지표들의 발전사는 추적 시스템이 범할 수 있는 ’오류(Error)’에 대한 컴퓨터 비전 커뮤니티의 이해가 점차 심화되고 정교해지는 과정을 보여준다. 초기의 CLEAR MOT(MOTA)는 오류를 ’탐지 실패(FP/FN)’와 ’단기적 ID 변경(IDSW)’이라는 프레임 단위의 직관적인 유형으로 정의했다. 이후 IDF1은 오류의 시간적 범위를 ‘궤적(trajectory)’ 단위로 확장하여 장기적 ID 유지 실패를 평가하고자 했다. 마지막으로 HOTA는 오류의 근본적인 원인을 ’탐지 문제’와 ’연관 문제’라는 두 개의 큰 축으로 구조적으로 분리하고 이를 다시 정밀도와 재현율로 세분화함으로써, 오류를 단순히 계산하는 것을 넘어 그 원인을 분석하는 단계로 나아갔다.</p>
<h2>5. MOT 평가 지표 심층 비교 분석</h2>
<p>MOT 성능을 평가하는 데 사용되는 주요 지표인 MOTA, IDF1, HOTA는 각각 다른 철학과 장단점을 가지고 있어, 평가 목적과 시나리오에 따라 적합한 지표를 선택하는 것이 중요하다.</p>
<h3>5.1. MOTA vs. IDF1: 탐지 중심 평가와 연관 중심 평가의 대립</h3>
<p>MOTA와 IDF1은 MOT 평가에서 오랫동안 양대 산맥을 이루어 왔으나, 각각 뚜렷한 편향성을 보인다.</p>
<ul>
<li>
<p><strong>MOTA의 한계</strong>: MOTA 공식에서 탐지 오류(FP, FN)와 연관 오류(IDSW)는 동일한 가중치로 합산된다. 그러나 일반적으로 한 시퀀스에서 발생하는 IDSW의 수는 FP나 FN의 수보다 훨씬 적기 때문에, 전체 점수에서 탐지 오류가 차지하는 비중이 압도적으로 커진다. 이로 인해 연관 성능이 아무리 뛰어나도 탐지기의 성능이 조금만 부족하면 MOTA 점수가 급격히 하락하는 ‘탐지 편향’ 문제가 발생한다.42</p>
</li>
<li>
<p><strong>IDF1의 장단점</strong>: IDF1은 궤적 전체를 하나의 단위로 보고 전역적인 매칭을 수행하므로, 장기적인 ID 일관성을 평가하는 데 매우 효과적이다.39 하지만 이는 반대로 연관 성능에 지나치게 집중하게 만들어, 때로는 탐지 성능이 개선되었음에도(더 많은 객체를 정확히 찾았음에도) 최적의 궤적 매칭 조합이 바뀌면서 전체 IDF1 점수가 오히려 하락하는 비직관적인 결과를 낳기도 한다.39 또한, 위치 정확도(localization accuracy)는 전혀 고려하지 않는다는 명백한 단점이 있다.</p>
</li>
</ul>
<h3>5.2. HOTA의 혁신: MOTA와 IDF1의 한계를 극복하는 방법</h3>
<p>HOTA는 MOTA와 IDF1이 가진 각각의 편향 문제를 해결하고, 보다 균형 잡히고 포괄적인 평가를 제공하기 위해 설계되었다.</p>
<ul>
<li>
<p><strong>균형</strong>: HOTA는 탐지 정확도(DetA)와 연관 정확도(AssA)를 독립적으로 계산한 뒤, 이 둘의 기하 평균을 취한다. 이로써 탐지 성능과 연관 성능 중 어느 한쪽만 뛰어나서는 높은 점수를 얻을 수 없으며, 두 능력이 조화를 이룰 때 비로소 최고의 평가를 받게 된다.42</p>
</li>
<li>
<p><strong>포괄성</strong>: HOTA는 위치 정확도를 평가에 직접적으로 통합한다. 이는 IoU 임계값을 0.05에서 0.95까지 0.05 간격으로 변화시키면서 각 임계값에 대한 HOTA 점수를 계산하고, 이들을 평균 내는 방식으로 이루어진다. 이를 통해 위치 예측이 정밀한 추적기에게 더 높은 점수를 부여하며, 이는 MOTA와 IDF1에는 없던 중요한 혁신이다.42</p>
</li>
<li>
<p><strong>분석 용이성</strong>: HOTA는 단일 점수뿐만 아니라, DetA, AssA, LocA(위치 정확도)와 같은 상위 지표와 DetRe/Pr, AssRe/Pr 같은 4개의 하위 지표로 성능을 분해하여 제공한다. 이를 통해 개발자는 자신의 추적기가 탐지를 못 하는지(낮은 DetRe), 허상을 만드는지(낮은 DetPr), 한 객체를 여러 궤적으로 나누는지(낮은 AssRe), 여러 객체를 하나의 궤적으로 합치는지(낮은 AssPr) 등을 명확하게 진단할 수 있다.42</p>
</li>
</ul>
<p>결론적으로 ’최고의 지표’는 없으며, 주어진 과업에 ’최적의 지표’만 존재한다. 각 지표는 서로 다른 질문에 답하고 있다. MOTA는 “이 추적기는 매 순간 얼마나 많은 오류를 범하는가?“라는 실용적 질문에, IDF1은 “이 추적기는 한 객체의 정체성을 얼마나 오랫동안 정확하게 유지하는가?“라는 장기적 일관성에 대한 질문에 답한다. HOTA는 “이 추적기는 탐지, 연관, 위치 특정이라는 세 가지 핵심 역량을 얼마나 균형 있게 갖추고 있는가?“라는 종합적인 능력에 대한 질문에 답한다. 따라서 평가 지표의 선택은 해당 애플리케이션의 ’성공’을 어떻게 정의하느냐에 따라 달라지는 전략적 결정이다.</p>
<h3>5.3. 시나리오별 적합한 평가 지표 선택 가이드</h3>
<ul>
<li>
<p><strong>자율주행</strong>: 안전이 최우선인 자율주행 시나리오에서는 주변의 모든 객체를 놓치지 않는 것(높은 탐지 재현율, 낮은 FN)과 있지도 않은 장애물을 인식하지 않는 것(높은 탐지 정밀도, 낮은 FP)이 모두 치명적으로 중요하다. 또한, 객체의 정확한 위치(localization)는 충돌 예측에 필수적이다. 따라서 탐지 성능을 비중 있게 다루는 MOTA나, 탐지와 위치 정확도를 모두 균형 있게 평가하는 HOTA가 적합한 지표이다.45</p>
</li>
<li>
<p><strong>장기 영상 감시</strong>: CCTV를 통한 특정 인물 추적과 같은 시나리오에서는, 한 번 포착한 대상의 ID를 놓치지 않고 동선 전체를 파악하는 것이 핵심 목표이다. 중간에 탐지가 잠시 실패하더라도 ID가 유지되는 것이 중요하다. 이러한 경우, 장기적인 ID 유지 능력을 직접적으로 평가하는 IDF1이나 HOTA의 하위 지표인 AssA 점수가 중요한 판단 기준이 될 수 있다.</p>
</li>
<li>
<p><strong>알고리즘 종합 성능 비교</strong>: 특정 응용 분야에 치우치지 않고, 추적 알고리즘 자체의 전반적인 성능을 학술적으로 공정하게 비교하고 분석하고자 할 때는, 탐지, 연관, 위치 정확도를 모두 균형 있게 고려하는 HOTA가 현재로서는 가장 이상적인 표준 지표라고 할 수 있다.</p>
</li>
</ul>
<p>아래 표는 주요 MOT 평가 지표들의 특징을 요약하여 비교한 것이다.</p>
<table><thead><tr><th>특징 (Feature)</th><th>MOTA (Multiple Object Tracking Accuracy)</th><th>IDF1 (ID F1 Score)</th><th>HOTA (Higher Order Tracking Accuracy)</th></tr></thead><tbody>
<tr><td><strong>주요 초점</strong></td><td>탐지 정확도 (Detection Accuracy)</td><td>연관 정확도 (Association Accuracy)</td><td>탐지, 연관, 위치 정확도의 균형</td></tr>
<tr><td><strong>평가 단위</strong></td><td>프레임(Frame) 단위 오류 집계</td><td>궤적(Trajectory) 단위 전역 매칭</td><td>탐지(Detection) 단위 매칭 및 궤적 정렬</td></tr>
<tr><td><strong>ID 변경 민감도</strong></td><td>단기적 ID Switch만 페널티 부여</td><td>장기적 ID 일관성 전체를 평가</td><td>장기적 궤적 분할/병합 모두 평가 (AssRe/AssPr)</td></tr>
<tr><td><strong>위치 정확도 반영</strong></td><td>간접적 (MOTP로 별도 측정)</td><td>미반영</td><td>직접적 (IoU 임계값에 대한 적분)</td></tr>
<tr><td><strong>점수 범위</strong></td><td><code>$(-\infty, 1]$</code></td><td><code>$$</code></td><td><code>$$</code></td></tr>
<tr><td><strong>주요 강점</strong></td><td>직관적, 탐지 오류에 민감</td><td>장기 ID 유지 능력 평가에 탁월</td><td>균형 잡힌 종합 평가, 상세한 오류 분석 가능</td></tr>
<tr><td><strong>주요 약점</strong></td><td>탐지 성능에 과도하게 편향</td><td>연관 성능에 편향, 탐지 개선 시 점수 하락 가능</td><td>계산이 상대적으로 복잡함</td></tr>
</tbody></table>
<h2>3.  주요 MOT 벤치마크 및 데이터셋</h2>
<p>알고리즘의 성능을 공정하게 평가하고 비교하기 위해서는 잘 설계된 표준 벤치마크 데이터셋이 필수적이다. 벤치마크는 단순히 평가용 데이터를 제공하는 것을 넘어, 해결해야 할 ‘문제’ 자체를 구체적으로 제시함으로써 전체 연구 커뮤니티의 기술 발전 방향을 이끄는 강력한 역할을 한다.</p>
<h3>3.1  MOTChallenge</h3>
<p>MOTChallenge는 다중 객체 추적 연구 분야에서 사실상의 표준 벤치마크로 인정받고 있다.47 2015년부터 시작하여 MOT16, MOT17, MOT20 등 여러 버전을 거치면서 데이터셋의 난이도를 지속적으로 높여왔다. 특히 후기 버전으로 갈수록 보행자가 매우 밀집된 혼잡한(crowded) 시나리오를 다수 포함하여, 가림(occlusion)과 객체 간 상호작용에 대한 알고리즘의 강건성을 시험하는 데 중점을 둔다.48 초기에는 CLEAR MOT 지표(MOTA, MOTP 등)를 중심으로 평가했으나, 2021년부터 HOTA를 공식 메인 지표로 채택하여 평가 기준의 현대화를 선도하고 있다.47</p>
<h3>3.2  KITTI</h3>
<p>KITTI Vision Benchmark Suite는 자율주행 기술 연구를 위해 구축된 포괄적인 벤치마크이다.51 스테레오 카메라, 고해상도 컬러/흑백 카메라, 3D LiDAR, GPS/IMU 등 실제 자율주행 차량에 탑재되는 다양한 센서로부터 수집된 데이터를 제공하는 것이 가장 큰 특징이다. 추적 벤치마크는 주로 도로 환경의 핵심 객체인 ’자동차(Car)’와 ‘보행자(Pedestrian)’ 클래스를 대상으로 하며, 2D 경계 상자뿐만 아니라 3D 공간에서의 추적 성능까지 평가한다.53 KITTI 역시 MOTChallenge와 마찬가지로 최근 평가 기준을 HOTA로 업데이트하여, 다른 주요 벤치마크와의 평가 기준 통일성을 높였다.51 KITTI 벤치마크의 등장은 2D 이미지 기반 추적을 넘어, 센서 융합(sensor fusion)과 3D 공간 인지를 활용하는 방향으로 연구를 촉진하는 계기가 되었다.</p>
<h3>3.3  평가 자동화 도구: TrackEval</h3>
<p>TrackEval은 다양한 MOT 평가 지표를 통합하여 제공하는 공식 파이썬 라이브러리이다.50 HOTA, CLEAR MOT, IDF1 등 현재 사용되는 대부분의 주요 지표들을 포함하고 있으며, MOTChallenge와 KITTI 등 주요 벤치마크의 공식 평가 코드로 채택되었다.50 TrackEval의 등장은 연구자들이 각기 다른 평가 코드를 구현하면서 발생할 수 있는 미세한 차이나 오류를 없애고, 연구 결과의 재현성과 비교 가능성을 크게 향상시켰다. 또한, 연구자들이 자신의 커스텀 데이터셋에 대해서도 표준화된 방식으로 손쉽게 성능을 평가할 수 있는 환경을 제공한다.</p>
<p>아래 표는 본문에서 다룬 주요 SOT 및 MOT 벤치마크의 특징을 요약한 것이다.</p>
<table><thead><tr><th>벤치마크 (Benchmark)</th><th>추적 유형 (Type)</th><th>주요 평가 프로토콜</th><th>핵심 평가 지표</th><th>주요 특징 및 적용 분야</th></tr></thead><tbody>
<tr><td><strong>OTB-100</strong></td><td>SOT</td><td>One-Pass Evaluation (OPE)</td><td>Precision, Success (AUC)</td><td>초기 표준, 11개 속성별 분석, 일반적 SOT 성능 평가</td></tr>
<tr><td><strong>VOT Challenge</strong></td><td>SOT</td><td>Reset-based (전통적)</td><td>Accuracy, Robustness</td><td>단기 추적 정확성 및 실패 회피 능력 집중 평가</td></tr>
<tr><td><strong>LaSOT</strong></td><td>SOT</td><td>One-Pass Evaluation (OPE)</td><td>Precision, Success (AUC)</td><td>대규모, 장기 추적 시나리오, 객체 재등장 포함</td></tr>
<tr><td><strong>MOTChallenge</strong></td><td>MOT</td><td>프레임 단위 매칭</td><td>HOTA, MOTA, IDF1</td><td>보행자 추적 표준, 혼잡한 환경, 다양한 카메라 움직임</td></tr>
<tr><td><strong>KITTI</strong></td><td>MOT</td><td>프레임 단위 매칭</td><td>HOTA, MOTA (3D/2D)</td><td>자율주행, 멀티센서(카메라, LiDAR) 데이터, 3D 공간 평가</td></tr>
</tbody></table>
<h2>4.  결론: 객체 추적 평가의 현재와 미래</h2>
<p>객체 추적 모델의 성능 평가는 지난 10여 년간 기술의 발전과 함께 눈부신 진화를 거듭해왔다. SOT 평가는 위치 및 중첩 기반의 직관적인 지표에서 시작하여, 추적기의 장기적 강건성(OPE)과 단기적 정밀성(Reset)을 측정하는 서로 다른 철학의 평가 프로토콜로 분화되었다. MOT 평가는 더욱 극적인 변화를 겪었다. 프레임 단위의 단순 오류(FP, FN, IDSW)를 집계하던 MOTA에서 출발하여, 궤적의 장기적 일관성을 강조하는 IDF1을 거쳐, 마침내 탐지, 연관, 위치 정확도라는 세 가지 핵심 요소를 균형 있게 분석하는 종합적인 평가 도구인 HOTA에 이르렀다. 이러한 평가 지표의 발전사는 객체 추적이라는 복잡한 문제에 대한 우리 커뮤니티의 이해가 점차 깊어졌음을 반영한다.</p>
<p>현재 HOTA는 가장 균형 잡히고 분석적인 지표로 널리 인정받고 있지만, 객체 추적 평가의 여정은 여기서 멈추지 않을 것이다. 미래 연구를 위해 다음과 같은 방향을 제언한다.</p>
<ul>
<li>
<p><strong>도메인 특화 지표의 필요성</strong>: 현재의 표준 지표들은 일반적인 추적 성능을 측정하는 데 초점을 맞추고 있다. 하지만 자율주행에서의 ’충돌 위험도’나 지능형 감시에서의 ’이상 행동 패턴 감지’와 같이, 특정 응용 분야의 최종 목표와 직결되는 새로운 도메인 특화 지표의 개발이 필요하다.45 이는 알고리즘이 실제 산업 현장에서 요구하는 핵심 가치를 얼마나 잘 충족하는지를 직접적으로 평가하는 데 도움을 줄 것이다.</p>
</li>
<li>
<p><strong>설명가능성(Explainability)과의 결합</strong>: 현재 지표들은 ‘무엇을’ 잘못했는지는 알려주지만 ‘왜’ 잘못했는지는 설명하지 못한다. 예를 들어, ID Switch가 발생했을 때 그것이 유사한 외형을 가진 다른 객체 때문인지, 급격한 움직임 예측 실패 때문인지를 정량적으로 분석해주는 지표가 개발된다면, 모델 디버깅과 개선에 획기적인 도움을 줄 수 있을 것이다.</p>
</li>
<li>
<p><strong>End-to-End 모델을 위한 새로운 평가</strong>: 최근 탐지와 추적, 심지어 재식별(Re-ID)까지 단일 네트워크에서 처리하는 End-to-End 모델들이 등장하고 있다. 이러한 모델들은 전통적인 ‘탐지 후 연관(Tracking-by-Detection)’ 패러다임과 다른 방식으로 작동하므로, 기존의 평가 방식이 이들의 성능을 공정하게 평가하고 있는지에 대한 지속적인 검토와 새로운 평가 프로토콜에 대한 논의가 필요하다.</p>
</li>
</ul>
<p>객체 추적 기술이 더욱 복잡하고 다양한 현실 세계의 문제에 적용됨에 따라, 성능 평가 방법론 역시 그에 맞춰 끊임없이 진화해야 할 것이다. 미래의 평가 지표는 단순히 점수를 매기는 것을 넘어, 모델의 행동을 깊이 이해하고 신뢰성을 보장하며, 궁극적으로 더 안전하고 유용한 인공지능 시스템을 만드는 데 기여해야 할 것이다.</p>
<h2>5. 참고 자료</h2>
<ol>
<li>Improving Multiple Object Tracking With Single Object Tracking - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Improving_Multiple_Object_Tracking_With_Single_Object_Tracking_CVPR_2021_paper.pdf</li>
<li>Multi-Object Tracking Model Based on Detection Tracking Paradigm in Panoramic Scenes, https://www.mdpi.com/2076-3417/14/10/4146</li>
<li>The Complete Guide to Object Tracking [+V7 Tutorial] - V7 Labs, https://www.v7labs.com/blog/object-tracking-guide</li>
<li>The Complete Guide to Object Tracking [Tutorial] - Encord, https://encord.com/blog/object-tracking-guide/</li>
<li>Performance Metrics for Evaluating Object and Human Detection and Tracking Systems - GovInfo, https://www.govinfo.gov/content/pkg/GOVPUB-C13-a52a74766ab7f0f45eaa6be1df5cb85e/pdf/GOVPUB-C13-a52a74766ab7f0f45eaa6be1df5cb85e.pdf</li>
<li>HOOT: Heavy Occlusions in Object Tracking Benchmark - CVF Open Access, https://openaccess.thecvf.com/content/WACV2023/papers/Sahin_HOOT_Heavy_Occlusions_in_Object_Tracking_Benchmark_WACV_2023_paper.pdf</li>
<li>MOT and SOT, https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/mot_and_sot.html</li>
<li>Object Tracking Benchmark - UC Merced, https://faculty.ucmerced.edu/mhyang/papers/pami15_tracking_benchmark.pdf</li>
<li>Performance Evaluation of Object Detection and Tracking Systems - ResearchGate, https://www.researchgate.net/publication/237749648_Performance_Evaluation_of_Object_Detection_and_Tracking_Systems</li>
<li>Efficient Single-Shot Multi-Object Tracking for Vehicles in Traffic Scenarios - MDPI, https://www.mdpi.com/1424-8220/21/19/6358</li>
<li>Performance Metrics Deep Dive - Ultralytics YOLO Docs, https://docs.ultralytics.com/guides/yolo-performance-metrics/</li>
<li>Object Detection: Key Metrics for Computer Vision Performance - Label Your Data, https://labelyourdata.com/articles/object-detection-metrics</li>
<li>Generalized Intersection over Union, https://giou.stanford.edu/</li>
<li>Jaccard index, https://en.wikipedia.org/wiki/Jaccard_index</li>
<li>Understanding Intersection over Union for Model Accuracy - Viso Suite, https://viso.ai/computer-vision/intersection-over-union-iou/</li>
<li>Intersection over Union (IoU): Definition, Calculation, Code - V7 Labs, https://www.v7labs.com/blog/intersection-over-union-guide</li>
<li>The Complete Guide to Object Detection Evaluation Metrics: From IoU to mAP and More | by Prathamesh Amrutkar | Medium, https://medium.com/@prathameshamrutkar3/the-complete-guide-to-object-detection-evaluation-metrics-from-iou-to-map-and-more-1a23c0ea3c9d</li>
<li>Key Object Detection Metrics for Computer Vision - Roboflow Blog, https://blog.roboflow.com/object-detection-metrics/</li>
<li>Measuring Success: Comprehensive Evaluation Metrics for Object Detection Models - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2024/02/evaluation-matrix-for-object-detection-using-iou-and-map/</li>
<li>Visual object tracking challenges revisited: VOT vs. OTB - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC6160004/</li>
<li>(PDF) Visual Object Tracking Performance Measures Revisited - ResearchGate, https://www.researchgate.net/publication/272752538_Visual_Object_Tracking_Performance_Measures_Revisited</li>
<li>The Visual Object Tracking VOT2016 Challenge Results - Simple search, http://liu.diva-portal.org/smash/get/diva2:1063965/FULLTEXT02.pdf</li>
<li>The Tenth Visual Object Tracking VOT2022 Challenge Results - Tampere University Research Portal, https://researchportal.tuni.fi/files/83081924/Kristan2022The_1.pdf</li>
<li>Visual object tracking challenges revisited: VOT vs. OTB | PLOS One - Research journals, https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0203188</li>
<li>Trending Papers - Hugging Face, https://paperswithcode.com/dataset/otb</li>
<li>Machine Learning Datasets - Papers With Code, https://paperswithcode.com/datasets?mod=videos&amp;task=visual-object-tracking</li>
<li>BioDrone: A Bionic Drone-based Single Object Tracking Benchmark for Robust Vision - arXiv, https://arxiv.org/html/2402.04519v1</li>
<li>VOT Challenge: Computer Vision Competition - Communications of the ACM, https://cacm.acm.org/blogcacm/vot-challenge-computer-vision-competition/</li>
<li>l-lt/LaSOT · Datasets at Hugging Face, https://huggingface.co/datasets/l-lt/LaSOT</li>
<li>LaSOT - Large-scale Single Object Tracking, https://vision.cs.stonybrook.edu/~lasot/</li>
<li>LaSOT: A High-Quality Benchmark for Large-Scale Single Object Tracking - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2019/papers/Fan_LaSOT_A_High-Quality_Benchmark_for_Large-Scale_Single_Object_Tracking_CVPR_2019_paper.pdf</li>
<li>trackCLEARMetrics - CLEAR multi-object tracking metrics - MATLAB - MathWorks, https://www.mathworks.com/help/fusion/ref/trackclearmetrics.html</li>
<li>(PDF) Evaluating multiple object tracking performance: The CLEAR …, https://www.researchgate.net/publication/26523191_Evaluating_multiple_object_tracking_performance_The_CLEAR_MOT_metrics</li>
<li>Evaluating multiple object tracking accuracy and performance metrics in a real-time setting, https://visailabs.com/evaluating-multiple-object-tracking-accuracy-and-performance-metrics-in-a-real-time-setting/</li>
<li>Object Tracking and Reidentification with FairMOT - LearnOpenCV, https://learnopencv.com/object-tracking-and-reidentification-with-fairmot/</li>
<li>Evaluating multiple object tracking performance: the … - SciSpace, https://scispace.com/pdf/evaluating-multiple-object-tracking-performance-the-clear-tm8230vs80.pdf</li>
<li>CLEAR MOT Metrics — Stone Soup 1.5 documentation, https://stonesoup.readthedocs.io/en/v1.5/stonesoup.metricgenerator.clearmotmetrics.html</li>
<li>What is Multiple Object Tracking Precision (MOTP)? - Stack Overflow, https://stackoverflow.com/questions/62049456/what-is-multiple-object-tracking-precision-motp</li>
<li>Understanding Object Tracking Metrics - Miguel Mendez, https://miguel-mendez-ai.com/2024/08/25/mot-tracking-metrics</li>
<li>HOTA: A Higher Order Metric for Evaluating Multi-Object Tracking - Andreas Geiger, https://www.cvlibs.net/publications/Luiten2020IJCV.pdf</li>
<li>shenh10/mot_evaluation: A python implementation of … - GitHub, https://github.com/shenh10/mot_evaluation</li>
<li>HOTA: A Higher Order Metric for Evaluating Multi-object Tracking - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC7881978/</li>
<li>HOTA: A Higher Order Metric for Evaluating Multi-object Tracking - ResearchGate, https://www.researchgate.net/publication/345343240_HOTA_A_Higher_Order_Metric_for_Evaluating_Multi-object_Tracking</li>
<li>How to evaluate tracking with the HOTA metrics | by Jonathon Luiten | Medium, https://jonathonluiten.medium.com/how-to-evaluate-tracking-with-the-hota-metrics-754036d183e1</li>
<li>Benchmarking 2D Multi-Object Detection and Tracking Algorithms in Autonomous Vehicle Driving Scenarios - MDPI, https://www.mdpi.com/1424-8220/23/8/4024</li>
<li>A Literature Review of Performance Metrics of Automated Driving Systems for On-Road Vehicles - Frontiers, https://www.frontiersin.org/journals/future-transportation/articles/10.3389/ffutr.2021.759125/full</li>
<li>MOT Challenge, https://motchallenge.net/</li>
<li>[PDF] MOT16: A Benchmark for Multi-Object Tracking - Semantic Scholar, <a href="https://www.semanticscholar.org/paper/MOT16%3A-A-Benchmark-for-Multi-Object-Tracking-Milan-Leal-Taix%C3%A9/ac0d88ca5f75a4a80da90365c28fa26f1a26d4c4">https://www.semanticscholar.org/paper/MOT16%3A-A-Benchmark-for-Multi-Object-Tracking-Milan-Leal-Taix%C3%A9/ac0d88ca5f75a4a80da90365c28fa26f1a26d4c4</a></li>
<li>Frequently Asked Questions - MOT Challenge, https://motchallenge.net/faq/</li>
<li>JonathonLuiten/TrackEval: HOTA (and other) evaluation metrics for Multi-Object Tracking (MOT). - GitHub, https://github.com/JonathonLuiten/TrackEval</li>
<li>The KITTI Vision Benchmark Suite - Andreas Geiger, https://www.cvlibs.net/datasets/kitti/</li>
<li>AWS Marketplace: KITTI Vision Benchmark Suite - Amazon.com, https://aws.amazon.com/marketplace/pp/prodview-bybj3nqbbg4r6</li>
<li>Tracking Evaluation - The KITTI Vision Benchmark Suite, https://www.cvlibs.net/datasets/kitti/eval_tracking_overview.php</li>
<li>Object Tracking Evaluation (2D bounding-boxes) - The KITTI Vision Benchmark Suite, https://www.cvlibs.net/datasets/kitti/eval_tracking.php</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>