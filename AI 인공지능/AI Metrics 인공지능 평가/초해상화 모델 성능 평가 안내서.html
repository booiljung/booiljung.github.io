<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:초해상화 모델 성능 평가 안내서</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>초해상화 모델 성능 평가 안내서</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">인공지능 평가지표 (AI evaluation metrics)</a> / <span>초해상화 모델 성능 평가 안내서</span></nav>
                </div>
            </header>
            <article>
                <h1>초해상화 모델 성능 평가 안내서</h1>
<h2>1.  초해상화 성능 평가의 원칙과 과제</h2>
<h3>1.1 초해상화(Super-Resolution, SR) 문제의 본질</h3>
<p>초해상화(Super-Resolution, SR)는 저해상도(Low-Resolution, LR) 영상으로부터 고해상도(High-Resolution, HR) 영상을 복원하는 컴퓨터 비전 기술의 한 분야이다.1 이 기술의 핵심 목표는 단순히 이미지의 크기를 키우는 것을 넘어, 영상 획득 및 압축 과정에서 손실된 고주파수(high-frequency) 세부 정보를 지능적으로 추론하고 복원하는 데 있다.1 수학적 관점에서 SR은 본질적으로 비정형 문제(ill-posed problem)로 정의된다.2 이는 하나의 LR 입력에 대해 이론적으로 다수의 타당한 HR 결과가 존재할 수 있음을 의미하며, 이러한 본질적 불확실성은 SR 모델의 성능을 평가하는 과정을 매우 복잡하고 다차원적으로 만드는 근본적인 원인이 된다.</p>
<h3>1.2 성능 평가의 두 가지 상충하는 목표: 충실도(Fidelity)와 인지 품질(Perceptual Quality)</h3>
<p>SR 모델의 성능을 평가하는 기준은 크게 두 가지 상충하는 목표로 나뉜다. 첫째는 **충실도(Fidelity)**로, 복원된 HR 영상이 원본(Ground Truth, GT) 영상과 픽셀 단위에서 얼마나 수학적으로 정확하게 일치하는지를 측정한다.5 이는 신호 처리 관점에서 왜곡이 얼마나 적은지를 나타내는 척도이다. 둘째는 **인지 품질(Perceptual Quality)**로, 복원된 영상이 인간의 시각 시스템(Human Visual System, HVS)에 얼마나 자연스럽고 사실적으로 보이는지를 평가하는 기준이다.6</p>
<p>이 두 목표는 ’인지-왜곡 상충 관계(Perception-Distortion Tradeoff)’라는 고질적인 딜레마를 형성한다.5 즉, 픽셀 단위의 오차를 최소화하여 충실도를 높이면(예: PSNR 점수 향상) 결과물이 종종 흐릿해져 인지 품질이 저하되고, 반대로 인간이 보기에 좋은 이미지를 생성하면(예: GAN 기반 모델) 원본 GT와 픽셀 단위의 차이는 커져 충실도가 낮아지는 경향이 있다. SR 평가 방법론의 발전 과정은 이 근본적인 긴장 관계를 이해하고 해결하려는 노력의 역사와 같다. 초기에는 수학적 ’정확성’을 대변하는 충실도 지표가 주를 이루었으나 9, 이러한 지표들이 인간의 시각적 만족도를 반영하지 못한다는 한계가 명확해지면서 10, 인간의 ‘현실감’ 인지를 모델링하려는 인지 품질 지표와 생성 모델이 평가의 중요한 축으로 자리 잡게 되었다.9</p>
<h3>1.3 평가 지표의 분류</h3>
<p>이러한 배경 속에서 SR 성능 평가 지표는 다음과 같이 분류할 수 있다.</p>
<ul>
<li>
<p><strong>정량적 평가(Quantitative Evaluation) vs. 정성적 평가(Qualitative Evaluation):</strong> 정량적 평가는 PSNR, SSIM과 같은 수치적 지표를 사용하여 모델 성능을 객관적으로 측정하는 방식이다. 반면, 정성적 평가는 인간 평가자가 직접 시각적으로 결과물의 품질, 아티팩트 유무 등을 판단하는 주관적 방식이다.12</p>
</li>
<li>
<p><strong>참조 기반(Full-Reference, FR) vs. 무참조 기반(No-Reference, NR):</strong> FR 지표는 평가 시 원본 GT 영상을 참조하여 복원된 영상과의 차이를 측정한다.14 반면, NR 지표는 GT 영상 없이 복원된 결과물 자체의 통계적 특성이나 학습된 모델을 통해 품질을 예측한다.15</p>
</li>
</ul>
<table><thead><tr><th>지표 (Metric)</th><th>유형 (Type)</th><th>핵심 원리 (Core Principle)</th><th>주요 장점 (Advantage)</th><th>치명적 한계 (Limitation)</th></tr></thead><tbody>
<tr><td><strong>PSNR</strong></td><td>Full-Reference</td><td>픽셀 단위 오차(MSE)의 로그 스케일 변환</td><td>계산이 간단하고 빠름. 신호 왜곡 측정에 용이.6</td><td>인간의 시각적 인지와 상관관계가 매우 낮음.9</td></tr>
<tr><td><strong>SSIM</strong></td><td>Full-Reference</td><td>휘도, 대비, 구조적 정보 비교</td><td>PSNR보다 인간의 인지와 더 잘 부합함. 구조적 왜곡 포착.6</td><td>색상 왜곡에 둔감하며, 블러링된 이미지에 높은 점수를 줄 수 있음.19</td></tr>
<tr><td><strong>LPIPS</strong></td><td>Full-Reference</td><td>딥러닝 모델의 특징 공간(feature space)에서 패치 간 거리 측정</td><td>인간의 인지적 유사성과 매우 높은 상관관계를 보임.11</td><td>사전 학습된 네트워크가 필요하며, 계산 비용이 높음.12</td></tr>
<tr><td><strong>NIQE</strong></td><td>No-Reference</td><td>자연 영상의 통계적 규칙성(NSS)과의 편차 측정</td><td>원본(GT) 영상 없이 품질 평가 가능. “완전한 블라인드” 평가.16</td><td>학습 데이터에 의존적이며, 인간의 주관적 선호도와 상관관계가 낮을 수 있음.15</td></tr>
</tbody></table>
<h2>2.  정량적 성능 평가: 참조 기반 지표</h2>
<p>참조 기반(Full-Reference) 지표는 원본 고해상도(GT) 영상이 존재할 때, SR 모델이 생성한 영상과 GT를 직접 비교하여 품질을 측정한다. 이는 SR 모델의 ‘복원’ 능력을 평가하는 가장 직접적인 방법이다.</p>
<h3>2.1  픽셀 기반 충실도 지표</h3>
<p>이 지표들은 이미지의 시각적 구조나 의미를 고려하지 않고, 오직 픽셀 값의 수학적 차이에만 집중하여 신호의 충실도를 평가한다.</p>
<h4>2.1.1  최대 신호 대 잡음비 (Peak Signal-to-Noise Ratio, PSNR)</h4>
<ul>
<li>
<p><strong>원리:</strong> PSNR은 영상 품질 평가에서 가장 전통적이고 널리 사용되는 지표로, 원본 영상과 복원된 영상 간의 평균 제곱 오차(Mean Squared Error, MSE)에 기반한다.24 MSE는 같은 위치에 있는 픽셀 값들의 차이를 제곱하여 평균 낸 값으로, 두 영상 간의 평균적인 오차 에너지를 나타낸다.26 PSNR은 신호가 가질 수 있는 최대 에너지(최대 픽셀 값)와 이 오차 에너지(MSE) 간의 비율을 로그 스케일로 변환하여 데시벨(dB) 단위로 표현한 것이다.24</p>
</li>
<li>
<p>수학적 정의:</p>
</li>
</ul>
<p><span class="math math-inline">m \times n</span> 크기의 원본 흑백 영상 <span class="math math-inline">I</span>와 복원 영상 <span class="math math-inline">K</span>에 대한 MSE는 다음과 같이 정의된다.</p>
<p><span class="math math-display">
  MSE = \frac{1}{mn} \sum_{i=0}^{m-1} \sum_{j=0}^{n-1} [I(i,j) - K(i,j)]^2
</span><br />
이를 기반으로 한 PSNR(dB)의 정의는 다음과 같다.</p>
<p><span class="math math-display">
  PSNR = 10 \cdot \log_{10} \left( \frac{MAX_I^2}{MSE} \right) = 20 \cdot \log_{10} \left( \frac{MAX_I}{\sqrt{MSE}} \right)
</span><br />
여기서 <span class="math math-inline">MAX_I</span>는 영상의 최대 픽셀 값으로, 8비트 영상의 경우 255가 된다.24</p>
<ul>
<li><strong>해석, 장점 및 한계:</strong> PSNR 값이 높을수록 MSE가 낮다는 의미이며, 이는 원본 대비 왜곡이 적음을 나타낸다. 일반적으로 손실 압축 영상에서 30-50dB 사이의 값을 가지며, 높을수록 품질이 좋다고 간주된다.6 PSNR의 가장 큰 장점은 계산이 매우 간단하고 빨라 실시간 적용이 용이하며, 오랫동안 표준처럼 사용되어 과거 연구들과의 성능 비교에 편리하다는 점이다.6</li>
</ul>
<p>그러나 PSNR의 치명적인 한계는 인간의 시각적 인지와 상관관계가 매우 낮다는 점이다.7 예를 들어, 약간의 블러가 적용된 이미지와 JPEG 압축 아티팩트가 심한 이미지가 인간에게는 전혀 다른 품질로 인식되지만, 동일한 PSNR 값을 가질 수 있다.10 특히, MSE와 같은 픽셀 단위 손실 함수로 학습된 SR 모델은 PSNR 점수를 높이기 위해 불확실한 텍스처 영역을 평균값에 가깝게 예측하는 경향이 있으며, 이는 시각적으로 흐릿하고 세부 묘사가 부족한 과도하게 평활화(over-smoothing)된 결과를 초래한다.9</p>
<h4>2.1.2  구조적 유사도 (Structural Similarity Index Measure, SSIM)</h4>
<ul>
<li>
<p><strong>원리:</strong> SSIM은 PSNR의 한계를 극복하기 위해 제안된 지표로, 인간의 시각 시스템이 픽셀 값의 절대적인 차이보다는 이미지 내 객체의 구조적 정보 변화에 더 민감하게 반응한다는 점에 착안했다.26 SSIM은 두 이미지 패치를 비교할 때 휘도(luminance), 대비(contrast), 구조(structure)라는 세 가지 요소를 독립적으로 측정하고, 이를 종합하여 최종 유사도 점수를 계산한다.27</p>
</li>
<li>
<p>수학적 정의:</p>
</li>
</ul>
<p>두 이미지 패치 <span class="math math-inline">x</span>와 <span class="math math-inline">y</span>에 대해 휘도, 대비, 구조 비교 함수는 각각 다음과 같이 정의된다.</p>
<p><span class="math math-display">
  l(x, y) = \frac{2\mu_x\mu_y + C_1}{\mu_x^2 + \mu_y^2 + C_1}
</span></p>
<p><span class="math math-display">
  c(x, y) = \frac{2\sigma_x\sigma_y + C_2}{\sigma_x^2 + \sigma_y^2 + C_2}
</span></p>
<p><span class="math math-display">
  s(x, y) = \frac{\sigma_{xy} + C_3}{\sigma_x\sigma_y + C_3}
</span></p>
<p>여기서 <span class="math math-inline">\mu</span>는 평균, <span class="math math-inline">\sigma^2</span>는 분산, <span class="math math-inline">\sigma_{xy}</span>는 공분산이며, <span class="math math-inline">C_1, C_2, C_3</span>는 분모가 0이 되는 것을 방지하기 위한 안정화 상수이다.29 최종 SSIM 지수는 이 세 요소를 결합하여 계산하며, 일반적으로 지수는 1로 설정하여 단순화된 형태를 사용한다.</p>
<p><span class="math math-display">
  SSIM(x, y) = l(x, y) \cdot c(x, y) \cdot s(x, y)
</span></p>
<ul>
<li><strong>해석, 장점 및 한계:</strong> SSIM 값은 -1에서 1 사이의 범위를 가지며, 1에 가까울수록 두 이미지가 구조적으로 완벽하게 일치함을 의미한다.29 SSIM은 단순 픽셀 오차를 넘어 이미지의 구조적 왜곡을 포착하므로 PSNR보다 인간의 인지적 품질 평가와 더 높은 상관관계를 보인다.6</li>
</ul>
<p>하지만 SSIM 역시 한계를 가진다. 휘도, 대비, 구조 정보만을 사용하기 때문에 색상 왜곡이나 톤 변화에는 둔감하게 반응한다.20 또한, 이미지가 전반적으로 블러 처리되거나 특정 영역이 평균값으로 채워지더라도 픽셀 값의 분포와 구조는 유사하게 유지될 수 있어, 선명도가 떨어짐에도 불구하고 SSIM 점수가 높게 측정될 수 있다.19</p>
<h3>2.2  인지 기반 지표</h3>
<p>인지 기반 지표는 인간의 시각 시스템을 보다 정교하게 모방하려는 시도에서 출발했다. 특히 딥러닝 기술의 발전은 픽셀 공간을 넘어, 인간의 뇌가 정보를 처리하는 방식과 유사한 특징 공간(feature space)에서 이미지 유사도를 측정하는 새로운 패러다임을 열었다.</p>
<h4>2.2.1  학습된 인지적 이미지 패치 유사도 (Learned Perceptual Image Patch Similarity, LPIPS)</h4>
<ul>
<li>
<p><strong>원리:</strong> LPIPS는 전통적인 지표들이 인간의 복잡한 시각 인지 과정을 제대로 모델링하지 못한다는 문제의식에서 개발되었다.11 이 지표의 핵심 아이디어는 이미지 분류와 같은 고차원적인 작업을 위해 사전 학습된 심층 신경망(e.g., AlexNet, VGG)이 이미 인간의 시각 인지와 유사한 특징들을 내부적으로 학습했다는 것이다.22 따라서 LPIPS는 두 이미지를 이 네트워크에 통과시킨 후, 여러 계층에서 추출된 특징 맵(feature map)의 활성화 값들 간의 거리를 측정한다.21 이는 픽셀 공간에서의 비교가 아닌, 보다 추상적이고 의미론적인 특징 공간에서의 유사도를 계산하는 방식으로, 인간의 인지적 판단과 매우 높은 상관관계를 보인다.</p>
</li>
<li>
<p>작동 방식 및 수학적 표현:</p>
</li>
</ul>
<p>두 이미지 <span class="math math-inline">x</span>, <span class="math math-inline">x_0</span>를 특징 추출 네트워크 <span class="math math-inline">F</span>에 입력하여 <span class="math math-inline">L</span>개의 계층에서 특징 맵 <span class="math math-inline">\hat{y}^l, \hat{y}_0^l</span>을 추출한다. 각 계층의 특징 맵은 채널 차원에서 정규화된 후, 학습된 가중치 벡터 <span class="math math-inline">w^l</span>와 곱해진다. 최종적으로 모든 계층에서 계산된 L2 거리의 가중 합을 통해 LPIPS 점수를 얻는다.22</p>
<p><span class="math math-display">
  d(x, x_0) = \sum_l \frac{1}{H_l W_l} \sum_{h,w} \Vert w^l \odot (\hat{y}_{hw}^l - \hat{y}_{0,hw}^l) \Vert_2^2
</span><br />
여기서 <span class="math math-inline">l</span>은 계층, <span class="math math-inline">h, w</span>는 공간 차원, <span class="math math-inline">\odot</span>는 원소별 곱셈을 의미한다.</p>
<ul>
<li><strong>해석, 장점 및 한계:</strong> LPIPS 점수는 두 이미지 간의 인지적 ’거리’를 나타내므로, <strong>값이 낮을수록</strong> 두 이미지가 시각적으로 더 유사함을 의미한다.21 LPIPS는 PSNR이나 SSIM이 놓치기 쉬운 미묘한 텍스처, 스타일, 왜곡 등을 효과적으로 포착하여 인간의 주관적 평가와 높은 일관성을 보인다.11</li>
</ul>
<p>그러나 LPIPS는 대규모 데이터셋으로 사전 학습된 심층 신경망을 필요로 하므로 계산 비용이 높다는 단점이 있다.12 또한, 기반이 되는 신경망 자체가 적대적 공격(adversarial attacks)에 취약하기 때문에, 인간이 인지할 수 없는 미세한 노이즈를 추가하여 LPIPS 점수를 의도적으로 조작할 수 있는 가능성이 제기되었다.33</p>
<h2>3.  정량적 성능 평가: 무참조 지표</h2>
<p>실제 응용 환경에서는 원본 고해상도(GT) 영상을 사용할 수 없는 경우가 많다. 예를 들어, 오래된 사진을 복원하거나 CCTV 영상을 분석할 때, 비교할 대상인 ’완벽한 원본’은 존재하지 않는다. 이러한 시나리오를 위해 무참조(No-Reference) 지표가 개발되었다. 이 지표들은 SR 문제의 패러다임을 ’정확한 복원’에서 ’그럴듯한 생성’으로 확장하는 데 결정적인 역할을 한다.</p>
<h3>3.1 자연스러움 이미지 품질 평가 (Naturalness Image Quality Evaluator, NIQE)</h3>
<ul>
<li>
<p><strong>원리:</strong> NIQE는 “시각적으로 우수한 품질의 이미지는 왜곡되지 않은 자연 영상(natural scene)이 갖는 통계적 규칙성을 따른다“는 강력한 가정에 기반한다.15 이 지표는 인간의 주관적 평가 점수나 왜곡된 이미지 예시를 전혀 사용하지 않고, 오직 자연 영상의 통계 모델로부터 평가 대상 이미지가 얼마나 벗어났는지를 측정한다. 이 때문에 ‘완전한 블라인드(completely blind)’ 평가 방식으로 불린다.16</p>
</li>
<li>
<p><strong>작동 방식 및 수학적 표현:</strong></p>
</li>
</ul>
<ol>
<li>
<p><strong>자연 영상 통계(NSS) 모델 구축:</strong> 먼저, 수많은 고품질 자연 영상 데이터베이스로부터 지역적으로 정규화된 휘도 계수와 같은 자연 영상 통계(Natural Scene Statistics, NSS) 특징들을 추출한다. 이 특징들의 분포는 다변량 가우시안(Multivariate Gaussian, MVG) 모델로 근사하여 평균 벡터(<span class="math math-inline">\nu_1</span>)와 공분산 행렬(<span class="math math-inline">\Sigma_1</span>)을 계산한다.16</p>
</li>
<li>
<p><strong>품질 점수 계산:</strong> 평가하고자 하는 이미지에 대해서도 동일한 NSS 특징을 추출하고, 이를 MVG 모델(<span class="math math-inline">\nu_2</span>, <span class="math math-inline">\Sigma_2</span>)로 피팅한다. 최종 NIQE 점수는 이 두 MVG 모델 간의 통계적 거리로 정의된다.16</p>
</li>
</ol>
<p><span class="math math-display">
  D(\nu_1, \nu_2, \Sigma_1, \Sigma_2) = \sqrt{ (\nu_1 - \nu_2)^T \left( \frac{\Sigma_1 + \Sigma_2}{2} \right)^{-1} (\nu_1 - \nu_2) }
</span></p>
<p>이 수식은 두 가우시안 분포 간의 거리를 나타내며, 값이 작을수록 평가 이미지가 자연 영상의 통계적 특성과 유사함을 의미한다.16</p>
<ul>
<li><strong>해석, 장점 및 한계:</strong> NIQE 점수는 ’거리’를 의미하므로, <strong>값이 낮을수록</strong> 이미지가 더 자연스럽고 인지적 품질이 높다고 판단한다.15 NIQE의 가장 큰 장점은 GT 영상 없이도 이미지 품질을 평가할 수 있어, 생성 모델의 결과물이나 실제 환경에서 획득한 영상 평가에 매우 유용하다는 점이다.23 또한, 특정 유형의 왜곡에 국한되지 않고 다양한 종류의 왜곡을 포괄적으로 평가할 수 있다.16</li>
</ul>
<p>그러나 NIQE의 성능은 기준이 되는 자연 영상 데이터베이스의 품질과 특성에 크게 의존한다는 한계가 있다.15 만약 평가하려는 이미지가 의료, 위성, 예술 작품 등 일반적인 자연 영상과 통계적 특성이 다른 도메인에 속한다면, 기본 모델의 신뢰도가 떨어질 수 있다. 이런 경우, 해당 도메인의 고품질 이미지로 NIQE 모델을 재학습(custom training)하여 사용해야 한다.35</p>
<h2>4.  정성적 성능 평가와 아티팩트 분석</h2>
<p>정량적 지표는 모델 성능을 간결한 숫자로 요약해주지만, 그 숫자가 시각적 품질의 모든 측면을 대변하지는 못한다. 특히 SR 모델이 만들어내는 미묘한 시각적 결함, 즉 아티팩트(artifact)를 탐지하고 분석하기 위해서는 인간의 직접적인 시각 평가, 즉 정성적 평가가 필수적이다.</p>
<h3>4.1  주관적 평가의 중요성</h3>
<p>아무리 정교한 정량적 지표라도 인간의 복잡한 시각 인지 시스템을 완벽하게 대체할 수는 없다. 따라서 다수의 평가자가 직접 결과물의 품질을 평가하는 주관적 평가는 여전히 SR 모델의 인지 품질을 측정하는 ’황금 표준(gold standard)’으로 여겨진다.7 가장 대표적인 방법은 **평균 의견 점수(Mean Opinion Score, MOS)**로, 여러 평가자가 주어진 척도(예: 1점 ‘나쁨’ ~ 5점 ‘우수’)에 따라 이미지 품질을 평가하고 그 점수들의 평균을 최종 점수로 사용한다.9 MOS는 모델의 최종적인 시각적 만족도를 가장 직접적으로 반영하지만, 많은 인원과 시간이 소요되어 비용이 높다는 단점이 있다.7</p>
<h3>4.2  초해상화 결과물의 주요 시각적 아티팩트</h3>
<p>정성적 평가는 단순히 ’좋다/나쁘다’를 판단하는 것을 넘어, 모델이 어떤 유형의 오류를 범하는지 분석하는 데 중요한 역할을 한다. 아티팩트의 유형을 분석하면 모델의 구조적 특성과 학습 목표의 한계를 역으로 추론할 수 있다.</p>
<table><thead><tr><th>아티팩트 유형</th><th>시각적 특징</th><th>주요 발생 원인</th><th>관련 모델/기술</th></tr></thead><tbody>
<tr><td><strong>블러링 (Blurring)</strong></td><td>이미지의 디테일이 소실되고 전반적으로 흐릿함.</td><td>픽셀 단위 손실(MSE/L1) 최적화로 인한 과도한 평활화 (Regression-to-the-mean).37</td><td>PSNR 최적화 모델 (e.g., SRCNN, EDSR)</td></tr>
<tr><td><strong>링잉 (Ringing)</strong></td><td>날카로운 경계 주변에 파동 또는 후광 형태의 왜곡 발생.</td><td>업샘플링 필터의 부적절한 설계 또는 고주파수 성분의 급격한 차단 (깁스 현상).38</td><td>전통적인 보간법, 초기 CNN 모델</td></tr>
<tr><td><strong>텍스처 환각 (Hallucination)</strong></td><td>원본에 없거나 사실과 다른, 비현실적인 텍스처 생성.</td><td>생성적 적대 신경망(GAN)이 학습 데이터 분포에 기반하여 디테일을 ’상상’하여 생성.40</td><td>GAN 기반 모델 (e.g., SRGAN, ESRGAN)</td></tr>
<tr><td><strong>체커보드 (Checkerboard)</strong></td><td>이미지 전반에 미세한 격자무늬 패턴이 나타남.</td><td>전치 컨볼루션(Transposed Convolution) 레이어의 커널 오버랩 문제.9</td><td>Deconvolution을 사용하는 초기 생성 모델</td></tr>
</tbody></table>
<ul>
<li>
<p><strong>1. 블러링(Blurring):</strong> 이미지의 날카로운 모서리나 미세한 텍스처가 뭉개져 전반적으로 흐릿하게 보이는 현상이다. 이는 주로 PSNR과 같은 충실도 지표에 최적화된 모델에서 발생한다.9 모델이 LR 이미지에 없는 정보를 추론해야 할 때, 틀릴 위험이 있는 날카로운 디테일을 생성하기보다 가능한 모든 정답들의 평균에 가까운 ‘안전한’ 픽셀 값을 예측하기 때문에 발생한다. 이러한 현상을 ’평균으로의 회귀(regression-to-the-mean)’라고 한다.37</p>
</li>
<li>
<p><strong>2. 링잉(Ringing) 및 깁스 현상(Gibbs Phenomenon):</strong> 이미지의 강한 경계(edge) 주변에 물결이나 후광(halo)처럼 보이는 패턴이 나타나는 현상이다.38 이는 신호 처리에서 유한한 주파수 샘플로 불연속적인 신호를 복원하려 할 때 발생하는 깁스 현상과 원리가 같다.38 SR에서는 업샘플링 과정이나 네트워크 필터가 경계를 과도하게 강조하려 할 때 발생할 수 있다.42</p>
</li>
<li>
<p><strong>3. 텍스처 환각(Texture Hallucination):</strong> 모델이 원본 LR 이미지에는 존재하지 않거나, GT와는 전혀 다른 새로운 텍스처를 만들어내는 현상이다.37 이는 주로 GAN 기반 모델에서 두드러지게 나타난다. GAN은 결과물이 ‘실제 이미지처럼 보이도록’ 학습되기 때문에, 픽셀 단위의 정확성보다는 학습 데이터의 통계적 분포에 기반하여 가장 그럴듯한 텍스처를 ’상상’하여 채워 넣는다.41 이로 인해 때로는 매우 사실적으로 보이지만 실제와는 다른 디테일(예: 돌 표면에 동물의 털 무늬를 생성)이 만들어지는 심각한 오류를 낳을 수 있다.37</p>
</li>
<li>
<p><strong>4. 기타 아티팩트:</strong> 이 외에도 특정 업샘플링 방식(전치 컨볼루션)에서 발생하는 <strong>체커보드 패턴(Checkerboard Artifacts)</strong> 9이나, 입력 LR 이미지가 이미 손실 압축(예: JPEG)된 경우 발생하는 블록 현상 등이 SR 과정에서 증폭되는</p>
</li>
</ul>
<p><strong>압축 아티팩트 증폭(Compression Artifact Amplification)</strong> 45 등이 있다.</p>
<p>이처럼 아티팩트는 단순한 오류가 아니라, 모델의 아키텍처, 학습 목표, 학습 데이터의 특성이 남긴 ’지문’과 같다. 따라서 정성적 평가를 통해 어떤 종류의 아티팩트가 주로 발생하는지 분석하면, 해당 모델의 강점과 약점을 깊이 있게 이해하고 개선 방향을 설정하는 데 중요한 단서를 얻을 수 있다.</p>
<h2>5.  표준 평가 프로토콜 및 벤치마크</h2>
<p>SR 모델의 성능을 공정하고 재현 가능하게 비교하기 위해서는 표준화된 평가 절차와 벤치마크 데이터셋이 필수적이다. 이는 연구 커뮤니티가 공통의 기준으로 모델의 발전을 측정하고 검증할 수 있는 기반을 제공한다.</p>
<h3>5.1  표준 벤치마크 데이터셋</h3>
<p>SR 연구에서는 주로 대규모 데이터셋으로 모델을 학습시킨 후, 널리 알려진 소규모 테스트 데이터셋으로 성능을 평가하는 방식을 따른다.</p>
<table><thead><tr><th>데이터셋</th><th>이미지 수</th><th>해상도</th><th>주요 이미지 콘텐츠</th><th>주 사용 목적</th></tr></thead><tbody>
<tr><td><strong>DIV2K</strong></td><td>800 (Train) / 100 (Val)</td><td>2K</td><td>다양한 고품질 자연 및 인공물 이미지</td><td>모델 <strong>학습</strong> 및 검증 46</td></tr>
<tr><td><strong>Set5</strong></td><td>5</td><td>다양함</td><td>아기, 새, 나비 등 일반적인 객체</td><td>표준 <strong>성능 평가</strong> (가장 기본적인 벤치마크) 48</td></tr>
<tr><td><strong>Set14</strong></td><td>14</td><td>다양함</td><td>Set5보다 다양한 자연 및 인공물 이미지</td><td>표준 <strong>성능 평가</strong> (Set5보다 약간 더 도전적) 49</td></tr>
<tr><td><strong>BSD100</strong></td><td>100</td><td>~481x321</td><td>동물, 사람, 풍경 등 복잡한 자연 이미지</td><td><strong>텍스처 복원</strong> 및 일반화 성능 평가 52</td></tr>
<tr><td><strong>Urban100</strong></td><td>100</td><td>~1024x768</td><td>도시 건축물, 거리 풍경 등 인공 구조물</td><td><strong>인공 구조물</strong>의 직선, 반복 패턴 등 고주파 디테일 복원 능력 평가 55</td></tr>
</tbody></table>
<ul>
<li>
<p><strong>학습용 데이터셋:</strong></p>
</li>
<li>
<p><strong>DIV2K:</strong> NTIRE SR 챌린지를 위해 구축된 데이터셋으로, 800개의 훈련용, 100개의 검증용 고품질 2K 해상도 이미지로 구성되어 있다.9 현재 대부분의 최신 SR 모델들이 이 데이터셋을 표준 학습 데이터로 사용하고 있다.58</p>
</li>
<li>
<p><strong>평가용 데이터셋:</strong></p>
</li>
<li>
<p><strong>Set5 &amp; Set14:</strong> SR 연구 초창기부터 널리 사용된 클래식 벤치마크로, 각각 5개와 14개의 다양한 이미지로 구성된다. 모델의 기본적인 성능을 빠르게 확인하는 데 사용된다.48</p>
</li>
<li>
<p><strong>BSD100:</strong> Berkeley Segmentation Dataset에서 파생된 100개의 자연 이미지로 구성되어 있으며, Set5/14보다 복잡하고 다양한 텍스처를 포함하고 있어 모델의 일반화 성능을 평가하는 데 적합하다.52</p>
</li>
<li>
<p><strong>Urban100:</strong> 100장의 도시 풍경 이미지로 구성되어 있다. 이 데이터셋은 건물, 다리 등 인공 구조물이 갖는 직선, 격자, 반복 패턴과 같은 고주파수 디테일을 얼마나 잘 복원하는지 평가하는 데 특화되어 있다.60</p>
</li>
</ul>
<h3>5.2  영상 열화 모델과 Bicubic Downsampling의 역할</h3>
<p>공정한 비교를 위해서는 LR 이미지를 생성하는 열화(degradation) 과정이 일관되어야 한다.62 이 표준화된 열화 과정을 통해 생성된 LR-HR 이미지 쌍은 모델 학습과 평가의 기반이 된다.</p>
<ul>
<li>
<p><strong>Bicubic Downsampling:</strong> SR 벤치마크에서 가장 보편적으로 사용되는 표준 열화 방법은 <strong>Bicubic Downsampling</strong>이다.63 이는 고해상도 원본 이미지를 Bicubic 보간법을 사용하여 특정 배율(예: x2, x3, x4)로 축소하는 방식이다.65 이 방법은 구현이 간단하고 결과가 결정론적이어서, 모든 연구자가 동일한 조건에서 LR 이미지를 생성하고 모델을 평가할 수 있는 공정한 환경을 제공한다.8</p>
</li>
<li>
<p><strong>한계와 실제 환경과의 차이 (Domain Gap):</strong> 표준 평가 프로토콜의 성공은 역설적으로 그 한계를 만들었다. 대부분의 SR 모델들은 이 ‘Bicubic 세계’ 안에서 Bicubic 연산을 되돌리는 역함수를 학습하는 데 특화되었다.63 그러나 실제 세상에서 얻어지는 LR 이미지는 단순한 Bicubic 축소뿐만 아니라, 카메라 렌즈의 왜곡, 센서 노이즈, 촬영 시의 흔들림, JPEG 압축으로 인한 아티팩트 등 훨씬 복잡하고 예측 불가능한 열화 과정을 거친다.64 이로 인해 Bicubic 데이터셋에서 높은 점수를 받은 모델이 실제 사진에 적용되었을 때 기대 이하의 성능을 보이는 ‘도메인 갭’ 문제가 발생한다.63 이러한 간극을 메우기 위해, 실제 열화 과정을 더 현실적으로 모델링하고 평가하려는 **실제 초해상화(Real-World Super-Resolution, Real-SR)**라는 새로운 연구 분야가 활발히 진행되고 있다.69</p>
</li>
</ul>
<h2>6.  종합 분석 및 심층 고찰</h2>
<p>초해상화 모델의 성능을 올바르게 평가하기 위해서는 개별 지표의 수치를 나열하는 것을 넘어, 지표들 간의 관계를 이해하고, 평가 결과가 특정 응용 분야에서 어떤 의미를 갖는지 심층적으로 분석해야 한다.</p>
<h3>6.1  인지-왜곡 상충 관계 (Perception-Distortion Tradeoff)</h3>
<p>이 상충 관계는 SR 평가의 핵심 딜레마로, 충실도(낮은 왜곡)와 인지 품질(높은 현실감)을 동시에 극대화할 수 없는 현상을 의미한다.5</p>
<ul>
<li>
<p><strong>원인 분석:</strong></p>
</li>
<li>
<p><strong>충실도 최적화:</strong> PSNR과 같은 충실도 지표를 높이는 것은 MSE 손실 함수를 최소화하는 것과 같다. MSE는 모든 픽셀의 오차 제곱 합을 줄이는 것을 목표로 하므로, 모델은 불확실한 고주파수 텍스처를 복원하려 시도하기보다 모든 가능한 정답들의 ’평균’에 가까운 값을 예측하는 것이 손실을 줄이는 데 유리하다. 이 과정이 바로 결과물을 흐릿하게 만드는 블러링의 원인이다.70</p>
</li>
<li>
<p><strong>인지 품질 최적화:</strong> 반면, GAN 기반 모델은 생성자가 판별자를 속이도록 하는 적대적 손실(adversarial loss)을 사용한다. 이는 결과물이 픽셀 단위로 GT와 정확히 일치하는지가 아니라, ’실제 이미지 데이터의 통계적 분포’에 속하는지를 기준으로 학습됨을 의미한다. 따라서 모델은 GT와는 다르더라도 통계적으로 그럴듯한 텍스처를 생성(환각)하여 인지 품질을 높인다.40</p>
</li>
<li>
<p><strong>GAN 기반 모델 평가의 어려움:</strong> 이러한 학습 방식의 차이로 인해, GAN 기반 모델은 시각적으로 매우 우수함에도 불구하고 PSNR/SSIM 점수는 현저히 낮은 경우가 많다.72 이는 충실도 지표가 GAN 모델의 최적화 목표와 근본적으로 다르기 때문이다. 따라서 GAN 모델 평가는 LPIPS와 같은 인지 지표나 MOS와 같은 주관적 평가에 더 크게 의존해야 하지만, 이러한 지표들조차 텍스처 환각과 같은 문제를 완벽하게 정량화하지는 못하는 한계가 있다.</p>
</li>
</ul>
<h3>6.2  상충되는 지표 결과의 해석</h3>
<p>단일 지표만으로는 모델을 온전히 평가할 수 없으므로, 여러 지표를 종합적으로 해석하는 능력이 중요하다.74 특히 충실도 지표와 인지 지표가 상반된 결과를 보일 때, 그 의미를 정확히 파악해야 한다.</p>
<ul>
<li>
<p><strong>시나리오 1: 높은 PSNR, 높은 LPIPS (낮은 인지 품질):</strong></p>
</li>
<li>
<p><strong>의미:</strong> 복원된 이미지가 원본 GT와 픽셀 단위로는 매우 유사하지만(높은 PSNR), 인간이 보기에는 흐릿하거나 어색하다는 것을 의미한다(LPIPS는 점수가 높을수록 인지적 거리가 멀어 품질이 낮음).8</p>
</li>
<li>
<p><strong>해석:</strong> 이는 모델이 충실도에 과도하게 최적화되어 인지 품질을 희생한 전형적인 ’인지-왜곡 상충 관계’의 사례다. 이러한 모델은 픽셀 값의 정확성이 중요한 과학 연구나 의료 영상 분석 등에는 유용할 수 있으나, 일반적인 미디어 소비 환경에서는 시각적 만족도가 떨어진다.</p>
</li>
<li>
<p><strong>시나리오 2: 낮은 PSNR, 낮은 LPIPS (높은 인지 품질):</strong></p>
</li>
<li>
<p><strong>의미:</strong> 복원된 이미지가 픽셀 단위의 오차는 크지만(낮은 PSNR), 인간이 보기에는 매우 선명하고 사실적이라는 것을 의미한다(LPIPS는 점수가 낮을수록 인지적 거리가 가까워 품질이 높음).8</p>
</li>
<li>
<p><strong>해석:</strong> 모델이 인지 품질 향상에 성공적으로 최적화된 경우로, 주로 GAN 기반 모델에서 나타나는 결과다. 생성된 텍스처가 GT와는 다를 수 있지만, 시각적으로 매우 그럴듯하여 만족도가 높다. 오래된 사진 복원이나 저화질 비디오 업스케일링 등 시각적 결과물이 최종 목표인 분야에 적합하다.</p>
</li>
</ul>
<p>결론적으로, 어떤 단일 지표도 SR 모델의 모든 측면을 대변할 수 없다. 따라서 PSNR/SSIM과 LPIPS/NIQE를 함께 제시하고, 정성적 평가를 통해 각 지표가 의미하는 바를 보완 설명하는 종합적인 접근 방식이 필수적이다.6</p>
<h3>6.3  애플리케이션별 최적의 평가 전략</h3>
<p>최적의 SR 모델과 평가 방식은 절대적인 것이 아니라, 최종 응용 분야의 요구사항에 따라 상대적으로 결정된다. 평가 지표의 선택은 곧 해당 애플리케이션의 ’성공’을 어떻게 정의할 것인가에 대한 문제와 직결된다.</p>
<ul>
<li>
<p><strong>의료 영상, 위성 분석 등 ’정확성’이 중요한 분야:</strong></p>
</li>
<li>
<p><strong>요구사항:</strong> 이 분야에서는 ’사실과 다른 그럴듯함’이 치명적인 오류로 이어질 수 있다. 예를 들어, 의료 영상에서 없던 병변을 생성(환각)하거나 있던 병변을 제거하는 것은 오진을 유발할 수 있다.77 따라서 생성된 모든 디테일의 신뢰성이 무엇보다 중요하다.</p>
</li>
<li>
<p><strong>평가 전략:</strong> <strong>PSNR/SSIM</strong>과 같은 충실도 지표를 최우선으로 고려해야 한다. LPIPS는 보조적으로 사용하되, 전문가에 의한 정성적 평가를 통해 환각 현상 발생 여부를 반드시 검증해야 한다. 가장 이상적인 평가는 SR 적용 후, 분할(segmentation)이나 분류(classification)와 같은 후속 임상 작업(downstream task)의 성능이 실제로 향상되는지를 직접 측정하는 **기능적 지표(Functional Metrics)**를 활용하는 것이다.13</p>
</li>
<li>
<p><strong>미디어 스트리밍, 콘텐츠 복원 등 ’시각적 품질’이 중요한 분야:</strong></p>
</li>
<li>
<p><strong>요구사항:</strong> 이 분야의 최종 목표는 사용자의 시각적 만족도를 극대화하는 것이다. 픽셀 단위의 미세한 부정확성이 있더라도, 전반적으로 더 선명하고 생생하게 보이는 결과물이 선호된다.9</p>
</li>
<li>
<p><strong>평가 전략:</strong> <strong>LPIPS, NIQE</strong>와 같은 인지/자연스러움 지표와 <strong>MOS</strong>와 같은 주관적 평가가 핵심적인 역할을 한다. PSNR/SSIM은 결과물이 원본의 내용을 심각하게 왜곡하지 않았는지 확인하는 최소한의 충실도 보증 기준으로 참고할 수 있다.</p>
</li>
</ul>
<h2>7.  결론: 미래 전망과 평가 방법론의 발전</h2>
<p>초해상화 기술이 빠르게 발전함에 따라, 이를 평가하는 방법론 역시 기존의 틀을 넘어서는 새로운 방향으로 진화하고 있다. 이제 SR 평가는 단순히 ’더 높은 점수’를 추구하는 단계를 지나, 평가 과정 자체의 신뢰성과 한계를 비판적으로 고찰하는 성숙한 단계로 접어들고 있다.</p>
<h3>7.1 기존 평가 방식의 한계 재조명</h3>
<ul>
<li>
<p><strong>Ground Truth의 불완전성:</strong> 그동안 ’완벽한 정답’으로 간주되었던 벤치마크 데이터셋의 GT 이미지들이 실제로는 완벽하지 않을 수 있다는 인식이 확산되고 있다.8 오래된 데이터셋의 경우, 당시의 촬영 기술 한계나 데이터 처리 과정에서 미세한 블러나 노이즈가 포함될 수 있다.8 이러한 ’불완전한 GT’를 기준으로 평가할 경우, 오히려 GT의 결함까지 복원한 저품질 SR 결과물이 높은 충실도 점수를 받고, GT보다 더 깨끗하고 선명한 고품질 결과물이 낮은 점수를 받는 모순적인 상황이 발생할 수 있다.8</p>
</li>
<li>
<p><strong>단일 점수의 한계:</strong> 모델의 성능을 데이터셋 전체에 대한 평균 점수 하나로 요약하는 방식은 모델의 복잡한 행동 양상을 파악하는 데 한계가 있다.74 예를 들어, 특정 텍스처(예: 잔디, 머리카락)에서는 뛰어난 성능을 보이지만 다른 종류의 텍스처(예: 글자)에서는 실패하는 모델의 특성이 평균 점수 뒤에 가려질 수 있다.</p>
</li>
</ul>
<h3>7.2 새로운 평가 지표의 등장</h3>
<p>이러한 한계를 극복하기 위해 SR 평가 방법론은 다음과 같은 새로운 방향으로 나아가고 있다.</p>
<ul>
<li>
<p><strong>상대적 품질 지수(Relative Quality Index, RQI):</strong> GT가 완벽하지 않다는 전제하에, SR 결과물과 GT 간의 ‘절대적’ 차이가 아닌 ‘상대적’ 품질 우위를 측정하려는 새로운 시도이다.80 이 지표는 SR 결과물이 GT보다 더 나을 수 있다는 가능성을 열어두고, 두 이미지 간의 품질 차이를 평가함으로써 불완전한 GT로 인한 평가 편향을 완화하고자 한다.79</p>
</li>
<li>
<p><strong>Task-based Metrics:</strong> SR 기술의 실용적 가치를 평가하기 위해, SR 결과물을 객체 탐지, 이미지 분할 등과 같은 후속 작업(downstream task)의 입력으로 사용하여, 최종 애플리케이션의 성능 향상에 얼마나 기여하는지를 직접 측정하는 방식이 중요해지고 있다.13 이는 “이미지가 얼마나 좋아 보이는가?“라는 질문을 넘어 “이미지가 얼마나 더 유용한가?“라는 질문에 답하는 실용적인 평가 패러다임이다.</p>
</li>
</ul>
<h3>7.3 신뢰성 높은 초해상화 모델 평가를 위한 제언</h3>
<p>미래의 SR 연구는 모델 개발뿐만 아니라, 그 성능을 신뢰성 있게 입증하는 평가 방법론에도 깊은 주의를 기울여야 한다. 이를 위해 다음의 원칙을 제안한다.</p>
<ol>
<li>
<p><strong>다중 지표 사용의 의무화:</strong> 충실도(PSNR, SSIM), 인지 품질(LPIPS), 자연스러움(NIQE)을 아우르는 다각적인 정량 지표를 함께 제시하고, 각 지표가 가지는 의미와 상충 관계를 종합적으로 분석해야 한다.</p>
</li>
<li>
<p><strong>정성적 분석 강화:</strong> 대표적인 성공 및 실패 사례에 대한 시각적 결과와 함께, 발생하는 아티팩트의 유형과 원인에 대한 심도 있는 분석을 반드시 포함해야 한다.</p>
</li>
<li>
<p><strong>애플리케이션 중심의 벤치마킹:</strong> 범용 벤치마크 점수에만 의존하지 말고, 목표 애플리케이션의 특성을 반영하는 특정 데이터셋과 기능적 지표를 활용한 맞춤형 평가를 수행해야 한다.</p>
</li>
<li>
<p><strong>GT 품질에 대한 비판적 접근:</strong> 벤치마크 데이터셋을 사용할 때 GT의 품질 한계를 인지하고, 결과 해석에 이를 반영해야 한다. 향후 데이터셋 구축 시에는 GT 이미지의 품질을 엄격하게 관리하고 그 특성을 명시하는 노력이 필요하다.</p>
</li>
</ol>
<p>결론적으로, 초해상화 성능 평가는 정답이 정해진 측정의 문제가 아니라, 기술의 목표와 한계를 끊임없이 탐색하는 복잡한 해석의 과정이다. 신뢰성 있는 평가를 위해서는 다양한 도구를 비판적으로 사용하며, 최종적으로는 기술이 기여하고자 하는 가치에 부합하는지를 판단하는 종합적인 시각이 요구된다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>What Is Super Resolution? - e-con Systems, https://www.e-consystems.com/blog/camera/technology/what-is-super-resolution/</li>
<li>코렉터 어텐션 네트워크을 이용한 로우 센서 영상 초해상화 기법 - 한국방송·미디어공학회, https://www.kibme.org/resources/journal/20230217145715967.pdf</li>
<li>원격 탐사 영상을 활용한 CNN 기반의 초해상화 기법 연구 - Korea Science, https://koreascience.kr/article/JAKO202018853212063.pdf</li>
<li>Introduction to Super-Resolution Machine Learning Models - Fritz ai, https://fritz.ai/introduction-to-super-resolution-machine-learning-models/</li>
<li>On The Classification-Distortion-Perception Tradeoff - NIPS, https://papers.nips.cc/paper/8404-on-the-classification-distortion-perception-tradeoff</li>
<li>Making Sense of PSNR, SSIM, VMAF - Visionular, https://visionular.ai/vmaf-ssim-psnr-quality-metrics/</li>
<li>Perceptual Visual Quality Assessment: Principles, Methods, and Future Directions - arXiv, https://arxiv.org/html/2503.00625v1</li>
<li>Rethinking Image Evaluation in Super-Resolution - arXiv, https://arxiv.org/html/2503.13074v2</li>
<li>A Complete Guide to Image Super-Resolution in Deep Learning and AI | DigitalOcean, https://www.digitalocean.com/community/tutorials/image-super-resolution</li>
<li>[영상처리] 영상 품질 비교 지표 (PSNR / SSIM) - 코딩뚠뚠 - 티스토리, https://dbstndi6316.tistory.com/376</li>
<li>MiWord of the Day is… Learned Perceptual Image Patch Similarity (LPIPS)!, https://www.tyrrell4innovation.ca/miword-of-the-day-is-learned-perceptual-image-patch-similarity-lpips/</li>
<li>A comprehensive review of image super-resolution metrics: classical and AI-based approaches - Acta IMEKO, https://acta.imeko.org/index.php/acta-imeko/article/download/1679/2939</li>
<li>Evaluating Super-Resolution Models in Biomedical Imaging: Applications and Performance in Segmentation and Classification - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC12027580/</li>
<li>The SSIM Index for Image Quality Assessment, https://www.cns.nyu.edu/~lcv/ssim/</li>
<li>Naturalness Image Quality Evaluator, https://quality.nfdi4ing.de/en/latest/image_quality/NIQE.html</li>
<li>Making a ‘Completely Blind’ Image Quality Analyzer - Laboratory for …, http://live.ece.utexas.edu/research/Quality/niqe_spl.pdf</li>
<li>Ways of cheating on popular objective metrics: blurring, noise, super-resolution and others, https://videoprocessing.ai/metrics/ways-of-cheating-on-popular-objective-metrics.html</li>
<li>Measuring What Matters: Objective Metrics for Image Generation Assessment, https://huggingface.co/blog/PrunaAI/objective-metrics-for-image-generation-assessment</li>
<li>[평가 지표] PSNR / SSIM / LPIPS - xoft - 티스토리, https://xoft.tistory.com/3</li>
<li>영상 스티칭의 지역 차분 픽셀 평가 방법 - 한국방송·미디어공학회, https://www.kibme.org/resources/journal/20191015154522441.pdf</li>
<li>Learned Perceptual Image Patch Similarity (LPIPS) - Lightning AI, https://lightning.ai/docs/torchmetrics/stable/image/learned_perceptual_image_patch_similarity.html</li>
<li>Perceptual Similarity - Spencer’s Wiki, https://wiki.spencerwoo.com/perceptual-similarity.html</li>
<li>Image Quality Metrics - MATLAB &amp; Simulink - MathWorks, https://www.mathworks.com/help/images/image-quality-metrics.html</li>
<li>Peak signal-to-noise ratio - Wikipedia, https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio</li>
<li>PSNR - Compute peak signal-to-noise ratio (PSNR) between images - Simulink - MathWorks, https://www.mathworks.com/help/vision/ref/psnr.html</li>
<li>Image Quality Assessment through FSIM, SSIM, MSE and PSNR—A Comparative Study, https://www.scirp.org/journal/paperinformation?paperid=90911</li>
<li>A Review of the Image Quality Metrics used in Image Generative Models - Paperspace Blog, https://blog.paperspace.com/review-metrics-image-synthesis-models/</li>
<li>psnr - Peak signal-to-noise ratio (PSNR) - MATLAB - MathWorks, https://www.mathworks.com/help/images/ref/psnr.html</li>
<li>Structural similarity index measure - Wikipedia, https://en.wikipedia.org/wiki/Structural_similarity_index_measure</li>
<li>ssim - Structural similarity (SSIM) index for measuring image quality - MATLAB - MathWorks, https://www.mathworks.com/help/images/ref/ssim.html</li>
<li>FID, IS, precision, recall, PSNR, SSIM, LPIPS - 잘하는게없어요 - 티스토리, https://mj9245.tistory.com/21</li>
<li>The Unreasonable Effectiveness of Deep Features as a Perceptual …, https://richzhang.github.io/PerceptualSimilarity/</li>
<li>LipSim: A Provably Robust Perceptual Similarity Metric - arXiv, https://arxiv.org/html/2310.18274v2</li>
<li>R-LPIPS: An Adversarially Robust Perceptual Similarity Metric - OpenReview, https://openreview.net/pdf/010bd9e426c2c4e83d9e213bf2fd87bba87c172f.pdf</li>
<li>niqe - Naturalness Image Quality Evaluator (NIQE) no-reference image quality score - MATLAB - MathWorks, https://www.mathworks.com/help/images/ref/niqe.html</li>
<li>(PDF) Comparison of image quality assessment: PSNR, HVS, SSIM, UIQI - ResearchGate, https://www.researchgate.net/publication/303517356_Comparison_of_image_quality_assessment_PSNR_HVS_SSIM_UIQI</li>
<li>Towards Mitigating Hallucinations in Generative Image Super-Resolution - arXiv, https://arxiv.org/html/2507.14367v1</li>
<li>Ringing artifacts - Wikipedia, https://en.wikipedia.org/wiki/Ringing_artifacts</li>
<li>Gibbs-ringing artifact suppression with knowledge transfer from natural images to MR images - GitHub Pages, https://zxlation.github.io/xiaole.github.com/files/XLZhao_GASCNN.pdf</li>
<li>Deep Learning for Satellite Image Super-Resolution: Mainstream Methods in the State of the Art - Digital Sense, https://www.digitalsense.ai/blog/deep-learning-for-satellite-image-super-resolution-mainstream-methods-in-the-state-of-the-art</li>
<li>Hallucination (artificial intelligence) - Wikipedia, https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)</li>
<li>What are ringing artifacts in image processing? - Quora, https://www.quora.com/What-are-ringing-artifacts-in-image-processing</li>
<li>Ringing artifacts in image deconvolution. Upper-left - ResearchGate, https://www.researchgate.net/figure/Ringing-artifacts-in-image-deconvolution-Upper-left-original-image-Upper-right_fig1_359182021</li>
<li>(PDF) Image upsampling via texture hallucination - ResearchGate, https://www.researchgate.net/publication/224177314_Image_upsampling_via_texture_hallucination</li>
<li>Super Efficient Neural Network for Compression Artifacts Reduction and Super Resolution - arXiv, https://arxiv.org/html/2401.14641v1</li>
<li>DIV2K Dataset, https://data.vision.ee.ethz.ch/cvl/DIV2K/</li>
<li>Div2k Dataset for Super Resolution - Kaggle, https://www.kaggle.com/datasets/takihasan/div2k-dataset-for-super-resolution</li>
<li>eugenesiow/Set5 · Datasets at Hugging Face, https://huggingface.co/datasets/eugenesiow/Set5</li>
<li>Set 5 &amp; 14 Super Resolution Dataset - Kaggle, https://www.kaggle.com/datasets/ll01dm/set-5-14-super-resolution-dataset</li>
<li>eugenesiow/Set14 · Datasets at Hugging Face, https://huggingface.co/datasets/eugenesiow/Set14</li>
<li>An example of super-resolution results on Set14 dataset (4 ×) - ResearchGate, https://www.researchgate.net/figure/An-example-of-super-resolution-results-on-Set14-dataset-4_fig6_368605841</li>
<li>WaveMixSR-V2: Enhancing Super-resolution with Higher Efficiency (Student Abstract), https://ojs.aaai.org/index.php/AAAI/article/view/35262/37417</li>
<li>WaveMixSR: Resource-Efficient Neural Network for Image Super-Resolution - CVF Open Access, https://openaccess.thecvf.com/content/WACV2024/papers/Jeevan_WaveMixSR_Resource-Efficient_Neural_Network_for_Image_Super-Resolution_WACV_2024_paper.pdf</li>
<li>WaveMixSR-V2: Enhancing Super-resolution with Higher Efficiency - arXiv, https://arxiv.org/html/2409.10582v3</li>
<li>MSWSR: A Lightweight Multi-Scale Feature Selection Network for Single-Image Super-Resolution Methods - MDPI, https://www.mdpi.com/2073-8994/17/3/431</li>
<li>Voxel51/Urban100 · Datasets at Hugging Face, https://huggingface.co/datasets/Voxel51/Urban100</li>
<li>eugenesiow/Urban100 · Datasets at Hugging Face, https://huggingface.co/datasets/eugenesiow/Urban100</li>
<li>Mdsr · Models - Dataloop, https://dataloop.ai/library/model/eugenesiow_mdsr/</li>
<li>Awesome Datasets for Super-Resolution: Introduction and Pre-processing | by OpenMMLab, https://openmmlab.medium.com/awesome-datasets-for-super-resolution-introduction-and-pre-processing-55f8501f8b18</li>
<li>A Lightweight Image Super-Resolution Transformer Trained on Low-Resolution Images Only - arXiv, https://arxiv.org/html/2503.23265v1</li>
<li>OSFFNet: Omni-Stage Feature Fusion Network for Lightweight Image Super-Resolution | Proceedings of the AAAI Conference on Artificial Intelligence, https://ojs.aaai.org/index.php/AAAI/article/view/28377</li>
<li>NTIRE 2025 Challenge on Image Super-Resolution (×4): Methods and Results - arXiv, https://arxiv.org/html/2504.14582v1</li>
<li>Benefiting From Bicubically Down-Sampled Images for Learning Real-World Image Super-Resolution, https://openaccess.thecvf.com/content/WACV2021/papers/Rad_Benefiting_From_Bicubically_Down-Sampled_Images_for_Learning_Real-World_Image_Super-Resolution_WACV_2021_paper.pdf</li>
<li>Benefiting from Bicubically Down-Sampled Images for Learning Real-World Image Super-Resolution | Request PDF - ResearchGate, https://www.researchgate.net/publication/342763537_Benefitting_from_Bicubically_Down-Sampled_Images_for_Learning_Real-World_Image_Super-Resolution</li>
<li>Enhancing Remote Sensing Image Super-Resolution Guided by Bicubic-Downsampled Low-Resolution Image - MDPI, https://www.mdpi.com/2072-4292/15/13/3309</li>
<li>Single-Image Super-Resolution: A Benchmark - UC Merced, https://faculty.ucmerced.edu/mhyang/papers/eccv14_super.pdf</li>
<li>[2109.03444] Toward Real-World Super-Resolution via Adaptive Downsampling Models, https://arxiv.org/abs/2109.03444</li>
<li>Benefiting from bicubically down-sampled images for learning real-world image super-resolution - Istanbul Technical University, https://research.itu.edu.tr/en/publications/benefiting-from-bicubically-down-sampled-images-for-learning-real</li>
<li>SEAL: A Framework for Systematic Evaluation of Real-World Super-Resolution, https://openreview.net/forum?id=CGlczSBBSj</li>
<li>A Theory of the Distortion-Perception Tradeoff in Wasserstein Space, https://papers.neurips.cc/paper_files/paper/2021/file/d77e68596c15c53c2a33ad143739902d-Paper.pdf</li>
<li>CVPR Poster Augmenting Perceptual Super-Resolution via Image Quality Predictors, https://cvpr.thecvf.com/virtual/2025/poster/34051</li>
<li>Review of GAN-Based Image Super-Resolution Techniques - ResearchGate, https://www.researchgate.net/publication/384442759_Review_of_GAN-Based_Image_Super-Resolution_Techniques</li>
<li>SRGAN: The Power of GANs in Super-Resolution | by Dong-Keon Kim | Medium, https://medium.com/@kdk199604/srgan-the-power-of-gans-in-super-resolution-94f39a530a61</li>
<li>Texture-based Error Analysis for Image Super-Resolution - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC9719360/</li>
<li>A Real-World Benchmark for Sentinel-2 Multi-Image Super-Resolution - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC10514337/</li>
<li>AIM 2024 Challenge on Video Super-Resolution Quality Assessment: Methods and Results - arXiv, https://arxiv.org/html/2410.04225v1</li>
<li>Super-resolution techniques for biomedical applications and challenges - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC11026337/</li>
<li>초해상화 기술, 영상복원의 새 지표를 열다 - 성대신문, http://www.skkuw.com/news/articleView.html?idxno=23674</li>
<li>[2503.13074] Rethinking Image Evaluation in Super-Resolution - arXiv, https://arxiv.org/abs/2503.13074</li>
<li>Rethinking Image Evaluation in Super-Resolution, https://rqi-sr.github.io/</li>
<li>Paper page - Rethinking Image Evaluation in Super-Resolution - Hugging Face, https://huggingface.co/papers/2503.13074</li>
<li>(PDF) Rethinking Image Evaluation in Super-Resolution - ResearchGate, https://www.researchgate.net/publication/389946158_Rethinking_Image_Evaluation_in_Super-Resolution</li>
<li>AutoSR4EO: An AutoML Approach to Super-Resolution for Earth Observation Images, https://www.mdpi.com/2072-4292/16/3/443</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>