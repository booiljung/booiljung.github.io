<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:이미지 인페인팅 모델 성능 평가 안내서</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>이미지 인페인팅 모델 성능 평가 안내서</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">인공지능 평가지표 (AI evaluation metrics)</a> / <span>이미지 인페인팅 모델 성능 평가 안내서</span></nav>
                </div>
            </header>
            <article>
                <h1>이미지 인페인팅 모델 성능 평가 안내서</h1>
<h2>1. 이미지 인페인팅의 본질과 평가의 중요성</h2>
<h3>1.1 Inpainting의 역사적 배경과 개념 정의</h3>
<p>이미지 인페인팅(Image Inpainting)은 손상되거나 불필요한 영역을 주변 정보와 조화를 이루도록 채워 넣어 완전한 이미지를 복원하는 기술이다.1 그 기원은 르네상스 시대 예술 작품 복원까지 거슬러 올라가며, 당시 예술가들은 손상된 벽화나 회화의 미학적, 역사적 가치를 보존하기 위해 수작업으로 결손부를 채워 넣었다.1 이러한 전통은 18세기 이탈리아의 복원가 피에트로 에드워즈(Pietro Edwards)에 의해 과학적 접근법으로 체계화되었고, 20세기 초 국제 컨퍼런스를 통해 현대적 복원 기술의 기틀을 마련했다.1</p>
<p>디지털 시대가 도래하면서 인페인팅은 컴퓨터 비전의 핵심 연구 분야로 자리 잡았다.3 초기 디지털 인페인팅 기술은 전통적인 복원가의 기법을 수학적으로 모방하려는 시도에서 출발했다.2 대표적으로 편미분 방정식(Partial Differential Equations, PDE)을 이용한 확산 기반(diffusion-based) 방식과 주변의 유사한 패치(patch)를 복사하여 채우는 예제 기반(exemplar-based) 방식이 주를 이루었다.5 확산 기반 방식은 결손 영역의 경계로부터 픽셀 정보를 부드럽게 확산시켜 작은 흠집이나 선을 채우는 데 효과적이었으나, 넓은 영역에서는 흐릿한 결과를 초래하는 한계가 있었다.5 반면, 예제 기반 방식은 이미지 내의 유사한 텍스처를 찾아 복제함으로써 텍스처가 풍부한 영역을 비교적 잘 복원했지만, 이미지에 존재하지 않는 새로운 구조를 생성하지는 못했다.7</p>
<p>딥러닝 기술의 등장은 인페인팅 분야에 패러다임 전환을 가져왔다. 전통적 방식이 이미지의 지역적 정보에 의존하여 제한된 복원 능력만을 보였던 것과 달리, 딥러닝, 특히 생성 모델(Generative Models)은 대규모 데이터셋을 학습하여 이미지의 전역적인 문맥(global context)과 의미론적 구조(semantic structure)를 이해하는 능력을 갖추게 되었다.2 이로 인해 인페인팅은 단순히 ‘손상된 픽셀을 채우는(filling)’ 저차원적 작업을 넘어, ‘시각적으로 그럴듯하고(visually plausible) 의미론적으로 일관된(semantically consistent)’ 콘텐츠를 창의적으로 생성하는 고차원적 과제로 발전했다.6 즉, 현대의 인페인팅 모델은 이미지에 없는 새로운 객체를 그리거나 복잡한 배경을 상상하여 채워 넣을 수 있게 된 것이다.</p>
<h3>1.2 성능 평가의 다차원적 목표</h3>
<p>인페인팅 기술이 이처럼 고도화됨에 따라, 모델의 성능을 평가하는 기준 또한 다차원적으로 변모했다. 단순히 원본 이미지와 얼마나 유사한지를 측정하는 것을 넘어, 생성된 결과물이 인간의 시각 시스템에 얼마나 자연스럽게 인식되는지를 종합적으로 판단해야 한다. 인페인팅 모델의 성능 평가는 단일 지표로 요약될 수 없으며, 다음과 같은 세 가지 핵심 차원으로 구성된다.</p>
<ol>
<li>
<p><strong>충실성 (Fidelity):</strong> 복원된 이미지가 원본 이미지(ground truth)와 픽셀 수준에서 얼마나 유사한가를 측정한다. 이는 주로 픽셀 단위 오차를 기반으로 하는 정량적 지표로 평가된다.</p>
</li>
<li>
<p><strong>사실성 (Realism/Plausibility):</strong> 생성된 영역이 인공적인 느낌 없이 얼마나 자연스럽고 사실적으로 보이는가를 평가한다. 이는 특히 원본 이미지가 존재하지 않는 실제 애플리케이션(예: 사진에서 원치 않는 객체 제거)에서 매우 중요한 기준으로, 주로 인간의 주관적 평가를 통해 측정된다.3</p>
</li>
<li>
<p><strong>일관성 (Consistency):</strong> 생성된 영역이 주변 영역의 구조, 텍스처, 색상, 그리고 의미론적 맥락과 얼마나 조화롭게 어우러지는가를 평가한다. 예를 들어, 사람의 얼굴 일부를 복원할 때 피부 질감뿐만 아니라 전체적인 인상과 정체성까지 일관되게 유지해야 한다.</p>
</li>
</ol>
<p>이러한 다차원적 목표는 인페인팅 기술의 발전이 평가 방법론의 발전을 필연적으로 동반해야 함을 시사한다. 초기 확산 및 패치 기반 모델의 평가는 주로 ’재구성 정확도(reconstruction accuracy)’에 초점을 맞추었기에 PSNR과 같은 픽셀 단위 지표가 유용하게 사용되었다. 그러나 GAN이나 확산 모델과 같은 딥러닝 생성 모델은 손실된 영역에 대해 유일한 정답을 재구성하는 것이 아니라, 학습된 데이터 분포로부터 통계적으로 ‘그럴듯한’ 여러 샘플 중 하나를 생성한다.7 예를 들어, 원본 이미지의 벽에 아무것도 없었더라도 모델이 자연스러운 창문을 생성했다면, 이는 픽셀 단위로는 오차가 크지만 시각적으로는 완벽하게 타당한 결과일 수 있다. 이처럼 모델의 목표가 재구성에서 생성으로 이동함에 따라, 평가의 초점 역시 픽셀 단위의 충실도를 넘어 생성된 결과물의 지각적 품질과 분포적 유사성을 측정하는 방향으로 전환되어야만 했다. 따라서 연구자가 어떤 평가 지표를 선택하는지는 그가 인페인팅 문제를 단순한 신호 처리 문제로 보는지, 아니면 복잡한 생성적 비전 과제로 이해하는지를 드러내는 척도가 된다.</p>
<h3>1.3 보고서의 구조와 목적</h3>
<p>본 안내서는 인공지능 이미지 인페인팅 모델의 성능을 체계적이고 종합적으로 평가하기 위한 방법론을 총망라하는 것을 목표로 한다. 이를 위해 보고서는 세 가지 주요 부분으로 구성된다.</p>
<ul>
<li>
<p><strong>제1부: 정량적 성능 평가 지표</strong>에서는 픽셀 단위 오차 기반의 전통적 지표부터 인간의 시각적 인지를 모사하는 지표, 그리고 최신 생성 모델 평가에 사용되는 학습 기반 및 분포 기반 지표까지 심도 있게 다룬다. 각 지표의 수학적 원리와 특징, 그리고 내재된 한계를 명확히 분석한다.</p>
</li>
<li>
<p><strong>제2부: 정성적 성능 평가 방법론</strong>에서는 정량적 지표만으로는 포착할 수 없는 시각적 품질을 평가하기 위한 사용자 연구(user study) 설계 방안을 제시한다. 평균 의견 점수(MOS)와 이대 강제 선택(2AFC) 등 대표적인 주관적 평가 프로토콜을 상세히 설명한다.</p>
</li>
<li>
<p><strong>제3부: 표준화된 평가 환경 구축</strong>에서는 연구 결과의 재현성과 공정한 비교를 위해 필수적인 벤치마크 데이터셋과 평가용 마스크 생성 프로토콜을 소개한다.</p>
</li>
</ul>
<p>본 안내서를 통해 연구자들과 개발자들은 자신이 개발한 모델의 성능을 객관적이고 다각적으로 평가하고, 그 결과를 신뢰도 높게 보고할 수 있는 전문성을 갖추게 될 것이다. 나아가, 이는 인페인팅 기술 분야 전체의 건전한 발전에 기여하는 것을 궁극적인 목적으로 한다.</p>
<h2>2.  정량적 성능 평가 지표</h2>
<p>정량적 평가는 객관적인 수치를 통해 모델의 성능을 측정하는 방법으로, 모델 간의 성능을 공정하게 비교하고 개발 과정에서 개선 정도를 추적하는 데 필수적이다. 인페인팅 분야의 평가 지표는 픽셀 단위의 단순한 비교에서 시작하여 인간의 시각적 인지 특성을 반영하고, 나아가 생성된 이미지 집합의 통계적 분포를 비교하는 방향으로 발전해왔다.</p>
<h3>2.1  픽셀 단위 오차 기반 지표</h3>
<p>이 지표들은 복원된 이미지(<span class="math math-inline">I_{pred}</span>)와 원본 이미지(<span class="math math-inline">I_{gt}</span>) 간의 픽셀 값 차이를 직접적으로 계산한다. 구현이 간단하고 직관적이라는 장점이 있지만, 인간의 시각적 품질 인식과는 상당한 괴리가 있다는 근본적인 한계를 지닌다.</p>
<h4>2.1.1 L1 손실 (Mean Absolute Error, MAE)</h4>
<p>L1 손실은 예측된 픽셀 값과 실제 픽셀 값 간의 차이의 절댓값의 평균으로 정의된다. 수식은 다음과 같다.12</p>
<p><span class="math math-display">
L_1 = \frac{1}{N} \sum_{i=1}^{N} |I_{gt}^{(i)} - I_{pred}^{(i)}|
</span><br />
여기서 <code>N</code>은 전체 픽셀의 수를 의미한다. L1 손실은 오차의 크기에 정비례하여 페널티를 부여하므로, L2 손실에 비해 이상치(outlier)에 덜 민감한 특성을 보인다. 이로 인해 학습 과정이 더 안정적으로 수렴하는 경향이 있으며, 생성된 이미지가 L2 손실을 사용했을 때보다 덜 흐릿하게(blurry) 나타나는 경향이 있다.14 그러나 L1 손실 역시 픽셀 단위 비교의 본질적 한계를 벗어나지 못한다. 예를 들어, 생성된 이미지가 원본과 비교하여 단 몇 픽셀만 이동(shift)해도 시각적으로는 거의 동일해 보이지만, L1 값은 매우 크게 계산될 수 있다.15 이는 지각적 품질을 제대로 반영하지 못하는 대표적인 사례이다.</p>
<h4>2.1.2 L2 손실 (Mean Squared Error, MSE)</h4>
<p>L2 손실, 즉 평균 제곱 오차는 오차를 제곱하여 평균을 낸 값으로, PSNR 계산의 기초가 된다. 수식은 다음과 같다.16</p>
<p><span class="math math-display">
MSE = \frac{1}{N} \sum_{i=1}^{N} (I_{gt}^{(i)} - I_{pred}^{(i)})^2
</span><br />
MSE는 오차를 제곱하기 때문에 큰 오차에 대해 L1 손실보다 훨씬 더 큰 페널티를 부여한다. 이러한 특성은 모델이 극단적인 예측을 피하고 모든 가능한 결과의 평균에 가까운, 즉 시각적으로는 흐릿하고 보수적인 결과를 생성하도록 유도하는 경향이 있다.13 수학적으로 미분이 용이하여 최적화에 널리 사용되어 왔지만, 생성 모델의 결과물 품질 평가 지표로서는 부적합하다는 것이 중론이다.</p>
<h4>2.1.3 최대 신호 대 잡음비 (Peak Signal-to-Noise Ratio, PSNR)</h4>
<p>PSNR은 신호(원본 이미지)가 가질 수 있는 최대 전력과 잡음(오차)의 전력 간의 비율을 로그 스케일로 나타낸 것이다. MSE를 기반으로 계산되며, 수식은 다음과 같다.16</p>
<p><span class="math math-display">
PSNR = 10 \cdot \log_{10} \left( \frac{MAX_I^2}{MSE} \right)
</span><br />
여기서 <span class="math math-inline">MAX_I</span>는 이미지의 최대 픽셀 값(예: 8비트 이미지의 경우 255)을 나타낸다. PSNR 값은 데시벨(dB) 단위로 표현되며, 값이 클수록 잡음이 적고 원본과의 차이가 적음을 의미한다. PSNR은 이미지 압축이나 노이즈 제거와 같은 전통적인 이미지 처리 분야에서 신호 복원의 충실도를 측정하는 표준 지표로 오랫동안 사용되어 왔다.18</p>
<p>하지만 PSNR은 인간의 시각 시스템이 이미지 품질을 인지하는 방식과 큰 차이를 보인다. 인간의 눈은 평탄한 영역의 미세한 오차보다 구조적으로 중요한 가장자리(edge)나 텍스처 영역의 오차에 더 민감하게 반응하지만, PSNR은 모든 픽셀의 오차를 동등하게 취급한다. 이로 인해 PSNR 점수가 높음에도 불구하고 시각적으로는 부자연스럽거나 중요한 디테일이 소실된 결과가 나올 수 있다. 따라서 현대의 생성적 인페인팅 모델 평가에서 PSNR은 참조용 보조 지표로 사용될 수는 있으나, 모델의 핵심 성능을 대변하는 지표로 사용하기에는 신뢰도가 매우 낮다는 비판을 받는다.18</p>
<h3>2.2  인간의 시각적 인지 모델 기반 지표</h3>
<p>픽셀 단위 지표의 한계를 극복하기 위해, 인간의 시각 시스템(Human Visual System, HVS)이 정보를 처리하는 방식을 모방한 지표들이 제안되었다.</p>
<h4>2.2.1 구조적 유사도 (Structural Similarity Index, SSIM)</h4>
<p>SSIM은 두 이미지 간의 유사성을 평가할 때 픽셀 값의 차이가 아닌, 이미지에 내재된 구조적 정보의 유사성을 측정하는 데 초점을 맞춘다. 이는 인간의 시각 시스템이 이미지의 구조적 정보 추출에 매우 특화되어 있다는 관찰에 기반한다.20 SSIM은 두 이미지 패치</p>
<p><code>x</code>와 <code>y</code>를 비교하기 위해 세 가지 핵심 요소를 사용한다: 휘도(luminance), 대비(contrast), 구조(structure).</p>
<ul>
<li>
<p><strong>휘도 비교 (<span class="math math-inline">l(x, y)</span>):</strong> 두 패치의 평균 픽셀 값(평균 휘도)을 비교한다.<br />
<span class="math math-display">
l(x, y) = \frac{2\mu_x\mu_y + C_1}{\mu_x^2 + \mu_y^2 + C_1}
</span></p>
</li>
<li>
<p><strong>대비 비교 (<span class="math math-inline">c(x, y)</span>):</strong> 픽셀 값의 표준편차(대비)를 비교한다.</p>
<p><span class="math math-display">
c(x, y) = \frac{2\sigma_x\sigma_y + C_2}{\sigma_x^2 + \sigma_y^2 + C_2}
</span></p>
</li>
<li>
<p><strong>구조 비교 (<span class="math math-inline">s(x, y)</span>):</strong> 두 패치 간의 상관계수(구조적 유사성)를 측정한다.</p>
<p><span class="math math-display">
s(x, y) = \frac{\sigma_{xy} + C_3}{\sigma_x\sigma_y + C_3}
</span></p>
</li>
</ul>
<p>여기서 <span class="math math-inline">\mu_x</span>, <span class="math math-inline">\mu_y</span>는 각 패치의 평균, <span class="math math-inline">\sigma_x</span>, <span class="math math-inline">\sigma_y</span>는 표준편차, <span class="math math-inline">\sigma_{xy}</span>는 공분산을 나타낸다. <span class="math math-inline">C_1, C_2, C_3</span>는 분모가 0에 가까워지는 것을 방지하기 위한 안정화 상수이다.</p>
<p>이 세 가지 요소를 결합하여 최종 SSIM 점수를 계산한다.20</p>
<p><span class="math math-display">
SSIM(x, y) = [l(x, y)]^\alpha \cdot [c(x, y)]^\beta \cdot [s(x, y)]^\gamma
</span><br />
일반적으로 가중치 <span class="math math-inline">\alpha, \beta, \gamma</span>는 1로 설정된다. SSIM 값은 -1에서 1 사이의 값을 가지며, 1에 가까울수록 두 이미지가 구조적으로 매우 유사함을 의미한다. 전체 이미지의 SSIM은 이미지 전체를 슬라이딩 윈도우 방식으로 탐색하며 계산된 지역적 SSIM 값들의 평균(Mean SSIM, MSSIM)으로 구한다.</p>
<p>SSIM은 PSNR보다 인간의 주관적 품질 평가와 높은 상관관계를 보이는 것으로 널리 알려져 있다.20 그러나 SSIM 역시 한계를 가진다. 특히 인페인팅에서처럼 이미지의 넓은 영역이 손실되어 완전히 새로운 콘텐츠로 채워지는 경우, 원본과 구조적으로 다를 수밖에 없으므로 SSIM 점수가 낮게 나올 수 있다. 이는 생성된 콘텐츠가 시각적으로 매우 우수하더라도 마찬가지이다.23 또한, 이미지의 기하학적 왜곡이나 흐릿함(blur), 노이즈 등에 대해서는 인간의 인식과 다른 결과를 보일 수 있다.20</p>
<h3>2.3  학습 기반 지각적 유사도 지표</h3>
<p>최근에는 인간의 시각적 인지를 더욱 정교하게 모방하기 위해, 대규모 이미지 데이터셋으로 사전 학습된 심층 신경망(Deep Neural Networks, DNNs)을 활용하는 지표들이 각광받고 있다.</p>
<h4>2.3.1 LPIPS (Learned Perceptual Image Patch Similarity)</h4>
<p>LPIPS는 “좋은 이미지 품질 지표는 인간의 지각적 판단과 일치해야 한다“는 철학에서 출발한다. 이를 위해 픽셀 공간이나 단순한 통계량 대신, 심층 신경망의 특징 공간(feature space)에서 이미지 간의 거리를 측정한다.24 LPIPS는 이미지 분류를 위해 사전 학습된 VGG나 AlexNet과 같은 네트워크가 이미지의 저수준 특징(색상, 질감)부터 고수준 특징(객체의 형태, 의미)까지 계층적으로 학습한다는 점을 활용한다.13</p>
<p>LPIPS의 작동 방식은 다음과 같다.</p>
<ol>
<li>
<p>비교하려는 두 이미지(원본, 생성)를 사전 학습된 네트워크(예: VGG)에 각각 입력한다.</p>
</li>
<li>
<p>네트워크의 여러 중간 계층(예: conv1, conv2,…, conv5)에서 활성화 맵(activation map)을 추출한다.</p>
</li>
<li>
<p>각 계층에서 두 이미지로부터 나온 활성화 맵 간의 차이를 계산한다. 이때 채널별로 가중치를 적용하고 L2 거리를 구한 후, 공간적으로 평균을 낸다.</p>
</li>
<li>
<p>모든 계층에서 계산된 거리 값들을 최종적으로 합산하여 LPIPS 점수를 산출한다.13</p>
</li>
</ol>
<p>LPIPS는 인간의 주관적 평가 결과가 담긴 대규모 데이터셋을 사용하여 네트워크의 가중치를 미세 조정함으로써, 인간의 판단과 매우 높은 상관관계를 갖도록 설계되었다.24 LPIPS 점수가 낮을수록 두 이미지가 인간이 보기에 지각적으로 더 유사하다는 것을 의미한다. PSNR이나 SSIM이 포착하지 못하는 미묘한 텍스처의 차이나 스타일의 불일치 등을 효과적으로 감지할 수 있어, 생성 모델의 시각적 품질을 평가하는 데 매우 강력한 도구로 인정받고 있다.</p>
<h3>2.4  생성 이미지 분포 기반 지표</h3>
<p>인페인팅 모델, 특히 GAN이나 확산 모델은 단일 정답을 찾는 것이 아니라 가능한 여러 결과물 중 하나를 생성한다. 따라서 개별 이미지 쌍을 비교하는 것을 넘어, 모델이 생성하는 이미지들의 전체적인 분포가 실제 이미지들의 분포와 얼마나 유사한지를 평가하는 것이 중요해졌다.</p>
<h4>2.4.1 FID (Fréchet Inception Distance)</h4>
<p>FID는 생성 모델 평가의 사실상 표준(de facto standard) 지표로, 생성된 이미지 집합의 사실성(quality)과 다양성(diversity)을 종합적으로 평가한다.26 FID는 픽셀 수준이 아닌, 심층 특징 공간에서 실제 이미지 분포와 생성 이미지 분포 간의 거리를 측정한다.</p>
<p>FID의 작동 방식은 다음과 같다.</p>
<ol>
<li>
<p>대량의 실제 이미지(<span class="math math-inline">X_r</span>)와 모델이 생성한 이미지(<span class="math math-inline">X_g</span>)를 준비한다.</p>
</li>
<li>
<p>ImageNet으로 사전 학습된 Inception-v3 네트워크에 각 이미지를 통과시켜, 마지막 풀링 계층(pooling layer) 이전의 2048차원 활성화 벡터를 추출한다. 이 벡터는 이미지의 고수준 의미론적 특징을 요약한 것으로 간주된다.26</p>
</li>
<li>
<p>실제 이미지와 생성 이미지로부터 얻은 특징 벡터 집합이 각각 다변량 정규분포를 따른다고 가정하고, 각 집합의 평균(<span class="math math-inline">\mu_r, \mu_g</span>)과 공분산 행렬(<span class="math math-inline">\Sigma_r, \Sigma_g</span>)을 계산한다.</p>
</li>
<li>
<p>두 정규분포 간의 프레셰 거리(Fréchet distance)를 계산하여 최종 FID 점수를 얻는다. 수식은 다음과 같다.26</p>
</li>
</ol>
<p><span class="math math-display">
FID(X_r, X_g) = \|\mu_r - \mu_g\|_2^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r\Sigma_g)^{1/2})
</span></p>
<p>FID 점수가 낮을수록 생성된 이미지의 분포가 실제 이미지의 분포와 가깝다는 것을 의미하며, 이는 생성된 이미지들이 사실적일 뿐만 아니라 실제 이미지처럼 다양하다는 것을 시사한다. 인페인팅 평가에서 FID는 복원된 영역이 주변과 조화를 이루면서도, 매번 똑같은 패턴을 반복하는 것이 아니라 다양하고 자연스러운 결과를 생성하는지를 평가하는 데 매우 유용하다.</p>
<p>이처럼 평가 지표는 단순한 픽셀 비교에서 시작하여 구조적 유사성, 지각적 유사성을 거쳐 최종적으로는 통계적 분포의 유사성을 측정하는 방향으로 진화해왔다. 이러한 계층적 발전은 인페인팅 과제에 대한 이해가 ’신호 복원’에서 ’콘텐츠 생성’으로 심화되었음을 반영한다. 따라서 모델의 성능을 온전히 이해하기 위해서는 어느 한 지표에만 의존해서는 안 된다. 예를 들어, PSNR은 높지만 FID가 나쁜 모델은 원본에 충실하지만 다양성이 부족하고 흐릿한 결과(mode collapse)를 낼 가능성이 높다. 반대로, PSNR은 낮지만 FID가 좋은 모델은 원본과 픽셀 단위로는 다르지만, 시각적으로 그럴듯하고 다양한 대안을 성공적으로 생성하고 있을 수 있다. 이처럼 여러 계층의 지표를 함께 분석함으로써 모델의 행동에 대한 훨씬 더 풍부하고 미묘한 “성능 지문(performance fingerprint)“을 얻을 수 있다.</p>
<table><thead><tr><th>지표 (Metric)</th><th>평가 대상 (What it Measures)</th><th>수식 기반 (Formula Basis)</th><th>장점 (Pros)</th><th>단점 (Cons)</th></tr></thead><tbody>
<tr><td><strong>PSNR</strong></td><td>픽셀 값의 충실도 (Pixel-wise Fidelity)</td><td><span class="math math-inline">10 \cdot \log_{10} \left( \frac{MAX_I^2}{MSE} \right)</span></td><td>계산이 간단하고 빠름. 신호 복원 품질의 표준 지표.</td><td>인간의 시각적 인지와 상관관계 낮음. 구조적, 의미론적 품질 평가 불가.</td></tr>
<tr><td><strong>SSIM</strong></td><td>구조, 휘도, 대비의 유사도 (Structural, Luminance, Contrast Similarity)</td><td><span class="math math-inline">[l]^{\alpha} \cdot [c]^{\beta} \cdot [s]^{\gamma}</span></td><td>PSNR보다 인간의 인지와 상관관계 높음. 이미지의 구조적 정보 고려.</td><td>큰 결손 영역, 텍스처 복잡도 높은 경우 신뢰도 저하.</td></tr>
<tr><td><strong>LPIPS</strong></td><td>학습된 심층 특징의 유사도 (Learned Deep Feature Similarity)</td><td><span class="math math-inline">\sum_l \frac{1}{H_l W_l} \sum_{h,w} \Vert w_l \odot (f^l_{hw} - \hat{f}^l_{hw}) \Vert_2^2</span></td><td>인간의 지각적 판단과 매우 높은 상관관계. 의미론적, 스타일적 유사성 평가에 강함.</td><td>사전 학습된 네트워크 필요. 계산 비용이 상대적으로 높음.</td></tr>
<tr><td><strong>FID</strong></td><td>실제와 생성 이미지 특징 분포 간의 거리 (Distance between Real and Generated Feature Distributions)</td><td><span class="math math-inline">\Vert \mu_r - \mu_g \Vert^2 + Tr(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})</span></td><td>생성된 이미지의 사실성과 다양성을 동시에 평가. 생성 모델 평가의 표준.</td><td>대량의 샘플 필요. 특정 편향(bias)에 민감할 수 있음. InceptionNet의 한계.</td></tr>
</tbody></table>
<h2>3.  정성적 성능 평가 방법론</h2>
<p>정량적 지표는 모델 성능의 특정 측면을 객관적으로 측정하는 데 유용하지만, 인간이 느끼는 종합적인 시각적 품질을 완벽하게 대변하지는 못한다. 미묘한 시각적 결함(artifact), 의미론적 부조화, 전역적 일관성 부족과 같은 문제들은 정량적 수치만으로는 포착하기 어렵다.2 따라서 인간 평가자가 직접 결과물의 품질을 판단하는 정성적 평가는 인페인팅 모델의 최종 성능을 검증하는 데 있어 ’골드 스탠더드(gold standard)’로 여겨진다.</p>
<h3>3.1  사용자 연구의 필요성과 설계</h3>
<p>사용자 연구(user study)는 정량적 지표의 한계를 보완하고, 모델이 생성한 결과물이 실제 사용자에게 어떻게 인식되는지를 직접적으로 확인하는 과정이다. 신뢰도 높은 결과를 얻기 위해서는 연구 설계 단계에서 잠재적인 편향을 체계적으로 통제하는 것이 매우 중요하다.</p>
<p>연구 설계 시 고려해야 할 주요 원칙은 다음과 같다.</p>
<ul>
<li>
<p><strong>참가자 선정:</strong> 충분한 수의 다양한 배경을 가진 참가자를 모집하여 결과의 통계적 유의성을 확보해야 한다.27</p>
</li>
<li>
<p><strong>평가 환경 표준화:</strong> 모든 참가자가 동일한 조건에서 평가를 진행하도록 조명, 디스플레이의 종류와 밝기, 시청 거리 등을 통제해야 한다.27</p>
</li>
<li>
<p><strong>자극 제시 방법:</strong> 평가 대상 이미지들의 제시 순서를 무작위화하여 순서 효과(order effect)로 인한 편향을 방지해야 한다. 또한, 각 이미지를 보여주는 시간을 제한하거나 충분히 제공하는 등 시간적 요소도 일관되게 유지해야 한다.</p>
</li>
<li>
<p><strong>명확한 지침 제공:</strong> 참가자에게 무엇을, 어떤 기준으로 평가해야 하는지 명확하고 일관된 지침을 제공하여 평가자 간의 주관적 기준 차이를 최소화해야 한다.27</p>
</li>
</ul>
<p>이러한 사용자 연구의 필요성은 현재의 자동화된 평가 지표들이 아직 인간의 복잡한 시각적 판단 능력을 완전히 모사하지 못하고 있음을 방증한다. 정성적 평가는 비용과 시간이 많이 소요되는 과정이지만 27, 이는 더 나은 정량적 지표를 개발하기 위한 연구의 동인이 되기도 한다.</p>
<h3>3.2  주관적 평가 프로토콜</h3>
<p>사용자로부터 주관적인 품질 평가를 수집하기 위한 대표적인 프로토콜은 다음과 같다.</p>
<h4>3.2.1 평균 의견 점수 (Mean Opinion Score, MOS)</h4>
<p>MOS는 참가자가 주어진 결과물에 대해 절대적인 품질 점수를 매기는 방식으로, 가장 직관적이고 널리 사용되는 방법 중 하나이다.28</p>
<ul>
<li>
<p><strong>방법론:</strong> 참가자에게 하나의 인페인팅 결과 이미지를 보여주고, 사전에 정의된 척도(예: 1점에서 5점)에 따라 시각적 품질을 평가하도록 요청한다. 최종 MOS는 모든 참가자가 매긴 점수의 산술 평균으로 계산된다.29</p>
</li>
<li>
<p><strong>평가 척도 예시:</strong> 일반적으로 ITU-T 권고안 등에서 표준화된 5점 척도가 사용된다.28</p>
</li>
<li>
<p><strong>5 (Excellent):</strong> 복원된 영역을 전혀 인지할 수 없으며, 원본 이미지와 완벽하게 조화를 이룬다. 결함이 전혀 없다.</p>
</li>
<li>
<p><strong>4 (Good):</strong> 복원된 영역이 거의 인지되지 않으며, 매우 자연스럽다. 결함이 미미하여 거슬리지 않는다.</p>
</li>
<li>
<p><strong>3 (Fair):</strong> 복원된 영역이 약간 인지되지만, 거슬리지 않는 수준이다. 전반적인 이미지 품질은 수용 가능하다.</p>
</li>
<li>
<p><strong>2 (Poor):</strong> 복원된 영역이 명확하게 인지되며, 부자연스러운 결함(artifact)이 존재하여 이미지 품질을 저해한다.</p>
</li>
<li>
<p><strong>1 (Bad):</strong> 복원된 영역이 심각하게 왜곡되거나 주변과 전혀 어울리지 않아 이미지 전체를 훼손한다.</p>
</li>
<li>
<p><strong>장단점:</strong> MOS는 구현이 간단하고 결과 해석이 직관적이라는 장점이 있다. 특정 모델이 상용화 가능한 수준의 절대적인 품질을 만족하는지 판단하는 데 유용하다. 그러나 평가자마다 점수를 매기는 기준이 다를 수 있어 ’척도 사용 편향(scale-use bias)’이 발생할 수 있다. 즉, 한 평가자의 ’4점’이 다른 평가자의 ’3점’에 해당할 수 있어 결과의 일관성이 떨어질 수 있다.31</p>
</li>
</ul>
<h4>3.2.2 이대 강제 선택 (Two-Alternative Forced Choice, 2AFC)</h4>
<p>2AFC는 두 개의 결과물을 직접 비교하여 어느 쪽이 더 나은지를 선택하게 하는 방식으로, 모델 간의 상대적인 성능 우위를 판단하는 데 매우 효과적이다.32</p>
<ul>
<li>
<p><strong>방법론:</strong> 참가자에게 기준이 되는 이미지(원본 또는 손상된 이미지)를 먼저 보여준 후, 두 개의 다른 인페인팅 모델이 생성한 결과물을 나란히 제시한다. 그리고 “어느 쪽 결과물이 더 자연스럽습니까?” 또는 “어느 쪽이 원본과 더 유사합니까?“와 같은 명확한 질문에 대해 둘 중 하나를 ‘강제로’ 선택하도록 요구한다.32 ’둘 다 비슷하다’와 같은 중간 선택지는 일반적으로 허용하지 않아 인지적 부담을 줄이고 명확한 선호도를 얻는다.33</p>
</li>
<li>
<p><strong>데이터 수집 및 분석:</strong> 각 모델이 다른 모델에 비해 선택된 횟수, 즉 선호도 비율을 집계한다. 이 비율을 통해 어떤 모델이 다른 모델보다 통계적으로 유의미하게 더 나은 성능을 보이는지 판단할 수 있다.</p>
</li>
<li>
<p><strong>장단점:</strong> 2AFC는 절대 점수를 매기는 것보다 인지적으로 더 쉬운 과제이므로, 평가자 간의 일관성이 높고 모델 간의 미세한 품질 차이를 민감하게 감지할 수 있다. 이는 새로운 모델이 기존 최고 성능(SOTA) 모델보다 우수함을 입증해야 하는 학술 연구에 특히 적합하다. 단점으로는, 여러 모델의 전체 순위를 매기기 위해서는 모든 가능한 쌍에 대한 비교를 수행하거나 토너먼트 방식과 같은 복잡한 실험 설계가 필요할 수 있다는 점이다.32</p>
</li>
</ul>
<p>결론적으로, MOS와 2AFC는 상호 보완적인 관계에 있다. MOS가 모델의 ’절대적인 품질’을 측정하는 데 유용하다면, 2AFC는 모델 간의 ’상대적인 우위’를 정밀하게 비교하는 데 강점을 가진다. 따라서 가장 이상적인 사용자 연구는 2AFC를 통해 여러 모델의 성능 순위를 결정한 후, 최상위 모델에 대해 MOS 평가를 진행하여 그 모델이 사용자 수용 기준을 만족하는지 확인하는 다단계 접근법을 취하는 것이다.</p>
<h2>4.  표준화된 평가 환경 구축</h2>
<p>인페인팅 모델의 성능을 공정하고 재현 가능하게 평가하기 위해서는 모든 연구자가 동일한 조건에서 실험을 수행할 수 있는 표준화된 환경이 필수적이다. 이는 공통된 벤치마크 데이터셋의 사용과 일관된 평가 마스크 프로토콜의 적용을 통해 이루어진다.</p>
<h3>4.1  벤치마크 데이터셋 분석</h3>
<p>어떤 데이터셋을 사용하느냐는 모델이 학습하고 평가받는 문제의 종류를 결정한다. 특정 데이터셋에 과적합된 모델은 다른 종류의 이미지에서는 성능이 저하될 수 있으므로, 다양한 특성을 가진 표준 데이터셋들을 사용하여 모델의 일반화 성능을 검증하는 것이 중요하다.</p>
<h4>4.1.1 Places2</h4>
<ul>
<li>
<p><strong>특징:</strong> Places2는 MIT에서 구축한 대규모 장면 인식(scene recognition) 데이터셋으로, 365개의 장소 카테고리에 걸쳐 약 180만 장의 학습 이미지와 수만 장의 검증 및 테스트 이미지를 포함한다.34 이미지들은 주로 256x256 또는 512x512 해상도를 가진다. 실내, 실외, 자연, 도시 등 매우 다양한 종류의 풍경과 장면을 담고 있다.</p>
</li>
<li>
<p><strong>Inpainting 연구 활용:</strong> 방대한 종류의 텍스처(하늘, 물, 나무, 벽돌 등)와 복잡한 구조를 포함하고 있어, 인페인팅 모델이 얼마나 다양한 실제 환경에 잘 일반화되는지를 평가하는 표준 벤치마크로 널리 사용된다.36 모델이 특정 도메인에 국한되지 않고 범용적인 복원 능력을 갖추었는지 검증하는 데 필수적이다.</p>
</li>
</ul>
<h4>4.1.2 CelebA-HQ</h4>
<ul>
<li>
<p><strong>특징:</strong> CelebA-HQ는 대규모 얼굴 속성 데이터셋인 CelebA에서 고품질 이미지만을 선별하여 1024x1024의 고해상도로 만든 데이터셋이다. 총 30,000장의 유명인 얼굴 이미지로 구성되어 있다.39</p>
</li>
<li>
<p><strong>Inpainting 연구 활용:</strong> 얼굴은 인간의 시각 시스템이 가장 민감하게 인식하는 대상으로, 미세한 불일치나 왜곡도 쉽게 감지된다. CelebA-HQ는 눈, 코, 입과 같은 정교한 구조와 피부 질감, 그리고 전체적인 인물의 정체성(identity)을 얼마나 잘 보존하며 복원하는지를 평가하는 데 특화된 핵심 데이터셋이다.41 얼굴과 같이 의미론적으로 매우 중요한 객체에 대한 모델의 성능을 극한까지 테스트하는 데 사용된다.</p>
</li>
</ul>
<h4>4.1.3 Paris StreetView</h4>
<ul>
<li>
<p><strong>특징:</strong> 파리 시내의 거리 풍경을 담은 약 15,000장의 이미지로 구성되어 있으며, 주로 건물 정면(facade)을 촬영한 사진들이 많다.43 이미지 해상도는 936x537 픽셀이다.</p>
</li>
<li>
<p><strong>Inpainting 연구 활용:</strong> 창문, 문, 발코니, 벽돌 패턴 등 반복적이고 기하학적인 구조를 다수 포함하고 있어, 모델이 직선, 대칭, 원근감과 같은 구조적 일관성을 얼마나 잘 유지하는지 평가하는 데 이상적이다.45 인공적인 건축물의 정교한 구조를 복원하는 능력을 측정하기 위한 중요한 벤치마크로 활용된다.</p>
</li>
</ul>
<p>이처럼 각 데이터셋은 평가하고자 하는 모델의 능력이 다르다. Paris StreetView에서 뛰어난 성능을 보인 모델이 기하학적 패턴 복원에는 강하지만, CelebA-HQ의 미묘하고 유기적인 얼굴 텍스처 복원에는 실패할 수 있다. 따라서 진정으로 우수한 인페인팅 모델임을 주장하기 위해서는, 이처럼 다양한 특성을 가진 데이터셋 포트폴리오 전반에 걸쳐 강건한 성능을 입증해야 한다. 단일 데이터셋에서의 결과만으로 모델의 우수성을 주장하는 것은 제한적이고 편향된 결론으로 이어질 수 있다.</p>
<table><thead><tr><th>데이터셋 (Dataset)</th><th>총 이미지 수 (Total Images)</th><th>해상도 (Resolution)</th><th>이미지 종류 (Image Type)</th><th>Inpainting 평가 초점 (Inpainting Evaluation Focus)</th></tr></thead><tbody>
<tr><td><strong>Places2</strong></td><td>~180만 (학습)</td><td>256x256, 512x512</td><td>다양한 풍경 및 실내/외 장면 (Diverse Scenes)</td><td>일반화 성능, 다양한 텍스처 및 복잡한 구조 복원 능력</td></tr>
<tr><td><strong>CelebA-HQ</strong></td><td>30,000</td><td>1024x1024</td><td>고해상도 인물 얼굴 (High-res Faces)</td><td>의미론적 일관성, 미세한 얼굴 특징 및 정체성 보존 능력</td></tr>
<tr><td><strong>Paris StreetView</strong></td><td>~15,000</td><td>936x537</td><td>도시 건물 파사드 (Building Facades)</td><td>기하학적 구조, 직선, 반복 패턴 등 구조적 정확성</td></tr>
</tbody></table>
<h3>4.2  평가 마스크 프로토콜</h3>
<p>손실된 영역의 형태와 크기는 인페인팅의 난이도를 결정하는 가장 중요한 요소이다. 따라서 모델 간의 성능을 공정하게 비교하기 위해서는 일관되고 현실적인 마스크를 사용하는 표준화된 프로토콜이 필요하다.</p>
<h4>4.2.1 마스크 종류의 진화</h4>
<ol>
<li>
<p><strong>사각형 마스크 (Rectangular Masks):</strong> 초기 연구에서는 주로 이미지 중앙에 위치한 고정된 크기의 사각형 마스크가 사용되었다.47 이는 실험의 통제는 용이하지만, 실제 이미지 손상이나 객체 제거 시나리오와는 큰 차이가 있다는 명백한 한계가 있다.</p>
</li>
<li>
<p><strong>자유형 마스크 (Free-form Masks):</strong> 이러한 한계를 극복하기 위해, 사용자가 브러시로 그림을 그리듯 자유로운 형태와 크기로 생성된 불규칙한 마스크가 제안되었다.48 자유형 마스크는 사진 속 불필요한 인물이나 물체를 지우는 등 실제 애플리케이션 환경을 훨씬 더 잘 모사하여, 모델의 실용적인 성능과 강건성(robustness)을 평가하는 데 더 적합하다.</p>
</li>
</ol>
<p>자유형 마스크의 등장은 모델 아키텍처의 혁신을 촉진하기도 했다. 기존 컨볼루션 연산은 유효한 픽셀과 마스킹된 무효한 픽셀을 구분하지 못해 경계 부분에 인공적인 결함을 유발했다. 이를 해결하기 위해 부분 컨볼루션(Partial Convolution)이나 게이트 컨볼루션(Gated Convolution)과 같이 마스크 영역을 동적으로 처리하는 새로운 연산 방식이 개발되었다.48 이는 더 현실적인 평가 프로토콜이 모델 아키텍처의 발전을 직접적으로 이끌어낸 사례라 할 수 있다.</p>
<h4>4.2.2 마스크 생성 및 평가 프로토콜</h4>
<p>공정한 평가를 위해, 연구 커뮤니티에서는 표준화된 마스크 생성 알고리즘과 평가 프로토콜을 사용한다.</p>
<ul>
<li>
<p><strong>자유형 마스크 생성 알고리즘:</strong> 훈련 및 평가 시 매번 새로운 자유형 마스크를 즉석에서 생성하기 위한 알고리즘이 널리 사용된다. 이는 무작위로 선을 그리고, 회전하고, 두께를 조절하는 방식으로 인간이 지우개를 사용하는 행동을 시뮬레이션한다.48 이를 통해 매우 다양한 형태와 크기의 마스크를 생성하여 모델을 테스트할 수 있다.</p>
</li>
<li>
<p><strong>마스크 비율 프로토콜:</strong> 모델의 성능은 손실 영역의 크기에 따라 크게 달라지므로, 전체 이미지 면적 대비 마스크가 차지하는 비율에 따라 성능을 구간별로 나누어 보고하는 것이 일반적인 프로토콜이다.7 예를 들어, 마스크 면적 비율을 1-10%, 10-20%, 20-30%, 30-40%, 40-50% 등으로 나누어 각 구간에서의 PSNR, SSIM, FID 등의 지표를 측정한다. 이 방식은 모델이 작은 흠집 복원과 거대한 영역 생성 작업을 얼마나 균형 있게 잘 처리하는지를 체계적으로 분석할 수 있게 해준다.</p>
</li>
</ul>
<p>결론적으로, 진정한 최고 성능의 인페인팅 모델은 다양한 데이터셋(장면, 얼굴, 건축물)과 다양한 마스크 프로토콜(작고 큰, 규칙적이고 불규칙적인) 전반에 걸쳐 일관되게 우수한 성능을 보여야 한다. 평가 환경의 신중한 선택은 연구 결과의 신뢰도와 직결되는 핵심적인 과정이다.</p>
<h2>5. 결론: 종합적인 인페인팅 모델 성능 평가 전략</h2>
<p>이미지 인페인팅 모델의 성능을 정확하고 깊이 있게 평가하는 것은 기술 발전을 위한 필수적인 과정이다. 본 안내서에서 논의한 바와 같이, 성공적인 평가는 단일 지표에 의존하는 것이 아니라, 다수의 정량적, 정성적 지표를 종합적으로 활용하여 모델의 다차원적인 능력을 입체적으로 조명하는 전략을 필요로 한다.</p>
<h3>5.1 다중 지표 평가의 중요성</h3>
<p>어떤 단일 지표도 인페인팅 모델의 모든 성능 측면을 완벽하게 담아낼 수는 없다. 각 지표는 서로 다른 강점과 약점을 가지며, 상호 보완적인 역할을 수행한다. 따라서 충실도, 사실성, 일관성이라는 세 가지 핵심 목표를 종합적으로 평가하기 위해서는 다층적인 접근법이 권장된다.</p>
<p>이상적인 평가 파이프라인은 다음과 같은 요소들을 포함해야 한다.</p>
<ol>
<li>
<p><strong>기초 충실도 검증:</strong> PSNR과 SSIM을 통해 픽셀 및 구조 수준에서 원본과의 기본적인 유사성을 확인한다. 이 지표들은 특히 원본 보존이 중요한 애플리케이션에서 유용하며, 다른 연구들과의 비교를 위한 기준선 역할을 한다.</p>
</li>
<li>
<p><strong>지각적 품질 평가:</strong> LPIPS와 FID를 통해 생성된 결과물의 시각적 사실성과 통계적 분포의 자연스러움을 평가한다. 이 지표들은 인간의 인지와 더 높은 상관관계를 가지며, 현대 생성 모델의 핵심 성능을 측정하는 데 필수적이다.</p>
</li>
<li>
<p><strong>최종 주관적 검증:</strong> 사용자 연구(MOS, 2AFC)를 통해 정량적 지표로는 포착할 수 없는 미묘한 결함이나 의미론적 타당성을 최종적으로 검증한다. 이는 모델이 실제 사용자에게 얼마나 만족스러운 경험을 제공하는지를 판단하는 궁극적인 기준이 된다.</p>
</li>
</ol>
<h3>5.2 평가 목적에 따른 전략적 지표 선택</h3>
<p>모든 평가에 동일한 지표 조합을 적용하기보다는, 평가의 구체적인 목적에 따라 전략적으로 지표를 선택하고 가중치를 두는 것이 효율적이다.</p>
<ul>
<li>
<p><strong>애플리케이션 중심 평가:</strong> 특정 응용 분야를 목표로 한다면, 해당 분야의 요구사항에 맞는 평가에 집중해야 한다. 예를 들어, 오래된 사진 복원과 같이 원본의 충실한 복원이 최우선인 경우, PSNR과 SSIM의 중요도가 상대적으로 높아질 수 있다. 반면, 사진에서 원치 않는 객체를 제거하는 것처럼 ‘그럴듯한’ 대안을 생성하는 것이 목표라면, FID, LPIPS, 그리고 사용자 평가가 핵심적인 성공 척도가 될 것이다.</p>
</li>
<li>
<p><strong>연구 중심 평가:</strong> 새로운 아키텍처나 손실 함수의 학술적 우수성을 입증하는 것이 목표라면, 기존 최고 성능(SOTA) 모델과의 공정하고 엄밀한 비교가 무엇보다 중요하다. 이를 위해서는 Places2, CelebA-HQ, Paris StreetView와 같은 표준 벤치마크 데이터셋과 다양한 비율의 자유형 마스크를 포함하는 표준 프로토콜 하에서, PSNR, SSIM, LPIPS, FID 등 커뮤니티에서 널리 인정받는 모든 정량적 지표를 보고하는 것이 바람직하다. 이는 연구 결과의 투명성과 재현성을 보장하고, 다른 연구자들이 결과를 신뢰하고 그 위에 새로운 연구를 쌓아나갈 수 있게 하는 기반이 된다.</p>
</li>
</ul>
<h3>5.3 미래 연구 방향 및 도전 과제</h3>
<p>이미지 인페인팅 평가 방법론은 기술의 발전과 함께 계속해서 진화해야 한다. 앞으로 해결해야 할 주요 도전 과제와 연구 방향은 다음과 같다.</p>
<ul>
<li>
<p><strong>더 나은 자동 평가 지표 개발:</strong> 현재 가장 진보된 FID나 LPIPS조차도 인간의 복잡한 인지 과정, 특히 이미지의 전역적인 조화나 고차원적인 의미론적 타당성을 완벽하게 측정하지는 못한다. 예를 들어, 생성된 객체가 주변 환경의 물리 법칙이나 상식에 부합하는지 등을 판단할 수 있는 새로운 지표에 대한 연구가 필요하다.</p>
</li>
<li>
<p><strong>참조 없는(No-Reference) 품질 평가:</strong> 대부분의 정량적 지표는 비교 대상인 원본 이미지(ground truth)를 필요로 한다. 하지만 실제 대부분의 응용 시나리오에서는 원본이 존재하지 않는다. 따라서 원본 이미지 없이 인페인팅된 결과물 자체만으로 품질을 평가할 수 있는 참조 없는 이미지 품질 평가(No-Reference Image Quality Assessment, NR-IQA) 기술의 발전이 매우 중요한 연구 방향이다.</p>
</li>
<li>
<p><strong>윤리적 고려사항의 통합:</strong> 인페인팅 기술은 사진 조작, 딥페이크, 허위 정보 생성 등에 악용될 잠재적 위험을 내포하고 있다.50 향후 모델 평가는 단순히 기술적 성능을 측정하는 것을 넘어, 생성된 결과물이 악의적으로 사용될 가능성을 탐지하거나 방지하는 능력까지 평가에 포함해야 할 필요가 있다. 기술의 사회적 영향을 고려하는 윤리적 평가 프레임워크의 구축이 시급한 과제이다.</p>
</li>
</ul>
<p>결론적으로, 이미지 인페인팅 모델의 평가는 고정된 체크리스트가 아니라, 연구 가설과 적용 목표에 따라 유연하게 조정되어야 하는 동적인 프레임워크이다. 궁극적인 목표는 단일 점수로 모델을 서열화하는 것이 아니라, 다양한 측면에서의 성능을 종합하여 모델 고유의 ’성능 지문’을 만들어내는 것이다. 본 안내서가 제시한 다각적이고 심층적인 평가 전략을 통해, 연구 커뮤니티가 더욱 엄밀하고 통찰력 있는 과학적 실천을 이어 나가기를 기대한다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Inpainting - Wikipedia, https://en.wikipedia.org/wiki/Inpainting</li>
<li>A Review of Image Inpainting Methods Based on Deep Learning, https://www.mdpi.com/2076-3417/13/20/11189</li>
<li>Image inpainting, https://atlantis-ar.github.io/infotube/image_inpainting.html</li>
<li>A comprehensive review of past and present image inpainting methods - ResearchGate, https://www.researchgate.net/publication/346098293_A_comprehensive_review_of_past_and_present_image_inpainting_methods</li>
<li>A Survey on Various Image Inpainting Techniques - Arab Journals Platform, https://digitalcommons.aaru.edu.jo/cgi/viewcontent.cgi?article=1019&amp;context=fej</li>
<li>(PDF) A review on various methods of image inpainting - ResearchGate, https://www.researchgate.net/publication/370134325_A_review_on_various_methods_of_image_inpainting</li>
<li>Evaluation of Image Inpainting for Classification … - CVF Open Access, https://openaccess.thecvf.com/content_WACV_2020/papers/Black_Evaluation_of_Image_Inpainting_for_Classification_and_Retrieval_WACV_2020_paper.pdf</li>
<li>Comparative Analysis of Image Inpainting Techniques - IJSAT, https://www.ijsat.org/papers/2025/2/3019.pdf</li>
<li>Deep Learning for Image Inpainting: A Survey - ResearchGate, https://www.researchgate.net/publication/363675072_Deep_Learning_for_Image_Inpainting_A_Survey</li>
<li>Deep Learning-based Image and Video Inpainting: A Survey - arXiv, https://arxiv.org/html/2401.03395v1</li>
<li>PrefPaint: Enhancing Image Inpainting through Expert Human Feedback - arXiv, https://arxiv.org/html/2506.21834v1</li>
<li>A Network with Composite Loss and Parameter-free Chunking Fusion Block for Super-Resolution MR Image - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC10279494/</li>
<li>arXiv:2207.14168v1 [cs.CV] 28 Jul 2022, https://arxiv.org/pdf/2207.14168</li>
<li>ON ADVERSARIAL TRAINING AND LOSS FUNCTIONS FOR SPEECH ENHANCEMENT, https://ashutosh620.github.io/files/CGAN_ICASSP_2018.pdf</li>
<li>PERCEPTUALLY-MOTIVATED ENVIRONMENT-SPECIFIC SPEECH ENHANCEMENT - Princeton University, https://pixl.cs.princeton.edu/pubs/Su_2019_PM/Su_2019_enhancement.pdf</li>
<li>Peak Signal-to-Noise Ratio (PSNR) - Python - GeeksforGeeks, https://www.geeksforgeeks.org/python/python-peak-signal-to-noise-ratio-psnr/</li>
<li>PSNR - Compute peak signal-to-noise ratio (PSNR) between images - Simulink - MathWorks, https://www.mathworks.com/help/vision/ref/psnr.html</li>
<li>Peak signal-to-noise ratio - Wikipedia, https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio</li>
<li>Figure 2 from A critical survey of state-of-the-art image inpainting quality assessment metrics, https://www.semanticscholar.org/paper/A-critical-survey-of-state-of-the-art-image-quality-Qureshi-Deriche/af2789eee94c37f875147b274812831c984dd06e/figure/1</li>
<li>Structural similarity index family for image quality assessment in radiological images - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC5527267/</li>
<li>Structural similarity index measure - Wikipedia, https://en.wikipedia.org/wiki/Structural_similarity_index_measure</li>
<li>Structural Similarity Index - NI - National Instruments, https://www.ni.com/docs/en-US/bundle/ni-vision-concepts-help/page/structural_similarity_index.html</li>
<li>A critical survey of state-of-the-art image inpainting quality assessment metrics | Request PDF - ResearchGate, https://www.researchgate.net/publication/319617416_A_critical_survey_of_state-of-the-art_image_inpainting_quality_assessment_metrics</li>
<li>Loss Functions in Machine Learning - Blog - Metaphysic.ai, https://blog.metaphysic.ai/loss-functions-in-machine-learning/</li>
<li>A Dedicated Loss Function for Neural Face Training - Blog - Metaphysic.ai, https://blog.metaphysic.ai/a-dedicated-loss-function-for-neural-face-training/</li>
<li>Fréchet inception distance - Wikipedia, <a href="https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance">https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance</a></li>
<li>crowdMOS: An Approach for Crowdsourcing Mean Opinion Score Studies - Microsoft, https://www.microsoft.com/en-us/research/wp-content/uploads/2011/05/0002416.pdf</li>
<li>Mean opinion score - Wikipedia, https://en.wikipedia.org/wiki/Mean_opinion_score</li>
<li>What is a Mean Opinion Score (MOS) - A Complete Guide - HeadSpin, https://www.headspin.io/blog/what-is-a-mean-opinion-score-or-mos</li>
<li>Mean Opinion Score as a New Metric for User-Evaluation of XAI MethodsSupported by organization Laboratoire Bordelais de Recherche en Informatique. - arXiv, https://arxiv.org/html/2407.20427v1</li>
<li>How is Mean Opinion Score (MOS) used in TTS evaluation? - Milvus, https://milvus.io/ai-quick-reference/how-is-mean-opinion-score-mos-used-in-tts-evaluation</li>
<li>Evaluating Perceptual Distances by Fitting Binomial Distributions to Two-Alternative Forced Choice Data - arXiv, https://arxiv.org/html/2403.10390v1</li>
<li>Relaxed forced choice improves performance of visual quality assessment methods - arXiv, https://arxiv.org/pdf/2305.00220</li>
<li>Image Inpainting Algorithm Based on Structure-Guided Generative Adversarial Network, https://www.mdpi.com/2227-7390/13/15/2370</li>
<li>Image inpainting based on Mamba-GAN network - SPIE Digital Library, https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13650/136502F/Image-inpainting-based-on-Mamba-GAN-network/10.1117/12.3067571.full</li>
<li>Comparison on the Places2 dataset. - ResearchGate, https://www.researchgate.net/figure/Comparison-on-the-Places2-dataset_tbl1_368669554</li>
<li>Image Inpainting with Local and Global Refinement - Weize QUAN, https://weizequan.github.io/TIP2022/Image_Inpainting_with_Local_and_Global_Refinement-compressed.pdf</li>
<li>Image Inpainting Technique Incorporating Edge Prior and Attention Mechanism, https://www.techscience.com/cmc/v78n1/55383/html</li>
<li>CelebA-HQ Dataset - Papers With Code, https://paperswithcode.com/dataset/celeba-hq</li>
<li>Sample images from CelebA-HQ Dataset [28]. - ResearchGate, https://www.researchgate.net/figure/Sample-images-from-CelebA-HQ-Dataset-28_fig4_338570232</li>
<li>Multi-Modal-CelebA-HQ Dataset - COVE - Computer Vision Exchange, https://cove.thecvf.com/datasets/464</li>
<li>[CVPR 2021] Multi-Modal-CelebA-HQ: A Large-Scale Text-Driven Face Generation and Understanding Dataset - GitHub, https://github.com/IIGROUP/MM-CelebA-HQ-Dataset</li>
<li>What makes Paris look like Paris? | Request PDF - ResearchGate, https://www.researchgate.net/publication/314727145_What_makes_Paris_look_like_Paris</li>
<li>Image Inpainting using Wasserstein Generative Imputation Network | by Tomáš Halama, https://medium.com/@tomhalama/image-inpainting-using-wasserstein-generative-imputation-network-65b537601c19</li>
<li>UCLA Electronic Theses and Dissertations - eScholarship, https://escholarship.org/content/qt3fn5f194/qt3fn5f194.pdf</li>
<li>Qualitative comparisons on the Paris StreetView dataset. All images are scaled to 256 × 256. - ResearchGate, https://www.researchgate.net/figure/Qualitative-comparisons-on-the-Paris-StreetView-dataset-All-images-are-scaled-to-256_fig5_322788099</li>
<li>Brief Review of 6 Different Image Inpainting Articles | by Murat Çelik - Medium, https://muratclk.medium.com/brief-review-of-6-different-image-inpainting-articles-d3784d7ed874</li>
<li>[1806.03589] Free-Form Image Inpainting with Gated Convolution, https://ar5iv.labs.arxiv.org/html/1806.03589</li>
<li>Daily Papers - Hugging Face, <a href="https://huggingface.co/papers?q=free-form+masks">https://huggingface.co/papers?q=free-form%20masks</a></li>
<li>A Review of Image Inpainting Methods Based on Deep Learning - MDPI, https://www.mdpi.com/2076-3417/13/20/11189/review_report</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>