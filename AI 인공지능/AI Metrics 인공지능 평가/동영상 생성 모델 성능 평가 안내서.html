<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:동영상 생성 모델 성능 평가 안내서</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>동영상 생성 모델 성능 평가 안내서</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">인공지능 평가지표 (AI evaluation metrics)</a> / <span>동영상 생성 모델 성능 평가 안내서</span></nav>
                </div>
            </header>
            <article>
                <h1>동영상 생성 모델 성능 평가 안내서</h1>
<h2>1. 서론</h2>
<p>인공지능(AI) 동영상 생성 모델의 발전은 콘텐츠 제작의 패러다임을 바꾸고 있으나, 그 성능을 신뢰성 있게 평가하는 것은 여전히 어려운 과제로 남아있다. 동영상 생성 모델의 평가는 정적 이미지 평가와는 근본적으로 다른 차원의 복잡성을 내포한다. 이미지 평가가 주로 개별 결과물의 공간적 충실도(spatial fidelity)와 텍스트 프롬프트와의 의미론적 정렬(semantic alignment)에 초점을 맞추는 반면, 동영상 평가는 여기에 **시간적 차원(temporal dimension)**이라는 결정적인 축이 추가되기 때문이다.1</p>
<p>동영상의 품질은 개별 프레임의 시각적 완성도에 국한되지 않는다. 프레임과 프레임이 연결되며 나타나는 <strong>시간적 일관성(temporal coherence)</strong>, 물리 법칙을 거스르지 않는 <strong>동작의 사실성(motion dynamics)</strong>, 그리고 프롬프트가 의도한 **서사적 흐름(narrative consistency)**이 모두 조화를 이루어야 비로소 높은 품질의 결과물이라 할 수 있다.1 이러한 다면적 특성으로 인해, 단일 지표만으로는 모델의 성능을 온전히 평가하는 것이 불가능하다. 예를 들어, 시간적 일관성은 단순히 프레임 간의 유사도를 측정하는 것을 넘어, 영상 속 객체가 자신의 정체성을 유지하는지(object permanence), 특정 행동이 논리적 인과관계를 따르는지(causality)와 같은 고차원적인 추론 능력을 요구한다. 이는 동영상 평가가 단순한 패턴 인식 문제를 넘어, 모델의 암묵적인 세계 이해 및 추론 능력을 측정하는 과제로 진화하고 있음을 시사한다. 이와 관련하여 최근 다중모달 거대 언어 모델(MLLM)을 평가의 주체로 활용하려는 시도는 이러한 패러다임의 변화를 명확히 보여준다.1</p>
<p>본 안내서는 동영상 생성 모델 평가의 복잡성을 체계적으로 분석하고 실질적인 지침을 제공하는 것을 목표로 한다. 이를 위해 평가 체계를 세 가지 핵심 축—<strong>정량적 평가 지표</strong>, <strong>정성적 평가 방법론</strong>, <strong>표준 벤치마크 데이터셋</strong>—으로 나누어 심도 있게 탐구한다. 각 장에서는 이론적 배경과 실제 적용 방법, 그리고 내재된 한계를 비판적으로 검토할 것이다. 마지막으로, 현재 평가 체계가 직면한 종합적인 한계를 진단하고, 이를 극복하기 위한 미래 연구 방향을 제시하며 마무리한다.</p>
<h2>2.  정량적 평가 지표 (Quantitative Evaluation Metrics)</h2>
<p>이 장에서는 자동화된 알고리즘을 통해 동영상 생성 모델의 성능을 객관적인 수치로 측정하는 정량적 방법론을 심도 있게 다룬다. 각 지표의 수학적 원리, 계산 과정, 유용성 및 내재된 한계를 비판적으로 분석하여, 연구자와 개발자가 특정 평가 목적에 맞는 지표를 신중하게 선택하고 그 결과를 올바르게 해석할 수 있도록 돕는다.</p>
<h3>2.1  분포 기반 화질 및 다양성 평가</h3>
<p>분포 기반 평가는 생성된 동영상 집합의 통계적 분포를 실제 동영상 집합의 분포와 비교하여 모델의 전반적인 성능을 측정하는 접근법이다. 이는 개별 샘플의 품질을 넘어, 모델이 실제 데이터가 가진 복잡하고 다양한 특성을 얼마나 잘 학습했는지를 평가하는 데 중점을 둔다. 초기 비디오 평가는 압축이나 전송 오류와 같은 저수준 왜곡을 측정하기 위해 참조 비디오와의 픽셀 단위 차이를 계산하는 PSNR(Peak Signal-to-Noise Ratio)이나 SSIM(Structural Similarity Index)과 같은 지표를 사용했다.1 그러나 참조 비디오 없이 독창적인 결과물을 생성하는 현대 생성 모델의 등장으로 이러한 방식은 유효성을 잃었고, 대신 데이터 분포 전체를 비교하는 방식이 표준으로 자리 잡았다.</p>
<h4>2.1.1  프레셰 비디오 거리 (Fréchet Video Distance, FVD)</h4>
<p>개념 및 원리</p>
<p>FVD는 이미지 생성 모델 평가에 널리 사용되는 프레셰 인셉션 거리(Fréchet Inception Distance, FID)를 동영상 영역으로 자연스럽게 확장한 핵심 지표이다.4 FVD의 기본 아이디어는 사전 훈련된 강력한 동영상 특징 추출기를 사용하여 실제 동영상과 생성된 동영상 집합을 각각 고차원 벡터 공간에 임베딩한 후, 두 임베딩 집합의 분포 간 거리를 측정하는 것이다.4 이때 특징 추출기로는 주로 Kinetics 행동 인식 데이터셋으로 사전 훈련된 Inflated 3D ConvNet (I3D) 모델이 사용된다.6 FVD 점수가 낮을수록 두 분포가 통계적으로 유사하다는 것을 의미하며, 이는 생성된 동영상들의 전반적인 시각적 품질과 다양성이 실제 동영상에 가깝다는 것을 나타낸다.4</p>
<p>수학적 공식 및 계산 과정</p>
<p>FVD는 실제 동영상과 생성 동영상의 특징 벡터 분포를 각각 다변량 정규분포로 가정하고, 두 정규분포 간의 2-Wasserstein 거리를 계산한다. 실제 동영상 임베딩 분포를 <span class="math math-inline">N(\mu_r, \Sigma_r)</span>로, 생성 동영상 임베딩 분포를 <span class="math math-inline">N(\mu_g, \Sigma_g)</span>로 가정할 때, FVD는 다음과 같은 공식으로 정의된다.4</p>
<p><span class="math math-display">
\text{FVD} = \|\mu_r - \mu_g\|_2^2 + \text{Tr}\left( \Sigma_r + \Sigma_g - 2 \left( \Sigma_r\Sigma_g \right)^{\frac{1}{2}} \right)
</span><br />
여기서 <span class="math math-inline">\mu_r</span>과 <span class="math math-inline">\mu_g</span>는 각각 실제 동영상과 생성 동영상 특징 벡터들의 평균 벡터를, <span class="math math-inline">\Sigma_r</span>과 <span class="math math-inline">\Sigma_g</span>는 공분산 행렬을 나타낸다. <span class="math math-inline">\|\cdot\|_2^2</span>는 평균 벡터 간의 제곱 유클리드 거리를, <span class="math math-inline">\text{Tr}(\cdot)</span>은 행렬의 대각합(trace)을 의미한다. 계산 과정은 다음과 같은 3단계로 이루어진다.7</p>
<ol>
<li>
<p><strong>특징 추출</strong>: 실제 동영상과 생성 동영상 집합 각각에 대해 I3D 모델을 통과시켜 고차원 특징 벡터(임베딩)를 추출한다.</p>
</li>
<li>
<p><strong>통계량 계산</strong>: 추출된 특징 벡터들을 사용하여 각 집합의 평균 벡터(<span class="math math-inline">\mu</span>)와 공분산 행렬(<span class="math math-inline">\Sigma</span>)을 계산한다.</p>
</li>
<li>
<p><strong>FVD 계산</strong>: 계산된 통계량을 FVD 공식에 대입하여 최종 점수를 산출한다.</p>
</li>
</ol>
<p>핵심적 한계: ‘콘텐츠 편향 (Content Bias)’</p>
<p>FVD는 널리 사용되는 표준 지표임에도 불구하고 치명적인 한계를 가지고 있다. 바로 시간적 일관성이나 움직임의 사실성보다 개별 프레임의 정적인 시각적 품질을 과도하게 중시하는 ’콘텐츠 편향’이다.7 이 편향으로 인해 FVD 점수가 인간의 지각적 판단과 상충되는 경우가 빈번하게 발생한다.9 예를 들어, 영상 내 객체가 갑자기 사라지거나 형태가 심하게 왜곡되는 등 시간적으로 심각한 오류가 발생하더라도, 각 프레임의 텍스처나 색감이 좋으면 오히려 FVD 점수가 낮게(좋게) 평가되는 역설적인 현상이 관찰된다.7 이러한 편향은 모델이 장기적인 시간 구조를 유지하는 능력을 평가하는 데 FVD가 부적합하게 만들며, ’동작 붕괴(motion collapse)’와 같이 움직임이 거의 없는 정적인 영상을 생성하는 문제를 간과하게 만든다.6</p>
<p>편향의 근원과 개선 방향</p>
<p>이러한 콘텐츠 편향의 근원은 FVD가 사용하는 I3D 특징 추출기가 학습된 Kinetics 데이터셋의 특성에서 기인한다.4 Kinetics 데이터셋은 주로 특정 행동을 분류하는 것을 목표로 하는데, 많은 경우 ’색소폰 연주하기’와 같은 행동은 연주하는 ’동작’보다 ’색소폰’이라는 ’객체’의 존재 유무로 더 쉽게 인식될 수 있다. 이로 인해 I3D 모델은 시간적 정보보다 프레임 내의 공간적, 정적 콘텐츠 정보에 더 민감하게 반응하도록 학습되었고, 이 편향이 FVD 지표에 그대로 전이된 것이다.7 이 문제를 완화하기 위해, 최근에는 레이블 없이 대규모 비디오 데이터로 학습하여 시간적 패턴 자체에 집중하는 자기지도학습(self-supervised learning) 기반의 특징 추출기(예: VideoMAE)를 FVD 계산에 활용하려는 연구가 활발히 진행되고 있다.6</p>
<h4>2.1.2  인셉션 스코어 (Inception Score, IS)</h4>
<p>개념 및 원리</p>
<p>인셉션 스코어(IS)는 실제 데이터셋 없이 생성된 결과물만으로 품질을 평가하는 참조-부재(reference-free) 지표이다. IS는 생성된 동영상(또는 이미지)이 두 가지 상호보완적인 기준을 얼마나 잘 만족시키는지를 측정한다.10</p>
<ol>
<li>
<p><strong>품질 (Quality)</strong>: 생성된 각 결과물이 사전 훈련된 이미지 분류기(주로 Inception-v3)에 의해 특정 클래스로 명확하고 확신에 차게 분류되는가? 이는 조건부 확률분포 <span class="math math-inline">p(y|x)</span>의 엔트로피가 낮아야 함을 의미한다. 즉, “이것은 명백히 고양이다“와 같이 분류기의 예측이 명확해야 한다.</p>
</li>
<li>
<p><strong>다양성 (Diversity)</strong>: 생성된 결과물 전체가 다양한 클래스에 걸쳐 균등하게 분포하는가? 이는 모든 클래스의 주변 확률분포 <span class="math math-inline">p(y)</span>의 엔트로피가 높아야 함을 의미한다. 즉, 모델이 고양이만 생성하는 것이 아니라 개, 자동차, 나무 등 다양한 종류의 결과물을 생성해야 한다.</p>
</li>
</ol>
<p>수학적 공식</p>
<p>IS는 이 두 확률분포 간의 차이를 쿨백-라이블러 발산(Kullback-Leibler Divergence, KL Divergence)을 사용하여 측정하고, 그 기댓값에 지수 함수를 취해 최종 점수를 계산한다.11</p>
<p><span class="math math-display">
\text{IS}(G) = \exp\left(\mathbb{E}_{x \sim p_g} D_{KL}(p(y|x) || p(y))\right)
</span><br />
여기서 <span class="math math-inline">x</span>는 생성된 샘플, <span class="math math-inline">y</span>는 레이블, <span class="math math-inline">p_g</span>는 생성 모델의 분포를 의미한다. 점수가 높을수록 생성된 결과물의 품질과 다양성이 우수함을 나타낸다.</p>
<p>한계</p>
<p>IS는 계산이 빠르고 직관적이지만 몇 가지 근본적인 한계를 가진다. 첫째, 실제 데이터 분포와 전혀 비교하지 않기 때문에, 모델이 실제 세계와는 동떨어져 있지만 자체적으로는 선명하고 다양한 이미지를 생성할 경우에도 부당하게 높은 점수를 받을 수 있다.10 둘째, Inception-v3 모델이 학습한 ImageNet의 1000개 클래스에 포함되지 않는 객체나 장면을 생성할 경우, 분류기가 이를 제대로 인식하지 못해 품질이 우수함에도 불구하고 낮은 점수를 받게 된다.13 동영상 평가에 IS를 적용할 때는 보통 각 프레임에 대해 IS를 계산한 후 평균을 내는 방식을 사용하는데, 이는 동영상의 핵심적인 속성인 시간적 구조를 전혀 반영하지 못하는 치명적인 단점을 가진다.15</p>
<h3>2.2  텍스트-비디오 정렬 평가</h3>
<p>텍스트 프롬프트를 기반으로 동영상을 생성하는 T2V(Text-to-Video) 모델이 고도화되면서, 생성된 동영상이 주어진 텍스트의 복잡하고 미묘한 의미를 얼마나 충실하게 시각적으로 구현했는지를 평가하는 것이 중요해졌다. 분포 기반 지표들이 전반적인 ’진짜 같음’을 측정한다면, 정렬 평가는 ’말을 얼마나 잘 알아듣는가’를 측정한다. 이는 생성 모델의 목표가 단순한 현실 모방을 넘어, 인간의 의도를 이해하고 실행하는 방향으로 나아가고 있음을 반영한다.</p>
<h4>2.2.1  CLIP 스코어 (CLIP Score)</h4>
<p>개념 및 원리</p>
<p>CLIP 스코어는 OpenAI가 개발한 CLIP(Contrastive Language-Image Pre-Training) 모델을 활용하여 텍스트와 시각 데이터 간의 의미론적 유사성을 측정하는 대표적인 지표이다.17 CLIP은 방대한 양의 (이미지, 텍스트) 쌍 데이터를 대조 학습하여, 서로 다른 양식의 데이터를 동일한 다차원 임베딩 공간에 배치한다. 이 공간에서는 의미적으로 유사한 텍스트와 이미지가 서로 가깝게 위치하게 된다. CLIP 스코어는 바로 이 임베딩 공간에서 텍스트 프롬프트의 임베딩 벡터와 생성된 동영상 프레임의 임베딩 벡터 간의 코사인 유사도를 계산하여 정렬 수준을 정량화한다.19 이 점수는 인간의 주관적인 판단과 높은 상관관계를 보이는 것으로 알려져 있다.19</p>
<p>계산 방식 및 한계</p>
<p>동영상에 대한 CLIP 스코어를 계산하는 가장 일반적인 방식(예: CLIPSim)은 동영상을 구성하는 각 프레임과 주어진 텍스트 프롬프트 간의 CLIP 스코어를 개별적으로 계산한 후, 이 값들을 산술 평균하여 최종 점수를 도출하는 것이다.20</p>
<p><span class="math math-display">
\text{CLIPScore}(T, V) = \frac{1}{N} \sum_{i=1}^{N} \max(100 \cdot \cos(E_T, E_{I_i}), 0)
</span><br />
여기서 <span class="math math-inline">T</span>는 텍스트 프롬프트, <span class="math math-inline">V</span>는 동영상, <span class="math math-inline">N</span>은 총 프레임 수, <span class="math math-inline">E_T</span>는 텍스트 임베딩, <span class="math math-inline">E_{I_i}</span>는 <span class="math math-inline">i</span>번째 프레임의 이미지 임베딩을 나타낸다.19 이 방식은 계산이 간단하고 직관적이지만, 동영상을 단순히 ’이미지의 가방(bag of frames)’으로 취급하여 프레임의 순서나 시간적 관계를 완전히 무시하는 근본적인 한계를 가진다.16 따라서 “A가 B를 한 후에 C를 한다“와 같은 시간적 순서나, “개가 고양이를 쫓아간다“와 같은 동적인 상호작용을 제대로 평가할 수 없다.</p>
<h4>2.2.2  VQA 기반 평가 (VQA-based Evaluation)</h4>
<p>개념 및 동향</p>
<p>단순 유사도 측정의 한계를 극복하기 위해, 생성된 동영상의 내용에 대해 직접 질문하고 답하는 시각 질의응답(Visual Question Answering, VQA) 모델을 평가에 활용하는 최신 접근법이 주목받고 있다.21 이 방식은 프롬프트에 포함된 복합적인 관계(예: “파란 공 왼쪽에 있는 빨간 큐브”), 객체의 속성, 행동의 순서, 논리적 관계(예: 부정, 비교) 등을 정확하게 구현했는지 심층적으로 평가하는 데 매우 효과적이다. 이는 평가의 기준이 ’유사성’에서 ’이해력’으로 한 단계 발전했음을 보여준다.</p>
<p>주요 지표 (VQAScore, T2V-Score)</p>
<p>VQAScore는 이러한 아이디어를 구체화한 지표로, 주어진 텍스트 프롬프트를 간단한 질문 형태로 변환하여 VQA 모델에 제시한다. 예를 들어, 프롬프트가 “A red cube is to the left of a blue sphere“라면, “Is a red cube to the left of a blue sphere in this video? Answer Yes or No.“와 같은 질문을 생성하고, VQA 모델이 ’예(Yes)’라고 답할 확률을 최종 점수로 사용한다.21 이 방식은 CLIP 스코어가 놓치기 쉬운 구성적(compositional) 이해 능력을 정밀하게 측정할 수 있다. 한편, T2V-Score는 여기서 한 걸음 더 나아가, ’텍스트-비디오 정렬’과 ’비디오 품질’이라는 두 가지 핵심 축을 종합적으로 평가하기 위해 전문가 혼합(mixture of experts) 아키텍처를 사용한다. 이는 VQA를 통한 의미 이해도 평가와 별도의 품질 평가 모델의 점수를 결합하여 더 총체적인 평가를 제공한다.15</p>
<h2>3.  정성적 평가 방법론 (Qualitative Evaluation Methodology)</h2>
<p>자동화된 정량적 지표는 계산의 용이성과 객관성이라는 장점을 가지지만, 인간이 느끼는 미학적 가치, 창의성, 감성적 전달력과 같은 미묘하고 주관적인 품질을 포착하는 데에는 명백한 한계가 있다. FVD와 같은 주요 지표들이 인간의 판단과 종종 불일치하는 사례가 보고되면서 7, 모델의 실질적인 성능을 검증하기 위한 정성적, 즉 인간 중심 평가의 중요성은 더욱 커지고 있다. 이 장에서는 인간 평가를 체계적으로 설계하고 실행하기 위한 방법론을 안내한다.</p>
<h3>3.1  인간 평가의 중요성 및 평가 기준</h3>
<p>’골드 스탠더드’로서의 인간 평가</p>
<p>인간 평가는 현재 동영상 생성 모델 평가의 ’골드 스탠더드(gold standard)’로 여겨진다. 자동화된 지표가 측정하지 못하는 동영상의 종합적인 품질을 판단할 수 있는 유일한 수단이기 때문이다.3 특히, 생성 모델이 만들어내는 예측 불가능한 실패 모드(failure modes)—예를 들어, 사람의 손가락이 6개로 변하거나, 객체가 비현실적으로 합쳐지는 현상—를 발견하고 그 심각성을 판단하는 데 필수적이다. 정량적 지표는 모델 성능에 대한 단일 점수를 제공하여 ’얼마나 나쁜지’를 알려줄 뿐이지만, 정성적 평가는 ’왜, 그리고 어떻게 나쁜지’에 대한 깊이 있는 통찰을 제공함으로써 모델 개선을 위한 직접적인 단서를 준다.1 따라서 인간 평가는 모델의 순위를 매기는 최종 심판의 역할을 넘어, 연구 개발 과정에서 모델의 약점을 진단하고 개선 방향을 설정하는 핵심적인 분석 도구로 기능한다.</p>
<p>핵심 평가 기준</p>
<p>신뢰도 높고 일관된 인간 평가를 위해서는 명확하고 구체적인 평가 기준을 사전에 정의하는 것이 무엇보다 중요하다. 일반적으로 사용되는 핵심 기준은 다음과 같다.</p>
<ul>
<li>
<p><strong>시각적 품질 (Visual Quality)</strong>: 생성된 동영상의 전반적인 시각적 완성도를 평가한다. 여기에는 개별 프레임의 선명도, 색감의 자연스러움, 조명의 일관성, 그리고 블러(blur), 노이즈, 왜곡과 같은 인공적인 결함(artifact)의 유무가 포함된다.2</p>
</li>
<li>
<p><strong>시간적 일관성 (Temporal Consistency)</strong>: 동영상이 시간의 흐름에 따라 논리적이고 시각적인 일관성을 유지하는지를 평가한다. 영상에 등장하는 객체의 정체성, 형태, 색상, 크기가 갑작스럽게 변하지 않아야 하며, 배경 또한 안정적으로 유지되어야 한다. 불필요한 깜빡임이나 객체의 갑작스러운 출현 및 소멸은 시간적 일관성을 해치는 주요 요인이다.1</p>
</li>
<li>
<p><strong>프롬프트 충실도 (Prompt Fidelity/Alignment)</strong>: T2V 모델의 경우, 생성된 동영상이 주어진 텍스트 프롬프트의 모든 요소를 얼마나 정확하고 충실하게 반영하는지를 평가한다. 이는 단순히 객체나 배경을 포함하는 것을 넘어, 명시된 행동, 스타일, 분위기, 객체 간의 관계까지 모두 구현했는지를 포함한다.1</p>
</li>
<li>
<p><strong>물리적 사실성 (Physical Plausibility)</strong>: 생성된 동영상 속 객체들의 움직임과 상호작용이 현실 세계의 물리 법칙에 부합하는지를 평가한다. 중력의 영향, 유체의 흐름, 충돌 시의 반응, 그림자의 방향 등이 자연스럽게 표현되어야 한다.1</p>
</li>
<li>
<p><strong>서사적 일관성 (Narrative Consistency)</strong>: 동영상이 하나의 일관된 이야기나 논리적인 흐름을 가지고 있는지를 평가한다. 장면의 전환이 자연스럽고, 사건의 전개가 개연성을 가지며, 전체적으로 통일된 주제나 메시지를 전달해야 한다.1</p>
</li>
</ul>
<h3>3.2  평가 설계 및 실행</h3>
<p>체계적인 인간 평가를 위해서는 신중하게 설계된 실험 방법론이 필요하다. 대표적으로 사용되는 두 가지 방법은 A/B 테스팅과 리커트 척도이다.</p>
<h4>3.2.1  A/B 테스팅 (A/B Testing)</h4>
<p>방법</p>
<p>A/B 테스팅은 두 개 이상의 모델(A, B,…)이 동일한 입력(예: 텍스트 프롬프트)으로 생성한 결과물을 평가자에게 나란히 제시하고, 특정 기준에 따라 어느 쪽이 더 우수한지를 선택하게 하는 직접 비교 방식이다.24 예를 들어, “두 동영상 중 어느 쪽이 텍스트 프롬프트를 더 잘 반영했습니까?” 또는 “어느 쪽의 움직임이 더 자연스럽습니까?“와 같은 질문을 통해 모델 간의 상대적 우위를 명확하게 판단할 수 있다.</p>
<p>설계 시 유의점</p>
<p>A/B 테스팅의 신뢰도를 높이기 위해서는 평가자의 잠재적 편향을 최소화하는 것이 중요하다. 이를 위해, 모델 A와 B의 결과물이 표시되는 순서(왼쪽/오른쪽)를 무작위로 바꾸어 제시해야 한다. 또한, ’어느 쪽이 더 마음에 드는가?’와 같이 지나치게 주관적이고 모호한 질문보다는, 앞서 정의한 핵심 평가 기준에 기반한 구체적이고 명확한 질문을 사용해야 한다.25 이는 평가의 일관성을 높이고, 수집된 데이터로부터 유의미한 결론을 도출하는 데 도움이 된다.</p>
<h4>3.2.2  리커트 척도 (Likert Scale)</h4>
<p>방법</p>
<p>리커트 척도는 특정 진술에 대한 평가자의 동의 수준을 정량적으로 측정하는 방법이다. 일반적으로 ’전혀 그렇지 않다(1점)’부터 ’매우 그렇다(5점 또는 7점)’까지의 순서형 척도를 사용한다.26 예를 들어, “생성된 동영상의 시간적 일관성은 뛰어났다“라는 진술에 대해 평가자가 1점에서 5점 사이의 점수를 매기게 할 수 있다. 이 방식은 각 평가 기준에 대해 독립적인 점수를 얻을 수 있어 모델의 강점과 약점을 세부적으로 분석하는 데 유용하다.</p>
<p>장점 및 설계</p>
<p>리커트 척도의 가장 큰 장점은 주관적인 평가를 통계적으로 분석 가능한 정량 데이터로 변환할 수 있다는 점이다.26 평가 설문 설계 시에는 각 질문이 하나의 명확한 개념만을 묻도록 주의해야 한다. 예를 들어, “동영상의 화질과 움직임이 모두 자연스러웠다“와 같이 두 가지 개념을 동시에 묻는 ’이중 질문(double-barreled question)’은 피해야 한다.26 또한, 평가자가 특정 진술에 대해 강한 의견이 없거나 판단하기 어려운 경우를 위해 ‘보통’ 또는 ‘중립’ 옵션을 포함하는 것이 일반적이다. 이는 응답의 정확성을 높이는 데 기여한다.26</p>
<h2>4.  표준 벤치마크 데이터셋 (Standard Benchmark Datasets)</h2>
<p>동영상 생성 모델의 성능을 공정하고 재현 가능하게 비교, 평가하기 위해서는 표준화된 벤치마크 데이터셋의 역할이 절대적이다. 벤치마크는 연구 커뮤니티 내에서 각기 다른 모델의 성능을 동일한 잣대로 측정할 수 있는 공통의 기반을 제공하며, 이를 통해 분야의 발전을 객관적으로 추적하고 가속화할 수 있다.21 이 장에서는 동영상 생성 모델 평가에 널리 사용되는 주요 벤치마크 데이터셋의 특징과 역할, 그리고 각 데이터셋이 지닌 한계를 분석한다.</p>
<h3>4.1  데이터셋의 역할과 선택 기준</h3>
<p>역할</p>
<p>표준 벤치마크의 핵심 역할은 **재현성(reproducibility)**과 **비교 가능성(comparability)**을 보장하는 것이다. 모든 연구자가 동일한 데이터와 평가 프로토콜을 사용함으로써, 새로운 모델의 성능 향상이 실제 아키텍처나 학습 방법의 개선에 의한 것인지, 아니면 단순히 더 쉽거나 편향된 데이터를 사용한 결과인지를 명확히 구분할 수 있다. 이상적인 벤치마크는 현실 세계의 다양성을 충분히 반영하면서도, 현재 최고 수준의 모델들조차 완벽하게 해결하지 못하는 도전적인 과제들을 포함하여 기술의 한계를 명확히 드러내고 미래 연구 방향을 제시해야 한다.</p>
<p>선택 기준</p>
<p>평가 목적에 따라 적합한 데이터셋을 신중하게 선택하는 것이 중요하다. 예를 들어, 특정 종류의 행동(예: ‘수영하기’, ‘기타 연주하기’)을 얼마나 잘 생성하는지를 평가하고자 한다면, 다양한 행동 클래스로 레이블링된 행동 인식 데이터셋(예: UCF-101, Kinetics)이 적합하다. 반면, “노을 지는 해변에서 슬픈 표정으로 걷는 남자“와 같이 복잡하고 추상적인 텍스트 지시를 시각적으로 구현하는 능력을 평가하려면, 비디오 클립과 상세한 자연어 설명이 쌍으로 구성된 비디오 캡셔닝 데이터셋(예: MSR-VTT)을 사용해야 한다.</p>
<h3>4.2  주요 벤치마크 데이터셋 분석</h3>
<h4>4.2.1  UCF-101</h4>
<p>특징</p>
<p>UCF-101은 YouTube에서 수집된 101개의 현실적인 인간 행동 카테고리에 대한 약 13,320개의 짧은 비디오 클립으로 구성된 데이터셋이다.27 각 클래스는 스포츠, 악기 연주, 일상 활동 등 다양한 행동을 포함한다. 본래 행동 인식(action recognition) 연구를 위해 구축되었으나, 동영상 생성 분야에서는 특정 클래스의 행동을 조건부로 생성(class-conditional generation)하는 모델의 성능을 평가하는 표준 벤치마크로 널리 활용되고 있다.30</p>
<p>평가에서의 활용 및 한계</p>
<p>UCF-101을 이용한 평가는 주로 FVD나 IS와 같은 지표를 사용하여, 특정 클래스 레이블이 주어졌을 때 생성된 동영상의 품질과 다양성을 측정하는 방식으로 이루어진다.30 하지만 UCF-101은 몇 가지 명백한 한계를 가진다. 대부분의 비디오 클립 길이가 수 초에 불과하고 해상도가 320x240으로 비교적 낮으며, 101개라는 행동의 종류도 제한적이어서 최신 고해상도 장편 동영상 생성 모델의 복합적인 능력을 평가하기에는 부족하다.28</p>
<h4>4.2.2  Kinetics (400/600/700)</h4>
<p>특징</p>
<p>Kinetics는 UCF-101을 훨씬 능가하는 규모와 다양성을 자랑하는 대규모 고품질 행동 인식 데이터셋이다. DeepMind에 의해 구축되었으며, 버전에 따라 클래스 수가 확장된다. 예를 들어, Kinetics-400은 400개의 행동 클래스에 대해 약 30만 개의 비디오 클립을, Kinetics-700은 700개 클래스에 대해 약 65만 개의 10초 내외 비디오 클립을 포함한다.32 방대한 규모 덕분에 비디오 이해 모델을 처음부터 학습시키거나, 다른 태스크를 위한 사전 학습에 이상적인 데이터셋으로 평가받는다.34</p>
<p>평가에서의 이중적 역할</p>
<p>Kinetics 데이터셋은 동영상 생성 평가 생태계에서 매우 독특하고 이중적인 역할을 수행한다. 한편으로는 UCF-101과 마찬가지로 조건부 동영상 생성 모델의 성능을 평가하는 주요 벤치마크로 사용된다.35 다른 한편으로는, 가장 널리 사용되는 정량 평가지표인 FVD를 계산하는 데 필요한 I3D 특징 추출기를 학습시킨 바로 그 데이터셋이기도 하다.4 이로 인해 Kinetics 데이터셋 자체에 내재된 편향(예: 정적인 객체나 배경에 의존하여 행동을 분류하는 경향)이 FVD 지표의 ’콘텐츠 편향’으로 직접 이어진다는 심각한 문제가 발생했다.7 이는 벤치마크 데이터셋이 평가 지표와 어떻게 상호작용하며 전체 평가 체계의 신뢰성에 영향을 미칠 수 있는지를 보여주는 중요한 사례이다.</p>
<h4>4.2.3  MSR-VTT (Microsoft Research Video to Text)</h4>
<p>특징</p>
<p>MSR-VTT는 음악, 스포츠, 요리, 게임 등 20개의 다양한 카테고리에서 수집된 10,000개의 웹 비디오 클립으로 구성된 대규모 비디오 캡셔닝 데이터셋이다.36 이 데이터셋의 가장 큰 특징은 각 비디오 클립마다 Amazon Mechanical Turk 작업자들에 의해 작성된 약 20개의 자연어 문장(캡션)이 달려 있다는 점이다. 총 20만 개의 (비디오, 문장) 쌍을 제공하여, 텍스트를 기반으로 비디오를 생성하거나 검색하는 연구를 위한 핵심 벤치마크로 자리매김했다.38</p>
<p>평가에서의 활용 및 한계</p>
<p>MSR-VTT는 텍스트 프롬프트의 복잡한 의미를 모델이 얼마나 잘 이해하고 시각적으로 구현하는지, 즉 의미론적 정렬(semantic alignment) 능력을 평가하는 데 최적화되어 있다.31 CLIP 스코어와 같은 텍스트-비디오 유사도 지표를 계산하거나, 생성된 비디오에 대한 인간의 주관적 정렬 평가를 수행하는 데 널리 사용된다. 그러나 데이터셋 구축 과정에서 발생한 주석(annotation)의 품질 문제가 한계로 지적된다. 일부 캡션은 서로 중복되거나, 문법적 오류 및 철자 오류를 포함하고 있어 평가의 정밀도를 저해할 수 있다는 비판이 제기된 바 있다.40</p>
<h3>4.3 Table 1: 주요 비디오 생성 벤치마크 데이터셋 비교</h3>
<table><thead><tr><th>특징 (Feature)</th><th>UCF-101</th><th>Kinetics-700</th><th>MSR-VTT</th></tr></thead><tbody>
<tr><td><strong>유형 (Type)</strong></td><td>행동 인식 (Action Recognition)</td><td>행동 인식 (Action Recognition)</td><td>비디오 캡셔닝 (Video Captioning)</td></tr>
<tr><td><strong>영상 수 (Num Videos)</strong></td><td>13,320</td><td>~650,000</td><td>10,000</td></tr>
<tr><td><strong>클래스/캡션 (Classes/Captions)</strong></td><td>101개 행동 클래스</td><td>700개 행동 클래스</td><td>영상 당 ~20개 캡션</td></tr>
<tr><td><strong>총 길이 (Total Duration)</strong></td><td>-</td><td>-</td><td>41.2 시간</td></tr>
<tr><td><strong>주요 용도 (Primary Use Case)</strong></td><td>클래스 조건부 생성, 행동 인식</td><td>대규모 모델 학습, FVD 특징 추출기 학습</td><td>텍스트-비디오 생성, 의미론적 정렬 평가</td></tr>
<tr><td><strong>데이터 출처 (Source)</strong></td><td>YouTube</td><td>YouTube</td><td>웹 비디오 검색</td></tr>
<tr><td><strong>주요 한계 (Key Limitation)</strong></td><td>규모가 작고 다양성 부족</td><td>콘텐츠 편향 가능성, URL 소실</td><td>주석 품질 문제(중복, 오류)</td></tr>
</tbody></table>
<h2>5.  결론: 현재의 한계와 미래 연구 방향</h2>
<p>본 안내서는 동영상 생성 모델의 성능을 평가하기 위한 정량적 지표, 정성적 방법론, 그리고 표준 벤치마크 데이터셋을 종합적으로 검토했다. 분석을 통해 현재의 평가 체계가 모델의 다면적인 능력을 온전히 측정하기에는 여러 근본적인 한계에 직면해 있음을 확인했다. 이 장에서는 이러한 한계들을 종합하고, 이를 극복하기 위한 미래 연구의 방향성을 제시한다.</p>
<h3>5.1  현 평가 체계의 종합적 한계</h3>
<ul>
<li>
<p><strong>정량적 지표의 편향성과 불완전성</strong>: 현재 가장 널리 사용되는 FVD는 ’콘텐츠 편향’으로 인해 시간적 일관성을 제대로 평가하지 못하며, IS는 실제 데이터와의 비교가 부재하고, CLIP Score는 동영상을 시간적 구조가 없는 ’이미지의 나열’로 취급한다.1 이처럼 현존하는 주요 정량적 지표들은 동영상 품질의 특정 단면만을 측정할 뿐, 인간의 종합적인 지각적 판단과는 상당한 괴리를 보인다.</p>
</li>
<li>
<p><strong>정성적 평가의 확장성 문제</strong>: 인간 평가는 평가의 ‘골드 스탠더드’ 역할을 하지만, 막대한 비용과 시간이 소요되고 평가자 개인의 주관성이 개입될 여지가 있어 대규모 평가에서의 일관성과 확장성을 확보하기 어렵다.3 이는 빠른 속도로 발전하는 모델들을 적시에 평가하는 데 큰 장애물이 된다.</p>
</li>
<li>
<p><strong>벤치마크의 편중성 및 노후화</strong>: 대부분의 표준 벤치마크는 인간의 특정 행동을 인식하는 데 초점이 맞춰져 있어, 자연 풍경, 예술적 영상, 과학적 시뮬레이션 등 다양한 도메인의 동영상을 생성하는 모델의 능력을 평가하기에는 부적합하다.42 또한, 대부분이 저해상도의 짧은 클립으로 구성되어 있어, 최신 모델들이 생성하는 고해상도 장편 동영상의 품질을 제대로 평가하지 못한다.</p>
</li>
<li>
<p><strong>단기 평가에의 집중</strong>: 현재의 평가 패러다임은 대부분 수 초에서 수십 초 길이의 짧은 비디오 클립에 집중되어 있다. 이로 인해 장편 동영상 생성의 핵심 과제인 서사의 장기적 일관성, 캐릭터 정체성 유지, 복잡한 인과관계 표현 능력 등을 평가할 수 있는 표준화된 방법론이 부재한 상황이다.23</p>
</li>
</ul>
<h3>5.2  미래 연구 방향</h3>
<p>이러한 한계들을 극복하고 더 신뢰성 높은 평가 체계를 구축하기 위해, 평’가’ 자체를 하나의 고도화된 AI 문제로 재정의하는 패러다임의 전환이 필요하다. 이는 단순히 새로운 지표를 개발하는 것을 넘어, 인간의 복합적인 판단 능력을 모델링하고, 평가와 생성이 함께 발전하는 생태계를 구축하는 것을 의미한다.</p>
<ul>
<li>
<p><strong>차세대 평가 지표 개발</strong>:</p>
</li>
<li>
<p><strong>시간적 일관성 특화 지표</strong>: 객체의 정체성이 영상 전반에 걸쳐 얼마나 일관되게 유지되는지(identity consistency), 물리 법칙을 얼마나 충실히 따르는지(physical fidelity) 등 장기적인 시간적 특성을 정량화할 수 있는 새로운 지표 개발이 시급하다.22</p>
</li>
<li>
<p><strong>해석 가능한(Interpretable) 지표</strong>: 단순히 하나의 점수만을 반환하는 ‘블랙박스’ 지표를 넘어서야 한다. 모델이 구체적으로 어떤 지점에서 실패했는지(예: ‘프레임 150에서 객체 A의 색상이 갑자기 바뀜’)에 대한 진단적 피드백을 제공하는 해석 가능한 지표는 모델 개선에 직접적인 도움을 줄 수 있다.42</p>
</li>
<li>
<p><strong>종합적이고 체계적인 벤치마크 구축</strong>:</p>
</li>
<li>
<p><strong>다양한 도메인 포괄</strong>: 인간 행동 중심에서 벗어나, 자연 풍경, 추상적 영상, 과학 시뮬레이션, CGI 등 다양한 도메인을 포괄하는 종합 벤치마크가 필요하다.42</p>
</li>
<li>
<p><strong>세분화된 능력 평가 벤치마크</strong>: GenAI-Bench와 같이, 모델의 특정 능력을 정밀하게 측정할 수 있는 세분화된 벤치마크의 개발이 요구된다. 예를 들어, ‘객체와 속성 결합 능력’, ‘공간 관계 이해 능력’, ‘논리적 추론 능력’ 등을 독립적으로 평가할 수 있어야 한다.21</p>
</li>
<li>
<p><strong>평가자로서의 다중모달 거대 언어 모델 (MLLMs as Evaluators)</strong>:</p>
</li>
<li>
<p><strong>패러다임의 전환</strong>: 미래 평가 기술의 가장 유력한 방향은 시각, 언어, 음향 등 여러 양식을 동시에 이해하고 추론할 수 있는 MLLM을 자동화된 평가자로 활용하는 것이다.1 MLLM은 “주인공의 감정 변화가 서사적으로 설득력이 있는가?“와 같이 기존 지표로는 측정 불가능했던 추상적이고 주관적인 질문에 답함으로써, 정성적 평가의 깊이와 정량적 평가의 확장성을 결합할 잠재력을 가진다. 이는 평가의 패러다임이 정해진 규칙에 따른 ’계산(computation)’에서 방대한 지식에 기반한 ’판단(judgment)’으로 진화하고 있음을 의미한다.</p>
</li>
<li>
<p><strong>윤리적 및 사회적 영향 평가</strong>: 생성된 동영상이 사회에 미칠 수 있는 영향을 평가하는 프레임워크 구축이 필수적이다. 모델이 특정 인종, 성별, 문화에 대한 편견이나 유해한 스테레오타입을 학습하고 증폭하지는 않는지, 딥페이크와 같은 기술이 악용될 가능성은 없는지 등을 체계적으로 검증하는 방법론이 마련되어야 한다.3</p>
</li>
</ul>
<p>결론적으로, 동영상 생성 모델의 발전은 이를 평가하는 기술의 발전을 요구하고, 역으로 정교한 평가 기술은 다시 생성 모델이 나아가야 할 방향을 제시한다. 이처럼 생성과 평가는 서로의 발전을 추동하는 공진화(co-evolution) 관계에 있으며, 이 순환 구조가 인공지능 영상 기술의 미래를 이끌어갈 것이다. 앞으로는 ’Sora’와 같은 혁신적인 생성 모델의 성능만큼이나, ’Sora를 공정하고 깊이 있게 평가할 수 있는 AI 평가자’의 성능이 중요해지는 시대가 될 것이다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>A Perspective on Quality Evaluation for AI-Generated Videos - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC12349415/</li>
<li>A Survey of AI-Generated Video Evaluation - arXiv, https://arxiv.org/html/2410.19884v1</li>
<li>Challenges in Generated Video Evaluation | Deepti Ghadiyaram - YouTube, https://www.youtube.com/watch?v=mKkGmntxl8M</li>
<li>Fréchet Video Distance (FVD) - Emergent Mind, https://www.emergentmind.com/topics/frechet-video-distance-fvd</li>
<li>Fréchet inception distance - Wikipedia, <a href="https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance">https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance</a></li>
<li>Beyond FVD: Enhanced Evaluation Metrics for Video Generation Quality - arXiv, https://arxiv.org/html/2410.05203v2</li>
<li>On the Content Bias in Frechet Video Distance - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Ge_On_the_Content_Bias_in_Frechet_Video_Distance_CVPR_2024_paper.pdf</li>
<li>On the Content Bias in Fréchet Video Distance - arXiv, https://arxiv.org/html/2404.12391v1</li>
<li>[2404.12391] On the Content Bias in Fréchet Video Distance - arXiv, https://arxiv.org/abs/2404.12391</li>
<li>What are Inception Score and FID, and how do they apply here? - Milvus, https://milvus.io/ai-quick-reference/what-are-inception-score-and-fid-and-how-do-they-apply-here</li>
<li>Inception score - Wikipedia, https://en.wikipedia.org/wiki/Inception_score</li>
<li>Inception Score - Dataforest, https://dataforest.ai/glossary/inception-score</li>
<li>A simple explanation of the Inception Score | by David Mack | Octavian | Medium, https://medium.com/octavian-ai/a-simple-explanation-of-the-inception-score-372dff6a8c7a</li>
<li>A Very Short Introduction to Inception Score(IS) | by Kailash Ahirwar | Medium, https://kailashahirwar.medium.com/a-very-short-introduction-to-inception-score-is-c9b03a7dd788</li>
<li>Towards A Better Metric for Text-to-Video Generation - GitHub Pages, https://showlab.github.io/T2VScore/</li>
<li>[2401.07781] Towards A Better Metric for Text-to-Video Generation - arXiv, https://arxiv.org/abs/2401.07781</li>
<li>openai/CLIP: CLIP (Contrastive Language-Image Pretraining), Predict the most relevant text snippet given an image - GitHub, https://github.com/openai/CLIP</li>
<li>CLIP: Connecting text and images - OpenAI, https://openai.com/index/clip/</li>
<li>CLIP Score — PyTorch-Metrics 1.8.2 documentation - Lightning AI, https://lightning.ai/docs/torchmetrics/stable/multimodal/clip_score.html</li>
<li>Subjective-Aligned Dateset and Metric for Text-to-Video Quality Assessment - arXiv, https://arxiv.org/html/2403.11956v1</li>
<li>VQAScore: Evaluating Text-to-Visual Generation with Image-to-Text Generation - Zhiqiu Lin, https://linzhiqiu.github.io/papers/vqascore/</li>
<li>(PDF) A Comprehensive Survey on Human Video Generation: Challenges, Methods, and Insights - ResearchGate, https://www.researchgate.net/publication/382178392_A_Comprehensive_Survey_on_Human_Video_Generation_Challenges_Methods_and_Insights</li>
<li>A Survey on Long Video Generation: Challenges, Methods, and Prospects - arXiv, https://arxiv.org/html/2403.16407v1</li>
<li>An Online A/B Testing Decision Support System for Web Usability Assessment Based on a Linguistic Decision-making Methodology: Case of Study a Virtual Learning Environment - arXiv, https://arxiv.org/html/2507.12118</li>
<li>Guide to Conduct User Research Like an Expert - Qualaroo, https://qualaroo.com/user-research/</li>
<li>Likert Scale for UX Surveys: Optimizing Data Collection | Aguayo Blog, https://aguayo.co/en/blog-aguayo-user-experience/likert-scale-ux-surveys/</li>
<li>UCF101 Videos - Kaggle, https://www.kaggle.com/datasets/abdallahwagih/ucf101-videos</li>
<li>quchenyuan/UCF101-ZIP · Datasets at Hugging Face, https://huggingface.co/datasets/quchenyuan/UCF101-ZIP</li>
<li>Prepare the UCF101 dataset — gluoncv 0.11.0 documentation, https://cv.gluon.ai/build/examples_datasets/ucf101.html</li>
<li>Daily Papers - Hugging Face, https://huggingface.co/papers?q=UCF-101-24</li>
<li>Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_Preserve_Your_Own_Correlation_A_Noise_Prior_for_Video_Diffusion_ICCV_2023_paper.pdf</li>
<li>cvdfoundation/kinetics-dataset - GitHub, https://github.com/cvdfoundation/kinetics-dataset</li>
<li>The Kinetics dataset - GTDLBench, https://git-disl.github.io/GTDLBench/datasets/kinetics_datasets/</li>
<li>The Kinetics Human Action Video Dataset | alphaXiv, https://www.alphaxiv.org/overview/1705.06950</li>
<li>Open-VCLIP: Transforming CLIP to an Open-vocabulary Video Model via Interpolated Weight Optimization - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v202/weng23b/weng23b.pdf</li>
<li>MSR-VTT: A Large Video Description Dataset for Bridging Video and Language - Microsoft, https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/cvpr16.msr-vtt.tmei_-1.pdf</li>
<li>MSRVTT - Kaggle, https://www.kaggle.com/datasets/vishnutheepb/msrvtt</li>
<li>MSR-VTT - COVE - Computer Vision Exchange, https://cove.thecvf.com/datasets/839</li>
<li>MagDiff: Multi-Alignment Diffusion for High-Fidelity Video Generation and Editing, https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02738.pdf</li>
<li>The MSR-Video to Text Dataset with Clean Annotations - arXiv, https://arxiv.org/pdf/2102.06448</li>
<li>The MSR-Video to Text Dataset with Clean Annotations - arXiv, https://arxiv.org/html/2102.06448v4</li>
<li>(PDF) VideoLLM Benchmarks and Evaluation: A Survey, https://www.researchgate.net/publication/391530685_VideoLLM_Benchmarks_and_Evaluation_A_Survey</li>
<li>[2412.18688] Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation - arXiv, https://arxiv.org/abs/2412.18688</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>