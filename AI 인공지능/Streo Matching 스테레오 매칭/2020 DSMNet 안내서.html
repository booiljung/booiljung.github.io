<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:DSMNet 안내서</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>DSMNet 안내서</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">스테레오 매칭 (Stereo Matchings)</a> / <span>DSMNet 안내서</span></nav>
                </div>
            </header>
            <article>
                <h1>DSMNet 안내서</h1>
<h2>1.  스테레오 매칭의 도메인 일반화 문제</h2>
<h3>1.1 스테레오 매칭의 근본적 중요성</h3>
<p>스테레오 매칭(Stereo Matching)은 컴퓨터 비전 분야의 근본적인 과제 중 하나로, 서로 다른 시점에서 촬영된 한 쌍의 이미지로부터 깊이 정보를 추정하여 3차원 형상을 복원하는 핵심 기술이다.1 이 기술은 자율주행 차량의 환경 인식, 로봇의 정밀한 작업 수행, 증강현실(AR)에서의 가상 객체 배치 등 다양한 첨단 산업 분야에서 필수적인 역할을 수행한다.1 정확한 3D 정보는 시스템이 주변 환경을 이해하고 안전하게 상호작용하기 위한 전제 조건이 되기 때문에, 스테레오 매칭 기술의 발전은 이러한 응용 분야의 성패를 좌우하는 중요한 요소로 간주된다.</p>
<h3>1.2 딥러닝 기반 접근법의 패러다임 전환</h3>
<p>전통적으로 스테레오 매칭은 매칭 비용 계산(matching cost computation), 비용 집계(cost aggregation), 불일치 계산(disparity calculation), 그리고 불일치 정제(disparity refinement)의 4단계 파이프라인으로 구성된 최적화 문제로 다루어졌다.2 그러나 딥러닝 기술의 발전은 이 분야에 혁신적인 변화를 가져왔다. MC-CNN, DispNetC와 같은 초기 모델들을 시작으로, PSMNet, GANet 등 엔드투엔드(end-to-end) 방식으로 전체 파이프라인을 학습하는 심층 신경망 모델들이 등장하면서 기존의 성능을 압도하기 시작했다.2 이러한 모델들은 대규모 데이터셋으로부터 복잡하고 추상적인 특징을 학습하여, 이전에는 해결하기 어려웠던 텍스처가 부족한 영역이나 폐색(occlusion) 영역에서도 비교적 정확한 불일치 맵을 추정할 수 있게 되었다.</p>
<h3>1.3 합성 데이터셋의 양면성 - Sim2Real 문제</h3>
<p>딥러닝 모델의 성능은 학습 데이터의 양과 질에 크게 의존한다. 하지만 스테레오 매칭을 위한 대규모의 실제 환경(real-world) 데이터셋을 구축하는 것은 LiDAR와 같은 고가의 장비와 정밀한 보정 작업을 요구하기 때문에 막대한 비용과 시간이 소요된다.10 이러한 한계로 인해, 연구 커뮤니티는 SceneFlow와 같이 컴퓨터 그래픽스로 생성된 대규모 합성 데이터셋에 크게 의존하게 되었다.9 합성 데이터셋은 완벽한 정답(ground-truth) 불일치 맵을 제공하여 모델 학습을 용이하게 했지만, 동시에 ’Sim2Real’이라는 근본적인 문제를 야기했다.</p>
<h3>1.4 도메인 이동(Domain Shift)의 심층 탐구</h3>
<p>’도메인 이동’은 합성 데이터로 학습된 모델이 실제 환경에서 촬영된 이미지에 적용될 때 성능이 급격히 저하되는 현상을 의미한다.1 이 문제의 근본 원인은 합성 이미지와 실제 이미지 간의 시각적 특성 차이, 즉 ’도메인 격차(domain gap)’에 있다. 합성 데이터는 특유의 렌더링 스타일로 인해 실제 세계와는 다른 색상 분포, 조명 모델, 대비, 질감 패턴, 노이즈 특성을 갖는다.1 딥러닝 모델은 이러한 합성 데이터의 피상적인 특성을 학습하여 불일치를 추정하는 ’지름길 학습(shortcut learning)’에 빠지기 쉽다.9 결과적으로, 모델은 스테레오 매칭의 본질적인 기하학적 원리를 학습하는 대신, 특정 텍스처나 색상 패턴을 특정 깊이 값과 연관시키는 방식으로 과적합되어, 새로운 도메인에 대한 일반화 능력을 상실하게 된다.</p>
<h3>1.5 도메인 일반화(Domain Generalization)의 필요성</h3>
<p>이러한 Sim2Real 문제를 해결하기 위한 접근법으로 ’도메인 적응(Domain Adaptation)’과 ’도메인 일반화(Domain Generalization)’가 있다. 도메인 적응은 목표 도메인의 일부 데이터를 사용하여 모델을 파인튜닝하는 방식이지만, 실제 응용 시나리오에서는 목표 도메인의 데이터를 사전에 확보하기 어려운 경우가 많다.1 따라서 더 도전적이지만 실용적인 목표는 타겟 도메인의 데이터에 전혀 접근하지 않고도 모델이 강건한 성능을 유지하도록 하는 ’도메인 일반화’이다.1 이는 스테레오 매칭 연구의 초점을 단순히 특정 데이터셋에서의 정확도를 높이는 것에서, 도메인에 불변하는 강건한 특징을 어떻게 학습할 것인가에 대한 근본적인 질문으로 이동시켰다. 본 보고서에서 심층적으로 분석할 DSMNet은 이러한 도메인 일반화 문제에 대한 중요한 해법을 제시한 선구적인 연구라 할 수 있다.</p>
<h2>2.  DSMNet: 도메인 불변 스테레오 매칭 네트워크 개요</h2>
<h3>2.1 논문 정보</h3>
<p>DSMNet(Domain-invariant Stereo Matching Network)은 Feihu Zhang 등이 저술하여 컴퓨터 비전 분야의 최고 학회 중 하나인 European Conference on Computer Vision (ECCV) 2020에서 발표된 “Domain-invariant Stereo Matching Networks” 논문에서 제안된 모델이다.7 이 연구는 당시 딥러닝 기반 스테레오 매칭 분야의 가장 큰 난제였던 도메인 일반화 문제를 정면으로 다루며 큰 주목을 받았다.</p>
<h3>2.2 핵심 목표</h3>
<p>DSMNet의 핵심 목표는 명확하다: 오직 합성 데이터만으로 모델을 학습시킨 후, 어떠한 파인튜닝이나 적응 과정도 거치지 않고 이전에 보지 못한 다양한 실제 환경(unseen real environments)에서 높은 일반화 성능을 달성하는 것이다.1 이는 시뮬레이션 환경에서 개발된 모델을 실제 세계에 직접 배포할 수 있는 가능성을 열어준다는 점에서 매우 실용적이고 중요한 목표이다.</p>
<h3>2.3 핵심 전략</h3>
<p>이 목표를 달성하기 위해 DSMNet은 기존 모델들과 다른 근본적인 전략을 채택했다. 대부분의 모델들이 학습 데이터의 특정 텍스처, 색상, 대비와 같은 도메인에 민감한 정보에 의존하여 매칭을 수행하는 경향이 있었던 반면, DSMNet은 이러한 피상적인 정보의 영향을 최소화하고자 했다. 대신, 이미지의 근본적인 구조(structure)와 기하학(geometry) 정보와 같이 도메인 변화에 상대적으로 덜 민감한, 즉 ‘도메인 불변적인(domain-invariant)’ 표현을 학습하는 데 집중했다.1</p>
<h3>2.4 두 가지 핵심 기술 소개</h3>
<p>DSMNet은 이러한 도메인 불변 표현을 효과적으로 학습하기 위해 두 가지 독창적이고 핵심적인 기술을 제안했다.</p>
<ol>
<li>
<p><strong>도메인 정규화 (Domain Normalization, DN):</strong> 이는 기존의 배치 정규화(Batch Normalization)를 대체하는 새로운 정규화 기법이다. DN은 신경망의 각 계층에서 학습되는 특징 표현(representation)의 통계적 분포를 조절하여, 입력 이미지의 도메인(e.g., 합성 또는 실제) 차이에 둔감하고 불변하도록 만든다.7</p>
</li>
<li>
<p><strong>구조 보존 그래프 기반 필터 (Structure-preserving Graph-based Filter, SGF):</strong> 이는 고정된 사각 커널을 사용하는 표준 컨볼루션 연산을 보완하는 새로운 필터링 계층이다. SGF는 이미지의 내용에 기반하여 동적으로 그래프를 구성하고 정보를 전파함으로써, 지역적인 텍스처 패턴보다는 이미지의 전체적인 구조와 기하학적 관계를 포착한다. 이를 통해 강건한 특징을 추출하고 도메인 불변 일반화 성능을 한층 더 강화한다.7</p>
</li>
</ol>
<p>이 두 가지 기술의 유기적인 결합을 통해 DSMNet은 도메인 이동 문제에 대한 강력한 해결책을 제시하며, 스테레오 매칭 연구의 새로운 방향을 열었다.</p>
<h2>3.  DSMNet 아키텍처 심층 분석</h2>
<h3>3.1 기반 모델(Backbone): GANet</h3>
<p>DSMNet은 아키텍처의 기반으로 당시 최고 수준의 성능(State-Of-The-Art, SOTA)을 보이던 GANet(Guided Aggregation Net)을 채택했다.1 GANet은 비용 집계 단계에서 기존의 3D 컨볼루션뿐만 아니라, 반-전역 집계(Semi-Global Aggregation, SGA)와 지역 유도 집계(Local Guided Aggregation, LGA)라는 두 가지 새로운 계층을 도입하여 높은 정확도를 달성했다. DSMNet은 이러한 GANet의 강력한 비용 집계 구조의 장점을 계승하면서도, 도메인 일반화에 취약할 수 있는 부분을 선별적으로 수정하고 개선하는 방향으로 설계되었다.</p>
<h3>3.2 구조적 변경점</h3>
<p>DSMNet의 핵심은 기반 모델의 구조를 도메인 불변성을 강화하는 방향으로 재설계한 데 있다. 주요 변경점은 다음과 같다.</p>
<ul>
<li>
<p><strong>LGA(Local Guided Aggregation) 계층 제거:</strong> GANet의 LGA 계층은 가이던스 맵을 활용하여 지역적으로 적응적인 비용 집계를 수행한다. 그러나 이 과정이 도메인에 민감한 지역적 텍스처나 패턴에 과도하게 의존하여 학습될 가능성이 높다고 판단되었다. 따라서 도메인 일반화 성능에 저해가 될 수 있는 LGA 계층을 과감히 제거했다.1</p>
</li>
<li>
<p><strong>DN 계층으로의 교체:</strong> 기존 딥러닝 모델에서 널리 사용되는 배치 정규화(Batch Normalization, BN)는 미니배치 단위의 통계량을 사용하기 때문에 학습 데이터의 도메인 특성에 과적합될 위험이 있다. DSMNet은 이 문제를 해결하기 위해 특징 추출 네트워크와 가이던스 네트워크에 포함된 모든 BN 계층을 새롭게 제안하는 도메인 정규화(Domain Normalization, DN) 계층으로 전면 교체했다.1</p>
</li>
<li>
<p><strong>SGF 계층의 전략적 삽입:</strong> 고정된 커널 크기를 갖는 표준 컨볼루션의 한계를 극복하고, 이미지의 구조적 정보를 보다 효과적으로 활용하기 위해 구조 보존 그래프 기반 필터(SGF) 계층을 네트워크 곳곳에 전략적으로 삽입했다. 구체적으로, 다중 스케일 특징을 추출하는 특징 추출 네트워크에 총 7개의 SGF 계층을 통합했으며, 3D 비용 볼륨을 정규화하는 비용 집계 단계에서는 각 채널(disparity) 및 깊이(feature) 차원에 대해 2개의 SGF 계층을 추가로 적용했다.1</p>
</li>
</ul>
<h3>3.3 데이터 처리 흐름 (Data Processing Flow)</h3>
<p>DSMNet의 전체적인 데이터 처리 흐름은 일반적인 엔드투엔드 스테레오 매칭 네트워크의 파이프라인을 따르며, 각 단계는 다음과 같이 구성된다.</p>
<ol>
<li>
<p><strong>특징 추출 (Feature Extraction):</strong> 입력으로 주어진 스테레오 이미지 쌍(좌, 우)은 먼저 가중치를 공유하는(weight-sharing) 특징 추출 네트워크를 통과한다. 이 네트워크는 다수의 컨볼루션 계층, DN 계층, 그리고 SGF 계층으로 구성되어 있으며, 이미지로부터 다양한 스케일의 풍부한 시각적 특징 맵(feature map)을 추출한다.</p>
</li>
<li>
<p><strong>비용 볼륨 생성 (Cost Volume Construction):</strong> 다음으로, 추출된 좌측 이미지의 특징 맵과 우측 이미지의 특징 맵을 이용하여 비용 볼륨을 생성한다. 이는 각 픽셀 위치에서 가능한 모든 불일치(disparity) 값에 대한 매칭 비용(matching cost)을 계산하여 4차원 텐서(높이 x 너비 x 최대 불일치 x 특징 채널 수) 형태로 표현한 것이다.</p>
</li>
<li>
<p><strong>비용 집계 (Cost Aggregation):</strong> 생성된 초기 비용 볼륨은 노이즈가 많고 모호성이 높다. 따라서 3D 컨볼루션과 SGF 계층으로 구성된 비용 집계 네트워크를 통해 주변 픽셀들의 컨텍스트 정보를 통합하고 비용 값을 정규화(regularize)한다. 이 과정은 매칭의 정확도를 높이는 데 결정적인 역할을 한다.</p>
</li>
<li>
<p><strong>불일치 맵 추정 (Disparity Estimation):</strong> 마지막으로, 집계되고 정제된 비용 볼륨으로부터 최종 불일치 맵을 추정한다. DSMNet은 각 픽셀 위치에서 불일치 차원에 대한 비용 분포를 확률 분포로 변환한 후, 기댓값을 계산하는 soft-argmin 연산을 사용하여 미분 가능한 방식으로 최종 불일치 값을 회귀(regression)한다.</p>
</li>
</ol>
<h3>3.4 손실 함수 (Loss Function)</h3>
<p>모델의 학습 과정에서는 예측된 불일치 맵과 실제 정답 불일치 맵 간의 오차를 최소화하는 방향으로 네트워크 가중치가 업데이트된다. DSMNet은 다른 많은 최신 스테레오 매칭 모델들과 마찬가지로, 이상치(outlier)에 덜 민감하고 안정적인 학습을 유도하는 Smooth L1 손실 함수를 사용하여 역전파(back-propagation)를 위한 오차를 계산한다.1</p>
<h2>4.  핵심 기술 1: 도메인 정규화</h2>
<p>도메인 정규화(Domain Normalization, DN)는 DSMNet의 핵심적인 기여 중 하나로, 신경망이 도메인 변화에 강건한 특징을 학습하도록 유도하는 새로운 정규화 기법이다. DN의 개념을 이해하기 위해서는 먼저 기존 정규화 기법들의 한계를 파악할 필요가 있다.</p>
<h3>4.1 기존 정규화의 한계</h3>
<ul>
<li>
<p><strong>배치 정규화 (Batch Normalization, BN):</strong> BN은 학습 과정에서 각 특징 맵의 채널별 평균과 분산을 미니배치 단위로 계산하여 정규화를 수행한다. 이는 학습을 안정화하고 가속하는 데 효과적이지만, 미니배치의 통계량이 학습 데이터셋의 전반적인 스타일(e.g., 색감, 밝기)에 강하게 의존하게 만든다. 결과적으로, BN을 사용한 모델은 학습 데이터의 도메인 특성에 과적합되어, 다른 도메인의 데이터가 입력되었을 때 성능이 저하되는 경향을 보인다. BN의 수식은 다음과 같다 1:</p>
<p><span class="math math-display">
\hat{x}_i = \frac{1}{\sigma_i}(x_i - \mu_i)
</span><br />
여기서 평균 <span class="math math-inline">\mu_i</span>와 표준편차 <span class="math math-inline">\sigma_i</span>는 다음과 같이 계산된다.</p>
<p><span class="math math-display">
\mu_i = \frac{1}{m} \sum_{k \in S_i} x_k, \quad \sigma_i = \sqrt{\frac{1}{m} \sum_{k \in S_i} (x_k - \mu_i)^2 + \epsilon}
</span></p>
</li>
<li>
<p><strong>인스턴스 정규화 (Instance Normalization, IN):</strong> IN은 이러한 문제를 해결하기 위해 미니배치 전체가 아닌, 각 데이터 샘플(이미지)과 채널별로 독립적인 정규화를 수행한다. 이 방식은 이미지의 전반적인 스타일 변화에 대한 불변성을 제공하는 데 효과적이다. 하지만, 각 채널을 독립적으로 정규화하기 때문에 채널 간의 상대적인 대비(contrast) 정보와 같이 매칭에 중요한 단서가 될 수 있는 정보를 약화시킬 수 있다는 단점이 있다.1</p>
</li>
</ul>
<h3>4.2 DN의 2단계 접근법</h3>
<p>DSMNet이 제안한 DN은 BN과 IN의 한계를 극복하고 각 기법의 장점을 취하기 위해, 이미지 레벨과 픽셀 레벨에서 순차적으로 정규화를 수행하는 독창적인 2단계 접근법을 사용한다.1</p>
<ol>
<li>
<p><strong>1단계: 이미지 레벨 공간적 정규화 (Image-level Spatial Normalization):</strong> 첫 번째 단계는 IN과 유사하게 작동한다. 각 데이터 샘플과 채널에 대해 독립적으로, 공간적 차원(높이 H, 너비 W)에 걸쳐 평균과 표준편차를 계산하여 정규화를 수행한다. 이 과정은 이미지의 전반적인 스타일, 즉 색상, 조명, 전역 대비와 같은 도메인 종속적인 정보를 제거하는 역할을 한다. 이를 통해 네트워크는 이미지의 스타일에 의존하지 않고 내용(content)에 집중하여 특징을 학습하게 된다.</p>
</li>
<li>
<p><strong>2단계: 픽셀 레벨 채널 정규화 (Pixel-level Channel Normalization):</strong> 첫 번째 단계를 거친 특징 맵에 대해, 각 픽셀 위치(H, W)에서 채널(C) 방향으로 L2 정규화를 적용한다. 이는 해당 픽셀의 특징 벡터를 단위 벡터로 만드는 것과 같다. 이 과정은 지역적 대비(local contrast) 변화에 대한 불변성을 강화하는 효과를 가져온다. 예를 들어, 특정 엣지(edge)가 강한 대비를 갖든 약한 대비를 갖든, L2 정규화를 통해 특징 벡터의 방향(상대적인 채널 간 활성화 패턴)은 유지되면서 크기(magnitude)는 1로 통일된다. 이를 통해 네트워크는 특징의 절대적인 크기가 아닌, 특징들 간의 상대적인 관계에 집중하게 된다. 이 단계의 수식은 다음과 같다 1:</p>
<p><span class="math math-display">
\hat{x}&#39;_i = \frac{\hat{x}_i}{\sqrt{\sum_{j \in S&#39;_i} |\hat{x}_j|^2 + \epsilon}}
</span></p>
</li>
</ol>
<h3>4.3 최종 출력</h3>
<p>이 두 단계의 정규화 과정은 도메인 종속적인 정보를 상당 부분 제거하지만, 동시에 네트워크의 표현력을 과도하게 제한할 수 있다. 이를 보완하기 위해, L2 정규화 이후에 학습 가능한(trainable) 채널별 스케일 파라미터 <span class="math math-inline">\gamma</span>와 시프트 파라미터 <span class="math math-inline">\beta</span>를 적용하여 필요한 정보를 복원하고 표현력을 향상시킨다 1:</p>
<p><span class="math math-display">
y_i = \gamma_i \hat{x}&#39;_i + \beta_i
</span><br />
이러한 DN의 설계는 특징의 ’스타일’과 ’콘텐츠’를 암묵적으로 분리하는 메커니즘으로 해석될 수 있다. 1단계 공간적 정규화는 이미지 전체의 스타일 정보를 제거하고, 2단계 채널 정규화는 지역적 대비와 같은 스타일 변형을 제거한다. 결과적으로 네트워크는 특징 벡터의 크기나 전반적인 통계량이 아닌, 채널 간의 상대적인 활성화 패턴, 즉 특징 벡터의 ’방향’에 집중하여 구조적이고 기하학적인 ‘콘텐츠’ 정보를 인코딩하도록 강제된다. 이는 명시적인 분리 손실 함수 없이도, 정규화 계층의 설계만으로 특징의 불변성을 효과적으로 달성하는 정교한 접근법이다.</p>
<h2>5.  핵심 기술 2: 구조 보존 그래프 기반 필터 (SGF)</h2>
<p>구조 보존 그래프 기반 필터(Structure-preserving Graph-based Filter, SGF)는 DSMNet의 또 다른 핵심 혁신으로, 도메인에 민감한 지역적 텍스처가 아닌 이미지의 근본적인 구조적 정보를 학습하도록 설계된 동적 필터링 메커니즘이다.</p>
<h3>5.1 SGF의 동기</h3>
<p>전형적인 컨볼루션 신경망(CNN)에서 사용되는 필터는 고정된 크기의 사각형 커널(e.g., 3x3)을 사용하여 지역적인 정보를 집계한다. 이러한 방식은 효율적이지만, 두 가지 주요 한계를 가진다. 첫째, 커널의 모양이 고정되어 있어 이미지의 복잡한 구조나 객체의 경계를 효과적으로 반영하지 못한다. 둘째, 필터가 지역적인 텍스처 패턴에 과적합될 위험이 크며, 이는 도메인 이동 문제의 주요 원인 중 하나이다. SGF는 이러한 한계를 극복하기 위해, 이미지의 내용(content)에 따라 동적으로 연결 관계와 가중치를 결정하는 그래프 기반의 정보 전파 방식을 도입했다. 이를 통해 텍스처가 아닌 픽셀 간의 구조적 관계를 기반으로 컨텍스트 정보를 집계하도록 네트워크를 유도한다.1</p>
<h3>5.2 그래프 구성</h3>
<p>SGF를 적용하기 위해, 먼저 입력으로 주어진 2D 특징 맵을 그래프로 변환한다. 각 픽셀은 그래프의 노드(node)가 되고, 인접한 픽셀들은 엣지(edge)로 연결된다. DSMNet에서는 8-방향 연결(8-connected) 그래프를 사용한다. 그러나 일반적인 무방향성 그래프는 정보 전파 과정에서 루프(loop)가 발생하여 계산이 복잡하고 불안정해질 수 있다. 이 문제를 해결하고 효율적인 정보 집계를 위해, SGF는 이 그래프를 두 개의 방향성 비순환 그래프(Directed Acyclic Graphs, DAGs)로 분리한다. 예를 들어, <span class="math math-inline">G_1</span>은 왼쪽에서 오른쪽으로, 위에서 아래로 정보가 흐르는 방향성 그래프이고, <span class="math math-inline">G_2</span>는 그 반대 방향으로 정보가 흐르는 그래프이다. 최종 결과는 이 두 방향성 그래프에서 집계된 결과를 결합하여 얻는다.1</p>
<h3>5.3 SGF의 수학적 정의</h3>
<p>그래프 <span class="math math-inline">G_i</span> (i=1, 2)에 대해, 특정 노드(픽셀) <span class="math math-inline">p</span>에서의 필터링된 값 <span class="math math-inline">C^A_i(p)</span>는 그래프 내의 다른 모든 노드 <span class="math math-inline">q</span>의 원래 값 <span class="math math-inline">C(q)</span>들의 가중 평균으로 계산된다. 여기서 가중치 <span class="math math-inline">W(q,p)</span>는 노드 <span class="math math-inline">q</span>에서 <span class="math math-inline">p</span>로 정보가 얼마나 많이 전파되는지를 나타내며, <span class="math math-inline">q</span>에서 <span class="math math-inline">p</span>로 가는 모든 가능한 경로(path)들의 가중치를 합산하여 결정된다. 각 경로의 가중치는 해당 경로를 구성하는 모든 엣지 가중치 <span class="math math-inline">\omega_e</span>의 곱으로 계산된다. 이를 수식으로 표현하면 다음과 같다 1:</p>
<p><span class="math math-display">
C^A_i(p) = \frac{\sum_{q \in G_i} W(q,p) \cdot C(q)}{\sum_{q \in G_i} W(q,p)}, \quad \text{where} \quad W(q,p) = \sum_{l_{q,p} \in G_i} \prod_{e \in l_{q,p}} \omega_e
</span></p>
<h3>5.4 엣지 가중치 (Edge Weight)</h3>
<p>엣지의 가중치 <span class="math math-inline">\omega_e(q,p)</span>는 두 인접 노드 <span class="math math-inline">p</span>와 <span class="math math-inline">q</span>의 특징 벡터 <span class="math math-inline">x_p</span>와 <span class="math math-inline">x_q</span> 간의 유사도에 따라 결정된다. DSMNet에서는 코사인 유사도(cosine similarity)를 사용하여 이를 계산한다. 이는 두 픽셀의 특징이 유사할수록 더 강한 연결을 갖고 더 많은 정보를 서로에게 전파하도록 만든다. 결과적으로, SGF는 특징이 유사한 영역(e.g., 동일 객체 내부) 내에서는 정보를 넓게 전파하고, 특징이 급격히 변하는 영역(e.g., 객체 경계)에서는 정보 전파를 차단하는, 내용 기반의 적응적 필터링을 수행하게 된다 1:</p>
<p><span class="math math-display">
\omega_e(q,p) = \frac{x_p^T x_q}{\|x_p\|_2 \cdot \|x_q\|_2}
</span></p>
<h3>5.5 효율적 구현: 선형 반복 집계 (Iterative Linear Aggregation)</h3>
<p>위에서 정의한 SGF의 수식은 모든 가능한 경로를 고려해야 하므로 직접 계산하기에는 계산 복잡도가 매우 높다. 이를 해결하기 위해 DSMNet은 동적 프로그래밍(dynamic programming) 원리를 적용한 효율적인 구현 방식을 제안한다. 먼저, 각 노드 <span class="math math-inline">p</span>로 들어오는 엣지들의 가중치 합이 1이 되도록 정규화 제약(<span class="math math-inline">\sum_{q \in N_p} \omega_e(q,p) = 1</span>)을 추가한다. 이 제약 하에서, SGF는 그래프의 방향(e.g., 위에서 아래로, 왼쪽에서 오른쪽으로)을 따라 순차적으로 각 노드의 값을 갱신하는 선형 반복 집계(iterative linear aggregation) 방식으로 구현될 수 있다. 특정 노드 <span class="math math-inline">p</span>의 갱신된 값 <span class="math math-inline">C^A_i(p)</span>는 자신의 원래 값 <span class="math math-inline">C(p)</span>와, 이미 갱신된 이웃 노드 <span class="math math-inline">q</span>들의 값 <span class="math math-inline">C^A_i(q)</span>의 가중치 합으로 계산된다 1:</p>
<p><span class="math math-display">
C^A_i(p) = \omega_e(p,p) \cdot C(p) + \sum_{q \in N_p, q \neq p} \omega_e(q,p) \cdot C^A_i(q)
</span><br />
이 방식은 한 번의 패스(pass)만으로 전체 특징 맵에 대한 필터링을 효율적으로 수행할 수 있게 하여, SGF를 심층 신경망에 실용적으로 통합할 수 있도록 만든다.</p>
<h2>6.  실험 및 성능 평가</h2>
<p>DSMNet의 효과를 입증하기 위해, 논문에서는 광범위한 실험을 통해 제안된 구성 요소의 기여도를 분석하고, 기존 최신 모델들과의 성능을 다각적으로 비교했다.</p>
<h3>6.1 학습 및 평가 데이터셋</h3>
<ul>
<li>
<p><strong>학습 데이터셋:</strong> 모델의 도메인 일반화 능력을 엄격하게 평가하기 위해, 학습에는 순수 합성 데이터셋만을 사용했다. 주로 사용된 데이터셋은 35,000개 이상의 스테레오 이미지 쌍을 포함하는 대규모 합성 데이터셋인 SceneFlow이다. 추가적으로, 자율주행 시뮬레이터로 생성된 CARLA 데이터셋(20,000개 이미지 쌍)을 학습에 사용하여 데이터의 다양성을 높였다.1</p>
</li>
<li>
<p><strong>평가 데이터셋:</strong> 학습된 모델은 파인튜닝 없이 다양한 실제 환경 데이터셋에 직접 적용하여 성능을 평가했다. 여기에는 자율주행 시나리오의 KITTI 2012 및 2015, 고해상도 실내외 장면을 포함하는 Middlebury, 그리고 다양한 실제 객체 및 장면을 담은 ETH3D 데이터셋이 포함된다.1</p>
</li>
</ul>
<h3>6.2 평가 지표 (Evaluation Metrics)</h3>
<p>모델의 성능은 주로 불일치 맵의 정확도를 측정하는 표준 지표들을 사용하여 평가되었다.</p>
<ul>
<li>
<p><strong>N-pixel error:</strong> 예측된 불일치 값과 실제 정답 값의 절대 오차가 <span class="math math-inline">N</span> 픽셀을 초과하는 픽셀의 비율(%)을 나타낸다. 일반적으로 2-pixel, 3-pixel, 5-pixel error 등이 사용되며, 값이 낮을수록 성능이 우수함을 의미한다.1</p>
</li>
<li>
<p><strong>D1-all:</strong> KITTI 벤치마크의 공식 평가 지표 중 하나로, 정답 값이 있는 모든 픽셀(폐색 영역 제외)에 대해 3-pixel error와 5% 상대 오차 기준을 모두 초과하는 이상치(outlier) 픽셀의 비율(%)을 의미한다.5</p>
</li>
<li>
<p><strong>EPE (End-Point Error):</strong> 모든 픽셀에 대해 예측값과 정답 값의 유클리드 거리(L2 norm)의 평균을 계산한 값이다. 주로 SceneFlow 데이터셋 평가에 사용된다.16</p>
</li>
</ul>
<h3>6.3 성능 분석</h3>
<h4>6.3.1 Table 1: DSMNet 구성 요소별 성능 기여도 분석 (Ablation Study)</h4>
<p>이 실험은 DSMNet의 핵심 구성 요소인 도메인 정규화(DN)와 구조 보존 그래프 기반 필터(SGF)가 각각 성능에 얼마나 기여하는지를 분석한다. 모든 모델은 SceneFlow 데이터셋으로 학습되었다.</p>
<table><thead><tr><th>정규화 방식</th><th>SGF (특징)</th><th>SGF (비용 볼륨)</th><th>기반 모델</th><th>Middlebury (2-pixel)</th><th>KITTI (3-pixel)</th></tr></thead><tbody>
<tr><td>BN</td><td>-</td><td>-</td><td>PSMNet</td><td>39.5</td><td>16.3</td></tr>
<tr><td>BN</td><td>-</td><td>-</td><td>GANet</td><td>32.2</td><td>11.7</td></tr>
<tr><td>DN</td><td>+7</td><td>+2</td><td>PSMNet</td><td>26.1</td><td>8.5</td></tr>
<tr><td>DN</td><td>+7</td><td>+2</td><td>GANet</td><td>23.7</td><td>7.3</td></tr>
<tr><td>BN</td><td>ours</td><td>-</td><td>-</td><td>30.3</td><td>9.4</td></tr>
<tr><td>DN</td><td>ours</td><td>-</td><td>-</td><td>27.1</td><td>7.9</td></tr>
<tr><td>DN</td><td>+7</td><td>-</td><td>ours</td><td>22.9</td><td>6.8</td></tr>
<tr><td>DN</td><td>+7</td><td>+2</td><td>ours</td><td><strong>21.8</strong></td><td><strong>6.5</strong></td></tr>
</tbody></table>
<ul>
<li><strong>분석:</strong> 위 표는 DN과 SGF의 효과를 명확하게 보여준다. 기존의 배치 정규화(BN)를 DN으로 교체하는 것만으로도 PSMNet과 GANet 모두에서 에러율이 크게 감소했다. 여기에 SGF 계층을 특징 추출 네트워크와 비용 볼륨에 추가했을 때 성능은 더욱 향상되어, 최종 DSMNet 모델(DN + SGF 7개 + SGF 2개)이 가장 낮은 에러율을 기록했다. 이는 제안된 두 기술이 독립적으로도 효과적이며, 함께 사용될 때 강력한 시너지 효과를 발휘하여 도메인 일반화 성능을 극대화함을 정량적으로 입증한다.</li>
</ul>
<h4>6.3.2 Table 2: 합성 데이터 학습 기반 최신 모델과의 교차 도메인 성능 비교</h4>
<p>이 표는 동일한 합성 데이터(SceneFlow 또는 SceneFlow+CARLA)로만 학습된 DSMNet과 다른 최신 모델들의 성능을 다양한 실제 데이터셋에서 직접 비교한다.</p>
<table><thead><tr><th>모델</th><th>KITTI 2012</th><th>KITTI 2015</th><th>Middlebury (full)</th><th>ETH3D</th></tr></thead><tbody>
<tr><td><strong>학습셋: SceneFlow</strong></td><td></td><td></td><td></td><td></td></tr>
<tr><td>SGM</td><td>7.1</td><td>7.6</td><td>38.1</td><td>12.9</td></tr>
<tr><td>PSMNet</td><td>15.1</td><td>16.3</td><td>39.5</td><td>23.8</td></tr>
<tr><td>GANet</td><td>10.1</td><td>11.7</td><td>32.2</td><td>14.1</td></tr>
<tr><td>Our DSMNet</td><td><strong>6.2</strong></td><td><strong>6.5</strong></td><td><strong>21.8</strong></td><td><strong>6.2</strong></td></tr>
<tr><td><strong>학습셋: SceneFlow + Carla</strong></td><td></td><td></td><td></td><td></td></tr>
<tr><td>PSMNet</td><td>10.3</td><td>11.0</td><td>35.5</td><td>20.3</td></tr>
<tr><td>GANet</td><td>7.2</td><td>7.6</td><td>31.9</td><td>13.5</td></tr>
<tr><td>Our DSMNet</td><td><strong>3.9</strong></td><td><strong>4.1</strong></td><td><strong>20.1</strong></td><td><strong>6.0</strong></td></tr>
</tbody></table>
<ul>
<li><strong>분석:</strong> 결과는 압도적이다. 모든 평가 데이터셋과 학습 조건에서 DSMNet은 SGM, PSMNet, GANet 등 당대의 모든 SOTA 모델들을 큰 차이로 능가했다. 예를 들어, SceneFlow+Carla로 학습했을 때 KITTI 2015에서의 3-pixel error는 GANet이 7.6%인 반면, DSMNet은 4.1%에 불과했다. 이는 DSMNet이 학습 데이터의 도메인 편향을 성공적으로 극복하고, 실제 환경에 대한 뛰어난 일반화 능력을 갖추었음을 명확히 증명하는 결과이다.</li>
</ul>
<h4>6.3.3 Table 3: KITTI 2015 벤치마크 성능 비교</h4>
<p>이 실험은 DSMNet의 일반화 능력이 어느 정도인지를 보여주는 가장 극적인 결과 중 하나이다. 합성 데이터로만 학습한 DSMNet을 KITTI 데이터셋으로 직접 학습하거나 파인튜닝한 다른 모델들과 비교했다.</p>
<table><thead><tr><th>모델</th><th>학습 데이터</th><th>에러율 (%)</th></tr></thead><tbody>
<tr><td><strong>Our DSMNet</strong></td><td><strong>Synthetic</strong></td><td><strong>3.71</strong></td></tr>
<tr><td>MC-CNN-acrt</td><td>Kitti-gt</td><td>3.89</td></tr>
<tr><td>DispNetC</td><td>Kitti-gt</td><td>4.34</td></tr>
<tr><td>MADNet-finetune</td><td>Kitti-gt</td><td>4.66</td></tr>
<tr><td>MADNet</td><td>Kitti (no gt)</td><td>8.23</td></tr>
<tr><td>Unsupervised</td><td>Kitti (no gt)</td><td>9.91</td></tr>
</tbody></table>
<ul>
<li><strong>분석:</strong> 놀랍게도, DSMNet은 목표 도메인인 KITTI 데이터를 전혀 보지 않았음에도 불구하고, KITTI 학습 데이터(정답 값 포함)를 사용하여 학습한 MC-CNN-acrt나 DispNetC와 같은 지도학습 기반 모델들보다 더 낮은 에러율(3.71%)을 기록했다. 이는 DSMNet이 단순히 도메인 간의 시각적 차이를 극복하는 수준을 넘어, 도메인에 불변하는 본질적인 기하학적 특징을 학습했음을 시사하는 강력한 증거이다.</li>
</ul>
<h4>6.3.4 Table 4: 파인튜닝 후 도메인 내 성능 비교</h4>
<p>이 실험은 DSMNet의 일반화 능력이 특정 도메인에 대한 전문성(specialization)을 희생시킨 결과가 아님을 보여주기 위해, 모든 모델을 KITTI 2015 학습 데이터로 파인튜닝한 후의 성능을 비교했다.</p>
<table><thead><tr><th>모델</th><th>Non-Occluded</th><th>All Area</th></tr></thead><tbody>
<tr><td>GANet + Our SGF</td><td><strong>1.58</strong></td><td><strong>1.77</strong></td></tr>
<tr><td>GANet-deep</td><td>1.63</td><td>1.81</td></tr>
<tr><td><strong>DSMNet-finetune</strong></td><td>1.71</td><td><strong>1.90</strong></td></tr>
<tr><td>AcfNet</td><td>1.72</td><td>1.89</td></tr>
<tr><td>GANet-15</td><td>1.73</td><td>1.93</td></tr>
<tr><td>PSMNet</td><td>2.14</td><td>2.32</td></tr>
</tbody></table>
<ul>
<li><strong>분석:</strong> 파인튜닝 후, DSMNet은 여전히 SOTA 수준의 높은 성능(All Area 1.90%)을 달성했다. 이는 DSMNet이 획득한 도메인 불변 표현이 특정 도메인에 대한 적응을 방해하지 않으며, 오히려 더 나은 초기화(initialization) 지점을 제공하여 파인튜닝 시 더 높은 성능에 도달할 수 있는 견고한 기반이 됨을 시사한다. 즉, 일반화와 전문화는 상충 관계가 아니라 상호 보완적일 수 있음을 보여준다.</li>
</ul>
<h2>7.  응용 분야 및 중요성</h2>
<p>DSMNet이 제시한 도메인 불변 학습 방법론은 단순히 학술적인 성과를 넘어, 다양한 실제 산업 분야에 깊은 영향을 미칠 잠재력을 가지고 있다.</p>
<h3>7.1 자율주행 및 로보틱스</h3>
<p>자율주행차와 로봇은 예측 불가능하고 끊임없이 변화하는 실제 환경에서 작동해야 한다. 이러한 시스템의 3D 인식 모듈을 개발할 때, 모든 가능한 도로 조건, 날씨, 조명 상황에 대한 실제 데이터를 수집하여 학습시키는 것은 거의 불가능하다. DSMNet은 시뮬레이션 환경에서 안전하고 저렴하게 대규모 학습을 진행한 후, 학습 데이터에 없었던 실제 환경에서도 안정적으로 3D 깊이 정보를 추정할 수 있는 가능성을 열어준다.1 이는 LiDAR와 같은 고가의 센서에 대한 의존도를 낮추고, 상대적으로 저렴한 스테레오 카메라의 활용도를 극대화하여 자율 시스템의 상용화를 앞당기는 데 기여할 수 있다.11</p>
<h3>7.2 D 재구성 및 증강현실(AR)</h3>
<p>고품질의 3D 모델을 생성하거나, 현실 공간에 가상의 객체를 자연스럽게 증강시키기 위해서는 다양한 환경 조건에서 촬영된 이미지로부터 일관되고 정확한 깊이 정보가 필수적이다. DSMNet과 같이 도메인 변화에 강건한 모델은 조명이나 촬영 환경이 달라져도 안정적인 깊이 맵을 제공할 수 있다. 이는 여러 장의 사진을 이용한 3D 재구성(3D reconstruction)의 품질을 향상시키고, 사실적인 AR 콘텐츠를 생성하며 가상 환경을 구축하는 데 핵심적인 기술로 활용될 수 있다.3</p>
<h3>7.3 원격 탐사 (Remote Sensing)</h3>
<p>위성이나 항공기에서 촬영한 스테레오 이미지를 이용하여 지표면의 고도를 나타내는 디지털 표면 모델(Digital Surface Model, DSM)을 생성하는 것은 원격 탐사 분야의 중요한 응용 중 하나이다.18 이러한 이미지는 계절, 날씨, 촬영 시각에 따라 지표면의 색상과 질감이 크게 달라질 수 있다. DSMNet의 도메인 불변 학습 원리는 이러한 시각적 변화에도 불구하고 일관된 지형 정보를 추출하는 데 적용될 수 있어, 보다 신뢰성 높은 DSM 생성에 기여할 수 있다.19</p>
<h3>7.4 학술적 의의</h3>
<p>DSMNet은 Sim2Real 문제에 대한 효과적인 아키텍처 수준의 해결책을 제시함으로써, 이후의 도메인 일반화 연구에 중요한 기준점이자 강력한 베이스라인이 되었다. 이 연구 이후, 많은 후속 연구들이 DSMNet의 아이디어를 확장하거나 비교 대상으로 삼으며 발전해왔다. 예를 들어, 좌우 시점 간의 특징 일관성(Feature Consistency)을 명시적으로 학습하는 연구 2, 또는 다양한 데이터 증강(Data Augmentation) 기법을 통해 도메인 불변성을 학습하는 연구 9 등은 모두 DSMNet이 제시한 문제의식과 방향성 위에서 이루어진 성과라 할 수 있다. 이처럼 DSMNet은 스테레오 매칭 분야의 연구 패러다임을 ‘정확도’ 중심에서 ‘일반화’ 중심으로 전환하는 데 결정적인 역할을 했다.</p>
<h2>8.  한계 및 향후 연구 방향</h2>
<p>DSMNet은 도메인 일반화 문제에 대한 획기적인 해결책을 제시했지만, 이는 연구의 끝이 아닌 새로운 시작이었다. DSMNet의 성공은 후속 연구자들이 더 정교하고 다양한 접근법을 탐구하도록 자극했으며, 이 과정에서 DSMNet의 잠재적 한계와 새로운 연구 방향이 드러났다.</p>
<h3>8.1 DSMNet 이후의 연구 동향</h3>
<p>DSMNet이 네트워크 아키텍처 자체를 재설계하여 문제를 해결하려 했다면, 후속 연구들은 다른 차원에서 도메인 불변성을 확보하려는 시도를 했다.</p>
<ul>
<li>
<p><strong>특징 일관성 기반 학습 (Feature Consistency Learning):</strong> 이 접근법은 스테레오 이미지 쌍에서 물리적으로 동일한 지점을 나타내는 픽셀들은, 딥러닝 네트워크를 통과한 후의 특징 공간에서도 서로 가까운 표현을 가져야 한다는 직관에 기반한다. FCStereo와 같은 연구들은 대조 학습(contrastive learning)과 같은 기법을 도입하여, 매칭되는 픽셀 쌍의 특징은 서로 가깝게, 매칭되지 않는 픽셀 쌍의 특징은 서로 멀어지도록 명시적인 손실 함수를 추가했다. 이는 아키텍처 변경 없이도 네트워크가 더 강건한 매칭 표현을 학습하도록 유도한다.2</p>
</li>
<li>
<p><strong>계층적 데이터 증강 (Hierarchical Data Augmentation):</strong> HVT와 같은 연구들은 학습 데이터 자체를 변형하여 도메인 일반화 능력을 키우는 데 집중했다. 이들은 원본 합성 데이터에 전역적(global), 지역적(local), 픽셀 단위의 다양한 시각적 변환을 가하여 수많은 가상의 새로운 도메인을 생성한다. 모델이 이처럼 다양화된 데이터를 학습하게 함으로써, 특정 도메인의 피상적인 통계적 특성(shortcut)에 의존하는 것을 방지하고, 본질적인 구조적 특징을 학습하도록 강제한다.9</p>
</li>
<li>
<p><strong>보조 작업을 통한 학습 (Auxiliary Task Learning):</strong> Masked-stereo와 같은 모델들은 스테레오 매칭이라는 주된 작업 외에, 이미지의 일부를 가리고 이를 복원하게 하는 이미지 복원(image reconstruction)과 같은 보조 작업을 함께 학습시킨다. 이러한 방식은 모델이 단순히 불일치 값만 예측하는 것을 넘어, 이미지의 전체적인 구조와 컨텍스트를 더 깊이 이해하도록 유도하여 일반화 성능을 향상시킨다.9</p>
</li>
</ul>
<p>이러한 후속 연구들의 등장은 도메인 일반화 문제가 단일한 해결책보다는 다각적인 접근이 필요함을 시사한다. DSMNet이 아키텍처를 통해 스타일과 지역 텍스처 문제를 효과적으로 해결했다면, 후속 연구들은 특징 표현의 일관성이나 데이터 분포의 다양성과 같은 더 미묘한 측면을 다루기 위해 정교한 손실 함수나 데이터 중심의 전략을 발전시켰다. 이는 DSMNet의 아이디어가 실패했다기보다는, 그 해결 범위의 한계를 보여주는 것이며, 문제의 다면성을 드러내는 자연스러운 학문적 발전 과정으로 볼 수 있다.</p>
<h3>8.2 대규모 데이터셋과 파운데이션 모델의 부상</h3>
<p>최근 컴퓨터 비전 분야의 또 다른 거대한 흐름은 특정 작업과 데이터셋에 국한되지 않는 범용적인 ’파운데이션 모델(Foundation Model)’의 등장이다. 스테레오 매칭 분야에서도 특정 합성 데이터셋(e.g., SceneFlow)에 대한 의존에서 벗어나, 다양한 실제 및 합성 데이터를 통합한 초거대 데이터셋으로 대규모 모델을 사전 학습하려는 움직임이 나타나고 있다.21 이는 개별 모델의 아키텍처나 학습 전략을 통해 일반화 능력을 확보하려는 기존의 패러다임을 넘어, 데이터의 규모와 다양성 그 자체로 일반화 문제를 해결하려는 새로운 시도이다.</p>
<h3>8.3 도전적인 시나리오</h3>
<p>DSMNet을 포함한 대부분의 현재 모델들은 여전히 특정 시나리오에서 한계를 보인다. 예를 들어, 원격 탐사 이미지에서 흔히 발견되는 넓은 사막이나 숲과 같이 텍스처가 거의 없는(texture-less) 영역, 혹은 아파트 단지처럼 반복적인 패턴이 극심한 영역에서는 매칭의 모호성이 크게 증가한다.19 또한, 객체 간의 경계가 복잡하여 심한 폐색(occlusion)이 발생하는 경우에도 정확한 깊이 추정은 여전히 어려운 과제로 남아있다. 이러한 극한 환경에 대한 강건성을 확보하는 것은 향후 스테레오 매칭 연구가 해결해야 할 중요한 과제이다.</p>
<h2>9.  결론</h2>
<h3>9.1 DSMNet의 핵심 기여 요약</h3>
<p>DSMNet은 ’도메인 정규화(Domain Normalization, DN)’와 ’구조 보존 그래프 기반 필터(Structure-preserving Graph-based Filter, SGF)’라는 두 가지 독창적이고 혁신적인 모듈을 제안함으로써, 딥러닝 기반 스테레오 매칭 분야의 고질적인 문제였던 ‘Sim2Real’ 도메인 일반화 성능을 획기적으로 개선했다. 합성 데이터만으로 학습했음에도 불구하고, 다양한 실제 환경 데이터셋에서 기존의 최신 모델들을 압도하는 성능을 보였으며, 심지어 목표 도메인 데이터로 학습한 일부 모델보다도 우수한 결과를 달성하는 놀라운 성과를 거두었다.</p>
<h3>9.2 기술적 의의</h3>
<p>DSMNet의 가장 큰 기술적 의의는 문제 해결을 위해 아키텍처 자체를 재설계하여 접근했다는 점에 있다. 도메인에 따라 민감하게 변하는 색상, 조명, 텍스처와 같은 피상적인 특징 대신, 변화에 강건한 이미지의 구조적, 기하학적 특징을 학습하도록 네트워크의 기본 구성 요소(정규화, 필터링)를 근본적으로 바꾸었다. 이는 단순히 더 깊거나 복잡한 네트워크를 만드는 것을 넘어, 문제의 본질을 파악하고 그에 맞는 구조를 설계했다는 점에서 높은 평가를 받는다. DSMNet이 구축한 이 견고한 기반 위에서, 후속 연구들은 보다 정교한 손실 함수나 데이터 증강 기법을 탐구하며 분야를 더욱 발전시킬 수 있었다.</p>
<h3>9.3 영향 및 전망</h3>
<p>DSMNet의 성공은 자율주행, 로보틱스, 3D 재구성 등 실제 환경에서의 3D 인식 기술 상용화를 위한 중요한 기술적 진보를 이루었다. 시뮬레이션 환경에서의 학습만으로 실제 세계에서 작동 가능한 모델을 만들 수 있다는 가능성을 보여줌으로써, 연구 개발의 비용과 시간을 크게 단축할 수 있는 길을 열었다. DSMNet은 컴퓨터 비전 분야에서 도메인 일반화 문제를 해결하기 위한 중요한 이정표로 기록되었으며, 그 핵심 아이디어는 여전히 유효하다. 향후, 다양한 데이터를 포괄하는 대규모 파운데이션 모델의 시대가 도래함에 따라, DSMNet이 제시한 도메인 불변 학습의 원리는 이러한 거대 모델의 강건성을 더욱 향상시키는 데 중요한 역할을 할 것이며, 더욱 강력한 시너지를 발휘할 것으로 기대된다.</p>
<h2>10. 참고 자료</h2>
<ol>
<li>Domain-invariant Stereo Matching Networks - European Computer Vision Association, https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470409.pdf</li>
<li>Revisiting Domain Generalized Stereo Matching Networks from a Feature Consistency Perspective - White Rose Research Online, https://eprints.whiterose.ac.uk/id/eprint/188119/1/Zhang_Revisiting_Domain_Generalized_Stereo_Matching_Networks_From_a_Feature_Consistency_CVPR_2022_paper.pdf</li>
<li>RAFT-Stereo: Multilevel Recurrent Field Transforms for Stereo, https://www.alphaxiv.org/overview/2109.07547</li>
<li>Domain-invariant Stereo Matching Networks | Request PDF - ResearchGate, https://www.researchgate.net/publication/337671541_Domain-invariant_Stereo_Matching_Networks</li>
<li>Accuracy and efficiency stereo matching network with adaptive feature modulation - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC11045109/</li>
<li>Adaptive Recurrent Iterative Updating Stereo Matching Network - Scientific Research Publishing, https://www.scirp.org/pdf/jcc_2023033013515709.pdf</li>
<li>Domain-Invariant Stereo Matching Networks | Request PDF - ResearchGate, https://www.researchgate.net/publication/346002598_Domain-Invariant_Stereo_Matching_Networks</li>
<li>Revisiting Domain Generalized Stereo Matching Networks From a Feature Consistency Perspective - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Revisiting_Domain_Generalized_Stereo_Matching_Networks_From_a_Feature_Consistency_CVPR_2022_paper.pdf</li>
<li>Domain Generalized Stereo Matching with Uncertainty-guided Data Augmentation - arXiv, https://arxiv.org/html/2508.01303v1</li>
<li>Constraining Multimodal Distribution for Domain Adaptation in Stereo Matching - arXiv, https://arxiv.org/html/2504.21302v1</li>
<li>Revisiting Non-Parametric Matching Cost Volumes for Robust and Generalizable Stereo Matching, https://proceedings.neurips.cc/paper_files/paper/2022/file/6794f555524c9069e26970a408d353cc-Paper-Conference.pdf</li>
<li>[1911.13287] Domain-invariant Stereo Matching Networks - arXiv, https://arxiv.org/abs/1911.13287</li>
<li>Domain Generalized Stereo Matching via Hierarchical Visual Transformation - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_Domain_Generalized_Stereo_Matching_via_Hierarchical_Visual_Transformation_CVPR_2023_paper.pdf</li>
<li>feihuzhang/DSMNet: Domain-invariant Stereo Matching Networks - GitHub, https://github.com/feihuzhang/DSMNet</li>
<li>Review of Stereo Matching Algorithms Based on Deep Learning - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC7125450/</li>
<li>OpenStereo: A Comprehensive Benchmark for Stereo Matching and Strong Baseline - arXiv, https://arxiv.org/html/2312.00343v8</li>
<li>Autonomous Navigation Technology for Robots and Its Applications in Intelligent Transportation and Industrial Fields, https://hsetdata.com/index.php/ojs/article/view/836</li>
<li>Comparative Analysis of Deep Learning-Based Stereo Matching and Multi-View Stereo for Urban DSM Generation - MDPI, https://www.mdpi.com/2072-4292/17/1/1</li>
<li>Disparity Estimation of High-Resolution Remote Sensing Images with Dual-Scale Matching Network - MDPI, https://www.mdpi.com/2072-4292/13/24/5050</li>
<li>Revisiting Domain Generalized Stereo Matching Networks from a Feature Consistency Perspective - Griffith Research Online, https://research-repository.griffith.edu.au/bitstreams/da285e74-f959-4870-afbe-10218a882318/download</li>
<li>Stereo Anything: Unifying Stereo Matching with Large-Scale Mixed Data - arXiv, https://arxiv.org/html/2411.14053v1</li>
<li>Disparity Estimation of High-Resolution Remote Sensing Images with Dual-Scale Matching Network - ResearchGate, https://www.researchgate.net/publication/357012045_Disparity_Estimation_of_High-Resolution_Remote_Sensing_Images_with_Dual-Scale_Matching_Network</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>