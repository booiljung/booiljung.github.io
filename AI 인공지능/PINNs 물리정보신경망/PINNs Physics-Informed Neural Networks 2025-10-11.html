<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:물리 정보 신경망(Physics-Informed Neural Networks, PINNs)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>물리 정보 신경망(Physics-Informed Neural Networks, PINNs)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">물리정보신경망 (Physics-Informed Neural Networks, PINNs)</a> / <span>물리 정보 신경망(Physics-Informed Neural Networks, PINNs)</span></nav>
                </div>
            </header>
            <article>
                <h1>물리 정보 신경망(Physics-Informed Neural Networks, PINNs)</h1>
<p>2025-10-11, G25DR</p>
<h2>1.  과학적 기계 학습의 새로운 패러다임</h2>
<h3>1.1  배경: 데이터와 물리 모델의 간극</h3>
<p>지난 수십 년간 과학 및 공학 분야의 발전은 두 가지 핵심적인 계산 패러다임에 의해 주도되어 왔다. 첫 번째는 유한요소법(Finite Element Method, FEM), 유한차분법(Finite Difference Method, FDM) 등으로 대표되는 전통적인 수치 해석 기법이다. 이러한 방법론들은 물리 법칙을 지배하는 편미분방정식(Partial Differential Equations, PDEs)에 대한 높은 충실도(high-fidelity)를 보장하며, 수많은 공학 문제 해결의 근간을 이루어 왔다. 그러나 이들은 복잡한 기하학적 형상에 대한 고품질의 계산 격자(mesh) 생성에 상당한 노력을 요구하며, 이는 종종 전체 해석 과정에서 가장 많은 시간을 소요하는 병목 구간이 된다.1 더욱이, 문제의 차원이 증가함에 따라 계산 비용이 기하급수적으로 증가하는 ’차원의 저주(curse of dimensionality)’에 본질적으로 취약하며, 실험이나 현장에서 측정된 데이터를 모델에 통합하는 과정이 복잡하고 비효율적이라는 근본적인 한계를 지닌다.1</p>
<p>두 번째 패러다임은 인공 신경망(Artificial Neural Networks, ANNs)을 필두로 한 데이터 기반 기계 학습이다. 특히 심층 신경망(Deep Neural Networks, DNNs)은 대규모 데이터셋으로부터 복잡하고 비선형적인 패턴을 학습하는 데 있어 전례 없는 성공을 거두었다. 하지만 이러한 순수 데이터 기반 모델들은 물리적 시스템을 모델링할 때 몇 가지 중대한 약점을 드러낸다. 가장 큰 문제는 대규모의 레이블된 훈련 데이터를 필요로 한다는 점이다.3 많은 과학 및 공학 분야에서는 고품질의 데이터를 대량으로 확보하는 것이 비용적으로나 기술적으로 불가능한 경우가 많다. 설령 데이터가 충분하더라도, 모델의 예측이 에너지 보존 법칙이나 질량 보존 법칙과 같은 기본적인 물리 원리를 준수한다는 보장이 없다. 이로 인해 훈련 데이터가 부족하거나 노이즈가 포함된 영역에서는 물리적으로 타당하지 않거나 심지어는 터무니없는 예측을 생성할 위험이 상존한다.5</p>
<p>이처럼 물리 법칙의 엄밀함을 따르는 전통적 수치 해석과 데이터의 유연성을 활용하는 기계 학습 사이에는 명백한 간극이 존재했다. 한쪽은 데이터 활용에, 다른 한쪽은 물리적 일관성 보장에 어려움을 겪었다. 이러한 배경 속에서 두 패러다임의 장점을 결합하여 간극을 메우려는 시도가 자연스럽게 대두되었다. 즉, 데이터의 통계적 패턴 학습 능력과 물리 법칙의 강력한 사전 지식(prior knowledge)을 통합하여, 데이터 효율성을 높이고 예측의 물리적 타당성을 보장하는 새로운 하이브리드 모델링 프레임워크의 필요성이 절실해진 것이다.6</p>
<h3>1.2  PINN의 등장과 정의</h3>
<p>이러한 요구에 부응하여 등장한 혁신적인 접근법이 바로 물리 정보 신경망(Physics-Informed Neural Networks, PINNs)이다.6 PINN은 심층 신경망의 학습 과정에 편미분방정식(PDE)으로 명확하게 기술되는 물리 법칙을 직접 통합하는 새로운 종류의 기계 학습 모델이다.5 때로는 이론 훈련 신경망(Theory-Trained Neural Networks, TTNs)이라고도 불리는 PINN의 핵심 아이디어는, 신경망이 단순히 주어진 데이터를 모방하는 것을 넘어, 시스템을 지배하는 근본적인 물리 법칙을 ’존중’하도록 훈련시키는 데 있다.8</p>
<p>이 목표는 신경망의 손실 함수(loss function)를 독창적으로 설계함으로써 달성된다. PINN의 손실 함수는 전통적인 데이터 기반 손실 항(data loss term)과 새롭게 추가된 물리 기반 손실 항(physics loss term)의 합으로 구성된다.9 데이터 손실 항은 신경망의 예측값이 관측된 데이터(예: 초기 조건, 경계 조건)와 얼마나 일치하는지를 측정하는 역할을 한다. 반면, 물리 기반 손실 항은 신경망의 출력이 지배 PDE를 얼마나 잘 만족시키는지를 정량화한다. 이는 계산 영역 내부에 무작위로 선택된 지점들, 즉 배열점(collocation points)에서 PDE의 잔차(residual)를 계산하고, 이 잔차의 크기를 최소화하는 방식으로 구현된다.11</p>
<p>이러한 구조를 통해 PINN은 보편적 함수 근사기(universal function approximators)로서의 역할을 수행하며, 물리 법칙이라는 강력한 제약을 통해 학습 과정을 안내한다.8 그 결과, 훈련 데이터가 매우 부족하거나 심지어 전혀 없는 영역에서도 물리적으로 일관된 해를 추론하고 일반화(generalization)하는 능력이 비약적으로 향상된다.8 이 독특한 접근법 덕분에 PINN은 과학적 기계 학습(Scientific Machine Learning) 분야의 핵심적인 기둥으로 빠르게 자리 잡았으며, 데이터와 물리 모델을 융합하는 새로운 연구 방향을 제시하고 있다.13</p>
<p>PINN은 단순한 ’물리학을 가미한 회귀 분석’을 넘어서는 깊은 의미를 지닌다. 이는 물리적 제약을 통한 강력한 정규화(regularization) 기법으로 이해될 수 있다. 일반적인 신경망은 데이터가 부족하거나 노이즈가 많을 때 훈련 데이터에 과적합(overfitting)될 본질적인 위험을 안고 있다.5 정규화는 모델의 복잡도를 제한하여 이러한 과적합을 방지하는 핵심적인 기법이다. PINN의 손실 함수에서 물리 법칙 잔차 항(<span class="math math-inline">L_{physics}</span>)은 신경망의 출력이 PDE를 만족하도록 강제함으로써, 신경망이 학습할 수 있는 무한한 함수 공간을 물리적으로 타당한 해들로 구성된 훨씬 작은 부분 공간(subspace)으로 제한하는 역할을 한다.15 즉, 물리 법칙은 데이터가 없는 영역에서 해의 형태와 거동을 안내하는 매우 정보 밀도가 높은 사전 지식으로 작용한다. 이는 기존의 L1/L2 정규화와 같이 모델의 가중치 크기만을 제한하는 방식보다 훨씬 강력하고 유의미한 제약 조건이며, 이를 통해 PINN은 데이터 효율성을 극대화하고 일반화 성능을 획기적으로 향상시킨다.5</p>
<p>더 나아가, PINN의 등장은 과학 계산의 패러다임을 ’순방향 문제 해결사(forward problem solver)’에서 ’역문제 및 데이터 동화 플랫폼(inverse problem and data assimilation platform)’으로 전환시키는 중요한 계기가 되었다. 전통적인 수치 해석 기법은 주로 순방향 문제, 즉 모든 시스템 매개변수와 경계 조건이 완벽하게 주어졌을 때 시스템의 반응을 예측하는 데 초점을 맞추어 왔다.2 반면, 역문제(inverse problem)는 제한적이고 노이즈 섞인 관측 결과(데이터)로부터 시스템의 알려지지 않은 원인이나 매개변수(예: 재료의 물성치, 미지의 열원)를 추론하는 훨씬 더 어려운 과제이다.3 PINN의 손실 함수는 데이터 항과 물리 법칙 항을 동시에 최적화하는 구조를 가지고 있다. 이는 미지의 물리 매개변수를 신경망의 학습 가능한 가중치와 함께 또 다른 최적화 변수로 설정하여, 훈련 과정에서 자연스럽게 추론할 수 있게 한다.3 따라서 PINN은 별도의 복잡한 데이터 동화(data assimilation) 프레임워크 없이도, 희소하고 노이즈 낀 측정 데이터를 물리 모델에 직접 통합하여 역문제를 해결하는 우아하고 통합된 프레임워크를 제공한다. 이는 전통적인 방법론이 가진 근본적인 한계를 극복하는 중요한 패러다임의 전환이라 할 수 있다.2</p>
<h3>1.3  보고서의 구조와 목표</h3>
<p>본 보고서는 과학 및 공학 분야의 연구자와 전문가들을 대상으로 물리 정보 신경망(PINNs)에 대한 심층적이고 종합적인 분석을 제공하는 것을 목표로 한다. 이를 위해, 보고서는 다음과 같은 체계적인 구조로 전개된다.</p>
<p>먼저, 2장에서는 PINN의 근간을 이루는 신경망 아키텍처, 물리 정보 손실 함수의 수학적 구성, 그리고 이 모든 것을 가능하게 하는 핵심 기술인 자동 미분(Automatic Differentiation)의 역할에 대해 상세히 기술한다. 3장에서는 PINN 분야의 초석을 다진 Raissi, Perdikaris, Karniadakis의 선구적인 연구들을 조명하고, PINN이 가진 본질적인 장점과 내재적 한계를 명확하게 분석한다.</p>
<p>4장에서는 PINN을 전통적인 수치 해석 기법인 FEM 및 FDM과 다각적으로 비교하여, 정확도, 계산 비용, 적용 유연성 측면에서 각 방법론의 상대적인 위치를 재정립한다. 5장에서는 ’PINN Zoo’라 불리는 다양한 변형 모델들, 특히 확장성 문제를 해결하기 위한 XPINN, 보존 법칙을 강제하는 cPINN, 그리고 불확실성을 정량화하는 B-PINN의 핵심 원리와 목적을 심도 있게 탐구한다.</p>
<p>6장에서는 유체 역학, 열 전달, 금융 공학과 같은 구체적인 응용 분야에서의 성공적인 적용 사례를 통해 PINN의 실용적 가치와 잠재력을 입증한다. 마지막으로 7장에서는 현재까지의 논의를 종합하여 PINN 연구의 미래 전망을 제시하고, 실용화를 위해 해결해야 할 주요 도전 과제들을 논하며 보고서를 마무리한다. 본 보고서는 이러한 체계적인 분석을 통해 독자들이 PINN의 이론적 깊이와 실용적 넓이를 모두 이해할 수 있도록 돕고자 한다.</p>
<h2>2.  PINN의 구조와 핵심 원리</h2>
<p>물리 정보 신경망의 혁신성은 그 구조와 학습 원리의 독창적인 결합에 있다. PINN은 표준적인 신경망 아키텍처를 기반으로 하되, 물리 법칙을 통합한 특수한 손실 함수와 이를 계산하기 위한 자동 미분 기술을 핵심 요소로 삼는다. 이 세 가지 요소가 유기적으로 결합하여 데이터와 물리 법칙을 동시에 만족시키는 해를 학습하는 강력한 프레임워크를 구성한다.</p>
<h3>2.1  신경망 아키텍처</h3>
<p>PINN의 근간을 이루는 신경망은 일반적으로 다층 퍼셉트론(Multi-Layer Perceptron, MLP)으로 알려진 완전 연결 피드포워드 신경망(fully-connected feedforward neural network)이다.17 이는 보편적 근사 정리(Universal Approximation Theorem)에 따라 충분한 수의 뉴런을 가질 경우 어떠한 연속 함수도 근사할 수 있는 능력을 지니기 때문이다.19 PINN 아키텍처는 입력, 은닉층, 출력의 세 부분으로 구성된다.</p>
<h4>2.1.1 입력 (Input)</h4>
<p>신경망의 입력은 해를 구하고자 하는 물리 문제의 독립 변수들이다. 대부분의 물리 시스템은 시간과 공간에 따라 변화하므로, 입력은 일반적으로 시간 좌표(<span class="math math-inline">t</span>)와 공간 좌표(<span class="math math-inline">x, y, z</span>)의 조합으로 구성된다.17 예를 들어, 1차원 공간에서의 시간에 따른 온도 변화를 모델링한다면 입력은 <span class="math math-inline">(x, t)</span>가 된다. 이처럼 PINN은 특정 시공간 지점을 입력받아 해당 지점에서의 물리량을 출력하는 연속적인 함수, 즉 신경장(neural field)을 학습하는 것을 목표로 한다.</p>
<h4>2.1.2 은닉층 (Hidden Layers)</h4>
<p>입력층과 출력층 사이에 위치하는 은닉층은 PINN의 핵심적인 계산을 수행하는 부분이다. 각 은닉층은 다수의 뉴런(neuron)으로 구성되며, 이전 층의 모든 뉴런과 다음 층의 모든 뉴런이 가중치(weight)와 편향(bias)으로 연결된 완전 연결 구조를 가진다.17 은닉층의 깊이(층의 수)와 너비(층당 뉴런의 수)는 문제의 복잡도에 따라 결정되는 중요한 하이퍼파라미터이다. 더 깊고 넓은 네트워크는 더 복잡하고 비선형적인 물리 현상을 포착할 수 있는 표현력을 갖지만, 동시에 훈련이 더 어렵고 과적합의 위험이 커질 수 있다.18</p>
<h4>2.1.3 활성화 함수 (Activation Functions)</h4>
<p>각 뉴런은 입력 신호의 가중합에 비선형 활성화 함수(activation function)를 적용하여 다음 층으로 신호를 전달한다. 이 비선형성 덕분에 신경망은 복잡한 함수를 근사할 수 있다. PINN에서는 활성화 함수의 선택이 특히 중요한데, 그 이유는 손실 함수를 계산하는 과정에서 PDE에 포함된 고계도 미분을 계산해야 하기 때문이다. 따라서 활성화 함수는 반드시 모든 점에서 충분히 미분 가능해야 한다.21 이러한 이유로, 미분값이 조각별 상수(piecewise constant) 형태가 되어 2계 이상의 미분값이 0이 되는 ReLU(Rectified Linear Unit) 함수는 PINN에 적합하지 않은 경우가 많다. 대신, 무한히 미분 가능한 쌍곡탄젠트(<span class="math math-inline">\tanh</span>)나 시그모이드(<code>sigmoid</code>), 사인(<span class="math math-inline">\sin</span>) 함수 등이 널리 사용된다.17</p>
<h4>2.1.4 출력 (Output)</h4>
<p>신경망의 최종 출력은 해결하고자 하는 PDE의 종속 변수, 즉 해(solution)에 해당하는 물리량이다.17 예를 들어, 푸아송 방정식(<span class="math math-inline">-\Delta u = f</span>)을 푼다면 출력은 스칼라 함수 <code>u(x, y)</code>가 될 것이고, 비압축성 나비에-스토크스 방정식을 푼다면 출력은 속도 벡터 <span class="math math-inline">(u(x,y,t), v(x,y,t))</span>와 압력 스칼라 <span class="math math-inline">p(x,y,t)</span>가 될 수 있다. 이처럼 PINN은 입력 좌표에 대한 해 함수의 근사치 <span class="math math-inline">u_{\theta}(x, t)</span>를 출력하며, 여기서 <span class="math math-inline">\theta</span>는 신경망의 모든 학습 가능한 파라미터(가중치와 편향) 집합을 의미한다.</p>
<h3>2.2  물리 정보 손실 함수</h3>
<p>PINN의 가장 핵심적인 혁신은 데이터와 물리 법칙을 하나의 최적화 목표로 통합하는 복합 손실 함수(composite loss function)의 설계에 있다.5 이 손실 함수는 신경망의 파라미터 <span class="math math-inline">\theta</span>를 최적화하는 기준이 되며, 일반적으로 데이터 불일치 손실과 물리 법칙 잔차 손실이라는 두 가지 주요 구성 요소의 가중합으로 정의된다.10</p>
<h4>2.2.1 일반적인 형태</h4>
<p>PINN의 총 손실 함수 <span class="math math-inline">\mathcal{L}(\theta)</span>는 다음과 같이 표현될 수 있다 10:</p>
<p><span class="math math-display">
\mathcal{L}(\theta) = \mathcal{L}_{data}(\theta) + \lambda \mathcal{L}_{physics}(\theta)
</span></p>
<p>여기서 <span class="math math-inline">\mathcal{L}_{data}</span>는 데이터로부터의 손실, <span class="math math-inline">\mathcal{L}_{physics}</span>는 물리 법칙으로부터의 손실을 나타내며, <span class="math math-inline">\lambda</span>는 두 손실 항의 상대적 중요도를 조절하는 가중치 하이퍼파라미터이다. 종종 초기 조건과 경계 조건에 대한 손실을 분리하여 더 세분화된 형태로 표현하기도 한다.10</p>
<h4>2.2.2 데이터 불일치 손실 (<span class="math math-inline">\mathcal{L}_{data}</span> 또는 <span class="math math-inline">\mathcal{L}_{u}</span>)</h4>
<p>이 손실 항은 신경망의 예측이 주어진 데이터와 얼마나 잘 맞는지를 측정한다. 이 데이터는 주로 문제에 명시된 초기 조건(Initial Conditions, ICs)과 경계 조건(Boundary Conditions, BCs)에서 비롯된다. 예를 들어, 특정 경계에서 해의 값이 주어지는 디리클레(Dirichlet) 경계 조건이나, 해의 미분값이 주어지는 노이만(Neumann) 경계 조건이 이에 해당한다. <span class="math math-inline">\mathcal{L}_{data}</span>는 일반적으로 해당 데이터 지점들에서 신경망의 예측값과 실제값 사이의 평균 제곱 오차(Mean Squared Error, MSE)로 계산된다.22</p>
<p>수학적으로, 경계 조건 데이터 집합 <span class="math math-inline">\{ (x_{BC}^i, u_{BC}^i) \}_{i=1}^{N_{BC}}</span>과 초기 조건 데이터 집합 <span class="math math-inline">\{ (x_{IC}^j, u_{IC}^j) \}_{j=1}^{N_{IC}}</span>이 주어졌을 때, 데이터 손실은 다음과 같이 정의할 수 있다:</p>
<p><span class="math math-display">
\mathcal{L}_{data}(\theta) = \frac{1}{N_{BC}} \sum_{i=1}^{N_{BC}} |u_{\theta}(x_{BC}^i) - u_{BC}^i|^2 + \frac{1}{N_{IC}} \sum_{j=1}^{N_{IC}} |u_{\theta}(x_{IC}^j) - u_{IC}^j|^2
</span></p>
<p>이 항은 신경망의 해가 주어진 물리적 제약 조건을 만족하도록 강제하는 역할을 한다.</p>
<h4>2.2.3 물리 법칙 잔차 손실 (<span class="math math-inline">\mathcal{L}_{physics}</span> 또는 <span class="math math-inline">\mathcal{L}_{f}</span>)</h4>
<p>이것이 PINN을 기존의 데이터 기반 신경망과 구별하는 가장 중요한 요소이다. 이 손실 항은 신경망이 근사한 해 <span class="math math-inline">u_{\theta}</span>가 지배 PDE를 얼마나 잘 만족시키는지를 측정한다. 일반적인 형태의 PDE가 <span class="math math-inline">\mathcal{F}(x, t; u, \nabla u, \nabla^2 u, \dots) = 0</span>으로 주어진다고 가정하자. 신경망의 출력 <span class="math math-inline">u_{\theta}</span>를 이 방정식에 대입하여 잔차(residual) 함수 <span class="math math-inline">r(x, t; \theta) = \mathcal{F}(x, t; u_{\theta}, \nabla u_{\theta}, \dots)</span>를 정의할 수 있다. 물리 법칙을 완벽하게 만족하는 해라면 이 잔차는 모든 점에서 0이 되어야 한다.</p>
<p>PINN은 계산 영역 <span class="math math-inline">\Omega</span>와 시간 구간 $$ 내부에 무작위로 또는 격자 형태로 샘플링된 다수의 배열점(collocation points) <span class="math math-inline">\{ (x_f^k, t_f^k) \}_{k=1}^{N_f}</span>에서 이 잔차의 크기를 최소화하는 것을 목표로 한다.10 물리 법칙 잔차 손실은 이 점들에서 계산된 잔차들의 평균 제곱 오차로 정의된다.24</p>
<p>예를 들어, 시간에 따라 변하는 비선형 PDE <span class="math math-inline">\frac{\partial u}{\partial t} + \mathcal{N}(u) = 0</span>에 대해, 물리 법칙 잔차 손실은 다음과 같이 표현된다:</p>
<p><span class="math math-display">
\mathcal{L}_{physics}(\theta) = \frac{1}{N_f} \sum_{k=1}^{N_f} \left| \frac{\partial u_{\theta}}{\partial t}(x_f^k, t_f^k) + \mathcal{N}(u_{\theta}(x_f^k, t_f^k)) \right|^2
</span></p>
<p>이 항은 레이블된 데이터가 전혀 없는 영역에서도 물리 법칙을 통해 신경망의 학습을 안내하는 역할을 수행하며, PINN의 뛰어난 일반화 성능의 원천이 된다.</p>
<h4>손실 함수 변형</h4>
<p>기본적인 MSE 기반의 배열점 손실 함수 외에도, 문제의 특성에 따라 다양한 변형된 손실 함수가 제안되었다. 예를 들어, 구조 역학 문제에서는 시스템의 전체 포텐셜 에너지를 최소화하는 원리에 기반한 에너지 기반(energy-based) 손실 함수가 사용될 수 있다.25 또한, 각 손실 항의 스케일 차이 문제를 완화하기 위해 최소 제곱 가중 잔차(Least Squares Weighted Residual, LSWR) 방법과 같이 잔차를 무차원 형태로 통합하는 기법도 개발되었다.25</p>
<p>PINN의 손실 함수는 물리적으로 이질적인 요소들, 즉 관측 데이터, 경계 조건, 그리고 PDE 잔차를 하나의 통합된 최적화 문제로 묶어준다. 그러나 이들 요소 간의 ’균형’을 맞추는 것은 PINN 훈련의 성공과 실패를 가르는 핵심적인 난제이다. <span class="math math-inline">\mathcal{L}_{data}</span>와 <span class="math math-inline">\mathcal{L}_{physics}</span>는 서로 다른 물리적 의미와 단위를 가질 뿐만 아니라, 그 크기(scale)가 훈련 과정에서 수십 배에서 수백 배까지 차이 날 수 있다.25 만약 $\mathcal{L}_{physics}`가 <span class="math math-inline">\mathcal{L}_{data}</span>를 압도하게 되면, 신경망은 PDE는 잘 만족시키지만 경계 조건을 무시하는, 물리적으로 무의미한 해를 학습할 위험이 있다. 반대의 경우에는 데이터에 과적합되어 물리 법칙을 위반하는 해를 생성할 수 있다.27 이 문제를 해결하기 위해 정적인 가중치 <span class="math math-inline">\lambda</span>를 사용하는 것이 일반적이지만, 최적의 <span class="math math-inline">\lambda</span> 값을 찾는 것은 매우 어려운 튜닝 과정이며, 훈련 중에 동적으로 변화하는 복잡한 손실 지형(loss landscape)에 효과적으로 대응하지 못한다.25 이러한 ‘경사 병리(gradient pathology)’ 현상, 즉 특정 손실 항의 경사도가 다른 항들에 의해 소실되거나 압도되는 문제를 해결하기 위해, 학습률 어닐링(learning rate annealing), 각 손실 항의 경사도를 동적으로 재조정하는 동적 가중치 기법, 그리고 MultiAdam과 같은 적응형 옵티마이저 등 정교한 최적화 전략에 대한 연구가 활발히 진행되고 있다.24 결국, 성공적인 PINN 훈련은 단순히 신경망 아키텍처를 설계하는 것을 넘어, 이질적인 손실 요소들 간의 동적인 상호작용을 이해하고 제어하는 다중 목표 최적화(multi-objective optimization) 문제로 귀결된다.</p>
<h3>2.3. 자동 미분 (Automatic Differentiation, AD)의 역할</h3>
<p>물리 정보 손실 함수, 특히 <span class="math math-inline">\mathcal{L}_{physics}</span>를 실제로 계산하기 위해서는 신경망의 출력 <span class="math math-inline">u_{\theta}</span>를 입력 변수 <span class="math math-inline">(x, t)</span>에 대해 미분하여 <span class="math math-inline">\frac{\partial u_{\theta}}{\partial t}</span>, <span class="math math-inline">\frac{\partial^2 u_{\theta}}{\partial x^2}</span>와 같은 다양한 미분 항들을 구해야 한다. 이 과정을 가능하게 하는 핵심 기술이 바로 자동 미분(Automatic Differentiation, AD)이다.8</p>
<p>AD는 컴퓨터 프로그램으로 표현된 함수(이 경우, 신경망의 순방향 전파 과정)의 도함수를 계산하는 일련의 기법이다. AD는 두 가지 주요 미분 방법, 즉 기호 미분(symbolic differentiation)과 수치 미분(numerical differentiation)의 장점을 결합한 독특한 접근법이다.</p>
<ul>
<li>
<p><strong>기호 미분</strong>은 수학 공식을 직접 조작하여 도함수의 정확한 해석적 표현을 찾는다. 이는 매우 정확하지만, 표현식이 복잡해질수록 ‘표현식 팽창(expression swell)’ 문제로 인해 계산이 비효율적이 될 수 있다.</p>
</li>
<li>
<p><strong>수치 미분</strong>은 유한 차분 근사를 사용하여 도함수 값을 추정한다. 구현이 간단하지만, 절단 오차(truncation error)와 반올림 오차(round-off error)에 민감하여 정확도에 한계가 있다.</p>
</li>
</ul>
<p>AD는 이 두 방법의 단점을 극복한다. 신경망과 같은 복잡한 함수는 일련의 기본 연산(덧셈, 곱셈, 지수 함수, 삼각 함수 등)의 조합으로 구성된다. AD는 연쇄 법칙(chain rule)을 이러한 기본 연산들에 반복적으로 적용하여, 기계 정밀도(machine precision) 수준의 정확도로 도함수 값을 계산한다.8 TensorFlow의 <code>tf.GradientTape</code>나 PyTorch의 <code>autograd</code>와 같은 현대 딥러닝 프레임워크들은 이 AD 기능을 효율적으로 구현하여 내장하고 있다.20</p>
<p>PINN 프레임워크에서 AD의 역할은 절대적이다. AD 덕분에 개발자는 복잡한 PDE의 각 미분 항에 대한 코드를 수동으로 작성할 필요 없이, 신경망 모델과 PDE 잔차의 형태만 정의하면 된다. 훈련 과정에서 역전파(backpropagation) 알고리즘이 실행될 때, AD는 손실 함수를 신경망 파라미터 <span class="math math-inline">\theta</span>에 대해 미분하여 경사도(gradient)를 계산할 뿐만 아니라, 동시에 PDE 잔차 계산에 필요한 <span class="math math-inline">u_{\theta}</span>의 공간 및 시간 도함수도 정확하게 계산해준다. 이처럼 AD는 PINN이 복잡한 물리 법칙을 손실 함수에 원활하게 통합하고, 경사도 기반 최적화 알고리즘을 통해 효율적으로 학습할 수 있도록 만드는 근간 기술이다.</p>
<p>PINN의 또 다른 근본적인 강점은 ‘메시-프리(mesh-free)’ 특성에서 비롯된다. 이는 전통적인 수치 해석의 기하학적 제약에서 벗어나 복잡하고 불규칙한 도메인 문제 해결에 본질적인 유연성을 제공한다. FEM이나 FDM은 계산을 수행하기 위해 반드시 도메인을 유한한 요소나 격자, 즉 메시(mesh)로 이산화해야 한다.28 특히 복잡한 3차원 형상이나 움직이는 경계를 다루는 문제에서 고품질의 메시를 생성하는 작업은 그 자체로 매우 어렵고 시간이 많이 소요되는 전문가의 영역이다.1 반면, PINN은 신경망이 연속적인 좌표 함수를 학습하기 때문에 이러한 메시에 전혀 의존하지 않는다. 손실 함수는 도메인 내에서 무작위로 또는 체계적으로 샘플링된 배열점들에서 계산될 뿐이다.8 이 ‘메시-프리’ 접근법은 기하학적 형상에 대한 제약을 극적으로 완화한다. 예를 들어, PointNet과 같은 점 구름 처리 아키텍처와 결합된 Physics-informed PointNet (PIPN)은 여러 개의 서로 다른 불규칙한 형상에 대해 단일 모델을 훈련시킬 수 있다. 이는 형상이 바뀔 때마다 메시 생성부터 다시 시작하고 모델을 재훈련해야 하는 기존 PINN의 한계마저 극복하는 것이다.8 따라서 PINN의 메시-프리 특성은 단순히 계산상의 편의성을 넘어서, 산업 디자인 최적화, 생물학적 시스템 모델링, 혹은 균열 전파 문제와 같이 기하학적 형태 자체가 중요한 변수인 문제들을 해결하는 데 근본적인 유연성과 확장성을 제공하는 핵심적인 장점이다.</p>
<h2>3. PINN의 이론적 기반과 발전</h2>
<p>물리 정보 신경망은 비교적 최근에 등장한 기술이지만, 그 이론적 기반과 발전은 Maziar Raissi, Paris Perdikaris, George Em Karniadakis의 선구적인 연구에 깊이 뿌리내리고 있다. 이들의 연구는 PINN의 기본 개념을 정립하고, 순방향 및 역문제 해결을 위한 통합 프레임워크를 제시하며 해당 분야의 폭발적인 성장을 이끌었다. 본 장에서는 이들의 초석이 된 연구를 살펴보고, 이를 통해 드러난 PINN의 본질적인 장점과 내재적 한계를 분석한다.</p>
<h3>3.1. 초석이 된 연구: Raissi, Perdikaris, Karniadakis</h3>
<p>PINN 분야의 현대적인 형태는 M. Raissi, P. Perdikaris, G. E. Karniadakis가 발표한 일련의 논문들에 의해 사실상 정립되었다고 평가받는다. 이들의 연구는 2017년 arXiv에 공개된 두 편의 프리프린트와, 이를 종합하여 2019년 Journal of Computational Physics에 게재된 논문으로 대표된다.16</p>
<ol>
<li>
<p><strong>“Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations” (2017):</strong> 이 논문은 PINN의 핵심 개념을 처음으로 체계적으로 제시했다. 저자들은 신경망이 희소한 데이터 포인트와 지배 PDE의 잔차를 동시에 최소화하도록 훈련될 수 있음을 보였다. 즉, 데이터 기반으로 비선형 PDE의 해를 근사하는 PINN의 기본 프레임워크를 정립한 것이다. 이 연구는 데이터가 부족한 상황에서도 물리 법칙이 강력한 정규화 역할을 하여 해를 효과적으로 찾을 수 있음을 입증했다.31</p>
</li>
<li>
<p><strong>“Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations” (2017):</strong> 첫 번째 논문이 주어진 PDE의 해를 찾는 ’순방향 문제’에 집중했다면, 이 두 번째 논문은 패러다임을 전환하여 ’데이터 기반 발견(data-driven discovery)’이라는 새로운 문제를 다루었다. 이 연구에서는 관측 데이터만으로 시스템을 지배하는 미지의 PDE 자체를 발견하는 데 PINN을 적용하는 방법을 선보였다. PDE의 항들을 후보 라이브러리로 구성하고, 각 항의 계수를 신경망의 파라미터와 함께 학습함으로써 데이터에 가장 잘 부합하는 지배 방정식을 찾아내는 독창적인 접근법을 제시했다.32</p>
</li>
<li>
<p><strong>“Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations” (2019):</strong> 이 논문은 이전 두 연구를 종합하고 확장하여 PINN을 순방향 문제와 역문제를 모두 해결할 수 있는 일반화된 딥러닝 프레임워크로 공식화했다. 이 논문은 PINN의 방법론을 명확하게 정리하고, 유체 역학, 양자 역학, 반응-확산 시스템 등 다양한 분야의 고전적인 문제들에 적용하여 그 효과성을 광범위하게 입증했다. 특히, 관측 데이터로부터 PDE의 미지 계수를 추정하는 역문제를 자연스럽게 해결할 수 있음을 보여줌으로써 PINN의 활용 가능성을 크게 넓혔다. 이 논문은 PINN 분야의 ’정전(canon)’으로 여겨지며, 이후 수많은 후속 연구의 이론적, 실용적 기반이 되었다.16</p>
</li>
</ol>
<p>이들의 연구는 신경망을 단순한 데이터 패턴 인식 도구에서 물리 법칙을 이해하고 해석하는 과학적 도구로 격상시키는 중요한 전환점이 되었다.</p>
<h3>3.2. 장점과 내재적 한계</h3>
<p>Raissi 등의 연구와 그 이후의 수많은 연구를 통해 PINN은 기존 방법론과 차별화되는 명확한 장점과 함께, 해결해야 할 내재적인 한계를 동시에 드러냈다.</p>
<h4>장점 (Advantages)</h4>
<ul>
<li>
<p><strong>데이터 효율성 (Data Efficiency):</strong> PINN의 가장 큰 장점은 물리 법칙이라는 강력한 사전 정보를 활용한다는 점이다. 지배 PDE는 데이터가 없는 광범위한 영역에 대한 제약을 제공하므로, 매우 적은 양의 데이터나 노이즈가 섞인 데이터만으로도 효과적인 학습이 가능하다.5 이는 실험 데이터 확보가 어려운 분야에서 특히 강력한 이점으로 작용한다.4</p>
</li>
<li>
<p><strong>역문제 해결 능력 (Solving Inverse Problems):</strong> 앞서 언급했듯이, PINN은 순방향 문제와 역문제를 동일한 프레임워크 내에서 원활하게 해결한다. PDE의 미지 계수나 소스 항을 신경망의 학습 가능한 파라미터로 취급하여, 관측 데이터와 물리 법칙을 동시에 만족시키는 최적의 값을 찾아낼 수 있다. 이는 전통적인 역문제 해결 기법에 비해 구현이 간단하고 직관적이다.3</p>
</li>
<li>
<p><strong>메시-프리 (Mesh-Free):</strong> PINN은 계산 격자를 필요로 하지 않는다. 이는 복잡한 기하학적 형상이나 고차원 공간에서 메시 생성에 드는 막대한 노력을 절감시켜 준다. 따라서 불규칙한 도메인이나 고차원 PDE 문제에 대한 적용이 상대적으로 용이하다.1</p>
</li>
<li>
<p><strong>빠른 추론 속도 (Fast Inference):</strong> PINN의 훈련 과정은 계산 집약적이고 시간이 오래 걸릴 수 있지만, 일단 훈련이 성공적으로 완료되면 새로운 입력(시공간 좌표)에 대한 해를 계산하는 것은 신경망의 순방향 전파(forward pass)만으로 이루어진다. 이 과정은 매우 빠르기 때문에, 실시간 예측, 제어, 최적화 등 반복적인 해 계산이 필요한 응용 분야에 큰 잠재력을 가진다.12</p>
</li>
</ul>
<h4>단점 (Disadvantages)</h4>
<ul>
<li>
<p><strong>높은 훈련 비용 (High Training Cost):</strong> PINN의 손실 함수는 일반적으로 비볼록(non-convex) 형태를 띤다. 이는 최적화 과정이 국소 최솟값(local minima)에 빠지기 쉽고, 전역 최솟값(global minimum)을 찾는 것을 보장하기 어렵다는 것을 의미한다. 따라서 만족스러운 해를 얻기까지 수많은 반복 계산이 필요하며, 이는 상당한 계산 시간과 자원을 소모한다.12</p>
</li>
<li>
<p><strong>수렴 불안정성 및 경사 소실 (Convergence Instability &amp; Vanishing Gradients):</strong> 특히 깊은 신경망을 사용하거나, 다루는 PDE가 매우 복잡하고 비선형성이 강할 경우 훈련 과정이 불안정해질 수 있다. 또한, 역전파 과정에서 경사(gradient)가 층을 거치면서 점차 작아져 소실되는 문제(vanishing gradients)가 발생하여 학습이 제대로 이루어지지 않을 수 있다.34</p>
</li>
<li>
<p><strong>하이퍼파라미터 민감성 (Sensitivity to Hyperparameters):</strong> PINN의 성능은 신경망의 구조(층의 수, 뉴런의 수), 활성화 함수의 종류, 학습률, 옵티마이저 선택, 그리고 특히 데이터 손실과 물리 손실 간의 가중치(<span class="math math-inline">\lambda</span>)와 같은 수많은 하이퍼파라미터에 매우 민감하게 반응한다. 최적의 하이퍼파라미터 조합을 찾는 것은 많은 경험과 시행착오를 요구하는 어려운 작업이다.12</p>
</li>
<li>
<p><strong>이론적 기반 부족 (Lack of Rigorous Theory):</strong> PINN이 다양한 문제에서 경험적으로 뛰어난 성공을 거두었음에도 불구하고, 그 성공을 뒷받침하는 엄밀한 수학적 이론은 아직 발전 초기 단계에 머물러 있다. PINN 근사 해의 수렴성, 오차의 상한(error bound), 일반화 성능 등에 대한 이론적 분석은 여전히 활발한 연구 주제이며, 이는 PINN의 신뢰성과 예측 가능성을 보장하는 데 있어 중요한 과제로 남아있다.10</p>
</li>
</ul>
<p>PINN의 발전 과정을 더 깊이 들여다보면, 그 진화가 ’단일 문제 해결사(single-problem solver)’에서 ’매개변수화된 문제군에 대한 해 공간을 학습하는 메타-모델(meta-model)’로 나아가고 있음을 알 수 있다. 초기 PINN 모델은 특정 경계 조건과 고정된 PDE 매개변수(예: 유체의 점성 계수, 재료의 열전도율)를 가진 단일 문제에 대한 해 <span class="math math-inline">u(x,t)</span>를 학습하는 데 집중했다. 이는 만약 문제의 매개변수가 조금이라도 변경되면, 모델 전체를 처음부터 다시 훈련해야 한다는 실용적인 제약을 의미했다.4 이러한 방식은 반복적인 시뮬레이션이 필수적인 공학 설계 최적화나 불확실성 정량화 작업에는 비효율적이다.</p>
<p>이러한 한계를 극복하기 위해, 연구자들은 PDE의 매개변수를 신경망의 추가적인 입력으로 취급하는 ‘매개변수화된 PINN(parameterized PINN)’ 개념을 발전시키고 있다.33 이 접근법에서 신경망은 단일 해 <span class="math math-inline">u(x,t)</span>가 아닌, 매개변수 벡터 <span class="math math-inline">p</span>에 의해 결정되는 해의 족(family of solutions) <span class="math math-inline">u(x, t; p)</span>를 학습한다. 예를 들어, 다양한 점성 계수 <span class="math math-inline">\nu</span>에 대한 유동 해석을 수행하고자 한다면, <span class="math math-inline">\nu</span>를 <span class="math math-inline">(x, t)</span>와 함께 신경망의 입력으로 제공한다. 이렇게 훈련된 모델은 이전에 보지 못했던 새로운 매개변수 <span class="math math-inline">p_{new}</span>에 대해서도 재훈련 과정 없이 즉시 해를 생성할 수 있는 능력을 갖추게 된다.</p>
<p>이는 PINN의 역할을 특정 PDE의 ’해 연산자(solution operator)’를 근사하는 모델로 확장하는 것이다. 즉, 입력 함수(매개변수, 초기/경계 조건)를 출력 함수(해)로 매핑하는 연산자 자체를 학습하는 것이다. 이러한 개념은 신경 연산자(Neural Operator)와 같은 더 일반적이고 강력한 프레임워크와 맥을 같이하며, PINN 연구의 최전선이 어디를 향하고 있는지를 명확히 보여준다.4 결론적으로, PINN 연구는 단일 문제 인스턴스를 푸는 것을 넘어, 전체 문제군을 포괄하는 일반화된 해 공간을 학습함으로써, 빠른 설계 탐색, 실시간 제어, 그리고 효율적인 불확실성 분석을 가능하게 하는 방향으로 진화하고 있다.</p>
<h2>4. 전통적 수치 해석 기법과의 비교 분석</h2>
<p>물리 정보 신경망(PINN)은 과학 계산 분야에 새로운 가능성을 제시했지만, 이는 수십 년간 검증되고 발전해 온 유한요소법(FEM)이나 유한차분법(FDM)과 같은 전통적인 수치 해석 기법의 완전한 대체를 의미하지는 않는다. 오히려, PINN의 진정한 가치와 위치를 이해하기 위해서는 이들 전통적 방법론과의 다각적이고 객관적인 비교 분석이 필수적이다. 본 장에서는 방법론, 정확도, 계산 비용, 그리고 복잡성 및 유연성이라는 핵심 지표를 기준으로 PINN과 전통적 수치 해석 기법을 비교하고, 이를 통해 각 방법론의 강점과 약점을 명확히 하고자 한다.</p>
<h3>4.1. 핵심 비교 지표</h3>
<h4>방법론 (Methodology)</h4>
<p>가장 근본적인 차이는 도메인을 다루는 방식에 있다. FEM과 FDM은 ‘메시-기반(mesh-based)’ 방법론이다.29 이들은 해석하고자 하는 물리적 공간을 수많은 작은 요소(element)나 격자점(grid point)으로 이산화(discretize)하고, 각 요소나 격자점에서 PDE를 대수 방정식으로 근사하여 연립 방정식을 풀어 해를 구한다. 반면, PINN은 ‘메시-프리(mesh-free)’ 접근법을 취한다.8 신경망 자체가 시공간 좌표를 입력받아 해를 출력하는 연속적인 함수를 근사하기 때문에, 도메인을 이산화할 필요가 없다. 물리 법칙의 만족 여부는 도메인 내에 샘플링된 배열점(collocation points)에서 평가된다.</p>
<h4>정확도 (Accuracy)</h4>
<p>정확도 측면에서는 일반적으로 전통적 수치 해석 기법이 우위를 점한다. FEM/FDM은 오랜 기간에 걸쳐 발전해 온 엄밀한 수학적 이론을 바탕으로 하며, 해의 수렴성과 오차에 대한 명확한 보장을 제공한다. 메시를 세분화할수록 해의 정확도가 체계적으로 향상된다는 것이 이론적으로나 실제적으로 잘 확립되어 있다.1 반면, PINN의 정확도는 훈련 데이터의 질과 양, 신경망 아키텍처의 적절성, 그리고 비볼록 최적화 과정의 성공 여부에 크게 의존한다. 훈련이 성공적으로 수렴하더라도, 그 해가 얼마나 정확한지에 대한 엄밀한 보장을 하기는 어렵다. 여러 연구에서 잘 정립된 문제에 대해 PINN이 FEM의 정확도를 능가하지 못하는 경우가 보고되었다.1</p>
<h4>계산 비용 (Computational Cost)</h4>
<p>계산 비용은 ’훈련/해석 시간’과 ’평가/추론 시간’으로 나누어 비교해야 한다.</p>
<ul>
<li>
<p><strong>훈련/해석 시간:</strong> PINN은 손실 함수의 비볼록성으로 인해 최적화 과정, 즉 ’훈련’에 매우 긴 시간이 소요될 수 있다.1 반면, FEM/FDM은 단일 문제 ’해석’에 있어서는, 특히 선형 문제의 경우, 매우 효율적인 솔버가 개발되어 있어 PINN의 훈련 시간보다 훨씬 빠를 수 있다. 그러나 문제가 비선형적이거나 고차원적으로 변하면 FEM/FDM의 계산 비용 역시 급격히 증가한다.36</p>
</li>
<li>
<p><strong>평가/추론 시간:</strong> 이 지점에서는 PINN이 명백한 우위를 가진다. 일단 신경망이 성공적으로 훈련되고 나면, 새로운 좌표 지점에서의 해를 얻는 것은 단순히 훈련된 네트워크에 입력을 통과시키는 순방향 전파(forward pass) 과정에 불과하다. 이는 거의 실시간으로 이루어질 수 있다.1 반면, FEM/FDM으로 구한 이산적인 해에서 새로운 지점의 값을 얻기 위해서는 보간(interpolation)이 필요한데, 이는 PINN의 추론보다 훨씬 느릴 수 있다. 특히, 3차원 문제에서 FEM 해를 새로운 메시 상에 보간하는 것은 PINN의 평가 시간보다 수백 배에서 수천 배까지 느릴 수 있다는 연구 결과도 있다.1 따라서, 동일한 해를 반복적으로 평가해야 하는 최적화나 불확실성 정량화 작업에서는 훈련된 PINN이 매우 효율적일 수 있다.</p>
</li>
</ul>
<h4>복잡성 및 유연성 (Complexity and Flexibility)</h4>
<ul>
<li>
<p><strong>기하학적 복잡성:</strong> PINN의 메시-프리 특성은 복잡한 형상을 다룰 때 큰 장점으로 작용한다. FEM/FDM에서 복잡한 형상에 대한 고품질 메시를 생성하는 것은 매우 어렵고 시간이 많이 소요되는 작업이지만, PINN은 이러한 부담에서 자유롭다.1</p>
</li>
<li>
<p><strong>고차원 문제:</strong> FEM/FDM은 차원이 증가함에 따라 필요한 격자점의 수가 기하급수적으로 늘어나는 ’차원의 저주’에 매우 취약하다. 3차원 이상에서는 실질적인 적용이 어려운 경우가 많다. 반면, PINN은 신경망의 구조가 문제의 차원에 직접적으로 의존하지 않기 때문에, 고차원 PDE를 해결하는 데 있어 이론적인 잠재력을 가지고 있다.1</p>
</li>
<li>
<p><strong>데이터 통합:</strong> PINN의 프레임워크는 희소하고 노이즈 섞인 실험 데이터를 손실 함수에 자연스럽게 통합하여 모델을 보정하는 데 매우 효과적이다. 이는 PINN의 핵심적인 설계 철학 중 하나이다. 반면, FEM/FDM에 데이터를 통합하기 위해서는 칼만 필터와 같은 복잡한 데이터 동화(data assimilation) 기법을 별도로 적용해야 하며, 이는 구현이 복잡하고 계산 비용이 높다.2</p>
</li>
</ul>
<h3>4.2. 비교 분석 테이블</h3>
<p>PINN과 전통적 수치 해석 기법의 주요 특징을 요약하면 다음 표와 같다. 이 표는 두 방법론 간의 근본적인 트레이드오프 관계를 명확하게 보여준다. PINN은 FEM/FDM의 보장된 정확도와 이론적 엄밀함을 희생하는 대신, 기하학적 유연성, 데이터 통합 능력, 그리고 훈련 후 빠른 평가 속도를 얻는다.</p>
<p><strong>Table 1: PINN과 전통적 수치 해석 기법 비교 (Comparison of PINN and Traditional Numerical Methods)</strong></p>
<table><thead><tr><th>특징 (Feature)</th><th>Physics-Informed Neural Network (PINN)</th><th>유한요소법/유한차분법 (FEM/FDM)</th></tr></thead><tbody>
<tr><td><strong>기본 방법론</strong></td><td>메시-프리(Mesh-free). 연속 함수 근사.</td><td>메시-기반(Mesh-based). 도메인 이산화.</td></tr>
<tr><td><strong>정확도</strong></td><td>문제 및 훈련에 따라 가변적. 수렴 보장 어려움.</td><td>잘 정립된 이론. 높은 정확도 및 수렴 보장.</td></tr>
<tr><td><strong>계산 비용 (훈련/해석)</strong></td><td>훈련 비용이 높고 시간이 오래 걸림 (Non-convex 최적화).</td><td>해석 비용이 높음 (특히 비선형/고차원 문제).</td></tr>
<tr><td><strong>계산 비용 (평가/추론)</strong></td><td>매우 빠름 (단순 순방향 전파).</td><td>상대적으로 느림 (보간 또는 재해석 필요).</td></tr>
<tr><td><strong>복잡한 형상 처리</strong></td><td>유연함. 메시 생성 불필요.</td><td>어려움. 고품질 메시 생성에 많은 노력 필요.</td></tr>
<tr><td><strong>고차원 문제</strong></td><td>’차원의 저주’에 강한 잠재력 보유.</td><td>’차원의 저주’에 매우 취약함.</td></tr>
<tr><td><strong>데이터 통합</strong></td><td>희소/노이즈 데이터를 손실 함수에 자연스럽게 통합.</td><td>복잡한 데이터 동화(Data Assimilation) 기법 필요.</td></tr>
<tr><td><strong>역문제(Inverse Problems)</strong></td><td>순방향 문제와 동일한 프레임워크에서 해결 가능.</td><td>특화된 알고리즘이 필요하며 일반적으로 더 복잡함.</td></tr>
</tbody></table>
<p>이러한 비교 분석을 통해 PINN과 FEM/FDM은 서로를 대체하는 경쟁 관계라기보다는, 상호 보완적인 관계로 발전하고 있음을 알 수 있다. 두 방법론을 결합한 하이브리드 접근법이 새로운 연구 방향으로 부상하고 있는 것은 이러한 맥락에서 자연스러운 귀결이다. PINN은 경계 조건을 손실 함수의 페널티 항으로 ‘부드럽게(softly)’ 강제하기 때문에, 특히 디리클레 경계 조건이 중요한 문제에서 경계 근처의 정확도가 떨어지는 경향이 있다.38 반면, FEM은 함수 공간의 기저 함수를 통해 경계 조건을 ‘강하게(strongly)’ 정확히 만족시키는 데 본질적인 강점을 가진다.</p>
<p>이러한 각자의 장점을 결합하려는 시도가 바로 하이브리드 솔버의 개발이다. 예를 들어, PINN-FEM 하이브리드 모델은 도메인을 분할하여, 경계 조건이 중요한 경계 근처 영역에서는 FEM 기반의 표현을 사용하여 정확성을 보장하고, 복잡한 비선형 현상이 발생하는 도메인 내부에서는 PINN의 유연성을 활용하여 해를 근사한다.38 또 다른 접근법으로는, S-FEM(Smoothed FEM)과 같은 고정밀 수치 해석 기법으로 고품질의 합성 데이터를 대량 생성하여 PINN의 훈련을 돕거나, FEA-Regulated PINN(FEA-PINN)처럼 PINN의 추론 과정 중에 주기적으로 FEA 시뮬레이션을 수행하여 누적되는 오차를 보정하는 연구도 있다.29</p>
<p>결론적으로, PINN과 전통적 방법론 간의 이분법적 대결 구도를 넘어, 각자의 강점을 체계적으로 결합하여 약점을 보완하는 ’하이브리드 솔버’의 개발은, 더 복잡하고 현실적인 문제들을 해결하기 위한 차세대 계산 과학의 중요한 흐름이 될 것으로 전망된다.</p>
<h2>5. PINN 변형 모델 심화 탐구: The “PINN Zoo”</h2>
<p>초기의 “바닐라(Vanilla)” PINN 모델은 그 가능성을 입증했지만, 동시에 확장성, 특정 물리 법칙 준수, 불확실성 처리 등 여러 측면에서 한계를 드러냈다. 이러한 한계들을 극복하기 위해 지난 몇 년간 수많은 PINN 변형 모델들이 제안되었으며, 이는 마치 다양한 종이 공존하는 동물원과 같다고 하여 “PINN Zoo“라는 별칭으로 불리기도 한다. 이들 변형 모델은 PINN의 핵심 아이디어를 유지하면서 특정 문제나 요구사항에 맞게 구조나 학습 방식을 수정한 것이다. 본 장에서는 그중에서도 가장 중요하고 영향력 있는 변형 모델인 XPINN, cPINN, B-PINN을 중심으로 그 핵심 원리와 목적을 심도 있게 탐구한다.</p>
<h3>5.1. XPINN (eXtended PINN): 영역 분해를 통한 확장</h3>
<h4>핵심 개념</h4>
<p>XPINN(eXtended Physics-Informed Neural Networks)은 대규모 계산 도메인이나 복잡한 물리 현상을 다룰 때 발생하는 바닐라 PINN의 훈련 비용과 수렴 문제를 해결하기 위해 제안된 모델이다.40 그 핵심 아이디어는 전통적인 수치 해석에서 오랫동안 사용되어 온 영역 분해법(Domain Decomposition Method)에서 영감을 얻었다.40 즉, 하나의 거대한 계산 도메인을 여러 개의 작고 관리하기 쉬운 하위 도메인(subdomain)으로 분할하고, 각 하위 도메인마다 독립적인 소규모 신경망을 할당하여 해를 근사하는 방식이다.42</p>
<h4>원리</h4>
<p>XPINN 프레임워크에서 각 하위 신경망은 자신이 할당받은 영역 내에서 지배 PDE의 잔차와 해당 영역에 속한 경계 조건을 최소화하도록 개별적으로 훈련된다. 여기서 가장 중요한 부분은 하위 도메인들이 만나는 경계면(interface)에서의 연속성을 보장하는 것이다. 이를 위해, 인접한 두 하위 신경망의 예측값이 경계면에서 동일해야 한다는 해의 연속성 조건(<span class="math math-inline">u_1(x) = u_2(x)</span>)과, 해의 미분값(물리적으로는 유량(flux)에 해당)이 연속적이어야 한다는 조건(<span class="math math-inline">\nabla u_1(x) = \nabla u_2(x)</span>)이 추가적인 손실 항으로 손실 함수에 포함된다.44 이 경계면 손실 항들은 각 하위 도메인의 해들이 물리적으로 타당하게 연결되어 전체 도메인에서 하나의 일관된 해를 형성하도록 강제하는 역할을 한다.</p>
<h4>장점</h4>
<ul>
<li>
<p><strong>병렬화 (Parallelization):</strong> 각 하위 신경망은 독립적으로 훈련될 수 있으므로, 이를 다수의 GPU나 컴퓨팅 노드에 분산하여 병렬적으로 처리할 수 있다. 이는 전체 훈련 시간을 획기적으로 단축시켜 대규모 문제에 대한 PINN의 적용 가능성을 높인다.40</p>
</li>
<li>
<p><strong>표현력 증가 (Increased Representation Capacity):</strong> 하나의 거대하고 복잡한 신경망으로 전체 도메인을 학습하는 것보다, 여러 개의 작고 단순한 신경망이 각자의 영역에 집중하는 것이 더 효율적일 수 있다. 특히, 해의 거동이 복잡한 영역(예: 충격파 근처)에는 깊고 넓은 신경망을, 해가 완만하게 변하는 영역에는 얕은 신경망을 할당하는 등, 해의 국소적 특성에 맞춰 신경망 아키텍처를 유연하게 조절할 수 있다.47 이는 모델의 전체적인 표현력을 크게 향상시킨다.</p>
</li>
<li>
<p><strong>다중 물리/스케일 문제 해결:</strong> 재료의 물성이 급격하게 변하거나 서로 다른 물리 법칙이 지배하는 영역이 공존하는 다중 물리(multiphysics) 또는 다중 스케일(multiscale) 문제에 매우 효과적이다. 각 물리 영역에 특화된 하위 신경망을 할당하여 문제를 자연스럽게 분해할 수 있다.41</p>
</li>
<li>
<p><strong>일반성 (Generality):</strong> 다음에 설명할 cPINN이 주로 보존 법칙에 특화된 반면, XPINN은 경계면에서 해와 그 도함수의 연속성만을 요구하므로 원칙적으로 모든 유형의 PDE에 적용할 수 있다. 또한, 공간 영역뿐만 아니라 시간 영역에 대해서도 분해가 가능하여(space-time decomposition), 시간에 따라 단계적으로 문제를 푸는 것과 같은 유연한 전략을 구사할 수 있다.40</p>
</li>
</ul>
<h3>5.2. cPINN (conservative PINN): 보존 법칙의 강제</h3>
<h4>핵심 개념</h4>
<p>cPINN(conservative Physics-Informed Neural Networks)은 유체 역학 등에서 매우 중요한 비선형 보존 법칙(nonlinear conservation laws)을 다루기 위해 특별히 설계된 영역 분해 기반 PINN이다.44 보존 법칙은 질량, 운동량, 에너지와 같은 물리량이 시스템 전체에서 보존되어야 함을 의미한다. cPINN의 핵심 목표는 영역 분해를 사용하면서도 이러한 전역적인 보존 특성을 엄격하게 만족시키는 것이다.</p>
<h4>원리</h4>
<p>cPINN 역시 XPINN과 마찬가지로 도메인을 분할하고 각 하위 도메인에 신경망을 할당한다. cPINN을 XPINN과 구별하는 가장 중요한 특징은 경계면 조건을 처리하는 방식에 있다. cPINN은 단순히 해와 그 도함수의 수학적 연속성을 강제하는 것을 넘어, 물리 법칙의 핵심인 ’유량(flux) 연속성’을 강한 형태(strong form)로 직접 강제한다.44 보존 법칙은 일반적으로 <span class="math math-inline">\frac{\partial u}{\partial t} + \nabla \cdot \mathbf{F}(u) = 0</span> 형태로 표현되는데, 여기서 <span class="math math-inline">\mathbf{F}(u)</span>가 바로 유량 벡터이다. cPINN은 인접한 두 하위 도메인 1과 2가 공유하는 경계면 <span class="math math-inline">\Gamma</span>의 모든 점 <span class="math math-inline">x</span>에 대해, 경계면에 수직한 방향의 유량이 동일해야 한다는 조건, 즉 <span class="math math-inline">\mathbf{F}_1(u_1(x)) \cdot \mathbf{n} = \mathbf{F}_2(u_2(x)) \cdot \mathbf{n}</span>을 손실 함수에 직접적인 페널티 항으로 추가한다.44 이를 통해 각 하위 도메인 사이에서 물리량의 흐름이 정확히 일치하게 되어, 전체 도메인에 걸쳐 보존 법칙이 깨지지 않도록 보장한다.48</p>
<h4>적용</h4>
<p>cPINN의 이러한 특성은 유체 유동에서 충격파(shock wave)가 발생하거나, 해에 불연속성이 나타나는 쌍곡선 보존 법칙(hyperbolic conservation laws) 문제를 해결하는 데 특히 강력한 성능을 발휘한다.50 일반적인 PINN은 이러한 불연속적인 해를 학습하는 데 어려움을 겪지만, cPINN은 물리적으로 올바른 불연속성을 포착하고 보존 법칙을 만족시키는 능력이 뛰어나다.</p>
<h3>5.3. B-PINN (Bayesian PINN): 불확실성 정량화</h3>
<h4>핵심 개념</h4>
<p>B-PINN(Bayesian Physics-Informed Neural Networks)은 PINN 프레임워크에 베이즈 추론(Bayesian inference)의 원리를 결합하여, PDE 해의 예측과 함께 그 예측에 내재된 불확실성(uncertainty)을 정량화하는 것을 목표로 하는 모델이다.52 과학 및 공학 문제에서 예측값만큼이나 그 예측이 얼마나 신뢰할 수 있는지를 아는 것이 중요하기 때문에, B-PINN은 매우 중요한 의미를 가진다.</p>
<h4>원리</h4>
<p>전통적인 PINN은 최적화 과정을 통해 신경망의 가중치와 편향 <span class="math math-inline">\theta</span>에 대한 단일 최적값(point estimate)을 찾는다. 반면, B-PINN은 베이즈 신경망(Bayesian Neural Network, BNN)을 기반으로 하여, 이 파라미터 <span class="math math-inline">\theta</span>들을 고정된 값이 아닌 확률 분포로 모델링한다.54</p>
<p>B-PINN의 학습 과정은 베이즈 정리를 따른다:</p>
<p><span class="math math-display">P(\theta | \text{Data}) \propto P(\text{Data} | \theta) \times P(\theta)</span></p>
<p>여기서 <span class="math math-inline">P(\theta)</span>는 파라미터에 대한 사전 믿음을 나타내는 **사전 분포(prior distribution)**이다. <span class="math math-inline">P(\text{Data} | \theta)</span>는 주어진 파라미터 <span class="math math-inline">\theta</span> 하에서 관측 데이터가 나타날 확률을 의미하는 **가능도(likelihood)**이며, PINN의 맥락에서 ’Data’는 관측된 측정값뿐만 아니라 물리 법칙(PDE 잔차=0)까지 포함한다. 이 두 가지를 결합하여 데이터가 주어진 후의 파라미터에 대한 업데이트된 믿음, 즉 사후 분포(posterior distribution) <span class="math math-inline">P(\theta | \text{Data})</span>를 추론한다.56</p>
<p>이 사후 분포는 단일 해가 아닌, 데이터와 물리 법칙을 만족하는 가능한 모든 해(solution)들의 앙상블(ensemble)을 나타낸다. 이 복잡한 사후 분포로부터 직접 샘플링하는 것은 어렵기 때문에, 해밀토니안 몬테카를로(Hamiltonian Monte Carlo, HMC)나 변분 추론(Variational Inference, VI)과 같은 근사 추론 기법이 사용된다.53</p>
<h4>2.2.4 장점</h4>
<ul>
<li>
<p><strong>불확실성 정량화 (Uncertainty Quantification):</strong> 사후 분포로부터 여러 개의 파라미터 샘플을 추출하고, 각 샘플에 해당하는 신경망 예측값들을 얻음으로써, 최종 예측값의 평균뿐만 아니라 분산이나 신뢰 구간(confidence interval)을 계산할 수 있다. 이는 데이터가 부족하거나 노이즈가 많은 영역에서 예측이 얼마나 불확실한지를 정량적으로 알려준다.52</p>
</li>
<li>
<p><strong>과적합 방지 및 강건성 (Overfitting Prevention and Robustness):</strong> 베이즈 프레임워크는 모델의 복잡도에 페널티를 부여하는 자연스러운 정규화(regularization) 효과를 가진다. 이는 특히 노이즈가 큰 데이터에 대해 모델이 과적합되는 것을 방지하고, 더 강건한(robust) 예측을 가능하게 한다.53</p>
</li>
</ul>
<p>이러한 “PINN Zoo“의 등장은 PINN이 단일한 고정된 방법론이 아니라, 특정 문제의 구조적 약점을 해결하기 위해 모듈식으로 확장 가능한 ’플랫폼’으로 진화하고 있음을 명확히 보여준다. 초기 “바닐라 PINN“이 수렴 문제, 보존 법칙 위반, 불확실성 처리 부재 등 여러 한계를 드러내자 8, 연구자들은 이를 해결하기 위해 각기 다른 ’모듈’을 PINN의 핵심 아이디어에 접목했다. XPINN은 ‘확장성’ 문제를 해결하기 위해 고전적인 영역 분해 아이디어를 도입했고 40, cPINN은 ’물리적 보존성’이라는 엄격한 제약을 만족시키기 위해 경계면 유량 연속성이라는 물리적 원리를 손실 함수에 명시적으로 추가했다.44 또한 B-PINN은 ’불확실성 정량화’라는 통계적 요구사항을 충족시키기 위해 베이즈 추론 프레임워크를 신경망에 통합했다.52 이처럼 다양한 변형 모델들은 PINN의 핵심 철학, 즉 신경망과 물리 정보 손실 함수의 결합을 공유하면서도, 해결하고자 하는 문제의 특성(예: 다중 스케일, 보존 법칙, 통계적 추론)에 맞춰 필요한 구성 요소를 유연하게 조합하고 확장할 수 있는 프레임워크로서의 PINN의 발전 가능성을 시사한다.</p>
<p><strong>Table 2: 주요 PINN 변형 모델 요약 (Summary of Key PINN Variants)</strong></p>
<table><thead><tr><th>모델명 (Variant)</th><th>핵심 혁신 (Core Innovation)</th><th>해결하는 주요 문제 (Primary Problem Solved)</th><th>주요 응용 분야 (Key Application Area)</th></tr></thead><tbody>
<tr><td><strong>XPINN</strong></td><td>공간-시간 영역 분해 (Space-Time Domain Decomposition)</td><td>대규모/복잡한 도메인 문제의 훈련 비용 및 확장성</td><td>다중 물리/스케일 시뮬레이션, 병렬 컴퓨팅</td></tr>
<tr><td><strong>cPINN</strong></td><td>경계면 유량 연속성 강제 (Flux Continuity at Interfaces)</td><td>보존 법칙(Conservation Laws) 위반</td><td>충격파, 불연속성이 있는 유체 역학 문제</td></tr>
<tr><td><strong>B-PINN</strong></td><td>베이즈 신경망(BNN) 통합</td><td>예측의 불확실성 정량화, 과적합 방지</td><td>노이즈가 많은 데이터 기반의 역문제, 신뢰성 분석</td></tr>
<tr><td><strong>PIPN</strong></td><td>PointNet 아키텍처 결합</td><td>여러 불규칙한 형상에 대한 일반화</td><td>형상 최적화, 산업 디자인</td></tr>
<tr><td><strong>gPINN</strong></td><td>손실 함수에 잔차의 경사(Gradient) 추가</td><td>훈련 속도 및 정확도 향상</td><td>일반적인 PDE 문제의 수렴 가속화</td></tr>
</tbody></table>
<h2>3.  주요 응용 분야 분석</h2>
<p>물리 정보 신경망(PINN)의 이론적 발전은 다양한 과학 및 공학 분야에서의 실질적인 응용을 통해 그 가치를 입증하고 있다. PINN은 전통적인 수치 해석 방법이 어려움을 겪거나, 실험 데이터가 부족한 문제에 대해 새로운 해결책을 제시하며 그 적용 범위를 빠르게 넓혀가고 있다. 본 장에서는 유체 역학 및 열 전달, 그리고 금융 공학이라는 두 가지 대표적인 분야를 중심으로 PINN의 구체적인 활용 사례와 그 성과를 분석한다.</p>
<h3>3.1  유체 역학 및 열 전달</h3>
<p>유체 유동과 열 전달 현상은 나비에-스토크스(Navier-Stokes) 방정식이나 열 방정식(Heat Equation)과 같은 복잡한 편미분방정식에 의해 지배된다. 이 분야는 전통적으로 계산 유체 역학(Computational Fluid Dynamics, CFD)의 주 무대였으나, PINN은 메시 생성의 부담이 없고 데이터 통합이 용이하다는 장점을 바탕으로 강력한 대안이자 보완 도구로 부상하고 있다.5</p>
<h4>3.1.1 사례 1: 전자 장비 및 배터리 열 관리 (Thermal Management of Electronics and Batteries)</h4>
<p>현대 전자 장비와 고성능 배터리의 설계에서 열 관리는 성능과 안전을 결정하는 핵심적인 요소이다. PINN은 이러한 시스템의 복잡한 열 분포를 빠르고 정확하게 예측하는 데 매우 효과적으로 사용되고 있다. 한 연구에서는 반도체 칩의 열 모델링에 PINN을 적용하여, 전통적인 상용 CFD 시뮬레이션 소프트웨어에 비해 <strong>최대 300,000배 빠른 계산 속도</strong>를 달성하면서도, 온도 예측 오차를 <strong>0.1K 미만</strong>으로 유지하는 놀라운 성과를 보였다.27 이는 제품 설계 과정에서 수많은 시나리오를 신속하게 탐색할 수 있게 하여 개발 주기를 획기적으로 단축시킬 수 있음을 의미한다. 또한, 리튬이온 배터리의 충전 과정에서 발생하는 열을 예측하기 위해 PINN과 순환 신경망의 일종인 LSTM(Long Short-Term Memory)을 결합한 하이브리드 모델이 개발되었다. 이 모델은 배터리의 열 상태를 매우 높은 정확도(결정계수 <span class="math math-inline">R^2 = 0.9863</span>)로 예측하여, 배터리 관리 시스템(BMS)의 성능을 향상시키고 안전성을 확보하는 데 기여할 수 있음을 보여주었다.27</p>
<h4>3.1.2 사례 2: 다공성 매질 내 열 전달 (Heat Transfer in Porous Media)</h4>
<p>지열 에너지, 필터레이션, 촉매 반응기 등 다공성 매질 내에서의 열 및 물질 전달은 다양한 공학 분야에서 중요한 문제이다. 이러한 시스템은 복잡한 내부 구조로 인해 모델링이 매우 어렵다. 한 연구에서는 다공성 매질 내 열 전달 문제에 PINN을 적용하여, 레이블된 데이터(즉, 정답이 있는 데이터)가 전혀 없는 상황에서도 물리 법칙(열 방정식)만을 이용하여 온도 및 열 유속 필드를 정확하게 예측하는 데 성공했다. 더욱이, 이 접근법은 기존 수치 해석 방법에 비해 <strong>5배수(five orders of magnitude, 100,000배)에 달하는 계산 가속</strong>을 달성하여, PINN이 데이터가 없는 순수 순방향 문제 해결에서도 강력한 잠재력을 가지고 있음을 입증했다.27</p>
<h4>3.1.3 사례 3: 복잡한 유동 문제 (Complex Fluid Dynamics)</h4>
<p>PINN의 연구는 단순한 층류 유동을 넘어, 공학적으로 더 중요하고 도전적인 문제들로 확장되고 있다. 난류(turbulence), 다상 유동(multiphase flows), 그리고 유체-구조 상호작용(fluid-structure interaction)과 같은 다중 물리 결합 유동(multi-field coupled flows)은 기존 CFD 해석으로도 많은 계산 자원과 전문성을 요구하는 영역이다. PINN은 이러한 복잡한 유체 역학 문제에 대한 새로운 대리 모델(surrogate model)을 신속하게 구축하거나, 희소한 실험 데이터를 활용하여 기존 모델을 보정하는 데 적용될 가능성이 활발히 연구되고 있다.15 비록 해결해야 할 과제는 남아있지만, PINN은 이 분야의 난제들을 해결하는 데 중요한 역할을 할 것으로 기대된다.</p>
<h3>3.2  금융 공학</h3>
<p>금융 공학, 특히 파생 상품 가격 결정 분야는 확률 미분방정식과 편미분방정식이 핵심적인 역할을 하는 영역이다. PINN은 이 분야의 고전적인 문제들을 새로운 관점에서 해결하는 도구로 주목받고 있다.</p>
<h4>3.2.1 적용: 블랙-숄즈 방정식 (Black-Scholes Equation)</h4>
<p>블랙-숄즈 방정식은 금융 시장에서 유럽형 옵션의 이론적 가격을 결정하는 데 사용되는 가장 기본적인 편미분방정식이다.58 방정식은 옵션 가격 <span class="math math-inline">C</span>를 기초 자산 가격 <span class="math math-inline">S</span>와 만기까지 남은 시간 <span class="math math-inline">t</span>의 함수로 기술하며, 다음과 같이 표현된다:</p>
<p><span class="math math-display">
</span>\frac{\partial C}{\partial t} + \frac{1}{2}\sigma^2 S^2 \frac{\partial^2 C}{\partial S^2} + rS \frac{\partial C}{\partial S} - rC = 0<br />
$$</p>
<p>여기서 <span class="math math-inline">r</span>은 무위험 이자율, <span class="math math-inline">\sigma</span>는 기초 자산의 변동성을 나타낸다. PINN은 이 블랙-숄즈 방정식을 물리 법칙 잔차 항으로, 그리고 옵션의 만기 시점 가치(payoff)를 경계 조건으로 사용하여 옵션 가격 함수 <span class="math math-inline">C(S, t)</span>를 학습한다.58</p>
<h4>장점 및 확장</h4>
<p>전통적으로 블랙-숄즈 방정식은 유한차분법(FDM)이나 몬테카를로 시뮬레이션을 통해 수치적으로 해결되어 왔다. 그러나 이러한 방법들은 계산 속도가 느리거나, 이산화로 인한 오차가 발생하는 등의 단점을 가진다.60 PINN은 이러한 문제에 대한 잠재적인 해결책을 제시한다.</p>
<ul>
<li>
<p><strong>속도와 유연성:</strong> 일단 훈련된 PINN은 다양한 기초 자산 가격과 시간에 대해 옵션 가격을 매우 빠르게 계산할 수 있다. 또한, 모델을 재훈련하지 않고도 다른 행사가나 만기에 대해 유연하게 적응할 수 있는 가능성을 보여준다.60</p>
</li>
<li>
<p><strong>데이터와 모델의 결합:</strong> PINN은 실제 시장에서 관측된 옵션 가격 데이터를 데이터 손실 항으로, 블랙-숄즈 방정식을 물리 손실 항으로 결합하여 훈련할 수 있다. 이를 통해 이론적 모델과 실제 시장 데이터 사이의 괴리를 줄이는, 보다 현실적인 가격 결정 모델을 만들 수 있다.58</p>
</li>
<li>
<p><strong>불확실성 정량화:</strong> 금융 시장의 본질적인 불확실성을 다루기 위해 B-PINN을 적용하는 연구가 활발하다. B-PINN을 사용하면 옵션 가격에 대한 단일 예측값을 넘어, 시장의 변동성이나 데이터의 노이즈를 반영한 신뢰 구간이나 확률 분포를 얻을 수 있다. 이는 단순한 가격 예측을 넘어 정교한 리스크 관리 및 헤징 전략 수립에 혁신을 가져올 수 있다.60</p>
</li>
</ul>
<p>PINN의 적용은 고전적인 블랙-숄즈 모델을 넘어, 자산 가격의 비정상적인 움직임을 모델링하기 위한 시간 분수 블랙-숄즈 방정식(time fractional Black-Scholes equations)이나, 변동성 자체가 확률적으로 변하는 것을 가정한 헤스턴(Heston) 모델과 같은 더 복잡하고 현실적인 금융 모델로 확장되고 있다.60</p>
<p>이러한 응용 사례들을 통해 드러나는 PINN의 핵심적인 가치는, 단순히 기존 시뮬레이터를 대체하는 것을 넘어선다. PINN은 데이터가 희소하거나 얻기 힘든 ‘회색 상자(gray-box)’ 문제에서 독보적인 가치를 창출한다. ‘블랙박스(black-box)’ 모델이 순수 데이터에만 의존하고, ‘화이트박스(white-box)’ 모델이 모든 물리 법칙과 매개변수가 완벽히 알려진 이상적인 상황을 가정하는 반면, 대부분의 현실 세계 공학 및 과학 문제는 이 둘 사이의 ‘회색 상자’ 영역에 속한다. 예를 들어, 유체 역학에서는 지배 방정식인 나비에-스토크스 방정식은 알고 있지만, 특정 신소재의 정확한 점성 계수나 복잡한 표면에서의 마찰 계수는 모를 수 있다.3 금융 공학에서는 블랙-숄즈 방정식의 형태는 알고 있지만, 실제 시장의 변동성 <span class="math math-inline">\sigma</span>는 시간에 따라 예측 불가능하게 변하며 정확히 알 수 없다.62</p>
<p>PINN은 알려진 물리 법칙(<span class="math math-inline">\mathcal{L}_{physics}</span>)과 희소하게 관측된 데이터(<span class="math math-inline">\mathcal{L}_{data}</span>)를 자신의 손실 함수 내에서 자연스럽게 결합한다. 이는 알려지지 않은 매개변수를 신경망의 학습 가능한 변수로 취급하여, 주어진 데이터로부터 최적으로 추론할 수 있게 한다. 따라서 PINN은 순수 시뮬레이션만으로는 해결할 수 없고(매개변수를 모르므로), 순수 데이터 기반 모델만으로는 신뢰할 수 없는(데이터가 부족하므로) 광범위한 회색 상자 문제에 대한 이상적인 해결책을 제공한다. 이것이 바로 PINN이 가진 가장 강력하고 독특한 응용 가치 중 하나이다.</p>
<h2>7. 결론: 미래 전망과 도전 과제</h2>
<p>물리 정보 신경망(PINN)은 지난 몇 년간 과학적 기계 학습 분야에서 가장 주목받는 기술 중 하나로 부상했다. 데이터 기반 학습의 유연성과 물리 기반 모델링의 엄밀함을 결합함으로써, PINN은 전통적인 방법론의 한계를 극복하고 과학 및 공학 분야의 난제들을 해결할 수 있는 강력한 프레임워크로 자리매김했다.6 본 보고서에서 살펴본 바와 같이, PINN은 그 이론적 기반을 빠르게 확장하고 있으며 유체 역학, 열 전달, 금융 공학 등 다양한 분야에서 실질적인 성공 사례를 만들어내고 있다. 이제 PINN 연구의 현재 위치를 종합하고, 미래를 향한 전망과 여전히 남아있는 도전 과제들을 논하며 결론을 맺고자 한다.</p>
<h3>7.1. PINN 연구의 현재와 미래</h3>
<p>PINN은 현재 데이터와 물리 법칙을 융합하는 패러다임의 선두주자로서, 그 가능성을 지속적으로 입증하고 있다. 초기 연구가 개념 증명에 집중했다면, 현재의 연구는 PINN의 성능, 확장성, 신뢰성을 향상시키는 방향으로 나아가고 있다. 학계에서는 이를 ‘PINN 2.0’ 시대로의 전환으로 보고 있으며, 향후 연구는 다음과 같은 핵심적인 방향으로 전개될 것으로 전망된다.30</p>
<ul>
<li>
<p><strong>이론적 기반 강화 (Strengthening Theoretical Foundations):</strong> PINN의 가장 시급한 과제 중 하나는 그 경험적 성공을 뒷받침할 엄밀한 수학적 이론을 정립하는 것이다. 현재 PINN 근사 해의 수렴성, 오차의 상한(error bound), 그리고 일반화 성능에 대한 이론적 분석은 아직 초기 단계에 머물러 있다.10 향후 연구는 특정 종류의 PDE에 대해 PINN의 수렴 속도를 보장하거나, 신경망 아키텍처와 훈련 데이터의 양이 오차에 미치는 영향을 정량적으로 분석하는 방향으로 나아가야 한다. 이러한 이론적 기반의 확립은 PINN의 신뢰성을 높이고, 그 적용 범위를 더욱 넓히는 데 필수적이다.</p>
</li>
<li>
<p><strong>확장성 및 효율성 향상 (Improving Scalability and Efficiency):</strong> 대규모의 복잡한 3차원 시공간 문제에 바닐라 PINN을 직접 적용하는 것은 여전히 계산 비용 측면에서 비효율적이다. XPINN과 같은 영역 분해 기법을 더욱 정교하게 발전시키고, 대규모 병렬 컴퓨팅 환경에 최적화하는 연구가 중요하다.63 또한, 새로운 PDE 문제에 직면했을 때 매번 처음부터 모델을 훈련하는 비효율을 줄이기 위해, 이전에 학습된 지식을 새로운 문제에 빠르게 적용하는 전이 학습(transfer learning)과 메타 학습(meta-learning) 기법을 PINN에 접목하는 연구가 활발히 진행되고 있다. 이는 PINN의 적응 속도를 높이고 실용성을 크게 향상시킬 것이다.63</p>
</li>
<li>
<p><strong>강건성 및 신뢰성 확보 (Ensuring Robustness and Reliability):</strong> 실제 공학 문제에 PINN을 적용하기 위해서는 예측의 신뢰성을 확보하는 것이 무엇보다 중요하다. B-PINN을 통한 불확실성 정량화 연구를 더욱 발전시켜, 데이터의 노이즈나 모델 자체의 불완전함에서 비롯되는 불확실성을 체계적으로 평가하고 사용자에게 제공하는 것이 표준화되어야 한다.34 또한, 훈련이 실패하거나 물리적으로 타당하지 않은 해로 수렴하는 사례들에 대한 원인을 깊이 분석하고, 이를 감지하고 방지할 수 있는 자동화된 기법을 개발하는 것도 중요한 연구 방향이다.4</p>
</li>
</ul>
<h3>7.2. 남은 도전 과제</h3>
<p>PINN의 밝은 미래 전망에도 불구하고, 광범위한 실용화를 위해서는 반드시 해결해야 할 몇 가지 중요한 도전 과제들이 남아있다.</p>
<ul>
<li>
<p><strong>최적화의 어려움:</strong> PINN의 비볼록 손실 지형은 훈련 과정의 가장 큰 걸림돌이다. 국소 최솟값 문제, 손실 항들 간의 불균형으로 인한 경사 병리 현상, 그리고 수많은 하이퍼파라미터 튜닝의 복잡성은 여전히 PINN을 다루기 어려운 기술로 만들고 있다.12 이를 극복하기 위한 새로운 최적화 알고리즘과 적응형 손실 가중치 기법의 개발이 지속적으로 필요하다.</p>
</li>
<li>
<p><strong>강한 비선형성 및 불연속성:</strong> 난류와 같이 매우 강한 비선형성을 가지거나, 충격파나 상변화처럼 해에 불연속성이 나타나는 문제에서 바닐라 PINN은 여전히 정확한 해를 학습하는 데 어려움을 겪는다.8 cPINN과 같은 특화된 모델이 일부 해결책을 제시했지만, 더 일반적이고 강건한 방법론의 개발이 요구된다.</p>
</li>
<li>
<p><strong>실용적 도입:</strong> PINN이 연구실 수준을 넘어 산업 현장에서 널리 사용되기 위해서는, 비전문가도 쉽게 사용할 수 있는 고수준의 소프트웨어 프레임워크와 검증된 모범 사례(best practices)의 확립이 필요하다.14 문제 정의만으로 적절한 신경망 구조와 하이퍼파라미터를 자동으로 제안해주는 기능 등이 포함되어야 할 것이다.</p>
</li>
</ul>
<p>PINN의 궁극적인 성공은 현재의 ‘장인(artisan)’ 단계에서 벗어나 표준화되고 자동화된 ‘공학(engineering)’ 단계로 전환될 수 있는지에 달려 있다. 현재 PINN을 성공적으로 적용하기 위해서는 신경망 아키텍처 설계, 활성화 함수 선택, 손실 가중치 튜닝, 옵티마이저 선택 등 수많은 요소에 대한 깊은 이해와 많은 시행착오가 필요하다.12 이는 마치 장인이 경험과 직관에 의존해 작품을 만드는 것과 유사하며, 전문가가 아닌 사용자의 접근을 매우 어렵게 만든다. 과거 FEM 소프트웨어가 산업 표준으로 자리 잡을 수 있었던 이유는, 복잡한 수학적 이론을 추상화하고 사용자가 물리 문제 정의에만 집중할 수 있도록 표준화된 워크플로우와 자동화된 도구(예: 자동 메시 생성)를 제공했기 때문이다. PINN이 이와 같은 수준의 실용성을 갖추기 위해서는, 문제의 특성에 따라 최적의 하이퍼파라미터와 아키텍처를 자동으로 제안하는 AutoML(Automated Machine Learning) 기술의 통합이 필수적이다.34 또한, 다양한 PINN 변형 모델들을 쉽게 조합하고 사용할 수 있는 모듈식 라이브러리와 프레임워크의 발전이 중요하다.14 따라서 PINN의 미래는 알고리즘 자체의 발전뿐만 아니라, 이러한 알고리즘을 비전문가도 신뢰하고 쉽게 활용할 수 있도록 만드는 ’민주화(democratization)’와 ’공학적 성숙도’를 달성하는 데 달려 있다.</p>
<h3>7.3. 종합적 결론</h3>
<p>물리 정보 신경망(PINN)은 전통적인 수치 해석과 현대적인 데이터 과학의 경계를 허무는 혁신적인 도구이다. 비록 해결해야 할 이론적, 실용적 과제들이 남아있지만, 데이터가 부족한 환경에서의 역문제 해결, 복잡한 시스템에 대한 데이터 동화, 그리고 고차원 문제에 대한 잠재력은 과학 및 공학 시뮬레이션의 미래를 근본적으로 바꿀 강력한 동력이 될 것이다. PINN은 기존의 수치 해석 방법론을 완전히 대체하기보다는, 이들과 상호 보완적인 관계를 형성하며 발전할 가능성이 높다. 전통적 방법론이 제공하는 정확성과 신뢰성을 바탕으로, PINN이 제공하는 유연성과 데이터 통합 능력을 결합함으로써, 우리는 이전에는 다룰 수 없었던 더 복잡하고 현실적인 문제들에 도전할 수 있게 될 것이다. 궁극적으로, PINN은 과학적 발견과 공학적 혁신을 가속화하는 새로운 길을 열어주는 핵심 기술로 자리매김할 것으로 기대된다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>Can physics-informed neural networks beat the finite element method? - Oxford Academic, 10월 11, 2025에 액세스, https://academic.oup.com/imamat/article/89/1/143/7680268</li>
<li>From PINNs to PIKANs: Recent Advances in Physics-Informed Machine Learning - arXiv, 10월 11, 2025에 액세스, https://arxiv.org/html/2410.13228v1</li>
<li>A Short Introduction to Physics-informed Neural Networks (PINNs) | by Vivek Karmarkar, 10월 11, 2025에 액세스, https://medium.com/@vivek-karmarkar/a-short-introduction-to-physics-informed-neural-networks-pinns-cd342f5a3c5e</li>
<li>[D] I don’t understand why Physics Informed Neural Networks (PINNs) are an area of Research : r/MachineLearning - Reddit, 10월 11, 2025에 액세스, https://www.reddit.com/r/MachineLearning/comments/18mnl9f/d_i_dont_understand_why_physics_informed_neural/</li>
<li>What Are Physics-Informed Neural Networks (PINNs)? - MATLAB &amp; Simulink - MathWorks, 10월 11, 2025에 액세스, https://www.mathworks.com/discovery/physics-informed-neural-networks.html</li>
<li>Understanding Physics-Informed Neural Networks: Techniques, Applications, Trends, and Challenges - MDPI, 10월 11, 2025에 액세스, https://www.mdpi.com/2673-2688/5/3/74</li>
<li>Physics Informed Neural Networks explained for beginners | From scratch implementation and code - YouTube, 10월 11, 2025에 액세스, https://www.youtube.com/watch?v=1AyAia_NZhQ</li>
<li>Physics-informed neural networks - Wikipedia, 10월 11, 2025에 액세스, https://en.wikipedia.org/wiki/Physics-informed_neural_networks</li>
<li>(PDF) Physics-Informed Neural Network: Principles and Applications - ResearchGate, 10월 11, 2025에 액세스, https://www.researchgate.net/publication/382821954_Physics-informed_neural_network_principles_and_applications</li>
<li>Physics-Informed Neural Networks (PINNs) - Emergent Mind, 10월 11, 2025에 액세스, https://www.emergentmind.com/topics/physics-informed-neural-network-pinn-based-method</li>
<li>Physics Informed Neural Networks (PINNs) in PhysicsNeMo Sym - NVIDIA Docs, 10월 11, 2025에 액세스, https://docs.nvidia.com/physicsnemo/latest/physicsnemo-sym/user_guide/theory/phys_informed.html</li>
<li>Physics Informed Neural Network, 10월 11, 2025에 액세스, https://iitu.edu.kz/documents/3421/Lecture_14_TQAJmps.pdf</li>
<li>[2408.16806] Physics-Informed Neural Networks and Extensions - arXiv, 10월 11, 2025에 액세스, https://arxiv.org/abs/2408.16806</li>
<li>From PINNs to PIKANs: Recent Advances in Physics-Informed Machine Learning - arXiv, 10월 11, 2025에 액세스, https://arxiv.org/abs/2410.13228</li>
<li>(PDF) A comprehensive review of advances in physics-informed neural networks and their applications in complex fluid dynamics - ResearchGate, 10월 11, 2025에 액세스, https://www.researchgate.net/publication/384576812_A_comprehensive_review_of_advances_in_physics-informed_neural_networks_and_their_applications_in_complex_fluid_dynamics</li>
<li>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations (Journal Article) - OSTI, 10월 11, 2025에 액세스, https://www.osti.gov/pages/biblio/1595805</li>
<li>Revolutionary Physics Informed Neural Networks (PINNs) Guide, 10월 11, 2025에 액세스, https://caeassistant.com/blog/physics-informed-neural-networks-pinns/</li>
<li>Marrying the benefits of Automatic and Numerical Differentiation in Physics-Informed Neural Network, 10월 11, 2025에 액세스, https://ml4physicalsciences.github.io/2021/files/NeurIPS_ML4PS_2021_50.pdf</li>
<li>Physics Informed and Neural Operator Networks - Stone Ridge Technology, 10월 11, 2025에 액세스, https://stoneridgetechnology.com/company/blog/physics-informed-and-neural-operator-networks/</li>
<li>Physics Informed Neural Networks (PINNs) - Kaggle, 10월 11, 2025에 액세스, https://www.kaggle.com/code/newtonbaba12345/physics-informed-neural-networks-pinns</li>
<li>Physics Informed Neural Networks (PINNs): An Intuitive Guide | by Ian Henderson - Medium, 10월 11, 2025에 액세스, https://medium.com/data-science/physics-informed-neural-networks-pinns-an-intuitive-guide-fff138069563</li>
<li>(PDF) A Physics-Informed Neural Network Framework For Partial Differential Equations on 3D Surfaces: Time Independent Problems - ResearchGate, 10월 11, 2025에 액세스, https://www.researchgate.net/publication/338288312_A_Physics-Informed_Neural_Network_Framework_For_Partial_Differential_Equations_on_3D_Surfaces_Time_Independent_Problems</li>
<li>Solve Poisson Equation on Unit Disk Using Physics-Informed Neural Networks - MATLAB &amp; Simulink - MathWorks, 10월 11, 2025에 액세스, https://www.mathworks.com/help/pde/ug/solve-poisson-equation-on-unit-disk-using-pinn.html</li>
<li>AAAI Press Formatting Instructions for Authors Using LaTeX – A Guide, 10월 11, 2025에 액세스, https://www.osti.gov/servlets/purl/1836905</li>
<li>(PDF) A physics-informed neural network technique based on a modified loss function for computational 2D and 3D solid mechanics - ResearchGate, 10월 11, 2025에 액세스, https://www.researchgate.net/publication/365820180_A_physics-informed_neural_network_technique_based_on_a_modified_loss_function_for_computational_2D_and_3D_solid_mechanics</li>
<li>Physics Informed Neural Network for Dynamic Stress Prediction - Michigan State University, 10월 11, 2025에 액세스, https://hal.cse.msu.edu/assets/pdfs/papers/2023-apin-pinn-dynamic-stress-prediction.pdf</li>
<li>Physics-Informed Neural Networks for Advanced Thermal … - MDPI, 10월 11, 2025에 액세스, https://www.mdpi.com/2313-0105/11/6/204</li>
<li>Hybrid Finite-Difference Physics-Informed Neural Networks Partial Differential Equation Solver for Complex Geometries | Journal of Thermophysics and Heat Transfer - Aerospace Research Central, 10월 11, 2025에 액세스, https://arc.aiaa.org/doi/10.2514/1.T7077</li>
<li>Enhancing Computational Accuracy in Surrogate Modeling for Elastic–Plastic Problems by Coupling S-FEM and Physics-Informed Deep Learning - MDPI, 10월 11, 2025에 액세스, https://www.mdpi.com/2227-7390/11/9/2016</li>
<li>Physics-Informed Neural Networks: A Review of Methodological Evolution, Theoretical Foundations, and Interdisciplinary Frontiers Toward Next-Generation Scientific Computing - MDPI, 10월 11, 2025에 액세스, https://www.mdpi.com/2076-3417/15/14/8092</li>
<li>[1711.10561] Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations - arXiv, 10월 11, 2025에 액세스, https://arxiv.org/abs/1711.10561</li>
<li>maziarraissi/PINNs: Physics Informed Deep Learning: Data … - GitHub, 10월 11, 2025에 액세스, https://github.com/maziarraissi/PINNs</li>
<li>[D] What is the point of physics-informed neural networks if you need to know the actual physics? - Reddit, 10월 11, 2025에 액세스, https://www.reddit.com/r/MachineLearning/comments/12lzzv6/d_what_is_the_point_of_physicsinformed_neural/</li>
<li>Exploring Physics-Informed Neural Networks: From Fundamentals to Applications in Complex Systems - arXiv, 10월 11, 2025에 액세스, https://arxiv.org/html/2410.00422v1</li>
<li>FDM data driven U-Net as a 2D Laplace PINN solver - PMC, 10월 11, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC10241951/</li>
<li>Can physics-informed neural networks beat the finite element method? - PMC, 10월 11, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC11197852/</li>
<li>[2302.04107] Can Physics-Informed Neural Networks beat the Finite Element Method? - arXiv, 10월 11, 2025에 액세스, https://arxiv.org/abs/2302.04107</li>
<li>PINN-FEM: A Hybrid Approach for Enforcing Dirichlet Boundary Conditions in Physics-Informed Neural Networks - arXiv, 10월 11, 2025에 액세스, https://arxiv.org/html/2501.07765v1</li>
<li>[2506.20537] Physics-Informed Machine Learning Regulated by Finite Element Analysis for Simulation Acceleration of Laser Powder Bed Fusion - arXiv, 10월 11, 2025에 액세스, https://arxiv.org/abs/2506.20537</li>
<li>Extended Physics-informed Neural Networks (XPINNs): A Generalized Space-Time Domain Decomposition based Deep Learning Framework for Nonlinear Partial Differential Equations - CEUR-WS.org, 10월 11, 2025에 액세스, https://ceur-ws.org/Vol-2964/article_60.pdf</li>
<li>When Do Extended Physics-Informed Neural Networks (XPINNs) Improve Generalization? | SIAM Journal on Scientific Computing, 10월 11, 2025에 액세스, https://epubs.siam.org/doi/10.1137/21M1447039</li>
<li>Extended Physics-Informed Neural Networks (XPINNs): A Generalized Space-Time Domain Decomposition Based Deep Learning Framework for Nonlinear Partial Differential Equations - Global Science Press, 10월 11, 2025에 액세스, https://global-sci.com/pdf/article/79747/extended-physics-informed-neural-networks-xpinns-a-generalized-space-time-domain-decomposition-based-deep-learning-framework-for-nonlinear-partial-differential-equations.pdf</li>
<li>Extended Physics-Informed Neural Networks (XPINNs): A Generalized Space-Time Domain Decomposition Based Deep Learning Framework for Nonlinear Partial Differential Equations - Global Science Press, 10월 11, 2025에 액세스, https://global-sci.com/article/79747/extended-physics-informed-neural-networks-xpinns-a-generalized-space-time-domain-decomposition-based-deep-learning-framework-for-nonlinear-partial-differential-equations</li>
<li>Conservative_PINNs (cPINNs on decomposed domains for conservation laws) - GitHub, 10월 11, 2025에 액세스, https://github.com/AmeyaJagtap/Conservative_PINNs</li>
<li>Physics-Informed Neural Networks and Extensions - arXiv, 10월 11, 2025에 액세스, https://arxiv.org/html/2408.16806v1</li>
<li>[2104.10013] Parallel Physics-Informed Neural Networks via Domain Decomposition - arXiv, 10월 11, 2025에 액세스, https://arxiv.org/abs/2104.10013</li>
<li>Extended Physics-Informed Neural Networks (XPINNs): A Generalized Space-Time Domain Decomposition Based Deep Learning Framework for Nonlinear Partial Differential Equations (Journal Article) - OSTI, 10월 11, 2025에 액세스, https://www.osti.gov/pages/biblio/2282003</li>
<li>Physics-Informed Neural Networks: Bridging the Divide Between Conservative and Non-Conservative Equations - arXiv, 10월 11, 2025에 액세스, https://arxiv.org/html/2506.22413v1</li>
<li>$PINN - a Domain Decomposition Method for Bayesian Physics-Informed Neural Networks1footnote 11footnote 1 - arXiv, 10월 11, 2025에 액세스, https://arxiv.org/html/2504.19013v3</li>
<li>Recipes for when physics fails: recovering robust learning of physics informed neural networks - PMC, 10월 11, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC10481851/</li>
<li>[2305.12817] Conservative Physics-Informed Neural Networks for Non-Conservative Hyperbolic Conservation Laws Near Critical States - arXiv, 10월 11, 2025에 액세스, https://arxiv.org/abs/2305.12817</li>
<li>Bayesian Physics-Informed Neural Networks for Inverse Uncertainty Quantification problems in Cardiac Electrophysiology - POLITesi - Politecnico di Milano, 10월 11, 2025에 액세스, https://www.politesi.polimi.it/retrieve/f8c9fa2a-9bac-4f9c-83b9-28b137f379c7/2021_04_Ceccarelli.pdf</li>
<li>B-PINNs: Bayesian physics-informed neural networks for forward and inverse PDE problems with noisy data (Journal Article) - OSTI, 10월 11, 2025에 액세스, https://www.osti.gov/pages/biblio/2282008</li>
<li>Bayesian Deep Learning Framework for Uncertainty Quantification …, 10월 11, 2025에 액세스, https://epubs.siam.org/doi/10.1137/23M1560574</li>
<li>Andrewpensoneault/HMC-B-PINN—Jax-tutorial - GitHub, 10월 11, 2025에 액세스, https://github.com/Andrewpensoneault/HMC-B-PINN—Jax-tutorial</li>
<li>On the estimation rate of Bayesian PINN for inverse problems - Mathematics &amp; Statistics, 10월 11, 2025에 액세스, https://math.bu.edu/people/atchade/B_pinn.pdf</li>
<li>Guaranteeing Conservation Laws with Projection in Physics …, 10월 11, 2025에 액세스, https://research.ibm.com/publications/guaranteeing-conservation-laws-with-projection-in-physics-informed-neural-networks</li>
<li>PieroPaialungaAI/BlackScholesPINN: A Python implementation of Physics Informed Neural Networks for Black-Scholes Model - GitHub, 10월 11, 2025에 액세스, https://github.com/PieroPaialungaAI/BlackScholesPINN</li>
<li>Physics-Informed Neural Networks (PINNs) for Option Pricing » Quantitative Finance - MATLAB &amp; Simulink - MathWorks Blogs, 10월 11, 2025에 액세스, https://blogs.mathworks.com/finance/2025/01/07/physics-informed-neural-networks-pinns-for-option-pricing/</li>
<li>Bayesian Fourier-Feature PINNs for Option Pricing: A New Frontier in Financial Modeling, 10월 11, 2025에 액세스, https://medium.com/@shriyaejanthker/bayesian-fourier-feature-pinns-for-option-pricing-a-new-frontier-in-financial-modeling-667f33f6b33f</li>
<li>A Physics informed neural network approach for solving time fractional Black-Scholes partial differential equations - ResearchGate, 10월 11, 2025에 액세스, https://www.researchgate.net/publication/382826826_A_Physics_informed_neural_network_approach_for_solving_time_fractional_Black-Scholes_partial_differential_equations</li>
<li>A Hybrid PINNs-FNO Approach for Advanced Option Pricing Models - Preprints.org, 10월 11, 2025에 액세스, https://www.preprints.org/manuscript/202501.0629/v1</li>
<li>[2503.18181] Adaptive Physics-informed Neural Networks: A Survey - arXiv, 10월 11, 2025에 액세스, https://arxiv.org/abs/2503.18181</li>
<li>Scientific Machine Learning through Physics-Informed Neural …, 10월 11, 2025에 액세스, https://arxiv.org/abs/2201.05624</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>