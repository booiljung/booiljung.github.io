<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:LLaVA-NeXT (Large Language and Vision Assistant - Next)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>LLaVA-NeXT (Large Language and Vision Assistant - Next)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">VLM (Vision Language Model, 시각 언어 모델)</a> / <span>LLaVA-NeXT (Large Language and Vision Assistant - Next)</span></nav>
                </div>
            </header>
            <article>
                <h1>LLaVA-NeXT (Large Language and Vision Assistant - Next)</h1>
<p>2025-12-09, G30DR</p>
<h2>1.  서론: 멀티모달 지능의 민주화와 LLaVA의 위상</h2>
<p>인공지능 연구의 흐름은 텍스트 중심의 거대 언어 모델(LLM, Large Language Model)에서 시각, 청각 등 다양한 감각 정보를 통합적으로 처리하는 대형 멀티모달 모델(LMM, Large Multimodal Model)로 급격히 이동하고 있다. 이러한 기술적 패러다임의 전환 속에서 LLaVA(Large Language and Vision Assistant) 시리즈는 오픈소스 진영의 혁신을 주도하는 핵심 축으로 자리 잡았다. 특히 본 보고서에서 중점적으로 다루는 **LLaVA-NeXT(또는 LLaVA-1.6)**는 단순한 성능 개선을 넘어, 모델 아키텍처의 유연성, 데이터 처리의 효율성, 그리고 모달리티의 확장성 측면에서 괄목할 만한 기술적 도약을 이루어냈다.</p>
<p>초기의 LMM 연구가 GPT-4V나 Gemini와 같은 폐쇄형(Closed-source) 상용 모델들에 의해 주도되었다면, LLaVA 시리즈는 누구나 접근 가능한 고성능 모델을 제공함으로써 연구와 응용의 민주화를 이끌고 있다. LLaVA-NeXT는 LLaVA-1.5의 “미니멀리즘(Minimalism)” 설계 철학을 계승하면서도, 고해상도 이미지 처리 기술인 ’AnyRes’와 다양한 최신 LLM(Llama-3, Qwen-1.5 등)의 통합을 통해 상용 모델과 대등하거나 이를 능가하는 성능을 입증하였다.1</p>
<p>본 연구 보고서는 LLaVA-NeXT의 기술적 기반이 되는 아키텍처의 심층 분석부터 시작하여, 학습 방법론, 비디오 및 다중 이미지 처리 능력, 그리고 방대한 벤치마크 결과를 통한 성능 검증까지 포괄적으로 다룬다. 또한, 오픈소스 생태계 내에서의 파생 모델(Video, Interleave, Critic)들의 역할과 라이선스 정책이 산업계에 미치는 영향까지 면밀히 고찰함으로써, LLaVA-NeXT가 제시하는 멀티모달 AI의 미래 방향성을 제시하고자 한다. 모든 분석은 제공된 연구 자료에 기반하여 엄밀하게 수행되었으며, 각 기술적 주장은 명확한 근거를 바탕으로 서술된다.</p>
<h2>2.  LLaVA 시리즈의 진화와 기술적 계보</h2>
<p>LLaVA-NeXT의 기술적 성취를 온전히 이해하기 위해서는 그 전신인 LLaVA-1.5와 초기 LLaVA 모델로부터 이어지는 기술적 계보를 파악해야 한다. 이는 단순한 버전 업그레이드가 아닌, 시각적 정보를 언어적 지식과 결합하는 방식의 근본적인 최적화 과정을 보여주기 때문이다.</p>
<h3>2.1  초기 LLaVA와 Visual Instruction Tuning의 정립</h3>
<p>초기 LLaVA 모델은 “Visual Instruction Tuning“이라는 개념을 대중화시킨 선구적인 모델이다. 이는 텍스트 기반의 지시 튜닝(Instruction Tuning)을 시각 도메인으로 확장한 것으로, 이미지와 텍스트가 쌍을 이루는 데이터를 통해 모델이 사용자의 시각적 질문에 대해 논리적으로 답변하도록 훈련시키는 방법론이다.3 LLaVA는 비전 인코더(Vision Encoder)인 CLIP과 언어 모델인 Vicuna를 연결(Connector)하는 간단한 투영 레이어(Projection Layer)만으로도 놀라운 성능을 발휘할 수 있음을 증명했다. 이는 복잡한 크로스 어텐션(Cross-attention) 모듈이나 Q-Former와 같은 무거운 구조 없이도 효율적인 멀티모달 학습이 가능함을 시사했다.</p>
<h3>2.2  LLaVA-1.5의 도약과 데이터 효율성</h3>
<p>LLaVA-1.5는 2023년 10월에 공개되어 단 하루 만에 8개의 A100 GPU로 학습을 완료할 수 있는 효율성을 보여주었다.3 이 모델은 MLP(Multi-Layer Perceptron) 커넥터의 도입과 약 66만 개의 정제된 데이터를 사용하여 당시 11개 벤치마크에서 최고 성능(SoTA)을 기록했다.3 LLaVA-1.5의 성공은 “데이터의 양보다 질과 구조가 중요하다“는 가설을 입증했으며, 이는 후속 모델인 LLaVA-NeXT 설계의 밑거름이 되었다.</p>
<h3>2.3  LLaVA-NeXT: 한계를 넘어서는 확장</h3>
<p>2024년 1월, LLaVA 팀은 LLaVA-NeXT(LLaVA-1.6)를 공개하며 다시 한번 기준을 높였다.1 LLaVA-NeXT의 개발 목표는 명확했다. 기존 모델들이 가졌던 고정 해상도의 한계를 극복하고, 더 강력한 LLM 백본을 수용하며, 이미지뿐만 아니라 비디오와 다중 이미지로의 확장을 꾀하는 것이다. 특히, LLaVA-NeXT는 LLaVA-1.5의 데이터 효율성을 유지하면서도 처리 가능한 픽셀 수를 4배로 늘려 세밀한 시각 정보 처리 능력을 비약적으로 향상시켰다.1</p>
<h2>3.  아키텍처 혁신: AnyRes와 동적 시각 처리</h2>
<p>LLaVA-NeXT의 가장 핵심적인 기술적 혁신은 <strong>AnyRes(Any Resolution)</strong> 기술의 도입이다. 이는 기존의 LMM들이 겪었던 고질적인 문제인 ’해상도와 인식률의 트레이드오프’를 해결하는 데 결정적인 역할을 했다.</p>
<h3>3.1  고정 해상도의 한계와 AnyRes의 등장 배경</h3>
<p>대부분의 기존 LMM은 사전 학습된 CLIP(Contrastive Language-Image Pre-training) 비전 인코더를 사용한다. CLIP 모델은 일반적으로 224x224 또는 336x336과 같은 고정된 정사각형 해상도의 이미지를 입력으로 받는다. 따라서 입력 이미지가 고해상도이거나 극단적인 종횡비(예: 긴 영수증, 파노라마 사진)를 가질 경우, 강제적인 리사이징(Resizing)이나 패딩(Padding) 과정에서 심각한 정보 손실이나 왜곡이 발생했다.5 이는 작은 텍스트를 읽는 OCR(Optical Character Recognition) 작업이나 세밀한 객체 탐지에서 성능 저하의 주원인이 되었다.</p>
<h3>3.2  AnyRes(Dynamic High-Resolution) 기술의 메커니즘</h3>
<p>LLaVA-NeXT에 적용된 AnyRes 기술은 이러한 제약을 극복하기 위해 이미지를 동적으로 분할하고 재구성하는 전략을 취한다. 그 구체적인 작동 원리는 다음과 같다5:</p>
<ol>
<li><strong>동적 그리드 결정 (Dynamic Grid Configuration):</strong> 모델은 입력 이미지의 해상도와 종횡비를 분석하여, 사전 정의된 여러 그리드 설정(예: 1x2, 2x1, 2x2, 1x3, 3x1 등) 중 가장 적합한 구성을 선택한다. 이를 통해 이미지의 원본 비율을 최대한 보존하면서 효율적인 처리가 가능하다.</li>
<li><strong>이미지 분할 및 패치화 (Image Splitting &amp; Patching):</strong> 선택된 그리드에 따라 이미지는 여러 개의 ’하위 이미지(Sub-image)’로 분할된다. 예를 들어, 672x672 해상도의 이미지는 4개의 336x336 패치로 나뉠 수 있다. 이를 통해 모델은 기존 LLaVA-1.5 대비 최대 4배 더 많은 픽셀 정보를 처리할 수 있게 되었다.1</li>
<li><strong>글로벌 컨텍스트 보존 (Global Context Preservation):</strong> 단순히 이미지를 잘라서 입력할 경우, 전체적인 맥락(Global Context)을 잃어버릴 위험이 있다. LLaVA-NeXT는 이를 방지하기 위해 원본 이미지를 축소한 ‘전체 뷰(Global View)’ 이미지를 별도로 생성하여 분할된 패치들과 함께 입력한다.</li>
<li><strong>토큰 연결 및 입력 (Token Concatenation):</strong> 각 하위 이미지와 전체 뷰 이미지는 비전 인코더를 통과하여 임베딩 벡터로 변환된다. 이 벡터들은 순차적으로 연결(Concatenation)되어 LLM의 입력으로 들어간다. LLM은 특수 토큰(예: <code>&lt;image&gt;</code>, <code>&lt;newline&gt;</code> 등)을 통해 각 패치의 위치와 공간적 관계를 인식한다.</li>
</ol>
<p>이러한 AnyRes 기술 덕분에 LLaVA-NeXT는 672x672, 336x1344, 1344x336 등 다양한 해상도를 유연하게 처리할 수 있으며, 이는 특히 문서 인식, 차트 분석, 그리고 복잡한 장면 이해에서 탁월한 성능 향상을 가져왔다.1</p>
<h3>3.3  비전-언어 커넥터의 최적화</h3>
<p>LLaVA-NeXT는 LLaVA-1.5에서 검증된 <strong>MLP(Multi-Layer Perceptron) 프로젝션 레이어</strong>를 커넥터로 사용한다.1 일부 연구에서는 Q-Former나 복잡한 어텐션 기반 커넥터를 제안하기도 했으나, LLaVA 팀은 단순한 MLP 구조가 학습의 안정성과 효율성 측면에서 더 유리하다고 판단했다. 이 커넥터는 비전 인코더가 추출한 시각적 특징(Visual Features)을 언어 모델의 임베딩 공간(Embedding Space)으로 투영하여, LLM이 시각 정보를 마치 텍스트 토큰처럼 처리할 수 있게 한다.</p>
<p>LLaVA-NeXT의 커넥터는 이전 버전의 사전 학습된 가중치를 재사용함으로써 학습 시간을 단축하고 초기 수렴 속도를 높이는 전략을 취했다.1 이는 제한된 컴퓨팅 자원으로도 고성능 모델을 개발할 수 있게 하는 중요한 요인 중 하나이다.</p>
<h2>4.  백본 LLM의 다양화와 지능의 확장</h2>
<p>LLaVA-NeXT의 또 다른 특징은 단일 LLM에 종속되지 않고, 오픈소스 커뮤니티에서 가장 강력한 성능을 보이는 다양한 LLM을 백본(Backbone)으로 채택했다는 점이다. 이는 멀티모달 모델의 전반적인 추론 능력과 언어 처리 능력을 극대화하기 위한 전략이다.</p>
<h3>4.1  Vicuna에서 Mistral, 그리고 그 너머로</h3>
<p>초기 LLaVA 모델은 Llama 기반의 Vicuna 모델을 주로 사용했다. 그러나 LLaVA-NeXT(1.6)는 당시 7B 파라미터 규모에서 압도적인 성능을 보였던 <strong>Mistral-7B</strong>와 <strong>Vicuna-1.5-7B</strong>를 기반으로 시작되었다.8 Mistral-7B의 도입은 모델의 지시 이행 능력과 일반 상식 추론 능력을 크게 향상시켰다.</p>
<h3>4.2  Llama-3와 Qwen-1.5의 통합: “Stronger” 모델군</h3>
<p>2024년 5월, LLaVA 팀은 **Llama-3 (8B)**와 **Qwen-1.5 (72B/110B)**를 탑재한 “Stronger” 버전의 LLaVA-NeXT를 공개했다.2</p>
<ul>
<li><strong>Llama-3 통합:</strong> Meta의 Llama-3는 8B라는 비교적 작은 크기에도 불구하고 뛰어난 언어 능력을 보여주었다. 이를 LLaVA-NeXT에 통합함으로써, 사용자들은 가벼우면서도 강력한 대화형 에이전트를 구축할 수 있게 되었다.</li>
<li><strong>Qwen-1.5 통합:</strong> Alibaba의 Qwen-1.5 시리즈, 특히 72B와 110B 모델의 통합은 오픈소스 LMM의 성능 상한선을 대폭 끌어올렸다.2 110B 모델은 매우 깊고 복잡한 추론이 필요한 작업에서 GPT-4V와 같은 상용 모델에 근접하거나 능가하는 성능을 보여준다.</li>
</ul>
<p>이러한 다양한 백본 지원은 <strong>vLLM</strong>과 같은 추론 라이브러리에서 공식적으로 지원되며, 사용자의 하드웨어 환경(VRAM 용량 등)과 목적에 따라 최적의 모델을 선택할 수 있는 유연성을 제공한다.9</p>
<h2>5.  학습 데이터 전략: 양보다는 질</h2>
<p>LLaVA-NeXT의 성공 요인 중 하나는 무작위적인 대규모 데이터 학습을 지양하고, 고도로 정제된 데이터 믹스처(Data Mixture)를 사용했다는 점이다.</p>
<h3>5.1  데이터 효율성의 극대화</h3>
<p>LLaVA-NeXT는 약 130만(1.3M) 개의 시각적 지시 튜닝 데이터를 사용하여 학습되었다.1 이는 수천만, 수억 개의 이미지-텍스트 쌍을 사용하는 다른 모델들에 비해 매우 적은 양이다. 그럼에도 불구하고 최고의 성능을 낼 수 있었던 이유는 데이터의 **다양성(Diversity)**과 **품질(Quality)**에 집중했기 때문이다.</p>
<h3>5.2  데이터 구성의 3대 축</h3>
<p>LLaVA-NeXT의 학습 데이터는 크게 세 가지 범주로 나눌 수 있다:</p>
<ol>
<li><strong>대화 및 지시 이행 데이터:</strong> 사용자의 다양한 질문에 자연스럽게 대답하고, 복잡한 지시를 수행할 수 있도록 돕는 데이터이다. 여기에는 LLaVA-1.5에서 사용된 데이터셋 외에도 GPT-4V 등을 이용해 생성한 고품질 대화 데이터가 포함된다.6</li>
<li><strong>문서 및 차트 이해 (Document &amp; Chart Understanding):</strong> LLaVA-NeXT는 텍스트가 많은 문서나 복잡한 차트를 해석하는 능력을 강화하기 위해 <strong>ChartQA, DVQA, AI2D</strong>와 같은 전문 데이터셋을 학습 데이터에 추가했다.1 이는 Qwen-VL과 같은 경쟁 모델들이 보여준 OCR 및 차트 해석 능력에 대응하기 위한 전략적 선택이었다.</li>
<li><strong>다국어 및 세계 지식:</strong> 비록 영어 데이터 중심이지만, 기반이 되는 LLM들의 다국어 능력 덕분에 중국어 등 타 언어에 대한 제로샷 처리 능력이 발현되었다. 이는 데이터셋 자체의 언어적 다양성보다는 LLM의 사전 학습된 지식이 전이된 결과로 해석된다.1</li>
</ol>
<h2>6.  모달리티의 확장: 비디오 및 다중 이미지</h2>
<p>LLaVA-NeXT는 정지 이미지(Static Image) 처리에 머무르지 않고, 시간 축(Temporal Axis)을 포함하는 비디오와 여러 이미지를 동시에 처리하는 다중 이미지(Multi-image) 영역으로 확장되었다.</p>
<h3>6.1  LLaVA-NeXT-Video: 제로샷 전이와 성능 혁신</h3>
<p>가장 놀라운 발견 중 하나는 이미지 데이터로만 학습된 LLaVA-NeXT가 비디오 이해 작업에서도 뛰어난 <strong>제로샷(Zero-shot) 성능</strong>을 보인다는 것이다.2</p>
<ul>
<li><strong>작동 원리:</strong> 비디오는 본질적으로 연속된 이미지 프레임의 집합이다. LLaVA-NeXT는 AnyRes 기술을 통해 이미지를 여러 패치로 나누어 처리하는 방식과 유사하게, 비디오의 각 프레임을 독립적인 이미지 패치처럼 인식하여 처리한다. 이를 통해 별도의 비디오 데이터 학습 없이도 비디오의 내용을 설명하거나 질문에 답할 수 있다.</li>
<li><strong>긴 비디오 처리 (Length Generalization):</strong> LLaVA-NeXT-Video 모델은 선형 스케일링(Linear Scaling) 기법을 통해 학습 때보다 더 긴 비디오 시퀀스(최대 56프레임 이상)를 처리할 수 있다.12 이는 모델이 시간적 맥락을 유지하면서 더 많은 정보를 받아들일 수 있음을 의미한다.</li>
<li><strong>DPO를 통한 정렬 (Alignment):</strong> 비디오 모델의 성능을 더욱 높이기 위해 <strong>DPO(Direct Preference Optimization)</strong> 기법이 적용되었다. AI 피드백을 기반으로 모델의 응답을 최적화함으로써, 비디오 캡셔닝의 정확도와 자연스러움을 크게 향상시켰다.12</li>
</ul>
<h3>6.2  LLaVA-NeXT-Interleave: 멀티 이미지 시나리오</h3>
<p>실제 웹 환경이나 문서는 텍스트와 이미지가 번갈아 나타나는(Interleaved) 경우가 많다. <strong>LLaVA-NeXT-Interleave</strong> 모델은 이러한 다중 이미지 시나리오를 처리하기 위해 개발되었다.7</p>
<ul>
<li><strong>다중 이미지 간의 관계 추론:</strong> 여러 장의 이미지를 동시에 입력받아 이미지 간의 차이점 찾기, 스토리텔링, 순서 배열 등의 작업을 수행할 수 있다.</li>
<li><strong>응용 분야:</strong> 뉴스 기사 분석, 매뉴얼 단계별 설명, 만화(Comic) 이해 등 다양한 실제 응용 분야에서 강력한 성능을 발휘한다.</li>
</ul>
<h2>7.  성능 벤치마크 및 비교 분석</h2>
<p>LLaVA-NeXT의 성능은 다양한 벤치마크를 통해 객관적으로 입증되었다. 특히 상용 모델인 Gemini Pro, GPT-4V와의 비교에서 괄목할 만한 성과를 거두었다.</p>
<h3>7.1  이미지 이해 성능 (Image Understanding)</h3>
<p>다음은 주요 벤치마크에서의 성능 비교 요약이다.1</p>
<table><thead><tr><th><strong>벤치마크 (Benchmark)</strong></th><th><strong>LLaVA-NeXT-34B</strong></th><th><strong>Gemini Pro</strong></th><th><strong>GPT-4V</strong></th><th><strong>LLaVA-1.5-13B</strong></th><th><strong>설명</strong></th></tr></thead><tbody>
<tr><td><strong>VQAv2</strong></td><td><strong>Comparable</strong></td><td>71.2</td><td>77.2</td><td>80.0</td><td>일반적인 시각적 질의응답</td></tr>
<tr><td><strong>TextVQA</strong></td><td><strong>SoTA Level</strong></td><td>74.6</td><td>78.0</td><td>61.3</td><td>이미지 내 텍스트 인식 및 추론</td></tr>
<tr><td><strong>MMBench</strong></td><td><strong>Competitive</strong></td><td>-</td><td>-</td><td>-</td><td>종합적인 멀티모달 능력 평가</td></tr>
</tbody></table>
<p>위 표에서 볼 수 있듯이, LLaVA-NeXT(특히 34B 모델)는 TextVQA와 같은 OCR 중심의 벤치마크에서 기존 LLaVA-1.5 대비 비약적인 성능 향상을 보였으며, Google의 Gemini Pro를 상회하는 결과를 기록하기도 했다.1 이는 AnyRes 기술을 통한 고해상도 처리 능력이 OCR 성능에 결정적인 영향을 미쳤음을 시사한다.</p>
<h3>7.2  비디오 이해 성능 (Video Understanding)</h3>
<p>Video-MME 벤치마크 결과에 따르면, <strong>LLaVA-NeXT-Video</strong>는 오픈소스 모델 중 최상위권을 차지했다.15</p>
<ul>
<li><strong>상용 모델 비교:</strong> Gemini 1.5 Pro(약 75%)나 GPT-4o(약 71.9%)에는 다소 미치지 못하지만, 오픈소스 모델로서는 이례적으로 높은 성능을 보여주며 상용 모델과의 격차를 빠르게 좁히고 있다.</li>
<li><strong>오픈소스 내 위상:</strong> VideoChat-GPT나 기타 비디오 전용 LMM들과 비교했을 때, 이미지 기반 모델임에도 불구하고 더 우수하거나 대등한 성능을 보인다는 점은 LLaVA 아키텍처의 범용성을 증명한다.</li>
</ul>
<h2>8.  LLaVA-Critic: 자체 평가 및 진화하는 생태계</h2>
<p>오픈소스 LMM 생태계의 성숙도를 보여주는 또 하나의 사례는 <strong>LLaVA-Critic</strong>의 등장이다.</p>
<h3>8.1  LMM-as-a-Judge</h3>
<p>LLaVA-Critic은 다른 멀티모달 모델의 성능을 평가하고 점수를 매기는 ‘심판(Judge)’ 역할을 수행하도록 설계된 최초의 오픈소스 LMM이다.16 기존에는 GPT-4V와 같은 고성능 상용 모델만이 이러한 평가자 역할을 수행할 수 있었으나, LLaVA-Critic의 등장으로 오픈소스 커뮤니티는 비용 효율적이고 확장 가능한 자체 평가 파이프라인을 구축할 수 있게 되었다.</p>
<h3>8.2  선호도 학습 (Preference Learning)</h3>
<p>LLaVA-Critic은 단순히 점수만 매기는 것이 아니라, 모델의 응답에 대한 보상 신호(Reward Signal)를 생성하여 강화 학습(RLHF/DPO)에 활용될 수 있다. 이는 LMM이 인간의 피드백 없이도 스스로 성능을 개선하는 ‘자기 진화(Self-evolving)’ 시스템으로 나아가는 중요한 발판을 마련했다.</p>
<h2>9.  기술적 구현 및 배포 생태계</h2>
<p>LLaVA-NeXT는 연구용 모델에 그치지 않고, 실제 서비스에 적용될 수 있도록 강력한 배포 도구와 생태계를 지원한다.</p>
<h3>9.1  SGLang을 이용한 추론 가속화</h3>
<p>고해상도 이미지와 긴 비디오를 처리하는 LMM은 연산 비용이 매우 높다. 이를 해결하기 위해 LLaVA 팀은 <strong>SGLang</strong>이라는 효율적인 추론 프레임워크를 도입했다.2 SGLang을 활용하면 비디오 캡셔닝과 같은 작업에서 기존 대비 5배 이상의 속도 향상을 기대할 수 있으며, 이는 대규모 서비스 구축 시 필수적인 요소이다.</p>
<h3>9.2  vLLM 및 HuggingFace 통합</h3>
<p>LLaVA-NeXT는 널리 사용되는 LLM 서빙 라이브러리인 <strong>vLLM</strong>에 공식적으로 통합되어 있다.9 이는 개발자들이 복잡한 설정 없이도 <code>LlavaNextForConditionalGeneration</code> 클래스를 통해 모델을 쉽게 로드하고, 고속 추론 및 양자화(Quantization) 기능을 활용할 수 있게 해준다.8 또한 HuggingFace Transformers 라이브러리와의 호환성을 통해 전 세계의 연구자들이 손쉽게 모델을 실험하고 파인튜닝할 수 있는 환경을 제공한다.</p>
<h2>10.  라이선스 및 윤리적 고려사항</h2>
<p>LLaVA-NeXT의 활용에 있어 라이선스 정책을 이해하는 것은 매우 중요하다. 특히 상업적 이용을 계획하는 경우 각 구성 요소의 라이선스를 면밀히 검토해야 한다.</p>
<h3>10.1  복합 라이선스 구조</h3>
<p>LLaVA-NeXT 프로젝트 자체는 <strong>Apache-2.0 라이선스</strong>를 따르므로, 코드의 수정과 배포가 자유롭다.4 그러나 모델의 가중치(Checkpoint)와 학습 데이터는 원천 소스의 라이선스를 승계한다.</p>
<ul>
<li><strong>기반 LLM:</strong> Llama-3 기반 모델은 Meta의 커뮤니티 라이선스(월간 사용자 7억 명 이하 상업적 이용 가능)를 따르며, Qwen이나 Vicuna 기반 모델은 각각의 원작자가 정한 라이선스(연구용 또는 허가된 상업용)를 따른다.10</li>
<li><strong>학습 데이터:</strong> OpenAI의 GPT-4 등으로 생성된 데이터가 포함되어 있을 경우, “경쟁 모델 개발 금지” 조항이 적용될 여지가 있으나, LLaVA 팀은 이를 연구 목적으로 공개하고 있다. 특히 CC-BY-NC 4.0(비영리) 라이선스가 적용된 데이터셋이 섞여 있을 수 있으므로 상업적 데이터셋 구축 시 주의가 필요하다.3</li>
</ul>
<h3>10.2  사용 제한 및 책임</h3>
<p>LLaVA-NeXT의 모델 카드에는 “연구 목적 외 사용 금지” 또는 “상업적 사용 금지” 문구가 명시된 경우가 있다(특히 특정 데이터셋이나 초기 모델의 경우).10 따라서 기업 환경에서 이를 서비스에 도입하고자 할 때는 반드시 법적 검토를 거쳐야 하며, LLaVA 팀은 원천 라이선스 외에 추가적인 제약을 두지 않음을 명시하고 있다.4</p>
<h2>11.  결론: LLaVA-NeXT가 여는 미래</h2>
<p>LLaVA-NeXT에 대한 포괄적인 조사를 통해 도출된 결론은, 이 모델이 오픈소스 멀티모달 AI의 새로운 표준을 정립했다는 것이다.</p>
<p>첫째, <strong>효율성과 성능의 조화</strong>를 이루어냈다. AnyRes 기술과 고품질 데이터 큐레이션을 통해, 무조건적인 모델 크기 확장 없이도 상용 모델과 경쟁할 수 있는 성능을 달성했다. 이는 자원이 제한된 환경에서도 고성능 AI를 개발하고 운영할 수 있는 가능성을 보여준다.</p>
<p>둘째, <strong>확장 가능한 아키텍처</strong>를 증명했다. 이미지 모델을 기반으로 비디오, 다중 이미지, 그리고 평가 모델(Critic)로 유연하게 확장해 나가는 모습은 LMM이 범용 인공지능(AGI)으로 진화하는 과정의 단면을 보여준다.</p>
<p>셋째, <strong>개방형 생태계의 힘</strong>을 확인시켜 주었다. LLaVA-NeXT는 코드, 데이터, 모델, 평가 도구(LMMs-Eval)를 모두 공개함으로써 전 세계 연구자들의 집단지성을 활용하고 있다. 이는 폐쇄적인 상용 모델들이 따라올 수 없는 혁신의 속도를 만들어내는 원동력이 된다.</p>
<p>향후 LLaVA-NeXT는 더욱 강력한 LLM(Llama-4, Qwen-2 등)의 등장과 함께 지속적으로 진화할 것이며, 3D, 오디오 등 더 다양한 모달리티를 통합하는 방향으로 나아갈 것이다. 연구자와 개발자들에게 LLaVA-NeXT는 현재 가장 강력하고 유연한 멀티모달 연구 플랫폼이자, 미래의 AI 에이전트를 구축하기 위한 핵심 도구이다.</p>
<h2>12. 참고 자료</h2>
<ol>
<li>LLaVA-NeXT: Improved reasoning, OCR, and world knowledge …, https://llava-vl.github.io/blog/2024-01-30-llava-next/</li>
<li>haotian-liu/LLaVA: [NeurIPS’23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond. - GitHub, https://github.com/haotian-liu/LLaVA</li>
<li>LLaVA, https://llava-vl.github.io/</li>
<li>LLaVA-VL/LLaVA-NeXT - GitHub, https://github.com/LLaVA-VL/LLaVA-NeXT</li>
<li>LLaVa-NeXT-Video - Hugging Face, https://huggingface.co/docs/transformers/model_doc/llava_next_video</li>
<li>Papers Explained 107: LLaVA 1.6 - Ritvik Rastogi, https://ritvik19.medium.com/papers-explained-107-llava-1-6-a312efd496c5</li>
<li>LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models, https://arxiv.org/html/2407.07895v1</li>
<li>LLaVA-NeXT - Hugging Face, https://huggingface.co/docs/transformers/en/model_doc/llava_next</li>
<li>Supported Models - vLLM, https://docs.vllm.ai/en/v0.5.0/models/supported_models.html</li>
<li>lmms-lab/llava-next-110b - Hugging Face, https://huggingface.co/lmms-lab/llava-next-110b</li>
<li>Supported Models - vLLM, https://docs.vllm.ai/en/v0.6.4/models/supported_models.html</li>
<li>LLaVA-NeXT: A Strong Zero-shot Video Understanding Model, https://llava-vl.github.io/blog/2024-04-30-llava-next-video/</li>
<li>Long Context Transfer from Language to Vision - arXiv, https://arxiv.org/html/2406.16852v1</li>
<li>lmms-lab/llava-next-interleave-qwen-7b - Hugging Face, https://huggingface.co/lmms-lab/llava-next-interleave-qwen-7b</li>
<li>Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis - arXiv, https://arxiv.org/html/2405.21075v2</li>
<li>arXiv:2410.02712v2 [cs.CV] 4 Mar 2025, https://arxiv.org/pdf/2410.02712</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>