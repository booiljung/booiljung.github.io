<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Q-Former (Querying Transformer) 시각-언어 정렬을 위한 정보 병목 아키텍처 및 파생 모델</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Q-Former (Querying Transformer) 시각-언어 정렬을 위한 정보 병목 아키텍처 및 파생 모델</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">VLM (Vision Language Model, 시각 언어 모델)</a> / <span>Q-Former (Querying Transformer) 시각-언어 정렬을 위한 정보 병목 아키텍처 및 파생 모델</span></nav>
                </div>
            </header>
            <article>
                <h1>Q-Former (Querying Transformer) 시각-언어 정렬을 위한 정보 병목 아키텍처 및 파생 모델</h1>
<p>2025-12-15, G30DR</p>
<h2>1.  서론: 멀티모달 인공지능의 진화와 효율성 딜레마</h2>
<p>현대 인공지능 연구의 흐름은 단일 모달리티(Unimodal) 처리를 넘어 시각(Vision)과 언어(Language)를 통합적으로 이해하고 추론하는 멀티모달(Multimodal) 모델로 급격히 이동하고 있다. 특히 대규모 언어 모델(Large Language Model, LLM)과 비전 트랜스포머(Vision Transformer, ViT)가 각각 독립적인 도메인에서 인간 수준의 성능을 달성함에 따라, 이 두 거대 모델을 효과적으로 결합하려는 시도가 연구의 중심에 섰다. 초기 시각-언어 모델(Vision-Language Models, VLM) 연구는 주로 시각 인코더와 언어 모델을 처음부터 끝까지(end-to-end) 함께 학습시키는 방식을 채택했다. 그러나 모델의 규모가 기하급수적으로 커짐에 따라 이러한 접근 방식은 계산 비용의 비효율성을 초래했을 뿐만 아니라, 이미 학습된 강력한 단일 모달리티 모델의 지식을 훼손하는 ‘치명적 망각(catastrophic forgetting)’ 문제를 야기했다.1</p>
<p>이러한 딜레마를 해결하기 위해 등장한 패러다임이 바로 사전 학습된(pre-trained) 거대 모델들을 ‘동결(frozen)’ 상태로 유지하면서, 이들 사이를 가볍고 효율적인 모듈로 연결하는 방식이다. Salesforce Research가 제안한 BLIP-2(Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models)는 이러한 접근법의 정점을 보여주었으며, 그 핵심에는 **Q-Former (Querying Transformer)**라는 혁신적인 아키텍처가 존재한다.1 Q-Former는 단순한 연결 고리를 넘어, 시각 정보와 언어 정보 사이의 ‘정보 병목(Information Bottleneck)’ 역할을 수행하며 두 이질적인 모달리티 간의 간극을 좁히는 중추적인 기능을 담당한다.</p>
<p>본 보고서는 Q-Former의 아키텍처적 특성, 학습 메커니즘, 그리고 이를 기반으로 발전한 다양한 파생 모델(InstructBLIP, HierarQ, DisenQ 등)을 포괄적으로 분석한다. 특히 Q-Former가 어떻게 고정된 수의 학습 가능한 쿼리(Learnable Queries)를 통해 가변적인 시각 정보를 압축하고, 이를 LLM이 이해할 수 있는 언어적 공간으로 투영하는지 그 기술적 원리를 심층 규명한다. 또한, DeepMind의 Flamingo 모델 등 유사한 목적을 가진 다른 아키텍처와의 비교를 통해 Q-Former가 갖는 효율성과 성능의 우위를 논증하고, 최신 연구 동향을 바탕으로 향후 발전 가능성을 전망한다.</p>
<h2>2.  시각-언어 모델링의 난제와 Q-Former의 등장 배경</h2>
<h3>2.1  단일 모달리티 모델의 거대화와 연동의 필요성</h3>
<p>최근 몇 년간 자연어 처리(NLP) 분야에서는 BERT, GPT 시리즈, T5 등 트랜스포머(Transformer) 기반의 모델들이 비약적인 발전을 이루었다. 동시에 컴퓨터 비전 분야에서도 CNN(Convolutional Neural Network)에서 ViT(Vision Transformer)로의 패러다임 전환이 일어나며, 대규모 데이터셋으로 사전 학습된 강력한 시각 인코더들이 등장했다. 예를 들어, CLIP의 ViT-L/14나 EVA-CLIP의 ViT-g/14와 같은 모델들은 이미 이미지 내의 객체와 의미를 포착하는 데 탁월한 성능을 보여준다.4</p>
<p>문제는 이 두 거대 산맥을 어떻게 연결하느냐에 있다. 단순히 두 모델의 출력을 이어 붙여 재학습(Fine-tuning)하는 것은 막대한 컴퓨팅 자원을 요구한다. 더욱이, 수십억(Billions) 개 이상의 파라미터를 가진 LLM을 시각 데이터에 맞춰 미세 조정할 경우, 언어 모델이 본래 가지고 있던 일반적인 언어 생성 능력이나 지식 추론 능력이 저하되는 현상이 발생한다. 따라서 연구자들은 ’사전 학습된 모델을 고정(Freeze)’하고, 그 사이를 연결하는 ‘어댑터(Adapter)’ 혹은 ‘브릿지(Bridge)’ 모듈에 집중하기 시작했다.</p>
<h3>2.2  모달리티 간극(Modality Gap)과 정렬(Alignment)의 문제</h3>
<p>이미지 인코더가 출력하는 시각적 특징(Visual Features)과 언어 모델이 처리하는 텍스트 임베딩(Text Embeddings) 사이에는 본질적인 ’모달리티 간극’이 존재한다. 시각 정보는 연속적이고 고차원적인 픽셀 정보의 추상화인 반면, 언어 정보는 이산적(Discrete)인 토큰들의 시퀀스이다. 또한, LLM은 학습 과정에서 이미지를 본 적이 없기 때문에(Unimodal Pre-training), 시각 인코더의 출력을 그대로 입력받으면 이를 해석할 수 없다.2</p>
<p>따라서 시각 정보를 언어 모델이 이해할 수 있는 형태, 즉 ’언어적 특징 공간(Language Feature Space)’으로 변환하고 정렬하는 과정이 필수적이다. 기존의 Flamingo 모델은 Perceiver Resampler라는 모듈을 통해 이를 시도했으나, BLIP-2는 여기서 한 걸음 더 나아가 텍스트 정보와의 상호작용을 강화하고 경량화된 구조를 통해 효율성을 극대화한 Q-Former를 제안했다.7 Q-Former는 시각 정보를 단순히 압축하는 것을 넘어, 텍스트 쿼리와의 상호작용을 통해 ’언어적으로 유의미한 시각 정보’만을 추출하도록 설계되었다.</p>
<h2>3.  Q-Former 아키텍처의 구조적 분석</h2>
<p>Q-Former는 BLIP-2 프레임워크의 핵심 구성 요소로, 동결된 이미지 인코더와 동결된 LLM 사이에서 ‘가교’ 역할을 수행한다. 그 구조는 BERT-base 모델의 가중치로 초기화되지만, 시각-언어 정렬을 위한 독창적인 메커니즘들이 추가되어 있다.9</p>
<h3>3.1  학습 가능한 쿼리 (Learnable Queries)</h3>
<p>Q-Former의 가장 큰 특징은 입력 이미지의 해상도나 크기에 상관없이 항상 고정된 개수의 출력 벡터를 생성한다는 점이다. BLIP-2의 구현에서는 32개의 **학습 가능한 쿼리(Learnable Queries)**가 사용되며, 각 쿼리는 768차원의 벡터이다.7 이 쿼리들은 모델의 입력 데이터가 아니라, 학습 과정에서 업데이트되는 모델 파라미터(Parameter)의 일부로 취급된다.3</p>
<p>이 32개의 쿼리 토큰은 이미지 인코더에서 출력되는 수백 개(예: ViT의 경우 <span class="math math-inline">14 \times 14 = 196</span>개 이상의 패치 토큰)의 시각 특징 중 텍스트 생성에 가장 필요한 정보만을 선별적으로 추출하는 역할을 한다. 이를 통해 Q-Former는 방대한 시각 정보 중에서 언어 모델이 처리해야 할 핵심 정보만을 걸러내는 **정보 병목(Information Bottleneck)**으로 기능한다.2 이 병목 구조는 LLM이 불필요한 시각적 잡음에 과적합(Overfitting)되는 것을 방지하고, 시각-언어 일반화 성능을 높이는 데 기여한다.</p>
<h3>3.2  이중 트랜스포머 (Dual Transformer) 구조와 가중치 공유</h3>
<p>Q-Former는 내부적으로 두 개의 하위 트랜스포머 모듈로 구성되어 있다: (1) 시각 특징과 상호작용하는 **이미지 트랜스포머(Image Transformer)**와 (2) 텍스트를 처리하는 **텍스트 트랜스포머(Text Transformer)**이다. 흥미로운 점은 이 두 모듈이 **Self-Attention 레이어를 공유(Share)**한다는 것이다.3</p>
<ul>
<li><strong>이미지 트랜스포머:</strong> 학습 가능한 쿼리(<span class="math math-inline">Z</span>)를 입력으로 받아, 동결된 시각 인코더의 출력(<span class="math math-inline">F</span>)과 <strong>Cross-Attention</strong>을 수행한다. 이를 통해 쿼리는 시각 정보와 결합된다.</li>
<li><strong>텍스트 트랜스포머:</strong> 텍스트 입력을 받아 처리하며, Self-Attention을 통해 텍스트 내부의 문맥을 파악한다. 또한, 쿼리 토큰과 텍스트 토큰 간의 상호작용도 이 Self-Attention 층에서 일어난다.</li>
</ul>
<p>Self-Attention 층을 공유함으로써 모델의 파라미터 수를 줄이고, 쿼리와 텍스트가 동일한 임베딩 공간에서 상호작용할 수 있는 기반을 마련한다. 반면, Cross-Attention 층은 이미지 트랜스포머에만 존재하며, 이는 BERT의 가중치로 초기화되지 않고 랜덤하게 초기화되어 처음부터 학습된다.9</p>
<h3>3.3  어텐션 메커니즘과 수식적 이해</h3>
<p>Q-Former 내에서의 정보 흐름은 교차 어텐션(Cross-Attention)과 자기 어텐션(Self-Attention)의 조화로 이루어진다.</p>
<ul>
<li>Cross-Attention (<span class="math math-inline">Z&#39;&#39; = \text{CrossAttn}(Z&#39;, F)</span>):</li>
</ul>
<p>학습 가능한 쿼리 <span class="math math-inline">Z&#39;</span>가 Query(Q) 역할을 하고, 동결된 이미지 특징 <span class="math math-inline">F</span>가 Key(K)와 Value(V) 역할을 한다.12<br />
<span class="math math-display">
  Q_b = W_q \cdot Z&#39;, \quad K_b = W_k \cdot F, \quad V_b = W_v \cdot F
</span><br />
이 과정은 쿼리 벡터가 이미지의 어떤 패치(Patch)에 집중해야 할지를 결정하는 과정이다. 예를 들어, “남자가 차를 운전한다“라는 텍스트 생성 과제에서 특정 쿼리는 ‘남자’ 영역에, 다른 쿼리는 ‘차’ 영역에 어텐션을 집중하게 된다.13</p>
<ul>
<li>Self-Attention (<span class="math math-inline">Z&#39; = \text{SelfAttn}(Z)</span>):</li>
</ul>
<p>쿼리 벡터들끼리, 혹은 쿼리와 텍스트 토큰들끼리 정보를 교환한다. 이는 쿼리들이 서로 중복된 정보를 담지 않고 분산된(Distributed) 정보를 학습하도록 돕는다.</p>
<h3>3.4  다양한 어텐션 마스킹(Attention Masking) 전략</h3>
<p>Q-Former의 독창성은 단일 모델로 대조 학습(Contrastive Learning), 생성(Generation), 매칭(Matching)이라는 세 가지 다른 성격의 과제를 동시에 수행한다는 점에 있다. 이를 가능하게 하는 것이 바로 <strong>어텐션 마스킹(Attention Masking)</strong> 전략이다.2</p>
<table><thead><tr><th><strong>마스크 유형 (Mask Type)</strong></th><th><strong>설명</strong></th><th><strong>적용 목적</strong></th><th><strong>쿼리-텍스트 상호작용</strong></th></tr></thead><tbody>
<tr><td><strong>Bi-directional Mask</strong></td><td>모든 쿼리와 텍스트 토큰이 서로를 볼 수 있음</td><td>Image-Text Matching (ITM)</td><td>전면 허용 (Full Interaction)</td></tr>
<tr><td><strong>Multimodal Causal Mask</strong></td><td>쿼리는 서로를 볼 수 있음. 텍스트는 이전 토큰과 쿼리만 볼 수 있음</td><td>Image-grounded Text Generation (ITG)</td><td>인과적 허용 (Causal Interaction)</td></tr>
<tr><td><strong>Uni-modal Mask</strong></td><td>쿼리와 텍스트가 서로를 볼 수 없음 (차단)</td><td>Image-Text Contrastive Learning (ITC)</td><td>차단 (No Interaction)</td></tr>
</tbody></table>
<p>이러한 마스킹 전략을 통해 Q-Former는 학습 단계 1에서 시각-언어 표현을 효과적으로 학습할 수 있다.</p>
<h2>4.  BLIP-2의 1단계 사전 학습: 시각-언어 표현 학습 (Representation Learning)</h2>
<p>BLIP-2의 학습 과정은 두 단계로 나뉜다. 첫 번째 단계는 동결된 이미지 인코더와 Q-Former를 연결하여, Q-Former가 시각 정보를 언어 친화적인 형태로 추출하도록 학습시키는 단계이다. 이 단계에서는 LLM이 관여하지 않는다.6</p>
<h3>4.1  이미지-텍스트 대조 학습 (Image-Text Contrastive Learning, ITC)</h3>
<p>ITC 손실 함수는 시각적 표현과 텍스트 표현을 공통된 특징 공간에 정렬하여 상호정보량(Mutual Information)을 최대화하는 것을 목표로 한다.</p>
<ul>
<li><strong>동작 원리:</strong> Uni-modal Mask를 사용하여 쿼리가 텍스트 정보를 보지 못하게 한다. 쿼리는 오직 이미지만을 참조하여 시각적 특징 <span class="math math-inline">Z</span>를 생성하고, 텍스트 트랜스포머는 텍스트 특징 <span class="math math-inline">T</span>를 생성한다.</li>
<li><strong>정렬:</strong> 출력된 쿼리 임베딩 <span class="math math-inline">Z</span>와 텍스트 임베딩 <span class="math math-inline">T</span> 사이의 코사인 유사도(Cosine Similarity)를 계산한다. 같은 이미지-텍스트 쌍(Positive Pair)의 유사도는 높이고, 다른 쌍(Negative Pair)의 유사도는 낮추는 방식으로 학습된다.9</li>
<li><strong>특징:</strong> Q-Former의 출력이 여러 개의 쿼리 벡터이므로, 텍스트 임베딩과 가장 유사도가 높은 쿼리 벡터를 선택하여 대조 학습을 수행한다. 이는 정보 병목 현상을 유도하여 쿼리가 서로 다른 시각적 측면을 포착하도록 장려한다.</li>
</ul>
<h3>4.2  이미지-텍스트 매칭 (Image-Text Matching, ITM)</h3>
<p>ITM은 이미지와 텍스트 간의 더 세밀한(Fine-grained) 정렬을 학습한다. ITC가 거시적인 특징 공간의 정렬이라면, ITM은 주어진 이미지와 텍스트가 실제로 일치하는지 여부를 판단하는 이진 분류(Binary Classification) 문제이다.</p>
<ul>
<li><strong>동작 원리:</strong> Bi-directional Mask를 사용하여 쿼리가 텍스트의 모든 정보를 참조할 수 있게 한다. 즉, 쿼리 벡터 <span class="math math-inline">Z</span>는 이미지 정보뿐만 아니라 텍스트 정보까지 통합한 멀티모달 표현이 된다.</li>
<li><strong>분류:</strong> 이 멀티모달 임베딩을 별도의 분류 헤드(Classifier Head)에 통과시켜 ‘일치(Match)’ 또는 ’불일치(Non-match)’를 예측한다.</li>
<li><strong>Hard Negative Mining:</strong> 학습의 효율성을 높이기 위해, 대조 학습 단계에서 유사도가 높게 측정되었으나 실제로는 정답이 아닌 ’어려운 부정 샘플(Hard Negatives)’을 선별하여 학습에 활용한다.9 이는 모델이 미묘한 차이를 구별하는 능력을 키워준다.</li>
</ul>
<h3>4.3  이미지 기반 텍스트 생성 (Image-grounded Text Generation, ITG)</h3>
<p>ITG는 Q-Former가 시각적 정보를 바탕으로 텍스트를 생성하는 능력을 학습하는 과정이다. 이는 2단계에서 LLM과 연결될 때, 시각 정보를 언어적 맥락으로 전달하는 기초 훈련이 된다.</p>
<ul>
<li><strong>동작 원리:</strong> Multimodal Causal Mask를 사용한다. 쿼리는 모든 이미지 정보를 참조할 수 있지만, 텍스트 생성 시에는 자기회귀적(Auto-regressive) 속성을 유지해야 하므로 현재 시점 이후의 텍스트 토큰은 볼 수 없다.</li>
<li><strong>역할:</strong> 이 과정은 쿼리가 이미지의 정보를 텍스트 생성에 필요한 형태로 압축하고 구조화하도록 강제한다. 즉, 쿼리가 ’언어 생성의 씨앗’이 되도록 훈련시키는 것이다.9</li>
</ul>
<p>이 세 가지 목표(ITC, ITM, ITG)는 공동으로 최적화(Joint Optimization)되며, 이를 통해 Q-Former는 시각 정보를 효과적으로 추출하고, 텍스트와 정렬하며, 언어로 변환할 수 있는 다재다능한 모듈로 거듭난다.3</p>
<h2>5.  BLIP-2의 2단계 사전 학습: 시각-언어 생성 학습 (Generative Learning)</h2>
<p>1단계에서 Q-Former가 시각적 특징을 언어적으로 유의미한 형태로 추출하는 법을 배웠다면, 2단계에서는 이를 실제 거대 언어 모델(LLM)과 연결하여 생성 능력을 극대화하는 과정이다. 이 단계에서도 시각 인코더와 LLM은 모두 동결 상태를 유지한다.2</p>
<h3>5.1  LLM과의 연결: 완전 연결 계층 (Fully Connected Layer)</h3>
<p>Q-Former의 출력인 32개의 쿼리 벡터 <span class="math math-inline">Z</span> (크기: <span class="math math-inline">32 \times 768</span>)는 LLM이 바로 이해할 수 있는 차원이 아니다. LLM마다 고유한 임베딩 차원(예: OPT-2.7B는 2560, FlanT5-XXL은 4096 등)을 가지고 있기 때문이다. 따라서 Q-Former의 출력을 LLM의 입력 차원에 맞게 변환해주는 선형 투영(Linear Projection) 레이어가 도입된다.2</p>
<p>이 투영된 쿼리 임베딩들은 LLM의 텍스트 토큰 임베딩과 동일한 차원을 갖게 되며, 텍스트 입력의 앞부분에 위치하는 <strong>‘소프트 비주얼 프롬프트(Soft Visual Prompts)’</strong> 역할을 수행한다.</p>
<h3>5.2  소프트 프롬프트로서의 쿼리</h3>
<p>“소프트 프롬프트“라는 개념은 쿼리 벡터들이 사람이 읽을 수 있는 이산적인 텍스트 토큰(Hard Prompt)은 아니지만, LLM 내부에서 마치 텍스트 토큰인 것처럼 처리되어 문맥을 형성한다는 의미이다.</p>
<ul>
<li><strong>입력 구조:</strong> <code>[Projected Queries (32)] +</code></li>
<li><strong>작동 방식:</strong> LLM은 앞선 32개의 쿼리 토큰을 통해 이미지의 내용을 ’이해’하고, 이를 바탕으로 뒤따르는 텍스트 생성이나 질의응답을 수행한다. 쿼리 벡터들은 LLM의 활성화(Activation) 패턴을 조절하여 시각적 맥락에 맞는 텍스트가 생성되도록 유도한다.13</li>
</ul>
<h3>5.3  LLM 유형에 따른 학습 전략</h3>
<p>BLIP-2는 디코더 기반(Decoder-based) LLM과 인코더-디코더 기반(Encoder-Decoder) LLM 모두에 적용 가능하다.</p>
<ol>
<li><strong>Decoder-based LLM (예: OPT):</strong></li>
</ol>
<ul>
<li>언어 모델링 손실(Language Modeling Loss)을 사용한다.</li>
<li>입력: <code>[Queries] + [Question]</code> -&gt; 출력: <code>[Answer]</code></li>
<li>모델은 쿼리와 질문을 보고 답변을 자기회귀적으로 생성하도록 학습된다.</li>
</ul>
<ol start="2">
<li><strong>Encoder-Decoder LLM (예: FlanT5):</strong></li>
</ol>
<ul>
<li>접두사 언어 모델링 손실(Prefix Language Modeling Loss)을 사용한다.</li>
<li>입력(인코더): <code>[Queries] + [Question]</code></li>
<li>출력(디코더): <code>[Answer]</code></li>
<li>인코더가 시각 정보와 질문을 함께 처리하고, 디코더가 답변을 생성하는 구조이다.4</li>
</ul>
<p>이 단계에서 Q-Former는 LLM이 가진 방대한 지식과 언어 능력을 시각적 작업에 활용할 수 있도록 ‘통역사’ 역할을 수행한다. 중요한 점은 LLM의 파라미터는 업데이트되지 않으므로, LLM의 일반적인 언어 능력은 그대로 보존된다는 것이다.</p>
<h2>6.  Q-Former 파생 모델과 진화: 한계를 넘어서</h2>
<p>BLIP-2의 성공 이후, Q-Former 아키텍처는 다양한 형태로 변형 및 발전되었다. 특히 초기 모델의 한계를 극복하고 특정 도메인(지시어 수행, 비디오, 생체 인식)에 특화된 모델들이 제안되었다.</p>
<h3>6.1  InstructBLIP: 지시어 인식(Instruction-Aware) Q-Former</h3>
<p>BLIP-2의 1단계 학습에서 Q-Former는 텍스트 정보를 볼 수 없거나 제한적으로만 활용했다(ITC, ITG). 이는 2단계에서 LLM에 쿼리를 전달할 때, 쿼리가 사용자의 구체적인 지시(Instruction)를 충분히 반영하지 못하고 일반적인 시각 특징만을 추출하는 한계로 이어졌다. 이를 해결하기 위해 등장한 것이 <strong>InstructBLIP</strong>이다.16</p>
<ul>
<li><strong>핵심 개선:</strong> InstructBLIP은 텍스트 지시어(Instruction)를 Q-Former의 입력으로 직접 제공한다.</li>
<li><strong>메커니즘:</strong> 지시어 토큰들은 Q-Former의 Self-Attention 층에서 쿼리 토큰과 상호작용한다. 이를 통해 쿼리들은 “이미지에서 빨간색 옷을 입은 사람을 찾아라“와 같은 구체적인 지시에 맞춰, 이미지의 해당 영역에 집중적으로 어텐션을 수행한다.18</li>
<li><strong>효과:</strong> 이러한 ‘지시어 인식’ 기능은 단순히 이미지를 묘사하는 것을 넘어, 시각적 추론(Visual Reasoning)이나 특정 객체 식별과 같은 복잡한 과제에서 성능을 대폭 향상시켰다. 실험 결과에 따르면 지시어 인식이 없는 경우에 비해 공간적 추론과 같은 과제에서 월등한 성능을 보였다.19</li>
</ul>
<h3>6.2  HierarQ: 비디오 이해를 위한 계층적 접근</h3>
<p>비디오 데이터는 시간(Temporal) 축이 추가되어 정보량이 폭발적으로 증가한다. 단일 Q-Former로 긴 비디오 프레임들을 모두 처리하는 것은 계산적으로 비효율적이며, 문맥 정보를 잃을 위험이 있다. **HierarQ (Hierarchical Q-Former)**는 이러한 문제를 해결하기 위해 계층적 구조를 도입했다.20</p>
<ul>
<li><strong>구조:</strong> 두 개의 스트림으로 구성된 계층적 구조를 갖는다.</li>
</ul>
<ol>
<li><strong>엔티티 레벨(Entity-level) Q-Former:</strong> 개별 프레임이나 짧은 구간에서 객체(Entity)의 세부적인 특징을 추출한다.</li>
<li><strong>씬 레벨(Scene-level) Q-Former:</strong> 긴 시간적 문맥을 아우르며 전체적인 장면(Scene)의 흐름과 사건의 전개를 파악한다.</li>
</ol>
<ul>
<li><strong>메모리 뱅크(Memory Bank):</strong> 과거의 시각적 정보를 저장하는 전용 메모리 뱅크를 활용하여, 긴 비디오 시퀀스에서도 정보가 소실되지 않고 유지되도록 한다.22 이는 Q-Former가 정지 이미지(Static Image)를 넘어 동적 비디오(Dynamic Video) 이해로 확장될 수 있음을 보여준 사례이다.</li>
</ul>
<h3>6.3  DisenQ: 특징 분리(Disentanglement)를 통한 행동 기반 생체 인식</h3>
<p>**DisenQ (Disentangling Q-Former)**는 Q-Former의 쿼리 메커니즘을 응용하여, 비디오 내 사람의 행동 기반 생체 인식(Activity-Biometrics) 성능을 높이는 데 초점을 맞췄다.12</p>
<ul>
<li><strong>문제 의식:</strong> 기존의 생체 인식은 사람의 신원(Identity) 정보가 입고 있는 옷(Appearance)이나 현재 하고 있는 행동(Motion) 정보와 뒤섞여(Entangled) 정확한 인식을 방해받는 문제가 있었다.</li>
<li><strong>해결책:</strong> DisenQ는 단일 쿼리 세트 대신 세 가지 독립적인 쿼리 세트를 도입하여 특징을 분리한다.</li>
</ul>
<ol>
<li><strong><span class="math math-inline">z_b</span> (Biometrics Query):</strong> 신원 고유의 특징(체형, 걷는 자세 등)을 학습.</li>
<li><strong><span class="math math-inline">z_m</span> (Motion Query):</strong> 행동의 동적인 패턴을 학습.</li>
<li><strong><span class="math math-inline">z_{\hat{b}}</span> (Non-biometrics Query):</strong> 옷차림 등 신원과 무관한 정보를 학습.</li>
</ol>
<ul>
<li><strong>결과:</strong> 텍스트 가이드(Language Guidance)를 통해 각 쿼리가 자신의 역할에 맞는 정보만을 추출하도록 강제함으로써, 옷을 갈아입거나 다른 행동을 하더라도 사람을 정확히 식별할 수 있는 강건한 모델을 구축했다.23</li>
</ul>
<h2>7.  타 아키텍처와의 비교 및 성능 분석</h2>
<h3>7.1  DeepMind Flamingo와의 비교</h3>
<p>Q-Former와 가장 자주 비교되는 모델은 DeepMind의 Flamingo이다. Flamingo는 <strong>Perceiver Resampler</strong>라는 모듈을 사용하여 가변적인 시각 입력을 고정된 수의 토큰으로 변환한다.7</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>BLIP-2 (Q-Former)</strong></th><th><strong>Flamingo (Perceiver Resampler)</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 모듈</strong></td><td>Q-Former (Querying Transformer)</td><td>Perceiver Resampler</td></tr>
<tr><td><strong>특징 추출 방식</strong></td><td>텍스트 조건부 쿼리 추출 (Text-conditioned)</td><td>시각 정보 압축 중심 (Visual Compression)</td></tr>
<tr><td><strong>학습 파라미터</strong></td><td><strong>매우 적음 (약 188M)</strong></td><td>많음 (수십억 개 단위의 튜닝 필요)</td></tr>
<tr><td><strong>효율성</strong></td><td><strong>54배 더 적은 파라미터로 고성능 달성</strong></td><td>대규모 모델 의존적</td></tr>
<tr><td><strong>상호작용</strong></td><td>쿼리가 텍스트와 깊게 상호작용</td><td>초기 모델은 텍스트 상호작용 제한적</td></tr>
</tbody></table>
<p>BLIP-2 논문에 따르면, BLIP-2는 Flamingo-80B 모델보다 <strong>54배 적은 학습 파라미터</strong>를 사용하면서도 VQAv2(Visual Question Answering) 제로샷 성능에서 8.7% 더 높은 점수(65.0 vs 56.3)를 기록했다.3 이는 Q-Former가 수행하는 ’정보 병목’과 ’사전 정렬(Pre-alignment)’이 단순히 모델의 크기를 키우는 것보다 훨씬 효율적인 전략임을 시사한다.</p>
<h3>7.2  어텐션 맵 시각화를 통한 정성적 분석</h3>
<p>연구자들은 Q-Former의 쿼리들이 실제로 이미지의 어떤 영역을 보는지 확인하기 위해 어텐션 맵(Attention Map)을 시각화했다.</p>
<ul>
<li><strong>객체 중심의 집중:</strong> 32개의 쿼리는 이미지 전체에 흩뿌려지는 것이 아니라, 사람의 얼굴, 주요 사물, 행동이 일어나는 부위 등 의미론적으로 중요한 영역에 집중적으로 어텐션을 할당하는 경향이 있다.25</li>
<li><strong>역할 분담:</strong> 각 쿼리 벡터들은 서로 다른 역할을 분담하는 것으로 보인다. 어떤 쿼리는 배경 정보를, 다른 쿼리는 전경의 객체를 담당하며, 이들이 합쳐져 전체적인 시각적 문맥을 형성한다.13</li>
<li><strong>할루시네이션(Hallucination)과의 연관성:</strong> 모델이 잘못된 답변을 생성할 때, 어텐션 맵이 엉뚱한 배경이나 관련 없는 객체에 집중되어 있는 현상이 관찰되었다.26 이는 Q-Former의 시각적 추출 실패가 LLM의 환각 현상으로 직결됨을 보여주며, 정교한 정렬의 중요성을 역설한다.</li>
</ul>
<h2>8.  기술적 한계점과 미래 연구 방향</h2>
<p>Q-Former는 혁신적인 성과를 이뤘지만, 여전히 해결해야 할 과제들이 존재한다.</p>
<ol>
<li><strong>정보 손실(Information Loss)의 딜레마:</strong> 수백, 수천 개의 이미지 패치를 단 32개의 쿼리로 압축하는 과정에서 필연적으로 정보의 손실이 발생한다. 특히 이미지 내의 아주 작은 객체나 미세한 텍스트(OCR) 정보는 이 압축 과정에서 사라질 위험이 크다.27 쿼리의 수를 늘리면 계산 비용이 증가하고, 줄리면 정보 손실이 커지는 트레이드오프(Trade-off) 관계에 있다.</li>
<li><strong>공간 정보(Spatial Information)의 희석:</strong> Q-Former의 출력 쿼리들은 순서가 없는 집합(Set)으로 취급되는 경향이 있어, 이미지의 2차원적인 공간 구조 정보가 약화될 수 있다. 이는 “왼쪽에 있는 컵“과 같이 위치 정보가 중요한 과제에서 약점으로 작용할 수 있다.</li>
<li><strong>환각(Hallucination) 제어:</strong> Q-Former가 잘못된 시각 특징을 추출하여 LLM에 전달하면, LLM은 이를 사실로 받아들이고 그럴듯한 거짓말을 생성하게 된다. 시각적 근거(Visual Grounding)를 더욱 강화하여 이러한 환각을 억제하는 연구가 필요하다.</li>
</ol>
<p>향후 연구는 이러한 한계를 극복하기 위해 동적 쿼리 할당(Dynamic Query Allocation), 공간 정보를 보존하는 새로운 어텐션 메커니즘, 그리고 비디오/3D/오디오 등 더 다양한 모달리티로의 확장(예: AudioFlamingo, Point-Bind 등)으로 나아갈 것으로 전망된다. 또한, Q-Former와 같은 모듈형 아키텍처는 모델의 유지 보수와 업그레이드를 용이하게 하므로, 온디바이스 AI나 엣지 컴퓨팅 환경에서도 중요한 역할을 할 것으로 기대된다.</p>
<h2>9.  결론</h2>
<p>Q-Former는 거대 시각 모델과 거대 언어 모델 사이의 간극을 메우는 가장 효율적이고 강력한 솔루션 중 하나로 자리 잡았다. BLIP-2를 통해 증명된 Q-Former의 우수성은 ’동결된 모델의 활용’과 ’경량화된 정렬 모듈’이라는 새로운 설계 원칙을 확립했다. 학습 가능한 쿼리를 이용한 정보 병목 메커니즘은 시각 데이터에서 언어적으로 유의미한 정보만을 정제해내는 탁월한 능력을 보여주었으며, 이는 InstructBLIP, HierarQ, DisenQ 등 다양한 후속 연구로 이어지며 그 확장성을 입증하고 있다.</p>
<p>멀티모달 AI의 미래는 단순히 모델의 크기를 키우는 경쟁이 아니라, 이질적인 정보들을 얼마나 효율적으로 연결하고 통합하느냐에 달려 있다. Q-Former는 이러한 연결성(Connectivity)의 핵심 기술로서, 인간처럼 보고, 읽고, 이해하는 진정한 의미의 인공지능을 구현하는 데 중요한 기술적 토대를 제공하고 있다.</p>
<h2>10. 참고 자료</h2>
<ol>
<li>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models - arXiv, https://arxiv.org/abs/2301.12597</li>
<li>BLIP-2: A new Visual Language Model by Salesforce - Wandb, https://wandb.ai/gladiator/BLIP-2/reports/BLIP-2-A-new-Visual-Language-Model-by-Salesforce–VmlldzozNjM0NjYz</li>
<li>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v202/li23q/li23q.pdf</li>
<li>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models - arXiv, https://arxiv.org/html/2301.12597</li>
<li>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models - The Nemati Lab, https://www.nematilab.info/bmijc/assets/081823_paper.pdf</li>
<li>BLIP-2: A Detailed Look at the Architecture, Training, and Inference | by shashank Jain, https://medium.com/@jain.sm/blip-2-a-detailed-look-at-the-architecture-training-and-inference-4a2a991b7391</li>
<li>Towards Efficient Visual-Language Alignment of the Q-Former for Visual Reasoning Tasks - arXiv, https://arxiv.org/html/2410.09489v1</li>
<li>Design choices for Vision Language Models in 2024 - Hugging Face, https://huggingface.co/blog/gigant/vlm-design</li>
<li>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models | by heping_LU | Medium, https://medium.com/@jiangmen28/blip-2-bootstrapping-language-image-pre-training-with-frozen-image-encoders-and-large-language-fb8e87ddc853</li>
<li>Querying Transformer (Q-Former) in BLIP-2 improves Image-Text Generation in E-Commerce Applications - Deeraj Manjaray, https://deerajmanjaray.medium.com/querying-transformer-q-former-in-blip-2-improves-image-text-generation-in-e-commerce-applications-2d2402cb93e3</li>
<li>Autonomous Aspect-Image Instruction a2II: Q-Former Guided Multimodal Sentiment Classification - ACL Anthology, https://aclanthology.org/2024.lrec-main.180.pdf</li>
<li>Q-Former Architecture - Emergent Mind, https://www.emergentmind.com/topics/q-former-architecture</li>
<li>BLIP-2 : How Transformers Learn to ‘See’ and Understand Images | Towards AI, https://towardsai.net/p/computer-vision/blip-2-how-transformers-learn-to-see-and-understand-images</li>
<li>BLIP: Bootstrapping Language-Image Pre- training for Unified Vision, https://faculty.cc.gatech.edu/~zk15/teaching/AY2024_cs8803vlm_fall/slides/L11_BLIP.pdf</li>
<li>BLIP-2 : How Transformers Learn to ‘See’ and Understand Images | by Arnavbhatt, https://pub.towardsai.net/inside-blip-2-how-queries-extract-meaning-from-images-9a26cf4765f4</li>
<li>InstructBLIP - Hugging Face, https://huggingface.co/docs/transformers/en/model_doc/instructblip</li>
<li>[Quick Review] InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning - Liner, https://liner.com/review/instructblip-towards-generalpurpose-visionlanguage-models-with-instruction-tuning</li>
<li>Papers Explained 156: InstructBLIP | by Ritvik Rastogi - Medium, https://ritvik19.medium.com/papers-explained-156-instructblip-c3cf3291a823</li>
<li>InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning - Boyang “Albert” Li, http://www.boyangli.org/paper/InstructBLIP-2023.pdf</li>
<li>HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video Understanding - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2025/papers/Azad_HierarQ_Task-Aware_Hierarchical_Q-Former_for_Enhanced_Video_Understanding_CVPR_2025_paper.pdf</li>
<li>HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video Understanding - arXiv, https://arxiv.org/html/2503.08585v1</li>
<li>HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video Understanding, https://sacrcv.github.io/HierarQ-website/</li>
<li>DisenQ: Disentangling Q-Former for Activity-Biometrics - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2025/papers/Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics_ICCV_2025_paper.pdf</li>
<li>[Quick Review] BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models - Liner, https://liner.com/review/blip2-bootstrapping-languageimage-pretraining-with-frozen-image-encoders-and-large</li>
<li>BLIP-2: Multi-Modal Vision-Language Fusion - Emergent Mind, https://www.emergentmind.com/topics/blip-2-model</li>
<li>Visualizing Vision-Language Models - DigitalOcean, https://www.digitalocean.com/community/tutorials/visualizing-vision-language-models-multimodal-reasoning</li>
<li>BLIP-2 Finetuning: End-to-End Fine-tuning of BLIP-2 using transformers, datasets, PEFT, bitsandbytes &amp; Hugging Face | by Kunal Bhashkar | Oct, 2025, https://bhashkarkunal.medium.com/blip-2-finetuning-end-to-end-fine-tuning-of-blip-2-using-transformers-datasets-peft-6bdda6e5a1d4</li>
<li>HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video Understanding - Liner, https://liner.com/review/hierarq-taskaware-hierarchical-qformer-for-enhanced-video-understanding</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>