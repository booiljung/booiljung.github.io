<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:LLaVA-OneVision 및 LLaVA-OneVision-1.5: 개방형 멀티모달 모델의 통합 아키텍처와 성능 확장</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>LLaVA-OneVision 및 LLaVA-OneVision-1.5: 개방형 멀티모달 모델의 통합 아키텍처와 성능 확장</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">VLM (Vision Language Model, 시각 언어 모델)</a> / <span>LLaVA-OneVision 및 LLaVA-OneVision-1.5: 개방형 멀티모달 모델의 통합 아키텍처와 성능 확장</span></nav>
                </div>
            </header>
            <article>
                <h1>LLaVA-OneVision 및 LLaVA-OneVision-1.5: 개방형 멀티모달 모델의 통합 아키텍처와 성능 확장</h1>
<p>2026-01-20, G30DR</p>
<p>인공지능(AI) 연구의 최전선은 텍스트, 이미지, 비디오 등 서로 다른 모달리티(Modality)를 단일 모델 내에서 유기적으로 통합하여 처리하는 대규모 멀티모달 모델(LMM, Large Multimodal Models)의 개발로 수렴하고 있다. 과거의 컴퓨터 비전 및 자연어 처리 연구가 각기 다른 도메인에서 독립적으로 발전해 온 것과 달리, 인간 수준의 범용 인공지능(AGI)을 달성하기 위해서는 시각적 정보를 언어적 맥락 안에서 해석하고 추론하는 능력이 필수적이다. 그러나 기존의 오픈 소스 LMM 생태계는 단일 이미지(Single-Image), 다중 이미지(Multi-Image), 비디오(Video)라는 세 가지 핵심 시나리오를 각각 별개의 모델이나 파이프라인으로 처리하는 파편화된 접근 방식을 취해왔다. 이는 모델의 범용성을 저해할 뿐만 아니라, 모달리티 간의 지식 전이(Knowledge Transfer)를 어렵게 만드는 주요 원인이 되었다.1</p>
<p>이러한 기술적 난제를 해결하기 위해 등장한 LLaVA-OneVision 프로젝트는 “OneVision“이라는 명칭이 시사하듯, 단일 모델로 모든 시각적 시나리오를 통합 처리하는 것을 목표로 한다. 이 모델은 시각적 입력을 처리하는 아키텍처를 통일함으로써 이미지에서 학습한 세밀한 인식 능력이 비디오의 시간적 이해 능력으로 전이되도록 설계되었다.1 특히, 최근 공개된 LLaVA-OneVision-1.5 버전은 기존 모델의 성능을 비약적으로 향상시켰을 뿐만 아니라, 훈련 과정의 비효율성을 개선하여 “멀티모달 훈련의 민주화(Democratized Multimodal Training)“를 실현했다는 점에서 학계와 산업계의 주목을 받고 있다. 약 16,000달러(USD)라는, LMM 훈련으로서는 비교적 적은 예산으로 GPT-4V나 Gemini 1.5 Pro와 같은 상용 모델(Proprietary Models)에 필적하는 성능을 달성한 것은 데이터 엔지니어링과 훈련 효율화의 승리라 평가할 수 있다.4</p>
<p>본 보고서는 LLaVA-OneVision 및 1.5 버전의 기술적 아키텍처, 훈련 파이프라인, 데이터 전략, 그리고 벤치마크 성능을 포괄적으로 분석한다. 특히 AnyRes 전략을 통한 해상도 적응성, RICE-ViT 인코더 도입을 통한 시각적 표현의 고도화, 그리고 강화학습(RL) 기반의 사후 훈련 전략 등 핵심 기술 요소를 심층적으로 해부하여, 이 모델이 제시하는 멀티모달 AI의 미래 방향성을 고찰한다.</p>
<h2>1.  LLaVA-OneVision 아키텍처의 핵심 설계 원칙 및 구성</h2>
<p>LLaVA-OneVision의 아키텍처는 LLaVA 시리즈 특유의 미니멀리즘(Minimalism) 철학을 계승하면서도, 다양한 시각적 입력을 유연하게 수용할 수 있는 확장성을 갖추고 있다. 이 아키텍처의 핵심은 강력한 언어 모델과 최첨단 비전 인코더, 그리고 이 둘을 연결하는 효율적인 프로젝터의 결합에 있으며, 무엇보다 입력 이미지의 해상도와 형태에 따라 동적으로 토큰을 할당하는 전략적 유연성에 있다.2</p>
<h3>1.1  언어 모델 백본 (Language Model Backbone): Qwen-2 및 Qwen-3</h3>
<p>LLaVA-OneVision 시리즈는 언어 처리의 중추로서 Qwen 모델 패밀리를 채택하였다. 이는 다국어 처리 능력과 추론 능력이 검증된 오픈 소스 LLM으로, LLaVA-OneVision의 텍스트 생성 및 지시 이행 능력의 기반이 된다.</p>
<ul>
<li><strong>Qwen-2의 채택 (초기 버전):</strong> 초기 LLaVA-OneVision 모델은 Qwen-2를 기반으로 구축되었다. 0.5B, 7B, 72B의 세 가지 파라미터 크기로 제공되어, 모바일 엣지 디바이스부터 대규모 서버급 추론까지 다양한 배포 환경을 지원한다.6 Qwen-2는 32K 토큰의 긴 컨텍스트 윈도우(Context Window)를 지원하여, 긴 비디오 시퀀스나 다중 이미지 입력에서 발생하는 방대한 시각 토큰을 효과적으로 처리할 수 있는 능력을 제공한다.8</li>
<li><strong>Qwen-3로의 진화 (1.5 버전):</strong> LLaVA-OneVision-1.5 기술 보고서에 따르면, 최신 버전은 Qwen-3를 언어 백본으로 도입하여 추론 능력과 명령어 수행 능력을 더욱 강화하였다.9 (일부 비교 실험에서는 Qwen2.5 기반 모델들과의 비교가 주를 이루나, 1.5 버전의 아키텍처 명세에서는 Qwen3의 도입이 명시되어 있다). 이는 단순한 이미지 캡셔닝을 넘어 복잡한 논리적 추론이 필요한 멀티모달 작업에서의 성능 향상을 목표로 한다.10</li>
</ul>
<h3>1.2  비전 인코더의 진화: SigLIP에서 RICE-ViT로</h3>
<p>시각적 정보를 텍스트와 정렬 가능한 형태로 변환하는 비전 인코더(Vision Encoder)는 LLaVA-OneVision의 성능을 결정짓는 가장 중요한 요소 중 하나이다.</p>
<ul>
<li><strong>SigLIP (LLaVA-OneVision):</strong> 초기 버전은 SigLIP(Sigmoid Loss for Language-Image Pre-training) 모델, 구체적으로 <code>SigLIP-SO400M</code>을 사용하였다.11 SigLIP은 기존 CLIP 모델이 사용하는 Softmax 손실 함수 대신 Sigmoid 손실 함수를 사용하여 배치 크기(Batch Size)에 대한 의존성을 줄이고 훈련 효율성을 높인 모델이다. 이는 언어와 정렬된 시각적 특징(Language-aligned visual features)을 추출하는 데 최적화되어 있다.12</li>
<li><strong>RICE-ViT (LLaVA-OneVision-1.5):</strong> 1.5 버전에서는 비전 인코더가 <code>RICE-ViT</code> (Region-aware Cluster Discrimination Vision Transformer)로 교체되었다.9 이는 기존 SigLIP이나 DFN과 같은 인코더들이 전역적인 이미지 이해에는 강하지만 세밀한 영역(Region) 정보나 텍스트(OCR) 인식에는 한계가 있다는 점을 보완하기 위함이다.</li>
<li><strong>영역 인식 능력 강화:</strong> RICE-ViT는 영역 수준의 의미론적 표현을 학습하도록 설계되어, 이미지 내의 작은 객체나 텍스트를 더 정확하게 포착한다. 이는 ChartQA나 DocVQA와 같은 문서 이해 벤치마크에서의 성능 향상으로 직결된다.13</li>
<li><strong>2D RoPE (Rotary Positional Encoding):</strong> 1.5 버전의 가장 큰 기술적 혁신 중 하나는 2D RoPE의 도입이다. 기존의 절대 위치 임베딩(Absolute Positional Embedding)과 달리, 2D RoPE는 상대적인 위치 정보를 인코딩하여 모델이 학습되지 않은 해상도의 이미지도 유연하게 처리할 수 있게 한다.9 이를 통해 해상도별로 별도의 미세 조정(Fine-tuning)을 거치지 않고도 원본 해상도(Native Resolution) 처리가 가능해졌다.14</li>
</ul>
<h3>1.3  프로젝터 (Projector) 및 연결 구조</h3>
<p>비전 인코더에서 추출된 시각적 특징은 프로젝터를 통해 언어 모델의 임베딩 공간으로 투영된다. LLaVA-OneVision은 복잡한 Q-Former나 Resampler 대신 단순하면서도 효율적인 2층 MLP(Multi-Layer Perceptron) 구조를 사용한다.6</p>
<ul>
<li><strong>구조적 특징:</strong> GELU 활성화 함수를 포함한 2-Layer MLP는 시각적 토큰과 텍스트 토큰 간의 차원을 일치시키고 의미론적 정렬을 수행한다.11</li>
<li><strong>토큰 집계 (Pooling/Concatenation):</strong> 1.5 버전에서는 인접한 2x2 패치 블록을 그룹화하여 연결(Concatenate)한 후 프로젝터로 전달하는 방식을 통해 시각적 정보의 밀도를 높이고 LLM이 처리해야 할 토큰 수를 효율적으로 관리한다.9 또한, 토큰을 유지하여 이미지 전체의 전역적인 맥락 정보를 보존하는 전략을 취한다.9</li>
</ul>
<h2>2.  AnyRes 전략과 시각적 표현의 통합</h2>
<p>LLaVA-OneVision이 “OneVision“이라는 목표를 달성할 수 있었던 핵심 기술은 바로 <strong>AnyRes (Any Resolution)</strong> 전략이다. 이 전략은 단일 이미지, 다중 이미지, 비디오라는 서로 다른 형태의 입력을 일관된 방식으로 처리하면서도 각 시나리오에 최적화된 해상도와 토큰 예산을 할당한다.16</p>
<h3>2.1  AnyRes-9 및 그리드 구성 (Grid Configuration)</h3>
<p>고해상도 이미지를 처리하기 위해 모델은 이미지를 여러 개의 패치(Patch)로 분할하는 전략을 사용한다. 이를 통해 기존 모델들이 고정된 해상도(예: 336x336)로 이미지를 강제 리사이징(Resizing)하면서 발생했던 정보 손실과 왜곡(Aspect Ratio Distortion) 문제를 해결한다.</p>
<ul>
<li><strong>동적 그리드 할당:</strong> 이미지는 그 비율에 따라 <code>{2×2, 1×{2,3,4}, {2,3,4}×1}</code>과 같은 다양한 그리드 구성으로 분할된다.16 예를 들어, 가로로 긴 이미지는 2x1 또는 3x1 그리드로, 정사각형에 가까운 이미지는 2x2 그리드로 분할되어 원본의 비율을 최대한 유지한다.</li>
<li><strong>AnyRes-9:</strong> 특히 고해상도 처리를 위해 이미지를 최대 9개의 패치로 분할하여 처리하는 ‘AnyRes-9’ 기법이 적용된다.18 이는 각 패치가 독립적인 이미지처럼 인코더를 통과한 후 다시 재조립되는 방식으로, 이미지의 세밀한 디테일을 놓치지 않게 한다.</li>
</ul>
<h3>2.2  시나리오별 토큰 관리 전략</h3>
<p>LLaVA-OneVision은 시나리오 간의 전이 학습 효과를 극대화하기 위해 각 시나리오에서 LLM에 입력되는 시각적 토큰의 총량을 유사한 수준으로 맞추는 전략을 사용한다.19</p>
<table><thead><tr><th><strong>시나리오</strong></th><th><strong>처리 방식 (Processing Strategy)</strong></th><th><strong>토큰 할당 및 특징</strong></th></tr></thead><tbody>
<tr><td><strong>단일 이미지</strong></td><td><strong>High-Resolution Crop:</strong> 원본 해상도를 유지하기 위해 최대 공간 구성(Spatial Configuration)을 사용.</td><td><strong>최대 토큰:</strong> 패치 분할을 통해 많은 수의 토큰을 할당하여 세밀한 인식(Fine-grained recognition)을 극대화함. 긴 시퀀스는 비디오 처리 능력으로의 전이를 유도함.2</td></tr>
<tr><td><strong>다중 이미지</strong></td><td><strong>Base Resolution Only:</strong> 고해상도 크롭 과정을 생략하고 기본 해상도로만 처리.</td><td><strong>효율성:</strong> 다수의 이미지를 처리해야 하므로, 각 이미지당 토큰 수를 줄여(Crop 생략) 전체적인 계산 비용을 제어함. 이미지 간의 관계 파악에 집중.2</td></tr>
<tr><td><strong>비디오</strong></td><td><strong>Bilinear Interpolation &amp; Pooling:</strong> 각 프레임을 기본 해상도로 리사이징 후, 이중 선형 보간법으로 풀링.</td><td><strong>프레임당 196 토큰:</strong> 시간적 차원의 확장에 대응하기 위해 공간적 해상도를 압축. 프레임당 토큰 수를 196개로 제한하여 긴 비디오 시퀀스를 메모리 효율적으로 처리.18</td></tr>
</tbody></table>
<p>이러한 전략은 단일 이미지에서 학습된 고해상도 처리 능력이 다중 이미지의 관계 파악 능력으로, 그리고 비디오의 시공간적 이해 능력으로 자연스럽게 전이되도록 유도한다. 특히 단일 이미지를 비디오처럼 긴 토큰 시퀀스로 표현하여 학습함으로써, 모델이 시각적 데이터의 ’길이’에 구애받지 않고 일관된 추론을 수행할 수 있게 한다.2</p>
<h2>3.  훈련 파이프라인 및 데이터 전략</h2>
<p>LLaVA-OneVision-1.5의 성공은 단순히 모델 아키텍처의 개선뿐만 아니라, 정교하게 설계된 3단계 훈련 파이프라인과 데이터 엔지니어링에 기인한다.</p>
<h3>3.1  3단계 훈련 파이프라인 (Three-Stage Training Pipeline)</h3>
<ol>
<li>1단계: 언어-이미지 정렬 (Language-Image Alignment):</li>
</ol>
<p>이 단계에서는 비전 인코더와 LLM을 동결(Freeze)하고 프로젝터 레이어만을 훈련시킨다. LLaVA-1.5의 558K 데이터셋을 활용하여 시각적 특징을 언어 모델의 임베딩 공간에 초기 정렬시키는 과정이다.9 이는 모델이 시각적 입력을 텍스트 토큰처럼 인식할 수 있는 기초를 다지는 단계이다.</p>
<ol start="2">
<li>1.5단계: 고품질 지식 학습 (High-Quality Knowledge Learning / Mid-Training):</li>
</ol>
<p>1.5 버전에서 새롭게 강조된 핵심 단계이다. 비전 인코더와 LLM을 포함한 전체 모델의 파라미터를 훈련(Full-parameter training)한다. 8,500만 개(85M)의 대규모 데이터셋을 사용하여 모델에 광범위한 시각적 개념과 지식을 주입한다.9 연구진은 이 단계에서의 데이터 규모 확장이 복잡한 훈련 기법보다 성능 향상에 더 결정적인 역할을 한다고 분석한다.</p>
<ol start="3">
<li>2단계: 시각적 지시 튜닝 (Visual Instruction Tuning):</li>
</ol>
<p>2,200만 개(22M)의 정제된 지시 데이터셋을 사용하여 모델이 사용자의 복잡한 요구사항을 이해하고 수행하도록 미세 조정한다. 이 단계에서는 일반적인 VQA뿐만 아니라, 수학적 추론, OCR, 문서 이해 등 특화된 능력을 집중적으로 배양한다.9</p>
<h3>3.2  강화학습(RL) 기반 사후 훈련 (Post-training)</h3>
<p>LLaVA-OneVision-1.5는 지도 학습(SFT) 이후에 경량화된 강화학습 단계를 추가하여 모델의 추론 능력을 극대화한다.9</p>
<ul>
<li><strong>AReaL 시스템:</strong> 비동기식(Asynchronous) 강화학습 시스템을 통해 모델의 잠재된 ‘연쇄적 사고(Chain-of-Thought, CoT)’ 능력을 이끌어낸다.</li>
<li><strong>불일치 기반 데이터 선택 (Discrepancy-driven Data Selection):</strong> 모델이 정답을 맞혔더라도 추론 과정이 빈약하거나, 오답을 냈지만 추론 과정이 논리적인 경우 등을 구별하여 학습 데이터로 활용함으로써, 단순 암기가 아닌 논리적 추론 과정을 강화하는 데 초점을 맞춘다. 이는 복잡한 멀티모달 추론 벤치마크에서의 성능 향상에 크게 기여한다.9</li>
</ul>
<h3>3.3  데이터셋 구축 및 효율화 전략</h3>
<p>LLaVA-OneVision-1.5의 데이터셋은 양과 질 모든 면에서 기존 모델을 압도한다.</p>
<ul>
<li><strong>LLaVA-OneVision-1.5-Mid-Training (85M):</strong> 2,000만 개의 중국어 데이터와 6,500만 개의 영어 데이터로 구성된다.9 핵심은 **개념 균형(Concept-balanced)**이다. MetaCLIP 인코더를 활용한 특징 기반 매칭을 통해 데이터의 시각적 개념 분포를 균일하게 맞추어, 모델이 특정 데이터에 편향되지 않도록 했다.13 또한 노이즈가 많은 웹 수집 캡션 대신 정제된 캡션을 사용하여 학습 효율을 높였다.</li>
<li><strong>오프라인 병렬 데이터 패킹 (Offline Parallel Data Packing):</strong> 훈련 효율성을 극대화하기 위해 개발된 기술이다. 길이가 서로 다른 시퀀스들을 빈 공간 없이 패킹하여 GPU 연산 효율을 높이고, 훈련 시간을 단축시킨다. 이 기술 덕분에 전체 훈련 비용을 약 16,000달러(A100 GPU 기준)로 억제할 수 있었다.5</li>
</ul>
<h2>4.  벤치마크 성능 및 비교 분석</h2>
<p>LLaVA-OneVision-1.5는 다양한 벤치마크에서 상용 모델 및 최신 오픈 소스 모델들을 상회하거나 대등한 성능을 입증하였다. 특히 Qwen2.5-VL과의 비교에서 탁월한 효율성을 보여준다.</p>
<h3>4.1  비디오 이해 (Video Understanding): Video-MME</h3>
<p>Video-MME는 단기, 중기, 장기 비디오에 대한 모델의 이해력을 평가하는 척도이다. LLaVA-OneVision은 별도의 비디오 특화 훈련 없이 전이 학습만으로 이 분야에서 괄목할 성과를 거두었다.</p>
<table><thead><tr><th><strong>모델</strong></th><th><strong>파라미터</strong></th><th><strong>전체 점수 (Overall %)</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>Gemini 1.5 Pro</strong></td><td>-</td><td>75.0 / 78.6</td><td>상용 모델 최상위권 (Google) 22</td></tr>
<tr><td><strong>GPT-4o</strong></td><td>-</td><td>71.9</td><td>상용 모델 (OpenAI) 22</td></tr>
<tr><td><strong>LLaVA-OneVision-72B</strong></td><td>72B</td><td><strong>66.2 ~ 66.3</strong></td><td>오픈 소스 모델 중 최상위권 성능 22</td></tr>
<tr><td><strong>Qwen2.5-VL-7B</strong></td><td>7B</td><td>-</td><td>LLaVA-OV-1.5-8B가 비교 우위 점함 9</td></tr>
</tbody></table>
<p>LLaVA-OneVision-72B는 Video-MME에서 66.3%를 기록하며, GPT-4o mini(64.8%)와 같은 경량 상용 모델을 앞서는 성능을 보였다.24 특히 장기 비디오 처리에서 AnyRes 전략과 효율적인 토큰 풀링이 문맥 유지에 효과적임을 시사한다.</p>
<h3>4.2  멀티모달 추론 및 종합 성능 (MMMU, MathVista)</h3>
<p>대학 수준의 다학제적 지식을 요구하는 MMMU와 수학적 시각 추론을 평가하는 MathVista에서도 강력한 성능을 보여준다.</p>
<ul>
<li><strong>MathVista:</strong> LLaVA-OneVision은 67.5점을 기록하여 InternVL2-Pro(66.8)와 TextGrad(GPT-4o 기반, 66.1)를 상회하였다.25 이는 도표, 차트, 기하학적 도형을 해석하고 수리적으로 추론하는 능력이 뛰어남을 증명한다.</li>
<li><strong>MMMU:</strong> 72B 모델은 56.8~57.4점대를 기록하며 오픈 소스 생태계 내에서 경쟁력 있는 위치를 점하고 있다.8 최근 공개된 더 어려운 버전인 MMMU-Pro에서는 전체적인 성능 하락이 있었으나, 여전히 오픈 소스 모델 그룹 상위권을 유지한다.27</li>
</ul>
<h3>4.3  LLaVA-OneVision-1.5 vs Qwen2.5-VL</h3>
<p>기술 보고서에 따르면, LLaVA-OneVision-1.5는 경쟁 모델인 Qwen2.5-VL 대비 높은 효율성과 성능을 자랑한다.</p>
<ul>
<li><strong>8B 모델:</strong> Qwen2.5-VL-7B와 비교하여 27개 벤치마크 중 18개에서 우위를 점했다.9</li>
<li><strong>4B 모델:</strong> 1.5-4B 모델은 Qwen2.5-VL-3B를 27개 모든 벤치마크에서 능가하였다.9 이는 RICE-ViT의 도입과 데이터 큐레이션의 고도화가 파라미터 수의 열세를 극복하고 더 높은 ’성능 밀도(Performance Density)’를 달성했음을 의미한다.</li>
<li><strong>OCR 성능:</strong> RICE-ViT의 영역 인식 능력 덕분에 ChartQA, DocVQA 등의 문서 이해 벤치마크에서 특히 두각을 나타낸다.13</li>
</ul>
<h2>5.  결론: 민주화된 멀티모달 AI의 미래</h2>
<p>LLaVA-OneVision 및 1.5 버전은 단순히 성능 좋은 또 하나의 오픈 소스 모델이 아니다. 이 프로젝트는 “어떻게 하면 제한된 자원으로 최상의 멀티모달 모델을 만들 수 있는가?“라는 질문에 대한 구체적인 해답을 제시한다.</p>
<p>첫째, **아키텍처의 통합(Unification)**이다. AnyRes 전략과 2D RoPE를 통해 이미지와 비디오의 경계를 기술적으로 허물었으며, 이는 향후 등장할 모델들이 모달리티별로 별도의 아키텍처를 설계할 필요가 없음을 시사한다.</p>
<p>둘째, **데이터의 승리(Triumph of Data)**이다. 85M 규모의 개념 균형 데이터셋과 합성 데이터의 활용은 모델 파라미터를 키우는 것보다 데이터의 질을 높이는 것이 훨씬 효율적임을 증명했다.</p>
<p>셋째, **훈련의 민주화(Democratization)**이다. 16,000달러라는 훈련 비용은 대학 연구실이나 스타트업도 자체적인 SOTA(State-of-the-Art) 모델을 구축할 수 있는 가능성을 열어주었다.</p>
<p>결론적으로 LLaVA-OneVision은 기술적 진보와 접근성 확대를 동시에 달성한 이정표적인 연구이다. Hugging Face를 통해 공개된 가중치와 코드는 전 세계 연구자들에게 강력한 기반 기술을 제공하며, 이는 향후 멀티모달 에이전트, 로보틱스, 실시간 비디오 분석 등 다양한 응용 분야에서의 혁신을 가속화할 것이다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>LLaVA-OneVision: Easy Visual Task Transfer, https://researchportal.hkust.edu.hk/en/publications/llava-onevision-easy-visual-task-transfer</li>
<li>LLaVA-OneVision: Easy Visual Task Transfer - arXiv, https://arxiv.org/html/2408.03326v1</li>
<li>#211 LLaVA-OneVision: Easy Visual Task Transfer - YouTube, https://www.youtube.com/watch?v=VVcVpZxpgdU</li>
<li>LLaVA-OneVision-1.5: Fully Open Framework for Democratized …, https://arxiv.org/html/2509.23661v1</li>
<li>EvolvingLMMs-Lab/LLaVA-OneVision-1.5: Fully Open … - GitHub, https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5</li>
<li>LLaVA-OneVision — Pushing boundaries in multimodal AI with …, https://unfoldai.com/llava-onevision/</li>
<li>LLaVA-OneVision - Hugging Face, https://huggingface.co/docs/transformers/en/model_doc/llava_onevision</li>
<li>lmms-lab/llava-onevision-qwen2-72b-si - Hugging Face, https://huggingface.co/lmms-lab/llava-onevision-qwen2-72b-si</li>
<li>LLaVA-OneVision-1.5: Fully Open Framework for … - arXiv, https://arxiv.org/abs/2509.23661</li>
<li>LLaVA-OneVision: Advanced Multimodal Framework - Emergent Mind, https://www.emergentmind.com/topics/llava-onevision</li>
<li>Dataseeds/LLaVA-OneVision-Qwen2-0.5b-ov-DSD-FineTune, https://huggingface.co/Dataseeds/LLaVA-OneVision-Qwen2-0.5b-ov-DSD-FineTune</li>
<li>Llava-Onevision-Qwen2-0.5b-Si Free Chat Online – skywork.ai, https://skywork.ai/blog/models/llava-onevision-qwen2-0-5b-si-free-chat-online-skywork-ai/</li>
<li>LLaVA-OneVision-1.5: Fully Open Framework for Democratized …, https://liner.com/review/llavaonevision15-fully-open-framework-for-democratized-multimodal-training</li>
<li>Fully Open Framework for Democratized Multimodal Training, https://www.alphaxiv.org/overview/2509.23661v1</li>
<li>LLaVA-OneVision: The Powerful Multimodal Model for Next …, https://medium.com/data-science-in-your-pocket/llava-onevision-the-powerful-multimodal-model-for-next-generation-image-and-video-analysis-8b7e2dd7ae3e</li>
<li>LLaVA Roadmap, <a href="https://datascience.hku.hk/wp-content/uploads/Journals%20Club/20241121_Chenming%20Zhu.pdf">https://datascience.hku.hk/wp-content/uploads/Journals%20Club/20241121_Chenming%20Zhu.pdf</a></li>
<li>[Quick Review] LLaVA-OneVision: Easy Visual Task Transfer - Liner, https://liner.com/review/llavaonevision-easy-visual-task-transfer</li>
<li>LLaVA-OneVision - Hugging Face, https://huggingface.co/docs/transformers/v4.52.3/model_doc/llava_onevision</li>
<li>LLaVA-OneVision: Easy Visual Task Transfer, https://llava-vl.github.io/blog/2024-08-05-llava-onevision/</li>
<li>LLaVA-NeXT/docs/LLaVA_OneVision.md at main - GitHub, https://github.com/LLaVA-VL/LLaVA-NeXT/blob/main/docs/LLaVA_OneVision.md</li>
<li>EvolvingLMMs-Lab/LLaVA-OneVision-1.5-RL: Fully Open … - GitHub, https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5-RL</li>
<li>Chain-of-Frames: Advancing Video Understanding in Multimodal …, https://arxiv.org/html/2506.00318v1</li>
<li>Video-MME Leaderboard - LLM Stats, https://llm-stats.com/benchmarks/video-mme</li>
<li>The First-Ever Comprehensive Evaluation Benchmark … - Video-MME, https://video-mme.github.io/home_page.html</li>
<li>MathVista: data, code, and evaluation for Mathematical Reasoning …, https://github.com/lupantech/MathVista</li>
<li>MMMU/MMMU_Pro · Datasets at Hugging Face, https://huggingface.co/datasets/MMMU/MMMU_Pro</li>
<li>A More Robust Multi-discipline Multimodal Understanding Benchmark, https://aclanthology.org/2025.acl-long.736.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>