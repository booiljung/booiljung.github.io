<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:VLM (Vision-Language Models, 시각-언어 모델)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>VLM (Vision-Language Models, 시각-언어 모델)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">VLM (Vision Language Model, 시각 언어 모델)</a> / <span>VLM (Vision-Language Models, 시각-언어 모델)</span></nav>
                </div>
            </header>
            <article>
                <h1>VLM (Vision-Language Models, 시각-언어 모델)</h1>
<p>2025-12-09, G30DR</p>
<h2>1.  서론: 인공지능 패러다임의 통합과 다중모달의 부상</h2>
<p>인공지능(Artificial Intelligence, AI) 기술의 발전사는 오랫동안 인간의 인지 기능을 모방하기 위해 시각과 언어라는 두 가지 거대한 축을 중심으로 독립적으로 진화해 왔다. 컴퓨터 비전(Computer Vision, CV)은 픽셀 데이터의 통계적 패턴을 분석하여 이미지를 분류하거나 객체를 탐지하는 시각적 인지 능력의 구현에 주력해 왔으며, 자연어 처리(Natural Language Processing, NLP)는 텍스트 데이터의 시퀀스(Sequence)를 학습하여 언어적 의미를 이해하고 생성하는 데 집중해 왔다.1 그러나 인간의 지능은 단일 감각 정보에 의존하지 않는다. 인간은 눈으로 본 것을 언어로 설명하고, 언어로 된 지시를 듣고 시각적 대상을 탐색하며, 시각과 언어 정보를 실시간으로 통합하여 복잡한 상황을 추론한다. 이러한 인간 인지 과정의 통합성을 기계 학습 모델에 구현하고자 하는 시도가 바로 ’다중모달(Multimodal) AI’이며, 그 중심에 시각-언어 모델(Vision-Language Models, VLM)이 있다.</p>
<p>VLM은 이미지와 텍스트라는 이질적인 데이터 형식을 단일한 수학적 공간(Embedding Space) 내에서 결합하고, 상호 연관성을 학습함으로써 두 모달리티 간의 자유로운 변환과 추론을 가능하게 하는 AI 시스템이다.3 초기 VLM 연구가 단순히 이미지에 대한 캡션을 생성하거나(Image Captioning), 이미지 내 특정 객체의 위치를 찾는 시각적 접지(Visual Grounding)와 같은 제한적 작업에 머물렀다면, 2024년과 2025년을 기점으로 등장한 최신 모델들은 텍스트, 이미지, 비디오, 오디오를 아우르는 포괄적인 이해 능력을 바탕으로 고차원적인 시각적 추론(Visual Reasoning), 복잡한 문서 분석, 그리고 물리적 세계에서의 행동 제어(Robotics)로까지 그 응용 범위를 급격히 확장하고 있다.5</p>
<p>본 보고서는 VLM의 기술적 근간이 되는 아키텍처의 진화 과정과 핵심 구성 요소, 대규모 데이터 처리 및 훈련 방법론, 그리고 현재 글로벌 AI 시장을 주도하고 있는 최신 모델들의 기술적 특성을 상세히 분석한다. 아울러 VLM의 신뢰성을 저해하는 주요 요인인 환각(Hallucination) 현상의 원인과 이를 해결하기 위한 최신 연구 동향을 고찰하고, 자율주행 및 로보틱스와 같은 실제 산업 현장에서 VLM이 어떻게 ’월드 모델(World Model)’로서 기능하며 물리적 세계를 혁신하고 있는지 심층적으로 논의한다. 특히, 한국어 데이터와 문화적 맥락에 특화된 국내 주요 VLM(HyperCLOVA X, Exaone 3.0, Gauss2)의 기술적 성취와 경쟁력을 글로벌 모델들과 비교 분석함으로써, 다가오는 AGI(Artificial General Intelligence) 시대에 VLM이 갖는 전략적 중요성을 제고하고자 한다.</p>
<h2>2.  VLM 아키텍처의 기술적 해부와 진화</h2>
<p>VLM의 핵심 도전 과제는 본질적으로 다른 구조를 가진 시각 데이터(고차원, 연속적, 공간적 특징)와 언어 데이터(저차원, 이산적, 순차적 특징)를 어떻게 효과적으로 결합하여 상호 이해 가능한 형태로 변환할 것인가에 있다. 이를 해결하기 위해 연구자들은 시각적 특징 추출, 모달리티 간 정렬(Alignment), 그리고 데이터 융합(Fusion)이라는 세 가지 핵심 단계를 거치는 다양한 아키텍처를 고안해 냈다.</p>
<h3>2.1  시각적 토큰화(Visual Tokenization)와 인코더의 역할</h3>
<p>거대 언어 모델(Large Language Models, LLM)이 텍스트를 처리 가능한 최소 단위인 토큰(Token)으로 분절하여 이해하듯, VLM 역시 이미지를 모델이 해석할 수 있는 토큰 형태로 변환하는 과정이 선행되어야 한다. 이 과정에서 가장 지배적으로 사용되는 아키텍처는 비전 트랜스포머(Vision Transformer, ViT)이다.7</p>
<p>이미지 패치 분할과 선형 투영:</p>
<p>전통적인 합성곱 신경망(CNN)이 픽셀 간의 국소적 특징(Local Features)을 추출하는 데 강점을 보였다면, ViT는 이미지를 전체적으로 조망하는 전역적 특징(Global Features)을 포착하는 데 유리하다. ViT는 입력 이미지를 고정된 크기(예: <span class="math math-inline">14 \times 14</span> 또는 <span class="math math-inline">16 \times 16</span> 픽셀)의 패치(Patch)로 분할한다.8 예를 들어, <span class="math math-inline">224 \times 224</span> 해상도의 이미지를 <span class="math math-inline">16 \times 16</span> 크기의 패치로 나누면 총 196개의 패치가 생성된다. 각 패치는 1차원 벡터로 평탄화(Flatten)된 후, 학습 가능한 선형 투영(Linear Projection) 층을 통과하여 고차원 임베딩 벡터로 변환된다. 이 임베딩 벡터들은 텍스트 처리에서의 ’단어 임베딩’과 유사한 역할을 수행하게 되며, 이를 ’시각적 토큰(Visual Token)’이라 칭한다.10</p>
<p>위치 인코딩(Positional Encoding)의 중요성:</p>
<p>트랜스포머(Transformer) 아키텍처는 데이터의 입력 순서에 영향을 받지 않는(Permutation Invariant) 특성을 가진다. 따라서 이미지 패치들의 공간적 배치 정보(왼쪽 상단, 중앙, 오른쪽 하단 등)를 모델에 주입하기 위해 각 패치 임베딩에 위치 정보를 담은 벡터를 더해주는 위치 인코딩 과정이 필수적이다.8 이를 통해 모델은 이미지 내 객체들 간의 상대적 위치 관계와 공간적 맥락을 학습할 수 있게 된다.</p>
<p>동적 해상도(Dynamic Resolution)와 AnyRes 기술:</p>
<p>초기 VLM은 모든 입력 이미지를 고정된 해상도(예: <span class="math math-inline">224 \times 224</span> 또는 <span class="math math-inline">336 \times 336</span>)로 리사이징(Resizing)하여 처리했다. 그러나 이러한 방식은 가로로 긴 파노라마 이미지나 세로로 긴 문서 이미지의 비율을 왜곡시켜 정보 손실을 초래하고, 특히 작은 텍스트나 세밀한 객체 인식을 방해하는 요인이 되었다. 이를 극복하기 위해 LLaVA-NeXT나 Qwen2-VL과 같은 최신 모델들은 ‘AnyRes’ 또는 ‘Naive Dynamic Resolution’ 기술을 도입했다.11 이 기술은 이미지를 여러 개의 그리드(Grid)로 분할하여 각각을 독립적인 패치 그룹으로 처리한 뒤, 전체적인 구성을 파악하는 글로벌 토큰을 추가하는 방식이다. 이를 통해 모델은 원본 이미지의 해상도와 비율을 유지하면서도 세밀한 국소 정보와 전체적인 맥락을 동시에 파악할 수 있게 되었다.</p>
<h3>2.2  모달리티 간 정렬(Modality Alignment)과 투영 전략</h3>
<p>시각 인코더가 추출한 시각적 특징은 언어 모델이 이해하는 언어 임베딩 공간과는 전혀 다른 분포를 가진다. 따라서 이 두 공간을 일치시키는 ‘정렬(Alignment)’ 과정이 VLM의 성능을 좌우하는 결정적인 요소가 된다. 이 역할을 수행하는 모듈을 ‘프로젝터(Projector)’ 또는 ’어댑터(Adapter)’라 부르며, 크게 MLP 방식과 Q-Former/Perceiver 방식으로 나뉜다.13</p>
<h4>2.2.1  MLP(Multi-Layer Perceptron) 투영: 단순함의 미학</h4>
<p>LLaVA(Large Language and Vision Assistant) 시리즈가 채택하여 그 효율성을 입증한 방식이다. 시각 인코더에서 출력된 시각 토큰들을 단순한 선형 레이어(Linear Layer) 또는 2층의 MLP(Multi-Layer Perceptron)를 통과시켜 언어 모델의 입력 차원과 일치시킨다.15</p>
<ul>
<li><strong>작동 원리:</strong> 시각 토큰 <span class="math math-inline">V</span>에 대해 가중치 행렬 <span class="math math-inline">W</span>와 편향 <span class="math math-inline">b</span>를 적용하여 <span class="math math-inline">V&#39; = \sigma(W_2 \cdot \sigma(W_1 \cdot V + b_1) + b_2)</span>와 같은 연산을 수행한다. 이렇게 변환된 <span class="math math-inline">V&#39;</span>는 텍스트 토큰과 결합(Concatenation)되어 LLM에 입력된다.</li>
<li><strong>장점 및 한계:</strong> 구조가 매우 단순하여 학습 및 추론 속도가 빠르고, 시각적 정보의 손실을 최소화하여 LLM에 풍부한 시각 정보를 전달할 수 있다는 장점이 있다. 그러나 이미지 해상도가 높아질수록 시각 토큰의 수가 급격히 증가하여(예: 4K 이미지의 경우 수천 개의 토큰 발생) LLM의 연산 부하를 가중시키는 단점이 있다. 이를 완화하기 위해 인접한 토큰들을 하나로 합치는 풀링(Pooling) 기법이나 C-Abstractor와 같은 압축 기술이 연구되고 있다.15</li>
</ul>
<h4>2.2.2  크로스 어텐션(Cross-Attention)과 Q-Former: 선택적 정보 추출</h4>
<p>BLIP-2, Flamingo, 그리고 구글의 일부 모델에서 채택한 방식이다. 시각 토큰 전체를 LLM에 입력하는 대신, 고정된 개수(예: 32개, 64개)의 학습 가능한 쿼리(Learnable Queries)를 사용하여 시각 정보 중 필요한 부분만을 선택적으로 추출한다.13</p>
<ul>
<li><strong>작동 원리:</strong> Q-Former 또는 Perceiver Resampler라 불리는 모듈은 텍스트 쿼리와 시각 특징 사이의 교차 주의(Cross-Attention) 연산을 수행한다. 수식적으로는 <span class="math math-inline">Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</span>에서 Query(<span class="math math-inline">Q</span>)는 학습 가능한 쿼리 벡터, Key(<span class="math math-inline">K</span>)와 Value(<span class="math math-inline">V</span>)는 시각 인코더의 출력이 된다. 이를 통해 방대한 시각 정보 중에서 텍스트 질문과 관련성이 높은 정보만을 ’압축’하여 고정된 길이의 토큰으로 변환한다.</li>
<li><strong>장점 및 한계:</strong> 이미지의 크기나 해상도에 관계없이 항상 일정한 수의 토큰만을 LLM에 전달하므로 연산 효율성이 매우 높다. 또한, 텍스트 질문에 따라 이미지의 다른 영역에 집중하는 가변적 정보 추출이 가능하다. 반면, MLP 방식에 비해 구현이 복잡하고, 정보 압축 과정에서 미세한 시각적 디테일(예: 배경의 작은 글씨나 흐릿한 물체)이 소실될 위험이 있어 OCR이나 세밀한 객체 탐지 성능이 다소 떨어질 수 있다.13</li>
</ul>
<h3>2.3  융합 전략(Fusion Strategies): 정보의 결합 시점</h3>
<p>시각 정보와 언어 정보가 모델 내부에서 언제, 어떻게 만나는가에 따라 VLM의 추론 능력과 유연성이 결정된다.7</p>
<ol>
<li><strong>초기 융합(Early Fusion):</strong> 이미지와 텍스트를 처음부터 하나의 시퀀스로 결합하여 모델의 입력단에 넣는 방식이다. 최근의 GPT-4V나 Gemini와 같은 대형 모델들이 이 방식을 선호하는데, 이는 트랜스포머의 모든 레이어(Self-Attention Layers)에서 텍스트와 이미지가 깊이 있게 상호작용할 수 있게 하여 복잡한 추론 능력을 극대화하기 위함이다.18 이미지를 마치 ’외국어’처럼 취급하여 텍스트와 동등한 위상에서 처리하는 것이 특징이다.</li>
<li><strong>중간 융합(Middle Fusion):</strong> 시각 인코더와 언어 모델이 별도로 존재하며, 중간 단계에서 크로스 어텐션 레이어나 게이트(Gated) 메커니즘을 통해 정보를 주입하는 방식이다. Flamingo 모델이 대표적으로, 사전 학습된 LLM의 가중치를 고정(Freeze)한 상태에서 시각 정보를 처리하는 어댑터 레이어만을 추가로 학습시키는 효율적인 전략을 취한다.13 이는 기존 LLM의 언어 능력을 보존하면서 시각 능력을 추가하는 데 유리하다.</li>
<li><strong>후기 융합(Late Fusion):</strong> 시각 모델과 언어 모델이 독립적으로 결과를 도출한 후, 마지막 단계에서 로짓(Logit) 점수를 합치거나 결정 논리를 통해 결과를 병합하는 방식이다. 현대적인 엔드투엔드(End-to-End) 학습 VLM에서는 거의 사용되지 않으나, 앙상블 모델 등에서 제한적으로 활용된다.</li>
</ol>
<h2>3.  데이터 파이프라인과 훈련 방법론의 혁신</h2>
<p>VLM을 ‘똑똑하게’ 만드는 것은 결국 데이터와 훈련 방법론이다. 단순한 이미지-텍스트 매칭을 넘어, 복잡한 지시를 이해하고 논리적으로 추론하는 능력을 배양하기 위해 훈련 과정은 다단계로 정교화되고 있다.19</p>
<h3>3.1  사전 학습(Pre-training): 대조 학습과 생성적 학습의 조화</h3>
<p>사전 학습 단계의 목표는 모델에게 시각 정보와 언어 정보 간의 기본적인 연관성을 학습시키는 것이다.</p>
<p>대조 학습(Contrastive Learning):</p>
<p>CLIP(Contrastive Language-Image Pretraining) 모델이 정립한 방식으로, 수억 개에서 수십억 개의 이미지-텍스트 쌍(Image-Text Pairs)을 사용한다.20 배치(Batch) 내에서 올바른 쌍(Positive Pair)의 임베딩 거리는 좁히고, 잘못된 쌍(Negative Pair)의 거리는 멀어지게 하는 손실 함수(Loss Function)를 사용한다. 이는 모델이 이미지와 텍스트의 의미적 유사성을 학습하게 하여, 별도의 미세 조정 없이도 이미지를 분류할 수 있는 강력한 제로샷(Zero-shot) 성능을 제공한다.18</p>
<p>생성적 학습(Generative Modeling):</p>
<p>최근 VLM 훈련의 주류는 대조 학습을 넘어 ‘생성적’ 방식으로 진화했다. 이는 LLM의 기본 학습 원리인 “다음 토큰 예측(Next Token Prediction)“을 시각 데이터까지 확장한 것이다. 모델은 이미지를 입력받은 상태에서 텍스트 캡션을 생성하도록 학습된다. 이 과정에서 모델은 이미지의 내용을 상세하게 묘사하고 설명하는 능력을 획득하게 되며, 이는 후술할 지시 튜닝 단계의 기초 체력이 된다.7</p>
<h3>3.2  시각적 지시 튜닝(Visual Instruction Tuning): 추론 능력의 배양</h3>
<p>사전 학습된 모델은 이미지를 보고 단순히 “고양이“라고 말할 수는 있지만, “이 고양이가 왜 화가 난 것 같니?“라는 복잡한 질문에는 대답하지 못한다. 이를 해결하기 위해 ‘지시 튜닝(Instruction Tuning)’ 단계가 도입되었다.21</p>
<p>데이터셋의 진화:</p>
<p>초기에는 COCO Caption이나 Visual Genome과 같이 단순한 객체 설명 위주의 데이터가 사용되었으나, 이는 모델의 논리적 추론 능력을 키우기에 부족했다. 최근에는 GPT-4V나 Gemini Pro와 같은 고성능 모델을 활용하여 생성한 합성 데이터(Synthetic Data)가 핵심 자원이 되고 있다.23</p>
<ul>
<li><strong>복합적 추론 데이터:</strong> 단순한 묘사를 넘어, “이미지 속 상황을 바탕으로 다음에 일어날 일을 예측하라“거나 “이 도표를 분석하여 2024년의 매출 추이를 요약하라“는 식의 고차원적 질문과 답변 쌍을 대량으로 생성하여 학습시킨다. LLaVA-NeXT 프로젝트나 ShareGPT4V 데이터셋은 이러한 고품질의 상세 캡션과 논리적 문답 데이터를 통해 모델 성능을 비약적으로 향상시켰다.</li>
<li><strong>다양성 확보:</strong> 일반적인 대화뿐만 아니라 OCR(광학 문자 인식), 코딩, 수학 문제 풀이, 과학 도표 해석 등 다양한 도메인의 데이터를 혼합(Mixture)하여 모델이 특정 영역에 치우치지 않고 범용적인 문제 해결 능력을 갖추도록 한다.24</li>
</ul>
<h3>3.3  RLHF와 V-DPO: 인간 선호도 반영</h3>
<p>텍스트 모델에서 성공을 거둔 ’인간 피드백 기반 강화학습(RLHF)’이 VLM에도 적용되고 있다. 특히 V-DPO(Vision-guided Direct Preference Optimization)와 같은 기법은 보상 모델(Reward Model)을 별도로 학습시키는 복잡한 과정 없이, 선호하는 답변(Positive)과 비선호 답변(Negative) 데이터 쌍만을 이용하여 모델을 직접 최적화한다.25</p>
<ul>
<li><strong>환각 억제:</strong> 이미지에 없는 물체를 있다고 대답하는 ’환각’이 포함된 답변을 Negative 샘플로, 정확하고 사실적인 답변을 Positive 샘플로 구성하여 학습시킴으로써, 모델이 시각 정보에 더 충실하게 반응하도록 유도한다. 이는 모델의 신뢰성을 높이는 데 결정적인 역할을 한다.</li>
</ul>
<h2>4.  2024-2025 최신 VLM 모델 지형도 및 비교 분석</h2>
<p>현재 VLM 생태계는 구글, 오픈AI 등 빅테크 기업의 독점적 모델(Proprietary Models)과 연구 커뮤니티 중심의 오픈 소스 모델, 그리고 각국의 언어와 문화에 특화된 소버린(Sovereign) AI 모델들이 치열하게 경쟁하고 있다.</p>
<h3>4.1  글로벌 프론티어 모델: Gemini와 GPT-4o</h3>
<p>Google Gemini 2.5/3 Pro:</p>
<p>구글의 Gemini 시리즈는 설계 단계부터 멀티모달을 염두에 두고 개발된(Native Multimodal) 모델이다.5</p>
<ul>
<li><strong>압도적인 컨텍스트:</strong> 100만 토큰에서 최대 200만 토큰에 이르는 방대한 컨텍스트 윈도우(Context Window)를 지원한다.28 이는 수천 페이지의 문서나 수 시간 분량의 비디오를 한 번에 입력받아 분석할 수 있음을 의미한다.</li>
<li><strong>Thinking Model:</strong> ‘사고(Thinking)’ 프로세스를 도입하여, 사용자의 질문에 즉답하기 전에 내부적으로 문제를 분해하고 단계적으로 추론하는 과정을 거친다. 이는 복잡한 수학 문제나 코딩, 논리 퀴즈에서 GPT-4o를 상회하는 성능을 보여주는 원동력이 된다.28</li>
<li><strong>비디오 이해:</strong> 비디오를 단순히 이미지의 나열이 아닌, 시간의 흐름과 인과 관계가 있는 시퀀스로 이해한다. Gemini 1.5 Pro와 그 후속작들은 비디오 내 특정 사건의 발생 시점을 정확히 찾아내는 타임스탬핑 능력에서 타의 추종을 불허한다.</li>
</ul>
<p>OpenAI GPT-4o:</p>
<p>GPT-4o(’o’는 omni를 의미)는 텍스트, 오디오, 이미지를 실시간으로 통합 처리하는 데 최적화된 모델이다.29</p>
<ul>
<li><strong>실시간 상호작용:</strong> 기존 모델들이 음성을 텍스트로 변환(STT)한 뒤 처리하고 다시 음성으로 변환(TTS)하는 과정을 거쳤다면, GPT-4o는 오디오 신호를 직접 입력받아 처리함으로써 인간과 유사한 반응 속도와 감정 표현이 가능하다.</li>
<li><strong>벤치마크 성능:</strong> 일반적인 시각적 질문 응답(VQA)과 코딩 능력에서 여전히 최상위권의 성능을 유지하고 있으며, 특히 사용자의 의도를 파악하고 자연스러운 대화를 이어가는 능력에서 강점을 보인다.</li>
</ul>
<p>Anthropic Claude 3.5 Sonnet:</p>
<p>Claude 3.5 Sonnet은 시각적 추론, 특히 차트 해석과 과학 도표 분석, 그리고 코딩 관련 시각 자료 이해 능력에서 GPT-4o와 Gemini를 위협하거나 일부 능가하는 모습을 보여준다.29 ’Artifacts’라 불리는 인터페이스를 통해 코드로 생성된 시각 결과물을 실시간으로 렌더링하며 상호작용하는 기능이 특징이다.</p>
<table><thead><tr><th><strong>벤치마크</strong></th><th><strong>Gemini 1.5 Pro</strong></th><th><strong>GPT-4o</strong></th><th><strong>Claude 3.5 Sonnet</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>MMMU</strong> (대학 수준 다중모달)</td><td>62.2%</td><td>69.1%</td><td>68.3%</td><td>GPT-4o 근소 우위</td></tr>
<tr><td><strong>MathVista</strong> (시각적 수학)</td><td>63.9%</td><td>63.8%</td><td>67.7%</td><td>Claude 3.5 Sonnet 우위</td></tr>
<tr><td><strong>AI2D</strong> (과학 도표 이해)</td><td>94.4%</td><td>94.2%</td><td>-</td><td>Gemini 1.5 Pro 우위</td></tr>
<tr><td><strong>ANLS</strong> (문서 VQA)</td><td>93.1%</td><td>92.8%</td><td>95.2%</td><td>Claude 3.5 Sonnet 우위</td></tr>
</tbody></table>
<p>표 1. 주요 VLM 모델의 성능 비교 (2024-2025 기준) 29</p>
<h3>4.2  오픈 소스 혁신: LLaVA와 Qwen</h3>
<p>LLaVA-NeXT (Large Language and Vision Assistant - Next):</p>
<p>LLaVA 시리즈는 오픈 소스 VLM 연구의 표준(De facto Standard)으로 자리 잡았다.12</p>
<ul>
<li><strong>민주화된 연구:</strong> 누구나 접근 가능한 구조와 데이터셋을 공개하여 VLM 연구의 진입 장벽을 낮췄다. 최신 LLaVA-NeXT는 비디오 처리에 ‘AnyRes’ 기술을 적용하여, 이미지 훈련만으로도 놀라운 수준의 제로샷 비디오 이해 능력을 보여준다. 이는 고가의 비디오-텍스트 데이터 쌍 없이도 고성능 비디오 모델을 만들 수 있다는 가능성을 제시했다.12</li>
</ul>
<p>Qwen2.5-VL (Alibaba):</p>
<p>중국 알리바바의 Qwen2.5-VL은 현재 오픈 소스 모델 중 가장 강력한 성능을 자랑한다.11</p>
<ul>
<li><strong>동적 해상도와 문서 처리:</strong> ‘Naive Dynamic Resolution’ 기술을 통해 입력 이미지의 해상도 제한을 없애, 깨알 같은 글씨가 적힌 문서나 복잡한 설계도면을 완벽하게 인식한다.</li>
<li><strong>비주얼 에이전트:</strong> 단순히 이미지를 읽는 것을 넘어, 컴퓨터 화면을 보고 마우스를 클릭하거나 키보드를 입력하는 등 GUI 환경을 제어하는 에이전트(Agent)로서의 능력이 대폭 강화되었다. 1시간 이상의 긴 비디오 이해 능력 또한 오픈 소스 모델 중 최고 수준이다.11</li>
</ul>
<h3>4.3  한국형 특화 모델: 소버린 AI의 도약</h3>
<p>한국 기업들은 방대한 한국어 데이터와 문화적 이해도를 바탕으로 글로벌 모델이 해결하지 못하는 틈새 영역을 공략하고 있다.</p>
<p>Naver HyperCLOVA X:</p>
<p>네이버의 HyperCLOVA X는 한국어 처리와 한국 문화적 맥락 이해에서 독보적이다.35</p>
<ul>
<li><strong>HyperCLOVA X Vision:</strong> 한국의 독특한 문서 양식(공문서, 영수증 등)과 손글씨 인식에 특화되어 있으며, 한국의 지리적 정보나 문화적 뉘앙스가 담긴 이미지를 정확하게 해석한다.37</li>
<li><strong>THINK 모델:</strong> 최근 공개된 ‘THINK’ 모델은 시스템 2(System 2)적 사고 방식을 도입하여, 복잡한 추론 과정을 텍스트와 이미지를 통해 수행한다. 이는 네이버의 검색, 쇼핑, 지도 서비스와 결합되어 실사용자에게 구체적인 가치를 제공한다.38</li>
</ul>
<p>LG Exaone 3.0:</p>
<p>LG AI연구원의 Exaone 3.0은 ’산업 현장의 전문가’를 지향한다.40</p>
<ul>
<li><strong>이중 언어(Bilingual) 및 전문성:</strong> 한국어와 영어를 동시에 완벽하게 구사하며, 논문 4,500만 건, 특허 데이터 등 전문 문헌을 학습하여 화학, 바이오, 기계 공학 등 전문 분야의 도표와 이미지를 해석하는 데 탁월하다.</li>
<li><strong>효율성 혁신:</strong> 경량화 기술을 통해 이전 모델 대비 추론 속도를 56% 높이고 비용을 72% 절감하여, 실제 기업 환경에서의 도입 장벽을 낮췄다.42</li>
</ul>
<p>Samsung Gauss2:</p>
<p>삼성전자의 Gauss2는 온디바이스(On-device) AI와 클라우드의 하이브리드 전략을 취한다.43</p>
<ul>
<li><strong>3가지 모델 라인업:</strong> 모바일 기기에서 구동 가능한 Compact, 범용적인 Balanced, 고성능 서버용 Supreme으로 구성되어 있다.</li>
<li><strong>통합 플랫폼:</strong> 코드 생성, 이미지 생성, 언어 모델을 통합하여 삼성의 가전제품 생태계(SmartThings)를 지능적으로 제어하고, 사내 개발자들의 생산성을 높이는 도구로 활용된다.45</li>
</ul>
<h2>5.  핵심 난제: 환각(Hallucination) 현상의 원인과 해결</h2>
<p>VLM의 뛰어난 성능에도 불구하고, 여전히 가장 큰 걸림돌은 ’환각(Hallucination)’이다. 환각이란 모델이 이미지에 존재하지 않는 사물을 묘사하거나, 사물의 속성(색상, 위치, 행동)을 잘못 설명하는 현상을 말한다.46 이는 의료나 자율주행과 같은 안전이 중요한 분야에서 치명적인 결과를 초래할 수 있다.</p>
<h3>5.1  환각의 구조적 원인</h3>
<ol>
<li><strong>언어적 편향(Language Prior):</strong> 모델이 시각 정보보다 강력한 언어 모델의 통계적 패턴에 과도하게 의존할 때 발생한다.46 예를 들어, 학습 데이터에서 “바나나“와 “노란색“이 자주 함께 등장했다면, 이미지 속 바나나가 초록색임에도 불구하고 모델은 습관적으로 “노란 바나나“라고 묘사할 확률이 높다. 이는 시각 인코더의 정보가 언어 디코더의 강력한 사전 지식을 뚫고 들어가지 못하기 때문이다.</li>
<li><strong>공동 발생 편향(Co-occurrence Bias):</strong> 특정 객체들이 함께 등장하는 패턴을 과대 해석하는 경우이다. 식탁이 보이면 의자가 보이지 않아도 의자가 있다고 추측하거나, 야구 방망이가 있으면 야구공이 있다고 단정 짓는 식이다.47</li>
<li><strong>불완전한 시각적 정렬:</strong> 시각 인코더가 이미지의 세부 정보를 충분히 포착하지 못하거나, 프로젝터가 이를 언어 공간으로 완벽하게 매핑하지 못했을 때 모델은 부족한 정보를 ’상상’으로 채워 넣게 된다.</li>
</ol>
<h3>5.2  최신 환각 완화 기술</h3>
<p>연구계는 훈련 단계와 추론 단계 모두에서 환각을 억제하기 위한 다양한 기법을 제안하고 있다.</p>
<p>시각적 대조 디코딩(Visual Contrastive Decoding, VCD):</p>
<p>추론 단계(Inference-time)에서 적용 가능한 기술이다.48 원본 이미지를 입력했을 때의 출력 확률 분포와, 이미지에 노이즈를 섞거나 일부를 가린(Masking) 이미지를 입력했을 때의 확률 분포를 비교한다. 만약 이미지가 손상되었음에도 불구하고 특정 단어(예: “의자”)의 생성 확률이 여전히 높다면, 이는 시각 정보가 아닌 언어적 편향에 의한 것일 가능성이 높다. VCD는 이러한 단어의 확률을 페널티로 낮추어, 시각 정보에 근거한 단어만이 선택되도록 유도한다.</p>
<p>비전 유도 선호도 최적화(Vision-guided Direct Preference Optimization, V-DPO):</p>
<p>훈련 단계의 기술로, RLHF의 DPO를 VLM에 맞게 개량한 것이다.25 이미지와 함께 ’환각이 포함된 답변(Negative)’과 ‘정확한 답변(Positive)’ 쌍을 데이터로 구성한다. 모델은 이 두 답변의 차이를 학습하며, 시각 정보와 일치하지 않는 텍스트 생성을 억제하는 방향으로 파라미터를 업데이트한다. 실험 결과, V-DPO는 기존 SFT(Supervised Fine-Tuning) 대비 환각 생성률을 유의미하게 감소시키는 것으로 나타났다.25</p>
<p>상호 정보량 디코딩(M3ID):</p>
<p>텍스트 생성 시, 생성된 토큰이 시각 입력과 얼마나 높은 상호 정보량(Mutual Information)을 가지는지를 계산하여 디코딩에 반영한다.46 즉, 이미지를 보지 않고도 생성할 수 있는 뻔한 텍스트보다는, 이미지를 봐야만 생성할 수 있는 정보가 담긴 텍스트에 가산점을 부여하는 방식이다.</p>
<h2>6.  산업적 응용과 미래: 월드 모델과 행동하는 AI</h2>
<p>VLM 기술은 단순한 정보 인식을 넘어 물리적 세계를 이해하고 예측하며 행동하는 단계로 진입하고 있다.</p>
<h3>6.1  자율주행의 새로운 뇌: 월드 모델(World Models)</h3>
<p>기존 자율주행 시스템이 객체 감지, 차선 인식 등 개별 모듈의 집합으로 구성되었다면, VLM 기반 자율주행은 주행 환경 전체를 통찰하는 ’월드 모델’로 진화하고 있다.50</p>
<ul>
<li><strong>맥락적 이해와 예측:</strong> Kodiak Robotics나 Waymo 등의 선도 기업들은 VLM을 통해 도로 위의 비정형적인 상황을 해석한다. 예를 들어, 도로변에 서 있는 사람이 경찰관인지 공사 인부인지, 그들의 수신호가 무엇을 의미하는지를 텍스트와 시각 정보를 결합하여 판단한다.52</li>
<li><strong>제로샷 대응 능력:</strong> 학습 데이터에 없는 희귀한 상황(Edge Case)에서도 VLM의 방대한 일반 상식을 활용하여 합리적인 대처가 가능하다. “도로 위에 넘어진 트럭에서 쏟아진 화물이 액체인지 고체인지“를 판단하여 회피 경로를 설정하는 식이다.</li>
</ul>
<h3>6.2  로보틱스(Robotics)와 VLA(Vision-Language-Action)</h3>
<p>VLM에 로봇 팔이나 이동 장치를 제어하는 ‘행동(Action)’ 토큰을 추가한 VLA 모델은 로봇을 단순 반복 기계에서 범용 도우미로 변모시키고 있다.53</p>
<ul>
<li><strong>자연어 명령 수행:</strong> “나 목말라, 마실 것 좀 줘“라는 사용자의 모호한 명령을 듣고, 로봇은 주방을 스캔(Vision)하여 물병과 컵을 찾고, 컵에 물을 따르는 일련의 동작(Action)을 계획하고 수행한다. 구글의 RT-2(Robotic Transformer 2)와 같은 모델이 이 분야를 선도하고 있으며, 이는 가사 도우미 로봇이나 스마트 팩토리의 유연성을 극대화하는 핵심 기술이 될 것이다.</li>
</ul>
<h3>6.3  비디오 생성과 시뮬레이션</h3>
<p>OpenAI의 Sora나 구글의 Veo와 같은 비디오 생성 모델 역시 VLM 기술의 연장선상에 있다.55 텍스트 설명을 시각적 영상으로 변환하는 능력은, 역으로 시각적 영상을 텍스트로 이해하는 능력과 동전의 양면과 같다. VLM이 세상을 깊이 이해할수록 더욱 정교하고 물리 법칙에 맞는 영상을 생성할 수 있게 되며, 이는 영화, 게임, 가상 현실(VR) 산업의 제작 방식을 근본적으로 변화시킬 것이다.</p>
<h2>7.  결론</h2>
<p>2025년 현재, 시각-언어 모델(VLM)은 인공지능이 텍스트라는 상징의 세계를 넘어, 빛과 형상이 존재하는 물리적 실재의 세계로 나아가는 가장 중요한 관문이다. ViT와 트랜스포머의 결합으로 시작된 아키텍처 혁신은 이제 동적 해상도 처리, 비디오 시공간 이해, 그리고 행동 제어 능력으로까지 확장되었다. Gemini와 GPT-4o가 보여주는 압도적인 성능은 AI가 인간의 지각 능력을 빠르게 따라잡고 있음을 증명하며, Qwen과 LLaVA 등의 오픈 소스 모델은 기술의 민주화를 가속화하고 있다. 한국의 HyperCLOVA X, Exaone, Gauss2 또한 각자의 영역에서 독자적인 생태계를 구축하며 기술 주권을 지켜내고 있다.</p>
<p>그러나 여전히 해결해야 할 과제는 명확하다. 환각 현상의 완전한 제어, 고해상도 비디오 처리에 소요되는 막대한 연산 비용의 절감, 그리고 로봇 등 물리적 장치와의 완벽한 통합이 그것이다. 앞으로의 VLM 연구는 단순히 모델의 크기를 키우는 것을 넘어, 보다 효율적이고 신뢰할 수 있으며, 인간의 삶에 실질적으로 기여하는 ’에이전트(Agent)’로서의 진화에 초점을 맞추게 될 것이다. VLM은 단순한 기술 트렌드가 아니다. 그것은 기계가 세상을 ‘보고’, ‘이해하고’, ‘행동하는’ 방식의 근본적인 재정의이며, 진정한 의미의 인공일반지능(AGI)으로 향하는 가장 확실한 이정표이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>How do Vision-Language Models differ from traditional computer vision and natural language processing models? - Milvus, https://milvus.io/ai-quick-reference/how-do-visionlanguage-models-differ-from-traditional-computer-vision-and-natural-language-processing-models</li>
<li>Computer Vision vs NLP - Medium, https://medium.com/@amit25173/computer-vision-vs-nlp-3c8cf8b9f25a</li>
<li>Guide to Vision-Language Models (VLMs) - Encord, https://encord.com/blog/vision-language-models-guide/</li>
<li>Vision Language Models (VLMs) Explained - DataCamp, https://www.datacamp.com/blog/vlms-ai-vision-language-models</li>
<li>Top 10 Vision Language Models in 2025 - DataCamp, https://www.datacamp.com/blog/top-vision-language-models</li>
<li>A Survey of State of the Art Large Vision Language Models: Alignment, Benchmark, Evaluations and Challenges - arXiv, https://arxiv.org/html/2501.02189v6</li>
<li>Generative AI Model Architecture — Part4: Vision Language Model | by Manimala Kumar, https://manimalakumar-29300.medium.com/generative-ai-model-architecture-part4-vision-language-model-cc03bd604ab5</li>
<li>Image Tokenization — The GenAI Guidebook - Ravin Kumar, https://ravinkumar.com/GenAiGuidebook/image/image_tokenization.html</li>
<li>Improving Vision Transformer Efficiency and Accuracy by Learning to Tokenize, https://research.google/blog/improving-vision-transformer-efficiency-and-accuracy-by-learning-to-tokenize/</li>
<li>How does a Vision-Language-Model (VLM) work? - Groundlight AI, https://www.groundlight.ai/blog/how-vlm-works-tokens</li>
<li>[2502.13923] Qwen2.5-VL Technical Report - arXiv, https://arxiv.org/abs/2502.13923</li>
<li>LLaVA-NeXT: A Strong Zero-shot Video Understanding Model, https://llava-vl.github.io/blog/2024-04-30-llava-next-video/</li>
<li>Design choices for Vision Language Models in 2024 - Hugging Face, https://huggingface.co/blog/gigant/vlm-design</li>
<li>Vision Language Models | Rohit Bandaru, https://rohitbandaru.github.io/blog/Vision-Language-Models/</li>
<li>LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for MLLMs - arXiv, https://arxiv.org/html/2507.00505v2</li>
<li>Vision LLMs: Multimodal Integration - Emergent Mind, https://www.emergentmind.com/topics/vision-large-language-models-vlms</li>
<li>Primers • VLM Architectures - aman.ai, https://aman.ai/primers/ai/VLM/</li>
<li>An Introduction to Vision-Language Modeling - arXiv, https://arxiv.org/html/2405.17247v1</li>
<li>How Vision-Language Models Are Trained: A Deep Dive into the VLM Training Process, https://medium.com/@hexiangnan/how-vision-language-models-are-trained-a-deep-dive-into-the-vlm-training-process-1ba1d8704bb0</li>
<li>What Are Vision Language Models (VLMs)? - IBM, https://www.ibm.com/think/topics/vision-language-models</li>
<li>Vision-Language Instruction Tuning: A Review and Analysis | OpenReview, https://openreview.net/forum?id=ul2tbUPtIQ</li>
<li>[2311.08172] Vision-Language Instruction Tuning: A Review and Analysis - arXiv, https://arxiv.org/abs/2311.08172</li>
<li>Teaching AI to See: A Technical Deep-Dive on Vision Language Models with Will Hardman of Veratai - The Cognitive Revolution, https://www.cognitiverevolution.ai/teaching-ai-to-see-a-technical-deep-dive-on-vision-language-models-with-will-hardman-data-sci/</li>
<li>(2) An Introduction to Vision-Language Modeling: A Guide to VLM Training - bequiet-log, https://bequiet-log.vercel.app/a-guide-to-VLM-training</li>
<li>V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization - NUS Computing, https://www.comp.nus.edu.sg/~kanmy/papers/2024.findings-emnlp.775.pdf</li>
<li>[2411.02712] V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization - arXiv, https://arxiv.org/abs/2411.02712</li>
<li>Exploring Feature Fusion and Matching in Vision-Language Models | by Shawn | Medium, https://medium.com/@hexiangnan/exploring-feature-fusion-and-matching-in-vision-language-models-3573b6e529b4</li>
<li>Top 10 Vision Language Models in 2025 | Benchmark, Use Cases - Dextra Labs, https://dextralabs.com/blog/top-10-vision-language-models/</li>
<li>Gemini 1.5 Pro vs GPT-4o - LLM Stats, https://llm-stats.com/models/compare/gemini-1.5-pro-vs-gpt-4o-2024-05-13</li>
<li>Gemini 2.5: Our most intelligent AI model - Google Blog, https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/</li>
<li>GPT-4o vs. Gemini 1.5 Pro vs. Claude 3 Opus: Multimodal AI Model Comparison - Encord, https://encord.com/blog/gpt-4o-vs-gemini-vs-claude-3-opus/</li>
<li>Claude 3.5 Sonnet vs GPT-4o Comparison Analysis (2025): Which is superior? - Elephas, https://elephas.app/blog/claude-sonnet-vs-gpt4o</li>
<li>LLaVA-VL/LLaVA-NeXT - GitHub, https://github.com/LLaVA-VL/LLaVA-NeXT</li>
<li>Qwen2.5 VL! Qwen2.5 VL! Qwen2.5 VL! | Qwen, https://qwenlm.github.io/blog/qwen2.5-vl/</li>
<li>HyperCLOVA X THINK, https://clova.ai/cdn/media/2025/06/HyperCLOVA_X_THINK_Technical_Report.pdf</li>
<li>HyperCLOVA X Technical Report - arXiv, https://arxiv.org/html/2404.01954v1</li>
<li>Introducing HyperCLOVA X Vision | CLOVA, https://clova.ai/en/tech-blog/en-introducing-hyperclova-x-vision</li>
<li>(PDF) HyperCLOVA X THINK Technical Report - ResearchGate, https://www.researchgate.net/publication/393149221_HyperCLOVA_X_THINK_Technical_Report</li>
<li>How CLOVA X’s CAST system reveals AI’s role in daily life, https://clova.ai/en/tech-blog/how-clova-xs-cast-system-reveals-ais-role-in-daily-life</li>
<li>AWS Marketplace: EXAONE_v3.0 7.8B Instruct, https://aws.amazon.com/marketplace/pp/prodview-yg5edmwlh4tko</li>
<li>EXAONE 3.0 : Showcasing Our First Open-Source LLM with Global Top-Level Performance - LG AI Research BLOG, https://www.lgresearch.ai/blog/view?seq=460</li>
<li>LG’s Pride, ExaOne 3.0 — Is It Really That Impressive? | by AI Network - Medium, https://ai-network.medium.com/lgs-pride-exaone-3-0-is-it-really-that-impressive-213cfa339c4f</li>
<li>Samsung Electronics Hosts Samsung Developer Conference Korea 2024, Unveils Its Improved Gen AI Model, https://news.samsung.com/global/samsung-electronics-hosts-samsung-developer-conference-korea-2024-unveils-its-improved-gen-ai-model</li>
<li>Samsung Electronics Hosts Samsung Developer Conference Korea 2024, Unveils Its Improved Gen AI Model, https://news.samsung.com/uk/samsung-electronics-hosts-samsung-developer-conference-korea-2024-unveils-its-improved-gen-ai-model</li>
<li>[AI Leadership] ③ Samsung’s AI Strategy Centered on Customer Experiences, <a href="https://news.samsung.com/global/ai-leadership-%E2%91%A2-samsungs-ai-strategy-centered-on-customer-experiences">https://news.samsung.com/global/ai-leadership-%E2%91%A2-samsungs-ai-strategy-centered-on-customer-experiences</a></li>
<li>Multi-Modal Hallucination Control by Visual Information Grounding - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Favero_Multi-Modal_Hallucination_Control_by_Visual_Information_Grounding_CVPR_2024_paper.pdf</li>
<li>Treble Counterfactual VLMs: A Causal Approach to Hallucination - arXiv, https://arxiv.org/html/2503.06169v1</li>
<li>Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Leng_Mitigating_Object_Hallucinations_in_Large_Vision-Language_Models_through_Visual_Contrastive_CVPR_2024_paper.pdf</li>
<li>DAMO-NLP-SG/VCD: [CVPR 2024 Highlight] Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding - GitHub, https://github.com/DAMO-NLP-SG/VCD</li>
<li>A Survey of World Models for Autonomous Driving - arXiv, https://arxiv.org/html/2501.11260v4</li>
<li>Predictability in Autonomous Driving Systems - AAAI Publications, https://ojs.aaai.org/index.php/AIES/article/download/36789/38927/40864</li>
<li>How Generative AI and VLMs Improve Autonomous Vehicle Safety - Kodiak AI, https://kodiak.ai/news/llms-take-the-wheel</li>
<li>Large language and vision-language models for robot: safety challenges, mitigation strategies and future directions - Emerald Publishing, https://www.emerald.com/ir/article/doi/10.1108/IR-02-2025-0074/1269979/Large-language-and-vision-language-models-for</li>
<li>Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications - IEEE Xplore, https://ieeexplore.ieee.org/iel8/6287639/10820123/11164279.pdf</li>
<li>Vertex AI Platform | Google Cloud, https://cloud.google.com/vertex-ai</li>
<li>Generate videos with Veo 3.1 in Gemini API content_copy - Google AI for Developers, https://ai.google.dev/gemini-api/docs/video</li>
<li>Unraveling the Latest Breakthroughs in AI: Sora, Gemini 1.5 Pro, and Mistral-Next - Medium, https://medium.com/@Shawanwang/unraveling-the-latest-breakthroughs-in-ai-sora-gemini-1-5-pro-and-mistral-next-231689b64464</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>