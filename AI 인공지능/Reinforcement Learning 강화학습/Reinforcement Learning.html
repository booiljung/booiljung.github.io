<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:강화학습 안내서</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>강화학습 안내서</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">강화 학습 (Reinforcement Learning)</a> / <span>강화학습 안내서</span></nav>
                </div>
            </header>
            <article>
                <h1>강화학습 안내서</h1>
<h2>1.  강화학습의 서막: 학습하는 에이전트의 등장</h2>
<p>강화학습(Reinforcement Learning, RL)은 단순히 기계학습의 한 분야를 넘어, 불확실한 환경 속에서 목표 지향적 학습을 수행하는 에이전트의 상호작용을 정형화하는 강력한 패러다임이다. 이는 행동심리학에서 깊은 영감을 받은 학습 방식으로, 명시적인 정답을 제공받는 대신 오직 시행착오를 통해 최적의 행동 전략을 스스로 발견해 나가는 과정을 다룬다.1 강화학습의 핵심 철학은 에이전트가 환경과 상호작용하며 얻는 스칼라 값의 보상 신호를 통해, 누적 보상을 최대화하는 일련의 행동 순서를 학습하는 데 있다.2 이러한 접근법은 정적인 데이터셋에서 패턴을 찾는 기존의 기계학습 방법론과 근본적인 차이를 보이며, 순차적 의사결정 문제에 대한 일반적인 해결책을 제시한다. 이는 인공지능 분야를 넘어 경제학, 제어 이론, 운영 연구 등 시간에 따라 최적의 결정을 내려야 하는 다양한 학문 분야에 광범위하게 적용될 수 있는 보편적 프레임워크로서의 가능성을 내포한다. ‘상태’, ‘행동’, ’보상’이라는 추상화된 요소는 강화학습을 시간에 따라 전개되는 최적화 문제를 해결하기 위한 범용 언어로 만들어주며, 이 분야의 발전이 전통적인 인공지능 응용을 넘어 광범위한 파급 효과를 가질 수 있음을 시사한다.</p>
<h3>1.1  강화학습의 정의와 핵심 철학</h3>
<p>강화학습은 특정 상태(state)에서 어떤 행동(action)을 취하는 것이 최적인지를 학습하는 기계학습의 한 영역이다.4 이 학습 과정은 정답이 라벨링된 데이터를 통해 배우는 지도학습이나 데이터 내의 숨겨진 구조를 찾는 비지도학습과는 구별된다. 강화학습의 에이전트는 수동으로 레이블이 지정된 입력 데이터 없이 주변 환경과 직접 상호작용하며 문제에 대해 학습한다.2</p>
<p>이 과정을 아기가 처음 걸음마를 떼는 상황에 비유하여 직관적으로 이해할 수 있다.5 여기서 아기는 ’에이전트’에 해당하며, 아기에게 긍정적이거나 부정적인 피드백을 주는 부모와 주변 환경은 ’환경’이 된다. 아기가 한 발을 내딛는 행동을 취했을 때(새로운 상태로 전이), 부모가 미소로 긍정적인 피드백(보상)을 주면 아기는 그 행동을 반복할 가능성이 커진다. 반면, 발을 헛디뎌 넘어지면(다른 상태로 전이) 부정적인 피드백(벌점)을 경험하고, 다음에는 넘어지지 않는 방향으로 행동을 수정하려 할 것이다.5</p>
<p>이처럼 강화학습 에이전트는 환경으로부터 현재 상태에 대한 정보를 받고, 그 정보를 바탕으로 행동을 결정한다. 그 행동의 결과로 환경으로부터 보상이라는 신호를 받게 되며, 이 신호는 에이전트가 미래에 유사한 상태에 처했을 때 해당 행동을 다시 수행하도록 장려하거나 억제하는 역할을 한다.2 이 과정은 모든 새로운 상태에 대해 반복되며, 에이전트는 시간이 지남에 따라 보상과 벌점을 통해 누적 보상을 최대화하는 방향으로 자신의 행동 전략, 즉 ’정책(policy)’을 점진적으로 개선해 나간다.5 궁극적인 목표는 단기적인 보상이 아닌, 장기적인 관점에서 총 보상의 합을 극대화하는 것이다.1</p>
<h3>1.2  지도학습, 비지도학습과의 근본적 차이점</h3>
<p>강화학습의 독창성은 지도학습 및 비지도학습과의 비교를 통해 더욱 명확해진다. 이 세 가지 패러다임은 학습 목표, 데이터의 성격, 그리고 피드백 메커니즘에서 근본적인 차이를 보인다.</p>
<p>**지도학습(Supervised Learning)**은 명시적인 ‘정답’, 즉 레이블(label)이 있는 데이터를 사용하여 입력과 출력 간의 관계를 모델링한다.7 예를 들어, 이미지에 ‘고양이’ 또는 ’개’라는 레이블을 붙여주고 이를 학습시켜 새로운 이미지를 분류하는 문제(분류, Classification)나, 주택의 여러 특징(feature)을 바탕으로 가격을 예측하는 문제(회귀, Regression)가 이에 해당한다.8 지도학습의 목표는 주어진 데이터셋에 대한 정확한 예측 모델을 구축하는 것이다.</p>
<p>**비지도학습(Unsupervised Learning)**은 레이블이 없는 데이터에서 숨겨진 구조나 패턴을 발견하는 것을 목표로 한다.7 비슷한 특성을 가진 데이터들을 그룹으로 묶는 군집화(Clustering)나, 데이터의 복잡성을 줄이면서 핵심적인 특징을 추출하는 차원 축소(Dimensionality Reduction)가 대표적인 예다.6 비지도학습은 데이터 자체의 내재적 구조를 이해하는 데 초점을 맞춘다.</p>
<p>반면, **강화학습(Reinforcement Learning)**은 정답 레이블이 없다는 점에서는 비지도학습과 유사하지만, 학습의 방향을 제시하는 평가적 피드백, 즉 ’보상’이 존재한다는 점에서 결정적인 차이를 보인다.7 강화학습의 데이터는 정적이지 않고, 에이전트와 환경의 상호작용을 통해 순차적으로 생성된다. 여기서 가장 중요한 특징은 ‘지연된 보상(Delayed Reward)’ 개념이다.1 현재의 행동이 즉각적인 보상뿐만 아니라 미래에 받게 될 모든 보상에 영향을 미치기 때문에, 에이전트는 단기적인 이익과 장기적인 이익 사이의 균형을 맞추는 복잡한 과제에 직면하게 된다.</p>
<table><thead><tr><th>기준</th><th>지도학습 (Supervised Learning)</th><th>비지도학습 (Unsupervised Learning)</th><th>강화학습 (Reinforcement Learning)</th></tr></thead><tbody>
<tr><td><strong>학습 목표</strong></td><td>입력과 출력 간의 매핑 함수 학습 (예측)</td><td>데이터의 내재적 구조 및 패턴 발견</td><td>누적 보상을 최대화하는 최적의 행동 정책 학습</td></tr>
<tr><td><strong>입력 데이터</strong></td><td>레이블링된 데이터셋 (<code>(X, Y)</code>)</td><td>레이블 없는 데이터셋 (<code>X</code>)</td><td>에이전트와 환경의 상호작용으로 생성되는 순차적 데이터 (상태, 행동, 보상)</td></tr>
<tr><td><strong>피드백 신호</strong></td><td>명시적인 정답 (레이블)</td><td>피드백 없음 (데이터 구조 자체 활용)</td><td>스칼라 값의 보상 (평가적, 지연될 수 있음)</td></tr>
<tr><td><strong>주요 과제</strong></td><td>분류 (Classification), 회귀 (Regression)</td><td>군집화 (Clustering), 차원 축소 (Dimensionality Reduction)</td><td>제어 (Control), 의사결정 (Decision Making)</td></tr>
<tr><td><strong>대표 알고리즘</strong></td><td>SVM, Random Forest, Neural Networks</td><td>K-Means, PCA, Autoencoders</td><td>Q-Learning, DQN, PPO, Actor-Critic</td></tr>
</tbody></table>
<p>이처럼 강화학습은 정적인 데이터에서 패턴을 찾는 것이 아니라, 상호작용을 통해 생성되는 경험으로부터 최적의 ’행동 전략’을 학습한다는 점에서 다른 기계학습 패러다임과 뚜렷이 구분된다.</p>
<h3>1.3  강화학습의 핵심 구성요소</h3>
<p>강화학습 시스템은 상호작용하는 여러 핵심 요소로 구성되며, 이들의 관계는 강화학습 문제의 근간을 이룬다.2</p>
<ul>
<li>
<p><strong>에이전트 (Agent)</strong>: 학습의 주체이자 의사결정을 내리는 시스템이다.3 에이전트는 환경을 관찰하여 현재 상태를 인식하고, 이 정보를 바탕으로 행동을 선택한다. 로봇, 게임 플레이어, 자율주행차 등이 에이전트의 예가 될 수 있다.6</p>
</li>
<li>
<p><strong>환경 (Environment)</strong>: 에이전트가 상호작용하는 외부 세계를 의미한다.3 환경은 에이전트의 행동을 입력으로 받아 내부 상태를 변경하고, 그 결과로 다음 상태와 보상을 에이전트에게 전달한다. 환경은 강화학습 문제의 규칙과 동역학을 정의한다.5</p>
</li>
<li>
<p><strong>상태 (State, <span class="math math-inline">s</span>)</strong>: 특정 시점에서 환경에 대한 관찰 결과 또는 정보를 나타낸다.1 상태는 에이전트가 다음 행동을 결정하는 데 필요한 모든 정보를 담고 있어야 한다. 체스 게임에서의 말의 위치, 자율주행차의 센서 데이터 등이 상태의 예이다.</p>
</li>
<li>
<p><strong>행동 (Action, <span class="math math-inline">a</span>)</strong>: 에이전트가 특정 상태에서 취할 수 있는 선택 가능한 동작이다.1 에이전트의 목표는 각 상태에서 누적 보상을 최대화할 수 있는 최적의 행동을 선택하는 것이다.</p>
</li>
<li>
<p><strong>보상 (Reward, <span class="math math-inline">r</span>)</strong>: 에이전트가 특정 상태에서 특정 행동을 취한 결과로 환경으로부터 받는 즉각적인 피드백 신호다.1 보상은 보통 스칼라 값으로 주어지며, 긍정적일 수도(보상) 있고 부정적일 수도(벌점) 있다. 보상 함수는 에이전트의 학습 목표를 암시적으로 정의하며, 강화학습의 성공에 있어 매우 중요한 역할을 한다. 에이전트는 이 보상 신호를 통해 자신의 행동이 얼마나 좋았는지를 평가하고 정책을 수정한다.11</p>
</li>
</ul>
<p>이러한 구성요소들은 ’에이전트-환경 루프(Agent-Environment Loop)’라는 상호작용 사이클을 형성한다. 매 시점 <span class="math math-inline">t</span>마다 에이전트는 상태 <span class="math math-inline">S_t</span>를 관찰하고, 이를 바탕으로 행동 <span class="math math-inline">A_t</span>를 선택한다. 환경은 <span class="math math-inline">A_t</span>에 따라 다음 상태 <span class="math math-inline">S_{t+1}</span>로 전이하고, 보상 <span class="math math-inline">R_{t+1}</span>을 에이전트에게 반환한다. 이 과정이 반복되면서 에이전트는 최적의 행동 전략을 학습하게 된다.</p>
<h2>2.  수학적 토대: 마르코프 결정 과정</h2>
<p>강화학습의 이론적 견고함은 순차적 의사결정 문제를 수학적으로 정형화하는 프레임워크인 마르코프 결정 과정(Markov Decision Process, MDP)에서 비롯된다.2 MDP는 에이전트와 환경의 상호작용, 그리고 그에 따른 불확실성을 확률적으로 모델링함으로써, 강화학습 문제를 명확하게 정의하고 분석할 수 있는 기반을 제공한다. MDP의 핵심 가정인 마르코프 속성은 문제의 복잡성을 현실적으로 다룰 수 있는 수준으로 낮추는 결정적인 역할을 한다. 더 나아가, MDP를 구성하는 요소들(<code>S</code>, <code>A</code>, <code>P</code>, <code>R</code>, <code>γ</code>)은 강화학습 알고리즘 전체를 체계적으로 분류하는 기준이 된다. 예를 들어, 에이전트가 환경의 모델, 즉 상태 전이 확률(<code>P</code>)과 보상 함수(<code>R</code>)를 아는지 여부에 따라 알고리즘은 모델 기반(Model-Based)과 모델 프리(Model-Free)로 나뉜다. 또한, 알고리즘이 가치 함수를 학습하는지, 정책을 직접 학습하는지, 혹은 둘 다 학습하는지에 따라 가치 기반(Value-Based), 정책 기반(Policy-Based), 액터-크리틱(Actor-Critic)으로 분류된다. 이처럼 MDP의 정의 자체는 강화학습 알고리즘의 광범위한 지형도를 그리는 근본적인 분기점을 형성한다. 따라서 MDP를 이해하는 것은 단지 문제를 정의하는 것을 넘어, 가능한 모든 해결책의 공간을 이해하는 것과 같다.</p>
<h3>2.1  순차적 의사결정 문제의 정형화</h3>
<p>강화학습이 다루는 대부분의 문제는 본질적으로 순차적 의사결정 문제다. 즉, 현재의 결정이 미래의 상황과 받을 수 있는 보상에 영향을 미치는 문제다. MDP는 이러한 문제를 수학적으로 명확하게 정의하기 위한 프레임워크로, 1960년대에 제안된 이후 강화학습의 이론적 근간이 되어왔다.4</p>
<p>MDP의 가장 중요한 기저에는 **마르코프 속성(Markov Property)**이 있다. 마르코프 속성이란, 미래의 상태와 보상은 오직 현재의 상태와 행동에만 의존하며, 과거의 역사와는 무관하다는 가정이다.12 다시 말해, 현재 상태</p>
<p><span class="math math-inline">S_t</span>는 과거의 모든 정보(<span class="math math-inline">S_0, A_0, R_1,..., S_{t-1}, A_{t-1}, R_t</span>)를 충분히 요약하고 있어, 미래를 예측하는 데 과거의 정보가 추가적으로 필요하지 않다는 것이다. 이를 수식으로 표현하면 다음과 같다.</p>
<p><span class="math math-display">
P(S_{t+1} = s&#39;, R_{t+1} = r | S_t = s, A_t = a) = P(S_{t+1} = s&#39;, R_{t+1} = r | S_t, A_t,..., S_0, A_0)
</span><br />
이 가정은 매우 강력하여, 에이전트가 의사결정을 내릴 때 과거의 모든 이력을 고려할 필요 없이 오직 현재 상태만을 기반으로 최적의 행동을 선택할 수 있게 해준다. 이는 문제의 복잡도를 획기적으로 줄여주어, 강화학습 문제를 수학적으로 다룰 수 있게 만드는 핵심적인 역할을 한다.12</p>
<h3>2.2  MDP의 구성요소 상세 분석</h3>
<p>MDP는 다섯 가지 핵심 요소의 튜플 (<span class="math math-inline">S, A, P, R, \gamma</span>)로 정의된다.14</p>
<ul>
<li>
<p><strong>상태 공간 (State Space, <span class="math math-inline">S</span>)</strong>: 에이전트가 처할 수 있는 모든 가능한 상태의 유한 집합이다.4 예를 들어, 2x2 미로 찾기 문제에서 상태 공간은 4개의 칸 <code>{(1,1), (1,2), (2,1), (2,2)}</code>가 될 수 있다.12</p>
</li>
<li>
<p><strong>행동 공간 (Action Space, <span class="math math-inline">A</span>)</strong>: 각 상태에서 에이전트가 취할 수 있는 모든 가능한 행동의 유한 집합이다.4 미로 찾기 예제에서는 <code>A = {상, 하, 좌, 우}</code>가 될 수 있다.12</p>
</li>
<li>
<p><strong>상태 전이 확률 (State Transition Probability, <span class="math math-inline">P</span>)</strong>: 환경의 동역학(dynamics)을 나타내는 함수다. 현재 상태 <span class="math math-inline">s \in S</span>에서 행동 <span class="math math-inline">a \in A</span>를 취했을 때, 다음 상태 <span class="math math-inline">s&#39; \in S</span>로 전이될 확률을 정의한다.11 이는 <span class="math math-inline">P(s&#39;|s, a) = P(S_{t+1}=s&#39; | S_t=s, A_t=a)</span>로 표현된다.4 상태 전이 확률은 환경의 불확실성을 모델링하는 데 중요한 역할을 한다. 예를 들어, 로봇이 ’앞으로 가기’라는 행동을 취해도 미끄러짐 때문에 100% 원하는 위치로 이동하지 못할 수 있는데, 이러한 불확실성을 확률로 표현한다. 모든 가능한 다음 상태 <span class="math math-inline">s&#39;</span>에 대한 확률의 합은 1이 되어야 한다: <span class="math math-inline">\sum_{s&#39; \in S} P(s&#39;|s, a) = 1</span>.</p>
</li>
<li>
<p><strong>보상 함수 (Reward Function, <span class="math math-inline">R</span>)</strong>: 에이전트의 즉각적인 목표를 정의하는 함수다. 상태 <span class="math math-inline">s</span>에서 행동 <span class="math math-inline">a</span>를 취했을 때 에이전트가 받을 것으로 기대되는 보상 값을 나타낸다.13 이는 <span class="math math-inline">R(s, a) = E</span>로 표현된다.4 보상 함수는 강화학습의 방향성과 목표를 제공하며, 에이전트는 이 보상을 최대화하는 방향으로 행동을 조정하게 된다.13</p>
</li>
<li>
<p><strong>감가율 (Discount Factor, <span class="math math-inline">\gamma</span>)</strong>: 미래 보상의 현재 가치를 결정하는 0과 1 사이의 값(<span class="math math-inline">0 \le \gamma \le 1</span>)이다.13 감가율은 미래에 받을 보상을 현재 시점에서 얼마나 중요하게 여길지를 결정한다.</p>
</li>
</ul>
<p><span class="math math-inline">\gamma</span>가 0에 가까우면 에이전트는 즉각적인 보상만을 중시하는 근시안적(myopic)인 결정을 내리게 되고, 1에 가까우면 미래의 보상까지 고려하는 원시안적(far-sighted)인 결정을 내리게 된다.11 또한, 감가율은 무한한 시간 지평(infinite horizon)을 가진 문제에서 총 보상의 합이 무한대로 발산하는 것을 막아주는 수학적 장치로서의 역할도 수행한다.12</p>
<h3>2.3  정책과 가치 함수: 최적의 길을 찾는 기준</h3>
<p>MDP가 문제 자체를 정의한다면, 정책과 가치 함수는 그 문제를 해결하기 위한 핵심 도구다.</p>
<ul>
<li>
<p><strong>정책 (Policy, <span class="math math-inline">\pi</span>)</strong>: 정책은 에이전트의 행동 방식을 정의하는 규칙으로, 특정 상태에서 어떤 행동을 선택할지를 결정한다.1 즉, 상태를 행동에 매핑하는 함수다.</p>
</li>
<li>
<p><strong>결정론적 정책 (Deterministic Policy)</strong>: 각 상태마다 단 하나의 행동을 지정한다. <span class="math math-inline">a = \pi(s)</span>.1</p>
</li>
<li>
<p><strong>확률론적 정책 (Stochastic Policy)</strong>: 각 상태에서 가능한 행동들에 대한 확률 분포를 정의한다. <span class="math math-inline">\pi(a|s) = P(A_t=a | S_t=s)</span>.1 확률론적 정책은 특히 불확실한 환경이나, 최적의 행동이 여러 개일 수 있는 상황에서 유용하다.</p>
</li>
<li>
<p><strong>가치 함수 (Value Function)</strong>: 가치 함수는 특정 정책 <span class="math math-inline">\pi</span>를 따를 때, 장기적으로 얼마나 좋은지를 나타내는 척도다. 이는 미래에 받을 보상들의 총합, 즉 **반환값(Return)**의 기댓값으로 정의된다.1</p>
</li>
<li>
<p><strong>반환값 (Return, <span class="math math-inline">G_t</span>)</strong>: <span class="math math-inline">t</span> 시점부터 에피소드가 끝날 때까지 받을 감가된 보상의 총합을 의미한다.12</p>
<p><span class="math math-display">
  G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
</span></p>
</li>
<li>
<p><strong>상태 가치 함수 (State-Value Function, <span class="math math-inline">V^{\pi}(s)</span>)</strong>: 상태 <span class="math math-inline">s</span>에서 시작하여 정책 <span class="math math-inline">\pi</span>를 따랐을 때 기대되는 반환값을 의미한다. 즉, 상태 <span class="math math-inline">s</span>가 얼마나 좋은지를 나타낸다.4</p>
<p><span class="math math-display">
  V^{\pi}(s) = E_{\pi}
</span></p>
</li>
<li>
<p><strong>행동 가치 함수 (Action-Value Function, <span class="math math-inline">Q^{\pi}(s, a)</span>)</strong>: 상태 <span class="math math-inline">s</span>에서 행동 <span class="math math-inline">a</span>를 취한 후, 그 이후부터 정책 <span class="math math-inline">\pi</span>를 따랐을 때 기대되는 반환값을 의미한다. 즉, 상태 <span class="math math-inline">s</span>에서 행동 <span class="math math-inline">a</span>를 하는 것이 얼마나 좋은지를 나타낸다.4</p>
<p><span class="math math-display">
  Q^{\pi}(s, a) = E_{\pi}
</span></p>
</li>
</ul>
<p>강화학습의 궁극적인 목표는 이러한 가치 함수들을 최대화하는 **최적 정책(Optimal Policy, <span class="math math-inline">\pi^*</span>)**을 찾는 것이다. 최적 정책을 찾으면, 어떤 상태에 처하더라도 그 정책을 따름으로써 최대의 누적 보상을 얻을 수 있다.</p>
<h2>3.  최적성의 원리: 벨만 방정식</h2>
<p>벨만 방정식(Bellman Equation)은 강화학습의 이론적 핵심을 이루는 방정식으로, 동적 프로그래밍(Dynamic Programming)의 창시자인 리처드 벨만의 최적성의 원리(Principle of Optimality)에 기반한다.16 이 방정식은 가치 함수의 재귀적 성질을 명확하게 정의하며, 현재 상태의 가치를 즉각적인 보상과 다음 상태의 가치로 분해하여 표현한다.18 이 재귀적 관계는 강화학습 문제를 풀 수 있는 열쇠를 제공하며, 가치 반복(Value Iteration)이나 정책 반복(Policy Iteration)과 같은 고전적인 동적 프로그래밍 기법의 이론적 토대가 된다. 더 나아가, Q-러닝의 업데이트 규칙, DQN의 손실 함수, 액터-크리틱의 가치 평가 등 현대 강화학습 알고리즘의 대부분이 벨만 방정식을 풀거나 근사적으로 해결하려는 시도라고 볼 수 있다. 이처럼 벨만 방정식은 이론적 개념에 머무르지 않고, 강화학습 분야의 다양한 알고리즘들을 관통하는 통일된 원리로서 작동하며, 이들의 업데이트 규칙과 손실 함수 설계의 운영 청사진 역할을 한다.</p>
<h3>3.1  가치 함수의 재귀적 관계 정의</h3>
<p>벨만 방정식의 핵심 아이디어는 가치 함수가 현재와 미래 사이의 관계를 통해 재귀적으로 정의될 수 있다는 점이다. 특정 상태 <span class="math math-inline">s</span>의 가치 <span class="math math-inline">V(s)</span>는, 그 상태에서 어떤 행동 <span class="math math-inline">a</span>를 취했을 때 즉시 얻는 보상 <span class="math math-inline">R(s, a)</span>와, 그 행동으로 인해 전이된 다음 상태 <span class="math math-inline">s&#39;</span>의 가치 <span class="math math-inline">V(s&#39;)</span>의 합으로 표현될 수 있다. 이 관계는 다음과 같이 일반화할 수 있다.</p>
<p><span class="math math-display">
V(s) = E[\text{즉각적인 보상} + \gamma \times (\text{다음 상태의 가치})]
</span><br />
이러한 재귀적 구조는 복잡한 장기 보상 문제를 한 단계(one-step)의 예측 문제로 분해할 수 있게 해주며, 반복적인 계산을 통해 가치 함수를 수렴시키는 알고리즘의 기반이 된다.16</p>
<h3>3.2  벨만 기대 방정식: 특정 정책의 가치 평가</h3>
<p>벨만 기대 방정식(Bellman Expectation Equation)은 에이전트가 <strong>특정 정책 <span class="math math-inline">\pi</span>를 따를 때</strong>의 가치 함수(<span class="math math-inline">V^{\pi}</span>와 <span class="math math-inline">Q^{\pi}</span>)가 만족하는 관계식을 나타낸다.16 이 방정식은 주어진 정책이 얼마나 좋은지를 평가(Prediction 또는 Evaluation)하는 데 사용된다.</p>
<ul>
<li>상태 가치 함수 <span class="math math-inline">V^{\pi}(s)</span>에 대한 벨만 기대 방정식:</li>
</ul>
<p>상태 <span class="math math-inline">s</span>의 가치는, 정책 <span class="math math-inline">\pi</span>에 따라 가능한 모든 행동 <span class="math math-inline">a</span>를 선택하고, 각 행동에 대한 즉각적인 보상과 그로 인해 도달하게 될 다음 상태 <span class="math math-inline">s&#39;</span>의 가치를 더한 값의 기댓값과 같다.</p>
<p><span class="math math-display">
  V^{\pi}(s) = E_{\pi}
</span><br />
이를 상태 전이 확률 <span class="math math-inline">P</span>와 보상 함수 <span class="math math-inline">R</span>을 이용해 명시적으로 풀어서 쓰면 다음과 같다.16</p>
<p><span class="math math-display">
  V^{\pi}(s) = \sum_{a \in A} \pi(a \vert s) \left( R(s, a) + \gamma \sum_{s&#39; \in S} P(s&#39; \vert s, a) V^{\pi}(s&#39;) \right)
</span></p>
<ul>
<li>행동 가치 함수 <span class="math math-inline">Q^{\pi}(s, a)</span>에 대한 벨만 기대 방정식:</li>
</ul>
<p>상태 <span class="math math-inline">s</span>에서 행동 <span class="math math-inline">a</span>를 취했을 때의 가치는, 즉각적인 보상과, 그 결과로 도달하게 될 다음 상태 <span class="math math-inline">s&#39;</span>에서 다시 정책 <span class="math math-inline">\pi</span>를 따라 행동할 때의 기대 가치를 더한 값과 같다.</p>
<p><span class="math math-display">
  Q^{\pi}(s, a) = E_{\pi}
</span><br />
이를 명시적으로 풀어서 쓰면 다음과 같다.16</p>
<p><span class="math math-display">
  Q^{\pi}(s, a) = R(s, a) + \gamma \sum_{s&#39; \in S} P(s&#39; \vert s, a) \sum_{a&#39; \in A} \pi(a&#39; \vert s&#39;) Q^{\pi}(s&#39;, a&#39;)
</span></p>
<p>이 방정식들은 선형 방정식 시스템으로, 이론적으로는 행렬 연산을 통해 직접 풀 수 있다. 하지만 상태 공간이 매우 큰 경우에는 반복적인 업데이트를 통해 근사해를 구하는 동적 프로그래밍 기법이 사용된다.</p>
<h3>3.3  벨만 최적 방정식: 최적 정책을 향한 길</h3>
<p>벨만 최적 방정식(Bellman Optimality Equation)은 <strong>최적 정책 <span class="math math-inline">\pi^*</span>를 따를 때</strong>의 최적 가치 함수(<span class="math math-inline">V^*</span>와 <span class="math math-inline">Q^*</span>)가 만족하는 관계식을 나타낸다.16 최적 정책이란 모든 상태에서 다른 어떤 정책보다 크거나 같은 기댓값을 보장하는 정책을 의미한다. 이 방정식은 제어(Control), 즉 최적 정책을 찾는 문제의 핵심이다.</p>
<p>벨만 기대 방정식과의 핵심적인 차이는 정책 <span class="math math-inline">\pi</span>에 대한 기댓값을 취하는 대신, 가능한 모든 행동 중에서 가치를 최대화하는 행동을 선택하는 <span class="math math-inline">\max</span> 연산자를 사용한다는 점이다. 이 <span class="math math-inline">\max</span> 연산자로 인해 벨만 최적 방정식은 비선형 방정식이 된다.</p>
<ul>
<li>최적 상태 가치 함수 <span class="math math-inline">V^*(s)</span>에 대한 벨만 최적 방정식:</li>
</ul>
<p>상태 <span class="math math-inline">s</span>의 최적 가치는, 가능한 모든 행동 <span class="math math-inline">a</span> 중에서 즉각적인 보상과 그로 인해 도달하게 될 다음 상태 <span class="math math-inline">s&#39;</span>의 최적 가치를 더한 값을 최대화하는 행동을 선택했을 때의 가치와 같다.</p>
<p><span class="math math-display">
  V^{*}(s) = \max_{a \in A} E
</span><br />
이를 명시적으로 풀어서 쓰면 다음과 같다.16</p>
<p><span class="math math-display">
  V^{*}(s) = \max_{a \in A} \left( R(s, a) + \gamma \sum_{s&#39; \in S} P(s&#39; \vert s, a) V^{*}(s&#39;) \right)
</span></p>
<ul>
<li>최적 행동 가치 함수 <span class="math math-inline">Q^*(s, a)</span>에 대한 벨만 최적 방정식:</li>
</ul>
<p>상태 <span class="math math-inline">s</span>에서 행동 <span class="math math-inline">a</span>를 취했을 때의 최적 가치는, 즉각적인 보상과, 그 결과로 도달하게 될 다음 상태 <span class="math math-inline">s&#39;</span>에서 가능한 모든 행동 <span class="math math-inline">a&#39;</span> 중 최적 가치를 최대화하는 행동을 선택했을 때의 기대 가치를 더한 값과 같다.</p>
<p><span class="math math-display">
  Q^{*}(s, a) = E
</span><br />
이를 명시적으로 풀어서 쓰면 다음과 같다.16</p>
<p><span class="math math-display">
  Q^{*}(s, a) = R(s, a) + \gamma \sum_{s&#39; \in S} P(s&#39; \vert s, a) \max_{a&#39; \in A} Q^{*}(s&#39;, a&#39;)
</span></p>
<p>이 <span class="math math-inline">Q^*</span>에 대한 벨만 최적 방정식은 특히 중요한데, 이는 모델 프리(model-free) 환경에서 최적 정책을 학습하는 Q-러닝 알고리즘의 이론적 기반을 형성하기 때문이다. 벨만 최적 방정식을 만족하는 <span class="math math-inline">V^*</span> 또는 <span class="math math-inline">Q^*</span>를 찾으면, 이를 통해 최적 정책 <span class="math math-inline">\pi^*</span>를 쉽게 도출할 수 있다. 예를 들어, <span class="math math-inline">Q^*(s, a)</span>를 알고 있다면, 어떤 상태 <span class="math math-inline">s</span>에서든 <span class="math math-inline">Q^*(s, a)</span>를 최대화하는 행동 <span class="math math-inline">a</span>를 선택하는 것이 바로 최적 정책이 된다 (<span class="math math-inline">\pi^*(s) = \arg\max_{a} Q^*(s, a)</span>).</p>
<h2>4.  강화학습 알고리즘 분류 체계</h2>
<p>강화학습 분야는 다양한 문제 설정과 해결 접근법에 따라 수많은 알고리즘으로 분화되었다. 이 복잡한 알고리즘 지형도를 이해하기 위해서는 체계적인 분류 기준이 필요하다. 강화학습 알고리즘은 주로 세 가지 핵심적인 축을 기준으로 분류할 수 있다: (1) 환경 모델의 사용 여부에 따른 <strong>모델 기반(Model-Based)과 모델 프리(Model-Free)</strong>, (2) 학습 대상이 무엇인지에 따른 <strong>가치 기반(Value-Based)과 정책 기반(Policy-Based)</strong>, 그리고 (3) 학습 데이터 생성 방식에 따른 <strong>On-Policy와 Off-Policy</strong>.</p>
<p>이러한 분류 기준들은 서로 배타적이지 않고, 오히려 직교하는(orthogonal) 설계 축으로 작용한다. 즉, 하나의 알고리즘이 여러 분류에 동시에 속할 수 있다. 예를 들어, DDPG 알고리즘은 환경 모델을 학습하지 않으므로 ’모델 프리’이며, 정책(액터)과 가치(크리틱)를 모두 학습하므로 ’액터-크리틱’이고, 과거의 경험 데이터를 재사용하므로 ’Off-Policy’이다.19 반면, PPO는 ’모델 프리’이며 ‘액터-크리틱’ 구조를 가지지만, 매 업데이트마다 새로운 데이터를 수집하므로 ’On-Policy’이다.21 이처럼 알고리즘을 여러 축에서 바라보는 것은 각 알고리즘의 설계 철학과 장단점을 깊이 있게 이해하는 데 도움을 준다. 이는 특정 문제에 적합한 알고리즘을 선택할 때, 샘플 효율성, 학습 안정성, 계산 복잡도와 같은 요소들 사이의 트레이드오프를 체계적으로 고려할 수 있게 해주는 분석적 틀을 제공한다.</p>
<h3>4.1  모델 기반 vs. 모델 프리 학습</h3>
<p>가장 근본적인 분류 기준은 에이전트가 환경의 작동 방식, 즉 ’모델’을 학습하거나 사용하는지 여부다. 여기서 모델이란 상태 전이 확률(<span class="math math-inline">P</span>)과 보상 함수(<span class="math math-inline">R</span>)를 의미한다.</p>
<ul>
<li>
<p><strong>모델 기반 (Model-Based) 강화학습</strong>: 이 접근법에서 에이전트는 환경의 모델을 명시적으로 학습하거나, 문제 설정에서 모델이 주어진다. 에이전트는 이 모델을 사용하여 실제 환경과 상호작용하기 전에, 머릿속으로 시뮬레이션(또는 ‘상상’)을 해볼 수 있다. 이 과정을 **계획(Planning)**이라고 한다.22 계획을 통해 에이전트는 행동의 결과를 예측하고 최적의 행동 순서를 미리 탐색할 수 있으므로, 실제 환경과의 상호작용 횟수를 크게 줄일 수 있다. 이는 **높은 샘플 효율성(sample efficiency)**으로 이어진다.24 하지만 모델 기반 학습은 두 가지 주요 단점을 가진다. 첫째, 정확한 환경 모델을 구축하는 것 자체가 매우 어려운 문제일 수 있다. 둘째, 만약 학습된 모델이 실제 환경을 제대로 반영하지 못한다면, 이 모델 오류는 에이전트의 성능 저하로 직결된다.22</p>
</li>
<li>
<p><strong>모델 프리 (Model-Free) 강화학습</strong>: 이 접근법에서 에이전트는 환경의 모델에 대한 어떠한 가정도 하지 않는다. 대신, 순수한 시행착오를 통해 학습한다.25 에이전트는 환경과 직접 상호작용하며 얻은 경험(상태, 행동, 보상)을 바탕으로 가치 함수나 정책을 점진적으로 개선한다.23 모델을 학습할 필요가 없기 때문에 개념적으로 더 간단하고, 복잡하고 미묘한 동역학을 가진 환경에 더 쉽게 적용할 수 있다. Q-러닝, DQN, PPO 등 현대 심층 강화학습의 주류를 이루는 알고리즘 대부분이 모델 프리 방식이다. 그러나 이 방식은 최적의 정책을 찾기 위해 방대한 양의 경험 데이터를 필요로 하므로,</p>
</li>
</ul>
<p><strong>샘플 효율성이 낮다</strong>는 단점이 있다.24</p>
<h3>4.2  가치 기반 vs. 정책 기반 접근법</h3>
<p>모델 프리 학습은 다시 학습의 주된 대상이 무엇인지에 따라 가치 기반과 정책 기반으로 나뉜다.</p>
<ul>
<li><strong>가치 기반 (Value-Based) 접근법</strong>: 이 방법은 최적 행동 가치 함수 <span class="math math-inline">Q^*(s, a)</span>를 학습하는 데 중점을 둔다.4</li>
</ul>
<p><span class="math math-inline">Q^*(s, a)</span>는 상태 <span class="math math-inline">s</span>에서 행동 <span class="math math-inline">a</span>를 취하는 것이 장기적으로 얼마나 좋은지를 나타내는 값이다. 일단 최적의 Q-함수를 학습하면, 정책은 암시적으로(implicitly) 결정된다. 즉, 어떤 상태 <span class="math math-inline">s</span>에서든 <span class="math math-inline">Q^*(s, a)</span> 값을 최대화하는 행동을 선택하는 탐욕적 정책(greedy policy)을 사용하면 된다: <span class="math math-inline">\pi(s) = \arg\max_a Q^*(s, a)</span>. Q-러닝과 DQN이 대표적인 가치 기반 알고리즘이다.26 이 방법은 주로 이산적인(discrete) 행동 공간을 가진 문제에서 강력한 성능을 보인다.</p>
<ul>
<li>
<p><strong>정책 기반 (Policy-Based) 접근법</strong>: 이 방법은 가치 함수를 거치지 않고, 정책 <span class="math math-inline">\pi(a|s; \theta)</span> 자체를 직접 매개변수화하여 최적화한다.27 여기서 <span class="math math-inline">\theta</span>는 신경망의 가중치와 같은 파라미터다. 정책 기반 방법은 정책의 성능을 나타내는 목적 함수를 정의하고, 이 함수의 그래디언트를 따라 파라미터 <span class="math math-inline">\theta</span>를 업데이트하는 정책 경사(Policy Gradient) 기법을 사용한다.4 이 접근법은 여러 장점을 가진다. 첫째, <strong>연속적인(continuous) 행동 공간</strong>을 자연스럽게 다룰 수 있다. 둘째, 확률적인 정책을 직접 학습할 수 있어, 최적 정책이 확률적일 경우에도 효과적이다. REINFORCE 알고리즘이 대표적인 예다.27</p>
</li>
<li>
<p><strong>액터-크리틱 (Actor-Critic) 접근법</strong>: 가치 기반과 정책 기반의 장점을 결합한 하이브리드 방식이다.26 액터-크리틱 시스템은 두 개의 구성요소로 이루어진다.</p>
</li>
<li>
<p><strong>액터(Actor)</strong>: 정책을 담당하며, 현재 상태를 바탕으로 어떤 행동을 할지 결정한다. 이는 정책 기반 방법과 유사하다.19</p>
</li>
<li>
<p><strong>크리틱(Critic)</strong>: 가치 함수를 학습하며, 액터가 선택한 행동이 얼마나 좋았는지를 평가한다. 이는 가치 기반 방법과 유사하다.19</p>
</li>
</ul>
<p>크리틱은 액터의 학습을 돕는 ‘비평가’ 역할을 한다. 액터가 행동을 하면 크리틱이 그 행동을 평가하고, 액터는 이 피드백을 바탕으로 더 좋은 방향으로 정책을 업데이트한다. 이 구조는 학습의 분산을 줄여 안정성을 높이는 데 효과적이다. A2C, A3C, DDPG, PPO 등이 모두 액터-크리틱 계열에 속한다.</p>
<h3>4.3  On-Policy vs. Off-Policy 학습</h3>
<p>마지막 분류 기준은 학습에 사용되는 데이터를 어떤 정책으로 수집했는지에 관한 것이다.</p>
<ul>
<li>
<p><strong>On-Policy 학습</strong>: 이 방식에서는 에이전트가 <strong>현재 학습하고 개선하려는 정책 <span class="math math-inline">\pi</span>를 따라서 행동</strong>하고, 그로부터 수집된 경험 데이터만을 사용하여 정책 <span class="math math-inline">\pi</span>를 업데이트한다.26 즉, 행동하는 주체와 배우는 주체가 동일하다. 에피소드가 끝나거나 일정량의 데이터가 수집되면, 그 데이터는 정책 업데이트에 한 번 사용된 후 버려진다. 이는 마치 선수가 경기 중에 자신의 플레이를 바탕으로 즉시 전략을 수정하는 것과 같다. 이 방식은 학습 과정이 비교적 안정적이라는 장점이 있지만, 과거 데이터를 재사용할 수 없어 <strong>샘플 효율성이 낮다</strong>. 대표적인 On-Policy 알고리즘으로는 SARSA, A2C, PPO가 있다.26</p>
</li>
<li>
<p><strong>Off-Policy 학습</strong>: 이 방식에서는 에이전트가 <strong>행동을 생성하는 정책(behavior policy, <span class="math math-inline">\mu</span>)과 업데이트 대상이 되는 목표 정책(target policy, <span class="math math-inline">\pi</span>)을 분리</strong>한다.26 예를 들어, 에이전트는 탐험을 많이 하도록 설계된 정책 <span class="math math-inline">\mu</span>에 따라 행동하면서 데이터를 수집하고, 이 데이터를 사용하여 최적의 행동을 하도록 설계된 목표 정책 <span class="math math-inline">\pi</span>를 학습할 수 있다. 이 방식의 가장 큰 장점은 **경험 리플레이(Experience Replay)**를 통해 과거 데이터를 재사용할 수 있다는 점이다. 이는 <strong>샘플 효율성을 크게 향상시킨다</strong>.26 하지만 행동 정책과 목표 정책의 차이가 너무 크면 학습이 불안정해지거나 발산할 위험이 있다. 대표적인 Off-Policy 알고리즘으로는 Q-러닝, DQN, DDPG가 있다.28</p>
</li>
</ul>
<table><thead><tr><th>분류 기준</th><th>세부 항목 1</th><th>세부 항목 2</th></tr></thead><tbody>
<tr><td><strong>모델 유무</strong></td><td><strong>모델 기반 (Model-Based)</strong></td><td><strong>모델 프리 (Model-Free)</strong></td></tr>
<tr><td>핵심 아이디어</td><td>환경의 동역학 모델을 학습/사용하여 계획(Planning) 수행</td><td>모델 없이 순수한 시행착오와 경험을 통해 학습</td></tr>
<tr><td>장점</td><td>높은 샘플 효율성</td><td>구현 용이성, 복잡한 환경에 대한 강건함</td></tr>
<tr><td>단점</td><td>모델의 정확도에 성능이 좌우됨, 모델 구축의 어려움</td><td>낮은 샘플 효율성</td></tr>
<tr><td>대표 알고리즘</td><td>AlphaZero, Dynamic Programming</td><td>Q-Learning, DQN, REINFORCE, PPO</td></tr>
<tr><td><strong>학습 대상</strong></td><td><strong>가치 기반 (Value-Based)</strong></td><td><strong>정책 기반 (Policy-Based)</strong></td></tr>
<tr><td>핵심 아이디어</td><td>최적 가치 함수(<span class="math math-inline">Q^*</span>)를 학습하고, 암시적으로 정책 도출</td><td>정책(<span class="math math-inline">\pi_{\theta}</span>) 자체를 직접 매개변수화하여 최적화</td></tr>
<tr><td>장점</td><td>샘플 효율성이 상대적으로 높음 (Off-Policy와 결합 시)</td><td>연속 행동 공간 처리 가능, 확률적 정책 학습</td></tr>
<tr><td>단점</td><td>연속 행동 공간 처리 어려움, 결정론적 정책만 도출</td><td>높은 분산, 지역 최적해에 수렴할 수 있음</td></tr>
<tr><td>대표 알고리즘</td><td>Q-Learning, DQN, SARSA</td><td>REINFORCE, Policy Gradient</td></tr>
<tr><td><strong>데이터 정책</strong></td><td><strong>On-Policy</strong></td><td><strong>Off-Policy</strong></td></tr>
<tr><td>핵심 아이디어</td><td>현재 정책이 생성한 데이터로만 현재 정책을 업데이트</td><td>행동 정책과 목표 정책을 분리하여 학습</td></tr>
<tr><td>장점</td><td>학습의 안정성이 높음</td><td>높은 샘플 효율성 (경험 리플레이 가능)</td></tr>
<tr><td>단점</td><td>낮은 샘플 효율성</td><td>학습의 불안정성</td></tr>
<tr><td>대표 알고리즘</td><td>SARSA, A2C, PPO</td><td>Q-Learning, DQN, DDPG</td></tr>
</tbody></table>
<h2>5.  모델 프리 제어 기법: 가치 함수 학습</h2>
<p>모델 프리 강화학습에서 가장 고전적이면서도 강력한 접근법 중 하나는 가치 함수를 직접 학습하여 최적의 제어 정책을 찾는 것이다. 이 접근법의 핵심에는 **시간차 학습(Temporal-Difference Learning, TD Learning)**이라는 개념이 자리 잡고 있다. TD 학습은 에피소드가 끝날 때까지 기다리지 않고, 매 시간 단계마다 얻는 정보를 이용해 가치 추정치를 점진적으로 업데이트하는 방식이다. 이 방식은 다른 학습된 추정치를 이용해 현재의 추정치를 업데이트하는 <strong>부트스트래핑(bootstrapping)</strong> 특성을 가지며, 이는 모델 프리 학습의 효율성을 크게 높인다.29</p>
<p>이러한 TD 학습 원리를 기반으로, On-Policy 방식의 SARSA와 Off-Policy 방식의 Q-러닝이라는 두 가지 대표적인 알고리즘이 파생되었다. 이 두 알고리즘의 미묘하지만 결정적인 차이는 On-Policy와 Off-Policy 학습의 본질을 이해하는 데 중요한 단서를 제공한다. 이후, 상태-행동 공간이 거대해지는 문제에 대응하기 위해 신경망과 같은 함수 근사 기법과 Q-러닝을 결합한 **심층 Q-네트워크(Deep Q-Network, DQN)**가 등장했다. DQN의 성공은 단순히 신경망을 적용한 것을 넘어, 함수 근사, 부트스트래핑, Off-Policy 학습이 결합될 때 발생하는 고질적인 불안정성 문제, 이른바 ’죽음의 삼각편대(Deadly Triad)’를 해결하기 위한 독창적인 공학적 해법, 즉 <strong>경험 리플레이</strong>와 <strong>목표 네트워크</strong>를 제시했다는 점에서 심층 강화학습 혁명의 기폭제가 되었다.31</p>
<h3>5.1  시간차 학습(Temporal-Difference Learning)의 원리</h3>
<p>시간차 학습(TD)은 몬테카를로(Monte Carlo, MC) 방법과 동적 프로그래밍(Dynamic Programming, DP)의 아이디어를 결합한 학습 방식이다.</p>
<ul>
<li>
<p><strong>몬테카를로(MC) 방법과의 비교</strong>: MC 방법은 하나의 에피소드가 완전히 종료된 후에야 실제 얻은 반환값(<span class="math math-inline">G_t</span>)을 계산하여 가치 함수를 업데이트한다.29 이는 실제 경험에 기반하므로 편향(bias)이 없는 추정치를 제공하지만, 에피소드가 끝나기 전까지는 학습이 불가능하고, 반환값의 분산(variance)이 크다는 단점이 있다. 특히, 에피소드가 매우 길거나 끝이 없는 연속적인 문제에는 적용하기 어렵다.29</p>
</li>
<li>
<p><strong>TD 학습의 접근법</strong>: TD 학습은 MC와 달리, 에피소드가 끝날 때까지 기다리지 않고 단 한 시간 단계(one step) 후의 정보만을 이용해 가치 함수를 업데이트한다.30 즉,</p>
</li>
</ul>
<p><span class="math math-inline">t+1</span> 시점에서 얻은 실제 보상 <span class="math math-inline">R_{t+1}</span>과 다음 상태 <span class="math math-inline">S_{t+1}</span>의 현재 가치 추정치 <span class="math math-inline">V(S_{t+1})</span>를 이용해 <span class="math math-inline">t</span> 시점의 가치 <span class="math math-inline">V(S_t)</span>를 업데이트한다. 이 과정을 **부트스트래핑(bootstrapping)**이라고 하며, 추정치를 이용해 또 다른 추정치를 개선하는 방식이다.29</p>
<p>TD 학습의 업데이트 규칙은 다음과 같이 표현된다.</p>
<p><span class="math math-display">
V(S_t) \leftarrow V(S_t) + \alpha
</span><br />
여기서 <span class="math math-inline">\alpha</span>는 학습률(learning rate)이고, 괄호 안의 항 $$를 **TD 오차(TD Error)**라고 부른다.30</p>
<p><span class="math math-inline">R_{t+1} + \gamma V(S_{t+1})</span>는 <span class="math math-inline">V(S_t)</span>에 대한 더 나은 추정치, 즉 **TD 목표(TD Target)**가 된다. TD 학습은 현재의 추정치가 TD 목표에 가까워지도록 점진적으로 조정하는 과정이다. 이 방식은 MC보다 분산이 낮고, 모델이 없는 환경에서도 학습이 가능하며, 매 스텝마다 학습이 이루어져 효율적이다.</p>
<h3>5.2. On-Policy 제어: SARSA</h3>
<p>SARSA는 TD 학습 원리를 제어(control) 문제, 즉 최적 정책을 찾는 문제에 적용한 On-Policy 알고리즘이다. SARSA라는 이름은 업데이트에 필요한 정보의 순서, 즉 <strong>S</strong>tate, <strong>A</strong>ction, <strong>R</strong>eward, 다음 <strong>S</strong>tate, 다음 <strong>A</strong>ction에서 유래했다.28</p>
<p>SARSA는 상태 가치 함수 <span class="math math-inline">V(s)</span> 대신 행동 가치 함수 <span class="math math-inline">Q(s, a)</span>를 학습한다. <span class="math math-inline">Q(s, a)</span>는 특정 상태에서 특정 행동을 하는 것의 가치를 나타내므로, 어떤 행동을 선택해야 할지 직접적으로 알려줄 수 있어 제어 문제에 더 적합하다.</p>
<p>SARSA의 업데이트 규칙은 다음과 같다.28</p>
<p><span class="math math-display">
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha
</span><br />
이 수식의 핵심은 TD 목표를 계산할 때 다음 상태 <span class="math math-inline">S_{t+1}</span>에서 **현재 정책 <span class="math math-inline">\pi</span>에 따라 실제로 선택된 다음 행동 <span class="math math-inline">A_{t+1}</span>**의 Q값을 사용한다는 점이다. 일반적으로 행동은 <span class="math math-inline">\epsilon</span>-greedy` 정책에 따라 선택되는데, 이는 대부분의 경우 Q값이 가장 높은 행동을 선택(활용, exploitation)하지만, <span class="math math-inline">\epsilon</span>의 확률로 무작위 행동을 선택(탐험, exploration)하는 방식이다.28</p>
<p>SARSA는 On-Policy 알고리즘이므로, 행동을 선택하는 정책과 학습(평가)하는 정책이 동일하다. 따라서 탐험적인 행동으로 인해 절벽 근처와 같은 위험한 경로를 선택하게 되면, 그 부정적인 결과까지 Q값에 반영된다. 이로 인해 SARSA는 종종 최적의 경로보다 안전한 경로를 학습하는 ‘보수적인’ 정책을 찾는 경향이 있다.28</p>
<h3>5.3. Off-Policy 제어: Q-러닝 (Q-Learning)</h3>
<p>Q-러닝은 모델 프리 제어 분야에서 가장 중요한 알고리즘 중 하나로, TD 학습을 사용하는 Off-Policy 알고리즘이다.34</p>
<p>Q-러닝의 업데이트 규칙은 SARSA와 매우 유사하지만, 결정적인 차이점이 하나 있다.28</p>
<p><span class="math math-display">
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha
</span><br />
Q-러닝은 TD 목표를 계산할 때, 다음 상태 <span class="math math-inline">S_{t+1}</span>에서 실제로 어떤 행동 <span class="math math-inline">A_{t+1}</span>을 선택했는지와는 상관없이, **가능한 모든 행동 <span class="math math-inline">a&#39;</span> 중에서 가장 큰 Q값을 가지는 행동의 가치(<span class="math math-inline">\max_{a&#39;} Q(S_{t+1}, a&#39;)</span>)**를 사용한다.</p>
<p>이것이 바로 Q-러닝이 Off-Policy인 이유다. 행동을 생성하는 **행동 정책(behavior policy)**은 탐험을 위해 <span class="math math-inline">\epsilon</span>-greedy` 정책을 사용하지만, 가치 함수를 업데이트하는 **목표 정책(target policy)**은 항상 최적의 행동을 가정하는 탐욕적(greedy) 정책을 따른다. 즉, Q-러닝은 “탐험적으로 행동하되, 배울 때는 최적의 경로를 따라간다고 가정하고 배우라“는 철학을 따른다.28 이 덕분에 Q-러닝은 행동 정책과 무관하게 최적 행동 가치 함수</p>
<p><span class="math math-inline">Q^*</span>로 직접 수렴할 수 있으며, SARSA보다 더 공격적으로 최적의 정책을 찾아가는 경향이 있다.</p>
<table><thead><tr><th>항목</th><th>SARSA</th><th>Q-러닝 (Q-Learning)</th></tr></thead><tbody>
<tr><td><strong>학습 정책 유형</strong></td><td>On-Policy</td><td>Off-Policy</td></tr>
<tr><td><strong>업데이트 수식</strong></td><td><span class="math math-inline">Q(s,a) \leftarrow Q(s,a) + \alpha(R_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s,a))</span></td><td><span class="math math-inline">Q(s,a) \leftarrow Q(s,a) + \alpha(R_{t+1} + \gamma \max_{a&#39;}Q(s_{t+1}, a&#39;) - Q(s,a))</span></td></tr>
<tr><td><strong>TD 목표</strong></td><td><span class="math math-inline">R_{t+1} + \gamma Q(s_{t+1}, a_{t+1})</span> (실제 다음 행동의 가치)</td><td><span class="math math-inline">R_{t+1} + \gamma \max_{a&#39;}Q(s_{t+1}, a&#39;)</span> (최적 다음 행동의 가치)</td></tr>
<tr><td><strong>핵심 차이</strong></td><td>다음 행동 <span class="math math-inline">a_{t+1}</span>을 현재 정책에 따라 선택하여 업데이트에 사용</td><td>다음 상태에서 Q값이 최대가 되는 행동을 가정하여 업데이트</td></tr>
<tr><td><strong>수렴성</strong></td><td>현재 정책 <span class="math math-inline">\pi</span>에 대한 <span class="math math-inline">Q^{\pi}</span>로 수렴</td><td>최적 정책 <span class="math math-inline">\pi^*</span>에 대한 <span class="math math-inline">Q^*</span>로 수렴 (탐험이 보장될 경우)</td></tr>
<tr><td><strong>학습 정책 특성</strong></td><td>상대적으로 보수적이고 안전한 경로 학습 경향</td><td>최적 경로를 더 공격적으로 학습하는 경향</td></tr>
</tbody></table>
<h3>5.4. 심층 Q-네트워크(DQN): 신경망과의 결합</h3>
<p>SARSA와 Q-러닝은 상태와 행동의 모든 조합에 대한 Q값을 테이블(Q-table)에 저장하고 업데이트한다. 이 방식은 상태와 행동 공간이 작은 문제에서는 효과적이지만, 체스나 바둑처럼 상태 공간이 천문학적으로 크거나, 로봇 제어처럼 상태가 연속적인 값(예: 관절 각도)으로 주어지는 문제에서는 Q-테이블을 만드는 것 자체가 불가능하다. 이를 **차원의 저주(Curse of Dimensionality)**라고 한다.35</p>
<p>**심층 Q-네트워크(Deep Q-Network, DQN)**는 이 문제를 해결하기 위해 Q-테이블 대신 심층 신경망(Deep Neural Network)을 사용하여 Q-함수를 근사하는 방법이다.32 즉, Q-함수를 파라미터 <span class="math math-inline">\theta</span>를 가진 신경망 <span class="math math-inline">Q(s, a; \theta)</span>로 표현한다. 이 신경망은 상태 <span class="math math-inline">s</span>를 입력으로 받아, 가능한 모든 행동 <span class="math math-inline">a</span>에 대한 Q값을 출력한다.</p>
<p>하지만 Q-러닝과 신경망을 단순하게 결합하면 학습이 매우 불안정해지는 문제가 발생한다. 이는 두 가지 주요 원인 때문이다. 첫째, 강화학습에서 수집되는 데이터는 시간적으로 연속되어 있어 상관관계가 매우 높다. 이는 신경망 학습의 기본 가정인 데이터의 독립성(i.i.d. assumption)을 위배하여 학습을 불안정하게 만든다. 둘째, Q-러닝의 업데이트는 현재 Q-네트워크가 예측한 값을 목표(target)로 사용하여 자기 자신을 업데이트하는 구조다. 이로 인해 목표값이 매 스텝마다 흔들리는 **‘움직이는 목표 문제(moving target problem)’**가 발생하여 학습이 발산하기 쉽다.31</p>
<p>DQN은 이 두 가지 불안정성 문제를 해결하기 위해 다음과 같은 핵심 기법을 도입했다.32</p>
<ul>
<li>
<p><strong>경험 리플레이 (Experience Replay)</strong>: 에이전트가 환경과 상호작용하며 얻는 경험 튜플 (<span class="math math-inline">s_t, a_t, r_{t+1}, s_{t+1}</span>)을 순서대로 학습에 사용하는 대신, **리플레이 버퍼(replay buffer)**라는 큰 메모리에 저장한다.33 신경망을 업데이트할 때는 이 버퍼에서 무작위로 미니배치(mini-batch)를 샘플링하여 학습한다. 이 방식은 다음과 같은 장점을 가진다.</p>
<ol>
<li>
<p><strong>데이터 상관관계 제거</strong>: 무작위 샘플링을 통해 시간적 상관관계를 깨뜨려 데이터의 i.i.d. 가정을 만족시킨다.31</p>
</li>
<li>
<p><strong>데이터 효율성 증대</strong>: 하나의 경험을 여러 번의 업데이트에 재사용할 수 있어 데이터 활용도를 높인다.33</p>
</li>
<li>
<p><strong>학습 안정화</strong>: 최근 경험에만 치우치지 않고 다양한 과거 경험을 학습하여 ’파국적 망각(catastrophic forgetting)’을 방지하고 학습을 안정시킨다.32</p>
</li>
</ol>
</li>
<li>
<p><strong>목표 네트워크 (Target Network)</strong>: ’움직이는 목표 문제’를 해결하기 위해, Q-값을 예측하는 주 네트워크(main network, 파라미터 <span class="math math-inline">\theta</span>)와는 별개로, TD 목표를 생성하는 **목표 네트워크(target network, 파라미터 <span class="math math-inline">\theta^-</span>)**를 사용한다.31 TD 목표는</p>
<p><span class="math math-inline">y_t = r_{t+1} + \gamma \max_{a&#39;} Q(s_{t+1}, a&#39;; \theta^-)</span>와 같이 목표 네트워크를 통해 계산된다. 목표 네트워크의 파라미터 <span class="math math-inline">\theta^-</span>는 매 스텝마다 업데이트되지 않고, 일정 주기(예: 수천 스텝)마다 주 네트워크의 파라미터 <span class="math math-inline">\theta</span>로 복사된다. 이렇게 목표 네트워크를 고정함으로써 TD 목표값이 한동안 안정적으로 유지되어, 주 네트워크가 안정적인 목표를 향해 수렴할 수 있게 된다.37</p>
</li>
</ul>
<p>이 두 가지 기법 덕분에 DQN은 고차원의 시각적 입력(예: 아타리 게임 화면)으로부터 직접 학습하여 인간 전문가 수준을 뛰어넘는 성능을 보이는 최초의 알고리즘이 되었고, 심층 강화학습 시대의 서막을 열었다.31</p>
<h2>VI. 정책 직접 최적화: 정책 경사 방법</h2>
<p>가치 기반 방법이 ’어떤 행동이 좋은가’를 학습하는 데 초점을 맞춘다면, 정책 기반 방법은 ’어떤 행동을 할 것인가’라는 정책 자체를 직접적으로 최적화한다. 이 접근법의 핵심에는 **정책 경사 정리(Policy Gradient Theorem)**가 있으며, 이는 정책의 성능을 개선하기 위해 정책의 파라미터를 어떤 방향으로 조정해야 하는지를 알려주는 수학적 기반을 제공한다.39 이 정리를 바탕으로 한 가장 기본적인 알고리즘이 <strong>REINFORCE</strong>이며, 이는 몬테카를로 샘플링을 통해 전체 에피소드의 보상을 바탕으로 정책을 업데이트한다.27</p>
<p>하지만 REINFORCE는 학습 신호인 반환값(return)의 분산이 매우 커서 학습이 불안정하고 비효율적이라는 본질적인 한계를 가진다.27 이러한 한계를 극복하는 과정에서 기계학습의 근본적인 개념인 **편향-분산 트레이드오프(Bias-Variance Tradeoff)**가 중요한 역할을 한다. REINFORCE의 몬테카를로 반환값은 편향이 없는(unbiased) 추정치이지만 분산이 높다. 이 문제를 해결하기 위해 등장한 <strong>액터-크리틱(Actor-Critic)</strong> 구조는, 분산이 높은 실제 반환값 대신 학습된 가치 함수(크리틱)가 제공하는 추정치를 사용한다.19 크리틱의 추정치는 실제 값과 차이가 있을 수 있어 약간의 편향(bias)을 도입하지만, 단일 궤적의 무작위성에 덜 민감하므로 분산(variance)을 획기적으로 줄여준다. 이처럼 REINFORCE에서 액터-크리틱으로의 발전은, 약간의 편향을 감수하는 대신 분산을 크게 낮춤으로써 훨씬 더 안정적이고 효율적인 학습을 달성하려는 의도적인 설계 선택의 결과다. 이 트레이드오프는 A3C, PPO와 같은 대부분의 현대 정책 경사 알고리즘의 근간을 이루는 핵심 원리다.</p>
<h3>6.1. 정책 경사 정리(Policy Gradient Theorem)의 수학적 이해</h3>
<p>정책 기반 방법의 목표는 정책 <span class="math math-inline">\pi_{\theta}(a|s)</span>의 파라미터 <span class="math math-inline">\theta</span>를 조정하여, 보상의 총합에 대한 기댓값인 목적 함수 <span class="math math-inline">J(\theta)</span>를 최대화하는 것이다.27 목적 함수는 시작 상태의 가치 함수로 정의될 수 있다:</p>
<p><span class="math math-display">
J(\theta) = V^{\pi_{\theta}}(s_0) = E_{\tau \sim \pi_{\theta}}[G_0]
</span><br />
우리는 경사 상승법(Gradient Ascent)을 사용하여 이 목적 함수를 최적화하고자 한다. 즉, 목적 함수의 그래디언트 <span class="math math-inline">\nabla_{\theta} J(\theta)</span>를 계산하고, 그 방향으로 파라미터 <span class="math math-inline">\theta</span>를 업데이트한다: <span class="math math-inline">\theta \leftarrow \theta + \alpha \nabla_{\theta} J(\theta)</span>.</p>
<p>문제는 목적 함수 <span class="math math-inline">J(\theta)</span>가 정책 <span class="math math-inline">\pi_{\theta}</span>에 복잡하게 의존한다는 점이다. 정책 파라미터 <span class="math math-inline">\theta</span>는 행동 선택 확률에 직접적으로 영향을 미칠 뿐만 아니라, 에이전트가 방문하게 될 상태의 분포에도 간접적으로 영향을 미친다. 따라서 <span class="math math-inline">\nabla_{\theta} J(\theta)</span>를 계산하는 것은 간단하지 않다.</p>
<p><strong>정책 경사 정리</strong>는 이 문제를 해결하는 우아한 해법을 제공한다. 이 정리에 따르면, 목적 함수의 그래디언트는 상태 분포의 미분을 포함하지 않는 간단한 기댓값 형태로 표현될 수 있다.39<br />
<span class="math math-display">
\nabla_{\theta} J(\theta) = \mathbb{E}_{\pi_{\theta}} \left[ \nabla_{\theta} \log \pi_{\theta}(a|s) Q^{\pi_{\theta}}(s, a) \right]
</span><br />
여기서 <span class="math math-inline">\mathbb{E}_{\pi_{\theta}}[\cdot]</span>는 정책 <span class="math math-inline">\pi_{\theta}</span>를 따를 때의 상태와 행동에 대한 기댓값을 의미한다. 이 식은 직관적인 해석을 제공한다.</p>
<ul>
<li>
<p><span class="math math-inline">\nabla_{\theta} \log \pi_{\theta}(a|s)</span>: 로그-확률의 그래디언트로, ’스코어 함수(score function)’라고도 불린다. 이 벡터는 파라미터 <span class="math math-inline">\theta</span>를 약간 변경했을 때, 상태 <span class="math math-inline">s</span>에서 행동 <span class="math math-inline">a</span>를 선택할 확률이 얼마나 변하는지를 나타낸다. 즉, 특정 행동의 확률을 높이는 방향을 가리킨다.</p>
</li>
<li>
<p><span class="math math-inline">Q^{\pi_{\theta}}(s, a)</span>: 상태 <span class="math math-inline">s</span>에서 행동 <span class="math math-inline">a</span>를 취했을 때의 행동 가치. 이 행동이 장기적으로 얼마나 좋은지를 평가하는 척도다.</p>
</li>
</ul>
<p>따라서 정책 경사는, 좋은 행동(Q값이 높은)의 확률은 높이고(<span class="math math-inline">\nabla_{\theta} \log \pi_{\theta}</span> 방향으로 업데이트), 나쁜 행동(Q값이 낮은)의 확률은 낮추는(<span class="math math-inline">-\nabla_{\theta} \log \pi_{\theta}</span> 방향으로 업데이트) 방향으로 정책을 조정하라는 의미를 담고 있다. 이 정리는 샘플링을 통해 그래디언트를 근사할 수 있는 길을 열어주어, 다양한 정책 경사 알고리즘의 이론적 토대가 된다.</p>
<h3>6.2. REINFORCE 알고리즘: 몬테카를로 정책 경사</h3>
<p>REINFORCE 알고리즘은 정책 경사 정리를 가장 직접적으로 구현한 몬테카를로 기반 방법이다.27 이 알고리즘은 정책 경사 정리의</p>
<p><span class="math math-inline">Q^{\pi_{\theta}}(s, a)</span> 항을, 에피소드가 끝난 후 실제로 얻은 반환값 <span class="math math-inline">G_t</span>로 대체하여 사용한다. <span class="math math-inline">G_t</span>는 <span class="math math-inline">Q^{\pi_{\theta}}(S_t, A_t)</span>의 편향 없는(unbiased) 샘플 추정치이기 때문에, 이론적으로 타당한 근사다.</p>
<p>REINFORCE 알고리즘의 절차는 다음과 같다.41</p>
<ol>
<li>
<p><strong>궤적 생성</strong>: 현재 정책 <span class="math math-inline">\pi_{\theta}</span>를 사용하여 하나의 완전한 에피소드 궤적 <span class="math math-inline">\tau = (S_0, A_0, R_1, S_1, A_1, R_2, \dots, S_{T-1}, A_{T-1}, R_T)</span>를 생성한다.</p>
</li>
<li>
<p><strong>반환값 계산</strong>: 궤적의 각 시점 <span class="math math-inline">t=0, 1, \dots, T-1</span>에 대해, 그 시점부터 끝까지의 감가된 반환값 <span class="math math-inline">G_t = \sum_{k=t+1}^{T} \gamma^{k-t-1} R_k</span>를 계산한다.</p>
</li>
<li>
<p><strong>파라미터 업데이트</strong>: 계산된 반환값을 사용하여 정책 파라미터 <span class="math math-inline">\theta</span>를 업데이트한다.</p>
</li>
</ol>
<p><span class="math math-display">
\theta \leftarrow \theta + \alpha \sum_{t=0}^{T-1} \gamma^t G_t \nabla_{\theta} \log \pi_{\theta}(A_t|S_t)
   </span><br />
(여기서 <span class="math math-inline">\gamma^t</span>는 먼 미래의 그래디언트보다 가까운 미래의 그래디언트에 더 비중을 두기 위한 항으로, 생략되기도 한다.)</p>
<p>REINFORCE는 구현이 간단하고 이론적으로도 잘 정립되어 있지만, 치명적인 단점을 가지고 있다. 몬테카를로 방식으로 추정한 반환값 <span class="math math-inline">G_t</span>는 한 번의 궤적에 크게 의존하기 때문에 <strong>분산(variance)이 매우 높다</strong>.27 이로 인해 그래디언트 추정치가 매번 크게 요동치게 되어 학습 과정이 불안정하고 수렴 속도가 매우 느리다.</p>
<h3>6.3. 액터-크리틱(Actor-Critic) 구조: 정책과 가치의 결합</h3>
<p>액터-크리틱(Actor-Critic) 방법은 REINFORCE의 높은 분산 문제를 해결하기 위해 정책 기반 방법과 가치 기반 방법을 영리하게 결합한 구조다.19 이 구조는 학습을 담당하는 두 개의 분리된 모델, 즉 액터와 크리틱으로 구성된다.19</p>
<ul>
<li>
<p><strong>액터 (Actor)</strong>: 정책 <span class="math math-inline">\pi_{\theta}(a|s)</span>를 학습하고, 행동을 결정하는 역할을 한다. 이는 정책 기반 방법의 ’정책’에 해당한다.19</p>
</li>
<li>
<p><strong>크리틱 (Critic)</strong>: 가치 함수 <span class="math math-inline">V_w(s)</span> 또는 <span class="math math-inline">Q_w(s, a)</span>를 학습하여, 액터가 수행한 행동을 평가하는 역할을 한다. 여기서 <span class="math math-inline">w</span>는 크리틱 네트워크의 파라미터다. 이는 가치 기반 방법의 ’가치 함수’에 해당한다.19</p>
</li>
</ul>
<p>액터-크리틱의 학습 과정은 다음과 같이 상호 보완적으로 이루어진다.</p>
<ol>
<li>
<p><strong>액터의 행동</strong>: 현재 상태 <span class="math math-inline">S_t</span>에서 액터의 정책 <span class="math math-inline">\pi_{\theta}</span>에 따라 행동 <span class="math math-inline">A_t</span>를 선택하고 수행한다.</p>
</li>
<li>
<p><strong>크리틱의 평가</strong>: 환경으로부터 다음 상태 <span class="math math-inline">S_{t+1}</span>과 보상 <span class="math math-inline">R_{t+1}</span>을 받는다. 크리틱은 이 정보를 이용해 TD 오차 <span class="math math-inline">\delta_t</span>를 계산한다.</p>
</li>
</ol>
<p><span class="math math-display">
\delta_t = R_{t+1} + \gamma V_w(S_{t+1}) - V_w(S_t)
   </span><br />
이 TD 오차는 액터의 행동이 예상보다 얼마나 더 좋았거나 나빴는지를 나타내는 척도가 된다.</p>
<ol start="3">
<li>
<p><strong>크리틱 업데이트</strong>: 크리틱은 TD 오차를 최소화하는 방향으로 자신의 파라미터 <span class="math math-inline">w</span>를 업데이트한다 (예: <span class="math math-inline">\min ( \delta_t^2 )</span>).</p>
</li>
<li>
<p><strong>액터 업데이트</strong>: 액터는 크리틱이 계산한 TD 오차를 사용하여 정책 파라미터 <span class="math math-inline">\theta</span>를 업데이트한다. 높은 분산을 가진 <span class="math math-inline">G_t</span> 대신, 분산이 낮은 <span class="math math-inline">\delta_t</span>를 사용함으로써 학습의 안정성을 크게 높인다.</p>
</li>
</ol>
<p><span class="math math-display">
\theta \leftarrow \theta + \alpha \delta_t \nabla_{\theta} \log \pi_{\theta}(A_t|S_t)
   </span></p>
<p>더 나아가, 분산을 더욱 줄이기 위해 <strong>어드밴티지 함수(Advantage Function)</strong> <span class="math math-inline">A(s, a) = Q(s, a) - V(s)</span>를 사용하는 것이 일반적이다. 어드밴티지는 상태 <span class="math math-inline">s</span>의 평균적인 가치(<span class="math math-inline">V(s)</span>) 대비 행동 <span class="math math-inline">a</span>가 얼마나 더 좋은 선택인지를 나타낸다. 이를 학습 신호로 사용하면, 어떤 상태의 전반적인 가치가 높고 낮음에 상관없이, 오직 행동의 상대적인 좋음만을 기준으로 학습할 수 있어 더욱 안정적인 그래디언트 추정이 가능하다.19 실제 구현에서는</p>
<p><span class="math math-inline">Q(s, a)</span>를 직접 추정하는 대신, TD 오차 <span class="math math-inline">\delta_t</span>를 어드밴티지 함수의 추정치로 사용한다. 이 액터-크리틱 구조는 현대 정책 경사 알고리즘의 근간을 형성하며, 이후 소개될 DDPG와 PPO의 핵심적인 구성 요소가 된다.</p>
<h2>VII. 최신 강화학습 알고리즘 심층 분석</h2>
<p>강화학습 분야는 DQN과 액터-크리틱의 기본 원리를 바탕으로, 더 복잡하고 현실적인 문제에 대응하기 위해 끊임없이 발전해왔다. 특히 연속적인 행동 공간을 다루는 문제와 학습의 안정성을 확보하는 문제는 현대 강화학습 연구의 핵심 과제였다. 이러한 배경에서 등장한 **심층 결정론적 정책 경사(DDPG)**와 **근접 정책 최적화(PPO)**는 각각의 방식으로 이 문제들에 대한 중요한 해결책을 제시하며 현재 가장 널리 사용되는 알고리즘으로 자리매김했다.</p>
<p>이러한 최신 알고리즘들의 발전 과정은 ’제약된 최적화(Constrained Optimization)’라는 통일된 주제로 이해할 수 있다. REINFORCE와 같은 초기 정책 경사 방법은 제한 없이 정책을 업데이트하다 보니, 한 번의 잘못된 업데이트가 전체 학습 과정을 망가뜨리는 불안정성을 보였다. 이에 대한 해법으로 등장한 것이 정책 업데이트의 크기를 명시적으로 제한하려는 시도다. TRPO는 정책 변화량을 KL 발산(KL-divergence)으로 측정하여 특정 ‘신뢰 영역(trust region)’ 내에서만 업데이트를 수행하도록 강제했지만, 계산이 복잡했다.5 PPO는 이 아이디어를 클리핑(clipping)이라는 훨씬 간단하고 효율적인 1차 최적화 기법으로 구현하여, 안정성과 성능, 구현 용이성 사이의 절묘한 균형을 맞췄다.21 심지어 DDPG에서 사용되는 목표 네트워크의 ‘소프트 업데이트’ 방식 역시, 목표 정책이 급격하게 변하는 것을 막는 일종의 제약으로 볼 수 있다.20 이처럼 현대 강화학습 알고리즘의 성공은 단순히 그래디언트를 찾는 것을 넘어, ‘안전하고 안정적인’ 그래디언트를 찾는 방향으로 진화해왔음을 보여준다. 이는 강화학습 연구가 이론적 가능성을 넘어 실제적 안정성을 추구하는 성숙 단계에 접어들었음을 시사한다.</p>
<h3>7.1. 심층 결정론적 정책 경사 (Deep Deterministic Policy Gradient, DDPG)</h3>
<p>DDPG는 DQN의 성공을 로보틱스나 자율 주행과 같이 **연속적인 행동 공간(continuous action space)**을 가진 문제에 확장하기 위해 개발된 알고리즘이다.20 DQN은 각 이산 행동(discrete action)에 대한 Q값을 출력하고 그중 최댓값을 선택하는 방식이므로, 행동이 연속적인 실수 값(예: 로봇 팔의 토크, 자동차의 핸들 각도)일 경우 적용할 수 없다.</p>
<p>DDPG는 이 문제를 해결하기 위해 **결정론적 정책 경사(Deterministic Policy Gradient)**와 <strong>액터-크리틱</strong> 구조를 결합하고, DQN의 안정화 기법들을 차용했다.</p>
<ul>
<li>
<p><strong>액터-크리틱 구조</strong>: DDPG는 두 개의 심층 신경망을 사용한다.20</p>
<ul>
<li>
<p><strong>액터 네트워크 (<span class="math math-inline">\mu_{\theta}(s)</span>)</strong>: 상태 <span class="math math-inline">s</span>를 입력받아 특정 행동 <span class="math math-inline">a</span>를 출력하는 <strong>결정론적 정책</strong>을 학습한다. 확률 분포가 아닌 단일 행동 값을 직접 출력하므로 계산 효율성이 높다.20</p>
</li>
<li>
<p><strong>크리틱 네트워크 (<span class="math math-inline">Q_w(s, a)</span>)</strong>: 상태 <span class="math math-inline">s</span>와 액터가 제안한 행동 <span class="math math-inline">a</span>를 함께 입력받아 그 행동의 가치, 즉 Q값을 평가한다.</p>
</li>
</ul>
</li>
<li>
<p><strong>DQN 아이디어의 차용</strong>: DDPG는 Off-policy 알고리즘으로, DQN과 마찬가지로 학습 안정성을 위해 <strong>경험 리플레이 버퍼</strong>와 <strong>목표 네트워크</strong>를 사용한다.19 중요한 점은 액터와 크리틱 각각에 대해 별도의 목표 네트워크를 유지한다는 것이다.</p>
<ul>
<li>
<p><strong>경험 리플레이</strong>: 과거의 경험 튜플 (<span class="math math-inline">s, a, r, s&#39;</span>)을 버퍼에 저장하고 무작위로 샘플링하여 학습에 사용한다.</p>
</li>
<li>
<p><strong>목표 네트워크</strong>: 크리틱의 TD 목표를 계산할 때 목표 크리틱 네트워크(<span class="math math-inline">Q_{w&#39;}</span>)와 목표 액터 네트워크(<span class="math math-inline">\mu_{\theta&#39;}</span>)를 사용한다: <span class="math math-inline">y_i = r_i + \gamma Q_{w&#39;}(s_{i+1}, \mu_{\theta&#39;}(s_{i+1}))</span>. 이를 통해 DQN과 마찬가지로 ’움직이는 목표 문제’를 완화한다.</p>
</li>
</ul>
</li>
<li>
<p><strong>소프트 업데이트 (Soft Update)</strong>: 목표 네트워크를 일정 주기마다 주 네트워크로 통째로 복사하는 DQN의 방식(하드 업데이트) 대신, DDPG는 매 스텝마다 주 네트워크의 가중치를 아주 조금씩 목표 네트워크에 반영하는 <strong>소프트 업데이트</strong> 방식을 사용한다.20</p>
</li>
</ul>
<p><span class="math math-display">
\theta&#39; \leftarrow \tau\theta + (1-\tau)\theta&#39;
  </span></p>
<p><span class="math math-display">
w&#39; \leftarrow \tau w + (1-\tau)w&#39;
  </span></p>
<p>여기서 <span class="math math-inline">\tau \ll 1</span> (예: 0.001)이다. 이 방식은 목표 네트워크가 부드럽게 변하게 하여 학습 과정을 더욱 안정시킨다.</p>
<ul>
<li><strong>탐험 (Exploration)</strong>: 정책이 결정론적이기 때문에, 탐험을 위해서는 행동에 인위적인 노이즈(noise)를 추가해야 한다. 예를 들어, 액터가 출력한 행동에 오차 과정(Ornstein-Uhlenbeck process)이나 가우시안 노이즈를 더하여 실행한다.20</li>
</ul>
<p>DDPG는 이러한 요소들을 결합하여 고차원의 연속 행동 공간 문제에서 뛰어난 성능을 보이며, 많은 로봇 제어 및 시뮬레이션 환경에서 표준적인 알고리즘으로 사용되고 있다.</p>
<h3>7.2. 근접 정책 최적화 (Proximal Policy Optimization, PPO)</h3>
<p>PPO는 OpenAI에서 개발한 정책 경사 알고리즘으로, 이전의 TRPO(Trust Region Policy Optimization)가 가졌던 안정적인 학습 성능은 유지하면서도, 구현이 훨씬 간단하고 계산 효율성이 높아 현재 강화학습 분야에서 가장 널리 사용되는 알고리즘 중 하나가 되었다.5 PPO는 On-policy 액터-크리틱 알고리즘으로, 핵심 아이디어는 정책 업데이트가 이전 정책으로부터 너무 멀리 벗어나지 않도록 제한하여 학습의 안정성을 확보하는 것이다.</p>
<p>PPO는 이 목표를 **클리핑된 대리 목적 함수(Clipped Surrogate Objective Function)**라는 독창적인 방법을 통해 달성한다.43</p>
<ul>
<li><strong>대리 목적 함수 (Surrogate Objective Function)</strong>: 먼저, 정책 업데이트의 이점을 측정하기 위해 다음과 같은 목적 함수를 고려한다.</li>
</ul>
<p><span class="math math-display">
L^{CPI}(\theta) = \hat{\mathbb{E}}_t \left[ \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A}_t \right] = \hat{\mathbb{E}}_t \left[ r_t(\theta) \hat{A}_t \right]
  </span><br />
여기서 <span class="math math-inline">r_t(\theta) = \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}</span>는 현재 정책과 이전 정책의 **확률 비율(probability ratio)**이며, <span class="math math-inline">\hat{A}_t</span>는 어드밴티지 추정치다. 이 목적 함수를 제한 없이 최대화하면 정책이 너무 크게 변하여 학습이 불안정해질 수 있다.45</p>
<ul>
<li>
<p><strong>클리핑 (Clipping)</strong>: PPO는 이 문제를 해결하기 위해 확률 비율 <span class="math math-inline">r_t(\theta)</span>의 값을 특정 범위 <span class="math math-inline">[1-\epsilon, 1+\epsilon]</span> 내로 강제로 제한(clipping)한다. 여기서 <span class="math math-inline">\epsilon</span>은 보통 0.1 또는 0.2와 같은 작은 하이퍼파라미터다.44</p>
</li>
<li>
<p><strong>PPO의 최종 목적 함수</strong>: PPO는 클리핑되지 않은 원래의 목적 함수와 클리핑된 목적 함수 중 더 작은(비관적인) 값을 최종 목적 함수로 선택한다.43</p>
</li>
</ul>
<p><span class="math math-display">
L^{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[ \min(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t) \right]
  </span></p>
<p>이 목적 함수의 작동 원리는 어드밴티지 <span class="math math-inline">\hat{A}_t</span>가 양수일 때와 음수일 때로 나누어 생각하면 명확해진다.46</p>
<ul>
<li>
<p><strong><span class="math math-inline">\hat{A}_t &gt; 0</span> (행동이 평균보다 좋았을 때)</strong>: 목적 함수는 <span class="math math-inline">\min(r_t(\theta) \hat{A}_t, (1+\epsilon)\hat{A}_t)</span>가 된다. 이는 <span class="math math-inline">r_t(\theta)</span>가 <span class="math math-inline">1+\epsilon</span>보다 커지는 것을 막아, 좋은 행동의 확률을 너무 과도하게 증가시키지 않도록 제한하는 역할을 한다. 즉, 정책 업데이트에 ’상한선’을 둔다.</p>
</li>
<li>
<p><strong><span class="math math-inline">\hat{A}_t &lt; 0</span> (행동이 평균보다 나빴을 때)</strong>: 목적 함수는 <span class="math math-inline">\max(r_t(\theta) \hat{A}_t, (1-\epsilon)\hat{A}_t)</span>가 된다 (음수이므로 <span class="math math-inline">\min</span>이 <span class="math math-inline">\max</span>로 바뀜). 이는 <span class="math math-inline">r_t(\theta)</span>가 <span class="math math-inline">1-\epsilon</span>보다 작아지는 것을 막아, 나쁜 행동의 확률을 너무 과도하게 감소시키지 않도록 제한하는 역할을 한다. 즉, 정책 업데이트에 ’하한선’을 둔다.</p>
</li>
</ul>
<p>결론적으로, PPO의 클리핑 메커니즘은 정책이 한 번의 업데이트로 급격하게 변하는 것을 방지하여, 마치 보이지 않는 ‘신뢰 영역’ 내에서 안정적으로 학습이 진행되도록 유도한다. 이러한 단순하면서도 효과적인 접근법 덕분에 PPO는 복잡한 하이퍼파라미터 튜닝 없이도 다양한 환경에서 견고한 성능을 보이며, 심층 강화학습 연구와 응용의 사실상 표준(de-facto standard) 알고리즘으로 자리 잡았다.44</p>
<h2>VIII. 강화학습의 응용: 이론에서 현실로</h2>
<p>강화학습은 더 이상 학문적 이론에만 머무르지 않고, 게임, 로보틱스, 금융, 추천 시스템 등 다양한 산업 분야에서 복잡한 실제 문제를 해결하는 핵심 기술로 부상하고 있다. 특히 딥마인드의 알파고가 세계 최정상급 바둑 기사를 이긴 사건은 강화학습의 잠재력을 전 세계에 각인시키는 결정적인 계기가 되었다.47 이후 강화학습은 가상 환경에서의 게임 AI를 넘어, 물리적 세계와 상호작용하는 로봇 제어, 동적인 시장 상황에 대응하는 금융 시스템, 그리고 사용자와의 상호작용을 통해 개인화된 경험을 제공하는 추천 시스템으로 그 영역을 확장해 나가고 있다.</p>
<p>그러나 이러한 성공적인 응용 사례들을 깊이 들여다보면 공통적으로 마주치는 중요한 실질적 과제가 드러나는데, 바로 <strong>’보상 함수 설계(Reward Engineering)’의 어려움</strong>이다. 바둑과 같이 승패가 명확한 게임에서는 보상(+1 또는 -1)이 단순하지만, 현실 세계의 문제는 그렇지 않다.47 로봇에게 물건을 집는 법을 가르칠 때, 단지 성공 여부만 보상으로 주어야 할까, 아니면 부드러운 움직임, 에너지 효율성, 충돌 회피 등에도 보상을 주어야 할까?.3 잘못 설계된 보상 함수는 에이전트가 의도치 않은 편법(reward hacking)을 배우게 할 수 있다. 거대 언어 모델(LLM) 분야에서 이 문제는 더욱 두드러진다. ’좋은 답변’에 대한 보상은 정량화하기 어렵기 때문에, 인간의 선호도를 학습하는 별도의 ’보상 모델’을 먼저 구축하고 이를 바탕으로 강화학습을 수행하는 복잡한 <strong>RLHF(인간 피드백 기반 강화학습)</strong> 프로세스가 고안되었다.3 이처럼 강화학습이 시뮬레이션을 넘어 현실 세계로 나아갈수록, 문제는 ’어떻게 최적화할 것인가’에서 ’무엇을 최적화할 것인가’로 이동하고 있다. 보상 함수 설계의 어려움은 강화학습을 현실에 적용하는 데 있어 가장 큰 병목 현상 중 하나이며, 이를 자동화하려는 연구(예: Eureka)는 이 문제가 얼마나 중요하고 어려운지를 방증한다.3</p>
<h3>8.1. 게임 AI의 정복: 알파고에서 알파스타까지</h3>
<p>게임은 명확한 규칙, 정량화 가능한 목표, 그리고 안전한 시뮬레이션 환경을 제공하기 때문에 강화학습 알고리즘을 개발하고 테스트하는 이상적인 테스트베드 역할을 해왔다.47</p>
<ul>
<li>
<p><strong>알파고 (AlphaGo)와 알파고 제로 (AlphaGo Zero)</strong>: 2016년 딥마인드의 알파고가 이세돌 9단과의 대국에서 승리한 것은 인공지능 역사상 중요한 이정표였다. 알파고는 인간 전문가의 기보 16만 개를 학습(지도학습)한 후, 스스로와의 대국(강화학습)을 통해 정책망과 가치망을 강화하는 방식으로 훈련되었다.50 하지만 이세돌 9단에게 내준 1패는, 알파고가 학습 데이터에 없는 창의적인 수에 대응하지 못했기 때문이라는 분석을 낳았다. 이후 등장한</p>
<p><strong>알파고 제로</strong>는 인간의 데이터를 전혀 사용하지 않고, 오직 바둑의 규칙만을 가지고 무작위로 시작하여 수백만 번의 셀프 플레이(self-play)를 통해 학습했다. 이 과정에서 알파고 제로는 인간이 수천 년간 쌓아온 정석을 스스로 재발견하고, 심지어 새로운 전략까지 창조해내며 이전 알파고를 압도했다. 이는 강화학습이 인간의 지식을 넘어설 수 있는 잠재력을 가졌음을 증명한 사례다.50</p>
</li>
<li>
<p><strong>알파스타 (AlphaStar)와 OpenAI Five</strong>: 강화학습은 바둑과 같은 턴제 보드게임을 넘어, 훨씬 더 복잡한 실시간 전략(RTS) 게임과 멀티플레이어 온라인 배틀 아레나(MOBA) 게임으로 확장되었다. 딥마인드의 <strong>알파스타</strong>는 스타크래프트 II에서, OpenAI의 <strong>OpenAI Five</strong>는 도타 2에서 세계 최상위권 프로게이머들을 상대로 승리했다. 이 게임들은 불완전한 정보(전장의 안개), 수천 개의 유닛과 건물을 제어해야 하는 거대한 행동 공간, 그리고 수십 분에 걸친 장기적인 전략 수립 등 바둑보다 훨씬 더 어려운 과제를 포함한다.3 이러한 성공은 강화학습이 매우 복잡하고 동적인 환경에서도 인간 수준을 뛰어넘는 전략적 의사결정 능력을 학습할 수 있음을 보여주었다.</p>
</li>
</ul>
<h3>8.2. 로보틱스 제어와 자율 주행</h3>
<p>강화학습은 가상 세계를 넘어 물리적 세계와 상호작용하는 로보틱스 및 자율 시스템 분야에서 혁신을 주도하고 있다.3</p>
<ul>
<li>
<p><strong>로봇 제어</strong>: 로봇이 걷고, 달리고, 물건을 집고, 조립하는 등의 복잡한 동작을 학습시키는 데 강화학습이 활발히 사용된다. 예를 들어, 딥마인드는 에이전트가 아무런 사전 지식 없이 다양한 지형에서 걷는 법을 스스로 학습하는 가상 환경을 공개했다.47 보스턴 다이내믹스의 로봇 ’아틀라스’가 보여주는 놀라운 균형 감각과 운동 능력에도 강화학습의 원리가 깊숙이 관여하고 있다.50 로보틱스 분야의 주요 과제 중 하나는 <strong>‘Sim-to-Real’</strong> 문제, 즉 시뮬레이션에서 학습한 정책을 실제 로봇으로 성공적으로 이전하는 것이다. 시뮬레이션은 데이터를 저렴하고 안전하게 대량으로 수집할 수 있는 장점이 있지만, 실제 세계와의 미세한 차이(reality gap)를 극복해야 하는 어려움이 있다.</p>
</li>
<li>
<p><strong>자율 주행</strong>: 자율 주행은 주변 차량, 보행자, 신호등 등 수많은 동적 요소와 상호작용하며 최적의 결정을 내려야 하는 대표적인 순차적 의사결정 문제다. 강화학습은 특히 차선 변경, 교차로 통행, 주차와 같은 국소적인 제어 문제에 적용되고 있다. 예를 들어, MATLAB에서 제공하는 자동 주차 예제는 PPO 알고리즘을 사용하여, 주어진 환경 정보(빈 주차 공간, 다른 차량 위치)를 바탕으로 정밀한 주차 기동을 수행하는 에이전트를 훈련시킨다.5</p>
</li>
</ul>
<h3>8.3. 추천 시스템, 금융, 그리고 거대 언어 모델</h3>
<p>강화학습의 응용 범위는 물리적 제어를 넘어 디지털 상호작용과 고도의 지적 작업으로 확장되고 있다.</p>
<ul>
<li>
<p><strong>추천 시스템</strong>: 기존의 추천 시스템은 사용자의 과거 클릭이나 구매 기록을 바탕으로 즉각적인 예측 정확도를 높이는 데 초점을 맞추었다. 반면, 강화학습 기반 추천 시스템은 사용자와의 상호작용을 장기적인 관점에서 모델링한다. 추천을 하나의 ’행동’으로, 사용자의 반응(클릭, 시청 시간 등)을 ’보상’으로 간주하여, 단기적인 클릭률뿐만 아니라 장기적인 사용자 만족도와 참여도(engagement)를 극대화하는 추천 ’정책’을 학습한다.48</p>
</li>
<li>
<p><strong>금융</strong>: 금융 시장은 변동성이 크고 예측이 어려운 동적 환경이다. 강화학습은 이러한 환경에서 최적의 거래 전략을 수립하는 데 사용될 수 있다. JP모건의 <strong>LOXM</strong> 시스템은 강화학습을 이용해 대규모 주식 주문을 시장에 미치는 영향을 최소화하면서 최적의 가격으로 빠르게 실행하는 방법을 학습한다.50 이는 언제, 얼마나, 어떤 방식으로 주식을 사고팔지를 효과적으로 판단하여 거래 비용을 줄이고 손실을 최소화하는 데 기여한다.</p>
</li>
<li>
<p><strong>거대 언어 모델 (LLM)과 RLHF</strong>: 최근 인공지능 분야에서 가장 큰 주목을 받고 있는 거대 언어 모델(ChatGPT, Claude 등)의 발전에 강화학습이 결정적인 역할을 했다. **인간 피드백 기반 강화학습(RLHF)**은 LLM이 생성한 여러 답변에 대해 인간 평가자가 선호도를 매기면, 이 피드백을 보상 신호로 사용하여 모델을 미세 조정하는 기술이다.3 이 과정은 크게 세 단계로 이루어진다.</p>
<ol>
<li>
<p><strong>지도 학습 미세 조정</strong>: 인간이 작성한 고품질의 예시 답변으로 기본 LLM을 학습시킨다.</p>
</li>
<li>
<p><strong>보상 모델 학습</strong>: 인간 평가자들이 여러 답변 중 어떤 것을 더 선호하는지에 대한 데이터를 수집하여, 답변의 품질을 예측하는 ’보상 모델’을 학습시킨다.</p>
</li>
<li>
<p>강화학습 최적화: LLM을 ’에이전트’로, 답변 생성을 ’행동’으로 간주한다. LLM이 생성한 답변을 보상 모델이 평가하여 점수(보상)를 매기면, 이 보상을 최대화하는 방향으로 PPO와 같은 강화학습 알고리즘을 사용해 LLM을 최적화한다. RLHF는 LLM의 응답을 단순히 문법적으로 옳은 것을 넘어, 더 유용하고(helpful), 정직하며(honest), 무해하게(harmless) 만드는 데 핵심적인 역할을 한다.3</p>
</li>
</ol>
</li>
</ul>
<h2>IX. 강화학습의 도전 과제와 미래 전망</h2>
<p>강화학습은 알파고의 승리 이후 비약적인 발전을 거듭하며 다양한 분야에서 그 가능성을 입증했지만, 이론을 현실 세계의 복잡한 문제에 적용하기까지는 여전히 극복해야 할 중요한 도전 과제들이 남아있다. 이러한 한계들은 현재 강화학습 연구의 주요 방향을 제시하고 있으며, 이를 해결하려는 노력은 강화학습을 다른 인공지능 분야와 융합시키는 방향으로 나아가고 있다.</p>
<p>미래의 강화학습은 고립된 분야가 아닌, 다른 인공지능 기술과의 깊은 통합을 통해 발전할 것이다. 예를 들어, <strong>샘플 효율성</strong> 문제를 해결하기 위해 환경 모델을 학습하는 <strong>모델 기반 RL</strong>은 <strong>예측 모델링</strong> 및 <strong>지도학습</strong>과의 융합을 의미한다. <strong>보상 함수 설계</strong>의 어려움은 <strong>RLHF</strong>나 <strong>역강화학습</strong>과 같은 기법을 통해 <strong>자연어 처리</strong> 및 <strong>인간-컴퓨터 상호작용</strong> 분야와의 결합을 촉진한다. 새로운 과제에 대한 <strong>일반화</strong> 능력 부족은 <strong>전이 학습</strong> 및 <strong>메타 학습</strong>과의 연계를 필요로 한다. 이처럼 강화학습의 미래는 순수한 형태가 아닌, 다른 AI 패러다임의 강점을 활용하는 하이브리드 시스템에서 찾을 수 있다. 강화학습의 강력한 의사결정 능력과 지도학습의 패턴 인식 능력, 예측 시스템의 세계 모델링 능력, 그리고 자연어 처리의 인간 이해 능력이 시너지 효과를 낼 때, 비로소 강화학습은 더욱 복잡하고 광범위한 현실 세계의 문제들을 해결하는 핵심 동력이 될 것이다.</p>
<h3>9.1. 강화학습의 주요 한계</h3>
<ul>
<li>
<p><strong>샘플 효율성 (Sample Efficiency)</strong>: 대부분의 모델 프리 강화학습 알고리즘은 최적의 정책을 학습하기 위해 막대한 양의 데이터, 즉 환경과의 상호작용이 필요하다.35 시뮬레이션 환경에서는 수백만, 수천만 번의 에피소드를 실행할 수 있지만, 실제 로봇이나 드론, 혹은 비용이 많이 드는 산업 공정에 이를 직접 적용하는 것은 시간과 비용 측면에서 비현실적이다. 이는 강화학습의 실세계 적용을 가로막는 가장 큰 장벽 중 하나다.51</p>
</li>
<li>
<p><strong>탐험-활용 딜레마 (Exploration vs. Exploitation Dilemma)</strong>: 강화학습의 근본적인 딜레마다. 에이전트는 현재까지의 경험을 바탕으로 최선이라고 알려진 행동을 선택하여 보상을 ’활용’해야 하는 동시에, 더 나은 보상을 가져다줄지도 모르는 미지의 행동을 시도하며 ’탐험’해야 한다.2 탐험이 부족하면 차선의 정책에 머무를 위험이 있고, 활용이 부족하면 이미 알고 있는 좋은 정책을 충분히 이용하지 못해 낮은 성능을 보일 수 있다. 이 둘 사이의 적절한 균형을 맞추는 것은 여전히 어려운 문제다.35</p>
</li>
<li>
<p><strong>안정성 및 재현성 (Stability and Reproducibility)</strong>: 심층 강화학습 알고리즘은 학습률, 감가율, 신경망 구조 등 수많은 하이퍼파라미터에 매우 민감하다. 이 하이퍼파라미터를 어떻게 설정하느냐에 따라 학습이 안정적으로 수렴하기도 하고, 완전히 실패하기도 한다.51 심지어 동일한 알고리즘과 하이퍼파라미터를 사용하더라도, 초기 가중치의 미세한 차이나 무작위성의 영향으로 결과가 크게 달라질 수 있어 연구의 재현성을 확보하기가 어렵다.</p>
</li>
<li>
<p><strong>크레딧 할당 문제 (Credit Assignment Problem)</strong>: 강화학습에서는 행동의 결과가 즉시 나타나지 않고, 한참 뒤에 보상으로 주어지는 경우가 많다. 예를 들어, 체스 게임에서 초반에 둔 하나의 수가 수십 수 뒤의 승패를 결정할 수 있다. 이처럼 긴 행동 시퀀스 끝에 받은 최종적인 보상(또는 벌점)이 어떤 특정 행동 덕분(또는 때문)이었는지를 파악하는 것은 매우 어렵다. 이를 크레딧 할당 문제라고 하며, 지연된 보상이 클수록 문제는 더욱 심각해진다.35</p>
</li>
<li>
<p><strong>환경의 불확실성 및 부분적 관찰성</strong>: 실제 세계는 예측 불가능한 외부 요인으로 가득 차 있으며, 같은 행동이라도 다른 결과를 낳을 수 있다(환경의 변동성).35 또한, 에이전트는 대부분의 경우 환경의 전체 상태를 완벽하게 관찰할 수 없다. 예를 들어, 자율주행차는 센서가 감지하는 범위 내의 정보만을 얻을 수 있을 뿐, 보이지 않는 곳의 상황까지 알 수는 없다. 이러한 **부분적 관찰성(Partial Observability)**은 문제를 MDP가 아닌, 더 어려운 POMDP(Partially Observable Markov Decision Process)로 만들며, 에이전트는 불완전한 정보를 바탕으로 최적의 결정을 내려야 하는 더 큰 도전에 직면하게 된다.35</p>
</li>
</ul>
<h3>9.2. 미래 연구 방향 및 전망</h3>
<p>이러한 한계들을 극복하기 위한 연구가 활발히 진행 중이며, 이는 강화학습의 미래를 형성하는 중요한 동력이 되고 있다.</p>
<ul>
<li>
<p><strong>모델 기반 강화학습의 부상</strong>: 낮은 샘플 효율성 문제를 해결하기 위한 가장 유망한 접근법 중 하나는 모델 기반 RL이다. 에이전트가 환경의 동역학 모델을 학습하고, 이 가상 모델 내에서 계획(planning)을 수행함으로써 실제 환경과의 상호작용 횟수를 크게 줄일 수 있다.35 정확하고 효율적인 세계 모델(world model)을 구축하는 연구는 앞으로 더욱 중요해질 것이다.</p>
</li>
<li>
<p><strong>오프라인 강화학습 (Offline RL)</strong>: 실제 시스템과의 상호작용이 위험하거나 비용이 많이 드는 경우(예: 의료, 발전소 제어)를 위해, 미리 수집된 고정된 데이터셋만으로 정책을 학습하는 오프라인 RL(배치 RL이라고도 함) 연구가 주목받고 있다. 이는 새로운 데이터를 수집하지 않고 기존 로그 데이터를 활용하여 안전하게 정책을 평가하고 개선할 수 있는 길을 열어준다.</p>
</li>
<li>
<p><strong>일반화와 전이 학습 (Generalization and Transfer Learning)</strong>: 현재의 강화학습 에이전트는 훈련된 특정 작업에는 뛰어나지만, 약간만 다른 새로운 환경이나 작업에는 잘 적응하지 못하는 경향이 있다. 하나의 작업에서 학습한 지식이나 기술을 다른 관련 작업으로 이전하는 <strong>전이 학습</strong>이나, 다양한 작업에 빠르게 적응하는 법을 배우는 **메타 학습(Meta-Learning)**은 에이전트의 일반화 능력을 향상시키는 핵심 연구 분야다.35</p>
</li>
<li>
<p><strong>다중 에이전트 강화학습 (Multi-Agent RL, MARL)</strong>: 현실 세계의 많은 문제들은 여러 에이전트가 서로 협력하거나 경쟁하는 상황을 포함한다. 자율주행차 군집, 로봇 축구팀, 경제 시스템 모델링 등이 그 예다. 단일 에이전트 환경보다 훨씬 복잡한 다중 에이전트 환경에서의 조정, 협력, 경쟁 문제를 다루는 MARL은 앞으로 그 중요성이 더욱 커질 것이다.35</p>
</li>
</ul>
<p>결론적으로, 강화학습은 여전히 많은 도전 과제를 안고 있지만, 그 해결을 위한 노력 속에서 끊임없이 발전하고 있다. 데이터의 양과 질이 지속적으로 증가하고, 컴퓨팅 파워가 향상됨에 따라 더 정교하고 강력한 모델의 학습이 가능해지고 있다.52 불확실성이 높고 정답이 명확하지 않은 복잡한 실제 세계의 문제를 해결하는 데 있어, 최적의 해결책을 탐색적으로 찾아내는 강화학습의 고유한 특성은 미래 사회의 다양한 영역에서 핵심적인 역할을 수행할 것으로 전망된다.3</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>강화학습 개요 - Introduction of Reinforcement Learning - CoolJune’s 기록물 저장소 - 티스토리, 9월 7, 2025에 액세스, https://skidrow6122.tistory.com/3</li>
<li>강화 학습이란 무엇인가요? - IBM, 9월 7, 2025에 액세스, https://www.ibm.com/kr-ko/think/topics/reinforcement-learning</li>
<li>강화학습을 활용한 Applications 소개, 9월 7, 2025에 액세스, <a href="https://medium.com/@hugmanskj/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-applications-%EC%86%8C%EA%B0%9C-300a382bf584">https://medium.com/@hugmanskj/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-applications-%EC%86%8C%EA%B0%9C-300a382bf584</a></li>
<li>강화학습 - 나무위키, 9월 7, 2025에 액세스, <a href="https://namu.wiki/w/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5">https://namu.wiki/w/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5</a></li>
<li>강화학습(Reinforcement Learning) - Dayeon Yu, 9월 7, 2025에 액세스, https://udayeon.github.io/2022/01/14/reinforcement-learning/</li>
<li>[ML] - 지도학습, 비지도학습, 강화학습 (Supervised vs Unsupervised vs Reinforcement Learning), 9월 7, 2025에 액세스, <a href="https://velog.io/@deep-of-machine/AI-%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5-%EB%B9%84%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5-%EA%B0%95%ED%99%94-%ED%95%99%EC%8A%B5-Supervised-vs-Unsupervised-vs-Reinforcement-Learning">https://velog.io/@deep-of-machine/AI-%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5-%EB%B9%84%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5-%EA%B0%95%ED%99%94-%ED%95%99%EC%8A%B5-Supervised-vs-Unsupervised-vs-Reinforcement-Learning</a></li>
<li>[AI] 지도학습, 비지도학습, 강화학습(RL) 개념과 차이점 - 방구의 개발냄새, 9월 7, 2025에 액세스, https://bangu4.tistory.com/96</li>
<li>[인공지능] 지도학습, 비지도학습, 강화학습 - 삶은 확률의 구름, 9월 7, 2025에 액세스, https://ebbnflow.tistory.com/165</li>
<li>[머신러닝] - 지도학습? 비지도학습? 강화학습? - 주식투자는 알파스퀘어와 함께, 9월 7, 2025에 액세스, https://alphasquare.tistory.com/34</li>
<li>[Machine Learning] 지도 학습 vs 비지도 학습 vs 강화 학습 - 인사이티드의 데이터 맛보기, 9월 7, 2025에 액세스, https://insighted-h.tistory.com/3</li>
<li>단단한 강화학습 Chapter3_(1) _유한 마르코프 결정 과정(Finite Markov DecisionProcesses), 9월 7, 2025에 액세스, https://roboharco12.tistory.com/56</li>
<li>Markov Decision Process (1) - 개요 - YJJo - 티스토리, 9월 7, 2025에 액세스, https://yjjo.tistory.com/23</li>
<li>[강화학습] 마르코프 결정 과정(MDP) - 마인드스케일, 9월 7, 2025에 액세스, https://www.mindscale.kr/docs/reinforcement-learning/mdp</li>
<li>[RL] 마르코프 결정 프로세스 (Markov Decision Process) - velog, 9월 7, 2025에 액세스, https://velog.io/@hkun_ho/RL-Markov-Decision-Process</li>
<li>마르코프 결정 프로세스, MDP (Markov Decision Process) - 도리의 디지털라이프, 9월 7, 2025에 액세스, <a href="https://blog.skby.net/%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84-%EA%B2%B0%EC%A0%95-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4-mdp-markov-decision-process/">https://blog.skby.net/%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84-%EA%B2%B0%EC%A0%95-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4-mdp-markov-decision-process/</a></li>
<li>ch 3. 벨만 방정식 - 기타등등 - 티스토리, 9월 7, 2025에 액세스, https://myetc.tistory.com/36</li>
<li>벨만 방정식 - 성장通 - 티스토리, 9월 7, 2025에 액세스, https://kevin-rain.tistory.com/78</li>
<li>벨만 방정식 - Bellman Equation and Optimality - CoolJune’s 기록물 저장소 - 티스토리, 9월 7, 2025에 액세스, https://skidrow6122.tistory.com/5</li>
<li>Chapter 9. Policy Gradients - JunHan’s AI Factory - 티스토리, 9월 7, 2025에 액세스, https://junhan-ai.tistory.com/83</li>
<li>[강화학습] 심층 결정론적 정책 경사(DDPG) - 마인드스케일, 9월 7, 2025에 액세스, https://www.mindscale.kr/docs/reinforcement-learning/ddpg</li>
<li>Proximal Policy Optimization (PPO) Agent - MATLAB &amp; Simulink - MathWorks, 9월 7, 2025에 액세스, https://www.mathworks.com/help/reinforcement-learning/ug/proximal-policy-optimization-agents.html</li>
<li>강화학습 알고리즘의 종류(분류) - DACON, 9월 7, 2025에 액세스, https://dacon.io/forum/406104</li>
<li>모델결합 학습 - Integrating Learning and Planning - CoolJune’s 기록물 저장소 - 티스토리, 9월 7, 2025에 액세스, https://skidrow6122.tistory.com/13</li>
<li>Model-based Reinforcement Learning - 삶은 확률의 구름 - 티스토리, 9월 7, 2025에 액세스, https://ebbnflow.tistory.com/343</li>
<li>모델 기반 강화 학습 vs. 모델 프리 강화 학습 - Neuroscience Study, 9월 7, 2025에 액세스, https://createtheworld.tistory.com/8</li>
<li>RL 알고리즘의 종류 : Model-Free vs Model-Based - deeep - 티스토리, 9월 7, 2025에 액세스, https://dalpo0814.tistory.com/52</li>
<li>정책 경사법(Policy Gradient Methods)으로 강화 학습 이해하기 -, 9월 7, 2025에 액세스, <a href="https://datacodelab.co.kr/%EC%A0%95%EC%B1%85-%EA%B2%BD%EC%82%AC%EB%B2%95%EC%9C%BC%EB%A1%9C-%EA%B0%95%ED%99%94-%ED%95%99%EC%8A%B5-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0/">https://datacodelab.co.kr/%EC%A0%95%EC%B1%85-%EA%B2%BD%EC%82%AC%EB%B2%95%EC%9C%BC%EB%A1%9C-%EA%B0%95%ED%99%94-%ED%95%99%EC%8A%B5-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0/</a></li>
<li>[강화학습 기초] SARSA 와 Q-Learning(큐러닝) - AI - 티스토리, 9월 7, 2025에 액세스, https://vstylestdy.tistory.com/58</li>
<li>강화 학습 기본 - 시간차 학습(Temporal-Difference Learning) part 1. 살사(SARSA), 9월 7, 2025에 액세스, https://wnthqmffhrm.tistory.com/10</li>
<li>[강화학습] 시간차 학습(Temporal-Difference Learning) - 마인드스케일, 9월 7, 2025에 액세스, https://www.mindscale.kr/docs/reinforcement-learning/td</li>
<li>Divergence in Deep Q-Learning: Tips and Tricks - Aman, 9월 7, 2025에 액세스, https://amanhussain.com/post/divergence-deep-q-learning/</li>
<li>The Deep Q-Learning Algorithm - Hugging Face Deep RL Course, 9월 7, 2025에 액세스, https://huggingface.co/learn/deep-rl-course/unit3/deep-q-algorithm</li>
<li>Deep Q-Network (DQN)-II | Towards Data Science, 9월 7, 2025에 액세스, https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c/</li>
<li>[강화학습] Q-러닝 - 마인드스케일, 9월 7, 2025에 액세스, https://www.mindscale.kr/docs/reinforcement-learning/q-learning</li>
<li>[강화학습] 강화학습의 어려움 - 마인드스케일, 9월 7, 2025에 액세스, https://www.mindscale.kr/docs/reinforcement-learning/challenges</li>
<li>Reinforcement Learning (DQN) Tutorial - PyTorch documentation, 9월 7, 2025에 액세스, https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html</li>
<li>deep learning - Why is a target network required? - Stack Overflow, 9월 7, 2025에 액세스, https://stackoverflow.com/questions/54237327/why-is-a-target-network-required</li>
<li>DQN — Stable Baselines3 2.7.1a0 documentation - Read the Docs, 9월 7, 2025에 액세스, https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html</li>
<li>Deterministic Policy Gradient Algorithms - Proceedings of Machine Learning Research, 9월 7, 2025에 액세스, https://proceedings.mlr.press/v32/silver14.pdf</li>
<li>Policy Gradient Algorithms | Lil’Log, 9월 7, 2025에 액세스, https://lilianweng.github.io/posts/2018-04-08-policy-gradient/</li>
<li>Deriving Policy Gradients and Implementing REINFORCE | by Chris Yoon | Medium, 9월 7, 2025에 액세스, https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63</li>
<li>REINFORCE Algorithm - GeeksforGeeks, 9월 7, 2025에 액세스, https://www.geeksforgeeks.org/machine-learning/reinforce-algorithm/</li>
<li>Off-Policy Proximal Policy Optimization, 9월 7, 2025에 액세스, https://ojs.aaai.org/index.php/AAAI/article/view/26099/25871</li>
<li>Proximal policy optimization - Wikipedia, 9월 7, 2025에 액세스, https://en.wikipedia.org/wiki/Proximal_policy_optimization</li>
<li>Introducing the Clipped Surrogate Objective Function - Hugging …, 9월 7, 2025에 액세스, https://huggingface.co/learn/deep-rl-course/unit8/clipped-surrogate-objective</li>
<li>Proximal Policy Optimization Algorithms | by Eleventh Hour Enthusiast - Medium, 9월 7, 2025에 액세스, https://medium.com/@EleventhHourEnthusiast/proximal-policy-optimization-algorithms-8b8e6596c713</li>
<li>보상을 통해 학습하는 머신러닝 기술 2편 - LG CNS, 9월 7, 2025에 액세스, https://www.lgcns.com/blog/cns-tech/ai-data/2746/</li>
<li>데이터를 기반으로 새로운 경험을 선사하는 추천팀 이야기 - Kakao Tech, 9월 7, 2025에 액세스, https://tech.kakao.com/posts/393</li>
<li>“알파고도 칠전팔기?” [특별기획 AI 2030] ③ 강화학습 - AI타임스, 9월 7, 2025에 액세스, https://www.aitimes.com/news/articleView.html?idxno=136181</li>
<li>RL : Reinforcement Learning 사례 - 월곡동 로봇팔의 대학원일지 - 티스토리, 9월 7, 2025에 액세스, <a href="https://mambo-coding-note.tistory.com/entry/RL-Reinforcement-Learning-%EC%82%AC%EB%A1%80">https://mambo-coding-note.tistory.com/entry/RL-Reinforcement-Learning-%EC%82%AC%EB%A1%80</a></li>
<li>강화학습의 한계 - Deep Campus - 티스토리, 9월 7, 2025에 액세스, https://pasus.tistory.com/37</li>
<li>[IT-인공지능] 생성형 AI의 모든 것: 기술, 활용, 한계와 극복방안, 그리고 미래 전망, 9월 7, 2025에 액세스, <a href="https://newitlec.com/entry/IT-%ED%8A%B8%EB%9E%9C%EB%93%9C-%EC%83%9D%EC%84%B1%ED%98%95-AI%EC%9D%98-%EB%AA%A8%EB%93%A0-%EA%B2%83-%EA%B8%B0%EC%88%A0-%ED%99%9C%EC%9A%A9-%ED%95%9C%EA%B3%84%EC%99%80-%EA%B7%B9%EB%B3%B5%EB%B0%A9%EC%95%88-%EA%B7%B8%EB%A6%AC%EA%B3%A0-%EB%AF%B8%EB%9E%98-%EC%A0%84%EB%A7%9D">https://newitlec.com/entry/IT-%ED%8A%B8%EB%9E%9C%EB%93%9C-%EC%83%9D%EC%84%B1%ED%98%95-AI%EC%9D%98-%EB%AA%A8%EB%93%A0-%EA%B2%83-%EA%B8%B0%EC%88%A0-%ED%99%9C%EC%9A%A9-%ED%95%9C%EA%B3%84%EC%99%80-%EA%B7%B9%EB%B3%B5%EB%B0%A9%EC%95%88-%EA%B7%B8%EB%A6%AC%EA%B3%A0-%EB%AF%B8%EB%9E%98-%EC%A0%84%EB%A7%9D</a></li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>