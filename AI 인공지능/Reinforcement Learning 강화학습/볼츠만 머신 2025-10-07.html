<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:볼츠만 머신 (2025-10-07)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>볼츠만 머신 (2025-10-07)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">강화 학습 (Reinforcement Learning)</a> / <span>볼츠만 머신 (2025-10-07)</span></nav>
                </div>
            </header>
            <article>
                <h1>볼츠만 머신 (2025-10-07)</h1>
<h2>1.  에너지 기반 모델과 볼츠만 머신</h2>
<h3>1.1  생성 모델(Generative Model)의 개념과 목표</h3>
<p>기계 학습 모델은 크게 판별 모델(Discriminative Model)과 생성 모델(Generative Model)로 나뉜다. 판별 모델은 입력 데이터 x가 주어졌을 때, 해당 데이터의 레이블 y가 무엇일지에 대한 조건부 확률 <span class="math math-inline">P(y \vert x)</span>를 학습하는 데 초점을 맞춘다.1 분류(classification)나 회귀(regression)와 같이 입력과 출력 사이의 관계를 찾는 대부분의 지도 학습 문제가 여기에 해당한다.</p>
<p>반면, 생성 모델은 데이터 자체의 근원적인 확률 분포 <span class="math math-inline">P(x)</span>를 학습하는 것을 목표로 한다.1 이는 단순히 데이터를 분류하는 것을 넘어, 데이터가 어떻게 생성되는지에 대한 깊은 이해를 추구한다. 생성 모델의 궁극적인 목적은 학습된 확률 분포로부터 현실 세계의 데이터와 유사한 새로운 데이터를 샘플링(sampling)하는 것이다.2 이러한 능력은 데이터 생성, 이상 탐지(anomaly detection), 결측치 보완(missing value imputation) 등 광범위한 응용의 기반이 된다.</p>
<h3>1.2  에너지 기반 모델(Energy-Based Model, EBM)의 원리</h3>
<p>에너지 기반 모델(EBM)은 통계 물리학(statistical mechanics)의 원리에서 깊은 영감을 받은 프레임워크다.4 물리계에서 시스템은 에너지가 낮은 안정적인 상태를 선호하는 경향이 있는데, EBM은 이 개념을 데이터 모델링에 차용한다.</p>
<p>EBM은 각 데이터 샘플 x에 대해 스칼라 값인 ‘에너지’ <span class="math math-inline">E(x)</span>를 할당하는 에너지 함수를 정의한다. 여기서 에너지는 해당 데이터 샘플의 적합성이나 안정성을 나타내는 척도로, 에너지가 낮을수록 그 데이터가 발생할 확률이 높고, 에너지가 높을수록 발생할 확률이 낮다고 해석한다.7 이처럼 EBM의 핵심 철학은 데이터의 확률 분포를 직접 모델링하는 대신, 분포를 암시하는 ’에너지 지형(energy landscape)’을 학습하는 것이다. 학습 과정은 마치 물리계가 안정적인 상태를 찾아가듯, 실제 데이터 포인트에 해당하는 지역의 에너지를 낮추고(안정화) 그 외의 공간의 에너지를 높이는(불안정화) 직관적인 과정으로 재해석될 수 있다.</p>
<p>이렇게 정의된 에너지 값은 볼츠만 분포(Boltzmann distribution)를 통해 확률로 변환된다. 특정 데이터 x가 나타날 확률 <span class="math math-inline">P(x)</span>는 정규화되지 않은 확률 <span class="math math-inline">e^{-E(x)}</span>를 분배 함수(partition function) Z로 나누어 계산된다.7</p>
<h3>1.3  볼츠만 머신의 정의와 의의</h3>
<p>볼츠만 머신(Boltzmann Machine)은 1985년 제프리 힌튼(Geoffrey Hinton)과 테렌스 세즈노스키(Terrence Sejnowski)에 의해 제안된 초기 신경망 모델로, 앞서 설명한 통계 물리학의 개념을 선구적으로 도입했다.8 볼츠만 머신은 확률적(stochastic) 뉴런을 기반으로 하는 에너지 기반 생성 모델(probabilistic, generative, energy-based model)로 정의할 수 있다.4 각 뉴런(유닛)이 결정론적으로 값을 출력하는 대신 확률적으로 켜지거나(1) 꺼지는(0) 특징을 가진다.</p>
<p>볼츠만 머신은 딥러닝의 역사에서 중요한 이정표 역할을 했다. 특히, 심층 신경망의 학습이 어려웠던 시기에 심층 구조의 사전 훈련(pre-training)이라는 새로운 패러다임을 열었으며, 에너지 기반 모델링의 가능성을 제시한 선구적인 모델로 평가받는다.4 볼츠만 머신은 이후 등장한 변이형 오토인코더(VAE)나 생성적 적대 신경망(GAN)과 같은 현대적인 생성 모델들이 해결하고자 했던 근본적인 문제, 즉 ’어떻게 복잡한 데이터 분포를 학습하고 그로부터 효율적으로 샘플링할 것인가’라는 질문을 신경망 프레임워크 내에서 처음으로 제기했다는 점에서 그 역사적 의의가 크다. 볼츠만 머신이 가진 계산적 한계는 역설적으로 이러한 새로운 모델들의 등장을 촉발하는 계기가 되었다.</p>
<h2>2.  볼츠만 머신 아키텍처와 확률 분포</h2>
<h3>2.1  네트워크 구조</h3>
<p>볼츠만 머신의 구조는 방향성이 없는 무방향 그래프(undirected graphical model)로, 마르코프 임의장(Markov Random Field, MRF)의 한 형태로 볼 수 있다.15 그 구조적 특징은 다음과 같다.</p>
<ul>
<li><strong>유닛(Units)</strong>: 네트워크는 두 종류의 유닛으로 구성된다. 외부로부터 입력을 받거나 외부로 상태를 보여주는 **가시 유닛(visible units, v)**과, 데이터의 내부적인 표현이나 잠재적 특징을 학습하는 **은닉 유닛(hidden units, h)**이다.10 모든 유닛은 일반적으로 0 또는 1의 이진(binary) 상태를 가진다.9</li>
<li><strong>연결(Connections)</strong>: 일반적인 볼츠만 머신은 <strong>완전 연결(fully connected)</strong> 구조를 가진다. 이는 모든 유닛이 자기 자신을 제외한 다른 모든 유닛과 연결되어 있음을 의미한다. 따라서 가시 유닛과 은닉 유닛 간의 연결뿐만 아니라, 가시 유닛들 사이의 연결, 그리고 은닉 유닛들 사이의 연결도 모두 존재한다.11</li>
<li><strong>대칭성(Symmetry)</strong>: 유닛 i와 유닛 j를 연결하는 가중치 <span class="math math-inline">w_{ij}</span>는 대칭적(wij=wji)이다.18 이 대칭성은 모델이 에너지 함수를 정의하고 안정적인 상태로 수렴하는 데 중요한 역할을 한다.</li>
</ul>
<h3>2.2  에너지 함수(Energy Function)</h3>
<p>네트워크의 특정 전역 상태(global state), 즉 모든 가시 유닛과 은닉 유닛의 상태가 결정된 구성 <span class="math math-inline">(v, h)</span>에 대해 에너지 <span class="math math-inline">E(v, h)</span>가 정의된다. 이 에너지는 모든 유닛 쌍의 상호작용과 각 유닛의 개별적인 편향(bias)을 합산하여 계산된다. 이진 유닛 <span class="math math-inline">s_i \in {0, 1}</span>을 사용하는 일반적인 볼츠만 머신의 에너지 함수는 다음과 같이 표현할 수 있다.23<br />
<span class="math math-display">
E(v, h) = - \sum_{i,j} v_i h_j w_{ij} - \sum_{i,k} v_i v_k u_{ik} - \sum_{j,l} h_j h_l r_{jl} - \sum_i a_i v_i - \sum_j b_j h_j
</span><br />
여기서 wij, uik, <span class="math math-inline">r_{jl}</span>은 각각 가시-은닉, 가시-가시, 은닉-은닉 유닛 간의 연결 가중치이며, ai와 bj는 각각 가시 유닛과 은닉 유닛의 편향이다. 이 에너지 함수는 모델의 파라미터(가중치와 편향)에 대해 선형적인 관계를 가지며, 이는 학습 규칙을 유도하는 데 중요한 수학적 특성이 된다.24</p>
<h3>2.3  볼츠만 분포와 분배 함수</h3>
<p>네트워크가 특정 상태 <span class="math math-inline">(v, h)</span>를 가질 확률 <span class="math math-inline">P(v, h)</span>는 해당 상태의 에너지 <span class="math math-inline">E(v, h)</span>에 지수적으로 반비례하는 볼츠만 분포(또는 깁스 분포)에 의해 결정된다.6<br />
<span class="math math-display">
P(v, h) = \frac{e^{-E(v, h)}}{Z}
</span><br />
여기서 분모에 위치한 Z는 **분배 함수(Partition Function)**라 불리며, 이는 모든 가능한 상태 <span class="math math-inline">(v&#39;, h&#39;)</span>에 대한 <span class="math math-inline">e^{-E(v&#39;, h&#39;)}</span>의 총합이다.10<br />
<span class="math math-display">
Z = \sum_{v&#39;, h&#39;} e^{-E(v&#39;, h&#39;)}
</span><br />
분배 함수 Z는 모든 상태의 확률을 합했을 때 1이 되도록 만드는 정규화 상수(normalizing constant) 역할을 한다. 그러나 이 Z를 계산하는 것은 볼츠만 머신 계열 모델이 직면하는 가장 근본적인 **계산적 난제(Computational Challenge)**다. 네트워크의 유닛 수가 N개일 때, 가능한 모든 상태의 수는 2N개가 된다. N이 조금만 커져도 이 모든 상태에 대해 합산을 수행하는 것은 지수적인 계산 시간으로 인해 사실상 불가능(intractable)해진다.12</p>
<p>이러한 분배 함수의 계산 불가능성은 단순히 하나의 기술적 장애물이 아니라, 볼츠만 머신 연구의 전체 방향을 결정한 핵심 동인이었다. Z를 직접 계산하는 것을 피하기 위한 노력은 결국 근사적인 학습 알고리즘인 대조 발산(Contrastive Divergence)의 개발과, 구조적으로 문제를 단순화한 제한된 볼츠만 머신(RBM)의 탄생으로 이어졌다.</p>
<p><strong>표 1: 에너지 함수 구성 요소</strong></p>
<table><thead><tr><th>기호</th><th>명칭</th><th>설명</th></tr></thead><tbody>
<tr><td>vi</td><td>가시 유닛 i의 상태</td><td>관찰된 데이터의 i번째 요소를 나타내는 이진 변수. vi∈{0,1}</td></tr>
<tr><td>hj</td><td>은닉 유닛 j의 상태</td><td>데이터의 잠재적 특징을 나타내는 이진 변수. hj∈{0,1}</td></tr>
<tr><td>wij</td><td>가중치 (Weight)</td><td>가시 유닛 i와 은닉 유닛 j 사이의 상호작용 강도를 나타내는 대칭적 파라미터. wij=wji</td></tr>
<tr><td>uik</td><td>가중치 (Weight)</td><td>가시 유닛 i와 k 사이의 상호작용 강도를 나타내는 파라미터 (일반 BM에만 존재).</td></tr>
<tr><td>rjl</td><td>가중치 (Weight)</td><td>은닉 유닛 j와 l 사이의 상호작용 강도를 나타내는 파라미터 (일반 BM에만 존재).</td></tr>
<tr><td>ai</td><td>편향 (Bias)</td><td>가시 유닛 i의 활성화 경향성을 조절하는 파라미터.</td></tr>
<tr><td>bj</td><td>편향 (Bias)</td><td>은닉 유닛 j의 활성화 경향성을 조절하는 파라미터.</td></tr>
</tbody></table>
<h2>3.  볼츠만 머신 학습 알고리즘</h2>
<h3>3.1  학습 목표: 로그 우도(Log-Likelihood) 최대화</h3>
<p>볼츠만 머신의 학습 목표는 주어진 훈련 데이터셋 D가 모델의 확률 분포 <span class="math math-inline">P(v)</span>로부터 생성되었을 확률, 즉 우도(likelihood)를 최대화하는 것이다.15 이는 모델이 실제 데이터의 분포를 가장 잘 모사하도록 파라미터를 조정하는 것을 의미한다. 계산의 편의를 위해 직접적인 우도 대신 로그 우도(log-likelihood) <span class="math math-inline">L = \sum_{v \in D} \log P(v)</span>를 최대화하는 방식을 사용하며, 이는 일반적으로 경사 상승법(gradient ascent)을 통해 수행된다.15</p>
<p>수학적으로 로그 우도를 최대화하는 것은 데이터의 실제 분포 <span class="math math-inline">P_{\text{data}}</span>와 모델이 학습한 분포 Pmodel 사이의 쿨백-라이블러 발산(Kullback-Leibler Divergence, KL-Divergence) <span class="math math-inline">KL(P_{\text{data}} \vert\vert P_{\text{model}})</span>을 최소화하는 것과 동일하다.15 KL 발산은 두 확률 분포 사이의 차이를 측정하는 척도로, 이 값이 0에 가까워질수록 모델이 데이터 분포를 더 정확하게 모사하고 있음을 의미한다.</p>
<h3>3.2  그래디언트 유도: 긍정 단계와 부정 단계</h3>
<p>로그 우도를 가중치 <span class="math math-inline">w_{ij}</span>에 대해 편미분하여 그래디언트를 구하면, 놀랍도록 간단하고 직관적인 형태의 수식을 얻을 수 있다.24<br />
<span class="math math-display">
\frac{\partial \log P(v)}{\partial w_{ij}} = \langle s_i s_j \rangle_{\text{data}} - \langle s_i s_j \rangle_{\text{model}}
</span><br />
이 수식은 두 부분의 차이로 구성되며, 각각 **긍정 단계(Positive Phase)**와 **부정 단계(Negative Phase)**에 해당한다.</p>
<ul>
<li><strong>긍정 단계</strong>: 첫 번째 항 <span class="math math-inline">\langle s_i s_j \rangle_{\text{data}}</span>는 ‘데이터 주도(data-driven)’ 통계량이다. 이는 훈련 데이터 v를 가시 유닛에 고정(clamping)시킨 상태에서 네트워크가 열 평형(thermal equilibrium)에 도달했을 때, 유닛 i와 j가 동시에 활성화될 확률의 기댓값을 의미한다.15 이 단계는 실제 데이터가 보여주는 유닛 간의 긍정적 상관관계를 학습하여 해당 데이터의 에너지를 낮추는(즉, 확률을 높이는) 역할을 한다.12</li>
<li><strong>부정 단계</strong>: 두 번째 항 <span class="math math-inline">\langle s_i s_j \rangle_{\text{model}}</span>은 ‘모델 주도(model-driven)’ 통계량이다. 이는 외부 입력 없이 모델이 자유롭게 작동하여 열 평형에 도달했을 때의 기댓값이다.15 이 단계는 모델이 현재 자체적으로 생성해내는 허상(fantasy) 상태들의 에너지를 높여(즉, 확률을 낮춰), 모델의 분포가 데이터 분포와 무관한 방향으로 발산하는 것을 억제하는 역할을 한다.24</li>
</ul>
<p>결과적으로 가중치 업데이트 규칙 <span class="math math-inline">\Delta w_{ij} \propto \langle s_i s_j \rangle_{\text{data}} - \langle s_i s_j \rangle_{\text{model}}</span>은, 모델이 실제 데이터(현실)를 보았을 때의 내부 상태와 아무것도 보지 않고 스스로 생성한 상태(꿈) 사이의 간극을 줄이려는 직관적인 과정으로 해석될 수 있다. 이는 모델이 데이터 분포를 내재화하도록 유도하는 강력한 메커니즘이다.</p>
<h3>3.3  깁스 샘플링(Gibbs Sampling)과 열 평형</h3>
<p>긍정 단계와 부정 단계의 기댓값 ⟨⋅⟩을 정확하게 계산하는 것은 분배 함수 Z의 계산과 마찬가지로 모든 가능한 상태를 고려해야 하므로 불가능하다. 따라서 이 기댓값들을 근사하기 위해 마르코프 연쇄 몬테카를로(MCMC) 방법, 특히 **깁스 샘플링(Gibbs Sampling)**을 사용한다.22</p>
<p>깁스 샘플링은 다변수 확률 분포에서 직접 샘플링하기 어려울 때, 각 변수를 나머지 모든 변수가 현재 값으로 고정된 조건부 확률 분포로부터 반복적으로 샘플링하는 기법이다.16 볼츠만 머신에서는 한 번에 하나의 유닛을 무작위로 선택하고, 나머지 유닛들의 상태가 주어진 조건부 확률 <span class="math math-inline">P(s_i \vert s_{\neg i})</span>에 따라 그 유닛의 상태를 0 또는 1로 업데이트한다. 이 과정을 충분히 반복하면, 네트워크의 전체 상태는 초기 상태와 무관하게 모델이 정의하는 볼츠만 분포를 따르게 되는데, 이 상태를 <strong>열 평형(thermal equilibrium)</strong> 상태라고 한다.15</p>
<h3>3.4  일반 볼츠만 머신의 학습상 한계</h3>
<p>깁스 샘플링은 볼츠만 머신 학습의 이론적 토대를 제공하지만, 동시에 실용적 적용을 가로막는 가장 큰 아킬레스건으로 작용한다. 열 평형 상태에 도달하는 데 필요한 샘플링 횟수는 네트워크의 크기와 가중치의 크기에 따라 지수적으로 증가하는 경향이 있다.15 특히 외부 입력이 없는 부정 단계에서 무작위 상태로부터 시작하여 수렴하기까지는 매우 오랜 시간이 걸릴 수 있다. 이 샘플링 과정의 극심한 비효율성은 일반 볼츠만 머신의 실용적인 학습을 거의 불가능하게 만들며 15, 이러한 한계를 극복하기 위한 노력이 제한된 볼츠만 머신(RBM)과 대조 발산(CD) 알고리즘의 등장을 필연적으로 만들었다.</p>
<h2>4.  제한된 볼츠만 머신 (Restricted Boltzmann Machine, RBM)</h2>
<h3>4.1  구조적 제약: 층 내 연결 제거와 이분 그래프</h3>
<p>제한된 볼츠만 머신(RBM)은 일반 볼츠만 머신의 학습 비효율성 문제를 해결하기 위해 구조에 강력한 제약을 가한 모델이다.15</p>
<p>가장 핵심적인 제약은 <strong>층 내 연결(intra-layer connections)의 완전한 제거</strong>이다. 즉, 가시 유닛들 사이의 연결과 은닉 유닛들 사이의 연결이 모두 존재하지 않는다.15 오직 가시 유닛과 은닉 유닛 사이의 연결만 허용되며, 이로 인해 네트워크는 두 개의 분리된 노드 집합 사이의 연결만 존재하는 <strong>이분 그래프(bipartite graph)</strong> 구조를 갖게 된다.10 이러한 구조적 특성 때문에 초기에는 ’하모니움(Harmonium)’이라고 불리기도 했다.32</p>
<p>이러한 제약으로 인해 RBM의 에너지 함수는 일반 볼츠만 머신의 에너지 함수에서 층 내 상호작용 항들이 제거된 더 간단한 형태를 띤다.23<br />
<span class="math math-display">
E(v, h) = - \sum_{i,j} v_i h_j w_{ij} - \sum_i a_i v_i - \sum_j b_j h_j = -h^T W v - a^T v - b^T h
</span></p>
<h3>4.2  조건부 독립성(Conditional Independence)</h3>
<p>RBM의 이분 그래프 구조는 계산적으로 매우 중요한 통계적 속성인 **조건부 독립성(Conditional Independence)**을 보장한다.12</p>
<ul>
<li>
<p><strong>원리</strong>: 가시 유닛들의 상태 v가 주어지면(관찰되면), 모든 은닉 유닛 hj들은 서로 조건부적으로 독립이 된다. 마찬가지로, 은닉 유닛들의 상태 h가 주어지면, 모든 가시 유닛 vi들 또한 서로 조건부 독립이 된다.31 이는 한 은닉 유닛의 상태가 다른 은닉 유닛의 상태에 직접적인 영향을 받지 않고 오직 가시 유닛 전체에만 의존하기 때문이다.</p>
</li>
<li>
<p><strong>수학적 표현</strong>: 이 조건부 독립성 덕분에, 한 층 전체의 조건부 확률 분포가 각 개별 유닛의 조건부 확률의 곱으로 분해(factorize)될 수 있다.31<br />
<span class="math math-display">
P(h | v) = \prod_j P(h_j | v) \quad \text{and} \quad P(v | h) = \prod_i P(v_i | h)
</span><br />
더 나아가, 각 개별 유닛의 활성화 확률은 로지스틱 시그모이드 함수 <span class="math math-inline">\sigma(x) = 1 / (1 + e^{-x})</span>를 사용하여 간단하게 계산할 수 있다.12<br />
<span class="math math-display">
P(h_j=1 | v) = \sigma\left(b_j + \sum_i v_i w_{ij}\right)
</span></p>
<p><span class="math math-display">
P(v_i=1 | h) = \sigma\left(a_i + \sum_j h_j w_{ij}\right)
</span></p>
</li>
</ul>
<h3>4.3  학습 효율성 증대</h3>
<p>조건부 독립성은 RBM의 학습 과정을 획기적으로 효율화한다. 일반 볼츠만 머신에서 깁스 샘플링을 수행할 때 한 번에 하나의 유닛씩 순차적으로 상태를 업데이트해야 했던 것과 달리, RBM에서는 <strong>한 번에 한 층(layer) 전체를 병렬적으로 업데이트</strong>할 수 있다.44 예를 들어, 가시층 v가 주어지면 모든 은닉 유닛 hj의 활성화 확률을 동시에 계산하고 각각 독립적으로 샘플링할 수 있다. 이러한 ‘블록 깁스 샘플링(Block Gibbs Sampling)’ 방식은 계산적으로 매우 효율적이다.50</p>
<p>RBM의 구조적 ’제약’은 정보의 흐름에 의도적인 병목(bottleneck)을 만들어, 은닉층이 가시 유닛들 간의 복잡한 상관관계를 포착하는 ’압축된 표현(compressed representation)’을 학습하도록 강제하는 효과를 낳는다. 가시 유닛 간의 직접적인 연결이 없기 때문에, 두 가시 유닛 vi와 vk 사이의 상관관계는 반드시 은닉층을 통해서만(vi→h→vk) 모델링될 수 있다. 이는 정보가 은닉층을 통과하며 압축되고 재구성되는 과정에서, 은닉층이 자연스럽게 데이터의 핵심적인 잠재 요인(latent factors) 또는 특징 탐지기(feature detectors)의 역할을 수행하도록 유도한다.3 이 ’강제된 추상화’가 바로 RBM의 강력한 특징 학습 능력의 근원이다.</p>
<p>이러한 계산적 효율성과 특징 학습 능력 덕분에, RBM은 대조 발산(CD)과 같은 근사 학습 알고리즘을 실용적으로 만들어 딥러닝의 핵심 구성 요소로 자리 잡을 수 있었다.15</p>
<p><strong>표 2: 볼츠만 머신(BM)과 제한된 볼츠만 머신(RBM) 비교</strong></p>
<table><thead><tr><th>특징</th><th>일반 볼츠만 머신 (General BM)</th><th>제한된 볼츠만 머신 (RBM)</th></tr></thead><tbody>
<tr><td><strong>구조</strong></td><td>완전 연결 그래프</td><td>이분 그래프 (Bipartite Graph)</td></tr>
<tr><td><strong>층 내 연결</strong></td><td>존재 (가시-가시, 은닉-은닉)</td><td>없음</td></tr>
<tr><td><strong>조건부 독립성</strong></td><td>성립하지 않음</td><td><strong>성립</strong>: P(h∣v)=∏jP(hj∣v), P(v∣h)=∏iP(vi∣h)</td></tr>
<tr><td><strong>깁스 샘플링</strong></td><td>유닛 단위 순차적 샘플링 (매우 느림)</td><td>층 단위 병렬적 샘플링 (Block Gibbs, 매우 빠름)</td></tr>
<tr><td><strong>추론 P(h∣v)</strong></td><td>MCMC 샘플링 필요 (근사적, 느림)</td><td>한 번의 행렬 곱셈과 시그모이드 연산으로 계산 (정확, 빠름)</td></tr>
<tr><td><strong>주요 학습 알고리즘</strong></td><td>이론상 로그 우도 최대화 (실제로는 거의 불가능)</td><td>대조 발산 (Contrastive Divergence) (근사적, 실용적)</td></tr>
<tr><td><strong>실용성</strong></td><td>이론적 모델, 실용적 적용 어려움</td><td>딥러닝의 구성 요소로 널리 활용됨</td></tr>
</tbody></table>
<h2>5.  대조 발산 (Contrastive Divergence, CD-k)</h2>
<h3>5.1  알고리즘의 원리: 로그 우도 그래디언트의 근사</h3>
<p>대조 발산(Contrastive Divergence, CD)은 제프리 힌튼이 RBM을 효율적으로 학습시키기 위해 제안한 핵심적인 알고리즘이다.35 CD는 로그 우도 그래디언트의 계산이 어려운 부정 단계 항 <span class="math math-inline">\langle s_i s_j \rangle_{\text{model}}</span>을 정확히 계산하는 대신, 짧은 단계의 깁스 샘플링을 통해 이를 근사한다.19</p>
<p>CD의 핵심 아이디어는 마르코프 연쇄 몬테카를로(MCMC) 체인을 무작위 상태에서 시작하여 수렴시킬 때까지 기다리는 대신, <strong>훈련 데이터 v0에서 시작</strong>하는 것이다. 훈련 데이터는 이미 우리가 모델링하려는 목표 분포에서 추출된 샘플이므로, 이 지점에서 MCMC 체인을 시작하면 훨씬 빠르게 평형 분포 근처에 도달할 것이라는 가정에 기반한다.35</p>
<p>CD-k 알고리즘은 훈련 데이터에서 시작하여 k번의 깁스 샘플링 단계를 거친 후 얻은 샘플을 사용하여 부정 단계 통계량을 계산한다. 놀랍게도 실제 많은 응용에서는 단 한 번의 샘플링 단계만 거치는 <strong>CD-1</strong>만으로도 충분히 좋은 성능을 보이는 것으로 알려져 있다.35</p>
<h3>5.2  CD-k 알고리즘의 단계별 절차</h3>
<p>하나의 훈련 샘플 <span class="math math-inline">v^{(0)}</span>에 대한 CD-k 알고리즘의 절차는 다음과 같이 긍정 단계와 부정 단계로 구성된다.32</p>
<ol>
<li><strong>긍정 단계 (Positive Phase)</strong>:</li>
</ol>
<ul>
<li>입력 데이터 <span class="math math-inline">v^{(0)}</span>를 RBM의 가시층에 고정한다.</li>
<li>조건부 확률 <span class="math math-inline">P(h \vert v^{(0)})</span>를 계산하여 은닉층의 활성화 벡터 <span class="math math-inline">h^{(0)}</span>를 샘플링한다.</li>
<li>긍정 통계량(positive statistics)인 외적(outer product) <span class="math math-inline">v^{(0)} \otimes h^{(0)}</span>를 계산한다.</li>
</ul>
<ol>
<li><strong>부정 단계 (Negative Phase) - 깁스 샘플링 <span class="math math-inline">k</span>회 반복</strong>:</li>
</ol>
<ul>
<li><span class="math math-inline">t=0</span>부터 <span class="math math-inline">k−1</span>까지 다음 과정을 반복한다:<br />
a. 이전 단계에서 얻은 은닉 벡터 <span class="math math-inline">h^{(t)}</span>를 사용하여, 조건부 확률 <span class="math math-inline">P(v \vert h^{(t)})</span>로부터 가시층의 재구성(reconstruction) 벡터 <span class="math math-inline">v^{(t+1)}</span>을 샘플링한다.<br />
b. 새롭게 재구성된 가시 벡터 <span class="math math-inline">v^{(t+1)}</span>을 사용하여, 조건부 확률 <span class="math math-inline">P(h \vert v^{(t+1)})</span>로부터 새로운 은닉 벡터 <span class="math math-inline">h^{(t+1)}</span>을 샘플링한다.</li>
</ul>
<ol>
<li><span class="math math-inline">k</span>번의 반복 후 최종적으로 얻은 재구성된 가시 벡터 <span class="math math-inline">v^{(k)}</span>와 은닉 벡터 <span class="math math-inline">h^{(k)}</span>를 사용하여 부정 통계량(negative statistics) <span class="math math-inline">v^{(k)} \otimes h^{(k)}</span>를 계산한다.</li>
</ol>
<h3>5.3  가중치 및 편향 업데이트 규칙</h3>
<p>CD-k를 통해 얻은 긍정 통계량과 부정 통계량의 차이를 이용하여 모델의 파라미터(가중치와 편향)를 업데이트한다. 가중치 <span class="math math-inline">w_{ij}</span>의 업데이트 규칙은 다음과 같다.32<br />
<span class="math math-display">
\Delta w_{ij} = \eta \left( (v_i^{(0)} h_j^{(0)}) - (v_i^{(k)} h_j^{(k)}) \right)
</span><br />
여기서 <span class="math math-inline">\eta</span>는 학습률(learning rate)이다. 이 식은 직관적으로 “실제 데이터가 유도하는 상관관계“와 “모델이 생성한 데이터(허상)가 유도하는 상관관계“의 차이를 줄이는 방향으로 가중치를 조정하는 것을 의미한다.58</p>
<p>편향 <span class="math math-inline">a_i</span>와 <span class="math math-inline">b_j</span>에 대한 업데이트 규칙도 유사하게 유도된다.32<br />
<span class="math math-display">
\Delta a_i = \eta \left( v_i^{(0)} - v_i^{(k)} \right)
</span></p>
<p><span class="math math-display">
\Delta b_j = \eta \left( h_j^{(0)} - h_j^{(k)} \right)
</span></p>
<p>CD-k는 이론적으로 편향된(biased) 그래디언트 추정치를 사용하지만, 실제로는 매우 효과적으로 작동한다. 학습 초기에는 모델이 데이터 분포에서 멀리 떨어져 있어 그래디언트가 크고 올바른 방향으로 빠르게 학습을 진행시키고, 학습이 진행되어 모델이 데이터 분포에 가까워지면 긍정 항과 부정 항이 유사해져 그래디언트가 자연스럽게 작아지며 안정되는 일종의 ‘자기 조절(self-regulating)’ 특성을 보인다.35</p>
<h2>6.  심층 구조: 심층 신뢰 신경망(DBN)과 심층 볼츠만 머신(DBM)</h2>
<p>RBM의 가장 중요한 의의 중 하나는 딥러닝의 구성 요소(building block)로서의 역할이다. RBM을 여러 층으로 쌓아 올림으로써, 초창기 딥러닝의 발전을 이끈 두 가지 중요한 심층 아키텍처, 즉 심층 신뢰 신경망(DBN)과 심층 볼츠만 머신(DBM)이 탄생했다.</p>
<h3>6.1  심층 신뢰 신경망(Deep Belief Network, DBN)</h3>
<p>심층 신뢰 신경망(DBN)은 여러 개의 RBM을 <strong>쌓아서(stacking)</strong> 만든 심층 생성 모델이다.15 그 구조는 한 RBM의 은닉층이 그 위에 쌓이는 다음 RBM의 가시층이 되는 계층적 형태를 띤다.15</p>
<p>DBN은 독특한 하이브리드 구조를 가진다. 최상위 두 개 층은 RBM과 같이 무방향(undirected) 대칭 연결을 가지지만, 그 아래의 모든 층 간 연결은 상위 층에서 하위 층으로 향하는 방향성(directed)을 가진다. 이 구조 덕분에 DBN은 전체적으로 데이터의 분포를 학습하고 새로운 데이터를 생성할 수 있는 생성 모델로 작동한다.15</p>
<h3>6.2  탐욕적 층별 사전 훈련(Greedy Layer-wise Pre-training)</h3>
<p>DBN의 등장이 딥러닝 역사에서 혁신적이었던 이유는 바로 **탐욕적 층별 사전 훈련(Greedy Layer-wise Pre-training)**이라는 독창적인 학습 방법 때문이다. 2006년 당시, 여러 개의 은닉층을 가진 심층 신경망은 기울기 소실(vanishing gradient) 문제 등으로 인해 한 번에 학습시키기가 매우 어려웠다. DBN의 층별 사전 훈련은 이 문제를 해결하는 실마리를 제공했다.63</p>
<p>이 학습 방식은 복잡한 심층망 전체를 한 번에 최적화하는 어려운 문제를, 여러 개의 얕은 RBM을 순차적으로 학습시키는 더 쉬운 문제들로 분해하는 ‘분할 정복(divide and conquer)’ 전략이다. 학습 절차는 다음과 같다.63</p>
<ol>
<li>가장 아래의 RBM(입력층 v와 첫 번째 은닉층 h1)을 입력 데이터를 사용하여 비지도 방식으로 학습시킨다.</li>
<li>첫 번째 RBM의 학습이 완료되면, 그 파라미터를 고정(freeze)한다.</li>
<li>입력 데이터를 학습된 첫 번째 RBM에 통과시켜 얻은 은닉층의 활성화 값(또는 확률)을 다음 RBM의 입력 데이터로 사용한다.</li>
<li>두 번째 RBM(h1을 가시층, h2를 은닉층으로 가짐)을 이 새로운 데이터로 학습시킨다.</li>
<li>원하는 수의 층이 쌓일 때까지 이 과정을 반복한다.</li>
</ol>
<p>이러한 층별 학습 과정은 네트워크가 데이터의 계층적 특징을 자연스럽게 학습하도록 유도한다. 낮은 층의 RBM은 엣지나 질감과 같은 저수준 특징을 학습하고, 높은 층으로 갈수록 이러한 저수준 특징들의 조합으로 구성된 더 복잡하고 추상적인 특징(예: 객체의 부분, 전체 객체)을 학습하게 된다.71</p>
<p>사전 훈련이 완료된 후, 전체 네트워크의 가중치는 데이터의 구조를 잘 반영하는 좋은 초기값으로 설정된다. 이후 분류나 회귀와 같은 특정 지도 학습 과제를 수행하기 위해 네트워크의 최상단에 출력층을 추가하고, 역전파(backpropagation) 알고리즘을 사용하여 전체 네트워크의 가중치를 미세 조정(fine-tuning)할 수 있다.32</p>
<h3>6.3  심층 볼츠만 머신(Deep Boltzmann Machine, DBM)</h3>
<p>심층 볼츠만 머신(DBM) 역시 여러 개의 은닉층을 가진 심층 구조이지만, DBN과는 결정적인 구조적 차이를 보인다. DBM의 <strong>모든 인접한 층 간의 연결은 DBN과 달리 무방향(undirected)이고 대칭적</strong>이다.15 이는 DBM이 DBN과 같은 하이브리드 모델이 아니라, 전체가 하나의 거대한 무방향 그래프 모델(마르코프 임의장)임을 의미한다.</p>
<p>이 완전한 양방향 구조는 DBM이 정보 처리에 있어 DBN보다 더 유연한 잠재력을 갖게 한다. DBN이 주로 데이터를 입력받아 고수준 특징으로 변환하는 상향식(bottom-up) 특징 추출기로 작동하는 반면, DBM은 상향식 인식(recognition) 과정과 하향식 생성(generation) 과정이 상호작용하는 통합된 추론 시스템으로 작동할 수 있다. 예를 들어, 상위층의 추상적 개념(예: ‘고양이’)이 하위층의 구체적 특징(예: ‘뾰족한 귀’)의 인식을 조절하는 하향식 피드백을 자연스럽게 모델링할 수 있다.15</p>
<p>그러나 이러한 유연성은 큰 대가를 치른다. DBM의 학습은 DBN처럼 간단한 층별 사전 훈련 방식이 직접적으로 적용되지 않으며, 전체 층을 동시에 고려하는 더 복잡한 근사 추론(approximate inference) 과정이 필요하여 계산 비용이 훨씬 높고 학습이 더 어렵다.15</p>
<h2>7.  볼츠만 머신의 응용</h2>
<p>볼츠만 머신, 특히 RBM은 그 독특한 구조와 학습 방식 덕분에 다양한 분야에서 성공적으로 응용되었다.</p>
<h3>7.1  협업 필터링(Collaborative Filtering)</h3>
<p>RBM이 가장 큰 성공을 거둔 분야 중 하나는 추천 시스템, 특히 협업 필터링이다. 가장 유명한 사례는 2000년대 후반에 열린 <strong>넷플릭스 프라이즈(Netflix Prize)</strong> 경연이다.82</p>
<ul>
<li><strong>작동 원리</strong>: 협업 필터링 문제에서 RBM은 사용자의 영화 평점 패턴을 학습하는 데 사용된다. 예를 들어, 한 사용자가 평가한 영화들의 평점 목록을 가시 유닛 벡터로 입력한다. 이때 RBM의 은닉 유닛들은 사용자의 잠재적인 취향(예: ‘액션 영화 선호’, ‘로맨틱 코미디 선호’)이나 영화가 가진 잠재적 속성(예: ‘어두운 분위기’, ‘가족용’)과 같은 **잠재 요인(latent factors)**을 비지도 방식으로 학습하게 된다.39</li>
<li><strong>예측 및 추천</strong>: RBM이 학습을 마치면, 특정 사용자가 아직 평가하지 않은 영화에 대한 평점을 예측하는 데 사용될 수 있다. 이는 해당 사용자의 평점 데이터를 가시층에 입력하고, 활성화된 은닉 유닛(사용자의 취향)을 통해 평가하지 않은 영화에 해당하는 가시 유닛의 상태를 재구성(reconstruct)하는 방식으로 이루어진다.11</li>
</ul>
<p>넷플릭스 프라이즈에서 우승한 ‘BellKor’s Pragmatic Chaos’ 팀은 특이값 분해(SVD)와 RBM을 결합한 앙상블 모델을 사용하여 넷플릭스 자체 알고리즘의 예측 정확도를 10% 이상 향상시켰다. 이는 RBM이 희소하고(sparse) 이산적인(discrete) 평점 데이터를 다루는 데 매우 효과적임을 입증한 대표적인 사례다.83</p>
<h3>7.2  특징 학습(Feature Learning) 및 차원 축소</h3>
<p>RBM은 레이블이 없는 데이터로부터 유용한 특징을 추출하는 비지도 특징 학습에 탁월한 능력을 보인다.4</p>
<ul>
<li><strong>차원 축소</strong>: RBM의 은닉층 활성화 값은 원본 고차원 입력 데이터(예: 수천 개의 이미지 픽셀)의 저차원적이면서도 의미 있는 표현으로 간주될 수 있다. 이는 데이터를 단순히 압축하는 것을 넘어, 데이터의 본질적인 생성 요인을 발견하는 과정에 가깝다. 주성분 분석(PCA)과 같은 전통적인 선형 차원 축소 기법과 달리, RBM은 데이터의 비선형적이고 복잡한 구조를 포착할 수 있다.4</li>
<li><strong>사전 훈련</strong>: 이렇게 추출된 저차원 특징 벡터는 분류나 회귀와 같은 후속 지도 학습 과제의 입력으로 사용될 수 있다. RBM을 통해 데이터의 유용한 표현을 미리 학습하는 사전 훈련 단계를 거치면, 최종 지도 학습 모델의 성능을 크게 향상시킬 수 있다. 이는 DBN의 핵심 아이디어이기도 하다.52</li>
</ul>
<h3>7.3  생성적 응용</h3>
<p>RBM은 학습 데이터의 확률 분포를 모델링하므로, 이를 바탕으로 새로운 데이터를 생성하는 데 사용될 수 있다.4</p>
<ul>
<li><strong>이미지 복원 및 생성</strong>: 손상되거나 일부가 가려진 이미지를 RBM의 가시 유닛에 입력으로 제공하고 깁스 샘플링을 반복적으로 수행하면, 모델이 학습한 이미지의 통계적 특성에 기반하여 손상된 부분을 자연스럽게 채워 넣을 수 있다(inpainting).14 또한, 학습된 RBM에서 깁스 샘플링을 오랫동안 실행하면 훈련 데이터와 유사한 스타일의 새로운 이미지를 생성할 수도 있다. 이는 기계가 학습한 내용을 바탕으로 ’꿈을 꾸는 것’에 비유되기도 한다.46 DBN과 같은 심층 구조는 필기체 숫자와 같이 더 복잡하고 추상적인 데이터를 생성하는 데 사용될 수 있다.90</li>
<li><strong>분류</strong>: 생성 모델임에도 불구하고 RBM은 분류 작업에도 활용될 수 있다. 각 클래스별로 RBM을 학습시킨 후, 새로운 데이터가 들어왔을 때 어떤 클래스의 RBM이 해당 데이터를 가장 그럴듯하게(즉, 가장 낮은 에너지로) 재구성하는지를 기준으로 클래스를 판별하는 방식이다.14</li>
</ul>
<h2>8.  역사적 의의와 현재 위상</h2>
<h3>8.1  딥러닝 역사에서의 역할</h3>
<p>볼츠만 머신, 특히 RBM을 기반으로 한 DBN은 현대 딥러닝의 역사에서 빼놓을 수 없는 중요한 역할을 수행했다.</p>
<ul>
<li><strong>딥러닝의 부흥</strong>: 1990년대부터 2000년대 초반까지, 여러 개의 은닉층을 가진 심층 신경망은 기울기 소실 문제 등으로 인해 효과적으로 학습시키기 어렵다는 인식이 팽배하여 ’AI의 겨울’이라 불리는 침체기를 겪었다. 2006년, 제프리 힌튼이 제안한 DBN의 ‘탐욕적 층별 사전 훈련’ 방식은 이 문제를 우회하여 심층 구조의 학습 가능성을 실증적으로 보여주었고, 이는 딥러닝의 부흥을 이끈 결정적인 계기가 되었다.18 볼츠만 머신의 가장 큰 역사적 공헌은 특정 과제에서의 뛰어난 성능 그 자체가 아니라, 심층 학습에 대한 심리적, 기술적 장벽을 허물어뜨린 ’개념 증명(proof-of-concept)’으로서의 역할이었다.</li>
<li><strong>비지도 사전 훈련 패러다임</strong>: DBN의 성공은 레이블이 없는 대규모 데이터로부터 유용한 표현을 미리 학습하는 비지도 사전 훈련의 효과를 입증했다. 이는 이후 전이 학습(transfer learning)이나 자기 지도 학습(self-supervised learning)과 같은 현대 딥러닝의 핵심 아이디어로 이어진다.78</li>
<li><strong>에너지 기반 모델의 개척</strong>: 볼츠만 머신은 통계 물리학의 원리를 신경망에 성공적으로 접목하여, 확률 분포를 에너지 함수라는 새로운 관점에서 모델링하는 EBM 프레임워크를 개척했다. 이는 이후 다양한 고급 생성 모델의 이론적 기반을 제공했다.4</li>
</ul>
<h3>8.2  현대 생성 모델(VAE, GAN)과의 비교</h3>
<p>현대 생성 모델의 발전사는 볼츠만 머신이 제기한 ‘다루기 힘든 확률 분포(intractable probability distribution)’ 문제를 해결하기 위한 여정으로 볼 수 있다. RBM, VAE, GAN은 이 문제에 대해 각기 다른 철학적 해법을 제시한다.</p>
<ul>
<li><strong>제한된 볼츠만 머신 (RBM)</strong>: 명시적인 확률 밀도 함수 <span class="math math-inline">P(x)</span>를 정의하지만, 분배 함수 Z 때문에 직접 계산은 불가능하다. MCMC 기반의 근사(대조 발산)를 통해 학습하며, 샘플 생성 역시 반복적인 깁스 샘플링을 필요로 하여 느리다. 생성된 샘플의 품질이 VAE나 GAN에 비해 떨어지는 경향이 있다.8</li>
<li><strong>변이형 오토인코더 (VAE)</strong>: RBM과 마찬가지로 명시적인 확률 모델이지만, 다루기 힘든 사후 분포를 더 간단한 분포로 ’근사’하는 변분 추론(variational inference) 방식을 도입한다. 이를 통해 안정적인 학습이 가능하고, 잠재 공간의 구조를 잘 학습하여 데이터 보간(interpolation) 등에 유리하다. 다만, 생성된 이미지가 다소 흐릿하게(blurry) 나오는 단점이 있다.97</li>
<li><strong>생성적 적대 신경망 (GAN)</strong>: 확률 밀도를 명시적으로 모델링하는 것 자체를 포기하고, 생성자와 판별자의 제로섬 게임을 통해 문제를 ’우회’한다. 이 적대적 학습 방식은 매우 사실적이고 선명한 고품질 샘플을 생성할 수 있게 해주었지만, 학습 과정이 매우 불안정하고 생성되는 샘플의 다양성이 부족해지는 모드 붕괴(mode collapse) 문제가 발생하기 쉽다.30</li>
</ul>
<p><strong>표 3: 주요 생성 모델 비교: RBM, VAE, GAN</strong></p>
<table><thead><tr><th>기준</th><th>제한된 볼츠만 머신 (RBM)</th><th>변이형 오토인코더 (VAE)</th><th>생성적 적대 신경망 (GAN)</th></tr></thead><tbody>
<tr><td><strong>모델 종류</strong></td><td>에너지 기반 모델 (무방향 확률 그래프)</td><td>확률적 오토인코더 (방향성 확률 그래프)</td><td>암시적 생성 모델 (게임 이론 기반)</td></tr>
<tr><td><strong>확률 밀도</strong></td><td>명시적 (P(x) 정의), 계산 불가능</td><td>명시적 (<span class="math math-inline">P(x)</span>의 하한(ELBO) 계산 가능)</td><td>암시적 (P(x) 정의 안 함)</td></tr>
<tr><td><strong>학습 방식</strong></td><td>대조 발산 (MCMC 기반 근사)</td><td>변분 추론 (경사 하강법)</td><td>적대적 학습 (경사 하강법)</td></tr>
<tr><td><strong>학습 안정성</strong></td><td>비교적 안정적이나 수렴 속도 느림</td><td>안정적</td><td>불안정 (모드 붕괴, 기울기 소실/폭주)</td></tr>
<tr><td><strong>샘플 품질</strong></td><td>상대적으로 낮음, 노이즈가 많을 수 있음</td><td>다소 흐릿함(blurry)</td><td>매우 사실적이고 선명함</td></tr>
<tr><td><strong>잠재 공간</strong></td><td>해석 가능성 있으나 직접 제어 어려움</td><td>구조화됨 (연속적, 보간 가능)</td><td>구조화되지 않음 (제어 어려움)</td></tr>
<tr><td><strong>주요 장점</strong></td><td>특징 학습, 사전 훈련에 효과적</td><td>안정적인 학습, 잠재 공간 해석 용이</td><td>최고 수준의 샘플 품질</td></tr>
<tr><td><strong>주요 단점</strong></td><td>느린 학습/샘플링, 낮은 샘플 품질</td><td>흐릿한 결과물, ELBO 근사의 한계</td><td>학습 불안정, 모드 붕괴</td></tr>
</tbody></table>
<h3>8.3  볼츠만 머신의 한계와 현대적 재조명</h3>
<p>일반 볼츠만 머신의 학습은 사실상 불가능하며, RBM과 DBN 역시 학습 과정이 역전파보다 복잡하고 직관적이지 않다.15 또한, 성능 면에서 현대의 CNN, RNN, 트랜스포머 기반 모델에 의해 대체되어 현재는 대부분의 응용 분야에서 주류로 사용되지 않는다.4</p>
<p>하지만 볼츠만 머신의 근간이 된 ’에너지 기반 모델’이라는 아이디어 자체는 최근 다시 주목받고 있다. 이미지 생성 분야에서 최첨단 성능을 보이는 확산 모델(Diffusion Models)은 EBM 또는 점수 기반 모델(score-based models)의 관점에서 깊이 있게 해석될 수 있다. 이는 에너지 함수의 개념이 여전히 복잡한 확률 분포를 모델링하는 데 강력한 이론적 프레임워크임을 시사한다.8 또한, 양자 컴퓨팅의 발전과 함께 양자 어닐링(quantum annealing)을 이용한 양자 볼츠만 머신(Quantum Boltzmann Machine, QBM)과 같은 새로운 형태의 연구가 이어지며 그 명맥을 잇고 있다.96</p>
<p>결론적으로 볼츠만 머신은 딥러닝의 역사를 연 중요한 모델이지만, 현재는 그 자체로 사용되기보다는 그 안에 담긴 에너지 기반 모델링, 비지도 사전 훈련과 같은 핵심 아이디어들이 더 발전된 형태로 현대 딥러닝 기술에 녹아들어 그 유산을 이어가고 있다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>볼츠만 머신: 생성모형의 원리 - 고등과학원 HORIZON, https://horizon.kias.re.kr/18001/</li>
<li>Restricted Boltzmann Machine(RBM) - Aero-Machine Learning - 티스토리, <a href="https://metar.tistory.com/entry/Restricted-Boltzmann-MachineRBM-%EC%A0%9C%ED%95%9C%EB%90%9C-%EB%B3%BC%EC%B8%A0%EB%A7%8C-%EB%A8%B8%EC%8B%A0">https://metar.tistory.com/entry/Restricted-Boltzmann-MachineRBM-%EC%A0%9C%ED%95%9C%EB%90%9C-%EB%B3%BC%EC%B8%A0%EB%A7%8C-%EB%A8%B8%EC%8B%A0</a></li>
<li>Restricted Boltzmann Machine - 공돌이의 수학정리노트 (Angelo’s Math Notes), https://angeloyeo.github.io/2020/10/02/RBM.html</li>
<li>A Complete Guide to Boltzmann Machine — Deep Learning | by Hey Amit - Medium, https://medium.com/@heyamit10/a-complete-guide-to-boltzmann-machine-deep-learning-98f8eccfd506</li>
<li>idplab-konkuk.tistory.com, <a href="https://idplab-konkuk.tistory.com/14#:~:text=%EB%B3%BC%EC%B8%A0%EB%A7%8C%20%EB%A8%B8%EC%8B%A0%EC%9D%80%20%ED%86%B5%EA%B3%84%EB%AC%BC%EB%A6%AC%ED%95%99,%EB%A5%BC%20%EC%A0%95%EC%9D%98%ED%95%9C%20%EC%8B%A0%EA%B2%BD%20%EB%AA%A8%EB%8D%B8%EC%9D%B4%EB%8B%A4.">https://idplab-konkuk.tistory.com/14#:~:text=%EB%B3%BC%EC%B8%A0%EB%A7%8C%20%EB%A8%B8%EC%8B%A0%EC%9D%80%20%ED%86%B5%EA%B3%84%EB%AC%BC%EB%A6%AC%ED%95%99,%EB%A5%BC%20%EC%A0%95%EC%9D%98%ED%95%9C%20%EC%8B%A0%EA%B2%BD%20%EB%AA%A8%EB%8D%B8%EC%9D%B4%EB%8B%A4.</a></li>
<li>Beginner’s Guide to Boltzmann Machines in PyTorch | Paperspace Blog, https://blog.paperspace.com/beginners-guide-to-boltzmann-machines-pytorch/</li>
<li>11_energy_based_models - Jakub M. Tomczak, https://jmtomczak.github.io/blog/11/11_energy_based_models.html</li>
<li>Energy-Based &amp; Early Neural Generative Models: Boltzmann Machines &amp; Restricted Boltzmann Machines (RBMs) | by Jim Canary | Medium, https://medium.com/@jimcanary/energy-based-early-neural-generative-models-boltzmann-machines-restricted-boltzmann-machines-bdd61bc31a71</li>
<li>5.5.6 볼츠만 머신 - 뜻 지, 가르칠 훈, https://teach-meaning.tistory.com/452</li>
<li>[오일석 기계학습] 10.4 - 확률 그래피컬 모델 RBM과 DBN (1), https://biomadscientist.tistory.com/61</li>
<li>Udemy - 딥러닝의 모든 것(볼츠만 머신) - (1) - 상훈’s CANVAS, https://canvas4sh.tistory.com/281</li>
<li>A Derivation and Application of Restricted Boltzmann Machines (2024 Nobel Prize), https://towardsdatascience.com/a-derivation-and-application-of-restricted-boltzmann-machines-2024-nobel-prize-ead5fe66908c/</li>
<li>Energy-based model - Wikipedia, https://en.wikipedia.org/wiki/Energy-based_model</li>
<li>볼츠만 머신, Boltzmann machine | helpingstar, https://helpingstar.github.io/dl/other_network/</li>
<li>Boltzmann machine - Wikipedia, https://en.wikipedia.org/wiki/Boltzmann_machine</li>
<li>Training Restricted Boltzmann Machines: An Introduction⋆ - Christian Igel, https://christian-igel.github.io/paper/TRBMAI.pdf</li>
<li>Boltzmann machine - Scholarpedia, http://www.scholarpedia.org/article/Boltzmann_machine</li>
<li>인공신경망/종류 - 나무위키:대문, <a href="https://namu.wiki/w/%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D/%EC%A2%85%EB%A5%98">https://namu.wiki/w/%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D/%EC%A2%85%EB%A5%98</a></li>
<li>Tutorial: restricted Boltzmann machines, https://www.cs.toronto.edu/~tijmen/csc321/documents/maddison_rbmtutorial.pdf</li>
<li>Boltzmann Machine - Kaggle, https://www.kaggle.com/code/abedi756/boltzmann-machine</li>
<li>www.researchgate.net, <a href="https://www.researchgate.net/figure/Boltzmann-and-Restricted-Boltzmann-Machines-A-Boltzmann-machine-is-fully-connected_fig1_257649811#:~:text=A%20Boltzmann%20machine%20is%20fully,visible%20units%2C%20and%20vice%20versa.">https://www.researchgate.net/figure/Boltzmann-and-Restricted-Boltzmann-Machines-A-Boltzmann-machine-is-fully-connected_fig1_257649811#:~:text=A%20Boltzmann%20machine%20is%20fully,visible%20units%2C%20and%20vice%20versa.</a></li>
<li>볼츠만 머신 - 위키원, <a href="http://wiki.hash.kr/index.php/%EB%B3%BC%EC%B8%A0%EB%A7%8C_%EB%A8%B8%EC%8B%A0">http://wiki.hash.kr/index.php/%EB%B3%BC%EC%B8%A0%EB%A7%8C_%EB%A8%B8%EC%8B%A0</a></li>
<li>A Practical Guide to Training Restricted Boltzmann Machines, https://csrc.ac.cn/upload/file/20170703/1499052743888438.pdf</li>
<li>Lecture 12a The Boltzmann Machine learning algorithm, https://www.cs.toronto.edu/~hinton/coursera/lecture12/lec12.pdf</li>
<li>Derivation: Maximum Likelihood for Boltzmann Machines, https://theclevermachine.wordpress.com/2014/09/23/derivation-maximum-likelihood-for-boltzmann-machines/</li>
<li>Boltzmann distribution - Wikipedia, https://en.wikipedia.org/wiki/Boltzmann_distribution</li>
<li>Learning thermodynamics with Boltzmann machines | Phys. Rev. B, https://link.aps.org/doi/10.1103/PhysRevB.94.165134</li>
<li>papers.neurips.cc, http://papers.neurips.cc/paper/1412-boltzmann-machine-learning-using-mean-field-theory-and-linear-response-correction.pdf</li>
<li>Partition function (mathematics) - Wikipedia, https://en.wikipedia.org/wiki/Partition_function_(mathematics)</li>
<li>Energy-based model 설명 (EBM 설명) - 유니의 공부 - 티스토리, https://process-mining.tistory.com/215</li>
<li>Accelerate Training of Restricted Boltzmann Machines via Iterative Conditional Maximum Likelihood Estimation - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC8046342/</li>
<li>Restricted Boltzmann machine - Wikipedia, https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine</li>
<li>Resticted Boltzmann Machines, https://qucumber.readthedocs.io/en/stable/_static/RBM_tutorial.pdf</li>
<li>Boltzmann Machine, https://www.cs.jhu.edu/~ayuille/courses/Stat271-Fall15/BoltzmannMachine.pdf</li>
<li>On Contrastive Divergence Learning - Department of Computer Science, University of Toronto, https://www.cs.toronto.edu/~fritz/absps/cdmiguel.pdf</li>
<li>“Machine learning - Restricted Boltzmann Machines.”, https://jhui.github.io/2017/01/15/Machine-learning-Boltzmann-machines/</li>
<li>Bolzman machine - sampling - Cross Validated, https://stats.stackexchange.com/questions/309997/bolzman-machine-sampling</li>
<li>Lecture 18 Learning Boltzmann Machines, https://www.cs.toronto.edu/~hinton/csc321/notes/lec18.pdf</li>
<li>Restricted Boltzmann Machine Tutorial – Introduction to Deep Learning Concepts - Edureka, https://www.edureka.co/blog/restricted-boltzmann-machine-tutorial/</li>
<li>Gibbs Sampling for Training RBMs - Medium, https://medium.com/@heyamit10/gibbs-sampling-for-training-rbms-0021a59c876d</li>
<li>Contrastive Divergence in Restricted Boltzmann Machines …, https://www.geeksforgeeks.org/deep-learning/contrastive-divergence-in-restricted-boltzmann-machines/</li>
<li>Deep Boltzmann Machines - Department of Statistical Sciences, https://www.utstat.toronto.edu/~rsalakhu/papers/dbm.pdf</li>
<li>RBM-DBNetwork | Deep Learning, https://adioshun.gitbook.io/deep-learning/auto-encoder/rbm-db-network</li>
<li>Why is RBM more practical than a non-restricted Boltzmann Machine? - Quora, https://www.quora.com/Why-is-RBM-more-practical-than-a-non-restricted-Boltzmann-Machine</li>
<li>딥러닝 파이토치 교과서: 4.3.4 제한된 볼츠만 머신 - 더북(TheBook), https://thebook.io/080289/0190/</li>
<li>제한 볼츠만 머신 - 위키원, <a href="http://wiki.hash.kr/index.php/%EC%A0%9C%ED%95%9C_%EB%B3%BC%EC%B8%A0%EB%A7%8C_%EB%A8%B8%EC%8B%A0">http://wiki.hash.kr/index.php/%EC%A0%9C%ED%95%9C_%EB%B3%BC%EC%B8%A0%EB%A7%8C_%EB%A8%B8%EC%8B%A0</a></li>
<li>Restricted Boltzmann Machines, https://www.cs.toronto.edu/~hinton/csc2535/notes/lec4new.pdf</li>
<li>On the propriety of restricted Boltzmann machines, https://dr.lib.iastate.edu/server/api/core/bitstreams/fcddf545-bb15-4ecc-bdb7-11de5231cf06/content</li>
<li>Understanding Contrastive Divergence - Data Science Stack Exchange, https://datascience.stackexchange.com/questions/30186/understanding-contrastive-divergence</li>
<li>딥러닝 #3 생성모델 part 1 Boltzmann Machine - IDPLab - 티스토리, https://idplab-konkuk.tistory.com/14</li>
<li>3-5. 딥러닝의 알고리즘 III - 코딩의 시작, TCP School, https://tcpschool.com/deeplearning/deep_algorithm3</li>
<li>Restricted Boltzmann Machine : How it works - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/restricted-boltzmann-machine-how-it-works/</li>
<li>Understanding the Boltzmann Machine and It’s Applications - Great Learning, https://www.mygreatlearning.com/blog/understanding-boltzmann-machines/</li>
<li>Deep Learning using Restricted Boltzmann Machines - INTERNATIONAL JOURNAL OF COMPUTER SCIENCE AND INFORMATION TECHNOLOGIES, <a href="https://www.ijcsit.com/docs/Volume%207/vol7issue3/ijcsit20160703110.pdf">https://www.ijcsit.com/docs/Volume%207/vol7issue3/ijcsit20160703110.pdf</a></li>
<li>What is Contrastive Divergence | AI Basics - Ai Online Course, https://www.aionlinecourse.com/ai-basics/contrastive-divergence</li>
<li>Training Binary Restricted Boltzmann Machines with Contrastive Divergence - cs.wisc.edu, https://pages.cs.wisc.edu/~andrzeje/research/rbm.pdf</li>
<li>Restricted Boltzmann Machine, a complete analysis. Part 3: Contrastive Divergence algorithm | by Nguyễn Văn Lĩnh | datatype | Medium, https://medium.com/datatype/restricted-boltzmann-machine-a-complete-analysis-part-3-contrastive-divergence-algorithm-3d06bbebb10c</li>
<li>canvas4sh.tistory.com, <a href="https://canvas4sh.tistory.com/282#:~:text=%EB%8C%80%EC%A1%B0%EB%B0%9C%EC%82%B0%EC%9D%80%20%EC%9D%B4%EB%9F%AC%ED%95%9C%20%EB%B3%BC%EC%B8%A0%EB%A7%8C,%EC%9D%80%20%EB%91%90%20%EB%8B%A8%EA%B3%84%EB%A1%9C%20%EC%9D%B4%EB%A3%A8%EC%96%B4%EC%A7%84%EB%8B%A4.">https://canvas4sh.tistory.com/282#:~:text=%EB%8C%80%EC%A1%B0%EB%B0%9C%EC%82%B0%EC%9D%80%20%EC%9D%B4%EB%9F%AC%ED%95%9C%20%EB%B3%BC%EC%B8%A0%EB%A7%8C,%EC%9D%80%20%EB%91%90%20%EB%8B%A8%EA%B3%84%EB%A1%9C%20%EC%9D%B4%EB%A3%A8%EC%96%B4%EC%A7%84%EB%8B%A4.</a></li>
<li>Udemy - 딥러닝의 모든 것(볼츠만 머신) - (2) - 상훈’s CANVAS, https://canvas4sh.tistory.com/282</li>
<li>Restricted Boltzmann Machines - Understanding contrastive divergence vs. ML learning, https://stats.stackexchange.com/questions/301842/restricted-boltzmann-machines-understanding-contrastive-divergence-vs-ml-lear</li>
<li>Neural Network and Deep Belief Network | Baeldung on Computer Science, https://www.baeldung.com/cs/deep-belief-network</li>
<li>Deep Belief Networks (DBNs) explained - Viso Suite, https://viso.ai/deep-learning/deep-belief-networks/</li>
<li>An Overview of Deep Belief Network (DBN) in Deep Learning - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2022/03/an-overview-of-deep-belief-network-dbn-in-deep-learning/</li>
<li>초보자용 RBM(Restricted Boltzmann Machines) 튜토리얼 | by 안종찬 - Medium, <a href="https://medium.com/@ahnchan2/%EC%B4%88%EB%B3%B4%EC%9E%90%EC%9A%A9-rbm-restricted-boltzmann-machines-%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-791ce740a2f0">https://medium.com/@ahnchan2/%EC%B4%88%EB%B3%B4%EC%9E%90%EC%9A%A9-rbm-restricted-boltzmann-machines-%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-791ce740a2f0</a></li>
<li>Deep Learning Tutorial part 3/3: Deep Belief Networks - Lazy Programmer, https://lazyprogrammer.me/deep-learning-tutorial-part-33-deep-belief/</li>
<li>Deep Belief Nets - Department of Computer Science, University of Toronto, https://www.cs.toronto.edu/~hinton/nipstutorial/nipstut3.pdf</li>
<li>Deep belief networks or Deep Boltzmann Machines? - Stats StackExchange, https://stats.stackexchange.com/questions/95428/deep-belief-networks-or-deep-boltzmann-machines</li>
<li>Deep belief network - Wikipedia, https://en.wikipedia.org/wiki/Deep_belief_network</li>
<li>심층 신뢰 신경망 - 위키백과, 우리 모두의 백과사전, <a href="https://ko.wikipedia.org/wiki/%EC%8B%AC%EC%B8%B5_%EC%8B%A0%EB%A2%B0_%EC%8B%A0%EA%B2%BD%EB%A7%9D">https://ko.wikipedia.org/wiki/%EC%8B%AC%EC%B8%B5_%EC%8B%A0%EB%A2%B0_%EC%8B%A0%EA%B2%BD%EB%A7%9D</a></li>
<li>Greedy Layer-Wise Training of Deep Networks, https://proceedings.neurips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf</li>
<li>Greedy Layer-Wise Training of Deep Networks - NIPS, https://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks</li>
<li>A Study of Greedy Layer-wise Training on Deep Neural Networks - University of Illinois Urbana-Champaign, https://courses.grainger.illinois.edu/ece544na/fa2014/Tianqi_Gao.pdf</li>
<li>딥러닝 파이토치 교과서: 4.3.5 심층 신뢰 신경망 - 더북(TheBook), https://thebook.io/080289/0191/</li>
<li>심층신뢰망 (DBN, Deep Belief Network) - 도리의 디지털라이프, <a href="https://blog.skby.net/%EC%8B%AC%EC%B8%B5%EC%8B%A0%EB%A2%B0%EB%A7%9D-dbn-deep-belief-network/">https://blog.skby.net/%EC%8B%AC%EC%B8%B5%EC%8B%A0%EB%A2%B0%EB%A7%9D-dbn-deep-belief-network/</a></li>
<li>[Artificial Intelligence] Deep Learning Overview | 딥러닝 개요 - Archive, https://dad-rock.tistory.com/889</li>
<li>4장 딥러닝(2) - 딥러닝 알고리즘(심층 신경망, 합성곱 신경망, 순환 신경망, 제한된 볼츠만 머신, 심층 신뢰 신경망) - zz.log, https://markme-inur.tistory.com/32</li>
<li>Deep Belief Network (DBN) in Deep Learning - GeeksforGeeks, https://www.geeksforgeeks.org/deep-learning/deep-belief-network-dbn-in-deep-learning/</li>
<li>Representation Learning (1) — Greedy Layer-Wise Unsupervised Pretraining - Medium, https://medium.com/@andrehuang0/representation-learning-1-greedy-layer-wise-unsupervised-pretraining-de483ead2d0a</li>
<li>Boltzmann machine | Engati, https://www.engati.com/glossary/boltzmann-machine</li>
<li>Deep Belief Networks and Deep Boltzmann Machines (Unsupervised Learning, article #4) | by Justin Milner | Medium, https://medium.com/@justinmilner/deep-belief-networks-and-deep-boltzmann-machines-unsupervised-learning-article-4-e84bbf68f17a</li>
<li>A Better Way to Pretrain Deep Boltzmann Machines - CMU School of Computer Science, https://www.cs.cmu.edu/~rsalakhu/papers/DBM_pretrain.pdf</li>
<li>Netflix Prize - Wikipedia, https://en.wikipedia.org/wiki/Netflix_Prize</li>
<li>Netflix Prize and SVD, http://buzzard.ups.edu/courses/2014spring/420projects/math420-UPS-spring-2014-gower-netflix-SVD.pdf</li>
<li>Model Based Collaborative Filtering — SVD | by Cory Maklin - Medium, https://medium.com/@corymaklin/model-based-collaborative-filtering-svd-19859c764cee</li>
<li>Restricted Boltzmann Machine and Its Application | LatentView Analytics, https://www.latentview.com/blog/restricted-boltzmann-machine-and-its-application/</li>
<li>Netflix Recommendations: Beyond the 5 stars (Part 1), https://netflixtechblog.com/netflix-recommendations-beyond-the-5-stars-part-1-55838468f429</li>
<li>A Beginner’s Guide to Restricted Boltzmann Machines (RBMs) | Pathmind, https://wiki.pathmind.com/restricted-boltzmann-machine</li>
<li>Restricted Boltzmann Machines (RBMs) - Saturn Cloud, https://saturncloud.io/glossary/restricted-boltzmann-machines-rbms/</li>
<li>Restricted Boltzmann Machine - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/restricted-boltzmann-machine/</li>
<li>Deep Belief Network Definition | DeepAI, https://deepai.org/machine-learning-glossary-and-terms/deep-belief-network</li>
<li>DBN architecture for recognition and generation. The Visual Input Layer… - ResearchGate, https://www.researchgate.net/figure/DBN-architecture-for-recognition-and-generation-The-Visual-Input-Layer-was-split-into-a_fig5_257649811</li>
<li>Research on Information Visualization Graphic Design Teaching Based on DBN Algorithm, https://pmc.ncbi.nlm.nih.gov/articles/PMC8492239/</li>
<li>Nobel Lecture: Boltzmann machines | Rev. Mod. Phys. - Physical Review Link Manager, https://link.aps.org/doi/10.1103/RevModPhys.97.030502</li>
<li>How is “deep learning” different from previous work on neural networks? : r/compsci - Reddit, https://www.reddit.com/r/compsci/comments/2soosy/how_is_deep_learning_different_from_previous_work/</li>
<li>Why does Unsupervised Pre-training Help Deep Learning? - Google Research, https://research.google.com/pubs/archive/35536.pdf</li>
<li>Comparing Classical and Quantum Generative Learning Models for High-Fidelity Image Synthesis - MDPI, https://www.mdpi.com/2227-7080/11/6/183</li>
<li>(PDF) Advancements in VAEs and GANs - ResearchGate, https://www.researchgate.net/publication/388820889_Advancements_in_VAEs_and_GANs</li>
<li>The Evolution of Generative AI: GANs, VAEs, and Diffusion Models | by Shaswata Tripathy, https://medium.com/@tripathyshaswata/the-evolution-of-generative-ai-gans-vaes-and-diffusion-models-3a014c03aa1c</li>
<li>생성형 AI의 주요 기술(2) - VAE(Variational Autoencoder), <a href="https://agi-chatgpt.com/entry/%EC%83%9D%EC%84%B1%ED%98%95-AI%EC%9D%98-%EC%A3%BC%EC%9A%94-%EA%B8%B0%EC%88%A02-VAEVariational-Autoencoder">https://agi-chatgpt.com/entry/%EC%83%9D%EC%84%B1%ED%98%95-AI%EC%9D%98-%EC%A3%BC%EC%9A%94-%EA%B8%B0%EC%88%A02-VAEVariational-Autoencoder</a></li>
<li>VAE랑 GAN의 차이점? : r/learnmachinelearning - Reddit, https://www.reddit.com/r/learnmachinelearning/comments/w58wzg/difference_between_vae_and_gan/?tl=ko</li>
<li>Why are deep belief networks (DBN) rarely used? - Cross Validated - Stats StackExchange, https://stats.stackexchange.com/questions/261751/why-are-deep-belief-networks-dbn-rarely-used</li>
<li>[2203.15220] Comparing the effects of Boltzmann machines as associative memory in Generative Adversarial Networks between classical and quantum sampling - arXiv, https://arxiv.org/abs/2203.15220</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>