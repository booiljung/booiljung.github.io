<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:오프라인 강화학습 (Offline RL)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>오프라인 강화학습 (Offline RL)</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">강화 학습 (Reinforcement Learning)</a> / <a href="index.html">오프라인 강화학습 (Offline RL)</a> / <span>오프라인 강화학습 (Offline RL)</span></nav>
                </div>
            </header>
            <article>
                <h1>오프라인 강화학습 (Offline RL)</h1>
<p>2025-12-10, G30DR</p>
<h2>1.  서론: 데이터 중심 강화학습의 부상과 패러다임의 전환</h2>
<p>인공지능 연구의 역사에서 강화학습(Reinforcement Learning, RL)은 에이전트가 환경과의 상호작용을 통해 시행착오를 겪으며 스스로 최적의 행동 양식을 학습하는 독보적인 방법론으로 자리 잡았다. 알파고(AlphaGo)가 보여준 바둑에서의 초인적인 성능이나, 복잡한 로봇 제어 문제에서의 성과는 강화학습의 잠재력을 증명했다. 그러나 전통적인 강화학습, 즉 온라인(Online) 강화학습은 ’상호작용(Interaction)’이라는 필수 조건으로 인해 실제 산업 현장에 적용하는 데 있어 치명적인 병목 현상을 겪어왔다.</p>
<p>온라인 강화학습 에이전트는 학습 과정에서 수만 번, 많게는 수천만 번의 행동을 수행하고 그 결과를 관측해야 한다.1 시뮬레이션 환경에서는 이것이 시간과 연산 자원의 문제일 뿐이지만, 물리적인 현실 세계에서는 비용, 안전, 그리고 윤리의 문제로 직결된다. 자율 주행 자동차가 고속도로에서 학습을 위해 무작위로 핸들을 꺾거나, 의료 AI가 환자에게 검증되지 않은 약물을 투여하며 반응을 살피는 것은 불가능하다.3 또한, 로봇 팔이 물체를 집는 기술을 배우기 위해 수주 간 작동하며 기계적 마모를 일으키는 것 역시 비효율적이다.</p>
<p>이러한 배경에서 <strong>오프라인 강화학습(Offline Reinforcement Learning)</strong>, 또는 배치 강화학습(Batch RL)이 대두되었다. 오프라인 RL은 환경과의 추가적인 상호작용 없이, 이미 수집된 정적 데이터셋(Static Dataset) <span class="math math-inline">D</span> 만을 활용하여 최적의 정책을 학습하는 것을 목표로 한다.1 이는 지도 학습(Supervised Learning)이 거대한 데이터셋을 통해 비약적인 발전을 이룬 것처럼, 강화학습을 ’능동적 탐색’의 영역에서 ’데이터 기반 학습’의 영역으로 전환하려는 시도다. 이는 단순히 알고리즘의 변경을 넘어, 과거의 경험을 지능적인 의사결정 능력으로 승화시킨다는 측면에서 인공지능의 일반화(Generalization)를 위한 핵심 열쇠로 평가받는다.3</p>
<p>본 보고서는 오프라인 강화학습의 이론적 기초부터 시작하여, 분포 변화(Distribution Shift)라는 핵심 난제를 해결하기 위해 제안된 다양한 알고리즘(정책 제약, 가치 정규화, 인-샘플 학습, 시퀀스 모델링)을 심도 있게 분석한다. 또한 D4RL, RL Unplugged와 같은 벤치마크 생태계를 조망하고, 자율 주행, 헬스케어, 추천 시스템 등 실제 산업 분야에서의 구체적인 적용 사례와 성과를 기술한다. 마지막으로 Sim-to-Real 격차 해소와 파운데이션 모델(Foundation Models)과의 융합 등 최신 연구 동향을 통해 오프라인 RL의 미래를 전망한다.</p>
<h2>2.  이론적 배경 및 핵심 난제 분석</h2>
<h3>2.1  오프라인 RL과 오프-폴리시(Off-Policy) RL의 본질적 차이</h3>
<p>오프라인 RL을 이해하기 위해서는 기존의 오프-폴리시(Off-Policy) RL과의 관계를 명확히 정립해야 한다. 두 방법론 모두 현재 학습 중인 정책(Target Policy, <span class="math math-inline">\pi</span>)과 데이터를 수집한 정책(Behavior Policy, <span class="math math-inline">\mu</span> 또는 <span class="math math-inline">\beta</span>)이 다를 수 있다는 점을 공유하지만, 학습의 전제 조건과 목표에서 근본적인 차이가 있다.</p>
<table><thead><tr><th><strong>구분</strong></th><th><strong>오프-폴리시(Off-Policy) RL</strong></th><th><strong>오프라인(Offline) RL</strong></th></tr></thead><tbody>
<tr><td><strong>데이터 수집</strong></td><td>리플레이 버퍼를 사용하되, 주기적으로 환경과 상호작용하여 새로운 데이터 추가 1</td><td>고정된 정적 데이터셋(Static Dataset)만 사용. 추가 상호작용 불가 2</td></tr>
<tr><td><strong>목표</strong></td><td>탐색(Exploration)을 통해 점진적으로 정책을 개선</td><td>주어진 데이터 내에서 최적의 정책(Exploitation) 도출 및 일반화</td></tr>
<tr><td><strong>주요 난제</strong></td><td>샘플 효율성(Sample Efficiency), 탐색과 이용의 균형</td><td>분포 변화(Distribution Shift), 외삽 오차(Extrapolation Error)</td></tr>
<tr><td><strong>알고리즘 예시</strong></td><td>DQN, SAC, DDPG</td><td>BCQ, CQL, IQL, Decision Transformer</td></tr>
</tbody></table>
<p>오프-폴리시 알고리즘은 이론적으로 과거의 데이터를 재사용할 수 있지만, 실제로는 온라인 상호작용을 통해 수집된 최신 데이터가 버퍼에 지속적으로 공급되어야만 성능이 유지된다.1 반면, 오프라인 RL은 “더 이상의 데이터 수집은 없다“는 강력한 제약 조건 하에서 작동하며, 이는 기존 오프-폴리시 알고리즘을 그대로 적용했을 때 학습 붕괴를 초래하는 원인이 된다.3</p>
<h3>2.2  분포 변화(Distribution Shift)와 외삽 오차(Extrapolation Error)</h3>
<p>오프라인 RL이 직면하는 가장 치명적인 문제는 학습된 정책 <span class="math math-inline">\pi</span>와 데이터 수집 정책 <span class="math math-inline">\mu</span> 간의 분포 불일치, 즉 **분포 변화(Distribution Shift)**다.7 데이터셋 <span class="math math-inline">D</span>는 <span class="math math-inline">\mu</span>에 의해 생성되었으므로, 상태-행동 공간 <span class="math math-inline">(S \times A)</span> 중 <span class="math math-inline">\mu</span>가 자주 방문한 영역에 데이터가 집중된다. 그러나 강화학습의 목적은 <span class="math math-inline">\mu</span>보다 더 뛰어난 성능을 보이는 최적 정책 <span class="math math-inline">\pi^*</span>를 찾는 것이며, 이는 필연적으로 <span class="math math-inline">\pi</span>가 <span class="math math-inline">\mu</span>와 다른 행동을 선택하도록 유도한다.</p>
<p>문제는 에이전트가 데이터셋에 존재하지 않거나 빈도가 극히 낮은 상태-행동 쌍(Out-of-Distribution, OOD)에 대해 가치 함수(Q-function)를 추정하려 할 때 발생한다. 이를 **외삽 오차(Extrapolation Error)**라고 한다.7</p>
<h4>2.2.1  가치 과대평가(Value Overestimation)의 메커니즘</h4>
<p>Q-learning 기반 알고리즘은 벨만 최적 방정식(Bellman Optimality Equation)을 사용하여 Q값을 업데이트한다.</p>
<p><span class="math math-display">Q(s, a) \leftarrow \mathbb{E}[r(s, a) + \gamma \max_{a&#39;} Q(s&#39;, a&#39;)]</span></p>
<p>여기서 <span class="math math-inline">\max_{a&#39;} Q(s&#39;, a&#39;)</span> 연산은 다음 상태 <span class="math math-inline">s&#39;</span>에서 가능한 모든 행동 중 Q값이 가장 높은 행동을 선택한다. 오프라인 설정에서 함수 근사기(Function Approximator, 예: 신경망)는 가보지 않은 OOD 영역에 대해 부정확한 Q값을 출력할 수 있다. 특히 노이즈로 인해 실제보다 높게 예측된 Q값이 존재할 경우, 최대화 연산은 이 ’오류로 인해 높은 값’을 선택하게 된다.3</p>
<p>이러한 과대평가된 값은 벨만 업데이트를 통해 이전 상태의 Q값으로 전파(Backpropagation)되며, 부트스트래핑(Bootstrapping) 과정을 거치며 오차가 누적된다. 결과적으로 에이전트는 데이터셋에 없는 ‘환상적인(delusional)’ 상태-행동 경로가 존재한다고 믿게 되며, 실제 환경에 배치되었을 때 터무니없는 행동을 하거나 시스템을 위험에 빠뜨리게 된다.9</p>
<h3>2.3 데들리 트라이어드(The Deadly Triad)와 오프라인 RL</h3>
<p>Sutton과 Barto가 정의한 ’데들리 트라이어드’는 강화학습이 불안정해지는 세 가지 조건을 의미한다:</p>
<ol>
<li><strong>함수 근사(Function Approximation):</strong> 딥러닝과 같이 고차원 데이터를 처리하는 근사 모델 사용.</li>
<li><strong>부트스트래핑(Bootstrapping):</strong> 실제 완료된 보상이 아닌 추정치에 기반한 업데이트.</li>
<li><strong>오프-폴리시 학습(Off-policy Learning):</strong> 데이터 분포와 목표 정책의 불일치.</li>
</ol>
<p>오프라인 RL은 이 세 가지 요소가 가장 극단적으로 결합된 형태다. 특히 온라인 RL에서는 에이전트가 과대평가된 행동을 실제로 수행해보고 낮은 보상을 받음으로써(“Correction”), 잘못된 Q값을 수정할 기회가 있다. 그러나 오프라인 RL에서는 이러한 수정 피드백 고리(Corrective Feedback Loop)가 단절되어 있어, 데들리 트라이어드로 인한 발산 위험이 훨씬 높다.11</p>
<h2>3. 알고리즘적 접근법의 진화: 불확실성 제어와 정규화</h2>
<p>연구자들은 분포 변화와 외삽 오차를 해결하기 위해 다양한 전략을 개발해 왔다. 이들은 크게 정책 제약(Policy Constraint), 가치 정규화(Value Regularization), 그리고 인-샘플 학습(In-Sample Learning)으로 분류할 수 있다.</p>
<h3>3.1 정책 제약 방법론 (Policy Constraint Methods)</h3>
<p>이 접근법의 핵심은 학습하는 정책 <span class="math math-inline">\pi</span>가 데이터 수집 정책 <span class="math math-inline">\mu</span>와 너무 멀어지지 않도록 강제하는 것이다. 즉, 에이전트가 “아는 범위 내에서만” 행동하도록 제한한다.</p>
<h4>3.1.1 BCQ (Batch-Constrained deep Q-learning)</h4>
<p>BCQ는 오프라인 RL의 초기 돌파구를 마련한 알고리즘이다. BCQ는 행동 정책 <span class="math math-inline">\mu</span>를 명시적으로 추정하기 위해 **변이형 오토인코더(Conditional VAE)**를 생성 모델로 학습시킨다.12</p>
<ul>
<li>
<p><strong>작동 원리:</strong> 정책 업데이트 시, VAE를 통해 현재 상태 <span class="math math-inline">s</span>에서 데이터셋 분포와 유사한 행동들을 <span class="math math-inline">n</span>개 샘플링한다. 그 후, Q-네트워크를 사용하여 이 후보 행동들 중 가장 가치가 높은 행동을 선택한다.</p>
</li>
<li>
<p><strong>섭동 모델(Perturbation Model):</strong> 단순히 모방만 하면 행동의 다양성이 부족하므로, 별도의 섭동 모델 <span class="math math-inline">\xi_\phi(s, a)</span>를 두어 VAE가 생성한 행동을 소폭 수정(<span class="math math-inline">[-\Phi, \Phi]</span> 범위 내)한다. 이를 통해 데이터 분포 근처에서 국소적인 탐색을 수행하여 더 나은 행동을 찾는다.13</p>
</li>
</ul>
<p><span class="math math-display">\pi(s) = \underset{a_i + \xi_\phi(s, a_i)}{\text{argmax}} Q_\theta(s, a_i + \xi_\phi(s, a_i)), \quad \{a_i \sim G_\omega(s)\}_{i=1}^n</span></p>
<p>BCQ는 OOD 행동을 원천적으로 차단함으로써 외삽 오차를 방지하지만, 생성 모델의 성능에 크게 의존한다는 단점이 있다.</p>
<h4>2.2.2  BEAR (Bootstrapping Error Accumulation Reduction)</h4>
<p>BEAR는 BCQ보다 완화된 제약을 사용한다. 행동 자체가 아니라 정책의 분포와 데이터 분포 간의 거리(MMD, Maximum Mean Discrepancy)를 측정하여, 이 거리가 특정 임계값 <span class="math math-inline">\epsilon</span> 이하가 되도록 제약한다.10 이를 ’서포트 매칭(Support Matching)’이라 하며, 데이터가 존재하는 영역(Support) 내에서는 정책이 자유롭게 최적화될 수 있도록 허용한다.</p>
<h3>2.3  가치 정규화 방법론 (Value Regularization Methods)</h3>
<p>정책을 직접 제약하는 대신, Q-함수 학습 시 정규화 항을 추가하여 OOD 행동에 대한 Q값을 보수적으로 낮추는 방식이다.</p>
<h4>2.3.1  CQL (Conservative Q-Learning)</h4>
<p>CQL은 현재 오프라인 RL의 표준 베이스라인으로 자리 잡았다. CQL의 핵심 아이디어는 Q-함수의 **하한(Lower Bound)**을 학습하는 것이다.15</p>
<p>CQL은 벨만 오차를 최소화하는 기본 목적 함수에 다음과 같은 정규화 항을 추가한다:</p>
<p><span class="math math-display">\min_Q \alpha \left( \mathbb{E}_{s \sim D, a \sim \mu(s)} [Q(s, a)] - \mathbb{E}_{s \sim D, a \sim \hat{\pi}(s)} [Q(s, a)] \right) + \frac{1}{2} \mathbb{E}_{s, a, s&#39; \sim D}</span></p>
<ul>
<li>
<p><strong>첫 번째 항:</strong> 현재 정책 <span class="math math-inline">\hat{\pi}</span>가 선택할 가능성이 높은 행동들의 Q값을 최소화(minimize)한다. 이는 OOD 행동에 대한 과대평가를 억제하는 역할을 한다.</p>
</li>
<li>
<p>두 번째 항: 데이터셋 <span class="math math-inline">D</span>에 존재하는 행동들의 Q값을 최대화(maximize)한다.</p>
<p>이 두 항의 균형을 통해 CQL은 데이터셋에 있는 행동의 가치는 유지하면서, 데이터셋에 없는 행동의 가치는 낮추어 에이전트가 위험한 OOD 행동을 선택하지 않도록 유도한다. 이론적으로 적절한 <span class="math math-inline">\alpha</span> 값에 대해 CQL이 학습한 Q값은 실제 정책 가치의 하한임이 증명되었다.15</p>
</li>
</ul>
<h3>3.3 인-샘플 학습 (In-Sample Learning) 및 기대치 회귀</h3>
<p>최근에는 OOD 행동을 아예 쿼리하지 않고, 데이터셋 내의 샘플만으로 가치 함수를 학습하는 방법이 주목받고 있다.</p>
<h4>3.3.1 IQL (Implicit Q-Learning)</h4>
<p>IQL은 기존의 정책 제약이나 가치 정규화와 달리, **기대치 회귀(Expectile Regression)**를 도입하여 오프라인 RL 문제를 해결한다.18</p>
<p>일반적인 Q-learning은 <span class="math math-inline">\max_a Q(s,a)</span>를 계산해야 하므로 OOD 문제가 발생한다. 반면 SARSA와 같은 온-폴리시 방식은 데이터셋의 평균적인 행동만 학습하여 최적 성능에 도달하지 못한다. IQL은 이 딜레마를 해결하기 위해 상태 가치 함수 <span class="math math-inline">V(s)</span>를 행동 가치 분포의 상위 기대치(Expectile)로 정의한다.</p>
<p><span class="math math-display">L_V(\psi) = \mathbb{E}_{(s, a) \sim D} [L_2^\tau (Q_\theta(s, a) - V_\psi(s))]</span></p>
<p>여기서 <span class="math math-inline">L_2^\tau</span>는 비대칭 손실 함수로, <span class="math math-inline">\tau \in (0.5, 1)</span> 값을 조절하여 데이터셋 내에서 상위 <span class="math math-inline">100\tau%</span>에 해당하는 좋은 전이들만을 이용해 <span class="math math-inline">V(s)</span>를 학습시킨다. 이후 정책은 AWR(Advantage-Weighted Regression)을 통해 추출된다.19</p>
<p>IQL의 가장 큰 장점은 학습 과정에서 데이터셋 외부의 행동을 전혀 참조하지 않는다는 점이다. 이로 인해 학습이 매우 안정적이며, 계산 비용이 낮고, 특히 오프라인 사전 학습 후 온라인 파인튜닝(Online Fine-tuning) 시나리오에서 탁월한 성능을 보인다.19</p>
<h2>3.  시퀀스 모델링으로의 확장: 트랜스포머와 강화학습</h2>
<p>자연어 처리(NLP) 분야를 혁신한 트랜스포머(Transformer) 아키텍처는 강화학습을 순차적 의사결정 문제에서 **시퀀스 예측 문제(Sequence Prediction Problem)**로 재해석하는 계기를 마련했다.</p>
<h3>3.1  Decision Transformer (DT)</h3>
<p>Decision Transformer는 강화학습의 궤적(Trajectory)을 상태, 행동, 보상의 시퀀스 <span class="math math-inline">\tau = (R_1, s_1, a_1, R_2, s_2, a_2, \dots)</span>로 모델링한다.22 여기서 <span class="math math-inline">R_t</span>는 시점 <span class="math math-inline">t</span>에서의 기대 누적 보상(Return-to-go)이다.</p>
<ul>
<li><strong>작동 방식:</strong> DT는 GPT와 유사한 구조를 가지며, 지난 <span class="math math-inline">K</span>개 시점의 상태와 행동, 그리고 목표 보상 <span class="math math-inline">R</span>을 입력받아 다음에 취해야 할 행동 <span class="math math-inline">a_t</span>를 예측한다.</li>
<li><strong>특징:</strong> DT는 Q-함수나 벨만 업데이트를 전혀 사용하지 않으며, 순수한 지도 학습(Supervised Learning) 손실 함수(Cross-Entropy or MSE)로 학습된다. 이는 부트스트래핑으로 인한 불안정성을 근본적으로 제거한다.22</li>
<li><strong>성능 비교:</strong> 연구 결과에 따르면 DT는 보상이 희소(Sparse Reward)하거나 데이터 품질이 낮은 환경에서 기존 RL 알고리즘보다 강건한 성능을 보이지만, 보상이 밀집(Dense Reward)된 환경이나 확률적 요소가 강한 환경에서는 CQL이나 IQL과 같은 가치 기반 방법론이 여전히 우세한 경향이 있다.25</li>
</ul>
<h3>3.2  Trajectory Transformer (TT)</h3>
<p>Trajectory Transformer는 DT와 달리 모델 기반(Model-Based) 접근법을 취한다. TT는 상태, 행동, 보상의 결합 분포 <span class="math math-inline">P(\tau)</span>를 학습한다.27</p>
<ul>
<li><strong>계획(Planning)으로서의 빔 서치(Beam Search):</strong> 테스트 시 TT는 빔 서치 알고리즘을 사용하여 높은 누적 보상을 얻을 확률이 가장 높은 미래 궤적을 탐색하고 생성한다. 이는 언어 모델이 가장 그럴듯한 문장을 생성하는 과정과 유사하다.28</li>
<li><strong>장단점:</strong> TT는 궤적 전체를 고려하므로 장기적인 계획(Long-horizon Planning)에 유리하지만, 추론 시 빔 서치에 소요되는 계산 비용이 DT나 IQL에 비해 현저히 높다는 단점이 있다.28</li>
</ul>
<table><thead><tr><th><strong>모델</strong></th><th><strong>접근 방식</strong></th><th><strong>학습 목표</strong></th><th><strong>장점</strong></th><th><strong>단점</strong></th></tr></thead><tbody>
<tr><td><strong>Decision Transformer</strong></td><td>Model-free / Conditional Policy</td><td>지도 학습 (Action Prediction)</td><td>학습 안정성, 구현 용이성, 희소 보상에 강함</td><td>확률적 환경에서 성능 저하, 밀집 보상에서 한계</td></tr>
<tr><td><strong>Trajectory Transformer</strong></td><td>Model-based / Planning</td><td>시퀀스 모델링 (Discretized Distribution)</td><td>장기 계획 능력 우수, 높은 정확도</td><td>높은 계산 비용, 느린 추론 속도</td></tr>
</tbody></table>
<h2>4.  벤치마크 및 데이터셋: 성능 평가의 표준화</h2>
<p>오프라인 RL 연구가 가속화된 배경에는 표준화된 벤치마크 데이터셋의 등장이 있다. 알고리즘의 성능을 객관적으로 비교하기 위해서는 다양한 난이도와 특성을 가진 데이터셋이 필수적이다.</p>
<h3>4.1  D4RL (Datasets for Deep Data-Driven RL)</h3>
<p>D4RL은 현재 학계에서 가장 널리 사용되는 벤치마크다.31 MuJoCo 물리 엔진을 기반으로 하며, 다음과 같은 다양한 도메인을 포함한다.</p>
<ul>
<li><strong>Gym-MuJoCo (Locomotion):</strong> Hopper, HalfCheetah, Walker2d 등의 로봇이 걷거나 뛰는 동작을 학습한다. 데이터 품질에 따라 Expert(전문가), Medium(중급), Random(무작위), Medium-Replay(학습 중간 버퍼) 등으로 세분화된다.33</li>
<li><strong>AntMaze:</strong> 로봇 개미가 미로를 통과하여 목표 지점에 도달해야 한다. 이 작업은 보상이 매우 희소하며, 단순히 전문가 경로를 모방하는 것으로는 해결할 수 없고 여러 차선의 궤적들을 **결합(Stitching)**하여 새로운 최적 경로를 찾아내는 추론 능력을 요구한다.34</li>
<li><strong>Adroit / Franka Kitchen:</strong> 고차원 로봇 조작 과제로, 인간의 시연 데이터나 멀티태스크 데이터를 포함하여 현실 세계의 난이도를 반영한다.32</li>
</ul>
<h3>4.2  RL Unplugged</h3>
<p>DeepMind에서 제안한 벤치마크로, DM Control Suite와 Atari 게임 등을 포함한다.36 D4RL과 유사하지만 데이터 수집 프로토콜과 평가 방식에서 차이가 있으며, 특히 대규모 분산 학습 환경에서의 성능 평가를 위해 설계되었다. RL Unplugged는 다양한 난이도의 태스크(예: Humanoid Run, Rodent Maze)를 포함하며, 오프라인 정책 선택(Offline Policy Selection)의 어려움을 평가하는 데 중점을 둔다.37</p>
<h3>4.3  벤치마크 결과가 시사하는 통찰</h3>
<p>최신 벤치마크 결과들은 “만능 알고리즘(One-size-fits-all)“은 존재하지 않음을 보여준다.</p>
<ul>
<li><strong>AntMaze:</strong> IQL과 같은 인-샘플 학습 방법이 CQL이나 DT보다 월등히 우수한 성능을 보인다. 이는 미로 탐색과 같이 경로 결합(Stitching)이 중요한 과제에서 기대치 회귀를 통한 가치 학습이 효과적임을 시사한다.19</li>
<li><strong>희소 보상(Sparse Reward):</strong> Decision Transformer는 보상이 매우 드물게 주어지는 환경에서 강력한 성능을 발휘한다. 반면, 보상이 풍부한(Dense Reward) 환경에서는 전통적인 가치 기반 방법론(CQL, IQL)이 더 정교한 제어를 보여준다.25</li>
</ul>
<h2>5.  산업적 응용과 실제 사례 연구</h2>
<p>오프라인 RL은 시뮬레이션의 한계를 넘어 실제 산업 현장에 AI를 적용할 수 있는 가장 현실적인 대안이다.</p>
<h3>5.1  자율 주행 (Autonomous Driving)</h3>
<p>자율 주행 분야에서 오프라인 RL은 안전성을 담보하며 주행 지능을 고도화하는 핵심 기술이다. Waymo와 같은 선도 기업들은 실제 도로 주행에서 수집한 수백만 마일의 로그 데이터를 보유하고 있다.39</p>
<ul>
<li><strong>예측 및 계획(Prediction &amp; Planning):</strong> 주변 차량이나 보행자의 행동을 예측하고 자신의 경로를 계획하는 데 오프라인 RL이 사용된다. 단순히 사람의 운전을 모방(Imitation Learning)하는 것을 넘어, 충돌이나 급정거, 차선 이탈과 같은 부정적인 상황에 페널티를 부여하고 재학습(Fine-tuning)함으로써 사람보다 더 안전한 정책을 도출한다.40</li>
<li><strong>사례:</strong> Waymo의 연구에 따르면, 행동 모델에 강화학습을 적용하여 미세 조정한 결과 충돌률과 같은 안전 지표가 유의미하게 개선되었으며, 시뮬레이션 에이전트의 현실성(Realism) 또한 향상되었다.40 트랜스포머 기반 모델을 적용하여 복잡한 교차로 상황에서의 장기적인 상호작용을 모델링하는 연구도 활발하다.42</li>
</ul>
<h3>5.2  헬스케어 (Healthcare)</h3>
<p>의료 분야는 무작위 실험이 불가능하므로 오프라인 RL의 적용이 필수적이다.</p>
<ul>
<li><strong>패혈증(Sepsis) 치료:</strong> MIMIC-III와 같은 중환자실 임상 데이터를 활용하여 패혈증 환자에게 투여할 수액과 혈압상승제의 최적 용량을 결정하는 AI 의사가 연구되었다.43 연구 결과, 오프라인 RL 모델이 제안한 치료 정책이 실제 의사의 처방보다 환자의 생존율을 이론적으로 더 높일 수 있음이 확인되었다.</li>
<li><strong>안전성 제약:</strong> 의료 AI에서는 환자의 상태를 악화시키지 않는 것이 최우선이다. 따라서 불확실성이 높은 상황에서는 보수적인 행동을 취하도록 강제하거나, 의사의 처방 범위 내에서만 행동하도록 하는 <strong>제약 조건이 있는 강화학습(Constrained RL)</strong> 기법이 적용된다.44</li>
</ul>
<h3>5.3  추천 시스템 (Recommender Systems)</h3>
<p>유튜브, 넷플릭스, 이커머스 등에서 추천 시스템은 기업의 수익과 직결된다.</p>
<ul>
<li><strong>장기적 가치(Long-term Engagement) 최적화:</strong> 기존의 지도 학습 기반 추천(클릭률 예측 등)은 당장의 클릭을 유도하는 자극적인 콘텐츠(Clickbait)를 추천하는 경향이 있어 장기적으로 사용자 이탈을 초래한다. 반면 오프라인 RL은 사용자의 세션 전체를 하나의 에피소드로 보고, 총 체류 시간이나 장기적인 만족도를 최대화하는 방향으로 학습한다.46</li>
<li><strong>오프라인 정책 평가(OPE):</strong> 추천 알고리즘을 변경했을 때의 효과를 실제 서비스 배포 전에 검증하기 위해, 과거 로그 데이터를 이용한 OPE 기술이 필수적으로 사용된다. 이를 통해 매출 하락의 리스크 없이 모델을 고도화할 수 있다.48</li>
</ul>
<h2>6.  기술적 난제와 미래 전망</h2>
<p>오프라인 RL은 비약적인 발전을 이루었으나, 여전히 해결해야 할 과제들이 남아 있다.</p>
<h3>6.1  Sim-to-Real 격차 및 하이브리드 학습</h3>
<p>오프라인 데이터(주로 시뮬레이션이나 제한된 환경)로 학습된 정책을 실제 물리 로봇에 탑재했을 때, 마찰계수나 센서 노이즈 등의 미세한 차이로 인해 성능이 급격히 하락하는 Sim-to-Real 격차가 발생한다.50</p>
<p>이를 해결하기 위해 최근에는 하이브리드(Hybrid) RL 접근법이 대두되고 있다. 오프라인 데이터로 정책을 안전하게 사전 학습(Pre-training)한 후, 실제 환경에서 매우 적은 양의 데이터를 수집하며 조심스럽게 파인튜닝(Fine-tuning)하는 방식이다. IQL과 같은 알고리즘이 이러한 하이브리드 설정에서 특히 강점을 보인다.19</p>
<h3>6.2  하이퍼파라미터 튜닝과 모델 선택 (Model Selection)</h3>
<p>오프라인 RL의 실무 적용을 가로막는 가장 큰 장벽은 하이퍼파라미터 튜닝의 어려움이다. 온라인 RL에서는 에이전트를 실행해보고 점수를 확인하면 되지만, 오프라인 RL에서는 환경 상호작용 없이 어떤 하이퍼파라미터 설정이 좋은지 평가해야 한다. 오프라인 정책 평가(OPE) 지표들조차 완벽하지 않기 때문에, 연구자들은 통계적으로 보수적인 하한을 추정하거나, 시뮬레이션 모델을 학습하여(Model-based evaluation) 간접적으로 평가하는 방법을 모색하고 있다.50</p>
<h3>6.3  파운데이션 모델(Foundation Models)과 멀티태스크 학습</h3>
<p>2024년과 2025년의 연구 트렌드는 파운데이션 모델과의 융합이다. 거대 언어 모델(LLM)이 방대한 텍스트 데이터로 일반적인 언어 능력을 학습했듯, 오프라인 RL에서도 다양한 로봇, 다양한 작업, 다양한 환경에서 수집된 대규모 데이터셋(예: Bridge Data)을 통합하여 **범용 의사결정 모델(Generalist Agent)**을 학습하려는 시도가 진행 중이다.54</p>
<p>이러한 모델은 특정 작업에 국한되지 않고, 새로운 작업이 주어졌을 때 소량의 데이터만으로도 빠르게 적응(Few-shot Adaptation)할 수 있는 능력을 보여준다. ’JOWA’와 같은 최신 모델은 수십억 토큰 규모의 데이터로 세계 모델(World Model)과 행동 모델을 공동 학습하여, 오프라인 RL의 확장성(Scalability) 한계를 극복하고 있다.54</p>
<h2>7.  결론</h2>
<p>오프라인 강화학습은 데이터의 바다에서 지능을 추출하는 기술이다. 과거의 경험을 단순히 기록으로 남겨두지 않고, 미래의 최적 행동을 결정하는 지침으로 변환한다는 점에서 그 가치는 막대하다. 초기 오프-폴리시 알고리즘의 한계를 극복하기 위해 등장한 BCQ, CQL, IQL 등의 알고리즘은 분포 변화라는 난제를 이론적으로, 그리고 실용적으로 해결해 나가고 있다. 더 나아가 트랜스포머 아키텍처와의 만남은 강화학습을 시퀀스 모델링이라는 더 거대한 패러다임으로 통합시키고 있다.</p>
<p>자율 주행차의 안전한 운행부터 환자를 살리는 정밀 의료, 사용자의 마음을 읽는 추천 시스템에 이르기까지, 오프라인 RL은 이미 우리 삶의 중요한 영역에 침투하고 있다. 비록 Sim-to-Real 격차나 모델 평가의 어려움과 같은 난관이 남아있지만, 파운데이션 모델과의 결합을 통한 대규모화와 일반화는 오프라인 강화학습이 범용 인공지능(AGI)을 향한 핵심 경로임을 시사한다. 정적인 데이터에 생명력을 불어넣는 오프라인 강화학습의 진화는 앞으로도 계속될 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>12월 10, 2025에 액세스, [https://apxml.com/courses/advanced-reinforcement-learning/chapter-7-offline-reinforcement-learning/offline-vs-online-offpolicy#:<sub>:text=Off%2DPolicy%20(Online)%20uses,of%20difficulty%20in%20Offline%20RL.](https://apxml.com/courses/advanced-reinforcement-learning/chapter-7-offline-reinforcement-learning/offline-vs-online-offpolicy#:</sub>:text=Off-Policy (Online) uses, <a href="https://apxml.com/courses/advanced-reinforcement-learning/chapter-7-offline-reinforcement-learning/offline-vs-online-offpolicy#:~:text=Off-Policy%20(Online)%20uses,of%20difficulty%20in%20Offline%20RL.">https://apxml.com/courses/advanced-reinforcement-learning/chapter-7-offline-reinforcement-learning/offline-vs-online-offpolicy#:~:text=Off%2DPolicy%20(Online)%20uses,of%20difficulty%20in%20Offline%20RL.</a></li>
<li>Off-policy vs On-Policy vs Offline Reinforcement Learning Demystified! | by Kowshik chilamkurthy, https://kowshikchilamkurthy.medium.com/off-policy-vs-on-policy-vs-offline-reinforcement-learning-demystified-f7f87e275b48</li>
<li>What is offline reinforcement learning？ : r/reinforcementlearning - Reddit, https://www.reddit.com/r/reinforcementlearning/comments/utnhia/what_is_offline_reinforcement_learning/</li>
<li>A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems - IEEE Xplore, https://ieeexplore.ieee.org/iel7/5962385/10623582/10078377.pdf</li>
<li>On-policy and off-policy Reinforcement Learning: Key features and differences - Ericsson, https://www.ericsson.com/en/blog/2023/12/online-and-offline-reinforcement-learning-what-are-they-and-how-do-they-compare</li>
<li>What is the relation between online (or offline) learning and on-policy (or off-policy) algorithms? - Artificial Intelligence Stack Exchange, https://ai.stackexchange.com/questions/10474/what-is-the-relation-between-online-or-offline-learning-and-on-policy-or-off</li>
<li>Offline Reinforcement Learning - Weinan Zhang, https://wnzhang.net/teaching/sjtu-rl-2024/slides/11-offline-rl.pdf</li>
<li>Addressing Bootstrapping Errors in Offline Reinforcement Learning with Ensembles - Student Theses Faculty of Science and Engineering, https://fse.studenttheses.ub.rug.nl/28178/1/thesis.pdf</li>
<li>Addressing Distribution Shift in Online Reinforcement Learning with Offline Datasets, https://offline-rl-neurips.github.io/pdf/13.pdf</li>
<li>Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction, http://papers.neurips.cc/paper/9349-stabilizing-off-policy-q-learning-via-bootstrapping-error-reduction.pdf</li>
<li>Reinforcement Learning to Optimize Long-term User Engagement in Recommender Systems - arXiv, https://arxiv.org/pdf/1902.05570</li>
<li>Batch Constrained Deep Reinforcement Learning - Seita’s Place, https://danieltakeshi.github.io/2019/02/09/batch-constrained-deep-rl/</li>
<li>Off-Policy Deep Reinforcement Learning without Exploration, http://proceedings.mlr.press/v97/fujimoto19a/fujimoto19a.pdf</li>
<li>Short Notes on Batch Constrained Deep Reinforcement Learning (BCQ) - Medium, https://medium.com/@f20170720/short-notes-on-batch-constrained-deep-reinforcement-learning-bcq-6fd69feca521</li>
<li>Conservative Q-Learning for Offline Reinforcement Learning, https://papers.neurips.cc/paper_files/paper/2020/file/0d2b2061826a5df3221116a5085a6052-Paper.pdf</li>
<li>[RL] Conservative Q-Learning (CQL) - Youngdo Lee, https://leeyngdo.github.io/blog/reinforcement-learning/2024-04-21-conservative-q-learning/</li>
<li>Conservative Q-Learning for Offline Reinforcement Learning - arXiv, https://arxiv.org/pdf/2006.04779</li>
<li>Implicit Q-Learning in Offline RL - Emergent Mind, https://www.emergentmind.com/topics/implicit-q-learning-iql</li>
<li>Offline Reinforcement Learning with Implicit Q-Learning, https://offline-rl-neurips.github.io/2021/pdf/24.pdf</li>
<li>[2110.06169] Offline Reinforcement Learning with Implicit Q-Learning - arXiv, https://arxiv.org/abs/2110.06169</li>
<li>arXiv:2110.06169v1 [cs.LG] 12 Oct 2021, https://arxiv.org/pdf/2110.06169</li>
<li>Reframing Reinforcement Learning as Sequence Modeling with Transformers?, https://danieltakeshi.github.io/2021/06/24/transformers-for-rl/</li>
<li>How Crucial is Transformer in Decision Transformer? - OpenReview, https://openreview.net/pdf?id=RV6fghh1As_</li>
<li>WHEN SHOULD WE PREFER DECISION TRANSFORMERS FOR OFFLINE REINFORCEMENT LEARNING? - ICLR Proceedings, https://proceedings.iclr.cc/paper_files/paper/2024/file/852f50969a9e523ec41d26f2f68bd456-Paper-Conference.pdf</li>
<li>A Comparison Between Decision Transformers and Traditional Offline Reinforcement Learning Algorithms in Varying Reward Settings for Continuous Control Tasks - arXiv, https://arxiv.org/html/2511.16475v1</li>
<li>A Comparison Between Decision Transformers and Traditional Offline Reinforcement Learning Algorithms - arXiv, https://arxiv.org/pdf/2511.16475</li>
<li>Sequence Modeling Solutions for Reinforcement Learning Problems - Berkeley AI Research, https://bair.berkeley.edu/blog/2021/11/19/trajectory-transformer/</li>
<li>Offline Reinforcement Learning as One Big Sequence Modeling Problem - Trajectory Transformer, https://trajectory-transformer.github.io/trajectory-transformer-neurips-2021.pdf</li>
<li>Trajectory Transformer, https://trajectory-transformer.github.io/</li>
<li>Feedback Decision Transformer: Offline Reinforcement Learning With Feedback | IEEE Conference Publication | IEEE Xplore, https://ieeexplore.ieee.org/document/10415685/</li>
<li>NeoRL: A Near Real-World Benchmark for Offline Reinforcement Learning - OpenReview, https://openreview.net/pdf?id=jNdLszxdtra</li>
<li>D4RL (MuJoCo) - DI-engine’s documentation! - Read the Docs, https://di-engine-docs.readthedocs.io/en/latest/13_envs/d4rl.html</li>
<li>D4RL: Building Better Benchmarks for Offline Reinforcement Learning – The Berkeley Artificial Intelligence Research Blog, https://bair.berkeley.edu/blog/2020/06/25/D4RL/</li>
<li>D5RL: Diverse Datasets for Data-Driven Deep Reinforcement Learning - arXiv, https://arxiv.org/html/2408.08441v1</li>
<li>[2004.07219] D4RL: Datasets for Deep Data-Driven Reinforcement Learning - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/2004.07219</li>
<li>RL Unplugged: Benchmarks for Offline Reinforcement Learning - ResearchGate, https://www.researchgate.net/publication/342435753_RL_Unplugged_Benchmarks_for_Offline_Reinforcement_Learning</li>
<li>RL Unplugged: A Suite of Benchmarks for Offline Reinforcement Learning, https://proceedings.neurips.cc/paper/2020/file/51200d29d1fc15f5a71c1dab4bb54f7c-Paper.pdf</li>
<li>Offline Reinforcement Learning with Imbalanced Datasets - arXiv, https://arxiv.org/html/2307.02752v3</li>
<li>Scalable Offline Metrics for Autonomous Driving - arXiv, https://arxiv.org/html/2510.08571v1</li>
<li>Improving Agent Behaviors with RL Fine-tuning for Autonomous Driving - Waymo, https://waymo.com/research/improving-agent-behaviors-with-rl-fine-tuning-for-autonomous-driving/</li>
<li>Behavior Prediction and Decision-Making in Self-Driving Cars Using Deep Learning — Part 2 - Isaac Kargar, https://kargarisaac.medium.com/behavior-prediction-and-decision-making-in-self-driving-cars-using-deep-learning-part-2-54e41cc33d04</li>
<li>New Insights for Scaling Laws in Autonomous Driving - Waymo, https://waymo.com/blog/2025/06/scaling-laws-in-autonomous-driving</li>
<li>Reinforcement Learning in Healthcare: Applications and Challenges - ResearchGate, https://www.researchgate.net/publication/379796276_Reinforcement_Learning_in_Healthcare_Applications_and_Challenges</li>
<li>Offline Inverse Constrained Reinforcement Learning for Safe-Critical Decision Making in Healthcare - arXiv, https://arxiv.org/html/2410.07525v1</li>
<li>Model Selection for Offline Reinforcement Learning: Practical Considerations for Healthcare Settings, https://proceedings.mlr.press/v149/tang21a/tang21a.pdf</li>
<li>Improving Long-Term Metrics in Recommendation Systems using Short-Horizon Offline RL, https://www.researchgate.net/publication/352054087_Improving_Long-Term_Metrics_in_Recommendation_Systems_using_Short-Horizon_Offline_RL</li>
<li>RecoMind: A Reinforcement Learning Framework for Optimizing In-Session User Satisfaction in Recommendation Systems - arXiv, https://arxiv.org/html/2508.00201v1</li>
<li>Offline Evaluation for Reinforcement Learning-based Recommendation: A Critical Issue and Some Alternatives - SIGIR, https://sigir.org/wp-content/uploads/2023/01/p03.pdf</li>
<li>MARS-Gym: Offline Reinforcement Learning for Recommender Systems in Marketplaces, https://offline-rl-neurips.github.io/pdf/21.pdf</li>
<li>Offline Hyperparameter Tuning for RL - OpenReview, https://openreview.net/pdf?id=AiOUi3440V</li>
<li>Revealing the Challenges of Sim-to-Real Transfer in Model-Based Reinforcement Learning via Latent Space Modeling - arXiv, https://arxiv.org/html/2506.12735v1</li>
<li>State Revisit and Re-explore: Bridging Sim-to-Real Gaps in Offline-and-Online Reinforcement Learning with An Imperfect Simulator - IJCAI, https://www.ijcai.org/proceedings/2025/0970.pdf</li>
<li>(PDF) Improving and Benchmarking Offline Reinforcement Learning Algorithms, https://www.researchgate.net/publication/371223323_Improving_and_Benchmarking_Offline_Reinforcement_Learning_Algorithms</li>
<li>Scaling Offline Model-Based RL via Jointly-Optimized World-Action Model Pretraining, https://arxiv.org/html/2410.00564v3</li>
<li>SCALING OFFLINE MODEL-BASED RL VIA JOINTLY- OPTIMIZED WORLD-ACTION MODEL PRETRAINING - ICLR Proceedings, https://proceedings.iclr.cc/paper_files/paper/2025/file/689cffc97600f9deb8374fc8fa918b8e-Paper-Conference.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>