<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:SAC (Soft Actor-Critic, 2018)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>SAC (Soft Actor-Critic, 2018)</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">강화 학습 (Reinforcement Learning)</a> / <a href="index.html">최대 엔트로피 강화학습 (Maximum Entropy Reinforcement Learning)</a> / <span>SAC (Soft Actor-Critic, 2018)</span></nav>
                </div>
            </header>
            <article>
                <h1>SAC (Soft Actor-Critic, 2018)</h1>
<h2>1.  안정성과 효율성의 새로운 지평</h2>
<p>심층 강화학습(Deep Reinforcement Learning, DRL)은 신경망의 강력한 표현력을 바탕으로 복잡한 의사결정 문제에서 인간을 뛰어넘는 성과를 보이며 광범위한 주목을 받아왔다. 그러나 이러한 성공의 이면에는 심층 강화학습 방법론이 지닌 근본적인 한계점들이 존재한다. 그중 가장 대표적인 두 가지 난제는 **높은 샘플 복잡도(High Sample Complexity)**와 **취약한 수렴성(Brittle Convergence)**이다.1 전자는 에이전트가 유의미한 정책을 학습하기 위해 환경과 수백만 번 이상의 상호작용을 요구하는 문제로, 데이터 수집에 물리적, 시간적 비용이 많이 드는 실제 세계 응용을 어렵게 만든다. 후자는 학습률, 탐험 상수 등 다양한 하이퍼파라미터의 미세한 변화에도 알고리즘의 성능이 급격히 저하되거나 발산하는 현상을 의미하며, 이는 안정적인 학습을 보장하기 어렵게 만든다.2 이러한 문제들은 심층 강화학습 기술을 시뮬레이션을 넘어 로보틱스와 같은 실제 물리 시스템에 적용하는 데 있어 핵심적인 장벽으로 작용해왔다.</p>
<p>이러한 배경 속에서, 강화학습의 전통적인 목표를 재해석하는 새로운 패러다임이 등장하였으니, 바로 **최대 엔트로피 강화학습(Maximum Entropy Reinforcement Learning)**이다.3 기존 강화학습이 누적 보상의 기댓값을 최대화하는 단일 목표를 추구했다면, 최대 엔트로피 강화학습은 보상과 함께 정책의 **엔트로피(Entropy)**를 동시에 극대화하고자 한다.4 엔트로피는 정보 이론에서 불확실성의 척도로 사용되며, 강화학습 정책의 맥락에서는 행동 선택의 무작위성을 의미한다.5 따라서 이 프레임워크는 에이전트가 주어진 과업을 성공적으로 수행하면서도, 가능한 한 예측 불가능하고 다양한 방식으로 행동하도록 장려한다.1 이는 탐험과 활용의 균형을 맞추는 문제에 대한 보다 근본적이고 수학적인 접근법을 제시한다.</p>
<p>**Soft Actor-Critic (SAC)**은 바로 이 최대 엔트로피 강화학습 프레임워크에 이론적 기반을 둔 Off-policy Actor-Critic 알고리즘이다.1 2018년 UC 버클리와 구글의 연구진에 의해 처음 제안된 SAC는, 심층 강화학습 분야의 오랜 난제였던 샘플 효율성과 학습 안정성 사이의 트레이드오프를 해결하기 위한 정교한 설계의 결과물이다. 당시 심층 강화학습 알고리즘은 크게 두 갈래로 나뉘어 있었다. DDPG(Deep Deterministic Policy Gradient)와 같은 Off-policy 알고리즘은 리플레이 버퍼를 통해 과거 경험을 재사용하여 샘플 효율성을 높였으나, 학습이 극도로 불안정하고 하이퍼파라미터에 민감하다는 치명적인 단점이 있었다.2 반면, PPO(Proximal Policy Optimization)와 같은 On-policy 알고리즘은 안정적인 학습을 보장했지만, 매 업데이트마다 새로운 데이터를 수집해야 하므로 샘플 효율성이 매우 낮았다.7 SAC는 이 두 세계의 장점을 결합하려는 시도에서 출발했다. 즉, Off-policy 학습의 높은 샘플 효율성을 취하면서도, 최대 엔트로피 원리를 통해 DDPG의 불안정성을 극복하고자 한 것이다. SAC는 <strong>Off-policy 학습</strong>, 안정적인 <strong>확률적 Actor-Critic 구조</strong>, 그리고 <strong>엔트로피 최대화</strong>라는 세 가지 핵심 요소를 유기적으로 결합함으로써, 기존의 On-policy 및 Off-policy 방법론들을 샘플 효율성과 최종 성능, 그리고 학습 안정성 측면에서 모두 뛰어넘는 SOTA(State-of-the-art) 성능을 달성했다.2 이는 강화학습 연구의 패러다임이 단순히 ’최고 성능 달성’을 넘어 ’어떻게 안정적이고 효율적으로 학습할 것인가’라는 근본적인 질문으로 이동하고 있음을 보여주는 중요한 이정표였다.</p>
<p>본 안내서는 SAC 알고리즘에 대한 심층적인 고찰을 목표로 한다. 제 2장에서는 SAC의 철학적 기반이 되는 최대 엔트로피 강화학습의 이론을 탐구하고, 제 3장에서는 알고리즘을 구성하는 수학적 원리와 구조를 상세히 해부한다. 제 4장에서는 PPO, TD3 등 주요 알고리즘과의 비교 분석을 통해 SAC의 독자적인 위치를 조명하며, 제 5장에서는 벤치마크 및 실제 로보틱스 응용 사례를 통해 그 실증적 성능을 확인한다. 마지막으로 제 6장과 7장에서는 SAC의 최신 연구 동향과 미래 전망을 논하며 마무리한다.</p>
<h2>2.  이론적 토대: 최대 엔트로피 강화학습</h2>
<p>SAC를 이해하기 위한 첫걸음은 그 근간을 이루는 최대 엔트로피 강화학습 프레임워크를 이해하는 것이다. 이 프레임워크는 전통적인 강화학습의 목적 함수를 수정하여, 에이전트의 행동에 내재된 불확실성, 즉 엔트로피를 명시적으로 장려한다.</p>
<h3>2.1  표준 강화학습 목적 함수의 재정의</h3>
<p>표준 강화학습에서 에이전트의 목표는 시간 <span class="math math-inline">t=0</span>부터 <span class="math math-inline">T</span>까지 할인된 누적 보상의 기댓값을 최대화하는 정책 <span class="math math-inline">\pi</span>를 찾는 것이다. 이를 수식으로 표현하면 다음과 같다.<br />
<span class="math math-display">
J(\pi) = \sum_{t=0}^{T} \mathbb{E}_{(\mathbf{s}_t, \mathbf{a}_t) \sim \rho_\pi} \left[ \gamma^t r(\mathbf{s}_t, \mathbf{a}_t) \right]
</span><br />
여기서 <span class="math math-display">\rho_\pi</span>는 정책 <span class="math math-display">\pi</span>를 따를 때의 상태-행동 방문 분포, <span class="math math-display">\gamma</span>는 할인율, <span class="math math-display">r(\mathbf{s}_t, \mathbf{a}_t)</span>는 보상 함수이다.</p>
<p>최대 엔트로피 강화학습은 이 목적 함수에 새로운 항을 추가한다. 에이전트는 매 타임스텝마다 보상과 더불어, 해당 시점에서의 정책 엔트로피에 비례하는 ’보너스 보상’을 받는다.5 수정된 목적 함수는 다음과 같이 정의된다.9<br />
<span class="math math-display">
J(\pi) = \sum_{t=0}^{T} \mathbb{E}_{(\mathbf{s}_t, \mathbf{a}_t) \sim \rho_\pi} \left[ r(\mathbf{s}_t, \mathbf{a}_t) + \alpha \mathcal{H}(\pi(\cdot|\mathbf{s}_t)) \right]
</span><br />
여기서 <span class="math math-display">\mathcal{H}(\pi(\cdot|\mathbf{s}_t))</span>는 상태 <span class="math math-display">\mathbf{s}_t</span>에서의 정책 <span class="math math-display">\pi</span>의 엔트로피를 나타내며, <span class="math math-display">\mathbb{E}_{\mathbf{a}_t \sim \pi}[-\log \pi(\mathbf{a}_t|\mathbf{s}_t)]</span>로 계산된다. <span class="math math-display">\alpha</span>는 **온도 파라미터(temperature parameter)**라 불리는 양의 실수로, 보상 극대화와 엔트로피 극대화 사이의 상대적 중요도를 조절하는 역할을 한다.11</p>
<p><span class="math math-display">\alpha</span>가 0에 가까우면 표준 강화학습 문제와 동일해지며, <span class="math math-display">\alpha</span>가 클수록 에이전트는 보상을 다소 희생하더라도 더 무작위적인 행동을 선택하게 된다.5</p>
<h3>2.2 엔트로피의 다각적 역할</h3>
<p>목적 함수에 추가된 엔트로피 항은 단순히 에이전트를 더 무작위적으로 만드는 것을 넘어, 학습 과정 전반에 걸쳐 다각적인 긍정적 효과를 가져온다.</p>
<h4>2.2.1 탐험 촉진 (Exploration Enhancement)</h4>
<p>엔트로피를 극대화하려는 경향은 정책이 특정 행동에 대한 확률을 1로 수렴시키는 것을 방지한다. 즉, 정책이 너무 빨리 결정론적으로 변하는 것을 막아준다.12 이는 에이전트가 초기에 발견한 그럴듯한 전략에만 매몰되지 않고, 잠재적으로 더 나은 보상을 가져올 수 있는 미지의 행동들을 지속적으로 탐색하도록 유도한다. 예를 들어, 미로 찾기 문제에서 약간의 보상을 주는 짧은 경로와 큰 보상을 주는 아직 발견되지 않은 긴 경로가 있을 때, 엔트로피 보너스가 없다면 에이전트는 짧은 경로만 반복적으로 이용하는 지역 최적해(local optima)에 빠지기 쉽다. 하지만 엔트로피 보너스는 에이전트가 두 경로 모두에 유의미한 확률을 할당하도록 장려하여, 결국 더 높은 보상을 주는 경로를 발견할 가능성을 높인다.4</p>
<h4>2.2.2 강건성 향상 (Robustness Improvement)</h4>
<p>학습 과정에서 의도적으로 무작위성을 주입하고 이를 견뎌내도록 훈련된 정책은 자연스럽게 외부의 예기치 않은 변화나 방해(perturbation)에 더 강한 모습을 보인다.3 이는 마치 운동선수가 훈련 시 더 무거운 중량을 다루거나 불리한 조건을 설정하여 실제 경기에서의 돌발 상황에 대비하는 것과 유사하다. SAC로 학습된 정책은 테스트 시점에 모델의 부정확성이나 환경의 미세한 변화에 덜 민감하게 반응하며, 이는 실제 로봇과 같이 예측 불가능한 요소가 많은 환경에서 매우 중요한 특성이다.3</p>
<h4>2.2.3 최적화 지형 평탄화 (Optimization Landscape Smoothing)</h4>
<p>엔트로피 정규화의 가장 심오한 역할 중 하나는 강화학습 문제의 최적화 지형(optimization landscape)을 변형시키는 능력에 있다. 정책 파라미터 공간에서 목적 함수는 수많은 지역 최적해와 급격한 경사를 가진 복잡한 형태를 띤다. 이러한 지형은 경사 기반 최적화 알고리즘이 안정적으로 전역 최적해를 찾아가는 것을 방해한다. 연구에 따르면, 엔트로피 항을 추가하면 이 복잡한 지형이 전반적으로 더 부드러워지는 효과가 있다.13 이는 뾰족한 지역 최적해들을 서로 연결하고, 가파른 절벽을 완만한 경사로 만들어, 최적화 알고리즘이 더 큰 학습률(learning rate)을 사용해도 안정적으로 수렴할 수 있게 돕는다.14 이처럼 엔트로피 최대화는 단순히 ’더 나은 탐험 전략’을 제공하는 것을 넘어, 강화학습 문제를 근본적으로 ’더 풀기 쉬운 최적화 문제’로 변환하는 정규화(regularization) 기법으로 작용한다. 이는 SAC가 다른 알고리즘에 비해 하이퍼파라미터에 덜 민감하고 2, 다양한 과업에 동일한 설정을 적용해도 준수한 성능을 내는 3 이유를 설명하는 핵심적인 메커니즘이다.</p>
<h3>2.3 엔트로피와 보상의 관계</h3>
<p>최근 연구는 엔트로피와 보상 간의 관계를 더 깊이 파고든다. 한 연구에 따르면, 학습 과정에서 정책의 엔트로피는 급격히 감소하며, 이러한 엔트로피의 ’소모’가 보상의 ’획득’으로 이어진다는 것이 관찰되었다.15 이 관계는 <span class="math math-inline">R = -a \exp(\mathcal{H}) + b</span>와 같은 지수 함수 형태로 나타나며, 이는 엔트로피가 고갈되면 성능 향상이 정체될 수 있음을 시사한다. 즉, 엔트로피는 무한한 자원이 아니라, 효율적으로 관리하고 사용해야 하는 일종의 ’탐험 예산’으로 볼 수 있다. 이 관점은 엔트로피를 단순히 최대화하는 것을 넘어, 학습 과정 전반에 걸쳐 어떻게 동적으로 조절할 것인가에 대한 중요한 질문을 제기한다.</p>
<h2>제 3장: SAC 알고리즘의 해부: 수학적 원리와 구조</h2>
<p>SAC는 최대 엔트로피 강화학습이라는 철학적 토대 위에 정교한 수학적 구조를 쌓아 올린 알고리즘이다. 이 장에서는 SAC를 구성하는 핵심 요소들, 즉 소프트 가치 함수와 벨만 방정식, 그리고 각 신경망의 학습 과정을 상세히 분석한다.</p>
<h3>3.1 핵심 구성 요소</h3>
<p>SAC는 Actor-Critic 구조를 따르며, 일반적으로 다음과 같은 신경망들로 구성된다 4:</p>
<ol>
<li><strong>정책 네트워크 (Actor, <span class="math math-display">\pi_\phi</span>):</strong> 상태 <span class="math math-display">\mathbf{s}</span>를 입력받아 행동 <span class="math math-display">\mathbf{a}</span>의 확률 분포를 출력하는 확률적 정책이다. 파라미터 <span class="math math-display">\phi</span>로 매개변수화된다.</li>
<li><strong>소프트 Q-네트워크 (Critic, <span class="math math-display">Q_{\theta_1}, Q_{\theta_2}</span>):</strong> 상태 <span class="math math-display">\mathbf{s}</span>와 행동 <span class="math math-display">\mathbf{a}</span>를 입력받아, 해당 상태-행동 쌍의 소프트 Q-값(엔트로피가 포함된 가치)을 추정한다. 파라미터 <span class="math math-display">\theta_1, \theta_2</span>로 매개변수화된 두 개의 독립적인 Q-네트워크를 사용하여 가치 추정의 과대평가 편향을 완화한다 (Clipped Double Q-Learning).</li>
<li><strong>소프트 가치 네트워크 (Value, <span class="math math-display">V_\psi</span>):</strong> 상태 <span class="math math-display">\mathbf{s}</span>를 입력받아 해당 상태의 소프트 V-값(엔트로피가 포함된 상태 가치)을 추정한다. 파라미터 <span class="math math-display">\psi</span>로 매개변수화된다. 초기 SAC 버전에서는 이 네트워크를 명시적으로 사용했으나, 후속 버전에서는 Q-네트워크로부터 V-값을 직접 계산하여 구조를 단순화하기도 한다. 안정적인 학습을 위해, V-네트워크(또는 Q-네트워크)에 대한 **타겟 네트워크(<span class="math math-display">V_{\bar{\psi}}</span> 또는 <span class="math math-display">Q_{\bar{\theta}}</span>)**를 별도로 유지한다.</li>
</ol>
<h3>2.2  소프트 가치 함수와 소프트 벨만 방정식</h3>
<p>최대 엔트로피 목적 함수에 따라, 가치 함수들 역시 엔트로피 항을 포함하도록 재정의된다. 이를 <strong>소프트(soft)</strong> 가치 함수라고 부른다.</p>
<h4>2.2.1  소프트 가치 함수의 정의</h4>
<p>**소프트 상태 가치 함수(Soft State-Value Function, <span class="math math-display">V^\pi(\mathbf{s})</span>)**는 상태 <span class="math math-display">\mathbf{s}</span>에서 시작하여 정책 <span class="math math-display">\pi</span>를 따랐을 때 기대되는 누적 보상과 엔트로피의 합이다. 이는 다음 행동의 기대 소프트 Q-값과 기대 엔트로피로 표현할 수 있다.9<br />
<span class="math math-display">
V^\pi(\mathbf{s}_t) = \mathbb{E}_{\mathbf{a}_t \sim \pi} \left[ Q^\pi(\mathbf{s}_t, \mathbf{a}_t) - \alpha \log \pi(\mathbf{a}_t|\mathbf{s}_t) \right]
</span><br />
**소프트 행동 가치 함수(Soft Action-Value Function, <span class="math math-display">Q^\pi(\mathbf{s}, \mathbf{a})</span>)**는 상태 <span class="math math-display">\mathbf{s}</span>에서 행동 <span class="math math-display">\mathbf{a}</span>를 취한 후, 정책 <span class="math math-display">\pi</span>를 따랐을 때 기대되는 누적 보상과 엔트로피의 합이다. 이는 즉각적인 보상과 다음 상태의 기대 소프트 V-값의 합으로 표현된다.9<br />
<span class="math math-display">
Q^\pi(\mathbf{s}_t, \mathbf{a}_t) = r(\mathbf{s}_t, \mathbf{a}_t) + \gamma \mathbb{E}_{\mathbf{s}_{t+1} \sim p} \left[ V^\pi(\mathbf{s}_{t+1}) \right]
</span></p>
<h4>2.2.2  소프트 벨만 기대 방정식</h4>
<p>위 두 정의를 결합하면, <span class="math math-display">Q^\pi</span>에 대한 재귀적인 관계식인 **소프트 벨만 기대 방정식(Soft Bellman Expectation Equation)**을 유도할 수 있다. 이 방정식은 SAC의 크리틱 학습의 이론적 토대를 형성한다.9<br />
<span class="math math-display">
Q^\pi(\mathbf{s}_t, \mathbf{a}_t) = \mathbb{E}_{\mathbf{s}_{t+1} \sim p} \left[ r(\mathbf{s}_t, \mathbf{a}_t) + \gamma \mathbb{E}_{\mathbf{a}_{t+1} \sim \pi} \left[ Q^\pi(\mathbf{s}_{t+1}, \mathbf{a}_{t+1}) - \alpha \log \pi(\mathbf{a}_{t+1}|\mathbf{s}_{t+1}) \right] \right]
</span><br />
이 방정식에 대응하는 소프트 벨만 백업 연산자 <span class="math math-display">\mathcal{T}^\pi</span>는 수축 사상(contraction mapping)임이 증명되었다. 이는 바나흐 고정점 정리에 의해, 이 연산자를 임의의 Q 함수에 반복적으로 적용하면 유일한 고정점, 즉 참된 소프트 Q-값 <span class="math math-display">Q^\pi</span>로 수렴함을 보장한다.17 이는 SAC의 크리틱 학습 과정이 이론적으로 수렴 안정성을 가짐을 의미한다.</p>
<h3>2.3  네트워크 학습 및 최적화</h3>
<p>SAC는 리플레이 버퍼 <span class="math math-display">\mathcal{D}</span>에서 샘플링된 미니배치를 사용하여 각 네트워크의 파라미터를 업데이트한다.</p>
<h4>3.3.1 소프트 Q-네트워크 (크리틱) 학습</h4>
<p>크리틱 네트워크 <span class="math math-display">Q_\theta</span>는 소프트 벨만 방정식을 기반으로, 평균 제곱 벨만 오차(Mean Squared Bellman Error, MSBE)를 최소화하도록 학습된다. 이 과정은 동시대의 SOTA 알고리즘인 TD3의 핵심 아이디어를 차용하여 안정성을 극대화한다. Q-러닝 계열 알고리즘의 고질적인 문제인 Q-값 과대추정(overestimation)을 완화하기 위해, <strong>Clipped Double Q-Learning</strong> 기법을 사용한다. 즉, 두 개의 독립적인 Q-네트워크(<span class="math math-display">Q_{\theta_1}, Q_{\theta_2}</span>)를 학습시키고, 타겟 Q-값을 계산할 때는 둘 중 더 작은 값을 사용한다.5</p>
<p>Q-네트워크의 손실 함수는 다음과 같다.5<br />
<span class="math math-display">
J_Q(\theta_i) = \mathbb{E}_{(\mathbf{s}_t, \mathbf{a}_t, r_t, \mathbf{s}_{t+1}) \sim \mathcal{D}} \left[ \frac{1}{2} \left( Q_{\theta_i}(\mathbf{s}_t, \mathbf{a}_t) - y(\mathbf{s}_{t+1}, r_t) \right)^2 \right], \quad i=1,2
</span><br />
여기서 타겟 값 <span class="math math-display">y</span>는 타겟 네트워크들을 사용하여 계산되며, 다음과 같이 정의된다.<br />
<span class="math math-display">
y(\mathbf{s}_{t+1}, r_t) = r_t + \gamma \left( \min_{j=1,2} Q_{\theta_{\text{targ},j}}(\mathbf{s}_{t+1}, \tilde{\mathbf{a}}_{t+1}) - \alpha \log \pi_\phi(\tilde{\mathbf{a}}_{t+1}|\mathbf{s}_{t+1}) \right), \quad \tilde{\mathbf{a}}_{t+1} \sim \pi_\phi(\cdot|\mathbf{s}_{t+1})
</span><br />
타겟 네트워크의 파라미터 <span class="math math-display">\theta_{\text{targ}}</span>는 주 네트워크의 파라미터 <span class="math math-display">\theta</span>를 느리게 따라가도록 Polyak averaging을 통해 업데이트된다: <span class="math math-display">\theta_{\text{targ}} \leftarrow \tau \theta + (1-\tau) \theta_{\text{targ}}</span>.16</p>
<h4>2.3.1  정책 네트워크 (액터) 학습</h4>
<p>액터 네트워크 <span class="math math-display">\pi_\phi</span>는 기대 소프트 Q-값을 최대화하는 방향으로 업데이트된다. 이는 정책 분포 <span class="math math-display">\pi_\phi(\cdot|\mathbf{s}_t)</span>와 소프트 Q-값의 지수 함수에 비례하는 분포 사이의 KL-발산(Kullback-Leibler divergence)을 최소화하는 것과 동일하다.16<br />
<span class="math math-display">
\pi_{\text{new}} = \arg \min_{\pi&#39; \in \Pi} D_{KL} \left( \pi&#39;(\cdot|\mathbf{s}_t) \left\| \frac{\exp(\frac{1}{\alpha}Q^{\pi_{\text{old}}}(\mathbf{s}_t, \cdot))}{Z^{\pi_{\text{old}}}(\mathbf{s}_t)} \right. \right)
</span><br />
이 최적화 문제를 풀기 위해, 정책의 샘플링 과정이 미분 가능해야 한다. SAC는 이를 위해 <strong>Reparameterization Trick</strong>을 사용한다.5 정책 네트워크가 가우시안 분포의 평균 <span class="math math-display">\mu_\phi(\mathbf{s}_t)</span>과 표준편차 <span class="math math-display">\sigma_\phi(\mathbf{s}_t)</span>를 출력하면, 행동 <span class="math math-display">\mathbf{a}_t</span>는 다음과 같이 파라미터 <span class="math math-display">\phi</span>와 무관한 노이즈 <span class="math math-display">\epsilon_t</span>의 함수로 표현된다.<br />
<span class="math math-display">
\mathbf{a}_t = f_\phi(\epsilon_t; \mathbf{s}_t) = \mu_\phi(\mathbf{s}_t) + \sigma_\phi(\mathbf{s}_t) \odot \epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0, I)
</span><br />
이를 통해 정책 손실 함수를 기댓값 형태로 다시 쓸 수 있으며, 경사 하강법으로 최적화할 수 있다. 액터는 두 Q-네트워크 중 더 작은 값을 사용하여 업데이트되는데, 이는 액터가 Q-값의 과대추정된 부분을 악용하는 것을 방지한다.5 최종적인 정책 손실 함수는 다음과 같다.19<br />
<span class="math math-display">
J_\pi(\phi) = \mathbb{E}_{\mathbf{s}_t \sim \mathcal{D}, \epsilon_t \sim \mathcal{N}} \left[ \alpha \log \pi_\phi(f_\phi(\epsilon_t; \mathbf{s}_t)|\mathbf{s}_t) - \min_{i=1,2} Q_{\theta_i}(\mathbf{s}_t, f_\phi(\epsilon_t; \mathbf{s}_t)) \right]
</span></p>
<h4>3.3.3 온도 파라미터 <span class="math math-display">\alpha</span> 자동 튜닝</h4>
<p>탐험과 활용의 균형을 결정하는 중요한 하이퍼파라미터인 <span class="math math-display">\alpha</span>를 수동으로 설정하는 것은 매우 번거롭다. SAC의 후속 연구에서는 <span class="math math-display">\alpha</span>를 학습 가능한 변수로 취급하여 자동으로 최적화하는 기법을 제안했다.20 이는 강화학습 문제를 기대 누적 보상을 최대화하되, 평균 엔트로피가 사전에 정의된 목표 엔트로피 <span class="math math-display">\mathcal{H}_0</span> 이상이어야 한다는 제약 최적화 문제로 재구성함으로써 이루어진다.<br />
<span class="math math-display">
\max_{\pi} \mathbb{E} \left[ \sum_t r_t \right] \quad \text{s.t.} \quad \mathbb{E}_{(\mathbf{s}_t, \mathbf{a}_t) \sim \rho_\pi} [-\log \pi(\mathbf{a}_t|\mathbf{s}_t)] \ge \mathcal{H}_0 \quad \forall t
</span><br />
이 문제의 라그랑주 쌍대 문제(Lagrangian dual problem)를 풀면, <span class="math math-display">\alpha</span>에 대한 다음과 같은 손실 함수를 유도할 수 있다.5<br />
<span class="math math-display">
J(\alpha) = \mathbb{E}_{\mathbf{a}_t \sim \pi_t} \left[ -\alpha \log \pi_t(\mathbf{a}_t|\mathbf{s}_t) - \alpha \mathcal{H}_0 \right]
</span><br />
이 손실 함수를 경사 하강법으로 최소화함으로써, 매 스텝마다 정책의 현재 엔트로피와 목표 엔트로피 간의 차이를 줄이는 방향으로 <span class="math math-display">\alpha</span>가 자동으로 조절된다. 이는 SAC의 자율성과 강건성을 크게 향상시키는 핵심적인 혁신이다.4</p>
<h3>2.4  안정성을 위한 추가 기법: Squashing 함수</h3>
<p>실제 로봇 제어와 같은 많은 문제에서 행동은 특정 범위 내로 제한되어야 한다(예: 모터 토크는 <span class="math math-inline">[-1, 1]</span>). SAC는 정책 네트워크의 최종 출력에 쌍곡탄젠트(hyperbolic tangent, <span class="math math-inline">tanh</span>) 함수를 적용하여 이 문제를 해결한다.5 이 <strong>squashing 함수</strong>는 행동을 유한한 범위로 매끄럽게 제한한다. 하지만 이는 정책의 확률 분포를 변형시키므로, 로그 확률을 계산할 때 야코비안 행렬식(Jacobian determinant)을 고려한 보정이 필요하다.5</p>
<p>결론적으로, SAC의 아키텍처는 단일 혁신이라기보다는, 동시대 SOTA 알고리즘들의 핵심 아이디어를 최대 엔트로피라는 통일된 원리 아래 유기적으로 통합한 ’종합 예술’에 가깝다. TD3로부터 Clipped Double Q-learning을 차용하여 안정적인 크리틱을, DDPG로부터 Off-policy Actor-Critic 구조를 계승하여 샘플 효율성을 확보했다. 여기에 확률적 정책과 엔트로피 최대화라는 독자적인 철학을 더해, TD3가 ’타겟 정책 평활화’라는 별도의 기법으로 달성하려던 효과를 더 근본적인 방식으로 내재화했다. 마지막으로 <span class="math math-display">\alpha</span> 자동 튜닝 메커니즘은 알고리즘의 사용 편의성과 강건성을 완성하는 화룡점정이라 할 수 있다.</p>
<h2>제 4장: 비교 분석: 주요 알고리즘과의 관계 설정</h2>
<p>SAC의 혁신성을 제대로 평가하기 위해서는 동시대의 다른 주요 강화학습 알고리즘들과의 비교를 통해 그 위치를 명확히 할 필요가 있다. 본 장에서는 On-policy의 대표주자인 PPO, 그리고 Off-policy의 경쟁자인 TD3/DDPG와 SAC를 심층적으로 비교 분석한다.</p>
<h3>4.1 SAC vs. PPO: Off-Policy와 On-Policy의 대결</h3>
<p>SAC와 PPO는 Actor-Critic 계열의 알고리즘이라는 공통점을 가지지만, 데이터를 사용하는 방식에서 근본적인 차이를 보인다. 이 차이는 두 알고리즘의 성능 특성을 결정하는 가장 중요한 요인이다.</p>
<h4>4.1.1 샘플 효율성 (Sample Efficiency)</h4>
<p>가장 두드러진 차이점은 샘플 효율성이다. SAC는 <strong>Off-policy</strong> 알고리즘으로, 과거의 모든 상호작용 기록(<span class="math math-inline">(s, a, r, s&#39;)</span> 튜플)을 리플레이 버퍼에 저장하고 학습에 반복적으로 재사용한다.4 이는 데이터 수집 비용이 비싼 환경, 예를 들어 실제 로봇을 작동시키거나 복잡한 물리 시뮬레이션을 실행하는 경우에 결정적인 장점이 된다.4 반면, PPO는 <strong>On-policy</strong> 알고리즘으로, 현재 정책에 의해 수집된 데이터 궤적(trajectory)만을 사용하여 파라미터를 업데이트하고, 업데이트가 끝나면 해당 데이터를 폐기한다.23 따라서 PPO는 SAC에 비해 동일한 성능에 도달하기 위해 훨씬 더 많은 환경과의 상호작용을 필요로 한다.</p>
<h4>4.1.2 안정성 및 구현 복잡성 (Stability &amp; Implementation Complexity)</h4>
<p>안정성 측면에서는 전통적으로 On-policy 알고리즘이 더 우수하다고 알려져 있다. PPO는 신뢰 영역(trust region) 내에서 정책 업데이트 크기를 제한하는 <strong>클리핑(clipping)</strong> 기법을 통해, 파괴적인(destabilizing) 업데이트를 방지하고 매우 안정적인 학습 과정을 보장한다.24 또한, PPO는 상대적으로 구현이 간단하고 직관적이라는 장점이 있다. SAC는 Off-policy 학습 고유의 불안정성(예: 현재 정책과 과거 데이터 간의 분포 불일치)을 내포하고 있지만, 최대 엔트로피 프레임워크, Clipped Double Q-learning, 타겟 네트워크 등 다양한 안정화 장치를 통해 PPO에 버금가는 높은 안정성을 달성했다.2 다만, 여러 네트워크와 복잡한 손실 함수로 인해 PPO보다는 구현이 복잡하다.</p>
<h4>4.1.3 탐험 전략 (Exploration Strategy)</h4>
<p>두 알고리즘 모두 탐험을 장려하기 위해 엔트로피를 활용하지만, 그 방식과 철학에 차이가 있다. SAC는 목적 함수 자체에 엔트로피 최대화 항을 명시적으로 포함시켜, 탐험을 학습의 핵심 목표 중 하나로 삼는다.25 이는 에이전트가 보상 신호가 희박하거나 복잡한 환경에서도 꾸준히 새로운 행동을 시도하도록 체계적으로 유도한다. PPO 역시 손실 함수에 엔트로피 보너스를 정규화 항(regularization term)으로 추가하여 정책이 너무 빨리 결정론적으로 수렴하는 것을 막지만, 이는 SAC만큼 탐험을 중심에 둔 접근법은 아니다.4</p>
<h3>4.2 SAC vs. TD3/DDPG: 확률적 정책의 우위</h3>
<p>SAC와 TD3/DDPG는 모두 연속 행동 공간을 위한 Off-policy Actor-Critic 알고리즘이라는 점에서 공통분모를 가지지만, 정책의 형태와 안정화 전략에서 중요한 차이를 보인다.</p>
<h4>4.2.1 정책 유형 및 탐험 (Policy Type &amp; Exploration)</h4>
<p>SAC는 <strong>확률적(stochastic) 정책</strong>을 학습하는 반면, TD3와 DDPG는 <strong>결정론적(deterministic) 정책</strong>을 학습한다. 결정론적 정책은 주어진 상태에 대해 항상 동일한 행동을 출력하므로, 탐험을 위해서는 행동에 직접적으로 노이즈(예: Ornstein-Uhlenbeck noise)를 주입하는 별도의 과정이 필요하다.26 이 노이즈의 크기와 스케줄을 튜닝하는 것은 종종 어려운 문제가 된다. 반면, SAC의 확률적 정책은 본질적으로 행동의 분포를 학습하므로, 이 분포로부터 행동을 샘플링하는 것만으로 자연스럽고 체계적인 탐험이 이루어진다.</p>
<h4>4.2.2 타겟 정책 평활화 (Target Policy Smoothing)</h4>
<p>TD3는 DDPG의 불안정성을 개선하기 위해 여러 기법을 도입했는데, 그중 하나가 **타겟 정책 평활화(Target Policy Smoothing)**이다. 이는 타겟 Q-값을 계산할 때 다음 상태의 행동에 작은 노이즈를 추가하여, Q 함수의 값 추정이 특정 행동에 대해 뾰족한 스파이크(spike) 형태를 띠는 것을 방지하고 함수 지형을 전반적으로 평탄하게 만드는 정규화 기법이다.27 SAC는 이러한 명시적인 평활화 기법을 사용하지 않는다. 그러나 최대 엔트로피 목적 함수에 따라 학습된 SAC의 확률적 정책은 본질적으로 유사한 효과를 낳는다. 엔트로피를 최대화하려는 경향은 정책이 단일 행동에만 높은 확률을 부여하는 것을 막고, 유사한 가치를 가진 여러 행동에 대해 확률을 분산시키도록 유도한다. 이는 결과적으로 Q 함수가 다양한 행동에 대한 가치를 학습하게 만들어, TD3의 타겟 정책 평활화와 같은 효과를 내재적으로 달성하게 된다.5</p>
<h4>4.2.3 성능 및 안정성 (Performance &amp; Stability)</h4>
<p>다수의 표준 벤치마크 환경(예: MuJoCo)에서 SAC는 TD3 및 DDPG보다 더 높은 최종 성능을 더 안정적으로 달성하는 경향이 입증되었다.28 특히, DDPG가 하이퍼파라미터에 극도로 민감하여 재현성이 낮았던 문제를 SAC는 크게 개선하여, 다른 랜덤 시드에서도 일관된 성능을 보여준다.2 TD3는 DDPG에 비해 안정성이 크게 향상되었지만, 여전히 SAC가 전반적으로 더 강건하고 높은 성능을 보이는 경우가 많다.30</p>
<h3>4.3 핵심 비교 요약</h3>
<p>아래 표는 SAC, TD3, PPO 세 가지 알고리즘의 핵심적인 설계 철학과 기술적 특징을 요약하여 비교한다. 이 표는 각 알고리즘이 어떤 문제 상황에 더 적합한지, 그리고 어떤 기술적 트레이드오프를 가지고 있는지 직관적으로 파악하는 데 도움을 준다.</p>
<table><thead><tr><th>특성 (Feature)</th><th>Soft Actor-Critic (SAC)</th><th>Twin Delayed DDPG (TD3)</th><th>Proximal Policy Optimization (PPO)</th></tr></thead><tbody>
<tr><td><strong>패러다임</strong></td><td>Off-Policy, Actor-Critic</td><td>Off-Policy, Actor-Critic</td><td>On-Policy, Actor-Critic</td></tr>
<tr><td><strong>정책 유형</strong></td><td>확률적 (Stochastic)</td><td>결정론적 (Deterministic)</td><td>확률적 (Stochastic)</td></tr>
<tr><td><strong>핵심 아이디어</strong></td><td>최대 엔트로피 RL</td><td>과대추정 편향 완화</td><td>신뢰 영역(Trust Region) 기반 안정적 업데이트</td></tr>
<tr><td><strong>탐험 전략</strong></td><td>엔트로피 최대화</td><td>행동에 노이즈 추가</td><td>정책의 확률적 샘플링</td></tr>
<tr><td><strong>샘플 효율성</strong></td><td>매우 높음 (리플레이 버퍼)</td><td>높음 (리플레이 버퍼)</td><td>상대적으로 낮음</td></tr>
<tr><td><strong>안정성</strong></td><td>높음</td><td>DDPG보다 개선됨</td><td>매우 높음</td></tr>
<tr><td><strong>Q-함수 업데이트</strong></td><td>Clipped Double Q-Learning</td><td>Clipped Double Q-Learning</td><td>Generalized Advantage Estimation (GAE)</td></tr>
<tr><td><strong>주요 적용 분야</strong></td><td>연속적 행동 공간 (로보틱스)</td><td>연속적 행동 공간</td><td>이산적/연속적 행동 공간</td></tr>
</tbody></table>
<h2>제 5장: 실증적 성능과 응용</h2>
<p>알고리즘의 이론적 우수성은 결국 실증적 성능으로 입증되어야 한다. SAC는 제안된 이래로 수많은 표준 벤치마크와 실제 세계 문제에 적용되어 그 효과를 입증해왔다. 특히 데이터 수집 비용이 높고 안정성이 중요한 로보틱스 분야에서 두각을 나타내었다.</p>
<h3>5.1 벤치마크 성능 분석</h3>
<p>SAC는 MuJoCo, DeepMind Control Suite, OpenAI Gym 등 널리 사용되는 연속 제어 벤치마크 환경에서 뛰어난 성능을 보여주었다. 초기 논문들에서부터 SAC는 PPO, TD3, DDPG 등 당시의 SOTA 알고리즘들과 비교되었으며, 대부분의 환경에서 더 빠른 학습 속도(샘플 효율성)와 더 높은 최종 성능(asymptotic performance)을 달성했다.7 예를 들어, “Ant“나 “Humanoid“와 같이 고차원적이고 복잡한 제어를 요구하는 환경에서 SAC의 성능 우위는 더욱 두드러졌다.28 또한, 여러 번의 독립적인 실행에서 성능 편차가 적어 학습의 안정성과 재현성이 높다는 점도 중요한 강점으로 평가받았다.2</p>
<h3>5.2 실제 세계 적용: 로보틱스</h3>
<p>SAC의 진정한 가치는 시뮬레이션을 넘어 실제 로봇에 적용되었을 때 나타난다. 실제 로봇 학습은 데이터 수집에 많은 시간과 비용이 소요되고, 잘못된 탐험은 하드웨어 손상을 유발할 수 있으며, 시뮬레이션과 현실 사이의 차이(sim-to-real gap) 등 여러 제약 조건을 가진다. SAC의 설계 철학은 이러한 실제 세계의 제약 조건들과 정확히 부합한다. Off-policy 학습은 수집된 데이터를 최대한 재활용하여 샘플 효율성을 극대화하고, 최대 엔트로피 프레임워크는 부드러운 탐험을 유도하며 학습된 정책의 강건성을 높여준다.3</p>
<h4>5.2.1 Minitaur 사족보행 로봇</h4>
<p>Google과 UC Berkeley의 연구팀은 SAC를 사용하여 실제 <strong>Minitaur 사족보행 로봇</strong>이 걷는 법을 처음부터 학습시키는 데 성공했다.3 이 과업은 8개의 액추에이터를 정교하게 조종하여 균형을 잡고 전진해야 하는 어려운 문제이다. 놀랍게도 SAC는 단 <strong>2시간</strong>의 실제 로봇 상호작용만으로 안정적인 보행 정책을 학습해냈다.3 이는 On-policy 알고리즘으로는 달성하기 매우 어려운 수준의 샘플 효율성이다. 더욱 인상적인 점은, 평평한 지형에서만 훈련했음에도 불구하고 학습된 정책이 이전에 경험하지 못한 다양한 지형이나 장애물이 있는 환경에서도 넘어지지 않고 강건하게 작동했다는 것이다. 이는 최대 엔트로피 학습이 정책의 일반화 성능과 강건성을 향상시킨다는 이론을 강력하게 뒷받침하는 실증적 증거이다.3</p>
<h4>5.2.2 Dynamixel Claw 밸브 조작</h4>
<p>또 다른 주목할 만한 사례는 9자유도(9-DoF)를 가진 <strong>Dynamixel Claw 로봇팔</strong>을 이용한 밸브 조작 과업이다.3 이 과업에서 로봇은 원시 RGB 카메라 이미지 입력만을 사용하여 밸브의 현재 방향을 인식하고, 특정 방향으로 돌려야 했다. 이는 고차원 시각 정보 처리와 정교한 매니퓰레이션 제어가 결합된 매우 도전적인 문제이다. SAC는 이 복잡한 과업을</p>
<p><strong>20시간</strong> 만에 성공적으로 학습했다. 비교를 위해, 이미지 정보 대신 실제 밸브의 각도 정보를 직접 입력으로 제공한 더 쉬운 버전의 과업은 단 <strong>3시간</strong> 만에 학습을 완료했다. 이는 동일한 과업(이미지 정보 제외)을 이전 연구에서 자연 정책 경사(Natural Policy Gradients, NPG)를 사용하여 학습하는 데 7.4시간이 걸렸던 것과 비교하면 2배 이상 빠른 속도이다.3 이 사례들은 SAC가 고차원 관측 공간에서도 효율적으로 학습할 수 있음을 보여준다.</p>
<h3>5.3 자율주행으로의 확장</h3>
<p>SAC의 적용 범위는 로보틱스를 넘어 자율주행 분야로도 확장되고 있다. 자율주행, 특히 도심 환경에서의 의사결정은 다른 차량, 보행자, 자전거 등 예측 불가능한 동적 장애물들로 인해 매우 복잡하다. 최근 연구에서는 SAC를 사용하여 자율주행 차량이 복잡한 교통 상황, 특히 여러 차량이 얽혀 있는 **원형 교차로(roundabout)**에 안전하고 부드럽게 진입하는 문제를 해결하는 데 성공적으로 적용했다.32 CARLA 시뮬레이터를 이용한 실험에서 SAC 기반 에이전트는 DQN이나 PPO와 같은 다른 알고리즘에 비해 더 높은 성공률과 안전성을 보였다.32 이는 SAC가 동적인 환경 변화에 효과적으로 대응하고, 장기적인 보상과 단기적인 안전 사이의 균형을 맞추는 복잡한 의사결정을 내릴 수 있는 잠재력을 가지고 있음을 시사한다.</p>
<h2>제 6장: 심화 주제 및 미래 연구 방향</h2>
<p>SAC는 연속 제어 분야에서 표준적인 알고리즘으로 자리 잡았지만, 그 연구는 여기서 멈추지 않고 다양한 방향으로 확장 및 발전하고 있다. 이 장에서는 SAC를 이산적 행동 공간으로 확장하려는 노력, 가치 분포를 학습하는 새로운 패러다임의 도입, 그리고 생성 모델과의 결합 등 최신 연구 동향을 살펴본다. 이러한 진화의 방향은 강화학습 분야 전체의 거시적인 연구 트렌드를 반영한다.</p>
<h3>6.1 이산적 행동 공간으로의 확장 (Discrete SAC)</h3>
<h4>6.1.1 도전 과제</h4>
<p>SAC는 연속 행동 공간에서 괄목할 만한 성공을 거두었지만, 이를 Atari 게임과 같은 **이산적 행동 공간(discrete action space)**에 적용하려는 초기 시도는 기대에 미치지 못했다. 많은 연구에서 바닐라 이산 SAC(vanilla discrete SAC)가 학습 불안정성과 저조한 성능을 보인다고 보고했다.34 그 원인으로는 여러 가설이 제기되었는데, 주요 원인으로는</p>
<p><strong>Q-값의 과소평가(Q-value underestimation)</strong> 문제와 액터-크리틱 간의 불안정한 상호작용이 지목된다.36 연속 공간에서는 reparameterization trick을 통해 정책 업데이트가 Q 함수에 미치는 영향이 간접적인 반면, 이산 공간에서는 정책 확률의 작은 변화가 Q-값 추정에 직접적이고 급격한 변화를 유발하여 학습을 불안정하게 만들 수 있다.</p>
<h4>6.1.2 개선 연구</h4>
<p>이러한 문제들을 해결하기 위해 다양한 변형된 이산 SAC 알고리즘들이 제안되었다.</p>
<ul>
<li><strong>Stable Discrete SAC (SDSAC):</strong> 이 연구는 이산 SAC의 불안정성 원인을 ’불안정한 결합 훈련(Unstable Coupling Training)’과 ’비관적 탐험(Pessimistic Exploration)’으로 진단했다. 이를 해결하기 위해, 정책 손실에 <strong>엔트로피 페널티 항</strong>을 추가하고, Q-값 업데이트에 <strong>Double Average Q-learning</strong>과 <strong>Q-clip</strong> 기법을 도입하여 Q-값의 과소평가를 완화하고 학습 안정성을 크게 향상시켰다.36</li>
<li><strong>Target Entropy Scheduled SAC (TES-SAC):</strong> <span class="math math-display">\alpha</span> 자동 튜닝에 사용되는 목표 엔트로피(<span class="math math-display">\mathcal{H}_0</span>)를 고정된 상수가 아닌, 학습 초기에는 높게 설정하고 점차 감소시키는 <strong>어닐링(annealing)</strong> 스케줄을 적용하는 방법을 제안했다. 이는 학습 초기에 충분한 탐험을 보장하고, 후반부에는 정책을 수렴시켜 안정성을 높이는 효과를 가져온다.35</li>
<li><strong>SAC-BBF:</strong> 최근 연구에서는 이산 SAC를 Rainbow DQN의 최신 변형인 BBF(Bootstrap-based Belief Function)와 결합하여, Atari 100k 벤치마크에서 기존의 가치 기반 방법들을 능가하는 SOTA 샘플 효율성을 달성했다. 이는 정책 기반 Off-policy 학습이 이산 영역에서도 충분한 잠재력을 가짐을 보여준다.37</li>
</ul>
<p>이러한 연구들은 SAC의 핵심 원리를 이산적 행동 공간의 특성에 맞게 조정함으로써, 그 적용 범위를 성공적으로 확장하고 있다.38</p>
<h3>2.5  가치 분포 학습 (Distributional SAC)</h3>
<p>전통적인 강화학습은 누적 보상의 <strong>기댓값</strong>, 즉 Q-값을 학습하는 데 초점을 맞춘다. 그러나 실제 보상은 확률적인 환경과 정책으로 인해 하나의 값이 아닌, 특정 <strong>확률 분포</strong>를 따른다. **분포 강화학습(Distributional RL)**은 이 전체 확률 분포를 직접 모델링하여, 불확실성에 대한 더 풍부한 정보를 학습하고 이를 통해 더 나은 의사결정을 내리고자 하는 패러다임이다.39 이는 특히 최악의 경우를 피해야 하는</p>
<p><strong>위험 민감(risk-sensitive)</strong> 제어 문제에서 중요하다.</p>
<p>**Distributional Soft Actor-Critic (DSAC)**은 SAC에 분포 RL 개념을 결합한 알고리즘이다.39 DSAC는 크리틱이 단일 Q-값을 출력하는 대신, 가치 분포를 나타내는 파라미터(예: 가우시안 분포의 평균과 분산, 또는 분위수(quantile) 집합)를 출력하도록 설계된다. 이를 통해 다음과 같은 이점을 얻는다.</p>
<ul>
<li><strong>정확한 가치 추정:</strong> 가치 분포를 학습함으로써 시스템의 내재적 무작위성과 함수 근사 오차로 인한 가치 추정의 불확실성을 더 잘 포착할 수 있다. 한 연구에서는 Q-값 과대추정 편향이 가치 분포의 분산에 반비례함을 수학적으로 보여, 분산을 학습하는 것이 편향을 완화하는 데 도움이 됨을 입증했다.42</li>
<li><strong>위험 민감 제어:</strong> 학습된 가치 분포를 이용하여 CVaR(Conditional Value at Risk)과 같은 다양한 위험 척도를 최적화할 수 있다. 이를 통해 에이전트는 단순히 평균 보상을 높이는 것을 넘어, 치명적인 실패(낮은 보상)의 확률을 최소화하는 등 더 안전하고 강건한 정책을 학습할 수 있다.39</li>
</ul>
<h3>2.6  생성 모델과의 결합: Diffusion Policy 기반 SAC</h3>
<p>SAC의 또 다른 한계는 정책을 가우시안과 같은 단일 모드(uni-modal) 분포로 가정한다는 점이다. 이는 로봇이 문을 밀어서 열거나 당겨서 여는 것처럼, 동등하게 좋은 여러 가지 해결책이 존재하는 <strong>다중 모드(multi-modal)</strong> 과업을 학습하는 데 어려움을 겪을 수 있다.</p>
<p>이러한 표현력의 한계를 극복하기 위해, 최근 이미지 생성 분야에서 놀라운 성공을 거둔 **확산 모델(Diffusion Model)**을 정책 네트워크로 도입하려는 연구가 활발히 진행되고 있다.43 확산 모델은 노이즈로부터 데이터를 점진적으로 생성해내는 강력한 생성 모델로, 복잡한 다중 모드 분포를 표현하는 데 매우 뛰어나다.</p>
<p>**DSAC-D (Distributional Soft Actor-Critic with Diffusion Policy)**는 가치 함수와 정책 네트워크 모두를 확산 모델로 대체한 최신 알고리즘이다.45</p>
<ul>
<li><strong>확산 가치 네트워크(Diffusion Value Network):</strong> 크리틱은 확산 모델을 사용하여 다중 모드 가치 분포를 직접 생성한다. 이는 기존의 가우시안 가정보다 훨씬 더 유연하고 표현력 높은 방식으로 가치 분포를 모델링하여, 가치 추정 편향을 크게 줄인다.47</li>
<li><strong>확산 정책 네트워크(Diffusion Policy Network):</strong> 액터 역시 확산 모델로 구성되어, 주어진 상태에 대해 다중 모드 행동 분포를 생성할 수 있다. 이를 통해 에이전트는 Ant 로봇의 다양한 걸음걸이나 자율주행 차량의 여러 주행 스타일과 같은 복잡하고 다양한 행동 양식을 학습할 수 있다.45</li>
</ul>
<p>실험 결과, DSAC-D는 MuJoCo 벤치마크에서 기존의 모든 주류 알고리즘을 능가하는 SOTA 성능을 달성했으며, 특히 가치 추정 편향을 현저히 억제하는 효과를 보였다.45 이는 SAC의 진화가 강화학습의 목표를 단순한 성능 향상에서 ‘안전’, ‘신뢰성’, 그리고 ’다양성’으로 확장하는 거시적인 연구 흐름과 궤를 같이하고 있음을 보여준다.</p>
<h2>3.  결론</h2>
<p>Soft Actor-Critic(SAC)은 심층 강화학습 분야에서 중요한 이정표를 제시한 알고리즘이다. 이는 단순히 새로운 기술적 기교의 조합을 넘어, <strong>최대 엔트로피</strong>라는 일관된 이론적 프레임워크를 통해 강화학습의 오랜 난제였던 <strong>샘플 효율성, 학습 안정성, 그리고 효과적인 탐험</strong>이라는 세 마리 토끼를 동시에 잡으려는 시도였다. 그 결과, SAC는 Off-policy 학습의 데이터 효율성과 On-policy 학습의 안정성 사이의 간극을 성공적으로 메웠으며, 특히 연속 제어 문제에서 새로운 표준을 정립했다.</p>
<p>본 안내서를 통해 고찰한 바와 같이, SAC의 성공은 여러 핵심 요소의 유기적인 결합에 기인한다. 첫째, 최대 엔트로피 목적 함수는 탐험을 체계적으로 장려하고, 학습된 정책의 강건성을 높이며, 최적화 지형을 평탄화하여 학습 과정을 안정시킨다. 둘째, 확률적 정책과 Reparameterization Trick의 사용은 효과적인 탐험과 안정적인 정책 업데이트를 가능하게 했다. 셋째, Clipped Double Q-Learning과 타겟 네트워크와 같은 기존 SOTA 기법들을 비판적으로 수용하여 크리틱 학습의 안정성을 확보했다. 마지막으로, 온도 파라미터 <span class="math math-display">\alpha</span>의 자동 튜닝 메커니즘은 알고리즘의 실용성과 강건성을 한 단계 끌어올렸다.</p>
<p>이러한 이론적 강점은 실제 세계 응용, 특히 로보틱스 분야에서 그 가치를 입증했다. SAC는 데이터 수집 비용이 비싸고 예측 불가능성이 높은 실제 물리 환경에서 단 몇 시간 만에 복잡한 보행 및 조작 기술을 학습시키는 데 성공하며, 심층 강화학습의 실용화 가능성을 한층 앞당겼다. 현재 SAC는 연속 제어 분야, 특히 로보틱스 연구에서 가장 널리 사용되고 신뢰받는 표준 알고리즘 중 하나로 확고히 자리매김했다.</p>
<p>SAC의 여정은 여기서 끝나지 않는다. 이산적 행동 공간으로의 성공적인 확장, 불확실성을 더 정교하게 다루기 위한 분포적 접근법(DSAC)의 도입, 그리고 다중 모드 행동의 표현력을 극대화하기 위한 생성 모델(DSAC-D)과의 결합은 SAC가 여전히 진화하고 있음을 보여준다. 이러한 미래 연구 방향은 강화학습이 더 복잡하고, 더 불확실하며, 더 다양한 해법이 존재하는 현실 세계의 문제들을 해결하기 위해 나아가야 할 길을 제시한다. 결론적으로, Soft Actor-Critic은 그 자체로도 강력한 알고리즘일 뿐만 아니라, 미래의 더 정교하고 안전하며 지능적인 에이전트를 개발하기 위한 풍부한 영감과 견고한 토대를 제공하는 중요한 학문적 자산이라 평가할 수 있다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/abs/1801.01290</li>
<li>Soft Actor-Critic: - Proceedings of Machine Learning Research, 8월 24, 2025에 액세스, https://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf</li>
<li>Soft Actor-Critic: Deep Reinforcement Learning for Robotics, 8월 24, 2025에 액세스, https://research.google/blog/soft-actor-critic-deep-reinforcement-learning-for-robotics/</li>
<li>Soft Actor-Critic Reinforcement Learning algorithm - GeeksforGeeks, 8월 24, 2025에 액세스, https://www.geeksforgeeks.org/deep-learning/soft-actor-critic-reinforcement-learning-algorithm/</li>
<li>Soft Actor-Critic — Spinning Up documentation, 8월 24, 2025에 액세스, https://spinningup.openai.com/en/latest/algorithms/sac.html</li>
<li>Maximum Entropy Policies in Reinforcement Learning &amp; Everyday Life - Arthur Juliani, PhD, 8월 24, 2025에 액세스, https://awjuliani.medium.com/maximum-entropy-policies-in-reinforcement-learning-everyday-life-f5a1cc18d32d</li>
<li>SAC: A Soft Touch to Efficient Reinforcement Learnings | by Dong-Keon Kim | Medium, 8월 24, 2025에 액세스, https://medium.com/@kdk199604/sac-a-soft-touch-to-efficient-reinforcement-learnings-c89acb1cb6b8</li>
<li>[PDF] Soft Actor-Critic Algorithms and Applications - Semantic Scholar, 8월 24, 2025에 액세스, https://www.semanticscholar.org/paper/Soft-Actor-Critic-Algorithms-and-Applications-Haarnoja-Zhou/12c0751b4f51ed833172a713b7e32390032ead93</li>
<li>Entropy in Soft Actor-Critic (Part 1) | by Rafael Stekolshchik | TDS Archive | Medium, 8월 24, 2025에 액세스, https://medium.com/data-science/entropy-in-soft-actor-critic-part-1-92c2cd3a3515</li>
<li>Deriving Soft Actor Critic (SAC) - Ignat Georgiev, 8월 24, 2025에 액세스, http://www.imgeorgiev.com/2023-06-27-sac/</li>
<li>Soft Actor-Critic (SAC) | Practical Reinforcement Learning for Robotics and AI, 8월 24, 2025에 액세스, https://www.reinforcementlearningpath.com/soft-actor-critic-sac/</li>
<li>How does entropy regularization improve exploration? - Milvus, 8월 24, 2025에 액세스, https://milvus.io/ai-quick-reference/how-does-entropy-regularization-improve-exploration</li>
<li>Understanding the Impact of Entropy on Policy Optimization, 8월 24, 2025에 액세스, https://proceedings.mlr.press/v97/ahmed19a.html</li>
<li>Understanding the Impact of Entropy on Policy Optimization - Proceedings of Machine Learning Research, 8월 24, 2025에 액세스, https://proceedings.mlr.press/v97/ahmed19a/ahmed19a.pdf</li>
<li>The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/html/2505.22617v1</li>
<li>Soft Actor-Critic Demystified. An intuitive explanation of the theory …, 8월 24, 2025에 액세스, https://medium.com/data-science/soft-actor-critic-demystified-b8427df61665</li>
<li>Why soft Bellman backup operator is converged in tabular settings? - AI Stack Exchange, 8월 24, 2025에 액세스, https://ai.stackexchange.com/questions/45452/why-soft-bellman-backup-operator-is-converged-in-tabular-settings</li>
<li>Maximum Entropy Reinforcement Learning - andrew.cmu.ed, 8월 24, 2025에 액세스, https://andrew.cmu.edu/course/10-403/slides/S19maxentRL.pdf</li>
<li>What is the gradient of the objective function in the Soft Actor-Critic paper?, 8월 24, 2025에 액세스, https://ai.stackexchange.com/questions/10549/what-is-the-gradient-of-the-objective-function-in-the-soft-actor-critic-paper</li>
<li>[1812.05905] Soft Actor-Critic Algorithms and Applications - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/abs/1812.05905</li>
<li>SAC - Ted Staley, 8월 24, 2025에 액세스, https://www.tedstaley.com/posts/sac/sac.html</li>
<li>Mastering Soft Actor-Critic in ML - Number Analytics, 8월 24, 2025에 액세스, https://www.numberanalytics.com/blog/ultimate-guide-to-soft-actor-critic</li>
<li>Are there any papers or theories on why SAC is better for continuous control tasks than on-policy methods? - Reddit, 8월 24, 2025에 액세스, https://www.reddit.com/r/reinforcementlearning/comments/y2af2i/are_there_any_papers_or_theories_on_why_sac_is/</li>
<li>PPO: Efficient, Stable, and Scalable Policy Optimization | by Dong-Keon Kim | Medium, 8월 24, 2025에 액세스, https://medium.com/@kdk199604/ppo-efficient-stable-and-scalable-policy-optimization-15b5b9c74a88</li>
<li>AWS DeepRacer training algorithms, 8월 24, 2025에 액세스, https://docs.aws.amazon.com/deepracer/latest/developerguide/deepracer-how-it-works-reinforcement-learning-algorithm.html</li>
<li>DDPG vs PPO vs SAC: when to use? : r/reinforcementlearning - Reddit, 8월 24, 2025에 액세스, https://www.reddit.com/r/reinforcementlearning/comments/holioy/ddpg_vs_ppo_vs_sac_when_to_use/</li>
<li>Twin Delayed Deep Deterministic Policy Gradient (TD3) | by Hey Amit - Medium, 8월 24, 2025에 액세스, https://medium.com/@heyamit10/twin-delayed-deep-deterministic-policy-gradient-td3-fc8e9950f029</li>
<li>Learning efficiency comparison of various RL algorithms. Four kinds of… - ResearchGate, 8월 24, 2025에 액세스, https://www.researchgate.net/figure/Learning-efficiency-comparison-of-various-RL-algorithms-Four-kinds-of-RL-algorithms_fig5_359411539</li>
<li>A Comparison of PPO, TD3 and SAC Reinforcement Algorithms for Quadruped Walking Gait Generation - Scientific Research Publishing, 8월 24, 2025에 액세스, https://www.scirp.org/journal/paperinformation?paperid=123401</li>
<li>DD-PPO, TD3, SAC: which is the best? : r/reinforcementlearning - Reddit, 8월 24, 2025에 액세스, https://www.reddit.com/r/reinforcementlearning/comments/r83umm/ddppo_td3_sac_which_is_the_best/</li>
<li>Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots – The Berkeley Artificial Intelligence Research Blog, 8월 24, 2025에 액세스, https://bair.berkeley.edu/blog/2018/12/14/sac/</li>
<li>Enhancing Autonomous Driving Navigation Using Soft Actor-Critic - ResearchGate, 8월 24, 2025에 액세스, https://www.researchgate.net/publication/381965264_Enhancing_Autonomous_Driving_Navigation_Using_Soft_Actor-Critic</li>
<li>Enhancing Autonomous Driving Navigation Using Soft Actor-Critic - MDPI, 8월 24, 2025에 액세스, https://www.mdpi.com/1999-5903/16/7/238</li>
<li>DSAC-C: Constrained Maximum Entropy for Robust Discrete Soft-Actor Critic - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/pdf/2310.17173</li>
<li>[2112.02852] Target Entropy Annealing for Discrete Soft Actor-Critic - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/abs/2112.02852</li>
<li>Revisiting Discrete Soft Actor-Critic | OpenReview, 8월 24, 2025에 액세스, https://openreview.net/forum?id=EUF2R6VBeU</li>
<li>Generalizing soft actor-critic algorithms to discrete action spaces - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/html/2407.11044v1</li>
<li>[PDF] Soft Actor-Critic for Discrete Action Settings - Semantic Scholar, 8월 24, 2025에 액세스, https://www.semanticscholar.org/paper/Soft-Actor-Critic-for-Discrete-Action-Settings-Christodoulou/0a0866ec7180bbf87b1c87ed48bd4fa00574b814</li>
<li>DSAC: Distributional Soft Actor-Critic for Risk-Sensitive Reinforcement Learning - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/html/2004.14547v3</li>
<li>Distributional Soft Actor Critic for Risk Sensitive Learning - Semantic Scholar, 8월 24, 2025에 액세스, https://www.semanticscholar.org/paper/Distributional-Soft-Actor-Critic-for-Risk-Sensitive-Ma-Zhang/a28bb7633c93ffdf34f3f45f659a0c6a8d4e05d9</li>
<li>(PDF) DSAC: Distributional Soft Actor-Critic for Risk-Sensitive Reinforcement Learning, 8월 24, 2025에 액세스, https://www.researchgate.net/publication/393067955_DSAC_Distributional_Soft_Actor-Critic_for_Risk-Sensitive_Reinforcement_Learning</li>
<li>Distributional Soft Actor-Critic with Three Refinements - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/html/2310.05858v5</li>
<li>Discrete Policy: Learning Disentangled Action Space for Multi-Task Robotic Manipulation - OpenReview, 8월 24, 2025에 액세스, https://openreview.net/pdf/20f631ce953fc90bc9622ec7dcad96ec9c147ca9.pdf</li>
<li>Discrete Policy: Learning Disentangled Action Space for Multi-Task Robotic Manipulation, 8월 24, 2025에 액세스, https://arxiv.org/html/2409.18707v2</li>
<li>[2507.01381] Distributional Soft Actor-Critic with Diffusion Policy - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/abs/2507.01381</li>
<li>[Literature Review] Distributional Soft Actor-Critic with Diffusion Policy - Moonlight, 8월 24, 2025에 액세스, https://www.themoonlight.io/en/review/distributional-soft-actor-critic-with-diffusion-policy</li>
<li>(PDF) Distributional Soft Actor-Critic with Diffusion Policy - ResearchGate, 8월 24, 2025에 액세스, https://www.researchgate.net/publication/393332712_Distributional_Soft_Actor-Critic_with_Diffusion_Policy</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>