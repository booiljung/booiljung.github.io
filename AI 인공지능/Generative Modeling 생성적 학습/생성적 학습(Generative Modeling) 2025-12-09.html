<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:생성적 학습(Generative Modeling)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>생성적 학습(Generative Modeling)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">생성적 학습 (Generative Modeling)</a> / <span>생성적 학습(Generative Modeling)</span></nav>
                </div>
            </header>
            <article>
                <h1>생성적 학습(Generative Modeling)</h1>
<p>2025-12-09, G30DR</p>
<h2>1.  서론: 데이터의 인식에서 창조로의 패러다임 전환</h2>
<p>인공지능(AI)의 역사는 오랜 기간 동안 데이터를 분류하고 예측하는 판별적 모델(Discriminative Model)이 주도해 왔다. 입력된 이미지에서 고양이와 개를 구분하거나, 스팸 메일을 걸러내는 등의 작업은 주어진 데이터 <span class="math math-inline">x</span>에 대해 정답 레이블 <span class="math math-inline">y</span>가 무엇인지를 맞추는 조건부 확률 <span class="math math-inline">P(Y|X)</span>의 최적화 문제였다. 그러나 최근 몇 년간 AI 연구의 최전선은 데이터의 분포 자체를 학습하여 새로운 데이터 인스턴스를 창조해내는 **생성적 학습(Generative Modeling)**으로 급격히 이동하였다. 생성적 모델은 단순히 입력값을 레이블로 매핑하는 것을 넘어, 데이터가 존재하는 공간의 본질적인 결합 확률 분포 <span class="math math-inline">P(X, Y)</span> 또는 <span class="math math-inline">P(X)</span>를 모델링함으로써, 데이터가 생성되는 원리 그 자체를 이해하려고 시도한다.1</p>
<p>이러한 패러다임의 전환은 단순한 기술적 진보를 넘어 기계가 ’상상’하고 ’창조’할 수 있는 능력을 갖추게 되었음을 시사한다. 초기 통계적 모델에서 시작하여 적대적 생성 신경망(GAN), 변분 오토인코더(VAE)를 거쳐, 최근의 확산 모델(Diffusion Models)과 자기회귀 트랜스포머(Autoregressive Transformers)에 이르기까지, 생성 모델은 텍스트, 이미지, 오디오, 비디오, 그리고 3D 공간에 이르기까지 모든 모달리티를 정복해 나가고 있다. 특히 2024년과 2025년을 기점으로 생성 모델은 단순한 미디어 생성을 넘어 물리 법칙을 이해하고 시뮬레이션하는 ’월드 모델(World Model)’로서의 가능성을 보여주고 있으며, 이는 인공 일반 지능(AGI)으로 향하는 중요한 이정표로 평가받는다.3</p>
<p>본 보고서는 생성적 학습의 수학적 기초부터 시작하여, 현재 기술 생태계를 지배하고 있는 최신 아키텍처(Rectified Flow, Diffusion Transformer 등)의 작동 원리를 심층적으로 분석한다. 또한, 텍스트-투-비디오(Text-to-Video), 멀티모달 Any-to-Any 생성 등 최신 SOTA(State-of-the-Art) 모델들의 기술적 특성을 상세히 조사하고, 이를 평가하기 위한 정량적 지표와 벤치마크 시스템을 다룬다. 마지막으로, 기술의 발전 속도만큼이나 중요하게 대두되고 있는 저작권, 환각(Hallucination), 딥페이크 등 윤리적·법적 쟁점들을 포괄적으로 고찰함으로써 생성 AI 기술의 현재와 미래를 조망한다.</p>
<h2>2.  생성 모델링의 핵심 이론 및 수학적 기초</h2>
<h3>2.1  생성 모델과 판별 모델의 통계적 차이</h3>
<p>기계 학습 모델은 확률적 관점에서 데이터를 처리하는 방식에 따라 크게 생성 모델과 판별 모델로 구분된다. 이 두 접근 방식은 데이터를 바라보는 관점과 학습의 최종 목표에서 근본적인 차이를 보인다.</p>
<p>**판별 모델(Discriminative Model)**은 데이터 <span class="math math-inline">x</span>가 주어졌을 때 레이블 <span class="math math-inline">y</span>가 나타날 조건부 확률 <span class="math math-inline">P(Y|X)</span>를 직접적으로 모델링한다. 예를 들어, 로지스틱 회귀(Logistic Regression)나 서포트 벡터 머신(SVM), 그리고 전통적인 CNN 기반의 분류기들이 이에 속한다. 이들의 목표는 클래스 간의 결정 경계(Decision Boundary)를 찾아내는 것이다.4 판별 모델은 데이터가 생성된 원리에는 관심이 없으며, 오직 입력과 출력 사이의 매핑 관계에만 집중하기 때문에 분류나 회귀 문제에서 일반적으로 더 높은 정확도와 적은 연산 비용을 보인다.6</p>
<p>반면, **생성 모델(Generative Model)**은 데이터의 결합 확률 분포 <span class="math math-inline">P(X, Y)</span>를 학습하거나, 레이블이 없는 경우 데이터 자체의 분포 <span class="math math-inline">P(X)</span>를 학습한다. 베이즈 정리(Bayes’ Theorem)에 따르면 <span class="math math-inline">P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}</span>이므로, 생성 모델은 이론적으로 <span class="math math-inline">P(X|Y)</span>(우도, Likelihood)와 <span class="math math-inline">P(Y)</span>(사전 확률, Prior)를 추정하여 판별 작업을 수행할 수도 있다.1 하지만 생성 모델의 진정한 가치는 학습된 확률 분포에서 새로운 샘플 <span class="math math-inline">\hat{x} \sim P(X)</span>를 추출(Sampling)할 수 있다는 점에 있다. 이는 모델이 데이터 클래스 간의 경계뿐만 아니라, 각 클래스가 데이터 공간 상에서 어떻게 분포하고 있는지를 파악해야 함을 의미한다. 따라서 생성 모델은 판별 모델보다 훨씬 더 복잡한 정보를 학습해야 하며, 이는 더 많은 데이터와 연산 자원을 필요로 하는 원인이 된다.2</p>
<table><thead><tr><th><strong>특성</strong></th><th><strong>생성 모델 (Generative Model)</strong></th><th><strong>판별 모델 (Discriminative Model)</strong></th></tr></thead><tbody>
<tr><td><strong>학습 목표</strong></td><td>결합 확률 <span class="math math-inline">P(X, Y)</span> 또는 <span class="math math-inline">P(X)</span> 추정</td><td>조건부 확률 <span class="math math-inline">P(Y|X</span> 추정</td></tr>
<tr><td><strong>핵심 기능</strong></td><td>새로운 데이터 생성, 분포 학습, 결측치 추정</td><td>데이터 분류, 레이블 예측, 결정 경계 학습</td></tr>
<tr><td><strong>수학적 관계</strong></td><td>베이즈 정리를 통해 분류기 유도 가능</td><td>생성 모델 유도 불가</td></tr>
<tr><td><strong>장점</strong></td><td>데이터의 구조적 이해, 비지도 학습 용이</td><td>높은 예측 정확도, 빠른 학습 및 추론 속도</td></tr>
<tr><td><strong>단점</strong></td><td>높은 계산 복잡도, 학습의 어려움, 이상치에 민감</td><td>데이터 생성 불가, 데이터 구조에 대한 이해 부족</td></tr>
<tr><td><strong>대표 알고리즘</strong></td><td>GAN, VAE, Diffusion, Flow-based, Naive Bayes</td><td>Logistic Regression, SVM, Random Forest, CNN, Transformer(Encoder-only)</td></tr>
</tbody></table>
<h3>2.2  다양체 가설(Manifold Hypothesis)과 잠재 공간(Latent Space)</h3>
<p>생성 모델이 고차원 데이터(예: <span class="math math-inline">1024 \times 1024</span> 픽셀의 이미지)를 다룰 수 있는 이론적 근거는 **다양체 가설(Manifold Hypothesis)**에 있다. 이 가설은 “고차원 공간에 존재하는 실제 데이터는 그보다 훨씬 낮은 차원의 매니폴드(Manifold) 상에, 혹은 그 근방에 집중되어 있다“는 가정이다.7 예를 들어, 무작위로 생성된 픽셀 노이즈는 전체 픽셀 공간을 가득 채우지만, 의미 있는 ’이미지’는 그 공간의 극히 일부분인 저차원 다양체에만 존재한다.</p>
<p>생성 모델의 학습 과정은 고차원 데이터 공간 <span class="math math-inline">\mathcal{X}</span>를 저차원의 <strong>잠재 공간(Latent Space)</strong> <span class="math math-inline">\mathcal{Z}</span>로 매핑(Encoding)하고, 다시 <span class="math math-inline">\mathcal{Z}</span>에서 <span class="math math-inline">\mathcal{X}</span>로 복원(Decoding)하거나 변환하는 함수를 찾는 과정으로 해석될 수 있다. 여기서 잠재 변수 <span class="math math-inline">z \in \mathcal{Z}</span>는 원본 데이터의 가장 중요한 특징(Feature)들을 압축적으로 표현한다.8</p>
<p>잠재 공간의 기하학적 특성은 생성 모델의 성능을 결정짓는 중요한 요소다. 이상적인 잠재 공간은 다음 두 가지 특성을 가져야 한다:</p>
<ol>
<li><strong>연속성(Continuity):</strong> 잠재 공간 상에서 가까운 두 점 <span class="math math-inline">z_1</span>과 <span class="math math-inline">z_2</span>는 디코딩되었을 때 의미적으로 유사한 데이터 <span class="math math-inline">x_1</span>과 <span class="math math-inline">x_2</span>를 생성해야 한다.</li>
<li><strong>완전성(Completeness):</strong> 잠재 공간 내의 임의의 점을 샘플링하더라도, 디코딩 결과는 유효하고 의미 있는 데이터여야 한다.9</li>
</ol>
<p>이러한 특성 덕분에 **잠재 공간 보간(Latent Space Interpolation)**이 가능하다. 두 잠재 벡터 <span class="math math-inline">z_A</span>와 <span class="math math-inline">z_B</span> 사이를 선형(Linear) 또는 구면(Spherical) 보간할 때, 생성되는 이미지가 두 이미지의 특징을 부드럽게 혼합하며 변형된다면, 이는 모델이 단순히 데이터를 암기한 것이 아니라 데이터 다양체의 구조를 학습했음을 입증한다.10 만약 잠재 공간이 매끄럽지 않다면, 보간 과정에서 갑자기 이미지가 붕괴하거나 전혀 다른 형태가 나타날 수 있다.</p>
<h2>3.  생성 모델 아키텍처의 진화와 메커니즘</h2>
<p>생성 모델은 확률 분포를 모델링하는 방식에 따라 크게 우도 기반(Likelihood-based) 모델과 암시적(Implicit) 모델로 나뉘며, 최근에는 이 둘의 장점을 결합하고 물리적 확산 과정을 도입한 모델들이 주류를 이루고 있다.</p>
<h3>3.1  Variational Autoencoders (VAEs): 확률적 압축과 생성</h3>
<p>VAE는 오토인코더(Autoencoder) 구조에 통계적 기법을 도입하여 생성 능력을 부여한 모델이다. 일반적인 오토인코더가 입력 <span class="math math-inline">x</span>를 고정된 벡터 <span class="math math-inline">z</span>로 압축한다면, VAE는 <span class="math math-inline">x</span>를 잠재 공간의 확률 분포(주로 다변량 정규분포)로 매핑한다.</p>
<ul>
<li>
<p><strong>메커니즘:</strong> 인코더 <span class="math math-inline">q_\phi(z|x)</span>는 입력 <span class="math math-inline">x</span>로부터 잠재 분포의 파라미터인 평균 <span class="math math-inline">\mu</span>와 분산 <span class="math math-inline">\sigma^2</span>을 추정한다. 그런 다음 이 분포에서 <span class="math math-inline">z</span>를 샘플링(<span class="math math-inline">z = \mu + \sigma \odot \epsilon</span>, 여기서 <span class="math math-inline">\epsilon \sim \mathcal{N}(0, I)</span>)하여 디코더 <span class="math math-inline">p_\theta(x|z)</span>에 전달하고, 디코더는 원본 <span class="math math-inline">x</span>를 복원한다. 이 과정에서 **재매개변수화 트릭(Reparameterization Trick)**을 사용하여 역전파(Backpropagation)가 가능하게 한다.12</p>
</li>
<li>
<p>손실 함수: VAE는 ELBO(Evidence Lower Bound)를 최대화하는 방향(또는 손실을 최소화하는 방향)으로 학습한다.</p>
<p>$$ \mathcal{L}{\text{VAE}} = \mathbb{E}{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) |</p>
</li>
</ul>
<p>| p(z)) $$</p>
<p>첫 번째 항은 재구성 오차(Reconstruction Loss)를 최소화하여 데이터의 충실도를 높이고, 두 번째 항인 KL 발산(Kullback-Leibler Divergence)은 잠재 분포가 사전 분포(표준 정규분포)와 유사해지도록 규제(Regularization)한다.13</p>
<ul>
<li><strong>한계:</strong> VAE는 수학적으로 탄탄하고 학습이 안정적이지만, 생성된 이미지가 다소 흐릿(Blurry)하게 나오는 경향이 있다. 이는 픽셀 단위의 L2 손실 함수 사용과 가우시안 가정에 의한 평균화 효과 때문이다.14</li>
</ul>
<h3>3.2  Generative Adversarial Networks (GANs): 적대적 경쟁을 통한 학습</h3>
<p>2014년 Ian Goodfellow가 제안한 GAN은 생성자(Generator, <span class="math math-inline">G</span>)와 판별자(Discriminator, <span class="math math-inline">D</span>)라는 두 신경망의 경쟁을 통해 데이터를 생성한다.</p>
<ul>
<li>
<p><strong>메커니즘:</strong> 생성자 <span class="math math-inline">G</span>는 랜덤 노이즈 <span class="math math-inline">z</span>를 입력받아 가짜 데이터 <span class="math math-inline">G(z)</span>를 생성하여 판별자를 속이려 한다. 반면 판별자 <span class="math math-inline">D</span>는 입력된 데이터가 실제 데이터 <span class="math math-inline">x</span>인지 가짜 데이터 <span class="math math-inline">G(z)</span>인지 구별하려고 한다. 이 과정은 2인 제로섬 게임(Two-player Zero-sum Game)으로 모델링된다.1</p>
<p><span class="math math-display">\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)} + \mathbb{E}_{z \sim p_z(z)}</span></p>
</li>
<li>
<p><strong>장점과 한계:</strong> GAN은 VAE보다 훨씬 선명하고 사실적인 이미지를 생성할 수 있다. 그러나 학습 과정이 매우 불안정하며, 생성자와 판별자의 균형이 깨지기 쉽다. 가장 큰 문제는 <strong>모드 붕괴(Mode Collapse)</strong> 현상으로, 생성자가 판별자를 속이기 쉬운 특정 소수의 이미지만 반복적으로 생성하여 데이터의 다양성을 잃어버리는 것이다.16 또한, 손실 함수가 수렴했는지 판단하기 어렵고 하이퍼파라미터 튜닝이 까다롭다.13</p>
</li>
</ul>
<h3>3.3 Autoregressive Models (Transformer): 시퀀스 예측의 힘</h3>
<p>자기회귀 모델은 데이터의 시퀀스 의존성을 학습한다. 텍스트 생성의 표준인 GPT 계열이 여기에 속하며, 이미지 생성(예: PixelCNN, Parton)에도 적용된다.</p>
<ul>
<li>메커니즘: 데이터 <span class="math math-inline">x</span>를 일련의 토큰 시퀀스 <span class="math math-inline">x = (x_1, x_2, \dots, x_T)</span>로 간주하고, 이전 토큰들이 주어졌을 때 다음 토큰의 조건부 확률들의 곱으로 결합 확률을 모델링한다.</li>
</ul>
<p><span class="math math-display">p(x) = \prod_{t=1}^T p(x_t | x_1, \dots, x_{t-1})</span></p>
<p>트랜스포머(Transformer) 아키텍처의 어텐션 메커니즘(Attention Mechanism)을 활용하여 긴 거리의 의존성을 효과적으로 학습한다.18</p>
<ul>
<li><strong>특징:</strong> 명시적인 밀도 추정(Explicit Density Estimation)이 가능하고 학습이 안정적이다. 그러나 순차적으로 데이터를 생성해야 하므로, 이미지나 비디오와 같은 고차원 데이터 생성 시 병렬화가 어렵고 추론 속도가 느리다는 단점이 있다.19</li>
</ul>
<h3>3.3  Diffusion Models (확산 모델): 노이즈 제거를 통한 생성</h3>
<p>확산 모델은 비평형 열역학에서 영감을 받아, 데이터에 서서히 노이즈를 추가하여 파괴하는 과정(Forward Process)과 이를 역으로 복원하여 데이터를 생성하는 과정(Reverse Process)을 학습한다. 현재 이미지 및 비디오 생성 분야의 SOTA(State-of-the-Art) 모델들은 대부분 확산 모델을 기반으로 한다.20</p>
<ul>
<li>
<p>Forward Process (확산 과정): 데이터 <span class="math math-inline">x_0</span>에 시간 <span class="math math-inline">t</span>에 따라 점진적으로 가우시안 노이즈를 추가한다. <span class="math math-inline">t</span>가 충분히 크면 데이터는 완전한 등방성 가우시안 노이즈 <span class="math math-inline">x_T \sim \mathcal{N}(0, I)</span>가 된다. 이 과정은 파라미터가 고정된 마르코프 체인(Markov Chain)이다.</p>
<p><span class="math math-display">q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I)</span></p>
</li>
<li>
<p>Reverse Process (역확산 과정): 노이즈 <span class="math math-inline">x_T</span>에서 시작하여, 신경망을 통해 노이즈를 조금씩 제거해가며 <span class="math math-inline">x_0</span>를 복원한다. 신경망은 각 시점 <span class="math math-inline">t</span>에서 추가된 노이즈 <span class="math math-inline">\epsilon_\theta(x_t, t)</span>를 예측하도록 학습된다.</p>
</li>
</ul>
<p><span class="math math-display">p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))</span></p>
<ul>
<li>
<p><strong>Score-based Generative Modeling:</strong> 확산 모델은 데이터 분포의 로그 밀도 함수의 기울기(Score Function, <span class="math math-inline">\nabla_x \log p(x)</span>)를 추정하는 방식(Score Matching)으로도 해석된다. 이는 SDE(Stochastic Differential Equation)를 통해 연속적인 시간 영역으로 확장될 수 있다.22</p>
</li>
<li>
<p><strong>Latent Diffusion Model (LDM):</strong> 픽셀 공간에서 직접 확산을 수행하는 것은 연산 비용이 매우 높다. Stable Diffusion과 같은 LDM은 VAE를 사용하여 이미지를 저차원 잠재 공간으로 압축한 후, 그 공간에서 확산 과정을 수행함으로써 효율성과 품질을 동시에 확보했다.20</p>
</li>
</ul>
<h3>3.4  Rectified Flow와 Flow Matching: 확산 모델의 가속화</h3>
<p>확산 모델의 주요 단점은 고품질 샘플 생성을 위해 수십에서 수천 번의 반복적인 노이즈 제거 단계(Step)가 필요하다는 것이다. 이를 해결하기 위해 등장한 것이 <strong>Rectified Flow</strong>와 <strong>Flow Matching</strong>이다.</p>
<ul>
<li>
<p><strong>개념:</strong> 기존 확산 모델의 노이즈 제거 경로(Trajectory)는 구불구불한 곡선 형태를 띠어, 이를 따라가기 위해 많은 단계가 필요했다. Rectified Flow는 데이터 분포 <span class="math math-inline">\pi_0</span>와 노이즈 분포 <span class="math math-inline">\pi_1</span>를 잇는 경로를 **직선(Straight Line)**이 되도록 학습한다.25</p>
</li>
<li>
<p>수식화: Rectified Flow는 데이터 <span class="math math-inline">X_0</span>와 노이즈 <span class="math math-inline">X_1</span> 사이를 선형 보간 <span class="math math-inline">X_t = tX_1 + (1-t)X_0</span>으로 정의하고, 이 경로를 따르는 ODE(Ordinary Differential Equation)의 속도장(Velocity Field) <span class="math math-inline">v(Z_t, t)</span>를 학습한다.</p>
<p><span class="math math-display">\min_\theta \mathbb{E}_{t, X_0, X_1} [ \| (X_1 - X_0) - v_\theta(X_t, t) \|^2 ]</span></p>
<p>이 손실 함수는 모델이 <span class="math math-inline">X_0</span>에서 <span class="math math-inline">X_1</span>으로 가는 직선 경로의 기울기(방향)를 예측하도록 강제한다.27</p>
</li>
<li>
<p><strong>이점:</strong> 직선 경로는 ODE 솔버가 훨씬 더 큰 보폭(Step size)으로 적분할 수 있게 하여, 단 몇 번의 스텝(예: 4~8 스텝)만으로도 고품질 이미지를 생성할 수 있게 한다. 이는 Stable Diffusion 3(SD3)와 Flux 같은 최신 모델의 핵심 기술로 채택되었다.29</p>
</li>
</ul>
<h2>4. 심층 기술 분석 및 최신 방법론</h2>
<h3>4.1 Latent Consistency Models (LCM) 및 가속화 기술</h3>
<p>**Latent Consistency Models (LCM)**은 확산 모델의 추론 속도를 극단적으로 높이기 위해 개발되었다. LCM은 기존 확산 모델(Teacher)을 증류(Distillation)하여 만들어진다.</p>
<ul>
<li><strong>Consistency Distillation:</strong> LCM은 어떤 시점 <span class="math math-inline">t</span>에서의 노이즈 데이터 <span class="math math-inline">x_t</span>를 입력받더라도, 즉시 <span class="math math-inline">t=0</span> 시점의 원본 데이터 <span class="math math-inline">x_0</span>를 예측하도록(Self-Consistency) 학습된다. 이는 ODE 궤적을 따라가는 과정을 단축시켜 1~4 스텝 만에 이미지를 생성할 수 있게 한다.31</li>
<li><strong>Speculative Decoding (추측성 디코딩):</strong> LLM의 추론 가속화를 위한 기술로, 작고 빠른 ’초안 모델(Draft Model)’이 여러 토큰을 미리 생성하고, 크고 정확한 ’타겟 모델(Target Model)’이 이를 병렬로 검증하는 방식이다. 타겟 모델이 초안 모델의 예측을 승인(Accept)하면 한 번의 연산으로 여러 토큰을 생성한 효과를 얻는다.33 이는 메모리 대역폭의 병목을 해소하여 2배 이상의 속도 향상을 가져온다.35</li>
</ul>
<h3>4.2 모델 정렬(Alignment): RLHF에서 DPO로</h3>
<p>생성 모델이 인간의 의도와 윤리적 기준에 부합하는 결과를 내도록 조정하는 ’정렬(Alignment)’은 필수적이다.</p>
<ul>
<li>
<p><strong>RLHF (Reinforcement Learning from Human Feedback):</strong> 인간의 선호도 데이터를 기반으로 별도의 보상 모델(Reward Model)을 학습시키고, PPO(Proximal Policy Optimization)와 같은 강화학습 알고리즘을 사용해 생성 모델을 튜닝한다. 효과적이지만 학습 과정이 복잡하고 불안정하며 비용이 많이 든다.36</p>
</li>
<li>
<p>DPO (Direct Preference Optimization): 2024년 이후 대세가 된 DPO는 별도의 보상 모델 없이, 선호 데이터 쌍(Preferred vs Dispreferred)을 사용하여 생성 모델의 손실 함수를 직접 최적화한다.</p>
</li>
</ul>
<p><span class="math math-display">\mathcal{L}_{DPO} = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} [\log \sigma (\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)})]</span></p>
<p>확산 모델에 적용된 Diffusion-DPO는 이미지 생성 시 텍스트 프롬프트와의 정합성과 시각적 미학을 동시에 향상시키는 것으로 입증되었다.38</p>
<h3>3.5  KV Cache 최적화와 장기 문맥 처리</h3>
<p>LLM이 처리할 수 있는 문맥(Context)의 길이가 늘어남에 따라(예: Gemini 1.5 Pro의 100만 토큰), 추론 시 메모리 병목 현상이 심화되었다. 이를 해결하기 위해 <strong>KV Cache</strong> 최적화 기술이 발전하고 있다.</p>
<ul>
<li><strong>Prefill과 Generation:</strong> 모델은 입력 프롬프트를 처리하는 Prefill 단계에서 계산된 Key와 Value 벡터를 캐시에 저장하고, 이후 토큰 생성 단계에서 이를 재사용하여 중복 연산을 피한다.40</li>
<li><strong>최적화 기법:</strong> PagedAttention(운영체제의 가상 메모리 페이징 기법 차용), Quantized KV Cache(캐시 데이터를 저비트 정밀도로 압축), 그리고 KV Cache Aware Routing(캐시가 존재하는 GPU로 요청을 라우팅) 등의 기술이 적용되어 긴 문맥에서도 효율적인 추론을 가능하게 한다.41</li>
</ul>
<h2>4.  모달리티별 SOTA 모델 및 멀티모달 월드 시뮬레이터</h2>
<h3>4.1  텍스트-투-이미지 (Text-to-Image): Stable Diffusion 3와 Flux</h3>
<p>이미지 생성 분야는 정교한 텍스트 이해와 고화질 생성을 동시에 달성하는 방향으로 진화했다.</p>
<ul>
<li><strong>Stable Diffusion 3 (SD3):</strong> Stability AI가 발표한 SD3는 <strong>Multimodal Diffusion Transformer (MMDiT)</strong> 아키텍처를 도입했다. 텍스트와 이미지 임베딩에 대해 별도의 가중치(Weights)를 사용하면서도, 어텐션 메커니즘을 통해 양방향으로 정보가 흐를 수 있게 설계되었다. 이는 이전 모델들이 텍스트 이해(특히 텍스트 렌더링)에 취약했던 점을 획기적으로 개선했다.29 또한 앞서 언급한 Rectified Flow를 적용하여 샘플링 효율성을 극대화했다.44</li>
<li><strong>Flux:</strong> Black Forest Labs가 개발한 Flux 모델 역시 Flow Matching 기반의 대형 모델로, 현재 오픈 소스 이미지 생성 모델 중 최고의 퀄리티와 프롬프트 준수 능력을 보여주며 SD3와 경쟁하고 있다.</li>
</ul>
<h3>4.2  텍스트-투-비디오 (Text-to-Video): 월드 시뮬레이터로서의 Sora</h3>
<p>OpenAI의 Sora는 단순한 비디오 생성 모델을 넘어 **월드 시뮬레이터(World Simulator)**라는 새로운 비전을 제시했다.</p>
<ul>
<li><strong>Spacetime Patches:</strong> Sora는 비디오 데이터를 시공간 패치(Spacetime Patches)로 토큰화하여 처리한다. 이는 텍스트의 토큰과 유사하게 작동하여, 다양한 해상도, 비율, 길이의 비디오를 하나의 모델로 학습할 수 있게 한다.3</li>
<li><strong>창발적 시뮬레이션 능력:</strong> 대규모 데이터와 연산량을 통해 학습된 Sora는 명시적인 3D 모델링이나 물리 엔진 없이도 3D 공간의 일관성(3D Consistency), 가려진 객체의 영속성(Object Permanence), 그리고 기본적인 물리적 상호작용을 시뮬레이션하는 능력을 보여주었다.3 이는 비디오 생성 모델이 데이터의 통계적 패턴을 넘어 물리 세계의 인과관계를 내재적으로 학습할 수 있음을 시사한다.45</li>
<li><strong>경쟁 모델:</strong> 구글의 Veo, 런웨이(Runway)의 Gen-3 Alpha, 중국의 HunyuanVideo 등이 치열하게 경쟁하고 있으며, 이들 모두 물리적 정확성과 긴 영상 생성 능력 향상에 집중하고 있다.46</li>
</ul>
<h3>4.3  인터랙티브 환경 생성: Google Genie</h3>
<p>Google DeepMind의 Genie는 **생성적 인터랙티브 환경(Generative Interactive Environments)**이라는 새로운 범주를 열었다.</p>
<ul>
<li><strong>비지도 학습 기반 제어:</strong> Genie는 레이블이 없는 2D 플랫포머 게임 비디오만을 학습하여, 사용자가 조작 가능한 가상 세계를 생성한다. 핵심은 비디오에서 프레임 간의 변화를 분석하여 ’잠재 행동(Latent Action)’을 스스로 학습한다는 점이다. 이를 통해 사용자는 명시적인 행동 레이블 없이도 생성된 세계 내의 캐릭터를 제어할 수 있다.48 이는 게임 개발뿐만 아니라 로보틱스 에이전트 훈련을 위한 시뮬레이션 환경 구축에도 큰 잠재력을 가진다.</li>
</ul>
<h3>4.4  Any-to-Any 멀티모달 생성: NExT-GPT와 CoDi</h3>
<p>단일 모달리티의 한계를 넘어, 텍스트, 이미지, 오디오, 비디오를 자유롭게 오가는 모델들이 등장했다.</p>
<ul>
<li><strong>NExT-GPT:</strong> 거대 언어 모델(LLM)을 허브로 사용하여, 다양한 모달리티의 인코더와 디코더를 어댑터(Adapter)로 연결했다. 모달리티 간의 정렬 학습을 통해, 텍스트를 입력받아 비디오를 생성하고, 다시 그 비디오에 맞는 오디오를 생성하는 식의 유연한 작업이 가능하다. 특히 전체 파라미터의 1%만 튜닝하여 효율적인 학습을 달성했다.50</li>
<li><strong>CoDi (Composable Diffusion):</strong> 여러 모달리티의 확산 모델을 병렬로 구성하고, 공유된 잠재 공간을 통해 이들을 연결한다. 이를 통해 텍스트, 비디오, 오디오가 동기화된 복합 콘텐츠를 생성할 수 있다.52</li>
</ul>
<h2>5.  정량적 평가 지표 및 벤치마크</h2>
<p>생성 모델의 성능은 주관적일 수밖에 없으나, 객관적인 비교를 위한 다양한 지표들이 개발되었다.</p>
<h3>5.1  주요 평가 지표 (Metrics)</h3>
<table><thead><tr><th><strong>지표</strong></th><th><strong>전체 이름</strong></th><th><strong>설명 및 용도</strong></th><th><strong>장점 및 한계</strong></th></tr></thead><tbody>
<tr><td><strong>FID</strong></td><td>Fréchet Inception Distance</td><td>실제 이미지와 생성 이미지의 특징 벡터 분포 간의 거리를 측정(낮을수록 좋음).</td><td>품질과 다양성을 모두 반영하지만, Inception 모델에 의존적이며 인간의 지각과 완벽히 일치하지 않음.53</td></tr>
<tr><td><strong>IS</strong></td><td>Inception Score</td><td>생성된 이미지의 클래스 명확성과 다양성을 측정(높을수록 좋음).</td><td>실제 데이터 분포와의 거리를 측정하지 않는다는 한계가 있어 최근에는 FID가 더 선호됨.54</td></tr>
<tr><td><strong>FVD</strong></td><td>Fréchet Video Distance</td><td>FID를 비디오 도메인으로 확장. 시각적 품질과 시간적 일관성을 평가.55</td><td>비디오 생성 모델 평가의 표준 지표.</td></tr>
<tr><td><strong>FAD</strong></td><td>Fréchet Audio Distance</td><td>오디오 임베딩 분포 간의 거리 측정.</td><td>배경 잡음, 왜곡 등을 포함한 오디오 품질 평가에 사용.56</td></tr>
<tr><td><strong>CLIP Score</strong></td><td>-</td><td>텍스트 프롬프트와 생성된 이미지/비디오 간의 의미적 유사도(Cosine Similarity) 측정.</td><td>텍스트 정합성(Alignment) 평가에 필수적. 생성 품질 자체는 평가하지 못함.58</td></tr>
<tr><td><strong>Perplexity</strong></td><td>-</td><td>언어 모델이 다음 토큰을 예측할 때의 불확실성을 측정(낮을수록 좋음).</td><td>텍스트 생성 모델의 기본 지표. 실제 문장 생성 품질과는 괴리가 있을 수 있음.60</td></tr>
</tbody></table>
<h3>5.2  종합 벤치마크 (Benchmarks)</h3>
<p>단순한 지표를 넘어 모델의 종합적인 능력을 평가하는 벤치마크가 중요해지고 있다.</p>
<ul>
<li><strong>MME Benchmark:</strong> 멀티모달 LLM(MLLM)의 인지 및 지각 능력을 14개 하위 태스크(존재 유무, 개수 세기, 위치 파악, 상식 추론 등)로 평가한다. 데이터 오염을 방지하기 위해 수작업으로 구축된 데이터셋을 사용한다.62</li>
<li><strong>OmniGenBench:</strong> 이미지 생성 모델의 명령 수행 능력을 50개 이상의 태스크에 걸쳐 평가한다. 지각 중심(Perception-centric)과 인지 중심(Cognition-centric) 태스크로 나누어, 모델이 복잡한 지시사항을 얼마나 잘 따르는지 검증한다.64</li>
<li><strong>VBench:</strong> 비디오 생성 모델을 위해 품질, 일관성, 모션의 자연스러움 등 다양한 차원을 종합적으로 평가하는 벤치마크 툴킷이다.66</li>
</ul>
<h2>6.  윤리적, 법적 쟁점 및 사회적 과제</h2>
<p>생성 AI의 기술적 도약은 필연적으로 기존의 사회 시스템과 충돌하며 다양한 문제를 야기하고 있다.</p>
<h3>6.1  저작권 침해 및 공정 이용 논쟁</h3>
<p>생성 모델의 학습 데이터에 포함된 저작물에 대한 권리 문제는 전 세계적인 법적 분쟁의 중심에 있다.</p>
<ul>
<li><strong>주요 소송:</strong> 뉴욕타임즈(NYT)는 OpenAI와 Microsoft가 자사의 기사를 무단으로 학습하여 경쟁 제품(ChatGPT)을 만들었다며 소송을 제기했다. 이미지 생성 분야에서는 화가들이 Stability AI와 Midjourney 등을 상대로 집단 소송을 진행 중이다.67</li>
<li><strong>핵심 쟁점:</strong> AI 기업들은 저작물 학습이 ’공정 이용(Fair Use)’에 해당한다고 주장한다. 이들은 AI 학습이 원작을 그대로 복제하는 것이 아니라, 데이터의 패턴을 분석하여 새로운 창작물을 만드는 ’변형적 이용(Transformative Use)’이라고 강조한다. 반면 저작권자들은 AI 모델이 원작자의 스타일을 모방하거나 원작을 대체하는 결과물을 생성함으로써 잠재적 시장(Market)을 침해한다고 반박한다. 미국 저작권청(USCO)과 법원의 판결은 향후 AI 산업의 향방을 가를 중요한 분수령이 될 것이다.69</li>
</ul>
<h3>6.2  환각(Hallucination)과 신뢰성</h3>
<p>모델이 사실이 아닌 정보를 그럴듯하게 생성해내는 환각 현상은 생성 AI의 상용화에 가장 큰 걸림돌이다. 이는 학습 데이터의 편향, 압축된 지식의 손실, 또는 확률적 생성 과정의 본질적 특성에서 기인한다.</p>
<ul>
<li><strong>대응 기술:</strong> 검색 증강 생성(RAG)을 통해 외부 지식 베이스를 참조하게 하거나, 생성된 결과의 사실성을 검증(Fact-checking)하는 추가적인 레이어를 두는 방식이 연구되고 있다. 또한 모델 내부의 활성화 패턴을 분석하여 환각을 실시간으로 탐지하는 기술도 개발 중이다.72</li>
</ul>
<h3>6.3  규제와 안전성: EU AI Act</h3>
<p>유럽연합(EU)은 세계 최초로 포괄적인 AI 규제법인 <strong>EU AI Act</strong>를 제정했다.</p>
<ul>
<li><strong>내용:</strong> 범용 AI 모델(GPAI) 제공자는 모델 학습에 사용된 데이터의 상세한 요약을 공개하고, EU 저작권법을 준수해야 한다. 또한 딥페이크나 AI 생성 콘텐츠에는 워터마크 등을 통해 AI가 만들었음을 명시해야 한다(Transparency Obligations). 고위험(High-risk) AI 시스템에 대해서는 더욱 엄격한 위험 관리 및 품질 관리 의무가 부과된다.74 이는 기술 개발의 속도 조절과 투명성을 요구하는 강력한 신호로 작용하고 있다.</li>
</ul>
<h2>7.  결론 및 미래 전망: 시뮬레이션을 통한 지능의 확장</h2>
<p>생성적 학습은 데이터의 패턴을 모방하는 단계를 지나, 이제 물리 세계를 시뮬레이션하고 복잡한 인과관계를 이해하는 **월드 모델(World Model)**로 진화하고 있다. 트랜스포머와 확산 모델, 그리고 Rectified Flow의 결합은 생성 모델의 품질과 효율성을 동시에 끌어올렸으며, 텍스트, 비디오, 오디오를 아우르는 멀티모달 통합은 AI가 인간과 유사한 방식으로 세상을 인식하고 표현할 수 있는 길을 열었다.</p>
<p>향후 생성 AI 기술은 다음 세 가지 방향으로 발전할 것으로 전망된다.</p>
<ol>
<li><strong>물리적 세계와의 통합(Embodied AI):</strong> 비디오 생성 모델이 로봇 공학 및 자율 주행과 결합하여, 가상 시뮬레이션에서 학습한 지능을 실제 물리 세계의 행동으로 전이(Sim-to-Real)하는 연구가 가속화될 것이다.</li>
<li><strong>효율성의 극대화:</strong> 엣지 디바이스(Edge Device)에서도 구동 가능한 경량화 모델과 실시간 생성 기술이 보편화되어, 개인화된 AI 경험을 제공할 것이다.</li>
<li><strong>신뢰할 수 있는 AI:</strong> 저작권 문제를 해결할 수 있는 새로운 데이터 라이선싱 모델, 환각을 통제하는 기술, 그리고 윤리적 가이드라인 준수가 기술 개발의 핵심 전제 조건이 될 것이다.</li>
</ol>
<p>생성적 학습은 인공지능이 ’지각(Perception)’을 넘어 ’창조(Creation)’와 ’이해(Understanding)’의 영역으로 나아가는 핵심 엔진이다. 이 기술이 가져올 미래는 무한한 가능성을 품고 있지만, 그에 따른 책임과 사회적 합의 또한 무겁게 다가오고 있다. 우리는 지금 인공지능 역사의 가장 역동적인 변곡점을 지나고 있다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Generative model - Wikipedia, https://en.wikipedia.org/wiki/Generative_model</li>
<li>Background: What is a Generative Model? | Machine Learning - Google for Developers, https://developers.google.com/machine-learning/gan/generative</li>
<li>Video generation models as world simulators | OpenAI, https://openai.com/index/video-generation-models-as-world-simulators/</li>
<li>What is the difference between a generative and a discriminative algorithm? [closed], https://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-a-discriminative-algorithm</li>
<li>Decoding Generative and Discriminative Models - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2021/07/deep-understanding-of-discriminative-and-generative-models-in-machine-learning/</li>
<li>Generative vs Discriminative Models: Differences &amp; Use Cases - DataCamp, https://www.datacamp.com/blog/generative-vs-discriminative-models</li>
<li>Manifold hypothesis - Wikipedia, https://en.wikipedia.org/wiki/Manifold_hypothesis</li>
<li>A Comprehensive Guide to Latent Space in Machine Learning | by Amit Yadav | Biased-Algorithms | Medium, https://medium.com/biased-algorithms/a-comprehensive-guide-to-latent-space-in-machine-learning-b70ad51f1ff6</li>
<li>What Is Latent Space? | IBM, https://www.ibm.com/think/topics/latent-space</li>
<li>Latent Space Interpolation Is Powering the Next Wave of Generative AI | HackerNoon, https://hackernoon.com/latent-space-interpolation-is-powering-the-next-wave-of-generative-ai</li>
<li>Linear combinations of latents in diffusion models: interpolation and beyond - arXiv, https://arxiv.org/html/2408.08558v1</li>
<li>Generating Images using VAEs, GANs, and Diffusion Models - Towards Data Science, https://towardsdatascience.com/generating-images-using-vaes-gans-and-diffusion-models-48963ddeb2b2/</li>
<li>Generative Models in AI: A Comprehensive Comparison of GANs and VAEs, https://www.geeksforgeeks.org/deep-learning/generative-models-in-ai-a-comprehensive-comparison-of-gans-and-vaes/</li>
<li>Complete Guide to Five Generative AI Models - Coveo, https://www.coveo.com/blog/generative-models/</li>
<li>Comparing Diffusion, GAN, and VAE Techniques | by Roberto Iriondo | Generative AI Lab, https://medium.com/generative-ai-lab/a-tale-of-three-generative-models-comparing-diffusion-gan-and-vae-techniques-1423d5db5981</li>
<li>GAN Mode Collapse Explanation. A detailed analysis of the causes of… | by Ainur Gainetdinov | Towards AI, https://pub.towardsai.net/gan-mode-collapse-explanation-fa5f9124ee73</li>
<li>Comparative Analysis of Generative Models: Enhancing Image Synthesis with VAEs, GANs, and Stable Diffusion - arXiv, https://arxiv.org/html/2408.08751v1</li>
<li>11.9. Large-Scale Pretraining with Transformers - Dive into Deep Learning, http://d2l.ai/chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html?highlight=transformer</li>
<li>How do Transformers Perform In-Context Autoregressive Learning? - arXiv, https://arxiv.org/html/2402.05787v2</li>
<li>What are Diffusion Models? | Lil’Log, https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</li>
<li>Diffusion model - Wikipedia, https://en.wikipedia.org/wiki/Diffusion_model</li>
<li>Flow Matching vs Diffusion. Briefly going into mathematical… - Harsh Maheshwari, https://harshm121.medium.com/flow-matching-vs-diffusion-79578a16c510</li>
<li>An Introduction to Flow Matching and Diffusion Models, https://diffusion.csail.mit.edu/docs/lecture-notes.pdf</li>
<li>Understanding the Loss Functions of Latent Diffusion Model | by Preranabora - Medium, https://medium.com/@preranabora12/understanding-the-loss-functions-of-latent-diffusion-model-fe53a551fa14</li>
<li>Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - arXiv, https://arxiv.org/abs/2403.03206</li>
<li>Diffusion Meets Flow Matching, https://diffusionflow.github.io/</li>
<li>Rectified Flow - UT Austin Computer Science, https://www.cs.utexas.edu/~lqiang/rectflow/html/intro.html</li>
<li>Rectified Flows for Fast Multiscale Fluid Flow Modeling - arXiv, https://arxiv.org/html/2506.03111v1</li>
<li>Stable Diffusion 3: Research Paper - Stability AI, https://stability.ai/news/stable-diffusion-3-research-paper</li>
<li>Flow matching models vs (traditional) diffusion models, which one do you like better? : r/StableDiffusion - Reddit, https://www.reddit.com/r/StableDiffusion/comments/1m5d2t8/flow_matching_models_vs_traditional_diffusion/</li>
<li>How latent consistency models work - Baseten, https://www.baseten.co/blog/how-latent-consistency-models-work/</li>
<li>Latent Consistency Models (LCMs) Explained | by Abhinav Gopal - Medium, https://medium.com/@abhinavgopal_43342/latent-consistency-models-lcms-explained-3293f912694c</li>
<li>An Introduction to Speculative Decoding for Reducing Latency in AI Inference, https://developer.nvidia.com/blog/an-introduction-to-speculative-decoding-for-reducing-latency-in-ai-inference/</li>
<li>Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding - arXiv, https://arxiv.org/html/2401.07851v2</li>
<li>Speculative Decoding: The Clever Trick Making LLMs 2× Faster, https://sudhirpol522.medium.com/speculative-decoding-the-clever-trick-making-llms-2-faster-69a2adee98a7</li>
<li>Reinforcement learning from human feedback - Wikipedia, https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback</li>
<li>What is RLHF? - Reinforcement Learning from Human Feedback Explained - AWS, https://aws.amazon.com/what-is/reinforcement-learning-from-human-feedback/</li>
<li>Aligning Diffusion Models to Human Preferences - Salesforce, https://www.salesforce.com/blog/diffusion-dpo/</li>
<li>Diffusion-DPO: Aligned Diffusion Models - Emergent Mind, https://www.emergentmind.com/topics/diffusion-dpo</li>
<li>LLM Inference: KV Cache and Optimization | by Tushar Vatsa | Nov, 2025 - Towards AI, https://pub.towardsai.net/llm-inference-kv-cache-and-optimization-39650deee77c</li>
<li>Introducing New KV Cache Reuse Optimizations in NVIDIA TensorRT-LLM, https://developer.nvidia.com/blog/introducing-new-kv-cache-reuse-optimizations-in-nvidia-tensorrt-llm/</li>
<li>A Survey on Large Language Model Acceleration based on KV Cache Management - arXiv, https://arxiv.org/abs/2412.19442</li>
<li>Stable Diffusion 3: Multimodal Diffusion Transformer Model Explained - Encord, https://encord.com/blog/stable-diffusion-3-text-to-image-model/</li>
<li>Stable Diffusion 3 — Explained. How Rectified Flow and Transformers… | by Pietro Bolcato, https://medium.com/@pietrobolcato/stable-diffusion-3-explained-84fd085934cb</li>
<li>Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond, https://arxiv.org/html/2405.03520v2</li>
<li>Open-Sora 2.0: Training a Commercial-Level Video Generation Model in $200k - arXiv, https://arxiv.org/pdf/2503.09642</li>
<li>Text-to-video model - Wikipedia, https://en.wikipedia.org/wiki/Text-to-video_model</li>
<li>Genie: Generative Interactive Environments - arXiv, https://arxiv.org/html/2402.15391v1</li>
<li>myscience/open-genie: Pytorch implementation of “Genie: Generative Interactive Environments”, Bruce et al. (2024). - GitHub, https://github.com/myscience/open-genie</li>
<li>[2309.05519] NExT-GPT: Any-to-Any Multimodal LLM - arXiv, https://arxiv.org/abs/2309.05519</li>
<li>NExT-GPT: Any-to-Any Multimodal LLM - arXiv, https://arxiv.org/html/2309.05519v3</li>
<li>Any-to-Any Generation via Composable Diffusion - OpenReview, https://openreview.net/forum?id=2EDqbSCnmF</li>
<li>Evaluating Generative AI: A Comprehensive Guide with Metrics, Methods &amp; Visual Examples, https://medium.com/genusoftechnology/evaluating-generative-ai-a-comprehensive-guide-with-metrics-methods-visual-examples-2824347bfac3</li>
<li>Evaluation metrics for generative image models - SoftwareMill, https://softwaremill.com/evaluation-metrics-for-generative-image-models/</li>
<li>FVD: A NEW METRIC FOR VIDEO GENERATION - OpenReview, https://openreview.net/pdf?id=rylgEULtdN</li>
<li>Evaluating &amp; Finetuning Text-To-Audio Multimodal Models - Labellerr, https://www.labellerr.com/blog/enhancing-text-to-audio-multimodal-systems-fine-tuning-evaluation-metrics-and-real-world-applications/</li>
<li>haoheliu/audioldm_eval: This toolbox aims to unify audio generation model evaluation for easier comparison. - GitHub, https://github.com/haoheliu/audioldm_eval</li>
<li>Subjective-Aligned Dateset and Metric for Text-to-Video Quality Assessment - arXiv, https://arxiv.org/html/2403.11956v1</li>
<li>ziqihuangg/Awesome-Evaluation-of-Visual-Generation - GitHub, https://github.com/ziqihuangg/Awesome-Evaluation-of-Visual-Generation</li>
<li>Primers • Evaluation Metrics - aman.ai, https://aman.ai/primers/ai/evaluation-metrics/</li>
<li>AI Metrics that Matter: A Guide to Assessing Generative AI Quality - Encord, https://encord.com/blog/generative-ai-metrics/</li>
<li>MME Benchmark - QwenLM/Qwen-VL - GitHub, https://github.com/QwenLM/Qwen-VL/blob/master/eval_mm/mme/EVAL_MME.md</li>
<li>MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models, https://arxiv.org/html/2306.13394v5</li>
<li>[2505.18775] OmniGenBench: A Benchmark for Omnipotent Multimodal Generation across 50+ Tasks - arXiv, https://arxiv.org/abs/2505.18775</li>
<li>OmniGenBench: A Benchmark for Omnipotent Multimodal Generation across 50+ Tasks - arXiv, https://arxiv.org/html/2505.18775v1</li>
<li>WorldSimBench: Towards Video Generation Models as World Simulators - OpenReview, https://openreview.net/forum?id=j9pVnmulQm</li>
<li>New York Times sues AI startup for ‘illegal’ copying of millions of articles, https://www.theguardian.com/technology/2025/dec/05/new-york-times-perplexity-ai-lawsuit</li>
<li>Case Tracker: Artificial Intelligence, Copyrights and Class Actions | BakerHostetler, https://www.bakerlaw.com/services/artificial-intelligence-ai/case-tracker-artificial-intelligence-copyrights-and-class-actions/</li>
<li>A Tale of Three Cases: How Fair Use Is Playing Out in AI Copyright Lawsuits | Insights, https://www.ropesgray.com/en/insights/alerts/2025/07/a-tale-of-three-cases-how-fair-use-is-playing-out-in-ai-copyright-lawsuits</li>
<li>Two U.S. Courts Address Fair Use in Generative AI Training Cases | Insights | Jones Day, https://www.jonesday.com/en/insights/2025/06/two-us-courts-address-fair-use-in-genai-training-cases</li>
<li>Copyright and Artificial Intelligence, Part 3: Generative AI Training Pre-Publication Version, https://www.copyright.gov/ai/Copyright-and-Artificial-Intelligence-Part-3-Generative-AI-Training-Report-Pre-Publication-Version.pdf</li>
<li>LLM Hallucinations in 2025: How to Understand and Tackle AI’s Most Persistent Quirk, https://www.lakera.ai/blog/guide-to-hallucinations-in-large-language-models</li>
<li>A Comprehensive Survey of Hallucination in Large Language Models: Causes, Detection, and Mitigation - arXiv, https://arxiv.org/html/2510.06265v1</li>
<li>AI Act | Shaping Europe’s digital future - European Union, https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai</li>
<li>Navigating Generative AI Under the European Union’s Artificial Intelligence Act - WilmerHale, https://www.wilmerhale.com/en/insights/blogs/wilmerhale-privacy-and-cybersecurity-law/20241002-navigating-generative-ai-under-the-european-unions-artificial-intelligence-act</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>