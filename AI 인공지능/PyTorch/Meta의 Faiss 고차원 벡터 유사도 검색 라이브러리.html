<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Meta의 Faiss 고차원 벡터 유사도 검색 라이브러리</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Meta의 Faiss 고차원 벡터 유사도 검색 라이브러리</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">Pytorch</a> / <span>Meta의 Faiss 고차원 벡터 유사도 검색 라이브러리</span></nav>
                </div>
            </header>
            <article>
                <h1>Meta의 Faiss 고차원 벡터 유사도 검색 라이브러리</h1>
<h2>1.  고차원 공간에서의 효율적 유사도 검색</h2>
<h3>1.1  유사도 검색의 패러다임 전환과 Faiss의 등장</h3>
<p>현대의 인공지능(AI) 애플리케이션은 데이터를 이해하고 처리하는 방식에서 근본적인 패러다임 전환을 겪고 있다. 과거의 정보 검색 시스템이 주로 키워드 일치에 의존했다면, 오늘날의 시스템은 데이터의 의미적, 문맥적 유사성을 파악하는 ’시맨틱 검색(semantic search)’을 지향한다.1 이러한 변화의 중심에는 BERT, CLIP과 같은 딥러닝 모델이 생성하는 고차원 임베딩 벡터(high-dimensional embedding vector)가 있다.3 텍스트, 이미지, 오디오 등 비정형 데이터를 고정된 크기의 실수 벡터로 변환함으로써, 데이터 간의 복잡한 관계를 벡터 공간 내의 거리로 표현할 수 있게 되었다. 이는 추천 시스템, 이미지 검색, 자연어 처리(NLP), 그리고 최근 각광받는 검색 증강 생성(Retrieval-Augmented Generation, RAG) 시스템의 핵심 기술로 자리 잡았다.1</p>
<p>그러나 고차원 벡터 공간에서의 검색은 ’차원의 저주(Curse of Dimensionality)’라는 본질적인 도전에 직면한다.7 차원이 증가할수록 벡터 공간은 기하급수적으로 방대해지고, 데이터 포인트들은 서로 희소하게 분포하게 된다. 이로 인해, 쿼리 벡터가 주어졌을 때 데이터베이스 내의 모든 벡터와 거리를 일일이 계산하는 전수 조사(brute-force search) 방식은 수백만 개를 넘어가는 대규모 데이터셋에서는 계산 비용 측면에서 비현실적이 된다.8</p>
<p>이러한 문제를 해결하기 위해 Meta AI Research(FAIR)에서 개발한 Faiss(Facebook AI Similarity Search)는 효율적인 유사도 검색을 위한 강력한 솔루션으로 등장했다.1 Faiss는 단순히 하나의 알고리즘을 제공하는 라이브러리가 아니다. 이는 속도(Speed), 정확도(Accuracy), 그리고 메모리 사용량(Memory)이라는 세 가지 상충 관계(trade-off)를 사용자가 자신의 특정 요구사항에 맞게 능동적으로 제어하고 최적의 균형점을 찾을 수 있도록 설계된, 고도로 최적화된 알고리즘들의 ’도구상자(toolbox)’이다.8</p>
<p>Faiss의 핵심 철학은 특정 사용 사례에 맞는 단일 ’솔루션’을 제공하는 것이 아니라, 사용자가 자신의 제약 조건 하에서 최적의 솔루션을 ’직접 구축’할 수 있도록 강력하고 모듈화된 구성 요소들을 제공하는 데 있다. Faiss의 공식 문서는 데이터베이스 기능, 분산 컴퓨팅, 특징 추출 알고리즘 등은 의도적으로 라이브러리의 범위에서 제외했다고 명시하고 있다.11 대신, 전처리(preprocessing), 압축(compression), 비소모적 검색(non-exhaustive search) 등 다양한 인덱싱 구성 요소를 ’체인(chain)’처럼 연결하여 IndexIVFPQ와 같은 복합적인 인덱스를 만들 수 있도록 지원한다.11 이러한 설계 철학 덕분에 Faiss는 학술 연구의 빠른 프로토타이핑부터 수십억 개 규모의 프로덕션 시스템에 이르기까지 폭넓은 스펙트럼에서 채택될 수 있었다. Milvus, OpenSearch와 같은 다수의 벡터 데이터베이스가 Faiss를 내부 검색 엔진의 핵심 구성 요소로 활용하고 있다는 사실은 Faiss의 이러한 철학이 성공적이었음을 방증한다.11</p>
<h3>1.2  Faiss 개발의 역사와 핵심 기여자</h3>
<p>Faiss는 2017년 2월 22일에 처음 공개되었으며, Meta의 Fundamental AI Research(FAIR) 그룹 소속의 세계적인 연구자들에 의해 개발되었다.11 이 프로젝트는 단순히 기존 알고리즘을 모아놓은 것이 아니라, 개발자들이 직접 개척한 선구적인 연구 결과를 최적화하여 구현한 결과물이라는 점에서 그 기술적 깊이가 남다르다.</p>
<p>프로젝트의 핵심 기여자들과 그들의 역할은 다음과 같다 13:</p>
<ul>
<li><strong>Hervé Jégou</strong>: Faiss 프로젝트를 시작하고 최초의 구현을 담당했다. 그는 특히 Faiss의 핵심 압축 기술인 ’곱 양자화(Product Quantization, PQ)’를 제안한 2011년 PAMI 논문의 주 저자이기도 하다.10</li>
<li><strong>Matthijs Douze</strong>: 대부분의 CPU 기반 Faiss 알고리즘을 구현했다. 그 역시 PQ 논문의 공저자로서, 이론적 배경과 실제 구현을 연결하는 데 핵심적인 역할을 수행했다.10</li>
<li><strong>Jeff Johnson</strong>: Faiss의 모든 GPU 관련 구현을 전담했다. 그의 기여 덕분에 Faiss는 GPU를 활용한 압도적인 검색 성능을 확보할 수 있었다.8</li>
<li><strong>Lucas Hosseini</strong>: 이진 인덱스(binary indexes)와 빌드 시스템을 구현했다.13</li>
<li><strong>Chengqi Deng</strong>: NSG, NNdescent와 같은 그래프 기반 인덱싱 알고리즘과 가산 양자화(additive quantization) 코드의 상당 부분을 구현했다.13</li>
</ul>
<p>이처럼 Faiss는 벡터 검색 분야의 최고 전문가들이 자신들의 연구를 직접 코드로 옮긴 결과물이다. 이는 Faiss가 제공하는 알고리즘의 성능과 효율성에 대한 강력한 신뢰를 부여하며, 단순한 엔지니어링의 산물을 넘어 과학적 성취의 결정체임을 보여준다.</p>
<h2>2.  Faiss의 수학적 기반과 핵심 개념</h2>
<h3>2.1  최근접 이웃 탐색(Nearest Neighbor Search)의 수학적 정의</h3>
<p>Faiss의 근본적인 목표는 주어진 고차원 벡터 집합 내에서 쿼리 벡터와 가장 유사한 벡터를 찾는 것이다. 이는 수학적으로 최근접 이웃 탐색(Nearest Neighbor Search, NNS) 문제로 정의된다. d차원의 벡터 공간에 존재하는 n개의 벡터 집합 <span class="math math-inline">X = \{x_1,..., x_n\}</span>이 주어졌을 때, 새로운 쿼리 벡터 <span class="math math-inline">x_q</span>에 대해 집합 <span class="math math-inline">X</span> 내에서 가장 가까운 벡터 <span class="math math-inline">x_j</span>를 찾는 연산이다.10</p>
<p>Faiss는 주로 두 가지 거리 척도(distance metric)를 지원한다.</p>
<ol>
<li>
<p><strong>유클리드 거리 (Euclidean Distance, L2)</strong>: 두 벡터 간의 직선 거리를 측정하며, 최소화하는 것을 목표로 한다. 이는 가장 일반적인 유사도 척도이다.<br />
<span class="math math-display">
j = \underset{i \in \{1,..., n\}}{\mathrm{argmin}} \Vert x_q - x_i \Vert_2
</span><br />
여기서 <span class="math math-inline">\Vert \cdot \Vert_2</span>는 L2 노름(norm)을 의미한다.10</p>
</li>
<li>
<p><strong>최대 내적 (Maximum Inner Product, IP)</strong>: 두 벡터 간의 내적 값을 측정하며, 최대화하는 것을 목표로 한다. 벡터들이 L2 정규화(normalization)되어 단위 벡터가 되면, 내적은 코사인 유사도(cosine similarity)와 비례하므로, 이는 방향성의 유사도를 찾는 데 사용된다.10<br />
<span class="math math-display">
j = \underset{i \in \{1,..., n\}}{\mathrm{argmax}} \langle x_q, x_i \rangle
</span><br />
여기서 <span class="math math-inline">\langle \cdot, \cdot \rangle</span>는 벡터 내적을 의미한다.15</p>
</li>
</ol>
<p>Faiss는 이 외에도 L1, Linf 등 다른 거리 척도를 제한적으로 지원한다.10 Faiss의 모든 인덱스는 이러한 기본적인 수학적 연산을 효율적으로 수행하기 위해 설계된 데이터 구조이다.</p>
<h3>2.2  성능의 3요소: 속도, 정확도, 메모리</h3>
<p>Faiss를 사용하여 유사도 검색 시스템을 구축할 때, 모든 설계 결정은 세 가지 핵심 성능 지표 간의 상충 관계(trade-off)를 고려해야 한다. Faiss의 다양한 인덱스 유형은 이 세 가지 축 위에서 각기 다른 지점에 위치하는 옵션들로 볼 수 있다.8</p>
<ol>
<li><strong>속도 (Speed)</strong>: 쿼리당 검색에 소요되는 시간을 의미하며, 보통 초당 처리 가능한 쿼리 수(Queries Per Second, QPS)로 측정된다. 속도는 실시간 애플리케이션에서 가장 중요한 요소 중 하나이다.8</li>
<li><strong>정확도 (Accuracy)</strong>: 근사 검색(Approximate Nearest Neighbor Search, ANNS) 알고리즘이 얼마나 정확한 결과를 반환하는지를 나타내는 척도이다. 일반적으로 전수 조사(brute-force search)를 통해 찾은 실제 정답과 비교하여, 상위 k개의 결과 중 실제 최근접 이웃이 얼마나 포함되었는지를 나타내는 **재현율(Recall@k)**로 평가된다. 예를 들어, ’1-recall@1’은 가장 가까운 이웃 하나를 정확히 찾았는지의 비율을 의미한다.8</li>
<li><strong>메모리 (Memory)</strong>: 인덱스를 저장하는 데 필요한 RAM의 양을 의미한다. 이는 보통 데이터베이스 벡터 하나당 사용되는 바이트(bytes/vector)로 표현된다. 수십억 개 규모의 데이터셋을 다룰 때 메모리 효율성은 시스템의 확장성을 결정하는 치명적인 요소가 된다.8</li>
</ol>
<p>이 세 가지 요소는 서로 밀접하게 연관되어 있다. 예를 들어, 벡터 압축률을 높여 메모리 사용량을 줄이면(메모리 ↑), 양자화 오류로 인해 정확도가 감소(정확도 ↓)할 수 있다. 반대로, 검색 시 더 많은 후보군을 탐색하여 정확도를 높이면(정확도 ↑), 검색 시간이 길어져 속도가 느려진다(속도 ↓).10 Faiss의 진정한 강력함은 개발자가 이러한 트레이드오프를 명시적으로 이해하고, 자신의 애플리케이션 요구사항에 가장 적합한 균형점을 선택할 수 있도록 다양한 도구를 제공한다는 점에 있다.</p>
<h2>3.  Faiss 인덱싱 아키텍처 심층 분석</h2>
<p>Faiss는 다양한 인덱싱 기법을 제공하며, 이들은 종종 여러 단계를 거쳐 조합된다. 가장 단순한 전수 조사부터 시작하여, 검색 공간을 줄이는 단계, 그리고 메모리 사용량을 줄이는 단계를 거쳐 최첨단 그래프 기반 방식에 이르기까지 점진적으로 이해하는 것이 효과적이다.</p>
<h3>3.1  기준점: IndexFlatL2 - 정확하지만 느린 전수 조사</h3>
<p>IndexFlatL2는 Faiss에서 가장 기본적이고 직관적인 인덱스이다.1 이 인덱스의 작동 원리는 매우 간단하다.</p>
<ul>
<li><strong>저장</strong>: 데이터베이스에 추가되는 모든 벡터를 아무런 변환이나 압축 없이 원본 형태 그대로 메모리에 순차적으로 저장한다.18</li>
<li><strong>검색</strong>: 쿼리 벡터가 주어지면, 인덱스에 저장된 <strong>모든</strong> 벡터와 유클리드 거리를 일일이 계산한 후, 가장 거리가 짧은 k개의 벡터를 반환한다.17</li>
</ul>
<p>이 방식은 모든 벡터를 비교하기 때문에 100%의 재현율을 보장하며, 따라서 다른 근사 검색 인덱스들의 정확도를 평가하기 위한 ‘그라운드 트루스(ground truth)’ 또는 ’황금 표준(gold standard)’으로 사용된다.8 또한, 데이터를 변형하지 않으므로 별도의 훈련(training) 과정이 필요 없다.21</p>
<p>하지만 IndexFlatL2의 치명적인 단점은 확장성이다. 데이터베이스 벡터의 수가 N개일 때, 검색 시간 복잡도는 <span class="math math-inline">O(N)</span>으로 N에 정비례하여 증가한다. 데이터셋이 수백만 개를 넘어가면 검색 속도는 실시간 서비스에 사용하기 어려울 정도로 느려진다.19 따라서</p>
<p>IndexFlatL2는 데이터셋의 크기가 작거나, 오프라인 분석 등 검색 속도가 중요하지 않은 경우, 또는 다른 복잡한 인덱스의 구성 요소(예: quantizer)로 사용된다.</p>
<h3>3.2  1단계 최적화 (검색 공간 축소): 역파일 시스템 (IndexIVF)</h3>
<p>IndexFlat의 선형적인 검색 시간 문제를 해결하기 위한 첫 번째 최적화 단계는 전체 데이터셋을 탐색하는 대신, 쿼리와 관련성이 높은 일부 영역만 선택적으로 탐색하는 것이다. 이 아이디어를 구현한 것이 바로 역파일(Inverted File) 시스템 기반의 IndexIVF이다.10</p>
<p>IndexIVF의 작동 방식은 세 단계로 나눌 수 있다.</p>
<ol>
<li><strong>훈련 (Training)</strong>: IndexIVF를 사용하기 전에 반드시 훈련 과정을 거쳐야 한다. 이 단계에서는 k-평균(k-means) 군집화 알고리즘을 사용하여 전체 벡터 공간을 사용자가 지정한 nlist개의 클러스터(Voronoi 셀)로 분할한다.18 각 클러스터의 중심점을 센트로이드(centroid)라고 하며, 이 센트로이드들의 집합이 전체 벡터 공간을 대표하는 일종의 ‘지도’ 역할을 한다. 이 센트로이드들을 인덱싱하고 검색하는 역할은<br />
quantizer라고 불리는 또 다른 (보통 IndexFlatL2) 인덱스가 담당한다.23</li>
<li><strong>추가 (Adding)</strong>: 데이터베이스에 새로운 벡터를 추가할 때, 이 벡터는 nlist개의 센트로이드 중 가장 가까운 센트로이드에 할당된다. 그리고 해당 센트로이드에 연결된 ’역파일 리스트(inverted list)’에 벡터의 ID와 정보가 저장된다. 이는 전통적인 텍스트 검색에서 특정 단어가 포함된 문서들의 목록을 저장하는 방식과 유사하다.23</li>
<li><strong>검색 (Searching)</strong>: 검색 과정은 다음과 같이 진행된다.</li>
</ol>
<ul>
<li>쿼리 벡터와 nlist개의 모든 센트로이드 간의 거리를 계산한다.</li>
<li>쿼리 벡터와 가장 가까운 nprobe개의 센트로이드를 선택한다.19<br />
nprobe는 사용자가 검색 시점에 지정하는 파라미터이다.</li>
<li>선택된 nprobe개의 센트로이드에 해당하는 역파일 리스트에 저장된 벡터들하고만 실제 거리를 계산하여 최종 k개의 최근접 이웃을 찾는다.18</li>
</ul>
<p>IndexIVF는 nprobe 값을 조절함으로써 속도와 정확도 간의 균형을 맞춘다. nprobe가 작을수록 더 적은 수의 벡터만 비교하므로 검색 속도는 빨라지지만, 실제 정답이 선택되지 않은 셀에 있을 확률이 높아져 정확도(재현율)는 떨어진다. 반대로 nprobe를 늘리면 정확도는 높아지지만 검색 속도는 느려진다.19</p>
<p>nprobe가 nlist와 같아지면 사실상 모든 벡터를 검사하게 되어 IndexFlat과 유사한 결과를 내지만, 센트로이드 검색 과정 때문에 더 비효율적이다.</p>
<p>nlist와 nprobe는 Faiss 성능 튜닝에서 가장 중요한 하이퍼파라미터이다. nlist 값은 데이터셋의 크기 N에 따라 결정하는 것이 일반적이며, Faiss 공식 가이드라인은 N이 1백만 미만일 때 <span class="math math-inline">4\sqrt{N}</span>에서 <span class="math math-inline">16\sqrt{N}</span> 사이의 값을 추천한다.16</p>
<p>nlist가 너무 작으면 각 셀에 너무 많은 벡터가 담겨 검색 효율이 떨어지고, 너무 크면 공간 분할이 너무 세밀해져 쿼리가 여러 셀의 경계에 위치할 확률이 높아져 정확도가 저하될 수 있다. 따라서 사용자는 자신의 데이터셋 특성과 애플리케이션이 요구하는 지연 시간 및 정확도 목표에 맞춰 이 두 파라미터를 신중하게 튜닝해야 한다.</p>
<h3>3.3  2단계 최적화 (메모리 절감): 벡터 양자화 (Vector Quantization)</h3>
<p>IndexIVF는 검색 대상 벡터의 수를 줄여 속도를 향상시켰지만, 여전히 각 벡터를 원본(float32) 형태로 저장하기 때문에 대규모 데이터셋에서는 메모리 사용량이 부담이 될 수 있다.8 예를 들어, 10억 개의 128차원 벡터는 약 512GB의 RAM을 필요로 한다. 벡터 양자화(Vector Quantization)는 이러한 문제를 해결하기 위해 벡터를 더 적은 비트로 표현하는 손실 압축(lossy compression) 기법이다.10</p>
<h4>3.3.1  곱 양자화 (Product Quantization, PQ)</h4>
<p>곱 양자화(PQ)는 Faiss에서 가장 핵심적인 압축 기술 중 하나로, 메모리 사용량을 획기적으로 줄이면서도 비교적 정확한 거리 계산을 가능하게 한다.10</p>
<ul>
<li><strong>원리</strong>: PQ의 핵심 아이디어는 고차원 벡터를 여러 개의 저차원 부벡터(sub-vector)로 분할하여 각각을 독립적으로 양자화하는 것이다.26</li>
</ul>
<ol>
<li>D차원의 원본 벡터를 M개의 D/M차원 부벡터로 분할한다.</li>
<li>각 부벡터 공간(총 M개)에 대해 독립적으로 k-평균 군집화를 수행하여 k<em>개(보통 k</em>=256)의 센트로이드를 갖는 코드북(codebook)을 생성한다.</li>
<li>원본 벡터의 각 부벡터는 해당 공간의 코드북에서 가장 가까운 센트로이드의 ID(0~255, 즉 8비트)로 대체된다.</li>
<li>결과적으로, D차원의 float32 벡터( D * 32 비트)는 M개의 8비트 ID 시퀀스(M * 8 비트)로 압축된다. 예를 들어, 128차원 벡터를 M=8로 분할하면, 128 * 4 = 512 바이트가 8 * 1 = 8 바이트로 압축되어 메모리 사용량이 64배 감소한다.25</li>
</ol>
<ul>
<li>
<p><strong>거리 계산</strong>: 압축된 코드를 이용해 벡터 간 거리를 근사 계산하는 방식에는 두 가지가 있다.</p>
</li>
<li>
<p><strong>비대칭 거리 계산 (Asymmetric Distance Computation, ADC)</strong>: 쿼리 벡터는 원본 float32 형태를 유지하고, 데이터베이스 벡터는 압축된 PQ 코드를 사용한다. 검색 시, 쿼리 벡터의 각 부벡터와 해당 코드북의 모든(k*개) 센트로이드 간의 거리를 미리 계산하여 거리 테이블(distance table)을 만든다. 이후 데이터베이스의 각 PQ 코드에 대한 전체 거리는, 코드의 각 ID를 인덱스로 하여 이 거리 테이블을 M번 조회(lookup)하고 그 값들을 합산하는 방식으로 매우 빠르게 근사할 수 있다. 이 방식은 쿼리 벡터의 정보 손실이 없어 더 정확하기 때문에 Faiss에서 기본적으로 사용된다.26</p>
</li>
<li>
<p><strong>대칭 거리 계산 (Symmetric Distance Computation, SDC)</strong>: 쿼리 벡터와 데이터베이스 벡터를 모두 PQ 코드로 변환한 후, 두 코드 간의 거리를 계산한다. 이는 미리 계산된 센트로이드 간 거리 테이블을 조회하여 수행된다. 쿼리 관련 메모리 사용량이 적다는 장점이 있지만, 양쪽 모두에서 양자화 오류가 발생하여 정확도가 ADC보다 낮다.27</p>
</li>
</ul>
<h4>3.3.2  스칼라 양자화 (Scalar Quantization, SQ)</h4>
<p>스칼라 양자화는 PQ보다 간단한 압축 방법이다. 벡터의 각 차원(스칼라 값)을 독립적으로 양자화한다. 예를 들어, 각 float32 값을 특정 범위에 맞춰 8비트 정수(uint8)로 변환하는 방식이다. 이 경우 메모리 사용량은 4분의 1로 줄어든다.29 PQ만큼 압축률이 높지는 않지만, 변환 과정이 더 빠르고 구현이 단순하다는 장점이 있다.</p>
<h4>3.3.3  복합 인덱스: IndexIVFPQ</h4>
<p>IndexIVF와 PQ를 결합한 IndexIVFPQ는 Faiss에서 가장 널리 사용되는 강력한 인덱스 유형 중 하나이다. 이는 검색 공간 축소(IVF)와 메모리 절감(PQ)의 이점을 모두 취한다.10</p>
<ul>
<li><strong>작동 방식</strong>:</li>
</ul>
<ol>
<li><strong>훈련</strong>: IVF와 마찬가지로 k-평균을 통해 nlist개의 센트로이드를 찾는다.</li>
<li><strong>추가</strong>: 각 데이터 벡터는 가장 가까운 센트로이드에 할당된다. 이때 원본 벡터 전체를 저장하는 대신, 원본 벡터와 해당 센트로이드 간의 **차이 벡터(잔차, residual vector)**를 계산하고, 이 잔차 벡터를 PQ로 압축하여 저장한다.</li>
<li><strong>검색</strong>: nprobe개의 셀을 방문하여, 쿼리 벡터와 각 셀에 저장된 압축된 잔차 벡터들 간의 거리를 ADC 방식으로 빠르게 계산한다. 이 과정은 ’쿼리 벡터 - 센트로이드’로 계산된 잔차 쿼리를 사용한다.23</li>
</ol>
<p>IndexIVFPQ는 수십억 개 규모의 데이터셋을 단일 서버의 RAM에 상주시키면서도 합리적인 속도와 정확도로 검색할 수 있게 해주는 핵심 기술이다.</p>
<h3>3.4  최신 그래프 기반 인덱싱: IndexHNSW</h3>
<p>IndexHNSW는 최근접 이웃 검색 분야에서 가장 뛰어난 성능을 보이는 알고리즘 중 하나인 HNSW(Hierarchical Navigable Small World)를 구현한 인덱스이다.10</p>
<ul>
<li>
<p><strong>원리</strong>: HNSW는 데이터 포인트들을 노드(vertex)로, 이들 간의 근접성을 엣지(edge)로 표현하는 근접성 그래프(proximity graph)를 기반으로 한다. 특히, HNSW는 스킵 리스트(skip list)에서 영감을 받은 계층적(hierarchical) 구조를 도입하여 탐색 효율을 극대화한다.32</p>
</li>
<li>
<p><strong>계층 구조</strong>: 여러 개의 그래프 레이어로 구성된다. 최상위 레이어(layer 0)는 가장 적은 수의 노드를 가지며, 노드 간 긴 엣지로 연결되어 있어 탐색 공간을 빠르게 건너뛰는 ‘고속도로’ 역할을 한다. 하위 레이어로 내려갈수록 노드의 수가 많아지고 엣지들은 더 짧아져, 점차적으로 탐색 범위를 좁혀가며 정밀한 탐색을 수행한다.32</p>
</li>
<li>
<p><strong>그래프 생성 (Construction)</strong>: 새로운 벡터가 인덱스에 추가될 때, 먼저 무작위 확률 분포(지수 감쇠)에 따라 해당 벡터가 존재할 최대 레벨이 결정된다. 그 후, 최상위 레벨의 고정된 진입점(entry point)부터 탐욕적(greedy) 탐색을 시작하여 아래로 내려오면서 각 레벨에서 새로 추가된 벡터와 연결될 이웃 노드들을 찾는다. 이 과정에서 efConstruction(탐색 깊이)과 M(노드당 최대 이웃 수) 파라미터가 그래프의 품질과 밀도, 그리고 메모리 사용량을 결정한다.23</p>
</li>
<li>
<p><strong>탐색 (Search)</strong>: 검색 역시 최상위 레벨의 진입점에서 시작된다. 각 레벨에서 현재 위치한 노드의 이웃들 중 쿼리 벡터와 가장 가까운 노드로 이동하는 탐욕적 탐색을 반복한다. 이 과정을 최하위 레벨까지 계속하여 최종적인 최근접 이웃 후보군을 찾는다. efSearch 파라미터는 각 레벨에서 유지하는 동적 후보 리스트의 크기를 결정하며, 이 값을 높일수록 정확도는 향상되지만 검색 시간은 길어진다.23</p>
</li>
</ul>
<p>HNSW는 IVF 계열 인덱스에 비해 일반적으로 더 빠른 검색 속도와 높은 재현율을 제공하지만, 이는 상당한 메모리 사용량을 대가로 한다. HNSW는 원본 벡터를 압축하지 않고 저장할 뿐만 아니라, 각 노드마다 이웃 노드의 ID를 저장하기 위한 추가적인 공간이 필요하기 때문이다.32 Faiss 공식 가이드라인은 HNSW의 벡터당 메모리 사용량을</p>
<p><span class="math math-inline">(d \times 4 + M \times 2 \times 4)</span> 바이트로 명시하며, 메모리 제약이 없는 경우 가장 좋은 옵션으로 추천한다.16 따라서 HNSW는 지연 시간에 매우 민감하고 메모리 자원이 풍부한 애플리케이션에 가장 적합한 최첨단 기술이라 할 수 있다. 반면, 수십억 개 규모의 초대형 데이터셋이나 메모리가 제한된 환경에서는</p>
<p>IndexIVFPQ 조합이 여전히 더 실용적인 선택지가 될 수 있다. 이는 Faiss가 단 하나의 ‘최고’ 알고리즘에 의존하지 않고, 다양한 시나리오에 맞는 옵션을 제공하는 ’도구상자’로서의 철학을 잘 보여주는 예이다.</p>
<h2>4.  성능 최적화 및 대규모 확장</h2>
<p>Faiss는 알고리즘적 효율성뿐만 아니라, 최신 하드웨어의 성능을 최대한 활용하기 위한 저수준 최적화에도 많은 노력을 기울였다. 이를 통해 CPU와 GPU 환경 모두에서 최고의 성능을 발휘하며, 수십억 단위의 벡터 검색을 가능하게 한다.</p>
<h3>4.1  CPU 최적화</h3>
<p>Faiss의 핵심 연산은 C++로 구현되어 있으며, CPU 성능을 극대화하기 위해 다음과 같은 기법들이 적용되었다.</p>
<ul>
<li><strong>BLAS 라이브러리 활용</strong>: 정확한 거리 계산은 대규모 행렬 곱셈 연산으로 변환될 수 있다. Faiss는 OpenBLAS나 Intel MKL과 같은 고도로 최적화된 BLAS(Basic Linear Algebra Subprograms) 라이브러리를 활용하여 이 연산을 가속화한다.8</li>
<li><strong>SIMD 명령어</strong>: 단일 명령어 다중 데이터(Single Instruction, Multiple Data, SIMD)는 CPU가 한 번의 명령으로 여러 개의 데이터에 대해 동일한 연산을 수행하게 하는 기술이다. Faiss는 x86 아키텍처의 AVX2, AVX512나 ARM의 NEON과 같은 SIMD 명령어 셋을 위한 커스텀 커널을 구현하여, 벡터 간 거리 계산과 같은 반복적인 연산의 속도를 획기적으로 향상시킨다.8</li>
<li><strong>멀티스레딩</strong>: OpenMP를 통해 다중 코어를 효율적으로 활용하여 검색 작업을 병렬 처리한다.11</li>
</ul>
<h3>4.2  GPU 가속 아키텍처</h3>
<p>Faiss의 가장 큰 강점 중 하나는 강력한 GPU 지원이다. GPU의 대규모 병렬 처리 능력을 활용하여 유사도 검색을 극적으로 가속화한다.10</p>
<ul>
<li>
<p><strong>성능</strong>: IndexFlat, IndexIVFFlat, IndexIVFPQ 등 주요 인덱스 유형에 대해 GPU 구현이 제공된다.38 단일 GPU 사용 시, 동일한 작업에 대한 CPU 구현보다 통상 5배에서 10배 더 빠르며, 최신 Pascal 아키텍처 이상의 GPU에서는 최대 20배 이상의 성능 향상을 보인다.8</p>
</li>
<li>
<p><strong>CPU-GPU 상호운용성</strong>: Faiss의 GPU 인덱스는 CPU 인덱스를 대체하여 바로 사용할 수 있도록(drop-in replacement) 설계되었다. 개발자는 CUDA API를 직접 다룰 필요 없이, CPU 메모리에 있는 데이터를 입력으로 전달하면 Faiss가 자동으로 데이터를 GPU로 복사하고 연산을 수행한다. 물론, 최상의 성능을 위해서는 입력 데이터와 인덱스가 모두 동일한 GPU 메모리 내에 상주하는 것이 좋다.39</p>
</li>
<li>
<p><strong>다중 GPU 확장</strong>: Faiss는 단일 머신에 장착된 여러 개의 GPU를 활용하여 검색 성능과 용량을 확장할 수 있는 두 가지 주요 메커니즘을 제공한다.39</p>
</li>
<li>
<p><strong>IndexReplicas</strong>: 인덱스 전체를 여러 GPU에 복제하는 방식이다. 검색 시, 쿼리 배치를 각 GPU에 분산하여 병렬로 처리하므로, 전체 처리량(throughput)이 GPU 수에 비례하여 선형적으로 증가하는 효과를 얻을 수 있다.</p>
</li>
<li>
<p><strong>IndexShards</strong>: 데이터셋을 여러 부분으로 분할(sharding)하여 각 GPU가 데이터의 일부만을 저장하는 방식이다. 이를 통해 단일 GPU의 메모리 용량을 초과하는 매우 큰 데이터셋을 다룰 수 있다.</p>
</li>
<li>
<p><strong>float16 정밀도 활용</strong>: Faiss GPU는 16비트 부동소수점(float16) 정밀도를 지원한다. 이를 사용하면 float32 대비 GPU 메모리 사용량을 절반으로 줄일 수 있으며, half-precision 연산을 네이티브로 지원하는 최신 GPU 아키텍처에서는 연산 속도 또한 크게 향상된다. 재현율(Recall)과 같은 정확도 지표에는 큰 영향이 없으나, 반환되는 거리 값의 정밀도는 다소 저하될 수 있다.39</p>
</li>
</ul>
<h3>4.3  10억 단위 벡터 검색(Billion-Scale Search)의 도전 과제와 해결 전략</h3>
<p>Faiss는 10억 개(billion-scale)를 넘어 1조 개(trillion-scale) 단위의 벡터를 처리할 수 있도록 설계되었다.11 이러한 규모에서는 몇 가지 핵심적인 도전 과제가 발생하며, Faiss는 이를 해결하기 위한 전략을 제공한다.</p>
<ul>
<li>
<p><strong>메모리 문제</strong>: 10억 개의 128차원 float32 벡터는 원본만으로도 약 512GB의 RAM을 필요로 한다. 여기에 인덱스 오버헤드까지 더해지면 단일 서버에 수용하기 어렵다.</p>
</li>
<li>
<p><strong>해결 전략</strong>: IndexIVFPQ와 같은 강력한 압축 인덱스의 사용이 필수적이다. 예를 들어, PQ를 사용하여 벡터당 64바이트로 압축하면 전체 데이터셋 크기를 약 64GB로 줄일 수 있다. 더 나아가 32바이트, 16바이트로 압축하면 수십 GB의 RAM으로도 10억 개 벡터 인덱싱이 가능해진다.8</p>
</li>
<li>
<p><strong>인덱스 구축 시간</strong>: 10억 개 규모의 데이터셋에 대해 IndexIVF의 훈련 단계인 k-평균 군집화를 수행하는 것은 엄청난 시간이 소요될 수 있다.</p>
</li>
<li>
<p><strong>해결 전략 1 (GPU 훈련)</strong>: 훈련에 사용할 샘플 데이터가 GPU 메모리에 적재될 수 있다면, GPU를 사용하여 훈련 시간을 수십 배 단축할 수 있다. 예를 들어, 1백만 개의 256차원 벡터를 2만개 클러스터로 군집화하는 작업은 CPU에서 11분이 걸리지만, 단일 Pascal P100 GPU에서는 1분 미만이 소요된다.16</p>
</li>
<li>
<p><strong>해결 전략 2 (2단계 클러스터링)</strong>: 데이터셋이 너무 커서 훈련 샘플조차 RAM에 올리기 힘든 경우, 먼저 데이터를 더 적은 수의 대규모 클러스터로 나눈 뒤, 각 대규모 클러스터를 다시 세부 클러스터로 나누는 계층적 방식을 사용하여 훈련 복잡도를 낮출 수 있다.16</p>
</li>
<li>
<p><strong>검색 지연 시간</strong>: IndexIVFPQ를 사용하면 nprobe 값을 적절히 설정하여 지연 시간과 정확도 사이의 최적점을 찾아야 한다. Meta가 공개한 Deep1B 데이터셋(10억 개 이미지 벡터)에 대한 벤치마크 결과에 따르면, 1-recall@1 40%의 정확도를 달성하는 데 쿼리당 2ms 미만이 소요되었으며, 이는 단일 코어에서 초당 500개 이상의 쿼리를 처리할 수 있는 속도이다.8</p>
</li>
</ul>
<h2>5.  Faiss 인덱스 선택 가이드 및 성능 벤치마크</h2>
<p>Faiss의 가장 큰 장점이자 초심자에게는 가장 큰 장벽은 바로 다양한 인덱스 유형과 조합의 존재이다. 어떤 상황에서 어떤 인덱스를 선택해야 하는지에 대한 명확한 이해는 Faiss를 효과적으로 활용하기 위한 필수 조건이다.</p>
<h3>5.1  공식 가이드라인 기반의 인덱스 선택 전략</h3>
<p>Faiss 공식 Wiki는 사용자의 요구사항에 따라 최적의 인덱스 조합을 선택할 수 있도록 구체적인 가이드라인을 제공한다.16 이 가이드라인은 “정확한 결과가 필요한가?”, “메모리 제약이 있는가?”, “데이터셋의 크기는 얼마인가?“와 같은 질문에 답하는 의사결정 트리 형태로 구성되어 있다. 다음 표는 이 가이드라인을 데이터셋 크기와 메모리 제약이라는 두 가지 핵심 축을 기준으로 요약한 것이다.</p>
<p><strong>표 2: 요구사항별 추천 인덱스 조합</strong></p>
<table><thead><tr><th>메모리 제약</th><th>데이터셋 &lt; 1M</th><th>데이터셋 1M ~ 100M</th><th>데이터셋 &gt; 100M</th><th></th></tr></thead><tbody>
<tr><td><strong>제약 없음 (속도/정확도 최우선)</strong></td><td>HNSW<em>M</em> (M: 4~64)</td><td>HNSW<em>M</em> (M: 4~64)</td><td>HNSW<em>M</em> (M: 4~64)</td><td></td></tr>
<tr><td><strong>어느 정도 제약 (원본 벡터 저장)</strong></td><td>IVF<em>K</em>,Flat (K ≈ 4√N)</td><td>IVF65536_HNSW32,Flat</td><td>IVF262144_HNSW32,Flat</td><td></td></tr>
<tr><td><strong>상당한 제약 (벡터 압축)</strong></td><td>OPQ<em>M</em>_<em>D</em>,IVF<em>K</em>,PQ<em>M</em></td><td>OPQ<em>M</em>_<em>D</em>,IVF65536_HNSW32,PQ<em>M</em></td><td>OPQ<em>M</em>_<em>D</em>,IVF262144_HNSW32,PQ<em>M</em></td><td></td></tr>
<tr><td><strong>매우 중요 (최대 압축)</strong></td><td>…,PQ<em>M</em> (M ≤ 64)</td><td>…,PQ<em>M</em> (M ≤ 64)</td><td>…,PQ<em>M</em> (M ≤ 64)</td><td></td></tr>
</tbody></table>
<p>N은 데이터셋 크기, K는 nlist 값, M은 PQ 부벡터 수, D는 OPQ 출력 차원. … 부분은 데이터셋 크기에 맞는 IVF 방식을 의미한다. 데이터 출처:.16</p>
<p>이 표에서 IVF65536_HNSW32와 같은 문자열은 IVF의 quantizer로 HNSW를 사용하여 클러스터 할당 속도를 높이는 고급 기법을 의미한다. OPQ는 벡터를 PQ로 압축하기 전에 데이터 분포를 회전시켜 압축 효율을 높이는 전처리 단계이다. 이 가이드라인은 사용자가 복잡한 인덱스 문자열의 의미를 완전히 이해하지 못하더라도, 자신의 상황에 맞는 ’레시피’를 즉시 찾아 적용할 수 있도록 돕는다.</p>
<h3>5.2  주요 인덱스 유형별 성능 비교 분석</h3>
<p>각 인덱스 유형의 특성을 더 직관적으로 이해하기 위해, 표준 벤치마크 데이터셋인 Sift1M(128차원, 1백만 개 벡터)에 대한 정량적 성능 비교는 다음과 같다.</p>
<p><strong>표 1: 주요 Faiss 인덱스 유형 비교 (Sift1M 데이터셋 기준)</strong></p>
<table><thead><tr><th>인덱스 유형</th><th>검색 방식</th><th>메모리 사용량</th><th>검색 시간 (ms)</th><th>재현율 (Recall@10)</th><th>훈련 필요 여부</th><th>주요 사용 사례</th><th></th></tr></thead><tbody>
<tr><td>IndexFlatL2</td><td>정확 (Exact)</td><td>~500 MB</td><td>~18.0</td><td>1.0</td><td>아니요</td><td>소규모 데이터셋, 벤치마크 기준점 설정</td><td></td></tr>
<tr><td>IndexLSH</td><td>근사 (ANN)</td><td>20 - 600 MB</td><td>1.7 - 30.0</td><td>0.4 - 0.85</td><td>예</td><td>저차원 데이터, 메모리 사용량이 매우 중요할 때</td><td></td></tr>
<tr><td>IndexIVFFlat</td><td>근사 (ANN)</td><td>~520 MB</td><td>1.0 - 9.0</td><td>0.7 - 0.95</td><td>예</td><td>대규모 데이터셋, 속도/정확도/메모리 균형</td><td></td></tr>
<tr><td>IndexHNSWFlat</td><td>근사 (ANN)</td><td>600 - 1600 MB</td><td>0.6 - 2.1</td><td>0.5 - 0.95</td><td>아니요</td><td>초저지연 시간과 높은 정확도가 필요하며 메모리가 충분할 때</td><td></td></tr>
<tr><td>IndexIVFPQ</td><td>근사 (ANN)</td><td>~9.2 MB</td><td>~0.08 (nprobe=1)</td><td>~0.34 (nprobe=1)</td><td>예</td><td>초대규모 데이터셋, 메모리 효율성이 최우선일 때</td><td></td></tr>
<tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
</tbody></table>
<p>IndexIVFPQ는 nlist=1024, m=8, nbits=8 기준. 데이터 출처:.19</p>
<p>이 표는 각 인덱스가 속도, 정확도, 메모리라는 세 가지 축 위에서 어떤 위치를 차지하는지를 명확하게 보여준다. 예를 들어, IndexHNSWFlat은 가장 빠른 검색 속도와 높은 재현율을 보이지만 메모리 사용량이 가장 크다. 반면 IndexIVFPQ는 메모리 사용량을 극단적으로 줄이지만, 낮은 nprobe 값에서는 재현율이 상대적으로 낮다. 사용자는 이 표를 통해 자신의 우선순위에 맞는 인덱스 유형을 직관적으로 선택할 수 있다.</p>
<h3>5.3  주요 ANN 라이브러리와의 벤치마크 비교</h3>
<p>Faiss는 유일한 ANN 라이브러리가 아니며, 다른 여러 라이브러리들과 경쟁 및 협력 관계에 있다.</p>
<ul>
<li><strong>Faiss vs. ScaNN</strong>: Google에서 개발한 ScaNN(Scalable Nearest Neighbors)은 CPU 기반의 근사 검색에 특화되어 있으며, 특히 내적 유사도(inner product) 검색에서 뛰어난 성능을 보인다.43 ScaNN은 비등방성 벡터 양자화(anisotropic vector quantization)와 같은 독자적인 기술을 사용하여 높은 정확도를 달성한다. 반면 Faiss는 정확 검색과 근사 검색을 모두 지원하고, 다양한 인덱스 조합을 제공하며, 특히 GPU 가속에서 강력한 이점을 가진다.43</li>
<li><strong>Faiss vs. Annoy</strong>: Spotify에서 개발한 Annoy(Approximate Nearest Neighbors Oh Yeah)는 사용 편의성과 메모리 효율성에 중점을 둔 라이브러리이다. 랜덤 투영 트리(random projection tree)를 기반으로 하며, 인덱스를 파일에 저장하고 메모리 맵(memory-map) 방식으로 로드할 수 있어 리소스가 제한된 환경이나 빠른 시작이 중요한 경우에 유리하다.44 하지만 대규모 데이터셋에서의 인덱스 구축 시간과 검색 속도는 GPU를 활용하는 Faiss에 비해 느리다.46</li>
<li><strong>Faiss vs. HNSWlib</strong>: HNSW 알고리즘의 원조 구현체인 HNSWlib는 CPU 환경에서 Faiss의 IndexHNSW 구현보다 더 빠른 성능을 보이는 것으로 벤치마크 결과 알려져 있다.47 이는 Faiss의 HNSW 구현이 데이터 저장소와 그래프 구조를 분리하여 메모리 접근 패턴이 비효율적일 수 있기 때문이라는 분석이 있다.47</li>
<li><strong>ann-benchmarks.com</strong>: 이러한 라이브러리들의 성능을 객관적으로 비교하기 위해, ann-benchmarks.com과 같은 공개 벤치마킹 프로젝트가 운영되고 있다.49 이 프로젝트는 GIST-1M, SIFT-1M 등 표준 데이터셋에 대해 다양한 라이브러리들의 QPS 대비 재현율(Recall) 성능을 그래프로 시각화하여 제공하며, 개발자들이 자신의 사용 사례에 가장 적합한 라이브러리를 선택하는 데 중요한 참고 자료가 된다.50</li>
</ul>
<h2>6.  실제 적용 사례 및 Python 활용법</h2>
<p>Faiss는 이론적인 강력함을 넘어 다양한 실제 산업 분야에서 핵심적인 역할을 수행하고 있다. Python 래퍼(wrapper)를 통해 손쉽게 활용할 수 있어 프로토타이핑부터 프로덕션 배포까지 원활하게 연결된다.</p>
<h3>6.1  응용 분야별 아키텍처 설계</h3>
<ul>
<li><strong>추천 시스템 (Recommendation Systems)</strong>: 전자상거래 플랫폼이나 콘텐츠 스트리밍 서비스에서 Faiss는 개인화 추천의 핵심 엔진으로 작동한다. 사용자와 아이템을 각각 임베딩 벡터로 표현한 후, 특정 사용자에 대해 모든 아이템과의 유사도(주로 최대 내적)를 빠르게 계산하여 가장 관련성 높은 아이템을 추천한다. 실시간성과 대규모 아이템 목록 처리 능력이 중요하므로, IndexIVFPQ가 메모리 효율성과 속도의 균형을 맞추기 위해 널리 사용된다.1</li>
<li><strong>이미지/비디오 검색 (Image/Video Search)</strong>: ResNet, VGG, CLIP과 같은 딥러닝 모델을 사용하여 이미지나 비디오에서 특징 벡터(feature vector)를 추출한다. 이 벡터들을 Faiss에 인덱싱하면, 특정 이미지와 시각적으로 유사한 다른 이미지를 찾거나(query-by-example), “웃고 있는 강아지“와 같은 텍스트 쿼리에 해당하는 이미지를 찾는(text-to-image) 시맨틱 이미지 검색 시스템을 구축할 수 있다.53</li>
<li><strong>시맨틱 검색 (NLP) 및 RAG</strong>: 문장이나 문서를 Sentence-BERT와 같은 모델로 임베딩하여 Faiss에 저장하면, 키워드 일치를 넘어 의미적으로 유사한 문서를 검색할 수 있다.3 이는 특히 검색 증강 생성(RAG) 파이프라인에서 중요한 역할을 한다. RAG는 대규모 언어 모델(LLM)이 답변을 생성하기 전에, 사용자 질문과 관련된 정보를 외부 지식 베이스(Faiss로 인덱싱된 문서 코퍼스)에서 먼저 검색(retrieve)하여 컨텍스트로 제공하는 방식이다. Faiss는 이 검색 단계를 빠르고 효율적으로 수행하여 RAG 시스템의 전반적인 성능을 향상시킨다.5</li>
</ul>
<h3>6.2  Faiss 설치 및 Python을 이용한 기본 워크플로우</h3>
<p>Faiss는 Python 환경에서 매우 쉽게 설치하고 사용할 수 있다.</p>
<ul>
<li>
<p><strong>설치</strong>: Anaconda 환경에서는 Conda를 사용하는 것이 권장된다. CPU 버전과 GPU 버전을 각각 다음 명령어로 설치할 수 있다. 두 패키지는 동시에 설치할 수 없다.10</p>
<pre><code class="language-Bash">\# CPU 버전 설치
conda install -c pytorch faiss-cpu
\# GPU 버전 설치 (CUDA 환경 필요)
conda install -c pytorch faiss-gpu
</code></pre>
</li>
</ul>
<pre><code>
pip를 통해서도 설치가 가능하다.17

  ```Bash
  pip install faiss-cpu
  pip install faiss-gpu
</code></pre>
<ul>
<li><strong>기본 코드 예제</strong>: 다음은 numpy 배열로 표현된 벡터들을 IndexFlatL2와 IndexIVFPQ로 인덱싱하고 검색하는 기본적인 Python 워크플로우이다.
<pre><code class="language-Python">import faiss
import numpy as np

# 1. 데이터 준비
d = 64   # 벡터 차원
nb = 100000 # 데이터베이스 벡터 수
nq = 10000 # 쿼리 벡터 수
np.random.seed(1234)
xb = np.random.random((nb, d)).astype('float32')
xb[:, 0] += np.arange(nb) / 1000.
xq = np.random.random((nq, d)).astype('float32')
xq[:, 0] += np.arange(nq) / 1000.

# --- 예제 1: IndexFlatL2 (전수 조사) ---
print("--- IndexFlatL2 예제 ---")
index_flat = faiss.IndexFlatL2(d)  # 인덱스 초기화
print(f"인덱스 훈련 여부: {index_flat.is_trained}")
index_flat.add(xb)         # 벡터 추가
print(f"총 인덱싱된 벡터 수: {index_flat.ntotal}")

k = 4 # 찾을 최근접 이웃 수
D, I = index_flat.search(xq, k)   # 검색 수행
print("검색 결과 (상위 5개 쿼리):")
print("인덱스 (I):\n", I[:5])
print("거리 (D):\n", D[:5])
print("\n")

# --- 예제 2: IndexIVFPQ (근사 검색) ---
print("--- IndexIVFPQ 예제 ---")
nlist = 100     # 클러스터(셀) 수
m = 8        # PQ 부벡터 수
nbits = 8      # 부벡터당 비트 수 (k* = 2^8 = 256)
quantizer = faiss.IndexFlatL2(d) # IVF를 위한 quantizer
index_ivfpq = faiss.IndexIVFPQ(quantizer, d, nlist, m, nbits)

# 훈련 (Training)
print(f"인덱스 훈련 여부 (훈련 전): {index_ivfpq.is_trained}")
index_ivfpq.train(xb) # 훈련 데이터로 훈련
print(f"인덱스 훈련 여부 (훈련 후): {index_ivfpq.is_trained}")

# 추가 (Adding)
index_ivfpq.add(xb)
print(f"총 인덱싱된 벡터 수: {index_ivfpq.ntotal}")

# 검색 (Searching)
index_ivfpq.nprobe = 10 # 검색할 셀의 수 설정
D, I = index_ivfpq.search(xq, k)
print("검색 결과 (상위 5개 쿼리):")
print("인덱스 (I):\n", I[:5])
print("거리 (D):\n", D[:5])

# 인덱스 저장 및 로딩
faiss.write_index(index_ivfpq, "my_index.faiss")
loaded_index = faiss.read_index("my_index.faiss")
print(f"\n로드된 인덱스의 벡터 수: {loaded_index.ntotal}")
</code></pre>
</li>
</ul>
<pre><code>
이 예제는 Faiss의 핵심적인 사용 흐름, 즉 **인덱스 생성 → (필요시) 훈련 → 데이터 추가 → 검색**을 명확히 보여준다.12 또한 faiss.write_index()와 faiss.read_index()를 통해 생성된 인덱스를 디스크에 저장하고 다시 불러올 수 있어, 매번 인덱스를 재구축할 필요 없이 재사용이 가능하다.5

## 7.  결론: Faiss의 현재와 미래


Faiss는 고차원 벡터 유사도 검색 분야에서 단순한 라이브러리를 넘어 하나의 표준으로 자리매김했다. 압도적인 속도, 뛰어난 메모리 효율성, 수십억 개를 넘나드는 확장성, 그리고 사용자의 요구에 맞춰 세밀하게 조정 가능한 유연성을 통해 학계와 산업계 모두에서 필수적인 도구가 되었다.53

Faiss의 가장 큰 의의는 그 '도구상자'로서의 철학에 있다. 특정 시나리오에 국한된 완제품이 아닌, 다양한 알고리즘적 부품(IVF, PQ, HNSW 등)과 하드웨어 가속(GPU, SIMD) 기능을 제공함으로써, 개발자가 직면한 고유한 제약 조건(속도, 정확도, 메모리, 비용) 하에서 최적의 시스템을 직접 설계하고 구축할 수 있는 힘을 부여했다.

현재 Faiss는 독립적인 라이브러리로서의 역할을 넘어, 더 큰 AI 생태계의 핵심 엔진으로 기능하고 있다. Milvus, OpenSearch와 같은 수많은 벡터 데이터베이스가 내부적으로 Faiss를 사용하여 검색 성능을 확보하고 있으며, LangChain, Haystack과 같은 LLM 프레임워크와의 통합을 통해 RAG와 같은 최신 AI 애플리케이션 개발을 가속화하고 있다.11

Faiss의 발전은 현재진행형이다. NVIDIA와의 지속적인 협력을 통해 GpuIndexCagra와 같은 최신 GPU 최적화 그래프 인덱스를 도입하는 등, 새로운 알고리즘 연구와 하드웨어 기술 발전을 끊임없이 흡수하며 진화하고 있다.38 임베딩 기술이 AI의 중심에 있는 한, 이들을 효율적으로 저장하고 검색하는 기술의 중요성은 더욱 커질 것이다. Faiss는 앞으로도 고성능 벡터 검색 기술의 발전을 선도하는 핵심 동력으로 그 역할을 계속해 나갈 것이다.

## 8. 참고 자료


1. FAISS (Facebook AI Similarity Search) - Metadesign Solutions, https://metadesignsolutions.com/faiss-facebook-ai-similarity-search/
2. Building a simple document search using FAISS and OpenAI | Forays Into AI, https://www.foraysintoai.com/generative-ai/semantic-search-using-faiss
3. What behind the architecture of FAISS in Semantic Search | by Mysterious obscure | Medium, https://medium.com/@mysterious_obscure/what-behind-the-architecture-of-faiss-in-semantic-search-c2c088fd7eaf
4. Semantic Image Search with OpenAI CLIP and Meta FAISS - Ultralytics YOLO Docs, https://docs.ultralytics.com/guides/similarity-search/
5. Understanding FAISS Vector Store and its Advantages | by thakur.amrita - Medium, https://medium.com/@amrita.thakur/understanding-faiss-vector-store-and-its-advantages-cdc7b54afe47
6. What Is Faiss (Facebook AI Similarity Search)? - DataCamp, https://www.datacamp.com/blog/faiss-facebook-ai-similarity-search
7. Billion-scale similarity search with GPUs - Electrical Engineering and Computer Science, https://www.eecs.umich.edu/courses/cse584/static_files/papers/1702.08734.pdf
8. Faiss: A library for efficient similarity search - Engineering at Meta - Facebook, https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/
9. Billion-scale Approximate Nearest Neighbor Search - GitHub Pages, https://wangzwhu.github.io/home/file/acmmm-t-part3-ann.pdf
10. Welcome to Faiss Documentation — Faiss documentation, https://faiss.ai/
11. FAISS - Wikipedia, https://en.wikipedia.org/wiki/FAISS
12. Understanding FAISS : Part 2. Compression Techniques and Product… | by Vedashree Patil | dotStar | Medium, https://medium.com/dotstar/understanding-faiss-part-2-79d90b1e5388
13. facebookresearch/faiss: A library for efficient similarity search and clustering of dense vectors. - GitHub, https://github.com/facebookresearch/faiss
14. Product quantization for nearest neighbor search - PubMed, https://pubmed.ncbi.nlm.nih.gov/21088323/
15. Home · facebookresearch/faiss Wiki - GitHub, https://github.com/facebookresearch/faiss/wiki
16. Guidelines to choose an index · facebookresearch/faiss Wiki · GitHub, https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index
17. What is FAISS? - GeeksforGeeks, https://www.geeksforgeeks.org/data-science/what-is-faiss/
18. FAISS: The Low-Level Inner Workings of Vector Search and Optimization | by Nishtha kukreti, https://medium.com/@nishthakukreti.01/faiss-the-low-level-inner-workings-of-vector-search-and-optimization-cd642dec3518
19. Nearest Neighbor Indexes for Similarity Search | Pinecone, https://www.pinecone.io/learn/series/faiss/vector-indexes/
20. Comparing different Indexes for RAG | by saidinesh pola - Medium, https://medium.com/@saidines12/comparing-different-indexes-for-rag-62ecd798d5ac
21. Facebook AI Similarity Search - Kaggle, https://www.kaggle.com/code/mustafashoukat/facebook-ai-similarity-search
22. 349 - Understanding FAISS for efficient similarity search of dense vectors - YouTube, https://www.youtube.com/watch?v=0jOlZpFFxCE
23. Faiss indexes · facebookresearch/faiss Wiki - GitHub, https://github.com/facebookresearch/faiss/wiki/Faiss-indexes
24. Mastering Faiss Vector Database: A Beginner's Handbook - TiDB, https://www.pingcap.com/article/mastering-faiss-vector-database-a-beginners-handbook/
25. Product Quantization: Compressing high-dimensional vectors by 97 ..., https://www.pinecone.io/learn/series/faiss/product-quantization/
26. Similarity Search, Part 2: Product Quantization - Towards Data Science, https://towardsdatascience.com/similarity-search-product-quantization-b2a1a6397701/
27. (PDF) Product Quantization for Nearest Neighbor Search, https://www.researchgate.net/publication/47815472_Product_Quantization_for_Nearest_Neighbor_Search
28. "Product quantization for nearest neighbor search", Jégou, Douze, Schmid - David Stutz, https://davidstutz.de/product-quantization-for-nearest-neighbor-search-jegou-douze-schmid/
29. Faiss Quantizers — BGE documentation, https://bge-model.com/tutorial/3_Indexing/3.1.4.html
30. IndexIVFFlat y IndexIVFPQ - DEV Community, https://dev.to/alonsoir/indexivfflat-y-indexivfpq-2el8
31. What is Faiss (Facebook AI Similarity Search)? - Zilliz Learn, https://zilliz.com/learn/faiss
32. Hierarchical Navigable Small Worlds (HNSW) - Pinecone, https://www.pinecone.io/learn/series/faiss/hnsw/
33. Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs - arXiv, https://arxiv.org/abs/1603.09320
34. HNSW indexing in Vector Databases: Simple explanation and code | by Will Tai - Medium, https://medium.com/@wtaisen/hnsw-indexing-in-vector-databases-simple-explanation-and-code-3ef59d9c1920
35. Great Algorithms Are Not Enough - Pinecone, https://www.pinecone.io/blog/hnsw-not-enough/
36. Efficient and robust approximate nearest neighbor search using ..., https://arxiv.org/pdf/1603.09320
37. Faiss - Meta AI, https://ai.meta.com/tools/faiss/
38. Accelerating GPU indexes in Faiss with NVIDIA cuVS – Lifeboat News: The Blog, https://lifeboat.com/blog/2025/08/accelerating-gpu-indexes-in-faiss-with-nvidia-cuvs
39. Faiss on the GPU · facebookresearch/faiss Wiki · GitHub, https://github.com/facebookresearch/faiss/wiki/Faiss-on-the-GPU
40. [1702.08734] Billion-scale similarity search with GPUs - arXiv, https://arxiv.org/abs/1702.08734
41. Scaling Semantic Search with FAISS: Challenges and Solutions for Billion-Scale Datasets | by Devesh Bajaj | Medium, https://medium.com/@deveshbajaj59/scaling-semantic-search-with-faiss-challenges-and-solutions-for-billion-scale-datasets-1cacb6f87f95
42. Low level benchmarks · facebookresearch/faiss Wiki - GitHub, https://github.com/facebookresearch/faiss/wiki/Low-level-benchmarks
43. Faiss vs ScaNN on Vector Search - Zilliz blog, https://zilliz.com/blog/faiss-vs-scann-choosing-the-right-tool-for-vector-search
44. Faiss: A library for efficient similarity search | Hacker News, https://news.ycombinator.com/item?id=35378521
45. How do FAISS and Annoy compare in terms of index build time and memory usage for large datasets, and what might drive the decision to use one over the other? - Milvus, https://milvus.io/ai-quick-reference/how-do-faiss-and-annoy-compare-in-terms-of-index-build-time-and-memory-usage-for-large-datasets-and-what-might-drive-the-decision-to-use-one-over-the-other
46. What's the difference between FAISS, Annoy, and ScaNN? - Milvus, https://milvus.io/ai-quick-reference/whats-the-difference-between-faiss-annoy-and-scann
47. Kids! Use hnswlib for HNSW - Terence Z. Liu, https://terencezl.github.io/blog/2022/09/28/kids-use-hnswlib/
48. Faiss: A library for efficient similarity search - Brian Lovin, https://brianlovin.com/hn/35378521
49. ANN-Benchmarks, https://ann-benchmarks.com/
50. Understanding ANN Benchmarks - Zilliz, https://zilliz.com/glossary/ann-benchmarks
51. gist-960-euclidean (k = 10) - ANN-Benchmarks, https://ann-benchmarks.com/gist-960-euclidean_10_euclidean.html
52. Speeding up similarity search in recommender systems using FAISS basics: Part 1 | by Caboom.ai | Artificial Intelligence in Plain English, https://ai.plainenglish.io/speeding-up-similarity-search-in-recommender-systems-using-faiss-basics-part-i-ec1b5e92c92d
53. Faiss: A Fast, Efficient Similarity Search Library - Programmer.ie: Modern AI programming, https://programmer.ie/post/faiss/
54. Building Image Similarity Search with VGG16 and FAISS - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2024/07/image-similarity-search-with-vgg16-and-fais/
55. Building an Image Similarity Search Engine with FAISS and CLIP | Towards Data Science, https://towardsdatascience.com/building-an-image-similarity-search-engine-with-faiss-and-clip-2211126d08fa/
56. Tutorial: semantic search using Faiss &amp; MPNet - Deepnote, https://deepnote.com/blog/semantic-search-using-faiss-and-mpnet
57. Faiss | 🦜️ LangChain, https://python.langchain.com/docs/integrations/vectorstores/faiss/
58. faiss-cpu - PyPI, https://pypi.org/project/faiss-cpu/
59. FAISS, Up Close: Fast Similarity Search For The Vector Age | Joshua Berkowitz, https://joshuaberkowitz.us/blog/github-repos-8/faiss-up-close-fast-similarity-search-for-the-vector-age-849</code></pre>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>