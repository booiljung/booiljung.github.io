<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:초해상화 영상 기반 최적 디블러링 기술</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>초해상화 영상 기반 최적 디블러링 기술</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">블러 제거 (Deblur)</a> / <span>초해상화 영상 기반 최적 디블러링 기술</span></nav>
                </div>
            </header>
            <article>
                <h1>초해상화 영상 기반 최적 디블러링 기술</h1>
<h2>1. 서론</h2>
<h3>1.1  문제 정의: 영상 열화의 복합성</h3>
<p>현실 세계의 디지털 영상은 촬영, 압축, 전송 등 다양한 과정을 거치면서 화질이 저하된다. 이러한 열화 현상 중 가장 대표적인 두 가지는 해상도 저하(low-resolution)와 블러(blur)이다.1 해상도 저하는 영상의 세밀한 디테일을 상실하게 만들며, 블러는 카메라 흔들림, 피사체 움직임, 초점 불일치 등으로 인해 영상의 경계를 흐릿하게 만든다. 중요한 점은 이 두 가지 열화가 독립적으로 발생하는 것이 아니라, 종종 복합적이고 상호 의존적으로 나타난다는 것이다. 예를 들어, 저해상도 영상에서는 블러를 유발한 원인(블러 커널)을 정확히 추정하기 어렵고, 반대로 블러가 심한 영상은 초해상화 과정에서 복원해야 할 고유의 고주파수(high-frequency) 정보를 이미 상당 부분 상실한 상태이다. 따라서 흐릿하고 해상도가 낮은 영상을 선명한 고품질 영상으로 복원하는 것은 단순히 두 문제를 개별적으로 해결하는 것을 넘어, 이들의 복합적인 상호작용을 이해하고 동시에 처리하는 통합적인 접근법을 요구한다.</p>
<h3>1.2  초해상화와 디블러링: 개별적 접근의 한계</h3>
<p>초해상화(Super-Resolution, SR)는 저해상도(Low-Resolution, LR) 영상으로부터 고해상도(High-Resolution, HR) 영상을 복원하여 손실된 디테일을 되살리는 기술이다.1 반면, 디블러링(Deblurring)은 블러 현상으로 인해 흐릿해진 영상에서 선명한 영상을 복원하는 기술을 의미한다.2 전통적으로 이 두 문제는 별개의 연구 분야로 다루어져 왔으며, 가장 직관적인 해결책은 두 기술을 순차적으로 적용하는 것이다.</p>
<p>하지만 이러한 순차적 접근 방식은 심각한 한계를 내포한다. 만약 디블러링을 먼저 수행하고 초해상화를 적용한다면, 디블러링 과정에서 블러 커널을 추정하고 제거하는 과정 중에 필연적으로 이미지의 미세한 텍스처와 같은 고주파수 정보가 영구적으로 손실된다. 이 손실된 정보는 후속 초해상화 단계에서 복원될 수 없으며, 결과적으로 초해상화 모델의 성능을 근본적으로 제한하게 된다.8 반대로, 초해상화를 먼저 적용할 경우 저해상도 영상에 존재하는 블러와 노이즈 아티팩트가 함께 증폭되어, 후속 디블러링 단계에서 블러 커널을 추정하는 것을 더욱 어렵게 만들고 새로운 왜곡을 유발할 수 있다. 이처럼 첫 번째 단계에서 발생한 오류와 정보 손실이 두 번째 단계에서 증폭되는 ‘오류 전파(error propagation)’ 문제는 순차적 접근법의 근본적인 딜레마이다.</p>
<h3>1.3  공동 복원(Joint Restoration)의 필요성 및 연구 목표</h3>
<p>순차적 처리의 한계를 극복하기 위한 대안으로 초해상화와 디블러링을 하나의 통합된 프레임워크 내에서 동시에 최적화하는 공동 복원(Joint Restoration) 또는 공동 최적화(Joint Optimization) 접근법이 필수적으로 요구된다.8 이 접근법의 목표는 단일의 흐릿하고 해상도가 낮은 영상으로부터 시작하여, 두 가지 열화 요인을 상호 보완적으로 고려함으로써 최종적으로 선명한 고해상도 영상을 직접 복원하는 것이다. 이는 두 문제가 서로 얽혀 있다는 현실을 인정하고, 하나의 역문제(inverse problem)로 재정의하여 해결하려는 시도이다.</p>
<p>본 안내서는 초해상화 기술을 기반으로 최적의 디블러링 성능을 달성하기 위한 통합적 접근법을 심층적으로 탐구하는 것을 목표로 한다. 이를 위해 영상 열화의 수학적 모델부터 시작하여, 초해상화와 디블러링 각각의 핵심 기술 발전 과정을 추적한다. 나아가, 두 기술을 융합하는 공동 복원 프레임워크의 이론적 배경과 최신 딥러닝 아키텍처를 분석하고, 성능 평가를 위한 객관적인 지표와 표준 벤치마크 데이터셋을 검토한다. 최종적으로 현재 기술의 한계와 미래 연구 방향을 제시함으로써, 해당 분야에 대한 포괄적이고 심도 있는 분석을 제공하고자 한다.</p>
<h2>2.  영상 열화 모델의 수학적 정립</h2>
<h3>2.1  영상 열화 프로세스의 일반 모델</h3>
<p>영상 복원 기술을 이해하기 위해서는 먼저 영상이 어떻게 열화되는지를 수학적으로 모델링하는 과정이 선행되어야 한다. 일반적으로 관측된 열화 영상 <span class="math math-inline">g</span>는 원본 고해상도 선명 영상 <span class="math math-inline">f</span>가 특정 열화 함수(degradation function) <span class="math math-inline">H</span>에 의해 변환되고, 그 과정에서 추가적인 노이즈(additive noise) <span class="math math-inline">n</span>이 더해진 결과물로 간주된다.11 이 관계는 영상 복원 문제를 주어진 관측 영상 <span class="math math-inline">g</span>와 열화 과정에 대한 사전 지식(prior knowledge)을 바탕으로 원본 영상 <span class="math math-inline">f</span>를 추정하는 잘 정의된 역문제(Inverse Problem)로 정립하는 기반이 된다.2 이 모델은 복원 알고리즘이 해결해야 할 목표를 명확히 하고, 문제의 난이도를 결정하는 핵심 요소들을 정의한다.</p>
<h3>2.2  공간 도메인 표현식</h3>
<p>공간 도메인(spatial domain)에서 영상 열화 과정은 주로 컨볼루션(Convolution) 연산을 통해 모델링된다. 특히, 블러와 해상도 저하가 동시에 발생한 경우, 관측된 저해상도 블러 영상 <span class="math math-inline">I_{LR}</span>은 원본 고해상도 선명 영상 <span class="math math-inline">I_{HR}</span>이 블러 커널(blur kernel) <span class="math math-inline">k</span>와 컨볼루션 연산을 거쳐 흐려진 후, 다운샘플링(downsampling) 연산자 <span class="math math-inline">D_s</span>에 의해 해상도가 낮아지고, 마지막으로 노이즈 <span class="math math-inline">\sigma</span>가 추가되는 과정으로 표현할 수 있다.1</p>
<p>이를 수식으로 표현하면 다음과 같다.<br />
<span class="math math-display">
I_{LR} = D_s(I_{HR} * k) + \sigma
</span><br />
여기서 <span class="math math-inline">*</span> 기호는 컨볼루션 연산을 의미한다.2 이 모델은 블러가 공간적으로 변하지 않는(spatially invariant)다는 가정 하에 성립하며, 복원 알고리즘은 주어진 <span class="math math-inline">I_{LR}</span>과 <span class="math math-inline">k</span>, <span class="math math-inline">D_s</span>, <span class="math math-inline">\sigma</span>에 대한 가정으로부터 <span class="math math-inline">I_{HR}</span>을 추정하는 것을 목표로 한다.</p>
<h3>2.3  주파수 도메인 표현식</h3>
<p>공간 도메인에서의 컨볼루션 연산은 계산적으로 복잡하지만, 컨볼루션 정리(Convolution Theorem)에 따라 주파수 도메인(frequency domain)으로 변환하면 간단한 곱셈 연산으로 표현될 수 있다. 이는 알고리즘의 계산 효율성을 높일 뿐만 아니라, 열화 함수가 영상의 주파수 성분에 미치는 영향을 직관적으로 분석할 수 있게 해준다.11</p>
<p>공간 도메인의 열화 모델을 푸리에 변환(Fourier Transform)하면 다음과 같은 주파수 도메인 표현식을 얻을 수 있다.<br />
<span class="math math-display">
G(u,v) = H(u,v)F(u,v) + N(u,v)
</span><br />
여기서 <span class="math math-inline">G(u,v)</span>, <span class="math math-inline">H(u,v)</span>, <span class="math math-inline">F(u,v)</span>, <span class="math math-inline">N(u,v)</span>는 각각 열화 영상 <span class="math math-inline">g(x,y)</span>, 열화 함수 <span class="math math-inline">h(x,y)</span>, 원본 영상 <span class="math math-inline">f(x,y)</span>, 노이즈 <span class="math math-inline">n(x,y)</span>를 푸리에 변환한 결과이다.11 이 식에서 <span class="math math-inline">H(u,v)</span>는 일반적으로 저주파 통과 필터(low-pass filter)의 특성을 가지므로, 원본 영상의 고주파 성분(<span class="math math-inline">F(u,v)</span>의 높은 <span class="math math-inline">u,v</span> 값)을 감쇠시켜 정보 손실을 야기한다. 영상 복원은 감쇠된 주파수 성분을 복원하는 과정으로 해석될 수 있다.</p>
<h3>2.4  실용적 열화 모델의 필요성</h3>
<p>초기 영상 복원 연구들은 학문적 편의를 위해 주로 간단하고 이상적인 열화 모델을 가정했다. 예를 들어, 다운샘플링은 바이큐빅 보간법(Bicubic interpolation)을, 블러는 가우시안(Gaussian) 블러를 사용하는 식이었다.16 그러나 이러한 단순한 모델은 실제 카메라 센서의 노이즈, 렌즈 왜곡, 압축 아티팩트, 인터넷 전송 중 발생하는 손실 등 현실 세계의 복잡하고 예측 불가능한 열화 과정을 제대로 반영하지 못했다. 이로 인해 단순한 합성 데이터로 학습된 딥러닝 모델은 실제 영상에 적용했을 때 성능이 급격히 저하되는 ‘도메인 격차(domain gap)’ 문제를 겪었다.</p>
<p>이러한 한계를 극복하기 위해 BSRGAN, Real-ESRGAN과 같은 최신 연구들은 ’실용적 열화 모델(Practical Degradation Model)’을 제안하며 패러다임의 전환을 이끌었다. 이 모델들은 블러(가우시안, 이방성), 다운샘플링(바이큐빅, 빌리니어, 최근접 이웃), 노이즈(가우시안, 포아송, JPEG 압축), 리사이징 등 다양한 열화 요인들을 정의하고, 이들의 적용 순서와 파라미터를 무작위로 조합(random shuffle)하거나 여러 번 중첩하여(high-order) 적용한다.17 이렇게 생성된 방대하고 다양한 합성 데이터는 실제 영상의 열화 분포에 훨씬 더 가깝다. 결과적으로, 이 데이터로 학습된 딥러닝 모델은 특정 열화에 과적합되지 않고, 미지의 실제 열화에 대해 훨씬 강건하고 우수한 일반화 성능을 보이게 된다. 이는 단순히 네트워크 구조를 개선하는 것을 넘어, 학습 데이터의 질이 모델의 실용성을 결정하는 핵심 요소임을 보여주는 중요한 발전이다.</p>
<h2>3.  초해상화(Super-Resolution) 기술의 발전</h2>
<h3>3.1  전통적 보간법과 그 한계</h3>
<p>초해상화 기술의 가장 초기 형태는 보간법(interpolation) 기반의 접근 방식이었다. 최근접 이웃 보간법(Nearest Neighbor), 쌍선형 보간법(Bilinear Interpolation), 바이큐빅 보간법(Bicubic Interpolation)과 같은 전통적인 방법들은 확대하고자 하는 위치의 픽셀 값을 주변에 이미 존재하는 픽셀들의 값으로부터 수학적으로 추정한다.4 예를 들어, 바이큐빅 보간법은 주변의 16개(<span class="math math-inline">4 \times 4</span>) 픽셀을 참조하여 가중 평균을 계산함으로써 더 부드러운 결과물을 생성한다.</p>
<p>그러나 이러한 방법들은 근본적인 한계를 가지고 있다. 보간법은 기존에 없는 새로운 정보를 창출하는 것이 아니라, 단지 존재하는 정보를 바탕으로 빈 공간을 채우는 방식이다. 이 과정에서 이미지의 날카로운 경계나 미세한 텍스처와 같은 고주파수 정보는 생성되지 않고 오히려 평균화되어 사라진다. 결과적으로 보간법을 통해 확대된 이미지는 원본보다 크기는 커지지만, 세부 디테일이 뭉개지고 전반적으로 흐릿해 보이는 ‘과도한 평활화(over-smoothing)’ 현상이 발생한다.21 이는 저해상도 이미지에 내재된 정보의 한계를 극복하지 못하기 때문에 발생하는 필연적인 결과이다.</p>
<h3>3.2  CNN 기반 초기 모델</h3>
<p>딥러닝의 등장은 초해상화 분야에 혁신을 가져왔다. 합성곱 신경망(Convolutional Neural Networks, CNN)은 데이터로부터 직접 고주파수 디테일을 복원하는 방법을 학습함으로써 보간법의 한계를 뛰어넘었다.</p>
<h4>3.2.1 SRCNN (Super-Resolution Convolutional Neural Network)</h4>
<p>SRCNN은 딥러닝을 초해상화 문제에 성공적으로 적용한 선구적인 모델이다.1 SRCNN의 구조는 비교적 단순한 3개의 컨볼루션 레이어로 구성된다. 이 모델의 가장 큰 특징은 ‘사전 업샘플링(pre-upsampling)’ 방식을 사용한다는 점이다. 즉, 저해상도 입력 영상을 먼저 바이큐빅 보간법을 사용해 목표 해상도로 확대한 후, 이를 CNN에 입력한다. 네트워크의 세 단계는 다음과 같다:</p>
<ol>
<li><strong>패치 추출 및 표현 (Patch extraction and representation):</strong> 첫 번째 레이어는 확대된 이미지에서 겹치는 이미지 패치들을 추출하고 각 패치를 고차원 벡터로 표현한다.</li>
<li><strong>비선형 매핑 (Non-linear mapping):</strong> 두 번째 레이어는 추출된 고차원 특징 벡터를 다른 고차원 벡터로 비선형적으로 매핑한다. 이 단계가 실질적인 초해상화 학습의 핵심이다.</li>
<li><strong>재구성 (Reconstruction):</strong> 마지막 레이어는 비선형 매핑을 거친 특징 벡터들을 다시 이미지 공간으로 변환하여 최종 고해상도 이미지를 생성한다.</li>
</ol>
<p>SRCNN은 엔드-투-엔드(end-to-end) 방식으로 저해상도와 고해상도 간의 매핑 함수를 직접 학습할 수 있음을 보여주었지만, 모든 연산을 고해상도 공간에서 수행하기 때문에 계산 비용이 매우 높다는 단점이 있었다.</p>
<h4>3.2.2 FSRCNN (Fast SRCNN)</h4>
<p>FSRCNN은 SRCNN의 높은 계산 비용 문제를 해결하기 위해 제안되었다.1 핵심적인 개선점은 ‘사후 업샘플링(post-upsampling)’ 구조를 채택한 것이다. 즉, 업샘플링 과정을 네트워크의 가장 마지막 단계로 옮기고, 모든 특징 추출 및 비선형 매핑 연산을 원본 저해상도 공간에서 직접 수행한다. 이로 인해 연산량이 획기적으로 줄어들었다. 또한, FSRCNN은 다음과 같은 구조적 개선을 포함했다:</p>
<ul>
<li><strong>축소 및 확장 레이어:</strong> 특징 추출 직후에 <span class="math math-inline">1 \times 1</span> 컨볼루션을 사용하여 채널 수를 줄이고(shrinking), 비선형 매핑 후에 다시 채널 수를 늘리는(expanding) 구조를 도입하여 파라미터 수를 감소시켰다.</li>
<li><strong>Deconvolution 레이어:</strong> 네트워크의 마지막에 학습 가능한 업샘플링 레이어인 Deconvolution(Transposed Convolution)을 사용하여 고정된 보간법보다 우수한 품질의 업샘플링을 수행했다.</li>
</ul>
<h4>3.2.3 ESPCN (Efficient Sub-Pixel Convolutional Network)</h4>
<p>ESPCN은 FSRCNN과 마찬가지로 사후 업샘플링 방식을 사용하지만, 업샘플링을 위한 더욱 효율적이고 효과적인 메커니즘인 ’서브픽셀 컨볼루션(Sub-pixel Convolution)’을 제안했다.1 이 기법은 저해상도 특징 맵의 채널(depth) 차원을 공간(space) 차원으로 재배열하는 ‘depth-to-space’ 연산을 통해 해상도를 높인다. 예를 들어, <span class="math math-inline">H \times W</span> 크기의 특징 맵을 <span class="math math-inline">r</span>배 확대하기 위해, 네트워크는 <span class="math math-inline">H \times W \times C \cdot r^2</span> 채널을 가진 특징 맵을 출력한다. 이후 이 특징들을 재배열하여 <span class="math math-inline">rH \times rW \times C</span> 크기의 고해상도 특징 맵을 형성한다. 이 방식은 Deconvolution 레이어에서 종종 발생하는 체커보드 아티팩트(checkerboard artifacts) 문제를 해결하면서도 계산 효율성을 크게 높였다.</p>
<h3>3.3  성능 고도화를 위한 심층 네트워크</h3>
<p>초기 CNN 모델들의 성공 이후, 연구자들은 더 깊고 정교한 네트워크 구조를 통해 초해상화 성능을 한계까지 끌어올리기 시작했다.</p>
<h4>3.3.1 EDSR (Enhanced Deep Super-Resolution)</h4>
<p>EDSR은 당시 SOTA(State-of-the-Art) 성능을 달성한 대표적인 심층 네트워크 모델이다.21 이 모델은 ResNet 아키텍처에서 영감을 받았지만, 초해상화와 같은 저수준 비전(low-level vision) 태스크에 최적화하기 위해 중요한 수정을 가했다. 가장 핵심적인 변화는 ResNet의 잔차 블록(residual block)에서 배치 정규화(Batch Normalization, BN) 레이어를 제거한 것이다. 연구팀은 BN 레이어가 특징의 스케일을 정규화하면서 이미지의 미세한 색상 대비와 같은 범위 유연성(range flexibility)을 제한하여 성능에 오히려 해가 된다는 사실을 발견했다. BN 레이어를 제거함으로써 모델의 표현력을 높이고 메모리 사용량을 줄여, 동일한 자원으로 더 깊고 넓은 모델을 학습시킬 수 있게 되었다. 이로 인해 EDSR은 이전 모델들보다 훨씬 깊은 네트워크 구조를 가지면서도 안정적인 학습이 가능했고, 성능을 크게 향상시켰다.</p>
<h4>3.3.2 GAN 기반 모델 (e.g., ESRGAN)</h4>
<p>전통적인 심층 네트워크들은 주로 픽셀 단위의 평균 제곱 오차(Mean Squared Error, MSE)를 손실 함수로 사용하여 학습되었다. MSE를 최소화하는 것은 높은 PSNR(Peak Signal-to-Noise Ratio) 값을 달성하는 데는 효과적이지만, 종종 인간의 시각 시스템이 인지하기에는 지나치게 부드럽고 세부 텍스처가 부족한, 즉 ’인식 품질(perceptual quality)’이 낮은 이미지를 생성하는 문제가 있었다.1</p>
<p>이러한 ’인식-왜곡 상충 관계(Perception-Distortion Tradeoff)’를 해결하기 위해 생성적 적대 신경망(Generative Adversarial Networks, GAN) 기반의 초해상화 모델이 등장했다. SRGAN과 이를 개선한 ESRGAN이 대표적이다.1 이 모델들은 두 개의 네트워크, 즉 고해상도 이미지를 생성하는 ’생성자(Generator)’와 생성된 이미지가 실제 고해상도 이미지인지 판별하는 ’판별자(Discriminator)’를 적대적으로 학습시킨다. 생성자는 판별자를 속일 수 있을 만큼 사실적인 이미지를 만들도록, 판별자는 생성된 이미지와 실제 이미지를 더 잘 구별하도록 경쟁적으로 학습한다. 이 과정에서 픽셀 단위의 MSE 손실뿐만 아니라, 이미지의 고수준 특징(예: VGG 네트워크의 중간 특징 맵) 간의 차이를 측정하는 ’인식 손실(Perceptual Loss)’과 판별자의 출력을 이용한 ’적대적 손실(Adversarial Loss)’을 함께 사용한다. 그 결과, ESRGAN은 PSNR 수치는 다소 낮을 수 있어도, 인간이 보기에 훨씬 더 자연스럽고 세밀한 텍스처를 가진 이미지를 생성할 수 있게 되었다.</p>
<h3>3.4  Transformer 기반 최신 아키텍처</h3>
<p>최근 몇 년간 자연어 처리 분야에서 시작된 Transformer는 컴퓨터 비전 분야에서도 강력한 성능을 입증하며 초해상화 기술의 새로운 지평을 열었다. Transformer의 핵심인 Self-Attention 메커니즘은 이미지의 전역적인(global) 문맥 정보를 효과적으로 모델링하여 CNN의 제한된 수용 영역(receptive field) 문제를 극복할 수 있다.</p>
<h4>3.4.1 SwinIR</h4>
<p>SwinIR은 Swin Transformer를 영상 복원 문제에 성공적으로 적용한 모델이다.25 기존 Vision Transformer가 고해상도 이미지에 적용될 때 발생하는 막대한 계산량을 해결하기 위해, SwinIR은 ’이동된 윈도우 기반 Self-Attention(Shifted Window Self-Attention)’을 사용한다. 이 기법은 전체 이미지에 대해 어텐션을 계산하는 대신, 겹치지 않는 작은 윈도우 내부에서만 지역적으로 어텐션을 계산하고, 다음 레이어에서는 이 윈도우를 이동시켜 인접 윈도우 간의 정보 교류를 가능하게 한다. 이를 통해 계산 복잡도를 이미지 크기에 대해 선형적으로 줄이면서도 지역적 정보와 전역적 정보를 효율적으로 통합할 수 있다. SwinIR의 핵심 구성 요소인 ’잔여 Swin Transformer 블록(Residual Swin Transformer Block, RSTB)’은 이러한 Swin Transformer 레이어 여러 개와 잔차 연결(residual connection)을 결합하여 깊은 특징을 효과적으로 추출하고 안정적인 학습을 가능하게 한다.</p>
<h4>3.4.2 Restormer</h4>
<p>Restormer는 고해상도 영상 복원을 위해 더욱 효율적인 Transformer 구조를 제안한 모델이다.29 Restormer의 핵심 혁신은 ’Multi-Dconv Head Transposed Attention (MDTA)’에 있다. 기존 Transformer가 픽셀 간의 관계를 모델링하기 위해 공간 차원(spatial dimension)에서 Self-Attention을 계산하는 것과 달리, MDTA는 채널 차원(channel dimension)에서 Self-Attention을 계산한다. 즉, 특징 맵의 채널 간 공분산(cross-covariance)을 계산하여 어텐션 맵을 생성한다. 이는 픽셀 간의 장거리 의존성을 명시적으로 계산하지 않고도 전역적인 문맥 정보를 효율적으로 포착할 수 있게 해준다. 또한, ’Gated-Dconv Feed-Forward Network (GDFN)’는 게이팅 메커니즘을 통해 불필요한 특징의 흐름을 제어하고 중요한 정보만을 다음 레이어로 전달하여 특징 표현을 더욱 정제한다. 이러한 설계 덕분에 Restormer는 고해상도 이미지를 패치로 나누지 않고 전체적으로 처리하면서도 계산 효율성을 유지할 수 있으며, 다양한 영상 복원 태스크에서 SOTA 성능을 달성했다.</p>
<p>이러한 아키텍처의 발전 과정은 단순히 성능을 높이는 것을 넘어, 계산 효율성, 수용 영역의 크기, 그리고 인식 품질이라는 세 가지 핵심 요소 사이의 균형을 맞추려는 지속적인 노력의 결과물이다. SRCNN의 비효율성은 FSRCNN과 ESPCN의 사후 업샘플링 구조를 탄생시켰고, CNN의 제한된 수용 영역은 EDSR과 같은 심층 네트워크와 Transformer의 등장을 촉발했다. 또한, 픽셀 기반 손실 함수의 한계는 GAN과 인식 손실의 도입으로 이어졌다. 마지막으로, Transformer의 계산 복잡도 문제는 SwinIR과 Restormer의 혁신적인 어텐션 메커니즘을 통해 해결되었다. 이처럼 각 기술의 등장은 이전 기술의 한계를 극복하기 위한 필연적인 과정이었다.</p>
<table><thead><tr><th>모델 (Model)</th><th>발표 연도</th><th>핵심 아키텍처</th><th>업샘플링 방식</th><th>주요 혁신 및 기여</th></tr></thead><tbody>
<tr><td><strong>SRCNN</strong></td><td>2014</td><td>3-Layer CNN</td><td>사전 업샘플링 (Bicubic)</td><td>딥러닝을 SR에 최초로 적용한 엔드-투-엔드 모델 1</td></tr>
<tr><td><strong>FSRCNN</strong></td><td>2016</td><td>Shallow CNN</td><td>사후 업샘플링 (Deconvolution)</td><td>저해상도 공간에서 특징 추출, 연산 속도 획기적 개선 1</td></tr>
<tr><td><strong>ESPCN</strong></td><td>2016</td><td>Shallow CNN</td><td>사후 업샘플링 (Sub-pixel Conv)</td><td>효율적인 ‘depth-to-space’ 업샘플링 방식 제안 1</td></tr>
<tr><td><strong>EDSR</strong></td><td>2017</td><td>Deep ResNet</td><td>사후 업샘플링 (Sub-pixel Conv)</td><td>배치 정규화 제거를 통해 매우 깊은 네트워크 학습 성공 21</td></tr>
<tr><td><strong>ESRGAN</strong></td><td>2018</td><td>Deep ResNet + GAN</td><td>사후 업샘플링 (Sub-pixel Conv)</td><td>적대적 학습과 인식 손실을 통해 사실적인 텍스처 생성 1</td></tr>
<tr><td><strong>SwinIR</strong></td><td>2021</td><td>Swin Transformer</td><td>픽셀 셔플 (Pixel Shuffle)</td><td>이동된 윈도우 기반 Self-Attention으로 효율과 성능 동시 달성 28</td></tr>
<tr><td><strong>Restormer</strong></td><td>2022</td><td>Transformer</td><td>Transposed Conv</td><td>채널 차원 Self-Attention으로 고해상도 영상의 전역 문맥 효율적 처리 31</td></tr>
</tbody></table>
<h2>4.  영상 디블러링(Deblurring) 기술의 원리</h2>
<h3>4.1  디블러링의 기본 원리: Deconvolution</h3>
<p>영상 디블러링은 블러가 적용된 영상으로부터 원본의 선명한 영상을 복원하는 기술이다. 수학적으로 이 과정은 영상 열화 모델의 역연산을 수행하는 것과 같다. 앞서 정의한 열화 모델 <span class="math math-inline">g = h * f + n</span>에서, 블러 영상 <span class="math math-inline">g</span>와 블러 커널 <span class="math math-inline">h</span>를 알고 있을 때 원본 영상 <span class="math math-inline">f</span>를 추정하는 과정을 디컨볼루션(Deconvolution)이라고 한다.2</p>
<p>하지만 이 역문제는 본질적으로 매우 불안정하고 풀기 어려운 비정형 문제(ill-posed problem)이다.2 그 이유는 블러 커널 <span class="math math-inline">h</span>가 대부분 저주파 통과 필터(low-pass filter)의 특성을 갖기 때문이다. 즉, 블러 과정에서 이미지의 날카로운 경계나 미세한 텍스처에 해당하는 고주파수 정보가 크게 감쇠되거나 완전히 소실된다. 주파수 도메인에서 디컨볼루션은 나눗셈(<span class="math math-inline">F(u,v) \approx G(u,v) / H(u,v)</span>)으로 표현되는데, <span class="math math-inline">H(u,v)</span>의 고주파수 영역 값이 0에 가깝기 때문에 작은 노이즈 성분 <span class="math math-inline">N(u,v)</span>가 크게 증폭되어 결과를 완전히 왜곡시킬 수 있다. 따라서 안정적인 해를 구하기 위해서는 추가적인 제약 조건이나 사전 정보(prior)가 반드시 필요하다.</p>
<h3>4.2  Non-Blind Deconvolution vs. Blind Deconvolution</h3>
<p>디블러링 문제는 블러 커널(PSF, Point Spread Function)의 인지 여부에 따라 두 가지로 나뉜다.</p>
<ul>
<li><strong>Non-Blind Deconvolution:</strong> 블러를 유발한 블러 커널 <span class="math math-inline">h</span>가 사전에 알려져 있다고 가정하는 경우이다. 이 문제는 <span class="math math-inline">g</span>와 <span class="math math-inline">h</span>로부터 <span class="math math-inline">f</span>만을 추정하면 되므로 상대적으로 간단하다. 전통적인 방법으로는 Wiener 필터나 Lucy-Richardson 알고리즘 등이 있으며, 이들은 통계적 모델이나 반복적 추정을 통해 노이즈 증폭을 억제하면서 영상을 복원한다.2 하지만 실제 상황에서는 블러 커널을 정확히 알기 어려운 경우가 대부분이다.</li>
<li><strong>Blind Deconvolution:</strong> 블러 커널 <span class="math math-inline">h</span>와 원본 영상 <span class="math math-inline">f</span>가 모두 미지수인 훨씬 더 도전적인 문제이다. 대부분의 실제 디블러링 시나리오가 여기에 해당한다.2 이 문제는 해가 유일하게 결정되지 않기 때문에(예를 들어, 더 흐릿한 커널과 더 날카로운 영상의 조합도 동일한 블러 영상을 만들 수 있음), 해 공간을 줄이기 위한 강력한 제약 조건이 필수적이다. 일반적인 접근법은 블러 커널과 원본 영상을 번갈아 가며 추정하는 반복적인 최적화 방식을 사용하거나, 자연 영상(natural image)과 블러 커널의 통계적 특성에 대한 사전 정보를 활용하여 가장 그럴듯한(plausible) 해를 찾는 것이다.</li>
</ul>
<h3>4.3  정규화 기법의 역할</h3>
<p>Blind deconvolution과 같은 비정형 문제를 해결하기 위해 도입되는 수학적 장치가 정규화(regularization)이다. 정규화는 목적 함수(objective function)에 추가적인 제약 항(regularization term)을 더하여 해가 가져야 할 바람직한 특성(prior)을 부여하는 역할을 한다. 이를 통해 수많은 가능한 해 중에서 우리의 사전 지식에 부합하는, 의미 있는 해를 선택하도록 유도한다.2</p>
<p>대표적인 정규화 기법 중 하나는 <strong>총 변위(Total Variation, TV) 정규화</strong>이다.37 TV 정규화는 영상의 그래디언트(gradient)에 대한 <span class="math math-inline">L_1</span>-norm`을 최소화하는 것을 목표로 한다. 이는 영상의 대부분 영역이 부드럽게 변하지만(piecewise constant), 객체의 경계와 같은 일부 영역에서는 급격한 변화가 나타나는 자연 영상의 통계적 특성을 반영한 것이다. TV 정규화를 사용하면 복원 과정에서 노이즈로 인한 미세한 변화는 억제하여 평탄한 영역을 부드럽게 만들면서도, 객체의 중요한 경계선은 날카롭게 보존하는 효과를 얻을 수 있다.</p>
<h3>4.4  딥러닝 기반 디블러링</h3>
<p>전통적인 최적화 기반 디블러링 방법들은 강력한 이론적 기반을 가지고 있지만, 복잡한 실제 블러를 처리하는 데 한계가 있으며 계산 비용이 높다. 딥러닝 기반 디블러링은 대규모 데이터셋으로부터 블러와 선명한 영상 간의 복잡한 비선형 관계를 직접 학습하여 이러한 문제를 해결한다.</p>
<ul>
<li><strong>CNN 기반 접근:</strong> 초기 딥러닝 디블러링 모델들은 CNN을 활용했다. 특히, 실제 블러는 영상의 위치마다 그 형태와 강도가 다를 수 있는 비균일(non-uniform) 블러인 경우가 많다. 이를 해결하기 위해 멀티스케일(multi-scale) CNN 아키텍처가 제안되었다.6 이 구조는 입력 영상을 여러 해상도의 피라미드로 만들고, 낮은 해상도에서 거친 수준의 블러를 먼저 제거한 뒤 점차 높은 해상도로 올라가면서 세밀한 부분을 복원하는 ‘coarse-to-fine’ 방식을 모방한다. 이를 통해 다양한 크기와 형태의 블러에 효과적으로 대응할 수 있다.</li>
<li><strong>GAN 기반 접근 (e.g., DeblurGAN):</strong> CNN 기반 모델이 MSE 손실 함수를 사용할 경우, 초해상화와 마찬가지로 평균적인 흐릿한 결과를 생성하는 경향이 있다. DeblurGAN과 같은 GAN 기반 모델은 조건부 GAN(Conditional GAN)을 사용하여 이 문제를 해결한다.6 블러 영상을 조건으로 입력받아, 생성자는 사실적인 선명한 영상을 생성하도록 학습된다. 이 과정에서 MSE와 같은 내용 손실(content loss)과 함께, 판별자를 통해 계산되는 적대적 손실(adversarial loss)을 사용하여 결과물이 시각적으로 더 자연스럽고 현실적이 되도록 유도한다.</li>
</ul>
<p>결론적으로, blind deconvolution의 핵심 난제는 원본 영상과 블러 커널 사이의 모호성을 해결하는 것이다. 전통적인 방법은 TV 정규화와 같은 수학적으로 명시된 사전(prior)을 통해 이 문제를 해결하려 했고, 현대의 딥러닝 방법은 대규모 데이터셋으로부터 자연 영상의 통계적 특성을 암묵적으로 학습하여 사전으로 활용한다. 즉, 문제의 비정형적 본질은 사전 정보의 도입을 필수로 만들었으며, 기술의 발전은 이 사전을 어떻게 정의하고 활용하는가의 차이로 귀결된다.</p>
<h2>5.  초해상화 기반 최적 디블러링: 통합적 접근법</h2>
<h3>5.1  순차적 처리의 근본적 한계: 정보 손실의 비가역성</h3>
<p>저해상도 블러 영상을 복원하기 위해 디블러링 후 초해상화를 적용하는 순차적 파이프라인은 직관적이지만 근본적인 한계를 지닌다. 이 한계의 핵심에는 ’정보 손실의 비가역성’이 있다. 디블러링 단계는 블러 과정에서 손실된 고주파수 정보를 마법처럼 되살리는 것이 아니다. 블러 커널은 저주파 통과 필터로 작용하여 원본 영상의 미세한 텍스처와 날카로운 경계 정보를 물리적으로 제거한다. 디블러링 알고리즘은 단지 남아있는 정보를 바탕으로 가장 그럴듯한 원본을 ’추정’할 뿐이다.8</p>
<p>이 추정 과정에서 이미 소실된 고주파수 정보는 영구적으로 사라진다. 따라서 이 디블러링된 결과물을 후속 초해상화 단계에 입력하면, 초해상화 모델은 존재하지 않는 정보를 기반으로 디테일을 만들어내야 한다. 이는 진정한 의미의 ’복원’이 아니라, 학습된 데이터 분포에 기반한 ’환각(hallucination)’에 가깝다. 결과적으로, 순차적 파이프라인은 첫 단계에서 발생한 정보 손실의 한계를 절대로 넘어설 수 없으며, 이는 최종 결과물의 품질을 근본적으로 제약하는 원인이 된다.</p>
<h3>5.2  공동 최적화의 이론적 당위성</h3>
<p>공동 최적화(Joint Optimization)는 초해상화와 디블러링을 별개의 문제가 아닌, 상호 연관된 하나의 통합된 문제로 재정의한다. 이는 두 태스크가 서로에게 유용한 제약 조건(constraint)을 제공할 수 있다는 아이디어에 기반한다. 초해상화 과정은 잠재적인 고해상도 공간에서 디테일을 복원하려는 ’생성’의 역할을 수행하는 반면, 디블러링 과정은 이 생성된 고해상도 결과에 물리적으로 타당한 블러 모델과 다운샘플링을 적용했을 때, 우리가 실제로 관측한 저해상도 블러 영상과 일치해야 한다는 ’제약’의 역할을 수행한다.</p>
<p>이러한 관계는 하나의 통합된 목적 함수(objective function)로 공식화될 수 있다. 이 목적 함수는 일반적으로 두 가지 주요 항으로 구성된다:</p>
<ol>
<li><strong>데이터 충실도 항 (Data Fidelity Term):</strong> 복원된 고해상도 추정치(<span class="math math-inline">\hat{I}_{HR}</span>)에 다시 열화 과정(블러 커널 <span class="math math-inline">k</span>와의 컨볼루션 및 다운샘플링 <span class="math math-inline">D_s</span>)을 적용했을 때, 실제 관측된 저해상도 영상(<span class="math math-inline">I_{LR}</span>)과의 차이를 최소화하는 항이다. 이는 복원 결과가 관측된 증거와 일치하도록 보장한다.</li>
<li><strong>정규화 항 (Regularization Term):</strong> 복원될 고해상도 영상(<span class="math math-inline">\hat{I}_{HR}</span>)과 블러 커널(<span class="math math-inline">k</span>) 자체에 대한 사전 지식(prior)을 나타내는 항이다. 예를 들어, 영상은 경계가 보존되어야 한다는 사전(<span class="math math-inline">R(I_{HR})</span>)이나 블러 커널은 희소(sparse)해야 한다는 사전(<span class="math math-inline">P(k)</span>)을 포함할 수 있다.</li>
</ol>
<p>이를 수식으로 표현하면 다음과 같다 8:<br />
<span class="math math-display">
\hat{I}_{HR}, \hat{k} = \arg\min_{I_{HR}, k} \| D_s(I_{HR} * k) - I_{LR} \|_2^2 + \lambda R(I_{HR}) + \gamma P(k)
</span><br />
이 최적화 문제를 풀면, 데이터 충실도와 사전 지식을 모두 만족시키는 최적의 고해상도 선명 영상 <span class="math math-inline">\hat{I}_{HR}</span>과 블러 커널 <span class="math math-inline">\hat{k}</span>를 동시에 찾을 수 있다.</p>
<h3>5.3  딥러닝 프레임워크에서의 공동 복원</h3>
<p>딥러닝 프레임워크는 이러한 공동 최적화 문제를 엔드-투-엔드(End-to-end) 방식으로 해결하는 강력한 도구를 제공한다.9 별도의 최적화 알고리즘을 설계하는 대신, 깊은 신경망이 저해상도 블러 영상을 입력받아 고해상도 선명 영상을 직접 출력하도록 학습시킨다. 이 과정에서 네트워크는 대규모 학습 데이터를 통해 복잡한 열화 과정의 역함수를 내재적으로, 그리고 비선형적으로 학습하게 된다.</p>
<p>이 접근법은 명시적인 블러 커널 추정 단계를 생략하고, 네트워크 내부의 방대한 특징 공간(feature space)에서 두 문제를 동시에 해결한다. 예를 들어, 네트워크의 초기 레이어들은 블러와 노이즈를 제거하는 디블러링과 관련된 특징을 추출하고, 후기 레이어들은 이 정제된 특징을 바탕으로 해상도를 높이고 미세한 디테일을 복원하는 초해상화 관련 특징을 학습하도록 계층적으로 분화될 수 있다.</p>
<p>순차적 접근법이 각 단계에서 정보의 ’병목 현상’을 겪는 것과 달리, 공동 최적화 프레임워크는 전체 복원 과정에 걸쳐 정보가 자유롭게 흐르도록 한다. 초해상화에 필요한 고주파수 정보와 디블러링에 필요한 저주파 구조 정보가 네트워크 내에서 상호작용하며 서로를 보완한다. 예를 들어, 초해상화 모듈이 제안하는 잠재적인 텍스처 정보는 디블러링 모듈이 블러의 일관성을 검증하는 데 사용될 수 있고, 반대로 디블러링을 통해 얻어진 선명한 구조 정보는 초해상화 모듈이 더 정확한 디테일을 생성하는 데 도움을 준다. 이처럼 두 태스크 간의 시너지는 순차적 방법으로는 도달할 수 없는 더 높은 품질의 복원 결과를 가능하게 하는 핵심적인 이유이다.</p>
<h2>6.  최신 공동 복원 아키텍처 및 기법</h2>
<h3>6.1  실용적 열화 모델 기반 접근: BSRGAN &amp; Real-ESRGAN</h3>
<p>현대적인 공동 복원 기술의 성공은 단순히 네트워크 아키텍처의 발전뿐만 아니라, 훈련 데이터를 생성하는 ’실용적 열화 모델’의 정교화에 크게 힘입었다. 이 접근법의 핵심은 실제 세계에서 발생하는 복잡하고 다양한 영상 열화를 최대한 현실적으로 모방하여, 모델이 미지의 열화에 대해서도 강건한 일반화 성능을 갖도록 하는 것이다.</p>
<ul>
<li><strong>BSRGAN (Blind Super-Resolution GAN):</strong> BSRGAN은 실제 영상의 복잡한 열화를 모방하기 위해 새로운 패러다임을 제시했다.17 기존의 고정된 ‘블러 → 다운샘플링 → 노이즈’ 파이프라인에서 벗어나, 블러, 다운샘플링, 노이즈, JPEG 압축 등 다양한 열화 단계를 정의하고 이들의 적용 순서를 무작위로 섞는(random shuffle) 혁신적인 전략을 도입했다. 예를 들어, 노이즈가 추가된 후 블러가 적용되거나, 다운샘플링 후 다시 블러가 적용되는 등 현실에서 발생할 수 있는 다양한 시나리오를 시뮬레이션한다. 이렇게 생성된 방대하고 예측 불가능한 학습 데이터로 훈련된 BSRGAN 모델은 특정 열화 유형에 과적합되지 않고, 다양한 종류의 실제 열화에 대해 뛰어난 강건성(robustness)을 보인다.</li>
<li><strong>Real-ESRGAN:</strong> Real-ESRGAN은 BSRGAN의 아이디어를 한 단계 더 발전시켜 ‘고차(high-order)’ 열화 모델을 제안했다.17 이는 실제 이미지가 촬영, 편집, 인터넷 업로드, 재압축 등의 과정을 거치며 여러 번의 열화 과정을 중첩해서 겪는다는 현실을 반영한 것이다. Real-ESRGAN은 BSRGAN과 유사한 열화 단계를 하나의 ’1차 열화 프로세스’로 정의하고, 이를 여러 번 반복 적용하는 ’2차 열화 프로세스’를 구현했다. 또한, 이미지 샤프닝(sharpening) 알고리즘이나 JPEG 압축 시 흔히 발생하는 경계 주변의 인공적인 노이즈인 링잉(ringing) 및 오버슈팅(overshoot) 아티팩트를 모방하기 위해 Sinc 필터를 열화 과정에 포함시켰다. 이러한 고도로 정교화된 열화 모델 덕분에 Real-ESRGAN은 인터넷에서 수집된 실제 이미지(in-the-wild images)에 대해 매우 뛰어난 복원 성능을 보여주며 실용성을 크게 높였다.</li>
</ul>
<h3>6.2  Transformer 기반 공동 복원 모델</h3>
<p>Transformer 아키텍처는 전역적인 맥락 이해 능력을 바탕으로 공동 복원 문제에서도 최첨단 성능을 이끌고 있다.</p>
<ul>
<li><strong>Restormer:</strong> Restormer는 고해상도 이미지 복원을 위해 설계된 효율적인 Transformer 모델이다.29 핵심 구성 요소인 MDTA(Multi-Dconv Head Transposed Attention)는 공간 차원이 아닌 채널 차원에서 Self-Attention을 계산함으로써, 이미지 전체의 장거리 의존성을 모델링하면서도 계산 복잡도를 선형적으로 유지한다. 또한, GDFN(Gated-Dconv Feed-Forward Network)은 특징 흐름을 효과적으로 제어하여 복원 성능을 향상시킨다. 인코더-디코더 구조를 통해 멀티스케일 특징을 효과적으로 학습하며, 디블러링, 디노이징, 디레이닝 등 다양한 복원 작업에서 통합적으로 SOTA 성능을 달성하여 범용 복원 모델로서의 가능성을 입증했다.</li>
<li><strong>NAFNet (Nonlinear Activation Free Network):</strong> NAFNet은 ’단순함이 강력하다’는 철학을 바탕으로 설계된 모델이다.52 기존 딥러닝 모델의 핵심 요소였던 ReLU, Sigmoid와 같은 비선형 활성화 함수(Nonlinear Activation Function)를 과감히 제거하고, 이를 단순한 채널별 곱셈(SimpleGate)으로 대체했다. 또한, 채널 어텐션(Channel Attention)도 복잡한 구조 대신 간단한 전역 평균 풀링과 컨볼루션으로 구현했다. 이러한 극단적인 단순화에도 불구하고, NAFNet은 GoPro 및 REDS와 같은 주요 디블러링 벤치마크에서 이전의 복잡한 모델들을 능가하는 SOTA 성능을 달성했다. 이는 불필요한 복잡성이 오히려 성능에 해가 될 수 있음을 시사하며, 모델 효율성과 성능 간의 새로운 균형점을 제시했다.</li>
</ul>
<h3>6.3  최신 연구 동향: 2023-2024년 CVPR/ICCV/ECCV</h3>
<p>최근 주요 컴퓨터 비전 학회에서는 단일 이미지 복원을 넘어 비디오 복원으로 연구의 초점이 이동하고 있으며, 새로운 센서 기술을 융합하려는 시도가 활발하다.</p>
<ul>
<li><strong>비디오 초해상화 및 디블러링 (BVSR):</strong> FMA-Net (Youk, Oh, and Kim 2024)과 같은 연구들은 비디오의 시간적 정보를 활용하여 프레임 간의 움직임을 분석하고, 이를 통해 시공간적으로 변화하는(spatially-variant) 복잡한 모션 블러를 더 정확하게 제거하고 해상도를 복원하는 방법을 제안했다.9 이는 정적인 이미지에서는 얻을 수 없는 움직임 정보를 활용하여 복원의 정확도를 높이는 접근법이다.</li>
<li><strong>이벤트 카메라 융합:</strong> Ev-DeblurVSR과 같은 최신 연구는 기존 RGB 카메라의 한계를 극복하기 위해 이벤트 카메라(Event Camera)를 활용한다.56 이벤트 카메라는 밝기 변화가 감지될 때만 비동기적으로 신호를 생성하여, 마이크로초 단위의 매우 높은 시간 해상도를 가진다. 이를 통해 극심하고 빠른 움직임으로 인해 일반 카메라 영상에서는 모든 정보가 뭉개져 버리는 상황에서도 정확한 모션 궤적 정보를 얻을 수 있다. 이 모션 정보를 가이드로 사용하여 디블러링 및 초해상화를 수행함으로써, 기존 방법으로는 복원이 불가능했던 극심한 모션 블러 상황에서도 놀라운 SOTA 성능을 달성했다.</li>
</ul>
<h3>6.4  새로운 패러다임: Diffusion Model 기반 복원</h3>
<p>Diffusion Model은 최근 생성 모델 분야에서 가장 주목받는 기술로, 영상 복원 분야에서도 새로운 패러다임을 제시하고 있다. 이 모델은 원본 이미지에 점진적으로 노이즈를 추가하여 완전한 노이즈로 만드는 순방향 프로세스(forward process)와, 이 과정을 역으로 거슬러 올라가 노이즈로부터 원본 이미지를 점진적으로 복원하는 역방향 프로세스(reverse process)를 학습한다.58</p>
<p>영상 복원 문제에 이를 적용하기 위해 ‘조건부(conditional)’ 확산 모델이 사용된다. 저해상도 블러 영상을 조건(condition)으로 제공하여 역방향 노이즈 제거 과정을 가이드하는 것이다. SR3, Palette와 같은 초기 모델들은 저품질 영상을 U-Net 기반 노이즈 예측 네트워크의 각 시간 스텝(timestep)에 조건으로 주입하여, 생성 과정이 단순히 임의의 이미지가 아닌 ’입력 영상의 복원된 버전’을 향하도록 유도한다.60 이 방식은 GAN 기반 모델에 비해 학습이 더 안정적이고, 생성되는 결과물의 다양성과 품질이 뛰어나며, 특히 매우 사실적이고 자연스러운 텍스처를 복원하는 데 강점을 보인다.59 Diffusion Model은 복원 문제를 확률적 생성 과정으로 재해석함으로써, 기존의 결정론적 매핑 방식의 한계를 뛰어넘을 잠재력을 보여주고 있다.</p>
<p>이러한 기술들의 발전은 특정 원칙으로 수렴하고 있다. BSRGAN과 Real-ESRGAN의 성공은 현실적인 열화 모델이 SOTA 모델의 필수 전제 조건임을 증명했다. 또한, U-Net 구조가 GAN, Transformer, Diffusion 모델에 공통적으로 사용되는 현상은 멀티스케일 처리가 복원 문제 해결에 필수적임을 보여준다. 마지막으로, CNN의 한계를 극복하기 위해 등장한 Transformer의 장거리 의존성 모델링 능력은 이제 Restormer의 채널 어텐션이나 Diffusion 모델의 반복적 정제 과정 등 다양한 방식으로 구현되고 있다. 이 세 가지 기둥—현실적인 데이터, 멀티스케일 아키텍처, 전역적 문맥 모델링—은 복잡한 실제 영상 복원 문제를 해결하기 위한 표준 원칙으로 자리 잡고 있다.</p>
<h2>7.  성능 평가 지표 및 벤치마크 데이터셋</h2>
<h3>7.1  객관적 영상 품질 평가 지표</h3>
<p>영상 복원 알고리즘의 성능을 정량적으로 평가하기 위해 다양한 객관적 지표가 사용된다. 이 지표들은 복원된 영상이 원본(Ground Truth) 영상과 얼마나 유사한지를 수치로 나타낸다.</p>
<ul>
<li>
<p><strong>PSNR (Peak Signal-to-Noise Ratio):</strong> 최대 신호 대 잡음비는 영상 품질 평가에 가장 널리 사용되는 전통적인 지표이다. 원본 영상과 복원 영상 간의 픽셀 단위 오차의 제곱 평균인 MSE(Mean Squared Error)를 기반으로 계산된다. PSNR 값은 데시벨(dB) 단위로 표현되며, 높을수록 원본 영상과의 오차가 적음을 의미한다. 계산이 간단하고 물리적 의미가 명확하지만, 픽셀 값의 절대적인 차이만을 측정하기 때문에 인간의 시각 시스템이 인지하는 품질과 일치하지 않는 경우가 많다는 단점이 있다.1<br />
<span class="math math-display">
\text{PSNR} = 10 \log_{10} \left( \frac{\text{MAX}_I^2}{\text{MSE}} \right)
</span><br />
여기서 <span class="math math-inline">\text{MAX}_I</span>는 이미지의 최대 픽셀 값(예: 8비트 영상의 경우 255)이다.</p>
</li>
<li>
<p><strong>SSIM (Structural Similarity Index Measure):</strong> 구조적 유사성 지수는 PSNR의 한계를 보완하기 위해 제안된 지표로, 인간의 시각 시스템이 구조적 정보에 민감하다는 점에 착안했다. SSIM은 두 이미지 간의 유사성을 휘도(Luminance), 대비(Contrast), 구조(Structure)라는 세 가지 요소를 비교하여 측정한다. 값의 범위는 -1에서 1 사이이며, 1에 가까울수록 두 이미지가 구조적으로 매우 유사함을 의미한다. SSIM은 픽셀 단위의 오차보다는 이미지의 전반적인 구조와 질감의 유사성을 평가하므로, PSNR보다 인간의 시각적 품질 평가와 더 높은 상관관계를 보이는 것으로 알려져 있다.1<br />
<span class="math math-display">
\text{SSIM}(x,y) = [l(x,y)]^\alpha [c(x,y)]^\beta [s(x,y)]^\gamma
</span><br />
여기서 <span class="math math-inline">l, c, s</span>는 각각 휘도, 대비, 구조 비교 함수를 나타낸다.</p>
</li>
</ul>
<h3>7.2  인식-왜곡 상충 관계</h3>
<p>영상 복원 모델을 평가할 때 중요한 개념 중 하나는 ’인식-왜곡 상충 관계(Perception-Distortion Tradeoff)’이다.23</p>
<ul>
<li><strong>왜곡(Distortion):</strong> PSNR, SSIM과 같이 원본 영상과의 충실도(fidelity)를 측정하는 지표이다. 이 지표를 최적화하면 픽셀 수준에서는 원본과 매우 유사한 이미지를 얻을 수 있지만, 종종 세부 텍스처가 뭉개져 시각적으로 부자연스러운 결과가 나올 수 있다.</li>
<li><strong>인식(Perception):</strong> 인간이 보기에 얼마나 사실적이고 자연스러운지를 나타내는 품질이다. GAN 기반 모델처럼 인식 품질을 높이는 방향으로 학습하면, 매우 사실적인 텍스처를 생성할 수 있지만, 이 텍스처가 원본과 픽셀 단위로 정확히 일치하지는 않기 때문에 PSNR과 같은 왜곡 지표는 오히려 낮아질 수 있다.</li>
</ul>
<p>이처럼 두 종류의 지표는 서로 상충하는 경향이 있으므로, 모델의 성능을 종합적으로 평가하기 위해서는 두 가지 측면을 모두 고려해야 한다. NTIRE와 같은 주요 챌린지에서는 이 두 가지 지표를 모두 반영하여 모델을 평가한다.</p>
<h3>7.3  주요 벤치마크 데이터셋</h3>
<p>딥러닝 기반 영상 복원 모델의 성능을 공정하게 비교하고 평가하기 위해 표준화된 벤치마크 데이터셋이 사용된다.</p>
<ul>
<li><strong>DIV2K:</strong> NTIRE 챌린지를 통해 공개된 초해상화 연구의 표준 벤치마크 데이터셋이다.16 고품질의 2K 해상도(약 2048x1080) RGB 이미지 1,000장으로 구성되어 있으며, 800장은 훈련용, 100장은 검증용, 100장은 테스트용으로 사용된다. 각 고해상도 이미지에 대해 바이큐빅(bicubic) 다운샘플링과 미지의 열화 방식(unknown degradation)으로 생성된 2배, 3배, 4배 스케일의 저해상도 이미지가 쌍으로 제공되어, 다양한 조건에서 모델을 학습하고 평가할 수 있다.</li>
<li><strong>GoPro:</strong> 동적 장면 디블러링(dynamic scene deblurring)을 위한 대규모 데이터셋으로, 실제적인 모션 블러를 구현한 것으로 유명하다.10 GoPro 카메라를 사용하여 240fps의 고속 비디오를 촬영한 후, 연속된 여러 프레임을 평균 내어 긴 노출 시간으로 촬영한 것과 같은 자연스러운 모션 블러 이미지를 합성했다. 총 3,214개의 블러-선명 영상 쌍으로 구성되며, 이 중 2,103개는 훈련용, 1,111개는 테스트용으로 사용된다. 해상도는 1280x720이다.</li>
<li><strong>REDS (REalistic and Dynamic Scenes):</strong> 비디오 디블러링 및 비디오 초해상화(VSR)를 위해 NTIRE 2019 챌린지에서 제안된 대규모 데이터셋이다.52 GoPro 데이터셋보다 더 현실적이고 다양한 동적 장면을 포함하도록 설계되었다. 총 300개의 비디오 시퀀스로 구성되며, 이 중 240개는 훈련용, 30개는 검증용, 30개는 테스트용이다. 각 시퀀스는 720x1280 해상도의 100개 연속 프레임으로 이루어져 있어, 시간적 일관성을 평가하는 데 매우 중요하다.</li>
</ul>
<table><thead><tr><th>모델 (Model)</th><th>발표 연도</th><th>PSNR (GoPro)</th><th>SSIM (GoPro)</th><th>PSNR (REDS)</th><th>SSIM (REDS)</th><th>핵심 아키텍처 특징</th></tr></thead><tbody>
<tr><td><strong>VRT</strong> 75</td><td>2022</td><td>34.81</td><td>0.9724</td><td>-</td><td>-</td><td>비디오 복원 Transformer</td></tr>
<tr><td><strong>GShift-Net</strong> 75</td><td>2022</td><td>35.88</td><td>0.979</td><td>-</td><td>-</td><td>Grouped Spatial-temporal Shift</td></tr>
<tr><td><strong>NAFNet</strong> 53</td><td>2022</td><td>33.71</td><td>0.9668</td><td>29.09</td><td>0.8671</td><td>비선형 활성화 함수 제거</td></tr>
<tr><td><strong>Restormer</strong> 30</td><td>2022</td><td>32.95</td><td>0.962</td><td>31.84</td><td>0.915</td><td>채널 차원 Transformer</td></tr>
<tr><td><strong>BSSTNet</strong> 75</td><td>2024</td><td><strong>35.98</strong></td><td><strong>0.9792</strong></td><td>-</td><td>-</td><td>블러 인지 시공간 희소 Transformer</td></tr>
</tbody></table>
<p><em>참고: 위 표는 각 논문에서 보고된 최고 성능을 기준으로 작성되었으며, 모델의 크기나 세부 설정에 따라 다를 수 있다.</em></p>
<table><thead><tr><th>평가 지표</th><th>수학적 정의/핵심 원리</th><th>장점</th><th>단점</th></tr></thead><tbody>
<tr><td><strong>PSNR</strong></td><td>MSE 기반의 픽셀 단위 오차 측정. <span class="math math-inline">10 \log_{10} (\text{MAX}^2 / \text{MSE})</span></td><td>계산이 매우 간단하고 빠름. 널리 사용되어 비교 기준으로 적합함. 극심한 왜곡 감지에 유용함.63</td><td>인간의 시각적 인식 품질과 상관관계가 낮음. 구조적 왜곡(블러, 아티팩트)에 둔감함.23</td></tr>
<tr><td><strong>SSIM</strong></td><td>휘도, 대비, 구조의 세 가지 요소를 비교하여 구조적 유사성을 측정함.</td><td>인간의 시각적 인식과 더 높은 상관관계를 보임. 블러, 압축 아티팩트 등 구조적 왜곡 감지에 민감함.63</td><td>극심한 왜곡이나 노이즈가 많은 영상에서는 성능이 저하될 수 있음. 비디오의 시간적 정보는 고려하지 않음.63</td></tr>
</tbody></table>
<h2>8.  결론 및 향후 연구 방향</h2>
<h3>8.1  연구 요약 및 기술적 기여</h3>
<p>본 안내서는 초해상화 기술을 기반으로 최적의 디블러링 성능을 달성하기 위한 통합적 접근법을 다각도로 분석했다. 먼저, 영상 열화를 블러, 다운샘플링, 노이즈가 복합적으로 작용하는 통합된 수학적 모델로 정립하고, 이를 해결하기 위한 역문제로서 복원 기술을 정의했다. 초해상화와 디블러링 기술의 발전사를 SRCNN부터 최신 Transformer 기반 모델에 이르기까지 추적하며, 각 기술이 이전 세대의 한계를 어떻게 극복했는지 분석했다. 특히, 순차적 처리 방식의 근본적인 한계인 ’정보 손실의 비가역성’을 지적하고, 이를 극복하기 위한 공동 최적화 프레임워크의 이론적, 실용적 당위성을 제시했다. BSRGAN, Real-ESRGAN, Restormer, NAFNet, 그리고 Diffusion Model과 같은 최신 아키텍처들의 핵심 원리와 기여를 심층적으로 검토함으로써, 현재 기술 수준을 명확히 했다. 마지막으로, PSNR, SSIM과 같은 정량적 평가 지표와 GoPro, REDS 등의 표준 벤치마크 데이터셋을 통해 기술의 성능을 객관적으로 가늠할 수 있는 기준을 제시했다. 본 안내서는 초해상화와 디블러링이 더 이상 별개의 문제가 아니며, 이들의 시너지를 극대화하는 공동 복원 패러다임이 실질적인 성능 향상의 핵심임을 이론적, 기술적, 실험적 측면에서 종합적으로 밝혔다.</p>
<h3>8.2  현재 기술의 한계점</h3>
<p>최첨단 공동 복원 기술들은 괄목할 만한 성과를 이루었지만, 여전히 실용적인 적용을 가로막는 몇 가지 한계점을 가지고 있다.</p>
<ul>
<li><strong>계산 복잡도:</strong> Restormer와 같은 Transformer 기반 모델이나 Diffusion Model은 뛰어난 성능을 보이는 만큼 막대한 계산 자원을 요구한다. 수많은 파라미터와 반복적인 연산 과정은 고사양 GPU를 필요로 하며, 이는 실시간 처리가 요구되거나 모바일, 엣지 디바이스와 같은 자원이 제한된 환경에 배포하는 데 큰 장벽이 된다.53</li>
<li><strong>데이터 의존성:</strong> 대부분의 SOTA 모델은 대규모의 고품질 ‘블러-선명’ 영상 쌍(paired data)으로 구성된 데이터셋에 기반하여 지도 학습(supervised learning) 방식으로 훈련된다. 그러나 실제 환경에서는 이러한 완벽하게 정렬된 페어 데이터를 대량으로 수집하는 것이 거의 불가능하다. 이는 모델이 학습 데이터의 분포에 과적합되어 실제 환경과의 도메인 격차를 유발하는 원인이 된다.</li>
<li><strong>일반화 문제:</strong> BSRGAN과 Real-ESRGAN의 실용적 열화 모델은 일반화 성능을 크게 향상시켰지만, 여전히 모든 실제 열화 시나리오를 포괄하지는 못한다. 훈련 과정에서 고려되지 않은 완전히 새로운 유형의 블러나 극심한 수준의 열화에 직면했을 때, 모델의 성능은 예측 불가능하게 저하될 수 있다. 예를 들어, NAFNet은 GoPro 데이터셋에서는 높은 성능을 보이지만, 분포가 다른 REDS 데이터셋에서는 ‘모드 붕괴(mode collapse)’ 현상을 보이며 성능이 급격히 떨어지는 경우가 보고되었다.77</li>
</ul>
<h3>8.3  향후 연구 방향</h3>
<p>현재 기술의 한계는 미래 연구가 나아가야 할 방향을 제시한다. 데이터와 효율성의 제약을 극복하고, 보다 범용적이고 실용적인 복원 기술을 개발하기 위한 연구가 활발히 진행될 것으로 예상된다.</p>
<ul>
<li>
<p><strong>경량화 및 효율화:</strong> NAFNet이 보여주었듯이, 불필요한 구조적 복잡성을 제거하고 네트워크의 핵심 기능에 집중함으로써 계산 효율성을 극대화하면서도 SOTA 성능을 달성하는 연구가 지속될 것이다.53 모델 양자화(quantization), 프루닝(pruning)과 같은 모델 압축 기술을 공동 복원 모델에 적용하여 온디바이스 AI 구현을 가속화하는 연구도 중요해질 것이다.76</p>
</li>
<li>
<p><strong>비지도 및 자기지도 학습:</strong> 페어 데이터의 한계를 극복하기 위해, 실제 열화된 영상만을 사용하여 복원 모델을 학습하는 비지도 학습(unsupervised learning) 및 자기지도 학습(self-supervised learning) 방법론에 대한 연구가 더욱 중요해질 것이다. 이는 실제 세계의 무한한 열화 분포를 모델이 스스로 학습하도록 유도하여 일반화 성능을 극대화하는 것을 목표로 한다.</p>
</li>
<li>
<p><strong>통합 복원 모델 (All-in-One Restoration):</strong> 현재의 공동 복원 모델을 더욱 확장하여, 디블러링과 초해상화뿐만 아니라 디노이징(denoising), 디헤이징(dehazing), 디레이닝(deraining), 압축 아티팩트 제거 등 모든 종류의 영상 열화를 단일 모델로 처리하는 ‘범용 영상 복원(All-in-One Image Restoration)’ 모델로의 발전이 가속화될 것이다.78 이는 다양한 열화 유형을 인식하고, 그에 맞는 최적의 복원 경로를 동적으로 선택하는 지능형 시스템의 형태를 띨 것이다.</p>
</li>
<li>
<p><strong>실세계 응용 분야 심화:</strong> 범용 기술의 발전과 더불어, 특정 도메인의 고유한 문제를 해결하기 위한 특화된 공동 복원 기술 연구가 심화될 것이다. <strong>법의학(Forensics)</strong> 분야에서는 저화질 CCTV 영상에서 용의자의 얼굴을 식별하거나 차량 번호판을 복원하는 기술 80,</p>
</li>
</ul>
<p><strong>의료 영상</strong> 분야에서는 저선량 CT나 고속 MRI 촬영으로 인한 노이즈와 블러를 제거하여 진단의 정확도를 높이는 기술 84,</p>
<p><strong>위성 및 항공 영상</strong> 분석에서는 대기 난류로 인한 왜곡을 보정하고 객체 탐지 성능을 향상시키는 기술 88 등이 대표적인 예이다. 이러한 응용 분야는 기술의 사회적, 경제적 가치를 극대화하는 중요한 원동력이 될 것이다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>A Complete Guide to Image Super-Resolution in Deep Learning and …, https://www.digitalocean.com/community/tutorials/image-super-resolution</li>
<li>Deblurring - Wikipedia, https://en.wikipedia.org/wiki/Deblurring</li>
<li>Joint image deblurring and super resolution. While the state-of-the-art… - ResearchGate, https://www.researchgate.net/figure/Joint-image-deblurring-and-super-resolution-While-the-state-of-the-art-super-resolution_fig1_338568614</li>
<li>Python OpenCV - Super resolution with deep learning - GeeksforGeeks, https://www.geeksforgeeks.org/computer-vision/python-opencv-super-resolution-with-deep-learning/</li>
<li>초해상화(Super-resolution)란? 저화질 영상을 고화질로 바꿔주는 기술 - bskyvision.com, <a href="https://bskyvision.com/entry/%EC%B4%88%ED%95%B4%EC%83%81%ED%99%94Super-resolution%EB%9E%80-%EC%A0%80%ED%99%94%EC%A7%88-%EC%98%81%EC%83%81%EC%9D%84-%EA%B3%A0%ED%99%94%EC%A7%88%EB%A1%9C-%EB%B0%94%EA%BF%94%EC%A3%BC%EB%8A%94-%EA%B8%B0%EC%88%A0">https://bskyvision.com/entry/%EC%B4%88%ED%95%B4%EC%83%81%ED%99%94Super-resolution%EB%9E%80-%EC%A0%80%ED%99%94%EC%A7%88-%EC%98%81%EC%83%81%EC%9D%84-%EA%B3%A0%ED%99%94%EC%A7%88%EB%A1%9C-%EB%B0%94%EA%BF%94%EC%A3%BC%EB%8A%94-%EA%B8%B0%EC%88%A0</a></li>
<li>What is the image deblurring deep learning method? | by Saiwa - Medium, https://medium.com/@saiwadotai/what-is-the-image-deblurring-deep-learning-method-b40621406ae3</li>
<li>영상 디블러링 연구 동향 (Research Trends of Image Deblurring) - Korea Science, https://koreascience.kr/article/JAKO201216238707860.pdf</li>
<li>(PDF) Joint Motion Deblurring and Superresolution from Single …, https://www.researchgate.net/publication/283955418_Joint_Motion_Deblurring_and_Superresolution_from_Single_Blurry_Image</li>
<li>CVPR Poster FMA-Net: Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring, https://cvpr.thecvf.com/virtual/2024/poster/31426</li>
<li>Joint Image Deblurring and Super-Resolution - CSE IITM, https://www.cse.iitm.ac.in/~vplab/courses/CV_DIP/PDF_2021/TPA_8.pdf</li>
<li>Digital Image Processing Prof. P. K. Biswas Department of …, http://elearn.psgcas.ac.in/nptel/courses/video/117105135/lec44.pdf</li>
<li>Chapter 5. Image Restoration, http://math.ewha.ac.kr/~jylee/SciComp/dip-diml.yonsei/chap5-1.pdf</li>
<li>Image Restoration: From Sparse and Low-rank Priors to Deep Priors - PolyU, https://www4.comp.polyu.edu.hk/~cslzhang/paper/SPM_IR.pdf</li>
<li>Image Restoration, <a href="https://faraday.emu.edu.tr/ee583/Lectures/EE%20583-Lecture07.pdf">https://faraday.emu.edu.tr/ee583/Lectures/EE%20583-Lecture07.pdf</a></li>
<li>디지털 영상 처리 - 열화 함수 추정 - Everyday Image Processing - 티스토리, https://everyday-image-processing.tistory.com/178</li>
<li>DIV2K Dataset, https://data.vision.ee.ethz.ch/cvl/DIV2K/</li>
<li>Designing a Practical Degradation Model for Deep Blind Image Super-Resolution | Request PDF - ResearchGate, https://www.researchgate.net/publication/359001935_Designing_a_Practical_Degradation_Model_for_Deep_Blind_Image_Super-Resolution</li>
<li>cszn/BSRGAN: Designing a Practical Degradation Model … - GitHub, https://github.com/cszn/BSRGAN</li>
<li>Real-ESRGAN: Training Real-World Blind … - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Wang_Real-ESRGAN_Training_Real-World_Blind_Super-Resolution_With_Pure_Synthetic_Data_ICCVW_2021_paper.pdf</li>
<li>Super Resolution: Basics with Python Code - YouTube, https://www.youtube.com/watch?v=jtYh_GwlrlM</li>
<li>Super Resolution in OpenCV - LearnOpenCV, https://learnopencv.com/super-resolution-in-opencv/</li>
<li>딥러닝을 이용하여 생성한 초해상화 드론 영상의 정량적 평가 - Korea Science, https://koreascience.kr/article/JAKO202304757615010.pdf</li>
<li>Single Image Super Resolution using Deep Learning Overview - Hoya012’s Research Blog, https://hoya012.github.io/blog/SIngle-Image-Super-Resolution-Overview/</li>
<li>Super-Resolution for Satellite Imagery Explained - Geoawesome, https://geoawesome.com/super-resolution-for-satellite-imagery-explained/</li>
<li>SwinIR: Image Restoration Using Swin Transformer | Request PDF - ResearchGate, https://www.researchgate.net/publication/356520699_SwinIR_Image_Restoration_Using_Swin_Transformer</li>
<li>SwinIR: Image Restoration Using Swin Transformer (official repository) - GitHub, https://github.com/JingyunLiang/SwinIR</li>
<li>[PDF] SwinIR: Image Restoration Using Swin Transformer - Semantic Scholar, https://www.semanticscholar.org/paper/SwinIR%3A-Image-Restoration-Using-Swin-Transformer-Liang-Cao/7a9a708ca61c14886aa0dcd6d13dac7879713f5f</li>
<li>SwinIR: Image Restoration Using Swin … - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.pdf</li>
<li>Restormer: Efficient Transformer for High-Resolution Image Restoration | Request PDF, https://www.researchgate.net/publication/363908111_Restormer_Efficient_Transformer_for_High-Resolution_Image_Restoration</li>
<li>[CVPR 2022–Oral] Restormer: Efficient Transformer for High-Resolution Image Restoration. SOTA for motion deblurring, image deraining, denoising (Gaussian/real data), and defocus deblurring. - GitHub, https://github.com/swz30/Restormer</li>
<li>Restormer: Efficient Transformer for High … - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Zamir_Restormer_Efficient_Transformer_for_High-Resolution_Image_Restoration_CVPR_2022_paper.pdf</li>
<li>Andrew0613/X-Restormer: ECCV2024：A Comparative Study of Image Restoration Networks for General Backbone Network Design - GitHub, https://github.com/Andrew0613/X-Restormer</li>
<li>Deblurring of MRI Image Using Blind and Non-Blind Deconvolution Methods, https://biomedpharmajournal.org/vol10no3/deblurring-of-mri-image-using-blind-and-non-blind-deconvolution-methods/</li>
<li>Non-Blind Image Deblurring using Neural Networks, http://stanford.edu/class/ee367/Winter2018/gilbert_messingher_patel_ee367_win18_report.pdf</li>
<li>GAN Based Image Deblurring Using Dark Channel Prior - ResearchGate, https://www.researchgate.net/publication/331413365_GAN_Based_Image_Deblurring_Using_Dark_Channel_Prior</li>
<li>Deblurring Images Using the Blind Deconvolution Algorithm - MATLAB &amp; Simulink Example, https://www.mathworks.com/help/images/deblurring-images-using-the-blind-deconvolution-algorithm.html</li>
<li>Efficient and Interpretable Deep Blind Image Deblurring Via Algorithm Unrolling - Weizmann Institute of Science, <a href="https://www.weizmann.ac.il/math/yonina/sites/math.yonina/files/publications/Efficient%20and%20Interpretable%20Deep%20Blind%20Image.pdf">https://www.weizmann.ac.il/math/yonina/sites/math.yonina/files/publications/Efficient%20and%20Interpretable%20Deep%20Blind%20Image.pdf</a></li>
<li>Understanding and evaluating blind deconvolution algorithms - People | MIT CSAIL, https://people.csail.mit.edu/billf/publications/Understanding_and_Evaluating_Blind.pdf</li>
<li>Learning Deep Non-Blind Image Deconvolution Without Ground Truths - European Computer Vision Association, https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660631.pdf</li>
<li>Regularization techniques are widely used for image restoration where an unknown image must be recovered from noisy and blurre - Systems and Computer Engineering, https://www.sce.carleton.ca/faculty/adler/publications/2004/youmaran-adler-2004-ccece-combining-regularization.pdf</li>
<li>Image Restoration Using a Fixed-Point Method for a TVL2 Regularization Problem - MDPI, https://www.mdpi.com/1999-4893/13/1/1</li>
<li>Total Variation in Image Processing - Number Analytics, https://www.numberanalytics.com/blog/total-variation-in-image-processing</li>
<li>ALTERNATING DIRECTION ALGORITHMS FOR TOTAL VARIATION DECONVOLUTION IN IMAGE RECONSTRUCTION 1. Introduction. In this paper, we co, https://legacy.sites.fas.harvard.edu/~cs278/papers/adm.pdf</li>
<li>Image Text Deblurring Method Based on Generative Adversarial Network - MDPI, https://www.mdpi.com/2079-9292/9/2/220</li>
<li>KupynOrest/DeblurGAN: Image Deblurring using Generative Adversarial Networks - GitHub, https://github.com/KupynOrest/DeblurGAN</li>
<li>A Closer Look at Blind Super-Resolution: Degradation Models, Baselines, and Performance Upper Bounds - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Zhang_A_Closer_Look_at_Blind_Super-Resolution_Degradation_Models_Baselines_and_CVPRW_2022_paper.pdf</li>
<li>Designing a Practical Degradation Model for Deep Blind Image Super-Resolution - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Designing_a_Practical_Degradation_Model_for_Deep_Blind_Image_Super-Resolution_ICCV_2021_paper.pdf</li>
<li>Designing a Practical Degradation Model for Deep Blind Image Super-Resolution, https://www.researchgate.net/publication/350398181_Designing_a_Practical_Degradation_Model_for_Deep_Blind_Image_Super-Resolution</li>
<li>Unsupervised Diffusion-Based Degradation Modeling for Real-World Super-Resolution, https://ojs.aaai.org/index.php/AAAI/article/view/32235/34390</li>
<li>Real-World Video Super-Resolution with a Degradation-Adaptive Model - MDPI, https://www.mdpi.com/1424-8220/24/7/2211</li>
<li>Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data, https://arxiv.org/abs/2107.10833</li>
<li>megvii-model/HINet - GitHub, https://github.com/megvii-model/HINet</li>
<li>megvii-research/NAFNet: The state-of-the-art image … - GitHub, https://github.com/megvii-research/NAFNet</li>
<li>Simple Baselines for Image Restoration - European Computer Vision Association, https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670017.pdf</li>
<li>[2204.04676] Simple Baselines for Image Restoration - arXiv, https://arxiv.org/abs/2204.04676</li>
<li>Event-Enhanced Blurry Video Super-Resolution, https://ojs.aaai.org/index.php/AAAI/article/download/32438/34593</li>
<li>Event-Enhanced Blurry Video Super-Resolution - arXiv, https://arxiv.org/html/2504.13042v2</li>
<li>Diffusion Model-Based Image Editing: A Survey - arXiv, https://arxiv.org/html/2402.17525v4</li>
<li>Taming Diffusion Models for Image Restoration: A Review - arXiv, https://arxiv.org/html/2409.10353v1</li>
<li>A Comprehensive Review of Image Restoration Research Based on …, https://www.mdpi.com/2227-7390/13/13/2079</li>
<li>Denoising Diffusion-based Generative Modeling: Foundations and Applications, https://cvpr2022-tutorial-diffusion-models.github.io/</li>
<li>Deblurring via Video Diffusion Models, http://poster-openaccess.com/files/icic2024/540.pdf</li>
<li>VMAF vs. PSNR vs. SSIM: Understanding Video Quality Metrics, https://www.fastpix.io/blog/understanding-vmaf-psnr-and-ssim-full-reference-video-quality-metrics</li>
<li>Image Quality Assessment through FSIM, SSIM, MSE and PSNR—A Comparative Study, https://www.scirp.org/journal/paperinformation?paperid=90911</li>
<li>(PDF) Image quality metrics: PSNR vs. SSIM - ResearchGate, https://www.researchgate.net/publication/220931731_Image_quality_metrics_PSNR_vs_SSIM</li>
<li>Making Sense of PSNR, SSIM, VMAF - Visionular, https://visionular.ai/vmaf-ssim-psnr-quality-metrics/</li>
<li>Structural similarity index measure - Wikipedia, https://en.wikipedia.org/wiki/Structural_similarity_index_measure</li>
<li>Div2K High Resolution Images - Kaggle, https://www.kaggle.com/datasets/soumikrakshit/div2k-high-resolution-images</li>
<li>ChunmingHe/awesome-diffusion-models-in-low-level-vision - GitHub, https://github.com/ChunmingHe/awesome-diffusion-models-in-low-level-vision</li>
<li>A Curated List of Image Deblurring Datasets - Kaggle, https://www.kaggle.com/datasets/jishnuparayilshibu/a-curated-list-of-image-deblurring-datasets</li>
<li>Publications Datasets CV | Seungjun Nah - GitHub Pages, https://seungjunnah.github.io/Datasets/gopro.html</li>
<li>Publications Datasets CV | Seungjun Nah - GitHub Pages, https://seungjunnah.github.io/Datasets/reds.html</li>
<li>Awesome Datasets for Super-Resolution: Introduction and Pre-processing | by OpenMMLab, https://openmmlab.medium.com/awesome-datasets-for-super-resolution-introduction-and-pre-processing-55f8501f8b18</li>
<li>NTIRE 2019 Challenge on Video Deblurring and Super-Resolution: Dataset and Study - CVF Open Access, https://openaccess.thecvf.com/content_CVPRW_2019/papers/NTIRE/Nah_NTIRE_2019_Challenge_on_Video_Deblurring_and_Super-Resolution_Dataset_and_CVPRW_2019_paper.pdf</li>
<li>Papers with code · GitHub, https://paperswithcode.com/sota/deblurring-on-gopro?p=dark-and-bright-channel-prior-embedded</li>
<li>초해상화 AI 모델의 가중치를 최적화하는 경량화 개발, https://www.kotechmarket.com/tech-transfer/2514</li>
<li>SAM-Deblur: Let Segment Anything Boost Image Deblurring - arXiv, https://arxiv.org/html/2309.02270v2</li>
<li>[2504.04869] DSwinIR: Rethinking Window-based Attention for Image Restoration - arXiv, https://arxiv.org/abs/2504.04869</li>
<li>A Survey on All-in-One Image Restoration: Taxonomy, Evaluation and Future Trends - arXiv, https://arxiv.org/html/2410.15067v1</li>
<li>Forensic Photogrammetry: A Case Study – PhotoModeler, https://www.photomodeler.com/forensic-photogrammetry-case-study/</li>
<li>Computer Forensics and Image Deblurring: An Inclusive Investigation - ResearchGate, https://www.researchgate.net/publication/259127577_Computer_Forensics_and_Image_Deblurring_An_Inclusive_Investigation</li>
<li>Super-resolution of facial images in forensics scenarios - Aalborg Universitets forskningsportal, https://vbn.aau.dk/files/215609407/IPTA_Forensics.pdf</li>
<li>(PDF) AI-POWERED IMAGE ENHANCEMENT IN FORENSIC APPLICATIONS: CHALLENGES AND OPPORTUNITIES - ResearchGate, https://www.researchgate.net/publication/383273341_AI-POWERED_IMAGE_ENHANCEMENT_IN_FORENSIC_APPLICATIONS_CHALLENGES_AND_OPPORTUNITIES</li>
<li>Improving Medical Image Quality Using a Super-Resolution Technique with Attention Mechanism - MDPI, https://www.mdpi.com/2076-3417/15/2/867</li>
<li>Super-resolution and deblurring enhancement for narrow band imaging bronchoscopy, https://profiles.foxchase.org/en/publications/super-resolution-and-deblurring-enhancement-for-narrow-band-imagi</li>
<li>Super-resolution techniques for biomedical applications and challenges - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC11026337/</li>
<li>Evaluating Super-Resolution Models in Biomedical Imaging … - MDPI, https://www.mdpi.com/2313-433X/11/4/104</li>
<li>Super-Resolution in Satellite Imaging: Techniques, Applications, and Insights, https://www.digitalsense.ai/blog/introduction-to-super-resolution-for-satellite-images</li>
<li>An Efficient Image Deblurring Method with a Deep Convolutional Neural Network for Satellite Imagery | Request PDF - ResearchGate, https://www.researchgate.net/publication/354833823_An_Efficient_Image_Deblurring_Method_with_a_Deep_Convolutional_Neural_Network_for_Satellite_Imagery</li>
<li>ESatSR: Enhancing Super-Resolution for Satellite Remote Sensing Images with State Space Model and Spatial Context - MDPI, https://www.mdpi.com/2072-4292/16/11/1956</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>