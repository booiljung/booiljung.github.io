<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:인공지능 시스템의 결정이론</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>인공지능 시스템의 결정이론</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">인공지능 결정이론 (Decision Theory)</a> / <span>인공지능 시스템의 결정이론</span></nav>
                </div>
            </header>
            <article>
                <h1>인공지능 시스템의 결정이론</h1>
<p>2025-11-30, G30DR</p>
<h2>1.  서론: 인공지능과 합리성의 규범적 토대</h2>
<p>인공지능(Artificial Intelligence, AI) 연구의 궁극적인 지향점은 인간의 지적 능력을 모방하거나 초월하는 시스템을 구축하는 것에 있다. 이 거대한 목표의 중심에는 ’의사결정(Decision Making)’이라는 핵심 과제가 자리 잡고 있다. 컴퓨터 과학과 인지 과학의 교차점에서 AI를 정의할 때, 가장 널리 받아들여지는 정의 중 하나는 “환경을 지각하고, 목표를 달성할 확률을 극대화하는 행동을 수행하는 합리적 에이전트(Rational Agent)의 설계“이다.1 이러한 맥락에서 결정이론(Decision Theory)은 AI 시스템이 복잡하고 불확실한 세계에서 어떻게 판단하고 행동해야 하는지를 규정하는 수학적이고 논리적인 중추 신경계 역할을 수행한다.</p>
<p>초기의 인공지능이 미리 정의된 규칙에 따라 작동하는 결정론적 알고리즘에 의존했다면, 현대의 AI는 불확실성(Uncertainty)을 내재한 환경에서 확률적 추론을 통해 최적의 해를 찾아내는 방향으로 진화했다. 센서 데이터의 노이즈, 예측 불가능한 사람의 행동, 그리고 불완전한 정보 등 현실 세계의 복잡성은 AI가 단순한 논리 연산을 넘어, 확률론(Probability Theory)과 효용 이론(Utility Theory)의 결합인 결정이론을 필수적으로 채택하게 만들었다.2 이 이론적 틀 안에서 AI는 가능한 행동들의 결과를 예측하고, 각 결과가 가져올 가치(Utility)를 평가하며, 최종적으로 기대 효용(Expected Utility)을 극대화하는 선택을 내린다.</p>
<p>최근 2024년과 2025년의 기술 동향을 살펴보면, 결정이론은 강화학습(Reinforcement Learning, RL) 및 거대언어모델(Large Language Models, LLM)과 결합하며 새로운 국면을 맞이하고 있다.3 과거의 결정이론이 주로 소규모의 닫힌 세계(Closed World) 문제를 다루는 데 그쳤다면, 최신 에이전트형 AI(Agentic AI)는 방대한 지식 베이스와 도구 사용 능력을 바탕으로 열린 세계(Open World)에서 자율적으로 계획을 수립하고 목표를 수정해 나간다.3 특히 텍스트 기반의 추론 능력을 갖춘 LLM이 ’생각의 사슬(Chain of Thought)’이나 ’ReAct(Reasoning + Acting)’와 같은 프레임워크를 통해 복잡한 의사결정 과정을 수행하게 됨에 따라, 자연어 처리와 결정이론의 경계가 허물어지고 있다.6</p>
<p>본 보고서는 인공지능 분야에서 결정이론이 어떻게 적용되고 발전해 왔는지를 심층적으로 조망한다. 고전적인 효용 이론의 기초부터 시작하여, 순차적 의사결정을 다루는 마르코프 결정 과정(MDP), 불완전 정보를 다루는 부분 관찰 마르코프 결정 과정(POMDP), 다중 에이전트 간의 전략적 상호작용을 다루는 게임 이론(Game Theory), 그리고 데이터로부터 최적의 정책을 학습하는 강화학습(RL)과 최신 생성형 에이전트 기술까지 포괄적으로 분석한다. 이를 통해 현대 AI 시스템이 어떻게 ’합리적’인 판단을 내리는지, 그 이론적 배경과 실제적 구현 메커니즘을 상세히 규명한다.</p>
<h2>2.  결정이론의 이론적 배경과 구성 요소</h2>
<p>결정이론은 근본적으로 ’선택의 과학’이다. 인공지능 에이전트가 마주하는 선택의 문제는 결과가 확정적인 결정론적 상황보다는, 결과가 확률적으로 결정되는 불확실한 상황이 대부분이다. 따라서 AI 분야의 결정이론은 주로 규범적(Normative) 접근을 취한다. 즉, 인간이 실제로 어떻게 결정하는지를 묘사(Descriptive)하기보다는, 합리적인 에이전트라면 ’마땅히 어떻게 결정해야 하는가’에 대한 이상적인 모델을 제시한다.2</p>
<h3>2.1  불확실성의 정량화와 확률</h3>
<p>AI 에이전트가 현실 세계를 모델링할 때 가장 먼저 마주하는 장벽은 불확실성이다. 자율주행차가 전방의 물체를 감지했을 때, 그것이 보행자인지 가로수인지 100% 확신할 수 없다. 또한, 브레이크를 밟았을 때 노면 상태에 따라 제동 거리가 달라질 수 있다. 이러한 불확실성을 다루기 위해 결정이론은 확률론을 차용한다.</p>
<p>에이전트의 지식 상태는 신념(Belief)으로 표현되며, 이는 가능한 상태(State)들의 확률 분포로 나타난다. 새로운 정보(증거, Evidence)가 입수될 때마다 에이전트는 베이즈 정리(Bayes’ Theorem)를 사용하여 자신의 신념을 갱신한다.<br />
<span class="math math-display">
P(H|E) = \frac{P(E|H) P(H)}{P(E)}
</span><br />
여기서 <span class="math math-inline">P(H)</span>는 가설 <span class="math math-inline">H</span>에 대한 사전 확률(Prior Probability), <span class="math math-inline">P(E|H)</span>는 가설이 참일 때 증거 <span class="math math-inline">E</span>가 관측될 가능도(Likelihood), 그리고 <span class="math math-inline">P(H|E)</span>는 증거를 관측한 후의 사후 확률(Posterior Probability)이다. 이 갱신 과정은 에이전트가 환경 변화에 적응하고 더 정확한 판단을 내리기 위한 필수적인 전제 조건이 된다.8 확률은 에이전트가 세계를 바라보는 ’창’이며, 결정이론은 이 창을 통해 들어온 정보를 바탕으로 행동을 선택하는 ’엔진’이다.</p>
<h3>2.2  선호도와 효용 함수 (Utility Function)</h3>
<p>확률만으로는 결정을 내릴 수 없다. ’내일 비가 올 확률이 70%’라는 정보 자체는 우산을 가져갈지 말지를 결정해주지 않는다. 이를 결정하기 위해서는 비를 맞았을 때의 불쾌함과 우산을 들고 다니는 번거로움에 대한 에이전트의 주관적 평가, 즉 선호도(Preferences)가 필요하다.</p>
<p>결정이론에서는 이러한 선호도를 수학적으로 다루기 위해 효용 함수(Utility Function, <span class="math math-inline">U(S)</span>)를 도입한다. 효용 함수는 특정 상태 <span class="math math-inline">S</span>를 실수 값(Real Number)으로 매핑하여, 에이전트가 그 상태를 얼마나 바람직하게 여기는지를 정량화한다.9 예를 들어, 로봇 청소기의 경우 ‘청소 완료 &amp; 배터리 충분’ 상태에는 +100의 효용을, ‘배터리 방전’ 상태에는 -1000의 효용을 부여할 수 있다.</p>
<p>폰 노이만과 모르겐슈턴(Von Neumann &amp; Morgenstern)은 합리적인 에이전트가 가져야 할 선호도의 공리(Axioms)를 정립했다.11</p>
<ol>
<li><strong>완전성 (Orderability):</strong> 에이전트는 두 결과 A와 B에 대해, A를 더 선호하거나, B를 더 선호하거나, 둘 다 무차별(Indifferent)하다고 판단할 수 있어야 한다. 판단을 유보해서는 안 된다.</li>
<li><strong>이행성 (Transitivity):</strong> A보다 B를 선호하고, B보다 C를 선호한다면, 반드시 A보다 C를 선호해야 한다. 이 공리가 깨지면 에이전트는 순환 논리에 빠져 무한히 자원을 소모하는 비합리적 행동을 보일 수 있다.</li>
<li><strong>연속성 (Continuity):</strong> A &gt; B &gt; C일 때, 최악의 결과 C와 최상의 결과 A를 확률적으로 섞은 복권(Lottery)이 B와 동등해지는 확률 <span class="math math-inline">p</span>가 존재해야 한다.</li>
<li><strong>대체성 (Substitutability):</strong> 두 결과가 무차별하다면, 더 복잡한 결정 상황에서도 서로 바꿔 쓸 수 있어야 한다.</li>
</ol>
<p>이러한 공리들을 만족하는 에이전트의 행동은 마치 어떤 효용 함수를 극대화하려는 것처럼 수학적으로 기술될 수 있다. 이는 AI 에이전트의 행동 원리를 설계하는 데 있어 강력한 이론적 근거를 제공한다.11</p>
<h3>2.3  최대 기대 효용 원칙 (MEU)</h3>
<p>확률과 효용이 결합되면, 합리적 에이전트의 행동 원칙은 최대 기대 효용(Maximum Expected Utility, MEU) 원칙으로 귀결된다. 에이전트는 행동 <span class="math math-inline">a</span>를 선택했을 때 발생할 수 있는 모든 결과 상태 <span class="math math-inline">s&#39;</span>에 대해, 그 상태가 발생할 확률과 그 상태의 효용을 곱한 값의 총합(기대 효용)을 계산하고, 이 값이 가장 큰 행동을 선택해야 한다.8<br />
<span class="math math-display">
\text{Action}_{MEU}(s) = \arg\max_{a \in A} \sum_{s&#39; \in S} P(s&#39; | s, a) U(s&#39;)
</span><br />
여기서 <span class="math math-inline">P(s&#39;|s, a)</span>는 현재 상태 <span class="math math-inline">s</span>에서 행동 <span class="math math-inline">a</span>를 취했을 때 다음 상태가 <span class="math math-inline">s&#39;</span>이 될 확률(전이 모델)이며, <span class="math math-inline">U(s&#39;)</span>는 결과 상태의 효용이다.</p>
<p>이 원칙은 간단해 보이지만, 실제 AI 시스템에서 이를 완벽하게 구현하는 것은 매우 어렵다. 첫째, 정확한 확률 모델 <span class="math math-inline">P(s&#39;|s, a)</span>를 얻기 위해서는 방대한 데이터와 환경에 대한 깊은 이해가 필요하다. 둘째, 가능한 상태 공간 <span class="math math-inline">S</span>가 매우 클 경우(예: 바둑, 자율주행), 모든 결과에 대해 기대값을 계산하는 것은 계산 복잡도 면에서 불가능에 가깝다. 셋째, 효용 함수 <span class="math math-inline">U</span>를 정의하는 것 자체가 어려운 경우가 많다(예: 사람의 생명 가치와 시간 절약의 가치를 어떻게 비교할 것인가?). 따라서 현대 AI 연구는 근사(Approximation), 학습(Learning), 그리고 휴리스틱(Heuristics)을 통해 MEU 원칙을 실용적으로 구현하는 데 초점을 맞추고 있다.8</p>
<h2>3.  순차적 의사결정 프레임워크: MDP와 시간의 문제</h2>
<p>단발성 의사결정(One-shot decision)과 달리, 대부분의 지능형 에이전트는 연속적인 시간의 흐름 속에서 일련의 결정을 내려야 한다. 현재의 행동은 즉각적인 보상뿐만 아니라 미래의 상태에도 영향을 미치며, 보상은 행동 직후가 아니라 먼 미래에 지연되어 나타날 수 있다. 이러한 순차적 의사결정(Sequential Decision Making) 문제를 다루기 위해 AI는 마르코프 결정 과정(Markov Decision Process, MDP)을 표준 모델로 채택한다.14</p>
<h3>3.1  MDP의 정의와 구성 요소</h3>
<p>MDP는 완전 관찰 가능한(Fully Observable) 확률적 환경을 수학적으로 정식화한 모델이다. MDP는 일반적으로 5개의 튜플 <span class="math math-inline">(S, A, T, R, \gamma)</span>로 정의된다.14</p>
<table><thead><tr><th><strong>구성 요소</strong></th><th><strong>기호</strong></th><th><strong>설명</strong></th></tr></thead><tbody>
<tr><td><strong>상태 집합</strong></td><td><span class="math math-inline">S</span></td><td>에이전트가 처할 수 있는 모든 가능한 상황의 집합. 환경의 모든 관련 정보를 포함해야 한다 (Markov Property).</td></tr>
<tr><td><strong>행동 집합</strong></td><td><span class="math math-inline">A</span></td><td>에이전트가 각 상태에서 선택할 수 있는 행동들의 집합.</td></tr>
<tr><td><strong>전이 함수</strong></td><td><span class="math math-inline">T</span></td><td><span class="math math-inline">T(s, a, s&#39;) = P(s&#39;|s, a</span>. 상태 $<span class="math math-display">에서 행동 </span>$를 수행했을 때 상태 <span class="math math-inline">s&#39;</span>로 전이될 확률. 환경의 물리 법칙이나 규칙을 나타낸다.</td></tr>
<tr><td><strong>보상 함수</strong></td><td><span class="math math-inline">R</span></td><td><span class="math math-inline">R(s, a, s&#39;)</span>. 상태 전이 과정에서 에이전트가 받는 즉각적인 보상 값. 목표 달성을 유도하는 신호이다.</td></tr>
<tr><td><strong>할인 인자</strong></td><td><span class="math math-inline">\gamma</span></td><td>$\gamma \in $. 미래의 보상을 현재 가치로 환산하는 비율. 무한한 시간 지평에서의 보상 합이 발산하는 것을 막고, 당장의 보상과 미래 보상 간의 가중치를 조절한다.</td></tr>
</tbody></table>
<p>MDP의 핵심 가정인 마르코프 속성(Markov Property)은 “미래는 오직 현재 상태에만 의존하며, 과거의 역사와는 무관하다“는 것이다. 즉, 현재 상태 <span class="math math-inline">S_t</span>는 과거의 모든 정보를 충분히 요약하고 있어야 한다. 이 가정 덕분에 에이전트는 복잡한 과거 이력을 기억할 필요 없이 현재 상태만 보고 최적의 행동을 결정할 수 있다.15</p>
<h3>3.2 정책(Policy)과 가치 함수(Value Function)</h3>
<p>MDP의 목표는 단순한 행동 선택이 아니라, 어떤 상태에 처하든 최적의 행동을 알려주는 함수인 **정책(Policy, <span class="math math-inline">\pi</span>)**을 찾는 것이다. 정책은 상태를 입력으로 받아 행동을 출력하는 매핑 함수 <span class="math math-inline">\pi: S \rightarrow A</span>이다.</p>
<p>어떤 정책이 얼마나 좋은지를 평가하기 위해 **가치 함수(Value Function)**가 사용된다. 상태 가치 함수 <span class="math math-inline">V^\pi(s)</span>는 정책 <span class="math math-inline">\pi</span>를 따랐을 때 상태 <span class="math math-inline">s</span>로부터 기대되는 누적 할인 보상(Cumulative Discounted Reward)의 합이다.</p>
<p><span class="math math-display">V^\pi(s) = E_\pi \left| \right| </span></p>
<p>최적 정책 <span class="math math-inline">\pi^*</span>는 모든 상태 <span class="math math-inline">s</span>에 대해 <span class="math math-inline">V^{\pi^*}(s) \ge V^\pi(s)</span>를 만족하는 정책이다. 이를 찾기 위해 벨만 방정식(Bellman Equation)이 사용된다. 벨만 방정식은 현재 상태의 가치와 다음 상태의 가치 사이의 재귀적인 관계를 정의한다.<br />
<span class="math math-display">
V^*(s) = \max_{a \in A} \sum_{s&#39; \in S} T(s, a, s&#39;)
</span><br />
이 방정식은 동적 계획법(Dynamic Programming)의 기초가 되며, 이를 통해 복잡한 순차적 의사결정 문제를 작은 하위 문제들로 쪼개어 해결할 수 있다.17</p>
<h3>3.2  MDP 해결 알고리즘</h3>
<p>MDP를 푼다는 것은 최적 정책 <span class="math math-inline">\pi^*</span>를 찾는 것을 의미한다. 이를 위해 주로 두 가지 알고리즘이 사용된다.</p>
<ol>
<li><strong>가치 반복 (Value Iteration):</strong> 임의의 가치 함수로 시작하여, 벨만 최적 방정식을 반복적으로 적용해 가치 함수를 갱신한다. 가치 함수가 수렴하면, 그 가치 함수에 대해 탐욕적(Greedy)으로 행동을 선택함으로써 최적 정책을 얻을 수 있다. 이는 바나흐 고정점 정리(Banach Fixed-point Theorem)에 의해 수렴이 보장된다.18</li>
<li><strong>정책 반복 (Policy Iteration):</strong> 임의의 정책으로 시작하여, 현재 정책의 가치를 평가(Policy Evaluation)하고, 그 가치를 바탕으로 더 나은 정책으로 개선(Policy Improvement)하는 과정을 반복한다. 일반적으로 가치 반복보다 적은 반복 횟수로 수렴하지만, 각 단계의 연산 비용이 높을 수 있다.17</li>
</ol>
<p>의료 분야에서는 환자의 상태 변화를 MDP로 모델링하여 최적의 치료 계획을 수립하는 연구가 활발하다. 예를 들어, 당뇨병 환자의 혈당 수치 등을 상태로 정의하고, 약물 투여나 생활 습관 조정을 행동으로 설정하여 장기적인 건강 합병증 위험을 최소화하는 정책을 도출한다.19 이러한 접근은 임상의의 경험적 판단을 넘어 데이터에 기반한 정밀 의료(Precision Medicine)를 가능하게 한다.</p>
<h2>4.  불완전 정보와 부분 관찰 가능성: POMDP</h2>
<p>현실 세계는 MDP의 가정처럼 투명하지 않다. 로봇은 자신의 위치를 정확히 알 수 없고(GPS 오차), 의사는 환자의 내부 장기 상태를 완벽히 볼 수 없으며(진단의 불확실성), 포커 게임 플레이어는 상대의 패를 볼 수 없다. 이러한 불완전 정보(Incomplete Information) 상황을 다루기 위해 **부분 관찰 마르코프 결정 과정(Partially Observable MDP, POMDP)**이 고안되었다.23</p>
<h3>4.1  POMDP의 구조와 신념 상태 (Belief State)</h3>
<p>POMDP는 MDP의 5가지 요소에 **관측 집합(<span class="math math-inline">\Omega</span>)**과 **관측 함수(<span class="math math-inline">O</span>)**를 추가한 7-tuple <span class="math math-inline">(S, A, T, R, \Omega, O, \gamma)</span>로 정의된다.</p>
<ul>
<li><strong>관측 함수 <span class="math math-inline">O(o|s&#39;, a)</span>:</strong> 행동 <span class="math math-inline">a</span>를 수행하여 상태 <span class="math math-inline">s&#39;</span>에 도달했을 때, 관측 <span class="math math-inline">o</span>를 얻을 확률.</li>
</ul>
<p>POMDP에서 에이전트는 현재 상태 <span class="math math-inline">s</span>를 직접 알 수 없다. 대신 과거의 행동과 관측 이력을 바탕으로 현재 상태가 무엇일지에 대한 확률 분포인 **신념 상태(Belief State, <span class="math math-inline">b</span>)**를 유지해야 한다.24 신념 상태 <span class="math math-inline">b(s)</span>는 에이전트가 상태 <span class="math math-inline">s</span>에 있을 확률을 나타낸다.</p>
<p>에이전트가 행동 <span class="math math-inline">a</span>를 하고 관측 <span class="math math-inline">o</span>를 얻었을 때, 신념 상태는 다음과 같이 베이즈 갱신을 통해 업데이트된다 (<span class="math math-inline">b \to b&#39;</span>).<br />
<span class="math math-display">
b&#39;(s&#39;) = \eta O(o | s&#39;, a) \sum_{s \in S} T(s&#39; | s, a) b(s)
</span><br />
여기서 <span class="math math-inline">\eta</span>는 정규화 상수이다. 이 과정은 에이전트가 불확실한 정보 속에서도 끊임없이 세계에 대한 내부 모델을 수정해 나가는 과정을 수학적으로 표현한 것이다.15</p>
<h3>4.2  신념 공간에서의 의사결정</h3>
<p>POMDP의 가장 큰 특징은 문제를 ’신념 상태 공간(Belief Space)에서의 MDP’로 변환하여 풀 수 있다는 점이다.24 원래 상태 공간 <span class="math math-inline">S</span>는 유한하더라도, 신념 공간은 연속적인 확률 분포의 공간이므로 무한하다. 이로 인해 POMDP의 정확한 해를 구하는 것은 계산적으로 매우 어렵다(PSPACE-complete).8</p>
<p>하지만 신념 공간에서의 가치 함수가 조각별 선형 볼록(Piecewise Linear Convex, PWLC) 함수라는 성질을 이용하여 근사해를 구할 수 있다. 초기에는 정확한 해법이 연구되었으나, 최근에는 <strong>포인트 기반(Point-based)</strong> 알고리즘들이 주류를 이룬다. PBVI(Point-Based Value Iteration)나 POMCP(Partially Observable Monte Carlo Planning)와 같은 알고리즘은 신념 공간 전체를 다루는 대신, 도달 가능한 신념점(Belief Points)들만을 샘플링하여 가치 함수를 갱신함으로써 계산 효율성을 획기적으로 높였다.18</p>
<h3>4.3  자율주행과 POMDP의 응용</h3>
<p>자율주행 분야에서 POMDP는 필수적인 도구이다. 자율주행차는 센서를 통해 주변 차량의 위치와 속도는 관측할 수 있지만, 그 차량 운전자의 ‘의도(Intention)’(예: 끼어들기를 할 것인가, 양보할 것인가)는 직접 관측할 수 없다. 여기서 ’의도’는 숨겨진 상태(Hidden State)가 되고, 차량의 움직임은 관측(Observation)이 된다.26</p>
<p>연구 사례에 따르면, 교차로 진입이나 차선 변경 시나리오를 POMDP로 모델링했을 때, 단순한 규칙 기반(Rule-based) 시스템이나 MDP 기반 시스템보다 훨씬 더 안전하고 효율적인 주행이 가능했다. POMDP 에이전트는 상대방의 의도를 확률적으로 추론하고, 불확실성이 높을 때는 조심스럽게 행동(예: 감속)하여 정보를 더 수집하려는(Information Gathering) 행동을 보이기 때문이다.25 최근 연구에서는 계산 속도를 높이기 위해 계층적 옵션(Hierarchical Options)을 도입한 HOMDP 등을 통해 실시간 의사결정을 지원하고 있다.26</p>
<h2>5.  다중 에이전트 시스템과 게임 이론적 전략</h2>
<p>지금까지 논의한 MDP와 POMDP는 환경이 에이전트의 행동에 반응하긴 하지만, 환경 자체가 전략적(Strategic)이지는 않은 단일 에이전트 상황을 가정했다. 그러나 현실 세계, 특히 도로 교통망이나 금융 시장, 경매 시스템 등에는 목표를 가진 여러 에이전트가 공존한다. 이러한 **다중 에이전트 시스템(Multi-Agent Systems, MAS)**에서는 나의 최적 행동이 상대방의 행동에 따라 달라지며, 상대방 또한 나의 행동을 예측하여 움직인다. 이를 분석하기 위해 결정이론은 게임 이론(Game Theory)과 결합한다.28</p>
<h3>5.1  전략적 상호작용과 내쉬 균형</h3>
<p>게임 이론은 상호 의존적인 의사결정 상황을 수학적으로 모델링한다. 가장 핵심적인 개념은 **내쉬 균형(Nash Equilibrium)**이다. 내쉬 균형은 모든 참가자가 상대방의 전략을 주어진 것으로 가정하고 자신의 이익을 최대화하는 최적 대응(Best Response)을 선택했을 때, 아무도 자신의 전략을 단독으로 변경하여 이득을 볼 수 없는 상태를 말한다.29</p>
<p>예를 들어, 교통 혼잡 제어 문제에서 각 운전자(또는 자율주행차)는 통행 시간을 최소화하기 위해 경로를 선택한다. 만약 모든 운전자가 내쉬 균형 상태에 도달한다면, 이는 더 이상 지름길이 없는 안정적인 교통 흐름을 의미한다(Wardrop’s Principle). AI 기반 교통 신호 제어 시스템에서는 각 교차로의 신호등을 에이전트로 설정하고, 이들이 협력하거나 경쟁하며 전체 교통망의 흐름을 최적화하도록 게임 이론 모델을 적용한다.31 스택엘버그(Stackelberg) 게임 모델은 리더(신호등 시스템)와 팔로워(운전자) 간의 비대칭적 상호작용을 모델링하여 더욱 현실적인 제어 전략을 수립하는 데 활용된다.33</p>
<h3>5.2  협력과 경쟁: 메커니즘 디자인 (Mechanism Design)</h3>
<p>MAS에서는 에이전트들이 제한된 자원을 두고 경쟁하거나(예: 클라우드 서버 할당, 주파수 경매), 공통의 목표를 위해 협력해야 한다(예: 자율주행차 군집 주행). **메커니즘 디자인(Mechanism Design)**은 이러한 상호작용의 규칙(Rule of the game)을 설계하여, 이기적인 에이전트들이 개별 이익을 추구하더라도 결과적으로 시스템 전체의 목표(사회적 후생 극대화 등)가 달성되도록 유도하는 분야이다. 이를 ’역게임 이론(Reverse Game Theory)’이라고도 부른다.28</p>
<p>대표적인 예가 VCG(Vickrey-Clarke-Groves) 메커니즘이다. VCG 경매에서는 입찰자들이 자신의 진정한 가치를 입찰하는 것이 지배적 전략(Dominant Strategy)이 되도록 보장한다. 낙찰자는 자신이 타인에게 끼친 기회비용(Externality)만큼을 지불하게 함으로써, 전략적 거짓말을 방지하고 자원의 효율적 배분을 달성한다.34</p>
<p>이러한 이론은 AI 에이전트 간의 자원 할당뿐만 아니라, 블록체인 네트워크의 합의 알고리즘이나 온라인 광고 경매 시스템 등 현대 디지털 경제의 핵심 인프라를 설계하는 데 필수적으로 사용된다.28 건설 입찰 분야에서도 AI와 게임 이론을 결합하여 경쟁자의 입찰 성향을 예측하고 최적의 입찰가를 산정하는 모델이 연구되고 있다.36</p>
<h2>6.  학습하는 결정주체: 강화학습 (Reinforcement Learning)</h2>
<p>전통적인 결정이론(MDP/POMDP)은 환경의 모델(전이 확률 <span class="math math-inline">T</span>, 보상 함수 <span class="math math-inline">R</span>)을 미리 알고 있거나(Model-based), 이를 완벽하게 추정할 수 있다고 가정하는 경우가 많다. 그러나 실제 환경은 너무 복잡하여 모델링하기 어렵거나, 사전에 알 수 없는 경우가 대부분이다. **강화학습(Reinforcement Learning, RL)**은 이러한 상황에서 에이전트가 환경과의 상호작용(Trial and Error)을 통해 데이터로부터 최적의 의사결정 정책을 스스로 학습하는 방법론이다.14 RL은 결정이론의 실용적 구현체이자, 가장 성공적인 AI 패러다임 중 하나이다.</p>
<h3>6.1  강화학습과 결정이론의 관계</h3>
<p>강화학습은 기본적으로 MDP 프레임워크 위에서 작동한다. 하지만 “기대 효용을 계산하여 행동한다“는 연역적 접근 대신, “행동을 해보고 결과를 관찰하여 가치를 갱신한다“는 귀납적 접근을 취한다.</p>
<ul>
<li><strong>탐험(Exploration) vs. 이용(Exploitation):</strong> RL 에이전트는 이미 알고 있는 좋은 행동을 반복하여 보상을 챙길 것인지(이용), 아니면 더 좋은 행동이 있을지 모르니 새로운 행동을 시도해 볼 것인지(탐험)의 딜레마를 해결해야 한다. 이는 결정이론의 정보 가치(Value of Information) 이론과 맞닿아 있다.</li>
<li><strong>신용 할당 문제(Credit Assignment Problem):</strong> 현재 받은 보상이 과거의 어떤 행동 덕분인지, 혹은 탓인지 알아내는 문제이다. RL은 할인 인자(<span class="math math-inline">\gamma</span>)와 가치 함수 업데이트를 통해 이 문제를 해결한다.</li>
</ul>
<h3>6.2  모델 기반(Model-based) vs 모델 프리(Model-free) RL</h3>
<p>강화학습 알고리즘은 환경 모델의 학습 여부에 따라 크게 두 가지로 분류된다.14</p>
<table><thead><tr><th><strong>구분</strong></th><th><strong>특징</strong></th><th><strong>장점</strong></th><th><strong>단점</strong></th><th><strong>대표 알고리즘</strong></th></tr></thead><tbody>
<tr><td><strong>Model-based RL</strong></td><td>에이전트가 환경의 전이(<span class="math math-inline">T</span>)와 보상(<span class="math math-inline">R</span>) 모델을 학습하고, 이를 통해 미래를 시뮬레이션(Planning)하여 정책을 결정.</td><td>데이터 효율성 높음 (적은 경험으로 학습 가능). 계획 수립 가능.</td><td>모델의 오차(Bias)가 성능 저하로 이어질 수 있음. 모델링 자체가 복잡함.</td><td>Dyna-Q, AlphaZero (MCTS 결합)</td></tr>
<tr><td><strong>Model-free RL</strong></td><td>환경 모델 없이, 상태-행동 쌍의 가치(Q-value)나 정책 함수(<span class="math math-inline">\pi</span>)를 직접 학습.</td><td>구현이 상대적으로 간단. 모델링 오차 걱정 없음.</td><td>방대한 양의 학습 데이터(Interaction)가 필요함.</td><td>Q-Learning, DQN, PPO, A3C</td></tr>
</tbody></table>
<p>최근에는 두 방식의 장점을 결합하는 시도가 늘고 있다. 예를 들어, 학습된 모델을 사용하여 가상의 경험을 생성하고 이를 통해 정책을 학습하거나(Dyna 아키텍처), 불확실성이 높은 상황에서는 모델 기반 계획을 사용하고 익숙한 상황에서는 모델 프리 직관을 사용하는 방식이다.38</p>
<h3>6.3  역강화학습(Inverse Reinforcement Learning, IRL)과 가치 정렬</h3>
<p>일반적인 RL은 보상 함수가 주어졌을 때 최적 행동을 찾는다. 그러나 자율주행차에게 “안전하고 예의 바르게 운전하라“는 목표를 수치적인 보상 함수로 정의하기는 매우 어렵다. 자칫하면 목적 함수를 잘못 설정하여 부작용이 발생하는 ‘보상 해킹(Reward Hacking)’ 문제가 발생할 수 있다.</p>
<p>**역강화학습(IRL)**은 이를 뒤집어, 전문가(인간)의 행동 데이터(Demonstration)를 관찰하고 그 행동 이면에 숨겨진 보상 함수(효용 함수)를 역으로 추론한다.14 이는 “전문가가 이렇게 행동하는 것을 보니, 아마도 이런 효용 함수를 최대화하려는 것 같다“라고 추론하는 과정이다. IRL은 인간의 복잡한 선호도를 AI에게 가르치는 핵심 기술로, AI 안전(Safety) 및 가치 정렬(Value Alignment) 분야에서 매우 중요하다. 최근에는 최대 엔트로피(Maximum Entropy) 원리를 적용하여, 데이터의 불완전성을 극복하고 강건한 보상 함수를 추정하는 기법들이 개발되고 있다.40</p>
<h2>7.  생성형 AI 시대의 결정이론: LLM 에이전트와 ReAct</h2>
<p>2023년 이후 대형언어모델(LLM)의 급격한 발전은 AI 결정이론의 판도를 완전히 바꾸어 놓았다. LLM은 단순한 언어 생성기를 넘어, 일반적인 추론 능력과 방대한 상식(Common Sense)을 갖춘 ’기반 모델(Foundation Model)’로서 기능하며, 자율 에이전트의 인지적 중추(Cognitive Engine) 역할을 수행하고 있다.3</p>
<h3>7.1  언어 모델을 이용한 상태-행동 매핑 및 계획</h3>
<p>기존 RL 에이전트가 수치적 상태 벡터를 입력받아 행동 번호를 출력했다면, LLM 에이전트는 자연어로 기술된 상황(State)을 이해하고, 자연어로 행동(Action)을 계획한다. 이는 에이전트가 다룰 수 있는 상태 공간과 행동 공간을 무한대로 확장시킨다.</p>
<p>LLM은 그 자체로 일종의 ‘세계 모델(World Model)’ 역할을 할 수 있다. 에이전트가 “내가 A 행동을 하면 무슨 일이 일어날까?“라고 물으면, LLM은 방대한 훈련 데이터를 바탕으로 그 결과를 예측해준다.5 이를 통해 별도의 학습 없이도 제로샷(Zero-shot) 계획 수립이 가능해졌다.</p>
<h3>7.2  생각의 사슬(Chain of Thought)과 의사결정 트리 탐색</h3>
<p>‘생각의 사슬(Chain of Thought, CoT)’ 프롬프팅은 LLM에게 “단계별로 생각하라“고 지시함으로써 복잡한 추론 성능을 비약적으로 향상시킨다.44 결정이론 관점에서 CoT는 복잡한 문제를 하위 문제로 분해(Decomposition)하고, 논리적 단계를 거쳐 해답을 찾아가는 휴리스틱 탐색 과정과 유사하다.</p>
<p>더 나아가, ’Reasoning via Planning (RAP)’이나 ‘Tree of Thoughts (ToT)’ 같은 기법은 LLM의 추론 과정을 트리 구조로 시각화하고, 몬테카를로 트리 탐색(MCTS)이나 너비 우선 탐색(BFS)과 같은 고전적 탐색 알고리즘을 적용한다.43 이는 LLM이 단순히 가장 그럴듯한 다음 단어를 생성하는 것을 넘어, 여러 가능성을 시뮬레이션하고, 각 경로의 가치를 평가(Evaluation)하여 최적의 사고 과정을 선택하도록 유도하는 진정한 의미의 ‘숙고(Deliberation)’ 시스템을 구현한 것이다.</p>
<h3>7.3  ReAct 프레임워크: 추론과 행동의 루프</h3>
<p><strong>ReAct (Reasoning + Acting)</strong> 프레임워크는 LLM이 닫힌 세계에서 벗어나 외부 도구(검색 엔진, 계산기, API 등)를 사용하며 현실 세계와 상호작용하도록 만든다.6 ReAct 에이전트는 다음과 같은 루프를 반복한다.</p>
<ol>
<li><strong>Thought (추론):</strong> 현재 상황을 분석하고 필요한 정보나 행동을 결정한다. (정책 수립)</li>
<li><strong>Action (행동):</strong> 외부 도구를 호출한다. (행동 수행)</li>
<li><strong>Observation (관측):</strong> 도구의 실행 결과를 받아들인다. (정보 획득)</li>
<li><strong>Repeat:</strong> 새로운 정보를 바탕으로 다시 추론한다.</li>
</ol>
<p>이 구조는 POMDP의 <code>Sense -&gt; Reason -&gt; Act</code> 루프와 정확히 일치하며, LLM이 불확실한 환경에서 부족한 정보를 능동적으로 수집하고 계획을 수정하는 적응형 에이전트(Adaptive Agent)로 기능하게 한다.46</p>
<h3>7.4  결정 트랜스포머 (Decision Transformer)</h3>
<p>결정이론과 딥러닝의 가장 급진적인 결합 중 하나는 **결정 트랜스포머(Decision Transformer, DT)**이다. 기존 RL이 벨만 방정식을 이용해 가치 함수를 추정하려 했다면, DT는 강화학습 문제를 ‘시퀀스 모델링(Sequence Modeling)’ 문제로 재정의한다.48</p>
<p>DT는 상태, 행동, 보상의 궤적(Trajectory)인 <span class="math math-inline">\tau = (R_1, s_1, a_1, R_2, s_2, a_2,...)</span>를 마치 문장처럼 취급하여 트랜스포머 모델에 학습시킨다. 그리고 테스트 단계에서는 원하는 목표 보상(Target Return)과 현재 상태를 입력으로 주면, 그 보상을 달성할 수 있는 행동 시퀀스를 생성(Generation)해낸다.50 이는 최적화 문제를 조건부 생성 문제(Conditional Generation)로 치환한 것으로, 오프라인 RL(Offline RL) 분야에서 기존 알고리즘을 능가하는 성능과 안정성을 보여주며 큰 주목을 받고 있다.51</p>
<h2>8.  주요 응용 사례 심층 분석</h2>
<h3>8.1  자율주행 (Autonomous Driving)</h3>
<p>자율주행은 결정이론의 모든 난제가 집약된 분야이다.</p>
<ul>
<li><strong>센서 융합과 불확실성:</strong> 카메라는 역광에 취약하고, 라다는 비금속 물체를 놓칠 수 있다. 이러한 센서 데이터의 불확실성을 통합하기 위해 베이지안 필터링과 POMDP가 사용된다.</li>
<li><strong>사회적 상호작용:</strong> 교차로에서 누가 먼저 갈지 눈치 싸움을 하는 것은 전형적인 게임 이론 상황이다. 자율주행차는 상대방의 양보 확률을 추정하고, 자신의 공격성(Aggressiveness)을 조절하는 정책을 학습한다. 최근에는 인간 운전자의 내부 상태를 ’잠재 변수(Latent Variable)’로 모델링하여 예측력을 높이는 연구가 진행 중이다.25</li>
</ul>
<h3>8.2  헬스케어 (Healthcare)</h3>
<p>의료 AI는 진단 보조를 넘어 치료 계획(Treatment Planning)으로 영역을 확장하고 있다.</p>
<ul>
<li><strong>동적 치료 계획:</strong> 암 치료나 패혈증 관리처럼 시간에 따라 환자 상태가 급변하고, 약물 반응이 지연되어 나타나는 경우 MDP/RL이 매우 효과적이다. AI는 방대한 임상 데이터를 통해 기존 가이드라인보다 더 정밀한 개인 맞춤형 투약 스케줄을 제안한다.21</li>
<li><strong>비용-효용 분석:</strong> 모든 검사를 다 하는 것이 능사는 아니다. 결정이론은 검사 비용, 환자의 고통, 그리고 검사로 얻을 수 있는 정보의 가치(Information Gain)를 종합적으로 고려하여, ’추가 검사’를 할지 ’치료 시작’을 할지 최적의 타이밍을 결정한다.</li>
</ul>
<h3>8.3  금융 (Finance) 및 트레이딩</h3>
<p>금융 시장은 데이터의 노이즈가 극심하고(Low Signal-to-Noise Ratio), 시장 참여자들의 상호작용이 가격을 결정하는 복잡계이다.</p>
<ul>
<li><strong>알고리즘 트레이딩:</strong> 강화학습 에이전트는 과거 주가 패턴, 거래량, 뉴스 감성 분석 등을 상태로 입력받아 매수/매도/관망을 결정한다. 이때 단순히 수익률만 최대화하는 것이 아니라, 샤프 지수(Sharpe Ratio)와 같은 위험 대비 수익률을 보상 함수로 설정하여 안정적인 투자를 유도한다.53</li>
<li><strong>포트폴리오 관리:</strong> 현대 포트폴리오 이론의 평균-분산 최적화는 2차 계획법(Quadratic Programming)으로 풀리지만, 현실적인 제약 조건(거래 비용, 최소 투자 단위 등)이 붙으면 난해해진다. 여기에 전망 이론(Prospect Theory)과 같은 행동 경제학적 요소를 반영한 비선형 효용 함수를 적용하여 더욱 인간 친화적인 자산 배분 모델을 구축한다.55</li>
</ul>
<h2>9.  결론 및 2025년 미래 전망</h2>
<p>2024년 말에서 2025년으로 넘어가는 현재, 인공지능 결정이론은 중대한 전환점을 맞이하고 있다.</p>
<p>첫째, <strong>’System 1’에서 ’System 2’로의 진화</strong>가 가속화되고 있다. 노벨 경제학상 수상자 대니얼 카너먼이 분류한 것처럼, 기존의 직관적이고 빠른 딥러닝(System 1)에, 논리적이고 숙고하는 탐색/계획 알고리즘(System 2)이 결합되고 있다. OpenAI의 o1 모델이나 Google의 Gemini 등은 응답을 생성하기 전에 내부적으로 긴 ’생각의 시간’을 가지며 추론 과정을 거친다.57 이는 AI가 단순한 패턴 매칭을 넘어 진정한 의미의 문제 해결 능력(Problem Solving)을 갖추기 시작했음을 시사한다.</p>
<p>둘째, <strong>하이브리드 AI와 신경-상징적(Neuro-Symbolic) 접근</strong>의 부상이다. 딥러닝의 블랙박스 특성은 안전이 중요한(Safety-critical) 결정에 AI를 도입하는 데 걸림돌이 되어 왔다. 이를 해결하기 위해 명시적인 규칙(Rule)이나 논리(Logic)를 딥러닝 모델과 결합하려는 시도가 늘고 있다. 예를 들어, 자율주행차의 제어는 강화학습으로 하되, “중앙선을 넘지 말라“는 절대적인 안전 규칙은 상징적 논리로 강제하는 식이다.59</p>
<p>셋째, **윤리적 결정과 가치 정렬(Alignment)**이 기술적 과제로 편입되고 있다. AI가 사회적으로 수용 가능한 결정을 내리도록 하기 위해, 공정성(Fairness), 투명성(Transparency), 책임성(Accountability)을 수학적인 제약 조건이나 보상 함수에 통합하려는 연구가 필수적이다.4 결정이론은 이제 효율성뿐만 아니라 윤리성까지 최적화 변수로 고려해야 한다.</p>
<p>요약하자면, 인공지능 분야의 결정이론은 **불확실성을 다루는 수학적 기초(확률/효용)에서 시작하여, 순차적 문제를 해결하는 알고리즘(MDP/POMDP)으로 확장되었고, 데이터로부터 배우는 학습 시스템(RL)으로 진화했으며, 이제는 언어를 통해 추론하고 도구를 사용하는 자율 에이전트(LLM Agents)**로 꽃피우고 있다. 이러한 발전은 AI가 단순한 도구를 넘어, 인간과 협력하고 복잡한 현실 세계의 난제를 함께 해결하는 동반자로서의 지위를 확립하는 데 핵심적인 역할을 수행할 것이다.</p>
<h2>10. 참고 자료</h2>
<ol>
<li>Intelligent agent - Wikipedia, https://en.wikipedia.org/wiki/Intelligent_agent</li>
<li>Decision Theory in AI - GeeksforGeeks, https://www.geeksforgeeks.org/artificial-intelligence/decision-theory-in-ai/</li>
<li>What Are AI Agents? | IBM, https://www.ibm.com/think/topics/ai-agents</li>
<li>AI Trends From 2024 and Outlook on 2025 | Barclay Damon, https://www.barclaydamon.com/blog-post/ai-trends-from-2024-and-outlook-on-2025</li>
<li>Toward a Theory of Agents as Tool-Use Decision-Makers - arXiv, https://arxiv.org/html/2506.00886v1</li>
<li>Building Intelligent Agents with ReAct: A Deep Dive into Reasoning and Acting in AI, https://christianmendieta.ca/building-intelligent-ai-agents-with-react/</li>
<li>ReAct - Prompt Engineering Guide, https://www.promptingguide.ai/techniques/react</li>
<li>
<ol start="13">
<li>Acting under Uncertainty Maximizing Expected Utility - Foundations of Artificial Intelligence, https://gki.informatik.uni-freiburg.de/teaching/ss14/gki/lectures/ai13.pdf</li>
</ol>
</li>
<li>Preferences and utility functions The MEU principle - KCiR, https://kcir.pwr.edu.pl/~witold/ai/aie_util_d.pdf</li>
<li>Module 3 Utility Theory - University of Waterloo, https://cs.uwaterloo.ca/~ppoupart/teaching/cs886-spring13/slides/cs886-module3-utlity-theory.pdf</li>
<li>CS 188: Artificial Intelligence - University of California, Berkeley, https://inst.eecs.berkeley.edu/~cs188/sp24/assets/lectures/cs188-sp24-lec16.pdf</li>
<li>Information Theory for Agents in Artificial Intelligence, Psychology, and Economics - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC8001993/</li>
<li>CSC384: Intro to Artificial Intelligence Decision Making Under Uncertainty - University of Toronto, http://www.cs.toronto.edu/~torsten/csc384-f11/lectures/csc384f11-Lecture08-DecisionMaking.pdf</li>
<li>Are decision theory and reinforcement learning equivalent? - Quora, https://www.quora.com/Are-decision-theory-and-reinforcement-learning-equivalent</li>
<li>Partially Observable Markov Decision Process (POMDP) in AI - GeeksforGeeks, https://www.geeksforgeeks.org/artificial-intelligence/partially-observable-markov-decision-process-pomdp-in-ai/</li>
<li>11월 30, 2025에 액세스, [https://vstylestdy.tistory.com/54#:<sub>:text=%EC%A2%80%20%EB%8D%94%20%EA%B5%AC%EC%B2%B4%EC%A0%81%EC%9C%BC%EB%A1%9C%EB%8A%94,%EC%A0%90%EC%97%90%EC%84%9C%20%EC%B0%A8%EC%9D%B4%EA%B0%80%20%EB%82%A9%EB%8B%88%EB%8B%A4.](https://vstylestdy.tistory.com/54#:</sub>:text=좀 더 구체적으로는, <a href="https://vstylestdy.tistory.com/54#:~:text=%EC%A2%80%20%EB%8D%94%20%EA%B5%AC%EC%B2%B4%EC%A0%81%EC%9C%BC%EB%A1%9C%EB%8A%94,%EC%A0%90%EC%97%90%EC%84%9C%20%EC%B0%A8%EC%9D%B4%EA%B0%80%20%EB%82%A9%EB%8B%88%EB%8B%A4.">https://vstylestdy.tistory.com/54#:~:text=%EC%A2%80%20%EB%8D%94%20%EA%B5%AC%EC%B2%B4%EC%A0%81%EC%9C%BC%EB%A1%9C%EB%8A%94,%EC%A0%90%EC%97%90%EC%84%9C%20%EC%B0%A8%EC%9D%B4%EA%B0%80%20%EB%82%A9%EB%8B%88%EB%8B%A4.</a></li>
<li>Reinforcement Learning Algorithms | Model-Based vs Model-Free | L-05 - YouTube, https://www.youtube.com/watch?v=rB5bz_1cj04</li>
<li>Partially Observable Markov Decision Processes and Robotics - Annual Reviews, https://www.annualreviews.org/content/journals/10.1146/annurev-control-042920-092451</li>
<li>Comparison of MDP and Poisson Treatment Policies | Download Table - ResearchGate, https://www.researchgate.net/figure/Comparison-of-MDP-and-Poisson-Treatment-Policies_tbl1_309278563</li>
<li>Markov Decision Processes for Screening and Treatment of Chronic Diseases - Brian Denton, http://btdenton.engin.umich.edu/wp-content/uploads/sites/138/2015/10/Steimle-2015.pdf</li>
<li>A Promising Approach to Optimizing Sequential Treatment Decisions for Depression: Markov Decision Process - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC9550715/</li>
<li>MDP recommendation actions on each state for male patients. This figure… - ResearchGate, https://www.researchgate.net/figure/MDP-recommendation-actions-on-each-state-for-male-patients-This-figure-shows-the_fig3_350387789</li>
<li>11월 30, 2025에 액세스, [https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process#:<sub>:text=A%20partially%20observable%20Markov%20decision,directly%20observe%20the%20underlying%20state.](https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process#:</sub>:text=A partially observable Markov decision, <a href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process#:~:text=A%20partially%20observable%20Markov%20decision,directly%20observe%20the%20underlying%20state.">https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process#:~:text=A%20partially%20observable%20Markov%20decision,directly%20observe%20the%20underlying%20state.</a></li>
<li>Partially observable Markov decision process - Wikipedia, https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process</li>
<li>Improving Automated Driving through POMDP Planning with Human Internal States - arXiv, https://arxiv.org/pdf/2005.14549</li>
<li>POMDP and Hierarchical Options MDP with Continuous Actions for Autonomous Driving at Intersections, https://www.ri.cmu.edu/app/uploads/2019/06/1811-ITSC-Carolyn-Final.pdf</li>
<li>POMDPs for Safe Visibility Reasoning in Autonomous Vehicles - Stanford University, https://web.stanford.edu/~blange/data/WLJWKHIisr21.pdf</li>
<li>What is the role of game theory in multi-agent systems? - Milvus, https://milvus.io/ai-quick-reference/what-is-the-role-of-game-theory-in-multiagent-systems</li>
<li>Integrating Game Theory and Artificial Intelligence: Strategies for Complex Decision-Making, https://www.geeksforgeeks.org/artificial-intelligence/integrating-game-theory-and-artificial-intelligence-strategies-for-complex-decision-making/</li>
<li>Nash equilibrium - Wikipedia, https://en.wikipedia.org/wiki/Nash_equilibrium</li>
<li>An Intersection Game-Theory-Based Traffic Control Algorithm in a Connected Vehicle Environment - IEEE Xplore, https://ieeexplore.ieee.org/document/7313157</li>
<li>Game Theoretical Approaches to the Handling Road Traffic, https://jrwright.info/bgtcourse/winter2019/slides/P7-GameTheoreticRoadTraffic.pdf</li>
<li>Game Theory-Based Signal Control Considering Both Pedestrians and Vehicles in Connected Environment - MDPI, https://www.mdpi.com/1424-8220/23/23/9438</li>
<li>Lecture 4 1 Examples of Mechanism Design Problems - USC Viterbi, https://viterbi-web.usc.edu/~shaddin/teaching/cs699fa17/lectures/lec4.pdf</li>
<li>Undominated VCG Redistribution Mechanisms - CMU School of Computer Science, https://www.cs.cmu.edu/~conitzer/undominatedAAMAS08.pdf</li>
<li>AI-Assisted Game Theory Approaches to Bid Pricing Under Uncertainty in Construction, https://www.mdpi.com/2673-9909/5/2/39</li>
<li>Mastering Decision Making: The Power of Reinforcement Learning in AI - Medium, https://medium.com/@nadarshah717/mastering-decision-making-the-power-of-reinforcement-learning-in-ai-9b68825fa4bf</li>
<li>[2006.16712] Model-based Reinforcement Learning: A Survey - arXiv, https://arxiv.org/abs/2006.16712</li>
<li>Cost Function Estimation Using Inverse Reinforcement Learning with Minimal Observations, https://arxiv.org/html/2505.08619v1</li>
<li>Inverse Reinforcement Learning by Estimating Expertise of Demonstrators, https://ojs.aaai.org/index.php/AAAI/article/view/33705/35860</li>
<li>A Survey of Maximum Entropy-Based Inverse Reinforcement Learning: Methods and Applications - MDPI, https://www.mdpi.com/2073-8994/17/10/1632</li>
<li>STRIDE: A Tool-Assisted LLM Agent Framework for Strategic and Interactive Decision-Making - Cowles Foundation for Research in Economics, https://cowles.yale.edu/sites/default/files/2024-05/d2393.pdf</li>
<li>Reasoning with Language Model is Planning with World Model - ACL Anthology, https://aclanthology.org/2023.emnlp-main.507.pdf</li>
<li>What is chain of thought (CoT) prompting? - IBM, https://www.ibm.com/think/topics/chain-of-thoughts</li>
<li>What is a ReAct Agent? | IBM, https://www.ibm.com/think/topics/react-agent</li>
<li>Implement ReAct Prompting for Better AI Decision-Making, https://relevanceai.com/prompt-engineering/implement-react-prompting-for-better-ai-decision-making</li>
<li>Part 1 : ReACT AI Agents: A Guide to Smarter AI Through Reasoning and Action. - Medium, https://medium.com/@gauritr01/part-1-react-ai-agents-a-guide-to-smarter-ai-through-reasoning-and-action-d5841db39530</li>
<li>Decision Transformer Model: Architecture, Use Cases, Applications and Advancements, https://www.leewayhertz.com/decision-transformer/</li>
<li>[2106.01345] Decision Transformer: Reinforcement Learning via Sequence Modeling - arXiv, https://arxiv.org/abs/2106.01345</li>
<li>Decision Transformer: Reinforcement Learning via Sequence Modeling - Google Sites, https://sites.google.com/berkeley.edu/decision-transformer</li>
<li>Decision Transformer: Reinforcement Learning via Sequence Modeling - OpenReview, https://openreview.net/pdf?id=a7APmM4B9d</li>
<li>Draft CMS Quality Measure Development Plan (MDP), https://www.cms.gov/medicare/quality-initiatives-patient-assessment-instruments/value-based-programs/macra-mips-and-apms/draft-cms-quality-measure-development-plan-mdp.pdf</li>
<li>7 Applications of Reinforcement Learning in Finance and Trading - Neptune.ai, https://neptune.ai/blog/7-applications-of-reinforcement-learning-in-finance-and-trading</li>
<li>Stock Trading Using Deep Reinforcement Learning | Case Study - Intellekt AI, https://www.intellektai.com/case-studies/stock-trading-using-deep-reinforcement-learning</li>
<li>Portfolio Optimization with Cumulative Prospect Theory Utility via Convex Optimization - Stanford University, https://stanford.edu/~boyd/papers/pdf/cpt_opt.pdf</li>
<li>Teaching Utility Theory with an Application in Modern Portfolio Optimization, https://scholarlyworks.adelphi.edu/esploro/outputs/journalArticle/Teaching-Utility-Theory-with-an-Application/991004356199406266</li>
<li>AI in the workplace: A report for 2025 - McKinsey, https://www.mckinsey.com/capabilities/tech-and-ai/our-insights/superagency-in-the-workplace-empowering-people-to-unlock-ais-full-potential-at-work</li>
<li>The Top Artificial Intelligence Trends | IBM, https://www.ibm.com/think/insights/artificial-intelligence-trends</li>
<li>Decision making in autonomous driving | TNO, https://www.tno.nl/en/digital/artificial-intelligence/safe-autonomous-systems/decision-making-autonomous-driving/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>