<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Gemini Robotics 1.5 (2025-09-25)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Gemini Robotics 1.5 (2025-09-25)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">Vision-Language-Action(VLA) 모델</a> / <span>Gemini Robotics 1.5 (2025-09-25)</span></nav>
                </div>
            </header>
            <article>
                <h1>Gemini Robotics 1.5 (2025-09-25)</h1>
<h2>1. 서론: 물리적 에이전트 시대의 도래</h2>
<h3>1.1 범용 로봇(General-Purpose Robots) 패러다임의 정의</h3>
<p>2025년 9월 25일, Google DeepMind는 Gemini Robotics 1.5 모델군을 발표하며 로보틱스 분야의 새로운 패러다임을 제시했다.1 이 기술은 사전에 엄격하게 프로그래밍된 제한된 작업을 반복 수행하는 기존의 자동화 시스템을 넘어선다. 대신, 정형화되지 않은 복잡한 실제 환경에서 다단계 과업을 자율적으로 이해하고, 계획하며, 수행하는 ’물리적 에이전트(Physical Agents)’의 시대를 여는 핵심 기술로 평가된다.1 본 보고서에서 정의하는 범용 로봇이란, 특정 작업에 국한되지 않고 다양한 상황과 요구에 동적으로 적응하며, 인간과 유사한 수준의 일반화된 문제 해결 능력을 물리적 세계에서 발휘하는 지능형 시스템을 의미한다. 이는 고정된 환경에서 예측 가능한 작업을 수행하는 산업용 로봇과는 근본적으로 다른 개념으로, 로봇이 단순한 ’도구’에서 지능적인 ’파트너’로 진화하는 전환점을 상징한다.5</p>
<h3>1.2 연구의 필요성 및 보고서의 목표</h3>
<p>Gemini Robotics 1.5는 단순히 이전 세대 모델의 성능을 점진적으로 개선한 것이 아니다. 이는 ‘이중 모델 아키텍처(Dual-Model Architecture)’, ‘체화된 사고(Embodied Thinking)’, 그리고 ’모션 전이(Motion Transfer)’와 같은 혁신적인 개념을 도입하여 로보틱스 분야의 오랜 난제들을 해결하려는 근본적인 시도라는 점에서 학술적, 산업적 중요성이 매우 크다.4 기존 로봇 기술은 특정 하드웨어에 종속되거나, 예측 불가능한 상황에 대한 대처 능력이 부족하며, 복잡한 다단계 작업을 자율적으로 수행하는 데 한계를 보여왔다. Gemini Robotics 1.5는 이러한 한계를 극복하기 위한 구체적인 기술적 해법을 제시한다. 따라서 본 보고서의 목표는 이 기술의 핵심 구성요소들을 체계적으로 분해하고, 그 작동 원리와 성능을 객관적인 데이터에 기반하여 평가하며, 나아가 기술의 안전성과 미래 발전 가능성까지 포괄적으로 조망하는 것이다.</p>
<h3>1.3 보고서의 구조</h3>
<p>본 보고서는 총 4개의 장으로 구성된다. 제1장에서는 시스템의 근간을 이루는 이중 모델 아키텍처를 분석하여, 고차원적 추론과 물리적 실행이 어떻게 분리되고 협력하는지 탐구한다. 제2장에서는 ’사고’와 ’학습’을 중심으로 한 핵심 기술 혁신, 즉 ’체화된 사고’와 ‘모션 전이’ 기술의 원리와 의의를 심층적으로 다룬다. 제3장에서는 학술 벤치마크 성능과 구체적인 응용 시연 사례를 통해 모델의 실질적인 역량을 검증하고, 다양한 산업 분야에 미칠 잠재적 영향을 예측한다. 마지막으로 제4장에서는 ASIMOV 벤치마크를 중심으로 기술의 안전성과 책임감 있는 개발 철학을 논하며, 결론에서는 현재의 기술적 한계를 명확히 하고 물리적 범용 인공지능(Physical AGI)을 향한 미래 로드맵을 제시한다.</p>
<h2>2.  Gemini Robotics 1.5의 이중 모델 아키텍처 분석</h2>
<p>Gemini Robotics 1.5 시스템의 핵심은 고도의 인지적 작업을 수행하는 ’두뇌’와 물리적 실행을 담당하는 ’손과 눈’으로 역할을 명확히 분담한 이중 모델 아키텍처에 있다. 이 구조는 인간이 복잡한 문제를 해결하는 방식을 모방하여, 추상적 계획과 구체적 실행을 계층적으로 분리함으로써 시스템 전체의 효율성과 강건성을 극대화한다.</p>
<h3>2.1  시스템의 두뇌: 체화된 추론 모델 (Gemini Robotics-ER 1.5)</h3>
<h4>2.1.1 역할 정의</h4>
<p>Gemini Robotics-ER 1.5 (Embodied Reasoning)는 시스템의 고차원적 ‘두뇌’ 또는 ‘전략 기획자(Strategic Planner)’ 역할을 수행하는 최첨단 비전-언어 모델(VLM)이다.1 이 모델은 로봇의 팔다리를 직접 제어하는 대신, 복잡한 사용자 목표를 이해하고, 논리적인 다단계 계획을 수립하며, 전체 작업 과정을 조율(Orchestrate)하는 데 특화되어 있다.4 즉, 물리적 세계에 대한 깊은 시공간적 이해를 바탕으로 ‘무엇을’, ‘왜’, ‘어떤 순서로’ 해야 하는지에 대한 전략적 결정을 내린다.</p>
<h4>2.1.2 핵심 기능 - 계획 및 추론</h4>
<p>ER 모델의 핵심 역량은 “부엌을 청소하라“와 같이 모호하고 장기적인(long-horizon) 명령을 받았을 때 발휘된다. 모델은 이 추상적인 목표를 ‘카운터 위 물건 치우기’, ‘식기세척기에 그릇 넣기’, ‘표면 닦기’ 등 구체적이고 실행 가능한 하위 단계들로 자동 분해(Task Decomposition)한다.1 이 과정에서 뛰어난 공간-시간적 이해(Spatio-temporal reasoning) 능력을 활용하여, 비디오나 실시간 시각 정보를 처리하며 객체의 현재 위치, 상태 변화, 그리고 특정 행동이 초래할 인과관계를 정확히 파악한다.1 예를 들어, 로봇 팔이 펜을 용기 안으로 옮기는 비디오를 보고 어떤 작업이 어떤 순서로 일어났는지 정확히 기술할 수 있다.1</p>
<h4>2.1.3 핵심 기능 - 도구 사용(Tool Use)</h4>
<p>ER 모델을 이전 세대와 차별화하는 가장 중요한 특징 중 하나는 디지털 도구를 네이티브하게 호출하고 그 결과를 활용하는 능력이다.2 이는 로봇이 사전 학습된 내부 지식의 한계를 넘어, 외부 세계의 최신 정보를 동적으로 활용하여 상황에 맞는 지능적인 결정을 내릴 수 있게 함을 의미한다. 대표적인 시연 사례로, “내 위치를 기준으로 쓰레기를 분리수거하라“는 지시를 받은 로봇은 ER 모델을 통해 Google 검색 도구를 호출하여 샌프란시스코 지역의 최신 폐기물 처리 규정을 실시간으로 조회했다.2 그리고 이 정보를 바탕으로 테이블 위의 물체들을 각각 퇴비, 재활용, 일반 쓰레기로 분류하는 구체적인 계획을 수립했다. 이 기능은 로봇이 고정된 지식 체계에 갇히지 않고, 변화하는 외부 환경과 규정에 적응할 수 있는 진정한 의미의 에이전트로 기능하게 한다.</p>
<h4>2.1.4 기술적 기반</h4>
<p>Gemini Robotics-ER 1.5는 Google의 최신 파운데이션 모델인 Gemini의 강력한 멀티모달 이해 및 추론 능력을 그대로 계승한다.6 여기에 더해, 로보틱스 응용에 필수적인 복잡한 체화된 추론 문제, 즉 작업 계획, 공간 전문성 추론, 과제 진행 상황 추정 등에 최적화되도록 방대한 로봇 데이터와 실제 사용 사례 기반의 벤치마크를 통해 미세 조정되었다.1 그 결과, 15개의 주요 학술 체화된 지능 벤치마크에서 종합적으로 최고 수준(SOTA)의 성능을 달성하며 그 우수성을 입증했다.2</p>
<h3>2.2  시스템의 손과 눈: 비전-언어-행동 모델 (Gemini Robotics 1.5)</h3>
<h4>2.2.1 역할 정의</h4>
<p>Gemini Robotics 1.5는 시스템의 ’손과 눈’에 해당하는 비전-언어-행동(Vision-Language-Action, VLA) 모델이다.4 이 모델의 핵심 임무는 ‘두뇌’ 역할을 하는 ER 모델로부터 받은 자연어 형태의 단계별 지시사항을 정확히 해석하고, 이를 로봇의 물리적 움직임을 직접 제어하는 저수준 모터 명령어(low-level motor commands)로 변환하여 실행하는 것이다.11 VLA 모델은 시각(Vision), 언어(Language), 행동(Action)을 하나의 통합된 모델 안에서 처리함으로써, 인식과 실행 사이의 간극을 최소화한다.</p>
<h4>2.2.2 핵심 기능 - 정교한 조작(Dexterity)</h4>
<p>VLA 모델은 인간에게는 직관적이지만 전통적인 로봇에게는 극도로 어려웠던 정교한 조작(Dexterity) 능력을 구현하는 데 중점을 두고 개발되었다.17 수많은 관절을 동시에 제어하여 미세한 힘 조절과 정확한 움직임을 요구하는 작업, 예를 들어 종이접기, 아이의 도시락 싸기, 지퍼백 잠그기 등과 같은 복잡한 과제를 부드럽고 안정적인 동작으로 수행할 수 있다.9 이는 방대한 양의 로봇 조작 데이터를 학습하여, 다양한 물체와 상황에 대한 섬세한 제어 능력을 내재화했기 때문에 가능하다.</p>
<h4>2.2.3 핵심 기능 - 실시간 상호작용(Interactivity)</h4>
<p>VLA 모델은 단순히 사전에 수립된 계획을 기계적으로 수행하는 것을 넘어, 동적인 실제 환경의 변화에 실시간으로 반응하고 적응하는 높은 수준의 상호작용 능력을 보여준다.9 예를 들어, 시연 중에 작업자가 로봇이 집으려고 하는 물체의 위치를 갑자기 바꾸자, 로봇은 잠시 동작을 멈춘 뒤 변화된 상황을 즉각적으로 인지하고 새로운 경로를 계획하여 작업을 성공적으로 계속했다.19 이는 모델이 일방적인 명령 수행(open-loop)이 아닌, 지속적인 시각적 피드백을 통해 자신의 행동을 능동적으로 조율하는 폐쇄 루프(closed-loop) 제어 시스템으로 작동하고 있음을 명확히 보여준다.</p>
<h3>2.3  두 모델의 상호작용: 에이전틱 프레임워크(Agentic Framework)</h3>
<h4>2.3.1 협력 메커니즘</h4>
<p>두 모델은 단독으로 작동하는 것이 아니라, ’두뇌’와 ’신체’처럼 유기적으로 협력하는 에이전틱 프레임워크(Agentic Framework) 내에서 시너지를 창출한다.1 이 프레임워크에서 정보의 흐름은 계층적이다. 먼저 사용자가 고차원적 목표를 제시하면, ER 모델이 이를 분석하여 ’무엇을(What)’과 ’왜(Why)’에 해당하는 전략적 계획을 수립한다. 이 계획은 자연어 형태의 구체적인 단계별 지시사항으로 변환되어 VLA 모델에 전달된다. 그러면 VLA 모델은 이 지시를 해석하여 ’어떻게(How)’를 실행, 즉 물리적 세계에 실제 행동으로 구현한다.4 이 과정은 단방향이 아니며, VLA 모델이 실행 중 마주하는 환경 변화나 예외 상황은 다시 ER 모델에 피드백되어 계획을 수정하는 동적인 순환 구조를 가진다.</p>
<h4>2.3.2 시너지 효과</h4>
<p>이러한 이중 모델 구조는 각 모델이 자신의 전문 분야에 계산 자원과 모델 용량을 집중하게 함으로써 시스템 전체의 효율성과 강건성(robustness)을 극대화한다. ER 모델은 웹 검색, 다단계 추론과 같이 복잡하고 시간이 소요될 수 있는 인지적 작업에 집중하고, VLA 모델은 빠르고 정교한 실시간 물리적 제어에 집중할 수 있다. 이러한 명확한 역할 분담은, 추상적 계획부터 저수준 모터 제어까지 모든 것을 처리하려 할 때 발생하는 단일 모델의 복잡성과 비효율성 문제를 효과적으로 해결한다. 결과적으로, 이 구조는 단일 모델로는 달성하기 어려운 더 길고 복잡한 작업을 일반화하여 안정적으로 해결할 수 있는 능력을 시스템에 부여한다.2</p>
<h4>2.3.3 비교 분석 테이블</h4>
<p>두 모델의 역할과 기능을 명확히 비교하기 위해 아래 표를 제시한다.</p>
<table><thead><tr><th>기능</th><th>Gemini Robotics-ER 1.5 (VLM - “두뇌”)</th><th>Gemini Robotics 1.5 (VLA - “손과 눈”)</th></tr></thead><tbody>
<tr><td><strong>주요 역할</strong></td><td>고차원적 계획, 오케스트레이션, 추론</td><td>물리적 실행, 직접 제어, 상호작용</td></tr>
<tr><td><strong>핵심 기능</strong></td><td>장기 과제 분해, 공간/시간 이해, 도구 사용</td><td>정교한 조작, 실시간 환경 반응, 모터 제어</td></tr>
<tr><td><strong>입력</strong></td><td>사용자 목표, 시각 데이터, 센서 정보, 도구 결과</td><td>ER 모델의 자연어 지시, 시각 데이터</td></tr>
<tr><td><strong>출력</strong></td><td>자연어 계획, VLA를 위한 단계별 지시</td><td>로봇 액추에이터를 위한 저수준 모터 명령어</td></tr>
<tr><td><strong>핵심 기술</strong></td><td>체화된 추론(Embodied Reasoning)</td><td>모션 전이(Motion Transfer), 체화된 사고</td></tr>
</tbody></table>
<p>이 이중 모델 아키텍처는 단순한 기술적 구현을 넘어, 인간의 계층적 사고 과정을 모방한 인지 아키텍처(Cognitive Architecture)를 의도적으로 설계한 결과물이다. 인간은 “부엌을 청소하라“는 결정을 내릴 때, 팔 근육의 모든 토크 값을 계산하지 않는다. 대신 고차원적인 계획(ER의 역할)을 세운 뒤, 학습된 운동 기술(VLA의 역할)을 사용하여 하위 작업을 실행한다. 이와 같은 ’관심사의 분리(separation of concerns)’는 복잡한 영역에서 확장 가능하고 강건한 지능을 달성하기 위한 근본 원리이다. 단일 모델이 웹 검색부터 관절 토크 제어까지 모든 것을 처리하려 한다면, 이질적인 추상화 수준 사이에서 일반화에 어려움을 겪을 것이다. 그러나 ER과 VLA의 분리는 각 모델이 전문성을 발휘하게 하며, ER 모델이 생성하는 자연어 ’생각’과 계획을 VLA의 실행과 독립적으로 분석할 수 있게 하여 시스템의 해석 가능성과 디버깅 용이성을 크게 향상시킨다. 이는 범용 로봇의 발전이 단순히 더 큰 단일 모델을 만드는 것이 아니라, 전문화된 에이전트들이 효과적으로 협력하는 모듈식 인지 아키텍처를 설계하는 데 달려있을 수 있음을 시사한다.</p>
<h2>3.  핵심 기술 혁신: 사고, 학습, 그리고 일반화</h2>
<p>Gemini Robotics 1.5는 아키텍처뿐만 아니라, 로봇이 ’사고’하고 ’학습’하는 방식에서도 근본적인 혁신을 이루었다. ’체화된 사고’는 로봇에게 행동의 의미와 결과를 예측하는 능력을 부여하며, ’모션 전이’는 로보틱스 분야의 오랜 난제였던 하드웨어 종속성을 극복하는 새로운 길을 제시한다.</p>
<h3>3.1  체화된 사고(Embodied Thinking): 행동 전 사고(Thinking Before Acting) 메커니즘</h3>
<h4>3.1.1 개념 정의</h4>
<p>Gemini Robotics 1.5의 가장 혁신적인 특징 중 하나는 ‘행동 전 사고(Thinking Before Acting)’ 능력이다.2 이는 기존의 VLA 모델들이 사용자의 지시나 계획을 로봇의 움직임으로 직접 변환했던 반응적(reactive) 패러다임에서 벗어나는 중대한 전환이다. Gemini Robotics 1.5는 물리적 행동을 수행하기 전에, 내부적으로 자연어 형태의 다단계 추론 과정(internal sequence of reasoning)을 명시적으로 생성한다.2 이 ‘사고’ 과정은 로봇이 단순히 무엇을 해야 하는지를 아는 것을 넘어, 왜 그 행동을 해야 하는지, 그리고 그 행동이 어떤 결과를 가져올지를 스스로 분석하고 검토하게 한다.</p>
<h4>3.1.2 다단계 추론 과정 분석 (빨래 분류 사례)</h4>
<p>“색깔별로 빨래를 분류하라“는 비교적 간단해 보이는 명령에 대해, 모델이 수행하는 계층적 사고 과정은 이 메커니즘의 깊이를 잘 보여준다.2</p>
<ol>
<li><strong>고차원적 의미론적 이해 (Semantic Understanding)</strong>: 로봇은 먼저 명령의 핵심 의미를 파악한다. 내부적으로 “색깔별 분류는 흰 옷을 흰 통에, 다른 색 옷을 검은 통에 넣는 것을 의미한다“와 같은 추론을 생성한다.2 이는 단어와 픽셀의 패턴 매칭을 넘어, 과업의 목표와 제약 조건을 개념적으로 이해하고 있음을 보여준다.</li>
<li><strong>중간 수준의 단계별 계획 (Step-by-step Planning)</strong>: 의미를 이해한 후, 구체적인 실행 계획을 수립한다. 예를 들어, 시야에 있는 빨간 스웨터를 보고 “따라서, 이 빨간 스웨터를 집어서 검은 통에 넣어야 한다“는 중간 목표를 설정한다.2 이 단계는 장기적인 목표를 현재 상황에 맞는 즉각적인 행동으로 연결하는 역할을 한다.</li>
<li><strong>저차원적 미세 운동 고려 (Fine-motor Consideration)</strong>: 마지막으로, 물리적 실행의 세부 사항을 고려한다. “스웨터를 더 쉽고 안정적으로 집기 위해, 먼저 몸을 스웨터에 더 가까이 움직여야겠다“와 같은 저수준의 전략을 수립한다.2 이는 성공적인 조작을 위한 사전 조건과 물리적 제약을 고려하는 능력이다.</li>
</ol>
<h4>3.1.3 기술적 의의</h4>
<p>이 ‘사고’ 과정은 세 가지 중요한 기술적, 실용적 이점을 제공한다. 첫째, **투명성(Transparency)**을 획기적으로 높인다. 로봇이 자신의 행동 이유를 자연어로 설명할 수 있게 되면서, 개발자와 사용자는 로봇의 의사결정 과정을 직관적으로 이해하고 신뢰할 수 있게 된다.4 둘째, **강건성(Robustness)**을 향상시킨다. 복잡한 문제를 더 작고 관리 가능한 하위 문제로 스스로 분해함으로써, 각 단계의 성공률을 높이고 예상치 못한 문제가 발생했을 때(예: 물건을 놓쳤을 때) “왼손으로 다시 집어야겠다“와 같이 즉각적인 회복 행동을 생성할 수 있다.2 셋째, <strong>일반화(Generalization)</strong> 능력을 촉진한다. 추론 과정을 통해 과업의 근본적인 구조를 학습하게 되므로, 훈련 데이터에서 직접 보지 못한 새로운 과제나 환경에 마주쳤을 때도 학습된 원리를 적용하여 문제를 해결할 가능성이 높아진다.11</p>
<h3>3.2  모션 전이(Motion Transfer): 이종 로봇 간 학습의 구현</h3>
<h4>3.2.1 ’체화 문제(Embodiment Problem)’의 극복</h4>
<p>로보틱스 연구 및 개발의 가장 큰 병목 현상 중 하나는 ’체화 문제(Embodiment Problem)’였다. 이는 특정 로봇의 형태(embodiment), 즉 관절 구성, 센서 종류, 동역학적 특성에 맞춰 학습된 기술이 다른 형태의 로봇에게는 거의 이전되지 않는 현상을 말한다.2 이로 인해 새로운 로봇을 개발할 때마다 방대한 양의 데이터를 처음부터 다시 수집하고 모델을 훈련해야 하는 비효율이 발생했다. Gemini Robotics 1.5는 ’모션 전이(Motion Transfer, MT)’라는 새로운 아키텍처와 학습 메커니즘을 통해 이 고질적인 문제를 정면으로 해결하고자 한다.6</p>
<h4>3.2.2 이종 데이터셋(Heterogeneous Datasets) 기반 학습</h4>
<p>모션 전이 기술의 핵심 전략은 ’다양성’을 통한 ’일반화’이다. 단일 종류의 로봇 데이터에 의존하는 대신, ALOHA(연구용 양팔 로봇), Bi-arm Franka(산업용 양팔 로봇), 그리고 Apptronik의 Apollo(인간형 휴머노이드)와 같이 기구학적 구조, 자유도, 센서 구성이 완전히 다른 여러 로봇으로부터 수집된 이종 데이터(heterogeneous data)를 단일 VLA 모델 학습에 동시에 활용한다.6 이 접근법은 모델이 특정 로봇의 고유한 특성에 과적합(overfitting)되는 것을 방지한다. 대신, 모델은 다양한 ’신체’를 통해 공통적으로 나타나는 움직임과 물리적 상호작용의 근본적인 원리, 즉 ’물리학’에 대한 보다 추상적이고 일반화된 이해를 형성하도록 강제된다.6</p>
<h4>3.2.3 제로샷(Zero-Shot) 기술 전이 시연</h4>
<p>이러한 이종 학습의 결과는 한 로봇에서 학습된 기술이 별도의 미세 조정이나 추가 훈련 없이 다른 형태의 로봇으로 즉시(zero-shot) 전이되는 놀라운 능력으로 나타났다.6 실제 시연에서, 연구팀은 주로 ALOHA 로봇의 데이터로 훈련된 ‘옷장 문 열기’ 기술을 이전에 해당 작업을 한 번도 본 적 없는 Apollo 휴머노이드에게 시켰다. 놀랍게도 Apollo는 처음 보는 환경에서 이 새로운 동작을 성공적으로 수행했다.20 이는 로봇 기술 개발의 패러다임을 바꿀 수 있는 중대한 돌파구이다. 개별 로봇마다 수개월이 걸릴 수 있는 데이터 수집 및 훈련 과정을 생략하고, 하나의 범용 모델이 다양한 하드웨어를 제어할 수 있게 됨으로써 로봇 기술의 개발 및 배포 속도를 획기적으로 가속화할 수 있음을 시사한다.2</p>
<p>모션 전이 기술은 단순히 데이터의 다양성을 늘리는 것을 넘어, 모델이 물리적 과업에 대한 추상적이고 하드웨어에 구애받지 않는 표현(embodiment-agnostic representation)을 학습하도록 강제하는 메커니즘이다. 전통적인 로봇 학습은 특정 로봇의 센서, 관절, 동역학에 특화된 매핑을 학습하기 때문에 취약했다. 그러나 Gemini Robotics 1.5는 물리적으로 다른 여러 로봇의 데이터를 동시에 처리함으로써, 특정 하드웨어에 의존하는 손쉬운 해법을 찾을 수 없게 된다. 모든 로봇 타입에 걸쳐 손실 함수를 최소화하기 위해, 모델은 과업의 공통적이고 근본적인 표현을 찾아야만 한다. 이 표현은 “A 관절을 X도 움직여라“와 같은 저수준 명령이 아니라, “손잡이를 잡고 앞으로 당겨라“와 같은 기능적 목표에 대한 고차원적 개념이어야 한다. 이는 데이터 기반의 단순한 모방 학습에서, 과업의 ’물리학’을 암묵적으로 발견하는 원리 기반 학습으로의 전환을 의미한다. 이는 향후 다양한 하드웨어를 제어할 수 있는 보편적인 ’로봇 파운데이션 모델’의 등장을 예고하며, 새로운 응용 분야와 환경으로 로봇 기술을 확장하는 데 결정적인 역할을 할 것이다.20</p>
<h2>4.  성능 평가 및 응용 사례 분석</h2>
<p>Gemini Robotics 1.5의 혁신성은 객관적인 벤치마크 성능과 실제 시연 과제를 통해 구체적으로 입증된다. 이 장에서는 모델의 정량적 성능을 평가하고, 주요 시연 사례를 심층 분석하여 그 능력의 실체와 잠재적 응용 분야를 탐구한다.</p>
<h3>4.1  학술 및 내부 벤치마크 성능</h3>
<h4>4.1.1 체화된 추론(ER) 벤치마크 SOTA 달성</h4>
<p>Gemini Robotics-ER 1.5 모델은 로봇의 지능을 평가하는 데 있어 핵심적인 ‘체화된 추론’ 능력에서 전례 없는 성과를 보였다. 이 모델은 Point-Bench (객체 포인팅), RefSpatial (공간 관계 이해), ERQA (체화된 질의응답) 등을 포함한 총 15개의 권위 있는 학술 벤치마크에서 종합적으로 최고 성능(State-of-the-Art, SOTA)을 달성했다.2 이는 로봇에게 필수적인 공간 추론, 시각 정보에 기반한 객체 참조 이해, 그리고 행동 계획 능력이 현재 학계에 알려진 최고 수준에 도달했음을 객관적으로 입증하는 결과다.2</p>
<h4>4.1.2 정량적 성능 향상</h4>
<p>성능 향상은 단순히 순위 상승에 그치지 않고, 정량적인 수치에서도 질적인 도약을 보여준다. 이전 세대 모델인 Gemini 2.0 Flash와 비교했을 때, 특히 공간적 이해를 요구하는 작업에서의 성능 향상이 두드러진다. 예를 들어, 이미지 내 특정 지점을 정확히 찾아내는 Pixmo-Point 벤치마크에서의 포인팅 정확도는 25.8%에서 49.5%로 거의 두 배 가까이 급증했다. 또한, 3차원 공간에서 객체를 탐지하는 SUN-RGBD 벤치마크 성능 역시 30.7%에서 48.3%로 크게 향상되었다.8 이러한 수치는 점진적인 개선이 아니라, 모델이 물리적 세계를 인식하고 이해하는 방식에 근본적인 변화가 있었음을 시사하며, 이는 로봇이 더 신뢰성 있게 작업을 수행할 수 있는 기반이 된다.</p>
<h4>4.1.3 유연한 ‘사고 예산(Thinking Budget)’</h4>
<p>Gemini Robotics-ER 1.5는 성능과 효율성 사이의 균형을 맞추기 위한 독창적인 메커니즘을 제공한다. 모델의 추론 성능은 추론 시간(inference-time)에 할당되는 ’사고 토큰 예산(thinking token budget)’에 따라 동적으로 확장된다.1 이는 개발자가 애플리케이션의 요구사항에 맞춰 모델의 행동을 조절할 수 있게 한다. 예를 들어, 즉각적인 반응이 중요한 간단한 객체 탐지 작업에는 낮은 예산을 할당하여 지연 시간(latency)을 최소화하고, 복잡한 다단계 계획이나 정밀한 추론이 필요한 작업에는 높은 예산을 할당하여 정확도를 극대화할 수 있다.1 이 유연성은 다양한 실제 시나리오에 모델을 효과적으로 적용할 수 있게 하는 중요한 실용적 특징이다.</p>
<h3>4.2  주요 시연 과제 심층 분석</h3>
<h4>4.2.1 사례 1: 상황인지형 쓰레기 분리수거</h4>
<p>이 시연은 Gemini Robotics 1.5가 단순한 명령 수행자를 넘어, 능동적인 문제 해결 에이전트로서 기능할 수 있음을 명확히 보여준다. 로봇은 “샌프란시스코 규정에 따라 쓰레기를 분류하라“는 고차원적인 지시를 받았다.1 이에 ER 모델은 즉시 웹 검색 도구를 호출하여 해당 지역의 최신 재활용 규정을 학습했다. 이 정보를 바탕으로 테이블 위에 놓인 바나나 껍질, 플라스틱 병, 종이컵 등을 각각 퇴비(compost), 재활용(recycling), 일반 쓰레기(trash)로 분류하는 계획을 수립했다. 그 후, VLA 모델이 이 계획에 따라 각 물체를 정확한 통에 집어넣는 물리적 작업을 수행했다. 이 사례는 <strong>외부 정보 획득(도구 사용), 장기 계획 수립, 그리고 정교한 물리적 실행</strong>이 하나의 통합된 프로세스로 완벽하게 결합된 에이전트의 능력을 보여주는 대표적인 예이다.</p>
<h4>4.2.2 사례 2: 다단계 추론 기반 빨래 분류</h4>
<p>“색깔별로 빨래를 분류하라“는 지시에 대해, 로봇은 단순히 흰색 옷과 유색 옷을 구분하여 다른 통에 넣는 작업을 수행하는 것을 넘어, 자신의 <strong>‘사고’ 과정을 자연어로 설명</strong>했다.2 로봇은 “색깔별 분류는 흰 옷을 흰 통에, 유색 옷을 검은 통에 넣는 것을 의미하므로, 이 빨간 스웨터를 집어 검은 통에 넣겠습니다“와 같이 자신의 의도를 명확히 밝혔다.5 이는 로봇의 행동이 단순한 시각적 패턴 매칭의 결과가 아니라, 과업의 의미를 개념적으로 이해하고 그에 기반한 논리적 추론을 통해 도출되었음을 시사한다. 이러한 투명성은 인간과 로봇 간의 신뢰를 구축하는 데 결정적인 역할을 한다.</p>
<h4>4.2.3 사례 3: 동적 환경 변화에 대한 실시간 대응</h4>
<p>작업의 강건성을 시험하기 위한 시연에서, 작업자는 로봇이 특정 물체를 집으려는 순간 그 위치를 갑자기 옆으로 옮겼다.19 기존의 많은 로봇 시스템은 이러한 예외 상황에서 오류를 일으키거나 작업을 중단했을 것이다. 하지만 Gemini Robotics 1.5는 잠시 동작을 멈춘 뒤, 바뀐 물체의 위치를 즉각적으로 재인식하고 새로운 접근 경로를 실시간으로 계획하여 작업을 성공적으로 계속했다. 이는 모델이 사전에 정해진 경로를 맹목적으로 따르는 <strong>개방 루프(open-loop) 제어</strong> 방식이 아니라, 지속적인 시각적 피드백을 통해 자신의 상태와 환경을 비교하며 행동을 수정하는 고도의 <strong>폐쇄 루프(closed-loop) 제어</strong>를 수행하고 있음을 증명한다.</p>
<h3>4.3  잠재적 응용 분야 및 산업적 영향</h3>
<h4>4.3.1 제조 및 물류</h4>
<p>Motion Transfer 기술은 다양한 종류의 로봇 팔과 그리퍼가 혼재하는 현대의 제조 및 물류 현장에 혁신을 가져올 수 있다. 새로운 제품 라인이나 작업 공정을 도입할 때, 각기 다른 로봇을 개별적으로 프로그래밍하거나 재훈련하는 데 드는 막대한 시간과 비용을 획기적으로 줄일 수 있다.4 또한, 모델의 정교한 조작(Dexterity) 능력은 복잡한 부품 조립, 품질 검사, 또는 다양한 형태의 상품을 다루는 이커머스 물류센터의 피킹 및 포장 작업에 직접적으로 적용될 수 있다.20</p>
<h4>4.3.2 헬스케어 및 가정 환경</h4>
<p>노인 돌봄, 재활 보조, 가사 지원과 같이 비정형적이고 예측 불가능한 인간 중심의 환경은 Gemini Robotics 1.5의 일반화 및 적응 능력이 가장 큰 잠재력을 발휘할 수 있는 분야이다.4 “부엌 정리”, “점심 도시락 싸기”, “약 챙겨주기“와 같은 시연 과제들은 로봇이 일상 생활 공간에서 인간과 안전하게 상호작용하며 실질적인 도움을 줄 수 있는 가정용 로봇의 실현 가능성을 한 단계 끌어올린다.4</p>
<h4>4.3.3 산업적 파급 효과</h4>
<p>Gemini Robotics 1.5 기술은 로봇의 역할을 근본적으로 변화시킬 잠재력을 지닌다. 지금까지 로봇은 특정 작업을 효율적으로 수행하기 위해 고도로 통제된 환경에 배치되는 ’도구’에 가까웠다. 그러나 이 기술은 로봇을 다양한 문제를 스스로 이해하고 해결책을 찾아 실행할 수 있는 지능적인 ’파트너’로 전환시킨다.5 이는 로봇 도입의 기술적, 경제적 장벽을 낮추어 대기업뿐만 아니라 중소기업에서도 자동화의 혜택을 누릴 수 있게 할 것이다. 궁극적으로, 지금까지는 복잡성과 비정형성 때문에 자동화가 불가능하다고 여겨졌던 농업, 건설, 서비스업 등 새로운 분야로 로봇의 활용 범위를 폭넓게 확장시키는 기폭제가 될 것이다.</p>
<h2>5.  안전성 및 책임감 있는 AI 개발</h2>
<p>범용 로봇이 실제 세계에서 작동하기 위해서는 기술적 성능만큼이나 안전성과 신뢰성이 중요하다. Google DeepMind는 Gemini Robotics 1.5를 개발하며, 기술의 잠재적 위험을 식별하고 완화하기 위한 체계적인 접근법을 채택했다. 이는 엄격한 벤치마크를 통한 평가와 다층적인 안전 프레임워크 구축을 포함한다.</p>
<h3>5.1  ASIMOV 벤치마크를 통한 의미론적 안전성 평가</h3>
<h4>5.1.1 안전성 평가</h4>
<p>Google DeepMind는 Gemini Robotics 모델의 안전성을 객관적으로 평가하고 개선하기 위한 핵심 도구로 ASIMOV 벤치마크를 적극적으로 활용하고, 더 나아가 새로운 안전 시나리오와 데이터 유형을 포함하여 벤치마크 자체를 업그레이드했다.1 ASIMOV 벤치마크는 로봇이 단순히 물리적 충돌을 피하는 수준을 넘어, 주어진 지시가 내포하는 의미를 이해하고 그것이 위험하거나(dangerous), 유해하거나(harmful), 또는 비윤리적인 경우 이를 인지하고 실행을 거부하는 능력, 즉 **의미론적 안전성(Semantic Safety)**을 측정하는 데 중점을 둔다.1</p>
<h4>5.1.2 ‘사고’ 능력과 안전성의 연관성</h4>
<p>Gemini Robotics-ER 1.5는 이 ASIMOV 벤치마크에서 최고 수준(SOTA)의 성능을 보였다. 특히 주목할 점은, 모델의 핵심 기능인 ‘행동 전 사고’ 능력이 의미론적 안전성을 이해하고 물리적 제약 조건(예: 로봇이 들 수 있는 최대 무게를 초과하는 물건을 들라는 지시)을 준수하는 데 결정적으로 기여했다는 분석 결과이다.2 이는 안전성이 단순히 유해 단어를 필터링하는 별도의 모듈에 의해 부가적으로 구현되는 것이 아니라, 모델의 핵심적인 추론 능력 자체에 깊숙이 내재되어 있음을 시사한다. 로봇이 행동의 결과를 미리 ’생각’하고 시뮬레이션함으로써 잠재적인 위험을 사전에 식별하고 회피할 수 있게 되는 것이다.</p>
<h3>5.2  다층적 안전 프레임워크</h3>
<h4>5.2.1 종합적 접근</h4>
<p>Google은 단일 벤치마크 점수에 의존하는 대신, 실제 환경의 복잡성과 예측 불가능성을 고려한 다층적이고 종합적인 안전 프레임워크를 개발하고 있다. 이 프레임워크는 의미론적, 물리적, 운영적 안전 장치를 여러 겹으로 쌓아 올리는 것을 목표로 한다.10 이는 흔히 ’스위스 치즈 모델’에 비유되는데, 각 안전 장치(치즈 슬라이스)에는 개별적인 구멍(결함)이 있을 수 있지만, 여러 겹을 겹침으로써 전체 시스템의 위험이 구멍을 통과하여 사고로 이어질 확률을 최소화하는 개념이다.10</p>
<h4>5.2.2 프레임워크 구성 요소</h4>
<p>이 다층적 프레임워크는 다음과 같은 핵심 요소들로 구성된다:</p>
<ol>
<li><strong>의미론적 안전 (Semantic Safety)</strong>: 모델이 위험하거나 부적절한 지시를 이해하고 거부하는 능력. 이는 ASIMOV 벤치마크를 통해 지속적으로 평가 및 강화된다.1</li>
<li><strong>물리적 안전 (Physical Safety)</strong>: 충돌 회피 센서, 토크 제한, 비상 정지 버튼 등 로봇 하드웨어에 내장된 저수준 안전 하위 시스템과 VLA 모델을 긴밀하게 연동하여, 예측하지 못한 모델의 행동이 물리적 사고로 이어지는 것을 방지한다.2</li>
<li><strong>인간-로봇 상호작용 (Human-Robot Interaction, HRI)</strong>: 로봇이 인간과 소통하고 협력하는 과정에서 발생할 수 있는 사회적, 심리적 위험을 최소화한다. 이는 Gemini의 기존 안전 정책에 부합하도록, 존중하고 명확한 언어와 행동을 생성하도록 모델을 정렬(align)하는 것을 포함한다.2</li>
<li><strong>지속적인 취약성 평가 (Continuous Vulnerability Assessment)</strong>: 시스템의 잠재적인 허점이나 예상치 못한 실패 모드를 능동적으로 찾기 위해, 내부 전문가들로 구성된 팀이 지속적인 레드팀(red-teaming) 및 적대적 평가를 수행한다. 이를 통해 발견된 취약점은 즉시 모델 개선에 반영된다.10</li>
</ol>
<p>Gemini Robotics 1.5의 안전성 접근법에서 가장 심오한 측면은, 안전을 외부에서 가해지는 제약이나 후처리 필터가 아닌, 고도화된 추론 능력에서 자연스럽게 발현되는 속성(emergent property)으로 간주한다는 점이다. 행동의 결과를 미리 ’생각’하는 능력 자체가 시스템을 본질적으로 더 안전하게 만든다. 전통적인 로봇 안전 장치는 충돌 센서와 같이 반응적이거나, 지오펜싱처럼 사전에 정의된 엄격한 규칙에 기반했다. 이러한 방식은 “아기에게 이 칼을 건네줘“와 같이 새로운 의미론적 위험에 직면했을 때 실패하기 쉽다. 그러나 Gemini Robotics 1.5는 ’생각’하는 과정을 통해 일종의 내부 시뮬레이션과 결과 분석을 수행한다. 칼은 날카롭다는 객체의 속성, 아기는 취약하다는 행위자의 상태, 그리고 그 행동의 가장 가능성 있는 결과를 종합적으로 추론할 수 있다. 이는 원시 센서 데이터만으로는 즉각적으로 파악하기 어려운 위험을 식별하게 해준다. 이러한 접근법은 AI 안전에 대한 패러다임의 전환을 예고한다. 즉, 명시적인 규칙(“인간을 만지지 마시오”)을 프로그래밍하는 것에서 벗어나, 풍부한 세계 모델과 인과 관계 추론 능력을 시스템에 부여하여 스스로 안전한 결정을 내리게 하는 방향으로 나아가는 것이다. 이는 예측 불가능한 인간 환경에 자율 에이전트를 배포하기 위해 필수적인, 보다 강건하고 일반화 가능한 안전 접근법이라 할 수 있다.</p>
<h2>6. 결론: 현재의 한계와 물리적 AGI를 향한 로드맵</h2>
<p>Gemini Robotics 1.5는 물리적 세계와 상호작용하는 지능형 에이전트 개발에 있어 기념비적인 진전을 이루었지만, 범용 로봇의 광범위한 보급을 위해서는 여전히 해결해야 할 명확한 한계와 과제들이 존재한다.</p>
<h3>6.1 현재 기술의 명확한 한계점</h3>
<ul>
<li><strong>정교한 조작의 한계</strong>: 종이접기와 같은 인상적인 시연에도 불구하고, 시스템은 여전히 천, 케이블, 액체와 같이 형태가 일정하지 않고 유연하며 예측 불가능한 물체를 다루는 데에는 상당한 어려움을 겪는다.5 이러한 물체와의 안정적인 상호작용은 미래 로봇 기술의 핵심 과제로 남아있다.</li>
<li><strong>예측 불가능한 환경에서의 강건성</strong>: 연구실이나 통제된 시연 환경을 벗어나, 실제 가정이나 공공장소와 같이 조명 조건이 급격히 변하거나, 예상치 못한 장애물이 많고, 아이나 애완동물이 갑자기 나타나는 극도로 동적이고 예측 불가능한 환경에서의 장기적인 강건성은 아직 완전히 검증되지 않았다.5</li>
<li><strong>비용 및 확장성 문제</strong>: 현재 Gemini Robotics 1.5의 이중 모델 아키텍처를 실시간으로 구동하기 위해서는 상당한 계산 자원이 필요하다. 이러한 높은 계산 요구사항과 고가의 로봇 하드웨어는 당분간 기술의 광범위한 보급에 큰 경제적 장벽으로 작용할 것이다.5 또한, 현재 Gemini Robotics-ER 1.5는 개발자 프리뷰로 제한적으로 제공되며, VLA 모델은 일부 선정된 파트너에게만 제공되는 등 기술에 대한 접근성이 매우 제한적이다.1</li>
</ul>
<h3>6.2 향후 연구 개발 방향</h3>
<ul>
<li><strong>개발자 생태계 확장</strong>: Google DeepMind는 더 많은 개발자와 연구자들이 이 기술을 활용하여 새로운 애플리케이션을 만들 수 있도록 API 및 SDK(소프트웨어 개발 키트)를 지속적으로 개선하고 접근성을 확대할 계획이다. 특히, 인터넷 연결 없이도 로봇이 독립적으로 작동할 수 있도록 하는 온디바이스(On-Device)용 경량화 모델 개발은 핵심적인 연구 방향 중 하나이다.2</li>
<li><strong>산업별 파일럿 프로젝트</strong>: 기술의 실질적인 유용성과 한계를 검증하기 위해, 제조, 물류, 헬스케어 등 특정 산업 분야의 선도적인 파트너들과 협력하여 실제 산업 현장에서의 파일럿 프로젝트를 진행할 것이다.22 이러한 현장 적용을 통해 얻은 피드백은 모델을 더욱 강건하고 실용적으로 만드는 데 중요한 자양분이 될 것이다.</li>
<li><strong>인간으로부터의 학습</strong>: 현재 모델은 주로 인간이 원격으로 조종한 데이터를 통해 학습하지만, 장기적으로는 로봇이 인간의 행동을 직접 관찰하여 새로운 기술을 배우는(learning from observation) 능력을 갖추는 것이 중요하다. 이는 데이터 수집의 병목 현상을 해결하고 로봇이 보다 직관적이고 자연스럽게 기술을 습득하게 하는 핵심 연구 과제로 남아있다.21</li>
</ul>
<h3>6.3 물리적 AGI를 향한 이정표</h3>
<p>Gemini Robotics 1.5는 그 자체로 완전한 범용 인공지능(AGI)은 아니다. 그러나 이 기술은 디지털 세계에 머물러 있던 AI의 추상적 추론 능력을 물리적 세계의 인식 및 행동과 성공적으로 결합시켰다는 점에서 중대한 의의를 가진다. 이는 물리적 세계를 진정한 지능의 궁극적인 시험대로 삼는 ‘물리적 AGI(Physical AGI)’ 연구의 중요한 이정표를 세운 것으로 평가할 수 있다.4 기계가 단순히 정보를 처리하는 것을 넘어, 인간처럼 실제 세계를 이해하고, 상호작용하며, 그 안에서 유용한 작업을 수행하는 미래를 향한 구체적이고 의미 있는 발걸음인 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Building the Next Generation of Physical Agents with Gemini Robotics-ER 1.5, https://developers.googleblog.com/en/building-the-next-generation-of-physical-agents-with-gemini-robotics-er-15/</li>
<li>Gemini Robotics 1.5 brings AI agents into the physical world - Google DeepMind, https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/</li>
<li>Google AI - How we’re making AI helpful for everyone, https://ai.google/</li>
<li>Robots That Reason: Google’s Gemini 1.5 Raises the Bar - eWeek, https://www.eweek.com/news/google-gemini-robotics-1-5-er-1-5-launch/</li>
<li>From Tools to Partners: Google’s Gemini Robotics 1.5 Moves Beyond Automation Towards True Intelligence - Greg Robison, https://gregrobison.medium.com/from-tools-to-partners-googles-gemini-robotics-1-5-640c3828129e</li>
<li>Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer - Googleapis.com, https://storage.googleapis.com/deepmind-media/gemini-robotics/Gemini-Robotics-1-5-Tech-Report.pdf</li>
<li>[2510.03342] Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer - arXiv, https://arxiv.org/abs/2510.03342</li>
<li>Gemini Robotics 1.5: Robots That Think Before Acting | by Cogni Down Under - Medium, https://medium.com/@cognidownunder/gemini-robotics-1-5-robots-that-think-before-acting-a8f97816caaf</li>
<li>Gemini Robotics 1.5 - Google DeepMind, https://deepmind.google/models/gemini-robotics/gemini-robotics/</li>
<li>Responsibly advancing AI and robotics - Google DeepMind, https://deepmind.google/models/gemini-robotics/responsibly-advancing-ai-and-robotics/</li>
<li>Gemini Robotics 1.5 enables agentic experiences, explains Google DeepMind, https://www.therobotreport.com/gemini-robotics-1-5-enables-agentic-experiences-explains-google-deepmind/</li>
<li>Gemini Robotics 1.5: Using agentic capabilities - YouTube, https://www.youtube.com/watch?v=AMRxbIO04kQ</li>
<li>How Gemini Robotics 1.5 Enables robots to plan, think and use tools to solve complex tasks, https://www.franksworld.com/2025/09/26/how-gemini-robotics-1-5-enables-robots-to-plan-think-and-use-tools-to-solve-complex-tasks/</li>
<li>How the Gemini Robotics family translates foundational intelligence into physical action, https://levelup.gitconnected.com/how-the-gemini-robotics-family-translates-foundational-intelligence-into-physical-action-87df6bee02c5</li>
<li>Google DeepMind adds agentic AI models to robots - Silicon Republic, https://www.siliconrepublic.com/machines/google-deepmind-robotics-ai-gemini-agentic</li>
<li>New Gemini Model Spotted: gemini-robotics-er-1.5-preview : r/Bard - Reddit, https://www.reddit.com/r/Bard/comments/1nqb20e/new_gemini_model_spotted_geminiroboticser15preview/</li>
<li>Gemini Robotics brings AI into the physical world - Google DeepMind, https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/</li>
<li>How we built the new family of Gemini Robotics models - Google Blog, https://blog.google/products/gemini/how-we-built-gemini-robotics/</li>
<li>Gemini Robotics 1.5: Thinking while acting - YouTube, https://www.youtube.com/watch?v=eDyXEh8XqjM</li>
<li>Gemini Robotics 1.5: Learning across embodiments - YouTube, https://www.youtube.com/watch?v=9FV5ZYytkOQ</li>
<li>Google DeepMind’s new Gemini robots adapt to complex tasks like laundry sorting and recycling - Cosmico, https://www.cosmico.org/google-deepminds-new-gemini-robots-adapt-to-complex-tasks-like-laundry-sorting-and-recycling/</li>
<li>Gemini Robotics 1.5: Bringing AI Agents to the Real World - Linkdood Technologies, https://linkdood.com/gemini-robotics-1-5-bringing-ai-agents-to-the-real-world/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>