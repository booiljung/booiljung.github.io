<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Compressor-VLA 효율적 로봇 조작을 위한 명령어 유도 시각 토큰 압축 프레임워크</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Compressor-VLA 효율적 로봇 조작을 위한 명령어 유도 시각 토큰 압축 프레임워크</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">Vision-Language-Action(VLA) 모델</a> / <span>Compressor-VLA 효율적 로봇 조작을 위한 명령어 유도 시각 토큰 압축 프레임워크</span></nav>
                </div>
            </header>
            <article>
                <h1>Compressor-VLA 효율적 로봇 조작을 위한 명령어 유도 시각 토큰 압축 프레임워크</h1>
<p>2025-12-07, G30DR</p>
<h2>1.  서론: 체화된 인공지능과 효율성의 딜레마</h2>
<h3>1.1  체화된 AI(Embodied AI)의 패러다임 전환</h3>
<p>현대 인공지능 연구의 최전선은 가상 공간의 데이터 처리를 넘어, 물리적 세계와 상호작용하는 체화된 AI(Embodied AI)로 급격히 이동하고 있다. 이러한 흐름의 중심에는 시각-언어-행동(Vision-Language-Action, VLA) 모델이 존재한다. VLA 모델은 대규모 언어 모델(LLM)의 추론 능력과 시각 모델(Vision Encoder)의 인식 능력을 결합하여, 로봇이 비정형 환경에서도 인간의 자연어 명령을 이해하고 복잡한 물리적 작업을 수행할 수 있도록 돕는다. 이는 과거의 규칙 기반(Rule-based) 제어 시스템이나 제한된 상태 공간에서의 강화 학습(RL)이 가졌던 확장성의 한계를 극복하는 혁신적인 접근법으로 평가받는다.1</p>
<p>특히 OpenVLA와 같은 최신 오픈소스 VLA 모델들은 인터넷 규모의 방대한 데이터셋을 통해 학습된 시각적 상식과 언어적 이해력을 바탕으로, 처음 마주하는 물체나 환경에서도 일반화된 조작(Manipulation) 능력을 보여주었다. 이는 로봇 공학의 오랜 난제였던 ‘일반화(Generalization)’ 문제에 대한 실마리를 제공하며, 가정용 서비스 로봇이나 다목적 산업용 로봇의 상용화 가능성을 높이고 있다.1</p>
<h3>1.2  VLA 모델의 연산 병목과 실시간성의 위기</h3>
<p>그러나 VLA 모델의 뛰어난 잠재력 이면에는 심각한 기술적 장벽이 존재한다. 바로 막대한 연산 비용(Computational Overhead)과 그로 인한 추론 지연(Inference Latency)이다. VLA 모델은 통상적으로 수십억 개 이상의 파라미터를 가진 거대 모델이며, 고해상도 이미지를 처리하기 위해 수백, 수천 개의 시각 토큰(Visual Tokens)을 입력으로 받아들인다. Transformer 아키텍처의 핵심인 어텐션(Attention) 메커니즘은 입력 토큰 수의 제곱(<span class="math math-inline">O(N^2)</span>)에 비례하여 연산량이 증가하는 특성을 가진다. 따라서 시각 토큰의 수가 많아질수록 모델의 추론 속도는 기하급수적으로 느려지게 된다.4</p>
<p>로봇 조작 작업은 정적인 이미지 분류와는 달리 고도의 실시간성(Real-time capability)을 요구한다. 로봇 팔이 물체를 잡거나 이동시키는 과정에서, 시각 인식과 행동 생성 사이의 지연은 곧 물리적 충돌, 파지 실패, 혹은 불안정한 제어 루프로 이어진다. 기존의 고성능 GPU 서버 환경에서는 이러한 지연이 어느 정도 감내될 수 있으나, 배터리와 전력, 쿨링 제한이 있는 로봇의 온보드(On-board) 컴퓨터 환경에서는 VLA 모델을 실시간으로 구동하는 것이 거의 불가능에 가까운 도전 과제가 된다.6</p>
<h3>1.3  시각적 중복성과 비효율적인 토큰 처리</h3>
<p>이러한 연산 비효율성의 근본적인 원인은 ’시각 정보의 중복성(Visual Redundancy)’에 있다. 텍스트 정보는 각 단어가 높은 정보 밀도를 가지는 반면, 이미지 정보는 픽셀 간의 상관관계가 높고 중복된 정보가 많다. 특히 로봇이 특정 작업을 수행할 때, 입력된 이미지의 전체 영역 중 실제 작업과 관련된 영역은 극히 일부분에 불과하다. 예를 들어 “식탁 위의 사과를 집어라“라는 명령을 수행할 때, 배경에 있는 벽지, 바닥의 무늬, 창문 밖의 풍경 등은 작업 수행에 아무런 영향을 주지 않는 정보이다. 그러나 기존의 VLA 모델들은 이러한 배경 정보까지 모두 토큰화하여 LLM의 연산 과정에 포함시키므로, 불필요한 자원을 낭비하게 된다.4</p>
<p>기존의 연구들은 이러한 문제를 해결하기 위해 토큰 가지치기(Token Pruning)나 병합(Merging) 기법을 도입했으나, 대다수가 ’작업 불가지론적(Task-Agnostic)’인 접근을 취했다. 즉, 현재 로봇이 어떤 명령을 수행 중인지를 고려하지 않고, 단순히 이미지 내의 정보량이나 정적인 중요도만을 기준으로 토큰을 제거했다. 이는 로봇 조작에 필수적인 미세한 공간 정보나 맥락 정보를 훼손하여 작업 성공률을 치명적으로 떨어뜨리는 결과를 초래했다.5</p>
<h3>1.4  연구의 목적 및 보고서 구성</h3>
<p>본 보고서는 이러한 VLA 모델의 효율성 문제를 해결하기 위해 베이징 공업대학교(Beijing University of Technology)와 LiAuto Inc.의 연구진이 제안한 <strong>Compressor-VLA</strong> 모델을 심층적으로 분석한다.4 Compressor-VLA는 자연어 명령어에 기반하여 시각 토큰을 동적으로 압축하는 새로운 하이브리드 프레임워크로, 불필요한 시각 정보를 과감히 제거하면서도 작업 수행에 필요한 핵심 정보는 정밀하게 보존하는 방법론을 제시한다.</p>
<p>본 보고서는 다음과 같은 구조로 전개된다.</p>
<ol>
<li><strong>기술적 배경:</strong> VLA 모델의 아키텍처적 특성과 기존 토큰 압축 기법의 한계를 분석한다.</li>
<li><strong>Compressor-VLA 방법론:</strong> 핵심 기술인 의미적 작업 압축기(STC)와 공간 정제 압축기(SRC)의 작동 원리를 해부한다.</li>
<li><strong>실험 결과 분석:</strong> LIBERO 벤치마크를 통한 정량적 성능 평가와 경쟁 모델과의 비교 분석을 수행한다.</li>
<li><strong>정성적 분석 및 실증:</strong> 어텐션 시각화와 실제 로봇 배포 결과를 통해 모델의 작동 기제를 검증한다.</li>
<li><strong>결론 및 시사점:</strong> 본 연구가 로봇 공학 및 AI 분야에 미치는 파급 효과를 논의한다.</li>
</ol>
<h2>2.  기술적 배경 및 관련 연구의 한계</h2>
<h3>2.1  VLA 모델의 구조적 특성과 병목</h3>
<p>VLA(Vision-Language-Action) 모델은 일반적으로 시각 인코더(Vision Encoder)와 대형 언어 모델(LLM)이 결합된 형태를 띤다.</p>
<ul>
<li><strong>시각 인코더:</strong> 입력된 RGB 이미지를 고정된 크기의 패치(Patch)로 분할하고, 이를 임베딩 벡터로 변환하여 시각 토큰 시퀀스를 생성한다. 대표적으로 CLIP이나 SigLIP과 같은 모델이 사용된다.1</li>
<li><strong>프로젝터 및 LLM:</strong> 시각 토큰은 프로젝터(Projector)를 거쳐 LLM의 입력 차원에 맞춰지고, 텍스트 토큰과 함께 LLM에 입력된다. LLM은 이 멀티모달 입력을 처리하여 로봇의 다음 행동(End-effector의 위치, 회전, 그리퍼 개폐 등)을 예측하는 토큰을 생성한다.</li>
</ul>
<p>이 과정에서 LLM의 Self-Attention 연산은 전체 입력 시퀀스 길이의 제곱에 비례하는 연산량을 요구한다. 일반적으로 이미지는 텍스트보다 훨씬 많은 수의 토큰(예: 256개~576개)을 생성하므로, 전체 연산 비용의 대부분이 시각 토큰 처리에 소모된다. 따라서 시각 토큰의 수를 줄이는 것은 VLA 모델 가속화의 가장 직접적이고 효과적인 방법이다.4</p>
<h3>2.2  기존 효율화 기법의 한계: 작업 불가지론적 압축의 위험성</h3>
<p>VLA 모델의 효율성을 높이기 위해 다양한 토큰 압축 기법이 제안되었으나, 로봇 조작이라는 특수성을 충분히 반영하지 못했다.</p>
<h4>2.2.1  단순 가지치기 (Pruning)</h4>
<p>FastV7나 SparseVLM7과 같은 방식은 어텐션 점수(Attention Score)가 낮은 토큰을 덜 중요한 정보로 간주하고 제거한다. 이는 일반적인 이미지 분류나 캡셔닝 작업에서는 효과적일 수 있으나, 로봇 조작에서는 위험하다.</p>
<ul>
<li><strong>공간 정보 손실:</strong> 로봇이 물체를 잡기 위해서는 물체의 가장자리나 손잡이와 같은 미세한 기하학적 정보가 필수적이다. 이러한 정보가 담긴 토큰이 전체적인 어텐션 점수가 낮다는 이유로 제거되면, 로봇은 물체의 위치를 정확히 파악하지 못하고 허공을 잡거나 물체를 넘어뜨릴 수 있다.</li>
<li><strong>명령어 무시:</strong> “파란색 컵“을 집으라는 명령이 있을 때, 화면 구석에 있는 파란색 컵은 픽셀 수나 주목도 면에서 미미할 수 있다. 작업 불가지론적 가지치기는 이러한 ’작업 관련성’을 고려하지 않으므로 핵심 객체를 배경으로 오인하여 제거할 위험이 있다.4</li>
</ul>
<h4>2.2.2  정적 캐싱 (Static Caching)</h4>
<p>VLA-Cache5와 같은 방식은 연속된 프레임 간의 변화가 적은 배경 영역의 토큰을 캐싱하여 재사용한다.</p>
<ul>
<li><strong>동적 환경 대응 불가:</strong> 로봇 팔이 움직이거나 카메라 시점이 변하는 동적인 환경에서는 배경과 전경이 수시로 바뀐다. 정적인 캐싱은 이러한 변화에 유연하게 대처하지 못하며, 시각적 오류(Visual Artifacts)를 유발할 수 있다.</li>
</ul>
<h4>2.2.3  요약 (Summarization)</h4>
<p>전체 이미지를 몇 개의 대표 토큰으로 요약하는 방식은 정보의 밀도를 높일 수 있지만, 로봇 제어에 필요한 ’공간적 정밀도(Spatial Granularity)’를 잃어버릴 수 있다. “어디로(Where)” 이동해야 하는지에 대한 좌표 정보는 압축된 요약 벡터에서는 복원하기 어렵기 때문이다.</p>
<p>이러한 배경 속에서, Compressor-VLA는 **“어떤 토큰을 남길 것인가?”**라는 질문에 대해 **“명령어와 관련된 토큰을 남긴다”**라는 명확한 답을 제시하며 등장했다.</p>
<h2>3.  Compressor-VLA 방법론: 명령어 유도 하이브리드 압축</h2>
<p>Compressor-VLA의 핵심 철학은 시각 정보 처리를 인간의 인지 과정과 유사하게 만드는 것이다. 인간은 “컵을 집으라“는 명령을 들으면 시각적 주의(Visual Attention)를 컵과 손의 위치에 집중하고, 나머지 배경 정보는 무시한다. Compressor-VLA는 이를 구현하기 위해 **의미적 작업 압축기(Semantic Task Compressor, STC)**와 **공간 정제 압축기(Spatial Refinement Compressor, SRC)**라는 두 가지 모듈을 병렬적으로 배치하고, 이를 자연어 명령어로 제어하는 구조를 채택했다.4</p>
<h3>3.1  전체 아키텍처 및 데이터 흐름 (Architecture Overview)</h3>
<p>Compressor-VLA는 사전 학습된 VLA 모델(예: OpenVLA)을 백본(Backbone)으로 사용하며, 시각 인코더와 LLM 사이에 압축 모듈이 삽입된다.</p>
<ol>
<li><strong>입력 단계:</strong></li>
</ol>
<ul>
<li>이미지 <span class="math math-inline">I</span>는 시각 인코더를 통해 <span class="math math-inline">N</span>개의 시각 토큰 <span class="math math-inline">V = {v_1, v_2,..., v_N}</span>으로 변환된다.</li>
<li>명령어 <span class="math math-inline">L</span>은 텍스트 인코더를 통해 임베딩 벡터 <span class="math math-inline">T</span>로 변환된다.</li>
</ul>
<ol start="2">
<li><strong>압축 단계 (Hybrid Compression):</strong></li>
</ol>
<ul>
<li>시각 토큰 <span class="math math-inline">V</span>는 STC와 SRC로 동시에 전달된다.</li>
<li><strong>STC (Global Pathway):</strong> 명령어 <span class="math math-inline">T</span>를 쿼리(Query)로 사용하여 시각 토큰 전체에서 의미적으로 관련된 정보를 추출, 고정된 길이의 전역 요약 토큰 <span class="math math-inline">Z_G</span>를 생성한다.</li>
<li><strong>SRC (Local Pathway):</strong> 공간적 구조를 유지하면서 불필요한 영역을 제거하거나 병합하여, 지역적 세부 정보가 담긴 토큰 <span class="math math-inline">Z_L</span>을 생성한다. 이 과정 역시 명령어 <span class="math math-inline">T</span>에 의해 가이드된다.</li>
</ul>
<ol start="3">
<li><strong>통합 및 추론 단계:</strong></li>
</ol>
<ul>
<li>압축된 토큰 <span class="math math-inline">Z_G</span>와 <span class="math math-inline">Z_L</span>은 결합(Concatenation)되어 최종 시각 표현 <span class="math math-inline">Z_{final}</span>을 형성한다.</li>
<li><span class="math math-inline">Z_{final}</span>은 텍스트 임베딩 <span class="math math-inline">T</span>와 함께 LLM에 입력되어 로봇의 행동 <span class="math math-inline">A</span>를 예측한다.</li>
</ul>
<h3>3.2  의미적 작업 압축기 (Semantic Task Compressor, STC)</h3>
<p>STC는 로봇에게 **“무엇을 해야 하는가(What to do)”**와 **“대략적인 상황(Context)”**을 전달하는 역할을 한다.</p>
<ul>
<li><strong>작동 원리 (Mechanism):</strong> STC는 크로스 어텐션(Cross-Attention) 메커니즘을 핵심으로 한다.</li>
<li><strong>Query (<span class="math math-inline">Q</span>):</strong> 사용자의 자연어 명령어 임베딩 <span class="math math-inline">T</span>가 쿼리로 사용된다. 이는 모델이 “무엇을 찾아야 하는지“를 정의한다.</li>
<li><strong>Key (<span class="math math-inline">K</span>) &amp; Value (<span class="math math-inline">V</span>):</strong> 원본 시각 토큰 <span class="math math-inline">V</span>가 키와 밸류로 사용된다.</li>
<li><strong>연산:</strong> <span class="math math-inline">Attention(Q, K, V) = Softmax(\frac{QK^T}{\sqrt{d_k}})V</span></li>
<li>이 과정을 통해 명령어와 의미적 유사도가 높은 시각 정보만이 가중치를 받아 추출된다.</li>
<li><strong>결과물:</strong> STC는 입력 이미지의 크기와 상관없이 사전에 정의된 적은 수(예: <span class="math math-inline">k</span>개)의 고정 길이 토큰 <span class="math math-inline">Z_G</span>를 출력한다.</li>
<li><strong>의의:</strong> 이 과정에서 명령어와 관련 없는 배경 잡음(Visual Clutter)은 자연스럽게 필터링된다. 예를 들어, “빨간 컵“이라는 명령어가 주어지면, 파란색 접시나 식탁보의 무늬와 관련된 정보는 어텐션 점수가 낮아져 최종 요약에 거의 포함되지 않게 된다.4</li>
</ul>
<h3>3.3  공간 정제 압축기 (Spatial Refinement Compressor, SRC)</h3>
<p>STC가 전체적인 맥락을 잡는다면, SRC는 로봇의 정밀 제어를 위한 **“구체적인 좌표와 형상(How to act &amp; Where exactly)”**을 보존하는 역할을 한다. 전역 요약만으로는 물체의 정확한 엣지(Edge)나 그리퍼(Gripper)의 미세한 위치를 파악하기 어렵기 때문이다.</p>
<ul>
<li><strong>작동 원리 (Mechanism):</strong> SRC는 이미지의 2D 공간 구조를 훼손하지 않는 범위 내에서 압축을 수행한다.</li>
<li><strong>중요도 평가:</strong> 명령어 <span class="math math-inline">T</span>와의 관련성을 기반으로 각 시각 토큰 패치의 중요도 맵(Importance Map)을 생성한다.</li>
<li><strong>선택적 보존 및 병합:</strong> 중요도가 높은 영역(예: 대상 물체, 로봇 팔)의 토큰은 그대로 유지하거나 높은 해상도로 보존한다. 반면, 중요도가 낮은 배경 영역은 풀링(Pooling)을 통해 해상도를 낮추거나 인접한 토큰끼리 병합(Merging)한다.</li>
<li><strong>결과물:</strong> 공간적 위상(Topology)이 유지된 축소된 토큰 맵 <span class="math math-inline">Z_L</span>을 출력한다.</li>
<li><strong>의의:</strong> 이 모듈 덕분에 Compressor-VLA는 단순히 정보를 요약하는 것을 넘어, 로봇 조작에 필수적인 ’공간적 정밀성(Tactical Precision)’을 확보할 수 있다. 이는 기존의 요약 기반 모델들이 잃어버리기 쉬운 세부 정보를 지켜내는 핵심 기술이다.4</li>
</ul>
<h3>3.4  동적 변조 (Dynamic Modulation)</h3>
<p>Compressor-VLA의 가장 큰 혁신은 **명령어 유도(Instruction Guidance)**에 의한 동적 변조이다. 기존 모델들은 이미지가 들어오면 항상 동일한 방식으로 특징을 추출했다. 그러나 Compressor-VLA는 동일한 이미지라도 명령어에 따라 모델의 ’관심사’가 바뀐다.</p>
<ul>
<li>
<p><strong>Case A:</strong> “사과를 집어라” <span class="math math-inline">\rightarrow</span> 사과 주변의 토큰 활성화, 컵 관련 토큰 억제.</p>
</li>
<li>
<p>Case B: “컵을 밀어라” <span class="math math-inline">\rightarrow</span> 컵 주변의 토큰 활성화, 사과 관련 토큰 억제.</p>
</li>
</ul>
<p>이러한 동적 메커니즘은 제한된 토큰 용량(Token Budget) 내에서 정보의 효율을 극대화(Maximize Information Density regarding the Task)하는 결과를 낳는다.4</p>
<h2>4.  실험 방법론 및 평가 지표</h2>
<p>연구진은 Compressor-VLA의 성능을 검증하기 위해 체계적이고 광범위한 실험을 설계했다.</p>
<h3>4.1  벤치마크: LIBERO</h3>
<p>실험에는 로봇 학습 분야의 표준 벤치마크 중 하나인 **LIBERO (Lifelong Robot Learning)**가 사용되었다. LIBERO는 다양한 환경과 작업 유형을 포함하고 있어 VLA 모델의 일반화 능력과 정밀 조작 능력을 평가하기에 적합하다.4</p>
<ul>
<li><strong>LIBERO-Spatial:</strong> 물체의 위치 관계와 공간적 추론 능력을 평가.</li>
<li><strong>LIBERO-Object:</strong> 다양한 형태와 속성을 가진 물체에 대한 조작 능력 평가.</li>
<li><strong>LIBERO-Goal:</strong> 특정 목표 상태(Goal State)를 달성하는 능력 평가.</li>
<li><strong>LIBERO-Long:</strong> 긴 시퀀스의 복합 작업(Long-horizon tasks) 수행 능력 평가.</li>
</ul>
<h3>4.2  비교 대상 (Baselines)</h3>
<p>Compressor-VLA의 성능을 객관적으로 입증하기 위해 두 가지 그룹의 베이스라인 모델과 비교했다.</p>
<ol>
<li><strong>일반 목적 VLA (General-Purpose VLAs):</strong> 압축을 적용하지 않은 원본 모델들.</li>
</ol>
<ul>
<li><strong>OpenVLA-OFT:</strong> 본 연구의 백본 모델이자 성능 비교의 기준점.</li>
<li><strong>CogACT, <span class="math math-inline">\pi_0</span>:</strong> 최신 고성능 VLA 모델들.4</li>
</ul>
<ol start="2">
<li><strong>효율적 VLA (Efficient VLAs):</strong> 기존의 토큰 압축 기법을 적용한 모델들.</li>
</ol>
<ul>
<li><strong>FastV, SparseVLM, SpecPrune-VLA:</strong> 가지치기(Pruning) 기반 방법론.</li>
<li><strong>SP-VLA, VLA-Cache:</strong> 캐싱 및 효율화 기법 적용 모델.</li>
</ul>
<h3>4.3  평가 지표 (Metrics)</h3>
<ul>
<li><strong>성공률 (Success Rate, SR):</strong> 각 작업의 완료 여부를 백분율로 표시. 가장 중요한 성능 지표.</li>
<li><strong>FLOPs (Floating Point Operations):</strong> 모델의 연산량을 나타내며, 효율성의 척도. (단위: T - Tera)</li>
<li><strong>토큰 수 (Token Count):</strong> LLM에 입력되는 시각 토큰의 개수.</li>
</ul>
<h2>5.  정량적 성능 분석 및 결과</h2>
<p>실험 결과는 Compressor-VLA가 효율성과 성능이라는 두 마리 토끼를 모두 잡았음을 명확히 보여준다. 아래 표는 LIBERO 벤치마크에서의 종합적인 비교 결과를 요약한 것이다.4</p>
<table><thead><tr><th><strong>모델 (Model)</strong></th><th><strong>유형</strong></th><th><strong>Spatial (%)</strong></th><th><strong>Object (%)</strong></th><th><strong>Goal (%)</strong></th><th><strong>Long (%)</strong></th><th><strong>평균 성공률 (%)</strong></th><th><strong>FLOPs (T)</strong></th><th><strong>토큰 수</strong></th></tr></thead><tbody>
<tr><td><strong>OpenVLA-OFT</strong></td><td>General</td><td>97.6</td><td>98.4</td><td>97.9</td><td>94.5</td><td>97.1</td><td>3.95</td><td>512</td></tr>
<tr><td>CogACT</td><td>General</td><td>97.2</td><td>98.0</td><td>90.2</td><td>88.8</td><td>93.6</td><td>-</td><td>512</td></tr>
<tr><td><span class="math math-inline">\pi_0</span></td><td>General</td><td>96.8</td><td>98.8</td><td>95.8</td><td>85.2</td><td>94.2</td><td>-</td><td>512</td></tr>
<tr><td>SP-VLA</td><td>Efficient</td><td>75.4</td><td>85.6</td><td>84.4</td><td>54.2</td><td>74.9</td><td>3.10</td><td>229</td></tr>
<tr><td>FastV</td><td>Efficient</td><td>96.8</td><td>81.0</td><td>96.4</td><td>73.0</td><td>86.8</td><td>3.18</td><td>256</td></tr>
<tr><td>SparseVLM</td><td>Efficient</td><td>96.8</td><td>94.2</td><td>97.6</td><td>93.6</td><td>95.6</td><td>3.04</td><td>256</td></tr>
<tr><td>SpecPrune-VLA</td><td>Efficient</td><td>98.2</td><td>96.3</td><td>97.7</td><td>94.0</td><td>96.6</td><td>1.70</td><td>197</td></tr>
<tr><td><strong>Compressor-VLA</strong></td><td><strong>Ours</strong></td><td><strong>98.8</strong></td><td><strong>99.2</strong></td><td><strong>96.4</strong></td><td><strong>94.8</strong></td><td><strong>97.3</strong></td><td><strong>1.62</strong></td><td><strong>160</strong></td></tr>
</tbody></table>
<h3>5.1  압도적인 효율성 달성</h3>
<p>Compressor-VLA는 비교된 모든 모델 중 가장 낮은 연산량과 가장 적은 토큰 수를 기록했다.</p>
<ul>
<li><strong>FLOPs 감소:</strong> 기준 모델인 OpenVLA-OFT(3.95T) 대비 **59%의 FLOPs 감소(1.62T)**를 달성했다. 이는 하드웨어 자원이 제한된 로봇에서 추론 속도를 2배 이상 가속화할 수 있음을 의미한다.</li>
<li><strong>토큰 다이어트:</strong> 입력 시각 토큰 수를 512개에서 <strong>160개</strong>로, 약 <strong>3.2배</strong> 감소시켰다. 이는 메모리 대역폭 절약과 KV 캐시(KV Cache) 점유율 감소로 직결되어 긴 작업을 수행할 때 시스템 안정성을 높인다.4</li>
</ul>
<h3>5.2  성능의 역설: 더 적은 정보로 더 높은 정확도</h3>
<p>일반적으로 정보를 압축하면 성능 하락(Performance degradation)이 발생하기 마련이다. 그러나 Compressor-VLA는 평균 성공률 **97.3%**를 기록하며, 원본 모델인 OpenVLA-OFT(97.1%)보다 오히려 소폭 향상된 성능을 보여주었다.</p>
<ul>
<li><strong>노이즈 제거 효과:</strong> 이는 압축 과정에서 작업과 무관한 ’시각적 잡음(Visual Noise)’이 제거되어, 모델이 핵심 정보에 더 집중할 수 있었기 때문으로 해석된다.</li>
<li><strong>경쟁 모델과의 격차:</strong> 단순히 토큰을 잘라낸 FastV(86.8%)나 SP-VLA(74.9%)는 처참한 성능 하락을 겪었다. 특히 <strong>LIBERO-Long</strong> 항목에서 FastV는 73.0%, SP-VLA는 54.2%로 무너졌으나, Compressor-VLA는 **94.8%**를 유지했다. 이는 장기 작업일수록 문맥 유지(STC)와 세밀한 조작(SRC)이 모두 중요하다는 것을 방증한다.</li>
</ul>
<h3>5.3  세부 작업 분석</h3>
<ul>
<li><strong>LIBERO-Object (99.2%):</strong> 물체 조작 성능이 가장 뛰어나다. 이는 SRC 모듈이 물체의 형상 정보를 효과적으로 보존하여 정확한 파지(Grasping)를 가능하게 했음을 시사한다.</li>
<li><strong>LIBERO-Long (94.8%):</strong> 긴 시퀀스 작업에서의 높은 성공률은 STC 모듈이 작업의 단계별 목표를 잃지 않고 전역적인 맥락을 잘 유지했음을 보여준다.</li>
</ul>
<hr />
<h2>6.  정성적 분석 및 해석 가능성 (Qualitative Analysis)</h2>
<p>Compressor-VLA의 높은 성능이 우연이 아님을 증명하기 위해, 연구진은 모델 내부의 어텐션 맵(Attention Map)을 시각화하여 분석했다.4</p>
<h3>6.1  명령어에 따른 시선 변화</h3>
<p>동일한 장면에서 명령어가 달라질 때, 모델이 선택하는 토큰 영역이 어떻게 변하는지 관찰했다.</p>
<ul>
<li>
<p><strong>“주전자를 옮겨라”:</strong> 주전자 영역의 토큰들이 높은 중요도로 활성화되고, 주변의 컵이나 그릇은 억제되었다.</p>
</li>
<li>
<p>“서랍을 열어라”: 서랍 손잡이와 서랍장 모서리 부분의 토큰이 집중적으로 선택되었다.</p>
</li>
</ul>
<p>이러한 결과는 Compressor-VLA가 텍스트 명령어를 일종의 ’스포트라이트(Spotlight)’로 사용하여 시각 정보의 우선순위를 재배열하고 있음을 시각적으로 증명한다.</p>
<h3>6.2  STC와 SRC의 협업</h3>
<p>어텐션 시각화 결과, STC는 화면 전체에 흩어진 작업 관련 정보(예: 출발지와 목적지)를 포괄적으로 잡는 반면, SRC는 로봇의 엔드 이펙터(End-effector)와 대상 물체의 접촉 부위 등 국소적이고 정밀한 영역에 집중하는 패턴을 보였다. 이는 두 모듈이 설계 의도대로 상호 보완적인 역할을 수행하고 있음을 확인시켜 준다.</p>
<h2>7.  실제 로봇 배포 및 검증 (Real-Robot Deployment)</h2>
<p>시뮬레이션에서의 성공이 실제 세계에서의 성공을 보장하지는 않는다. 이를 검증하기 위해 연구진은 <strong>양팔 로봇(Dual-arm robot)</strong> 플랫폼에 Compressor-VLA를 탑재하고 실증 실험을 진행했다.4</p>
<h3>7.1  Sim-to-Real 전이 (Transferability)</h3>
<p>시뮬레이션 데이터로 학습된 압축 정책이 실제 카메라 이미지와 조명 조건에서도 견고하게 작동하는지가 핵심이었다.</p>
<ul>
<li><strong>견고성 (Robustness):</strong> 실제 환경의 복잡한 배경과 조명 변화에도 불구하고, Compressor-VLA는 높은 성공률로 작업을 완수했다. 이는 STC가 배경 노이즈를 효과적으로 필터링해 준 덕분으로 분석된다.</li>
<li><strong>일반화:</strong> 학습 때 보지 못한 새로운 물체 배치나 색상에 대해서도 유연하게 대처하는 능력을 보여주었다.</li>
</ul>
<h3>7.2  실시간 제어 효율성</h3>
<p>실제 로봇 하드웨어에서의 추론 속도 향상은 더욱 극적이었다.</p>
<ul>
<li>연산량 감소 덕분에 제어 주파수(Control Frequency)가 높아져, 로봇의 움직임이 더욱 부드럽고 자연스러워졌다(Smoothness).</li>
<li>지연 시간(Latency) 감소는 로봇이 돌발 상황에 더 빠르게 반응할 수 있게 하여 작업의 안전성을 높이는 데 기여했다.</li>
</ul>
<h2>8.  논의 및 파급 효과 (Discussion &amp; Impact)</h2>
<h3>8.1  엣지 로보틱스(Edge Robotics)의 가능성</h3>
<p>Compressor-VLA의 가장 큰 의의는 고성능 GPU 서버가 없는 환경에서도 고지능 VLA 모델을 운용할 수 있는 길을 열었다는 점이다. 드론, 사족 보행 로봇, 가정용 모바일 로봇 등 배터리와 연산 자원이 제한된 **엣지 디바이스(Edge Devices)**에서 VLA 모델의 탑재 가능성이 현실화되었다. 59%의 FLOPs 절감은 단순한 속도 향상을 넘어 전력 소모 감소와 작동 시간 연장을 의미한다.</p>
<h3>8.2  정보의 가치 재정의</h3>
<p>본 연구는 로봇 공학 관점에서 “정보의 가치(Value of Information)“를 재정의했다. 기존의 정보 이론적 관점(엔트로피 등)이 아닌, **“행동에 영향을 미치는가(Action-Relevance)”**가 정보의 가치를 결정한다는 것이다. 이는 향후 로봇 인지 시스템 설계에 있어 “무조건 많은 데이터를 처리하는 것“보다 “필요한 데이터를 똑똑하게 선별하는 것“이 중요함을 시사한다.</p>
<h3>8.3  확장 가능성</h3>
<p>Compressor-VLA의 하이브리드 압축 구조(STC+SRC)는 로봇 조작뿐만 아니라 자율 주행, 영상 감시, 증강 현실(AR) 등 다양한 멀티모달 애플리케이션에 확장 적용될 수 있다. 전역적 맥락과 지역적 세부 정보를 동시에 요구하는 모든 실시간 비전 작업에 이 프레임워크가 유효할 것으로 전망된다.</p>
<h2>9.  결론 (Conclusion)</h2>
<p>본 보고서를 통해 살펴본 <strong>Compressor-VLA</strong>는 시각-언어-행동 모델의 비효율성 문제를 근본적으로 해결하고자 한 시도이다.4 베이징 공업대학교와 LiAuto Inc.의 연구진은 자연어 명령어의 맥락을 활용하여 시각 정보를 동적으로 압축하는 방법론을 제안하였고, 이를 통해 연산량을 59% 줄이면서도 3배 적은 토큰으로 기존 모델보다 우수한 성능(97.3% 성공률)을 달성했다.4</p>
<p>Compressor-VLA의 성공은 다음과 같은 결론을 도출한다:</p>
<ol>
<li><strong>목적 지향적 압축:</strong> 로봇의 시각 처리는 수동적인 데이터 입력이 아니라, 작업 목적(명령어)에 따른 능동적인 정보 선별 과정이어야 한다.</li>
<li><strong>구조의 승리:</strong> 의미적 요약(STC)과 공간적 정제(SRC)를 결합한 하이브리드 아키텍처는 단일 방식의 압축보다 월등히 뛰어난 성능/효율 트레이드오프를 제공한다.</li>
<li><strong>실용화 가속:</strong> 이러한 경량화 기술은 VLA 모델이 연구실을 벗어나 실제 산업 현장과 가정으로 진입하는 시기를 획기적으로 앞당길 것이다.</li>
</ol>
<p>Compressor-VLA는 단순히 모델을 가볍게 만든 것이 아니라, 로봇이 세상을 ’어떻게 바라봐야 하는지’에 대한 새로운 시각을 제시했다는 점에서 그 학술적, 산업적 가치가 매우 높다.</p>
<h2>10. 참고 자료</h2>
<ol>
<li>Vision Language Action (VLA) Models Powering Robotics| Exxact Blog, https://www.exxactcorp.com/blog/deep-learning/vision-language-action-vla-models-powers-robotics</li>
<li>Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications, https://vla-survey.github.io/</li>
<li>Vision Language Action Models (VLA) &amp; Policies for Robots - LearnOpenCV, https://learnopencv.com/vision-language-action-models-lerobot-policy/</li>
<li>Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation - arXiv, https://arxiv.org/html/2511.18950v1</li>
<li>[2511.18950] Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation - arXiv, https://arxiv.org/abs/2511.18950</li>
<li>RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models - arXiv, https://arxiv.org/html/2506.17639v1</li>
<li>Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation - ResearchGate, https://www.researchgate.net/publication/397933956_Compressor-VLA_Instruction-Guided_Visual_Token_Compression_for_Efficient_Robotic_Manipulation</li>
<li>Arxiv今日论文| 2025-11-25 - 闲记算法, http://lonepatient.top/2025/11/25/arxiv_papers_2025-11-25</li>
<li>机器学习2025_11_25[2] - arXiv每日学术速递, http://arxivdaily.com/thread/74056</li>
<li>BaiShuanghao/my_arXiv_daily - GitHub, https://github.com/BaiShuanghao/my_arXiv_daily</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>