<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Embodiment Transfer Learning (신체 전이 학습, ET) for VLA</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Embodiment Transfer Learning (신체 전이 학습, ET) for VLA</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">Vision-Language-Action(VLA) 모델</a> / <span>Embodiment Transfer Learning (신체 전이 학습, ET) for VLA</span></nav>
                </div>
            </header>
            <article>
                <h1>Embodiment Transfer Learning (신체 전이 학습, ET) for VLA</h1>
<p>2025-12-07, G30DR</p>
<h2>1.  서론: 구현된 인공지능(Embodied AI)의 패러다임 전환과 신체 전이의 필요성</h2>
<p>인공지능(AI) 기술은 대규모 언어 모델(LLM)과 비전-언어 모델(VLM)의 등장을 통해 텍스트와 이미지라는 디지털 데이터 영역에서 인간 수준의 이해력과 생성 능력을 입증했다. 그러나 이러한 지능을 물리적 세계와 상호작용하는 로봇 시스템, 즉 ’구현된 AI(Embodied AI)’로 확장하는 과정은 여전히 거대한 장벽에 부딪혀 있다. 디지털 세계의 데이터는 표준화되어 있고 무한히 복제 가능하지만, 물리적 세계의 데이터는 수집 비용이 높고, 무엇보다 로봇 하드웨어—즉 ‘신체(Embodiment)’—의 다양성으로 인해 파편화되어 있기 때문이다.1</p>
<p>전통적인 로봇 제어 방식은 특정 하드웨어에 종속된 제어 코드를 수동으로 작성하거나, 제한된 환경에서 수집된 데이터로 특정 작업(Specialist Task)만을 수행하는 정책을 학습하는 데 집중했다. 이러한 접근은 로봇의 하드웨어가 조금만 바뀌거나 작업 환경이 달라지면 시스템 전체를 재설계해야 하는 비효율성을 초래했다.1 이에 대한 대안으로 등장한 비전-언어-행동(Vision-Language-Action, VLA) 모델은 인터넷 규모의 웹 데이터에서 학습한 일반적인 상식과 추론 능력을 로봇 제어와 통합함으로써, 범용 로봇(Generalist Robot)을 실현할 수 있는 유력한 방법론으로 부상하고 있다.3</p>
<p>VLA 모델의 핵심 목표는 “인터넷에서 세상을 배운 두뇌를 로봇에게 이식하여 움직이는 법을 가르치는 것“이다.1 그러나 여기에는 결정적인 난제가 존재한다. 바로 ’신체 격차(Embodiment Gap)’이다. 텍스트나 이미지는 전 세계 어디서나 동일한 포맷을 가지지만, 로봇은 제조사마다, 모델마다 관절의 수(DoF), 팔의 길이, 센서의 종류와 위치, 구동 방식(Actuation)이 천차만별이다. 따라서 하나의 VLA 모델이 다양한 로봇 하드웨어를 아우르며 지식을 공유하고 전이할 수 있는 ‘신체 전이 학습(Embodiment Transfer Learning)’ 기술은 범용 로봇 시대를 열기 위한 성배(Holy Grail)와도 같다.5</p>
<p>본 보고서는 VLA 모델의 최신 아키텍처와 학습 방법론을 ’신체 전이’라는 관점에서 심층적으로 분석한다. 구글 딥마인드의 RT-2부터 최신 모델인 <span class="math math-inline">\pi_0</span>(Pi-Zero), OpenVLA, Octo, 그리고 Helix와 같은 계층적 제어 모델까지 포괄적으로 다루며, 이들이 어떻게 이질적인 하드웨어 간의 장벽을 허물고 지식을 공유하는지 기술적 메커니즘을 규명한다. 또한, 이산적 행동 토큰화와 연속적 유체 흐름(Flow Matching) 간의 기술적 논쟁, Open X-Embodiment 데이터셋의 역할, 그리고 실제 환경 적용 시 발생하는 고유 감각(Proprioception) 처리 문제 등을 면밀히 검토하여 향후 연구 방향을 제시한다.</p>
<h2>2.  VLA 모델의 이론적 배경 및 아키텍처 진화</h2>
<h3>2.1  VLA의 정의 및 작동 원리</h3>
<p>비전-언어-행동(VLA) 모델은 시각적 관찰(Vision)과 자연어 명령(Language)을 입력으로 받아, 물리적 세계에 영향을 미치는 행동(Action)을 출력으로 생성하는 통합된 신경망 아키텍처를 의미한다.3 이는 기존의 파이프라인 방식, 즉 인식(Perception), 계획(Planning), 제어(Control)가 분리된 모듈형 접근 방식과 대조된다. VLA는 거대 언어 모델(LLM)의 트랜스포머 아키텍처를 기반으로 하며, 로봇의 행동 또한 언어와 같은 일련의 ‘토큰(Token)’ 또는 연속적인 값으로 취급하여 처리한다.</p>
<p>VLA의 핵심 경쟁력은 ’의미론적 추론(Semantic Reasoning)’과 ’저수준 제어(Low-level Control)’의 결합에 있다. 기존 로봇 시스템은 “파란색 컵을 집어라“라는 명령을 수행하기 위해 물체 인식기, 파지 계획기, 역운동학 풀이기가 순차적으로 작동해야 했다. 반면 VLA 모델은 “쓰레기를 버려라“라는 추상적인 명령을 받으면, 이미지 내에서 무엇이 쓰레기인지(의미론적 이해), 쓰레기통이 어디 있는지(공간적 추론), 그리고 이를 수행하기 위해 팔을 어떻게 움직여야 하는지(행동 생성)를 단일 모델 내에서 End-to-End로 처리한다.1</p>
<h3>2.2  아키텍처의 진화: RT 시리즈에서 OpenVLA까지</h3>
<p>VLA 모델의 아키텍처는 행동을 표현하는 방식과 백본 모델의 통합 방식에 따라 크게 발전해 왔다.</p>
<h4>2.2.1  초기 통합 모델: RT-1 및 RT-2</h4>
<p>구글의 RT-1(Robotic Transformer 1)은 트랜스포머 아키텍처를 로봇 제어에 도입한 초기 시도로, 이미지와 언어 명령을 토큰화하여 행동을 출력했다.3 이후 등장한 RT-2는 550억 개(55B) 이상의 파라미터를 가진 거대 비전-언어 모델(VLM)인 PaLI-X와 PaLM-E를 기반으로 하여, 로봇 제어 데이터를 텍스트 데이터와 함께 미세 조정(Fine-tuning)하는 방식을 택했다.3</p>
<p>RT-2의 가장 큰 혁신은 로봇의 행동을 텍스트 토큰으로 변환한 것이다. 로봇의 6자유도(6-DoF) 위치 및 회전 변화량과 그리퍼 개폐 상태를 포함한 행동 공간을 256개의 구간(Bin)으로 이산화(Discretization)하여, 이를 정수형 토큰으로 표현했다.9 이를 통해 로봇 제어 문제를 다음 토큰 예측(Next-token Prediction)이라는 전형적인 언어 모델링 문제로 환원시켰으며, 인터넷 웹 데이터에서 학습된 추론 능력을 로봇 제어에 전이시키는 데 성공했다.</p>
<h4>2.2.2  오픈소스 생태계의 부상: OpenVLA</h4>
<p>RT-2가 폐쇄적인 독점 모델인 반면, OpenVLA는 Llama 2 7B 모델을 기반으로 한 오픈소스 VLA 모델로, 누구나 접근 가능한 VLA 연구의 민주화를 이끌었다.10 OpenVLA는 SigLIP과 DINOv2라는 강력한 시각 인코더를 결합하여 시각적 특징 추출 능력을 극대화했다. DINOv2는 공간적 기하학 정보를, SigLIP은 의미론적 정보를 잘 포착하는 특성이 있어, 이 둘의 결합은 로봇 조작에 필요한 정밀한 공간 인식과 물체 이해를 동시에 제공한다.10 또한 OpenVLA는 전체 모델을 재학습하는 대신 LoRA(Low-Rank Adaptation)와 같은 효율적인 튜닝 기법을 적용하여, 일반 GPU 환경에서도 새로운 로봇에 대한 적응 훈련이 가능하도록 설계되었다.12</p>
<h3>2.3  연속적 제어와 생성 모델의 도입: Octo와 <span class="math math-inline">\pi_0</span></h3>
<p>이산적인 토큰 방식(RT-2, OpenVLA)은 구현이 쉽고 안정적이지만, 정밀도가 떨어지고 고빈도 제어가 어렵다는 단점이 있다. 이를 극복하기 위해 최신 모델들은 확산 모델(Diffusion Model)과 흐름 매칭(Flow Matching) 기술을 도입하고 있다.</p>
<table><thead><tr><th><strong>모델</strong></th><th><strong>행동 생성 방식</strong></th><th><strong>특징 및 장점</strong></th><th><strong>관련 연구</strong></th></tr></thead><tbody>
<tr><td><strong>RT-2</strong></td><td>이산적 토큰화 (Discrete Binning)</td><td>언어 모델과 통합 용이, 추론 능력 우수</td><td>3</td></tr>
<tr><td><strong>Octo</strong></td><td>확산 정책 (Diffusion Policy)</td><td>멀티모달 행동 분포 표현 가능, 유연한 입력 처리</td><td>13</td></tr>
<tr><td><strong><span class="math math-inline">\pi_0</span> (Pi-Zero)</strong></td><td>흐름 매칭 (Flow Matching)</td><td>최대 50Hz 고속 제어, 정밀한 조작(Dexterity) 실현</td><td>15</td></tr>
</tbody></table>
<h4>2.3.1  Octo: 트랜스포머 기반 확산 정책</h4>
<p>Octo는 80만 개 이상의 로봇 궤적 데이터로 사전 학습된 모델로, 확산 모델을 사용하여 연속적인 행동 궤적을 생성한다.14 확산 모델은 노이즈에서 시작하여 점진적으로 노이즈를 제거하며 원하는 행동 분포를 찾아가는 방식으로, 여러 가지 가능한 행동(예: 장애물을 왼쪽으로 피하거나 오른쪽으로 피하는 것)이 공존할 때 이를 평균내지 않고 각각의 모드(Mode)를 보존할 수 있다는 장점이 있다. Octo는 ’블록 단위(Chunking)’로 미래 행동을 예측하여 동작의 부드러움을 향상시킨다.17</p>
<h4>2.3.2  <span class="math math-inline">\pi_0</span>: 흐름 매칭을 통한 고빈도 제어</h4>
<p>Physical Intelligence 사가 개발한 <span class="math math-inline">\pi_0</span>는 VLM 백본에 흐름 매칭(Flow Matching) 헤드를 결합한 구조다.15 흐름 매칭은 확산 모델의 일반화된 형태로, 노이즈 분포와 데이터 분포 사이의 최적 경로(Vector Field)를 학습하여 추론 속도와 품질을 개선한다. <span class="math math-inline">\pi_0</span>는 이를 통해 최대 50Hz의 제어 주기를 달성했는데, 이는 기존 VLA 모델들이 3~5Hz 수준에 머물렀던 것과 비교하면 비약적인 발전이다.18 빠른 제어 주기는 달걀 깨기, 빨래 개기 등 복잡하고 동적인 힘 조절이 필요한 작업에서 필수적이다.</p>
<h2>3.  데이터: 신체 전이의 연료, Open X-Embodiment</h2>
<h3>3.1  Open X-Embodiment (OXE) 데이터셋의 구조와 의의</h3>
<p>신체 전이 학습을 위해서는 단일 로봇 데이터가 아닌, 다양한 로봇의 데이터가 통합된 대규모 데이터셋이 필수적이다. 구글 딥마인드와 21개 연구소의 협력으로 구축된 Open X-Embodiment (OXE) 데이터셋은 로봇 학습 분야의 ‘이미지넷(ImageNet)’ 모멘트를 목표로 한다.19</p>
<p>OXE 데이터셋은 22종의 로봇 형태, 527개 기술, 160,266개 작업, 100만 개 이상의 에피소드를 포함한다.21 포함된 로봇은 Franka Emika Panda, WidowX와 같은 협동 로봇 팔부터, 구글의 모바일 조작 로봇, 그리고 쿼드러페드(4족 보행) 로봇까지 다양하다. 이 데이터셋의 가장 큰 기여는 파편화되어 있던 로봇 데이터를 **RLDS (Reinforcement Learning Datasets)**라는 표준 포맷으로 통일했다는 점이다.19 RLDS는 에피소드 단위로 데이터를 직렬화하고, 다양한 센서 입력과 행동 공간을 효율적으로 로딩할 수 있게 해준다.</p>
<h3>3.2  행동 공간의 정규화와 한계</h3>
<p>서로 다른 로봇 데이터를 하나의 모델에 학습시키기 위해서는 입력과 출력 공간의 정렬이 선행되어야 한다. OXE 데이터셋과 RT-X 모델은 이를 위해 <strong>7-DoF 말단 장치(End-Effector) 제어 공간</strong>을 표준으로 채택했다.19</p>
<ul>
<li><strong>구성:</strong> 위치 변화(<span class="math math-inline">\Delta x, \Delta y, \Delta z</span>), 회전 변화(<span class="math math-inline">\Delta roll, \Delta pitch, \Delta yaw</span>), 그리퍼 상태(개폐).</li>
<li><strong>이점:</strong> 관절 공간(Joint Space) 제어는 로봇마다 관절 수(6축, 7축 등)와 기구학적 구조가 달라 통합이 어렵지만, 말단 장치 제어는 카르테시안 좌표계상에서의 움직임이므로 로봇 형태에 상대적으로 불변(Invariant)하다. 이는 서로 다른 로봇 간의 지식 전이를 용이하게 하는 핵심 요인이다.23</li>
<li><strong>한계:</strong> 그러나 이 방식은 로봇의 특이점(Singularity) 문제나 여유 자유도(Redundancy) 활용(예: 팔꿈치 위치 제어)을 어렵게 만든다. 또한 이동형 로봇(Mobile Base)의 경우 베이스의 움직임을 별도로 처리해야 하므로 완전한 통합에는 한계가 있다. 최근 연구들은 내비게이션 데이터와 조작 데이터를 함께 학습시켜(Co-training), 이동과 조작을 아우르는 통합 행동 공간을 모색하고 있다.24</li>
</ul>
<h2>4.  신체 전이 학습(Embodiment Transfer Learning)의 핵심 메커니즘</h2>
<p>신체 전이 학습은 단순히 여러 로봇의 데이터를 섞어서 학습하는 것을 넘어, 로봇 A에서 배운 기술을 로봇 B에 적용하거나, 로봇 A, B, C의 데이터로 학습한 모델이 본 적 없는 로봇 D를 제어할 수 있게 만드는 것을 목표로 한다. 이를 실현하기 위한 주요 기술적 접근법들을 상세히 분석한다.</p>
<h3>4.1  RT-X와 긍정적 전이 (Positive Transfer)</h3>
<p>RT-X 연구는 데이터가 풍부한 로봇(Data-rich embodiment)의 지식이 데이터가 부족한 로봇(Data-poor embodiment)의 성능을 향상시키는 <strong>긍정적 전이(Positive Transfer)</strong> 현상을 실증적으로 입증했다.22</p>
<ul>
<li><strong>실험 결과:</strong> OXE 데이터셋 전체로 학습된 RT-2-X 모델은 특정 로봇 전용 데이터로만 학습된 모델보다 평균 50% 높은 성공률을 보였으며, 특히 훈련 데이터에 없던 새로운 기술(Emergent Skills)을 수행하는 능력이 3배 향상되었다.26</li>
<li><strong>해석:</strong> 이는 거대 모델이 다양한 로봇의 데이터에서 물리적 상호작용의 보편적인 원리(예: 물체를 집으려면 그리퍼를 벌리고 접근해야 한다)를 학습하고, 이를 개별 로봇의 구체적인 제어에 적용할 수 있음을 시사한다. 그러나 모델의 용량이 작을 경우(RT-1-X), 다양한 데이터의 간섭(Negative Transfer)으로 인해 성능 향상이 제한적이었다는 점은 모델 용량(Capacity)과 데이터 다양성 간의 상관관계를 보여준다.22</li>
</ul>
<h3>4.2  적응형 모듈: LoRA와 Soft Prompts</h3>
<p>거대 VLA 모델을 새로운 로봇에 적용할 때마다 전체 모델을 재학습(Full Fine-tuning)하는 것은 막대한 비용이 소요된다. 따라서 <strong>X-VLA</strong>와 같은 연구는 효율적인 적응(Adaptation)을 위한 기술에 주목한다.</p>
<h4>4.2.1  Soft Prompting (소프트 프롬프트)</h4>
<p>X-VLA는 각 로봇 신체(Embodiment)마다 고유한 학습 가능한 임베딩 벡터인 ’소프트 프롬프트’를 할당한다.27</p>
<ul>
<li><strong>작동 방식:</strong> 입력 이미지와 텍스트 명령 앞에 해당 로봇을 식별하는 소프트 프롬프트 벡터를 추가하여 트랜스포머에 입력한다. 이 벡터는 해당 로봇의 기구학적 특성이나 제어 뉘앙스를 암묵적으로 인코딩하게 된다.</li>
<li><strong>장점:</strong> 모델의 가중치를 전혀 수정하지 않고 입력단에서 컨텍스트를 조절하므로, 하나의 모델로 수십, 수백 종류의 로봇을 동시에 지원할 수 있다. 연구 결과, 전체 파라미터의 0.1% 미만인 프롬프트 튜닝만으로도 전체 미세 조정에 버금가는 전이 성능을 보였다.28</li>
</ul>
<h4>4.2.2  LoRA (Low-Rank Adaptation)</h4>
<p>OpenVLA 등에서 주로 사용되는 LoRA는 트랜스포머의 어텐션 가중치 행렬에 저랭크(Low-Rank) 행렬을 추가하여 학습하는 방식이다.12 LoRA는 소프트 프롬프트보다 더 많은 파라미터를 수정할 수 있어 표현력이 높지만, 로봇마다 별도의 어댑터 가중치를 저장하고 교체해야 하는 관리적 부담이 있다.</p>
<h3>4.3  ET-VLA: 합성 데이터를 통한 가속 학습</h3>
<p><strong>ET-VLA (Embodiment Transfer VLA)</strong> 프레임워크는 실제 로봇 데이터 수집의 어려움을 극복하기 위해 **합성 지속 사전 학습(Synthetic Continued Pretraining, SCP)**이라는 새로운 패러다임을 제안했다.5</p>
<ul>
<li><strong>개념:</strong> 새로운 로봇(Target Embodiment)을 도입할 때, 즉시 실제 환경에서 데이터를 모으는 대신 시뮬레이터에서 해당 로봇의 모델을 생성하고 대량의 합성 데이터를 만들어 VLA 모델을 1차적으로 튜닝(Warm-up)한다.</li>
<li><strong>효과:</strong> 이 과정을 통해 모델은 새로운 로봇의 행동 공간 구조와 기초적인 제어 특성을 미리 학습하게 되며, 이후 소량의 실제 데이터(Real Data)만으로도 빠르게 고성능에 도달할 수 있다. 이는 ‘Sim-to-Real’ 전이의 일종으로, 신체 격차를 줄이는 데 매우 효과적인 전략임이 입증되었다.29</li>
</ul>
<h3>4.4  시각적 격차 해소: Mirage와 Cross-Painting</h3>
<p>신체 전이의 또 다른 장벽은 ’시각적 차이’이다. 로봇마다 카메라의 위치, 렌즈 왜곡, 그리고 로봇 자신의 모습(팔의 색상, 형태)이 다르기 때문에, 모델은 시각적으로 낯선 로봇 팔을 자신의 신체로 인식하지 못할 수 있다.</p>
<p>Mirage 연구는 Cross-Painting이라는 기법을 통해 이를 해결한다.23</p>
<ul>
<li><strong>작동 원리:</strong> 로봇 A의 데이터로 학습된 모델을 로봇 B에 적용할 때, 로봇 B의 카메라로 들어온 이미지에서 로봇 B의 팔 부분을 마스킹하고, 그 위에 로봇 A의 팔 이미지를 덧입혀(In-painting) 렌더링한다. 즉, 모델에게는 익숙한 로봇 A의 팔이 보이는 것처럼 속이는 것이다.</li>
<li><strong>성과:</strong> 이를 통해 별도의 재학습 없이도 Franka 로봇용 정책을 WidowX 로봇에서 제로샷(Zero-shot)으로 실행하는 데 성공했으며, 단순한 시각적 변환만으로도 상당한 수준의 전이가 가능함을 보여주었다.23</li>
</ul>
<h2>5.  행동 표현(Action Representation)의 기술적 쟁점</h2>
<p>VLA 모델 설계에서 가장 치열한 논쟁 중 하나는 “로봇의 행동을 어떻게 표현할 것인가?“이다. 이는 모델의 정밀도, 학습 속도, 그리고 전이 가능성에 직접적인 영향을 미친다.</p>
<h3>5.1  이산적 토큰화 (Discrete Tokenization) vs. 연속적 표현 (Continuous Representation)</h3>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>이산적 토큰화 (RT-2, OpenVLA)</strong></th><th><strong>연속적 표현 (Octo, π0, Diffusion)</strong></th></tr></thead><tbody>
<tr><td><strong>기본 원리</strong></td><td>행동 공간을 256개 구간(Bin)으로 나누어 정수로 표현</td><td>실수(Float) 값을 직접 예측하거나 흐름(Flow) 생성</td></tr>
<tr><td><strong>학습 손실 함수</strong></td><td>Cross-Entropy Loss (분류 문제)</td><td>MSE, Flow Matching Loss (회귀/생성 문제)</td></tr>
<tr><td><strong>장점</strong></td><td>LLM 아키텍처 그대로 사용 가능, 학습 안정성 높음</td><td>정밀도(Millimeter-level) 우수, 부드러운 동작</td></tr>
<tr><td><strong>단점</strong></td><td>양자화 오차 발생, 토큰 시퀀스 길이 증가</td><td>계산 비용 높음, 별도의 디코더(Readout Head) 필요</td></tr>
<tr><td><strong>전이 학습 관점</strong></td><td>표준화된 토큰으로 다양한 로봇 통합 용이</td><td>로봇별 행동 스케일링/정규화에 민감</td></tr>
</tbody></table>
<p>최근의 <strong>FAST (Frequency-space Action Sequence Tokenization)</strong> 연구는 이산적 방식의 효율성과 연속적 방식의 정보량을 결합하려는 시도다. 시간 도메인의 행동 신호를 이산 코사인 변환(DCT)을 통해 주파수 도메인으로 변환한 뒤 토큰화함으로써, 긴 시퀀스를 압축하고 고빈도 정보를 보존한다.31 이는 <span class="math math-inline">\pi_0</span>-FAST 모델에 적용되어 높은 효율성을 입증했다.</p>
<h3>5.2  고유 감각(Proprioception)의 딜레마</h3>
<p>로봇 제어에서 팔의 현재 각도나 속도 정보(Proprioception)는 필수적일 것 같지만, VLA 모델에서의 역할은 복합적이다. 연구에 따르면 고유 감각 정보를 무분별하게 입력에 포함하면 오히려 전이 성능이 떨어질 수 있다.32</p>
<ul>
<li><strong>이유:</strong> 서로 다른 로봇 간에 고유 감각 데이터 값의 범위와 분포가 상이하여, 모델이 시각적 맥락보다 특정 로봇의 수치 값에 과적합(Overfitting)되기 때문이다.</li>
<li><strong>해결책:</strong> <strong>LeVERB</strong>와 같은 모델은 ’System 1(저수준 제어)’과 ’System 2(고수준 계획)’를 분리하여, 고유 감각 정보는 로봇 특화된 저수준 제어기에서만 처리하고, 범용 VLA 모델은 시각 정보만을 처리하도록 설계하여 이 문제를 회피한다.33 또는 X-VLA처럼 신체 특화 임베딩을 통해 고유 감각 정보를 문맥화(Contextualize)하는 방법이 사용된다.</li>
</ul>
<h2>6.  벤치마크 및 성능 평가 분석</h2>
<h3>6.1  SimplerEnv: 시뮬레이션을 통한 대규모 평가</h3>
<p>VLA 모델의 성능을 실제 로봇에서 일일이 테스트하는 것은 불가능에 가깝다. <strong>SimplerEnv</strong>는 실제 로봇 환경(Google Fractal, Bridge 등)을 시뮬레이션상에 정교하게 복제한 디지털 트윈 벤치마크이다.34</p>
<ul>
<li><strong>시각적 매칭:</strong> 실제 이미지와 거의 유사한 렌더링을 제공하여, 실제 데이터로 학습된 모델을 시뮬레이션에서 평가(Real-to-Sim Evaluation)할 수 있게 한다.</li>
<li><strong>평가 결과:</strong> OpenVLA는 SimplerEnv의 Bridge 데이터셋 환경에서 기존 RT-2-X보다 높은 성공률을 기록했으며, 특히 훈련 데이터 분포 밖의(Out-of-Distribution) 환경 설정에서도 강건함을 보였다.35</li>
</ul>
<h3>6.2  실세계 교차 신체 전이 (Real-world Cross-Embodiment Transfer)</h3>
<p>실제 물리적 환경에서의 전이 실험 결과는 VLA의 가능성과 한계를 동시에 보여준다.</p>
<ul>
<li><strong>Franka <span class="math math-inline">\rightarrow</span> WidowX:</strong> RT-X 실험에서 Franka 로봇 데이터가 포함된 대규모 데이터셋으로 학습된 모델을 WidowX 로봇에 적용했을 때, WidowX 전용 데이터만 쓴 것보다 성능이 향상되었다. 이는 형태가 다른 로봇 간에도 ’물체를 집는 법’과 같은 고차원적인 운동 기술(Motor Skill)이 전이될 수 있음을 증명한다.23</li>
<li><strong>성공률 데이터:</strong> 최근 연구인 <strong>UniSkill</strong>은 Franka 로봇의 데모를 보고 학습한 후, 완전히 다른 구조의 로봇에서 작업을 수행했을 때 91%의 높은 성공률을 달성했다고 보고했다.38 이는 단순한 모방을 넘어선 의미론적 기술 전이(Semantic Skill Transfer)가 일어나고 있음을 시사한다.</li>
</ul>
<h2>7.  향후 과제 및 결론</h2>
<h3>7.1  해결되지 않은 과제들</h3>
<ol>
<li><strong>촉각 및 힘 제어의 부재:</strong> 현재 대부분의 VLA 모델은 시각 정보에만 의존한다. 조립이나 정밀 조작에 필수적인 힘(Force)이나 촉각(Tactile) 정보의 통합은 아직 초기 단계이며, 이는 시각-언어 모델을 넘어선 ‘비전-언어-촉각-행동’ 모델로의 확장을 요구한다.</li>
<li><strong>모바일 조작의 완전한 통합:</strong> 로봇 팔 제어와 이동(Navigation)은 여전히 별개의 문제로 다뤄지는 경향이 있다. 두 행동 공간을 매끄럽게 통합하여 “부엌으로 가서 사과를 가져와라“와 같은 장기적이고 복합적인 작업을 수행하는 단일 모델 구축이 필요하다.39</li>
<li><strong>데이터의 롱테일(Long-tail) 문제:</strong> OXE 데이터셋도 특정 로봇(Franka 등)에 데이터가 편중되어 있다. 희귀한 형태의 로봇이나 특수한 작업에 대한 일반화 성능을 높이기 위해서는 데이터 불균형을 해소하거나, 소량의 데이터로도 적응 가능한 메타 러닝(Meta-Learning) 기술이 발전해야 한다.</li>
</ol>
<h3>7.2  결론: 범용 로봇을 향한 청사진</h3>
<p>비전-언어-행동(VLA) 모델을 통한 신체 전이 학습은 로봇 공학의 패러다임을 하드웨어 중심에서 데이터 및 소프트웨어 중심으로 근본적으로 변화시키고 있다. RT-X와 Open X-Embodiment가 ’다양한 로봇 데이터의 통합’이라는 첫 단추를 끼웠다면, OpenVLA, Octo, <span class="math math-inline">\pi_0</span>와 같은 최신 모델들은 ’효율적인 적응’과 ’정교한 제어’라는 다음 단계의 문제를 해결하고 있다.</p>
<p>특히 합성 데이터를 활용한 웜업(ET-VLA), 흐름 매칭을 통한 고빈도 제어(<span class="math math-inline">\pi_0</span>), 그리고 계층적 제어 구조(Helix)는 VLA 모델이 단순한 연구실 수준을 넘어 실제 산업 및 가정 현장에 투입될 수 있는 가능성을 높이고 있다. 향후 연구는 시각을 넘어선 다양한 감각의 통합, 그리고 더욱 이질적인 신체(드론, 휴머노이드 등)로의 확장에 초점을 맞출 것이며, 이는 궁극적으로 인간과 유사한 적응력을 가진 범용 로봇 지능의 탄생으로 이어질 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Vision-Language-Action (VLA) Models: The AI Brain Behind the Next Generation of Robots &amp; Physical AI | by RAKTIM SINGH | Nov, 2025 | Medium, https://medium.com/@raktims2210/vision-language-action-vla-models-the-ai-brain-behind-the-next-generation-of-robots-physical-bced48e8ae94</li>
<li>[2510.07773] Trajectory Conditioned Cross-embodiment Skill Transfer - arXiv, https://arxiv.org/abs/2510.07773</li>
<li>Vision-language-action model - Wikipedia, https://en.wikipedia.org/wiki/Vision-language-action_model</li>
<li>Vision Language Action Models (VLA) &amp; Policies for Robots - LearnOpenCV, https://learnopencv.com/vision-language-action-models-lerobot-policy/</li>
<li>[2511.01224] Embodiment Transfer Learning for Vision-Language-Action Models - arXiv, https://arxiv.org/abs/2511.01224</li>
<li>[2506.14608] Latent Action Diffusion for Cross-Embodiment Manipulation - arXiv, https://arxiv.org/abs/2506.14608</li>
<li>A Survey on Vision-Language-Action Models for Embodied AI - arXiv, https://arxiv.org/html/2405.14093v3</li>
<li>Vision-Language-Action Models: Concepts, Progress, Applications and Challenges - arXiv, https://arxiv.org/html/2505.04769v1</li>
<li>(PDF) RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, https://www.researchgate.net/publication/372784419_RT-2_Vision-Language-Action_Models_Transfer_Web_Knowledge_to_Robotic_Control</li>
<li>OpenVLA: An Open-Source Vision-Language-Action Model, https://openvla.github.io/</li>
<li>OpenVLA: An Open-Source Vision-Language-Action Model - arXiv, https://arxiv.org/html/2406.09246v3</li>
<li>OpenVLA: An Open-Source Vision-Language-Action Model - arXiv, https://arxiv.org/html/2406.09246v1</li>
<li>Octo: An Open-Source Generalist Robot Policy, https://www.roboticsproceedings.org/rss20/p090.pdf</li>
<li>Octo: An Open-Source Generalist Robot Policy, https://octo-models.github.io/</li>
<li>π0 and π0-FAST: Vision-Language-Action Models for General Robot Control, https://huggingface.co/blog/pi0</li>
<li>π 0 : Our First Generalist Policy - Physical Intelligence, https://www.physicalintelligence.company/blog/pi0</li>
<li>octo-models/octo: Octo is a transformer-based robot policy trained on a diverse mix of 800k robot trajectories. - GitHub, https://github.com/octo-models/octo</li>
<li>𝜋₀: A Vision-Language-Action Flow Model for General Robot Control - arXiv, https://arxiv.org/html/2410.24164v1</li>
<li>google-deepmind/open_x_embodiment - GitHub, https://github.com/google-deepmind/open_x_embodiment</li>
<li>Open X-Embodiment: The ImageNet of Robot Learning? | by Fotios (Fotis) Lygerakis, https://medium.com/@ligerfotis/open-x-embodiment-the-imagenet-of-robot-learning-e527e77de37c</li>
<li>Open X-Embodiment: Robotic Learning Datasets and RT-X Models, https://www.tri.global/research/open-x-embodiment-robotic-learning-datasets-and-rt-x-models</li>
<li>Open X-Embodiment: Robotic Learning Datasets and RT-X Models - arXiv, https://arxiv.org/html/2310.08864v4</li>
<li>Mirage: Cross-Embodiment Zero-Shot Policy Transfer with Cross-Painting - Robotics, https://www.roboticsproceedings.org/rss20/p069.pdf</li>
<li>Pushing the Limits of Cross-Embodiment Learning for Manipulation and Navigation, https://roboticsconference.org/2024/program/papers/93/</li>
<li>Pushing the Limits of Cross-Embodiment Learning for Manipulation and Navigation - arXiv, https://arxiv.org/html/2402.19432v1</li>
<li>Scaling up learning across many different robot types - Google DeepMind, https://deepmind.google/blog/scaling-up-learning-across-many-different-robot-types/</li>
<li>2toinf/X-VLA: The offical Implementation of “Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model” - GitHub, https://github.com/2toinf/X-VLA</li>
<li>X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model - arXiv, https://arxiv.org/html/2510.10274v1</li>
<li>Embodiment Transfer Learning for Vision-Language-Action Models - arXiv, https://arxiv.org/html/2511.01224v1</li>
<li>MIRAGE: Cross-Embodiment Zero-Shot Policy Transfer with Cross-Painting | Request PDF, https://www.researchgate.net/publication/383891537_MIRAGE_Cross-Embodiment_Zero-Shot_Policy_Transfer_with_Cross-Painting</li>
<li>FAST: Efficient Action Tokenization for Vision-Language-Action Models - arXiv, https://arxiv.org/html/2501.09747v1</li>
<li>When Would Vision-Proprioception Policy Fail in Robotic Manipulation? - OpenReview, https://openreview.net/forum?id=Nbj1GFCKB3</li>
<li>LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction - arXiv, https://arxiv.org/html/2506.13751v3</li>
<li>simpler-env/SimplerEnv: Evaluating and reproducing real-world robot manipulation policies (e.g., RT-1, RT-1-X, Octo) in simulation under common setups (e.g., Google Robot, WidowX+Bridge) (CoRL 2024) - GitHub, https://github.com/simpler-env/SimplerEnv</li>
<li>DisCrete Diffusion VLA: Bring - Pangram Labs, https://www.pangram.com/history/b41056a0-a8ce-4361-bf83-2c4b93bd17f9</li>
<li>NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards - arXiv, https://arxiv.org/html/2511.14659v1</li>
<li>Polybot: Training One Policy Across Robots While Embracing Variability, https://proceedings.mlr.press/v229/yang23c/yang23c.pdf</li>
<li>UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations - arXiv, https://arxiv.org/html/2505.08787v1</li>
<li>MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation, https://arxiv.org/html/2503.13446v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>