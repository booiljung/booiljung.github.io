<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:OpenVLA</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>OpenVLA</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">Vision-Language-Action(VLA) 모델</a> / <span>OpenVLA</span></nav>
                </div>
            </header>
            <article>
                <h1>OpenVLA</h1>
<p>2025-12-09, G30DR</p>
<h2>1.  서론: 로봇 파운데이션 모델의 민주화와 패러다임의 전환</h2>
<p>인공지능(AI) 기술의 비약적인 발전은 텍스트와 이미지를 이해하는 멀티모달 모델의 탄생을 이끌었으며, 이제 그 파고는 물리적 세계와 상호작용하는 로봇 공학 분야로 확장되고 있다. 과거의 로봇 제어 시스템이 특정 작업과 환경에 국한된 ‘전용(Specialist)’ 모델이었다면, 현재는 다양한 환경과 임무를 수행할 수 있는 ‘범용(Generalist)’ 로봇 정책으로의 전환이 가속화되고 있다. 이러한 흐름의 중심에는 시각적 인식(Vision)과 언어적 추론(Language)을 물리적 행동(Action)으로 직접 변환하는 ‘시각-언어-행동(Vision-Language-Action, VLA)’ 모델이 존재한다.1</p>
<p>Google DeepMind가 RT-2를 통해 VLA 모델의 가능성을 입증했으나, 해당 모델은 폐쇄적이고 방대한 계산 자원을 요구하여 일반 연구자들의 접근이 사실상 불가능했다. 이러한 배경에서 등장한 <strong>OpenVLA</strong>는 70억(7B) 매개변수를 가진 Llama 2 기반의 오픈 소스 VLA 모델로서, 97만 개 이상의 로봇 에피소드로 구성된 Open X-Embodiment 데이터셋을 통해 학습되었다.1 OpenVLA는 단순히 성능 좋은 모델을 공개한 것을 넘어, 소비자용 GPU에서도 미세 조정(Fine-tuning)과 추론이 가능한 효율적인 파이프라인을 제공함으로써 로봇 연구의 진입 장벽을 획기적으로 낮추었다.4</p>
<p>본 보고서는 OpenVLA의 기술적 아키텍처, 데이터 처리 파이프라인, 훈련 전략, 그리고 최신 최적화 기법인 OFT(Optimized Fine-Tuning)와 FAST 토크나이저 기술까지 포괄적으로 분석한다. 또한, 경쟁 모델인 Octo, RT-2, 그리고 최근 부상하는 π0(Pi-zero)와의 비교를 통해 OpenVLA가 로봇 공학 생태계에 미치는 영향과 미래 전망을 심층적으로 논의한다.</p>
<h2>2.  기술적 아키텍처: 통합된 인지-행동 시스템의 설계</h2>
<p>OpenVLA의 아키텍처는 기존의 ‘구성 요소 결합(Stitching)’ 방식과 차별화된다. Octo와 같은 이전 세대 모델들이 사전 학습된 언어 임베딩이나 시각 인코더를 가져와 새로운 정책 헤드(Policy Head)와 결합하여 학습시켰다면, OpenVLA는 시각-언어 모델(VLM) 전체를 로봇 제어를 위해 ’End-to-End’로 미세 조정하는 방식을 채택한다.5 이는 로봇의 행동을 언어 토큰과 동일한 위상으로 처리함으로써, LLM이 가진 추론 능력과 세계 지식을 물리적 제어에 직접적으로 전이시키는 것을 가능하게 한다.</p>
<h3>2.1  이중 시각 인코더 전략: 공간 인식과 의미론적 이해의 결합</h3>
<p>로봇 조작(Manipulation) 작업은 두 가지 상반된 시각적 능력을 요구한다. 하나는 “빨간 사과“가 무엇인지 아는 ‘의미론적(Semantic)’ 이해이고, 다른 하나는 그 사과가 정확히 어디에 위치해 있는지 파악하는 ‘기하학적(Geometric)’ 또는 ‘공간적(Spatial)’ 인식이다. OpenVLA는 이를 충족시키기 위해 <strong>Prismatic</strong> VLM 아키텍처에 기반하여 두 개의 서로 다른 비전 트랜스포머(ViT)를 결합하는 독창적인 전략을 사용한다.1</p>
<ul>
<li><strong>DINOv2 (ViT-L/14):</strong> 자기지도 학습(Self-supervised Learning)으로 훈련된 DINOv2는 이미지 내 객체 간의 공간적 관계, 깊이 정보, 그리고 세밀한 기하학적 특징을 포착하는 데 탁월하다. 이는 로봇이 물체를 정확하게 파지(Grasping)하거나 장애물을 회피하는 데 필수적인 저수준(Low-level) 시각 정보를 제공한다.1</li>
<li><strong>SigLIP (ViT-So400M/14):</strong> SigLIP은 이미지와 텍스트 간의 정렬(Alignment)을 목표로 학습된 모델로, CLIP보다 개선된 성능을 보인다. 이 모델은 사용자의 자연어 명령과 시각적 입력 간의 의미적 연결고리를 형성하여, 로봇이 “투명한 컵“이나 “구겨진 종이“와 같은 추상적 개념을 시각 정보와 매칭할 수 있게 한다.1</li>
</ul>
<p>이 두 인코더에서 추출된 특징 맵(Feature Maps)은 채널 차원에서 연결(Concatenate)된 후, 다층 퍼셉트론(MLP) 기반의 투사기(Projector)를 통해 언어 모델의 임베딩 공간으로 매핑된다.9 이러한 융합(Fusion) 전략은 단일 인코더를 사용할 때보다 로봇 제어 성공률을 유의미하게 향상시키는 핵심 요인으로 분석된다.</p>
<h3>2.2  언어 모델 백본과 추론 엔진</h3>
<p>OpenVLA의 두뇌 역할을 하는 것은 Meta의 <strong>Llama 2 7B</strong> 모델이다.1 이 모델은 방대한 인터넷 텍스트 데이터로 사전 학습되어 있어 복잡한 지시사항을 해석하고, 단계별 추론(Reasoning)을 수행할 수 있는 능력을 갖추고 있다. OpenVLA 훈련 과정에서 이 언어 모델의 가중치는 동결되지 않고 미세 조정되는데, 이는 시각적 입력이 단순한 이미지가 아니라 로봇의 현재 상태(State)와 목표(Goal)를 나타내는 맥락 정보로 언어 모델 내부에서 통합되도록 유도한다.10</p>
<h3>2.3  행동 공간의 토큰화 및 이산화 메커니즘</h3>
<p>언어 모델은 근본적으로 이산적인(Discrete) 토큰을 출력하는 구조를 가진다. 반면, 로봇의 관절 제어나 말단 장치(End-effector)의 좌표는 연속적인(Continuous) 실수 값을 필요로 한다. 이 간극을 메우기 위해 OpenVLA는 RT-2에서 제안된 <strong>행동 이산화(Action Discretization)</strong> 방식을 채택하고 개선하였다.2</p>
<p>OpenVLA는 7 자유도(7-DoF)의 행동 공간을 정의하며, 이는 <span class="math math-inline">\Delta x, \Delta y, \Delta z</span>, 롤(Roll), 피치(Pitch), 요(Yaw), 그리퍼(Gripper) 개폐 상태로 구성된다.1 각 차원은 256개의 구간(Bin)으로 나뉘며, 모델은 각 타임스텝마다 7개의 토큰을 순차적으로 생성한다.</p>
<p>여기서 주목할 점은 <strong>분위수(Quantile) 기반의 이산화</strong> 전략이다. 9과 5에 따르면, OpenVLA는 훈련 데이터의 전체 통계 분포를 분석하여 1분위(1st percentile)에서 99분위(99th percentile) 사이의 값을 256개 구간으로 균등하게 나눈다.</p>
<ul>
<li><strong>기존 방식(Uniform Binning)의 한계:</strong> 단순히 데이터의 최소값과 최대값을 기준으로 구간을 나누면, 극단적인 이상치(Outlier)로 인해 대부분의 정상적인 동작 값이 소수의 구간에 뭉쳐버리는 현상이 발생하여 제어의 정밀도가 떨어진다.</li>
<li><strong>분위수 방식의 장점:</strong> 이상치를 배제하고 실제 동작이 빈번하게 일어나는 구간에 더 많은 해상도를 할당함으로써, 로봇의 미세한 움직임을 효과적으로 표현할 수 있다.</li>
</ul>
<p>생성된 토큰은 Llama 2의 기존 어휘(Vocabulary)에 추가된 특수 행동 토큰(Extra Action Tokens)이나, 사용 빈도가 낮은 마지막 256개 토큰을 덮어쓰는 방식으로 매핑된다.9</p>
<h2>3.  대규모 데이터셋과 훈련 방법론</h2>
<p>OpenVLA의 강력한 일반화 성능은 모델 아키텍처뿐만 아니라 데이터의 규모와 다양성에서 기인한다. 단일 로봇 데이터로 학습된 모델은 환경이 조금만 바뀌어도 실패하기 쉽지만, OpenVLA는 다양한 로봇과 환경을 아우르는 데이터셋을 통해 ’로봇 공학의 시각적 상식’을 학습했다.</p>
<h3>3.1  Open X-Embodiment 데이터셋의 활용</h3>
<p>OpenVLA는 <strong>Open X-Embodiment</strong> 데이터셋에서 선별된 약 970,000개(970k)의 로봇 조작 궤적(Trajectory)을 사용하여 학습되었다.1 이 데이터셋은 70개 이상의 서로 다른 로봇 데이터셋의 집합체로, 다음과 같은 다양성을 제공한다:</p>
<ul>
<li><strong>기종 다양성:</strong> Franka Emika Panda, UR5, WidowX 등 다양한 형태(Morphology)를 가진 로봇들의 데이터가 포함되어 있어, 특정 기구학적 구조에 과적합되는 것을 방지한다.</li>
<li><strong>환경 다양성:</strong> 실험실 환경, 가정 환경, 부엌 등 다양한 배경과 조명 조건이 포함되어 시각적 강건성(Robustness)을 높인다.</li>
<li><strong>작업 다양성:</strong> 단순한 집기(Pick-and-Place)부터 서랍 열기, 물체 밀기, 도구 사용하기 등 광범위한 작업 기술을 포괄한다.9</li>
</ul>
<p>훈련 과정에서는 데이터의 질을 높이기 위해 동작이 없는(All-zero actions) 구간을 필터링하고, 데이터셋 간의 불균형을 해소하기 위해 옥토(Octo) 정책에서 영감을 받은 데이터 재가중(Reweighting) 기법이 적용되었다.13</p>
<h3>3.2  훈련 하이퍼파라미터 및 인프라 최적화</h3>
<p>OpenVLA의 훈련은 대규모 언어 모델의 표준적인 사전 학습 절차를 따르되, 로봇 제어의 특수성을 고려하여 조정되었다.</p>
<ul>
<li><strong>학습률(Learning Rate):</strong> 연구진은 광범위한 실험 끝에 <strong>2e-5</strong>의 고정 학습률(Constant Learning Rate)을 채택했다. 이는 VLM 백본의 사전 학습 시 사용된 학습률과 동일한 값이다. 흥미롭게도 학습률 웜업(Warmup)이나 코사인 감쇠(Cosine Decay) 스케줄러는 로봇 제어 성능 향상에 기여하지 않는 것으로 나타났다.5</li>
<li><strong>에포크(Epochs):</strong> 모델은 전체 데이터셋에 대해 <strong>27 에포크</strong> 동안 학습되었으며, 이는 데이터의 다양성을 충분히 흡수하면서도 과적합을 피하는 최적점으로 판단된다.5</li>
<li><strong>컴퓨팅 인프라:</strong> 대규모 모델 훈련을 위해 A100 또는 H100 GPU 클러스터가 사용되었으며, PyTorch의 FSDP(Fully Sharded Data Parallelism)와 FlashAttention-2 기술을 적용하여 메모리 효율성과 훈련 속도를 극대화했다.6</li>
</ul>
<h2>4.  효율적인 적응과 배포: LoRA 및 양자화 전략</h2>
<p>OpenVLA의 가장 실용적인 기여는 고성능 모델을 일반 소비자용 하드웨어에서도 운용 가능하게 만들었다는 점이다. 기존의 55B 규모 RT-2-X 모델은 추론과 미세 조정에 막대한 자원이 필요했지만, OpenVLA는 파라미터 효율적 미세 조정(PEFT)과 양자화 기술을 통해 이러한 제약을 극복했다.</p>
<h3>4.1  LoRA(Low-Rank Adaptation)를 통한 미세 조정</h3>
<p>새로운 로봇이나 작업에 모델을 적응시키기 위해 전체 파라미터를 재학습(Full Fine-tuning)하는 것은 비효율적이다. OpenVLA는 <strong>LoRA</strong> 기술을 적극 활용하여 적은 수의 파라미터만 업데이트하면서도 전체 미세 조정에 버금가는 성능을 달성한다.</p>
<table><thead><tr><th><strong>전략 (Strategy)</strong></th><th><strong>랭크 (Rank)</strong></th><th><strong>성공률 (Success Rate)</strong></th><th><strong>학습 파라미터 수</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>Full Fine-Tuning</strong></td><td>-</td><td>69.7 ± 7.2%</td><td>6,760M</td><td>막대한 VRAM 필요</td></tr>
<tr><td><strong>LoRA</strong></td><td>32</td><td>68.2 ± 7.5%</td><td>97.6M</td><td>소비자 GPU 가능</td></tr>
<tr><td><strong>LoRA</strong></td><td>64</td><td>68.2 ± 7.8%</td><td>195.2M</td><td>성능 포화 상태</td></tr>
<tr><td><strong>Frozen Vision</strong></td><td>-</td><td>47.0 ± 6.9%</td><td>-</td><td>시각 인코더 동결 시 성능 저하</td></tr>
</tbody></table>
<p>데이터 출처: 6</p>
<p>위 표에서 볼 수 있듯이, LoRA Rank 32를 사용했을 때의 성공률은 전체 미세 조정과 통계적으로 유의미한 차이가 없다. 이는 모델의 핵심 지식은 유지하면서 로봇 제어에 필요한 적응(Adaptation)만 효율적으로 수행할 수 있음을 시사한다. 또한, LoRA 적용 시 시각 인코더를 동결(Frozen)하면 성능이 급격히 하락하는데(47.0%), 이는 VLA 모델에서 시각적 표현을 로봇 작업에 맞게 미세 조정하는 것이 필수적임을 보여준다.6</p>
<h3>4.2  양자화(Quantization)와 메모리 효율성</h3>
<p>로봇에 탑재되는 온보드(On-board) 컴퓨터는 통상적으로 데이터센터의 GPU보다 성능이 낮다. OpenVLA는 4-bit 및 8-bit 양자화를 지원하여 모델의 메모리 발자국(Memory Footprint)을 줄인다.</p>
<ul>
<li><strong>VRAM 요구사항의 변화:</strong></li>
<li><strong>BF16 (16-bit):</strong> 추론 시 약 <strong>16.8 GB</strong> VRAM 소요. (RTX 3090/4090 필요)</li>
<li><strong>Int8 (8-bit):</strong> 약 <strong>10.2 GB</strong> 소요. (RTX 3080 Ti 등)</li>
<li><strong>Int4 (4-bit):</strong> 약 <strong>7.0 GB</strong> 소요. (RTX 3070, 4060 Ti 등 보급형 GPU에서도 구동 가능).5</li>
<li><strong>성능 보존:</strong> 놀랍게도 4-bit 양자화를 적용했을 때의 작업 성공률(71.9%)이 원본 16-bit 모델(71.3%)과 거의 동일하거나 오차 범위 내에서 유지된다.5 이는 언어 모델의 파라미터 중 상당수가 로봇 제어 작업에 있어 중복적(Redundant)일 수 있으며, 핵심적인 제어 정책은 낮은 비트 수로도 충분히 표현 가능함을 시사한다. 이는 로봇 공학의 민주화(Democratization) 측면에서 매우 중요한 발견이다.</li>
</ul>
<h2>5.  성능 벤치마크 및 경쟁 모델 비교</h2>
<p>OpenVLA의 성능은 시뮬레이션 환경과 실제 로봇 환경(Real-world) 모두에서 광범위하게 검증되었다. 특히 Google의 RT-2-X 및 Octo와의 비교는 OpenVLA의 상대적 우위를 명확히 보여준다.</p>
<h3>5.1  BridgeData V2 및 Google Robot 평가</h3>
<p>29개의 다양한 작업을 포함하는 실제 로봇 평가에서 OpenVLA는 압도적인 성능을 기록했다.</p>
<ul>
<li><strong>RT-2-X 대비:</strong> 550억(55B) 파라미터를 가진 RT-2-X에 비해 7B 파라미터의 OpenVLA는 29개 작업 평균 성공률에서 <strong>16.5% 더 높은 성과</strong>를 보였다.4 이는 단순히 파라미터 수가 많은 것이 로봇 제어 성능을 보장하지 않으며, 효율적인 아키텍처와 데이터 큐레이션이 더 중요함을 입증한다.</li>
<li><strong>Octo 및 RT-1 대비:</strong> Octo 모델은 낯선 물체가 등장하거나 배경에 방해 요소(Distractors)가 있을 때 물체를 제대로 인식하지 못하고 허공을 휘젓는(Wave aimlessly) 등의 실패 양상을 보였으나, OpenVLA는 이러한 상황에서도 강건한 수행 능력을 보였다.5</li>
<li><strong>확산 정책(Diffusion Policy) 대비:</strong> 모방 학습의 강자인 Diffusion Policy와 비교했을 때, OpenVLA는 다중 객체 조작이나 언어적 지시가 복잡한 과제에서 <strong>20.4% 더 높은 성능</strong>을 기록했다.4 이는 VLM 백본이 제공하는 상식적 추론 능력이 복잡한 작업 계획(Planning)에 도움을 주기 때문이다.</li>
</ul>
<h3>5.2  LIBERO 시뮬레이션 벤치마크</h3>
<p>LIBERO 벤치마크는 로봇 학습의 전이(Transfer) 능력과 장기(Long-horizon) 과제 수행 능력을 평가한다.</p>
<ul>
<li>OpenVLA는 LIBERO-Spatial 및 LIBERO-Object와 같은 하위 과제에서 기존 베이스라인 모델들을 상회하는 결과를 보였다.16</li>
<li>특히 최신 연구인 <strong>OpenVLA-OFT</strong>를 적용할 경우, LIBERO 벤치마크에서의 성능 격차는 더욱 벌어지며, 이는 OpenVLA가 단순한 베이스라인을 넘어 지속적으로 발전 가능한 플랫폼임을 보여준다.17</li>
</ul>
<h3>5.3  최신 경쟁 모델과의 비교: π0 (Pi-zero)</h3>
<p>최근 Physical Intelligence 사에서 발표한 <strong>π0</strong> 모델은 VLM 백본에 흐름 매칭(Flow Matching) 기법을 적용하여 연속적인 행동을 생성한다.18</p>
<ul>
<li><strong>연속성 vs 이산성:</strong> OpenVLA는 행동을 이산적인 토큰으로 끊어서 생성하는 반면, π0는 부드러운 연속 궤적을 생성한다. 이론적으로는 연속적 생성이 더 유리할 수 있으나, OpenVLA는 방대한 데이터 학습을 통해 이산화의 한계를 극복하고 있다.</li>
<li><strong>접근성:</strong> π0나 RT-2는 여전히 폐쇄적이거나 제한적으로 공개된 반면, OpenVLA는 모델 가중치, 훈련 코드, 데이터셋 큐레이션 과정 전체가 공개되어 있어 연구 커뮤니티의 표준(Standard)으로 자리 잡았다.</li>
</ul>
<table><thead><tr><th><strong>모델</strong></th><th><strong>파라미터</strong></th><th><strong>아키텍처</strong></th><th><strong>행동 생성 방식</strong></th><th><strong>공개 여부</strong></th><th><strong>특징</strong></th></tr></thead><tbody>
<tr><td><strong>OpenVLA</strong></td><td>7B</td><td>Llama 2 + SigLIP/DINOv2</td><td>이산적 토큰 (Autoregressive)</td><td><strong>완전 공개</strong></td><td>높은 접근성, 효율적 미세조정</td></tr>
<tr><td><strong>RT-2-X</strong></td><td>55B</td><td>PaLM-E 기반</td><td>이산적 토큰</td><td>비공개</td><td>대규모 모델, 높은 자원 요구</td></tr>
<tr><td><strong>Octo</strong></td><td>다양</td><td>Transformer/Diffusion</td><td>Diffusion/Token</td><td>공개</td><td>구성 요소 조립 방식 (Stitching)</td></tr>
<tr><td><strong>π0 (Pi-zero)</strong></td><td>3B/Small</td><td>PaliGemma 기반</td><td>Flow Matching (연속)</td><td>일부 공개</td><td>연속적 동작 생성 강점</td></tr>
</tbody></table>
<p>데이터 종합: 1</p>
<h2>6.  발전된 연구 방향: OFT와 FAST 토크나이저</h2>
<p>OpenVLA 출시 이후, 커뮤니티는 이 모델의 한계를 극복하기 위한 새로운 기술들을 접목하고 있다. 이는 OpenVLA가 정적인 모델이 아니라 진화하는 생태계임을 보여준다.</p>
<h3>6.1  OFT (Optimized Fine-Tuning) 레시피</h3>
<p>2025년 발표된 OFT 연구는 OpenVLA의 미세 조정 방식을 혁신적으로 개선했다.14</p>
<ul>
<li><strong>연속적 행동 표현:</strong> 기존의 토큰 기반 이산화를 넘어, 연속적인 값을 회귀(Regression) 방식으로 예측하도록 헤드를 수정하거나 확산 모델을 결합하는 시도가 포함된다.</li>
<li><strong>병렬 디코딩 &amp; 청킹:</strong> 한 번에 하나의 토큰만 생성하는 자기회귀 방식은 속도가 느리다. OFT는 행동 덩어리(Chunk)를 한 번에 예측하거나 병렬로 디코딩하여 추론 속도를 <strong>25~50배</strong> 가속화했다. 이는 실시간 제어(Real-time Control)가 필수적인 로봇 분야에서 결정적인 개선점이다.</li>
<li><strong>양팔 제어(Bimanual Control):</strong> OFT를 통해 고주파수의 양팔 로봇 제어까지 가능해짐으로써, OpenVLA의 적용 범위가 단순한 외팔 로봇을 넘어 휴머노이드 상체 제어 등으로 확장되고 있다.</li>
</ul>
<h3>6.2  FAST (Fast Action Tokenization)</h3>
<p>행동 토큰화의 효율성을 높이기 위해 제안된 FAST 기술은 행동 시퀀스를 더 적은 수의 토큰으로 압축한다.14 기존 256-bin 방식이 모든 차원을 개별 토큰으로 처리했다면, FAST는 행동의 패턴을 학습하여 압축률을 높임으로써 추론 속도를 최대 15배까지 향상시킨다. 이는 언어 모델의 문맥 길이(Context Length) 제한을 완화하고 더 긴 호흡의 작업을 수행할 수 있게 돕는다.</p>
<h2>7.  배포 및 실무 가이드</h2>
<p>연구자나 개발자가 OpenVLA를 실제로 활용하기 위한 절차는 HuggingFace 생태계와 긴밀히 통합되어 매우 간소화되었다.</p>
<h3>7.1  추론 파이프라인의 구조</h3>
<p>OpenVLA의 추론은 다음과 같은 표준화된 코드로 수행될 수 있다.7</p>
<ol>
<li><strong>모델 로드:</strong> <code>AutoModelForVision2Seq</code> 클래스를 사용하여 모델을 로드한다. 이때 <code>load_in_4bit=True</code> 옵션을 통해 4-bit 양자화를 활성화하면 VRAM 사용량을 7GB 수준으로 낮출 수 있다.</li>
<li><strong>입력 구성:</strong> 로봇의 카메라에서 획득한 이미지를 전처리하고, 사용자의 명령을 프롬프트 템플릿(“In: What action should the robot take to {instruction}?”)에 삽입한다.</li>
<li><strong>행동 예측:</strong> <code>model.predict_action()</code> 메서드를 호출하면 모델은 내부적으로 시각-언어 처리를 수행하고 7-DoF 행동 벡터를 반환한다.</li>
<li><strong>역정규화(Un-normalization):</strong> 모델이 출력한 값은 학습 데이터의 통계에 기반하여 정규화된 값이므로, 이를 실제 로봇의 물리적 제어 값(관절 각도 등)으로 변환하는 과정을 거친다.1</li>
</ol>
<h3>7.2  문제 해결 및 팁</h3>
<ul>
<li><strong>성능 저하 시:</strong> 미세 조정 후 성능이 기대에 미치지 못한다면, 데이터셋의 다양성이 부족하거나 특정 동작에 과적합되었을 가능성이 높다. 이 경우 LoRA의 Rank를 조절하거나 데이터 재가중(Reweighting) 비율을 조정하는 것이 권장된다.14</li>
<li><strong>하드웨어 호환성:</strong> NVIDIA GPU 외의 하드웨어 지원은 아직 제한적이나, PyTorch 생태계의 확장에 따라 개선될 여지가 있다. 현재로서는 CUDA 지원 GPU가 필수적이다.</li>
</ul>
<h2>8.  결론 및 향후 전망</h2>
<p>OpenVLA는 로봇 공학 분야에서 “리눅스(Linux)“와 같은 결정적인 모멘텀을 제공했다. 그동안 소수의 빅테크 기업만이 점유했던 고성능 VLA 모델의 기술을 오픈 소스로 공개함으로써, 전 세계의 연구자들이 이를 기반으로 새로운 아이디어를 실험하고 발전시킬 수 있는 토대를 마련했다.</p>
<p>본 보고서의 분석을 통해 도출된 핵심적인 결론은 다음과 같다.</p>
<ol>
<li><strong>범용성의 실현:</strong> 인터넷 규모의 텍스트와 이미지로 사전 학습된 지능(Llama 2, SigLIP, DINOv2)이 로봇의 물리적 제어 능력으로 성공적으로 전이될 수 있음을 증명했다. 이는 로봇을 처음부터(Scratch) 학습시키는 시대가 저물고 있음을 시사한다.</li>
<li><strong>접근성의 혁명:</strong> 4-bit 양자화와 LoRA 미세 조정을 통해 고가의 장비 없이도 최신 AI 로봇 기술을 활용할 수 있게 되었다. 이는 대학 연구실, 스타트업, 개인 개발자들에게 강력한 도구를 쥐여준 셈이다.</li>
<li><strong>지속 가능한 생태계:</strong> OFT, FAST와 같은 후속 연구들이 OpenVLA를 플랫폼으로 삼아 쏟아져 나오고 있다. 이는 OpenVLA가 단발성 모델이 아니라, 로봇 파운데이션 모델의 표준(Standard)으로 진화하고 있음을 보여준다.</li>
</ol>
<p>향후 로봇 공학은 OpenVLA와 같은 파운데이션 모델을 기반으로 더 복잡하고 장기적인 작업을 수행하는 자율 에이전트(Autonomous Agent) 개발로 나아갈 것이다. OpenVLA는 그 여정의 가장 중요한 이정표이자, 현재 가장 강력하고 실용적인 도구임이 분명하다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>openvla/openvla-7b - Hugging Face, https://huggingface.co/openvla/openvla-7b</li>
<li>Vision-language-action model - Wikipedia, https://en.wikipedia.org/wiki/Vision-language-action_model</li>
<li>OpenVLA: An Open-Source Vision-Language-Action Model, https://openvla.github.io/</li>
<li>[2406.09246] OpenVLA: An Open-Source Vision-Language-Action Model - arXiv, https://arxiv.org/abs/2406.09246</li>
<li>OpenVLA: An Open-Source Vision-Language-Action Model - arXiv, https://arxiv.org/html/2406.09246v3</li>
<li>OpenVLA: An Open-Source Vision-Language-Action Model - arXiv, https://arxiv.org/html/2406.09246v1</li>
<li>Vision Language Action Models (VLA) &amp; Policies for Robots - LearnOpenCV, https://learnopencv.com/vision-language-action-models-lerobot-policy/</li>
<li>VLA: Part 2: OpenVLA - Medium, https://medium.com/@yianyao1994/vla-part-2-openvla-b631a29efecc</li>
<li>[Paper Review] OpenVLA: An Open-Source Vision-Language-Action Model - bequiet-log, https://bequiet-log.vercel.app/openvla</li>
<li>Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies - arXiv, https://arxiv.org/html/2508.20072v1</li>
<li>Stanford-ILIAD/openvla-mini: OpenVLA: An open-source vision-language-action model for robotic manipulation. - GitHub, https://github.com/Stanford-ILIAD/openvla-mini</li>
<li>OpenVLA: An Open-Source Vision-Language-Action Model - Research, https://groups.csail.mit.edu/robotics-center/public_papers/Kim24.pdf</li>
<li>OpenVLA: Open Source VLA for Robotics - Emergent Mind, https://www.emergentmind.com/topics/openvla</li>
<li>OpenVLA: An open-source vision-language-action model for robotic manipulation. - GitHub, https://github.com/openvla/openvla</li>
<li>OpenVLA: An Open-Source Vision-Language-Action Model - arXiv, https://arxiv.org/html/2406.09246v2</li>
<li>Sylvest/openvla-7b-oft-finetuned-libero-plus-mixdata - Hugging Face, https://huggingface.co/Sylvest/openvla-7b-oft-finetuned-libero-plus-mixdata</li>
<li>Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success, https://openvla-oft.github.io/</li>
<li>Comparing 5 Pioneering Robotics Foundation Models for ML-Based Control | by Genki Sano (Co-founder &amp; CTO, Telexistence Inc.) | Medium, https://medium.com/@genki-sano/a-practical-comparison-of-five-leading-ml-based-robotics-control-approaches-49e1977dd3ec</li>
<li>VLAs that Train Fast, Run Fast, and Generalize Better - Physical Intelligence, https://www.physicalintelligence.company/research/knowledge_insulation</li>
<li>Experiences from Benchmarking Vision–Language–Action Models for Robotic Manipulation - arXiv, https://arxiv.org/html/2511.11298v1</li>
<li>[2502.19645] Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success, https://arxiv.org/abs/2502.19645</li>
<li>(PDF) FAST: Efficient Action Tokenization for Vision-Language-Action Models, https://www.researchgate.net/publication/388081369_FAST_Efficient_Action_Tokenization_for_Vision-Language-Action_Models</li>
<li>OpenVLA: An open-source vision-language-action model for robotic manipulation. - GitHub, https://github.com/reazon-research/openvla</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>