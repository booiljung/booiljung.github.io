<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Vision-Language-Action(VLA) 모델 (2025-10-08)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Vision-Language-Action(VLA) 모델 (2025-10-08)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">Vision-Language-Action(VLA) 모델</a> / <span>Vision-Language-Action(VLA) 모델 (2025-10-08)</span></nav>
                </div>
            </header>
            <article>
                <h1>Vision-Language-Action(VLA) 모델 (2025-10-08)</h1>
<h2>1.  통합적 체화 인공지능의 패러다임 전환</h2>
<h3>1.1  VLA 모델의 정의와 핵심 비전</h3>
<p>Vision-Language-Action (VLA) 모델은 시각적 인식(Vision), 자연어 이해(Language), 그리고 물리적 행동(Action)이라는 세 가지 핵심 양식(modality)을 단일 계산 프레임워크 내에서 통합하는 것을 목표로 하는 변혁적인 인공지능 패러다임이다.1 이는 단순히 보고, 이해하고, 행동하는 개별 능력을 나열하는 것을 넘어, 이 세 가지 양식을 유기적으로 융합하여 로봇이 복잡하고 동적인 실제 환경에서 지능적으로 상호작용할 수 있도록 하는 데 그 본질이 있다.3 VLA 모델은 로봇이 주변 환경을 공동으로 인식하고, 언어적 지시를 이해하며, 그에 맞는 행동을 수행하게 함으로써 이전 접근법들의 단편적인 한계를 극복하고, 적응적이며 일반화 가능한 지능형 체화 에이전트(embodied agent)를 향한 중요한 진전을 이룬다.1</p>
<p>VLA 모델의 핵심 비전은 범용 로봇(general-purpose robot)의 실현에 있다. 예를 들어, 사용자가 “테이블 위에 있는 빨간 컵을 집어서 선반 위에 올려줘“와 같은 높은 수준의 자연어 명령을 내렸을 때, VLA 모델로 구동되는 로봇은 다음과 같은 일련의 통합된 과정을 수행한다. 먼저, 시각 입력을 통해 ’빨간 컵’과 ’선반’이라는 객체를 정확히 식별하고 그들의 공간적 관계를 파악한다 (Vision). 다음으로, “집어서”, “위에 올려줘“라는 언어적 지시의 의도를 파악하고, 이를 시각적 맥락과 결합하여 구체적인 행동 계획을 수립한다 (Language). 마지막으로, 이 계획을 로봇 팔의 관절 각도, 그리퍼의 개폐 등 일련의 정밀한 모터 제어 명령으로 변환하여 과업을 완수한다 (Action).7 이처럼 VLA는 인간과 로봇 간의 상호작용을 직관적으로 만들고, 로봇이 비정형 환경에서도 유연하게 대처할 수 있는 능력을 부여하는 것을 궁극적인 목표로 한다.</p>
<h3>1.2  기존 접근법의 한계와 VLA의 필요성</h3>
<p>VLA 모델이 등장하기 이전, 로봇 공학과 인공지능 연구는 주로 분절된 영역에서 개별적으로 발전해왔다. 컴퓨터 비전 시스템은 객체를 인식하고 분류하는 데 탁월한 성능을 보였고 1, 자연어 처리(NLP) 시스템은 텍스트를 이해하고 생성하는 능력을 발전시켰으며 1, 제어 시스템은 사전에 정의된 특정 동작을 정밀하게 수행하는 데 초점을 맞추었다.1 이러한 시스템들은 각자의 영역에서는 인상적인 성과를 거두었지만, 이들을 하나의 로봇 시스템으로 통합하는 과정에서 근본적인 한계에 부딪혔다.</p>
<p>이러한 분절된 파이프라인(fragmented pipeline) 구조는 각 모듈이 독립적으로 작동하여 정보를 순차적으로 전달하는 방식으로 이루어졌다. 예를 들어, 로봇은 시각 모듈을 통해 “사과“를 인식하고, 언어 모듈을 통해 “사과를 집어라“라는 명령을 이해하며, 행동 모듈을 통해 미리 프로그래밍된 “움켜쥐는” 동작을 수행할 수 있었다. 그러나 이 세 가지 능력을 유기적으로 결합하여 예측 불가능한 실제 환경의 변화에 실시간으로 적응하는 유연한 행동을 생성하는 것은 거의 불가능했다.1 이는 마치 서로 소통하지 않고 각자의 일만 처리하는 작업자들로 구성된 공장 라인과 같아서, 전체적인 효율성과 강건성이 떨어지는 문제를 낳았다.5 만약 사과가 약간 다른 위치에 있거나, 조명이 바뀌거나, “그 과일을 집어줘“와 같이 모호한 명령이 주어지면 전체 시스템은 쉽게 실패했다.</p>
<p>VLA 모델은 이러한 단편화를 극복하기 위한 통합적 해결책으로 등장했다. VLA는 인식, 추론, 제어를 분리된 모듈이 아닌 하나의 종단간(end-to-end) 정책으로 학습한다. 이를 통해 시각적 맥락 속에서 언어의 의미를 파악하고(grounding), 이를 즉각적으로 행동 계획에 반영할 수 있게 된다. 이러한 통합적 접근은 시스템 설계를 대폭 단순화하고, 각 모듈 간의 정보 손실을 최소화하며, 전체 시스템의 강건성(robustness)을 획기적으로 향상시킨다.1</p>
<p>결과적으로, VLA의 등장은 단순히 기술적 통합을 넘어 로봇 지능에 대한 철학적 전환을 의미한다. 과거의 로봇 지능이 수작업으로 설계된 제어 정책이나 명시적인 프로그래밍에 의존했다면 7, 머신러닝 도입 이후에도 각 기능이 분리된 파이프라인 형태에 머물렀다.1 VLA는 이러한 패러다임에서 벗어나, 대규모 언어 모델(LLM)과 비전-언어 모델(VLM)의 성공을 발판으로 삼는다.3 VLM은 웹 스케일의 방대한 이미지-텍스트 쌍 데이터를 통해 시각적, 언어적 개념에 대한 풍부한 사전 지식, 즉 일종의 ’상식’을 학습했다.1 RT-2와 같은 선구적인 VLA 모델의 핵심 아이디어는 이 사전 지식을 ’행동(Action)’이라는 새로운 양식으로 확장하는 것이었다.12 즉, “컵“이라는 단어와 컵의 이미지를 연결하는 능력을 “컵을 잡는 행동“과 직접 연결하는 것이다. 따라서 VLA는 로봇이 더 이상 고립된 환경에서 소규모 데이터로만 학습하는 것이 아니라, 인류가 인터넷에 축적한 방대한 시각적, 언어적 지식을 물리적 행동의 기반으로 삼게 만드는 근본적인 변화를 나타낸다. 이는 로봇 지능이 ’상식’을 획득하는 방식을 바꾸는 패러다임 전환이라 할 수 있다.13</p>
<h3>1.3  보고서의 구조와 목표</h3>
<p>본 보고서는 VLA 모델 분야의 전반을 체계적으로 조망하고 심도 있는 분석을 제공하는 것을 목표로 한다. 이를 위해 보고서는 다음과 같은 구조로 구성된다.</p>
<ul>
<li><strong>II장</strong>에서는 VLA 모델의 구조적 원리를 탐구한다. 시각 인코더, 언어 모듈, 행동 디코더 등 핵심 구성 요소를 분석하고, 단일 시스템과 이중 시스템, 계층적 모델 등 주요 아키텍처 패러다임을 비교한다. 또한, 행동을 표현하는 방식과 기술 발전의 연대기를 추적한다.</li>
<li><strong>III장</strong>에서는 VLA 패러다임을 정립한 RT-2, 그 기반이 된 PaLM-E, 그리고 오픈소스 생태계를 연 OpenVLA 등 핵심적인 VLA 모델들을 심층적으로 분석하여 각 모델의 기술적 기여와 특징을 명확히 한다.</li>
<li><strong>IV장</strong>에서는 VLA 모델의 학습 및 적응 방법론을 다룬다. 모방 학습과 강화 학습 패러다임을 설명하고, Open X-Embodiment와 같은 대규모 데이터셋의 중요성을 강조하며, 시뮬레이션과 현실 세계 간의 간극을 메우기 위한 Sim-to-Real 전이 문제를 논의한다.</li>
<li><strong>V장</strong>에서는 로봇 조작, 자율 주행, 헬스케어 등 VLA 모델의 주요 응용 분야와 잠재적 확장 가능성을 탐색한다.</li>
<li><strong>VI장</strong>에서는 현재 VLA 기술이 직면한 실시간 추론, 일반화, 안전성 등의 기술적 난제들을 분석하고, 계층적 강화 학습, 진정한 다중 양식 학습, 세계 모델 등 미래 연구 방향을 제시한다.</li>
<li><strong>VII장</strong>에서는 전체 논의를 종합하여 VLA 모델의 현재 위상을 요약하고, 범용 체화 인공지능을 향한 장기적 비전을 제시하며 보고서를 마무리한다.</li>
</ul>
<p>본 보고서는 최근 3년간 발표된 80개 이상의 VLA 모델과 관련 핵심 연구들을 종합적으로 검토하여 1, 이 빠르게 발전하는 분야의 기술적 궤적을 명확히 추적하고, 관련 분야의 연구자 및 개발자들에게 깊이 있는 통찰과 실질적인 가이드를 제공하고자 한다.</p>
<h2>2.  VLA 모델의 구조적 원리 및 발전</h2>
<p>VLA 모델의 핵심은 시각, 언어, 행동이라는 이질적인 양식을 하나의 신경망 아키텍처 내에서 효과적으로 융합하고 변환하는 데 있다. 이를 위해 VLA 모델은 일반적으로 시각 인코더, 언어 모듈, 행동 디코더라는 세 가지 핵심 구성 요소로 이루어지며, 이들을 결합하는 방식에 따라 다양한 구조적 패러다임으로 발전해왔다.</p>
<h3>2.1  핵심 구성 요소</h3>
<h4>2.1.1  시각 인코더 (Vision Encoder)</h4>
<p>시각 인코더는 로봇의 카메라와 같은 센서로부터 입력된 원시 이미지 또는 비디오 데이터를 처리하여, 모델의 나머지 부분이 이해하고 활용할 수 있는 의미 있는 특징 표현(feature representation)으로 변환하는 역할을 담당한다. 이는 VLA 모델의 ’눈’에 해당하며, 환경에 대한 정확한 시각적 이해는 전체 시스템 성능의 기반이 된다.</p>
<p>초기 컴퓨터 비전 연구에서는 ResNet과 같은 합성곱 신경망(CNN) 기반 모델이 주로 사용되었으나 10, 최근 VLA 모델의 주류는 Vision Transformer(ViT) 기반 아키텍처로 전환되었다.10 ViT는 이미지를 여러 개의 패치(patch)로 나누고, 각 패치를 토큰처럼 처리하여 어텐션 메커니즘을 통해 이미지 전반의 전역적인 관계를 효과적으로 학습할 수 있다는 장점이 있다.</p>
<p>특히 최신 VLA 모델들은 특정 과업에 맞춰 처음부터 학습시키는 대신, 인터넷 스케일의 데이터로 사전 학습된 대규모 비전 모델을 활용하여 그 지식을 전이하는 방식을 채택한다. 대표적인 예로 OpenVLA는 DINOv2와 SigLIP라는 두 가지 강력한 비전 모델을 융합하여 사용한다.14 DINOv2는 자기지도학습(self-supervised learning)을 통해 레이블 없이도 이미지의 풍부한 기하학적 및 공간적 특징을 추출하는 데 뛰어나다.16 반면, SigLIP은 대조적 언어-이미지 사전학습(Contrastive Language-Image Pre-training) 방식을 통해 이미지 특징을 언어적 의미와 정렬(align)시키는 데 강점을 가진다.14 이 두 모델을 융합함으로써, VLA는 “컵이 노트북 뒤에 있다“와 같은 정밀한 공간 관계를 이해하는 능력(DINOv2)과 “차가운 음료“라는 단어를 냉장고 안의 소다수와 연결하는 의미론적 이해 능력(SigLIP)을 동시에 확보하게 된다.5 이러한 융합 인코더는 VLA가 복잡한 시각적 장면을 다각도로 깊이 있게 이해하는 기반을 제공한다.</p>
<h4>2.1.2  언어 모듈 (Language Module)</h4>
<p>언어 모듈은 VLA 시스템의 ‘뇌’ 역할을 수행하며, 사용자의 자연어 명령을 이해하고 해석하는 핵심적인 기능을 담당한다. 이 모듈은 단순한 키워드 매칭을 넘어, 문장의 구조, 의도, 그리고 시각적 맥락 속에서의 미묘한 뉘앙스까지 파악해야 한다. 예를 들어, “물을 천천히 따르라“는 명령에서 ’천천히’라는 부사가 행동의 속도를 조절해야 함을 이해하고, “이 난장판을 치워줘“와 같은 모호한 지시를 현재 시각적 상황에 맞게 구체적인 행동 순서로 변환하는 능력이 요구된다.5</p>
<p>이러한 고도의 언어 이해 능력을 구현하기 위해, VLA 모델들은 Llama-2, PaLM, Gemma와 같은 대규모 언어 모델(LLM)을 백본(backbone)으로 채택한다.14 LLM은 수십억에서 수백억 개에 달하는 파라미터를 가지며, 방대한 텍스트 코퍼스로 사전 학습되어 인간 언어에 대한 깊이 있는 통계적 모델을 내재하고 있다. VLA 아키텍처 내에서 LLM은 시각 인코더로부터 전달받은 시각 정보와 사용자의 텍스트 명령을 함께 입력받아, 이들을 융합하고 추론하여 최종적인 행동 계획을 수립하는 역할을 한다. 이 과정에서 LLM의 강력한 문맥 이해 및 추론 능력이 발휘되어, VLA가 복잡하고 다단계적인 과업을 성공적으로 수행할 수 있게 만든다.</p>
<h4>2.1.3  행동 디코더 (Action Decoder)</h4>
<p>행동 디코더는 언어 모듈에서 생성된 고수준의 행동 계획을 로봇이 물리적으로 실행할 수 있는 구체적인 제어 명령으로 변환하는 역할을 한다. 이는 VLA의 ’운동 피질’에 해당하며, 시스템의 최종 출력을 담당하는 정책(policy)의 역할을 수행한다. 행동 디코더는 융합된 시각-언어 표현을 입력받아, 로봇 팔의 다음 스텝에서의 6-DoF(자유도) 포즈 변화량(<span class="math math-inline">\Delta x, \Delta y, \Delta z, \Delta roll, \Delta pitch, \Delta yaw</span>)과 그리퍼의 개폐 상태 같은 저수준 모터 명령 시퀀스를 생성한다.16</p>
<p>행동 디코더를 구현하는 방식은 VLA 아키텍처의 핵심적인 설계 결정 중 하나이며, 주로 다음과 같은 접근법들이 사용된다.</p>
<ul>
<li><strong>Transformer 디코더:</strong> 언어 모델과 마찬가지로, 행동을 이산적인 토큰 시퀀스로 간주하고 Transformer의 자기회귀적(autoregressive) 디코더 구조를 사용하여 다음 행동 토큰을 순차적으로 예측한다.20</li>
<li><strong>디퓨전 모델 (Diffusion Model) / 플로우 매칭 네트워크 (Flow-matching Network):</strong> 정규 분포 노이즈로부터 점진적으로 노이즈를 제거(denoising)하는 과정을 통해 목표 행동을 생성하는 생성 모델의 일종이다. 이 방식은 특히 연속적인 행동 공간에서 부드럽고 자연스러운 궤적을 생성하는 데 강점을 보인다.3 CogAct와 같은 모델은 디퓨전 Transformer를 사용하여 시간적으로 일관된 모션 명령을 생성한다.18</li>
</ul>
<p>이러한 다양한 디코더 아키텍처는 VLA가 수행해야 할 과업의 특성(예: 정밀한 조작, 부드러운 움직임)에 따라 선택적으로 적용된다.</p>
<h3>2.2  구조적 패러다임</h3>
<p>VLA 모델의 핵심 구성 요소들을 어떻게 조직하고 연결하는지에 따라 여러 구조적 패러다임이 존재한다. 이들은 주로 시스템의 통합 수준과 기능적 분리 여부에 따라 구분되며, 각각의 장단점은 VLA 아키텍처의 발전 방향을 이해하는 데 중요한 단서를 제공한다.</p>
<h4>2.2.1  단일 시스템 (Single-System) vs. 이중 시스템 (Dual-System)</h4>
<ul>
<li><strong>단일 시스템 (Single-System):</strong> 이 패러다임은 시각적 인식, 언어적 추론, 행동 생성을 하나의 거대한 종단간(end-to-end) 모델 내에서 모두 처리하는 통합 아키텍처를 지향한다.3 Google의 RT-2와 스탠포드의 OpenVLA가 이 접근법의 대표적인 예이다.3 단일 시스템의 가장 큰 장점은 구조적 단순함에 있다. 모든 구성 요소가 단일 신경망 내에 통합되어 있어, 전체 시스템을 한 번에 최적화하기 용이하며, 각 모듈 간의 정보 흐름이 잠재 공간 내에서 자연스럽게 이루어진다. 하지만 이 방식은 거대한 모델 크기로 인해 실시간 추론에 제약을 받을 수 있으며, 상대적으로 낮은 주파수(예: 1-5 Hz)로 제어 명령을 생성하게 되어 로봇의 민첩한 움직임에는 한계가 있을 수 있다.22</li>
<li><strong>이중 시스템 (Dual-System):</strong> 이 패러다임은 로봇 제어의 실시간성 요구를 충족시키기 위해 기능을 분리하는 보다 실용적인 접근법을 취한다. 시스템은 두 개의 주요 하위 시스템으로 구성된다. 첫 번째는 고수준의 인식과 추론을 담당하는 VLM 모듈이고, 두 번째는 저수준의 정밀한 모터 제어를 고주파로 생성하는 ‘행동 전문가(Action Expert)’ 모듈이다.3 Helix, Groot N1, π0와 같은 모델들이 이 구조를 채택했다.3 VLM 모듈은 상대적으로 느린 속도(예: 1-5 Hz)로 카메라 이미지와 언어 명령을 처리하여 장면을 이해하고 전반적인 행동 계획을 수립한다. 이 계획은 잠재 표현(latent representation)의 형태로 행동 전문가에게 전달되고, 행동 전문가는 이를 바탕으로 훨씬 빠른 속도(예: 50-120 Hz)로 부드럽고 정밀한 로봇 제어 신호를 생성한다.3 이 구조는 로봇의 민첩성과 반응성을 크게 향상시킬 수 있지만, 두 시스템을 종단간으로 학습시켜야 하고 모듈 간 정보 교환으로 인한 오버헤드가 발생하는 등 계산 복잡성이 증가한다는 단점이 있다.3</li>
</ul>
<h4>2.2.2  계층적 VLA 모델 (Hierarchical Models)</h4>
<p>계층적 모델은 이중 시스템의 아이디어를 더욱 정교하게 발전시킨 구조이다. 이 패러다임에서는 고수준의 VLM이 최종적인 저수준 모터 명령을 직접 생성하는 대신, 과업 수행에 필요한 **중간 표현(intermediate representation)**을 생성하는 역할을 한다.21 이 중간 표현은 과업의 핵심적인 기하학적 또는 의미론적 정보를 담고 있으며, 예를 들어 목표 객체의 2D 키포인트, 로봇이 상호작용할 수 있는 영역을 나타내는 어포던스 맵(affordance map), 또는 로봇 팔 끝단(end-effector)이 따라가야 할 대략적인 2D 경로 등이 될 수 있다.21 이렇게 생성된 중간 표현은 그 자체로 해석 가능하며(interpretable), 이후 저수준 정책(low-level policy)이 이 명시적인 가이드를 입력받아 실제 로봇의 3D 공간에서의 정밀한 모터 명령을 생성한다.</p>
<p>이러한 계층적 분리는 여러 가지 중요한 이점을 제공한다. 첫째, 복잡한 장기 과업(long-horizon task)을 “무엇을 할 것인가(what to do)“와 “어떻게 할 것인가(how to do)“의 문제로 자연스럽게 분해하여 문제의 복잡도를 낮춘다. 둘째, 각 계층이 서로 다른 종류의 데이터로 학습될 수 있어 데이터 효율성을 높인다. 예를 들어, 고수준 VLM은 웹에서 수집된 방대한 이미지-텍스트 데이터로 의미론적 이해 능력을 학습하고, 저수준 정책은 실제 로봇의 궤적 데이터로 정밀한 제어 능력을 학습할 수 있다.23 이는 특히 데이터 수집 비용이 비싼 로봇 공학 분야에서 매우 중요한 장점이다.</p>
<p>VLA 아키텍처의 이러한 발전사는 ’통합의 우아함(elegance of integration)’과 ‘분리의 실용성(practicality of separation)’ 사이의 근본적인 긴장 관계를 반영한다. VLA의 초기 매력은 LLM/VLM이라는 거대한 단일 모델이 인식, 추론, 행동을 모두 처리할 수 있다는 가능성에서 비롯되었다. RT-2는 행동마저 언어의 일부로 취급하는 ’단일 시스템’의 우아함을 보여주며 이 아이디어를 성공적으로 구현했다.3 하지만 이 아이디어를 실제 로봇에 적용하자 물리적 세계의 현실적인 제약과 마주하게 되었다. 수십억 개의 파라미터를 가진 거대 모델은 실시간 추론에 병목이 되었고 22, 이는 로봇의 반응 속도를 저하시켰다. 이는 순수한 인공지능 연구의 이상과 물리 세계의 공학적 현실 사이의 괴리를 보여주는 사례였다.</p>
<p>이에 대한 공학적 해답으로 등장한 것이 ’이중 시스템’과 ’계층적 모델’이다. 이들은 문제를 기능적으로 분할하여 해결하는 고전적인 접근법을 따른다. “생각은 천천히, 행동은 빠르게“라는 원칙에 따라, 무거운 VLM은 고수준의 인지와 계획을, 가볍고 전문화된 행동 모듈은 저수준의 빠른 제어를 담당하게 한 것이다.3 계층적 모델은 여기서 한 걸음 더 나아가, 두 모듈 사이에 ’중간 표현’이라는 명시적인 인터페이스를 정의함으로써 시스템의 해석 가능성과 모듈성을 극대화했다.21 결론적으로, VLA 아키텍처의 진화는 ‘하나의 뇌로 모든 것을 해결하려는’ 초기 이상주의에서 출발하여, 로봇 공학의 실질적인 제약(실시간성, 정밀도, 안전성)을 해결하기 위해 ‘역할을 분담하는’ 기능적 전문화 및 계층화로 나아가고 있다. 이는 기술이 성숙하면서 나타나는 자연스러운 과정이며, 순수 AI와 응용 로봇 공학의 접점에서 발생하는 필연적인 타협과 진화를 보여준다.</p>
<table><thead><tr><th>특성 (Feature)</th><th>단일 시스템 (Single-System)</th><th>이중 시스템 (Dual-System)</th><th>계층적 모델 (Hierarchical Model)</th></tr></thead><tbody>
<tr><td><strong>핵심 아이디어 (Core Idea)</strong></td><td>인식, 추론, 제어의 종단간 통합 (End-to-end integration)</td><td>고수준 추론과 저수준 제어의 분리 (Separation of reasoning and control)</td><td>고수준 계획, 중간 표현, 저수준 정책의 단계적 분리 (Staged separation via intermediate representation)</td></tr>
<tr><td><strong>대표 모델 (Examples)</strong></td><td>RT-2, OpenVLA</td><td>Helix, Groot N1, π0</td><td>HAMSTER</td></tr>
<tr><td><strong>장점 (Pros)</strong></td><td>구조적 단순함, 통합적 최적화 용이 (Simplicity, holistic optimization)</td><td>고주파 제어, 민첩성, 반응성 향상 (High-frequency control, improved dexterity)</td><td>장기 과업 계획, 데이터 효율성, 해석 가능성 증대 (Long-horizon planning, data efficiency, interpretability)</td></tr>
<tr><td><strong>단점 (Cons)</strong></td><td>실시간 추론 제약, 저주파 제어 (Real-time inference bottleneck)</td><td>계산 복잡성 증가, 모듈 간 정보 교환 오버헤드 (Increased complexity, communication overhead)</td><td>시스템 복잡성 극대화, 중간 표현 설계의 어려움 (Highest complexity, design challenge of intermediate representation)</td></tr>
<tr><td><strong>주요 인용 (Source)</strong></td><td>3</td><td>3</td><td>21</td></tr>
</tbody></table>
<h3>2.3  행동 표현 방식</h3>
<p>행동 디코더가 생성하는 로봇의 행동을 어떻게 표현할 것인가는 VLA 모델의 성능과 특성을 결정하는 또 다른 중요한 축이다. 이는 크게 이산적 토큰화 방식과 연속적 출력 방식으로 나뉜다.</p>
<h4>2.3.1  이산적 토큰화 (Discrete Tokenization)</h4>
<p>이 방식은 로봇의 연속적인 행동 공간을 불연속적인 값들의 집합으로 변환하는 접근법이다. 예를 들어, 로봇 팔 끝단의 3차원 위치 변화량(<span class="math math-inline">\Delta x, \Delta y, \Delta z</span>)과 3차원 회전 변화량, 그리고 그리퍼의 상태와 같은 7-DoF 행동 공간이 있다고 가정하자. 각 차원(dimension)의 연속적인 값 범위를 256개의 균등한 구간(bin)으로 나누고, 각 구간에 0부터 255까지의 고유한 정수 토큰을 할당한다.24 이렇게 하면 하나의 7차원 연속 행동 벡터는 7개의 정수 토큰 시퀀스로 변환된다.</p>
<p>이 방식의 가장 큰 장점은 행동 데이터를 마치 자연어처럼 취급할 수 있다는 점이다. 변환된 행동 토큰들은 LLM의 기존 어휘(vocabulary)에 새로운 단어처럼 추가될 수 있으며, 모델은 텍스트를 생성하는 것과 동일한 자기회귀적(autoregressive) 방식으로 행동 토큰 시퀀스를 예측하게 된다.3 RT-2와 OpenVLA가 이 방식을 성공적으로 사용하여, 기존 VLM 아키텍처를 거의 수정하지 않고도 로봇 제어에 적용할 수 있음을 보여주었다.3 이는 학습 과정을 매우 직관적으로 만들고, VLM의 강력한 시퀀스 생성 능력을 행동 생성에 직접 활용할 수 있게 한다. 하지만 이산화 과정에서 필연적으로 양자화 오류(quantization error)가 발생하여 행동의 정밀도가 저하될 수 있으며, 긴 행동 시퀀스를 생성할 때 추론 속도가 느려질 수 있다는 단점이 있다.</p>
<h4>2.3.2  연속적 행동 출력 (Continuous Action Output)</h4>
<p>이산화 방식의 한계를 극복하기 위해, 행동 디코더가 연속적인 행동 값을 직접 출력하도록 설계하는 방식이다. 이 접근법은 주로 디퓨전 모델(diffusion model)이나 플로우 매칭 네트워크(flow-matching network)와 같은 연속적인 생성 모델을 행동 디코더로 사용하여 구현된다.3 이 모델들은 행동 공간 상의 확률 분포를 직접 학습하여, 주어진 시각-언어 문맥에 가장 적합한 연속적인 행동 벡터 또는 궤적을 샘플링한다.</p>
<p>π0와 같은 모델은 이 방식을 통해 최대 50Hz에 달하는 고주파 제어를 달성했으며, 이는 액체를 따르거나 부드럽게 물건을 내려놓는 등 섬세하고 유연한 움직임이 요구되는 작업에 필수적이다.3 또한, 로봇의 자유도가 매우 높아질 경우(예: 휴머노이드 로봇), 각 자유도를 모두 이산화하는 것이 비현실적이기 때문에 연속적 출력 방식이 확장성 측면에서 더 유리하다.3 그러나 이 방식은 VLM 백본에 새로운 모듈(예: 디퓨전 헤드)을 추가해야 하므로 아키텍처가 더 복잡해지고, 학습 과정에서 VLM의 사전 학습된 지식과 새로운 행동 생성 모듈 간의 균형을 맞추는 것이 어려울 수 있다.25</p>
<h3>2.4  발전 연대기</h3>
<p>VLA 모델 분야는 2022년경 개념이 정립된 이후 매우 빠른 속도로 발전해왔다. 그 발전 과정은 크게 세 단계로 구분할 수 있다.1</p>
<ul>
<li><strong>1단계: 기초 통합 (Foundational Integration, 2022–2023):</strong> 이 시기는 VLA의 기본 개념을 확립하고, 시각 정보와 모터 제어를 통합하는 기초적인 다중 양식 융합 아키텍처를 구축하는 데 집중했다. CLIP 임베딩과 모션 프리미티브를 결합하는 초기 시도에서부터 1, 대규모 모방 학습을 통해 높은 조작 성공률을 달성한 연구들이 등장했다.1 이 단계의 정점은 2023년 Google DeepMind의 <strong>RT-2</strong> 발표로, ‘행동을 언어로’ 다루는 혁신적인 아이디어를 통해 웹 스케일 VLM의 지식을 로봇 제어에 성공적으로 전이시키고, 시각적 연쇄 사고(visual chain-of-thought) 추론의 가능성을 보여주며 VLA 패러다임을 본격적으로 열었다.1 이 시기 모델들은 주로 저수준 제어 능력 확보에 초점을 맞추었다.</li>
<li><strong>2단계: 전문화 및 체화 추론 (Specialization and Embodied Reasoning, 2024):</strong> 2세대 VLA 모델들은 기초적인 통합을 넘어, 특정 도메인에 맞는 귀납적 편향(inductive bias)을 도입하여 성능을 고도화하기 시작했다. 검색 증강 생성(RAG)을 활용하여 소량의 데이터만으로 새로운 과업에 빠르게 적응하는 능력을 향상시키거나 1, 3D 씬 그래프(scene graph)를 통합하여 내비게이션 성능을 최적화하는 등의 연구가 진행되었다.1 또한, 물리 법칙을 고려하는 어텐션 메커니즘을 도입하여 부분적인 관측 가능성(partial observability) 문제를 해결하려는 시도도 있었다.1 이 시기는 VLA가 단순한 시각-운동 매핑을 넘어, 더 깊이 있는 추론과 상황 이해를 바탕으로 행동하도록 발전하는 단계이다.</li>
<li><strong>3단계: 일반화 및 안전성 확보 (Generalization and Safety-Critical Deployment, 2025):</strong> 현재 진행 중인 이 단계에서는 모델의 강건성(robustness), 인간과의 상호작용 및 정렬(alignment), 그리고 실제 환경 배포를 위한 안전성과 효율성이 핵심 연구 주제로 부상하고 있다. 형식적 검증(formal verification) 방법을 통합하여 위험을 인지하고 회피하는 의사결정을 내리거나 1, 휴머노이드 로봇의 전신 제어(whole-body control)를 위한 계층적 VLA 아키텍처를 개발하는 연구가 이루어지고 있다.1 또한, 임베디드 시스템에 배포하기 위한 모델의 계산 효율성 최적화, 그리고 더 나은 인과 관계 추론을 위한 신경-기호(neuro-symbolic) 접근법의 결합 등이 활발히 탐구되고 있다.1 이는 VLA 기술이 실험실을 넘어 실제 산업 및 일상생활에 적용되기 위해 반드시 해결해야 할 과제들을 다루는 성숙 단계라 할 수 있다.</li>
</ul>
<h2>3.  핵심 VLA 모델 심층 분석</h2>
<p>VLA 분야의 빠른 발전은 몇몇 선구적인 모델들에 의해 주도되었다. 이 모델들은 새로운 아키텍처를 제안하거나, 대규모 데이터를 활용하는 혁신적인 방법을 제시하며 VLA 연구의 방향성을 설정했다. 본 장에서는 VLA 패러다임의 문을 연 RT-2, 그 기반 기술인 PaLM-E, 그리고 오픈소스 생태계를 확립한 OpenVLA를 중심으로 각 모델의 구조, 학습 방식, 그리고 핵심적인 기여를 심층적으로 분석한다.</p>
<h3>3.1  RT-2 (Robotic Transformer 2): VLA 패러다임의 개척자</h3>
<h4>3.1.1  개요</h4>
<p>RT-2(Robotic Transformer 2)는 2023년 중반 Google DeepMind가 발표한 모델로, VLA 패러다임을 본격적으로 정립하고 그 잠재력을 세상에 알린 기념비적인 연구이다.3 이전 모델인 RT-1이 로봇 시연 데이터만으로 학습하여 특정 기술과 객체의 조합을 학습하는 데 그쳤다면, RT-2는 웹 스케일의 방대한 비전-언어 데이터를 로봇 제어에 직접적으로 전이시키는 혁신적인 방법을 제시했다.12 이를 통해 RT-2는 학습 데이터에 명시적으로 존재하지 않았던 새로운 개념, 객체, 그리고 추상적인 명령에 대해서도 놀라운 일반화 성능을 보여주었다.</p>
<h4>3.1.2  아키텍처 및 학습</h4>
<p>RT-2의 성공은 기존의 강력한 VLM을 최소한의 수정으로 로봇 제어에 적용한 영리한 설계에 기인한다.</p>
<ul>
<li><strong>VLM 백본:</strong> RT-2는 Google이 개발한 두 가지 최첨단 VLM, 즉 PaLI-X (550억 파라미터)와 PaLM-E (120억 파라미터)를 백본으로 사용한다.13 이 VLM들은 이미 방대한 인터넷 데이터로부터 이미지와 텍스트 사이의 복잡한 관계를 학습한 상태이므로, 풍부한 시각적, 의미론적 지식을 내재하고 있다.</li>
<li><strong>‘행동을 언어처럼 (Action as Language)’ 전략:</strong> RT-2의 가장 핵심적인 아이디어는 로봇의 물리적 행동을 언어의 일부로 취급하는 것이다. 이를 위해 로봇 팔의 연속적인 7-DoF 행동 공간(3D 위치 변화, 3D 회전 변화, 그리퍼 상태)을 각각 256개의 이산적인 빈(bin)으로 양자화한다. 그리고 각 빈에 고유한 정수 토큰을 할당하여, 하나의 행동을 7개의 토큰 시퀀스로 표현한다.13 예를 들어, “terminate=0, <span class="math math-inline">\Delta pos_x</span>=128, <span class="math math-inline">\Delta pos_y</span>=130,…“와 같은 문자열로 변환하는 것이다. 이 행동 문자열은 VLM의 어휘 사전에 추가되어, 모델이 마치 다음 단어를 예측하듯이 다음 행동 토큰을 자기회귀적으로 생성하게 된다.</li>
<li><strong>공동 미세조정 (Co-fine-tuning):</strong> RT-2는 VLM 백본을 로봇 데이터로만 미세조정하는 대신, 기존 VLM이 학습했던 방대한 웹 데이터(이미지-캡션, VQA 등)와 새로운 로봇 시연 데이터(이미지, 언어 명령, 행동 토큰)를 혼합하여 학습을 진행한다. 이를 공동 미세조정이라 하며, 학습 배치에 로봇 데이터의 비율을 높게 설정(예: 50-66%)하면서도 웹 데이터를 계속해서 보여줌으로써, 모델이 웹에서 학습한 일반적인 지식을 잊어버리는 ’파국적 망각(catastrophic forgetting)’을 방지하고 일반화 성능을 극대화한다.12</li>
</ul>
<h4>3.1.3  주요 기여 및 창발적 능력</h4>
<p>RT-2의 가장 큰 기여는 웹 스케일 지식이 로봇의 물리적 제어 능력으로 직접 전이될 수 있음을 증명한 것이다. 이로 인해 RT-2는 훈련 데이터에서는 볼 수 없었던 다양한 **창발적 능력(emergent capabilities)**을 보여주었다.13</p>
<ul>
<li><strong>개념 일반화:</strong> 로봇 데이터에는 ’사과’를 옮기는 시연만 있었음에도 불구하고, 웹 데이터를 통해 ’과일’이라는 상위 개념을 학습한 RT-2는 처음 보는 ’바나나’를 옮기는 데 성공할 수 있다.</li>
<li><strong>추론 능력:</strong> “가장 작은 물체를 집어라” 또는 “막 테이블에서 떨어지려는 가방을 집어라“와 같이, 단순한 객체 인식을 넘어 객체 간의 관계나 상태에 대한 추론을 요구하는 명령을 수행할 수 있다.12</li>
<li><strong>기호 이해:</strong> 학습 데이터에 없었던 특정 숫자나 아이콘 위에 물체를 놓는 등, 추상적인 기호의 의미를 이해하고 이를 물리적 위치와 연결하는 능력을 보여주었다.13</li>
<li><strong>연쇄 사고 (Chain-of-Thought) 추론:</strong> RT-2는 더 복잡한 문제에 대해 스스로 추론 과정을 자연어로 생성하고, 그 결과를 바탕으로 행동을 결정할 수 있다. 예를 들어, “망치로 쓸 만한 물건을 집어줘“라는 명령에 대해 “망치가 없다. 돌이 단단하고 무거우니 망치 대용으로 쓸 수 있겠다. 돌을 집어야겠다“와 같은 내부적인 추론을 거친 후 돌을 집는 행동을 수행한다. 또 다른 예로, “피곤한 사람에게는 어떤 음료가 좋을까?“라는 질문에 대해 에너지 드링크가 적합하다고 추론하고 이를 집어주는 다단계 의미론적 추론이 가능하다.12</li>
</ul>
<p>이러한 능력들은 RT-2가 단순한 패턴 매칭을 넘어, 웹에서 학습한 방대한 상식을 바탕으로 진정한 의미의 ’이해’와 ’추론’을 통해 행동하고 있음을 시사한다.</p>
<h3>3.2  PaLM-E: RT-2의 체화된 VLM 백본</h3>
<p>PaLM-E(Pathways Language model Embodied)는 RT-2의 강력한 성능을 뒷받침하는 핵심 VLM 백본 중 하나로, 그 자체로 VLA 패러다임의 중요한 개념적 토대를 제공한다. PaLM-E의 목표는 순수한 텍스트 데이터로만 학습된 대규모 언어 모델(LLM)에 현실 세계의 연속적인 센서 정보를 직접 통합하여, 언어를 물리적 세계와 ’접지(grounding)’시키는 것이다.28</p>
<h4>3.2.1  아키텍처</h4>
<p>PaLM-E의 핵심 아키텍처 아이디어는 ’다중 양식 문장(multimodal sentences)’이라는 개념에 있다. 이는 텍스트 토큰 시퀀스 중간에 이미지나 로봇 상태와 같은 비-텍스트 정보를 자연스럽게 삽입하는 방식이다.29</p>
<ol>
<li><strong>입력 인코딩:</strong> 이미지와 같은 연속적인 센서 데이터는 ViT(Vision Transformer)와 같은 인코더를 통해 벡터 시퀀스로 변환된다.31</li>
<li><strong>임베딩 공간 주입:</strong> 이 벡터 시퀀스는 학습 가능한 프로젝션 레이어를 거쳐, LLM(PaLM)의 텍스트 토큰 임베딩과 동일한 차원의 벡터 공간으로 매핑된다.28 이렇게 변환된 시각 정보는 마치 새로운 ’단어’처럼 취급되어, “Bring me the rice chips from &lt;img_1&gt;“과 같이 텍스트 시퀀스에 직접 삽입된다.</li>
<li><strong>통합 처리:</strong> 이렇게 구성된 다중 양식 문장은 PaLM의 Transformer 레이어에 입력되어, 텍스트와 이미지가 어텐션 메커니즘을 통해 동등하게 상호작용하며 처리된다.</li>
</ol>
<h4>3.2.2  역할 및 기여</h4>
<p>PaLM-E는 VLA 시스템 내에서 주로 고수준의 계획을 생성하는 ‘플래너(planner)’ 역할을 수행한다. 예를 들어, “서랍에서 감자칩을 가져와“라는 최종 목표가 주어지면, PaLM-E는 현재의 시각적 관찰(서랍과 로봇의 위치 등)을 바탕으로 “1. 서랍으로 이동한다”, “2. 서랍을 연다”, “3. 감자칩을 잡는다”, “4. 사용자에게 가져온다“와 같은 일련의 텍스트 기반 하위 목표(subgoal)를 생성한다.28</p>
<p>이렇게 생성된 텍스트 계획은 그 자체로 로봇을 직접 제어할 수는 없다. 대신, 이 텍스트 하위 목표가 RT-1과 같은 저수준 정책(low-level policy)의 새로운 입력으로 주어져, 해당 하위 목표를 수행하기 위한 구체적인 모터 명령을 생성하게 된다.30 이처럼 PaLM-E는 LLM의 강력한 추론 및 계획 능력을 활용하여 복잡한 과업을 분해하고, 저수준 제어는 전문화된 정책에 위임하는 계층적 제어 구조의 가능성을 보여주었다. 이는 VLA가 어떻게 고수준의 상징적 추론과 저수준의 연속적 제어를 효과적으로 결합할 수 있는지에 대한 중요한 청사진을 제시했다.</p>
<h3>3.3  OpenVLA: 오픈소스 생태계의 확립</h3>
<p>OpenVLA는 스탠포드 대학을 중심으로 한 연구진이 2024년 6월에 공개한 70억 파라미터 규모의 오픈소스 VLA 모델이다.3 OpenVLA의 등장은 VLA 연구의 민주화와 생태계 확장에 결정적인 계기가 되었다. 특히, Google의 RT-2-X(55B)와 같은 거대하고 폐쇄적인 모델보다 7배 이상 작은 크기에도 불구하고, 다양한 로봇 조작 과업에서 더 높은 성능을 달성함으로써, VLA의 성공이 반드시 모델의 크기에만 비례하는 것이 아님을 입증했다.5</p>
<h4>3.3.1  아키텍처</h4>
<p>OpenVLA는 효율성과 성능의 균형을 맞춘 영리한 아키텍처 설계를 특징으로 한다.</p>
<ul>
<li><strong>듀얼 비전 백본:</strong> 시각 인코더로 DINOv2와 SigLIP의 사전 학습된 특징을 융합하여 사용한다.14 이 융합 방식은 DINOv2가 제공하는 뛰어난 픽셀 수준의 공간적, 기하학적 이해 능력과 SigLIP이 제공하는 언어와 정렬된 고수준의 의미론적 이해 능력을 결합하여, 시각적 장면을 더욱 풍부하고 다각적으로 해석할 수 있게 한다.16</li>
<li><strong>LLM 백본:</strong> 언어 및 추론 모듈로는 널리 사용되는 오픈소스 LLM인 Llama-2 (7B)를 채택했다.14</li>
<li><strong>프로젝터 및 행동 생성:</strong> 비전 인코더에서 추출된 이미지 패치 임베딩은 간단한 MLP(Multi-Layer Perceptron) 프로젝터를 통해 LLM의 토큰 공간으로 매핑된다.14 행동 생성 방식은 RT-2와 유사하게, 7-DoF 행동을 이산적인 토큰으로 변환하여 LLM이 자기회귀적으로 예측하도록 한다.16</li>
</ul>
<h4>3.3.2  학습 및 데이터</h4>
<p>OpenVLA의 뛰어난 성능은 아키텍처뿐만 아니라 학습 데이터의 질과 다양성에서 비롯된다.</p>
<ul>
<li><strong>Open X-Embodiment (OpenX) 데이터셋:</strong> OpenVLA는 21개의 학술 및 산업 연구 기관이 협력하여 구축한 전례 없는 규모의 로봇 데이터셋인 OpenX를 기반으로 학습되었다. OpenVLA 학습에는 이 중 22가지의 서로 다른 로봇 형태(embodiment)로부터 수집된 97만 개의 실제 로봇 조작 궤적이 사용되었다.3 이처럼 방대한 규모와 이종(heterogeneous) 로봇 데이터의 다양성은 모델이 특정 로봇이나 환경에 과적합되는 것을 방지하고, 보편적인 조작 기술을 학습하여 일반화 성능을 극대화하는 데 결정적인 역할을 했다.</li>
</ul>
<h4>3.3.3  적응성 및 기여</h4>
<p>OpenVLA의 가장 큰 기여 중 하나는 VLA 모델의 실용적인 배포와 적응을 위한 길을 열었다는 점이다.</p>
<ul>
<li><strong>파라미터 효율적 미세조정 (PEFT):</strong> OpenVLA는 LoRA(Low-Rank Adaptation)와 같은 PEFT 기법을 완벽하게 지원한다.7 LoRA는 거대한 VLA 모델의 전체 파라미터를 모두 업데이트하는 대신, 일부 레이어에 작은 크기의 학습 가능한 행렬을 추가하여 이 부분만 미세조정하는 방식이다. 연구에 따르면, LoRA를 사용하여 전체 파라미터의 단 1.4%만 튜닝해도, 모델 전체를 미세조정하는 것과 거의 동등한 성능을 달성할 수 있었다.15 이는 제한된 계산 자원(예: 단일 GPU)을 가진 연구실이나 기업에서도 VLA 모델을 자신들의 특정 로봇이나 새로운 과업에 맞게 빠르고 효율적으로 적응시킬 수 있음을 의미한다.</li>
<li><strong>오픈소스 생태계:</strong> 모델 가중치, 학습 코드, 데이터셋 처리 파이프라인을 모두 공개함으로써, OpenVLA는 전 세계 연구자들이 VLA 기술을 직접 사용하고, 개선하며, 그 위에 새로운 연구를 구축할 수 있는 건강한 생태계의 기반을 마련했다.5</li>
</ul>
<p>이러한 핵심 모델들의 발전 과정을 살펴보면, VLA 분야가 ’폐쇄형 거대 모델’과 ’개방형 효율적 모델’이라는 두 축의 경쟁과 상호 보완을 통해 가속화되고 있음을 알 수 있다. Google의 RT-2가 웹 스케일 데이터의 막대한 힘을 증명하며 VLA라는 새로운 패러다임을 열었다면, OpenVLA는 더 작은 모델과 개방형 데이터셋으로 더 높은 성능을 달성함으로써 중요한 교훈을 주었다. 즉, 일반화 능력의 핵심은 단순히 모델의 파라미터 수가 아니라, 다양한 물리적 상호작용을 담고 있는 ‘데이터의 질과 다양성’ 및 공간적/의미론적 특징을 효과적으로 추출하는 ’아키텍처의 영리함’에 있을 수 있다는 것이다. RT-2가 스케일링 법칙의 한계를 탐구하며 가능성을 보여주었다면, OpenVLA는 효율적인 아키텍처와 데이터 활용법을 통해 그 가능성을 더 많은 사람들이 실현할 수 있도록 만들었다. 이러한 역동적인 상호작용은 VLA 연구의 발전을 촉진하고 있다.</p>
<table><thead><tr><th>모델명 (Model)</th><th>개발 주체 (Developer)</th><th>LLM 백본 (Backbone)</th><th>비전 인코더 (Vision Encoder)</th><th>행동 표현 (Action Repr.)</th><th>핵심 데이터셋 (Key Dataset)</th><th>주요 기여 (Key Contribution)</th></tr></thead><tbody>
<tr><td><strong>RT-2</strong></td><td>Google DeepMind</td><td>PaLM-E, PaLI-X</td><td>PaLM-E/PaLI-X Internal</td><td>이산적 토큰 (Discrete)</td><td>WebLI + Robotics Data</td><td>웹 스케일 지식 전이를 통한 VLA 패러다임 정립, 창발적 추론 능력 증명 3</td></tr>
<tr><td><strong>OpenVLA</strong></td><td>Stanford, et al.</td><td>Llama-2 (7B)</td><td>DINOv2 + SigLIP (Fused)</td><td>이산적 토큰 (Discrete)</td><td>Open X-Embodiment</td><td>오픈소스 고성능 VLA, 데이터 다양성의 중요성 입증, PEFT 지원 3</td></tr>
<tr><td><strong>Octo</strong></td><td>UC Berkeley</td><td>- (Transformer-based)</td><td>CNN (Lightweight)</td><td>연속적 궤적 (Continuous)</td><td>Open X-Embodiment</td><td>경량화된 범용 정책, 디퓨전 기반 연속 행동, 빠른 적응성 3</td></tr>
<tr><td><strong>π0</strong></td><td>Physical Intelligence</td><td>- (VLM-based)</td><td>-</td><td>연속적 궤적 (Continuous)</td><td>-</td><td>이중 시스템 아키텍처, 고주파(50Hz) 연속 제어 달성 3</td></tr>
</tbody></table>
<h2>4.  VLA 모델의 학습 및 적응</h2>
<p>VLA 모델이 복잡한 물리적 과업을 수행하는 능력을 갖추기 위해서는 효과적인 학습 방법론과 방대한 양의 고품질 데이터가 필수적이다. VLA의 학습 패러다임은 주로 전문가의 행동을 모방하는 것에서 시작하여, 환경과의 상호작용을 통해 정책을 개선하는 방향으로 발전하고 있다. 또한, 시뮬레이션과 현실 세계 간의 간극을 줄이는 것은 VLA의 실용적인 적용을 위한 핵심 과제이다.</p>
<h3>4.1  학습 패러다임</h3>
<h4>4.1.1  모방 학습 (Imitation Learning / Behavior Cloning)</h4>
<p>모방 학습, 특히 행동 복제(Behavior Cloning)는 현재 VLA 모델 학습에서 가장 지배적으로 사용되는 패러다임이다.14 이 접근법의 핵심은 전문가(인간 원격 조종사 또는 자동화된 정책)가 특정 과업을 수행하는 과정을 기록한 대규모 시연 데이터를 사용하는 것이다. 이 데이터는 각 시간 단계별로 로봇이 관찰한 것(카메라 이미지, proprioceptive state 등)과 그때 수행한 행동(모터 명령)의 쌍으로 구성된다. VLA 모델은 이 (관찰, 행동) 쌍 데이터를 이용해, 주어진 관찰에 대해 전문가의 행동을 최대한 유사하게 예측하도록 지도 학습(supervised learning) 방식으로 훈련된다.7</p>
<p>모방 학습의 가장 큰 장점은 개념적으로 간단하고 학습 과정이 안정적이라는 점이다. 복잡한 보상 함수를 설계할 필요 없이, 단순히 전문가의 행동을 따라 하도록 학습시키면 되기 때문에 대규모 데이터셋을 활용하기에 용이하다. OpenVLA와 같은 성공적인 모델들은 수십만 개의 전문가 시연 궤적을 이용한 모방 학습을 통해 강력한 일반화 성능을 갖춘 기초 모델(foundation model)을 구축했다.32 하지만 모방 학습은 근본적인 한계를 가지고 있다. 모델은 학습 데이터 분포 내에 있는 상황에 대해서는 잘 작동하지만, 한 번도 경험해보지 못한 새로운 상황(out-of-distribution)에 직면했을 때는 어떻게 행동해야 할지 모르기 때문에 취약한 모습을 보일 수 있다.</p>
<h4>4.1.2  강화 학습 (Reinforcement Learning)</h4>
<p>강화 학습(RL)은 모방 학습의 한계를 보완하고, 특정 환경이나 과업에 모델을 더욱 정교하게 적응시키기 위한 강력한 패러다임으로 주목받고 있다.14 RL에서 에이전트(로봇)는 환경과 직접 상호작용하며 다양한 행동을 시도하고, 그 결과로 주어지는 보상(reward) 신호(예: 과업 성공 시 +1, 실패 시 -1)를 최대화하는 방향으로 자신의 정책을 점진적으로 개선해 나간다.35</p>
<p>VLA 분야에서 RL은 처음부터 정책을 학습시키는 것(learning from scratch)보다는, 모방 학습으로 잘 사전 학습된 모델을 특정 목표에 맞게 미세조정하거나 **사후 학습(post-training)**하는 데 주로 사용된다. 이는 RL이 일반적으로 데이터 효율성이 매우 낮아 방대한 양의 시행착오를 요구하기 때문이다.7 사전 학습된 VLA는 이미 과업에 대한 기본적인 이해를 갖추고 있어, RL의 탐색 공간을 크게 줄여주고 학습을 가속화할 수 있다.</p>
<p>최근 연구인 RIPT-VLA는 이러한 접근법의 잠재력을 극명하게 보여준다. 이 연구에서는 단 하나의 전문가 시연 데이터와 과업 성공 여부만을 알려주는 희소한 이진 보상(sparse binary reward)만을 사용하여 사전 학습된 VLA 모델을 미세조정했다. 그 결과, 초기 성공률이 4%에 불과했던 모델을 단 15번의 학습 반복만으로 97.5%의 성공률로 끌어올리는 놀라운 성과를 거두었다.35 이는 RL 기반의 상호작용적 사후 학습이 VLA 모델을 실제 환경의 미묘한 특성에 적응시키고 성능을 극대화하는 데 매우 효과적인 방법임을 시사한다.</p>
<p>이러한 학습 패러다임의 흐름은 VLA 분야가 “대규모 사전학습 -&gt; 소규모 적응“이라는 현대 딥러NING의 표준 공식을 따르고 있음을 보여준다. 그러나 데이터 수집 비용이 비싸고 물리적 상호작용의 위험성이 존재하는 로봇 공학의 특수성으로 인해, ‘적응’ 단계의 중요성이 더욱 부각되고 있다. 모방 학습으로 구축된 ’기초 모델’은 광범위한 일반 지식을 제공하지만, 실제 세계의 특정 과업을 완벽하게 수행하기에는 부족하다. 따라서 강화 학습 기반의 ’온라인 미세조정’은 이러한 ‘마지막 1마일(last-mile)’ 문제를 해결하고, 사전 학습된 모델이 실제 환경의 고유한 특성에 정밀하게 적응하도록 만드는 필수적인 다음 단계로 자리 잡고 있다. 결과적으로, VLA의 이상적인 학습 파이프라인은 <strong>“모방 학습으로 넓고 얕은 지식을 구축하고, 강화 학습으로 좁고 깊은 기술을 연마하는”</strong> 상호 보완적인 방향으로 진화하고 있다. 이는 데이터 효율성과 성능 최적화 사이의 균형을 맞추는 가장 실용적인 전략이다.</p>
<h3>4.2  데이터의 중요성</h3>
<p>VLA 모델의 성능, 특히 일반화 능력은 학습 데이터의 규모와 다양성에 의해 결정된다고 해도 과언이 아니다. “더 좋은 데이터가 더 좋은 성능을 만든다(Better data = better performance)“는 원칙은 VLA 분야에서 특히 중요하게 작용한다.7</p>
<h4>4.2.1  대규모 다중 양식 데이터셋</h4>
<p>VLA 연구의 비약적인 발전은 Open X-Embodiment (OpenX)와 같은 대규모 다중 양식 데이터셋의 등장과 밀접한 관련이 있다.32 OpenX는 21개의 서로 다른 연구 기관이 각자 수집한 로봇 데이터를 통합하여 구축한 전례 없는 규모의 데이터셋으로, 22가지의 다양한 로봇 팔, 500개 이상의 기술, 15만 개 이상의 과업을 포괄하는 100만 개 이상의 로봇 궤적 데이터를 제공한다.3 이러한 데이터의 방대한 규모와 이질성(heterogeneity)은 VLA 모델이 특정 로봇의 형태나 특정 환경의 시각적 특징에 과적합되는 것을 방지하고, 다양한 상황에 적용될 수 있는 보편적인 시각-언어-행동의 연관성을 학습하도록 돕는다. OpenVLA가 상대적으로 작은 모델 크기에도 불구하고 RT-2보다 뛰어난 성능을 보인 주된 이유 중 하나로 바로 이 OpenX 데이터셋의 활용이 꼽힌다.15</p>
<h4>4.2.2  데이터 수집 전략</h4>
<p>고품질의 VLA 학습 데이터를 수집하는 것은 여전히 많은 비용과 노력이 요구되는 과정이다. 데이터 수집은 주로 두 가지 방식으로 이루어진다.</p>
<ul>
<li><strong>실제 로봇 데이터 수집:</strong> 인간이 가상현실(VR) 장비 등을 착용하고 로봇을 원격으로 조종(teleoperation)하여 시연 데이터를 수집하거나, 특정 과업을 수행하도록 잘 설계된 스크립트 정책(scripted policy)을 실행하여 데이터를 생성한다.3 실제 데이터는 현실 세계의 물리적 뉘앙스를 가장 정확하게 반영한다는 장점이 있지만, 수집 속도가 느리고 비용이 많이 든다.</li>
<li><strong>시뮬레이션을 통한 데이터 생성:</strong> Isaac Gym, PyBullet, SAPIEN과 같은 고충실도 물리 시뮬레이션 환경을 사용하여 데이터를 대규모로 생성하는 방식이다.3 시뮬레이션은 수백, 수천 개의 환경을 GPU 상에서 병렬로 실행하여 저비용으로 방대한 양의 데이터를 빠르게 수집할 수 있다는 결정적인 장점을 가진다.7 또한, 객체의 위치, 조명 조건, 물리 파라미터 등을 무작위로 변경하는 도메인 무작위화(domain randomization)를 쉽게 적용하여 데이터의 다양성을 인위적으로 높일 수 있다. 하지만 시뮬레이션 데이터는 필연적으로 현실 세계와의 차이, 즉 ‘Sim-to-Real 갭’ 문제를 안고 있다.</li>
</ul>
<h3>4.3  Sim-to-Real 전이 문제</h3>
<h4>4.3.1  정의</h4>
<p>Sim-to-Real 전이 문제(Sim-to-Real Transfer Challenge)는 시뮬레이션 환경에서 성공적으로 학습된 정책이 실제 로봇에 적용되었을 때 성능이 크게 저하되는 현상을 의미한다.17 이 문제의 근본적인 원인은 시뮬레이터가 현실 세계를 완벽하게 복제할 수 없기 때문에 발생하는 **현실 격차(reality gap)**에 있다. 이 격차는 크게 두 가지 측면에서 발생한다.</p>
<ul>
<li><strong>시각적 격차 (Visual Gap):</strong> 시뮬레이터의 렌더링 엔진이 만들어내는 이미지와 실제 카메라로 촬영한 이미지 사이에는 조명, 그림자, 질감, 반사 등 미묘하지만 중요한 차이가 존재한다.38</li>
<li><strong>동역학적 격차 (Dynamics Gap):</strong> 시뮬레이터의 물리 엔진이 계산하는 객체 간의 상호작용(예: 마찰, 접촉, 탄성)은 실제 세계의 복잡한 물리 현상과 차이를 보일 수 있다.39</li>
</ul>
<p>이러한 격차로 인해, 시뮬레이션 이미지에만 익숙해진 VLA의 시각 인코더는 실제 이미지를 제대로 해석하지 못하거나, 시뮬레이션의 물리 법칙에 최적화된 행동 정책이 실제 환경에서는 예상치 못한 결과를 낳을 수 있다.</p>
<h4>4.3.2  해결 방안</h4>
<p>Sim-to-Real 문제를 완화하고 시뮬레이션의 이점을 극대화하기 위해 다양한 연구가 진행되고 있다.</p>
<ul>
<li><strong>도메인 무작위화 (Domain Randomization):</strong> 가장 널리 사용되는 기법 중 하나로, 시뮬레이션 환경의 시각적 파라미터(조명, 색상, 질감 등)와 물리적 파라미터(질량, 마찰 계수 등)를 매 에피소드마다 무작위로 변경하여 정책을 학습시킨다. 이를 통해 모델은 특정 환경의 피상적인 특징에 의존하지 않고, 다양한 변화에 강건한(robust) 일반적인 특징을 학습하게 된다.</li>
<li><strong>도메인 불변 표현 학습 (Domain-Invariant Representation Learning):</strong> 시뮬레이션 이미지와 실제 이미지로부터 공통적으로 존재하는, 과업 수행에 핵심적인 의미론적 특징을 추출하도록 시각 인코더를 학습시키는 방식이다. 최근 한 연구에서는 <strong>자연어</strong>를 시뮬레이션과 현실이라는 두 도메인을 연결하는 강력한 ’중재 신호(unifying signal)’로 활용하는 방법을 제안했다.38 이 방법은 시뮬레이션 이미지에 대한 언어적 설명과 실제 이미지에 대한 언어적 설명이 유사해지도록 인코더를 사전 학습시킨다. 예를 들어, 시뮬레이션의 ’빨간색 블록’과 실제의 ’붉은색 벽돌’은 시각적으로는 다르지만, “빨간 물체를 집어라“는 언어적 맥락에서는 동일한 의미를 갖는다. 이러한 언어적 공통점을 활용하여, 인코더가 두 도메인에 걸쳐 일관된 표현을 학습하도록 유도하는 것이다.</li>
<li><strong>시스템 식별 및 동적 적응:</strong> 또 다른 접근법은 실제 로봇과의 상호작용 데이터를 바탕으로 시뮬레이터의 물리 파라미터를 자동으로 튜닝하여 현실과의 격차를 줄이는 시스템 식별(system identification) 기법이다.39 더 나아가, 실제 작업 수행 중에 발생하는 힘/토크 피드백을 이용하여 로봇의 순응 제어 게인(compliance control gain)을 실시간으로 조정함으로써, 예측하지 못한 동역학적 차이에 동적으로 적응하는 방법도 연구되고 있다.40</li>
</ul>
<p>이러한 노력들은 VLA가 시뮬레이션의 데이터 효율성과 실제 세계의 복잡성 사이에서 균형을 잡고, 더 강건하고 실용적인 로봇 지능으로 발전하기 위한 필수적인 연구 방향이다.</p>
<h2>5.  응용 분야 및 도메인 확장</h2>
<p>VLA 모델은 시각, 언어, 행동을 통합하는 고유한 능력 덕분에 전통적인 로봇 공학의 영역을 넘어 다양한 산업 분야로 빠르게 확장되고 있다. 가정 및 산업 현장에서의 로봇 조작은 물론, 자율 주행, 헬스케어 등 인간의 삶과 밀접한 분야에서 VLA의 잠재력이 활발히 탐구되고 있다.</p>
<h3>5.1  로봇 조작 (Robotic Manipulation)</h3>
<p>로봇 조작은 VLA 모델의 가장 핵심적이고 직접적인 응용 분야이다. VLA는 복잡하고 비정형적인 환경에서 인간의 자연어 지시를 이해하고 정밀한 조작 과업을 수행하는 데 있어 기존의 프로그래밍 기반 로봇 시스템을 뛰어넘는 유연성을 제공한다.</p>
<h4>5.1.1  가정 및 산업 자동화</h4>
<p>가정 환경에서 VLA 기반 로봇은 “테이블 위를 깨끗이 닦고 드라이버를 가져다줘“와 같은 복합적인 명령을 수행할 수 있다.5 이는 단순히 물건을 옮기는 것을 넘어, 오염물을 닦아내고, 특정 도구를 식별하며, 다른 물건들을 피해서 움직이는 등 다단계의 의미론적 이해와 행동 계획을 요구하는 작업이다. 또한, 식기 세척기에서 접시를 꺼내거나 7, 샐러드를 만드는 것과 같은 4 복잡한 가사 노동을 자동화할 수 있는 가능성을 보여준다.</p>
<p>산업 현장에서는 부품을 종류별로 분류하거나, 조립 라인에서 특정 부품을 집어 옮기거나, 생산된 제품의 외관을 검사하는 등의 과업에 VLA가 적용될 수 있다.5 기존의 산업용 로봇이 고정된 위치에서 반복적인 작업만 수행할 수 있었던 반면, VLA 기반 로봇은 작업 환경의 변화나 새로운 제품 종류에 대해 수작업 프로그래밍 없이 자연어 지시만으로 유연하게 대처할 수 있다.7 이는 생산 라인의 유연성을 크게 높이고 다품종 소량 생산 환경에 효과적으로 대응할 수 있게 한다.</p>
<h4>5.1.2  양팔 협응 및 정밀 조작</h4>
<p>VLA의 능력은 단일 로봇 팔 제어를 넘어, 인간처럼 두 팔을 협응하여 사용하는 더 복잡한 작업으로 확장되고 있다. Shake-VLA와 같은 시스템은 양팔 로봇을 제어하여 칵테일을 제조하는 과업을 성공적으로 시연했다.41 이 시스템은 사용자의 음성 명령을 인식하고, 비전 모듈을 통해 여러 재료 병을 식별하며, 레시피 데이터베이스를 참조하여 양팔을 사용해 쉐이커를 흔들고, 컵에 따르는 등 정교한 협응 동작을 생성한다. 이는 VLA가 단일 행동 생성을 넘어, 두 팔의 움직임을 시간적으로 동기화하고 조율하는 복잡한 계획을 수립할 수 있음을 보여준다. 이러한 양팔 조작 능력은 조립, 포장, 수술 보조 등 더 넓은 범위의 응용 분야로 VLA 기술이 확장될 수 있는 중요한 가능성을 시사한다.</p>
<h3>5.2  자율 주행 (Autonomous Driving)</h3>
<p>자율 주행은 VLA 패러다임이 가장 활발하게 적용되고 있는 안전-필수(safety-critical) 분야 중 하나이다. 전통적인 자율 주행 시스템이 인식, 예측, 계획, 제어 등 여러 모듈로 분리된 파이프라인 구조를 가졌던 반면, VLA는 이러한 기능들을 하나의 종단간(end-to-end) 모델로 통합하여 더 인간과 유사한 방식으로 주행 결정을 내릴 수 있는 잠재력을 가진다.42</p>
<p>VLA 기반 자율 주행 모델은 차량에 장착된 카메라로부터 원시 센서 입력을 받고, “다음 교차로에서 좌회전해서 마트 주차장으로 들어가 줘“와 같은 언어적 지시를 함께 입력받는다. 모델은 이 정보를 바탕으로 복잡한 교통 상황을 의미론적으로 이해하고(예: “전방의 자전거 타는 사람이 갑자기 차선으로 들어올 위험이 있다”), 최종적으로 차량의 조향각, 가속/감속과 같은 제어 값을 직접 출력하는 것을 목표로 한다.42</p>
<p>AutoVLA와 같은 최신 모델들은 연쇄 사고(Chain-of-Thought, CoT) 추론 능력을 주행 정책에 통합하려는 시도를 하고 있다.43 예를 들어, 복잡한 교차로에 진입할 때, 모델은 “좌회전 신호가 녹색이다. 횡단보도에 보행자가 없다. 반대편 직진 차량이 멀리 있다. 안전하게 좌회전할 수 있다“와 같은 내부적인 추론 과정을 명시적으로 생성하고, 이를 바탕으로 궤적을 계획한다. 이러한 접근법은 주행 결정의 안정성과 신뢰성을 높일 수 있다.</p>
<p>또한, VLA는 자율 주행 시스템의 **해석 가능성(interpretability)**을 향상시키는 데 중요한 역할을 한다. VLAAD와 같은 연구는 모델이 왜 특정 주행 결정을 내렸는지에 대해 “전방에 갑자기 정차한 차량을 피하기 위해 차선을 변경했습니다“와 같이 자연어로 설명을 생성하도록 학습시킨다.44 이는 사용자와 시스템 간의 신뢰를 구축하고, 사고 발생 시 원인을 분석하는 데 결정적인 정보를 제공할 수 있어 VLA의 중요한 연구 방향으로 주목받고 있다.</p>
<h3>5.3  기타 유망 분야</h3>
<p>VLA 모델의 적용 범위는 앞서 언급한 분야 외에도 인간의 활동이 이루어지는 거의 모든 영역으로 확장될 잠재력을 가지고 있다.</p>
<ul>
<li><strong>헬스케어 및 보조 로봇:</strong> 병원이나 요양 시설에서 VLA 기반 로봇은 환자나 노인의 구두 요청을 이해하고 물건을 가져다주는 단순한 작업부터, 환자의 자세나 피부색 변화와 같은 시각적 건강 지표를 감지하여 의료진에게 알리는 등 능동적인 보조 역할을 수행할 수 있다.6 이는 의료 인력의 부담을 줄이고 환자 케어의 질을 높이는 데 기여할 수 있다.</li>
<li><strong>정밀 농업, 증강 현실, 재난 대응:</strong> 농업 분야에서는 특정 과일이나 채소만을 선별하여 수확하는 로봇에 적용될 수 있으며 5, 증강 현실(AR) 분야에서는 사용자의 시야와 음성 명령을 바탕으로 실시간 내비게이션 정보를 제공하는 데 활용될 수 있다.1 또한, 건설 현장이나 재난 지역과 같이 인간이 직접 접근하기 위험한 환경에서 잔해를 피하며 순찰하거나 특정 물체를 회수하는 임무를 수행하는 로봇에도 VLA 기술이 핵심적인 역할을 할 것으로 기대된다.5</li>
</ul>
<p>이처럼 VLA의 응용 분야 확장은 단순히 ’더 많은 작업’을 수행하는 것을 넘어, 각 도메인의 고유한 요구사항에 맞춰 VLA 아키텍처 자체가 ’분화’하고 ’특화’되는 과정으로 이해할 수 있다. 초기 VLA 연구가 주로 통제된 환경에서의 물체 조작에 집중하여 기술의 기본 가능성을 검증했다면 7, 자율 주행과 같은 복잡하고 실용적인 도메인으로의 확장은 VLA에 새로운 차원의 요구사항을 부과하고 있다.</p>
<p>특히 자율 주행과 같은 안전-필수 분야는 VLA에 <strong>안전성</strong>과 <strong>해석 가능성</strong>이라는 매우 엄격한 기준을 요구한다.44 잘못된 행동 하나가 치명적인 결과를 초래할 수 있기 때문에, 모델은 단순히 정답을 맞히는 것을 넘어 자신의 결정 과정을 논리적으로 설명하고 그 결정의 안전성을 스스로 검증할 수 있어야 한다. 이러한 요구사항은 VLA 모델이 단순한 ‘입력-출력’ 매핑 기계에서 벗어나, AutoVLA의 CoT 추론 43이나 VLAAD의 시각적 지시 튜닝 44처럼, 자신의 ’생각 과정’을 명시적으로 생성하고 외부와 소통하도록 만드는 강력한 동인이 되고 있다.</p>
<p>따라서 VLA의 도메인 확장은 일방적인 기술 전파 과정이 아니다. 오히려 각 도메인, 특히 자율 주행과 같이 사회적 책임이 큰 분야가 제기하는 고유한 문제들이 VLA 연구 커뮤니티에 피드백을 주어, 모델이 더 안전하고, 해석 가능하며, 강건한 방향으로 진화하도록 이끄는 ’상호작용적 과정’이다. 이는 VLA가 실험실의 흥미로운 기술에서 실제 산업 현장의 신뢰할 수 있는 기술로 전환되는 과정에서 겪는 중요한 성숙 단계라 할 수 있다.</p>
<h2>6.  당면 과제 및 연구 방향</h2>
<p>VLA 모델은 체화 인공지능 분야에서 괄목할 만한 진전을 이루었지만, 범용 로봇 지능의 실현까지는 여전히 해결해야 할 많은 기술적, 개념적 과제들이 남아있다. 실시간 추론의 제약, 미지의 상황에 대한 일반화 능력의 한계, 그리고 물리적 세계에서의 안전성 확보는 VLA 연구가 앞으로 나아가야 할 방향을 제시하는 핵심적인 도전 과제들이다.</p>
<h3>6.1  기술적 난제</h3>
<h4>6.1.1  실시간 추론 및 계산 비용</h4>
<p>현재 가장 성능이 뛰어난 VLA 모델들은 수십억에서 수백억 개에 달하는 파라미터를 가진 대규모 신경망에 기반하고 있다. 이는 모델의 강력한 성능을 뒷받침하는 원동력이지만, 동시에 실시간 제어가 필수적인 로봇 시스템에 배포하는 데 있어 심각한 병목으로 작용한다.25 예를 들어, 550억 파라미터 규모의 RT-2 모델은 여러 개의 고성능 TPU가 탑재된 클라우드 서버를 통해서도 약 1-3 Hz의 추론 속도를 보이는데 22, 이는 동적인 환경 변화에 민첩하게 반응해야 하는 로봇에게는 너무 느린 속도이다. 이 문제를 해결하기 위해 다음과 같은 연구들이 활발히 진행되고 있다.</p>
<ul>
<li><strong>모델 경량화:</strong> 지식 증류(knowledge distillation), 프루닝(pruning), 양자화(quantization) 등의 기법을 통해 모델의 크기를 줄이면서 성능 저하를 최소화하는 연구.</li>
<li><strong>추론 최적화:</strong> 액션 청킹(action chunking)과 병렬 디코딩(parallel decoding)을 결합하여 자기회귀적 생성 과정의 속도를 높이는 PD-VLA와 같은 연구.45</li>
<li><strong>하드웨어 가속:</strong> VLA 모델의 연산을 효율적으로 처리할 수 있는 전용 하드웨어(NPU, TPU 등)를 로봇에 탑재하여 온보드 추론(on-board inference) 속도를 향상시키는 접근법.</li>
</ul>
<h4>6.1.2  일반화의 한계</h4>
<p>VLA 모델은 OpenX와 같은 대규모 이종 데이터셋으로 학습함으로써 전례 없는 일반화 능력을 보여주었지만, 그 능력에는 여전히 명확한 한계가 존재한다. 현재 모델들은 학습 데이터에 포함된 로봇 형태(embodiment)나 환경 분포 내에서는 뛰어난 제로샷(zero-shot) 성능을 보이지만, 학습 데이터에서 완전히 벗어나는 새로운 로봇이나 한 번도 본 적 없는 생소한 과업(unseen tasks)에 대해서는 성능이 급격히 저하되는 경향이 있다.33</p>
<p>AGNOSTOS와 같은 새로운 벤치마크는 이러한 문제를 명확히 보여준다. 이 벤치마크는 일반적인 학습 데이터 분포와는 다른 23개의 새로운 조작 과업으로 구성되어 있는데, 실험 결과 기존의 VLA 모델들이 이러한 미지의 과제에 대해 일반화하는 데 큰 어려움을 겪는 것으로 나타났다.47 이는 VLA 모델이 아직 진정한 의미의 ’추상적 기술’을 학습했다기보다는, 방대한 데이터 속의 패턴을 ’보간(interpolation)’하는 데 더 가깝다는 것을 시사한다. 이 한계를 극복하기 위해서는 더 근본적인 기술 전이(skill transfer)와 메타 학습(meta-learning)에 대한 연구가 필요하다.</p>
<h4>6.1.3  물리적 상호작용의 정확성</h4>
<p>현재 대부분의 VLA 모델은 주로 시각 정보에 의존하여 행동을 결정한다. 이로 인해 물체와의 접촉(contact), 힘(force), 마찰(friction)과 같은 복잡하고 미묘한 물리적 상호작용을 정밀하게 다루는 데 한계를 보인다. 이는 섬세한 조립 과업이나, 물체와 충돌하지 않고 좁은 공간을 통과해야 하는 작업에서 실패의 주요 원인이 된다.7</p>
<p>이 문제를 해결하기 위해, 단순히 현재 상태에서 다음 행동을 결정하는 반응적인(reactive) 정책을 넘어, 자신의 행동이 미래에 어떤 물리적 결과를 가져올지 예측하는 **예측적 세계 모델링(predictive world modeling)**을 VLA에 통합하려는 시도가 이루어지고 있다. DreamVLA와 같은 모델은 행동을 생성하기 전에 미래의 장면이 어떻게 변할지를 예측하고, 이 예측을 바탕으로 역동역학 모델링(inverse dynamics modeling)을 수행하여 더 정확하고 물리적으로 타당한 행동을 계획한다.49 또한, 촉각이나 힘/토크 센서와 같은 비-시각적 양식을 모델에 통합하여 물리적 상호작용에 대한 더 풍부한 정보를 제공하는 연구도 중요한 방향이다.</p>
<h4>6.1.4  데이터 희소성</h4>
<p>OpenX와 같은 대규모 데이터셋의 등장에도 불구하고, 고품질의 로봇 시연 데이터를 수집하는 것은 여전히 비용과 시간이 많이 소요되는 어려운 문제로 남아있다.7 특히 새로운 로봇이나 특정 산업 분야에 VLA를 적용하기 위해서는 해당 도메인에 맞는 데이터가 필요한데, 이를 매번 대규모로 수집하는 것은 비현실적이다. 따라서 더 적은 데이터로 효율적으로 학습하고 적응할 수 있는 데이터 효율적인(data-efficient) 학습 방법론, 자기지도학습(self-supervised learning)을 통한 데이터 생성, 그리고 시뮬레이션 데이터와 소량의 실제 데이터를 효과적으로 결합하는 Sim-to-Real 기술의 발전이 지속적으로 요구된다.</p>
<h3>6.2  안전성 및 정렬</h3>
<p>VLA 기반 로봇이 실험실을 넘어 인간과 함께 생활하고 작업하는 공간으로 들어오기 위해서는, 기술적 성능을 넘어 안전성과 신뢰성을 확보하는 것이 무엇보다 중요하다. 이는 VLA가 단순히 주어진 명령을 수행하는 것을 넘어, 그 명령의 타당성을 판단하고 인간의 의도 및 사회적 가치와 부합하게(aligned) 행동해야 함을 의미한다.</p>
<h4>6.2.1  잘못된 전제 (False Premise) 문제</h4>
<p>VLA 모델의 안전성과 관련된 중요한 문제 중 하나는 ‘잘못된 전제’ 문제이다. 이는 사용자가 현재 환경에 존재하지 않는 객체나 불가능한 상태를 가정하고 명령을 내리는 상황을 말한다. 예를 들어, 방 안에 파란색 금고가 없음에도 “파란색 금고를 닫아줘“라고 명령할 경우, 대부분의 현재 VLA 모델들은 이 명령이 수행 불가능하다는 사실을 인지하지 못하고, 엉뚱한 행동을 하거나 아무런 반응 없이 실패하는 경향이 있다.50</p>
<p>이 문제를 해결하기 위해 제안된 IVA(Instruct-Verify-and-Act) 프레임워크는 VLA 모델에 ‘검증’ 단계를 추가한다.50 이 프레임워크로 학습된 모델은 명령을 받으면 먼저 시각 정보를 통해 그 명령이 현재 상태에서 실행 가능한지 검증한다. 만약 불가능하다고 판단되면, “파란색 금고는 없지만, 대신 갈색 항아리가 있습니다. 항아리를 닫아드릴까요?“와 같이 언어적으로 문제를 지적하고, 대안을 제시하거나, 명확한 설명을 요구하는 방식으로 사용자와 소통한다. 이러한 능력은 로봇이 맹목적인 명령 수행 기계가 아니라, 상황을 판단하고 소통할 수 있는 지능적인 협력 파트너로 발전하는 데 있어 필수적인 단계이다.</p>
<h4>6.2.2  물리적 안전성 및 인간과의 정렬</h4>
<p>로봇은 물리적 세계에서 직접 행동하기 때문에, 그 행동이 인간이나 주변 환경에 해를 끼치지 않도록 보장하는 것이 매우 중요하다. 이는 단순히 장애물을 피하는 충돌 회피(collision avoidance)와 같은 저수준의 안전 기능을 넘어 52, 더 높은 수준의 의미론적 안전성을 포함한다. 예를 들어, 로봇은 ‘주의’ 테이프로 둘러싸인 구역에 들어가서는 안 된다는 사회적 규범을 이해해야 하고, 날카로운 칼을 다룰 때는 평소보다 더 조심스럽게 움직여야 한다는 상황적 판단을 할 수 있어야 한다.</p>
<p>이러한 고수준의 안전성과 인간의 가치와의 정렬(alignment)을 위해, 최신 VLA 연구들은 모델의 의사결정 과정에 명시적인 안전 고려 단계를 통합하고 있다. Google의 Gemini Robotics 1.5는 이러한 접근법을 잘 보여준다. 이 시스템은 행동을 실행하기 전에, 계획된 행동이 안전한지 스스로 생각하고(thinking about safety before acting), 인간과의 대화에서는 기존의 Gemini 안전 정책을 준수하며, 필요한 경우 저수준의 충돌 회피 시스템을 능동적으로 호출하는 등 다층적인 안전 아키텍처를 구현하고 있다.53 이러한 연구들은 VLA가 단순한 명령 수행 능력을 넘어, 책임감 있고 신뢰할 수 있는 방식으로 행동하도록 만드는 것을 목표로 한다.50</p>
<p>VLA 분야의 핵심 과제가 ’지능의 확장’에서 ’지능의 제어 및 검증’으로 이동하고 있다는 점은 주목할 만하다. RT-2와 같은 초기 연구가 “VLA가 무엇을 할 수 있는가?“라는 가능성을 탐구하는 데 집중했다면 13, 현재와 미래의 연구는 “VLA가 무엇을 ‘하지 말아야’ 하는가?“와 “우리는 VLA의 행동을 어떻게 신뢰하고 검증할 수 있는가?“라는 더 근본적이고 어려운 질문에 답하는 방향으로 심화되고 있다. ‘잘못된 전제’ 문제 50는 VLA가 언어를 문자 그대로 해석할 뿐, 그 기저에 있는 상식적 타당성을 검증하지 못함을 보여준다. 이는 지능은 있지만 지혜는 부족한 상태와 같다. IVA 프레임워크 50는 이러한 문제에 대한 직접적인 해결책으로, 단순한 행동 생성을 넘어 ’명령 검증’과 ’의사소통을 통한 수정’이라는 메타-인지적(meta-cognitive) 능력을 모델에 부여하려는 시도이다. Gemini Robotics 1.5가 ’행동 전 안전 고려’를 수행한다는 점 53 역시, 모델의 행동 출력이 더 이상 반사적인 반응이 아니라, 내부적인 검증과 숙고의 과정을 거쳐야 한다는 패러다임의 변화를 의미한다. 결론적으로, VLA 분야는 기술적 성능의 한계를 넓히는 동시에, 그 성능을 안전하고 신뢰할 수 있는 방식으로 제어하기 위한 ’가드레일’을 구축하는 이중의 과제에 직면해 있다. 미래의 가장 중요한 혁신은 더 화려한 기술 시연이 아니라, VLA가 불가능한 것을 거절하고 50, 자신의 행동을 설명하며, 안전 원칙을 준수하는 능력을 갖추는 데서 나올 것이다. 이는 기술적 성숙의 필연적인 다음 단계이다.</p>
<h3>6.3  미래 연구 전망</h3>
<p>앞서 논의된 당면 과제들을 해결하고 VLA 모델의 능력을 한 단계 더 발전시키기 위해, 다음과 같은 연구 방향들이 미래의 핵심적인 조류를 형성할 것으로 전망된다.</p>
<h4>6.3.1  계층적 강화 학습 (Hierarchical Reinforcement Learning, HRL)의 통합</h4>
<p>현재 VLA 모델들은 대부분 단일 시간 스텝의 행동을 예측하는 반응적인 정책에 의존하고 있어, “아침 식사 준비하기“와 같이 여러 단계로 구성된 장기적인 과업을 효과적으로 계획하고 수행하는 데 어려움이 있다. 계층적 강화 학습(HRL)은 이러한 문제를 해결하기 위한 유력한 프레임워크를 제공한다.56 HRL은 복잡한 과업을 “커피 만들기”, “토스트 굽기”, “계란 프라이 하기“와 같은 더 작은 하위 목표(subgoal)들로 자동 분해하고, 각 하위 목표를 달성하기 위한 저수준 정책(option)을 학습하는 계층적 구조를 가진다.56 고수준 정책은 상황에 맞는 하위 목표의 순서를 결정하고, 저수준 정책은 해당 하위 목표를 수행하기 위한 구체적인 모터 명령을 생성한다. HRL을 VLA에 통합함으로써, 모델은 시간적 추상화(temporal abstraction) 능력을 갖추게 되어 더 효율적으로 장기 계획을 수립하고, 학습된 하위 기술을 새로운 과업에 재사용하여 일반화 성능을 높일 수 있을 것이다.</p>
<h4>6.3.2  진정한 다중 양식 학습 (Beyond Vision and Language)</h4>
<p>현재 VLA 모델의 이름이 암시하듯, 입력 양식은 주로 시각과 언어에 국한되어 있다. 하지만 인간은 세상을 인식하고 상호작용할 때 촉각, 청각, 고유수용성감각 등 훨씬 더 다양한 감각을 복합적으로 활용한다. 미래의 VLA는 이러한 비-시각적, 비-언어적 양식을 적극적으로 통합하는 방향으로 나아갈 것이다. 예를 들어, 로봇 손에 부착된 촉각 센서는 물체의 질감, 미끄러짐, 접촉 압력을 감지하여 물건을 더 안정적으로 잡거나 섬세하게 조작하는 데 결정적인 정보를 제공할 수 있다.59 힘/토크 센서는 문을 열거나 나사를 조이는 등 정확한 힘 조절이 필요한 작업에 필수적이다.7 또한, 오디오 정보는 물체가 부서지는 소리나 기계의 작동음 등을 통해 환경의 상태 변화를 감지하는 데 사용될 수 있다.60 이러한 진정한 의미의 다중 양식(multi-modal) 입력을 통해 VLA는 주변 환경에 대한 훨씬 더 풍부하고 강건한 이해를 바탕으로, 더 정밀하고 안전한 물리적 상호작용을 수행할 수 있게 될 것이다.</p>
<h4>6.3.3  세계 모델 (World Models) 및 인과 관계 추론</h4>
<p>현재의 반응적인(reactive) VLA 모델들은 “현재 상태에서 무엇을 해야 하는가“에 대한 답을 찾는 데 집중한다. 하지만 더 높은 수준의 지능을 위해서는 “내가 이 행동을 하면 미래에 어떤 일이 일어날 것인가“를 예측하고 이해하는 능력이 필요하다. 이를 위해 VLA 내부에 환경의 동역학을 학습하고 미래를 시뮬레이션할 수 있는 **세계 모델(World Model)**을 내재화하는 연구가 중요해질 것이다.10 세계 모델을 통해 로봇은 여러 행동 후보들의 결과를 미리 예측해보고 가장 좋은 결과를 낳는 행동을 선택하는, 더 깊이 있는 계획(planning)을 수행할 수 있다. 이는 특히 객체들이 서로 상호작용하는 동적인 환경에서 시스템의 강건성을 크게 향상시킬 수 있다. 더 나아가, 단순히 현상 간의 상관관계를 학습하는 것을 넘어, 행동과 결과 사이의 **인과 관계(causality)**를 추론하는 능력은 모델이 예상치 못한 상황에 더 잘 대처하고, 자신의 실패 원인을 분석하여 스스로 학습하는 능력을 갖추는 데 핵심적인 역할을 할 것이다.</p>
<h4>6.3.4  신경-기호 (Neuro-symbolic) 접근법</h4>
<p>딥러닝 기반의 VLA 모델은 패턴 인식과 일반화에 강점을 가지지만, 그 의사결정 과정이 불투명하여 해석하기 어렵고, 논리적 추론에 취약할 수 있다는 단점이 있다. 반면, 기호적 AI(Symbolic AI)는 명시적인 규칙과 논리를 기반으로 작동하여 해석 가능하고 강건한 추론을 제공한다. <strong>신경-기호 접근법</strong>은 이 두 패러다임의 장점을 결합하려는 시도이다.1 예를 들어, VLA의 신경망 부분이 시각적 장면에서 객체와 그 관계를 인식하여 “컵(A), 테이블(B), A는 B 위에 있다“와 같은 기호적 표현으로 변환하면, 기호적 추론 엔진이 이 정보를 바탕으로 논리적인 계획을 수립하고, 다시 신경망 기반의 제어기가 이 계획을 실행하는 방식이다. 이러한 결합은 VLA의 해석 가능성과 신뢰성을 크게 향상시켜, 특히 안전이 중요한 응용 분야에서 모델의 결정을 검증하고 디버깅하는 것을 용이하게 만들 것이다.</p>
<h2>7.  결론</h2>
<h3>7.1  VLA 모델의 현재 위상과 핵심 기여 요약</h3>
<p>Vision-Language-Action (VLA) 모델은 지난 몇 년간 인공지능 및 로봇 공학 분야에서 가장 주목받는 연구 영역 중 하나로 부상하며, 범용 체화 인공지능(general-purpose embodied AI)을 향한 중요한 이정표를 세웠다. VLA 모델의 가장 핵심적인 기여는 전통적으로 분리되어 있던 시각 인식, 자연어 이해, 로봇 행동 제어라는 세 가지 영역을 단일 프레임워크 내에서 성공적으로 통합했다는 점이다. 이를 통해 기존의 분절된 파이프라인 방식이 가졌던 정보 손실, 복잡한 통합, 그리고 제한된 일반화 능력의 한계를 근본적으로 극복하는 새로운 패러다임을 제시했다.</p>
<p>특히, VLA 모델은 대규모 사전 학습된 비전-언어 모델(VLM)이 인터넷 스케일 데이터로부터 학습한 방대한 시각적, 의미론적 지식을 로봇의 물리적 행동으로 성공적으로 전이시킬 수 있음을 증명했다. RT-2와 같은 선구적인 모델은 ‘행동을 언어처럼’ 다루는 혁신적인 접근을 통해, 로봇이 훈련 데이터에 없던 새로운 개념을 이해하고 추론 기반의 행동을 수행하는 창발적 능력을 보여주었다. 뒤이어 OpenVLA와 같은 강력한 오픈소스 모델의 등장은 VLA 연구의 민주화를 이끌고, 데이터의 다양성과 효율적인 아키텍처의 중요성을 부각시키며 학계와 산업계 전반에 걸쳐 폭발적인 연구와 발전을 촉진하고 있다. 결과적으로 VLA는 로봇에게 전례 없는 수준의 일반화 능력과 의미론적 이해 능력을 부여함으로써, 인간과 자연스럽게 소통하고 비정형 환경에 유연하게 적응하는 지능형 로봇의 실현 가능성을 한층 앞당겼다.</p>
<h3>7.2  범용 체화 인공지능을 향한 장기적 관점</h3>
<p>VLA 모델이 이룬 눈부신 성과에도 불구하고, 진정한 의미의 범용 체화 인공지능을 실현하기까지는 아직 갈 길이 멀다. 현재의 VLA는 실시간 추론 성능의 제약, 미지의 환경과 과업에 대한 강건한 일반화 능력의 부족, 그리고 물리적 세계에서의 안전성 및 신뢰성 확보라는 중대한 과제들을 안고 있다. 이러한 도전들은 VLA 연구가 나아가야 할 미래 방향을 명확히 제시한다.</p>
<p>미래의 VLA 모델은 현재의 반응적인 정책을 넘어, 더 깊이 있는 이해와 계획 능력을 갖추는 방향으로 진화할 것이다. 복잡하고 장기적인 과업을 효율적으로 학습하고 수행하기 위해 계층적 강화 학습(HRL)의 원리가 통합될 것이며, 시각과 언어를 넘어 촉각, 청각 등 더 다양한 감각 양식을 통합하여 주변 세계에 대한 훨씬 더 풍부한 이해를 구축하게 될 것이다. 또한, 자신의 행동이 가져올 결과를 예측하는 내재적 세계 모델(world model)과 인과 관계 추론 능력을 통해, 동적이고 예측 불가능한 환경에 대한 강건성을 획기적으로 높일 것이다. 더 나아가, 딥러닝의 직관적 패턴 인식 능력과 기호적 추론의 논리적 강건성을 결합하는 신경-기호(Neuro-symbolic) 접근법은 모델의 해석 가능성과 신뢰성을 확보하는 데 중요한 역할을 할 것이다.</p>
<p>궁극적으로 VLA 연구의 지향점은 단순히 기술적으로 유능한 로봇을 만드는 것을 넘어서야 한다. 인간과 안전하게 상호작용하고, 인간의 의도를 깊이 있게 이해하며, 예측 불가능한 세상에 강건하게 대처할 수 있는 **신뢰할 수 있는 파트너(trustworthy partner)**로서의 인공지능을 실현하는 것이 최종 목표가 되어야 한다. 이 원대한 여정은 이제 막 시작되었으며, VLA 기술이 앞으로 인류의 삶과 사회를 어떻게 변화시킬지 그 무한한 가능성에 대한 기대가 크다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Vision-Language-Action Models: Concepts, Progress, Applications and Challenges - arXiv, https://arxiv.org/html/2505.04769v1</li>
<li>(PDF) Vision-Language-Action Models: Concepts, Progress, Applications and Challenges, https://www.researchgate.net/publication/391575814_Vision-Language-Action_Models_Concepts_Progress_Applications_and_Challenges</li>
<li>Vision-language-action model - Wikipedia, https://en.wikipedia.org/wiki/Vision-language-action_model</li>
<li>Vision-Language-Action Model - Emergent Mind, https://www.emergentmind.com/topics/vision-language-action-vla-model</li>
<li>How Vision-Language-Action Models Powering Humanoid Robots - Labellerr, https://www.labellerr.com/blog/vision-language-action-vla-models-2/</li>
<li>Multimodal &amp; Vision-Language-Action Models (VLA): The Future of AI That Sees, Thinks, and Acts | by Dr Sudersan behera | Sep, 2025 | Medium, https://medium.com/@sbehera.04/multimodal-vision-language-action-models-vla-the-future-of-ai-that-sees-thinks-and-acts-8855aa1fc59b</li>
<li>Vision-Language-Action (VLA) Models: LLMs for robots, https://www.blackcoffeerobotics.com/blog/vision-language-action-vla-models-llms-for-robots</li>
<li>What is a Vision-Language-Action (VLA) model? - The AI Navigator, https://www.theainavigator.com/blog/what-is-a-vision-language-action-vla-model</li>
<li>Vision-Language-Action (VLA) Models: LLMs for robots | by Gaurav Gupta - Medium, https://medium.com/black-coffee-robotics/vision-language-action-vla-models-llms-for-robots-f60ba0b79579</li>
<li>Pure Vision Language Action (VLA) Models: A Comprehensive Survey - arXiv, https://arxiv.org/html/2509.19012v1</li>
<li>Vision-Language-Action - LEGENT, https://docs.legent.ai/documentation/model/vla/</li>
<li>RT-2: New model translates vision and language into action - Google DeepMind, https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/</li>
<li>Vision-Language-Action Models: RT-2, https://robotics-transformer2.github.io/</li>
<li>Vision Language Action Models (VLA) Overview: LeRobot Policies Demo, https://learnopencv.com/vision-language-action-models-lerobot-policy/</li>
<li>OpenVLA: An Open-Source Vision-Language-Action Model, https://openvla.github.io/</li>
<li>Episode 2: Under the Hood of OpenVLA – Architecture and Inference, https://bettercallshao.com/episode-2-under-the-hood-of-openvla-architecture-and-inference/</li>
<li>The Complete Guide to Vision-Language-Action Models: How Robots Are Learning to Think, https://medium.com/@ian_25476/the-complete-guide-to-vision-language-action-models-how-robots-are-learning-to-think-f1a788d003ed</li>
<li>Deploying Vision Language Action (VLA) based AI Models in Robotics: Optimization for Real-Time Edge Inference - MulticoreWare, https://multicorewareinc.com/deploying-vision-language-action-vla-based-ai-models-in-robotics-optimization-for-real-time-edge-inference/</li>
<li>OpenVLA: An Open-Source Vision-Language-Action Model - arXiv, https://arxiv.org/html/2406.09246v1</li>
<li>Bridging Worlds: How Visual Language Action Models are Teaching Robots to See, Understand, and Act - Finding Theta, https://www.findingtheta.com/blog/bridging-worlds-how-visual-language-action-models-are-teaching-robots-to-see-understand-and-act</li>
<li>Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey, https://arxiv.org/html/2508.13073v1</li>
<li>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control - Texas Computer Science, https://www.cs.utexas.edu/~yukez/cs391r_fall2023/slides/pre_10-24_Ming.pdf</li>
<li>\method: Hierarchical Action Models for Open-World Robot Manipulation - arXiv, https://arxiv.org/html/2502.05485v1</li>
<li>RT-2, Robotic Transformer 2 Review | gracefullight.dev, https://gracefullight.dev/en/2025/08/24/rt-2-review/</li>
<li>[2505.23705] Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better - arXiv, https://arxiv.org/abs/2505.23705</li>
<li>VLAs that Train Fast, Run Fast, and Generalize Better - Physical Intelligence, https://www.physicalintelligence.company/research/knowledge_insulation</li>
<li>kyegomez/RT-2: Democratization of RT-2 “RT-2: New model translates vision and language into action” - GitHub, https://github.com/kyegomez/RT-2</li>
<li>PaLM-E: An Embodied Multimodal Language Model, https://palm-e.github.io/</li>
<li>arXiv:2303.03378v1 [cs.LG] 6 Mar 2023 - PaLM-E, https://palm-e.github.io/assets/palm-e.pdf</li>
<li>PaLM-E: An embodied multimodal language model - Google Research, https://research.google/blog/palm-e-an-embodied-multimodal-language-model/</li>
<li>PaLM-E An Embodied Multimodal Language Model Review - gracefullight.dev, https://gracefullight.dev/2025/08/24/palm-e-review/</li>
<li>Episode 3: Training a Robot’s Brain – OpenVLA’s Learning and Adaptation, https://bettercallshao.com/episode-3-training-a-robot-s-brain-openvla-s-learning-and-adaptation/</li>
<li>openvla/openvla-7b - Hugging Face, https://huggingface.co/openvla/openvla-7b</li>
<li>OpenVLA: An open-source vision-language-action model for robotic manipulation. - GitHub, https://github.com/openvla/openvla</li>
<li>[2505.17016] Interactive Post-Training for Vision-Language-Action Models - arXiv, https://arxiv.org/abs/2505.17016</li>
<li>Vision Language Action Models in Robotic Manipulation: A Systematic Review - arXiv, https://arxiv.org/html/2507.10672v1</li>
<li>Sim-to-real via latent prediction: Transferring visual non-prehensile manipulation policies, https://pmc.ncbi.nlm.nih.gov/articles/PMC9879568/</li>
<li>Natural Language Can Help Bridge the Sim2Real Gap - RobIn Robot Interactive Intelligence Lab, https://robin-lab.cs.utexas.edu/lang4sim2real/</li>
<li>What Went Wrong? Closing the Sim-to-Real Gap via Differentiable Causal Discovery, https://proceedings.mlr.press/v229/huang23c/huang23c.pdf</li>
<li>Bridging the Sim-to-Real Gap with Dynamic Compliance Tuning for Industrial Insertion, https://www.research.autodesk.com/publications/sim-to-real-gap-dynamic-compliance-tuning-industrial-insertion/</li>
<li>Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid Mixing - arXiv, https://arxiv.org/html/2501.06919v1</li>
<li>A Survey on Vision-Language-Action Models for Autonomous Driving - ResearchGate, https://www.researchgate.net/publication/393183933_A_Survey_on_Vision-Language-Action_Models_for_Autonomous_Driving</li>
<li>AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning - arXiv, https://arxiv.org/html/2506.13757v1</li>
<li>VLAAD: Vision and Language Assistant for Autonomous Driving - CVF Open Access, https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/papers/Park_VLAAD_Vision_and_Language_Assistant_for_Autonomous_Driving_WACVW_2024_paper.pdf</li>
<li>[2503.02310] Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding - arXiv, https://arxiv.org/abs/2503.02310</li>
<li>Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends | Request PDF - ResearchGate, https://www.researchgate.net/publication/393065604_Parallels_Between_VLA_Model_Post-Training_and_Human_Motor_Learning_Progress_Challenges_and_Trends</li>
<li>[2505.15660] Exploring the Limits of Vision-Language-Action Manipulations in Cross-task Generalization - arXiv, https://arxiv.org/abs/2505.15660</li>
<li>(PDF) Integrating Motion Planning in Vision Language Action Agents - ResearchGate, https://www.researchgate.net/publication/395828753_Integrating_Motion_Planning_in_Vision_Language_Action_Agents</li>
<li>[2507.04447] DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge - arXiv, https://arxiv.org/abs/2507.04447</li>
<li>Do What? Teaching Vision-Language-Action Models to Reject the Impossible - alphaXiv, https://www.alphaxiv.org/overview/2508.16292</li>
<li>Paper page - Do What? Teaching Vision-Language-Action Models to Reject the Impossible, https://huggingface.co/papers/2508.16292</li>
<li>Embodied Artificial Intelligence Safety, https://abajcsy.github.io/embodied-ai-safety/</li>
<li>Gemini Robotics 1.5 brings AI agents into the physical world - Google DeepMind, https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/</li>
<li>Safety of Embodied Navigation: A Survey - arXiv, https://arxiv.org/html/2508.05855v1</li>
<li>Embodied AI could help robots take flight, benefitting travel, logistics and many other industries, https://www.weforum.org/stories/2025/09/embodied-ai-robots-flight-travel-transport/</li>
<li>Hierarchical Reinforcement Learning, Sequential Behavior, and the Dorsal Frontostriatal System - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC9274316/</li>
<li>Hierarchical Reinforcement Learning: A Survey and Open Research Challenges - MDPI, https://www.mdpi.com/2504-4990/4/1/9</li>
<li>A Neural Signature of Hierarchical Reinforcement Learning - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC3145918/</li>
<li>Multi-Modal Perception with Vision, Language, and Touch for Robot Manipulation, https://escholarship.org/uc/item/71x5g5kk</li>
<li>multimodal learning integrating vision, language, and beyond - KSV Ejournal of Engineering, Management, Science and Humanities, https://jems.ksv.ac.in/wp-content/uploads/2025/08/MULTIMODAL-LEARNING-INTEGRATING-VISION-LANGUAGE-AND-BEYOND.pdf</li>
<li>“The First Multimodal AI Research Sprint: Beyond Vision &amp; Language.” This unique gathering is where the convergence of diverse data types and cutting-edge machine learning models and algorithms is not just discussed but actively pursued. Our focus is on exploring the untapped potential in Multimodal AI, venturing beyond the established domains of vision and language. This event is a journey into uncharted territories, seeking to uncover new challenges and opportunities in data and problem-solving that have yet to be fully explored. While our primary aim is to delve into these new areas, we also value and welcome the rich insights and contributions from the fields of vision and language. The knowledge from these areas will provide a solid foundation for our exploration and innovation. - UK Open Multimodal AI Network, https://multimodalai.github.io/multimodalai-sprint23/</li>
<li>: A Vision-Language-Action Model Bridging Understanding and Generation to Actions - arXiv, https://arxiv.org/html/2509.06951v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>