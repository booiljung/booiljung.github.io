<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:TinyVLA 로봇 조작을 위한 고속·데이터 효율적 시각-언어-행동 모델</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>TinyVLA 로봇 조작을 위한 고속·데이터 효율적 시각-언어-행동 모델</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">Vision-Language-Action(VLA) 모델</a> / <span>TinyVLA 로봇 조작을 위한 고속·데이터 효율적 시각-언어-행동 모델</span></nav>
                </div>
            </header>
            <article>
                <h1>TinyVLA 로봇 조작을 위한 고속·데이터 효율적 시각-언어-행동 모델</h1>
<p>2025-12-07, G30DR</p>
<h2>1.  서론: 임바디드 AI(Embodied AI)의 패러다임 전환</h2>
<h3>1.1  배경: 거대 모델의 시대와 로봇 공학의 딜레마</h3>
<p>현대 인공지능 연구는 대규모 언어 모델(Large Language Models, LLM)과 시각-언어 모델(Vision-Language Models, VLM)의 비약적인 발전에 힘입어 새로운 전기를 맞이했다. GPT-4, Gemini와 같은 모델들은 텍스트와 이미지를 이해하고 생성하는 데 있어 인간 수준, 혹은 그 이상의 능력을 보여주며 디지털 세계를 정복했다. 이러한 흐름은 자연스럽게 물리적 세계로 확장되어, 로봇이 인간의 언어 명령을 이해하고 시각 정보를 바탕으로 행동을 수행하는 ‘임바디드 AI(Embodied AI)’ 분야의 폭발적인 성장을 견인했다.</p>
<p>이러한 추세 속에서 등장한 것이 바로 시각-언어-행동(Vision-Language-Action, VLA) 모델이다. Google DeepMind의 RT-1(Robotic Transformer 1)과 RT-2, 그리고 최근의 OpenVLA와 같은 모델들은 인터넷 규모의 데이터로 사전 학습된 VLM의 강력한 상식 추론 능력을 로봇 제어 정책(Policy)으로 전이시키는 전략을 취했다. 이들 모델은 “파란색 칩 봉지를 집어서 숫자가 쓰여진 매트 위에 올려라“와 같은 복잡하고 모호한 자연어 명령을 수행할 수 있는 놀라운 일반화 성능을 입증했다.</p>
<p>그러나 이러한 ‘거대 모델(Large Model)’ 중심의 접근 방식은 로봇 공학의 본질적인 제약 조건과 충돌하며 심각한 딜레마를 야기하고 있다.</p>
<p>첫째, <strong>추론 속도(Inference Latency)의 한계</strong>다. 로봇 제어, 특히 동적인 환경에서의 정교한 조작(Manipulation)은 실시간성이 생명이다. 로봇은 변화하는 환경을 밀리초(ms) 단위로 감지하고 즉각적으로 관절을 제어해야 한다. 그러나 수십억(Billions)에서 수백억 개의 파라미터를 가진 트랜스포머(Transformer) 기반의 VLA 모델들은 추론에 막대한 연산 비용이 소요된다. 예를 들어, RT-2의 경우 추론 속도가 약 1~3Hz에 불과하여, 고속 피드백 제어가 필요한 실제 산업 현장이나 복잡한 가사 노동에 적용하기에는 턱없이 느리다.1</p>
<p>둘째, <strong>데이터 효율성(Data Efficiency)과 학습 비용</strong>의 문제다. 기존의 SOTA(State-of-the-Art) 모델들은 일반화 성능을 확보하기 위해 Open X-Embodiment와 같은 초대형 로봇 데이터셋을 사용한 사전 학습(Pre-training)에 의존한다. OpenVLA는 약 97만 개의 에피소드로 구성된 데이터셋을 학습하는 데 수백 개의 A100 GPU와 막대한 시간을 소모했다.2 더욱이 이러한 대규모 데이터셋은 대부분 단일 팔(Single-arm) 로봇 데이터로 편향되어 있어, 양팔(Bimanual) 로봇이나 휴머노이드와 같은 새로운 하드웨어(Embodiment)에 적용할 때 심각한 성능 저하를 겪거나, 다시 막대한 비용을 들여 재학습해야 하는 비효율성을 내포하고 있다.2</p>
<h3>1.2  TinyVLA의 제안과 연구의 목적</h3>
<p>이러한 배경 속에서 등장한 <strong>TinyVLA</strong>는 기존의 “더 큰 모델, 더 많은 데이터“라는 도그마에 정면으로 도전한다. TinyVLA 연구진은 “과연 로봇 제어를 위해 거대한 모델과 막대한 사전 학습이 필수적인가?“라는 근본적인 질문을 던지며, <strong>경량화된 VLM 백본(Backbone)과 확산 정책(Diffusion Policy)의 결합</strong>이라는 새로운 아키텍처를 제안한다.4</p>
<p>본 보고서는 TinyVLA의 기술적 아키텍처, 학습 전략, 그리고 실험적 성능을 포괄적이고 심층적으로 분석한다. 특히, TinyVLA가 어떻게 파라미터 수를 획기적으로 줄이면서도 OpenVLA와 같은 거대 모델을 상회하는 성능을 달성했는지, 그 기저에 깔린 공학적 원리를 규명한다. 또한, 사전 학습 없이도 높은 일반화 성능을 발휘하는 메커니즘을 분석하고, 이를 통해 향후 엣지(Edge) 디바이스 기반의 고속 로봇 제어 시스템이 나아가야 할 방향을 제시한다. 본 보고서는 단순한 모델 소개를 넘어, VLA 모델의 설계 철학(Design Philosophy)과 로봇 학습(Robot Learning)의 미래에 대한 통찰을 제공하는 것을 목적으로 한다.</p>
<h2>2.  VLA 모델의 기술적 배경 및 현황 분석</h2>
<h3>2.1  시각-언어-행동(VLA) 모델의 작동 원리</h3>
<p>VLA 모델은 기본적으로 시각적 입력(<span class="math math-inline">I</span>)과 자연어 명령어(<span class="math math-inline">L</span>)를 받아 로봇의 행동(<span class="math math-inline">A</span>)을 출력하는 함수 <span class="math math-inline">f(I, L) \rightarrow A</span>로 정의될 수 있다. 기존의 접근 방식은 이를 ‘다음 토큰 예측(Next-token prediction)’ 문제로 치환하는 것이었다.</p>
<ol>
<li><strong>입력 처리:</strong> 카메라로 촬영된 이미지는 비전 트랜스포머(ViT) 등을 통해 임베딩 벡터로 변환되고, 텍스트 명령어는 토크나이저를 통해 텍스트 토큰으로 변환된다.</li>
<li><strong>멀티모달 융합:</strong> VLM 백본이 시각 토큰과 텍스트 토큰을 결합하여 문맥을 이해한다.</li>
<li><strong>행동 생성 (Action Tokenization):</strong> 기존 모델(예: RT-2, OpenVLA)은 로봇의 연속적인 행동 공간(예: 관절 각도, 그리퍼 위치)을 256개 등 1, 2 등의 이산적인 구간(Bin)으로 나누어 토큰화한다. 그리고 언어 모델이 문장을 생성하듯, 행동 토큰을 자기회귀적(Autoregressive)으로 하나씩 예측한다.6</li>
</ol>
<h3>2.2  기존 대형 모델의 구조적 한계: 자기회귀적 생성의 병목</h3>
<p>OpenVLA나 RT-2가 채택한 ‘이산적 토큰화 및 자기회귀적 생성’ 방식은 로봇 제어 관점에서 치명적인 단점을 갖는다.</p>
<ul>
<li><strong>정보 손실(Quantization Error):</strong> 연속적인 물리량을 이산적인 토큰으로 변환하는 과정에서 필연적으로 정밀도가 손실된다. 이는 정밀 조립과 같이 미세한 조작이 필요한 작업에서 성능 저하의 원인이 된다.7</li>
<li><strong>추론 지연(Latency):</strong> 로봇의 한 번의 행동(Action Step)을 생성하기 위해, 모델은 여러 개의 행동 토큰(예: 7자유도 로봇의 경우 7개 이상의 토큰)을 순차적으로 생성해야 한다. 각 토큰 생성마다 거대 트랜스포머 전체를 통과해야 하므로, 추론 시간이 토큰 수에 비례하여 증가한다. 이는 30Hz 이상의 고주파 제어를 어렵게 만든다.1</li>
<li><strong>복잡한 분포 표현의 한계:</strong> 로봇의 행동은 단일한 정답이 아닌, 여러 가지 가능한 궤적(Multimodal distribution)을 가질 수 있다. 자기회귀적 모델은 이러한 다봉성(Multimodality)을 표현하는 데 있어 확산 모델(Diffusion Model)보다 효율성이 떨어진다.4</li>
</ul>
<h3>2.3  데이터 편향과 일반화의 역설</h3>
<p>OpenVLA는 구글이 주도한 Open X-Embodiment 데이터셋을 통해 강력한 일반화 성능을 얻었다고 주장한다. 그러나 이 데이터셋은 대부분 단일 팔(Single-arm) 로봇, 특히 구글의 자체 로봇이나 Franka Emika와 같은 특정 기종의 데이터로 구성되어 있다.2</p>
<p>이는 <strong>‘부정적 전이(Negative Transfer)’</strong> 의 위험성을 내포한다. 예를 들어, 양팔 로봇(Bimanual Robot)을 제어해야 하는 상황에서, 단일 팔 데이터로만 학습된 모델은 두 팔 간의 협응(Coordination)을 이해하지 못하거나, 불필요한 단일 팔 동작 패턴을 강요할 수 있다. 실제로 연구 결과에 따르면 OpenVLA는 양팔 작업에서 심각한 성능 저하를 보였으며, 이는 대규모 사전 학습 데이터가 오히려 독이 될 수 있음을 시사한다.2</p>
<h2>3.  TinyVLA: 아키텍처 및 핵심 기술 심층 분석</h2>
<p>TinyVLA는 앞서 언급한 한계점들을 극복하기 위해 설계된 새로운 계열의 VLA 모델이다. 그 핵심은 <strong>“가벼운 두뇌(Small VLM)“와 “정교한 손(Diffusion Policy)“의 효율적 결합</strong>에 있다.</p>
<h3>3.1  전체 시스템 아키텍처 (Overall Architecture)</h3>
<p>TinyVLA의 아키텍처는 크게 세 가지 모듈로 구성된다.</p>
<ol>
<li><strong>시각-언어 백본 (VLM Backbone):</strong> 이미지와 텍스트를 입력받아 고차원 특징(Feature)을 추출하고 상황을 이해한다.</li>
<li><strong>프로젝션 모듈 (Projection Module):</strong> VLM의 잠재 표현(Latent Representation)을 정책 디코더가 이해할 수 있는 형태로 변환한다.</li>
<li><strong>확산 정책 디코더 (Diffusion Policy Decoder):</strong> VLM의 출력을 조건(Condition)으로 받아, 실제 로봇의 연속적인 행동 궤적을 생성한다.9</li>
</ol>
<p>이 구조의 가장 큰 특징은 <strong>모듈성(Modularity)</strong> 이다. 언어적 추론과 행동 생성을 단일 네트워크에 섞지 않고 분리함으로써, 각 기능에 최적화된 모델을 사용할 수 있게 되었다.</p>
<h3>3.2  경량화된 VLM 백본: Pythia의 채택</h3>
<p>TinyVLA는 7B(70억) 이상의 파라미터를 가진 Llama 2 등을 사용하는 대신, <strong>Pythia</strong> 모델 시리즈를 백본으로 채택했다.9 TinyVLA에서 사용된 모델의 크기는 70M(7천만)에서 1.4B(14억) 파라미터 수준으로, 기존 OpenVLA 대비 최대 1/70 수준으로 작다.</p>
<ul>
<li><strong>Pythia의 전략적 선택:</strong> Pythia는 학습 데이터와 과정이 투명하게 공개된 모델로, 연구진이 모델의 동작 원리를 정밀하게 제어하기에 적합하다. TinyVLA는 LLaVA(Large Language-and-Vision Assistant)의 학습 파이프라인을 따라 Pythia를 시각-언어 모델로 튜닝하였다. 이 과정에서 시각 인코더(Vision Encoder)와 언어 모델 간의 정렬(Alignment)이 이루어진다.11</li>
<li><strong>경량화의 효과:</strong> 모델 크기의 축소는 단순히 메모리 절약만을 의미하지 않는다. 이는 엣지 디바이스에서의 <strong>실시간 추론</strong>을 가능하게 하는 핵심 요인이다. 작은 모델은 캐시 효율성이 높고 연산량이 적어, 로봇에 내장된 저전력 GPU에서도 높은 FPS(Frame Per Second)를 달성할 수 있다.4</li>
</ul>
<h3>3.3  확산 정책 디코더 (Diffusion Policy Decoder): 행동 생성의 혁신</h3>
<p>TinyVLA의 가장 결정적인 혁신은 행동 생성부에 <strong>확산 모델(Diffusion Model)</strong> 을 도입한 것이다.4</p>
<ul>
<li><strong>작동 원리:</strong> 확산 정책은 가우시안 노이즈(Gaussian Noise)로부터 시작하여, VLM이 추출한 상황 정보(Context)를 조건으로 점진적으로 노이즈를 제거(Denoising)하며 올바른 행동을 생성한다. 이는 이미지 생성 AI인 Stable Diffusion이 텍스트를 받아 이미지를 그려내는 것과 유사하게, 로봇의 상황 설명을 받아 행동 궤적을 ‘그려내는’ 과정이다.</li>
<li><strong>연속 공간 제어의 우위:</strong> 이산적인 토큰 예측 방식과 달리, 확산 정책은 연속적인 실수값(Continuous Values)을 직접 출력한다. 따라서 양자화 오차 없이 부드럽고 정밀한 로봇 제어가 가능하다. 이는 바늘에 실을 꿰거나 컵에 물을 따르는 것과 같은 고정밀 작업에서 결정적인 차이를 만든다.6</li>
<li><strong>다봉성(Multimodality) 처리:</strong> 로봇이 장애물을 피하는 방법은 왼쪽으로 갈 수도, 오른쪽으로 갈 수도 있다. 자기회귀 모델은 이 두 가지를 평균내어 중간의 장애물로 돌진하는 오류를 범할 수 있지만, 확산 모델은 데이터 분포를 정확히 학습하여 명확한 하나의 경로를 선택해 생성할 수 있다.</li>
</ul>
<h3>3.4  학습 전략: 파라미터 효율적 미세 조정 (LoRA)</h3>
<p>TinyVLA는 전체 모델을 학습시키는 비효율성을 피하기 위해 <strong>LoRA (Low-Rank Adaptation)</strong> 기법을 적극 활용한다.2</p>
<ul>
<li><strong>동결(Freezing)과 적응(Adaptation):</strong> 로봇 데이터로 학습할 때, 사전 학습된 VLM 백본(Vision Encoder + LLM)의 가중치는 고정(Freeze)한다. 이는 VLM이 인터넷 데이터로부터 습득한 일반적인 시각적 인지 능력과 상식 지식을 파괴하지 않기 위함이다(Catastrophic Forgetting 방지).</li>
<li><strong>LoRA 적용:</strong> 대신 VLM의 레이어 사이에 학습 가능한 저랭크 행렬(Low-rank Matrices)을 삽입하고, 정책 디코더와 이 행렬들만을 학습시킨다. 연구진에 따르면, 전체 파라미터의 약 <strong>5%</strong> 만이 학습에 참여한다.2</li>
<li><strong>데이터 효율성 증대:</strong> 학습해야 할 파라미터 수가 적다는 것은, 모델이 수렴하기 위해 필요한 데이터의 양도 적다는 것을 의미한다. 이것이 바로 TinyVLA가 대규모 사전 학습 없이도 소량의 데모 데이터만으로 높은 성능을 낼 수 있는 비결이다.5</li>
</ul>
<h2>4.  데이터 효율성 및 사전 학습 없는(Pre-training Free) 접근법</h2>
<p>TinyVLA 연구의 가장 도발적인 주장은 <strong>“로봇 데이터셋을 이용한 대규모 사전 학습 단계가 불필요하다”</strong> 는 것이다.5</p>
<h3>4.1  기존 파이프라인(OpenVLA)의 비효율성</h3>
<p>OpenVLA의 학습 과정은 다음과 같다:</p>
<ol>
<li>일반 VLM(Prismatic 등) 준비.</li>
<li>Open X-Embodiment 데이터셋(970k 에피소드)으로 대규모 사전 학습 -&gt; 일반 VLM을 ’로봇 VLM’으로 변환.</li>
<li>특정 작업(Target Task) 데이터로 미세 조정(Fine-tuning).</li>
</ol>
<p>이 과정은 막대한 컴퓨팅 자원을 소모하며, 2단계에서 학습된 로봇 데이터의 편향이 3단계의 타겟 작업 학습을 방해할 수 있다.</p>
<h3>4.2  TinyVLA의 직관적 파이프라인</h3>
<p>TinyVLA는 2단계를 완전히 생략한다:</p>
<ol>
<li>일반 VLM(Pythia+LLaVA) 준비.</li>
<li><strong>즉시</strong> 타겟 작업 데이터로 LoRA + Diffusion Head 학습.</li>
</ol>
<p>이 접근법은 VLM이 이미 충분한 ’눈(Vision)’과 ’뇌(Language)’를 가지고 있으므로, 여기에 ’손(Action Head)’을 연결하는 훈련만으로 충분하다는 가설에 기반한다. 실험 결과는 이 가설이 옳음을 증명했다. TinyVLA는 사전 학습 없이도 OpenVLA를 능가하거나 대등한 성능을 보였으며, 이는 VLM의 강력한 초기화(Initialization) 효과와 확산 정책의 뛰어난 데이터 피팅(Fitting) 능력 덕분이다.2</p>
<h3>4.3  이종 로봇 및 양팔 작업(Bimanual Tasks)에서의 강점</h3>
<p>사전 학습을 건너뛰는 전략은 새로운 형태의 로봇을 도입할 때 빛을 발한다. 기존 방식대로라면 양팔 로봇을 위한 거대 데이터셋을 새로 구축하여 사전 학습을 해야 하지만, TinyVLA는 양팔 로봇의 데모 데이터만 있으면 즉시 학습이 가능하다.</p>
<p>실제로 Bimanual UR5 Real Robot 실험에서 OpenVLA는 단일 팔 데이터 편향으로 인해 고전했으나, TinyVLA-H는 해당 작업의 데이터 분포를 정확히 학습하여 월등한 성능을 기록했다.2 이는 TinyVLA가 “Tabula Rasa(백지)” 상태에서 시작하는 것이 아니라, “유연한 지능“을 가진 상태에서 새로운 몸(Body)을 배우는 방식이기 때문이다.</p>
<hr />
<h2>5.  성능 평가 및 비교 실험 (Experimental Results)</h2>
<p>TinyVLA의 성능은 시뮬레이션과 실제 로봇 환경 모두에서 광범위하게 검증되었다. 비교 대상은 SOTA 모델인 <strong>OpenVLA (7B)</strong>, <strong>Diffusion Policy</strong>, 그리고 <strong>RT-2</strong> 등이다.</p>
<h3>5.1  실험 설정 (Benchmark Setup)</h3>
<ol>
<li><strong>시뮬레이션 (Simulation):</strong></li>
</ol>
<ul>
<li><strong>MetaWorld:</strong> 50개의 다양한 로봇 조작 과제를 포함하는 벤치마크. 일반화 성능을 측정하기에 적합하다.12</li>
<li><strong>LIBERO:</strong> 공간적 추론, 물체 조작, 목표 조건부 작업, 장기 계획(Long-horizon) 등을 평가하는 포괄적 벤치마크 수트.6</li>
</ul>
<ol start="2">
<li><strong>실제 로봇 (Real World):</strong></li>
</ol>
<ul>
<li><strong>Franka Emika Panda (Single-arm):</strong> 5가지 주요 조작 과제 (예: 물건 집기, 서랍 열기 등).</li>
<li><strong>UR5 (Bimanual):</strong> 두 팔을 동시에 사용하여 물체를 옮기거나 조립하는 고난도 과제.2</li>
</ul>
<h3>5.2  작업 성공률 (Success Rate) 비교 분석</h3>
<p>다음 표는 주요 실험 결과를 요약한 것이다.</p>
<p><strong>[표 1] TinyVLA와 SOTA 모델 간의 성능 비교 (성공률, %)</strong></p>
<table><thead><tr><th><strong>모델 (Model)</strong></th><th><strong>사전 학습 데이터 (Pre-training Data)</strong></th><th><strong>학습 파라미터 (Trainable Params)</strong></th><th><strong>MetaWorld (Avg Success)</strong></th><th><strong>Real World (Single-arm)</strong></th><th><strong>Real World (Bimanual)</strong></th></tr></thead><tbody>
<tr><td><strong>OpenVLA (7B)</strong></td><td>Open X (970K)</td><td>7B (Full/LoRA)</td><td>60~70% 대 (Est.)</td><td>81.7 ± 0.6</td><td><strong>실패 (Low)</strong></td></tr>
<tr><td><strong>Diffusion Policy</strong></td><td>N/A</td><td>Small</td><td>-</td><td>10.0 ± 0</td><td>-</td></tr>
<tr><td><strong>TinyVLA-S</strong></td><td><strong>None</strong></td><td><strong>111M</strong></td><td>-</td><td>35.0 ± 0.3</td><td>35.0 ± 0.3</td></tr>
<tr><td><strong>TinyVLA-B</strong></td><td><strong>None</strong></td><td><strong>195M</strong></td><td>-</td><td>80.0 ± 0.2</td><td>80.0 ± 0.2</td></tr>
<tr><td><strong>TinyVLA-H</strong></td><td><strong>None</strong></td><td><strong>~1.3B (Total)</strong></td><td><strong>High (+21.5% vs DP)</strong></td><td><strong>94.0 ± 0.0</strong></td><td><strong>High (Superior)</strong></td></tr>
</tbody></table>
<p>데이터 출처: 2 재구성</p>
<ul>
<li><strong>압도적인 성능 향상:</strong> TinyVLA-H(High capacity)는 실제 로봇의 단일 팔 작업에서 **94.0%**의 성공률을 기록하여, 81.7%에 그친 OpenVLA를 12.3% 포인트(상대적 성능 격차로는 25.7%라고 언급됨 12) 차이로 제쳤다. 이는 데이터 효율적인 학습이 거대 데이터 사전 학습보다 특정 도메인에서는 더 강력할 수 있음을 증명한다.</li>
<li><strong>모델 크기의 영향:</strong> TinyVLA-S(Small)는 35%의 성공률로 저조했으나, 모델 크기가 커질수록(S -&gt; B -&gt; H) 성능이 급격히 향상되는 양상을 보였다.13 이는 VLM 백본의 인지 능력이 행동 생성의 품질에 결정적인 영향을 미침을 시사한다.</li>
<li><strong>Diffusion Policy의 한계 극복:</strong> 순수 Diffusion Policy(VLM 없음)는 시각적 이해 부족으로 인해 10%대의 낮은 성공률을 보였으나, TinyVLA는 VLM을 결합함으로써 이를 극복했다.2</li>
</ul>
<h3>5.3  추론 속도 (Inference Speed) 및 자원 효율성</h3>
<p>TinyVLA의 가장 큰 실용적 가치는 속도다.</p>
<ul>
<li><strong>FPS (Frames Per Second):</strong> OpenVLA와 같은 7B 모델은 일반적으로 고성능 GPU에서도 5~6Hz 수준의 제어 주기를 갖는다.14 반면, TinyVLA는 파라미터 수가 훨씬 적고(1.3B 이하), 확산 디코더가 효율적으로 설계되어 있어 이보다 훨씬 빠른 추론 속도를 제공한다. 일부 관련 연구에서는 소형 VLA가 10~15Hz 이상의 고속 제어가 가능함을 보고하고 있다.15</li>
<li><strong>메모리 사용량 (VRAM):</strong> Pythia-1.3B 기반의 TinyVLA는 7B 모델 대비 메모리 사용량이 현저히 적다. 이는 24GB VRAM을 가진 RTX 3090/4090급 소비자용 GPU나, 젯슨(Jetson) AGX Orin과 같은 엣지 디바이스에도 모델을 탑재(On-device)할 수 있음을 의미한다.</li>
</ul>
<h3>5.4  일반화 능력 (Generalization Capability)</h3>
<p>TinyVLA는 학습 데이터에 없는 상황에서도 강건함을 유지했다.2</p>
<ol>
<li><strong>시각적 변화:</strong> 물체의 색상이나 배경이 바뀌어도 VLM의 강인한 특징 추출 능력 덕분에 행동이 무너지지 않았다.</li>
<li><strong>공간적 변화:</strong> 물체의 위치나 카메라의 각도가 변경되어도(Viewpoint shift), 3D 공간 이해 능력을 바탕으로 적응했다.</li>
<li><strong>명령어 변형:</strong> “집어라(Pick)” 대신 “들어올려라(Lift)“와 같은 동의어를 사용하거나 문장 구조를 바꿔도 명령을 정확히 수행했다.</li>
</ol>
<h2>6.  심층 논의: TinyVLA가 시사하는 2차, 3차 통찰</h2>
<h3>6.1  로봇 AI 개발의 “민주화(Democratization)”</h3>
<p>지금까지 고성능 VLA 연구는 구글, NVIDIA와 같이 수백 개의 GPU를 동원할 수 있는 빅테크 기업의 전유물이었다. 그러나 TinyVLA는 <strong>“소형 백본 + LoRA + 데이터 효율적 학습”</strong> 이라는 레시피를 통해, 학계 연구실이나 중소기업도 최첨단 로봇 AI를 개발할 수 있는 길을 열었다. 이는 로봇 기술의 다양성을 폭발적으로 증가시키는 촉매제가 될 것이다.</p>
<h3>6.2  언어와 제어의 “디커플링(Decoupling)”</h3>
<p>기존 VLA는 언어 모델에게 행동 제어까지 강요하는 형태였다. 이는 언어 모델의 “다음 단어 예측” 능력을 과도하게 확장 해석한 것이다. TinyVLA의 성공은 <strong>“언어는 의도(Intent)를, 확산 모델은 행동(Action)을”</strong> 담당하는 역할 분담이 훨씬 효율적임을 시사한다. 언어 모델은 고차원적인 계획과 추론에 집중하고, 저수준의 모터 제어는 전용 모듈에 위임하는 이 <strong>하이브리드 구조</strong>는 향후 로봇 뇌 구조의 표준이 될 가능성이 높다.</p>
<h3>6.3  “Scaling Law“의 재해석</h3>
<p>AI 분야의 지배적인 법칙인 “스케일링 법칙(Scaling Law)“은 모델과 데이터가 클수록 좋다고 말한다. 그러나 로봇 공학에서만큼은 데이터의 <strong>질(Quality)</strong> 과 <strong>밀도(Density)</strong> 가 양(Quantity)보다 중요할 수 있다. TinyVLA는 무차별적인 데이터 수집보다, 타겟 도메인에 맞는 양질의 소량 데이터가 훨씬 강력한 성능을 낼 수 있음을 보여주며, 로봇 데이터 수집 전략의 전면적인 수정을 요구하고 있다.</p>
<h2>7.  결론</h2>
<p>TinyVLA는 단순히 “작은 모델“이 아니다. 이는 로봇 제어 시스템의 비효율성을 타개하기 위한 정교한 공학적 해법의 집약체다.</p>
<p>본 보고서의 분석을 통해 도출된 결론은 다음과 같다.</p>
<ol>
<li><strong>TinyVLA는 속도와 성능의 트레이드오프를 깼다.</strong> 경량 VLM과 확산 정책의 결합을 통해, OpenVLA보다 빠르면서도 더 정확한 제어 성능을 달성했다.</li>
<li><strong>사전 학습 없는 학습이 가능하다.</strong> 로봇 특화 사전 학습 없이도 일반 VLM의 지식을 효율적으로 전이시켜, 새로운 로봇 도입 비용을 획기적으로 낮췄다.</li>
<li><strong>엣지 컴퓨팅 시대의 핵심 기술이다.</strong> 낮은 하드웨어 요구 사항은 로봇이 클라우드 연결 없이도 독자적인 지능을 가질 수 있게 하여, 보안과 반응 속도 문제를 동시에 해결한다.</li>
</ol>
<p>결국 TinyVLA는 로봇이 실험실을 벗어나 우리의 일상으로 들어오기 위해 필요한 <strong>“가볍지만 강력한 뇌”</strong> 의 청사진을 제시하고 있다. 향후 이 아키텍처는 휴머노이드 로봇의 전신 제어, 드론의 자율 비행, 스마트 팩토리의 유연 생산 시스템 등 다양한 분야로 확장되어 임바디드 AI의 상용화를 앞당길 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey, https://arxiv.org/html/2510.17111v1</li>
<li>TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation - arXiv, https://arxiv.org/html/2409.12514v1</li>
<li>Embodiment Transfer Learning for Vision-Language-Action Models - arXiv, https://arxiv.org/html/2511.01224v1</li>
<li>TinyVLA in 5 Minutes - Medium, https://medium.com/correll-lab/tinyvla-in-5-minutes-e9815931b463</li>
<li>TinyVLA: Toward Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation - Semantic Scholar, https://www.semanticscholar.org/paper/TinyVLA%3A-Toward-Fast%2C-Data-Efficient-Models-for-Wen-Zhu/dc62bc6536e9e3ad80242f10f44c046e4c7bd3d1</li>
<li>vovw/tinyvla - GitHub, https://github.com/vovw/tinyvla</li>
<li>Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey, https://arxiv.org/html/2510.17111v3</li>
<li>CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame Vision-Language-Action Modeling - arXiv, https://arxiv.org/html/2506.19816v2</li>
<li>TinyVLA: Toward Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation | Request PDF - ResearchGate, https://www.researchgate.net/publication/389282468_TinyVLA_Towards_Fast_Data-Efficient_Vision-Language-Action_Models_for_Robotic_Manipulation</li>
<li>[2409.12514] TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation - arXiv, https://arxiv.org/abs/2409.12514</li>
<li>TinyVLA: Toward Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation - IEEE Xplore, https://ieeexplore.ieee.org/iel8/7083369/10893718/10900471.pdf</li>
<li>TinyVLA: Fast, Data-Efficient VLA for Robotics - Emergent Mind, https://www.emergentmind.com/papers/2409.12514</li>
<li>TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation - arXiv, https://arxiv.org/html/2409.12514v5</li>
<li>MiniVLA: A Better VLA with a Smaller Footprint | SAIL Blog - Stanford AI Lab, https://ai.stanford.edu/blog/minivla/</li>
<li>HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model - arXiv, https://arxiv.org/html/2503.10631v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>