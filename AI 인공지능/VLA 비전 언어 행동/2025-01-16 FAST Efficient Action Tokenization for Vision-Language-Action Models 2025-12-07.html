<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:FAST - VLA 모델을 위한 효율적 행동 토큰화</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>FAST - VLA 모델을 위한 효율적 행동 토큰화</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">Vision-Language-Action(VLA) 모델</a> / <span>FAST - VLA 모델을 위한 효율적 행동 토큰화</span></nav>
                </div>
            </header>
            <article>
                <h1>FAST - VLA 모델을 위한 효율적 행동 토큰화</h1>
<p>2025-12-07, G30DR</p>
<h2>1.  서론: 로봇 제어의 새로운 패러다임과 데이터 표현의 병목</h2>
<p>인공지능 연구의 최전선은 이제 텍스트와 이미지를 넘어 물리적 세계와 상호작용하는 로보틱스(Robotics) 분야로 급격히 확장되고 있다. 대규모 언어 모델(LLM)이 인터넷상의 방대한 텍스트 데이터를 통해 언어적 추론 능력을 획득했듯이, 로봇 연구자들은 로봇 또한 대규모 데이터 학습을 통해 일반화된 물리적 지능(General Physical Intelligence)을 가질 수 있을 것이라 기대한다. 이러한 흐름의 중심에 있는 것이 바로 비전-언어-행동(Vision-Language-Action, VLA) 모델이다.1 VLA 모델은 시각 정보와 자연어 명령을 입력받아 로봇의 구체적인 행동(Action)을 생성하는 종단간(End-to-End) 학습 모델로, 기존의 모듈식 파이프라인(인식-계획-제어)을 대체하며 로봇 제어의 새로운 표준으로 부상하고 있다.</p>
<p>그러나 언어 모델의 성공 방정식을 로봇에 그대로 적용하는 데에는 근본적인 장벽이 존재한다. 바로 ’데이터의 이질성’이다. 텍스트는 본질적으로 이산적(Discrete)인 상징(Symbol)들의 나열인 반면, 로봇의 행동은 연속적(Continuous)이고 고차원적이며 시간의 흐름에 따라 매끄럽게 변화하는 물리량이다. 트랜스포머(Transformer) 아키텍처는 이산적인 토큰(Token) 시퀀스를 처리하는 데 최적화되어 있기에, 연속적인 로봇 행동을 트랜스포머가 이해하고 생성할 수 있는 형태인 ’행동 토큰(Action Token)’으로 변환하는 과정, 즉 행동 토큰화(Action Tokenization)가 필수적이다.3</p>
<p>기존의 초기 VLA 모델들은 이 문제를 해결하기 위해 각 행동 차원의 값을 단순하게 구간화(Binning)하여 정수형 토큰으로 매핑하는 ‘나이브 토큰화(Naïve Tokenization)’ 방식을 사용했다. 예를 들어, 로봇 팔의 관절 각도를 256개의 구간으로 나누어 표현하는 것이다.4 하지만 이 방식은 로봇의 움직임이 빠르고 정교해질수록, 즉 고주파수(High-frequency) 제어가 필요할수록 심각한 한계를 드러낸다. 인접한 시간 단계(Time step) 간의 행동 값이 거의 유사하여 토큰 간의 상관관계가 극도로 높아지고, 모델은 다음 행동을 예측하는 대신 이전 행동을 단순히 복사하는 경향을 보이게 된다. 이는 결과적으로 로봇의 움직임을 경직되게 만들거나, 정교한 조작(Dexterous manipulation)을 불가능하게 만드는 원인이 된다.3</p>
<p>이러한 배경 속에서 Physical Intelligence, UC Berkeley, Stanford 연구진이 제안한 <strong>FAST (Framework for Action Tokenization)</strong> 는 신호 처리 이론과 현대적인 토큰화 기법을 결합하여 이 문제를 정면으로 돌파한다.3 FAST는 이산 코사인 변환(Discrete Cosine Transform, DCT)을 통해 행동 데이터를 주파수 도메인으로 변환하고, 이를 압축하여 표현함으로써 데이터의 시간적 상관성을 제거하고 효율성을 극대화한다. 본 보고서는 FAST의 기술적 원리와 실험적 성과, 그리고 이것이 로봇 파운데이션 모델의 훈련 및 추론 효율성에 미치는 영향을 포괄적으로 분석한다. 특히, 100만 개 이상의 실제 로봇 궤적으로 훈련된 유니버설 토크나이저인 FAST+의 등장과 이것이 시사하는 ‘행동의 언어화’ 가능성을 심도 있게 논의하며, 확산(Diffusion) 기반 정책 모델과의 비교를 통해 FAST가 제시하는 트레이드오프와 미래 발전 방향을 모색한다.</p>
<hr />
<h2>2.  로봇 파운데이션 모델의 진화와 행동 표현의 난제</h2>
<p>FAST의 등장이 갖는 의미를 정확히 파악하기 위해서는, 로봇 학습 모델이 어떻게 발전해 왔으며 그 과정에서 ’행동 표현(Action Representation)’이 왜 핵심적인 병목으로 작용했는지를 이해해야 한다.</p>
<h3>2.1  모방 학습에서 VLA 모델로의 전환</h3>
<p>과거의 로봇 제어는 명시적인 수학적 모델링에 기반한 제어 이론(Control Theory)이나, 상태(State)와 보상(Reward)을 정의해야 하는 강화 학습(Reinforcement Learning)이 주류를 이루었다. 그러나 이러한 방식은 비정형 환경에서의 일반화 능력이 부족하거나, 보상 함수 설계의 어려움이라는 한계를 안고 있었다. 딥러닝의 발전과 함께 등장한 모방 학습(Imitation Learning), 특히 행동 복제(Behavior Cloning, BC)는 전문가의 시연(Demonstration) 데이터를 지도 학습(Supervised Learning) 방식으로 학습하여 이러한 문제를 해결하고자 했다.</p>
<p>VLA 모델은 이 행동 복제 기법을 거대 언어 모델(LLM)의 아키텍처와 결합한 형태이다. RT-1(Robotics Transformer 1)과 같은 초기 모델은 이미지를 토큰화하고, 자연어 명령을 토큰화한 뒤, 로봇의 행동 또한 텍스트 토큰처럼 취급하여 하나의 긴 시퀀스로 처리했다.6 이 접근법은 “다음 토큰 예측(Next-token prediction)“이라는 단순하고 강력한 학습 목표(Objective)를 통해 로봇이 다양한 작업을 수행하도록 가르칠 수 있다는 가능성을 보여주었다.</p>
<h3>2.2  이산화(Discretization)의 딜레마와 확산 모델의 부상</h3>
<p>하지만 트랜스포머 기반의 VLA 모델은 행동 데이터를 어떻게 이산화할 것인가 하는 문제에 직면했다.</p>
<ul>
<li><strong>저주파수/저차원 제어의 경우:</strong> RT-1과 같은 모델은 이동 로봇이나 단순한 집기(Pick-and-place) 작업에 집중했다. 이러한 작업은 제어 주파수가 낮고(약 3~5Hz), 정밀도가 상대적으로 덜 요구되므로, 각 차원을 256개 정도의 구간(Bin)으로 나누는 단순 비닝(Binning) 방식으로도 충분한 성능을 낼 수 있었다.4</li>
<li><strong>고주파수/고차원 제어의 경우:</strong> 그러나 인간과 유사한 양팔 로봇이나 정교한 손놀림이 필요한 작업(예: 옷 개기, 요리하기)은 50Hz 이상의 고주파수 제어와 미세한 조작을 요구한다. 이 경우 단순 비닝 방식은 두 가지 치명적인 문제를 야기한다.</li>
</ul>
<ol>
<li><strong>시퀀스 길이 폭발:</strong> 1초에 50번의 행동을 생성해야 하고, 로봇의 자유도(DoF)가 14라면, 1초 분량의 행동만으로도 <span class="math math-inline">50 \times 14 = 700</span>개의 토큰이 필요하다. 이는 트랜스포머의 문맥 길이(Context Length) 제한을 빠르게 소진시킨다.</li>
<li><strong>상관관계에 의한 학습 실패:</strong> 50Hz로 샘플링된 데이터에서 <span class="math math-inline">t</span> 시점과 <span class="math math-inline">t+1</span> 시점의 관절 각도 차이는 매우 미미하다. 모델은 이 미세한 차이를 학습하기보다, 단순히 <span class="math math-inline">a_t</span> 토큰을 <span class="math math-inline">a_{t+1}</span>로 복사하는 것이 손실 함수를 줄이는 가장 쉬운 방법임을 깨닫게 된다. 이는 로봇이 목표 지점으로 이동하지 않고 제자리에 멈추거나 떨리는 현상(Dithering)을 유발한다.3</li>
</ol>
<p>이러한 이산화의 한계를 극복하기 위해 연구자들은 연속적인 값을 직접 다루는 <strong>확산 모델(Diffusion Models)</strong> 로 눈을 돌렸다. Physical Intelligence의 <span class="math math-inline">\pi_0</span> 모델이나, 다른 최신 연구들은 디퓨전 프로세스나 플로우 매칭(Flow Matching)을 사용하여 행동 분포를 연속적인 공간에서 직접 모델링했다.1 확산 모델은 다봉성(Multimodality)을 잘 표현하고 고주파수 제어에서도 부드러운 동작을 생성할 수 있어 사실상의 표준(De facto standard)으로 자리 잡는 듯했다. 그러나 확산 모델은 추론과 훈련에 막대한 계산 비용이 들며, 트랜스포머의 자기회귀적 특성을 온전히 활용하지 못한다는 단점이 있었다.</p>
<h3>2.3  FAST의 제안: 자기회귀 모델의 귀환</h3>
<p>FAST는 이러한 상황에서 다시 트랜스포머의 자기회귀 모델링으로 회귀하되, ’스마트한 토큰화’를 통해 이산화의 문제를 해결하자는 제안이다. 확산 모델이 ‘행동 표현의 방식’ 자체를 바꾸어 문제를 우회했다면, FAST는 ’데이터 압축과 변환’을 통해 트랜스포머가 소화할 수 있는 형태로 데이터를 가공하는 정공법을 택한 것이다. 이는 훈련 효율성을 획기적으로 높이면서도 확산 모델 수준의 성능을 달성할 수 있는 돌파구가 되었다.</p>
<hr />
<h2>3.  FAST 방법론: DCT와 BPE를 결합한 혁신적 토큰화</h2>
<p>FAST(Framework for Action Tokenization)의 핵심 아이디어는 시간 영역(Time domain)의 행동 궤적을 주파수 영역(Frequency domain)으로 변환하여 정보를 압축하는 것이다. 이는 JPEG 이미지 압축이나 MP3 오디오 압축과 동일한 신호 처리 원리를 로봇 행동 데이터에 적용한 것이다.</p>
<h3>3.1  이산 코사인 변환 (DCT)을 통한 스펙트럼 분해</h3>
<p>로봇의 행동 데이터는 물리 법칙, 특히 관성(Inertia)의 지배를 받는다. 로봇 팔은 순간적으로 텔레포트할 수 없으며, 관절 각도는 연속적으로 부드럽게 변화한다. 신호 처리 관점에서 보면, 이는 로봇 행동 데이터의 에너지가 대부분 ‘저주파(Low-frequency)’ 대역에 집중되어 있음을 의미한다. 급격한 변화를 나타내는 고주파 성분은 노이즈이거나 정보량이 매우 적다.</p>
<p>FAST는 이러한 특성을 활용하기 위해 <strong>이산 코사인 변환(DCT)</strong> 을 사용한다.4</p>
<ol>
<li><strong>입력 데이터:</strong> <span class="math math-inline">T</span> 길이의 시간 윈도우(Chunk)와 <span class="math math-inline">D</span> 차원의 행동 공간을 가진 행동 시퀀스 <span class="math math-inline">A \in \mathbb{R}^{T \times D}</span>.</li>
<li><strong>정규화:</strong> 각 차원의 값을 데이터셋 통계(1분위수, 99분위수)를 기반으로 <span class="math math-inline">[-1, 1]</span> 범위로 정규화한다. 이는 모든 차원이 동등한 중요도를 갖도록 보장한다.</li>
<li><strong>DCT 적용:</strong> 시간 축을 따라 1차원 DCT를 적용한다. 이를 통해 시간 흐름에 따른 위치 값들이 주파수 성분을 나타내는 계수(Coefficient)들로 변환된다.</li>
</ol>
<ul>
<li>DCT는 이산 푸리에 변환(DFT)과 달리 실수(Real number) 만을 출력하며, 신호의 에너지를 저주파 계수로 집중시키는 ‘에너지 압축(Energy Compaction)’ 능력이 뛰어나다.</li>
</ul>
<ol start="4">
<li><strong>스펙트럼 분석:</strong> 변환된 계수 행렬에서 앞쪽의 계수들은 전체적인 움직임의 경향(Trend)을, 뒤쪽의 계수들은 미세한 떨림이나 급격한 변화를 나타낸다. 로봇 데이터의 특성상 뒤쪽 계수들의 값은 0에 가깝다.</li>
</ol>
<h3>3.2  손실 압축과 양자화 (Truncation &amp; Quantization)</h3>
<p>DCT 변환 자체는 손실이 없는(Lossless) 변환이다. 그러나 토큰화를 위해서는 정보의 손실을 감수하고 데이터를 압축해야 한다.</p>
<ul>
<li><strong>고주파 제거(Truncation):</strong> 정보량이 적은 고주파 대역의 계수들을 과감히 제거한다. 이는 데이터의 차원을 줄이는 효과가 있다.</li>
<li><strong>양자화(Quantization):</strong> 실수형 계수들을 정수형 토큰으로 변환하기 위해 반올림을 수행한다. 이때 스케일링 계수(Scaling factor, 논문에서는 10 사용)를 곱하여 정밀도를 조절한다. 예를 들어, 0.123이라는 계수에 10을 곱해 1.23을 만들고, 이를 반올림하여 1이라는 정수 토큰을 얻는 식이다.3</li>
</ul>
<p>이 과정은 데이터 간의 시간적 상관관계를 해제(Decorrelation)하는 결정적인 역할을 한다. 시간 영역에서는 <span class="math math-inline">a_t</span>와 <span class="math math-inline">a_{t+1}</span>이 비슷했지만, 주파수 영역의 계수들은 서로 독립적인 정보를 담고 있다. 따라서 트랜스포머 모델은 ’이전 토큰 복사’라는 꼼수를 부릴 수 없게 되며, 각 계수가 의미하는 행동의 패턴을 학습해야만 한다.</p>
<h3>3.3  수정된 BPE (Byte Pair Encoding) 적용</h3>
<p>DCT와 양자화를 거친 데이터는 정수 시퀀스가 되지만, 여전히 최적화의 여지가 있다. 행동 데이터에는 자주 등장하는 패턴(예: ‘천천히 앞으로 이동’, ‘정지’)이 존재하며, 이는 양자화된 DCT 계수들의 특정 조합으로 나타난다.</p>
<p>FAST는 자연어 처리(NLP)에서 서브워드(Subword) 토큰화에 사용되는 BPE(Byte Pair Encoding) 알고리즘을 로봇 행동 데이터에 적용한다.3</p>
<ul>
<li><strong>원리:</strong> 데이터 내에서 가장 빈번하게 등장하는 인접한 토큰 쌍(Pair)을 찾아 새로운 하나의 토큰으로 병합한다. 이 과정을 반복하여 원하는 어휘 크기(Vocabulary Size)에 도달할 때까지 압축한다.</li>
<li><strong>효과:</strong> DCT 계수 행렬은 고주파 성분이 제거되거나 0으로 양자화되는 경우가 많아 희소성(Sparsity)이 높다. BPE는 이러한 0의 연속이나 반복되는 패턴을 매우 효율적으로 압축한다.</li>
<li><strong>결과:</strong> 나이브 토큰화 방식이 700개의 토큰을 필요로 했던 작업을, FAST는 BPE를 통해 약 53개의 토큰으로 줄여 <strong>13배 이상의 압축률</strong>을 달성했다.8 이는 트랜스포머의 연산 부담을 획기적으로 줄여준다.</li>
</ul>
<h3>3.4  FAST+: 1M 궤적 기반의 유니버설 토크나이저</h3>
<p>FAST 연구진은 방법론 제시에 그치지 않고, 이를 범용적으로 사용할 수 있는 <strong>FAST+</strong> 토크나이저를 공개했다.3</p>
<ul>
<li><strong>데이터 규모:</strong> 100만 개(1M) 이상의 실제 로봇 행동 궤적으로 훈련되었다. 이는 로봇 학습 분야에서 전례 없는 규모이다.</li>
<li><strong>다양성(Diversity):</strong> 단일 암(Single-arm), 양팔 로봇(Bi-manual), 모바일 매니퓰레이터, 휴머노이드 등 다양한 로봇 형태(Morphology)와 제어 주파수 데이터를 포함한다.</li>
<li><strong>블랙박스 활용:</strong> 연구자들은 자신의 로봇 데이터에 맞춰 별도의 토크나이저를 학습할 필요 없이, FAST+를 사전 학습된 BERT 토크나이저처럼 가져다 쓸 수 있다. 이는 로봇 데이터의 이질성을 극복하고 ’표준화된 행동 언어’를 구축하려는 시도로 해석된다.</li>
</ul>
<table><thead><tr><th><strong>특징</strong></th><th><strong>기존 방식 (Simple Binning)</strong></th><th><strong>FAST / FAST+</strong></th></tr></thead><tbody>
<tr><td><strong>기반 원리</strong></td><td>시간 영역 균등 분할</td><td>주파수 영역 (DCT) 변환 및 압축</td></tr>
<tr><td><strong>토큰 간 상관성</strong></td><td>매우 높음 (학습 저해)</td><td>낮음 (Decorrelated, 학습 용이)</td></tr>
<tr><td><strong>시퀀스 길이</strong></td><td>매우 김 (1초당 ~700 토큰)</td><td>매우 짧음 (1초당 ~50 토큰, 13x 압축)</td></tr>
<tr><td><strong>적용 대상</strong></td><td>저주파수 단순 작업</td><td>고주파수 정교 작업 및 모든 로봇</td></tr>
<tr><td><strong>일반화 가능성</strong></td><td>데이터셋별 재설계 필요</td><td>사전 학습된 범용 토크나이저 사용 가능</td></tr>
</tbody></table>
<hr />
<h2>4.  모델 아키텍처 및 훈련: <span class="math math-inline">\pi_0</span>-FAST</h2>
<p>FAST 토크나이저는 VLA 모델의 입출력 인터페이스 역할을 한다. 이를 실제 모델에 통합한 것이 바로 <strong><span class="math math-inline">\pi_0</span>-FAST</strong> 이다. 이 모델은 Physical Intelligence의 베이스 모델인 <span class="math math-inline">\pi_0</span> 아키텍처를 기반으로 하되, 행동 생성 헤드(Head)를 확산 방식에서 자기회귀 방식으로 교체한 것이다.</p>
<h3>4.1  아키텍처 및 입출력 흐름</h3>
<p><span class="math math-inline">\pi_0</span>-FAST는 전형적인 디코더 온리(Decoder-only) 트랜스포머 구조를 따른다 (또는 인코더-디코더 구조일 수 있으나, 최신 트렌드 및 GPT 계열의 활용을 고려하면 디코더 중심일 가능성이 높음. 문맥상 VLM 베이스를 사용하므로 Paligemma나 유사 모델 변형일 수 있음).</p>
<ol>
<li><strong>입력 처리:</strong></li>
</ol>
<ul>
<li><strong>이미지:</strong> 로봇의 카메라로 들어온 이미지는 비전 인코더(예: SigLIP 등)를 거쳐 패치 단위의 토큰으로 변환된다.</li>
<li><strong>텍스트:</strong> 사용자의 자연어 명령(“파란색 블록을 집어서 빨간 그릇에 담아줘”)은 텍스트 토크나이저를 통해 토큰화된다.</li>
<li>이 두 가지 멀티모달 입력이 모델의 컨텍스트(Context)로 주입된다.</li>
</ul>
<ol start="2">
<li><strong>행동 생성 (Action Generation):</strong></li>
</ol>
<ul>
<li>모델은 주어진 이미지와 텍스트 컨텍스트를 바탕으로 FAST 토큰 시퀀스를 자기회귀적으로(Autoregressively) 생성한다.</li>
<li>즉, <span class="math math-inline">P(a_t | I, L, a_{&lt;t})</span> 확률 분포를 학습하여 다음 토큰을 예측한다.</li>
<li>생성된 FAST 토큰 시퀀스는 역(Inverse) 토크나이저를 통해 다시 실수형의 연속적인 행동 궤적(Action Chunk)으로 복원되어 로봇 제어기에 전달된다.</li>
</ul>
<h3>4.2  훈련 효율성의 혁신: Scaling Law의 실현</h3>
<p>FAST의 가장 강력한 장점은 훈련 효율성이다. 확산 모델은 노이즈를 제거하는 복잡한 과정을 학습해야 하므로 수렴에 오랜 시간이 걸리고, 손실 함수의 변동성이 크다. 반면, <span class="math math-inline">\pi_0</span>-FAST는 정답 토큰을 맞추는 Cross-Entropy Loss를 사용하므로 학습이 매우 안정적이고 빠르다.</p>
<ul>
<li><strong>5배 빠른 학습:</strong> 실험 결과, <span class="math math-inline">\pi_0</span>-FAST는 확산 기반의 <span class="math math-inline">\pi_0</span> 모델과 동일한 성능에 도달하는 데 걸리는 GPU 시간이 1/5 수준에 불과했다.3</li>
<li><strong>대규모 데이터 확장:</strong> 이러한 효율성은 모델을 더 많은 데이터로 학습시킬 수 있게 한다. 연구진은 10,000시간 분량의 로봇 데이터를 사용하여 모델을 훈련시켰으며, 이는 기존 연구들이 수백 시간 단위의 데이터에 머물렀던 것과 대조적이다.</li>
<li><strong>컴퓨팅 자원 절감:</strong> 훈련 비용의 감소는 더 큰 모델(Parameter size)을 시도하거나, 더 다양한 데이터를 포함시키는 실험을 가능하게 하여 로봇 분야에서도 ’Scaling Law(규모의 법칙)’가 작동할 수 있는 기반을 마련해준다.</li>
</ul>
<hr />
<h2>5.  실험 결과 및 성능 비교 분석</h2>
<p>FAST의 성능은 단순한 수치적 우위를 넘어, 기존 방식으로는 불가능했던 작업들을 가능하게 했다는 점에서 질적인 도약을 보여준다. 연구진은 LIBERO, DROID 등 표준 벤치마크와 실제 로봇 하드웨어를 이용한 고난도 작업에서 광범위한 평가를 수행했다.</p>
<h3>5.1  고주파수 및 정교한 조작 (Dexterous Manipulation)</h3>
<p>나이브 토큰화 방식이 가장 취약했던 영역인 고주파수 제어 작업에서 FAST의 진가가 드러났다.</p>
<ul>
<li><strong>티셔츠 접기 (T-shirt Folding):</strong> 옷과 같은 유연 물체(Deformable object)를 다루는 작업은 물체의 상태가 예측하기 어렵고, 로봇의 양팔이 정교하게 협응해야 한다. 또한 동작의 주파수가 높아 나이브 토큰화 사용 시 토큰 간 상관관계 문제가 극심하게 발생한다. 실험 결과, 나이브 토큰화 모델은 옷을 제대로 잡지 못하거나 동작이 뚝뚝 끊기는 모습을 보였으나, <span class="math math-inline">\pi_0</span>-FAST는 확산 모델인 <span class="math math-inline">\pi_0</span>와 유사한 수준의 부드럽고 정확한 동작을 생성하여 성공적으로 옷을 접었다.8</li>
<li><strong>테이블 치우기 (Table Bussing):</strong> 여러 물체를 순차적으로 정리하는 긴 호라이즌(Long-horizon) 작업에서도 FAST는 압축된 토큰 표현 덕분에 문맥을 잃지 않고 작업을 완수했다. 나이브 방식은 시퀀스 길이가 너무 길어져 모델이 작업의 순서를 “잊어버리는” 경향을 보였다.</li>
</ul>
<h3>5.2  확산 모델(<span class="math math-inline">\pi_0</span>)과의 정면 승부</h3>
<p>현재 로봇 학습의 SOTA(State-of-the-art)인 확산 모델과의 비교는 FAST의 실용성을 가늠하는 중요한 척도이다.</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>확산 모델 (π0)</strong></th><th><strong>π0-FAST</strong></th><th><strong>분석 및 함의</strong></th></tr></thead><tbody>
<tr><td><strong>성공률 (Success Rate)</strong></td><td>매우 높음</td><td><strong>매우 높음 (동등)</strong></td><td>FAST가 이산화 모델의 성능 한계를 극복했음을 증명. 5개 주요 작업에서 확산 모델과 통계적으로 유의미한 차이 없음.8</td></tr>
<tr><td><strong>훈련 속도 (Training Speed)</strong></td><td>느림 (기준)</td><td><strong>5배 빠름</strong></td><td>FAST의 압도적 우위. 모델 개발 사이클을 단축하고 대규모 학습을 가능케 함.</td></tr>
<tr><td><strong>추론 방식 (Inference)</strong></td><td>병렬/Iterative (Action Chunk 전체 생성)</td><td>순차적 (Token-by-token)</td><td>확산 모델은 한 번에 청크를 생성하지만 반복 연산 필요. FAST는 한 번의 연산은 가볍지만 토큰 수만큼 반복해야 함.</td></tr>
<tr><td><strong>데이터 효율성</strong></td><td>보통</td><td><strong>높음</strong></td><td>더 적은 데이터로도 복잡한 분포를 학습하는 경향을 보임 (압축된 표현 덕분).</td></tr>
</tbody></table>
<h3>5.3  제로샷 일반화 (Zero-shot Generalization)와 DROID 데이터셋</h3>
<p>FAST의 범용성을 입증하는 가장 인상적인 결과는 DROID 데이터셋을 활용한 실험에서 나왔다. DROID는 다양한 환경에서 수집된 대규모 로봇 데이터셋이다.</p>
<ul>
<li><strong>완전한 제로샷:</strong> <span class="math math-inline">\pi_0</span>-FAST는 훈련 과정에서 한 번도 보지 못한 새로운 실험실 환경, 새로운 배경, 새로운 물체 배치 상태에서 오직 자연어 명령(“파란색 큐브를 오른쪽으로 치워”)만으로 작업을 수행하는 데 성공했다.3</li>
<li><strong>비교 우위:</strong> 기존의 OpenVLA와 같은 모델들은 DROID 데이터셋을 활용할 때 주로 미세 조정(Fine-tuning)을 통해서만 성능을 냈거나, 제로샷 성능이 현저히 떨어졌다. FAST의 성공은 DCT 기반의 압축된 행동 표현이 로봇의 움직임에 대한 근본적인 특징(Feature)을 잘 포착하여, 환경이 바뀌어도 강건하게 작동함을 시사한다. 이는 로봇 파운데이션 모델이 추구하는 “어디서나 작동하는(Work everywhere)” 목표에 한 걸음 다가선 성과이다.</li>
</ul>
<hr />
<h2>6.  심층 분석: 추론 지연(Inference Latency) - FAST의 아킬레스건인가?</h2>
<p>FAST가 훈련 효율성과 성능 면에서 탁월한 성과를 보였음에도 불구하고, 실시간 로봇 제어 관점에서는 <strong>추론 지연(Inference Latency)</strong> 이라는 중요한 트레이드오프가 존재한다. 본 보고서는 이 문제를 심층적으로 해부하고, 이것이 실제 애플리케이션에 미치는 영향을 분석한다.</p>
<h3>6.1  자기회귀 디코딩의 병목 현상</h3>
<p>확산 모델이나 플로우 매칭 모델은 추론 시 노이즈로부터 행동 청크 전체(예: 50단계의 행동)를 동시에 생성한다(Non-autoregressive generation). 물론 노이즈 제거를 위해 여러 단계(Step)를 거쳐야 하지만, 이는 병렬화가 가능하거나 최적화된 솔버(Solver)를 통해 속도를 높일 수 있다.</p>
<p>반면, <span class="math math-inline">\pi_0</span>-FAST는 언어 모델처럼 토큰을 하나씩 순차적으로 생성해야 한다.</p>
<ul>
<li>FAST가 1초 분량의 행동(50 steps)을 약 50개의 토큰으로 압축했다고 가정하자.</li>
<li>로봇이 다음 1초 동안 움직이기 위해서는 트랜스포머 모델이 50번의 포워드 패스(Forward pass)를 수행해야 한다.</li>
<li>트랜스포머 모델이 거대할수록(수 십억 파라미터), 이 50번의 연산은 상당한 시간(Wall-clock time)을 소모한다. 실제 실험에서도 <span class="math math-inline">\pi_0</span>-FAST의 추론 속도는 확산 기반 모델보다 느린 것으로 보고되었다.1</li>
</ul>
<h3>6.2  실시간 제어 루프와의 충돌</h3>
<p>로봇 제어 시스템은 정해진 주기(Control Loop) 내에 명령을 받아야 한다. 예를 들어 50Hz 제어라면 20ms 안에 다음 행동이 계산되어야 한다.</p>
<ul>
<li><strong>동기식 실행의 한계:</strong> 만약 모델의 추론 시간이 행동을 실행하는 시간(1초)보다 길어진다면, 로봇은 행동을 멈추고 다음 명령을 기다려야 하는 “Thinking time“이 발생한다. 이는 동작의 끊김(Stuttering)을 유발하며, 유체 역학적 상호작용이 필요한 작업(액체 따르기)이나 균형 유지가 필요한 작업(이족 보행)에서는 실패로 이어질 수 있다.</li>
<li><strong>비동기식 실행과 청킹(Chunking):</strong> 이를 완화하기 위해 현재 행동 청크를 실행하는 동안 백그라운드에서 다음 행동 청크를 미리 계산하는 비동기식 처리가 필수적이다.11 하지만 FAST의 추론 시간이 너무 길다면, 비동기 처리를 해도 제때에 다음 행동을 준비하지 못할 위험이 있다.</li>
</ul>
<h3>6.3  해결 방안 및 기술적 전망</h3>
<p>연구진과 관련 커뮤니티는 이러한 추론 지연 문제를 해결하기 위해 다양한 기법을 모색하고 있다.</p>
<ol>
<li><strong>추측적 디코딩 (Speculative Decoding):</strong> 작은 “드래프트 모델“이 빠르게 토큰을 생성하고, 큰 모델이 이를 검증하는 방식이다. 행동 데이터는 연속성이 강하므로 드래프트 모델의 예측이 맞을 확률이 높아 효과적일 수 있다.</li>
<li><strong>비자기회귀(Non-autoregressive) 생성:</strong> 트랜스포머가 토큰을 순차적이 아니라 병렬로 생성하도록 훈련하는 기법이다. 행동 데이터의 저주파 성분(DCT 계수의 앞부분)은 자기회귀로 예측하고, 고주파 성분은 병렬로 예측하는 하이브리드 방식도 고려해볼 만하다.</li>
<li><strong>추론 엔진 최적화:</strong> vLLM, TensorRT-LLM 등 LLM 추론 가속화 기술을 VLA에 적용하여 처리량(Throughput)을 높이는 엔지니어링적 접근이 필요하다.</li>
<li><strong>토큰 압축률 증대:</strong> BPE 알고리즘을 개선하거나 더 공격적인 양자화를 통해 시퀀스 길이를 더욱 줄이는 연구가 진행될 수 있다.</li>
</ol>
<hr />
<h2>7.  결론 및 미래 전망: 행동의 언어화가 여는 가능성</h2>
<p><strong>FAST: Efficient Action Tokenization for Vision-Language-Action Models</strong> 연구는 로봇 학습의 고질적인 난제였던 ‘연속 행동의 이산화’ 문제를 신호 처리와 토큰화 기술의 융합으로 해결한 이정표적인 연구이다. 본 보고서의 분석을 종합하여 도출한 결론과 시사점은 다음과 같다.</p>
<ol>
<li><strong>로봇 파운데이션 모델의 ‘공용어’ 정립:</strong> FAST+ 토크나이저는 서로 다른 하드웨어와 제어 주기를 가진 로봇들이 하나의 모델 안에서 통합될 수 있는 ’공용어(Lingua Franca)’를 제공한다. 이는 인터넷의 텍스트 데이터가 LLM을 탄생시켰듯, 다양한 로봇 데이터가 하나의 거대 지능으로 수렴하는 기폭제가 될 것이다.</li>
<li><strong>훈련 효율성이 이끄는 규모의 경제:</strong> 5배 빠른 훈련 속도는 단순히 비용 절감을 넘어, 실험의 사이클을 가속화하고 데이터의 규모를 폭발적으로 늘릴 수 있는 동력을 제공한다. 이는 로봇 AI의 발전 속도를 한 단계 끌어올릴 핵심 경쟁력이다.</li>
<li><strong>아직 끝나지 않은 과제, 실시간성:</strong> 훈련에서의 승리가 추론에서의 승리로 직결되지는 않았다. 자기회귀 모델의 추론 지연 문제는 실시간 로봇 제어 시스템에 통합되기 위해 반드시 넘어야 할 산이다. 하드웨어 가속, 알고리즘 최적화, 그리고 새로운 디코딩 전략의 융합이 향후 연구의 핵심 주제가 될 것이다.</li>
</ol>
<p>궁극적으로 FAST는 “로봇의 행동도 언어와 같다“는 가설을 기술적으로 입증해 보였다. 로봇이 세상을 이해하고 물리적으로 개입하는 과정이 ’토큰의 생성’이라는 하나의 통일된 프로세스로 처리됨으로써, 우리는 진정한 의미의 멀티모달 제너럴리스트 에이전트(Generalist Agent)의 탄생을 목격하고 있다. 향후 FAST의 방법론이 비디오 생성, 4족 보행, 더 나아가 휴머노이드의 전신 제어에까지 확장 적용될 때, 로봇 지능의 지평은 지금보다 훨씬 넓어질 것이다.</p>
<h2>8. 주요 참고 문헌 및 출처</h2>
<ul>
<li><strong>핵심 논문:</strong> FAST: Efficient Action Tokenization for Vision-Language-Action Models.3</li>
<li><strong>모델 및 데이터:</strong> <span class="math math-inline">\pi_0</span>, OpenVLA, DROID Dataset, LIBERO Benchmark.1</li>
<li><strong>기술적 배경:</strong> Discrete Cosine Transform (DCT), Byte Pair Encoding (BPE), Action Chunking.3</li>
<li><strong>성능 지표:</strong> Training speedup (5x), Token compression (~13x), Zero-shot success rates.3</li>
<li><strong>비교 대상:</strong> Diffusion Models (Flow Matching), Naïve Binning, ACT.3</li>
</ul>
<h2>9. 참고 자료</h2>
<ol>
<li>FAST: Efficient Robot Action Tokenization - Physical Intelligence, https://www.physicalintelligence.company/research/fast</li>
<li>π0 and π0-FAST: Vision-Language-Action Models for General Robot Control, https://huggingface.co/blog/pi0</li>
<li>FAST: Efficient Action Tokenization for Vision-Language-Action Models - arXiv, https://arxiv.org/html/2501.09747v1</li>
<li>FAST: Efficient Action Tokenization for Vision-Language-Action Models - Physical Intelligence, https://www.physicalintelligence.company/download/fast.pdf</li>
<li>[2501.09747] FAST: Efficient Action Tokenization for Vision-Language-Action Models - arXiv, https://arxiv.org/abs/2501.09747</li>
<li>RT-1: ROBOTICS TRANSFORMER FOR REAL-WORLD CONTROL AT SCALE, https://robotics-transformer1.github.io/assets/rt1.pdf</li>
<li>Physical-Intelligence/openpi - GitHub, https://github.com/Physical-Intelligence/openpi</li>
<li>FAST: Efficient Action Tokenization for Vision-Language-Action Models (Paper Walkthrough), https://www.youtube.com/watch?v=v7-RSX24CGM</li>
<li>Paper page - FAST: Efficient Action Tokenization for Vision …, https://huggingface.co/papers/2501.09747</li>
<li>FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization - arXiv, https://arxiv.org/html/2512.04952v1</li>
<li>Real-Time Action Chunking with Large Models - Physical Intelligence, https://www.physicalintelligence.company/research/real_time_chunking</li>
<li>Action Tokenizer Matters in In-Context Imitation Learning - arXiv, https://arxiv.org/html/2503.01206v3</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>