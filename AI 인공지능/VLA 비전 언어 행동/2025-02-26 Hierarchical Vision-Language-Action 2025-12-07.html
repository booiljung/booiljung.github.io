<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Hierarchical Vision-Language-Action (Hierarchical VLA, 계층적 시각-언어-행동) 모델</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Hierarchical Vision-Language-Action (Hierarchical VLA, 계층적 시각-언어-행동) 모델</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">Vision-Language-Action(VLA) 모델</a> / <span>Hierarchical Vision-Language-Action (Hierarchical VLA, 계층적 시각-언어-행동) 모델</span></nav>
                </div>
            </header>
            <article>
                <h1>Hierarchical Vision-Language-Action (Hierarchical VLA, 계층적 시각-언어-행동) 모델</h1>
<p>2025-12-07, G25DR</p>
<h2>1.  서론: 구체화된 인공지능(Embodied AI)의 패러다임 전환</h2>
<h3>1.1  로봇 조작의 역사적 흐름과 VLA의 등장</h3>
<p>인공지능 연구의 궁극적인 목표 중 하나는 물리적 세계와 상호작용하며 복잡한 작업을 수행할 수 있는 일반화된 로봇 에이전트를 개발하는 것이다. 초기 로봇 제어 시스템은 정교하게 설계된 상태 머신(State Machine)과 모션 플래닝(Motion Planning) 알고리즘에 의존했다. 이러한 시스템은 구조화된 공장 환경에서는 탁월한 성능을 발휘했으나, 비정형화된 가정이나 사무실 환경과 같은 ’오픈 월드(Open World)’에서는 무력했다. 이후 딥러닝의 부상과 함께 인간의 시연을 모방하는 모방 학습(Imitation Learning, IL)이 주류로 부상했다. 특히 BC-Z나 RT-1(Robotic Transformer 1)과 같은 모델은 대규모 데이터셋을 통해 다양한 작업을 수행할 수 있는 가능성을 보여주었다.1</p>
<p>최근 거대 언어 모델(LLM)과 시각-언어 모델(VLM)의 비약적인 발전은 로봇 공학에도 혁명적인 변화를 가져왔다. 인터넷 규모의 방대한 데이터로 사전 학습된 모델들이 로봇의 인지 능력을 획기적으로 향상시킬 수 있다는 사실이 입증되면서, 시각(Vision), 언어(Language), 그리고 행동(Action)을 하나의 통합된 신경망에서 처리하는 ‘시각-언어-행동(Vision-Language-Action, VLA)’ 모델이 등장했다. RT-2와 OpenVLA로 대표되는 1세대 VLA 모델들은 이미지와 텍스트 명령을 입력받아 로봇 팔의 제어 신호(Action Token)를 직접 출력하는 ‘단일(Flat)’ 구조를 채택했다.1 이들은 “쓰레기를 주워“와 같은 추상적인 명령을 물리적 행동으로 변환하는 데 성공하며 로봇 학습의 새로운 지평을 열었다.</p>
<h3>1.2  단일(Flat) VLA 아키텍처의 한계와 병목</h3>
<p>그러나 단일 구조의 Flat VLA 모델은 현실 세계의 복잡성을 완전히 감당하기에는 구조적인 한계를 드러내고 있다.</p>
<p>첫째, <strong>데이터의 희소성(Data Scarcity)과 학습 효율성 문제</strong>이다. 로봇의 물리적 제어 데이터는 수집 비용이 매우 높고 위험하다. 인터넷상의 텍스트나 이미지 데이터는 무한에 가깝지만, 로봇 팔의 관절 토크나 속도와 매칭된 데이터는 극히 드물다. 단일 모델이 고수준의 의미론적 추론(Semantic Reasoning)과 저수준의 정밀 제어(Fine-grained Control)를 동시에 학습하려면, 이 두 가지 역량을 모두 커버하는 방대한 양의 ’로봇 데모 데이터’가 필요하다. 이는 모델의 확장성을 심각하게 저해한다.4</p>
<p>둘째, <strong>추론(Reasoning)과 제어(Control)의 상충</strong>이다. 거대 모델은 문맥을 이해하고 계획을 수립하는 데 탁월하지만, 100Hz 이상의 고빈도로 모터를 제어하는 데는 적합하지 않다. 반대로 저수준 제어 정책은 빠르고 정교하지만, “채식주의자를 위한 샌드위치를 만들어라“와 같은 복잡한 사회적, 논리적 맥락을 이해하지 못한다. 이 두 가지 서로 다른 특성을 가진 작업을 하나의 신경망 파라미터에 압축해 넣는 것은 ’망각(Forgetting)’이나 성능 저하를 유발한다.6</p>
<p>셋째, <strong>장기 시계열(Long-horizon) 작업에서의 실패</strong>이다. 요리나 청소와 같이 수많은 단계가 연속되는 작업에서 단일 모델은 시간이 지날수록 초기 의도를 잊어버리거나, 작은 오차가 누적되어 결국 실패하는 ‘오차 전파(Error Propagation)’ 문제에 취약하다. 또한, 작업 중간에 “그거 말고 빨간색 집어“와 같은 인간의 개입(Intervention)이 발생했을 때, 전체 맥락을 유지하며 유연하게 대처하기 어렵다.1</p>
<h3>1.3  계층적(Hierarchical) 접근의 필요성</h3>
<p>이러한 한계를 극복하기 위해 학계와 산업계는 <strong>계층적 시각-언어-행동(Hierarchical VLA)</strong> 모델로 눈을 돌리고 있다. 이 접근 방식은 인지과학의 이중 프로세스 이론에 기반하여, 고수준의 추론과 저수준의 행동을 분리하고 이들을 효율적인 인터페이스로 연결하는 것을 골자로 한다. 본 보고서는 계층적 VLA의 이론적 배경부터 최신 아키텍처(Hi Robot, HAMSTER, RT-H, VAMOS, VINE 등), 그리고 미래의 기술적 과제까지 포괄적으로 분석한다.</p>
<h2>2.  이론적 프레임워크와 인지적 배경</h2>
<h3>2.1  시스템 1과 시스템 2: 인지적 분업</h3>
<p>계층적 VLA의 설계 철학은 다니엘 카네만(Daniel Kahneman)이 제시한 ’시스템 1’과 ’시스템 2’의 인지 구조와 맞닿아 있다.6</p>
<ul>
<li><strong>시스템 2 (고수준 추론기):</strong> 느리지만 논리적이고 계획적이며 의식적인 사고를 담당한다. 계층적 VLA에서는 대규모 VLM이 이 역할을 수행한다. “책상을 치워줘“라는 명령을 들었을 때, “책상 위의 물건들을 식별하고 -&gt; 쓰레기는 쓰레기통으로 -&gt; 컵은 싱크대로 -&gt; 책은 책꽂이로” 이동하는 전체적인 계획(Plan)을 수립한다. 이는 인터넷 데이터로 학습된 일반 상식과 추론 능력을 활용한다.</li>
<li><strong>시스템 1 (저수준 제어기):</strong> 빠르고 직관적이며 자동화된 반응을 담당한다. 로봇 공학에서는 모터 제어 정책(Policy)이 이에 해당한다. “컵을 잡아“라는 구체적인 하위 목표가 주어지면, 물체의 정확한 3D 위치를 파악하고 그리퍼를 닫는 일련의 근육 기억(Muscle Memory)과 같은 동작을 수행한다.</li>
</ul>
<p>계층적 VLA는 이 두 시스템을 명시적으로 분리함으로써, 각 모델이 가장 잘하는 영역에 집중하게 한다. 고수준 모델은 물리적 제어의 복잡성에서 벗어나 추론에 집중하고, 저수준 모델은 복잡한 문맥 해석의 부담 없이 빠르고 정확한 동작 수행에 집중한다.7</p>
<h3>2.2  계층적 강화학습(HRL)과 옵션 프레임워크</h3>
<p>기술적으로 이는 계층적 강화학습(Hierarchical Reinforcement Learning, HRL)의 현대적 구현으로 볼 수 있다. HRL의 ’옵션 프레임워크(Options Framework)’에서 상위 정책(High-level Policy)은 전체 작업을 수행하기 위해 어떤 하위 정책(Option)을 실행할지 결정하고, 하위 정책은 종료 조건이 충족될 때까지 구체적인 행동(Primitive Action)을 생성한다.9 최근의 계층적 VLA 모델들은 고전적인 HRL과 달리, 상위 정책을 학습시키는 데 강화학습 대신 방대한 언어-이미지 데이터셋을 활용한 자기지도학습(Self-Supervised Learning)이나 미세조정(Fine-tuning)을 주로 사용한다는 점에서 차별화된다.</p>
<h3>2.3  의미론적 계획(Semantic Planning)과 구체화(Embodiment)의 분리</h3>
<p>로봇 공학의 난제인 ‘모라벡의 역설(Moravec’s Paradox)’—고수준의 추론은 쉽지만 저수준의 감각운동 제어는 어렵다는 역설—을 해결하기 위해, 계층적 VLA는 **‘의미(Semantics)’**와 **‘신체(Embodiment)’**를 분리한다.</p>
<p>단일 모델은 특정 로봇의 신체 구조(Embodiment)에 과적합(Overfitting)되기 쉽다. 반면, 계층적 모델의 고수준 플래너는 “문으로 이동해“라는 추상적인 목표를 설정하며, 이는 로봇이 바퀴로 이동하든 네 발로 걷든 상관없는 ’신체 중립적(Embodiment-agnostic)’인 계획이다. 저수준 모델만이 해당 로봇의 구체적인 하드웨어 제약을 해결하는 ’신체 특화적(Embodiment-specific)’인 역할을 맡는다. 이러한 분리는 하나의 똑똑한 뇌(Planner)를 다양한 신체(Robot Hardware)에 공유할 수 있게 하여 범용성을 극대화한다.11</p>
<h2>3.  계층적 VLA의 핵심 아키텍처 및 방법론 심층 분석</h2>
<p>계층적 VLA 모델들을 구분하는 가장 중요한 기준은 <strong>“고수준 지능과 저수준 신체가 어떻게 대화하는가?”</strong>, 즉 **중간 표현(Intermediate Representation)**의 형식이다. 이 장에서는 대표적인 최신 모델들을 통해 다양한 접근 방식을 분석한다.</p>
<h3>3.1  HAMSTER: 시각적 흔적(Visual Trace)을 통한 직관적 유도</h3>
<p>HAMSTER(Hierarchical Action Models For Open-World Robot Manipulation)는 2025년 발표된 모델로, 언어가 아닌 **‘시각적 경로(Visual Path)’**를 중간 인터페이스로 사용하는 혁신적인 접근법을 제시했다.4</p>
<h4>3.1.1  아키텍처 메커니즘: Coarse-to-Fine 전략</h4>
<p>HAMSTER는 두 단계의 파이프라인으로 구성된다.</p>
<ol>
<li><strong>고수준 VLM (The Planner):</strong> 사전 학습된 VLM(예: Paligemma, PaLM-E 등)을 ‘Off-domain’ 데이터로 미세 조정한다. 여기서 Off-domain 데이터란 실제 로봇 운영 환경이 아닌 시뮬레이션 데이터나 유튜브 비디오 등을 의미한다. VLM은 현재 관측된 이미지와 텍스트 명령을 입력받아, 로봇의 엔드 이펙터(손끝)가 이동해야 할 대략적인 2D 궤적(Path)과 주요 경유점(Waypoints)을 예측한다. 이 출력은 이미지 좌표계상의 점들의 집합(List of points)으로 나타난다.</li>
<li><strong>중간 표현 (Path-drawn Image):</strong> 예측된 2D 경로는 원본 이미지 위에 시각적으로 덧그려진다(Overlay). 마치 내비게이션 앱이 지도 위에 파란 선을 그려주듯, 로봇이 가야 할 길을 이미지 자체에 표시하는 것이다.</li>
<li><strong>저수준 정책 (The Controller):</strong> 3D 공간 정보를 처리하는 저수준 정책(예: 3D Diffuser Actor 등)은 이 ’경로가 그려진 이미지’를 입력으로 받는다. 이미 경로가 시각적으로 주어졌으므로, 저수준 정책은 복잡한 추론을 할 필요 없이 선을 따라가며 정밀한 3D 제어(깊이 조절, 그리퍼 개폐 등)만 수행하면 된다.</li>
</ol>
<h4>3.1.2  데이터 전략과 성능: 값싼 데이터의 힘</h4>
<p>HAMSTER의 가장 큰 강점은 <strong>데이터 효율성</strong>이다. 고수준 VLM은 로봇 제어 데이터 없이도 학습할 수 있다. 예를 들어, 사람이 손으로 물건을 옮기는 비디오만 보고도 “물건을 어디서 어디로 옮기는지“에 대한 경로는 학습할 수 있다. 연구진은 시뮬레이션(RLBench)이나 사람의 비디오와 같은 저렴한 데이터를 대량으로 사용하여 고수준 모델의 일반화 능력을 극대화했다. 실험 결과, HAMSTER는 OpenVLA와 같은 최신 Flat VLA 대비 7가지 일반화 축(새로운 물체, 배경, 조명, 지시어 등)에서 평균 20% 이상의 성공률 향상을 보였으며, 이는 상대적으로 50%의 성능 개선에 해당한다.4 특히 학습 데이터에 전혀 없는 새로운 환경에서도 VLM이 경로를 제안할 수 있어 강력한 ‘Zero-shot’ 적응 능력을 보여주었다.</p>
<h3>3.2  Hi Robot: 언어적 상호작용과 연속적 행동 생성</h3>
<p>Hi Robot(Hierarchical Interactive Robot)은 복잡한 장기 작업 수행과 <strong>인간과의 실시간 언어적 상호작용</strong>에 초점을 맞춘 모델이다.6</p>
<h4>3.2.1  시스템 구성: 대화하는 로봇</h4>
<ul>
<li><strong>System 2 (고수준 VLM):</strong> 이 모델은 단순히 명령을 듣는 것을 넘어, 스스로 생각하고 질문한다(“Talking to itself”). 사용자가 “채식 샌드위치 만들어줘“라고 하면, 모델은 내부적으로 “채식 샌드위치에는 햄이 들어가면 안 된다. 치즈와 야채를 찾아야 한다“라고 추론 과정을 거친 뒤, “치즈 집기“라는 하위 목표를 설정한다. 또한, 사용자가 “그건 쓰레기가 아니야!“라고 외치면, 즉시 계획을 수정하여 해당 물체를 내려놓는 유연성을 가진다.</li>
<li><strong>System 1 (저수준 정책):</strong> 고수준 모델이 설정한 텍스트 기반의 하위 목표나 상태 정보를 받아 실제 모터를 제어한다. Hi Robot은 **‘Flow Matching’**이라는 최신 생성 모델 기법을 사용하여, 로봇의 움직임을 매우 부드럽고 연속적인 분포로 모델링한다. 이는 기존의 확산 모델(Diffusion Model)보다 빠르고 안정적인 행동 생성을 가능하게 한다.8</li>
</ul>
<h4>3.2.2  실험 결과: 샌드위치 만들기와 장기 기억</h4>
<p>Hi Robot은 ’샌드위치 만들기’와 같은 고난도 작업에서 그 진가를 발휘했다. 샌드위치를 만들려면 빵을 놓고, 재료를 순서대로 쌓고, 뚜껑을 덮는 등 긴 시퀀스가 필요하며, 재료가 흐트러지거나 빵이 찢어지는 등의 돌발 상황에 대처해야 한다. Flat VLA나 GPT-4o 기반의 단순 플래너들은 이러한 물리적 변수와 장기 계획을 동시에 처리하는 데 실패하는 경우가 많았다. 반면 Hi Robot은 계층적 구조를 통해 상위 목표를 유지하면서도 하위 정책이 물리적 조작을 정교하게 수행함으로써, 인간 전문가 수준에 근접하는 성공률을 기록했다.8 특히 사용자의 실시간 수정 지시(Correction)를 40% 이상 더 정확하게 반영하는 등 상호작용성 면에서 압도적인 성능을 보였다.</p>
<h3>3.3  RT-H: 언어 모션(Language Motions)을 통한 추상화</h3>
<p>Google DeepMind의 RT-H(Robotic Transformer-Hierarchical)는 고수준과 저수준 사이의 연결 고리로 **‘언어 모션(Language Motions)’**이라는 독창적인 개념을 도입했다.1</p>
<h4>3.3.1  언어 모션의 개념과 어휘</h4>
<p>기존 모델들이 단순히 목표 좌표(x, y)나 작업 이름(“Pick apple”)을 전달했다면, RT-H는 저수준의 물리적 동작 자체를 언어로 묘사한다.</p>
<ul>
<li><strong>어휘 예시:</strong> “팔을 앞으로 이동(move arm forward)”, “왼쪽으로 회전(rotate left)”, “그리퍼 닫기(close gripper)”, “위로 들어올리기(lift up)”.</li>
<li><strong>작동 방식:</strong> (이미지, “콜라 캔을 집어”) -&gt; <strong>고수준 VLM</strong> -&gt; “팔을 캔 쪽으로 앞으로 이동”, “그리퍼를 캔에 맞게 회전”, “집어” -&gt; <strong>저수준 정책</strong> -&gt; (구동 신호).</li>
</ul>
<h4>3.3.2  데이터 공유(Data Sharing)와 교정(Correction)의 이점</h4>
<p>이러한 방식의 가장 큰 장점은 서로 다른 작업 간의 데이터 공유가 가능하다는 것이다. “사과를 집는 것“과 “캔을 집는 것“은 고수준의 작업 목표(Task)는 다르지만, 저수준에서 “팔을 앞으로 뻗어 집는다“는 동작(Language Motion)은 동일하다. RT-H는 이러한 공통된 동작 패턴을 학습함으로써, 데이터가 적은 새로운 작업에서도 “집는다“는 동작을 이미 알고 있는 상태에서 시작할 수 있다.</p>
<p>또한, 이 인터페이스는 사람이 개입하기에 매우 직관적이다. 로봇이 실수하려고 할 때 “아니, 왼쪽으로 좀 더 가“라고 말하면, 이 문장 자체가 저수준 정책의 입력(Language Motion)이 되어 즉시 반영된다. RT-H는 이러한 인간의 언어적 개입을 통해 학습된 정책을 실시간으로 수정하고, 이를 다시 학습 데이터로 활용하여 성능을 지속적으로 개선하는 루프를 형성한다.17 실험에서 RT-H는 이러한 계층적 구조 덕분에 기존 RT-2 대비 다양한 작업에서 더 높은 성공률과 강건함을 보여주었다.</p>
<h3>3.4  VAMOS: 내비게이션을 위한 시각-공간적 분리 및 행동 유도성</h3>
<p>VAMOS는 로봇의 이동(Navigation) 문제에 특화된 계층적 VLA 모델로, 공간 계획과 로봇의 물리적 능력(Affordance)을 명확히 분리한다.11</p>
<h4>3.4.1  이미지 공간(Image Space) 인터페이스</h4>
<p>VAMOS의 고수준 플래너(Generalist Planner)는 로봇의 신체적 특징을 모르는 상태에서 오직 시각 정보만을 바탕으로 경로를 계획한다. VLM은 입력 이미지와 목표를 받아 이미지 상에 여러 개의 **‘후보 경로(Candidate Paths)’**를 생성한다. 이는 “이쪽으로도 갈 수 있고, 저쪽으로도 갈 수 있다“는 가능성의 제안이다.</p>
<p>그다음, 저수준의 **‘행동 유도성 모델(Affordance Model)’**이 등장한다. 이 모델은 특정 로봇(예: 4족 보행 로봇 또는 바퀴형 로봇)의 물리적 능력을 시뮬레이션에서 학습한 전문가이다. 이 모델은 고수준 플래너가 제안한 후보 경로들을 평가하여, “이 경로는 계단이라서 바퀴형 로봇은 못 가”, “이 경로는 너무 좁아서 위험해“와 같이 점수를 매기고 재순위화(Re-ranking)한다.</p>
<h4>3.4.2  교차 구체화(Cross-Embodiment)와 안정성</h4>
<p>VAMOS의 구조는 하나의 고수준 지능을 여러 종류의 로봇이 공유할 수 있게 한다. 4족 보행 로봇용 VAMOS와 바퀴형 로봇용 VAMOS는 고수준 플래너를 공유하고 저수준 모델만 갈아 끼우면 된다. 이는 하드웨어 변경에 따른 재학습 비용을 획기적으로 낮춘다. 또한, 물리적으로 불가능한 경로를 저수준 모델이 사전에 차단함으로써, 단일 모델 대비 3배 높은 성공률과 안정성을 달성했다. 특히 야외의 복잡한 지형이나 실내의 좁은 통로 등 다양한 환경에서 강인한 성능을 입증했다.11</p>
<h3>3.5  VINE: 실패로부터 배우는 계층적 강화학습</h3>
<p>VINE(Vision-Language-Action model Integrating Negative Experience)은 성공적인 데모뿐만 아니라 **실패한 경험(Negative Experience)**을 학습의 핵심 자원으로 활용한다는 점에서 독보적이다.9</p>
<h4>3.5.1  도달-회피(Reach-Avoid) 목표와 가치 함수</h4>
<p>대부분의 VLA 모델은 성공한 경로만을 모방하지만, VINE은 “무엇을 하면 안 되는지“를 배운다. 이는 계층적 강화학습(HRL) 프레임워크 내에서 구현되는데, 고수준 계획기는 목표 상태(Success set)에 도달하면서 동시에 실패 상태(Failure set, 예: 충돌, 낙하)를 회피하는 도달-회피(Reach-Avoid) 문제를 푼다.</p>
<p>VINE은 실패 데이터를 사용하여 특정 상태나 계획의 ’위험도’를 평가하는 가치 함수(Value Function)를 학습한다. 고수준 계획기가 여러 하위 목표를 제안하면, 가치 함수가 “이 목표는 실패 확률이 높다“고 판단하여 사전에 가지치기(Pruning)를 수행한다.</p>
<h4>3.5.2  안전성과 강건성</h4>
<p>이러한 방식은 로봇이 시행착오를 겪으며 위험한 행동을 할 가능성을 크게 줄여준다. VINE은 텔레오퍼레이션(원격 제어) 과정에서 발생한 인간의 실수나 로봇의 실패 기록을 버리지 않고, 이를 “피해야 할 행동“에 대한 강력한 신호로 변환하여 학습한다. 결과적으로 VINE은 기존 VLA 대비 훨씬 높은 안전성과 강건함을 확보했으며, 특히 파손 위험이 있거나 정교한 조작이 필요한 작업에서 우수한 성능을 보였다.10</p>
<h2>4.  비교 분석: 단일(Flat) VLA 대 계층적(Hierarchical) VLA</h2>
<p>아래 표는 단일 구조와 계층적 구조의 VLA 모델을 주요 기술적 지표를 통해 비교 분석한 것이다.</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>단일(Flat) VLA (예: RT-2, OpenVLA)</strong></th><th><strong>계층적(Hierarchical) VLA (예: HAMSTER, RT-H)</strong></th><th><strong>분석 및 시사점</strong></th></tr></thead><tbody>
<tr><td><strong>기본 아키텍처</strong></td><td>입력(이미지+텍스트) <span class="math math-inline">\rightarrow</span> VLM Backbone <span class="math math-inline">\rightarrow</span> Action Token</td><td>입력 <span class="math math-inline">\rightarrow</span> <strong>System 2 (Planner)</strong> <span class="math math-inline">\rightarrow</span> <strong>Intermediate Rep.</strong> <span class="math math-inline">\rightarrow</span> <strong>System 1 (Controller)</strong> <span class="math math-inline">\rightarrow</span> Action</td><td>계층적 모델은 모듈성을 통해 복잡도를 분산시킴 5</td></tr>
<tr><td><strong>추론 프로세스</strong></td><td>종단간(End-to-End) 직접 매핑. 직관적이지만 블랙박스 성격이 강함.</td><td>2단계 이상의 단계적 추론. 계획(Plan) 후 실행(Execute). 해석 가능성(Interpretability)이 높음.</td><td>오류 발생 시 원인 파악(계획 오류 vs 제어 오류)이 용이함 7</td></tr>
<tr><td><strong>데이터 요구량</strong></td><td><strong>High (Data Hungry).</strong> 모든 도메인에 대해 고비용의 로봇 데모 데이터 필요.</td><td><strong>Low to Medium (Data Efficient).</strong> 고수준 모델은 저렴한 비로봇 데이터(Sim, Web) 활용 가능.</td><td>확장성 측면에서 계층적 모델이 압도적으로 유리함 4</td></tr>
<tr><td><strong>일반화 (Generalization)</strong></td><td>학습 데이터 분포 내(In-distribution)에서 강력함. 분포 외(OOD) 상황에 취약.</td><td>의미론적, 시각적, 기하학적 일반화 능력이 뛰어남. 낯선 환경 적응력 우수.</td><td>HAMSTER의 경우 OOD 상황에서 50% 성능 향상 기록 5</td></tr>
<tr><td><strong>장기 작업 (Long-horizon)</strong></td><td>시계열이 길어질수록 오차 누적 및 의도 망각(Drift) 발생.</td><td>작업을 하위 목표로 분할하여 관리하므로 긴 작업도 안정적으로 수행.</td><td>복잡한 요리, 조립 등 실제 가사 노동에 필수적 1</td></tr>
<tr><td><strong>상호작용 및 수정</strong></td><td>모델 재학습 없이는 행동 수정이 어려움.</td><td>언어/시각적 피드백을 통해 실시간으로 계획을 수정하고 행동을 교정하기 용이.</td><td>인간-로봇 협업(HRI) 관점에서 계층적 모델이 필수적 16</td></tr>
<tr><td><strong>주요 한계</strong></td><td>확장성의 한계, 추론 능력 부족.</td><td>추론 지연(Latency), 계층 간 인터페이스 병목, 오차 전파 문제.</td><td>실시간성을 위한 최적화 기술이 요구됨 1</td></tr>
</tbody></table>
<p>심층 분석:</p>
<p>비교 결과, 계층적 VLA는 로봇 공학의 오랜 난제인 **‘일반화(Generalization)’**와 **‘데이터 효율성(Data Efficiency)’**을 해결하는 데 있어 Flat VLA보다 근본적으로 우월한 구조를 가지고 있다. Flat VLA가 입력과 출력 사이의 통계적 상관관계를 암기하는 ’기억력 좋은 기계’라면, Hierarchical VLA는 문제를 분해하고 추론하여 해결하는 ’응용력 있는 지능’에 가깝다. 특히 시뮬레이션 데이터와 인터넷 데이터를 적극 활용하여 ’두뇌’를 학습시키고, 소량의 로봇 데이터로 ’신체’를 학습시키는 전략은 데이터 수집의 병목을 해결할 가장 현실적인 대안으로 평가받는다.5</p>
<h2>5.  핵심 기술 요소: 중간 표현(Intermediate Representation)의 스펙트럼</h2>
<p>계층적 VLA의 성능과 특성은 고수준 모델과 저수준 모델이 정보를 주고받는 <strong>‘매개체’</strong>, 즉 중간 표현이 무엇이냐에 따라 결정된다.</p>
<h3>5.1  시각적/공간적 표현 (Visual/Spatial Representations)</h3>
<ul>
<li><strong>형태:</strong> 2D 궤적(Trajectory), 웨이포인트(Waypoints), 픽셀 마스크(Mask), 히트맵(Heatmap), 3D 복셀 맵(Voxel Map).</li>
<li><strong>대표 모델:</strong> HAMSTER(2D Path), VAMOS(Candidate Paths), VoxPoser(3D Value Map).</li>
<li><strong>장점:</strong> 언어로 표현하기 힘든 정밀한 공간 정보를 손실 없이 전달한다. “저 컵 손잡이를 조심스럽게 잡아“라는 말보다, 손잡이에 그려진 정확한 곡선 궤적이 로봇에게는 훨씬 명확한 지령이 된다.</li>
<li><strong>단점:</strong> 카메라 시점(Viewpoint)에 의존적이거나, 가려짐(Occlusion)이 발생할 경우 정보가 부정확해질 수 있다. 또한 2D 이미지는 3D 깊이 정보를 완벽히 담지 못할 수 있다.4</li>
</ul>
<h3>5.2  언어적 표현 (Linguistic Representations)</h3>
<ul>
<li><strong>형태:</strong> 자연어 문장, 짧은 명령어구(Action Pharse), 구조화된 텍스트.</li>
<li><strong>대표 모델:</strong> RT-H(Language Motions), Hi Robot(Text Sub-goals).</li>
<li><strong>장점:</strong> 인간이 이해하기 쉽고(Interpretable), 수정하기 쉽다. 또한 의미론적으로 유사한 작업들끼리 지식을 공유하기 좋다.</li>
<li><strong>단점:</strong> 언어의 해상도(Resolution) 문제. “조금 더 오른쪽으로“라는 표현이 정확히 몇 cm인지, 어떤 속도인지 정의하기 모호하다. 정밀한 기하학적 제어가 필요한 작업에서는 한계가 있을 수 있다.17</li>
</ul>
<h3>5.3  상징적/코드 기반 표현 (Symbolic/Code Representations)</h3>
<ul>
<li><strong>형태:</strong> Python 코드, PDDL(Planning Domain Definition Language) 스타일의 논리식, 이산적인 토큰(Discrete Tokens).</li>
<li><strong>대표 모델:</strong> Code as Policies, ProgPrompt (엄밀히 말해 VLA 이전 세대지만 계층적 사상의 시초).</li>
<li><strong>장점:</strong> 논리적 흐름 제어(if-else, loop)가 가능하여 복잡한 알고리즘적 작업을 수행하기 좋다.</li>
<li><strong>단점:</strong> 비정형화된 물리적 세계의 변수(마찰력, 미끄러짐 등)를 코드로 완벽하게 모델링하기 어렵다. 최근 VLA 트렌드에서는 순수 코드보다는 시각/언어 표현이 더 선호되는 추세다.10</li>
</ul>
<h2>6.  데이터 전략: Sim-to-Real과 이종 데이터의 융합</h2>
<p>계층적 VLA는 학습 데이터의 **질(Quality)**과 **양(Quantity)**의 비대칭성을 전략적으로 활용한다.</p>
<h3>6.1  고수준 모델: Off-domain 데이터의 무한한 확장</h3>
<p>고수준 VLM을 학습시키는 데는 로봇 데이터가 굳이 필요 없다.</p>
<ul>
<li><strong>인터넷 비디오:</strong> Ego4D나 Epic-Kitchens와 같은 대규모 1인칭 비디오 데이터셋을 통해, 요리 순서나 도구 사용법과 같은 절차적 지식(Procedural Knowledge)을 학습한다.</li>
<li><strong>시뮬레이션 데이터:</strong> RLBench, Maniskill 같은 시뮬레이터에서 생성된 데이터는 물리적으로는 완벽하지 않더라도, “어디로 움직여야 하는가“에 대한 기하학적 계획 능력을 학습하는 데는 충분하다. HAMSTER 연구팀은 시뮬레이션 데이터로 학습된 VLM이 실제 현실 세계의 이미지에 대해서도 정확한 경로를 예측함을 증명했다.4 이는 ‘Sim-to-Real’ 격차를 고수준 추론 단계에서 효과적으로 극복할 수 있음을 시사한다.</li>
</ul>
<h3>6.2  저수준 모델: In-domain 데이터의 정밀화</h3>
<p>저수준 정책은 실제 로봇 하드웨어의 동역학(Dynamics)을 배워야 하므로 실제 로봇 데이터가 필수적이다. 그러나 계층적 구조 덕분에 필요한 데이터의 양은 획기적으로 줄어든다. 고수준 모델이 탐색 공간(Search Space)을 좁혀주기 때문에, 저수준 모델은 전체 작업을 처음부터 배우는 것이 아니라 주어진 경로를 따라가는 제어 능력만 배우면 되기 때문이다. 이는 수천 시간의 데모가 필요했던 과거와 달리, 수백 개의 데모만으로도 새로운 기술을 습득할 수 있게 한다.4</p>
<h2>7.  기술적 과제와 구현의 난관</h2>
<p>계층적 VLA가 많은 장점을 가지고 있음에도 불구하고, 실제 현장에 배포하기 위해서는 해결해야 할 중요한 기술적 과제들이 남아있다.</p>
<h3>7.1  추론 지연(Inference Latency)과 실시간성</h3>
<p>두 개의 거대 모델(VLM + Policy)을 순차적으로 실행하는 구조는 필연적으로 계산 비용을 증가시킨다. 특히 수십억 파라미터(7B~)를 가진 VLM을 매 프레임마다 실행하는 것은 현재 하드웨어로는 불가능에 가깝다.</p>
<ul>
<li><strong>해결책:</strong> 비동기적(Asynchronous) 실행이 주된 해법이다. 고수준 모델은 1Hz(초당 1회) 정도로 느리게 실행되어 큰 계획을 업데이트하고, 저수준 모델은 50Hz 이상의 빠른 속도로 실행되어 최신 센서 정보를 반영하며 모터를 제어한다.1</li>
</ul>
<h3>7.2  오차 전파(Error Propagation)와 복구</h3>
<p>“Garbage In, Garbage Out.” 고수준 모델이 엉뚱한 계획(예: 벽을 뚫고 지나가라는 경로)을 내리면, 저수준 모델이 아무리 뛰어나도 작업은 실패한다. 계층적 시스템은 상위 계층의 판단 착오가 하위 계층의 실패로 직결되는 위험성을 안고 있다.</p>
<ul>
<li><strong>해결책:</strong> VINE과 같이 계획의 타당성을 검증하는 가치 함수(Critic)를 도입하거나, Hi Robot처럼 하위 모델의 실행 결과를 상위 모델이 모니터링하여 계획을 수정하는 폐루프(Closed-loop) 시스템 구축이 필수적이다.10</li>
</ul>
<h3>7.3  인터페이스 불일치(Interface Mismatch)</h3>
<p>고수준 모델이 생성한 중간 표현이 저수준 모델이 이해하기 어려운 형태이거나, 정보가 충분하지 않을 수 있다. 예를 들어, 2D 경로만으로는 물건을 집을 때의 손목 각도(Orientation)를 알 수 없어 파지(Grasping)에 실패할 수 있다. 이를 해결하기 위해 2D 경로와 함께 텍스트 설명이나 3D 깊이 정보를 함께 전달하는 멀티모달 인터페이스 연구가 진행 중이다.</p>
<h2>8.  결론 및 향후 전망: 지능형 로봇의 미래</h2>
<p>본 보고서는 계층적 시각-언어-행동(Hierarchical VLA) 모델이 로봇 공학의 현재 한계를 돌파할 가장 유력한 패러다임임을 확인하였다.</p>
<p>Hi Robot, HAMSTER, RT-H, VAMOS, VINE 등의 연구 성과는 “생각하는 머리(System 2)“와 “움직이는 신체(System 1)“의 전략적 분리가 데이터 효율성, 일반화 능력, 그리고 인간과의 상호작용성을 동시에 향상시킬 수 있음을 실증적으로 보여주었다.</p>
<p>향후 이 분야의 연구는 다음과 같은 방향으로 진화할 것으로 전망된다.</p>
<ol>
<li><strong>월드 모델(World Models)과의 통합:</strong> 단순히 현재 상태를 보고 행동하는 것을 넘어, 미래를 시뮬레이션하고 예측하는 비디오 생성 모델(Video Generative Model)이 고수준 플래너의 역할을 대체하거나 보완하게 될 것이다. 이는 로봇이 행동하기 전에 결과를 ’상상’해보고 최적의 행동을 선택하는 진정한 의미의 계획 능력을 부여할 것이다.</li>
<li><strong>뉴로-심볼릭(Neuro-Symbolic) 융합:</strong> 딥러닝의 직관과 심볼릭 AI의 논리적 엄밀함을 결합하여, 더 투명하고 검증 가능한(Verifiable) 로봇 제어 시스템을 구축하려는 시도가 이어질 것이다.</li>
<li><strong>하드웨어-소프트웨어 공진화:</strong> 계층적 모델의 연산 부하를 처리하기 위해, 엣지 디바이스에서의 고효율 추론을 위한 전용 NPU(Neural Processing Unit)나 모델 경량화 기술이 로봇 하드웨어와 함께 발전할 것이다.</li>
</ol>
<p>결론적으로, 계층적 VLA는 로봇이 실험실의 통제된 환경을 벗어나, 예측 불가능하고 복잡한 인간의 일상 속으로 들어오게 만드는 핵심적인 교두보가 될 것이다. 이는 단순히 기술적인 진보를 넘어, 로봇이 인간의 파트너로서 공존할 수 있는 인지적 기반을 마련하는 중요한 이정표이다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications - IEEE Xplore, https://ieeexplore.ieee.org/iel8/6287639/10820123/11164279.pdf</li>
<li>Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications, https://arxiv.org/html/2510.07077v1</li>
<li>Vision-Language-Action Models: Concepts, Progress, Applications and Challenges - arXiv, https://arxiv.org/html/2505.04769v1</li>
<li>HAMSTER: Hierarchical Action Models For Open-World Robot …, https://hamster-robot.github.io/</li>
<li>\method: Hierarchical Action Models for Open-World Robot Manipulation - arXiv, https://arxiv.org/html/2502.05485v1</li>
<li>Hi Robot, https://hi-robot-vla.github.io/</li>
<li>[2502.05485] HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation, https://arxiv.org/abs/2502.05485</li>
<li>Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models - arXiv, https://arxiv.org/html/2502.19417v1</li>
<li>12월 7, 2025에 액세스, [https://arxiv.org/html/2512.03913v1#:<sub>:text=We%20introduce%20VINE%2C%20a%20hierarchical,signal%20rather%20than%20noisy%20supervision.](https://arxiv.org/html/2512.03913v1#:</sub>:text=We introduce VINE%2C a hierarchical, <a href="https://arxiv.org/html/2512.03913v1#:~:text=We%20introduce%20VINE%2C%20a%20hierarchical,signal%20rather%20than%20noisy%20supervision.">https://arxiv.org/html/2512.03913v1#:~:text=We%20introduce%20VINE%2C%20a%20hierarchical,signal%20rather%20than%20noisy%20supervision.</a></li>
<li>Hierarchical Vision Language Action Model Using Success and Failure Demonstrations, https://arxiv.org/html/2512.03913v1</li>
<li>[2510.20818] VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation - arXiv, https://arxiv.org/abs/2510.20818</li>
<li>VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation - ResearchGate, https://www.researchgate.net/publication/396847609_VAMOS_A_Hierarchical_Vision-Language-Action_Model_for_Capability-Modulated_and_Steerable_Navigation</li>
<li>Abhishek Gupta - CatalyzeX, <a href="https://www.catalyzex.com/author/Abhishek%20Gupta">https://www.catalyzex.com/author/Abhishek%20Gupta</a></li>
<li>[2502.19417] Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models - arXiv, https://arxiv.org/abs/2502.19417</li>
<li>Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models - Physical Intelligence, https://www.physicalintelligence.company/download/hirobot.pdf</li>
<li>Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models - OpenReview, https://openreview.net/pdf/641fc522a201fc660b34e1224cbf7afa6ace2eee.pdf</li>
<li>RT-H: Action Hierarchies Using Language, https://rt-hierarchy.github.io/</li>
<li>RT-H: Action Hierarchies Using Language - arXiv, https://arxiv.org/html/2403.01823v1</li>
<li>RT-H: Action Hierarchies using Language - Robotics: Science and Systems, https://roboticsconference.org/2024/program/papers/49/</li>
<li>VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation - arXiv, https://arxiv.org/html/2510.20818</li>
<li>SLAM-Free Visual Navigation with Hierarchical Vision-Language Perception and Coarse-to-Fine Semantic Topological Planning | Request PDF - ResearchGate, https://www.researchgate.net/publication/395848493_SLAM-Free_Visual_Navigation_with_Hierarchical_Vision-Language_Perception_and_Coarse-to-Fine_Semantic_Topological_Planning</li>
<li>[Literature Review] Hierarchical Vision Language Action Model Using Success and Failure Demonstrations - Moonlight, https://www.themoonlight.io/en/review/hierarchical-vision-language-action-model-using-success-and-failure-demonstrations</li>
<li>Experiences from Benchmarking Vision–Language–Action Models for Robotic Manipulation - arXiv, https://arxiv.org/html/2511.11298v1</li>
<li>Hierarchical Instruction-aware Embodied Visual Tracking - OpenReview, https://openreview.net/forum?id=4O8CG6dPzK</li>
<li>VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v229/huang23b/huang23b.pdf</li>
<li>Pure Vision Language Action (VLA) Models: A Comprehensive Survey - arXiv, https://arxiv.org/html/2509.19012v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>