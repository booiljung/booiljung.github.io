<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:CoA-VLA (Chain-of-Affordance VLA)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>CoA-VLA (Chain-of-Affordance VLA)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">Vision-Language-Action(VLA) 모델</a> / <span>CoA-VLA (Chain-of-Affordance VLA)</span></nav>
                </div>
            </header>
            <article>
                <h1>CoA-VLA (Chain-of-Affordance VLA)</h1>
<p>2025-12-07, G30DR</p>
<h2>1.  서론: 로봇 파운데이션 모델의 인지적 도약과 한계</h2>
<p>최근 인공지능 분야, 특히 로봇 공학(Robotics) 영역에서는 대규모 데이터를 기반으로 학습된 시각-언어-행동(Vision-Language-Action, VLA) 모델이 새로운 패러다임으로 자리 잡았다. 인터넷 규모의 방대한 데이터셋을 통해 학습된 OpenVLA나 Octo와 같은 파운데이션 모델들은 기존의 전문화된 로봇 정책(Specialized Policies)들이 해결하지 못했던 일반화(Generalization) 문제에서 괄목할 만한 성과를 거두었다.1 그러나 이러한 엔드투엔드(End-to-End) 방식의 모델들은 입력을 행동으로 직접 매핑하는 과정에서 심각한 ‘블랙박스’ 문제를 내포하고 있으며, 인간이 물리적 세계와 상호작용할 때 수행하는 복잡하고 다단계적인 추론 과정을 결여하고 있다는 비판에 직면해 있다. 특히 대규모 언어 모델(LLM)이 ’생각의 사슬(Chain-of-Thought, CoT)’을 통해 복잡한 논리적 문제를 해결하는 추론 능력을 보여준 것과 대조적으로, 기존의 VLA 모델들은 시각적 관찰에 대한 즉각적이고 반사적인 반응에 의존하는 경향이 짙다.2</p>
<p>이러한 배경에서 등장한 Chain-of-Affordance (CoA-VLA)는 로봇의 행동 생성 과정에 명시적인 추론 단계를 도입함으로써 로봇 제어의 새로운 지평을 열었다. CoA-VLA는 단순히 시각 정보를 행동으로 변환하는 것을 넘어, 객체(Object), 파지(Grasp), 공간(Spatial), 이동(Movement)이라는 네 가지 핵심 어포던스(Affordance)를 순차적으로 추론하고, 이를 시각적 및 언어적 형태로 구체화하여 정책 결정에 반영한다.1 이는 로봇에게 ’생각할 시간’과 ’물리적 상식’을 부여하는 과정으로 해석될 수 있으며, 결과적으로 복잡한 장기 작업(Long-horizon Tasks)과 낯선 환경에서의 적응력을 획기적으로 향상시킨다. 본 보고서는 CoA-VLA의 아키텍처적 혁신, 어포던스 기반 추론의 세부 메커니즘, 그리고 시뮬레이션 및 실제 환경에서의 검증 결과를 포괄적으로 분석하고, 이것이 차세대 로봇 지능에 시사하는 바를 논한다.</p>
<h2>2.  기존 로봇 제어 시스템의 한계와 CoA-VLA의 이론적 배경</h2>
<h3>2.1  직관적 반응형 모델의 구조적 결함</h3>
<p>기존의 VLA 모델들, 예를 들어 RT-2나 OpenVLA 등은 시각적 관찰(Observation)과 언어적 명령어(Instruction)를 입력받아 로봇 팔의 관절 제어 신호(Action)를 직접 출력하는 구조를 취한다. 이러한 방식은 데이터 효율성 측면에서는 유리할 수 있으나, 모델이 내부적으로 어떤 판단 근거를 가지고 행동을 결정했는지 알 수 없는 불투명성을 가진다. 더욱이, “테이블을 치워라“와 같은 추상적인 명령을 수행할 때, 로봇은 ‘무엇을’, ‘어떻게’, ‘어디로’ 옮겨야 하는지에 대한 단계적 계획 없이 과거의 학습 데이터 분포에 의존하여 평균적인 행동을 생성하게 된다. 이는 훈련 데이터와 조금이라도 다른 환경 변수—예를 들어 물체의 낯선 포즈나 예상치 못한 장애물—가 등장했을 때 급격한 성능 저하(Catastrophic Failure)로 이어진다.5</p>
<h3>2.2  인지심리학적 접근: 어포던스 이론의 재해석</h3>
<p>제임스 깁슨(James Gibson)이 제안한 어포던스(Affordance) 이론은 환경이 에이전트에게 제공하는 행동의 가능성을 의미한다. CoA-VLA는 이 개념을 로봇 공학적으로 재해석하여, 추상적인 언어 명령을 구체적인 물리적 행동으로 연결하는 매개체로 활용한다. LLM의 CoT가 텍스트 기반의 논리적 연결 고리를 강화했다면, CoA-VLA는 물리적 공간에서의 상호작용 가능성을 탐색하는 ’어포던스 사슬’을 구축한다. 이는 로봇이 행동하기 전에 환경을 구조적으로 해석하게 만듦으로써, 단순한 모방 학습(Imitation Learning)을 넘어선 인지적 제어(Cognitive Control)를 가능하게 한다.3</p>
<h2>3.  CoA-VLA 아키텍처 및 구현 원리</h2>
<p>CoA-VLA의 기술적 핵심은 로봇의 의사결정 과정을 네 단계의 어포던스 생성 과정으로 분해하고, 이를 다시 정책 네트워크에 통합하는 독창적인 아키텍처에 있다. 이 과정에서 시각 정보와 텍스트 정보는 단순히 병렬적으로 처리되는 것이 아니라, 상호 주입(Co-injection) 메커니즘을 통해 유기적으로 결합된다.</p>
<h3>3.1  4단계 어포던스 체인 (Chain-of-Affordance Taxonomy)</h3>
<p>CoA-VLA는 로봇의 작업을 성공적으로 수행하기 위해 필수적인 네 가지 질문—무엇을, 어떻게, 어디에, 어떤 경로로—에 대응하는 어포던스를 정의한다. 이들은 상호 의존적인 관계를 가지며, 이전 단계의 추론 결과가 다음 단계의 입력으로 작용하는 연쇄적인 구조를 띤다.4</p>
<h4>3.1.1  객체 어포던스 (Object Affordance): 의미적 접지(Grounding)의 구체화</h4>
<p>첫 번째 단계는 사용자의 명령어와 시각적 환경을 매칭하는 과정이다. “빵을 집어라“라는 명령이 있을 때, 로봇은 시야 내의 수많은 객체 중에서 ’빵’을 정확히 식별해야 한다.</p>
<ul>
<li><strong>구현 메커니즘:</strong> CoA-VLA는 Grounding DINO와 같은 개방형 어휘 객체 감지(Open-vocabulary Object Detection) 모델을 활용하여 텍스트 쿼리에 해당하는 객체의 대략적인 위치를 파악한다. 이후 SAM(Segment Anything Model)을 통해 해당 객체의 정밀한 마스크(Mask)를 생성하고, 이를 기반으로 노이즈가 제거된 정교한 바운딩 박스(Bounding Box)를 추출한다.</li>
<li><strong>데이터 처리:</strong> 단순히 텍스트로 “빵이 [x, y]에 있다“라고 출력하는 것에 그치지 않고, 이미지 상에 시각적인 표시(마스크 또는 박스)를 오버레이하여 후속 네트워크가 해당 영역에 집중하도록 유도한다. 이는 모호한 명령어를 물리적 실체와 연결하는 가장 기초적이면서도 중요한 단계이다.4</li>
</ul>
<h4>3.1.2  파지 어포던스 (Grasp Affordance): 조작 가능성의 기하학적 해석</h4>
<p>객체를 식별한 후에는 해당 객체를 어떻게 잡을 것인지 결정해야 한다. 모든 물체는 형상, 무게 중심, 재질에 따라 잡을 수 있는 부위가 제한적이다. 특히 도구(Tool)의 경우, 기능적 부위가 아닌 손잡이를 잡아야 하는 등 의미론적 이해가 동반되어야 한다.</p>
<ul>
<li><strong>구현 메커니즘:</strong> CoA-VLA는 물체 표면상의 구체적인 파지 점(Grasping Point)이나 영역을 예측한다. 실험적으로, 이 단계는 낯선 포즈로 놓인 물체를 조작할 때 결정적인 역할을 수행한다. 기존 모델들이 학습된 데이터의 평균적인 파지 위치를 추정하다가 실패하는 반면, CoA-VLA는 현재 관찰된 물체의 기하학적 형상을 분석하여 최적의 파지 좌표를 생성한다.4</li>
<li><strong>텍스트-시각 표현:</strong> 텍스트로는 “망치의 손잡이 좌표 [0.73, 0.80]을 파지하라“와 같은 구체적인 지시가 생성되며, 시각적으로는 해당 좌표에 마커가 표시된다.7</li>
</ul>
<h4>3.1.3  공간 어포던스 (Spatial Affordance): 3차원 관계의 이해</h4>
<p>물체를 파지한 이후에는 이를 배치하거나 이동시킬 목표 지점에 대한 공간적 판단이 필요하다. “서랍 안에 넣어라” 또는 “컵 옆에 두어라“와 같은 명령은 객체 간의 상대적인 위치 관계와 빈 공간(Free Space)에 대한 인식을 요구한다.</p>
<ul>
<li><strong>구현 메커니즘:</strong> RoboPoint와 같은 VLM 기반의 공간 추론 모델을 통합하여, 2D 이미지 내에서 3차원 공간 관계를 해석한다. 이는 로봇이 물체를 놓을 때 기존 물체와 충돌하거나, 불안정한 위치에 놓는 실수를 방지한다.</li>
<li><strong>데이터 처리:</strong> 시스템은 “접시 위의 빈 공간 좌표 [0.45, 0.68]에 배치 가능하다“와 같은 텍스트 정보와 함께, 이미지 상에 배치 가능한 영역을 히트맵(Heatmap)이나 좌표로 시각화한다.4</li>
</ul>
<h4>3.1.4  이동 어포던스 (Movement Affordance): 동적 경로 계획</h4>
<p>마지막 단계는 현재 위치에서 목표 위치까지의 이동 경로를 생성하는 것이다. 이는 단순한 직선 이동이 아니라, 환경 내의 장애물을 회피하고 로봇 팔의 기구학적 제약을 고려한 ‘실행 가능한(Feasible)’ 경로여야 한다.</p>
<ul>
<li><strong>구현 메커니즘:</strong> 시작점과 목표점 사이의 충돌 없는 경로(Collision-free trajectory)를 웨이포인트(Waypoint) 형태로 생성한다. 이는 기존의 고전적 경로 계획 알고리즘(RRT* 등)과 딥러닝 기반의 추론이 결합된 형태를 띠며, 복잡한 환경에서도 안전한 조작을 보장한다.</li>
<li><strong>시각적 가이드:</strong> 이미지 상에 로봇 엔드 이펙터(End-effector)가 따라가야 할 궤적을 선으로 그려 넣음으로써, 정책 네트워크가 시각적인 가이드를 따라 행동을 생성하도록 돕는다.2</li>
</ul>
<h3>3.2  시각-언어 상호 주입 (Visual-Text Co-injection) 모듈</h3>
<p>CoA-VLA의 성능을 뒷받침하는 또 다른 기술적 축은 생성된 어포던스 정보를 정책 네트워크에 주입하는 방식이다. 단순히 텍스트 임베딩과 이미지 임베딩을 연결(Concatenation)하는 기존 방식은 두 모달리티 간의 정보 교환을 제한한다.</p>
<ul>
<li><strong>상호 주입 메커니즘:</strong> CoA-VLA는 어포던스 텍스트와 시각적 특징(Visual Features)을 트랜스포머 레이어 내에서 상호 교차(Cross-attention)시킨다. 즉, 텍스트 토큰이 처리될 때 관련된 이미지 영역의 특징을 참조하고, 반대로 이미지 토큰이 처리될 때 관련된 텍스트 정보를 참조하도록 설계되었다.</li>
<li><strong>효과:</strong> 이 메커니즘은 ‘파지 어포던스’ 텍스트가 생성될 때 모델이 물체의 손잡이 부분 이미지에 집중하게 만들고, ‘이동 어포던스’ 이미지가 처리될 때 “장애물을 피하라“는 텍스트 제약 조건을 반영하게 한다. 결과적으로 로봇의 행동 결정은 언어적 논리와 시각적 현실이 정밀하게 동기화된 상태에서 이루어진다.1</li>
</ul>
<h3>3.3  동적 어포던스 선택 (Dynamic Affordance Selection)</h3>
<p>모든 작업이 네 가지 어포던스를 모두 필요로 하는 것은 아니다. 예를 들어, 단순히 제자리에서 물체를 조작하는 작업은 이동 어포던스나 공간 어포던스가 불필요할 수 있다. CoA-VLA는 작업의 맥락을 파악하여 필요한 어포던스 단계만을 선별적으로 활성화하는 동적 선택 메커니즘을 탑재하고 있다. 이는 불필요한 연산 부하를 줄이고 추론 속도를 최적화하는 데 기여하며, 과도한 정보로 인한 모델의 혼란을 방지한다.5</p>
<h2>4.  정량적 성능 평가: LIBERO 벤치마크</h2>
<p>CoA-VLA의 효용성은 로봇 학습 분야의 표준 벤치마크인 LIBERO(Lifelong Robot Learning)를 통해 객관적으로 검증되었다. 이 벤치마크는 공간적 추론(Spatial), 객체 조작(Object), 목표 지향(Goal), 장기 작업(Long) 등 다양한 측면에서 로봇의 능력을 평가한다.</p>
<h3>4.1  벤치마크 결과 비교 분석</h3>
<p>아래 표는 CoA-VLA와 기존의 최신 모델들(State-of-the-Art) 간의 성능을 비교한 것이다.</p>
<table><thead><tr><th><strong>모델 (Model)</strong></th><th><strong>LIBERO-Spatial</strong></th><th><strong>LIBERO-Goal</strong></th><th><strong>LIBERO-Average (전체 평균)</strong></th></tr></thead><tbody>
<tr><td><strong>Diffusion Policy</strong> 4</td><td>78.3%</td><td>73.6%</td><td>72.4%</td></tr>
<tr><td><strong>ScaleDP</strong> 4</td><td>79.1%</td><td>-</td><td>72.9%</td></tr>
<tr><td><strong>Octo</strong> 4</td><td>78.9%</td><td>84.6%</td><td>75.1%</td></tr>
<tr><td><strong>OpenVLA</strong> 4</td><td>84.7%</td><td>79.2%</td><td>76.5%</td></tr>
<tr><td><strong>CoA-VLA (제안 모델)</strong> 4</td><td><strong>85.3%</strong></td><td><strong>(상위)</strong></td><td><strong>79.8%</strong></td></tr>
</tbody></table>
<ul>
<li><strong>종합 성능 우위:</strong> CoA-VLA는 전체 평균 성공률 79.8%를 기록하며, 직전 최고 성능 모델인 OpenVLA(76.5%)를 약 3.3%p 차이로 앞섰다.5 이는 수치상으로는 근소해 보일 수 있으나, 이미 고도화된 모델들 간의 경쟁에서, 특히 실패 비용이 높은 로봇 조작 작업에서 3% 이상의 신뢰성 향상은 실질적인 기술적 진보를 의미한다.</li>
<li><strong>공간 추론 능력:</strong> LIBERO-Spatial 벤치마크에서 85.3%의 가장 높은 성공률을 기록했다. 이는 ’공간 어포던스’와 ‘이동 어포던스’ 모듈이 복잡한 공간적 제약 조건을 해결하는 데 실질적인 도움을 주고 있음을 시사한다. OpenVLA 역시 높은 성능을 보였으나, CoA-VLA는 명시적인 공간 추론 단계를 통해 더욱 정교한 조작을 가능케 했다.</li>
<li><strong>비교 모델 분석:</strong> Diffusion Policy나 ScaleDP와 같은 비-VLA(Non-VLA) 모델들은 평균 72%대의 성공률에 머물러, 대규모 언어 모델의 지식을 활용하는 VLA 접근법의 우위가 재확인되었다. 동시에, 같은 VLA 모델군 내에서도 단순한 데이터 학습(OpenVLA, Octo)보다 구조화된 추론(CoA-VLA)이 더 효과적임이 입증되었다.4</li>
</ul>
<h2>5.  현실 세계(Real-World) 검증 및 일반화 능력</h2>
<p>시뮬레이션에서의 성공이 실제 로봇 하드웨어에서의 성능을 보장하지는 않는다. 조명 변화, 센서 노이즈, 물리 엔진의 불완전성 등으로 인한 ’Sim-to-Real Gap’은 로봇 공학의 난제이다. CoA-VLA는 실제 환경에서의 복잡한 작업 수행을 통해 이 격차를 극복하는 능력을 입증했다.</p>
<h3>5.1  장기 작업(Long-Horizon Tasks)에서의 강건성</h3>
<p>연구팀은 ‘차 서빙하기(Serving tea)’, ‘쓰레기 치우기(Cleaning garbage)’, ‘테이블 닦기(Wiping tables)’ 등 7가지의 고난도 실제 작업을 구성하여 평가를 진행했다. 이러한 작업들은 단일 동작으로 끝나지 않고, [객체 인식 -&gt; 파지 -&gt; 이동 -&gt; 조작 -&gt; 복귀]의 시퀀스가 완벽하게 이어져야 한다.</p>
<ul>
<li><strong>실험 결과:</strong> CoA-VLA는 이러한 장기 작업에서 OpenVLA와 Octo를 압도하는 성공률을 보였다. 예를 들어, 쓰레기를 집어서 쓰레기통에 버리는 작업에서, 기존 모델들은 쓰레기통의 위치를 정확히 인식하지 못하고 허공에 버리거나, 이동 중 의자와 충돌하는 경우가 빈번했다. 반면 CoA-VLA는 ’공간 어포던스’를 통해 쓰레기통 입구를 정확히 타게팅하고, ’이동 어포던스’를 통해 충돌 없는 경로를 생성하여 높은 성공률을 기록했다.4</li>
</ul>
<h3>5.2  보지 못한 객체 포즈(Unseen Object Poses)에 대한 적응력</h3>
<p>CoA-VLA의 가장 강력한 강점 중 하나는 훈련 데이터에 없었던 낯선 상황에 대한 대처 능력이다. 이를 검증하기 위해 연구팀은 학습 시 보지 못했던 다양한 객체 포즈(Orientation)를 테스트했다.</p>
<ul>
<li><strong>파지 어포던스의 위력:</strong> 물체가 뒤집혀 있거나, 옆으로 누워 있거나, 다른 물체 위에 겹쳐 있는 상황에서 기존 모델들은 학습된 정형화된 파지법을 시도하다 실패했다. 그러나 CoA-VLA는 실시간으로 관찰된 이미지에서 ’파지 어포던스’를 생성하여, 현재 물체의 형상에 맞는 새로운 파지 점을 찾아냈다.</li>
<li><strong>한계와 극복:</strong> 실험 결과에 따르면, 물체가 로봇에 대해 완전히 수평으로(Horizontally) 놓여 있어 잡을 수 있는 면적이 극도로 적은 경우에는 모든 모델이 고전했다. 그러나 이 극단적인 케이스를 제외한 대부분의 비정형 포즈에서 CoA-VLA는 탁월한 일반화 능력을 보여주었다.2</li>
</ul>
<h3>5.3  장애물 회피 및 동적 환경 적응</h3>
<p>실제 환경은 정지해 있지 않으며, 예기치 못한 장애물이 등장한다. ‘이동 어포던스’ 모듈은 이러한 동적 환경에서 빛을 발했다. 기존 모델들이 목표 지점을 향해 최단 거리로 이동하다가 장애물과 충돌하는 반면, CoA-VLA는 장애물을 인식하고 이를 우회하는 곡선 경로를 생성했다. 또한, 물체를 놓을 때도 바닥에 흩어진 다른 물체들을 피해서 ’빈 공간(Free Space)’을 찾아내는 능력이 확인되었다. 이는 로봇이 단순히 명령을 따르는 기계가 아니라, 환경을 이해하고 적응하는 지능형 에이전트로 진화했음을 보여준다.2</p>
<h2>6.  심층 논의: CoA-VLA가 제시하는 로봇 지능의 미래</h2>
<h3>6.1  인지적 추론(Cognitive Reasoning)의 시스템화</h3>
<p>CoA-VLA의 핵심 기여는 로봇 제어 시스템에 ’시스템 2(System 2)’적 사고 방식을 도입한 것이다. 대니얼 카너먼(Daniel Kahneman)이 제시한 바와 같이, 직관적이고 빠른 ‘시스템 1’ 사고만으로는 복잡하고 낯선 문제를 해결하는 데 한계가 있다. 기존의 VLA 모델들이 ’시스템 1’에 해당한다면, CoA-VLA는 어포던스 체인을 통해 분석적이고 논리적인 ‘시스템 2’ 사고 과정을 로봇 제어 루프에 통합했다. 이는 로봇이 행동의 인과관계를 이해하고, 실패 가능성을 사전에 시뮬레이션해볼 수 있는 기초를 마련한 것이다.3</p>
<h3>6.2  해석 가능성(Explainability)과 신뢰성(Trustworthiness)</h3>
<p>딥러닝 기반 로봇의 가장 큰 문제는 ‘왜 실패했는지’ 알기 어렵다는 점이다. CoA-VLA는 이 문제를 획기적으로 개선한다. 로봇이 작업을 실패했을 때, 사용자는 생성된 어포던스 체인을 역추적하여 원인을 진단할 수 있다. 예를 들어, 객체 인식은 정확했으나(객체 어포던스 성공), 파지 점을 잘못 잡았는지(파지 어포던스 실패), 아니면 이동 경로에 문제가 있었는지(이동 어포던스 실패)를 명확히 파악할 수 있다. 이러한 해석 가능성은 산업 현장에서 로봇 시스템을 디버깅하고 신뢰성을 확보하는 데 필수적인 요소이다.</p>
<h3>6.3  파운데이션 모델 생태계의 확장성</h3>
<p>CoA-VLA는 단일 모델이라기보다는 다양한 전문 모델(Expert Models)들을 통합하는 프레임워크에 가깝다. 시각 인식을 위한 Grounding DINO와 SAM, 공간 추론을 위한 RoboPoint 등 각 분야의 최고 성능 모델(SOTA)들을 모듈형으로 활용한다. 이러한 구조는 향후 더 강력한 비전 모델이나 공간 지능 모델이 등장했을 때, 전체 아키텍처를 변경하지 않고도 해당 모듈만 교체하여 성능을 업그레이드할 수 있는 유연성(Extensibility)을 제공한다. 이는 기술 발전 속도가 빠른 AI 분야에서 시스템의 수명을 연장하고 지속적인 성능 향상을 보장하는 현명한 전략이다.</p>
<h2>7.  결론</h2>
<p>본 연구 보고서는 Chain-of-Affordance (CoA-VLA) 모델이 제시하는 로봇 조작 기술의 혁신적인 진보를 상세히 분석하였다. CoA-VLA는 객체, 파지, 공간, 이동이라는 네 가지 핵심 어포던스를 명시적으로 추론하고, 이를 시각-언어 상호 주입 메커니즘을 통해 정책 결정 과정에 통합함으로써 기존 VLA 모델들이 가진 ‘블랙박스’ 한계와 낮은 일반화 능력을 극복하였다.</p>
<p>LIBERO 벤치마크에서의 우수한 성적과 실제 환경에서의 복잡한 작업 수행 능력은 CoA-VLA의 접근 방식이 단순한 이론적 제안을 넘어 실질적인 효용성을 가짐을 증명한다. 특히 훈련 데이터에 없는 낯선 물체 포즈와 장애물 환경에서도 유연하게 대처하는 모습은, 로봇이 정해진 스크립트를 반복하는 기계에서 상황을 이해하고 판단하는 지능형 에이전트로 진화하고 있음을 보여주는 강력한 증거이다.</p>
<p>결론적으로, CoA-VLA는 시각적 인식과 언어적 추론, 그리고 물리적 행동을 하나의 논리적 사슬로 연결함으로써 로봇 지능의 새로운 표준을 제시하였다. 향후 연구는 이러한 추론 과정의 실시간성을 더욱 강화하고, 더욱 다양한 어포던스(예: 도구 사용, 힘 조절 등)를 포괄하는 방향으로 확장될 것으로 전망된다. CoA-VLA는 로봇이 인간의 생활 공간으로 들어와 안전하고 유용하게 공존하기 위해 반드시 거쳐야 할 ’인지적 성숙’의 중요한 이정표가 될 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>CoA-VLA: Improving Vision-Language-Action Models via Visual-Textual Chain-of-Affordance - Semantic Scholar, https://www.semanticscholar.org/paper/CoA-VLA%3A-Improving-Vision-Language-Action-Models-Li-Zhu/5760cc722b5eabc14487e9c497a78038ab46f37d</li>
<li>[2412.20451] CoA-VLA: Improving Vision-Language-Action Models via Visual-Textual Chain-of-Affordance - arXiv, https://arxiv.org/abs/2412.20451</li>
<li>Improving Vision-Language-Action Models via Chain-of-Affordance, https://chain-of-affordance.github.io/</li>
<li>Improving Vision-Language-Action Models via Chain-of-Affordance - arXiv, https://arxiv.org/html/2412.20451v1</li>
<li>CoA-VLA: Improving Vision-Language-Action Models via Visual-Textual Chain-of-Affordance - arXiv, https://arxiv.org/html/2412.20451v2</li>
<li>Improving Vision-Language-Action Models via Chain-of-Affordance - ResearchGate, https://www.researchgate.net/publication/387539786_Improving_Vision-Language-Action_Models_via_Chain-of-Affordance</li>
<li>CoA-VLA: Improving Vision-Language-Action Models via Visual-Text Chain-of-Affordance - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2025/papers/Li_CoA-VLA_Improving_Vision-Language-Action_Models_via_Visual-Text_Chain-of-Affordance_ICCV_2025_paper.pdf</li>
<li>CoA-VLA: Improving Vision-Language-Action Models via Visual-Textual Chain-of-Affordance Supplementary Material - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2025/supplemental/Li_CoA-VLA_Improving_Vision-Language-Action_ICCV_2025_supplemental.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>