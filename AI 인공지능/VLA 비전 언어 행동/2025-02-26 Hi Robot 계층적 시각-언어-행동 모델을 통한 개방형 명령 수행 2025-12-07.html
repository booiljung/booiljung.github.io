<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Hi Robot 계층적 시각-언어-행동 모델을 통한 개방형 명령 수행</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Hi Robot 계층적 시각-언어-행동 모델을 통한 개방형 명령 수행</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">Vision-Language-Action(VLA) 모델</a> / <span>Hi Robot 계층적 시각-언어-행동 모델을 통한 개방형 명령 수행</span></nav>
                </div>
            </header>
            <article>
                <h1>Hi Robot 계층적 시각-언어-행동 모델을 통한 개방형 명령 수행</h1>
<h2>1.  서론: 로봇 공학의 새로운 패러다임</h2>
<h3>1.1  범용 로봇을 향한 여정</h3>
<p>인류는 오랫동안 인간의 생활 공간에서 공존하며 복잡한 잡무를 대신해 줄 수 있는 ’범용 로봇(Generalist Robot)’을 꿈꿔왔다. 과거 산업용 로봇이 통제된 환경, 즉 공장의 펜스 안에서 반복적이고 정밀한 작업을 수행하는 데 최적화되어 있었다면, 현대 로봇 공학의 최전선은 비정형화된 일상 환경으로 그 무대를 옮기고 있다.1 가정이나 사무실, 상점과 같은 ’야생(In the wild)’의 환경은 예측 불가능성으로 가득 차 있다. 조명은 수시로 바뀌고, 물체의 위치는 고정되어 있지 않으며, 무엇보다 로봇에게 명령을 내리는 인간 사용자의 의도는 모호하고 가변적이다.</p>
<p>이러한 환경에서 로봇이 제 기능을 수행하기 위해서는 두 가지 핵심 능력이 요구된다. 첫째는 물리적 세계를 인식하고 조작하는 ’신체적 지능(Physical Intelligence)’이며, 둘째는 인간의 언어와 의도를 해석하고 상황에 맞게 계획을 수립하는 ’인지적 지능(Cognitive Intelligence)’이다. 최근 딥러닝, 특히 대규모 언어 모델(LLM)과 시각-언어 모델(VLM)의 폭발적인 발전은 이 두 가지 지능을 결합할 수 있는 새로운 가능성을 열었다. 텍스트와 이미지를 동시에 이해하는 VLM은 로봇에게 ’눈’과 ’뇌’를 제공하였고, 이를 행동(Action)으로 직접 연결하려는 시각-언어-행동(Vision-Language-Action, VLA) 모델의 등장은 로봇 학습의 패러다임을 근본적으로 변화시키고 있다.2</p>
<h3>1.2  개방형 명령 수행의 난제</h3>
<p>그러나 기존의 VLA 모델, 예를 들어 RT-2(Robotic Transformer 2)와 같은 선행 연구들은 여전히 한계를 보였다. 이들은 “파란색 컵을 집어라“와 같은 단발성이고 구체적인 명령(Atomic Command)을 수행하는 데에는 탁월한 성능을 보였으나, “배가 고픈데 간단한 요기 거리 좀 만들어줄래?“와 같은 개방형(Open-ended)이고 추상적인 명령을 처리하는 데에는 어려움을 겪었다.1 이러한 명령은 단순히 물체를 집는 행위를 넘어, 사용자의 의도를 추론하고, 필요한 재료를 탐색하며, 다단계의 작업 순서를 계획하고, 작업 도중 발생하는 사용자의 피드백(“아, 그건 말고 다른 거”)에 유연하게 대처해야 하는 복합적인 능력을 요구하기 때문이다.</p>
<p>단일 신경망 모델이 고차원적인 논리적 추론과 저차원적인 모터 제어를 동시에 수행하려 할 때 발생하는 ‘추론-제어의 병목(Reasoning-Control Bottleneck)’ 현상은 로봇의 성능 저하를 야기하는 주된 원인이다. 복잡한 언어적 문맥을 유지하기 위해 거대한 모델을 사용하면 제어 주기가 느려져 물리적 반응성이 떨어지고, 반대로 제어에 최적화된 모델을 사용하면 복잡한 추론을 감당하지 못하는 딜레마가 존재해 왔다.</p>
<h3>1.3  하이 로봇(Hi Robot)의 제안</h3>
<p>본 보고서에서 심층 분석할 “하이 로봇(Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models)“은 이러한 딜레마를 해결하기 위해 인간의 인지 구조에서 영감을 받은 계층적 아키텍처를 제안한다.1 이 시스템은 노벨 경제학상 수상자 대니얼 카너먼이 제시한 ’시스템 1(직관적이고 빠른 사고)’과 ’시스템 2(숙고적이고 느린 사고)’의 이중 과정 이론을 로봇 제어에 적용하였다.5</p>
<p>하이 로봇은 고수준의 계획과 언어적 추론을 담당하는 VLM 기반의 고수준 정책(High-Level Policy)과, 구체적인 물리적 동작을 생성하는 VLA 기반의 저수준 정책(Low-Level Policy)으로 역할을 분담한다. 또한, 실제 데이터 수집의 한계를 극복하기 위해 VLM을 활용한 대규모 합성 데이터(Synthetic Data) 생성 파이프라인을 도입하여 로봇의 언어 이해 능력과 상황 대처 능력을 비약적으로 향상시켰다.5</p>
<p>본 연구 보고서는 하이 로봇의 기술적 아키텍처, 데이터 생성 전략, 그리고 다양한 실험 결과를 면밀히 분석함으로써, 이 시스템이 어떻게 기존 로봇의 한계를 극복하고 인간과 공존 가능한 지능형 로봇의 청사진을 제시하는지 규명하고자 한다.</p>
<h2>2.  이론적 배경 및 관련 연구 동향</h2>
<h3>2.1  로봇 학습의 진화: 모방 학습에서 파운데이션 모델까지</h3>
<p>전통적인 로봇 제어는 역운동학(Inverse Kinematics)과 궤적 최적화(Trajectory Optimization) 등 수치 해석적 방법에 의존했다. 이는 정확하지만 환경 변화에 취약하고, 사전에 정의되지 않은 물체나 상황을 다루기 어렵다는 단점이 있었다. 이후 딥러닝의 등장과 함께, 인간의 시연 데이터를 학습하여 정책(Policy)을 도출하는 모방 학습(Imitation Learning)이 주류로 부상했다. 특히 BC(Behavior Cloning)는 상태(State)와 행동(Action) 쌍을 지도 학습하여 로봇을 학습시키는 직관적인 방법론을 제공했다.</p>
<p>최근에는 인터넷 규모의 텍스트와 이미지 데이터로 사전 학습된 파운데이션 모델(Foundation Models)을 로봇에 적용하려는 시도가 폭발적으로 증가했다. LLM의 방대한 상식과 추론 능력을 로봇의 계획(Planning) 단계에 활용하거나(예: SayCan), VLM을 통해 시각적 상황을 텍스트로 설명하고 이를 다시 제어 신호로 변환하는 방식들이 연구되었다. 하이 로봇은 이러한 흐름의 연장선상에 있으면서도, 단순히 모델을 가져다 쓰는 것을 넘어 로봇 제어에 특화된 구조로 재설계했다는 점에서 차별성을 갖는다.</p>
<h3>2.2  시각-언어-행동 모델(VLA)의 등장과 한계</h3>
<p>구글의 RT-1과 RT-2는 VLA 모델의 시초라 할 수 있다. RT-1은 트랜스포머 아키텍처를 로봇 제어에 도입하여 데이터 효율성을 높였고, RT-2는 VLM을 파인튜닝하여 로봇의 행동을 언어 토큰처럼 출력하도록 만들었다.2 RT-2는 “스파이더맨 피규어를 집어서 컵에 넣어라“와 같이 시각적 의미를 이해해야 하는 작업에서 놀라운 일반화 성능을 보여주었다.</p>
<p>그러나 RT-2와 같은 ‘플랫(Flat)’ 아키텍처는 모델 하나가 모든 것을 처리해야 한다는 구조적 한계를 지닌다. 언어 모델의 토큰 생성 방식은 이산적(Discrete)이기 때문에, 로봇 팔의 연속적이고 정교한 움직임을 표현하기 위해서는 행동 공간을 강제로 이산화(Discretization)해야 했다. 이는 정밀한 제어를 어렵게 만들고, 긴 시퀀스의 행동을 생성할 때 오차가 누적되는 문제를 야기한다. 또한, 수백 단계 이상의 긴 작업(Long-horizon Task)을 수행할 때, 단일 모델은 초기의 문맥을 잃어버리거나(Forgetting), 중간에 개입되는 새로운 명령을 적절히 반영하지 못하는 경향이 있다.8</p>
<h3>2.3  이중 과정 이론(Dual-Process Theory)의 로봇 공학적 해석</h3>
<p>인지 심리학의 이중 과정 이론은 인간의 사고를 두 가지 모드로 구분한다.</p>
<ul>
<li><strong>시스템 1 (System 1):</strong> 빠르고, 자동적이며, 무의식적이다. (예: 텅 빈 도로에서 운전하기, 표정 읽기)</li>
<li><strong>시스템 2 (System 2):</strong> 느리고, 순차적이며, 의식적인 노력이 필요하다. (예: 복잡한 수학 문제 풀기, 좁은 주차장에 주차하기)</li>
</ul>
<p>로봇 공학에서 이를 해석하면, 물체를 잡거나 이동하는 것과 같은 기본 동작 기술(Primitive Skills)은 ’시스템 1’에 해당하며, 전체적인 작업 순서를 계획하거나 예외 상황을 판단하는 것은 ’시스템 2’에 해당한다. 하이 로봇은 이러한 인지 구조를 아키텍처에 직접 반영하였다. 저수준 정책은 시스템 1로서 감각 입력에 대해 즉각적인 행동 반응을 생성하고, 고수준 정책은 시스템 2로서 전체적인 상황을 조망하고 저수준 정책에게 지시를 내린다.6 이는 기존의 계층적 강화학습(HRL)과 유사해 보이지만, 상위 레벨과 하위 레벨의 인터페이스가 ’자연어’로 이루어진다는 점에서 결정적인 차이가 있다. 자연어 인터페이스는 인간이 로봇의 사고 과정을 해석(Interpretability)하고 개입(Intervention)하기 쉽게 만든다.</p>
<h3>2.4  행동 생성을 위한 생성 모델: 확산 모델과 플로우 매칭</h3>
<p>로봇의 행동 생성(Action Generation) 방식 또한 진화하고 있다. 초기에는 결정론적 정책(Deterministic Policy)이나 가우시안 혼합 모델(GMM)이 주로 사용되었으나, 최근에는 확산 모델(Diffusion Model)이 각광받고 있다. 확산 모델은 노이즈로부터 데이터를 복원하는 역확산 과정을 학습하여 복잡하고 다중 모드(Multi-modal)인 행동 분포를 정교하게 모델링할 수 있다.</p>
<p>하이 로봇의 저수준 정책은 확산 모델의 최신 형태인 <strong>플로우 매칭(Flow Matching)</strong> 기법을 활용한다.1 플로우 매칭은 데이터 분포와 노이즈 분포 사이의 확률 흐름(Probability Flow)을 미분 방정식(ODE) 형태로 직접 모델링한다. 이는 기존 확산 모델에 비해 학습이 안정적이며, 추론 시 더 적은 단계(Steps)만으로도 고품질의 샘플을 생성할 수 있어 실시간성이 중요한 로봇 제어에 적합하다. 특히 연속적인 행동 공간(Continuous Action Space)을 다루는 데 있어 이산적인 토큰 방식보다 훨씬 부드럽고 자연스러운 움직임을 생성해낸다.9</p>
<h2>3.  하이 로봇(Hi Robot) 시스템 아키텍처 상세 분석</h2>
<p>하이 로봇의 핵심은 “생각하는 뇌“와 “움직이는 몸“을 분리하되, 이 둘을 “언어“라는 매개체로 긴밀하게 연결한 데에 있다. 이 섹션에서는 각 구성 요소의 작동 원리와 기술적 세부 사항을 분석한다.</p>
<h3>3.1  시스템 개요 및 데이터 흐름</h3>
<p>하이 로봇은 사용자의 명령(Prompt)과 로봇의 시각 정보(Image Observations)를 입력받아 최종적으로 로봇 관절의 제어 명령(Action)을 출력하는 파이프라인 구조를 가진다.</p>
<ol>
<li><strong>입력 단계:</strong> 사용자가 자연어 명령(예: “식탁 좀 치워줘”)을 내리면, 로봇은 현재의 시각 정보(Base 카메라 및 Wrist 카메라 이미지)를 획득한다.</li>
<li><strong>고수준 추론 (High-Level Reasoning):</strong> 고수준 정책(VLM)이 입력된 명령과 이미지를 분석하여, 현재 상황에서 수행해야 할 가장 적절한 하위 작업(Sub-task)을 자연어 형태로 생성한다(예: “빈 콜라 캔을 집어라”).</li>
<li><strong>저수준 실행 (Low-Level Execution):</strong> 저수준 정책(VLA)은 고수준 정책이 내린 자연어 명령과 실시간 로봇 상태, 이미지를 입력받아 구체적인 행동 궤적(Trajectory)을 생성한다.</li>
<li><strong>행동 수행 및 피드백:</strong> 생성된 행동은 로봇 하드웨어를 통해 실행되며, 환경의 변화는 다시 시각 정보로 피드백되어 다음 주기의 추론에 활용된다.</li>
</ol>
<h3>3.2  고수준 정책 (High-Level Policy): 인공적 ‘내면의 목소리’</h3>
<p>고수준 정책은 시스템의 사령탑 역할을 한다. 이 모듈은 거대 시각-언어 모델(VLM)을 기반으로 하며, 사용자의 불명확하고 추상적인 명령을 로봇이 실행 가능한 구체적인 단위의 명령(Atomic Instruction)으로 변환하는 ’번역기’이자 ’계획기’이다.</p>
<ul>
<li><strong>모델 구조:</strong> <span class="math math-inline">\pi_0</span>와 유사한 대규모 트랜스포머 기반 VLM을 사용한다. 시각 인코더(Vision Encoder)는 이미지를 임베딩 벡터로 변환하고, 언어 모델은 이를 텍스트 토큰과 함께 처리한다.6</li>
<li><strong>내면의 목소리(Inner Voice) 메커니즘:</strong> 연구진은 고수준 정책의 출력을 인간이 복잡한 작업을 할 때 마음속으로 되뇌는 “내면의 목소리“에 비유한다.6 예를 들어 샌드위치를 만들 때 “이제 토마토를 넣어야지”, “아, 소스를 깜빡했네“라고 생각하는 것처럼, 고수준 정책은 끊임없이 현재 상태를 평가하고 다음 행동을 언어적으로 생성한다.</li>
<li><strong>상태 추적 및 문맥 유지:</strong> 고수준 정책은 이전 단계에서 어떤 명령을 내렸고 그 결과가 어떠했는지를 문맥(Context)으로 유지한다. 이를 통해 “나머지는 그대로 둬(Leave the rest)“와 같은 대명사나 생략이 포함된 명령도 정확히 해석할 수 있다.</li>
<li><strong>인터랙티브 피드백 수용:</strong> 사용자가 작업 도중 “그거 아니야!“라고 외치면, 고수준 정책은 이를 즉각적인 제약 조건(Constraint)으로 받아들여 계획을 수정한다. 이는 기존의 정적인 계획기(Static Planner)와 가장 차별화되는 지점으로, 동적인 환경에서의 적응성을 보장한다.10</li>
</ul>
<h3>3.3  저수준 정책 (Low-Level Policy): 플로우 매칭 기반의 정밀 제어</h3>
<p>저수준 정책은 고수준 정책의 지시를 충실히 이행하는 숙련된 작업자 역할을 한다. 이 모듈은 언어적 이해보다는 시각-운동 협응(Visuomotor Coordination)에 최적화되어 있다.</p>
<ul>
<li><strong>입력:</strong> 고수준 정책이 생성한 텍스트 명령(예: “집게로 빵을 잡아라”), 현재 시점의 이미지, 로봇의 관절 상태(Proprioception).</li>
<li><strong>출력:</strong> 미래 <span class="math math-inline">H</span> 단계(Horizon)의 행동 궤적(Action Chunk). 일반적으로 50스텝 정도를 한 번에 예측한다.3</li>
<li><strong>액션 청킹(Action Chunking)과 플로우 매칭(Flow Matching):</strong></li>
<li>로봇 제어는 고빈도(예: 10Hz~50Hz)로 이루어져야 하므로, 매 스텝마다 무거운 모델을 추론하는 것은 비효율적이다. 따라서 한 번의 추론으로 일정 구간(Chunk)의 행동을 미리 생성해두고 이를 순차적으로 실행한다.</li>
<li>이때 행동 생성 알고리즘으로 <strong>플로우 매칭</strong>을 사용한다. 플로우 매칭은 연속적인 행동 공간상에서 최적의 경로를 생성하는 벡터 필드를 학습한다. 이는 확산 모델의 장점(다중 모드 분포 표현)을 유지하면서도, 샘플링 속도가 빠르고 궤적이 더 매끄럽다는 장점이 있다.</li>
<li>저수준 정책은 언어 명령에 조건부(Conditional)로 동작하므로, 동일한 시각 입력이라도 “집어라“는 명령과 “밀어라“는 명령에 따라 전혀 다른 행동 궤적을 생성한다.</li>
</ul>
<h3>3.4  계층 간 인터페이스의 중요성</h3>
<p>두 정책 사이를 연결하는 인터페이스가 ’자연어’라는 점은 시스템의 디버깅과 해석 가능성(Explainability) 측면에서 큰 이점을 제공한다. 만약 로봇이 엉뚱한 행동을 했을 때, 개발자는 고수준 정책이 잘못된 명령을 내린 것인지(“쓰레기를 집어라“라고 해야 하는데 “컵을 집어라“라고 함), 아니면 저수준 정책이 명령을 제대로 수행하지 못한 것인지(명령은 맞았으나 그리퍼가 미끄러짐)를 텍스트 로그만 보고도 즉시 파악할 수 있다.3 이는 블랙박스 형태의 단일 모델에서는 불가능한 진단 능력이다.</p>
<h2>4.  합성 데이터 생성 파이프라인: 로봇 지능의 연료</h2>
<h3>4.1  데이터의 병목과 해결책</h3>
<p>로봇 학습의 가장 큰 걸림돌은 양질의 데이터 부족이다. 텍스트나 이미지는 웹 크롤링을 통해 수십억 개를 얻을 수 있지만, 로봇 행동 데이터는 실제 물리적 시간과 장비가 소요되므로 확장이 매우 어렵다. 특히 “상황에 따른 대화“나 “수정 명령에 대한 반응“과 같은 복잡한 상호작용 데이터는 자연적으로 발생하기 드물어 수집하기가 더욱 난해하다.</p>
<p>하이 로봇 연구진은 이를 타개하기 위해 <strong>VLM 기반의 합성 데이터 생성(Synthetic Data Generation)</strong> 전략을 채택했다. 이는 물리적인 행동 데이터는 실제 로봇에서 얻되, 그에 수반되는 언어적 상황과 시나리오는 인공지능이 생성해내는 하이브리드 방식이다.5</p>
<h3>4.2  4단계 데이터 생성 프로세스 상세</h3>
<p>데이터 파이프라인은 다음 4단계로 정교하게 설계되었다.5</p>
<ol>
<li><strong>로봇 데이터 수집 (Robot Data Collection):</strong></li>
</ol>
<ul>
<li>인간 작업자가 VR 기기나 컨트롤러를 이용해 원격 조종(Teleoperation)으로 로봇을 조작한다.</li>
<li>이 단계에서는 복잡한 시나리오를 연출할 필요 없이, 단순히 다양한 물체를 집고, 옮기고, 놓는 기본적인 동작(Primitive Skills)들을 최대한 다양하게 수집하는 데 집중한다.</li>
<li>데이터: 이미지 시퀀스, 로봇 관절 상태, 그리퍼 동작 등.</li>
</ul>
<ol start="2">
<li><strong>인간 주석 및 분할 (Human Annotation &amp; Segmentation):</strong></li>
</ol>
<ul>
<li>수집된 긴 에피소드를 의미 있는 최소 단위인 ’원자적 기술(Atomic Skill)’로 분할한다. (예: ‘접근하기’, ‘집기’, ‘들어올리기’, ‘이동하기’, ‘놓기’)</li>
<li>각 구간에 대해 인간이 정확한 텍스트 라벨을 붙인다. (예: “빨간 사과를 집어라”). 이 데이터는 저수준 정책의 정확성을 보장하는 기초 진실(Ground Truth)이 된다.</li>
</ul>
<ol start="3">
<li><strong>합성 데이터 생성 (Synthetic Data Generation):</strong></li>
</ol>
<ul>
<li><strong>핵심 혁신 단계</strong>이다. 고성능 VLM(Data-generator VLM)에게 주석 처리된 기술의 시퀀스와 이미지를 보여주고, “이 상황에서 사용자가 로봇에게 어떤 복잡한 명령을 내렸을지, 그리고 로봇은 어떻게 반응했을지 상상해 보라“는 프롬프트를 입력한다.</li>
<li><strong>다양한 시나리오 생성:</strong></li>
<li><strong>추상적 명령:</strong> “사과를 집어라” <span class="math math-inline">\rightarrow</span> “비타민 섭취가 필요해, 과일 좀 줄래?”</li>
<li><strong>조건부 명령:</strong> “초록색 사과는 덜 익었으니 빨간 것만 골라줘.”</li>
<li><strong>부정적 피드백:</strong> (로봇이 사과를 집으려 할 때) “아니, 지금은 배가 안 고파. 그냥 치워줘.”</li>
<li><strong>수정 명령:</strong> “아, 마음이 바뀌었어. 사과 말고 배로 줘.”</li>
<li>VLM은 시각적 문맥(Visual Context)과 상식(World Knowledge)을 동원하여 수만 가지의 그럴듯한 대화 시나리오를 생성한다. 이는 실제로는 한 번도 일어나지 않았지만, 충분히 일어날 법한 가상의 상호작용 데이터이다.</li>
</ul>
<ol start="4">
<li><strong>정책 학습 (Policy Training):</strong></li>
</ol>
<ul>
<li><strong>고수준 정책 학습:</strong> 생성된 합성 대화 데이터와 이미지 시퀀스를 입력으로 하여, 사용자 명령에 대해 적절한 하위 기술 명령(Sub-skill Instruction)을 생성하도록 학습한다. 합성 데이터를 통해 모델은 무수히 많은 언어적 변형과 논리적 패턴을 익히게 된다.</li>
<li><strong>저수준 정책 학습:</strong> 인간이 라벨링한 원본 데이터(이미지, 텍스트, 행동)를 사용하여, 특정 명령이 주어졌을 때 정확한 물리적 동작을 수행하도록 플로우 매칭 목적함수(Flow Matching Objective)로 학습한다.</li>
</ul>
<h3>4.3  합성 데이터의 전략적 가치</h3>
<p>이러한 방식은 두 가지 결정적인 이점을 제공한다.</p>
<p>첫째, **데이터의 확장성(Scalability)**이다. 물리적 실험을 추가로 하지 않고도, 컴퓨팅 자원만으로 학습 데이터를 무한히 증강할 수 있다.</p>
<p>둘째, 희귀 상황(Corner Case) 학습이다. 실제 환경에서는 사용자가 로봇에게 화를 내거나 명령을 번복하는 일이 드물지만, 안전하고 유연한 로봇을 위해서는 이러한 상황에 대한 학습이 필수적이다. 합성 데이터는 이러한 부정적 예시(Negative Examples)와 예외 상황을 인위적으로 생성하여 모델의 강건성(Robustness)을 극대화한다.5</p>
<h2>5.  실험적 검증: 성능과 한계의 경계</h2>
<p>연구진은 하이 로봇의 성능을 입증하기 위해 단일 암, 양팔, 양팔 모바일 로봇 등 3가지 하드웨어 플랫폼에서 3가지의 고난도 태스크를 수행하는 대규모 실험을 진행하였다.11</p>
<h3>5.1  태스크 정의 및 난이도 분석</h3>
<h4>5.1.1  테이블 정리 (Table Bussing)</h4>
<ul>
<li><strong>개요:</strong> 어지러진 테이블 위에 놓인 다양한 물체들(음료 캔, 과자 봉지, 컵, 접시, 과일 등)을 정리하는 작업이다.</li>
<li><strong>복잡성:</strong> 단순히 모든 것을 치우는 것이 아니라, “쓰레기만 버려라(Clean up only trash)”, “플라스틱은 재활용 통에 넣고 음식물은 남겨라“와 같은 조건부 명령을 수행해야 한다. 이는 로봇이 물체의 속성(쓰레기 vs 식기)을 시각적으로 분류하고 의미론적으로 이해해야 함을 뜻한다. 또한 작업 도중 사용자가 “그건 아직 마시는 중이야!“라고 개입할 때 즉시 행동을 멈추고 다른 물체로 목표를 변경해야 하는 순발력이 요구된다.8</li>
</ul>
<h4>5.1.2  샌드위치 만들기 (Sandwich Making)</h4>
<ul>
<li><strong>개요:</strong> 빵, 햄, 치즈, 양상추, 토마토 등 여러 식재료를 조합하여 사용자가 원하는 샌드위치를 제조한다.</li>
<li><strong>복잡성:</strong> 식재료는 형태가 변형되기 쉬운(Deformable) 물체이므로 정교한 힘 제어와 파지 전략이 필요하다. 더 중요한 것은 ’순서(Sequencing)’와 ’조합(Composition)’이다. “채식 샌드위치(Vegetarian Sandwich)“라는 명령을 받으면 로봇은 햄을 제외해야 한다는 것을 추론해야 한다. “빵 위에 치즈, 그 위에 햄“과 같은 순차적 조립 과정을 기억하고 수행해야 하며, 중간에 “토마토는 빼줘“라는 수정 명령이 들어오면 이미 계획된 동작을 취소해야 한다.4</li>
</ul>
<h4>5.1.3  장보기 (Grocery Shopping)</h4>
<ul>
<li><strong>개요:</strong> 모바일 로봇이 상점 환경을 돌아다니며 선반에 진열된 물품 중 사용자가 요청한 것을 찾아 바구니에 담는다.</li>
<li><strong>복잡성:</strong> 이동(Navigation)과 조작(Manipulation)이 결합된 전신 제어(Whole-body Control)가 필요하다. 또한 “단 것 좀 사다 줘(Get me something sweet)“와 같은 매우 추상적인 명령을 처리해야 한다. 로봇은 초콜릿, 사탕, 과일 등을 ’단 것’으로 범주화하여 인식하고 탐색해야 한다. 시각적으로 유사한 물체들 사이에서 정확한 품목을 구별하는 세밀한 인식 능력도 필수적이다.6</li>
</ul>
<h3>5.2  비교 실험 및 베이스라인</h3>
<p>성능 평가는 **명령 수행 정확도(Instruction Accuracy, IA)**와 <strong>작업 진척도(Task Progress, TP)</strong> 두 가지 척도로 이루어졌다. 비교 대상 모델(Baseline)은 다음과 같다.</p>
<ol>
<li><strong>Flat VLA (RT-2 스타일):</strong> 계층적 구조 없이 단일 VLA 모델이 이미지와 텍스트를 받아 직접 행동을 생성한다. <span class="math math-inline">\pi_0</span> 아키텍처를 사용하되 고수준 정책이 없다.</li>
<li><strong>GPT-4o High-Level:</strong> 고수준 정책으로 현재 가장 강력한 상용 모델인 GPT-4o를 사용하고, 저수준 정책은 하이 로봇과 동일한 것을 사용한다. GPT-4o는 로봇 데이터로 미세 조정(Fine-tuning)되지 않고 프롬프트만으로 제어된다.6</li>
<li><strong>Expert Human (Oracle):</strong> 사람이 직접 원격 조종한 결과로, 성능의 상한선(Upper Bound)을 의미한다.</li>
</ol>
<h3>5.3  실험 결과 분석</h3>
<h4>5.3.1  정량적 성능 평가</h4>
<table><thead><tr><th><strong>모델 (Model)</strong></th><th><strong>명령 수행 정확도 (IA)</strong></th><th><strong>작업 진척도 (TP)</strong></th><th><strong>분석</strong></th></tr></thead><tbody>
<tr><td><strong>Flat VLA</strong></td><td>36%</td><td>61%</td><td>단순 명령만 수행 가능, 복잡한 추론 실패</td></tr>
<tr><td><strong>GPT-4o High-Level</strong></td><td>35%</td><td>63%</td><td>언어 이해는 좋으나 물리적 그라운딩(Grounding) 실패</td></tr>
<tr><td><strong>Hi Robot</strong></td><td><strong>74%</strong></td><td><strong>77%</strong></td><td><strong>압도적인 성능 향상, 베이스라인 대비 2배 이상</strong></td></tr>
<tr><td><strong>Expert Human</strong></td><td>100%</td><td>100%</td><td>-</td></tr>
</tbody></table>
<ul>
<li><strong>Flat VLA의 실패:</strong> 36%라는 낮은 정확도는 단일 모델이 복잡한 논리와 정밀 제어를 동시에 학습하는 것이 얼마나 어려운지 보여준다. 모델 용량의 한계로 인해 시각적 특징 추출과 언어적 추론 사이에서 경합이 발생하며, 결국 두 가지 모두 어중간한 성능을 보이게 된다.</li>
<li><strong>GPT-4o의 역설:</strong> 흥미로운 점은 현존 최고의 LLM인 GPT-4o조차 35%의 낮은 정확도를 기록했다는 것이다. GPT-4o는 “채식 샌드위치“의 의미는 완벽히 알지만, 로봇의 카메라에 비친 물체 중 어떤 것이 ’햄’이고 어떤 것이 ’치즈’인지 정확히 매핑(Grounding)하지 못했다. 또한 로봇 손이 이미 물건을 들고 있는데 또 다른 물건을 집으라고 지시하는 등, 로봇의 물리적 상태(Affordance)에 대한 이해가 부족했다.6 이는 범용 LLM을 로봇에 그대로 적용하는 것의 한계를 명확히 보여준다.</li>
<li><strong>Hi Robot의 우위:</strong> 하이 로봇은 74%의 정확도로 타 모델을 압도했다. 이는 로봇 데이터로 학습된 고수준 정책이 물리적 현실과 언어적 의미를 성공적으로 연결했음을 의미한다.</li>
</ul>
<h4>5.3.2  정성적 분석: 피드백 대응 사례</h4>
<p>하이 로봇의 진가는 실시간 피드백 상황에서 드러났다.</p>
<ul>
<li><strong>상황:</strong> 샌드위치를 만들던 중 사용자가 “아, 나 다이어트 중이야. 치즈는 빼줘.“라고 말함.</li>
<li><strong>Flat VLA:</strong> 문맥을 이해하지 못하고 계속 치즈를 집어넣거나, 멈칫거리다 동작이 꼬임.</li>
<li><strong>GPT-4o:</strong> “알겠습니다.“라고 대답은 하지만, 이미 큐에 쌓인 이전 명령을 취소하지 못하거나 시각적 피드백이 늦어 치즈를 넣어버림.</li>
<li><strong>Hi Robot:</strong> 고수준 정책이 즉시 새로운 하위 명령(“치즈를 내려놓아라”, “양상추를 집어라”)을 생성하고, 저수준 정책이 이를 실행하여 유연하게 대처함.6</li>
</ul>
<h3>5.4  절제 연구 (Ablation Study): 구성 요소의 기여도</h3>
<p>연구진은 하이 로봇의 성능이 어디서 기인했는지 확인하기 위해 절제 연구를 수행했다.</p>
<ol>
<li><strong>합성 데이터 제거:</strong> 고수준 정책을 인간 라벨 데이터로만 학습시켰을 때, 정확도는 약 46%p 급락했다. 이는 로봇이 다양한 언어 표현을 이해하고 낯선 상황에 대처하는 능력이 전적으로 합성 데이터에서 왔음을 시사한다.5</li>
<li><strong>계층 구조 제거:</strong> 동일한 양의 데이터(합성 데이터 포함)를 사용하여 Flat 모델을 학습시켰을 때도 성능이 크게 떨어졌다. 이는 단순히 데이터의 양 문제가 아니라, ’추론’과 ’실행’을 구조적으로 분리하는 것이 학습 효율과 성능에 결정적임을 증명한다.</li>
</ol>
<h2>6.  논의 및 향후 전망</h2>
<h3>6.1  로봇 지능의 ‘시스템 2’ 구축의 의의</h3>
<p>하이 로봇은 로봇 공학에 인지 심리학적 모델을 성공적으로 이식한 사례이다. 기존의 로봇이 외부에서 주입된 명령을 맹목적으로 수행하는 ’수동적 기계’였다면, 하이 로봇은 스스로 상황을 언어적으로 해석하고 계획을 수정하는 ’능동적 에이전트’의 모습을 보여주었다. 내면의 목소리(Inner Voice)를 통해 자신의 사고 과정을 드러내는 방식은 인간-로봇 상호작용(HRI)의 투명성과 신뢰성을 높이는 데 크게 기여할 것이다.</p>
<h3>6.2  합성 데이터: 로봇 학습의 게임 체인저</h3>
<p>본 연구는 로봇 분야에서 합성 데이터의 효용성을 강력하게 입증했다. 물리적 시뮬레이션(Sim-to-Real)을 넘어, 의미론적 시뮬레이션(Semantic Simulation)이 가능해짐에 따라 로봇 학습 데이터의 병목 현상이 상당 부분 해소될 것으로 기대된다. 앞으로는 더 정교한 VLM을 이용해 로봇의 사회적 상호작용, 윤리적 판단, 안전 수칙 준수 등을 학습시키는 방향으로 연구가 확장될 것이다.</p>
<h3>6.3  플로우 매칭을 통한 동작 생성의 고도화</h3>
<p>저수준 정책에 적용된 플로우 매칭 기술은 로봇 동작의 질을 한 단계 높였다. 춤을 추거나, 스포츠를 하거나, 요리를 하는 등 섬세하고 유려한 동작이 필요한 분야에서 플로우 매칭 기반의 VLA 모델이 표준으로 자리 잡을 가능성이 높다. 연속적인 시간과 공간을 다루는 이 기술은 로봇이 더욱 생물학적인 움직임을 모사할 수 있게 해준다.</p>
<h3>6.4  남은 과제와 미래</h3>
<p>물론 하이 로봇도 완벽하지 않다.</p>
<ul>
<li><strong>속도 문제:</strong> 두 개의 거대 모델을 순차적으로 실행해야 하므로 추론 지연(Latency)이 발생할 수 있다. 실시간성을 더욱 높이기 위해 모델 경량화(Distillation)나 병렬 처리 기술이 필요하다.</li>
<li><strong>장기 기억(Long-term Memory):</strong> 현재 시스템은 작업 수행 중의 단기 기억은 훌륭하지만, 사용자의 오랜 습관이나 며칠 전의 대화를 기억하는 장기 기억 능력은 부족하다. 진정한 반려 로봇이 되기 위해서는 에피소드 기억(Episodic Memory) 시스템의 통합이 필요하다.</li>
<li><strong>안전성(Safety):</strong> 합성 데이터가 생성한 시나리오에 편향(Bias)이나 오류(Hallucination)가 포함될 경우, 로봇이 예상치 못한 위험한 행동을 할 수 있다. 실세계 배포를 위해서는 더욱 엄격한 검증(Verification) 절차가 마련되어야 한다.</li>
</ul>
<h2>7.  결론</h2>
<p>“하이 로봇“은 시각-언어-행동 모델의 계층적 결합을 통해 개방형 명령 수행이라는 로봇 공학의 난제에 의미 있는 해법을 제시하였다. 인간의 사고 과정을 모사한 ‘시스템 1 &amp; 2’ 아키텍처, 데이터의 한계를 뛰어넘는 ‘합성 데이터 파이프라인’, 그리고 정교한 제어를 위한 ‘플로우 매칭’ 기술의 융합은 로봇 지능을 한 차원 높은 단계로 끌어올렸다.</p>
<p>실험 결과는 이 시스템이 단순히 명령을 따르는 것을 넘어, 사용자의 의도를 파악하고, 실수를 교정하며, 환경과 상호작용하는 능력을 갖추었음을 보여준다. 비록 아직 해결해야 할 과제들이 남아있지만, 하이 로봇이 보여준 기술적 성취는 미래의 범용 로봇이 우리의 가정과 일상에 스며드는 시기를 앞당기는 데 결정적인 기여를 할 것으로 평가된다. 로봇이 인간의 언어를 이해하고, 인간처럼 사고하며, 인간을 위해 봉사하는 시대가 머지않았음을 이 연구는 시사하고 있다.</p>
<p><strong>키워드:</strong> 하이 로봇(Hi Robot), 시각-언어-행동 모델(VLA), 계층적 강화학습, 플로우 매칭(Flow Matching), 합성 데이터(Synthetic Data), 로봇 공학(Robotics), 인공지능(AI), 시스템 1과 2(System 1 &amp; 2).</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models - Physical Intelligence, https://www.physicalintelligence.company/download/hirobot.pdf</li>
<li>12월 7, 2025에 액세스, [https://www.alphaxiv.org/overview/2502.19417v2#:<sub>:text=Brohan%2C%20A.%2C%20Brown%2C,preprint%20arXiv%3A2307.15818%2C%202023a.](https://www.alphaxiv.org/overview/2502.19417v2#:</sub>:text=Brohan%2C A.%2C Brown%2C, <a href="https://www.alphaxiv.org/overview/2502.19417v2#:~:text=Brohan%2C%20A.%2C%20Brown%2C,preprint%20arXiv%3A2307.15818%2C%202023a.">https://www.alphaxiv.org/overview/2502.19417v2#:~:text=Brohan%2C%20A.%2C%20Brown%2C,preprint%20arXiv%3A2307.15818%2C%202023a.</a></li>
<li>Foundation Models for Robotics: Vision-Language-Action (VLA) | Rohit Bandaru, https://rohitbandaru.github.io/blog/Foundation-Models-for-Robotics-VLA/</li>
<li>Hi Robot, https://hi-robot-vla.github.io/</li>
<li>Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models | alphaXiv, https://www.alphaxiv.org/overview/2502.19417v2</li>
<li>Teaching Robots to Listen and Think Harder - Physical Intelligence, https://www.physicalintelligence.company/research/hirobot</li>
<li>To assess transfer between embodiments, we evaluate the RT-2-X model on… - ResearchGate, https://www.researchgate.net/figure/To-assess-transfer-between-embodiments-we-evaluate-the-RT-2-X-model-on_fig3_382982173</li>
<li>Hi Robot: Open-Ended Instruction Following With Hierarchical Vision-Language-Action Models - Scribd, https://www.scribd.com/document/950169891/2502-19417v1-2</li>
<li>A VLA with Open-World Generalization - Physical Intelligence, https://www.physicalintelligence.company/blog/pi05</li>
<li>Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models - arXiv, https://arxiv.org/html/2502.19417v1</li>
<li>[2502.19417] Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models - arXiv, https://arxiv.org/abs/2502.19417</li>
<li>Hi Robot: Open-Ended Instruction Following with Hierarchical Vision, https://www.alphaxiv.org/zh/overview/2502.19417v1</li>
<li>Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models | OpenReview, <a href="https://openreview.net/forum?id=lNVHg9npif&amp;referrer=%5Bthe+profile+of+Karl+Pertsch%5D(/profile?id%3D~Karl_Pertsch1)">https://openreview.net/forum?id=lNVHg9npif&amp;referrer=%5Bthe%20profile%20of%20Karl%20Pertsch%5D(%2Fprofile%3Fid%3D~Karl_Pertsch1)</a></li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>