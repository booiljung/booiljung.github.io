<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:OpenVLA-OFT (Optimized Fine-Tuning)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>OpenVLA-OFT (Optimized Fine-Tuning)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">Vision-Language-Action(VLA) 모델</a> / <span>OpenVLA-OFT (Optimized Fine-Tuning)</span></nav>
                </div>
            </header>
            <article>
                <h1>OpenVLA-OFT (Optimized Fine-Tuning)</h1>
<p>2025-12-07, G30DR</p>
<h2>1.  서론: 구현된 인공지능(Embodied AI)과 VLA 모델의 패러다임 전환</h2>
<p>인공지능 연구의 최전선은 이제 디지털 세계의 텍스트와 이미지를 처리하는 단계를 넘어, 물리적 세계와 직접 상호작용하는 ’구현된 인공지능(Embodied AI)’으로 빠르게 확장되고 있다. 이러한 흐름의 중심에는 거대 언어 모델(LLM)의 추론 능력과 컴퓨터 비전의 인식 능력을 로봇의 행동 제어와 결합하려는 시도, 즉 ‘시각-언어-행동(Vision-Language-Action, VLA)’ 모델이 자리 잡고 있다. Google DeepMind의 RT-1, RT-2, 그리고 최근의 OpenVLA와 같은 모델들은 인터넷 규모의 데이터로 사전 학습된 비전-언어 모델(VLM)을 기반으로 하여, 로봇이 이전에 보지 못한 환경이나 물체에 대해서도 자연어 명령을 통해 작업을 수행할 수 있는 일반화(Generalization) 능력을 보여주었다.1</p>
<p>그러나 이러한 VLA 모델들은 로봇 공학의 본질적인 제약 조건인 ’실시간성(Real-time)’과 ’정밀성(Precision)’이라는 거대한 장벽에 직면해 있다. 텍스트 생성 모델에서 차용된 ‘다음 토큰 예측(Next-token prediction)’ 방식은 로봇의 연속적이고 다차원적인 관절 움직임을 이산적인(Discrete) 토큰의 나열로 변환하여 처리한다. 이 방식은 의미론적 추론에는 유리할지 모르나, 초당 수십 회 이상의 제어 신호를 생성해야 하는 고주파수(High-frequency) 로봇 제어에는 치명적인 병목 현상을 야기한다. 또한, 물리적 공간의 연속성을 256개의 구간으로 강제 이산화(Discretization)하는 과정에서 발생하는 정보 손실은 정밀한 조작 작업의 실패로 이어진다.3</p>
<p>이러한 맥락에서 등장한 <strong>OpenVLA-OFT</strong>는 기존 VLA 모델의 구조적 한계를 극복하기 위한 파인튜닝(Fine-Tuning)의 새로운 표준을 제시한다. 여기서 ’OFT’는 문헌에 따라 ’Orthogonal Finetuning(직교 파인튜닝)’이라는 파라미터 효율적 학습 기법(PEFT)과 혼동될 수 있으나, 본 보고서가 심층 분석할 OpenVLA-OFT의 핵심은 <strong>“Optimized Fine-Tuning(최적화된 파인튜닝)”</strong> 레시피에 있다.5 이 레시피는 병렬 디코딩(Parallel Decoding), 행동 청킹(Action Chunking), 연속적 행동 표현(Continuous Action Representation), 그리고 L1 회귀 목표(L1 Regression Objective)라는 네 가지 기술적 기둥을 바탕으로, VLA 모델의 추론 속도를 획기적으로 가속화하고 작업 성공률을 비약적으로 향상시킨다.6</p>
<p>본 보고서는 OpenVLA-OFT의 아키텍처와 기술적 메커니즘을 전례 없는 수준으로 상세하게 분석한다. 기초가 되는 트랜스포머 아키텍처와 VLM의 원리에서부터 시작하여, OpenVLA 베이스 모델의 구조, 그리고 OFT 레시피가 적용되는 구체적인 수식과 원리를 파헤친다. 나아가, 머신러닝 학계에서 논의되는 ’직교 파인튜닝(Orthogonal Finetuning)’의 수학적 원리를 별도로 상세히 기술하여 용어의 중의성으로 인한 혼란을 해소하고, 잠재적인 기술적 연관성을 고찰한다. 마지막으로 LIBERO 시뮬레이션과 ALOHA 로봇 하드웨어에서의 실험 결과를 통해 이 기술이 로봇 학습의 미래에 던지는 시사점을 도출한다.</p>
<h2>2.  기술적 배경: 트랜스포머에서 VLA까지</h2>
<h3>2.1  트랜스포머 아키텍처와 어텐션 메커니즘의 로봇 적용</h3>
<p>현대 VLA 모델의 근간은 트랜스포머(Transformer) 아키텍처이다. 트랜스포머의 핵심인 ‘자기 주의(Self-Attention)’ 메커니즘은 입력 시퀀스 내의 모든 요소가 서로 어떻게 연관되어 있는지를 계산하여 문맥(Context)을 파악한다. 로봇 제어에서 이는 시각적 관측(Observation)의 특정 부분이 사용자의 언어 명령(Instruction)과 어떻게 연결되는지, 그리고 과거의 행동 이력(History)이 현재의 결정에 어떤 영향을 미치는지를 모델링하는 데 사용된다.</p>
<p>OpenVLA는 이러한 트랜스포머 구조를 활용하여 시각 토큰과 언어 토큰, 그리고 행동 토큰을 하나의 통합된 시퀀스로 처리한다. 이는 멀티모달 데이터를 공통된 임베딩 공간(Embedding Space)으로 투영함으로써 가능해진다. 그러나 텍스트 데이터와 달리 로봇의 상태 정보(Proprioception)와 행동(Action)은 물리적 단위를 가지는 연속적인 값이다. 이를 트랜스포머가 처리할 수 있는 형태로 변환하는 과정이 VLA 설계의 핵심 과제 중 하나이다.7</p>
<h3>2.2  시각-언어 모델(VLM)의 구성: Prismatic, SigLIP, DINOv2</h3>
<p>OpenVLA는 바닥부터 새로 학습된 모델이 아니라, 강력한 사전 학습된 모델들을 조합하여 구축되었다. 이를 이해하기 위해서는 구성 요소들을 살펴볼 필요가 있다.</p>
<ol>
<li><strong>DINOv2 (Self-supervised Vision Transformer):</strong> DINOv2는 레이블이 없는 대규모 이미지 데이터셋에서 자기지도 학습(Self-supervised Learning)을 통해 학습된 비전 인코더다. 이 모델은 물체의 형태, 깊이, 부분적 특징 등 기하학적이고 구조적인 정보를 포착하는 데 탁월한 성능을 보인다. 로봇이 물체를 조작하기 위해서는 물체의 정확한 위치와 형상을 파악해야 하므로 DINOv2의 특징 추출 능력은 필수적이다.9</li>
<li><strong>SigLIP (Sigmoid Loss for Language Image Pre-training):</strong> SigLIP은 이미지와 텍스트 쌍 데이터를 학습하여 시각 정보와 언어 정보 간의 의미론적 정렬(Alignment)을 수행한다. 사용자가 “파란색 컵을 집어라“라고 명령했을 때, 이미지 내에서 ’파란색 컵’에 해당하는 영역을 식별하는 능력은 SigLIP에서 비롯된다. 기존의 CLIP 모델 대비 Sigmoid 손실 함수를 사용하여 학습 효율과 성능을 개선한 것이 특징이다.2</li>
<li><strong>Llama 2 (Large Language Model):</strong> OpenVLA의 두뇌에 해당하는 부분으로, 70억(7B) 파라미터 규모의 Llama 2 모델이 사용된다. 비전 인코더에서 추출된 시각적 특징들은 프로젝터(Projector)를 거쳐 Llama 2의 입력 토큰으로 변환된다. Llama 2는 방대한 텍스트 데이터로 학습된 추론 능력을 바탕으로, 복잡한 명령을 이해하고 논리적인 작업 순서를 계획(Planning)하는 역할을 수행한다.9</li>
</ol>
<p>이러한 구성 요소들이 결합된 ‘Prismatic’ VLM 백본은 OpenVLA가 시각적 상황 인식과 언어적 추론을 동시에 수행할 수 있는 강력한 기반을 제공한다.</p>
<h2>3.  OpenVLA 베이스 모델의 구조와 한계</h2>
<h3>3.1  아키텍처 개요 및 학습 데이터</h3>
<p>OpenVLA는 Prismatic VLM 백본을 기반으로, ‘Open X-Embodiment’ 데이터셋을 사용하여 파인튜닝된 모델이다. Open X-Embodiment는 전 세계 20개 이상의 연구 기관에서 수집한 97만 개 이상의 로봇 조작 궤적(Trajectory)을 포함하는 대규모 데이터셋이다.1 이 데이터셋은 다양한 로봇 형태(Embodiment)와 환경, 작업을 포괄하고 있어, 모델이 특정 로봇이나 환경에 과적합(Overfitting)되지 않고 범용적인 조작 능력을 학습하는 데 기여한다.</p>
<p>OpenVLA의 학습 방식은 LLM의 표준 학습 방식인 ’다음 토큰 예측’을 따른다. 로봇의 행동 공간(Action Space)은 7자유도 팔의 관절 각도 또는 엔드 이펙터(End-effector)의 위치 및 회전(6차원)과 그리퍼 개폐(1차원)를 포함하여 총 7차원으로 구성된다. 베이스 모델은 각 차원의 값을 256개의 구간으로 이산화(Discretization)하여 총 256개의 가능한 토큰 중 하나를 예측하도록 학습된다.7</p>
<h3>3.2  자기회귀적(Autoregressive) 행동 생성의 병목</h3>
<p>베이스 OpenVLA 모델의 가장 큰 특징이자 한계는 행동 생성 방식에 있다. 모델은 시각 및 언어 토큰을 입력받은 후, 첫 번째 행동 차원(예: x축 이동)에 대한 토큰을 생성하고, 이를 다시 입력으로 사용하여 두 번째 행동 차원(예: y축 이동)을 생성하는 식으로 순차적으로 작동한다. 7차원 행동을 생성하기 위해서는 모델을 7번 실행(Forward Pass)해야 한다.</p>
<p>이는 두 가지 심각한 문제를 초래한다.</p>
<ol>
<li><strong>추론 지연(Inference Latency):</strong> 70억 파라미터 모델을 7번 실행하는 것은 상당한 연산 시간을 소요한다. 로봇 제어 주기가 보통 10Hz~50Hz(초당 10~50회)를 요구한다는 점을 고려할 때, 이러한 지연은 로봇의 반응 속도를 느리게 하고 동적인 환경 변화에 대처하지 못하게 만든다. 실제로 베이스 OpenVLA의 제어 주파수는 수 Hz 수준에 머무르는 경우가 많다.3</li>
<li><strong>오류 누적(Error Propagation):</strong> 앞선 차원의 예측이 잘못될 경우, 뒤따르는 차원의 예측에도 악영향을 미쳐 전체 행동이 엉뚱하게 생성될 수 있다.</li>
</ol>
<h3>3.3  이산화(Discretization)에 따른 정밀도 저하</h3>
<p>연속적인 실수(Real number) 값을 256개의 정수로 변환하는 과정에서 필연적으로 해상도(Resolution)의 손실이 발생한다. 예를 들어, 로봇 팔이 0.1mm 단위로 움직여야 하는 정밀 조립 작업에서 이산화된 토큰은 1mm 단위의 움직임밖에 표현하지 못할 수 있다. 이는 “흔들림(Jittering)” 현상을 유발하거나 미세한 조작 실패의 원인이 된다.5</p>
<h2>4.  OpenVLA-OFT: 최적화된 파인튜닝(Optimized Fine-Tuning) 레시피 심층 분석</h2>
<p>OpenVLA-OFT는 앞서 언급한 베이스 모델의 한계를 극복하기 위해 제안된 포괄적인 파인튜닝 전략이다. 이 전략은 단순히 하이퍼파라미터를 조정하는 수준을 넘어, 모델의 입출력 구조와 손실 함수를 재설계하는 근본적인 변화를 포함한다.</p>
<h3>4.1  병렬 디코딩(Parallel Decoding)을 통한 추론 가속화</h3>
<p>OpenVLA-OFT의 가장 혁신적인 변화는 자기회귀적 순차 생성을 **병렬 생성(Parallel Generation)**으로 전환한 것이다.</p>
<ul>
<li><strong>구현 메커니즘:</strong> OpenVLA-OFT는 Llama 2 백본의 마지막 트랜스포머 레이어 위에 다층 퍼셉트론(MLP) 헤드(Action Head)를 부착한다. 이 헤드는 트랜스포머의 마지막 타임스텝의 은닉 상태(Hidden State) 벡터를 입력받아, 필요한 모든 행동 차원(예: 7차원)의 값을 한 번에 출력한다.3</li>
<li><strong>수학적 표현:</strong> 기존 방식이 <span class="math math-inline">P(a_t^i | a_t^{&lt;i}, s_t, l)</span> (여기서 <span class="math math-inline">a_t^i</span>는 시점 <span class="math math-inline">t</span>의 <span class="math math-inline">i</span>번째 행동 차원, <span class="math math-inline">s_t</span>는 상태, <span class="math math-inline">l</span>은 언어)의 조건부 확률을 연쇄적으로 계산했다면, OpenVLA-OFT는 <span class="math math-inline">P(\mathbf{a}_t | s_t, l)</span>를 직접 모델링하여 벡터 <span class="math math-inline">\mathbf{a}_t</span>를 단일 연산으로 도출한다.</li>
<li><strong>성능 향상:</strong> 이 구조적 변화를 통해 모델 호출 횟수가 획기적으로 줄어든다. 실험 결과에 따르면, 병렬 디코딩은 행동 생성 처리량(Throughput)을 기존 대비 26배에서 최대 43배까지 증가시켰다.6 이는 로봇이 25Hz 이상의 고주파수 제어를 수행할 수 있게 함으로써, 인간과 유사한 부드럽고 반응성 높은 움직임을 구현하는 기반이 된다.</li>
</ul>
<h3>4.2  행동 청킹(Action Chunking)과 시간적 일관성</h3>
<p>단일 시점의 행동만을 예측하는 것(Markovian assumption)은 로봇 제어에서 불연속적이고 떨리는 움직임을 유발할 수 있다. 이를 해결하기 위해 OpenVLA-OFT는 <strong>행동 청킹(Action Chunking)</strong> 기법을 도입한다.</p>
<ul>
<li><strong>개념:</strong> 모델은 현재 시점 <span class="math math-inline">t</span>에서의 행동뿐만 아니라, 미래의 일정 구간 <span class="math math-inline">t</span>부터 <span class="math math-inline">t+k</span>까지의 행동 시퀀스(Chunk)를 한 번에 예측한다. 여기서 <span class="math math-inline">k</span>는 청크 크기(Chunk Size)이다.6</li>
<li><strong>시간적 집계(Temporal Aggregation):</strong> 매 타임스텝마다 모델은 <span class="math math-inline">k</span> 길이의 미래 궤적을 예측한다. 실제 로봇 제어 시에는 이들 예측값을 시간축에 따라 평균을 내거나 가중치를 적용하여 부드럽게 연결하는 ‘시간적 집계’ 기술이 사용되기도 한다. OpenVLA-OFT는 추론 효율을 위해 한 번 예측된 청크를 <span class="math math-inline">k</span> 스텝 동안 실행하고, 청크가 끝난 후에 다시 추론하는 방식을 사용하여 연산 부하를 최소화하면서도 부드러운 궤적을 생성한다.12</li>
<li><strong>효과:</strong> 청킹은 로봇의 움직임에 시간적 일관성(Temporal Consistency)을 부여한다. 또한, 한 번의 추론으로 긴 시간 동안의 동작을 결정하므로 실시간 제어의 안정성을 크게 높인다.</li>
</ul>
<h3>4.3  연속적 행동 표현(Continuous Action Representation)</h3>
<p>OpenVLA-OFT는 이산적인 토큰 예측을 버리고, 연속적인 실수 값을 직접 회귀(Regression)하는 방식을 택한다.</p>
<ul>
<li><strong>MLP 헤드의 역할:</strong> 트랜스포머 백본의 출력 벡터는 고차원(예: 4096차원)의 의미 정보를 담고 있다. MLP 헤드는 이를 로봇의 물리적 제어 공간(예: 관절 각도 라디안 값)으로 선형 및 비선형 변환을 통해 매핑한다.</li>
<li><strong>정밀도 회복:</strong> 이산화 단계가 제거됨에 따라, 모델은 이론상 무한한 해상도로 행동을 표현할 수 있게 된다. 이는 바늘에 실을 꿰거나 컵을 쌓는 등 미세한 힘 조절과 위치 제어가 필요한 정교한 작업(Dexterous Manipulation)에서 필수적인 요소다.12</li>
</ul>
<h3>4.4  학습 목표: L1 회귀 vs. 확산(Diffusion)</h3>
<p>행동 생성 모델의 학습 목표(Loss Function) 선택은 성능에 지대한 영향을 미친다. 최근 로봇 학습 분야에서는 ’확산 모델(Diffusion Model)’을 사용하여 행동 분포를 모델링하는 것이 유행하고 있다. 확산 모델은 멀티모달(Multi-modal) 분포(예: 장애물을 왼쪽으로 피할 수도 있고 오른쪽으로 피할 수도 있는 상황)를 표현하는 데 강점이 있다.</p>
<p>그러나 OpenVLA-OFT 연구진은 흥미로운 발견을 제시한다.</p>
<ul>
<li><strong>L1 회귀의 재발견:</strong> 강력한 VLM 백본이 이미 충분한 문맥 정보를 처리하여 상황에 맞는 최적의 행동 모드를 좁혀주기(Mode collapse 방지) 때문에, 행동 헤드(Action Head) 자체는 단순한 <strong>L1 손실(L1 Loss)</strong> 기반의 회귀만으로도 충분히 높은 성능을 낼 수 있다는 것이다.6</li>
<li><strong>확산 모델의 단점:</strong> 확산 모델은 추론 시 수십 번의 노이즈 제거(Denoising) 단계를 거쳐야 하므로 연산 비용이 매우 높다. 반면 L1 회귀는 단 한 번의 연산으로 결과를 도출한다.</li>
<li><strong>결론:</strong> OpenVLA-OFT는 L1 회귀를 채택함으로써, 확산 모델에 버금가는 성능을 유지하면서도 압도적인 추론 속도와 학습 수렴 속도를 확보하였다. 이는 “VLA 모델에서는 백본의 표현력이 행동 생성기의 복잡성을 대체할 수 있다“는 중요한 통찰을 제공한다.</li>
</ul>
<h3>4.5  FiLM을 통한 언어 접지(Language Grounding) 강화 (OFT+)</h3>
<p>기본적인 OFT 레시피에 더해, <strong>FiLM (Feature-wise Linear Modulation)</strong> 기법을 적용한 ‘OpenVLA-OFT+’ 변형은 언어 명령의 영향력을 더욱 강화한다.</p>
<ul>
<li><strong>메커니즘:</strong> FiLM 생성기는 사용자의 언어 명령 임베딩을 입력받아, 비전 인코더나 VLM 백본의 중간 특징 맵(Feature Map)에 적용할 스케일(Scale) 및 시프트(Shift) 파라미터를 생성한다.11 수식으로 표현하면, 특징 맵 <span class="math math-inline">F</span>에 대해 <span class="math math-inline">FiLM(F) = \gamma(l) \cdot F + \beta(l)</span>와 같이 언어 <span class="math math-inline">l</span>에 의존적인 변환을 가한다.</li>
<li><strong>효과:</strong> 이는 모델이 시각 정보 처리 단계에서부터 언어 명령과 관련된 특징(예: “빨간색“이라는 단어가 주어지면 이미지의 빨간색 채널을 강조)에 집중하도록 유도한다. 결과적으로 복잡한 다중 객체 환경에서 특정 물체만을 조작해야 하는 지시 이행(Instruction Following) 능력이 대폭 향상된다.</li>
</ul>
<h2>5.  용어의 명확화 및 비교 기술 분석: Orthogonal Finetuning (PEFT)</h2>
<p>사용자가 입력한 “OpenVLA-OFT“라는 쿼리에서 ’OFT’는 문맥에 따라 다른 기술을 의미할 수 있다. 앞서 설명한 것이 로봇 학습을 위한 ’최적화된 파인튜닝(Optimized Fine-Tuning)’이라면, 머신러닝의 파라미터 효율적 학습(PEFT) 분야에는 **‘Orthogonal Finetuning(직교 파인튜닝)’**이라는 동명의 기술이 존재한다. 본 보고서의 완결성을 위해 이 기술에 대해서도 상세히 분석하고, OpenVLA와의 잠재적 연관성을 짚어본다.</p>
<h3>5.1  Orthogonal Finetuning (OFT)의 원리</h3>
<p>직교 파인튜닝은 사전 학습된 모델의 가중치를 파인튜닝할 때, 가중치 행렬의 구조적 특성을 보존하기 위해 제안된 방법이다.13</p>
<ul>
<li><strong>핵심 아이디어:</strong> 신경망의 가중치 행렬 <span class="math math-inline">W_{pre}</span>에 대해, 파인튜닝된 가중치 <span class="math math-inline">W</span>를 <span class="math math-inline">W = R \cdot W_{pre}</span> 형태로 표현한다. 여기서 <span class="math math-inline">R</span>은 직교 행렬(Orthogonal Matrix, <span class="math math-inline">R^T R = I</span>)이다.</li>
<li><strong>기하학적 해석:</strong> 직교 행렬은 벡터를 회전(Rotation)시키거나 반사(Reflection)시킬 뿐, 벡터의 크기(Norm)나 벡터 간의 상대적인 각도 관계를 변화시키지 않는다. OFT 연구진은 사전 학습된 모델의 지식(Knowledge)이 뉴런 간의 각도 관계(Hyperspherical Energy)에 인코딩되어 있다고 가정한다.13 따라서 직교 변환을 통한 파인튜닝은 기존 지식의 구조를 파괴하지 않으면서(Catastrophic Forgetting 방지) 새로운 도메인에 적응하도록 돕는다.</li>
<li><strong>구현:</strong> 직교 행렬 <span class="math math-inline">R</span>을 학습하기 위해 케일리 변환(Cayley Transform)이나 블록 대각(Block-diagonal) 구조, 버터플라이 분해(Butterfly Factorization) 등의 기법을 사용하여 파라미터 수를 줄이고 연산 효율을 높인다.13</li>
</ul>
<h3>5.2  LoRA vs. PEFT-OFT</h3>
<p>가장 널리 쓰이는 PEFT 기법인 LoRA(Low-Rank Adaptation)는 가중치에 저랭크 행렬을 <strong>더하는</strong> 방식(<span class="math math-inline">W = W_{pre} + BA</span>)이다. 이는 가중치의 방향과 크기를 모두 변화시킬 수 있어 유연하지만, 사전 학습된 특징 공간의 구조를 왜곡할 위험이 있다. 반면 PEFT-OFT는 가중치를 <strong>곱하는</strong> 방식(Multiplicative)으로, 특징 공간의 위상(Topology)을 보존한다는 측면에서 더 강력한 일반화 성능과 안정성을 제공한다고 주장된다.17</p>
<h3>5.3  OpenVLA 맥락에서의 해석</h3>
<p>현재 OpenVLA-OFT(Optimized Fine-Tuning)의 공식 구현체나 논문6은 LoRA를 사용하여 백본을 파인튜닝하고 있다. 즉, 현재의 OpenVLA-OFT는 PEFT 기법으로서의 OFT를 사용하는 것이 아니라, 로봇 제어 파이프라인의 최적화를 의미한다.</p>
<p>그러나 향후 연구 방향으로서, OpenVLA의 백본인 Llama 2를 파인튜닝할 때 LoRA 대신 PEFT-OFT를 적용하는 것은 충분히 고려해볼 만한 시도이다. 특히 로봇 데이터가 적은 상황에서 사전 학습된 상식이나 추론 능력을 잃지 않으면서(Forgetting 없이) 새로운 작업에 적응해야 할 때, 직교 파인튜닝의 특성이 유리하게 작용할 수 있다.</p>
<h2>6.  실험적 평가 및 성과 분석</h2>
<p>OpenVLA-OFT(Optimized Fine-Tuning)의 성능은 시뮬레이션과 실제 로봇 환경 모두에서 검증되었다.</p>
<h3>6.1  LIBERO 시뮬레이션 벤치마크</h3>
<p>LIBERO(Language-Informed Benchmark for Evaluate RObots)는 로봇의 장기 작업 수행 능력과 일반화 능력을 평가하는 표준 벤치마크다. 총 130개의 작업이 포함되어 있으며, Spatial(공간), Object(물체), Goal(목표), Long(장기)의 네 가지 스위트로 구성된다.</p>
<ul>
<li><strong>정량적 성과:</strong> OpenVLA-OFT는 4개 스위트 평균 **97.1%**의 성공률을 기록했다. 이는 기존 OpenVLA 베이스 모델을 LoRA로 파인튜닝한 경우(76.5%)보다 20% 포인트 이상 향상된 결과이며, 기존 최고 성능(SOTA) 모델이었던 <span class="math math-inline">\pi_0</span> (94.2%)마저 능가하는 수치이다.6</li>
<li><strong>분석:</strong> 특히 복잡한 시퀀스가 필요한 ‘Long’ 스위트나 정밀한 위치 제어가 필요한 ‘Spatial’ 스위트에서 성능 향상 폭이 컸다. 이는 행동 청킹과 연속적 행동 표현이 로봇의 제어 정밀도와 일관성을 높이는 데 결정적인 역할을 했음을 시사한다.</li>
</ul>
<table><thead><tr><th><strong>모델 (Model)</strong></th><th><strong>LIBERO 평균 성공률</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>OpenVLA (Base + LoRA)</strong></td><td>76.5%</td><td>자기회귀, 이산 토큰</td></tr>
<tr><td><strong><span class="math math-inline">\pi_0</span> (Pi-Zero)</strong></td><td>94.2%</td><td>이전 SOTA</td></tr>
<tr><td><strong>OpenVLA-OFT</strong></td><td><strong>97.1%</strong></td><td>병렬 디코딩, 연속 제어</td></tr>
</tbody></table>
<h3>6.2  ALOHA 로봇 실증 실험</h3>
<p>시뮬레이션뿐만 아니라 실제 물리 환경에서의 검증도 수행되었다. Bimanual(양팔) 로봇 시스템인 ALOHA를 사용하여 옷 개기, 음식 요리하기 등 정교한 조작(Dexterous Manipulation) 작업을 테스트하였다.</p>
<ul>
<li><strong>실시간 제어 달성:</strong> 기존 VLA 모델들은 느린 추론 속도로 인해 로봇이 뚝뚝 끊기며 움직이는(Stop-and-go) 현상을 보였으나, OpenVLA-OFT는 25Hz 이상의 제어 주기를 안정적으로 유지하며 인간 시연(Demonstration)과 유사한 부드러운 움직임을 구현했다. 25스텝 청킹을 적용했을 때, 추론 처리량은 베이스 모델 대비 <strong>43배</strong> 빨라졌다.19</li>
<li><strong>성공률 비교:</strong> Diffusion Policy나 ACT와 같은 ’처음부터 학습(Train from scratch)’하는 방식들과 비교했을 때, OpenVLA-OFT+는 평균 15% 포인트 더 높은 성공률을 보였다. 이는 인터넷 규모의 데이터로 학습된 VLM의 사전 지식이 로봇 작업 수행에 효과적으로 전이(Transfer)되었음을 증명한다. 특히, “살짝 구겨진 수건을 펴라“와 같이 애매모호한 언어 명령을 이해하고 수행하는 능력에서 VLA 기반 모델의 우위가 확연했다.</li>
</ul>
<h3>6.3  효율성 분석</h3>
<p>OpenVLA-OFT는 컴퓨팅 자원 효율성 측면에서도 주목할 만하다. 거대 모델임에도 불구하고, 양자화(Quantization)와 LoRA, 그리고 병렬 디코딩을 결합하여 소비자용 GPU(RTX 3090, 4090 등) 단 한 장으로도 실시간 추론이 가능하다.5 학습 측면에서도, 전체 모델을 재학습하는(Full Fine-tuning) 대신 LoRA 어댑터와 가벼운 MLP 헤드만을 학습하므로, A100 GPU 1장으로 10~15시간 내에 새로운 작업 도메인에 적응시킬 수 있다.21 이는 로봇 연구의 진입 장벽을 크게 낮추는 결과이다.</p>
<h2>7.  결론 및 향후 전망: 로봇 학습의 새로운 지평</h2>
<p>OpenVLA-OFT는 시각-언어-행동 모델이 실험실의 호기심 대상을 넘어, 실제 산업 현장과 가정의 로봇에 적용될 수 있는 가능성을 구체적으로 보여주었다. 본 보고서의 분석을 통해 도출된 핵심 결론은 다음과 같다.</p>
<p>첫째, <strong>하이브리드 아키텍처의 승리</strong>다. 인지(Cognition)는 이산적인 언어 모델(Transformer)이, 행동(Action)은 연속적인 회귀 모델(MLP/Regression)이 담당하는 구조적 분업이 로봇 제어에 가장 효과적임이 입증되었다. 모든 것을 토큰화하려는 시도는 로봇 공학의 물리적 특성을 간과한 것이었다.</p>
<p>둘째, <strong>속도가 곧 성능</strong>이다. 병렬 디코딩과 행동 청킹을 통한 추론 가속화는 단순히 시간을 단축하는 것을 넘어, 로봇 제어의 주파수를 높여 물리적 상호작용의 안정성을 확보하는 핵심 품질 요소(Critical Quality Factor)임이 확인되었다.</p>
<p>셋째, <strong>단순함의 미학</strong>이다. 복잡한 확산 모델(Diffusion) 없이도, 강력한 사전 학습 백본이 있다면 단순한 L1 회귀만으로 SOTA 성능을 달성할 수 있다는 사실은 모델 설계의 효율성을 재고하게 만든다.</p>
<p>넷째, <strong>용어의 정립</strong>이다. ’OpenVLA-OFT’는 ’Optimized Fine-Tuning’을 의미하며, 이는 VLA의 로봇 특화 파인튜닝 레시피다. 향후에는 이 레시피에 ’Orthogonal Finetuning’과 같은 고급 PEFT 기법이 결합되어, 파라미터 효율성과 학습 안정성을 더욱 높이는 방향으로 진화할 것으로 예상된다.</p>
<p>결론적으로 OpenVLA-OFT는 범용 로봇(Generalist Robot)을 향한 여정에서 중요한 이정표를 세웠다. 공개된 코드와 모델, 그리고 효율적인 파인튜닝 방법론은 전 세계의 로봇 연구자들이 거대 모델의 어깨 위에서 자신만의 로봇을 학습시킬 수 있는 ’로봇 학습의 민주화’를 가속화할 것이다. 이는 머지않은 미래에 우리가 일상에서 마주할 지능형 로봇의 탄생을 앞당기는 촉매제가 될 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>OpenVLA: An Open-Source Vision-Language-Action Model, https://openvla.github.io/</li>
<li>[2406.09246] OpenVLA: An Open-Source Vision-Language-Action Model - arXiv, https://arxiv.org/abs/2406.09246</li>
<li>Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies | OpenReview, https://openreview.net/forum?id=YWeNCMxdhM</li>
<li>CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding - arXiv, https://arxiv.org/html/2506.13725v1</li>
<li>moojink/openvla-oft: Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success - GitHub, https://github.com/moojink/openvla-oft</li>
<li>Fine-Tuning Vision-Language-Action Models: Optimizing Speed and …, https://openvla-oft.github.io/</li>
<li>Vision Language Action Models (VLA) &amp; Policies for Robots - LearnOpenCV, https://learnopencv.com/vision-language-action-models-lerobot-policy/</li>
<li>Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications - IEEE Xplore, https://ieeexplore.ieee.org/iel8/6287639/10820123/11164279.pdf</li>
<li>OpenVLA: An open-source vision-language-action model for robotic manipulation. - GitHub, https://github.com/openvla/openvla</li>
<li>Paper notes: OpenVLA. With the large-scale accessibility of… | by Jay Vakil | Toward Humanoids | Medium, https://medium.com/correll-lab/paper-notes-openvla-17540381187e</li>
<li>Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success - arXiv, https://arxiv.org/html/2502.19645v2</li>
<li>Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success - arXiv, https://arxiv.org/html/2502.19645v1</li>
<li>Orthogonal Finetuning (OFT and BOFT) - Hugging Face, https://huggingface.co/docs/peft/conceptual_guides/oft</li>
<li>OFT - Hugging Face, https://huggingface.co/docs/peft/en/package_reference/oft</li>
<li>Weiyang Liu*, Zeju Qiu*, Yao Feng**, Yuliang Xiu**, Yuxuan Xue**, Longhui Yu**, Haiwen Feng, Zhen Liu, Juyeon Heo, Songyou Peng,, https://wyliu.com/papers/BOFT_slides_v2.pdf</li>
<li>Orthogonal Finetuning Made Scalable - ACL Anthology, https://aclanthology.org/2025.emnlp-main.1627.pdf</li>
<li>Orthogonal Finetuning Made Scalable - Weiyang Liu, https://wyliu.com/papers/oftv2.pdf</li>
<li>EFFICIENT ORTHOGONAL FINE-TUNING WITH PRINCIPAL SUBSPACE ADAPTATION - OpenReview, https://openreview.net/pdf/60e982674d5e22bbfac400b0ac03564556848051.pdf</li>
<li>Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success · Robotics, https://roboticsconference.org/program/papers/17/</li>
<li>OpenVLA finetuning with online RL | Haonan’s blog, https://www.haonanyu.blog/post/openvla_rl/</li>
<li>OpenVLA: An Open-Source Vision-Language-Action Model - arXiv, https://arxiv.org/html/2406.09246v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>