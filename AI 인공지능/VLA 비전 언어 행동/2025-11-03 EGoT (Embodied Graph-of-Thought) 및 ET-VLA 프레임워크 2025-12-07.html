<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:EGoT (Embodied Graph-of-Thought) 및 ET-VLA 프레임워크</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>EGoT (Embodied Graph-of-Thought) 및 ET-VLA 프레임워크</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">Vision-Language-Action(VLA) 모델</a> / <span>EGoT (Embodied Graph-of-Thought) 및 ET-VLA 프레임워크</span></nav>
                </div>
            </header>
            <article>
                <h1>EGoT (Embodied Graph-of-Thought) 및 ET-VLA 프레임워크</h1>
<h2>1.  서론: 로봇 학습의 패러다임 전환과 다중 신체성(Multi-Embodiment)의 난제</h2>
<p>현대 로봇 공학 및 인공지능 분야는 대규모 언어 모델(LLM)의 성공적인 방법론을 물리적 세계로 확장하려는 시도인 비전-언어-행동(Vision-Language-Action, VLA) 모델의 등장으로 새로운 전환점을 맞이했다. RT-2, OpenVLA와 같은 최신 모델들은 인터넷 규모의 방대한 텍스트와 이미지 데이터를 학습하여, 시각적 입력을 해석하고 자연어 명령을 이해하며 이를 구체적인 로봇 제어 명령으로 변환하는 놀라운 능력을 보여주었다.1 이러한 모델들은 기존의 특수 목적형 로봇 제어기를 넘어, 범용적인 작업을 수행할 수 있는 ’Generalist Robot’의 가능성을 제시하고 있다.</p>
<p>그러나 현재의 VLA 모델들은 근본적인 한계에 직면해 있다. 대부분의 학습 데이터와 모델 아키텍처가 단일 로봇 팔(Unimanual) 시스템에 최적화되어 있다는 점이다. Open X-Embodiment (OXE) 데이터셋이나 Droid 데이터셋과 같은 현존하는 대규모 로봇 데이터셋은 단일 로봇 팔의 조작 데이터에 편중되어 있다.1 이로 인해 VLA 모델을 두 개의 팔을 가진 양팔 로봇(Bimanual Robot)이나 다중 로봇 시스템(Multi-Robot System)에 적용할 경우, 심각한 성능 저하가 발생한다. 특히, 자기회귀(Autoregressive) 방식으로 토큰을 순차 생성하는 VLA 모델의 특성상, 두 로봇이 동시에 협업하거나 병렬적으로 작업을 수행해야 하는 상황에서 구조적으로 유효하지 않은 행동 시퀀스(예: 토큰 개수 불일치)를 생성하는 문제가 빈번하게 관찰된다.1</p>
<p>본 보고서는 이러한 난제를 해결하기 위해 2025년 11월 Chengmeng Li와 Yaxin Peng 등이 제안한 <strong>ET-VLA (Embodiment Transfer Learning for Vision-Language-Action Models)</strong> 프레임워크와 그 핵심 기술인 **Embodied Graph-of-Thought (EGoT)**를 심층적으로 분석한다.1 이 연구는 기존의 선형적인 사고 사슬(Chain-of-Thought)을 넘어, 작업의 구조를 그래프로 형상화하여 다중 로봇 간의 역할 분담과 시간적 동기화를 명확히 하는 새로운 접근법을 제시한다. 본고에서는 ET-VLA의 이론적 배경, EGoT의 구조적 특징, 합성 지속 사전 학습(SCP)의 역할, 그리고 시뮬레이션 및 실제 로봇 환경에서의 성능 검증 결과를 종합적으로 고찰함으로써, 이 기술이 차세대 로봇 지능에 미칠 파급력을 논의한다.</p>
<h2>2.  기존 VLA 모델의 구조적 한계와 다중 로봇 협업의 복잡성</h2>
<h3>2.1  자기회귀적 토큰 생성과 병렬성의 충돌</h3>
<p>VLA 모델은 기본적으로 트랜스포머(Transformer) 아키텍처를 기반으로 하며, 시각적 관찰(Observation)과 언어적 지시(Instruction)를 입력받아 일련의 이산화된 행동 토큰(Discrete Action Tokens)을 생성한다.1 이 과정은 <span class="math math-inline">t</span> 시점의 출력이 <span class="math math-inline">t+1</span> 시점의 입력이 되는 자기회귀적 방식을 따른다.</p>
<p>단일 로봇 팔의 경우, 행동은 본질적으로 순차적(Sequential)이다. “이동 -&gt; 잡기 -&gt; 들어올리기“와 같은 시퀀스는 선형적인 토큰 생성 방식과 잘 부합한다. 그러나 양팔 로봇이나 다중 로봇 시스템은 **동시성(Concurrency)**과 **비동기성(Asynchrony)**을 내포한다. 예를 들어, 왼팔이 물체를 고정하는 동안 오른팔이 뚜껑을 여는 작업은 두 행동이 동시에 발생해야 하며, 서로의 상태에 밀접하게 의존한다. 기존 VLA 모델은 이러한 병렬적 상태 공간을 단일한 선형 토큰 시퀀스로 압축하려다 보니, 두 로봇 간의 행동 토큰 순서가 뒤섞이거나, 특정 로봇의 제어 토큰이 누락되는 현상이 발생한다.1</p>
<h3>2.2  데이터의 편향성과 신체성 격차(Embodiment Gap)</h3>
<p>VLA 모델의 성능은 사전 학습 데이터의 양과 질에 크게 의존한다. 그러나 현재 공개된 로봇 데이터셋의 대다수는 단일 팔 로봇의 데이터로 구성되어 있다. 이는 모델이 양팔 로봇의 기구학적 특성이나 협업 로직을 학습할 기회를 박탈한다. 새로운 하드웨어(Embodiment)에 모델을 적용하기 위해 기존에는 인간이 원격 조작(Teleoperation)으로 수집한 시연 데이터를 사용하여 미세 조정(Fine-tuning)을 수행했으나, 이는 비용이 매우 높고 확장이 어렵다는 단점이 있다.1 특히 양팔 로봇의 원격 조작은 단일 팔보다 조작 난이도가 훨씬 높아 양질의 데이터를 대량으로 확보하기 어렵다.</p>
<h2>3.  ET-VLA 프레임워크: 다중 신체성으로의 효율적 전이</h2>
<p>ET-VLA 프레임워크는 이러한 데이터 부족과 구조적 한계를 극복하기 위해 제안되었다. 이 프레임워크는 크게 두 가지 핵심 기술적 기둥으로 구성된다: **합성 지속 사전 학습(Synthetic Continued Pretraining, SCP)**과 **Embodied Graph-of-Thought (EGoT)**이다.1</p>
<h3>3.1  합성 지속 사전 학습 (SCP): 비용 효율적인 신체성 적응</h3>
<p>SCP는 값비싼 실제 인간 시연 데이터를 대체하기 위해, 시뮬레이션 환경에서 생성된 **합성 데이터(Synthetic Data)**를 활용하여 모델을 새로운 신체성(Embodiment)에 적응시키는 단계이다.1</p>
<h4>3.1.1  SCP의 작동 원리 및 목표</h4>
<p>SCP의 주된 목표는 시각적 인식의 정밀함보다는 **“행동의 문법(Syntax of Action)”**을 학습하는 것이다. 즉, 양팔 로봇이 작동하기 위해 필요한 토큰의 개수, 순서, 그리고 기본적인 협업 패턴을 모델에 주입하는 과정이다.</p>
<ul>
<li><strong>비용 절감:</strong> 텔레오퍼레이션 장비나 인간의 노동력 없이 무한대에 가까운 데이터를 생성할 수 있다.1</li>
<li><strong>토큰 구조 학습:</strong> SCP를 거친 모델은 실제 미세 조정 단계에 진입하기 전에 이미 양팔 로봇 제어에 필요한 정확한 토큰 시퀀스 구조를 이해하게 된다. 이는 학습 초기 단계에서 모델이 유효하지 않은 행동(Invalid Actions)을 생성하여 학습이 발산하는 것을 방지한다.1</li>
<li><strong>전이 학습 가속화:</strong> 합성 데이터로 ’웜업(Warm-up)’된 모델은 소량의 실제 데이터만으로도 빠르게 목표 작업 성능에 수렴할 수 있다. 연구 결과에 따르면, SCP는 데이터 수집 비용을 획기적으로 줄이면서도 모델의 초기 성능을 안정화하는 데 결정적인 역할을 한다.1</li>
</ul>
<h3>3.2  Embodied Graph-of-Thought (EGoT): 협업의 구조화</h3>
<p>ET-VLA의 가장 혁신적인 요소는 Embodied Graph-of-Thought (EGoT)이다. 이는 거대 언어 모델(LLM)의 추론 능력을 강화하기 위해 제안된 Graph-of-Thought (GoT)의 개념을 물리적 로봇 제어(Embodied Control)의 영역으로 확장한 것이다.1</p>
<h4>3.2.1  선형적 사고(Chain)에서 입체적 사고(Graph)로의 전환</h4>
<p>기존의 CoT(Chain-of-Thought) 방식은 복잡한 문제를 <span class="math math-inline">A \rightarrow B \rightarrow C</span>와 같은 선형적 단계로 분해한다. 이는 단일 에이전트가 순차적으로 작업을 수행할 때는 유효하지만, 다중 에이전트 시스템의 복잡한 의존성을 표현하기에는 역부족이다. EGoT는 작업을 그래프 구조로 모델링하여 각 하위 작업(Sub-task) 간의 비선형적 관계를 명시한다.1</p>
<h4>3.2.2  EGoT의 수학적 정의 및 구성 요소</h4>
<p>EGoT에서 작업 계획은 다음과 같은 그래프 <span class="math math-inline">G</span>로 정의된다 1:<br />
<span class="math math-display">
G = (V, E, T, N)
</span></p>
<p>이때 각 구성 요소는 다음과 같은 의미를 지닌다:</p>
<ul>
<li><strong><span class="math math-inline">V</span> (Vertices/Nodes):</strong> 개별적인 하위 작업(Sub-tasks)을 나타낸다. 예를 들어, “왼손으로 병을 잡음”, “오른손으로 뚜껑을 돌림” 등이 각각의 노드가 된다.1</li>
<li><strong><span class="math math-inline">E</span> (Edges):</strong> 노드 간의 의존성(Dependencies)을 나타낸다. 유향 간선(Directed Edge)은 작업의 선후 관계를, 무향 간선이나 쌍방향 연결은 동시성이나 협업 관계를 나타낼 수 있다. 이는 모델이 어떤 작업이 완료되어야 다음 작업이 가능한지, 또는 어떤 작업들이 병렬로 수행되어야 하는지를 파악하게 한다.1</li>
<li><strong><span class="math math-inline">T</span> (Types):</strong> 작업의 유형(Task Types)을 분류한다. 이는 해당 작업이 이동(Move), 조작(Manipulate), 대기(Wait) 등의 성격을 규정하여 모델이 적절한 행동 전략을 수립하도록 돕는다.</li>
<li><strong><span class="math math-inline">N</span>:</strong> 각 노드에 할당된 <strong>신체성(Embodiment)의 역할</strong>이나 속성을 의미하는 것으로 해석된다. 즉, 특정 하위 작업을 수행할 주체(왼팔 로봇, 오른팔 로봇 등)를 명시하여 VLA 모델이 각 로봇의 기능을 명확히 구분하고 역할을 할당할 수 있게 한다.1</li>
</ul>
<p>이러한 그래프 구조는 VLA 모델의 입력 단계에서 프롬프트나 구조화된 데이터 형태로 제공되어, 모델이 행동 토큰을 생성할 때 강력한 <strong>조건부 가이드(Conditional Guidance)</strong> 역할을 수행한다. 결과적으로 모델은 각 로봇의 역할을 혼동하지 않고, 전체 작업의 흐름(Flow) 속에서 자신의 현재 위치를 파악하며 정밀한 제어를 수행하게 된다.</p>
<h2>4.  방법론적 비교 및 차별성 분석</h2>
<p>EGoT 및 ET-VLA의 우수성을 입증하기 위해 기존의 주요 방법론들과의 비교 분석이 필수적이다.</p>
<h3>4.1  vs. 기존 VLA 및 CoT-VLA</h3>
<ul>
<li><strong>OpenVLA:</strong> 현재 오픈소스로 공개된 가장 강력한 VLA 모델 중 하나이나, 단일 로봇 데이터(OXE)에 의존하여 다중 로봇 작업 시 구조적 붕괴(토큰 오류)를 겪는다.1</li>
<li><strong>CoT-VLA:</strong> 시각적 혹은 텍스트 기반의 사고 사슬(Chain-of-Thought)을 도입하여 추론 능력을 강화했으나, 여전히 선형적 사고방식에 머물러 있어 병렬적 협업 작업(예: Block Handover)에서의 성능이 제한적이다.5 EGoT는 ’그래프’를 통해 동시성을 내재화했다는 점에서 CoT-VLA의 상위 호환 개념으로 볼 수 있다.</li>
</ul>
<h3>4.2  vs. Diffusion Policy</h3>
<ul>
<li><strong>Diffusion Policy:</strong> 행동 생성을 위해 확산 모델(Diffusion Model)을 사용하는 방식은 연속적인 행동 공간을 잘 표현하고 멀티모달 분포를 학습하는 데 강점이 있다. 그러나 이는 주로 저수준 제어(Low-level control)에 집중되어 있어, 복잡한 장기 계획(Long-horizon planning)이나 논리적 추론이 필요한 작업에서는 VLA 모델에 비해 약점을 보일 수 있다.1 ET-VLA는 실험 결과, 특정 협업 작업에서 Diffusion Policy보다 월등한 성공률을 기록함으로써 구조적 이해의 중요성을 입증했다.</li>
</ul>
<h3>4.3  vs. Code as Policies</h3>
<ul>
<li><strong>Code as Policies:</strong> LLM을 사용하여 파이썬 코드 등을 생성해 로봇을 제어하는 방식이다.1 이는 논리적 흐름 제어에는 탁월하나, 시각적 피드백에 따른 실시간 적응이나 비정형 물체의 정밀 조작에는 한계가 있다. ET-VLA는 고수준의 계획(그래프)과 저수준의 제어(VLA 토큰)를 통합하여 이러한 이분법적 한계를 극복했다.</li>
</ul>
<h2>5.  실험 및 성능 검증</h2>
<p>저자들은 ET-VLA와 EGoT의 성능을 검증하기 위해 시뮬레이션과 실제 환경 모두에서 광범위한 실험을 수행했다. 특히 양팔 로봇 조작 벤치마크인 <strong>RoboTwin</strong>과 실제 로봇 하드웨어를 활용하여 정량적, 정성적 평가를 진행했다.1</p>
<h3>5.1  시뮬레이션 벤치마크 (RoboTwin) 평가</h3>
<p>RoboTwin 벤치마크는 양팔 로봇의 협업 능력을 평가하기 위해 고안된 시뮬레이션 환경으로, 다양한 난이도의 작업을 포함한다.7 주요 작업에 대한 ET-VLA와 Diffusion Policy (DP)의 성공률 비교는 다음과 같다.</p>
<p><strong>[표 1] RoboTwin 벤치마크 주요 작업 성공률 비교 분석</strong> [1]</p>
<table><thead><tr><th><strong>작업 명 (Task Name)</strong></th><th><strong>작업 특성</strong></th><th><strong>Diffusion Policy (DP)</strong></th><th><strong>ET-VLA (제안 모델)</strong></th><th><strong>성능 향상 (Insight)</strong></th></tr></thead><tbody>
<tr><td><strong>Dual Bottles Pick (Easy)</strong></td><td>병렬성, 비협업</td><td>54%</td><td><strong>70%</strong></td><td>+16%p. 두 팔이 독립적으로 움직여야 하는 상황에서 그래프 구조가 역할 분리를 명확히 하여 간섭을 줄임.</td></tr>
<tr><td><strong>Block Handover</strong></td><td>고도의 협업, 순차성</td><td>28%</td><td><strong>45%</strong></td><td>+17%p. 물체를 주고받는 타이밍과 위치 정밀도가 핵심. EGoT가 두 팔의 상태 의존성을 효과적으로 모델링함.</td></tr>
<tr><td><strong>Blocks Stack (Easy)</strong></td><td>정밀 조작, 순차성</td><td>2%</td><td><strong>18%</strong></td><td>+16%p. 매우 정밀한 제어가 필요한 작업에서도 유의미한 성능 향상을 보임.</td></tr>
<tr><td><strong>Container Place</strong></td><td>공간 추론</td><td>0%</td><td><strong>15%</strong></td><td>+15%p. 기존 모델이 실패하던 복잡한 공간 작업에서 성공 가능성을 입증.</td></tr>
<tr><td><strong>Shoe Place</strong></td><td>비정형 물체 조작</td><td>9%</td><td><strong>24%</strong></td><td>+15%p.</td></tr>
<tr><td><strong>Block Hammer Beat</strong></td><td>도구 사용</td><td>0%</td><td><strong>12%</strong></td><td>+12%p. 도구 사용과 같은 복합 동작에서도 개선됨.</td></tr>
</tbody></table>
<p><strong>분석 및 통찰:</strong></p>
<ol>
<li><strong>협업 난이도와 성능 격차:</strong> ’Block Handover’와 같이 두 팔의 시공간적 동기화가 필수적인 작업에서 ET-VLA는 Diffusion Policy를 압도했다. 이는 DP가 픽셀-행동 매핑에 집중하는 반면, ET-VLA는 EGoT를 통해 “건네준다“는 작업의 구조적 맥락을 이해하고 있기 때문으로 분석된다.1</li>
<li><strong>실패 영역에서의 회복:</strong> ’Blocks Stack’이나 ’Container Place’와 같이 DP가 거의 0%에 가까운 성공률을 보이는 고난이도 작업에서 ET-VLA는 15~18%의 성공률을 기록했다. 이는 완벽하지는 않지만, 모델이 작업의 논리적 해결책을 찾아가고 있음을 시사한다.</li>
</ol>
<h3>5.2  실제 로봇(Real-World) 환경 평가</h3>
<p>시뮬레이션을 넘어, 3가지의 서로 다른 양팔 로봇 하드웨어(Embodiments)를 대상으로 실제 실험이 진행되었다.1</p>
<ul>
<li><strong>일반화 능력(Generalization):</strong> ET-VLA는 학습되지 않은 새로운 배경이나 시각적 방해물(Distractors)이 존재하는 환경에서도 강건한 성능을 보였다.10 이는 모델이 단순한 이미지 패턴 매칭이 아닌, 그래프 기반의 추론을 통해 작업의 본질에 집중하고 있음을 방증한다.</li>
<li><strong>기준 모델 대비 성능:</strong> 6개의 실제 로봇 작업에서 <strong>OpenVLA 대비 평균 53.2% 이상의 성능 향상</strong>을 기록했다.3</li>
<li><strong>데이터 효율성:</strong> OpenVLA에 2배 더 많은 데이터와 학습 시간을 투자한 모델(OpenVLA-extra data)과 비교했을 때도, ET-VLA는 학습 시간을 절반으로 단축하면서도 9.1% 더 높은 성공률을 달성했다.1 이는 SCP 단계에서 학습한 ’행동 문법’이 실제 데이터 학습 효율을 극대화했음을 보여주는 강력한 증거이다.</li>
</ul>
<h3>5.3  절제 연구 (Ablation Study)</h3>
<p>연구진은 SCP와 EGoT의 개별 기여도를 분석하기 위한 절제 연구를 수행했다.</p>
<ul>
<li><strong>SCP의 효과:</strong> BridgeData V2로 OpenVLA를 1 에포크만 학습시켰을 때, SCP를 적용하지 않은 경우 성공률이 6.49%에 불과했으나, 적용 후 37.66%로 급증했다.1 이는 초기 학습 단계에서 모델이 올바른 토큰 구조를 배우는 것이 얼마나 중요한지를 보여준다.</li>
<li><strong>EGoT의 효과:</strong> 그래프 구조를 제거했을 때, 특히 다중 로봇 간의 간섭이 발생하거나 순서가 꼬이는 현상이 증가하여 전체 성공률이 하락했다. 이는 EGoT가 단순한 부가 기능이 아니라 다중 로봇 제어의 핵심 메커니즘임을 시사한다.</li>
</ul>
<h2>6.  논의 및 향후 전망</h2>
<h3>6.1  VLA 모델의 확장성(Scalability)과 한계</h3>
<p>ET-VLA는 VLA 모델을 다중 신체성으로 확장하는 데 성공적인 방법론을 제시했다. 특히 합성 데이터를 활용한 SCP 전략은 로봇 데이터 부족 문제를 해결할 현실적인 대안으로 평가된다. 그러나 여전히 해결해야 할 과제들이 존재한다.</p>
<ul>
<li><strong>추론 속도와 계산 비용:</strong> 그래프 구조를 해석하고 처리하는 과정, 그리고 거대한 VLA 모델의 크기(수십억 파라미터)는 실시간 제어에 부담이 될 수 있다. 연구진은 DiffusionVLA-2B 모델 기준 82Hz의 추론 속도를 언급했으나 10, 더 복잡한 그래프와 고해상도 이미지를 처리할 때의 지연 시간(Latency) 최적화는 지속적인 연구가 필요하다.5</li>
<li><strong>그래프 생성의 자동화:</strong> 현재 EGoT의 그래프가 얼마나 자동으로 생성되는지, 아니면 인간의 개입이 필요한지에 대한 부분은 연구의 확장성에 중요한 변수이다. 향후 LLM 자체가 환경을 인식하고 스스로 최적의 EGoT를 생성(Self-generated Graph)하는 방향으로 발전할 것으로 예상된다.</li>
</ul>
<h3>6.2  Embodied AI의 미래: 구조화된 지능</h3>
<p>EGoT의 성공은 단순히 로봇 팔을 두 개 쓰는 문제를 넘어, 인공지능이 물리적 세계를 이해하는 방식에 대한 시사점을 던진다. 픽셀에서 행동으로 직행하는 ‘End-to-End’ 방식과, 논리적 구조를 거치는 ‘Modular’ 방식 사이에서, EGoT는 두 장점을 결합한 하이브리드 접근법의 유효성을 증명했다. 이는 향후 휴머노이드 로봇과 같이 더욱 복잡한 신체 구조를 가진 로봇이 등장할 때, 그들의 행동을 제어하고 협업을 조율하는 핵심 OS(Operating System) 역할을 수행할 가능성이 높다.</p>
<h2>7.  결론</h2>
<p>본 보고서에서는 다중 로봇 협업을 위한 VLA 모델의 한계를 극복하기 위해 제안된 ET-VLA 프레임워크와 Embodied Graph-of-Thought 기술을 심층 분석했다. 분석 결과, ET-VLA는 **합성 지속 사전 학습(SCP)**을 통해 데이터 부족과 토큰 구조 학습 문제를 해결하고, <strong>EGoT</strong>를 통해 다중 에이전트 작업의 병렬성과 의존성을 효과적으로 모델링함으로써 기존 SOTA 모델 대비 비약적인 성능 향상을 이루어냈다.</p>
<p>특히 시뮬레이션 및 실제 환경에서의 실험 결과는 이 기술이 단순한 이론적 제안을 넘어 실질적인 로봇 제어 솔루션으로서의 가치를 지님을 증명한다. ’Block Handover’와 같은 정밀 협업 작업에서의 높은 성공률은 로봇이 단순한 동작의 반복이 아닌, 작업의 맥락과 파트너(다른 팔)의 상태를 이해하고 있음을 시사한다. 향후 이 기술은 공장 자동화, 재난 구조, 가사 지원 로봇 등 복잡한 다중 로봇 협업이 요구되는 다양한 분야에서 핵심적인 기반 기술로 자리 잡을 것으로 전망된다. 데이터 효율성과 일반화 능력을 동시에 갖춘 ET-VLA는 진정한 의미의 ‘범용 로봇 지능(Generalist Robot Intelligence)’ 실현을 앞당기는 중요한 이정표가 될 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Embodiment Transfer Learning for Vision-Language-Action Models - arXiv, https://arxiv.org/html/2511.01224v1</li>
<li>[2511.01224] Embodiment Transfer Learning for Vision-Language-Action Models - arXiv, https://arxiv.org/abs/2511.01224</li>
<li>FAST: Efficient Action Tokenization for Vision-Language-Action Models | Request PDF - ResearchGate, https://www.researchgate.net/publication/395364524_FAST_Efficient_Action_Tokenization_for_Vision-Language-Action_Models</li>
<li>A Review of Prominent Paradigms for LLM-Based Agents: Tool Use, Planning (Including RAG), and Feedback Learning - ACL Anthology, https://aclanthology.org/2025.coling-main.652.pdf</li>
<li>CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models, https://www.researchgate.net/publication/394511717_CoT-VLA_Visual_Chain-of-Thought_Reasoning_for_Vision-Language-Action_Models</li>
<li>Code as Policies: Language Model Programs for Embodied Control - ResearchGate, https://www.researchgate.net/publication/372122437_Code_as_Policies_Language_Model_Programs_for_Embodied_Control</li>
<li>RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2025/papers/Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins_CVPR_2025_paper.pdf</li>
<li>RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins - ResearchGate, https://www.researchgate.net/publication/390893352_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins</li>
<li>Embodiment Transfer Learning for Vision-Language-Action Models - ResearchGate, https://www.researchgate.net/publication/397232748_Embodiment_Transfer_Learning_for_Vision-Language-Action_Models</li>
<li>Chengmeng Li’s research works - ResearchGate, https://www.researchgate.net/scientific-contributions/Chengmeng-Li-2299172144</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>