<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:RT-2 Vision-Language-Action 모델 (2023-07-28)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>RT-2 Vision-Language-Action 모델 (2023-07-28)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">Vision-Language-Action(VLA) 모델</a> / <span>RT-2 Vision-Language-Action 모델 (2023-07-28)</span></nav>
                </div>
            </header>
            <article>
                <h1>RT-2 Vision-Language-Action 모델 (2023-07-28)</h1>
<h2>1.  로보틱스 지능의 새로운 지평</h2>
<h3>1.1  기존 로봇 학습의 근본적 한계와 ‘현실 세계 접지(Grounding)’ 문제</h3>
<p>전통적인 로봇 학습 패러다임은 범용 로봇 지능 개발에 있어 본질적인 한계에 직면해 왔다. 가장 근본적인 문제는 데이터의 병목 현상이다. 유용한 작업을 수행할 수 있는 로봇을 훈련시키기 위해서는 물리적 세계의 모든 객체, 환경, 작업 및 상황에 걸쳐 수십억 개의 데이터 포인트를 직접 수집하고 상호작용을 통해 학습해야 했다.1 이러한 접근법은 막대한 시간과 비용을 수반하여, 혁신가들에게는 사실상 비실용적인 과제였다.1 결과적으로 기존 로봇 시스템은 고도로 구조화되고 예측 가능한 특정 환경에서는 효과적이었으나, 동적으로 변화하는 복잡한 실제 시나리오에 대한 적응력과 확장성이 현저히 부족했다.2</p>
<p>이러한 한계의 핵심에는 ‘현실 세계 접지(Grounding)’ 문제가 자리 잡고 있다. 순수한 디지털 공간에서 작동하는 챗봇과 같은 인공지능과 달리, 로봇은 언어적, 시각적으로 습득한 추상적 개념을 물리적 현실과 자신의 능력에 ’접지’시켜야만 한다.1 예를 들어, ’사과’라는 개념을 이해하는 것은 단순히 사과의 생물학적 특성이나 역사적 일화를 아는 것을 넘어선다. 로봇은 실제 환경 속에서 사과를 시각적으로 인식하고, 비슷한 모양의 빨간 공과 구별하며, 그 물리적 특성을 고려하여 어떻게 집어 들어야 하는지를 알아야 한다.1 이처럼 추상적 지식과 물리적 실행 사이의 간극을 메우는 것이 로봇 지능의 핵심 과제였으며, 기존의 데이터 수집 방식으로는 이 간극을 효과적으로 해결하기 어려웠다.</p>
<h3>1.2  Vision-Language-Action (VLA) 모델의 개념 정립과 RT-2의 등장 배경</h3>
<p>이러한 근본적인 한계를 극복하기 위한 새로운 패러다임으로 Vision-Language-Action (VLA) 모델이 등장했으며, 구글 딥마인드의 Robotics Transformer 2 (RT-2)는 이 패러다임의 효시로 평가받는다.1 VLA 모델의 핵심 목표는 시각 정보 처리(Vision), 자연어 이해(Language), 그리고 물리적 행동 생성(Action)이라는 세 가지 분리된 영역을 단일 통합 모델 내에서 종단간(end-to-end) 방식으로 처리하는 것이다.</p>
<p>RT-2의 등장은 컴퓨터 비전, 자연어 처리, 로보틱스라는 개별적으로 발전해 온 AI 분야들이 파운데이션 모델의 등장으로 인해 융합되는 기술적 변곡점에서 이루어졌다. 이전의 로봇 시스템은 일반적으로 고수준의 추론 시스템(예: 작업 계획)과 저수준의 조작 시스템(예: 모터 제어)이 분리된 복잡한 스택 구조를 가졌다. 이 구조는 각 시스템 간의 정보 전달이 불완전하고 비효율적인 ’전화 게임’과 같은 문제를 야기했다.1 RT-2는 이러한 복잡성을 제거하고, 파운데이션 모델에서 볼 수 있는 복잡한 추론 능력과 로봇의 물리적 행동 출력을 단일 신경망 모델에 통합함으로써 이 문제를 해결하고자 했다.1 이러한 시도는 RT-1(Robotic Transformer 1)이 보여준 트랜스포머 기반 다중 작업 학습의 가능성과, PaLM-E와 같은 대규모 Vision-Language Model(VLM)이 제공한 강력한 환경 이해 능력을 기반으로 이루어졌다.1</p>
<h3>1.3  RT-2의 핵심 철학: 웹 지식의 로봇 행동으로의 변환</h3>
<p>RT-2가 제시한 가장 중요한 철학적 전환은 로봇 학습의 패러다임을 근본적으로 재정의한 데 있다. 이는 단순히 더 많은 물리적 데이터를 수집하는 데이터 중심 접근법에서 벗어나, 인터넷이라는 방대한 지식 저장소를 활용하는 지식 중심 접근법으로의 전환을 의미한다. 로보틱스의 진짜 병목은 데이터의 절대적인 부족이 아니라, 물리적으로 접지된 <em>로봇 데이터</em>의 부족이었다. RT-2 개발팀은 일반화에 필요한 방대한 시각적, 의미론적 지식(예: ’멸종된 동물’이 무엇인지, ’에너지 드링크’가 피로와 어떤 관련이 있는지)이 이미 웹에 존재한다는 사실에 주목했다. 따라서 문제는 더 많은 로봇 데이터를 수집하는 것이 아니라, 기존의 웹 지식과 로봇 제어 사이의 간극을 어떻게 연결할 것인가로 재정의되었다.</p>
<p>이러한 철학에 따라, RT-2는 대규모 언어 모델(LLM)이 웹의 텍스트를 통해 일반적인 개념과 상식을 학습하는 것과 동일한 원리를 로보틱스에 적용했다.1 즉, 웹 스케일 VLM 데이터로 사전 훈련된 모델의 시각적, 의미론적 지식을 로봇의 행동을 결정하는 데 직접적으로 이전(Knowledge Transfer)하는 것을 목표로 삼았다.1 그 결과, RT-2는 로봇 훈련 데이터에서 명시적으로 학습한 적 없는 추상적인 개념(예: 먹고 남은 과자 봉지는 ’쓰레기’라는 개념)을 이해하고, 이를 바탕으로 적절한 물리적 행동(예: ‘쓰레기통에 버리기’)을 생성하는 능력을 갖추게 되었다.1 이는 로봇이 단순히 프로그래밍된 동작을 반복하는 것을 넘어, 인간과 유사한 방식으로 개념을 학습하고 새로운 상황에 유연하게 적용할 수 있는 가능성을 연 최초의 사례 중 하나로 기록된다.</p>
<h2>2.  RT-2의 기술 아키텍처 및 작동 원리</h2>
<h3>2.1  핵심 전략: 로봇 행동의 언어화(Actions as Text Tokens)</h3>
<p>RT-2의 기술적 독창성은 ‘로봇 행동을 언어처럼 다루는’ 아이디어에서 비롯된다. 이는 로봇의 연속적이고 고차원적인 물리적 행동(예: 로봇 팔 엔드 이펙터의 3차원 위치 및 회전 변화량, 그리퍼의 개폐 상태)을 이산적인 텍스트 토큰의 시퀀스로 표현하는 혁신적인 접근법이다.4 예를 들어, 특정 로봇 행동은 “1 128 91 241 5 101 127 217“과 같은 일련의 숫자 토큰으로 구성된 문자열로 변환될 수 있다.4</p>
<p>이러한 ‘행동의 언어화’ 전략은 VLM의 아키텍처를 근본적으로 변경하지 않고도, 완전히 다른 모달리티인 로봇 제어 데이터를 학습 데이터셋에 완벽하게 통합할 수 있게 한다. 모델의 관점에서 보면, 로봇 제어는 주어진 시각-언어 프롬프트(이미지 + 명령어)에 대해 특정 ‘언어’(액션 토큰 시퀀스)로 응답하는 또 다른 시퀀스-투-시퀀스(sequence-to-sequence) 과제가 된다.6 이는 고수준의 의미론적 의도(예: “사과를 집어라”)와 저수준의 모터 실행(예: 특정 관절 각도 제어) 사이의 문제를 추상화하는 효과를 낳는다. VLM은 물리 법칙을 직접 이해할 필요 없이, 입력과 출력(액션 토큰) 간의 통계적 상관관계만을 학습하면 된다. 추론 단계에서는, 모델이 생성한 텍스트 토큰들이 다시 로봇이 실행할 수 있는 구체적인 제어 신호로 역토큰화(de-tokenized)되어, 실시간 폐쇄 루프 제어(closed-loop control)를 가능하게 한다.7</p>
<h3>2.2  기반 모델 아키텍처: PaLI-X 및 PaLM-E VLM 백본</h3>
<p>RT-2는 새로운 아키텍처를 처음부터 설계하는 대신, 구글이 이미 막대한 자원을 투입하여 개발한 강력한 사전 훈련 VLM인 PaLI-X(550억 파라미터)와 PaLM-E(120억 파라미터)를 백본(backbone)으로 활용한다.4 이러한 VLM들은 본래 하나 이상의 이미지를 입력받아 자연어 텍스트 토큰 시퀀스를 출력하도록 설계되었으며, 웹 스케일 데이터로 훈련되어 시각 질문 답변(VQA), 이미지 캡셔닝, 객체 인식 등 다양한 작업에서 최첨단 성능을 입증한 모델들이다.4</p>
<p>이러한 사전 훈련된 VLM을 사용하는 것은 단순히 개발 효율성을 높이는 것을 넘어, 모델이 이미 학습한 방대한 ’세계 모델(world model)’을 전략적으로 상속받기 위함이다. PaLI-X나 PaLM-E와 같은 거대 모델을 훈련하는 과정에서, 모델은 이미지와 텍스트의 관계를 학습하며 객체, 속성, 상식, 물리 현상에 대한 풍부하고 암묵적인 이해를 내재화하게 된다. RT-2는 처음부터 훈련하는 대신 기존 모델을 미세조정함으로써, 단순히 좋은 초기 가중치를 얻는 것을 넘어 이 거대한 사전 지식 체계를 그대로 이전받는다.6 이후의 공동 미세조정 과정은 이 추상적인 세계 모델을 특정 로봇의 물리적 신체(embodiment)와 행동 공간에 ’접지’시키는 역할을 수행한다. 이는 미래 로보틱스 발전이 로봇 특화 모델을 처음부터 구축하기보다, 기존의 거대 파운데이션 모델을 로봇의 몸에 맞게 적응시키는 방향으로 나아갈 수 있음을 시사한다.</p>
<h3>2.3  엔드-투-엔드(End-to-End) 제어 흐름</h3>
<p>RT-2의 전체 제어 흐름은 여러 개의 분리된 모듈이 아닌, 통합된 단일 신경망 내에서 종단간(end-to-end)으로 이루어진다.</p>
<ol>
<li><strong>입력 (Input):</strong> 매 제어 주기마다 로봇의 카메라로부터 현재 시점의 이미지와 사용자로부터 받은 자연어 명령(예: “멸종된 동물을 집어줘”)이 모델의 입력으로 동시에 제공된다.6</li>
<li><strong>처리 (Processing):</strong> VLM 백본(일반적으로 Vision Transformer 인코더와 LLM 디코더로 구성됨)이 이미지와 텍스트 입력을 받아들여, 이를 융합된 다중 모달 표현(multimodal representation)으로 변환한다. 이 과정에서 모델은 주어진 시각적 상황과 언어적 명령의 의미를 통합적으로 이해한다.</li>
<li><strong>출력 (Output):</strong> 모델은 이 통합된 표현을 바탕으로, 다음에 수행해야 할 로봇의 행동을 나타내는 텍스트 토큰 시퀀스를 자기회귀적(auto-regressively) 방식으로 한 토큰씩 생성한다.7</li>
<li><strong>실행 (Execution):</strong> 생성된 액션 토큰 시퀀스는 로봇 제어 시스템에 의해 해석되어, 로봇 팔을 움직이는 구체적인 제어 값(예: 3차원 공간에서의 위치 변화량, 회전 각도 변화량, 그리퍼 상태)으로 변환된다. 이 제어 신호가 로봇에 전달되어 실제 물리적 행동이 수행된다. 이 전체 과정이 빠른 속도로 반복되면서 연속적인 작업 수행이 가능해진다.</li>
</ol>
<h3>2.4  부록: 트랜스포머와 Self-Attention 메커니즘</h3>
<p>RT-2의 근간을 이루는 트랜스포머 아키텍처는 2017년 “Attention Is All You Need” 논문에서 제안된 이래 자연어 처리 및 비전 분야에 혁명을 일으켰다. 이 모델은 기존의 순환 신경망(RNN)이나 컨볼루션 신경망(CNN) 구조를 탈피하고, 오직 ‘셀프 어텐션(Self-Attention)’ 메커니즘에만 의존하여 입력 시퀀스 내의 요소들 간의 관계를 파악한다.10</p>
<p>셀프 어텐션은 시퀀스 내의 한 요소(토큰)가 다른 모든 요소들과 직접 상호작용하여, 어떤 요소에 더 ’주의(attend)’를 기울여야 할지를 동적으로 결정하는 메커니즘이다.12 이를 통해 문맥에 따른 단어의 의미 변화나 장거리 의존성 문제를 효과적으로 해결할 수 있다. 이 과정은 각 입력 토큰에 대해 쿼리(Query), 키(Key), 값(Value)이라는 세 가지 학습 가능한 벡터를 생성하여 계산된다.12</p>
<h4>2.4.1  Scaled Dot-Product Attention</h4>
<p>트랜스포머에서 사용되는 구체적인 어텐션 계산 방식은 Scaled Dot-Product Attention이며, 그 계산식은 다음과 같다.11</p>
<p>코드 스니펫</p>
<p>$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $</p>
<p>계산 과정은 다음과 같이 단계별로 이루어진다.</p>
<ol>
<li><strong>유사도 계산:</strong> 현재 토큰의 쿼리 벡터(<span class="math math-inline">Q</span>)와 다른 모든 토큰의 키 벡터(<span class="math math-inline">K</span>)를 내적(dot product)하여 유사도 점수(attention score)를 계산한다.</li>
<li><strong>스케일링:</strong> 계산된 점수를 키 벡터의 차원(<span class="math math-inline">d_k</span>)의 제곱근(<span class="math math-inline">\sqrt{d_k}</span>)으로 나누어준다. 이 스케일링 과정은 벡터 차원이 커질수록 내적 값이 지나치게 커져 소프트맥스 함수의 기울기가 0에 가까워지는 기울기 소실(vanishing gradient) 문제를 방지하는 중요한 역할을 한다.11</li>
<li><strong>가중치 계산:</strong> 스케일링된 점수에 소프트맥스 함수를 적용하여 합이 1이 되는 어텐션 가중치(attention weights)를 얻는다. 이 가중치는 각 토큰이 다른 토큰들에 비해 얼마나 중요한지를 나타내는 확률 분포이다.</li>
<li><strong>최종 출력:</strong> 계산된 어텐션 가중치를 각 토큰의 값 벡터(<span class="math math-inline">V</span>)에 곱한 후 모두 더하여, 현재 토큰에 대한 문맥이 풍부하게 반영된 최종 출력 벡터를 생성한다.</li>
</ol>
<h4>2.4.2  Multi-Head Attention</h4>
<p>Multi-Head Attention은 단일 어텐션 메커니즘을 병렬적으로 여러 번 수행하여 모델의 표현력을 극대화하는 기법이다.11 이는 모델이 하나의 관점이 아닌, 여러 다른 표현 부분 공간(representation subspaces)에서 동시에 정보를 포착할 수 있게 한다. 예를 들어, 한 어텐션 헤드는 문법적 관계에 집중하고, 다른 헤드는 의미론적 관계에 집중하는 식이다.</p>
<p>코드 스니펫</p>
<p>$ \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h)W^O $</p>
<p>코드 스니펫</p>
<p>$ \quad \text{where} \quad \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $</p>
<p>여기서 <span class="math math-inline">W_i^Q</span>, <span class="math math-inline">W_i^K</span>, <span class="math math-inline">W_i^V</span>는 각 헤드마다 독립적으로 학습되는 투영 행렬(projection matrix)이다. 각 헤드는 입력 <span class="math math-inline">Q</span>, <span class="math math-inline">K</span>, <span class="math math-inline">V</span>를 서로 다른 방식으로 선형 변환한 후 Scaled Dot-Product Attention을 독립적으로 수행한다. 이렇게 얻어진 <span class="math math-inline">h</span>개의 헤드 출력은 모두 연결(concatenate)된 후, 또 다른 학습 가능한 가중치 행렬 <span class="math math-inline">W^O</span>를 통해 최종 출력 차원으로 변환된다.11 이 구조는 트랜스포머가 복잡하고 다층적인 관계를 효과적으로 학습하는 핵심적인 이유 중 하나이다.</p>
<h2>3.  훈련 방법론: 공동 미세조정(Co-fine-tuning) 전략</h2>
<h3>3.1  웹 데이터와 로봇 데이터의 융합: 공동 미세조정의 원리</h3>
<p>RT-2의 성공적인 지식 이전을 가능하게 한 핵심 훈련 전략은 ’공동 미세조정(co-fine-tuning)’이다.4 이는 사전 훈련된 VLM을 로봇 데이터에만 국한하여 미세조정(fine-tuning)하는 단순한 접근법을 넘어선다. 대신, 기존의 방대한 웹 스케일 시각-언어 데이터와 새로 추가된 로봇 궤적 데이터를 혼합하여 모델을 동시에 훈련시킨다.7</p>
<p>이 전략의 근본적인 목적은 두 가지 상충될 수 있는 목표를 동시에 달성하는 것이다. 첫째, 로봇이 물리 세계에서 특정 작업을 수행하는 데 필요한 구체적인 기술(예: 물건을 집고, 서랍을 여는 동작)을 효과적으로 학습하도록 하는 것이다. 둘째, 이 과정에서 모델이 웹 데이터를 통해 이미 습득한 광범위한 일반화 능력과 의미론적 추론 능력을 잃지 않고 유지하거나 오히려 강화하는 것이다. 만약 로봇 데이터에만 집중하여 미세조정할 경우, 모델이 새로운 작업에 과적합되어 기존의 일반 지식을 잊어버리는 ‘파국적 망각(catastrophic forgetting)’ 현상이 발생할 수 있다. 공동 미세조정은 웹 데이터를 계속해서 주입함으로써 이러한 문제를 방지하고, 두 데이터 소스의 장점을 모두 취하는 균형 잡힌 학습을 가능하게 한다.6 훈련 과정에서는 각 훈련 배치(batch) 내에서 로봇 데이터와 웹 데이터의 비율을 신중하게 조절하며, 일반적으로 로봇 데이터셋에 더 높은 샘플링 가중치를 부여하여 모델이 로봇 제어라는 핵심 과제를 효과적으로 학습하도록 유도한다.7</p>
<h3>3.2  훈련 데이터셋 구성 분석</h3>
<p>RT-2의 훈련은 크게 두 가지 종류의 이질적인 데이터셋을 융합하여 이루어진다.</p>
<ul>
<li><strong>로봇 시연 데이터 (Robotics Dataset):</strong> 이 데이터는 RT-2의 물리적 실행 능력의 근간을 이룬다. 주로 선행 연구인 RT-1 프로젝트를 통해 수집된 데이터가 활용되었으며, 이는 13대의 실제 로봇이 17개월 동안 사무실 주방과 같은 현실적인 환경에서 수집한 수많은 시연 에피소드로 구성된다.4 각 시연 궤적 데이터에는 “Pick Object(물건 집기)”, “Open Drawer(서랍 열기)”, “Place Object into Receptacle(용기 안에 물건 넣기)” 등 7가지의 핵심 기술에 대한 자연어 명령이 함께 주석으로 달려 있다.7 이 데이터는 모델에게 특정 명령이 어떤 물리적 동작 시퀀스에 해당하는지를 가르치는 역할을 한다.</li>
<li><strong>웹 스케일 데이터 (Web Scale Data):</strong> 이 데이터는 RT-2의 의미론적 이해와 일반화 능력의 원천이다. 기존 VLM을 훈련시키는 데 사용된 인터넷 규모의 데이터셋을 그대로 활용한다. 대표적으로 WebLI 데이터셋이 있으며, 이는 약 100억 개의 이미지-텍스트 쌍을 포함하는 방대한 자료이다.9 이 데이터셋은 시각 질문 답변(VQA), 이미지 캡셔닝, 그리고 웹 페이지에서 추출된 비구조적인 이미지와 텍스트의 조합 등 다양한 형태로 구성되어 있다.7 이 데이터를 통해 모델은 세상의 수많은 객체와 개념에 대한 시각적, 언어적 지식을 습득한다.</li>
<li><strong>기타 데이터셋:</strong> 특정 벤치마크에서의 성능을 극대화하기 위해 추가적인 데이터셋이 사용되기도 한다. 예를 들어, Language-Table 벤치마크 평가를 위해 해당 시뮬레이션 및 실제 환경 데이터셋을 훈련에 포함시켜 모델의 특정 작업 수행 능력을 강화했다.7</li>
</ul>
<h3>3.3  데이터 혼합 비율과 출력 제약</h3>
<p>공동 미세조정 과정에서 로봇 데이터와 웹 데이터의 혼합 비율은 모델의 최종 성능에 중요한 영향을 미치는 설계 변수이다. 한쪽 데이터에 치우칠 경우, 모델은 물리적 기술 습득이나 의미론적 일반화 능력 중 하나를 잃을 수 있다. 연구에 따르면, RT-2-PaLI-X (55B) 모델의 경우 로봇 데이터가 전체 훈련 혼합의 약 50%를 차지하도록 가중치를 부여했으며, RT-2-PaLM-E (12B) 모델의 경우에는 약 66%로 더 높은 비중을 두었다.7 로봇 데이터의 비중이 상당히 높다는 사실은 중요한 점을 시사한다. 웹 지식이 의미론적 일반화의 ’씨앗’을 제공하더라도, 추상적 개념을 정밀하고 반복 가능한 물리적 기술로 변환하는 과정은 여전히 상당한 양의 구체적인 시연 데이터를 필요로 한다는 것이다. 즉, 물리적 기술 습득의 샘플 효율성은 개념 학습보다 본질적으로 낮을 수 있음을 보여준다.</p>
<p>훈련 과정의 또 다른 중요한 기술적 세부사항은 ’출력 제약(Output Constraint)’이다. VLM은 본질적으로 다음에 올 토큰에 대한 확률 분포를 생성하는 모델이다. 아무런 제약이 없다면, 모델은 물리적으로 불가능하거나 무의미한 행동 토큰 시퀀스를 생성할 위험이 있다. 이를 방지하기 위해, 모델이 로봇 제어 작업에 대한 프롬프트를 받았을 때, 디코딩(출력 생성) 과정에서 어휘를 유효한 액션 토큰 집합으로 제한한다.7 반면, 일반적인 VQA와 같은 시각-언어 작업에서는 전체 자연어 토큰 어휘를 사용하여 자유롭게 응답을 생성하도록 허용한다. 이 출력 제약은 확률적인 언어 모델의 출력과 결정론적인 로봇 제어의 요구사항 사이를 잇는 실용적이고 중요한 안전장치 역할을 하며, 시스템의 안정성과 신뢰성을 보장하는 데 필수적이다.</p>
<h4>3.3.1 Table 1: RT-2 훈련 데이터셋 구성</h4>
<table><thead><tr><th>데이터셋</th><th>설명</th><th>출처</th><th>훈련 혼합 비율 (RT-2-PaLI-X)</th><th>훈련 혼합 비율 (RT-2-PaLM-E)</th></tr></thead><tbody>
<tr><td><strong>WebLI</strong></td><td>109개 언어에 걸친 약 100억 개의 이미지-텍스트 쌍. 교차 모달 유사도 점수 상위 10%인 10억 개 예시로 필터링됨.</td><td>Chen et al. (2023b), Driess et al. (2023)</td><td>~50%</td><td>~34%</td></tr>
<tr><td><strong>Robotics Dataset</strong></td><td>7가지 기술에 대한 자연어 명령이 주석으로 달린 모바일 조작 로봇의 시연 에피소드.</td><td>Brohan et al. (2022)</td><td>~50%</td><td>~66%</td></tr>
<tr><td><strong>Language-Table</strong></td><td>시뮬레이션 및 실제 환경에서의 언어 기반 테이블 위 조작 작업 데이터셋.</td><td>Lynch et al. (2022)</td><td>N/A (별도 실험에 사용)</td><td>N/A (별도 실험에 사용)</td></tr>
</tbody></table>
<h2>4.  성능 평가 및 창발적 능력(Emergent Capabilities) 분석</h2>
<h3>4.1  정량적 성능 평가: RT-1 및 기타 기반 모델과의 비교</h3>
<p>RT-2의 효과를 검증하기 위해 6,000회 이상의 광범위한 실제 로봇 실험이 수행되었다.4 평가는 크게 두 가지 시나리오, 즉 훈련 데이터에서 본 적 있는(seen) 작업과 본 적 없는(unseen) 새로운 작업으로 나누어 진행되었다.</p>
<p>평가 결과, RT-2는 웹 지식 이전의 핵심 목표인 일반화 성능에서 괄목할 만한 성과를 보였다. 훈련 데이터에 포함된 익숙한 작업에서는 이전 모델인 RT-1과 대등한 수준의 성공률을 유지하면서도, 훈련 데이터에서 접하지 못한 새롭고 추상적인 명령이 포함된 ‘본 적 없는’ 시나리오에서는 성공률이 62%를 기록했다. 이는 RT-1의 성공률인 32%에 비해 거의 두 배 가까이 향상된 수치이다.1 이는 새로운 능력을 얻는 과정에서 기존의 능력을 희생하지 않았으며, 웹 스케일 데이터의 통합이 순수한 성능 향상으로 이어졌음을 명확히 보여준다.</p>
<p>또한, 모델의 설계 선택이 성능에 미치는 영향을 분석하기 위한 제거 연구(ablation study)도 수행되었다. 그 결과, 모델의 크기가 클수록(예: 55B 파라미터 모델이 5B 모델보다) 일반화 성능이 더 높은 경향을 보였다. 더 중요하게는, 훈련 방식에 따른 성능 차이가 두드러졌다. 사전 훈련된 VLM을 기반으로 (공동) 미세조정하는 방식이, 동일한 데이터를 사용하여 모델을 처음부터 훈련(training from scratch)하는 방식보다 월등히 우수한 성능을 나타냈다.6 이는 RT-2의 성능 향상이 단순히 데이터의 양이나 모델 크기 때문이 아니라, 사전 훈련 과정에서 VLM이 학습한 방대한 양의 사전 지식을 효과적으로 이전했기 때문임을 실험적으로 입증한다.</p>
<h4>4.1.1 Table 2: RT-1 대비 RT-2 일반화 성능 비교</h4>
<table><thead><tr><th>평가 시나리오</th><th>RT-1 성공률</th><th>RT-2 (PaLI-X 55B) 성공률</th><th>성능 향상</th></tr></thead><tbody>
<tr><td><strong>본 적 있는(Seen) 작업</strong></td><td>유사 수준</td><td>유사 수준</td><td>-</td></tr>
<tr><td><strong>본 적 없는(Unseen/Novel) 작업</strong></td><td>32%</td><td>62%</td><td><strong>+94% (거의 2배)</strong></td></tr>
</tbody></table>
<h3>4.2  창발적 능력(Emergent Capabilities) 심층 탐구</h3>
<p>RT-2의 가장 혁신적인 성과는 훈련 데이터에 명시적으로 존재하지 않았던 새로운 지능적 행동들이 ’창발(emerge)’했다는 점이다.5 이는 웹 스케일 데이터에 암묵적으로 내재된 방대한 시각적, 의미론적 지식이 로봇의 물리적 제어 능력과 성공적으로 결합되었음을 보여주는 강력한 증거이다. 이러한 창발적 능력은 단순한 데이터 내 보간(interpolation)을 넘어, 추상적 지식에 기반한 진정한 외삽(extrapolation) 능력, 즉 원시적인 형태의 유추적 추론(analogical reasoning)이 가능해졌음을 시사한다.</p>
<p>연구팀은 이러한 창발적 능력을 체계적으로 평가하기 위해 세 가지 주요 범주로 분류했다: <strong>기호 이해(Symbol Understanding), 추론(Reasoning), 인간 인식(Human Recognition)</strong>.4</p>
<ul>
<li>
<p><strong>기호 이해 (Symbol Understanding):</strong> 이 능력은 로봇 훈련 데이터에는 전혀 등장하지 않았던 숫자, 아이콘, 수학적 연산과 같은 추상적인 기호를 이해하고, 이를 물리적 세계의 좌표나 객체와 연결하여 행동하는 능력을 의미한다.</p>
</li>
<li>
<p><em>사례:</em> “바나나를 2 더하기 1의 합으로 옮겨라“는 명령을 받았을 때, RT-2는 먼저 ’2+1’이라는 수학적 기호를 ’3’이라는 숫자로 해석하고, 테이블 위에 놓인 숫자 ’3’을 시각적으로 찾아 그 위치로 바나나를 옮기는 작업을 성공적으로 수행했다.4 정량 평가에서 이 범주의 성공률은 75%로, 베이스라인 모델의 19%에 비해 압도적으로 높았다.</p>
</li>
<li>
<p><strong>추론 (Reasoning):</strong> 이 능력은 객체의 속성(예: 크기, 색상), 객체 간의 관계(예: 상대적 위치), 또는 일반적인 상식에 기반하여 논리적으로 판단하고 행동하는 능력을 포함한다.</p>
</li>
<li>
<p><em>사례 1:</em> “가장 작은 물건을 집어라” 또는 “테이블에서 막 떨어지려는 가방을 집어라“와 같은 상대적이거나 상황적인 명령을 정확히 해석하고 수행했다.4</p>
</li>
<li>
<p><em>사례 2:</em> 가장 인상적인 사례는 “멸종된 동물을 집어라“는 명령에 대해, 로봇이 테이블 위의 여러 장난감 중에서 공룡 장난감을 정확히 집어든 것이다.6 이 행동은 로봇 훈련 데이터에는 존재하지 않는 지식의 연결을 필요로 한다. 모델은 웹 데이터를 통해 (1) 특정 형태의 장난감이 ’공룡’이라는 시각적 지식과, (2) ’공룡’이라는 단어가 ’멸종된 동물’이라는 개념과 강하게 연관된다는 언어적 지식을 학습했다. RT-2는 이 두 가지 이질적인 지식을 즉석에서 종합하여, 눈앞의 시각적 객체([공룡 장난감])를 명령의 추상적 속성([멸종된 동물])과 연결하는 유추적 추론을 수행한 것이다.</p>
</li>
<li>
<p><strong>인간 인식 (Human Recognition):</strong> 이 능력은 사람의 존재나 특징(예: 착용하고 있는 액세서리)을 인식하고, 이를 바탕으로 상호작용하는 작업을 수행하는 능력을 말한다.</p>
</li>
<li>
<p><em>사례:</em> “안경 쓴 사람에게 콜라 캔을 옮겨줘“와 같은 명령을 수행할 수 있었다.7 정량 평가에서 이 범주의 성공률은 53%를 기록하여, 순수 시각 기반 모델(VC-1, 13%)이나 이전 로봇 모델(RT-1, 20%)에 비해 2배에서 3배 이상 높은 성능을 보였다.7</p>
</li>
</ul>
<h4>4.2.1 Table 3: 창발적 능력 정량 평가 (RT-2-PaLI-X-55B)</h4>
<table><thead><tr><th>능력 범주</th><th>설명</th><th>예시 명령</th><th>RT-2 성공률</th><th>베이스라인(VC-1/RT-1) 성공률</th></tr></thead><tbody>
<tr><td><strong>기호 이해</strong></td><td>훈련 데이터에 없는 추상적 기호(숫자, 아이콘 등)를 이해하고 행동</td><td>“바나나를 2+1의 합으로 옮겨라”</td><td>75%</td><td>19%</td></tr>
<tr><td><strong>추론</strong></td><td>객체의 속성, 관계, 상식에 기반한 추론</td><td>“가장 작은 물건을 집어라”</td><td>61%</td><td>15%</td></tr>
<tr><td><strong>인간 인식</strong></td><td>사람의 특징이나 상태를 인식하고 행동</td><td>“안경 쓴 사람에게 콜라를 옮겨라”</td><td>53%</td><td>13-20%</td></tr>
</tbody></table>
<h3>4.3  사고 연쇄(Chain-of-Thought) 추론</h3>
<p>RT-2, 특히 PaLM-E를 백본으로 하는 모델은 대규모 언어 모델의 고유한 속성인 ‘사고 연쇄(Chain-of-Thought, CoT)’ 추론 능력을 로봇 제어에 통합할 수 있음을 보여주었다.4 CoT는 단순히 최종 답을 내놓는 것이 아니라, 문제 해결 과정을 단계별로 서술하게 함으로써 복잡한 추론 문제의 정확도를 높이는 기법이다.</p>
<p>RT-2에 CoT 프롬프팅을 적용하면, 모델은 최종 행동 토큰을 출력하기 전에 먼저 자연어로 자신의 추론 과정을 명시적으로 생성한다. 예를 들어, Reasoning: [추론 내용] Action: [행동 토큰]과 같은 형식의 출력을 만들어낸다.6 이 내부적인 독백은 단순한 설명이 아니라, 모호하고 복잡한 문제를 실행 가능한 하위 문제로 분해하는 기능적인 메커니즘으로 작동한다.</p>
<ul>
<li><em>사례 1:</em> “임시 망치로 쓸 만한 물건을 집어라“는 고도로 추상적인 명령을 받았을 때, 모델은 먼저 “A rock is hard and heavy, it can be used as a hammer.(돌은 단단하고 무거워서 망치로 사용할 수 있다)“와 같은 추론을 생성한다. 이 추론 과정은 ’임시 망치’라는 기능적 요구사항을 ’돌’이라는 구체적인 시각적 객체로 변환하는 역할을 한다. 그 후에야 모델은 ‘돌을 집는’ 행동 토큰을 출력한다.4</li>
<li><em>사례 2:</em> “피곤한 사람에게 가장 좋은 음료는 무엇인가?“라는 질문과 함께 집으라는 명령을 받았을 때, 모델은 에너지 드링크가 피로 회복에 도움이 된다는 상식을 바탕으로 추론하고, 테이블 위의 여러 음료 중 에너지 드링크를 선택하여 집는 행동을 수행했다.4</li>
</ul>
<p>이처럼 CoT는 모델에게 자신의 행동 계획을 명시적으로 수립하고 검토할 수 있는 ’작업 공간’을 제공함으로써, 더 정교하고 다단계적인 의미론적 추론을 가능하게 한다. 이는 향후 고수준의 작업 플래너와 저수준의 로봇 제어 정책을 별도의 모듈로 나누지 않고, 단일 VLA 모델 내에서 유기적으로 통합할 수 있다는 유망한 연구 방향을 제시한다.6</p>
<h2>5.  모델의 한계와 안전성 고찰</h2>
<p>RT-2는 로보틱스 분야에 중요한 돌파구를 마련했지만, 범용 로봇 지능을 향한 여정에서 해결해야 할 명확한 한계와 중대한 안전성 문제를 동시에 드러냈다.</p>
<h3>5.1  내재적 한계: 새로운 물리적 동작(Motion) 생성의 부재</h3>
<p>RT-2의 가장 근본적인 한계는 ’무엇을 해야 하는가(semantics)’에 대한 이해와 ’어떻게 해야 하는가(motor skills)’에 대한 능력 사이에 존재하는 본질적인 불균형에서 비롯된다. 모델은 웹 지식을 통해 의미론적 일반화에서는 전례 없는 능력을 보여주지만, 물리적 동작의 일반화는 훈련 데이터의 분포에 엄격하게 제한된다.7</p>
<p>구체적으로, RT-2는 훈련 데이터에서 학습한 ‘집기’, ‘놓기’, ’밀기’와 같은 기본적인 동작 프리미티브(motion primitives)들을 새로운 객체나 상황에 맞게 재조합하고 재사용할 수는 있다. 하지만 훈련 데이터에 존재하지 않았던 완전히 새로운 유형의 물리적 동작, 예를 들어 ’문워크’와 같은 복잡하고 연속적인 움직임을 창의적으로 생성해낼 수는 없다.15 이는 모델이 행동을 이산적인 토큰의 시퀀스로 표현하고 예측하는 방식에서 기인하는 내재적 한계이다. 모델은 기존에 학습된 동작 ’단어’들의 조합으로만 ’문장’을 만들 수 있을 뿐, 새로운 ’단어’를 만들어내지는 못한다. 이는 현재의 VLA 패러다임이 의미론적 지식은 효과적으로 이전하지만, 정교하고 새로운 운동 지능을 생성하는 데에는 여전히 한계가 있음을 명확히 보여준다.</p>
<h3>5.2  기술적 제약 사항</h3>
<p>실제 환경에 RT-2와 같은 모델을 배포하기 위해서는 여러 기술적 장벽을 넘어야 한다.</p>
<ul>
<li><strong>높은 연산 비용 (Computational Cost):</strong> RT-2는 수백억 개의 파라미터를 가진 거대 VLM을 기반으로 하므로, 실시간 추론을 위해 막대한 연산 자원을 필요로 한다. 이는 온보드 컴퓨팅 자원이 제한적인 대부분의 로봇 플랫폼에 모델을 직접 탑재하는 것을 어렵게 만드는 주요 요인이다.15 이러한 문제를 완화하기 위해 모델 양자화(quantization), 증류(distillation), 하드웨어 가속과 같은 모델 경량화 및 최적화 기술에 대한 연구가 필수적이다.15</li>
<li><strong>저주파 제어 (Low-Frequency Control):</strong> 현재 RT-2 모델은 상대적으로 낮은 제어 주파수(약 1Hz)로 작동한다.16 이는 테이블 위에서 물건을 옮기는 것과 같이 정적인 작업에는 충분할 수 있으나, 동적인 환경과 상호작용하거나 빠르고 정밀한 움직임이 요구되는 작업(예: 날아오는 공 잡기)에는 부적합하다. 실시간 고주파 제어를 VLA 모델에 통합하는 것은 향후 중요한 연구 과제이다.</li>
<li><strong>오픈소스 VLM의 부족:</strong> RT-2 훈련에 핵심적인 역할을 한 PaLI-X, PaLM-E와 같은 고성능 VLM은 대부분 상용 모델로, 오픈소스로 공개되어 있지 않다.16 이는 학계 및 연구 커뮤니티의 접근성을 제한하고, 연구 결과의 재현성과 폭넓은 발전을 저해하는 요인으로 작용한다.</li>
</ul>
<h3>5.3  안전성 및 견고성 문제 (Safety and Robustness)</h3>
<p>VLA 모델의 가장 중대한 과제는 안전성이다. 디지털 공간에 머무는 LLM과 달리, VLA 모델은 물리적 세계와 직접 상호작용하기 때문에, 모델의 오류는 단순한 오답이 아닌 물리적 피해로 직결될 수 있다.17 이는 VLA 모델의 안전성 문제가 기존 LLM의 안전성 문제와는 차원이 다른, 디지털 영역에서 물리적 영역으로의 ’위험의 상전이(phase shift in risk)’를 의미한다.</p>
<ul>
<li><strong>잠재적 물리적 위험:</strong> 모델의 오작동은 다양한 형태의 물리적 위험을 초래할 수 있다. 예를 들어, (1) 목표물 주변의 다른 물체를 거칠게 다루어 파손시키는 경우, (2) 독성이 있는 세제 병을 음료수 병으로 오인하여 사람에게 건네는 경우, (3) ’사과를 자르라’는 명령에 칼을 사용하면서 주변에 있는 사람의 안전을 고려하지 않는 경우 등이 발생할 수 있다.17</li>
<li><strong>견고성 부족 (Lack of Robustness):</strong> LLM/VLM 기반 로봇은 입력의 사소한 변화에 매우 민감하게 반응하여 예측 불가능한 행동을 보일 수 있다. 한 연구에 따르면, 명령어의 단어를 동의어로 바꾸는 등 작은 표현 차이가 작업 성공률을 평균 19.4% 감소시켰으며, 조명 조건의 변화와 같은 시각적 변동은 성공률을 29.1%나 하락시켰다.18 또 다른 연구에서는 RT-2 모델을 Spot과 같은 로봇에 배포했을 때, 훈련 데이터에 없던 새로운 장애물 구성 환경에서 항법 성능이 42%나 저하되는 현상을 관찰했다.18 실제 환경에서 1-3 cm의 물체 위치 변화가 30% 이상의 성공률 저하를 유발할 수 있다는 점은, 평균적인 성능을 넘어 최악의 경우(worst-case)에 대한 안전 보장이 얼마나 어려운 과제인지를 보여준다.</li>
<li><strong>적대적 공격 (Adversarial Attacks):</strong> 악의를 가진 사용자가 모델의 취약점을 이용하여 의도적으로 위험한 행동을 유도할 수 있다. 예를 들어, 명령어에 교묘하게 숨겨진 지시를 삽입하는 ‘프롬프트 주입(prompt injection)’ 공격을 통해 안전 제약 조건을 우회하고 로봇이 위험한 작업을 수행하도록 만들 수 있다.18</li>
<li><strong>사회적 및 윤리적 문제:</strong> VLA 모델은 기반 LLM이 학습한 데이터에 내재된 사회적 편견을 상속받아, 특정 그룹의 사람들에게 차별적인 행동을 보이거나 유해한 고정관념을 강화할 수 있다.17 더 나아가, 이러한 기술이 제조업이나 군사 분야에 대규모로 적용될 경우, 대량 실업이나 자율 살상 무기와 같은 심각한 사회적, 윤리적 문제를 야기할 수 있다는 우려가 지속적으로 제기되고 있다.19</li>
</ul>
<h2>6.  RT-2 이후의 발전과 VLA 모델의 미래 전망</h2>
<h3>6.1  RT-2-X: 교차 기종(Cross-Embodiment) 학습을 통한 일반화 확장</h3>
<p>RT-2의 성공에 이어, 연구는 단일 로봇 플랫폼을 넘어선 일반화로 확장되었다. 후속 연구인 RT-2-X는 RT-2 아키텍처를 기반으로 하되, 한 종류의 로봇이 아닌 여러 다른 종류의 로봇(embodiments)에서 수집된 매우 다양한 데이터를 사용하여 모델을 훈련시켰다.20</p>
<p>이러한 ‘교차 기종’ 훈련 방식은 놀라운 결과를 낳았다. 여러 로봇의 데이터를 함께 학습한 RT-2-X는 기존 RT-2가 학습하지 않았던 새로운 기술(예: 물체를 밀어서 옮기기)을 추가적으로 습득했으며, 기존의 창발적 기술(emergent skills)에 대한 성공률이 RT-2 대비 3배나 향상되었다.20 이는 모델이 특정 로봇의 관절 각도나 모터 제어 신호를 단순히 암기하는 것이 아니라, 다른 물리적 형태를 아우를 수 있는 더 추상적이고 ‘기종에 구애받지 않는(embodiment-agnostic)’ 기술 표현을 학습하고 있음을 시사한다. 이는 인간이 오른손, 왼손, 심지어 도구를 사용하더라도 ’잡는다’는 행위의 본질을 이해하는 것과 유사하다. 이 연구는 충분히 큰 용량의 아키텍처(55B 파라미터)와 극도로 다양한 데이터가 결합될 때, 특정 하드웨어에 종속되지 않는 보편적인 로봇 지능의 실현이 가능할 수 있음을 보여주었다.20</p>
<h3>6.2  관련 후속 연구 동향</h3>
<p>RT-2가 제시한 VLA 패러다임은 로보틱스 연구 커뮤니티에 큰 영감을 주었으며, 그 아이디어를 계승하거나 한계를 보완하는 다양한 후속 연구들이 등장했다. 이는 단일 종단간 모델이 모든 것을 해결하는 방식이 아닌, 여러 모델이 각기 다른 추상화 수준을 담당하는 하이브리드 시스템으로 발전할 가능성을 보여준다.</p>
<ul>
<li><strong>RT-H (Robot Transformer with Action Hierarchies):</strong> RT-2가 저수준의 이산적인 액션 토큰을 직접 예측하는 반면, RT-H는 행동을 계층적으로 모델링하는 접근법을 제안했다. 이 모델은 “팔을 위로 뻗는다” 또는 “물체를 향해 전진한다“와 같은 자연어 형태의 중간 단계 ’언어 동작(language motion)’을 먼저 예측한다.21 이 중간 단계의 추상적인 행동 표현은 모델이 더 복잡하고 긴 작업을 효과적으로 계획하고 수행하도록 돕는다. 실험 결과, RT-H는 다양한 다중 작업 데이터셋에서 RT-2보다 15% 높은 성능을 달성했으며, 특히 새로운 장면과 객체에 대한 일반화 능력이 향상되었다.21</li>
<li><strong>RVT-2 (Robotic View Transformer 2):</strong> 이 연구는 RT-2의 약점인 정밀 제어 능력에 초점을 맞춘다. VLA 모델과는 다른 계열의 연구(PerAct, RVT)에 속하지만, VLA 모델이 다루는 고수준 의미 이해와 상보적인 관계에 있다. RVT-2는 단 10개의 시연 데이터만으로 플러그를 소켓에 삽입하는 것과 같은 밀리미터 수준의 고정밀 조작 작업을 학습할 수 있음을 보여주었다.22 이는 미래의 로봇 시스템이 RT-2와 같은 VLA 모델을 고수준 ’작업 플래너’로 사용하여 사용자의 의도를 파악하고, RVT-2와 같은 정밀 제어 모델을 저수준 ’실행기’로 사용하여 섬세한 물리적 동작을 수행하는 하이브리드 형태로 발전할 수 있음을 시사한다.</li>
<li><strong>기타 연구 동향:</strong> 이 외에도 LLM을 활용하여 로봇이 실행할 수 있는 파이썬 코드를 직접 생성하게 하는 연구 23, 또는 강화학습의 제약된 마르코프 결정 과정(CMDP) 패러다임을 도입하여 VLA 모델의 안전성을 수학적으로 보장하려는 연구 17 등, VLA 모델의 개념은 다양한 방향으로 빠르게 확장되고 있다.</li>
</ul>
<h3>6.3  VLA 모델의 미래와 산업적 응용</h3>
<p>VLA 모델은 로봇 지능의 미래를 향한 중요한 이정표이다. 향후 이 기술은 더욱 발전된 멀티모달 파운데이션 모델, 스스로 목표를 설정하고 계획을 수정하는 에이전트적 추론(agentic reasoning), 그리고 실제 환경과의 상호작용을 통해 지속적으로 학습하고 발전하는 평생 학습(continual learning) 기술과 융합될 것으로 전망된다.24 이러한 발전은 궁극적으로 인간 사회와 조화를 이루며 적응적으로 행동하는 범용 로봇 에이전트의 등장을 앞당길 것이다.</p>
<p>이러한 기술적 잠재력은 다양한 산업 분야에서 혁신적인 응용을 가능하게 할 것이다.9</p>
<ul>
<li><strong>자동화 공장:</strong> 기존의 정형화된 작업만 반복하던 로봇을 넘어, 예상치 못한 상황이나 복잡한 시각적, 언어적 지시를 이해하고 유연하게 대처하는 차세대 스마트 팩토리 구현이 가능해진다.</li>
<li><strong>헬스케어:</strong> 로봇 수술 보조 시스템이 집도의의 미묘한 구두 지시나 수술 부위의 시각적 변화를 즉각적으로 이해하고 반응하거나, 환자 돌봄 로봇이 환자의 상태와 요구를 더 정확하게 파악하여 개인화된 서비스를 제공할 수 있다.</li>
<li><strong>스마트 홈:</strong> 가정용 로봇이 “어질러진 거실 좀 정리해줘“와 같은 모호하고 복합적인 명령을 이해하고, 상황에 맞는 일련의 작업을 자율적으로 수행하는 진정한 의미의 스마트 홈 비서 역할을 할 수 있게 된다.</li>
</ul>
<h3>6.4  결론: RT-2가 로보틱스 연구에 남긴 유산</h3>
<p>RT-2는 그 자체로 완벽한 범용 로봇은 아니지만, 로봇 지능을 개발하는 방식에 대한 근본적인 사고의 전환을 촉발했다는 점에서 그 의의가 매우 크다. 이 모델은 인터넷에 축적된 인류의 방대한 집단 지성을 로봇의 구체적인 물리적 행동으로 연결하는, 실용적이고 효과적인 방법론을 최초로 제시했다. 이를 통해 로보틱스 분야의 오랜 난제였던 데이터 병목 현상을 우회하고, 의미론적 일반화라는 새로운 가능성의 문을 활짝 열었다.</p>
<p>RT-2가 보여준 경이로운 창발적 능력과 동시에 드러난 명확한 한계점들은, 역설적으로 미래 로보틱스 연구가 나아가야 할 길을 더욱 명확하게 비추는 등대 역할을 하고 있다. 새로운 물리적 동작을 어떻게 생성할 것인가(모션 합성), 물리적 세계에서의 안전을 어떻게 보장할 것인가(안전성 및 견고성), 그리고 다양한 로봇의 몸을 넘어선 보편적 지능을 어떻게 구현할 것인가(교차 기종 일반화)와 같은 질문들은 모두 RT-2가 던진 화두에서 비롯된 것이다. 따라서 RT-2는 단순히 하나의 성공적인 모델을 넘어, 로보틱스 연구의 새로운 시대를 여는 중요한 이정표로 역사에 기록될 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>What is RT-2? Google DeepMind’s vision-language-action model for …, https://blog.google/technology/ai/google-deepmind-rt2-robotics-vla-model/</li>
<li>A Survey of Robot Intelligence with Large Language Models - MDPI, https://www.mdpi.com/2076-3417/14/19/8868</li>
<li>arxiv.org, https://arxiv.org/html/2505.04769v1</li>
<li>RT-2: New model translates vision and language into action - Google DeepMind, https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/</li>
<li>(PDF) RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, https://www.researchgate.net/publication/372784419_RT-2_Vision-Language-Action_Models_Transfer_Web_Knowledge_to_Robotic_Control</li>
<li>Vision-Language-Action Models: RT-2, https://robotics-transformer2.github.io/</li>
<li>RT-2: Vision-Language-Action Models Transfer Web … - arXiv, https://arxiv.org/pdf/2307.15818</li>
<li>Vision Language Action Models (VLA) Overview: LeRobot Policies Demo, https://learnopencv.com/vision-language-action-models-lerobot-policy/</li>
<li>kyegomez/RT-2: Democratization of RT-2 “RT-2: New model translates vision and language into action” - GitHub, https://github.com/kyegomez/RT-2</li>
<li>Advances in Transformers for Robotic Applications: A Review - arXiv, https://arxiv.org/html/2412.10599v1</li>
<li>The Transformer Attention Mechanism - MachineLearningMastery.com, https://machinelearningmastery.com/the-transformer-attention-mechanism/</li>
<li>Self-Attention in Transformers: A Deep Dive | by Manish Negi | Medium, https://medium.com/@manishnegi101/self-attention-in-transformers-a-deep-dive-ec1d7eadc390</li>
<li>Transformer Architecture Explained With Self-Attention Mechanism - Codecademy, https://www.codecademy.com/article/transformer-architecture-self-attention-mechanism</li>
<li>Self - Attention in NLP - GeeksforGeeks, https://www.geeksforgeeks.org/nlp/self-attention-in-nlp/</li>
<li>RT2 (Robotics Transformer 2) from DeepMind - YouTube, https://www.youtube.com/watch?v=JF1ySCeed5Q</li>
<li>RT-2: Vision-Language-Action Models Transfer Web Knowledge to …, https://medium.com/@faryal.saud/rt-2-vision-language-action-models-transfer-web-knowledge-to-robotic-control-8cffbd038781</li>
<li>SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning - arXiv, https://arxiv.org/html/2503.03480v1</li>
<li>Large language and vision-language models for robot: safety challenges, mitigation strategies and future directions - Emerald Publishing, https://www.emerald.com/ir/article/doi/10.1108/IR-02-2025-0074/1269979/Large-language-and-vision-language-models-for</li>
<li>Google Deepmind presents RT-2, the first vision-language-action (VLA) Robotics Transformer and it may have drastic implications our future. : r/singularity - Reddit, https://www.reddit.com/r/singularity/comments/15cs38b/google_deepmind_presents_rt2_the_first/</li>
<li>Scaling up learning across many different robot types - Google DeepMind, https://deepmind.google/discover/blog/scaling-up-learning-across-many-different-robot-types/</li>
<li>RT-H: Action Hierarchies Using Language - arXiv, https://arxiv.org/html/2403.01823v1</li>
<li>RVT-2: Learning Precise Manipulation from Few Demonstrations - arXiv, https://arxiv.org/html/2406.08545v1</li>
<li>Enabling robots to follow abstract instructions and complete complex dynamic tasks - arXiv, https://arxiv.org/html/2406.11231v1</li>
<li>[Literature Review] Vision-Language-Action Models: Concepts, Progress, Applications and Challenges - Moonlight, https://www.themoonlight.io/en/review/vision-language-action-models-concepts-progress-applications-and-challenges</li>
<li>(PDF) Vision-Language-Action Models: Concepts, Progress, Applications and Challenges, https://www.researchgate.net/publication/391575814_Vision-Language-Action_Models_Concepts_Progress_Applications_and_Challenges</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>