<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:GoT (Graph of Thoughts)와 VLA(Vision-Language-Action) 모델의 융합</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>GoT (Graph of Thoughts)와 VLA(Vision-Language-Action) 모델의 융합</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">Vision-Language-Action(VLA) 모델</a> / <span>GoT (Graph of Thoughts)와 VLA(Vision-Language-Action) 모델의 융합</span></nav>
                </div>
            </header>
            <article>
                <h1>GoT (Graph of Thoughts)와 VLA(Vision-Language-Action) 모델의 융합</h1>
<p>2025-12-07, G30DR</p>
<h2>1.  서론: 인지적 아키텍처와 신체적 실행의 결합</h2>
<p>인공지능 연구의 최전선은 단순히 텍스트를 생성하거나 정지된 이미지를 분류하는 과업을 넘어, 물리적 세계와 실시간으로 상호작용하며 복잡한 조작(Manipulation)을 수행하는 ’체화된 지능(Embodied Intelligence)’의 구현으로 이동하고 있다. 이러한 흐름의 정점에 있는 것이 바로 <strong>시각-언어-행동(Vision-Language-Action, 이하 VLA)</strong> 모델이다. Google DeepMind의 RT-2(Robotic Transformer 2)나 OpenVLA와 같은 최신 모델들은 인터넷 규모의 방대한 데이터로 학습된 시각-언어 모델(VLM)의 능력을 로봇 제어 영역으로 확장하여, 로봇이 사전에 프로그래밍되지 않은 낯선 물체를 조작하거나 모호한 자연어 명령을 수행하는 데 있어 획기적인 성과를 거두었다.1</p>
<p>그러나 현재의 VLA 모델은 심각한 인지적 한계에 봉착해 있다. 대부분의 VLA 모델은 입력을 행동으로 직접 매핑하는 ‘엔드투엔드(End-to-End)’ 방식을 채택하고 있어, 인간의 뇌로 치면 직관적이고 빠른 반응을 담당하는 ’시스템 1(System 1)’적 사고에 머물러 있다. 이는 “사과를 집어라“와 같은 단순 명령에는 효과적이지만, “탁자 위의 물건들을 정리하되, 깨지기 쉬운 물건은 아래 칸에, 나머지는 위 칸에 넣고, 공간이 부족하면 다시 배치하라“와 같은 다단계 추론과 오류 수정, 대안 탐색이 필요한 ’시스템 2(System 2)’적 과업에서는 빈번히 실패한다.3</p>
<p>이러한 배경에서 **사고의 그래프(Graph of Thoughts, 이하 GoT)**는 VLA 모델의 인지적 깊이를 더해줄 핵심 아키텍처로 부상했다. GoT는 대규모 언어 모델(LLM)의 추론 과정을 선형적인 사슬(Chain)이 아닌, 분기하고 병합하며 순환하는 네트워크(Graph) 형태로 모델링한다. 이는 인간이 복잡한 문제를 해결할 때 뇌에서 일어나는 비선형적 사고 흐름과 유사하며, 로봇에게 ‘계획(Planning)’, ‘반성(Reflection)’, ’수정(Correction)’의 능력을 부여하는 이론적 토대가 된다.3</p>
<p>본 보고서는 GoT와 VLA의 융합이 가져올 로봇 지능의 패러다임 변화를 심층적으로 분석한다. VLA의 기술적 구조와 한계를 명확히 하고, GoT가 이를 어떻게 보완하는지 이론적으로 규명하며, <strong>Embodied Graph-of-Thought (EGoT)</strong>, <strong>Chain-of-Affordance (CoA)</strong>, <strong>ReFineVLA</strong>, <strong>Visualizing Thought</strong> 등 최신 연구 사례를 통해 구체적인 구현 방법론과 성과를 상세히 논한다. 나아가 이 융합 기술이 직면한 데이터 희소성, 추론 지연, 토큰화의 딜레마 등 기술적 난제들을 분석하고 미래 발전 방향을 제시한다.</p>
<pre><code class="language-mermaid">graph TD

subgraph "GoT-VLA 융합 모델 (System 2)"
    A2["입력: '복잡한 정리 정돈'"] --&gt; B2["GoT 기반 추론 (Planning)"]
    B2 --&gt; C2["대안 탐색 및 시뮬레이션"]
    C2 --&gt; D2{"검증 (Reflection)"}
    D2 -- "오류 발견" --&gt; B2
    D2 -- "계획 확정" --&gt; E2["최적 행동 실행"]
    E2 --&gt; F2["복잡 과업 성공"]
end

subgraph "기존 VLA 모델 (System 1)"
    A1["입력: '사과를 집어라'"] --&gt; B1["엔드투엔드 처리 (End-to-End)"]
    B1 --&gt; C1["즉각적 행동 생성 (Greedy Decoding)"]
    C1 --&gt; D1["단순 과업 성공"]
    C1 -.-&gt; E1["복잡 과업 실패 (오류 수정 불가)"]
end

</code></pre>
<h2>2.  시각-언어-행동(VLA) 모델의 기술적 토대와 구조적 한계</h2>
<pre><code class="language-mermaid">graph TD

subgraph "입력 처리 (Tokenization)"
    IMG["카메라 이미지 (Visual Tokens)"] --&gt; UTS["통합된 토큰 스트림 (Unified Token Stream)"]
    TXT["명령어 텍스트 (Text Tokens)"] --&gt; UTS
end

UTS --&gt; TR["트랜스포머 (Transformer)"]

TR --&gt; OUT["출력: 텍스트 + 행동 토큰 (Action Tokens 0-255)"]

subgraph "구조적 한계 (Linear Trap)"
    OUT --&gt; ERR1["백트래킹 부재 (되돌아가기 불가)"]
    OUT --&gt; ERR2["장기 계획 결여 (Greedy)"]
    OUT --&gt; ERR3["환각 (물리적 불가능 행동 생성)"]
end

</code></pre>
<h3>2.1  VLA 모델의 아키텍처: 통합된 토큰 스트림</h3>
<p>VLA 모델의 핵심 혁신은 로봇의 물리적 행동을 자연어 처리(NLP) 문제로 환원했다는 점에 있다. 기존의 로봇 제어 시스템이 <code>인식(Perception) -&gt; 계획(Planning) -&gt; 제어(Control)</code>의 모듈화된 파이프라인을 따랐다면, VLA는 이를 하나의 거대한 트랜스포머(Transformer) 신경망으로 통합했다.</p>
<h4>2.1.1  행동의 토큰화 (Action Tokenization)</h4>
<p>RT-2와 같은 모델은 로봇 팔의 관절 각도나 엔드 이펙터(End-effector)의 위치 변화량(Delta)과 같은 연속적인 실수값(Continuous Value)을 이산적인 토큰(Discrete Token)으로 변환한다. 일반적으로 7자유도(DoF) 로봇 팔의 각 축의 움직임을 0부터 255 사이의 정수 구간(Bin)으로 나누어, 이를 텍스트 어휘 사전(Vocabulary)에 추가한다.6</p>
<ul>
<li><strong>입력</strong>: 카메라 이미지 패치(Visual Tokens) + 사용자 명령어 텍스트(Text Tokens)</li>
<li><strong>처리</strong>: 트랜스포머의 어텐션 메커니즘을 통한 통합 처리</li>
<li><strong>출력</strong>: “나는 컵을 집을 것이다.” (Text) + <code>  ...</code> (Action Tokens)</li>
</ul>
<p>이러한 <strong>통합된 토큰 스트림(Unified Token Stream)</strong> 구조는 인터넷상의 방대한 텍스트와 이미지 데이터(Web Scale Data)를 통해 학습된 일반 상식과 추론 능력을 로봇 제어 데이터(Robotics Data)와 결합할 수 있게 해준다.7 예를 들어, 로봇은 로봇 데이터셋에 없는 “슈퍼맨 피규어“를 본 적이 없더라도, 인터넷 데이터를 통해 그것이 무엇인지 알고 “슈퍼맨을 집어라“는 명령에 반응할 수 있게 된다.</p>
<pre><code class="language-mermaid">graph TB
    subgraph "물리적 세계 (Continuous)"
        Motion["로봇 팔 움직임 (Continuous Values)"]
        Delta["위치 변화량 / 관절 각도"]
    end

    subgraph "이산화 (Discretization)"
        Binning["0~255 정수 구간 (Binning)"]
        TokenID["Action Token ID 부여"]
    end

    subgraph "언어 모델 공간 (Token Space)"
        Vocab["어휘 사전 (Vocabulary)"]
        Unified["통합된 토큰 스트림"]
    end

    Motion --&gt;|"센서 측정"| Delta
    Delta --&gt;|"구간 매핑"| Binning
    Binning --&gt;|"인덱싱"| TokenID
    TokenID --&gt;|"확장 (Expansion)"| Vocab
    Vocab --&gt;|"텍스트+행동 결합"| Unified
</code></pre>
<h3>2.2  시스템 1의 한계: 선형적 추론의 덫</h3>
<p>VLA의 이러한 엔드투엔드 방식은 강력하지만, 근본적으로 <strong>자기회귀적(Auto-regressive) 생성</strong> 방식에 의존한다. 모델은 이전 토큰을 바탕으로 다음 토큰을 확률적으로 예측하며, 이 과정은 시간축을 따라 선형적으로 진행된다.</p>
<ul>
<li><strong>백트래킹의 부재</strong>: 로봇이 복잡한 조립 작업을 수행하다가 중간 단계에서 실수를 범했을 때, 선형적 모델은 이전 상태로 돌아가(Backtrack) 다른 경로를 시도하는 메커니즘이 부재하다. 단순히 학습된 확률 분포에 따라 계속해서 다음 행동을 생성하려다 실패의 늪에 빠지게 된다.3</li>
<li><strong>전략적 계획의 결여</strong>: “체스“를 두는 것과 같이 여러 수 앞을 내다보고 현재의 최적 행동을 결정해야 하는 장기 계획(Long-horizon Planning) 과업에서, VLA는 현재 눈에 보이는 상태에 대한 즉각적인 반응(Greedy Decoding)에 치중하는 경향이 있다.</li>
<li><strong>환각(Hallucination)과 물리적 불가능성</strong>: 언어 모델과 마찬가지로 VLA 또한 물리적으로 불가능한 행동(예: 벽을 뚫고 지나가려는 시도)을 그럴듯하게 생성할 위험이 있다. 이를 검증하고 필터링할 내부 루프가 선형 구조에는 없다.8</li>
</ul>
<h2>3.  사고의 그래프(GoT): 인지적 유연성의 수학적 모델링</h2>
<p>GoT는 LLM의 추론 단위를 그래프의 **노드(Node)**로, 추론의 전이 과정을 **엣지(Edge)**로 정의하여, 임의의 유향 그래프(Directed Graph) 구조 내에서 최적의 해를 탐색하는 프롬프팅 및 아키텍처 방법론이다. 이는 기존의 사고 사슬(Chain-of-Thought, CoT)이나 사고 트리(Tree-of-Thoughts, ToT)를 포괄하고 확장한 개념이다.3</p>
<pre><code class="language-mermaid">graph TB
N1["사고 노드 (Thought Node)"]

subgraph "GoT의 3대 핵심 연산"
    OP1["병합 (Aggregation)"] 
    OP2["정제 (Refinement)"]
    OP3["생성 (Generation)"]
end

N1 --&gt; OP3
OP3 --&gt;|"탐색 공간 확장"| BN1["분기 노드 A"]
OP3 --&gt;|"탐색 공간 확장"| BN2["분기 노드 B"]

BN1 --&gt; OP2
BN2 --&gt; OP2
OP2 --&gt;|"순환 루프 (Self-correction)"| BN1

BN1 --&gt; OP1
BN2 --&gt; OP1
OP1 --&gt;|"최적 해 도출"| RES["통합된 계획"]

RES --&gt;|"동형 매핑 (Isomorphic)"| ROB["로봇 물리적 상태 공간"]
</code></pre>
<h3>3.1  구조적 비교와 변환 연산</h3>
<p>GoT는 다음과 같은 핵심 연산을 통해 인간의 사고 과정을 모사한다:</p>
<table><thead><tr><th><strong>구조적 방법론</strong></th><th><strong>구조 (Topology)</strong></th><th><strong>핵심 특징</strong></th><th><strong>로봇/VLA 적용 시 한계 및 장점</strong></th></tr></thead><tbody>
<tr><td><strong>Chain-of-Thought (CoT)</strong></td><td><span class="math math-inline">A \rightarrow B \rightarrow C</span> (선형)</td><td>단계적 추론</td><td>단순 작업에 효율적이나, 오류 복구가 불가능함.</td></tr>
<tr><td><strong>Tree-of-Thoughts (ToT)</strong></td><td><span class="math math-inline">A \rightarrow \{B1, B2\} \rightarrow \dots</span> (트리)</td><td>대안 탐색, 가지치기</td><td>여러 계획을 비교 가능하나, 서로 다른 계획을 결합하지 못함.</td></tr>
<tr><td><strong>Graph-of-Thoughts (GoT)</strong></td><td>네트워크 (순환, 병합 포함)</td><td><strong>병합(Aggregation)</strong>, <strong>순환(Loop)</strong>, <strong>되감기(Backtracking)</strong></td><td>복잡한 협업, 장기 계획, 오류 수정에 최적화됨.</td></tr>
</tbody></table>
<ul>
<li><strong>병합(Aggregation)</strong>: 서로 다른 추론 경로(예: 시각적 분석 결과와 언어적 지시 해석)를 하나의 노드로 합쳐 더 강력한 해를 도출한다. VLA에서는 “오른손의 계획“과 “왼손의 계획“을 합쳐 “양손 협업 계획“을 만드는 데 사용될 수 있다.9</li>
<li><strong>정제(Refinement)</strong>: 순환 루프(Self-correction Loop)를 통해 동일한 사고 노드를 반복적으로 개선한다. 로봇이 물체를 잡는 자세(Grasp Pose)를 시뮬레이션해보고, 불안정할 경우 다시 수정하는 과정을 그래프상의 루프로 구현한다.</li>
<li><strong>생성(Generation)</strong>: 하나의 노드에서 여러 개의 후속 노드를 생성하여 탐색 공간을 확장한다.</li>
</ul>
<pre><code class="language-mermaid">graph TD

subgraph "Graph-of-Thoughts (GoT)"
    G1["Node A"] --&gt; G2["Node B1"]
    G1 --&gt; G3["Node B2"]
    G2 --&gt; G4["Aggregation (병합)"]
    G3 --&gt; G4
    G4 --&gt;|"Loop (순환/수정)"| G1
    G4 --&gt;|"Refinement"| G_End["Complex Result"]
end

subgraph "Tree-of-Thoughts (ToT)"
    T1["Node A"] --&gt; T2["Node B1"]
    T1 --&gt; T3["Node B2"]
    T2 --&gt; T4["Node C1"]
    T3 --&gt; T5["Node C2"]
    T4 --&gt;|"가지치기 (Pruning)"| T_End["Result"]
end

subgraph "Chain-of-Thought (CoT)"
    C1["Node A"] --&gt; C2["Node B"]
    C2 --&gt; C3["Node C"]
    C3 --&gt;|"선형적 흐름/오류 복구 불가"| C_End["Result"]
end
</code></pre>
<h3>3.2  로봇 공학에서의 필연성</h3>
<p>로봇의 작업 계획(Task Planning)은 본질적으로 상태 공간(State Space)에서의 그래프 탐색 문제이다. GoT는 LLM 내부의 잠재 공간(Latent Space)에서 이루어지는 언어적/의미론적 추론을 로봇의 물리적 상태 공간 그래프와 동형(Isomorphic)으로 만들어준다. 즉, GoT-VLA 융합은 <strong>“언어로 생각하고(Reasoning), 그래프로 계획하며(Planning), 토큰으로 행동하는(Acting)”</strong> 통합된 인지 시스템을 구축하는 과정이다.10</p>
<h2>4.  GoT와 VLA의 융합: 핵심 구현 방법론 및 사례 분석</h2>
<p>GoT의 추상적 개념을 VLA라는 구체적인 모델에 적용하는 방식은 크게 세 가지 흐름으로 나타난다: (1) 다중 로봇/신체 제어를 위한 명시적 그래프 구성(EGoT), (2) 물리적 어포던스 기반의 추론 사슬 강화(CoA), (3) 그래프 추론 과정의 내재화 및 증류(ReFineVLA).</p>
<h3>4.1  Embodied Graph-of-Thought (EGoT)와 ET-VLA: 다중 신체의 조율</h3>
<p><strong>ET-VLA (Embodiment Transfer Learning for Vision-Language-Action Models)</strong> 연구에서 제안된 <strong>EGoT</strong>는 GoT를 로봇의 신체 구조(Embodiment) 문제에 특화시킨 기술이다. 기존 VLA 모델들이 단일 팔(Unimanual) 데이터로 학습되어 양팔(Bimanual)이나 다중 로봇 협업 시 역할 분담에 실패한다는 점에 착안했다.11</p>
<pre><code class="language-mermaid">graph TD        
    subgraph "노드 구조 (Node Definition)"
        NODE["노드 N_i"]
        ATTR1["Who: 신체 ID (Embodiment)"]
        ATTR2["What: 작업 (Task)"]
        ATTR3["How: 파라미터"]
        NODE --- ATTR1
        NODE --- ATTR2
        NODE --- ATTR3
    end

    subgraph "실행 흐름 (Graph Execution)"
        START["작업 시작"] --&gt; LEFT["Node A: 왼손 (컵 잡기)"]
        START --&gt; RIGHT["Node B: 오른손 (주전자 들기)"]

        LEFT --&gt; DEP["의존성 (Edge): A 완료 후 B 진행"]
        DEP --&gt; SYNC["동기화 및 협업"]
        SYNC --&gt; ACT["물 따르기 실행"]
    end

    subgraph "학습 전략"
        SINGLE["단일 로봇 데이터"] --&gt; SCP["합성 지속 사전학습 (SCP)"]
        SCP --&gt; GEN["협업 시나리오 생성"] --&gt; MODEL["ET-VLA 모델"]
    end
</code></pre>
<h4>4.1.1  EGoT의 노드 및 엣지 설계</h4>
<p>EGoT에서 그래프의 각 노드는 단순한 사고의 상태가 아니라, <strong>“누가(Who), 무엇을(What), 어떻게(How)”</strong> 수행할지를 정의하는 실행 가능한 서브 태스크(Sub-task) 단위이다.</p>
<ul>
<li><strong>노드 정의 (<span class="math math-inline">N</span>)</strong>: <span class="math math-inline">N_i = \{Task\_Description, Embodiment\_ID, Action\_Parameters, Preconditions\}</span></li>
<li>예: <code>Node_A = {Task: "병 잡기", Embodiment: "Left_Arm",...}</code>, <code>Node_B = {Task: "뚜껑 열기", Embodiment: "Right_Arm",...}</code></li>
<li><strong>엣지 정의 (<span class="math math-inline">E</span>)</strong>: 작업의 시간적 순서와 의존성. <code>Node_A</code>가 완료되어야 <code>Node_B</code>가 실행될 수 있음을 나타낸다.</li>
</ul>
<h4>4.1.2  합성 지속 사전학습 (Synthetic Continued Pretraining, SCP)</h4>
<p>EGoT의 효과적인 작동을 위해 연구진은 SCP라는 방법론을 도입했다. 실제 다중 로봇 데이터를 수집하는 것은 비용이 많이 들기 때문에, 단일 로봇 데이터를 바탕으로 합성된 협업 시나리오 데이터를 생성하여 모델을 사전 학습시킨다.12 이 과정에서 모델은 “왼팔이 A를 하는 동안 오른팔은 대기하거나 B를 한다“는 식의 동시성 제어 토큰 생성을 학습한다.</p>
<pre><code class="language-mermaid">graph TD
    subgraph "Data Source"
        Single["단일 로봇 데이터 (Single Arm)"]
    end

    subgraph "Synthetic Generation Process"
        Logic["협업 로직 합성기"]
        Single --&gt; Logic
        Logic --&gt;|"Role Assignment"| Role["역할 분담 (Left/Right)"]
        Logic --&gt;|"Time Sync"| Sync["동시성 제어 토큰 삽입"]
    end

    subgraph "Output Dataset"
        Scenario["합성된 협업 시나리오"]
        Role --&gt; Scenario
        Sync --&gt; Scenario
    end

    subgraph "Training"
        Scenario --&gt;|"Supervised Learning"| Model["ET-VLA Model"]
    end
</code></pre>
<h4>4.1.3  실험적 성과 및 시사점</h4>
<p>실제 로봇(Dual UR5 Arms) 실험에서 EGoT가 적용된 ET-VLA는 OpenVLA 대비 6개의 복잡한 조작 작업에서 평균 <strong>53.2%</strong> 더 높은 성공률을 기록했다.11</p>
<ul>
<li><strong>시나리오</strong>: “왼손으로 컵을 잡고, 오른손으로 주전자를 들어 물을 따라라.”</li>
<li><strong>분석</strong>: 기존 모델은 두 팔을 동시에 움직이려다 충돌하거나 순서를 혼동했으나, EGoT는 이를 그래프상의 별도 노드로 분리하고 의존성을 설정함으로써 명확한 역할 분담과 순차적 실행을 달성했다. 이는 GoT 구조가 로봇의 물리적 신체 확장(Embodiment Expansion)에 필수적인 인지적 프레임워크임을 증명한다.</li>
</ul>
<h3>4.2  Chain-of-Affordance (CoA-VLA): 기하학적 사고의 사슬</h3>
<p><strong>CoA-VLA</strong>는 GoT의 개념을 로봇이 환경과 상호작용하는 **어포던스(Affordance)**의 관점에서 재해석했다. 단순한 텍스트 추론이 아니라, 이미지 내의 구체적인 좌표와 영역을 추론의 단계로 포함시킨다.13</p>
<pre><code class="language-mermaid">graph TD
STEP1["1단계: 객체 어포던스 (Semantic)"] --&gt;|"무엇을?"| STEP2["2단계: 파지 어포던스 (Part-level)"]
STEP2 --&gt;|"어디를?"| STEP3["3단계: 공간 어포던스 (Free Space)"]

STEP3 -- "공간 부족 (충돌 감지)" --&gt; BACK["백트래킹 (Backtracking)"]
BACK --&gt;|"방해물 치우기 목표 설정"| STEP1

STEP3 -- "공간 확보" --&gt; STEP4["4단계: 동작 어포던스 (Trajectory)"]
STEP4 --&gt;|"경로 생성"| EXEC["로봇 행동 실행"]

subgraph "성과"
    RES1["Unseen Object 대응"]
    RES2["방해물 회피 능력 향상"]
end
EXEC --- RES1
EXEC --- RES2
</code></pre>
<h4>4.2.1  4단계 어포던스 체인 메커니즘</h4>
<p>CoA-VLA는 행동 생성 이전에 다음 4가지 어포던스를 순차적(Chain)이면서도 의존적(Graph-like)으로 추론한다.14 이 과정은 사실상 GoT의 ’Generation’과 ‘Refinement’ 과정을 포함한다.</p>
<ol>
<li><strong>객체 어포던스 (Object Affordance)</strong>: “무엇을 조작해야 하는가?” (Semantic to Visual Mapping).</li>
<li><strong>파지 어포던스 (Grasp Affordance)</strong>: “어디를 잡아야 하는가?” (Part-level Understanding). 객체의 손잡이 등 구체적 부위의 좌표를 예측한다.</li>
<li><strong>공간 어포던스 (Spatial Affordance)</strong>: “어디로 이동/배치해야 하는가?” (Free Space Identification). 충돌 없는 배치 공간을 탐색한다.</li>
<li><strong>동작 어포던스 (Motion Affordance)</strong>: “어떤 경로로 움직여야 하는가?” (Trajectory Planning).</li>
</ol>
<h4>4.2.2  그래프적 특성과 피드백 루프</h4>
<p>CoA는 명목상 ’Chain’이지만, 실제로는 그래프적 성격을 띤다. 예를 들어, 3단계(공간 어포던스)에서 물건을 놓을 공간이 없다고 판단되면, 모델은 즉시 행동을 멈추고 1단계로 돌아가(Backtracking) “방해물을 먼저 치운다“는 새로운 하위 목표를 설정한다. 이러한 **동적 의존성 해결(Dynamic Dependency Resolution)**이 GoT의 핵심이다.</p>
<h4>4.2.3  성능 데이터</h4>
<p>시뮬레이션 및 실제 로봇 실험(쓰레기 치우기, 차 대접하기 등)에서 CoA-VLA는 기존 OpenVLA 대비 <strong>14.29%</strong> 높은 정확도를 보였으며, 전체 성공률 **79.8%**를 달성했다.14 특히 보지 못한 물체 자세(Unseen Object Poses)나 방해물이 있는 환경에서의 일반화 성능이 두드러졌다. 이는 추론 과정에 ’공간적/기하학적 제약’을 명시적인 노드로 포함시킨 결과이다.</p>
<h3>4.3  ReFineVLA: 추론의 증류(Distillation)와 내재화</h3>
<p><strong>ReFineVLA</strong>는 GoT와 같은 복잡한 추론 과정이 실시간 제어에 너무 많은 비용(시간, 연산량)을 요구한다는 현실적 문제에서 출발한다. 따라서 GoT를 실행 시간(Inference Time)에 돌리는 대신, 학습 시간(Training Time)에 활용하여 모델 자체를 똑똑하게 만드는 전략을 취한다.16</p>
<pre><code class="language-mermaid">graph TD       
subgraph "ReFineVLA: 추론의 증류 (Distillation)"
    TEACHER["교사 모델 (GPT-4V)"] --&gt;|"깊은 추론 (Reasoning)"| DATA["데이터: (이유 + 행동) 쌍"]
    DATA --&gt;|"교사-학생 학습"| STUDENT["학생 모델 (OpenVLA)"]
    STUDENT --&gt;|"Reasoning-Aware Fine-Tuning"| FAST["실시간 추론 (상위 레이어만 조정)"]
end
</code></pre>
<pre><code class="language-mermaid">sequenceDiagram
    participant Teacher as "Teacher (GPT-4V)"
    participant Data as "Instruction Data"
    participant Student as "Student (VLA Policy)"
    participant Robot as "Physical Robot"

    Note over Teacher, Data: "Training Phase (Distillation)"
    Teacher-&gt;&gt;Data: "Analyze Complex Scene"
    Teacher-&gt;&gt;Data: "Generate Chain of Thought (Rationale)"
    Teacher-&gt;&gt;Data: "Generate Optimal Action"
    
    Data-&gt;&gt;Student: "Input: Image + Instruction"
    Data-&gt;&gt;Student: "Target: Rationale + Action"
    Student-&gt;&gt;Student: "Fine-tune Upper Layers Only"

    Note over Student, Robot: "Inference Phase (Real-time)"
    Robot-&gt;&gt;Student: "Current Observation"
    Student-&gt;&gt;Student: "Internalize Reasoning (Implicit)"
    Student-&gt;&gt;Robot: "Fast Action Execution"
</code></pre>
<h4>4.3.1  교사-학생 학습 (Teacher-Student Learning)</h4>
<ul>
<li><strong>교사 모델 (Teacher)</strong>: GPT-4V와 같은 초거대 모델을 사용하여, 복잡한 상황에 대해 GoT 방식의 깊은 추론을 수행하고, 그 결과로 상세한 **‘이유(Rationale)’**와 <strong>‘행동(Action)’</strong> 쌍을 생성한다.</li>
<li><strong>학생 모델 (Student)</strong>: 실제 로봇에 탑재될 VLA 모델(예: OpenVLA 기반)은 이 데이터를 학습한다. 이때 단순히 행동만 모방하는 것이 아니라, 행동을 하기 전에 자연어로 그 ’이유’를 설명하도록 학습된다(Reasoning-Aware Fine-Tuning).</li>
</ul>
<h4>4.3.2  선택적 전이 미세조정 (Selective Transfer Fine-Tuning)</h4>
<p>ReFineVLA는 VLA 모델의 모든 파라미터를 업데이트하는 대신, 추론과 관련된 상위 레이어(Upper Layers)만 선택적으로 미세 조정하여, 사전 학습된 시각적 특징 추출 능력(General Features)을 보존하면서도 추론 능력을 주입한다.18</p>
<p>이 방식은 GoT의 사고 과정을 모델의 직관(Weights)으로 압축하는 과정으로 볼 수 있다. 결과적으로 ReFineVLA는 별도의 그래프 탐색 없이도 복잡한 상황에서 GoT와 유사한 수준의 판단을 훨씬 빠른 속도로 내릴 수 있게 된다.17</p>
<h3>4.4  Visualizing Thought: 시각적 다이어그램을 통한 그래프 추론</h3>
<p>가장 최근의 연구인 <strong>Visualizing Thought</strong>는 텍스트 중심의 GoT를 시각적 영역으로 확장하여, 로봇의 물리적 계획 능력을 극대화한다.19</p>
<pre><code class="language-mermaid">graph TD       
subgraph "Visualizing Thought: 시각적 사고"
    GEN["텍스트 계획 생성"] --&gt; DIAG["개념적 다이어그램 생성 (SVG/Code)"]
    DIAG --&gt; VERI{"시각적 검증 (Visual Verification)"}
    VERI -- "물리적 불가능" --&gt; PRUNE["가지치기 (Pruning)"]
    VERI -- "가능" --&gt; ACCEPT["다음 단계 진행 (Beam Search)"]
    ACCEPT --&gt; SUCCESS["Blocksworld 성공률 78%"]
end
</code></pre>
<pre><code class="language-mermaid">graph TD
    Root["현재 상태 (Start)"] --&gt; Gen["행동 생성 (Generation)"]
    
    subgraph "Beam Search Process"
        Gen --&gt; Path1["경로 A"]
        Gen --&gt; Path2["경로 B"]
        Gen --&gt; Path3["경로 C"]
        
        Path1 --&gt; Draw1["다이어그램 그리기 A"]
        Path2 --&gt; Draw2["다이어그램 그리기 B"]
        Path3 --&gt; Draw3["다이어그램 그리기 C"]
        
        Draw1 --&gt; Check1{"물리적 검증"}
        Draw2 --&gt; Check2{"물리적 검증"}
        Draw3 --&gt; Check3{"물리적 검증"}
    end
    
    Check1 -- "불가능 (공중 부양 등)" --&gt; Prune1["가지치기 (Pruning)"]
    Check2 -- "가능 (Valid)" --&gt; Keep2["보존 (Keep)"]
    Check3 -- "충돌 위험" --&gt; Prune3["가지치기 (Pruning)"]
    
    Keep2 --&gt; Next["다음 단계 계획 수립"]
</code></pre>
<h4>4.4.1  개념적 다이어그램 노드 (Conceptual Diagram Nodes)</h4>
<p>이 연구는 텍스트만으로는 복잡한 공간 관계(예: 블록 A가 B 위에 있고 C가 A 옆에 있음)를 정확히 추적하기 어렵다는 점을 지적한다. 따라서 GoT의 각 노드에 **‘개념적 다이어그램(Conceptual Diagram)’**을 생성하여 포함시킨다. 이는 모델이 자신의 머릿속에 있는 현재 상태를 간단한 그림(SVG, Matplotlib 코드 등)으로 그려보는 것과 같다.</p>
<h4>4.4.2  빔 서치와 시각적 검증</h4>
<ul>
<li><strong>빔 서치 (Beam Search)</strong>: 현재 상태에서 가능한 다음 행동들을 여러 개 생성(Generation)한다.</li>
<li><strong>시각적 검증 (Visual Verification)</strong>: 생성된 행동의 결과 상태를 다이어그램으로 그리고, 이 그림이 물리적으로 가능한지(예: 블록이 공중에 떠 있지 않은지)를 시각적으로 검증한다.</li>
<li><strong>백트래킹</strong>: 검증에 실패한 노드는 가지치기하고, 유효한 노드들만 선택하여 다음 단계로 넘어간다.19</li>
</ul>
<h4>4.4.3  성능 비교</h4>
<p>Blocksworld(Hard)와 같은 고난이도 계획 과제에서 기존의 텍스트 기반 추론(RAP)이 **4%**의 성공률에 그친 반면, Visualizing Thought는 **78%**라는 압도적인 성공률을 기록했다.19 이는 VLA 모델이 ’상상(Visual Imagination)’을 통해 계획을 검증하는 GoT의 강력한 잠재력을 보여준다.</p>
<h2>5.  비교 분석: 방법론별 특성 및 성능 요약</h2>
<p>아래 표는 앞서 논의한 세 가지 주요 방법론(EGoT, CoA-VLA, ReFineVLA/Visualizing Thought)의 특성을 비교 요약한 것이다.</p>
<table><thead><tr><th><strong>구분</strong></th><th><strong>EGoT (ET-VLA)</strong></th><th><strong>CoA-VLA</strong></th><th><strong>ReFineVLA</strong></th><th><strong>Visualizing Thought</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 초점</strong></td><td>다중 로봇/신체 협업 및 역할 분담</td><td>기하학적 어포던스 및 공간 추론</td><td>추론 과정의 효율적 내재화 (경량화)</td><td>시각적 상태 추적 및 물리적 검증</td></tr>
<tr><td><strong>노드 구성</strong></td><td>서브태스크 + 신체(Embodiment) ID</td><td>객체 <span class="math math-inline">\rightarrow</span> 파지 <span class="math math-inline">\rightarrow</span> 공간 <span class="math math-inline">\rightarrow</span> 이동 (4단계)</td><td>자연어 이유(Rationale) + 행동</td><td>텍스트 상태 설명 + 개념적 다이어그램</td></tr>
<tr><td><strong>추론 방식</strong></td><td>명시적 그래프 탐색 및 할당</td><td>순차적 의존성 및 피드백 루프</td><td>교사 모델의 추론 결과 모방 (암묵적)</td><td>빔 서치(Beam Search) 및 깊이 우선 탐색</td></tr>
<tr><td><strong>주요 성과</strong></td><td>OpenVLA 대비 협업 과제 +53.2%</td><td>기준 모델 대비 +14.29% 정확도</td><td>환경 변화에 강인한(Robust) 일반화</td><td>복잡한 계획 과제(Blocksworld) 78% 달성</td></tr>
<tr><td><strong>적용 시나리오</strong></td><td>양팔 로봇 조립, 다중 로봇 물류</td><td>정밀 조작, 정리 정돈, 장애물 회피</td><td>실시간 제어가 필요한 모든 동적 환경</td><td>장기 계획이 필요한 퍼즐, 복합 조립</td></tr>
</tbody></table>
<h2>6.  도전 과제와 미래 전망</h2>
<p>GoT와 VLA의 융합은 로봇 지능의 새로운 지평을 열었지만, 여전히 해결해야 할 중대한 기술적 난제들이 남아 있다.</p>
<pre><code class="language-mermaid">mindmap
  root(("6. 도전 과제 및 미래 전망"))
    ("추론 지연 (Latency)")
      ("문제: 높은 계산 비용")
      ("해결: 모델 경량화 (TinyVLA)")
      ("해결: 하드웨어 가속 (NPU)")
      ("해결: 증류 (Distillation)")
    ("데이터 희소성 (Data Scarcity)")
      ("문제: 사고 과정(Thought Trace) 데이터 부족")
      ("해결: Sim-to-Real")
      ("해결: Self-Play (자가 생성)")
      ("해결: 합성 데이터 (SCP)")
    ("신경-상징적 통합 (Neuro-symbolic)")
      ("문제: 환각 및 논리 오류")
      ("해결: 지식 그래프 (Knowledge Graph) 결합")
      ("해결: 형식 논리 (Formal Logic) 도입")
      ("해결: 하이브리드 아키텍처 (NaviWM)")
</code></pre>
<h3>6.1  추론 지연(Latency)과 실시간성의 충돌</h3>
<p>GoT는 본질적으로 여러 경로를 탐색하고 평가해야 하므로 계산 비용이 높다. 로봇 제어 루프는 보통 10Hz~50Hz 이상의 빈도로 작동해야 하는데, LLM이 그래프를 탐색하며 수 초(Seconds)를 소모하는 것은 치명적이다.</p>
<ul>
<li><strong>전망</strong>: ReFineVLA와 같이 오프라인에서 ’깊게 생각’하고 온라인에서는 ’빠르게 행동’하는 <strong>증류(Distillation)</strong> 기법이 더욱 고도화될 것이다. 또한, <strong>TinyVLA</strong>나 <strong>Fast-VLA</strong>와 같이 경량화된 모델이나 전용 하드웨어 가속기(NPU)의 발전이 필수적이다.20</li>
</ul>
<h3>6.2  데이터 희소성(Data Scarcity)과 사고의 흔적</h3>
<p>“사과를 집어라“와 같은 (명령, 행동) 쌍의 데이터는 많지만, “사과를 집기 위해 장애물을 치우고 손목을 비틀어야 한다“는 식의 **사고 과정(Thought Trace)**이 포함된 데이터는 극히 드물다. GoT 기반 VLA를 학습시키기 위해서는 이러한 사고 데이터가 필수적이다.</p>
<ul>
<li><strong>전망</strong>: ET-VLA의 SCP와 같이 시뮬레이션 환경에서 자동으로 사고 과정을 생성하고 검증하여 데이터를 증강하는 <strong>Sim-to-Real</strong> 및 <strong>Self-Play</strong> 기술이 핵심이 될 것이다.12</li>
</ul>
<h3>6.3  신경-상징적(Neuro-symbolic) 통합의 필요성</h3>
<p>VLA의 환각 문제를 해결하고 논리적 무결성을 보장하기 위해, GoT 구조에 형식 논리(Formal Logic)나 지식 그래프(Knowledge Graph)를 명시적으로 결합하는 시도가 필요하다. 22에서 언급된 <strong>NaviWM</strong>이나 <strong>KGoT</strong>23와 같이, 딥러닝의 유연함과 심볼릭 AI의 정확성을 결합한 하이브리드 아키텍처가 발전할 것이다.</p>
<h2>7.  결론</h2>
<p>사고의 그래프(GoT)와 시각-언어-행동(VLA) 모델의 결합은 로봇 공학의 오랜 난제였던 **‘인식과 행동 사이의 인지적 간극’**을 메우는 결정적인 열쇠이다. VLA가 로봇에게 세상을 보고 이해할 수 있는 ’눈’과 ’직관’을 주었다면, GoT는 그 직관 위에 논리적 구조, 계획, 그리고 반성의 능력을 갖춘 ’전두엽’을 구축하고 있다.</p>
<p>본 보고서의 분석에 따르면, 이 융합은 단순히 두 기술을 이어 붙인 것이 아니라, 로봇의 제어 아키텍처를 근본적으로 재편하고 있다. EGoT를 통해 로봇은 자신의 신체를 인지적으로 분리하여 협업할 수 있게 되었고, CoA-VLA를 통해 기하학적 공간을 논리적으로 탐색하게 되었으며, ReFineVLA를 통해 숙고의 과정을 내재화하고 있다.</p>
<p>향후 이 기술은 제조 현장의 비정형 조립 작업, 가정 내 가사 도우미 로봇, 그리고 재난 현장의 구조 로봇 등 고도의 자율성과 판단력이 요구되는 분야에서 핵심적인 역할을 수행할 것이다. GoT 기반 VLA는 로봇을 단순한 자동화 기계에서, 인간과 함께 문제를 고민하고 해결책을 찾아가는 진정한 의미의 **‘인지적 파트너(Cognitive Partner)’**로 진화시킬 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>What are Vision Language Action (VLA) Models? Complete Guide - Articsledge, https://www.articsledge.com/post/vision-language-action-vla-models</li>
<li>RT-2: New model translates vision and language into action - Google DeepMind, https://deepmind.google/blog/rt-2-new-model-translates-vision-and-language-into-action/</li>
<li>Graph of Thoughts: Solving Elaborate Problems with Large …, https://ojs.aaai.org/index.php/AAAI/article/view/29720/31236</li>
<li>GoT: Effective Graph-of-Thought Reasoning in Language Models - ResearchGate, https://www.researchgate.net/publication/382628005_GoT_Effective_Graph-of-Thought_Reasoning_in_Language_Models</li>
<li>(PDF) A Survey of Task Planning with Large Language Models - ResearchGate, https://www.researchgate.net/publication/391333689_A_Survey_of_Task_Planning_with_Large_Language_Models</li>
<li>RT-2-X Model: Unified Robotic Control, https://www.emergentmind.com/topics/rt-2-x-model</li>
<li>Vision-Language-Action Models: Concepts, Progress, Applications and Challenges - arXiv, https://arxiv.org/html/2505.04769v1</li>
<li>Daily Papers - Hugging Face, <a href="https://huggingface.co/papers?q=hypertree-structured+planning+outlines">https://huggingface.co/papers?q=hypertree-structured%20planning%20outlines</a></li>
<li>X-of-Thought: 3 Variants of the s Chain of Thoughts (CoT) , ToT, GoT, LoT | by Joyce Birkins, https://medium.com/@joycebirkins/x-of-thought-3-variants-of-the-s-chain-of-thoughts-cot-tot-got-lot-11a529d09dc1</li>
<li>CLIMB: Language-Guided Continual Learning for Task Planning with Iterative Model Building - arXiv, https://arxiv.org/html/2410.13756v1</li>
<li>Embodiment Transfer Learning for Vision-Language-Action Models - arXiv, https://arxiv.org/html/2511.01224v1</li>
<li>Embodiment Transfer Learning for Vision-Language-Action Models - ResearchGate, https://www.researchgate.net/publication/397232748_Embodiment_Transfer_Learning_for_Vision-Language-Action_Models</li>
<li>CoA-VLA: Improving Vision-Language-Action Models via Visual-Textual Chain-of-Affordance - Semantic Scholar, https://www.semanticscholar.org/paper/CoA-VLA%3A-Improving-Vision-Language-Action-Models-Li-Zhu/5760cc722b5eabc14487e9c497a78038ab46f37d</li>
<li>CoA-VLA: Improving Vision-Language-Action Models via Visual-Textual Chain-of-Affordance - arXiv, https://arxiv.org/html/2412.20451v2</li>
<li>Chengmeng Li’s research works - ResearchGate, https://www.researchgate.net/scientific-contributions/Chengmeng-Li-2299172144</li>
<li>DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action - arXiv, https://arxiv.org/html/2511.22134v1</li>
<li>ReFineVLA: Multimodal Reasoning-Aware Generalist Robotic Policies via Teacher-Guided Fine-Tuning - OpenReview, https://openreview.net/pdf/eadb1593ebc948f5159d30345bcf1e79a6d419c9.pdf</li>
<li>Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey, https://arxiv.org/html/2508.13073v1</li>
<li>Visualizing Thought: Conceptual Diagrams Enable Robust Planning in LMMs - arXiv, https://arxiv.org/html/2503.11790v3</li>
<li>TinyVLA: Toward Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation | Request PDF - ResearchGate, https://www.researchgate.net/publication/389282468_TinyVLA_Towards_Fast_Data-Efficient_Vision-Language-Action_Models_for_Robotic_Manipulation</li>
<li>FAST: Efficient Action Tokenization for Vision-Language-Action Models | Request PDF - ResearchGate, https://www.researchgate.net/publication/395364524_FAST_Efficient_Action_Tokenization_for_Vision-Language-Action_Models</li>
<li>Deductive Chain-of-Thought Augmented Socially-aware Robot Navigation World Model - arXiv, https://arxiv.org/html/2510.23509v1</li>
<li>An LLM Using a Knowledge Graph to Reason - ETH Zurich Research Collection, https://www.research-collection.ethz.ch/bitstreams/a03fe1a6-16b6-456c-83f2-c91296eda847/download</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>