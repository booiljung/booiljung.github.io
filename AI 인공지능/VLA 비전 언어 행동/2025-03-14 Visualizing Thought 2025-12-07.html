<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Visualizing Thought (시각적 사고)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Visualizing Thought (시각적 사고)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">Vision-Language-Action(VLA) 모델</a> / <span>Visualizing Thought (시각적 사고)</span></nav>
                </div>
            </header>
            <article>
                <h1>Visualizing Thought (시각적 사고)</h1>
<h2>1.  서론: 텍스트 기반 추론의 한계와 시각적 전환의 필연성</h2>
<p>인공지능 연구의 최전선에서 대규모 언어 모델(LLM)과 대규모 멀티모달 모델(LMM)은 텍스트 처리와 이미지 인식 분야에서 인간의 능력을 상회하는 성과를 보여주었다. 그러나 이러한 모델들이 복잡한 다단계 추론(Multi-step Reasoning)이나 조합 계획(Combinatorial Planning) 문제에 직면했을 때, 여전히 근본적인 병목 현상을 겪고 있다는 사실은 학계의 주요한 화두이다. 특히, ’A를 B 위에 놓고, C를 A 옆으로 옮긴다’와 같은 연속적인 상태 변화를 추적해야 하는 문제에서 텍스트 기반 모델은 단계가 거듭될수록 기하급수적으로 증가하는 오류율을 보인다. 이는 언어라는 매체가 가진 선형적(linear) 특성과 기호적 추상화가 복잡한 위상학적(topological) 관계나 공간적 상태(spatial state)를 보존하는 데 비효율적이기 때문이다.</p>
<p>2025년 Borazjanizadeh 등이 제안한 “Visualizing Thought: Conceptual Diagrams Enable Robust Planning in LMMs” 연구는 이러한 텍스트 추론의 한계를 극복하기 위해 인간의 인지심리학적 기제인 ’정신 모델(Mental Models)’을 AI에 이식하려는 시도로서 중대한 의의를 갖는다.1 이 연구는 LMM이 외부의 도움 없이 스스로 문제 상황을 시각화하는 ’개념적 도식(Conceptual Diagrams)’을 생성하고, 이를 추론의 중간 매개체로 활용함으로써 계획 수립의 견고성(Robustness)을 획기적으로 향상시킬 수 있음을 입증했다. 이는 단순히 텍스트를 이미지로 변환하는 것을 넘어, AI가 자신의 사고 과정을 시각적으로 ’외현화(Externalize)’하고 검증하는 새로운 추론 패러다임, 즉 ’시각적 사고(Visual Thinking)’의 시대를 예고한다.1</p>
<p>본 보고서는 Borazjanizadeh 등의 연구를 중심으로, Li 등이 제안한 ‘Multimodal Visualization-of-Thought (MVoT)’ 등 관련 연구들을 포괄적으로 분석한다. 텍스트 기반 추론의 인지적 결함을 이론적으로 규명하고, 이를 보완하기 위한 시각적 도식 생성 프레임워크의 기술적 아키텍처와 알고리즘을 상세히 해부한다. 또한, PDDL(Planning Domain Definition Language) 기반의 다양한 벤치마크 실험 결과를 통해 시각적 추론의 효용성을 정량적으로 검증하고, 향후 로보틱스 및 에이전트 AI 분야에 미칠 파급 효과를 논의한다.</p>
<h2>2.  이론적 배경: 인지심리학과 AI 추론의 융합</h2>
<h3>2.1  존슨-레어드의 정신 모델과 이중 부호화 이론</h3>
<p>인간은 복잡한 문제를 해결할 때 언어적 명제(proposition)에만 의존하지 않는다. 인지심리학자 필립 존슨-레어드(Philip Johnson-Laird)의 정신 모델 이론에 따르면, 인간은 추론 과정에서 상황에 대한 구조적 유사성을 가진 내부 표상(internal representation)을 구축하고 조작한다.3 이는 Paivio의 이중 부호화 이론(Dual Coding Theory)과도 맥을 같이한다. 이중 부호화 이론은 인간의 인지 시스템이 언어적 시스템(Logogens)과 비언어적/시각적 시스템(Imagens)이라는 두 가지 독립적이면서도 상호 연결된 하위 시스템으로 구성되어 있다고 설명한다.</p>
<p>기존의 ‘사고의 사슬(Chain-of-Thought, CoT)’ 프레임워크는 오직 언어적 시스템만을 모방하여 중간 추론 단계를 텍스트로 생성하게 했다. 그러나 텍스트는 정보를 순차적으로 나열해야 하므로, 객체 간의 다차원적 관계를 한눈에 파악하거나 전체적인 상태(Global State)를 유지하는 데 한계가 있다. 반면, 시각적 도식은 “그림 하나가 천 개의 단어보다 낫다“는 격언처럼, 복잡한 공간적 관계와 상태 정보를 하나의 프레임 안에 압축적으로, 그리고 병렬적으로 인코딩할 수 있다.4 Borazjanizadeh 등의 연구는 이러한 인간의 이중 부호화 전략을 LMM에 적용하여, 텍스트 추론과 시각적 시뮬레이션을 결합한 하이브리드 추론 엔진을 구축한 것이다.</p>
<h3>2.2  텍스트 기반 계획 수립(Planning)의 붕괴 현상</h3>
<p>조합 계획 문제는 초기 상태에서 목표 상태로 도달하기 위한 일련의 행동(Action) 시퀀스를 찾아내는 작업이다. Blocksworld와 같은 전형적인 도메인에서 LLM은 행동의 결과로 변화된 세계 상태(World State)를 텍스트로 갱신하며 추적해야 한다. 그러나 텍스트 설명은 불완전하거나 모호할 수 있으며, 모델은 종종 이전 단계에서 이동시킨 블록의 위치를 잊어버리거나 물리적으로 불가능한 상태를 생성하는 환각(Hallucination) 현상을 보인다.</p>
<p>특히 기존의 최신 방법론인 RAP(Reasoning via Planning)조차도 텍스트 기반의 몬테카를로 트리 탐색(MCTS)을 사용했음에도 불구하고, 탐색 깊이가 깊어질수록 상태 추적의 정확도가 급격히 떨어지는 현상이 관찰된다. 예를 들어, RAP는 복잡한 Blocksworld 문제(Hard)에서 불과 4%의 성공률을 기록했는데, 이는 텍스트만으로는 복잡한 조합 공간을 효율적으로 탐색하고 가지치기(Pruning)하는 것이 불가능에 가깝다는 것을 시사한다.4</p>
<h2>3.  방법론 심층 분석: Visualizing Thought 프레임워크</h2>
<p>Borazjanizadeh 등이 제안한 ‘Visualizing Thought’ 프레임워크는 LMM이 도메인에 구애받지 않고(Domain-agnostic), 훈련 없이(Training-free) 즉시 적용 가능한 제로샷(Zero-shot) 아키텍처를 기반으로 한다.2 이 시스템은 크게 도메인 특화 시각적 스키마 생성, 시각적 사고 그래프(Visual Graph-of-Thought) 추론, 그리고 최적화된 탐색 전략으로 구성된다.</p>
<h3>3.1  제로샷 도메인 스키마 생성 (Domain-Specific Visual Schema Generation)</h3>
<p>이 프레임워크의 가장 혁신적인 점은 인간의 개입 없이 모델이 스스로 ’어떻게 그릴 것인가’를 결정한다는 것이다.</p>
<ul>
<li><strong>메타 인지적 시각화 설계:</strong> LMM은 PDDL로 기술된 문제 도메인(예: 엘리베이터, 테트리스, 바맨 등)의 정의를 분석하고, 해당 도메인의 객체(Object)와 서술어(Predicate)를 가장 효과적으로 표현할 수 있는 시각적 스키마를 고안한다. 예를 들어, ‘엘리베이터’ 도메인에서는 층(Floor)을 수직적 y좌표로, 승객(Passenger)을 색상이 있는 사각형으로, 탑승 여부를 사각형 내부의 포함 관계로 정의할 수 있다.4</li>
<li><strong>코드 기반 도식 생성 (Code-as-Schema):</strong> 픽셀 단위로 이미지를 생성하는 생성 모델(Generative Models)과 달리, 본 연구는 Python의 Matplotlib 라이브러리를 활용하여 이미지를 그리는 ’코드’를 생성한다. 이는 매우 중요한 전략적 선택이다. 픽셀 생성 방식은 확률적 노이즈로 인해 “사각형 3개를 그려라“라는 명령에도 2개나 4개를 그리는 등 정밀한 수적(numerical) 제어를 보장하기 어렵다. 반면, 코드는 논리적이고 결정론적(deterministic)이므로, 데이터의 정확성과 관계의 명확성을 100% 보장하는 도식을 생성할 수 있다.4</li>
<li><strong>자동화된 스키마 선택:</strong> 모델은 여러 개의 시각화 코드 후보를 생성하고, 이를 실행하여 렌더링된 이미지를 다시 스스로 평가한다. 객체 간의 공간적 관계가 명확한지, 겹침이 없는지 등을 시각적으로 검증하여 최적의 스키마를 ’예제(Example)’로 선정하고, 이후 모든 추론 단계에서 이 스키마를 참조한다.6</li>
</ul>
<h3>3.2  시각적 사고 그래프 (Visual Graph-of-Thought) 아키텍처</h3>
<p>추론 과정은 단선적인 사슬(Chain)이 아닌, 가지를 뻗어나가는 그래프(Graph) 형태로 구조화된다.</p>
<ul>
<li><strong>멀티모달 노드(Multimodal Node)의 구성:</strong> 추론 그래프의 각 노드 <span class="math math-inline">N_t</span>는 현재 상태에 대한 텍스트 설명 <span class="math math-inline">T(s_t)</span>와 이에 대응하는 시각적 도식 <span class="math math-inline">V(s_t)</span>를 쌍으로 포함한다. 즉, 각 추론 단계에서 LMM은 “현재 상태는 A가 B 위에 있다“라고 텍스트로 기술함과 동시에, 이를 앞서 정의한 Matplotlib 코드로 렌더링한 이미지를 생성한다.2</li>
<li><strong>상호 검증 메커니즘 (Consistency Check):</strong> 생성된 이미지 <span class="math math-inline">V(s_t)</span>는 다시 LMM의 비전 인코더(Vision Encoder)로 입력되어, 텍스트 설명 <span class="math math-inline">T(s_t)</span>와 일치하는지 검증받는다. 만약 이미지가 텍스트 설명과 모순되거나(예: 텍스트는 ’위에 있다’고 했으나 그림은 ’옆에 있다’인 경우), 이전 단계의 행동 <span class="math math-inline">a_t</span>와 논리적으로 불일치할 경우, 모델은 이를 오류로 간주하고 해당 노드를 폐기하거나 재생성한다.4 이 과정은 인간이 그림을 그리면서 “어, 이게 아닌데?“라고 수정하는 자기 성찰적(Self-reflective) 과정을 모방한 것이다.</li>
</ul>
<h3>3.3  빔 서치(Beam Search)와 깊이 우선 백트래킹(Depth-wise Backtracking)</h3>
<p>단순한 탐색은 무한한 가능성의 바다에서 길을 잃기 쉽다. 본 연구는 시각적 도식의 이점을 극대화하기 위해 정교한 탐색 알고리즘을 도입했다.</p>
<ul>
<li><strong>시각적 휴리스틱을 활용한 빔 서치:</strong> 각 깊이 <span class="math math-inline">d</span>에서 가능한 모든 행동을 시뮬레이션하여 <span class="math math-inline">N</span>개의 자식 상태를 생성한다. LMM은 각 자식 상태의 도식 <span class="math math-inline">V(s_{t+1})</span>을 목표 상태의 도식 <span class="math math-inline">V(s_{goal})</span>과 시각적으로 비교하여, 목표에 얼마나 근접했는지를 직관적으로 평가한다. 이를 바탕으로 상위 <span class="math math-inline">k=4</span>개의 가장 유망한 상태만을 선택하여 다음 단계로 확장한다. 시각적 비교는 텍스트 비교보다 훨씬 빠르고 직관적인 거리 추정(Distance Estimation)을 가능하게 한다.4</li>
<li><strong>깊이 제한 및 백트래킹:</strong> 계산 효율성을 위해 최대 깊이(Max Depth)를 설정(예: 간단한 Blocksworld는 28, 복잡한 도메인은 100)하고, 탐색 가능한 최대 상태 수(Max States)를 제한(120~450개)한다. 만약 선택된 경로가 막다른 길(Dead end)에 다다르거나 검증에 실패하면, 모델은 가장 깊은 유효한 조상 노드로 즉시 복귀(Backtrack)하여 다른 경로를 탐색한다.4 이러한 구조적 탐색은 모델이 국소 최적해(Local Optima)에 빠지는 것을 방지한다.</li>
</ul>
<h2>4.  비교 방법론: MVoT (Multimodal Visualization-of-Thought)</h2>
<p>Borazjanizadeh의 연구와 유사한 시기에 발표된 Li 등의 “Imagine while Reasoning in Space: Multimodal Visualization-of-Thought (MVoT)” 연구 역시 시각적 사고의 중요성을 강조하지만, 기술적 접근 방식에서 뚜렷한 대조를 이룬다.7</p>
<h3>4.1  생성적 접근(Generative Approach) vs. 프로그래밍적 접근(Programmatic Approach)</h3>
<p>MVoT는 멀티모달 LLM(Chameleon-7B 등)이 텍스트 토큰과 이미지 토큰을 인터리빙(Interleaving)하여 직접 생성하는 방식을 채택했다. 즉, 모델이 픽셀 단위(혹은 VQ-VAE 토큰 단위)로 이미지를 그려낸다.</p>
<ul>
<li><strong>MVoT의 특징:</strong> 훈련 단계에서 텍스트와 이미지의 정렬을 학습시키기 위해 ’토큰 불일치 손실(Token Discrepancy Loss)’을 도입했다. 이는 시각적 사고가 자연스럽게 텍스트 사고와 섞이도록 유도한다.7</li>
<li><strong>비교 분석:</strong> MVoT의 방식은 기하학 문제나 상식적인 공간 추론에는 유용할 수 있으나, PDDL과 같이 엄밀한 논리가 요구되는 계획 문제에서는 한계를 갖는다. 생성 모델의 확률적 특성상, 블록의 색깔이 미묘하게 바뀌거나 개수가 변하는 등의 ’시각적 환각’이 발생할 수 있기 때문이다. 반면, Borazjanizadeh의 <strong>코드 생성 방식</strong>은 이러한 불확실성을 완전히 제거하고 논리적 정합성을 강제한다는 점에서 계획 수립 작업에 훨씬 더 적합하다.4</li>
</ul>
<h2>5.  실험적 검증 및 벤치마크 성과 분석</h2>
<p>Visualizing Thought 프레임워크의 성능은 국제 계획 대회(IPC)의 표준 벤치마크인 PDDL 도메인에서 광범위하게 검증되었다. 특히 Blocksworld, Parking, Tetris, Elevator, Barman 등 다양한 난이도의 도메인이 포함되었다.4</p>
<h3>5.1  Blocksworld (Hard)에서의 압도적 성능 격차</h3>
<p>가장 주목할 만한 데이터는 고난이도 Blocksworld(10~20개 블록)에서의 성능이다.</p>
<table><thead><tr><th><strong>모델 / 방법론</strong></th><th><strong>Blocksworld (Hard) 정확도</strong></th><th><strong>방법론적 특징</strong></th></tr></thead><tbody>
<tr><td><strong>RAP (Reasoning via Planning)</strong></td><td><strong>4%</strong></td><td>텍스트 기반 MCTS + LLM World Model 4</td></tr>
<tr><td><strong>CoT (Chain-of-Thought)</strong></td><td>10~20% (추정)</td><td>단순 텍스트 연쇄 추론</td></tr>
<tr><td><strong>Visualizing Thought (GPT-4o)</strong></td><td><strong>78%</strong></td><td>텍스트 + Matplotlib 도식 + 빔 서치 4</td></tr>
<tr><td><strong>Llama 4 (Visualizing Thought 적용)</strong></td><td><strong>74%</strong></td><td>베이스라인 10% 대비 7배 향상 4</td></tr>
<tr><td><strong>Claude 3.5 Sonnet (Visualizing Thought 적용)</strong></td><td><strong>98%</strong></td><td>베이스라인 54.8% 대비 비약적 향상 4</td></tr>
</tbody></table>
<p>위 표에서 볼 수 있듯이, 텍스트 기반의 최첨단 방법론인 RAP가 불과 4%의 성공률에 그친 반면, 시각적 사고를 도입한 Visualizing Thought는 GPT-4o 기준 78%, Claude 3.5 Sonnet 기준 98%라는 경이적인 성공률을 기록했다.4 이는 텍스트만으로는 관리할 수 없었던 복잡한 상태 공간(State Space)을 시각적 도식이 얼마나 효과적으로 압축하고 정리해주는지를 증명한다. RAP의 실패 원인은 탐색 트리가 깊어질수록 텍스트 상태 설명의 일관성이 무너졌기 때문으로 분석된다.4</p>
<h3>5.2  다양한 도메인으로의 확장성 (Generalizability)</h3>
<p>이 프레임워크는 특정 도메인에 과적합(Overfitting)되지 않았다.</p>
<ul>
<li><strong>Tetris:</strong> <span class="math math-inline">4 \times 4</span> 및 <span class="math math-inline">6 \times 6</span> 격자에서 타일을 재배치하는 테트리스 문제에서도 시각적 도식은 빈 공간과 채워진 공간의 기하학적 관계를 명확히 보여주어 계획 수립을 도왔다.4</li>
<li><strong>Elevator &amp; Parking:</strong> 승객의 이동 경로 최적화나 차량의 주차 순서 결정과 같은 제약 충족 문제(Constraint Satisfaction Problems)에서도 시각적 시뮬레이션은 유효한 전략임이 입증되었다.</li>
</ul>
<h3>5.3  모델 불가지론적(Model-Agnostic) 효용성</h3>
<p>중요한 점은 이 성능 향상이 GPT-4o와 같은 특정 모델에 국한되지 않는다는 것이다. 오픈 소스 모델인 Llama 4나 다른 상용 모델인 Claude 3.5 Sonnet에 동일한 프레임워크를 적용했을 때도 유사하거나 더 높은 폭의 성능 향상이 관찰되었다.4 이는 ’시각적 사고’라는 방법론 자체가 모델의 기본 지능을 증폭시키는 범용적인 ‘인지적 보철(Cognitive Prosthesis)’ 역할을 수행함을 시사한다.</p>
<h2>6.  심층 분석: 왜 도식(Diagram)인가?</h2>
<h3>6.1  정보의 집약과 관계의 외현화</h3>
<p>텍스트는 정보를 순차적으로 나열(Serialized)해야 하므로, <span class="math math-inline">N</span>개의 객체 간 상호작용을 설명하려면 <span class="math math-inline">O(N^2)</span>에 가까운 문장이 필요할 수 있다. 그러나 도식은 2차원 평면 위에 객체들을 배치함으로써, 위치 관계, 인접성, 포함 관계 등 수많은 정보를 하나의 이미지 안에 암묵적으로(implicitly) 그러나 완벽하게 인코딩한다. 이는 모델이 처리해야 할 정보의 ’문맥 길이(Context Length)’를 획기적으로 줄여주는 효과를 낳는다.4</p>
<h3>6.2  접지(Grounding)와 환각의 억제</h3>
<p>심볼 그라운딩 문제(Symbol Grounding Problem)는 AI가 기호의 의미를 실세계의 감각 정보와 연결하지 못하는 현상을 말한다. 텍스트 추론만으로는 기호가 기호를 참조하는 순환 논리에 빠지기 쉽다. 그러나 Visualizing Thought 프레임워크에서 생성된 도식은 텍스트 기호를 시각적 실체로 ’접지’시킨다. 모델이 “블록 A가 블록 B 위에 있다“라고 말할 때, 실제로 그려진 그림에서 A가 B 위에 위치해야만 검증을 통과할 수 있다. 이러한 시각적 피드백 루프는 환각을 물리적으로 차단하는 강력한 제약 조건으로 작용한다.4</p>
<h2>7.  한계점 및 향후 과제</h2>
<p>혁신적인 성과에도 불구하고, 본 프레임워크는 몇 가지 실질적인 한계점을 안고 있다.</p>
<ol>
<li><strong>계산 비용과 지연 시간 (Latency):</strong> 매 추론 단계마다 코드를 생성하고, 실행하여 이미지를 렌더링하고, 다시 비전 모델로 분석하는 과정은 단순 텍스트 생성에 비해 훨씬 많은 계산 자원과 시간을 소모한다. 추론 시간(Inference Time)의 증가는 실시간 응답이 필요한 애플리케이션에는 걸림돌이 될 수 있다.11 그러나 연구진은 빔 서치의 효율적 가지치기를 통해, 성공적인 계획을 수립하는 데 드는 총 비용(Total Cost)은 반복적인 실패를 겪는 텍스트 모델보다 오히려 경제적일 수 있음을 주장한다.4</li>
<li><strong>도식화의 난이도:</strong> 물리적 객체가 명확한 도메인(블록, 엘리베이터 등)은 시각화가 용이하지만, 추상적인 개념(예: 법률적 논쟁, 철학적 인과관계)이나 고차원 데이터는 2차원 도식으로 표현하기 어려울 수 있다. 시각적 스키마를 정의하는 것 자체가 또 다른 복잡한 추론 문제가 될 수 있다.</li>
<li><strong>코드 생성 능력에 대한 의존성:</strong> 이 시스템은 백본 모델이 Matplotlib 코드를 정확하게 작성할 수 있다는 가정 하에 작동한다. 코딩 능력이 부족한 소형 모델에서는 적용이 어려울 수 있다.</li>
</ol>
<h2>8.  결론: 구현된 지능(Embodied Intelligence)과 AGI를 향하여</h2>
<p>Borazjanizadeh 등의 “Visualizing Thought” 연구는 LMM이 단순히 데이터를 처리하는 기계를 넘어, 인간처럼 내적 시뮬레이션을 통해 사고하는 지능체로 진화하고 있음을 보여주는 이정표적 연구이다. 이 연구가 제시한 ’코드 기반의 개념적 도식 생성’과 ’그래프 기반 시각적 추론’은 텍스트 중심의 AI가 겪어온 ’추론의 깊이’와 ‘기억의 지속성’ 문제를 해결하는 열쇠가 될 것이다.</p>
<p>향후 이 기술은 로보틱스(Robotics) 분야에서 로봇이 행동하기 전에 자신의 행동 결과를 시각적으로 예견(Visual Foresight)하고 계획하는 데 필수적인 기술로 자리 잡을 것이다. 또한, 에이전트 AI(Agentic AI)가 복잡한 디지털 환경(예: 웹 브라우징, GUI 조작)에서 작업을 수행할 때도 화면의 상태를 시각적으로 시뮬레이션하며 최적의 경로를 찾는 데 기여할 수 있다. 결론적으로, 시각적 사고의 통합은 인공지능이 인간 수준의 유동적 지능(Fluid Intelligence)과 일반화된 문제 해결 능력(AGI)을 갖추기 위한 필수 불가결한 단계임이 분명하다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>Visualizing Thought: Conceptual Diagrams Enable Robust Planning in LMMs, https://www.semanticscholar.org/paper/Visualizing-Thought%3A-Conceptual-Diagrams-Enable-in-Borazjanizadeh-Herzig/badb84ec2d95d3fe51807914bbd2f56a8d0e6c3c</li>
<li>Visualizing Thought: Conceptual Diagrams Enable Robust Planning in LMMs, https://www.researchgate.net/publication/389916490_Visualizing_Thought_Conceptual_Diagrams_Enable_Robust_Planning_in_LMMs</li>
<li>Visualizing Thought: Conceptual Diagrams Enable Robust Planning …, https://openreview.net/forum?id=UeufAyipBS</li>
<li>Visualizing Thought: Conceptual Diagrams Enable Robust Planning in LMMs - arXiv, https://arxiv.org/html/2503.11790v3</li>
<li>Visualizing Thought: Conceptual Diagrams Enable Robust Planning in LMMs - arXiv, https://arxiv.org/abs/2503.11790</li>
<li>Visualizing Thought: Conceptual Diagrams Enable Robust Planning in LMMs - arXiv, https://arxiv.org/html/2503.11790v1</li>
<li>Imagine while Reasoning in Space: Multimodal Visualization-of-Thought - arXiv, https://arxiv.org/html/2501.07542v1</li>
<li>Imagine while Reasoning in Space: Multimodal Visualization-of-Thought - ResearchGate, https://www.researchgate.net/publication/387976168_Imagine_while_Reasoning_in_Space_Multimodal_Visualization-of-Thought</li>
<li>[Literature Review] Imagine while Reasoning in Space: Multimodal Visualization-of-Thought, https://www.themoonlight.io/en/review/imagine-while-reasoning-in-space-multimodal-visualization-of-thought</li>
<li>v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning - arXiv, https://arxiv.org/html/2505.18842v3</li>
<li>Mechanical Reasoning by Mental Simulation | Request PDF - ResearchGate, https://www.researchgate.net/publication/8540048_Mechanical_Reasoning_by_Mental_Simulation</li>
<li>arXiv:2503.11790v1 [cs.AI] 14 Mar 2025, https://arxiv.org/pdf/2503.11790</li>
<li>Our proposed approach. (1) Model generates an initial diagram schema… - ResearchGate, https://www.researchgate.net/figure/Our-proposed-approach-1-Model-generates-an-initial-diagram-schema-and-conceptual_fig1_389916490</li>
<li>Daily Papers - Hugging Face, <a href="https://huggingface.co/papers?q=hypertree-structured+planning+outlines">https://huggingface.co/papers?q=hypertree-structured%20planning%20outlines</a></li>
<li>Flow of Reasoning: Efficient Training of LLM Policy with … - SciSpace, https://scispace.com/pdf/flow-of-reasoning-efficient-training-of-llm-policy-with-1bhmhzha7k.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>