<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:컴퓨터 비전 기술의 진화와 FAMNet 아키텍처 (A Lightweight Stereo Matching Network)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>컴퓨터 비전 기술의 진화와 FAMNet 아키텍처 (A Lightweight Stereo Matching Network)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">비전 기반 깊이 추정 (Vision Based Dept)</a> / <span>컴퓨터 비전 기술의 진화와 FAMNet 아키텍처 (A Lightweight Stereo Matching Network)</span></nav>
                </div>
            </header>
            <article>
                <h1>컴퓨터 비전 기술의 진화와 FAMNet 아키텍처 (A Lightweight Stereo Matching Network)</h1>
<p>2025-12-16, G30DR</p>
<h2>1.  서론: 컴퓨터 비전의 난제와 FAMNet의 다의적 등장</h2>
<p>현대 컴퓨터 비전(Computer Vision)과 딥러닝(Deep Learning) 연구의 흐름을 관통하는 핵심 과제는 ’특징의 정밀한 추출(Feature Extraction)’과 시공간적 맥락에서의 ‘정렬(Alignment)’ 및 ’매칭(Matching)’으로 요약될 수 있다. 단순한 이미지 분류를 넘어 픽셀 단위의 위치를 추정하거나, 시간의 흐름 속에서 객체의 동일성을 유지하고, 찰나의 미세한 변화를 감지하는 고난도 작업들은 기존의 정적인 합성곱 신경망(CNN) 구조만으로는 해결하기 어려운 한계에 봉착해 왔다. 이러한 배경 속에서 ’FAMNet’이라는 명칭은 단일한 모델을 지칭하는 것을 넘어, 특징 정렬(Feature Alignment), 다중 스케일(Multi-scale), 어텐션(Attention), 그리고 매칭(Matching) 메커니즘을 딥러닝 아키텍처 내에 내재화하려는 일련의 혁신적인 시도들을 대변하는 고유명사로 자리 잡았다.</p>
<p>본 보고서는 문헌에 보고된 다양한 FAMNet 변형들을 포괄적으로 분석하고, 각 모델이 특정 도메인에서 해결하고자 했던 기술적 난제와 이를 극복하기 위해 도입된 방법론적 혁신을 심층적으로 다룬다. 구체적으로 본 보고서는 FAMNet이 적용된 네 가지 주요 영역인 (1) 얼굴 랜드마크 검출(Facial Landmark Detection), (2) 다중 객체 추적(Multiple Object Tracking, MOT), (3) 미세 표정 인식(Micro-Expression Recognition, MER), 그리고 (4) 스테레오 매칭(Stereo Matching) 및 의료 영상 분석 분야를 중점적으로 고찰한다.</p>
<p>각기 다른 연구 그룹에 의해 독립적으로 제안되었음에도 불구하고, 이들 FAMNet 연구들은 ’고차원 특징 공간에서의 정밀한 제어’라는 공통된 철학을 공유한다.1 얼굴 랜드마크 검출에서의 FAMNet은 자기 반복 회귀(Self-Iterative Regression)를 통해 랜드마크 위치를 점진적으로 정제하며 1, 다중 객체 추적에서의 FAMNet은 특징 추출과 데이터 연관(Data Association) 과정을 단일 네트워크 내에서 종단간(End-to-End)으로 학습하는 프레임워크를 제시한다.2 또한, 가장 최근인 2025년에 제안된 미세 표정 인식 모델로서의 FAMNet은 2D와 3D 특징의 융합과 계층적 어텐션을 통해 인간의 눈으로도 감지하기 힘든 미세한 표정 변화를 포착하는 데 특화되어 있다.3</p>
<p>본 보고서는 이러한 FAMNet 아키텍처들의 수학적 배경, 네트워크 구조, 학습 전략, 그리고 벤치마크 실험 결과를 상세히 기술함으로써, 이들이 현대 컴퓨터 비전 기술의 진보에 기여한 바를 명확히 하고자 한다. 나아가 각 모델이 제시하는 ’정렬’과 ’통합’의 원리가 향후 인공지능 연구에 어떠한 시사점을 주는지 논의한다.</p>
<h2>2.  얼굴 랜드마크 검출을 위한 FAMNet: 자기 반복 회귀와 랜드마크 어텐션의 결합</h2>
<p>얼굴 랜드마크 검출(Facial Landmark Detection)은 얼굴 인식, 표정 분석, 3D 얼굴 재구성 등 상위 컴퓨터 비전 작업의 기초가 되는 핵심 기술이다. 이 분야에서 제안된 FAMNet은 ’Facial Landmarks Detection by Self-Iterative Regression Based Landmarks-Attention Network’라는 연구를 통해 소개되었으며, 비선형 최소 제곱 문제를 해결하기 위한 새로운 회귀 프레임워크를 제시하였다.1</p>
<h3>2.1  기존 방법론의 한계와 FAMNet의 접근</h3>
<p>기존의 얼굴 랜드마크 검출 방법론 중 널리 사용되던 계단식 회귀(Cascaded Regression) 방법은 초기 형상에서 시작하여 단계적으로 랜드마크 위치를 갱신하는 방식을 취했다. 그러나 이러한 방식은 각 단계마다 별도의 회귀기(Regressor)를 학습해야 하거나, 모델 파라미터가 과도하게 많아지는 구조적 비효율성을 내포하고 있었다. 또한, 국소적인 특징(Local Features)을 추출하는 과정에서 랜드마크 주변의 문맥 정보를 충분히 활용하지 못하거나, 식별력(Discriminative power)이 부족한 특징을 생성하는 경향이 있었다.</p>
<p>FAMNet은 이러한 문제를 해결하기 위해 <strong>자기 반복 회귀(Self-Iterative Regression, SIR)</strong> 프레임워크와 **랜드마크 어텐션 네트워크(Landmarks-Attention Network, LAN)**를 결합한 구조를 제안하였다. 이는 단일 CNN 모델이 자신의 출력을 다시 입력으로 받아들이는 재귀적 구조를 통해 모델의 복잡도를 낮추면서도 성능을 극대화하는 전략이다.1</p>
<h3>2.2  자기 반복 회귀(SIR) 프레임워크의 수학적 모델링</h3>
<p>SIR 프레임워크의 핵심은 랜드마크 위치 추정 문제를 반복적인 최적화 과정으로 정식화하는 것이다. 네트워크는 현재 추정된 랜드마크 위치 <span class="math math-inline">P_k</span>를 기반으로 정답 위치와의 차이(Offset)인 <span class="math math-inline">R_\Delta(P_k)</span>를 예측하고, 이를 통해 위치를 갱신한다.</p>
<p><span class="math math-display">\theta_{k+1} = \theta_k + R_\Delta(P_k), \quad k = 0, 1, \dots</span></p>
<p>여기서 <span class="math math-inline">\theta</span>는 랜드마크의 위치 파라미터를 나타낸다. 이 과정은 <span class="math math-inline">k</span>가 증가함에 따라 랜드마크 위치가 점진적으로 정답(Ground Truth)에 수렴하도록 유도한다. SIR 프레임워크는 기존의 계단식 회귀와 달리, 모든 반복 단계에서 동일한 회귀기 <span class="math math-inline">R_\Delta</span>를 공유한다. 이는 모델 파라미터의 수를 획기적으로 줄이면서도, 반복적인 학습을 통해 모델이 다양한 초기 조건과 섭동(Perturbation)에 강건해지도록 돕는다.1</p>
<h3>2.3 가우시안 무작위 샘플링(Gaussian Random Sampling)을 통한 데이터 공간 확장</h3>
<p>FAMNet의 학습 과정에서 가장 독창적인 요소 중 하나는 학습 데이터를 생성하고 증강하는 방식이다. 연구진은 랜드마크의 위치 파라미터 <span class="math math-inline">\theta = (x_1, y_1, \dots, x_M, y_M)</span> 공간에서 직접 샘플링을 수행하는 것이 고차원 공간의 희소성 문제로 인해 비효율적임을 지적하였다. 대신, 3D Morphable Model(3DMM)과 유사한 얼굴 랜드마크 모델 파라미터 <span class="math math-inline">S</span> 공간에서 샘플링을 수행한 후, 이를 위치 파라미터로 변환하는 간접적인 방식을 채택했다.1</p>
<p>얼굴 랜드마크 모델 파라미터 <span class="math math-inline">S</span>는 다음과 같이 정의된다.</p>
<p><span class="math math-display">S = [\alpha, t_{2d}, \beta, f]</span></p>
<p>여기서 <span class="math math-inline">\alpha</span>는 PCA(주성분 분석)를 통해 얻어진 고유 형상 파라미터, <span class="math math-inline">t_{2d}</span>는 2D 평행 이동, <span class="math math-inline">\beta</span>는 평면 내 회전 각도, <span class="math math-inline">f</span>는 스케일 계수를 의미한다. 이 파라미터 <span class="math math-inline">S</span>는 다음과 같은 변환식을 통해 랜드마크 위치 <span class="math math-inline">\theta</span>로 변환된다.</p>
<p><span class="math math-display">\theta(S) = f \cdot R_\beta \cdot (S_0 + A \cdot \alpha) + t_{2d}</span></p>
<p>여기서 <span class="math math-inline">S_0</span>는 평균 형상(Mean Shape), <span class="math math-inline">A</span>는 PCA 형상 행렬, <span class="math math-inline">R_\beta</span>는 회전 행렬이다. 연구진은 각 얼굴의 샘플 공간 <span class="math math-inline">D</span>를 평균 형상 <span class="math math-inline">S_0</span>와 정답 형상 <span class="math math-inline">S_{gt}</span>를 중심으로 하는 가우시안 분포의 합집합으로 정의하였다.</p>
<p><span class="math math-display">D \sim \{ N(S_0, \sigma) \cup N(S_{gt}, \sigma) \}</span></p>
<p>이러한 샘플링 전략은 초기 단계(Coarse stages)부터 미세 조정 단계(Fine stages)까지 발생할 수 있는 다양한 랜드마크 분포를 수학적으로 모델링하여 학습 데이터로 제공한다. 이는 모델이 엉뚱한 초기값에서 시작하더라도 안정적으로 정답을 찾아갈 수 있는 궤적을 학습하게 한다.1</p>
<h3>2.3  랜드마크 어텐션 네트워크(LAN)의 구조와 역할</h3>
<p>LAN은 SIR 프레임워크 내에서 작동하는 핵심 회귀 모델이다. 기존 방법들이 랜드마크별로 독립적인 패치(Patch)를 잘라내어 특징을 추출했던 것과 달리, LAN은 전체 얼굴 이미지의 특징 맵(Feature Map)을 공유하면서 각 랜드마크 위치에 해당하는 영역에 ’어텐션(Attention)’을 적용한다.</p>
<p>LAN의 구조는 크게 특징 추출을 위한 백본(Backbone) 네트워크와 랜드마크별 어텐션 맵을 생성하는 서브 모듈로 구성된다. 어텐션 메커니즘은 현재 추정된 랜드마크 위치 주변의 특징을 강조하고, 배경이나 다른 랜드마크와 관련된 정보는 억제한다. 이는 단일 CNN 구조 내에서 전역적인 문맥 정보와 국소적인 세부 정보를 동시에 활용할 수 있게 하며, 결과적으로 랜드마크 간의 구조적 관계(Geometric relationship)를 유지하면서도 개별 랜드마크의 위치를 정밀하게 보정할 수 있다.1</p>
<h3>2.4  300-W 데이터셋 실험 결과 및 분석</h3>
<p>FAMNet은 얼굴 랜드마크 검출 분야의 대표적인 벤치마크인 300-W 데이터셋(300 Faces in-the-Wild)에서 평가되었다. 300-W는 다양한 조명, 포즈, 표정 변화를 포함하는 이미지들로 구성되어 있어 난이도가 높은 데이터셋이다.</p>
<p>실험 결과, FAMNet은 기존의 SOTA(State-of-the-Art) 방법론들과 비교하여 우수한 성능을 기록하였다. 특히, 정규화된 평균 오차(Normalized Mean Error, NME) 지표에서 경쟁 모델 대비 낮은 오차율을 보였으며, 초기화 위치에 따른 성능 편차가 적어 높은 안정성을 입증하였다.1 가려짐(Occlusion)이 발생하거나 얼굴이 측면으로 많이 돌아간 경우(Large pose)에도, LAN이 제공하는 강건한 특징 표현력 덕분에 랜드마크를 놓치지 않고 추적하는 능력을 보였다. 이는 SIR 프레임워크의 반복적인 정제 과정이 실제로 비선형적인 얼굴 형상 변형을 효과적으로 모델링하고 있음을 시사한다.</p>
<h2>3.  다중 객체 추적(MOT)을 위한 FAMNet: 특징, 친화도, 할당의 종단간 결합 학습</h2>
<p>다중 객체 추적(MOT) 분야에서 FAMNet은 Chu와 Ling(ICCV 2019)에 의해 제안되었으며, ’Joint Learning of Feature, Affinity and Multi-Dimensional Assignment’라는 제목으로 발표되었다.2 이 연구는 MOT 시스템의 구성 요소들이 분절적으로 최적화되던 기존의 관행을 깨고, 추적 파이프라인 전체를 하나의 미분 가능한 네트워크로 통합했다는 점에서 기념비적이다.</p>
<h3>3.1  추적-탐지(Tracking-by-Detection) 패러다임의 한계 극복</h3>
<p>전통적인 MOT 시스템은 주로 ‘추적-탐지’ 패러다임을 따랐다. 즉, 외부 검출기(Detector)가 매 프레임마다 객체를 탐지하면, 추적 알고리즘은 이 탐지 결과들을 시간 축에 따라 연결(Association)한다. 이 과정은 통상적으로 (1) 특징 추출(Feature Extraction), (2) 친화도 계산(Affinity Metric), (3) 할당(Assignment)의 세 단계로 나뉜다. 기존 연구들은 이 세 단계를 각각 독립적으로 수행하거나, 특징 추출과 친화도 계산까지만 딥러닝으로 처리하고 할당은 헝가리안 알고리즘(Hungarian Algorithm)과 같은 이산적인 최적화 기법에 의존했다.</p>
<p>그러나 이러한 분리된 접근법은 각 단계의 최적화가 전체 시스템의 성능 향상으로 직결되지 않는다는 문제를 안고 있다. 예를 들어, 식별력이 좋은 특징을 추출하더라도 할당 알고리즘이 이를 제대로 활용하지 못하면 추적 실패로 이어진다. FAMNet은 이 세 가지 요소를 단일 네트워크 내에서 통합하여, 최종 추적 결과의 손실(Loss)이 특징 추출 단계까지 역전파(Backpropagation)될 수 있는 구조를 설계하였다.2</p>
<h3>3.2  FAMNet의 아키텍처: 세 가지 서브 네트워크의 통합</h3>
<p>FAMNet의 전체 아키텍처는 특징 서브 네트워크, 친화도 서브 네트워크, 그리고 다차원 할당(MDA) 서브 네트워크로 구성된다.</p>
<ol>
<li>특징 서브 네트워크 (Feature Sub-network):</li>
</ol>
<p>이 모듈은 샴 네트워크(Siamese Network) 구조를 기반으로 한다. 트랙렛(Tracklet, 이전 프레임까지 연결된 궤적)과 현재 프레임의 탐지 후보(Detection Candidate)로부터 시각적 특징을 추출한다. 여기서 중요한 기술적 요소는 **ROI 풀링(Region of Interest Pooling)**을 통한 공간적 정렬이다. 입력 이미지에서 객체 영역을 정확히 잘라내고 정렬함으로써, 배경 잡음을 최소화하고 객체 고유의 외형 정보만을 추출한다.2 이는 딥러닝 특징 맵 상에서의 정밀한 정렬을 의미하며, 추후 친화도 계산의 신뢰도를 높이는 기반이 된다.</p>
<ol start="2">
<li>친화도 서브 네트워크 (Affinity Sub-network):</li>
</ol>
<p>추출된 특징 벡터들을 입력으로 받아, 트랙렛과 탐지 후보 간의 유사도(Affinity)를 계산한다. 단순히 두 벡터 간의 거리(Distance)를 측정하는 것을 넘어, 다층 퍼셉트론(MLP) 등을 이용해 두 특징이 ’동일한 객체’일 확률을 학습한다. 이 네트워크의 출력은 친화도 텐서(Affinity Tensor) <span class="math math-inline">C</span>를 형성한다.</p>
<p><span class="math math-display">c_{i_0 i_1 \dots i_K} = \phi(F_{i_1}, b_{i_1}, \dots)</span></p>
<p>여기서 <span class="math math-inline">\phi</span>는 친화도 함수를 나타내며, 시각적 유사성뿐만 아니라 운동 정보(Motion) 등 다양한 단서를 결합하여 계산된다.2</p>
<ol start="3">
<li>
<p>다차원 할당 서브 네트워크 (MDA Sub-network):</p>
<p>FAMNet의 가장 혁신적인 부분은 이산적인 할당 문제를 미분 가능한 레이어로 구현한 것이다. MOT 문제는 수학적으로 다차원 할당 문제(Multi-dimensional Assignment Problem)로 정식화될 수 있다.</p>
</li>
</ol>
<p><span class="math math-display">\arg \max_Z \sum c_{i_0:i_K} z_{i_0:i_K}</span></p>
<p>여기서 <span class="math math-inline">Z</span>는 할당 여부를 나타내는 이진 텐서이다. FAMNet은 이 최적화 문제를 해결하기 위해 <strong>라그랑주 이완(Lagrangian Relaxation)</strong> 기법을 신경망 내부의 연산으로 구현하였다. 이를 통해 할당 과정 자체가 미분 가능해지며, 네트워크는 최종 할당 결과와 정답(Ground Truth) 사이의 차이를 손실함수로 정의하여 학습할 수 있게 된다.</p>
<h3>3.3  단일 객체 추적(SOT) 기능의 통합과 훈련 전략</h3>
<p>FAMNet은 탐지 결과가 누락되거나 부정확한 경우를 대비하여 단일 객체 추적(Single Object Tracking, SOT) 기능을 내재화하였다.6 이는 신뢰할 수 있는 탐지 결과가 없을 때, 이전 프레임의 정보를 바탕으로 객체의 위치를 예측하여 추적의 연속성을 유지하는 데 기여한다.</p>
<p>훈련 과정에서는 실제 추적 데이터셋(MOT15, MOT17 등)을 사용하여 종단간 학습을 수행한다. 손실 함수는 예측된 할당 행렬과 실제 정답 할당 행렬 사이의 차이를 최소화하도록 설계되었으며, 이 기울기(Gradient)는 MDA 레이어를 통과하여 친화도 모델과 특징 추출기까지 전달된다. 결과적으로 모델은 ‘추적 알고리즘이 할당하기 쉬운’ 형태의 특징을 학습하게 된다.</p>
<h3>3.4  벤치마크 성능 및 기술적 파급력</h3>
<p>MOT15, MOT17, KITTI-Car, UA-DETRAC 등 주요 벤치마크에서의 실험 결과, FAMNet은 발표 당시 SOTA 수준의 성능을 달성하였다.5 특히, MOTA(Multiple Object Tracking Accuracy)와 IDF1(Identification F1-score) 지표에서 두드러진 성과를 보였는데, 이는 ID 스위칭(ID Switching)과 궤적 단절(Fragmentation)이 크게 감소했음을 의미한다.</p>
<p>FAMNet의 성공은 이후 등장한 ‘DeepMOT’, ‘FairMOT’ 등의 후속 연구들에 지대한 영향을 미쳤다. 데이터 연관 과정을 딥러닝 네트워크의 일부로 포함시키려는 시도는 이제 현대 MOT 연구의 표준적인 흐름 중 하나로 자리 잡았다. 또한, 특징 정렬과 매칭을 결합한 아키텍처는 비디오 객체 분할(Video Object Segmentation) 등 인접 분야로도 확장되고 있다.</p>
<h2>4.  미세 표정 인식을 위한 FAMNet: 2D/3D 특징 융합과 계층적 어텐션</h2>
<p>가장 최근인 2025년에 발표된 FAMNet 연구는 미세 표정 인식(Micro-Expression Recognition, MER) 분야에서의 기술적 진보를 다루고 있다. “FAMNet: Integrating 2D and 3D Features for Micro-expression Recognition via Multi-task Learning and Hierarchical Attention“이라는 논문에서 제안된 이 모델은 미세 표정의 짧은 지속 시간과 낮은 강도라는 고유한 특성을 포착하기 위해 설계되었다.3</p>
<h3>4.1  미세 표정의 특성과 기술적 난제</h3>
<p>미세 표정(Micro-expression)은 0.5초 미만의 매우 짧은 시간 동안 얼굴의 국소 부위에서 억제된 감정이 무의식적으로 드러나는 현상이다. 이는 일반적인 표정(Macro-expression)과 달리 강도가 매우 약하고 순식간에 지나가기 때문에, 기존의 표정 인식 모델로는 감지하기 어렵다. 주요 난제는 (1) 미세한 공간적 변화를 감지하는 고해상도 특징 추출, (2) 짧은 시간 동안의 동적 변화를 포착하는 시간적 모델링, (3) 훈련 데이터의 부족과 클래스 불균형 문제 등이다.</p>
<h3>4.2  2D 및 3D 하이브리드 아키텍처: AMNet2D와 AMNet3D</h3>
<p>2025년형 FAMNet은 이러한 난제를 해결하기 위해 공간 정보와 시간 정보를 분리하여 처리한 후 통합하는 이원화된 전략을 취한다.</p>
<ol>
<li><strong>AMNet2D (2D CNN 브랜치):</strong> 이 브랜치는 정점 프레임(Apex Frame)과 같이 표정 변화가 가장 두드러지는 스틸 이미지에서 공간적인 특징을 추출한다. ResNet-18의 2D 변형을 백본으로 사용하며, 얼굴의 기하학적 구조와 텍스처 정보를 학습한다.4 정점 프레임은 전체 비디오 시퀀스 내에서 가장 중요한 정보를 담고 있는 ‘키 프레임’ 역할을 한다.</li>
<li><strong>AMNet3D (3D CNN 브랜치):</strong> 이 브랜치는 미세 표정 비디오 시퀀스 전체를 입력으로 받아 시간적 변화를 학습한다. 3D Convolution(C3D) 블록을 사용하여 시공간적(Spatiotemporal) 특징을 동시에 추출하며, 표정의 시작(Onset)부터 소멸(Offset)까지의 흐름을 포착한다. 이는 단순한 이미지의 나열이 아니라, 시간 축에 따른 픽셀의 변화량(Motion)을 모델링한다는 점에서 중요하다.4</li>
</ol>
<h3>4.3  계층적 어텐션(Hierarchical Attention) 모듈의 설계</h3>
<p>단순히 2D와 3D 특징을 결합하는 것만으로는 미세한 변화를 감지하기에 충분하지 않다. 배경 노이즈나 머리카락의 움직임 등 불필요한 정보가 미세 표정 신호를 덮어버릴 수 있기 때문이다. FAMNet은 이를 보완하기 위해 **계층적 어텐션 모듈(Hierarchical Attention Module)**을 네트워크의 여러 깊이(Layer)에 배치하였다.4</p>
<p>이 모듈은 소프트 어텐션(Soft Attention) 방식을 사용하여 어텐션 매트릭스를 계산한다. 이는 특징 맵(Feature Map)의 각 픽셀 위치에 가중치를 부여하는 방식으로, 미세 표정이 발생하는 눈가, 입가, 미간 등의 국소 영역(Region of Interest, ROI)에 네트워크가 집중하도록 유도한다. 계층적으로 적용된 어텐션은 저수준의 엣지 정보부터 고수준의 의미론적 정보까지 전 과정에서 중요한 신호를 보존하고 증폭하는 역할을 한다. 특히 AMNet2D와 AMNet3D 모두에 이 어텐션 모듈이 적용되어, 공간적 중요도와 시간적 중요도를 동시에 고려한다.</p>
<h3>4.4  멀티태스크 학습(Multi-task Learning)과 불확실성 손실 함수</h3>
<p>FAMNet은 미세 표정 인식(MER) 성능을 극대화하기 위해 **안면 액션 유닛 검출(Facial Action Unit Detection, FAUD)**을 보조 과제(Auxiliary Task)로 함께 학습한다.4 액션 유닛(AU)은 표정을 구성하는 기본 근육 움직임 단위(예: 눈썹 올리기, 입꼬리 당기기 등)로, MER과 밀접한 인과관계를 가진다. 멀티태스크 학습을 통해 공유된 파라미터들은 표정의 표면적인 클래스뿐만 아니라, 그 근원적인 근육 움직임 패턴을 학습하게 되며, 이는 주 과제인 MER의 일반화 성능 향상으로 이어진다.</p>
<p>또한, 두 가지 이상의 과제를 동시에 학습할 때 발생할 수 있는 손실 가중치(Loss Weight) 불균형 문제를 해결하기 위해 <strong>불확실성 손실(Uncertainty Loss)</strong> 함수를 도입하였다. 이는 각 태스크의 불확실성(Homoscedastic Uncertainty)을 학습 가능한 파라미터 <span class="math math-inline">\sigma</span>로 모델링하여, 손실 가중치를 동적으로 조절한다.4</p>
<p><span class="math math-display">L_{Total} = \frac{1}{\sigma_1^2} L_{MER} + \frac{1}{\sigma_2^2} L_{FAUD} + \log \sigma_1 + \log \sigma_2</span></p>
<p>이 수식에서 <span class="math math-inline">\sigma_1</span>과 <span class="math math-inline">\sigma_2</span>는 각각 MER과 FAUD 태스크의 불확실성을 나타낸다. 불확실성이 높은 태스크의 손실은 작게 반영되어 전체 학습의 안정성을 해치지 않도록 조정된다.</p>
<h3>4.5 실험 결과 및 미세 표정 인식 분야에서의 위상</h3>
<p>이 모델은 SAMM, CASME II, MMEW, CAS(ME)³ 등 주요 미세 표정 벤치마크 데이터셋에서 평가되었다. 실험 결과, FAMNet은 기존의 SOTA 방법론들(예: Dual-Inception, STSTNet 등) 대비 UAR(Unweighted Average Recall)과 UF1(Unweighted F1-score) 지표에서 유의미한 성능 향상을 보였다.</p>
<table><thead><tr><th><strong>데이터셋</strong></th><th><strong>UAR (%)</strong></th><th><strong>UF1 (%)</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>SAMM</strong></td><td>83.75</td><td>84.03</td><td>서양인 위주, 다양한 조명 조건 포함</td></tr>
<tr><td><strong>CASME II</strong></td><td>83.75</td><td>84.03</td><td>높은 프레임 레이트(200fps), 자연스러운 표정</td></tr>
<tr><td><strong>MMEW</strong></td><td>83.75</td><td>84.03</td><td>다중 모달, 높은 해상도</td></tr>
<tr><td><strong>CAS(ME)³</strong></td><td>51.00</td><td>43.42</td><td>깊이 정보 포함, 매우 도전적인 데이터셋</td></tr>
</tbody></table>
<p>표에서 볼 수 있듯이 FAMNet은 특히 SAMM과 CASME II 데이터셋에서 80% 이상의 높은 정확도를 달성하여, 미세 표정 인식 기술의 상용화 가능성을 한층 높였다는 평가를 받는다.3</p>
<h2>5. 스테레오 매칭, 퓨샷 학습 및 기타 응용 분야로의 확장</h2>
<p>FAMNet이라는 명칭은 자율주행을 위한 스테레오 매칭과 의료 영상 분할, 퓨샷 객체 카운팅 등 다양한 영역에서도 사용되고 있다. 이는 ’특징 정렬 및 매칭’이라는 개념이 컴퓨터 비전 전반에 걸쳐 보편적으로 중요한 주제임을 반증한다.</p>
<h3>5.1 경량화 스테레오 매칭 네트워크 (Stereo FAMNet)</h3>
<p>2025년 <em>Symmetry</em> 저널에 게재된 연구에서는 자율주행 시스템의 실시간 깊이 추정(Depth Estimation)을 위한 경량화된 모델로 FAMNet을 제안하였다.8 기존의 고성능 스테레오 매칭 네트워크들은 정확도를 높이기 위해 무거운 3D Convolution 연산을 과도하게 사용하는 경향이 있어, 실시간성이 요구되는 자율주행 환경에 적용하기 어려웠다.</p>
<p>Stereo FAMNet은 두 가지 핵심 모듈을 통해 이 문제를 해결한다.</p>
<ol>
<li>
<p>융합 어텐션 기반 비용 볼륨(Fusion Attention-based Cost Volume, FACV):</p>
<p>기존 방식은 왼쪽과 오른쪽 이미지의 특징을 단순히 연결(Concatenation)하거나 상관관계(Correlation)만 계산했다. FAMNet은 전체 상관관계(Full correlation)와 그룹 상관관계(Group correlation)를 동시에 계산하고, 이를 어텐션 메커니즘으로 융합한다. 이 과정에서 채널 재가중(Channel Reweighting) 기법을 사용하여 3D 연산 의존도를 줄이면서도 표현력이 풍부한 비용 볼륨을 생성한다.8 텍스처가 풍부한 영역에서는 상관관계가, 밋밋한 영역에서는 연결 정보가 더 중요한 역할을 하도록 동적으로 가중치를 조절한다.</p>
</li>
<li>
<p>다중 스케일 어텐션 집계(Multi-scale Attention Aggregation, MAA):</p>
<p>추출된 특징을 다양한 스케일의 피라미드 구조에서 집계한다. 이는 텍스처가 없는 영역(Textureless region)이나 반복적인 패턴이 있는 영역, 혹은 객체의 경계 부분에서도 강건한 매칭이 가능하도록 돕는다. 듀얼 패스 어텐션 메커니즘은 특징의 식별력을 더욱 강화한다.</p>
</li>
</ol>
<p>실험 결과, 이 모델은 BGNet이나 CoEx와 같은 기존 경량화 모델 대비 연산 비용을 약 20% 절감하면서도 KITTI 2012/2015 벤치마크에서 더 높은 정확도를 달성하였다. 이는 자원 제약이 있는 엣지 디바이스(Edge Device)에서의 실시간 깊이 인식 가능성을 열어준 연구로 평가된다.8</p>
<h3>5.2 퓨샷 카운팅 및 의료 영상 분할 (Few-Shot &amp; Medical FAMNet)</h3>
<p>Ranjan 등(CVPR 2021)은 퓨샷(Few-shot) 객체 카운팅을 위해 FAMNet을 제안하였다.10 이 모델은 소수의 예시(Exemplar) 이미지가 주어졌을 때, 쿼리 이미지 내의 동일한 객체 수를 세는 작업을 수행한다. 여기서 FAMNet은 **퓨샷 적응 및 매칭 네트워크(Few-shot Adaptation &amp; Matching Network)**를 의미한다. 이 모델의 특징은 테스트 시점에 새로운 카테고리에 적응(Adaptation)하는 메커니즘을 내장하고 있다는 점이다. 메타 러닝(Meta-learning) 기반의 접근을 통해, 모델은 훈련 중에 보지 못한 새로운 객체(예: 세포, 동물, 과일 등)에 대해서도 몇 개의 예시만으로 밀도 맵(Density Map)을 생성하여 정확한 개수를 추정한다.</p>
<p>또한, Bo 등(2025)은 의료 영상 분할을 위한 FAMNet을 제안하였는데, 이는 **주파수 인식 매칭 네트워크(Frequency-aware Matching Network)**를 뜻한다.11 의료 영상(MRI, CT 등)은 촬영 장비나 프로토콜에 따라 도메인 차이(Domain Gap)가 심하게 발생한다. 이 FAMNet은 공간 영역(Spatial Domain)뿐만 아니라 주파수 영역(Frequency Domain)에서의 특징 매칭을 수행함으로써, 도메인 간의 스타일 차이를 극복하고 병변이나 장기 분할의 일반화 성능을 높이는 데 주력한다.</p>
<h2>6. FAMNet 아키텍처의 종합적 비교 및 통찰: 정렬의 진화</h2>
<p>앞서 살펴본 다양한 FAMNet 변형들은 각기 다른 응용 분야를 타겟으로 하지만, 기술적으로 깊은 연관성을 지닌다. 이들을 관통하는 핵심 주제(Theme)를 분석하면 딥러닝 아키텍처의 진화 방향에 대한 통찰을 얻을 수 있다.</p>
<h3>6.1 기하학적 정렬에서 의미론적 특징 공간 정렬로의 진화</h3>
<p>초기 얼굴 랜드마크 FAMNet 1은 3DMM 파라미터를 이용한 <strong>물리적/기하학적 정렬</strong>에 집중했다. 이는 얼굴이라는 객체의 물리적 구조를 직접적으로 모델링하는 방식이다. 반면, MOT를 위한 FAMNet 2은 딥러닝 특징 맵 상에서의 <strong>ROI 정렬</strong>과 시간적 차원에서의 <strong>할당 정렬</strong>로 개념을 확장했다. 이는 물리적 위치뿐만 아니라 객체의 ID라는 추상적 개념을 정렬하는 것이다.</p>
<p>최신 MER FAMNet 4과 Stereo FAMNet 8은 이를 <strong>어텐션 기반의 특징 재가중(Reweighting) 및 융합</strong>으로 더욱 발전시켰다. 이제 정렬은 단순히 픽셀 위치를 맞추는 것을 넘어, 추상적인 특징 공간(Feature Space) 상에서 정보의 흐름을 조절하고 중요한 정보끼리 매칭시키는 고차원적인 과정이 되었다. 이는 딥러닝 모델이 단순한 패턴 매칭을 넘어 의미론적 이해(Semantic Understanding)를 수행하고 있음을 시사한다.</p>
<h3>6.2 다중 스케일(Multi-scale) 전략의 필수화</h3>
<p>모든 FAMNet 변형에서 다중 스케일 처리는 필수적인 요소로 자리 잡았다. 미세 표정 인식에서는 국소 부위(눈, 입)의 미세 특징과 얼굴 전체의 전역 특징을 결합해야 하며 4, 객체 추적이나 카운팅에서는 다양한 크기의 객체를 처리하기 위해 피라미드 구조(Feature Pyramid)가 사용된다.12 이는 딥러닝 모델이 ‘무엇을(What)’ 인식하는지를 넘어 ‘어디에(Where)’ 그리고 ‘얼마나(How much)’ 변화했는지를 정밀하게 파악하기 위해 필수불가결한 접근법이다. 특히 어텐션 메커니즘과 결합된 다중 스케일 전략은 모델이 이미지의 전체 맥락(Context)을 놓치지 않으면서도 세부적인 디테일에 집중할 수 있게 한다.</p>
<h3>6.3 종단간(End-to-End) 미분 가능 학습의 심화</h3>
<p>MOT용 FAMNet 2이 보여주듯, 기존에 분리되어 있던 최적화 단계(특징 추출, 매칭, 할당)를 하나의 미분 가능한 파이프라인으로 통합하려는 시도는 딥러닝 연구의 큰 흐름이다. 이는 모델이 각 서브 모듈의 국소 최적해(Local Minima)에 빠지는 것을 방지하고, 전체 시스템의 전역 최적해(Global Optima)를 찾도록 유도한다. 이러한 경향은 MER FAMNet에서의 멀티태스크 학습(MER+FAUD)에서도 나타나며, 서로 다른 과제가 상호 보완적으로 학습을 돕는 구조로 발전하고 있다. 미분 불가능해 보이는 과정(예: 할당, 카운팅, 샘플링)을 미분 가능한 형태로 근사(Approximation)하거나 이완(Relaxation)하여 네트워크에 포함시키는 기술은 FAMNet 연구들이 보여주는 중요한 기술적 성취이다.</p>
<table><thead><tr><th><strong>특징</strong></th><th><strong>얼굴 랜드마크 FAMNet</strong></th><th><strong>MOT FAMNet</strong></th><th><strong>MER FAMNet</strong></th><th><strong>스테레오 FAMNet</strong></th></tr></thead><tbody>
<tr><td><strong>주요 목적</strong></td><td>랜드마크 위치 회귀</td><td>다중 객체 추적</td><td>미세 표정 분류</td><td>깊이(Disparity) 추정</td></tr>
<tr><td><strong>핵심 모듈</strong></td><td>자기 반복 회귀, LAN</td><td>특징+친화도+할당 결합</td><td>2D/3D CNN, 계층적 어텐션</td><td>FACV, MAA</td></tr>
<tr><td><strong>정렬 방식</strong></td><td>파라미터 공간 샘플링</td><td>ROI 풀링, 할당 최적화</td><td>시공간 특징 융합</td><td>상관관계+어텐션 융합</td></tr>
<tr><td><strong>학습 전략</strong></td><td>반복적 위치 갱신 (SIR)</td><td>종단간 미분 가능 할당</td><td>멀티태스크(AU+MER)</td><td>비용 볼륨 정규화</td></tr>
<tr><td><strong>성능 지표</strong></td><td>NME (Error)</td><td>MOTA, IDF1</td><td>UAR, UF1</td><td>EPE, Outlier Ratio</td></tr>
</tbody></table>
<h2>7. 결론 및 향후 전망</h2>
<p>본 보고서에서는 ’FAMNet’이라는 명칭을 공유하는 컴퓨터 비전 분야의 주요 연구들을 심층적으로 분석하였다. 분석 결과, FAMNet은 단순한 하나의 모델이 아니라 **특징 정렬(Feature Alignment), 다중 스케일 처리(Multi-scale Processing), 어텐션 메커니즘(Attention Mechanism)**을 통해 고차원의 매칭 및 인식 문제를 해결하고자 하는 일련의 연구 흐름을 대변한다.</p>
<p>얼굴 랜드마크 검출에서는 반복적인 회귀와 어텐션을 통해 정밀도를 높였고, 객체 추적에서는 특징 학습과 데이터 연관을 통합하여 추적의 안정성을 확보했다. 최신의 미세 표정 인식 및 스테레오 매칭 연구에서는 경량화와 실시간 처리, 그리고 미세한 시공간적 변화 포착을 위해 구조적으로 더욱 고도화된 어텐션 융합 모듈을 채택하고 있다. 특히 2025년의 연구들은 2D 공간 정보와 3D 시간 정보를 효과적으로 결합하는 하이브리드 아키텍처가 미세한 신호 탐지에 탁월함을 입증하였다.</p>
<p>이러한 FAMNet의 진화 과정은 컴퓨터 비전 기술이 단순히 정적인 이미지를 해석하는 단계를 넘어, 시간의 흐름, 미세한 변화, 그리고 복잡한 객체 간의 관계를 종합적으로 이해하고 추론하는 방향으로 발전하고 있음을 명확히 보여준다. 향후 연구에서는 이러한 다양한 도메인의 FAMNet 기술들이 상호 융합되어, 예를 들어 ‘미세 표정을 실시간으로 인식하며 다중 인물을 정밀하게 추적하는’ 복합적인 지능형 시스템으로 발전할 가능성이 높다. 또한, 트랜스포머(Transformer) 아키텍처와의 결합, 대규모 언어 모델(LLM)을 활용한 멀티모달 이해 등으로의 확장이 기대된다. 연구자들은 각 FAMNet 변형에서 제안된 ’미분 가능한 매칭’과 ’계층적 특징 융합’의 원리를 자신의 도메인에 맞게 창의적으로 변용함으로써 지속적인 성능 향상을 꾀할 수 있을 것이다.</p>
<p>이상으로 FAMNet에 대한 포괄적인 분석 보고서를 마친다. 각 연구의 세부적인 수학적 모델링과 실험 설정은 인용된 참고문헌과 본문의 기술적 내용을 통해 더욱 깊이 있게 탐구할 수 있다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>Facial Landmarks Detection by Self-Iterative Regression Based …, 12월 16, 2025에 액세스, https://cdn.aaai.org/ojs/12275/12275-13-15803-1-2-20201228.pdf</li>
<li>FAMNet: Joint Learning of Feature, Affinity and Multi-Dimensional …, 12월 16, 2025에 액세스, https://openaccess.thecvf.com/content_ICCV_2019/papers/Chu_FAMNet_Joint_Learning_of_Feature_Affinity_and_Multi-Dimensional_Assignment_for_ICCV_2019_paper.pdf</li>
<li>[2508.13483] FAMNet: Integrating 2D and 3D Features for Micro …, 12월 16, 2025에 액세스, https://arxiv.org/abs/2508.13483</li>
<li>FAMNet: Integrating 2D and 3D Features for Micro-expression …, 12월 16, 2025에 액세스, https://arxiv.org/html/2508.13483v1</li>
<li>[1904.04989] FAMNet: Joint Learning of Feature, Affinity and Multi …, 12월 16, 2025에 액세스, https://arxiv.org/abs/1904.04989</li>
<li>Associative affinity network learning for multi-object tracking∗, 12월 16, 2025에 액세스, https://jzus.zju.edu.cn/opentxt.php?doi=10.1631/FITEE.2000272</li>
<li>One More Check: Making “Fake Background” Be Tracked Again, 12월 16, 2025에 액세스, https://www.researchgate.net/publication/350991940_One_More_Check_Making_Fake_Background_Be_Tracked_Again</li>
<li>FAMNet: A Lightweight Stereo Matching Network for Real-Time …, 12월 16, 2025에 액세스, https://www.mdpi.com/2073-8994/17/8/1214</li>
<li>FAMNet: A Lightweight Stereo Matching Network for Real-Time …, 12월 16, 2025에 액세스, https://www.researchgate.net/publication/394231216_FAMNet_A_Lightweight_Stereo_Matching_Network_for_Real-Time_Depth_Estimation_in_Autonomous_Driving</li>
<li>Learning To Count Everything - CVF Open Access, 12월 16, 2025에 액세스, https://openaccess.thecvf.com/content/CVPR2021/papers/Ranjan_Learning_To_Count_Everything_CVPR_2021_paper.pdf</li>
<li>FAMNet: Frequency-aware Matching Network for Cross-domain Few …, 12월 16, 2025에 액세스, https://chatpaper.com/paper/159500</li>
<li>Multiple Object Tracking With Correlation Learning - CVF Open Access, 12월 16, 2025에 액세스, https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Multiple_Object_Tracking_With_Correlation_Learning_CVPR_2021_paper.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>