<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:S2M2 확장형 스테레오 매칭 모델 (Scalable Stereo Matching Model for Reliable Depth Estimation)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>S2M2 확장형 스테레오 매칭 모델 (Scalable Stereo Matching Model for Reliable Depth Estimation)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">비전 기반 깊이 추정 (Vision Based Dept)</a> / <span>S2M2 확장형 스테레오 매칭 모델 (Scalable Stereo Matching Model for Reliable Depth Estimation)</span></nav>
                </div>
            </header>
            <article>
                <h1>S2M2 확장형 스테레오 매칭 모델 (Scalable Stereo Matching Model for Reliable Depth Estimation)</h1>
<p>2025-12-16, G30DR</p>
<h2>1.  서론: 스테레오 비전의 확장성 딜레마와 패러다임의 전환</h2>
<p>컴퓨터 비전(Computer Vision) 분야에서 스테레오 매칭(Stereo Matching)은 두 개의 2차원 이미지로부터 3차원 기하 정보를 복원하는 가장 근본적이면서도 난이도 높은 문제 중 하나이다. 인간의 시각 시스템이 양안 시차(Binocular Disparity)를 이용하여 깊이(Depth)를 지각하는 원리를 모방한 이 기술은, 자율 주행 자동차, 로봇 내비게이션, 증강 현실(AR), 그리고 디지털 트윈(Digital Twin) 생성에 이르기까지 공간지능(Spatial Intelligence)을 요구하는 모든 현대 산업의 핵심 기반 기술로 자리 잡았다.</p>
<p>최근 딥러닝(Deep Learning) 기술의 비약적인 발전은 스테레오 매칭의 정확도를 획기적으로 향상시켰다. 초기 CNN(Convolutional Neural Network) 기반의 접근법들은 전통적인 알고리즘이 해결하지 못했던 텍스처 부족 영역(Texture-less regions)이나 반사 표면(Non-Lambertian surfaces)에서의 매칭 성능을 크게 개선했다. 그러나 이러한 발전 이면에는 ’확장성(Scalability)’과 ’일반화(Generalization)’라는 두 가지 거대한 장벽이 존재해 왔다. 기존의 SOTA(State-of-the-Art) 모델들은 특정 데이터셋이나 제한된 해상도 내에서는 뛰어난 성능을 보이지만, 해상도가 4K 이상으로 증가하거나 학습되지 않은 낯선 도메인 환경에서는 메모리 병목 현상과 급격한 성능 저하를 겪는 ’확장성의 딜레마’에 봉착해 있었다.1</p>
<p>2025년 ICCV에서 발표된 **S2M2 (Scalable Stereo Matching Model)**는 이러한 딜레마를 해결하기 위해 제안된 혁신적인 아키텍처이다. S2M2는 기존의 3D CNN 기반 비용 볼륨(Cost Volume) 방식이나 반복적 지역 정제(Iterative Local Refinement) 방식이 갖는 구조적 한계를 극복하기 위해, <strong>트랜스포머(Transformer)</strong> 기반의 글로벌 매칭(Global Matching)과 <strong>최적 수송(Optimal Transport)</strong> 이론을 융합했다.1 본 보고서는 S2M2의 기술적 메커니즘을 심층적으로 해부하고, 이를 RAFT-Stereo, CREStereo, LEAStereo 등 기존의 주요 모델들과 비교 분석함으로써, 현대 스테레오 매칭 기술의 현주소와 미래 방향성을 포괄적으로 조망한다. 특히 ’확장성’이 단순한 해상도의 증가를 넘어, 모델의 크기, 연산 자원, 그리고 도메인 적응력 측면에서 어떤 의미를 갖는지 면밀히 고찰한다.</p>
<h2>2.  스테레오 매칭의 기술적 계보와 한계점 분석</h2>
<p>S2M2의 혁신성을 이해하기 위해서는 스테레오 매칭 기술이 걸어온 발전 과정과 각 단계에서 마주했던 임계점을 명확히 파악해야 한다.</p>
<h3>2.1  1세대: 고전적 알고리즘과 SGM의 시대</h3>
<p>딥러닝 이전 시대의 스테레오 매칭은 주로 국소적인 윈도우 매칭(Local Window Matching)이나 전역적 에너지 최소화(Global Energy Minimization) 기법에 의존했다. 특히 **SGM(Semi-Global Matching)**은 2차원 이미지를 여러 개의 1차원 경로(Path)로 분해하여 동적 계획법(Dynamic Programming)을 적용함으로써, 효율성과 정확도 사이의 균형을 맞춘 기념비적인 알고리즘이었다.4 SGM은 현재까지도 FPGA나 임베디드 시스템에서 실시간 처리를 위해 널리 사용되고 있으나, 텍스처가 없는 영역이나 반복적인 패턴에서는 매칭 모호성(Ambiguity)을 해결하지 못하고 노이즈가 많은 시차 맵(Disparity Map)을 생성한다는 본질적인 한계를 지닌다.1</p>
<h3>2.2  2세대: 딥러닝과 3D 비용 볼륨 (Volumetric Methods)</h3>
<p>심층 신경망(DNN)의 도입은 매칭 비용 계산(Matching Cost Computation)을 학습 기반으로 대체하는 것에서 시작되었다. MC-CNN을 필두로, GC-Net, PSMNet과 같은 모델들은 좌우 이미지의 특징(Feature)을 결합하여 4차원 비용 볼륨(<span class="math math-inline">H \times W \times D \times C</span>)을 생성하고, 이를 3D CNN으로 정제(Aggregation)하는 방식을 채택했다.5</p>
<p>이 방식은 기하학적 문맥(Geometric Context)을 학습할 수 있어 정확도가 매우 높았지만, 치명적인 단점이 있었다. 바로 연산량과 메모리 사용량이 입력 해상도와 시차 범위(Disparity Range)에 따라 3제곱(Cubic)으로 증가한다는 점이다. 고해상도 이미지일수록 탐색해야 할 시차 범위(<span class="math math-inline">D</span>)가 늘어나는데, 3D CNN 기반 모델들은 이러한 고해상도 환경에서 메모리 부족(OOM) 현상을 빈번하게 유발하여 ’확장성’이 극도로 제한되었다.1</p>
<h3>2.3  3세대: 반복적 정제 (Iterative Refinement)와 RAFT-Stereo</h3>
<p>이러한 메모리 문제를 해결하기 위해 등장한 것이 <strong>RAFT-Stereo</strong>와 같은 반복적 정제 모델이다.7 이들은 거대한 4D 볼륨을 한 번에 처리하는 대신, 모든 픽셀 쌍의 상관관계(All-pairs correlation)를 계산하고, GRU(Gated Recurrent Unit)를 이용해 시차 필드(Disparity Field)를 반복적으로 업데이트하는 방식을 취했다.</p>
<ul>
<li><strong>장점:</strong> 미세한 구조 복원에 강력하며, 언제든지 추론을 중단하여 속도를 조절할 수 있는 유연성을 가진다.</li>
<li><strong>한계:</strong> 본질적으로 ’지역적 탐색(Local Search)’에 기반하기 때문에, 초기 추정이 빗나가거나 거대한 텍스처리스 영역을 만날 경우 전역적인 일관성을 잃고 지역 최적점(Local Minima)에 빠지는 경향이 있다.2 이는 모델의 일반화 성능을 저해하는 주된 요인이 된다.</li>
</ul>
<h3>2.4  확장성 위기 (Scalability Crisis)와 S2M2의 등장 배경</h3>
<p>결국 기존의 모델들은 “정확하지만 무거운 3D 볼륨 방식“과 “가볍지만 전역적 일관성이 부족한 반복적 방식” 사이의 딜레마에 갇혀 있었다. 고해상도(High-Resolution), 큰 시차(Large Disparity), 그리고 실시간(Real-Time) 처리를 동시에 만족하는 모델은 부재했다. <strong>S2M2</strong>는 이러한 배경 속에서, 트랜스포머의 장거리 의존성(Long-range dependency) 학습 능력과 최적 수송 이론을 결합하여 이 딜레마를 타파하고자 등장했다.1</p>
<h2>3.  S2M2 (Scalable Stereo Matching Model) 아키텍처 심층 분석</h2>
<p>S2M2의 아키텍처는 크게 <strong>특징 추출(Feature Extraction)</strong>, <strong>글로벌 매칭(Global Matching)</strong>, <strong>정제(Refinement)</strong>, 그리고 **업샘플링(Upsampling)**의 4단계로 구성된다. 각 단계는 확장성과 정확도를 극대화하기 위해 정교하게 설계되었다.</p>
<h3>3.1  계층적 특징 추출과 다해상도 트랜스포머 (MRT)</h3>
<p>스테레오 매칭의 첫 단계는 입력 이미지로부터 매칭에 유용한 특징을 추출하는 것이다. S2M2는 단순한 CNN 백본 대신 **다해상도 트랜스포머(Multi-Resolution Transformer, MRT)**를 도입하여 특징 추출 단계에서부터 전역적인 문맥 정보를 확보한다.1</p>
<h4>3.1.1  FPN과 초기 특징 피라미드</h4>
<p>입력 이미지는 먼저 경량화된 CNN 백본과 FPN(Feature Pyramid Network)을 통과하며 <span class="math math-inline">{F_4, F_8, F_{16}, F_{32}}</span>의 다중 스케일 특징 피라미드로 변환된다. 여기서 아래 첨자는 원본 해상도 대비 다운샘플링 비율을 나타낸다.1 이는 이미지의 세밀한 텍스처 정보와 거시적인 구조 정보를 동시에 포착하기 위함이다.</p>
<h4>3.1.2  다해상도 트랜스포머 (MRT)의 역할</h4>
<p>일반적인 비전 트랜스포머(ViT)는 고해상도 이미지의 모든 픽셀 간 어텐션(Attention)을 계산해야 하므로 연산량이 픽셀 수의 제곱에 비례해 폭증한다. S2M2의 MRT는 이를 효율화하기 위해 설계되었다.</p>
<ul>
<li><strong>확장성 확보:</strong> MRT는 입력 해상도뿐만 아니라 모델의 크기(채널 수, 레이어 수)에 따라 유동적으로 확장 가능하다. 이는 S2M2가 모바일 기기용 소형 모델(S)부터 서버용 대형 모델(XL)까지 다양한 버전을 제공할 수 있는 기반이 된다.9</li>
<li><strong>글로벌 컨텍스트:</strong> 트랜스포머의 셀프 어텐션(Self-Attention) 메커니즘은 이미지 내의 멀리 떨어진 픽셀 간의 관계를 학습한다. 이는 텍스처가 없는 벽면이나 반복 패턴 영역에서 주변의 텍스처 정보를 끌어와 매칭 모호성을 해결하는 데 결정적인 역할을 한다.</li>
</ul>
<h4>3.1.3  적응형 게이트 융합 레이어 (Adaptive Gated Fusion Layer, AGFL)</h4>
<p>서로 다른 해상도의 특징 맵을 단순히 더하거나 연결(Concatenation)하는 기존 방식은 정보의 손실이나 불필요한 노이즈의 유입을 초래할 수 있다. S2M2는 <strong>AGFL</strong>을 도입하여 이를 개선했다.1</p>
<ul>
<li><strong>메커니즘:</strong> AGFL은 각 스케일의 특징 맵에 대해 게이트(Gate) 값을 계산하여, 현재 픽셀 위치에서 어떤 스케일의 정보가 더 중요한지를 동적으로 결정한다. 예를 들어, 복잡한 엣지 영역에서는 고해상도 특징(<span class="math math-inline">F_4</span>)의 비중을 높이고, 평탄한 영역에서는 저해상도 특징(<span class="math math-inline">F_{32}</span>)의 비중을 높이는 식이다. 이를 통해 특징 맵의 정제 효율을 극대화한다.</li>
</ul>
<h3>3.2  최적 수송(Optimal Transport) 기반의 글로벌 매칭</h3>
<p>S2M2의 가장 독창적인 부분은 매칭 단계를 **최적 수송 문제(Optimal Transport Problem)**로 재정의한 것이다.1</p>
<h4>3.2.1  문제 정의</h4>
<p>전통적인 매칭은 각 픽셀에 대해 비용(Cost)이 가장 낮은 시차를 독립적으로 선택하는 ‘승자 독식(Winner-takes-all)’ 방식이거나, 1D 라인을 따른 에너지 최소화 방식이었다. 반면, S2M2는 좌측 이미지의 특징 분포를 우측 이미지의 특징 분포로 옮기는 데 드는 ’전역적 비용’을 최소화하는 해를 찾는다.</p>
<ul>
<li><strong>글로벌 일관성:</strong> 최적 수송은 본질적으로 전역 최적화(Global Optimization) 알고리즘이다. 따라서 국소적으로는 매칭이 모호하더라도(예: 흰 벽), 전체적인 구조적 정합성을 고려하여 가장 타당한 매칭을 강제한다. 이는 텍스처리스 영역이나 폐색 영역에서의 성능을 비약적으로 향상시킨다.</li>
<li><strong>구현:</strong> S2M2는 이를 위해 싱크혼(Sinkhorn) 알고리즘이나 이와 유사한 미분 가능한 최적 수송 레이어를 사용하여, 학습 과정(End-to-End Learning) 내에서 매칭을 수행한다.</li>
</ul>
<h3>3.3  신뢰도, 폐색, 시차의 결합 추정 (Joint Estimation)</h3>
<p>S2M2는 단순히 시차(Disparity) 값 하나만을 출력하지 않는다. **시차(Disparity), 폐색(Occlusion), 신뢰도(Confidence)**를 하나의 통합된 프레임워크 내에서 동시에 추정한다.1</p>
<ul>
<li><strong>상호 보완적 관계:</strong></li>
<li><strong>폐색(Occlusion):</strong> 가려진 영역을 정확히 알면, 해당 영역에서의 무의미한 매칭 시도를 차단하고 주변 정보를 이용한 보간(Interpolation)을 유도할 수 있다.</li>
<li><strong>신뢰도(Confidence):</strong> 모델이 자신의 예측에 대해 얼마나 확신하는지를 나타낸다. 신뢰도가 낮은 영역(주로 폐색이나 경계 부분)은 후처리 단계에서 필터링하거나 가중치를 낮추어 전체 깊이 맵의 품질을 높이는 데 기여한다.</li>
<li><strong>새로운 손실 함수 (Novel Loss Function):</strong> S2M2는 가능한 매칭(Feasible Matches)에 확률을 집중시키는 새로운 손실 함수를 제안했다.1 이는 기존의 Soft-Argmin 방식이 갖는 확률 분포의 퍼짐(Dispersion) 현상을 억제하여, 물체의 경계면에서 발생하는 스무딩(Smoothing) 효과를 줄이고 날카로운 엣지를 복원하게 한다.</li>
</ul>
<h3>3.4  모델 변형(Variants)과 자원 효율성</h3>
<p>S2M2는 ’확장성’이라는 이름에 걸맞게 다양한 자원 제약 조건에 맞춘 네 가지 모델 변형을 제공한다.9 이는 사용자가 정확도와 속도 사이의 트레이드오프(Trade-off)를 선택할 수 있게 한다.</p>
<table><thead><tr><th><strong>모델 변형</strong></th><th><strong>채널 (CH)</strong></th><th><strong>Transformer Blocks (NTR)</strong></th><th><strong>640x480 (FPS)</strong></th><th><strong>1216x1024 (FPS)</strong></th><th><strong>2432x2048 (FPS)</strong></th><th><strong>특징 및 용도</strong></th></tr></thead><tbody>
<tr><td><strong>S2M2-S</strong></td><td>128</td><td>1</td><td><strong>68.7</strong></td><td>18.8</td><td>3.9</td><td>초고속 추론, 모바일/드론용</td></tr>
<tr><td><strong>S2M2-M</strong></td><td>192</td><td>2</td><td>34.8</td><td>8.9</td><td>1.9</td><td>성능과 속도의 균형</td></tr>
<tr><td><strong>S2M2-L</strong></td><td>256</td><td>3</td><td>20.5</td><td>5.3</td><td>1.2</td><td>고해상도 정밀 계측용</td></tr>
<tr><td><strong>S2M2-XL</strong></td><td>384</td><td>3</td><td>11.2</td><td>2.7</td><td>0.64</td><td><strong>SOTA 성능</strong>, 오프라인 3D 재구성</td></tr>
</tbody></table>
<p>위 표에서 주목할 점은 S2M2-S 모델이 VGA 해상도에서 60 FPS를 초과하는 실시간 성능을 보여준다는 것이다. 반면, 가장 무거운 S2M2-XL 모델조차도 4K급(2432x2048 이상) 이미지를 메모리 오버플로우 없이 처리할 수 있다. 이는 기존의 3D CNN 기반 모델들이 고해상도에서 겪었던 구조적 한계를 극복했음을 증명하는 데이터이다.9</p>
<h2>4.  SOTA 모델과의 비교 분석: 경쟁의 지형도</h2>
<p>S2M2의 위치를 명확히 하기 위해서는 현재 스테레오 매칭 분야를 주도하고 있는 <strong>RAFT-Stereo</strong>, <strong>CREStereo</strong>, <strong>LEAStereo</strong>와의 상세한 비교가 필요하다. 이들은 각기 다른 철학과 아키텍처를 기반으로 발전해 왔다.</p>
<h3>4.1  RAFT-Stereo: 반복적 흐름 추정의 강자</h3>
<p><strong>RAFT-Stereo</strong>는 2021년 등장하여 스테레오 매칭의 패러다임을 바꾼 모델이다.7</p>
<ul>
<li><strong>아키텍처 특징:</strong> 전체 비용 볼륨을 구축하는 대신, 모든 픽셀 쌍의 상관관계를 계산하고 이를 4D 볼륨으로 압축하지 않은 채 유지한다. 이후 GRU 기반의 업데이트 블록이 현재의 시차 추정치 주변에서 상관관계 값을 조회(Lookup)하며 시차를 반복적으로 수정한다.</li>
<li><strong>S2M2와의 비교:</strong></li>
<li><strong>장점:</strong> RAFT-Stereo는 반복 횟수를 늘릴수록 미세한 디테일이 살아나는 특성이 있다. 특히 얇은 구조물 복원에 강하다.</li>
<li><strong>단점:</strong> ’지역적 탐색’을 반복하는 구조이기에, 초기 추정이 크게 벗어나면 회복이 불가능한 경우가 많다. 또한 고해상도 이미지에서 ’모든 픽셀 쌍’의 상관관계를 계산하는 것은 메모리 부담이 크다. S2M2는 트랜스포머를 통한 ’글로벌 매칭’으로 초기부터 전역적으로 일관된 해를 찾으므로, RAFT-Stereo가 실패하기 쉬운 거대한 텍스처리스 영역에서 더 강인하다.1</li>
</ul>
<h3>4.2  CREStereo: 실세계 적응을 위한 계층적 접근</h3>
<p>**CREStereo (Cascaded Recurrent Stereo)**는 2022년 CVPR에서 발표되었으며, 실제 환경에서의 강인함(Robustness)을 최우선으로 설계되었다.11</p>
<ul>
<li><strong>적응형 그룹 상관관계 (Adaptive Group Correlation):</strong> 실제 카메라 시스템은 완벽하게 정류(Rectification)되지 않을 수 있다. CREStereo는 이를 고려하여, 탐색 범위를 수평선(Epipolar line)에 고정하지 않고 변형 가능한 윈도우를 통해 수직 시차 오차까지 수용할 수 있는 적응형 상관관계 레이어를 제안했다.11</li>
<li><strong>S2M2와의 비교:</strong> CREStereo는 정류 오차에 매우 강인하다는 독보적인 장점이 있다. 그러나 S2M2는 정류가 이상적(수직 시차 &lt; 2px)이라는 가정하에, 트랜스포머의 강력한 문맥 이해 능력을 바탕으로 순수한 매칭 정확도와 폐색 처리 능력에서 우위를 점한다. 벤치마크 결과는 이상적인 정류 환경에서 S2M2가 더 높은 정밀도를 달성함을 보여준다.3</li>
</ul>
<h3>4.3  LEAStereo: 신경망 구조 탐색(NAS)의 효율성</h3>
<p><strong>LEAStereo</strong>는 사람이 설계한 구조의 비효율성을 지적하며, NAS(Neural Architecture Search)를 통해 최적의 네트워크 구조를 자동으로 탐색한 모델이다.14</p>
<ul>
<li><strong>특징:</strong> 특징 추출기와 매칭 네트워크의 구조를 동시에 탐색하여, 매우 적은 파라미터(1.81M)로도 당시 SOTA 성능을 달성했다. 이는 연산 효율성 측면에서 큰 혁신이었다.16</li>
<li><strong>S2M2와의 비교:</strong> LEAStereo는 효율성이 뛰어나지만, NAS로 찾은 구조가 특정 데이터셋에 과적합(Overfitting)되는 경향이 있다. S2M2는 인간의 직관(트랜스포머의 글로벌 컨텍스트, 최적 수송의 일관성)을 반영한 설계를 통해, 데이터셋에 구애받지 않는 더 넓은 범위의 ‘일반화’ 능력을 보여준다.</li>
</ul>
<h3>4.4  비교 요약 표</h3>
<table><thead><tr><th><strong>특징</strong></th><th><strong>S2M2 (2025)</strong></th><th><strong>CREStereo (2022)</strong></th><th><strong>RAFT-Stereo (2021)</strong></th><th><strong>LEAStereo (2020)</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 메커니즘</strong></td><td>Transformer + Optimal Transport</td><td>Adaptive Cascade + Recurrent</td><td>All-pairs Corr. + GRU</td><td>Neural Architecture Search</td></tr>
<tr><td><strong>글로벌 컨텍스트</strong></td><td><strong>최상</strong> (Transformer Self-Attention)</td><td>중 (Hierarchical Refinement)</td><td>하 (Iterative Local Update)</td><td>중 (Optimized Receptive Field)</td></tr>
<tr><td><strong>정류 오차 허용</strong></td><td>낮음 (이상적 정류 가정)</td><td><strong>높음</strong> (Adaptive Correlation)</td><td>낮음</td><td>낮음</td></tr>
<tr><td><strong>확장성 (Res)</strong></td><td><strong>최상</strong> (4K+ 지원, OOM 방지)</td><td>상</td><td>중 (메모리 사용량 큼)</td><td>상</td></tr>
<tr><td><strong>주요 강점</strong></td><td>일반화, 텍스처리스 영역, 신뢰도</td><td>실세계 강인성, 정류 오차 보정</td><td>미세 디테일, 얇은 구조</td><td>파라미터 효율성, 속도</td></tr>
</tbody></table>
<h2>5.  성능 벤치마크 및 데이터 분석</h2>
<p>S2M2의 우수성은 주요 공개 벤치마크에서의 정량적 결과를 통해 입증된다. 특히 Middlebury V3, ETH3D, 그리고 Booster 데이터셋에서의 성과는 S2M2가 현재 가장 신뢰할 수 있는 모델임을 시사한다.</p>
<h3>5.1  Middlebury V3 벤치마크</h3>
<p>Middlebury 데이터셋은 고해상도 실내 이미지와 매우 정밀한 구조광(Structured Light) 기반의 GT(Ground Truth)를 제공하여 스테레오 매칭의 난공불락 요새로 불린다.13</p>
<ul>
<li><strong>결과:</strong> S2M2는 <code>bad 2.0</code> (2픽셀 이상의 오차를 가진 픽셀 비율) 지표에서 경쟁 모델들을 압도하며 1위를 차지했다.</li>
<li><strong>정성적 분석:</strong> 특히 자전거 바퀴살(Bicycle spokes), 식물의 줄기 등 얇고 복잡한 구조물에서 S2M2의 복원력이 돋보인다. 기존 모델들이 과도한 스무딩으로 인해 이러한 디테일을 뭉개버리는 반면, S2M2는 트랜스포머가 중요한 특징을 보존하면서 배경과 분리해내는 능력을 보여주었다.1</li>
</ul>
<h3>5.2  Booster 벤치마크 (Extreme Conditions)</h3>
<p>Booster 데이터셋은 투명한 유리, 거울, 강한 반사광 등 일반적인 스테레오 매칭 알고리즘이 실패하는 극한의 조건들을 포함한다.17</p>
<ul>
<li><strong>의의:</strong> 이 데이터셋에서의 1위 달성은 S2M2가 단순히 픽셀 간의 색상 유사도(Photometric Consistency)에만 의존하지 않음을 증명한다. 색상 정보가 왜곡되거나 없는 상황에서도, 주변 문맥과 최적 수송의 기하학적 제약 조건을 통해 올바른 깊이를 추론해낸 것이다.9 이는 S2M2가 진정한 의미의 ‘지각적(Perceptual)’ 매칭을 수행하고 있음을 시사한다.</li>
</ul>
<h3>5.3  추론 속도와 메모리 효율성</h3>
<p>9의 데이터에 따르면, S2M2-XL 모델은 NVIDIA 4090 GPU에서 2K 해상도 이미지를 처리할 때 약 0.64 FPS의 속도를 보인다. 이는 실시간 처리가 불가능한 속도이지만, 4K 이상의 해상도를 처리할 수 있다는 ‘메모리 확장성’ 측면에서는 큰 의미가 있다. 반면 S2M2-S 모델은 VGA 해상도에서 68.7 FPS를 기록하여, 드론이나 모바일 로봇과 같은 엣지 디바이스(Edge Device)에서의 활용 가능성을 열었다. S2M2는 사용 목적에 따라 모델의 깊이와 너비를 조절할 수 있는 유연성을 제공함으로써, 연구실 수준의 알고리즘을 넘어선 상용화 가능성을 보여준다.</p>
<h2>6.  일반화(Generalization)와 실세계 적용</h2>
<p>딥러닝 기반 스테레오 매칭의 가장 큰 숙제는 ’도메인 갭(Domain Gap)’의 극복이다. 합성 데이터(SceneFlow 등)로 학습된 모델을 실제 환경(Real World)에 적용했을 때 성능이 급락하는 현상은 일반적이다.</p>
<h3>6.1  Zero-Shot Generalization</h3>
<p>S2M2는 별도의 도메인 적응(Domain Adaptation) 과정이나 데이터셋별 미세 조정(Fine-tuning) 없이도 다양한 벤치마크에서 SOTA를 달성했다.2 이를 ’Zero-Shot Generalization’이라 부른다.</p>
<ul>
<li><strong>원인 분석:</strong> 이러한 강력한 일반화 능력은 두 가지 요인에서 기인한다.</li>
</ul>
<ol>
<li><strong>트랜스포머 백본:</strong> 대규모 데이터로 학습된 트랜스포머는 CNN보다 더 강인하고 불변하는(Invariant) 특징 표현을 학습하는 것으로 알려져 있다.</li>
<li><strong>글로벌 매칭 전략:</strong> 국소적인 텍스처 패턴은 도메인마다 크게 다르지만(예: 실내의 벽지 vs 실외의 도로), 물체 간의 기하학적 관계나 구조적 배치는 보편적인 물리 법칙을 따른다. 최적 수송 기반의 글로벌 매칭은 이러한 보편적 구조에 집중함으로써 도메인 변화에 덜 민감하게 반응한다.</li>
</ol>
<h3>6.2  신뢰도 기반의 안전성 확보</h3>
<p>자율 주행과 같은 안전 필수(Safety-Critical) 시스템에서, 모델이 “모르는 것을 모른다고” 말할 수 있는 능력은 정확도만큼이나 중요하다. S2M2가 제공하는 **신뢰도 맵(Confidence Map)**은 시스템이 불확실한 깊이 정보를 필터링하거나, 라이다(LiDAR) 등 다른 센서와 퓨전(Sensor Fusion)할 때 가중치를 조절하는 데 핵심적인 지표로 사용된다.1 이는 S2M2가 단순한 비전 알고리즘을 넘어 신뢰할 수 있는 센싱 시스템의 구성 요소가 될 수 있음을 의미한다.</p>
<h2>7.  한계점 및 향후 연구 방향</h2>
<p>S2M2가 보여준 성과에도 불구하고, 여전히 해결해야 할 과제들은 남아 있다.</p>
<ol>
<li><strong>극한의 연산 비용:</strong> S2M2-XL의 성능은 뛰어나지만, 0.64 FPS라는 속도는 고성능 GPU 없이는 활용이 불가능하다는 것을 의미한다. 트랜스포머의 연산 효율을 높이기 위한 경량화 기법(예: Sparse Attention, Linear Attention)의 적용이 시급하다.</li>
<li><strong>정류 오차에 대한 민감도:</strong> CREStereo와 달리 S2M2는 이상적인 정류를 가정한다. 실제 하드웨어에서는 진동이나 열변형으로 인해 카메라 정렬이 틀어질 수 있다. S2M2 아키텍처 내에 CREStereo와 같은 적응형 보정 모듈을 통합한다면 더욱 완벽한 모델이 될 것이다.</li>
<li><strong>학습의 난이도:</strong> 트랜스포머 기반 모델은 학습 수렴이 느리고 방대한 데이터가 필요하다. S2M2의 학습 파이프라인 최적화 및 데이터 효율적인 학습 방법론(Data-Efficient Learning)에 대한 연구가 필요하다.</li>
</ol>
<h2>8.  결론</h2>
<p>S2M2(Scalable Stereo Matching Model)는 스테레오 매칭 기술의 역사에서 중요한 변곡점을 제시한다. 기존의 CNN과 3D 비용 볼륨에 의존하던 방식에서 탈피하여, <strong>트랜스포머</strong>와 <strong>최적 수송</strong>이라는 새로운 도구를 통해 고해상도 확장성과 글로벌 일반화라는 난제를 해결했다.</p>
<p>본 보고서의 분석을 종합하면, S2M2는 다음과 같은 의의를 갖는다.</p>
<ul>
<li><strong>기술적 진보:</strong> 글로벌 컨텍스트의 효과적인 활용을 통해 텍스처리스 영역과 폐색 영역에서의 매칭 신뢰도를 획기적으로 높였다.</li>
<li><strong>실용적 가치:</strong> 다양한 크기의 모델 변형을 제공하고 신뢰도 맵을 함께 출력함으로써, 실제 산업 현장에서의 요구사항을 충족시킨다.</li>
<li><strong>미래 방향성:</strong> 도메인 일반화 능력을 입증함으로써, 특정 환경에 종속되지 않는 범용적인 3D 비전 시스템의 가능성을 보여주었다.</li>
</ul>
<p>향후 S2M2는 자율 주행, 로봇 공학, 정밀 계측 등 다양한 분야에서 표준적인 참조 모델로 자리 잡을 것으로 예상된다. 연구자들은 S2M2가 제시한 글로벌 매칭의 패러다임을 바탕으로, 연산 효율성을 개선하고 실세계의 불확실성에 더욱 강인한 차세대 모델을 개발하는 데 주력해야 한다. S2M2는 단순한 모델 하나가 아니라, 스테레오 비전이 나아가야 할 ’확장 가능한 공간 지능’의 청사진을 제시하고 있다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>S2M2: Scalable Stereo Matching Model for Reliable Depth Estimation - arXiv, https://arxiv.org/html/2507.13229v4</li>
<li>S2M2: Scalable Stereo Matching Model for Reliable Depth Estimation - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2025/papers/Min_S2M2_Scalable_Stereo_Matching_Model_for_Reliable_Depth_Estimation_ICCV_2025_paper.pdf</li>
<li>S 2 M 2 : Scalable Stereo Matching Model for Reliable Depth Estimation, https://junhong-3dv.github.io/s2m2-project/</li>
<li>Hardware module for low-resource and real-time stereo vision engine using semi-global matching approach | IEEE Conference Publication, http://ieeexplore.ieee.org/document/8088395/</li>
<li>Multi-scale Iterative Residuals for Fast and Scalable Stereo Matching - DFKI, https://www.dfki.de/fileadmin/user_upload/import/11905_raza2021frsnet.pdf</li>
<li>Adaptive Kernel Convolutional Stereo Matching Recurrent Network - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC11598454/</li>
<li>RAFT-Stereo Models - Emergent Mind, https://www.emergentmind.com/topics/raft-stereo-models</li>
<li>RAFT: Recurrent All-Pairs Field Transforms for Optical Flow (Extended Abstract) - IJCAI, https://www.ijcai.org/proceedings/2021/0662.pdf</li>
<li>Official implementation of “S2M2: Scalable Stereo Matching Model for Reliable Depth Estimation, ICCV 2025” - GitHub, https://github.com/junhong-3dv/s2m2</li>
<li>princeton-vl/RAFT-Stereo - GitHub, https://github.com/princeton-vl/RAFT-Stereo</li>
<li>Practical Stereo Matching via Cascaded Recurrent Network With Adaptive Correlation - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Practical_Stereo_Matching_via_Cascaded_Recurrent_Network_With_Adaptive_Correlation_CVPR_2022_paper.pdf</li>
<li>Practical Stereo Matching via Cascaded Recurrent Network with Adaptive Correlation - arXiv, https://arxiv.org/abs/2203.11483</li>
<li>Middlebury Stereo Evaluation - Version 3, https://vision.middlebury.edu/stereo/eval3/</li>
<li>Hierarchical Neural Architecture Search for deep stereo matching - Monash University, https://research.monash.edu/en/publications/hierarchical-neural-architecture-search-for-deep-stereo-matching/</li>
<li>[2010.13501] Hierarchical Neural Architecture Search for Deep Stereo Matching - arXiv, https://arxiv.org/abs/2010.13501</li>
<li>[Quick Review] Hierarchical Neural Architecture Search for Deep Stereo Matching - Liner, https://liner.com/review/hierarchical-neural-architecture-search-for-deep-stereo-matching</li>
<li>A Survey on Deep Stereo Matching in the Twenties - arXiv, https://arxiv.org/html/2407.07816v1</li>
<li>[2507.13229] {S\textsuperscript{2}M\textsuperscript{2}}: Scalable Stereo Matching Model for Reliable Depth Estimation - arXiv, https://arxiv.org/abs/2507.13229</li>
<li>ICCV Poster S²M²: Scalable Stereo Matching Model for Reliable Depth Estimation, https://iccv.thecvf.com/virtual/2025/poster/547</li>
<li>[2003.12039] RAFT: Recurrent All-Pairs Field Transforms for Optical Flow - arXiv, https://arxiv.org/abs/2003.12039</li>
<li>Practical Stereo Matching via Cascaded Recurrent Network with Adaptive Correlation | Request PDF - ResearchGate, https://www.researchgate.net/publication/363910925_Practical_Stereo_Matching_via_Cascaded_Recurrent_Network_with_Adaptive_Correlation</li>
<li>Constraining Multimodal Distribution for Domain Adaptation in Stereo Matching - arXiv, https://arxiv.org/html/2504.21302v1</li>
<li>Domain Generalized Stereo Matching with Uncertainty-guided Data Augmentation - arXiv, https://arxiv.org/html/2508.01303v1</li>
<li>A Survey on Deep Stereo Matching in the Twenties - ResearchGate, https://www.researchgate.net/publication/389357580_A_Survey_on_Deep_Stereo_Matching_in_the_Twenties</li>
<li>Domain Generalized Stereo Matching with Uncertainty-guided Data Augmentation, https://www.researchgate.net/publication/394293276_Domain_Generalized_Stereo_Matching_with_Uncertainty-guided_Data_Augmentation</li>
<li>Domain Generalized Stereo Matching via Hierarchical Visual Transformation - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_Domain_Generalized_Stereo_Matching_via_Hierarchical_Visual_Transformation_CVPR_2023_paper.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>