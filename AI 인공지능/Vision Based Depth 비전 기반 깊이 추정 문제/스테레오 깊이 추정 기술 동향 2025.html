<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2025 스테레오 깊이 추정 기술 동향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2025 스테레오 깊이 추정 기술 동향</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">비전 기반 깊이 추정 (Vision Based Dept)</a> / <span>2025 스테레오 깊이 추정 기술 동향</span></nav>
                </div>
            </header>
            <article>
                <h1>2025 스테레오 깊이 추정 기술 동향</h1>
<h2>1.  서론: 스테레오 비전의 새로운 지평</h2>
<p>2024년과 2025년은 컴퓨터 비전 역사상 스테레오 깊이 추정(Stereo Depth Estimation) 분야에서 가장 급진적인 패러다임의 전환이 발생한 시기로 기록될 것이다. 과거 수십 년간 스테레오 매칭 기술은 두 이미지 간의 픽셀 대응점(Correspondence)을 찾기 위해 지역적 특징(Local Feature)을 매칭하고 비용 볼륨(Cost Volume)을 최적화하는 기하학적 연산에 치중해 왔다. 그러나 최근의 연구 성과는 이러한 고전적인 접근 방식을 넘어, 인공지능이 사물의 맥락(Context)을 이해하고, 학습하지 않은 데이터에 대해서도 즉각적으로 적응하며, 단안(Monocular) 시각 정보와 양안(Binocular) 기하학을 능동적으로 융합하는 방향으로 진화하고 있다.</p>
<p>특히 2025년 ICCV와 CVPR, 그리고 2024년 NeurIPS 등 주요 학회에서 발표된 연구 결과들은 ’데이터 맞춤형(Dataset-specific) 튜닝’의 시대가 저물고, **‘범용적 시각 지능(General Visual Intelligence)’**을 활용한 스테레오 매칭의 시대가 도래했음을 선언하고 있다. 이는 단순히 벤치마크 점수를 0.1% 올리는 경쟁이 아니라, 자율주행 차량이 비 오는 밤거리나 거울로 된 건물 앞에서도 인간처럼 정확하게 거리를 인지할 수 있게 만드는 실질적인 인지 능력의 향상을 의미한다.</p>
<p>본 보고서는 2024년부터 2025년 상반기까지 발표된 방대한 연구 자료와 최신 기술 논문들을 심층 분석하여, 현대 스테레오 깊이 추정 기술의 핵심 조류를 규명한다. <strong>FoundationStereo</strong>, <strong>BridgeDepth</strong>, <strong>S2M2</strong>, <strong>Selective-Stereo</strong> 등 시장을 선도하는 핵심 모델들의 아키텍처적 혁신을 해부하고, 이들이 해결하고자 했던 난제들과 그에 따른 기술적 해법을 상세히 논할 것이다. 또한, 이러한 기술들이 자율주행, 로보틱스, 디지털 트윈 등 산업 현장에 미칠 파급 효과와 미래의 기술 로드맵을 제시한다.1</p>
<h2>2.  파운데이션 모델과 스테레오 매칭의 결합: 제로샷(Zero-Shot) 혁명</h2>
<p>전통적인 딥러닝 기반 스테레오 매칭 모델들은 특정 데이터셋(예: Scene Flow, KITTI)에 과적합(Overfitting)되는 경향이 강했다. 이는 모델이 ’깊이를 추론’하는 것이 아니라, 특정 데이터셋의 ’텍스처 분포를 암기’하는 방식으로 학습되었기 때문이다. 그 결과, 실내 데이터로 학습된 모델은 실외에서 무력했고, 맑은 날 학습된 모델은 우천 시 작동하지 않았다. 2025년의 연구들은 이 문제를 해결하기 위해 **파운데이션 모델(Foundation Model)**의 거대한 사전 지식(Prior)을 스테레오 파이프라인에 주입하는 전략을 취하고 있다.</p>
<h3>2.1  FoundationStereo: 범용 시각 지능의 이식</h3>
<p>NVIDIA Research가 주도하여 CVPR 2025에서 발표한 <strong>FoundationStereo</strong>는 스테레오 매칭 분야의 ’GPT-3 모먼트’라 불릴 만큼 충격적인 제로샷 성능을 보여주었다. 이 모델의 핵심 철학은 “이미지 자체를 이해하지 못하면, 픽셀 매칭도 제대로 할 수 없다“는 것이다. 즉, 단순한 텍스처 비교를 넘어, 물체의 의미론적(Semantic) 정보를 활용하여 매칭의 모호성을 해결한다.4</p>
<h4>2.1.1  아키텍처 심층 분석: 사이드 튜닝 어댑터 (Side-Tuning Adapter)</h4>
<p>FoundationStereo는 기존의 스테레오 특화 백본(Feature Extractor) 대신, 인터넷 규모의 데이터로 학습된 <strong>DepthAnythingV2</strong>와 같은 비전 파운데이션 모델(VFM)을 활용한다. 그러나 거대 모델을 직접 파인튜닝(Fine-tuning)하는 것은 막대한 연산 비용을 초래할 뿐만 아니라, 모델이 가진 범용적인 일반화 능력을 훼손할 위험이 있다.7</p>
<p>이를 해결하기 위해 연구진은 **사이드 튜닝 어댑터(STA)**라는 독창적인 구조를 도입했다.</p>
<ul>
<li><strong>작동 원리:</strong> VFM(ViT 기반)의 가중치는 동결(Frozen) 상태로 유지하여 범용적인 시각적 특징과 객체 인식 능력을 보존한다. 동시에, 별도의 경량화된 CNN 기반 사이드 네트워크를 병렬로 배치하여 스테레오 매칭에 필수적인 고주파 세부 정보(Edge, Texture details)와 기하학적 특징을 학습시킨다.</li>
<li><strong>융합 메커니즘:</strong> 동결된 VFM에서 추출된 풍부한 의미론적 특징(Semantic Feature)과 사이드 네트워크에서 추출된 구조적 특징(Structural Feature)은 다중 스케일(Multi-scale)에서 결합된다. 이 과정에서 STA는 단안 모델이 가진 ’물체의 형태와 깊이감에 대한 직관’을 스테레오 매칭의 ‘정밀한 거리 계산’ 과정에 주입하는 역할을 한다. 이는 텍스처가 없는 흰 벽이나 반복적인 패턴이 있는 펜스 등 전통적인 매칭 알고리즘이 실패하는 영역에서 강력한 성능을 발휘하는 원동력이 된다.5</li>
</ul>
<h4>2.1.2  4차원 비용 필터링의 혁신: Attentive Hybrid Cost Filtering (AHCF)</h4>
<p>스테레오 매칭의 정확도는 비용 볼륨(Cost Volume)을 얼마나 효과적으로 정제(Filtering)하느냐에 달려 있다. 기존의 3D CNN 기반 필터링은 수용 영역(Receptive Field)이 제한적이어서 이미지 전체의 문맥을 고려하지 못하는 한계가 있었다.</p>
<p>FoundationStereo는 AHCF 모듈을 통해 이 문제를 해결했다.</p>
<ul>
<li><strong>3D 축-평면 컨볼루션 (Axial-Planar Convolution, APC):</strong> 4차원 비용 볼륨(너비 <span class="math math-inline">\times</span> 높이 <span class="math math-inline">\times</span> 변위 <span class="math math-inline">\times</span> 특징)을 처리할 때, 공간적 차원과 변위 차원을 분리하여 연산 효율성을 극대화하면서도 전역적인 정보를 집계한다.</li>
<li><strong>변위 트랜스포머 (Disparity Transformer, DT):</strong> APC로 정제된 특징에 대해 변위(Disparity) 축을 따라 자기 주의(Self-Attention) 메커니즘을 적용한다. 이는 픽셀 단위의 지역적 매칭을 넘어, 시차 범위 전체에 걸친 확률 분포를 분석하여 가장 그럴듯한 깊이 값을 선택하게 돕는다. 이러한 장거리 문맥 추론(Long-range Context Reasoning)은 반복 패턴이나 거울 반사 등으로 인한 허위 매칭(False Positive)을 억제하는 데 결정적인 역할을 한다.7</li>
</ul>
<h4>2.1.3  데이터 중심 AI: 100만 쌍의 합성 데이터와 자가 정제</h4>
<p>FoundationStereo의 성공은 아키텍처뿐만 아니라 데이터 전략에도 기인한다. 연구진은 실세계 데이터의 부족을 해결하기 위해 100만(1M) 쌍 이상의 대규모 합성 데이터셋을 구축했다. 중요한 점은 단순히 양을 늘린 것이 아니라, <strong>자가 큐레이션(Self-curation) 파이프라인</strong>을 통해 모호하거나 품질이 낮은 샘플을 자동으로 제거하여 학습 데이터의 밀도를 높였다는 것이다. 이러한 대규모 고품질 데이터 학습은 모델이 다양한 조명, 날씨, 재질 환경에 대해 견고한(Robust) 제로샷 성능을 확보하게 했다.5</p>
<h3>2.2  Monster &amp; StereoVGGT: 다양한 모달리티의 통합</h3>
<p>FoundationStereo가 단안 깊이 모델을 활용했다면, <strong>StereoVGGT</strong>와 같은 모델들은 시각-언어(Vision-Language) 모델의 지식을 스테레오 매칭에 접목하는 시도를 보여주고 있다.1 이는 자율주행과 같이 객체의 종류(예: 보행자 vs 가로수)에 따라 깊이 추정의 중요도나 특성이 달라져야 하는 응용 분야에서 큰 잠재력을 가진다. 또한, <strong>MonSter (Monocular-Stereo)</strong> 계열의 연구들은 단안 깊이 추정 결과를 스테레오 매칭의 초기 가이드로 활용하거나, 스테레오 매칭이 불가능한 영역(폐색 영역 등)을 단안 예측으로 보완하는 하이브리드 접근 방식을 취하고 있다. KITTI 2015 리더보드에서 상위권을 차지한 <strong>BridgeDepth</strong> 역시 이러한 흐름의 연장선상에 있으며, 이는 3장(Bridging Modalities)에서 더욱 상세히 다룬다.1</p>
<h2>3.  단안 추론과 양안 기하학의 능동적 융합 (Active Fusion)</h2>
<p>스테레오 비전의 고질적인 약점은 텍스처가 없는 영역(Textureless regions), 반사체(Reflective surfaces), 투명체(Transparent objects)이다. 이러한 영역에서는 픽셀 간의 대응점을 찾는 것이 물리적으로 불가능하거나 수학적으로 모호하다. 반면, 단안 깊이 추정(Monocular Depth Estimation)은 기하학적 정밀도는 떨어지지만, “유리창 뒤에는 방이 있다“거나 “거울에 비친 상은 실제 물체가 아니다“라는 문맥적 추론이 가능하다. 2025년의 핵심 트렌드는 이 두 가지 상반된 접근 방식을 하나의 신경망 안에서 융합하는 것이다.</p>
<h3>3.1  BridgeDepth: 잠재 공간에서의 동기화</h3>
<p>ICCV 2025의 하이라이트(Highlight) 논문으로 선정된 <strong>BridgeDepth</strong>는 단안과 스테레오의 결합을 단순한 ’후처리(Post-processing)’나 ‘앙상블(Ensemble)’ 수준에서 벗어나, 심층 특징(Deep Feature) 레벨에서의 **‘잠재 정렬(Latent Alignment)’**로 승화시켰다.10</p>
<h4>3.1.1  양방향 교차 주의 (Bidirectional Cross-Attention) 메커니즘</h4>
<p>BridgeDepth의 가장 혁신적인 점은 단안 특징과 스테레오 비용 볼륨이 서로 정보를 주고받으며 상호 보완한다는 점이다.</p>
<ul>
<li><strong>단안에서 스테레오로 (Mono-to-Stereo):</strong> 단안 모델이 추출한 문맥 정보(Contextual Feature)는 스테레오 매칭 과정에서 ’사전 확률(Prior Probability)’로 작용한다. 예를 들어, 스테레오 매칭이 흰 벽에서 픽셀을 찾지 못해 헤맬 때, 단안 모델이 “여기는 평평한 벽면이다“라는 구조적 정보를 제공하여 매칭 범위를 좁혀준다.</li>
<li><strong>스테레오에서 단안으로 (Stereo-to-Mono):</strong> 반대로, 스테레오 매칭을 통해 얻어진 정밀한 거리 정보(Metric Depth)는 단안 모델의 스케일 모호성(Scale Ambiguity)을 교정한다. 단안 모델은 상대적인 깊이(Relative Depth)는 잘 알지만 절대적인 거리는 모르는 경우가 많은데, 스테레오 정보가 이를 보정해 주는 것이다.</li>
<li><strong>반사체 및 투명체 해결:</strong> 특히 거울이나 유리창 같은 비-람베르트(Non-Lambertian) 표면에서 이 메커니즘은 빛을 발한다. 기존 스테레오 모델이 거울에 비친 허상을 실제 깊이로 착각할 때, BridgeDepth의 단안 분기는 거울 프레임과 주변 문맥을 인지하여 올바른 깊이 값을 추론하도록 스테레오 분기를 유도한다. 실험 결과, BridgeDepth는 Middlebury와 ETH3D 벤치마크의 제로샷 설정에서 기존 최고 성능 모델 대비 에러율을 40% 이상 감소시키는 놀라운 성과를 거두었다.12</li>
</ul>
<h3>3.2  융합의 다양성: MonsterMarryMonoDepthStereo 등</h3>
<p>2025년에는 BridgeDepth 외에도 **“MonsterMarryMonoDepthStereo”**와 같이 직관적인 이름을 가진 연구들이 등장하여 단안과 스테레오의 결합을 시도하고 있다.14 이들은 주로 단안 모델의 출력을 스테레오 비용 볼륨의 정규화(Regularization) 항으로 사용하거나, 스테레오 매칭의 신뢰도(Confidence)가 낮은 영역을 단안 예측값으로 대체하는 적응형 융합 전략을 사용한다. 이는 스테레오 비전 시스템이 더 이상 독립적인 센서가 아니라, 전체 장면 이해(Scene Understanding) 시스템의 일부로 통합되고 있음을 시사한다.</p>
<h2>4.  글로벌 매칭의 부활과 확장성: 최적 수송(Optimal Transport)</h2>
<p>지난 몇 년간 스테레오 매칭은 RAFT-Stereo와 같이 반복적 정제(Iterative Refinement)를 수행하는, 일종의 국소적 최적화(Local Optimization) 방식이 지배했다. 그러나 2025년, 전체 이미지의 상관관계를 고려하여 매칭의 전역적 일관성(Global Consistency)을 보장하는 글로벌 매칭 아키텍처가 다시금 주목받고 있다. 그 중심에는 <strong>S2M2</strong>가 있다.</p>
<h3>4.1  S2M2: 확장 가능한 스테레오 매칭 모델 (Scalable Stereo Matching Model)</h3>
<p>ICCV 2025에서 발표된 <strong>S2M2</strong>는 기존 글로벌 매칭 방법들이 가진 계산 복잡도의 한계를 극복하고, 고해상도 이미지에서도 효율적으로 작동하는 새로운 아키텍처를 제안했다.3</p>
<h4>4.1.1  최적 수송(Optimal Transport) 이론의 적용</h4>
<p>S2M2의 기술적 백미는 매칭 문제를 <strong>최적 수송(Optimal Transport, OT)</strong> 문제로 재해석했다는 점이다.</p>
<ul>
<li><strong>Softmax vs. Optimal Transport:</strong> 기존의 스테레오 모델들은 각 픽셀별로 비용(Cost)을 계산하고 Softmax를 취해 확률을 얻었다. 이는 각 픽셀이 독립적으로 처리되므로, 텍스처가 없는 영역에서는 주변과 모순되는 깊이 값이 나오기 쉽다. 반면, OT는 이미지 전체의 픽셀 공급(Supply)과 수요(Demand)를 맞추는 전역 최적화 문제이다. 즉, “왼쪽 이미지의 한 픽셀은 오른쪽 이미지의 한 픽셀과만 매칭되어야 한다“는 고유성(Uniqueness) 제약 조건을 전역적으로 강제할 수 있다.</li>
<li><strong>효과:</strong> 이를 통해 S2M2는 얇은 구조물(자전거 바퀴살, 전선 등)이나 반복되는 패턴(타일 바닥, 펜스)에서 발생하는 오매칭을 획기적으로 줄였다. 실제로 Middlebury 벤치마크의 ‘자전거’ 예제에서 S2M2는 기존 모델들이 뭉개버리던 바퀴살의 디테일을 선명하게 복원해 냈다.16</li>
</ul>
<h4>4.1.2  다해상도 트랜스포머 (Multi-Resolution Transformer, MRT)</h4>
<p>글로벌 매칭의 단점인 연산량을 해결하기 위해 S2M2는 계층적인 트랜스포머 구조를 채택했다.</p>
<ul>
<li><strong>하이브리드 어텐션:</strong> 고해상도 특징 맵에서는 수평 방향의 1D 어텐션만을 수행하여 연산량을 줄이고(에피폴라 제약 활용), 저해상도 특징 맵에서는 2D 전체 어텐션을 수행하여 전역적인 문맥을 파악한다.</li>
<li><strong>적응형 게이트 융합 (Adaptive Gated Fusion Layer, AGFL):</strong> 서로 다른 해상도에서 추출된 특징들을 융합할 때, 단순한 덧셈이 아니라 학습 가능한 게이트(Gate)를 통해 정보의 흐름을 조절한다. 이를 통해 상위 레벨의 문맥 정보가 하위 레벨의 세부 매칭을 가이드하는 효과적인 정보 전달 체계를 구축했다.17</li>
</ul>
<h4>4.1.3  신뢰도와 폐색의 동시 추정</h4>
<p>S2M2는 단순히 깊이 값만 내놓는 것이 아니라, **변위(Disparity), 폐색(Occlusion), 신뢰도(Confidence)**를 동시에 추정하는 다목적 손실 함수를 사용한다. 이는 자율주행 시스템이 “이 물체는 10m 앞에 있다“는 정보뿐만 아니라 “이 정보의 정확도는 99%이다” 혹은 “이 부분은 가려져서 알 수 없다“는 메타 정보를 함께 획득할 수 있게 하여, 시스템 전체의 안전성을 높이는 데 기여한다.18</p>
<h2>5.  정밀도의 극한: 주파수 분석과 국소 구조 정제</h2>
<p>파운데이션 모델이 거시적인 흐름을 주도한다면, 2024년 발표된 연구들은 미시적인 영역에서 정밀도를 극한으로 끌어올리는 데 집중했다.</p>
<h3>5.1  Selective-Stereo: 주파수 영역에서의 해법</h3>
<p>CVPR 2024에 발표된 <strong>Selective-Stereo</strong>는 이미지의 영역마다 필요한 정보의 종류가 다르다는 점에 착안했다. 물체의 경계(Edge)는 고주파(High-frequency) 성분이 중요하고, 평탄한 면(Smooth region)은 저주파(Low-frequency) 성분이 중요하다.19</p>
<h4>5.1.1  선택적 순환 유닛 (Selective Recurrent Unit, SRU)</h4>
<p>기존의 반복적 정제 모듈(GRU 등)은 모든 픽셀에 대해 동일한 업데이트 규칙을 적용했다. Selective-Stereo는 <strong>SRU</strong>를 도입하여, 현재 픽셀이 경계인지 평탄면인지에 따라 정보의 수용 정도를 조절한다.</p>
<ul>
<li><strong>문맥적 공간 어텐션 (Contextual Spatial Attention, CSA):</strong> 이 모듈은 입력 특징 맵을 분석하여 주파수별 중요도 맵(Attention Map)을 생성한다. SRU는 이 맵을 바탕으로 게이트(Gate)를 열고 닫으며, 경계 부분에서는 급격한 깊이 변화를 허용하고, 평탄한 부분에서는 노이즈를 억제하고 부드럽게 이어지도록 업데이트한다. 그 결과, 물체의 윤곽선이 뭉개지는 블러링(Blurring) 현상을 방지하고 매우 날카로운 깊이 경계면을 얻을 수 있었다.21</li>
</ul>
<h3>5.2  LoS: 국소 구조 유도 (Local Structure-Guided) 매칭</h3>
<p>CVPR 2024의 **LoS (Local Structure-Guided Stereo Matching)**는 평면 가정(Planar Assumption)의 한계를 지적했다. 대부분의 스테레오 알고리즘은 작은 윈도우 내의 픽셀들이 같은 평면 위에 있다고 가정하지만, 실제 세상은 곡면과 불연속면으로 가득하다.</p>
<ul>
<li><strong>국소 구조 정보 (LSI):</strong> LoS는 각 픽셀 주변의 3D 기하학적 구조(곡률, 법선 벡터 등)를 명시적으로 추정하고, 이를 매칭 비용 집계(Aggregation) 과정에 반영한다.</li>
<li><strong>국소 구조 유도 전파 (LSGP):</strong> 추정된 구조 정보를 가이드로 삼아 변위 값을 이웃 픽셀로 전파한다. 예를 들어, 둥근 기둥의 경우 곡면을 따라 깊이 값이 부드럽게 변하도록 유도하고, 모서리에서는 전파를 차단하여 경계를 보존한다. 이 방법은 ETH3D와 같은 고정밀 벤치마크에서 탁월한 성능을 발휘했다.22</li>
</ul>
<h2>6.  생성형 AI를 활용한 데이터 혁명: ZeroStereo</h2>
<p>딥러닝 모델의 성능은 데이터의 양과 질에 비례한다. 그러나 실세계에서 정밀한 깊이 정답(Ground Truth)을 얻는 것(LiDAR 스캔 등)은 매우 비용이 많이 들고 희소(Sparse)하다. <strong>ZeroStereo</strong> (CVPR 2025)는 생성형 AI, 특히 확산 모델(Diffusion Model)을 활용하여 이 데이터 병목 현상을 타파했다.25</p>
<h3>6.1  확산 인페인팅(Diffusion Inpainting) 파이프라인</h3>
<p>ZeroStereo는 단일 이미지(Single Image)만 있으면 이를 고품질의 스테레오 쌍(Stereo Pair)으로 변환할 수 있는 파이프라인을 제안했다.</p>
<ol>
<li><strong>가상 시점 변환:</strong> 단일 이미지에서 단안 깊이를 추정하고, 이를 바탕으로 가상의 오른쪽 시점 이미지를 생성(Warping)한다.</li>
<li><strong>폐색 영역 복원:</strong> 시점 변환 시 필연적으로 발생하는 빈 공간(Occlusion hole)을 채워야 한다. 기존 방법들은 주변 픽셀을 복사하는 수준이었지만, ZeroStereo는 <strong>Stable Diffusion</strong> 모델을 스테레오 데이터셋으로 파인튜닝하여 사용한다.</li>
<li><strong>스테레오 특화 파인튜닝:</strong> 일반적인 이미지 인페인팅 모델은 배경을 아무렇게나 채워 넣지만, 스테레오 매칭을 위해서는 기하학적으로 올바른 배경을 생성해야 한다. ZeroStereo는 Scene Flow 데이터셋을 활용해 확산 모델의 U-Net을 미세 조정하여, 가려진 부분의 깊이 정보와 텍스처가 논리적으로 일치하도록 학습시켰다.25</li>
</ol>
<h3>6.2  훈련 없는 신뢰도 생성 (Training-Free Confidence Generation)</h3>
<p>생성된 데이터는 필연적으로 노이즈를 포함한다. ZeroStereo는 별도의 학습 없이도 생성된 의사 라벨(Pseudo Label)의 신뢰도를 측정하는 알고리즘을 개발했다. 이는 학습 과정에서 품질이 낮은 데이터를 자동으로 배제하거나 가중치를 낮추는 데 사용되어, 결과적으로 모델이 실제 데이터에서도 강인한 성능을 내도록 돕는다. 이러한 “데이터 생성-정제-학습“의 선순환 구조는 향후 스테레오 연구의 표준이 될 가능성이 높다.25</p>
<h2>7.  엣지 디바이스를 위한 효율성과 경량화</h2>
<p>연구실 환경에서의 최고 성능도 중요하지만, 드론이나 로봇 청소기와 같은 엣지 디바이스(Edge Device)에서 작동할 수 있는 경량 모델에 대한 수요도 폭발하고 있다.</p>
<h3>7.1  FAMNet: 3D 컨볼루션의 다이어트</h3>
<p><strong>FAMNet</strong>은 고성능 스테레오 모델들이 공통적으로 사용하는 3D 컨볼루션이 연산량 폭증의 주원인임을 지적했다. 이를 해결하기 위해 <strong>FACV (Fusion Attention-based Cost Volume)</strong> 모듈을 제안했다.</p>
<ul>
<li><strong>2D 기반 비용 집계:</strong> 3D 공간에서의 연산을 2D 채널 어텐션(Channel Attention)과 다중 스케일 집계(Multi-scale Aggregation)로 대체했다. 이를 통해 정보 손실을 최소화하면서도 연산량을 획기적으로 줄였다.</li>
<li><strong>성능:</strong> RTX 3090 GPU 기준 <strong>47.1 FPS</strong>라는 압도적인 속도를 달성했으며, 이는 기존 실시간 모델들보다 20% 이상 빠르고 정확도 또한 높은 수치이다. 자율주행 차량이 고속으로 주행하는 상황에서도 지연(Latency) 없는 깊이 추정이 가능함을 의미한다.26</li>
</ul>
<h3>7.2  Lite Any Stereo &amp; IGFNet</h3>
<ul>
<li><strong>Lite Any Stereo (2025):</strong> 파운데이션 모델의 지식을 경량 모델로 **증류(Distillation)**하는 접근법을 취한다. 수억 개의 파라미터를 가진 교사 모델(Teacher Model)의 추론 결과를 수백만 개의 파라미터를 가진 학생 모델(Student Model)이 모방하도록 학습시켜, 모바일 기기에서도 제로샷에 준하는 성능을 구현하려 한다.14</li>
<li><strong>IGFNet (2024):</strong> Jetson Nano와 같은 저전력 임베디드 보드에서도 <strong>30.3 FPS</strong>의 실시간 처리를 달성했다. 이는 드론이나 소형 로봇과 같이 배터리와 연산 자원이 극도로 제한된 환경에서도 고품질의 3D 시각을 제공할 수 있음을 입증한 것이다.28</li>
</ul>
<h2>8.  벤치마크 리더보드 심층 분석: 기술 트렌드의 반영</h2>
<p>주요 벤치마크(Middlebury v3, KITTI 2015, ETH3D)의 최신 순위는 각 기술의 장단점을 명확히 보여준다.</p>
<h3>8.1  Middlebury v3 &amp; ETH3D: 제로샷과 고해상도의 전장</h3>
<p>Middlebury와 ETH3D는 다양한 실내외 환경, 고해상도 이미지, 그리고 복잡한 기하학적 구조를 포함하고 있어 현대 스테레오 모델의 종합적인 성능을 평가하는 데 최적이다.</p>
<ul>
<li><strong>FoundationStereo:</strong> 이 두 벤치마크에서 1위를 석권했다. 이는 학습 데이터에 없는 새로운 환경(Zero-shot)에서의 적응력이 현재 기술의 가장 중요한 척도임을 보여준다. 특히 합성 데이터만으로 학습했음에도 실사 데이터 벤치마크를 장악한 것은 데이터 파이프라인과 아키텍처의 승리라 할 수 있다.2</li>
<li><strong>S2M2:</strong> 고해상도 이미지와 큰 변위(Disparity) 범위에서 안정적인 성능을 보이며 상위권에 랭크되었다. 특히 얇은 물체 복원 능력 덕분에 복잡한 실내 장면에서 강점을 보였다.15</li>
</ul>
<h3>8.2  KITTI 2015: 자율주행과 실외 환경</h3>
<p>도로 주행 환경에 초점을 맞춘 KITTI에서는 <strong>BridgeDepth</strong>와 <strong>StereoVGGT</strong>가 두각을 나타냈다.</p>
<ul>
<li><strong>BridgeDepth (D1-fg: 2.73%):</strong> 자동차 유리창, 차체 반사 등 실외 환경의 난반사 문제를 단안 융합으로 해결하여 오차를 줄였다. 배경(Background) 에러율에서도 탁월한 성능을 보여, 정적 환경 재구성 능력 또한 뛰어남을 입증했다.1</li>
<li><strong>StereoVGGT (D1-fg: 2.38%):</strong> 시각-언어 모델의 특징을 활용하여 객체 인식 기반의 깊이 추정을 수행함으로써, 보행자나 차량과 같은 동적 객체(Foreground)에서의 에러율을 획기적으로 낮췄다.</li>
</ul>
<table><thead><tr><th><strong>순위</strong></th><th><strong>모델명</strong></th><th><strong>주요 벤치마크 강점</strong></th><th><strong>핵심 기술 요약</strong></th></tr></thead><tbody>
<tr><td><strong>Top-Tier</strong></td><td><strong>FoundationStereo</strong></td><td>Middlebury, ETH3D (제로샷)</td><td>DepthAnythingV2 백본, Side-Tuning, AHCF</td></tr>
<tr><td><strong>Top-Tier</strong></td><td><strong>S2M2</strong></td><td>Middlebury, ETH3D (고해상도)</td><td>Optimal Transport, Multi-Resolution Transformer</td></tr>
<tr><td><strong>Top-Tier</strong></td><td><strong>BridgeDepth</strong></td><td>KITTI, Middlebury (반사체)</td><td>Latent Alignment, Cross-Attention (Mono+Stereo)</td></tr>
<tr><td><strong>High</strong></td><td><strong>Selective-Stereo</strong></td><td>KITTI, ETH3D (경계 정밀도)</td><td>Frequency-aware SRU, Contextual Spatial Attention</td></tr>
<tr><td><strong>Real-time</strong></td><td><strong>FAMNet</strong></td><td>KITTI (속도/효율)</td><td>2D Cost Aggregation, Multi-scale Attention</td></tr>
</tbody></table>
<p><em>표 1. 2024-2025 주요 스테레오 깊이 추정 모델 비교 분석</em></p>
<h2>9.  결론 및 향후 전망</h2>
<p>2024년과 2025년의 연구 성과들을 종합해 볼 때, 스테레오 깊이 추정 기술은 **“순수 기하학(Geometry-only)에서 인지적 기하학(Cognitive Geometry)으로의 진화”**를 겪고 있다.</p>
<p>첫째, <strong>파운데이션 모델의 지배력 확대:</strong> 앞으로의 스테레오 연구는 처음부터 모델을 학습시키는 것이 아니라, 이미 학습된 거대 모델을 얼마나 효율적으로 ’조정(Adaptation)’하고 ’정렬(Alignment)’하느냐가 핵심 경쟁력이 될 것이다. FoundationStereo와 ZeroStereo는 그 서막에 불과하다.</p>
<p>둘째, <strong>능동적이고 다중적인 모달리티 융합:</strong> 단안 깊이뿐만 아니라 언어(Text), 그리고 시간(Temporal) 정보까지 융합되는 추세이다. 비디오 기반 스테레오 매칭이나 언어 지시어에 반응하는 3D 재구성 기술이 차세대 연구 주제가 될 것이다.</p>
<p>셋째, <strong>실세계 문제 해결:</strong> 벤치마크 점수 놀이를 넘어, 반사체, 투명체, 악천후 등 기존 스테레오가 실패했던 영역을 정복하는 기술(BridgeDepth)들이 실제 산업 현장(자율주행, 로봇)에 빠르게 적용될 것이다.</p>
<p>넷째, <strong>효율성의 양극화와 통합:</strong> 초거대 파운데이션 모델과 초경량 엣지 모델로 기술이 양분되면서도, 지식 증류를 통해 서로 연결되는 생태계가 조성될 것이다.</p>
<p>결론적으로, 최신 스테레오 기술은 인간의 시각 시스템을 닮아가고 있다. 두 눈으로 들어오는 시차 정보(Stereo)와 뇌 속에 축적된 세상에 대한 지식(Prior)을 결합하여, 보이지 않는 것도 추론하고 모호한 것도 해석해 내는 수준으로 발전하고 있다. 이는 완전 자율주행과 고도화된 인공지능 로봇의 실현을 앞당기는 핵심적인 기술적 도약이 될 것이다.</p>
<h2>10. 참고 자료</h2>
<ol>
<li>KITTI Stereo 2015 - The KITTI Vision Benchmark Suite, https://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo</li>
<li>FoundationStereo: Zero-Shot Stereo Matching - arXiv, https://arxiv.org/html/2501.09898v3</li>
<li>S2M2: Scalable Stereo Matching Model for Reliable Depth Estimation - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2025/papers/Min_S2M2_Scalable_Stereo_Matching_Model_for_Reliable_Depth_Estimation_ICCV_2025_paper.pdf</li>
<li>FoundationStereo Overview - NGC Catalog - NVIDIA, https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/foundationstereo</li>
<li>FoundationStereo: Zero-Shot Stereo Matching - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2025/papers/Wen_FoundationStereo_Zero-Shot_Stereo_Matching_CVPR_2025_paper.pdf</li>
<li>[CVPR 2025 Best Paper Nomination] FoundationStereo: Zero-Shot Stereo Matching - GitHub, https://github.com/NVlabs/FoundationStereo</li>
<li>FoundationStereo: Zero-Shot Stereo Matching - alphaXiv, https://www.alphaxiv.org/overview/2501.09898v4</li>
<li>FoundationStereo: Zero-Shot Stereo Matching - arXiv, https://arxiv.org/html/2501.09898v4</li>
<li>FoundationStereo: Zero-Shot Stereo Matching - NVlabs, https://nvlabs.github.io/FoundationStereo/</li>
<li>ICCV 2025 Papers, https://iccv.thecvf.com/virtual/2025/papers.html</li>
<li>Paper Digest: ICCV 2025 Papers &amp; Highlights, https://www.paperdigest.org/2025/10/iccv-2025-papers-highlights/</li>
<li>BridgeDepth: Bridging Monocular and Stereo … - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2025/papers/Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment_ICCV_2025_paper.pdf</li>
<li>BridgeDepth: Bridging Monocular and Stereo Reasoning with Latent Alignment - arXiv, https://arxiv.org/html/2508.04611v2</li>
<li>Lite Any Stereo: Efficient Zero-Shot Stereo Matching - arXiv, https://arxiv.org/html/2511.16555v1</li>
<li>Official implementation of “S2M2: Scalable Stereo Matching Model for Reliable Depth Estimation, ICCV 2025” - GitHub, https://github.com/junhong-3dv/s2m2</li>
<li>stereo matching model(s2m2) released : r/computervision - Reddit, https://www.reddit.com/r/computervision/comments/1oknswb/stereo_matching_models2m2_released/</li>
<li>S 2 M 2 : Scalable Stereo Matching Model for Reliable Depth Estimation, https://junhong-3dv.github.io/s2m2-project/</li>
<li>S2M2: Scalable Stereo Matching Model for Reliable Depth Estimation - arXiv, https://arxiv.org/html/2507.13229v4</li>
<li>Wang Selective-Stereo Adaptive Frequency Information Selection For Stereo Matching CVPR 2024 Paper | PDF - Scribd, https://www.scribd.com/document/860518533/Wang-Selective-Stereo-Adaptive-Frequency-Information-Selection-for-Stereo-Matching-CVPR-2024-paper</li>
<li>Adaptive Frequency Information Selection for Stereo Matching - CVPR 2024 Open Access Repository, https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Selective-Stereo_Adaptive_Frequency_Information_Selection_for_Stereo_Matching_CVPR_2024_paper.html</li>
<li>Adaptive Frequency Information Selection for Stereo Matching - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Selective-Stereo_Adaptive_Frequency_Information_Selection_for_Stereo_Matching_CVPR_2024_paper.pdf</li>
<li>LoS: Local Structure-Guided Stereo Matching - CVPR 2024 Open Access Repository, https://openaccess.thecvf.com/content/CVPR2024/html/Li_LoS_Local_Structure-Guided_Stereo_Matching_CVPR_2024_paper.html</li>
<li>LoS: Local Structure-Guided Stereo Matching | IEEE Conference Publication, https://ieeexplore.ieee.org/document/10655701/</li>
<li>LoS: Local Structure-Guided Stereo Matching - IEEE Computer Society, https://www.computer.org/csdl/proceedings-article/cvpr/2024/530000t746/20hQaCkDCM0</li>
<li>ZeroStereo: Zero-shot Stereo Matching from Single Images, <a href="https://arxiv.org/pdf/2501.08654">https://arxiv.org/pdf/2501.08654?</a></li>
<li>FAMNet: A Lightweight Stereo Matching Network for Real-Time …, https://www.mdpi.com/2073-8994/17/8/1214</li>
<li>Efficient Zero-Shot Stereo Matching - arXiv, https://arxiv.org/pdf/2511.16555</li>
<li>Information-Guided Fusion Lightweight Stereo Matching Network - IEEE Xplore, https://ieeexplore.ieee.org/document/10797128/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>