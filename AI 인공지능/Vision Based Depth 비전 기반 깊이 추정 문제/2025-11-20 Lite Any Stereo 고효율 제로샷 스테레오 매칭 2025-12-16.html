<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Lite Any Stereo 고효율 제로샷 스테레오 매칭</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Lite Any Stereo 고효율 제로샷 스테레오 매칭</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">비전 기반 깊이 추정 (Vision Based Dept)</a> / <span>Lite Any Stereo 고효율 제로샷 스테레오 매칭</span></nav>
                </div>
            </header>
            <article>
                <h1>Lite Any Stereo 고효율 제로샷 스테레오 매칭</h1>
<p>2025-12-16, G30DR</p>
<h2>1.  서론 (Introduction)</h2>
<h3>1.1  현대 스테레오 비전의 기술적 딜레마와 진화</h3>
<p>컴퓨터 비전(Computer Vision) 분야에서 스테레오 매칭(Stereo Matching)은 두 개의 시점(Viewpoint)에서 획득한 이미지를 분석하여 픽셀 간의 대응 관계(Correspondence)를 파악하고, 이를 통해 시차(Disparity) 맵을 생성하여 3차원 심도(Depth) 정보를 복원하는 핵심 기술이다.1 이 기술은 자율주행 자동차(Autonomous Driving), 로보틱스(Robotics), 증강 현실(AR) 및 가상 현실(VR) 등 공간 인지 능력이 필수적인 다양한 산업 분야의 기반이 된다. 과거의 블록 매칭(Block Matching)이나 준전역 매칭(Semi-Global Matching, SGM)과 같은 고전적 알고리즘에서 시작된 스테레오 매칭 기술은 딥러닝(Deep Learning)의 도입과 함께 비약적인 발전을 이루었다. 특히 합성곱 신경망(Convolutional Neural Networks, CNN)과 최근의 트랜스포머(Transformer) 아키텍처의 적용은 텍스처가 부족한 영역(Texture-less regions), 반복적인 패턴(Repetitive patterns), 그리고 가려짐(Occlusion) 영역과 같은 난제들을 해결하는 데 크게 기여했다.</p>
<p>그러나 기술의 고도화는 필연적으로 계산 복잡도(Computational Complexity)의 증가를 수반했다. 최근 학계와 산업계에서 최고 성능(State-of-the-Art, SOTA)을 기록하고 있는 RAFT-Stereo 3, CREStereo 3, IGEV-Stereo 6 등의 모델들은 정확도를 극대화하기 위해 반복적인 순환 신경망(Recurrent Neural Networks) 구조나 고차원(3D/4D) 비용 볼륨(Cost Volume)을 활용한다. 이러한 접근 방식은 미세한 픽셀 단위의 정확도를 보장하지만, 수백 기가플롭스(Giga-FLOPs)에서 테라플롭스(Tera-FLOPs)에 이르는 막대한 연산량과 높은 메모리 대역폭을 요구한다. 이는 고성능 GPU 서버 환경이 아닌, 전력과 연산 자원이 제한된 엣지 디바이스(Edge Device)나 임베디드 시스템(Embedded System)에서의 실시간 구동을 어렵게 만드는 주요 원인이다.</p>
<h3>1.2  경량화 모델의 한계와 제로샷 일반화의 난제</h3>
<p>이러한 연산 비용 문제를 해결하기 위해 MobileStereoNet 1과 같은 다양한 경량화(Efficient) 모델들이 제안되었다. 이들은 모델의 파라미터 수를 줄이거나 연산 비용이 높은 3D 합성곱 연산을 2D 연산으로 대체하는 등의 최적화를 수행하여 추론 속도를 비약적으로 향상시켰다. 그러나 기존의 경량화 모델들은 치명적인 한계점을 안고 있었다. 바로 ‘제로샷 일반화(Zero-Shot Generalization)’ 능력의 부재다.1</p>
<p>일반적으로 딥러닝 모델은 학습 데이터와 동일한 분포(Distribution)를 가진 테스트 데이터에 대해서는 높은 성능을 보이지만, 학습 과정에서 경험하지 못한 새로운 도메인(Unseen Domain)에 대해서는 성능이 급격히 저하되는 경향이 있다. 이를 도메인 갭(Domain Gap) 문제라고 한다. 파라미터 수가 많고 용량(Capacity)이 큰 모델들은 방대한 데이터를 통해 다양한 환경 변수를 학습하고 일반화할 수 있는 여유가 있지만, 용량이 제한된 경량 모델들은 특정 학습 데이터(주로 합성 데이터인 Scene Flow 등)에 과적합(Overfitting)되기 쉽다. 따라서 학계에서는 “효율적인 경량 모델은 제한된 표현력(Representation Power)으로 인해 제로샷 일반화 능력을 갖추기 어렵다“는 인식이 지배적이었다.1</p>
<p>최근에는 ’Depth Anything’이나 ’Segment Anything’과 같은 대규모 기초 모델(Foundation Model)에서 추출한 깊이 사전 정보(Depth Prior)를 스테레오 매칭에 접목하여 제로샷 성능을 높이려는 시도들이 등장했다.8 그러나 이러한 방식은 스테레오 매칭 모델 자체의 성능을 높이는 것이 아니라 외부의 거대 모델을 추가로 실행해야 하므로, 전체 시스템의 연산 효율성을 다시 저하시키는 모순적인 결과를 낳는다. 즉, 진정한 의미에서의 ’효율적인 제로샷 스테레오 매칭’은 요원한 상태였다.</p>
<h3>1.3  Lite Any Stereo의 제안 및 연구 목적</h3>
<p>이러한 기술적 공백을 메우기 위해 Imperial College London 연구진은 <strong>Lite Any Stereo</strong>라는 혁신적인 프레임워크를 제안했다.1 본 연구의 핵심 목표는 외부의 거대 사전 모델이나 추가적인 연산 부하 없이, 순수하게 최적화된 아키텍처 설계와 데이터 중심(Data-Centric)의 학습 전략만으로 경량 모델이 고성능 모델에 버금가는 제로샷 일반화 성능을 달성하도록 하는 것이다.</p>
<p>Lite Any Stereo는 기존의 정확도 중심 모델들이 사용하는 연산량의 1% 미만(약 33 G-MACs)만을 사용하면서도, KITTI, ETH3D, Middlebury 등 주요 벤치마크에서 타 경량 모델들을 압도하고 일부 고성능 모델과 대등한 성능을 입증했다.1 본 보고서는 Lite Any Stereo의 기술적 성취를 심층적으로 분석하기 위해 작성되었다. 아키텍처의 설계 철학, 하이브리드 비용 집계 모듈의 수학적 원리, 단계별 학습 전략의 구체적 메커니즘, 그리고 광범위한 실험 결과를 토대로 이 기술이 가지는 학문적 의의와 산업적 파급 효과를 포괄적으로 논의한다.</p>
<h2>2.  관련 연구 및 기술적 배경 (Theoretical Background &amp; Related Work)</h2>
<p>Lite Any Stereo의 기술적 위치를 명확히 이해하기 위해서는 스테레오 매칭 기술의 흐름과 기존 접근 방식들의 한계를 먼저 살펴볼 필요가 있다.</p>
<h3>2.1  딥러닝 기반 스테레오 매칭의 패러다임 변화</h3>
<p>전통적인 스테레오 매칭 파이프라인은 정합 비용 계산(Matching Cost Computation), 비용 집계(Cost Aggregation), 시차 최적화(Disparity Optimization), 시차 정제(Disparity Refinement)의 단계를 거친다. 딥러닝의 도입은 이 각 단계를 신경망으로 대체하거나 통합하는 방향으로 진행되었다.</p>
<ol>
<li><strong>초기 딥러닝 모델 (MC-CNN 등):</strong> 초기의 연구들은 주로 정합 비용 계산 단계만을 CNN으로 대체하여 패치(Patch) 간의 유사도를 학습하는 데 집중했다.4 이는 정확도를 높였으나 후처리 과정이 여전히 고전적인 방식에 의존하는 한계가 있었다.</li>
<li><strong>엔드투엔드(End-to-End) 모델 (DispNet, GC-Net):</strong> 입력 이미지부터 시차 맵 출력까지 전체 과정을 하나의 네트워크로 학습하는 방식이 주류가 되었다. 특히 GC-Net은 4D 비용 볼륨(Cost Volume)과 3D 합성곱을 도입하여 기하학적 문맥(Context)을 효과적으로 학습할 수 있음을 증명했다.9</li>
<li><strong>피라미드 및 적응형 구조 (PSMNet, GA-Net):</strong> 다양한 스케일의 정보를 통합하기 위한 피라미드 풀링(Pyramid Pooling)이나 가이드 집계(Guided Aggregation) 기법이 도입되어, 텍스처가 없거나 복잡한 영역에서의 성능이 향상되었다.3</li>
<li><strong>반복적 정제 모델 (RAFT-Stereo, CREStereo):</strong> 최근의 SOTA 모델들은 광학 흐름(Optical Flow) 추정에서 영감을 받아, 초기에 대략적인 시차를 추정한 후 반복적인 업데이트(Iterative Update)를 통해 잔차(Residual)를 줄여나가는 방식을 채택했다. 이는 매우 높은 정확도를 보장하지만 반복 횟수에 비례하여 연산량이 증가한다는 단점이 있다.3</li>
</ol>
<h3>2.2  비용 볼륨(Cost Volume)의 구성 방식과 효율성</h3>
<p>비용 볼륨은 좌우 이미지의 특징 맵(Feature Map) 간의 상관관계를 저장하는 텐서(Tensor)로, 스테레오 매칭 네트워크의 핵심 구성 요소다. 비용 볼륨을 구성하는 방식은 크게 두 가지로 나뉜다.</p>
<ul>
<li><strong>연결 기반(Concatenation-based):</strong> 좌우 특징 벡터를 채널 축으로 연결하여 4D 볼륨(Height <span class="math math-inline">\times</span> Width <span class="math math-inline">\times</span> Disparity <span class="math math-inline">\times</span> Feature)을 생성한다. PSMNet이나 GC-Net이 이 방식을 사용하며, 많은 정보를 보존하지만 메모리와 연산량 소모가 극심하다.</li>
<li><strong>상관관계 기반(Correlation-based):</strong> 좌우 특징 벡터의 내적(Inner Product)을 통해 단일 채널의 3D 볼륨(Height <span class="math math-inline">\times</span> Width <span class="math math-inline">\times</span> Disparity)을 생성한다. RAFT-Stereo나 Lite Any Stereo가 이 방식을 따른다.1 정보가 압축되어 손실될 수 있으나 효율성이 매우 높다. Lite Any Stereo는 이 상관관계 기반 볼륨을 채택하되, 이를 보완하기 위한 독창적인 집계 모듈을 설계하여 효율성과 정보 보존의 균형을 맞추었다.</li>
</ul>
<h3>2.3  제로샷 학습과 도메인 적응 (Domain Adaptation)</h3>
<p>합성 데이터(Scene Flow 등)로 학습한 모델을 실제 환경(KITTI, Middlebury 등)에 적용할 때 발생하는 성능 저하는 오랫동안 해결되지 않은 문제였다. 기존에는 이를 해결하기 위해 타겟 도메인의 데이터로 미세 조정(Fine-tuning)을 수행하거나, 도메인 적응(Domain Adaptation) 기법을 사용했다. 그러나 이는 타겟 도메인의 데이터가 확보되어야 한다는 전제 조건이 붙는다. ’제로샷(Zero-Shot)’은 타겟 도메인에 대한 정보 없이도 범용적으로 작동해야 하므로 훨씬 난이도가 높다. 최근 ’Foundation Model’을 활용한 연구들은 강력한 사전 지식(Prior Knowledge)을 활용하여 이 문제를 우회하려 했으나, Lite Any Stereo는 모델 자체의 학습 전략(Training Strategy)을 고도화하여 내재적인 일반화 능력을 키우는 정공법을 택했다.1</p>
<h2>3.  Lite Any Stereo 아키텍처 심층 분석 (Methodology: Architecture)</h2>
<p>Lite Any Stereo는 효율성을 극대화하기 위해 불필요한 연산을 제거하면서도, 스테레오 매칭에 필수적인 기하학적 정보와 공간적 문맥 정보를 놓치지 않도록 설계되었다. 전체 프레임워크는 특징 추출(Feature Extraction), 상관관계 볼륨 생성(Correlation Volume Construction), 하이브리드 비용 집계(Hybrid Cost Aggregation), 그리고 시차 추정 및 업샘플링(Disparity Estimation &amp; Upsampling)의 4단계로 구성된다.1</p>
<h3>3.1  특징 추출 모듈 (Feature Extraction)</h3>
<p>입력된 좌우 이미지 쌍 <span class="math math-inline">I_L, I_R \in \mathbb{R}^{H \times W \times 3}</span>에 대해 가중치를 공유(Weight-sharing)하는 샴 네트워크(Siamese Network) 구조의 백본을 사용하여 특징을 추출한다. Lite Any Stereo는 MobileNetV2나 EfficientNet과 유사한 경량화된 CNN 블록을 사용하여 다중 스케일(Multi-scale) 특징 맵을 생성한다.1</p>
<ul>
<li><strong>다중 해상도 활용:</strong> 특징 추출기는 원본 해상도의 <span class="math math-inline">1/4</span>, <span class="math math-inline">1/8</span>, <span class="math math-inline">1/16</span>, <span class="math math-inline">1/32</span> 등 다양한 스케일의 특징 맵 <span class="math math-inline">{F_L^s}, {F_R^s}</span>를 생성한다. 낮은 해상도의 특징 맵은 전역적인 문맥(Global Context)과 큰 물체의 구조를 파악하는 데 유리하고, 높은 해상도의 특징 맵은 가장자리(Edge)와 같은 세부 정보를 복원하는 데 사용된다.</li>
<li><strong>효율성 중심 설계:</strong> 정확도 지향 모델들이 수백 개의 채널을 사용하는 거대한 ResNet 백본을 사용하는 것과 달리, Lite Any Stereo는 채널 수를 억제하고 깊이를 조절하여 특징 추출 단계에서의 지연 시간(Latency)을 최소화한다.</li>
</ul>
<h3>3.2  상관관계 기반 비용 볼륨 (Correlation Cost Volume)</h3>
<p>추출된 좌우 특징 맵 중 <span class="math math-inline">1/4</span> 해상도의 특징 맵 <span class="math math-inline">F_L^{1/4}, F_R^{1/4}</span>을 사용하여 비용 볼륨을 생성한다. 앞서 논의한 바와 같이, 메모리 효율성을 위해 내적(Inner Product) 기반의 상관관계를 계산한다. 특정 시차 <span class="math math-inline">d</span>에 대한 비용 <span class="math math-inline">C(d, h, w)</span>는 다음과 같이 정의된다.1<br />
<span class="math math-display">
C(d, h, w) = \frac{1}{N_c} \langle F_L^{1/4}(h, w), F_R^{1/4}(h, w - d) \rangle
</span><br />
여기서 <span class="math math-inline">N_c</span>는 채널 수, <span class="math math-inline">\langle \cdot, \cdot \rangle</span>는 내적 연산, <span class="math math-inline">(h, w)</span>는 픽셀 좌표, <span class="math math-inline">D_{max}</span>는 최대 시차 범위를 나타낸다. 이 연산을 통해 <span class="math math-inline">D_{max}/4 \times H/4 \times W/4</span> 크기의 3D 비용 볼륨이 생성된다. 이 방식은 특징 벡터를 단순히 스칼라 값(유사도)으로 압축하기 때문에 정보 손실의 위험이 있으나, Lite Any Stereo는 후속 단계인 하이브리드 집계 모듈을 통해 이를 효과적으로 보완한다.</p>
<h3>3.3  하이브리드 비용 집계 (Hybrid Cost Aggregation)</h3>
<p>Lite Any Stereo의 가장 독창적인 기술적 기여는 바로 <strong>하이브리드 비용 집계(Hybrid Cost Aggregation)</strong> 모듈이다.1 기존 연구들은 정확도를 위한 3D CNN(3D Convolution)과 속도를 위한 2D CNN(2D Convolution) 사이에서 양자택일하는 경향이 있었다.</p>
<ul>
<li><strong>3D CNN의 장점:</strong> 시차(Disparity) 차원 간의 관계를 학습하여 기하학적 일관성을 유지하고 매칭 모호성을 줄이는 데 탁월하다.</li>
<li><strong>2D CNN의 장점:</strong> 연산 비용이 저렴하며, 넓은 수용 영역(Receptive Field)을 통해 공간적 문맥(Spatial Context)을 파악하는 데 유리하다.</li>
</ul>
<p>Lite Any Stereo는 이 두 가지 합성곱 연산을 유기적으로 결합하여 상호 보완적인 이점을 취한다.</p>
<ol>
<li><strong>3D Aggregation (기하학적 정제):</strong> 초기 비용 볼륨에 대해 가벼운 3D CNN 레이어를 적용한다. 이 단계에서는 시차 축을 따라 인접한 시차 값들 간의 관계를 분석하여, 노이즈가 섞인 매칭 비용 분포를 매끄럽게(Smooth) 만들고 다중 피크(Multi-peak)와 같은 모호한 신호를 억제한다. 파라미터 수를 줄이기 위해 채널 수와 레이어 깊이는 최소화된다.</li>
<li><strong>2D Aggregation (공간적 문맥 확장):</strong> 3D 처리를 거친 볼륨 정보를 공간 차원으로 투영하거나 변환하여 2D CNN으로 전달한다. 2D CNN은 더 깊고 넓은 구조를 가질 수 있어, 이미지 전체의 문맥 정보를 활용하여 텍스처가 없는 영역(예: 흰 벽, 도로)에서의 매칭 성능을 보완한다. 1의 절제 연구(Ablation Study)에 따르면, 2D와 3D를 혼합한 구성(Interleaved or Hybrid)이 단독 사용 시보다 월등히 높은 정확도와 효율성 균형을 보여준다.</li>
<li><strong>상호 보완적 통합:</strong> 결과적으로 이 모듈은 3D CNN이 제공하는 정밀한 시차 후보군 선별 능력과 2D CNN이 제공하는 광범위한 문맥 이해 능력을 동시에 활용한다. 이는 적은 연산량으로도 복잡한 실세계 장면에서 강건한 성능을 내는 핵심 요인이다.</li>
</ol>
<h3>3.4  시차 추정 및 볼록 업샘플링 (Disparity Estimation &amp; Convex Upsampling)</h3>
<p>집계된 비용 볼륨 <span class="math math-inline">C_{agg}</span>는 Softmax 함수를 통과하여 각 시차에 대한 확률 분포로 변환된다. 최종 시차 <span class="math math-inline">\mathbf{d}</span>는 Soft-argmin(또는 기대값 연산)을 통해 미분 가능한 방식으로 계산된다.<br />
<span class="math math-display">
\mathbf{d} = \sum_{d=0}^{D_{max}/4 - 1} d \times \sigma(C_{agg}(d))
</span><br />
여기서 <span class="math math-inline">\sigma(\cdot)</span>는 Softmax 연산이다. 이렇게 계산된 시차 맵은 원본 해상도의 <span class="math math-inline">1/4</span> 크기이다. 이를 원본 해상도로 복원하기 위해 볼록 업샘플링(Convex Upsampling) 기법을 사용한다.1</p>
<p>단순한 이중선형 보간(Bilinear Interpolation)은 물체의 경계면을 뭉개는(Blurring) 부작용이 있다. 반면 볼록 업샘플링은 고해상도 특징 맵에서 학습된 가중치(Weights)를 사용하여 주변 <span class="math math-inline">3 \times 3</span> 영역의 저해상도 시차 값들의 가중 합(Convex Combination)으로 고해상도 시차 값을 예측한다. 이는 RAFT에서 사용된 방식과 유사하며, 경계면의 날카로움(Sharpness)을 유지하면서도 효율적인 업샘플링을 가능하게 한다.</p>
<h2>4.  3단계 학습 전략 및 데이터 활용 (Training Strategy)</h2>
<p>아키텍처의 혁신만으로는 제로샷 일반화의 난제를 완벽히 해결하기 어렵다. Lite Any Stereo 연구진은 이를 보완하기 위해 **대규모 데이터와 지식 증류(Knowledge Distillation)**를 결합한 정교한 3단계 학습 전략(Three-Stage Training Strategy)을 제안했다.1 이 전략은 모델이 합성 데이터의 편향(Bias)에서 벗어나 실세계의 다양성에 적응하도록 유도한다.</p>
<h3>4.1  1단계: 합성 데이터 기반 지도 학습 (Synthetic Supervision)</h3>
<ul>
<li>
<p><strong>목표:</strong> 모델이 기본적인 스테레오 매칭의 기하학적 원리를 학습하고 초기 가중치를 안정적으로 수렴시키는 단계이다.</p>
</li>
<li>
<p><strong>데이터셋:</strong> Scene Flow 1, Virtual KITTI 2, Falling Things 등 약 180만 장(1.8M)의 합성 스테레오 이미지 쌍을 사용한다. 이 데이터들은 정확한 정답(Ground Truth) 시차 맵을 제공하므로, 픽셀 단위의 정확한 지도 학습이 가능하다.</p>
</li>
<li>
<p>학습 방법: 표준적인 평활 L1 손실(Smooth L1 Loss) 함수를 사용하여 예측 시차와 정답 시차 간의 오차를 최소화한다.<br />
<span class="math math-display">
\mathcal{L}_{disp} = \text{Smooth}_{L1}(\mathbf{D}_{pred} - \mathbf{D}_{gt})
</span></p>
</li>
<li>
<p><strong>한계:</strong> 이 단계만 거친 모델은 합성 데이터 특유의 텍스처나 조명 환경에 과적합되어, 실제 사진을 입력받았을 때 성능이 떨어진다.</p>
</li>
</ul>
<h3>4.2  2단계: 자가 증류를 통한 도메인 불변성 학습 (Self-Distillation)</h3>
<ul>
<li><strong>목표:</strong> 입력 이미지의 스타일이나 노이즈 변화에 영향을 받지 않는 강건한(Robust) 특징 표현을 학습한다.</li>
<li><strong>데이터셋:</strong> 1단계와 동일한 합성 데이터셋을 사용하되, 데이터 증강(Augmentation) 기법을 적극 활용한다.</li>
<li><strong>메커니즘:</strong> 교사-학생(Teacher-Student) 네트워크 구조를 활용한다.</li>
<li>두 모델 모두 1단계에서 학습된 가중치로 초기화된다.</li>
<li><strong>교사 모델(Teacher):</strong> 원본(Clean) 이미지를 입력받아 시차를 예측한다.</li>
<li><strong>학생 모델(Student):</strong> 색상 변환(Color Jittering), 가우시안 노이즈 추가, 밝기 조절 등 강한 섭동(Perturbation)이 가해진 이미지를 입력받는다.</li>
<li>학생 모델은 자신의 예측이 교사 모델의 예측(일종의 정답으로 간주)과 일치하도록 학습된다. 이를 통해 모델은 시각적 방해 요소(Visual Artifacts)를 무시하고 본질적인 기하학적 구조에 집중하는 법을 배운다. 이는 일관성 정규화(Consistency Regularization)의 일종으로 볼 수 있다.8</li>
</ul>
<h3>4.3  3단계: 실세계 지식 증류 (Real-World Knowledge Distillation)</h3>
<ul>
<li><strong>목표:</strong> 합성 데이터의 한계를 넘어 실제 세계의 무한한 다양성을 모델에 주입한다.</li>
<li><strong>데이터셋:</strong> 레이블이 없는(Unlabeled) 대규모 실세계 데이터셋 약 50만 장(0.5M)을 사용한다. 여기에는 자율주행 데이터인 DrivingStereo 1, 다양한 야외 환경을 포함한 Mapillary, Holopix50k 등이 포함된다.</li>
<li><strong>메커니즘:</strong> 의사 레이블링(Pseudo-Labeling) 기법을 핵심으로 한다.</li>
<li><strong>교사 모델:</strong> RAFT-Stereo나 IGEV-Stereo와 같이 성능은 매우 뛰어나지만 무거운 최신 모델을 교사로 선정한다. 이 모델의 가중치는 고정(Frozen)된다.</li>
<li><strong>지식 전이:</strong> 교사 모델이 실세계 데이터에 대해 예측한 시차 맵을 ’의사 정답(Pseudo-Label)’으로 생성한다. 비록 이 의사 정답이 완벽하지 않을 수 있지만, 대규모 데이터에 대한 통계적인 패턴을 학습하는 데는 충분하다.</li>
<li>학생 모델(Lite Any Stereo)은 이 의사 정답을 학습함으로써, 자신이 직접 경험하지 못한 복잡한 실세계 패턴(반사, 투명 물체, 복잡한 텍스처 등)을 처리하는 고성능 모델의 ’지식’을 전수받는다.</li>
<li>이 단계는 모델의 용량 한계를 외부의 지식으로 보완하는 과정으로, 제로샷 성능을 비약적으로 향상시키는 결정적인 역할을 한다.11</li>
</ul>
<h2>5.  실험 결과 및 성능 평가 (Experimental Evaluation)</h2>
<p>Lite Any Stereo의 성능은 KITTI 2012, KITTI 2015, ETH3D, Middlebury 등 대표적인 스테레오 매칭 벤치마크를 통해 검증되었다. 특히 훈련 데이터에 포함되지 않은 데이터셋에 대한 제로샷(Zero-Shot) 평가가 중점적으로 수행되었다.</p>
<h3>5.1  제로샷 일반화 성능 비교</h3>
<p>Lite Any Stereo는 모든 효율적(Efficient) 모델 중에서 압도적인 제로샷 성능을 기록했다. 다음 표는 주요 벤치마크에서의 에러율(Error Rate)과 연산량(MACs)을 비교한 것이다.1</p>
<table><thead><tr><th><strong>모델 (Method)</strong></th><th><strong>분류</strong></th><th><strong>KITTI 2012 (D1-all)</strong></th><th><strong>KITTI 2015 (D1-all)</strong></th><th><strong>ETH3D (Bad 1.0)</strong></th><th><strong>Middlebury (Bad 2.0)</strong></th><th><strong>연산량 (MACs)</strong></th></tr></thead><tbody>
<tr><td>Efficient Baseline (SceneFlow Only)</td><td>Efficient</td><td>5.02%</td><td>5.01%</td><td>31.97%</td><td>26.42%</td><td>~30G</td></tr>
<tr><td>MobileStereoNet-2D 1</td><td>Efficient</td><td>19.30%</td><td>21.88%</td><td>17.10%</td><td>37.98%</td><td>127G</td></tr>
<tr><td>CoEx 1</td><td>Efficient</td><td>22.30%</td><td>17.33%</td><td>31.97%</td><td>26.42%</td><td>54G</td></tr>
<tr><td><strong>Lite Any Stereo</strong></td><td><strong>Efficient</strong></td><td><strong>1.36%</strong></td><td><strong>1.71%</strong></td><td><strong>6.48%</strong></td><td><strong>11.29%</strong></td><td><strong>33G</strong></td></tr>
<tr><td>RAFT-Stereo (Ref) 3</td><td>Accurate</td><td>Low</td><td>Low</td><td>Low</td><td>Low</td><td>&gt;2000G</td></tr>
</tbody></table>
<p><em>표 1: 주요 스테레오 매칭 모델의 제로샷 성능 및 연산량 비교. 수치가 낮을수록 성능이 우수하다. (D1-all: 3픽셀 또는 5% 이상의 오차를 가진 픽셀 비율)</em></p>
<ul>
<li><strong>압도적인 효율성:</strong> Lite Any Stereo는 단 <strong>33G MACs</strong>의 연산량만으로 작동한다. 이는 경쟁 모델인 MobileStereoNet(127G) 대비 약 1/4 수준이며, 정확도 지향 모델인 RAFT-Stereo나 IGEV-Stereo(수천 G) 대비 1% 미만의 연산량이다.1</li>
<li><strong>SOTA급 제로샷 성능:</strong> KITTI 2015 데이터셋에서 1.71%의 D1-all 오차율을 기록했다. 이는 단순히 효율적인 모델 중 1위일 뿐만 아니라, 수천 G MACs를 사용하는 일부 무거운 모델들과 비교해도 대등하거나 더 우수한 수치이다. 특히 Scene Flow만 학습한 베이스라인(5.01%) 대비 오차율을 1/3 수준으로 줄인 것은 3단계 학습 전략의 유효성을 입증한다.</li>
<li><strong>다양한 도메인 적응:</strong> 실내 데이터인 Middlebury와 실내외 복합 데이터인 ETH3D에서도 매우 낮은 오차율을 기록하여, 특정 도메인에 편중되지 않고 범용적으로 작동함을 보여준다.</li>
</ul>
<h3>5.2  정확도 대비 효율성 (Accuracy-Efficiency Trade-off)</h3>
<p>연구 결과는 Lite Any Stereo가 “정확도와 효율성은 상충 관계(Trade-off)“라는 기존의 통념을 깼음을 시사한다. 그래프 상에서 Lite Any Stereo는 좌하단(적은 연산량, 낮은 오차)의 가장 이상적인 위치인 파레토 프론티어(Pareto Frontier)를 점유한다. 이는 아키텍처의 비효율성을 제거하고 데이터의 힘으로 성능을 끌어올린 결과로 해석된다.</p>
<h3>5.3  실행 속도 및 하드웨어 호환성</h3>
<ul>
<li><strong>추론 속도:</strong> KITTI 해상도(<span class="math math-inline">1242 \times 375</span>) 기준으로 약 **0.02초 (50 FPS)**의 처리 속도를 보인다.12 이는 자율주행 차량의 센서 주기(보통 10Hz~30Hz)를 여유 있게 만족시키는 속도이다.</li>
<li><strong>레거시 하드웨어 지원:</strong> 연구진은 NVIDIA GTX 1080과 같은 구형 GPU에서도 실시간 처리가 가능함을 보고했다.7 이는 최신 고가 장비(RTX 3090/4090 등)가 없는 환경에서도 충분히 배포 및 활용이 가능함을 의미하며, 산업 현장에서의 가용성을 크게 높여준다.</li>
</ul>
<h3>5.4  절제 연구 (Ablation Study)</h3>
<p>연구진은 하이브리드 비용 집계 모듈의 효용성을 검증하기 위해 구성 요소를 변경하며 실험을 진행했다.1</p>
<ul>
<li><strong>2D Only:</strong> 연산은 빠르지만, 시차 차원의 정보 부재로 인해 복잡한 영역에서 매칭 실패가 빈번함.</li>
<li><strong>3D Only:</strong> 정확도는 높으나 연산 비용이 급증하여 경량 모델의 범주를 벗어남.</li>
<li><strong>Hybrid (Lite Any Stereo):</strong> 2D와 3D를 혼합하여 사용했을 때, 3D Only 모델에 근접한 정확도를 유지하면서도 연산량은 2D Only 모델 수준으로 억제하는 시너지 효과를 확인했다. 또한 3단계 학습 전략을 단계별로 적용할 때마다 성능이 계단식으로 향상됨을 확인하여, 아키텍처와 학습 전략이 모두 필수적임을 증명했다.</li>
</ul>
<h2>6.  심층 분석 및 논의 (In-depth Analysis &amp; Discussion)</h2>
<h3>6.1  “경량 모델은 제로샷이 불가능하다“는 통념의 파괴</h3>
<p>Lite Any Stereo의 가장 큰 학문적 기여는 모델의 크기(Capacity) 자체가 제로샷 성능의 절대적인 제약 조건이 아님을 실증한 것이다. 기존에는 모델이 거대해야만 다양한 도메인의 지식을 ’암기’하고 일반화할 수 있다고 믿어졌다. 그러나 본 연구는 **“잘 설계된 귀납적 편향(Inductive Bias)”**과 **“양질의 대규모 데이터 학습(Data-Centric AI)”**이 결합되면, 작은 모델도 충분히 일반화된 표현을 학습할 수 있음을 보여준다. 특히 의사 레이블링을 통해 교사 모델의 지식을 ’증류’해 넣은 것은, 모델의 물리적 크기는 작게 유지하면서 논리적 지식의 밀도를 높인 영리한 접근이다.</p>
<h3>6.2  기초 모델(Foundation Model) 시대의 현실적 대안</h3>
<p>최근 컴퓨터 비전 분야는 거대 기초 모델이 주도하고 있다. 그러나 이러한 모델들은 추론 비용이 너무 높아 엣지 디바이스에 탑재하기 어렵다. Lite Any Stereo는 기초 모델을 **실행 시간(Runtime)**에 사용하는 대신, **학습 시간(Training Time)**에 교사(Teacher)로 활용하는 전략을 취했다. 이는 “거인(Foundation Model)의 어깨 위에 올라탄 난장이(Lite Model)“와 같은 형국으로, 배포 단계에서의 효율성을 최우선으로 하는 엔지니어링 관점에서 매우 실용적이고 경제적인 접근이다.</p>
<h3>6.3  산업적 파급 효과: 자율주행과 로보틱스</h3>
<p>자율주행차, 배달 로봇, 드론 등은 배터리 수명과 발열 문제로 인해 탑재할 수 있는 컴퓨팅 파워에 한계가 있다. 기존에는 LiDAR와 같은 고가의 센서에 의존하거나 성능이 낮은 스테레오 모델을 사용해야 했다. Lite Any Stereo는 저사양 하드웨어에서도 고신뢰도의 3D 인지(Perception)를 가능하게 하여, 로보틱스 분야의 센서 퓨전(Sensor Fusion) 비용을 획기적으로 낮출 수 있다. 특히 0.02초의 빠른 반응 속도는 급박한 상황에서의 회피 기동이나 경로 계획(Path Planning)에 있어 결정적인 이점을 제공한다.1</p>
<h3>6.4  향후 연구 방향</h3>
<p>Lite Any Stereo는 정지 영상(Static Image)에 대한 스테레오 매칭에 집중했다. 향후에는 비디오(Video) 데이터의 시간적 일관성(Temporal Consistency)을 활용하여 성능을 더욱 높이거나, 카메라 파라미터가 보정되지 않은(Uncalibrated) 환경에서도 작동할 수 있는 강건함을 확보하는 방향으로 연구가 확장될 수 있다. 또한, 이 연구에서 입증된 ‘하이브리드 아키텍처 + 지식 증류’ 파이프라인은 광학 흐름(Optical Flow)이나 씬 플로우(Scene Flow) 등 다른 픽셀 단위 매칭(Dense Matching) 과제에도 적용 가능한 보편적인 방법론이 될 가능성이 높다.</p>
<h2>7.  결론 (Conclusion)</h2>
<p><strong>Lite Any Stereo</strong>는 효율적인 스테레오 매칭 모델이 도달할 수 있는 성능의 상한선을 재정의한 연구이다. 본 보고서의 분석을 종합하면 다음과 같은 결론을 도출할 수 있다.</p>
<ol>
<li><strong>혁신적인 아키텍처 설계:</strong> 33G MACs라는 극소량의 연산 자원으로 기존의 수백~수천 G MACs 모델들과 경쟁 가능한 성능을 확보했다. 이는 3D와 2D 합성곱의 장점만을 취합한 <strong>하이브리드 비용 집계</strong> 모듈의 설계 덕분이다.</li>
<li><strong>데이터 중심의 제로샷 성능 확보:</strong> 합성 데이터에서 실세계 데이터로 이어지는 <strong>3단계 학습 전략</strong>을 통해, 추가적인 미세 조정 없이도 다양한 환경에서 즉시 사용 가능한 수준(Production-Ready)의 일반화 성능을 달성했다. 이는 학습 데이터의 양과 질, 그리고 학습 방법론이 모델 아키텍처만큼이나 중요함을 시사한다.</li>
<li><strong>실용성과 확장성:</strong> 거대 모델의 지식을 학습 단계에서만 활용하고 추론 단계에서는 배제함으로써, 성능과 속도라는 두 가지 상충되는 목표를 동시에 달성했다. 이는 즉각적인 상용화가 가능한 수준의 기술적 완성도를 보여준다.</li>
</ol>
<p>결론적으로, Lite Any Stereo는 단순히 하나의 새로운 모델을 제안한 것을 넘어, **“작은 모델도 올바르게 가르치면 모든 곳에서 잘 작동할 수 있다(Efficient models can master Any-world knowledge)”**는 새로운 패러다임을 제시하였다. 이는 자원이 제한된 환경에서의 AI 기술 발전 방향에 중요한 이정표가 될 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Lite Any Stereo: Efficient Zero-Shot Stereo Matching - arXiv, https://arxiv.org/html/2511.16555v1</li>
<li>Efficient Zero-Shot Stereo Matching - arXiv, https://arxiv.org/pdf/2511.16555</li>
<li>Multilevel Recurrent Field Transforms for Stereo Matching, https://www.researchgate.net/publication/357645961_RAFT-Stereo_Multilevel_Recurrent_Field_Transforms_for_Stereo_Matching</li>
<li>fabiotosi92/Awesome-Deep-Stereo-Matching - GitHub, https://github.com/fabiotosi92/Awesome-Deep-Stereo-Matching</li>
<li>Practical Stereo Matching via Cascaded Recurrent Network with …, https://www.researchgate.net/publication/363910925_Practical_Stereo_Matching_via_Cascaded_Recurrent_Network_with_Adaptive_Correlation</li>
<li>[PDF] Iterative Geometry Encoding Volume for Stereo Matching, https://www.semanticscholar.org/paper/Iterative-Geometry-Encoding-Volume-for-Stereo-Xu-Wang/697e176d66a17c0b24613b8513ab951dc4112c34</li>
<li>Lite Any Stereo: Efficient Zero-Shot Stereo Matching - ResearchGate, https://www.researchgate.net/publication/397824321_Lite_Any_Stereo_Efficient_Zero-Shot_Stereo_Matching</li>
<li>Lite Any Stereo: Efficient Zero-Shot Stereo Matching - ChatPaper, https://chatpaper.com/paper/211776</li>
<li>(PDF) UASOL, a large-scale high-resolution outdoor stereo dataset, https://www.researchgate.net/publication/335478678_UASOL_a_large-scale_high-resolution_outdoor_stereo_dataset</li>
<li>Adaptive Frequency Information Selection for Stereo Matching, https://www.researchgate.net/publication/384236981_Selective-Stereo_Adaptive_Frequency_Information_Selection_for_Stereo_Matching</li>
<li>Overview of the proposed three-stage training strategy. Stage ①, https://www.researchgate.net/figure/Overview-of-the-proposed-three-stage-training-strategy-Stage-The-lite-model-is_fig1_397824321</li>
<li>The KITTI Vision Benchmark Suite - Lite Any Stereo, https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=a4226e0faf3689f163f6ae2aabc28dd3c7bb55b5</li>
<li>9 Great Plugins for Enhancing Stereo Width (+ Mix Tips), https://theproaudiofiles.com/stereo-width-plugins/</li>
<li>Favorite stereo imaging plugins? : r/audioengineering - Reddit, https://www.reddit.com/r/audioengineering/comments/1df7wtp/favorite_stereo_imaging_plugins/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>