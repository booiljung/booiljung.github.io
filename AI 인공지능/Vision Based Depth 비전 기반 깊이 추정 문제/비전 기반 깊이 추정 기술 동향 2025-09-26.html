<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:비전 기반 깊이 추정 기술 동향 (2025-09-26)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>비전 기반 깊이 추정 기술 동향 (2025-09-26)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">비전 기반 깊이 추정 (Vision Based Dept)</a> / <span>비전 기반 깊이 추정 기술 동향 (2025-09-26)</span></nav>
                </div>
            </header>
            <article>
                <h1>비전 기반 깊이 추정 기술 동향 (2025-09-26)</h1>
<h2>1.  서론</h2>
<h3>1.1  비전 기반 깊이 추정의 정의와 중요성</h3>
<p>비전 기반 깊이 추정(Vision-Based Depth Estimation)은 2차원(2D) 이미지로부터 3차원(3D) 장면 구조에 대한 핵심 정보인 깊이(depth)를 픽셀 단위로 추정하는 프로세스를 의미한다.1 이는 단순히 거리를 측정하는 것을 넘어, 3D 재구성, 장면 이해(scene understanding), 객체 인식 등 컴퓨터 비전의 수많은 근본적인 작업을 해결하기 위한 필수적인 선행 기술이다.1</p>
<p>이 기술의 중요성은 다양한 첨단 응용 분야에서 명확하게 드러난다. 자율주행 시스템에서 깊이 정보는 차량과 전방의 장애물, 보행자, 다른 차량 간의 거리를 정확히 파악하여 충돌을 회피하고 안전한 주행 경로를 계획하는 데 결정적인 역할을 한다.3 로보틱스 분야에서는 로봇이 주변 환경을 3D로 인식하고, 물체의 위치와 형태를 파악하여 정교한 조작(manipulation)이나 충돌 없는 이동을 수행할 수 있도록 한다.1 또한, 증강현실(AR) 및 가상현실(VR)에서는 현실 세계 위에 가상 객체를 이질감 없이 겹쳐 보이게 하거나(occlusion), 물리적으로 상호작용하도록 만드는 등 몰입감 높은 경험을 구현하는 기반 기술로 활용된다.2</p>
<h3>1.2  기술 패러다임의 전환: 능동 센서에서 비전 기반 접근법으로</h3>
<p>전통적으로 3D 깊이 정보는 LiDAR(Light Detection and Ranging)나 ToF(Time-of-Flight)와 같은 능동 센서(Active Sensor)를 통해 획득했다.2 이 센서들은 레이저나 적외선 신호를 직접 방출하고, 그것이 물체에 반사되어 돌아오는 시간을 측정하여 거리를 계산하는 방식으로 매우 정확한 깊이 값을 제공한다.2 그러나 이러한 능동 센서는 높은 비용, 상대적으로 낮은 해상도, 데이터의 희소성(sparsity), 그리고 강한 햇빛이나 특정 재질의 표면에서 발생하는 노이즈 간섭 등 명확한 한계를 가지고 있다.2</p>
<p>이러한 한계에 대한 대안으로, 스마트폰이나 차량용 블랙박스 등 일상 기기에 보편적으로 탑재된 카메라를 활용하는 비전 기반 깊이 추정 기술이 부상했다. 이는 주변광을 이용하는 수동 센서(Passive Sensor) 방식으로, 별도의 고가 장비 없이 소프트웨어만으로 깊이 정보를 얻을 수 있어 비용 효율성이 매우 높다.2 또한, 카메라는 고해상도의 조밀한(dense) 정보를 제공하며, 능동 센서에 비해 다양한 환경 조건에 덜 민감하다는 장점을 가진다.2 과거 3D 인식 기술이 고가의 전문 장비에 의존하여 일부 연구나 고예산 프로젝트에 국한되었다면, 비전 기반 기술의 발전은 저렴하고 보편적인 카메라를 통해 3D 인식을 대중화하는 결정적인 계기가 되었다. 이는 단순한 비용 절감을 넘어, 이전에는 상상하기 어려웠던 AR, 소비자 로봇 공학 등 새로운 응용 분야를 창출하는 ’3D 인식의 대중화’를 이끌고 있다.</p>
<h3>1.3  딥러닝과 파운데이션 모델: 깊이 추정의 새로운 시대</h3>
<p>초기 비전 기반 깊이 추정은 주로 기하학적 원리에 기반했으나, 최근에는 대규모 데이터셋과 강력한 컴퓨팅 파워를 바탕으로 한 딥러닝, 특히 CNN(Convolutional Neural Networks) 기반 모델로 패러다임이 전환되었다.1 딥러닝 모델은 이미지에 내재된 복잡하고 미묘한 깊이 단서들을 데이터로부터 직접 학습하여 기존 방법론의 성능을 크게 뛰어넘었다.</p>
<p>더 나아가, 최근 컴퓨터 비전 분야 전반의 가장 중요한 변화는 ’파운데이션 모델(Foundation Models)’의 등장이다. 깊이 추정 분야 역시 이 흐름에 합류하여, 특정 벤치마크 데이터셋에 과적합되는 경향을 보이던 기존 모델의 한계를 극복하려는 시도가 이루어지고 있다. ’깊이 파운데이션 모델’은 수천만 장 이상의 방대하고 다양한 데이터로 사전 학습되어, 이전에 보지 못한 새로운 환경에서도 별도의 미세조정(fine-tuning) 없이 높은 일반화 성능을 보이는 것을 목표로 한다.2 이는 특정 ’문제’를 해결하는 모델에서, 어떠한 상황에서도 작동하는 범용적인 ’깊이 인식 역량’을 구축하는 방향으로 연구의 초점이 이동하고 있음을 시사한다. 이는 NLP 분야에서 개별 과제 모델이 LLM(Large Language Model)으로 전환된 것과 유사한 패러다임의 변화이며, 깊이 추정 연구의 새로운 시대를 열고 있다.</p>
<h2>2.  깊이 추정의 고전적 및 딥러닝 기반 접근법</h2>
<h3>2.1  스테레오 비전</h3>
<h4>2.1.1 원리 및 핵심 공식</h4>
<p>스테레오 비전(Stereo Vision)은 인간의 양안시(binocular vision) 원리를 모방한 기술이다.3 알려진 거리, 즉 베이스라인(B)만큼 수평으로 떨어져 있는 두 대의 카메라로 동일한 장면을 동시에 촬영한다. 이때, 가까운 물체는 두 이미지 상에서 위치 차이가 크게 나타나고, 먼 물체는 작게 나타나는데, 이 픽셀 단위의 수평 위치 차이를 시차(disparity, d)라고 한다.11 삼각 측량(triangulation) 원리에 따라, 이 시차와 카메라의 초점 거리(<span class="math math-inline">f</span>), 베이스라인(<span class="math math-inline">B</span>)을 알면 3D 공간상의 깊이(<span class="math math-inline">Z</span>)를 계산할 수 있다.11 핵심 공식은 다음과 같으며, 깊이(<span class="math math-inline">Z</span>)는 시차(<span class="math math-inline">d</span>)에 반비례한다.15<br />
<span class="math math-display">
Z = \frac{f \cdot B}{d}
</span></p>
<h4>2.1.2 작업 흐름</h4>
<p>스테레오 비전 기반 깊이 추정은 다음과 같은 체계적인 단계를 거친다.11</p>
<ol>
<li><strong>카메라 캘리브레이션 (Camera Calibration):</strong> 정확한 깊이 계산을 위해 카메라의 내부 파라미터(초점 거리, 주점 등)와 두 카메라 간의 상대적인 위치 및 회전을 나타내는 외부 파라미터를 사전에 정밀하게 측정한다.18</li>
<li><strong>이미지 정류 (Image Rectification):</strong> 두 이미지 평면을 가상으로 회전시켜, 대응하는 에피폴라 라인(epipolar lines)이 서로 평행한 수평선이 되도록 변환한다. 이 과정을 통해, 한 이미지의 특정 픽셀에 대응하는 점을 다른 이미지에서 찾을 때 전체 2D 영역이 아닌 동일한 수평선상(1D)에서만 탐색하면 되므로, 대응점 탐색 문제의 복잡도가 크게 감소한다.11</li>
<li><strong>스테레오 매칭 (Stereo Matching):</strong> 정류된 왼쪽 이미지의 각 픽셀(또는 픽셀 블록)과 가장 유사한 대응점을 오른쪽 이미지의 동일한 수평선상에서 찾는다. 전통적으로는 블록 매칭(Block Matching)이나 Semi-Global Matching (SGM)과 같은 알고리즘이 사용되었으나, 최근에는 딥러닝 기반의 매칭 네트워크가 텍스처가 부족하거나 반사되는 표면 등 고전적 방식이 실패하는 까다로운 영역에서 훨씬 우수한 성능을 보인다.11</li>
<li><strong>깊이 계산 (Depth Computation):</strong> 스테레오 매칭을 통해 얻은 픽셀별 시차 값을 담은 시차 맵(disparity map)을 생성하고, 이를 앞서 언급한 핵심 공식에 대입하여 최종적으로 조밀한 깊이 맵(depth map)을 얻는다.11</li>
</ol>
<h4>2.1.3 장단점</h4>
<p>스테레오 비전은 물리적 원리에 기반하므로 상대적으로 정확하며, 실제 미터법 스케일(metric scale)의 깊이를 직접적으로 제공할 수 있다는 큰 장점이 있다. 하지만 하늘이나 흰 벽과 같이 텍스처가 부족한 영역, 반복적인 패턴이 나타나는 영역, 그리고 반사되거나 투명한 표면에서는 픽셀 값 기반의 매칭이 어려워 큰 오차가 발생할 수 있다.19 또한, 한쪽 카메라에서는 보이지만 다른 쪽에서는 가려지는 영역(occlusion)은 원리적으로 깊이 계산이 불가능하다.17</p>
<p>엔드-투-엔드 딥러닝 모델이 등장하면서 고전적인 기하학 원리가 대체될 것이라는 예상과 달리, 최신 연구 동향은 기하학이 딥러닝에 의해 대체되는 것이 아니라 오히려 강화되고 있음을 보여준다. FoundationStereo와 같은 최신 SOTA 모델들은 여전히 스테레오 매칭의 기본 틀을 유지하되, SGM과 같은 고전 알고리즘 대신 강력한 신경망을 특징 추출 및 매칭 엔진으로 사용하여 전통적인 실패 사례에 대해 훨씬 강건한 성능을 달성한다.19 더 나아가, 에피폴라 기하학과 같은 원리들은 이제 자기지도 학습에서 강력한 감독 신호(supervision signal)를 제공하는 귀납적 편향(inductive bias)으로 작용하여, 기하학적 프레임워크와 데이터 기반 딥러닝 간의 강력한 시너지를 창출하고 있다.22</p>
<h3>2.2  단안 깊이 추정</h3>
<h4>2.2.1 본질적 문제와 규모 모호성</h4>
<p>단안 깊이 추정(Monocular Depth Estimation)은 단 한 장의 2D 이미지로부터 3D 깊이를 복원하는 기술이다. 이는 기하학적으로 해가 무수히 많아 본질적으로 ’잘못 제기된 문제(ill-posed problem)’로 간주된다.2 예를 들어, 가까이 있는 작은 물체와 멀리 있는 큰 물체는 2D 이미지 상에서 동일하게 보일 수 있다.</p>
<p>이로 인해 발생하는 가장 근본적인 한계는 ’규모 모호성(Scale Ambiguity)’이다.8 단안 이미지로부터 예측된 깊이 맵은 픽셀 간의 상대적인 깊이 관계는 정확할 수 있으나, 그 값의 절대적인 스케일(예: 미터 단위)은 알 수 없다.9</p>
<h4>2.2.2 학습 단서 및 추정 방식</h4>
<p>이러한 한계에도 불구하고, 딥러닝 모델은 대규모 데이터셋을 통해 이미지에 내재된 다양한 단안 깊이 단서(monocular cues)를 학습하여 깊이를 추론한다. 이러한 단서에는 원근법(perspective), 사물의 상대적 크기, 텍스처의 밀도 변화, 물체 간 가려짐(occlusion), 조명과 그림자로 인한 음영 등이 포함된다.8</p>
<p>단안 깊이 추정은 예측하는 깊이의 종류에 따라 두 가지로 나뉜다.26</p>
<ul>
<li><strong>상대적 깊이 추정 (Relative Depth Estimation):</strong> 픽셀 간의 깊이 순서, 즉 어느 지점이 다른 지점보다 더 가깝고 먼지에 대한 상대적인 관계만을 예측한다. 이는 스케일에 불변(scale-invariant)하므로 더 쉬운 문제로 여겨진다.12</li>
<li><strong>절대적/미터법 깊이 추정 (Absolute/Metric Depth Estimation):</strong> 실제 세계의 미터(meter)와 같은 물리적 단위로 정확한 깊이 값을 예측하는 것을 목표로 한다. 이는 규모 모호성을 해결해야 하므로 추가적인 정보나 강력한 사전 지식을 요구하는 훨씬 더 어려운 과제이다.27</li>
</ul>
<p>스테레오 비전이 두 시점 간의 기하학적 관계를 푸는 ’기하학 문제’라면, 단안 깊이 추정은 본질적으로 ’세상에 대한 사전 지식’을 요구하는 ’추론 문제’에 가깝다. 모델은 자동차의 평균적인 크기, 도로의 질감, 하늘은 항상 멀리 있다는 사실 등 암묵적인 ’세상 지식’을 학습해야만 단일 이미지로부터 깊이를 합리적으로 추론할 수 있다.2 이것이 바로 딥러닝, 특히 방대한 데이터로부터 복잡하고 계층적인 시각적 사전 지식을 학습하는 데 뛰어난 CNN과 트랜스포머 아키텍처가 이 분야에서 비약적인 성능 향상을 이끌어낸 이유이다.12 더 나아가, 이는 왜 파운데이션 모델이 이 분야의 자연스러운 다음 단계인지를 설명한다. 잘못 제기된 문제를 풀기 위해서는 가능한 한 많은 사전 지식이 필요하며, Depth Anything 모델이 6,200만 개 이상의 이미지로 학습하는 것은 전례 없는 양의 ’세상 지식’을 모델에 주입하여 보지 못한 시나리오에서도 강건한 추론을 가능하게 하려는 직접적인 시도라 할 수 있다.13</p>
<h2>3.  자기지도 학습: 데이터의 한계를 넘어서</h2>
<h3>3.1  학습 원리: 기하학을 감독 신호로</h3>
<p>지도 학습 기반의 깊이 추정은 LiDAR 스캐너 등으로 수집한 고품질의 정답(ground-truth) 깊이 데이터를 대량으로 필요로 한다. 그러나 이러한 데이터를 구축하는 것은 비용과 시간이 많이 소요되는 어려운 작업이다.28 자기지도 학습(Self-Supervised Learning)은 이러한 한계를 극복하기 위해, 별도의 정답 레이블 없이 쉽게 구할 수 있는 비디오 시퀀스나 스테레오 이미지 쌍만으로 모델을 학습시키는 혁신적인 패러다임이다.23</p>
<p>자기지도 학습의 핵심 원리는 ’뷰 합성(View Synthesis)’과 ’포토메트릭 재투영 오차(Photometric Reprojection Error)’를 이용해 기하학적 일관성을 감독 신호로 활용하는 것이다.1 학습 과정은 다음과 같다.</p>
<ol>
<li><strong>깊이 및 포즈 예측:</strong> 학습 데이터(예: 연속된 비디오 프레임)에서 하나의 프레임을 ’타겟 이미지’로, 그 인접 프레임을 ’소스 이미지’로 설정한다. 깊이 예측 네트워크(Depth Network)는 타겟 이미지의 픽셀별 깊이를 예측하고, 포즈 예측 네트워크(Pose Network)는 타겟과 소스 이미지 사이의 카메라 상대 변환(회전 및 이동), 즉 ego-motion을 예측한다.9</li>
<li><strong>뷰 합성:</strong> 1단계에서 예측된 깊이와 포즈 정보를 이용하여, 소스 이미지의 각 픽셀을 3D 공간으로 역투영(unproject)한 뒤, 다시 타겟 이미지의 카메라 시점으로 재투영(reproject)한다. 이 과정을 통해 소스 이미지를 타겟 이미지의 시점에서 본 것처럼 ’워핑(warping)’하여 합성 이미지를 생성한다.23</li>
<li><strong>포토메트릭 재투영 오차 최소화:</strong> 만약 깊이와 포즈 예측이 완벽하다면, 합성된 이미지는 원본 타겟 이미지와 거의 동일해야 한다. 따라서, 두 이미지 간의 픽셀 값 차이(예: L1 손실)와 구조적 유사성 차이(예: SSIM 손실)를 결합한 ’포토메트릭 재투영 오차’를 계산한다. 이 오차를 손실 함수(loss function)로 사용하여, 이를 최소화하는 방향으로 깊이 네트워크와 포즈 네트워크의 가중치를 동시에 업데이트한다.8</li>
</ol>
<h3>3.2  주요 과제와 해결책</h3>
<p>자기지도 학습은 강력하지만, 그 기반이 되는 기하학적 가정들로 인해 몇 가지 중요한 과제에 직면한다.</p>
<ul>
<li><strong>정적 세계 가정(Static World Assumption) 위반:</strong> 이 학습 방식은 장면 내의 모든 요소가 카메라의 움직임에 대해서만 상대적으로 변하는 정적인 상태라고 가정한다. 그러나 실제 주행 영상 등에는 독립적으로 움직이는 자동차나 보행자와 같은 **동적 객체(dynamic objects)**가 존재한다. 이러한 객체들은 정적 세계 가정을 위반하여 잘못된 포토메트릭 오차를 유발하고 학습을 심각하게 방해한다.9</li>
<li><strong>해결책 1 (마스킹):</strong> 의미론적 분할(semantic segmentation) 네트워크를 함께 학습시켜 자동차나 사람과 같은 동적 클래스로 분류될 가능성이 있는 객체 영역을 식별하고, 이 영역을 손실 계산에서 제외(masking)하는 방식이다. <strong>SGDepth</strong>는 여기서 한 걸음 더 나아가, 동적 객체가 움직이지 않을 때는 마스킹을 해제하여 해당 객체의 깊이를 정상적으로 학습시키는 영리한 전략을 제안했다.30</li>
<li><strong>해결책 2 (움직임 모델링):</strong> 카메라의 ego-motion과 별개로, 객체 자체의 3D 움직임을 명시적으로 모델링하는 네트워크를 추가하여 동적 요소를 분리해 처리한다.32</li>
<li><strong>해결책 3 (기하학적 제약 활용):</strong> 대부분의 동적 객체는 지면에 닿아 있다는 ’ground-contacting prior’와 같은 강력한 기하학적 사전 지식을 활용하여, 객체의 깊이를 주변 지면의 깊이와 연관시켜 학습을 안정화한다.32</li>
<li><strong>가려짐(Occlusion) 및 텍스처 부족:</strong> 카메라가 움직이면서 한 뷰에서는 보였던 픽셀이 다른 뷰에서는 가려지거나, 하늘이나 흰 벽처럼 텍스처가 없는 영역은 대응점을 찾기 어려워 포토메트릭 오차가 부정확해진다.29</li>
<li><strong>해결책:</strong> <strong>Monodepth2</strong>는 여러 소스 프레임으로부터 합성된 이미지들 중 가장 작은 재투영 오차만을 선택하여 가려짐 문제에 강건하게 대응했다. 또한, 카메라가 정지해 있거나 주변과 동일하게 움직이는 픽셀(예: 동적 객체에 붙어서 같이 움직이는 픽셀)을 식별하여 손실 계산에서 제외하는 ‘auto-masking’ 기법을 도입하여 학습 안정성을 크게 향상시켰다.22</li>
<li><strong>다중 프레임 정보 활용:</strong> 초기 모델들이 두 프레임 간의 관계에만 집중했다면, 최신 연구들은 여러 시간대의 프레임을 동시에 활용하여 시간적 일관성을 높이고 더 정확한 깊이를 추정하는 방향으로 발전하고 있다.10</li>
<li><strong>해결책:</strong> <strong>DualRefine</strong>과 같은 모델은 깊이와 포즈가 서로의 정확도에 영향을 미치는 공생 관계에 있음을 파악하고, 이를 적극적으로 활용한다. 즉, 개선된 깊이 추정치를 사용하여 포즈를 정제하고, 다시 정제된 포즈를 사용하여 깊이를 더욱 정제하는 피드백 루프를 반복적으로 수행한다. 이 과정은 두 추정치를 기하학적으로 일관되고 더 정확한 해로 점진적으로 수렴시키며, 이는 고전적인 Structure-from-Motion(SfM)의 결합 최적화(bundle adjustment) 원리를 딥러닝 프레임워크 내에서 효과적으로 구현한 것으로 볼 수 있다.22</li>
</ul>
<h2>4.  최신 SOTA 기술 동향: 파운데이션 모델과 새로운 패러다임</h2>
<h3>4.1  깊이 추정 파운데이션 모델의 등장</h3>
<p>최근 깊이 추정 분야의 가장 중요한 변화는 ’파운데이션 모델’의 등장이다. 이는 소규모의 특정 벤치마크 데이터셋(예: KITTI, NYUv2)에 맞춰진 모델 개발에서 벗어나, 훨씬 더 큰 규모와 다양성을 갖춘 데이터로 거대 모델을 사전 학습시키는 패러다임을 의미한다.2 파운데이션 모델의 핵심 목표는 이전에 보지 못한 새로운 환경, 조명 조건, 카메라 종류에 대해서도 별도의 파인튜닝 없이 높은 성능을 발휘하는 ‘제로샷 일반화(Zero-shot Generalization)’ 능력을 확보하는 것이다.2</p>
<p>기존 딥러닝 모델들은 학습 데이터와 테스트 데이터의 분포가 다를 경우(예: 합성 데이터로 학습 후 실제 데이터에 적용) 성능이 저하되는 도메인 격차(domain gap) 문제에 취약했다.2 파운데이션 모델은 인터넷에서 수집한 수천만 장 이상의 방대한 이미지 데이터를 학습함으로써 이러한 한계를 극복하고, 특정 데이터셋의 편향을 넘어 세상에 대한 보다 보편적인 시각적 이해를 모델에 내재화하고자 한다.</p>
<h3>4.2  주요 SOTA 모델 심층 분석 (2023-2024)</h3>
<h4>4.2.1 Depth Anything (상대적 깊이 추정의 강자)</h4>
<p>Depth Anything은 복잡한 아키텍처 혁신보다 ’데이터 스케일링’의 힘에 집중하여 상대적 깊이 추정 분야에서 SOTA 성능을 달성한 대표적인 파운데이션 모델이다.13</p>
<ul>
<li><strong>핵심 전략:</strong> 약 6,200만 개의 레이블 없는 이미지들을 자동으로 주석 처리하는 ’데이터 엔진’을 구축하여 데이터 커버리지를 전례 없는 수준으로 확장했다. 이는 모델이 매우 다양한 시나리오를 학습하게 하여 일반화 성능을 극대화하는 기반이 되었다.13</li>
<li><strong>학습 기법:</strong> 두 가지 독창적인 전략을 사용했다. 첫째, 강력한 데이터 증강(data augmentation)을 통해 모델이 풀어야 할 최적화 목표를 더 어렵게 만들어, 모델이 스스로 추가적인 시각적 지식을 탐색하고 강건한 표현(representation)을 학습하도록 유도했다. 둘째, DINO와 같이 대규모 데이터로 사전 학습된 비전 인코더가 가진 풍부한 의미론적 사전 지식(semantic prior)을 깊이 추정 모델이 상속받도록 보조적인 감독(auxiliary supervision)을 추가했다.13</li>
<li><strong>성과:</strong> 그 결과, Depth Anything은 특정 데이터셋에 대한 파인튜닝 없이도 다양한 실내외 환경, 심지어 인터넷에서 무작위로 수집한 이미지에 대해서도 인상적인 제로샷 상대 깊이 추정 성능을 보여주었다. 또한, 실시간 웹캠 영상에서도 빠르고 정교한 깊이 맵을 생성하는 모습을 시연하며 높은 실용성을 입증했다.4</li>
</ul>
<h4>4.2.2 Metric3D &amp; UniDepth (범용 미터법 깊이 예측)</h4>
<p>이 모델들은 단안 깊이 추정의 근본적인 한계인 ’규모 모호성’을 해결하여, 어떤 카메라로 촬영된 이미지에 대해서도 실제 미터(meter) 단위의 깊이를 예측하는 범용 모델을 목표로 한다.</p>
<ul>
<li><strong>Metric3D (ICCV 2023):</strong> 수천 종류의 서로 다른 카메라 모델로 촬영된 800만 개 이상의 대규모 혼합 데이터셋으로 학습했다. 다양한 카메라 내부 파라미터(초점 거리 등)로 인해 발생하는 스케일 모호성을 해결하기 위해, 모든 카메라 정보를 일관된 ’정규 카메라 공간(canonical camera space)’으로 변환하는 모듈을 제안했다. 이를 통해 안정적인 혼합 데이터 학습과 제로샷 미터법 깊이 예측을 가능하게 했다.34</li>
<li><strong>UniDepth (CVPR 2024):</strong> 여기서 한 걸음 더 나아가, 입력 이미지 외에 어떠한 카메라 정보도 주어지지 않는 상황에서도 미터법 깊이를 예측한다. 이를 위해 이미지 자체로부터 카메라 정보를 추론하는 ’자기-프롬프트 카메라 모듈(self-promptable camera module)’을 도입했다. 또한, 카메라와 깊이 표현이 서로 얽히는 문제를 해결하기 위해 ’유사-구면 표현(pseudo-spherical representation)’이라는 새로운 출력 공간을 설계하여 훈련 안정성과 일반화 성능을 크게 향상시켰다.36</li>
</ul>
<h4>4.2.3 ECoDepth &amp; Diffusion4RobustDepth (확산 모델의 활용)</h4>
<p>최근 이미지 생성 분야를 주도하는 확산 모델(Diffusion Model)을 깊이 추정에 접목하려는 시도 또한 활발하다.</p>
<ul>
<li><strong>ECoDepth (CVPR 2024):</strong> 확산 모델을 깊이 추정 네트워크의 백본으로 사용했다. 특히, 모델에 컨텍스트를 제공하는 방식에서 혁신을 보였다. 기존 연구들이 이미지 캡션을 생성하고 이를 텍스트 임베딩으로 변환하여 조건으로 사용한 반면, ECoDepth는 사전 학습된 ViT 모델에서 직접 추출한 이미지 임베딩을 조건으로 사용하여 훨씬 더 풍부하고 상세한 시각적 컨텍스트를 제공했다. 이 접근법을 통해 NYUv2와 KITTI 벤치마크에서 새로운 SOTA를 달성했다.38</li>
<li><strong>Diffusion4RobustDepth (ECCV 2024):</strong> 확산 모델을 데이터 증강 도구로 활용하여 모델의 강건성을 높이는 데 집중했다. ControlNet과 같은 텍스트-이미지 확산 모델을 사용하여, 깊이 추정이 쉬운 맑은 날 이미지로부터 3D 구조는 그대로 유지한 채 비, 눈, 안개, 야간 등 수집하기 어려운 악천후 상황의 이미지를 대량으로 생성했다. 이후, 원본 이미지에서 추정한 깊이 맵을 ’의사 정답(pseudo ground truth)’으로 삼아, 생성된 악조건 이미지로 모델을 파인튜닝하는 자기-증류(self-distillation) 방식을 통해 악조건에 대한 강건성을 획기적으로 개선했다.40</li>
</ul>
<h4>4.2.4 M2Depth (ECCV 2024, 자율주행 특화)</h4>
<p>M2Depth는 자율주행 차량의 360도 주변 환경 인지를 위해 특화된 자기지도 학습 모델이다.</p>
<ul>
<li><strong>아키텍처:</strong> 차량에 장착된 여러 대의 카메라에서 촬영된, 시간적으로 인접한 두 시점의 프레임들(two-frame multi-camera)을 동시에 입력으로 받는다.41</li>
<li><strong>학습 방식:</strong> 카메라 간의 공간적 관계와 프레임 간의 시간적 관계를 모두 활용한다. 즉, 공간적 정합 비용 볼륨(spatial cost volume)과 시간적 정합 비용 볼륨(temporal cost volume)을 각각 구성한 뒤, 이를 효과적으로 융합하여 360도 주변 환경에 대해 시간적, 공간적으로 일관되고 스케일이 정확한 미터법 깊이를 자기지도 방식으로 학습한다.42</li>
</ul>
<p>이러한 SOTA 모델들의 등장은 LLM의 성공을 이끈 ’데이터의 불합리한 효과(The Unreasonable Effectiveness of Data)’가 3D 비전 분야에도 도달했음을 보여준다. Depth Anything의 핵심 혁신은 복잡한 아키텍처가 아닌, 6,200만 장의 데이터셋을 구축한 ’데이터 엔진’이었다.13 그러나 깊이 추정은 3D 기하학과 카메라 파라미터라는 물리적 제약에 본질적으로 묶여 있다는 점에서 순수한 데이터 스케일링만으로는 한계가 있다. Metric3D와 UniDepth의 연구는 스케일링이 실제 세계의 기하학적 복잡성, 즉 인터넷에서 수집한 데이터에 내재된 수천 가지 다른 카메라의 특성을 처리하는 메커니즘과 결합되어야 함을 명확히 보여준다.34 따라서 미래의 SOTA 기술은 단순히 더 많은 데이터가 아니라, ’기하학을 고려한 스케일링(geometrically-aware scaling)’을 통해 달성될 것이며, 이는 다양한 시각적 콘텐츠와 함께 다양한 기하학적 맥락을 소화하고 정규화할 수 있는 아키텍처가 핵심이 될 것임을 시사한다.</p>
<h2>5.  성능 평가: 벤치마크 및 리더보드 분석</h2>
<h3>5.1  표준 벤치마크 데이터셋</h3>
<p>깊이 추정 모델의 성능은 표준화된 벤치마크 데이터셋을 통해 객관적으로 평가된다. 대표적인 데이터셋은 다음과 같다.</p>
<ul>
<li><strong>KITTI:</strong> 자율주행 시나리오를 위한 실외(outdoor) 데이터셋의 사실상 표준이다. 스테레오 카메라, Velodyne LiDAR, GPS/IMU 센서가 장착된 차량으로 독일 카를스루에 시내, 시골, 고속도로 등 다양한 환경에서 수집되었다. LiDAR 스캔으로부터 생성된 희소하지만 정확한 깊이 정답을 제공하며, 스테레오, 깊이 예측/완성, 시각 주행기록계(visual odometry) 등 여러 벤치마크를 운영하고 있다.43</li>
<li><strong>NYU Depth V2:</strong> 실내(indoor) 장면 깊이 추정의 가장 널리 사용되는 벤치마크이다. Microsoft Kinect 센서를 사용하여 침실, 부엌, 사무실 등 464개의 다양한 실내 공간에서 1449개의 정렬된 RGB-D 이미지 쌍과 40만 개 이상의 레이블 없는 프레임을 수집했다. 조밀하게(densely) 레이블링된 깊이 맵을 제공하여 지도 학습 기반 모델의 성능 평가에 필수적으로 사용된다.46</li>
</ul>
<h3>5.2  핵심 평가 지표</h3>
<p>모델이 예측한 깊이 맵(d)과 실제 정답 깊이 맵(d∗)을 비교하여 성능을 정량화하며, 주로 사용되는 지표는 다음과 같다.48</p>
<ul>
<li><strong>오차 기반 지표 (Error-based Metrics, 낮을수록 우수):</strong></li>
<li><strong>AbsRel (Absolute Relative Error):</strong> <span class="math math-inline">\frac{1}{N} \sum \frac{|d_i^* - d_i|}{d_i^*}</span>. 상대적인 절대 오차의 평균.</li>
<li><strong>SqRel (Squared Relative Error):</strong> <span class="math math-inline">\frac{1}{N} \sum \frac{\|d_i^* - d_i\|^2}{d_i^*}</span>. 상대적인 제곱 오차의 평균.</li>
<li><strong>RMSE (Root Mean Square Error):</strong> <span class="math math-inline">\sqrt{\frac{1}{N} \sum (d_i^* - d_i)^2}</span>. 예측 오차의 크기를 직접적으로 나타내는 평균 제곱근 오차.</li>
<li><strong>RMSE log:</strong> 로그 스케일에서의 RMSE로, 큰 오차에 대한 패널티를 줄여준다.</li>
<li><strong>SILog (Scale-invariant Logarithmic Error):</strong> 스케일 불변성을 고려한 로그 오차로, 특히 단안 깊이 추정의 스케일 모호성을 고려한 평가에 적합하며 KITTI 벤치마크의 주요 랭킹 지표로 사용된다.37</li>
<li><strong>정확도 기반 지표 (Accuracy-based Metrics, 높을수록 우수):</strong></li>
<li><strong>Threshold Accuracy (<span class="math math-inline">\delta &lt; thr</span>):</strong> <span class="math math-inline">\max(\frac{d_i}{d_i^*}, \frac{d_i^*}{d_i}) &lt; thr</span> 조건을 만족하는 픽셀의 비율. 일반적으로 임계값(<span class="math math-inline">thr</span>)으로 1.25, <span class="math math-inline">1.25^2</span>, <span class="math math-inline">1.25^3</span>을 사용한다.48</li>
</ul>
<h3>5.3  벤치마크 리더보드 분석</h3>
<p>최신 SOTA 모델들의 성능을 주요 벤치마크에서 비교하면 다음과 같다.</p>
<p><strong>Table 1: KITTI 깊이 예측(Eigen Split) 벤치마크 SOTA 모델 성능 비교</strong></p>
<p>KITTI 벤치마크는 실외 자율주행 환경에서의 성능을 측정하는 중요한 척도이다. 아래 표는 공식 리더보드를 기반으로 상위권 모델들의 성능을 요약한 것이다.37</p>
<table><thead><tr><th>Rank</th><th>Method</th><th>SILog (↓)</th><th>SqRel (↓)</th><th>AbsRel (↓)</th><th>iRMSE (↓)</th><th>Publication</th></tr></thead><tbody>
<tr><td>1</td><td>G2I</td><td>7.34</td><td>0.93</td><td>6.01</td><td>7.37</td><td>-</td></tr>
<tr><td>2</td><td>xNet</td><td>7.51</td><td>0.93</td><td>6.14</td><td>7.62</td><td>-</td></tr>
<tr><td>3</td><td>UniDepthV2</td><td>7.74</td><td>0.91</td><td>5.53</td><td>7.19</td><td>-</td></tr>
<tr><td>4</td><td><strong>UniDepth</strong></td><td>8.13</td><td>1.09</td><td>6.54</td><td>8.24</td><td>CVPR 2024</td></tr>
<tr><td>5</td><td>HyperDepth</td><td>9.16</td><td>1.55</td><td>7.70</td><td>10.15</td><td>-</td></tr>
<tr><td>13</td><td>iDisc</td><td>9.90</td><td>1.62</td><td>8.10</td><td>10.58</td><td>CVPR 2023</td></tr>
<tr><td>17</td><td>TrapNet</td><td>10.15</td><td>1.66</td><td>7.92</td><td>10.45</td><td>CVPR 2023</td></tr>
</tbody></table>
<p><strong>Table 2: NYU Depth V2 벤치마크 SOTA 모델 성능 비교</strong></p>
<p>NYU Depth V2는 실내 환경에서의 깊이 추정 정확도를 평가하는 표준 벤치마크이다. 아래 표는 주요 논문들에서 보고된 SOTA 모델들의 성능을 비교한 것이다.38</p>
<table><thead><tr><th>Rank</th><th>Method</th><th>AbsRel (↓)</th><th>RMSE (↓)</th><th><span class="math math-inline">\delta_1 &gt; 1.25</span> (↑)</th><th>Publication</th></tr></thead><tbody>
<tr><td>1</td><td><strong>ECoDepth</strong></td><td><strong>0.059</strong></td><td><strong>0.238</strong></td><td><strong>0.996</strong></td><td>CVPR 2024</td></tr>
<tr><td>2</td><td>VPD</td><td>0.069</td><td>0.254</td><td>0.964</td><td>ECCV 2022</td></tr>
<tr><td>3</td><td>Distill Any Depth</td><td>0.070</td><td>0.261</td><td>0.993</td><td>-</td></tr>
<tr><td>4</td><td>ZoeDepth</td><td>0.076</td><td>0.276</td><td>0.991</td><td>ICLR 2023</td></tr>
<tr><td>5</td><td>PixelFormer</td><td>0.090</td><td>0.322</td><td>0.929</td><td>-</td></tr>
</tbody></table>
<p>리더보드 분석 결과, UniDepth, ECoDepth 등 2024년 CVPR과 같은 최신 학회에서 발표된 모델들이 기존 SOTA 성능을 경신하며 빠르게 기술 발전을 주도하고 있음을 확인할 수 있다. 특히, 파운데이션 모델과 확산 모델 기반의 새로운 아키텍처들이 다양한 환경에서 높은 정확도와 일반화 성능을 달성하고 있다.</p>
<h2>6.  결론: 현재와 미래</h2>
<h3>6.1  현재 기술의 성과와 한계 요약</h3>
<p>비전 기반 깊이 추정 기술은 딥러닝, 특히 파운데이션 모델과 자기지도 학습의 발전 덕분에 괄목할 만한 성과를 이루었다. 파운데이션 모델의 등장은 특정 데이터셋의 한계를 넘어 전례 없는 제로샷 일반화 성능을 현실화했으며, 자기지도 학습은 값비싼 정답 데이터 없이도 대규모 비디오 데이터를 활용할 수 있는 길을 열었다. 또한, 확산 모델과 같은 새로운 아키텍처가 도입되면서 성능의 한계는 계속해서 돌파되고 있다.</p>
<p>그러나 여전히 해결해야 할 중요한 한계점들이 존재한다.</p>
<ul>
<li><strong>여전한 스케일 모호성:</strong> 자기지도 단안 방식은 외부 정보(예: 카메라 높이, 스테레오 쌍) 없이는 여전히 정확한 미터법 스케일을 안정적으로 복원하기 어렵다.25</li>
<li><strong>악조건에서의 강건성 부족:</strong> 비, 눈, 안개와 같은 악천후나 야간 저조도 환경에서는 이미지 품질 저하로 인해 깊이 추정 성능이 급격히 저하되는 문제가 있다.4</li>
<li><strong>실시간 처리와 경량화:</strong> SOTA 모델들은 수억 개에 달하는 파라미터를 가진 거대 모델인 경우가 많아, 계산 비용이 높고 추론 속도가 느리다. 이를 실제 로봇이나 임베디드 시스템에 탑재하기 위한 모델 경량화는 중요한 과제로 남아있다.10</li>
<li><strong>지도 학습과의 성능 격차:</strong> 자기지도 학습은 많은 발전을 이루었지만, 여전히 양질의 지도 데이터를 사용한 모델과의 절대적인 성능 격차가 존재하며, 이를 줄이기 위한 연구가 필요하다.25</li>
</ul>
<h3>6.2  미래 연구 방향</h3>
<p>이러한 한계를 극복하고 기술을 한 단계 더 발전시키기 위한 미래 연구는 다음과 같은 다각적인 접근법의 융합으로 나아갈 것으로 전망된다.</p>
<ul>
<li><strong>멀티모달 센서 융합 (Multimodal Sensor Fusion):</strong> 카메라가 가진 본질적인 한계(예: 악천후 취약성)를 보완하기 위해, 레이더(Radar), IMU 등 비교적 저렴한 다른 센서 데이터를 융합하는 연구가 더욱 활발해질 것이다. 예를 들어, 레이더는 날씨에 강건하고 직접적인 거리 정보를 제공하므로, 카메라의 조밀한 시각 정보와 결합하여 모든 환경에서 강건한 깊이 추정 시스템을 구축할 수 있다.53</li>
<li><strong>물리 법칙 기반 학습 (Physics-Informed Learning):</strong> 카메라의 물리적 특성(내부/외부 파라미터)과 투영 기하학 원리를 학습 과정에 명시적으로 통합하는 새로운 접근법이 주목받고 있다. 예를 들어, 카메라의 높이와 각도를 알면 지면(ground plane)의 깊이를 물리적으로 계산할 수 있으며, 이를 ’무료 감독 신호(free supervision signal)’로 활용하여 모델이 절대 스케일을 학습하도록 유도할 수 있다. 이는 외부 센서 없이도 물리적 제약을 모델에 주입하여 예측을 안정시키고 스케일 모호성 문제를 완화할 잠재력을 가진다.25</li>
<li><strong>생성 모델의 적극적 활용 (Advanced Use of Generative Models):</strong> 현실 세계에서 수집하기 어려운 다양한 실패 사례(failure case)나 엣지 케이스(edge case) 데이터를 확산 모델과 같은 생성 AI로 대량 합성하여 학습에 활용하는 연구가 심화될 것이다. 이는 실제 데이터셋에서는 드문 악조건이나 특정 시나리오에 대해 모델을 체계적으로 훈련시켜 강건성과 일반화 성능을 체계적으로 향상시키는 효과적인 방법이다.40</li>
</ul>
<p>결론적으로, 미래의 궁극적인 SOTA 깊이 추정 모델은 단일한 접근법에 의존하지 않을 것이다. 그것은 방대한 데이터로 학습된 거대 파운데이션 모델(‘스케일링’)을 기반으로, 미터법 정확도를 위해 ’물리 기반 사전 지식’을 활용하고, 다양한 엣지 케이스에 대한 강건성을 높이기 위해 방대한 ’생성 데이터’로 추가 학습하는 하이브리드 형태가 될 가능성이 높다. 이 세 가지 트렌드는 서로 경쟁하는 것이 아니라, 상호 보완하며 융합하여 인간의 시각 능력을 뛰어넘는 3D 인식 기술의 완성을 향해 나아갈 것이다.</p>
<p><strong>참고 자료</strong></p>
<ol>
<li>Depth Estimation using DNN Architecture and Vision-Based …, https://www.itm-conferences.org/articles/itmconf/pdf/2023/03/itmconf_icdsia2023_02010.pdf</li>
<li>Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation - arXiv, <a href="https://arxiv.org/pdf/2507.11540">https://arxiv.org/pdf/2507.11540?</a></li>
<li>Depth Estimation Techniques in Computer Vision - Orchestra, https://www.getorchestra.io/guides/depth-estimation-techniques-in-computer-vision</li>
<li>The Application of Depth_anything_v2 in Autonomous Driving: Depth Estimation, https://www.researchgate.net/publication/395361309_The_Application_of_Depth_anything_v2_in_Autonomous_Driving_Depth_Estimation</li>
<li>A Novel Panorama Depth Estimation Framework for Autonomous …, https://www.mdpi.com/1424-8220/24/21/7013</li>
<li>2D vision-based with monocular depth estimation for pose estimation - SPIE Digital Library, https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13636/1363606/2D-vision-based-with-monocular-depth-estimation-for-pose-estimation/10.1117/12.3062820.full</li>
<li>The Role of Depth Perception in Advanced Robotic Systems …, https://www.technexion.com/resources/the-role-of-depth-perception-in-advanced-robotic-systems/</li>
<li>Monocular Depth Estimation and Feature Tracking, https://web.stanford.edu/class/cs231a/course_notes/08-monocular_depth_estimation.pdf</li>
<li>Monocular Depth in the Real World | by Toyota Research Institute - Medium, https://medium.com/toyotaresearch/monocular-depth-in-the-real-world-99c2b287df34</li>
<li>Self-supervised recurrent depth estimation with attention mechanisms - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC9044223/</li>
<li>Stereo Vision and Depth Estimation - GeeksforGeeks, https://www.geeksforgeeks.org/computer-vision/stereo-vision-and-depth-estimation/</li>
<li>Monocular Depth Estimation Using Relative Depth Maps - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2019/papers/Lee_Monocular_Depth_Estimation_Using_Relative_Depth_Maps_CVPR_2019_paper.pdf</li>
<li>Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data, https://arxiv.org/abs/2401.10891</li>
<li>Depth (Distance) Perception with Stereo Cameras: Epipolar Geometry and Disparity Map | by NasuhcaN | Medium, https://medium.com/@nasuhcanturker/depth-perception-with-stereo-cameras-epipolar-geometry-and-disparity-map-675b86c0298b</li>
<li>Depth Estimation From Stereo Images Using Deep Learning | by Satya - Medium, https://medium.com/@satya15july_11937/depth-estimation-from-stereo-images-using-deep-learning-314952b8eaf9</li>
<li>Depth Map from Stereo Images - OpenCV Documentation, https://docs.opencv.org/4.x/dd/d53/tutorial_py_depthmap.html</li>
<li>Stereo Vision – A simple system, http://people.scs.carleton.ca/~c_shu/Courses/comp4900d/notes/simple-stereo.pdf</li>
<li>Depth Estimation from Stereo Video - MATLAB &amp; Simulink - MathWorks, https://www.mathworks.com/help/vision/ug/depth-estimation-from-stereo-video.html</li>
<li>Enhancing Stereo Depth Estimation with Deep Learning Techniques, https://www.teledynevisionsolutions.com/learn/learning-center/machine-vision/enhancing-stereo-depth-estimation-with-deep-learning-techniques/</li>
<li>Chapter 11. Depth, https://cse.usf.edu/~r1k/MachineVisionBook/MachineVision.files/MachineVision_Chapter11.pdf</li>
<li>FoundationStereo: INSANE Stereo Depth Estimation for 3D Reconstruction - YouTube, https://www.youtube.com/watch?v=es87f9pQpTo</li>
<li>DualRefine: Self-Supervised Depth and Pose … - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Bangunharcana_DualRefine_Self-Supervised_Depth_and_Pose_Estimation_Through_Iterative_Epipolar_Sampling_CVPR_2023_paper.pdf</li>
<li>Self-supervised Learning in Depth — part 1 of 2 | by Toyota Research Institute - Medium, https://medium.com/toyotaresearch/self-supervised-learning-in-depth-part-1-of-2-74825baaaa04</li>
<li>Breaking New Ground in Monocular Depth Estimation with Dynamic Iterative Refinement and Scale Consistency - MDPI, https://www.mdpi.com/2076-3417/15/2/674</li>
<li>Self-Supervised Depth Estimation Based on Camera Models - arXiv, https://arxiv.org/html/2408.01565v1</li>
<li>Monocular depth estimation - Hugging Face, https://huggingface.co/docs/transformers/tasks/monocular_depth_estimation</li>
<li>[2501.11841] Survey on Monocular Metric Depth Estimation - arXiv, https://arxiv.org/abs/2501.11841</li>
<li>Self-Supervised Monocular Depth Estimation Based on Differential …, https://www.mdpi.com/1999-4893/18/9/590</li>
<li>DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium - arXiv, https://arxiv.org/html/2304.03560v2</li>
<li>Self-Supervised Monocular Depth Estimation: Solving the Dynamic …, https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650579.pdf</li>
<li>Monocular Vision-Based Depth Estimation of Forward-Looking Scenes for Mobile Platforms, https://www.mdpi.com/2076-3417/15/8/4267</li>
<li>Coarse-to-Fine Self-supervised Monocular Depth Estimation of Dynamic Objects with Ground Contact Prior - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Moon_From-Ground-To-Objects_Coarse-to-Fine_Self-supervised_Monocular_Depth_Estimation_of_Dynamic_Objects_with_CVPR_2024_paper.pdf</li>
<li>(PDF) Self-Supervised Monocular Depth Estimation with Scene Dynamic Pose, https://www.researchgate.net/publication/390553962_Self-Supervised_Monocular_Depth_Estimation_with_Scene_Dynamic_Pose</li>
<li>Metric3D: Towards Zero-shot Metric 3D … - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2023/html/Yin_Metric3D_Towards_Zero-shot_Metric_3D_Prediction_from_A_Single_Image_ICCV_2023_paper.html</li>
<li>Depth Estimation on Single Camera with new Depth Anything State-of-the-art Model, https://www.youtube.com/watch?v=mtLiCmtjpi8</li>
<li>arXiv:2403.18913v1 [cs.CV] 27 Mar 2024, <a href="https://arxiv.org/pdf/2403.18913">https://arxiv.org/pdf/2403.18913?</a></li>
<li>The KITTI Vision Benchmark Suite - Andreas Geiger, https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction</li>
<li>ECoDepth: Effective Conditioning of Diffusion Models for Monocular …, https://openaccess.thecvf.com/content/CVPR2024/html/Patni_ECoDepth_Effective_Conditioning_of_Diffusion_Models_for_Monocular_Depth_Estimation_CVPR_2024_paper.html</li>
<li>ECoDepth: Effective Conditioning of Diffusion … - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Patni_ECoDepth_Effective_Conditioning_of_Diffusion_Models_for_Monocular_Depth_Estimation_CVPR_2024_paper.pdf</li>
<li>fabiotosi92/Diffusion4RobustDepth: [ECCV 2024] Diffusion Models for Monocular Depth Estimation: Overcoming Challenging Conditions - GitHub, https://github.com/fabiotosi92/Diffusion4RobustDepth</li>
<li>ECCV 2024 Orals, https://eccv.ecva.net/virtual/2024/events/oral</li>
<li>ECVA | European Computer Vision Association, https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6281_ECCV_2024_paper.php</li>
<li>Implement Stereo Depth Estimation on Python with KITTI Dataset | by Ko - Medium, https://medium.com/@koko0915/implement-stereo-depth-estimation-on-python-with-kitti-dataset-3a117492c55a</li>
<li>The KITTI Vision Benchmark Suite - Andreas Geiger, https://www.cvlibs.net/datasets/kitti/</li>
<li>KITTI Depth Prediction Evaluation - Kaggle, https://www.kaggle.com/datasets/artemmmtry/kitti-depth-prediction-evaluation</li>
<li>NYU Depth V2 « Nathan Silberman, https://cs.nyu.edu/~fergus/datasets/nyu_depth_v2.html</li>
<li>nyu_depth_v2 - Dataset Search, https://toolbox.google.com/datasetsearch/search?query=nyu&amp;docid=wT7gS3xITQNbnuY3AAAAAA%3D%3D</li>
<li>A Comprehensive Evaluation of Monocular Depth Estimation Methods in Low-Altitude Forest Environment - MDPI, https://www.mdpi.com/2072-4292/17/4/717</li>
<li>Towards Real-Time Monocular Depth Estimation for Robotics … - arXiv, https://arxiv.org/pdf/2111.08600</li>
<li>Virtually Enriched NYU Depth V2 Dataset for Monocular Depth Estimation: Do We Need Artificial Augmentation? - arXiv, https://arxiv.org/html/2404.09469v1</li>
<li>Self-supervised Monocular Depth Estimation: Let’s Talk About the Weather., https://publications.aston.ac.uk/id/eprint/45683/</li>
<li>[2408.01565] Embodiment: Self-Supervised Depth Estimation Based on Camera Models, https://arxiv.org/abs/2408.01565</li>
<li>Depth Estimation Based on MMwave Radar and Camera Fusion with Attention Mechanisms and Multi-Scale Features for Autonomous Driving Vehicles - MDPI, https://www.mdpi.com/2079-9292/14/2/300</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>