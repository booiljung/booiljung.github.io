<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:MonSter 단안 깊이 추정과 스테레오 매칭 (Marry Monodepth to Stereo Unleashes Power)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>MonSter 단안 깊이 추정과 스테레오 매칭 (Marry Monodepth to Stereo Unleashes Power)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">비전 기반 깊이 추정 (Vision Based Dept)</a> / <span>MonSter 단안 깊이 추정과 스테레오 매칭 (Marry Monodepth to Stereo Unleashes Power)</span></nav>
                </div>
            </header>
            <article>
                <h1>MonSter 단안 깊이 추정과 스테레오 매칭 (Marry Monodepth to Stereo Unleashes Power)</h1>
<h2>1.  서론: 3차원 인식의 난제와 융합의 필요성</h2>
<p>컴퓨터 비전(Computer Vision) 분야에서 2차원(2D) 이미지로부터 3차원(3D) 기하학적 구조를 복원하는 것은 수십 년간 지속되어 온 난제이자 핵심 과제이다. 자율 주행 자동차가 복잡한 도심 환경에서 안전하게 주행하고, 로봇이 미지의 환경을 탐색하며 조작 작업을 수행하고, 증강 현실(AR) 및 가상 현실(VR) 기기가 사용자에게 몰입감 있는 경험을 제공하기 위해서는 정확하고 신뢰할 수 있는 깊이(Depth) 정보가 필수적이다. 이러한 깊이 정보를 획득하는 방식은 크게 능동형(Active) 센서를 사용하는 방식(LiDAR, ToF 등)과 수동형(Passive) 카메라를 사용하는 방식으로 나뉘는데, 비용 효율성과 데이터의 풍부함 측면에서 카메라 기반의 깊이 추정 기술이 지속적인 주목을 받고 있다.1</p>
<p>카메라 기반 깊이 추정 기술은 다시 두 가지의 거대한 흐름으로 양분되어 발전해 왔다. 첫 번째 흐름은 **스테레오 매칭(Stereo Matching)**이다. 이는 인간의 양안 시차 원리를 모방하여, 수평으로 정렬된 두 대의 카메라(Stereo Pair)에서 획득한 좌우 이미지 간의 픽셀 대응점(Correspondence)을 찾고, 삼각 측량(Triangulation) 기법을 통해 절대적인 거리(Metric Depth)를 계산하는 방식이다. 이 방식은 기하학적 원리에 기반하므로 이론적으로 명확하고, 정확한 물리적 거리를 산출할 수 있다는 강력한 장점이 있다. 그러나 스테레오 매칭은 본질적으로 ’대응점 탐색’이라는 과정에 의존하기 때문에, 대응점을 찾기 어려운 영역에서 치명적인 약점을 노출한다. 예를 들어, 텍스처가 없는 흰 벽이나 맑은 하늘(Textureless regions), 반복적인 패턴이 존재하는 펜스, 빛을 반사하는 유리나 금속 표면(Reflective surfaces), 그리고 한쪽 카메라에서는 보이지만 다른 쪽에서는 보이지 않는 가려짐 영역(Occlusions) 등이 대표적인 ’부적절한 영역(Ill-posed regions)’이다. 이러한 영역에서 기존의 스테레오 알고리즘들은 매칭 모호성에 빠져 심각한 노이즈를 발생시키거나 깊이 정보를 소실하는 경향이 있다.1</p>
<p>두 번째 흐름은 **단안 깊이 추정(Monocular Depth Estimation)**이다. 이는 단 한 장의 이미지만을 입력으로 받아, 딥러닝 모델이 학습한 방대한 데이터의 문맥적(Contextual) 정보를 바탕으로 깊이를 ’추론’하는 방식이다. 최근 Vision Transformer(ViT)와 같은 대규모 파운데이션 모델(Foundation Model)의 등장으로 단안 깊이 추정 기술은 비약적으로 발전하였다. DepthAnything이나 DINOv2와 같은 최신 모델들은 이미지 내의 객체 구조, 원근감, 그림자 등을 해석하여 매우 조밀하고 시각적으로 자연스러운 깊이 맵을 생성한다.3 단안 방식은 스테레오 매칭이 실패하기 쉬운 텍스처리스 영역에서도 강건한(Robust) 구조적 정보를 제공한다는 큰 장점이 있다. 그러나 여기에는 ’스케일 모호성(Scale Ambiguity)’이라는 근본적인 한계가 존재한다. 단일 이미지만으로는 사진 속의 물체가 거대한 빌딩인지 장난감 모형인지, 혹은 카메라로부터 1미터 떨어져 있는지 100미터 떨어져 있는지 물리적으로 확정할 수 없다. 따라서 단안 깊이 추정 결과는 픽셀 간의 상대적인 깊이 관계(Relative Depth)만을 나타낼 뿐, 실제 물리적 거리를 대변하지 못한다.3</p>
<p>이러한 배경 속에서 등장한 **MonSter (Monocular Stereo)**는 이 두 가지 상반된 접근 방식의 장점만을 취합하고 단점을 상호 보완하여 3D 인식의 성능을 극대화하고자 하는 혁신적인 프레임워크이다. Cheng 등이 제안한 MonSter는 “Marry Monodepth to Stereo Unleashes Power“라는 논문 제목이 시사하듯, 단안 깊이의 강력한 구조적 사전 지식(Structural Prior)과 스테레오 매칭의 정밀한 기하학적 단서(Geometric Cues)를 ’결합(Marry)’시킨다.1 단순히 두 결과를 평균 내거나 이어 붙이는 수준을 넘어, MonSter는 **상호 정제 모듈(Mutual Refinement Module)**을 통해 두 분기(Branch)가 반복적으로 정보를 교환하며 서로를 진화시키는 구조를 채택하였다. 스테레오 분기는 단안 분기에 절대적인 스케일 정보를 제공하여 모호성을 제거하고, 단안 분기는 스테레오 분기에 매끄러운 표면과 경계 정보를 제공하여 부적절한 영역의 노이즈를 제거한다.1</p>
<p>본 보고서는 이러한 MonSter 모델에 대해 기술적, 이론적, 실험적 측면에서 포괄적이고 심층적인 조사를 수행한다. 특히 SceneFlow, KITTI, Middlebury, ETH3D 등 주요 벤치마크에서 달성한 1위 성과와 그 의미를 분석하고 2, MonSter가 제시하는 반복적 상호 향상(Iterative Mutual Enhancement) 메커니즘의 수학적 원리를 규명한다. 또한, MonSter의 확장 모델인 MonSter++와 실시간 처리를 위한 RT-MonSter++에 대해서도 다루며 6, 이 기술이 향후 3D 비전 분야에 미칠 파급력을 논의한다.</p>
<h2>2.  이론적 배경 및 관련 연구 심층 분석</h2>
<p>MonSter의 아키텍처적 우수성을 논하기에 앞서, 스테레오 매칭과 단안 깊이 추정 기술의 진화 과정과 기존 융합 시도들의 한계를 명확히 이해할 필요가 있다.</p>
<h3>2.1  스테레오 매칭: 기하학적 원리에서 딥러닝 최적화까지</h3>
<p>스테레오 매칭의 역사는 컴퓨터 비전의 역사와 궤를 같이한다. 초기의 알고리즘들은 윈도우 기반의 블록 매칭(Block Matching)이나 동적 계획법(Dynamic Programming)을 이용한 전역 최적화 방식에 의존했다. 특히 **SGM(Semi-Global Matching)**은 픽셀 단위의 매칭 비용을 여러 방향(Path)으로 집계(Aggregation)하여 전역적 일관성을 유지하면서도 계산 효율성을 확보한 획기적인 방법론이었다. 그러나 이러한 전통적인 방식들은 수작업으로 설계된 특징(Hand-crafted Features)에 의존했기 때문에 조명 변화나 텍스처 부족과 같은 환경적 요인에 매우 취약했다.</p>
<p>딥러닝의 도입은 스테레오 매칭의 패러다임을 완전히 바꾸어 놓았다.</p>
<ol>
<li><strong>초기 CNN 기반 방법:</strong> MC-CNN과 같은 초기 모델들은 단순히 매칭 비용을 계산하는 부분만을 CNN으로 대체하여 정확도를 높였다.</li>
<li><strong>비용 볼륨 필터링 (Cost Volume Filtering):</strong> <strong>GC-Net</strong>과 <strong>PSMNet</strong>의 등장은 스테레오 매칭의 사실상 표준(De facto standard) 아키텍처를 정립했다. 이들은 좌우 이미지 특징을 추출한 뒤 4D 비용 볼륨(너비 <span class="math math-inline">\times</span> 높이 <span class="math math-inline">\times</span> 시차 <span class="math math-inline">\times</span> 특징 채널)을 구성하고, 3D CNN을 통해 이를 정규화(Regularization)한다. 마지막으로 soft-argmax 연산을 통해 미분 가능한 방식으로 시차(Disparity)를 회귀한다. 이 방식은 높은 정확도를 보장하지만, 3D CNN의 막대한 메모리 사용량과 연산 비용이 문제로 지적되었다.1</li>
<li><strong>반복적 최적화 (Iterative Optimization):</strong> 광학 흐름(Optical Flow) 추정 모델인 RAFT의 성공에 영감을 받아, <strong>RAFT-Stereo</strong>와 **IGEV(Iterative Geometry Encoding Volume)**와 같은 모델들이 등장했다. 이들은 전체 비용 볼륨을 한 번에 처리하는 대신, GRU(Gated Recurrent Unit)와 같은 순환 신경망을 사용하여 시차 필드(Disparity Field)를 점진적으로 업데이트한다. 특히 IGEV는 기하학적 정보를 인코딩한 볼륨과 전역적 문맥 정보를 결합하여 반복적 정제 과정에서 시차의 미세한 오차를 수정하는 데 탁월한 성능을 보인다.3</li>
</ol>
<p>MonSter는 이 중 IGEV 아키텍처를 스테레오 분기의 백본으로 채택하였다. IGEV의 반복적 업데이트 구조는 MonSter가 제안하는 ‘상호 정제(Mutual Refinement)’ 메커니즘과 구조적으로 매우 잘 부합하기 때문이다. 반복적인 루프 내에서 외부(단안 분기)의 정보를 주입하여 수렴 방향을 조정하기 용이하다는 점이 핵심이다.3</p>
<h3>2.2  단안 깊이 추정: 데이터 기반의 문맥 이해</h3>
<p>단안 깊이 추정은 “하나의 눈으로 거리를 어떻게 알 수 있는가?“라는 질문에서 시작된다. 기하학적으로는 불가능한 문제(Ill-posed problem)이지만, 인간은 경험적 지식(원근법, 객체의 크기, 가려짐 관계 등)을 통해 이를 수행한다. 딥러닝 모델 역시 방대한 이미지-깊이 쌍 데이터를 학습함으로써 이러한 경험적 지식을 내재화한다.</p>
<p>초기 단안 깊이 모델들은 CNN을 사용하여 픽셀별 깊이를 회귀하거나 분류하는 방식을 취했으나, 최근에는 **Vision Transformer (ViT)**의 도입으로 성능의 퀀텀 점프가 이루어졌다. ViT는 이미지를 패치(Patch) 단위로 나누고 어텐션(Attention) 메커니즘을 적용함으로써, 이미지 전체의 전역적인 문맥(Global Context)을 파악하는 데 탁월하다. 이는 지역적인 특징(Local Features)에 집중하는 CNN과 대비되는 강점이다.</p>
<p>특히 <strong>DepthAnything</strong> 및 <strong>DepthAnythingV2</strong>와 같은 최신 모델들은 레이블이 없는 대규모 데이터셋을 활용한 자기 지도 학습(Self-Supervised Learning)과 강력한 인코더(DINOv2)를 결합하여, 어떠한 환경의 이미지에서도 매우 정밀하고 일관성 있는 상대적 깊이 맵을 생성한다.3 이 모델들은 머리카락처럼 얇은 구조물이나 거울과 같은 반사면에서도 구조적 정보를 잃지 않는다. 그러나 앞서 언급했듯, 이들이 출력하는 값은 ‘미터(Metric)’ 단위가 아닌 정규화된 ‘상대적(Relative)’ 값이므로, 실제 물리적 상호작용이 필요한 로보틱스 등의 분야에 직접 적용하기에는 한계가 있다.</p>
<h3>2.3  기존 융합 연구의 한계점 분석</h3>
<p>스테레오의 ’정확한 스케일’과 단안의 ’강건한 구조’를 결합하려는 시도는 이전에도 존재했다.</p>
<ul>
<li><strong>초기화 전략:</strong> 단안 깊이 추정 결과를 스테레오 매칭의 초기 시차 값으로 사용하거나 탐색 범위를 줄이는 데 사용하는 방식이다.</li>
<li><strong>특징 연결 (Feature Concatenation):</strong> 단안 특징을 스테레오 비용 볼륨에 추가적인 채널로 결합하는 방식이다.</li>
<li><strong>사후 처리 (Post-processing):</strong> 스테레오 결과의 빈 곳(Hole)을 단안 깊이로 메우는 방식이다.</li>
</ul>
<p>그러나 이러한 단순 결합(Naive Fusion) 방식들은 몇 가지 치명적인 문제에 직면했다.</p>
<ol>
<li><strong>노이즈 전파 (Noise Propagation):</strong> 단안 깊이가 가진 스케일 불확실성이나 왜곡이 스테레오 매칭 과정에 노이즈로 작용하여, 오히려 멀쩡한 스테레오 성능을 저하시키는 현상이 발생한다.3</li>
<li><strong>도메인 격차 (Domain Gap):</strong> 단안 모델이 학습한 데이터 도메인과 실제 테스트 환경이 다를 경우, 단안 정보가 잘못된 가이드를 제공할 수 있다.</li>
<li><strong>단방향성 (Unidirectionality):</strong> 정보가 단안에서 스테레오로, 또는 그 반대로만 흐르는 단방향 구조가 대부분이었으며, 두 소스가 서로를 상호 보완하며 발전하는 피드백 루프가 부재했다.</li>
</ol>
<p>MonSter는 이러한 한계를 극복하기 위해 ’반복적 상호 정제’라는 새로운 개념을 도입함으로써, 기존의 단순 결합이 갖는 문제점들을 근본적으로 해결하고자 하였다.</p>
<h2>3.  MonSter 아키텍처 및 핵심 방법론 상세</h2>
<p>MonSter의 핵심은 두 개의 독립적인 전문가(단안 및 스테레오)가 서로 대화하며 답을 찾아가는 과정으로 비유할 수 있다. 이 섹션에서는 MonSter의 전체 아키텍처와 각 구성 요소의 작동 원리를 수학적, 기술적으로 상세히 분석한다.</p>
<h3>3.1  전체 아키텍처 개요 (Overview)</h3>
<p>MonSter는 크게 세 가지 모듈로 구성된다:</p>
<ol>
<li><strong>단안 깊이 추정 분기 (Monocular Depth Estimation Branch)</strong></li>
<li><strong>스테레오 매칭 분기 (Stereo Matching Branch)</strong></li>
<li><strong>상호 정제 모듈 (Mutual Refinement Module)</strong></li>
</ol>
<p>이 시스템은 입력된 스테레오 이미지 쌍에 대해 초기 추정치를 생성한 후, 반복적인 루프(Iteration)를 통해 최종 시차 맵(Disparity Map)을 생성한다.1</p>
<h3>3.2  단안 및 스테레오 분기 (Initial Estimation)</h3>
<h4>3.2.1  단안 깊이 분기</h4>
<p>MonSter는 단안 분기의 성능을 극대화하기 위해 사전 학습된 <strong>DepthAnythingV2</strong> 모델을 백본으로 사용한다.</p>
<ul>
<li><strong>인코더 (Encoder):</strong> <strong>DINOv2</strong> (ViT 기반)를 사용하여 이미지로부터 풍부한 의미론적(Semantic) 및 구조적(Structural) 특징을 추출한다. DINOv2는 객체의 경계를 인식하는 능력이 탁월하다.</li>
<li><strong>디코더 (Decoder):</strong> <strong>DPT (Dense Prediction Transformer)</strong> 헤드를 사용하여 인코딩된 특징을 픽셀 단위의 깊이 맵으로 디코딩한다.</li>
<li><strong>출력:</strong> 초기 단안 깊이 <span class="math math-inline">D_{mono}</span>를 생성한다. 이 값은 아직 스케일이 정해지지 않은 상대적 깊이이다. MonSter는 이를 시차 도메인으로 변환(역수 취함)하여 스테레오 분기와 단위를 맞춘다.3</li>
</ul>
<h4>3.2.2  스테레오 매칭 분기</h4>
<p>스테레오 분기는 <strong>IGEV</strong> 아키텍처를 따른다.</p>
<ul>
<li><strong>특징 추출:</strong> 샴 네트워크(Siamese Network) 구조의 CNN을 통해 좌우 이미지에서 특징 맵을 추출한다. MonSter는 단안 분기와의 특징 호환성을 위해 IGEV의 특징 추출기를 일부 수정하여 사용한다.</li>
<li><strong>비용 볼륨:</strong> 추출된 특징을 바탕으로 4D 비용 볼륨을 생성하고, 이를 기하학적 인코딩 볼륨(Geometry Encoding Volume)으로 변환하여 매칭 정보를 압축한다.</li>
<li><strong>출력:</strong> 초기 스테레오 시차 <span class="math math-inline">D_{stereo}</span>와 함께, 반복적 정제 과정에서 사용될 비용 특징(Cost Features) 및 숨겨진 상태(Hidden State)를 생성한다.3</li>
</ul>
<h3>3.3  상호 정제 모듈 (Mutual Refinement Module): 혁신의 핵심</h3>
<p>이 모듈은 MonSter의 심장부로, **SGA (Stereo Guided Alignment)**와 <strong>MGR (Mono Guided Refinement)</strong> 두 가지 프로세스가 교차하며 수행된다.</p>
<h4>3.3.1  전역 스케일-이동 정렬 (Global Scale-Shift Alignment)</h4>
<p>본격적인 정제에 앞서, 단안 깊이와 스테레오 시차 간의 극심한 스케일 차이를 줄이기 위해 전역적인 정렬을 수행한다. 단안 시차 <span class="math math-inline">D_M</span>을 스테레오 시차 <span class="math math-inline">D_S</span>에 맞추기 위해 최소 자승법(Least Squares) 등을 이용하여 최적의 스케일(<span class="math math-inline">s</span>)과 이동(<span class="math math-inline">t</span>) 파라미터를 찾아 <span class="math math-inline">D_M&#39; = s \cdot D_M + t</span>와 같이 1차적인 변환을 수행한다. 이는 이후의 미세 조정(Fine-tuning)을 위한 초기화 단계이다.1</p>
<h4>3.3.2  스테레오 가이드 정렬 (Stereo Guided Alignment, SGA)</h4>
<p>SGA는 스테레오 정보가 확실한 영역(Reliable regions)을 기준점으로 삼아, 단안 깊이 맵의 픽셀별 혹은 국소 영역별 스케일과 이동을 정밀하게 보정하는 과정이다.</p>
<p>수식적으로, <span class="math math-inline">j</span>번째 반복 단계에서의 단안 시차 <span class="math math-inline">D^j_M</span> 업데이트는 다음과 같이 표현된다:<br />
<span class="math math-display">
D^{j+1}_M = D^j_M + \Delta t
</span><br />
여기서 <span class="math math-inline">\Delta t</span>는 잔차 이동(Residual Shift) 값이다.</p>
<ul>
<li><strong>작동 원리:</strong> SGA 모듈은 현재의 스테레오 시차 <span class="math math-inline">D^j_S</span>와 단안 시차 <span class="math math-inline">D^j_M</span>의 차이를 입력으로 받아, CNN 기반의 예측 네트워크를 통해 보정값 <span class="math math-inline">\Delta t</span>를 생성한다.</li>
<li><strong>신뢰도 기반 가이던스 (Confidence-based Guidance):</strong> 스테레오 매칭은 텍스처가 없는 영역에서는 부정확하므로, 이러한 영역의 정보를 기준으로 단안 깊이를 보정하면 오히려 오차가 커진다. 이를 방지하기 위해 SGA는 스테레오 매칭의 신뢰도(Confidence Map)를 함께 고려한다. 신뢰도가 높은 영역(가장자리, 텍스처 풍부)에서의 차이를 최소화하는 방향으로 <span class="math math-inline">\Delta t</span>를 학습한다. 결과적으로 단안 깊이는 스테레오의 ’절대적 거리감’을 학습하게 된다.1</li>
</ul>
<h4>3.3.3  단안 가이드 정제 (Mono Guided Refinement, MGR)</h4>
<p>MGR은 스케일이 보정된 단안 정보를 활용하여, 스테레오 매칭이 실패한 부적절한 영역의 시차를 수정하는 과정이다. 이는 IGEV의 반복적 업데이트 GRU 모듈을 확장한 형태이다.</p>
<ul>
<li>
<p>흐름 잔차 맵 (Flow Residual Maps): MGR은 현재의 단안 시차 <span class="math math-inline">D^j_M</span>을 기준으로 좌우 특징 맵 간의 불일치를 계산한다.<br />
<span class="math math-display">
F^j_M(x, y) = F_L(x, y) - F_R(x - D^j_M, y)
</span><br />
이 잔차 맵 <span class="math math-inline">F^j_M</span>은 단안 깊이가 지시하는 위치에서의 매칭 에러를 나타낸다. 만약 단안 깊이가 (구조적으로) 정확하다면, 텍스처가 없는 영역이라도 주변 문맥에 의해 올바른 매칭 위치를 가리킬 것이므로 잔차가 작을 것이다.1</p>
</li>
<li>
<p><strong>기하학적 특징 주입:</strong> 단안 분기에서 추출된 고차원 기하학적 특징 <span class="math math-inline">G^j_M</span>도 함께 GRU에 입력된다. 이는 “이곳은 평면이다”, “이곳은 객체의 경계다“와 같은 의미론적 정보를 스테레오 업데이트 과정에 직접 주입하는 효과를 낸다.</p>
</li>
<li>
<p>스테레오 업데이트: GRU는 이러한 단안 가이드 정보를 바탕으로 스테레오 시차의 잔차 <span class="math math-inline">\Delta D</span>를 예측하여 업데이트한다:<br />
<span class="math math-display">
D^{j+1}_S = D^j_S + \Delta D(F^j_S, F^j_M, G^j_M, \dots)
</span><br />
이 과정을 통해 스테레오 시차 맵은 텍스처리스 영역이나 반사면에서도 단안 깊이의 매끄러운 구조를 따라가게 되어, 구멍(Hole)이나 노이즈가 없는 깨끗한 결과를 얻게 된다.1</p>
</li>
</ul>
<h4>3.3.4  반복적 상호 향상 (Iterative Loop)</h4>
<p>SGA와 MGR은 한 번으로 끝나는 것이 아니라 <span class="math math-inline">N</span>회 반복된다.</p>
<ol>
<li>초기 <span class="math math-inline">D_M, D_S</span> 생성.</li>
<li>SGA: <span class="math math-inline">D_S</span>를 참고하여 <span class="math math-inline">D_M</span>의 스케일 보정.</li>
<li>MGR: 보정된 <span class="math math-inline">D_M</span>을 참고하여 <span class="math math-inline">D_S</span>의 구조적 오류 수정.</li>
<li>수정된 <span class="math math-inline">D_S</span>를 다시 SGA에 입력… (반복)</li>
</ol>
<p>이러한 선순환 루프(Virtuous Loop)를 통해, 단안 깊이는 점차 정확한 물리적 스케일을 갖게 되고, 스테레오 깊이는 점차 완벽한 기하학적 구조를 갖추게 된다. 최종적으로 두 분기의 출력은 매우 유사한 고품질의 깊이 맵으로 수렴하게 된다.1</p>
<h2>4.  실험 결과 및 성능 분석</h2>
<p>MonSter의 성능은 주요 공개 벤치마크 데이터셋을 통해 광범위하게 검증되었다. 본 보고서에서는 <strong>SceneFlow</strong>, <strong>KITTI</strong>, <strong>Middlebury</strong>, <strong>ETH3D</strong> 데이터셋에 대한 정량적, 정성적 결과를 심층 분석한다.</p>
<h3>4.1  데이터셋 및 평가 지표 (Comparison Metrics)</h3>
<table><thead><tr><th><strong>데이터셋</strong></th><th><strong>특성</strong></th><th><strong>주요 평가 지표</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>SceneFlow</strong></td><td>대규모 합성 데이터셋 (35k 쌍)</td><td><strong>EPE (End-Point Error)</strong></td><td>픽셀 단위 평균 유클리드 거리 오차</td></tr>
<tr><td><strong>KITTI 2012/2015</strong></td><td>자율주행 실환경 (LiDAR GT)</td><td><strong>Out-Noc / D1-all</strong></td><td>오차가 3px 또는 5% 이상인 픽셀의 비율 (%)</td></tr>
<tr><td><strong>Middlebury</strong></td><td>고해상도 실내, 복잡한 조명/텍스처</td><td><strong>Bad 2.0</strong></td><td>오차가 2px 이상인 픽셀의 비율 (%)</td></tr>
<tr><td><strong>ETH3D</strong></td><td>다양한 실내외 환경, 정밀한 레이저 스캔 GT</td><td><strong>Bad 1.0</strong></td><td>오차가 1px 이상인 픽셀의 비율 (%)</td></tr>
</tbody></table>
<h3>4.2  주요 벤치마크 성능 비교 (Leaderboard Dominance)</h3>
<p>MonSter는 놀랍게도 5개의 주요 리더보드에서 모두 1위를 차지하거나 SOTA(State-of-the-Art)를 경신하였다. 이는 특정 도메인에 국한되지 않는 범용성을 증명한다.2</p>
<h4>4.2.1  SceneFlow (합성 데이터 성능)</h4>
<p>SceneFlow 데이터셋에서 MonSter는 <strong>EPE 0.37px</strong>를 기록하였다. 이는 베이스라인 모델인 IGEV의 0.47px 대비 약 <strong>21%</strong> 향상된 결과이다.</p>
<ul>
<li><strong>분석:</strong> 합성 데이터는 텍스처가 완벽하지 않거나 비현실적인 반사가 포함될 수 있는데, MonSter는 단안 정보를 통해 이러한 영역에서도 안정적인 추정을 수행함을 보여준다. 특히 1px 이상의 오차를 가진 픽셀 비율(&gt;1px)이 IGEV의 5.21%에서 MonSter는 4.25%로 감소하였다.7</li>
</ul>
<h4>4.2.2  KITTI 2012 &amp; 2015 (실외 주행 환경)</h4>
<p>KITTI 2015 벤치마크에서 MonSter는 **D1-all 1.41%**를 달성하였다.</p>
<ul>
<li><strong>경쟁 모델 비교:</strong> IGEV++ (1.43%), BridgeDepth (1.40%), CREStereo (1.69%) 등과 비교했을 때 최상위권의 성능을 유지한다.8</li>
<li><strong>배경 영역(Background)의 개선:</strong> 특히 주목할 점은 D1-bg(배경 영역 오차)의 개선이다. MonSter는 D1-bg 지표에서 기존 모델 대비 약 **18.12%**의 성능 향상을 보였다.1 이는 도로 표지판, 먼 거리의 건물 등 텍스처가 작거나 흐릿한 원거리 객체에 대해 단안 깊이 추정의 구조적 정보가 큰 도움을 주었음을 시사한다.</li>
</ul>
<h4>4.2.3  ETH3D (고정밀 실내외 환경)</h4>
<p>ETH3D는 매우 높은 해상도와 정밀한 Ground Truth를 제공하여 미세한 디테일 복원 능력을 평가하기에 적합하다. MonSter는 여기서 <strong>Bad 1.0 지표</strong> 기준 이전 최고 성능 모델 대비 **49.5%**라는 압도적인 성능 향상을 기록하였다.1</p>
<ul>
<li><strong>의미:</strong> 50%에 가까운 성능 향상은 딥러닝 모델의 발전 속도를 고려할 때 이례적인 수치이다. 이는 기존 스테레오 모델들이 ETH3D의 복잡한 구조(파이프, 식물 등)와 텍스처 없는 벽면에서 얼마나 고전했는지를 반증하며, MonSter의 상호 정제 방식이 이러한 문제를 근본적으로 해결했음을 보여준다.</li>
</ul>
<h3>4.3  제로샷 일반화 (Zero-Shot Generalization) 성능</h3>
<p>딥러닝 기반 스테레오 매칭의 가장 큰 약점 중 하나는 학습 데이터와 다른 환경(Domain Shift)에서의 성능 저하이다. MonSter는 SceneFlow(합성 데이터)로만 학습된 모델을 실제 데이터셋(KITTI, Middlebury 등)에 적용했을 때(Zero-shot), 타 모델을 압도하는 성능을 보였다.2</p>
<ul>
<li><strong>원인 분석:</strong> 기존 스테레오 모델들은 합성 데이터의 특정 패턴에 과적합(Overfitting)되기 쉽다. 반면, MonSter의 단안 분기(DepthAnythingV2)는 수억 장의 다양한 실제 이미지를 통해 학습되었기 때문에, “어떤 이미지가 들어와도 구조는 파악할 수 있는” 강력한 일반화 능력을 내재하고 있다. MonSter는 이 능력을 스테레오 매칭에 이식함으로써, 낯선 환경에서도 구조적으로 올바른 깊이를 추정할 수 있게 된 것이다. 이는 MonSter를 ’파운데이션 모델’급의 범용성을 가진 기술로 평가할 수 있는 근거가 된다.</li>
</ul>
<h3>4.4  절제 연구 (Ablation Study) 및 구성 요소 분석</h3>
<p>MonSter의 성능 향상이 단순히 모델의 크기를 키워서 얻어진 것이 아님을 증명하기 위해 수행된 절제 연구 결과는 다음과 같다.3</p>
<table><thead><tr><th><strong>모델 구성 (Model Configuration)</strong></th><th><strong>EPE (SceneFlow)</strong></th><th><strong>&gt;1px (%)</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>Baseline (IGEV)</strong></td><td>0.47</td><td>5.21</td><td>순수 스테레오</td></tr>
<tr><td><strong>Mono + Conv (단순 결합)</strong></td><td>0.46</td><td>5.12</td><td>미미한 향상</td></tr>
<tr><td><strong>Mono + MGR</strong></td><td>0.43</td><td>4.96</td><td>유의미한 향상</td></tr>
<tr><td><strong>Full Model (MGR + SGA)</strong></td><td><strong>0.37</strong></td><td><strong>4.25</strong></td><td>최고 성능</td></tr>
</tbody></table>
<ul>
<li><strong>단순 결합의 한계:</strong> ’Mono + Conv’의 결과는 단안 정보를 단순히 특징(Feature) 레벨에서 합치는 것만으로는 큰 효과가 없음을 보여준다.</li>
<li><strong>SGA의 중요성:</strong> MGR만 적용했을 때(0.43)보다 SGA를 함께 적용했을 때(0.37) 성능이 대폭 향상되었다. 이는 단안 깊이의 스케일을 먼저 스테레오에 맞춰 정렬(Alignment)해 주지 않으면, 단안 정보가 오히려 노이즈가 될 수 있음을 시사한다. 즉, **“먼저 맞추고(SGA), 그 다음 돕는다(MGR)”**는 MonSter의 설계 철학이 유효함을 입증한다.</li>
</ul>
<h3>4.5  강건성 (Robustness) 분석: 악천후와 저조도</h3>
<p>SQUID 데이터셋을 이용한 악천후(안개, 탁한 물 속 등) 시뮬레이션 실험에서, MonSter는 RAFT-Stereo와 같은 기존 모델보다 훨씬 완만한 성능 저하를 보였다.10</p>
<ul>
<li><strong>분석:</strong> 안개가 짙어지거나 조명이 어두워지면 이미지의 텍스처가 사라져 스테레오 매칭이 불가능해진다. 그러나 단안 깊이 모델은 객체의 윤곽선(Contour)이나 전체적인 형태(Shape)만 희미하게 보여도 깊이를 추론할 수 있는 능력이 있다. MonSter는 이러한 단안의 강건함을 물려받아, 극한의 환경에서도 최소한의 구조적 깊이 정보를 유지할 수 있다.</li>
</ul>
<h2>5.  MonSter++ 및 실시간성으로의 확장</h2>
<p>MonSter의 성공에 힘입어, 연구진은 이를 더욱 확장한 **MonSter++**와 **RT-MonSter++**를 발표하였다. 이는 MonSter를 단순한 스테레오 매칭 도구를 넘어 통합적인 3D 비전 솔루션으로 발전시키려는 시도이다.4</p>
<h3>5.1  MonSter++: 통합 기하학 파운데이션 모델</h3>
<p>MonSter++는 정류된(Rectified) 스테레오 이미지뿐만 아니라, 정류되지 않은 <strong>멀티뷰 스테레오(Multi-View Stereo, MVS)</strong> 작업까지 수행할 수 있도록 확장되었다. MVS는 카메라의 위치가 자유로운 상태에서 3D를 복원하는 기술로, 3D 스캐닝이나 사진 측량(Photogrammetry)에 사용된다.</p>
<ul>
<li><strong>핵심 확장:</strong> MonSter++는 에피폴라 기하(Epipolar Geometry) 제약 조건을 유연하게 적용하여, 다양한 베이스라인(Baseline)과 시점 변화에도 단안 사전 정보를 효과적으로 융합할 수 있는 구조로 개선되었다. 이는 MonSter의 방법론이 2-View Stereo에 국한되지 않고 N-View 기하학 문제로 일반화될 수 있음을 보여준다.</li>
</ul>
<h3>5.2  RT-MonSter++: 실시간 처리를 위한 경량화</h3>
<p>MonSter의 높은 성능에도 불구하고, 거대한 단안 백본(ViT-Large 등)과 반복적 정제 과정은 실시간 처리를 어렵게 만드는 요인이었다. 이를 해결하기 위해 개발된 **RT-MonSter++**는 경량화된 단안 모델과 최적화된 IGEV 백본을 사용한다.</p>
<ul>
<li><strong>성능:</strong> RT-MonSter++는 <strong>47ms</strong>의 추론 속도를 달성하였다. 이는 정확도 중심의 모델인 IGEV(180ms)보다 약 <strong>4배</strong> 빠르면서도, KITTI 2012 등 주요 벤치마크에서 더 높은 정확도를 기록하였다.7</li>
<li><strong>의의:</strong> 이는 자율 주행 자동차나 드론과 같이 제한된 컴퓨팅 자원(Edge Computing) 내에서 고정밀 3D 인식이 필요한 응용 분야에 MonSter 기술을 실제로 탑재할 수 있는 길을 열었다.</li>
</ul>
<h2>6.  결론: 3D 비전의 새로운 지평</h2>
<p>본 보고서를 통해 살펴본 <strong>MonSter</strong>는 단순한 성능 개선을 넘어, 컴퓨터 비전 분야에서 오랫동안 분리되어 발전해 온 두 가지 줄기, 즉 ’기하학적 접근(Stereo)’과 ’학습 기반 문맥적 접근(Mono)’을 성공적으로 통합한 이정표적인 연구이다.</p>
<p>MonSter가 제시한 <strong>“상호 정제(Mutual Refinement)”</strong> 개념은 3D 복원 문제의 불확실성을 해결하는 강력한 해법을 제공한다. 스테레오는 단안에게 ’물리적 척도(Scale)’를, 단안은 스테레오에게 ’구조적 일관성(Consistency)’을 제공함으로써, 각자의 약점을 완벽하게 상쇄하고 시너지를 극대화한다. 그 결과 MonSter는 5대 주요 벤치마크를 석권하고, 제로샷 일반화 성능에서 압도적인 우위를 점하며, 악천후와 같은 극한 환경에서도 강건함을 유지하는 전천후 3D 인식 모델로 자리매김하였다.</p>
<p>더 나아가 MonSter++와 RT-MonSter++로의 확장은 이 기술이 연구실의 실험 데이터를 넘어, 실제 산업 현장(자율 주행, 로보틱스, 디지털 트윈 등)에 적용될 수 있는 실용성까지 갖추었음을 시사한다. 향후 MonSter의 방법론은 비디오 기반 깊이 추정, 3D 장면 재구성(Scene Reconstruction), 그리고 최근 대두되는 3D Gaussian Splatting과 같은 렌더링 기술과 결합하여, 디지털 세계와 물리적 세계를 연결하는 핵심적인 인식 기술로 발전할 것으로 전망된다.</p>
<p>연구자와 엔지니어들은 MonSter가 입증한 **“상보적 데이터 융합(Complementary Data Fusion)”**의 철학을 주목해야 한다. 이는 비단 깊이 추정뿐만 아니라, 센서 융합(Sensor Fusion), 멀티모달 학습(Multimodal Learning) 등 인공지능의 다양한 난제를 해결하는 데 중요한 영감을 제공할 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>MonSter: Marry Monodepth to Stereo Unleashes Power - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2025/papers/Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power_CVPR_2025_paper.pdf</li>
<li>[2501.08643v1] MonSter: Marry Monodepth to Stereo Unleashes Power - arXiv, https://arxiv.org/abs/2501.08643v1/</li>
<li>MonSter: Marry Monodepth to Stereo Unleashes Power - arXiv, https://arxiv.org/html/2501.08643v1</li>
<li>Junda24/MonSter: 【CVPR 2025 Highlight】MonSter: Marry Monodepth to Stereo Unleashes Power - GitHub, https://github.com/Junda24/MonSter</li>
<li>MonSter: Marry Monodepth to Stereo Unleashes Power - IEEE Xplore, https://ieeexplore.ieee.org/iel8/11091818/11091608/11094634.pdf</li>
<li>[2501.08643] MonSter++: Unified Stereo Matching, Multi-view Stereo, and Real-time Stereo with Monodepth Priors - arXiv, https://arxiv.org/abs/2501.08643</li>
<li>MonSter++: Unified Stereo Matching, Multi-view Stereo, and Real-time Stereo with Monodepth Priors - arXiv, https://arxiv.org/html/2501.08643v2</li>
<li>KITTI Stereo 2015 - The KITTI Vision Benchmark Suite, https://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo</li>
<li>Stereo Research Paper | PDF | Computer Vision - Scribd, https://www.scribd.com/document/916416939/Stereo-Research-Paper</li>
<li>Joint Dual-Branch Denoising for Underwater Stereo Depth Estimation - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC12656204/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>