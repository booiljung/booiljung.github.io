<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:시계열 열화상 영상에서의 객체 인식 및 추적 기술 (2025-07-03)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>시계열 열화상 영상에서의 객체 인식 및 추적 기술 (2025-07-03)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 추적 (Object Tracking)</a> / <span>시계열 열화상 영상에서의 객체 인식 및 추적 기술 (2025-07-03)</span></nav>
                </div>
            </header>
            <article>
                <h1>시계열 열화상 영상에서의 객체 인식 및 추적 기술 (2025-07-03)</h1>
<h2>1.  컴퓨터 비전을 위한 열화상 이미징의 기초 원리</h2>
<p>컴퓨터 비전 알고리즘의 도전 과제와 기회를 이해하기 위해서는 열화상 이미징의 근본적인 물리적 원리와 특성을 먼저 확립해야 합니다. 이 파트에서는 열화상 기술의 핵심 원리를 분석하고, 기존의 RGB(가시광선) 카메라와의 비교를 통해 그 장단점을 명확히 하며, 열화상 데이터 고유의 문제점들을 심도 있게 탐구합니다.</p>
<h3>1.1  열화상 카메라의 물리학 및 기술</h3>
<p>열화상 카메라의 작동 원리는 절대영도 이상의 모든 물체가 방출하는 적외선 복사, 즉 열 에너지를 감지하는 것에 기반합니다.1 이는 물체 표면에서 반사된 가시광선을 감지하여 이미지를 생성하는 RGB 카메라와는 근본적으로 다른 원리입니다.3 이 과정은 특수 제작된 렌즈가 적외선 에너지를 센서에 집중시키고, 마이크로볼로미터(microbolometer)로 구성된 센서 어레이가 이 에너지를 전기 신호로 변환하며, 내장된 처리 장치가 이 신호 데이터를 분석하여 시각적인 온도 분포도, 즉 써모그램(thermogram)을 생성하는 단계를 포함합니다.1</p>
<p>이 기술의 핵심은 마이크로볼로미터 센서입니다.1 가시광선보다 파장이 훨씬 긴 적외선 에너지를 효과적으로 감지하기 위해, 각 센서 소자(픽셀)는 물리적으로 더 커야만 합니다.3 이러한 물리적 제약은 열화상 카메라가 동일한 크기의 가시광선 센서에 비해 본질적으로 낮은 해상도를 가질 수밖에 없는 주된 이유가 됩니다. 그러나 해상도의 한계에도 불구하고, 고품질의 열화상 카메라는 0.01°C 수준의 미세한 온도 차이까지 감지할 수 있는 뛰어난 열 감도(thermal sensitivity)를 자랑합니다.3 이 ’열 대비(thermal contrast)’는 가시광선 이미지의 색상 및 밝기 대비를 대체하며, 열화상 환경에서의 객체 인지의 기초를 이룹니다.4</p>
<h3>1.2  비교 분석: 객체 인지 관점에서의 열화상 대 RGB 영상</h3>
<p>객체 탐지 및 추적의 관점에서 열화상 카메라와 RGB 카메라를 비교하면, 각 기술의 고유한 장점과 단점이 명확하게 드러납니다.</p>
<p>조명 독립성 대 고해상도 및 색상 정보</p>
<p>열화상 이미징의 가장 큰 장점은 주변 조명 조건에 구애받지 않는다는 점입니다. 완전한 암흑, 연기, 대부분의 안개 속에서도 객체의 열 신호를 감지하여 명확한 형상을 제공할 수 있습니다.1 이러한 특성은 24시간 감시, 야간 자율주행, 수색 및 구조와 같은 안전이 중요한 분야에서 필수적인 역할을 합니다.7</p>
<p>반면, 센서 기술의 물리적 한계로 인해 열화상 카메라는 일반적으로 RGB 카메라보다 훨씬 낮은 공간 해상도를 가집니다.3 이는 객체의 미세한 특징을 식별하거나, 원거리의 작은 객체를 인식하는 데 명백한 한계로 작용합니다. 또한, 열화상 카메라는 온도 정보만을 제공할 뿐, 색상 정보를 포함하지 않습니다.3 따라서 색상 히스토그램이나 복잡한 텍스처 패턴에 의존하는 전통적인 컴퓨터 비전 알고리즘은 열화상 이미지에 직접 적용하기 어렵습니다. 예를 들어, 서로 다른 색상의 옷을 입었지만 체온이 비슷한 두 사람은 열화상 이미지에서 거의 동일하게 보일 수 있습니다.10</p>
<p>재료 및 환경 의존성</p>
<p>열화상 기술은 특정 물질에 의해 시야가 방해받는 독특한 한계를 가집니다. 대표적으로, 유리는 장파장 적외선을 투과시키지 않고 반사하거나 흡수하기 때문에, 열화상 카메라로는 유리창 너머를 볼 수 없습니다.11 이는 차량용 첨단 운전자 보조 시스템(ADAS)에 열화상 카메라를 적용할 때, 차량 외부에 장착해야 하는 실질적인 제약으로 이어집니다.</p>
<p>비용 및 접근성</p>
<p>과거 열화상 카메라는 높은 가격으로 인해 군사 및 일부 산업 분야에서만 제한적으로 사용되었습니다.2 그러나 마이크로볼로미터 기술의 발전과 BMW와 같은 고급 자동차 브랜드에서의 대량 채택을 기점으로 생산 비용이 크게 절감되었습니다.6 이러한 가격 하락은 열화상 기술이 자율주행, 보안, 산업 진단 등 더 넓은 상업적 시장으로 확산되는 중요한 계기가 되었습니다.</p>
<p>이러한 비교를 통해, RGB와 열화상 기술 사이에는 근본적인 상충 관계가 존재함을 알 수 있습니다. RGB는 객체가 ’무엇’인지 식별하는 데(고해상도, 색상, 질감) 탁월한 반면, 열화상 기술은 특히 악조건 하에서 객체가 ‘어디에’ 있는지 감지하는 데(열 대비를 통한 강건한 탐지) 더 뛰어납니다. 이 분석은 단일 센서만으로는 모든 시나리오에 대응할 수 없음을 시사하며, 두 센서의 장점을 결합하는 센서 퓨전 기술이 단순한 성능 향상을 넘어 시스템의 강건성을 확보하기 위한 필수적인 전략임을 보여줍니다.</p>
<table><thead><tr><th>특징</th><th>열화상 카메라</th><th>RGB 카메라</th><th>객체 인지에 대한 시사점</th></tr></thead><tbody>
<tr><td><strong>작동 원리</strong></td><td>물체에서 방출되는 적외선(열) 에너지 감지 1</td><td>물체에서 반사되는 가시광선 감지 3</td><td>열화상은 조명과 무관하게 작동, RGB는 조명에 의존적임</td></tr>
<tr><td><strong>조명 의존성</strong></td><td>없음. 완전한 어둠, 연기, 안개 속에서도 작동 1</td><td>높음. 빛이 없으면 성능 급격히 저하 3</td><td>야간 및 악천후 환경에서는 열화상이 객체 탐지에 절대적으로 유리함</td></tr>
<tr><td><strong>해상도 및 세부 묘사</strong></td><td>낮음. 센서 소자가 커서 픽셀 수가 적음 3</td><td>높음. 미세한 특징과 텍스처 식별에 용이 3</td><td>객체의 세부 식별(예: 얼굴 인식, 문자 판독)은 RGB가 우수함</td></tr>
<tr><td><strong>색상/텍스처 정보</strong></td><td>없음. 온도 분포를 의사 색상으로 표현 3</td><td>풍부함. 색상 기반 분류 및 분석에 필수적 3</td><td>색상에 의존하는 알고리즘은 열화상에 부적합. 유사 온도의 다른 객체 구분이 어려움 10</td></tr>
<tr><td><strong>주요 환경적 제약</strong></td><td>유리를 투과하지 못함.11 열 평형 상태에서 객체 소실</td><td>안개, 연기, 눈, 비, 역광 등 기상 조건에 민감 9</td><td>차량 내부 장착 불가 등 실용적 제약 발생. RGB는 기상 악화 시 성능 저하</td></tr>
<tr><td><strong>비용 동향</strong></td><td>과거 고가였으나, 자동차 등 대량 생산으로 가격 하락 중 7</td><td>상대적으로 저렴하고 보편화됨 1</td><td>접근성이 높아져 다양한 상업적 응용 분야로 확장되고 있음</td></tr>
<tr><td><strong>핵심 강점</strong></td><td>악조건에서의 강건한 <strong>탐지</strong> 능력</td><td>양호한 조건에서의 정밀한 <strong>식별</strong> 능력</td><td>“객체가 존재하는가?“에 대한 답은 열화상이, “그 객체는 무엇인가?“에 대한 답은 RGB가 더 잘 제공함</td></tr>
<tr><td><strong>핵심 약점</strong></td><td>낮은 해상도와 세부 정보 부족</td><td>조명 및 환경 조건에 대한 취약성</td><td>두 기술은 상호 보완적이며, 강건한 시스템 구축을 위해 퓨전이 필수적임</td></tr>
</tbody></table>
<p><em>표 1: 객체 인지 관점에서의 열화상 및 RGB 카메라 비교 분석</em></p>
<h3>1.3  열화상 데이터 인식의 내재적 도전 과제</h3>
<p>열화상 기술은 고유한 장점만큼이나 뚜렷한 기술적 난제들을 안고 있습니다.</p>
<p>첫째, <strong>낮은 해상도와 작은 객체 문제</strong>는 가장 근본적인 한계입니다.12 원거리에 있는 객체는 이미지 상에서 단 몇 개의 픽셀로만 표현될 수 있어, 의미 있는 특징을 추출하는 것이 매우 어렵습니다. 이 문제를 해결하기 위해 저해상도 이미지를 고해상도로 변환하는 초해상도(Super-Resolution) 기법에 대한 연구가 활발히 진행되고 있습니다.12</p>
<p>둘째, <strong>센서 노이즈 및 아티팩트</strong>는 열화상 이미지의 품질을 저하하는 주요 요인입니다. 성숙한 가시광선 CMOS 센서에 비해 열화상 센서는 고정 패턴 노이즈(fixed-pattern noise)나 블루밍(blooming) 현상과 같은 노이즈에 더 취약한 경향이 있습니다.10 이는 정교한 전처리 과정이나 노이즈에 강건한 알고리즘 설계를 요구합니다.</p>
<p>셋째, <strong>‘열 평형(Thermal Crossover)’ 현상</strong>은 열화상 시스템의 치명적인 약점입니다.15 이는 객체의 온도가 주변 배경의 온도와 일치하게 되어 열 대비가 사라지면서, 열화상 카메라가 일시적으로 객체를 감지하지 못하게 되는 현상을 말합니다.16 이 현상은 주로 일출이나 일몰 시간대에 발생하지만, 비나 구름과 같은 환경적 요인에 의해서도 언제든지 발생할 수 있습니다.16 열 평형 현상은 단순한 기술적 결함을 넘어, 예측 가능한 실패 지점을 만들어낸다는 점에서 시스템의 신뢰성과 안전성에 심각한 위협이 됩니다. 예를 들어, 군사 및 보안 시스템에서 적대 세력은 이 열 평형 시간을 의도적으로 활용하여 감시망을 무력화시킬 수 있습니다. 따라서 이 문제를 완화하기 위한 다중 스펙트럼(서로 다른 적외선 파장대역 사용) 이미징, 편광(polarimetric) 이미징, 또는 RGB나 레이더와 같은 이종 센서와의 퓨전은 선택이 아닌 필수적인 보안 요구사항으로 간주되어야 합니다.16</p>
<p>마지막으로, <strong>데이터 부족 문제</strong>는 딥러닝 기반 접근법의 발전을 가로막는 가장 큰 장애물입니다. ImageNet이나 COCO와 같이 방대한 양의 주석이 달린 공개 데이터셋이 존재하는 RGB 분야와 달리, 고품질의 주석 달린 열화상 데이터셋은 매우 부족합니다.19 이러한 데이터 희소성은 도메인 적응(Domain Adaptation)이나 합성 데이터 생성(Synthetic Data Generation)과 같은 기술 개발의 핵심 동기가 되고 있습니다.</p>
<h2>2.  열화상 비디오에서의 객체 탐지 방법론</h2>
<p>이 파트에서는 개별 열화상 프레임 내에서 객체를 찾는 데 사용되는 알고리즘을 전통적인 접근법과 현대적인 딥러닝 패러다임으로 나누어 심층적으로 분석합니다.</p>
<h3>2.1  전통적인 특징 기반 접근법</h3>
<p>딥러닝이 부상하기 전, 컴퓨터 비전 분야에서는 수작업으로 설계된 특징(handcrafted features)을 통해 객체를 표현하고 탐지했습니다.</p>
<p>HOG (Histogram of Oriented Gradients)</p>
<p>HOG는 객체의 형태를 경사도 방향의 분포로 표현하는 강력한 특징 기술자(feature descriptor)입니다.13 보행자 탐지 분야에서 기초적인 방법론으로 자리 잡았으며, 열화상 이미지에서도 SVM(Support Vector Machine)과 같은 분류기와 결합하여 널리 사용되었습니다.13 특히 열화상 데이터에서는 다른 전통적 특징인 LBP나 PCA보다 우수한 성능을 보이는 것으로 보고되었습니다.6</p>
<p>LBP (Local Binary Patterns)</p>
<p>LBP는 이미지의 질감(texture)을 표현하는 데 사용되는 특징 기술자입니다. HOG와 LBP를 단순하게 결합할 경우, 특정 객체 클래스에서는 오히려 성능이 저하될 수 있음이 관찰되었습니다.25 이는 열화상 이미지의 특성상 질감 정보가 제한적이기 때문일 수 있으며, 젠틀부스트(Gentle-Boost)와 같은 기법을 통해 유용한 특징만을 선별적으로 사용하는 것이 중요함을 시사합니다.25</p>
<p>이러한 전통적인 특징 기반 접근법들은 계산적으로 효율적이라는 장점이 있지만, 복잡하고 다양한 실제 환경에서는 딥러닝 모델에 비해 강건성과 일반화 성능이 떨어지는 명백한 한계를 가집니다.26</p>
<h3>2.2  딥러닝 패러다임을 이용한 열화상 객체 탐지</h3>
<p>현대의 객체 탐지는 대부분 딥러닝, 특히 합성곱 신경망(CNN)을 기반으로 합니다. 열화상 객체 탐지 분야는 완전히 새로운 모델을 설계하기보다는, 방대한 RGB 데이터셋으로 사전 훈련된 성공적인 아키텍처를 열화상 도메인에 맞게 ’적응(adapting)’시키는 방향으로 발전해왔습니다.6 이는 막대한 규모의 RGB 데이터로 학습된 모델의 강력한 특징 추출 능력을 활용하기 위한 전략적 선택입니다. 하지만 이러한 적응 과정은 도메인 불일치(domain shift)로 인한 성능 저하를 동반하며, 이를 극복하기 위한 다양한 기법이 요구됩니다.28</p>
<h4>2.2.1  CNN 아키텍처의 적응</h4>
<p>2단계 탐지기 (Two-Stage Detectors)</p>
<p>Faster R-CNN과 같은 2단계 탐지기는 먼저 객체가 있을 법한 후보 영역(region proposal)을 생성한 후, 각 후보 영역에 대해 분류를 수행하는 방식입니다.29 높은 정확도를 자랑하지만, 다단계 처리 과정으로 인해 속도가 느려 실시간 응용에는 부적합한 경우가 많습니다.27 주로 열화상 탐지 연구에서 성능 비교를 위한 기준 모델(baseline)로 활용됩니다.30</p>
<p>1단계 탐지기 (One-Stage Detectors)</p>
<p>YOLO(You Only Look Once), SSD(Single Shot MultiBox Detector)와 같은 1단계 탐지기는 후보 영역 생성 없이 한 번의 네트워크 통과(single pass)로 객체의 위치와 클래스를 동시에 예측합니다.29 이들은 속도와 정확도 사이의 균형이 뛰어나 실시간 시스템에 매우 적합합니다.9 특히 YOLO 계열 모델들은 빠른 속도와 활발한 오픈소스 커뮤니티 덕분에 열화상 객체 탐지 연구에서 가장 널리 사용되는 아키텍처 중 하나입니다.9</p>
<h4>2.2.2  실시간 추론을 위한 경량 모델</h4>
<p>드론, 자율주행차의 ECU(Electronic Control Unit) 등 많은 실제 응용 환경은 계산 능력, 메모리, 전력 소비에 대한 엄격한 제약을 가집니다.33 이러한 엣지 디바이스(edge device) 환경에서는 모델의 효율성이 정확도만큼이나 중요한 설계 기준이 됩니다. 이는 단순히 가장 높은 mAP(mean Average Precision)를 기록하는 모델이 아니라, 주어진 하드웨어의 FPS(Frames Per Second) 및 전력 예산 내에서 요구 성능을 만족하는 모델이 ’최적’의 모델임을 의미합니다. 따라서 열화상 객체 탐지는 순수한 알고리즘 설계 문제를 넘어, 소프트웨어와 하드웨어의 제약을 함께 고려하는 공동 설계(co-design) 문제로 접근해야 합니다.</p>
<p>이를 위해 <strong>YOLOv5s</strong> 9, <strong>YOLOv4-tiny</strong> 33, <strong>MobileNet</strong> 35, <strong>EfficientDet</strong>과 같은 경량 모델들이 개발되었습니다. 이 모델들은 깊이별 분리 합성곱(depthwise separable convolution) 39이나 간소화된 백본(backbone) 구조 35와 같은 기법을 사용하여 파라미터 수와 계산량(MACs, Multiply-Accumulate operations)을 획기적으로 줄입니다.40</p>
<p>한 연구 사례는 YOLOv5 모델을 열화상 데이터와 엣지 디바이스에 맞게 경량화하는 실용적인 접근법을 보여줍니다.9 이 연구에서는 기존 모델의 탐지 헤드(detection head) 레이어 수를 줄이고, 채널별 어텐션 모듈(CBAM)을 추가하여 중요한 특징에 집중하도록 했습니다. 그 결과, 정확도는 단 1.1% 감소하면서 파라미터 수는 4.375배나 줄이는 데 성공했습니다. 이는 속도와 정확도 사이의 실용적인 절충을 통해 실제 응용에 적합한 모델을 만들어내는 전형적인 예시입니다.</p>
<h4>2.2.3  Grad-CAM을 이용한 모델 해석 가능성</h4>
<p>딥러닝 모델은 종종 ’블랙박스’로 비유될 만큼 내부 작동 원리를 이해하기 어렵습니다.41 모델이 왜 특정 예측을 내렸는지 알 수 없다는 점은 특히 안전이 중요한 시스템에서 기술을 신뢰하고 도입하는 데 큰 장벽이 됩니다.</p>
<p>**Grad-CAM (Gradient-weighted Class Activation Mapping)**은 이러한 문제를 해결하기 위한 시각화 기법입니다.41 이 기법은 특정 클래스 예측에 가장 큰 영향을 미친 입력 이미지의 영역을 히트맵(heatmap) 형태로 보여줍니다. 이는 네트워크의 마지막 합성곱 레이어로 흘러 들어가는 그래디언트(gradient) 정보를 활용하여 계산됩니다.42</p>
<p>열화상 객체 탐지기에 Grad-CAM을 적용하면, 엔지니어는 모델이 엉뚱한 배경 정보가 아닌, 객체의 올바른 열 신호(예: 자동차의 엔진, 사람의 몸)에 집중하고 있는지를 시각적으로 확인할 수 있습니다.44 이는 모델의 예측을 신뢰할 수 있는지 검증하고, 오작동 시 원인을 분석하는 강력한 디버깅 도구로 활용됩니다.43 예를 들어, Grad-CAM은 일반 이미지에서 고양이와 개를 분류할 때 모델이 각 동물의 어느 부분에 주목하는지 보여줄 수 있으며 45, 동일한 원리가 포장도로 분석과 같은 열화상 응용에도 적용될 수 있습니다.44</p>
<h2>3.  시계열 열화상 데이터에서의 객체 추적 알고리즘</h2>
<p>이 파트는 단일 프레임에서의 객체 탐지를 넘어, 시계열 비디오 데이터의 핵심인 시간의 흐름에 따라 객체를 지속적으로 추적하는 기술들을 다룹니다.</p>
<h3>3.1  추적-탐지(Tracking-by-Detection) 프레임워크</h3>
<p>현대의 다중 객체 추적 시스템은 대부분 ’추적-탐지’라는 지배적인 패러다임을 따릅니다.46 이 방식은 각 비디오 프레임마다 먼저 객체 탐지기(파트 II에서 설명)를 실행하여 현재 프레임에 존재하는 모든 객체의 위치를 식별합니다. 그 후, 별도의 추적 알고리즘이 이 새로운 탐지 결과들을 이전 프레임에서부터 이어져 온 기존의 추적 궤도(track)와 연관시키는 역할을 담당합니다.46 이처럼 탐지와 추적을 모듈화하면 각 구성 요소를 독립적으로 개선하고 교체할 수 있어 유연성이 높습니다.</p>
<p>이러한 구조에서 탐지기와 추적기는 상호 보완적인 관계를 형성합니다. 시스템의 전체 성능은 단순히 각 부분의 성능을 합한 것이 아니라, 탐지기의 강점과 약점이 추적기의 설계 요구사항에 직접적인 영향을 미칩니다. 예를 들어, 탐지기의 바운딩 박스 예측이 불안정하고 노이즈가 많다면, 추적기는 칼만 필터와 같은 강력한 상태 추정 및 스무딩 기능을 갖추어야 합니다.32 만약 탐지기가 열 평형이나 짧은 가림 현상으로 인해 객체를 종종 놓친다면, 추적기의 예측 능력이 시스템의 강건성을 좌우하게 됩니다. 특히, 외형적 특징이 뚜렷하지 않은 열화상 이미지에서 탐지기가 두 객체를 구분하지 못하는 경우, 추적기는 단순한 위치 정보(IoU) 이상의 기준, 즉 외형 기반의 재식별(Re-identification) 능력을 갖추어야 합니다. 이는 DeepSORT와 같은 알고리즘이 개발된 핵심적인 이유입니다.49 따라서 성공적인 추적 시스템 설계는 특정 응용 분야(예: 열화상 감시)에서 예상되는 탐지기의 실패 모드를 보완할 수 있는 추적기를 선택하는 전체론적 관점을 요구합니다.</p>
<h3>3.2  확률적 상태 추정 필터</h3>
<h4>3.2.1  칼만 필터 (Kalman Filter)</h4>
<p>칼만 필터는 노이즈가 포함된 일련의 측정값으로부터 선형 동적 시스템의 상태를 재귀적으로 추정하는 알고리즘입니다.32 객체 추적 분야에서 칼만 필터는 객체의 운동 상태(예: 위치, 속도)를 모델링하고, 이를 바탕으로 다음 프레임에서의 위치를 예측하는 데 사용됩니다.29</p>
<p>새로운 탐지 결과가 기존 궤도와 연결되면, 칼만 필터는 이 측정값을 이용해 자신의 상태 추정치를 갱신합니다. 이 과정은 탐지기에서 발생하는 노이즈를 효과적으로 걸러내고 객체의 이동 경로를 부드럽게 만들어주는 역할을 합니다.32 만약 탐지기가 짧은 시간 동안 객체를 놓치더라도(예: 짧은 가림), 칼만 필터는 자체적인 운동 모델에 따라 객체의 위치를 계속해서 예측함으로써 추적의 연속성을 유지해 줍니다.32</p>
<p>칼만 필터를 구현하기 위해서는 상태 전이 모델(행렬 A)과 측정 모델(행렬 H)을 정의해야 합니다. 필터의 성능은 프로세스 노이즈 공분산(행렬 Q)과 측정 노이즈 공분산(행렬 R)이라는 두 파라미터를 통해 조절됩니다. Q와 R은 필터가 자신의 운동 모델 예측과 실제 탐지기 측정값 중 어느 쪽을 더 신뢰할지를 결정하는 역할을 합니다.48</p>
<h4>3.2.2  파티클 필터와 비선형 운동</h4>
<p>칼만 필터는 객체의 움직임이 선형적이라는 가정을 전제로 합니다. 하지만 실제 세계의 객체들은 급격한 방향 전환이나 속도 변화와 같은 비선형적인 움직임을 보이는 경우가 많습니다. 이러한 복잡하고 비선형적인 운동을 모델링하기 위해 파티클 필터(Particle Filter)가 대안으로 사용될 수 있습니다.51</p>
<p>파티클 필터는 객체의 상태에 대한 확률 분포를 ’파티클’이라고 불리는 다수의 가중치 있는 샘플들로 근사하여 표현합니다. 이를 통해 다중 모드(multi-modal) 분포나 비-가우시안(non-Gaussian) 분포를 가진 복잡한 상태를 추정할 수 있습니다. 하지만 일반적으로 칼만 필터에 비해 계산 비용이 훨씬 높다는 단점이 있습니다.52</p>
<h3>3.3  딥러닝 기반 추적 기법</h3>
<p>객체 추적 기술은 순수한 운동 기반의 추정에서 외형 기반의 재식별로 진화하는 뚜렷한 경향을 보입니다. 이는 단순히 ’움직이는 무언가’를 추적하는 것을 넘어, 각 객체의 ’정체성’을 유지하며 추적하려는 시도를 반영합니다. 칼만 필터나 SORT와 같은 초기 방법론들은 객체의 예측된 위치에만 관심을 가질 뿐, 그 객체가 어떻게 생겼는지는 고려하지 않습니다.47 이러한 접근법은 두 객체의 경로가 교차하거나 한 객체가 가려졌다가 다시 나타나는 등 운동 모델이 모호해지는 상황에서 쉽게 실패합니다.50 시스템은 다시 나타난 객체가 이전에 추적하던 객체와 동일한지, 아니면 완전히 새로운 객체인지 판단할 수 없기 때문입니다.</p>
<p>샴 네트워크(Siamese Network)와 DeepSORT는 이러한 한계를 극복하기 위한 다음 세대의 진화된 기술입니다.49 이들은 “새롭게 탐지된 객체가 이전에 추적하던 객체와 외형적으로 유사한가?“라는 질문에 답하기 위해 명시적인 외형 모델을 학습합니다. 이를 통해 객체 재식별(Re-ID)이 가능해지며, 이는 복잡한 상호작용 속에서도 오랜 시간 동안 일관된 추적 ID를 유지하는 데 결정적인 역할을 합니다. 따라서 복잡한 열화상 환경에서의 강건한 추적 기술의 미래는 운동 정보와 외형 정보를 효과적으로 융합하는 능력에 달려 있으며, 특히 세부 묘사가 부족한 열화상 데이터에 특화된, 더 변별력 있는 외형 모델을 개발하는 것이 중요한 연구 과제로 남아있습니다.</p>
<h4>3.3.1  샴 네트워크를 이용한 외형 정합</h4>
<p>샴 네트워크는 두 개의 동일한(가중치를 공유하는) 신경망 가지(branch)로 구성된 아키텍처입니다.55 이 네트워크는 두 개의 다른 이미지를 각각의 가지에 입력받아, 유사한 이미지 쌍(예: 동일 객체의 다른 이미지)은 특징 공간(feature space)에서 서로 가깝게, 다른 이미지 쌍은 서로 멀게 맵핑하도록 학습됩니다.56</p>
<p>객체 추적에 이를 적용할 때, 한쪽 가지에는 추적을 시작할 때의 목표 객체 템플릿 이미지를, 다른 쪽 가지에는 새로운 프레임의 탐색 영역 이미지를 입력합니다. 네트워크는 탐색 영역 내 모든 위치에 대한 유사도 맵(similarity map)을 출력하며, 가장 높은 유사도 점수를 가진 위치가 객체의 새로운 위치로 결정됩니다.55 이 방식은 추적 중에 별도의 온라인 학습이 거의 필요 없어 매우 빠른 속도를 자랑합니다.55 DSiam과 같은 최신 샴 네트워크 추적기들은 시간에 따른 객체의 외형 변화에 대응하기 위해 온라인 적응 메커니즘을 통합하여, 전통적인 업데이트 기반 추적기와의 성능 격차를 줄이고 있습니다.58</p>
<h4>3.3.2  탐지와 외형 정보의 통합: SORT와 DeepSORT</h4>
<p>SORT (Simple Online and Realtime Tracking)</p>
<p>SORT는 매우 실용적이고 효율적인 추적-탐지 알고리즘입니다. 칼만 필터를 이용해 객체의 움직임을 예측하고, 헝가리안 알고리즘(Hungarian algorithm)을 사용해 새로운 탐지 결과와 기존 궤도를 바운딩 박스의 중첩 영역(IoU, Intersection over Union)을 기준으로 연결합니다.47 SORT의 가장 큰 약점은 객체의 외형 정보를 전혀 사용하지 않기 때문에, 객체들이 서로 가리거나 교차할 때 ID가 쉽게 바뀐다는(ID switch) 점입니다.50</p>
<p>DeepSORT</p>
<p>DeepSORT는 SORT의 이러한 단점을 극복하기 위해 심층 연관 측정(deep association metric)을 도입한 확장 버전입니다.46 DeepSORT는 SORT의 IoU 기반 정합에 더해, 별도로 사전 훈련된 딥러닝 모델을 사용하여 각 탐지된 객체의 외형 특징 벡터(appearance feature vector)를 추출합니다.47</p>
<p>데이터 연관 단계에서 DeepSORT는 운동 정보(칼만 필터 예측에 기반한 마할라노비스 거리)와 외형 정보(특징 벡터 간의 코사인 거리)를 결합한 비용 행렬을 사용합니다.49 이를 통해 객체가 오랜 시간 가려졌다가 다시 나타나더라도 그 외형적 유사성을 바탕으로 동일한 객체임을 재식별할 수 있습니다. 이 능력은 ID 전환을 획기적으로 줄여, 혼잡한 장면에서도 훨씬 더 강건한 추적 성능을 제공합니다.49</p>
<table><thead><tr><th>알고리즘</th><th>핵심 원리</th><th>가림(Occlusion) 처리</th><th>ID 전환(ID Switch) 처리</th><th>계산 비용</th><th>대표적 사용 사례</th></tr></thead><tbody>
<tr><td><strong>칼만 필터</strong></td><td>선형 운동 모델 기반의 상태 예측 및 갱신 32</td><td>단기 예측 가능, 장기 가림에 취약</td><td>처리 능력 없음</td><td>매우 낮음</td><td>노이즈가 있는 탐지기의 궤적 스무딩, 단기 예측</td></tr>
<tr><td><strong>SORT</strong></td><td>칼만 필터 + IoU 기반 데이터 연관 (헝가리안 알고리즘) 47</td><td>단기 예측 가능, 가림 후 재식별 불가</td><td>IoU에만 의존하여 ID 전환이 잦음 50</td><td>낮음</td><td>실시간 성능이 중요하고 객체 간 상호작용이 적은 시나리오</td></tr>
<tr><td><strong>DeepSORT</strong></td><td>SORT + 외형 특징 벡터를 이용한 심층 연관 측정 49</td><td>외형 정보로 장기 가림 후 재식별 가능 49</td><td>외형 유사도를 함께 고려하여 ID 전환을 크게 줄임 50</td><td>중간 (특징 추출기 필요)</td><td>혼잡한 장면이나 가림이 잦은 환경에서의 강건한 다중 객체 추적</td></tr>
<tr><td><strong>샴 네트워크</strong></td><td>템플릿과 탐색 영역 간의 유사도 학습 및 정합 55</td><td>템플릿 매칭 기반으로 어느 정도 강건함. 심한 외형 변화에 취약</td><td>단일 객체 추적에 주로 사용. 다중 객체 추적 시 별도 관리 필요</td><td>낮음 (온라인 학습 불필요)</td><td>고속 단일 객체 추적 (Visual Object Tracking)</td></tr>
</tbody></table>
<p><em>표 2: 주요 추적 알고리즘 성능 비교</em></p>
<h2>4.  열화상 및 다중 모드 인식의 최신 연구 동향</h2>
<p>이 파트에서는 단일 모드 열화상 이미징의 근본적인 한계를 극복하기 위한 최첨단 연구 동향을 탐구합니다. 이 분야의 발전은 새로운 모델 아키텍처의 개발보다는, 데이터를 생성하고, 증강하며, 학습하는 혁신적인 ‘데이터 중심 AI(Data-centric AI)’ 접근법에 의해 주도되고 있습니다. 이는 열화상 데이터가 본질적으로 저해상도이며, 색상과 질감이 부족하고, 양적으로도 희소하다는 문제에 대한 직접적인 대응입니다. RGB-T 퓨전은 부족한 세부 정보를 보완하고, GAN은 더 많은 데이터를 생성하며, UDA와 SSL은 기존의 풍부한 RGB 데이터나 레이블 없는 열화상 데이터를 활용하는 방법을 제시합니다. 이러한 데이터 중심 전략들은 열화상 인식 기술의 성능을 한 단계 끌어올리는 핵심 동력으로 작용하고 있습니다.</p>
<h3>4.1  RGB-열화상(RGB-T) 센서 퓨전</h3>
<p>RGB와 열화상(TIR) 데이터를 융합하는 것은 각 센서의 상호 보완적인 강점을 결합하는 가장 직접적인 방법입니다. 즉, RGB의 풍부한 세부 묘사와 색상 정보, 그리고 열화상의 전천후 및 전천광(all-weather, all-lighting) 강건성을 통합하여 어느 한쪽만 사용하는 것보다 훨씬 더 강력한 인식 시스템을 구축하는 것입니다.5</p>
<p>초기 퓨전 방식은 단순히 픽셀 값을 평균 내는 수준이었지만, 정교한 딥러닝 접근법은 2-스트림(two-stream) CNN 구조를 사용하여 각 모달리티에서 독립적으로 특징을 추출한 후, 이를 다양한 단계(초기, 중간, 또는 후기)에서 융합합니다.61</p>
<h4>4.1.1  트랜스포머 기반 RGBT 추적</h4>
<p>CNN 기반 퓨전 방식은 두 모달리티 간의 전역적인 맥락(global context)이나 장거리 의존성(long-range dependencies)을 모델링하는 데 한계가 있습니다.66 최근 컴퓨터 비전 분야의 패러다임을 바꾸고 있는 트랜스포머(Transformer)와 그 핵심인 어텐션 메커니즘(attention mechanism)은 이러한 한계를 극복할 강력한 대안을 제시합니다. 트랜스포머는 이미지 내, 그리고 이미지 간의 전역적인 관계를 포착하는 데 탁월한 능력을 보입니다.67</p>
<p>최신 RGBT 추적 연구들은 트랜스포머를 백본으로 사용하여 RGB와 열화상 특징을 공동으로 추출, 융합, 그리고 관계 모델링까지 수행합니다.66 예를 들어, CSTNet과 같은 모델은 트랜스포머 블록 내부에 특화된 교차 모달 퓨전 모듈(cross-modal fusion module)을 직접 삽입하여 두 모달리티 간의 풍부한 상호작용을 유도합니다.67 이를 통해 모델은 열화상 이미지의 특정 ’핫스팟’과 RGB 이미지의 해당 영역에 있는 ‘질감 패턴’ 사이의 복잡한 상관관계를 스스로 학습할 수 있게 됩니다.</p>
<h3>4.2  데이터 부족 문제 극복</h3>
<p>대규모의 주석 달린 열화상 데이터셋 부재는 강력한 딥러닝 모델 훈련에 가장 큰 걸림돌입니다.19 이 문제를 해결하기 위해 데이터 자체를 생성하거나 증강하는 연구가 활발히 진행되고 있습니다.</p>
<h4>4.2.1  GAN을 이용한 열화상 이미지 합성</h4>
<p>생성적 적대 신경망(GAN, Generative Adversarial Networks)은 실제와 매우 유사한 이미지를 생성하는 데 뛰어난 성능을 보이는 딥러닝 모델입니다. GAN은 합성 이미지를 만드는 ’생성자(Generator)’와, 이 이미지가 진짜인지 가짜인지 판별하는 ’판별자(Discriminator)’가 서로 경쟁하며 학습하는 구조를 가집니다.71</p>
<p>연구자들은 이 GAN을 이미지-대-이미지 변환(image-to-image translation) 작업에 적용하여, 대규모로 존재하는 RGB 이미지를 그에 상응하는 그럴듯한 열화상 이미지로 변환하는 모델을 훈련시킵니다.20 COCO와 같이 방대한 양의 주석이 달린 RGB 데이터셋을 활용하면, 이 과정을 통해 자동으로 주석이 달린 대규모 합성 열화상 데이터셋을 생성할 수 있습니다.20 이는 부족한 실제 데이터를 보완하고 모델 훈련 과정을 효과적으로 지원하는 핵심적인 전략입니다.</p>
<h4>4.2.2  열화상 데이터 증강 기법</h4>
<p>단순한 기하학적 변환(좌우 반전, 회전 등)만으로는 열화상 데이터의 다양성을 충분히 확보하기 어렵습니다.75 열화상 이미지의 고유한 특성을 고려한 데이터 증강 기법이 필요합니다.</p>
<p>파라미터 기반 명암비 조절(parametric contrast stretching), 밝기 조절, 히스토그램 평활화(histogram equalization)와 같은 기법들은 열화상 이미지의 특성을 반영하여 더 현실적이고 도전적인 훈련 샘플을 만들어낼 수 있습니다.75 일부 연구에서는 한 걸음 더 나아가, 열 대비, 노이즈 수준과 같은 열화상 이미지 품질 지표를 사용하여 증강 과정을 안내합니다. 이를 통해 생성된 샘플이 실제 환경의 변화를 더 잘 반영하도록 하여 모델의 강건성과 일반화 성능을 향상시킵니다.75</p>
<h3>4.3  제한된 레이블을 활용한 학습</h3>
<h4>4.3.1  비지도 도메인 적응 (Unsupervised Domain Adaptation, UDA)</h4>
<p>UDA는 레이블이 풍부한 ‘소스 도메인’(예: RGB 이미지)에서 학습된 모델을, 레이블이 없는 ‘타겟 도메인’(예: 열화상 이미지)에서도 잘 작동하도록 적응시키는 기술입니다.77</p>
<p>가장 일반적인 기법 중 하나는 ’적대적 정렬(adversarial alignment)’입니다. 이 방법은 특징 추출기(feature extractor)와 도메인 판별자(domain discriminator)를 함께 사용합니다. 판별자는 주어진 특징 벡터가 소스 도메인에서 왔는지 타겟 도메인에서 왔는지 구별하도록 학습됩니다. 동시에, 특징 추출기는 판별자를 속이는, 즉 도메인에 무관한(domain-invariant) 특징을 생성하도록 학습됩니다.77 이 과정을 통해 모델은 두 도메인에 공통적으로 적용될 수 있는 일반적인 표현을 학습하게 됩니다. 더 나아가, 어텐션 메커니즘을 도입하여 색상 정보와 같이 특정 도메인에만 국한된 특징이 학습을 방해하는 ’부정적 전이(negative transfer)’를 줄이고, 두 도메인 간에 공유될 수 있는 유용한 특징(예: 형태 정보)에만 집중하여 전이 학습의 효율을 높이는 연구도 진행되고 있습니다.77</p>
<h4>4.3.2  자기 지도 학습 (Self-Supervised Learning, SSL)을 통한 열화상 표현 학습</h4>
<p>SSL은 레이블이 없는 대규모 데이터셋으로부터 모델 스스로 감독 신호(supervisory signal)를 만들어 유의미한 특징 표현을 학습하게 하는 패러다임입니다.82</p>
<p>대조 학습(Contrastive Learning)은 SSL의 대표적인 접근법 중 하나입니다. 이 방법은 동일한 이미지에 대해 서로 다른 증강(augmentation)을 적용한 ’긍정적 쌍(positive pair)’의 특징 표현은 서로 가깝게 만들고, 서로 다른 이미지에서 온 ’부정적 쌍(negative pair)’의 특징 표현은 서로 멀어지도록 모델을 학습시킵니다.83 이 원리를 RGB-열화상 이미지 쌍에 적용하면, 모델은 모달리티에 구애받지 않는 불변의 특징 표현을 학습할 수 있습니다.83</p>
<p>SSL은 대량의 레이블 없는 열화상 비디오 데이터를 쉽게 수집할 수 있다는 점에서 열화상 비전 분야에 엄청난 잠재력을 가집니다. 이를 통해 강력한 특징 추출기를 열화상 데이터로 직접 사전 훈련시킨 후, 소량의 레이블된 데이터로 특정 작업에 맞게 미세 조정(fine-tuning)하는 효율적인 학습 파이프라인을 구축할 수 있습니다.82</p>
<p>이러한 최신 연구 동향들은 하나의 공통된 방향을 가리킵니다. 바로 트랜스포머, 다중 모드 퓨전, 그리고 자기 지도 학습이 하나의 강력한 패러다임으로 수렴하고 있다는 것입니다. 미래의 최첨단 모델은 대규모의 레이블 없는 RGB-T 비디오 데이터셋을 통해 자기 지도 방식으로 사전 훈련된 거대한 트랜스포머 기반 모델이 될 가능성이 높습니다. 이러한 모델은 두 모달리티 간의 근본적인 상관관계를 깊이 이해하고, 소량의 레이블 데이터만으로 특정 작업에 매우 효과적으로 미세 조정될 수 있을 것입니다. 이는 현재보다 훨씬 높은 수준의 복잡성과 데이터 요구사항을 수반하지만, 성능과 강건성 면에서 비약적인 발전을 약속합니다.</p>
<h2>5.  이론에서 실제까지: 배포 및 현실적 고려사항</h2>
<p>이 마지막 파트에서는 이론적인 알고리즘을 현실 세계에 적용할 때 마주하게 되는 실질적인 도구, 하드웨어, 그리고 도전 과제들을 다룹니다. 성공적인 시스템 배포는 단순히 우수한 알고리즘을 선택하는 것을 넘어, 데이터셋, 하드웨어, 모델 아키텍처라는 세 가지 요소가 상호 유기적으로 작용하는 ’삼각대(Triad)’를 이해하고 최적화하는 과정입니다. 특정 데이터셋(예: FLIR ADAS)은 응용 분야(예: 자동차)를 정의하고, 타겟 하드웨어(예: Jetson Nano)는 모델의 복잡성에 대한 엄격한 제약을 부과하며, 이는 다시 경량 모델(예: YOLOv4-tiny)의 선택과 하드웨어 특화 최적화(예: TensorRT)를 강제합니다. 따라서 학술적 벤치마크에서 최고의 성능을 보이는 모델이 실제 배포 환경에서는 최적이 아닐 수 있으며, 이 세 요소의 동시적인 공동 설계(co-design)가 실용적인 성공의 핵심입니다.</p>
<h3>5.1  공개 열화상 및 RGBT 데이터셋 개요</h3>
<p>강력한 딥러닝 모델을 훈련하고 평가하기 위해서는 고품질의 데이터셋이 필수적입니다. 다음은 열화상 객체 인식 및 추적 연구에 널리 사용되는 주요 공개 데이터셋입니다.</p>
<table><thead><tr><th>데이터셋 이름</th><th>주요 초점</th><th>이미지 쌍/프레임</th><th>해상도</th><th>주석 상세</th><th>주요 해결 과제</th></tr></thead><tbody>
<tr><td><strong>FLIR ADAS</strong></td><td>자동차(ADAS)</td><td>26,000+ 프레임 (RGB+T) 60</td><td>640x512 (T), 가변 (RGB)</td><td>15개 클래스, MSCOCO JSON 형식 60</td><td>주/야간 ADAS, RGBT 퓨전 알고리즘 개발 5</td></tr>
<tr><td><strong>KAIST Multispectral</strong></td><td>보행자 탐지</td><td>95,000+ 쌍 (RGB+T) 87</td><td>640x480</td><td>보행자, 자전거 탑승자 등, 텍스트 형식</td><td>RGBT 보행자 탐지, 퓨전 기법 벤치마킹 88</td></tr>
<tr><td><strong>LLVIP</strong></td><td>저조도 환경</td><td>15,000+ 쌍 (RGB+T) 90</td><td>1280x1024</td><td>보행자, PASCAL VOC XML 형식</td><td>야간 환경에서의 퓨전 및 탐지 성능 평가 92</td></tr>
<tr><td><strong>C3I Thermal Automotive</strong></td><td>자동차(ADAS)</td><td>35,000+ 프레임 (T) 94</td><td>640x480</td><td>다수 클래스, YOLO 형식</td><td>저가형 열화상 센서를 이용한 스마트 인식 시스템 개발 94</td></tr>
</tbody></table>
<p><em>표 3: 주요 공개 열화상/RGBT 객체 인식 데이터셋</em></p>
<h3>5.2  엣지 배포를 위한 하드웨어 가속</h3>
<p>복잡한 딥러닝 모델을 실시간으로 추론하기 위해서는 하드웨어 가속이 필수적입니다. 특히 전력과 자원이 제한된 엣지 디바이스 환경에서는 더욱 그렇습니다.33</p>
<h4>5.2.1  GPU 기반 가속 (NVIDIA Jetson)</h4>
<p>NVIDIA Jetson 시리즈(Nano, TX2, Xavier 등)는 저전력 GPU를 탑재한 임베디드 플랫폼으로, 엣지 AI 분야에서 널리 사용됩니다.95 이러한 플랫폼에서의 가속은 주로 NVIDIA의 CUDA 툴킷과 TensorRT SDK를 통해 이루어집니다. TensorRT는 훈련된 모델을 추론에 최적화하는 라이브러리로, 레이어 융합, 정밀도 보정(예: 32비트 부동소수점(FP32)에서 FP16 또는 8비트 정수(INT8)로 변환), 하드웨어에 특화된 커널 선택 등의 최적화를 수행하여 상당한 속도 향상을 가져옵니다.96</p>
<h4>5.2.2  FPGA 기반 가속</h4>
<p>FPGA(Field-Programmable Gate Array)는 GPU의 대안으로, 진정한 하드웨어 병렬 처리를 통해 더 높은 전력 효율(GOPS/W)과 더 낮은 지연 시간(latency)을 제공할 수 있는 잠재력을 가집니다.100 FPGA 개발은 전통적으로 GPU보다 복잡했지만, Xilinx의 Vitis-AI와 같은 툴들이 등장하면서 DNN 모델을 FPGA에 배포하는 과정이 간소화되고 있습니다.101 특히 비디오 처리와 같이 데이터가 지속적으로 스트리밍되는 파이프라인 구조에 강점을 보입니다.102 일반적으로 FPGA는 GPU에 비해 전력 효율성에서 우위를 점하지만, 개발 곡선이 가파르고 개발 주기가 더 길다는 상충 관계가 존재합니다.95</p>
<h3>5.3  성능 벤치마킹 및 분석</h3>
<p>실제 하드웨어에서의 성능은 이론적인 계산량만으로는 예측하기 어렵습니다. 다음 표는 경량 모델을 임베디드 하드웨어에서 실행했을 때의 실제 성능을 보여주는 벤치마크 사례입니다. 이 데이터는 하드웨어별 최적화의 중요성을 명확히 보여줍니다. 예를 들어, YOLOv4-tiny 모델은 고성능 GPU에서는 수백 FPS를 기록하지만 104, 최적화되지 않은 상태의 Jetson Nano에서는 1-2 FPS에 그칠 수 있습니다.97 하지만 TensorRT와 같은 프레임워크로 최적화하면 동일한 하드웨어에서 30-40 FPS 수준까지 성능을 끌어올릴 수 있습니다.99</p>
<table><thead><tr><th>모델</th><th>하드웨어 플랫폼</th><th>최적화 프레임워크</th><th>입력 해상도</th><th>정밀도</th><th>보고된 FPS</th><th>보고된 정확도(mAP)</th><th>출처</th></tr></thead><tbody>
<tr><td>YOLOv4</td><td>Jetson Nano</td><td>TensorRT</td><td>608x608</td><td>FP16</td><td>1.77</td><td>48.4% (COCO)</td><td>99</td></tr>
<tr><td>YOLOv4</td><td>Jetson Nano</td><td>TensorRT</td><td>416x416</td><td>FP16</td><td>4.62</td><td>45.4% (COCO)</td><td>99</td></tr>
<tr><td>YOLOv4-tiny</td><td>Jetson Nano</td><td>TensorRT (tkDNN)</td><td>416x416</td><td>FP16</td><td>39</td><td>40.2% (COCO)</td><td>99</td></tr>
<tr><td>YOLOv4-tiny</td><td>Jetson TX2</td><td>Darknet (CUDNN)</td><td>416x416</td><td>-</td><td>8</td><td>-</td><td>97</td></tr>
<tr><td>YOLOv3</td><td>Jetson Nano</td><td>Darknet (CUDNN)</td><td>608x608</td><td>-</td><td>1.4</td><td>-</td><td>105</td></tr>
<tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
</tbody></table>
<p><em>표 4: 임베디드 하드웨어에서의 경량 모델 성능 벤치마크</em></p>
<p>주요 성능 지표로는 탐지 정확도를 나타내는 mAP(mean Average Precision), 추적 정확도를 나타내는 MOTA(Multiple Object Tracking Accuracy) 및 MOTP(Multiple Object Tracking Precision), 추적 ID의 안정성을 나타내는 ID Switches(IDs), 그리고 추론 속도를 나타내는 FPS가 사용됩니다.50</p>
<h3>5.4  주요 배포 과제 및 완화 전략</h3>
<p>학술 연구와 실제 배포 사이에는 상당한 격차가 존재하며, 이 격차는 주로 예측 불가능한 실패에 대한 강건성과 시스템 비용에 의해 정의됩니다. 학술 논문은 잘 정제된 벤치마크 데이터셋에서 최고의 성능을 달성하는 데 집중하는 반면 63, 실제 시스템은 센서 고장, 극단적인 환경 조건, 엄격한 비용 및 전력 예산과 같은 지저분하고 예측 불가능한 문제들을 해결해야 합니다.7</p>
<p>모달리티 데이터 누락 (Modality Missing)</p>
<p>RGBT 시스템에서 한쪽 센서가 고장 나거나 데이터 전송에 오류가 발생하는 것은 현실적인 실패 시나리오입니다.108 이를 완화하기 위한 전략으로는 단순히 마지막 프레임을 복사하는 간단한 방법부터, 가용한 모달리티 데이터를 기반으로 누락된 모달리티의 데이터를 그럴듯하게 생성해내는 역변환 가능 프롬프트 학습(invertible prompt learning)과 같은 고급 기법까지 다양합니다.108 이러한 연구는 실제 배포 환경에서는 매우 중요하지만, 잘 정제된 학술 벤치마크에서는 잘 다루어지지 않는 문제의 대표적인 예입니다.</p>
<p>환경적 도전 과제</p>
<p>열 평형 현상 외에도, 실제 추적기는 객체 가림, 배경 혼잡, 빠른 움직임 등 다양한 문제에 강건해야 합니다.46 DeepSORT와 같이 외형 정보를 활용하는 강건한 추적 알고리즘과 고급 퓨전 모델을 사용하는 것이 핵심적인 완화 전략입니다.</p>
<p>시스템 비용 및 복잡성</p>
<p>부품 비용은 점차 감소하고 있지만 7, 다중 센서 RGBT 시스템을 구축, 보정(calibration), 유지보수하는 것은 여전히 복잡하고 비용이 많이 듭니다. 특히 두 센서의 시야를 공간적으로, 그리고 캡처 시점을 시간적으로 정밀하게 일치시키는 것은 상당한 엔지니어링 노력을 요구하는 어려운 과제입니다.64</p>
<p>결론적으로, 2%의 정확도 손실이 있더라도 센서 고장과 같은 실제 문제 상황을 우아하게 처리할 수 있는 시스템이, 최고 성능을 자랑하지만 쉽게 실패하는 취약한 시스템보다 훨씬 더 가치가 있습니다. 이는 엔지니어링의 목표가 단순히 최고 성능 달성이 아니라, 신뢰성 확보에 있어야 함을 강조합니다.</p>
<h2>6.  결론 및 향후 전망</h2>
<h3>6.1  핵심 연구 결과 종합</h3>
<p>본 안내서는 시계열 열화상 영상을 이용한 객체 인식 및 추적 기술을 심층적으로 분석하였다. 분석을 통해 다음과 같은 핵심적인 결론을 도출할 수 있다.</p>
<ol>
<li><strong>근본적인 상충 관계와 센서 퓨전의 필연성</strong>: RGB 카메라는 객체의 ’정체성(what)’을 식별하는 데, 열화상 카메라는 ’존재(where)’를 감지하는 데 각각 강점을 가진다. 이러한 상호 보완적인 특성과 열 평형과 같은 단일 센서의 명백한 한계는, 강건한 인식 시스템 구축에 있어 RGB-T 센서 퓨전이 선택이 아닌 필수임을 시사한다.</li>
<li><strong>데이터 중심 AI 패러다임의 부상</strong>: 열화상 인식 기술의 발전은 새로운 모델 아키텍처 개발보다 데이터 희소성 문제를 해결하려는 데이터 중심 전략에 의해 주도되고 있다. GAN을 이용한 데이터 합성, UDA를 통한 지식 전이, SSL을 통한 비지도 학습 등은 제한된 데이터를 극복하고 모델 성능을 향상시키는 핵심 동력이다.</li>
<li><strong>기술의 융합</strong>: 트랜스포머, 다중 모드 퓨전, 자기 지도 학습은 개별 기술을 넘어 하나의 통합된 패러다임으로 수렴하고 있다. 대규모 비지도 다중 모드 데이터로 사전 훈련된 트랜스포머 모델은 미래의 최첨단 인식 시스템의 기반이 될 것이다.</li>
<li><strong>실용적 배포를 위한 삼각대 모델</strong>: 성공적인 시스템 배포는 데이터셋, 하드웨어, 모델 아키텍처라는 세 요소의 유기적인 균형에 달려 있다. 특정 응용 분야의 요구사항과 제약 조건 내에서 이 세 가지를 동시에 고려하는 공동 설계 접근법이 필수적이다.</li>
<li><strong>연구와 현실의 간극</strong>: 학술적 연구의 최고 성능과 실제 배포 환경의 요구사항 사이에는 상당한 격차가 존재한다. 이 간극은 센서 고장, 예측 불가능한 환경 변화, 비용 제약과 같은 현실적인 문제에 대한 강건성으로 정의된다. 따라서 미래 기술 개발은 신뢰성과 안정성 확보에 더 큰 중점을 두어야 한다.</li>
</ol>
<h3>6.2  미래 연구 및 개발 방향</h3>
<p>열화상 객체 인식 및 추적 기술은 다음과 같은 방향으로 더욱 발전할 것으로 전망된다.</p>
<ul>
<li><strong>차세대 센서 퓨전</strong>: 단순한 RGB-T 퓨전을 넘어, 3차원 공간 정보를 제공하는 라이다(LiDAR) 5, 동적 정보에 특화된 이벤트 카메라(event-based camera) 10, 그리고 기상 조건에 더욱 강건한 레이더(radar) 114 등 이종(heterogeneous) 센서들을 통합하는 연구가 가속화될 것이다. 이는 모든 조건에서 궁극적인 강건성을 확보하기 위한 방향이다.</li>
<li><strong>완전한 종단 간 학습 (End-to-End Learning)</strong>: 현재의 추적-탐지 프레임워크를 넘어, 탐지, 융합, 추적의 모든 단계를 하나의 거대한 트랜스포머 기반 모델 안에서 통합적으로 학습하는 완전한 종단 간 아키텍처가 주류가 될 것이다.61 이는 각 모듈 간의 최적화를 통합하여 전체 시스템 성능을 극대화할 수 있다.</li>
<li><strong>고도화된 자기 지도 학습</strong>: 다중 모드 시계열 데이터의 특성을 더 깊이 이해할 수 있는 정교한 SSL 사전 훈련 과제들이 개발될 것이다. 이는 객체의 외형, 움직임, 그리고 열적 특성 사이의 복잡한 시공간적 상관관계를 모델이 스스로 학습하게 하여, 전이 학습의 효율을 비약적으로 높일 것이다.82</li>
<li><strong>하드웨어-소프트웨어 공동 설계 (Hardware-Software Co-design)</strong>: 신경망 아키텍처 설계 단계에서부터 타겟 하드웨어(FPGA, 특정 GPU 등)의 특성을 고려하는 연구가 더욱 중요해질 것이다.14 예를 들어, 특정 하드웨어에서 매우 효율적으로 실행되는 연산자(operation)들로 구성된 네트워크를 설계함으로써, 전력 소비 대비 성능을 극대화하는 방향으로 발전할 것이다.</li>
</ul>
<h2>7. 참고 자료</h2>
<ol>
<li>열화상 카메라 vs 적외선 카메라: 차이점 및 응용 프로그램 이해 - Crate Club, accessed July 3, 2025, <a href="https://crateclub.com/ko/blogs/%EB%A1%9C%EB%93%9C%EC%95%84%EC%9B%83/thermal-vs-infrared-camera-understanding-the-differences-and-applications">https://crateclub.com/ko/blogs/%EB%A1%9C%EB%93%9C%EC%95%84%EC%9B%83/thermal-vs-infrared-camera-understanding-the-differences-and-applications</a></li>
<li>열 화상 카메라-배경, 원리, 장점, 단점, 활용, 전망 - 꿀팁전달자 - 티스토리, accessed July 3, 2025, <a href="https://lifehackdeliver.tistory.com/m/entry/%EC%97%B4-%ED%99%94%EC%83%81-%EC%B9%B4%EB%A9%94%EB%9D%BC-%EB%B0%B0%EA%B2%BD-%EC%9B%90%EB%A6%AC-%EC%9E%A5%EC%A0%90-%EB%8B%A8%EC%A0%90-%ED%99%9C%EC%9A%A9-%EC%A0%84%EB%A7%9D">https://lifehackdeliver.tistory.com/m/entry/%EC%97%B4-%ED%99%94%EC%83%81-%EC%B9%B4%EB%A9%94%EB%9D%BC-%EB%B0%B0%EA%B2%BD-%EC%9B%90%EB%A6%AC-%EC%9E%A5%EC%A0%90-%EB%8B%A8%EC%A0%90-%ED%99%9C%EC%9A%A9-%EC%A0%84%EB%A7%9D</a></li>
<li>열화상 카메라는 어떤 원리로 작동할까요? | Teledyne FLIR, accessed July 3, 2025, https://www.flirkorea.com/discover/rd-science/how-do-thermal-cameras-work/</li>
<li>Improving Object Detection in High-Altitude Infrared Thermal Images Using Magnitude-Based Pruning and Non-Maximum Suppression - MDPI, accessed July 3, 2025, https://www.mdpi.com/2313-433X/11/3/69</li>
<li>Thermal Datasets for ADAS Algorithm Training | Teledyne FLIR, accessed July 3, 2025, https://www.flir.com/oem/adas/dataset/</li>
<li>Object Detection in Thermal Spectrum for Advanced Driver-Assistance Systems (ADAS) - arXiv, accessed July 3, 2025, https://arxiv.org/pdf/2109.09854</li>
<li>열화상 카메라 전성시대! (2)활용분야 - 보안뉴스, accessed July 3, 2025, https://m.boannews.com/html/detail.html?idx=29937&amp;kind=3</li>
<li>FLIR – Application of Neural Networks with ADAS dataset Project Description, accessed July 3, 2025, <a href="https://courses.ece.ucsb.edu/ECE188/188_F18Ilan/Projects%20Offered/FLIR_ADAS-Neural-Network.pdf">https://courses.ece.ucsb.edu/ECE188/188_F18Ilan/Projects%20Offered/FLIR_ADAS-Neural-Network.pdf</a></li>
<li>엣지 디바이스용 실시간 열화상 객체 검출을 위한 YOLOv5기반 경량화 방법론, accessed July 3, 2025, https://www.kibme.org/resources/journal/20241002113223200.pdf</li>
<li>A Thermal Object Tracking Benchmark - DiVA portal, accessed July 3, 2025, http://www.diva-portal.org/smash/get/diva2:850688/FULLTEXT02.pdf</li>
<li>열화상 카메라의 원리 및 차량용 열화상 카메라의 장단점 - 자료실 - 루미솔, accessed July 3, 2025, <a href="http://www.lumisol.co.kr/sub/reference/lidar.asp?mode=view&amp;bid=4&amp;s_type&amp;s_keyword&amp;s_cate&amp;idx=1996&amp;page=1">http://www.lumisol.co.kr/sub/reference/lidar.asp?mode=view&amp;bid=4&amp;s_type=&amp;s_keyword=&amp;s_cate=&amp;idx=1996&amp;page=1</a></li>
<li>초소형 객체도 놓치지 않는 첨단 탐지 기술의 비밀, accessed July 3, 2025, https://changwontech.com/contents/transfers/903</li>
<li>Human Detection in Low Resolution Thermal Images Based on Combined HOG Classifier, accessed July 3, 2025, https://www.researchgate.net/publication/307965679_Human_Detection_in_Low_Resolution_Thermal_Images_Based_on_Combined_HOG_Classifier</li>
<li>Lightweight Thermal Super-Resolution and Object Detection for Robust Perception in Adverse Weather Conditions - CVF Open Access, accessed July 3, 2025, https://openaccess.thecvf.com/content/WACV2024/papers/Shyam_Lightweight_Thermal_Super-Resolution_and_Object_Detection_for_Robust_Perception_in_WACV_2024_paper.pdf</li>
<li>In a there are visible images that it is difficult to track the target… | Download Scientific Diagram - ResearchGate, accessed July 3, 2025, https://www.researchgate.net/figure/In-a-there-are-visible-images-that-it-is-difficult-to-track-the-target-in-low-light-and_fig1_366018393</li>
<li>Target Detection over the Diurnal Cycle Using a Multispectral Infrared Sensor - PMC, accessed July 3, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC5298629/</li>
<li>In Defense and Revival of Bayesian Filtering for Thermal Infrared Object Tracking - arXiv, accessed July 3, 2025, https://arxiv.org/html/2402.17098v1</li>
<li>Measured comparison of the crossover periods for mid- and long-wave IR (MWIR and LWIR) polarimetric and conventional thermal imagery - Optica Publishing Group, accessed July 3, 2025, https://opg.optica.org/oe/fulltext.cfm?uri=oe-18-15-15704</li>
<li>열적외선 영상을 사용한 미상 객체 검출 시스템 - RISS 검색 - 국내학술지논문 상세보기, accessed July 3, 2025, https://m.riss.kr/search/detail/DetailView.do?p_mat_type=1a0202e37d52c72d&amp;control_no=5f6329998fdea2bc7ecd42904f0c5d65</li>
<li>[2006.00821] Exploring Thermal Images for Object Detection in Underexposure Regions for Autonomous Driving - arXiv, accessed July 3, 2025, https://arxiv.org/abs/2006.00821</li>
<li>[2102.02005] Robust pedestrian detection in thermal imagery using synthesized images - arXiv, accessed July 3, 2025, https://arxiv.org/abs/2102.02005</li>
<li>Object Detection on Thermal Images: Performance of YOLOv4 Trained on Small Datasets., accessed July 3, 2025, https://www.esann.org/sites/default/files/proceedings/2021/ES2021-130.pdf</li>
<li>Energy-Efficient HOG-based Object Detection at 1080HD 60 fps with Multi-Scale Support, accessed July 3, 2025, https://eems.mit.edu/wp-content/uploads/2015/10/suleiman_sips_2014.pdf</li>
<li>A Lightweight Object Detection Algorithm for Remote Sensing Images Based on Attention Mechanism and YOLOv5s - MDPI, accessed July 3, 2025, https://www.mdpi.com/2072-4292/15/9/2429</li>
<li>Object Detection by Context and Boosted HOG-LBP - PASCAL VOC Challenge performance evaluation server, accessed July 3, 2025, http://host.robots.ox.ac.uk/pascal/VOC/voc2010/workshop/nlpr.pdf</li>
<li>Object Detection with Deep Learning: A Review - arXiv, accessed July 3, 2025, http://arxiv.org/pdf/1807.05511</li>
<li>인공지능을 적용한 실시간 적외선 이미지 객체 탐지 서비스 | capstone-2020-14, accessed July 3, 2025, https://kookmin-sw.github.io/capstone-2020-14/</li>
<li>Results on FLIR_ADAS Thermal Image dataset - ResearchGate, accessed July 3, 2025, https://www.researchgate.net/figure/Results-on-FLIR-ADAS-Thermal-Image-dataset_tbl2_351592409</li>
<li>[논문 리뷰] Deep Learning and Hybrid Approaches for Dynamic …, accessed July 3, 2025, https://www.themoonlight.io/ko/review/deep-learning-and-hybrid-approaches-for-dynamic-scene-analysis-object-detection-and-motion-tracking</li>
<li>[1904.06859] Pedestrian Detection in Thermal Images using Saliency Maps - arXiv, accessed July 3, 2025, https://arxiv.org/abs/1904.06859</li>
<li>Real-Time Infrared Object Detection by Visual Reasoning - CVF Open Access, accessed July 3, 2025, https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Gundogan_IR_Reasoner_Real-Time_Infrared_Object_Detection_by_Visual_Reasoning_CVPRW_2023_paper.pdf</li>
<li>딥 러닝 및 칼만 필터를 이용한 객체 추적 방법 - 한국방송/미디어공학회, accessed July 3, 2025, https://www.kibme.org/resources/journal/20190613143212078.pdf</li>
<li>Real-Time Object Detection and Recognition in Embedded Systems Using Open-Source Computer Vision Frameworks - ResearchGate, accessed July 3, 2025, https://www.researchgate.net/publication/390175520_Real-Time_Object_Detection_and_Recognition_in_Embedded_Systems_Using_Open-Source_Computer_Vision_Frameworks</li>
<li>(PDF) OBJECT DETECTION ALGORITHMS IMPLEMENTATION ON EMBEDDED DEVICES: CHALLENGES AND SUGGESTED SOLUTIONS - ResearchGate, accessed July 3, 2025, https://www.researchgate.net/publication/382834136_OBJECT_DETECTION_ALGORITHMS_IMPLEMENTATION_ON_EMBEDDED_DEVICES_CHALLENGES_AND_SUGGESTED_SOLUTIONS</li>
<li>Real-time object detection method for embedded devices - arXiv, accessed July 3, 2025, https://arxiv.org/pdf/2011.04244</li>
<li>[2506.23627] Brain Tumor Detection through Thermal Imaging and MobileNET - arXiv, accessed July 3, 2025, https://arxiv.org/abs/2506.23627</li>
<li>Brain Tumor Detection through Thermal Imaging and MobileNET - arXiv, accessed July 3, 2025, https://arxiv.org/html/2506.23627v1</li>
<li>A Thermal Infrared Pedestrian-Detection Method for Edge Computing Devices - PMC, accessed July 3, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC9460587/</li>
<li>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications - arXiv, accessed July 3, 2025, https://arxiv.org/abs/1704.04861</li>
<li>An Efficient Real-Time Object Detection Framework on Resource-Constricted Hardware Devices via Software and Hardware Co-design - arXiv, accessed July 3, 2025, https://arxiv.org/html/2408.01534v2</li>
<li>What is the Grad CAM method? - DataScientest, accessed July 3, 2025, https://datascientest.com/en/what-is-the-grad-cam-method</li>
<li>Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization - CVF Open Access, accessed July 3, 2025, https://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf</li>
<li>Grad-CAM: Visualize class activation maps with Keras, TensorFlow, and Deep Learning, accessed July 3, 2025, https://pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/</li>
<li>Visualization of the thermal images as inputs using Grad-CAM. - ResearchGate, accessed July 3, 2025, https://www.researchgate.net/figure/Visualization-of-the-thermal-images-as-inputs-using-Grad-CAM_fig12_362452474</li>
<li>Object Identification and Localization Using Grad-CAM++ with Mask Regional Convolution Neural Network - MDPI, accessed July 3, 2025, https://www.mdpi.com/2079-9292/10/13/1541</li>
<li>딥러닝 객체 추적: 알고리즘, 과제 및 응용 프로그램 - FlyPix AI, accessed July 3, 2025, https://flypix.ai/ko/blog/deep-learning-object-tracking/</li>
<li>Sort and Deep-SORT Based Multi-Object Tracking for Mobile Robotics: Evaluation with New Data Association Metrics - MDPI, accessed July 3, 2025, https://www.mdpi.com/2076-3417/12/3/1319</li>
<li>Part 3) 칼만 필터 응용 - Ch10. 영상 속의 물체 추적하기 - velog, accessed July 3, 2025, <a href="https://velog.io/@bbirong/Part-3-%EC%B9%BC%EB%A7%8C-%ED%95%84%ED%84%B0-%EC%9D%91%EC%9A%A9-Ch10.-%EC%98%81%EC%83%81-%EC%86%8D%EC%9D%98-%EB%AC%BC%EC%B2%B4-%EC%B6%94%EC%A0%81%ED%95%98%EA%B8%B0">https://velog.io/@bbirong/Part-3-%EC%B9%BC%EB%A7%8C-%ED%95%84%ED%84%B0-%EC%9D%91%EC%9A%A9-Ch10.-%EC%98%81%EC%83%81-%EC%86%8D%EC%9D%98-%EB%AC%BC%EC%B2%B4-%EC%B6%94%EC%A0%81%ED%95%98%EA%B8%B0</a></li>
<li>Deep SORT: Realtime Object Tracking Guide - Ikomia, accessed July 3, 2025, https://www.ikomia.ai/blog/deep-sort-object-tracking-guide</li>
<li>Comparative Evaluation of SORT, DeepSORT, and ByteTrack for Multiple Object Tracking in Highway Videos, accessed July 3, 2025, https://vectoral.org/index.php/IJSICS/article/view/97/89</li>
<li>Quadratic Kalman Filter Object Tracking with Moving Pictures - Korea Science, accessed July 3, 2025, https://koreascience.kr/article/JAKO201609757435979.page</li>
<li>Object Tracking Using Particle Filters in Moving Camera - SciSpace, accessed July 3, 2025, https://scispace.com/pdf/object-tracking-using-particle-filters-in-moving-camera-1t6gyphkus.pdf</li>
<li>파티클 필터를 이용한 다중객체의 움직임 환경에서 특정 객체의 움직임 추적 - 와이드경제, accessed July 3, 2025, https://www.widedaily.com/news/articleView.html?idxno=1350</li>
<li>SORT vs DeepSORT : r/computervision - Reddit, accessed July 3, 2025, https://www.reddit.com/r/computervision/comments/11mqk8c/sort_vs_deepsort/</li>
<li>A Twofold Siamese Network for Real-Time Object Tracking - CVF Open Access, accessed July 3, 2025, https://openaccess.thecvf.com/content_cvpr_2018/papers/He_A_Twofold_Siamese_CVPR_2018_paper.pdf</li>
<li>Online Siamese Network for Visual Object Tracking - MDPI, accessed July 3, 2025, https://www.mdpi.com/1424-8220/19/8/1858</li>
<li>Real-time object tracking in the wild with Siamese network, accessed July 3, 2025, https://cs.nju.edu.cn/rinc/publish/download/2023/2023_4.pdf</li>
<li>Learning Dynamic Siamese Network for Visual Object Tracking - Qing Guo, accessed July 3, 2025, https://tsingqguo.github.io/dsiam.html</li>
<li>Learning Dynamic Siamese Network for Visual Object Tracking, accessed July 3, 2025, http://www.ansuan.net/faculty/lwan/paper/siamese_tracking17/Guo_Learning_Dynamic_Siamese_ICCV_2017_paper.pdf</li>
<li>FREE Teledyne FLIR Thermal Dataset for Algorithm Training, accessed July 3, 2025, https://www.flir.com/oem/adas/adas-dataset-form/</li>
<li>A Survey for Deep RGBT Tracking - arXiv, accessed July 3, 2025, https://arxiv.org/pdf/2201.09296</li>
<li>Adopting the YOLOv4 Architecture for Low-Latency Multispectral Pedestrian Detection in Autonomous Driving - PubMed Central, accessed July 3, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC8837921/</li>
<li>Rgb-T Tracking - Papers With Code, accessed July 3, 2025, https://paperswithcode.com/task/rgb-t-tracking</li>
<li>RGBT Salient Object Detection: A Large-scale Dataset and Benchmark - arXiv, accessed July 3, 2025, https://arxiv.org/pdf/2007.03262</li>
<li>Use of Thermal Imagery for Robust Moving Object Detection - DiVA portal, accessed July 3, 2025, https://www.diva-portal.org/smash/get/diva2:1578091/FULLTEXT01.pdf</li>
<li>Transformer RGBT Tracking with Spatio-Temporal Multimodal Tokens - arXiv, accessed July 3, 2025, https://arxiv.org/html/2401.01674v1</li>
<li>Transformer-based RGB-T Tracking with Channel and Spatial Feature Fusion - arXiv, accessed July 3, 2025, https://arxiv.org/html/2405.03177v3</li>
<li>RGB-T Tracking Based on Mixed Attention - arXiv, accessed July 3, 2025, https://arxiv.org/pdf/2304.04264</li>
<li>Transformer-based RGB-T Tracking with Channel and Spatial Feature Fusion - arXiv, accessed July 3, 2025, https://arxiv.org/html/2405.03177v1</li>
<li>[2405.03177] Transformer-based RGB-T Tracking with Channel and Spatial Feature Fusion, accessed July 3, 2025, https://arxiv.org/abs/2405.03177</li>
<li>A Survey of Image Synthesis and Editing with Generative Adversarial Networks - Tsinghua Graphics and Geometric Computing Group, accessed July 3, 2025, https://cg.cs.tsinghua.edu.cn/people/~kun/papers/gan_survey_final.pdf</li>
<li>A METHOD FOR SYNTHESIZING THERMAL IMAGES USING GAN MULTI-LAYERED APPROACH - Semantic Scholar, accessed July 3, 2025, https://pdfs.semanticscholar.org/0481/aaee7128d20a88a2bd6cdb78cf6fcaf62864.pdf</li>
<li>Image synthesis with adversarial networks: A comprehensive survey and case studies, accessed July 3, 2025, https://faculty.uca.edu/ecelebi/documents/IF_2021a.pdf</li>
<li>Fully Automated DCNN-Based Thermal Images Annotation Using Neural Network Pretrained on RGB Data - MDPI, accessed July 3, 2025, https://www.mdpi.com/1424-8220/21/4/1552</li>
<li>Enhancing Thermal Image Classification with Novel Quality Metric-Based Augmentation Techniques - ResearchGate, accessed July 3, 2025, https://www.researchgate.net/publication/388076791_Enhancing_Thermal_Image_Classification_with_Novel_Quality_Metric-Based_Augmentation_Techniques</li>
<li>Enhancing Thermal Image Classification with Novel Quality Metric-Based Augmentation Techniques | Mathematical Problems of Computer Science, accessed July 3, 2025, https://mpcs.sci.am/index.php/mpcs/article/view/865</li>
<li>Unsupervised RGB-to-Thermal Domain Adaptation via Multi-Domain Attention Network, accessed July 3, 2025, https://www.researchgate.net/publication/364522558_Unsupervised_RGB-to-Thermal_Domain_Adaptation_via_Multi-Domain_Attention_Network</li>
<li>[2210.04367] Unsupervised RGB-to-Thermal Domain Adaptation via Multi-Domain Attention Network - arXiv, accessed July 3, 2025, https://arxiv.org/abs/2210.04367</li>
<li>ShadowSense: Unsupervised Domain Adaptation and Feature Fusion for Shadow-Agnostic Tree Crown Detection From RGB-Thermal Drone I, accessed July 3, 2025, https://openaccess.thecvf.com/content/WACV2024/papers/Kapil_ShadowSense_Unsupervised_Domain_Adaptation_and_Feature_Fusion_for_Shadow-Agnostic_Tree_WACV_2024_paper.pdf</li>
<li>Unsupervised Domain Adaptation for One-stage Object Detector using Offsets to Bounding Box - European Computer Vision Association, accessed July 3, 2025, https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136930679.pdf</li>
<li>Self-Training Guided Adversarial Domain Adaptation for Thermal Imagery - CVF Open Access, accessed July 3, 2025, https://openaccess.thecvf.com/content/CVPR2021W/PBVS/papers/Akkaya_Self-Training_Guided_Adversarial_Domain_Adaptation_for_Thermal_Imagery_CVPRW_2021_paper.pdf</li>
<li>[2410.07442] Self-Supervised Learning for Real-World Object Detection: a Survey - arXiv, accessed July 3, 2025, https://arxiv.org/abs/2410.07442</li>
<li>SSTN: Self-Supervised Domain Adaptation Thermal Object Detection for Autonomous Driving - arXiv, accessed July 3, 2025, https://arxiv.org/pdf/2103.03150</li>
<li>[2201.04387] Maximizing Self-supervision from Thermal Image for Effective Self-supervised Learning of Depth and Ego-motion - arXiv, accessed July 3, 2025, https://arxiv.org/abs/2201.04387</li>
<li>Teledyne FLIR ADAS Thermal Dataset v2 - Kaggle, accessed July 3, 2025, https://www.kaggle.com/datasets/samdazel/teledyne-flir-adas-thermal-dataset-v2</li>
<li>Fully Automated DCNN-Based Thermal Images Annotation Using Neural Network Pretrained on RGB Data - PubMed Central, accessed July 3, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC7926581/</li>
<li>CalayZhou/Multispectral-Pedestrian-Detection-Resource - GitHub, accessed July 3, 2025, https://github.com/CalayZhou/Multispectral-Pedestrian-Detection-Resource</li>
<li>KAIST Multispectral Pedestrian Detection Benchmark - Soonmin Hwang, accessed July 3, 2025, https://soonminhwang.github.io/rgbt-ped-detection/</li>
<li>KAIST Multispectral Pedestrian Detection Benchmark Dataset - Papers With Code, accessed July 3, 2025, https://paperswithcode.com/dataset/kaist-multispectral-pedestrian-detection</li>
<li>LLVIP: A Visible-infrared Paired Dataset for Low-light Vision - GitHub Pages, accessed July 3, 2025, https://bupt-ai-cz.github.io/LLVIP/</li>
<li>LLVIP: A Visible-infrared Paired Dataset for Low-light Vision - Emergent Mind, accessed July 3, 2025, https://www.emergentmind.com/articles/2108.10831</li>
<li>Three low-light vision tasks our LLVIP dataset can apply to. Task 1 - ResearchGate, accessed July 3, 2025, https://www.researchgate.net/figure/Three-low-light-vision-tasks-our-LLVIP-dataset-can-apply-to-Task-1-Image-fusion-Task_fig3_354115460</li>
<li>LLVIP: A Visible-infrared Paired Dataset for Low-light Vision | Papers With Code, accessed July 3, 2025, https://paperswithcode.com/paper/llvip-a-visible-infrared-paired-dataset-for</li>
<li>Thermal Imaging Datasets - University of Galway, accessed July 3, 2025, https://www.universityofgalway.ie/c3i/datasets/thermal/</li>
<li>Implementation of Thermal Event Image Processing Algorithms on NVIDIA Tegra Jetson TX2 Embedded System-on-a-Chip - ResearchGate, accessed July 3, 2025, https://www.researchgate.net/publication/353403392_Implementation_of_Thermal_Event_Image_Processing_Algorithms_on_NVIDIA_Tegra_Jetson_TX2_Embedded_System-on-a-Chip</li>
<li>Benchmarking Deep Learning Models on NVIDIA Jetson Nano for Real-Time Systems: An Empirical Investigation - arXiv, accessed July 3, 2025, https://arxiv.org/html/2406.17749v1</li>
<li>YOLOv4 on Jetson Nano - JK Jung’s blog, accessed July 3, 2025, https://jkjung-avt.github.io/yolov4/</li>
<li>Benchmarking YoloV4 Models on an Nvidia Jetson Xavier NX | by Jan Nitschke - ML6 blog, accessed July 3, 2025, https://blog.ml6.eu/benchmarking-yolov4-models-on-an-nvidia-jetson-xavier-nx-353e6d49dc1f</li>
<li>Running YOloV4 on jetson Nano at Higher FPS? - NVIDIA Developer Forums, accessed July 3, 2025, https://forums.developer.nvidia.com/t/running-yolov4-on-jetson-nano-at-higher-fps/123469</li>
<li>FPGA Implementation of a Deep Learning Acceleration Core Architecture for Image Target Detection - MDPI, accessed July 3, 2025, https://www.mdpi.com/2076-3417/13/7/4144</li>
<li>FPGA Implementation of Object Detection Accelerator Based on Vitis-AI - ResearchGate, accessed July 3, 2025, https://www.researchgate.net/publication/352126944_FPGA_Implementation_of_Object_Detection_Accelerator_Based_on_Vitis-AI</li>
<li>FPGA for Hardware Acceleration - Reddit, accessed July 3, 2025, https://www.reddit.com/r/FPGA/comments/i81sko/fpga_for_hardware_acceleration/</li>
<li>Implementation of Thermal Event Image Processing Algorithms on NVIDIA Tegra Jetson TX2 Embedded System-on-a-Chip - MDPI, accessed July 3, 2025, https://www.mdpi.com/1996-1073/14/15/4416</li>
<li>YOLOv4-tiny released: 40.2% AP50, 371 FPS (GTX 1080 Ti) / Issue #2201 / pjreddie/darknet, accessed July 3, 2025, https://github.com/pjreddie/darknet/issues/2201</li>
<li>Inference performance of YOLOv4 and YOLOv4-tiny with S = 416 runs on Jetson Nano. - ResearchGate, accessed July 3, 2025, https://www.researchgate.net/figure/Inference-performance-of-YOLOv4-and-YOLOv4-tiny-with-S-416-runs-on-Jetson-Nano_tbl6_359689579</li>
<li>딥러닝대회_열화상 이미지 객체탐지 경진대회(입상) - Zeus Data Note - 티스토리, accessed July 3, 2025, https://zeuskwon-ds.tistory.com/53</li>
<li>DuSiamIE: A Lightweight Multidimensional Infrared-Enhanced RGBT Tracking Algorithm for Edge Device Deployment - MDPI, accessed July 3, 2025, https://www.mdpi.com/2079-9292/13/23/4721</li>
<li>Modality-missing RGBT Tracking via Invertible Prompt Learning and A High-quality Data Simulation Method - arXiv, accessed July 3, 2025, https://arxiv.org/html/2312.16244v1</li>
<li>Modality-missing RGBT Tracking: Invertible Prompt Learning and High-quality Benchmarks - arXiv, accessed July 3, 2025, https://arxiv.org/html/2312.16244v4</li>
<li>[논문 리뷰] Object Detection and Tracking, accessed July 3, 2025, https://www.themoonlight.io/ko/review/object-detection-and-tracking</li>
<li>A Comprehensive Review of RGBT Tracking | Request PDF - ResearchGate, accessed July 3, 2025, https://www.researchgate.net/publication/382762309_A_Comprehensive_Review_of_RGBT_Tracking</li>
<li>Attribute-Based Progressive Fusion Network for RGBT Tracking | Proceedings of the AAAI Conference on Artificial Intelligence, accessed July 3, 2025, https://ojs.aaai.org/index.php/AAAI/article/view/20187</li>
<li>RISS 검색 - 학위논문 상세보기, accessed July 3, 2025, https://m.riss.kr/search/detail/DetailView.do?p_mat_type=be54d9b8bc7cdb09&amp;control_no=f014ac8b905632deffe0bdc3ef48d419</li>
<li>Vehicle Detection and Tracking Using Thermal Cameras in Adverse Visibility Conditions, accessed July 3, 2025, https://www.mdpi.com/1424-8220/22/12/4567</li>
<li>Deploying AI Object Detection, Target Tracking and Computational Imaging Algorithms on Embedded Processors - FLIR, accessed July 3, 2025, https://www.flir.com/discover/cores-components/deploying-ai-object-detection–target-tracking-and-computational–imaging-algorithms-on-embedded-processors/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>