<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:AD-H (Autonomous Driving with Hierarchical agents, 2024-06-05)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>AD-H (Autonomous Driving with Hierarchical agents, 2024-06-05)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">자율주행 (Autonomous Driving)</a> / <span>AD-H (Autonomous Driving with Hierarchical agents, 2024-06-05)</span></nav>
                </div>
            </header>
            <article>
                <h1>AD-H (Autonomous Driving with Hierarchical agents, 2024-06-05)</h1>
<h2>1.  서론 (Introduction)</h2>
<p>현대 자율 주행(Autonomous Driving, AD) 기술은 기계적 제어와 규칙 기반(Rule-based) 알고리즘의 시대를 넘어, 데이터 중심의 인공지능(AI) 모델로 급격히 진화하고 있다. 초기 자율 주행 시스템은 인지(Perception), 예측(Prediction), 계획(Planning), 제어(Control)가 명확히 분리된 모듈형 파이프라인(Modular Pipeline)에 의존하였다. 이러한 방식은 각 모듈의 해석 가능성을 보장한다는 장점이 있으나, 모듈 간의 정보 손실과 오류 전파(Error Propagation)라는 구조적 한계를 지니고 있었다. 이에 대한 대안으로 등장한 종단간(End-to-End) 자율 주행 방식은 센서 입력에서 제어 신호 출력까지를 하나의 신경망으로 연결하여 최적화함으로써 주행 성능을 비약적으로 향상시켰다.</p>
<p>최근 거대 언어 모델(Large Language Models, LLM)과 멀티모달 거대 언어 모델(Multimodal Large Language Models, MLLM)의 등장은 자율 주행 분야에 또 다른 패러다임 전환을 예고하고 있다. LLM의 방대한 지식과 추론 능력은 자율 주행 차량이 복잡한 교통 법규를 이해하고, 비정형적인 도로 상황에서 인간과 유사한 상식적 판단(Common Sense Reasoning)을 내릴 수 있는 가능성을 열어주었다. 그러나 기존의 MLLM 기반 연구들은 고차원적인 언어 명령을 차량의 조향각이나 가속도와 같은 저차원 제어 신호로 직접 변환(Direct Translation)하는 방식을 취했다. 이러한 접근은 MLLM이 가진 언어 생성 능력을 수치적 회귀(Regression) 문제로 격하시킴으로써 모델의 잠재력을 온전히 활용하지 못하게 하며, 학습 데이터에 존재하지 않는 상황에 대한 일반화 능력을 저해하는 요인이 되었다.1</p>
<p>본 보고서에서 심층적으로 다루는 **AD-H(Autonomous Driving with Hierarchical agents)**는 이러한 한계를 극복하기 위해 제안된 혁신적인 프레임워크이다. AD-H는 인간의 인지 과정이 고차원적인 사고와 저차원적인 반사 행동으로 나뉘어 있다는 점에 착안하여, 자율 주행 시스템을 ’계획(Planner)’과 ’제어(Controller)’라는 두 개의 독립적이지만 상호 협력적인 에이전트로 분리하였다.2 본 연구 보고서는 AD-H의 이론적 배경, 시스템 아키텍처, 데이터셋 구축 방법론, 그리고 시뮬레이션 환경에서의 성능 평가 결과를 망라하여 분석한다. 특히, 본 보고서는 단순한 사실의 나열을 넘어, AD-H가 제시하는 ’중간 단계 언어 명령(Mid-level language-driven commands)’의 전략적 가치와 이것이 자율 주행의 해석 가능성 및 일반화 성능에 미치는 영향을 다각도로 조명한다.</p>
<h2>2.  이론적 배경 및 문제의식 (Theoretical Background &amp; Problem Statement)</h2>
<h3>2.1  자율 주행 패러다임의 진화와 한계</h3>
<p>자율 주행 기술은 크게 세 가지 흐름으로 발전해 왔다. 첫째, <strong>모듈형 접근</strong>은 시스템을 세분화하여 관리하기 용이하나, 복잡한 도심 환경에서의 유연성이 부족하다. 둘째, **종단간 학습(End-to-End Learning)**은 데이터 효율성을 극대화했으나, ‘블랙박스(Black-box)’ 특성으로 인해 사고 발생 시 원인 규명이 어렵고 모델의 결정을 신뢰하기 어렵다는 문제가 지속적으로 제기되었다.4</p>
<p>셋째, 최근 부상한 <strong>VLA(Vision-Language-Action) 모델</strong>은 시각 정보와 언어 명령을 통합하여 행동을 생성한다. LMDrive와 같은 선도적인 연구들은 MLLM을 활용하여 자연어 명령을 이해하고 주행하는 폐루프(Closed-loop) 시스템을 구현하였다.5 그러나 이들 연구의 대부분은 MLLM에게 고차원 명령(예: “교차로에서 좌회전해”)을 입력받아 즉각적인 제어 신호(예: 조향각 0.5, 가속 0.2)를 출력하도록 요구한다.</p>
<p>이러한 <strong>‘직접 변환(Direct Mapping)’</strong> 방식은 다음과 같은 근본적인 문제를 내포한다:</p>
<ol>
<li><strong>추론 능력의 낭비:</strong> MLLM은 텍스트 기반의 논리적 추론에 특화되어 있다. 이를 단순한 연속적인 수치 값 예측에 사용하는 것은 모델의 거대한 파라미터가 가진 창발적 능력(Emergent Capability)을 낭비하는 것이다.1</li>
<li><strong>데이터 편향 및 과적합:</strong> 제어 신호는 차량의 물리적 특성과 도로 환경에 매우 민감하다. 따라서 특정 데이터셋에 과적합(Overfitting)되기 쉽고, 훈련 데이터에 없는 새로운 시나리오(Unseen Scenario)나 긴 시계열의 명령(Long-horizon Instruction)에 취약하다.5</li>
<li><strong>설명 불가능성:</strong> 모델이 왜 특정 조향각을 선택했는지 언어적으로 설명할 수 없으므로, 시스템의 투명성이 보장되지 않는다.</li>
</ol>
<h3>2.2  계층적 제어 이론과 이중 시스템 이론</h3>
<p>AD-H는 인지 심리학의 **‘이중 시스템 이론(Dual-process theory)’**과 로보틱스의 **계층적 제어(Hierarchical Control)**에서 영감을 받았다. 인간은 운전할 때 목적지 설정이나 복잡한 교차로 판단과 같은 ’느리지만 논리적인 사고(System 2)’와, 차선 유지나 장애물 회피와 같은 ’빠르고 반사적인 행동(System 1)’을 동시에 수행한다.</p>
<p>AD-H는 이를 모사하여 <strong>MLLM 플래너</strong>가 System 2의 역할을, <strong>경량 컨트롤러</strong>가 System 1의 역할을 수행하도록 설계되었다. 이러한 구조적 분리는 각 에이전트가 자신의 강점에 집중하게 함으로써 전체 시스템의 효율성과 성능을 극대화한다.7</p>
<h2>3.  AD-H 시스템 아키텍처 (System Architecture)</h2>
<p>AD-H의 핵심은 고차원 명령과 저차원 제어 신호 사이를 연결하는 **‘중간 단계 언어 명령(Mid-level language-driven commands)’**의 도입이다. 이 시스템은 크게 MLLM 플래너, 중간 단계 인터페이스, 그리고 경량 컨트롤러의 세 부분으로 구성된다.</p>
<h3>3.1  MLLM 플래너: 고차원 추론의 중심 (The MLLM Planner)</h3>
<p>MLLM 플래너는 AD-H 시스템의 ’두뇌’에 해당한다. 이 모듈은 주변 환경에 대한 시각적 정보와 사용자의 고차원 명령을 입력받아, 현재 상황에 필요한 구체적인 주행 전략을 수립한다.</p>
<ul>
<li><strong>백본 모델(Backbone Model):</strong> 연구진은 LLaVA-7B(Large Language and Vision Assistant)와 Mipha-3B와 같은 고성능 멀티모달 모델을 플래너로 채택하였다.6 이 모델들은 이미지 인코더(Vision Encoder)를 통해 입력된 서라운드 뷰(Surround-view) 이미지를 텍스트 임베딩 공간으로 투영(Project)하여 이해한다.</li>
<li><strong>추론 과정:</strong> 플래너는 단순히 명령을 전달받는 수동적 존재가 아니다. “다음 교차로에서 좌회전하라“는 명령이 입력되면, 플래너는 신호등의 상태, 주변 차량의 유무, 보행자의 움직임 등을 종합적으로 고려하여 “전방에 적색 신호가 있으므로 감속하며 정지선에 멈출 준비를 하라“는 식의 구체적이고 맥락적인 판단을 내린다.5</li>
</ul>
<h3>3.2  중간 단계 언어 명령: 가교의 역할 (Mid-Level Language-Driven Commands)</h3>
<p>AD-H의 가장 독창적인 기여는 MLLM 플래너와 컨트롤러 사이의 소통 방식에 있다. 기존 방식이 벡터(Vector) 형태의 잠재 표현(Latent Representation)을 공유했다면, AD-H는 자연어(Natural Language) 기반의 중간 명령을 사용한다.</p>
<ul>
<li><strong>정의 및 특징:</strong> 중간 단계 명령은 고차원 명령보다는 세밀(Fine-grained)하지만, 제어 신호보다는 추상적이고 범용적이다. 이는 인간 운전자가 머릿속으로 생각하는 “속도를 조금 줄이면서 오른쪽으로 붙자“와 같은 내적 독백(Inner Monologue)과 유사하다.1</li>
<li><strong>구조적 예시:</strong></li>
<li><strong>입력:</strong> “다음 교차로에서 좌회전.” (High-level)</li>
<li><strong>중간 명령:</strong> “교차로에 접근 중이다. 교통 법규 준수를 위해 속도를 줄여라. 안전을 확보한 후 완만하게 좌회전하라.” (Mid-level).5</li>
<li><strong>효과:</strong> 이러한 언어적 매개체는 두 가지 이점을 제공한다. 첫째, **설명 가능성(Explainability)**이다. 시스템이 어떤 의도로 움직이는지 인간이 직관적으로 이해할 수 있다. 둘째, **일반화(Generalization)**이다. 다양한 도로 환경에서도 “속도를 줄여라“라는 언어적 지침은 동일하게 적용될 수 있어, 특정 픽셀 패턴에 과적합되는 것을 방지한다.2</li>
</ul>
<h3>3.3  경량 컨트롤러: 실행의 주체 (Lightweight Controller)</h3>
<p>컨트롤러는 플래너의 지시를 물리적인 차량 제어로 변환하는 ‘신체’ 역할을 수행한다.</p>
<ul>
<li><strong>모델 구성:</strong> MLLM 플래너에 비해 훨씬 작은 파라미터(예: OPT-350M 기반)를 가진 경량 모델이나 Transformer 기반 구조를 사용한다.3 이는 빠른 반응 속도를 보장하기 위함이다.</li>
<li><strong>동작 원리:</strong> 컨트롤러는 플래너가 생성한 중간 단계 명령의 텍스트 임베딩과 현재의 시각적 특징(Visual Features)을 결합하여, 차량이 따라가야 할 미래의 경로점(Waypoints)을 예측한다. 예측된 경로점은 PID 제어기(Proportional-Integral-Derivative Controller)를 통해 최종적인 가속(Throttle), 제동(Brake), 조향(Steering) 신호로 변환된다.9</li>
</ul>
<h2>4.  데이터셋 구축 및 학습 방법론 (Dataset &amp; Methodology)</h2>
<p>AD-H와 같은 계층적 모델을 학습시키기 위해서는 고차원 명령, 중간 단계 명령, 그리고 저차원 행동이 모두 포함된 데이터셋이 필수적이다.</p>
<h3>4.1  계층적 행동 주석 데이터셋 (Hierarchical Action Annotation Dataset)</h3>
<p>기존의 자율 주행 데이터셋(예: NuScenes, Waymo)은 주로 센서 데이터와 차량의 궤적 정보만을 포함하고 있어, 언어적 추론을 학습시키기에 부족함이 있었다. 이를 해결하기 위해 AD-H 연구진은 대규모의 새로운 데이터셋을 구축하였다.</p>
<ul>
<li><strong>규모:</strong> 총 1,753,000개(약 175만 개)의 주석(Annotation)을 포함한다.6</li>
<li><strong>구성:</strong></li>
</ul>
<ol>
<li><strong>센서 데이터:</strong> 4개 방향(전, 후, 좌, 우)의 RGB 카메라 이미지와 라이다(LiDAR) 포인트 클라우드 데이터.</li>
<li><strong>명령 계층:</strong></li>
</ol>
<ul>
<li><strong>고차원 명령:</strong> 내비게이션 시스템에서 제공하는 경로 안내 (예: “Turn Left”).</li>
<li><strong>중간 단계 명령:</strong> 상황 인지 및 구체적 행동 지침을 포함하는 자연어 문장. 이는 전문가의 운전 방식이나 규칙 기반 알고리즘의 결정을 언어화하여 생성된 것으로 추정된다.3</li>
<li><strong>저차원 궤적:</strong> 실제 차량의 이동 경로(Waypoints) 및 제어 신호.</li>
<li><strong>의의:</strong> 이 데이터셋은 단순히 입력과 출력의 쌍을 넘어, ’입력-사고과정-출력’이라는 연쇄적 학습을 가능하게 함으로써 MLLM의 추론 능력을 극대화하는 기반이 된다.6</li>
</ul>
<h3>4.2  학습 파이프라인 (Training Pipeline)</h3>
<p>AD-H의 학습은 두 단계로 나뉘어 진행된다.</p>
<ol>
<li><strong>1단계: MLLM 플래너 학습:</strong> 시각적 입력과 고차원 명령이 주어졌을 때, 적절한 중간 단계 명령을 생성하도록 학습시킨다. 이때 대규모 언어 모델의 사전 학습된 지식을 활용하여 언어 생성의 자연스러움과 논리성을 유지한다.</li>
<li><strong>2단계: 컨트롤러 학습:</strong> 생성된 중간 단계 명령을 입력받아 정확한 경로점을 예측하도록 경량 모델을 학습시킨다. 이 과정에서 컨트롤러는 언어 명령을 물리적 공간의 좌표로 해석하는 능력(Grounding)을 배운다.</li>
</ol>
<h2>5.  실험 및 성능 평가 (Experiments &amp; Evaluation)</h2>
<p>AD-H의 성능 검증은 자율 주행 시뮬레이터인 CARLA(Car Learning to Act) 환경에서 수행되었다. 이는 폐루프(Closed-loop) 평가를 통해 모델이 실제 주행 상황에서 환경과 상호작용하며 발생하는 누적 오차를 얼마나 잘 제어하는지 확인하기 위함이다.</p>
<h3>5.1  실험 환경 및 벤치마크 (Experimental Setup)</h3>
<ul>
<li><strong>시뮬레이터:</strong> CARLA 0.9.10 버전을 사용하며, Town01부터 Town08까지 다양한 도시 환경을 포함한다.</li>
<li><strong>벤치마크:</strong> <strong>LangAuto</strong> 벤치마크를 주력으로 사용하였다. 이는 언어 명령 기반의 자율 주행 성능을 평가하기 위해 설계된 표준화된 테스트 셋이다. 세부적으로는 LangAuto, LangAuto-Short(단거리), LangAuto-Tiny(초단거리), 그리고 LangAuto-Novel-Environment(학습하지 않은 새로운 환경)로 구성된다.6</li>
<li><strong>평가 지표 (Metrics):</strong></li>
<li><strong>Driving Score (DS):</strong> 경로 완주율과 페널티(충돌, 신호 위반 등)를 결합한 종합 점수. (높을수록 좋음)</li>
<li><strong>Route Completion (RC):</strong> 목적지까지 도달한 거리의 비율.</li>
<li><strong>Infraction Score (IS):</strong> 교통 법규 위반에 따른 감점 지수. (1.0에 가까울수록 위반이 적음)</li>
</ul>
<h3>5.2  정량적 성능 분석 (Quantitative Analysis)</h3>
<p>AD-H는 기존의 최신 모델(State-of-the-Art, SOTA)인 LMDrive와 비교하여 탁월한 성능을 기록하였다.</p>
<p><strong>표 1. LangAuto 벤치마크 주요 성능 비교 분석</strong> 6</p>
<table><thead><tr><th><strong>모델 (Backbone)</strong></th><th><strong>벤치마크</strong></th><th><strong>DS (Driving Score) ↑</strong></th><th><strong>RC (Route Completion) ↑</strong></th><th><strong>IS (Infraction Score) ↑</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td>LMDrive (LLaVA-7B)</td><td>LangAuto</td><td>36.2</td><td>-</td><td>-</td><td>Baseline (E2E 방식)</td></tr>
<tr><td><strong>AD-H (Mipha-3B)</strong></td><td>LangAuto</td><td>41.1</td><td>-</td><td>-</td><td>경량 백본 사용</td></tr>
<tr><td><strong>AD-H (LLaVA-7B)</strong></td><td>LangAuto</td><td><strong>44.0</strong></td><td>-</td><td>-</td><td><strong>SOTA 달성</strong></td></tr>
<tr><td>LMDrive (LLaVA-7B)</td><td>LangAuto-Tiny</td><td>66.5</td><td>77.9</td><td>0.85</td><td>-</td></tr>
<tr><td><strong>AD-H (LLaVA-7B)</strong></td><td>LangAuto-Tiny</td><td><strong>77.5</strong></td><td><strong>85.1</strong></td><td><strong>0.91</strong></td><td>큰 폭의 성능 향상</td></tr>
</tbody></table>
<p>위 표에서 확인할 수 있듯이, 동일한 백본(LLaVA-7B)을 사용했음에도 불구하고 AD-H는 LMDrive 대비 LangAuto 벤치마크에서 <strong>약 21.5% (36.2 → 44.0)</strong> 향상된 Driving Score를 기록하였다. 이는 단순히 모델의 크기가 성능을 결정하는 것이 아니라, <strong>’중간 단계 명령’을 통한 계층적 구조</strong>가 주행의 안정성과 정확성을 크게 높인다는 것을 실증한다. 특히 Infraction Score(IS)의 향상은 AD-H가 교통 법규를 더 잘 준수함을 의미하며, 이는 플래너가 생성하는 “신호를 준수하라“는 명시적인 언어 명령이 효과적으로 작동했음을 시사한다.</p>
<h3>5.3  정성적 분석: 자가 수정 및 일반화 (Qualitative Analysis)</h3>
<p>AD-H의 진가는 정량적 수치뿐만 아니라, 복잡하고 예기치 못한 상황에서의 대처 능력에서 드러난다.</p>
<h4>5.3.1  자가 수정 능력 (Self-Correction Capability)</h4>
<p>가장 주목할 만한 발견은 AD-H의 <strong>자가 수정(Self-correction)</strong> 능력이다. 훈련 데이터에 포함되지 않은 상황, 예를 들어 차량이 코너링 중 과도하게 조향(Oversteering)되어 차선을 이탈하려는 순간, 기존의 LMDrive 모델은 상황을 인식하지 못하고 잘못된 경로를 고수하거나 멈춰버리는 경향을 보였다.</p>
<p>반면, AD-H의 MLLM 플래너는 시각 정보를 통해 현재 차량이 의도한 궤적에서 벗어났음을 인지한다. 그리고 즉시 “현재 경로를 벗어났다. 오른쪽으로 완만하게 조향하여 차선 중앙으로 복귀하라“는 새로운 중간 단계 명령을 생성한다.5 컨트롤러는 이 명령을 받아 궤적을 수정한다. 이러한 피드백 루프는 별도의 지도 학습(Supervised Learning) 없이 MLLM의 내재된 추론 능력에서 발현된 것으로, 시스템의 안전성을 획기적으로 높이는 요소이다.2</p>
<h4>5.3.2  장기 시계열 명령 및 새로운 환경에 대한 적응력</h4>
<p>“다음 신호등에서 우회전한 뒤, 로터리가 나올 때까지 직진하고, 거기서 두 번째 출구로 나가라“와 같은 긴 호흡의 명령(Long-horizon Instruction)은 단일 단계 모델에게 매우 어려운 과제이다. AD-H는 이를 단계별로 분해(Decomposition)하여 현재 시점에 필요한 중간 명령만을 컨트롤러에게 전달함으로써, 긴 명령을 성공적으로 수행해냈다.5</p>
<p>또한, 훈련 과정에서 보지 못한 새로운 도시(Unseen Environments)에서의 테스트 결과, AD-H는 LMDrive 대비 월등히 높은 성공률을 보였다. 이는 중간 단계 언어 명령이 시각적 배경(건물, 날씨 등)의 변화에 영향을 덜 받는 <strong>불변의 특징(Invariant Feature)</strong> 역할을 하기 때문이다. 즉, 비가 오나 눈이 오나 “멈춰라“는 언어 명령의 의미는 변하지 않으므로, 시스템의 강건성(Robustness)이 확보된다.6</p>
<h2>6.  토의: 한계점과 최신 연구 동향 (Discussion)</h2>
<p>AD-H는 자율 주행의 새로운 가능성을 제시했으나, 여전히 해결해야 할 과제와 경쟁 기술들이 존재한다.</p>
<h3>6.1  계산 비용과 지연 시간 (Latency &amp; Computational Cost)</h3>
<p>AD-H의 가장 큰 약점은 <strong>실시간성</strong> 문제이다. 시스템은 매 프레임마다 거대 언어 모델인 플래너와 컨트롤러를 순차적으로 실행해야 한다. 특히 수십억 개의 파라미터를 가진 MLLM을 매번 추론하는 것은 막대한 연산 자원을 소모하며, 추론 지연(Inference Latency)을 발생시킨다. 이는 밀리초(ms) 단위의 반응이 필요한 고속 주행 상황에서 치명적일 수 있다. 최근 연구인 <strong>AdaDrive</strong>는 이러한 AD-H의 순차적 처리 방식을 ’1세대 언어 기반 주행’으로 규정하고, 평소에는 경량 모델만 사용하다가 위급 상황에서만 LLM을 활성화하는 ‘Slow-Fast’ 아키텍처를 통해 AD-H보다 높은 효율성과 성능(DS 70.6% 이상)을 달성하기도 하였다.7</p>
<h3>6.2  물리적 제약과 안전성</h3>
<p>또 다른 최신 연구인 <strong>VLA-MP</strong>는 AD-H가 생성하는 경로가 차량의 물리적 운동학(Kinematics)을 충분히 고려하지 않을 수 있음을 지적했다. VLA-MP는 물리 법칙을 강제하는 어댑터를 도입하여 LangAuto 벤치마크에서 AD-H(44.0)를 소폭 상회하는 44.3의 DS 점수를 기록하였다.10 이는 향후 AD-H가 언어적 추론뿐만 아니라 물리적 제약 조건을 더 엄격하게 통합하는 방향으로 발전해야 함을 시사한다.</p>
<h3>6.3  데이터 및 코드의 가용성</h3>
<p>연구 보고서 작성 시점을 기준으로, AD-H의 GitHub 저장소에는 “코드와 데이터를 정리 중“이라는 메시지가 있으며 구체적인 코드는 아직 완전히 공개되지 않은 상태이다.12 이는 외부 연구자들이 결과를 재현하거나 시스템을 상세히 분석하는 데 제약이 된다. 하지만 175만 개의 데이터셋 공개 계획은 자율 주행 연구 커뮤니티에 큰 기여가 될 것으로 기대된다.</p>
<h2>7.  결론 (Conclusion)</h2>
<p>본 보고서는 계층적 다중 에이전트 시스템을 표방하는 **AD-H(Autonomous Driving with Hierarchical agents)**에 대해 심층적으로 조사하였다. AD-H는 거대 언어 모델의 추론 능력을 자율 주행 제어와 결합하는 과정에서 발생하는 ’직접 변환’의 한계를 **‘중간 단계 언어 명령’**이라는 독창적인 인터페이스를 통해 극복하였다.</p>
<p>연구 결과, AD-H는 복잡한 도심 주행 시나리오에서 기존 SOTA 모델인 LMDrive를 압도하는 주행 성능과 법규 준수 능력을 보여주었다. 특히 훈련 데이터에 없는 상황에서도 스스로 오류를 수정하는 자가 수정 능력과, 새로운 환경에 대한 뛰어난 적응력은 이 시스템이 레벨 4 이상의 완전 자율 주행으로 나아가는 데 중요한 디딤돌이 될 수 있음을 시사한다.</p>
<p>물론 실시간 처리를 위한 연산 효율성 확보와 물리적 제약 조건의 통합 등은 여전히 해결해야 할 과제로 남아 있다. 그러나 AD-H가 제시한 **‘인지-판단-제어의 계층적 분리’**와 **‘언어 기반의 투명한 의사결정’**이라는 철학은 향후 등장할 모든 LLM 기반 자율 주행 시스템의 표준 아키텍처(Reference Architecture)로 자리 잡을 가능성이 높다. AD-H는 단순한 기술적 진보를 넘어, 인공지능이 인간의 언어를 이해하고 물리 세계와 상호작용하는 방식을 재정의한 선구적인 연구로 평가받아 마땅하다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>[2406.03474] AD-H: Autonomous Driving with Hierarchical Agents - arXiv, https://arxiv.org/abs/2406.03474</li>
<li>AD-H: Autonomous Driving with Hierarchical Agents - ResearchGate, https://www.researchgate.net/publication/381190412_AD-H_Autonomous_Driving_with_Hierarchical_Agents</li>
<li>AD-H: Autonomous Driving with Hierarchical Agents - OpenReview, https://openreview.net/forum?id=JwrnoB1tR0</li>
<li>Learning-Based Hierarchical Decision-Making Framework for Automatic Driving in Incompletely Connected Traffic Scenarios - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC11054000/</li>
<li>AD-H: Autonomous Driving with Hierarchical Agents - arXiv, <a href="https://arxiv.org/pdf/2406.03474">https://arxiv.org/pdf/2406.03474?</a></li>
<li>AD-H: Autonomous Driving with Hierarchical Agents - arXiv, https://arxiv.org/html/2406.03474v1</li>
<li>AdaDrive: Self-Adaptive Slow-Fast System for Language-Grounded Autonomous Driving, https://arxiv.org/html/2511.06253v1</li>
<li>Daily Papers - Hugging Face, <a href="https://huggingface.co/papers?q=low-frequency+guidance">https://huggingface.co/papers?q=low-frequency%20guidance</a></li>
<li>AD-H: AUTONOMOUS DRIVING WITH HIERARCHICAL AGENTS - OpenReview, https://openreview.net/pdf/e15ef4c8e8f4e0d2db875b42314bcc25546c73dc.pdf</li>
<li>VLA-MP: A Vision-Language-Action Framework for Multimodal Perception and Physics-Constrained Action Generation in Autonomous Driving - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC12526522/</li>
<li>AdaDrive: Self-Adaptive Slow-Fast System for Language-Grounded Autonomous Driving Supplementary Material, https://openaccess.thecvf.com/content/ICCV2025/supplemental/Zhang_AdaDrive_Self-Adaptive_Slow-Fast_ICCV_2025_supplemental.pdf</li>
<li>zhangzaibin/AD-H - GitHub, https://github.com/zhangzaibin/AD-H</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>