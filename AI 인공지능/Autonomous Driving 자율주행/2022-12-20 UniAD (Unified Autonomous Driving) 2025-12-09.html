<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:UniAD (Unified Autonomous Driving) 계획 중심의 엔드투엔드 자율 주행 프레임워크</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>UniAD (Unified Autonomous Driving) 계획 중심의 엔드투엔드 자율 주행 프레임워크</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">자율주행 (Autonomous Driving)</a> / <span>UniAD (Unified Autonomous Driving) 계획 중심의 엔드투엔드 자율 주행 프레임워크</span></nav>
                </div>
            </header>
            <article>
                <h1>UniAD (Unified Autonomous Driving) 계획 중심의 엔드투엔드 자율 주행 프레임워크</h1>
<p>2025-12-09, G30DR</p>
<h2>1.  서론: 자율 주행 패러다임의 진화와 UniAD의 등장</h2>
<p>자율 주행 기술은 현대 로보틱스와 인공지능 분야에서 가장 도전적이고 복잡한 과제 중 하나로 꼽힌다. 초기 자율 주행 시스템은 인간의 인지 과정을 모방하여 감지(Perception), 예측(Prediction), 계획(Planning)이라는 일련의 독립적인 모듈을 순차적으로 연결하는 ‘모듈식 파이프라인(Modular Pipeline)’ 방식을 채택해 왔다. 이러한 접근법은 각 모듈을 독립적으로 개발하고 검증할 수 있다는 공학적 이점을 제공했으나, 시스템이 고도화될수록 상위 모듈의 작은 오류가 하위 모듈로 전파되며 증폭되는 ’오류 누적(Accumulative Error)’이라는 근본적인 한계에 직면하게 되었다.1 특히, 인지 모듈이 ‘완벽한’ 감지 결과를 내놓지 못할 경우, 예측과 계획 모듈은 잘못된 입력을 기반으로 의사결정을 내려야 했으며, 이는 치명적인 안전 사고로 이어질 수 있는 잠재적 위험 요인이었다.</p>
<p>이러한 배경 속에서 등장한 ’UniAD (Unified Autonomous Driving)’는 기존의 모듈식 접근법이 가진 한계를 극복하고, 자율 주행의 모든 과정을 하나의 통합된 신경망으로 처리하려는 시도로서 학계와 산업계의 큰 주목을 받았다. 2023년 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR)에서 최우수 논문상(Best Paper Award)을 수상한 UniAD는 SenseTime Research, OpenDriveLab, 상하이 AI 연구소(Shanghai AI Laboratory) 등의 공동 연구로 탄생하였으며, ’계획 중심(Planning-oriented)’이라는 새로운 철학을 제시하였다.3</p>
<p>UniAD는 단순히 인지와 예측, 계획을 하나의 네트워크에 묶어놓은 멀티태스크 학습(Multi-Task Learning, MTL) 모델이 아니다. 기존의 MTL 모델들이 각 작업을 병렬적으로 수행하며 단순히 특징 추출기(Backbone)를 공유하는 수준에 그쳤다면, UniAD는 모든 모듈이 ’안전한 주행 계획 수립’이라는 궁극적인 목표를 달성하기 위해 유기적으로 상호작용하도록 설계되었다. 이를 위해 쿼리(Query) 기반의 통합 인터페이스를 도입하여, 인지 단계에서 추출된 정보가 손실 없이 예측 및 계획 단계로 전파되도록 하였으며, 결과적으로 기존 방식 대비 압도적인 성능 향상을 입증하였다.1</p>
<p>본 보고서는 UniAD의 아키텍처, 핵심 알고리즘, 성능 평가 결과, 그리고 최신 v2.0 업데이트 사항까지 포괄적으로 분석하여, 이 프레임워크가 자율 주행 기술의 미래에 던지는 시사점과 기술적 가치를 심층적으로 규명하고자 한다.</p>
<h2>2.  기존 자율 주행 아키텍처의 한계와 UniAD의 철학적 기반</h2>
<h3>2.1  모듈식 파이프라인의 구조적 취약성</h3>
<p>전통적인 자율 주행 스택은 라이다(LiDAR), 카메라, 레이더 등 센서 데이터를 처리하여 객체의 위치를 파악하는 인지 모듈, 파악된 객체의 미래 궤적을 예상하는 예측 모듈, 그리고 이를 바탕으로 자차의 경로를 생성하는 계획 모듈로 엄격하게 구분되어 있었다. 이러한 구조 하에서는 각 모듈이 서로 다른 최적화 목표를 가진다. 예를 들어, 인지 모듈은 바운딩 박스(Bounding Box)의 정확도(IoU)를 높이는 데 집중하고, 예측 모듈은 궤적 오차(ADE/FDE)를 줄이는 데 집중한다.</p>
<p>문제는 이러한 개별 최적화가 전체 시스템의 최적화, 즉 ’안전한 주행’을 보장하지 않는다는 점이다. 인지 모듈이 99%의 정확도를 달성하더라도, 놓친 1%의 객체가 충돌 위험이 높은 근거리 차량이라면 시스템 전체는 실패한 것이다. 또한, 모듈 간 정보 전달 과정에서 발생하는 정보의 손실(Information Loss)은 하위 모듈이 상황을 문맥적으로 이해하는 것을 방해한다. 인지 모듈이 단순히 객체의 좌표값만 전달할 경우, 예측 모듈은 그 객체의 시각적 특징(예: 방향지시등 점등 여부, 차량의 종류에 따른 거동 특성)을 활용할 수 없게 된다.1</p>
<pre><code class="language-mermaid">graph TD
    subgraph "초기 단계: 미세한 오류 발생"
        SENS["센서 데이터 (노이즈 포함)"] --&gt; PER["인지 모듈"]
        PER -- "작은 오류: 객체 위치 0.5m 오차" --&gt; OUT1["부정확한 Bounding Box"]
    end

    subgraph "중간 단계: 오류의 해석 실패"
        OUT1 --&gt; PRED["예측 모듈"]
        PRED -- "중간 오류: 궤적 예측 2.0m 오차&lt;br/&gt;(잘못된 위치 기반 예측)" --&gt; OUT2["잘못된 미래 경로"]
    end

    subgraph "최종 단계: 치명적 사고 위험"
        OUT2 --&gt; PLAN["계획 모듈"]
        PLAN -- "치명적 오류: 충돌 경로 생성" --&gt; CRASH["안전 사고 발생 (Failure)"]
    end

    style PER fill:#fffbf0,stroke:#d6b656
    style PRED fill:#ffe6e6,stroke:#ff9999
    style PLAN fill:#ffcccc,stroke:#ff0000,stroke-width:4px
</code></pre>
<h3>2.2  단순 멀티태스크 학습(MTL)의 한계와 부정적 전이</h3>
<pre><code class="language-mermaid">graph TD
    subgraph "단순 멀티태스크 학습 (Negative Transfer 위험)"
        B1["공유 Backbone"]
        B1 --&gt; H1["Head 1: 감지"]
        B1 --&gt; H2["Head 2: 세그멘테이션"]
        B1 --&gt; H3["Head 3: 예측"]
        H1 -.-&gt; C1["독립적 손실 함수 (Loss)"]
        H2 -.-&gt; C2["자원 경쟁 발생"]
        H3 -.-&gt; C3["상충되는 Gradient"]
    end

    subgraph "UniAD: 계획 중심 통합 (Planning-oriented)"
        B2["공유 Backbone &amp; BEV"]
        B2 --&gt; Q1["Perception Query"]
        Q1 -- "쿼리 전달 (정보 보존)" --&gt; Q2["Prediction Query"]
        Q2 -- "쿼리 전달 (문맥 유지)" --&gt; Q3["Planning Query"]
        
        Q3 --&gt; G["단일 목표: 안전한 주행"]
        style Q1 fill:#e1f5fe,stroke:#01579b
        style Q2 fill:#e1f5fe,stroke:#01579b
        style Q3 fill:#e1f5fe,stroke:#01579b
        style G fill:#b3e5fc,stroke:#0277bd,stroke-width:2px
    end
</code></pre>
<p>엔드투엔드(End-to-End) 방식의 초기 접근법 중 하나인 멀티태스크 학습은 하나의 신경망에서 감지, 세그멘테이션, 예측 등을 동시에 수행하도록 설계되었다. 그러나 단순한 MTL 모델들은 ‘부정적 전이(Negative Transfer)’ 문제에 직면하곤 했다. 이는 서로 다른 성격의 작업들이 하나의 네트워크 내에서 자원을 경쟁하거나, 상충되는 그래디언트(Gradient)를 생성하여 오히려 개별 작업의 성능을 저하시키는 현상을 말한다. UniAD 연구진은 이러한 문제가 작업 간의 효과적인 조정(Coordination) 메커니즘이 부재하기 때문에 발생한다고 진단하였다.1</p>
<pre><code class="language-mermaid">graph TD
    subgraph "기존 모듈식 파이프라인 (Modular Pipeline)"
        A1["센서 입력 (Camera, LiDAR)"] --&gt; B1["인지 (Perception)"]
        B1 -- "객체 좌표만 전달 (정보 손실)" --&gt; C1["예측 (Prediction)"]
        C1 -- "예측 궤적 전달 (오류 누적)" --&gt; D1["계획 (Planning)"]
        D1 --&gt; E1["제어 (Control)"]
        style B1 fill:#ffcccc,stroke:#333
        style C1 fill:#ffcccc,stroke:#333
        style D1 fill:#ffcccc,stroke:#333
    end

    subgraph "UniAD 통합 프레임워크 (Unified Framework)"
        A2["멀티 카메라 입력"] --&gt; B2["Backbone &amp; BEV Encoder"]
        B2 -- "BEV Feature (B)" --&gt; C2["Perception &amp; Prediction &amp; Planning"]
        C2 --&gt; F2["통합된 쿼리 (Query) 상호작용"]
        F2 -- "목표 지향적 정보 전달" --&gt; G2["안전한 주행 계획 수립"]
        style C2 fill:#ccffcc,stroke:#333
        style F2 fill:#ccffcc,stroke:#333
        style G2 fill:#ccffcc,stroke:#333
    end
</code></pre>
<h3>2.3  UniAD의 핵심 철학: Planning-oriented Philosophy</h3>
<p>UniAD는 이러한 문제의식 위에서 “모든 작업은 계획(Planning)을 위해 존재한다“는 철학을 정립하였다. 이는 인지와 예측이 독립적인 목표를 달성하기 위해 수행되는 것이 아니라, 최종적으로 자아 차량(Ego-vehicle)이 안전하고 효율적인 경로를 생성하는 데 필요한 정보를 제공하는 ’수단’으로서 기능해야 함을 의미한다.</p>
<p>UniAD는 이를 구현하기 위해 다음과 같은 설계 원칙을 따른다:</p>
<ol>
<li><strong>계층적 통합 (Hierarchical Integration):</strong> 인지 -&gt; 예측 -&gt; 계획의 순서를 따르되, 각 단계의 잠재 표현(Latent Representation)을 쿼리 형태로 다음 단계에 직접 전달한다.</li>
<li><strong>쿼리 기반 인터페이스 (Unified Query Interface):</strong> 트랜스포머(Transformer)의 쿼리-키-밸류(Query-Key-Value) 메커니즘을 활용하여 모듈 간 정보 교환을 매개한다. 이를 통해 하위 모듈은 상위 모듈의 추상화된 정보를 선택적으로 조회(Attend)하고 활용할 수 있다.</li>
<li><strong>전역적 상호작용 (Global Interaction):</strong> 객체 간, 객체와 지도 간, 객체와 자차 간의 상호작용을 전역적인 관점에서 모델링하여 복잡한 교통 상황을 문맥적으로 이해한다.1</li>
</ol>
<pre><code class="language-mermaid">graph TD
    subgraph "Backbone"
        IMG["이미지 입력"] --&gt; CNN["CNN (ResNet)"]
        CNN -- "2D Features" --&gt; BEV["BEVFormer"]
        BEV -- "BEV Feature (B)" --&gt; ALL["전역 환경 정보 공유"]
    end

    subgraph "Perception Layer"
        ALL --&gt; TF["TrackFormer"]
        ALL --&gt; MF["MapFormer"]
        TF -- "Track Query (Q_A)" --&gt; MOF["MotionFormer"]
        MF -- "Map Query (Q_M)" --&gt; MOF
    end

    subgraph "Prediction Layer"
        MOF -- "Motion Query (Q_motion)" --&gt; OCC["OccFormer"]
        MOF -- "미래 궤적" --&gt; PLAN["Planner"]
        OCC -- "점유 그리드 맵" --&gt; PLAN
    end

    subgraph "Planning Layer"
        PLAN --&gt; EGO["자차 쿼리 (Ego-Query)"]
        EGO -- "충돌 최적화" --&gt; OUT["최종 웨이포인트 생성"]
    end

    style PLAN fill:#d4e1f5,stroke:#333,stroke-width:2px
    style ALL fill:#fff2cc,stroke:#333
</code></pre>
<h2>3.  UniAD 아키텍처 상세 분석</h2>
<p>UniAD의 아키텍처는 크게 백본(Backbone) 및 BEV 인코더, 인지(Perception) 계층, 예측(Prediction) 계층, 그리고 계획(Planning) 계층으로 구성된다. 각 계층은 트랜스포머 디코더 구조를 기반으로 하며, 앞선 단계의 쿼리를 입력받아 더 고차원의 정보를 생성해낸다.</p>
<pre><code class="language-mermaid">graph TD
    B["BEV Feature (B)"]

    subgraph "Perception: 쿼리 생성"
        TF["TrackFormer"]
        MF["MapFormer"]
        B --&gt; TF
        B --&gt; MF
        TF -- "객체 감지 및 추적" --&gt; QA["Track Query (Q_A)&lt;br/&gt;(위치, 크기, 속도, 시각 정보)"]
        MF -- "도로 요소 인식 (Online Mapping)" --&gt; QM["Map Query (Q_M)&lt;br/&gt;(차선, 중앙분리대 등)"]
    end

    subgraph "Prediction: 상호작용 (MotionFormer)"
        QA --&gt; INTER["Interaction Heads"]
        QM --&gt; INTER
        
        INTER -- "Agent-Agent" --&gt; IA["사회적 상호작용&lt;br/&gt;(끼어들기, 양보)"]
        INTER -- "Agent-Map" --&gt; IM["도로 기하학적 제약&lt;br/&gt;(신호, 차선 준수)"]
        INTER -- "Agent-Goal" --&gt; IG["목적지 기반&lt;br/&gt;궤적 일관성"]
        
        IA --&gt; PRED["궤적 예측"]
        IM --&gt; PRED
        IG --&gt; PRED
    end

    PRED --&gt; QOUT["Motion Query (Q_motion)"]
</code></pre>
<h3>3.1  Backbone 및 BEV Representation</h3>
<p>UniAD는 다중 카메라(Multi-camera) 이미지 입력을 처리하기 위해 먼저 ResNet과 같은 CNN 백본을 통해 2D 이미지 특징(Feature)을 추출한다. 이후, 이 2D 특징들을 3차원 공간 인식을 위한 조감도(Bird’s-Eye-View, BEV) 특징 <span class="math math-inline">B</span>로 변환한다. 이 과정에서 <strong>BEVFormer</strong>의 구조를 차용하여, 시간적(Temporal) 정보와 공간적(Spatial) 정보를 통합한 강력한 BEV 특징을 생성한다. 이 BEV 특징 <span class="math math-inline">B</span>는 이후의 모든 모듈(Track, Map, Motion, Occ, Planner)이 공유하는 전역적인 환경 정보(Global Context)로 활용된다.8</p>
<h3>3.2  Perception Layer: TrackFormer와 MapFormer</h3>
<p>인지 계층은 동적 객체와 정적 환경요소를 각각 처리하는 두 개의 모듈로 나뉜다.</p>
<h4>3.2.1  TrackFormer: 동적 객체 추적</h4>
<p>TrackFormer는 주변의 차량, 보행자 등 동적 객체(Agent)를 검출하고 추적한다. 이는 <strong>MOTR (End-to-End Multiple-Object Tracking with Transformer)</strong> 아키텍처를 기반으로 한다.</p>
<ul>
<li><strong>작동 원리:</strong> 학습 가능한 쿼리(Learnable Query)인 ’Track Query’가 BEV 특징과 상호작용하여 객체의 위치, 크기, 속도 등을 회귀(Regress)한다. 중요한 점은 이전 프레임의 쿼리가 다음 프레임으로 전달되어 지속적으로 객체를 추적한다는 점이다.</li>
<li><strong>출력:</strong> TrackFormer의 출력인 객체 쿼리(<span class="math math-inline">Q_A</span>)는 단순히 Bounding Box 좌표만을 담고 있는 것이 아니라, 해당 객체의 시각적 특징과 운동 정보가 임베딩된 고차원 벡터이다. 이 <span class="math math-inline">Q_A</span>는 다음 단계인 MotionFormer의 핵심 입력이 된다.9</li>
</ul>
<h4>3.2.2  MapFormer: 온라인 정적 지도 생성</h4>
<p>MapFormer는 차선(Lane), 중앙분리대(Divider), 횡단보도(Pedestrian Crossing) 등 도로의 정적 요소를 인식한다. <strong>Panoptic SegFormer</strong>를 기반으로 하여, 지도의 각 요소를 쿼리(<span class="math math-inline">Q_M</span>)로 추상화한다.</p>
<ul>
<li><strong>의의:</strong> 이는 고정밀(HD) 지도에 의존하지 않고 센서 데이터만으로 주행 가능한 영역을 실시간으로 파악하는 ’Online Mapping’을 수행함을 의미한다. 생성된 지도 쿼리 <span class="math math-inline">Q_M</span>은 예측 단계에서 객체가 도로의 구조(예: 차선)를 따르는지 판단하는 데 중요한 문맥 정보를 제공한다.10</li>
</ul>
<pre><code class="language-mermaid">graph TD
    input1["BEV 특징 (B)"]
    input2["Motion Query (Q_motion)"]

    subgraph "OccFormer 내부 로직"
        input1 &amp; input2 --&gt; ATTN["Pixel-Agent 상호작용 (Attention)"]
        
        ATTN --&gt; DENSE["밀집 특징 (Dense Feature)"]
        
        DENSE --&gt; P1["점유 예측 (Occupancy Logits)"]
        DENSE --&gt; P2["흐름 예측 (Flow Logits)"]
        
        input2 --&gt; MASK["인스턴스 마스크 생성&lt;br/&gt;(어떤 객체가 점유하는가?)"]
        
        P1 &amp; MASK --&gt; COMB["마스킹 된 점유 그리드"]
    end

    COMB --&gt; OUT["미래 시점 점유 맵 (Occupancy Grid Map)&lt;br/&gt;계획 모듈 입력으로 전달"]
</code></pre>
<h3>3.3  Prediction Layer: MotionFormer와 OccFormer</h3>
<p>UniAD의 예측 계층은 객체 단위의 궤적 예측과 씬(Scene) 단위의 점유 예측을 상호 보완적으로 수행한다.</p>
<h4>3.3.1  MotionFormer: 상호작용 기반 궤적 예측</h4>
<p>MotionFormer는 TrackFormer에서 전달받은 객체 쿼리(<span class="math math-inline">Q_A</span>)와 MapFormer의 지도 쿼리(<span class="math math-inline">Q_M</span>)를 활용하여 각 객체의 미래 움직임을 예측한다. UniAD는 단순한 물리적 운동 모델을 넘어, 다음과 같은 세 가지 수준의 상호작용(Interaction)을 어텐션 메커니즘으로 구현하였다 9:</p>
<ol>
<li><strong>Agent-Agent Interaction:</strong> 주변의 다른 객체 쿼리들과 상호작용하여, 끼어들기나 양보와 같은 사회적 상호작용을 모델링한다.</li>
<li><strong>Agent-Map Interaction:</strong> 지도 쿼리(<span class="math math-inline">Q_M</span>)와 상호작용하여, 차량이 차선을 따라 주행하거나 교차로에서 신호를 준수하는 등의 도로 기하학적 제약을 학습한다.</li>
<li><strong>Agent-Goal Interaction:</strong> 객체의 잠재적인 목적지(Goal point)를 고려하여 장기적인 궤적의 일관성을 확보한다.</li>
</ol>
<p>이러한 고도화된 상호작용 모델링을 통해 MotionFormer는 복잡한 도심 환경에서도 높은 정확도의 궤적 예측(<span class="math math-inline">Q_{motion}</span>)을 수행하며, 이는 벤치마크 결과에서 타 모델 대비 압도적인 성능 차이를 만드는 핵심 요인이다.9</p>
<h4>3.3.2  OccFormer: 씬 레벨 점유 예측</h4>
<p>OccFormer는 개별 객체로 검출되지 않는 장애물이나 비정형 물체(예: 떨어진 화물, 덤불 등)를 포함하여, 씬 전체의 시공간적 점유(Occupancy) 상태를 예측한다.</p>
<ul>
<li><strong>구조:</strong> MotionFormer의 출력인 <span class="math math-inline">Q_{motion}</span>과 BEV 특징 <span class="math math-inline">B</span>를 입력받아, 미래 시점의 점유 그리드 맵(Occupancy Grid Map)을 생성한다.</li>
<li><strong>기능:</strong> 단순히 픽셀 단위의 예측을 넘어, **인스턴스 수준의 어텐션 마스크(Instance-level Attention Mask)**를 활용하여 각 객체의 미래 위치를 마스킹한다. 이는 계획 모듈이 ‘어디가 비어있는지’ 뿐만 아니라 ’어디가 특정 객체에 의해 점유될 것인지’를 명확히 인지하게 돕는다.12</li>
</ul>
<h3>3.4  Planning Layer: 통합 Planner와 충돌 최적화</h3>
<p>UniAD의 최상위 모듈인 Planner는 자아 차량 쿼리(Ego-vehicle query)를 사용하여 최종적인 주행 궤적(Waypoints)을 생성한다.</p>
<ul>
<li><strong>입력:</strong> 자아 차량 쿼리는 MotionFormer와 OccFormer의 출력(미래 궤적 및 점유 맵)을 조회(Query)하여, 동적 장애물과 정적 환경 정보를 모두 고려한 안전한 경로를 탐색한다.</li>
<li><strong>충돌 최적화 (Collision Optimization):</strong> UniAD Planner의 가장 큰 특징은 <strong>비선형 최적화(Non-linear Optimization)</strong> 과정을 포함한다는 것이다. 신경망이 초기에 생성한 궤적이더라도, 물리적으로 충돌 가능성이 있거나 운동학적으로 불가능한 움직임(예: 급격한 회전)을 포함할 수 있다. 이를 보정하기 위해 UniAD는 추론 단계에서 뉴턴법(Newton’s method) 등을 활용하여 예측된 점유 맵과의 충돌 비용(Collision Cost)을 최소화하는 방향으로 궤적을 미세 조정한다.9</li>
</ul>
<pre><code class="language-mermaid">graph LR
    INPUTS["입력 데이터"] --&gt; P_MAIN["UniAD Planner"]
    
    subgraph "Planner Inputs"
        IN1["자차 쿼리&lt;br&gt;(Ego-Vehicle Query)"]
        IN2["예측된 객체 궤적&lt;br&gt;(from MotionFormer)"]
        IN3["점유 그리드 맵&lt;br&gt;(from OccFormer)"]
    end
    
    IN1 &amp; IN2 &amp; IN3 --&gt; P_MAIN

    P_MAIN --&gt; OPT["충돌 최적화&lt;br&gt;(Collision Optimization)&lt;br/&gt;Non-linear Optimization"]
    OPT --&gt; TRAJ["최종 주행 궤적&lt;br&gt;(Waypoints)"]

    subgraph "학습 비용 함수 (Cost Function L_plan)"
        L_TOTAL["L_plan&lt;br&gt;(Total Loss)"]
        
        C1["모방 학습 항"]
        C1_DESC["L2 거리 오차 최소화&lt;br/&gt;(인간 운전자 궤적 모방)"]
        
        C2["충돌 페널티 항"]
        C2_DESC["IoU 기반 충돌 계산&lt;br/&gt;(예측된 점유 맵 회피)"]
        
        C3["운동학적 제약 항"]
        C3_DESC["승차감 및 물리적 한계 고려&lt;br/&gt;(저크, 곡률, 가속도)"]

        C1 --&gt; L_TOTAL
        C2 --&gt; L_TOTAL
        C3 --&gt; L_TOTAL
        
        C1_DESC -.-&gt; C1
        C2_DESC -.-&gt; C2
        C3_DESC -.-&gt; C3
    end
</code></pre>
<h3>3.5  비용 함수 (Cost Function) 구성</h3>
<p>Planner의 학습을 위한 비용 함수 <span class="math math-inline">L_{plan}</span>은 다음과 같이 구성된다 9:</p>
<p><span class="math math-display"> L_{plan} = \lambda_{coord} |\tau - \hat{\tau}|*2 + \lambda*{col} \sum_t IoU(box(\tau_t), \hat{O}*t) + \lambda*{kin} \Phi(\tau) </span></p>
<p>여기서:</p>
<ul>
<li><span class="math math-inline">\|\tau - \hat{\tau}\|_2</span>: 인간 운전자의 실제 주행 궤적(<span class="math math-inline">\hat{\tau}</span>)과 예측 궤적(<span class="math math-inline">\tau</span>) 간의 L2 거리 오차를 최소화하는 모방 학습 항이다.</li>
<li><span class="math math-inline">IoU(box(\tau_t), \hat{O}_t)</span>: 시간 <span class="math math-inline">t</span>에서의 자차 영역(<span class="math math-inline">box(\tau_t)</span>)과 예측된 점유 맵(<span class="math math-inline">\hat{O}_t</span>) 간의 교차 영역(IoU)을 계산하여 충돌을 페널티로 부과한다.</li>
<li><span class="math math-inline">\Phi(\tau)</span>: 저크(Jerk, 가속도의 변화율), 곡률(Curvature), 가속도 등 차량의 운동학적 제약을 위반하지 않도록 하는 정규화 항으로, 승차감(Comfort)과 물리적 주행 가능성을 보장한다.14</li>
</ul>
<h2>4. UniAD의 성능 평가 및 비교 분석</h2>
<h3>4.1 nuScenes 벤치마크 결과 (Quantitative Analysis)</h3>
<p>UniAD는 권위 있는 자율 주행 데이터셋인 nuScenes 벤치마크에서 모든 작업(Tasks)에 대해 최첨단(SOTA) 성능을 기록하였다. 특히, 엔드투엔드 모델의 핵심 지표인 계획(Planning) 성능에서 기존 모델들을 압도하였다.</p>
<p>아래 표는 주요 엔드투엔드 모델 간의 계획 성능 비교를 보여준다.</p>
<p><strong>[표 1] nuScenes 데이터셋에서의 Planning 성능 비교 (L2 Error 및 Collision Rate)</strong></p>
<table><thead><tr><th><strong>모델 (Method)</strong></th><th><strong>입력 (Input)</strong></th><th><strong>Avg. L2 Error (m) ↓</strong></th><th><strong>Collision Rate (%) ↓</strong></th><th><strong>비고 (Remarks)</strong></th></tr></thead><tbody>
<tr><td><strong>ST-P3</strong> 16</td><td>Camera</td><td>2.54</td><td>1.07</td><td>모듈식 설계, 높은 충돌률</td></tr>
<tr><td><strong>VAD-Base</strong> 16</td><td>Camera</td><td>0.41</td><td>0.17</td><td>벡터화된 씬 표현, 빠른 속도</td></tr>
<tr><td><strong>UniAD</strong> 16</td><td>Camera</td><td><strong>0.48</strong></td><td><strong>0.31</strong></td><td><strong>CVPR 2023 Best Paper</strong></td></tr>
<tr><td><strong>UniAD v2.0</strong> 7</td><td>Camera</td><td>-</td><td><strong>0.29</strong></td><td>최신 업데이트, 성능 개선</td></tr>
</tbody></table>
<p>주: L2 Error와 Collision Rate는 평가 프로토콜(NoAvg: 평균 없이 특정 시점 값 사용, TemAvg: 시간 평균 사용)에 따라 논문마다 보고되는 수치가 상이할 수 있음. UniAD는 초기 논문에서 0.31%의 충돌률을 보고했으나, 이후 v2.0 업데이트를 통해 0.29%까지 성능을 향상시켰음.7</p>
<p>UniAD는 특히 <strong>Motion Forecasting</strong> 분야에서 minADE(Minimum Average Displacement Error) 0.71m를 기록하여, 이전 SOTA 모델인 ViP3D(2.05m) 대비 오차를 약 65.4% 감소시키는 기염을 토했다.9 이는 UniAD의 통합된 쿼리 구조가 객체의 미래 움직임을 예측하는 데 있어 얼마나 효과적인지를 증명하는 결과이다.</p>
<h3>4.2 경쟁 모델과의 비교 (vs VAD, ST-P3)</h3>
<ul>
<li><strong>ST-P3 (Spatial-Temporal Perception, Prediction, and Planning):</strong> 초기 엔드투엔드 모델로, 인지와 예측을 통합하려 했으나 점유 예측의 정밀도가 낮고 모듈 간 정보 교환이 제한적이어서 복잡한 시나리오에서의 충돌률이 높게 나타났다.18</li>
<li><strong>VAD (Vectorized Autonomous Driving):</strong> VAD는 씬 전체를 벡터화(Vectorization)하여 처리 속도를 극대화한 모델이다. VAD-Base 모델은 UniAD 대비 약 2~3배 빠른 추론 속도(FPS)를 보이며, 계획 성능 면에서도 대등하거나 일부 지표에서 우수한 결과를 보인다. 그러나 UniAD는 픽셀 단위의 조밀한 점유(Dense Occupancy) 정보를 활용하므로, 비정형 장애물이 많은 환경에서는 더 풍부한 정보를 계획 모듈에 제공할 수 있다는 강점이 있다.16</li>
</ul>
<h3>4.3 추론 속도 및 하드웨어 효율성 이슈</h3>
<p>UniAD의 가장 큰 아킬레스건은 연산 복잡도이다. 거대한 트랜스포머 백본과 복잡한 어텐션 연산, 그리고 덴스(Dense)한 쿼리 상호작용으로 인해 NVIDIA A100 GPU 하나에서도 약 1.8 ~ 2.1 FPS의 낮은 추론 속도를 보인다.16</p>
<p>이는 실시간 자율 주행(통상 10 FPS 이상 요구)에 직접 적용하기에는 무리가 있는 수치이다. 반면 VAD와 같은 후속 모델들은 4.5 FPS 이상, 경량화 모델은 16 FPS 이상을 달성하며 실용성 측면에서 UniAD를 추격하고 있다.19 따라서 UniAD의 향후 연구 방향은 성능을 유지하면서 연산 효율성을 높이는 경량화(Distillation) 및 최적화에 집중될 것으로 보인다.</p>
<pre><code class="language-mermaid">graph LR
    %% 좌표축 설정의 의미를 그래프로 표현
    yAxis["▲ 계획 성능 (Planning Performance / Safety)"]
    xAxis["▶ 추론 속도 (Inference Speed / FPS)"]

    subgraph "모델 포지셔닝 (Positioning)"
        UNIAD["UniAD&lt;br/&gt;(High Perf, Low Speed)&lt;br/&gt;SOTA 성능, 2 FPS 수준"]
        VAD["VAD-Base&lt;br/&gt;(High Perf, Mid Speed)&lt;br/&gt;벡터화로 속도 개선"]
        STP3["ST-P3&lt;br/&gt;(Low Perf, Low Speed)&lt;br/&gt;초기 모델, 높은 충돌률"]
        FUTURE["이상적 목표 (Future Goal)&lt;br/&gt;(High Perf, High Speed)&lt;br/&gt;경량화 + 고성능"]
    end

    style UNIAD fill:#d4edda,stroke:#155724,stroke-width:2px
    style VAD fill:#fff3cd,stroke:#856404
    style STP3 fill:#f8d7da,stroke:#721c24
    style FUTURE fill:#cce5ff,stroke:#004085,stroke-dasharray: 5 5
</code></pre>
<h2>5. UniAD v2.0: 진화와 확장</h2>
<p>2025년 10월, OpenDriveLab은 UniAD의 대규모 업데이트 버전인 <strong>UniAD v2.0</strong>을 공개하였다. 이는 단순한 코드 리팩토링을 넘어, 자율 주행 연구의 최신 트렌드를 반영한 중요한 진화를 보여준다.7</p>
<pre><code class="language-mermaid">graph LR
    ROOT["UniAD v2.0 (2025.10)"]

    subgraph "기술적 개선 (Tech Stack)"
        T1["코드 마이그레이션"]
        T1_D["PyTorch 2.x &amp; mmdet3d 1.x"]
        T2["가속화"]
        T2_D["Flash Attention 적용"]
    end

    subgraph "데이터셋 확장 (Datasets)"
        D1["nuScenes"]
        D1_D["기본 인지/예측 평가"]
        D2["nuPlan (신규)"]
        D2_D["대규모 계획 전용 데이터&lt;br/&gt;(1000시간+, 장기 계획)"]
        D3["NAVSIM (신규)"]
        D3_D["폐루프(Closed-loop) 시뮬레이션"]
    end

    subgraph "성능 지표 (Performance)"
        P1["Planning"]
        P1_D["Collision Rate: 0.29% (SOTA)"]
        P2["Motion"]
        P2_D["minADE: 0.71m"]
        P3["Occupancy"]
        P3_D["IoU: 63.4%"]
    end

    ROOT --&gt; T1
    T1 --&gt; T1_D
    ROOT --&gt; T2
    T2 --&gt; T2_D

    ROOT --&gt; D2
    D2 --&gt; D2_D
    ROOT --&gt; D3
    D3 --&gt; D3_D
    
    ROOT --&gt; P1
    P1 --&gt; P1_D
    ROOT --&gt; P2
    P2 --&gt; P2_D
</code></pre>
<h3>5.1 주요 업데이트 사항</h3>
<ol>
<li><strong>최신 프레임워크 도입:</strong> 기존의 구형 라이브러리 의존성을 탈피하고, <strong>PyTorch 2.x</strong> 및 <strong>mmdet3d 1.x</strong> 기반으로 전체 코드를 마이그레이션하였다. 이는 최신 GPU 가속 기능(Flash Attention 등)을 활용할 수 있게 하여 학습 및 추론 효율을 개선하였다.</li>
<li><strong>데이터셋의 확장 (nuPlan &amp; NAVSIM):</strong> v1.0이 인지 중심의 nuScenes 데이터셋에 집중했다면, v2.0은 대규모 계획 전용 데이터셋인 <strong>nuPlan</strong>과 시뮬레이션 벤치마크인 <strong>NAVSIM</strong>을 통합하였다. nuPlan은 1,000시간 이상의 실제 주행 데이터를 포함하고 있어, 단순한 3초 미래 예측을 넘어 장기적인 주행 계획 능력을 검증하기에 적합하다. NAVSIM 통합은 폐루프(Closed-loop) 평가를 강화하려는 의도로, 자차의 행동이 환경에 영향을 미치는 실제 주행 상황을 시뮬레이션하여 모델의 강건성을 테스트할 수 있게 한다.7</li>
<li><strong>성능의 재확인:</strong> 최적화된 v2.0 모델은 Motion minADE 0.71m, Occupancy IoU 63.4%, Planning avg.Col 0.29%를 달성하며 여전히 해당 분야의 최고 성능(SOTA)을 유지하고 있음을 입증하였다.7</li>
</ol>
<pre><code class="language-mermaid">graph TD
    INIT["초기 씬 데이터 (nuPlan)"] --&gt; MODEL["UniAD v2.0 모델"]
    
    subgraph "Closed-loop Simulation Cycle (NAVSIM)"
        MODEL -- "주행 계획 (Trajectory) 생성" --&gt; SIM["시뮬레이터 (Simulator)"]
        SIM -- "자차 이동에 따른 환경 변화" --&gt; UPDATE["상태 업데이트"]
        UPDATE -- "주변 차량의 반응 (Reactive Agents)" --&gt; NEXT["다음 프레임 입력 생성"]
        NEXT --&gt; MODEL
    end

    SIM -- "충돌 여부 / 교통 법규 준수 확인" --&gt; EVAL["최종 평가 (Score)"]
    
    style SIM fill:#e2e3e5,stroke:#383d41
    style MODEL fill:#c3e6cb,stroke:#155724
</code></pre>
<h2>6. 한계점 및 향후 과제</h2>
<p>UniAD가 제시한 통합 아키텍처는 혁신적이지만, 여전히 해결해야 할 과제들이 남아있다.</p>
<ol>
<li><strong>롱테일(Long-tail) 시나리오의 실패:</strong> UniAD 역시 데이터 기반의 딥러닝 모델이므로, 학습 데이터에 거의 등장하지 않는 희귀한 상황(Long-tail case)에 취약하다. 예를 들어, 대형 트레일러가 급격하게 회전하며 차선을 침범하는 경우나, 도로 공사로 인해 차선이 복잡하게 얽힌 경우 인지 모듈이 객체의 형태를 잘못 파악하고, 이것이 잘못된 궤적 예측으로 이어지는 사례가 보고되었다.8 시각적 자료(Supplementary Material)에 따르면, 인지 단계에서 객체의 헤딩(Heading) 각도를 잘못 추정하여 계획 모듈이 엉뚱한 경로를 생성하는 경우가 관찰된다.9</li>
<li><strong>높은 연산 비용:</strong> 앞서 언급한 바와 같이 낮은 FPS는 상용화를 위해 반드시 극복해야 할 장벽이다. 모델의 크기를 줄이거나, 중요하지 않은 쿼리를 가지치기(Pruning)하는 등의 경량화 기법이 필수적이다.</li>
<li><strong>폐루프 평가의 한계:</strong> nuScenes 벤치마크는 기본적으로 오픈 루프(Open-loop) 평가이다. 즉, 모델이 예측한 경로대로 차가 이동했을 때 주변 차량이 어떻게 반응할지를 시뮬레이션하지 않고, 단순히 미리 녹화된 데이터와 비교만 한다. 이는 모델의 실제 주행 능력을 과대평가할 위험이 있다. v2.0에서 nuPlan과 NAVSIM을 도입한 것은 이러한 한계를 극복하기 위한 긍정적인 시도이다.20</li>
</ol>
<pre><code class="language-mermaid">graph TD
    CASE["Long-tail 입력: &lt;br/&gt;기형적인 대형 트레일러의 급회전"]
    
    subgraph "UniAD 처리 과정의 실패 사례"
        CASE --&gt; PER["Perception Layer"]
        PER -- "Heading 각도 오추정&lt;br/&gt;(차량 방향 착각)" --&gt; ERR_Q["오염된 Query (Q_A)"]
        
        ERR_Q --&gt; PRED["MotionFormer"]
        PRED -- "물리적으로 불가능한 궤적 예측" --&gt; ERR_TRAJ["잘못된 점유 예측"]
        
        ERR_TRAJ --&gt; PLAN["Planner"]
        PLAN -- "회피 기동 실패 또는&lt;br/&gt;불필요한 급정거" --&gt; RESULT["주행 실패 (Failure)"]
    end
    
    style CASE fill:#f5c6cb,stroke:#721c24
    style RESULT fill:#f5c6cb,stroke:#721c24
</code></pre>
<h2>7. 결론</h2>
<p>UniAD는 자율 주행 기술의 발전에 있어 하나의 분기점이 되는 연구이다. 모듈식 파이프라인의 구조적 한계를 ’계획 중심(Planning-oriented)’이라는 철학과 통합된 쿼리 아키텍처로 극복함으로써, 자율 주행 AI가 나아가야 할 방향을 제시하였다.</p>
<p>본 연구 보고서를 통해 분석한 UniAD의 핵심 가치는 다음과 같다:</p>
<ul>
<li><strong>통합의 힘:</strong> 인지, 예측, 계획을 하나의 네트워크로 결합하여 정보 손실을 최소화하고 성능을 극대화하였다.</li>
<li><strong>상호작용의 모델링:</strong> 객체와 환경 간의 복잡한 상호작용을 어텐션 메커니즘으로 구현하여, 단순한 물리적 예측을 넘어 사회적 문맥을 이해하는 주행을 가능케 했다.</li>
<li><strong>지속적인 진화:</strong> v2.0 업데이트를 통해 최신 데이터셋과 평가 방법론을 수용하며, 실용적인 자율 주행 시스템을 향해 발전하고 있다.</li>
</ul>
<p>비록 높은 연산 비용과 실시간성 확보라는 과제가 남아있지만, UniAD가 입증한 엔드투엔드 학습의 가능성은 향후 VAD, SparseDrive 등 후속 연구들의 기반이 되었으며, 레벨 4 이상의 완전 자율 주행 시대를 앞당기는 데 중요한 기술적 토대가 될 것임이 분명하다. UniAD는 단순히 ‘보는’ AI를 넘어, 상황을 ‘이해하고 행동하는’ AI로의 진화를 보여주는 대표적인 사례로 기록될 것이다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>[2212.10156] Planning-oriented Autonomous Driving - arXiv, 12월 9, 2025에 액세스, https://arxiv.org/abs/2212.10156</li>
<li>End-to-end Autonomous Driving: Challenges and Frontiers - Andreas Geiger, 12월 9, 2025에 액세스, https://www.cvlibs.net/publications/Chen2024PAMI.pdf</li>
<li>SenseTime Wins the Best Paper Award at CVPR 2023 For Groundbreaking Perception-Decision Integrated Autonomous Driving Foundation Model, 12월 9, 2025에 액세스, https://www.sensetime.com/en/news-detail/51166787</li>
<li>CVPR 2023 Best Paper Award Winners Announced - The Computer Vision Foundation, 12월 9, 2025에 액세스, https://cvpr.thecvf.com/Conferences/2023/BestPaperAwards</li>
<li>Insights into Top Paper Nominee, “Planning-oriented Autonomous Driving” - CVPR, 12월 9, 2025에 액세스, https://cvpr.thecvf.com/Conferences/2023/AuthorQAAutonomous</li>
<li>[Quick Review] Planning-oriented Autonomous Driving - Liner, 12월 9, 2025에 액세스, https://liner.com/review/planningoriented-autonomous-driving</li>
<li>OpenDriveLab/UniAD: [CVPR 2023 Best Paper Award] Planning-oriented Autonomous Driving - GitHub, 12월 9, 2025에 액세스, https://github.com/OpenDriveLab/UniAD</li>
<li>arXiv:2212.10156v1 [cs.CV] 20 Dec 2022, 12월 9, 2025에 액세스, https://arxiv.org/pdf/2212.10156</li>
<li>Planning-Oriented Autonomous Driving - CVF Open Access, 12월 9, 2025에 액세스, https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Planning-Oriented_Autonomous_Driving_CVPR_2023_paper.pdf</li>
<li>Planning-oriented Autonomous Driving Supplementary Material - CVF Open Access, 12월 9, 2025에 액세스, https://openaccess.thecvf.com/content/CVPR2023/supplemental/Hu_Planning-Oriented_Autonomous_Driving_CVPR_2023_supplemental.pdf</li>
<li>Exploring Autonomous Driving, 12월 9, 2025에 액세스, <a href="https://vds.sogang.ac.kr/wp-content/uploads/2023/07/2023_%ED%95%98%EA%B3%84%EC%84%B8%EB%AF%B8%EB%82%98_%EA%B0%95%EB%B3%91%EC%9A%B0.pdf">https://vds.sogang.ac.kr/wp-content/uploads/2023/07/2023_%ED%95%98%EA%B3%84%EC%84%B8%EB%AF%B8%EB%82%98_%EA%B0%95%EB%B3%91%EC%9A%B0.pdf</a></li>
<li>Planning-oriented Autonomous Driving - OpenDriveLab, 12월 9, 2025에 액세스, https://opendrivelab.github.io/UniAD_plenary_talk_slides.pdf</li>
<li>UniAD: Foundational Model for End-to-End Autonomous Driving - Medium, 12월 9, 2025에 액세스, https://medium.com/axinc-ai/uniad-foundational-model-for-end-to-end-autonomous-driving-aa593496eb53</li>
<li>(PDF) Planning-oriented Autonomous Driving - ResearchGate, 12월 9, 2025에 액세스, https://www.researchgate.net/publication/373321358_Planning-oriented_Autonomous_Driving</li>
<li>GAD-Generative Learning for HD Map-Free Autonomous Driving - arXiv, 12월 9, 2025에 액세스, https://arxiv.org/html/2405.00515v3</li>
<li>End-to-End Autonomous Driving without Costly Modularization and 3D Manual Annotation, 12월 9, 2025에 액세스, https://arxiv.org/html/2406.17680v1</li>
<li>VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision, 12월 9, 2025에 액세스, https://arxiv.org/html/2412.14446v2</li>
<li>Uncertainty-Aware End-to-End Autonomous Driving Modeling, 12월 9, 2025에 액세스, https://ieeexplore.ieee.org/iel8/6488907/11261327/11153424.pdf</li>
<li>hustvl/VAD: [ICCV 2023] VAD: Vectorized Scene Representation for Efficient Autonomous Driving - GitHub, 12월 9, 2025에 액세스, https://github.com/hustvl/VAD</li>
<li>End-to-End Autonomous Driving without Costly Modularization and 3D Manual Annotation - OpenReview, 12월 9, 2025에 액세스, https://openreview.net/pdf/bed49d5f0786075d1eec82e7669974d7794ac992.pdf</li>
<li>nuCarla: A nuScenes-Style Bird’s-Eye View Perception Dataset for CARLA Simulation, 12월 9, 2025에 액세스, https://arxiv.org/html/2511.13744v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>