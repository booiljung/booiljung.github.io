<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:자율 주행을 위한 LLM 및 VLA 모델</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>자율 주행을 위한 LLM 및 VLA 모델</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">자율주행 (Autonomous Driving)</a> / <span>자율 주행을 위한 LLM 및 VLA 모델</span></nav>
                </div>
            </header>
            <article>
                <h1>자율 주행을 위한 LLM 및 VLA 모델</h1>
<p>2025-12-09, G30DR</p>
<h2>1.  서론: 인지 혁명에서 추론 혁명으로의 전환</h2>
<p>2025년 현재, 자율주행(Autonomous Driving, AD) 기술은 그야말로 변곡점을 지나고 있다. 지난 10년이 센서 융합(Sensor Fusion)과 딥러닝 기반의 인지(Perception) 능력을 고도화하여 차량이 “무엇을 보고 있는가“를 해결하는 데 집중했던 시기라면, 현재는 차량이 “무엇을 이해하고, 왜 그렇게 판단하는가“를 해결하는 ’추론(Reasoning) 혁명’의 시대로 정의할 수 있다. 이러한 변화의 중심에는 거대 언어 모델(Large Language Model, LLM)과 이를 시각 정보와 결합한 멀티모달 대형 모델(Multimodal Large Language Model, MLLM), 그리고 행동까지 생성해내는 VLA(Vision-Language-Action) 모델이 존재한다.1</p>
<p>과거의 자율주행 시스템은 주로 모듈형 아키텍처(Modular Architecture)를 따랐다. 이는 인지, 위치 추정, 예측, 계획, 제어의 각 단계가 명확히 분리되어 있어 개발과 디버깅이 용이했지만, 모듈 간 정보 전달 과정에서 발생하는 정보 손실과 오류 전파(Error Propagation) 문제, 그리고 복잡한 비정형 상황(Corner Case)에 대한 대응력 부족이라는 고질적인 한계를 안고 있었다. 반면, 최근 급부상한 LLM 기반의 접근 방식은 방대한 인터넷 텍스트 데이터와 주행 영상 데이터를 통해 학습한 ’세계 지식(World Knowledge)’과 ’상식(Common Sense)’을 자율주행 시스템에 주입함으로써, 기존 시스템이 해결하지 못했던 인간 수준의 상황 해석과 유연한 대처를 가능케 하고 있다.4</p>
<p>특히 2024년 말부터 2025년 초에 걸쳐 발표된 주요 연구 성과들은 자율주행 연구의 패러다임이 완전히 바뀌었음을 시사한다. NVIDIA의 Alpamayo-R1은 시각적 입력에서 직접적인 제어 명령을 생성하면서도 그 판단의 근거를 언어로 설명할 수 있는 VLA 모델의 가능성을 입증했으며 6, Waymo의 연구는 자율주행 모델의 성능이 데이터와 연산량에 비례하여 기하급수적으로 향상된다는 ’스케일링 법칙(Scaling Laws)’을 모션 플래닝 분야에서 확인시켜 주었다.5 또한 Tesla의 FSD v13은 엔드투엔드(End-to-End) 신경망에 월드 모델 개념을 통합하여 예측 능력을 비약적으로 향상시켰다.8</p>
<p>본 보고서는 2025년 현재 자율주행 산업과 학계에서 논의되고 있는 LLM의 응용 현황을 포괄적이고 심층적으로 분석한다. LLM이 기존 자율주행 아키텍처를 어떻게 재편하고 있는지, 시뮬레이션과 데이터 생성에는 어떤 혁신을 가져왔는지, 그리고 실시간 제어와 안전성 검증이라는 난제들을 어떻게 극복해 나가고 있는지를 상세히 기술한다. 나아가 주요 선도 기업들의 기술 전략을 비교 분석하고, 향후 자율주행 기술이 나아가야 할 방향성을 제시한다.</p>
<h2>2.  자율주행 아키텍처의 진화와 LLM의 역할</h2>
<p>자율주행 시스템의 아키텍처는 기술의 발전 단계에 따라 크게 세 가지 형태로 진화해왔으며, 2025년 현재 LLM은 이 모든 아키텍처에 깊숙이 침투하여 그 기능을 확장시키고 있다.</p>
<h3>2.1  기존 모듈형 아키텍처의 한계와 LLM의 보완</h3>
<p>전통적인 자율주행 스택은 인지(Perception), 예측(Prediction), 계획(Planning), 제어(Control)가 순차적으로 연결된 파이프라인 구조를 가진다. 이 구조에서 각 모듈은 고도로 전문화되어 있으나, 상호 간의 소통은 제한적인 수치 데이터(예: 객체의 좌표, 속도 벡터)로만 이루어진다. 이로 인해 상위 레벨의 맥락 정보가 하위 제어 단계로 전달되지 못하거나, 반대로 제어 단계의 물리적 제약이 인지 단계에 피드백되지 못하는 문제가 발생한다.</p>
<p>LLM은 이러한 모듈형 시스템 내에서 다음과 같은 핵심적인 역할을 수행하며 기존의 한계를 보완한다.</p>
<ol>
<li><strong>고차원 의사결정 지원 (High-Level Decision Making):</strong> 기존의 규칙 기반(Rule-based) 플래너는 수천, 수만 가지의 교통 상황을 일일이 코드로 정의해야 했다. 하지만 LLM은 자연어로 기술된 교통 법규나 사회적 관습을 이해하고 이를 주행 정책에 반영할 수 있다. 예를 들어, “경찰관이 수신호로 통행을 지시할 때는 신호등보다 수신호가 우선한다“는 복잡한 규칙을 LLM은 별도의 하드코딩 없이도 이해하고 판단할 수 있다.4</li>
<li><strong>의미론적 가교 (Semantic Bridge):</strong> LLM은 센서 데이터에서 추출한 객체 정보를 텍스트 기반의 의미론적 정보로 변환하여 모듈 간의 소통을 돕는다. ‘LeAD(LLM-empowered Autonomous Driving)’ 프레임워크는 양방향 자연어 인코더-디코더를 사용하여 인지 모듈의 출력을 언어로 변환하고, 이를 LLM이 추론한 뒤 다시 모션 플래닝을 위한 웨이포인트(Waypoint) 좌표로 변환하는 방식을 제안했다.4 이는 수치 데이터만으로는 표현하기 힘든 ’의도(Intent)’나 ’맥락(Context)’을 시스템 전반에 전파하는 데 기여한다.</li>
</ol>
<h3>2.2  종단간(End-to-End) 자율주행의 부상</h3>
<p>종단간 자율주행은 센서 입력부터 제어 출력까지를 하나의 거대 신경망으로 처리하는 방식이다. 이 방식은 모듈 간의 인터페이스 정의가 필요 없고, 데이터 기반으로 전체 시스템을 최적화할 수 있다는 장점이 있다. 특히 2025년에는 트랜스포머(Transformer) 아키텍처의 발전과 함께 이 방식이 주류로 부상하고 있다.</p>
<ul>
<li><strong>통합된 특징 추출:</strong> 기존에는 차선 인식용 모델, 차량 검출용 모델, 신호등 인식용 모델이 따로 존재했지만, 종단간 모델은 이를 통합하여 처리한다. 이는 시스템의 복잡도를 낮추고 연산 효율을 높인다. Tesla의 FSD v12 이후 버전이 대표적인 사례로, “광자(Photons)에서 제어(Controls)까지“라는 슬로건 아래 거의 모든 주행 코드를 신경망으로 대체했다.9</li>
<li><strong>데이터 스케일링의 이점:</strong> Waymo의 연구에 따르면, 종단간 모델의 성능은 학습 데이터의 양과 모델의 파라미터 수에 비례하여 멱법칙(Power Law)을 따르며 향상된다.5 이는 LLM 분야에서 관찰된 현상이 자율주행 도메인에서도 유효함을 의미하며, 향후 자율주행 성능 향상을 위해서는 더 큰 모델과 더 많은 데이터가 필수적임을 시사한다.</li>
</ul>
<h3>2.3  차세대 패러다임: VLA (Vision-Language-Action) 모델</h3>
<p>2025년 자율주행 연구의 최전선에는 시각(Vision)과 언어(Language)를 이해하고 이를 바탕으로 행동(Action)을 직접 생성하는 VLA 모델이 있다.3 VLA 모델은 단순히 이미지를 보고 핸들을 꺾는 것을 넘어, 왜 그런 행동을 해야 하는지에 대한 논리적 추론 과정을 내재화한다.</p>
<h4>2.3.1  VLA 모델의 아키텍처와 작동 원리</h4>
<p>VLA 모델은 대규모 언어 모델(LLM)을 두뇌로 사용하되, 시각적 입력을 처리할 수 있는 시각 인코더(Visual Encoder)와 물리적 행동을 출력할 수 있는 액션 헤드(Action Head)를 결합한 형태다.</p>
<ol>
<li><strong>멀티모달 토큰화 (Multimodal Tokenization):</strong> VLA 모델은 텍스트뿐만 아니라 이미지, 비디오, 그리고 차량의 제어 신호까지도 모두 ’토큰(Token)’으로 변환하여 처리한다. 예를 들어, 전방 카메라 영상은 ViT(Vision Transformer)를 통해 일련의 패치 토큰으로 변환되고, “좌회전하라“는 명령어는 텍스트 토큰으로 변환된다.13</li>
<li><strong>동작의 언어화 (Action as Language):</strong> 가장 혁신적인 부분은 차량의 연속적인 제어 값(조향각, 가속도 등)을 이산적인(Discrete) 토큰으로 취급한다는 점이다. 일반적인 방식은 제어 값을 256개 정도의 구간(Bin)으로 양자화(Quantization)하여 LLM의 어휘 사전(Vocabulary)에 추가하는 것이다. 이렇게 하면 LLM은 “전방에”, “장애물이”, “있으므로”, “[브레이크_강하게]”, “[핸들_좌로_5도]“와 같이 텍스트와 행동을 자연스럽게 섞어서 생성할 수 있다.13</li>
<li><strong>FAST+ 토큰화:</strong> 단순한 양자화는 정밀한 제어를 어렵게 하거나 토큰 길이를 불필요하게 늘릴 수 있다. 이를 개선하기 위해 2025년 발표된 FAST+ 기법은 이산 코사인 변환(DCT)을 사용하여 동작의 주파수 성분을 분석하고, 이를 통해 소수의 토큰만으로도 복잡하고 부드러운 궤적을 표현하는 방법을 제안했다.13 이는 VLA 모델의 추론 속도와 제어 정밀도를 동시에 높이는 핵심 기술로 평가받는다.</li>
</ol>
<h4>2.3.2  NVIDIA Alpamayo-R1: VLA의 결정체</h4>
<p>NVIDIA가 공개한 Alpamayo-R1은 VLA 아키텍처의 정점을 보여주는 모델이다.6</p>
<ul>
<li><strong>모델 구성:</strong> 82억 파라미터(8.2B)의 Cosmos-Reason VLM 백본과 23억 파라미터(2.3B)의 액션 디코더로 구성되어 있다.</li>
<li><strong>인과적 추론 (Chain of Causation):</strong> 이 모델은 단순히 입력에 대한 출력을 매핑하는 것이 아니라, ’인과적 추론(Chain of Causation, CoC)’이라는 중간 단계를 거친다. 시스템은 먼저 상황을 인식하고(Perception), 그 상황이 발생한 원인과 잠재적 결과를 추론(Reasoning)한 뒤, 최종적으로 행동(Action)을 결정한다. 예를 들어, “전방 차량이 감속함(관찰) <span class="math math-inline">\rightarrow</span> 전방에 횡단보도가 있고 보행자가 진입 중임(원인 분석) <span class="math math-inline">\rightarrow</span> 안전 거리를 확보해야 함(판단) <span class="math math-inline">\rightarrow</span> 감속 제어 수행(행동)“과 같은 식이다.7</li>
<li><strong>성과:</strong> Alpamayo-R1은 이러한 추론 과정을 통해 기존 엔드투엔드 모델 대비 코너 케이스 대응 능력을 30% 이상 향상시켰으며, 판단의 근거를 인간이 이해할 수 있는 언어로 설명할 수 있다는 점에서 ’설명 가능한 자율주행(Explainable AD)’의 가능성을 열었다.7</li>
</ul>
<h2>3.  생성형 AI와 월드 모델을 이용한 시뮬레이션 혁신</h2>
<p>자율주행 상용화의 가장 큰 걸림돌 중 하나는 ’데이터의 희소성(Data Scarcity)’이다. 일반적인 주행 데이터는 넘쳐나지만, 사고가 발생하거나 극한의 날씨 상황 등 AI가 반드시 학습해야 할 ‘엣지 케이스(Edge Case)’ 데이터는 턱없이 부족하다. 2025년, 생성형 AI와 LLM은 이 문제를 해결하는 게임 체인저로 등장했다.</p>
<h3>3.1  텍스트 기반 시나리오 생성 (Text-to-Scenario)</h3>
<p>과거의 시뮬레이터는 엔지니어가 일일이 차량의 위치와 속도, 날씨 등을 설정해야 했다. 하지만 LLM의 등장으로 자연어 명령만으로 복잡한 시나리오를 생성하는 것이 가능해졌다.</p>
<ul>
<li><strong>DriveDreamer-2 &amp; DriveDreamer4D:</strong> 이 모델들은 “비 오는 밤, 무단 횡단하는 보행자가 있는 교차로를 생성해줘“와 같은 텍스트 프롬프트를 입력받으면, 이를 고해상도의 3D 시뮬레이션 환경이나 4D 비디오로 구현해낸다.16 DriveDreamer-2는 LLM을 활용해 사용자 프롬프트를 구체적인 교통 객체의 배치와 이동 경로(Trajectory)로 변환하고, 이를 다시 확산 모델(Diffusion Model)에 입력하여 사실적인 주행 영상을 생성한다. 실험 결과, 이 모델이 생성한 비디오의 품질(FID/FVD 점수)은 기존 최고 수준 모델 대비 약 30~50% 향상되었으며, 이를 학습 데이터로 활용했을 때 3D 객체 검출 성능도 크게 개선되었다.17</li>
<li><strong>생성 과정의 구체성:</strong> DriveDreamer-2는 두 단계의 훈련 과정을 거친다. 첫 번째 단계에서는 구조화된 교통 제약 조건(Traffic Constraints)을 이해하고, 두 번째 단계에서는 미래 상태를 예측하는 능력을 학습한다. 이를 통해 생성된 시나리오는 단순히 시각적으로만 리얼한 것이 아니라, 교통 법규와 물리 법칙을 준수하는 논리적인 정합성을 갖춘다.19</li>
</ul>
<h3>3.2  코너 케이스 자동 생성 및 검증 (ScenarioFuzz-LLM)</h3>
<p>안전성 검증을 위해 LLM이 스스로 시스템을 공격(Attack)하는 시나리오를 생성하기도 한다.</p>
<ul>
<li><strong>ScenarioFuzz-LLM:</strong> 이 연구는 유전 알고리즘(Genetic Algorithm)과 LLM을 결합하여 자율주행 시스템이 실패할 법한 시나리오를 진화적으로 생성한다.20 LLM은 “차선 변경 시점에 맞춰 옆 차선 차량을 가속시키라“거나 “터널 출구의 역광 상황에서 전방 차량을 급정거시키라“는 식의 교묘한 시나리오를 설계한다. 실험 결과, 이 방식은 기존 방법보다 10.51% 더 많은 충돌/위반 사례를 찾아냈으며, 이전에는 발견되지 않았던 24개의 새로운 결함을 찾아내는 성과를 거두었다.20</li>
<li><strong>AutoScenario:</strong> 실제 사고 보고서나 텍스트 데이터를 입력받아 이를 시뮬레이션 내에서 재현 가능한 스크립트(예: CARLA 또는 SUMO용 코드)로 변환하는 기술도 발전했다. 이를 통해 뉴스 기사로만 접했던 사고 상황을 가상 공간에서 즉시 재현하고 테스트할 수 있게 되었다.22</li>
</ul>
<h3>3.3  월드 모델 (World Models)의 구현</h3>
<p>월드 모델은 자율주행 시스템이 세상이 작동하는 방식(물리 법칙, 인과 관계)을 내재화한 모델이다. 이는 단순히 다음 프레임을 예측하는 비디오 생성을 넘어, “만약 내가 여기서 핸들을 왼쪽으로 꺾으면 무슨 일이 일어날까?“라는 반사실적(Counterfactual) 질문에 답할 수 있게 한다.</p>
<ul>
<li><strong>Tesla의 월드 모델:</strong> Tesla FSD v13에 통합된 월드 모델은 주변 차량과 보행자의 미래 행동을 시뮬레이션하고, 이를 바탕으로 최적의 경로를 계획한다. 이는 외부의 물리 엔진 없이 신경망 내부에서 가상의 시뮬레이션을 돌리는 것과 같다.8</li>
<li><strong>Waymo의 접근:</strong> Waymo 또한 방대한 주행 데이터를 통해 학습된 월드 모델을 활용하여, 실제 도로에서 발생할 수 있는 수만 가지의 변수를 미리 예측하고 대응하는 훈련을 수행한다.5</li>
</ul>
<h2>4.  LLM의 핵심 능력: 추론(Reasoning)과 계획(Planning)</h2>
<p>자율주행의 레벨을 4, 5단계로 끌어올리기 위해서는 ’운전 기술’보다 ’운전 지능’이 필요하다. LLM은 이 지능의 핵심인 추론과 계획 능력에서 독보적인 가치를 제공한다.</p>
<h3>4.1  인과적 추론과 상황 설명 (Chain of Causation)</h3>
<p>NVIDIA의 Alpamayo-R1에서 강조된 ’Chain of Causation(CoC)’은 자율주행의 의사결정을 투명하게 만든다. 기존 딥러닝 모델은 “전방에 차가 있어서 멈췄다“는 단순한 인과만 학습했다면, CoC 기반 모델은 다음과 같은 다단계 추론을 수행한다.</p>
<ol>
<li><strong>관측(Observation):</strong> “전방 교차로 신호등이 황색으로 바뀌었고, 선행 차량의 브레이크등이 켜졌다.”</li>
<li><strong>추론(Reasoning):</strong> “선행 차량은 정지할 의도로 보인다. 현재 내 속도로는 교차로 진입 전 정지가 가능하다. 무리하게 진입 시 선행 차량과 추돌하거나 신호 위반 위험이 있다.”</li>
<li><strong>결정(Decision):</strong> “감속하여 정지선에 멈춘다.”</li>
<li><strong>행동(Action):</strong> “[감속_0.3g], [브레이크_페달_전개]”</li>
</ol>
<p>이러한 추론 과정은 사고 발생 시 원인 규명을 용이하게 할 뿐만 아니라, 모델이 데이터의 상관관계(Correlation)가 아닌 인과관계(Causation)를 학습하게 하여 낯선 환경에서의 일반화 능력을 극대화한다.6</p>
<h3>4.2  사회적 상호작용과 협상 (Social Interaction)</h3>
<p>도로 위 운전은 다른 운전자와의 끊임없는 소통과 협상이다. 끼어들기, 비보호 좌회전, 좁은 골목길 교행 등의 상황에서 LLM은 게임 이론(Game Theory)적 접근과 인간 심리 이해를 결합하여 최적의 행동을 도출한다.</p>
<ul>
<li><strong>협상 능력:</strong> LLM+Debrief 연구는 차량 간 통신(V2V)에 자연어를 사용하여 서로의 의도를 명확히 전달하고 협상하는 메커니즘을 제안했다.23 예를 들어 “제가 먼저 진입하겠습니다“라고 메시지를 보내거나, 상대방의 양보에 “감사합니다“라는 신호를 보내는 식이다. 이는 단순한 신호 교환을 넘어선 고차원적인 협력을 가능케 한다.</li>
<li><strong>비언어적 신호 해석:</strong> VLA 모델은 보행자의 시선, 제스처, 혹은 다른 차량의 미세한 움직임(Nudge)을 통해 상대방의 의도를 파악한다. 보행자가 스마트폰을 보고 있는지, 차량을 주시하고 있는지를 구별하여 멈출지 지나갈지를 결정하는 섬세한 판단이 가능해졌다.3</li>
</ul>
<h3>4.3  장기 계획 (Long-horizon Planning)</h3>
<p>단기적인 충돌 회피뿐만 아니라, 목적지까지의 전체 경로를 고려한 장기적인 전략 수립에도 LLM이 활용된다. “목적지 부근 주차장이 혼잡하니 미리 인근 공영주차장으로 안내해“와 같은 복합적인 판단이나, “배터리 잔량과 교통 체증을 고려할 때 이 경로가 최적이다“라는 전략적 조언을 수행한다.24</p>
<h2>5.  인간-차량 상호작용(HMI)과 설명 가능성</h2>
<p>자율주행차가 운전의 주도권을 가져갈수록, 탑승자는 차량이 무엇을 보고 있고 왜 그렇게 행동하는지 알고 싶어 한다. LLM은 차량과 탑승자 사이의 소통 장벽을 허무는 핵심 인터페이스가 된다.</p>
<h3>5.1  자연어 인터페이스와 시각적 접지 (Visual Grounding)</h3>
<ul>
<li><strong>Talk2Car 데이터셋의 진화:</strong> Talk2Car는 자율주행차에 자연어로 명령을 내리는 시나리오를 연구하기 위한 데이터셋이다. “저기 빨간 차 뒤에 주차해줘“라는 명령을 내리면, 시스템은 ‘저기’, ‘빨간 차’, ’뒤’라는 언어적 표현을 실제 카메라 영상 속의 픽셀 좌표 및 3D 공간 좌표와 매칭(Visual Grounding)시켜야 한다.26 2025년 업데이트된 Talk2Car-Trajectory는 단순한 객체 인식을 넘어, 명령 수행을 위한 구체적인 궤적 생성 능력까지 평가한다.27</li>
<li><strong>지능형 콕핏 (Smart Cockpit):</strong> Li Auto의 ’Mind GPT’와 ’Lixiang Tongxue’는 차량 내 모든 기능을 대화형 AI로 통합했다. 사용자는 복잡한 메뉴를 찾을 필요 없이 “아기가 자고 있으니 조용히 운전해줘“라고 말하면, 차량은 서스펜션을 부드럽게 조정하고, 음악 볼륨을 낮추며, 급가속을 제한하는 일련의 조치를 자동으로 수행한다.28</li>
</ul>
<h3>5.2  설명 가능한 AI (XAI)로서의 역할</h3>
<p>자율주행의 ‘블랙박스’ 문제는 신뢰성 저하의 주원인이다. LLM은 자신의 판단 근거를 실시간으로 설명함으로써 이 문제를 해결한다.</p>
<ul>
<li><strong>실시간 대화 로그:</strong> 연구에 따르면, 자율주행차가 위험 상황에서 “좌측 사각지대 차량 감지로 인해 차선 변경을 취소했습니다“라고 즉각적으로 설명할 때 탑승자의 불안감이 현저히 낮아지는 것으로 나타났다.30</li>
<li><strong>디버깅 도구:</strong> 개발자 입장에서도 LLM이 생성한 추론 로그는 시스템 오류의 원인을 파악하는 데 매우 유용한 도구가 된다. 수치 데이터만으로는 알 수 없었던 ’판단 미스’의 논리적 배경을 확인할 수 있기 때문이다.31</li>
</ul>
<h2>6.  하드웨어 및 구현 기술: 엣지(Edge)에서의 도전</h2>
<p>LLM과 VLA 모델은 막대한 연산 자원을 필요로 한다. 이를 제한된 전력과 공간을 가진 차량 내 컴퓨터(Edge Device)에서 실시간으로 구동하는 것은 2025년 하드웨어 기술의 최대 과제다.</p>
<h3>6.1  차량용 AI 칩셋의 진화</h3>
<p>주요 반도체 기업들은 LLM 구동에 최적화된 차세대 SoC(System on Chip)를 경쟁적으로 출시하고 있다.</p>
<ul>
<li><strong>NVIDIA DRIVE Thor:</strong> 블랙웰(Blackwell) 아키텍처 기반의 Thor 칩셋은 생성형 AI 엔진을 내장하여 무려 1,000 TFLOPS(테라플롭스)의 연산 성능을 제공한다.32 이는 기존 Orin 칩셋 대비 4배 이상의 성능이며, 특히 트랜스포머 모델의 추론 속도를 비약적으로 높였다. 또한 FP8(8비트 부동소수점) 및 FP4(4비트) 정밀도를 지원하여 거대 모델을 효율적으로 처리한다.</li>
<li><strong>Mobileye EyeQ7:</strong> Mobileye는 2027년 양산을 목표로 EyeQ7 칩셋을 개발 중이며, 이는 5나노 공정 기반으로 전력 효율성을 극대화하면서도 레벨 4/5 자율주행에 필요한 연산 능력을 제공할 예정이다.34 Mobileye는 범용 GPU보다는 자사의 알고리즘에 특화된 가속기를 통해 효율을 높이는 전략을 취한다.</li>
<li><strong>Tesla FSD Chip:</strong> Tesla는 자체 설계한 AI 가속기를 통해 하드웨어와 소프트웨어의 완벽한 통합을 추구한다. 특히 HW4(AI4) 하드웨어는 이전 세대보다 훨씬 높은 처리량으로 VLA 모델의 실시간 추론을 뒷받침한다.35</li>
</ul>
<h3>6.2  모델 경량화 기술: 양자화(Quantization)와 가지치기(Pruning)</h3>
<p>하드웨어의 발전만으로는 수백억 파라미터의 모델을 감당하기 어렵다. 소프트웨어 차원의 최적화 기술이 필수적이다.</p>
<ul>
<li><strong>4비트 양자화 (FP4/INT4):</strong> NVIDIA의 DriveOS LLM SDK는 최신 LLM을 4비트로 압축(Quantization)하여 구동하는 기능을 지원한다.37 일반적으로 모델 비트 수를 16비트에서 4비트로 줄이면 메모리 사용량은 1/4로 줄고 추론 속도는 2~4배 빨라진다. 연구에 따르면 적절한 양자화 기법을 사용하면 정확도 손실(Degradation)을 1% 미만으로 억제할 수 있다.</li>
<li><strong>가지치기 (Pruning):</strong> VLA 모델에서 주행 판단에 기여도가 낮은 뉴런이나 레이어를 제거하는 가지치기 기법도 활발히 연구되고 있다. 특히 비전 인코더와 언어 디코더 각각의 특성에 맞는 차별화된 가지치기 전략이 엣지 디바이스 적용의 핵심이다.39</li>
</ul>
<table><thead><tr><th><strong>기술</strong></th><th><strong>주요 내용</strong></th><th><strong>기대 효과</strong></th><th><strong>관련 제품/연구</strong></th></tr></thead><tbody>
<tr><td><strong>FP4 Quantization</strong></td><td>모델 가중치를 4비트로 압축</td><td>메모리 절감, 추론 가속</td><td>NVIDIA DriveOS SDK 38</td></tr>
<tr><td><strong>Architecture Search</strong></td><td>하드웨어에 최적화된 모델 구조 탐색</td><td>연산 효율 극대화</td><td>Mobileye EyeQ 41</td></tr>
<tr><td><strong>Token Merging</strong></td><td>유사한 비주얼 토큰을 병합</td><td>트랜스포머 연산량 감소</td><td>VLA 최적화 연구 42</td></tr>
</tbody></table>
<h2>7.  안전성, 윤리, 그리고 규제</h2>
<p>LLM의 도입은 ’환각(Hallucination)’이라는 새로운 유형의 안전 문제를 야기한다. AI가 없는 신호등을 보거나, 잘못된 교통 법규를 사실인 양 판단할 수 있기 때문이다.</p>
<h3>7.1  환각 제어 및 안전성 검증</h3>
<ul>
<li><strong>RAG (Retrieval-Augmented Generation):</strong> LLM이 학습된 지식에만 의존하지 않고, 정밀 지도(HD Map)나 실시간 교통 정보를 외부 데이터베이스에서 검색(Retrieval)하여 판단의 근거로 삼게 한다.43 이는 최신성 부족이나 정보 왜곡 문제를 완화한다.</li>
<li><strong>RSS (Responsibility-Sensitive Safety):</strong> Mobileye가 주창한 RSS는 AI의 판단을 검증하는 수학적 안전 모델이다. LLM이 아무리 창의적인 경로를 생성하더라도, RSS가 정의한 물리적 안전 거리나 통행 우선순위를 위반하면 그 명령은 실행되지 않는다.44 이는 확률적 모델인 LLM의 불확실성을 확정적(Deterministic) 모델로 제어하는 ‘이중 안전장치(Safety Cocoon)’ 역할을 한다.</li>
<li><strong>RLHF (Reinforcement Learning from Human Feedback):</strong> 인간 운전자의 피드백을 통해 모델을 미세 조정(Fine-tuning)하여, 안전하고 편안한 운전 스타일을 학습시킨다. 이는 AI가 기술적으로는 가능하지만 승객에게 불안감을 주는 급조작을 하지 않도록 억제한다.46</li>
</ul>
<h3>7.2  윤리적 딜레마와 규제 동향</h3>
<ul>
<li><strong>트롤리 딜레마의 현실화:</strong> 사고가 불가피한 상황에서 AI가 어떤 판단을 내려야 하는지에 대한 윤리적 기준이 필요하다. LLM은 이러한 상황에서 윤리적 가이드라인에 따라 피해를 최소화하는 방향으로 판단하도록 프롬프트 엔지니어링이나 학습 데이터 조정을 거친다.</li>
<li><strong>규제 표준화:</strong> ISO 26262(기능 안전)나 ISO 21448(SOTIF)과 같은 기존 표준에 LLM과 같은 AI 모델의 안전성을 평가하는 항목이 추가되고 있다. 유럽과 미국, 중국 등 주요 국가들은 자율주행 AI의 설명 가능성과 데이터 투명성을 요구하는 법안을 강화하는 추세다.47</li>
</ul>
<h2>8.  주요 기업별 전략 비교 및 사례 분석</h2>
<p>2025년 자율주행 시장은 기술 철학에 따라 크게 세 그룹으로 나뉜다.</p>
<h3>8.1  통합형 거대 모델 진영 (Tesla, NVIDIA)</h3>
<p>이들은 막대한 컴퓨팅 파워와 데이터를 앞세워 End-to-End 및 VLA 모델을 주도한다.</p>
<ul>
<li><strong>Tesla:</strong> ’리얼 월드 AI’를 표방하며, 수백만 대의 차량에서 수집된 비디오 데이터를 통해 가장 강력한 비전 기반 월드 모델을 구축했다. FSD v13은 이러한 월드 모델을 통해 복잡한 도심 주행을 해결하고 있다.8</li>
<li><strong>NVIDIA:</strong> ‘AI 파운드리’ 전략을 통해 하드웨어(Thor)부터 플랫폼(DriveOS), 모델(Alpamayo-R1)까지 풀스택을 제공한다. 특히 Alpamayo-R1을 오픈소스로 공개하며 생태계 확장을 노리고 있다.49</li>
</ul>
<h3>8.2  하이브리드/검증 중시 진영 (Mobileye, Waymo)</h3>
<p>안전성과 검증 가능성을 최우선으로 하며, LLM을 전체 시스템의 일부(Compound AI)로 활용한다.</p>
<ul>
<li><strong>Mobileye:</strong> ‘Compound AI Systems (CAIS)’ 철학을 고수한다. 하나의 거대 모델에 의존하기보다는 인지, 매핑, 정책 등 각 분야에 특화된 모델을 조합하고 이를 RSS로 통제하여 높은 신뢰성(MTBF)을 확보한다.45</li>
<li><strong>Waymo:</strong> 스케일링 법칙을 연구하며 모델 크기를 키우고 있지만, 여전히 LiDAR와 지도 등 다양한 센서 융합을 중시하며 안전 중심의 보수적인 접근을 유지한다.5</li>
</ul>
<h3>8.3  서비스 및 사용자 경험 중심 진영 (Baidu, Li Auto)</h3>
<p>중국 기업들은 로보택시 서비스와 차량 내 엔터테인먼트 경험 향상에 LLM을 적극 도입한다.</p>
<ul>
<li><strong>Baidu Apollo:</strong> 세계 최대 규모의 로보택시 서비스를 운영하며, VLM 기술을 통해 무인 주행의 안정성을 높이고 있다.53</li>
<li><strong>Li Auto:</strong> Mind GPT를 통해 차량을 ’움직이는 집’으로 만든다. 자율주행 능력 강화(DriveVLM)와 동시에 탑승자 경험(Lixiang Tongxue) 혁신에 집중한다.28</li>
</ul>
<h2>9.  결론: 자율주행의 새로운 표준</h2>
<p>2025년, LLM은 자율주행 기술의 주변부에서 중심으로 이동했다. 시뮬레이션 데이터 생성부터 실시간 주행 판단, 그리고 탑승자와의 소통에 이르기까지 LLM이 닿지 않는 곳이 없다.</p>
<p>본 보고서의 분석을 통해 도출된 핵심 결론은 다음과 같다.</p>
<p>첫째, 자율주행은 ’코딩’에서 ’학습’의 영역으로 완전히 넘어갔다. 규칙을 작성하는 것보다 양질의 데이터를 수집하고 모델을 키우는 것이 성능 향상의 지름길임이 입증되었다.</p>
<p>둘째, 설명 가능성은 선택이 아닌 필수다. VLA 모델의 인과적 추론 능력은 AI 운전자에 대한 사회적 수용성을 높이는 핵심 열쇠가 될 것이다.</p>
<p>셋째, 하드웨어와 소프트웨어의 공진화(Co-evolution)가 가속화된다. 4비트 양자화, 트랜스포머 전용 가속기 등 LLM을 엣지에서 돌리기 위한 기술 혁신이 자율주행 상용화 시기를 앞당길 것이다.</p>
<p>앞으로의 자율주행은 단순한 무인 이동 수단을 넘어, 인간을 이해하고 세상과 소통하는 지능형 모빌리티 로봇으로 진화할 것이다. LLM은 그 진화의 가장 강력한 엔진이다.</p>
<h2>10. 참고 자료</h2>
<ol>
<li>Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis - arXiv, https://arxiv.org/html/2506.11526v1</li>
<li>[2506.24044] A Survey on Vision-Language-Action Models for Autonomous Driving - arXiv, https://arxiv.org/abs/2506.24044</li>
<li>A Survey on Vision-Language-Action Models for Autonomous … - arXiv, https://arxiv.org/pdf/2506.24044</li>
<li>LeAD: The LLM Enhanced Planning System Converged with End-to-end Autonomous Driving - arXiv, https://arxiv.org/html/2507.05754v1</li>
<li>New Insights for Scaling Laws in Autonomous Driving - Waymo, https://waymo.com/blog/2025/06/scaling-laws-in-autonomous-driving</li>
<li>nvidia/Alpamayo-R1-10B - Hugging Face, https://huggingface.co/nvidia/Alpamayo-R1-10B</li>
<li>Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail - Research at NVIDIA, https://research.nvidia.com/publication/2025-10_alpamayo-r1</li>
<li>A Deep Dive into Tesla FSD V13 and the New Era of Autonomous Driving - TESMAG, https://www.teslaacessories.com/blogs/news/a-deep-dive-into-tesla-fsd-v13-and-the-new-era-of-autonomous-driving</li>
<li>Tesla VP explains why end-to-end AI is the future of self-driving - Teslarati, https://www.teslarati.com/tesla-vp-explains-why-end-to-end-ai-future-self-driving/</li>
<li>Autonomous decisions: The bias-variance tradeoff in self-driving technology | Mobileye Blog, https://www.mobileye.com/blog/autonomous-decisions-the-bias-variance-tradeoff-in-self-driving-technology/</li>
<li>A Survey on Vision-Language-Action Models for Autonomous Driving - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2025W/WDFM-AD/papers/Jiang_A_Survey_on_Vision-Language-Action_Models_for_Autonomous_Driving_ICCVW_2025_paper.pdf</li>
<li>A Survey on Vision-Language-Action Models for Autonomous Driving - ICCV 2025 Open Access Repository, https://openaccess.thecvf.com/content/ICCV2025W/WDFM-AD/html/Jiang_A_Survey_on_Vision-Language-Action_Models_for_Autonomous_Driving_ICCVW_2025_paper.html</li>
<li>10 Open Challenges Steering the Future of Vision-Language-Action Models - arXiv, https://arxiv.org/html/2511.05936v1</li>
<li>Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail - arXiv, https://arxiv.org/html/2511.00088v1</li>
<li>NVIDIA First Open-Sources Autonomous Driving VLA, Robotaxi Enters “Android Era”; Product by All-Chinese Team Led by WU Xinzhou - 36氪, https://eu.36kr.com/en/p/3578018857958537</li>
<li>A Survey of World Models for Autonomous Driving - arXiv, https://arxiv.org/html/2501.11260v4</li>
<li>DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation - arXiv, https://arxiv.org/html/2403.06845v1</li>
<li>DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation, https://ojs.aaai.org/index.php/AAAI/article/view/33130</li>
<li>DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving - GitHub, https://github.com/JeffWang987/DriveDreamer</li>
<li>ScenarioFuzz-LLM: Enhancing Diversity in Autonomous Driving Scenario Fuzzing with LLMs - IEEE Xplore, https://ieeexplore.ieee.org/document/11033362</li>
<li>ldegao/ScenarioFuzz-LLM - GitHub, https://github.com/ldegao/ScenarioFuzz-LLM</li>
<li>Realistic Corner Case Generation for Autonomous Vehicles with Multimodal Large Language Model - arXiv, https://arxiv.org/html/2412.00243v1</li>
<li>Talking Vehicles: Cooperative Driving via Natural Language - OpenReview, https://openreview.net/forum?id=VYlfoA8I6A</li>
<li>A survey of decision-making and planning methods for self-driving vehicles - Frontiers, https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2025.1451923/full</li>
<li>Applications of Large Language Models and Multimodal Large Models in Autonomous Driving: A Comprehensive Review - MDPI, https://www.mdpi.com/2504-446X/9/4/238</li>
<li>Talk2Car: Taking Control of Your Self-Driving Car, https://talk2car.github.io/</li>
<li>The official Talk2Car-Trajectory repo - GitHub, https://github.com/ThierryDeruyttere/Talk2Car-Trajectory</li>
<li>AI Lixiang Tongxue - Li Auto, https://liauto.ph/li-ai/ai-lixiang-tongxue-25.html</li>
<li>Li Auto Inc. Launches Li L6, A Five-Seat Premium Family SUV, https://ir.lixiang.com/news-releases/news-release-details/li-auto-inc-launches-li-l6-five-seat-premium-family-suv</li>
<li>Transcript of the conversation between the car and driver with classification of the driver’s emotional state by the human listener. 4.40 [car] What do you think about the driving conditions? - ResearchGate, https://www.researchgate.net/figure/Transcript-of-the-conversation-between-the-car-and-driver-with-classification-of-the_tbl2_221332168</li>
<li>Thinklab-SJTU/Awesome-LLM4AD: A curated list of awesome LLM/VLM/VLA for Autonomous Driving(LLM4AD) resources (continually updated) - GitHub, https://github.com/Thinklab-SJTU/Awesome-LLM4AD</li>
<li>NVIDIA DRIVE Powers Next Generation of Transportation — From Cars and Trucks to Robotaxis and Autonomous Delivery Vehicles - NVIDIA Investor Relations, https://investor.nvidia.com/news/press-release-details/2024/NVIDIA-DRIVE-Powers-Next-Generation-of-Transportation–From-Cars-and-Trucks-to-Robotaxis-and-Autonomous-Delivery-Vehicles/default.aspx</li>
<li>Accelerate Autonomous Vehicle Development with the NVIDIA DRIVE AGX Thor Developer Kit | NVIDIA Technical Blog, https://developer.nvidia.com/blog/accelerate-autonomous-vehicle-development-with-the-nvidia-drive-agx-thor-developer-kit/</li>
<li>Mobileye - Wikipedia, https://en.wikipedia.org/wiki/Mobileye</li>
<li>AI &amp; Robotics | Tesla, https://www.tesla.com/AI</li>
<li>Tesla FSD v13: Autonomous Driving AI Milestone - AI CERTs, https://www.aicerts.ai/news/tesla-fsd-v13-autonomous-driving-ai-milestone/</li>
<li>Announcement from NVIDIA: Introducing the NVIDIA DRIVE AGX Thor Development Platform, https://forums.developer.nvidia.com/t/announcement-from-nvidia-introducing-the-nvidia-drive-agx-thor-development-platform/343108</li>
<li>Streamline LLM Deployment for Autonomous Vehicle Applications with NVIDIA DriveOS LLM SDK | NVIDIA Technical Blog, https://developer.nvidia.com/blog/streamline-llm-deployment-for-autonomous-vehicle-applications-with-nvidia-driveos-llm-sdk/</li>
<li>[2502.07855] Vision-Language Models for Edge Networks: A Comprehensive Survey - arXiv, https://arxiv.org/abs/2502.07855</li>
<li>Vision-Language Models for Edge Networks: A Comprehensive Survey - arXiv, https://arxiv.org/html/2502.07855v1</li>
<li>The Evolution of EyeQ - Mobileye, https://www.mobileye.com/technology/eyeq-chip/</li>
<li>Vision-Language Models for Edge Networks: A Comprehensive Survey - IEEE Xplore, https://ieeexplore.ieee.org/iel8/6488907/11121332/11032085.pdf</li>
<li>Hallucinations in LLMs: Why they happen, how to detect them and what you can do | ITWeb, https://www.itweb.co.za/article/hallucinations-in-llms-why-they-happen-how-to-detect-them-and-what-you-can-do/KPNG878NpQGq4mwD</li>
<li>Mobileye at CES 2025, https://www.mobileye.com/ces-2025/</li>
<li>5 Takeaways from Mobileye’s AI day, https://www.mobileye.com/blog/driving-ai/</li>
<li>Hallucination Reduction and Optimization for Large Language Model-Based Autonomous Driving - MDPI, https://www.mdpi.com/2073-8994/16/9/1196</li>
<li>Report to Congress: NHTSA Research and Rulemaking Activities on Vehicles Equipped with Automated Driving Systems, https://www.nhtsa.gov/sites/nhtsa.gov/files/2025-07/report-congress-research-rulemaking-automated-driving-systems-july-2025-tag.pdf</li>
<li>Explainable Artificial Intelligence for Autonomous Driving: A Comprehensive Overview and Field Guide for Future Research Directions - arXiv, https://arxiv.org/html/2112.11561v4</li>
<li>NVIDIA and Hyundai Motor Group Team on AI Factory to Power AI-Driven Mobility Solutions, https://investor.nvidia.com/news/press-release-details/2025/NVIDIA-and-Hyundai-Motor-Group-Team-on-AI-Factory-to-Power-AI-Driven-Mobility-Solutions/default.aspx</li>
<li>Nvidia Drops Alpamayo R1 As Open Source Leap Toward Level 4 Autonomy, https://www.opensourceforu.com/2025/12/nvidia-drops-alpamayo-r1-as-open-source-leap-toward-level-4-autonomy/</li>
<li>Compound AI: The framework powering scalable autonomy | Mobileye Blog, https://www.mobileye.com/blog/compound-ai-the-framework-powering-scalable-autonomy/</li>
<li>Comparing Robotaxis: Baidu’s Apollo and Alphabet’s Waymo, https://cyberlaw.stanford.edu/blog/2025/05/comparing-robotaxis-baidus-apollo-and-alphabets-waymo/</li>
<li>Apollo Go completes 2.2 million fully driverless rides in Q2 2025, up 148% YoY - Gasgoo, https://autonews.gasgoo.com/icv/70038700.html</li>
<li>Baidu’s Apollo Go robotaxi leads global autonomous driving with 17M+ orders, targets profit this year - Car News China, https://carnewschina.com/2025/11/13/baidus-apollo-go-robotaxi-leads-global-autonomous-driving-with-17m-orders-targets-profit-this-year/</li>
<li>Generative AI Developers Harness NVIDIA Technologies to Transform In-Vehicle Experiences, https://blogs.nvidia.com/blog/generative-ai-in-vehicle-experiences/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>