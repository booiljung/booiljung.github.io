<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:LMDrive 대형 언어 모델을 활용한 폐루프 E2E 자율 주행 프레임워크 (2023-12-12)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>LMDrive 대형 언어 모델을 활용한 폐루프 E2E 자율 주행 프레임워크 (2023-12-12)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">자율주행 (Autonomous Driving)</a> / <span>LMDrive 대형 언어 모델을 활용한 폐루프 E2E 자율 주행 프레임워크 (2023-12-12)</span></nav>
                </div>
            </header>
            <article>
                <h1>LMDrive 대형 언어 모델을 활용한 폐루프 E2E 자율 주행 프레임워크 (2023-12-12)</h1>
<p>2025-12-09, G30DR</p>
<h2>1.  서론: 자율 주행의 패러다임 전환과 언어 모델의 융합</h2>
<p>자율 주행(Autonomous Driving) 기술은 현대 로보틱스와 인공지능(AI) 분야에서 가장 도전적이고 복잡한 과제 중 하나로 꼽힌다. 과거 수십 년간 자율 주행 시스템은 주로 인지(Perception), 예측(Prediction), 계획(Planning), 제어(Control) 등 각각의 기능이 독립적인 모듈로 구성된 **모듈형 아키텍처(Modular Architecture)**에 의존해 왔다. 이 방식은 각 모듈의 해석 가능성(Interpretability)이 높고 디버깅이 용이하다는 장점이 있으나, 상위 모듈의 오류가 하위 모듈로 전파되는 오류 전파(Error Propagation) 문제와 시스템 전체의 최적화가 어렵다는 구조적 한계를 지닌다.1 이에 대한 대안으로 센서 입력부터 제어 신호 출력까지를 하나의 거대한 신경망으로 연결하여 학습하는 <strong>엔드투엔드(End-to-End) 자율 주행</strong> 방식이 부상하였다. 엔드투엔드 방식은 데이터 기반의 최적화를 통해 복잡한 규칙 기반(Rule-based) 시스템의 한계를 극복하고, 일반화 성능을 높일 수 있는 잠재력을 입증해 왔다.</p>
<p>그러나 기존의 엔드투엔드 모델들, 예를 들어 <strong>InterFuser</strong>나 **TCP(Trajectory-guided Control Prediction)**와 같은 모델들은 주로 시각적 정보(Visual Information)나 라이다(LiDAR) 데이터와 같은 기하학적 입력에만 의존한다는 뚜렷한 한계를 가지고 있었다.2 이러한 모델들은 “전방의 장애물을 피하라“는 암묵적인 주행 목표는 달성할 수 있으나, 인간의 언어로 주어지는 구체적이고 복잡한 지시(예: “다음 교차로에서 좌회전하되, 횡단보도의 보행자를 주의하라”)를 이해하거나 처리할 수 없었다. 이는 자율 주행 차량이 실제 도로 환경에서 인간 운전자나 승객, 그리고 교통 통제관과 상호작용(Interaction)하는 데 있어 치명적인 결함으로 작용한다.</p>
<p>이러한 맥락에서 등장한 **LMDrive (Language-Guided Closed-Loop End-to-End Driving)**는 대형 언어 모델(Large Language Model, LLM)의 강력한 추론 능력과 일반화된 지식을 자율 주행의 제어 루프(Control Loop)에 직접 통합하려는 시도이다.1 LMDrive는 단순히 언어를 입력으로 받는 것을 넘어, 다중 모달(Multi-modal) 센서 데이터와 자연어 지시를 결합하여 복잡한 도심 환경에서 차량을 제어하는 폐루프(Closed-Loop) 시스템을 구축하였다. 이는 기존의 비전-언어 모델(Vision-Language Model)들이 주로 시각적 질의응답(VQA)이나 개루프(Open-Loop) 궤적 예측에 머물렀던 것과 달리, 실제 시뮬레이션 환경에서 차량의 거동을 실시간으로 제어한다는 점에서 학술적, 실용적 의의가 매우 크다. 본 보고서는 CVPR 2024에 채택된 LMDrive의 기술적 아키텍처, 데이터셋 구축 방법론, LangAuto 벤치마크 성능, 그리고 한계점과 미래 전망을 심층적으로 분석한다.5</p>
<h2>2.  LMDrive 아키텍처 및 방법론적 프레임워크</h2>
<p>LMDrive의 핵심 설계 철학은 시각적 정보와 언어적 지시를 동등한 수준의 의미론적 공간(Semantic Space)으로 투영하여, LLM이 주행 상황을 ’이해’하고 ’판단’하게 만드는 것이다. 이를 위해 LMDrive는 비전 인코더(Vision Encoder), 시각-언어 어댑터(Vision-Language Adapter), 그리고 LLM 백본(LLM Backbone)으로 이어지는 정교한 3단계 파이프라인을 채택하고 있다.6</p>
<h3>2.1  멀티모달 센서 데이터의 인코딩 및 특징 추출</h3>
<p>자율 주행 환경은 고도로 동적이며 불확실성이 높다. 차량은 주변 360도 상황을 실시간으로 파악해야 하므로, 단일 시점의 이미지만으로는 충분하지 않다. LMDrive는 <strong>CARLA 시뮬레이터</strong> 환경에서 제공하는 다중 시점(Multi-view) 카메라와 라이다(LiDAR) 데이터를 입력으로 활용한다.8</p>
<ul>
<li><strong>입력 데이터 구성</strong>: LMDrive는 전방, 좌측, 우측, 후방을 커버하는 4개의 RGB 카메라 이미지와 라이다 포인트 클라우드 데이터를 동시에 처리한다. 라이다 데이터는 3D 공간 정보를 제공하여 깊이(Depth) 정보가 부족한 카메라의 단점을 보완하며, 카메라는 풍부한 텍스처와 의미론적 정보(예: 신호등 색상, 표지판 내용)를 제공한다.7</li>
<li><strong>비전 인코더 (Vision Encoder)</strong>: 시각적 특징 추출을 위해 <code>timm</code> 라이브러리를 기반으로 하는 <strong>ResNet-50</strong> 아키텍처를 사용한다. ResNet-50은 심층 신경망의 기울기 소실 문제를 해결한 잔차 연결(Residual Connection) 구조를 통해 이미지의 저수준 특징(Edge, Texture)부터 고수준 특징(Object, Scene)까지 효과적으로 추출한다.6</li>
<li><strong>토큰 생성 (Tokenization)</strong>: 인코더를 통과한 데이터는 트랜스포머(Transformer) 구조가 처리할 수 있는 형태인 ’토큰(Token)’으로 변환된다. 구체적으로, 각 프레임은 <span class="math math-inline">H \times W</span> 크기의 조감도(Bird’s Eye View, BEV) 토큰, <span class="math math-inline">N</span>개의 웨이포인트(Waypoint) 토큰, 그리고 신호등 상태를 나타내는 토큰 등으로 분해된다. 단일 프레임당 생성되는 시각적 토큰의 수는 약 406개에 달한다.7 자율 주행은 시간적 연속성이 중요하므로 수백 개의 프레임을 연속적으로 처리해야 하는데, 프레임당 400개 이상의 토큰은 LLM의 컨텍스트 윈도우(Context Window)를 빠르게 소진시키고 연산 부하를 급격히 증가시키는 요인이 된다.</li>
</ul>
<h3>2.2  Q-Former를 활용한 시각-언어 정렬 및 토큰 압축</h3>
<p>LMDrive 아키텍처에서 가장 기술적으로 주목할 부분은 방대한 시각적 데이터를 LLM이 처리 가능한 수준으로 압축하는 과정이다. 이를 위해 연구진은 <strong>BLIP-2</strong> 모델에서 제안된 <strong>Q-Former (Querying Transformer)</strong> 구조를 도입하였다.7</p>
<ul>
<li><strong>Q-Former의 메커니즘</strong>: Q-Former는 고정된 개수(<span class="math math-inline">M</span>)의 **학습 가능한 쿼리(Learnable Queries)**를 사용하여 비전 인코더에서 생성된 가변적인 수의 시각적 토큰들과 교차 주의(Cross-Attention)를 수행한다. 이 과정은 마치 데이터베이스에서 쿼리를 통해 필요한 정보만을 추출하는 것과 유사하다. 쿼리 벡터들은 시각적 토큰들 사이를 탐색하며, 현재의 주행 맥락과 관련된 가장 중요한 시각적 정보만을 선별하여 자신의 임베딩에 통합한다.</li>
<li><strong>정보 압축 및 차원 변환</strong>: 이 과정을 통해 프레임당 406개에 달하던 시각적 토큰은 <span class="math math-inline">M</span>개의 압축된 쿼리 임베딩으로 변환된다. 이렇게 압축된 토큰은 다시 2계층 MLP(Multi-Layer Perceptron) 어댑터를 통과하며 LLM의 입력 임베딩 차원과 동일한 크기로 선형 투영(Linear Projection)된다. 이는 시각 정보(이미지)를 언어 모델이 이해할 수 있는 ’시각적 단어(Visual Words)’로 번역하는 과정으로 볼 수 있다.7 이로써 시각 정보와 텍스트 정보는 동일한 벡터 공간 상에 존재하게 되며, LLM은 텍스트 지시와 시각적 상황을 통합적으로 추론할 수 있게 된다.</li>
</ul>
<h3>2.3  LLM 기반의 추론 및 제어 신호 생성 파이프라인</h3>
<p>압축된 시각적 컨텍스트와 사용자의 자연어 명령(Navigation Instruction)은 LLM에 입력되어 최종적인 주행 결정을 내리는 데 사용된다.</p>
<ul>
<li><strong>LLM 백본 (Backbone)</strong>: LMDrive는 <strong>LLaVA-v1.5-7B</strong>, <strong>Vicuna-v1.5-7B</strong>, <strong>LLaMA-7B</strong> 등 약 70억 개의 파라미터를 가진 모델들을 백본으로 사용하여 실험을 진행하였다.6 이 중 LLaVA 모델은 이미지와 텍스트 쌍으로 사전 학습(Instruction Tuning)이 되어 있어, 시각적 정보를 처리하는 데 가장 뛰어난 성능을 보인다. LLM은 사전 학습된 방대한 지식을 바탕으로 “빨간불에는 멈춰야 한다”, “비가 오면 감속해야 한다“와 같은 상식적 추론을 수행한다.</li>
<li><strong>액션 토큰 및 웨이포인트 예측</strong>: LLM은 입력된 정보를 바탕으로 **액션 토큰(Action Tokens)**을 자기회귀적(Auto-regressive)으로 생성한다. LMDrive는 LLM이 직접 스티어링 휠의 각도나 페달의 압력을 출력하게 하는 대신, 차량이 이동해야 할 미래의 경로점인 **웨이포인트(Waypoints)**를 예측하도록 설계되었다. 구체적으로, LLM의 출력은 또 다른 2계층 MLP 어댑터를 통해 미래 시점의 2D 좌표들로 변환된다.7 또한, 현재 주어진 지시가 완료되었는지 여부를 판단하는 플래그(Completed Flag)도 함께 예측하여, 연속적인 지시 수행이 가능하도록 한다.</li>
<li><strong>PID 제어기를 통한 물리적 제어</strong>: 예측된 웨이포인트는 **PID 제어기(Proportional-Integral-Derivative Controller)**에 입력된다. PID 제어기는 예측된 경로와 현재 차량의 상태(위치, 속도, 방향) 간의 오차를 계산하여, 이를 최소화하기 위한 조향(Steering), 가속(Throttle), 제동(Brake) 값을 산출한다. LBC(Learning by Cheating) 방식에서 차용한 이 구조는 LLM의 고수준 추론 능력과 PID 제어기의 저수준 제어 안정성을 결합한 하이브리드 방식이다.7 LLM이 직접 제어 신호를 출력할 경우 발생할 수 있는 불안정성(Jittering)이나 물리적으로 불가능한 움직임을 PID 제어기가 완충해 주는 역할을 한다.</li>
</ul>
<h3>2.4  학습 목표 및 손실 함수</h3>
<p>LMDrive의 학습은 크게 두 단계로 나뉜다. 첫 번째는 비전 인코더의 사전 학습(Pre-training) 단계이고, 두 번째는 전체 모델의 인스트럭션 미세 조정(Instruction Finetuning) 단계이다.</p>
<ul>
<li><strong>비전 인코더 사전 학습</strong>: 방대한 주행 데이터를 통해 시각적 특징을 잘 추출하도록 인코더를 학습시킨다.</li>
<li><strong>인스트럭션 미세 조정</strong>: LLM과 어댑터를 포함한 전체 네트워크를 학습시킨다. 이때 사용되는 손실 함수(Loss Function)는 크게 두 가지이다.7</li>
</ul>
<ol>
<li><strong>L1 웨이포인트 손실 (L1 Waypoint Loss)</strong>: 모델이 예측한 웨이포인트와 전문가 에이전트(Ground Truth)의 실제 경로 간의 거리 차이를 최소화한다.</li>
<li><strong>분류 손실 (Classification Loss)</strong>: 신호등 상태 예측이나 지시 완료 여부 판단과 같은 분류 작업에 대한 교차 엔트로피(Cross-Entropy) 손실을 사용한다.</li>
</ol>
<h2>3.  데이터셋 구축 전략: LMDrive Dataset</h2>
<p>LMDrive와 같은 대형 모델을 학습시키기 위해서는 양질의 데이터가 필수적이다. 특히 언어 지시와 주행 행동이 쌍을 이루는 데이터셋은 기존에 존재하지 않았으므로, 연구진은 자체적인 데이터 수집 및 생성 파이프라인을 구축하였다.</p>
<h3>3.1  전문가 에이전트를 활용한 데이터 수집</h3>
<p>데이터 수집은 CARLA 시뮬레이터 내에서 **규칙 기반 전문가 에이전트(Rule-based Expert Agent)**를 구동하여 이루어졌다.6</p>
<ul>
<li><strong>전문가 에이전트의 역할</strong>: 이 에이전트는 시뮬레이터의 **특권 정보(Privileged Information)**에 접근할 수 있다. 즉, 주변 차량의 정확한 위치, 신호등의 상태, 도로의 정확한 지도 정보를 모두 알고 있는 상태에서 주행한다. 따라서 항상 교통 법규를 준수하고 충돌을 회피하는 최적의 주행 경로를 생성할 수 있다. LMDrive는 이 전문가 에이전트의 주행을 모방(Imitation Learning)하도록 학습된다.</li>
<li><strong>데이터 규모 및 다양성</strong>: 총 300만 프레임, 약 **64,000개(64K)**의 클립으로 구성된 데이터셋이 구축되었다.8 각 클립의 길이는 2초에서 20초 사이이며, 데이터 수집 주파수는 약 2Hz에서 10Hz 사이로 설정되어 시간적 연속성을 보장한다.9 데이터의 다양성을 확보하기 위해 CARLA의 8개 마을(Town) 전체에서 데이터를 수집하였으며, 7가지 날씨 조건(맑음, 비, 흐림 등)과 3가지 조명 조건(낮, 노을, 밤)을 조합하여 총 21가지 이상의 환경 조건을 포함시켰다. 이는 모델이 특정 환경에 과적합(Overfitting)되는 것을 방지하고, 악천후나 야간 주행과 같은 롱테일(Long-tail) 시나리오에 대한 대응력을 높이기 위함이다.7</li>
</ul>
<h3>3.2  자연어 지시어 생성 및 분류 체계</h3>
<p>단순히 “목적지로 가라“는 식의 명령만으로는 복잡한 도심 주행을 커버할 수 없다. LMDrive 데이터셋은 주행 상황에 맞는 다양한 자연어 지시어를 포함하고 있다. 지시어는 크게 세 가지 범주로 나뉜다.7</p>
<table><thead><tr><th><strong>지시어 유형 (Type)</strong></th><th><strong>예시 (Example)</strong></th><th><strong>설명 및 목적</strong></th></tr></thead><tbody>
<tr><td><strong>Follow (경로 추종)</strong></td><td>“다음 교차로까지 현재 차로를 유지하라.”, “좌측 차선으로 변경하라.”</td><td>기본적인 차선 유지 및 경로 변경을 지시한다. 장거리 주행의 기본이 된다.</td></tr>
<tr><td><strong>Turn (방향 전환)</strong></td><td>“앞의 T자형 교차로에서 왼쪽으로 꺾어라.”, “교차로에서 직진하라.”</td><td>교차로, 로터리 등에서의 구체적인 방향 전환 행동을 정의한다.</td></tr>
<tr><td><strong>Notice (상황 주의)</strong></td><td>“전방에 보행자가 있으니 주의하라.”, “우측의 자전거를 조심하라.”</td><td>단순한 이동 명령이 아닌, 안전 운전을 위한 상황 인식(Situational Awareness)을 요구한다. 이는 LMDrive가 단순한 기계적 제어를 넘어 상황을 ’해석’하게 만드는 핵심 요소이다.</td></tr>
<tr><td><strong>Misleading (오도)</strong></td><td>(주행과 무관하거나, 위험을 초래하는 잘못된 지시)</td><td>전체 데이터의 약 5%를 차지한다. 모델이 맹목적으로 명령을 따르는 것이 아니라, 시각적 정보를 바탕으로 잘못된 명령을 거부할 수 있는지 평가하기 위한 적대적(Adversarial) 데이터이다.8</td></tr>
</tbody></table>
<p>이러한 지시어들은 템플릿 기반으로 생성된 후, LLM을 통해 자연스러운 문장으로 재구성(Paraphrasing)되는 과정을 거쳐 다양성을 확보하였다.</p>
<h2>4.  LangAuto 벤치마크: 언어 기반 폐루프 자율 주행 평가의 새로운 기준</h2>
<p>기존의 자율 주행 벤치마크인 <strong>Town05 Benchmark</strong>나 <strong>Longest6 Benchmark</strong>는 웨이포인트 좌표를 기반으로 주행 성능을 평가하였다. 그러나 이러한 방식은 자연어 명령 수행 능력을 검증할 수 없다는 한계가 명확했다. LMDrive 연구진은 이를 해결하기 위해 <strong>LangAuto (Language-Guided Autonomous Driving) 벤치마크</strong>를 새롭게 제안하였다.1 이는 CARLA Leaderboard를 기반으로 하되, 입력 인터페이스를 이산적인 명령(Discrete Command)에서 자연어 지시(Natural Language Instruction)로 변경한 것이다.</p>
<h3>4.1  벤치마크 트랙 구성</h3>
<p>LangAuto 벤치마크는 주행의 난이도와 목적에 따라 다양한 트랙으로 구성되어 있다.8</p>
<ol>
<li><strong>LangAuto (Long) Track</strong>: 평균 주행 거리가 500m 이상인 장거리 주행 트랙이다. 모델은 일련의 연속적인 지시를 기억하고 수행해야 하며, 장기적인 경로 계획(Long-term Planning) 능력이 요구된다.</li>
<li><strong>LangAuto-Short Track</strong>: 150m에서 500m 사이의 중단거리 주행 트랙이다. 기본적인 명령 수행 능력과 교차로 통과 능력을 평가한다.</li>
<li><strong>LangAuto-Tiny Track</strong>: 150m 미만의 단거리 주행 트랙으로, 즉각적인 제어 반응성과 정밀한 조향 능력을 평가하는 데 초점을 맞춘다.</li>
<li><strong>LangAuto-Notice Track</strong>: ‘Notice’ 지시어가 포함된 트랙이다. 예를 들어 “전방에 사고 차량이 있다“는 지시를 받았을 때, 모델이 속도를 줄이거나 차선을 변경하는 등의 안전 행동을 취하는지를 중점적으로 평가한다. 이는 LMDrive의 맥락 이해 능력을 검증하는 중요한 트랙이다.</li>
<li><strong>LangAuto-Sequential Track</strong>: 여러 개의 지시가 하나의 문장으로 결합된 형태(예: “직진하다가 두 번째 신호등에서 우회전하라”)를 처리하는 능력을 평가한다.</li>
</ol>
<h3>4.2  주요 평가 지표 (Metrics)</h3>
<p>평가는 CARLA Leaderboard의 표준 지표를 따르며, 주행의 완성도와 안전성을 종합적으로 측정한다.6</p>
<ul>
<li>
<p>Driving Score (DS): 가장 핵심적인 지표로, 주행 완료율(RC)과 페널티 점수(Infraction Penalty)의 곱으로 계산된다. 경로를 완주하더라도 신호 위반이나 충돌이 많으면 점수가 낮아진다.</p>
<p><span class="math math-display">DS = RC \times P_{penalty}</span></p>
</li>
<li>
<p><strong>Route Completion (RC)</strong>: 전체 경로 중 차량이 주행을 완료한 비율(%)이다.</p>
</li>
<li>
<p><strong>Infraction Score (IS)</strong>: 충돌, 차선 이탈, 신호 위반, 보행자 위협 등의 위반 횟수를 점수화한 것이다. 1.0에 가까울수록 위반이 없는 안전한 주행을 의미한다.</p>
</li>
</ul>
<h2>5. 실험 결과 및 성능 분석</h2>
<p>LMDrive의 성능은 LangAuto 벤치마크를 통해 다양한 베이스라인 모델들과 비교 분석되었다. 실험은 주로 NVIDIA A100 GPU 환경에서 진행되었으며, 추론 속도와 메모리 효율성 또한 중요한 분석 대상이었다.14</p>
<h3>5.1 모델 백본별 성능 비교</h3>
<p>LMDrive는 다양한 LLM 백본을 적용하여 성능을 비교하였다. 아래 표는 LangAuto 및 LangAuto-Short 트랙에서의 주요 결과이다.6</p>
<table><thead><tr><th><strong>모델 (Model Backbone)</strong></th><th><strong>파라미터 수</strong></th><th><strong>DS (LangAuto)</strong></th><th><strong>DS (LangAuto-Short)</strong></th><th><strong>분석</strong></th></tr></thead><tbody>
<tr><td><strong>LMDrive (LLaVA-v1.5)</strong></td><td>7B</td><td><strong>36.2</strong></td><td><strong>50.6</strong></td><td><strong>최고 성능</strong>. 이미지-텍스트 쌍으로 학습된 LLaVA의 시각적 이해 능력이 주행 성능 향상에 기여함.</td></tr>
<tr><td><strong>LMDrive (Vicuna-v1.5)</strong></td><td>7B</td><td>33.5</td><td>45.3</td><td>텍스트 중심의 챗봇 모델인 Vicuna는 시각적 상황 판단에서 LLaVA에 비해 다소 뒤처짐.</td></tr>
<tr><td><strong>LMDrive (LLaMA)</strong></td><td>7B</td><td>31.3</td><td>42.8</td><td>순수 언어 모델인 LLaMA는 시각-언어 정렬이 부족하여 가장 낮은 성능을 보임.</td></tr>
</tbody></table>
<p>이 결과는 자율 주행과 같은 멀티모달 태스크에서 **시각적 인스트럭션 튜닝(Visual Instruction Tuning)**의 중요성을 여실히 보여준다. LLaVA 모델은 이미지를 보고 텍스트로 설명하거나 질문에 답하는 훈련이 되어 있어, 주행 환경의 복잡한 시각 정보를 언어 지시와 연결하는 능력이 탁월하다. 반면, LLaMA나 Vicuna는 이러한 시각적 훈련이 부족하여, 동일한 Q-Former를 사용하더라도 입력된 시각 정보를 효율적으로 활용하지 못하는 것으로 분석된다.</p>
<h3>5.2 베이스라인 모델(InterFuser)과의 비교 평가</h3>
<p>InterFuser는 기존 CARLA 벤치마크에서 SOTA(State-of-the-Art)를 기록한 강력한 엔드투엔드 모델이다. 그러나 언어 지시를 처리하는 능력이 부재하여 LangAuto 벤치마크에서는 LMDrive에 비해 현저히 낮은 성능을 기록하였다.7</p>
<table><thead><tr><th><strong>모델</strong></th><th><strong>입력 모달리티</strong></th><th><strong>DS (LangAuto)</strong></th><th><strong>특징 및 한계</strong></th></tr></thead><tbody>
<tr><td><strong>InterFuser</strong></td><td>이미지 + 라이다 + 웨이포인트</td><td><strong>10.7 ± 3.8</strong></td><td>웨이포인트 추종에는 능하나, 자연어 지시의 맥락을 이해하지 못해 엉뚱한 경로로 주행하거나 지시를 무시함.</td></tr>
<tr><td><strong>LMDrive (Ours)</strong></td><td>이미지 + 라이다 + 언어</td><td><strong>36.2 ± 1.5</strong></td><td>언어 지시를 이해하고 시각 정보와 결합하여 경로를 수정함. InterFuser 대비 약 <strong>3배 이상의 성능 향상</strong>.</td></tr>
</tbody></table>
<p>InterFuser의 낮은 점수는 기존의 SOTA 모델들이 특정 형식(웨이포인트)의 입력에 과적합되어 있으며, 입력 인터페이스가 변경되었을 때 유연하게 대처하지 못함을 시사한다. 반면 LMDrive는 폐루프 환경에서 언어적 맥락을 이해하고 이를 제어 신호로 변환하는 능력을 입증하였다. 이는 “좌회전하라“는 명시적 명령뿐만 아니라, “혼잡하니 천천히 가라“는 추상적 명령이 주행 성능에 결정적인 영향을 미칠 수 있음을 보여준다.</p>
<h3>5.3 최신 후속 연구와의 비교 (BEVDriver, AD-H)</h3>
<p>LMDrive 이후 등장한 후속 연구들인 <strong>BEVDriver</strong>나 **AD-H (Autonomous Driving with Hierarchical agents)**는 LMDrive의 구조를 개선하여 더 높은 성능을 달성하였다.9</p>
<ul>
<li><strong>AD-H</strong>: LMDrive 대비 DS 점수가 44.0으로 향상되었다. 이는 계층적(Hierarchical) 에이전트 구조를 도입하여 계획(Planning)과 제어(Control)를 더 명확히 분리한 결과로 해석된다.</li>
<li><strong>BEVDriver</strong>: BEV(Bird’s Eye View) 표현을 강화하여 공간적 이해도를 높인 모델로, LangAuto 트랙에서 DS 48.9를 기록하며 LMDrive를 상회하였다.</li>
</ul>
<p>비록 후속 모델들이 더 높은 점수를 기록했으나, LMDrive는 이 분야의 **개척자(Pioneer)**로서 LangAuto 벤치마크를 정립하고, LLM을 폐루프 주행에 처음으로 도입했다는 점에서 그 가치가 퇴색되지 않는다. LMDrive는 이러한 후속 연구들의 베이스라인(Baseline)으로서 중요한 비교 대상이 되고 있다.</p>
<h3>5.4 추론 속도(Latency) 및 실시간성 분석</h3>
<p>LMDrive와 같은 거대 모델의 가장 큰 약점은 연산 비용과 그로 인한 추론 지연(Latency)이다. 자율 주행은 0.1초의 지연도 사고로 이어질 수 있는 실시간 시스템이다.</p>
<ul>
<li><strong>추론 시간</strong>: LMDrive(LLaVA-7B 기준)의 평균 추론 시간은 약 <strong>526ms</strong> (약 2Hz)로 측정되었다.15</li>
<li><strong>비교</strong>: 이는 TinyLLaMA를 사용한 경량화 모델(445ms)이나 비전 전용 모델들에 비해 상당히 느린 속도이다. 일반적인 자율 주행 제어 루프가 10Hz(100ms) 이상을 요구한다는 점을 고려할 때, 2Hz의 속도는 고속 주행이나 급박한 돌발 상황 대처에 한계를 보일 수 있다.</li>
<li><strong>대응 방안</strong>: 연구진은 이를 보완하기 위해 10Hz로 작동하는 PID 제어기를 하위단에 배치하여, LLM이 갱신되지 않는 동안에도 차량의 안정성을 유지하도록 설계하였다. 그러나 근본적인 해결을 위해서는 모델 경량화(Quantization)나 추론 가속화 기술이 필수적이다.</li>
</ul>
<h2>6. 심층 분석 및 논의</h2>
<h3>6.1 토큰 압축의 딜레마와 Q-Former의 역할</h3>
<p>LMDrive가 채택한 Q-Former 기반의 토큰 압축 방식은 필수불가결한 선택이었다. 수백 프레임의 고해상도 이미지를 LLM에 직접 입력하는 것은 연산 비용상 불가능하기 때문이다. 그러나 406개의 시각적 토큰을 소수의 쿼리로 압축하는 과정에서 <strong>미세한 시각적 정보(Small Objects, Traffic Signs)의 손실</strong>이 발생할 가능성이 존재한다. 예를 들어, 멀리 있는 신호등의 색상 변화나 작은 도로 파편과 같은 정보가 압축 과정에서 누락될 수 있다. LMDrive가 신호등 상태나 웨이포인트를 별도의 토큰으로 명시적으로 예측하도록 설계된 것은 이러한 정보 손실을 보완하고, LLM이 중요한 정보에 집중하도록 유도(Attention Guidance)하기 위한 장치로 해석된다.7</p>
<h3>6.2 ‘Notice’ 지시어의 인과적 추론 효과</h3>
<p>LangAuto-Notice 트랙에서의 성능은 LMDrive가 단순한 패턴 매칭을 넘어 **인과적 추론(Causal Reasoning)**을 수행하고 있음을 시사한다. “전방에 사고가 났으니 천천히 가라“는 지시를 받았을 때, 모델은 시각적으로 사고 현장을 식별하고(Grounding), ’천천히’라는 부사를 구체적인 속도 제어 파라미터(감속)로 변환하는 과정을 성공적으로 수행한다. 이는 자율 주행 AI가 입력에 대해 즉각적으로 반응하는 ‘반사적(Reactive)’ 단계에서, 상황을 이해하고 행동의 결과를 예측하는 ‘숙고적(Deliberative)’ 단계로 진화하고 있음을 보여주는 중요한 증거이다.</p>
<h3>6.3 시뮬레이션과 현실의 간극 (Sim-to-Real Gap)</h3>
<p>LMDrive는 CARLA 시뮬레이터에서 훌륭한 성능을 보였지만, 현실 세계 적용에는 여전히 높은 장벽이 존재한다.</p>
<ol>
<li><strong>시각적 도메인 차이 (Domain Shift)</strong>: 시뮬레이터의 깔끔하고 정형화된 이미지와 달리, 현실의 센서 데이터는 노이즈가 많고, 날씨나 조명에 따른 변화가 극심하다. 렌즈 플레어, 모션 블러, 센서 오염 등의 현실적 문제를 해결하기 위한 도메인 적응(Domain Adaptation) 기술이 필요하다.</li>
<li><strong>지시어의 모호성</strong>: 실험에 사용된 지시어는 템플릿 기반으로 생성되어 비교적 명확하다. 그러나 실제 인간의 언어는 훨씬 모호하고, 비문법적이며, 때로는 반어적이기도 하다. “저기서 적당히 세워줘“와 같은 모호한 명령을 처리하는 능력은 아직 검증되지 않았다.</li>
<li><strong>안전성 보장 (Safety Assurance)</strong>: LLM 특유의 <strong>환각(Hallucination)</strong> 현상은 주행 안전에 치명적일 수 있다. 모델이 존재하지 않는 도로를 인식하거나, 잘못된 교통 법규를 적용할 위험이 있다. 따라서 LLM의 출력을 물리적으로 검증하고 제한하는 별도의 안전 계층(Safety Layer)이나 룰 기반의 감시 모듈(Monitor)이 반드시 병행되어야 한다.</li>
</ol>
<h3>6.4 오도하는(Misleading) 지시어와 적대적 방어</h3>
<p>데이터셋에 포함된 약 5%의 ’오도하는 지시어’에 대한 대응 능력은 LMDrive의 강건성을 보여주는 흥미로운 지점이다. 예를 들어, “절벽으로 직진하라“는 명령이 주어졌을 때, LMDrive는 시각적 정보를 통해 전방이 주행 불가능함을 인식하고 해당 명령을 무시해야 한다. 실험 결과, LMDrive는 이러한 적대적 명령 상황에서도 시각 정보 우선순위를 통해 사고를 회피하는 경향을 보였다. 이는 LLM이 단순히 언어를 따르는 것이 아니라, 시각적 생존 본능(?)을 어느 정도 내재화했음을 의미한다. 그러나 완벽하지 않으며, 여전히 언어 명령에 과도하게 의존하여 위험한 행동을 할 가능성은 남아있다.</p>
<h2>7. 결론 및 향후 전망</h2>
<p>LMDrive는 대형 언어 모델을 자율 주행의 폐루프 제어 시스템에 성공적으로 통합한 선구적인 연구이다. 이 프레임워크는 자연어 지시를 이해하고, 복잡한 교통 상황에서 상식적인 판단을 내릴 수 있는 자율 주행 에이전트의 가능성을 실증적으로 보여주었다. 특히 64,000 클립의 대규모 데이터셋과 LangAuto 벤치마크의 공개는 후속 연구를 위한 중요한 기반을 마련하였으며, InterFuser 등 기존 모델들이 해결하지 못한 ’언어 기반 상호작용’의 문제를 해결하는 돌파구를 제시하였다.1</p>
<p>비록 실시간 처리를 위한 500ms대의 느린 추론 속도와 시뮬레이션 환경에 국한된 검증이라는 한계가 있지만, LMDrive가 제시한 <strong>‘언어 가이드 기반 엔드투엔드 주행(Language-Guided End-to-End Driving)’</strong> 패러다임은 자율 주행 기술의 미래 방향성을 제시한다. 향후 연구는 LLM의 추론 능력을 유지하면서도 실시간성을 확보할 수 있는 경량화 기술, 그리고 현실 세계의 불확실성을 견딜 수 있는 강건성 확보에 집중될 것으로 예상된다. 또한, LMDrive와 같은 모델이 발전함에 따라, 미래의 자율 주행차는 단순한 이동 수단을 넘어 인간과 소통하고 교감하는 진정한 의미의 ’지능형 파트너’로 진화할 것이다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>LMDrive: Closed-Loop End-to-End Driving with Large Language Models - CVF Open Access, 12월 9, 2025에 액세스, https://openaccess.thecvf.com/content/CVPR2024/papers/Shao_LMDrive_Closed-Loop_End-to-End_Driving_with_Large_Language_Models_CVPR_2024_paper.pdf</li>
<li>LMDrive: Closed-Loop End-to-End Driving with Large Language Models - ResearchGate, 12월 9, 2025에 액세스, https://www.researchgate.net/publication/384220970_LMDrive_Closed-Loop_End-to-End_Driving_with_Large_Language_Models</li>
<li>Comparison of our InterFuser with six state-of-the-art methods in Town05 benchmark. Metrics - ResearchGate, 12월 9, 2025에 액세스, https://www.researchgate.net/figure/Comparison-of-our-InterFuser-with-six-state-of-the-art-methods-in-Town05-benchmark_tbl1_362324837</li>
<li>LMDrive: Closed-Loop End-to-End Driving with Large Language Models - arXiv, 12월 9, 2025에 액세스, https://arxiv.org/abs/2312.07488</li>
<li>LMDrive: Closed-Loop End-to-End Driving with Large Language Models - Semantic Scholar, 12월 9, 2025에 액세스, https://www.semanticscholar.org/paper/LMDrive%3A-Closed-Loop-End-to-End-Driving-with-Large-Shao-Hu/e0b05e314372ed580d9612ef5f0ee672b17ad2e4</li>
<li>[CVPR 2024] LMDrive: Closed-Loop End-to-End Driving with Large Language Models - GitHub, 12월 9, 2025에 액세스, https://github.com/opendilab/LMDrive</li>
<li>LMDrive: Closed-Loop End-to-End Driving with Large Language Models - arXiv, 12월 9, 2025에 액세스, https://arxiv.org/html/2312.07488v2</li>
<li>LMDrive - Hao Shao, 12월 9, 2025에 액세스, https://hao-shao.com/projects/lmdrive.html</li>
<li>VLA-MP: A Vision-Language-Action Framework for Multimodal Perception and Physics-Constrained Action Generation in Autonomous Driving - NIH, 12월 9, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC12526522/</li>
<li>LMDrive, 12월 9, 2025에 액세스, https://openxlab.org.cn/models/detail/deepcs233/LMDrive</li>
<li>DriveGPT4-V2: Harnessing Large Language Model Capabilities for Enhanced Closed-Loop Autonomous Driving - CVF Open Access, 12월 9, 2025에 액세스, https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous_CVPR_2025_paper.pdf</li>
<li>arXiv:2312.07488v2 [cs.CV] 21 Dec 2023, 12월 9, 2025에 액세스, https://img.shlab.org.cn/pjlab/files/2024/05/638510356453080000.pdf</li>
<li>LMGENDRIVE: LLM REASONING MEETS WORLD MODELS FOR END-TO-END DRIVING - OpenReview, 12월 9, 2025에 액세스, https://openreview.net/pdf?id=fSnYZZ6v49</li>
<li>BEVDriver: Leveraging BEV Maps in LLMs for Robust Closed-Loop Driving - arXiv, 12월 9, 2025에 액세스, https://www.arxiv.org/pdf/2503.03074</li>
<li>AdaDrive: Self-Adaptive Slow-Fast System for Language-Grounded Autonomous Driving - CVF Open Access, 12월 9, 2025에 액세스, https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_AdaDrive_Self-Adaptive_Slow-Fast_System_for_Language-Grounded_Autonomous_Driving_ICCV_2025_paper.pdf</li>
<li>BEVDriver: Leveraging BEV Maps in LLMs for Robust Closed-Loop Driving - arXiv, 12월 9, 2025에 액세스, https://arxiv.org/html/2503.03074v1</li>
<li>AdaDrive: Self-Adaptive Slow-Fast System for Language-Grounded, 12월 9, 2025에 액세스, https://www.alphaxiv.org/overview/2511.06253v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>