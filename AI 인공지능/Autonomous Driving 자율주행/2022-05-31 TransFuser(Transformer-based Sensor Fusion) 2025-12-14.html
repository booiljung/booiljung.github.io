<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:TransFuser(Transformer-based Sensor Fusion, 2022-05-31)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>TransFuser(Transformer-based Sensor Fusion, 2022-05-31)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">자율주행 (Autonomous Driving)</a> / <span>TransFuser(Transformer-based Sensor Fusion, 2022-05-31)</span></nav>
                </div>
            </header>
            <article>
                <h1>TransFuser(Transformer-based Sensor Fusion, 2022-05-31)</h1>
<p>2025-12-14, G30DR</p>
<h2>1.  서론: 자율주행 인지 시스템의 패러다임 전환</h2>
<p>현대 자율주행 시스템의 발전은 센서 기술의 진보와 이를 처리하는 인공지능 알고리즘의 고도화가 맞물려 가속화되고 있다. 특히 완전 자율주행(Level 4/5)을 달성하기 위해서는 차량 주변의 복잡하고 동적인 환경을 완벽하게 이해해야 하며, 이를 위해 카메라(RGB Camera)와 라이다(LiDAR)라는 두 가지 핵심 센서의 융합(Sensor Fusion)이 필수적이다. 카메라는 인간의 시각과 유사하게 풍부한 텍스처와 색상 정보를 제공하여 신호등, 표지판, 차선과 같은 의미론적(Semantic) 정보를 파악하는 데 탁월하다. 반면, 라이다는 레이저 펄스를 이용해 주변 환경의 정밀한 3D 기하학적(Geometric) 구조와 깊이(Depth) 정보를 제공함으로써, 거리 측정과 객체 위치 파악에 있어 독보적인 정확성을 자랑한다.1</p>
<p>그러나 이 두 센서의 데이터를 효과적으로 융합하는 것은 여전히 난제로 남아 있다. 초기 자율주행 연구들은 각 센서를 독립적으로 처리한 후 결과물을 합치는 후기 융합(Late Fusion) 방식을 채택했으나, 이는 센서 간의 상호보완적인 정보를 초기 단계에서 활용하지 못한다는 한계가 있었다. 이를 극복하기 위해 등장한 기하학적 융합(Geometric Fusion)은 라이다 포인트 클라우드를 카메라 이미지 평면에 투영하거나 그 반대의 방식을 사용하여 물리적 공간의 일치를 꾀했다. 하지만 이러한 방식은 센서 간의 엄격한 캘리브레이션(Calibration)을 요구하며, 캘리브레이션 오차에 매우 민감하다는 단점이 있다. 더 중요한 문제는 기하학적 융합이 ’공간적 인접성(Locality)’에 의존한다는 점이다. 즉, 물리적으로 같은 위치에 있는 데이터끼리만 융합이 이루어지기 때문에, 공간적으로는 떨어져 있지만 문맥적으로 연결된 정보(예: 원거리에 있는 신호등과 교차로 내 차량의 흐름)를 파악하는 데 실패할 수 있다.2</p>
<p>이러한 기술적 교착 상태에서 등장한 **TransFuser(Transformer-based Sensor Fusion)**는 자연어 처리(NLP) 분야를 석권한 트랜스포머(Transformer)의 어텐션(Attention) 메커니즘을 센서 퓨전에 도입하여 혁신적인 돌파구를 마련했다. TransFuser는 이미지와 라이다 데이터를 각각의 토큰(Token) 시퀀스로 변환하고, 셀프 어텐션(Self-Attention)을 통해 공간적 제약 없이 전역적인 문맥(Global Context)을 학습하도록 설계되었다. 이는 기하학적 정합성에 얽매이지 않고 데이터 내재적인 연관성을 찾아내어 융합한다는 점에서 ‘소프트 연관(Soft Association)’ 방식이라 할 수 있다.2</p>
<p>본 보고서는 TransFuser의 아키텍처, 학습 원리, 실험적 성과, 그리고 후속 연구인 TransFuser+ 및 InterFuser와의 비교를 통해 이 기술이 자율주행 분야에 미친 영향을 방대하고 심도 있게 분석한다. 특히 CARLA 시뮬레이터 환경에서의 벤치마크 결과를 바탕으로 기하학적 퓨전 대비 트랜스포머 기반 퓨전의 우월성을 논증하고, 실제 구현을 위한 하드웨어 요구 사항과 엔지니어링 디테일까지 포괄적으로 다룬다.</p>
<h2>2.  이론적 배경 및 관련 연구의 진화</h2>
<h3>2.1  조건부 모방 학습 (Conditional Imitation Learning, CIL)</h3>
<p>자율주행을 구현하는 방식은 크게 매개변수화된 규칙 기반 시스템(Modular Pipeline), 강화 학습(Reinforcement Learning, RL), 그리고 모방 학습(Imitation Learning, IL)으로 나뉜다. TransFuser는 모방 학습, 그중에서도 <strong>조건부 모방 학습(CIL)</strong> 프레임워크를 기반으로 한다.</p>
<p>모방 학습은 전문가(Expert)의 주행 데이터를 수집하고, 신경망이 주어진 상태(State)에서 전문가의 행동(Action)을 복제하도록 지도 학습(Supervised Learning)하는 방식이다. 초기 형태인 행동 복제(Behavior Cloning)는 단순히 입력 이미지에 대해 조향각과 가속도를 매핑하는 데 그쳤으나, 이는 교차로와 같이 다양한 선택지가 존재하는 상황에서 모호성(Ambiguity) 문제를 야기했다. 이를 해결하기 위해 Codevilla 등은 사용자의 의도(Command)를 네트워크의 입력으로 함께 제공하는 CIL을 제안했다.4 예를 들어, “좌회전하라“는 명령이 주어지면 네트워크는 좌회전에 필요한 제어 값을 출력하도록 학습된다.</p>
<p>CIL의 대표적인 베이스라인 모델인 **CILRS (CIL with ResNet and Speed)**는 이미지 처리를 위해 ResNet 백본을 사용하고, 현재 속도 정보를 함께 입력받아 제어 명령을 출력한다.5 그러나 CILRS와 같은 ‘End-to-End’ 제어 방식은 센서 입력에서 제어 출력까지의 과정이 블랙박스(Black-box)이며, 복잡한 도심 환경에서 필요한 전역적인 상황 판단 능력이 부족하다는 비판을 받아왔다. 특히 학습 데이터 분포와 다른 상황(Distribution Shift)이 발생했을 때, 모델이 누적된 오차를 수정하지 못하고 궤도를 이탈하는 현상이 자주 관찰되었다.6 TransFuser는 이러한 CIL의 구조적 한계를 다중 모달리티 퓨전과 웨이포인트(Waypoint) 기반 제어로 극복하고자 했다.</p>
<h3>2.2  센서 퓨전 기술의 발전: 초기 융합에서 트랜스포머까지</h3>
<p>센서 퓨전 기술은 데이터가 결합되는 단계에 따라 크게 세 가지로 분류된다.</p>
<ol>
<li><strong>초기 융합 (Early Fusion)</strong>: 라이다 포인트 클라우드를 이미지 평면에 투영하여 RGB 이미지의 추가 채널(예: RGB-D)로 만들거나, 이미지를 3D 공간으로 변환하여 결합한 후 단일 네트워크에 입력하는 방식이다. 이 방식은 데이터 처리의 초기 단계부터 정보를 결합하므로 낮은 수준(Low-level)의 특징을 잘 활용할 수 있지만, 센서 간의 시공간적 동기화(Synchronization)와 정밀한 캘리브레이션이 필수적이다.7</li>
<li><strong>후기 융합 (Late Fusion)</strong>: 각 센서 데이터를 별도의 네트워크로 처리하여 고수준(High-level)의 특징이나 결정을 내린 후, 이를 병합하는 방식이다. 예를 들어, 카메라 네트워크가 “전방에 보행자 있음“을 탐지하고, 라이다 네트워크가 “전방 10m에 장애물 있음“을 탐지한 후, 논리 회로가 “정지“를 결정하는 식이다. 이 방식은 모듈화가 용이하고 유연하지만, 복잡한 상황에서 센서 간의 상호보완적인 정보를 충분히 활용하지 못한다.5</li>
<li><strong>중간 융합 (Mid-level / Deep Fusion)</strong>: 네트워크의 중간 레이어에서 특징 맵(Feature Map)을 융합하는 방식이다. TransFuser가 속한 범주로, 각 센서의 고유한 특징을 유지하면서도 다단계의 추상화 레벨에서 정보를 교환할 수 있다는 장점이 있다.</li>
</ol>
<p>기존의 중간 융합 연구들은 주로 **기하학적 융합(Geometric Fusion)**에 집중했다. 예를 들어, PointPainting이나 MVX-Net과 같은 모델은 카메라의 시맨틱 정보를 라이다 포인트에 투영하여 객체 탐지 성능을 높였다. 그러나 앞서 언급했듯, 기하학적 융합은 3D 공간상의 물리적 인접성을 전제로 하므로, 신호등과 정지선, 혹은 원거리 차량과 자차의 경로와 같은 ‘비지역적(Non-local)’ 문맥을 놓치기 쉽다.2</p>
<h3>2.3  트랜스포머의 도입과 ‘소프트’ 퓨전</h3>
<p>트랜스포머 아키텍처는 데이터의 순차적 처리(Sequential Processing)를 병렬 처리로 대체하고, **셀프 어텐션(Self-Attention)**을 통해 입력 시퀀스 내의 모든 요소 간의 관계를 한 번에 계산한다. 이는 CNN의 커널(Kernel)이 갖는 국소적 수용장(Receptive Field)의 한계를 넘어, 입력 데이터 전체를 아우르는 전역적 수용장을 제공한다.</p>
<p>TransFuser는 이 점을 이용하여 이미지의 픽셀(또는 패치)과 라이다의 복셀(또는 필라)을 각각의 토큰으로 간주하고, 이들 사이의 어텐션 가중치를 학습한다. 이를 통해 모델은 기하학적인 투영 없이도 “이미지의 좌측 상단에 있는 붉은색 픽셀(신호등)이 라이다 BEV 맵의 중앙에 있는 자차의 거동(정지)과 강하게 연결된다“는 사실을 스스로 학습할 수 있다. 이것이 바로 TransFuser가 제안하는 **소프트 퓨전(Soft Fusion)**의 핵심이다.2</p>
<h2>3.  TransFuser 방법론 및 아키텍처 상세 분석</h2>
<p>TransFuser의 아키텍처는 입력 데이터를 처리하는 인코더, 멀티 모달 퓨전 트랜스포머(MFT) 모듈, 그리고 미래 경로를 예측하는 디코더로 구성된다. 각 구성 요소를 상세히 분석한다.</p>
<h3>3.1  입력 데이터 표현 (Input Representation)</h3>
<p>자율주행을 위한 입력 데이터는 차량에 장착된 센서로부터 수집된다. TransFuser는 다음과 같은 구성을 따른다.</p>
<h4>3.1.1  RGB 카메라 이미지</h4>
<p>초기 자율주행 모델들은 전방 단일 카메라만을 사용하는 경우가 많았으나, 이는 교차로 진입 시 측면에서 접근하는 차량을 감지하지 못하는 치명적인 단점이 있다. 이를 해결하기 위해 TransFuser는 <strong>전방(Forward)</strong>, <strong>좌측 60도(Left 60°)</strong>, **우측 60도(Right 60°)**를 바라보는 세 개의 카메라 이미지를 사용한다.</p>
<ul>
<li><strong>이미지 결합</strong>: 세 개의 이미지는 왜곡 보정 후 수평으로 이어 붙여져 하나의 파노라마 형태 이미지가 된다.</li>
<li><strong>해상도</strong>: 최종 입력 이미지의 해상도는 <strong>704 <span class="math math-inline">\times</span> 160 픽셀</strong>이다. 이는 CARLA 시뮬레이터의 모든 타운(Town)에서 근거리 및 원거리 신호등을 식별하기에 충분한 해상도로 검증되었다.2</li>
<li><strong>시야각 (FOV)</strong>: 결합된 이미지는 약 <strong>132도</strong>의 넓은 시야각을 커버하여, 교차로 상황에서의 상황 인지 능력을 극대화한다.2</li>
</ul>
<h4>3.1.2  라이다 BEV (Bird’s Eye View)</h4>
<p>3D 포인트 클라우드 데이터는 희소성(Sparsity)이 높고 데이터 양이 방대하여 직접 처리하기 까다롭다. TransFuser는 이를 효율적인 2D 격자 형태인 <strong>BEV</strong>로 변환하여 사용한다.</p>
<ul>
<li><strong>복셀화 (Voxelization)</strong>: 차량을 중심으로 좌우 32m, 전방 32m 영역을 0.25m 단위의 격자로 나눈다 (일부 문헌에서는 0.125m 언급, 설정에 따라 가변적).</li>
<li><strong>채널 구성</strong>: 각 격자 내의 포인트 정보를 요약하여 2개의 채널로 구성한다. 첫 번째 채널은 해당 격자 내 포인트들의 <strong>최대 높이(Max Height)</strong>, 두 번째 채널은 포인트의 <strong>밀도(Density)</strong> 정보를 담는다. 결과적으로 라이다 입력은 <span class="math math-inline">256 \times 256 \times 2</span> 형태의 2D 텐서가 된다.2</li>
</ul>
<h3>3.2  듀얼 스트림 인코더와 특징 추출</h3>
<p>TransFuser는 이미지와 라이다 데이터를 각각 처리하기 위해 두 개의 독립적인 CNN 백본을 사용한다.</p>
<ol>
<li><strong>이미지 스트림 (Image Stream)</strong>: ResNet-34 또는 RegNetY-3.2GF와 같은 깊은 신경망을 사용하여 이미지에서 특징을 추출한다. 입력 해상도(<span class="math math-inline">704 \times 160</span>)는 레이어를 거치며 점차 줄어들고 채널 수는 늘어난다.</li>
<li><strong>라이다 스트림 (LiDAR Stream)</strong>: ResNet-18 또는 유사한 경량화된 백본을 사용한다. BEV 데이터는 이미지에 비해 텍스처 정보가 적고 기하학적 구조가 명확하므로 상대적으로 얕은 네트워크로도 충분한 특징 추출이 가능하다.5</li>
</ol>
<p>두 스트림은 각각 4개의 스테이지(Block)로 구성되며, 각 스테이지가 끝날 때마다 특징 맵의 해상도는 절반으로 줄어든다. TransFuser의 핵심은 이 각 스테이지 사이에 <strong>Multi-Modal Fusion Transformer (MFT)</strong> 모듈을 배치하여 정보를 교환한다는 점이다.</p>
<h3>3.3  Multi-Modal Fusion Transformer (MFT) 모듈 상세</h3>
<p>MFT 모듈은 TransFuser의 두뇌에 해당한다. 각 인코더 스테이지에서 출력된 이미지 특징 맵(<span class="math math-inline">F_{img}</span>)과 라이다 특징 맵(<span class="math math-inline">F_{lidar}</span>)을 입력으로 받아 상호 연관성을 학습한다.</p>
<h4>3.3.1  토큰화 및 임베딩</h4>
<p>특징 맵은 <span class="math math-inline">H \times W \times C</span> 차원을 가진다. 이를 트랜스포머가 처리할 수 있는 <span class="math math-inline">N \times C</span> 형태의 시퀀스로 변환해야 한다.</p>
<ul>
<li>각 그리드 위치 <span class="math math-inline">(h, w)</span>의 <span class="math math-inline">C</span>차원 벡터를 하나의 토큰으로 간주한다.</li>
<li>CNN은 위치 불변성(Translation Invariance)을 가지므로 위치 정보가 소실될 수 있다. 이를 보완하기 위해 각 토큰에 학습 가능한 **위치 임베딩(Positional Embedding)**을 더해준다.</li>
</ul>
<h4>3.3.2  셀프 어텐션 메커니즘 (Self-Attention)</h4>
<p>이미지 토큰 시퀀스와 라이다 토큰 시퀀스는 하나의 긴 시퀀스로 결합(Concatenation)된다.</p>
<p><span class="math math-display">F_{in} =</span></p>
<p>이 결합된 시퀀스에 대해 표준 트랜스포머의 셀프 어텐션 연산이 수행된다.</p>
<p><span class="math math-display">Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</span></p>
<p>여기서 <span class="math math-inline">Q, K, V</span>는 입력 시퀀스 <span class="math math-inline">F_{in}</span>으로부터 선형 변환을 통해 생성된다. 이 연산을 통해 이미지의 특정 영역 토큰은 라이다의 모든 영역 토큰과 ’대화’할 수 있게 되며, 그 반대도 마찬가지다. 어텐션 행렬 <span class="math math-inline">A</span>는 모달리티 간의 상관관계를 나타내는 가중치 맵이 된다.</p>
<h4>3.3.3  비선형 변환 및 재구성</h4>
<p>어텐션 출력은 MLP(Multi-Layer Perceptron)를 거쳐 비선형성을 확보하고, 다시 원래의 특징 맵 형태(<span class="math math-inline">H \times W \times C</span>)로 분리 및 재구성된다.</p>
<p><span class="math math-display">F_{out} = MLP(Attention(F_{in})) + F_{in}</span></p>
<p>여기서 잔차 연결(Residual Connection, <span class="math math-inline">+ F_{in}</span>)은 퓨전 과정에서 원래 센서의 고유 정보가 유실되지 않도록 보호하는 중요한 역할을 한다.2</p>
<h3>3.4 웨이포인트 예측 네트워크 및 PID 제어</h3>
<p>인코더와 MFT 모듈을 모두 통과한 최종 특징 맵들은 **평균 풀링(Average Pooling)**을 통해 각각 512차원의 벡터로 압축된다. 이후 두 벡터는 **요소별 합(Element-wise Summation)**을 통해 512차원의 단일 문맥 벡터(Context Vector)로 통합된다.2</p>
<h4>3.4.1 GRU 기반 웨이포인트 예측</h4>
<p>통합된 특징 벡터는 GRU(Gated Recurrent Unit) 기반의 디코더에 입력된다. 디코더는 현재 차량의 위치를 원점 <span class="math math-inline">(0,0)</span>으로 하여, 미래의 <span class="math math-inline">T</span>개 시점(예: <span class="math math-inline">t=1, \dots, 4</span>)에 대한 차량의 목표 위치 좌표 <span class="math math-inline">(x_t, y_t)</span>를 자기회귀적(Auto-regressive)으로 예측한다.</p>
<ul>
<li>입력: 현재 상태 특징 벡터, 목표 지점(Goal Location - GPS 좌표 등).</li>
<li>출력: <span class="math math-inline">\{\delta w_t\}_{t=1}^{T}</span> (상대적 웨이포인트 변화량).</li>
</ul>
<p>이 방식은 직접 조향각을 출력하는 것보다 훨씬 안정적이며, 모델이 계획한 경로를 시각화하여 해석 가능성을 제공한다는 장점이 있다.2</p>
<h4>3.4.2 PID 제어기 (Controller)</h4>
<p>예측된 웨이포인트는 저수준 제어기인 PID(Proportional-Integral-Derivative) 제어기를 통해 실제 차량 제어 명령으로 변환된다. TransFuser 연구진은 실험을 통해 최적화된 PID 파라미터를 다음과 같이 제시했다.5</p>
<ul>
<li><strong>종방향 제어 (Longitudinal Control - 가속/제동)</strong>:
<ul>
<li><span class="math math-inline">K_p = 5.0</span></li>
<li><span class="math math-inline">K_i = 0.5</span></li>
<li><span class="math math-inline">K_d = 1.0</span></li>
<li>목표 속도와 현재 속도의 차이를 기반으로 가속페달과 브레이크를 조절한다.</li>
</ul>
</li>
<li><strong>횡방향 제어 (Lateral Control - 조향)</strong>:
<ul>
<li><span class="math math-inline">K_p = 1.25</span></li>
<li><span class="math math-inline">K_i = 0.75</span></li>
<li><span class="math math-inline">K_d = 0.3</span></li>
<li>예측된 경로의 곡률과 차량의 헤딩(Heading) 각도 차이를 최소화하도록 조향각을 조절한다.</li>
</ul>
</li>
<li><strong>버퍼</strong>: 적분항(Integral term) 계산을 위해 최근 40개 프레임의 오차를 버퍼에 저장하여 이동 평균을 사용한다.</li>
</ul>
<p>이러한 명시적인 PID 파라미터 공개는 재현성을 높이는 중요한 요소이다.</p>
<h3>3.5 손실 함수 (Loss Function)</h3>
<p>TransFuser는 전문가의 웨이포인트를 모방하도록 학습된다. 주된 손실 함수는 L1 Loss이다.</p>
<p>$$ \mathcal{L}{waypoint} = \sum{t=1}^{T} |</p>
<p>| w_t - w_t^{gt} ||_1 $$</p>
<p>여기서 <span class="math math-inline">w_t</span>는 예측된 웨이포인트, <span class="math math-inline">w_t^{gt}</span>는 전문가(Autopilot)의 실제 주행 궤적이다.</p>
<p>추가적으로, 모델이 신호등 상태나 정지 신호를 무시하는 것을 방지하기 위해 보조 손실(Auxiliary Loss)들이 사용될 수 있다. 예를 들어, TransFuser+ 등 후속 연구에서는 다음과 같은 손실들이 추가되었다.</p>
<ul>
<li><strong>깊이 추정 손실</strong>: L1 Loss.</li>
<li><strong>시맨틱 세그멘테이션 손실</strong>: Cross-Entropy Loss.</li>
<li><strong>객체 탐지 히트맵 손실</strong>: Focal Loss.2</li>
</ul>
<h2>4. 실험 환경 및 벤치마크 (CARLA)</h2>
<p>TransFuser의 성능 검증은 자율주행 연구의 표준 시뮬레이터로 자리 잡은 **CARLA (Car Learning to Act)**에서 수행되었다.</p>
<h3>4.1 데이터셋 구성</h3>
<p>연구진은 CARLA 0.9.10 버전을 사용하여 대규모 데이터셋을 구축했다.</p>
<ul>
<li><strong>데이터 생성</strong>: CARLA의 ’Rule-based Autopilot’을 전문가 에이전트로 사용하여 주행 데이터를 수집했다. 이 전문가는 시뮬레이터 내부의 특권 정보(Privileged Information - 신호등 상태, 주변 차량 위치 등)에 접근할 수 있어 완벽한 주행이 가능하다.</li>
<li><strong>규모</strong>: 총 8개의 타운(Town01 ~ Town07, Town10HD)에서 다양한 날씨와 교통 상황을 조합하여 약 15만 프레임(약 210GB)의 데이터를 수집했다. 이 중 13만 프레임은 학습용, 2만 프레임은 검증용으로 사용된다.5</li>
<li><strong>데이터 증강</strong>: 과적합을 방지하기 위해 카메라 위치를 약간 흔들거나 회전시키는 방식의 증강 기법이 적용되었다.</li>
</ul>
<h3>4.2 CARLA Leaderboard 및 Longest6 벤치마크</h3>
<p>모델 평가는 CARLA Leaderboard의 공식 메트릭을 따른다.</p>
<ol>
<li><strong>경로 완료율 (Route Completion, RC)</strong>: 에이전트가 주어진 경로를 몇 퍼센트 주행했는지 측정한다.</li>
<li><strong>위반 점수 (Infraction Score, IS)</strong>: 충돌(차량, 보행자, 지형물), 차선 침범, 신호 위반 등의 사고가 발생할 때마다 1.0에서 시작하여 점수가 삭감된다 (Multiplicative Penalty).
<ul>
<li>충돌 시 패널티 계수가 0.6 등으로 곱해지므로, 사고가 잦을수록 점수는 급격히 0에 수렴한다.</li>
</ul>
</li>
<li><strong>주행 점수 (Driving Score, DS)</strong>: 가장 중요한 지표로, <span class="math math-inline">DS = RC \times IS</span>로 계산된다. 즉, 경로를 끝까지 완주하면서도 위반 사항이 없어야 높은 점수를 받을 수 있다.</li>
</ol>
<p>연구진은 기존 리더보드의 경로들이 상대적으로 짧고 단순하다는 점을 지적하며, **‘Longest6’**라는 새로운 벤치마크를 제안했다. 이는 6개의 마을에서 가장 긴 경로들을 선별하고, 교통 밀도와 대향 차량 빈도를 높여 사고 위험이 극대화된 시나리오다. 이는 모델의 장기 주행 안정성과 극한 상황 대처 능력을 평가하는 데 적합하다.2</p>
<h2>5. 실험 결과 및 심층 분석</h2>
<h3>5.1 정량적 성능 평가: 압도적인 성능 격차</h3>
<p>TransFuser는 발표 당시(2021-2022) CARLA 리더보드에서 기존 방법론들을 압도하는 성능을 기록했다. 아래 표는 주요 모델들과의 성능 비교를 보여준다.</p>
<table><thead><tr><th><strong>모델 (Model)</strong></th><th><strong>입력 모달리티</strong></th><th><strong>주행 점수 (DS)</strong></th><th><strong>경로 완료율 (RC)</strong></th><th><strong>위반 점수 (IS)</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>TransFuser (Ours)</strong></td><td><strong>Camera + LiDAR</strong></td><td><strong>61.18</strong></td><td><strong>86.69</strong></td><td><strong>0.71</strong></td><td><strong>SOTA (당시 기준)</strong></td></tr>
<tr><td>LAV [Chen et al.]</td><td>Camera + LiDAR</td><td>61.85</td><td>94.46</td><td>0.64</td><td>후속 연구</td></tr>
<tr><td>Geometric Fusion</td><td>Camera + LiDAR</td><td>41.70</td><td>-</td><td>-</td><td>기하학적 퓨전 베이스라인</td></tr>
<tr><td>GRIAD</td><td>Camera + LiDAR</td><td>36.79</td><td>61.85</td><td>0.60</td><td>강화학습 기반</td></tr>
<tr><td>World on Rails</td><td>Camera</td><td>31.37</td><td>57.65</td><td>0.56</td><td>-</td></tr>
<tr><td>CILRS</td><td>Camera</td><td>7.47</td><td>13.40</td><td>-</td><td>기존 CIL 베이스라인</td></tr>
<tr><td>Latent TransFuser</td><td>Camera</td><td>45.20</td><td>66.31</td><td>0.72</td><td>LiDAR 없는 변형 모델</td></tr>
</tbody></table>
<p>(출처: Snippets 3 종합)</p>
<p><strong>분석 및 통찰:</strong></p>
<ol>
<li><strong>기하학적 퓨전 대비 우위</strong>: TransFuser(61.18)는 동일한 센서 구성을 가진 Geometric Fusion(41.70) 대비 약 <strong>47%</strong> 높은 주행 점수를 기록했다. 또한, 킬로미터당 충돌 횟수를 <strong>48%</strong> 감소시켰다.1 이는 “어떻게 융합하느냐“가 센서의 종류만큼이나 중요함을 시사한다. 트랜스포머를 통한 소프트 퓨전이 복잡한 도심 환경에서 훨씬 강건함을 입증했다.</li>
<li><strong>이미지 전용 모델의 한계 극복</strong>: CILRS(7.47)의 처참한 점수는 복잡한 시나리오에서 단순 CNN 기반의 End-to-End 제어가 불가능함을 보여준다. 반면, TransFuser는 다양한 센서 정보를 문맥적으로 통합하여 이를 극복했다.</li>
<li><strong>Latent TransFuser의 발견</strong>: 라이다를 사용하지 않고 위치 인코딩만으로 트랜스포머 구조를 적용한 Latent TransFuser(45.20)가 기존의 Geometric Fusion(41.70)보다도 높은 점수를 기록했다는 점은 매우 충격적이다. 이는 트랜스포머 아키텍처 자체가 이미지 내의 전역적 문맥(Global Context)을 파악하는 데 탁월하며, 이것이 라이다의 물리적 정보만큼이나 주행 성능에 결정적임을 의미한다.4</li>
</ol>
<h3>5.2 정성적 분석: 어텐션 맵의 시각화</h3>
<p>TransFuser가 실제로 어떻게 작동하는지 확인하기 위해 연구진은 어텐션 맵(Attention Map)을 시각화했다.</p>
<ul>
<li><strong>교차로 시나리오</strong>: 비보호 좌회전 상황에서, 이미지 스트림의 트랜스포머 토큰들은 전방의 신호등뿐만 아니라, 라이다 스트림에서 감지된 대향 차선의 다가오는 차량에 강한 어텐션 가중치를 부여했다.</li>
<li><strong>의미</strong>: 이는 모델이 명시적인 규칙(Rule)을 주입받지 않았음에도 불구하고, “좌회전을 하려면 신호도 봐야 하지만, 반대편에서 오는 차도 신경 써야 한다“는 운전의 논리를 데이터로부터 스스로 학습했음을 보여준다. 기하학적 퓨전은 공간적으로 분리된 이 두 객체(신호등과 대향 차량)를 연결하지 못해 사고를 유발하는 경우가 많았다.3</li>
</ul>
<h3>5.3 실패 사례 및 한계점</h3>
<p>비록 높은 성능을 보였지만 TransFuser도 완벽하지 않다.</p>
<ul>
<li><strong>후방 추돌</strong>: 후방 카메라가 없거나 정보를 충분히 활용하지 못해, 뒤에서 빠르게 접근하는 차량에 대한 대처가 미흡한 경우가 있다.</li>
<li><strong>악천후</strong>: 폭우가 쏟아지는 환경에서는 라이다 포인트 클라우드에 노이즈가 발생하고 카메라 시야가 가려져 성능 저하가 발생한다. 그러나 단일 센서 모델보다는 훨씬 강건한 모습을 보였다.</li>
</ul>
<h2>6. TransFuser의 진화: TransFuser+ 및 InterFuser</h2>
<p>TransFuser의 성공은 이후 더 고도화된 모델들의 등장으로 이어졌다.</p>
<h3>6.1 TransFuser+: 보조 작업(Auxiliary Tasks)을 통한 강화</h3>
<p>TransFuser+는 오리지널 아키텍처에 다양한 보조 작업을 추가하여 특징 추출기의 성능을 향상시킨 버전이다.11</p>
<ul>
<li><strong>멀티태스킹 학습</strong>: 단순히 웨이포인트만 예측하는 것이 아니라, 이미지 브랜치에서는 깊이(Depth)와 시맨틱 세그멘테이션을 예측하고, 라이다 브랜치에서는 HD 맵과 객체 탐지(Object Detection)를 수행하도록 학습된다.</li>
<li><strong>효과</strong>: 이러한 보조 작업은 인코더가 주행 환경의 3D 구조와 의미론적 정보를 더 명확하게 학습하도록 강제한다. 이는 주행 점수 향상뿐만 아니라 모델의 해석 가능성을 높이는 데 기여했다.</li>
</ul>
<h3>6.2 InterFuser: 안전성과 해석 가능성의 결합</h3>
<p>InterFuser(Interpretable Sensor Fusion Transformer)는 TransFuser의 가장 강력한 후속작 중 하나로, 안전성(Safety)에 초점을 맞추었다.4</p>
<ul>
<li><strong>안전 제어기 (Safety Controller)</strong>: InterFuser는 웨이포인트 외에도 ’객체 밀도 맵(Object Density Map)’을 예측한다. 이 맵은 차량 주변의 장애물 분포를 나타내며, 별도의 안전 제어기가 이를 모니터링한다. 만약 모델이 예측한 웨이포인트가 객체 밀도가 높은 지역을 통과하려 하면, 안전 제어기가 개입하여 제동을 건다.</li>
<li><strong>성능</strong>: InterFuser는 CARLA 리더보드에서 <strong>DS 76.18</strong>을 기록하며 TransFuser(61.18)를 크게 뛰어넘었다. 이는 딥러닝 모델의 확률적 예측에만 의존하지 않고, 명시적인 안전 제약 조건을 결합한 하이브리드 접근법이 자율주행의 안전성을 확보하는 데 필수적임을 시사한다.</li>
</ul>
<h2>7. 구현 및 하드웨어 엔지니어링 고려사항</h2>
<p>TransFuser를 실제 연구나 개발에 적용하기 위해 필요한 하드웨어 및 구현 세부 사항은 다음과 같다.</p>
<h3>7.1 하드웨어 요구 사항</h3>
<p>트랜스포머 아키텍처, 특히 고해상도 이미지와 3D 볼륨 데이터를 처리하는 경우 메모리 사용량이 상당하다.</p>
<ul>
<li><strong>GPU 메모리</strong>: 학습 시 배치 사이즈(Batch Size)를 10~12 정도로 설정할 경우, 최소 <strong>12GB 이상의 VRAM</strong>을 가진 GPU(예: RTX 3080 Ti, Titan XP)가 필요하다. TransFuser+와 같이 보조 작업이 추가된 모델은 24GB 이상의 메모리(RTX 3090, A100)가 권장된다.13</li>
<li><strong>학습 시간</strong>: 연구진은 8개의 NVIDIA 1080Ti GPU를 사용하여 약 15만 프레임 데이터를 <strong>2~3일</strong> 동안 학습시켰다고 보고했다.15 단일 GPU 환경에서는 1주일 이상 소요될 수 있으므로 분산 학습(Distributed Training) 설정이 권장된다.</li>
</ul>
<h3>7.2 소프트웨어 및 코드 구조</h3>
<p>TransFuser의 공식 코드는 PyTorch 프레임워크를 기반으로 하며, GitHub 저장소를 통해 공개되어 있다.9</p>
<ul>
<li><strong>폴더 구조</strong>:
<ul>
<li><code>team_code/</code>: 모델 아키텍처(<code>transfuser.py</code>), PID 제어기(<code>controller.py</code>), 데이터 로더 등이 포함된다.</li>
<li><code>leaderboard/</code>: CARLA 리더보드 평가 스크립트.</li>
<li><code>tools/</code>: 데이터 생성(<code>datagen.sh</code>) 및 시각화 도구.</li>
</ul>
</li>
<li><strong>설치</strong>: CARLA 0.9.10.1 버전 설치가 필수적이며, <code>conda</code> 가상 환경을 통해 <code>torch-scatter</code> 등 그래프 신경망 관련 라이브러리를 설치해야 한다. <code>setup_carla.sh</code> 스크립트가 이를 자동화한다.9</li>
</ul>
<h3>7.3 추론 속도 및 실시간성</h3>
<p>자율주행 시스템은 실시간성(Real-time capability)이 중요하다. TransFuser는 약 <strong>22Hz</strong>(초당 22프레임)의 추론 속도를 보인다고 보고되었다. 일반적인 자율주행 제어 루프가 10Hz~20Hz로 동작함을 고려할 때, 이는 실제 차량에 탑재 가능한 수준이다. 다만, 이는 고성능 데스크탑 GPU 기준이며, 임베디드 보드(NVIDIA Orin 등)에서의 최적화는 별도의 과제이다.</p>
<h2>8. 결론 및 향후 전망</h2>
<p>TransFuser는 자율주행 센서 퓨전 분야에서 ’기하학적 정렬’이라는 오랜 고정관념을 깨고, 트랜스포머 기반의 ’문맥적 연결’이라는 새로운 패러다임을 제시한 기념비적인 연구이다. 이 기술은 카메라와 라이다라는 이질적인 센서 데이터를 물리적 제약 없이 유연하게 융합함으로써, 교차로 비보호 좌회전과 같은 고난도 시나리오에서 인간 수준의 상황 판단 능력을 보여주었다.</p>
<p>실험 결과, TransFuser는 CARLA 리더보드에서 기존 SOTA 모델들을 큰 격차로 따돌리며 그 성능을 입증했다. 특히 라이다 없이 이미지 정보의 전역적 문맥만을 활용한 Latent TransFuser의 성공은 트랜스포머 아키텍처가 컴퓨터 비전 분야에서 갖는 잠재력을 다시 한번 확인시켜 주었다.</p>
<p>물론 높은 연산 비용과 시뮬레이션 환경에 편중된 검증이라는 한계는 존재한다. 그러나 TransFuser가 제시한 **‘Global Context-aware Fusion’**의 개념은 이후 InterFuser, TransFuser+, TF++ 등 수많은 후속 연구의 기반이 되었으며, End-to-End 자율주행 기술을 한 단계 도약시키는 데 결정적인 역할을 했다. 향후 연구는 이러한 강력한 인지-판단 모델을 경량화하여 실제 차량의 제한된 컴퓨팅 환경에 적용하고, 시뮬레이션과 현실 세계의 간극(Sim-to-Real Gap)을 줄이는 방향으로 나아가야 할 것이다. TransFuser는 단순한 모델 하나를 넘어, 자율주행 AI가 세상을 이해하는 방식을 근본적으로 확장시킨 연구로 평가받을 것이다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>TransFuser: Imitation with Transformer-Based Sensor Fusion … - arXiv, 12월 14, 2025에 액세스, https://arxiv.org/abs/2205.15997</li>
<li>TransFuser: Imitation with Transformer-Based Sensor Fusion for …, 12월 14, 2025에 액세스, https://www.cvlibs.net/publications/Chitta2022PAMI.pdf</li>
<li>TransFuser: Imitation with Transformer-Based Sensor Fusion, 12월 14, 2025에 액세스, https://www.ki-deltalearning.de/securedl/sdl-eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpYXQiOjE3NjM1NzQyNzQsImV4cCI6MTc2MzY2NDI3NCwidXNlciI6MCwiZ3JvdXBzIjpbMCwtMV0sImZpbGUiOiJmaWxlYWRtaW4vS0lfRGVsdGFMZWFybmluZy9FdmVudHMvZmluYWxfZXZlbnQvNDRfVHJhbnNGdXNlcl9yb2xsdXAtY29tcHJlc3NlZC5wZGYiLCJwYWdlIjo2Nn0.JjPfM3m-ulkMHvFDksaBrv-exeBiW4-z6-71zpXKy74/44_TransFuser_rollup-compressed.pdf</li>
<li>Safety-Enhanced Autonomous Driving Using Interpretable Sensor …, 12월 14, 2025에 액세스, https://proceedings.mlr.press/v205/shao23a/shao23a.pdf</li>
<li>Supplementary Material for Multi-Modal Fusion Transformer for End …, 12월 14, 2025에 액세스, https://www.cvlibs.net/publications/Prakash2021CVPR_supplementary.pdf</li>
<li>TransFuser: Imitation With Transformer-Based Sensor Fusion for …, 12월 14, 2025에 액세스, https://www.researchgate.net/publication/362808814_TransFuser_Imitation_with_Transformer-Based_Sensor_Fusion_for_Autonomous_Driving</li>
<li>Cross-Modal RGB–LiDAR Fusion - Emergent Mind, 12월 14, 2025에 액세스, https://www.emergentmind.com/topics/cross-modal-rgb-lidar-fusion</li>
<li>Multi-Modal Fusion Transformer for End-to-End Autonomous Driving, 12월 14, 2025에 액세스, https://openaccess.thecvf.com/content/CVPR2021/papers/Prakash_Multi-Modal_Fusion_Transformer_for_End-to-End_Autonomous_Driving_CVPR_2021_paper.pdf</li>
<li>[PAMI’23] TransFuser: Imitation with Transformer-Based Sensor …, 12월 14, 2025에 액세스, https://github.com/autonomousvision/transfuser</li>
<li>transfuser - Aditya Prakash, 12월 14, 2025에 액세스, https://ap229997.github.io/projects/transfuser/</li>
<li>arXiv:2309.07808v3 [cs.CV] 12 Sep 2024, 12월 14, 2025에 액세스, https://arxiv.org/pdf/2309.07808</li>
<li>Penalty-Based Imitation Learning With Cross Semantics Generation …, 12월 14, 2025에 액세스, https://arxiv.org/pdf/2303.11888</li>
<li>What are the minimum GPU memory requirements for training …, 12월 14, 2025에 액세스, <a href="https://massedcompute.com/faq-answers/?question=What+are+the+minimum+GPU+memory+requirements+for+training+transformer-based+models?">https://massedcompute.com/faq-answers/?question=What%20are%20the%20minimum%20GPU%20memory%20requirements%20for%20training%20transformer-based%20models?</a></li>
<li>Calculating GPU memory for serving LLMs | LLM Inference Handbook, 12월 14, 2025에 액세스, https://bentoml.com/llm/getting-started/calculating-gpu-memory-for-llms</li>
<li>How long does it take to generate the data set? And what is … - GitHub, 12월 14, 2025에 액세스, https://github.com/autonomousvision/transfuser/issues/10</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>