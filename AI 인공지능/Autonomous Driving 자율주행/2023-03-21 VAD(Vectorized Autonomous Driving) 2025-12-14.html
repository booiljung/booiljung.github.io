<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:VAD(Vectorized Autonomous Driving)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>VAD(Vectorized Autonomous Driving)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">자율주행 (Autonomous Driving)</a> / <span>VAD(Vectorized Autonomous Driving)</span></nav>
                </div>
            </header>
            <article>
                <h1>VAD(Vectorized Autonomous Driving)</h1>
<p>2025-12-14, G30DR</p>
<h2>1.  서론: 자율주행 패러다임의 구조적 전환과 벡터화의 필연성</h2>
<p>자율주행 기술의 발전사는 복잡한 현실 세계를 기계가 이해할 수 있는 형태로 추상화(Abstraction)하고, 이를 바탕으로 최적의 행동을 결정하는 과정의 진화라 정의할 수 있다. 지난 수십 년간 학계와 산업계를 지배해 온 고전적인 자율주행 아키텍처는 인지(Perception), 예측(Prediction), 계획(Planning), 제어(Control)가 명확히 분리된 모듈형 파이프라인(Modular Pipeline)이었다. 이 방식은 각 모듈을 독립적으로 개발하고 검증하기 용이하며, 시스템의 오작동 원인을 추적하기 쉽다는 공학적 장점이 있었다. 그러나 모듈 간 정보 전달 과정에서 발생하는 정보 손실과 상류 모듈의 오차가 하류 모듈로 전파되어 증폭되는 ‘오차 누적(Cascading Errors)’ 현상은 해결하기 어려운 난제였다. 또한, 각 모듈이 서로 다른 목적함수(Objective Function)를 최적화함에 따라, 전체 시스템 차원의 최적화가 이루어지지 않는 지역 최적화(Local Optimization) 문제에 직면하게 되었다.1</p>
<p>이러한 모듈형 접근의 한계를 극복하기 위해 등장한 것이 종단간(End-to-End) 자율주행 패러다임이다. 종단간 학습은 센서 입력부터 제어 출력까지를 하나의 거대한 신경망으로 연결하여, 주행이라는 최종 목적을 위해 전체 네트워크를 최적화한다. 초기 종단간 모델들은 주로 입력 이미지를 제어 신호로 직접 매핑하는 행동 복제(Behavioral Cloning) 방식에 의존했으나, 이는 해석 가능성(Interpretability)이 낮고 복잡한 도심 환경에서의 인과 관계 학습에 한계를 보였다. 이에 따라 최근의 연구들은 인지, 예측, 계획을 내부적으로 수행하되 이를 미분 가능한(Differentiable) 구조로 연결하는 통합 모델 형태로 발전하고 있다.3</p>
<p>하지만 초기 통합 모델들, 예를 들어 UniAD(Unified Autonomous Driving)와 같은 선구적인 연구들조차 주행 환경을 표현하는 방식(Scene Representation)에 있어 근본적인 비효율성을 안고 있었다. 이들은 주로 의미론적 분할(Semantic Segmentation) 맵이나 점유 그리드(Occupancy Grid)와 같은 픽셀 기반의 래스터화(Rasterized) 표현 방식을 사용했다. 래스터화된 표현은 시각적으로 직관적이지만, 자율주행에 필요한 객체 수준의 구조적 정보(예: 차선의 연결성, 차량의 개별 거동)를 명시적으로 담지 못한다. 또한, 고해상도의 덴스 맵(Dense Map)을 처리하기 위해서는 막대한 연산 자원이 소요되며, 이는 실시간 추론이 필수적인 자율주행 시스템에 큰 부담으로 작용한다. 더욱이 픽셀 형태의 출력값을 주행 궤적 생성에 활용하기 위해서는 복잡하고 계산 비용이 높은 후처리(Post-processing) 알고리즘이 필수적으로 요구되었다.2</p>
<p>이러한 배경 속에서 2023년 ICCV에 발표된 **VAD(Vectorized Autonomous Driving)**는 자율주행 씬 표현의 패러다임을 ’래스터’에서 ’벡터’로 전환할 것을 제안하며 등장했다. VAD는 주행 환경의 모든 요소—정적 맵(Static Map)과 동적 에이전트(Dynamic Agents)—를 희소한(Sparse) 벡터 형태로 모델링한다. 이는 데이터의 표현 효율성을 극대화할 뿐만 아니라, 계획(Planning) 단계에서 기하학적 제약 조건을 명시적으로 적용하는 것을 가능하게 한다. VAD는 기존 래스터 기반 모델 대비 압도적인 추론 속도(최대 9.3배 향상)와 최첨단(SOTA) 계획 성능을 동시에 달성하며, 벡터화된 표현이 자율주행의 효율성과 안전성을 동시에 확보할 수 있는 핵심 열쇠임을 증명하였다.2</p>
<p>본 보고서는 VAD(v1)와 그 후속 연구인 VADv2를 중심으로 벡터화된 자율주행 기술의 아키텍처, 작동 원리, 성능 특성, 그리고 한계점을 심층적으로 조사하고 분석한다. 특히 결정론적(Deterministic) 접근법에서 확률론적(Probabilistic) 계획으로의 진화 과정을 상세히 다루며, 이를 통해 자율주행 AI가 불확실한 현실 세계를 어떻게 이해하고 대응하는지 기술적 통찰을 제공하고자 한다.</p>
<h2>2.  자율주행 씬 표현 기술의 진화와 이론적 배경</h2>
<p>자율주행 시스템의 성능은 ’환경을 어떻게 디지털 형태로 추상화하는가’에 달려 있다. 씬 표현(Scene Representation) 방식의 진화는 곧 자율주행 인지 기술의 발전을 대변한다.</p>
<h3>2.1  덴스 래스터화 표현 (Dense Rasterized Representation)</h3>
<p>초기 딥러닝 기반 자율주행, 특히 조감도(Bird’s Eye View, BEV) 인지 기술은 주로 의미론적 분할(Semantic Segmentation) 작업에 집중했다.</p>
<ul>
<li><strong>특징:</strong> 카메라나 LiDAR로 수집된 데이터를 BEV 공간으로 투영한 후, 각 픽셀(Pixel) 또는 복셀(Voxel)이 도로인지, 차선인지, 차량인지를 분류한다. HDMapNet과 같은 초기 모델들이 이 방식을 채택했다.6</li>
<li><strong>한계:</strong></li>
</ul>
<ol>
<li><strong>계산 비용:</strong> 해상도가 <span class="math math-inline">200 \times 200</span>만 되어도 4만 개의 예측값이 필요하며, 공간이 넓어질수록 연산량과 메모리 사용량이 기하급수적으로 증가한다.</li>
<li><strong>구조 정보 부재:</strong> 픽셀들의 집합만으로는 차선이 어디서 시작해서 어디로 이어지는지, 인접한 픽셀들이 같은 객체인지 등 ‘인스턴스(Instance)’ 수준의 정보를 파악하기 어렵다.</li>
<li><strong>후처리 의존:</strong> 계획 모듈이 경로를 생성하려면 래스터 맵을 다시 벡터 형태로 변환하거나(Skeletonization), 복잡한 비용 맵(Cost Map) 기반의 탐색 알고리즘을 돌려야 하므로 전체 파이프라인의 속도를 저하시킨다.3</li>
</ol>
<h3>2.2  벡터화 표현의 부상 (Rise of Vectorized Representation)</h3>
<p>래스터화의 한계를 극복하기 위해 등장한 것이 벡터화 표현이다. 이는 도로의 중심선, 경계, 횡단보도 등을 일련의 점(Point) 집합이나 파라메트릭 곡선(Parametric Curve)으로 표현한다.</p>
<ul>
<li><strong>VectorMapNet:</strong> 맵 요소를 벡터의 시퀀스로 간주하고 자기회귀(Autoregressive) 방식을 통해 순차적으로 예측했다.</li>
<li><strong>MapTR:</strong> 맵 요소를 점들의 집합(Point Set)으로 정의하고, 순열 불변성(Permutation Equivalence)을 고려한 매칭 알고리즘을 도입하여 맵 생성 속도와 정확도를 비약적으로 향상시켰다. VAD는 이 MapTR의 벡터화 철학을 계승하고 확장한 모델이다.6</li>
<li><strong>장점:</strong> 벡터 데이터는 매우 희소(Sparse)하여 메모리를 적게 차지하며, 자율주행 차의 제어 알고리즘이 즉각적으로 활용할 수 있는 기하학적 정보(좌표, 방향 등)를 제공한다. VAD는 이러한 벡터 표현을 인지 단계에 그치지 않고, 계획 단계의 입력 및 제약 조건으로까지 확장 적용했다는 점에서 중요한 의의를 갖는다.2</li>
</ul>
<h3>2.3  쿼리 기반 학습 (Query-based Learning)</h3>
<p>VAD를 비롯한 최신 모델들의 핵심 메커니즘은 트랜스포머(Transformer)의 쿼리(Query) 개념이다.</p>
<ul>
<li><strong>작동 원리:</strong> 이미지 전체를 픽셀 단위로 처리하는 대신, 정보를 추출하고자 하는 대상(에이전트, 맵 요소, 자차 등)을 나타내는 학습 가능한 임베딩 벡터(Query)를 정의한다. 이 쿼리들이 인코딩된 센서 특징(Feature)과 교차 주의(Cross-Attention)를 수행하며 필요한 정보만을 선택적으로 흡수한다.</li>
<li><strong>효율성:</strong> 쿼리의 개수(예: 맵 요소 100개, 에이전트 100개)는 픽셀 수에 비해 현저히 적으므로 연산 효율성이 극대화된다. VAD는 이 방식을 통해 ’완전한 벡터화’를 구현하였다.2</li>
</ul>
<h2>3.  VAD(v1): 효율적인 자율주행을 위한 벡터화된 씬 표현 아키텍처</h2>
<p>VAD(v1)는 “자율주행은 고효율의 완전한 벡터화 방식(Fully Vectorized Manner)으로 수행될 수 있다“는 믿음 아래 설계되었다. VAD의 아키텍처는 크게 네 가지 단계로 구성된다: (1) 백본 및 BEV 인코딩, (2) 벡터화된 씬 학습, (3) 쿼리 상호작용을 통한 계획, (4) 벡터화된 계획 제약 조건.3</p>
<h3>3.1  백본 네트워크 및 BEV 특징 추출 (Backbone &amp; BEV Encoding)</h3>
<p>VAD의 입력은 차량 주변을 감싸는 다중 시점 카메라(Multi-view Cameras) 이미지이다.</p>
<ol>
<li><strong>2D 특징 추출:</strong> ResNet-50 또는 ResNet-101과 같은 표준 컨볼루션 신경망(CNN)을 사용하여 각 카메라 이미지에서 고차원 특징(Feature)을 추출한다.</li>
<li><strong>BEV 변환:</strong> 추출된 2D 특징들은 <strong>BEVFormer</strong>의 방법론을 차용하여 조감도(BEV) 특징 공간으로 투영된다. 이 과정에서 BEV 쿼리(BEV Queries)가 사용되며, 시간적 정보(Temporal Information)를 반영하기 위해 이전 프레임의 BEV 특징과 현재 프레임의 특징을 융합한다. 이렇게 생성된 BEV 특징 맵은 이후의 모든 벡터화 작업의 공통된 기반(Shared Feature)이 된다. 이는 멀티태스크 학습의 효율성을 높이는 중요한 설계이다.3</li>
</ol>
<h3>3.2  벡터화된 씬 학습 (Vectorized Scene Learning)</h3>
<p>VAD는 BEV 특징 맵으로부터 두 가지 핵심 정보, 즉 정적 맵과 동적 에이전트를 병렬적으로 벡터화하여 추출한다.</p>
<h4>3.2.1  벡터화된 맵 학습 (Vectorized Map Learning)</h4>
<p>VAD는 MapTR의 접근 방식을 활용하여 맵 요소를 학습한다.</p>
<ul>
<li><strong>맵 쿼리 (<span class="math math-inline">Q_{map}</span>):</strong> 시스템은 <span class="math math-inline">N_m</span>개의 맵 쿼리를 정의한다. 각 쿼리는 특정 맵 인스턴스(예: 하나의 차선)를 담당한다.</li>
<li><strong>요소 정의:</strong> VAD는 맵 요소를 세 가지로 분류한다: (1) 차선 분리대(Lane Divider), (2) 도로 경계(Road Boundary), (3) 횡단보도(Pedestrian Crossing).</li>
<li><strong>벡터 표현:</strong> 각 맵 요소는 <span class="math math-inline">N_p</span>개의 점으로 구성된 폴리라인 벡터 <span class="math math-inline">V_{map} \in \mathbb{R}^{N_m \times N_p \times 2}</span>로 표현된다.</li>
<li><strong>차선 분리대:</strong> 차량의 주행 방향 정보를 제공한다.</li>
<li><strong>도로 경계:</strong> 차량이 주행 가능한 물리적 한계(Drivable Area)를 정의한다. 이 정보는 이후 계획 단계에서 도로 이탈을 방지하는 강력한 제약 조건으로 작용한다.3</li>
</ul>
<h4>3.2.2  벡터화된 에이전트 모션 학습 (Vectorized Agent Motion Learning)</h4>
<p>동적 객체(차량, 보행자 등)의 정보는 에이전트 쿼리(<span class="math math-inline">Q_{agent}</span>)를 통해 추출된다.</p>
<ul>
<li><strong>Deformable Attention:</strong> 에이전트 쿼리는 BEV 특징 맵과 Deformable Attention을 수행하여 객체의 위치, 클래스, 크기 등의 속성을 디코딩한다.</li>
<li><strong>모션 벡터:</strong> 단순히 현재 위치만 파악하는 것이 아니라, 미래의 이동 경로를 예측한다. VAD는 에이전트의 미래 궤적을 다중 모드(Multi-modal) 모션 벡터로 표현한다.</li>
<li>각 에이전트에 대해 <span class="math math-inline">K</span>개의 가능한 미래 궤적(예: 직진, 좌회전, 감속 등)을 예측하고, 각 모드에 대한 확률 점수를 출력한다.</li>
<li>이 모션 벡터는 자차와의 충돌 가능성을 계산하는 데 사용되며, 래스터화된 점유 그리드보다 훨씬 적은 데이터로 정교한 충돌 예측을 가능하게 한다.2</li>
</ul>
<h3>3.3  상호작용을 통한 계획 (Planning via Interaction)</h3>
<p>기존의 모듈형 방식이나 일부 종단간 모델은 인지 결과를 단순히 ’연결(Concatenation)’하여 계획 모듈에 넘겼다. 반면 VAD는 **‘상호작용(Interaction)’**이라는 보다 능동적인 메커니즘을 도입했다.</p>
<ul>
<li><strong>에고 쿼리 (<span class="math math-inline">Q_{ego}</span>):</strong> 자차(Ego Vehicle)의 주행 의도와 상태를 나타내는 학습 가능한 벡터이다. 초기에는 랜덤하게 초기화된다.</li>
<li><strong>크로스 어텐션 메커니즘 (Cross-Attention Mechanism):</strong> 에고 쿼리는 트랜스포머 디코더 내에서 <strong>Query</strong> 역할을 수행하며, 앞서 학습된 에이전트 쿼리(<span class="math math-inline">Q_{agent}</span>)와 맵 쿼리(<span class="math math-inline">Q_{map}</span>)는 <strong>Key/Value</strong> 역할을 한다.</li>
<li><strong>Ego-Agent Interaction:</strong> 자차는 에이전트 쿼리들을 ’조회’함으로써, 주변 차량 중 어떤 차량이 나의 주행에 위협이 되는지, 어떤 차량을 주의해야 하는지를 학습한다. 어텐션 가중치(Attention Weight)는 자차에게 중요한 차량일수록 높게 배정된다.</li>
<li><strong>Ego-Map Interaction:</strong> 자차는 맵 쿼리들을 ’조회’하여 현재 차선의 곡률, 교차로의 구조, 횡단보도의 위치 등을 파악한다.</li>
<li><strong>주행 명령 통합:</strong> 상호작용을 거친 에고 쿼리는 자차의 현재 상태 정보(속도, 가속도, 각속도) 및 상위 레벨 명령(High-level Command: 직진, 좌회전, 우회전)과 결합된다.</li>
<li><strong>계획 헤드 (Planning Head):</strong> 최종적으로 MLP(Multi-Layer Perceptron) 기반의 계획 헤드가 미래 <span class="math math-inline">T</span>초 동안의 자차 궤적 <span class="math math-inline">\hat{V}_{ego}</span>를 출력한다.2</li>
</ul>
<h3>3.4  벡터화된 계획 제약 조건 (Vectorized Planning Constraints)</h3>
<p>VAD의 가장 큰 특징 중 하나는 계획 단계의 안전성을 확보하기 위해 손실 함수(Loss Function) 레벨에서 기하학적 제약 조건을 명시적으로 설계했다는 점이다. 이는 딥러닝 모델이 데이터 편향으로 인해 물리적으로 불가능하거나 위험한 경로를 생성하는 것을 방지한다.2</p>
<ol>
<li><strong>자차-에이전트 충돌 제약 (Ego-Agent Collision Constraint):</strong></li>
</ol>
<ul>
<li><strong>목적:</strong> 자차와 타 차량 간의 물리적 충돌 방지.</li>
<li><strong>원리:</strong> 예측된 자차의 궤적 점들과 타 에이전트의 예측 궤적 점들 사이의 유클리드 거리를 계산한다. 종방향(Longitudinal) 안전거리 <span class="math math-inline">\delta_x</span>와 횡방향(Lateral) 안전거리 <span class="math math-inline">\delta_y</span>를 설정하고, 이 거리 이내로 접근할 경우 높은 페널티(Loss)를 부여한다.</li>
<li><strong>효과:</strong> 모델은 학습 과정에서 타 차량의 미래 위치를 회피하는 경로를 생성하도록 유도된다.</li>
</ul>
<ol start="2">
<li><strong>자차-경계 이탈 제약 (Ego-Boundary Overstepping Constraint):</strong></li>
</ol>
<ul>
<li><strong>목적:</strong> 도로 이탈 방지.</li>
<li><strong>원리:</strong> 자차의 궤적과 도로 경계(Road Boundary) 벡터 사이의 거리를 계산한다. 궤적이 경계선 밖으로 넘어가는 경우(Overstepping), 이를 강하게 처벌한다.</li>
<li><strong>효과:</strong> 래스터 맵에서 주행 가능 영역 마스크를 사용하는 것보다 훨씬 적은 연산으로 정교한 도로 유지 기능을 구현한다.</li>
</ul>
<ol start="3">
<li><strong>자차-차선 방향성 제약 (Ego-Lane Directional Constraint):</strong></li>
</ol>
<ul>
<li><strong>목적:</strong> 역주행 및 불안정한 조향 방지.</li>
<li><strong>원리:</strong> 자차의 이동 벡터와 가장 가까운 차선 분리대(Lane Divider) 벡터 간의 코사인 유사도(Cosine Similarity)를 계산한다. 두 벡터의 방향이 일치하지 않을 경우(즉, 차선 흐름과 다르게 주행할 경우) 손실을 부여한다.</li>
<li><strong>효과:</strong> 차선을 따라 부드럽게 주행하는 궤적을 생성하며, 급격한 차선 변경이나 역주행을 억제한다.</li>
</ul>
<h2>4.  VADv2: 확률론적 계획으로의 진화와 기술적 도약</h2>
<p>VAD(v1)는 벡터화의 효율성을 입증했지만, 계획 단계에서 <strong>결정론적 회귀(Deterministic Regression)</strong> 방식을 사용한다는 한계가 있었다. 복잡한 도심 환경에서는 동일한 상황에서도 운전자의 성향이나 미묘한 변수에 따라 다양한 행동(Multi-modality)이 가능하다. 결정론적 모델은 이러한 불확실성(Uncertainty)을 평균화해버리는 경향이 있어, 장애물을 만났을 때 왼쪽이나 오른쪽으로 피하는 대신 장애물 정중앙으로 향하는(평균 경로) 치명적인 오류를 범할 수 있다(Non-convex solution space problem). 이를 해결하기 위해 VADv2는 **확률론적 계획(Probabilistic Planning)**을 도입했다.6</p>
<h3>4.1  확률론적 계획 패러다임 (Probabilistic Planning Paradigm)</h3>
<p>VADv2는 궤적 좌표를 직접 예측하는 대신, 가능한 모든 행동의 확률 분포를 학습하고 이를 샘플링하는 접근법을 취한다. 이는 언어 모델(LLM)이 다음에 올 단어를 확률적으로 예측하는 것과 유사하다.</p>
<h4>4.1.1  계획 어휘집 (Planning Vocabulary) 구축</h4>
<p>연속적인 행동 공간(Action Space)을 다루기 위해 VADv2는 이를 이산화(Discretize)한다.</p>
<ul>
<li><strong>대규모 데이터 활용:</strong> nuScenes나 자체 구축한 대규모 주행 데이터셋에서 수백만 개의 전문가 주행 궤적을 수집한다.</li>
<li><strong>가장 먼 궤적 샘플링 (Furthest Trajectory Sampling):</strong> 수집된 궤적들 중 서로 가장 거리가 먼 궤적들을 반복적으로 선택하여 약 4,096개의 대표 궤적(Anchor Trajectories)을 추출한다. 이 집합을 ’계획 어휘집’이라 부른다. 이는 주행 가능한 대부분의 패턴(직진, 좌/우회전, 유턴, 급정거, 차선 변경 등)을 포괄한다.6</li>
</ul>
<h4>4.1.2  확률 분포 학습</h4>
<p>VADv2 모델은 현재의 씬 상황(Condition)이 주어졌을 때, 어휘집에 있는 4,096개의 궤적 중 어느 것이 가장 적절한지에 대한 확률 분포를 출력한다.</p>
<ul>
<li><strong>입력:</strong> 스트리밍 방식의 멀티뷰 영상.</li>
<li><strong>출력:</strong> 행동 공간 <span class="math math-inline">A</span>에 대한 확률 분포 <span class="math math-inline">P(a|x)</span>.</li>
<li><strong>추론:</strong> 가장 높은 확률을 가진 궤적을 선택하거나, 상황에 따라 상위 <span class="math math-inline">k</span>개를 샘플링하여 평가할 수 있다.</li>
</ul>
<h3>4.2  주요 손실 함수 설계 (Loss Function Design)</h3>
<p>VADv2는 확률 분포의 정확한 학습을 위해 세 가지 손실 함수를 결합하여 사용한다.</p>
<ol>
<li><strong>분포 손실 (Distribution Loss):</strong></li>
</ol>
<ul>
<li>학습 데이터의 실제 주행 궤적(Ground Truth)과 모델이 예측한 확률 분포 사이의 차이를 최소화한다. 주로 KL Divergence가 사용된다. 이를 통해 모델은 인간 운전자의 주행 스타일과 불확실성을 모방하게 된다.</li>
</ul>
<ol start="2">
<li><strong>충돌 손실 (Conflict Loss):</strong></li>
</ol>
<ul>
<li>어휘집에 있는 궤적들 중, 현재 씬의 장애물(다른 차량, 보행자)이나 도로 경계와 충돌하는 궤적들을 식별한다. 이러한 ’충돌 궤적’들에 대해서는 확률값을 강제로 낮추도록 페널티를 부여한다. 이는 데이터에 없는 위험 상황(Counterfactual scenarios)에 대해서도 모델이 안전한 판단을 내리도록 돕는 중요한 정규화(Regularization) 기법이다.6</li>
</ul>
<ol start="3">
<li><strong>씬 토큰 손실 (Scene Token Loss):</strong></li>
</ol>
<ul>
<li>VADv1과 마찬가지로 맵과 에이전트를 벡터화하여 인식하는 과정에 대한 손실 함수이다. 인지의 정확도가 계획의 품질을 결정하므로 여전히 중요한 요소이다.</li>
</ul>
<h3>4.3  아키텍처의 변화: 스트리밍 및 토큰화</h3>
<p>VADv2는 입력 데이터를 처리함에 있어 <strong>스트리밍(Streaming)</strong> 방식을 채택했다. 이전 프레임의 정보들을 메모리 큐에 저장해두고 현재 프레임과 결합함으로써 시간적 연속성을 효율적으로 처리한다. 또한, 모든 센서 입력과 계획 출력을 ‘토큰(Token)’ 형태로 통일함으로써, 향후 대규모 언어 모델이나 월드 모델과의 결합을 용이하게 하는 구조적 유연성을 확보했다.6</p>
<h2>5.  성능 평가 및 비교 분석</h2>
<p>VAD와 VADv2의 성능은 자율주행 연구의 표준인 nuScenes 데이터셋(개방형 루프)과 CARLA 시뮬레이터(폐쇄형 루프)에서 광범위하게 검증되었다.</p>
<h3>5.1  nuScenes 개방형 루프(Open-loop) 성능 비교</h3>
<p>개방형 루프 평가는 모델이 주어진 데이터셋의 상황에서 얼마나 정답(인간 운전자)과 유사하게 예측했는지를 측정한다. VAD는 여기서 압도적인 효율성을 보여주었다.</p>
<p>표 1. nuScenes 데이터셋에서의 계획 성능 및 추론 효율성 비교</p>
<p>데이터 출처: 2</p>
<table><thead><tr><th><strong>모델명</strong></th><th><strong>입력 모달리티</strong></th><th><strong>L2 Error (m) ↓</strong></th><th><strong>Collision Rate (%) ↓</strong></th><th><strong>FPS (Inference Speed) ↑</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td>ST-P3</td><td>Camera</td><td>2.11</td><td>1.14</td><td>-</td><td>초기 E2E 모델</td></tr>
<tr><td>UniAD</td><td>Camera</td><td>1.03</td><td>0.31</td><td>1.8</td><td>멀티태스크 SOTA (CVPR’23)</td></tr>
<tr><td><strong>VAD-Base</strong></td><td>Camera</td><td><strong>0.37</strong></td><td><strong>0.14</strong></td><td><strong>4.5</strong></td><td><strong>UniAD 대비 2.5배 고속</strong></td></tr>
<tr><td><strong>VAD-Tiny</strong></td><td>Camera</td><td>0.41</td><td>0.16</td><td><strong>16.8</strong></td><td><strong>UniAD 대비 9.3배 고속</strong></td></tr>
</tbody></table>
<ul>
<li><strong>분석:</strong> VAD-Base는 UniAD 대비 충돌률(Collision Rate)을 절반 이하(0.31% -&gt; 0.14%)로 낮추었다. 이는 벡터화된 충돌 제약 조건이 얼마나 효과적인지를 증명한다. 또한, L2 오차 역시 획기적으로 줄어들어 인간 운전자의 궤적을 매우 정밀하게 모사함을 알 수 있다. 무엇보다 VAD-Tiny 모델은 16.8 FPS라는 실시간성을 확보하여 실제 차량 탑재 가능성을 열었다. UniAD가 1.8 FPS로 실시간 구동이 어려운 것과 대조적이다.</li>
</ul>
<h3>5.2  CARLA 폐쇄형 루프(Closed-loop) 성능 비교</h3>
<p>폐쇄형 루프 평가는 시뮬레이터 내에서 모델이 실제로 차량을 제어하며 목적지까지 주행하는 능력을 측정한다. 오차가 누적되는 상황에서의 안정성을 검증하는 데 필수적이다.</p>
<p>표 2. CARLA Town05 Long 벤치마크 성능 비교</p>
<p>데이터 출처: 6</p>
<table><thead><tr><th><strong>모델명</strong></th><th><strong>Drive Score ↑</strong></th><th><strong>Route Completion (%) ↑</strong></th><th><strong>Infraction Score ↓</strong></th><th><strong>특징</strong></th></tr></thead><tbody>
<tr><td>Transfuser</td><td>54.5</td><td>78.4</td><td>-</td><td>카메라+LiDAR 퓨전</td></tr>
<tr><td>UniAD</td><td>-</td><td>68.68 (avg)</td><td>-</td><td>계산 비용 과다로 제한적 평가</td></tr>
<tr><td>VAD (v1)</td><td>64.3</td><td>87.3</td><td>-</td><td>벡터화 도입으로 성능 향상</td></tr>
<tr><td><strong>VADv2</strong></td><td><strong>85.1</strong></td><td><strong>98.4</strong></td><td><strong>0.87</strong></td><td><strong>확률론적 계획 도입</strong></td></tr>
</tbody></table>
<ul>
<li><strong>분석:</strong> VADv2는 Town05 Long 벤치마크에서 85.1점이라는 기록적인 주행 점수(Drive Score)를 달성했다. 이는 VAD v1(64.3점) 대비 20점 이상 향상된 수치이다. 특히 경로 완료율(Route Completion)이 98.4%에 달해, 거의 모든 시나리오에서 목적지에 도달하는 데 성공했음을 보여준다. 이는 확률론적 계획이 복잡한 교차로, 급격한 끼어들기 등 예측 불가능한 상황(Long-tail scenarios)에서 훨씬 강건함을 입증한다. 규칙 기반의 안전장치(Rule-based Wrapper) 없이 순수 종단간 모델만으로 이러한 안정성을 확보한 것은 매우 고무적인 결과이다.</li>
</ul>
<h3>5.3  VAD vs. UniAD: 아키텍처 철학의 비교</h3>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>UniAD (Unified Autonomous Driving)</strong></th><th><strong>VAD (Vectorized Autonomous Driving)</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 표현 (Representation)</strong></td><td><strong>래스터화 (Rasterized)</strong>: 픽셀/복셀 기반의 덴스 맵, 점유 그리드 사용. 정보가 풍부하나 무거움.</td><td><strong>벡터화 (Vectorized)</strong>: 점 집합/폴리라인 기반의 스파스 벡터 사용. 정보가 압축적이고 가벼움.</td></tr>
<tr><td><strong>계획 방식 (Planning)</strong></td><td><strong>최적화 기반 (Optimization-based)</strong>: 비선형 최적화 솔버를 후처리로 사용하여 궤적 생성. 미분 불가능 요소 존재.</td><td><strong>완전 학습 기반 (Learning-based)</strong>: 벡터 제약 조건을 손실 함수에 포함하여 종단간 학습. 후처리 최소화.</td></tr>
<tr><td><strong>쿼리 메커니즘</strong></td><td>각 태스크(Track, Map, Motion)별로 쿼리가 존재하나 덴스 피처와 상호작용.</td><td>에이전트, 맵, 자차 쿼리 간의 희소한 상호작용(Sparse Interaction)에 집중.</td></tr>
<tr><td><strong>추론 속도</strong></td><td>느림 (약 1.8 ~ 2.1 FPS). 상용 하드웨어 배포 난항.</td><td>매우 빠름 (4.5 ~ 16.8 FPS). 엣지 디바이스 친화적.</td></tr>
<tr><td><strong>주요 철학</strong></td><td>“모든 정보를 통합하여 전체론적으로 이해하자.” (Maximizing Information)</td><td>“주행에 필요한 핵심 정보만 벡터로 추출하자.” (Maximizing Efficiency)</td></tr>
</tbody></table>
<p>UniAD가 ’성능의 상한선’을 탐구했다면, VAD는 ’실용성의 상한선’을 개척했다고 평가할 수 있다.</p>
<h2>6.  VAD의 파급 효과 및 파생 연구 (Derivative Works)</h2>
<p>VAD가 제시한 벡터화 패러다임과 쿼리 기반 상호작용 구조는 이후 수많은 후속 연구에 영감을 주었다.</p>
<h3>6.1  생성형 자율주행 모델 (Generative Autonomous Driving)</h3>
<ul>
<li><strong>DiffusionDrive:</strong> VAD의 벡터 표현을 기반으로 하되, 궤적 생성부에 **확산 모델(Diffusion Model)**을 도입한 연구이다. 확산 모델의 강력한 데이터 분포 모사 능력을 활용하여 VAD보다 더욱 다양하고 현실적인 주행 패턴을 생성한다. VAD 대비 L2 오차를 20% 이상 줄이는 성과를 보였다.11</li>
<li><strong>DrivingGPT:</strong> VADv2의 토큰화 아이디어를 확장하여, 자율주행을 ‘다음 토큰 예측(Next Token Prediction)’ 문제로 정의한 연구이다. 비디오 생성과 행동 생성을 통합하여 자율주행 모델이 물리 법칙과 인과 관계를 이해하도록 학습시킨다.13</li>
</ul>
<h3>6.2  강화학습과의 결합 (Integration with RL)</h3>
<ul>
<li><strong>CoDrive &amp; CoIRL-AD:</strong> 지도 학습(Imitation Learning) 기반인 VAD의 한계를 극복하기 위해 강화학습(Reinforcement Learning)을 결합한 하이브리드 아키텍처들이다. VAD의 인지 백본을 사용하되, 계획 단계에서 RL 에이전트가 개입하여 미지의 환경에 대한 탐험(Exploration) 능력을 부여한다. 이는 VAD가 훈련 데이터에 없는 상황에서 취약할 수 있다는 단점을 보완한다.14</li>
</ul>
<h3>6.3  인지-계획 분리 및 일반화 (AdaptiveAD)</h3>
<ul>
<li><strong>AdaptiveAD:</strong> VAD가 자차 상태(속도 등)에 과도하게 의존하여 씬 정보를 무시하는 ‘인과 혼동(Causal Confusion)’ 문제를 지적하며 등장했다. 이 모델은 VAD 구조를 기반으로 하되, 씬 인식 분기(Branch)와 자차 상태 분기를 명시적으로 분리했다가 융합하는 방식을 제안하여 일반화 성능을 높였다.16</li>
</ul>
<h2>7.  한계점 및 비판적 고찰</h2>
<p>VAD 시리즈가 거둔 성취에도 불구하고, 여전히 해결해야 할 과제들이 존재한다.</p>
<h3>7.1  자차 상태 의존성 (Ego-Status Dependency)</h3>
<p>연구자들은 VAD와 같은 종단간 모델들이 카메라 영상의 복잡한 시각 정보를 해석하기보다, 단순히 자차의 현재 속도나 관성 정보(Ego Status)에 의존하여 미래 궤적을 생성하는 경향(Shortcut Learning)이 있음을 발견했다. 예를 들어, 차가 정지해 있으면 신호등이 파란불로 바뀌어도 영상 정보보다는 ’정지 상태’라는 관성 정보에 가중치를 두어 출발하지 않을 수 있다. AdaptiveAD와 같은 연구가 이를 완화하려 노력하고 있으나, 근본적인 해결책은 여전히 연구 대상이다.5</p>
<h3>7.2  확률론적 어휘집의 정밀도 (Vocabulary Granularity in VADv2)</h3>
<p>VADv2의 계획 어휘집(4,096개)은 대부분의 주행 상황을 커버하지만, 이산화(Discretization) 과정에서 필연적으로 정밀도가 손실된다. 매우 미세한 조향이 필요한 상황이나, 어휘집에 없는 특수한 궤적(예: 사고 회피를 위한 비정상적 기동)이 요구될 때 모델의 대응력이 떨어질 수 있다. 또한, 샘플링 기반 방식은 추론 때마다 결과가 달라질 수 있어(Non-deterministic), 안전성 검증(Verification)이 까다롭다는 산업계의 우려가 있다.17</p>
<h3>7.3  시뮬레이션과 현실의 간극 (Sim-to-Real Gap)</h3>
<p>VAD와 VADv2는 주로 nuScenes 데이터셋과 CARLA 시뮬레이터에서 검증되었다. 그러나 실제 도로 환경은 시뮬레이터보다 훨씬 복잡하고 변수가 많다. 특히 맵 정보를 완벽하게 학습할 수 없는 환경(맵이 없거나 낡은 지역)에서의 강건성은 추가적인 검증이 필요하다. Mapless Driving을 지향하지만, 여전히 학습 단계에서는 정답 맵 데이터에 의존한다는 점도 한계로 지적된다.18</p>
<h2>8.  결론: 자율주행의 미래를 향한 벡터화의 이정표</h2>
<p>VAD(Vectorized Autonomous Driving)는 자율주행 기술의 발전 과정에서 중요한 변곡점을 마련한 연구이다. VAD는 방대하고 비효율적인 래스터 데이터를 처리하느라 실시간성을 확보하지 못했던 기존 종단간 모델들의 문제를 **‘벡터화(Vectorization)’**와 **‘스파스 쿼리(Sparse Query)’**라는 우아한 해법으로 해결했다. 이는 자율주행 시스템이 더 이상 무거운 연산 장비에 의존하지 않고도, 고도의 인지 및 계획 기능을 수행할 수 있는 가능성을 열어주었다.</p>
<p>나아가 VADv2는 결정론적 세계관에 머물러 있던 자율주행 계획 알고리즘을 <strong>‘확률론적(Probabilistic)’</strong> 영역으로 확장시켰다. 이는 자율주행 AI가 인간처럼 불확실성을 인지하고, 상황에 따른 다양한 가능성을 고려하여 가장 안전한 행동을 선택할 수 있는 지능형 에이전트로 진화하고 있음을 시사한다.</p>
<p>VAD가 제시한 ’효율적인 벡터 표현’과 ’확률적 의사결정’은 향후 대규모 언어 모델(LLM), 월드 모델(World Model), 그리고 생성형 AI(Generative AI)와 결합되어 자율주행 기술을 레벨 4/5 수준의 완전 자율주행으로 이끄는 핵심 기반 기술이 될 것으로 전망된다. VAD는 단순한 하나의 모델을 넘어, 효율적이고 안전한 자율주행을 위한 새로운 설계 철학을 정립했다는 점에서 그 학술적, 산업적 가치가 매우 크다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>A Survey on Vision-Language-Action Models for Autonomous Driving, https://arxiv.org/html/2506.24044v1</li>
<li>(PDF) VAD: Vectorized Scene Representation for Efficient …, https://www.researchgate.net/publication/369414179_VAD_Vectorized_Scene_Representation_for_Efficient_Autonomous_Driving</li>
<li>Vectorized Scene Representation for Efficient Autonomous Driving, https://ieeexplore.ieee.org/iel7/10376473/10376477/10377337.pdf</li>
<li>End-to-End Autonomous Driving without Costly Modularization and …, https://arxiv.org/html/2406.17680v1</li>
<li>Vectorized Scene Representation for Efficient Autonomous Driving, https://liner.com/review/vad-vectorized-scene-representation-for-efficient-autonomous-driving</li>
<li>VADv2: End-to-End Vectorized Autonomous Driving via Probabilistic …, https://arxiv.org/html/2402.13243v1</li>
<li>Vectorized Scene Representation for Efficient Autonomous Driving, https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_VAD_Vectorized_Scene_Representation_for_Efficient_Autonomous_Driving_ICCV_2023_paper.pdf</li>
<li>End-to-End Vectorized Autonomous Driving via Probabilistic Planning, https://arxiv.org/abs/2402.13243</li>
<li>Rethinking the Open-Loop Evaluation of End-to-End Autonomous …, https://www.alphaxiv.org/overview/2305.10430v2</li>
<li>arXiv:2402.13243v1 [cs.CV] 20 Feb 2024, https://www.i-newcar.com/uploads/ueditor/20250120/2-25012013245T13.pdf</li>
<li>Truncated Diffusion Model for End-to-End Autonomous Driving - arXiv, https://arxiv.org/html/2411.15139v3</li>
<li>Truncated Diffusion Model for End-to-End Autonomous Driving, https://www.researchgate.net/publication/394619633_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving</li>
<li>Unifying Driving World Modeling and Planning with Multi-modal …, https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_DrivingGPT_Unifying_Driving_World_Modeling_and_Planning_with_Multi-modal_Autoregressive_ICCV_2025_paper.pdf</li>
<li>LEARNING TO DRIVE WITH TWO MINDS:ACOMPET - OpenReview, https://openreview.net/pdf/04afe579e624691866faa19a9faf754ce2f455b3.pdf</li>
<li>CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement …, https://www.researchgate.net/publication/396499958_CoIRL-AD_Collaborative-Competitive_Imitation-Reinforcement_Learning_in_Latent_World_Models_for_Autonomous_Driving</li>
<li>Decoupling Scene Perception and Ego Status: A Multi-Context …, https://arxiv.org/html/2511.13079v2</li>
<li>VADv2: End-to-End Autonomous Driving via Probabilistic Planning, https://openreview.net/forum?id=0a4dA6eUHN</li>
<li>CoIRL-AD: Collaborative–Competitive Imitation–Reinforcement …, https://arxiv.org/html/2510.12560v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>