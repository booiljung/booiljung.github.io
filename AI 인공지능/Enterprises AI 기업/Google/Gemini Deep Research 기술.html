<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:제미나이 딥리서치 (Gemini Deep Research) 기술</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>제미나이 딥리서치 (Gemini Deep Research) 기술</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">인공지능 기업 (AI Enterprises)</a> / <a href="index.html">Google의 인공지능</a> / <span>제미나이 딥리서치 (Gemini Deep Research) 기술</span></nav>
                </div>
            </header>
            <article>
                <h1>제미나이 딥리서치 (Gemini Deep Research) 기술</h1>
<h2>1.  제미나이 모델 패밀리의 설계 철학</h2>
<p>구글이 개발한 제미나이(Gemini)는 단순히 기존 대규모 언어 모델(Large Language Model, LLM)의 성능을 점진적으로 개선하는 것을 넘어, 인공지능(AI) 패러다임의 근본적인 전환을 목표로 설계된 모델 패밀리다.1 이전 세대 모델인 PaLM 2와 비교할 때, 제미나이의 가장 핵심적인 설계 철학은 ’네이티브 멀티모달리티(Native Multimodality)’에 있다. 이는 텍스트, 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 별개의 모델로 처리하여 결합하는 기존의 방식에서 벗어나, 태생부터 단일 모델이 모든 데이터 양식을 통합적으로 이해하고 추론하도록 설계되었음을 의미한다.3 이러한 접근은 보다 인간과 유사한 방식으로 정보를 처리하여 범용 인공지능(Artificial General Intelligence, AGI)을 향한 구글의 장기적인 비전을 구체화하는 중요한 단계로 평가된다.</p>
<p>제미나이 패밀리는 각기 다른 컴퓨팅 환경과 응용 분야를 정밀하게 겨냥한 전략적 라인업으로 구성된다.1 첫째, ’제미나이 Ultra’는 가장 거대하고 유능한 모델로서, 고도로 복잡한 추론과 멀티모달 작업을 수행하며 최첨단(State-of-the-Art, SOTA) 성능의 한계를 돌파하는 역할을 담당한다.2 둘째, ’제미나이 Pro’는 성능과 비용, 확장성 사이의 최적의 균형을 제공하여 광범위한 클라우드 기반 AI 서비스와 애플리케이션에 적용되도록 설계되었다.2 마지막으로, ’제미나이 Nano’는 모바일 기기와 같은 온디바이스(on-device) 환경에서 효율적으로 작동하도록 최적화된 경량 모델이다.2 Nano 모델은 18억 개의 파라미터를 가진 Nano-1과 32.5억 개의 파라미터를 가진 Nano-2 버전으로 세분화되며, 대규모 제미나이 모델의 지식을 소형 모델로 이전하는 ‘증류(distilling)’ 기법을 통해 훈련되어 크기 대비 뛰어난 성능을 보인다.6</p>
<p>이러한 모델 라인업은 단순한 크기 조절(scaling)을 넘어, AI 기술의 ’심화’와 ’보급’이라는 이중 전략을 명확히 보여준다. 제미나이 Ultra가 MMLU 벤치마크에서 인간 전문가를 능가하는 등 기술적 한계를 끊임없이 확장하는 ‘심화’ 전략을 대표한다면 5, Pro와 Nano는 각각 클라우드와 엣지 디바이스로 AI의 적용 범위를 넓혀 기술의 접근성을 극대화하는 ‘보급’ 전략을 수행한다. 특히 Nano 모델 훈련에 사용된 증류 기법은, 최상위 모델이 확보한 고도의 지능을 자원이 제한된 환경에서도 활용할 수 있도록 하는 효율적인 기술 전파 메커니즘으로서 기능한다. 이는 구글 딥마인드(Google DeepMind)와 구글 브레인(Google Brain)의 핵심 연구 역량을 결집하여 탄생한 제미나이 프로젝트가 8, 단순히 학술적 성과에 머무르지 않고 실제 세계에 광범위한 영향을 미치려는 전략적 의도를 내포하고 있음을 시사한다.</p>
<h2>2.  제미나이의 근간: 트랜스포머 아키텍처 심층 분석</h2>
<p>현존하는 거의 모든 고성능 대규모 언어 모델과 마찬가지로, 제미나이의 근간 역시 2017년에 발표된 “Attention Is All You Need” 논문에서 제안된 트랜스포머(Transformer) 아키텍처에 있다.9 트랜스포머는 순환 신경망(Recurrent Neural Network, RNN)이나 컨볼루션 신경망(Convolutional Neural Network, CNN)과 같은 순차적 처리 방식을 완전히 배제하고, ‘어텐션(attention)’ 메커니즘만을 사용하여 입력 시퀀스 내 모든 토큰 간의 관계를 병렬적으로 계산하는 혁신적인 구조를 제시했다.10 제미나이는 이 중에서도 주로 텍스트 생성 과업에 강점을 보이는 ‘디코더-온리(Decoder-only)’ 구조를 기반으로 한다. 이 구조는 이전 타임스텝까지 생성된 토큰들을 바탕으로 다음 토큰을 예측하는 자기회귀적(auto-regressive) 방식으로 작동하며, 자연스러운 문장 생성에 최적화되어 있다.</p>
<h3>2.1  스케일드 닷-프로덕트 어텐션(Scaled Dot-Product Attention)의 수학적 원리</h3>
<p>트랜스포머의 핵심이자 제미나이의 모든 연산의 기초가 되는 것은 ’스케일드 닷-프로덕트 어텐션’이다. 이 메커니즘은 특정 토큰을 처리할 때 입력 시퀀스 내의 다른 모든 토큰들이 얼마나 중요한지를 가중치로 계산하여, 문맥적으로 풍부한 표현을 생성하는 역할을 한다. 이 과정은 세 가지 주요 벡터, 즉 쿼리(Query, Q), 키(Key, K), 밸류(Value, V)를 통해 이루어진다.11</p>
<ol>
<li>
<p><strong>쿼리, 키, 밸류 벡터 생성</strong>: 입력 시퀀스의 각 토큰 임베딩 벡터는 세 개의 서로 다른 학습 가능한 가중치 행렬(weight matrix)과 곱해져 각각 쿼리, 키, 밸류 벡터로 변환된다. 쿼리는 현재 처리 중인 토큰이 ’찾고 있는 정보’를, 키는 각 토큰이 ’제공하는 정보의 종류’를, 밸류는 각 토큰이 ’실제로 담고 있는 정보’를 나타낸다고 개념적으로 이해할 수 있다.</p>
</li>
<li>
<p><strong>어텐션 스코어 계산</strong>: 현재 토큰의 쿼리 벡터와 시퀀스 내 모든 토큰의 키 벡터 간의 내적(dot product)을 계산한다. 이 내적 값은 두 벡터 간의 유사도를 나타내며, 이를 ’어텐션 스코어(attention score)’라고 한다. 쿼리와 특정 키의 유사도가 높을수록 해당 키에 대응하는 토큰이 현재 토큰을 이해하는 데 더 중요하다는 의미다.13</p>
</li>
<li>
<p><strong>스케일링</strong>: 계산된 어텐션 스코어를 키 벡터의 차원 <span class="math math-inline">d_k</span>의 제곱근(<span class="math math-inline">\sqrt{d_k}</span>)으로 나누어준다. 이 스케일링 과정은 <span class="math math-inline">d_k</span> 값이 클 경우 내적 값이 지나치게 커져 소프트맥스 함수의 그래디언트가 0에 가까워지는 문제를 방지하고, 학습 과정을 안정화시키는 데 결정적인 역할을 한다.11</p>
</li>
<li>
<p><strong>소프트맥스 함수 적용</strong>: 스케일링된 어텐션 스코어에 소프트맥스 함수를 적용하여 모든 가중치의 합이 1이 되도록 정규화한다. 이렇게 얻어진 값을 ’어텐션 가중치(attention weight)’라 하며, 각 토큰의 중요도를 나타내는 확률 분포로 해석할 수 있다.13</p>
</li>
<li>
<p><strong>가중합 계산</strong>: 마지막으로, 계산된 어텐션 가중치를 각 토큰의 밸류 벡터에 곱한 뒤 모두 더하여 가중합(weighted sum)을 구한다. 이 결과물은 시퀀스 내 모든 토큰의 정보를 문맥적 중요도에 따라 가중하여 종합한 새로운 표현 벡터가 된다.</p>
</li>
</ol>
<p>이 전체 과정은 다음의 단일 수식으로 압축하여 표현할 수 있다.</p>
<p><span class="math math-display">
Attention(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
</span><br />
이 메커니즘 덕분에 모델은 시퀀스 내 토큰 간의 거리에 구애받지 않고 직접적인 의존성을 모델링할 수 있으며 10, 이는 장거리 의존성 포착에 취약했던 기존 RNN의 한계를 극복하는 핵심 요인이 되었다.</p>
<h3>2.2  멀티-헤드 어텐션(Multi-Head Attention)의 다각적 정보 처리 원리</h3>
<p>단일 어텐션 메커니즘은 시퀀스의 특정 관계성에만 집중하는 경향이 있을 수 있다. 이러한 한계를 극복하고 보다 풍부하고 다각적인 정보를 포착하기 위해 트랜스포머는 ’멀티-헤드 어텐션’을 도입했다.11 이는 단일 어텐션을 병렬적으로 여러 번 수행하는 것과 같다.</p>
<p>멀티-헤드 어텐션의 작동 원리는 다음과 같다. 먼저, 쿼리, 키, 밸류 벡터를 <span class="math math-inline">h</span>개의 ’헤드(head)’로 나눈다. 이는 <span class="math math-inline">d_{model}</span> 차원의 벡터를 <span class="math math-inline">h</span>개의 <span class="math math-inline">d_k = d_{model}/h</span> 차원 벡터로 분할하는 것을 의미한다. 각 헤드는 독립적인 가중치 행렬을 사용하여 입력 벡터를 자신만의 쿼리, 키, 밸류 표현으로 선형 투영(linear projection)한다.11 그 후, 각 헤드는 독립적으로 스케일드 닷-프로덕트 어텐션을 병렬 수행한다.</p>
<p>이러한 구조는 모델이 입력 시퀀스의 다양한 측면을 동시에 학습하도록 유도한다. 예를 들어, 어떤 헤드는 문장 내의 구문적 관계(예: 주어-동사 일치)에 집중하고, 다른 헤드는 의미적 관계(예: 동의어)에, 또 다른 헤드는 장거리 참조 관계에 집중하여 학습할 수 있다.15 각 헤드가 서로 다른 표현 하위 공간(representation subspace)에서 정보를 처리함으로써, 모델은 단일한 관점에 매몰되지 않고 다각적인 관점에서 문맥을 이해하게 된다. 이는 마치 전문가 팀이 각자의 전문 분야에서 문제를 분석한 뒤 결과를 종합하는 것과 유사하다.</p>
<p>각 헤드에서 계산된 어텐션 출력 벡터들은 다시 하나로 결합(concatenate)된 후, 최종적으로 또 다른 학습 가능한 가중치 행렬 <span class="math math-inline">W^O</span>와 곱해져 최종 출력 벡터를 생성한다.16 이 과정은 각 헤드가 포착한 다양한 정보들을 통합하여 최종적인 문맥 표현을 만드는 역할을 한다.</p>
<p>멀티-헤드 어텐션의 전체 과정은 다음 수식으로 표현된다.</p>
<p><span class="math math-display">
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h)W^O
</span><br />
여기서 각 헤드는 다음과 같이 정의된다.</p>
<p><span class="math math-display">
\text{where head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
</span><br />
이처럼 멀티-헤드 어텐션은 단순한 병렬 처리를 넘어, 모델 내부에 ’관점의 다각화’를 구조적으로 구현한 메커니즘이다. 이는 모델의 표현력을 극대화하고, 복잡하고 미묘한 문맥을 이해하는 능력의 핵심적인 기반이 되며, 제미나이와 같은 고성능 모델의 강력한 성능을 뒷받침한다.</p>
<h2>3.  네이티브 멀티모달리티의 구현 원리</h2>
<p>제미나이를 이전 세대 AI 모델과 구분 짓는 가장 혁신적인 특징은 ’네이티브 멀티모달리티’다.2 이는 텍스트, 이미지, 오디오 등 서로 다른 데이터 양식을 처리하기 위해 각각 별도의 인코더나 모델을 훈련시킨 뒤, 그 결과물을 후반 단계에서 결합하는 전통적인 파이프라인 방식과 근본적으로 다르다. 제미나이는 설계 초기부터 단일 트랜스포머 모델이 다양한 유형의 데이터를 하나의 통합된 시퀀스로서 동시에 처리하도록 훈련되었다.4</p>
<p>이러한 통합적 접근의 핵심은 모든 모달리티를 공통된 ’임베딩 공간(embedding space)’으로 투영하는 데 있다. 텍스트는 단어나 서브워드 단위로 토큰화되고, 비디오는 일련의 이미지 프레임으로 분해되며, 오디오는 음성의 미묘한 특징을 보존하는 방식으로 인코딩된다.7 특히 오디오 처리 방식은 주목할 만하다. 제미나이는 오디오를 텍스트로 변환하는 중간 단계를 거치지 않고, 구글의 Universal Speech Model (USM)을 통해 추출된 16kHz 오디오 신호 특징을 직접 입력으로 받는다.5 이 방식은 텍스트 변환 과정에서 필연적으로 손실되는 운율, 톤, 감정과 같은 비언어적 정보를 모델이 직접 학습할 수 있게 하여, 보다 깊이 있는 오디오 이해를 가능하게 한다.</p>
<p>이렇게 각기 다른 모달리티에서 추출된 토큰들은 동일한 벡터 공간상의 표현으로 변환된 후, 하나의 ’인터리브드 시퀀스(interleaved sequence)’로 결합된다.7 예를 들어, 사용자의 프롬프트는 “ 이 사진에 있는 강아지는 [IMAGE] 어떤 품종인가요? (강아지 짖는 소리)“와 같이 텍스트, 이미지, 오디오 토큰이 자유롭게 혼합된 형태로 구성될 수 있다.8</p>
<p>이 통합된 시퀀스는 제미나이의 트랜스포머 디코더에 입력되어, 모델의 모든 계층에서 ’교차 모달 어텐션(cross-modal attention)’을 통해 처리된다. 멀티-헤드 어텐션 메커니즘은 텍스트 토큰이 이미지 토큰의 특정 영역에 집중하거나, 이미지 토큰이 오디오 토큰의 특징과 연관성을 찾는 등 모달리티의 경계를 넘어 자유롭게 상호작용할 수 있게 한다. 이러한 심층적인 통합은 제미나이의 시각적 인코딩이 Flamingo, CoCa, PaLI와 같은 이전 연구에서 영감을 받았음에도 불구하고, “처음부터 멀티모달“이라는 근본적인 차별점을 만들어낸다.5</p>
<p>결론적으로, 제미나이의 ‘네이티브’ 멀티모달리티는 단순히 여러 데이터 유형을 처리하는 능력을 넘어선다. 이는 서로 다른 모달리티 간의 ’개념적 연결(Conceptual Bridging)’을 모델의 근본적인 가중치 수준에서 직접 학습하는 패러다임을 의미한다. 예를 들어, ’슬픈 톤의 목소리’라는 오디오 신호의 특징 벡터와 ’비극’이라는 텍스트 토큰의 임베딩 벡터는 모델의 내부 표현 공간에서 서로 가까운 위치에 맵핑될 수 있다. 이러한 방식은 텍스트, 이미지, 소리가 분리된 정보가 아니라 하나의 통합된 현실을 구성하는 여러 측면임을 모델이 내재적으로 이해하게 만든다. 이는 추상적이고 복잡한 멀티모달 추론 능력, 예를 들어 물리 문제의 손글씨 풀이 과정을 이미지로 보고 어느 단계에서 계산이 틀렸는지 텍스트로 설명하는 능력의 핵심 기반이 된다.6 이러한 근본적인 아키텍처의 우위는 파이프라인 방식의 경쟁 모델들이 쉽게 따라잡기 어려운 깊이 있는 이해와 추론 능력을 제미나이에 부여한다.</p>
<h2>4.  아키텍처 혁신 1: Mixture-of-Experts (MoE)와 효율적 확장</h2>
<p>제미나이 1.5 Pro 모델부터 도입된 핵심적인 아키텍처 혁신은 ’희소 전문가 혼합(Sparsely-Gated Mixture-of-Experts, MoE)’이다.18 MoE는 모델의 전체 파라미터 수를 수천억 개 이상으로 대폭 확장하면서도, 추론 시에는 입력된 각 토큰에 대해 가장 관련성이 높은 일부 ‘전문가(expert)’ 네트워크만을 선택적으로 활성화하여 계산 비용을 효율적으로 관리하는 ‘조건부 연산(conditional computation)’ 기법이다.20 이는 기존의 모든 파라미터가 모든 토큰 계산에 참여하는 ‘밀집(dense)’ 모델과 대조되는 방식이다.</p>
<p>MoE 아키텍처는 전통적인 트랜스포머의 순방향 신경망(Feed-Forward Network, FFN) 레이어를 여러 개의 작은 FFN, 즉 ’전문가’들과 이들을 제어하는 ’게이팅 네트워크(gating network)’로 대체한다.22 각 전문가는 특정 종류의 데이터 패턴이나 지식 영역을 처리하는 데 특화되도록 학습된다.24</p>
<h3>4.1  게이팅 네트워크와 토큰 라우팅</h3>
<p>’게이팅 네트워크’는 MoE 아키텍처의 핵심적인 라우터(router) 역할을 수행한다.25 이 작은 신경망은 입력 토큰 <span class="math math-inline">x</span>를 받아, 사용 가능한 <span class="math math-inline">n</span>개의 전문가 각각에 대한 선호도 점수(affinity score)를 계산한다. 이 점수들을 소프트맥스 함수에 통과시켜 각 전문가가 해당 토큰을 처리할 가중치, 즉 게이트 값 <span class="math math-inline">G(x)_i</span>를 산출한다.20</p>
<p>실제로는 계산 효율성을 극대화하기 위해 ‘Top-K’ 라우팅 전략이 주로 사용된다.23 이는 게이팅 네트워크가 산출한 가중치가 가장 높은 상위 <span class="math math-inline">k</span>개(보통 1개 또는 2개)의 전문가만을 선택하여 활성화하고, 나머지 전문가들은 계산에서 제외하는 방식이다. 선택된 전문가들은 입력 토큰을 처리하여 각자의 출력 결과를 내놓고, 이 결과들은 다시 게이팅 네트워크가 부여한 가중치에 따라 가중합되어 최종 출력을 형성한다. MoE 레이어의 전체 출력 <span class="math math-inline">y</span>는 다음과 같이 수식으로 표현할 수 있다.21</p>
<p><span class="math math-display">
y = \sum_{i=1}^{n} G(x)_i E_i(x)
</span><br />
여기서 <span class="math math-inline">E_i(x)</span>는 <span class="math math-inline">i</span>번째 전문가의 출력을 의미하며, Top-K 라우팅에서는 선택되지 않은 전문가들의 <span class="math math-inline">G(x)_i</span> 값이 0이 되므로 해당 전문가의 연산 <span class="math math-inline">E_i(x)</span>는 수행되지 않는다. 이 ‘희소 활성화(sparse activation)’ 덕분에 모델은 방대한 수의 파라미터를 보유하면서도, 실제 추론에 필요한 연산량은 훨씬 작은 규모의 밀집 모델과 유사한 수준으로 유지할 수 있다.20</p>
<h3>4.2  부하 분산을 위한 보조 손실 함수</h3>
<p>MoE 모델을 훈련할 때 발생할 수 있는 주요 문제 중 하나는 게이팅 네트워크가 소수의 인기 있는 전문가에게만 대부분의 토큰을 보내는 ‘표현 붕괴(representational collapse)’ 현상이다. 이는 특정 전문가들에게 과부하를 유발하고 나머지 전문가들은 거의 활용되지 않아 모델 전체의 용량을 비효율적으로 사용하는 결과를 낳는다. 이러한 불균형을 해결하기 위해, MoE 모델 훈련 시에는 주 손실 함수(main loss function) 외에 추가적인 ’보조 손실 함수(auxiliary loss function)’가 도입된다.20</p>
<p>이 보조 손실은 모든 전문가가 훈련 데이터 배치 내에서 균등하게 활용되도록 유도하는 역할을 한다. 일반적으로 이는 각 전문가에게 할당된 토큰의 수, 즉 ’부하(load)’의 분산을 최소화하는 방향으로 설계된다. 예를 들어, 한 배치 내에서 각 전문가가 처리하는 토큰 수의 변동 계수(coefficient of variation)를 계산하여, 이 값이 커질수록 페널티를 부여하는 손실 항을 추가할 수 있다. <code>L_importance</code>와 <code>L_load</code> 손실 함수는 이러한 목적을 위해 설계되었으며, 각각 전문가의 중요도와 부하를 균형 있게 맞추는 역할을 한다.20</p>
<ul>
<li>
<p><strong>전문가 중요도 균형 손실 (<span class="math math-inline">L_{\text{importance}}</span>)</strong>:</p>
<p><span class="math math-display">
L_{\text{importance}}(X) = w_{\text{importance}} \cdot \text{CV}(\text{Importance}(X))^2
</span></p>
</li>
<li>
<p><strong>전문가 부하 균형 손실 (<span class="math math-inline">L_{\text{load}}</span>)</strong>:</p>
<p><span class="math math-display">
L_{\text{load}}(X) = w_{\text{load}} \cdot \text{CV}(\text{Load}(X))^2
</span></p>
</li>
</ul>
<p>이러한 보조 손실 덕분에 게이팅 네트워크는 토큰을 여러 전문가에게 고르게 분산시키는 방법을 학습하게 되며, 이는 모델 전체의 학습 안정성을 높이고 성능을 향상시키는 데 기여한다.</p>
<p>MoE 아키텍처의 도입은 대규모 언어 모델 개발의 패러다임에 중요한 전환점을 제시한다. 이는 모델의 ’총 파라미터 수’와 추론 시 ’활성 파라미터 수’의 개념을 효과적으로 분리했기 때문이다. 모델의 지식과 능력을 결정하는 총 파라미터 수는 계속해서 확장하면서도, 실제 연산 비용과 직결되는 활성 파라미터 수는 상대적으로 작게 유지할 수 있게 된 것이다. 이 개념적 분리를 통해 제미나이 1.5 Pro와 같은 모델은 ’규모의 경제’를 통한 성능 향상과 ’운영 효율성’이라는 두 가지 목표를 동시에 달성하는 새로운 스케일링 법칙을 현실화했다. 이는 더 강력한 AI를 더 경제적으로 제공할 수 있는 길을 열어준 핵심적인 기술적 돌파구다.</p>
<h2>5.  아키텍처 혁신 2: 초장문 컨텍스트(Long-Context) 처리 능력의 기술적 기반</h2>
<p>제미나이 1.5 Pro가 AI 커뮤니티에 던진 가장 큰 충격 중 하나는 100만 토큰에 달하는 전례 없는 컨텍스트 창(context window) 크기였다.26 이는 표준 128,000 토큰에서 크게 확장된 것이며, 연구 단계에서는 최대 1000만 토큰까지 성공적으로 테스트되었다.28 100만 토큰은 약 70만 단어, 11시간 분량의 오디오, 또는 3만 줄 이상의 코드베이스에 해당하는 방대한 양의 정보로, 모델이 단일 프롬프트 내에서 전체 책 한 권이나 장시간의 회의록을 한 번에 처리할 수 있음을 의미한다.28</p>
<p>이러한 초장문 컨텍스트 처리 능력은 단순히 메모리를 늘리는 것만으로는 달성할 수 없는 복합적인 기술적 성취다. 그 기반에는 앞서 설명한 MoE 아키텍처의 효율성과 함께, 장거리 의존성을 효과적으로 처리하기 위해 개선된 어텐션 메커니즘 및 모델 아키텍처 전반의 최적화가 자리 잡고 있다.18 MoE 구조는 활성 파라미터 수를 제어함으로써 긴 시퀀스를 처리할 때 발생하는 막대한 계산 부하를 감당할 수 있게 하는 핵심적인 역할을 한다.</p>
<p>모델이 실제로 이 방대한 컨텍스트를 얼마나 효과적으로 활용하는지를 검증하기 위해, ’건초더미 속 바늘 찾기(Needle-in-a-Haystack, NIAH)’라는 엄격한 평가가 수행되었다.28 이 평가는 긴 문서나 데이터(건초더미) 속에 특정 정보(바늘)를 의도적으로 숨겨두고, 모델이 이 정보를 정확히 찾아낼 수 있는지를 테스트한다. 제미나이 1.5 Pro는 텍스트, 비디오, 오디오 등 모든 모달리티에서 최대 100만 토큰 길이의 ‘건초더미’ 속에서 99% 이상의 거의 완벽한 ‘바늘’ 회수율을 기록했다.28 이는 모델이 컨텍스트 창의 길이에 상관없이 거의 모든 위치의 정보를 손실 없이 정확하게 기억하고 접근할 수 있음을 실증적으로 보여주는 결과다.</p>
<p>100만 토큰 컨텍스트 창의 등장은 단순히 양적인 확장을 넘어, 대규모 언어 모델의 활용 방식에 질적인 변화를 가져온다. 기존의 수천 토큰 수준의 컨텍스트 창에서는 몇 가지 예시(few-shot)를 제공하여 모델의 행동을 유도하는 ’인-컨텍스트 학습(In-Context Learning)’이 주를 이루었다. 그러나 100만 토큰의 컨텍스트 창은, 모델을 특정 작업에 맞게 재훈련하는 ’미세조정(fine-tuning)’을 상당 부분 대체할 수 있는 새로운 가능성을 연다. 예를 들어, 특정 법률 분야에 특화된 AI를 만들기 위해 수 주에 걸쳐 모델을 미세조정하는 대신, 관련 법전 전체와 판례 수백 건을 프롬프트에 직접 입력하여 모델이 ‘즉석에서’ 법률 전문가처럼 추론하게 만들 수 있다. 이는 마치 모델을 특정 작업에 맞게 ’실시간으로 프로그래밍’하는 것과 같은 효과를 낳는다. 이러한 패러다임의 전환은 AI의 개인화와 전문화를 훨씬 더 빠르고 비용 효율적으로 만들어, 다양한 산업 분야에서 새로운 응용 사례를 창출할 잠재력을 지닌다.</p>
<h2>6.  제미나이 2.5: ‘사고(Thinking)’ 능력의 구현</h2>
<p>제미나이 2.5 시리즈는 ‘사고(Thinking)’ 또는 ’향상된 추론(Enhanced Reasoning)’이라는 새로운 개념을 도입하며 모델의 능력을 한 단계 더 끌어올렸다.32 이는 단순히 입력된 문맥을 기반으로 통계적으로 가장 확률 높은 다음 토큰을 예측하는 기존의 자기회귀적 방식을 넘어, 최종 응답을 생성하기 전에 내부적으로 문제 해결을 위한 명시적인 추론 단계를 거치는 것을 의미한다.33</p>
<p>이 ‘사고’ 능력은 강화학습(Reinforcement Learning)과 ’병렬 사고(parallel thinking)’와 같은 최첨단 후처리 기법들의 결합을 통해 구현된다.32 모델은 복잡한 질문을 받았을 때, 곧바로 답을 내놓는 대신 여러 가능한 해결 경로를 동시에 탐색하고(병렬 사고), 각 경로의 타당성을 평가하며, 최적의 해결책을 구성하기 위해 단계적으로 추론을 진행한다. 이러한 과정은 특히 다단계의 논리적 전개가 필수적인 코딩, 수학, 과학 문제 해결에서 두드러진 성능 향상을 가져왔다. 예를 들어, 제미나이 2.5 Pro는 실제 소프트웨어 엔지니어링 문제를 해결하는 능력을 측정하는 SWE-Bench Verified 벤치마크에서 63.8%라는 높은 점수를 기록하며, 복잡한 코드베이스를 이해하고 수정하는 에이전트로서의 잠재력을 입증했다.33</p>
<p>특히 주목할 만한 기능은 ‘딥 씽크(Deep Think)’ 모드다.32 이는 창의적인 아이디어를 구체화하거나, 전략적 계획을 수립하거나, 기존의 해결책을 점진적으로 개선하는 등 정해진 답이 없는 개방형 문제에 특화된 기능이다. 딥 씽크는 모델이 단순히 정답을 찾는 것을 넘어, 대안을 평가하고, 트레이드오프를 고려하며, 목표를 달성하기 위한 최적의 경로를 설계하는 고차원적인 인지 활동을 모방한다.</p>
<p>‘사고’ 모델의 등장은 대규모 언어 모델의 발전이 중요한 변곡점을 맞이했음을 시사한다. 이는 인지과학에서 말하는 ‘시스템 1’(빠르고 직관적인 사고)과 ‘시스템 2’(느리고 숙고적인 사고)의 개념과 맞닿아 있다. 기존 LLM의 다음 토큰 예측은 시스템 1에 가깝다면, 제미나이 2.5의 ‘사고’ 과정은 시스템 2를 AI 모델에 구현하려는 시도로 볼 수 있다. 이는 모델이 단일 순방향 패스(single forward pass) 연산의 한계를 스스로 인식하고, 문제의 난이도에 따라 내부적으로 탐색, 계획, 검증과 같은 추가적인 계산 자원을 동적으로 할당하기 시작했음을 의미한다. 이러한 변화는 AI가 단순한 패턴 인식 및 생성 도구를 넘어, 진정한 문제 해결 능력을 갖춘 지능형 에이전트로 진화하기 위한 필연적인 단계로 해석될 수 있다.</p>
<h2>7.  대규모 훈련 방법론 및 인프라</h2>
<p>제미나이와 같은 초거대 모델의 탄생은 혁신적인 아키텍처뿐만 아니라, 이를 현실화하는 대규모 훈련 방법론과 최첨단 인프라가 뒷받침되었기에 가능했다. 제미나이의 훈련 과정은 크게 세 단계로 구성된다.</p>
<ol>
<li>
<p><strong>사전 훈련 (Pre-training)</strong>: 이 단계는 모델이 세상에 대한 방대한 일반 지식을 학습하는 과정이다. 훈련 데이터는 웹 문서, 서적, 코드 등 텍스트 데이터뿐만 아니라, 제미나이의 멀티모달 특성을 위해 방대한 양의 이미지, 오디오, 비디오 데이터를 포함한다.5 모델은 이 데이터를 기반으로 ’다음 토큰 예측’과 같은 자기지도학습(self-supervised) 과제를 수행하며 언어와 데이터 양식 간의 근본적인 패턴을 학습한다. 구글은 고품질의 훈련을 위해 데이터셋에 엄격한 품질 및 안전 필터를 적용하고, 훈련 후반부로 갈수록 특정 도메인 데이터의 가중치를 높이는 단계적 훈련(staged training) 방식을 사용한다.35</p>
</li>
<li>
<p><strong>지도 미세조정 (Supervised Fine-Tuning, SFT)</strong>: 사전 훈련된 모델은 범용적인 능력을 갖추지만, 특정 작업(예: 지시 따르기, 대화)을 정교하게 수행하는 능력은 부족할 수 있다. SFT는 인간이 직접 작성한 고품질의 ‘프롬프트-응답’ 쌍으로 구성된 데이터셋을 사용하여 모델을 추가로 훈련시키는 과정이다. 이를 통해 모델은 사용자의 의도를 더 잘 파악하고, 유용하고 정확한 답변을 생성하는 방법을 학습한다.2</p>
</li>
<li>
<p><strong>인간 피드백 기반 강화학습 (Reinforcement Learning from Human Feedback, RLHF)</strong>: 이 단계는 모델의 출력을 인간의 복잡한 선호도 및 가치와 정렬(align)시키는 과정이다.37 먼저, 동일한 프롬프트에 대해 모델이 생성한 여러 응답을 인간 평가자가 순위를 매긴다. 이 데이터를 사용하여 어떤 응답이 더 ‘좋은’ 응답인지를 예측하는 ’보상 모델(reward model)’을 훈련시킨다.39 마지막으로, 본래의 언어 모델은 이 보상 모델로부터 더 높은 점수를 받는 방향으로 자신의 정책을 업데이트하는 강화학습을 통해 미세 조정된다. 제미나이 Ultra 모델의 최종 정제 과정에 이 RLHF가 사용되었다.2</p>
</li>
</ol>
<p>이러한 막대한 규모의 훈련을 가능하게 하는 것은 구글이 자체 설계한 AI 가속기인 TPU(Tensor Processing Unit)다.5 제미나이 훈련에는 TPUv4와 TPUv5e가 사용되었으며, 특히 제미나이 Ultra는 여러 데이터센터에 분산된 수천 개의 TPUv4 칩으로 구성된 ‘SuperPod’ 클러스터를 활용했다.6 이러한 대규모 분산 훈련을 효율적으로 관리하고 조율하는 것이 바로 ‘Pathways’ 소프트웨어 시스템이다. Pathways는 단일 Python 프로세스로 전체 훈련 과정을 오케스트레이션하여 개발 복잡성을 획기적으로 줄이고, 하드웨어 장애 발생 시 메모리 내 모델 상태의 중복 복사본에서 신속하게 복구하는 등 17 대규모 시스템의 안정성을 보장하는 핵심 역할을 수행한다.</p>
<p>제미나이의 성공은 단순히 모델 아키텍처의 우수성만으로 설명될 수 없다. 이는 AI 모델에 최적화된 하드웨어(TPU), 이를 효율적으로 제어하는 소프트웨어 스택(Pathways), 그리고 방대한 데이터를 처리하는 파이프라인이 긴밀하게 통합된 ‘수직적 통합(Vertical Integration)’ 생태계의 산물이다. 구글은 모델이 요구하는 특정 연산에 최적화된 하드웨어를 직접 설계하고, 이 하드웨어를 가장 효율적으로 활용하는 소프트웨어를 개발하며, 이 모든 것을 대규모로 안정적으로 운영하는 독보적인 노하우를 축적했다. 이러한 수직적 통합은 경쟁사들이 공개된 아키텍처를 모방하는 것만으로는 따라올 수 없는 근본적인 효율성과 확장성의 격차를 만들어내며, 제미나이의 기술적 리더십을 뒷받침하는 보이지 않는 강력한 힘이다.</p>
<h2>8.  정량적 성능 평가 및 경쟁 모델 비교</h2>
<p>제미나이 모델 패밀리의 기술적 우수성은 다양한 학술 및 산업 표준 벤치마크에서의 정량적 평가를 통해 입증된다. 이러한 벤치마크는 모델의 다방면적인 능력을 객관적인 수치로 측정하고, 주요 경쟁 모델들과의 성능을 비교하는 중요한 척도가 된다.</p>
<p>가장 널리 인용되는 벤치마크 중 하나인 MMLU(Massive Multitask Language Understanding)는 수학, 물리, 역사, 법률 등 57개 분야에 걸친 방대한 지식과 문제 해결 능력을 평가한다.41 제미나이 1.0 Ultra는 이 벤치마크에서 90.0%라는 점수를 기록하며, 인간 전문가의 평균 점수(89.8%)를 최초로 넘어선 모델이 되었다.2 이는 제미나이가 광범위한 분야에서 인간 수준의 지식을 갖추었음을 시사하는 중요한 성과다.</p>
<p>멀티모달 능력 평가에서는 MMMU(Multi-modal, Multi-task, Multi-domain Understanding) 벤치마크가 핵심적인 지표로 사용된다. 제미나이 1.0 Ultra는 이 벤치마크에서도 62.4%의 점수로 당시 최고 기록을 경신하며, 복잡한 이미지와 텍스트를 넘나드는 추론 능력을 입증했다.6</p>
<p>아래 표는 제미나이 모델들의 주요 버전과 GPT, Claude 등 최신 경쟁 모델들의 핵심 벤치마크 점수를 비교한 것이다. 이 표는 각 모델의 강점과 약점을 한눈에 파악할 수 있도록 돕는다. 예를 들어, MMLU는 전반적인 지식 수준을, GSM8K는 초등 수준의 수학적 추론 능력을, HumanEval과 SWE-Bench는 각각 기초 및 에이전트 기반의 고급 코딩 능력을, 그리고 MMMU는 멀티모달 추론 능력을 대표한다.</p>
<table><thead><tr><th>모델</th><th>MMLU (지식)</th><th>GSM8K (수학 추론)</th><th>HumanEval (코딩)</th><th>SWE-Bench (에이전트 코딩)</th><th>MMMU (멀티모달 추론)</th></tr></thead><tbody>
<tr><td><strong>Gemini 2.5 Pro</strong></td><td>88.6% 42</td><td>-</td><td>75.6% 42</td><td>63.8% 33</td><td>79.6% 42</td></tr>
<tr><td><strong>Gemini 1.5 Pro</strong></td><td>85.9% 43</td><td>90.8% 43</td><td>84.1% 43</td><td>-</td><td>62.2% 43</td></tr>
<tr><td><strong>Gemini 1.0 Ultra</strong></td><td>90.0% 5</td><td>-</td><td>74.4% 2</td><td>-</td><td>62.4% 6</td></tr>
<tr><td><strong>GPT-4o</strong></td><td>88.7% 44</td><td>-</td><td>90.2% 44</td><td>52-54.6% 45</td><td>82.9% 42</td></tr>
<tr><td><strong>Claude 3.5 Sonnet</strong></td><td>-</td><td>71.1% (MATH) 46</td><td>-</td><td>62.3% 45</td><td>-</td></tr>
<tr><td><strong>Claude 3 Opus</strong></td><td>86.8% 44</td><td>60.1% (MATH) 44</td><td>84.9% 44</td><td>72.5% 42</td><td>76.5% 42</td></tr>
</tbody></table>
<p>표 분석 결과, 몇 가지 중요한 경향이 관찰된다. 제미나이 1.0 Ultra는 MMLU에서 여전히 강력한 성능을 보이며, 이는 방대한 지식 기반을 잘 구축했음을 의미한다. 제미나이 1.5 Pro는 GSM8K와 HumanEval에서 높은 점수를 기록하며, 수학적 추론과 코딩 능력에서 뛰어난 균형을 보여준다. 최신 모델인 제미나이 2.5 Pro는 특히 에이전트 기반 코딩 능력을 측정하는 SWE-Bench에서 경쟁 모델들을 앞서며, ‘사고’ 능력이 실제 복잡한 문제 해결에 기여하고 있음을 시사한다.</p>
<p>경쟁 모델과 비교하면, GPT-4o는 HumanEval과 MMMU에서 매우 강력한 성능을 보이며, 특히 멀티모달 추론에서 현재 가장 높은 점수를 기록하고 있다. Claude 3 Opus는 SWE-Bench에서 가장 높은 점수를 기록하며 복잡한 코딩 작업에서의 강점을 나타낸다. 이처럼 각 모델은 특정 영역에서 미묘한 강점과 약점을 보이며, 이는 모델 선택 시 해결하고자 하는 문제의 특성을 고려해야 함을 시사한다. 전반적으로 제미나이 모델 패밀리는 모든 분야에서 최상위권의 성능을 유지하며, 특히 멀티모달 이해와 고급 추론 능력에서 지속적인 발전을 보여주고 있다.</p>
<h2>9.  결론: 제미나이의 기술적 의의와 향후 전망</h2>
<p>본 보고서에서 심층적으로 분석한 바와 같이, 구글 제미나이는 단순한 성능 경쟁을 넘어 AI 기술의 근본적인 발전을 이끄는 핵심적인 기술적 원리들을 구현하고 있다.</p>
<p>첫째, <strong>네이티브 멀티모달리티</strong>는 데이터를 처리하는 방식에 대한 패러다임 전환을 의미한다. 분리된 파이프라인을 통합된 단일 모델로 대체함으로써, 제미나이는 텍스트, 이미지, 오디오, 비디오 간의 개념적 연결을 모델의 가장 깊은 수준에서 학습한다. 이는 AI가 세상을 보다 통합적이고 인간과 유사한 방식으로 이해하기 시작했음을 알리는 신호탄이다.</p>
<p>둘째, <strong>MoE 아키텍처와 초장문 컨텍스트</strong>의 결합은 AI 모델의 스케일링 법칙을 재정의했다. 모델의 지식 용량(총 파라미터)과 운영 비용(활성 파라미터)을 분리하고, 미세조정을 대체할 수 있는 수준의 인-컨텍스트 학습을 가능하게 함으로써, 제미나이는 더 강력한 AI를 더 효율적이고 유연하게 활용할 수 있는 길을 열었다.</p>
<p>셋째, 제미나이 2.5에서 도입된 <strong>‘사고(Thinking)’ 능력</strong>은 AI가 단순한 패턴 생성기를 넘어 진정한 문제 해결 에이전트로 진화하는 중요한 단계다. 내부적인 추론과 계획 과정을 도입한 것은 AI가 더 복잡하고, 정답이 정해지지 않은 현실 세계의 문제에 도전하기 시작했음을 보여준다.</p>
<p>이러한 기술적 성취들은 구글의 <strong>수직적으로 통합된 인프라</strong>가 있었기에 가능했다. 자체 설계한 TPU 칩부터 Pathways 소프트웨어 스택에 이르기까지, 하드웨어와 소프트웨어의 긴밀한 결합은 경쟁사들이 쉽게 모방할 수 없는 효율성과 확장성을 제공하며 제미나이의 기술적 우위를 뒷받침한다.</p>
<p>향후 대규모 언어 모델은 제미나이가 제시한 방향을 따라 더욱 정교한 추론 능력, 강화된 에이전트 기능, 그리고 현실 세계와의 더 깊은 상호작용 능력으로 발전할 것으로 전망된다. AI는 더 이상 주어진 데이터에 대해 응답하는 수동적인 존재가 아니라, 복잡한 목표를 설정하고, 여러 도구를 활용하며, 단계적인 계획을 통해 문제를 해결하는 능동적인 파트너로 진화할 것이다. 제미나이는 이러한 미래를 향한 여정에서 중요한 이정표를 세웠으며, 그 기술적 원리에 대한 깊이 있는 이해는 다가올 AI 시대를 준비하는 데 필수적이다.</p>
<h2>10. 참고 자료</h2>
<ol>
<li>(PDF) Gemini: A Family of Highly Capable Multimodal Models - ResearchGate, https://www.researchgate.net/publication/392469607_Gemini_A_Family_of_Highly_Capable_Multimodal_Models</li>
<li>Introducing Gemini: our largest and most capable AI model, https://blog.google/technology/ai/google-gemini-ai/</li>
<li>Unlocking the Power of Multimodal AI and Insights from Google’s Gemini Models - Galileo AI, https://galileo.ai/blog/unlocking-multimodal-ai-google-gemini</li>
<li>Multimodal AI | Google Cloud, https://cloud.google.com/use-cases/multimodal-ai</li>
<li>Brief Review — Gemini: A Family of Highly Capable Multimodal Models | by Sik-Ho Tsang, https://sh-tsang.medium.com/review-gemini-a-family-of-highly-capable-multimodal-models-615c2a100592</li>
<li>Gemini: A Family of Highly Capable Multimodal Models - arXiv, https://arxiv.org/pdf/2312.11805</li>
<li>Gemini: A Family of Highly Capable Multimodal Models - Googleapis.com, https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf</li>
<li>Gemini (language model) - Wikipedia, https://en.wikipedia.org/wiki/Gemini_(language_model)</li>
<li>Attention is All you Need - NIPS, https://papers.nips.cc/paper/7181-attention-is-all-you-need</li>
<li>Attention Is All You Need - arXiv, https://arxiv.org/html/1706.03762v7</li>
<li>Attention Is All You Need - Wikipedia, https://en.wikipedia.org/wiki/Attention_Is_All_You_Need</li>
<li>What is the intuition behind the dot product attention? - Educative.io, https://www.educative.io/answers/what-is-the-intuition-behind-the-dot-product-attention</li>
<li>Scaled Dot-Product Attention Self-attention is the core … - iToBoS, https://itobos.eu/images/iTOBOS/Articles_Blog/NTUA/scaled_dot_attention.pdf</li>
<li>Understanding Scaled Dot-Product Attention in Transformer Models - Medium, https://medium.com/@saraswatp/understanding-scaled-dot-product-attention-in-transformer-models-5fe02b0f150c</li>
<li>Understanding Multi-Head Attention in Transformers | DataCamp, https://www.datacamp.com/tutorial/multi-head-attention-transformers</li>
<li>Tutorial 6: Transformers and Multi-Head Attention — UvA DL …, https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html</li>
<li>Gemini: A Family of Highly Capable Multimodal Models, https://assets.bwbx.io/documents/users/iqjWHBFdfxIU/r7G7RrtT6rnM/v0</li>
<li>What is Google Gemini? | IBM, https://www.ibm.com/think/topics/google-gemini</li>
<li>www.ibm.com, <a href="https://www.ibm.com/think/topics/google-gemini#:~:text=In%20addition%20to%20a%20transformer,depending%20on%20the%20input%20type.">https://www.ibm.com/think/topics/google-gemini#:~:text=In%20addition%20to%20a%20transformer,depending%20on%20the%20input%20type.</a></li>
<li>Outrageously Large Neural Networks: The Sparsely-Gated Mixture …, https://arxiv.org/abs/1701.06538</li>
<li>OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER, https://www.cs.toronto.edu/~hinton/absps/Outrageously.pdf</li>
<li>OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER, http://cslt.org/mediawiki/images/f/f0/OUTRAGEOUSLYLARGENEURALNETWORKSTHESPARSELY-GATEDMIXTURE-OF-EXPERTSLAYER.pdf</li>
<li>Mixture of Experts LLMs: Key Concepts Explained - Neptune.ai, https://neptune.ai/blog/mixture-of-experts-llms</li>
<li>Mixture of Expert Architecture. Definitions and Applications included …, https://ai.plainenglish.io/mixture-of-expert-architecture-7be02b74f311</li>
<li>From Multimodal Marvels to Mixing of Experts - Google’s Gemini Evolution - Medium, https://medium.com/happtiq-data-ai-hub/from-multimodal-marvels-to-mixing-of-experts-googles-gemini-evolution-e42622df65bf</li>
<li>Understanding the Gemini Context Window in Simple Terms - DhiWise, https://www.dhiwise.com/post/the-gemini-context-window-and-its-role-in-ai-precision</li>
<li>Google’s Gemini 1.5 Pro - Revolutionizing AI with a 1M Token Context Window - Medium, https://medium.com/google-cloud/googles-gemini-1-5-pro-revolutionizing-ai-with-a-1m-token-context-window-bfea5adfd35f</li>
<li>Introducing Gemini 1.5, Google’s next-generation AI model - The Keyword, https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/</li>
<li>Gemini 1.5: Google’s Generative AI Model with Mixture of Experts Architecture - Encord, https://encord.com/blog/google-gemini-1-5-generative-ai-model-with-mixture-of-experts/</li>
<li>Long context | Generative AI on Vertex AI - Google Cloud, https://cloud.google.com/vertex-ai/generative-ai/docs/long-context</li>
<li>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context - Googleapis.com, https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf</li>
<li>Gemini - Google DeepMind, https://deepmind.google/models/gemini/</li>
<li>Gemini 2.5: Our most intelligent AI model - The Keyword, https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/</li>
<li>Google AI Studio vs. Vertex AI vs. Gemini, https://cloud.google.com/ai/gemini</li>
<li>An overview of the Gemini app, https://gemini.google/overview/</li>
<li>Supervised Fine Tuning for Gemini LLM | Google Cloud Blog, https://cloud.google.com/blog/products/ai-machine-learning/supervised-fine-tuning-for-gemini-llm</li>
<li>RLHF on Google Cloud, https://cloud.google.com/blog/products/ai-machine-learning/rlhf-on-google-cloud</li>
<li>Comprehensive Guide to Reinforcement Learning in Modern AI - Hugging Face, https://huggingface.co/blog/ProCreations/guide-to-rl</li>
<li>What is RLHF? - Reinforcement Learning from Human Feedback Explained - AWS, https://aws.amazon.com/what-is/reinforcement-learning-from-human-feedback/</li>
<li>Gemini 2.5: Pushing the Frontier with Advanced … - Googleapis.com, https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf</li>
<li>Top LLM Benchmarks Explained: MMLU, HellaSwag, BBH, and Beyond - Confident AI, https://www.confident-ai.com/blog/llm-benchmarks-mmlu-hellaswag-and-beyond</li>
<li>Claude 4 vs GPT-4o vs Gemini 2.5 Pro: Which AI Codes Best in 2025? - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2025/05/best-ai-for-coding/</li>
<li>Gemini 1.5: Unlocking multimodal understanding across … - arXiv, https://arxiv.org/pdf/2403.05530</li>
<li>GPT-4o Benchmark - Detailed Comparison with Claude &amp; Gemini - Wielded, https://wielded.com/blog/gpt-4o-benchmark-detailed-comparison-with-claude-and-gemini</li>
<li>GPT-4.1 Comparison with Claude 3.7 Sonnet and Gemini 2.5 Pro, https://blog.getbind.co/2025/04/15/gpt-4-1-comparison-with-claude-3-7-sonnet-and-gemini-2-5-pro/</li>
<li>Claude 3.5 sonnet Vs GPT-4o: Key details and comparison - Pieces for Developers, https://pieces.app/blog/how-to-use-gpt-4o-gemini-1-5-pro-and-claude-3-5-sonnet-free</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>