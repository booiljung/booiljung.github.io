<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:8.1 시각 정보의 한계와 체화된 감각의 필요성 (The Limits of Vision & Need for Embodied Senses)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>8.1 시각 정보의 한계와 체화된 감각의 필요성 (The Limits of Vision & Need for Embodied Senses)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 8. 시각을 넘어서: 멀티모달 센서 융합 (Beyond Vision: Multimodal Fusion)</a> / <a href="index.html">8.1 시각 정보의 한계와 체화된 감각의 필요성 (The Limits of Vision & Need for Embodied Senses)</a> / <span>8.1 시각 정보의 한계와 체화된 감각의 필요성 (The Limits of Vision & Need for Embodied Senses)</span></nav>
                </div>
            </header>
            <article>
                <h1>8.1 시각 정보의 한계와 체화된 감각의 필요성 (The Limits of Vision &amp; Need for Embodied Senses)</h1>
<h2>1.  서론: 시각 중심주의의 맹점과 체화된 지능의 부상</h2>
<p>인공지능과 로봇 공학의 발전사에서 시각(Vision)은 오랫동안 ‘주요 감각(Primary Sense)’의 독점적 지위를 누려왔다. 인간의 뇌 피질 중 상당 부분이 시각 처리에 할당되어 있다는 생물학적 사실과, 2010년대 이후 심층 신경망(Deep Neural Networks), 특히 합성곱 신경망(CNN)과 시각 트랜스포머(ViT)가 보여준 객체 인식 및 분할(Segmentation) 성능의 비약적인 발전은 “로봇이 세상을 온전히 볼 수 있다면, 세상을 온전히 조작할 수 있을 것“이라는 암묵적인 기술적 낙관론을 형성했다. 이러한 시각 중심주의(Vision-centrism)는 로봇이 통제된 실험실 환경이나 시뮬레이션 속에서 동작할 때는 유효한 듯 보였다.</p>
<p>그러나 로봇이 정형화되지 않은 실제 세계(in-the-wild)로 진입하여 물리적 객체와 직접 상호작용을 시도하는 순간, 이 명제는 근본적인 도전에 직면한다. 로봇 조작(Manipulation)의 본질은 ‘관찰’이 아닌 ‘접촉’에 있기 때문이다. 시각은 본질적으로 대상과의 물리적 거리를 전제로 하는 원격 감각(Tele-ception)이다. 반면, 조작은 로봇의 말단 장치(End-effector)와 대상 사이의 거리를 0으로 만드는 과정이며, 이 접촉의 순간부터 작업의 성공 여부를 결정짓는 지배적인 변수는 시각적 외형(Appearance)이 아닌 마찰, 강성, 질량 분포, 표면 질감과 같은 물리적 속성들로 전환된다.</p>
<p>최근의 Embodied AI(체화된 인공지능) 연구 흐름은 이러한 인식의 대전환을 대변한다. 지능은 격리된 연산 장치 속의 알고리즘이 아니라, 신체를 통해 환경과 물리적으로 상호작용하는 과정에서 발현된다는 것이다. 특히 2024년과 2025년을 기점으로 발표된 다수의 서베이 논문과 기술 보고서들은 로봇이 시각 정보만으로는 파악할 수 없는 물리적 세계의 ‘보이지 않는 속성(Invisible Properties)’을 능동적으로 탐색하고, 다중 모달(Multi-modal) 감각 융합을 통해 상태 추정(State Estimation)의 불확실성을 낮추는 것이 차세대 로봇 제어의 핵심 과제임을 강조하고 있다.</p>
<p>본 장에서는 최신 SOTA(State-of-the-Art) 연구 결과들을 바탕으로, 로봇 조작에 있어 시각 정보가 갖는 광학적, 기하학적, 동역학적 한계를 심층 분석하고, 이를 보완하기 위한 체화된 감각—촉각(Tactile), 청각(Audio), 고유수용성 감각(Proprioception), 열 감각(Thermal)—의 필요성을 논증한다. 나아가 이들 이종 감각(Heterogeneous Senses)이 어떻게 융합되어 인간 수준의 적응형 조작(Adaptive Manipulation)을 가능케 하는지 기술한다.</p>
<h2>2.  시각 정보의 물리적 및 기하학적 한계</h2>
<p>컴퓨터 비전 기술은 객체 검출(Object Detection)과 6D 자세 추정(Pose Estimation)에서 인간의 인지 능력을 상회하는 정확도를 달성하기도 했다. 그러나 이러한 성취는 주로 ‘보는 것’에 국한된 것이며, ‘만지기 위해 보는 것’으로 목적이 전환될 때 센서 자체의 물리적 특성과 데이터 처리 과정의 정보 손실로 인한 근본적인 한계가 드러난다.</p>
<h3>2.1  광학적 특성에 의한 실패 모드 (Optical Failure Modes)</h3>
<p>로봇에 장착된 시각 센서(RGB 카메라, RGB-D 깊이 센서, LiDAR 등)는 물체 표면에서 반사된 빛을 수집하여 정보를 재구성한다. 이 과정에서 물체의 광학적 특성이 센서가 가정하는 람베르트 반사(Lambertian reflectance) 모델과 일치하지 않을 경우, 데이터는 왜곡되거나 소실된다.</p>
<ol>
<li><strong>투명성 및 반사성 (Transparency and Specularity):</strong> 유리, 플라스틱, 금속과 같은 소재는 가정, 병원, 산업 현장 등 로봇이 배치될 모든 환경에서 흔히 발견된다. 그러나 전통적인 깊이 센서, 특히 적외선 패턴을 투사하는 구조광(Structured Light) 방식이나 빛의 비행 시간을 측정하는 ToF(Time-of-Flight) 센서에게 투명한 물체는 ‘보이지 않는’ 존재이거나 배경의 깊이 값을 반환하게 만드는 노이즈원이다. 또한, 금속과 같은 반사성 표면(Specular surface)은 빛을 난반사가 아닌 정반사로 튕겨내어 카메라에 과도한 광량을 유입시키거나(Saturation), 엉뚱한 위치에 상을 맺게 하여(Flying pixels) 깊이 맵(Depth map)에 치명적인 구멍(Hole)을 만든다. 최근 NeRF(Neural Radiance Fields)나 3D Gaussian Splatting 기반의 방법론들이 이러한 투명/반사 객체의 형상을 복원하려 시도하고 있으나, 수백 Hz 이상의 제어 주기를 요구하는 실시간 로봇 조작 루프 내에서 수행하기에는 계산 비용이 과도하게 높다.</li>
<li><strong>조명 조건의 민감성 (Lighting Sensitivity):</strong> 로봇 비전 시스템은 조명 환경의 변화에 극도로 취약하다. 강한 광원 역광으로 인한 그림자(Shadows)는 물체의 형상을 왜곡하여 인식 오류를 일으키고, 금속 표면의 번짐(Glare)은 특징점(Feature point) 추출을 불가능하게 한다. 반대로 저조도 환경에서는 이미지 센서의 노이즈가 증폭되어 물체의 경계선 검출이 무력화된다. 공장이나 가정과 같이 조명을 완벽히 통제할 수 없는 비정형 환경에서 시각 정보만으로 물체의 표면 법선 벡터(Surface Normal)를 추정하여 파지(Grasp) 계획을 세우는 것은 매우 낮은 신뢰도를 갖는다.</li>
</ol>
<h3>2.2  가려짐(Occlusion)과 시점의 제약</h3>
<p>로봇 조작에서 발생하는 가장 역설적인 문제는, 로봇이 물체를 조작하기 위해 손(End-effector)을 뻗는 순간, 가장 중요한 시각 정보가 가려진다는 점이다.</p>
<ul>
<li><strong>자체 가려짐 (Self-Occlusion):</strong> 로봇의 팔과 그리퍼가 대상 물체와 카메라 사이를 가로막는 현상이다. 물체와 접촉하기 직전(Pre-grasp phase)과 접촉하여 조작하는 중(In-hand manipulation)에는 외부 카메라가 접촉면을 볼 수 없다. 손목에 카메라를 부착하는 Eye-in-Hand 구성을 사용하더라도, 물체를 쥐는 근접 거리에서는 초점 거리의 한계와 시야각(FoV) 제한으로 인해 접촉 부위가 사각지대가 된다.</li>
<li><strong>조작 중 상호 가려짐:</strong> 물체를 파지한 상태에서 다른 물체에 끼워 넣거나(Peg-in-hole), 쌓는 작업(Stacking)을 수행할 때, 목표 지점은 파지된 물체나 로봇 손에 의해 필연적으로 가려진다. 목표 지점의 시각 정보가 차단된 상태에서 미세한 힘 제어 없이 오직 사전에 계획된 위치 제어(Position Control)만으로 작업을 강행할 경우, 수 100μm 단위의 오차로도 부품 파손(Jamming)이나 작업 실패로 직결된다.</li>
</ul>
<h3>2.3  추정 불가능한 물리량 (The Unseen Physical Properties)</h3>
<p>시각 정보의 가장 결정적인 한계는 물체의 **동역학적 속성(Dynamical Properties)**을 직접 측정할 수 없다는 점이다. 시각은 기하학적 형상(Geometry)을 제공할 뿐, 그 형상을 유지하거나 변화시키는 물리적 성질은 제공하지 않는다.</p>
<table><thead><tr><th><strong>물리적 속성 (Physical Property)</strong></th><th><strong>시각 정보 기반 추정의 한계 (Limitations of Vision)</strong></th><th><strong>로봇 조작에 미치는 구체적 영향 (Impact on Manipulation)</strong></th></tr></thead><tbody>
<tr><td><strong>질량 및 질량 중심 (Mass &amp; CoM)</strong></td><td>부피(Volume)는 추정 가능하나, 내부 밀도 분포(내용물의 유무, 재질의 균일성)를 알 수 없어 정확한 질량과 질량 중심(CoM) 파악 불가.</td><td>파지 시 필요한 토크 계산 오류, 물체 이송 중 회전 모멘트 발생으로 인한 낙하, 로봇 팔의 동역학 모델 오차 유발.</td></tr>
<tr><td><strong>마찰 계수 (Friction Coefficient)</strong></td><td>표면 텍스처로 대략적 유추는 가능하나, 표면의 오염도, 습기, 미세한 거칠기 등 실제 마찰계수는 접촉 전까지 알 수 없음. ‘보기에 거친’ 표면이 실제로는 미끄러울 수 있음.</td><td>파지 시 미끄러짐(Slip) 발생, 과도한 악력 인가로 인한 유연 물체 손상, 정밀 조립 시 마찰 보상 실패.</td></tr>
<tr><td><strong>강성 및 탄성 (Stiffness &amp; Compliance)</strong></td><td>변형이 일어나기 전까지는 알 수 없으며, 시각적으로 감지될 정도의 변형은 이미 과도한 힘이 가해진 후일 가능성이 높음.</td><td>유연한 물체(천, 케이블, 식품) 조작 시 형태 유지 실패, 과일 등 연성 물체 파지 시 압력 조절 실패로 인한 파손.</td></tr>
<tr><td><strong>온도 및 열적 특성 (Thermal Properties)</strong></td><td>열화상 카메라 외 일반 비전으로는 측정 불가. 재질의 열전도율 등은 접촉 없이는 파악 불가능.</td><td>뜨거운 물체 파지 시 센서 및 그리퍼 손상, 미끄러운 얼음 등의 재질 오인, 인간 협업 시 안전 사고 위험.</td></tr>
</tbody></table>
<p>이러한 물리적 속성들은 시각적 외관(Appearance)과 반드시 일치하지 않는다. 예를 들어, 무거운 금속처럼 도색된 가벼운 플라스틱이나, 꽉 차 보이지만 실제로는 비어 있는 상자 등은 시각 정보만을 신뢰하는 로봇에게 ‘물리적 착시(Physical Illusion)’를 유발하여 치명적인 제어 실패를 야기한다.</p>
<h2>3.  체화된 감각의 필요성: 접촉이 만드는 확신</h2>
<p>로봇이 불확실성으로 가득 찬 실제 환경에서 강인하게 동작하기 위해서는 시각 정보를 넘어서는 ‘체화된 감각(Embodied Senses)’의 통합이 필수적이다. 이는 단순히 센서 하드웨어를 추가하는 차원을 넘어, 로봇 제어의 패러다임을 사전 ‘계획(Planning)’ 중심에서 실시간 ‘반응(Reaction) 및 상호작용(Interaction)’ 중심으로 전환하는 것을 의미한다.</p>
<h3>3.1  촉각(Tactile): 상호작용의 최전선과 제어의 확정</h3>
<p>촉각은 시각이 놓치는 0mm 거리의 접촉 정보를 제공하며, 로봇 조작의 성공을 최종적으로 확정(Confirmation)하는 유일한 감각이다.</p>
<ol>
<li><strong>미끄러짐 감지 및 파지 안정성 확보:</strong> 인간은 물체를 쥘 때 피부의 기계수용체(Mechanoreceptors, 특히 FA-I, FA-II)를 통해 미세한 진동(Micro-vibration)을 감지하여, 물체가 떨어지기 직전의 초기 미끄러짐(Incipient slip)을 0.1초 이내에 파악하고 반사적으로 악력을 조절한다. 로봇 공학에서도 이러한 생체 모방적 접근이 시도되고 있다. 고주파 진동이나 접촉면의 전단력(Shear force) 변위를 감지하는 젤사이트(GelSight)나 바이오택(BioTac)과 같은 촉각 센서를 통해, 로봇은 물체가 완전히 미끄러지기 전에 파지력을 조절할 수 있다. 이는 달걀이나 얇은 유리잔과 같이 깨지기 쉬운 물체를 다룰 때 필수적이다.</li>
<li><strong>Contact-Rich Manipulation (접촉 집약적 조작):</strong> 조립(Assembly), 끼워 맞추기(Insertion), 표면 닦기(Wiping) 등은 물체 간의 지속적인 접촉과 복잡한 힘의 상호작용을 필요로 한다. 이러한 작업에서 시각 센서는 가려짐(Occlusion)과 해상도 문제로 인해 무용지물이 되기 쉽다. 이때 오직 힘/토크 센서(F/T sensor)와 분포형 촉각 센서만이 현재의 접촉 상태(Contact state)와 힘의 작용점, 방향을 정확히 알려줄 수 있다. 최신 연구인 ViTaMIn 이나 Vision-Force Curriculum Learning  연구에 따르면, 시각 정보가 실패하기 쉬운 정밀 펙인홀(Peg-in-hole) 작업이나 가위 걸기 등의 작업에서 촉각 피드백을 융합했을 때 성공률이 20%~50% 이상 향상됨이 입증되었다.</li>
<li><strong>국소적 기하 정보 복원 (Tactile Exploration):</strong> 시각 센서가 전체적인 형상(Global Geometry)을 거시적으로 본다면, 촉각 센서는 접촉면의 초정밀 형상(Local Geometry)을 ‘읽는다’. 이는 시각적으로 구분이 어려운 미세한 텍스처, 표면의 굴곡, 마모 상태 등을 파악하는 데 사용되며, 이를 통해 로봇은 마치 눈을 감고 물체를 더듬어 형상을 파악하듯(Tactile Servoing) 미지의 물체를 탐색할 수 있다.</li>
</ol>
<h3>3.2  청각(Audio): 사건(Event)의 감지자 및 내부 상태 추정</h3>
<p>청각은 그동안 로봇 조작 분야에서 상대적으로 과소평가되어 왔으나, 최근 연구들은 청각 신호가 시각과 촉각의 사각지대를 메우는 중요한 역할을 수행할 수 있음을 보여준다.</p>
<ul>
<li><strong>이벤트 감지 (Event Detection):</strong> 스냅핏(Snap-fit) 조립 시 발생하는 ‘딸깍’ 소리는 작업의 성공적 완료를 알리는 가장 확실하고 즉각적인 신호이다. 시각적으로는 부품이 완전히 결합되었는지 미세한 틈을 확인하기 어렵고, 촉각 신호만으로는 삽입 시의 마찰력과 체결 시의 충격을 구분하기 어려울 때, 청각 신호는 높은 신뢰도의 ‘성공 판정’ 트리거가 된다.</li>
<li><strong>재질 분류 및 내부 상태 추정:</strong> 물체를 두드리거나 긁을 때 발생하는 충격음(Impact sound)의 주파수 특성은 물체의 재질(유리, 나무, 금속, 플라스틱)을 판별하는 데 결정적인 단서를 제공한다. 또한, 흔들었을 때 나는 소리를 통해 불투명한 용기의 내용물 유무나 알갱이의 종류를 추정할 수 있다. 이는 시각 정보만으로는 불가능한 내부 상태(Internal State) 분류 작업을 비파괴적으로 수행할 수 있게 한다.</li>
<li><strong>능동 음향 감지 (Active Acoustic Sensing):</strong> 최근에는 로봇 손가락 내부에서 스피커로 소리를 발생시키고, 손가락의 변형에 따라 달라지는 내부 공명(Resonance)이나 반사음을 마이크로 분석하여 접촉 위치와 힘을 추정하는 기술이 개발되었다. 이는 복잡한 배선이 필요한 기존 촉각 센서에 비해 저비용으로 높은 분해능의 접촉 정보를 얻을 수 있는 혁신적인 대안으로 주목받는다.</li>
<li><strong>음향 서보잉 (Acoustic Servoing):</strong> 시각 정보가 부재하거나 가려진 상황에서 소리의 발생원(Sound Source)을 향해 로봇 팔이나 이동 로봇을 유도하는 제어 기술이다. 이는 재난 현장의 구조 로봇이나 소음이 심한 산업 현장에서의 이상 감지 및 접근에 활용된다.</li>
</ul>
<h3>3.3  고유수용성 감각(Proprioception)과 열 감각(Thermal)</h3>
<ul>
<li><strong>고유수용성 감각:</strong> 로봇이 자신의 관절 각도(Encoder value)와 모터에 걸리는 토크(Current sensing)를 실시간으로 아는 것은 외부 세계를 인식하는 기준점이 된다. 시각 정보가 차단되거나 노이즈가 심한 상태에서도 로봇은 자신의 팔 끝(End-effector)이 공간상 어디에 위치하는지 정확히 알아야 하며, 이는 시각-운동 협응(Hand-Eye Coordination)의 기초가 된다. Jitendra Malik 교수는 “눈을 가리고도 걸을 수 있는 능력“이야말로 진정한 지능의 기반이며, 시각은 이를 보정하는 역할이라고 강조한 바 있다.</li>
<li><strong>열 감각 (Thermal Sensing):</strong> 물체의 온도는 재질 추정뿐만 아니라 로봇과 인간의 안전(Safety)을 위해서도 중요하다. 열전도율(Thermal Conductivity)의 차이를 이용하여 시각적으로 유사한 금속과 플라스틱을 구분하거나, 과열된 모터나 배터리, 뜨거운 조리 기구 등을 감지하여 로봇 하드웨어의 손상을 방지하고 화재 위험을 예방한다. 특히 인간과 협업하는 코봇(Cobot)의 경우, 인간의 체온을 감지하여 접촉 시 안전 모드로 전환하는 데 활용될 수 있다.</li>
</ul>
<h2>4.  생물학적 영감: Johansson &amp; Flanagan의 조작 단계 모델</h2>
<p>체화된 감각, 특히 시각과 촉각의 상호보완적 필요성은 인간의 정교한 운동 제어 메커니즘에서 명확한 근거를 찾을 수 있다. 신경과학자 Johansson과 Flanagan의 선구적인 연구에 따르면, 인간의 능숙한 물체 조작은 시각과 촉각이 <strong>순차적이고 상호보완적</strong>으로 작용하는 제어 루프(Control Loop)에 의해 달성된다.</p>
<ol>
<li><strong>도달 단계 (Reach Phase):</strong> 이 단계에서는 시각 정보가 지배적인 역할을 한다. 눈은 물체의 위치와 대략적인 형상을 파악하여 손을 이동시키는 피드포워드(Feedforward) 경로 계획을 주도한다.</li>
<li><strong>접촉 및 부하 단계 (Load Phase):</strong> 손가락이 물체에 닿는 순간, 제어의 주도권은 시각에서 촉각으로 극적으로 넘어간다. 피부의 기계수용체가 접촉 이벤트(Contact Event)를 감지하면, 뇌는 시각적으로 예측했던 무게와 마찰계수를 실제 촉각 피드백과 대조하며 반사적으로 악력을 조절한다. 이때 시각 정보는 거의 사용되지 않으며, 촉각이 ’진실(Ground Truth)’을 제공한다.</li>
<li><strong>조작 및 이송 (Manipulation Phase):</strong> 물체를 이동시키는 동안에는 시각(경로 및 장애물 확인), 고유수용성 감각(팔의 위치 추적), 촉각(미끄러짐 감시 및 외란 감지)이 고도로 융합되어 사용된다.</li>
<li><strong>해제 (Release Phase):</strong> 물체를 내려놓을 때 손가락 끝에서 느껴지는 진동(촉각)이나 소리(청각)가 작업 완료의 신호가 되어 손을 떼는 동작을 트리거한다.</li>
</ol>
<p>이 모델은 로봇 제어 시스템 설계에 중요한 시사점을 준다. <strong>“예측(Prediction)은 시각으로, 확인(Confirmation)은 촉각으로”</strong> 수행해야 한다는 것이다. 시각 정보만으로 전체 조작 과정을 제어하려는 시도는 생물학적으로도 비효율적이며, 공학적으로도 오류에 취약한 접근법이다.</p>
<h2>5.  멀티모달 센서 융합과 SOTA 기술 동향</h2>
<p>단일 센서의 한계를 극복하기 위해, 현대 로봇 제어 기술은 이종 센서 데이터를 융합하여 상호 보완적인 정보를 추출하는 방향으로 발전하고 있다.</p>
<h3>5.1  확률론적 융합과 딥러닝 기반 융합</h3>
<ul>
<li><strong>베이지안 최적 통합 (Bayesian Optimal Integration):</strong> 인간의 뇌가 감각 정보를 통합할 때 각 감각 신호의 신뢰도(불확실성의 역수)에 비례하여 가중치를 두는 원리를 모방한다. 로봇은 베이지안 추론을 통해 조명이 어두운 환경(시각 불확실성 증가)에서는 촉각 정보의 가중치를 높여 위치를 추정하고, 접촉이 불안정한 상황에서는 시각 정보에 더 의존하는 방식으로 최적의 상태 추정치를 도출한다.</li>
<li><strong>트랜스포머 기반 융합 (Transformer-based Fusion):</strong> 최근 딥러닝 연구에서는 Self-Attention 메커니즘을 활용하여 시각(이미지 패치), 촉각(압력 맵), 청각(스펙트로그램) 데이터를 각각의 토큰(Token)으로 변환하고 융합하는 구조가 주류를 이룬다. Cross-Modal Attention을 통해 시각적 특징과 촉각적 특징 간의 상관관계를 학습함으로써, 시각적으로 가려진 부분의 물리적 속성을 촉각 정보로 보완하거나(Cross-modal Completion), 소리를 통해 재질을 유추하여 시각적 인식을 돕는 상호 보완적 추론을 수행한다.</li>
</ul>
<h3>5.2  최신 연구 사례: World Models과 VLA</h3>
<ul>
<li><strong>World Models (세계 모델):</strong> Yann LeCun이 제창한 <strong>World Model</strong> 개념은 체화된 AI의 미래를 보여준다. 단순히 텍스트나 이미지를 생성하는 것을 넘어, 로봇이 물리 세계의 법칙을 내재화하여 자신의 행동이 환경에 미칠 결과를 예측(Simulation)하는 모델이다. 여기서 체화된 감각 데이터는 World Model이 물리 법칙(중력, 마찰, 충돌 등)을 학습하는 Ground Truth 역할을 한다. 즉, 로봇은 직접 만져보고(Tactile) 들어봄으로써(Proprioception) 물체의 거동을 배우고, 이를 바탕으로 보기만 해도(Vision) 만졌을 때의 느낌을 예측할 수 있게 된다.</li>
<li><strong>VLA (Vision-Language-Action) 모델:</strong> 구글의 RT-2나 OpenVLA와 같은 최신 모델들은 대규모 언어 모델(LLM)의 추론 능력에 시각과 행동(Action)을 결합하였다. 초기 모델들은 주로 시각 정보에 의존했으나, 최신 연구들은 여기에 **촉각 토큰(Tactile Tokens)**을 추가하여 “무겁고 미끄러운 물체를 조심해서 들어라“와 같은 추상적 자연어 명령을 구체적인 힘 제어 및 파지 전략으로 변환하는 시도를 하고 있다. 이는 언어적 지식과 체화된 감각을 연결하는 중요한 진보이다.</li>
</ul>
<h2>6.  결론</h2>
<p>시각 정보는 로봇에게 ‘어디로 가야 하는지’를 알려주는 지도(Map)이지만, ‘어떻게 상호작용해야 하는지’를 알려주는 매뉴얼(Manual)은 아니다. 시각 정보가 갖는 광학적, 기하학적, 물리적 한계는 명확하며, 이는 단순히 컴퓨터 비전 알고리즘의 개선만으로는 극복할 수 없는 정보의 부재(Absence of Information) 문제이다. 따라서 로봇이 진정으로 세상과 상호작용하기 위해서는 촉각, 청각, 고유수용성 감각 등 체화된 감각을 통해 물리적 실재를 확인(Confirm)하고, 이를 시각 정보와 융합하여 조작의 불확실성을 최소화해야 한다. 차세대 로봇 지능은 단순히 ‘보는 것’을 넘어, 온몸으로 ‘느끼고 예측하는’ 능력에서 비롯될 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>A Comprehensive Survey on Embodied AI - arXiv, https://arxiv.org/html/2407.06886v8</li>
<li>Multi-Modal Perception with Vision, Language, and Touch for Robot …, https://www2.eecs.berkeley.edu/Pubs/TechRpts/2025/EECS-2025-68.pdf</li>
<li>Embodied intelligence: Recent advances and future perspectives, https://www.the-innovation.org/data/article/informatics/preview/pdf/TII-2025-0015.pdf</li>
<li>Will embodied AI create robotic coworkers? | McKinsey, https://www.mckinsey.com/industries/industrials/our-insights/will-embodied-ai-create-robotic-coworkers</li>
<li>A SURVEY OF EMBODIED ARTIFICIAL INTELLIGENCE DATA …, https://airs.cuhk.edu.cn/sites/default/files/2025-06/Survey_Arxiv.pdf</li>
<li>Embodied AI Agents: Redefining Productivity, Intelligence, and …, https://globalxetfs.eu/embodied-ai-agents-redefining-productivity-intelligence-and-human-machine-collaboration/</li>
<li>Lighting Challenges in Robotic Machine Vision Applications | Remtec, https://www.remtecautomation.com/news/lighting-challenges-in-robotic-machine-vision-applications/</li>
<li>Practical Guide to Machine Vision Lighting - Advanced Illumination, https://advancedillumination.com/a-practical-guide-to-machine-vision-lighting/</li>
<li>Robotic Perception of Transparent Objects: A Review - arXiv, https://arxiv.org/pdf/2304.00157</li>
<li>Top 10 Challenges for Robot Vision - Robotiq’s blog, https://blog.robotiq.com/top-10-challenges-for-robot-vision</li>
<li>ViTaMIn: Learning Contact-Rich Tasks Through Robot-Free Visuo …, https://arxiv.org/html/2504.06156v1</li>
<li>Vision-force-fused curriculum learning for robotic contact-rich …, https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2023.1280773/full</li>
<li>Vision-force-fused curriculum learning for robotic contact-rich … - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC10590057/</li>
<li>Predictive Visuo-Tactile Interactive Perception Framework for … - Pure, https://pure.tue.nl/ws/portalfiles/portal/345475565/2411.09020v1.pdf</li>
<li>image2mass: Estimating the Mass of an Object from Its Image, http://proceedings.mlr.press/v78/standley17a/standley17a.pdf</li>
<li>KIT - IFL Studium und Lehre - Abschlussarbeiten - Estimating Object …, https://www.ifl.kit.edu/Abschlussarbeiten_6273.php</li>
<li>Identifying Terrain Physical Parameters from Vision, https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/697980/1/Chen_Identifying_Terrain_Physical_Parameters.pdf</li>
<li>Friction from Vision: A Study of Algorithmic and Human Performance …, https://www.researchgate.net/publication/310832317_Friction_from_Vision_A_Study_of_Algorithmic_and_Human_Performance_with_Consequences_for_Robot_Perception_and_Teleoperation</li>
<li>Review of Industrial Robot Stiffness Identification and Modelling, https://www.mdpi.com/2076-3417/12/17/8719</li>
<li>Material classification based on thermal properties — A robot and …, https://www.researchgate.net/publication/271548452_Material_classification_based_on_thermal_properties_-_A_robot_and_human_evaluation</li>
<li>Material Recognition from Heat Transfer given Varying Initial …, https://www.roboticsproceedings.org/rss11/p19.pdf</li>
<li>Slip detection for compliant robotic hands using inertial signals and …, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1698591/full</li>
<li>Slip detection for compliant robotic hands using inertial signals and …, https://pmc.ncbi.nlm.nih.gov/articles/PMC12756126/</li>
<li>Proactive slip control by learned slip model and trajectory adaptation, https://proceedings.mlr.press/v205/nazari23a/nazari23a.pdf</li>
<li>Universal slip detection of robotic hand with tactile sensing - Frontiers, https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2025.1478758/full</li>
<li>(PDF) ViTaMIn: Learning Contact-Rich Tasks Through Robot-Free …, https://www.researchgate.net/publication/390600894_ViTaMIn_Learning_Contact-Rich_Tasks_Through_Robot-Free_Visuo-Tactile_Manipulation_Interface</li>
<li>(PDF) Vision-force-fused curriculum learning for robotic contact-rich …, https://www.researchgate.net/publication/374528516_Vision-force-fused_curriculum_learning_for_robotic_contact-rich_assembly_tasks</li>
<li>A Review of Tactile Information: Perception and Action Through Touch, https://www.ri.cmu.edu/app/uploads/2021/08/LiTRo2020.pdf</li>
<li>Sensorimotor Control Strategies for Tactile Robotics - arXiv, https://arxiv.org/html/2501.09468v2</li>
<li>Recognizing object surface material from impact sounds for robot …, http://www.iri.upc.edu/files/scidoc/2619-Recognizing-object-surface-material-from-impact-sounds-for-robot-manipulation.pdf</li>
<li>Learning Robot Manipulation from In-the-Wild Audio-Visual Data, https://arxiv.org/html/2406.19464v1</li>
<li>Acoustic Collision Detection and Localization for Robot Manipulators, https://ox5bc.github.io/public_html/paper/Panotti.pdf</li>
<li>Transformers for snap-fit detection - Diva-portal.org, http://www.diva-portal.org/smash/get/diva2:1947171/FULLTEXT02.pdf</li>
<li>Active Acoustic Contact Sensing for Soft Pneumatic Actuators, https://www.researchgate.net/publication/344981709_Active_Acoustic_Contact_Sensing_for_Soft_Pneumatic_Actuators</li>
<li>Robot Learning for Manipulation of Granular Materials Using Vision …, https://www.ri.cmu.edu/app/uploads/2019/08/Masters_Thesis.pdf</li>
<li>Soft Tactile Sensors for Robot Grippers Using Acoustic Sensing, https://www.cs.cmu.edu/~justinc3/docs/soft_gripper.pdf</li>
<li>Sensor-Based Control for Collaborative Robots - Frontiers, https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2020.576846/full</li>
<li>Autonomous Microrobotic Manipulation Using Visual Servo Control, https://www.mdpi.com/2072-666X/11/2/132</li>
<li>Berkeley Talks transcript: Jitendra Malik on the sensorimotor road to …, https://news.berkeley.edu/2023/03/24/berkeley-talks-transcript-jitendra-malik/</li>
<li>Transcript of #110 – Jitendra Malik: Computer Vision, https://podcasts.happyscribe.com/lex-fridman-podcast-artificial-intelligence-ai/110-jitendra-malik-computer-vision</li>
<li>Eye–Hand Coordination in Object Manipulation, https://www.jneurosci.org/content/21/17/6917</li>
<li>Flanagan (2002) Hand movements - Mark Wexler, <a href="http://wexler.free.fr/library/files/flanagan%20(2002)%20hand%20movements.pdf">http://wexler.free.fr/library/files/flanagan%20(2002)%20hand%20movements.pdf</a></li>
<li>(PDF) Sensory control of object manipulation - ResearchGate, https://www.researchgate.net/publication/228981516_Sensory_control_of_object_manipulation</li>
<li>A Generalized Framework for Modeling Multisensory Integration, https://www.biorxiv.org/content/10.1101/2025.05.26.656124v1.full.pdf</li>
<li>The Neural Mechanisms of Visual and Vestibular Interaction in Self …, https://www.mdpi.com/2079-7737/14/7/740</li>
<li>Multi-modal Sensor Fusion for Learning Rich Models for Interacting …, https://www.repository.cam.ac.uk/bitstreams/6056540b-4476-4ecf-8034-c6ac611c0dda/download</li>
<li>Transformer-Based Sensor Fusion for Autonomous Driving: A Survey, https://openaccess.thecvf.com/content/ICCV2023W/VCL/papers/Singh_Transformer-Based_Sensor_Fusion_for_Autonomous_Driving_A_Survey_ICCVW_2023_paper.pdf</li>
<li>See, Hear, and Feel: Smart Sensory Fusion for Robotic Manipulation, https://proceedings.mlr.press/v205/li23c/li23c.pdf</li>
<li>(PDF) World Models in AI: A Comprehensive Overview *, https://www.researchgate.net/publication/397570106_World_Models_in_AI_A_Comprehensive_Overview</li>
<li>World Models Race 2026: How LeCun, DeepMind, and World Labs …, https://introl.com/blog/world-models-race-agi-2026</li>
<li>The World Model Rebellion: Yann LeCun Launches AMI Labs to …, https://www.humanoidsdaily.com/news/the-world-model-rebellion-yann-lecun-launches-ami-labs-to-challenge-the-llm-pilled-consensus</li>
<li>Learning to Feel the Future: DreamTacVLA for Contact-Rich … - arXiv, https://arxiv.org/abs/2512.23864</li>
<li>What Are World Models? - the physical intelligence… - Towards AI, https://pub.towardsai.net/what-are-world-models-41ff394ed871</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>