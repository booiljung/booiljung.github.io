<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:8.1.2 로봇 조작(Manipulation)과 내비게이션에서 비시각적 데이터가 갖는 결정적 역할</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>8.1.2 로봇 조작(Manipulation)과 내비게이션에서 비시각적 데이터가 갖는 결정적 역할</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 8. 시각을 넘어서: 멀티모달 센서 융합 (Beyond Vision: Multimodal Fusion)</a> / <a href="index.html">8.1 시각 정보의 한계와 체화된 감각의 필요성 (The Limits of Vision & Need for Embodied Senses)</a> / <span>8.1.2 로봇 조작(Manipulation)과 내비게이션에서 비시각적 데이터가 갖는 결정적 역할</span></nav>
                </div>
            </header>
            <article>
                <h1>8.1.2 로봇 조작(Manipulation)과 내비게이션에서 비시각적 데이터가 갖는 결정적 역할</h1>
<h2>1.  서론: 시각 중심 패러다임의 한계와 비시각 데이터의 부상</h2>
<p>현대 로봇 공학, 특히 자율 조작(Autonomous Manipulation)과 내비게이션(Navigation) 분야는 지난 수십 년간 컴퓨터 비전(Computer Vision) 기술의 비약적인 발전에 힘입어 성장해 왔다. 고해상도 RGB-D 카메라와 LiDAR(Light Detection and Ranging)를 기반으로 한 SLAM(Simultaneous Localization and Mapping) 기술은 로봇이 환경을 기하학적으로 매핑하고 자신의 위치를 추정하는 데 있어 표준적인 접근 방식으로 자리 잡았다. 그러나 이러한 ‘시각 중심(Vision-centric)’ 패러다임은 광학적 특성에 기인한 명확하고도 치명적인 물리적 한계에 봉착하고 있다. 투명한 유리나 거울과 같은 고반사 재질, 짙은 연기나 안개, 먼지가 자욱한 재난 현장, 그리고 조명이 없는 암흑이나 시야가 완전히 차단된(Occluded) 폐색 환경은 광학 센서의 데이터 신뢰도를 급격히 떨어뜨리며, 이는 곧 로봇의 자율성 실패(Autonomy Failure)로 직결된다. 시각 데이터는 ‘기하학적(Geometric)’ 정보는 제공할 수 있으나, 물체의 경도, 마찰력, 질감, 그리고 내부 구조와 같은 ‘물리적(Physical)’ 속성을 직접적으로 전달하지 못한다는 본질적인 결함을 안고 있다.</p>
<p>2024년과 2025년의 최신 연구 동향(SOTA)은 이러한 한계를 극복하기 위해 촉각(Tactile), 고유수용감각(Proprioception), 청각(Audio/Acoustic), 그리고 밀리미터파 레이다(mmWave Radar)와 같은 ‘비시각적(Non-visual)’ 데이터의 역할을 재정의하고 있다. 과거에는 이러한 데이터들이 시각 정보를 보조하는 부차적인 수단으로 여겨졌으나, 이제는 시각 정보가 무력화되는 극한 환경이나 정밀한 상호작용이 요구되는 작업에서 로봇의 생존과 임무 성공을 결정짓는 ‘결정적(Determinative)’ 요소로 격상되었다. 본 섹션에서는 로봇이 물리적 세계와 상호작용하는 데 있어 비시각 데이터가 수행하는 결정적 역할의 메커니즘과 최신 기술적 진보를 심층적으로 분석한다. 특히 미세 슬립(Incipient Slip) 감지, 블라인드 보행(Blind Locomotion), 그리고 능동적 재질 인식(Active Material Perception) 영역에서의 기술적 돌파구를 중점적으로 다룬다.</p>
<h2>2.  조작(Manipulation)에서의 촉각 및 물리적 상호작용 데이터</h2>
<p>로봇 핸드가 물체를 파지하고 조작할 때, 시각 정보는 물체의 전반적인 위치와 자세를 추정하는 거시적인 계획(Gross Motion Planning)에는 유용하지만, 접촉면(Contact Interface)에서 일어나는 미세한 물리적 현상을 포착하기에는 역부족이다. 파지 안정성을 위협하는 미끄러짐(Slip)이나, 물체의 유연성(Compliance), 표면 마찰계수와 같은 물리적 특성은 오직 접촉을 통해서만 직접적으로 측정 가능하다. 2024-2025년의 연구는 단순한 접촉 유무 감지를 넘어, 접촉면의 변형장을 미분적으로 해석하거나 능동적인 진동을 인가하여 물체의 내부 속성을 탐지하는 방향으로 진화하고 있다.</p>
<h3>2.1  시각 기반 촉각 센싱(GelSight)과 변형장(Deformation Field) 분석</h3>
<p>최근 로봇 조작 분야에서 가장 두드러진 진보는 GelSight와 같은 시각 기반 촉각 센서(Optical Tactile Sensors)의 고도화와 그 데이터 처리 알고리즘의 발전이다. 이 센서는 유연한 탄성체(Elastomer) 표면의 변형을 내부 카메라로 촬영하여 고해상도의 3D 형상과 힘의 분포를 측정하는 원리를 이용한다. 2024년과 2025년에 발표된 주요 연구들은 이 센서가 제공하는 풍부한 데이터를 활용하여, 단순한 접촉 감지를 넘어 탄성체 표면에 인쇄된 마커(Marker)들의 이동 궤적, 즉 ’변형 벡터장(Deformation Vector Field)’을 정밀 분석함으로써 미세 슬립을 사전에 예측하고 제어하는 데 집중하고 있다.</p>
<h4>2.1.1  변형장 기반의 슬립 심각도(Slip Severity) 추정 및 제어</h4>
<p>기존의 슬립 감지 기술이 ’미끄러짐 발생 여부(Slip vs. Stick)’를 이진 분류(Binary Classification)하는 수준에 머물렀다면, 최신 SOTA 기술은 ’슬립 심각도(Slip Severity)’를 연속적인 물리량(예: 슬립 속도)으로 추정하는 프레임워크로 발전했다. 와 에서 제안된 ’Learned Slip-Detection-Severity Framework’는 GelSight Mini 센서에서 추출한 마커 변형장을 분석하여 슬립의 진행 속도를 실시간으로 정량화한다.</p>
<p>이 프레임워크에서 핵심적인 역할을 하는 것은 벡터장의 발산(Divergence)과 회전(Curl), 그리고 엔트로피(Entropy)와 같은 고차원적 특징들이다.</p>
<ol>
<li>
<p><strong>발산(Divergence) 및 회전(Curl) 분석:</strong> 물체가 센서 표면에서 미끄러질 때 발생하는 전단력(Shear Force)은 마커들의 이동 패턴에 특정 방향성을 부여한다. 이를 벡터장의 발산(<span class="math math-inline">\nabla \cdot \mathbf{V}</span>)과 회전(<span class="math math-inline">\nabla \times \mathbf{V}</span>)으로 수식화하여 국소적인 비틀림(Torsional Slip)이나 확장/수축을 감지한다. 예를 들어, 물체가 회전하며 미끄러질 때는 벡터장의 Curl 성분이 급격히 증가하며, 이를 통해 단순 병진 슬립과 회전 슬립을 구분할 수 있다.</p>
</li>
<li>
<p><strong>변형장의 엔트로피(Entropy)와 무질서도:</strong> 안정적인 파지(Stable Grasp) 상태에서는 마커들의 이동이 균일하거나 정지해 있어 벡터장의 무질서도가 낮다. 반면, 미세 슬립(Incipient Slip)이 시작되면 접촉면의 일부는 고착(Stick)되어 있고 일부는 미끄러지는(Slip) 혼합 상태가 발생하며, 이는 벡터장의 무질서도, 즉 엔트로피의 급증으로 이어진다. 이를 수식으로 표현하면 섀넌 엔트로피(Shannon Entropy) 공식을 차용하여 다음과 같이 정의할 수 있다:<br />
<span class="math math-display">
H(X) = - \sum_{i=1}^{n} P(x_i) \log P(x_i)
</span><br />
여기서 <span class="math math-inline">P(x_i)</span>는 변형 벡터 크기 또는 방향의 확률 분포를 나타낸다. 연구 결과, 완전한 슬립(Gross Slip)이 발생하기 직전, 미세 슬립 단계에서 엔트로피가 급격히 상승(Spike)하는 현상이 확인되었으며, 이는 슬립을 사전에 감지하여 파지력을 조절할 수 있는 결정적인 ’전조 신호(Precursor)’로 활용된다.</p>
</li>
</ol>
<h4>2.1.2  LSTM 기반의 시계열 슬립 예측 및 외란 극복</h4>
<p>슬립은 단일 시점의 사건이 아니라 시간에 따라 발전하는 동적 과정(Dynamic Process)이다. 따라서 정적인 프레임 분석을 넘어 시계열적 패턴 학습이 필수적이다. 과 의 연구에서는 순환 신경망(RNN)의 일종인 LSTM(Long Short-Term Memory) 네트워크를 도입하여 과거의 변형장 데이터 시퀀스를 기반으로 현재 및 미래의 슬립 상태를 예측한다.</p>
<p>특히 의 연구는 농업용 로봇의 과일 수확 시나리오에서 잎사귀 간섭(Leaf Interference)과 같은 외부 외란이 존재하는 상황을 다루었다. 잎사귀가 로봇의 그리퍼나 대상 물체를 스치며 발생하는 진동이나 힘의 변화는 일반적인 센서 시스템에서 슬립으로 오인될 수 있는 노이즈이다. 그러나 3층 구조의 LSTM 네트워크를 적용한 결과, 이러한 비선형적 외란 속에서도 94% 이상의 높은 슬립 감지 정확도(F1-score)를 달성했다. 이는 시각적으로 잎사귀에 의해 물체가 가려지거나(Occlusion), 조명 조건이 수시로 변하는 비정형 야외 환경에서도 촉각 데이터만으로 안정적인 조작이 가능함을 입증한다.</p>
<h3>2.2  능동적 음향 센싱(Active Acoustic Sensing)과 VibeCheck</h3>
<p>촉각 센서가 표면의 변형을 감지한다면, 능동적 음향 센싱(Active Acoustic Sensing)은 물체 내부와 접촉면의 동적 특성을 ’청취’하는 기술이다. 2025년 발표된 ‘VibeCheck’ 시스템과 관련 연구들은 로봇 핑거에 진동을 발생시키는 발신부(Emitter)와 이를 수신하는 수신부(Receiver)를 내장하여 능동적으로 환경을 탐색하는 새로운 패러다임을 제시한다.</p>
<h4>2.2.1  주파수 응답을 통한 접촉 상태 및 재질 추정</h4>
<p>이 기술의 핵심 원리는 접촉 상태에 따른 시스템의 기계적 임피던스(Mechanical Impedance) 변화를 주파수 도메인에서 분석하는 것이다. 로봇 핑거에 내장된 압전 소자(Piezoelectric Actuator)가 특정 주파수 대역(예: 스윕 신호 또는 백색 소음)의 진동을 발생시키면, 이 진동은 핑거의 구조물과 접촉한 물체를 거쳐 수신부(마이크로폰 또는 가속도계)로 전달된다.</p>
<p>물체와 접촉하거나 파지력이 변화하면 진동의 전달 경로가 바뀌게 되며, 이는 수신된 신호의 공진 주파수(Resonant Frequency) 이동(Shift)과 진폭(Amplitude) 감쇠로 나타난다. 예를 들어, 단단한 물체를 강하게 쥘수록 시스템의 강성(Stiffness)이 증가하여 공진 주파수가 고주파 대역으로 이동하는 경향을 보인다. VibeCheck 시스템은 이러한 음향 신호를 스펙트로그램(Spectrogram)으로 변환하고, 데이터 기반 학습을 통해 접촉 위치, 파지력, 그리고 물체의 재질을 실시간으로 추정한다.</p>
<h4>2.2.2  시각 차단 환경에서의 정밀 조작(Peg Insertion)</h4>
<p>이 기술의 결정적 역할은 시각 정보가 완전히 차단된 상황에서의 정밀 조작 능력에서 드러난다. 의 연구에서는 오직 능동적 음향 피드백만을 사용하여 ‘펙 인 홀(Peg-in-Hole)’ 조립 작업을 수행하는 데 성공했다. 구멍의 위치를 시각적으로 찾을 수 없는 상황에서도, 로봇은 펙이 주변 표면을 긁거나 구멍 가장자리에 걸칠 때 발생하는 미세한 진동 패턴의 변화를 감지하여 정확한 삽입 위치를 찾아낸다. 이는 고해상도 촉각 센서에 비해 상대적으로 저렴한 하드웨어(압전 소자와 마이크로폰)만으로도 구현 가능하여 비용 효율성 측면에서도 큰 장점을 가진다.</p>
<h3>2.3  유체 기반 촉각 센서: FORTE와 고주파수 신호 분석</h3>
<p>또 다른 혁신적인 접근법은 유체(Fluid) 채널을 이용한 촉각 센싱이다. 에서 제안된 ‘FORTE’ 센서는 유연한 핑거(Compliant Finger) 내부에 공기나 유체가 흐르는 미세 채널을 배치하고, 기압 센서를 통해 채널 내부의 미세한 압력 변화를 감지한다.</p>
<p>물체와의 접촉이나 미끄러짐은 유체 채널의 기하학적 변형을 유발하며, 이는 즉각적인 기압 변화로 이어진다. 특히 슬립 발생 시 접촉면에서 발생하는 마찰 진동은 유체를 통해 고주파수 압력 신호로 전달된다. FORTE 시스템은 이 신호의 파워 스펙트럼 밀도(PSD, Power Spectral Density)를 분석하여 슬립을 감지한다. 연구 결과, FORTE는 31종의 다양한 물체에 대해 0.91의 F1-score로 슬립을 감지했으며, 특히 얇은 유리잔이나 과일처럼 깨지기 쉬운 물체를 다룰 때 로봇 핑거의 유연성(Compliance)을 해치지 않으면서도 높은 민감도를 유지한다는 점에서 기존의 딱딱한 로드셀 기반 센서와 차별화된다.</p>
<h2>3.  내비게이션(Navigation)에서의 비시각 데이터 퓨전</h2>
<p>로봇 내비게이션에서 비시각 데이터는 시각 센서가 실패하는 극한 환경(Extreme Environments)에서의 생존과 주행을 보장하는 최후의 보루이자 필수적인 감각이다.</p>
<h3>3.1  고유수용감각(Proprioception)을 이용한 블라인드 보행</h3>
<p>사족 보행 로봇(Quadruped Robot)이나 휴머노이드에게 있어, 발바닥의 접촉 감각과 관절의 상태 정보인 고유수용감각(Proprioception)은 시각 정보가 차단된 상황에서 지형을 ‘읽는’ 핵심 수단이다. 이를 ’블라인드 보행(Blind Locomotion)’이라 하며, 최신 연구인 VP-Nav , AMCO , VAPOR  등에서 그 중요성이 입증되었다.</p>
<h4>3.1.1  지형의 물리적 속성 추론과 순응성(Compliance)</h4>
<p>시각 정보만으로는 지형의 기하학적 형상(Geometry)은 파악할 수 있어도, 그 물리적 속성(Physics)은 알 수 없다. 예를 들어, 푹신한 수풀(Compressible Vegetation)과 단단한 바위(Solid Rock), 혹은 깊은 눈밭과 평평한 콘크리트 바닥은 시각적으로는 구분하기 어렵거나 오인될 수 있다. 하지만 로봇이 발을 내디뎠을 때 관절 모터에 걸리는 토크(Torque), 발의 침하 깊이, 그리고 IMU(Inertial Measurement Unit)로 감지되는 차체의 흔들림을 분석하면 지형의 ’순응성(Compliance)’과 마찰 특성을 추정할 수 있다.</p>
<ul>
<li><strong>VAPOR (Vegetation-Aware Navigation):</strong> 2024-2025년 SOTA 연구인 VAPOR 시스템은 오프라인 강화학습(Offline RL)을 통해 로봇의 고유수용감각 데이터(관절 위치, 힘, 배터리 전류 등)를 분석하여 수풀 속에 숨겨진 단단한 장애물과, 로봇이 밟고 지나갈 수 있는 유연한 풀숲을 구분한다. 이는 시각적 장애물 회피(Visual Obstacle Avoidance)가 불필요하게 높은 우회 비용을 초래하는 야외 환경에서 내비게이션 효율을 극대화한다. 로봇은 ‘발로 느껴보는’ 과정을 통해 시각적으로는 벽처럼 보이는 수풀을 통과 가능한 경로로 재인식하게 된다.</li>
<li><strong>AMCO (Adaptive Multimodal Coupling):</strong> AMCO는 시각 정보와 고유수용감각 정보를 적응적으로 결합하는 아키텍처를 제안한다. 이 시스템은 시각 정보의 신뢰도(Reliability)를 실시간으로 평가한다. 만약 조명이 어둡거나 연기가 자욱하여 시각 정보의 신뢰도가 임계값 이하로 떨어지면, 시스템은 즉시 고유수용감각 기반의 비용 지도(Proprioceptive Cost Map) 가중치를 높여 시각 정보 없이도 안전한 보행을 지속할 수 있게 한다.</li>
</ul>
<h4>3.1.2  안전 관리자(Safety Advisor) 모듈과 충돌 감지</h4>
<p>고유수용감각은 단순히 지형을 인식하는 것을 넘어, 충돌을 감지하고 예측하는 ‘안전 관리자(Safety Advisor)’ 역할을 수행한다. VP-Nav 시스템 은 충돌 감지기(Collision Detector)와 넘어짐 예측기(Fall Predictor) 모듈을 포함한다. 로봇이 투명한 유리벽이나 시야 밖의 장애물과 충돌했을 때, 비록 시각 센서는 이를 감지하지 못하더라도 관절에 가해지는 급격한 외력 변화를 통해 충돌을 즉각 인지하고 멈추거나 회피 기동을 수행한다. 실험 결과, 고유수용감각 기반의 안전 모듈을 활성화했을 때 보이지 않는 장애물이 있는 환경에서의 내비게이션 성공률이 5.7%에서 최대 15%까지 향상되었다.</p>
<h3>3.2  투명/반사 물체 및 연기 투과: 레이다와 초음파</h3>
<p>LiDAR와 카메라는 유리벽이나 거울과 같은 투명·반사 재질, 그리고 연기나 안개와 같은 부유 입자에 취약하다. 유리는 LiDAR의 레이저를 투과시키거나 굴절시켜 ’빈 공간’으로 오인하게 만들며, 거울은 허상(Ghost Object)을 만들어 로봇의 위치 추정을 교란한다. 이를 보완하기 위해 밀리미터파(mmWave) 레이다와 초음파 센서의 융합이 필수적이다.</p>
<h4>3.2.1  밀리미터파 레이다(mmWave Radar)의 강건성</h4>
<p>밀리미터파 레이다는 파장이 길어 연기, 먼지, 안개를 투과할 수 있으며, 조명 조건에 전혀 영향을 받지 않는다. 특히 60~120GHz 대역의 mmWave 센서는 유리 표면에서도 충분한 반사 신호를 검출할 수 있어 유리벽을 ’장애물’로 인식하는 데 탁월하다.</p>
<p>표 1은 극한 환경 조건에서 각 센서 모달리티의 성능을 비교 분석한 것이다. 시각 센서(Camera, LiDAR)가 실패하는 조건에서 비시각 센서(Radar, Ultrasonic, Tactile)가 어떻게 상호 보완적인 역할을 수행하는지 명확히 보여준다.</p>
<table><thead><tr><th><strong>센서 모달리티 (Sensor Modality)</strong></th><th><strong>암흑 (Darkness)</strong></th><th><strong>연기/안개 (Smoke/Fog)</strong></th><th><strong>투명 유리 (Glass)</strong></th><th><strong>거울/반사체 (Mirror)</strong></th><th><strong>폐색/질감 부족 (Occlusion/Textureless)</strong></th></tr></thead><tbody>
<tr><td><strong>RGB 카메라</strong></td><td><strong>실패 (Fail)</strong></td><td><strong>실패 (Fail)</strong></td><td>취약 (Weak)</td><td><strong>오작동 (Fail)</strong></td><td><strong>실패 (Fail)</strong></td></tr>
<tr><td><strong>LiDAR</strong></td><td>우수 (Excellent)</td><td>취약 (Weak)</td><td><strong>오작동 (Fail)</strong></td><td><strong>오작동 (Fail)</strong></td><td>우수 (Excellent)</td></tr>
<tr><td><strong>mmWave 레이다</strong></td><td><strong>우수 (Excellent)</strong></td><td><strong>우수 (Excellent)</strong></td><td><strong>감지 가능 (Good)</strong></td><td>양호 (Fair)</td><td><strong>우수 (Excellent)</strong></td></tr>
<tr><td><strong>초음파 (Ultrasonic)</strong></td><td><strong>우수 (Excellent)</strong></td><td><strong>우수 (Excellent)</strong></td><td><strong>우수 (Excellent)</strong></td><td><strong>우수 (Excellent)</strong></td><td>양호 (Fair)</td></tr>
<tr><td><strong>촉각/고유수용감각</strong></td><td><strong>우수 (Excellent)</strong></td><td><strong>우수 (Excellent)</strong></td><td><strong>직접 접촉 감지</strong></td><td><strong>직접 접촉 감지</strong></td><td><strong>직접 접촉 감지</strong></td></tr>
</tbody></table>
<p><em>표 1: 극한 환경 조건에서의 센서 모달리티별 성능 및 강건성 비교 분석. 붉은색 텍스트(Fail/Weak)는 해당 센서의 치명적 약점을, 굵은 텍스트(Excellent/Good)는 강점을 나타낸다.</em></p>
<h4>3.2.2  센서 융합 전략: 점유 격자 지도 통합</h4>
<p>최신 연구는 LiDAR의 정밀한 기하학적 정보에 mmWave 레이다와 초음파 센서의 강건한 탐지 정보를 융합하는 방식을 채택한다. 과 에서 제안된 시스템은 RGB-D 카메라와 2D LiDAR 데이터를 기반으로 작성된 점유 격자 지도(Occupancy Grid Map)에, 초음파나 레이다가 감지한 장애물 정보를 투영(Projection)하여 업데이트한다.</p>
<p>예를 들어, LiDAR가 ’빈 공간’으로 인식한 영역에서 초음파 센서가 반사파를 감지하면, 해당 좌표를 ’장애물(유리벽)’로 마킹하여 로봇이 충돌하지 않도록 경로를 수정한다. 또한의 연구에서는 전방 및 후방 LiDAR와 4개의 카메라, 그리고 초음파 센서를 통합하여 360도 전방위 감지 시스템을 구축하고, 이를 통해 Tiago 로봇이 유리벽과 거울이 많은 복잡한 실내 환경에서도 충돌 없이 주행함을 실증하였다. 이는 단일 센서의 한계를 다중 모달리티 융합(Multi-modality Fusion)으로 극복하는 모범 사례라 할 수 있다.</p>
<h2>4.  재질 인식(Material Recognition)을 위한 능동적 오디오-비전 퓨전</h2>
<p>로봇이 환경을 이해하는 데 있어 물체의 재질(Material)을 파악하는 것은 파지 전략 수립(예: 미끄러운 유리 vs 마찰력이 큰 고무)이나 주행 표면 판단(예: 빙판길 vs 아스팔트)에 매우 중요하다. 여기서 ‘충격음(Impact Sound)’ 분석은 시각 정보만으로는 구분하기 힘든 재질의 특성을 명확히 드러내는 열쇠가 된다.</p>
<h3>4.1  충격음 기반 재질 분류(Impact Sound Classification)</h3>
<p>로봇이 물체를 가볍게 두드리거나(Tapping) 표면을 긁을 때(Scratching) 발생하는 소리는 물체의 내부 밀도, 경도, 그리고 재질 고유의 공명 주파수(Resonant Frequency) 정보를 포함한다. 시각 정보는 표면의 색상이나 질감(Texture)만을 보여줄 뿐이지만, 소리는 물체의 ’속’을 반영한다. 의 연구는 로봇 매니퓰레이터가 다양한 물체를 두드려 발생하는 소리를 수집하고, 이를 스펙트로그램(Spectrogram)으로 변환하여 재질을 분류하는 전체 파이프라인을 제시했다.</p>
<h4>4.1.1  오디오 스펙트로그램 트랜스포머(AST)</h4>
<p>오디오 분류 성능을 극대화하기 위해 최신 연구는 전통적인 CNN(Convolutional Neural Network)을 넘어 트랜스포머(Transformer) 아키텍처를 적극적으로 도입하고 있다. 2024-2025년 오디오 분류 분야의 SOTA 모델인 ‘Audio Spectrogram Transformer (AST)’ 는 오디오 스펙트로그램을 이미지 패치(Patch)처럼 분할하여 처리한다.</p>
<p>AST는 어텐션 메커니즘(Attention Mechanism)을 활용하여 전체 주파수 대역과 시간 축에서 중요한 특징에 가중치를 부여한다.<br />
<span class="math math-display">
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
</span><br />
이러한 전역적(Global) 문맥 학습 능력을 통해, AST는 짧은 충격음 내에서도 재질을 특정하는 미세한 고조파(Harmonics) 패턴과 감쇠(Decay) 특성을 포착할 수 있다. 이는 금속, 나무, 플라스틱, 유리, 세라믹 등 7종 이상의 재질을 95% 이상의 정확도로 구분하게 해주며, 특히 금속으로 도장된 플라스틱과 진짜 금속처럼 시각적으로 구분이 불가능한(Visually Ambiguous) 경우에도 청각 정보를 통해 명확한 분류가 가능하다.</p>
<h3>4.2  멀티모달 퓨전(Deep Multimodal Fusion)</h3>
<p>단일 모달리티의 한계를 극복하기 위해와 은 시각 이미지와 충격음 오디오 데이터를 심층 신경망 수준에서 결합한 ‘심층 멀티모달 퓨전(Deep Multimodal Fusion)’ 네트워크를 제안했다. 이 모델은 시각 정보(CNN으로 추출한 표면 텍스처, 색상 특징)와 청각 정보(스펙트로그램에서 추출한 공명음 특징)를 특징 레벨(Feature Level)에서 결합(Concatenation)한다.</p>
<p>이러한 융합 모델은 조명이 변하거나(예: 어두운 곳, 강한 반사광), 주변 소음이 있는 환경(Noise Robustness)에서도 강건한 재질 인식을 수행한다. 실험 결과, 퓨전 모델은 단일 모달리티만 사용했을 때보다 분류 정확도가 10~20% 향상되었으며, YCB 객체 데이터셋에 대한 실험에서도 99% 이상의 높은 분류 정확도를 달성했다. 이는 로봇이 물체를 파지하기 전에 재질을 미리 파악하고, 그에 맞는 적절한 파지력(Grip Force)을 계산하여 파손이나 미끄러짐을 방지하는 데 결정적인 역할을 한다.</p>
<h2>5.  결론 및 미래 전망</h2>
<p>로봇 조작과 내비게이션에서 비시각적 데이터는 더 이상 시각 정보의 보조재가 아니다. 그것은 시각 정보가 무력화되는 물리적 현실(폐색, 투명성, 시야 차단, 극한 환경)을 극복하고, 로봇에게 ’접촉의 확신’과 ’보이지 않는 곳을 보는 능력’을 부여하는 결정적(Determinative) 요소로 자리 잡았다.</p>
<p>2024년과 2025년의 연구 성과는 하드웨어와 소프트웨어의 동반 혁신을 보여준다. 하드웨어 측면에서는 고해상도 변형장을 감지하는 <strong>GelSight</strong>, 능동적 진동 감지를 수행하는 <strong>VibeCheck</strong>, 유체 기반의 <strong>FORTE</strong>, 그리고 연기를 투과하는 <strong>mmWave 레이다</strong>와 같은 센서 기술이 비약적으로 발전했다. 소프트웨어 측면에서는 이러한 복잡한 비시각 데이터를 해석하기 위해 <strong>LSTM 기반의 슬립 예측</strong>, <strong>변형장 엔트로피 분석</strong>, <strong>오디오 스펙트로그램 트랜스포머(AST)</strong>, 그리고 <strong>오프라인 강화학습(Offline RL) 기반의 블라인드 내비게이션</strong> 알고리즘이 도입되었다.</p>
<p>향후 로봇 시스템의 발전 방향은 단일 센서의 성능 향상을 넘어, 시각과 비시각 데이터가 상호 보완적으로 결합되는 ‘적응형 멀티모달 퓨전(Adaptive Multimodal Fusion)’ 아키텍처로 수렴할 것이다. AMCO 시스템에서 보여준 것처럼, 환경의 변화에 따라 신뢰도가 높은 감각에 가중치를 동적으로 부여하는 능력은 로봇이 연구실을 벗어나 농업, 재난 구조, 심해 탐사, 그리고 가정용 서비스와 같은 예측 불가능한 비정형 환경(Unstructured Environments)에서 진정한 자율성을 확보하는 핵심 열쇠가 될 것이다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Learning-Based Slip Detection for Robotic Fruit Grasping … - MDPI, https://www.mdpi.com/1424-8220/22/15/5483</li>
<li>Mobile Robot Navigation with Enhanced 2D Mapping and Multi …, https://pubmed.ncbi.nlm.nih.gov/40285097/</li>
<li>Localisation and mapping in smoke filled environments, https://www.diva-portal.org/smash/get/diva2:1196750/FULLTEXT01.pdf</li>
<li>Learned Slip-Detection-Severity Framework using Tactile … - arXiv, https://arxiv.org/html/2411.07442v1</li>
<li>Learning to Detect Slip through Tactile Measures of the Contact …, https://scispace.com/pdf/learning-to-detect-slip-through-tactile-measures-of-the-2mmxw53y.pdf</li>
<li>[Literature Review] Learned Slip-Detection-Severity Framework …, https://www.themoonlight.io/en/review/learned-slip-detection-severity-framework-using-tactile-deformation-field-feedback-for-robotic-manipulation</li>
<li>Learning to Detect Slip through Tactile Estimation of the Contact …, https://arxiv.org/html/2303.00935v4</li>
<li>Using Active Acoustic Tactile Sensing for Contact-Rich Manipulation, https://arxiv.org/html/2504.15535v1</li>
<li>Using Active Acoustic Tactile Sensing for Contact-Rich Manipulation, https://www.researchgate.net/publication/391019746_VibeCheck_Using_Active_Acoustic_Tactile_Sensing_for_Contact-Rich_Manipulation</li>
<li>Using Active Acoustic Tactile Sensing for Contact-Rich Manipulation, https://www.researchgate.net/publication/398057053_VibeCheck_Using_Active_Acoustic_Tactile_Sensing_for_Contact-Rich_Manipulation</li>
<li>VibroTouch: Active Tactile Sensor for Contact Detection and Force …, https://pmc.ncbi.nlm.nih.gov/articles/PMC9460586/</li>
<li>FORTE: Tactile Force and Slip Sensing on Compliant Fingers … - arXiv, https://arxiv.org/html/2506.18960v2</li>
<li>Coupling Vision and Proprioception for Navigation of Legged Robots, https://openaccess.thecvf.com/content/CVPR2022W/VOCVALC/papers/Fu_Coupling_Vision_and_Proprioception_for_Navigation_of_Legged_Robots_CVPRW_2022_paper.pdf</li>
<li>Coupling Vision and Proprioception for Navigation of Legged Robots, https://openaccess.thecvf.com/content/CVPR2022/papers/Fu_Coupling_Vision_and_Proprioception_for_Navigation_of_Legged_Robots_CVPR_2022_paper.pdf</li>
<li>AMCO: Adaptive Multimodal Coupling of Vision and Proprioception …, https://arxiv.org/abs/2403.13235</li>
<li>AMCO: Adaptive Multimodal Coupling of Vision and Proprioception …, https://arxiv.org/html/2403.13235v1</li>
<li>Legged Robot Navigation in Unstructured Outdoor Environments …, https://www.researchgate.net/publication/382991181_VAPOR_Legged_Robot_Navigation_in_Unstructured_Outdoor_Environments_using_Offline_Reinforcement_Learning</li>
<li>VAPOR: Legged Robot Navigation in Unstructured Outdoor …, https://ieeexplore.ieee.org/iel8/10609961/10609862/10610132.pdf</li>
<li>Recent Advances in mmWave-Radar-Based Sensing, Its … - MDPI, https://www.mdpi.com/1424-8220/23/21/8901</li>
<li>mmWave radar sensors in robotics applications - Texas Instruments, https://www.ti.com/lit/pdf/spry311</li>
<li>Robotics Navigation: mmWave Radar vs LiDAR in 2025 - Linpowave, https://linpowave.com/blog/robotics-navigation-mmwave-vs-lidar</li>
<li>Characteristics and Differences Between LiDAR and Millimeter …, https://www.slamtec.com/en/news/detailen/characteristics-and-differences-between-lidar-and-millimeter-wave-radar</li>
<li>Comparing sensor technologies for mobile robot safety: LiDAR …, https://www.sonair.com/journal/comparing-sensor-technologies</li>
<li>Mobile Robot Navigation with Enhanced 2D Mapping and Multi …, https://pmc.ncbi.nlm.nih.gov/articles/PMC12030975/</li>
<li>Multi-sensor Fusion Glass Detection for Robot Navigation and …, https://www.semanticscholar.org/paper/Multi-sensor-Fusion-Glass-Detection-for-Robot-and-Wei-Li/3da5b12f5008658fd171808d8d3dbf0dca4c6f21</li>
<li>Mobile Robot Navigation with Enhanced 2D Mapping and Multi …, https://www.preprints.org/manuscript/202502.2212</li>
<li>Recognizing object surface material from impact sounds for robot …, http://www.iri.upc.edu/files/scidoc/2619-Recognizing-object-surface-material-from-impact-sounds-for-robot-manipulation.pdf</li>
<li>AST: Audio Spectrogram Transformer - MIT, https://sls.csail.mit.edu/archives/root/publications/2021/YuanGong_Interspeech-2021.pdf</li>
<li>[2104.01778] AST: Audio Spectrogram Transformer - arXiv, https://arxiv.org/abs/2104.01778</li>
<li>Deep Multimodal Fusion of Visual and Auditory Features for Robust …, https://univagora.ro/jour/index.php/ijccc/article/view/6457</li>
<li>(PDF) Deep Multimodal Fusion of Visual and Auditory Features for …, https://www.researchgate.net/publication/383683026_Deep_Multimodal_Fusion_of_Visual_and_Auditory_Features_for_Robust_Material_Recognition</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>