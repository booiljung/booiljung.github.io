<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:8.1.1 오클루전(Occlusion), 조명 변화, 투명 물체 등 비전 중심 인식의 실패 사례 분석</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>8.1.1 오클루전(Occlusion), 조명 변화, 투명 물체 등 비전 중심 인식의 실패 사례 분석</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 8. 시각을 넘어서: 멀티모달 센서 융합 (Beyond Vision: Multimodal Fusion)</a> / <a href="index.html">8.1 시각 정보의 한계와 체화된 감각의 필요성 (The Limits of Vision & Need for Embodied Senses)</a> / <span>8.1.1 오클루전(Occlusion), 조명 변화, 투명 물체 등 비전 중심 인식의 실패 사례 분석</span></nav>
                </div>
            </header>
            <article>
                <h1>8.1.1 오클루전(Occlusion), 조명 변화, 투명 물체 등 비전 중심 인식의 실패 사례 분석</h1>
<p>인간의 시각 시스템은 수백만 년의 진화 과정을 거치며 물리적 세계의 불확실성을 추론하고, 누락된 정보를 문맥적으로 복원하며, 다양한 광학적 조건에서도 강건한(Robust) 인식을 유지하는 능력을 획득했다. 그러나 로봇 공학, 특히 엠바디드 AI(Embodied AI)의 관점에서 시각(Vision)은 여전히 불완전하며, 특정 임계 조건에서 급격한 성능 저하를 보이는 ‘취약한’ 감각이다. 딥러닝과 컴퓨터 비전 기술이 비약적으로 발전했음에도 불구하고, 비전 중심(Vision-centric)의 인식 시스템은 3차원 공간 정보를 2차원 평면으로 투영(Projection)하는 과정에서 필연적인 정보 손실을 겪는다. 이러한 정보의 소실은 기하학적 폐색(Occlusion), 광학적 왜곡(Lighting &amp; Specularity), 그리고 물성적 투명성(Transparency)이라는 세 가지 주요 축에서 발생하며, 이는 단순한 인식 오차를 넘어 로봇 제어 시스템의 불안정성과 물리적 사고를 유발하는 근본적인 원인이 된다. 본 절에서는 최신 SOTA(State-of-the-Art) 연구와 기술적 사례를 바탕으로 비전 중심 인식이 붕괴하는 메커니즘을 심층적으로 분석하고, 이러한 실패가 로봇의 조작(Manipulation) 및 내비게이션(Navigation)에 미치는 파급 효과를 공학적 관점에서 규명한다.</p>
<h2>1.  오클루전(Occlusion): 기하학적 정보의 단절과 추론의 한계</h2>
<p>오클루전은 3차원 공간상의 객체가 시선(Line of Sight) 상의 다른 물체에 의해 가려져 센서에 투영되지 않는 현상을 의미한다. 정적 이미지 처리 분야에서 오클루전은 단순히 정보의 부재(Missing Information)로 취급되지만, 로봇 공학에서 오클루전은 동적 상태 추정(State Estimation)의 연속성을 깨뜨리고, 강화학습(RL) 에이전트의 마르코프(Markov) 가정을 위배하게 만드는 결정적인 실패 요인이다.</p>
<h3>1.1  자기 오클루전(Self-Occlusion)의 역설과 운동학적 딜레마</h3>
<p>로봇 매니퓰레이션 작업에서 가장 빈번하게 발생하면서도 해결하기 까다로운 문제는 외부 장애물이 아닌 로봇 자신의 신체에 의한 시야 차단, 즉 자기 오클루전이다. 이는 로봇이 물체와 물리적으로 상호작용하기 위해 접근할수록 시각 정보가 차단될 확률이 높아지는 역설적인 상황을 초래한다.</p>
<h4>1.1.1  아이-인-핸드(Eye-in-Hand)와 아이-투-핸드(Eye-to-Hand)의 구조적 맹점</h4>
<p>로봇 팔의 카메라 배치 전략은 크게 아이-인-핸드와 아이-투-핸드로 나뉘며, 각기 다른 형태의 오클루전 실패 모드를 가진다.</p>
<ul>
<li><strong>아이-인-핸드 시스템의 맹목적 접근(Blind Approach):</strong> 카메라가 엔드 이펙터(End-effector)에 장착된 경우, 로봇이 목표 물체에 근접할수록 해상도는 증가하지만 시야각(Field of View, FoV)은 좁아진다. 결정적으로, 파지(Grasping) 직전 단계(Pre-grasp phase)에서 그리퍼의 핑거(Finger)나 흡착 컵(Suction cup)이 목표 물체의 핵심 부위를 가리거나, 물체가 카메라의 최소 초점 거리(Minimum Focus Distance) 이내로 들어와 초점이 흐려지는 현상이 발생한다. 이는 정밀한 정렬이 필요한 조립 작업에서 ’마지막 1cm’의 제어를 불가능하게 만든다.</li>
<li><strong>아이-투-핸드 시스템의 동적 차폐:</strong> 외부 고정 카메라를 사용하는 경우, 로봇 팔이 작업 공간을 휘젓는 동안 팔뚝(Forearm)이나 엘보(Elbow)가 수시로 카메라와 작업물 사이를 가로막는다. 특히 복잡한 궤적을 그리며 움직이는 6자유도 이상의 로봇 팔은 예측하기 어려운 패턴으로 시야를 차단하며, 이는 시각적 서보잉(Visual Servoing) 루프를 끊어지게 한다.</li>
</ul>
<h4>1.1.2  강화학습 및 파지 정책(Grasping Policy)의 붕괴</h4>
<p>자기 오클루전은 데이터 기반 학습 알고리즘의 성능을 저하시키는 주요 원인이다.</p>
<ul>
<li><strong>부분 관찰성(Partial Observability):</strong> 시각적 관찰 <span class="math math-inline">o_t</span>가 로봇의 링크에 의해 가려질 경우, 에이전트는 불완전한 정보에 의존하여 행동 <span class="math math-inline">a_t</span>를 결정해야 한다. 이는 문제를 완전 관찰 가능한 MDP(Markov Decision Process)에서 부분 관찰 가능한 POMDP(Partially Observable MDP)로 격하시키며, 정책 네트워크의 수렴을 방해한다.</li>
<li><strong>푸시-그라스프(Push-Grasp) 시너지의 실패:</strong> 클러터(Clutter) 환경에서 물체를 밀어서(Push) 공간을 확보하고 잡는(Grasp) 전략을 학습할 때, 로봇 팔이 미는 동작을 수행하는 동안 시야를 가리면, 그 결과로 변화된 물체의 상태(새로운 위치 및 자세)를 즉각적으로 업데이트할 수 없다. 연구에 따르면, 이러한 시각적 피드백의 지연이나 소실은 로봇이 이미 밀려난 물체의 이전 위치를 향해 그리퍼를 닫게 만드는 헛손질(Air Grasp)의 주원인이 된다.</li>
</ul>
<h3>1.2  동적 오클루전(Dynamic Occlusion)과 군집 환경의 난제</h3>
<p>정적인 장애물에 의한 가림과 달리, 움직이는 객체에 의한 동적 오클루전은 시간적인 정보의 불연속성을 유발하며, 이는 로봇의 지도 작성(Mapping)과 위치 인식(Localization) 시스템을 교란한다.</p>
<h4>1.2.1  V-SLAM의 특징점 추적 실패 및 지도 오염</h4>
<p>V-SLAM(Visual Simultaneous Localization and Mapping) 시스템은 환경의 정적 랜드마크를 추적하여 로봇의 포즈를 추정한다. 그러나 보행자나 다른 로봇과 같은 동적 객체가 카메라 앞을 지나갈 때 다음과 같은 실패가 발생한다.</p>
<ul>
<li><strong>특징점 매칭의 단절:</strong> 연속된 프레임 간의 특징점(Keypoint) 추적이 동적 객체에 의해 끊기면서, 카메라 포즈 추정이 발산(Drift)하거나 추적 손실(Tracking Loss) 상태에 빠진다. 이는 로봇이 자신의 위치를 잃어버리는 ‘납치된 로봇(Kidnapped Robot)’ 문제를 야기할 수 있다.</li>
<li><strong>고스트 장애물(Ghost Obstacle):</strong> 움직이는 물체가 지도 상에 정적 장애물로 잘못 등록되는 경우이다. 예를 들어, 잠시 멈춰 섰던 사람이 지도에 벽처럼 기록되면, 이후 로봇은 그 공간이 비어있음에도 불구하고 우회 경로를 생성하게 된다. 이는 내비게이션의 효율성을 떨어뜨리고 불필요한 회피 기동을 유발한다.</li>
</ul>
<h4>1.2.2  상호 오클루전(Mutual Occlusion)과 인스턴스 분할 오류</h4>
<p>빈 피킹(Bin Picking)과 같이 다수의 물체가 무질서하게 쌓여 있는 환경에서는 물체끼리 서로를 가리는 상호 오클루전이 극심하다.</p>
<ul>
<li><strong>분할(Segmentation)의 모호성:</strong> 딥러닝 기반의 인스턴스 분할 네트워크(예: Mask R-CNN)는 겹쳐진 동일 물체들을 구분하는 데 어려움을 겪는다. 가려진 물체의 경계를 정확히 추론하지 못해 두 물체를 하나로 인식하거나(Under-segmentation), 하나의 물체를 둘로 나누는(Over-segmentation) 오류가 발생한다.</li>
<li><strong>6D 포즈 추정의 불확실성:</strong> PoseCNN이나 DenseFusion과 같은 6D 포즈 추정 알고리즘은 물체의 전체 형상을 기반으로 자세를 예측한다. 그러나 물체의 주요 기하학적 특징(예: 머그컵의 손잡이)이 가려져 있는 경우, 알고리즘은 대칭성을 잘못 가정하거나 180도 회전된 포즈로 오인식할 수 있다. 이는 로봇이 물체를 잡을 때 엉뚱한 방향으로 접근하게 하여 충돌을 유발한다.</li>
</ul>
<h3>1.3  최신 기술적 대응과 그 한계</h3>
<p>오클루전 문제를 극복하기 위해 다양한 기술이 시도되고 있으나, 완벽한 해결책은 요원하다.</p>
<ul>
<li><strong>아모달 완성(Amodal Completion)의 한계:</strong> 가려진 영역을 딥러닝으로 상상하여 채워 넣는 아모달 인식 기술은 학습 데이터 분포 내의 정형화된 물체에 대해서는 효과적이나, 비정형 물체나 복잡한 겹침 상황에서는 환각(Hallucination)에 가까운 잘못된 형상을 생성할 위험이 있다.</li>
<li><strong>능동 비전(Active Vision):</strong> 고정된 시점의 한계를 극복하기 위해 로봇이 카메라를 이동시켜 가려진 영역을 능동적으로 관찰하는 전략이다. 최근의 연구(예: EFM-10 벤치마크, BAP 전략)는 팔에 장착된 카메라를 움직여 최적의 시점을 찾는 정책을 학습하지만, 이는 탐색 시간을 증가시키고 실시간 제어의 복잡성을 높인다.</li>
<li><strong>NeRF 기반 재구성:</strong> Neural Radiance Fields(NeRF)나 Gaussian Splatting을 이용해 희소한 시점(Sparse View)에서도 3D 장면을 복원하려는 시도가 있다. 그러나 학습되지 않은 새로운 시점이나 데이터가 부족한 가려진 영역에서는 ’플로터(Floater)’라 불리는 부유하는 아티팩트가 생성되어 물리적 상호작용을 방해한다.</li>
</ul>
<h2>2.  조명 변화(Lighting Variation): 광도 불변성(Photometric Invariance)의 붕괴</h2>
<p>컴퓨터 비전 알고리즘의 근간이 되는 ‘밝기 불변성(Brightness Constancy)’ 가정은 실제 로봇 환경에서 빈번하게 위배된다. 조명 조건의 급격한 변화, 과도한 역광, 혹은 극단적인 저조도는 이미지 센서의 물리적 한계를 초과하며, 이는 픽셀 수준의 정보 소실을 넘어 상위 인지 기능의 마비를 초래한다.</p>
<h3>2.1  센서의 동적 범위(Dynamic Range) 초과와 정보 소실</h3>
<p>로봇이 실내에서 실외로 이동하거나, 창가에서 작업할 때 겪는 조도 차이는 일반적인 이미지 센서(CMOS/CCD)의 동적 범위를 넘어선다.</p>
<h4>2.1.1  과노출(Overexposure)과 저노출(Underexposure)의 물리학</h4>
<ul>
<li><strong>포화(Saturation)와 클리핑(Clipping):</strong> 강한 광원이나 반사광이 유입되는 영역에서는 센서의 포토다이오드가 전하 용량(Full Well Capacity)을 초과하여 포화 상태가 된다. 이로 인해 픽셀 값은 최댓값(예: 255)으로 고정(Clipping)되며, 해당 영역의 텍스처와 색상 정보는 영구적으로 소실된다. 또한, 인접 픽셀로 전하가 넘쳐흐르는 블루밍(Blooming) 현상은 광원 주변의 이미지를 왜곡시킨다.</li>
<li><strong>저노출과 노이즈 우세:</strong> 어두운 영역에서는 입사 광자가 부족하여 신호 대 잡음비(SNR)가 급격히 떨어진다. 샷 노이즈(Shot Noise)와 리드 노이즈(Read Noise)가 실제 신호를 덮어버리며, 이는 엣지 검출기나 특징점 추출기가 노이즈를 실제 텍스처로 오인하게 만든다.</li>
</ul>
<h4>2.1.2  CNN 특징맵(Feature Map)의 활성화 소실</h4>
<p>딥러닝 모델, 특히 합성곱 신경망(CNN)은 적절한 노출의 이미지 데이터로 학습되는 경우가 많다. 조명 조건이 극단적으로 변화하면 입력 데이터의 분포가 학습 데이터와 달라지는 <strong>도메인 시프트(Domain Shift)</strong> 가 발생한다.</p>
<ul>
<li><strong>활성화 희소성(Activation Sparsity) 변화:</strong> 과노출된 이미지에서는 텍스처가 사라져 CNN의 초기 레이어(Low-level filters)가 반응하지 못한다. 연구 분석에 따르면, 과노출 시 특징맵의 유효 활성화(Effective Activation) 빈도가 급감하며, 이는 상위 레이어에서의 객체 검출 실패로 이어진다. 예를 들어, 마스크 R-CNN은 과노출된 영역에서 객체의 경계를 찾지 못하거나 신뢰도 점수(Confidence Score)가 임계값 이하로 떨어져 객체를 놓치게(False Negative) 된다.</li>
</ul>
<h3>2.2  V-SLAM과 시각적 오도메트리의 표류</h3>
<p>조명 변화는 자율 주행 로봇의 위치 인식(Localization)에 치명적이다. 대부분의 V-SLAM 알고리즘(ORB-SLAM, DSO 등)은 연속된 이미지 간의 픽셀 밝기 차이를 기반으로 움직임을 추정하기 때문이다.</p>
<h4>2.2.1  광류(Optical Flow) 오추정</h4>
<p>급격한 조명 변화(예: 터널 진입, 조명 스위치 조작)는 전체 이미지의 픽셀 강도를 변화시킨다. 광류 알고리즘은 이를 카메라나 물체의 움직임으로 잘못 해석한다. <span class="math math-inline">I(x, y, t) \approx I(x+dx, y+dy, t+dt)</span>라는 기본 가정이 깨지면서, 로봇이 정지해 있음에도 움직이는 것으로 판단하거나, 실제 이동 방향과 다른 방향으로 오도메트리 값이 튀는 현상이 발생한다.</p>
<h4>2.2.2  루프 결합(Loop Closure)의 실패와 지도 일관성 붕괴</h4>
<p>로봇이 과거에 방문했던 장소를 다시 방문했을 때 조명 조건이 달라져 있으면(예: 낮과 밤), 시각적 장소 인식(Visual Place Recognition) 모듈이 동일 장소임을 인지하지 못한다. SIFT나 ORB와 같은 기술자(Descriptor)들이 어느 정도 조명 불변성을 가지도록 설계되었음에도 불구하고, 그림자의 위치 변화나 전반적인 대비(Contrast) 변화는 특징 벡터 간의 거리를 멀어지게 하여 매칭을 실패하게 만든다. 루프 결합 실패는 누적된 위치 오차(Drift)를 보정할 기회를 날려버리며, 전역 지도(Global Map)의 일관성을 파괴한다.</p>
<h3>2.3  인공 조명의 플리커(Flicker)와 롤링 셔터 왜곡</h3>
<p>형광등이나 LED 조명은 교류 전원 주파수에 맞춰 빠르게 깜빡인다(Flicker). 인간의 눈은 이를 통합하여 인식하지만, 짧은 노출 시간을 가지는 로봇 카메라는 이 깜빡임을 포착한다. 특히 롤링 셔터(Rolling Shutter) 방식의 CMOS 센서는 이미지의 각 행(Row)을 순차적으로 노광하기 때문에, 플리커 현상이 이미지 상에 밝고 어두운 가로줄 무늬(Banding) 형태로 나타난다.</p>
<ul>
<li><strong>텍스처 왜곡:</strong> 이러한 줄무늬는 CNN에 의해 가로 방향의 엣지 텍스처로 오인될 수 있다.</li>
<li><strong>제어 불안정:</strong> 시각 서보잉 시스템은 조명의 깜빡임을 물체의 진동으로 잘못 해석하여, 로봇 팔을 불필요하게 진동시키거나 제어 이득(Gain)을 발산시킬 위험이 있다.</li>
</ul>
<h2>3.  투명 물체(Transparent Objects)와 반사체(Specular Surfaces): 비전의 물리학적 한계</h2>
<p>유리, 투명 플라스틱, 거울, 금속과 같은 소재는 현대 로봇 비전 시스템에 있어 가장 까다로운 ‘적대적(Adversarial)’ 대상이다. 이들은 빛의 직진성과 람베르트 반사(Lambertian Reflection) 가정을 정면으로 위배하며, 깊이 센서의 측정 원리 자체를 무력화시킨다.</p>
<h3>3.1  깊이 센서(Depth Sensor)의 오작동 메커니즘</h3>
<p>로봇 인식의 핵심 센서인 RGB-D 카메라와 LiDAR는 투명하거나 반사율이 높은 물체 앞에서 심각한 측정 오류를 일으킨다.</p>
<h4>3.1.1  ToF(Time-of-Flight) 센서와 다중 경로 간섭(Multipath Interference, MPI)</h4>
<p>ToF 센서는 방출된 빛이 물체에 맞고 돌아오는 시간을 위상차(Phase Shift) 등을 통해 측정한다. 그러나 투명 물체 내부나 난반사가 심한 환경에서는 빛이 단일 경로가 아닌 여러 경로를 통해 센서 픽셀로 돌아오는 <strong>다중 경로 간섭(MPI)</strong> 현상이 발생한다.</p>
<ul>
<li><strong>페이저(Phasor) 합산 오류:</strong> 수신된 신호는 직진 경로와 반사 경로 등 다양한 경로를 거친 빛의 페이저 합으로 나타난다. <span class="math math-inline">S_{total} = \sum A_i e^{j\phi_i}</span>. 이 합산된 신호의 위상은 실제 물체의 거리와는 무관한 제3의 값을 가지게 된다.</li>
<li><strong>현상:</strong> 유리컵의 앞면이 아닌 뒷면이나 배경 벽의 깊이가 측정되거나(투과), 센서와 물체 사이 허공에 점군(Point Cloud)이 맺히는 ‘유령 점(Ghost Point)’ 현상이 발생한다. 이는 로봇이 물체를 잡으려 할 때 허공을 잡거나, 물체를 뚫고 지나가려다 충돌하는 사고를 유발한다.</li>
</ul>
<h4>3.1.2  스테레오 비전(Stereo Vision)과 텍스처 부재</h4>
<p>스테레오 카메라는 좌우 이미지 간의 시차(Disparity)를 계산하기 위해 대응점(Correspondence)을 매칭해야 한다. 그러나 투명한 유리나 매끄러운 금속 표면은 텍스처가 거의 없어(Textureless) 매칭이 불가능하다.</p>
<ul>
<li><strong>깊이 소실(Missing Depth):</strong> 매칭 실패로 인해 해당 영역의 깊이 값이 계산되지 않고 구멍(Hole)으로 남는다.</li>
<li><strong>굴절에 의한 왜곡:</strong> 투명 물체 뒤로 보이는 배경의 텍스처는 굴절에 의해 위치가 왜곡된다(스넬의 법칙). 스테레오 알고리즘은 이 왜곡된 배경 텍스처를 매칭하여 깊이를 계산하므로, 물체 표면이 아닌 물체 뒤쪽의 엉뚱한 거리를 출력하게 된다.</li>
</ul>
<h4>3.1.3  LiDAR와 거울 반사(Specular Reflection)</h4>
<p>LiDAR의 레이저 빔은 거울이나 유리창과 같은 정반사 표면에서 센서로 되돌아오지 않고 다른 방향으로 튕겨 나간다.</p>
<ul>
<li><strong>탐지 실패(False Negative):</strong> 반사광이 수신되지 않아 장애물이 없는 것으로 인식된다.</li>
<li><strong>가상 장애물 생성:</strong> 튕겨 나간 빔이 다른 물체에 맞고 돌아오면, LiDAR는 전체 이동 경로를 거리로 환산하여 벽 너머나 지하 등 존재하지 않는 위치에 장애물이 있는 것으로 인식한다. 이는 자율 주행 로봇의 경로 계획을 엉망으로 만든다.</li>
</ul>
<h3>3.2  투명 물체 파지(Grasping)의 난제와 최신 연구의 한계</h3>
<p>최근 딥러닝을 이용해 투명 물체의 깊이를 복원하려는 연구들(ClearGrasp, TransCG, ClearDepth)이 등장했다. 이들은 표면 법선(Surface Normal)을 추정하거나 깊이 완성을 수행하지만 여전히 한계가 있다.</p>
<ul>
<li><strong>경계 인식 실패와 Sim-to-Real 갭:</strong> 대부분의 모델이 합성 데이터(Synthetic Data)로 학습되는데, 실제 환경의 복잡한 굴절과 반사 특성을 완벽히 모사하지 못한다. 특히 투명 물체의 가장자리(Rim)는 굴절이 가장 심하게 일어나는 곳으로, 딥러닝 모델조차 이 경계를 정확히 분할하지 못해 그리퍼가 미끄러지는 사고가 빈번하다.</li>
<li><strong>팬텀 아티팩트(Phantom Artifacts):</strong> 깊이 복원 과정에서 실제 존재하는 얇은 구조물(예: 와인잔의 다리)을 노이즈로 간주해 지워버리거나, 반대로 빛 반사를 물체로 오인해 허상의 깊이를 생성하는 문제가 발생한다.</li>
<li><strong>SOTA 기술의 시도:</strong> 최근에는 편광(Polarization) 카메라나 열화상(Thermal) 카메라를 이용해 투명 물체의 특징을 잡아내거나 , 레이더(Radar)와 비전을 융합하여(FuseGrasp) 재질을 식별하고 깊이를 보정하려는 시도 , 그리고 마스크된 깊이 모델링(Masked Depth Modeling)을 통해 누락된 깊이를 추론하는 기법  등이 연구되고 있다. 하지만 이러한 이종 센서 융합은 하드웨어 복잡도를 높이고 데이터 동기화 문제를 수반한다.</li>
</ul>
<h2>4.  비전 중심 인식의 본질적 불확실성과 파급 효과</h2>
<p>상기한 실패 사례들은 단순한 기술적 버그가 아니라, 3차원 세상을 2차원 광학 정보로 압축하는 과정에서 발생하는 본질적인 <strong>‘역문제(Inverse Problem)’</strong> 의 난이도를 보여준다.</p>
<h3>4.1  제어 안전성(Safety)에 미치는 영향</h3>
<p>불확실한 시각 정보는 로봇 제어의 안전성을 직접적으로 위협한다.</p>
<ul>
<li><strong>충돌 확률(Collision Probability) 과소평가:</strong> 투명 장애물을 보지 못하거나 오클루전 영역을 빈 공간(Free Space)으로 가정하는 경로 계획 알고리즘은 필연적으로 충돌을 유발한다.</li>
<li><strong>제어 장벽 함수(CBF)의 무력화:</strong> 로봇의 동작 범위를 안전하게 제한하는 제어 장벽 함수(Control Barrier Function)는 정확한 상태 추정(State Estimation)을 전제로 한다. 비전 센서의 오류로 상태 <span class="math math-inline">x</span>의 불확실성이 커지면, 안전 조건 <span class="math math-inline">h(x) \geq 0</span>을 판단할 수 없게 되어 안전 제어기가 작동하지 않거나 오작동한다.</li>
</ul>
<h3>4.2  결론: 다중 감각 융합의 필연성</h3>
<p>결론적으로, 시각 정보만으로는 복잡하고 비정형적인 현실 세계를 온전히 이해하고 조작하는 데 명백한 한계가 존재한다. 오클루전 뒤의 물체를 확인하기 위해 시점을 변경해야 하고(능동 비전), 투명한 유리의 실체를 파악하기 위해 직접 만져봐야 하며(촉각), 어두운 곳에서는 청각과 같은 다른 감각에 의존해야 한다. 이러한 비전 중심 인식의 실패 분석은 <strong>‘체화된 지능(Embodied Intelligence)’</strong> 이 왜 단일 모달리티를 넘어 다중 감각 융합(Multimodal Fusion)으로 나아가야 하는지에 대한 강력한 당위성을 제공한다. 이는 다음 절에서 다룰 비시각적 데이터의 결정적 역할에 대한 논리적 토대가 된다.</p>
<table><thead><tr><th><strong>실패 유형</strong></th><th><strong>주요 원인 메커니즘</strong></th><th><strong>로봇 작업에 미치는 구체적 영향</strong></th><th><strong>SOTA 대응 기술 및 한계</strong></th></tr></thead><tbody>
<tr><td><strong>자기 오클루전</strong></td><td>Eye-in-Hand/Eye-to-Hand 구조적 시야 차단</td><td>파지 직전 정밀도 저하, 헛손질(Air Grasp)</td><td>강화학습(RL) 기반 회피, 촉각 융합 / 여전히 POMDP 문제 상존</td></tr>
<tr><td><strong>동적 오클루전</strong></td><td>이동 객체에 의한 특징점 추적 단절</td><td>V-SLAM 표류(Drift), 고스트 장애물 생성</td><td>동적 객체 마스킹(Masking), 평생 학습(Lifelong Learning) / 완전한 동적 객체 분리는 난제</td></tr>
<tr><td><strong>과도한 조명/역광</strong></td><td>센서 포화(Saturation), 블루밍</td><td>CNN 특징맵 활성화 소실, 객체 검출 실패</td><td>HDR 센서, 도메인 적응(Domain Adaptation) / 극단적 조도차에선 여전히 정보 소실</td></tr>
<tr><td><strong>투명/반사 물체</strong></td><td>다중 경로 간섭(MPI), 굴절, 정반사</td><td>깊이 구멍(Hole), 깊이 왜곡, 유령 점 생성</td><td>ClearDepth, TransCG, 편광 카메라 / Sim-to-Real 갭, 얇은 구조물 인식 실패</td></tr>
</tbody></table>
<h2>5. 참고 자료</h2>
<ol>
<li>Vision-Based Navigation and Perception for Autonomous Robots: Sensors, SLAM, Control Strategies, and Cross-Domain Applications—A Review - MDPI, https://www.mdpi.com/2673-4117/6/7/153</li>
<li>Multi-Modal Perception with Vision, Language, and Touch for Robot Manipulation - EECS at Berkeley, https://www2.eecs.berkeley.edu/Pubs/TechRpts/2025/EECS-2025-68.pdf</li>
<li>Learning Occlusions in Robotic Systems: How to Prevent Robots from Hiding Themselves, https://www.researchgate.net/publication/378691989_Learning_Occlusions_in_Robotic_Systems_How_to_Prevent_Robots_from_Hiding_Themselves</li>
<li>Methods for Simultaneous Robot-World-Hand–Eye Calibration: A Comparative Study - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC6631330/</li>
<li>[Q] Eye-in-hand vs Eye-to-hand setting : r/robotics - Reddit, https://www.reddit.com/r/robotics/comments/eq09rm/q_eyeinhand_vs_eyetohand_setting/</li>
<li>Review of Learning-Based Robotic Manipulation in Cluttered Environments - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC9607868/</li>
<li>Deep Reinforcement Learning-Based Robotic Grasping in Clutter and Occlusion - MDPI, https://www.mdpi.com/2071-1050/13/24/13686</li>
<li>STARS - Deep Diversified Analysis of Occlusion-Resiliency for Robot Vision in Medical Contexts - IEEE Xplore, https://ieeexplore.ieee.org/iel8/6287639/6514899/10918948.pdf</li>
<li>A Review of Research on SLAM Technology Based on the Fusion of LiDAR and Vision, https://www.mdpi.com/1424-8220/25/5/1447</li>
<li>Understanding why SLAM algorithms fail in modern indoor environments - ResearchGate, https://www.researchgate.net/publication/370633445_Understanding_why_SLAM_algorithms_fail_in_modern_indoor_environments</li>
<li>Self-Correcting Robot Manipulation via Gaussian-Splatted Foresight - AAAI Publications, https://ojs.aaai.org/index.php/AAAI/article/view/34866/37021</li>
<li>Occlusion-Aware 3D Hand-Object Pose Estimation with Masked AutoEncoders - arXiv, https://arxiv.org/html/2506.10816v1</li>
<li>Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy - arXiv, https://arxiv.org/html/2602.01939v1</li>
<li>Exploring Active Vision in Bimanual Robotic Manipulation - arXiv, https://arxiv.org/pdf/2409.17435</li>
<li>Active Vision Might Be All You Need: Exploring Active Vision in Bimanual Robotic Manipulation | Request PDF - ResearchGate, https://www.researchgate.net/publication/395225073_Active_Vision_Might_Be_All_You_Need_Exploring_Active_Vision_in_Bimanual_Robotic_Manipulation</li>
<li>Lighting Challenges in Robotic Machine Vision Applications - Remtec Automation, https://www.remtecautomation.com/news/lighting-challenges-in-robotic-machine-vision-applications/</li>
<li>CNN Based Image Restoration: Adjusting Ill-Exposed sRGB Images in Post-Processing | Request PDF - ResearchGate, https://www.researchgate.net/publication/338535201_CNN_Based_Image_Restoration_Adjusting_Ill-Exposed_sRGB_Images_in_Post-Processing</li>
<li>Two Residual Attention Convolution Models to Recover Underexposed and Overexposed Images - MDPI, https://www.mdpi.com/2073-8994/15/10/1850</li>
<li>Light-resistant target detection improvement algorithm for overexposed environments - Oxford Academic, https://academic.oup.com/tse/advance-article-pdf/doi/10.1093/tse/tdaf011/61870727/tdaf011.pdf</li>
<li>Object Detection with Deep Convolutional Neural Networks in Images with Various Lighting Conditions and Limited Resolution - Diva-portal.org, https://www.diva-portal.org/smash/get/diva2:1586980/FULLTEXT01.pdf</li>
<li>Feature Point Tracking Method for Visual SLAM Based on Multi-Condition Constraints in Light Changing Environment - MDPI, https://www.mdpi.com/2076-3417/13/12/7027</li>
<li>Solution to the SLAM Problem in Low Dynamic Environments Using a Pose Graph and an RGB-D Sensor - MDPI, https://www.mdpi.com/1424-8220/14/7/12467</li>
<li>Active Illumination for Visual Ego-Motion Estimation in the Dark - arXiv, https://arxiv.org/html/2502.13708v1</li>
<li>Robotic Perception of Transparent Objects: A Review - arXiv, https://arxiv.org/pdf/2304.00157</li>
<li>Learning to Remove Multipath Distortions in Time-of-Flight Range Images for a Robotic Arm Setup, https://merl.com/publications/docs/TR2016-036.pdf</li>
<li>SPUMIC: SIMULTANEOUS PHASE UNWRAPPING AND MULTIPATH INTERFERENCE CANCELLATION IN TIME-OF-FLIGHT CAMERAS USING SPECTRAL METHODS A - Microsoft, https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/06607553.pdf</li>
<li>Polarization-based approach for multipath interference mitigation in time-of-flight imaging, https://opg.optica.org/ao/upcoming_pdf.cfm?id=461954</li>
<li>ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation, https://arxiv.org/html/2409.08926v1</li>
<li>Detecting and Mapping Transparent or Mirror-like Surfaces with Lidar - MIT Media Lab, https://www.media.mit.edu/projects/detecting-and-mapping-transparent-or-mirror-like-surfaces-with-lidar/overview/</li>
<li>Seeing is Deceiving: Mirror-Based LiDAR Spoofing for Autonomous Vehicle Deception - arXiv, https://arxiv.org/html/2509.17253v1</li>
<li>FuseGrasp: Radar-Camera Fusion for Robotic Grasping of Transparent Objects - arXiv, https://arxiv.org/html/2502.20037v1</li>
<li>Vision-based manipulation of transparent plastic bags in industrial setups - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1506290/full</li>
<li>Robots recognize transparent and reflective objects: progress through ‘goROBOT3D’ system - All-About-Industries, https://www.all-about-industries.com/robots-detect-transparent-reflective-objects-gorobot3d-a-2707dbd0ab7556d3897d3ed2e463a206/</li>
<li>Robbyant’s LingBot-Depth AI Cuts Depth Error By 70% For Robotics - Quantum Zeitgeist, https://quantumzeitgeist.com/robbyant-lingbot-depth-ai-robotics-depth-sensing/</li>
<li>Estimating Probability of Collision for Safe Motion Planning under Gaussian Motion and Sensing Uncertainty, https://robotics.cs.unc.edu/publications/Patil2012_ICRA.pdf</li>
<li>Percieve With Confidence: Statistical Safety Assurances for Navigation with Learning-Based Perception - arXiv, https://arxiv.org/html/2403.08185v3</li>
<li>Guaranteeing Safety of Learned Perception Modules via Measurement-Robust Control Barrier Functions, https://proceedings.mlr.press/v155/dean21a/dean21a.pdf</li>
<li>Self-Supervised Online Learning for Safety-Critical Control using Stereo Vision - Aaron Ames - Caltech, http://ames.caltech.edu/cosner2022self.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>