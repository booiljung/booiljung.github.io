<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Chapter 8. 시각을 넘어서: 멀티모달 센서 융합 (Beyond Vision: Multimodal Fusion)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Chapter 8. 시각을 넘어서: 멀티모달 센서 융합 (Beyond Vision: Multimodal Fusion)</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">제목: Embodied AI & Modern Control</a> / <a href="index.html">Chapter 8. 시각을 넘어서: 멀티모달 센서 융합 (Beyond Vision: Multimodal Fusion)</a> / <span>Chapter 8. 시각을 넘어서: 멀티모달 센서 융합 (Beyond Vision: Multimodal Fusion)</span></nav>
                </div>
            </header>
            <article>
                <h1>Chapter 8. 시각을 넘어서: 멀티모달 센서 융합 (Beyond Vision: Multimodal Fusion)</h1>
<h2>1.  서론: 시각 중심주의의 한계와 감각 확장의 필연성</h2>
<h3>1.1  로봇 인지의 진화와 시각의 독점</h3>
<p>지난 수십 년간 로봇 공학의 인지(Perception) 시스템은 컴퓨터 비전(Computer Vision) 기술의 비약적인 발전에 힘입어 놀라운 성과를 이룩해 왔다. 초기 로봇이 단순히 색상이나 테두리를 감지하는 수준에 머물렀다면, 현대의 로봇은 심층 신경망(Deep Neural Networks, DNN)과 합성곱 신경망(CNN), 그리고 최근의 비전 트랜스포머(Vision Transformer, ViT)를 통해 객체를 분류하고, 분할(Segmentation)하며, 3차원 공간을 재구성하는 능력을 갖추게 되었다. 이러한 시각 중심의 접근 방식은 자율 주행 자동차가 도로 상황을 파악하거나, 물류 로봇이 정해진 규격의 상자를 분류하는 데 있어 핵심적인 역할을 수행해 왔다. 그러나 인간이 세상을 이해하고 상호작용하는 방식은 결코 시각이라는 단일 감각에 국한되지 않는다. 인간은 눈을 감고도 손끝의 감각만으로 주머니 속의 동전을 구별하고, 방문을 두드리는 소리만으로 그 재질이 나무인지 금속인지를 파악하며, 컵을 들어 올릴 때 느껴지는 미세한 진동을 통해 내용물의 유무를 직관적으로 인지한다.</p>
<p>로봇이 통제된 실험실 환경을 벗어나 비정형적이고 불확실성이 가득한 현실 세계(In-the-wild)로 진입함에 따라, 시각 정보 하나에만 의존하는 기존 방식은 근본적인 한계에 봉착하고 있다. 특히 로봇이 환경과 직접 물리적으로 접촉하고 힘을 가해야 하는 조작(Manipulation) 작업에 있어 시각 정보의 불완전성은 치명적인 오류를 야기한다. 본 장에서는 이러한 시각 중심주의가 마주한 구체적인 기술적 난제들을 분석하고, 이를 극복하기 위해 촉각(Tactile), 청각(Audio), 열화상(Thermal), LiDAR 등 이종 센서 데이터를 융합하는 최신 멀티모달(Multimodal) 기술의 이론과 실제를 심도 있게 다룬다.</p>
<h3>1.2  광학적 불확실성: 람베르트 반사 가정의 붕괴</h3>
<p>전통적인 컴퓨터 비전 알고리즘, 특히 깊이 추정(Depth Estimation)이나 스테레오 매칭(Stereo Matching) 기술은 대부분 물체 표면이 람베르트 반사(Lambertian Reflectance) 특성을 가진다고 가정한다. 이는 물체 표면의 밝기가 관찰자의 시야각과 무관하게 균일하게 보인다는 이상적인 전제이다. 그러나 우리가 일상에서 마주하는 수많은 물체들, 예컨대 유리잔, 플라스틱 용기, 금속 도구, 그리고 액체 등은 빛을 투과시키거나(Transparency) 거울처럼 정반사(Specular Reflection)시키는 광학적 특성을 지닌다. 이러한 물체들은 람베르트 가정을 정면으로 위배하며, 시각 센서에 심각한 노이즈를 유발한다.</p>
<p>투명한 와인잔이나 유리 그릇은 깊이 센서(Depth Sensor) 데이터에서 물체의 존재 자체가 사라지거나, 배경의 깊이 값이 대신 측정되는 ‘구멍(Hole)’ 현상을 발생시킨다. 적외선(IR) 패턴을 투사하여 거리를 측정하는 구조광(Structured Light) 센서나 ToF(Time-of-Flight) 센서의 경우, 투명한 재질을 통과하거나 표면에서 난반사되는 IR 신호로 인해 거리 측정값이 왜곡되거나 데이터가 소실되는 경우가 빈번하다. 연구 결과에 따르면, 이러한 광학적 노이즈는 로봇이 가정용 식기세척기에서 유리컵을 꺼내거나 실험실에서 투명한 비커를 조작하는 것과 같은 기본적인 작업의 실패율을 급격히 증가시킨다. 비록 딥러닝 기반의 투명 물체 깊이 복원 연구가 진행되고 있으나, 조명 조건이 시시각각 변하는 실제 환경에서 시각 정보만으로 완벽한 형상을 복원하는 것은 여전히 난제로 남아 있다. 이는 물체의 존재와 위치를 물리적으로 확증할 수 있는 접촉 기반 센서와의 융합이 필수적인 첫 번째 이유이다.</p>
<h3>1.3  기하학적 불확실성: 폐색(Occlusion)과 시점의 한계</h3>
<p>로봇 팔이 물체를 잡으려 접근하는(Reaching) 순간, 역설적으로 로봇의 손(End-effector) 자체가 카메라의 시야를 가리는 ‘셀프 오클루전(Self-occlusion)’ 현상이 발생한다. 눈과 손의 협응(Hand-Eye Coordination)이 중요한 조작 작업에서, 가장 정밀한 제어가 필요한 접촉 직전의 순간에 시각 정보가 차단되는 것이다. 또한, 잡동사니가 쌓여 있는(Cluttered) 환경에서 특정 물체를 집어내야 할 때, 목표 물체의 중요한 특징점이나 파지점(Grasp Point)이 다른 물체에 의해 가려져 보이지 않는 경우가 다반사이다.</p>
<p>시각 시스템은 본질적으로 “보이는 것만 인지할 수 있다“는 한계를 지닌다. 반면, 물리적 상호작용은 시야가 차단된 곳에서도 발생한다. 예를 들어, 로봇이 불투명한 상자 안의 물건을 탐색하거나, 어두운 선반 깊숙이 있는 부품을 꺼내야 할 때 카메라는 무력하다. 이러한 상황에서 인간은 자연스럽게 손을 뻗어 물체를 더듬으며 형상을 파악한다. 이는 시각적 폐색과 무관하게 물체의 형상, 표면의 거칠기, 그리고 접촉 상태를 감지할 수 있는 촉각 정보의 중요성을 시사한다. 최근의 연구들은 카메라가 놓친 기하학적 정보를 촉각 센서로 보완하거나, 심지어 완전한 어둠 속에서 촉각 정보만을 사용하여 물체의 6자유도(6-DoF) 포즈를 추정하는 기술로 발전하고 있다.</p>
<h3>1.4  물리적 속성의 불확실성: 재질과 마찰의 추론</h3>
<p>시각 정보만으로는 물체의 질량(Mass), 마찰 계수(Friction Coefficient), 경도(Stiffness), 온도와 같은 내재적 물리 속성을 정확히 파악하기 어렵다. 예를 들어, 시각적으로 동일한 형태와 색상을 가진 금속 큐브와 스폰지 큐브가 있다고 가정해보자. 로봇이 비전 센서만으로 이 두 물체를 구별하여 적절한 파지력(Gripping Force)을 결정하는 것은 불가능에 가깝다. 금속 큐브를 들어 올리기 위해 필요한 힘으로 스폰지 큐브를 잡으면 과도한 변형이나 파손이 발생할 것이고, 반대로 스폰지를 잡는 힘으로 금속을 잡으면 미끄러짐(Slip)이 발생하여 물체를 떨어뜨리게 된다.</p>
<p>특히 2024년 이후의 로봇 연구 트렌드는 단순히 물체의 겉모습을 인식하는 것을 넘어, 물체와의 동적인 상호작용 과정에서 발생하는 소리(Audio)와 진동(Vibration), 그리고 열적 특성(Thermal Properties)을 통해 재질을 분류하고 접촉 상태를 모니터링하는 방향으로 나아가고 있다. 긁는 소리, 두드리는 소리 등은 물체의 내부 속성과 표면 재질에 대한 풍부한 청각적 단서를 제공하며, 이는 시각적으로 구분이 어려운 ‘벨크로(Velcro)의 거친 면과 부드러운 면’ 등을 구분하는 데 결정적인 역할을 수행한다. 이처럼 시각을 넘어서는 감각의 확장은 로봇이 물리적 세계의 인과관계를 이해하고 적응하는 데 필수적인 요소가 된다.</p>
<h2>2.  촉각 인지(Tactile Perception): 로봇 피부의 진화와 혁신</h2>
<p>촉각(Touch)은 로봇이 환경과 직접적으로 물리적 접촉을 하는 유일한 감각이며, 조작의 안정성을 보장하는 최후의 보루이다. 과거에는 압전 소자(Piezoelectric)나 정전 용량(Capacitive) 방식의 단순한 1차원 힘 센서가 주류를 이루었으나, 이들은 공간 해상도가 낮고 배선이 복잡하다는 단점이 있었다. 최근에는 고해상도 카메라를 센서 내부에 장착하여 접촉면의 변형을 이미지로 포착하는 ’시각 기반 촉각 센서(Vision-based Tactile Sensors, VBTS)’가 로봇 촉각 기술의 표준으로 자리 잡고 있다.</p>
<h3>2.1  GelSight: 초고해상도 기하학적 정보의 획득</h3>
<p>MIT에서 개발된 GelSight 센서는 촉각 센싱의 패러다임을 바꾼 혁신적인 기술로 평가받는다. GelSight는 탄성체(Elastomer) 표면에 코팅된 반사막이 외부 물체와 접촉하여 미세하게 변형될 때, 센서 내부에 위치한 고해상도 카메라가 이를 촬영하여 3차원 표면 형상을 정밀하게 복원하는 방식을 사용한다.</p>
<h4>2.1.1 작동 원리와 구조적 특징</h4>
<p>GelSight 센서의 핵심은 <strong>포토메트릭 스테레오(Photometric Stereo)</strong> 원리의 응용에 있다.</p>
<ol>
<li><strong>탄성체 스킨(Elastomeric Skin):</strong> 센서의 가장 바깥쪽에는 투명한 실리콘 젤 층이 위치하며, 그 표면은 얇은 안료(Pigment) 층으로 코팅되어 있다. 이 코팅은 외부의 빛을 차단하여 내부 조명 환경을 일정하게 유지하는 동시에, 물체의 미세한 질감을 따라 변형되는 역할을 한다. 탄성체의 두께는 보통 1~5mm 정도이며, 쇼어 경도(Shore Hardness) 00 수준의 부드러운 재질을 사용한다.</li>
<li><strong>조명 시스템:</strong> 센서 내부에는 서로 다른 색상(예: Red, Green, Blue)의 LED 광원이 서로 다른 각도(Azimuth)에서 탄성체 표면을 비추도록 설계되어 있다.</li>
<li><strong>3차원 형상 복원:</strong> 물체가 탄성체를 누르면 표면의 기울기(Surface Normal)가 변화한다. 이때 각 픽셀에서 반사되는 R, G, B 빛의 강도 비율이 표면의 기울기에 따라 달라지게 되는데, 이를 역산하면 접촉면의 3차원 높이 맵(Height Map)을 픽셀 단위로 복원할 수 있다. GelSight는 이를 통해 마이크로미터(μm) 단위의 해상도로 물체의 표면 질감, 흠집, 패턴 등을 시각화한다.</li>
<li><strong>마커 추적을 통한 힘 측정:</strong> 탄성체 표면에 일정한 간격으로 인쇄된 검은 점(Marker)들의 움직임을 추적함으로써, 수직으로 작용하는 힘(Normal Force)뿐만 아니라 표면을 따라 작용하는 전단력(Shear Force)과 비틀림 토크(Torque)까지 추정할 수 있다. 이는 미끄러짐을 감지하는 데 핵심적인 정보를 제공한다.</li>
</ol>
<h4>2.1.2 응용 및 데이터 활용</h4>
<p>GelSight 센서는 그 높은 해상도 덕분에 단순한 접촉 감지를 넘어 정밀한 물성 분석에 활용된다. 예를 들어, USB 케이블의 로고 음각을 읽어 커넥터의 방향을 파악하거나, 옷감의 짜임새를 분석하여 섬유 종류를 분류하고, 투명한 물체의 표면 결함을 검사하는 등 시각 센서로는 불가능한 영역을 담당한다. 특히 GelSight Mini와 같은 소형화 모델은 로봇 그리퍼 끝단에 부착되어 실시간으로 파지 상태를 모니터링하며, 딥러닝 모델(CNN, LSTM 등)과 결합하여 물체의 경도를 추정하거나 미끄러짐이 발생하기 직전의 전조 증상을 감지하는 데 사용된다.</p>
<h3>2.2  DIGIT: 촉각 센서의 소형화와 민주화 (Democratization)</h3>
<p>GelSight의 뛰어난 성능에도 불구하고, 초기 모델은 크기가 크고 제작 공정이 복잡하여 다지(Multi-fingered) 로봇 핸드에 적용하기 어렵다는 한계가 있었다. 이를 극복하기 위해 Meta AI(페이스북)는 <strong>DIGIT</strong>이라는 소형, 저가형, 오픈소스 촉각 센서를 개발하여 공개하였다.</p>
<h4>2.2.1 설계 혁신과 접근성</h4>
<p>DIGIT은 광학 경로를 획기적으로 단축하고 부품을 모듈화하여 손가락 끝(Fingertip) 크기의 컴팩트한 폼팩터를 구현했다.</p>
<ul>
<li><strong>컴팩트한 광학 설계:</strong> DIGIT은 좁은 공간 내에서도 넓은 시야각을 확보하기 위해 특수 설계된 렌즈와 조명 배치를 적용했다. 이는 복잡한 로봇 손가락 관절의 움직임을 방해하지 않으면서도 접촉면 전체를 감지할 수 있게 한다.</li>
<li><strong>대량 생산 및 오픈소스:</strong> DIGIT은 3D 프린팅과 상용 부품을 활용하여 누구나 쉽게 제작할 수 있도록 설계 도면과 소프트웨어가 공개되었다. 이는 전 세계 연구자들이 촉각 데이터를 수집하고 공유할 수 있는 기반을 마련하여, 촉각 AI(Touch AI) 연구의 가속화를 이끌었다.</li>
</ul>
<h4>2.2.2 파지 성능의 향상</h4>
<p>연구 결과에 따르면, DIGIT 센서에서 수집된 촉각 데이터는 로봇의 파지 성공률을 획기적으로 높이는 데 기여한다. 특히 시각 정보만으로는 파지점의 높이나 물체의 두께를 정확히 예측하기 어려운 상황(예: 얇은 카드, 투명한 플라스틱)에서, DIGIT을 통한 접촉 피드백은 로봇이 물체를 안전하게 들어 올릴 수 있는지를 판단하는 결정적인 근거가 된다. DIGIT을 활용한 학습 모델은 난이도가 높은 물체에 대해 파지 성공률을 40% 이상 향상시킴이 입증되었다.</p>
<h3>2.3  촉각을 이용한 물리적 추론과 제어 알고리즘</h3>
<p>촉각 데이터는 단순히 접촉 여부를 O/N(On/Off)로 판단하는 스위치가 아니라, 물체의 물리적 성질을 학습하고 제어하는 풍부한 정보원이다.</p>
<ul>
<li><strong>경도(Hardness) 및 재질 추정:</strong> 시계열 촉각 이미지를 CNN(Convolutional Neural Network)이나 RNN(Recurrent Neural Network)에 입력하여 물체의 쇼어 경도(Shore Hardness)를 추정할 수 있다. 이는 로봇이 과일과 같이 무른 물체를 잡을 때 힘을 조절하는 데 필수적이다.</li>
<li><strong>미끄러짐 감지(Slip Detection)와 반사 신경:</strong> 물체를 들어 올리는 순간, 마커의 미세한 이동이나 표면 텍스처의 흐름을 감지하여 미끄러짐이 발생하기 수 밀리초(ms) 전에 파지력을 증가시키는 ‘반사 신경(Reflex)’ 알고리즘을 구현할 수 있다. 이는 인간이 미끄러운 비누를 잡을 때 본능적으로 힘을 더 주는 것과 유사한 메커니즘이다.</li>
<li><strong>인핸드 조작(In-hand Manipulation):</strong> 물체를 손에서 놓지 않고 손가락만으로 굴리거나 방향을 바꾸는 고난도 작업에서, 촉각 센서는 물체의 현재 자세(Pose)를 실시간으로 추적하는 데 사용된다. 시각적으로 손가락에 가려져 보이지 않는 물체의 상태를 촉각 이미지만으로 추정하는 기술은 로봇 조작의 정밀도를 한 차원 높였다.</li>
</ul>
<h2>3.  청각 인지(Audio Perception): 로봇, 접촉의 소리를 듣다</h2>
<h3>3.1  저평가된 모달리티: 청각의 재발견</h3>
<p>로봇 공학에서 청각(Audio)은 오랫동안 인간과의 음성 대화(Speech Recognition) 수단으로만 여겨졌으며, 물리적 조작을 위한 감각으로는 간과되어 왔다. 그러나 청각은 시각이 제공하는 공간적 정보, 촉각이 제공하는 국소적 접촉 정보와는 전혀 다른 차원의 정보를 제공한다. 청각은 **상호작용의 동적 특성(Dynamic Interaction)**과 **물체의 내부 속성(Internal Properties)**을 전달하는 데 탁월하다. 최근 연구들은 이러한 청각 정보를 로봇의 센서리 모터(Sensorimotor) 루프에 통합함으로써 조작 성능을 비약적으로 향상시킬 수 있음을 증명하고 있다.</p>
<h3>3.2  하드웨어: 접촉 마이크로폰과 능동적 청취</h3>
<p>로봇 조작 환경은 모터 소음, 팬 소리, 외부 소음 등으로 매우 시끄럽다. 일반적인 공기 전도 마이크(Air Microphone)는 이러한 주변 소음(Ambient Noise)에 취약하여, 로봇 손끝에서 발생하는 미세한 접촉음을 분리해 내기 어렵다. 이에 대한 기술적 해법으로 **압전 소자 기반의 접촉 마이크로폰(Contact Microphone)**이 도입되었다.</p>
<ul>
<li><strong>작동 원리:</strong> 접촉 마이크는 공기를 매질로 하지 않고, 물체의 표면 진동(Structure-borne Sound)을 직접 전기 신호로 변환한다. 이를 로봇의 그리퍼나 손가락에 부착하면, 로봇 팔의 진동을 통해 전달되는 미세한 긁힘, 충돌, 결합 소리를 고감도로 포착할 수 있으며, 외부 공기 중의 소음은 효과적으로 차단된다.</li>
<li><strong>능동적 음향 감지(Active Acoustic Sensing):</strong> 로봇이 수동적으로 소리를 듣는 것을 넘어, 스피커나 액추에이터를 통해 물체에 진동을 발생시키고(Ping), 물체를 통과하며 변형된 진동 신호를 수신하여 분석하는 기술이다. 이를 통해 물체 내부의 균열, 액체의 점성, 혹은 속이 비었는지 여부를 비파괴적으로 검사할 수 있다.</li>
</ul>
<h3>3.3  주요 연구 사례: ManiWAV와 Hearing Touch</h3>
<p>스탠포드 대학 및 연구팀들이 제안한 시스템들은 청각 정보를 로봇 조작에 통합하는 구체적인 방법론을 제시한다.</p>
<h4>3.3.1 ManiWAV: Ear-in-Hand 시스템</h4>
<p>ManiWAV는 로봇의 손(Gripper)에 카메라와 접촉 마이크를 내장한 데이터 수집 장치이자 학습 프레임워크이다. 기존의 데이터 수집이 로봇을 원격 조종(Teleoperation)하는 방식에 의존했다면, ManiWAV는 인간이 직접 장비를 들고 작업을 수행하면서 자연스러운 시각-청각 데이터를 수집할 수 있게 한다.</p>
<ul>
<li><strong>다양한 작업 시나리오:</strong> ManiWAV 연구는 베이글 뒤집기(Bagel Flipping), 화이트보드 지우기, 벨크로 붙이기 등의 작업에서 오디오 정보의 효용성을 입증했다. 예를 들어, 베이글을 뒤집개로 들어 올릴 때, 뒤집개가 팬 바닥을 긁는 소리와 베이글 표면을 스치는 소리는 주파수 특성이 확연히 다르다. 시각적으로는 뒤집개가 베이글 밑으로 정확히 들어갔는지 확인하기 어렵지만(폐색으로 인해), 청각 정보는 이를 명확히 구분해 준다.</li>
<li><strong>재질 및 상태 판별:</strong> 화이트보드를 지울 때, 펜 뚜껑이 닫혀 있는지, 잉크가 말랐는지, 지우개가 보드에 얼마나 밀착되었는지는 소리의 크기와 주파수 대역 분포(Spectral Distribution)를 통해 즉각적으로 알 수 있다.</li>
</ul>
<h4>3.3.2 Hearing Touch: 대규모 사전 학습의 전이</h4>
<p><strong>Hearing Touch</strong> 프로젝트는 로봇 데이터의 부족 문제를 해결하기 위해, 유튜브 등 인터넷상의 방대한 비디오 데이터에서 학습된 오디오-비주얼 표현을 활용한다. 연구진은 오디오와 비전의 상관관계를 학습한 대규모 모델(예: Audio-MAE, ImageBind)을 로봇의 촉각-청각 융합 네트워크에 전이 학습(Transfer Learning) 시켰다. 그 결과, 적은 양의 로봇 시연 데이터만으로도 처음 보는 물체나 환경 소음이 있는 상황에서도 강건한 조작 성능을 보여주었다. 이는 인간이 한 번도 다뤄보지 않은 물체라도 소리를 통해 재질을 짐작할 수 있는 것과 유사한 ’일반화된 청각 지능’을 로봇에게 부여한 사례이다.</p>
<h3>3.4  오디오 기반의 상태 추정 및 성공 판별</h3>
<p>청각은 로봇 작업의 성공 여부를 판단(Verification)하는 데 있어 시각보다 훨씬 빠르고 정확한 피드백을 제공한다.</p>
<ul>
<li><strong>이벤트 감지(Event Detection):</strong> “딸깍“하는 스냅핏(Snap-fit) 결합 소리, 병뚜껑이 열릴 때의 “치익” 소리 등은 작업이 성공적으로 완료되었음을 알리는 가장 확실한 신호이다.</li>
<li><strong>접촉 모드 분류(Contact Mode Classification):</strong> 물체를 톡톡 두드리는 것(Tapping), 표면을 긁는 것(Scratching), 미끄러지는 것(Sliding)은 각각 고유한 시간-주파수 패턴(Time-Frequency Pattern)을 생성한다. 딥러닝 모델은 이러한 스펙트로그램을 분석하여 현재 로봇이 물체와 어떤 방식으로 상호작용하고 있는지를 실시간으로 추론한다.</li>
<li><strong>환경 적응성:</strong> 연기가 자욱하거나 조명이 꺼진 재난 현장, 혹은 흙탕물 속에서의 작업과 같이 시각 정보가 완전히 차단된 상황에서도, 소리는 작업의 진행 상황을 모니터링할 수 있는 유일한 감각이 될 수 있다.</li>
</ul>
<h2>4.  비가시 영역의 확장: 열화상과 LiDAR</h2>
<p>인간의 오감을 모방하는 것을 넘어, 로봇만이 가질 수 있는 초감각(Super-sensing)인 열화상과 LiDAR는 로봇에게 환경에 대한 더 깊고 정밀한 이해를 제공한다.</p>
<h3>4.1  열화상(Thermal Imaging): 보이지 않는 에너지를 보다</h3>
<p>열화상 카메라는 물체에서 방사되는 적외선(Infrared) 에너지를 감지하여 온도 분포를 시각화한다. 이는 가시광선에 의존하지 않으므로 조명 조건의 제약을 받지 않으며, 완전한 암흑 속에서도 작동 가능하다.</p>
<h4>4.1.1 로봇 조작에서의 응용</h4>
<ul>
<li><strong>재질 분류:</strong> 열전도율(Thermal Conductivity)과 열용량(Heat Capacity)은 물질 고유의 특성이다. 로봇이 물체를 잠깐 만졌을 때, 금속은 체온을 빠르게 뺏어가 차갑게 느껴지고, 스티로폼은 따뜻하게 느껴지는 원리를 이용하여, 접촉 후 온도 변화 패턴을 분석함으로써 재질을 분류할 수 있다.</li>
<li><strong>열흔(Thermal Residual) 추적:</strong> 최근 연구에서는 사람이 물체를 만지고 난 후 남겨진 미세한 열 자국(Heat Signature)을 열화상 카메라로 감지하여, 사람이 물체의 어느 부위를 잡았는지를 역추적하고 로봇이 이를 모방하여 파지(Grasping)하는 기술이 개발되었다. 이는 데이터 수집 없이도 인간의 파지 전략을 학습할 수 있는 혁신적인 방법이다.</li>
<li><strong>액체 수위 감지:</strong> 불투명한 용기에 담긴 액체의 수위는 밖에서 보이지 않지만, 액체와 용기 벽면의 온도 차이를 이용하면 열화상으로 수위를 명확하게 확인할 수 있다.</li>
<li><strong>안전 및 HRI:</strong> 모터나 배터리의 과열을 감지하여 로봇의 고장을 예방하거나, 작업 공간 내에 있는 사람의 체온을 감지하여 충돌을 방지하는 안전 시스템으로 활용된다.</li>
</ul>
<h3>4.2  LiDAR: 정밀한 3차원 기하학의 척도</h3>
<p>주로 자율 주행의 눈으로 알려진 LiDAR(Light Detection and Ranging)는 최근 로봇 팔의 정밀 조작 영역으로 빠르게 도입되고 있다. 특히 기존의 회전형 LiDAR 대신 <strong>고체형(Solid-state) LiDAR</strong>나 <strong>방향성(Directional) LiDAR</strong>가 개발되면서, 근거리에서도 높은 해상도의 포인트 클라우드(Point Cloud) 획득이 가능해졌다.</p>
<h4>4.2.1 조작을 위한 LiDAR의 장점</h4>
<ul>
<li><strong>텍스처 불변성:</strong> 스테레오 카메라나 구조광 센서는 표면에 무늬가 없는(Textureless) 단색 물체나 조명이 균일하지 않은 환경에서 깊이 정보를 추출하는 데 실패하기 쉽다. 반면 LiDAR는 레이저 펄스의 비행 시간을 직접 측정하므로, 물체의 표면 패턴이나 주변 조명과 무관하게 정확한 형상 정보를 제공한다.</li>
<li><strong>악천후 강건성:</strong> 공장 내의 먼지, 야외 현장의 안개나 비, 강한 역광 등은 카메라 센서를 무력화시키지만, LiDAR는 이러한 환경적 요인에 훨씬 강건하다. 이는 농업용 로봇이나 건설 로봇이 야외에서 작업을 수행할 때 필수적인 기능이다.</li>
<li><strong>고정밀 공간 매핑:</strong> LiDAR는 로봇 팔의 작업 공간 전체를 밀리미터(mm) 단위의 정밀도로 실시간 매핑할 수 있어, 복잡한 장애물을 회피하고 좁은 틈새로 팔을 집어넣는 경로 계획(Path Planning)의 신뢰성을 보장한다.</li>
</ul>
<h2>5.  멀티모달 융합 아키텍처: 감각을 통합하는 디지털 뇌</h2>
<p>수집된 시각, 촉각, 청각, 열화상 데이터는 서로 다른 차원(Dimension)과 갱신 주기(Frequency), 데이터 형식(Format)을 가진다. 시각은 2D 픽셀 그리드, 촉각은 고해상도 이미지 또는 압력 값의 배열, 청각은 1D 시계열 파형, LiDAR는 희소한(Sparse) 3D 포인트 클라우드이다. 이들을 효과적으로 결합하여 하나의 통합된 판단을 내리기 위한 딥러닝 아키텍처는 단순한 연결(Concatenation)에서 벗어나, 통합된 잠재 공간(Unified Latent Space)을 학습하는 방향으로 진화하고 있다.</p>
<h3>5.1  융합의 단계: Early, Late, and Mid Fusion</h3>
<p>멀티모달 융합은 정보가 결합되는 단계에 따라 크게 세 가지로 분류된다.</p>
<ul>
<li><strong>Early Fusion (Input-level):</strong> 센서 데이터를 입력 단에서 직접 결합한다. 예를 들어, RGB 이미지 채널에 깊이 맵(D)을 추가하여 4채널(RGB-D) 입력을 만드는 방식이다. 저수준의 특징(Low-level feature) 간의 상관관계를 포착할 수 있지만, 데이터 간의 정렬(Registration)이 완벽해야 하며 센서별 샘플링 속도 차이를 처리하기 어렵다.</li>
<li><strong>Late Fusion (Decision-level):</strong> 각 모달리티를 개별적인 신경망으로 처리한 후, 최종 예측 값(예: 확률 벡터)을 평균 내거나 투표(Voting)하여 결합한다. 구현이 간단하고 유연하지만, 모달리티 간의 복잡한 상호작용이나 보완 관계를 모델이 학습하지 못한다는 단점이 있다.</li>
<li><strong>Mid Fusion (Feature-level):</strong> 각 센서 데이터에서 특징(Feature)을 추출한 중간 단계에서 정보를 교환하고 융합한다. 최근 가장 널리 사용되는 방식으로, 특히 <strong>크로스 어텐션(Cross-Attention)</strong> 메커니즘이 핵심 기술로 자리 잡았다.</li>
</ul>
<h3>5.2  Cross-Attention: 모달리티 간의 대화</h3>
<p>Transformer 모델에서 유래한 Cross-Attention은 서로 다른 두 모달리티 간의 연관성을 학습하는 강력한 도구이다.</p>
<ul>
<li><strong>메커니즘:</strong> 예를 들어, 오디오와 비디오를 융합한다고 가정하자. 오디오 특징을 **Query (Q)**로 설정하고, 비디오 특징을 **Key (K)**와 **Value (V)**로 설정한다. Attention 연산을 수행하면, 모델은 현재 들리는 소리(Q)와 가장 관련이 깊은 비디오 프레임의 특정 영역(K)을 찾아내고, 그 정보(V)를 가져와 오디오 특징을 강화한다.</li>
<li><strong>효과:</strong> 이 과정은 데이터 간의 시간적, 공간적 정렬이 완벽하지 않아도 모델이 스스로 중요한 상관관계를 찾아내도록(Learnable Alignment) 유도한다. Hearing Touch와 같은 모델은 이 메커니즘을 통해 충돌 소리가 발생한 시점의 시각적 변화에 집중하여 물체의 재질을 추론한다.</li>
</ul>
<h3>5.3  Perceiver IO: 모달리티에 구애받지 않는 범용 아키텍처</h3>
<p>DeepMind가 제안한 <strong>Perceiver IO</strong>는 입력 데이터의 종류나 크기에 상관없이 처리할 수 있는 혁신적인 아키텍처이다. 기존의 Transformer는 입력 시퀀스의 길이에 따라 연산량이 제곱(O(N^2))으로 증가하여 고해상도 이미지나 긴 비디오 처리에 한계가 있었다.</p>
<ul>
<li><strong>잠재 병목(Latent Bottleneck) 구조:</strong> Perceiver IO는 입력 데이터(Input Array)가 아무리 크더라도(예: 수만 개의 픽셀, 수천 개의 오디오 샘플), 이를 훨씬 작은 크기의 고정된 <strong>잠재 배열(Latent Array)</strong>(예: 512개 벡터)로 투영한다. 입력 데이터를 Key/Value로, 잠재 배열을 Query로 하는 Cross-Attention을 통해 방대한 입력 정보를 압축된 잠재 공간으로 요약한다.</li>
<li><strong>선형적 확장성:</strong> 이 구조 덕분에 연산량은 입력 크기에 대해 선형적(O(N))으로 증가하므로, 시각, 촉각, 오디오 등 다수의 고차원 데이터를 동시에 처리하는 데 매우 효율적이다.</li>
<li><strong>Perceiver-Actor (PerAct):</strong> 이 아키텍처를 로봇 조작에 적용한 PerAct 모델은 RGB-D 정보를 3D 복셀(Voxel)로 변환하여 입력받고, 언어 명령과 함께 처리하여 6-DoF 로봇 행동을 생성한다. PerAct는 수백만 개의 복셀을 효율적으로 처리하여 복잡한 3D 공간에서의 조작 작업을 적은 횟수의 시연만으로 학습해 냈다.</li>
</ul>
<h3>5.4  ImageBind: 하나의 임베딩으로 모든 감각을 묶다</h3>
<p>Meta AI의 <strong>ImageBind</strong>는 6가지 모달리티(이미지, 텍스트, 오디오, 깊이, 열화상, IMU)를 하나의 공통된 임베딩 공간(Embedding Space)으로 매핑하는 획기적인 연구이다.</p>
<ul>
<li><strong>이미지를 매개로 한 정렬:</strong> 모든 모달리티 쌍(예: 오디오-열화상)에 대한 학습 데이터가 존재하지 않는다는 현실적인 문제를 해결하기 위해, ImageBind는 이미지를 ’연결 고리(Binding Agent)’로 사용한다. (이미지, 텍스트), (이미지, 오디오), (이미지, 깊이) 쌍을 각각 학습시키면, 모델은 전이성(Transitivity)을 통해 직접 학습하지 않은 (텍스트, 오디오) 간의 연관성도 파악하게 된다.</li>
<li><strong>Zero-shot 멀티모달 추론:</strong> 로봇은 ImageBind를 통해 “뜨거운 물체“라는 텍스트 명령을 받으면, 이를 열화상 센서 데이터와 연결하여 인식하고, 오디오 피드백을 통해 끓는 소리를 감지하는 등 별도의 추가 학습 없이도 다양한 감각을 넘나드는 추론이 가능해진다.</li>
</ul>
<h3>5.5  EmbodiedMAE: 3D 마스킹을 통한 공간 지능 학습</h3>
<p><strong>EmbodiedMAE</strong>는 로봇 조작을 위해 특화된 마스크 오토인코더(Masked Autoencoder, MAE)이다.</p>
<ul>
<li><strong>3D 일관성 학습:</strong> 기존의 2D 이미지 기반 MAE와 달리, EmbodiedMAE는 RGB, 깊이, 포인트 클라우드 데이터를 동시에 입력받아 3D 공간상에서의 일관성을 학습한다. 입력 데이터의 상당 부분(예: 75%)을 무작위로 마스킹(Masking)하고 이를 복원하도록 학습시킴으로써, 모델은 데이터의 표면적 특징이 아닌 기하학적 구조와 의미론적 맥락을 깊이 있게 이해하게 된다.</li>
<li><strong>센서 결함에 대한 강건성:</strong> 이러한 학습 방식은 실제 환경에서 센서 데이터가 누락되거나 노이즈가 발생했을 때(예: 카메라가 가려지거나 깊이 센서가 튀는 경우)에도 모델이 누락된 정보를 스스로 채워 넣으며 강건하게 동작할 수 있게 한다. 실험 결과, EmbodiedMAE는 다양한 로봇 조작 작업에서 기존의 비전 파운데이션 모델(VFM)들을 능가하는 성능을 입증했다.</li>
</ul>
<h2>6.  멀티모달 데이터와 시뮬레이션: 학습의 연료</h2>
<p>멀티모달 AI의 성능은 데이터의 양과 질에 달려 있다. 그러나 로봇이 실제 환경에서 수백만 번의 시행착오를 겪으며 데이터를 수집하는 것은 시간적, 물리적으로 불가능하다.</p>
<h3>6.1  Sim-to-Real Gap과 멀티모달 시뮬레이션</h3>
<p>시각 정보는 그래픽 렌더링 기술의 발전으로 시뮬레이션과 실제 환경의 차이(Sim-to-Real Gap)가 많이 줄어들었으나, 촉각과 청각의 시뮬레이션은 여전히 난제이다.</p>
<ul>
<li><strong>촉각 렌더링:</strong> 물체의 변형, 마찰, 질감을 물리적으로 정확하게 계산하려면 유한 요소 해석(FEM)과 같은 고비용 연산이 필요하다. 최근에는 TACTO와 같은 시뮬레이터를 통해 GelSight 센서의 광학적 반응을 렌더링하거나, GAN(Generative Adversarial Network)을 이용해 시뮬레이션 촉각 이미지를 실제처럼 변환하는 연구가 진행되고 있다.</li>
<li><strong>오디오 렌더링:</strong> 물체의 충돌음을 합성하기 위해서는 물체의 재질, 형상, 충격 위치, 속도 등을 고려한 물리 기반 오디오 합성(Physically-based Audio Synthesis) 기술이 필요하다. ThreeDWorld(TDW)와 같은 시뮬레이터는 충돌 시 발생하는 소리를 실시간으로 생성하여 오디오-비주얼 학습을 지원한다.</li>
</ul>
<h3>6.2  데이터셋과 자가 지도 학습 (Self-Supervised Learning)</h3>
<p>레이블이 없는 방대한 데이터로부터 학습하기 위해 자가 지도 학습이 필수적이다.</p>
<ul>
<li><strong>DROID-3D:</strong> 대규모 로봇 조작 데이터셋인 DROID를 확장하여 고품질의 깊이 맵과 포인트 클라우드를 포함한 데이터셋으로, 3D 멀티모달 학습의 기준이 되고 있다.</li>
<li><strong>대조 학습(Contrastive Learning):</strong> ImageBind나 CLIP과 같이, 서로 다른 모달리티의 데이터가 같은 상황에서 발생했다면(Positive Pair) 임베딩 공간에서 가깝게, 그렇지 않다면(Negative Pair) 멀게 배치하도록 학습하여 데이터 간의 의미적 연결을 강화한다.</li>
</ul>
<h2>7.  미래 전망: 파운데이션 모델과 체화된 지능 (Embodied AI)</h2>
<p>최신 로봇 공학의 흐름은 거대 언어 모델(LLM)과 감각-행동 시스템을 결합한 <strong>VLA (Vision-Language-Action)</strong> 모델로 수렴하고 있다.</p>
<h3>7.1  Palm-E와 RT-2: 언어 모델에 감각을 주입하다</h3>
<p>Google의 <strong>Palm-E</strong>는 거대 언어 모델(PaLM)에 시각 및 로봇 상태 정보를 주입한 ‘구체화된(Embodied)’ 멀티모달 모델이다. 이미지나 센서 데이터를 ViT를 통해 인코딩한 후, 이를 언어 토큰과 동일한 차원의 벡터로 변환하여 LLM의 입력 시퀀스에 섞어 넣는다. 이를 통해 로봇은 “서랍에서 쌀과자를 꺼내라“는 명령을 수행할 때, 시각 정보뿐만 아니라 자신의 물리적 상태를 고려하여 추론하고 행동을 생성할 수 있다. <strong>RT-2</strong>는 여기서 더 나아가 이미지를 보고 직접 로봇의 행동 토큰을 출력함으로써, 인터넷 데이터의 일반 상식과 로봇의 물리적 제어 능력을 통합했다.</p>
<h3>7.2  결론: 감각의 융합이 만드는 로봇의 직관</h3>
<p>우리는 본 장에서 로봇의 인지가 시각 중심의 패러다임을 넘어, 촉각, 청각, 열화상 등 다양한 감각을 아우르는 멀티모달 융합으로 진화하고 있음을 확인했다.</p>
<ol>
<li><strong>상호 보완성의 극대화:</strong> 시각은 전역적인 계획(Global Planning)을, 촉각은 국소적인 제어(Local Control)를, 청각은 이벤트 판별(Event Detection)과 재질 추론을 담당하며 서로의 약점을 완벽하게 보완한다.</li>
<li><strong>아키텍처의 통합:</strong> Transformer 기반의 Cross-Attention, Perceiver IO의 잠재 병목 구조, ImageBind의 통합 임베딩 공간은 이질적인 센서 데이터를 하나의 지능 시스템으로 융합하는 강력한 도구임이 입증되었다.</li>
<li><strong>데이터 중심의 발전:</strong> GelSight, DIGIT, ManiWAV와 같은 혁신적인 하드웨어는 로봇이 학습할 수 있는 양질의 멀티모달 데이터를 생성하고 있으며, 이는 EmbodiedMAE나 VLA 모델과 같은 데이터 집약적 학습 방법론의 연료가 되고 있다.</li>
</ol>
<p>결론적으로, <strong>멀티모달 센서 융합</strong>은 로봇이 불확실하고 비정형적인 현실 세계에서 강건하게 동작하기 위한 선택이 아닌 필수 조건이다. 미래의 로봇은 단순히 카메라로 세상을 ‘보는’ 기계가 아니라, 피부로 느끼고, 소리로 듣고, 온도를 감지하며, 이 모든 감각을 통합하여 상황을 직관적으로 이해하는 ‘지각하는(Sentient)’ 존재로 거듭날 것이다. 이는 모라벡의 역설(Moravec’s Paradox)을 해결하고, 가사 로봇, 재난 구조 로봇, 정밀 조립 로봇이 실험실을 벗어나 우리 일상 속으로 들어오게 하는 핵심 열쇠가 될 것이다.</p>
<p><strong>[표 8-1] 로봇 조작을 위한 주요 센서 모달리티 비교 및 융합 시너지</strong></p>
<table><thead><tr><th><strong>센서 유형</strong></th><th><strong>주요 감지 정보</strong></th><th><strong>장점</strong></th><th><strong>단점 및 한계</strong></th><th><strong>시각(Vision)과의 융합 시너지</strong></th></tr></thead><tbody>
<tr><td><strong>RGB-D 카메라</strong></td><td>색상, 형태, 거리</td><td>풍부한 의미론적 정보, 넓은 시야, 높은 데이터 접근성</td><td>투명/반사 물체 취약, 폐색(Occlusion), 조명 민감</td><td>기본 베이스라인 제공, 전역적 경로 계획(Global Planning) 담당</td></tr>
<tr><td><strong>GelSight/DIGIT</strong></td><td>표면 형상(3D), 텍스처, 미세 변형</td><td>초고해상도, 투명 물체 감지, 미끄러짐 감지, 조명 불변</td><td>접촉 시에만 작동(Local), 센서 내구성, 좁은 감지 영역</td><td>폐색 상황에서의 로컬 제어, 물성 추정 보완, 파지 안정성 확보</td></tr>
<tr><td><strong>접촉 마이크</strong></td><td>진동, 마찰음, 충돌음</td><td>내부 재질/구조 판별, 이벤트(결합/파손) 감지, 시야 밖 감지</td><td>주변 소음 간섭, 접촉 필요, 데이터 해석의 난해함</td><td>접촉 모드(긁기/두드리기) 분류, 동작 성공 여부(Verification) 판단</td></tr>
<tr><td><strong>LiDAR</strong></td><td>정밀 거리(Point Cloud)</td><td>조명 불변, 장거리 정확도, 넓은 시야, 악천후 강건성</td><td>텍스처/색상 정보 부재, 고비용, 투명 물체 일부 취약</td><td>정밀 기하학적 구조 파악, 저조도/악천후 환경에서의 강건성 보완</td></tr>
<tr><td><strong>열화상 카메라</strong></td><td>온도 분포</td><td>완전한 어둠 속 작동, 생체 감지, 기기 과열 감지</td><td>재질(방사율)에 따른 오차, 저해상도, 텍스처 부족</td><td>재질 분류(열전도율), 안전 모니터링, 사람 인식 및 의도 파악</td></tr>
</tbody></table>
<h2>8. 참고 자료</h2>
<ol>
<li>Robotic Perception of Transparent Objects: A Review - arXiv, https://arxiv.org/pdf/2304.00157</li>
<li>Robot Guidance Using Machine Vision Techniques in Industrial Environments: A Comparative Review - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC4813910/</li>
<li>Robotic Grasping of Novel Objects using Vision, http://graphics.cs.cmu.edu/nsp/course/16899-s10/papers/visionAndGrasping/visionAndGraspingNGShapeIJRR.pdf</li>
<li>Top 10 Challenges for Robot Vision - Robotiq’s blog, https://blog.robotiq.com/top-10-challenges-for-robot-vision</li>
<li>Visual shape perception in the case of transparent objects | JOV - Journal of Vision, https://jov.arvojournals.org/article.aspx?articleid=2731845</li>
<li>Learning Height for Top-Down Grasps with the DIGIT Sensor - IEEE Xplore, https://ieeexplore.ieee.org/document/10160955/</li>
<li>Tactile Sensing applied to Robot Manipulation, https://www.ri.cmu.edu/app/uploads/2023/08/Sashank_MSR_Thesis-3.pdf</li>
<li>Adaptive visual–tactile fusion recognition for robotic operation of multi-material system - Frontiers, https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2023.1181383/full</li>
<li>ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data - arXiv, https://arxiv.org/html/2406.19464v1</li>
<li>GelSight Sensors: 3D Tactile Imaging - Emergent Mind, https://www.emergentmind.com/topics/gelsight-sensors</li>
<li>GelSight: High-Resolution Robot Tactile Sensors for Estimating Geometry and Force - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC5751610/</li>
<li>GelSight Mini, https://www.gelsight.com/wp-content/uploads/2022/09/GelSight_Datasheet_GSMini_9.20.22b.pdf</li>
<li>GelSight Mini, https://www.gelsight.com/wp-content/uploads/2023/01/GelSight_Datasheet_GSMini_12.20.22.pdf</li>
<li>DIGIT: A Novel Design for a Low-Cost Compact High-Resolution Tactile Sensor with Application to In-Hand Manipulation, https://www.seas.upenn.edu/~dineshj/publication/lambeta-2020-digit/lambeta-2020-digit.pdf</li>
<li>Grasping Force Estimation for Markerless Visuotactile Sensors This work was supported by Interreg-VI Sudoe and European Regional Development Fund through the REMAIN project (S1/1.1/E0111) and by the University of Alicante under Grant UAFPU21-26. Julio Castaño-Amoros is with the AUROVA Lab, Computer Science Research Institute, University of Alicante, - arXiv, https://arxiv.org/html/2410.22825v1</li>
<li>Hearing Touch: Audio-Visual Pretraining for Contact-Rich, https://www.researchgate.net/publication/382989554_Hearing_Touch_Audio-Visual_Pretraining_for_Contact-Rich_Manipulation</li>
<li>Hearing Touch: Audio-Visual Pretraining for Contact-Rich Manipulation - arXiv, https://arxiv.org/html/2405.08576v1</li>
<li>ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data - arXiv, https://arxiv.org/html/2406.19464v2</li>
<li>ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data - GitHub, https://raw.githubusercontent.com/mlresearch/v270/main/assets/liu25c/liu25c.pdf</li>
<li>ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data, https://mani-wav.github.io/</li>
<li>Thermal Image Sensing Model for Robotic Planning and Search - MDPI, https://www.mdpi.com/1424-8220/16/8/1253</li>
<li>Contact-aware and multi-modal robotic manipulation - DSpace@MIT, https://dspace.mit.edu/bitstream/handle/1721.1/158785/zhao-alanzhao-phd-meche-2025-thesis.pdf?sequence=-1&amp;isAllowed=y</li>
<li>Natural Multimodal Fusion-Based Human–Robot Interaction: Application With Voice and Deictic Posture via Large Language Model ∗ Corresponding Author. This work is supported by a grant of the EFRE and MWK ProFö-R&amp;D program, no. FEIH_ProT_2517820 and MWK32-7535-30/10/2. This work is also supported by “CAIpirinha - arXiv, https://arxiv.org/html/2501.00785v3</li>
<li>Artificial Intelligence in IR Thermal Imaging and Sensing for Medical Applications - MDPI, https://www.mdpi.com/1424-8220/25/3/891</li>
<li>Why Directional LiDAR is the Future of Robotics - Seyond, https://www.seyond.com/part-1-why-directional-lidar-is-the-future-of-robotics/</li>
<li>LiDAR Sensor for Robotics: Complete Guide to Navigation and Mapping in 2025, https://thinkrobotics.com/blogs/tutorials/lidar-sensor-for-robotics-complete-guide-to-navigation-and-mapping-in-2025</li>
<li>SAMFusion: Sensor-Adaptive Multimodal Fusion for 3D Object Detection in Adverse Weather - Princeton Computational Imaging Lab, https://light.princeton.edu/publication/samfusion/</li>
<li>LiDAR Navigation Explained: From Basic Principles to Advanced Applications - YellowScan, https://www.yellowscan.com/knowledge/lidar-navigation-explained-from-basic-principles-to-advanced-applications/</li>
<li>The Benefits of Spatial Intelligence with 3D LiDAR - Metrolla, https://www.metrolla.com/technology/the-benefits-of-spatial-intelligence-with-3d-lidar</li>
<li>Cross Attention Mid Fusion Architecture - Emergent Mind, https://www.emergentmind.com/topics/cross-attention-mid-fusion-architecture</li>
<li>Cross-Attention-Based Multimodal Representation Fusion for Parametric Gait Adaptation in Complex Terrains - arXiv, https://arxiv.org/html/2409.17262v2</li>
<li>Why Cross-Attention is the Secret Sauce of Multimodal Models | by Jakub Strawa | Medium, https://medium.com/@jakubstrawadev/why-cross-attention-is-the-secret-sauce-of-multimodal-models-f8ec77fc089b</li>
<li>Bi-Att3DDet: Attention-Based Bi-Directional Fusion for Multi-Modal 3D Object Detection, https://www.mdpi.com/1424-8220/25/3/658</li>
<li>Perceiver IO: A General Architecture for Structured Inputs &amp; Outputs - Drew Jaegle, https://www.drewjaegle.com/pdfs/jaegle2021perceiver_io.pdf</li>
<li>[2107.14795] Perceiver IO: A General Architecture for Structured Inputs &amp; Outputs - arXiv, https://arxiv.org/abs/2107.14795</li>
<li>Chapter 4 Further Topics | Multimodal Deep Learning, https://slds-lmu.github.io/seminar_multimodal_dl/c03-00-further.html</li>
<li>Perceiver-actor: A multi-task transformer for robotic manipulation - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v205/shridhar23a/shridhar23a.pdf</li>
<li>PerAct, https://peract.github.io/</li>
<li>UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All, https://cvpr.thecvf.com/virtual/2024/poster/31443</li>
<li>ImageBind by Meta AI, https://imagebind.metademolab.com/</li>
<li>How do unified multimodal models like FLAVA or ImageBind work? - Milvus, https://milvus.io/ai-quick-reference/how-do-unified-multimodal-models-like-flava-or-imagebind-work</li>
<li>[2305.05665] ImageBind: One Embedding Space To Bind Them All - arXiv, https://arxiv.org/abs/2305.05665</li>
<li>EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot Manipulation - arXiv, https://arxiv.org/abs/2505.10105</li>
<li>EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot Manipulation - arXiv, https://arxiv.org/html/2505.10105v1</li>
<li>Multi-Modal Contrastive Masked Autoencoders: A Two-Stage Progressive Pre-training Approach for RGBD Datasets, https://openaccess.thecvf.com/content/CVPR2025/papers/Jamal_Multi-Modal_Contrastive_Masked_Autoencoders_A_Two-Stage_Progressive_Pre-training_Approach_for_CVPR_2025_paper.pdf</li>
<li>Towards Audio-Visual Navigation in Noisy Environments: A Large-Scale Benchmark Dataset and an Architecture Considering Multiple Sound-Sources - AAAI Publications, https://ojs.aaai.org/index.php/AAAI/article/view/33608/35763</li>
<li>PaLM-E: An Embodied Multimodal Language Model, https://palm-e.github.io/</li>
<li>PaLM-E: An embodied multimodal language model - Google Research, https://research.google/blog/palm-e-an-embodied-multimodal-language-model/</li>
<li>HMG Robotics Lab Envisions a “Robot Intelligence Society” Where Humans and Robots Coexist - Hyundai Motor Group, https://www.hyundaimotorgroup.com/en/story/CONT0000000000201010</li>
<li>What Is Cross-Attention in Multimodal Models? - Magai, https://magai.co/what-is-cross-attention-in-multimodal-models/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>