<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Chapter 4. 하드웨어와 AI의 공진화 (Co-Evolution)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Chapter 4. 하드웨어와 AI의 공진화 (Co-Evolution)</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">제목: Embodied AI & Modern Control</a> / <a href="index.html">Chapter 4. 하드웨어와 AI의 공진화 (Co-Evolution)</a> / <span>Chapter 4. 하드웨어와 AI의 공진화 (Co-Evolution)</span></nav>
                </div>
            </header>
            <article>
                <h1>Chapter 4. 하드웨어와 AI의 공진화 (Co-Evolution)</h1>
<h2>1.  서론: 체화된 지능(Embodied Intelligence)을 위한 새로운 패러다임</h2>
<p>인공지능(AI)의 발전사는 오랫동안 소프트웨어 알고리즘의 도약과 하드웨어 연산 능력의 확장이 상호 교차하며 이루어낸 나선형 진화의 과정이었다. 초기 인공지능 연구가 상징적 기호 처리와 논리 연산에 집중하며 중앙처리장치(CPU)의 범용성에 의존했던 시기를 지나, 딥러닝(Deep Learning)의 폭발적인 성장은 행렬 연산의 병렬 처리에 특화된 그래픽처리장치(GPU)의 발전과 궤를 같이했다. 그러나 2020년대 중반에 이르러 로봇 공학, 즉 ‘체화된 지능(Embodied Intelligence)’ 또는 ’물리적 AI(Physical AI)’의 시대가 본격화되면서, 하드웨어와 AI의 관계는 단순한 상호 보완을 넘어선 생물학적 의미의 ‘공진화(Co-Evolution)’ 단계로 진입하고 있다.</p>
<p>공진화란 본래 생태계 내에서 밀접하게 연관된 두 종이 서로의 진화적 압력으로 작용하며 함께 변화해 나가는 현상을 의미한다. 이를 로봇 공학의 기술적 맥락으로 치환하면, 고도화된 AI 모델의 요구사항이 새로운 칩 아키텍처와 센서 설계를 주도하고, 반대로 혁신적인 하드웨어의 물리적 특성이 새로운 AI 알고리즘의 가능성을 열어주는 순환적 발전 구조를 뜻한다. 특히 휴머노이드 로봇과 같이 극도로 복잡한 동적 환경에서 실시간으로 상호작용해야 하는 시스템의 등장은 기존의 ‘클라우드 중심 AI’ 패러다임으로는 해결할 수 없는 근본적인 난제들을 제시했다. 전력 공급이 제한된 엣지(Edge) 환경에서 거대 언어 모델(LLM)과 시각-언어-행동(VLA) 모델을 구동해야 하며, 밀리초(ms) 단위의 반응 속도를 유지하면서도 수백 와트(W) 이내의 전력 소비를 달성해야 한다는 모순적인 요구가 그것이다.</p>
<p>이러한 도전에 대응하기 위해 현대 로봇 공학은 네 가지 주요 전선에서 급진적인 하드웨어-소프트웨어 융합을 시도하고 있다. 첫째, 트랜스포머(Transformer) 아키텍처를 하드웨어 레벨에서 가속하고 로봇의 인지 판단과 제어를 통합 처리하는 <strong>로보틱스 슈퍼컴퓨터의 등장</strong>이다. 둘째, 폰 노이만 구조의 비효율성을 극복하고 생물학적 뇌의 스파이크 신호 전달 방식을 모방하여 에너지 효율을 극대화하는 **뉴로모픽 컴퓨팅(Neuromorphic Computing)과 스파이킹 신경망(SNN)**의 결합이다. 셋째, 인간의 망막이 작동하는 방식을 차용하여 데이터의 중복을 제거하고 초고속 반응성을 확보하는 <strong>이벤트 기반 비전(Event-based Vision)과 인센서(In-Sensor) 컴퓨팅</strong>이다. 마지막으로, 연산의 주체를 실리콘 칩에 한정하지 않고 로봇의 신체 구조 자체가 지능적 행위의 일부를 수행하도록 설계하는 **형태학적 연산(Morphological Computation)과 뇌-신체 동시 최적화(Brain-Body Co-design)**이다.</p>
<p>본 장에서는 이 네 가지 기술적 흐름을 심층적으로 분석한다. 우리는 최신 연구 결과와 산업계의 기술 동향을 통해, 하드웨어가 더 이상 소프트웨어를 담는 수동적인 그릇이 아니라 지능을 구성하는 능동적인 주체로서 어떻게 진화하고 있는지, 그리고 이러한 공진화가 차세대 로봇 시스템에 어떤 혁신적인 능력을 부여하고 있는지를 규명할 것이다.</p>
<h2>2.  엣지 컴퓨팅의 퀀텀 점프: 생성형 AI와 로보틱스 슈퍼컴퓨터</h2>
<p>물리적 AI, 특히 인간과 유사한 형상을 가진 휴머노이드(Humanoid) 로봇의 상용화를 가로막던 가장 견고한 장벽은 ’지능의 무거움’과 ‘신체의 제약’ 사이의 불일치였다. 복잡한 추론과 자연어 이해 능력을 갖춘 생성형 AI 모델은 막대한 메모리 대역폭과 연산 자원을 요구하지만, 로봇은 배터리로 구동되는 제한된 전력과 협소한 공간이라는 물리적 한계 내에서 작동해야 하기 때문이다. 클라우드에 의존하는 방식은 네트워크 지연(Latency)과 보안 문제로 인해 실시간 제어가 필수적인 로봇에는 부적합하다. 이러한 딜레마를 해결하기 위해 등장한 것이 바로 엣지(Edge) 환경에 특화된 로보틱스 슈퍼컴퓨터이며, NVIDIA의 Jetson Thor 플랫폼은 이러한 진화의 정점에 있는 사례이다.</p>
<h3>2.1  NVIDIA Jetson Thor와 Blackwell 아키텍처의 혁신</h3>
<p>NVIDIA Jetson Thor는 단순한 임베디드 프로세서가 아니라, 로봇 공학의 새로운 요구사항인 ’생성형 AI의 엣지 구동’을 목표로 처음부터 설계된 시스템이다. 이 모듈은 NVIDIA의 차세대 Blackwell GPU 아키텍처를 기반으로 하며, 이는 데이터 센터 수준의 연산 능력을 손바닥 크기의 모듈에 집약시킨 기술적 쾌거라 할 수 있다.1</p>
<h4>2.1.1 Blackwell 아키텍처의 로보틱스 최적화</h4>
<p>Jetson Thor의 핵심인 Blackwell 아키텍처는 이전 세대인 Ampere 기반의 Orin 모듈과 비교하여 근본적인 설계 철학의 변화를 보여준다. 가장 두드러진 특징은 **트랜스포머 엔진(Transformer Engine)**의 도입이다. 트랜스포머 모델은 현재 자연어 처리(NLP)와 비전(Vision) 분야를 지배하고 있으며, 로봇의 두뇌 역할을 하는 VLA(Vision-Language-Action) 모델의 근간이 된다. Blackwell GPU는 트랜스포머 모델의 핵심 연산인 어텐션 메커니즘(Attention Mechanism)을 하드웨어적으로 가속화하여, 로봇이 시각 정보를 해석하고 다음 행동을 생성하는 추론 과정을 비약적으로 단축시킨다.1</p>
<p>또한, Jetson Thor는 8비트 부동소수점(FP8)을 넘어 <strong>4비트 부동소수점(FP4)</strong> 정밀도를 지원하는 텐서 코어(Tensor Core)를 탑재했다. 이는 거대 모델의 파라미터를 극도로 압축하면서도 성능 저하를 최소화하는 양자화(Quantization) 기술 트렌드를 하드웨어 차원에서 수용한 결과이다. FP4 연산 지원을 통해 Jetson Thor는 최대 **2,070 TFLOPS(테라플롭스)**의 AI 연산 성능을 달성했다.1 이는 이전 세대 최상위 모델인 Jetson AGX Orin의 275 TOPS(INT8 기준) 대비 약 7.5배에서 8배에 달하는 성능 향상이며, 단순한 수치적 증가를 넘어 로봇이 처리할 수 있는 모델의 복잡도와 크기가 질적으로 달라졌음을 의미한다.</p>
<h4>2.1.2 메모리 아키텍처와 대역폭의 확장</h4>
<p>거대 모델을 엣지에서 구동할 때 가장 큰 병목 현상은 연산 속도가 아니라 메모리 대역폭에서 발생한다. 모델의 파라미터가 GPU 메모리에 로드되고 연산 유닛으로 이동하는 과정이 지연의 주범이기 때문이다. Jetson Thor는 이를 해결하기 위해 <strong>128GB의 LPDDR5X 메모리</strong>를 탑재하고, 273GB/s의 대역폭을 제공한다.1 이는 수백억 개의 파라미터를 가진 파운데이션 모델(Foundation Model) 전체를 로봇 내부 메모리에 상주시키며 실시간으로 추론할 수 있게 하는 기반이 된다. 이러한 대용량 메모리는 로봇이 클라우드 연결 없이도 복잡한 장기 기억(Long-term Memory)을 유지하고, 과거의 경험을 바탕으로 현재의 상황을 판단하는 연속적인 학습을 가능하게 한다.</p>
<table><thead><tr><th><strong>특징</strong></th><th><strong>NVIDIA Jetson AGX Orin</strong></th><th><strong>NVIDIA Jetson Thor (T5000)</strong></th><th><strong>기술적 함의</strong></th></tr></thead><tbody>
<tr><td><strong>GPU 아키텍처</strong></td><td>Ampere (최대 2048 CUDA Cores)</td><td><strong>Blackwell (2560 CUDA Cores)</strong></td><td>트랜스포머 전용 엔진 탑재로 생성형 AI 가속</td></tr>
<tr><td><strong>AI 성능</strong></td><td>최대 275 TOPS (INT8)</td><td><strong>2070 TFLOPS (FP4)</strong></td><td>FP4 정밀도 지원을 통한 거대 모델 효율성 극대화 (약 7.5배 향상)</td></tr>
<tr><td><strong>CPU</strong></td><td>12-core Arm Cortex-A78AE</td><td><strong>14-core Arm Neoverse-V3AE</strong></td><td>서버급 CPU 코어 도입으로 복잡한 OS 및 제어 로직 처리</td></tr>
<tr><td><strong>메모리</strong></td><td>최대 64GB LPDDR5</td><td><strong>128GB LPDDR5X</strong></td><td>대규모 VLA 모델 및 멀티모달 데이터 로딩 가능</td></tr>
<tr><td><strong>전력 소모</strong></td><td>15W - 60W</td><td><strong>40W - 130W</strong></td><td>고성능을 위한 전력 확장 및 효율성 개선 (3.5배 효율)</td></tr>
<tr><td><strong>주요 기능</strong></td><td>DLA, PVA</td><td><strong>MIG (Multi-Instance GPU)</strong></td><td>안전 중요 작업과 AI 추론 작업의 하드웨어적 격리 지원</td></tr>
</tbody></table>
<p>표 4-1. NVIDIA Jetson AGX Orin과 Jetson Thor의 사양 및 기술적 차이점 비교 1</p>
<h3>2.2  로봇 전용 소프트웨어 스택과 멀티 태스킹: GR00T와 MIG</h3>
<p>하드웨어의 진화는 필연적으로 소프트웨어의 복잡성을 수용하기 위한 방향으로 이루어진다. Jetson Thor는 NVIDIA의 휴머노이드 로봇용 파운데이션 모델인 **GR00T(Generalist Robot 00 Technology)**를 구동하기 위한 플랫폼으로 최적화되었다.2 GR00T는 로봇이 자연어 지시를 이해하고, 인간의 시연 비디오를 보고 학습하며, 복잡한 동작을 생성하는 멀티모달 AI이다. 이러한 모델을 구동하기 위해서는 시각 처리, 언어 해석, 경로 계획, 모터 제어 등 서로 다른 성격의 워크로드가 동시에 실행되어야 한다.</p>
<p>이 지점에서 Jetson Thor가 지원하는 <strong>MIG(Multi-Instance GPU)</strong> 기술이 빛을 발한다. MIG는 하나의 물리적 GPU를 여러 개의 독립적인 인스턴스로 분할하여, 각 인스턴스가 고유한 메모리와 연산 자원을 갖도록 한다.1 예를 들어, 로봇은 하나의 GPU 인스턴스에서 GR00T 모델을 통해 사용자의 명령을 해석하고, 다른 인스턴스에서는 카메라에서 들어오는 장애물 정보를 실시간으로 처리하며, 또 다른 인스턴스에서는 모터 제어를 위한 고속 연산을 수행할 수 있다. 이들은 서로의 리소스를 침범하지 않으므로, 고부하 AI 작업 중에도 안전과 직결된 장애물 회피 기능이 지연되거나 중단되는 치명적인 상황을 방지할 수 있다. 이는 로봇 시스템의 안정성과 신뢰성을 하드웨어 수준에서 보장하는 핵심 기술이다.</p>
<h3>2.3  센서 퓨전과 실시간 I/O 아키텍처</h3>
<p>물리적 AI는 뇌(GPU/CPU)뿐만 아니라 감각 기관(센서)과의 연결성에서도 혁신을 요구한다. 현대의 로봇은 카메라, LiDAR, 레이더, 초음파 센서 등에서 쏟아지는 초당 수 기가바이트의 데이터를 지연 없이 처리해야 한다. Jetson Thor는 이를 위해 **100Gbps 이더넷(4x 25GbE)**과 PCIe Gen5 인터페이스를 탑재하여 대역폭의 한계를 확장했다.2</p>
<p>특히 주목할 점은 <strong>Holoscan Sensor Bridge</strong>와 같은 기술을 통해 센서 데이터를 CPU를 거치지 않고 GPU 메모리로 직접 전송(Direct Memory Access, DMA)하는 구조이다.4 이는 CPU의 오버헤드를 획기적으로 줄이고, 센서 입력에서 행동 출력까지의 ‘Motion-to-Photon’ 지연 시간을 최소화한다. 또한 14코어 Arm Neoverse-V3AE CPU는 차량용 및 산업용 등급의 안전 기준을 충족하는 고성능 코어로, 실시간 운영체제(RTOS) 위에서 결정론적(Deterministic) 제어 주기를 보장한다.1 이는 엣지 컴퓨팅 하드웨어가 단순한 연산 가속기를 넘어, 센서 데이터의 수집부터 판단, 제어에 이르는 전체 파이프라인을 통합하는 중추 신경계로 진화했음을 보여준다.</p>
<h2>3.  생체를 닮은 실리콘: 뉴로모픽 컴퓨팅과 SNN의 부상</h2>
<p>폰 노이만 구조 기반의 기존 컴퓨터(CPU, GPU)가 막대한 전력을 소모하며 AI 모델을 구동하는 동안, 생물학적 뇌는 불과 20W 정도의 에너지로 고도의 인지 작용과 복잡한 신체 제어를 수행한다. 이러한 생물학적 효율성의 비밀은 정보 처리 방식의 근본적인 차이에 있다. 뇌는 클럭에 맞춰 쉴 새 없이 연산하는 것이 아니라, 자극이 있을 때만 전기적 신호(스파이크)를 발생시키는 ’이벤트 기반’으로 작동하며, 기억(메모리)과 연산(프로세서)이 시냅스라는 구조 안에 통합되어 있다. 이러한 원리를 실리콘 칩에 구현하여 로봇에 이식하려는 시도가 바로 <strong>뉴로모픽(Neuromorphic) 컴퓨팅</strong>이며, 이는 하드웨어와 AI 알고리즘의 가장 급진적인 공진화 형태이다.</p>
<h3>3.1  뉴로모픽 하드웨어의 도약: 2025년의 풍경</h3>
<p>2024년과 2025년을 기점으로 뉴로모픽 칩은 오랜 학술적 연구 단계를 넘어 상용화 및 실질적인 응용 단계로 진입하고 있다. 인텔(Intel)의 Loihi 2, 브레인칩(BrainChip)의 Akida, 그리고 IBM의 TrueNorth와 같은 칩셋들은 각기 다른 접근 방식으로 뇌의 작동 원리를 모사하며 로봇 공학의 새로운 가능성을 열고 있다.6</p>
<h4>3.1.1 Intel Loihi 2: 프로그래밍 가능한 신경망의 진화</h4>
<p>인텔의 Loihi 2는 2024년 말 출시되어 2025년 현재 활발히 연구되고 있는 2세대 뉴로모픽 칩으로, 이전 세대 대비 집적도와 유연성이 대폭 향상되었다. Loihi 2는 100만 개 이상의 인공 뉴런을 집적하고 있으며, 각 뉴런의 동작을 마이크로코드(Microcode) 수준에서 프로그래밍할 수 있다는 점이 특징이다.6 이는 연구자들이 다양한 생물학적 뉴런 모델(예: Leaky Integrate-and-Fire 외의 복잡한 모델)을 실험할 수 있게 해준다. 특히 Loihi 2는 스파이크 신호에 단순한 이진(0/1) 정보뿐만 아니라 정수형 가중치(Graded Spikes)를 실어 보낼 수 있어, 정보 전달 효율을 높이고 정밀한 제어를 가능하게 했다. 이는 드론의 자세 제어와 같이 미세한 조정이 필요한 로봇 응용 분야에서 큰 강점을 발휘한다.10</p>
<h4>3.1.2 BrainChip Akida: 엣지에서의 온칩 학습(On-Chip Learning)</h4>
<p>BrainChip의 Akida 프로세서는 철저하게 엣지 디바이스, 즉 전력 제약이 심한 모바일 로봇이나 IoT 센서를 타깃으로 한다. Akida의 가장 큰 혁신은 클라우드 연결이나 외부 컴퓨터의 도움 없이 칩 자체에서 학습(On-chip Learning)이 가능하다는 점이다.6 이는 STDP(Spike Timing Dependent Plasticity)와 같은 생물학적 학습 규칙을 하드웨어적으로 구현했기 때문이다. 로봇은 공장에서 출하된 이후에도 새로운 냄새를 학습하거나, 사용자의 목소리 톤을 구별하는 등 환경에 적응하며 지속적으로 진화할 수 있다. Akida는 밀리와트(mW) 단위의 초저전력으로 동작하며, 웨어러블 기기나 초소형 드론(Micro-drones)의 지능형 제어기로 채택되고 있다.</p>
<h4>3.1.3 IBM TrueNorth와 그 유산</h4>
<p>2014년에 등장한 IBM의 TrueNorth는 뉴로모픽 컴퓨팅의 가능성을 알린 선구적인 칩으로, 100만 개의 뉴런과 2억 5600만 개의 시냅스를 통해 패턴 인식 작업에서 극강의 전력 효율을 입증했다. 비록 최신 칩들에 비해 유연성은 떨어지지만, TrueNorth가 확립한 비동기식 이벤트 구동 아키텍처와 네트워크 온 칩(NoC) 설계 사상은 현재의 뉴로모픽 칩 설계에 지대한 영향을 미쳤다.6</p>
<h3>3.2  스파이킹 신경망(SNN): 시간과 에너지를 연산하다</h3>
<p>하드웨어가 뉴런의 물리적 구조를 모사한다면, 그 위에서 구동되는 소프트웨어 알고리즘은 **스파이킹 신경망(SNN: Spiking Neural Networks)**이다. SNN은 3세대 인공신경망으로 불리며, 정보가 연속적인 실수값(Float)이 아니라 이산적인 스파이크(Spike) 신호의 빈도(Rate Coding)나 발생 타이밍(Temporal Coding)으로 인코딩된다.8</p>
<ul>
<li><strong>시간적 정보 처리의 본질:</strong> 기존 DNN(Deep Neural Networks)은 정적인 이미지를 처리하는 데 강점이 있지만, 시간의 흐름을 명시적으로 다루지는 않는다. 반면 SNN은 스파이크가 발생하는 시간 차(Time-difference) 자체가 정보가 되므로, 동적인 움직임 인식이나 시계열 데이터 처리에 탁월하다. 이는 로봇이 빠르게 움직이는 물체를 추적하거나, 촉각 센서에서 들어오는 진동 패턴을 분석하여 재질을 구분하는 작업에 매우 적합하다.</li>
<li><strong>희소성(Sparsity)과 에너지 효율:</strong> SNN의 뉴런은 입력 신호의 합이 특정 임계치(Threshold)를 넘을 때만 활성화되어 스파이크를 보낸다. 즉, 변화가 없거나 중요하지 않은 정보에 대해서는 침묵한다. 이는 변화가 없는 배경을 보고 있는 로봇의 시각 처리 뉴런이 에너지를 거의 소비하지 않음을 의미한다.12 연구 결과에 따르면, SNN 기반의 로봇 팔 제어 시스템은 기존 딥 강화학습(DRL) 모델 대비 에너지 소비를 획기적으로 줄이면서도 동등한 제어 성능을 달성했다.11</li>
</ul>
<h3>3.3  로보틱스 제어의 패러다임 전환: PID에서 뉴로모픽 제어로</h3>
<p>최근 2024-2025년의 연구들은 SNN을 단순한 인지(Perception) 도구를 넘어 로봇의 <strong>제어(Control)</strong> 영역으로 확장하고 있다. 기존의 로봇 제어는 PID(Proportional-Integral-Derivative) 제어기와 같은 고전 제어 이론에 의존했으나, 이는 복잡한 비선형 동역학을 가진 로봇이나 예측 불가능한 외란이 존재하는 환경에서는 한계를 보인다.</p>
<p>델프트 공과대학교(TU Delft)의 MAVLab 연구팀은 인텔의 Loihi 칩을 탑재한 드론을 통해 세계 최초로 <strong>완전한 뉴로모픽 비전 및 제어 루프</strong>를 실증했다.10 이 연구에서 SNN은 이벤트 카메라에서 들어오는 시각 정보를 처리하여 드론의 3차원 자세(Ego-motion)를 추정하고, 이를 바탕으로 모터의 추력을 제어하는 신호를 생성했다. 놀라운 점은 이 SNN 제어기가 기존의 GPU 기반 딥러닝 제어기보다 30배 이상 적은 전력을 소모하면서도, 5배 더 빠른 제어 주기를 달성했다는 것이다. 또한 SNN의 적응형 학습 능력 덕분에 프로펠러 하나가 손상되거나 갑작스러운 돌풍이 부는 상황에서도 드론이 추락하지 않고 자세를 회복하는 강건성을 보여주었다.13</p>
<p>또 다른 연구에서는 SNN을 심층 강화학습(Deep Reinforcement Learning) 알고리즘인 TD3(Twin Delayed DDPG)와 결합하여 로봇 팔의 3차원 도달(Reaching) 작업을 학습시켰다.14 이 ‘SNN-based TD3’ 모델은 로봇의 관절 제어와 같이 고속, 저지연이 필요한 연속 행동 공간(Continuous Action Space)에서 기존 ANN 기반 모델보다 뛰어난 에너지 효율과 안정성을 입증했다. 이는 뉴로모픽 제어가 로봇의 반사 신경(Reflex)을 담당하는 하위 레벨 제어기로서 PID를 대체하거나 보완할 강력한 대안임을 시사한다.</p>
<h3>3.4  하이브리드 아키텍처: 실용적 타협과 미래</h3>
<p>물리적 AI 시스템이 당장 완전한 뉴로모픽 구조로 전환되기는 어렵다. SNN의 학습 알고리즘(역전파의 어려움 등)이 아직 DNN만큼 성숙하지 않았기 때문이다. 따라서 현재의 가장 유력한 트렌드는 <strong>하이브리드 아키텍처</strong>이다.12</p>
<ul>
<li><strong>대뇌 피질(Cortex) 역할:</strong> Jetson Thor와 같은 고성능 GPU가 담당한다. 복잡한 추론, 경로 계획, 자연어 이해, 장기적인 전략 수립 등 고차원적인 인지 기능을 수행한다.</li>
<li><strong>척수(Spinal Cord) 및 소뇌(Cerebellum) 역할:</strong> 뉴로모픽 칩(Loihi, Akida)이 담당한다. 감각 정보의 전처리, 빠른 반사 반응, 자세 제어, 모터 협응 등 저지연과 고효율이 필요한 감각-운동 루프를 처리한다.</li>
</ul>
<p>이러한 하이브리드 구조는 생물학적 신경계의 효율적인 역할 분담을 로봇 하드웨어 아키텍처에 구현한 것으로, 로봇이 고성능 지능과 민첩한 신체 능력을 동시에 갖추게 하는 현실적인 해법이 되고 있다.</p>
<h2>4.  감각의 진화: 이벤트 기반 비전과 인센서 컴퓨팅</h2>
<p>로봇이 세상을 인식하는 방식 또한 하드웨어와 AI의 공진화를 통해 근본적인 변화를 겪고 있다. 지난 수십 년간 로봇 시각의 표준이었던 프레임 기반 카메라(Frame-based Camera)는 초당 30~60장의 정지 영상을 연속으로 촬영하는 방식이다. 이 방식은 인간의 영화 감상에는 적합하지만, 고속으로 움직이는 로봇이나 조도 변화가 극심한 환경에서는 치명적인 한계를 드러냈다. 빠른 움직임에서 발생하는 ’모션 블러(Motion Blur)’와 터널 진출입 시의 ‘동적 범위(Dynamic Range)’ 부족 문제가 그것이다. 이에 대한 해답으로 생물학적 망막을 모방한 **이벤트 카메라(Event Camera)**와 센서 자체가 연산을 수행하는 <strong>인센서(In-Sensor) 컴퓨팅</strong>이 부상하고 있다.</p>
<h3>4.1  이벤트 카메라(DVS): 망막을 모방한 시각 혁명</h3>
<p>이벤트 카메라(또는 동적 비전 센서, DVS: Dynamic Vision Sensor)는 기존 카메라와 전혀 다른 원리로 작동한다. 프레임 카메라는 모든 픽셀의 밝기 값을 정해진 시간 간격마다 동기적으로 읽어내는 반면, 이벤트 카메라는 각 픽셀이 독립적인 밝기 변화 감지기로 작동한다. 특정 픽셀에서 밝기가 설정된 임계값 이상 변할 때만 해당 픽셀의 좌표(x, y), 시간(t), 극성(p, 밝아짐/어두워짐) 정보를 비동기적으로 전송한다.15</p>
<p>이러한 작동 원리는 로봇 시스템에 다음과 같은 혁신적인 특성을 부여한다:</p>
<ol>
<li><strong>마이크로초(µs) 단위의 초저지연:</strong> 프레임 전체 노출을 기다릴 필요 없이 변화가 일어난 즉시 신호를 보내므로, 기존 카메라보다 1,000배 이상 빠른 반응 속도를 제공한다. 이는 드론이 장애물을 회피하거나, 로봇 팔이 날아오는 공을 잡는(High-speed Catching) 것과 같은 초고속 제어 시스템의 필수 조건이다.18</li>
<li><strong>모션 블러의 제거:</strong> 셔터가 열리고 닫히는 개념이 없으므로, 아무리 빠른 물체라도 블러 현상 없이 선명한 궤적 정보를 얻을 수 있다. 이는 고속 주행 중인 자율주행차나 빠르게 회전하는 드론의 위치 추정(SLAM) 정확도를 획기적으로 높인다.</li>
<li><strong>광대역 동적 범위(HDR):</strong> 각 픽셀이 독립적으로 노출을 조절하는 것과 같으므로, 140dB 이상의 높은 동적 범위를 제공한다. 어두운 밤에 가로등 불빛을 보거나, 터널 안에서 밖으로 나가는 역광 상황에서도 정보를 잃지 않는다.19</li>
<li><strong>데이터 효율성:</strong> 정지한 배경이나 변화가 없는 영역의 데이터는 전송하지 않으므로 불필요한 데이터 대역폭과 전력 소모를 줄인다. 이는 데이터 희소성(Sparsity)을 활용하는 뉴로모픽 프로세서와 완벽한 궁합을 이룬다.</li>
</ol>
<h3>4.2  이벤트 기반 알고리즘과의 공진화: 2025년의 SOTA</h3>
<p>이벤트 카메라는 하드웨어만으로는 그 잠재력을 발휘할 수 없다. 기존의 컴퓨터 비전 알고리즘(CNN 등)은 밀집된 행렬(Frame) 형태의 입력을 가정하므로, 희소하고 비동기적인 이벤트 스트림을 처리할 수 있는 새로운 AI 알고리즘이 필요했다. 하드웨어의 변화가 알고리즘의 진화를 강제한 대표적인 사례이다.</p>
<ul>
<li><strong>비동기 처리 및 재구성 알고리즘:</strong> 초기에는 이벤트를 일정 시간 모아 이미지처럼 만드는 방식이 쓰였으나, 최근에는 이벤트를 **시간 표면(Time Surface)**으로 변환하거나, <strong>그래프 신경망(GNN)</strong> 및 **스파이킹 신경망(SNN)**에 직접 입력하여 처리하는 방식이 주류를 이루고 있다.11 이를 통해 이벤트 카메라의 시간적 해상도를 잃지 않고 그대로 활용할 수 있게 되었다.</li>
<li><strong>고속 제어 및 내비게이션:</strong> 2025년 최신 연구들은 이벤트 카메라와 강화학습(RL)을 결합하여 극한의 성능을 끌어내고 있다. 예를 들어, <strong>이벤트 기반 강화학습 내비게이션</strong> 연구는 보행자가 많은 복잡한 환경에서 모션 블러 없이 장애물을 인식하고 회피하는 로봇 제어기를 개발했다.21 또한 취리히 대학(UZH)의 연구팀은 이벤트 카메라를 이용해 쿼드콥터가 좁은 파이프 안에서 발생하는 난기류와 공기 역학적 외란을 실시간으로 감지하고, 이에 대응하여 안정적으로 호버링하는 제어 시스템을 발표했다.23 이는 공기의 흐름과 같은 눈에 보이지 않는 미세한 변화까지 감지하여 제어에 반영하는 ‘초감각적’ 로봇의 가능성을 보여준다.</li>
</ul>
<h3>4.3  인센서 컴퓨팅(In-Sensor Computing): 센서 자체가 AI가 되다</h3>
<p>비전 센서의 또 다른 진화 축은 이미지 센서 내부에 AI 프로세서를 통합하는 <strong>인센서 컴퓨팅</strong>이다. Sony의 <strong>IMX500/501</strong> 지능형 비전 센서는 이러한 트렌드를 선도하는 제품으로, 픽셀 칩 하단에 AI 로직 칩을 3차원으로 적층(Stacked)한 구조를 가진다.24</p>
<h4>4.3.1 메커니즘과 이점</h4>
<p>기존 방식은 센서가 이미지를 찍어 CPU나 GPU로 보내면, 프로세서가 AI 모델을 돌려 분석하는 식이었다. 이 과정에서 막대한 데이터가 전송되며 전력과 대역폭을 소모하고 지연 시간이 발생했다. 반면, IMX500은 센서 내부의 로직 칩이 이미지를 획득하는 즉시 AI 추론을 수행한다.</p>
<p>이 센서는 원본 이미지를 밖으로 내보내는 대신, “사람이 3명 있다”, “좌표 (x, y)에 제품 결함이 있다“와 같은 **메타데이터(Metadata)**만을 출력할 수 있다. 이는 다음과 같은 이점을 제공한다:</p>
<ol>
<li><strong>개인정보 보호(Privacy):</strong> 얼굴이나 신체가 촬영된 원본 영상이 센서 밖으로 나가지 않으므로, CCTV나 공공장소 모니터링에서의 프라이버시 침해 우려를 원천적으로 차단한다.24 로마 시의 스마트 시티 프로젝트에 이 센서가 도입된 이유이기도 하다.27</li>
<li><strong>대역폭 및 전력 절감:</strong> 이미지 데이터 대신 텍스트 형태의 메타데이터만 전송하므로 데이터 전송량을 수천 배에서 수만 배 줄일 수 있다. 이는 통신 비용을 절감하고 전체 시스템의 전력 효율을 높인다.</li>
<li><strong>초고속 안전 반응:</strong> 협동 로봇(Cobot)에 적용될 경우, 작업자가 위험 구역에 접근하는 것을 센서 자체적으로 3.1ms 만에 감지하고 로봇을 정지시키는 신호를 보낼 수 있다.26 이는 클라우드나 중앙 제어기를 거치는 방식보다 훨씬 빠르고 신뢰성 높은 안전 장치로 기능한다.</li>
</ol>
<h2>5.  형태학적 연산(Morphological Computation): 신체가 곧 뇌다</h2>
<p>지금까지 논의한 엣지 컴퓨팅, 뉴로모픽 칩, 이벤트 비전이 ’전자적 두뇌’와 ’알고리즘’의 공진화라면, **형태학적 연산(Morphological Computation)**은 로봇의 ‘물리적 신체’ 그 자체가 연산의 주체로 참여하는 패러다임이다. 이는 “지능은 뇌에만 존재하는 것이 아니라, 뇌-신체-환경의 역동적인 상호작용 속에 존재한다“는 체화된 지능 이론을 하드웨어적으로 구현한 것이다.28</p>
<h3>5.1  정의 및 원리: 단순성(Simplexity)의 미학</h3>
<p>형태학적 연산은 로봇의 기계적 구조, 재료의 특성(탄성, 점성, 마찰 등), 그리고 동역학적 배열을 활용하여 정보 처리를 수행하거나 제어의 복잡성을 신체로 이관(Offloading)하는 것을 의미한다.28</p>
<p>이 개념의 핵심은 **단순성(Simplexity)**이다. 생물학적 시스템은 복잡한 제어 신호 없이도 신체의 구조적 특성 덕분에 자연스럽고 효율적인 동작을 만들어낸다. 예를 들어, 문어는 뇌에서 각 빨판과 근육을 일일이 제어하지 않는다. 대신 유연한 다리의 물성 자체가 물의 저항과 상호작용하며 복잡한 움직임을 만들어낸다.31 로봇 공학에서 이를 모방한 **소프트 그리퍼(Soft Gripper)**는 대상 물체의 정확한 3D 모델이나 파지점을 계산할 필요 없이, 유연한 소재가 물체를 감싸 쥐는 것만으로 ’형상 적응(Shape Adaptation)’이라는 복잡한 기하학적 연산을 물리적으로 수행한다. 이는 중앙 제어기(Brain)가 처리해야 할 막대한 연산 부하를 신체(Body)가 대신 처리해 주는 셈이다.</p>
<h3>5.2  수동 동역학(Passive Dynamics)과 리저버 컴퓨팅</h3>
<p>형태학적 연산의 또 다른 예는 <strong>수동 동역학</strong>을 이용한 보행이다. 인간이나 동물의 걷기는 근육의 힘뿐만 아니라 다리의 진자 운동(Swing)과 중력을 적절히 이용한다. 이를 모방한 ‘패시브 워커(Passive Walker)’ 로봇은 모터 없이도 경사면을 자연스럽게 걸어 내려갈 수 있다. 현대의 로봇들은 이러한 원리를 적용하여, 다리의 스프링 탄성을 이용해 에너지를 저장하고 방출함으로써 보행 에너지 효율을 극대화한다.29</p>
<p>더 나아가, 소프트 로봇의 신체 자체를 신경망의 일부로 활용하는 <strong>물리적 리저버 컴퓨팅(Physical Reservoir Computing)</strong> 연구도 진행되고 있다. 문어 다리 로봇의 유연한 신체가 가지는 비선형적인 동역학적 반응을 ’리저버(Reservoir, 저장소)’로 활용하여, 복잡한 시계열 제어 신호를 생성하거나 환경을 인식하는 데 사용하는 것이다.32 이는 하드웨어가 단순한 구동기가 아니라, 그 자체로 연산 능력을 가진 신경망의 레이어처럼 작동함을 의미한다.</p>
<h2>6.  뇌-신체 동시 최적화(Brain-Body Co-Design)</h2>
<p>하드웨어와 AI의 공진화는 이제 설계 단계에서의 자동화로 이어지고 있다. 과거에는 기계 공학자가 로봇의 몸체를 설계하고 제작하면, AI 연구자가 그 고정된 몸체에 맞는 제어 알고리즘(Brain)을 학습시키는 순차적인 방식이었다. 하지만 **뇌-신체 동시 최적화(Brain-Body Co-Design)**는 로봇의 형태(Morphology)와 제어 정책(Control Policy)을 하나의 거대한 최적화 문제로 묶어 동시에 진화시키는 방법론이다.33</p>
<h3>6.1  최신 연구 동향: RoboCraft와 RoboMoRe</h3>
<p>2024년과 2025년 발표된 주요 연구들은 이러한 공진화 설계를 가속화하고 있다. 이는 진화 알고리즘(Evolutionary Algorithm)과 심층 강화학습(DRL)을 결합하여, 주어진 임무에 가장 적합한 신체와 뇌를 찾아내는 과정이다.</p>
<ul>
<li><strong>RoboCraft (2025):</strong> 이 연구는 휴머노이드 로봇의 ‘넘어짐 회복(Fall Recovery)’ 능력을 향상시키기 위해 제어 정책과 신체 구조를 동시에 최적화하는 프레임워크를 제안했다. 기존에는 넘어진 로봇을 일으키기 위해 복잡한 제어 알고리즘을 개발하는 데 집중했지만, RoboCraft는 로봇의 팔 길이, 관절의 배치, 무게 중심 등 신체 구조를 함께 변경하며 실험했다. 그 결과, 최적화된 신체 구조가 제어 성능을 평균 44.55% 향상시킬 수 있음을 입증했다. 특히 놀라운 점은 성능 향상의 40% 이상이 제어 알고리즘의 개선이 아닌, 순수하게 신체 형태의 최적화에서 비롯되었다는 사실이다.33 이는 하드웨어 설계가 지능적 행위의 성공 여부에 얼마나 결정적인 영향을 미치는지를 정량적으로 보여준다.</li>
<li><strong>RoboMoRe:</strong> 이 프레임워크는 거대 언어 모델(LLM)의 창의성을 로봇 설계에 도입했다. LLM이 로봇의 구조적 다양성을 제안하고 보상 함수(Reward Function)를 설계하면, 진화 알고리즘이 이를 물리 시뮬레이션 상에서 검증하고 정제하는 ‘Coarse-to-Fine’ 전략을 사용한다. 이를 통해 인간 엔지니어의 편향(Bias)에 갇히지 않은, 창의적이고 효율적인 로봇 형태와 그에 맞는 동작을 동시에 도출해냈다.35</li>
</ul>
<h3>6.2  하드웨어 인식 신경망 탐색 (Hardware-Aware NAS)</h3>
<p>로봇의 물리적 형태뿐만 아니라, 로봇에 탑재될 컴퓨팅 칩의 특성에 맞춰 신경망 구조를 최적화하는 연구도 공진화의 중요한 축이다. **하드웨어 인식 신경망 탐색(Hardware-Aware NAS)**은 타겟 하드웨어(예: Jetson Thor, FPGA, 모바일 AP)의 지연 시간(Latency), 전력 소모, 메모리 제약 등을 고려하여 가장 적합한 신경망 아키텍처를 자동으로 탐색한다.38</p>
<p>최신 연구인 <strong>RAM-NAS</strong> (IROS 2024/2025)는 로봇의 엣지 하드웨어 리소스를 실시간으로 인식하여, 추론 속도와 정확도의 최적 균형점을 찾아내는 다목적 진화 알고리즘을 제안했다.38 이는 제한된 배터리와 컴퓨팅 자원을 가진 모바일 로봇이 자신의 하드웨어 상태에 맞춰 지능의 수준이나 모델의 크기를 동적으로 조절할 수 있는 가능성을 열어준다.</p>
<h2>7.  결론: 통합된 지능체를 향하여</h2>
<p>하드웨어와 AI의 공진화는 로봇을 더 이상 ’센서, 액추에이터, 컴퓨터의 단순한 기계적 집합체’가 아닌 **‘통합된 지능체(Unified Intelligent Agent)’**로 진화시키고 있다. 챕터 4에서 살펴본 기술적 흐름들은 다음과 같은 미래를 가리키고 있다.</p>
<p>첫째, **지능의 편재화(Ubiquity of Intelligence)**이다. 로봇의 지능은 중앙의 고성능 GPU(Jetson Thor)에만 머무르지 않는다. 말초 신경계에 해당하는 뉴로모픽 칩, 감각 기관인 인센서 컴퓨팅 및 이벤트 카메라, 그리고 물리적 신체 구조(형태학적 연산) 전체에 지능과 연산 기능이 분산되어 존재하게 된다. 이는 로봇 전체가 하나의 유기적인 신경망처럼 작동함을 의미한다.</p>
<p>둘째, <strong>효율성의 극대화와 생존성</strong>이다. 생물학적 모방(Biomimicry)과 뉴로모픽 기술을 통해 전력 소모는 줄어들고 반응 속도는 빨라진다. 이는 로봇이 통제된 실험실이나 공장을 벗어나, 전력 공급이 없는 야생(Wild)이나 재난 현장과 같은 극한 환경에서도 장시간 생존하고 임무를 수행할 수 있는 기반이 된다.</p>
<p>셋째, <strong>설계의 자동화와 다양성</strong>이다. AI가 스스로 자신의 몸과 뇌를 설계하는 Co-Design 기술은 인간의 직관과 상상력을 뛰어넘는 로봇 형태의 등장을 예고한다. 우리는 곧 특정 임무를 위해 최적화된, 우리가 상상하지 못했던 기괴하고도 아름다운 형태의 로봇들을 마주하게 될 것이다.</p>
<p>결론적으로, 우리는 지금 ’실리콘 지능’이 진정한 ’물리적 실체’를 얻어가는 거대한 과도기에 서 있다. 하드웨어와 소프트웨어의 경계가 허물어지고 서로가 서로를 정의하며 발전하는 공진화의 흐름 속에서, 차세대 로봇은 단순한 도구를 넘어 인간과 공존하는 새로운 지적 존재로 거듭날 것이다.</p>
<p><strong>표 4-2. 하드웨어와 AI 공진화의 주요 기술 요약 및 전망</strong></p>
<table><thead><tr><th><strong>기술 분야</strong></th><th><strong>기존 패러다임 (Conventional)</strong></th><th><strong>공진화 패러다임 (Co-Evolutionary Future)</strong></th><th><strong>대표 기술 및 제품</strong></th><th><strong>핵심 가치 및 로봇 공학적 함의</strong></th></tr></thead><tbody>
<tr><td><strong>컴퓨팅 아키텍처</strong></td><td>범용 GPU, 폰 노이만 구조</td><td><strong>AI 전용 가속기, 트랜스포머 엔진</strong></td><td>NVIDIA Jetson Thor, Blackwell</td><td>생성형 AI의 엣지 구동, VLA 모델 가속, 안전 중요 제어의 격리(MIG)</td></tr>
<tr><td><strong>뉴로모픽 &amp; 제어</strong></td><td>동기식 연산, PID 제어</td><td><strong>비동기 스파이킹(SNN), 학습형 제어</strong></td><td>Intel Loihi 2, BrainChip Akida</td><td>에너지 효율 극대화, 고속 적응형 제어, 엣지 온칩 학습</td></tr>
<tr><td><strong>비전 시스템</strong></td><td>프레임 기반 (30/60fps)</td><td><strong>이벤트 기반(DVS), 인센서 컴퓨팅</strong></td><td>Sony IMX500, Prophesee Event Camera</td><td>초저지연(µs), 모션 블러 제거, 프라이버시 보호, 대역폭 절감</td></tr>
<tr><td><strong>로봇 설계</strong></td><td>하드웨어 고정 -&gt; SW 최적화</td><td><strong>Brain-Body 동시 최적화 (Co-Design)</strong></td><td>RoboCraft, RoboMoRe, Soft Robotics</td><td>신체 지능(Morphological Computation) 활용, 설계 자동화 및 최적화</td></tr>
</tbody></table>
<h2>8. 참고 자료</h2>
<ol>
<li>NVIDIA Jetson AGX Thor module - Auvidea, https://auvidea.eu/download/robotics-edgeai-datasheet-jetson-thor-modules-nvidia-us-web.pdf</li>
<li>Jetson Thor | Advanced AI for Physical Robotics - NVIDIA, https://www.nvidia.com/en-au/autonomous-machines/embedded-systems/jetson-thor/</li>
<li>NVIDIA Jetson Thor: New SoC Features Guide, https://developer.ridgerun.com/wiki/index.php/NVIDIA_Jetson_Thor:_Powering_the_Future_of_Physical_AI</li>
<li>NVIDIA Jetson AGX Thor Dev Kit | 2070 TFLOPS Platform … - DFRobot, https://www.dfrobot.com/product-2952.html</li>
<li>NVIDIA Jetson AGX Thor Developer Kit - Silicon Highway, https://www.siliconhighway.com/wp-content/robotics-and-edge-ai-datasheet-jetson-thor-devkit-nvidia-us-web.pdf</li>
<li>Top Neuromorphic Chips in 2025 : Akida, Loihi &amp; TrueNorth, https://www.elprocus.com/top-neuromorphic-chips-in-2025/</li>
<li>Neuromorphic Chips Market Size, Share 2032, https://www.fortunebusinessinsights.com/neuromorphic-chips-market-111466</li>
<li>Neuromorphic Computing for Embodied Intelligence in Autonomous …, https://arxiv.org/html/2507.18139v1</li>
<li>Taking Neuromorphic Computing to the Next Level with Loihi 2 - Intel, https://download.intel.com/newsroom/2021/new-technologies/neuromorphic-computing-loihi-2-brief.pdf</li>
<li>Fully neuromorphic vision and control for autonomous drone flight, <a href="https://www.semanticscholar.org/paper/Fully-neuromorphic-vision-and-control-for-drone-Paredes-Vall%C3%A9s-Hagenaars/0c9c6f2c4182e888b3012f22d7dc086aadbbb886">https://www.semanticscholar.org/paper/Fully-neuromorphic-vision-and-control-for-drone-Paredes-Vall%C3%A9s-Hagenaars/0c9c6f2c4182e888b3012f22d7dc086aadbbb886</a></li>
<li>Population-Coded Spiking Neural Networks for High-Dimensional …, https://arxiv.org/html/2510.10516v1</li>
<li>Neuromorphic Computing and the Future Robotics - Medium, https://medium.com/@ConcernedhumanonAI/neuromorphic-computing-and-the-future-robotics-9e2f6f6daad1</li>
<li>Fully neuromorphic vision and control for autonomous drone flight, https://mavlab.tudelft.nl/fully_neuromorphic_drone/</li>
<li>Designing Spiking Neural Network-Based Reinforcement Learning …, https://www.mdpi.com/2079-9292/14/3/578</li>
<li>Guillermo Gallego - Event-based Vision - Google Sites, https://sites.google.com/view/guillermogallego/research/event-based-vision?authuser=0</li>
<li>CVPR 2025 Workshop on Event-based Vision - GitHub Pages, https://tub-rip.github.io/eventvision2025/</li>
<li>Event-based Vision, Event Cameras, Event Camera SLAM, https://rpg.ifi.uzh.ch/research_dvs.html</li>
<li>Real-time Event-based Tracking for Low-Latency Robot Interaction …, https://research.manchester.ac.uk/en/studentTheses/real-time-event-based-tracking-for-low-latency-robot-interaction-/</li>
<li>Emerging Trends and Applications of Neuromorphic Dynamic Vision …, https://ieeexplore.ieee.org/document/10795229/</li>
<li>Neuromorphic Computing 2025: Current SotA - human / unsupervised, https://humanunsupervised.com/papers/neuromorphic_landscape.html</li>
<li>Human-Robot Navigation using Event-based Cameras and … - arXiv, https://arxiv.org/html/2506.10790v1</li>
<li>Human-Robot Navigation using Event-based Cameras and …, https://openaccess.thecvf.com/content/CVPR2025W/EventVision/papers/Bugueno-Cordova_Human-Robot_Navigation_using_Event-based_Cameras_and_Reinforcement_Learning_CVPRW_2025_paper.pdf</li>
<li>Robotics and Perception Group, https://rpg.ifi.uzh.ch/</li>
<li>Intelligent Vision Sensors “IMX500/501”: Edge Solutions That Help …, https://www.challenge-zero.jp/en/casestudy/820</li>
<li>Triton SMART AI Camera with Sony IMX501 - LUCID Vision Labs, https://thinklucid.com/triton-smart-ai-camera-with-sony-imx501/</li>
<li>Will SONY’s New AI-Image Sensor Make Robots Smarter?, https://asianroboticsreview.com/home410-html</li>
<li>A video of a smart city trial project in Rome, Italy, using the intelligent …, https://www.sony-semicon.com/en/info/2021/2021071401.html</li>
<li>Morphological Computation Explained - Emergent Mind, https://www.emergentmind.com/topics/morphological-computation</li>
<li>Morphological computation | Soft Robotics Class Notes - Fiveable, https://fiveable.me/soft-robotics/unit-4/morphological-computation/study-guide/3BpwMBnwRS7iAIYD</li>
<li>Exploring Embodied Intelligence in Soft Robotics: A Review - MDPI, https://www.mdpi.com/2313-7673/9/4/248</li>
<li>The Symbiosis of Morphological Computation and Soft Robotics, https://ieeexplore.ieee.org/iel7/100/4600619/07552464.pdf</li>
<li>Editorial: Recent Trends in Morphological Computation - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC8202073/</li>
<li>Joint Optimization of Control and Morphology for Fall Recovery - arXiv, https://arxiv.org/pdf/2510.22336</li>
<li>Joint Optimization of Control and Morphology for Fall Recovery, https://www.researchgate.net/publication/396967924_Toward_Humanoid_Brain-Body_Co-design_Joint_Optimization_of_Control_and_Morphology_for_Fall_Recovery</li>
<li>RoboMoRe: LLM-based Robot Co-design via Joint Optimization of …, https://www.researchgate.net/publication/392334496_RoboMoRe_LLM-based_Robot_Co-design_via_Joint_Optimization_of_Morphology_and_Reward</li>
<li>Toward Humanoid Brain-Body Co-design: Joint Optimization of …, https://chatpaper.com/paper/203878</li>
<li>RoboMoRe: LLM-based Robot Co-design via Joint Optimization of …, https://arxiv.org/abs/2506.00276</li>
<li>RAM-NAS: Resource-aware Multiobjective Neural Architecture …, https://arxiv.org/abs/2509.20688</li>
<li>Hardware-Aware Neural Architecture Search: Survey and Taxonomy, https://www.ijcai.org/proceedings/2021/592</li>
<li>A Comprehensive Survey on Hardware-Aware Neural Architecture …, https://arxiv.org/abs/2101.09336</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>