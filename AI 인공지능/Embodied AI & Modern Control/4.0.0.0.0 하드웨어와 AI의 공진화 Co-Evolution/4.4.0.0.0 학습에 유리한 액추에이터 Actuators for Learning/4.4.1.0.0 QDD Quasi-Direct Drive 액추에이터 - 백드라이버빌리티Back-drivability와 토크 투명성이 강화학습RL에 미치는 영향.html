<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:4.4.1 QDD (Quasi-Direct Drive) 액추에이터: 백드라이버빌리티(Back-drivability)와 토크 투명성이 강화학습(RL)에 미치는 영향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>4.4.1 QDD (Quasi-Direct Drive) 액추에이터: 백드라이버빌리티(Back-drivability)와 토크 투명성이 강화학습(RL)에 미치는 영향</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 4. 하드웨어와 AI의 공진화 (Co-Evolution)</a> / <a href="index.html">4.4 학습에 유리한 액추에이터 (Actuators for Learning)</a> / <span>4.4.1 QDD (Quasi-Direct Drive) 액추에이터: 백드라이버빌리티(Back-drivability)와 토크 투명성이 강화학습(RL)에 미치는 영향</span></nav>
                </div>
            </header>
            <article>
                <h1>4.4.1 QDD (Quasi-Direct Drive) 액추에이터: 백드라이버빌리티(Back-drivability)와 토크 투명성이 강화학습(RL)에 미치는 영향</h1>
<p>현대 로봇 공학, 특히 사족 보행 로봇(Quadrupedal Robots)과 휴머노이드의 동적 보행(Dynamic Locomotion) 기술이 비약적으로 발전한 배경에는 인공지능(AI) 알고리즘의 진보뿐만 아니라 하드웨어 패러다임의 근본적인 전환이 존재한다. 그 중심에는 유사 직접 구동, 즉 <strong>QDD(Quasi-Direct Drive)</strong> 액추에이터가 있다. 심층 강화학습(Deep Reinforcement Learning, DRL) 기반의 제어 정책(Policy)이 시뮬레이션 환경을 넘어 실제 물리 세계(Real World)에서 성공적으로 작동하기 위해서는 ’Sim-to-Real Gap(시뮬레이션과 실제 환경의 차이)’을 최소화하는 것이 필수적이다. QDD 액추에이터는 기계적 임피던스(Mechanical Impedance)를 최소화하고 높은 백드라이버빌리티(Back-drivability)와 토크 투명성(Torque Transparency)을 제공함으로써, 이 간극을 하드웨어 레벨에서 해결하는 핵심 솔루션으로 자리 잡았다.</p>
<p>본 섹션에서는 QDD의 물리적 특성이 강화학습 알고리즘의 학습 효율성과 강건성(Robustness)에 미치는 영향을 심층적으로 분석하고, MIT Cheetah 시리즈와 같은 최신 SOTA(State-of-the-Art) 로봇들의 사례를 통해 하드웨어와 학습 알고리즘의 공진화(Co-evolution) 과정을 고찰한다.</p>
<h2>1.  QDD 액추에이터의 물리적 정의와 동역학적 특성</h2>
<p>전통적인 산업용 로봇(Industrial Manipulators)은 위치 제어(Position Control)의 정밀도를 극대화하기 위해 높은 감속비(High Gear Ratio, 통상 100:1 이상)의 기어박스를 사용해왔다. 하모닉 드라이브(Harmonic Drive)나 사이클로이드 감속기(Cycloidal Drive)와 같은 고감속 기어는 작은 모터로 높은 토크를 생성할 수 있게 해주지만, 모터 로터(Rotor)의 회전 관성(Inertia)을 증폭시켜 외부 충격에 취약하게 만들고, 복잡한 마찰과 백래시(Backlash)로 인해 토크 제어를 어렵게 한다는 근본적인 한계를 가진다.1</p>
<p>반면, QDD는 낮은 감속비(통상 6:1 ~ 10:1 수준)의 유성 기어(Planetary Gear)와 높은 토크 밀도를 가진 BLDC 모터(주로 Pancake type 또는 Out-runner type)를 결합하는 방식을 취한다.1 이러한 설계는 ’위치 정밀도’보다는 ’힘(Force) 제어 능력’과 ’동적 상호작용(Dynamic Interaction)’에 최적화된 하드웨어 구성을 지향한다.</p>
<h3>1.1 반사 관성(Reflected Inertia)과 <span class="math math-inline">N^2</span> 법칙의 함의</h3>
<p>QDD 설계 철학의 핵심은 감속비(<span class="math math-inline">N</span>)와 반사 관성(<span class="math math-inline">J_{eff}</span>) 사이의 물리적 관계에 있다. 로봇 조인트의 출력축(Output Shaft)에서 외부 환경으로 느껴지는 유효 관성, 즉 반사 관성은 감속비의 제곱에 비례하여 증가한다.<br />
<span class="math math-display">
J_{eff} = J_{load} + N^2 J_{motor}
</span><br />
여기서 <span class="math math-inline">J_{motor}</span>는 모터 로터 자체의 관성이고, <span class="math math-inline">J_{load}</span>는 부하 관성이다. 이 수식은 감속비 <span class="math math-inline">N</span>이 커질수록 모터 자체의 관성이 출력축에 미치는 영향이 기하급수적으로(<span class="math math-inline">N^2</span>) 커짐을 의미한다.3</p>
<p>예를 들어, 감속비가 100:1인 기존 산업용 액추에이터는 10:1인 QDD 대비 이론적으로 모터 관성의 영향이 <span class="math math-inline">10^2</span>배, 즉 100배 더 크게 작용한다. 높은 반사 관성은 로봇의 다리가 지면에 닿을 때 발생하는 충격 에너지가 모터의 회전 운동으로 변환되지 못하고 기어박스 내부의 응력으로 축적되게 만든다. 이는 기어 파손의 원인이 되며, 강화학습 에이전트가 예측하지 못한 비선형적인 반발력을 발생시켜 학습의 수렴을 방해한다.</p>
<p>QDD는 <span class="math math-inline">N</span>을 최소화함으로써 <span class="math math-inline">J_{eff}</span>를 획기적으로 낮춘다. 이는 로봇의 다리가 지면에 충돌(Impact)할 때 발생하는 급격한 외력이 기어박스나 모터에 축적되지 않고, 모터의 회전 운동으로 자연스럽게 변환되어 ‘도망갈(Backdrive)’ 수 있음을 의미한다. 이를 **기계적 투명성(Mechanical Transparency)**이라 하며, 이 특성 덕분에 별도의 물리적 스프링 없이도 소프트웨어적으로 컴플라이언스(Compliance)를 구현할 수 있게 된다.5</p>
<h3>1.2 임팩트 완화 지수 (Impact Mitigation Factor, IMF)</h3>
<p>MIT Cheetah 연구팀은 이러한 QDD의 충격 흡수 능력을 정량화하기 위해 **임팩트 완화 지수(Impact Mitigation Factor, IMF)**라는 새로운 지표를 도입하였다.10 IMF는 로봇의 다리가 지면과 충돌하는 순간, 시스템이 얼마나 효과적으로 충격력(Impact Force)을 흡수하고 하드웨어를 보호할 수 있는지를 나타낸다.<br />
<span class="math math-display">
\text{IMF} = \frac{\text{Impact Force with Gearbox}}{\text{Impact Force without Gearbox (Ideal)}}
</span><br />
<em>Proprioceptive Actuator Design in the MIT Cheetah</em> 12 논문에 따르면, QDD 기반의 MIT Cheetah 다리는 물리적 스프링을 직렬로 연결하여 충격을 흡수하는 **직렬 탄성 액추에이터(Series Elastic Actuator, SEA)**와 유사한 수준의 충격 완화 능력을 보여준다. 그러나 SEA가 스프링의 물리적 특성으로 인해 제어 대역폭(Control Bandwidth)의 손실(Low-pass filtering effect)을 겪는 반면, QDD는 높은 대역폭을 유지하면서도 충격을 완화할 수 있다는 결정적인 차별점을 가진다.1</p>
<p>강화학습 관점에서 대역폭은 매우 중요하다. 동적 보행이나 백플립과 같은 고난이도 동작을 학습할 때, 정책 네트워크(Policy Network)는 50Hz에서 100Hz 이상의 높은 주기로 제어 명령을 출력한다. 액추에이터의 대역폭이 낮으면 이러한 고주파 명령을 하드웨어가 물리적으로 이행하지 못하게 되며, 이는 상태-행동(State-Action) 쌍의 불일치를 초래하여 학습 성능을 저하시킨다. QDD는 40Hz에서 100Hz에 이르는 높은 토크 대역폭을 제공함으로써 SOTA 강화학습 알고리즘이 요구하는 민첩성(Agility)을 충족시킨다.10</p>
<h3>1.3 액추에이터 기술별 비교 분석</h3>
<p>강화학습 기반 로봇 제어의 관점에서 주요 액추에이터 방식의 장단점을 비교하면 아래의 표와 같다.</p>
<table><thead><tr><th><strong>특성</strong></th><th><strong>QDD (Quasi-Direct Drive)</strong></th><th><strong>SEA (Series Elastic Actuator)</strong></th><th><strong>High-Ratio Geared (Conventional)</strong></th></tr></thead><tbody>
<tr><td><strong>감속비 (<span class="math math-inline">N</span>)</strong></td><td>낮음 (6:1 ~ 10:1)</td><td>중간 ~ 높음</td><td>높음 (50:1 ~ 100:1 이상)</td></tr>
<tr><td><strong>백드라이버빌리티</strong></td><td><strong>매우 높음</strong></td><td>높음 (스프링 변형 한도 내)</td><td>매우 낮음 (불가능에 가까움)</td></tr>
<tr><td><strong>토크 투명성</strong></td><td>높음 (전류 <span class="math math-inline">\propto</span> 토크)</td><td>높음 (스프링 변위 측정)</td><td>낮음 (마찰, 비선형성 지배적)</td></tr>
<tr><td><strong>제어 대역폭</strong></td><td><strong>높음 (&gt; 50-100Hz)</strong></td><td>낮음 (스프링 고유진동수 제한)</td><td>낮음 (관성 및 마찰 제한)</td></tr>
<tr><td><strong>충격 흡수 메커니즘</strong></td><td>모터 관성 회피 (Backdriving)</td><td>물리적 스프링 에너지 저장</td><td>흡수 불가 (파손 위험 높음)</td></tr>
<tr><td><strong>Sim-to-Real 난이도</strong></td><td><strong>낮음 (강체 모델 근사 용이)</strong></td><td>높음 (스프링 동역학 모델링 필요)</td><td>매우 높음 (마찰 모델링 난해)</td></tr>
<tr><td><strong>RL 적합성</strong></td><td>최상 (Agile Locomotion)</td><td>상 (Safe Interaction)</td><td>하 (Static Position Control)</td></tr>
<tr><td><strong>대표 로봇</strong></td><td>MIT Cheetah, Unitree Go1</td><td>ANYmal (초기 버전), Valkyrie</td><td>ASIMO, Atlas (유압 제외)</td></tr>
</tbody></table>
<p>1의 연구 결과들을 종합하면, QDD는 SEA가 가진 ’컴플라이언스’의 장점과 직접 구동(Direct Drive, DD)이 가진 ’높은 대역폭’의 장점을 타협점에서 최적화한 형태로 분석된다. 이는 현대 강화학습 기반 4족 보행 로봇이 폭발적으로 성장할 수 있었던 하드웨어적 기반이 되었다.</p>
<h2>2.  고유수용성 감각(Proprioception)과 자코비안 전치(Jacobian Transpose) 제어</h2>
<p>강화학습 에이전트가 환경과 상호작용하기 위해서는 정확한 상태 추정(State Estimation)이 필수적이다. 특히 보행 로봇에서는 ’발이 지면에 닿았는지(Contact Detection)’와 ’지면 반력(Ground Reaction Force, GRF)이 얼마나 작용하는지’가 보행 안정성을 결정하는 핵심 정보(Observation)이다. 전통적인 휴머노이드 로봇은 발바닥에 고가의 6축 힘/토크 센서(F/T Sensor)나 압력 센서(FSR)를 부착하여 이를 측정했으나, 이러한 말단 부착형 센서는 반복적인 보행 충격에 의해 파손되기 쉽고, 배선이 복잡해지며, 신호에 노이즈가 많아 신뢰성이 떨어진다는 단점이 있다.16</p>
<p>QDD 액추에이터는 별도의 힘 센서 없이 모터의 전류값만으로 외부 힘을 추정하는 <strong>고유수용성(Proprioceptive) 제어</strong>를 가능하게 한다. 이는 생물학적 시스템이 근육의 긴장도를 통해 외력을 감지하는 것과 유사한 원리이다.10</p>
<h3>2.1 전류 기반 토크 추정과 투명성</h3>
<p>QDD는 기어 감속비가 낮아 기어박스 내부의 마찰 손실이 적고, 기어의 맞물림에 의한 비선형성이 상대적으로 작다. 따라서 모터에 인가되는 전류(<span class="math math-inline">I</span>)와 출력 토크(<span class="math math-inline">\tau</span>) 사이에 매우 선형적인 관계가 성립한다.<br />
<span class="math math-display">
\tau_{output} \approx K_t \cdot N \cdot I \cdot \eta_{gear}
</span><br />
여기서 <span class="math math-inline">K_t</span>는 모터의 토크 상수, <span class="math math-inline">\eta_{gear}</span>는 기어 효율이다. 높은 감속비의 기어박스에서는 마찰이 전체 토크의 상당 부분(30% 이상)을 차지하고, 회전 방향에 따른 히스테리시스가 심해 전류만으로는 출력 토크를 예측하기 어렵다. 그러나 QDD에서는 마찰 토크가 전체 출력 토크 대비 무시할 수 있을 정도로 작거나, 간단한 모델(예: 쿨롱 마찰 + 점성 마찰)로 보상 가능하므로, 전류 센서가 곧 고성능 토크 센서의 역할을 수행한다.20</p>
<h3>2.2 자코비안 전치를 이용한 지면 반력 추정 (Contact Force Estimation)</h3>
<p>로봇의 관절 토크(<span class="math math-inline">\tau</span>)와 말단 장치(End-effector, 발끝)에 작용하는 카르테시안 공간(Cartesian Space)의 힘(<span class="math math-inline">F</span>) 사이에는 가상 일의 원리(Virtual Work Principle)에 의해 자코비안의 전치 행렬(<span class="math math-inline">J^T</span>)을 매개로 하는 다음과 같은 관계가 성립한다.<br />
<span class="math math-display">
\tau = J(q)^T F
</span><br />
이 식을 힘에 대해 정리하면 다음과 같다.<br />
<span class="math math-display">
F = (J(q)^T)^{-1} \tau
</span><br />
또는 의사 역행렬(Pseudo-inverse)을 사용하여:<br />
<span class="math math-display">
F = (J(q)^T)^{+} \tau
</span><br />
이 수식은 로봇이 각 관절의 전류값(즉, <span class="math math-inline">\tau</span>)과 현재 관절 각도(<span class="math math-inline">q</span>, 이를 통해 <span class="math math-inline">J(q)</span> 계산)만 알면, 발끝에 작용하는 외력(<span class="math math-inline">F</span>)을 계산해낼 수 있음을 의미한다.23 MIT Cheetah 3와 같은 로봇은 이 원리를 이용하여 발바닥에 어떠한 터치 센서도 없이 지면의 요철을 감지하고(Blind Locomotion), 예상치 못한 장애물에 부딪혔을 때 즉각적으로 반응한다.19</p>
<p>이러한 <strong>센서리스 힘 제어(Sensorless Force Control)</strong> 능력은 강화학습에 있어 결정적인 이점을 제공한다.</p>
<ol>
<li><strong>관측 공간(Observation Space)의 단순화:</strong> 파손되기 쉬운 외부 센서 데이터에 의존하지 않고, 모터 드라이버에서 직접 얻어지는 강건한(Robust) 데이터를 상태 입력으로 사용한다.</li>
<li><strong>지연 시간(Latency) 최소화:</strong> 외부 센서의 신호 처리 및 통신 지연 없이, 모터 제어 루프(수십 kHz) 수준에서 즉각적인 힘 추정이 가능하다.</li>
<li><strong>Sim-to-Real 정합성:</strong> 시뮬레이션에서의 접촉 모델(Contact Model)은 실제 센서의 노이즈 특성과 다를 수 있지만, 관절 토크를 통한 추정 방식은 물리적 원리에 기반하므로 시뮬레이션과 현실 간의 괴리가 적다.</li>
</ol>
<p><em>Proprioceptive Actuator Design in the MIT Cheetah</em> 논문에서는 이러한 고유수용성 방식이 외부 힘 센서를 사용하는 방식보다 제어 루프의 안정성과 대역폭 면에서 우수함을 실험적으로 입증하였다.12</p>
<h2>3.  Sim-to-Real Gap 극복을 위한 하드웨어적 접근</h2>
<p>강화학습, 특히 심층 강화학습(DRL)은 수백만 번의 시행착오(Trial and Error)를 필요로 하므로 물리 로봇에서 직접 학습하는 것은 시간적, 비용적 제약으로 인해 불가능에 가깝다. 따라서 고속 물리 엔진(Physics Engine)상에서 정책을 학습시킨 후 실제 로봇에 이식하는 <strong>Sim-to-Real</strong> 방식이 표준으로 자리 잡았다. 이때 시뮬레이션 모델과 실제 로봇 간의 동역학적 불일치, 즉 Sim-to-Real Gap이 성능 저하의 주원인이 된다.28</p>
<p>QDD 액추에이터는 하드웨어의 복잡성을 낮춤으로써 이 간극을 줄이는 데 크게 기여한다.</p>
<h3>3.1 강체 동역학 시뮬레이터와의 정합성</h3>
<p>MuJoCo, PyBullet, Isaac Gym, RaiSim과 같은 최신 물리 엔진들은 <strong>강체 동역학(Rigid Body Dynamics)</strong> 시뮬레이션에 최적화되어 있다. 이들은 링크(Link)의 질량, 관성, 조인트 구속 조건 등은 매우 정확하게 계산하지만, 기어박스 내부의 복잡한 마찰, 기어 이빨 간의 유연성(Flexibility), 비선형 댐핑, 백래시 등은 단순화하거나 무시하는 경향이 있다.28</p>
<ul>
<li><strong>SEA 및 고감속 기어의 문제점:</strong> 스프링의 탄성으로 인해 고유 진동과 지연이 발생하며, 이는 단순 강체 모델로는 표현되지 않는다. 따라서 SEA 기반 로봇(예: 초기 ANYmal)을 위한 강화학습은 실제 액추에이터의 입출력 데이터를 학습한 “Actuator Net“과 같은 별도의 복잡한 신경망 모델을 물리 엔진에 추가해야만 Sim-to-Real 전이가 가능했다.33</li>
<li><strong>QDD의 이점:</strong> 낮은 기어비와 높은 백드라이버빌리티 덕분에 모터의 동작이 물리 엔진이 가정하는 **이상적인 토크 소스(Ideal Torque Source)**에 가깝다. 즉, <span class="math math-inline">\tau_{command} \approx \tau_{output}</span> 관계가 높은 정확도로 성립한다. 따라서 QDD를 사용하는 로봇(예: MIT Mini Cheetah)은 별도의 복잡한 액추에이터 신경망 모델링 없이도, 기본적인 마찰 모델(Viscous + Coulomb)과 모터 상수(<span class="math math-inline">K_t</span>, <span class="math math-inline">N</span>) 튜닝만으로 강체 동역학 시뮬레이션 결과가 실제 로봇에 잘 전이(Transfer)된다.31</li>
</ul>
<h3>3.2 Ben Katz의 드론 모터 혁명과 하드웨어의 민주화</h3>
<p>MIT의 Ben Katz는 그의 석사 학위 논문 <em>A Low Cost Modular Actuator for Dynamic Robots</em>을 통해 대형 드론용 모터(Out-runner BLDC)와 저가형 유성 기어(Planetary Gear)를 결합한 모듈식 QDD 설계를 제안하였다.39 이 설계는 MIT Mini Cheetah의 핵심 구동계가 되었으며, 이후 Unitree, Dogotix 등 다수의 로봇 기업과 전 세계 연구실에서 채택되어 “하드웨어의 민주화“를 이끌었다.</p>
<p>이러한 하드웨어의 보급은 강화학습 연구의 폭발적인 증가를 이끌었다.</p>
<ol>
<li><strong>접근성(Accessibility):</strong> 수천만 원을 호가하던 정밀 액추에이터 대신 양산형 드론 모터를 사용하여 로봇 제작 비용을 1/10 수준으로 낮추었다. 이는 더 많은 연구자가 실제 로봇을 보유하고 RL 알고리즘을 테스트할 수 있게 함을 의미한다.39</li>
<li><strong>내구성(Durability):</strong> 학습 초기 단계의 RL 정책(Random Policy)은 로봇을 넘어뜨리거나 다리를 격렬하게 떠는(Shaking) 등 하드웨어에 치명적인 충격을 준다. QDD의 높은 백드라이버빌리티는 이러한 충격을 기계적으로 흘려보내 모터와 기어박스를 보호하므로, 연구자들은 하드웨어 파손에 대한 두려움 없이 공격적인 탐색(Exploration)과 학습을 수행할 수 있게 되었다.9</li>
</ol>
<h2>4.  강화학습에서의 제어 모드와 행동 공간(Action Space) 설계</h2>
<p>QDD 액추에이터의 특성은 강화학습 에이전트가 출력하는 행동 공간(Action Space)의 설계에도 깊은 영향을 미친다. 초기 RL 연구에서는 생체 모방적 관점에서 관절의 토크를 직접 출력하는 방식을 시도했으나, 최근에는 위치 제어(Position Control) 기반의 PD 타겟을 출력하는 방식이 표준으로 자리 잡고 있다.</p>
<h3>4.1 토크 제어 vs. PD 타겟 제어</h3>
<ul>
<li>
<p><strong>토크 제어 (<span class="math math-inline">\tau</span>):</strong> 에이전트가 <span class="math math-inline">a_t = \tau_{cmd}</span>를 직접 출력한다. 이론적으로 가장 높은 자유도를 가지지만, 시뮬레이션과 실제 간의 마찰 모델 차이에 민감하고, 중력을 보상하지 않으면 로봇이 주저앉는(Sagging) 등 학습 초기 탐색(Exploration) 난이도가 매우 높다.45 또한, 토크 제어 정책은 고주파 진동을 유발하기 쉬워 실제 하드웨어에 무리를 줄 수 있다.</p>
</li>
<li>
<p>PD 타겟 제어 (<span class="math math-inline">q_{target}</span>): 에이전트는 목표 관절 각도(Desired Joint Position)를 출력하고, 하위 레벨(Low-level)의 PD 제어기가 이를 추종하기 위한 토크를 계산한다.<br />
<span class="math math-display">
\tau = K_p (q_{target} - q) - K_d \dot{q}
</span><br />
이 방식은 PD 제어기 자체가 일종의 가상 스프링-댐퍼(Virtual Spring-Damper) 역할을 하여 시스템에 수동적 안정성(Passivity)을 부여한다. QDD 액추에이터는 낮은 내부 마찰과 높은 투명성 덕분에, 소프트웨어적으로 설정한 <span class="math math-inline">K_p</span>(강성)와 <span class="math math-inline">K_d</span>(감쇠) 값을 물리적으로 매우 충실하게 재현한다. 즉, 소프트웨어가 의도한 가상의 스프링 거동이 실제 물리 세계에서 왜곡 없이 구현된다.46</p>
</li>
</ul>
<p><em>Learning to Walk via Deep Reinforcement Learning</em> (Haarnoja et al.) 48 논문에서 Minitaur 로봇은 직접적인 토크 제어 대신, 관절의 목표 위치를 추정하는 방식을 통해 2시간 만에 실제 환경에서 보행을 학습했다. 이 연구는 PD 타겟 제어가 토크 제어보다 샘플 효율성(Sample Efficiency)이 높고 Sim-to-Real 전이가 훨씬 용이함을 보여주었으며, 이는 QDD의 정확한 토크 응답성이 뒷받침되었기에 가능한 결과였다.</p>
<h3>4.2 가변 임피던스 제어 (Variable Impedance Control) 및 P-gain 스케줄링</h3>
<p>최근의 SOTA 연구들은 단순한 위치 제어를 넘어, 강화학습 에이전트가 위치(<span class="math math-inline">q_{target}</span>)뿐만 아니라 강성(<span class="math math-inline">K_p</span>)과 감쇠(<span class="math math-inline">K_d</span>) 계수까지 실시간으로 조절하는 **가변 임피던스 제어(Variable Impedance Control, VIC)**를 도입하고 있다.50</p>
<p>QDD는 기계적인 스프링 교체 없이도 제어 이득(Gain) 조절만으로 **소프트웨어 정의 컴플라이언스(Software-defined Compliance)**를 구현할 수 있는 이상적인 플랫폼이다.53 강화학습 에이전트는 이를 활용하여 다음과 같은 고도의 전략을 스스로 학습한다:</p>
<ul>
<li><strong>착지 순간(Touchdown):</strong> <span class="math math-inline">K_p</span>를 낮추어(Soft mode) 충격을 부드럽게 흡수하고 발의 미끄러짐을 방지하여 접촉 안정성을 확보한다.</li>
<li><strong>도약 순간(Push-off):</strong> <span class="math math-inline">K_p</span>를 높여(Stiff mode) 지면을 강하게 박차고 나가 추진력을 극대화한다.</li>
</ul>
<p>이러한 **P-gain 스케줄링(P-gain Scheduling)**은 QDD의 높은 대역폭이 뒷받침되지 않으면 구현 불가능한 전략이다. 하드웨어의 응답 속도가 느리면 RL 에이전트가 매 0.002초(500Hz)마다 변경하는 임피던스 명령을 물리적으로 따라갈 수 없기 때문이다.55 <em>Variable Stiffness for Robust Locomotion through Reinforcement Learning</em> 55 연구는 가변 강성을 학습한 에이전트가 고정 강성을 사용하는 에이전트보다 외란에 대해 훨씬 더 강건함을 보여주었다.</p>
<h2>5.  사례 연구: 학습 기반 제어의 진화와 액추에이터 모델링</h2>
<p>QDD가 Sim-to-Real 격차를 줄여주지만, 완벽히 제거하는 것은 아니다. 특히 고속 기동이나 정교한 조작 작업에서는 미세한 액추에이터 동역학이 성능의 병목이 될 수 있다. 이를 극복하기 위한 연구들은 QDD의 특성을 십분 활용하는 방향으로 진화해왔다.</p>
<h3>5.1 Actuator Network: ANYmal의 사례 (Hwangbo et al.)</h3>
<p>Jemin Hwangbo 등이 발표한 <em>Learning Agile and Dynamic Motor Skills for Legged Robots</em> (Science Robotics, 2019) 연구는 QDD와 강화학습의 결합을 한 단계 도약시킨 이정표적인 연구이다.36 이 연구에서 사용된 초기 ANYmal 로봇은 SEA 액추에이터를 사용하고 있었는데, SEA의 복잡한 동역학(스프링 진동, 비선형 마찰) 때문에 단순한 이상적 모터 모델로는 Sim-to-Real 전이가 실패했다.</p>
<p>연구팀은 실제 액추에이터의 입출력 데이터를 수집하여, 모터의 복잡한 거동을 모사하는 심층 신경망인 Actuator Network를 학습시켰다.<br />
<span class="math math-display">
\tau_{real} \approx f_{net}(q, \dot{q}, \tau_{cmd}, \text{history})
</span><br />
이 Actuator Network를 물리 엔진의 강체 시뮬레이션 루프 안에 포함시켜(In-the-loop) 강화학습 에이전트를 학습시킨 결과, 시뮬레이션에서의 보행 정책이 실제 로봇에서도 놀라운 성능을 발휘하며 1.6 m/s의 고속 보행과 넘어진 상태에서의 회복(Recovery) 동작을 성공시켰다.</p>
<p>이 연구 이후, QDD 기반 로봇(예: Mini Cheetah)을 위한 연구들은 Actuator Network의 필요성을 재평가하게 되었다. QDD는 물리적으로 훨씬 단순하므로(Linear), 복잡한 신경망 대신 간단한 DC 모터 모델이나 1차 저역 통과 필터(Low-pass filter) 모델만으로도 충분한 Sim-to-Real 성능을 낼 수 있음이 밝혀졌다. 이는 QDD가 데이터 기반(Data-driven) 접근법의 복잡도를 낮추고 학습 효율성을 높이는 데 기여함을 시사한다.33</p>
<h3>5.2 MIT Cheetah 시리즈와 모델 예측 제어(MPC)의 결합</h3>
<p>MIT Cheetah 3와 Mini Cheetah는 QDD 액추에이터의 성능을 극한으로 활용하여 백플립(Backflip)과 같은 고난이도 동작을 구현했다. 이들은 순수 강화학습뿐만 아니라 모델 예측 제어(Model Predictive Control, MPC)를 함께 사용하기도 한다.26</p>
<ul>
<li><strong>MPC의 역할:</strong> 짧은 미래(Horizon) 동안의 최적 궤적과 지면 반력을 실시간으로 계산한다. 이를 위해서는 로봇의 동역학 모델이 정확해야 한다.</li>
<li><strong>QDD의 기여:</strong> QDD는 다리의 관성(Leg Inertia)을 최소화하여 시스템을 단순한 **단일 강체 모델(Single Rigid Body Model, SRBM)**로 근사할 수 있게 해준다. 다리 자체의 무게가 가볍고 마찰이 적어, 복잡한 다리 관절 동역학을 무시하고 중심 몸체(Torso)의 동역학만으로도 제어가 가능해진다. 이는 MPC의 최적화 문제를 단순화시켜(Convex MPC), 온보드 컴퓨터에서 실시간(30Hz~500Hz)으로 풀 수 있게 만든다.10</li>
</ul>
<p>최근에는 MPC가 계산한 최적의 궤적을 강화학습 정책이 추종하거나, 반대로 강화학습이 상위 레벨에서 MPC의 비용 함수(Cost Function)나 목표 상태를 튜닝하는 <strong>하이브리드 아키텍처</strong>가 연구되고 있다.38 어느 경우든, 하드웨어의 단순성과 투명성(QDD)이 제어 알고리즘의 복잡성을 낮추고 실시간 연산을 가능하게 하는 핵심 요소로 작용한다.</p>
<h2>6.  결론 및 시사점: 하드웨어와 알고리즘의 공진화</h2>
<p>QDD 액추에이터는 단순한 모터 구동 방식을 넘어, <strong>‘제어를 위한 설계(Design for Control)’</strong> 철학을 대변한다. 낮은 감속비를 통해 달성한 높은 백드라이버빌리티와 토크 투명성은 로봇 하드웨어를 물리적으로 ‘투명하게’ 만들어, 강화학습 알고리즘이 복잡한 기계적 특성에 방해받지 않고 환경과 직접 상호작용할 수 있는 통로를 열어주었다.</p>
<p>본 섹션의 분석을 요약하면 다음과 같다.</p>
<ol>
<li><strong>Sim-to-Real Gap의 물리적 해소:</strong> QDD는 실제 로봇의 동역학을 이상적인 시뮬레이션 환경(Ideal Torque Source)에 근접시켜, 복잡한 시스템 식별(System Identification) 과정 없이도 강화학습의 전이 효율을 극대화한다.</li>
<li><strong>안전하고 효율적인 탐색(Exploration):</strong> 높은 충격 완화 능력(IMF)은 강화학습 초기 단계의 무작위적인 행동으로부터 하드웨어를 보호하여, 데이터 수집과 학습의 반복 속도를 가속화한다.</li>
<li><strong>소프트웨어 정의 컴플라이언스:</strong> 가변 임피던스 제어(VIC)를 통해 하드웨어 변경 없이 상황에 맞는 강성을 실시간으로 학습하고 적용할 수 있게 하여, 비정형 환경에서의 적응력을 높인다.</li>
<li><strong>센서리스 관측(Sensorless Observation):</strong> 고유수용성 감각을 통해 파손되기 쉬운 외부 센서 없이도 강건한 상태 추정을 가능하게 하여, 실제 환경(Wild)에서의 배치(Deployment) 가능성을 높였다.</li>
</ol>
<p>결국, QDD 기술은 SOTA 인공지능 알고리즘이 로봇이라는 물리적 신체(Embodiment)를 입고 현실 세계에서 민첩하게 움직일 수 있게 하는 가장 중요한 물리적 인터페이스라 할 수 있다. 향후 로봇 공학은 더욱 높은 토크 밀도와 열 효율을 가진 차세대 QDD(예: 내부 냉각 채널 통합, 신소재 기어) 개발과, 이를 완벽하게 제어할 수 있는 적응형(Adaptive) 및 메타(Meta) 강화학습 알고리즘의 결합으로 나아갈 것이다. 이는 로봇이 실험실을 벗어나 재난 현장, 물류 창고, 그리고 우리의 일상 공간으로 진입하는 데 있어 필수적인 기술적 토대가 될 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Quasi-Direct Drive Actuation for a Lightweight Hip Exoskeleton with High Backdrivability and High Bandwidth - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC7971415/</li>
<li>Design of a Quasi-Direct-Drive Actuator for Dynamic Motions - MDPI, https://www.mdpi.com/2504-3900/64/1/11</li>
<li>How do gearmotors impact reflected mass inertia from the load? - Motion Control Tips, https://www.motioncontroltips.com/how-do-gearmotors-impact-reflected-mass-inertia-from-the-load/</li>
<li>[PDF] Design of a Quasi Direct drive Actuator for Dynamic Motions | Semantic Scholar, https://www.semanticscholar.org/paper/Design-of-a-Quasi-Direct-drive-Actuator-for-Dynamic-Singh-Kashiri/51fe863885ffdb568355c5a047d45b85409bf2ca</li>
<li>Why Quasi Direct Drives? - Pulsar HRI, https://pulsarhri.com/technology/why-quasi-direct-drives/</li>
<li>Motor Sizing Basics Part 2: How to Calculate Load Inertia - Engineering Notes, https://blog.orientalmotor.com/motor-sizing-basics-part-2-load-inertia</li>
<li>How to calculate the inertia ratio with a gearbox: A Motion Control Classroom video, https://www.youtube.com/watch?v=-eflwvPJdJ0</li>
<li>What is Quasi Direct Drive actuation? And why does it seem to be the default choice for quadruped robotics? - Reddit, https://www.reddit.com/r/robotics/comments/kvaam2/what_is_quasi_direct_drive_actuation_and_why_does/</li>
<li>Direct Drive vs. Geared Actuators - ALVA Industries, https://www.alvaindustries.com/post/direct-drive-vs-geared-actuators</li>
<li>Proprioceptive Actuator Design in the MIT Cheetah: Impact Mitigation and High-Bandwidth Physical Interaction for Dynamic Legged Robots, https://fab.cba.mit.edu/classes/865.18/motion/papers/mit-cheetah-actuator.pdf</li>
<li>Proprioceptive Actuator Design in the MIT Cheetah: Impact Mitigation and High-Bandwidth Physical Interaction for Dynamic Legged Robots - IEEE Xplore, https://ieeexplore.ieee.org/document/7827048/</li>
<li>(PDF) Proprioceptive Actuator Design in the MIT Cheetah: Impact Mitigation and High-Bandwidth Physical Interaction for Dynamic Legged Robots - ResearchGate, https://www.researchgate.net/publication/312558722_Proprioceptive_Actuator_Design_in_the_MIT_Cheetah_Impact_Mitigation_and_High-Bandwidth_Physical_Interaction_for_Dynamic_Legged_Robots</li>
<li>Proprioceptive Actuator Design in the MIT Cheetah: Impact Mitigation and High-Bandwidth Physical Interaction for Dynamic Legged, https://dspace.mit.edu/bitstream/handle/1721.1/119863/Proprioceptive-TRO-RG.pdf</li>
<li>Quasi-Direct Drive Actuation for a Lightweight Hip Exoskeleton With High Backdrivability and High Bandwidth - IEEE Xplore, https://ieeexplore.ieee.org/document/9095261/</li>
<li>Impact Robustness vs. Torque Bandwidth: A Design Guide for Differential Elastic Actuators, https://elib.dlr.de/205476/1/main.pdf</li>
<li>Learning-Assisted Multi-IMU Proprioceptive State Estimation for Quadruped Robots - MDPI, https://www.mdpi.com/2078-2489/16/6/479</li>
<li>State Estimation for legged robots using proprioceptive sensors, https://www.ri.cmu.edu/app/uploads/2019/08/Thesis_report__Naman.pdf</li>
<li>Contact Force Estimation Method of Legged-Robot and Its Application in Impedance Control - IEEE Xplore, https://ieeexplore.ieee.org/iel7/6287639/8948470/09184795.pdf</li>
<li>MIT Open Access Articles MIT Cheetah 3: Design and Control of a Robust, Dynamic Quadruped Robot, https://dspace.mit.edu/bitstream/handle/1721.1/126619/iros.pdf?sequence=2</li>
<li>Actuator design for high force proprioceptive control in fast legged locomotion - IEEE Xplore, https://ieeexplore.ieee.org/document/6386252/</li>
<li>Torque Estimation Base on Quasi-Direct Drive Actuators | Request PDF - ResearchGate, https://www.researchgate.net/publication/362649583_Torque_Estimation_Base_on_Quasi-Direct_Drive_Actuators</li>
<li>Actuator Design for High Force Proprioceptive Control in Fast Legged Locomotion, https://www.researchgate.net/publication/260820531_Actuator_Design_for_High_Force_Proprioceptive_Control_in_Fast_Legged_Locomotion</li>
<li>Transposed Jacobian Control of a Mobile Robot - Digital Commons @ USF, https://digitalcommons.usf.edu/cgi/viewcontent.cgi?article=1031&amp;context=fcrar</li>
<li>Introduction to Inverse Kinematics with Jacobian Transpose, Pseudoinverse and Damped Least Squares methods - CMU School of Computer Science, https://www.cs.cmu.edu/~15464-s13/lectures/lecture6/iksurvey.pdf</li>
<li>FILIC: Dual-Loop Force-Guided Imitation Learning with Impedance Torque Control for Contact-Rich Manipulation Tasks - arXiv, https://arxiv.org/html/2509.17053v1</li>
<li>Dynamic Locomotion in the MIT Cheetah 3 Through Convex Model-Predictive Control, https://dspace.mit.edu/bitstream/handle/1721.1/138000/convex_mpc_2fix.pdf</li>
<li>Cheetah robot NOV 12 2013 - DSpace@MIT, https://dspace.mit.edu/bitstream/handle/1721.1/85490/870998828-MIT.pdf?sequence=2&amp;isAllowed=y</li>
<li>Sim-to-Real of Soft Robots with Learned Residual Physics - arXiv, https://arxiv.org/html/2402.01086v1</li>
<li>Revealing the Challenges of Sim-to-Real Transfer in Model-Based Reinforcement Learning via Latent Space Modeling - arXiv, https://arxiv.org/html/2506.12735v1</li>
<li>What Went Wrong? Closing the Sim-to-Real Gap via Differentiable Causal Discovery, https://proceedings.mlr.press/v229/huang23c/huang23c.pdf</li>
<li>What exactly makes sim to real transfer a challenge in reinforcement learning? : r/robotics, https://www.reddit.com/r/robotics/comments/1j99vrt/what_exactly_makes_sim_to_real_transfer_a/</li>
<li>Sim-to-Real Transfer in Deep Reinforcement Learning for Bipedal Locomotion - arXiv, https://arxiv.org/html/2511.06465v1</li>
<li>Bridging the Sim-to-Real Gap for Athletic Loco-Manipulation - arXiv, https://arxiv.org/html/2502.10894v1</li>
<li>paper.pdf - Bridging the Sim-to-Real Gap for Athletic Loco-Manipulation, https://uan.csail.mit.edu/rsc/paper.pdf</li>
<li>Bridging the Sim-to-Real Gap for Athletic Loco-Manipulation Nolan Fey - DSpace@MIT, https://dspace.mit.edu/bitstream/handle/1721.1/163694/fey-nolanfey-sm-eecs-2025-thesis.pdf?sequence=1&amp;isAllowed=y</li>
<li>[1901.08652] Learning agile and dynamic motor skills for legged robots - arXiv, https://arxiv.org/abs/1901.08652</li>
<li>VIRAL: Visual Sim-to-Real at Scale for Humanoid Loco-Manipulation - arXiv, https://arxiv.org/html/2511.15200v2</li>
<li>Learning Robust Terrain-Aware Locomotion Gabriel B Margolis - DSpace@MIT, https://dspace.mit.edu/bitstream/handle/1721.1/139325/Margolis-gmargo-meng-eecs-2021-thesis.pdf?sequence=1&amp;isAllowed=y</li>
<li>A Low Cost Modular Actuator for Dynamic Robots Benjamin G. Katz - DSpace@MIT, https://dspace.mit.edu/bitstream/handle/1721.1/118671/1057343368-MIT.pdf</li>
<li>Low Cost, High Performance Actuators for Dynamic Robots - DSpace@MIT, https://dspace.mit.edu/bitstream/handle/1721.1/105580/964525489-MIT.pdf</li>
<li>A low cost modular actuator for dynamic robots - DSpace@MIT, https://dspace.mit.edu/handle/1721.1/118671</li>
<li>Watch Legged Robot Run Circles Around Its Bigger Brethren | Hackaday, https://hackaday.com/2019/09/22/watch-legged-robot-run-circles-around-its-bigger-brethren/</li>
<li>A low cost modular actuator for dynamic robots - ResearchGate, https://www.researchgate.net/publication/328446448_A_low_cost_modular_actuator_for_dynamic_robots</li>
<li>Mini Cheetah Clone Teardown, By None Other Than Original Designer | Hackaday, https://hackaday.com/2022/12/19/mini-cheetah-clone-teardown-by-none-other-than-original-designer/</li>
<li>Controlling the Solo12 quadruped robot with deep reinforcement learning - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC10366154/</li>
<li>Deep reinforcement learning for real-world quadrupedal locomotion: a comprehensive review - OAE Publishing Inc., https://www.oaepublish.com/articles/ir.2022.20</li>
<li>Leveraging Engineering Expertise in Deep Reinforcement Learning Liam J. Ackerman - DSpace@MIT, https://dspace.mit.edu/bitstream/handle/1721.1/147435/Ackerman-liamack-meng-eecs-2022-thesis.pdf?sequence=1&amp;isAllowed=y</li>
<li>Learning to Walk via Deep Reinforcement Learning - Robotics, https://roboticsproceedings.org/rss15/p11.pdf</li>
<li>Learning to Walk via Deep Reinforcement Learning - YouTube, https://www.youtube.com/watch?v=n2gE7n11h1Y</li>
<li>Model Predictive Impedance Control and Gait Optimization for High-Speed Quadrupedal Running - MDPI, https://www.mdpi.com/2076-3417/15/16/8861</li>
<li>FACET: Force-Adaptive Control via Impedance Reference Tracking for Legged Robots, https://arxiv.org/html/2505.06883v1</li>
<li>User-Adaptive Variable Impedance Control Using Bayesian Optimization for Robot-Aided Ankle Rehabilitation - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC12519907/</li>
<li>Compliant Force Control for Robots: A Survey - MDPI, https://www.mdpi.com/2227-7390/13/13/2204</li>
<li>Learning Tool-Aware Adaptive Compliant Control for Autonomous Regolith Excavation - arXiv, https://arxiv.org/pdf/2509.05475</li>
<li>(PDF) Variable Stiffness for Robust Locomotion through Reinforcement Learning, https://www.researchgate.net/publication/388963627_Variable_Stiffness_for_Robust_Locomotion_through_Reinforcement_Learning</li>
<li>Variable Stiffness for Robust Locomotion through Reinforcement Learning* *submitted to the 16th IFAC joint symposia of mechatronics and robotics - arXiv, https://arxiv.org/html/2502.09436v1</li>
<li>Learning agile and dynamic motor skills for legged robots - SciSpace, https://scispace.com/pdf/learning-agile-and-dynamic-motor-skills-for-legged-robots-5bdcnhm1my.pdf</li>
<li>Learning agile and dynamic motor skills for legged robots | Request PDF - ResearchGate, https://www.researchgate.net/publication/330442740_Learning_agile_and_dynamic_motor_skills_for_legged_robots</li>
<li>Highly Dynamic Quadruped Locomotion via Whole-Body Impulse Control and Model Predictive Control - arXiv, https://arxiv.org/pdf/1909.06586</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>