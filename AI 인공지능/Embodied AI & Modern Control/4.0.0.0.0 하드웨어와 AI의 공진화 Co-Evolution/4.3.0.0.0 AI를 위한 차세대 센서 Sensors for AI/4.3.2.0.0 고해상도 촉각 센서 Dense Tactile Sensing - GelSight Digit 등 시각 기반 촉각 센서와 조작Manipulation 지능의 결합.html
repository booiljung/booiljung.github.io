<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:4.3.2 고해상도 촉각 센서 (Dense Tactile Sensing): GelSight, Digit 등 시각 기반 촉각 센서와 조작(Manipulation) 지능의 결합</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>4.3.2 고해상도 촉각 센서 (Dense Tactile Sensing): GelSight, Digit 등 시각 기반 촉각 센서와 조작(Manipulation) 지능의 결합</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 4. 하드웨어와 AI의 공진화 (Co-Evolution)</a> / <a href="index.html">4.3 AI를 위한 차세대 센서 (Sensors for AI)</a> / <span>4.3.2 고해상도 촉각 센서 (Dense Tactile Sensing): GelSight, Digit 등 시각 기반 촉각 센서와 조작(Manipulation) 지능의 결합</span></nav>
                </div>
            </header>
            <article>
                <h1>4.3.2 고해상도 촉각 센서 (Dense Tactile Sensing): GelSight, Digit 등 시각 기반 촉각 센서와 조작(Manipulation) 지능의 결합</h1>
<p>로봇 공학의 발전사에서 ’모라벡의 역설(Moravec’s paradox)’은 오랫동안 풀리지 않는 난제였다. 인간에게 고도의 지적 능력을 요구하는 체스나 바둑은 로봇에게 상대적으로 쉬운 과제였으나, 인간이 무의식적으로 수행하는 걷기나 물체 잡기와 같은 감각운동(Sensorimotor) 능력은 로봇에게 극한의 난이도를 요구했다. 특히 지난 10여 년간 컴퓨터 비전(Computer Vision)이 딥러닝(Deep Learning)의 수혜를 입어 폭발적으로 성장한 것에 비해, 촉각(Tactile Sensing)은 상대적으로 소외된 ’잃어버린 감각(Missing Modality)’이었다. 시각이 원거리에서 대상을 인지하는 수동적 감각이라면, 촉각은 대상과의 물리적 상호작용을 통해서만 획득되는 능동적 감각이며, 조작(Manipulation)의 성공을 결정짓는 최후의 접점(Last Mile)이기 때문이다.</p>
<p>최근 **시각 기반 촉각 센서(Vision-Based Tactile Sensors, VBTS)**의 등장은 이러한 불균형을 해소할 결정적인 돌파구로 부상했다. GelSight와 Digit으로 대표되는 이 기술은 탄성체(Elastomer)와 광학계(Optical System)를 결합하여, 접촉면의 미세한 형상과 힘의 분포를 고해상도 이미지 데이터로 변환한다. 이는 성숙한 컴퓨터 비전 기술을 촉각 처리에 직접 적용할 수 있게 만들었으며, 희소한(Sparse) 접촉 정보에 의존하던 기존 제어 이론의 한계를 넘어 데이터 중심(Data-Driven)의 정교한 조작 지능을 가능케 하고 있다. 본 장에서는 VBTS의 물리적 작동 원리부터 하드웨어의 진화, 이를 가상 환경에서 모사하는 시뮬레이션 기술, 그리고 궁극적으로 이 풍부한 촉각 정보를 활용하여 물체를 자유자재로 다루는 최신 조작 지능의 아키텍처까지 심도 있게 분석한다.</p>
<h2>1.  시각 기반 촉각 센싱의 물리적 원리와 광학적 메커니즘</h2>
<p>시각 기반 촉각 센서는 근본적으로 **“물리적 접촉을 시각 정보로 변환(Transduction)”**하는 장치다. 기존의 피에조(Piezoelectric)나 정전 용량(Capacitive) 방식이 제공하는 1차원적인 힘 벡터나 저해상도 배열 정보와 달리, VBTS는 수백만 픽셀에 달하는 조밀한(Dense) 표면 기하 정보를 제공한다. 이 변환 과정의 핵심은 탄성체의 변형을 광학적으로 해석하는 알고리즘에 있다.</p>
<h3>1.1  GelSight: 포토메트릭 스테레오와 마이크로 기하학</h3>
<p>MIT의 Edward Adelson 그룹에서 개발된 GelSight 센서는 VBTS의 효시이자 기술적 표준으로 자리 잡았다. GelSight의 핵심 원리는 컴퓨터 비전의 고전적 기법인 **포토메트릭 스테레오(Photometric Stereo)**를 미세 스케일의 폐쇄된 광학 환경에 적용한 것이다.</p>
<h4>1.1.1  탄성체 구조와 광학 경로</h4>
<p>GelSight 센서는 크게 세 가지 층위로 구성된다. 첫째, 외부 물체와 직접 접촉하는 **탄성체 스킨(Elastomeric Skin)**이다. 주로 투명한 열가소성 엘라스토머(Thermoplastic Elastomer)나 실리콘(Silicone)으로 제작되며, 물체의 형상에 따라 유연하게 변형된다. 둘째, 탄성체 표면에 도포된 **반사 코팅(Reflective Membrane)**이다. 알루미늄 플레이크나 은색 안료를 섞어 만든 이 코팅은 외부의 빛을 차단하는 동시에, 내부 조명에 대해 램버시안(Lambertian) 또는 반사광(Specular) 특성을 갖는 균일한 반사면을 제공한다. 셋째, 내부의 <strong>조명 및 카메라 모듈</strong>이다. 서로 다른 각도(주로 120도 간격)에서 비추는 RGB LED 조명은 탄성체 표면의 기울기(Gradient)에 따라 색상과 명암을 변화시키며, 카메라는 이를 촬영한다.</p>
<h4>1.1.2  표면 법선 벡터 복원 및 3D 형상 재구성</h4>
<p>센서가 물체에 눌려 탄성체가 변형되면, 코팅된 표면의 각 지점은 기울기에 따라 다른 색의 빛을 반사한다. 이를 수식으로 모델링하면, 카메라의 픽셀 좌표 <span class="math math-inline">(x, y)</span>에서 관측된 빛의 강도 <span class="math math-inline">I_i(x, y)</span>는 다음과 같이 표현된다1:<br />
<span class="math math-display">
I_i(x, y) = R(n(x, y), l_i) \approx \rho(x, y) [n(x, y) \cdot l_i] + \epsilon_i
</span><br />
여기서 <span class="math math-inline">R</span>은 표면의 반사 함수(Reflectance Function), <span class="math math-inline">\rho</span>는 표면의 반사율(Albedo), <span class="math math-inline">n(x, y)</span>는 해당 지점의 단위 표면 법선 벡터(Unit Surface Normal), <span class="math math-inline">l_i</span>는 <span class="math math-inline">i</span>번째 광원의 방향 벡터, <span class="math math-inline">\epsilon_i</span>는 센서 노이즈를 나타낸다. 3개 이상의 광원 방향(<span class="math math-inline">l_1, l_2, l_3</span>)에 대한 관측값이 있으면, 이 연립방정식을 풀어 각 픽셀에서의 법선 벡터 <span class="math math-inline">n(x, y)</span>를 역추정할 수 있다.</p>
<p>이렇게 구한 법선 벡터 필드(Normal Field)는 표면의 기울기 성분 <span class="math math-inline">p = \frac{\partial z}{\partial x}</span>, <span class="math math-inline">q = \frac{\partial z}{\partial y}</span>와 직접적인 관계를 맺는다. 최종적인 3D 높이 맵(Height Map) <span class="math math-inline">z(x, y)</span>는 푸아송 방정식(Poisson Equation)을 풀어 기울기 장을 적분함으로써 획득된다:<br />
<span class="math math-display">
\nabla^2 z = \frac{\partial p}{\partial x} + \frac{\partial q}{\partial y}
</span><br />
이 과정을 통해 GelSight는 수십 마이크론(micron) 수준의 미세한 텍스처(Texture)까지 복원해낸다. 이는 로봇이 단순히 물체의 존재 여부를 감지하는 것을 넘어, 표면의 거칠기, 직조 패턴, 미세한 흠집까지 인식할 수 있게 한다.</p>
<h3>1.2  마커 추적을 통한 전단력(Shear Force)과 비틀림(Torque) 측정</h3>
<p>순수한 형상 정보만으로는 접촉면에서 발생하는 전단력(Shear Force)이나 비틀림 모멘트(Torque), 그리고 미끄러짐(Slip)을 감지하는 데 한계가 있다. 이를 보완하기 위해 최신 VBTS는 탄성체 표면이나 내부에 검은색 점(Marker) 패턴을 인쇄하거나 레이저로 각인한다.2</p>
<ul>
<li><strong>변위 필드(Displacement Field) 분석:</strong> 물체와의 마찰에 의해 탄성체가 수평 방향으로 밀리면 마커의 위치가 이동한다. <strong>광학 흐름(Optical Flow)</strong> 알고리즘이나 마커 매칭 알고리즘을 통해 마커들의 초기 위치 대비 이동 벡터를 계산한다.</li>
<li><strong>힘의 분해:</strong> 마커의 이동 방향과 크기는 작용하는 전단력의 방향 및 크기와 비례한다. 또한, 마커들의 회전 성분(Curl)을 분석하면 비틀림 모멘트를 추정할 수 있다.</li>
<li><strong>하이브리드 센싱:</strong> 결과적으로 GelSight 계열 센서는 음영(Shading) 정보로부터 수직항력과 3D 형상을, 마커의 움직임(Motion)으로부터 전단력과 마찰 상태를 동시에 추출하는 하이브리드 센싱 메커니즘을 갖는다.1</li>
</ul>
<h2>2.  하드웨어의 공진화: GelSight에서 Digit 360까지</h2>
<p>초기의 GelSight 센서는 큰 부피와 복잡한 조명 장치로 인해 로봇 팔 끝단(End-effector)에 장착하기 어려웠다. 그러나 하드웨어 소형화와 광학 설계의 최적화를 통해 실용적인 로봇 센서로 진화해왔다.</p>
<h3>2.1  Digit: 촉각 센서의 대중화와 모듈화</h3>
<p>Meta AI(구 Facebook AI Research)와 GelSight가 협력하여 개발한 <strong>Digit</strong>은 고해상도 촉각 센서의 접근성을 획기적으로 높인 모델이다.5</p>
<ul>
<li><strong>설계 철학:</strong> Digit은 로봇 핸드와의 통합을 최우선으로 고려하여 소형 폼팩터로 설계되었다. 제조 비용을 낮추고 내구성을 확보하기 위해 모듈형 설계를 채택했으며, 이는 딥러닝 연구에 필수적인 대규모 데이터 수집을 가능케 했다.</li>
<li><strong>기술적 특징:</strong> RGB LED를 사용한 다색 조명 방식을 채택하여, 색상 정보만으로 표면의 기울기를 추정할 수 있도록 했다. 이는 복잡한 조명 제어 없이도 단일 프레임에서 형상 정보를 추출할 수 있게 하여 반응 속도를 높였다.</li>
</ul>
<h3>2.2  Digit 360: 전방위 감지(Omnidirectional Sensing)의 구현</h3>
<p>2024년 말 공개된 <strong>Digit 360</strong>은 평면형 센서의 한계를 극복하고 인간의 손가락과 유사한 곡면 및 전방위 감지 능력을 구현한 혁신적인 센서다.7</p>
<ul>
<li><strong>전방위 광학계:</strong> 기존 센서들이 한 면만 감지할 수 있었던 반면, Digit 360은 특수 설계된 광학 렌즈와 조명 시스템을 통해 손가락 끝의 둥근 면 전체에서 발생하는 접촉을 감지한다. 이는 약 830만 개의 택셀(Taxel)에 해당하는 정보를 제공하며, 공간 해상도는 7마이크론에 이른다.</li>
<li><strong>멀티모달 센싱:</strong> Digit 360은 단순한 시각적 변형뿐만 아니라 진동, 온도 등 18가지 이상의 감각 양상을 포착할 수 있는 기계적·전자적 설계를 포함한다.7 이는 시각 정보만으로는 알 수 없는 물체의 열전도율, 경도(Stiffness) 등을 통합적으로 추론할 수 있게 하여, 로봇이 물체를 쥐었을 때 “차갑고 딱딱한 금속“인지 “따뜻하고 부드러운 플라스틱“인지를 구별하는 물리적 맥락 이해를 돕는다.</li>
</ul>
<table><thead><tr><th><strong>특징</strong></th><th><strong>GelSight (Standard)</strong></th><th><strong>Digit</strong></th><th><strong>Digit 360</strong></th></tr></thead><tbody>
<tr><td><strong>형상</strong></td><td>평면형 블록 (Flat Block)</td><td>손가락 끝 (Compact Finger-tip)</td><td>곡면형/전방위 (Omnidirectional)</td></tr>
<tr><td><strong>해상도</strong></td><td>수 미크론 (초정밀 계측용)</td><td>고해상도 (조작용)</td><td>~7 미크론 (초정밀 조작용)</td></tr>
<tr><td><strong>센싱 범위</strong></td><td>단면 (Single Face)</td><td>전면 (Front Face)</td><td>전방위 (Omni-surface)</td></tr>
<tr><td><strong>주요 기술</strong></td><td>Photometric Stereo</td><td>Color/Intensity Mapping</td><td>Advanced Multi-modal Optics</td></tr>
<tr><td><strong>데이터 처리</strong></td><td>3D 형상 복원 위주</td><td>접촉 위치 및 힘 추정</td><td>멀티모달 융합 및 온디바이스 AI</td></tr>
<tr><td><strong>응용 분야</strong></td><td>정밀 표면 검사, 질감 분석</td><td>로봇 파지(Grasping)</td><td>정교한 인핸드 조작(In-hand Manipulation)</td></tr>
</tbody></table>
<h2>3.  시뮬레이션: 데이터 기근의 해소와 Sim-to-Real</h2>
<p>고해상도 촉각 센서가 제공하는 풍부한 데이터는 딥러닝 모델 학습에 이상적이지만, 실제 로봇으로 수백만 번의 접촉 실험을 수행하는 것은 물리적으로 불가능하며 센서의 마모를 유발한다. 따라서 시각 기반 촉각 센서를 가상 환경에서 물리적으로, 그리고 광학적으로 정교하게 모사하는 시뮬레이션 기술은 필수적이다.</p>
<h3>3.1  연성체 시뮬레이션의 난제</h3>
<p>기존 로봇 시뮬레이터(MuJoCo, PyBullet 등)는 강체(Rigid Body) 역학에 최적화되어 있어, 젤과 같은 연성체(Soft Body)의 비선형적인 변형과 그에 따른 광학적 반응을 실시간으로 계산하는 데 한계가 있었다. FEM(유한요소해석)은 정확하지만 계산 비용이 너무 높아 강화학습(RL)과 같은 대규모 반복 학습에 부적합하다.</p>
<h3>3.2  Taxim: 예제 기반(Example-based) 하이브리드 접근</h3>
<p><strong>Taxim</strong>은 이러한 딜레마를 해결하기 위해 물리적 변형과 광학적 렌더링을 분리하는 전략을 취했다.9</p>
<ul>
<li><strong>광학 응답의 룩업 테이블(Look-up Table):</strong> 복잡한 레이 트레이싱(Ray Tracing) 대신, 실제 센서에서 수집한 데이터(형상-이미지 쌍)를 바탕으로 다항식 룩업 테이블을 구축한다. 시뮬레이션 상의 깊이 맵(Geometry)이 주어지면, 이 테이블을 참조하여 즉각적으로 사실적인 GelSight 이미지를 합성해낸다.</li>
<li><strong>중첩 원리(Superposition Principle):</strong> 마커의 움직임을 계산하기 위해 선형 탄성 이론을 적용한다. 접촉면의 변형을 여러 기본 변형(Basis Deformation)의 합으로 근사함으로써, FEM 대비 계산 속도를 획기적으로 높이면서도 전단력 추정에 필요한 충분한 정확도를 확보했다.</li>
</ul>
<h3>3.3  Taccel: GPU 가속과 물리 기반 렌더링의 통합</h3>
<p>최신 연구인 <strong>Taccel</strong>은 시뮬레이션의 속도와 물리적 정확도를 한 차원 높였다.12 Taxim이 CPU 기반으로 작동하여 대규모 병렬 학습에 병목이 되었던 반면, Taccel은 GPU 상에서 모든 연산을 처리한다.</p>
<ul>
<li><strong>IPC (Incremental Potential Contact):</strong> 얇은 탄성체와 물체 간의 교차(Intersection)나 터널링(Tunneling) 현상을 방지하고, 물리적으로 타당한 접촉 거동을 보장하는 IPC 알고리즘을 적용했다. 이는 얇은 젤 패드가 강한 힘에 의해 뚫리는 시뮬레이션 오류를 방지한다.</li>
<li><strong>ABD (Affine Body Dynamics):</strong> 탄성체의 변형을 효율적으로 계산하기 위해 아핀 변환 기반의 동역학 모델을 사용한다.</li>
<li><strong>성능 혁신:</strong> Taccel은 4,096개의 병렬 환경에서 초당 915 프레임(FPS) 이상의 속도를 달성했다. 이는 시각 기반 촉각 센서를 활용한 복잡한 조작 작업의 강화학습 시간을 획기적으로 단축시켰으며, 단순한 이미지 생성을 넘어 로봇의 동역학, 탄성체의 변형, 광학적 렌더링이 통합된 파이프라인을 실시간보다 훨씬 빠르게 처리함을 의미한다.</li>
</ul>
<h3>3.4  Tactile Gym 2.0: 이종 센서 간의 통합 플랫폼</h3>
<p>Tactile Gym 2.0은 GelSight 스타일의 센서뿐만 아니라 TacTip(마커 기반 돔형 센서)과 같은 서로 다른 원리의 센서들을 통합된 인터페이스로 제공한다.15 이는 Sim-to-Real(가상에서 현실로의 전이) 연구에 중요한 기여를 했는데, 특정 하드웨어에 종속되지 않는 범용적인 촉각 정책(Tactile Policy)을 학습시킬 수 있는 환경을 조성했기 때문이다.</p>
<h2>4.  지각(Perception): 접촉 이미지에서 물리량을 읽어내다</h2>
<p>센서가 생성한 고해상도 이미지는 그 자체로는 원시 데이터(Raw Data)일 뿐이다. 로봇 지능의 관점에서는 이 이미지로부터 마찰, 미끄러짐, 재질과 같은 물리적 속성을 추출하는 지각 알고리즘이 필요하다.</p>
<h3>4.1  미끄러짐 감지 (Slip Detection)</h3>
<p>안정적인 파지(Grasping)를 위해 미끄러짐을 조기에 감지하는 것은 무엇보다 중요하다. 인간은 피부의 진동을 통해 미끄러짐을 감지하지만, VBTS는 **광학 흐름(Optical Flow)**과 <strong>엔트로피(Entropy)</strong> 변화를 시각적으로 분석하여 이를 수행한다.4</p>
<ul>
<li><strong>상대 운동 분석:</strong> 물체가 미끄러질 때, 센서 표면의 마커는 탄성체의 복원력에 의해 제자리를 지키려는 경향이 있는 반면, 물체의 표면 텍스처는 이동한다. 이 두 패턴 간의 상대 속도 차이(Velocity Difference)를 분석하여 미끄러짐을 감지한다.</li>
<li><strong>엔트로피 및 진동 분석:</strong> 미끄러짐이 발생하기 직전(Incipient Slip), 접촉면의 마커 배열에 국소적인 진동이나 무질서도(Entropy)의 급격한 변화가 나타난다. 딥러닝 모델(CNN+LSTM)은 이러한 시공간적 특징을 학습하여, 마찰 계수를 모르는 물체에 대해서도 강건하게 미끄러짐을 예측한다.19</li>
</ul>
<h3>4.2  질감 및 재질 인식 (Texture &amp; Material Recognition)</h3>
<p>GelSight와 같은 센서는 수 마이크론 단위의 표면 거칠기를 시각화할 수 있으므로, 재질 분류에 있어 인간을 능가하는 성능을 발휘하기도 한다.</p>
<ul>
<li><strong>능동적 감지(Active Sensing):</strong> 단순히 센서를 대고 있는 것(Static Touch)보다, 문지르거나(Slide) 두드리는(Tap) 동작을 통해 얻은 동적 촉각 정보가 재질 인식률을 높인다. 연구에 따르면, 인간의 옷감 재질 인식률이 약 67%인 데 비해, GelSight Mini와 딥러닝(ResNet, ViT 등)을 결합한 시스템은 90% 이상의 정확도를 달성했다.20</li>
<li><strong>촉각 지문(Tactile Fingerprint):</strong> 의류의 직조 패턴이나 금속의 미세한 가공 흔적은 물체 인식(Object Identification)을 위한 고유한 특징이 된다. 이는 어두운 환경이나 시각적으로 가려진(Occluded) 상황에서 물체를 식별하는 데 결정적인 단서가 된다.</li>
</ul>
<h2>5.  조작 지능(Manipulation Intelligence): 본능적 제어의 구현</h2>
<p>가장 중요한 진전은 이러한 고해상도 촉각 센서가 단순한 ’측정기’를 넘어, 로봇의 능동적인 ’행동’을 결정하는 **조작 정책(Manipulation Policy)**의 핵심 입력으로 사용되기 시작했다는 점이다.</p>
<h3>5.1  “성공의 감각(The Feeling of Success)“과 멀티모달 파지</h3>
<p>Roberto Calandra 등의 선구적인 연구 “The Feeling of Success“는 딥러닝 기반의 촉각 파지 제어의 가능성을 증명했다.22 이 연구는 시각(Vision)만으로는 파지 성공 여부를 예측하는 데 근본적인 한계가 있음을 지적했다. 물체의 무게중심, 마찰력, 경도는 눈으로만 봐서는 알 수 없기 때문이다.</p>
<p>연구진은 시각 정보와 GelSight 촉각 정보를 융합한 멀티모달 신경망을 구축했다. 로봇이 물체를 잡은 직후의 촉각 이미지를 분석하여 “이 파지가 안정적인가?“를 판단하고, 불안정하다면 파지 위치를 조정(Regrasping)하는 정책을 학습시켰다. 실험 결과, 시각 정보에 촉각 정보를 더했을 때 파지 성공률이 비약적으로 상승했으며, 이는 **“로봇이 직접 쥐어봐야 안다”**는 직관을 데이터로 증명한 사례다.</p>
<h3>5.2  RotateIt: 시각-촉각 융합을 통한 인핸드 조작(In-Hand Manipulation)</h3>
<p>단순한 파지를 넘어, 손안에서 물체를 굴리며 자세를 바꾸는 **인핸드 조작(In-Hand Manipulation)**은 로봇 손재주의 정점이다. <strong>RotateIt</strong> 시스템은 VBTS와 강화학습을 결합하여 이 난제를 해결한 대표적인 사례다.24</p>
<h4>5.2.1  아키텍처: 시각-촉각 트랜스포머 (Visuo-Tactile Transformer)</h4>
<p>RotateIt은 시각, 촉각, 고유수용성 감각을 융합하기 위해 <strong>트랜스포머(Transformer)</strong> 구조를 채택했다. 이는 각 감각 정보가 갖는 서로 다른 시간적, 공간적 특성을 효과적으로 통합한다.</p>
<ul>
<li><strong>Vision:</strong> 깊이 이미지(Depth Image)를 통해 물체의 거시적인 형상과 자세를 파악한다. 실제 환경에서는 Segment Anything Model(SAM) 등을 활용하여 배경 잡음을 제거하고 물체 정보만을 추출한다.</li>
<li><strong>Touch:</strong> 촉각 센서(Digit)에서 얻은 접촉 위치(Contact Location)와 힘의 분포를 입력받는다. 시뮬레이션에서는 이산적인 접촉점(Discrete Contact Points)으로 근사화하여 학습 속도를 높이고, 현실에서는 고해상도 이미지에서 가장 변형이 큰 지점을 추출하여 매핑하는 방식을 사용한다.25</li>
<li><strong>Proprioception:</strong> 손가락의 관절 각도와 이전 행동(Action History)을 입력받아 로봇 핸드의 현재 상태를 인지한다.</li>
</ul>
<h4>5.2.2  2단계 학습 파이프라인과 특권 정보의 증류(Distillation)</h4>
<p>RotateIt은 현실 세계의 센서 노이즈와 지연 시간을 극복하고, 시뮬레이션의 이점을 극대화하기 위해 **특권 정보(Privileged Information)**를 활용한 2단계 학습 전략을 사용한다.24</p>
<ol>
<li><strong>Oracle Policy 학습 (Teacher):</strong> 시뮬레이션 상에서 물체의 정확한 질량, 마찰계수, 무게중심, 3D 메쉬(Point Cloud) 등 모든 물리적 정보를 완벽하게 알고 있는 ‘Oracle’ 에이전트를 학습시킨다. 이 에이전트는 PPO(Proximal Policy Optimization) 알고리즘을 통해 최적의 조작 정책을 습득한다.</li>
<li><strong>Visuotactile Policy 증류 (Student):</strong> 실제 로봇은 물리적 속성을 직접 알 수 없다. 따라서 시각과 촉각 센서 데이터만을 입력으로 받아, Oracle Policy가 알고 있는 잠재 벡터(Latent Vector)를 추론하도록 Student 네트워크(Visuotactile Transformer)를 학습시킨다. 이 과정은 Student가 불완전한 센서 정보로부터 물체의 물리적 특성을 ’상상’하도록 훈련시키는 것과 같다.</li>
</ol>
<p>이 과정을 통해 RotateIt은 학습 과정에서 본 적 없는 물체(Unseen Objects)에 대해서도 손가락 끝만으로 X, Y, Z축 등 다양한 축을 따라 물체를 회전시킬 수 있는 범용적인 능력을 획득했다. 특히 시각 정보가 가려지더라도 촉각 피드백을 통해 물체의 자세를 추정하고 제어를 유지하는 강건함을 보였는데, 이는 인간이 어둠 속에서 물건을 다루는 능력과 유사하다.26</p>
<h2>6.  결론 및 미래 전망: 촉각 파운데이션 모델을 향하여</h2>
<p>고해상도 시각 기반 촉각 센서(VBTS)는 로봇에게 섬세한 ’피부’를 부여함으로써 Embodied AI의 새로운 지평을 열었다. GelSight의 포토메트릭 원리에서 시작된 이 기술은 Digit 360과 같은 전방위 멀티모달 센서로 진화하며 하드웨어적 완성도를 높여가고 있다. 동시에 Taxim과 Taccel 같은 고속 시뮬레이션 기술은 데이터 기근을 해결하며 강화학습의 적용을 가속화했고, RotateIt과 같은 시스템은 시각과 촉각의 융합이 복잡한 조작 작업에서 필수불가결함을 증명했다.</p>
<p>향후 이 분야는 **‘촉각 파운데이션 모델(Tactile Foundation Model)’**의 등장으로 새로운 국면을 맞이할 것이다. 대규모의 촉각 데이터셋(Touch-Image pairs)과 로봇의 행동 데이터를 학습하여, 새로운 물체를 만졌을 때 즉각적으로 물리적 특성을 추론하고 최적의 파지 전략을 생성하는 범용 모델이 개발될 것이다.27 또한, 이러한 촉각 지능은 거대 언어 모델(LLM) 및 시각-언어-행동 모델(VLA)과 결합하여, 로봇이 물리적 상호작용을 통해 세상을 이해하고 추론하는 진정한 의미의 ’일반 범용 로봇(Generalist Robot)’으로 진화하는 데 핵심적인 역할을 수행할 것이다. 시각이 로봇에게 ’어디로 갈지’를 알려주었다면, 이제 고해상도 촉각은 로봇에게 ’세상과 어떻게 접촉하고 변화시킬지’를 가르치고 있다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>GelSight Sensors: 3D Tactile Imaging - Emergent Mind, https://www.emergentmind.com/topics/gelsight-sensors</li>
<li>Tactile Measurement with a GelSight Sensor Wenzhen Yuan - People, https://people.csail.mit.edu/yuan_wz/GelSight1/Wenzhen_Thesis_final.pdf</li>
<li>GelSight: High-Resolution Robot Tactile Sensors for Estimating Geometry and Force - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC5751610/</li>
<li>Learned Slip-Detection-Severity Framework using Tactile Deformation Field Feedback for Robotic Manipulation - arXiv, https://arxiv.org/html/2411.07442v1</li>
<li>DIGIT: A Novel Design for a Low-Cost Compact High-Resolution Tactile Sensor with Application to In-Hand Manipulation - Facebook Research, https://research.facebook.com/publications/digit-a-novel-design-for-a-low-cost-compact-high-resolution-tactile-sensor-with-application-to-in-hand-manipulation/</li>
<li>DIGIT: A Novel Design for a Low-Cost Compact High-Resolution Tactile Sensor with Application to In-Hand Manipulation, https://www.seas.upenn.edu/~dineshj/publication/lambeta-2020-digit/lambeta-2020-digit.pdf</li>
<li>Introducing Digit 360, https://digit.ml/</li>
<li>Digit 360 | Digit - Dexterous Manipulation and Touch Perception, https://digit.ml/digit360.html</li>
<li>Taxim: An Example-based Simulation Model for GelSight Tactile Sensors - arXiv, https://arxiv.org/pdf/2109.04027</li>
<li>[2109.04027] Taxim: An Example-based Simulation Model for GelSight Tactile Sensors, https://arxiv.org/abs/2109.04027</li>
<li>Robo-Touch/Taxim - GitHub, https://github.com/Robo-Touch/Taxim</li>
<li>Taccel: Scaling Up Vision-based Tactile Robotics via High-performance GPU Simulation, https://neurips.cc/virtual/2025/poster/118179</li>
<li>Taccel: Scaling Up Vision-based Tactile Robotics via High-performance GPU Simulation - Yixin Zhu, https://yzhu.io/publication/tactile2025neurips/paper.pdf</li>
<li>Taccel: Scaling Up Vision-based Tactile Robotics via High-performance GPU Simulation, https://arxiv.org/html/2504.12908v2</li>
<li>Tactile Gym 2.0: Sim-to-Real Deep Reinforcement Learning for Comparing Low-Cost High-Resolution Robot Touch - Semantic Scholar, https://www.semanticscholar.org/paper/Tactile-Gym-2.0%3A-Sim-to-Real-Deep-Reinforcement-for-Lin-Lloyd/87808112171390772bbe4d0e9eade906de0bf8e6</li>
<li>Tactile Gym 2.0: Sim-to-Real Deep Reinforcement Learning for Comparing Low-Cost High-Resolution Robot Touc | Request PDF - ResearchGate, https://www.researchgate.net/publication/362469124_Tactile_Gym_20_Sim-to-Real_Deep_Reinforcement_Learning_for_Comparing_Low-Cost_High-Resolution_Robot_Touc</li>
<li>Exploration of methods for in-hand slip detection with an event-based camera during pick-and-place motions - UPCommons, https://upcommons.upc.edu/bitstreams/e6b949c2-f2dc-4d44-8a71-a42748958468/download</li>
<li>Learning to Detect Slip through Tactile Estimation of the Contact Force Field and its Entropy, https://arxiv.org/html/2303.00935v4</li>
<li>Slip Detection with Deep Neural Network by Using GelSight Touch Sensor | MIT CSAIL, https://www.csail.mit.edu/research/slip-detection-deep-neural-network-using-gelsight-touch-sensor</li>
<li>[2403.13701] What Matters for Active Texture Recognition With Vision-Based Tactile Sensors - arXiv, https://arxiv.org/abs/2403.13701</li>
<li>Tactile Active Texture Recognition With Vision-Based Tactile Sensors, https://www.touchprocessing.org/2023/camera_ready/camera_ready_1.pdf</li>
<li>The Feeling of Success: Does Touch Sensing Help Predict Grasp Outcomes? - Proceedings of Machine Learning Research, http://proceedings.mlr.press/v78/calandra17a/calandra17a.pdf</li>
<li>More Than a Feeling: Learning to Grasp and Regrasp using Vision and Touch - Penn Engineering, https://www.seas.upenn.edu/~dineshj/publication/calandra-2018-more/calandra-2018-more.pdf</li>
<li>General In-Hand Object Rotation with Vision and Touch, https://proceedings.mlr.press/v229/qi23a/qi23a.pdf</li>
<li>General In-Hand Object Rotation with Vision and Touch - Haozhi Qi, https://haozhi.io/rotateit/</li>
<li>General In-Hand Object Rotation with Vision and Touch, https://ippc-iros23.github.io/papers/qi.pdf</li>
<li>Sparsh: Self-supervised touch representations for vision-based tactile sensing - arXiv, https://arxiv.org/html/2410.24090v1</li>
<li>FreeTacMan: Robot-free Visuo-Tactile Data Collection System for Contact-rich Manipulation, https://arxiv.org/html/2506.01941v2</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>