<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2.4.1 엔드투엔드(End-to-End) 학습의 부상: 픽셀(Pixel)에서 토크(Torque)까지</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2.4.1 엔드투엔드(End-to-End) 학습의 부상: 픽셀(Pixel)에서 토크(Torque)까지</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 2. 제어 이론의 역사와 소프트웨어 2.0</a> / <a href="index.html">2.4 미분 가능한 프로그래밍 (Differentiable Programming)</a> / <span>2.4.1 엔드투엔드(End-to-End) 학습의 부상: 픽셀(Pixel)에서 토크(Torque)까지</span></nav>
                </div>
            </header>
            <article>
                <h1>2.4.1 엔드투엔드(End-to-End) 학습의 부상: 픽셀(Pixel)에서 토크(Torque)까지</h1>
<h2>1. 서론: 로보틱스 제어 패러다임의 거대한 전환</h2>
<p>지난 반세기 동안 로보틱스와 자율 시스템(Autonomous Systems)을 지배해 온 공학적 설계 철학은 ’분할 정복(Divide and Conquer)’이라는 강력한 원칙에 기반하고 있었다. 복잡하고 비정형적인 현실 세계에서 로봇이 목적을 달성하기 위해 필요한 기능들은 인간 엔지니어의 인지적 한계 내에서 다룰 수 있는 수준의 하위 모듈들로 명확히 분해(Decomposition)되었다. 이른바 ‘인지(Perception)’, ‘추론 및 계획(Reasoning and Planning)’, 그리고 ’제어(Control)’로 이어지는 삼분법적 파이프라인(Modular Pipeline)은 로봇 공학의 불문율과도 같았다. 이 구조 하에서 ‘인지’ 모듈은 카메라나 라이다(LiDAR) 센서의 원시 데이터(Raw Data)를 처리하여 차선, 장애물, 보행자 등 인간이 사전에 정의한 의미론적 객체 목록(Object List)으로 변환하고, ‘계획’ 모듈은 이 정제된 정보를 바탕으로 최적의 경로를 생성하며, 마지막으로 ‘제어’ 모듈은 이를 추종하기 위한 구동기(Actuator)의 명령값을 계산한다.1</p>
<p>그러나 2010년대 중반, 심층 신경망(Deep Neural Network)의 비약적인 발전과 GPU(Graphics Processing Unit)를 위시한 연산 자원의 폭발적인 증가는 이러한 전통적 접근법에 근본적인 균열을 일으키기 시작했다. 수십 년간 정교하게 쌓아 올린 모듈러 아키텍처가 가진 한계—모듈 간 정보 전달 과정에서 발생하는 필연적인 정보 손실(Information Loss)과 앞단 모듈의 오차가 뒷단으로 증폭되어 전달되는 오류 전파(Error Propagation) 문제—를 해결하기 위한 대안으로, 입력 센서 데이터로부터 출력 제어 신호까지를 하나의 거대한 미분 가능한 함수(Differentiable Function)로 연결하려는 시도가 등장한 것이다. 이것이 바로 <strong>엔드투엔드(End-to-End) 학습</strong>의 태동이다.</p>
<p>본 섹션에서는 “픽셀에서 토크까지(From Pixel to Torque)“라는 슬로건으로 대표되는 이 새로운 제어 패러다임의 역사적 기원과 기술적 진화 과정을 심도 있게 추적한다. 1989년 카네기 멜론 대학(CMU)의 딘 포멀로(Dean Pomerleau)가 개발한 자율 주행 시스템 ALVINN의 선구적인 시도에서 출발하여, 테슬라(Tesla)의 안드레이 카패시(Andrej Karpathy)가 주창한 ’소프트웨어 2.0(Software 2.0)’이라는 철학적 프레임워크를 거쳐, 세르게이 레빈(Sergey Levine) 등이 정립한 심층 시각운동 정책(Deep Visuomotor Policies)에 이르기까지의 기술적 궤적을 상세히 분석한다. 또한, 이 접근법이 기존의 분석적 모델링(Analytical Modeling)으로는 해결하기 어려웠던 접촉 동역학(Contact Dynamics)과 비정형 환경의 불확실성을 어떻게 극복하고 있는지, 그리고 그 과정에서 제기되는 데이터 효율성과 안전성(Safety)이라는 새로운 난제들을 어떻게 해결해 나가고 있는지를 포괄적으로 논의한다.</p>
<h2>2.  여명기: ALVINN과 신경망 기반 제어의 시초</h2>
<p>엔드투엔드 로보틱스의 개념은 현대 딥러닝 붐보다 훨씬 앞선 1980년대 후반, 신경망 연구의 첫 번째 부흥기인 연결주의(Connectionism) 시대에 이미 싹트고 있었다. 그 시초에는 CMU의 로봇 공학 연구소(Robotics Institute)에서 진행된 전설적인 프로젝트, **ALVINN(Autonomous Land Vehicle In a Neural Network)**이 존재한다.4 당시 자율 주행 연구의 주류는 규칙 기반(Rule-based) 알고리즘과 정교한 컴퓨터 비전 기술을 결합하여 도로의 경계를 수학적으로 검출하려는 시도였으나, 포멀로는 이러한 명시적 프로그래밍 없이 신경망이 스스로 도로를 ‘보고’ 운전할 수 있다는 급진적인 가설을 세웠다.</p>
<h3>2.1  ALVINN의 아키텍처와 감각 처리의 혁신</h3>
<p>ALVINN의 신경망 구조는 수백 개의 레이어를 가진 현대의 모델들과 비교하면 믿을 수 없을 정도로 단순하지만, 엔드투엔드 학습이 갖춰야 할 모든 핵심 요소를 완벽하게 내재하고 있었다. 이 시스템은 입력층(Input Layer), 은닉층(Hidden Layer), 출력층(Output Layer)으로 구성된 3층 피드포워드(Feed-forward) 네트워크, 즉 다층 퍼셉트론(MLP) 구조를 채택했다.5</p>
<p>입력층의 구성과 전처리 전략:</p>
<p>ALVINN의 입력층은 총 1,217개의 유닛으로 구성되었으며, 이는 다시 세 가지 서로 다른 감각 채널로 세분화되었다.</p>
<ol>
<li><strong>비디오 입력 망막(Video Input Retina):</strong> 30x32 해상도의 저해상도 그레이스케일 이미지를 받아들이는 960개의 유닛. 당시 컴퓨팅 파워의 제약으로 인해 컬러 이미지를 그대로 처리하는 것은 불가능했다. 포멀로는 도로(아스팔트)와 비도로(수풀, 흙) 사이의 대비(Contrast)가 가장 뚜렷하게 나타나는 스펙트럼이 <strong>청색(Blue) 대역</strong>이라는 사실을 실험적으로 발견하고, 입력 이미지의 청색 채널 강도만을 추출하여 신경망에 주입했다. 이는 데이터의 차원을 줄이면서도 정보의 질을 극대화한 초기 형태의 특징 공학(Feature Engineering)이었다.5</li>
<li><strong>레이저 거리 측정기 망막(Laser Range Finder Retina):</strong> 8x32 해상도의 256개 유닛으로 구성된 이 채널은 레이저 센서로부터 얻은 거리 정보를 처리했다. 각 유닛의 활성화 정도는 해당 공간 영역의 근접도에 비례하도록 설계되어, 단순한 2D 이미지만으로는 파악하기 어려운 3차원 장애물 정보를 보완하는 역할을 했다.5</li>
<li><strong>피드백 유닛(Feedback Unit):</strong> 마지막 1개의 유닛은 이전 프레임에서 도로가 주변 배경보다 밝았는지 어두웠는지를 기억하는 재귀적(Recurrent) 성격을 띠었다. 이는 터널 진입이나 도로 재질 변화와 같이 급격한 조명 변화가 발생할 때 신경망의 적응력을 높이는 결정적인 역할을 했다.</li>
</ol>
<p>은닉층과 완전 연결성:</p>
<p>이 1,217개의 입력 유닛들은 단 29개의 유닛으로 구성된 은닉층과 완전 연결(Fully Connected)되었다. 이 29개의 은닉 유닛들은 도로의 방향, 곡률, 장애물의 위치와 같은 고차원적인 특징들을 스스로 추출하고 조합하는 ‘블랙박스’ 역할을 수행했다.</p>
<h3>2.2  출력 표현의 정교화: 가우시안 활성화 언덕</h3>
<p>ALVINN의 출력층은 차량이 나아가야 할 조향 방향(Steering Direction)을 결정하는 45개의 유닛과 도로 밝기 피드백을 위한 1개의 유닛으로 구성되었다.5 여기서 주목할 점은 포멀로가 채택한 <strong>출력 인코딩(Output Encoding)</strong> 방식이다.</p>
<p>단순히 “왼쪽으로 10도 꺾어라“라는 하나의 스칼라 값을 출력하는 회귀(Regression) 방식 대신, ALVINN은 45개의 유닛이 각각 특정 조향 각도를 대표하도록 설계했다. 중앙의 유닛은 직진을, 좌우 끝단의 유닛은 각각 좌우 최대 회전을 의미했다. 학습 시 정답 레이블(Label)은 단 하나의 유닛만을 1로 만드는 원-핫(One-hot) 인코딩이 아니었다. 대신 정답 각도를 중심으로 주변 유닛들의 활성화 값이 점진적으로 줄어드는 <strong>‘가우시안 활성화 언덕(Hill of Activation)’</strong> 형태를 사용했다.5</p>
<p>예를 들어, 정답이 중앙(직진)이라면, 중앙 유닛은 1.0, 그 바로 옆 유닛은 0.89, 그 다음은 0.61, 0.32, 0.10 순으로 활성화되는 식이다. 이러한 분포형 출력 방식은 신경망이 미세한 조향 차이를 더 부드럽게 학습할 수 있게 도와주었으며, 단일 유닛의 오작동에 대한 강건성(Robustness)을 확보해 주었다. 주행 시에는 이 45개 유닛의 활성화 분포 무게중심(Centroid)을 계산하여 최종 조향 명령을 생성했다.</p>
<h3>2.3  훈련의 혁신: 가상 실수 생성과 주행 중 학습</h3>
<p>ALVINN 프로젝트가 남긴 가장 위대한 유산 중 하나는 데이터 수집과 학습 방법론에 있다. 포멀로는 사람이 운전하는 차량에 동승하여, 카메라가 보는 도로 이미지와 운전자가 조작하는 조향 핸들의 각도를 실시간으로 기록하여 학습 데이터로 사용했다. 이를 ‘모방 학습(Imitation Learning)’ 또는 ’행동 복제(Behavioral Cloning)’라고 한다.4</p>
<p>그러나 이 방식에는 치명적인 맹점이 있었다. 인간 운전자는 대부분의 시간 동안 도로 중앙을 따라 매우 안정적으로 주행한다. 따라서 수집된 데이터의 90% 이상은 ‘직진’ 또는 ’완만한 커브’에 편향되어 있었다. 이 데이터로만 학습한 신경망은 차량이 도로 가장자리로 밀려나거나 실수로 차선을 이탈했을 때 어떻게 복구해야 하는지를 전혀 배우지 못했다. 이를 <strong>공변량 변화(Covariate Shift)</strong> 문제라고 한다.</p>
<p>이를 해결하기 위해 포멀로는 <strong>‘가상 데이터 생성 및 증강(On-the-fly Simulation)’</strong> 기법을 고안했다. 그는 실시간으로 입력되는 도로 이미지를 기하학적으로 변환(Transformation)하여, 차량이 도로 좌측이나 우측으로 치우쳐 있는 상황을 가상으로 만들어냈다. 그리고 이렇게 치우친 상황에서는 원래의 조향 각도가 아닌, 도로 중앙으로 복귀하기 위한 보정된 조향 각도를 정답으로 제시했다.7 이를 통해 ALVINN은 실제로 발생하지 않은 ’실수 상황’들까지 학습할 수 있었고, 결과적으로 인간보다 더 안정적인 자율 주행 능력을 보여주었다.</p>
<p>ALVINN은 최대 시속 70마일(약 112km/h)로 90마일(약 145km) 이상의 거리를 자율 주행하는 데 성공하며 6, 복잡한 규칙 코딩 없이도 신경망이 픽셀에서 제어 신호로의 직접 매핑을 수행할 수 있음을 역사적으로 증명했다. 비록 90년대의 하드웨어 한계로 인해 상용화의 문턱을 넘지는 못했지만, ALVINN이 정립한 이 방법론은 20년 후 딥러닝의 부활과 함께 완벽하게 재현되었다.</p>
<h2>3.  소프트웨어 2.0: 코딩에서 최적화로의 철학적 전회</h2>
<p>ALVINN이 남긴 기술적 유산이 2010년대의 딥러닝 혁명과 결합하면서, 로보틱스 소프트웨어를 바라보는 관점 자체가 송두리째 바뀌기 시작했다. 2017년, 당시 테슬라(Tesla)의 AI 부문을 이끌던 안드레이 카패시(Andrej Karpathy)는 이러한 변화를 **‘소프트웨어 2.0(Software 2.0)’**이라는 도발적인 개념으로 정의하며, 엔드투엔드 학습을 단순한 기술이 아닌 새로운 프로그래밍 패러다임으로 격상시켰다.11</p>
<h3>3.1  소프트웨어 1.0 vs. 소프트웨어 2.0</h3>
<p>카패시의 정의에 따르면, 우리가 지금까지 작성해 온 전통적인 소프트웨어는 <strong>소프트웨어 1.0</strong>이다. C++, Python, Java 등의 언어를 사용하여 프로그래머가 컴퓨터에게 수행할 명령을 명시적(Explicit)인 논리 구조로 작성한다. 로봇 제어기를 예로 들면, 엔지니어는 센서 데이터를 받아 노이즈를 제거하고, 특정 임계값을 기준으로 상태를 판단하며, PID 제어 수식을 통해 모터 출력을 계산하는 수천 줄의 코드를 직접 작성해야 한다. 이 방식에서 프로그램의 동작 공간(Program Space)은 인간이 작성한 코드의 복잡도에 의해 제한된다.13</p>
<p>반면, <strong>소프트웨어 2.0</strong>은 프로그래머가 소스 코드를 직접 작성하지 않는다. 대신, 프로그램이 달성해야 할 <strong>목표(Goal)</strong>(예: “이 이미지 입력에 대해 올바른 조향 각도를 출력하라”)와 이를 충족시키는 **데이터셋(Dataset)**을 정의한다. 그리고 신경망이라는 거대한 범용 함수 근사기(Universal Function Approximator) 안에서, 최적화 알고리즘(예: 경사 하강법, Stochastic Gradient Descent)을 사용하여 이 목표를 만족시키는 최적의 프로그램(가중치 집합)을 자동으로 검색(Search)한다.11</p>
<table><thead><tr><th><strong>비교 속성</strong></th><th><strong>소프트웨어 1.0 (Classical Code)</strong></th><th><strong>소프트웨어 2.0 (Neural Networks)</strong></th></tr></thead><tbody>
<tr><td><strong>작성 주체</strong></td><td>인간 프로그래머</td><td>최적화 알고리즘 (예: SGD)</td></tr>
<tr><td><strong>구성 요소</strong></td><td>명시적 논리, 루프, 조건문 (C++, Python)</td><td>수백만 개의 부동소수점 가중치 (Weights)</td></tr>
<tr><td><strong>개발자의 역할</strong></td><td>알고리즘 설계 및 코딩</td><td>데이터 수집, 정제, 라벨링, 아키텍처 설계</td></tr>
<tr><td><strong>특징</strong></td><td>해석 가능, 모듈화 용이, 유지보수 복잡</td><td>해석 난해(Black Box), 데이터 의존적, 고성능</td></tr>
<tr><td><strong>실행 효율</strong></td><td>인간이 최적화한 만큼 효율적</td><td>하드웨어 가속(GPU/TPU)에 최적화된 연산</td></tr>
<tr><td><strong>적용 분야</strong></td><td>웹 서버, 운영체제, 명확한 규칙 시스템</td><td>이미지 인식, 음성 처리, 비정형 로봇 제어</td></tr>
</tbody></table>
<h3>3.2  로보틱스에서의 필연성과 ’컴파일러’로서의 최적화</h3>
<p>카패시는 경사 하강법(Backpropagation &amp; Gradient Descent)을 소프트웨어 2.0 시대의 **‘컴파일러’**에 비유했다.13 우리가 C++ 코드를 기계어로 컴파일하듯, 소프트웨어 2.0 엔지니어는 데이터셋이라는 고수준의 명세를 신경망 가중치라는 기계어로 컴파일한다.</p>
<p>로보틱스, 특히 자율 주행이나 휴머노이드 로봇 제어와 같은 분야에서 소프트웨어 2.0이 선택이 아닌 필수가 되는 이유는 현실 세계의 <strong>복잡성(Complexity)</strong> 때문이다. 비포장도로의 마찰 계수 변화, 눈보라 속에서의 라이다 반사 특성, 찌그러진 캔을 집을 때의 변형 역학 등을 모두 수식으로 모델링하고 코드로 작성하는 것은 불가능에 가깝다. 이를 ’모델링 불가능한 동역학(Unmodeled Dynamics)’이라고 한다.15 소프트웨어 2.0은 이러한 복잡성을 인간이 이해하려 노력하는 대신, 방대한 데이터 속에 내재된 패턴으로 치환하여 신경망이 스스로 학습하게 함으로써 문제를 해결한다.</p>
<p>테슬라의 오토파일럿(Autopilot) 팀이 차선 감지, 객체 인식 등의 개별 모듈을 직접 코딩하던 방식에서, 주행 영상 전체를 입력으로 받아 경로를 출력하는 엔드투엔드 신경망(FSD v12 등)으로 전환한 것은 이러한 철학의 가장 극적인 실현이다.14 이제 로봇 엔지니어의 주된 업무는 제어 이론을 연구하는 것에서 벗어나, 로봇에게 보여줄 ’양질의 교과서(데이터셋)’를 만드는 것으로 이동하고 있다.</p>
<h2>4.  핵심 방법론의 정립: 심층 시각운동 정책 (Deep Visuomotor Policies)</h2>
<p>소프트웨어 2.0의 철학이 로봇 팔(Manipulator) 제어라는 구체적인 도메인에서 실현된 결정적인 계기는 2015-2016년 UC 버클리의 세르게이 레빈(Sergey Levine) 교수 팀이 발표한 일련의 연구들, 특히 <strong>“End-to-End Training of Deep Visuomotor Policies”</strong> 논문이다.18 이 연구는 카메라 픽셀 입력에서 로봇 관절의 토크 출력까지를 하나의 신경망으로 매핑하는 <strong>‘픽셀 투 토크(Pixel-to-Torque)’</strong> 제어 이론을 체계적으로 정립했다.</p>
<h3>4.1  문제 의식: 기존 강화 학습의 샘플 비효율성</h3>
<p>당시에도 딥마인드(DeepMind)의 DQN(Deep Q-Network)이 아타리(Atari) 게임을 정복하는 등 딥러닝과 강화 학습(RL)의 결합이 주목받고 있었다. 그러나 이러한 알고리즘을 로봇 하드웨어에 직접 적용하는 것은 불가능에 가까웠다. 가상의 게임 환경과 달리, 실제 로봇은 동작 속도가 느리고, 하드웨어 마모나 파손 위험이 있으며, 데이터를 수집하는 데 엄청난 시간이 소요되기 때문이다. 이를 <strong>샘플 효율성(Sample Efficiency)</strong> 문제라고 한다.21 수백만 번의 시행착오를 겪어야만 학습되는 기존 RL 방식으로는 간단한 블록 끼우기 작업조차 학습시키는 데 며칠이 걸릴지 알 수 없었다.</p>
<h3>4.2  유도된 정책 탐색(Guided Policy Search, GPS) 알고리즘</h3>
<p>레빈 팀은 이 문제를 해결하기 위해 **유도된 정책 탐색(Guided Policy Search, GPS)**이라는 독창적인 하이브리드 알고리즘을 제안했다.20 GPS의 핵심 아이디어는 복잡한 시각 기반 제어 정책을 처음부터(from scratch) 학습시키는 대신, 로봇의 상태(State) 정보를 모두 알고 있는 ‘교사(Teacher)’ 알고리즘의 도움을 받아 학습을 가속화하는 것이다.</p>
<p>GPS 알고리즘은 크게 두 단계의 반복 루프로 구성된다:</p>
<ol>
<li>궤적 최적화 단계 (Trajectory Optimization Phase):</li>
</ol>
<p>이 단계에서는 로봇의 관절 각도, 속도, 물체의 위치 등 **완전한 상태 정보(Full State Information)**를 활용할 수 있다고 가정한다. 연구팀은 **BADMM(Bregman Alternating Direction Method of Multipliers)**이라는 최적화 기법을 사용하여, 현재 상황에서 로봇이 수행해야 할 이상적인 행동 궤적(Trajectory)을 생성한다.19 이는 시각 정보 처리의 어려움을 배제하고, 순수하게 물리적으로 최적화된 움직임을 만들어내는 과정이다. 이 궤적들은 신경망이 따라야 할 ‘정답지’ 역할을 한다.</p>
<ol start="2">
<li>지도 학습 단계 (Supervised Learning Phase):</li>
</ol>
<p>이제 딥러닝이 등장할 차례다. 1단계에서 생성된 최적 궤적 데이터를 훈련 데이터로 사용하여, 심층 시각운동 정책(Deep Visuomotor Policy) 신경망을 학습시킨다. 이때 신경망의 입력은 완전한 상태 정보가 아닌, 로봇의 카메라가 찍은 RGB 이미지와 로봇의 현재 관절 각도(Proprioception)뿐이다. 신경망은 최적 궤적 생성기가 만들어낸 토크 명령을 흉내 내도록(Cloning) 학습된다.18</p>
<p>이 두 단계는 서로 맞물려 돌아간다. 신경망이 학습됨에 따라 궤적 최적화 단계는 신경망이 흉내 낼 수 있는 범위 내의 궤적을 생성하도록 제약(Constraint)을 받게 된다. 결과적으로 로봇은 수백만 번의 시행착오 없이도, 수십 분 내지 수 시간의 데이터 수집만으로 복잡한 작업을 학습할 수 있게 된다.</p>
<h3>4.3  신경망 아키텍처와 실제 적용 사례</h3>
<p>레빈 팀이 설계한 신경망 아키텍처는 시각 처리를 담당하는 합성곱 신경망(CNN) 파트와 로봇의 상태 정보를 처리하는 완전 연결(Fully Connected) 파트가 병합되는 구조를 가졌다.</p>
<ul>
<li><strong>시각 처리:</strong> 7개의 합성곱 레이어(Convolutional Layers)가 이미지에서 로봇 팔의 위치와 대상 물체의 특징을 추출한다. 초기 레이어의 가중치는 ImageNet 데이터셋으로 사전 학습(Pre-trained)된 모델을 가져와 초기화함으로써 학습 속도를 높였다.18</li>
<li><strong>공간적 특징점(Spatial Softmax):</strong> 마지막 합성곱 레이어의 출력은 ’Spatial Softmax’라는 특수한 연산을 통해 이미지 상의 주요 특징점(Keypoints) 좌표로 변환된다. 이는 고차원 이미지 데이터를 로봇 제어에 적합한 저차원 좌표 정보로 압축하는 핵심적인 역할을 했다.</li>
<li><strong>제어 정책:</strong> 추출된 시각적 특징과 로봇의 관절 각도 정보가 결합되어, 최종적으로 7자유도 로봇 팔의 각 관절 모터에 전달될 <strong>토크(Torque)</strong> 값을 출력한다.</li>
</ul>
<p>이 시스템은 PR2 로봇을 사용하여 다음과 같은 정교한 작업들을 성공적으로 수행해냈다:</p>
<ul>
<li><strong>옷걸이 걸기:</strong> 옷걸이를 집어 옷장에 거는 작업. 옷걸이의 미세한 흔들림과 옷장 봉의 위치를 시각적으로 파악하고 조절해야 한다.</li>
<li><strong>도형 맞추기:</strong> 정육면체 블록을 모양에 맞는 구멍에 정확히 끼워 넣는 작업. 매우 높은 정밀도가 요구된다.</li>
<li><strong>병뚜껑 돌리기:</strong> 병뚜껑을 잡고 나사선에 맞춰 돌려 끼우는 작업. 접촉 힘 조절과 회전 동작의 협응이 필수적이다.</li>
</ul>
<p>특히 중요한 점은 이 모든 과정에서 카메라 캘리브레이션(Calibration)이나 물체의 3D 모델링이 전혀 필요 없었다는 것이다. 신경망은 픽셀 변화와 토크 출력 사이의 인과관계를 스스로 학습하여, 카메라 위치가 바뀌거나 조명이 변해도 적응할 수 있는 유연성을 보여주었다.18</p>
<h2>5.  극한의 도전: 고속 불안정 시스템과 접촉 동역학</h2>
<p>레빈의 연구가 정적인(Quasi-static) 조작 작업에서 픽셀 투 토크의 가능성을 열었다면, 이후의 연구들은 이를 더욱 극한의 환경—매우 빠르고 불안정한 시스템, 그리고 복잡한 마찰이 지배하는 환경—으로 확장해 나갔다.</p>
<h3>5.1  고속 제어의 한계 돌파: 후루타 펜듈럼 실험</h3>
<p>2022년 발표된 <strong>“Learning Fast and Precise Pixel-to-Torque Control”</strong> 연구는 시각 기반 제어가 얼마나 빨라질 수 있는지를 증명한 중요한 사례다.24 연구진은 <strong>후루타 펜듈럼(Furuta Pendulum)</strong>, 즉 회전하는 팔 끝에 달린 진자를 거꾸로 세워 균형을 잡아야 하는 매우 불안정한 시스템을 실험 대상으로 삼았다.</p>
<p>이 시스템을 제어하기 위해서는 최소 100Hz(초당 100회) 이상의 제어 주파수가 필요하다. 즉, 카메라 이미지를 찍고, 신경망을 통과시켜 토크를 계산하고, 모터에 명령을 내리는 전 과정이 0.01초(10ms) 이내에 이루어져야 한다. 기존의 무거운 컴퓨터 비전 알고리즘으로는 불가능한 속도였다.</p>
<p>연구진은 다음과 같은 전략으로 이 문제를 해결했다:</p>
<ul>
<li><strong>경량화된 신경망:</strong> 제어 루프 내에서 실시간 추론이 가능하도록 신경망의 크기를 최적화했다.</li>
<li><strong>상태 추정기(State Estimator)의 분리:</strong> 이미지를 직접 제어 정책에 넣는 대신, 먼저 이미지로부터 진자의 각도와 속도를 추정하는 신경망(State Estimator)을 학습시키고, 이 추정된 상태를 바탕으로 토크를 계산하는 구조를 채택했다. 이는 순수한 엔드투엔드보다는 약간의 모듈화를 도입한 것이지만, 여전히 픽셀 데이터에서 물리적 모델링 없이 상태를 추출한다는 점에서 엔드투엔드 철학을 공유한다.26</li>
<li><strong>데이터 편향 제거:</strong> 진자가 거꾸로 서 있는 상태(Upright)의 데이터가 압도적으로 많은 불균형을 해결하기 위해, 데이터 증강과 균형 잡힌 샘플링 기법을 적용했다.</li>
</ul>
<p>결과적으로 이 시스템은 고속 카메라와 신경망만으로 물리적 센서(인코더) 없이도 불안정한 진자의 균형을 완벽하게 잡아내는 데 성공했다. 이는 딥러닝 기반 제어가 느린 인지 작업뿐만 아니라, 고성능 동적 제어(Dynamic Control) 영역에서도 기존 제어 이론을 대체하거나 보완할 수 있음을 시사한다.</p>
<h3>5.2  모델링 불가능한 영역: 접촉과 마찰</h3>
<p>엔드투엔드 학습이 가장 빛을 발하는, 그리고 반드시 필요한 영역은 바로 **접촉 동역학(Contact Dynamics)**이다. 로봇이 물체를 쥐거나, 조립하거나, 표면을 닦는 작업에서 발생하는 물리 현상은 불연속적(Discontinuous)이고 비선형적이다. 마찰력, 점성, 물체의 변형 등은 수식으로 완벽하게 모델링하기가 사실상 불가능하다.27</p>
<p><strong>ContactNets</strong>와 같은 연구들은 이러한 접촉의 물리학을 명시적인 수식 대신 신경망 내부에 암시적(Implicit)으로 인코딩하는 방법을 제안했다.28 로봇이 물체와 상호작용하는 데이터를 대량으로 학습함으로써, 신경망은 “이런 픽셀 패턴(표면 거칠기)에서 이 정도 속도로 접근하면 미끄러진다“는 미묘한 물리 법칙을 경험적으로 체득하게 된다. 이는 로봇이 사전 지식 없는 낯선 물체를 조작하거나, 마찰 계수가 변하는 환경에서도 강건하게 작업을 수행할 수 있는 원동력이 된다.</p>
<h2>6.  비교 분석: 모듈러 파이프라인 vs. 엔드투엔드</h2>
<p>엔드투엔드 학습의 부상은 기존 모듈러 파이프라인의 종말을 의미하는가? 그렇지 않다. 두 접근법은 서로 다른 철학적 기반과 장단점을 가진 대안적 선택지이며, 적용하려는 도메인의 특성에 따라 그 유용성이 갈린다.1 이를 구조적, 기능적 측면에서 심층 비교해 보자.</p>
<h3>6.1  정보 병목(Information Bottleneck)과 표현력의 차이</h3>
<p><strong>모듈러 파이프라인</strong>의 가장 큰 특징은 정보를 단계적으로 압축하고 추상화한다는 점이다. ‘인지’ 모듈은 풍부한 픽셀 데이터에서 오직 ’객체의 종류와 위치’라는 정보만을 추출하여 다음 단계로 넘긴다. 이 과정에서 도로의 질감, 보행자의 미묘한 시선, 그림자의 형태 등 비정형 정보들은 ’불필요한 노이즈’로 간주되어 버려진다. 이를 <strong>정보 병목(Information Bottleneck)</strong> 현상이라고 한다.2</p>
<p>반면 <strong>엔드투엔드 시스템</strong>은 이러한 병목을 제거한다. 센서의 원시 데이터가 제어 출력단까지 손실 없이 흘러갈 수 있는 경로(Pathway)가 열려 있기 때문에, 신경망은 인간 엔지니어가 미처 중요하다고 생각하지 못했던 단서(Cues)—예를 들어, 앞차의 배기가스 형태나 숲길의 흙 색깔 변화—까지 활용하여 제어를 수행할 수 있다. 이는 비정형 환경에서의 강건성(Robustness)으로 이어진다.29</p>
<h3>6.2  최적화의 범위: 부분 최적화 vs. 전역 최적화</h3>
<p>모듈러 시스템은 각 모듈이 독립적으로 개발되고 최적화된다(Decoupled). 인지 팀은 인식률을 높이는 데 집중하고, 제어 팀은 경로 추종 오차를 줄이는 데 집중한다. 그러나 “인식률이 높은 것“이 반드시 “운전을 잘하는 것“으로 직결되지는 않는다. 각 모듈의 **국소 최적화(Local Optimization)**가 전체 시스템의 성능 저하(Sub-optimality)로 이어질 수 있다. 또한 앞 단계의 작은 오차가 뒤로 갈수록 증폭되는 <strong>오류 전파(Error Propagation)</strong> 문제는 모듈러 시스템의 고질적인 난제다.31</p>
<p>엔드투엔드 학습은 시스템 전체를 관통하는 단일 손실 함수(Loss Function)—예를 들어, “목적지까지 충돌 없이 도착했는가”—를 정의하고, 이를 통해 전체 네트워크를 한 번에 학습시킨다. 이를 **전역 최적화(Joint Optimization)**라고 한다. 이 과정에서 ‘인지’ 역할을 하는 신경망의 앞단 레이어들은 단순히 객체를 인식하는 것을 넘어, <strong>‘제어에 도움이 되는’</strong> 특징을 추출하도록 스스로를 조정한다. 즉, 인지가 행동의 종속 변수가 되어 유기적으로 결합되는 것이다.29</p>
<h3>6.3  해석 가능성(Interpretability)과 디버깅</h3>
<p>엔드투엔드 방식의 아킬레스건은 **해석 가능성(Interpretability)**의 부재다. 모듈러 시스템은 사고가 발생했을 때 “라이다가 장애물을 놓쳤다“거나 “경로 계획 알고리즘의 예외 처리가 실패했다“는 식으로 명확한 원인 규명이 가능하다. 반면, 엔드투엔드 신경망은 수백만 개의 파라미터가 얽혀 있는 블랙박스(Black Box)이므로, 왜 로봇이 그런 행동을 했는지 설명하기가 매우 어렵다.2 이는 안전이 최우선인 자율 주행이나 의료 로봇 분야에서 규제 당국과 대중을 설득하는 데 큰 걸림돌이 된다.</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>모듈러 파이프라인 (Modular Pipeline)</strong></th><th><strong>엔드투엔드 학습 (End-to-End Learning)</strong></th></tr></thead><tbody>
<tr><td><strong>구조</strong></td><td>인지 → 계획 → 제어의 순차적 연결</td><td>센서 → 신경망 → 액추에이터의 직접 연결</td></tr>
<tr><td><strong>최적화 단위</strong></td><td>각 모듈별 개별 최적화</td><td>전체 시스템 통합 최적화 (Joint Optimization)</td></tr>
<tr><td><strong>정보 흐름</strong></td><td>인간 정의 추상화 (객체 목록 등)</td><td>데이터 주도적 특징 표현 (Latent Representation)</td></tr>
<tr><td><strong>데이터 요구량</strong></td><td>적음 (규칙 및 사전 지식 활용)</td><td>매우 방대함 (Software 2.0 특성)</td></tr>
<tr><td><strong>해석 가능성</strong></td><td>높음 (화이트박스, 디버깅 용이)</td><td>낮음 (블랙박스, 설명 난해)</td></tr>
<tr><td><strong>유연성</strong></td><td>새로운 환경 적응 시 재설계 필요</td><td>데이터 추가 학습으로 적응 가능 (Adaptability)</td></tr>
<tr><td><strong>주요 난제</strong></td><td>오류 전파, 정보 병목</td><td>샘플 비효율성, 해석 불가능성, 안전 보장</td></tr>
</tbody></table>
<h2>7.  미래 전망 및 결론: 하이브리드와 안전한 학습을 향하여</h2>
<p>엔드투엔드 학습은 로보틱스의 오랜 꿈인 ’적응형 지능’을 실현할 가장 강력한 도구임이 입증되었다. 그러나 상용화를 위해서는 여전히 넘어야 할 산이 높다. 현재 연구의 최전선은 이 방식의 단점인 안전성과 데이터 효율성을 보완하는 방향으로 빠르게 이동하고 있다.</p>
<p>합성 데이터와 심투리얼(Sim-to-Real):</p>
<p>현실 세계의 데이터 수집 비용을 극복하기 위해, 물리 엔진 기반의 고정밀 시뮬레이션에서 수십억 번의 학습을 수행한 뒤 이를 현실로 전이하는 기술이 표준이 되고 있다. 도메인 무작위화(Domain Randomization)—시뮬레이션의 마찰, 조명, 질감 등을 무작위로 변동시켜 학습하는 기법—는 시뮬레이션과 현실의 격차(Reality Gap)를 메우는 핵심 기술로 자리 잡았다.33</p>
<p>안전 제약 조건(Safety Constraints)의 결합:</p>
<p>확률적인 신경망의 출력을 맹신하지 않고, 수학적으로 증명된 안전 장치를 결합하려는 시도도 활발하다. **제어 장벽 함수(Control Barrier Functions, CBF)**와 같은 이론을 적용하여, 신경망이 자유롭게 제어 명령을 내리되, 로봇이 절대로 넘어서는 안 되는 물리적 한계나 안전 영역(Safe Set)을 벗어나려 할 때만 개입하여 강제로 억제하는 ‘안전 필터(Safety Filter)’ 개념이 도입되고 있다.35</p>
<p><strong>결론적으로,</strong> “픽셀에서 토크까지“의 여정은 로봇 공학이 인간의 명시적 지식을 주입하는 단계(Software 1.0)를 지나, 데이터로부터 스스로 행동 양식을 깨우치는 단계(Software 2.0)로 진화하고 있음을 보여주는 거대한 이정표다. ALVINN이 쏘아 올린 작은 가능성은 이제 거대 언어 모델(LLM)과 결합한 ’거대 행동 모델(Large Behavior Model)’로 확장되며, 로봇이 단순한 기계를 넘어 진정한 의미의 자율 지능체로 거듭나는 시대를 예고하고 있다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Autonomous driving: Modular pipeline Vs. End-to-end and LLMs | by Samer Attrah | Medium, https://medium.com/@samiratra95/autonomous-driving-modular-pipeline-vs-end-to-end-and-llms-642ca7f4ef89</li>
<li>A Survey of End-to-End Driving: Architectures and Training Methods - arXiv, https://arxiv.org/pdf/2003.06404</li>
<li>Classical Robotics vs. End-to-End Learning - blogs.dal.ca, https://blogs.dal.ca/openthink/classical-robotics-vs-end-to-end-learning/</li>
<li>1월 3, 2026에 액세스, [https://python.plainenglish.io/an-autonomous-land-vehicle-in-a-neural-network-pioneering-autonomous-navigation-c87d6d4ed8d5#:<sub>:text=The%20Genesis%20of%20ALVINN&amp;text=At%20its%20core%2C%20ALVINN%20was,a%20simple%20artificial%20neural%20network.](https://python.plainenglish.io/an-autonomous-land-vehicle-in-a-neural-network-pioneering-autonomous-navigation-c87d6d4ed8d5#:</sub>:text=The Genesis of ALVINN&amp;text=At its core%2C ALVINN was, <a href="https://python.plainenglish.io/an-autonomous-land-vehicle-in-a-neural-network-pioneering-autonomous-navigation-c87d6d4ed8d5#:~:text=The%20Genesis%20of%20ALVINN&amp;text=At%20its%20core%2C%20ALVINN%20was,a%20simple%20artificial%20neural%20network.">https://python.plainenglish.io/an-autonomous-land-vehicle-in-a-neural-network-pioneering-autonomous-navigation-c87d6d4ed8d5#:~:text=The%20Genesis%20of%20ALVINN&amp;text=At%20its%20core%2C%20ALVINN%20was,a%20simple%20artificial%20neural%20network.</a></li>
<li>ALVINN: An Autonomous Land Vehicle in a Neural Network, https://papers.neurips.cc/paper_files/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf</li>
<li>Defense and Civilian Applications of the Alvinn Robot Driving System, https://www.ri.cmu.edu/pub_files/pub3/pomerleau_dean_1994_1/pomerleau_dean_1994_1.pdf</li>
<li>E cient Training of Arti cial Neural Networks for Autonomous Navigation 1 Introduction - Washington, https://courses.cs.washington.edu/courses/cse481c/11au/papers/Pomerleau.pdf</li>
<li>Progress in neural network-based vision for autonomous robot driving - IEEE Xplore, https://ieeexplore.ieee.org/document/252290/</li>
<li>Neural Network Vision for Robot Driving, https://www.ri.cmu.edu/pub_files/pub2/pomerleau_dean_1995_1/pomerleau_dean_1995_1.pdf</li>
<li>Neural network applications ALVINN (Pomerleau, mid 1990s) ALVINN overview ALVINN: input representation, https://mil.ufl.edu/nechyba/www/__eel6825.f2003/course_materials/t11.neural_networks/slides/neural_nets_slides5.pdf</li>
<li>Software 2.0: An Emerging Era of Automatic Code Generation - The Softtek Blog, https://blog.softtek.com/software-2.0-an-emerging-era-of-automatic-code-generation</li>
<li>The transition from Software 1.0 to Software 2.0 | Insights | Liontrust Asset Management PLC, https://www.liontrust.com/insights/blogs/2025/02/the-transition-from-software-1-to-software-2</li>
<li>Software 2.0 - Andrej Karpathy – Medium, https://karpathy.medium.com/software-2-0-a64152b37c35</li>
<li>Andrej Karpathy: Software Is Changing (Again) - The Singju Post, https://singjupost.com/andrej-karpathy-software-is-changing-again/</li>
<li>Performance of high-throughput DNA quantification methods - ResearchGate, https://www.researchgate.net/publication/9036633_Performance_of_high-throughput_DNA_quantification_methods</li>
<li>How do Tesla Bot Actuators actually work - Firgelli Automations, https://www.firgelliauto.com/blogs/actuators/how-do-tesla-bot-actuators-actually-work</li>
<li>Pros and cons of end-to-end learning in autonomous driving? : r/SelfDrivingCars - Reddit, https://www.reddit.com/r/SelfDrivingCars/comments/1e3g0kh/pros_and_cons_of_endtoend_learning_in_autonomous/</li>
<li>End-to-End Training of Deep Visuomotor Policies - Journal of …, https://www.jmlr.org/papers/volume17/15-522/15-522.pdf</li>
<li>End-to-End Training of Deep Visuomotor Policies - ResearchGate, https://www.researchgate.net/publication/274572264_End-to-End_Training_of_Deep_Visuomotor_Policies</li>
<li>End-to-End Training of Deep Visuomotor Policies - UC Berkeley Robot Learning Lab, https://rll.berkeley.edu/RSS2015-BlueSky-Shakey/Levine-ShakeyWS-2015.pdf</li>
<li>On the Role of Structure in Manipulation Skill Learning - Brown Computer Science, https://cs.brown.edu/people/gdk/pubs/structure_manip_skills.pdf</li>
<li>Why might reinforcement learning be risky for designing control systems in high-stakes environments like aviation or industrial robotics? - Quora, https://www.quora.com/Why-might-reinforcement-learning-be-risky-for-designing-control-systems-in-high-stakes-environments-like-aviation-or-industrial-robotics</li>
<li>End-to-End Training of Deep Visuomotor Policies - Semantic Scholar, https://pdfs.semanticscholar.org/038d/09889d7d388a8e58b1deff0dc1783228e976.pdf</li>
<li>Learning Fast and Precise Pixel-to-Torque Control - ResearchGate, https://www.researchgate.net/publication/357735346_Learning_Fast_and_Precise_Pixel-to-Torque_Control_A_Platform_for_Reproducible_Research_of_Learning_on_Hardware/fulltext/61dd16c8323a2268f9979f2a/Learning-Fast-and-Precise-Pixel-to-Torque-Control-A-Platform-for-Reproducible-Research-of-Learning-on-Hardware.pdf</li>
<li>Learning Fast and Precise Pixel-to-Torque Control: A Platform for Reproducible Research of Learning on Hardware - ResearchGate, https://www.researchgate.net/publication/357735346_Learning_Fast_and_Precise_Pixel-to-Torque_Control_A_Platform_for_Reproducible_Research_of_Learning_on_Hardware</li>
<li>(PDF) Learning Fast and Precise Pixel-to-Torque Control - ResearchGate, https://www.researchgate.net/publication/362489641_Learning_Fast_and_Precise_Pixel-to-Torque_Control</li>
<li>End-to-End Reinforcement Learning for Robotics | by Hey Amit | Medium, https://medium.com/@heyamit10/end-to-end-reinforcement-learning-for-robotics-f606f4596c08</li>
<li>ContactNets: Learning Discontinuous Contact Dynamics with Smooth, Implicit Representations - University of Pennsylvania, https://dair.seas.upenn.edu/assets/pdf/Pfrommer2020.pdf</li>
<li>End-to-end Autonomous Driving: Challenges and Frontiers - arXiv, https://arxiv.org/html/2306.16927v3</li>
<li>ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation - arXiv, https://arxiv.org/html/2509.20841v1</li>
<li>The stark contrast between “classical” robotics and learning methods - Medium, https://medium.com/@krrish94/the-stark-contrast-between-classical-robotics-and-learning-methods-4ae66525dbe4</li>
<li>End to End Learning for Self-Driving Cars - NVIDIA, https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf</li>
<li>Bridging the Gap Between Modular and End-to-end Autonomous Driving Systems - UC Berkeley EECS, https://www2.eecs.berkeley.edu/Pubs/TechRpts/2022/EECS-2022-79.pdf</li>
<li>A Survey on Imitation Learning for Contact-Rich Tasks in Robotics - arXiv, https://arxiv.org/html/2506.13498v1</li>
<li>Safe and Efficient Robot Learning by Biasing Exploration Towards Expert Demonstrations - UC Berkeley EECS, https://www2.eecs.berkeley.edu/Pubs/TechRpts/2023/EECS-2023-152.pdf</li>
<li>2022 Index IEEE Robotics&amp;Automation Magazine Vol. 29, https://ieeexplore.ieee.org/iel7/100/9975153/09980449.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>