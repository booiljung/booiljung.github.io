<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2.4.3 하이브리드 접근법: 제어 이론의 안정성(Stability)과 딥러닝의 유연성(Flexibility) 결합</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2.4.3 하이브리드 접근법: 제어 이론의 안정성(Stability)과 딥러닝의 유연성(Flexibility) 결합</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 2. 제어 이론의 역사와 소프트웨어 2.0</a> / <a href="index.html">2.4 미분 가능한 프로그래밍 (Differentiable Programming)</a> / <span>2.4.3 하이브리드 접근법: 제어 이론의 안정성(Stability)과 딥러닝의 유연성(Flexibility) 결합</span></nav>
                </div>
            </header>
            <article>
                <h1>2.4.3 하이브리드 접근법: 제어 이론의 안정성(Stability)과 딥러닝의 유연성(Flexibility) 결합</h1>
<h2>1.  서론: 결정론적 보증과 데이터 기반 적응성의 융합</h2>
<p>현대 로보틱스와 자율 시스템 공학이 직면한 가장 근본적인 도전 과제는 ’제어 이론(Control Theory)’이 제공하는 수학적 엄밀성과 ’딥러닝(Deep Learning)’이 제공하는 무한한 표현력(Expressiveness) 사이의 간극을 어떻게 메울 것인가에 있다. 지난 수십 년간 제어 공학은 뉴턴-오일러 방정식이나 라그랑주 역학에 기반한 모델링을 통해 시스템의 안정성(Stability)을 보장해 왔다. 리아프노프(Lyapunov) 함수나 H-infinity 제어와 같은 도구들은 시스템이 외란 속에서도 발산하지 않고 평형점으로 수렴함을 수학적으로 증명할 수 있게 해준다.1 그러나 이러한 모델 기반 접근법(Model-Based Approach)은 비정형 환경에서의 마찰, 공기역학적 난류, 연성 물체와의 접촉 등 복잡한 비선형성을 완벽히 포착하지 못한다는 한계를 지닌다.3</p>
<p>반면, 심층 강화 학습(Deep Reinforcement Learning, DRL)으로 대변되는 데이터 기반 접근법(Data-Driven Approach)은 명시적인 물리 모델 없이도 고차원의 관측 데이터로부터 복잡한 제어 정책을 학습하는 데 탁월한 성과를 보여주었다. 그러나 딥러닝 모델은 본질적으로 ‘블랙박스(Black-box)’ 성격을 띠며, 학습된 신경망이 미지의 상태 공간에서 안전성을 유지할 것이라는 이론적 보증(Certificate)을 제공하기 어렵다.2 이는 자율 주행 차량이나 인간 협동 로봇과 같이 안전이 필수적인(Safety-Critical) 응용 분야에서 딥러닝의 도입을 가로막는 주된 장벽이다.</p>
<p>본 절에서 다루는 **하이브리드 접근법(Hybrid Approach)**은 이 두 패러다임의 이분법적 대립을 넘어서는 제3의 길이다. 이는 Andrej Karpathy가 주창한 “Software 1.0”(인간이 작성한 명시적 코드)과 “Software 2.0”(최적화를 통해 발견된 신경망 가중치)의 융합으로 해석될 수 있다.6 하이브리드 제어 아키텍처는 제어 이론의 구조적 틀(Structural Framework) 내에 딥러닝의 유연한 함수 근사(Function Approximation) 능력을 삽입함으로써, **“증명 가능한 안전성(Provable Safety)“과 “데이터 기반의 적응적 성능(Adaptive Performance)”**을 동시에 달성하는 것을 목표로 한다. 본 장에서는 뉴럴 리아프노프 제어, 잔차 강화 학습, 미분 가능한 최적화 계층, 그리고 미분 가능한 모델 예측 제어(MPC) 등 최신 연구 성과를 심층적으로 분석하여, 차세대 로봇 제어 시스템의 청사진을 제시한다.</p>
<h2>2.  이론적 배경: 제어 이론과 딥러닝의 상호보완성</h2>
<p>하이브리드 시스템을 설계하기 위해서는 각 방법론이 갖는 고유한 장점과 한계를 명확히 이해하고, 이를 상호 보완적으로 배치하는 전략이 필요하다.</p>
<h3>2.1  제어 이론의 강점과 ‘모델 불일치’ 문제</h3>
<p>전통적인 제어 이론은 시스템 동역학이 <span class="math math-inline">\dot{x} = f(x, u)</span>와 같이 명확한 미분 방정식으로 표현될 때 가장 강력하다. 모델 예측 제어(MPC)나 LQR(Linear-Quadratic Regulator)은 최적성(Optimality)과 안정성을 보장하지만, 실제 물리 세계와 모델 사이의 오차(Model Mismatch)가 커질수록 성능이 급격히 저하된다.3 특히 조립 공정에서의 미세한 마찰이나 드론 비행 시의 가변적인 바람 효과는 1차 물리 모델로 표현하기에는 너무 복잡하거나 계산 비용이 높다.</p>
<h3>2.2  딥러닝의 유연성과 ‘샘플 효율성’ 문제</h3>
<p>딥러닝은 범용 함수 근사기(Universal Function Approximator)로서 어떠한 비선형 함수도 학습할 수 있는 잠재력을 가진다. 그러나 순수 강화 학습(RL from Scratch)은 초기에 아무런 사전 지식 없이 무작위 탐색을 수행하므로 샘플 효율성(Sample Efficiency)이 극도로 낮고, 학습 과정에서 로봇이 위험한 행동을 할 가능성이 높다.9 또한, 학습된 정책이 훈련 데이터 분포를 벗어난 상황(Out-of-Distribution)에 직면했을 때 예측 불가능한 거동을 보일 위험이 상존한다.</p>
<h3>2.3  하이브리드 스펙트럼 (The Hybrid Spectrum)</h3>
<p>이러한 문제를 해결하기 위한 하이브리드 접근법은 결합의 강도와 방식에 따라 다음과 같이 분류할 수 있다.</p>
<table><thead><tr><th><strong>분류</strong></th><th><strong>설명</strong></th><th><strong>주요 기술 및 특징</strong></th></tr></thead><tbody>
<tr><td><strong>증강된 제어</strong> (Augmented Control)</td><td>고전 제어기를 주축으로 하고, 신경망이 외란 관측기나 피드포워드 항을 학습하여 오차를 보정함.</td><td>• 신경망 기반 마찰 보상 • 외란 관측기(DOB)와 NN 결합 • 안정성 보장이 비교적 용이함 11</td></tr>
<tr><td><strong>구조화된 정책</strong> (Structured Policy)</td><td>신경망의 출력이나 구조 자체를 제어 이론적 원리(소산성, 에너지 함수 등)에 따라 제약함.</td><td>• 뉴럴 리아프노프 제어 (Neural Lyapunov) • 안정화 신경망 정책 • 수학적 검증 가능성 확보 1</td></tr>
<tr><td><strong>잔차 학습</strong> (Residual Learning)</td><td>명목 제어기(Nominal Controller)가 기본 거동을 담당하고, RL 에이전트가 잔차 신호(Residual Signal)를 학습하여 성능을 최적화함.</td><td>• Residual RL • 복잡한 접촉 작업 및 Sim-to-Real 전이에 효과적 • 탐색 효율성 극대화 3</td></tr>
<tr><td><strong>최적화 내장형 학습</strong> (Optimization-in-the-loop)</td><td>신경망의 계층(Layer) 자체가 최적화 문제(QP, MPC)를 풀도록 설계되어, 제약 조건을 내재화함.</td><td>• OptNet, CvxPyLayers • Differentiable MPC • 종단간(End-to-End) 학습과 제약 조건 준수 동시 달성 14</td></tr>
</tbody></table>
<p>이 표는 하이브리드 제어 시스템이 단순한 결합을 넘어, 각 구성 요소가 서로의 약점을 상쇄하는 유기적인 구조를 형성하고 있음을 보여준다.</p>
<h2>3.  안정성 보증을 위한 뉴럴 리아프노프 제어 (Neural Lyapunov Control)</h2>
<p>리아프노프 안정성 이론(Lyapunov Stability Theory)은 비선형 시스템의 안정성을 해석하는 가장 보편적이고 강력한 도구이다. 하이브리드 접근법의 선두에 있는 연구들은 이 리아프노프 함수 <span class="math math-inline">V(x)</span>를 심층 신경망으로 근사하고, 이를 통해 제어 정책 <span class="math math-inline">\pi(x)</span>의 안정성을 수학적으로 보증하는 데 집중하고 있다.1</p>
<h3>3.1  뉴럴 리아프노프 함수의 구조와 학습</h3>
<p>전통적으로 리아프노프 함수를 찾는 것은 제곱의 합(Sum of Squares, SOS) 프로그래밍과 같은 다항식 기반 방법에 의존했다. 그러나 이는 시스템 차원이 높아질수록 계산 복잡도가 기하급수적으로 증가하는 문제가 있었다. Chang et al. (2019)은 **뉴럴 리아프노프 제어(Neural Lyapunov Control)**를 제안하며, 신경망의 표현력을 활용해 비선형 시스템의 안정 영역(Region of Attraction, RoA)을 최대화하는 리아프노프 함수와 제어기를 동시에 학습시키는 프레임워크를 제시했다.1</p>
<p>이 접근법의 핵심은 **학습자(Learner)**와 **반증자(Falsifier)**로 구성된 적대적 훈련 루프(Adversarial Training Loop)에 있다.</p>
<ul>
<li>
<p>학습자(Learner): 상태 <span class="math math-inline">x</span>에 대해 리아프노프 후보 함수 <span class="math math-inline">V_\phi(x)</span>와 제어 정책 <span class="math math-inline">u = \pi_\theta(x)</span>를 출력하는 신경망을 훈련한다. 손실 함수는 리아프노프 조건(양의 정부호성 <span class="math math-inline">V(x) &gt; 0</span> 및 시간 미분의 음의 정부호성 <span class="math math-inline">\dot{V}(x) &lt; 0</span>)을 위반하는 정도를 최소화하도록 설계된다.<br />
<span class="math math-display">
\mathcal{L}(\theta, \phi) = \sum_{x \in \mathcal{D}} \left( \max(0, -V_\phi(x)) + \lambda \max(0, \dot{V}_\phi(x) + \alpha V_\phi(x)) \right)
</span><br />
여기서 <span class="math math-inline">\alpha</span>는 수렴 속도를 조절하는 파라미터이다.1</p>
</li>
<li>
<p><strong>반증자(Falsifier):</strong> 현재 학습된 <span class="math math-inline">V_\phi</span>와 <span class="math math-inline">\pi_\theta</span>에 대해 리아프노프 조건을 위반하는 상태 <span class="math math-inline">x_{ce}</span> (반례, Counterexample)를 찾는다. 이는 단순한 무작위 샘플링이 아니라, 반례를 찾기 위한 최적화 문제나 SMT(Satisfiability Modulo Theories) 솔버를 사용하여 수행된다. 발견된 반례는 학습 데이터셋 <span class="math math-inline">\mathcal{D}</span>에 추가되어, 다음 훈련 단계에서 학습자가 이를 수정하도록 강제한다.1</p>
</li>
</ul>
<h3>3.2  혼합 정수 계획법(MIP)을 이용한 검증</h3>
<p>신경망으로 리아프노프 함수를 근사하더라도, “모든 상태 공간에서 안정하다“는 것을 보장하기 위해서는 엄밀한 검증이 필요하다. Dai et al. (2021)은 ReLU(Rectified Linear Unit) 활성화 함수를 사용하는 신경망이 <strong>구분적 선형(Piecewise Linear)</strong> 함수라는 점에 착안했다.5 이들은 신경망의 안정성 검증 문제를 **혼합 정수 선형 계획법(Mixed-Integer Linear Programming, MIP)**으로 변환하여 해결했다.</p>
<p>MIP 검증기는 신경망의 가중치와 구조를 제약 조건으로 포함하여, <span class="math math-inline">\dot{V}(x) \geq 0</span>이 되는 상태 <span class="math math-inline">x</span>가 존재하는지를 전역적으로 탐색한다. 만약 MIP 솔버가 “해(Solution) 없음“을 반환하면, 해당 제어기와 리아프노프 함수는 수학적으로 완벽하게 안정성이 증명된 것이다. 이 방법은 쿼드로터와 같은 복잡한 시스템에서 LQR보다 훨씬 넓은 안정 영역을 확보하면서도 이론적 보증을 제공하는 데 성공했다.17</p>
<h3>3.3  증강된 뉴럴 리아프노프 제어 (Augmented Approaches)</h3>
<p>신경망이 제어의 전권을 가지는 것에 대한 우려를 해소하기 위해, <strong>Augmented Neural Lyapunov Control</strong>과 같은 연구는 기존의 안정된 제어기(예: Sontag’s formula)를 베이스라인으로 사용하고, 신경망은 잔차 항이나 파라미터를 조정하는 방식으로 구조화된다.18 이 경우, 신경망이 학습 초기에 불안정하더라도 베이스라인 제어기가 시스템의 발산을 막아주며, 학습이 진행될수록 신경망은 베이스라인의 성능을 능가하는 최적의 거동을 찾아내게 된다. 이는 “안전한 탐색(Safe Exploration)“을 가능하게 하는 핵심 기제이다.</p>
<h2>4.  안전 임계 제어와 뉴럴 제어 장벽 함수 (Neural CBFs)</h2>
<p>안정성(Stability)이 시스템을 평형점으로 수렴시키는 것이라면, **안전성(Safety)**은 시스템이 장애물 충돌이나 속도 제한 위반과 같은 위험 집합(Unsafe Set)에 도달하지 않도록 보장하는 것이다. **제어 장벽 함수(Control Barrier Functions, CBF)**는 이러한 집합 불변성(Set Invariance)을 보장하는 현대 제어의 핵심 도구이며, 딥러닝과의 결합을 통해 **Neural CBF (NCBF)**로 진화하고 있다.2</p>
<h3>4.1  데이터 기반 안전 증명서(Safety Certificates) 학습</h3>
<p>복잡한 환경에서 안전 영역을 정의하는 함수 <span class="math math-inline">h(x)</span>를 수작업으로 설계하는 것은 매우 어렵다. 예를 들어, LiDAR 센서 데이터만을 이용해 보행 로봇의 안전 영역을 수학적으로 정의하는 것은 불가능에 가깝다. Neural CBF는 시뮬레이션 데이터나 전문가의 시연(Demonstration)으로부터 이 장벽 함수 <span class="math math-inline">h(x)</span>를 직접 학습한다.2</p>
<p>학습된 신경망 <span class="math math-inline">h_\theta(x)</span>는 상태 <span class="math math-inline">x</span>가 안전 집합 내부에 있을 때 <span class="math math-inline">h(x) \ge 0</span>, 경계에 있을 때 <span class="math math-inline">h(x) = 0</span>, 위험 영역에 있을 때 <span class="math math-inline">h(x) &lt; 0</span>의 값을 가지며, 다음의 CBF 조건을 만족하도록 훈련된다:<br />
<span class="math math-display">
\sup_{u \in U} \left[ \frac{\partial h}{\partial x} f(x, u) + \gamma(h(x)) \right] \ge 0
</span><br />
여기서 <span class="math math-inline">\gamma</span>는 class <span class="math math-inline">\mathcal{K}</span> 함수이다. 이 조건은 시스템이 안전 영역의 경계에 다가갈 때, 이를 밀어내는 제어 입력 <span class="math math-inline">u</span>가 항상 존재함을 보장한다.20</p>
<h3>4.2  안전 필터(Safety Filter)로서의 역할</h3>
<p>Neural CBF의 가장 강력한 응용은 <strong>안전 필터(Safety Filter)</strong> 형태의 구현이다. 로봇은 강화 학습 에이전트나 인간 조작자로부터 임의의 성능 중심 제어 입력 <span class="math math-inline">u_{perf}</span>를 받는다. 이때 NCBF는 실시간 최적화 문제(주로 2차 계획법, QP)를 통해 <span class="math math-inline">u_{perf}</span>와 가장 가까우면서도 안전 조건(CBF 조건)을 만족하는 실제 제어 입력 <span class="math math-inline">u_{safe}</span>를 계산한다.20<br />
<span class="math math-display">
u_{safe} = \mathop{\arg\min}_{u} \| u - u_{perf} \|^2 \quad \text{s.t.} \quad \dot{h}(x, u) \ge -\gamma(h(x))
</span><br />
이 구조는 “최소 개입(Minimally Invasive)” 원칙을 따른다. 즉, 로봇이 안전할 때는 원래의 정책을 그대로 따르지만, 위험에 임박했을 때만 필터가 개입하여 궤적을 수정한다.</p>
<h3>4.3  고장 허용 및 불확실성 대응 (Fault Tolerance &amp; Robustness)</h3>
<p>최신 연구는 이상적인 환경을 넘어, 센서 고장이나 모델 불확실성이 존재하는 상황에서도 안전을 보장하는 방향으로 발전하고 있다.</p>
<ul>
<li><strong>고장 허용 NCBF (FT-NCBF):</strong> Zhang et al. (2024)은 센서 데이터에 노이즈나 고장이 섞여 있을 때도 안전성을 유지할 수 있는 FT-NCBF를 제안했다. 이는 최적화 제약 조건에 불확실성 마진(Margin)을 포함하거나, 센서 신뢰도를 추정하여 장벽 함수의 기울기를 조절하는 방식으로 구현된다.23</li>
<li><strong>입력 포화 대응:</strong> 로봇의 액추에이터 한계(Input Limits)로 인해 회피 기동이 불가능해지는 상황을 방지하기 위해, Liu et al. (2023)은 입력 포화를 고려한 NCBF 학습법을 제시했다. 이는 안전 영역을 더욱 보수적으로 설정하여, 액추에이터가 포화되기 전에 미리 감속하거나 경로를 변경하도록 유도한다.24</li>
</ul>
<p>이러한 Neural CBF 기술은 드론의 군집 비행, 자율 주행차의 차선 변경, 그리고 인간과 협업하는 로봇 팔의 충돌 방지 시스템에서 필수적인 안전 레이어로 자리 잡고 있다.21</p>
<h2>5.  잔차 강화 학습 (Residual RL): 모델 기반과 모델 프리의 실용적 결합</h2>
<p>하이브리드 접근법 중 산업 현장에서 가장 즉각적인 효과를 입증하고 있는 방법론은 **잔차 강화 학습(Residual Reinforcement Learning)**이다. 이 접근법은 “이미 알고 있는 물리는 제어하고, 모르는 잔차 역학(Residual Dynamics)만을 학습한다“는 실용주의적 철학에 기반한다.3</p>
<h3>5.1  중첩의 원리 (Superposition Principle)</h3>
<p>Residual RL 아키텍처는 최종 제어 입력 <span class="math math-inline">u</span>를 두 가지 성분의 합으로 정의한다:<br />
<span class="math math-display">
u_{total}(s) = u_{nominal}(s) + u_{residual}(s)
</span></p>
<ul>
<li><strong>명목 제어기 (<span class="math math-inline">u_{nominal}</span>):</strong> PID, 임피던스 제어(Impedance Control), 또는 역동역학(Inverse Dynamics) 솔버와 같이 기존의 모델 기반 제어기로 구성된다. 이는 로봇의 중력 보상, 기구학적 경로 추종 등 잘 정의된 물리적 동작을 담당하여 시스템의 기본적인 안정성을 확보한다.</li>
<li><strong>잔차 정책 (<span class="math math-inline">u_{residual}</span>):</strong> 심층 강화 학습 에이전트(주로 PPO, TD3 등)가 담당한다. 이 정책은 마찰, 유격, 접촉 반력, 케이블의 장력 등 모델링하기 어렵거나 시간에 따라 변하는 비선형적인 오차를 보정하는 값을 출력한다.3</li>
</ul>
<h3>5.2  탐색 효율성과 안전성 향상</h3>
<p>순수 강화 학습(Pure RL)은 초기 학습 단계에서 무작위 행동(Random Exploration)을 수행하므로, 로봇이 스스로를 손상시키거나 학습 시간이 매우 오래 걸리는 단점이 있다. 반면, Residual RL은 명목 제어기가 로봇을 목표 궤적 근처(Near-optimal region)에 머물게 하므로, RL 에이전트는 좁은 탐색 공간 내에서 미세 조정(Fine-tuning)에 집중할 수 있다.3 이는 샘플 효율성을 획기적으로 높이고, 초기 학습 단계에서도 일정 수준 이상의 성능과 안전성을 보장한다.</p>
<h3>5.3  산업적 응용: 조립 및 접촉 작업</h3>
<p>Johannink et al. (2019)과 Siemens의 연구진은 산업용 로봇의 블록 조립(Peg-in-hole) 작업에서 Residual RL의 강력함을 입증했다.3 위치 제어 기반의 명목 제어기만으로는 부품 간의 위치 오차나 마찰로 인해 조립(Insertion) 단계에서 빈번한 실패(Jamming)가 발생했다. 그러나 Residual RL을 적용하자, 에이전트는 접촉 힘 피드백을 기반으로 미세한 힘 제어 보정값을 학습하여, 사람의 개입 없이도 복잡한 조립 작업을 성공적으로 수행했다.</p>
<p>특히 주목할 점은 <strong>Sim-to-Real 전이</strong>에서의 강건성이다. 시뮬레이션과 실제 환경의 차이(Reality Gap)가 크더라도, 명목 제어기가 물리 법칙에 기반해 시스템을 지지하고 있으므로, 잔차 정책만이 실제 환경의 특성에 맞춰 적응(Adaptation)하면 된다. 실험 결과, Residual RL은 순수 RL에 비해 훨씬 적은 실세계 데이터만으로도 성공적으로 전이 학습이 가능함을 보여주었다.3</p>
<h3>5.4  내부 모델 원리와의 결합</h3>
<p>최근 연구에서는 내부 모델 원리(Internal Model Principle, IMP) 제어기와 RL을 결합하는 시도도 이루어지고 있다.12 IMP 제어기는 주기적인 외란 제거에 탁월하지만 과도 응답(Transient Response)이 느리다는 단점이 있다. 하이브리드 구조에서는 RL이 과도 응답 구간에서의 제어를 담당하여 오버슈트를 줄이고 수렴 속도를 높이며, 정상 상태(Steady State)에서는 IMP 제어기가 정밀도를 보장하는 방식으로 역할 분담이 이루어진다. 이는 고전 제어의 정밀함과 딥러닝의 동적 대응 능력을 최적으로 배합한 사례이다.</p>
<h2>6.  미분 가능한 최적화 계층 (Differentiable Optimization Layers)</h2>
<p>딥러닝 모델 내에 제어 이론적 구조를 ’내재화(Embed)’하기 위한 가장 혁신적인 기술은 **미분 가능한 최적화(Differentiable Optimization)**이다. 이는 최적화 문제(Quadratic Programming 등)를 신경망의 하나의 계층(Layer)으로 간주하고, 최적해(Optimal Solution)에 대한 파라미터의 미분값(Gradient)을 계산하여 종단간(End-to-End) 학습을 가능하게 하는 기술이다.14</p>
<h3>6.1  OptNet과 암시적 미분 (Implicit Differentiation)</h3>
<p>Amos와 Kolter (2017)가 제안한 <strong>OptNet</strong>은 2차 계획법(QP)을 신경망의 계층으로 통합했다.14 이 계층은 입력 <span class="math math-inline">z</span>에 대해 선형 부등식 및 등식 제약 조건(<span class="math math-inline">Az = b, Gz \leq h</span>)을 만족하면서 비용 함수 <span class="math math-inline">\frac{1}{2}z^T Q z + p^T z</span>를 최소화하는 최적해 <span class="math math-inline">z^*</span>를 출력한다.</p>
<p>핵심 기술은 <strong>KKT(Karush-Kuhn-Tucker) 조건</strong>에 대한 암시적 함수 정리(Implicit Function Theorem)의 적용이다. 이를 통해 최적화 솔버 내부의 반복 계산 과정을 일일이 추적(Unrolling)하지 않고도, 최적해 <span class="math math-inline">z^*</span>에서 <span class="math math-inline">Q, p, A, G, h</span> 등 문제의 파라미터에 대한 손실 함수의 기울기(Gradient)를 해석적으로 계산할 수 있다.14 이는 역전파(Backpropagation) 과정에서 최적화 계층을 통과하여 그 이전 계층의 가중치들을 업데이트할 수 있음을 의미한다.</p>
<h3>6.2  CvxPyLayers: 볼록 최적화의 딥러닝 통합</h3>
<p>이후 등장한 <strong>CvxPyLayers</strong> 라이브러리는 볼록 최적화(Convex Optimization) 문제 전반을 PyTorch나 TensorFlow 내에서 쉽게 미분 가능한 계층으로 변환할 수 있게 만들었다.28 제어 엔지니어는 이제 신경망 아키텍처 내에 물리적 제약 조건(최대 토크, 충돌 회피 거리, 에너지 보존 등)을 명시적인 최적화 문제로 서술하여 삽입할 수 있다. 이는 신경망이 물리적으로 불가능하거나 위험한 출력을 내놓는 것을 구조적으로 차단하는 강력한 ‘하드 제약(Hard Constraint)’ 역할을 수행한다.</p>
<h3>6.3  제어 이론적 유도 편향 (Inductive Bias)</h3>
<p>이 기술은 딥러닝 모델에 강력한 제어 이론적 유도 편향을 주입한다. 예를 들어, 로봇의 역기구학(Inverse Kinematics)이나 동역학 방정식을 미분 가능한 계층으로 구현하면, 딥러닝 모델은 단순히 입출력 매핑을 외우는 것이 아니라, <strong>물리 방정식을 만족하는 해 공간(Solution Space)</strong> 내에서 최적의 해를 찾도록 학습된다. 이는 데이터 효율성을 높일 뿐만 아니라, 학습되지 않은 데이터에 대해서도 물리적으로 타당한 거동을 보장하는 일반화 성능(Generalization)을 제공한다.30</p>
<h2>7.  미분 가능한 모델 예측 제어 (Differentiable MPC)</h2>
<p>하이브리드 접근법의 정점이라 할 수 있는 **미분 가능한 모델 예측 제어(Differentiable MPC)**는 MPC 자체를 미분 가능한 정책 네트워크(Policy Network)로 활용하는 기술이다. MPC는 미래의 일정 구간(Horizon)을 예측하여 최적의 제어 입력을 계산하는 제어 기법으로, 이를 딥러닝 파이프라인에 통합함으로써 “계획(Planning)하는 신경망“을 구현한다.15</p>
<h3>7.1  종단간(End-to-End) MPC 학습과 역강화 학습</h3>
<p>Amos et al. (2018)은 MPC를 신경망의 마지막 계층으로 배치하여, 관측된 이미지나 센서 데이터로부터 MPC의 내부 파라미터(비용 함수 <span class="math math-inline">Q, R</span>, 동역학 모델 <span class="math math-inline">A, B</span>)를 종단간으로 학습하는 아키텍처를 제안했다.15</p>
<ul>
<li><strong>전통적 방식의 한계:</strong> 기존에는 전문가가 <span class="math math-inline">Q, R</span> 행렬을 수동으로 튜닝하여 원하는 거동(예: 부드러운 주행, 공격적인 레이싱)을 만들어내야 했다. 또한, 시스템 식별(System Identification) 과정이 별도로 필요했다.</li>
<li><strong>Differentiable MPC의 혁신:</strong> 로봇의 최종 작업 목표(예: 궤적 추종 오차 최소화)를 손실 함수로 정의하면, 역전파를 통해 MPC가 내부적으로 사용하는 비용 함수와 동역학 모델이 자동으로 최적화된다. 이는 일종의 **구조화된 역강화 학습(Inverse Reinforcement Learning)**으로 볼 수 있으며, 전문가의 시연 데이터를 가장 잘 설명하는 비용 함수를 데이터로부터 스스로 학습한다.15</li>
</ul>
<h3>7.2  튜브 기반(Tube-based) MPC와 강건성</h3>
<p>최근 연구는 미분 가능한 MPC에 강건 제어 이론을 결합하여 안전성을 더욱 강화하고 있다. **미분 가능한 튜브 기반 MPC (Differentiable Tube-based MPC)**는 시스템의 불확실성을 다루기 위해 두 개의 계층을 사용한다.36</p>
<ul>
<li><strong>명목 MPC (Nominal MPC):</strong> 불확실성이 없는 이상적인 모델을 기반으로 최적의 기준 궤적을 계획한다.</li>
<li><strong>보조 제어기 (Ancillary Controller):</strong> 실제 시스템 상태가 명목 궤적 주변의 ‘튜브(Tube)’ 영역을 벗어나지 않도록 외란을 억제하는 피드백 제어를 수행한다.</li>
</ul>
<p>딥러닝은 여기서 튜브의 크기(안전 마진)나 보조 제어기의 이득(Gain)을 환경의 불확실성 수준에 맞춰 온라인으로 적응시키는 역할을 한다. 이를 통해 변화무쌍한 환경에서도 로봇은 수학적으로 계산된 튜브 영역 내에서 안전하게 작동함을 보장받으며, 동시에 최적화 기반의 고성능 제어를 수행할 수 있다.</p>
<h3>7.3  고성능 드론 레이싱 응용</h3>
<p>Romero et al. (2024)은 쿼드로터 레이싱과 같이 극한의 민첩성이 요구되는 작업에서 Differentiable MPC의 성능을 입증했다.37 이들은 강화 학습(Actor-Critic) 구조 내에 MPC를 Actor로 사용했다. 신경망은 현재의 로봇 상태와 환경 정보를 바탕으로 MPC의 비용 함수(Cost Function)를 실시간으로 변형(Morphing)시킨다. 즉, 장애물이 가까우면 회피 비용을 높이고, 직선 구간에서는 속도 가중치를 높이는 식의 전략을 스스로 학습한다. 결과적으로 이 시스템은 인간 챔피언을 능가하는 초인적인 비행 성능을 달성하면서도, 순수 RL 기반 방식보다 훨씬 높은 비행 안정성을 보여주었다.</p>
<h2>8.  비교 분석 및 사례 연구 (Case Studies)</h2>
<p>다양한 하이브리드 접근법들은 적용 대상과 목표에 따라 각기 다른 장단점을 가진다. 아래 표는 주요 방법론들의 특성을 비교 분석한 것이다.</p>
<table><thead><tr><th><strong>특성</strong></th><th><strong>뉴럴 리아프노프 제어 (Neural Lyapunov)</strong></th><th><strong>뉴럴 제어 장벽 함수 (Neural CBF)</strong></th><th><strong>잔차 강화 학습 (Residual RL)</strong></th><th><strong>미분 가능한 MPC (Diff-MPC)</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 목표</strong></td><td>전역적 안정성(Stability) 보장 및 RoA 최대화</td><td>안전성(Safety) 및 위험 집합 회피</td><td>성능 최적화 및 접촉/마찰 적응</td><td>계획(Planning) 및 최적 제어 학습</td></tr>
<tr><td><strong>이론적 보증</strong></td><td><strong>최상</strong> (MIP/SMT 검증 시)</td><td><strong>상</strong> (Forward Invariance 보장)</td><td><strong>중/하</strong> (명목 제어기에 의존)</td><td><strong>상</strong> (제약 조건 준수, 튜브 기반 시 강건성)</td></tr>
<tr><td><strong>유연성</strong></td><td>중 (구조적 제약 존재)</td><td>상 (필터 형태로 어떤 정책과도 결합 가능)</td><td><strong>최상</strong> (RL의 자유도 활용)</td><td>중 (볼록 최적화 틀 내 제한)</td></tr>
<tr><td><strong>계산 비용</strong></td><td>낮음 (학습 후 추론은 단순 신경망)</td><td>낮음 (QP는 매우 빠름)</td><td>낮음 (일반 RL과 유사)</td><td><strong>높음</strong> (매 단계 최적화 문제 해결 필요)</td></tr>
<tr><td><strong>주요 응용</strong></td><td>비선형 시스템 안정화, 드론 자세 제어</td><td>자율 주행, 충돌 회피, 군집 로봇</td><td>로봇 팔 조립, 연마, 고속 조작</td><td>궤적 계획, 고속 레이싱, 보행 로봇</td></tr>
</tbody></table>
<h3>8.1 주요 실험 사례</h3>
<ol>
<li>쿼드로터(Quadrotors)와 Neural-Fly:</li>
</ol>
<p>Caltech의 Neural-Fly 연구는 강풍 환경에서 드론의 제어를 위해 하이브리드 방식을 채택했다. 기본 제어기는 비선형 동역학을 처리하고, 신경망은 바람에 의한 공기역학적 외란을 실시간으로 학습하여 적응 제어(Adaptive Control) 성능을 극대화했다. 이는 풍동 실험에서 표준 제어기보다 수 배 낮은 추적 오차를 기록했다.1</p>
<ol start="2">
<li>보행 로봇(Legged Robots)과 험지 주파:</li>
</ol>
<p>MIT와 ETH Zurich의 연구팀은 보행 로봇의 험지 이동을 위해 Neural CBF와 미분 가능한 최적화를 결합했다. 로봇은 시각 정보를 통해 지형의 마찰 계수를 추정하고, 이를 기반으로 미끄러짐을 방지하는 CBF 제약 조건을 실시간으로 업데이트하여 얼음판 위에서도 넘어지지 않는 강건한 보행을 선보였다.21</p>
<ol start="3">
<li>산업용 매니퓰레이터의 정밀 조립:</li>
</ol>
<p>Residual RL은 수백 분의 일 밀리미터 공차가 요구되는 기어 조립 작업에서 인간 숙련공 수준의 힘 조절 능력을 보여주었다. 특히 위치 제어만 가능한 저가형 로봇 팔에 힘/토크 센서를 부착하고 Residual RL을 적용함으로써, 고가형 정밀 로봇을 대체할 수 있는 가능성을 열었다.3</p>
<h2>9.  결론 및 미래 전망: 인증된 자율성(Certified Autonomy)을 향하여</h2>
<p>하이브리드 접근법은 로봇 제어 분야의 “Software 2.0” 혁명을 이끄는 핵심 동력이다. 제어 이론은 시스템에 <strong>“해야 할 것(안정성, 최적성)“과 “하지 말아야 할 것(충돌, 위반)“의 명확한 경계</strong>를 설정해 주며, 딥러닝은 그 경계 안에서 <strong>데이터를 통해 최적의 행동을 스스로 찾아내는 지능</strong>을 부여한다.6</p>
<p>미래의 하이브리드 제어 시스템은 다음과 같은 방향으로 진화할 것이다:</p>
<ol>
<li><strong>검증 도구의 확장성(Scalability of Verification):</strong> 현재의 MIP/SMT 기반 검증은 저차원 시스템에 국한되어 있다. 향후에는 고차원 휴머노이드 로봇이나 자율 주행 시스템 전체를 검증할 수 있는 확장 가능한 신경망 검증 기술이 하이브리드 제어와 결합될 것이다.5</li>
<li><strong>파운데이션 모델(Foundation Models)과의 수직적 통합:</strong> 거대 언어 모델(LLM)이나 비전-언어 모델(VLM)이 상위 레벨의 추상적인 계획(High-level Planning)을 수립하면, 하이브리드 제어기가 이를 물리적으로 실행 가능하고 안전한 하위 레벨 제어 명령(Low-level Control)으로 변환하는 계층적 아키텍처가 표준이 될 것이다. 하이브리드 제어기는 LLM의 환각(Hallucination)이 물리적 사고로 이어지지 않도록 막는 <strong>‘물리적 방화벽(Physical Firewall)’</strong> 역할을 수행하게 된다.30</li>
<li><strong>하드웨어 가속 최적화:</strong> 미분 가능한 MPC나 QP를 kHz 단위의 초고속 주기로 실행하기 위해, 최적화 솔버를 FPGA나 GPU에 내장(Embedded Optimization)하는 기술이 보편화될 것이다.14</li>
</ol>
<p>결론적으로, 제어 이론의 안정성과 딥러닝의 유연성 결합은 선택이 아닌 필수이다. 이는 로봇이 통제된 실험실을 벗어나 예측 불가능한 현실 세계(The Wild)에서 인간과 공존하며 안전하고 유용하게 작동하기 위한 유일하고도 가장 강력한 해법이다. 이러한 하이브리드 패러다임은 향후 <strong>인증된 자율성(Certified Autonomy)</strong> 시대를 여는 열쇠가 될 것이다.</p>
<h2>10. 참고 자료</h2>
<ol>
<li>Neural Lyapunov Control - Ya-Chien Chang, https://yachienchang.github.io/NeurIPS2019/paper/Neural_Lyapunov_Control.pdf</li>
<li>Safe Control with Learned Certificates: A Survey of Neural Lyapunov, Barrier, and Contraction Methods for Robotics and Control - IEEE Xplore, https://ieeexplore.ieee.org/ielaam/8860/10144918/10015199-aam.pdf</li>
<li>Residual Reinforcement Learning for Robot Control - arXiv, https://arxiv.org/pdf/1812.03201</li>
<li>[1812.03201] Residual Reinforcement Learning for Robot Control - arXiv, https://arxiv.org/abs/1812.03201</li>
<li>MIT Open Access Articles Lyapunov-stable neural-network control, https://dspace.mit.edu/bitstream/handle/1721.1/143987/Dai21.pdf?sequence=2&amp;isAllowed=y</li>
<li>Building the Software 2 0 Stack (Andrej Karpathy) - YouTube, https://www.youtube.com/watch?v=y57wwucbXR8</li>
<li>The data lifecycle as a value add - aiEthix, https://aiethix.com/blog/the-new-data-lifecycle</li>
<li>Learning with Structured Priors for Robust Robot Manipulation, https://www.ri.cmu.edu/publications/learning-with-structured-priors-for-robust-robot-manipulation/</li>
<li>(PDF) Residual Reinforcement Learning for Robot Control - ResearchGate, https://www.researchgate.net/publication/329567994_Residual_Reinforcement_Learning_for_Robot_Control</li>
<li>Visual Reinforcement Learning with Residual Action, https://ojs.aaai.org/index.php/AAAI/article/view/34097/36252</li>
<li>A Hybrid Reinforcement Learning Framework Combining TD3 and PID Control for Robust Trajectory Tracking of a 5-DOF Robotic Arm - MDPI, https://www.mdpi.com/2673-4052/6/4/56</li>
<li>An Optimized Position Control via Reinforcement-Learning-Based Hybrid Structure Strategy, https://www.mdpi.com/2076-0825/14/4/199</li>
<li>Structured Priors for Policy Optimisation, https://www.mlmi.eng.cam.ac.uk/files/structured-priors-policy_wang.pdf</li>
<li>[1703.00443] OptNet: Differentiable Optimization as a Layer in Neural Networks - arXiv, https://arxiv.org/abs/1703.00443</li>
<li>Differentiable MPC for End-to-end Planning and Control - NIPS papers - NeurIPS, https://papers.nips.cc/paper/8050-differentiable-mpc-for-end-to-end-planning-and-control</li>
<li>Neural Lyapunov Control - NIPS papers, https://papers.nips.cc/paper/8587-neural-lyapunov-control</li>
<li>[2109.14152] Lyapunov-stable neural-network control - arXiv, https://arxiv.org/abs/2109.14152</li>
<li>grande-dev/Augmented-Neural-Lyapunov-Control - GitHub, https://github.com/grande-dev/Augmented-Neural-Lyapunov-Control</li>
<li>Augmented Neural Lyapunov Control - NERC Open Research Archive, https://nora.nerc.ac.uk/id/eprint/535467/1/Augmented_Neural_Lyapunov_Control.pdf</li>
<li>Neural Control Barrier Functions for Safe Navigation - arXiv, https://arxiv.org/html/2407.19907v1</li>
<li>Safety for learned control systems: data-driven proofs via neural certificates - ACM SIGBED, https://sigbed.org/2023/05/11/neural-certificates/</li>
<li>Safe Control with Learned Certificates: A Survey of Neural Lyapunov, Barrier, and Contraction methods - ResearchGate, https://www.researchgate.net/publication/358846072_Safe_Control_with_Learned_Certificates_A_Survey_of_Neural_Lyapunov_Barrier_and_Contraction_methods</li>
<li>Fault Tolerant Neural Control Barrier Functions for Robotic Systems under Sensor Faults and Attacks | IEEE Conference Publication | IEEE Xplore, https://ieeexplore.ieee.org/document/10610491/</li>
<li>Safe control under input limits with neural control barrier functions - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v205/liu23e/liu23e.pdf</li>
<li>[2106.08050] Residual Reinforcement Learning from Demonstrations - arXiv, https://arxiv.org/abs/2106.08050</li>
<li>Chapter 5: Differentiable optimization - Deep Implicit Layers, http://implicit-layers-tutorial.org/differentiable_optimization/</li>
<li>Differentiable Convex Optimization Layers - LocusLab blog, https://locuslab.github.io/2019-10-28-cvxpylayers/</li>
<li>cvxpy/cvxpylayers: Differentiable convex optimization layers - GitHub, https://github.com/cvxpy/cvxpylayers</li>
<li>[1910.12430] Differentiable Convex Optimization Layers - arXiv, https://arxiv.org/abs/1910.12430</li>
<li>Differentiable Robot Rendering - arXiv, https://arxiv.org/html/2410.13851v1</li>
<li>Unbundling and Mitigating Gradient Variance in Differentiable Bundle Adjustment Layers - arXiv, https://arxiv.org/html/2406.07785v1</li>
<li>Actor-Critic Model Predictive Control: Differentiable Optimization meets Reinforcement Learning - arXiv, https://arxiv.org/html/2306.09852v6</li>
<li>Differentiable MPC for End-to-end Planning and Control - SciSpace, https://scispace.com/pdf/differentiable-mpc-for-end-to-end-planning-and-control-2iojj2gcm4.pdf</li>
<li>[Quick Review] Differentiable MPC for End-to-end Planning and Control - Liner, https://liner.com/review/differentiable-mpc-for-endtoend-planning-and-control</li>
<li>[PDF] Differentiable MPC for End-to-end Planning and Control - Semantic Scholar, https://www.semanticscholar.org/paper/Differentiable-MPC-for-End-to-end-Planning-and-Amos-Rodriguez/68df76350dffba2e5f5f965df57c3747c66bb4d0</li>
<li>Differentiable Robust Model Predictive Control - Robotics, https://roboticsproceedings.org/rss20/p003.pdf</li>
<li>Actor-Critic Model Predictive Control: Differentiable Optimization meets Reinforcement Learning AC-MPC (Ours) AC-MLP - Robotics and Perception Group, https://rpg.ifi.uzh.ch/docs/Arxiv24_ACMPC_Romero.pdf</li>
<li>Actor-Critic Model Predictive Control: Differentiable Optimization meets Reinforcement Learning for Agile Flight - Robotics and Perception Group, https://rpg.ifi.uzh.ch/docs/TRO25_ACMPC_Romero.pdf</li>
<li>Full article: Model predictive control of legged and humanoid robots: models and algorithms, https://www.tandfonline.com/doi/full/10.1080/01691864.2023.2168134</li>
<li>A Survey of Robot Intelligence with Large Language Models - MDPI, https://www.mdpi.com/2076-3417/14/19/8868</li>
<li>Differentiable Predictive Control for Robotics: A Data-Driven Predictive Safety Filter Approach | Request PDF - ResearchGate, https://www.researchgate.net/publication/384266801_Differentiable_Predictive_Control_for_Robotics_A_Data-Driven_Predictive_Safety_Filter_Approach</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>