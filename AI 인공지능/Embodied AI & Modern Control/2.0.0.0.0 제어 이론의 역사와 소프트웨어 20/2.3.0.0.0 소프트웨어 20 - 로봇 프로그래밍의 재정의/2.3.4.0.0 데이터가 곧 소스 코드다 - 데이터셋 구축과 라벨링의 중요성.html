<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2.3.4 데이터가 곧 소스 코드다: 데이터셋 구축과 라벨링의 중요성</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2.3.4 데이터가 곧 소스 코드다: 데이터셋 구축과 라벨링의 중요성</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 2. 제어 이론의 역사와 소프트웨어 2.0</a> / <a href="index.html">2.3 소프트웨어 2.0: 로봇 프로그래밍의 재정의</a> / <span>2.3.4 데이터가 곧 소스 코드다: 데이터셋 구축과 라벨링의 중요성</span></nav>
                </div>
            </header>
            <article>
                <h1>2.3.4 데이터가 곧 소스 코드다: 데이터셋 구축과 라벨링의 중요성</h1>
<p>현대 로봇 공학의 지형도는 지난 10년간 근본적인 지각 변동을 겪었다. 과거 로봇 제어 기술이 정교한 수식과 명시적인 알고리즘 설계에 의존하던 ’Software 1.0’의 시대였다면, 현재는 거대한 데이터의 바다 위에서 신경망이라는 범용 연산기를 통해 기능을 발현시키는 ‘Software 2.0’, 나아가 자연어와 멀티모달 데이터가 융합된 ’Software 3.0’의 시대로 진입하고 있다.1 이 새로운 패러다임에서 로봇의 지능을 결정짓는 핵심 변수는 더 이상 제어 루프의 미분 방정식이 아니다. 안드레이 카파시(Andrej Karpathy)가 설파했듯, **데이터셋이 곧 프로그래머가 작성하는 소스 코드(Source Code)**이며, 신경망의 학습 과정은 이 코드를 실행 가능한 바이너리(Binary)로 변환하는 컴파일(Compilation) 과정과 동등하다.1 본 섹션에서는 이러한 관점을 바탕으로 로봇 학습을 위한 데이터셋 구축, 라벨링, 큐레이션, 그리고 표준화 기술의 최신 동향과 기술적 난제들을 심층적으로 분석한다.</p>
<h2>1.  패러다임의 전환: Software 1.0에서 3.0까지</h2>
<p>로봇 공학에서 소프트웨어의 진화는 문제 해결을 위한 추상화 레벨의 변화로 이해할 수 있다.</p>
<ol>
<li><strong>Software 1.0 (명시적 프로그래밍):</strong> 전통적인 로봇 제어 방식이다. 엔지니어는 C++이나 Python과 같은 언어를 사용하여 로봇의 행동을 한 줄 한 줄 코딩한다. 역운동학(Inverse Kinematics), 경로 계획(Path Planning), PID 제어기 등은 모두 인간이 설계한 논리적 구조물이다. 이 방식은 동작 원리가 투명하고 검증 가능하지만, 복잡하고 비정형적인 현실 세계의 모든 예외 상황을 미리 예측하여 코딩하는 것은 불가능에 가깝다. 즉, ’프로그램 공간(Program Space)’에서 인간이 탐색할 수 있는 영역은 극히 제한적이다.1</li>
<li><strong>Software 2.0 (데이터 기반 최적화):</strong> 신경망의 도입으로 시작된 혁명이다. 이제 엔지니어는 로봇의 행동을 직접 코딩하지 않는다. 대신, 입력(센서 데이터)과 출력(행동)의 쌍으로 구성된 데이터셋을 통해 ’바람직한 동작’을 정의한다. 그리고 경사 하강법(Gradient Descent)과 같은 최적화 알고리즘을 사용하여 프로그램 공간을 탐색하고, 최적의 가중치(Weights)를 찾아낸다. 이 과정에서 데이터셋은 소프트웨어의 스펙이자 소스 코드가 되며, 수백만 개의 가중치는 인간이 읽을 수 없는 기계어 코드가 된다.1</li>
<li><strong>Software 3.0 (자연어 및 멀티모달 인터페이스):</strong> 최근 LLM(Large Language Models)의 등장으로 데이터는 단순한 입출력 예시를 넘어선다. 자연어가 새로운 프로그래밍 언어가 되며, 로봇은 방대한 인터넷 텍스트와 비디오 데이터를 통해 상식과 추론 능력을 획득한다. 이제 데이터 수집은 단순한 기록이 아니라, 로봇에게 ’생각하는 법’을 가르치는 과정이 되었다.2</li>
</ol>
<p>이러한 관점에서 볼 때, 로봇 엔지니어링의 본질은 알고리즘 설계에서 <strong>데이터 엔지니어링</strong>으로 이동한다. 좋은 코드를 짜기 위해 리팩토링과 디버깅을 하듯, 좋은 로봇을 만들기 위해서는 데이터셋을 정제(Curation)하고 라벨링 오류를 수정(Debugging)해야 한다.3</p>
<h2>2.  데이터 수집 방법론: 소스 코드의 작성 (Writing the Code)</h2>
<p>데이터가 소스 코드라면, 데이터를 수집하는 행위는 곧 프로그래밍이다. 로봇 학습을 위한 데이터 수집 방식은 크게 인간의 개입 정도와 자동화 수준에 따라 분류할 수 있으며, 각각의 방식은 생성되는 ’코드’의 품질과 특성을 결정한다.</p>
<h3>2.1 (1) 원격 조작(Teleoperation)과 키네스테틱 티칭(Kinesthetic Teaching)</h3>
<p>가장 직관적인 방법은 인간이 직접 로봇의 몸을 빌려 시범을 보이는 것이다. 이는 모방 학습(Imitation Learning)의 근간이 되며, 인간의 암묵적 지식(Tacit Knowledge)을 로봇의 행동 공간으로 전이시키는 과정이다.</p>
<ul>
<li>키네스테틱 티칭 (Kinesthetic Teaching):</li>
</ul>
<p>사용자가 로봇의 관절이나 엔드 이펙터를 직접 잡고 움직여 궤적을 기록하는 방식이다. 이 방식은 로봇의 기구학적 한계를 물리적으로 체감하며 데이터를 생성하므로, 로봇이 실행 불가능한 동작이 기록될 위험이 없다. 연구 결과에 따르면, 키네스테틱 티칭은 가장 직관적이며 다운스트림 학습 성능이 우수한 ‘깨끗한(Clean)’ 데이터를 생성하는 것으로 평가된다.4 그러나 사용자가 로봇과 물리적으로 근접해야 하므로 안전상의 제약이 있고, 반복적인 작업 시 신체적 피로도가 높아 대규모 데이터셋 구축(Scaling)에는 한계가 명확하다.</p>
<ul>
<li>원격 조작 (Teleoperation):</li>
</ul>
<p>물리적 제약을 극복하기 위해 원격 제어 장치를 활용한다. VR 컨트롤러, 3D 마우스, 혹은 ALOHA 시스템과 같은 마스터-슬레이브(Master-Slave) 인터페이스가 사용된다.5</p>
<ul>
<li><strong>VR 인터페이스:</strong> HTC Vive와 같은 VR 기기는 6자유도(6-DoF) 추적을 통해 사용자의 손 움직임을 로봇에 매핑한다. 공간적 직관성이 뛰어나지만, 사람의 손 움직임을 로봇의 기구학에 맞추는 리타게팅(Retargeting) 과정에서 오차나 부자연스러운 동작이 발생할 수 있다.5</li>
<li><strong>저비용 마스터 암:</strong> 최근에는 저가형 로봇 팔을 입력 장치(Leader)로 사용하고, 동일하거나 유사한 구조의 로봇 팔(Follower)을 제어하는 방식이 유행하고 있다. 이는 촉각적 피드백(Haptic Feedback)을 어느 정도 제공하며, VR보다 정교한 조작(Fine Manipulation) 데이터를 수집하는 데 유리하다. 그러나 통신 지연(Latency)이 발생할 경우 데이터의 인과성이 깨질 수 있어, 시스템 엔지니어링 차원의 최적화가 필수적이다.6</li>
</ul>
<h3>2.2 (2) 시뮬레이션 기반 데이터 생성 (Simulation &amp; Sim-to-Real)</h3>
<p>현실 세계에서의 데이터 수집은 비용이 많이 들고 속도가 느리다. 이에 대한 대안으로 시뮬레이션 환경에서 데이터를 생성하고 이를 현실로 전이(Sim-to-Real)하는 기술이 비약적으로 발전했다.</p>
<ul>
<li>절차적 생성(Procedural Generation)과 도메인 무작위화(Domain Randomization):</li>
</ul>
<p>Software 1.0에서 단위 테스트(Unit Test)를 자동 생성하듯, 시뮬레이션에서는 환경 변수를 무작위로 조작하여 무한에 가까운 데이터를 생성할 수 있다. RoboTwin 2.0과 같은 최신 프레임워크는 생성형 AI를 활용하여 물체의 종류, 배치, 조명, 텍스처, 심지어 로봇의 물리적 파라미터까지 절차적으로 생성한다.7 이는 단순히 데이터 양을 늘리는 것을 넘어, 데이터의 **다양성(Diversity)**을 극대화한다. 연구에 따르면, 카메라의 위치(Viewpoint)와 객체의 공간적 배치 다양성은 로봇 정책의 일반화 성능을 결정짓는 가장 중요한 요소 중 하나이다.9</p>
<ul>
<li>디지털 트윈(Digital Twin):</li>
</ul>
<p>‘Autonomy 2.0’ 패러다임에서는 현실 세계를 시뮬레이션에 정밀하게 복제한 디지털 트윈을 활용한다. 이는 실제 환경에서 발생할 수 있는 드문 케이스(Edge Case)를 시뮬레이션에서 집중적으로 학습시킨 후 배포하는 방식으로, 엔지니어링 팀의 규모보다 컴퓨팅 자원의 투입량에 비례하여 성능을 향상시킬 수 있는 구조를 만든다.10</p>
<h3>2.3 (3) 자율 탐색 및 자기 지도 학습 (Autonomous Self-Supervision)</h3>
<p>인간의 개입 없이 로봇이 스스로 데이터를 수집하는 방식은 ’데이터 공장’의 자동화를 의미한다.</p>
<ul>
<li>놀이(Play)와 탐색:</li>
</ul>
<p>유아의 학습 과정에서 영감을 받아, 로봇이 환경과 자유롭게 상호작용하며 데이터를 수집한다. 초기에는 무작위 움직임에 가깝지만, 내적 호기심(Intrinsic Motivation) 보상이나 엔트로피 최대화 전략을 통해 점차 유의미한 조작 기술을 스스로 발견한다.11</p>
<ul>
<li>자기 라벨링(Self-Labeling):</li>
</ul>
<p>가장 큰 장점은 라벨링 비용이 ’0’에 수렴한다는 것이다. 로봇은 자신의 행동 결과를 센서(카메라, 촉각 등)로 확인하고, 성공/실패 여부를 스스로 판별한다. 예를 들어, SwingBot은 물체를 흔들어보는 행동을 통해 물체의 질량이나 마찰 같은 물리적 특성을 스스로 학습하고 라벨링한다.13 이는 지도 학습(Supervised Learning)의 한계를 넘어, 미지의 환경에 적응하는 능력을 부여한다.</p>
<h3>2.4 (4) 비디오 기반 학습 (Learning from Action-Free Videos)</h3>
<p>인터넷상에는 수십억 시간 분량의 인간 행동 비디오가 존재한다. 이 ’비디오 데이터’를 로봇 학습의 소스 코드로 활용하려는 시도가 활발하다.</p>
<ul>
<li>행동 레이블이 없는 학습 (Action-Free Learning):</li>
</ul>
<p>유튜브 비디오에는 로봇의 관절 각도(Action) 정보가 없다. 따라서 이를 학습에 활용하기 위해서는 비디오에서 인간의 의도나 손의 움직임을 추출하고, 이를 로봇의 행동으로 번역(Translation)하는 과정이 필요하다. MotoVLA와 같은 연구는 비디오에서 3D 동역학을 예측하거나, 역운동학을 통해 가상의 행동 레이블(Pseudo-label)을 생성하여 학습에 활용한다.14 또한, UPESV 프레임워크는 비디오 라벨링 모델을 학습시켜 보상 함수나 전문가의 행동 데이터 없이도 정책을 학습할 수 있음을 보여주었다.15 이는 주석이 없는 레거시 코드를 분석하여 로직을 추출하는 리버스 엔지니어링과 유사하다.</p>
<h2>3.  데이터 표현과 표준화: 구문(Syntax)과 컴파일(Compilation)</h2>
<p>수집된 원시 데이터(Raw Data)가 유효한 소스 코드가 되기 위해서는 통일된 문법(Syntax)과 포맷이 필요하다. 특히 로봇 데이터는 이미지, 라이다, 관절 상태, 제어 신호 등 이질적인 모달리티가 혼재되어 있어 표준화가 매우 까다롭다.</p>
<h3>3.1 (1) RLDS (Reinforcement Learning Datasets) 생태계</h3>
<p>과거에는 각 연구실마다 독자적인 데이터 포맷을 사용하여 데이터 공유와 재사용이 거의 불가능했다. 이를 해결하기 위해 구글 딥마인드 등을 중심으로 RLDS(Reinforcement Learning Datasets) 포맷이 제안되었으며, 현재 Open X-Embodiment 프로젝트의 표준으로 자리 잡았다.16</p>
<p>RLDS는 데이터를 에피소드(Episode) 단위의 시퀀스로 직렬화하며, 각 스텝(Step)을 (Observation, Action, Reward, Is_Terminal, Is_First, Is_Last)의 튜플로 정의한다. 이는 프로그래밍 언어의 표준 라이브러리와 같아서, 연구자들은 하부 데이터 로딩 파이프라인을 신경 쓰지 않고 모델 개발에 집중할 수 있다. 또한, TFRecord 형식으로 저장되어 대규모 병렬 처리와 스트리밍 학습에 최적화되어 있다.</p>
<h3>3.2 (2) 행동 토큰화 (Action Tokenization)와 정규화</h3>
<p>최신 로봇 제어 모델인 <strong>RT-1</strong>이나 <strong>RT-2</strong>와 같은 트랜스포머 기반 모델들은 연속적인(Continuous) 제어 신호를 이산적인(Discrete) 토큰(Token)으로 변환하여 처리한다.18</p>
<ul>
<li>이산화(Discretization):</li>
</ul>
<p>RT-1은 로봇의 행동을 7~11개의 차원(x, y, z, roll, pitch, yaw, gripper 등)으로 정의하고, 각 차원의 값을 256개의 구간(Bin)으로 나눈다. 이렇게 생성된 ’행동 토큰’은 자연어 토큰과 동일한 위상을 가지며, 비전-언어 모델의 출력으로 자연스럽게 통합된다. 이는 아날로그 신호를 디지털 코드로 변환하는 ADC(Analog-to-Digital Conversion) 과정과 유사하며, 모델이 확률적 분포를 학습하는 데 도움을 준다.</p>
<ul>
<li>정규화(Normalization):</li>
</ul>
<p>서로 다른 로봇(예: Franka Panda vs WidowX)은 작업 공간의 크기와 관절의 가동 범위가 다르다. 따라서 서로 다른 데이터셋을 통합 학습(Co-training)하기 위해서는 모든 행동 데이터를 통계적으로 정규화해야 한다. 일반적으로 각 데이터셋의 행동 값 분포에서 1분위수와 99분위수를 추출하여 <span class="math math-inline">[-1, 1]</span> 범위로 선형 매핑하는 방식이 사용된다.20 이 과정이 없으면 모델은 데이터셋 간의 스케일 차이를 학습하지 못해 ’컴파일 에러’에 가까운 성능 저하를 겪게 된다.</p>
<h3>3.3 (3) 멀티모달 동기화 (Multimodal Synchronization)의 난제</h3>
<p>로봇 데이터셋 구축에서 가장 간과하기 쉬우면서도 치명적인 문제가 바로 ’시간 동기화’이다. 로봇은 카메라(30~60Hz), 라이다(10~20Hz), 관절 센서(1kHz), 힘/토크 센서(1kHz) 등 서로 다른 주기를 가진 센서들의 데이터를 처리한다.</p>
<p>만약 카메라 이미지와 로봇의 관절 위치 정보 사이에 수 밀리초(ms)의 오차만 있어도, 모델은 인과관계가 어긋난 데이터를 학습하게 된다. 예를 들어, 로봇이 물체를 잡은 순간의 이미지가 잡기 전의 관절 명령과 매핑된다면, 학습된 정책은 헛손질을 하거나 물체를 떨어뜨리게 될 것이다.22</p>
<p>이를 해결하기 위해 **하드웨어 트리거링(Hardware Triggering)**을 사용하여 모든 센서의 셔터를 물리적으로 동기화하거나, ROS bag과 같은 미들웨어 레벨에서 타임스탬프를 기반으로 정밀한 보간(Interpolation)을 수행해야 한다. 이는 멀티스레드 프로그래밍에서의 ’Race Condition’을 방지하기 위한 동기화(Synchronization) 메커니즘과 동일한 중요성을 갖는다.</p>
<h2>4.  데이터 큐레이션과 품질 관리: 디버깅(Debugging)과 리팩토링(Refactoring)</h2>
<p>“Garbage In, Garbage Out“은 데이터 중심 AI의 불변의 법칙이다. 특히 로봇 데이터는 수집 비용이 매우 높기 때문에, 무작정 데이터의 양을 늘리는 것보다 데이터의 품질을 관리하는 것이 훨씬 효율적이다.</p>
<h3>4.1 (1) 데이터 큐레이션 알고리즘: CUPID</h3>
<p>대규모 데이터셋에는 필연적으로 실패한 궤적, 최적이지 않은(Sub-optimal) 행동, 혹은 센서 노이즈가 포함된다. 이를 사람이 일일이 검수하는 것은 불가능하다. 이에 따라 자동화된 데이터 큐레이션 기술이 주목받고 있다.</p>
<p>CUPID (Curating Data your Robot Loves) 알고리즘은 영향 함수(Influence Functions) 이론을 로봇 학습에 적용한 대표적인 사례이다.24 영향 함수는 특정 훈련 데이터 포인트가 제거되었을 때 모델의 검증 손실(Validation Loss)이 어떻게 변화할지를 근사적으로 계산한다.</p>
<ul>
<li>수식적 배경:</li>
</ul>
<p>데이터 포인트 <span class="math math-inline">z</span>가 모델 파라미터 <span class="math math-inline">\hat{\theta}</span>에 미치는 영향은 헤시안 행렬(Hessian Matrix)의 역행렬을 이용해 다음과 같이 추정할 수 있다.<br />
<span class="math math-display">
  \mathcal{I}_{up, loss}(z, z_{test}) = - \nabla_{\theta} L(z_{test}, \hat{\theta})^\top H_{\hat{\theta}}^{-1} \nabla_{\theta} L(z, \hat{\theta})
</span><br />
여기서 <span class="math math-inline">\mathcal{I}</span> 값이 음수라면 해당 데이터 <span class="math math-inline">z</span>는 모델의 성능을 향상시키는 유익한 데이터이며, 양수라면 성능을 저해하는 ‘버그’ 데이터로 간주할 수 있다.</p>
<ul>
<li><strong>효과:</strong> CUPID를 통해 큐레이션된 데이터셋은 원본 데이터의 30% 크기만으로도 전체 데이터를 사용했을 때보다 더 높은 성능을 달성할 수 있음이 입증되었다. 이는 코드 리팩토링을 통해 불필요한 라인을 삭제하고 실행 속도를 높이는 과정과 같다.26</li>
</ul>
<h3>4.2 (2) 다양성과 규모의 법칙 (Scaling Laws &amp; Diversity)</h3>
<p>Open X-Embodiment 프로젝트의 연구 결과는 데이터의 양(Scale)과 다양성(Diversity)이 모델 성능에 미치는 영향을 명확히 보여준다.</p>
<p>단일 로봇, 단일 환경에서 수집된 데이터로 학습한 모델(Specialist)은 훈련 환경에서는 높은 성능을 보이지만, 조명이나 배경이 조금만 바뀌어도 실패한다. 반면, 다양한 로봇과 환경에서 수집된 데이터(X-data)로 함께 학습(Co-training)한 RT-X 모델은 처음 보는 환경에서도 강건한 성능을 보였다.27</p>
<p>특히, **카메라 시점(Viewpoint)**과 **배경(Scene)**의 다양성은 과적합(Overfitting)을 방지하는 핵심 요소이다.9 이는 다양한 예외 상황을 포함하는 테스트 케이스를 많이 확보할수록 소프트웨어의 안정성이 높아지는 것과 같은 이치이다. 또한, 데이터의 분포가 특정 작업에 편중되지 않도록 균형을 맞추는 것 역시 중요하다.</p>
<h2>5.  라벨링의 심화: 보이지 않는 물리량과 의미론</h2>
<p>로봇이 다루는 물리 세계는 눈에 보이는 기하학적 정보(Geometry)만으로 구성되지 않는다. 물체의 질량, 마찰계수, 강성(Stiffness), 무게 중심 등은 직접 상호작용하기 전에는 알 수 없는 ’숨겨진 변수(Hidden Variables)’들이다.</p>
<h3>5.1 (1) 물리적 속성의 라벨링</h3>
<p>이러한 물리적 속성을 데이터셋에 라벨링하는 것은 매우 어렵다. 모든 물체의 무게를 재서 기록할 수는 없기 때문이다. 따라서 최근 연구들은 <strong>자기 지도 학습</strong>을 통해 물리량을 추정한다.</p>
<ul>
<li><strong>비전 기반 추정:</strong> 경사면에서 물체가 미끄러지는 영상이나 물체가 충돌하는 영상을 분석하여 마찰계수와 반발계수를 추정한다.29</li>
<li><strong>상호작용 기반 추정:</strong> 로봇이 물체를 살짝 밀어보거나 들어 올릴 때 발생하는 힘/토크 센서의 변화, 모터 전류량의 변화 등을 통해 질량과 마찰을 학습한다. <strong>SwingBot</strong>은 촉각 센서를 활용하여 물체를 흔들 때의 진동 패턴을 분석, 물체의 물리적 특성을 잠재 벡터(Latent Vector)로 임베딩한다.13</li>
</ul>
<h3>5.2 (2) 의미론적 라벨링과 VLA</h3>
<p>최근 VLA(Vision-Language-Action) 모델의 발전으로, 데이터 라벨링은 물리적 수치를 넘어 의미론적(Semantic) 차원으로 확장되고 있다. “빨간 컵을 집어라“와 같은 지시어뿐만 아니라, “이 물체는 깨지기 쉬우므로 조심해라”, “이 표면은 미끄럽다“와 같은 속성 정보가 자연어 형태로 데이터셋에 포함된다. 이는 로봇이 단순한 동작 수행을 넘어 상황에 맞는 적응적 제어(Adaptive Control)를 할 수 있게 만든다.30</p>
<h2>6.  최신 대규모 로봇 데이터셋 및 모델 사례 연구</h2>
<p>이러한 이론과 방법론이 집약된 최신 프로젝트들을 살펴보면, 데이터 중심 로봇 공학의 현주소를 파악할 수 있다.</p>
<table><thead><tr><th><strong>데이터셋/모델</strong></th><th><strong>특징 및 의의</strong></th><th><strong>규모 및 구성</strong></th><th><strong>핵심 기술</strong></th></tr></thead><tbody>
<tr><td><strong>Open X-Embodiment</strong> (Google DeepMind 등)</td><td>로봇 분야의 ‘ImageNet’. 이종 로봇 데이터 통합의 가능성 입증.</td><td>22개 로봇, 100만+ 궤적, 527개 스킬. 전 세계 34개 연구실 협력.</td><td>RLDS 포맷 표준화, RT-X 모델(Co-training), 행동 공간 정규화.27</td></tr>
<tr><td><strong>DROID</strong> (Distributed Robot Interaction Dataset)</td><td>분산형 데이터 수집의 표준. 고품질, 다양성 확보에 주력.</td><td>1.7TB 데이터, 76k 에피소드. 동일 하드웨어(Franka)로 분산 수집.</td><td>엄격한 데이터 품질 관리, 다양한 배경/조명 조건 포함.33</td></tr>
<tr><td><strong>Covariant RFM-1</strong> (Robotics Foundation Model)</td><td>상업용 물류 로봇 데이터 기반. 물리 세계의 시뮬레이터 역할 수행.</td><td>수천만 궤적의 실제 물류 현장 데이터. 텍스트, 비디오, 물리 센서 통합.</td><td>미래 프레임 예측(Video Prediction), 물리적 추론 능력 학습.35</td></tr>
<tr><td><strong>RoboTwin 2.0</strong></td><td>생성형 AI 기반의 절차적 시뮬레이션 데이터 생성 프레임워크.</td><td>731개 객체, 147개 카테고리, 50개 과업. 무한대에 가까운 변형 생성.</td><td>Sim-to-Real 최적화, 도메인 무작위화 자동화.7</td></tr>
</tbody></table>
<p>심층 분석: Open X-Embodiment와 RT-X</p>
<p>Open X-Embodiment 데이터셋은 로봇 학습의 역사적 전환점이다. 기존에는 특정 로봇을 위한 데이터는 그 로봇에게만 유효하다는 것이 정설이었다. 그러나 RT-X 연구는 충분히 큰 규모의 데이터와 모델 용량이 주어진다면, 서로 다른 형태(Embodiment)를 가진 로봇의 데이터도 서로에게 긍정적인 전이(Positive Transfer)를 일으킬 수 있음을 증명했다. 이는 데이터가 특정 하드웨어에 종속된 ’드라이버’가 아니라, 범용적인 ’지능의 원천’임을 시사한다.28</p>
<p>심층 분석: RFM-1의 물리적 추론</p>
<p>Covariant사의 RFM-1은 단순한 조작을 넘어, “이 물체를 집으면 다른 물체가 무너질까?“와 같은 인과적 추론을 수행한다. 이는 데이터셋에 단순한 성공 궤적뿐만 아니라, 다양한 실패 사례와 물리적 상호작용의 결과가 포함되어 있기 때문에 가능하다. RFM-1은 텍스트와 비디오를 입력받아 로봇의 미래 행동과 그에 따른 환경의 변화를 비디오로 생성(Prediction)할 수 있으며, 이는 로봇이 행동하기 전에 머릿속으로 시뮬레이션을 돌려보는 것과 같다.35</p>
<h2>7.  결론: 미래의 로봇 엔지니어링</h2>
<p>우리는 지금 ’로봇을 코딩하는 시대’에서 ’로봇을 가르치는 시대’로 넘어가고 있다. 안드레이 카파시의 “데이터가 곧 소스 코드“라는 통찰은 로봇 공학에서 더욱 강력한 힘을 발휘한다. 로봇 시스템 개발자의 핵심 역량은 이제 복잡한 제어 이론을 마스터하는 것에서, **(1) 어떤 데이터를 수집할 것인가(요구사항 정의), (2) 어떻게 고품질의 데이터를 효율적으로 확보할 것인가(파이프라인 구축), (3) 어떻게 데이터의 노이즈를 제거하고 정제할 것인가(QA 및 디버깅)**로 재정의되고 있다.</p>
<p>향후 로봇 기술의 발전은 더 정교한 하드웨어가 아니라, 더 방대하고 똑똑한 데이터셋에 의해 주도될 것이다. 생성형 AI를 이용한 데이터 증강, 인간의 개입을 최소화하는 자율 학습 파이프라인, 그리고 물리적 상식을 내재화한 파운데이션 모델의 등장은 로봇이 실험실을 벗어나 우리의 일상 속으로 들어오게 하는 기폭제가 될 것이다. 데이터 엔지니어링이 곧 로봇 엔지니어링인 시대, 데이터셋은 로봇에게 부여할 수 있는 가장 강력한 지능의 원천 코드이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Software 2.0 - Andrej Karpathy – Medium, https://karpathy.medium.com/software-2-0-a64152b37c35</li>
<li>Andrej Karpathy: Software Is Changing (Again) - YouTube, https://www.youtube.com/watch?v=LCEmiRjPEtQ</li>
<li>Andrej Karpathy - Programming is changing so fast : r/singularity - Reddit, https://www.reddit.com/r/singularity/comments/1ezssll/andrej_karpathy_programming_is_changing_so_fast/</li>
<li>How to Train Your Robots? The Impact of Demonstration Modality on Imitation Learning, https://arxiv.org/html/2503.07017v1</li>
<li>AR2-D2: Training a Robot Without a Robot - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v229/duan23a/duan23a.pdf</li>
<li>Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration - arXiv, https://arxiv.org/html/2504.12609v1</li>
<li>RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation - arXiv, https://arxiv.org/html/2506.18088v1</li>
<li>RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation - RoboGen workshop, https://robogen-iros.github.io/accepted/9_RoboTwin_2_0_A_Scalable_Data.pdf</li>
<li>What Matters in Learning from Large-Scale Datasets for Robot Manipulation - OpenReview, https://openreview.net/forum?id=LqhorpRLIm</li>
<li>Autonomy 2.0: The Quest for Economies of Scale - Communications of the ACM, https://cacm.acm.org/opinion/autonomy-2-0-the-quest-for-economies-of-scale/</li>
<li>So You Think You Can Scale Up Autonomous Robot Data Collection? - arXiv, https://arxiv.org/html/2411.01813v1</li>
<li>Self-Supervised Robot Learning, https://www.ri.cmu.edu/app/uploads/2019/06/dgandhi_thesis.pdf</li>
<li>SwingBot: Learning Physical Features from In-hand Tactile Exploration for Dynamic Swing-up Manipulation - MIT, https://gelsight.csail.mit.edu/swingbot/IROS2020_SwingBot.pdf</li>
<li>Generalist Robot Manipulation beyond Action Labeled Data - arXiv, https://arxiv.org/html/2509.19958v1</li>
<li>Sample-Efficient Unsupervised Policy Cloning from Ensemble Self-Supervised Labeled Videos - IEEE Xplore, https://ieeexplore.ieee.org/document/11127791/</li>
<li>google-deepmind/open_x_embodiment - GitHub, https://github.com/google-deepmind/open_x_embodiment</li>
<li>RLDS: An Ecosystem to Generate, Share, and Use Datasets in Reinforcement Learning, https://research.google/blog/rlds-an-ecosystem-to-generate-share-and-use-datasets-in-reinforcement-learning/</li>
<li>RT-1: Robotics Transformer for real-world control at scale - Google Research, https://research.google/blog/rt-1-robotics-transformer-for-real-world-control-at-scale/</li>
<li>RT-1: Robotics Transformer for Real-World Control at Scale - arXiv, https://arxiv.org/html/2212.06817</li>
<li>paper.pdf - Open X-Embodiment: Robotic Learning Datasets and RT-X Models, https://robotics-transformer-x.github.io/paper.pdf</li>
<li>FAST: Efficient Action Tokenization for Vision-Language-Action Models - Physical Intelligence, https://www.physicalintelligence.company/download/fast.pdf</li>
<li>Data Annotation for Robotics: Challenges and Innovations - iMerit, https://imerit.net/resources/blog/data-annotation-for-robotics-challenges-and-innovations/</li>
<li>8 Challenges in Multimodal Training Data Creation - DZone, https://dzone.com/articles/multimodal-training-data-challenges</li>
<li>CUPID: Curating Data your Robot Loves with Influence Functions - OpenReview, https://openreview.net/forum?id=1e6GFIZCaQ</li>
<li>CUPID: Curating Data your Robot Loves with Influence Functions - arXiv, https://arxiv.org/html/2506.19121v2</li>
<li>CUPID: Curating Data your Robot Loves with Influence Functions, https://proceedings.mlr.press/v305/agia25a.html</li>
<li>Open X-Embodiment: Robotic Learning Datasets and RT-X Models, https://robotics-transformer-x.github.io/</li>
<li>Open X-Embodiment: Robotic Learning Datasets and RT-X Models - arXiv, https://arxiv.org/html/2310.08864v4</li>
<li>Learning Physical Object Properties from Unlabeled Videos, http://phys101.csail.mit.edu/papers/phys101_bmvc.pdf</li>
<li>GraspCoT: Integrating Physical Property Reasoning for 6-DoF Grasping under Flexible Language Instructions - arXiv, https://arxiv.org/html/2503.16013v1</li>
<li>From Multi-Modal Property Dataset to Robot-Centric Conceptual Knowledge About Household Objects - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC8082111/</li>
<li>Open X-Embodiment Dataset - Emergent Mind, https://www.emergentmind.com/topics/open-x-embodiment-dataset</li>
<li>The DROID Dataset, https://droid-dataset.github.io/droid/the-droid-dataset</li>
<li>droid-dataset/droid_policy_learning: DROID Policy Learning and Evaluation - GitHub, https://github.com/droid-dataset/droid_policy_learning</li>
<li>RFM-1: A world model that understands physics - YouTube, https://www.youtube.com/watch?v=INp7I3Efspc</li>
<li>Covariant introduces RFM-1 to give robots the human-like ability to reason, https://covariant.ai/covariant-introduces-rfm-1-to-give-robots-the-human-like-ability-to-reason/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>