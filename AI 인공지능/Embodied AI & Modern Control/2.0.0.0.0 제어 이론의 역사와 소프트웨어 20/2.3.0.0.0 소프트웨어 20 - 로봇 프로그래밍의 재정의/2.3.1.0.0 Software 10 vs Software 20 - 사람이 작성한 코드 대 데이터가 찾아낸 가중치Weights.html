<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2.3.1 Software 1.0 vs. Software 2.0: 사람이 작성한 코드 대 데이터가 찾아낸 가중치(Weights)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2.3.1 Software 1.0 vs. Software 2.0: 사람이 작성한 코드 대 데이터가 찾아낸 가중치(Weights)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 2. 제어 이론의 역사와 소프트웨어 2.0</a> / <a href="index.html">2.3 소프트웨어 2.0: 로봇 프로그래밍의 재정의</a> / <span>2.3.1 Software 1.0 vs. Software 2.0: 사람이 작성한 코드 대 데이터가 찾아낸 가중치(Weights)</span></nav>
                </div>
            </header>
            <article>
                <h1>2.3.1 Software 1.0 vs. Software 2.0: 사람이 작성한 코드 대 데이터가 찾아낸 가중치(Weights)</h1>
<h2>1. 서론: 인공지능 시대의 소프트웨어 공학적 대전환</h2>
<p>로봇 공학(Robotics)과 인공지능(AI)이 융합된 Embodied AI(신체화된 인공지능) 분야는 현재 소프트웨어 개발 역사상 가장 근본적이고 파괴적인 패러다임 전환의 중심에 서 있다. 우리는 수십 년간 지속되어 온 ’명시적 프로그래밍(Explicit Programming)’의 시대, 즉 Andrej Karpathy가 정의한 <strong>Software 1.0</strong>의 시대에서, 데이터와 최적화 알고리즘이 프로그램의 동작을 결정하는 <strong>Software 2.0</strong>의 시대로 거대한 지각 변동을 겪고 있다.1 이 변화는 단순한 도구의 교체가 아니라, 문제 해결의 주체가 ’인간의 논리’에서 ’데이터의 통계적 패턴’으로 이동함을 의미하며, 이는 로봇 시스템의 아키텍처, 개발자의 역할, 그리고 로봇이 세상을 이해하는 방식을 송두리째 재정의하고 있다.2</p>
<p>전통적인 로봇 제어 시스템은 인간 엔지니어가 물리 법칙과 논리적 규칙을 바탕으로 한 줄 한 줄 코드를 작성하여 구축되었다. 그러나 현실 세계의 복잡성, 불확실성, 그리고 무한에 가까운 예외 상황(Edge Cases)은 인간이 작성한 규칙 기반 시스템의 한계를 드러냈다. 이에 대한 대안으로 등장한 Software 2.0은 신경망(Neural Networks)이라는 유연한 구조 속에 데이터를 통해 학습된 수백만, 수십억 개의 가중치(Weights)를 채워 넣음으로써, 인간이 명시적으로 기술하기 힘든 복잡한 인식 및 제어 문제를 해결하고 있다.2</p>
<p>본 섹션에서는 Software 1.0과 Software 2.0의 본질적인 차이를 심층적으로 분석하고, 로봇 공학 분야에서 이러한 전이가 왜 필연적인지, 그리고 구체적으로 어떠한 기술적 변화를 수반하는지를 상세히 기술한다. 또한, Tesla의 자율주행 시스템, Google DeepMind의 RT-2, Mobile ALOHA 프로젝트 등 최신 사례를 통해 데이터 중심(Data-driven) 로봇 개발의 실제를 조명하고, 이 새로운 패러다임이 가져올 기회와 도전 과제인 해석 가능성(Interpretability) 및 안전성(Safety) 문제를 포괄적으로 다룬다.</p>
<h2>2.  두 패러다임의 철학적 및 구조적 비교</h2>
<h3>2.1  Software 1.0: 명시적 논리의 결정체 (The Classical Stack)</h3>
<p>Software 1.0, 즉 고전적 소프트웨어 스택(The Classical Stack)은 우리가 컴퓨터 과학 교육을 통해 익숙해진 전통적인 프로그래밍 방식을 지칭한다. 이 패러다임에서 프로그래머는 문제를 해결하기 위해 명확한 알고리즘을 설계하고, 이를 C++, Python, Java 등의 프로그래밍 언어를 사용하여 명시적인 명령어(Instructions)의 집합으로 구현한다.2 로봇 공학의 관점에서 Software 1.0은 철저한 **모듈화(Modularity)**와 <strong>분해(Decomposition)</strong> 원칙에 기반한다.5</p>
<p>전통적인 로봇 제어 아키텍처는 일반적으로 ’인식(Perception) - 계획(Planning) - 제어(Control)’라는 순차적인 파이프라인으로 구성된다. 각 모듈은 해당 분야의 전문가에 의해 수학적으로 모델링되고 최적화된다.</p>
<ul>
<li><strong>인식 (Perception):</strong> 카메라, LiDAR, IMU 등의 센서 데이터를 처리하여 로봇의 상태(State)와 주변 환경의 지도(Map)를 생성한다. 여기서는 칼만 필터(Kalman Filter), SLAM(Simultaneous Localization and Mapping) 알고리즘, 기하학적 비전 처리 기술이 주로 사용된다.</li>
<li><strong>계획 (Planning):</strong> 현재 상태에서 목표 상태로 이동하기 위한 최적의 경로를 생성한다. A* 알고리즘, RRT(Rapidly-exploring Random Tree), 다익스트라(Dijkstra) 알고리즘 등이 사용되며, 장애물 회피와 같은 기하학적 제약 조건을 만족시키는 것이 핵심이다.</li>
<li><strong>제어 (Control):</strong> 생성된 경로를 따라 로봇이 실제로 움직이도록 모터에 전압이나 토크 명령을 내린다. PID 제어기(Proportional-Integral-Derivative Controller), MPC(Model Predictive Control) 등 제어 이론에 기반한 수식들이 코드로 구현된다.</li>
</ul>
<p>Software 1.0의 가장 큰 특징은 <strong>인과관계의 명확성</strong>과 <strong>해석 가능성</strong>이다. 프로그래머는 코드의 어느 부분이 어떤 기능을 수행하는지 정확히 알고 있으며, 오류가 발생했을 때 로직을 추적하여 디버깅할 수 있다. 예를 들어, 로봇이 장애물에 부딪혔다면, 라이다 센서의 노이즈 처리 문제인지, 경로 계획 알고리즘의 파라미터 문제인지, 아니면 제어기의 게인(Gain) 값 문제인지를 모듈별로 분석할 수 있다.5</p>
<p>그러나 Software 1.0은 <strong>복잡성(Complexity)의 장벽</strong>에 취약하다. 현실 세계는 수학적으로 완벽하게 모델링하기에는 너무나 많은 변수와 불확실성을 내포하고 있다. 특히 로봇이 물체와 접촉하며 조작(Manipulation)하는 작업에서는 마찰, 변형, 미세한 표면 특성 등을 수식으로 표현하는 것이 거의 불가능에 가깝다.6 구글의 연구원들이 지적했듯, 로봇 팔의 역운동학(IK) 솔버를 교체하거나 하드웨어의 미세한 변화만으로도, 기존에 하드코딩된 파라미터들이 무용지물이 되는 ’경직성(Brittleness)’이 Software 1.0의 치명적인 약점이다.8</p>
<h3>2.2  Software 2.0: 데이터가 찾아낸 최적화 (The Neural Stack)</h3>
<p>Software 2.0은 코드가 인간에 의해 작성되는 것이 아니라, <strong>최적화(Optimization)</strong> 과정에 의해 작성되는 새로운 패러다임을 의미한다. 여기서 ’코드’는 신경망의 <strong>가중치(Weights)</strong> 그 자체이다.2</p>
<p>Software 2.0 개발자는 알고리즘의 세부 단계를 명시하지 않는다. 대신, 다음과 같은 요소를 정의함으로써 프로그래밍을 수행한다:</p>
<ol>
<li><strong>목표 (Goal):</strong> 로봇이 수행해야 할 바람직한 행동 (예: “이 데이터셋의 입력에 대해 정답을 맞춰라”, “바둑 게임에서 승리해라”).</li>
<li><strong>아키텍처 (Architecture):</strong> 가중치들이 담길 그릇이자 대략적인 계산 구조 (예: CNN, Transformer, RNN). 이는 탐색할 프로그램 공간(Program Space)의 범위를 제한하는 역할을 한다.</li>
<li><strong>데이터 (Data):</strong> 바람직한 프로그램의 동작을 정의하는 입출력 예시들의 집합.</li>
</ol>
<p>이러한 요소들이 준비되면, **경사 하강법(Gradient Descent)**과 <strong>역전파(Backpropagation)</strong> 알고리즘이 방대한 연산 자원을 사용하여 프로그램 공간을 탐색한다. 이 과정은 수천만 번, 수십억 번의 반복을 통해 에러(Loss)를 최소화하는 최적의 가중치 조합을 찾아낸다. 결과적으로 생성된 신경망은 인간이 작성한 그 어떤 코드보다 복잡하고 미묘한 패턴을 인식하고 처리할 수 있게 된다.2</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>Software 1.0 (Classical Stack)</strong></th><th><strong>Software 2.0 (Neural Stack)</strong></th></tr></thead><tbody>
<tr><td><strong>작성 주체 (Author)</strong></td><td>인간 프로그래머</td><td>최적화 알고리즘 (Optimizer)</td></tr>
<tr><td><strong>코드의 형태</strong></td><td>명시적 소스 코드 (C++, Python 등)</td><td>신경망 가중치 (Float parameters)</td></tr>
<tr><td><strong>개발자의 역할</strong></td><td>알고리즘 설계, 로직 구현, 코딩</td><td>데이터 수집/정제, 아키텍처 설계, 목표 설정</td></tr>
<tr><td><strong>컴파일 과정</strong></td><td>소스 코드 <span class="math math-inline">\rightarrow</span> 바이너리 (기계어)</td><td>데이터 + 아키텍처 <span class="math math-inline">\rightarrow</span> 학습 <span class="math math-inline">\rightarrow</span> 가중치 (Frozen Model)</td></tr>
<tr><td><strong>문제 해결 방식</strong></td><td>문제 분해 (Divide and Conquer)</td><td>종단간 학습 (End-to-End Learning)</td></tr>
<tr><td><strong>실행 특성</strong></td><td>조건문에 따른 가변적 실행 시간</td><td>일정한 실행 시간 (Constant Runtime), 고도의 병렬성</td></tr>
<tr><td><strong>유지 보수</strong></td><td>코드 수정, 버그 픽스</td><td>데이터셋 추가/수정 후 재학습 (Re-training)</td></tr>
<tr><td><strong>강점</strong></td><td>논리적 명확성, 검증 가능성, 적은 데이터</td><td>비정형 데이터 처리(이미지/음성), 일반화 능력</td></tr>
<tr><td><strong>약점</strong></td><td>복잡한 현실 모델링 한계, 예외 처리 비용</td><td>불투명성(Black Box), 막대한 데이터/컴퓨팅 요구</td></tr>
</tbody></table>
<p>이 표는 두 패러다임의 근본적인 차이를 요약한다. Software 2.0은 본질적으로 연역적(Deductive) 접근이 아닌 귀납적(Inductive) 접근을 취하며, 이는 로봇이 복잡한 환경에서 유연하게 대처할 수 있는 능력을 부여한다.</p>
<h2>3.  Software 2.0의 작동 원리: 경사 하강법은 새로운 컴파일러다</h2>
<p>Software 2.0을 이해하는 가장 직관적인 비유는 Andrej Karpathy가 제안한 **“경사 하강법은 컴파일러다(Gradient Descent is the Compiler)”**라는 개념이다.2 이 비유는 Software 2.0 개발 프로세스의 각 단계를 기존 소프트웨어 공학의 개념과 매핑시킴으로써, 데이터 중심 개발의 본질을 명확히 드러낸다.</p>
<h3>3.1  소스 코드로서의 데이터셋 (Dataset as Source Code)</h3>
<p>Software 1.0에서 소스 코드가 프로그램의 행동을 정의하듯, Software 2.0에서는 <strong>데이터셋</strong>이 바로 소스 코드이다. 개발자가 작성하는 수만 줄의 C++ 코드는, Software 2.0에서 수만 개의 라벨링된 이미지나 로봇의 동작 시연(Demonstration) 데이터로 대체된다.</p>
<ul>
<li>Software 1.0에서 버그를 수정하기 위해 코드를 고치듯, Software 2.0에서는 데이터셋의 오라벨(Mislabel)을 수정하거나, 부족한 케이스의 데이터를 추가 수집한다.8</li>
<li>데이터셋이 커지고 다양해질수록, 컴파일된 프로그램(신경망)의 성능과 일반화 능력은 향상된다. 이는 Software 1.0에서 기능을 추가하고 예외 처리를 보강하는 것과 유사하다.</li>
</ul>
<h3>3.2  컴파일러로서의 학습 알고리즘 (Training as Compilation)</h3>
<p>Software 1.0의 컴파일러가 인간이 읽을 수 있는 소스 코드를 기계가 실행 가능한 바이너리로 변환하듯, Software 2.0의 <strong>학습(Training)</strong> 과정은 추상적인 데이터셋과 목표 함수를 실행 가능한 신경망 가중치로 변환한다.10</p>
<ul>
<li>이 ‘컴파일’ 과정은 GPU 클러스터 상에서 막대한 연산(Floating Point Operations)을 소모하며 며칠, 심지어 몇 주가 걸리기도 한다. 이는 대규모 C++ 프로젝트의 빌드 시간과 비견될 수 있다.</li>
<li>경사 하강법은 고차원의 파라미터 공간(Parameter Space)에서 손실 함수(Cost Function)의 기울기(Gradient)를 따라 가장 낮은 지점(Optimal Point)을 찾아 내려가는 과정이다. 이는 마치 눈을 가린 등산객이 발의 감각만으로 산을 내려가는 것과 같으며, 이 과정에서 최적의 프로그램 로직이 ’발견’된다.10</li>
</ul>
<h3>3.3  실행 파일로서의 신경망 (Neural Network as Binary)</h3>
<p>학습이 완료된 신경망(Frozen Model)은 Software 1.0의 바이너리 실행 파일에 해당한다. 이 ’Neural Binary’는 몇 가지 흥미로운 계산적 특성을 가진다.11</p>
<ul>
<li><strong>계산 균일성 (Computational Homogeneity):</strong> 신경망의 실행은 대부분 행렬 곱셈(Matrix Multiplication)과 활성화 함수(ReLU 등)로 이루어진다. 이는 복잡한 분기문(Branching)과 논리 연산이 뒤섞인 Software 1.0 코드와 달리, 하드웨어(Silicon) 수준에서 최적화하기 매우 유리하다.</li>
<li><strong>일정한 실행 시간 (Constant Runtime):</strong> 입력 데이터의 값과 상관없이, 신경망의 연산 량(FLOPs)은 항상 일정하다. 이는 실시간성(Real-time)이 중요한 로봇 제어 시스템에서 최악의 경우(Worst-case) 실행 시간을 보장하기 쉽게 만든다. 무한 루프에 빠질 위험도 원천적으로 배제된다.</li>
<li><strong>메모리 사용의 예측 가능성:</strong> 필요한 메모리 양이 아키텍처에 의해 고정되므로, 동적 메모리 할당으로 인한 힙(Heap) 파편화나 메모리 누수 문제를 걱정할 필요가 없다.</li>
</ul>
<h2>4.  로봇 공학에서의 Software 2.0 혁명: 사례와 아키텍처</h2>
<p>로봇 공학은 Software 2.0이 가장 급진적으로 적용되고 있는 분야 중 하나이다. 물리 세계와의 상호작용은 명시적 규칙으로 정의하기 어렵기 때문에, 데이터로부터 배우는 접근 방식이 필수적이다.</p>
<h3>4.1  End-to-End Visuomotor Learning (종단간 시각-운동 학습)</h3>
<p>전통적인 파이프라인(인식-계획-제어)을 허물고, 센서 입력(픽셀)에서 제어 신호(액션)까지를 하나의 거대한 신경망으로 연결하는 <strong>End-to-End Learning</strong>은 Software 2.0의 정점이다.</p>
<ul>
<li><strong>Tesla FSD v12 및 Optimus:</strong> Tesla는 자율주행 소프트웨어 FSD v12 업데이트를 통해 기존의 30만 줄이 넘는 C++ 제어 로직(예: “빨간 불이면 멈춰라”, “회전교차로 진입 시 양보해라”)을 제거했다. 대신 수백만 시간의 주행 영상 데이터를 학습한 신경망이 카메라 입력을 받아 직접 조향, 가속, 제동 명령을 출력한다.12 이는 로봇(자동차)이 명시적 규칙이 아닌, 인간 운전자의 데이터를 모방함으로써 운전이라는 복잡한 기술을 습득했음을 의미한다. Tesla의 휴머노이드 로봇 Optimus 역시 동일한 아키텍처를 사용하여, 2D 카메라 영상만으로 전신 제어(Full-body Control)를 수행한다.13</li>
<li><strong>Google RT-2 (Robotic Transformer 2):</strong> 구글의 RT-2는 시각-언어-행동(Vision-Language-Action, VLA) 모델로, 웹 스케일의 이미지와 텍스트 데이터로 학습된 거대 언어 모델(LLM)에 로봇의 행동 데이터를 통합했다. 여기서 로봇의 행동(팔의 움직임 등)은 텍스트 토큰과 동일한 방식으로 처리되어, “테이블 위에서 멸종 위기 동물을 집어라“와 같이 추상적이고 복잡한 추론이 필요한 명령을 수행할 수 있다.15 이는 Software 2.0이 단순한 제어를 넘어 의미론적 이해(Semantic Understanding)까지 확장될 수 있음을 보여준다.</li>
</ul>
<h3>4.2  행동 복제(Behavior Cloning)와 원격 조작</h3>
<p>Software 2.0에서 로봇을 프로그래밍하는 가장 주된 방법은 **행동 복제(Behavior Cloning, BC)**이다. 이는 “어떻게 해라(How)“를 코딩하는 대신, “이렇게 해라(Do like this)“라고 시범을 보이는 것이다.18</p>
<ul>
<li><strong>Mobile ALOHA:</strong> 스탠포드 대학의 Mobile ALOHA 프로젝트는 저렴한 원격 조작(Teleoperation) 장치를 사용하여 사람이 직접 로봇을 조종하며 요리, 청소, 엘리베이터 탑승 등의 데이터를 수집했다. 이 연구는 단 50회의 시연(Demonstration) 데이터만으로도, 트랜스포머 기반의 <strong>ACT (Action Chunking with Transformers)</strong> 알고리즘을 통해 복잡한 양팔 조작 작업을 성공적으로 학습할 수 있음을 증명했다.19</li>
<li><strong>원격 조작의 민주화:</strong> 과거에는 로봇 프로그래밍이 고도의 전문 지식을 요하는 작업이었으나, Software 2.0 시대에는 누구나 원격 조작 장치를 통해 로봇에게 작업을 가르칠 수 있게 되었다. 이는 ’Vibe Coding’이라는 새로운 용어로도 불리며, 자연어와 시연을 통해 로봇을 프로그래밍하는 Software 3.0 시대로의 전조로도 해석된다.21</li>
</ul>
<h3>4.3  로봇 파운데이션 모델 (Robotics Foundation Models)</h3>
<p>최근에는 특정 작업에 국한되지 않고 다양한 로봇과 환경에 범용적으로 적용될 수 있는 <strong>파운데이션 모델</strong>들이 등장하고 있다. 이는 Software 2.0의 “일반화(Generalization)” 능력을 극대화한 것이다.</p>
<ul>
<li><strong>Octo:</strong> 80만 개 이상의 로봇 조작 에피소드가 포함된 Open X-Embodiment 데이터셋으로 학습된 오픈소스 범용 로봇 정책(Generalist Robot Policy)이다. Octo는 다양한 카메라 설정과 로봇 팔 형태(Embodiment)를 아우르는 트랜스포머 기반 디퓨전 정책(Diffusion Policy)을 사용하여, 새로운 로봇 하드웨어에 대해서도 적은 데이터로 빠르게 적응(Fine-tuning)할 수 있다.22</li>
<li><strong>Covariant RFM-1 &amp; Skild AI:</strong> Covariant의 RFM-1은 로봇이 물리적 상호작용의 결과를 예측하고 계획할 수 있는 ‘월드 모델(World Model)’ 기능을 갖춘 파운데이션 모델이다.24 Skild AI는 대규모 시뮬레이션과 인터넷 비디오 데이터를 활용하여, 네 발 달린 로봇부터 휴머노이드까지 모든 형태의 로봇을 제어할 수 있는 “하나의 뇌(One Brain)“를 개발하고 있다.26</li>
</ul>
<h2>5.  Software 2.0 개발 주기의 핵심: 데이터 엔진 (Data Engine)</h2>
<p>Software 2.0 패러다임에서 엔지니어링의 중심은 ’코드 작성’에서 ’데이터 파이프라인 구축’으로 이동한다. 이를 체계화한 것이 바로 **데이터 엔진(Data Engine)**이다. 테슬라의 성공 요인으로 꼽히는 데이터 엔진은 지속적인 성능 향상을 위한 선순환 루프를 의미한다.28</p>
<h3>5.1  데이터 엔진의 작동 메커니즘</h3>
<ol>
<li><strong>모델 배포 및 섀도우 모드 (Shadow Mode):</strong> 학습된 초기 모델을 실제 로봇(또는 차량)에 배포한다. 모델은 실제로 제어 권한을 갖지 않더라도 백그라운드에서 계속 예측을 수행한다.</li>
<li><strong>트리거 및 데이터 수집 (Triggering):</strong> 모델의 예측이 사람의 조작과 다르거나(Disengagement), 불확실성이 높은 상황(Edge Case)이 발생하면 해당 순간의 데이터를 자동으로 캡처하여 클라우드로 전송한다. 예를 들어, 공사 현장이나 폭우 속에서의 주행 데이터가 이에 해당한다.</li>
<li><strong>자동 라벨링 (Auto-labeling) 및 정제:</strong> 수집된 데이터에 정답 라벨을 달아야 한다. 테슬라는 NeRF(Neural Radiance Fields)나 Occupancy Network와 같은 기술을 활용하여, 오프라인에서 과거와 미래의 프레임을 모두 참조해 고정밀 3D 라벨을 자동으로 생성한다.30 이는 사람이 일일이 라벨링하는 것보다 훨씬 효율적이고 정확하다.</li>
<li><strong>재학습 (Re-training) 및 평가:</strong> 새로 추가된 고난도 데이터를 포함하여 모델을 재학습시킨다. 이때 단위 테스트(Unit Testing)는 특정 시나리오 데이터셋에 대한 모델의 성능(정확도, 충돌률 등)을 평가하는 것으로 대체된다.32</li>
<li><strong>재배포:</strong> 성능이 향상된 모델을 다시 배포하고, 이 과정을 무한히 반복한다.</li>
</ol>
<p>이러한 데이터 엔진은 Software 2.0 시스템이 시간이 지날수록 스스로 똑똑해지는(Flywheel Effect) 핵심 동력이다. 데이터 큐레이션은 이제 가장 중요한 프로그래밍 행위가 되었다.33</p>
<h3>5.2  Sim-to-Real: 가상 세계에서의 컴파일</h3>
<p>현실 세계에서 데이터를 수집하는 것은 비용이 많이 들고 위험하다. 이에 대한 대안으로 <strong>시뮬레이션</strong>이 Software 2.0의 중요한 데이터 소스이자 훈련장으로 부상하고 있다.</p>
<ul>
<li><strong>디지털 트윈 (Digital Twin):</strong> NVIDIA Isaac Sim이나 Omniverse와 같은 도구를 사용하여 현실 환경을 가상 세계에 정밀하게 복제한다. 가상 로봇은 이 안에서 강화학습(Reinforcement Learning)을 통해 수백만 번의 시행착오를 겪으며 학습한다.35</li>
<li><strong>Sim-to-Real Transfer:</strong> 시뮬레이션에서 학습된 정책을 실제 로봇에 이식할 때 발생하는 ’현실 격차(Reality Gap)’를 줄이는 것이 핵심 과제이다. 도메인 무작위화(Domain Randomization) 기법을 통해 시뮬레이션의 물리 파라미터(마찰, 질량, 조명 등)를 다양하게 변주시킴으로써, 로봇이 현실의 불확실성에 강건해지도록 훈련시킨다.37 RialTo와 같은 최신 기술은 스마트폰 스캔만으로 실제 환경의 디지털 트윈을 즉석에서 생성하여 강화학습을 수행하는 수준에 이르렀다.39</li>
</ul>
<h2>6.  Software 2.0의 도전 과제와 하이브리드 미래</h2>
<p>Software 2.0이 모든 문제를 해결하는 만능열쇠는 아니다. 특히 안전이 최우선시되는 로봇 공학 분야에서는 Software 2.0의 본질적인 한계점들이 명확히 존재한다.</p>
<h3>6.1  불투명성과 해석 가능성 (The Black Box Problem)</h3>
<p>Software 1.0 코드는 읽을 수 있지만, Software 2.0의 수억 개 가중치는 인간이 직관적으로 이해할 수 없다. 로봇이 왜 멈췄는지, 왜 특정 물체를 잘못 인식했는지에 대한 명확한 인과관계를 설명하기 어렵다.2 이는 사고 발생 시 책임 소재를 규명하거나, 시스템의 안전성을 인증(Verification)해야 하는 산업 현장에서 큰 걸림돌이 된다. 디버깅은 코드 수정이 아닌, 데이터 분포 분석과 실패 케이스 수집으로 이루어지며, 이는 확률적이고 간접적인 과정이다.41</p>
<h3>6.2  안전성 필터와 하이브리드 아키텍처</h3>
<p>이러한 한계를 극복하기 위해, Software 2.0의 뛰어난 성능과 Software 1.0의 신뢰성을 결합하려는 <strong>하이브리드 아키텍처</strong> 연구가 활발하다.</p>
<ul>
<li><strong>예측 안전 필터 (Predictive Safety Filter):</strong> 신경망(Software 2.0)이 생성한 제어 명령을 로봇에 전달하기 전에, 제어 이론 기반의 안전 필터(Software 1.0)가 이를 검사한다. 만약 신경망의 명령이 로봇을 충돌이나 전복과 같은 위험한 상태로 이끌 것으로 예측되면, 필터가 개입하여 안전한 명령으로 수정하거나 로봇을 정지시킨다.42 이는 “샌드위치 구조“의 형태를 띠며, 딥러닝의 창의성과 고전 제어의 안전성을 동시에 확보하려는 시도이다.</li>
<li><strong>잔차 정책 학습 (Residual Policy Learning):</strong> 기본적인 로봇 제어는 신뢰할 수 있는 고전 제어기(PID, MPC)가 담당하고, 신경망은 마찰이나 외란과 같이 모델링하기 어려운 ‘잔차(Residual)’ 부분만을 학습하여 보정 신호를 더해주는 방식이다.44 이는 학습의 난이도를 낮추고 시스템의 기본 안정성을 보장한다.</li>
</ul>
<h3>6.3  벤치마킹과 평가의 어려움</h3>
<p>Software 1.0에서는 유닛 테스트와 통합 테스트를 통해 기능의 정상 작동을 확신할 수 있지만, Software 2.0에서는 통계적인 성능 평가만이 가능하다. “정확도 99%“는 “1%의 확률로 실패할 수 있음“을 의미하며, 이 1%가 치명적인 사고로 이어질 수 있다. 따라서 RobotPerf와 같은 새로운 벤치마킹 표준과, 행동 기반 테스트(Behavioral Testing) 방법론이 중요해지고 있다.32</p>
<h2>7.  결론: Software 3.0을 향하여</h2>
<p>우리는 지금 Software 1.0에서 2.0으로 넘어가는 과도기적 혼란과 기회 속에 있다. Software 2.0은 로봇에게 시각을 주고, 언어를 이해하게 하며, 복잡한 물리 세계를 다루는 능력을 부여했다. 이는 과거의 명시적 프로그래밍으로는 도달할 수 없었던 영역이다.</p>
<p>그러나 Karpathy가 예견했듯이, 미래는 순수한 Software 2.0만으로 이루어지지는 않을 것이다. 데이터 수집, 모델 학습, 배포를 관리하는 인프라와 안전 장치는 여전히 견고한 Software 1.0 기반 위에 구축되어야 한다. 또한, 자연어 프롬프트를 통해 로봇에게 지시를 내리고 프로그래밍하는 <strong>Software 3.0</strong>의 시대가 대규모 언어 모델(LLM)과 함께 열리고 있다.21</p>
<p>Embodied AI 개발자에게 요구되는 역량은 이제 알고리즘 구현 능력에서, <strong>데이터를 이해하고 설계하는 능력</strong>, 그리고 <strong>신경망 아키텍처와 학습 파이프라인을 조율하는 능력</strong>으로 확장되고 있다. “데이터가 곧 코드“인 세상에서, 우리는 더 이상 로봇을 코딩하지 않는다. 우리는 로봇을 가르친다.</p>
<h3>7.1 요약: Software 1.0 vs Software 2.0 비교 분석</h3>
<table><thead><tr><th><strong>비교 차원</strong></th><th><strong>Software 1.0 (Classical Robotics)</strong></th><th><strong>Software 2.0 (Embodied AI &amp; Deep Learning)</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 철학</strong></td><td><strong>명시적 설계 (Explicit Design)</strong> 인간의 지식을 규칙으로 변환</td><td><strong>최적화 및 학습 (Optimization &amp; Learning)</strong> 데이터에서 패턴과 기능을 발견</td></tr>
<tr><td><strong>구성 요소</strong></td><td><strong>모듈 (Modules)</strong> Perception, Planning, Control의 독립적 결합</td><td><strong>신경망 (Neural Networks)</strong> End-to-End Policy, VLA (Vision-Language-Action)</td></tr>
<tr><td><strong>지식 표현</strong></td><td><strong>수식 및 알고리즘</strong> 미분방정식, 기하학, 논리 연산</td><td><strong>가중치 (Weights)</strong> 분산된 실수형 파라미터 (Floating Point Numbers)</td></tr>
<tr><td><strong>개발 도구</strong></td><td>IDE, 컴파일러, 디버거, 버전 관리(Git)</td><td>데이터 엔진, GPU 클러스터, 시뮬레이터, 라벨링 툴</td></tr>
<tr><td><strong>데이터의 역할</strong></td><td>알고리즘 검증 및 테스트 용도</td><td><strong>소스 코드 그 자체</strong> 프로그램의 행동을 결정하는 핵심 요소</td></tr>
<tr><td><strong>장점</strong></td><td>해석 가능성(White Box), 검증 가능성, 신뢰성, 적은 데이터로도 기본 동작 구현 가능</td><td>범용성(Generalization), 비정형 환경 적응력, 복잡한 패턴(이미지, 언어) 처리 능력, 하드웨어 효율성</td></tr>
<tr><td><strong>단점</strong></td><td>복잡한 현실(접촉, 마찰 등) 모델링의 한계, 경직성(Brittleness), 확장성의 한계</td><td>불투명성(Black Box), 막대한 데이터 및 컴퓨팅 비용, 디버깅의 어려움, 안전성 보장의 난해함</td></tr>
<tr><td><strong>대표 사례</strong></td><td>Boston Dynamics (초기 Atlas), 산업용 로봇 팔</td><td>Tesla FSD v12, Google RT-2, Mobile ALOHA, Optimus</td></tr>
</tbody></table>
<h2>8. 참고 문헌 (References in Text)</h2>
<p>본 보고서의 모든 내용은 제공된 연구 자료(Snippets)에 기반하여 작성되었다. 주요 개념인 Software 2.0의 정의와 철학은 Andrej Karpathy의 에세이 및 강연1을 참조하였다. 로봇 공학 적용 사례로는 Tesla의 FSD 및 Optimus 아키텍처12, Google DeepMind의 RT-2 및 VLA 모델15, 스탠포드 대학의 Mobile ALOHA 및 ACT 알고리즘19을 인용하였다. 또한 파운데이션 모델에 대한 내용은 Octo22와 Covariant24, Skild AI26의 사례를 바탕으로 기술하였으며, 안전성 및 하이브리드 제어 기술은 관련 학술 논문42을 참고하였다. 데이터 엔진과 Sim-to-Real 기술은 관련 산업계 리포트 및 연구28를 기반으로 분석하였다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>Summary of Andrej Karpathy’s Talk on Software and the Era of AI - GitHub, https://gist.github.com/georgemandis/b2a68b345262b94782fa6b08e41fbcf2</li>
<li>Software 2.0. I sometimes see people refer to neural… | by Andrej …, https://karpathy.medium.com/software-2-0-a64152b37c35</li>
<li>AI-Driven Innovations in Software Engineering: A Review of Current Practices and Future Directions - MDPI, https://www.mdpi.com/2076-3417/15/3/1344</li>
<li>Software 2.0: AI-Driven Innovation in Custom Development - Bacancy Technology, https://www.bacancytechnology.com/blog/software-2.0</li>
<li>Classical Robotics vs. End-to-End Learning - blogs.dal.ca, https://blogs.dal.ca/openthink/classical-robotics-vs-end-to-end-learning/</li>
<li>R²D²: Unlocking Robotic Assembly and Contact Rich Manipulation with NVIDIA Research, https://developer.nvidia.com/blog/r2d2-unlocking-robotic-assembly-and-contact-rich-manipulation-with-nvidia-research/</li>
<li>Rethinking Contact Simulation for Robot Manipulation | by Toyota Research Institute, https://medium.com/toyotaresearch/rethinking-contact-simulation-for-robot-manipulation-434a56b5ec88</li>
<li>Science and Engineering for Learned Robots | Eric Jang, https://evjang.com/2021/03/14/learning-robots.html</li>
<li>Gradient Descent In Robotics - Meegle, https://www.meegle.com/en_us/topics/gradient-descent/gradient-descent-in-robotics</li>
<li>Gradient Descent: The Mountain Trekker’s Guide to Optimization with Mathematics, https://www.kdnuggets.com/gradient-descent-the-mountain-trekker-guide-to-optimization-with-mathematics</li>
<li>Software 2.0: An Emerging Era of Automatic Code Generation - The Softtek Blog, https://blog.softtek.com/software-2.0-an-emerging-era-of-automatic-code-generation</li>
<li>Tesla’s Neural Network Revolution: How Full Self-Driving Replaced 300000 Lines of Code with AI | FredPope.com, https://www.fredpope.com/blog/machine-learning/tesla-fsd-12</li>
<li>A Complete Review Of Tesla’s Optimus Robot - Brian D. Colwell, https://briandcolwell.com/a-complete-review-of-teslas-optimus-robot/</li>
<li>Optimus can now sort objects autonomously Its neural network is trained fully end-to-end: video in, controls out. : r/teslainvestorsclub - Reddit, https://www.reddit.com/r/teslainvestorsclub/comments/16qjgfj/optimus_can_now_sort_objects_autonomously_its/</li>
<li>Vision-Language-Action Models: Concepts, Progress, Applications and Challenges - arXiv, https://arxiv.org/html/2505.04769v1</li>
<li>RT-2: New model translates vision and language into action - Google DeepMind, https://deepmind.google/blog/rt-2-new-model-translates-vision-and-language-into-action/</li>
<li>What is RT-2? Google DeepMind’s vision-language-action model for robotics, https://blog.google/technology/ai/google-deepmind-rt2-robotics-vla-model/</li>
<li>Learning for a Robot: Deep Reinforcement Learning, Imitation Learning, Transfer Learning - MDPI, https://www.mdpi.com/1424-8220/21/4/1278</li>
<li>Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation - arXiv, https://arxiv.org/html/2401.02117v1</li>
<li>mobile-aloha.pdf, https://mobile-aloha.github.io/resources/mobile-aloha.pdf</li>
<li>Andrej Karpathy: Software Is Changing (Again) - YouTube, https://www.youtube.com/watch?v=LCEmiRjPEtQ</li>
<li>Octo: An Open-Source Generalist Robot Policy, https://octo-models.github.io/</li>
<li>Octo: An Open-Source Generalist Robot Policy - ResearchGate, https://www.researchgate.net/publication/380730391_Octo_An_Open-Source_Generalist_Robot_Policy</li>
<li>Covariant AI: A Deep Dive into the Future of Robotic Automation, <a href="https://skywork.ai/skypage/en/Covariant%20AI%3A%20A%20Deep%20Dive%20into%20the%20Future%20of%20Robotic%20Automation/1976506268149018624">https://skywork.ai/skypage/en/Covariant%20AI%3A%20A%20Deep%20Dive%20into%20the%20Future%20of%20Robotic%20Automation/1976506268149018624</a></li>
<li>Covariant introduces RFM-1 to give robots the human-like ability to reason, https://covariant.ai/covariant-introduces-rfm-1-to-give-robots-the-human-like-ability-to-reason/</li>
<li>Skild AI Builds Omni-Bodied Robot Brain With NVIDIA, https://www.nvidia.com/en-us/customer-stories/skild-ai/</li>
<li>Building the general-purpose robotic brain - Skild AI, https://www.skild.ai/blogs/building-the-general-purpose-robotic-brain</li>
<li>[Podcast] How AI models learn with data labeling - Schneider Electric Blog, https://blog.se.com/digital-transformation/artificial-intelligence/2024/11/24/podcast-how-ai-models-learn-with-data-labeling/</li>
<li>Active Learning, Data Selection, Data Auto-Labeling, and Simulation in Autonomous Driving — Part 4 - Isaac Kargar, https://kargarisaac.medium.com/active-learning-data-selection-data-auto-labeling-and-simulation-in-autonomous-driving-part-4-dc985e2c83f9</li>
<li>Tesla AI Day 2021 Review - Part 2: Training Data. How Does a Car Learn?, https://towardsdatascience.com/tesla-ai-day-2021-review-part-2-training-data-how-does-a-car-learn-e8863ba3f5b0/</li>
<li>Multi-sensor data labeling for robotics &amp; AV: the path to full automation | Segments.ai, https://segments.ai/blog/multi-sensor-data-labeling-for-robotics-and-av/</li>
<li>robotperf/benchmarks: Benchmarking suite to evaluate robotics computing performance. Vendor-neutral. Grey-box and Black-box approaches. - GitHub, https://github.com/robotperf/benchmarks</li>
<li>CUPID: Curating Data your Robot Loves with Influence Functions - arXiv, https://arxiv.org/html/2506.19121v2</li>
<li>Robot Data Curation with Mutual Information Estimators - Robotics: Science and Systems, https://roboticsconference.org/2025/program/papers/23/</li>
<li>NVIDIA Isaac - AI Robot Development Platform, https://developer.nvidia.com/isaac</li>
<li>Training Sim-to-Real Transferable Robotic Assembly Skills over Diverse Geometries, https://developer.nvidia.com/blog/training-sim-to-real-transferable-robotic-assembly-skills-over-diverse-geometries/</li>
<li>Should We Learn Contact-Rich Manipulation Policies from Sampling-Based Planners? - arXiv, https://arxiv.org/html/2412.09743v1</li>
<li>Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin - arXiv, https://arxiv.org/html/2504.03597v2</li>
<li>Reconciling Reality Through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation, https://real-to-sim-to-real.github.io/RialTo/</li>
<li>Security Challenges in AI-Enabled Robotics: Threats and Countermeasures - Tuijin Jishu/Journal of Propulsion Technology, https://www.propulsiontechjournal.com/index.php/journal/article/download/6782/4432/11638</li>
<li>Grand Challenges in the Verification of Autonomous Systems - arXiv, https://arxiv.org/html/2411.14155v1</li>
<li>A Predictive Safety Filter for Learning-Based Racing Control | IEEE Journals &amp; Magazine, https://ieeexplore.ieee.org/document/9484747/</li>
<li>Safe Reinforcement Learning with a Predictive Safety Filter for Motion Planning and Control: A Drifting Vehicle Example - arXiv, https://arxiv.org/html/2506.22894v1</li>
<li>Compliant Residual Policy Learning - Emergent Mind, https://www.emergentmind.com/topics/compliant-residual-policy</li>
<li>Visual Reinforcement Learning with Residual Action, https://ojs.aaai.org/index.php/AAAI/article/view/34097/36252</li>
<li>Behavioral Testing of ML Models (Unit tests for machine learning) - YouTube, https://www.youtube.com/watch?v=Cse-3MM7mso</li>
<li>AI &amp; Robotics | Tesla, https://www.tesla.com/AI</li>
<li>Google DeepMind’s Breakthrough: New Foundational Models for Robotics - Skywork.ai, <a href="https://skywork.ai/skypage/en/Google-DeepMind&#x27;s-Breakthrough-New-Foundational-Models-for-Robotics/1947941836505600000">https://skywork.ai/skypage/en/Google-DeepMind%27s-Breakthrough-New-Foundational-Models-for-Robotics/1947941836505600000</a></li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>