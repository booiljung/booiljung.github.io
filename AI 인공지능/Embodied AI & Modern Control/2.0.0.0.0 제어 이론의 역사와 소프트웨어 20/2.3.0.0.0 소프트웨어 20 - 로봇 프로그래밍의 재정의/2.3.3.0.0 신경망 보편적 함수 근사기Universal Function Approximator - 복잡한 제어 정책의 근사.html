<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:.3.3 신경망, 보편적 함수 근사기(Universal Function Approximator): 복잡한 제어 정책의 근사</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>.3.3 신경망, 보편적 함수 근사기(Universal Function Approximator): 복잡한 제어 정책의 근사</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 2. 제어 이론의 역사와 소프트웨어 2.0</a> / <a href="index.html">2.3 소프트웨어 2.0: 로봇 프로그래밍의 재정의</a> / <span>.3.3 신경망, 보편적 함수 근사기(Universal Function Approximator): 복잡한 제어 정책의 근사</span></nav>
                </div>
            </header>
            <article>
                <h1>.3.3 신경망, 보편적 함수 근사기(Universal Function Approximator): 복잡한 제어 정책의 근사</h1>
<h2>1.  서론: 제어의 난제와 함수 근사의 패러다임 전환</h2>
<p>현대 로봇 공학, 자율 주행 시스템, 그리고 복잡한 산업 자동화 분야가 직면한 도전은 단순히 ’움직임을 만드는 것’을 넘어섰다. 오늘날의 과제는 불확실성이 가득하고, 비정형적이며, 고차원적인 환경 속에서 최적의 의사결정을 내리는 ’지능적 제어’를 구현하는 것이다. 전통적인 제어 이론, 특히 PID(Proportional-Integral-Derivative) 제어나 LQR(Linear Quadratic Regulator)과 같은 선형 제어 기법들은 수십 년간 산업계를 지탱해 온 견고한 기반이었다. 이들은 시스템의 동역학(Dynamics)을 미분 방정식으로 정교하게 모델링하고, 이를 바탕으로 안정성(Stability)을 수학적으로 보장하는 제어 입력을 산출한다.1 그러나 이러한 고전적 접근법은 ’모델링의 정확성’에 전적으로 의존한다는 치명적인 전제를 안고 있다. 물리적 시스템의 모든 마찰, 공기 저항, 유연체의 변형, 센서의 노이즈를 완벽하게 수식화하는 것은 현실적으로 불가능에 가까우며, 특히 이족 보행 로봇이나 비정형 물체를 조작하는 로봇 팔과 같이 접촉(Contact)과 충돌이 빈번한 시스템에서는 선형화(Linearization)의 가정이 붕괴되기 쉽다.3</p>
<p>이러한 한계점, 즉 ’차원의 저주(Curse of Dimensionality)’와 ’모델 불확실성(Model Uncertainty)’을 극복하기 위해 등장한 개념이 바로 신경망(Neural Networks)을 이용한 **보편적 함수 근사(Universal Function Approximation)**이다. 이 접근법에서 제어 문제는 더 이상 미분 방정식을 푸는 해석적(Analytical) 과정이 아니라, 데이터로부터 입력(상태)과 출력(행동) 사이의 복잡한 매핑 함수를 학습하는 통계적 최적화 과정으로 재정의된다. 신경망은 시스템의 물리적 모델을 명시적으로 알지 못해도, 충분한 데이터와 학습 과정을 통해 제어 정책(Control Policy)을 근사할 수 있는 강력한 유연성을 제공한다.5</p>
<p>본 장에서는 인공 신경망이 어떻게 수학적으로 임의의 복잡한 함수를 근사할 수 있는지에 대한 이론적 토대인 보편적 근사 정리(Universal Approximation Theorem, UAT)를 심층적으로 분석한다. 나아가, 이러한 수학적 성질이 실제 제어 시스템에서 어떻게 구현되는지, 고전 제어 이론과는 어떤 관계를 맺고 있는지, 그리고 테슬라(Tesla)의 오토파일럿이나 최신 로봇 학습 트렌드인 확산 정책(Diffusion Policy)과 같은 첨단 응용 사례를 통해 신경망 기반 제어의 현재와 미래를 조망한다. 우리는 신경망을 단순히 ’블랙박스(Black-box)’로 치부하는 것을 넘어, 고차원 다양체(Manifold) 상에서 작동하는 최적 제어기(Optimal Controller)로서의 수학적 본질과 그 실용적 함의를 파헤칠 것이다.</p>
<h2>2.  보편적 근사 정리(UAT): 존재론적 증명과 심층 신경망의 부상</h2>
<h3>2.1  이론적 배경과 존재 정리의 수학적 본질</h3>
<p>신경망이 복잡한 비선형 제어 정책을 학습할 수 있다는 확신은 막연한 믿음이 아닌, **보편적 근사 정리(Universal Approximation Theorem, UAT)**라는 수학적 정리에 뿌리를 두고 있다. 1989년 George Cybenko와 Kurt Hornik 등에 의해 독립적으로 증명된 이 정리는 인공지능 역사에서 가장 중요한 이론적 이정표 중 하나이다. 이 정리는 단 하나의 은닉층(Hidden Layer)을 가진 피드포워드 신경망(Feedforward Neural Network)이라 할지라도, 활성화 함수(Activation Function)가 비선형적이고 유계(Bounded)라면, 임의의 연속 함수(Continuous Function)를 원하는 오차 범위 내에서 근사할 수 있음을 보증한다.5</p>
<p>이를 제어 이론의 관점에서 해석하면 다음과 같다. 상태 공간 <span class="math math-inline">\mathcal{S} \subset \mathbb{R}^n</span>에서 최적의 행동 공간 <span class="math math-inline">\mathcal{A} \subset \mathbb{R}^m</span>으로 매핑하는 이상적인 최적 제어 정책 함수 <span class="math math-inline">\pi^*(s)</span>가 존재한다고 가정하자. 이 함수 <span class="math math-inline">\pi^*</span>가 아무리 기괴하고 복잡한 형태를 띠더라도(예를 들어, 특정 상태에서 급격하게 행동이 바뀌는 불연속적 특성을 가지더라도), 이를 임의의 작은 오차 <span class="math math-inline">\epsilon &gt; 0</span> 이내로 흉내 낼 수 있는 신경망 파라미터 <span class="math math-inline">\theta</span>가 <strong>반드시 존재한다</strong>는 것이다.6</p>
<p>수학적으로, <span class="math math-inline">I_n</span>을 <span class="math math-inline">\mathbb{R}^n</span> 공간의 콤팩트 부분집합(Compact Subset, 유계이고 닫힌 집합)이라 하고, <span class="math math-inline">C(I_n)</span>을 <span class="math math-inline">I_n</span> 위에서 정의된 연속 함수들의 공간이라 하자. 시그모이드(Sigmoid)와 같은 비선형 활성화 함수 <span class="math math-inline">\sigma(\cdot)</span>를 사용하는 단일 은닉층 신경망 <span class="math math-inline">G(x)</span>는 다음과 같이 표현된다.<br />
<span class="math math-display">
G(x) = \sum_{j=1}^{N} \alpha_j \sigma(w_j^T x + b_j)
</span><br />
여기서 <span class="math math-inline">N</span>은 은닉 뉴런의 수, <span class="math math-inline">w_j \in \mathbb{R}^n</span>은 입력 가중치, <span class="math math-inline">b_j \in \mathbb{R}</span>은 편향, <span class="math math-inline">\alpha_j \in \mathbb{R}</span>은 출력 가중치이다. UAT는 임의의 함수 <span class="math math-inline">f \in C(I_n)</span>과 임의의 오차 허용치 <span class="math math-inline">\epsilon &gt; 0</span>에 대해, 다음 부등식을 만족하는 <span class="math math-inline">N</span>과 파라미터 세트 <span class="math math-inline">\{ \alpha_j, w_j, b_j \}_{j=1}^N</span>가 존재함을 증명한다.<br />
<span class="math math-display">
\sup_{x \in I_n} | G(x) - f(x) | &lt; \epsilon
</span><br />
이 정리의 핵심은 **조밀성(Density)**에 있다. 신경망 함수들의 집합은 전체 연속 함수 공간 내에서 조밀하다(dense). 이는 유리수가 실수 공간에서 조밀한 것과 유사한 이치로, 어떤 연속 함수를 선택하더라도 그 주변에는 항상 이를 근사할 수 있는 신경망이 존재한다는 것을 의미한다.6 Hornik은 1991년 연구에서 활성화 함수가 반드시 시그모이드일 필요는 없으며, 다항식이 아닌(non-polynomial) 거의 모든 활성화 함수에 대해 이 성질이 성립함을 보였다.6 이는 현대 딥러닝에서 주로 사용되는 ReLU(Rectified Linear Unit), Tanh, Swish 등의 함수들이 모두 보편적 근사기(Universal Approximator)의 자격을 갖추고 있음을 이론적으로 뒷받침한다.</p>
<h3>2.2  ‘임의의 폭(Width)’ 대 ‘임의의 깊이(Depth)’: 심층 신경망의 효율성</h3>
<p>초기 UAT 연구는 주로 ‘너비(Width)’, 즉 은닉층의 뉴런 개수 <span class="math math-inline">N</span>을 무한히 늘리는 경우에 초점을 맞추었다. 이론적으로는 층이 하나뿐인 얇은 네트워크(Shallow Network)도 충분히 넓다면 모든 함수를 근사할 수 있다. 그러나 ’가능하다’는 것과 ’효율적이다’라는 것은 별개의 문제이다. 얇은 네트워크로 고도로 변동성이 큰(highly oscillatory) 함수를 근사하려면 뉴런의 수가 지수적으로 증가해야 할 수 있다. 이는 파라미터 수의 폭발적 증가를 야기하며, 과적합(Overfitting) 위험과 계산 비용을 높인다.6</p>
<p>반면, 현대 딥러닝 이론은 **깊이(Depth)**의 중요성을 강조한다. 층을 깊게 쌓은 심층 신경망(Deep Neural Networks, DNN)은 얇은 신경망에 비해 훨씬 적은 수의 파라미터로 동일한 수준의 복잡성을 가진 함수를 표현할 수 있는 ’표현 효율성(Expressive Efficiency)’을 가진다.10 Lu 등의 연구(2017)는 너비가 제한된(bounded width) 상태에서도 깊이를 증가시킴으로써 보편적 근사 능력을 확보할 수 있음을 보였다.6</p>
<p>제어 정책의 관점에서 깊이는 **계층적 특징 추출(Hierarchical Feature Extraction)**을 가능하게 한다. 예를 들어, 자율 주행 차량의 제어 정책을 생각해보자.</p>
<ul>
<li><strong>초기 레이어:</strong> 카메라 이미지에서 엣지(Edge), 텍스처 등 저수준의 시각적 특징을 추출한다.</li>
<li><strong>중간 레이어:</strong> 차선, 보행자, 신호등과 같은 객체 단위의 의미론적 정보를 형성한다.</li>
<li><strong>상위 레이어:</strong> 주변 객체들의 관계와 상황을 판단하고, 최종적으로 조향각과 가속도라는 제어 명령을 결정한다.</li>
</ul>
<p>이러한 계층적 구조는 얕은 네트워크가 단순히 입력과 출력 사이의 매핑을 ’암기’하려 드는 것과 달리, 데이터 내재된 구조(Structure)와 인과관계(Causality)를 학습하여 더 높은 일반화 성능(Generalization)을 발휘하게 한다.10</p>
<h3>2.3  존재성(Existence)과 구성 가능성(Constructability)의 간극</h3>
<p>UAT가 제어 이론 연구자들에게 주는 가장 큰 딜레마는 이 정리가 **존재성 정리(Existence Theorem)**일 뿐, **구성적 정리(Constructive Theorem)**가 아니라는 점이다.5 정리는 “최적의 신경망이 어딘가에 있다“고 말해주지만, “그 신경망을 어떻게 찾을 수 있는지”, “뉴런이 몇 개나 필요한지”, “가중치 초기값은 어떻게 설정해야 하는지“에 대해서는 침묵한다.</p>
<p>실제 문제 해결 과정에서 우리는 경사 하강법(Gradient Descent)과 역전파(Backpropagation) 알고리즘에 의존하여 파라미터 공간을 탐색한다. 그러나 신경망의 손실 함수(Loss Function) 표면은 수많은 국소 최적해(Local Optima)와 평탄한 영역(Saddle Points)으로 가득 찬 비볼록(Non-convex) 형태이다. 따라서 이론적으로는 전역 최적해(Global Optimum)인 정책 신경망이 존재하더라도, 실제 학습 과정에서는 그에 도달하지 못하고 엉뚱한 국소 최적해에 수렴하거나 학습이 발산할 수 있다.6</p>
<p>이 간극은 강화학습(Reinforcement Learning, RL)에서 특히 두드러진다. 지도 학습(Supervised Learning)과 달리, 강화학습에서는 정답(Label)이 주어지지 않고 희소한 보상(Sparse Reward)만이 주어지기 때문에, 신경망이 적절한 함수 꼴로 수렴하도록 유도하는 것이 훨씬 어렵다. 이를 해결하기 위해 다양한 정규화(Regularization) 기법, 초기화 전략, 그리고 PPO(Proximal Policy Optimization)나 SAC(Soft Actor-Critic)와 같은 진보된 알고리즘들이 개발되었다.3 이들은 수학적 존재성을 실질적 유용성으로 변환하기 위한 공학적 노력의 산물이다.</p>
<h2>3.  고전 제어와 신경망 정책: 수학적 이중성과 통합</h2>
<h3>3.1  선형 제어의 정점 LQR과 그 한계</h3>
<p>고전 제어 이론에서 최적 제어(Optimal Control)의 표준은 **LQR(Linear Quadratic Regulator)**이다. LQR은 시스템이 선형이고, 비용 함수가 상태와 입력에 대한 2차 형식일 때, 수학적으로 완벽한 최적 제어기를 제공한다.1</p>
<p>시스템의 상태 방정식이 다음과 같다고 하자.<br />
<span class="math math-display">
\dot{x}(t) = Ax(t) + Bu(t)
</span><br />
여기서 <span class="math math-inline">x(t) \in \mathbb{R}^n</span>은 상태 벡터, <span class="math math-inline">u(t) \in \mathbb{R}^m</span>은 제어 입력 벡터이며, <span class="math math-inline">A, B</span>는 시스템 행렬이다. LQR은 다음의 무한 지평 비용 함수 <span class="math math-inline">J</span>를 최소화하는 것을 목표로 한다.<br />
<span class="math math-display">
J = \int_0^\infty (x(t)^T Q x(t) + u(t)^T R u(t)) dt
</span><br />
이 문제의 해는 **대수 리카티 방정식(Algebraic Riccati Equation, ARE)**을 통해 유도되는 상태 피드백 형태(State Feedback Form)로 주어진다.<br />
<span class="math math-display">
A^T P + P A - P B R^{-1} B^T P + Q = 0
</span></p>
<p><span class="math math-display">
u^*(t) = -K x(t), \quad \text{where } K = R^{-1} B^T P
</span></p>
<p>이때 <span class="math math-inline">K</span>는 고정된 이득 행렬(Gain Matrix)이다. LQR의 우아함은 그 명쾌함에 있다. 시스템 모델(<span class="math math-inline">A, B</span>)을 정확히 안다면, 최적의 제어 정책은 선형 함수 <span class="math math-inline">u = -Kx</span>로 유일하게 결정되며, 시스템의 안정성 또한 보장된다.2</p>
<p>그러나 현실 세계는 선형적이지 않다. 로봇 팔의 코리올리 힘(Coriolis force), 드론의 공기 역학적 항력, 보행 로봇의 지면 충돌 등은 모두 비선형 항을 포함한다. 이를 $ \dot{x} = f(x, u) $로 표현할 때, 고전 제어는 주로 평형점 근처에서 테일러 급수 전개를 통해 선형화(Jacobian Linearization)를 수행한다. $ A = \frac{\partial f}{\partial x}, B = \frac{\partial f}{\partial u} $. 이 선형화 모델은 평형점을 벗어나면 급격히 오차가 커지며, 제어 성능을 보장할 수 없게 된다. 이는 “코끼리가 아닌 동물에 대한 연구“라고 비유될 만큼, 비선형성을 무시한 제어의 한계를 드러낸다.1</p>
<h3>3.2  신경망 정책: 파라미터화된 비선형 제어기</h3>
<p>신경망 기반 강화학습은 제어 정책을 고정된 선형 행렬 <span class="math math-inline">K</span>가 아닌, 학습 가능한 파라미터 <span class="math math-inline">\theta</span>를 가진 비선형 함수 <span class="math math-inline">\pi_\theta(s)</span>로 정의함으로써 LQR의 한계를 돌파한다. 이를 수식적으로 비교하면 다음과 같다.</p>
<table><thead><tr><th><strong>구분</strong></th><th><strong>LQR (고전 제어)</strong></th><th><strong>신경망 정책 (강화학습)</strong></th></tr></thead><tbody>
<tr><td><strong>정책 형태</strong></td><td><span class="math math-inline">u(t) = -K x(t)</span> (선형)</td><td><span class="math math-inline">u(t) = \pi_\theta(x_t)</span> (심층 비선형)</td></tr>
<tr><td><strong>파라미터</strong></td><td><span class="math math-inline">K \in \mathbb{R}^{m \times n}</span> (행렬 요소)</td><td><span class="math math-inline">\theta \in \mathbb{R}^d</span> (신경망 가중치 <span class="math math-inline">10^6 \sim 10^9</span>개)</td></tr>
<tr><td><strong>최적화 대상</strong></td><td>비용 함수 <span class="math math-inline">J</span> (해석적 최소화)</td><td>보상 합의 기댓값 <span class="math math-inline">\mathbb{E}</span> (경사 상승법)</td></tr>
<tr><td><strong>모델 의존성</strong></td><td>Model-based (<span class="math math-inline">A, B</span> 행렬 필수)</td><td>Model-free (데이터 기반, 모델 불필요)</td></tr>
<tr><td><strong>적용 범위</strong></td><td>선형 또는 선형화 가능 시스템</td><td>비선형, 불연속, 고차원 시스템</td></tr>
</tbody></table>
<p>신경망 정책은 본질적으로 **가변 이득 제어기(Variable Gain Controller)**로 해석될 수 있다.18 신경망의 비선형 활성화 함수들은 상태 <span class="math math-inline">x</span>의 위치에 따라 국소적으로(locally) 다른 기울기를 가지므로, 이는 마치 상태에 따라 이득 행렬 <span class="math math-inline">K(x)</span>가 실시간으로 변하는 것과 유사한 효과를 낸다. 즉, 신경망은 상태 공간 전체에 걸쳐 수많은 국소 선형 제어기(Local Linear Controllers)들을 부드럽게 이어 붙인(stitch) 전역 제어기(Global Controller) 역할을 수행한다.19</p>
<h3>3.3  정책 경사(Policy Gradient)와 최적화</h3>
<p>신경망 정책의 파라미터 <span class="math math-inline">\theta</span>를 찾기 위해 강화학습은 리카티 방정식을 푸는 대신 **정책 경사 정리(Policy Gradient Theorem)**를 사용한다.14 목표 함수 <span class="math math-inline">J(\theta)</span>를 최대화하기 위한 그래디언트는 다음과 같이 유도된다.<br />
<span class="math math-display">
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t} \nabla_\theta \log \pi_\theta(a_t | s_t) A^\pi(s_t, a_t) \right]
</span><br />
여기서 <span class="math math-inline">\tau</span>는 궤적(trajectory), <span class="math math-inline">A^\pi</span>는 어드밴티지 함수(Advantage Function)이다. 이 식은 시스템의 물리적 모델(운동방정식)을 몰라도, 시뮬레이션이나 실제 경험에서 얻은 샘플 데이터 <span class="math math-inline">(s_t, a_t, r_t, s_{t+1})</span>만 있으면 최적 정책의 방향으로 파라미터를 업데이트할 수 있음을 보여준다.14</p>
<p>하지만 이 방법은 편향-분산 트레이드오프(Bias-Variance Trade-off) 문제를 안고 있다. 몬테카를로 샘플링에 의존하는 그래디언트 추정치는 분산이 매우 커서 학습이 불안정할 수 있다. 이를 완화하기 위해 베이스라인(Baseline)을 도입하거나, 비평가(Critic) 신경망을 추가하여 분산을 줄이는 액터-크리틱(Actor-Critic) 구조가 널리 사용된다.14</p>
<h3>3.4  안정성과 강건성(Robustness)의 문제</h3>
<p>고전 제어 이론가들이 신경망 제어에 대해 가지는 가장 큰 우려는 안정성 증명의 부재이다. LQR이나 <span class="math math-inline">H_\infty</span> 제어는 폐루프 시스템(Closed-loop System)의 안정성을 수학적으로 보장한다. 반면, 신경망은 학습 데이터 분포를 벗어난 입력(Out-of-Distribution, OOD)에 대해 예측 불가능한 행동을 보일 수 있다.26</p>
<p>최근 연구들은 이러한 간극을 메우기 위해 신경망 정책에 제어 이론적 구조를 주입하려는 시도를 하고 있다. 예를 들어, 신경망의 출력에 리야프노프 안정성(Lyapunov Stability) 조건을 강제하거나, 신경망을 통해 LQR의 가중치 행렬 <span class="math math-inline">Q, R</span>을 조절하는 하이브리드 접근법들이 연구되고 있다.2 이는 신경망의 유연함과 제어 이론의 엄밀함을 결합하여, 성능과 안전이라는 두 마리 토끼를 잡으려는 시도이다.</p>
<h2>4.  Software 2.0: 데이터 중심의 제어 정책 개발 혁명</h2>
<h3>4.1  Andrej Karpathy와 패러다임의 전환</h3>
<p>신경망이 제어 정책을 근사한다는 사실은 단순히 알고리즘의 변화를 넘어, 소프트웨어 개발 방법론의 근본적인 혁명을 의미한다. 전 테슬라(Tesla) AI 이사이자 OpenAI의 연구원이었던 Andrej Karpathy는 이를 **‘Software 2.0’**이라 명명하며, 프로그래밍의 본질이 변화하고 있음을 설파했다.29</p>
<ul>
<li><strong>Software 1.0 (고전적 프로그래밍):</strong> 프로그래머가 C++, Python 등의 언어로 명시적인 규칙과 논리를 작성한다. “빨간불이면 멈춰라”, “앞차와의 거리가 10m 이하면 브레이크를 밟아라“와 같이 인간의 지식을 코드로 번역하는 과정이다. 그러나 자율 주행과 같은 복잡한 환경에서는 예외 상황(Edge Case)이 무한히 발생하므로, 이를 모두 1.0 방식으로 코딩하는 것은 불가능하다. 코드가 복잡해질수록 유지보수는 어려워지고 버그는 증식한다.</li>
<li><strong>Software 2.0 (딥러닝 기반 프로그래밍):</strong> 프로그래머는 더 이상 제어 로직을 직접 짜지 않는다. 대신 **목표(Objective Function)**와 **데이터셋(Dataset)**을 정의한다. 프로그램의 로직(신경망의 가중치)은 최적화 알고리즘(Stochastic Gradient Descent, SGD)이 자동으로 작성한다. 여기서 신경망의 가중치는 기계가 작성한 ’소스 코드’이며, 인간이 관리해야 할 진짜 소스 코드는 ’데이터셋’이 된다.29</li>
</ul>
<p>이 패러다임에서 개발자의 역할은 알고리즘 설계자에서 **데이터 큐레이터(Data Curator)**로 변화한다. 더 나은 제어 성능을 얻기 위해서는 코드를 수정하는 것이 아니라, 더 양질의 데이터를 수집하고 라벨링 오류를 수정하는 데 집중해야 한다.33</p>
<h3>4.2  테슬라의 데이터 엔진(Data Engine): Software 2.0의 실제</h3>
<p>Software 2.0 철학을 로봇 제어 및 자율 주행에 가장 성공적으로 적용한 사례는 테슬라의 **데이터 엔진(Data Engine)**이다. 테슬라는 전 세계에 배포된 수백만 대의 차량을 거대한 분산 데이터 수집 장치로 활용하여 신경망 정책을 지속적으로 진화시킨다.30</p>
<p>이 과정은 다음과 같은 순환 구조를 가진다:</p>
<ol>
<li><strong>모델 배포 및 섀도 모드(Shadow Mode):</strong> 학습된 신경망 모델을 차량에 무선(OTA)으로 배포한다. 하지만 즉시 제어 권한을 주지 않고 ’섀도 모드’로 실행한다. 모델은 실시간으로 주행 상황을 판단하고 가상의 제어 명령을 내리지만, 실제 차량은 운전자가 제어한다.</li>
<li><strong>불일치 감지 및 트리거(Triggering):</strong> 모델의 예측과 운전자의 실제 행동이 크게 다르거나, 모델의 불확실성(Uncertainty)이 높은 상황이 발생하면 이를 ’실패 사례’로 간주하고 해당 순간의 데이터(이미지, 센서 값)를 트리거하여 클라우드 서버로 전송한다. 예를 들어, 모델은 직진하려 했으나 운전자가 급히 핸들을 꺾어 공사 현장을 피했다면, 이 데이터는 모델이 학습해야 할 중요한 ’엣지 케이스’가 된다.27</li>
<li><strong>오토 라벨링(Auto-labeling) 및 클린업:</strong> 수집된 데이터는 대규모 GPU 클러스터(Dojo 등)에서 더 크고 정교한 오프라인 모델(Teacher Model)이나 인간 라벨러에 의해 정답이 매겨진다. 테슬라는 3차원 공간 재구성을 통해 라벨링을 자동화하는 기술을 고도화했다.36</li>
<li><strong>재학습(Re-training) 및 컴파일:</strong> 새로운 엣지 케이스가 포함된 데이터셋으로 모델을 재학습시킨다. 이 과정은 Software 1.0의 ‘컴파일’ 과정에 해당한다.</li>
<li><strong>재배포:</strong> 성능이 향상된 모델을 다시 차량에 배포한다.</li>
</ol>
<p>이 데이터 엔진은 신경망 제어기가 인간 개발자가 상상조차 못한 복잡한 상황(비닐봉지가 날아다니는 도로, 특이한 형태의 트럭 등)에 적응할 수 있게 한다. 이는 UAT가 보장하는 ’존재하는 최적 정책’을 데이터라는 탐침을 통해 실질적으로 찾아가는 공학적 구현체이다.37</p>
<h3>4.3  로봇 공학에서의 데이터 중심 접근: RT-1과 RT-2</h3>
<p>구글의 로보틱스 트랜스포머(RT-1, RT-2) 프로젝트 또한 Software 2.0의 맥락에 있다. 과거 로봇 팔 제어는 운동학(Kinematics)과 역운동학(Inverse Kinematics)을 푸는 수식 위주였으나, RT 시리즈는 인터넷상의 방대한 텍스트/이미지 데이터와 실제 로봇의 조작 데이터(13만 개 이상의 에피소드)를 결합하여 학습했다.38</p>
<p>이를 통해 로봇은 “서랍을 열어라” 또는 “캔을 집어서 옮겨라“와 같은 추상적인 명령을 픽셀 단위의 제어 신호로 변환하는 범용적인 능력을 획득했다. 여기서 중요한 것은 로봇의 제어 알고리즘을 개선하기 위해 코드를 고친 것이 아니라, 더 다양하고 많은 데이터를 모델에 주입했다는 점이다. 데이터가 곧 로봇의 지능이 되고, 제어 정책의 원천 코드가 되는 시대가 도래한 것이다.39</p>
<h2>5.  비선형 역학, 접촉, 그리고 고차원 행동 분포의 정복</h2>
<h3>5.1  접촉 역학(Contact Dynamics)과 비평활성(Non-smoothness)</h3>
<p>로봇 제어, 특히 조작(Manipulation)과 보행(Locomotion)에서 가장 다루기 힘든 물리 현상은 **접촉(Contact)**과 **마찰(Friction)**이다. 강체 역학에서 접촉은 충돌 순간 속도가 불연속적으로 변하는 임펄스(Impulse)를 발생시키며, 마찰은 정지 마찰과 운동 마찰 사이를 오가는 스틱-슬립(Stick-Slip) 현상을 일으킨다. 수학적으로 이는 미분 불가능하거나 다가 함수(Multi-valued function) 형태를 띠는 <strong>비평활(Non-smooth) 역학</strong> 시스템이다.4</p>
<p>전통적인 모델 기반 제어(Model-based Control)는 이러한 비연속성을 다루기 위해 복잡한 상보성 조건(Complementarity Conditions)을 풀어야 하며, 이는 계산 비용이 매우 높고 수렴이 불안정하다.4 반면, 신경망은 매끄러운(Smooth) 활성화 함수(Sigmoid, Swish 등)들의 조합으로 이러한 불연속 함수를 매우 정밀하게 근사할 수 있다.13</p>
<p>강화학습은 물리 방정식을 직접 푸는 대신, 시뮬레이터나 실제 환경과의 상호작용을 통해 “어떻게 하면 넘어지지 않는지”, “어떻게 하면 미끄러운 물체를 놓치지 않는지“에 대한 정책을 내재적(Implicit)으로 학습한다. 신경망은 접촉 시점의 미세한 충격을 예측하고 보상받는 과정을 통해, 마치 인간이 본능적으로 균형을 잡듯 복잡한 접촉 역학을 제어 정책 내부에 인코딩한다.43 이는 해석적으로 풀기 난해한 하이브리드 시스템(Hybrid System) 제어 문제를 데이터 기반 근사 문제로 치환하여 해결하는 셈이다.</p>
<h3>5.2  다봉성(Multi-modality)과 행동 분포의 모델링</h3>
<p>복잡한 환경에서 최적의 행동은 하나가 아닐 수 있다. 장애물이 앞에 있을 때 왼쪽으로 피하는 것도 정답이고, 오른쪽으로 피하는 것도 정답이다. 하지만 그 평균인 ’직진’은 장애물과 충돌하는 최악의 오답이다. 이를 행동 분포의 **다봉성(Multi-modality)**이라 한다.44</p>
<p>전통적인 신경망 정책(예: 가우시안 정책)은 보통 단봉(Unimodal) 분포를 가정하여 평균과 분산<span class="math math-inline">(\mu, \sigma)</span>만을 출력한다. 이러한 모델은 다봉적 상황에서 두 정답의 중간값(평균)을 출력하여 실패하거나, 한쪽 모드로 붕괴(Mode Collapse)되는 경향이 있다.</p>
<p>반면, 최근 등장한 **확산 정책(Diffusion Policy)**은 이미지 생성 모델에서 영감을 받아 이 문제를 혁신적으로 해결했다. 확산 정책은 행동 <span class="math math-inline">a</span>를 직접 출력하는 대신, 무작위 노이즈에서 시작하여 점진적으로 노이즈를 제거(Denoising)하는 과정을 통해 행동을 생성한다.45 수학적으로 이는 행동 공간상의 데이터 분포 <span class="math math-inline">p(a|s)</span>의 스코어 함수(Score Function, <span class="math math-inline">\nabla_a \log p(a|s)</span>)를 학습하는 것과 같다.</p>
<p>확산 정책은 다음과 같은 장점을 가진다:</p>
<ol>
<li>
<p><strong>다봉성 표현:</strong> 복잡한 행동 분포를 정밀하게 표현하여, 로봇이 여러 유효한 해결책 중 하나를 자연스럽게 선택할 수 있게 한다.</p>
</li>
<li>
<p><strong>안정성:</strong> 적대적 생성 신경망(GAN)과 달리 학습이 안정적이며 모드 붕괴 현상이 적다.</p>
</li>
<li>
<p>고차원 행동 공간: 로봇 팔의 7자유도 관절이나 덱스터러스 핸드(Dexterous Hand)와 같은 고차원 행동 공간에서도 효과적으로 작동한다.47</p>
</li>
</ol>
<p>Columbia University와 MIT 연구진의 실험에 따르면, 확산 정책은 기존의 LSTM-GMM이나 IBC(Implicit Behavior Cloning) 방식 대비 로봇 조작 작업 성공률을 평균 46.9% 향상시켰다.45 이는 신경망이 단순한 함수 근사를 넘어, 조건부 확률 분포(Conditional Probability Distribution) 자체를 근사하는 강력한 도구로 진화했음을 시사한다.</p>
<h2>6.  생성적 제어와 세계 모델: 근사를 넘어선 상상</h2>
<h3>6.1  세계 모델(World Models)과 DreamerV3</h3>
<p>보편적 함수 근사기는 비단 제어 정책(Policy)에만 국한되지 않는다. 지능형 에이전트의 궁극적인 목표는 환경(Environment) 자체를 이해하고 예측하는 것이다. **세계 모델(World Model)**은 신경망을 사용하여 환경의 동역학, 즉 상태 전이 확률 <span class="math math-inline">P(s_{t+1} | s_t, a_t)</span>과 보상 함수 <span class="math math-inline">R(s_t, a_t)</span>를 근사한다.48</p>
<p>세계 모델이 학습되면, 로봇은 실제 환경에서 위험하게 시행착오를 겪을 필요 없이, 자신의 머릿속(Latent Space)에 구축된 가상 세계에서 시뮬레이션을 돌려보며 최적의 행동을 계획할 수 있다. 이를 **잠재 상상(Latent Imagination)**이라 한다.</p>
<p>DreamerV3 알고리즘은 이러한 접근법의 정점을 보여준다. DreamerV3는 고정된 하이퍼파라미터 세트만으로 아타리(Atari) 게임부터 마인크래프트(Minecraft)의 다이아몬드 채굴, 복잡한 로봇 팔 제어에 이르기까지 전혀 다른 특성을 가진 도메인들을 모두 정복했다.48</p>
<p>DreamerV3의 핵심은 다음과 같다:</p>
<ul>
<li><strong>RSSM (Recurrent State Space Model):</strong> 순환 신경망(RNN)과 CNN을 결합하여 고차원 이미지 입력을 저차원 잠재 상태(Latent State)로 압축하고, 시간적 흐름을 모델링한다.</li>
<li><strong>보편적 근사:</strong> 서로 다른 물리 법칙과 보상 체계를 가진 환경들을 동일한 신경망 아키텍처로 근사해냄으로써, 신경망이 **범용 제어 알고리즘(General-purpose Control Algorithm)**이 될 수 있음을 입증했다.</li>
<li><strong>Symlog 변환:</strong> 보상 스케일이 극단적으로 다른 다양한 환경(보상이 0.1인 환경 vs 1000인 환경)을 동시에 학습하기 위해, 로그 변환과 유사한 Symlog 함수를 적용하여 신경망의 학습 안정성을 확보했다.48</li>
</ul>
<h3>6.2  안전성 검증(Safety Verification)의 도전과 HardNet</h3>
<p>신경망의 강력한 성능에도 불구하고, 항공기나 수술 로봇과 같은 안전비평(Safety-critical) 시스템에 이를 적용하는 것은 여전히 조심스럽다. 신경망은 본질적으로 통계적 근사기이므로, 학습하지 않은 영역에서 100% 안전을 보장하기 어렵다.</p>
<p>이를 해결하기 위해 HardNet과 같은 연구들은 제어 이론의 엄밀한 제약 조건을 신경망 구조 자체에 통합한다.26 HardNet은 신경망의 출력 레이어에 최적화 문제(Optimization Problem)를 내장하여, 물리적 제약(예: 관절 각도 제한, 충돌 방지 거리)을 위반하는 제어 명령이 절대 출력되지 않도록 강제한다. 또는 **제어 장벽 함수(Control Barrier Functions, CBF)**를 신경망 학습의 손실 함수나 필터로 사용하여, 시스템 상태가 안전 영역(Safe Set)을 벗어나지 않도록 보장한다. 이는 신경망의 유연성(Approximation)과 제어 이론의 안전성 보장(Guarantees)을 결합하려는 현대 제어 공학의 최전선이다.26</p>
<h2>7.  결론 및 향후 전망: 결정론적 제어에서 학습된 직관으로</h2>
<p>본 장에서 우리는 신경망이 어떻게 고전 제어 이론의 한계를 넘어 복잡한 비선형 시스템을 제어하는 보편적 도구로 자리 잡았는지 살펴보았다. 보편적 근사 정리(UAT)는 신경망이 어떠한 제어 정책도 흉내 낼 수 있다는 이론적 가능성을 열어주었으며, 심층 신경망(DNN)의 깊이는 이러한 가능성을 효율적인 현실로 만들었다.</p>
<p>LQR로 대표되는 고전 제어가 ’해석적 해(Analytical Solution)’를 통해 명확하지만 제한된 세계를 다루었다면, 신경망 기반의 강화학습과 Software 2.0 패러다임은 ’데이터 기반 근사(Data-driven Approximation)’를 통해 불확실하고 광활한 현실 세계를 정복하고 있다. 테슬라의 데이터 엔진과 구글의 RT 로봇, 그리고 DreamerV3와 같은 세계 모델은 신경망이 단순한 패턴 인식을 넘어, 인과관계를 이해하고 미래를 상상하며 행동을 생성하는 **행동적 지능(Embodied Intelligence)**의 핵심 엔진임을 증명한다.</p>
<p>물론 과제는 남아 있다. 신경망의 ‘블랙박스’ 특성을 해소하기 위한 해석 가능성(Explainability) 연구, 데이터 효율성(Data Efficiency)을 높이기 위한 퓨샷 학습(Few-shot Learning), 그리고 수학적 안전성을 보장하기 위한 검증(Verification) 기법들은 여전히 활발히 연구되어야 할 분야이다.</p>
<p>그러나 분명한 것은, 제어 공학의 패러다임이 ’수식을 푼다(Solving equations)’는 개념에서 ’경험을 통해 최적화를 배운다(Learning optimization from experience)’는 개념으로 영구적으로 이동했다는 사실이다. 신경망은 이제 복잡한 제어 정책을 근사하는 수준을 넘어, 인간 엔지니어가 수식으로 표현할 수 없는 영역의 ’직관적 제어’를 창발(Emergence)시키는 단계로 진화하고 있다. 이것이 바로 로봇 공학이 맞이할 Software 2.0 시대의 본질이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Reinforcement Learning for Physical Dynamical Systems: An Alternative Approach, https://towardsdatascience.com/rl-for-physical-dynamical-systems-an-alternative-approach-8e2269dc1e79/</li>
<li>Neural-Assisted Synthesis of a Linear Quadratic Controller for Applications in Active Suspension Systems of Wheeled Vehicles - MDPI, https://www.mdpi.com/1996-1073/16/4/1677</li>
<li>Deep Reinforcement Learning: A Chronological Overview and Methods - MDPI, https://www.mdpi.com/2673-2688/6/3/46</li>
<li>Modeling and Analysis of Non-unique Behaviors in Multiple Frictional Impacts - Robotics, https://www.roboticsproceedings.org/rss15/p22.pdf</li>
<li>What is the universal approximation theorem, and what implications does it have for the design and capabilities of neural networks? - EITCA Academy, https://eitca.org/artificial-intelligence/eitc-ai-adl-advanced-deep-learning/neural-networks/neural-networks-foundations/examination-review-neural-networks-foundations/what-is-the-universal-approximation-theorem-and-what-implications-does-it-have-for-the-design-and-capabilities-of-neural-networks/</li>
<li>Universal approximation theorem - Wikipedia, https://en.wikipedia.org/wiki/Universal_approximation_theorem</li>
<li>Universal Approximation Theorem — Neural Networks - Theoretical Computer Science Stack Exchange, https://cstheory.stackexchange.com/questions/17545/universal-approximation-theorem-neural-networks</li>
<li>A visual proof that neural nets can compute any function - Neural networks and deep learning, http://neuralnetworksanddeeplearning.com/chap4.html</li>
<li>Multilayer feedforward networks are universal approximators, https://www.cs.cmu.edu/~epxing/Class/10715/reading/Kornick_et_al.pdf</li>
<li>Universal Function Approximation by Deep Neural Nets with Bounded Width and ReLU Activations - MDPI, https://www.mdpi.com/2227-7390/7/10/992</li>
<li>The Universal Approximation Theorem - deep mind, https://www.deep-mind.org/2023/03/26/the-universal-approximation-theorem/</li>
<li>Where can I find the proof of the universal approximation theorem? - AI Stack Exchange, https://ai.stackexchange.com/questions/13317/where-can-i-find-the-proof-of-the-universal-approximation-theorem</li>
<li>What is Universal approximation theorem | AI Basics - Ai Online Course, https://www.aionlinecourse.com/ai-basics/universal-approximation-theorem</li>
<li>Policy Gradient Methods for Reinforcement Learning with Function Approximation, https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation</li>
<li>A General Control-Theoretic Approach for Reinforcement Learning: Theory and Algorithms, https://arxiv.org/html/2406.14753v1</li>
<li>Ch. 8 - Linear Quadratic Regulators - Underactuated Robotics, https://underactuated.mit.edu/lqr.html</li>
<li>Policy Gradient Methods for the Noisy Linear Quadratic Regulator over a Finite Horizon - People, https://people.maths.ox.ac.uk/hambly/PDF/Papers/PGmethods.pdf</li>
<li>Neural Network Control of Robot Manipulators - IEEE Computer Society, https://www.computer.org/csdl/magazine/ex/1996/03/x3064/13rRUxlgxZL</li>
<li>MIT Open Access Articles Feedback controller parameterizations for reinforcement learning, <a href="https://dspace.mit.edu/bitstream/handle/1721.1/67496/Tedrake_Feedback%20controller.pdf;sequence=1">https://dspace.mit.edu/bitstream/handle/1721.1/67496/Tedrake_Feedback%20controller.pdf;sequence=1</a></li>
<li>Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics - People @EECS, https://people.eecs.berkeley.edu/~svlevine/papers/mfcgps.pdf</li>
<li>Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning, http://papers.neurips.cc/paper/2053-rates-of-convergence-of-performance-gradient-estimates-using-function-approximation-and-bias-in-reinforcement-learning.pdf</li>
<li>Policy Gradient Algorithms - Lil’Log, https://lilianweng.github.io/posts/2018-04-08-policy-gradient/</li>
<li>Policy Gradient Algorithms - Stanford University, https://web.stanford.edu/class/cme241/lecture_slides/PolicyGradient.pdf</li>
<li>Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning - NIPS papers, https://papers.nips.cc/paper/2053-rates-of-convergence-of-performance-gradient-estimates-using-function-approximation-and-bias-in-reinforcement-learning</li>
<li>Lecture 8 - Function Approximation and Policy Gradient Methods, <a href="https://mallada.ece.jhu.edu/docs/teaching/2025-summer%20school/Lecture%208%20-%20Notes.pdf">https://mallada.ece.jhu.edu/docs/teaching/2025-summer%20school/Lecture%208%20-%20Notes.pdf</a></li>
<li>Hard-Constrained Neural Networks with Universal Approximation Guarantees - arXiv, https://arxiv.org/html/2410.10807v2</li>
<li>Building the Software 2.0 Stack by Andrej Karpathy [video] - Hacker News, https://news.ycombinator.com/item?id=17280454</li>
<li>Towards a Theoretical Foundation of Policy Optimization for Learning Control Policies arXiv:2210.04810v1 [math.OC] 10 Oct 2022, https://arxiv.org/pdf/2210.04810</li>
<li>Software 2.0 - Andrej Karpathy – Medium, https://karpathy.medium.com/software-2-0-a64152b37c35</li>
<li>Lessons from Andrej Kaparthy - Antoine Buteau, https://www.antoinebuteau.com/lessons-from-andrej-kaparthy/</li>
<li>Software 2.0 – A Paradigm Shift - Robosoft Technologies, https://www.robosoftin.com/blog/software-2-0-a-paradigm-shift</li>
<li>Datasets for Software 2.0 with Taivo Pungas - James Le, https://jameskle.com/writes/taivo-pungas</li>
<li>Wrapping up my journey scaling Software 2.0 development for AV | by Clement Farabet, https://medium.com/@clementfarabet/wrapping-up-my-journey-scaling-software-2-0-development-for-av-69ee99d94f53</li>
<li>Active Learning, Data Selection, Data Auto-Labeling, and Simulation in Autonomous Driving — Part 4 - Isaac Kargar, https://kargarisaac.medium.com/active-learning-data-selection-data-auto-labeling-and-simulation-in-autonomous-driving-part-4-dc985e2c83f9</li>
<li>A visual deep dive into Tesla’s data engine as pioneered by Andrej Karpathy. - Reddit, https://www.reddit.com/r/deeplearning/comments/1c9fgy9/a_visual_deep_dive_into_teslas_data_engine_as/</li>
<li>Tesla AI Day 2022 - YouTube, https://www.youtube.com/watch?v=ODSJsviD_SU</li>
<li>The DATA driving Tesla innovation and valuation - Nimble Gravity, https://nimblegravity.com/blog/the-data-driving-tesla-innovation-and-valuation</li>
<li>Understanding Training Data in Large-Scale Machine Learning - Language Technologies Institute - Carnegie Mellon University, https://www.lti.cs.cmu.edu/research/dissertations/sangkeuc_phd_lti_2024.pdf</li>
<li>Should ‘Open Source AI’ Mean Exposing All Training Data?, https://shujisado.org/2025/02/18/should-open-source-ai-mean-exposing-all-training-data/</li>
<li>Analysis of Frictional Contact Models for Dynamic Simulation - Boston University, https://www.bu.edu/biorobotics/publications/98_Icra_friction.pdf</li>
<li>A Review of Numerical Techniques for Frictional Contact Analysis - MDPI, https://www.mdpi.com/2075-4442/13/1/18</li>
<li>The Estimate for Approximation Error of Neural Network with Two Weights - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC3891538/</li>
<li>Comparison of Deep Learning and Deterministic Algorithms for Control Modeling - MDPI, https://www.mdpi.com/1424-8220/22/17/6362</li>
<li>Diffusion Policy: - Robotics, https://www.roboticsproceedings.org/rss19/p026.pdf</li>
<li>Visuomotor Policy Learning via Action Diffusion, https://diffusion-policy.cs.columbia.edu/diffusion_policy_2023.pdf</li>
<li>Diffusion Policy, https://diffusion-policy.cs.columbia.edu/</li>
<li>Learning Multimodal Behaviors from Scratch with Diffusion Policy Gradient - arXiv, https://arxiv.org/html/2406.00681v1</li>
<li>Mastering Diverse Control Tasks through World Models - Danijar Hafner, https://danijar.com/project/dreamerv3/</li>
<li>[2301.04104] Mastering Diverse Domains through World Models - arXiv, https://arxiv.org/abs/2301.04104</li>
<li>danijar/dreamerv3: Mastering Diverse Domains through World Models - GitHub, https://github.com/danijar/dreamerv3</li>
<li>When should I prefer reinforcement learning over optimal control theory? - ResearchGate, https://www.researchgate.net/post/When_should_I_prefer_reinforcement_learning_over_optimal_control_theory</li>
<li>Learning Optimal Controllers for Linear Systems with Multiplicative Noise via Policy Gradient - The University of Texas at Dallas, https://www.utdallas.edu/~tyler.summers/papers/GravellEsfahaniSummers_TACfinal.pdf</li>
<li>Expected Policy Gradients for Reinforcement Learning, https://jmlr.org/papers/volume21/18-012/18-012.pdf</li>
<li>Convergence analysis of gradient flow for overparameterized LQR formulations - Sontag Lab, http://www.sontaglab.org/FTPDIR/2025_automatica_olivera_siami_sontag_overparametrized_lqr_reprint.pdf</li>
<li>Building the Software 2 0 Stack (Andrej Karpathy) - YouTube, https://www.youtube.com/watch?v=y57wwucbXR8</li>
<li>How Tesla uses neural network at scale in production | Phuc Nguyen, https://phucnsp.github.io/blog/self-taught/2020/04/30/tesla-nn-in-production.html</li>
<li>Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization - OpenReview, https://openreview.net/pdf/7c307a9a6f2b1e42e901aea2422257dc706fe28c.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>