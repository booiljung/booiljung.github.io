<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2.2.2 비정형 환경과 인식의 모호성: 센서 노이즈와 현실 세계의 무한한 변수</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2.2.2 비정형 환경과 인식의 모호성: 센서 노이즈와 현실 세계의 무한한 변수</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 2. 제어 이론의 역사와 소프트웨어 2.0</a> / <a href="index.html">2.2 모델링의 한계와 불확실성의 장벽 (The Modeling Bottleneck)</a> / <span>2.2.2 비정형 환경과 인식의 모호성: 센서 노이즈와 현실 세계의 무한한 변수</span></nav>
                </div>
            </header>
            <article>
                <h1>2.2.2 비정형 환경과 인식의 모호성: 센서 노이즈와 현실 세계의 무한한 변수</h1>
<h2>1. 서론: 결정론적 세계의 종말과 확률론적 현실의 도래</h2>
<p>로봇 공학의 역사는 통제 가능한 환경에서 통제 불가능한 현실로 나아가는 투쟁의 기록이다. 초기 산업용 로봇이 활약했던 공장 자동화 라인은 철저히 계산된 ’구조화된 환경(Structured Environment)’이었다. 이곳에서는 조명이 일정하게 유지되고, 바닥은 평탄하며, 작업 대상은 미리 정의된 CAD 모델과 정확히 일치하는 위치에 놓여 있었다. 이러한 환경에서 로봇의 인식(Perception)은 일종의 확인 절차에 불과했다. 센서 데이터 <span class="math math-inline">y</span>는 실제 상태 <span class="math math-inline">x</span>를 매우 높은 신뢰도로 반영하는 결정론적 함수 <span class="math math-inline">y = h(x)</span>로 모델링될 수 있었으며, 여기서 발생하는 오차 <span class="math math-inline">v</span>는 무시할 수 있거나 단순한 가우시안 백색 잡음(Gaussian White Noise)으로 취급되어 선형 필터만으로도 충분히 제거 가능했다.1</p>
<p>그러나 로봇이 연구실과 공장을 벗어나 가정, 도심의 도로, 재난 현장과 같은 ’비정형 환경(Unstructured Environment)’으로 진입하는 순간, 기존의 제어 이론을 지탱하던 가정들은 붕괴한다. 비정형 환경은 예측 불가능성이 지배하는 세계이다.1 이곳에서는 조명 조건이 구름의 이동에 따라 시시각각 변하고, 지면은 울퉁불퉁하며, 로봇이 상호작용해야 할 객체들은 형태가 고정되지 않은 옷가지나 케이블, 혹은 의도를 가진 인간과 같은 동적 에이전트들이다.4 이러한 환경적 복잡성은 로봇의 센서 시스템에 극한의 부하를 가하며, 단순한 측정 오차를 넘어선 ’인식의 모호성(Perception Ambiguity)’을 야기한다.</p>
<p>비정형 환경에서의 로봇 공학은 “센서 데이터는 거짓말을 한다“는 전제에서 출발해야 한다. 로봇이 센서를 통해 받아들이는 데이터는 현실 세계의 있는 그대로의 투영이 아니다. 그것은 물리적 센서 소자의 한계, 환경과의 복잡한 광학적·역학적 상호작용, 그리고 정보의 압축 과정에서 발생하는 손실이 뒤섞인 불완전한 파편이다. 본 장에서는 로봇이 직면하는 이러한 인식의 한계를 심층적으로 해부한다. 우리는 먼저 센서 데이터의 신뢰성을 근본적으로 훼손하는 물리적 노이즈의 기원(Camera, LiDAR, IMU)을 양자 역학적, 열역학적 수준에서 규명할 것이다. 이어 동일한 센서 데이터가 서로 다른 상태를 가리키는 지각적 에일리어싱(Perceptual Aliasing) 현상과 납치된 로봇 문제(Kidnapped Robot Problem)를 통해 상태 추정의 난제를 다룬다. 마지막으로, 변형 가능한 객체와 같은 무한한 자유도를 가진 변수들이 어떻게 로봇의 수학적 모델을 무력화시키는지, 그리고 이를 극복하기 위해 현대 로봇 공학이 채택한 확률론적 접근법(Probabilistic Robotics)과 불확실성(Uncertainty)의 분류 체계를 논의할 것이다.</p>
<h2>2.  센서 노이즈의 물리학: 불확실성의 근원</h2>
<p>로봇의 인지 시스템은 센서라는 물리적 인터페이스를 통해 세계와 접촉한다. 그러나 모든 센서는 물리 법칙의 제약을 받으며, 신호 변환 과정에서 필연적으로 노이즈가 발생한다. 구조화된 환경에서는 이러한 노이즈가 예측 가능한 범위 내에 있지만, 비정형 환경에서는 환경적 변수와 결합하여 비선형적이고 치명적인 데이터 왜곡을 일으킨다.</p>
<h3>2.1  광학 센서(Camera)의 노이즈 역학</h3>
<p>카메라는 로봇에게 가장 풍부한 의미론적(Semantic) 정보를 제공하는 센서이지만, 동시에 조명 변화와 환경적 요인에 가장 취약한 센서이기도 하다. 디지털 이미지 센서(CMOS 또는 CCD)에서 발생하는 노이즈는 빛이 전자로 변환되는 과정(Photo-electric conversion)과 이를 디지털 신호로 읽어내는 과정에서 발생하는 다양한 확률적 현상의 집합체이다.5</p>
<h4>2.1.1  광자 샷 노이즈 (Photon Shot Noise)와 양자 효율</h4>
<p>가장 근본적인 노이즈는 빛의 입자성(Corpuscular nature)에서 기인하는 ’광자 샷 노이즈’이다. 광자가 센서의 픽셀(Photosite)에 도달하는 과정은 무작위적인 포아송 분포(Poisson Distribution)를 따른다. 일정한 광량 하에서도 단위 시간 동안 픽셀에 도달하는 광자의 수는 통계적으로 변동한다.5</p>
<p>특정 노출 시간 <span class="math math-inline">t</span> 동안 입사하는 광자 플럭스를 <span class="math math-inline">\Phi</span>라고 하고, 센서의 양자 효율(Quantum Efficiency)을 <span class="math math-inline">QE</span>라고 할 때, 생성되는 신호 전자 수의 기댓값 <span class="math math-inline">N</span>은 다음과 같다:<br />
<span class="math math-display">
N = \Phi \cdot QE \cdot t
</span><br />
이때 포아송 통계에 의해 샷 노이즈의 표준편차 <span class="math math-inline">\sigma_{shot}</span>은 신호의 제곱근에 비례한다:<br />
<span class="math math-display">
\sigma_{shot} = \sqrt{N}
</span><br />
따라서 신호 대 잡음비(Signal-to-Noise Ratio, SNR)는 다음과 같이 유도된다:<br />
<span class="math math-display">
\text{SNR}_{shot} = \frac{N}{\sigma_{shot}} = \frac{N}{\sqrt{N}} = \sqrt{N}
</span><br />
이 수식은 비정형 환경, 특히 저조도(Low-light) 환경에서의 로봇 인식 성능 저하를 설명하는 핵심적인 물리적 근거가 된다. <span class="math math-inline">N</span>이 작아질수록 SNR은 급격히 감소한다.8 예를 들어, 로봇이 어두운 실내나 야간에 주행할 때, 이미지의 거친 질감(Graininess)은 센서의 결함이 아니라 빛의 양자적 불확실성이 드러난 것이다. 이는 로봇이 물체의 엣지(Edge)를 검출하거나 특징점(Feature point)을 추출하는 알고리즘의 신뢰도를 근본적으로 떨어뜨린다.</p>
<h4>2.1.2  암전류 노이즈 (Dark Current Noise)와 열역학적 간섭</h4>
<p>빛이 전혀 없는 상태에서도 센서 내부에서는 전류가 흐른다. 이를 ’암전류(Dark Current)’라고 한다. 이는 실리콘 칩 내부의 원자가 전자가 열에너지(Thermal Energy)를 얻어 전도대(Conduction Band)로 여기(Excitation)되면서 발생하는 자유 전자들이다.10 이 전자들은 광전 효과에 의해 생성된 전자와 구별되지 않으며, 이미지에 밝은 점이나 노이즈로 나타난다.</p>
<p>암전류의 발생량 <span class="math math-inline">I_{dark}</span>는 아레니우스 방정식(Arrhenius equation) 형태의 온도 의존성을 가진다:<br />
<span class="math math-display">
I_{dark} \propto T^{1.5} \exp \left( -\frac{E_g}{2kT} \right)
</span><br />
여기서 <span class="math math-inline">T</span>는 절대 온도, <span class="math math-inline">E_g</span>는 실리콘의 밴드 갭 에너지, <span class="math math-inline">k</span>는 볼츠만 상수이다.</p>
<p>이 식은 온도가 상승함에 따라 암전류 노이즈가 기하급수적으로 증가함을 시사한다.5 비정형 환경, 특히 한여름의 야외나 고열이 발생하는 재난 현장에서 로봇이 장시간 작동할 경우, 센서 자체의 발열로 인해 암전류 노이즈가 급증한다. 이는 이미지 전체에 ’안개’가 낀 듯한 효과를 주거나, 특정 픽셀이 항상 밝게 빛나는 ‘핫 픽셀(Hot Pixel)’ 현상을 유발하여 시각적 인식을 방해한다.11</p>
<h4>2.1.3  판독 노이즈 (Read Noise) 및 고정 패턴 노이즈</h4>
<p>판독 노이즈는 픽셀에 축적된 전하를 전압으로 변환하고, 이를 다시 디지털 신호로 변환(ADC)하는 회로 과정에서 발생하는 전자적 잡음이다.12 이는 광량과 무관하게 일정하게 발생하는 ‘바닥 노이즈(Noise Floor)’ 역할을 한다.</p>
<p>또한, CMOS 센서의 각 픽셀은 미세한 공정 오차로 인해 서로 다른 감도(Gain)와 오프셋(Offset)을 가진다. 이로 인해 균일한 조명 하에서도 픽셀값이 균일하지 않게 나타나는 ’고정 패턴 노이즈(Fixed Pattern Noise, FPN)’가 발생한다.5 FPN은 로봇이 텍스처가 없는 평면(예: 흰 벽)을 바라볼 때 마치 무늬가 있는 것처럼 오인하게 만들어, 시각적 주행 거리계(Visual Odometry) 알고리즘이 허위 특징점을 추적하게 만드는 원인이 된다.</p>
<table><thead><tr><th><strong>노이즈 종류</strong></th><th><strong>발생 원인 (Physics)</strong></th><th><strong>특성 및 수식적 관계</strong></th><th><strong>비정형 환경에서의 영향</strong></th></tr></thead><tbody>
<tr><td><strong>Photon Shot Noise</strong></td><td>광자의 양자적 입자성 (Poisson 통계)</td><td><span class="math math-inline">\sigma \propto \sqrt{\text{Signal}}</span> (신호 의존적)</td><td>저조도 환경(야간, 그늘)에서 객체 인식률 급감, 엣지 검출 실패</td></tr>
<tr><td><strong>Dark Current Noise</strong></td><td>반도체 내 열적 전자 생성 (Thermal Excitation)</td><td><span class="math math-inline">\sigma \propto \sqrt{t \cdot e^T}</span> (온도/시간 의존적)</td><td>고온 환경(여름철 야외)에서 이미지 품질 저하, 핫 픽셀 발생</td></tr>
<tr><td><strong>Read Noise</strong></td><td>증폭기 및 ADC 회로의 전자적 잡음</td><td>광량 무관, 고정적 (Gaussian)</td><td>신호가 약한 영역에서 지배적, 전체적인 이미지 SNR 제한</td></tr>
<tr><td><strong>Fixed Pattern Noise</strong></td><td>픽셀 간 증폭율/오프셋의 공정 미세 오차</td><td>공간적으로 고정된 패턴</td><td>단색 표면(벽, 도로)을 텍스처로 오인, 특징점 매칭 오류 유발</td></tr>
</tbody></table>
<h3>2.2  LiDAR의 기하학적 왜곡과 환경적 간섭</h3>
<p>LiDAR(Light Detection and Ranging)는 레이저 펄스를 발사하고 반사되어 돌아오는 시간(Time of Flight, ToF)을 측정하여 정밀한 거리 정보를 얻는 능동형 센서이다. 그러나 LiDAR 역시 비정형 환경의 다양한 물질적 특성과 기상 조건 앞에서 심각한 기하학적 왜곡을 겪는다.</p>
<h4>2.2.1  고반사율 객체와 블루밍(Blooming) 및 고스트(Ghosting)</h4>
<p>도로 환경에는 교통 표지판, 차선, 안전 조끼 등 재귀반사(Retro-reflective) 물질이 다수 존재한다. 이들은 입사된 빛을 광원 방향으로 집중 반사시키므로, LiDAR 수신부에 입사되는 신호 강도가 센서의 동적 범위(Dynamic Range)를 초과하는 포화(Saturation) 상태를 유발한다.13</p>
<ul>
<li><strong>블루밍(Blooming):</strong> 센서의 픽셀(Avalanche Photodiode 등)이 과도한 광자 에너지를 받으면 전하가 인접 픽셀로 넘쳐흐르는 현상(Crosstalk)이 발생한다. 이로 인해 고반사율 물체의 크기가 실제보다 부풀려져 보이는 ‘팽창’ 현상이 일어난다. 로봇은 이를 실제보다 더 큰 장애물로 인식하여 불필요한 회피 기동을 하거나 좁은 통로를 지나가지 못한다고 오판할 수 있다.13</li>
<li><strong>고스트(Ghosting):</strong> 강한 반사 신호는 센서 내부의 광학계(렌즈, 덮개 유리)에서 2차 산란을 일으키거나, 신호 처리 회로의 타이밍 에러를 유발하여 실제 물체가 없는 위치에 허상의 점군(Point Cloud)을 생성한다. 교통 표지판 근처 허공에 생성된 고스트 포인트는 자율 주행 차량이 급제동(Phantom Braking)을 하게 만드는 주요 원인 중 하나이다.13</li>
</ul>
<h4>2.2.2  정반사(Specular Reflection)와 다중 경로(Multi-path) 문제</h4>
<p>비정형 환경인 현대 도시나 실내 공간은 유리, 거울, 광택이 나는 금속, 대리석 등 정반사 특성을 가진 표면으로 가득 차 있다. 레이저 빔이 이러한 표면에 입사하면 난반사(Diffuse Reflection)되지 않고 입사각과 동일한 반사각으로 튕겨 나간다.14</p>
<ul>
<li><strong>다중 경로 오차(Multi-path Error):</strong> 정반사된 빔이 다른 물체에 맞고 센서로 돌아올 경우, 빛의 이동 경로가 실제 직선거리보다 길어진다. 로봇은 이 신호를 받아 “벽 너머에 물체가 있다“거나 “바닥이 뚫려 있다“고 잘못 판단하게 된다. 유리 건물 밀집 지역에서 발생하는 이러한 오차는 SLAM(Simultaneous Localization and Mapping) 알고리즘이 지도를 구축할 때 벽을 뚫고 지나가는 궤적을 그리게 하거나, 맵을 일그러뜨리는 치명적인 아티팩트(Artifact)를 남긴다.14</li>
<li><strong>투명 장애물 인식 실패:</strong> 깨끗한 유리창은 레이저를 대부분 통과시키거나 정반사시켜 센서로 돌아오는 신호가 거의 없다. 로봇은 유리문을 ’열린 공간’으로 인식하여 충돌할 수 있다.</li>
</ul>
<h4>2.2.3  기상 조건과 부유 입자 산란</h4>
<p>비, 안개, 눈, 먼지와 같은 대기 중의 입자들은 레이저의 진행을 방해한다. 특히 안개(Fog)는 수많은 미세 물방울로 구성되어 있어, 레이저 빔을 무작위로 산란시킨다(Mie Scattering). 이는 로봇 주변에 무수히 많은 가짜 장애물(노이즈 포인트)을 생성하여 로봇을 가상의 감옥에 가두거나, 반대로 실제 장애물로부터 반사된 신호를 감쇠(Attenuation)시켜 탐지 거리를 급격히 줄어들게 만든다.16</p>
<h3>2.3  관성 측정 장치(IMU)의 드리프트와 랜덤 워크: 적분되는 오차</h3>
<p>IMU(가속도계 및 자이로스코프)는 외부 환경과 무관하게 로봇의 내부 상태(가속도, 각속도)를 측정할 수 있는 유일한 센서(Proprioceptive Sensor)이다. 그러나 IMU는 물리량을 직접 측정하는 것이 아니라 미분된 값을 측정하며, 이를 통해 위치와 자세를 알기 위해서는 적분(Integration) 과정을 거쳐야 한다. 이 과정에서 미세한 센서 오차는 시간의 흐름에 따라 누적되고 증폭된다.17</p>
<h4>2.3.1  편향(Bias)과 온도 의존성</h4>
<p>이상적인 IMU는 정지 상태에서 0을 출력해야 하지만, 실제 센서는 제조 공정상의 한계로 인해 0이 아닌 값, 즉 편향(Bias)을 출력한다. 더 큰 문제는 이 편향이 고정되어 있지 않고 온도 변화나 기계적 응력에 따라 변한다는 점이다(Bias Instability).19 MEMS(Micro-Electro-Mechanical Systems) 구조물은 온도에 따라 미세하게 수축하거나 팽창하며, 이는 정전 용량 변화로 이어져 측정값에 오차를 더한다. 온도가 1도 변할 때마다 발생하는 편향 변화는 작아 보이지만, 이를 두 번 적분하여 위치를 추정할 경우 오차는 시간의 제곱(<span class="math math-inline">t^2</span>)에 비례하여 커진다.20</p>
<h4>2.3.2  랜덤 워크 (Random Walk) 현상</h4>
<p>IMU의 출력 신호에는 백색 잡음(White Noise)이 포함되어 있다. 백색 잡음을 적분하면 수학적으로 ‘랜덤 워크(Random Walk)’ 프로세스가 된다.20</p>
<ul>
<li>
<p>각도 랜덤 워크 (Angle Random Walk, ARW): 자이로스코프의 각속도 측정값에 포함된 백색 잡음을 적분하면 각도(자세) 오차가 된다. 이 오차의 표준편차는 시간의 제곱근(<span class="math math-inline">\sqrt{t}</span>)에 비례하여 증가한다.<br />
<span class="math math-display">
\theta_{error}(t) \propto \text{ARW} \times \sqrt{t}
</span><br />
센서 데이터시트에 보통 <span class="math math-inline">^\circ/\sqrt{h}</span> 단위로 표기되는 이 수치는, 시간이 지날수록 로봇의 방향 감각이 필연적으로 흐트러짐을 의미한다.</p>
</li>
<li>
<p>속도 랜덤 워크 (Velocity Random Walk, VRW): 가속도계의 백색 잡음을 적분하면 속도 오차가 되며, 이를 다시 적분하여 위치를 구하면 위치 오차는 시간의 3/2승(<span class="math math-inline">t^{1.5}</span>)에 비례하여 증가한다. (편향 드리프트까지 고려하면 <span class="math math-inline">t^2</span> 이상).<br />
<span class="math math-display">
\text{Position}_{error}(t) \propto t^{1.5} \sim t^2
</span><br />
이러한 특성 때문에 저가형 MEMS IMU만으로 위치를 추정하면 수십 초 내에 수십 미터의 오차가 발생하여 네비게이션이 불가능해진다.18</p>
</li>
</ul>
<p>이처럼 비정형 환경에서의 센서 데이터는 그 자체로 불확실성을 내포하고 있다. 카메라는 빛의 양자적 성질과 열에 흔들리고, LiDAR는 물질의 반사 특성에 속으며, IMU는 적분의 늪에 빠져 끝없이 표류한다. 이러한 물리적 노이즈는 단순한 필터링으로 제거할 수 없는 근본적인 한계이며, 로봇이 확률론적 접근을 취해야만 하는 첫 번째 이유이다.</p>
<h2>3.  인식의 모호성과 상태 추정의 난제</h2>
<p>물리적 센서 노이즈가 ‘데이터의 오염’ 문제라면, 인식의 모호성은 ‘데이터의 해석’ 문제이다. 비정형 환경의 구조적 특징은 때때로 완벽한 센서 데이터를 가지고도 로봇이 자신의 상태를 하나로 확정할 수 없는 상황을 만든다. 이를 ’인식의 모호성(Perceptual Ambiguity)’이라 하며, 이는 확률적 상태 추정(State Estimation) 알고리즘이 해결해야 할 가장 까다로운 과제 중 하나이다.</p>
<h3>3.1  지각적 에일리어싱(Perceptual Aliasing): 동일한 관측, 다른 위치</h3>
<p>’지각적 에일리어싱’은 서로 다른 장소나 상태가 로봇의 센서에는 구별 불가능한 동일한 패턴으로 관측되는 현상을 말한다.22 신호 처리 이론에서 고주파 신호가 낮은 샘플링 레이트로 인해 저주파 신호로 오인되는 에일리어싱 현상과 유사하게, 로봇 공학에서의 에일리어싱은 고차원의 현실 세계 정보를 저차원의 센서 데이터로 압축하는 과정에서 발생하는 정보 손실(Information Loss)에 기인한다.</p>
<h4>3.1.1  기하학적 대칭성과 반복적 환경</h4>
<p>현대 건축물이나 대형 시설물은 기하학적 대칭성과 반복적인 구조를 가진다. 예를 들어, 호텔의 긴 복도, 똑같은 모양의 사무실 문, 터널 내부, 혹은 특징 없는 넓은 창고 바닥 등이 있다.</p>
<p>로봇이 긴 복도의 중간 지점에 있다고 가정해보자. 전방을 주시하는 카메라나 LiDAR는 “양옆에 평행한 벽이 있고 천장이 있다“는 정보를 제공한다. 그러나 이 정보만으로는 로봇이 복도의 초입에 있는지, 중간에 있는지, 끝부분에 있는지 구별할 수 없다. 센서 데이터 <span class="math math-inline">z</span>에 대해 위치 <span class="math math-inline">x_1</span>과 <span class="math math-inline">x_2</span>에서의 관측 확률(Likelihood)이 거의 동일한 것이다:<br />
<span class="math math-display">
p(z | x_1) \approx p(z | x_2)
</span><br />
이러한 상황에서 로봇의 위치에 대한 확률 분포(Belief)는 단일한 정점(Peak)을 가지는 유니모달(Uni-modal) 형태가 아니라, 여러 위치에 가능성이 분산된 멀티모달(Multi-modal) 형태를 띠게 된다.23 만약 로봇이 칼만 필터(Kalman Filter)와 같이 가우시안(Gaussian) 분포를 가정하는 알고리즘을 사용한다면, 로봇은 두 가능성이 높은 위치 <span class="math math-inline">x_1</span>과 <span class="math math-inline">x_2</span>의 산술 평균인 <span class="math math-inline">\frac{x_1 + x_2}{2}</span>를 자신의 위치로 추정하게 된다. 물리적으로 이 중간 지점은 벽 속이거나 장애물 내부일 수 있으며, 이러한 잘못된 추정은 치명적인 충돌이나 경로 계획 실패로 이어진다.25</p>
<h4>3.1.2  Whitehead와 Ballard의 통찰 (1991)</h4>
<p>지각적 에일리어싱 문제는 Whitehead와 Ballard의 선구적인 연구 “Learning to perceive and act by trial and error” (1991)에서 강화 학습(Reinforcement Learning)의 맥락에서 정식화되었다.27 그들은 로봇이 즉각적인 센서 입력(Percept)만으로는 전체 상태(State)를 완전히 파악할 수 없는 ‘부분 관측(Partially Observable)’ 상황에서 발생하는 문제를 지적했다.</p>
<p>예를 들어, 미로 찾기 로봇에게 두 개의 똑같이 생긴 T자형 교차로가 있다고 하자. 첫 번째 교차로에서는 골인 지점으로 가기 위해 좌회전해야 하고, 두 번째 교차로에서는 우회전해야 한다. 그러나 로봇의 시각 센서에 비친 두 교차로의 모습이 동일하다면(에일리어싱), 로봇은 현재 입력만으로는 어떤 행동이 보상을 극대화할지 판단할 수 없다. 이는 로봇이 과거의 이력(History)을 기억하거나 능동적으로 정보를 수집하는 행동(Active Perception)을 하지 않는다면 최적의 행동 정책을 학습할 수 없음을 의미한다.</p>
<h3>3.2  납치된 로봇 문제(Kidnapped Robot Problem)와 다봉 분포</h3>
<p>인식의 모호성이 극대화된 최악의 시나리오가 바로 ’납치된 로봇 문제(Kidnapped Robot Problem, KRP)’이다.29 이는 자율 주행 중인 로봇을 누군가가 들어서 임의의 다른 장소로 옮겨 놓았을 때(순간 이동), 로봇이 자신의 위치가 변경되었음을 인지하고 새로운 위치를 찾아낼 수 있는지를 묻는 표준 벤치마크이다.</p>
<h4>3.2.1  베이지안 필터의 실패와 회복</h4>
<p>로봇이 납치된 직후, 로봇 내부의 위치 추정치(Belief)는 납치 전의 위치를 가리키고 있다. 그러나 새로운 위치에서 수집된 센서 데이터는 내부 지도상의 예상 위치와 전혀 일치하지 않는다.</p>
<ul>
<li><strong>EKF(확장 칼만 필터)의 실패:</strong> EKF는 현재 추정치 주변의 선형적인 오차만을 수정하도록 설계되어 있다. 납치로 인한 거대한 위치 오차는 비선형성을 띠므로, EKF는 센서 데이터를 ’말도 안 되는 노이즈’로 취급하여 무시(Outlier Rejection)하거나, 억지로 현재 위치 주변에 끼워 맞추려다 필터가 발산해버린다. 로봇은 자신이 엉뚱한 곳에 있다고 확신하며 벽을 향해 돌진하게 된다.30</li>
</ul>
<h4>3.2.2  파티클 필터(Particle Filter)와 센서 리세팅(Sensor Resetting)</h4>
<p>이 문제를 해결하기 위해 몬테카를로 위치 추정(MCL)과 같은 파티클 필터 방식이 도입되었다. 파티클 필터는 위치 가설을 수천 개의 입자(Particle)로 분산시켜 표현한다. 납치 상황에서 기존 입자들의 가중치(Importance Weight)는 센서 데이터와의 불일치로 인해 급격히 낮아진다. 이때 알고리즘은 가중치가 높은 새로운 입자들을 센서 데이터와 일치하는 임의의 위치(지도 전체)에 무작위로 뿌리는 ‘센서 리세팅(Sensor Resetting)’ 기법을 사용한다.24</p>
<p>이를 통해 로봇의 믿음(Belief)은 일시적으로 “나는 A에 있을 수도 있고, B에 있을 수도 있다“는 다봉 분포(Multi-modal distribution)를 형성하게 된다. 이후 로봇이 이동하며 추가적인 관측을 수행하면, 거짓 위치(Ghost location)에 있던 입자들은 소멸하고 실제 위치에 있는 입자들만 살아남아 위치 추정이 수렴하게 된다. KRP는 로봇이 단 하나의 ’정답’을 고집해서는 안 되며, 여러 가능성을 동시에 고려하는 확률적 유연성을 가져야 함을 보여준다.</p>
<h3>3.3  부분 관측 마르코프 결정 과정(POMDP)</h3>
<p>지각적 에일리어싱과 센서 노이즈가 만연한 비정형 환경을 수학적으로 가장 엄밀하게 모델링하는 프레임워크는 POMDP(Partially Observable Markov Decision Process)이다.32</p>
<p>일반적인 MDP가 “로봇은 현재 상태 <span class="math math-inline">s</span>를 안다“고 가정하는 반면, POMDP는 “로봇은 현재 상태를 직접 알 수 없고, 관측 <span class="math math-inline">o</span>를 통해 상태에 대한 확률 분포인 믿음 상태(Belief State) <span class="math math-inline">b(s)</span>만을 추론할 수 있다“고 가정한다.</p>
<p>믿음 상태 <span class="math math-inline">b</span>는 베이즈 정리(Bayes’ Theorem)를 통해 다음과 같이 업데이트된다:<br />
<span class="math math-display">
b&#39;(s&#39;) = \eta P(o | s&#39;) \sum_{s} P(s&#39; | s, a) b(s)
</span><br />
여기서:</p>
<ul>
<li><span class="math math-inline">b(s)</span>: 이전 단계에서의 상태 확률 분포 (Prior)</li>
<li><span class="math math-inline">P(s&#39; | s, a)</span>: 행동 <span class="math math-inline">a</span>에 의한 상태 전이 확률 (Motion Model)</li>
<li><span class="math math-inline">P(o | s&#39;)</span>: 상태 <span class="math math-inline">s&#39;</span>에서의 센서 관측 확률 (Measurement Model)</li>
<li><span class="math math-inline">\eta</span>: 정규화 상수</li>
</ul>
<p>POMDP 프레임워크에서 로봇의 목표는 단순히 현재 위치에서 보상을 얻는 것이 아니라, 불확실성을 줄이는 행동(정보 수집)과 목표를 달성하는 행동(보상 획득) 사이의 최적의 균형을 찾는 것이다.34</p>
<p>예를 들어, 어두운 방에서 문을 찾아야 하는 로봇(POMDP 에이전트)은 문 손잡이의 정확한 위치를 모른다(믿음 상태가 넓게 퍼져 있음). 이때 무작정 손을 뻗는 행동(Exploitation)은 실패 확률이 높다. 대신 로봇은 벽을 더듬거나 카메라 조명을 켜는 행동(Information Gathering)을 먼저 수행하여 손잡이 위치에 대한 믿음 상태를 뾰족하게(Peaked) 만든 후, 문을 여는 행동을 취한다. 이러한 ’정보 가치(Value of Information)’에 기반한 의사 결정은 비정형 환경에서 로봇이 생존하고 임무를 완수하기 위한 필수적인 지능이다. 하지만 POMDP는 믿음 상태 공간이 연속적이고 고차원이므로 계산 복잡도가 매우 높아(PSPACE-complete), 실제 적용을 위해서는 근사 알고리즘(Approximate POMDP)이 필수적으로 사용된다.35</p>
<h2>4.  현실 세계의 무한한 변수와 모델의 한계</h2>
<p>센서 노이즈와 위치 추정의 모호성을 극복한다 하더라도, 로봇은 비정형 환경의 본질적인 복잡성, 즉 ’무한한 변수’와 마주하게 된다. 강체(Rigid Body) 역학에 기반한 전통적인 로봇 제어 이론은 형태가 변하고, 끊임없이 움직이며, 인과관계가 불분명한 현실 세계 앞에서 한계를 드러낸다.</p>
<h3>4.1  변형 가능한 객체(Deformable Objects)와 무한 자유도</h3>
<p>전통적인 로봇 조작(Manipulation)은 컵, 상자, 부품과 같은 강체를 다루는 데 집중했다. 강체는 위치(<span class="math math-inline">x, y, z</span>)와 자세(Roll, Pitch, Yaw)라는 6개의 자유도(6-DoF)만으로 상태를 완벽하게 기술할 수 있다. 그러나 가정, 물류 창고, 의료 현장에서 로봇이 다루어야 할 대상은 옷, 수건, 케이블, 전선, 식품, 생체 조직(Organ)과 같은 변형 가능한 객체(Deformable Objects)들이다.37</p>
<h4>4.1.1  차원의 저주와 모델링 불가능성</h4>
<p>천 조각이나 밧줄과 같은 유연체는 이론적으로 무한한 자유도를 가진다. 1미터 길이의 밧줄의 형상을 완벽하게 수학적으로 표현하기 위해서는 밧줄을 구성하는 무수히 많은 입자들의 3차원 위치를 모두 정의해야 한다. 이는 유한한 데이터를 처리하는 컴퓨터에게 계산 불가능한 문제이다.39</p>
<p>로봇이 빨래를 개기 위해 구겨진 셔츠를 집어 올렸을 때, 셔츠가 중력에 의해 어떻게 축 늘어질지, 어디가 접힐지, 단추가 어디에 걸릴지를 예측하는 것은 강체 역학 시뮬레이션으로는 불가능하다. 옷의 미세한 마찰 계수, 습도, 섬유의 결, 올 풀림 정도에 따라 거동이 비선형적으로 달라지기 때문이다. 이를 모델링하기 위해 질량-스프링 모델(Mass-Spring Model)이나 유한 요소법(FEM) 등의 근사 모델이 사용되지만, 실시간 제어에 사용하기에는 계산 비용이 너무 높고, 시뮬레이션과 실제 현실 사이의 간극(Sim-to-Real Gap)이 매우 크다.40</p>
<h4>4.1.2  가려짐(Occlusion)과 은닉 상태</h4>
<p>유연체는 스스로 꼬이거나 접히면서 자신의 일부를 가리는 자기 가려짐(Self-occlusion) 현상이 빈번하게 발생한다.40 로봇의 카메라가 바닥에 놓인 뭉쳐진 옷 더미를 볼 때, 겉으로 드러난 표면 아래에 옷 소매가 있는지, 양말이 숨어 있는지 알 방법이 없다. 이는 인식의 불완전성을 심화시키며, 로봇이 옷을 물리적으로 펼치거나 뒤적거리기 전까지는 상태를 확정할 수 없는 ‘숨겨진 상태(Hidden State)’ 문제를 야기한다. 따라서 비정형 환경에서의 조작은 ‘보고 잡는(Look-and-Grasp)’ 단순한 시퀀스가 아니라, ‘잡아서 흔들고 다시 보는(Interact-and-Perceive)’ 능동적인 탐색 과정이 되어야 한다.</p>
<h4>4.1.3  차원 축소와 잠재 공간 역학</h4>
<p>이러한 무한한 자유도를 다루기 위해 현대 로봇 공학은 <strong>차원 축소(Dimensionality Reduction)</strong> 기법을 사용한다.</p>
<ul>
<li><strong>Keypoint Representation:</strong> 객체 전체를 표현하는 대신, 양 끝점과 곡률이 가장 큰 몇 개의 점(Keypoints)만을 추적하여 형상을 근사한다.41 케이블 조작의 경우 케이블의 중심선을 따라 일정 간격의 점들을 제어점으로 삼는다.</li>
<li><strong>Latent Space Dynamics:</strong> 딥러닝의 오토인코더(Autoencoder) 등을 활용하여 고차원의 이미지 데이터를 저차원의 잠재 벡터(Latent Vector, <span class="math math-inline">z</span>)로 압축하고, 이 잠재 공간상에서의 동역학(<span class="math math-inline">z_{t+1} = f(z_t, u_t)</span>)을 학습한다. 이는 비정형적인 현실 세계 데이터를 로봇이 처리 가능한 추상적인 수학적 공간으로 변환하려는 시도이다.</li>
</ul>
<h3>4.2  모델 불일치(Model Mismatch)와 두 가지 불확실성</h3>
<p>현실 세계의 물리 현상은 로봇이 내부적으로 가지고 있는 수학적 모델보다 항상 더 복잡하다. 로봇이 예측한 결과(<span class="math math-inline">\hat{y}</span>)와 실제 센서 관측(<span class="math math-inline">y</span>) 사이의 차이는 단순한 노이즈(<span class="math math-inline">v</span>)가 아니라 모델링되지 않은 동역학적 오차를 포함한다. 이를 체계적으로 다루기 위해 불확실성을 두 가지 범주로 구분해야 한다.42</p>
<table><thead><tr><th><strong>불확실성 종류</strong></th><th><strong>정의 (Definition)</strong></th><th><strong>원인 (Source)</strong></th><th><strong>해결 방안 (Mitigation)</strong></th></tr></thead><tbody>
<tr><td><strong>우연적 불확실성 (Aleatoric Uncertainty)</strong></td><td>데이터 자체에 내재된, 줄일 수 없는 무작위성</td><td>센서 열잡음, 양자적 샷 노이즈, 난류, 미세한 표면 마찰 변화</td><td>확률 분포로 모델링 (예: <span class="math math-inline">y \sim N(\mu, \sigma^2)</span>), 강건 제어(Robust Control)</td></tr>
<tr><td><strong>인식론적 불확실성 (Epistemic Uncertainty)</strong></td><td>지식의 부재로 인한, 줄일 수 있는 불확실성</td><td>학습 데이터 부족, 모델링되지 않은 물리 현상, 경험하지 못한 상황(OOD)</td><td>더 많은 데이터 수집, 모델 구조 개선, 능동적 탐색(Active Learning)</td></tr>
</tbody></table>
<h4>4.2.1  우연적 불확실성의 수용</h4>
<p>우연적 불확실성은 신(God)이 주사위를 던지는 것과 같다. 안개 낀 날씨에서 LiDAR 센서가 난반사를 일으킬 때, 이는 환경적 특성에 기인한 우연적 불확실성이다. 데이터의 양을 아무리 늘려도 안개 입자의 위치를 정확히 알 수는 없다.42 로봇은 이를 받아들이고 “전방에 장애물이 있을 확률이 70%이므로 감속한다“는 식의 보수적인 정책을 취해야 한다.</p>
<h4>4.2.2  인식론적 불확실성과 모델 적응</h4>
<p>인식론적 불확실성은 로봇의 무지(Ignorance)에서 비롯된다. 로봇이 마찰이 없는 평평한 바닥에서 움직이는 모델만 학습했다가, 털이 긴 카펫이나 자갈밭 위를 주행하게 되면 예상치 못한 미끄러짐이나 저항이 발생한다. 이때 발생하는 예측 오차는 노이즈가 아니라 로봇의 모델이 현실을 반영하지 못함(Model Mismatch)을 의미한다.44</p>
<p>이 경우, 로봇은 자신의 예측이 틀렸음을 인지하고(높은 인식론적 불확실성 추정), 해당 환경에서의 주행 데이터를 수집하여 모델을 업데이트하거나(Online Adaptation), 조심스럽게 행동하며 정보를 모으는 전략을 취해야 한다. 딥러닝 기반의 로봇 제어에서는 앙상블(Ensemble) 모델이나 베이지안 뉴럴 네트워크(BNN)를 통해 이러한 인식론적 불확실성을 정량화하고, 낯선 환경(Out-of-Distribution)에서의 오작동을 방지한다.42</p>
<h3>4.3  동적 환경과 예측 불가능한 에이전트</h3>
<p>마지막으로, 비정형 환경의 가장 큰 변수는 ’인간’과 같은 동적 에이전트이다.46 정적인 장애물은 지도에 기록하고 회피 경로를 생성하면 되지만, 움직이는 사람이나 차량은 시간에 따라 상태가 변하며 의도(Intent)를 가진다.</p>
<ul>
<li><strong>의도의 모호성:</strong> 보행자가 횡단보도 앞에 서 있을 때, 지금 건너려는 것인지, 택시를 기다리는 것인지, 아니면 스마트폰을 보느라 멈춘 것인지 로봇은 알 수 없다. 이는 단순한 물리적 상태 추적(Tracking)을 넘어 심리적, 사회적 맥락 추론(Social Context Inference)을 요구한다.</li>
<li><strong>비강체 객체로서의 인간:</strong> 인간은 형태가 끊임없이 변하는 비강체이다. 걷거나 뛸 때 팔다리의 모양이 변하고, 옷차림에 따라 외형이 달라진다. 로봇의 인식 알고리즘이 인간을 단일한 강체 박스(Bounding Box)로 단순화하여 추적할 경우, 팔을 뻗는 동작을 로봇 쪽으로 이동하는 것으로 오인하여 급정거하는 등의 오류를 범할 수 있다.48</li>
<li><strong>상호작용적 예측(Interactive Prediction):</strong> 로봇의 행동은 인간의 행동에 영향을 미친다. 로봇이 양보하려고 멈추면 인간도 멈칫하고, 로봇이 지나가려 하면 인간도 피하려다 충돌하는 ’교착 상태(Freezing Robot Problem)’가 발생할 수 있다. 따라서 비정형 환경에서의 로봇은 자신을 포함한 환경 전체의 동역학을 게임 이론(Game Theory)적 관점에서 예측해야 한다.</li>
</ul>
<h2>5. 결론: 불확실성을 안고 살아가는 로봇</h2>
<p>비정형 환경과 인식의 모호성, 그리고 무한한 변수들은 로봇 공학자들에게 겸손을 가르쳤다. 우리는 더 이상 현실 세계를 완벽하게 기술하는 <span class="math math-inline">y = h(x)</span>라는 깔끔한 수식을 찾을 수 없음을 인정해야 한다. 센서의 물리적 한계로 인한 노이즈, 기하학적 유사성으로 인한 에일리어싱, 그리고 무한한 자유도를 가진 현실 세계의 복잡성은 로봇이 확정적인 진리(Truth)에 도달하는 것을 원천적으로 차단한다.</p>
<p>그러나 이러한 한계의 인식은 패배가 아니라 새로운 패러다임, 즉 **확률론적 로봇 공학(Probabilistic Robotics)**의 시작점이 되었다. 현대의 로봇은 “나는 정확히 X 좌표에 있다“라고 말하지 않는다. 대신 “나는 확률적으로 이 영역 어딘가에 분포해 있으며, 내 센서 데이터의 신뢰도는 이 정도이다“라고 말한다. 로봇은 센서 노이즈를 제거해야 할 오류가 아니라, 데이터가 가진 정보량의 척도로 해석하게 되었다.</p>
<p>비정형 환경에서의 생존 전략은 다음과 같이 요약된다:</p>
<ol>
<li><strong>센서 융합(Sensor Fusion):</strong> 카메라, LiDAR, IMU가 가진 서로 다른 물리적 특성과 노이즈 패턴(빛, 반사율, 관성)을 상호 보완적으로 활용하여 단일 센서의 취약점을 극복한다.</li>
<li><strong>확률적 필터링(Probabilistic Filtering):</strong> 베이즈 필터, 파티클 필터, POMDP와 같은 수학적 도구를 통해 불확실성을 정량화하고, 시간의 흐름에 따른 데이터 축적을 통해 신뢰도를 높인다.</li>
<li><strong>데이터 기반 적응(Data-Driven Adaptation):</strong> 수식으로 정의하기 힘든 복잡한 변수(유연체, 비선형 노이즈)는 대량의 데이터를 통한 학습(Deep Learning)으로 근사하고, 실시간으로 모델을 수정해 나간다.</li>
</ol>
<p>결국 “2.2.2 비정형 환경과 인식의 모호성“이라는 주제는 로봇이 실험실의 온실을 벗어나 거친 야생으로 나아가는 과정에서 겪는 필연적인 성장통이자, 지능(Intelligence)이 왜 필요한지에 대한 근본적인 대답이다. 불확실성을 인지하고, 관리하며, 그 속에서 최선의(Optimal) 결정을 내리는 능력이야말로 진정한 자율성(Autonomy)의 핵심이기 때문이다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>What is the difference between structured and unstructured environments in robotics? - Zilliz, https://zilliz.com/ai-faq/what-is-the-difference-between-structured-and-unstructured-environments-in-robotics</li>
<li>The Future of Robotics in Unstructured Environments - SK Godelius, https://godelius.com/en/the-future-of-robotics-in-unstructured-environments/</li>
<li>What is the difference between structured and unstructured environments in robotics?, https://milvus.io/ai-quick-reference/what-is-the-difference-between-structured-and-unstructured-environments-in-robotics</li>
<li>The Rise of Autonomous Industrial Robotics: From Structured to Unstructured Environments, https://www.inbolt.com/resources/the-rise-of-autonomous-industrial-robotics-from-structured-to-unstructured-environments</li>
<li>Thorlabs · Camera Noise and Temperature Tutorial, https://www.thorlabs.com/camera-noise-and-temperature-tutorial</li>
<li>NOISE ANALYSIS IN CMOS IMAGE SENSORS - Stanford University, https://isl.stanford.edu/~abbas/group/papers_and_pub/hui_thesis.pdf</li>
<li>Shot-Noise in optical detectors - Physics Stack Exchange, https://physics.stackexchange.com/questions/484977/shot-noise-in-optical-detectors</li>
<li>Image noise - Wikipedia, https://en.wikipedia.org/wiki/Image_noise</li>
<li>CCD Noise Sources and Signal-to-Noise Ratio - Evident Scientific, https://evidentscientific.com/en/microscope-resource/knowledge-hub/digital-imaging/concepts/ccdsnr</li>
<li>Dark current (physics) - Wikipedia, https://en.wikipedia.org/wiki/Dark_current_(physics)</li>
<li>A Deep Dive into Types of Camera Noise, and Their Impact on Image Quality, https://www.e-consystems.com/blog/camera/technology/a-deep-dive-into-types-of-camera-noise-and-their-impact-on-image-quality/</li>
<li>Noise Reduction Techniques and Scaling Effects towards Photon Counting CMOS Image Sensors - MDPI, https://www.mdpi.com/1424-8220/16/4/514</li>
<li>Point Cloud Defects That 99% of LiDAR Suppliers Won’t Let You …, https://www.robosense.ai/en/tech-show-55</li>
<li>Detection and Utilization of Reflections in LiDAR Scans through Plane Optimization and Plane SLAM - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC11314935/</li>
<li>Reflective Noise Filtering of Large-Scale Point Cloud Using Multi-Position LiDAR Sensing Data - MDPI, https://www.mdpi.com/2072-4292/13/16/3058</li>
<li>DLP® DMD Technology: LIDAR ambient light reduction - Texas Instruments, https://www.ti.com/lit/pdf/dlpa093</li>
<li>Camera, LiDAR, and IMU Spatiotemporal Calibration: Methodological Review and Research Perspectives - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC12431046/</li>
<li>Deep Learning for Inertial Positioning: A Survey - arXiv, https://arxiv.org/html/2303.03757v3</li>
<li>Characterization of Errors and Noises in MEMS Inertial Sensors Using Allan Variance Method - UPCommons, https://upcommons.upc.edu/server/api/core/bitstreams/9872d2b7-3939-41f8-8bbf-7709b3632f3a/content</li>
<li>Understanding How Random Walk in IMU sensors | IMU Noise, https://daischsensor.com/understanding-how-random-walk-in-imu-sensors/</li>
<li>Tangram Vision Blog | IMU Fundamentals, Part 3: Stochastic Error Modeling, https://www.tangramvision.com/blog/stochastic-imu-error-modeling</li>
<li>Perceptual aliasing. Two distant locations in a building share similar… - ResearchGate, https://www.researchgate.net/figure/Perceptual-aliasing-Two-distant-locations-in-a-building-share-similar-visual-appearance_fig1_321815533</li>
<li>Modeling Perceptual Aliasing in SLAM via Discrete-Continuous Graphical Models - arXiv, https://arxiv.org/pdf/1810.11692</li>
<li>Robot Localization and Kalman Filters - Rudy Negenborn, http://www.negenborn.net/kal_loc/thesis.pdf</li>
<li>Multiple Model Kalman Filters: A Localization Technique for RoboCup Soccer - UT Austin Computer Science, https://www.cs.utexas.edu/~pstone/Courses/393Rfall11/resources/RC09-Quinlan.pdf</li>
<li>Multiple Model Kalman Filters: A Localization Technique for RoboCup Soccer, https://www.researchgate.net/publication/220797189_Multiple_Model_Kalman_Filters_A_Localization_Technique_for_RoboCup_Soccer</li>
<li>Publications learning to perceive and act by trial and error, https://www.cs.utexas.edu/~shivaram/readings/b2hd-WhiteheadBallard1991.html</li>
<li>[PDF] Learning to perceive and act by trial and error - Semantic Scholar, https://www.semanticscholar.org/paper/Learning-to-perceive-and-act-by-trial-and-error-Whitehead-Ballard/6104d0e68c7f67da58b2f84a663df45d82d86b18</li>
<li>[2511.01219] Tackling the Kidnapped Robot Problem via Sparse Feasible Hypothesis Sampling and Reliable Batched Multi-Stage Inference - arXiv, https://arxiv.org/abs/2511.01219</li>
<li>Detecting and Solving the Kidnapped Robot Problem Using Laser Range Finder and Wifi Signal, https://www.robot.t.u-tokyo.ac.jp/~yamashita/paper/B/B181Final.pdf</li>
<li>A High-Order Kalman Filter Method for Fusion Estimation of Motion Trajectories of Multi-Robot Formation - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC9371216/</li>
<li>Decentralized control of multi-robot systems using partially observable Markov Decision Processes and belief space macro-actions - DSpace@MIT, https://dspace.mit.edu/handle/1721.1/101447</li>
<li>(PDF) Pomdp: A Computational Infrastructure for Partially Observable Markov Decision Processes - ResearchGate, https://www.researchgate.net/publication/392139148_Pomdp_A_Computational_Infrastructure_for_Partially_Observable_Markov_Decision_Processes</li>
<li>Planning and acting in partially observable stochastic domains - People, https://people.csail.mit.edu/lpk/papers/aij98-pomdp.pdf</li>
<li>View of Online Planning Algorithms for POMDPs - Journal of Artificial Intelligence Research, https://jair.org/index.php/jair/article/view/10559/25275</li>
<li>1994-Acting Optimally in Partially Observable Stochastic Domain - Association for the Advancement of Artificial Intelligence (AAAI), https://cdn.aaai.org/AAAI/1994/AAAI94-157.pdf</li>
<li>Modeling of Deformable Objects for Robotic Manipulation: A Tutorial and Review - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2020.00082/full</li>
<li>Model-based Manipulation of Deformable Objects with Non-negligible Dynamics as Shape Regulation - arXiv, https://arxiv.org/html/2402.16114v1</li>
<li>Robot Learning for Deformable Object Manipulation Tasks - research.chalmers.se, https://research.chalmers.se/publication/540359/file/540359_Fulltext.pdf</li>
<li>Data-Driven Robotic Manipulation of Cloth-like Deformable Objects: The Present, Challenges and Future Prospects - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC10007406/</li>
<li>Manipulating deformable objects by learning-based representations, planning and control, https://ira.lib.polyu.edu.hk/handle/10397/114245</li>
<li>Estimating Epistemic and Aleatoric Uncertainty with a Single Model - NIPS papers, https://proceedings.neurips.cc/paper_files/paper/2024/file/c693c3ff83259aebcd55a41ab19a5d84-Paper-Conference.pdf</li>
<li>Lecture 14: Aleatoric vs. Epistemic Uncertainty, https://www.seas.upenn.edu/~obastani/cis7000/spring2024/docs/lecture14.pdf</li>
<li>Quantifying Aleatoric and Epistemic Dynamics Uncertainty via Local Conformal Calibration, https://arxiv.org/html/2409.08249v1</li>
<li>Quantifying Aleatoric and Epistemic Dynamics Uncertainty via Local Conformal Calibration, https://um-arm-lab.github.io/lucca/</li>
<li>Towards Robust Robot 3D Perception in Urban Environments: The UT Campus Object Dataset - ResearchGate, https://www.researchgate.net/publication/380579099_Towards_Robust_Robot_3D_Perception_in_Urban_Environments_The_UT_Campus_Object_Dataset</li>
<li>AI-Driven Robotics: Innovations in Design, Perception, and Decision-Making - MDPI, https://www.mdpi.com/2075-1702/13/7/615</li>
<li>Perceptual Biases in the Interpretation of Non-Rigid Shape Transformations from Motion - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC11270375/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>