<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2.5.2 일반화(Generalization) 능력: 본 적 없는 환경에서의 적응</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2.5.2 일반화(Generalization) 능력: 본 적 없는 환경에서의 적응</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 2. 제어 이론의 역사와 소프트웨어 2.0</a> / <a href="index.html">2.5 로봇 지능의 새로운 지평: Solver에서 Learner로</a> / <span>2.5.2 일반화(Generalization) 능력: 본 적 없는 환경에서의 적응</span></nav>
                </div>
            </header>
            <article>
                <h1>2.5.2 일반화(Generalization) 능력: 본 적 없는 환경에서의 적응</h1>
<h2>1.  서론: 로봇 공학에서의 일반화의 위기(Crisis)와 재정의</h2>
<p>현대 로봇 공학이 직면한 가장 심오한 도전 과제는 ’통제된 실험실’과 ‘비구조화된 현실 세계’ 사이의 거대한 간극을 메우는 것이다. 지난 반세기 동안 고전적 제어 이론(Classical Control Theory)은 시스템의 역학(Dynamics)을 미분 방정식으로 기술하고, 예측 가능한 외란(Disturbance)의 범위 내에서 시스템의 안정성(Stability)을 보장하는 ’강건성(Robustness)’을 확보하는 데 주력해 왔다.1 이러한 접근법은 공장의 조립 라인과 같이 환경 변수가 엄격히 통제되는 상황에서는 탁월한 성능을 발휘했으나, 가정이나 재난 현장, 혹은 인간과 공존하는 복잡한 사회적 공간과 같이 모든 변수를 사전에 모델링하는 것이 불가능한 ’개방형 환경(Open-ended Environment)’에서는 그 한계를 여실히 드러냈다. 로봇이 한 번도 본 적 없는 물체를 조작하거나, 미끄러운 빙판길이나 푹신한 모래사장처럼 급격히 변화하는 지면 위를 걸어야 할 때, 기존의 ‘해석기(Solver)’ 기반 패러다임은 ’모델링 병목(Modeling Bottleneck)’이라는 벽에 부딪힌다.2</p>
<p>이 지점에서 안드레 카패시(Andrej Karpathy)가 주창한 ‘Software 2.0’ 패러다임은 로봇 제어 시스템의 설계 철학에 근본적인 전환을 요구한다.4 Software 1.0이 인간 프로그래머가 명시적인 규칙과 논리를 코드로 작성하는 것이라면, Software 2.0은 개발자가 바람직한 동작의 입출력 데이터(Dataset)를 정의하고, 신경망(Neural Network)이 그 동작을 수행하는 최적의 프로그램(가중치)을 탐색하도록 하는 방식이다. 이 새로운 패러다임 하에서 로봇의 ’지능’은 알고리즘의 정교함보다는 데이터의 질과 양, 그리고 다양성에 의해 결정되며, ’일반화(Generalization)’의 정의 또한 ’설계된 외란에 대한 저항력’에서 ’훈련 분포(Training Distribution)를 벗어난 새로운 환경(Out-of-Distribution, OOD)에 대한 적응력’으로 확장된다.6</p>
<p>본 장에서는 로봇 학습의 성배(Holy Grail)라 불리는 일반화 능력을 확보하기 위한 최신 연구 흐름을 ‘물리적 신체(Body)’, ‘시각적 인지(Eyes)’, 그리고 ’인지적 추론(Brain)’의 세 가지 층위에서 심층적으로 분석한다. 물리적 역학의 변화에 적응하는 RMA(Rapid Motor Adaptation)와 같은 적응 제어 기법부터, 인터넷 스케일의 지식을 로봇 제어에 전이하는 VLA(Vision-Language-Action) 모델, 그리고 직관적 반응(System 1)과 논리적 추론(System 2)을 통합하려는 인지적 아키텍처에 이르기까지, 로봇이 본 적 없는 세상에 적응해 나가는 메커니즘을 총체적으로 조망할 것이다.</p>
<h2>2.  Software 2.0와 로봇 제어 패러다임의 전환</h2>
<p>로봇 제어 시스템의 발전사는 ’명시적 모델링’에서 ’데이터 기반 학습’으로의 이행 과정이라 할 수 있다. 이 변화의 핵심에는 소프트웨어를 바라보는 관점의 변화, 즉 Software 2.0이 자리 잡고 있다. 이 패러다임 전환이 로봇 공학, 특히 제어와 인지 시스템에 미치는 영향은 실로 지대하다.</p>
<h3>2.1 명시적 해석기(Solver)에서 학습기(Learner)로</h3>
<p>전통적인 로봇 제어, 즉 ‘Solver’ 패러다임은 세상을 완벽하게 이해하고 기술할 수 있다는 가정에서 출발한다. 로봇의 기구학(Kinematics)과 동역학(Dynamics)은 수식으로 표현되며, 환경에 대한 정보(지도, 물체의 위치 등)는 센서를 통해 상태(State)로 추정된다. 제어기는 이 모델을 바탕으로 최적화 문제(Optimization Problem)를 풀어 제어 명령을 생성한다.2 그러나 이 접근법은 비구조화된 환경에서 치명적인 약점을 노출한다.</p>
<ol>
<li><strong>모델링 병목(Modeling Bottleneck):</strong> 현실 세계의 물리 현상은 수식으로 완벽하게 담아내기에는 너무나 복잡하다. 유체 역학이 지배하는 수중 환경에서의 조작, 유연한 물체(Deformable Object)인 옷감을 개는 작업, 혹은 마찰 계수가 시시각각 변하는 험지 보행 등은 정확한 모델링이 사실상 불가능하거나, 실시간 제어에 사용하기에는 계산 비용이 너무 높다.3</li>
<li><strong>취약한 상태 추정(Brittle State Estimation):</strong> 전통적인 SLAM(Simultaneous Localization and Mapping)이나 물체 인식 알고리즘은 조명의 변화, 텍스처가 없는 표면, 혹은 동적인 물체의 개입과 같은 시각적 악조건 하에서 쉽게 실패한다.11 명시적 모델은 입력된 상태 정보가 정확하다는 전제하에 작동하므로, 인지 모듈의 작은 오류가 제어 전체의 실패(Catastrophic Failure)로 이어지기 쉽다.</li>
</ol>
<p>이에 반해 ‘Learner’ 패러다임, 즉 Software 2.0 접근법은 로봇이 환경과 상호작용하며 축적한 데이터로부터 제어 법칙을 스스로 학습한다. 이는 모델의 불확실성을 명시적으로 계산하는 대신, 방대한 데이터를 통해 다양한 상황에 대한 대응책을 신경망 내부에 내재화(Implicitly Internalize)하는 방식이다.13</p>
<table><thead><tr><th><strong>특성</strong></th><th><strong>Software 1.0 (Solver Paradigm)</strong></th><th><strong>Software 2.0 (Learner Paradigm)</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 자원</strong></td><td>도메인 전문가의 지식, 정교한 알고리즘</td><td>고품질의 대규모 데이터셋, 연산 자원(GPU)</td></tr>
<tr><td><strong>개발 방식</strong></td><td>C++/Python 코딩, 명시적 규칙 작성</td><td>데이터 수집/정제, 신경망 아키텍처 설계, 학습</td></tr>
<tr><td><strong>환경 모델링</strong></td><td>미분 방정식, 물리 법칙 기반의 명시적 모델</td><td>신경망 가중치에 내재된 암시적 모델</td></tr>
<tr><td><strong>일반화 전략</strong></td><td>강건 제어(Robust Control), 예외 처리(Exception Handling)</td><td>도메인 랜덤화, 메타 러닝, 파운데이션 모델 전이</td></tr>
<tr><td><strong>한계점</strong></td><td>복잡한 비선형 시스템 모델링의 어려움, 유연성 부족</td><td>데이터 편향, 해석 가능성(Interpretability) 부족, 안전성 검증 난해</td></tr>
</tbody></table>
<h3>2.2 강건성(Robustness)과 일반화(Generalization)의 이론적 함의</h3>
<p>로봇 학습 이론에서 강건성과 일반화는 밀접하게 연관되어 있으면서도 구별되는 개념이다. Xu와 Mannor(2012)의 연구에 따르면, 학습 알고리즘의 강건성—즉, 훈련 샘플과 ‘유사한’ 테스트 샘플에 대해 오차가 크지 않다는 속성—은 일반화를 위한 필요충분조건이다.6 이는 기하학적인 직관을 제공하는데, 데이터 공간상에서 훈련 데이터 주변의 작은 섭동(Perturbation)에 대해 출력이 안정적이라면, 그 알고리즘은 보지 못한 데이터에 대해서도 일반화될 가능성이 높다는 것이다.</p>
<p>그러나 현대 로봇 공학이 추구하는 일반화는 단순히 훈련 데이터 분포 내(In-Distribution)에서의 보간(Interpolation)을 넘어선다. 그것은 훈련 데이터 분포 밖(Out-of-Distribution)의 완전히 새로운 환경—예를 들어, 실험실 바닥에서 훈련된 로봇이 숲속의 진흙탕 길을 걷거나, 공장 부품만 다루던 로봇이 가정 내의 다양한 식기류를 다루는 것—에서의 적응을 포함한다.15 이를 위해 연구자들은 단순히 모델의 복잡도(VC Dimension)를 조절하는 것을 넘어, 훈련 환경 자체의 다양성을 극대화하거나(Domain Randomization), 적응 메커니즘 자체를 학습시키는(Meta-Learning) 등의 적극적인 개입을 시도하고 있다.</p>
<h2>3.  물리적 신체의 적응: Sim-to-Real과 동적 파라미터 변화</h2>
<p>로봇이 시뮬레이션 환경(Sim)에서 학습한 정책을 실제 물리 세계(Real)에 배포할 때 발생하는 성능 저하 현상인 ’Sim-to-Real Gap’은 물리적 일반화가 실패하는 가장 대표적인 사례이다. 시뮬레이터는 현실의 마찰, 접촉 역학(Contact Dynamics), 센서 노이즈, 액추에이터의 지연 등을 완벽하게 모사할 수 없기 때문이다.17 이 간극을 극복하고 로봇이 본 적 없는 물리적 환경에 적응하도록 만드는 기술은 크게 도메인 랜덤화와 온라인 시스템 식별 및 적응 기법으로 나뉜다.</p>
<h3>3.1 도메인 랜덤화(Domain Randomization): 불확실성의 포용</h3>
<p>도메인 랜덤화(DR)는 시뮬레이션 환경의 물리적, 시각적 파라미터를 매 에피소드마다 무작위로 변화시켜 학습하는 기법이다.19 이 방법론의 기저는 로봇에게 가능한 한 많은 ’다양한 세상’을 경험하게 함으로써, 실제 현실 세계를 그 수많은 시뮬레이션 환경 중 하나로 인식하게 만드는 것이다.</p>
<ol>
<li><strong>파라미터 공간의 확장:</strong> DR은 마찰 계수, 링크의 질량, 모터의 댐핑 계수, 지면의 경사도, 센서 노이즈 레벨 등을 광범위한 분포에서 샘플링한다. 이를 통해 학습된 정책은 특정 물리 파라미터 조합에 과적합(Overfitting)되지 않고, 다양한 역학적 특성을 아우르는 보수적이지만 강건한(Conservative but Robust) 행동 전략을 수립하게 된다.</li>
<li><strong>구조적 도메인 랜덤화(Structured Domain Randomization, SDR):</strong> 초기의 단순한 랜덤화는 비현실적인 환경(예: 공중에 떠 있는 벽, 물리적으로 불가능한 마찰력)을 생성하여 학습 효율을 저해하는 부작용이 있었다. 이에 대한 대안으로 SDR은 실제 세계의 구조적 제약 조건(예: 중력 방향, 물체 간의 포함 관계)과 문맥을 반영하여 파라미터를 랜덤화한다.21 이는 로봇이 현실 세계의 인과 관계와 구조를 학습하는 데 도움을 주며, 단순 랜덤화보다 높은 Sim-to-Real 성공률을 보장한다.</li>
<li><strong>한계와 최적성의 트레이드오프:</strong> DR은 기본적으로 ‘평균적인’ 성능을 최적화하거나 ’최악의 경우’를 방어하는 전략을 학습하게 만든다. 따라서 특정 환경(예: 마찰력이 매우 낮은 빙판)에 특화된 최적의 성능(Optimality)을 발휘하기보다는, 모든 환경에서 ‘실패하지 않는’ 수준의 성능에 머무르는 경향이 있다.22 이는 안정성을 위해 성능을 희생하는 결과를 낳을 수 있다.</li>
</ol>
<h3>3.2 RMA (Rapid Motor Adaptation): 암시적 시스템 식별의 혁명</h3>
<p>도메인 랜덤화의 한계, 즉 ’평균적인 정책’의 한계를 극복하고 실시간으로 변화하는 환경에 즉각적으로 적응하기 위해 제안된 획기적인 방법론이 **RMA(Rapid Motor Adaptation)**이다.23 UC 버클리와 페이스북 AI 리서치(FAIR)가 제안한 이 기법은 보행 로봇(Legged Robot)이 미끄러운 얼음판, 푹신한 모래사장, 무거운 짐을 진 상태 등 본 적 없는 환경에서 1초 미만의 짧은 시간 안에 적응하여 안정적으로 걷는 것을 가능하게 했다.</p>
<h4>3.2.1 RMA의 이중 아키텍처 및 학습 메커니즘</h4>
<p>RMA는 두 개의 비동기적인 프로세스, 즉 기본 정책(Base Policy)과 적응 모듈(Adaptation Module)로 구성되며, 이들의 유기적인 결합을 통해 작동한다.23</p>
<ol>
<li><strong>위상 1: 전문가 기본 정책(Base Policy, <span class="math math-inline">\pi</span>)의 학습</strong></li>
</ol>
<ul>
<li>이 단계는 시뮬레이션 내에서 수행된다. 강화학습 에이전트는 로봇의 상태(관절 각도, 속도 등)뿐만 아니라, 시뮬레이터가 제공하는 **특권 정보(Privileged Information)**인 환경 잠재 벡터(Extrinsics Vector, <span class="math math-inline">z_t</span>)를 입력받는다.</li>
<li><span class="math math-inline">z_t</span>는 마찰력, 질량, 지형의 높낮이 등 환경의 물리적 파라미터를 인코딩한 값이다. 에이전트는 이 정보를 바탕으로 해당 환경에 최적화된 행동을 생성하도록 학습된다. 즉, “지금 바닥이 미끄러우니(<span class="math math-inline">z_t</span>), 발을 조심스럽게 디뎌라(<span class="math math-inline">a_t</span>)“와 같은 매핑을 익히는 것이다.</li>
<li>이때 사용되는 보상 함수(Reward Function)는 전진 속도, 에너지 효율성, 동작의 부드러움, 지면 충격 최소화 등 다양한 항으로 구성되어 자연스러운 보행을 유도한다.26</li>
</ul>
<ol start="2">
<li><strong>위상 2: 적응 모듈(Adaptation Module, <span class="math math-inline">\phi</span>)의 지도 학습</strong></li>
</ol>
<ul>
<li>실제 세계에서는 마찰력이나 질량을 직접 측정할 수 없으므로, <span class="math math-inline">z_t</span>를 사용할 수 없다. 적응 모듈 <span class="math math-inline">\phi</span>는 이를 해결하기 위해 로봇의 **고유감각 이력(Proprioceptive History)**을 활용한다.</li>
<li>적응 모듈은 최근 수 초간의 로봇 상태(<span class="math math-inline">x_{t-k:t-1}</span>)와 행동(<span class="math math-inline">a_{t-k:t-1}</span>)의 시계열 데이터를 입력받아, 현재의 환경 잠재 벡터 <span class="math math-inline">\hat{z}_t</span>를 추정(Estimate)하도록 훈련된다.</li>
<li>이는 시뮬레이션에서 생성된 참값 <span class="math math-inline">z_t</span>를 정답(Ground Truth)으로 하는 지도 학습(Supervised Learning) 문제로 정식화된다. 즉, 모듈은 “지난 1초간 발이 이렇게 미끄러지고 모터에 부하가 이렇게 걸린 것을 보니, 지금 환경은 <span class="math math-inline">z_t</span>와 유사하다“라고 역추론(Inverse Inference)하는 법을 배운다.</li>
</ul>
<ol start="3">
<li><strong>위상 3: 실전 배포 및 적응</strong></li>
</ol>
<ul>
<li>실제 로봇에 배포될 때는 기본 정책 <span class="math math-inline">\pi</span>와 학습된 적응 모듈 <span class="math math-inline">\phi</span>가 탑재된다.</li>
<li>로봇이 낯선 환경(예: 젖은 풀밭)에 진입하면, 초기에는 약간의 불안정한 움직임을 보일 수 있다. 그러나 적응 모듈은 이 움직임 데이터(History)를 실시간으로 분석하여 그 환경에 맞는 <span class="math math-inline">\hat{z}_t</span>를 즉시 생성해낸다.</li>
<li>기본 정책 <span class="math math-inline">\pi</span>는 이 <span class="math math-inline">\hat{z}_t</span>를 입력받아 즉시 보행 전략을 수정한다. 이 모든 과정이 수십 밀리초(ms) 이내에 이루어지므로, 외부 관찰자에게는 로봇이 환경 변화에 즉각적으로 적응하는 것처럼 보인다.27</li>
</ul>
<h4>3.2.2 RMA의 시사점과 파급 효과</h4>
<p>RMA의 성공은 로봇 제어에서 <strong>’표현 학습(Representation Learning)’이 ’명시적 시스템 식별(Explicit System Identification)’을 대체할 수 있음</strong>을 강력하게 시사한다.23 로봇은 “마찰계수가 0.3이다“라는 물리적 수치를 추정하는 것이 아니라, “지금 내 움직임 패턴이 미끄러운 상황의 패턴과 유사하다“는 암시적인 물리 감각을 학습한다. 이 방식은 모델링하기 어려운 복잡한 상호작용(예: 발과 모래의 상호작용)까지도 데이터 기반으로 포착할 수 있어, 기존의 적응 제어보다 월등히 높은 일반화 성능을 보여준다.</p>
<h3>3.3 메타 러닝(Meta-Learning)과의 비교: 적응의 속도와 방식</h3>
<p>일반화를 위한 또 다른 접근법인 메타 러닝(예: MAML)은 “학습하는 법을 학습(Learning to Learn)“하는 것을 목표로 한다.19 메타 러닝은 새로운 태스크나 환경을 만났을 때, 소량의 데이터(Few-shot)와 몇 번의 그래디언트 업데이트(Gradient Update)만으로 빠르게 최적의 정책으로 수렴할 수 있는 초기 가중치(Meta-initialization)를 찾는 데 집중한다.</p>
<ul>
<li><strong>RMA vs. MAML:</strong> RMA는 정책의 가중치를 업데이트하지 않고, 입력(Latent Vector)을 변화시켜 행동을 조절하는 <strong>문맥 기반(Context-based)</strong> 적응 방식이다. 반면 MAML은 정책 신경망의 가중치 자체를 업데이트하는 <strong>그래디언트 기반(Gradient-based)</strong> 적응 방식이다.</li>
<li><strong>로봇 제어에서의 우위:</strong> 넘어지기 직전의 보행 로봇과 같이 실시간성이 중요한 제어 문제에서는, 그래디언트 계산과 역전파(Backpropagation)가 필요한 MAML보다는 순전파(Forward Pass)만으로 적응 벡터를 생성하는 RMA와 같은 방식이 계산 효율성과 반응 속도 면에서 훨씬 유리하다.22 그러나 새로운 기술 습득(예: 문을 여는 새로운 방법 학습)과 같은 장기적인 적응에는 메타 러닝이 더 적합할 수 있다.</li>
</ul>
<h2>4.  시각 및 의미론적 적응: 개방형 어휘(Open-Vocabulary)와 VLA 모델</h2>
<p>물리적 적응이 로봇의 ’몸’을 제어하는 문제라면, 시각 및 의미론적 적응은 로봇이 ’무엇’을 보고 조작하는지를 이해하는 문제이다. 과거의 로봇은 “빨간 컵”, “파란 공“과 같이 사전에 정의된 소수의 물체(Closed-set)만을 인식할 수 있었다. 그러나 일반화된 로봇은 “내가 어제 먹다 남은 과자 봉지“나 “반짝이는 금속 물체“와 같이 훈련 데이터에 없던 표현이나 물체(Open-set)도 처리할 수 있어야 한다.31</p>
<h3>4.1 CLIP과 개방형 어휘(Open-Vocabulary) 혁명</h3>
<p>이러한 의미론적 일반화의 기저에는 OpenAI의 CLIP(Contrastive Language-Image Pre-training)과 같은 대규모 비전-언어 모델(VLM)이 있다.33 CLIP은 인터넷상의 수십억 개의 이미지-텍스트 쌍을 대조 학습(Contrastive Learning)하여, 시각적 특징과 언어적 의미를 동일한 임베딩 공간(Joint Embedding Space)에 정렬시킨다.</p>
<ul>
<li><strong>정렬(Alignment)의 힘:</strong> 로봇이 “아보카도 모양의 의자“를 본 적이 없더라도, CLIP은 텍스트 “아보카도 의자“와 해당 이미지의 임베딩 벡터를 가깝게 위치시킨다. 이를 통해 로봇은 별도의 재학습 없이도(Zero-shot) 자연어 명령을 통해 새로운 물체를 인식하고 분류할 수 있게 되었다.34</li>
</ul>
<h3>4.2 VLA (Vision-Language-Action) 모델: 인지와 행동의 통합</h3>
<p>최근 연구는 VLM의 인지 능력을 로봇의 행동 생성과 직접 연결하는 <strong>VLA(Vision-Language-Action)</strong> 모델로 진화하고 있다.36 VLA 모델은 시각 정보(이미지)와 언어 명령(텍스트)을 입력받아, 이를 수행하기 위한 로봇의 행동(Action)을 직접 출력하는 ‘End-to-End’ 파운데이션 모델이다.</p>
<h4>4.2.1 RT-2 (Robotic Transformer 2): 언어로서의 행동</h4>
<p>구글 딥마인드의 RT-2는 VLA 모델의 효시격인 모델로, 거대 언어 모델(LLM)의 추론 능력을 로봇 제어에 전이하는 메커니즘을 명확히 보여준다.38</p>
<ol>
<li><strong>행동 토큰화(Action Tokenization):</strong> RT-2의 가장 큰 특징은 로봇의 연속적인 행동(팔의 관절 각도, 그리퍼의 개폐 등)을 텍스트와 동일한 형태의 **이산적인 토큰(Discrete Tokens)**으로 변환한다는 점이다. 예를 들어, 로봇 팔을 특정 위치로 이동시키는 명령은 “1 128 124…“와 같은 숫자 토큰 시퀀스로 표현된다.</li>
<li><strong>공동 학습(Co-training):</strong> RT-2는 인터넷의 방대한 비전-언어 데이터(VQA, 캡션 등)와 로봇의 행동 데이터(이미지-명령-행동)를 함께 학습한다. 이를 통해 모델은 인터넷 데이터에서 배운 ‘상식’(예: 쓰러진 병을 세우는 법, 썩은 사과와 신선한 사과의 차이)을 로봇의 조작 작업에 적용할 수 있게 된다.</li>
<li><strong>일반화의 발현:</strong> RT-2는 훈련 데이터에 없던 “아이언맨 피규어를 치워라“라는 명령을 받으면, 인터넷 데이터에서 학습한 아이언맨의 시각적 특징과 ’치우다(Move away)’라는 동사의 의미를 결합하여 적절한 행동 토큰을 생성해낸다. 이는 로봇 데이터만으로는 불가능했던 일반화 능력이 VLM의 지식 전이를 통해 가능해졌음을 의미한다.41</li>
</ol>
<h4>4.2.2 OpenVLA와 ObjectVLA: 모방 학습의 한계 극복</h4>
<p>RT-2 이후 등장한 오픈 소스 기반의 VLA 모델들은 기존 모방 학습(Imitation Learning)의 한계, 즉 훈련된 물체 외에는 조작하지 못하는 문제를 해결하는 데 집중하고 있다.</p>
<ul>
<li><strong>OpenVLA:</strong> Llama와 같은 오픈형 LLM과 DINOv2/SigLIP과 같은 고성능 비전 인코더를 결합하여, 다양한 로봇 데이터셋(Open X-Embodiment)으로 학습된 범용 모델이다. 효율적인 파인튜닝(LoRA 등)을 지원하여 새로운 태스크에 빠르게 적응할 수 있다.42</li>
<li><strong>ObjectVLA:</strong> 기존 VLA 모델들은 여전히 로봇 데모 데이터에 포함된 물체에 편향되는 경향이 있었다. ObjectVLA는 이를 극복하기 위해, 로봇 행동 데이터뿐만 아니라 물체의 위치 정보(Bounding Box)가 포함된 대규모 이미지-텍스트 데이터셋을 함께 학습한다. 이를 통해 시각적 특징과 물체의 의미론적 정보를 더욱 강력하게 연결(Grounding)하며, 데모 데이터 없이도(Zero-shot) 완전히 새로운 물체를 인식하고 조작하는 놀라운 능력을 보여주었다.32</li>
</ul>
<h3>4.3 MOO (Manipulation of Open-World Objects): 모듈형 일반화</h3>
<p>VLA 모델과 같이 거대한 단일 모델을 사용하는 대신, 기존의 검증된 로봇 기술과 최신 VLM을 모듈식으로 결합하는 접근법도 유효하다. <strong>MOO</strong>는 사전 학습된 VLM(OWL-ViT 등)을 ’눈’으로 사용하여 사용자 명령(“분홍색 고래 인형 집어줘”)에 해당하는 물체를 이미지 내에서 찾아내고(Segmentation/Detection), 이를 마스크(Mask) 형태나 픽셀 좌표로 변환하여 로봇 정책에 입력한다.45</p>
<ul>
<li><strong>작동 원리:</strong> 로봇 정책(Policy)은 “지정된 픽셀/마스크 위치의 물체를 집는다“는 범용적인 ‘집기(Grasping)’ 기술만 학습하면 된다. 물체가 무엇인지(What)를 식별하는 것은 VLM에게 전담시킨다.</li>
<li><strong>장점:</strong> 이 방식은 로봇 정책을 매번 재학습시킬 필요 없이, VLM 모델만 교체하거나 업데이트함으로써 인식 가능한 물체의 범위를 무한대로 확장할 수 있다. 이는 모듈형 일반화(Modular Generalization)의 효율성을 입증하는 사례이다.</li>
</ul>
<h3>4.4 환경 변화에 대한 시각적 적응: Bi-AQUA와 조명 불변성</h3>
<p>물체뿐만 아니라 조명, 배경의 복잡도(Clutter) 등 환경 조건의 변화도 시각적 일반화의 큰 걸림돌이다. 특히 수중 로봇이나 야외 로봇의 경우 빛의 산란, 그림자, 역광 등으로 인해 시각 정보가 극도로 불안정하다.</p>
<p><strong>Bi-AQUA</strong>는 이러한 문제를 해결하기 위해 **‘조명 인코더(Lighting Encoder)’**를 도입한 모방 학습 프레임워크이다.48</p>
<ul>
<li><strong>메커니즘:</strong> 비지도 학습 방식으로 현재 입력 이미지의 조명 조건만을 추출하여 저차원 벡터로 압축한다. 이 조명 벡터는 시각 백본(Visual Backbone) 네트워크의 특징 추출 과정을 변조(FiLM Modulation)하는 데 사용된다.</li>
<li><strong>효과:</strong> 이를 통해 로봇의 시각 처리 네트워크는 조명 변화에 영향을 받지 않는 물체의 본질적인 특징(Content Feature)에 집중할 수 있게 된다. 실험 결과, Bi-AQUA는 급격한 조명 변화나 탁한 수중 환경에서도 안정적으로 물체를 조작할 수 있음을 증명했다.49</li>
</ul>
<h2>5.  인지적 일반화: System 1과 System 2의 통합</h2>
<p>행동경제학자 대니얼 카너먼(Daniel Kahneman)이 제시한 인간의 인지 시스템인 ’System 1(빠른 직관)’과 ’System 2(느린 추론)’의 구분은 로봇의 인지 구조 설계에도 깊은 영감을 주고 있다.51 지금까지 살펴본 RMA나 VLA 모델들이 주로 입력에 대해 즉각적인 행동을 산출하는 System 1에 가깝다면, 진정한 일반화는 낯선 상황에서 멈춰 서서 생각하고 계획하는 System 2 능력을 필요로 한다.</p>
<h3>5.1 System 1과 System 2의 역할 분담</h3>
<ol>
<li><strong>System 1 (Fast &amp; Intuitive):</strong> 학습된 반사 신경과 같은 정책이다. 익숙한 환경에서의 보행, 단순한 물체 집기 등 반복적이고 빠른 반응이 필요한 작업에 적합하다. VLA나 RMA가 여기에 해당하며, 연산 속도가 빠르지만 낯선 상황이나 복잡한 논리가 필요한 문제에서는 실패할 확률이 높다.53</li>
<li><strong>System 2 (Slow &amp; Deliberative):</strong> 계획(Planning), 추론(Reasoning), 탐색(Search)을 담당한다. 새로운 문제를 만났을 때, LLM이나 시뮬레이터를 활용하여 가능한 행동의 결과를 미리 예측해보고, 논리적으로 타당한 해결책을 찾아낸다. 시간이 걸리지만 높은 일반화 성능을 제공한다.55</li>
</ol>
<h3>5.2 ThinkAct와 테스트 시간 연산(Test-time Compute)</h3>
<p><strong>ThinkAct</strong>와 같은 최신 프레임워크는 로봇이 행동하기 전에 “생각(Think)“할 수 있는 시간을 부여함으로써 일반화 성능을 극대화한다.56</p>
<ul>
<li><strong>메커니즘:</strong> 로봇은 카메라로 상황을 인식한 후, LLM을 통해 현재 상태를 텍스트로 기술하고 목표를 달성하기 위한 하위 목표(Sub-goal)들을 생성한다. 그리고 각 하위 목표에 대해 어떤 행동이 적절할지 내부적으로 시뮬레이션하거나 논리적 검증을 거친다.</li>
<li><strong>Test-time Compute의 중요성:</strong> 훈련 시점에 모든 상황을 학습하는 것은 불가능하다. 대신 추론 시점(Test-time)에 추가적인 연산 자원을 투입하여(예: 몬테카를로 트리 탐색, 다수의 시나리오 생성 후 평가) 최적의 해를 찾는 접근법은 로봇의 적응력을 획기적으로 높인다.58 이는 마치 사람이 낯선 기계를 다룰 때, 설명서를 읽거나 이리저리 만져보며(Trial-and-Error) 조작법을 익히는 과정과 유사하다.</li>
</ul>
<p>이러한 <strong>이중 시스템(Dual-System)</strong> 접근은 로봇이 평소에는 빠르고 효율적으로 움직이다가(System 1), 예기치 못한 장애물이나 새로운 과업을 마주치면 잠시 멈춰서 최적의 전략을 수립하는(System 2) 유연한 지능을 구현할 수 있게 한다. 특히 <strong>CoT(Chain-of-Thought)</strong> 프롬프팅을 로봇 계획에 적용함으로써, 로봇은 복잡한 다단계 명령(“파란 블록을 빨간 그릇에 넣고, 그 그릇을 다시 서랍에 넣어줘”)을 논리적으로 분해하고 수행할 수 있게 되었다.39</p>
<h3>5.3 표 2.5.2.A: 로봇 일반화 기술의 계층별 비교 요약</h3>
<table><thead><tr><th><strong>계층 (Level)</strong></th><th><strong>주요 도전 과제</strong></th><th><strong>대표적 기술 및 방법론</strong></th><th><strong>핵심 메커니즘</strong></th><th><strong>관련 연구 사례</strong></th></tr></thead><tbody>
<tr><td><strong>물리 제어 (Body)</strong></td><td>마찰, 질량, 지형 등 동역학 파라미터의 급격한 변화</td><td><strong>RMA (Rapid Motor Adaptation)</strong> <strong>Structured Domain Randomization</strong></td><td>고유감각(Proprioception) 이력을 통한 환경 인자(Extrinsics)의 암시적 추정 및 실시간 적응</td><td>23 19</td></tr>
<tr><td><strong>시각 인지 (Eyes)</strong></td><td>조명 변화, 배경 잡동사니(Clutter), 시각적 노이즈</td><td><strong>Bi-AQUA</strong> <strong>Data Augmentation</strong></td><td>조명 불변(Lighting-invariant) 특징 추출, 환경 정보의 명시적 인코딩(FiLM Modulation)</td><td>48 60</td></tr>
<tr><td><strong>물체 조작 (Semantic)</strong></td><td>훈련에 없던 새로운 물체(Novel Object) 및 범주 인식</td><td><strong>VLA (RT-2, OpenVLA)</strong> <strong>ObjectVLA</strong> <strong>MOO</strong></td><td>인터넷 스케일 VLM 지식 전이, 공동 임베딩 공간 정렬, 개방형 어휘(Open-Vocab) 인식</td><td>38 32 45</td></tr>
<tr><td><strong>인지 추론 (Brain)</strong></td><td>복잡하고 긴 호흡의 과업, 낯선 작업 순서, 예외 상황</td><td><strong>ThinkAct (Dual System)</strong> <strong>Test-time Compute</strong></td><td>LLM 기반의 계획 수립(System 2), 추론 시점의 시뮬레이션 및 논리적 검증, CoT 활용</td><td>54 58</td></tr>
</tbody></table>
<h2>6.  결론: 일반화된 로봇 지능의 미래</h2>
<p>지금까지 살펴본 바와 같이, 로봇의 일반화 능력은 단일 알고리즘이나 모델만으로 달성될 수 있는 성질의 것이 아니다. 그것은 물리적 신체의 즉각적인 적응(RMA)부터, 시각적 정보의 강건한 해석(Bi-AQUA, VLA), 그리고 낯선 상황에 대한 논리적 추론(System 2)에 이르기까지 로봇 시스템의 전 계층에 걸친 유기적인 혁신을 필요로 한다.</p>
<p>Software 2.0 시대의 로봇 공학은 이제 ’어떻게 제어할 것인가’를 넘어 ’어떻게 데이터를 통해 세상을 가르칠 것인가’로 질문을 옮겨가고 있다. 인터넷의 방대한 지식을 흡수한 VLA 모델과 물리적 본능을 학습한 RMA 제어기의 결합은 로봇을 단순한 자동화 기계에서 진정한 의미의 **‘체화된 지능(Embodied Intelligence)’**으로 진화시키고 있다. 앞으로의 연구는 이러한 다양한 적응 메커니즘을 하나의 통일된 아키텍처로 통합하고, 로봇이 스스로 데이터를 수집하고 학습하는 평생 학습(Lifelong Learning) 능력을 갖추게 하는 방향으로 나아갈 것이다. 비구조화된 현실 세계에서의 적응은 더 이상 먼 미래의 꿈이 아니라, 구체적인 기술적 해법들로 채워지고 있는 현재진행형의 과제이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Control theory - Wikipedia, https://en.wikipedia.org/wiki/Control_theory</li>
<li>Naive Problem Solving and Naive Mathematics - DSpace@MIT, https://dspace.mit.edu/bitstream/handle/1721.1/41194/AI_WP_249.pdf?…4</li>
<li>Identifying Behavior Models for Hybrid Production Systems - CORE, https://core.ac.uk/download/pdf/50520913.pdf</li>
<li>Science and Engineering for Learning Robots - Eric Jang, https://blog.evjang.com/2021/03/learning-robots.html</li>
<li>Lessons from Andrej Kaparthy - Antoine Buteau, https://www.antoinebuteau.com/lessons-from-andrej-kaparthy/</li>
<li>Robustness and Generalization - NUS, https://guppy.mpe.nus.edu.sg/~mpexuh/papers/NS-ML.pdf</li>
<li>Robustness and Generalization - Association for Computational Learning, https://www.learningtheory.org/colt2010/papers/30Xu.pdf</li>
<li>EFFICIENT AND PRINCIPLED ROBOT LEARNING: THEORY AND ALGORITHMS, https://repository.gatech.edu/bitstreams/5765b0f4-2a48-4ac5-aa2a-5e683759f14b/download</li>
<li>Fighting the Modeling Bottleneck – Learning Models for Production Plants - ResearchGate, https://www.researchgate.net/publication/257365002_Fighting_the_Modeling_Bottleneck_-_Learning_Models_for_Production_Plants</li>
<li>Bottleneck Prediction and Resilience Improvement for Manufacturing Systems - Deep Blue Repositories, https://deepblue.lib.umich.edu/bitstream/handle/2027.42/174268/xingjian_1.pdf?sequence=1</li>
<li>Legged Robot State-Estimation through Combined Forward Kinematic and Preintegrated Contact Factors - University of Michigan, http://robots.engin.umich.edu/publications/rhartley-2018a.pdf</li>
<li>Robust Direct Multi-Camera SLAM in Challenging Scenarios - MDPI, https://www.mdpi.com/2079-9292/14/23/4556</li>
<li>The stark contrast between “classical” robotics and learning methods - Medium, https://medium.com/@krrish94/the-stark-contrast-between-classical-robotics-and-learning-methods-4ae66525dbe4</li>
<li>Classical Robotics vs. End-to-End Learning - blogs.dal.ca, https://blogs.dal.ca/openthink/classical-robotics-vs-end-to-end-learning/</li>
<li>machine learning - Robustness vs Generalization - Data Science Stack Exchange, https://datascience.stackexchange.com/questions/102931/robustness-vs-generalization</li>
<li>Chapter 0 Machine Learning Robustness: A Primer - arXiv, https://arxiv.org/html/2404.00897v2</li>
<li>Domain Randomization for Sim2Real Transfer | Lil’Log, https://lilianweng.github.io/posts/2019-05-05-domain-randomization/</li>
<li>[R] Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey - Reddit, https://www.reddit.com/r/robotics/comments/j6ofro/r_simtoreal_transfer_in_deep_reinforcement/</li>
<li>Robot Learning From Randomized Simulations: A Review - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2022.799893/full</li>
<li>Evaluating Domain Randomization in Deep Reinforcement Learning Locomotion Tasks, https://www.mdpi.com/2227-7390/11/23/4744</li>
<li>Structured Domain Randomization - Emergent Mind, https://www.emergentmind.com/topics/structured-domain-randomization</li>
<li>Exploring Evolutionary Meta-Learning in Robotics - Google Research, https://research.google/blog/exploring-evolutionary-meta-learning-in-robotics/</li>
<li>RMA: Rapid Motor Adaptation for Walking and Manipulation | by Niraj Pudasaini | Toward Humanoids | Nov, 2025 | Medium, https://medium.com/correll-lab/rma-rapid-motor-adaptation-for-walking-and-manipulation-154d3315cb50</li>
<li>RMA: Rapid Motor Adaptation for Legged Robots - ResearchGate, https://www.researchgate.net/publication/353116578_RMA_Rapid_Motor_Adaptation_for_Legged_Robots</li>
<li>[2107.04034] RMA: Rapid Motor Adaptation for Legged Robots - arXiv, https://arxiv.org/abs/2107.04034</li>
<li>Rapid Adaptation for Robot Control - UC Berkeley EECS, https://www2.eecs.berkeley.edu/Pubs/TechRpts/2023/EECS-2023-205.pdf</li>
<li>RMA: Rapid Motor Adaptation for Legged Robots - Ashish Kumar, https://ashish-kmr.github.io/rma-legged-robots/</li>
<li>RMA: Rapid Motor Adaptation for Legged Robots | Paper Explained - YouTube, https://www.youtube.com/watch?v=B-1K4glzJ2Q</li>
<li>Robot Learning from Randomized Simulations: A Review - arXiv, https://arxiv.org/pdf/2111.00956</li>
<li>RA of meta-models trained by standard MAML, R-MAML both and R-MAMLout… - ResearchGate, https://www.researchgate.net/figure/RA-of-meta-models-trained-by-standard-MAML-R-MAML-both-and-R-MAMLout-versus-PGD-attacks_fig1_349520228</li>
<li>Open-Vocabulary Robotic Object Manipulation using Foundation Models, https://www.esann.org/sites/default/files/proceedings/2025/ES2025-35.pdf</li>
<li>ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration - arXiv, https://arxiv.org/html/2502.19250v1</li>
<li>Open-Vocabulary Visual-Language Reasoning: CLIP - Emergent Mind, https://www.emergentmind.com/topics/open-vocabulary-visual-language-reasoning-clip</li>
<li>Global Knowledge Calibration for Fast Open-Vocabulary Segmentation, https://openaccess.thecvf.com/content/ICCV2023/papers/Han_Global_Knowledge_Calibration_for_Fast_Open-Vocabulary_Segmentation_ICCV_2023_paper.pdf</li>
<li>Towards Open Vocabulary Learning: A Survey - IEEE Xplore, https://ieeexplore.ieee.org/iel7/34/10550108/10420487.pdf</li>
<li>Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications - IEEE Xplore, https://ieeexplore.ieee.org/iel8/6287639/10820123/11164279.pdf</li>
<li>Vision-Language-Action Models: Concepts, Progress, Applications and Challenges - arXiv, https://arxiv.org/html/2505.04769v1</li>
<li>kyegomez/RT-2: Democratization of RT-2 “RT-2: New model translates vision and language into action” - GitHub, https://github.com/kyegomez/RT-2</li>
<li>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v229/zitkovich23a/zitkovich23a.pdf</li>
<li>RT-2: New model translates vision and language into action - Google DeepMind, https://deepmind.google/blog/rt-2-new-model-translates-vision-and-language-into-action/</li>
<li>RT-2: Vision-Language-Action Models, https://robotics-transformer2.github.io/</li>
<li>OpenVLA: An open-source vision-language-action model for robotic manipulation. - GitHub, https://github.com/openvla/openvla</li>
<li>[Literature Review] ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration, https://www.themoonlight.io/en/review/objectvla-end-to-end-open-world-object-manipulation-without-demonstration</li>
<li>arXiv:2502.19250v2 [cs.RO] 28 Feb 2025, <a href="https://arxiv.org/pdf/2502.19250">https://arxiv.org/pdf/2502.19250?</a></li>
<li>moo.pdf - Open-World Object Manipulation using Pre-Trained Vision-Language Models, https://robot-moo.github.io/assets/moo.pdf</li>
<li>Open-World Object Manipulation using Pre-Trained Vision-Language Models - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v229/stone23a/stone23a.pdf</li>
<li>Daily Papers - Hugging Face, <a href="https://huggingface.co/papers?q=open-vocabulary+robotic+manipulation+systems">https://huggingface.co/papers?q=open-vocabulary%20robotic%20manipulation%20systems</a></li>
<li>Bi-AQUA: Bilateral Control-Based Imitation Learning for Underwater Robot Arms via Lighting-Aware Action Chunking with Transformers - arXiv, https://arxiv.org/html/2511.16050v1</li>
<li>Bi-AQUA: Bilateral Control-Based Imitation Learning for Underwater Robot Arms via Lighting-Aware Action Chunking with Transformers | Request PDF - ResearchGate, https://www.researchgate.net/publication/397824743_Bi-AQUA_Bilateral_Control-Based_Imitation_Learning_for_Underwater_Robot_Arms_via_Lighting-Aware_Action_Chunking_with_Transformers</li>
<li>[2511.16050] Bi-AQUA: Bilateral Control-Based Imitation Learning for Underwater Robot Arms via Lighting-Aware Action Chunking with Transformers - arXiv, https://arxiv.org/abs/2511.16050</li>
<li>System-2 Reasoning via Generality and Adaptation - ResearchGate, https://www.researchgate.net/publication/384811989_System-2_Reasoning_via_Generality_and_Adaptation</li>
<li>System 2 Thinking in AI - Emergent Mind, https://www.emergentmind.com/topics/system-2-thinking-in-ai</li>
<li>Hume: Introducing System-2 Thinking in Visual-Language-Action Model - arXiv, https://arxiv.org/html/2505.21432v1</li>
<li>A Dual Process VLA: Efficient Robotic Manipulation Leveraging VLM - arXiv, https://arxiv.org/html/2410.15549v1</li>
<li>System 2 Reasoning via Generality and Adaptation - arXiv, https://arxiv.org/html/2410.07866v2</li>
<li>ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning, https://jasper0314-huang.github.io/thinkact-vla/</li>
<li>Robots That “Think” Before They Move: Introducing ThinkAct | by Libin T Kurian - Medium, https://medium.com/@libintk200p/robots-that-think-before-they-move-introducing-thinkact-757d7c06c484</li>
<li>Mechanisms for test-time compute - Innovation Endeavors, https://www.innovationendeavors.com/insights/mechanisms-for-test-time-compute</li>
<li>RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models - arXiv, https://arxiv.org/html/2506.17811v1</li>
<li>On the Illumination Influence for Object Learning on Robot Companions - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC7805833/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>