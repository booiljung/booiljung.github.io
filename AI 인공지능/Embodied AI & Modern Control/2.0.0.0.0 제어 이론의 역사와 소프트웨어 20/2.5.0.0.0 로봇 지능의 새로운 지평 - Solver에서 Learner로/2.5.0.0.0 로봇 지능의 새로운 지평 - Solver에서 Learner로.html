<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2.5 로봇 지능의 새로운 지평: Solver에서 Learner로</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2.5 로봇 지능의 새로운 지평: Solver에서 Learner로</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 2. 제어 이론의 역사와 소프트웨어 2.0</a> / <a href="index.html">2.5 로봇 지능의 새로운 지평: Solver에서 Learner로</a> / <span>2.5 로봇 지능의 새로운 지평: Solver에서 Learner로</span></nav>
                </div>
            </header>
            <article>
                <h1>2.5 로봇 지능의 새로운 지평: Solver에서 Learner로</h1>
<p>로봇 공학의 역사는 물리적 세계를 제어하기 위한 투쟁의 기록이다. 고전 제어 이론이 정립된 이래, 로봇의 지능은 엔지니어가 설계한 수학적 모델 위에서 최적의 해(Solution)를 찾아내는 ’Solver(해결사)’의 패러다임에 굳건히 뿌리내리고 있었다. 뉴턴-오일러 방정식(Newton-Euler equations)으로 기술되는 강체 동역학, 라그랑주 역학(Lagrangian mechanics)에 기반한 에너지 최적화, 그리고 이를 실시간으로 풀어내는 모델 예측 제어(Model Predictive Control, MPC)는 로봇에게 정교한 움직임을 부여했다. 그러나 인공지능, 특히 딥러닝과 강화학습(Reinforcement Learning, RL)의 비약적인 발전은 이러한 전통적인 접근 방식에 근본적인 균열을 일으키고 있다.</p>
<p>안드레 카파시(Andrej Karpathy)가 주창한 “소프트웨어 2.0(Software 2.0)” 1의 물결은 이제 로봇 제어의 심장부로 침투하여, 명시적인 코드를 작성하여 해를 구하는 방식에서 데이터로부터 행동을 학습하는 ’Learner(학습자)’로의 거대한 전환을 이끌고 있다. 리치 서튼(Rich Sutton)이 “쓴 교훈(The Bitter Lesson)” 3에서 설파했듯이, 인간의 도메인 지식을 주입하려는 시도는 단기적으로는 효율적일지 모르나, 장기적으로는 방대한 계산 자원과 데이터를 활용하는 일반적인 학습 방법(Learning)과 검색(Search)에 필연적으로 추월당한다. 본 절에서는 정교한 수학적 솔버에서 데이터 기반의 학습자로 이동하는 로봇 지능의 패러다임 전환을 심층적으로 해부하고, 이 변화가 가져올 기술적 함의와 엔지니어링의 미래를 고찰한다.</p>
<h2>1.  Solver 패러다임: 명시적 제어의 정점과 한계</h2>
<p>전통적인 로봇 제어, 특히 고성능 동적 기동이 필요한 4족 보행 로봇(Quadruped)이나 휴머노이드의 제어는 오랫동안 Solver 패러다임의 지배하에 있었다. 이 접근 방식의 핵심은 로봇과 환경의 물리적 상호작용을 미분 방정식으로 모델링하고, 이를 바탕으로 미래의 상태를 예측하여 비용 함수(Cost Function)를 최소화하는 제어 입력을 실시간으로 계산하는 것이다.4</p>
<h3>1.1 수학적 우아함과 최적화의 미학</h3>
<p>Solver 패러다임에서 로봇의 ’지능’은 엔지니어가 설계한 모델의 정확도(Fidelity)와 최적화 솔버(Optimizer)의 성능에 의해 결정된다. 예를 들어, 보행 로봇의 제어는 매 제어 주기(Tick)마다 다음과 같은 유한 구간 최적 제어 문제(Finite-Horizon Optimal Control Problem)를 풀어내는 과정으로 환원된다.<br />
<span class="math math-display">
\begin{aligned} \min_{\mathbf{u}_{0:T-1}} &amp; \quad \sum_{t=0}^{T-1} l(\mathbf{x}_t, \mathbf{u}_t) + l_f(\mathbf{x}_T) \\ \text{subject to} &amp; \quad \mathbf{x}_{t+1} = f(\mathbf{x}_t, \mathbf{u}_t), \\ &amp; \quad \mathbf{x}_t \in \mathcal{X}, \quad \mathbf{u}_t \in \mathcal{U}, \\ &amp; \quad g(\mathbf{x}_t, \mathbf{u}_t) \leq 0 \end{aligned}
</span><br />
여기서 <span class="math math-inline">\mathbf{x}_t</span>는 로봇의 상태(위치, 속도, 각도 등), <span class="math math-inline">\mathbf{u}_t</span>는 제어 입력(토크, 전류), <span class="math math-inline">f(\cdot)</span>는 시스템의 동역학 모델, <span class="math math-inline">g(\cdot)</span>는 마찰 원뿔(Friction Cone)이나 관절 한계와 같은 물리적 제약 조건이다. 볼록 최적화(Convex Optimization) 기술의 발전으로 2차 계획법(Quadratic Programming, QP) 형태의 문제는 수 밀리초(ms) 단위로 해결 가능해졌으며, 이는 보스턴 다이내믹스(Boston Dynamics)의 아틀라스(Atlas)나 초기 유니트리(Unitree) 로봇들이 보여준 놀라운 동적 안정성의 기반이 되었다.6</p>
<p>MPC는 예측 지평(Prediction Horizon) 내의 외란을 미리 고려하여 현재의 최적 행동을 결정하므로, 모델이 정확하다면 매우 강건하고 예측 가능한 거동을 보장한다. 또한, 제어의 결과가 수식적으로 도출되므로 안정성(Stability)을 이론적으로 증명할 수 있고, 엔지니어가 로봇의 동작 실패 원인을 명확히 역추적할 수 있다는 점에서 ’설명 가능한 제어(Explainable Control)’의 특성을 가진다.</p>
<h3>1.2 모델링 병목(The Modeling Bottleneck)과 불확실성의 장벽</h3>
<p>그러나 Solver 패러다임은 로봇이 통제된 실험실을 벗어나 비정형 환경(Unstructured Environment)으로 나아갈 때 ’모델링 병목(Modeling Bottleneck)’이라는 치명적인 한계에 직면한다.9 현실 세계의 복잡성은 <span class="math math-inline">f(\mathbf{x}_t, \mathbf{u}_t)</span>로 표현되는 해석적 모델의 표현력을 압도한다.</p>
<ol>
<li><strong>접촉 동역학(Contact Dynamics)의 불연속성:</strong> 보행이나 조작(Manipulation) 작업에서 필수적인 접촉은 물리적으로 매우 짧은 시간에 발생하는 충격과 급격한 속도 변화를 동반하는 불연속적인 현상이다. 이를 Solver에서 다루기 위해서는 선형 상보성 문제(Linear Complementarity Problem, LCP)로 공식화하거나, 접촉을 부드러운 함수로 근사(Relaxation)해야 한다.11 그러나 이러한 근사는 현실과의 괴리(Sim-to-Real Gap)를 낳거나, LCP 솔버의 연산 복잡도를 기하급수적으로 증가시켜 실시간 제어를 불가능하게 만든다. 특히 유연한 물체(Soft body)나 마찰 계수가 불확실한 진흙, 모래와 같은 지면과의 상호작용은 명시적 수식으로 기술하기가 거의 불가능에 가깝다.13</li>
<li><strong>계산 비용과 지연(Latency)의 딜레마:</strong> MPC는 매 틱(tick)마다 최적화 문제를 처음부터 다시 풀어야 한다(Online Optimization). 로봇의 자유도(DoF)가 높아지거나 예측 지평을 길게 설정할수록 계산 비용은 <span class="math math-inline">O(N^3)</span> 이상으로 증가한다.15 이는 제어 주파수(Control Frequency)를 제한하여 로봇의 반응성을 떨어뜨리는 원인이 된다. 최근 연구에 따르면 신경망 기반 정책의 추론 속도(Inference time)는 마이크로초 단위인 반면, 고정밀 MPC 솔버의 해결 시간(Solve time)은 수십 밀리초에 달해 급격한 외란에 대한 대응력이 구조적으로 뒤처질 수밖에 없음이 지적되었다.5</li>
<li><strong>지각(Perception)과의 단절:</strong> 전통적인 파이프라인에서 시각 정보는 상태 추정(State Estimation)을 거쳐 로봇의 물리적 상태(위치, 속도 등)로 변환된 후 Solver에 입력된다. 이 과정에서 시각 정보에 포함된 풍부한 의미론적 맥락(Semantic Context)이나 물성의 미세한 단서들이 소실된다. “이 지형은 미끄러워 보인다“는 직관을 미분 방정식의 마찰 계수로 정확히 변환하는 것은 매우 난해하며, 오차를 유발하는 주된 원인이 된다.18</li>
</ol>
<h2>2.  Learner 패러다임: 경험의 압축과 소프트웨어 2.0</h2>
<p>Learner 패러다임, 즉 학습 기반 접근법은 “어떻게 제어할 것인가(How)“를 엔지니어가 직접 코딩하지 않고, “어떤 행동이 보상을 받는가(What)“를 목적 함수(Objective Function)로 정의하여 로봇 스스로 최적의 제어 법칙을 찾아내도록 한다. 이는 딥 강화학습(Deep Reinforcement Learning, DRL)과 모방 학습(Imitation Learning)을 통해 구현되며, 수십억 건의 시뮬레이션 데이터와 현실 데이터를 거대한 신경망의 가중치(Weights)로 압축하는 과정으로 이해할 수 있다.</p>
<h3>2.1 쓴 교훈(The Bitter Lesson)과 데이터의 승리</h3>
<p>리치 서튼(Rich Sutton)의 “쓴 교훈(The Bitter Lesson)“은 이 패러다임 전환의 철학적 기반을 제공한다.3 지난 70년의 AI 역사는 인간의 통찰력을 바탕으로 시스템을 설계하려는 시도(Solver 접근)가 초기에는 효과적일지라도, 결국에는 무한히 확장 가능한 연산 자원(Computation)을 활용하여 학습(Learning)과 검색(Search)을 수행하는 범용적인 방법론이 압도적인 승리를 거둔다는 것을 보여주었다.3</p>
<p>로봇 제어 분야에서도 이 교훈은 현실화되고 있다. 복잡한 4족 보행 로봇의 걸음걸이를 수식으로 유도하는 대신, 대규모 물리 시뮬레이션에서 수백 년에 해당하는 시간 동안 시행착오를 겪게 한 신경망이 더 자연스럽고 강건한(Robust) 보행을 보여준다. 취리히 연방 공과대학교(ETH Zurich)의 ANYmal 로봇 연구팀은 정교한 모델 기반 제어기보다 강화학습으로 학습된 정책이 미끄러운 빙판, 무너지는 잔해, 계단과 같은 가혹한 비정형 환경에서 월등한 복원력을 보임을 입증했다.21 이는 인간이 모델링할 수 없는 미세한 물리적 상호작용까지 신경망이 학습 과정에서 스스로 내재화했기 때문이다.</p>
<h3>2.2 추론(Inference) 대 최적화(Optimization): 속도의 혁명</h3>
<p>Learner 패러다임이 하드웨어 레벨에서 가지는 가장 큰 공학적 이점은 실행 시간(Run-time)의 효율성이다. Solver가 매 순간 무거운 최적화 문제를 풀어야 하는 반면, 학습된 정책(Policy)은 단순히 관측값(Observation)을 입력받아 행동(Action)을 출력하는 함수 통과(Forward Pass) 연산만을 수행한다.<br />
<span class="math math-display">
\mathbf{u}_t = \pi_\theta(\mathbf{o}_t)
</span><br />
여기서 <span class="math math-inline">\pi_\theta</span>는 학습된 신경망이다. 이 연산은 입력의 크기가 고정되어 있다면 일정한 시간 복잡도(Constant Time Complexity)를 가지며, GPU/NPU와 같은 가속기를 활용할 경우 수십 킬로헤르츠(kHz) 이상의 제어 주기도 쉽게 달성할 수 있다.16 이는 급격한 외란이나 미끄러짐이 발생했을 때, Solver가 새로운 해를 찾느라 계산 시간을 소비하는 동안 Learner는 이미 수십 번의 제어 명령을 수정하여 균형을 잡을 수 있음을 의미한다.</p>
<p>최근의 벤치마크 연구들은 이러한 차이를 명확히 보여준다. Unitree Go1 로봇을 대상으로 한 비교 실험에서 RL 기반 제어기는 100Hz 이상의 고빈도 제어를 통해 예측 불가능한 외력에 대해 즉각적인 반사 신경(Reflex)과 같은 반응을 보인 반면, MPC는 낮은 업데이트 주기로 인해 회복 불가능한 상태에 빠지는 경우가 관찰되었다.24 특히 발바닥이 지면에 닿는 순간의 충격과 같은 고주파수 이벤트를 처리하는 데 있어 Learner의 빠른 추론 속도는 결정적인 우위를 점한다.</p>
<h3>2.3 End-to-End Visuomotor Policy: 지각과 행동의 융합</h3>
<p>Learner 패러다임은 제어(Control)를 넘어 지각(Perception)과 행동(Action)의 경계를 허물고 있다. 기존의 모듈식 접근(Modular Approach)이 **‘지각 <span class="math math-inline">\rightarrow</span> 상태 추정 <span class="math math-inline">\rightarrow</span> 계획 <span class="math math-inline">\rightarrow</span> 제어’**의 순차적 파이프라인을 따랐다면, End-to-End 학습은 <strong>픽셀(Pixels) 입력에서 토크(Torques) 출력까지</strong>를 하나의 신경망으로 연결한다.19</p>
<p>이 방식은 중간 단계인 ’상태 추정’에서 발생하는 정보 손실을 방지한다. 전통적인 방식에서는 카메라가 젖은 바닥을 보더라도 이를 ’마찰 계수 0.2’라는 단일 숫자로 압축하여 제어기에 전달해야 했다. 그러나 End-to-End 신경망은 젖은 바닥의 텍스처, 광택, 기울기 등 풍부한 시각적 특징을 잠재 공간(Latent Space)에서 처리하여, “미끄러울 수 있으니 발을 더 조심스럽게 내딛고 무게 중심을 낮추라“는 복합적이고 미묘한 제어 명령을 생성할 수 있다.</p>
<p>Tesla의 Optimus 프로젝트는 이러한 경향의 최전선에 있다. 초기에는 고전적인 제어 스택을 고려했을 수 있으나, 최근 AI Day 등에서 공개된 기술적 세부 사항은 자율주행(FSD)에서 검증된 End-to-End 비전 네트워크 아키텍처를 휴머노이드 제어에 전면적으로 이식하고 있음을 시사한다.27 이는 로봇 제어를 더 이상 물리학의 문제가 아닌, 비디오 데이터 처리의 문제로 재정의하는 소프트웨어 2.0의 전형적인 사례다.</p>
<table><thead><tr><th><strong>특징</strong></th><th><strong>Solver (MPC/WBC)</strong></th><th><strong>Learner (RL/End-to-End)</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 원리</strong></td><td>물리 모델 기반 실시간 최적화</td><td>데이터 기반 함수 근사 (Neural Net)</td></tr>
<tr><td><strong>실행 시간</strong></td><td>느림 (ms 단위, <span class="math math-inline">O(N^3)</span>)</td><td>매우 빠름 (<span class="math math-inline">\mu s</span> 단위, <span class="math math-inline">O(1)</span>)</td></tr>
<tr><td><strong>제어 주파수</strong></td><td>30Hz ~ 500Hz (제한적)</td><td>1kHz ~ 30kHz (고빈도 가능)</td></tr>
<tr><td><strong>환경 적응성</strong></td><td>모델링 가능한 정형 환경에 강함</td><td>비정형, 노이즈 많은 환경에 강함</td></tr>
<tr><td><strong>설명 가능성</strong></td><td>높음 (수학적 증명 가능)</td><td>낮음 (Black-box)</td></tr>
<tr><td><strong>개발 비용</strong></td><td>도메인 지식 및 모델링 시간 소요</td><td>데이터 수집 및 학습 시간(GPU) 소요</td></tr>
</tbody></table>
<h2>3.  두 세계의 가교: 미분 가능한 물리학과 하이브리드 아키텍처</h2>
<p>Solver와 Learner는 상호 배타적인 개념이 아니다. 오히려 최근의 SOTA(State-of-the-Art) 연구들은 이 두 패러다임의 장점을 결합하여 단점을 상쇄하는 방향으로 나아가고 있다.</p>
<h3>3.1 미분 가능한 시뮬레이션 (Differentiable Simulation)</h3>
<p>전통적인 물리 엔진(MuJoCo, Bullet, Dart 등)은 입력에 대한 결과만 내놓을 뿐, 그 과정의 미분값(Gradient)을 제공하지 않는 ’블랙박스’였다. 따라서 강화학습은 그래디언트를 추정하기 위해 무작위 노이즈를 섞은 수많은 샘플링(예: PPO, REINFORCE)을 수행해야 했으며, 이는 샘플 효율성(Sample Efficiency)을 떨어뜨리는 주원인이었다.</p>
<p>그러나 <strong>DiffTaichi</strong> 29, <strong>Brax</strong> 30, <strong>Dojo</strong> 31와 같은 미분 가능한 물리 엔진(Differentiable Physics Engine)의 등장은 판도를 바꾸고 있다. 이들은 시뮬레이션의 모든 연산 과정(충돌, 관절 구동 등)을 미분 가능한 연산 그래프(Computational Graph)로 구현한다. 이를 통해 결과 상태(Loss)로부터 입력 제어(Control)까지의 그래디언트(<span class="math math-inline">\nabla_{\mathbf{u}} J</span>)를 역전파(Backpropagation)를 통해 해석적으로(analytically) 구할 수 있게 된다.</p>
<p>이는 Solver가 사용하는 기울기 기반 최적화(Gradient-based Optimization)의 강력함을 Learner의 학습 과정에 도입하는 것이다. 이를 통해 정책 최적화(Policy Optimization)의 수렴 속도를 수십 배에서 수백 배 단축시키거나, 시스템 식별(System Identification)을 통해 시뮬레이션과 현실의 물리 파라미터 간극(Sim-to-Real Gap)을 줄이는 데 활용된다.32 즉, Learner가 더 이상 맹목적인 시행착오(Trial and Error)가 아닌, 물리 법칙의 가이드를 받으며 학습하게 되는 셈이다.</p>
<h3>3.2 Solver를 교사로 삼는 Learner (Teacher-Student Distillation)</h3>
<p>현실적으로 가장 널리 쓰이는 하이브리드 방식은 Solver를 ’교사(Teacher)’로, Learner를 ’학생(Student)’으로 두는 지식 증류(Distillation) 기법이다. MPC나 WBC는 모델이 정확하다면 최적의 행동을 보여준다. 연구자들은 시뮬레이션 상에서 MPC가 완벽한 상태 정보(Privileged Information)를 가지고 생성한 최적 궤적을 데이터셋으로 삼아, 신경망(Learner)이 이를 모방하도록 학습시킨다.34</p>
<p>이때 학생 신경망은 카메라 이미지나 노이즈가 섞인 관절 센서 데이터와 같이 ’불완전한 정보’만을 입력받도록 제한한다. 학습이 완료되면, 학생 신경망은 교사(Solver)의 최적 행동을 모사하면서도, Solver가 겪는 계산 지연이나 모델 불확실성 문제 없이 고속으로 추론할 수 있게 된다. ETH Zurich의 연구진은 험지 보행을 위해 MPC로 생성한 궤적을 RL 정책이 모방하게 함으로써, MPC의 계획 능력과 RL의 강건함 및 실행 속도를 동시에 달성하는 성과를 거두었다.24 이는 “Solver의 뇌를 가진 Learner의 몸“을 만드는 것과 같다.</p>
<h3>3.3 잔차 학습 (Residual Learning): 안전을 위한 타협</h3>
<p>완전히 Learner에게 제어권을 넘기는 것이 부담스러운 안전 중요(Safety-critical) 시스템에서는 MPC를 기본 제어기로 두고, RL이 MPC가 해결하지 못하는 잔차(Residual) 성분만을 학습하여 더하는 방식이 사용된다.36</p>
<p><span class="math math-display">\mathbf{u}_{final} = \mathbf{u}_{MPC} + \pi_{RL}(\mathbf{x})</span></p>
<p>이 구조에서 MPC는 로봇의 기본적인 안정성과 제약 조건 준수를 보장하고, RL 정책 <span class="math math-inline">\pi_{RL}</span>은 복잡한 접촉이나 모델링 오차를 보정하는 역할을 수행한다. 이는 Solver의 신뢰성과 Learner의 적응성을 동시에 확보하려는 실용적인 접근법이다.</p>
<h2>2.5.4 최신 SOTA 기술 동향: 하드웨어 위의 패러다임 전쟁</h2>
<p>이론적 논의를 넘어, 실제 하드웨어 플랫폼에서의 적용 사례는 Solver에서 Learner로의 전환이 얼마나 급격하게 이루어지고 있는지를 보여준다.</p>
<h3>Unitree H1 및 쿼드러플 로봇: 벤치마크의 증언</h3>
<p>Unitree의 Go1, Go2 4족 보행 로봇과 H1 휴머노이드 로봇을 대상으로 한 최근의 벤치마크 연구들은 두 패러다임의 성능 격차를 실증적으로 보여준다.22</p>
<ul>
<li><strong>외란 거부(Disturbance Rejection):</strong> RL 기반 제어기는 측면에서 가해지는 강한 충격(Kick)이나 12kg 이상의 페이로드 변화에 대해 MPC보다 월등한 복원력을 보여주었다. RL은 시뮬레이션에서 수만 번 넘어져 보며(Falling) 다양한 실패 케이스에 대한 ‘반사적인’ 대응책을 익혔기 때문이다. 반면, MPC는 모델이 예측하지 못한 급격한 상태 변화가 발생하면 최적화 수렴에 실패하거나 발산하는 경향을 보였다.</li>
<li><strong>블라인드 보행(Blind Locomotion):</strong> 시각 정보 없이 고유수용감각(Proprioception)만으로 험지를 통과하는 실험에서, RL은 발바닥의 접촉 압력과 관절의 미세한 움직임을 통해 지면의 마찰력과 경사를 암묵적으로 추정(Implicit Estimation)하는 능력을 보여주었다.25 이는 명시적인 상태 추정기(State Estimator)에 의존하는 Solver보다 훨씬 빠르고 정확했다.</li>
<li><strong>에너지 효율성:</strong> 과거에는 RL의 움직임이 떨림(Jitter)이 많고 부자연스럽다는 비판이 있었으나, 최근에는 에너지 소모를 보상 함수에 포함시킴으로써 MPC와 대등하거나 더 효율적인 보행을 만들어내고 있다. MPC는 전신 토크를 최적 분배하여 에너지를 절약하는 데 강점이 있지만, 계산 지연으로 인한 불필요한 보정 동작이 에너지 손실을 유발하기도 한다.22</li>
</ul>
<h3>Tesla Optimus: Foundation Model과 로봇의 결합</h3>
<p>Tesla Optimus는 Learner 패러다임의 극단을 보여주는 사례다. 이들은 로봇의 제어 코드를 사람이 직접 작성하는 것을 최소화하고, 인간의 원격 조작(Teleoperation) 데이터와 시뮬레이션 데이터를 대량으로 학습시키는 데이터 중심(Data-Driven) 방식을 택했다.27</p>
<ul>
<li><strong>End-to-End Neural Nets:</strong> Optimus는 비전 인식부터 관절 제어까지를 하나의 거대한 트랜스포머(Transformer) 기반 신경망으로 처리하는 것을 목표로 한다. 이는 “입력은 광자(Photon)이고 출력은 제어(Control)여야 한다“는 자율주행의 철학을 로봇에 그대로 적용한 것이다.28</li>
<li><strong>일반 범용 로봇(Generalist Robot):</strong> 이러한 접근의 목표는 걷기나 박스 들기와 같은 특정 작업에 특화된 Solver를 만드는 것이 아니라, 인터넷 규모의 데이터와 비전-언어 모델(VLM)을 통해 다양한 작업을 수행하며 상황에 맞게 행동을 생성하는 범용 로봇을 만드는 것이다.20 이는 로봇을 특수 목적 기계에서 학습 가능한 에이전트로 재정의하는 것이다.</li>
</ul>
<h2>2.5.5 결론: Solver에서 Learner로, 그리고 그 너머</h2>
<p>로봇 지능은 수학적으로 정의된 모델을 푸는(Solve) 단계에서, 데이터와 경험을 통해 행동을 배우는(Learn) 단계로 비가역적인 전환을 맞이하고 있다. Solver 패러다임이 제공하는 수학적 보증과 명료함은 여전히 안전이 중요한 산업 현장이나 제어 이론의 기초로서 필수적이지만, 비정형 환경에서의 적응성과 범용성을 위해서는 Learner 패러다임이 필수불가결하다.</p>
<p>미래의 로봇 제어는 순수한 Solver나 순수한 Learner가 아닌, <strong>“구조화된 지식을 가진 Learner”</strong> 혹은 **“학습 능력을 갖춘 Solver”**의 형태를 띨 가능성이 높다. 미분 가능한 물리 엔진을 통해 물리 법칙을 신경망의 귀납적 편향(Inductive Bias)으로 주입하거나, 대규모 언어 모델(LLM)이 로봇의 고수준 추론과 계획(Planning)을 담당하고 저수준 제어는 고속 RL 정책이 담당하는 계층적 구조가 그 예시이다.39</p>
<p>결국, “Solver에서 Learner로“의 전환은 단순히 제어 알고리즘의 교체를 넘어, 로봇을 **’프로그래밍의 대상’에서 ‘교육(Training)의 대상’**으로 바라보는 관점의 근본적인 변화를 의미한다. 이는 로봇이 인간의 세밀한 지시 없이도 복잡한 물리 세계를 이해하고 상호작용할 수 있는 진정한 Embodied AI로 진화하는 데 있어 가장 중요한 이정표가 될 것이다. 로봇은 이제 방정식을 푸는 계산기가 아니라, 세상을 배우는 학생이 되었다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>Software 2.0 - Andrej Karpathy – Medium, 1월 4, 2026에 액세스, https://karpathy.medium.com/software-2-0-a64152b37c35</li>
<li>8+ AI-Driven Software 2.0: Karpathy’s Vision - Courageous Parents Network, 1월 4, 2026에 액세스, https://preprodapi.courageousparentsnetwork.org/software-20-andrej-karpathy/</li>
<li>The Bitter Lesson - Rich Sutton, 1월 4, 2026에 액세스, http://www.incompleteideas.net/IncIdeas/BitterLesson.html</li>
<li>Bounding the difference between model predictive control and neural networks - Proceedings of Machine Learning Research, 1월 4, 2026에 액세스, https://proceedings.mlr.press/v168/drummond22a/drummond22a.pdf</li>
<li>Automatica Large scale model predictive control with neural networks and primal active sets - Nikolay A. Atanasov, 1월 4, 2026에 액세스, https://natanaso.github.io/ref/Chen_NeuralMPC_Automatica22.pdf</li>
<li>Whole-body Humanoid Robot Locomotion with Human Reference - arXiv, 1월 4, 2026에 액세스, https://arxiv.org/html/2402.18294v1</li>
<li>Picking Up Momentum | Boston Dynamics, 1월 4, 2026에 액세스, https://bostondynamics.com/blog/picking-up-momentum/</li>
<li>Flipping the Script with Atlas | Boston Dynamics, 1월 4, 2026에 액세스, https://bostondynamics.com/blog/flipping-the-script-with-atlas/</li>
<li>How to Solve Your Inspection Bottleneck With a Robot - RoboDK blog, 1월 4, 2026에 액세스, https://robodk.com/blog/robot-inspection-bottleneck/</li>
<li>Simultaneous Learning of Contact and Continuous Dynamics - University of Pennsylvania, 1월 4, 2026에 액세스, https://dair.seas.upenn.edu/assets/pdf/Bianchini2023.pdf</li>
<li>Modeling Contact Friction and Joint Friction in Dynamic Robotic Simulation using the Principle of Maximum Dissipation - Dylan A. Shell, 1월 4, 2026에 액세스, https://cse-robotics.engr.tamu.edu/dshell/papers/wafr2010contact.pdf</li>
<li>Contact Models in Robotics: a Comparative Analysis - arXiv, 1월 4, 2026에 액세스, https://arxiv.org/pdf/2304.06372</li>
<li>Rethinking Contact Simulation for Robot Manipulation | by Toyota Research Institute, 1월 4, 2026에 액세스, https://medium.com/toyotaresearch/rethinking-contact-simulation-for-robot-manipulation-434a56b5ec88</li>
<li>Differentiable Simulation of Soft Multi-body Systems, 1월 4, 2026에 액세스, https://proceedings.neurips.cc/paper/2021/file/8e296a067a37563370ded05f5a3bf3ec-Paper.pdf</li>
<li>Efficient Learning of Inverse Dynamics Models for Adaptive Computed Torque Control, 1월 4, 2026에 액세스, https://ieeexplore.ieee.org/document/9981744/</li>
<li>Actor-Critic Model Predictive Control: Differentiable Optimization meets Reinforcement Learning for Agile Flight - arXiv, 1월 4, 2026에 액세스, https://arxiv.org/html/2306.09852v8</li>
<li>RT-HCP: Dealing with Inference Delays and Sample Efficiency to Learn Directly on Robotic Platforms - arXiv, 1월 4, 2026에 액세스, https://arxiv.org/html/2509.06714v1</li>
<li>Learning Task-Driven Control Policies via Information Bottlenecks - Robotics, 1월 4, 2026에 액세스, https://roboticsproceedings.org/rss16/p101.pdf</li>
<li>Science and Engineering for Learned Robots | Eric Jang, 1월 4, 2026에 액세스, https://evjang.com/2021/03/14/learning-robots.html</li>
<li>Neural Scaling Laws in Robotics - arXiv, 1월 4, 2026에 액세스, https://arxiv.org/html/2405.14005v2</li>
<li>Is reinforcement learning required for many quadrupedal robot actions, or can it be hard coded? - Reddit, 1월 4, 2026에 액세스, https://www.reddit.com/r/robotics/comments/1fmfy03/is_reinforcement_learning_required_for_many/</li>
<li>Benchmarking Model Predictive Control and Reinforcement Learning Based Control for Legged Robot Locomotion in MuJoCo Simulation - arXiv, 1월 4, 2026에 액세스, https://arxiv.org/html/2501.16590v1</li>
<li>Deep reinforcement learning for real-world quadrupedal locomotion: a comprehensive review - OAE Publishing Inc., 1월 4, 2026에 액세스, https://www.oaepublish.com/articles/ir.2022.20</li>
<li>Benchmarking Model Predictive Control and Reinforcement Learning-Based Control for Legged Robot Locomotion in MuJoCo Simulation - Digital Commons @ Michigan Tech, 1월 4, 2026에 액세스, https://digitalcommons.mtu.edu/cgi/viewcontent.cgi?article=2940&amp;context=michigantech-p2</li>
<li>BENCHMARKING MODEL PREDICTIVE CONTROL AND REINFORCEMENT LEARNING FOR LEGGED ROBOT LOCOMOTION - Digital Commons @ Michigan Tech, 1월 4, 2026에 액세스, https://digitalcommons.mtu.edu/cgi/viewcontent.cgi?article=2788&amp;context=etdr</li>
<li>End to end learning vs structured control : r/robotics - Reddit, 1월 4, 2026에 액세스, https://www.reddit.com/r/robotics/comments/1pp7xe8/end_to_end_learning_vs_structured_control/</li>
<li>A Complete Review Of Tesla’s Optimus Robot - Brian D. Colwell, 1월 4, 2026에 액세스, https://briandcolwell.com/a-complete-review-of-teslas-optimus-robot/</li>
<li>AI &amp; Robotics | Tesla, 1월 4, 2026에 액세스, https://www.tesla.com/AI</li>
<li>DIFFTAICHI: DIFFERENTIABLE PROGRAMMING FOR PHYSICAL SIMULATION - Immersive Computing Lab, 1월 4, 2026에 액세스, https://www.immersivecomputinglab.org/wp-content/uploads/2021/01/1910.00935.pdf</li>
<li>google/brax: Massively parallel rigidbody physics simulation on accelerator hardware. - GitHub, 1월 4, 2026에 액세스, https://github.com/google/brax</li>
<li>Dojo: A Differentiable Physics Engine for Robotics - arXiv, 1월 4, 2026에 액세스, https://arxiv.org/html/2203.00806v5</li>
<li>Learning Deployable Locomotion Control via Differentiable Simulation - arXiv, 1월 4, 2026에 액세스, https://arxiv.org/html/2404.02887v2</li>
<li>Improving Generalization of Differentiable Simulator Policies with Sharpness-Aware Optimization - OpenReview, 1월 4, 2026에 액세스, https://openreview.net/pdf?id=NgtwTM5eSA</li>
<li>RL + Model-based Control: Using On-demand Optimal Control to Learn Versatile Legged Locomotion - Computational Robotics Lab, 1월 4, 2026에 액세스, https://crl.ethz.ch/papers/kang2023rl.pdf</li>
<li>Using On-Demand Optimal Control to Learn Versatile Legged Locomotion - ETH Research Collection, 1월 4, 2026에 액세스, https://www.research-collection.ethz.ch/server/api/core/bitstreams/60378e12-1fb6-4dee-9ed7-76ca171ad7d2/content</li>
<li>Residual MPC: Blending Reinforcement Learning with GPU-Parallelized Model Predictive Control - arXiv, 1월 4, 2026에 액세스, https://arxiv.org/html/2510.12717v1</li>
<li>New Optimus video - 1,5x speed, not teleoperation, trained on one single neural net - Reddit, 1월 4, 2026에 액세스, https://www.reddit.com/r/robotics/comments/1kru5m9/new_optimus_video_15x_speed_not_teleoperation/</li>
<li>From Generalists to Specialists: A Case for Real-World RL in Robot Manipulation, 1월 4, 2026에 액세스, https://rasc.usc.edu/blog/from-generalists-to-specialists-a-case-for-real-world-rl-in-robot-manipulation/</li>
<li>DiffGen: Robot Demonstration Generation via Differentiable Physics Simulation, Differentiable Rendering, and Vision-Language Model - arXiv, 1월 4, 2026에 액세스, https://arxiv.org/html/2405.07309v1</li>
<li>Parkour in the wild: Learning a general and extensible agile locomotion policy using multi-expert distillation and RL Fine-tuning - arXiv, 1월 4, 2026에 액세스, https://arxiv.org/html/2505.11164v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>