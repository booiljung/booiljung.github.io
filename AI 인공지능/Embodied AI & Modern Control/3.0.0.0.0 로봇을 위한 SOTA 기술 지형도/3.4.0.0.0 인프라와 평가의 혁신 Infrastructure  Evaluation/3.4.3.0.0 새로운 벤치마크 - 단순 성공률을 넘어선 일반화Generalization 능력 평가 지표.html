<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.4.3 새로운 벤치마크: 단순 성공률을 넘어선 일반화(Generalization) 능력 평가 지표</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.4.3 새로운 벤치마크: 단순 성공률을 넘어선 일반화(Generalization) 능력 평가 지표</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 3. 로봇을 위한 SOTA 기술 지형도</a> / <a href="index.html">3.4 인프라와 평가의 혁신 (Infrastructure & Evaluation)</a> / <span>3.4.3 새로운 벤치마크: 단순 성공률을 넘어선 일반화(Generalization) 능력 평가 지표</span></nav>
                </div>
            </header>
            <article>
                <h1>3.4.3 새로운 벤치마크: 단순 성공률을 넘어선 일반화(Generalization) 능력 평가 지표</h1>
<p>인공지능 로봇 공학, 특히 <strong>Embodied AI(체화된 인공지능)</strong> 분야가 급격히 발전함에 따라, 에이전트의 지능을 측정하고 평가하는 방법론 또한 근본적인 패러다임의 전환을 맞이하고 있다. 과거 로봇 학습 연구는 통제된 실험실 환경이나 고정된 시뮬레이션 설정 내에서 특정 작업(Task)을 얼마나 잘 수행하는지를 나타내는 **‘성공률(Success Rate)’**을 유일한 성능 지표로 삼아왔다. 그러나 이러한 평가 방식은 훈련 데이터와 테스트 데이터가 동일하거나 매우 유사한 분포(Distribution)를 가지는 환경에서는 유효할지 모르나, 끊임없이 변화하고 예측 불가능한 변수들이 존재하는 실제 세계(Real-world)에서의 적응 능력을 대변하지 못한다는 비판에 직면했다.1</p>
<p>특정 조명 조건, 고정된 카메라 각도, 그리고 사전에 정의된 물체 위치에서 99%의 성공률을 기록한 로봇 팔이, 조명을 끄거나 물체의 색상을 바꾸는 사소한 변화만으로도 작업 수행에 실패하는 현상은 이 분야의 고질적인 문제인 **‘과적합(Overfitting)’**과 **‘암기(Memorization)’**의 한계를 여실히 보여준다.2 진정한 지능형 로봇은 단순히 과거의 경험을 반복하는 것이 아니라, 학습하지 않은 새로운 환경, 본 적 없는 물체, 그리고 복합적인 상황에 직면했을 때도 강건하게(Robustly) 대처할 수 있어야 한다.</p>
<p>본 장에서는 단순 성공률 중심의 평가가 갖는 한계를 심층적으로 분석하고, 이를 극복하기 위해 학계와 산업계에서 새롭게 제안하고 있는 **차세대 벤치마크(Benchmarks)**와 **다차원적 평가 지표(Multidimensional Metrics)**들을 포괄적으로 다룬다. 특히, 환경의 물리적·시각적 섭동(Perturbation)에 대한 저항성, 지식의 전이(Transfer) 효율성, 장기 작업(Long-horizon Task)의 수행 능력, 그리고 실세계 배포를 위한 안전성(Safety)과 시뮬레이션 예측력(Sim-to-Real Predictivity)을 정량화하는 방법론을 통해, Embodied AI의 <strong>일반화(Generalization)</strong> 능력을 평가하는 새로운 표준을 제시하고자 한다.</p>
<h2>1.  Embodied AI의 일반화 위기: 성공률의 허상과 새로운 정의</h2>
<p>전통적인 머신러닝, 특히 컴퓨터 비전이나 자연어 처리(NLP) 분야에서는 훈련 데이터(Train Set)와 평가 데이터(Test Set)의 분리가 명확하며, 이들 간의 분포 차이를 극복하는 일반화 능력이 모델 성능의 핵심 척도이다. 그러나 로봇 공학에서는 물리적 상호작용의 복잡성으로 인해 훈련 환경과 동일한 조건에서 테스트를 수행하는 경우가 빈번했으며, 이는 “로봇이 작업을 배웠는가, 아니면 환경을 외웠는가?“라는 근원적인 질문을 낳았다.2</p>
<h3>1.1 성공률 지표의 맹점과 과적합의 징후</h3>
<p>단순 성공률(Success Rate, SR)은 에피소드의 성공 여부를 이진값(Binary, 0 또는 1)으로 판별하여 전체 시도 횟수 대비 성공 횟수의 비율로 계산한다. 이 지표는 직관적이지만, 다음과 같은 심각한 맹점을 내포하고 있다.</p>
<ol>
<li><strong>과적합의 은폐(Masking Overfitting)</strong>: 에이전트가 물체의 기능적 특성(Affordance)이 아닌, 배경 텍스처나 특정 픽셀 패턴과 같은 허위 상관관계(Spurious Correlations)를 학습하더라도 고정된 환경에서는 높은 성공률을 달성할 수 있다. 예를 들어, 로봇이 ’컵의 손잡이’를 인식하는 것이 아니라 ’테이블 위의 특정 좌표에 있는 빨간색 픽셀 덩어리’에 반응하도록 학습될 수 있다.2</li>
<li><strong>효율성 무시(Ignoring Efficiency)</strong>: 성공률은 로봇이 목표를 달성하기까지 얼마나 많은 시간과 에너지를 소모했는지, 불필요한 동작은 없었는지를 반영하지 않는다. 비틀거리고 충돌하며 간신히 성공한 에피소드와 최적의 경로로 깔끔하게 성공한 에피소드가 동일하게 평가된다.6</li>
<li><strong>안전성 결여(Lack of Safety Awareness)</strong>: 작업은 성공했으나 그 과정에서 주변 기물을 파손하거나 위험한 궤적을 생성한 경우, 단순 성공률 지표는 이를 긍정적인 결과로 간주한다. 이는 실세계 배포 시 치명적인 사고로 이어질 수 있다.7</li>
</ol>
<h3>1.2 Embodied AI 일반화의 다차원적 분류 (Taxonomy of Generalization)</h3>
<p>이러한 한계를 극복하기 위해, 최근 연구들은 Embodied AI의 일반화 능력을 단일 차원이 아닌 다차원적인 축(Axes)으로 세분화하여 정의하고 있다. 이는 로봇이 직면할 수 있는 다양한 변화의 유형을 체계적으로 분류함으로써, 에이전트의 강점과 약점을 정밀하게 진단할 수 있게 한다.1</p>
<ul>
<li><strong>시각적 일반화(Visual Generalization)</strong>: 카메라의 위치(Viewpoint), 조명(Lighting), 배경(Background), 그리고 작업과 무관한 시각적 방해물(Distractors)의 변화에 대해 에이전트가 얼마나 강건한 인식을 유지하는지를 평가한다. 이는 컴퓨터 비전 모델의 강건성 평가와 유사하지만, 로봇의 행동 결정(Action)과 결합되어 있다는 점에서 차이가 있다.2</li>
<li><strong>물리적 일반화(Physical Generalization)</strong>: 시각적으로는 동일해 보일지라도 물체의 질량(Mass), 마찰 계수(Friction), 강성(Stiffness), 무게 중심(Center of Mass) 등 물리적 속성이 변화했을 때, 로봇이 자신의 제어 정책(Control Policy)을 적응시켜 작업을 완수할 수 있는지를 측정한다.2</li>
<li><strong>의미론적 일반화(Semantic Generalization)</strong>: 훈련 중에 본 적 없는 새로운 물체 인스턴스(Unseen Instances), 새로운 범주(Unseen Categories), 혹은 새로운 언어적 지시(Unseen Instructions)에 대해 에이전트가 개념적으로 이해하고 대응하는 능력을 평가한다. 예를 들어, “컵을 집어라“는 명령을 학습한 후, 처음 보는 모양의 ’와인잔’이나 ’머그컵’에 대해서도 동일한 기능을 수행할 수 있어야 한다.9</li>
<li><strong>구조적/절차적 일반화(Structural/Procedural Generalization)</strong>: 학습된 기본 기술(Primitive Skills)들을 새로운 순서로 조합하거나, 더 긴 호흡의 복잡한 작업(Long-horizon Tasks)을 수행하기 위해 절차적 지식을 응용하는 능력을 의미한다. 이는 단순한 모방을 넘어선 추론(Reasoning) 및 계획(Planning) 능력과 직결된다.11</li>
</ul>
<h3>1.3 Embodied AGI의 5단계 로드맵 (L1 ~ L5)</h3>
<p>이러한 일반화 수준을 종합하여, 자율 주행의 5단계 분류와 유사하게 <strong>Embodied AGI의 발달 단계</strong>를 L1에서 L5로 정의하는 새로운 프레임워크가 제안되고 있다.13</p>
<table><thead><tr><th><strong>단계</strong></th><th><strong>명칭</strong></th><th><strong>정의 및 특징</strong></th><th><strong>일반화 수준</strong></th></tr></thead><tbody>
<tr><td><strong>L1</strong></td><td><strong>단일 작업 수행 (Single-task)</strong></td><td>특정 환경에서 하나의 정의된 작업(예: 단순 파지)만 수행 가능.</td><td>일반화 능력 거의 없음. 환경 변화에 취약.</td></tr>
<tr><td><strong>L2</strong></td><td><strong>구성적 작업 수행 (Compositional)</strong></td><td>하위 작업들을 조합하여 복합적인 명령 수행 가능.</td><td>훈련된 기술 라이브러리 내에서의 조합 가능. 유사 작업 간 전이 제한적.</td></tr>
<tr><td><strong>L3</strong></td><td><strong>조건부 범용 수행 (Conditional General-purpose)</strong></td><td>다양한 범주의 작업을 수행하며, 환경 변화에 동적으로 적응.</td><td>작업 및 환경에 대한 <strong>조건부 일반화(Conditional Generalization)</strong> 달성. 실시간 반응성 확보.</td></tr>
<tr><td><strong>L4</strong></td><td><strong>고도 범용 로봇 (Highly General-purpose)</strong></td><td>훈련되지 않은 새로운 작업(Open-ended tasks)과 미지의 환경에 강건하게 대응.</td><td>물리 법칙과 세계 모델(World Model)을 내재화하여 <strong>강건한 일반화(Robust Generalization)</strong> 실현.</td></tr>
<tr><td><strong>L5</strong></td><td><strong>완전 범용 로봇 (All-purpose)</strong></td><td>인간 수준의 유연성과 사회적 상호작용, 창의적 문제 해결 능력 보유.</td><td>완전한 일반화 및 자율적 학습 능력.</td></tr>
</tbody></table>
<p>현재 대부분의 최첨단(SOTA) 연구들은 L1에서 L2로 넘어가는 단계에 있으며, 일부 선도적인 연구들이 L3 수준의 일반화를 목표로 새로운 벤치마크들을 고안하고 있다. 이어지는 절에서는 이러한 L3+ 수준의 일반화 능력을 검증하기 위해 설계된 구체적인 벤치마크 시스템들을 상세히 살펴본다.</p>
<h2>2.  환경 섭동에 대한 체계적 검증: The Colosseum</h2>
<p><strong>The Colosseum</strong> 벤치마크는 에이전트의 강건성(Robustness)을 평가하기 위해 설계된 가장 체계적이고 포괄적인 시뮬레이션 환경 중 하나이다. 기존의 벤치마크들이 훈련 데이터와 유사한 테스트 셋을 사용하여 성능을 과대평가하게 만드는 경향이 있었다면, The Colosseum은 에이전트를 극한의 상황으로 몰아넣는 <strong>‘스트레스 테스트(Stress Test)’</strong> 방식을 채택한다.2 이 벤치마크는 20개의 다양한 로봇 조작(Manipulation) 작업에 대해 총 **14가지 축의 환경 섭동(Perturbation Axes)**을 적용하여, 에이전트가 환경의 변화에 얼마나 민감하게 반응하는지를 정량화한다.</p>
<h3>2.1 가지 환경 섭동 축 (14 Axes of Environmental Perturbation)</h3>
<p>The Colosseum이 정의한 14가지 섭동 축은 시각적 요소뿐만 아니라 물리적 속성, 센서 노이즈 등을 망라하며, 이는 에이전트가 단순한 시각적 패턴 매칭을 넘어 환경의 인과적 구조를 이해하고 있는지를 판별하는 데 결정적인 역할을 한다.3</p>
<ol>
<li><strong>객체 색상 (MO Color)</strong>: 조작 대상(Manipulation Object)의 색상을 무작위로 변경한다. 이는 에이전트가 색상 정보에 과도하게 의존하는지(Color bias)를 평가한다.</li>
<li><strong>수신 객체 색상 (RO Color)</strong>: 수신 대상(Receiver Object, 예: 컵을 놓는 받침대)의 색상을 변경하여, 목표 지점 인식의 강건성을 테스트한다.</li>
<li><strong>객체 텍스처 (MO Texture)</strong>: 조작 대상의 표면 질감을 변경한다. 단순 색상 인식을 넘어 복잡한 패턴 내에서도 물체의 형태를 인식할 수 있는지 평가한다.</li>
<li><strong>수신 객체 텍스처 (RO Texture)</strong>: 수신 대상의 텍스처를 변경하여 배경과의 분리 능력을 시험한다.</li>
<li><strong>객체 크기 (MO Size)</strong>: 조작 대상의 스케일(Scale)을 변경한다. 이는 로봇이 3D 공간을 이해하고, 그리퍼(Gripper)의 개폐 너비를 적응적으로 조절할 수 있는지 평가하는 중요한 척도이다.</li>
<li><strong>수신 객체 크기 (RO Size)</strong>: 수신 대상의 크기를 변경하여 정밀한 제어 능력을 테스트한다.</li>
<li><strong>물리적 속성 (Physical Properties)</strong>: 물체의 <strong>질량(Mass)</strong> 및 <strong>마찰(Friction)</strong> 계수를 변경한다. 시각적으로는 감지할 수 없는 이 변화에 대해 로봇이 동역학적으로 적응(예: 더 강하게 쥐거나 힘을 조절)할 수 있는지 평가한다.2</li>
<li><strong>테이블 색상 (Table Color)</strong>: 작업대(Tabletop)의 색상을 변경하여 전경(Foreground)과 배경(Background)의 분리 능력을 평가한다.</li>
<li><strong>테이블 텍스처 (Table Texture)</strong>: 작업대에 복잡한 무늬를 적용하여 시각적 노이즈에 대한 저항성을 테스트한다.</li>
<li><strong>배경 텍스처 (Background Texture)</strong>: 벽면이나 바닥 등 주변 환경의 텍스처를 변경하여, 로봇이 작업 공간 외부의 시각적 정보에 현혹되지 않는지 확인한다.</li>
<li><strong>조명 색상 (Light Color)</strong>: 조명의 색상, 강도, 위치를 변경한다. 그림자의 방향이나 색조(Hue)의 변화가 물체 인식에 미치는 영향을 평가한다.</li>
<li><strong>방해 물체 (Distractor Objects)</strong>: 작업과 무관한 물체들을 작업 공간에 무작위로 배치한다. 이는 에이전트의 <strong>주의 집중(Attention)</strong> 능력과 <strong>충돌 회피(Collision Avoidance)</strong> 능력을 검증하는 가장 까다로운 테스트 중 하나이다.</li>
<li><strong>카메라 포즈 (Camera Pose)</strong>: 카메라의 위치와 각도를 변경한다. 에이전트가 특정 시점(Viewpoint)에 과적합되지 않고 **시점 불변성(Viewpoint Invariance)**을 확보했는지 평가한다.</li>
<li><strong>복합 적용 (Combined)</strong>: 위의 모든 요소들을 무작위로 복합 적용하여, 최악의 시나리오(Worst-case Scenario)에서의 성능을 평가한다.</li>
</ol>
<h3>2.2 평가 결과 분석 및 시사점</h3>
<p>The Colosseum을 활용하여 SOTA 모델들(예: RVT, PerAct 등)을 평가한 결과, 훈련 환경과 동일한 조건에서는 높은 성공률을 보였던 모델들이 섭동이 적용되자 성공률이 <strong>30~50% 급락</strong>하는 현상이 관찰되었다.2 특히, <strong>방해 물체의 추가(Distractors)</strong>, <strong>조명 조건의 변화(Lighting)</strong>, 그리고 **대상 물체의 색상 변경(Object Color)**이 성능 저하의 가장 큰 원인으로 지목되었다.2</p>
<p>주목할 만한 점은, 2D 이미지 기반의 에이전트보다 <strong>3D 포인트 클라우드(Point Cloud)나 복셀(Voxel) 기반의 에이전트</strong>가 이러한 환경 섭동에 대해 훨씬 더 강건한 성능을 보였다는 사실이다. 이는 3D 표현(Representation)이 조명이나 텍스처 변화와 같은 ’성가신 변수(Nuisance Variables)’에 덜 민감하고, 물체의 기하학적 구조를 더 본질적으로 포착하기 때문으로 분석된다.17 또한, The Colosseum 시뮬레이션에서의 성능 저하 패턴은 실제 로봇 환경(Real-world)에서 관찰되는 성능 저하와 높은 상관관계(<span class="math math-inline">R^2 \approx 0.61</span>)를 보여 2, 시뮬레이션 기반의 강건성 평가가 실제 세계 배포 전의 신뢰성 검증 도구로서 매우 유효함을 시사한다.</p>
<h2>3.  객체 수준의 다양성과 물리적 사실성: ManiSkill</h2>
<p>The Colosseum이 환경적 요인의 변화에 집중했다면, <strong>ManiSkill (SAPIEN Manipulation Skill Benchmark)</strong> 시리즈는 **객체 수준의 일반화(Object-level Generalization)**에 초점을 맞춘 벤치마크이다. 실제 세계에서 “문을 연다“는 행위는 수만 가지의 서로 다른 모양, 크기, 손잡이 형태를 가진 문들에 대해 보편적으로 적용되어야 한다. ManiSkill은 이러한 ’범주 내 다양성(Intra-category Diversity)’을 학습하고 평가하는 데 특화되어 있다.18</p>
<h3>3.1 PartNet-Mobility와 대규모 객체 다양성</h3>
<p>ManiSkill의 핵심 경쟁력은 <strong>PartNet-Mobility</strong> 데이터셋을 기반으로 구축된 방대한 3D 객체 라이브러리이다. 기존의 벤치마크들(예: Robosuite)이 소수의 고정된 3D 모델이나 단순한 기하학적 도형(YCB 객체 등)을 사용하는 반면, ManiSkill은 <strong>위상학적(Topological) 및 기하학적(Geometric) 다양성</strong>을 가진 수천 개의 관절 객체(Articulated Objects)를 제공한다.18</p>
<p>예를 들어, ManiSkill의 ‘캐비닛 열기(OpenCabinet)’ 작업에는 외문형, 양문형, 슬라이딩형, 손잡이가 있는 형, 없는 형 등 수백 가지 변형이 포함된 캐비닛 모델이 등장한다. 이를 통해 에이전트가 특정 캐비닛 모델의 3D 메쉬(Mesh)를 암기하는 것을 원천적으로 차단하고, ‘손잡이의 위치’, ‘경첩의 축’, ’문이 열리는 궤적’과 같은 기능적 **어포던스(Affordance)**를 학습하도록 강제한다. 이는 로봇이 처음 보는 가구 앞에서도 직관적으로 작동 원리를 파악하고 조작할 수 있는 **일반화된 기술(Generalizable Skill)**을 습득했는지 평가하는 척도가 된다.</p>
<h3>3.2 ManiSkill의 진화: 고속 시뮬레이션과 연성 물체(Soft Body)</h3>
<p>ManiSkill 벤치마크는 지속적으로 진화하며 Embodied AI의 최전선을 확장하고 있다.</p>
<ul>
<li><strong>ManiSkill 1 &amp; 2</strong>: 주로 강체(Rigid Body) 중심의 조작 작업에 집중하였으며, 포인트 클라우드(Point Cloud) 및 RGB-D 입력을 통한 시각적 학습을 강조했다. 대규모의 데모 데이터셋을 제공하여 모방 학습(Imitation Learning) 연구의 기틀을 마련했다.19</li>
<li><strong>ManiSkill 3</strong>: 최신 버전인 ManiSkill 3는 <strong>GPU 병렬 시뮬레이션(GPU Parallelized Simulation)</strong> 기술을 도입하여 데이터 수집 및 평가 속도를 비약적으로 향상시켰다. 단일 워크스테이션에서 <strong>30,000 FPS(초당 프레임 수)</strong> 이상의 속도로 시뮬레이션을 구동할 수 있어, 강화 학습(RL)의 샘플 효율성 문제를 완화하고 대규모 데이터 수집을 가능하게 했다.22 또한, 기존 시뮬레이터들이 다루기 어려워했던 <strong>연성 물체(Soft Body, 예: 옷 접기, 액체 따르기, 찰흙 빚기)</strong> 조작 작업을 포함시키고, 햅틱(Tactile) 센서 시뮬레이션을 강화하여 시각-촉각 융합(Visuo-Tactile) 정책의 일반화 성능까지 평가할 수 있는 플랫폼으로 거듭났다.23</li>
</ul>
<p>ManiSkill은 또한 <strong>Real2Sim</strong> 환경을 제공하여, 실제 환경을 스캔한 디지털 트윈(Digital Twin) 상에서 정책을 검증하고, 이를 다시 실제 로봇에 이식하는 <strong>Sim-to-Real Transfer</strong> 연구의 가교 역할을 수행한다.23</p>
<h2>4.  지식의 축적과 전이: 평생 학습 벤치마크 LIBERO</h2>
<p>단일 작업을 수행하는 능력을 넘어, 에이전트가 이전에 학습한 지식을 바탕으로 새로운 작업을 얼마나 빠르고 효율적으로 배우는지 평가하는 것이 **평생 학습(Lifelong Learning)**의 핵심이다. 인간은 젓가락질을 배우면 연필 잡는 법을 더 쉽게 배우지만, 인공신경망은 새로운 작업을 배울 때 기존의 지식을 잊어버리는 <strong>치명적 망각(Catastrophic Forgetting)</strong> 현상을 겪는다. **LIBERO (LIfelong learning BEchmark on RObot manipulation)**는 이러한 연속적 학습 능력과 지식 전이(Knowledge Transfer) 효율성을 정량화하기 위해 설계된 벤치마크이다.11</p>
<h3>4.1 지식 전이의 유형별 평가 (Task Suites)</h3>
<p>LIBERO는 절차적으로 생성된 130개의 작업을 4가지 주제로 분류하여, 에이전트가 어떤 종류의 지식을 전이하는 데 강점이 있는지 세밀하게 분석한다 25:</p>
<ol>
<li><strong>LIBERO-SPATIAL</strong>: 객체의 공간적 배치 관계(Spatial Layout)가 달라지는 작업들로 구성된다. “A를 B 옆에 놓아라“와 같은 공간적 개념의 전이 능력을 평가한다.</li>
<li><strong>LIBERO-OBJECT</strong>: 작업의 구조는 유사하지만 조작해야 하는 물체(Object)가 달라지는 경우이다. 새로운 물체의 시각적 특징을 빠르게 파악하고 적응하는 능력을 본다.</li>
<li><strong>LIBERO-GOAL</strong>: 동일한 객체와 배치를 가지지만, 달성해야 하는 목표(Goal)가 변경되는 작업들이다. (예: 컵을 ‘옮기는’ 것에서 ‘뒤집는’ 것으로 변경)</li>
<li><strong>LIBERO-100/LONG</strong>: 100개의 다양한 작업이 긴 시퀀스로 이어지는 환경으로, 장기적인 지식 유지 및 대규모 지식 전이 능력을 종합적으로 평가한다.</li>
</ol>
<h3>4.2 평생 학습 평가를 위한 동적 지표</h3>
<p>LIBERO는 단순 성공률 외에 다음과 같은 동적인 지표들을 도입하여 학습 과정의 효율성과 안정성을 다각도로 측정한다.11</p>
<ul>
<li>
<p>전방 전이 (Forward Transfer, FWT): 이전 작업들(<span class="math math-inline">T_{1...k-1}</span>)의 학습 경험이 새로운 작업(<span class="math math-inline">T_k</span>)을 학습하는 데 얼마나 도움을 주었는가를 측정한다.<br />
<span class="math math-display">
FWT = \frac{1}{K-1} \sum_{k=2}^{K} (S_{k, final} - S_{k, scratch})
</span><br />
여기서 <span class="math math-inline">S_{k, final}</span>은 전이 학습 후의 성능, <span class="math math-inline">S_{k, scratch}</span>는 처음부터(Scratch) 학습했을 때의 성능이다. 양수 값은 긍정적 전이(Positive Transfer)를, 음수 값은 이전 지식이 오히려 방해가 됨(Negative Transfer)을 의미한다.</p>
</li>
<li>
<p>부정적 후방 전이 (Negative Backward Transfer, NBT): 새로운 작업을 배운 후, 이전에 마스터했던 작업들의 성능이 얼마나 떨어졌는지를 측정한다. 이는 치명적 망각의 정도를 나타내는 핵심 지표이다.<br />
<span class="math math-display">
NBT = \frac{1}{K-1} \sum_{k=1}^{K-1} (S_{k, initial} - S_{k, current})
</span><br />
값이 작을수록(0에 가까울수록) 망각 없이 지식을 잘 유지하고 있음을 의미한다.</p>
</li>
<li>
<p><strong>AUC (Area Under the Success Rate Curve)</strong>: 학습 전 과정에 걸친 성공률 곡선의 아래 면적을 계산하여, 학습의 속도와 최종 성능을 동시에 반영한다. 빠르게 높은 성능에 도달할수록 AUC 값이 커진다.</p>
</li>
</ul>
<p>이러한 지표들은 Embodied AI가 단기적인 문제 해결을 넘어, <strong>지식을 축적하고 확장해 나가는 지능의 진화 과정</strong>을 평가한다는 점에서 중요한 의의를 갖는다.</p>
<h2>5.  장기 작업 계획과 논리적 추론: BEHAVIOR-1K &amp; CALVIN</h2>
<p>로봇이 가정이나 사무실과 같은 일상 공간에서 인간을 보조하기 위해서는 “아침 식사를 준비해라“와 같이 추상적이고 긴 호흡의 명령(Long-horizon Tasks)을 이해하고 수행할 수 있어야 한다. 이러한 능력은 단순한 동작 제어를 넘어, 작업의 순서를 계획하고(Planning), 상식적인 추론(Commonsense Reasoning)을 수행하는 고등 인지 능력을 요구한다.</p>
<h3>5.1 BEHAVIOR-1K: 논리-기호적 정의와 인간 중심 벤치마크</h3>
<p><strong>BEHAVIOR-1K</strong>는 인간이 실제 생활에서 필요로 하는 1,000가지의 일상 활동(청소, 요리, 정리, 유지보수 등)을 고충실도 시뮬레이션 환경(OmniGibson)에 구현한 대규모 벤치마크이다.12 이 벤치마크의 가장 독창적인 특징은 작업의 성공 여부를 시각적 상태가 아닌 **논리적 상태(Logical State)**로 정의한다는 점이다.</p>
<p>이를 위해 **BDDL (BEHAVIOR Domain Definition Language)**이라는 언어를 사용하여, 작업의 초기 상태와 목표 상태를 서술어 논리(Predicate Logic) 형태로 정의한다.12 예를 들어, “고기를 요리하라“는 작업은 단순히 고기에 열을 가하는 모션을 흉내 내는 것이 아니라, 시뮬레이션 내부의 상태 변수가 <code>cooked(meat) == True</code>로 변환되었는지를 확인한다. 또한 <code>inside(meat, oven)</code>, <code>toggled_on(oven)</code>과 같은 선행 조건들이 순차적으로 만족되었는지를 검증함으로써, 에이전트가 물리적 세계의 인과관계와 절차를 이해하고 있는지를 엄밀하게 평가한다. 이는 **절차적 일반화(Procedural Generalization)**와 <strong>상식적 추론</strong> 능력을 검증하는 데 최적화된 방법론이다.</p>
<h3>5.2 CALVIN: 언어-행동 접지(Language-Action Grounding)의 연속성</h3>
<p>**CALVIN (Composing Actions from Language and Vision)**은 로봇이 자연어 명령을 연속적으로 수행하는 능력을 평가하는 데 특화되어 있다. 단일 명령 수행에 그치지 않고, “서랍을 열어라” → “파란 블록을 집어라” → “블록을 서랍 안에 넣어라” → “서랍을 닫아라“와 같이 이어지는 일련의 명령(Sequence of Instructions)을 수행해야 한다.28</p>
<p>CALVIN의 주요 평가 지표는 **연속 성공률(Chain Success Rate)**이다. 에이전트가 1개, 2개,…, 5개의 연속된 지시를 실수 없이 수행한 비율을 각각 측정함으로써, 장기적인 문맥 유지 능력과 오차 누적에 대한 저항성을 평가한다. 초기 연구 결과, 단일 작업에서는 높은 성능을 보이던 모델들도 5단계 이상의 연속 작업에서는 성공률이 0.1% 미만으로 떨어지는 등, 장기 작업 수행이 여전히 큰 도전 과제임을 보여주었다.29</p>
<h2>6.  단순 성공 여부를 넘어서: 심층 정량 지표 (Advanced Quantitative Metrics)</h2>
<p>벤치마크 환경의 다변화와 더불어, 개별 에피소드의 성능을 측정하는 지표들 또한 성공/실패의 이분법적 평가를 넘어 효율성, 최적성, 안전성, 그리고 시뮬레이션의 신뢰성을 정량화하는 방향으로 고도화되고 있다.</p>
<h3>6.1  효율성 지표: SPL (Success weighted by Path Length)</h3>
<p>성공률(Success Rate, SR)만으로는 로봇이 목표 달성 과정에서 얼마나 효율적으로 움직였는지 알 수 없다. 로봇이 불필요하게 방황하거나, 제자리에서 회전하며 시간을 낭비하다가 우연히 목표 지점에 도달하더라도 성공으로 간주되기 때문이다. **SPL (Success weighted by Path Length)**은 이러한 비효율성을 페널티로 반영하기 위해 고안된 지표로, 주로 내비게이션(Navigation) 및 이동 조작(Mobile Manipulation) 과제에서 표준으로 사용된다.6<br />
<span class="math math-display">
\text{SPL} = \frac{1}{N} \sum_{i=1}^{N} S_i \frac{L_i}{\max(P_i, L_i)}
</span><br />
여기서 <span class="math math-inline">N</span>은 총 에피소드 수, <span class="math math-inline">S_i</span>는 <span class="math math-inline">i</span>번째 에피소드의 성공 여부(성공 시 1, 실패 시 0), <span class="math math-inline">P_i</span>는 에이전트가 실제로 이동한 경로의 길이, <span class="math math-inline">L_i</span>는 해당 작업의 이론적 최단 경로(Oracle Path Length) 길이이다. 성공한 에피소드라 할지라도 실제 이동 경로 <span class="math math-inline">P_i</span>가 최단 경로 <span class="math math-inline">L_i</span>보다 길어질수록 <span class="math math-inline">\frac{L_i}{\max(P_i, L_i)}</span> 항의 값이 1보다 작아지므로 점수가 깎이게 된다. 즉, SPL은 **“얼마나 많이 성공했는가(Success)”**와 **“얼마나 효율적으로 성공했는가(Efficiency)”**를 동시에 평가하는 엄격한 지표이다.</p>
<h3>6.2  최적성 격차: Optimality Gap</h3>
<p>강화 학습이나 최적 제어(Optimal Control) 문제에서는 에이전트가 달성한 보상(Reward)이나 비용(Cost)이 이론적으로 가능한 최적의 값(Optimal Value)과 얼마나 차이가 나는지를 측정하는 것이 중요하다. 이를 **최적성 격차(Optimality Gap)**라고 한다.31<br />
<span class="math math-display">
\text{Gap}(s) = J(s) - J^*
</span><br />
여기서 <span class="math math-inline">J(s)</span>는 에이전트가 수행한 정책의 비용(또는 음의 보상), <span class="math math-inline">J^*</span>는 최적 정책(Optimal Policy)의 비용이다. <span class="math math-inline">J^*</span>를 알 수 없는 복잡한 문제의 경우, 인간 전문가의 시연(Demonstration)이나 계산량이 매우 높은 오라클(Oracle) 알고리즘(예: NMPC)의 성능을 기준으로 삼기도 한다. 이 지표는 에이전트가 단순히 작업을 완수하는 수준을 넘어, 얼마나 <strong>숙련된(Proficient)</strong> 행동을 보이는지, 그리고 최적의 솔루션에 얼마나 근접했는지를 평가한다. 연구 결과에 따르면, 확률적 프로그래밍 기반 정책이 기존의 선형 근사 기반 정책보다 최적성 격차를 <span class="math math-inline">O(1/N)</span> 수준으로 줄일 수 있음이 증명되기도 했다.34</p>
<h3>6.3  Sim-to-Real 격차 정량화: SRCC (Sim-vs-Real Correlation Coefficient)</h3>
<p>시뮬레이션에서 개발된 알고리즘이 실제 로봇에서도 잘 작동할지 예측하는 것은 매우 중요하다. **SRCC (Sim-vs-Real Correlation Coefficient)**는 시뮬레이션 상의 성능 순위와 실제 환경에서의 성능 순위 간의 상관관계를 측정하는 지표이다.30<br />
<span class="math math-display">
\text{SRCC} = \frac{\sum (s_i - \bar{s})(r_i - \bar{r})}{\sqrt{\sum (s_i - \bar{s})^2} \sqrt{\sum (r_i - \bar{r})^2}}
</span><br />
여기서 <span class="math math-inline">s_i</span>는 <span class="math math-inline">i</span>번째 모델의 시뮬레이션 성능, <span class="math math-inline">r_i</span>는 동일 모델의 실제 로봇(Real-world) 성능이다. SRCC 값이 1에 가까울수록 시뮬레이터가 현실을 잘 반영하고 있음을 의미하며(Predictivity가 높음), 시뮬레이션 상에서의 성능 향상이 실제 성능 향상으로 이어진다는 확신을 가질 수 있다. 반면 SRCC가 낮다면, 시뮬레이션에서의 최적화가 실제로는 무의미하거나 오히려 해로울 수 있음을 시사한다. 최근 연구들은 시뮬레이터의 파라미터 튜닝을 통해 SRCC를 0.18에서 0.844까지 끌어올림으로써 시뮬레이션 평가의 신뢰도를 입증했다.30</p>
<h3>6.4  안전성 위반 분류 체계: Safety Violations Taxonomy</h3>
<p>로봇이 작업을 아무리 빠르고 정확하게 수행하더라도, 그 과정에서 인간에게 위협을 가하거나 기물을 파손한다면 실세계 배포는 불가능하다. 최근 제안된 <strong>ANNIE-Bench</strong> 등의 안전성 중심 벤치마크는 **안전 위반(Safety Violation)**을 그 심각도에 따라 3단계로 분류하여 평가한다.7</p>
<ul>
<li><strong>Critical (치명적 위반)</strong>: 인간이나 로봇 자신, 또는 환경에 직접적이고 되돌릴 수 없는 피해를 입히는 경우. (예: 날카로운 도구를 사람 쪽으로 향함, 과도한 힘으로 유리잔 파손, 충돌) - 배포 절대 불가.</li>
<li><strong>Dangerous (위험한 위반)</strong>: 즉각적인 사고는 발생하지 않았으나, 안전 임계치(Safety Threshold)를 초과하여 사고 발생 확률이 매우 높은 경우. (예: 규정 속도 초과, 뜨거운 물체를 불안정하게 파지, 사람과의 안전 거리 미확보) - 배포 전 반드시 수정 필요.</li>
<li><strong>Risky (잠재적 위험)</strong>: 사고 가능성은 낮으나 불안정하고 비정상적인 행동. (예: 불필요한 급가속, 물체를 너무 높이 들어 올림, 경로 이탈) - 성능 최적화 및 튜닝 필요.</li>
</ul>
<p>이러한 안전 지표들은 단순히 ’사고 횟수’를 세는 것을 넘어, 로봇의 행동이 얼마나 **안전 제약 조건(Safety Constraints)**을 준수하는지를 정밀하게 모니터링하며, <strong>안전 제약 최적화(Constrained Optimization)</strong> 문제의 목적함수로 활용된다.</p>
<h2>7.  결론 및 평가 프로토콜의 표준화 제언</h2>
<p>지금까지 살펴본 바와 같이, Embodied AI의 평가 체계는 단순한 ’작업 성공률’을 넘어 <strong>다양한 환경 섭동에 대한 강건성(The Colosseum)</strong>, <strong>물체 수준의 일반화(ManiSkill)</strong>, <strong>지식의 축적과 전이(LIBERO)</strong>, <strong>장기적 계획 및 추론 능력(BEHAVIOR-1K, CALVIN)</strong>, 그리고 **효율성, 최적성, 안전성(SPL, Optimality Gap, Safety Metrics)**을 아우르는 다차원적이고 정밀한 방향으로 진화하고 있다.</p>
<p>이러한 고도화된 벤치마크들은 연구자들에게 더 가혹한 검증을 요구하지만, 이는 실험실 환경에 머물러 있던 AI 로봇을 불확실성이 가득한 실제 가정과 산업 현장으로 진출시키기 위해 반드시 넘어야 할 관문이다.</p>
<p>마지막으로, 신뢰할 수 있는 평가를 위해 학계는 <strong>실험 프로토콜의 표준화</strong>를 강력히 요구하고 있다. 단발성 성공(Cherry-picking)을 배제하고 통계적 유의성을 확보하기 위해, 각 작업당 최소 **25~50회 이상의 충분한 실험(Trials)**을 수행해야 하며 35, 결과 보고 시에는 평균값뿐만 아니라 **표준 편차(Standard Deviation)**와 **신뢰 구간(Confidence Interval)**을 반드시 명시해야 한다. 또한, <strong>AutoEval</strong>과 같은 자동화된 평가 시스템의 도입을 통해 평가 과정에서의 인간 개입 편향을 최소화하려는 노력도 병행되어야 한다.38 이러한 엄격한 평가 문화의 정착은 Embodied AI 기술이 단순한 ’신기한 데모’를 넘어, 인간의 삶을 실질적으로 돕는 ’신뢰할 수 있는 제품’으로 거듭나기 위한 초석이 될 것이다.</p>
<table><thead><tr><th><strong>벤치마크</strong></th><th><strong>주요 특징 및 평가 대상</strong></th><th><strong>평가 중점 (Focus of Generalization)</strong></th><th><strong>주요 지표 (Metrics)</strong></th></tr></thead><tbody>
<tr><td><strong>The Colosseum</strong></td><td>20개 작업, 14개 섭동 축 (조명, 방해물 등)</td><td><strong>환경 강건성 (Environmental Robustness)</strong></td><td>Success Rate Degradation</td></tr>
<tr><td><strong>ManiSkill 3</strong></td><td>대규모 객체(PartNet), 연성 물체, GPU 시뮬레이션</td><td><strong>물체 수준 일반화 (Object Generalization)</strong></td><td>Soft Success, Time-to-Success</td></tr>
<tr><td><strong>LIBERO</strong></td><td>130개 작업, 4개 지식 전이 테마</td><td><strong>평생 학습, 지식 전이 (Knowledge Transfer)</strong></td><td>FWT, NBT, AUC</td></tr>
<tr><td><strong>BEHAVIOR-1K</strong></td><td>1,000개 일상 활동, 논리적 상태 정의(BDDL)</td><td><strong>장기 계획, 상식적 추론 (Reasoning)</strong></td><td>Logical State Success, Efficiency</td></tr>
<tr><td><strong>CALVIN</strong></td><td>언어 기반의 연속적 장기 작업 수행</td><td><strong>언어-행동 접지 (Language-Action Grounding)</strong></td><td>Chain Success Rate (1~5 steps)</td></tr>
<tr><td><strong>ANNIE-Bench</strong></td><td>3단계 안전 위반 분류 (Critical/Dangerous/Risky)</td><td><strong>안전성 (Safety Assurance)</strong></td><td>Safety Violation Counts/Rate</td></tr>
</tbody></table>
<p><strong>표 3.4.3-1.</strong> Embodied AI의 일반화 및 신뢰성 평가를 위한 주요 최신 벤치마크 비교 요약</p>
<h2>8. 참고 자료</h2>
<ol>
<li>(PDF) Embodied AI: A Survey on the Evolution from Perceptive to …, https://www.researchgate.net/publication/395884929_Embodied_AI_A_Survey_on_the_Evolution_from_Perceptive_to_Behavioral_Intelligence</li>
<li>A Benchmark for Evaluating Generalization for Robotic Manipulation, https://www.roboticsproceedings.org/rss20/p133.pdf</li>
<li>A Benchmark for Evaluating Generalization for Robotic Manipulation, https://arxiv.org/html/2402.08191v2</li>
<li>LIBERO-PRO: Towards Robust and Fair Evaluation of Vision … - arXiv, https://arxiv.org/html/2510.03827v1</li>
<li>(PDF) Improving Generalization Ability of Robotic Imitation Learning …, https://www.researchgate.net/publication/394121375_Improving_Generalization_Ability_of_Robotic_Imitation_Learning_by_Resolving_Causal_Confusion_in_Observations</li>
<li>Evaluation - EvalAI, https://eval.ai/web/challenges/challenge-page/97/evaluation</li>
<li>Annie: Be Careful of Your Robots - arXiv, https://arxiv.org/html/2509.03383v1</li>
<li>ANNIE: Be Careful of Your Robots - ChatPaper, https://chatpaper.com/paper/185716</li>
<li>A Taxonomy for Evaluating Generalist Robot Policies - arXiv, https://arxiv.org/html/2503.01238v1</li>
<li>Generalizing from Instance-level to Category-level Skills in Robot …, https://arxiv.org/html/2502.09389v2</li>
<li>Libero-Object Benchmark - Emergent Mind, https://www.emergentmind.com/topics/libero-object-benchmark</li>
<li>BEHAVIOR: Benchmark for Embodied AI - Emergent Mind, https://www.emergentmind.com/topics/behavior-benchmark-for-embodied-ai</li>
<li>Toward Embodied AGI: A Review of Embodied AI and the Road Ahead, https://arxiv.org/html/2505.14235v1</li>
<li>A Review of Embodied AI and the Road Ahead - arXiv, https://arxiv.org/pdf/2505.14235</li>
<li>A Review of Embodied AI and the Road Ahead, https://chatpaper.com/paper/138592</li>
<li>Colosseum Documentation: Overview, https://robot-colosseum.readthedocs.io/</li>
<li>THE COLOSSEUM: A Benchmark for Evaluating Generalization for …, https://embodied-ai.org/papers/2024/27_THE_COLOSSEUM_A_Benchmark_f.pdf</li>
<li>ManiSkill: Generalizable Manipulation Skill Benchmark with Large …, https://www.alphaxiv.org/overview/2107.14483v5</li>
<li>ManiSkill: Generalizable Manipulation Skill Benchmark with Large …, https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/eda80a3d5b344bc40f3bc04f65b7a357-Paper-round2.pdf</li>
<li>ManiSkill: Generalizable Manipulation Skill Benchmark with Large …, https://openreview.net/forum?id=zQIvkXHS_U5</li>
<li>SAPIEN Manipulation Skill Challenge, https://sapien.ucsd.edu/challenges/maniskill/2022/</li>
<li>ManiSkill3: GPU Parallelized Robotics Simulation and Rendering for …, https://arxiv.org/html/2410.00425v1</li>
<li>ManiSkill-ViTac 2025: Challenge on Manipulation Skill Learning …, https://arxiv.org/html/2411.12503v1</li>
<li>haosulab/ManiSkill: SAPIEN Manipulation Skill Framework … - GitHub, https://github.com/haosulab/ManiSkill</li>
<li>Benchmarking Knowledge Transfer for Lifelong Robot Learning, https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/liu_zhu_NeurIPS2023.pdf</li>
<li>Benchmarking Knowledge Transfer for Lifelong Robot Learning, https://papers.neurips.cc/paper_files/paper/2023/file/8c3c666820ea055a77726d66fc7d447f-Paper-Datasets_and_Benchmarks.pdf</li>
<li>BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with …, https://www.alphaxiv.org/overview/2403.09227v1</li>
<li>CALVIN: A Benchmark for Language-Conditioned Policy Learning …, http://ais.informatik.uni-freiburg.de/publications/papers/mees22ral_a.pdf</li>
<li>CALVIN Dataset: Language &amp; Vision Robotics - Emergent Mind, https://www.emergentmind.com/topics/calvin-dataset</li>
<li>Sim2Real Disparity Metric - Emergent Mind, https://www.emergentmind.com/topics/sim2real-disparity-metric</li>
<li>Benchmark Environments For Process Control Problems - arXiv, https://arxiv.org/pdf/2410.22093</li>
<li>Online Constrained Meta-Learning - NeurIPS, https://proceedings.neurips.cc/paper_files/paper/2023/file/320e941f53db45bddc8757d1c8c4f6aa-Paper-Conference.pdf</li>
<li>Bounding Optimality Gaps for Non-Convex Optimization Problems, https://arxiv.org/pdf/2304.03739</li>
<li>Achieving - O ~ ( 1 / N ) - Optimality Gap in Restless Bandits through …, https://openreview.net/forum?id=TQNlIQIrcK</li>
<li>RT-1: Robotics Transformer for Real-World Control at Scale - arXiv, https://arxiv.org/html/2212.06817</li>
<li>Is Your Imitation Learning Policy Better than Mine … - OpenReview, https://openreview.net/pdf/6f7135d1b481e037e59010d3050c0c8f26dec317.pdf</li>
<li>How Many Participants? How Many Trials? Maximizing the Power of …, https://pmc.ncbi.nlm.nih.gov/articles/PMC10991062/</li>
<li>AutoEval: Autonomous Evaluation of Generalist Robot Manipulation …, https://openreview.net/pdf/bb334e82c6d4db54ea34c3e89d306563db839e8b.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>