<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.4.1 데이터의 민주화: Open X-Embodiment 등 대규모 로봇 데이터셋 프로젝트 현황</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.4.1 데이터의 민주화: Open X-Embodiment 등 대규모 로봇 데이터셋 프로젝트 현황</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 3. 로봇을 위한 SOTA 기술 지형도</a> / <a href="index.html">3.4 인프라와 평가의 혁신 (Infrastructure & Evaluation)</a> / <span>3.4.1 데이터의 민주화: Open X-Embodiment 등 대규모 로봇 데이터셋 프로젝트 현황</span></nav>
                </div>
            </header>
            <article>
                <h1>3.4.1 데이터의 민주화: Open X-Embodiment 등 대규모 로봇 데이터셋 프로젝트 현황</h1>
<p>인공지능 연구의 역사는 데이터의 가용성이 기술적 도약을 견인해 온 과정이라고 해도 과언이 아니다. 컴퓨터 비전 분야가 <em>ImageNet</em>이라는 거대 데이터셋의 구축을 통해 심층 신경망(Deep Neural Networks)의 부흥을 맞이했고, 자연어 처리(NLP) 분야가 웹 스케일의 텍스트 말뭉치(Corpus)를 기반으로 한 거대 언어 모델(LLM)을 통해 인간 수준의 추론 능력에 도달했듯이, 로봇 공학 또한 바야흐로 ’데이터의 규모(Scale)’와 ’다양성(Diversity)’이라는 두 가지 축을 중심으로 거대한 패러다임의 전환을 겪고 있다. 과거의 로봇 제어 연구가 특정 실험실 환경, 고정된 하드웨어, 그리고 사전에 정의된 제한적인 작업(Task)에 최적화된 소규모 데이터셋에 의존했다면, 현재의 흐름은 전 세계의 로봇 데이터를 통합하고 표준화하여 범용 로봇 지능(Generalist Robot Intelligence)을 구현하려는 ’데이터의 민주화(Democratization of Data)’로 요약된다. 본 절에서는 이러한 변화의 중심에 있는 <em>Open X-Embodiment</em> 프로젝트를 필두로, <em>DROID</em>, <em>RH20T</em>, <em>BridgeData V2</em> 등 최신 대규모 로봇 데이터셋 프로젝트들의 기술적 특징과 구축 방법론, 그리고 이들이 제시하는 로봇 파운데이션 모델(Foundation Model)의 가능성과 한계를 심층적으로 분석한다.</p>
<h2>1.  로봇 학습의 새로운 지평: 파편화된 경험의 통합</h2>
<p>로봇 공학에서 데이터를 수집하는 과정은 컴퓨터 비전이나 NLP 분야와 비교할 때 근본적으로 높은 비용과 복잡성을 수반한다. 인터넷상에 존재하는 이미지를 크롤링하거나 텍스트를 수집하는 것과 달리, 로봇 데이터는 물리적 세계와의 직접적인 상호작용(Interaction)을 통해서만 생성될 수 있기 때문이다.1 로봇이 직접 환경과 부딪히며 데이터를 얻는 과정은 시간적 소모가 클 뿐만 아니라, 하드웨어의 마모, 안전 문제, 그리고 인간 감독자의 개입 필요성 등으로 인해 확장(Scaling)에 물리적 한계가 존재한다. 이러한 ’데이터 기근(Data Scarcity)’과 각 연구실마다 상이한 하드웨어 및 환경 설정으로 인한 ’데이터 파편화(Data Fragmentation)’는 오랫동안 범용 로봇 개발을 가로막는 가장 큰 장벽이었다.</p>
<p>그러나 트랜스포머(Transformer) 아키텍처의 등장과 함께 데이터의 양이 모델 성능의 비약적 향상을 담보한다는 ’스케일링 법칙(Scaling Law)’이 입증되면서, 로봇 연구자들은 개별 로봇의 경험을 넘어선 집단지성의 가능성에 주목하기 시작했다. 만약 전 세계의 서로 다른 로봇들이 수집한 데이터를 하나의 거대한 풀(Pool)로 통합하고, 이를 단일 모델에 학습시킨다면 어떨까? 서로 다른 신체 구조(Embodiment)를 가진 로봇들이 데이터를 공유함으로써, 특정 로봇이 겪어보지 못한 상황을 다른 로봇의 경험을 통해 학습하는 ’교차 체화 학습(Cross-Embodiment Learning)’이 가능해질 것이다. 이러한 가설을 검증하기 위해 출범한 것이 바로 <em>Open X-Embodiment</em> (OXE) 프로젝트이며, 이는 로봇 공학 분야의 <em>ImageNet</em> 모멘트로 평가받는다.2</p>
<h2>2.  Open X-Embodiment: 로봇 데이터의 거대 통합</h2>
<p><em>Open X-Embodiment</em> 프로젝트는 구글 딥마인드(Google DeepMind)를 주축으로 전 세계 21개 기관, 33개 이상의 연구실이 협력하여 구축한 사상 최대 규모의 로봇 데이터셋이자 오픈 소스 프로젝트이다.3 이 프로젝트의 핵심 목표는 이질적인 로봇 데이터들을 표준화된 형식으로 통합하여, 특정 로봇이나 작업에 국한되지 않는 범용 로봇 정책(Generalist Robot Policy)을 학습시키는 데 있다.</p>
<h3>2.1  데이터셋의 규모와 비균질성 (Heterogeneity)</h3>
<p><em>Open X-Embodiment</em> 데이터셋의 규모는 기존의 단일 로봇 데이터셋들과는 차원을 달리한다. 이 데이터셋은 22종의 서로 다른 로봇 형태(Embodiment)로부터 수집된 100만 개(1M+) 이상의 궤적(Trajectory)을 포함한다.3 여기에는 단순한 외팔 로봇(Single arm)인 Franka Panda나 UR5뿐만 아니라, 양팔 로봇(Bi-manual), 사족 보행 로봇(Quadruped), 그리고 모바일 매니퓰레이터 등 기구학적 구조가 판이한 다양한 로봇들이 포함되어 있다. 데이터셋은 총 527개의 기술(Skills)과 160,266개의 작업(Tasks)을 포괄하며, 이는 로봇이 수행할 수 있는 행동의 범위를 크게 확장시켰다.3</p>
<p>이 데이터셋의 가장 큰 특징은 단순한 규모의 확장이 아닌 ’비균질성(Heterogeneity)’에 있다. 각 데이터는 서로 다른 조명 조건, 배경, 카메라 시점(Viewpoint), 그리고 객체 분포를 가진 환경에서 수집되었다.1</p>
<ul>
<li><strong>환경적 다양성:</strong> 일부 데이터는 통제된 실험실 테이블 위에서 수집된 반면, 다른 데이터는 실제 주방, 사무실, 또는 공장 환경에서 수집되었다.</li>
<li><strong>작업의 다양성:</strong> 픽앤플레이스(Pick-and-place)와 같은 단순 작업부터, 서랍 열기, 물체 조립하기, 도구 사용하기 등 복잡한 조작 작업이 혼재되어 있다.</li>
<li><strong>데이터 출처:</strong> <em>RoboNet</em>, <em>Bridge Data</em>, <em>Language Table</em> 등 기존에 공개되었던 60여 개의 주요 로봇 데이터셋들이 하나의 저장소(Repository)로 통합되었다.5</li>
</ul>
<p>이러한 다양성은 모델이 특정 환경의 시각적 패턴이나 편향(Bias)에 과적합(Overfitting)되는 것을 방지하고, 물리적 상호작용의 본질적인 인과관계를 학습하도록 유도한다. 이는 컴퓨터 비전 모델이 다양한 이미지를 통해 ’고양이’라는 개념을 일반화하듯, 로봇 모델이 다양한 환경 데이터를 통해 ’집는다(Grasp)’는 행위의 물리적 의미를 일반화하는 과정과 유사하다.</p>
<h3>2.2  RT-X 모델: 범용성을 향한 아키텍처</h3>
<p><em>Open X-Embodiment</em> 협력체는 데이터셋 구축에 그치지 않고, 이를 기반으로 학습된 범용 로봇 제어 모델인 <strong>RT-X</strong>를 공개했다. RT-X는 구글의 기존 로보틱스 트랜스포머 모델인 <em>RT-1</em>과 <em>RT-2</em>를 기반으로 확장된 모델군(Family of Models)을 지칭하며, 각각 <strong>RT-1-X</strong>와 <strong>RT-2-X</strong>로 명명된다.2</p>
<h4>2.2.1  RT-1-X: 효율성과 제어 성능의 균형</h4>
<p><strong>RT-1-X</strong>는 <em>Robotics Transformer 1 (RT-1)</em> 아키텍처를 기반으로 한다. RT-1은 로봇 제어를 위해 설계된 효율적인 트랜스포머 모델로, 고해상도 이미지와 자연어 명령어를 입력받아 이산화된(Discretized) 행동 토큰을 실시간으로 출력한다.6</p>
<ul>
<li><strong>아키텍처 상세:</strong> RT-1-X는 EfficientNet-B3를 백본(Backbone) 네트워크로 사용하여 이미지 특징을 추출한다. 이때 FiLM(Feature-wise Linear Modulation) 레이어를 통해 자연어 명령어 조건(Instruction Conditioning)을 이미지 처리에 주입하여, 로봇이 현재 수행해야 할 작업에 집중하도록 한다.7 이후 <strong>TokenLearner</strong> 모듈을 통해 추출된 이미지 특징 맵을 소수의 토큰(8개)으로 압축하여 트랜스포머 레이어의 연산 부하를 획기적으로 줄였다. 이는 3Hz 이상의 제어 주파수를 유지하면서도 긴 시퀀스(Sequence)를 처리할 수 있게 하는 핵심 기술이다.7</li>
<li><strong>교차 검증 성능:</strong> UC Berkeley, Stanford, NYU 등 5개의 서로 다른 연구실에서 수행된 교차 검증 실험 결과, RT-1-X는 각 연구실의 데이터로만 학습된 모델(Original Method) 대비 평균 <strong>50% 향상된 성공률</strong>을 보였다.4 특히 WidowX와 같이 데이터가 상대적으로 적은 로봇의 경우, Franka와 같은 다른 로봇의 대규모 데이터를 함께 학습했을 때 성능 향상 폭이 가장 컸다. 이는 데이터가 풍부한 도메인의 지식이 데이터가 희소한 도메인으로 전이되는 <strong>‘긍정적 전이(Positive Transfer)’</strong> 현상을 명확히 입증한다.6</li>
</ul>
<h4>2.2.2  RT-2-X: 시각-언어-행동 모델(VLA)의 창발성</h4>
<p><strong>RT-2-X</strong>는 거대 시각-언어 모델(VLM)을 기반으로 한 <em>RT-2</em>를 <em>Open X-Embodiment</em> 데이터셋으로 미세 조정(Fine-tuning)한 모델이다. RT-2-X는 인터넷 규모의 웹 데이터(이미지-텍스트)를 사전 학습(Pre-training)하여 얻은 일반 상식과 추론 능력을 로봇 제어에 활용한다.5</p>
<ul>
<li><strong>모델 규모와 구조:</strong> RT-2-X는 약 550억(55B) 개의 파라미터를 가진 PaLM-E 기반 모델로, 로봇의 행동(Action)을 텍스트 토큰처럼 취급하여 언어 모델이 자연어 문장을 생성하듯이 로봇의 행동 시퀀스를 생성한다.5</li>
<li><strong>창발적 기술(Emergent Skills):</strong> RT-2-X는 기존 RT-2 모델 대비 <strong>3배 더 뛰어난 창발적 능력</strong>을 보여주었다.4 특히 공간적 관계(Spatial Understanding)에 대한 이해도가 비약적으로 상승했다. 예를 들어, “사과를 천 <em>근처</em>로 옮겨라(move apple near cloth)“와 “사과를 천 <em>위</em>로 옮겨라(move apple on cloth)“와 같은 미세한 전치사의 차이를 구분하여 행동을 생성할 수 있다.5 또한, 학습 데이터에 없던 물체나 배경에 대해서도 시각적 일반화 성능을 유지했다. 이는 단순히 모방 학습을 넘어, 언어적 의미를 물리적 행동으로 번역하는 고차원적 추론 능력이 대규모 다중 로봇 데이터 학습을 통해 강화되었음을 시사한다.</li>
</ul>
<h3>2.3  긍정적 전이(Positive Transfer)의 실증과 한계</h3>
<p>RT-X 모델의 실험 결과는 로봇 공학계의 오랜 난제였던 ’일반화’에 대한 중요한 단서를 제공한다. 서로 다른 기구학적 구조를 가진 로봇들이라 할지라도, ’물체를 집는다’거나 ’오른쪽으로 옮긴다’는 행위의 시각적 패턴과 고차원적 의미는 공유될 수 있다는 것이다.</p>
<ul>
<li><strong>성공 사례:</strong> WidowX 로봇의 경우, 자체 데이터만으로 학습했을 때보다 형태가 다른 Franka 로봇의 데이터를 포함하여 학습했을 때 작업 성공률이 현저히 높아졌다.2 이는 데이터의 다양성이 특정 로봇의 데이터 부족 문제를 보완할 수 있음을 의미한다.</li>
<li><strong>실패 사례와 한계:</strong> 그러나 모든 경우에 긍정적 전이가 일어나는 것은 아니다. 최근 연구에 따르면, RT-1-X 모델은 학습 데이터에 포함되지 않은 완전히 새로운 형태의 로봇, 예를 들어 <strong>SCARA(Selective Compliance Assembly Robot Arm)</strong> 로봇에 대해서는 제로샷(Zero-shot) 일반화에 실패했다.7 SCARA 로봇은 수직 축(Z축) 운동이 수평 평면(X-Y) 운동과 기구학적으로 분리된 독특한 구조를 가지는데, RT-1-X는 주로 6자유도(6-DoF) 이상의 다관절 로봇 데이터로 학습되었기 때문에 이러한 기구학적 차이를 극복하지 못한 것으로 분석된다. 미세 조정(Fine-tuning)을 통해서는 성능이 개선되었으나, 이는 여전히 ’형태 불문(Embodiment-agnostic)’의 완전한 일반화에 도달하기 위해서는 기구학적 구조를 명시적으로 고려하는 새로운 아키텍처나 더 다양한 형태의 로봇 데이터가 필요함을 시사한다.10</li>
</ul>
<h2>3.  데이터 표준화의 핵심 인프라: RLDS와 통합 액션 공간</h2>
<p>수십 개의 서로 다른 연구실에서 생성된 데이터를 하나로 묶는 작업은 단순한 파일 취합을 넘어선 고도의 엔지니어링 과제이다. 각 데이터셋은 서로 다른 파일 형식(HDF5, Bag file, MP4 등), 카메라 해상도, 제어 주파수, 그리고 액션 공간 정의를 가지고 있기 때문이다. <em>Open X-Embodiment</em> 프로젝트가 성공할 수 있었던 기술적 배경에는 이러한 혼돈을 정리한 <strong>RLDS</strong>와 <strong>통합 액션 공간</strong>이라는 표준화 기술이 존재한다.</p>
<h3>3.1  RLDS (Reinforcement Learning Datasets) 에코시스템</h3>
<p>구글 리서치(Google Research)에서 제안한 <strong>RLDS</strong>는 순차적 의사결정(Sequential Decision Making) 데이터를 기록, 재생, 조작, 공유하기 위한 표준화된 데이터 형식 및 도구 모음이다.11 RLDS는 텐서플로우 데이터셋(TensorFlow Datasets, TFDS)과 긴밀히 통합되어 있어, 연구자들이 대규모 데이터를 효율적으로 로딩하고 파이프라인을 구축할 수 있도록 지원한다.</p>
<ul>
<li><strong>계층적 데이터 구조:</strong> RLDS는 데이터를 에피소드(Episodes)와 단계(Steps)의 중첩된 데이터셋(Nested Dataset) 구조로 정의한다.13</li>
<li><strong>Episode:</strong> 로봇이 작업을 시작해서 끝날 때까지의 전체 상호작용 시퀀스이다. 각 에피소드는 고유한 ID와 메타데이터(로봇 종류, 환경 설정 등)를 포함할 수 있다.</li>
<li><strong>Step:</strong> 매 순간의 상태 전이(Transition)를 나타내며, <code>observation</code> (이미지, 관절 각도 등 센서 값), <code>action</code> (로봇에게 내려진 제어 명령), <code>reward</code> (보상), <code>discount</code> (할인율) 등의 핵심 필드를 포함한다. 또한 <code>is_first</code>, <code>is_last</code>, <code>is_terminal</code>과 같은 불리언(Boolean) 플래그를 통해 에피소드의 시작과 끝, 그리고 종료 상태 여부를 명확히 구분한다.13</li>
<li><strong>무손실(Lossless) 저장 원칙:</strong> RLDS는 원본 데이터를 손실 없이 저장하는 것을 원칙으로 한다. 데이터가 강화학습(RL), 모방 학습(IL), 또는 오프라인 RL 등 어떤 알고리즘에 사용될지 미리 가정하지 않고, 원시(Raw) 데이터를 그대로 보존함으로써 데이터의 재사용성을 극대화한다.11</li>
<li><strong>변환 파이프라인:</strong> <code>rlds.transformations</code> 라이브러리를 통해 사용자는 원시 데이터를 원하는 형태(예: n-step transition, state stacking)로 자유롭게 변환할 수 있다.15 이는 다양한 알고리즘을 테스트할 때 데이터 전처리에 드는 노력을 획기적으로 줄여준다.</li>
</ul>
<h3>3.2  통합 액션 공간 (Unified Action Space)과 정규화</h3>
<p>서로 다른 로봇 데이터를 통합할 때 가장 큰 기술적 난관은 ’액션 공간의 불일치’이다. 어떤 로봇은 관절 각도(Joint angles)를 직접 제어하고, 어떤 로봇은 엔드 이펙터의 데카르트 좌표(Cartesian coordinates)와 속도를 제어한다. 또한, 그리퍼의 제어 방식도 연속적인 값(0~1)일 수도 있고, 이산적인 값(열림/닫힘)일 수도 있다.</p>
<p><em>Open X-Embodiment</em>는 이를 해결하기 위해 모든 로봇의 액션을 <strong>8차원 벡터</strong>로 변환하는 표준화를 채택했다.5</p>
<ul>
<li>
<p>구성 요소: 7차원(7-DoF) 엔드 이펙터 포즈 + 1차원 그리퍼 상태<br />
<span class="math math-display">
\mathbf{a} = [x, y, z, \text{roll}, \text{pitch}, \text{yaw}, \text{gripper\_opening}, \text{terminate}]
</span><br />
여기서 처음 3개 요소는 엔드 이펙터의 위치 변화(또는 속도), 다음 3개 요소는 회전(오일러 각 또는 쿼터니언 변환), gripper_opening은 그리퍼의 개폐 정도, terminate는 에피소드 종료 신호를 나타낸다.7</p>
</li>
<li>
<p><strong>정규화(Normalization):</strong> 각 차원의 값은 데이터셋 전체의 통계(평균 및 표준편차)를 기반으로 정규화된다. 만약 특정 로봇이 해당 차원(예: 4축 로봇의 roll, pitch)을 물리적으로 제어할 수 없는 경우, 해당 값은 0으로 패딩(Zero-padding)된다.5 이러한 방식은 모델이 입력된 액션 벡터의 의미를 일관되게 해석할 수 있도록 돕는다.</p>
</li>
<li>
<p><strong>한계와 최신 동향:</strong> 이러한 정규화 방식은 대부분의 매니퓰레이터에 유효하지만, 사족 보행 로봇의 다리 제어나 다관절 손(Dexterous Hand)의 복잡한 움직임을 표현하기에는 정보 손실이 발생할 수 있다는 비판이 있다. 이에 따라 최근에는 <strong>Robo-DM</strong>과 같이 비디오, 언어, 액션을 더욱 효율적으로 압축하고 통합 관리할 수 있는 새로운 데이터 관리 프레임워크가 제안되고 있다. <em>Robo-DM</em>은 RLDS 형식 대비 저장 공간을 최대 70배 절약하고 데이터 로딩 속도를 가속화하는 등의 이점을 제공하며 데이터 인프라의 고도화를 이끌고 있다.18</p>
</li>
</ul>
<h2>4.  데이터셋의 다양성과 특화: DROID, RH20T, BridgeData V2</h2>
<p><em>Open X-Embodiment</em>가 데이터의 ’통합’을 통해 범용성을 추구했다면, 개별 데이터셋 프로젝트들은 데이터의 ‘질(Quality)’, ‘환경적 다양성(Environmental Diversity)’, 그리고 ’물리적 상호작용의 깊이(Physical Depth)’를 강화하는 방향으로 진화하고 있다. 특히 <em>DROID</em>, <em>RH20T</em>, <em>BridgeData V2</em>는 각기 다른 철학으로 로봇 데이터의 지평을 확장하고 있다.</p>
<h3>4.1  DROID: 실험실을 벗어난 야생(In-the-Wild)의 로봇</h3>
<p><strong>DROID (Distributed Robot Interaction Dataset)</strong> 는 “로봇을 실험실 밖으로 내보내자“는 철학 하에 구축된 대규모 분산 데이터셋이다.19 기존 데이터셋들이 통제된 실험실 환경에서 고정된 배경과 조명 아래 수집된 것과 달리, DROID는 북미, 아시아, 유럽 등 13개국 18개 연구실이 협력하여 1년 동안 수집한 방대한 데이터를 담고 있다.</p>
<ul>
<li><strong>압도적인 환경 다양성:</strong> DROID는 총 76,000개의 궤적과 350시간 분량의 상호작용 데이터를 포함한다. 가장 주목할 점은 <strong>564개의 서로 다른 장면(Scenes)</strong> 과 <strong>52개의 건물</strong>에서 데이터가 수집되었다는 것이다.20 여기에는 일반 가정집의 주방, 사무실, 세탁실, 공용 라운지 등 실제 생활 환경(Real-world settings)이 다수 포함되어 있다. 이는 로봇이 복잡한 배경, 어지러운 조명, 예기치 못한 시각적 방해물(Distractors)이 존재하는 환경에서도 강건하게 작동할 수 있는 능력을 배양한다.20</li>
<li><strong>하드웨어 표준화 전략:</strong> DROID는 분산 수집의 일관성을 보장하기 위해 모든 참여 기관이 동일한 하드웨어 설정을 사용하도록 강제했다. Franka Panda 7자유도 로봇 암과 Stereolabs의 ZED 2 스테레오 카메라 3대(외부 2대, 손목 1대)를 표준 플랫폼으로 채택하여, 하드웨어 차이로 인한 도메인 격차(Domain Gap)를 최소화하면서도 환경의 다양성을 극대화하는 전략을 취했다.20</li>
<li><strong>성능 향상:</strong> 실험 결과, DROID 데이터로 공동 학습(Co-training)된 정책은 OXE 데이터만 사용했을 때보다 분포 내(In-distribution) 성능에서 22%, 분포 외(Out-of-distribution) 성능에서 17% 향상된 결과를 보였다.20 이는 단순히 데이터의 양(Volume)을 늘리는 것보다, 데이터가 커버하는 환경적 다양성(Variance)을 확보하는 것이 로봇의 일반화 성능에 결정적임을 시사한다.</li>
</ul>
<h3>4.2  RH20T: 정교한 접촉(Contact)과 멀티모달의 융합</h3>
<p><strong>RH20T (Robot-Human 20 Trillion)</strong> 는 로봇 조작의 ’물리적 본질’에 집중한 데이터셋이다. 시각 정보에만 의존하는 기존 데이터셋의 한계를 극복하기 위해, 힘(Force), 토크(Torque), 오디오(Audio) 등 비시각적 모달리티를 대거 포함하여 정교한 조작 기술(Contact-rich manipulation)을 학습할 수 있도록 설계되었다.23</p>
<ul>
<li><strong>고빈도 멀티모달 센서 데이터:</strong> RH20T는 100Hz 이상의 고빈도로 6자유도 힘/토크 센서 데이터를 수집하며, RGBD 카메라, 오디오, 로봇 관절 토크, 그리퍼 너비 등 풍부한 센서 정보를 정밀하게 동기화하여 제공한다.24 이는 조립(Assembly), 절단(Cutting), 닦기(Wiping), 끼우기(Insertion) 등 물체와의 정교한 힘 조절이 필수적인 작업 학습에 결정적인 역할을 한다.</li>
<li><strong>복잡한 작업 난이도:</strong> 단순한 픽앤플레이스를 넘어, 전구 갈아 끼우기, 톱질하기, 헝겊으로 테이블 닦기 등 140개 이상의 복잡하고 현실적인 작업을 포함한다.25 각 에피소드에는 해당 작업을 수행하는 인간의 시연 비디오와 자연어 설명이 쌍(Pair)으로 제공되어, 인간의 행동을 관찰하고 모방하거나 언어로 지시된 작업을 수행하는 모델 연구에 최적화되어 있다.</li>
<li><strong>데이터 품질 관리:</strong> 햅틱 장치(Haptic device)와 페달을 이용한 직관적인 원격 조작 인터페이스를 도입하여, 인간 시연자가 로봇을 자연스럽게 제어하고 고품질의 궤적을 생성할 수 있도록 했다.24</li>
</ul>
<h3>4.3  BridgeData V2: 저비용 하드웨어와 확장성의 실험</h3>
<p><strong>BridgeData V2</strong>는 “누구나 데이터를 생산하고 활용할 수 있는가?“라는 질문에 답하기 위해 시작된 프로젝트이다. 고가의 산업용 로봇 대신 접근성이 높은 저비용 하드웨어인 WidowX 로봇을 활용하여 구축되었다.27</p>
<ul>
<li><strong>확장성(Scalability)과 접근성:</strong> 수천만 원을 호가하는 로봇 대신 수백만 원 대의 로봇을 사용함으로써, 더 많은 연구자와 학생들이 데이터 수집에 참여할 수 있는 기반을 마련했다. 24개 환경에서 수집된 60,096개의 궤적을 포함하며, 이는 이전 버전인 Bridge Dataset보다 7배 이상 큰 규모이다.28</li>
<li><strong>다중 작업 일반화:</strong> BridgeData V2는 목표 이미지(Goal Image)나 자연어 명령을 통한 다중 작업 학습(Multi-task Learning)을 지원하도록 설계되었다. 실험 결과, 데이터의 다양성이 증가할수록 새로운 환경과 물체에 대한 일반화 성능이 선형적으로 향상됨을 확인했으며, 이는 “데이터의 다양성이 모델의 크기만큼이나 중요하다“는 가설을 뒷받침하는 중요한 근거가 되었다.27</li>
</ul>
<h2>5.  분석 및 시사점: 로봇 데이터 생태계의 진화 방향</h2>
<p>지금까지 살펴본 대규모 데이터셋 프로젝트들은 로봇 인공지능이 나아가야 할 방향을 명확히 보여준다. 이들 프로젝트가 시사하는 바는 다음과 같다.</p>
<p>첫째, <strong>데이터의 통합과 표준화는 선택이 아닌 필수</strong>가 되었다. <em>Open X-Embodiment</em>와 RLDS는 개별 연구의 산출물을 공공재로 전환하는 플랫폼 역할을 하고 있다. 앞으로의 로봇 연구는 독자적인 데이터셋 구축에 매몰되기보다는, 기존의 거대 데이터셋에 기여하고 이를 활용하여 모델을 사전 학습(Pre-training)한 뒤, 소량의 데이터로 미세 조정(Fine-tuning)하는 방식이 표준으로 자리 잡을 것이다.29</p>
<p>둘째, <strong>’다양성’의 정의가 다차원적으로 확장되고 있다.</strong> 과거의 다양성이 단순히 물체의 색상이나 모양을 바꾸는 수준이었다면, <em>DROID</em>는 조명, 배경, 가구 배치 등 ’환경적 맥락(Context)’의 다양성을, <em>RH20T</em>는 힘과 소리 등 ’물리적 상호작용’의 다양성을, <em>Open X-Embodiment</em>는 로봇 ’형태(Embodiment)’의 다양성을 강조한다. 진정한 범용 로봇(Generalist Robot)은 이러한 모든 축에서의 다양성을 학습해야만 실세계의 불확실성에 대응할 수 있다.20</p>
<p>셋째, <strong>평가 지표와 데이터 혼합(Data Mixture) 전략의 고도화가 필요하다.</strong> 단순히 학습된 작업의 성공률을 측정하는 것을 넘어, RT-X의 실험에서처럼 ’창발적 기술’의 발현 여부나, 전혀 다른 형태의 로봇으로의 제로샷 전이 성능 등 일반화 능력을 정량적으로 평가할 수 있는 새로운 벤치마크가 요구된다. 또한, 무조건 많은 데이터를 섞는 것이 능사가 아니라, 데이터의 질과 분포를 고려하여 최적의 비율로 혼합하는 <strong>Re-Mix</strong>와 같은 데이터 큐레이션 기술이 중요해질 것이다.31</p>
<p>결론적으로, 데이터의 민주화는 로봇 공학을 ’하드웨어 및 제어 중심’에서 ’데이터 및 소프트웨어 중심’의 학문으로 변모시키고 있다. <em>Open X-Embodiment</em>를 위시한 대규모 데이터셋 프로젝트들은 로봇에게 세상을 이해하고 조작할 수 있는 ’상식’을 가르치는 교과서가 되고 있으며, 이는 머지않은 미래에 등장할 진정한 의미의 ’로봇 파운데이션 모델’의 초석이 될 것이다.</p>
<h3>5.1 요약 테이블: 주요 대규모 로봇 데이터셋 비교</h3>
<table><thead><tr><th><strong>데이터셋</strong></th><th><strong>규모 (궤적/장면)</strong></th><th><strong>로봇 형태 (Embodiment)</strong></th><th><strong>주요 특징 및 기여</strong></th><th><strong>센서 모달리티</strong></th></tr></thead><tbody>
<tr><td><strong>Open X-Embodiment</strong></td><td>1M+ 궤적 / 다양함</td><td>22종 (Franka, UR, WidowX 등)</td><td>전 세계 데이터 통합, RT-X 모델 공개, 긍정적 전이 입증 3</td><td>RGB, 언어, 액션 (통합 8-DoF)</td></tr>
<tr><td><strong>DROID</strong></td><td>76k 궤적 / 564 장면</td><td>Franka Panda (단일 표준)</td><td>야생(In-the-Wild) 환경, 분산 수집, 높은 환경적 강건성 20</td><td>스테레오 RGB (3대), 깊이, 액션</td></tr>
<tr><td><strong>RH20T</strong></td><td>110k 궤적 / 140+ 작업</td><td>다양함 (Flexiv, UR 등)</td><td>고빈도(100Hz) 물리 데이터, 정교한 접촉 기반 조작 23</td><td>RGBD, <strong>Force/Torque</strong>, Audio, Tactile</td></tr>
<tr><td><strong>BridgeData V2</strong></td><td>60k 궤적 / 24 환경</td><td>WidowX 250</td><td>저비용 하드웨어, 확장성, 다중 작업 일반화 연구 27</td><td>RGBD, 언어, 액션</td></tr>
</tbody></table>
<p>이러한 데이터셋들은 상호 배타적이지 않으며, 오히려 상호 보완적이다. OXE가 로봇 학습의 넓은 범위(Breadth)를 커버한다면, DROID는 환경적 깊이(Depth)를, RH20T는 물리적 정밀함(Precision)을 더해준다. 향후 연구는 이러한 이질적인 데이터셋을 얼마나 효과적으로 융합하여 최적의 정책을 학습시키느냐에 달려있다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Open X-Embodiment: Robotic Learning Datasets and RT-X Models, <a href="https://autolab.berkeley.edu/assets/publications/media/Open_X_Embodiment__Robotic_Learning_Datasets_and_RT_X_Models%20(1).pdf">https://autolab.berkeley.edu/assets/publications/media/Open_X_Embodiment__Robotic_Learning_Datasets_and_RT_X_Models%20(1).pdf</a></li>
<li>Open X-Embodiment: Robotic Learning Datasets and RT-X Models, https://arxiv.org/html/2310.08864v4</li>
<li>Open X-Embodiment: Robotic learning datasets and RT-X models, https://assistive-autonomy.ed.ac.uk/project/feature-publication-oxe/</li>
<li>Scaling up learning across many different robot types, https://deepmind.google/blog/scaling-up-learning-across-many-different-robot-types/</li>
<li>Open X-Embodiment: Robotic Learning Datasets and RT-X Models, https://robotics-transformer-x.github.io/</li>
<li>Open X-Embodiment: Robotic Learning Datasets and RT-X Models, https://www.robot-learning.ml/2023/files/paper18.pdf</li>
<li>arXiv:2409.03299v1 [cs.RO] 5 Sep 2024, <a href="https://arxiv.org/pdf/2409.03299">https://arxiv.org/pdf/2409.03299?</a></li>
<li>Open X-Embodiment: Towards a generic robot learning | AI-SCHOLAR, https://ai-scholar.tech/en/articles/robot%2Fopen-x-embodiment</li>
<li>Bringing the RT-1-X Foundation Model to a SCARA robot, https://www.researchgate.net/publication/383791890_Bringing_the_RT-1-X_Foundation_Model_to_a_SCARA_robot</li>
<li>Bringing the RT-1-X Foundation Model for Robotic Control to New …, https://staff.fnwi.uva.nl/a.visser/education/masterProjects/SE_master_thesis_Jonathan_Salzer.pdf</li>
<li>RLDS: An Ecosystem to Generate, Share, and Use Datasets in …, https://research.google/blog/rlds-an-ecosystem-to-generate-share-and-use-datasets-in-reinforcement-learning/</li>
<li>RLDS: an Ecosystem to Generate, Share and Use Datasets … - arXiv, https://arxiv.org/abs/2111.02767</li>
<li>(PDF) RLDS: an Ecosystem to Generate, Share and Use Datasets in …, https://www.researchgate.net/publication/355926053_RLDS_an_Ecosystem_to_Generate_Share_and_Use_Datasets_in_Reinforcement_Learning</li>
<li>google-research/rlds - GitHub, https://github.com/google-research/rlds</li>
<li>RLDS: Tutorial - Colab - Google, https://colab.research.google.com/github/google-research/rlds/blob/main/rlds/examples/rlds_tutorial.ipynb</li>
<li>Bringing the RT-1-X Foundation Model to a SCARA robot - arXiv, https://arxiv.org/html/2409.03299</li>
<li>google-deepmind/open_x_embodiment - GitHub, https://github.com/google-deepmind/open_x_embodiment</li>
<li>Robo-DM: Data Management For Large Robot Datasets, https://www.researchgate.net/publication/391954107_Robo-DM_Data_Management_For_Large_Robot_Datasets</li>
<li>DROID A Large-Scale in-The-Wild Robot Manipulation Dataset | PDF, https://www.scribd.com/document/885374941/DROID-a-Large-Scale-in-The-Wild-Robot-Manipulation-Dataset</li>
<li>DROID: A Large-Scale In-the-Wild Robot Manipulation Dataset, https://droid-dataset.github.io/</li>
<li>DROID: In-The-Wild Robot Manipulation Dataset - Emergent Mind, https://www.emergentmind.com/papers/2403.12945</li>
<li>DROID case study: How Stereolabs ZED cameras power …, https://www.stereolabs.com/blog/droid-case-study-robotics-research</li>
<li>RH20T-P: A Primitive-Level Robotic Manipulation Dataset … - arXiv, https://arxiv.org/html/2403.19622v2</li>
<li>RH20T: A Comprehensive Robotic Dataset for Learning Diverse …, https://rh20t.github.io/</li>
<li>RH20T: A Comprehensive Robotic Dataset for Learning Diverse …, https://www.robot-learning.ml/2023/files/paper51.pdf</li>
<li>A Comprehensive Robotic Dataset for Learning Diverse Skills in …, https://rh20t.github.io/static/RH20T_paper_compressed.pdf</li>
<li>BridgeData V2: A Dataset for Robot Learning at Scale, https://proceedings.mlr.press/v229/walke23a/walke23a.pdf</li>
<li>BridgeData V2: A Dataset for Robot Learning at Scale - arXiv, https://arxiv.org/html/2308.12952v3</li>
<li>Octo: An Open-Source Generalist Robot Policy - SciSpace, https://scispace.com/pdf/octo-an-open-source-generalist-robot-policy-3t5668mqca.pdf</li>
<li>“How Real-World Cross-Embodiment Data Will Lead to Robotic …, https://medium.com/@jianming.wang07/robotic-foundation-models-corl-2024-sergey-levines-talk-notes-e42bb3eb618e</li>
<li>Optimizing Data Mixtures for Large Scale Imitation Learning - GitHub, https://raw.githubusercontent.com/mlresearch/v270/main/assets/hejna25a/hejna25a.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>