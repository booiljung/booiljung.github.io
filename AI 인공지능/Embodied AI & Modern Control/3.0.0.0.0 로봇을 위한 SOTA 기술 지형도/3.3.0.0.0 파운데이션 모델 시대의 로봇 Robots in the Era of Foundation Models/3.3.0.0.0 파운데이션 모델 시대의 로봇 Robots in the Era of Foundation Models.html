<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.3 파운데이션 모델 시대의 로봇 (Robots in the Era of Foundation Models)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.3 파운데이션 모델 시대의 로봇 (Robots in the Era of Foundation Models)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 3. 로봇을 위한 SOTA 기술 지형도</a> / <a href="index.html">3.3 파운데이션 모델 시대의 로봇 (Robots in the Era of Foundation Models)</a> / <span>3.3 파운데이션 모델 시대의 로봇 (Robots in the Era of Foundation Models)</span></nav>
                </div>
            </header>
            <article>
                <h1>3.3 파운데이션 모델 시대의 로봇 (Robots in the Era of Foundation Models)</h1>
<h2>1.  서론: 로봇 학습의 패러다임 전환</h2>
<p>인공지능(AI)과 로보틱스의 융합은 지난 수십 년간 점진적인 발전을 거듭해 왔으나, 최근 몇 년 사이 그 양상은 과거와는 본질적으로 다른 근본적인 ’패러다임 전환(Paradigm Shift)’을 맞이하고 있다. 전통적인 로봇 공학이 제어 이론과 정밀한 프로그래밍에 기반을 두었다면, 2010년대 중반 이후의 로봇 학습은 딥러닝(Deep Learning)을 통해 데이터로부터 기능을 습득하는 방식으로 진화했다. 그러나 이 시기의 딥러닝 기반 로봇들, 즉 ‘전문가(Specialist)’ 모델들은 특정 작업(Task-Specific)과 통제된 환경(Controlled Environment)이라는 좁은 경계 안에서만 유효했다. 공장 자동화 라인이나 연구실의 고정된 테이블 위에서는 놀라운 성능을 발휘했지만, 조명이 바뀌거나 다루어야 할 물체의 위치가 조금만 변경되어도, 혹은 이전에 보지 못한 새로운 물체가 등장하면 그 성능은 급격히 저하되었다.1 이는 로봇이 세상에 대한 일반적인 이해(Common Sense)나 의미론적 맥락(Semantic Context)을 파악하지 못한 채, 픽셀 데이터와 모터 제어 신호 간의 좁은 상관관계만을 기계적으로 학습했기 때문이다.</p>
<p>그러나 2020년대 들어 대규모 언어 모델(LLM)과 비전-언어 모델(VLM)로 대표되는 **파운데이션 모델(Foundation Models)**의 등장은 이러한 한계를 돌파할 새로운 가능성을 제시했다. 파운데이션 모델은 인터넷 규모의 텍스트, 이미지, 비디오 등 방대한 데이터를 통해 사전 학습(Pre-training)되어 있어, 로봇이 직접 경험하지 못한 상황에서도 방대한 사전 지식을 바탕으로 추론하고 해결책을 제시할 수 있는 능력을 제공한다. 이는 로봇에게 ’상식’과 ’추론 능력’을 부여하여, 닫힌 환경의 전문가가 아닌 열린 환경의 ‘범용(Generalist)’ 로봇으로 진화시키는 핵심 동력이 되고 있다.2</p>
<p>본 장에서는 파운데이션 모델이 로보틱스에 통합되는 과정을 심도 있게 분석한다. 단순히 언어 모델을 로봇의 두뇌로 사용하는 것을 넘어, 시각적 인식과 언어적 이해, 그리고 물리적 행동을 하나의 신경망 안에서 통합한 <strong>비전-언어-행동(VLA, Vision-Language-Action) 모델</strong>의 기술적 구조와 발전 양상을 살펴본다. 또한, 이러한 모델의 성능을 결정짓는 데이터의 혁명적 변화—교차 신체(Cross-Embodiment) 데이터셋의 구축과 데이터 스케일링 법칙—을 고찰하고, 물리적 지능(Physical Intelligence)을 구현하기 위한 최신 아키텍처인 월드 모델(World Models)과 유동 매칭(Flow Matching) 기술을 탐구한다. 마지막으로, 실시간성, 안전성, 그리고 평가의 어려움 등 여전히 남아 있는 기술적 난제들과 이를 극복하기 위한 연구 커뮤니티의 노력을 종합적으로 조망한다.</p>
<h2>2.  비전-언어-행동(VLA) 모델: 인지와 행동의 통합</h2>
<p>로봇 제어의 역사에서 인지(Perception)와 행동(Action)은 오랫동안 분리된 모듈로 다루어져 왔다. 인식 모듈이 카메라를 통해 세상을 파악하고 상태를 추정하면, 계획 모듈이 경로를 생성하고, 제어 모듈이 모터를 구동하는 식이었다. 그러나 VLA 모델은 이 모든 과정을 하나의 거대 신경망(End-to-End Neural Network)으로 통합하려는 시도이다. 이는 텍스트나 이미지 토큰을 처리하는 트랜스포머(Transformer) 아키텍처가 로봇의 물리적 행동까지도 하나의 ’언어’처럼 처리할 수 있다는 발상에서 시작되었다.4</p>
<h3>2.1  RT-2: 행동을 토큰화하다 (Tokenizing Actions)</h3>
<p>구글 딥마인드(Google DeepMind)가 2023년 발표한 **RT-2(Robotic Transformer 2)**는 VLA 모델의 효시이자, 파운데이션 모델이 로봇 제어에 직접적으로 적용될 수 있음을 증명한 기념비적인 연구이다. RT-2의 가장 혁신적인 점은 로봇의 행동을 자연어 처리 모델이 이해할 수 있는 형태, 즉 텍스트 토큰으로 변환했다는 데 있다.</p>
<ul>
<li><strong>아키텍처 및 백본:</strong> RT-2는 바닥부터 새로 학습된 모델이 아니다. 이미 인터넷상의 방대한 데이터로 학습된 거대 비전-언어 모델(VLM)인 PaLI-X(5B, 55B 파라미터)와 PaLM-E(12B 파라미터)를 백본(Backbone)으로 사용한다. 이 거대 모델들은 이미 “사과가 무엇인지”, “떨어지려는 물건을 잡으려면 어떻게 해야 하는지“에 대한 시각적, 의미론적 지식을 갖추고 있다.5</li>
<li><strong>행동의 이산화(Discretization):</strong> 로봇의 팔이 움직이는 6자유도(x, y, z 위치 및 회전)와 그리퍼의 개폐 상태는 본래 연속적인(Continuous) 실수값이다. RT-2는 이를 처리하기 위해 각 차원을 256개의 구간(Bin)으로 나누어 이산화했다. 예를 들어, 로봇 팔을 앞으로 1cm 움직이는 행동은 특정 정수 값으로 변환된다. 이 256개의 값은 언어 모델의 어휘(Vocabulary) 중 잘 사용되지 않는 256개의 토큰에 매핑된다. 따라서 모델 입장에서는 “사과를 집어라“라는 텍스트 입력에 대해 “토큰_12, 토큰_88, 토큰_250…“과 같은 일련의 토큰을 출력하는 것이지만, 로봇 입장에서는 이것이 물리적인 움직임으로 해석되어 실행된다.7</li>
<li><strong>공동 미세 조정(Co-fine-tuning)과 긍정적 전이:</strong> RT-2 학습의 핵심은 로봇 데이터만을 사용하지 않는다는 것이다. 로봇 데이터(RT-1 데이터셋 등)와 기존의 웹 기반 비전-언어 데이터를 섞어서 공동으로 학습시킨다. 만약 로봇 데이터만으로 미세 조정(Fine-tuning)을 수행할 경우, 모델은 기존에 가지고 있던 일반적인 상식을 잊어버리는 ’파국적 망각(Catastrophic Forgetting)’을 겪을 수 있다. 웹 데이터를 함께 학습시킴으로써 RT-2는 인터넷에서 배운 추론 능력을 유지하면서도 로봇 제어 능력을 습득한다.8</li>
</ul>
<p>RT-2의 성능은 놀라웠다. 학습 데이터에 한 번도 등장하지 않은 새로운 물체나 명령어도 처리할 수 있는 **창발적 능력(Emergent Capabilities)**을 보였기 때문이다. 예를 들어, “슈퍼히어로 피규어를 멸종된 동물 옆으로 옮겨라“라는 명령을 받았을 때, 로봇 데이터셋에는 ’멸종된 동물’이라는 레이블이 붙은 데이터가 전무했다. 하지만 RT-2는 VLM의 지식을 통해 ’공룡’이 멸종된 동물임을 추론하고, 공룡 피규어 옆으로 물체를 옮기는 데 성공했다. 또한, “지친 사람에게 줄 음료를 골라라“라는 모호한 명령에 대해서도 ’에너지 드링크’를 선택하는 의미론적 추론(Semantic Reasoning)을 수행했다. 이는 로봇이 단순한 기계적 반복을 넘어 맥락을 이해하는 지능형 에이전트로 진화했음을 시사한다.5</p>
<h3>2.2  OpenVLA와 Octo: 개방형 생태계와 효율성의 추구</h3>
<p>RT-2가 VLA의 가능성을 열었지만, 거대한 모델 크기와 비공개성(Closed Source), 그리고 느린 추론 속도는 학계와 산업계의 접근을 어렵게 했다. 이에 대응하여 모델을 경량화하고 효율성을 높이며, 소스 코드를 공개하여 생태계를 확장하려는 노력이 <strong>OpenVLA</strong>와 <strong>Octo</strong>를 통해 구체화되었다.</p>
<p>OpenVLA: 접근 가능한 범용 모델</p>
<p>스탠퍼드 대학교와 UC 버클리 연구진 등이 주도하여 개발한 OpenVLA는 70억 파라미터(7B) 규모의 오픈소스 VLA 모델이다. RT-2와 유사하게 행동을 토큰화하여 처리하지만, 기반 아키텍처로 강력한 오픈소스 언어 모델인 Llama 2를 채택했다. 시각 정보 처리를 위해서는 DINOv2와 SigLIP이라는 두 가지 최첨단 비전 인코더를 결합하여 사용함으로써, 물체의 기하학적 특징과 의미론적 특징을 동시에 포착한다.9</p>
<p>OpenVLA의 가장 큰 기여는 <strong>효율성</strong>과 <strong>접근성</strong>이다.</p>
<ul>
<li><strong>소비자용 GPU에서의 구동:</strong> 거대한 TPU 포드(Pod)가 필요한 RT-2와 달리, OpenVLA는 양자화(Quantization) 기술을 통해 일반적인 소비자용 GPU(예: NVIDIA RTX 3090, 4090) 한 장에서도 추론이 가능하다.10</li>
<li><strong>효율적 미세 조정(LoRA):</strong> 새로운 로봇이나 작업에 적응시키기 위해 모델 전체를 다시 학습할 필요 없이, LoRA(Low-Rank Adaptation)와 같은 파라미터 효율적 미세 조정(PEFT) 기법을 지원한다. 이는 연구자들이 적은 비용으로 자신만의 VLA 모델을 커스터마이징할 수 있게 만들었다.9</li>
</ul>
<p>Octo: 트랜스포머와 디퓨전의 만남</p>
<p>UC 버클리의 Octo 모델은 행동 생성 방식에서 근본적인 차별점을 둔다. RT-2나 OpenVLA가 행동을 이산적인 토큰으로 예측하는 것과 달리, Octo는 **디퓨전 정책(Diffusion Policy)**을 채택했다. 디퓨전 모델은 이미지 생성 AI(예: Stable Diffusion)에서 사용되는 기술로, 노이즈로부터 점진적으로 데이터를 생성해낸다. 로보틱스에서 이는 연속적인(Continuous) 행동 분포를 생성하는 데 사용된다.12</p>
<ul>
<li><strong>멀티모달 분포(Multimodal Distribution) 표현:</strong> 로봇의 행동은 한 가지 정답만 있는 것이 아니다. 장애물을 피할 때 왼쪽으로 갈 수도 있고 오른쪽으로 갈 수도 있다. 이산적인 토큰 예측이나 단순 회귀(Regression) 모델은 이러한 여러 가능성의 평균값을 출력하여 엉뚱한 행동(예: 장애물 정면으로 돌진)을 할 위험이 있다. 반면 디퓨전 헤드를 장착한 Octo는 이러한 멀티모달 분포를 효과적으로 표현할 수 있다.</li>
<li><strong>유연한 입력 구조:</strong> Octo는 블록 단위의 어텐션(Block-wise Attention) 구조를 사용하여, 손목 카메라(Wrist Camera)가 있든 없든, 혹은 힘 센서 데이터가 추가되든 상관없이 유연하게 입력 구성을 변경할 수 있도록 설계되었다. 이는 다양한 형태의 로봇에 쉽게 적용될 수 있는 범용성을 제공한다.14</li>
</ul>
<table><thead><tr><th><strong>모델명</strong></th><th><strong>주요 개발 주체</strong></th><th><strong>백본 아키텍처</strong></th><th><strong>행동 생성 방식</strong></th><th><strong>특징 및 의의</strong></th></tr></thead><tbody>
<tr><td><strong>RT-2</strong></td><td>Google DeepMind</td><td>PaLM-E, PaLI-X</td><td>이산 토큰 (Discrete Tokens)</td><td>VLA의 효시, 웹 지식 전이, 강력한 추론 및 제로샷 능력, 비공개 모델</td></tr>
<tr><td><strong>OpenVLA</strong></td><td>Stanford, Berkeley</td><td>Llama 2, DINOv2, SigLIP</td><td>이산 토큰 (Discrete Tokens)</td><td>오픈소스, Llama 2 기반, 소비자용 GPU 구동 가능(효율성), LoRA 지원</td></tr>
<tr><td><strong>Octo</strong></td><td>UC Berkeley</td><td>Transformer</td><td>디퓨전 헤드 (Diffusion Head)</td><td>연속적 행동 생성, 멀티모달 분포 표현 강점, 유연한 입력 구조</td></tr>
<tr><td><strong>Pi0</strong></td><td>Physical Intelligence</td><td>PaliGemma</td><td>유동 매칭 (Flow Matching)</td><td>50Hz 고속 제어, 유동 매칭을 통한 고정밀 연속 제어, VLM과 운동 제어의 결합</td></tr>
</tbody></table>
<p>표 3.3.1 주요 파운데이션 기반 로봇 모델(VLA) 비교 15</p>
<h2>3.  물리적 지능을 위한 차세대 아키텍처: 월드 모델과 유동 매칭</h2>
<p>VLA 모델이 인식과 행동을 통합했다면, 최근의 연구는 로봇이 물리 법칙을 내재화하고 더 빠르고 정교하게 움직일 수 있도록 하는 데 초점을 맞추고 있다. 이는 **월드 모델(World Models)**과 **유동 매칭(Flow Matching)**이라는 두 가지 키워드로 요약된다.</p>
<h3>3.1  Covariant RFM-1: 물리학을 이해하고 미래를 생성하다</h3>
<p>Covariant 사가 발표한 **RFM-1(Robotics Foundation Model 1)**은 로봇을 위한 ’물리 월드 모델’을 표방한다. 기존 모델들이 “현재 상황에서 무엇을 해야 하는가?“에 집중했다면, RFM-1은 “내가 이 행동을 하면 세상이 어떻게 변할 것인가?“를 예측하는 데 중점을 둔다.17</p>
<ul>
<li><strong>비디오 예측(Video Prediction)을 통한 물리 이해:</strong> RFM-1은 텍스트, 이미지, 센서 데이터뿐만 아니라 비디오 프레임을 생성할 수 있다. 로봇이 물체를 밀거나 집어 올리는 행동을 계획할 때, 모델은 그 결과로 나타날 미래의 비디오를 생성해낸다. 만약 생성된 미래 영상에서 물체가 미끄러지거나 깨지는 것이 예측된다면, 로봇은 행동을 수정할 수 있다. 이는 복잡한 물리 시뮬레이터 없이도 로봇이 자신의 행동 결과를 ’상상’하고 계획(Planning)할 수 있게 만든다.18</li>
<li><strong>Any-to-Any 시퀀스 모델링:</strong> RFM-1은 입력과 출력의 모달리티에 구애받지 않는다. 텍스트로 비디오를 검색할 수도 있고, 비디오 상황을 텍스트로 설명할 수도 있으며, 현재 이미지와 목표 이미지를 주면 필요한 행동을 생성할 수도 있다. 이는 로봇 시스템의 디버깅과 인간-로봇 상호작용(HRI)에 혁명을 가져왔다. 로봇이 작업에 실패했을 때, 사용자가 “왜 실패했니?“라고 물으면 로봇이 “물체가 너무 미끄러워서 그리퍼가 놓쳤습니다“라고 대답하거나, 사용자가 “물체의 위쪽 2cm 지점을 잡아봐“라고 텍스트로 조언하면 이를 즉시 행동에 반영하는 것이 가능하다.20</li>
</ul>
<h3>3.2  Physical Intelligence π0 (Pi-Zero): 속도와 정교함의 한계를 넘다</h3>
<p>기존의 대형 VLA 모델들(RT-2 등)은 추론 속도가 1~3Hz(초당 1~3회) 수준으로 매우 느리다는 치명적인 단점이 있었다. 이는 정적인 작업에는 문제가 없지만, 흔들리는 물체를 잡거나 쏟아지는 액체를 받는 등 민첩성이 필요한 작업에는 부적합했다. <strong>Physical Intelligence</strong> 사가 개발한 **π0(Pi-Zero)**는 이러한 한계를 극복하기 위해 <strong>유동 매칭(Flow Matching)</strong> 기술을 도입했다.21</p>
<ul>
<li><strong>유동 매칭(Flow Matching):</strong> 이는 디퓨전 모델의 수학적 일반화 형태로, 노이즈 분포를 데이터 분포로 변환하는 최적의 경로(Vector Field)를 학습한다. 기존 디퓨전 모델보다 학습과 추론 효율이 뛰어나며, 연속적인 행동 공간에서 매우 부드럽고 정교한 궤적을 생성할 수 있다.</li>
<li><strong>아키텍처:</strong> π0는 구글의 <strong>PaliGemma</strong> (3B 파라미터) VLM을 백본으로 사용하되, 여기에 <strong>행동 전문가(Action Expert)</strong> 모듈을 결합했다. VLM은 이미지와 텍스트를 통해 고수준의 의미론적 추론(System 2)을 담당하고, 행동 전문가 모듈은 유동 매칭을 통해 초당 50회(50Hz)의 고속 제어 신호(System 1)를 생성한다. 이를 통해 π0는 빨래 개기(Laundry Folding), 테이블 정리(Bussing), 박스 조립 등 인간 수준의 손기술(Dexterity)이 필요한 작업을 부드럽게 수행할 수 있다.22</li>
<li><strong>지연 시간(Latency) 해결:</strong> 기존 VLA가 전체 모델을 매 스텝마다 실행하느라 느렸던 반면, π0는 텍스트와 이미지 처리는 VLM 백본이 담당하고, 빠른 주기의 모터 제어는 경량화된 행동 전문가가 담당하는 구조적 이점을 통해 200ms 이상의 지연을 20ms 수준(최적화 시)으로 단축할 수 있는 잠재력을 보여주었다.24</li>
</ul>
<h2>4.  데이터의 혁명: 교차 신체(Cross-Embodiment)와 스케일링 법칙</h2>
<p>“데이터가 곧 성능이다“라는 명제는 AI 분야의 불문율이다. 하지만 로보틱스에서 데이터는 텍스트나 이미지처럼 인터넷에서 쉽게 긁어모을 수 있는 자원이 아니다. 로봇 하드웨어는 비싸고, 고장 나기 쉬우며, 데이터를 수집하려면 물리적인 시간이 소요된다. 또한, 로봇마다 팔의 개수, 관절의 구조, 카메라의 위치가 제각각인 <strong>신체 불일치(Embodiment Mismatch)</strong> 문제는 데이터 통합의 가장 큰 걸림돌이었다. 그러나 최근 로봇 연구 커뮤니티는 이러한 장벽을 무너뜨리는 거대한 협력과 데이터 스케일링의 시대를 열었다.</p>
<h3>4.1  Open X-Embodiment (OXE): 로보틱스의 ImageNet 모멘트</h3>
<p>구글 딥마인드를 필두로 전 세계 33개 이상의 연구소가 협력하여 구축한 <strong>Open X-Embodiment(OXE)</strong> 데이터셋은 로봇 학습의 역사적인 전환점이다. OXE는 22종의 서로 다른 로봇(Franka, UR5, WidowX 등)에서 수집된 100만 개 이상의 에피소드를 하나의 표준화된 포맷(RLDS)으로 통합했다.25</p>
<ul>
<li><strong>긍정적 전이(Positive Transfer):</strong> OXE 프로젝트의 가장 중요한 발견은 서로 다른 로봇의 데이터를 섞어서 학습해도 성능이 저하되지 않고 오히려 향상된다는 점이다. 예를 들어, 외팔 로봇의 데이터를 학습한 모델이 양팔 로봇의 제어 능력을 향상시키는 데 기여한다. OXE 데이터로 학습된 <strong>RT-X</strong> 모델은 단일 로봇 데이터로만 학습된 모델 대비 평균 50% 이상의 성능 향상을 기록했다.27 이는 로봇 데이터에도 ’규모의 경제’가 적용됨을 입증한 것이다.</li>
</ul>
<h3>4.2  DROID와 AgiBot World: 다양성과 규모의 확장</h3>
<p>OXE 이후, 데이터의 다양성(Diversity)과 규모(Scale)를 더욱 극한으로 밀어붙이는 시도들이 등장했다.</p>
<ul>
<li><strong>DROID (Distributed Robot Interaction Dataset):</strong> 기존 데이터셋이 대부분 대학 연구실이라는 통제된 환경에서 수집된 반면, DROID는 ’야생(In-the-wild)’을 지향한다. 북미, 아시아, 유럽의 50개 이상의 기관이 참여하여 12개월간 564개의 다양한 장면(가정집 주방, 사무실 책상, 욕실 등)에서 76,000개의 궤적을 수집했다. DROID로 학습된 정책은 조명 변화나 배경의 복잡함에 대해 훨씬 강력한 강건성(Robustness)을 보이며, 분포 외(OOD) 환경에서의 성공률을 17% 이상 향상시켰다.28</li>
<li><strong>AgiBot World:</strong> 중국의 Zhiyuan Robotics가 주도한 이 프로젝트는 데이터의 ’규모’와 ’숙련도’에 집중했다. 100대 이상의 로봇을 동시에 운용하여 100만 개 이상의 궤적을 수집했으며, 이는 OXE를 능가하는 규모이다. 특히 단순한 그리퍼(Gripper)뿐만 아니라 고자유도 영리한 손(Dexterous Hand)을 사용한 데이터와 산업 현장, 소매점 등 실제 서비스 환경 데이터를 대거 포함하고 있어, 보다 정교하고 긴 호흡의 작업(Long-horizon Tasks)을 수행하는 모델 개발을 목표로 한다.31</li>
</ul>
<table><thead><tr><th><strong>데이터셋 명</strong></th><th><strong>주도 기관</strong></th><th><strong>데이터 규모</strong></th><th><strong>특징 및 차별점</strong></th><th><strong>주요 기여</strong></th></tr></thead><tbody>
<tr><td><strong>Open X-Embodiment (OXE)</strong></td><td>DeepMind 등 33개 기관</td><td>1M+ 에피소드, 22종 로봇</td><td>최초의 대규모 교차 신체 데이터 통합</td><td>교차 신체 학습의 긍정적 전이 입증 (RT-X)</td></tr>
<tr><td><strong>DROID</strong></td><td>Stanford, Berkeley 등</td><td>76k 에피소드, 564 장면</td><td>야생(In-the-wild) 환경, 높은 배경/조명 다양성</td><td>환경 강건성(Robustness) 및 OOD 성능 향상</td></tr>
<tr><td><strong>AgiBot World</strong></td><td>Zhiyuan Robotics</td><td>1M+ 에피소드, 100+ 로봇</td><td>산업용/서비스용 시나리오, 덱스터러스 핸드 데이터</td><td>고정밀 조작 및 장기 작업(Long-horizon) 성능</td></tr>
</tbody></table>
<p><em>표 3.3.2 주요 로봇 학습 대규모 데이터셋 비교</em></p>
<h3>4.3  로보틱스 스케일링 법칙 (Scaling Laws)</h3>
<p>LLM에서 모델 크기와 데이터 양이 늘어날수록 성능이 멱법칙(Power Law)을 그리며 향상된다는 것은 잘 알려진 사실이다. 그렇다면 로봇의 물리적 지능에도 이 법칙이 적용될까? 최신 연구들은 “그렇다“라고 답한다.</p>
<ul>
<li><strong>데이터 스케일링:</strong> Sartor와 Thompson(2024)의 메타 분석에 따르면, 로봇 모델의 성능 역시 데이터 양과 모델 파라미터 수에 대해 멱법칙 스케일링을 따른다. 특히 시각적 정보 처리가 중요한 로봇 작업의 특성상, 그 스케일링 계수(Coefficient)는 언어 모델보다는 컴퓨터 비전 모델의 스케일링 양상과 유사하다.34 또한, 단순한 데이터 양의 증가보다 <strong>환경과 객체의 다양성</strong>을 늘리는 것이 일반화 성능 향상에 훨씬 더 큰 영향을 미친다는 연구 결과도 있다. 32개 이상의 다양한 환경에서 데이터를 수집할 경우, 본 적 없는 환경에 대한 제로샷 성공률이 90%에 달할 수 있음이 보고되었다.36</li>
<li><strong>모델 크기의 임계점 (Intelligence Threshold):</strong> Generalist AI의 <strong>GEN-0</strong> 모델 연구는 흥미로운 ’임계점’을 제시한다. 그들은 27만 시간 분량의 데이터를 학습시키는 과정에서, 모델 크기가 약 <strong>70억(7B) 파라미터</strong>를 넘어서는 순간 학습 효율의 비약적인 상승(Phase Transition)이 발생함을 관찰했다. 10억(1B) 파라미터 수준의 작은 모델들은 데이터가 너무 많아지면 오히려 학습 능력이 포화되거나 저하되는 ‘경직(Ossification)’ 현상을 보인 반면, 7B 이상의 모델들은 데이터를 계속 흡수하며 성능이 지속적으로 향상되었다.38 이는 로봇이 복잡한 물리 세계를 이해하고 일반화하기 위해서는 일정 수준 이상의 ’두뇌 용량(Capacity)’이 필수적임을 시사한다.</li>
</ul>
<h2>5.  기술적 난제와 미래 전망</h2>
<p>파운데이션 모델은 로봇에게 전례 없는 지능을 부여했지만, 실험실을 벗어나 실제 세계(Real World)에 배치되기 위해서는 여전히 넘어야 할 높은 장벽들이 존재한다.</p>
<h3>5.1  실시간성(Real-Time)과 지연(Latency)의 딜레마</h3>
<p>거대 모델은 무겁다. 수십억, 수백억 개의 파라미터를 가진 모델을 로봇의 제어 루프(Control Loop) 안에서 실행하는 것은 막대한 연산 비용과 지연 시간(Latency)을 유발한다. 인간의 반사 신경과 같은 빠른 반응이 필요한 작업에서 300ms 이상의 지연은 치명적이다. RT-2와 같은 모델이 1~3Hz의 낮은 주파수로 구동되는 것은 이러한 연산 부하 때문이다.40 이를 해결하기 위해 π0와 같이 추론 과정을 계층화(Hierarchical)하거나, 행동 청크(Action Chunking) 기술을 통해 한 번의 추론으로 미래의 여러 행동을 미리 생성해두는 기법, 그리고 모델 양자화(Quantization)를 통한 경량화 연구가 필수적이다.10</p>
<h3>5.2  안전(Safety)과 물리적 환각</h3>
<p>LLM이 없는 사실을 지어내는 ’환각(Hallucination)’을 겪듯, VLA 모델 역시 물리적으로 불가능하거나 위험한 행동을 생성할 수 있다. 텍스트상의 환각은 잘못된 정보를 줄 뿐이지만, 로봇의 환각은 기물 파손이나 인명 사고로 직결된다. 따라서 VLA 모델의 출력단에 **제어 장벽 함수(Control Barrier Functions)**와 같은 안전 필터를 적용하여, 모델이 생성한 행동이 물리적 안전 영역을 벗어나지 않도록 강제하는 연구가 진행되고 있다.41</p>
<h3>5.3  시뮬레이션-현실 간극(Sim-to-Real Gap)과 평가의 부재</h3>
<p>데이터 부족을 해결하기 위해 시뮬레이션이 적극 활용되고 있으나, 마찰, 접촉, 변형과 같은 복잡한 물리 현상을 완벽히 모사하는 시뮬레이터는 아직 존재하지 않는다. 이로 인한 ’Sim-to-Real Gap’은 여전히 큰 숙제이다. 또한, NLP의 GLUE 벤치마크처럼 로봇의 범용 능력을 정량적으로 평가할 수 있는 표준화된 벤치마크가 부재하다는 점도 발전 속도를 저해하는 요인이다. 현재는 각 연구가 서로 다른 로봇과 환경에서 평가를 수행하여 직접적인 성능 비교가 어려운 실정이다.42</p>
<h3>5.4 결론: 구체화된 인공지능(Embodied AI)을 향하여</h3>
<p>파운데이션 모델 시대의 로봇은 더 이상 코드로 정의된 기계가 아니다. 그들은 데이터를 통해 세상을 배우고, 언어로 인간과 소통하며, 물리적 상호작용을 통해 지능을 확장해 나가는 **구체화된 AI(Embodied AI)**로 진화하고 있다. RT-2와 OpenVLA가 보여준 인지와 행동의 통합, OXE와 DROID가 증명한 데이터의 힘, 그리고 RFM-1과 Pi0가 제시한 물리적 지능의 미래는 로봇이 공장의 울타리를 넘어 우리의 일상 공간으로 들어오는 시점을 앞당기고 있다.</p>
<p>향후 로봇 연구는 단순히 모델의 크기를 키우는 경쟁을 넘어, ’System 1(직관적이고 빠른 행동)’과 ’System 2(논리적이고 느린 추론)’를 어떻게 효율적으로 결합할 것인지, 그리고 어떻게 하면 로봇이 인간의 가치와 안전을 보장하며 공존할 수 있을지에 대한 깊이 있는 탐구로 나아갈 것이다. 바야흐로 로봇은 하드웨어의 시대를 지나, 소프트웨어와 데이터가 정의하는 지능의 시대로 진입하였다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Foundation models in robotics: Applications, challenges, and the …, https://www.researchgate.net/publication/384387678_Foundation_models_in_robotics_Applications_challenges_and_the_future</li>
<li>Foundation models in robotics: Applications, challenges, and the …, https://collaborate.princeton.edu/en/publications/foundation-models-in-robotics-applications-challenges-and-the-fut-2/</li>
<li>Real-world robot applications of foundation models: a review, https://www.tandfonline.com/doi/full/10.1080/01691864.2024.2408593</li>
<li>(PDF) Vision-Language-Action Models: Concepts, Progress …, https://www.researchgate.net/publication/391575814_Vision-Language-Action_Models_Concepts_Progress_Applications_and_Challenges</li>
<li>RT-2: New model translates vision and language into action, https://deepmind.google/blog/rt-2-new-model-translates-vision-and-language-into-action/</li>
<li>DeepMind’s RT-2 makes robot control a matter of AI chat - ZDNET, https://www.zdnet.com/article/deepminds-rt-2-makes-robot-control-a-matter-of-chat/</li>
<li>RT-2, Robotic Transformer 2 Review | gracefullight.dev, https://gracefullight.dev/en/2025/08/24/rt-2-review/</li>
<li>RT-2: Vision-Language-Action Models, https://robotics-transformer2.github.io/</li>
<li>OpenVLA: An Open-Source Vision-Language-Action Model - arXiv, https://arxiv.org/html/2406.09246v3</li>
<li>Vision Language Action (VLA) Models Powering Robotics| Exxact Blog, https://www.exxactcorp.com/blog/deep-learning/vision-language-action-vla-models-powers-robotics</li>
<li>(PDF) OpenVLA: An Open-Source Vision-Language-Action Model, https://www.researchgate.net/publication/381404911_OpenVLA_An_Open-Source_Vision-Language-Action_Model</li>
<li>Octo: An Open-Source Generalist Robot Policy, https://www.roboticsproceedings.org/rss20/p090.pdf</li>
<li>Octo: An Open-Source Generalist Robot Policy, https://octo-models.github.io/</li>
<li>octo-models/octo: Octo is a transformer-based robot policy … - GitHub, https://github.com/octo-models/octo</li>
<li>Comparing 5 Pioneering Robotics Foundation Models for ML-Based …, https://medium.com/@genki-sano/a-practical-comparison-of-five-leading-ml-based-robotics-control-approaches-49e1977dd3ec</li>
<li>Vision-language-action model - Wikipedia, https://en.wikipedia.org/wiki/Vision-language-action_model</li>
<li>Covariant AI: A Deep Dive into the Future of Robotic Automation, https://skywork.ai/skypage/en/Covariant-AI-A-Deep-Dive-into-the-Future-of-Robotic-Automation/1976506268149018624</li>
<li>RFM-1: A world model that understands physics - YouTube, https://www.youtube.com/watch?v=INp7I3Efspc</li>
<li>Covariant Introduces RFM-1 to Give Robots the Human-like Ability to …, https://www.roboticstomorrow.com/news/2024/03/11/covariant-introduces-rfm-1-to-give-robots-the-human-like-ability-to-reason/22210/</li>
<li>RFM-1, A Model That Enables Robots to Understand and Act on …, https://www.deeplearning.ai/the-batch/rfm-1-a-model-that-enables-robots-to-understand-and-act-on-human-commands/</li>
<li>π0: A Vision-Language-Action Flow Model for General Robot Control, https://www.physicalintelligence.company/download/pi0.pdf</li>
<li>Vision-Language-Action Models for General Robot Control, https://huggingface.co/blog/pi0</li>
<li>Demystifying Robotic Foundational Models: Pi0 - TUUL.AI Research, https://tuul.ai/research/pi0-robotic-foundation-model</li>
<li>Pi0 Inference Latency Much Higher Than Reported in Pi0 Paper, https://github.com/huggingface/lerobot/issues/1537</li>
<li>Scaling up learning across many different robot types, https://deepmind.google/blog/scaling-up-learning-across-many-different-robot-types/</li>
<li>google-deepmind/open_x_embodiment - GitHub, https://github.com/google-deepmind/open_x_embodiment</li>
<li>Open X-Embodiment: Robotic Learning Datasets and RT-X Models, https://www.robot-learning.ml/2023/files/paper18.pdf</li>
<li>DROID: A Large-Scale In-the-Wild Robot Manipulation Dataset, https://droid-dataset.github.io/</li>
<li>DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset …, https://arxiv.org/html/2403.12945v2</li>
<li>DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset, https://www.researchgate.net/publication/383906519_DROID_A_Large-Scale_In-The-Wild_Robot_Manipulation_Dataset</li>
<li>AgiBotWorld-Alpha - ModelScope, https://www.modelscope.cn/datasets/AI-ModelScope/AgiBotWorld-Alpha</li>
<li>AgiBot World Colosseo: A Large-scale Manipulation Platform … - arXiv, https://arxiv.org/html/2503.06669v2</li>
<li>AgiBot World Colosseo - OpenDriveLab, https://opendrivelab.com/AgiBot-World/</li>
<li>Neural Scaling Laws for Embodied AI - ResearchGate, https://www.researchgate.net/publication/380821116_Neural_Scaling_Laws_for_Embodied_AI</li>
<li>Neural Scaling Laws for Embodied AI - arXiv, https://arxiv.org/html/2405.14005v1</li>
<li>Data Scaling Laws in Imitation Learning for Robotic Manipulation, https://arxiv.org/html/2410.18647v1</li>
<li>Data Scaling Laws in Imitation Learning for Robotic Manipulation, https://data-scaling-laws.github.io/</li>
<li>Generalist AI Unveils GEN-0, Claims Scaling Laws for Robotics …, https://www.humanoidsdaily.com/news/generalist-ai-unveils-gen-0-claims-scaling-laws-for-robotics-backed-by-270-000-hours-of-real-world-data</li>
<li>GEN-0 / Embodied Foundation Models That Scale with … - Generalist, https://generalistai.com/blog/nov-04-2025-GEN-0</li>
<li>Experiences from Benchmarking Vision–Language–Action Models …, https://arxiv.org/html/2511.11298v1</li>
<li>VLSA: Vision-Language-Action Models with Plug-and-Play Safety …, https://www.researchgate.net/publication/398721287_VLSA_Vision-Language-Action_Models_with_Plug-and-Play_Safety_Constraint_Layer</li>
<li>The Future of Robotics AI: Simulation, Foundation Models, and …, https://medium.com/inflectiv/the-future-of-robotics-ai-simulation-foundation-models-and-whats-next-fd7af1755a15</li>
<li>(PDF) Robotic Foundation Models and Physical AI Innovations …, https://www.researchgate.net/publication/388178070_Robotic_Foundation_Models_and_Physical_AI_Innovations_Applications_Ethical_Challenges_and_the_Future_of_Generalized_Robotics</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>