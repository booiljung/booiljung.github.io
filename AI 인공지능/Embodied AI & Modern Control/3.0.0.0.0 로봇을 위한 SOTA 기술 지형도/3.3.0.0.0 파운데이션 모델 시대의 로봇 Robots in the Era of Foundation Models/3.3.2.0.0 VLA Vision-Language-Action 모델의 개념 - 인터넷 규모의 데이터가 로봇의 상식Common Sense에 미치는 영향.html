<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.3.2 VLA (Vision-Language-Action) 모델의 개념: 인터넷 규모의 데이터가 로봇의 상식(Common Sense)에 미치는 영향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.3.2 VLA (Vision-Language-Action) 모델의 개념: 인터넷 규모의 데이터가 로봇의 상식(Common Sense)에 미치는 영향</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 3. 로봇을 위한 SOTA 기술 지형도</a> / <a href="index.html">3.3 파운데이션 모델 시대의 로봇 (Robots in the Era of Foundation Models)</a> / <span>3.3.2 VLA (Vision-Language-Action) 모델의 개념: 인터넷 규모의 데이터가 로봇의 상식(Common Sense)에 미치는 영향</span></nav>
                </div>
            </header>
            <article>
                <h1>3.3.2 VLA (Vision-Language-Action) 모델의 개념: 인터넷 규모의 데이터가 로봇의 상식(Common Sense)에 미치는 영향</h1>
<p>로봇 공학의 역사에서 ’제어(Control)’와 ’지능(Intelligence)’은 오랫동안 상호 보완적이면서도 독립적인 궤적을 그리며 발전해 왔다. 제어 이론은 로봇의 키네마틱스(Kinematics)와 다이내믹스(Dynamics)를 정교한 수학적 모델로 기술하여 물리적 세계에서의 정확한 움직임을 생성하는 데 주력해 온 반면, 인공지능은 시각적 인식이나 언어 이해와 같은 고차원적인 인지 능력을 담당해 왔다. 그러나 이 두 영역의 결합은 항상 ’상징 접지 문제(Symbol Grounding Problem)’라는 근본적인 난제에 부딪혀 왔다. 즉, “사과를 집어라“라는 추상적인 언어적 기호(Symbol)가 물리적 세계의 픽셀 데이터 및 모터 토크(Physical Reality)와 어떻게 유기적으로 연결될 수 있는가에 대한 문제는 오랫동안 로봇 공학자들의 숙원 과제였다.</p>
<p>2020년대 초반, 대규모 언어 모델(LLM)과 시각-언어 모델(VLM)의 폭발적인 성장은 이 난제를 해결할 새로운 패러다임을 제시했다. 바로 시각(Vision), 언어(Language), 그리고 행동(Action)을 단일한 신경망 아키텍처 내에서 통합 처리하는 <strong>Vision-Language-Action (VLA)</strong> 모델의 등장이다. VLA 모델은 단순히 로봇에게 ’눈’과 ’귀’를 달아주는 것을 넘어, 인터넷상에 존재하는 방대한 텍스트와 이미지 데이터를 통해 로봇에게 인간 수준의 ’상식(Common Sense)’과 ‘의미론적 추론(Semantic Reasoning)’ 능력을 부여하고 있다. 본 절에서는 VLA 모델의 기술적 정의와 아키텍처적 진화 과정을 심층적으로 분석하고, 인터넷 규모의 데이터가 로봇의 인지 및 제어 능력에 미치는 혁명적인 영향을 상세히 기술한다.</p>
<h2>1.  VLA 모델의 정의와 아키텍처적 진화: 통합된 인지-행동 루프</h2>
<p>전통적인 로봇 제어 아키텍처는 인식(Perception), 계획(Planning), 제어(Control)가 순차적으로 연결된 파이프라인 구조를 따랐다. 이러한 구조에서는 각 모듈 간의 정보 손실이 발생하고, 오류가 누적되는 전파(Error Propagation) 문제가 빈번했다. 반면, VLA 모델은 시각적 관찰(Observation)과 자연어 명령(Instruction)을 입력받아 로봇의 물리적 제어 명령인 행동(Action)을 직접 출력하는 ‘End-to-End’ 멀티모달 파운데이션 모델(Multimodal Foundation Model)로 정의된다.1</p>
<p>VLA는 이 모든 과정을 하나의 거대한 트랜스포머(Transformer) 네트워크 내에서 ‘시퀀스 변환(Sequence-to-Sequence)’ 문제로 재정의한다. 이는 로봇의 행동을 텍스트나 이미지와 동일한 형태의 ’토큰(Token)’으로 취급함으로써 가능해졌다. 즉, VLA 모델에게 있어 “사과를 집어라“라는 텍스트를 이해하는 것과, 로봇 팔을 10cm 움직이는 것은 본질적으로 동일한 ‘다음 토큰 예측(Next-Token Prediction)’ 과제가 된다.</p>
<h3>1.1  행동의 토큰화(Action Tokenization): 연속적 물리 세계의 이산화</h3>
<p>VLA 모델의 가장 핵심적인 기술적 혁신은 연속적인(Continuous) 물리적 행동을 이산적인(Discrete) 토큰으로 변환하여 언어 모델이 처리할 수 있는 형태로 만드는 데 있다. 대규모 언어 모델(LLM)은 기본적으로 이산적인 텍스트 토큰을 처리하도록 설계되어 있기 때문에, 로봇의 연속적인 제어 신호를 이 아키텍처에 통합하기 위해서는 정교한 변환 과정이 필요하다.</p>
<p>이산화(Discretization) 및 비닝(Binning) 전략:</p>
<p>로봇의 관절 각도(Joint Angles)나 엔드 이펙터(End-effector)의 위치 및 회전 변화량(Delta Pose)은 본래 연속적인 실수값(Float)이다. VLA 모델, 특히 구글 딥마인드(Google DeepMind)의 **RT-2(Robotic Transformer 2)**와 같은 초기 모델들은 이를 256개의 구간(Bin)으로 균일하게 이산화하는 방식을 채택했다.4 예를 들어, 로봇 팔의 x축 이동 범위가 -10cm에서 +10cm라면, 이를 256등분 하여 특정 이동량을 0부터 255 사이의 정수(Integer)로 매핑하는 것이다.</p>
<p>이러한 방식은 연속적인 제어 공간을 언어 모델의 어휘 사전(Vocabulary) 크기에 맞춰 압축하는 효과가 있다. 이때 각 Bin의 크기는 제어의 정밀도(Precision)를 결정한다. Bin의 개수가 너무 적으면 로봇의 움직임이 투박해지고(Jerky), 너무 많으면 어휘 사전의 크기가 폭발적으로 증가하여 학습 효율이 떨어진다. RT-2 연구진은 256개의 Bin이 일반적인 조작 작업(Manipulation Task)에서 충분한 정밀도를 제공하면서도 모델의 학습 안정성을 유지할 수 있는 최적의 균형점임을 실험적으로 입증했다.2</p>
<p>텍스트 토큰으로서의 행동 통합:</p>
<p>이렇게 정수화된 행동 값들은 LLM의 어휘 사전 내의 특정 토큰과 연결된다. RT-2의 경우, 행동을 나타내는 정수를 텍스트 토큰으로 변환하여 학습 데이터에 포함시켰다. 즉, 입력 시퀀스가 “사과를 집어라“라는 텍스트 토큰으로 시작하여, 그 뒤에 [128, 91, 241, 5,…]와 같은 행동 토큰 시퀀스가 이어지는 형태이다.4 이를 통해 모델은 인터넷상의 텍스트와 이미지를 학습하는 것과 동일한 메커니즘인 ‘자기회귀적(Auto-regressive)’ 방식으로 로봇의 행동을 예측하고 생성할 수 있게 된다.</p>
<p>이 접근 방식의 가장 큰 장점은 기존의 강력한 LLM 아키텍처와 사전 학습된 가중치(Pre-trained Weights)를 그대로 활용할 수 있다는 점이다. 별도의 행동 제어 헤드(Action Head)를 복잡하게 설계할 필요 없이, 언어 모델의 출력층(Output Layer)을 그대로 사용하여 행동 토큰을 샘플링하면 된다.</p>
<p>잠재 공간의 통합(Embedding Space Integration):</p>
<p>행동을 토큰화한다는 것은 시각 정보, 언어적 지시, 그리고 물리적 행동이 모두 동일한 임베딩 공간(Embedding Space) 내에서 표현됨을 의미한다. 트랜스포머의 셀프 어텐션(Self-Attention) 메커니즘은 이들 사이의 복잡한 상관관계를 학습한다. 예를 들어, “빨간색 물체(Vision)“라는 시각적 패턴과 “뜨겁다(Language)“라는 언어적 개념이 “손을 떼라(Action)“라는 행동 토큰과 높은 어텐션 가중치(Attention Weight)로 연결되도록 학습되는 것이다. 이는 로봇 제어에 있어 감각 정보와 행동 사이의 의미론적 연결고리를 강화하는 결정적인 역할을 한다.</p>
<h3>1.2  단일 시스템(Single-System) vs. 이중 시스템(Dual-System) 아키텍처</h3>
<p>VLA 모델의 설계 철학은 크게 모든 처리를 하나의 모델에서 수행하는 단일 시스템과, 추론과 제어를 분리하는 이중 시스템으로 나뉜다.2</p>
<p>단일 시스템 (Single-System Architecture):</p>
<p>RT-2나 OpenVLA와 같은 모델이 대표적이다. 거대한 VLM 백본(Backbone)이 인식, 추론, 행동 생성을 모두 수행한다. 시각 인코더(예: ViT, SigLIP)가 이미지를 처리하고, LLM(예: PaLM, Llama)이 언어와 시각 정보를 통합하여 행동 토큰을 출력한다. 이 구조는 모델의 모든 파라미터가 행동 생성에 기여하므로 강력한 의미론적 이해를 바탕으로 한 행동 생성이 가능하다는 장점이 있다. 또한, 아키텍처가 단순하여 학습 파이프라인 구축이 용이하다. 그러나 수백억 개의 파라미터를 가진 거대 모델을 매 제어 주기마다 실행해야 하므로 추론 속도 문제로 인해 실시간 제어 주파수(Frequency)가 낮아질 수 있다는 단점이 있다. 실제로 RT-2의 경우, 클라우드 TPU를 사용하더라도 1~3Hz 수준의 제어 주기를 보인다.9</p>
<p>이중 시스템 (Dual-System Architecture):</p>
<p>PaLM-E의 초기 개념이나 최근의 Figure AI의 Helix, NVIDIA의 GR00T N1과 같은 모델들이 채택하는 방식이다. 이는 인간의 인지 과정을 설명하는 다니엘 카너먼(Daniel Kahneman)의 ’이중 정보 처리 이론(Dual Process Theory)’에서 영감을 받았다.</p>
<ul>
<li><strong>System 2 (Slow &amp; Deliberative):</strong> 거대 VLM이 담당하며, 느리지만 고차원적인 시각적 해석과 논리적 추론을 수행한다. “서랍을 열고 칩 봉지를 꺼내라“와 같은 복잡한 명령을 하위 목표(Sub-goal)로 분해하거나, 현재 상황에 대한 의미론적 이해를 바탕으로 상위 수준의 계획을 수립한다.</li>
<li><strong>System 1 (Fast &amp; Reactive):</strong> 경량화된 정책 네트워크(Policy Network)가 담당하며, 빠르게 작동하며 모터 제어를 수행한다. System 2가 전달한 계획이나 잠재 벡터(Latent Vector)를 입력받아 100Hz 이상의 높은 주파수로 관절 토크나 속도 명령을 생성한다.8</li>
</ul>
<p>이중 시스템은 실시간성과 고지능을 동시에 확보하려는 시도이나, 두 시스템 간의 인터페이스 설계와 정보 동기화가 복잡하다는 기술적 난이도가 존재한다.</p>
<h2>2.  인터넷 규모 데이터의 전이: 의미론에서 물리적 행동으로</h2>
<p>VLA 모델이 기존의 로봇 학습 방법론(예: 모방 학습, 강화 학습)과 근본적으로 차별화되는 지점은 바로 **인터넷 규모의 데이터(Internet-Scale Data)**를 활용한다는 것이다. 전통적인 로봇 데이터셋은 수집 비용이 매우 높고, 물리적 마모와 안전 문제로 인해 그 양이 제한적일 수밖에 없다. 반면, 인터넷상에는 수십억 개의 이미지-텍스트 쌍(Image-Text Pairs)과 비디오 데이터가 존재한다. VLA 모델은 이러한 웹 데이터를 통해 로봇에게 물리적 경험을 넘어선 방대한 ’상식(Common Sense)’을 주입한다.5</p>
<h3>2.1  긍정적 전이(Positive Transfer)와 상식의 주입</h3>
<p>로봇 학습 데이터만을 사용한 모델(예: RT-1)은 학습한 물체나 환경에 대해서는 잘 작동하지만, 처음 보는 물체나 새로운 지시어, 낯선 환경에서는 급격히 성능이 저하되는 한계를 보였다. 그러나 VLM을 기반으로 구축된 VLA 모델은 웹 데이터에서 학습한 방대한 시각적, 언어적 개념을 로봇 제어 문제로 **전이(Transfer)**시킨다. 이를 **긍정적 전이(Positive Transfer)**라고 하며, 이는 다음과 같은 구체적인 형태로 발현된다.12</p>
<p>시각적 일반화(Visual Generalization)와 제로샷 인식:</p>
<p>로봇 데이터셋에 없는 물체라도, 웹 데이터에 존재하는 물체라면 인식하고 조작할 수 있다. 예를 들어, 로봇이 훈련 중에 ’아이언맨 피규어’를 한 번도 본 적이 없더라도, 인터넷상의 수많은 이미지를 통해 ’아이언맨’의 시각적 특징(빨간색과 금색의 갑옷 형태)을 이미 학습했기 때문에 “아이언맨을 집어라“라는 명령을 수행할 수 있다. 이는 로봇이 닫힌 세계(Closed World)에서 열린 세계(Open World)로 나아가는 핵심적인 능력이다.7</p>
<p>언어적 유연성(Linguistic Flexibility)과 의미론적 매핑:</p>
<p>정형화된 명령어가 아닌, 추상적이고 모호한 자연어 명령을 이해하고 행동으로 연결한다. “목마른 사람에게 줄 것을 가져와라“라는 명령에 대해, 로봇은 학습 데이터에 ’목마름’과 특정 물체 간의 명시적인 매핑이 없더라도, 웹 데이터에서 학습한 ’음료수’와 ’갈증 해소’의 의미론적 관계를 이용하여 물병이나 캔을 선택하는 행동을 취한다. 이는 단순한 패턴 매칭을 넘어선 의미론적 추론(Semantic Reasoning)의 영역이다.5</p>
<p>배경 지식의 통합(Background Knowledge Integration):</p>
<p>로봇은 웹 데이터를 통해 물리적 속성이나 사회적 맥락에 대한 암묵적인 지식을 습득한다. 예를 들어 “테이블을 청소해라“라는 명령을 수행할 때, 사과와 사과 껍질을 구별하거나, 스펀지와 빵 중에서 청소 도구로 스펀지를 선택하는 등의 판단이 가능해진다. 또한, “독일 국기가 그려진 물건을 집어라“와 같이 국가, 역사, 문화적 지식이 필요한 작업도 수행할 수 있게 된다. 이는 로봇 데이터만으로는 학습하기 불가능에 가까운 방대한 ’세계 지식(World Knowledge)’이다.</p>
<h3>2.2  로봇 데이터와 웹 데이터의 공동 미세 조정(Co-fine-tuning)</h3>
<p>VLA 모델을 학습시킬 때 가장 중요한 기술적 과제는 웹 데이터가 가진 일반적인 지식과 로봇 데이터가 가진 구체적인 행동 제어 능력을 균형 있게 유지하는 것이다. 웹 데이터만으로 학습하면 행동을 생성할 수 없고, 로봇 데이터만으로 학습하면 일반화 성능이 떨어진다. 따라서 RT-2와 같은 모델은 <strong>공동 미세 조정(Co-fine-tuning)</strong> 전략을 사용한다.4</p>
<p>데이터 믹스(Data Mixture) 전략:</p>
<p>로봇의 궤적 데이터(이미지, 명령어, 행동)와 웹상의 VQA(Visual Question Answering), 이미지 캡셔닝 데이터를 섞어서(Mix) 학습한다. 이때 로봇 데이터의 비율을 적절히 높여(Upsampling) 모델이 행동 생성 능력을 잃지 않도록 하면서도, 웹 데이터의 손실 함수(Loss Function)를 함께 최적화하여 기존의 상식 추론 능력을 유지한다. RT-2의 실험에 따르면, 웹 데이터와 로봇 데이터를 함께 학습시켰을 때가 로봇 데이터만으로 학습했을 때보다 미본 물체(Unseen Objects)에 대한 조작 성공률이 2배 이상 높은 것으로 나타났다.4</p>
<p>재앙적 망각(Catastrophic Forgetting) 방지:</p>
<p>신경망 학습에서 새로운 태스크(로봇 제어)를 학습할 때 기존에 학습한 지식(웹 상식)을 잊어버리는 현상을 ’재앙적 망각’이라고 한다. 만약 로봇 데이터로만 파운데이션 모델을 미세 조정(Fine-tuning)한다면, 모델은 “미국의 대통령은 누구인가?“라는 질문에 답할 수 있는 능력이나 복잡한 시각적 추론 능력을 상실하게 된다. 공동 미세 조정은 이를 방지하여 로봇이 웹 지식과 신체 제어 기술을 동시에 보유하게 한다. 이는 로봇이 단순히 작업을 수행하는 기계가 아니라, 세상에 대한 이해를 바탕으로 행동하는 지능형 에이전트(Intelligent Agent)가 되기 위한 필수 조건이다.9</p>
<h2>3.  주요 SOTA VLA 모델 분석: RT-2, PaLM-E, OpenVLA, Octo</h2>
<p>현재 로봇 공학계를 선도하고 있는 대표적인 VLA 모델들은 각기 다른 백본 모델과 학습 전략을 통해 인터넷 규모 데이터의 효용성을 입증하고 있다.</p>
<h3>3.1  RT-2 (Robotic Transformer 2)</h3>
<p>구글 딥마인드가 발표한 RT-2는 VLA 모델의 개념을 정립하고 그 효용성을 대규모로 입증한 선구적인 연구이다. <strong>PaLI-X</strong>와 <strong>PaLM-E</strong>라는 거대 VLM을 백본으로 사용하여, 로봇의 행동을 텍스트 토큰으로 변환해 학습했다.</p>
<ul>
<li><strong>규모의 경제:</strong> 550억(55B) 파라미터 규모의 모델을 사용하여 로봇 제어에 적용했다. 이는 기존의 로봇 제어 모델들이 수천만(M) 단위의 파라미터를 가졌던 것과 비교하면 수천 배 커진 규모이다. 이러한 규모의 확장은 기존 RT-1 모델 대비 보지 못한 물체나 환경에 대한 일반화 성능을 2배 이상 향상시키는 결과를 가져왔다.7</li>
<li><strong>상식 추론의 발현:</strong> RT-2는 훈련 데이터에 없던 상황에서도 웹 지식을 활용해 해결책을 찾아냈다. 예를 들어 “즉석 망치로 쓸 수 있는 물건을 집어라“라는 명령에 대해, 주변에 망치가 없음에도 불구하고 ’돌(Rock)’을 선택하는 능력을 보여주었다. 이는 웹 데이터에서 ‘돌은 단단하다’, ’단단한 것은 망치 대용이 될 수 있다’라는 연쇄적인 추론(Chain-of-Thought)이 가능했기 때문이다.5</li>
<li><strong>한계:</strong> 가장 큰 한계는 추론 속도이다. 55B 모델을 실시간으로 구동하기 위해서는 막대한 연산 자원이 필요하며, 클라우드 기반의 추론을 통해서도 1~3Hz 수준의 제어 주기를 확보하는 데 그쳤다. 이는 빠른 반응이 필요한 동적인 작업에는 적합하지 않을 수 있음을 시사한다.9</li>
</ul>
<h3>3.2  PaLM-E (Pathways Language Model - Embodied)</h3>
<p>PaLM-E는 엄밀히 말해 행동 토큰을 직접 출력하기보다는, 텍스트 형태의 고수준 계획을 생성하여 하위 제어기(Low-level Policy)에 전달하는 방식에 가깝지만, ’Embodied Language Model’이라는 개념을 통해 감각 데이터(이미지, 센서 데이터)를 언어 모델의 임베딩 공간에 직접 주입(Inject)하는 혁신적인 방식을 제안했다.12</p>
<ul>
<li><strong>멀티모달 문장(Multimodal Sentences):</strong> 텍스트와 이미지가 섞인 입력을 처리한다. 예를 들어 “ <code>&lt;이미지_임베딩&gt;</code> 이 상황에서 로봇이 해야 할 일은?“과 같은 형태의 프롬프트를 구성한다. 이를 위해 ViT(Vision Transformer)를 통해 인코딩된 이미지 벡터와 로봇의 상태 벡터(State Vector)를 언어 토큰과 동일한 차원으로 선형 투영(Linear Projection)하여 LLM에 입력한다.</li>
<li><strong>성능과 확장성:</strong> 최대 5620억(562B) 파라미터까지 확장되었으며, 로봇 조작뿐만 아니라 시각적 질의응답(VQA), 이미지 캡셔닝 등 일반적인 비전-언어 태스크에서도 SOTA 성능을 기록했다. 특히 로봇 데이터 학습이 언어 모델의 일반적인 인지 능력까지 향상시킬 수 있음을 보여주며, 로봇 데이터와 웹 데이터 간의 상호 보완적 관계를 증명했다.20</li>
</ul>
<h3>3.3  OpenVLA</h3>
<p>OpenVLA는 오픈소스 진영에서 개발된 70억(7B) 파라미터 규모의 VLA 모델로, 접근성과 효율성을 극대화하여 VLA 기술의 대중화를 이끌고 있다. 메타(Meta)의 <strong>Llama 2</strong>를 언어 백본으로, <strong>DINOv2</strong>와 <strong>SigLIP</strong>을 시각 인코더로 사용한다.23</p>
<ul>
<li><strong>이중 시각 인코더(Dual Visual Encoders):</strong> 시각적 특징 추출을 위해 공간적 이해에 강한 DINOv2와 의미적 이해에 강한 SigLIP을 결합(Fuse)하여 사용한다. 이는 조작 태스크에 필요한 정밀한 공간 정보와 웹 데이터 기반의 풍부한 의미 정보를 동시에 확보하기 위함이다.</li>
<li><strong>효율성 및 접근성:</strong> RT-2와 같은 폐쇄형(Closed) 모델과 달리, 누구나 사용할 수 있도록 모델 가중치와 훈련 코드가 공개되었다. 특히 **LoRA (Low-Rank Adaptation)**와 양자화(Quantization) 기술을 적용하여, 고가의 서버급 GPU가 아닌 일반 소비자용 GPU(예: NVIDIA RTX 3090, 4090)에서도 훈련 및 추론이 가능하도록 최적화되었다.24</li>
<li><strong>성능:</strong> 97만 개의 로봇 궤적 데이터(Open X-Embodiment)로 학습되었으며, 55B 규모의 RT-2 모델보다 일부 태스크에서 더 높은 성공률을 기록했다. 이는 모델의 절대적인 크기보다 데이터의 품질과 아키텍처의 효율성, 그리고 시각 인코더의 적절한 조합이 중요할 수 있음을 시사한다.23</li>
</ul>
<h3>3.4  Octo</h3>
<p>Octo는 트랜스포머 기반의 **확산 정책(Diffusion Policy)**을 채택한 모델로, 행동을 결정론적(Deterministic)인 토큰이 아니라 연속적인 분포(Distribution)로 생성한다는 점에서 다른 VLA 모델들과 차별화된다.10</p>
<ul>
<li><strong>확산 모델의 적용:</strong> 행동 생성에 디퓨전 프로세스(Denoising Diffusion Probabilistic Models)를 적용하여, 멀티모달(Multimodal)한 행동 분포를 효과적으로 모델링한다. 예를 들어, 장애물을 피할 때 왼쪽으로 갈 수도 있고 오른쪽으로 갈 수도 있는 상황에서, 단순한 평균값(장애물 정면으로 이동)을 출력하는 것이 아니라 두 가지 가능성을 모두 포함하는 분포를 학습한다.</li>
<li><strong>유연한 입력 구조:</strong> 블록 단위 어텐션(Block-wise Attention) 구조를 통해 다양한 카메라 구성이나 관찰 입력 형태에 유연하게 대응할 수 있다. 이는 다양한 로봇 하드웨어에 모델을 쉽게 이식(Porting)할 수 있게 해준다.</li>
</ul>
<p>| 모델명 | 백본 아키텍처 | 파라미터 크기 | 행동 출력 방식 | 주요 특징 | 학습 데이터 |</p>
<p>| :— | :— | :— | :— | :— | :— |</p>
<p>| RT-2 | PaLI-X, PaLM-E | 5B ~ 55B | 이산적 토큰 (Discretized Tokens) | 공동 미세 조정, 창발적 추론 능력, 텍스트로서의 행동 | 웹 데이터 + RT-1 로봇 데이터 |</p>
<p>| PaLM-E | PaLM + ViT | 12B ~ 562B | 텍스트 (Text) / 고수준 계획 | 연속적 관찰 주입(Continuous Injection), 멀티모달 문장 처리 | 웹 데이터 + 로봇 상태/이미지 |</p>
<p>| OpenVLA | Llama 2 + DINOv2/SigLIP | 7B | 이산적 토큰 (Discretized Tokens) | 오픈소스, 효율적 미세조정(LoRA), 소비자용 GPU 구동 가능 | Open X-Embodiment (970k 궤적) |</p>
<p>| Octo | Transformer + Diffusion | 27M ~ 93M | 연속적 분포 (Continuous via Diffusion) | 확산 정책 기반, 유연한 입력 구조, 경량화 | Open X-Embodiment (800k 궤적) |</p>
<h2>4.  데이터 중심 AI와 로봇 상식의 확장: Open X-Embodiment</h2>
<p>인터넷 규모의 데이터가 로봇의 상식에 미치는 영향을 논할 때, <strong>Open X-Embodiment (RT-X)</strong> 데이터셋의 역할을 빼놓을 수 없다. 이는 로봇 공학계의 ImageNet이라 불리며, 전 세계 21개 기관이 협력하여 22종의 서로 다른 로봇 형태(Embodiment)에서 수집한 100만 개 이상의 궤적 데이터를 포함한다.29</p>
<p>교차 형태 전이(Cross-Embodiment Transfer):</p>
<p>서로 다른 하드웨어 구조를 가진 로봇들의 데이터를 통합하여 학습함으로써, 특정 로봇에 종속되지 않는 **일반적인 조작 기술(General Manipulation Skill)**을 학습한다. 예를 들어, 프랭카(Franka) 로봇이 학습한 ‘집기(Pick)’ 동작의 데이터가 형태가 다른 위도우X(WidowX) 로봇의 학습을 도울 수 있다. 이는 로봇이 자신의 신체적 구조를 넘어서는 ’행동의 본질’을 학습하게 됨을 의미한다.</p>
<p>로봇을 위한 스케일링 법칙(Scaling Law):</p>
<p>RT-X 데이터셋을 사용한 실험 결과, 데이터의 양과 다양성이 증가할수록 로봇의 일반화 성능이 비례하여 향상됨이 확인되었다. 특히 대규모 VLM 백본을 사용한 RT-2-X 모델은 기존 모델 대비 3배 이상의 창발적 기술(Emergent Skills) 수행 능력을 보여주었다. 이는 로봇 지능 역시 데이터의 규모에 따라 예측 가능한 성능 향상을 보인다는 ’스케일링 법칙’이 적용됨을 시사한다.30</p>
<p>상식의 물리적 구현:</p>
<p>인터넷 텍스트/이미지 데이터가 ’의미론적 상식(Semantic Common Sense)’을 제공한다면, Open X-Embodiment와 같은 대규모 로봇 데이터는 ’물리적 상식(Physical Common Sense)’을 제공한다. VLA 모델은 이 두 가지 상식을 결합하여 “깨지기 쉬운 물건은 조심스럽게 다뤄야 한다“는 의미론적 지식을 실제 그리퍼의 힘 조절이나 속도 제어라는 물리적 행동으로 구현해낸다.</p>
<h2>5.  창발적 추론(Emergent Reasoning)과 생각의 사슬(Chain-of-Thought)</h2>
<p>인터넷 규모의 데이터 학습은 로봇에게 단순한 명령 수행을 넘어선 <strong>추론(Reasoning)</strong> 능력을 부여한다. 특히 LLM에서 관찰되는 <strong>생각의 사슬(Chain-of-Thought, CoT)</strong> 프롬프팅 기술이 로봇 제어에도 적용되고 있다.4</p>
<h3>5.1  로봇을 위한 CoT의 메커니즘</h3>
<p>로봇에게 “이 테이블을 치워라“라고 명령했을 때, 기존 모델은 단순히 눈에 보이는 물체를 무작위로 집으려 시도할 것이다. 그러나 CoT가 적용된 VLA 모델은 다음과 같은 내부 독백(Internal Monologue) 과정을 거친다.</p>
<ol>
<li><strong>상황 인식:</strong> “테이블 위에 캔, 과자 봉지, 그리고 뜯지 않은 편지가 있다.”</li>
<li><strong>상식 추론:</strong> “캔과 과자 봉지는 쓰레기일 가능성이 높지만, 편지는 중요한 물건일 수 있다.”</li>
<li><strong>계획 수립:</strong> “따라서 캔과 과자 봉지는 쓰레기통에 버리고, 편지는 서랍에 안전하게 넣어야 한다.”</li>
<li><strong>행동 생성:</strong> <code>[Pick Can]</code>, <code>, </code>, <code>, </code>[Pick Letter]<code>, </code></li>
</ol>
<p>이러한 단계적 추론 과정은 로봇이 복잡하고 모호한 명령을 수행할 때 오류를 줄이고, 인간의 의도에 더 부합하는 행동을 하도록 유도한다. RT-2 연구에서는 이러한 CoT를 적용했을 때, 다단계 추론이 필요한 작업에서의 성공률이 비약적으로 상승함을 확인했다.15</p>
<h3>5.2  구체화된 CoT (Embodied CoT)</h3>
<p>최근 연구에서는 텍스트 기반의 추론을 넘어, 물리적 상태(Bounding Box, Gripper Position) 정보를 추론 과정에 포함시키는 <strong>Embodied CoT</strong>가 제안되었다.33 단순히 텍스트로만 추론하는 것이 아니라, “빨간 사과(좌표: 10, 20)를 집으려면 그리퍼를 (15, 25)로 이동해야 한다“와 같이 시각적 근거(Grounding)를 바탕으로 추론을 진행한다. 이는 로봇이 “생각(Think)“하는 것뿐만 아니라 “자세히 보는(Look Carefully)” 행동을 유도하여 조작의 정확성을 높인다. 실험 결과, Embodied CoT를 적용한 모델은 일반적인 VLA 모델보다 보지 못한 환경에서의 성공률을 28%가량 향상시켰다.36</p>
<h2>6.  VLA 모델의 한계와 도전 과제</h2>
<p>인터넷 규모 데이터와 VLA 모델의 결합은 로봇 지능의 비약적인 발전을 가져왔지만, 여전히 해결해야 할 기술적 난제들이 존재한다.</p>
<p>추론 지연(Inference Latency):</p>
<p>수백억 개의 파라미터를 가진 모델을 매 제어 주기(Control Loop)마다 실행하는 것은 연산 비용이 매우 높다. 로봇의 안정적인 제어를 위해서는 10Hz~50Hz 이상의 주파수가 필요하지만, 거대 모델은 1~5Hz 수준에 머무르는 경우가 많다.9 이를 해결하기 위해 모델 경량화, 양자화, 또는 System 1/2와 같은 계층적 구조에 대한 연구가 지속적으로 필요하다.</p>
<p>공간적 정밀도 부족(Spatial Precision):</p>
<p>이미지를 토큰 단위(Patch)로 처리하고 행동을 이산화하는 과정에서 공간적 해상도가 손실될 수 있다. 이는 바늘에 실을 꿰거나 정밀한 부품을 조립하는 것과 같은 고정밀 작업(High-precision tasks)에서 VLA 모델의 성능을 제한하는 요소가 된다.37 이를 극복하기 위해 연속적인 값을 예측하는 헤드를 별도로 두거나, 이산화 단계를 더 세분화하는 연구가 진행 중이다.</p>
<p>데이터의 롱테일(Long-tail) 문제와 환각(Hallucination):</p>
<p>인터넷 데이터는 일반적인 상황에 대해서는 풍부하지만, 로봇이 마주칠 수 있는 특수한 물리적 상황(예: 복잡한 매듭 풀기, 유체 조작)에 대한 데이터는 부족하다. 또한 텍스트-이미지 데이터에는 ’행동’에 대한 명시적인 정보(힘, 속도, 마찰 등)가 결여되어 있어, 이를 로봇의 제어 신호로 변환하는 데에는 여전히 간극(Gap)이 존재한다.38 더불어 LLM의 고질적인 문제인 환각 현상이 로봇 행동에서도 나타날 수 있으며, 이는 물리적 세계에서 안전사고로 이어질 수 있기에 치명적이다.</p>
<h2>7. 결론: 일반 범용 로봇(Generalist Robot)을 향한 도약</h2>
<p>VLA 모델은 인터넷 규모의 데이터가 가진 의미론적 지식을 로봇의 물리적 행동 제어와 결합함으로써, 로봇이 닫힌 환경의 ’스페셜리스트(Specialist)’에서 열린 세계의 ’제너럴리스트(Generalist)’로 진화할 수 있는 가능성을 열었다. RT-2, PaLM-E, OpenVLA와 같은 모델들은 로봇이 이전에 본 적 없는 물체를 다루고, 복잡한 언어 명령을 이해하며, 상황에 맞는 추론을 통해 유연하게 대처할 수 있음을 증명했다. 비록 실시간성, 정밀도, 데이터의 물리적 접지(Grounding) 등 해결해야 할 과제들이 남아있지만, VLA 아키텍처는 로봇 공학이 규칙 기반의 ’소프트웨어 2.0’을 넘어 데이터 중심의 ‘소프트웨어 3.0’ 시대로 진입했음을 알리는 신호탄이라 할 수 있다. 앞으로 더 방대한 로봇 데이터의 축적(Open X-Embodiment 등)과 효율적인 모델 아키텍처의 발전은 로봇의 ’상식’을 인간 수준으로 끌어올리는 핵심 동력이 될 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>(PDF) Vision-Language-Action Models: Concepts, Progress …, https://www.researchgate.net/publication/391575814_Vision-Language-Action_Models_Concepts_Progress_Applications_and_Challenges</li>
<li>Vision-language-action model - Wikipedia, https://en.wikipedia.org/wiki/Vision-language-action_model</li>
<li>Vision-Language-Action Models for Robotics: A Review Towards …, https://ieeexplore.ieee.org/iel8/6287639/10820123/11164279.pdf</li>
<li>RT-2: Vision-Language-Action Models, https://robotics-transformer2.github.io/</li>
<li>(PDF) RT-2: Vision-Language-Action Models Transfer Web …, https://www.researchgate.net/publication/372784419_RT-2_Vision-Language-Action_Models_Transfer_Web_Knowledge_to_Robotic_Control</li>
<li>Learning-Deep-Learning/paper_notes/rt2.md at master - GitHub, https://github.com/patrick-llgc/Learning-Deep-Learning/blob/master/paper_notes/rt2.md</li>
<li>RT-2: New model translates vision and language into action, https://deepmind.google/blog/rt-2-new-model-translates-vision-and-language-into-action/</li>
<li>Vision-Language-Action Models: The Architecture Powering the …, https://medium.com/@nraman.n6/vision-language-action-models-the-architecture-powering-the-robot-revolution-76f2ce9f400a</li>
<li>RT-2, Robotic Transformer 2 Review | gracefullight.dev, https://gracefullight.dev/2025/08/24/rt-2-review/</li>
<li>Vision Language Action Models (VLA) &amp; Policies for Robots, https://learnopencv.com/vision-language-action-models-lerobot-policy/</li>
<li>Fast-in-Slow: A Dual-System VLA Model Unifying Fast Manipulation…, https://openreview.net/forum?id=4asFznbzJg</li>
<li>PaLM-E: An embodied multimodal language model, https://research.google/blog/palm-e-an-embodied-multimodal-language-model/</li>
<li>PaLM-E: An Embodied Multimodal Language Model, https://proceedings.mlr.press/v202/driess23a/driess23a.pdf</li>
<li>Google’s PaLM-E: An Embodied Multimodal Language Model, https://www.lesswrong.com/posts/sMZRKnwZDDy2sAX7K/google-s-palm-e-an-embodied-multimodal-language-model</li>
<li>1월 4, 2026에 액세스, [https://proceedings.mlr.press/v229/zitkovich23a.html#:<sub>:text=We%20further%20show%20that%20incorporating,tired%20(an%20energy%20drink).](https://proceedings.mlr.press/v229/zitkovich23a.html#:</sub>:text=We further show that incorporating, <a href="https://proceedings.mlr.press/v229/zitkovich23a.html#:~:text=We%20further%20show%20that%20incorporating,tired%20(an%20energy%20drink).">https://proceedings.mlr.press/v229/zitkovich23a.html#:~:text=We%20further%20show%20that%20incorporating,tired%20(an%20energy%20drink).</a></li>
<li>[Robotics] RT-2: Vision-Language-Action Models | by Ming-Hao Hsu, https://medium.com/@amiable_cardinal_crocodile_398/robotics-rt-2-vision-language-action-models-8db0c197d02</li>
<li>RT-2: Vision-Language-Action Models Transfer Web Knowledge to …, https://www.cs.utexas.edu/~yukez/cs391r_fall2023/slides/pre_10-24_Ming.pdf</li>
<li>RT-2: Vision-Language-Action Models Transfer Web Knowledge to…, https://openreview.net/forum?id=XMQgwiJ7KSX</li>
<li>arXiv:2303.03378v1 [cs.LG] 6 Mar 2023, https://arxiv.org/pdf/2303.03378</li>
<li>[Quick Review] PaLM-E: An Embodied Multimodal Language Model, https://liner.com/review/palme-embodied-multimodal-language-model</li>
<li>[R] PaLM-E: An Embodied Multimodal Language Model - Google 2023, https://www.reddit.com/r/MachineLearning/comments/11krgp4/r_palme_an_embodied_multimodal_language_model/</li>
<li>PaLM-E: An Embodied Multimodal Language Model - ar5iv, https://ar5iv.labs.arxiv.org/html/2303.03378</li>
<li>OpenVLA: An Open-Source Vision-Language-Action Model - arXiv, https://arxiv.org/html/2406.09246v3</li>
<li>OpenVLA Review | gracefullight.dev, https://gracefullight.dev/en/2025/08/29/open-vla-review/</li>
<li>openvla/openvla-v01-7b - Hugging Face, https://huggingface.co/openvla/openvla-v01-7b</li>
<li>OpenVLA: Open Source VLA for Robotics - Emergent Mind, https://www.emergentmind.com/topics/openvla</li>
<li>Octo: An Open-Source Generalist Robot Policy | PDF - Scribd, https://www.scribd.com/document/796333958/paper</li>
<li>Octo: An Open-Source Generalist Robot Policy, https://www.roboticsproceedings.org/rss20/p090.pdf</li>
<li>Open X-Embodiment: Robotic Learning Datasets and RT-X Models, https://arxiv.org/html/2310.08864v9</li>
<li>Open X-Embodiment: Robotic Learning Datasets and RT-X Models, https://www.robot-learning.ml/2023/files/paper18.pdf</li>
<li>Open-Weight Foundation Models for Robotics (2025 Guide), https://robocloud-dashboard.vercel.app/learn/blog/open-weight-robot-models</li>
<li>Foundation Models for Robotics: Vision-Language-Action (VLA), https://rohitbandaru.github.io/blog/Foundation-Models-for-Robotics-VLA/</li>
<li>Robotic Control via Embodied Chain-of-Thought Reasoning - arXiv, https://arxiv.org/html/2407.08693v2</li>
<li>RT-2: Vision-Language-Action Models Transfer Web Knowledge to …, https://proceedings.mlr.press/v229/zitkovich23a.html</li>
<li>Robotic Control via Embodied Chain-of-Thought Reasoning, https://openreview.net/forum?id=S70MgnIA0v</li>
<li>Robotic Control via Embodied Chain-of-Thought Reasoning, https://embodied-cot.github.io/</li>
<li>Vision Language Action (VLA) Models Powering Robotics| Exxact Blog, https://www.exxactcorp.com/blog/deep-learning/vision-language-action-vla-models-powers-robotics</li>
<li>Towards Generalist Robot Learning from Internet Video: A Survey, https://arxiv.org/html/2404.19664v5</li>
<li>An Anatomy of Vision-Language-Action Models: From Modules to …, https://arxiv.org/html/2512.11362v3</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>