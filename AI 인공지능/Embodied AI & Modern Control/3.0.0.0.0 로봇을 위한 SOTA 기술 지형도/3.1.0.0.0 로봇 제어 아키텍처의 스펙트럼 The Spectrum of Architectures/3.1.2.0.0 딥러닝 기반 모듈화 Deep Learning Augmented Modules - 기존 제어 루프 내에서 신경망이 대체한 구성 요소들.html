<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.1.2 딥러닝 기반 모듈화 (Deep Learning Augmented Modules): 기존 제어 루프 내에서 신경망이 대체한 구성 요소들</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.1.2 딥러닝 기반 모듈화 (Deep Learning Augmented Modules): 기존 제어 루프 내에서 신경망이 대체한 구성 요소들</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 3. 로봇을 위한 SOTA 기술 지형도</a> / <a href="index.html">3.1 로봇 제어 아키텍처의 스펙트럼 (The Spectrum of Architectures)</a> / <span>3.1.2 딥러닝 기반 모듈화 (Deep Learning Augmented Modules): 기존 제어 루프 내에서 신경망이 대체한 구성 요소들</span></nav>
                </div>
            </header>
            <article>
                <h1>3.1.2 딥러닝 기반 모듈화 (Deep Learning Augmented Modules): 기존 제어 루프 내에서 신경망이 대체한 구성 요소들</h1>
<p>로봇 공학과 인공지능이 융합되는 거대한 흐름 속에서, 우리는 종종 두 가지 극단적인 아키텍처 사이의 대립을 목격한다. 한쪽 끝에는 수십 년간 제어 이론과 시스템 공학이 쌓아 올린 견고한 ’고전적 모듈형 파이프라인(Classical Modular Pipeline)’이 존재한다. 이들은 해석 가능하고, 수학적으로 증명 가능하며, 모듈별 디버깅이 용이하다는 강력한 장점을 지닌다. 반대편 끝에는 딥러닝의 부상과 함께 등장한 ’End-to-End(E2E) 학습’이 있다. 입력 픽셀에서 제어 토크까지 하나의 거대한 신경망으로 연결하는 이 방식은 데이터만 충분하다면 복잡한 특징 엔지니어링 없이도 놀라운 성능을 보여준다.</p>
<p>그러나 현실의 로봇 공학은 이분법적이지 않다. 실제 현장과 최첨단 연구실에서 가장 활발하게 채택되고 있는, 그리고 실질적인 성과를 내고 있는 접근법은 바로 이 둘의 장점을 결합한 **딥러닝 기반 모듈화(Deep Learning Augmented Modules)**이다. 이 아키텍처는 제어 루프의 전체 구조—인식(Perception), 상태 추정(State Estimation), 모델링(Modeling), 계획(Planning), 제어(Control)—는 유지하되, 각 모듈 내부의 불확실성이 높거나 수학적 모델링이 난해한 지점을 딥러닝 모델로 대체하거나 보강하는 전략을 취한다.1</p>
<p>이는 안드레 카파시(Andrej Karpathy)가 주창한 ’소프트웨어 2.0’의 개념이 로봇 제어 시스템에 가장 현실적으로 투영된 형태라 할 수 있다. 즉, <span class="math math-inline">F=ma</span>와 같은 불변의 물리 법칙이나 기하학적 제약 조건은 명시적인 코드(소프트웨어 1.0)로 남겨두고, 마찰 계수의 변화, 센서 노이즈의 비정형성, 복잡한 환경의 시맨틱 정보 등은 데이터로 학습된 신경망(소프트웨어 2.0)이 처리하도록 하는 것이다. 본 절에서는 이러한 하이브리드 아키텍처가 로봇 제어의 핵심 구성 요소들을 어떻게 혁신하고 있는지, 상태 추정부터 제어기 설계, 그리고 안전성 보증에 이르기까지 심층적으로 분석한다.</p>
<h2>1.  뉴럴 상태 추정 (Neural State Estimation): 불확실성의 학습</h2>
<p>로봇이 환경 내에서 자신의 위치, 속도, 자세, 그리고 주변 객체들의 상태를 파악하는 상태 추정(State Estimation)은 자율 시스템의 가장 기초적인 단계다. 전통적으로 이 영역은 확률론적 로보틱스(Probabilistic Robotics)의 지배하에 있었으며, 칼만 필터(Kalman Filter, KF), 확장 칼만 필터(EKF), 파티클 필터(Particle Filter)와 같은 베이지안 필터링(Bayesian Filtering) 기법들이 표준으로 자리 잡아 왔다. 그러나 이러한 고전적 필터들은 시스템의 동역학 모델(Process Model)과 관측 모델(Observation Model)이 사전에 정의되어야 하며, 특히 프로세스 노이즈 공분산(<span class="math math-inline">\mathbf{Q}</span>)과 측정 노이즈 공분산(<span class="math math-inline">\mathbf{R}</span>)과 같은 파라미터를 정확히 알아야 한다는 강력한 가정을 전제로 한다.4</p>
<p>실제 환경, 특히 저가형 센서를 사용하거나 급격한 기동을 수행하는 로봇, 혹은 GPS 신호가 차단된 실내외 환경에서 이러한 파라미터를 수동으로 튜닝하는 것은 불가능에 가깝다. 딥러닝은 이러한 베이지안 필터의 내부 구성 요소를 학습 가능한 함수로 대체함으로써, 데이터로부터 노이즈 특성과 시스템 거동을 스스로 학습하는 ’뉴럴 상태 추정기(Neural State Estimator)’라는 새로운 패러다임을 열었다.5</p>
<h3>1.1  미분 가능한 칼만 필터 (Differentiable Kalman Filters)</h3>
<p>미분 가능한 칼만 필터(Differentiable Kalman Filter, DKF)는 고전적 칼만 필터의 순환적(Recursive) 알고리즘 구조를 계산 그래프(Computational Graph) 상에 구현하여, 필터의 출력이 입력과 파라미터에 대해 미분 가능하도록 만든 구조다. 이를 통해 최종 상태 추정 오차(State Estimation Error)에 대한 손실 함수(Loss Function)를 정의하고, 역전파(Backpropagation)를 통해 필터 내부의 파라미터를 직접 최적화할 수 있다.7</p>
<p>전통적인 칼만 필터의 예측(Prediction)과 업데이트(Update) 단계는 기본적으로 행렬 덧셈, 곱셈, 역행렬 연산으로 구성되어 있어 그 자체로 미분 가능하다. DKF의 핵심 혁신은 고정된 상수로 취급되던 행렬들을 신경망의 출력으로 대체하는 데 있다.</p>
<h4>1.1.1  구조와 작동 원리</h4>
<p>DKF 프레임워크에서 상태 전이 모델 <span class="math math-inline">f</span>와 관측 모델 <span class="math math-inline">h</span>, 그리고 노이즈 공분산 <span class="math math-inline">\mathbf{Q}, \mathbf{R}</span>은 다음과 같이 신경망으로 파라미터화된다.<br />
<span class="math math-display">
\begin{aligned} \mathbf{x}_{k|k-1} &amp;= f_\theta(\mathbf{x}_{k-1|k-1}, \mathbf{u}_{k-1}) \\ \mathbf{Q}_k &amp;= g_\phi(\mathbf{x}_{k-1|k-1}, \mathbf{u}_{k-1}) \\ \mathbf{z}_k &amp;= h_\psi(\mathbf{x}_{k|k-1}) + \mathbf{v}_k, \quad \mathbf{v}_k \sim \mathcal{N}(0, \mathbf{R}_k) \\ \mathbf{R}_k &amp;= r_\xi(\mathbf{z}_k) \end{aligned}
</span><br />
여기서 <span class="math math-inline">f_\theta, g_\phi, h_\psi, r_\xi</span>는 각각 가중치 <span class="math math-inline">\theta, \phi, \psi, \xi</span>를 가지는 신경망이다. 특히 <span class="math math-inline">\mathbf{Q}_k</span>와 <span class="math math-inline">\mathbf{R}_k</span>를 신경망으로 추정하는 것은 ’적응형 칼만 필터(Adaptive Kalman Filter)’의 딥러닝 버전이라 할 수 있다. 예를 들어, 로봇이 미끄러운 바닥을 주행할 때는 바퀴의 슬립으로 인해 프로세스 노이즈가 증가하는데, 신경망 <span class="math math-inline">g_\phi</span>가 IMU 데이터나 바퀴 인코더의 패턴을 분석하여 <span class="math math-inline">\mathbf{Q}_k</span> 값을 동적으로 키움으로써 필터가 모델 예측보다 센서 측정치(만약 신뢰할 수 있다면)나 다른 정보에 더 의존하게 만들 수 있다.9</p>
<p>최근 연구인 <strong>DKFNet</strong>은 이러한 접근을 더욱 확장하여 필드 반전(Field Inversion) 기술과 결합했다. 근사적인 물리 모델과 실제 시스템 간의 불일치를 신경망이 학습하도록 설계된 이 구조는, 로켓 동역학 모델과 같은 복잡한 비선형 시스템에서 상태 재구성 오차를 기존 칼만 필터 대비 90% 이상 감소시키는 성과를 보였다.8 이는 순수 데이터 기반 모델(LSTM 등)이 가지는 물리적 해석의 모호함과, 순수 모델 기반 필터가 가지는 모델링 오차의 취약성을 동시에 해결한 사례다.</p>
<h4>1.1.2  다중 센서 융합과 협력적 인식 (Multi-Sensor Fusion &amp; Cooperative Perception)</h4>
<p>DKF의 강력함은 이질적인 센서 데이터를 융합할 때 더욱 빛을 발한다. 자율 주행 차량에서 LiDAR, 레이더, 카메라 데이터를 융합하여 주변 객체를 추적하는 3D 다중 객체 추적(MOT) 작업에서, <strong>DMSTrack</strong>과 같은 연구는 미분 가능한 다중 센서 칼만 필터를 제안했다.10 이 시스템은 센서 간의 상호 보완적인 정보를 딥러닝으로 추출하여 공분산 행렬을 조절함으로써, 센서 중 하나가 가려지거나 오작동하는 상황에서도 강건한 추적 성능을 유지한다. 또한, V2V(Vehicle-to-Vehicle) 통신을 통한 협력적 인식 상황에서도 각 차량의 위치 불확실성을 학습된 공분산으로 전파하여 전체 시스템의 정확도를 높인다.</p>
<h3>1.2  학습된 관성 오도메트리 (Learned Inertial Odometry): 데드 레코닝의 한계 극복</h3>
<p>관성측정장치(IMU)는 모든 로봇에 필수적인 저비용 센서이지만, 가속도와 각속도를 적분하여 위치를 추정하는 방식은 센서 바이어스와 노이즈로 인해 시간이 지남에 따라 오차가 기하급수적(<span class="math math-inline">t^2</span>)으로 누적되는 치명적인 단점이 있다. 이를 해결하기 위해 등장한 것이 **학습된 관성 오도메트리(Learned Inertial Odometry)**이다.11</p>
<h4>1.2.1  딥러닝 기반 변위 추정</h4>
<p>전통적인 관성 항법 시스템(INS)이 가속도의 이중 적분에 의존하는 것과 달리, 학습 기반 접근법은 윈도우(Window) 단위의 IMU 데이터 시퀀스를 입력받아 해당 구간 동안의 변위(Displacement)와 방향 변화(Orientation Change)를 직접 회귀(Regression)한다. <strong>RoNIN</strong>이나 <strong>IONet</strong>과 같은 초기 연구들은 IMU 데이터를 2D/3D 이동 벡터로 매핑하는 CNN 또는 LSTM 모델을 사용하여, 적분 오차의 누적 없이 보행자나 로봇의 궤적을 복원할 수 있음을 증명했다.12</p>
<h4>1.2.2  하이브리드 접근: TLIO (Tight Learned Inertial Odometry)</h4>
<p>그러나 순수 학습 기반 방식은 절대적인 위치(Global Position)를 추정할 때 여전히 장기적인 드리프트(Long-term Drift) 문제가 발생할 수 있으며, 학습 데이터에 포함되지 않은 동작(Out-of-Distribution Motion)에 취약하다. 이를 보완하기 위해 <strong>TLIO</strong>는 딥러닝과 EKF를 타이트하게 결합했다.12</p>
<ol>
<li><strong>네트워크 역할:</strong> 신경망은 IMU 버퍼를 입력받아 3D 변위뿐만 아니라 그 **불확실성(Uncertainty, 공분산)**까지 함께 추정한다.</li>
<li><strong>필터 역할:</strong> EKF는 신경망의 추정치를 마치 ’가상의 센서 측정치(Pseudo-measurement)’로 받아들여, IMU의 바이어스, 속도, 자세를 보정하는 업데이트 단계에 사용한다.</li>
</ol>
<p>이 구조에서 신경망은 복잡한 모션 패턴과 센서 노이즈 특성을 학습하고, EKF는 물리적 상태 간의 관계(예: 속도는 위치의 미분)를 강제하여 전체 추정의 일관성(Consistency)을 유지한다. 최근에는 트랜스포머(Transformer) 아키텍처를 도입하여 시계열 데이터의 장기 의존성(Long-term Dependency)을 학습함으로써, 드론 레이싱과 같이 급격한 기동이 포함된 상황에서도 정밀한 오도메트리를 수행하는 <strong>Tartan-IMU</strong>와 같은 연구가 진행되고 있다.13 이는 SOTA 모델 대비 수렴 속도를 33% 향상시키고 다양한 로봇 플랫폼 간의 일반화 성능을 입증했다.</p>
<h3>1.3  미분 가능한 팩터 그래프 (Differentiable Factor Graph Optimization)</h3>
<p>SLAM(Simultaneous Localization and Mapping) 분야의 표준인 팩터 그래프 최적화(Factor Graph Optimization) 또한 딥러닝과 결합하여 진화하고 있다. 팩터 그래프는 변수 노드(Variable Node: 로봇의 포즈, 랜드마크 위치)와 팩터 노드(Factor Node: 센서 측정치에 기반한 제약 조건) 간의 확률적 관계를 그래프로 표현하고, 이를 통해 전체 시스템의 에러를 최소화하는 최적화 문제(MAP Inference)를 푼다.</p>
<h4>1.3.1  최적화 솔버의 학습 가능화</h4>
<p>**미분 가능한 팩터 그래프 최적화(DFGO)**는 Levenberg-Marquardt(LM)나 Gauss-Newton과 같은 비선형 최소자승(Non-linear Least Squares) 솔버의 과정을 미분 가능한 연산으로 구현한다. 이를 통해 최적화의 결과(추정된 궤적)와 정답(Ground Truth) 사이의 오차를 역전파하여, 팩터 그래프를 구성하는 요소들을 학습할 수 있다.14</p>
<ul>
<li><strong>학습 가능한 비용 함수:</strong> 기존에는 사람이 수동으로 설계하던 강건 커널(Robust Kernel, 예: Huber Loss)이나 에너지 함수를 신경망이 학습한다.</li>
<li><strong>적응형 가중치:</strong> 특징점 매칭의 신뢰도나 루프 클로저(Loop Closure) 감지의 정확도를 신경망이 예측하고, 이 값을 팩터의 정보 행렬(Information Matrix, <span class="math math-inline">\Sigma^{-1}</span>) 가중치로 사용한다.</li>
</ul>
<p>예를 들어, 조도가 급격히 변하거나 텍스처가 부족한 환경에서 비주얼 오도메트리를 수행할 때, CNN은 이미지의 특징점이 얼마나 신뢰할 수 있는지에 대한 불확실성을 예측하여 팩터 그래프에 전달한다.16 만약 CNN이 특정 특징점의 불확실성이 높다고 판단하면, 최적화 과정에서 해당 팩터의 영향력을 줄여 전체 궤적의 정확도를 보존한다. 이러한 ’학습된 스무더(Learned Smoother)’는 고정된 노이즈 모델이 처리하지 못하는 비정상(Non-stationary) 노이즈 환경에서 탁월한 성능을 발휘한다.17</p>
<h2>2.  잔차 물리학과 하이브리드 동역학 (Residual Physics &amp; Hybrid Dynamics): 모델링의 간극 메우기</h2>
<p>제어 이론의 핵심은 시스템의 미래 상태를 예측하는 동역학 모델(Dynamics Model)에 있다. 로봇 공학에서 강체 동역학(Rigid Body Dynamics)은 뉴턴-오일러 방정식을 통해 매우 정확하게 기술될 수 있다. 그러나 현실 세계의 물리 현상은 강체 가정만으로는 설명할 수 없는 복잡성을 띤다. 조인트의 마찰(Friction), 기어박스의 백래시(Backlash), 공기 역학적 저항, 케이블의 장력, 그리고 무엇보다 유연체(Soft Body)나 액체와의 상호작용은 수식으로 완벽히 모델링하기 어렵다. **잔차 물리학(Residual Physics)**은 이러한 분석적 모델의 한계를 데이터로 채우는 하이브리드 접근법이다.3</p>
<h3>2.1  잔차 학습 (Residual Learning)의 수식적 정식화</h3>
<p>잔차 물리학 프레임워크는 시스템의 실제 동역학 <span class="math math-inline">f_{real}</span>을 명목상의 분석적 모델(Nominal Analytical Model) <span class="math math-inline">f_{ana}</span>와 학습된 잔차 항(Residual Term) <span class="math math-inline">f_{res}</span>의 합으로 정의한다.<br />
<span class="math math-display">
\mathbf{x}_{t+1} = f_{real}(\mathbf{x}_t, \mathbf{u}_t) \approx f_{ana}(\mathbf{x}_t, \mathbf{u}_t; \theta_{phy}) + f_{res}(\mathbf{x}_t, \mathbf{u}_t; \theta_{nn})
</span></p>
<ul>
<li><strong><span class="math math-inline">f_{ana}</span> (Analytical Term):</strong> 질량, 관성 모멘트, 링크 길이 등 물리적 파라미터 <span class="math math-inline">\theta_{phy}</span>에 기반한 모델이다. 이는 데이터가 부족한 영역(Out-of-Distribution)에서도 기본적인 물리 법칙을 따르게 하여 모델의 일반화(Generalization) 성능을 보장하는 ‘안전판’ 역할을 한다.</li>
<li><strong><span class="math math-inline">f_{res}</span> (Residual Term):</strong> 신경망 <span class="math math-inline">\theta_{nn}</span>으로 구현되며, 분석적 모델이 놓친 비선형성이나 미세한 동역학적 효과를 학습한다.</li>
</ul>
<h4>2.1.1  활용 사례와 이점</h4>
<p>이 구조는 순수 블랙박스 모델이 학습 데이터 범위를 벗어났을 때 물리적으로 불가능한 예측(예: 에너지가 무한히 생성되거나 질량이 변하는 등)을 하는 것을 방지한다. Google의 <strong>TossingBot</strong> 연구는 이러한 잔차 물리학을 사용하여 물체를 던져서 잡는 동적인 작업을 수행했다.19 강체 투사체 운동 방정식으로 대략적인 던지기 속도를 계산하고, 공기 저항이나 물체의 무게 중심 변화로 인한 오차를 잔차 신경망이 보정함으로써, 로봇은 다양한 물체를 매우 높은 정확도로 바구니에 넣을 수 있었다.</p>
<p>최근 연구인 <strong>ActivePusher</strong> 프레임워크는 여기서 한 걸음 더 나아가, 커널 기반의 불확실성 추정을 잔차 학습에 통합했다.18 잔차 모델의 예측 불확실성이 높은 영역(데이터가 부족한 상태)을 식별하고, 로봇이 능동적으로 해당 영역의 데이터를 수집(Active Learning)하도록 유도함으로써 모델 기반 제어(Model-Based Control)의 신뢰성을 획기적으로 높였다.</p>
<h3>2.2  뉴럴 역동역학 (Neural Inverse Dynamics)</h3>
<p>로봇 팔 제어, 특히 힘 제어(Force Control)나 임피던스 제어(Impedance Control)를 수행하기 위해서는 목표 가속도를 생성하기 위해 필요한 조인트 토크를 계산하는 역동역학(Inverse Dynamics) 모델이 필수적이다.<br />
<span class="math math-display">
\tau = M(\mathbf{q})\ddot{\mathbf{q}} + C(\mathbf{q}, \dot{\mathbf{q}})\dot{\mathbf{q}} + G(\mathbf{q}) + F(\dot{\mathbf{q}})
</span><br />
여기서 관성 행렬 <span class="math math-inline">M</span>, 코리올리 힘 <span class="math math-inline">C</span>, 중력 <span class="math math-inline">G</span>는 CAD 모델로부터 비교적 정확하게 얻을 수 있지만, 마찰항 <span class="math math-inline">F</span>나 기타 비선형 효과는 모델링이 매우 까다롭다. <strong>뉴럴 역동역학</strong>은 분석적 역동역학 모델의 출력(<span class="math math-inline">\tau_{ana}</span>)에 신경망이 예측한 보정 토크(<span class="math math-inline">\tau_{nn}</span>)를 더하는 방식을 사용한다.<br />
<span class="math math-display">
\tau_{total} = \tau_{ana}(\mathbf{q}, \dot{\mathbf{q}}, \ddot{\mathbf{q}}) + \tau_{nn}(\mathbf{q}, \dot{\mathbf{q}}, \ddot{\mathbf{q}}, \mathbf{h}_t)
</span><br />
최신 연구에서는 단순한 피드포워드 신경망 대신 LSTM이나 GRU와 같은 순환 신경망(RNN)을 사용하여 시스템의 지연(Delay)과 히스테리시스(Hysteresis) 효과를 모델링한다.20 특히 협동 로봇(Cobot)과 같이 유연 관절(Elastic Joint)을 사용하는 경우, 기어의 탄성으로 인한 진동이나 마찰 변화가 심한데, 뉴럴 역동역학 모델은 이러한 비선형성을 효과적으로 보상한다. 이를 통해 피드백 제어기의 게인(Gain)을 낮출 수 있게 되어, 로봇을 더 ‘부드럽게(Compliant)’ 만들면서도 정확한 경로 추종(Tracking) 성능을 유지하는 안전한 제어가 가능해진다.</p>
<h3>2.3  미분 가능한 물리 시뮬레이션 (Differentiable Physics Simulation)</h3>
<p>나아가 시뮬레이터 자체를 미분 가능하게 구현하여 동역학 모델로 사용하는 시도가 확산되고 있다. 기존의 물리 엔진(Bullet, MuJoCo, ODE)은 미분이 불가능하거나 수치적 기울기 근사(Finite Difference)에 의존해야 했으나, <strong>Brax</strong>, <strong>DiffTaichi</strong>, <strong>Dojo</strong>와 같은 미분 가능한 시뮬레이터는 물리 연산 과정 전체를 체인 룰(Chain Rule)로 엮어낸다.22</p>
<p>이를 제어 루프에 통합하면, 시뮬레이션 상의 예측 오차를 줄이기 위해 물리 파라미터(질량, 마찰계수, 탄성계수 등)를 역전파를 통해 직접 최적화(System Identification)할 수 있다. 이는 <strong>Sim-to-Real</strong> 적응에 있어 강력한 도구가 된다. 실제 로봇의 궤적 데이터를 기반으로 시뮬레이터의 파라미터를 미세 조정(Fine-tuning)하여 현실과 가상의 격차를 줄이는 것이다. 특히 소프트 로봇이나 유체 역학이 개입된 시스템에서, 미분 가능한 시뮬레이터에 신경망 잔차를 결합한 모델은 수천 번의 실험 없이도 현실 세계의 복잡한 거동을 모사하고 제어 입력을 최적화하는 데 성공하고 있다.3</p>
<h2>3.  학습 기반 모델 예측 제어 (Learning-Augmented MPC): 최적 제어의 지능화</h2>
<p>모델 예측 제어(Model Predictive Control, MPC)는 미래의 일정 구간(Prediction Horizon) 동안 시스템의 거동을 예측하고, 비용 함수(Cost Function)를 최소화하는 최적의 제어 입력 시퀀스를 계산하는 강력한 기법이다. 제약 조건(Constraint)을 명시적으로 처리할 수 있다는 장점 덕분에 자율 주행, 보행 로봇, 화학 공정 등 다양한 분야에서 표준으로 자리 잡았다.</p>
<p>그러나 MPC는 계산 비용이 높아 고주파수 제어가 어렵고, 예측 모델이 부정확할 경우 성능이 급격히 저하되며, 긴 시계(Long Horizon)를 다루기 어렵다는 한계가 있다. 딥러닝은 MPC의 이러한 약점을 보완하는 ’직관(Intuition)’과 ’가속(Acceleration)’의 역할을 수행하며 새로운 형태의 제어 아키텍처를 탄생시켰다.</p>
<table><thead><tr><th><strong>특징</strong></th><th><strong>전통적 MPC</strong></th><th><strong>학습 기반 MPC (Learning-Augmented MPC)</strong></th></tr></thead><tbody>
<tr><td><strong>모델링</strong></td><td>물리 기반 수식 (White-box)</td><td>물리 모델 + 신경망 잔차 / 완전 학습 모델</td></tr>
<tr><td><strong>비용 함수</strong></td><td>수동 설계 (Quadratic Cost)</td><td>데이터로부터 학습 (Inverse RL) / 문맥 적응형</td></tr>
<tr><td><strong>최적화</strong></td><td>실시간 수치 최적화 (QP/NLP Solver)</td><td>정책 네트워크로 초기화 (Warm-start) / 미분 가능 최적화</td></tr>
<tr><td><strong>계획 범위</strong></td><td>짧은 호라이즌 (계산 부하 제약)</td><td>가치 함수(Value Function)로 무한 호라이즌 근사</td></tr>
</tbody></table>
<h3>3.1  계층적 제어: 고수준 정책과 저수준 MPC (High-level Policy for Low-level MPC)</h3>
<p>가장 실용적이고 널리 사용되는 결합 형태는 딥러닝/강화학습(RL) 정책이 고수준의 전략적 의사결정을 내리고, MPC가 저수준의 동역학적 실행과 안전성을 담당하는 계층적 구조이다.</p>
<h4>3.1.1  파라미터 적응형 MPC (High-MPC)</h4>
<p>이 아키텍처에서 신경망은 직접 제어 입력(토크, 전압)을 출력하지 않는다. 대신 환경의 상태(예: 지면의 미끄러움, 장애물 밀집도, 바람의 세기)를 인지하여 MPC의 메타 파라미터—비용 함수의 가중치(Weight), 예측 호라이즌 길이, 제약 조건의 여유분(Slack)—를 실시간으로 조정한다.23</p>
<p>예를 들어, 쿼드로터가 좁고 동적인 게이트를 통과해야 하는 미션을 수행할 때, 고수준 신경망 정책은 “공격적으로 비행하라“는 추상적 지령을 MPC의 속도 추종 오차에 대한 페널티 가중치를 낮추고 입력 변화율에 대한 페널티를 줄이는 방식으로 전달한다. MPC는 이렇게 조정된 최적화 문제를 풀어 물리적으로 실현 가능한 최적 궤적을 생성한다. 이는 RL의 상황 적응력과 MPC의 동역학적 안정성을 동시에 확보하는 전략이다.</p>
<h4>3.1.2  터미널 세트 및 비용 학습 (Learning Terminal Cost/Set)</h4>
<p>MPC는 계산량 문제로 유한한 호라이즌(예: 1~2초)만을 예측한다. 이로 인해 장기적인 목표를 보지 못하는 근시안적(Myopic) 행동을 할 수 있다. 이를 해결하기 위해 RL의 가치 함수(Value Function, <span class="math math-inline">V(x)</span>)를 학습하여 MPC의 종단 비용(Terminal Cost)으로 사용한다.26<br />
<span class="math math-display">
J = \sum_{t=0}^{N-1} l(x_t, u_t) + V_\phi(x_N)
</span><br />
여기서 <span class="math math-inline">V_\phi(x_N)</span>은 신경망으로 근사된 가치 함수로, 예측 호라이즌 이후의 장기적인 기대 비용을 나타낸다. Actor-Critic MPC 프레임워크에서는 Critic 네트워크가 MPC의 가치 함수 역할을 수행하여, 짧은 호라이즌의 MPC로도 초인적인 드론 레이싱 성능을 달성할 수 있음을 보였다.27</p>
<h3>3.2  미분 가능한 MPC (Differentiable MPC)</h3>
<p>**미분 가능한 MPC(DPC)**는 MPC 솔버 자체를 신경망의 하나의 미분 가능한 층(Layer)으로 통합하는 기술이다. Amos와 Kolter의 <strong>OptNet</strong> 연구에서 촉발된 이 흐름은, 제어 입력을 출력하는 최적화 문제(Quadratic Programming 등)의 해(Solution)를 입력 파라미터에 대해 미분할 수 있게 만들었다(KKT 조건의 미분을 통해).28</p>
<p>이 아키텍처의 강력함은 ’End-to-End 학습’이 가능하다는 점에 있다. 로봇이 수행한 작업의 결과(예: 충돌 여부, 에너지 소모량, 목표 도달 시간)로부터 얻은 손실(Loss)을 MPC 솔버를 통과해 역전파시켜, 동역학 모델이나 비용 함수의 파라미터를 직접 학습시킨다. 이는 사람이 수동으로 튜닝하던 MPC의 가중치 행렬(<span class="math math-inline">\mathbf{Q}, \mathbf{R}</span>)이나 제약 조건을 데이터 기반으로 자동 최적화하는 것을 의미한다.</p>
<p>최신 연구에서는 DPC를 활용하여 드리프트 주행(Drifting)과 같이 타이어 마찰이 극도로 비선형적인 영역에서도 강건한 제어 정책을 학습하는 성과를 보였다.30 DPC는 신경망 제어기(Policy Network)가 가지는 해석 불가능성 문제를 완화하면서, 명시적인 제약 조건을 준수하는 제어기를 데이터로 학습시킬 수 있는 이상적인 방법을 제공한다.</p>
<h3>3.3  확산 모델 기반 계획 (Diffusion Models for Motion Planning)</h3>
<p>생성형 AI의 일종인 확산 모델(Diffusion Model)이 이미지 생성을 넘어 로봇의 경로 계획(Motion Planning) 모듈로 진입했다. <strong>Diffuser</strong>, <strong>Decision Diffuser</strong>와 같은 모델은 경로 계획을 조건부 생성(Conditional Generation) 문제로 치환한다. 확산 모델은 노이즈로부터 시작하여 제약 조건과 목표 지점을 만족하는 유효한 경로를 점진적으로 ’Denoising’하여 생성해낸다.31</p>
<p>이는 복잡한 다중 모달(Multi-modal) 분포를 모델링할 수 있다는 점에서 기존의 샘플링 기반 계획법(RRT 등)이나 최적화 기반 계획법(TrajOpt)과 차별화된다. 예를 들어, 장애물을 피하는 방법이 왼쪽과 오른쪽 두 가지가 있을 때, 단일 최적해를 찾는 MPC는 국소 최적해(Local Minima)에 빠질 수 있지만, 확산 모델은 두 가지 모드를 모두 포함하는 분포를 학습하고 다양성을 가진 경로 후보를 생성할 수 있다.</p>
<p>최신 연구인 **CAMPD (Context-Aware Motion Planning Diffusion)**는 센서 데이터와 상황 정보를 조건(Conditioning)으로 받아, 재학습 없이 다양한 환경에 적응하는 경로를 실시간으로 생성함으로써 확산 모델의 단점인 느린 추론 속도 문제를 극복하고 있다.33 이 구조에서 확산 모델은 대략적인 전역 경로(Global Trajectory)를 생성하는 ‘상상(Imagination)’ 모듈 역할을 하며, 생성된 경로는 저수준 제어기(Inverse Dynamics Controller 등)에 의해 추적된다.</p>
<h2>4.  행동을 위한 인식 (Perception for Action): 시각 서보와 VLA의 모듈화</h2>
<p>로봇의 인식 모듈은 단순히 “이것은 컵이다“라고 분류하는 것을 넘어, “컵을 잡으려면 어디로 손을 뻗어야 하는가“를 알려주어야 한다. 딥러닝 기반 모듈화는 인식과 제어의 연결 고리를 재정의하고 있다.</p>
<h3>4.1  딥 특징 기반 시각 서보 (Deep Feature-based Visual Servoing)</h3>
<p>전통적인 시각 서보(Visual Servoing)는 이미지 상의 점, 선, 모서리와 같은 기하학적 특징(Feature)을 추출하고, 이 특징들의 현재 위치와 목표 위치 간의 오차를 줄이도록 로봇을 제어한다(Interaction Matrix 사용). 그러나 수작업으로 설계된 특징(Hand-crafted Features)은 조명 변화나 복잡한 배경에 취약하다.</p>
<p>딥러닝 기반 시각 서보는 이를 **학습된 특징(Learned Features)**으로 대체한다. CNN이나 비전 트랜스포머(ViT)가 추출한 고차원 특징 맵(Feature Map) 혹은 키포인트(Keypoints)를 시각 서보의 입력으로 사용한다.34</p>
<ul>
<li><strong>Feature Agnostic VS:</strong> DDPG와 같은 강화학습 알고리즘을 사용하여, 어떤 특징이 제어에 유용한지를 스스로 학습한다.</li>
<li><strong>Dense Descriptors:</strong> <strong>Florence-2</strong>나 <strong>DINOv2</strong>와 같은 파운데이션 모델에서 추출한 밀집 서술자(Dense Descriptor)를 사용하면, 별도의 학습 없이도 강건한 특징 매칭이 가능하여 시각 서보의 일반화 성능을 극대화할 수 있다.11</li>
</ul>
<p>이 방식은 End-to-End 비주모터 제어(3.1.3)와 달리, 제어 법칙(Control Law) 자체는 해석 가능한 수식(Jacobian 기반)을 유지하되, 그 입력이 되는 ’눈’을 딥러닝으로 교체한 것이다.</p>
<h3>4.2  모듈형 VLA (Vision-Language-Action) 아키텍처</h3>
<p>거대 언어 모델(LLM)과 비전-언어 모델(VLM)의 발전은 <strong>VLA(Vision-Language-Action)</strong> 모델이라는 새로운 흐름을 만들었다. RT-2나 OpenVLA와 같은 모델은 비전과 언어 입력을 받아 직접 로봇의 액션 토큰을 출력하는 E2E 방식을 취하지만, 제어 아키텍처 관점에서는 이를 <strong>모듈형</strong>으로 사용하는 흐름이 강화되고 있다.36</p>
<ul>
<li><strong>High-level Planner로서의 VLA:</strong> VLA 모델이 직접 1kHz의 모터 토크를 생성하는 것은 비효율적이다. 대신 VLA는 “빨간 컵을 집어라“라는 명령을 받아 “컵의 파지 포즈(Grasp Pose)“나 “경유지(Waypoint)“와 같은 중간 표현(Intermediate Representation)을 출력한다.38</li>
<li><strong>Code Generation:</strong> LLM이 로봇의 행동을 제어하는 Python 코드를 생성하고, 이 코드가 기존의 모션 플래닝 라이브러리(OMPL 등)나 MPC 제어기를 호출하는 방식이다.40</li>
</ul>
<p>이러한 모듈형 VLA 접근은 파운데이션 모델의 추론 능력(Reasoning)과 기존 제어기의 정밀성(Precision) 및 안전성을 결합하는 가장 현실적인 대안으로 평가받는다.</p>
<h2>5.  안전성과 이론적 보증 (Safety &amp; Theoretical Guarantees)</h2>
<p>딥러닝 모듈의 가장 큰 치명적 약점은 ‘블랙박스’ 특성으로 인한 안전성 보장의 부재이다. 제어 이론과 딥러닝의 결합은 이 문제를 해결하기 위해 <strong>안전 필터(Safety Filter)</strong> 개념을 도입한다.</p>
<h3>5.1  뉴럴 제어 장벽 함수 (Neural Control Barrier Functions)</h3>
<p>제어 장벽 함수(Control Barrier Functions, CBF)는 시스템 상태가 정의된 안전 집합(Safe Set)을 벗어나지 않도록 강제하는 수학적 도구다. 최근 연구들은 고차원 센서 데이터(LiDAR, Depth Image)로부터 CBF를 직접 학습하거나(<strong>Neural CBF</strong>), 강화학습 정책의 출력에 CBF 기반의 안전 필터를 씌워 위험한 행동을 실시간으로 차단하는 방식을 제안한다.42</p>
<p>이 구조에서 학습된 정책 <span class="math math-inline">\pi_\theta(s)</span>는 성능을 최대화하기 위해 자유롭게 행동을 생성하지만, 실제 로봇에 인가되는 제어 입력 <span class="math math-inline">u</span>는 다음의 최적화 문제(QP)를 거쳐 수정된다.<br />
<span class="math math-display">
\begin{aligned} u^* = \operatorname*{argmin}_u &amp; \quad \| u - \pi_\theta(s) \|^2 \\ \text{s.t.} &amp; \quad \dot{h}(s, u) \geq -\gamma h(s) \end{aligned}
</span><br />
여기서 <span class="math math-inline">h(s)</span>는 학습된 장벽 함수이다. 이 방식은 딥러닝의 불확실성을 수학적으로 정의된 안전 영역 내에 가둠으로써, ‘절대 충돌하지 않는’ 학습 기반 제어를 지향한다.</p>
<h3>5.2  도달 가능성 분석 (Reachability Analysis) 및 인증</h3>
<p>도달 가능성 분석은 로봇이 특정 시간 내에 도달할 수 있는 모든 상태의 집합(Reachable Set)을 계산하는 기술이다. 딥러닝 기반 제어기에 대해 도달 가능성 분석을 수행하여, 최악의 시나리오(Worst-case Scenario)에서도 로봇이 안전함을 수학적으로 ’인증(Certify)’하려는 연구가 활발하다.44 특히 고차원 비선형 시스템을 위한 도달-회피(Reach-Avoid) 문제를 학습 프레임워크로 해결하고, 사후 인증(Post-learning Certification)을 통해 안전성을 검증하는 기법들이 제안되고 있다. 이는 자율 주행이나 의료 로봇과 같이 안전이 최우선인 분야에서 딥러닝 모듈을 채택하기 위한 필수적인 전제 조건이 되고 있다.</p>
<h2>6.  결론: 공생의 아키텍처를 향하여</h2>
<p>지금까지 살펴본 **딥러닝 기반 모듈화(3.1.2)**는 고전적 파이프라인(3.1.1)의 경직성과 End-to-End 방식(3.1.3)의 불투명성 사이에서 단순한 타협점이 아니다. 이는 제어 이론이 제공하는 **구조적 보증(Structural Guarantee)**과 딥러닝이 제공하는 <strong>데이터 효율성(Data Efficiency)</strong> 및 표현력을 융합한 진화된 형태다.</p>
<p>현재의 SOTA 기술 지형도에서 상태 추정기(KF), 동역학 모델, 최적 제어기(MPC)라는 검증된 뼈대는 유지되고 있다. 그러나 그 내부를 채우는 파라미터와 비선형 함수들은 더 이상 사람이 손으로 튜닝하지 않는다. 대신 신경망이 방대한 데이터를 통해 이를 채워 넣는다. 미분 가능한 프로그래밍(Differentiable Programming) 패러다임 아래, 로봇 제어 시스템은 ‘설계되는’ 것에서 ’학습되는 구조’로 변모하고 있으며, 이는 향후 파운데이션 모델 시대(3.3)로 나아가는 가장 견고한 징검다리 역할을 수행할 것이다. 특히 잔차 학습과 미분 가능한 제어기의 결합은 Sim-to-Real 격차를 메우는 가장 현실적이고 강력한 해법으로 자리 잡았으며, LLM/VLA 모델이 고수준 추론을 담당하고 모듈형 신경망이 저수준 제어를 담당하는 계층적 공진화가 더욱 가속화될 것으로 전망된다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Research on Deep Learning-Based Human–Robot Static/Dynamic Gesture-Driven Control Framework - MDPI, https://www.mdpi.com/1424-8220/25/23/7203</li>
<li>The Integration of Prediction and Planning in Deep Learning Automated Driving Systems: A Review - arXiv, https://arxiv.org/html/2308.05731v3</li>
<li>Sim-to-Real of Soft Robots With Learned Residual Physics - ResearchGate, https://www.researchgate.net/publication/383361106_Sim-to-Real_of_Soft_Robots_with_Learned_Residual_Physics</li>
<li>STATE ESTIMATION FOR ROBOTICS - University of Toronto, http://asrl.utias.utoronto.ca/~tdb/bib/barfoot_ser17.pdf</li>
<li>STATE ESTIMATION AND TRAJECTORY TRACKING CONTROL FOR A NONLINEAR AND MULTIVARIABLE BIOETHANOL PRODUCTION SYSTEM STATE ESTIMATION AND TRAJECTORY TRACKING CONTROL FOR A NONLINEAR AND MULTIVARIABLE BIOETHANOL PRODUCTION SYSTEM - SciELO, https://www.scielo.br/j/bjce/a/xbXgjxgsjbsSYkxVDY5sPDQ/?lang=en</li>
<li>Neural Predictive Monitoring Under Partial Observability - ArTS, https://arts.units.it/retrieve/e2913fdf-8d9e-f688-e053-3705fe0a67e0/2998179_Cairoli2021_Chapter_NeuralPredictiveMonitoringUnde-Post_print.pdf</li>
<li>DKFNet: Differentiable Kalman Filter for Field Inversion and Machine Learning - arXiv, https://arxiv.org/html/2509.07474v1</li>
<li>DKFNet: Differentiable Kalman Filter for Field Inversion and Machine Learning - arXiv, https://arxiv.org/pdf/2509.07474</li>
<li>Hybrid Filtering Technique for Accurate GNSS State Estimation - MDPI, https://www.mdpi.com/2072-4292/17/9/1552</li>
<li>eddyhkchiu/DMSTrack: [ICRA2024] Official code of the paper “Probabilistic 3D Multi-Object Cooperative Tracking for Autonomous Driving via Differentiable Multi-Sensor Kalman Filter” - GitHub, https://github.com/eddyhkchiu/DMSTrack</li>
<li>Software/Datasets - Robotics and Perception Group, https://rpg.ifi.uzh.ch/software_datasets.html</li>
<li>(PDF) TLIO: Tight Learned Inertial Odometry - ResearchGate, https://www.researchgate.net/publication/342758907_TLIO_Tight_Learned_Inertial_Odometry</li>
<li>Tartan IMU: A Light Foundation Model for Inertial Positioning in Robotics - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_Tartan_IMU_A_Light_Foundation_Model_for_Inertial_Positioning_in_CVPR_2025_paper.pdf</li>
<li>Learning Dynamics of a Ball with Differentiable Factor Graph and Roto-Translational Invariant Representations - arXiv, https://arxiv.org/html/2409.16467v2</li>
<li>brentyi/dfgo: Differentiable Factor Graph Optimization @ IROS 2021 - GitHub, https://github.com/brentyi/dfgo</li>
<li>Differentiable Factor Graph Optimization with Intelligent Covariance Adaptation for Accurate Smartphone Positioning | Request PDF - ResearchGate, https://www.researchgate.net/publication/374526372_Differentiable_Factor_Graph_Optimization_with_Intelligent_Covariance_Adaptation_for_Accurate_Smartphone_Positioning</li>
<li>Differentiable Factor Graph Optimization for Learning Smoothers - arXiv, https://arxiv.org/pdf/2105.08257</li>
<li>ActivePusher: Active Learning and Planning with Residual Physics for Nonprehensile Manipulation - arXiv, https://arxiv.org/html/2506.04646v1</li>
<li>Learning Accurate Whole-body Throwing with High-frequency Residual Policy and Pullback Tube Acceleration - arXiv, https://arxiv.org/html/2506.16986v1</li>
<li>End-to-End Learning of Hybrid Inverse Dynamics Models for Precise and Compliant Impedance Control - Robotics, https://www.roboticsproceedings.org/rss18/p066.pdf</li>
<li>Bidirectional recurrent learning of inverse dynamic models for robots with elastic joints: a real-time real-world implementation - Frontiers, https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2023.1166911/full</li>
<li>Learning on the Fly: Rapid Policy Adaptation via Differentiable Simulation - Robotics and Perception Group, https://rpg.ifi.uzh.ch/docs/Arxiv25_Pan.pdf</li>
<li>Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control - arXiv, https://arxiv.org/abs/2509.15799</li>
<li>[PDF] Learning High-Level Policies for Model Predictive Control - Semantic Scholar, https://www.semanticscholar.org/paper/Learning-High-Level-Policies-for-Model-Predictive-Song-Scaramuzza/986af458432a57f5cdba4252b93381cdec631a39</li>
<li>[2007.10284] Learning High-Level Policies for Model Predictive Control - arXiv, https://arxiv.org/abs/2007.10284</li>
<li>Actor-Critic Model Predictive Control: Differentiable Optimization meets Reinforcement Learning for Agile Flight - Robotics and Perception Group, https://rpg.ifi.uzh.ch/docs/TRO25_ACMPC_Romero.pdf</li>
<li>Actor-Critic Model Predictive Control: Differentiable Optimization meets Reinforcement Learning for Agile Flight - arXiv, https://arxiv.org/html/2306.09852v8</li>
<li>Differentiable Model Predictive Control on the GPU - arXiv, https://arxiv.org/html/2510.06179v1</li>
<li>(PDF) Differentiable Robust Model Predictive Control - ResearchGate, https://www.researchgate.net/publication/373164008_Differentiable_Robust_Model_Predictive_Control</li>
<li>Differentiable Model Predictive Control on the GPU - OpenReview, https://openreview.net/forum?id=bFYfV6c9zu</li>
<li>Diffusion Models for Robotic Manipulation: A Survey - arXiv, https://arxiv.org/html/2504.08438v3</li>
<li>Potential Based Diffusion Motion Planning, https://energy-based-model.github.io/potential-motion-plan/potential_motion_plan.pdf</li>
<li>Accelerated Multi-Modal Motion Planning Using Context-Conditioned Diffusion Models, https://autonomousrobots.nl/assets/images/workshops/2025_iros/accepted_papers/paper_3_Accelerated.pdf</li>
<li>Model &amp; Feature Agnostic Eye-in-Hand Visual Servoing using Deep Reinforcement Learning with Prioritized Experience Replay - IEEE Xplore, https://ieeexplore.ieee.org/document/8956447/</li>
<li>[1703.11000] Learning Visual Servoing with Deep Features and Fitted Q-Iteration - arXiv, https://arxiv.org/abs/1703.11000</li>
<li>A Survey on Vision-Language-Action Models for Autonomous Driving - arXiv, https://arxiv.org/html/2506.24044v1</li>
<li>A Survey on Vision-Language-Action Models for Autonomous Driving - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2025W/WDFM-AD/papers/Jiang_A_Survey_on_Vision-Language-Action_Models_for_Autonomous_Driving_ICCVW_2025_paper.pdf</li>
<li>Vision-Language-Action (VLA) Models: The AI Brain Behind the Next Generation of Robots &amp; Physical AI | by RAKTIM SINGH | Nov, 2025 | Medium, https://medium.com/@raktims2210/vision-language-action-vla-models-the-ai-brain-behind-the-next-generation-of-robots-physical-bced48e8ae94</li>
<li>Vision-Language-Action Models: Concepts, Progress, Applications and Challenges - arXiv, https://arxiv.org/html/2505.04769v1</li>
<li>AuDeRe: Automated Strategy Decision and Realization in Robot Planning and Control via LLMs - arXiv, https://arxiv.org/html/2504.03015v1</li>
<li>(PDF) Model learning for robot control: A survey - ResearchGate, https://www.researchgate.net/publication/51046423_Model_learning_for_robot_control_A_survey</li>
<li>A Step Toward World Models: A Survey on Robotic Manipulation - arXiv, https://arxiv.org/html/2511.02097v1</li>
<li>Differentiable Predictive Control for Robotics: A Data-Driven Predictive Safety Filter Approach | Request PDF - ResearchGate, https://www.researchgate.net/publication/384266801_Differentiable_Predictive_Control_for_Robotics_A_Data-Driven_Predictive_Safety_Filter_Approach</li>
<li>CASE 2025 Program | Wednesday August 20, 2025, https://ras.papercept.net/conferences/conferences/CASE25/program/CASE25_ContentListWeb_4.html</li>
<li>Deterministic Safety Guarantees for Learning-Based Control of Monotone Nonlinear Systems Under Uncertainty - IEEE Xplore, https://ieeexplore.ieee.org/iel8/7782633/10411713/10542324.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>