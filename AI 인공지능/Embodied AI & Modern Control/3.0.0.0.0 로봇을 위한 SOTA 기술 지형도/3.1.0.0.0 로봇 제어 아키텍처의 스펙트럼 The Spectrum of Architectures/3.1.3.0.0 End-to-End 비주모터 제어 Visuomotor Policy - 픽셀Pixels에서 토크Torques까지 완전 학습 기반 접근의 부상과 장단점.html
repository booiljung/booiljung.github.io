<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.1.3 End-to-End 비주모터 제어 (Visuomotor Policy): 픽셀(Pixels)에서 토크(Torques)까지, 완전 학습 기반 접근의 부상과 장단점</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.1.3 End-to-End 비주모터 제어 (Visuomotor Policy): 픽셀(Pixels)에서 토크(Torques)까지, 완전 학습 기반 접근의 부상과 장단점</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 3. 로봇을 위한 SOTA 기술 지형도</a> / <a href="index.html">3.1 로봇 제어 아키텍처의 스펙트럼 (The Spectrum of Architectures)</a> / <span>3.1.3 End-to-End 비주모터 제어 (Visuomotor Policy): 픽셀(Pixels)에서 토크(Torques)까지, 완전 학습 기반 접근의 부상과 장단점</span></nav>
                </div>
            </header>
            <article>
                <h1>3.1.3 End-to-End 비주모터 제어 (Visuomotor Policy): 픽셀(Pixels)에서 토크(Torques)까지, 완전 학습 기반 접근의 부상과 장단점</h1>
<p>로봇 공학의 역사에서 ’제어(Control)’와 ’인지(Perception)’는 오랫동안 분리된 영역이었다. 고전적인 로봇 제어 시스템은 감지(Sense), 계획(Plan), 행동(Act)이라는 명확히 구분된 모듈들의 순차적 파이프라인으로 구성되었다. 그러나 심층 신경망(Deep Neural Networks)의 비약적인 발전은 이러한 전통적인 경계를 허물고, 카메라 렌즈에 맺힌 원시 픽셀(Raw Pixels) 정보가 복잡한 중간 단계 없이 곧바로 로봇 관절의 토크(Torques) 명령으로 변환되는 급진적인 패러다임을 탄생시켰다. 이것이 바로 ’End-to-End 비주모터 제어(End-to-End Visuomotor Control)’이다.1 본 절에서는 이 혁신적인 접근법이 태동하게 된 배경부터, CNN과 공간 소프트맥스(Spatial Softmax)를 거쳐 트랜스포머(Transformer)와 디퓨전(Diffusion), 그리고 최신 플로우 매칭(Flow Matching) 기술에 이르기까지의 기술적 진화를 심도 있게 분석한다. 또한, 완전 학습 기반 접근이 갖는 본질적인 장점과 치명적인 단점, 그리고 이를 극복하기 위한 최신 연구 동향을 포괄적으로 고찰한다.</p>
<h2>1.  서론: 모듈형 파이프라인의 한계와 단일 신경망의 등장</h2>
<h3>1.1  고전적 제어 파이프라인의 정보 병목 현상</h3>
<p>전통적인 로봇 조작(Manipulation) 및 자율 주행 시스템은 철저한 분업화 원칙에 따라 설계되었다.3 라이다(LiDAR)나 카메라로 수집된 데이터는 ’인지 모듈’을 통해 객체의 위치, 자세(Pose), 맵(Map) 등의 상태(State) 정보로 추상화된다. 이 추상화된 상태 정보는 ’계획 모듈’로 전달되어 경로를 생성하고, 최종적으로 ’제어 모듈’이 역기구학(Inverse Kinematics)과 PID 제어 등을 통해 모터를 구동한다.4</p>
<p>이러한 모듈형 접근(Modular Pipeline)은 각 단계를 독립적으로 검증할 수 있다는 공학적 이점이 있다. 그러나 여기에는 치명적인 약점이 존재한다. 바로 ’정보의 손실’과 ’오류의 전파(Error Propagation)’이다. 인지 모듈이 복잡한 현실 세계를 저차원의 상태 정보로 압축하는 과정에서, 제어에 필요할 수도 있는 미세한 시각적 단서(예: 표면의 거칠기, 미묘한 그림자)는 노이즈로 간주되어 제거된다. 또한, 앞단의 인지 모듈에서 발생한 작은 오차는 파이프라인을 거치며 증폭되어, 계획 및 제어 단계에서는 회복 불가능한 실패를 초래할 수 있다.6 “쓰레기가 들어가면 쓰레기가 나온다(Garbage In, Garbage Out)“는 원칙은 모듈형 로봇 시스템에서 더욱 가혹하게 작용한다.</p>
<h3>1.2  “Pixels to Torques”: 패러다임의 전환</h3>
<p>End-to-End 비주모터 정책(Policy)은 이러한 중간 단계를 제거하고, 입력(관측)과 출력(행동)을 잇는 단일 함수 <span class="math math-inline">\pi_\theta(a_t \vert o_t)</span>를 학습한다. 여기서 <span class="math math-inline">o_t</span>는 이미지와 같은 고차원 관측값이며, <span class="math math-inline">a_t</span>는 관절의 토크나 속도와 같은 저차원 제어 명령이다.2 이 접근법의 핵심 철학은 로봇이 수행해야 할 작업(Task)의 성공 여부를 유일한 목적함수(Objective Function)로 삼아, 인지와 제어의 모든 과정을 동시에 최적화하는 것이다.</p>
<p>이 방식은 로봇이 “무엇을 보아야 하는지“를 인간이 미리 정의하지 않고, 로봇 스스로 데이터로부터 중요한 특징(Feature)을 추출하게 만든다. 초기 연구인 Levine 등의 “End-to-End Training of Deep Visuomotor Policies” (2016)는 이러한 접근이 단순히 가능한 수준을 넘어, 복잡한 비정형 환경에서 정교한 조작을 수행하는 데 있어 기존 방식을 압도할 수 있음을 증명했다.8</p>
<h2>2.  아키텍처의 진화 1: CNN과 공간적 추론의 기초</h2>
<p>End-to-End 제어의 역사는 딥러닝 아키텍처의 발전사와 궤를 같이한다. 초기의 도전 과제는 컴퓨터 비전 분야에서 성공을 거둔 합성곱 신경망(CNN)을 어떻게 로봇의 연속적인 제어 공간과 연결하느냐였다.</p>
<h3>2.1  시각적 특징 추출과 CNN의 역할</h3>
<p>이미지는 수십만, 수백만 개의 픽셀로 이루어진 고차원 데이터이다. 이를 그대로 제어기에 입력하는 것은 계산적으로 불가능할 뿐만 아니라, 차원의 저주(Curse of Dimensionality)로 인해 학습이 수렴하지 않는다. 따라서 CNN은 이미지에서 유의미한 특징을 추출하는 인코더(Encoder) 역할을 수행한다. 초기 연구들은 ImageNet 등으로 사전 학습된(Pre-trained) 가중치를 사용하거나, 오토인코더(Autoencoder) 구조를 통해 저차원 잠재 공간(Latent Space)을 학습하는 방식을 사용했다.2</p>
<p>그러나 일반적인 이미지 분류(Classification)용 CNN은 마지막 단계에서 공간 정보를 완전히 압축해버리는 풀링(Pooling)이나 완전 연결(Fully Connected) 계층을 사용한다. “고양이가 있다“는 사실만 중요할 뿐, “고양이가 어디에 있다“는 정밀한 좌표 정보는 소실되기 쉽다. 하지만 로봇 제어, 특히 조작 작업에서는 물체의 정확한 위치와 로봇 팔의 상대적 거리가 무엇보다 중요하다.</p>
<h3>2.2  공간 소프트맥스(Spatial Softmax): 픽셀을 좌표로 변환하다</h3>
<p>이 문제를 해결하기 위해 도입된 핵심 기술이 바로 **공간 소프트맥스(Spatial Softmax)**이다.8 Levine 등은 마지막 합성곱 레이어의 출력인 특징 맵(Feature Map)에 대해 채널별로 공간적인 확률 분포를 계산하고, 이를 통해 특징점(Feature Point)의 2D 이미지 좌표 <span class="math math-inline">(x, y)</span>를 추출하는 메커니즘을 제안했다.</p>
<p>공간 소프트맥스의 수학적 원리는 다음과 같다. 특징 맵 <span class="math math-inline">f_c \in \mathbb{R}^{H \times W}</span> (여기서 <span class="math math-inline">c</span>는 채널)에 대해, 각 픽셀 위치 <span class="math math-inline">(i, j)</span>의 값 <span class="math math-inline">a_{cij}</span>를 소프트맥스 함수를 통해 확률 <span class="math math-inline">s_{cij}</span>로 변환한다.<br />
<span class="math math-display">
s_{cij} = \frac{\exp(a_{cij})}{\sum_{i&#39;,j&#39;} \exp(a_{ci&#39;j&#39;})}
</span><br />
이후, 이 확률 분포를 가중치로 하여 이미지 좌표의 기댓값(Expectation)을 계산한다.<br />
<span class="math math-display">
x_c = \sum_{i,j} s_{cij} \cdot i, \quad y_c = \sum_{i,j} s_{cij} \cdot j
</span><br />
이 과정은 다음과 같은 결정적인 이점을 제공한다10:</p>
<ol>
<li><strong>공간 정보의 명시적 보존:</strong> 신경망이 임의의 추상적 벡터가 아닌, 이미지 상의 구체적인 위치 좌표를 출력하도록 강제한다. 이는 로봇이 물체의 위치를 ’추적(Tracking)’하는 내부 모델을 형성하도록 유도한다.</li>
<li><strong>파라미터 효율성(Parameter Efficiency):</strong> <span class="math math-inline">H \times W \times C</span> 크기의 텐서를 단순히 평탄화(Flatten)하여 완전 연결 계층에 연결하면 파라미터 수가 폭발적으로 증가한다. 반면, 공간 소프트맥스는 이를 <span class="math math-inline">C \times 2</span> 크기의 매우 작은 좌표 벡터로 변환하므로, 과적합(Overfitting)을 방지하고 학습 속도를 높인다.</li>
<li><strong>해석 가능성의 실마리:</strong> 추출된 특징점 <span class="math math-inline">(x_c, y_c)</span>를 원본 이미지에 시각화하면, 로봇이 현재 작업을 위해 물체의 어느 부위(예: 병뚜껑의 모서리, 레고 블록의 중심)에 집중하고 있는지 파악할 수 있다.</li>
</ol>
<p>이 메커니즘은 로봇이 별도의 카메라 캘리브레이션(Calibration) 없이도, 시각적 피드백 루프(Visual Servoing)를 내재적으로 학습할 수 있게 한 일등 공신이었다.9</p>
<h2>3.  아키텍처의 진화 2: 트랜스포머와 시계열 행동의 모델링</h2>
<p>CNN 기반의 정책이 ’현재의 이미지’를 보고 ’현재의 토크’를 결정하는 반응형(Reactive) 제어에 가까웠다면, 복잡한 작업은 과거의 기억과 미래의 계획을 필요로 한다. 또한, 인간의 데모 데이터는 본질적으로 다중 모드(Multimodal) 특성을 띠며, 이는 단순한 회귀(Regression) 모델로는 학습하기 어렵다.</p>
<h3>3.1  행동 청킹(Action Chunking)과 ACT</h3>
<p>전통적인 모방 학습(Imitation Learning)인 행동 복제(Behavioral Cloning, BC)는 매 타임스텝마다 단 하나의 행동을 예측한다. 그러나 이는 두 가지 문제를 야기한다. 첫째, **오차 누적(Compounding Error)**이다. 작은 오차가 발생해 로봇이 학습된 경로를 살짝 벗어나면, 가보지 않은 상태(Out-of-Distribution)에 빠져 회복하지 못하고 표류하게 된다.13 둘째, **반응 지연(Latency)**이다. 이미지를 처리하고 행동을 계산하는 동안 로봇은 멈칫거리거나 부드럽지 못한 움직임을 보일 수 있다.</p>
<p>이를 해결하기 위해 등장한 것이 **ACT (Action Chunking with Transformers)**이다.14 ACT는 트랜스포머(Transformer) 아키텍처를 활용하여, 단일 행동이 아닌 미래 <span class="math math-inline">k</span> 타임스텝 동안의 행동 시퀀스(Action Chunk)를 한 번에 예측한다.</p>
<h4>3.1.1  CVAE를 통한 스타일 학습</h4>
<p>ACT는 단순한 트랜스포머가 아니다. 인간의 데이터는 같은 작업을 수행하더라도 매번 속도나 궤적이 미묘하게 다르다. ACT는 CVAE(Conditional Variational Autoencoder) 구조를 도입하여 이러한 ‘스타일’ 정보를 잠재 변수(Latent Variable) <span class="math math-inline">z</span>로 압축한다.16 인코더는 데모 데이터로부터 <span class="math math-inline">z</span>를 학습하고, 트랜스포머 디코더는 현재의 관측 <span class="math math-inline">o_t</span>와 스타일 변수 <span class="math math-inline">z</span>를 결합하여 행동 시퀀스를 생성한다. 이는 로봇이 단순히 평균적인 움직임을 흉내 내는 것이 아니라, 일관성 있는 하나의 스타일로 움직이게 한다.</p>
<h4>3.1.2  시간적 앙상블(Temporal Ensembling)</h4>
<p>ACT의 가장 강력한 기능 중 하나는 시간적 앙상블이다.13 매 타임스텝마다 로봇은 <span class="math math-inline">k</span> 길이의 행동 청크를 예측한다. 즉, 특정 시점 <span class="math math-inline">t</span>에 대해서는 <span class="math math-inline">t</span> 시점에 예측된 값, <span class="math math-inline">t-1</span> 시점에 예측된 값,…, <span class="math math-inline">t-k+1</span> 시점에 예측된 값들이 중첩되어 존재하게 된다. ACT는 이 중첩된 예측값들을 지수 이동 평균(Exponential Moving Average) 등으로 부드럽게 결합하여 실행한다.<br />
<span class="math math-display">
a_t = \sum_{i} w_i \cdot \hat{a}_{t}^{i}
</span><br />
이 방식은 급격한 움직임(Jitter)을 억제하고, 모터 토크 제어에서 발생할 수 있는 고주파 진동을 소프트웨어적으로 필터링하는 효과를 낳아 매우 부드럽고 자연스러운 동작을 생성한다.</p>
<h3>3.2  트랜스포머 기반의 범용 모델: RT-1과 RT-2</h3>
<p>구글 딥마인드의 **RT-1 (Robotics Transformer 1)**과 <strong>RT-2</strong>는 로봇 제어를 언어 모델링 문제로 재정의했다.18</p>
<ul>
<li><strong>RT-1:</strong> 이미지를 효율적으로 처리하기 위해 EfficientNet을 백본으로 사용하고, 트랜스포머를 통해 토큰화된 행동을 출력한다. 행동 공간을 이산화(Discretize)하여 256개의 bin으로 나누고, 이를 자연어 토큰처럼 취급하여 다음 토큰을 예측하는 방식으로 학습했다.20</li>
<li><strong>RT-2 (Vision-Language-Action, VLA):</strong> RT-2는 거대 언어 모델(LLM)의 지식을 로봇에 이식한 획기적인 모델이다. PaLM-E나 PaLI-X와 같은 대규모 VLM을 기반으로, 로봇의 행동 토큰을 언어 토큰과 함께 학습시켰다. 이를 통해 RT-2는 명시적인 훈련 없이도 “쓰레기를 치워라“와 같은 추상적인 명령을 이해하고, 시각적 추론을 통해 “무엇이 쓰레기인지” 판단하여 행동으로 옮길 수 있는 창발적(Emergent) 능력을 보여주었다.19</li>
</ul>
<h2>4.  아키텍처의 진화 3: 생성형 모델과 분포의 학습</h2>
<p>End-to-End 학습에서 가장 까다로운 문제는 **멀티모달 행동 분포(Multimodal Action Distribution)**이다. 예를 들어, 로봇 앞에 기둥이 있을 때 왼쪽으로 피해 가는 것과 오른쪽으로 피해 가는 것은 모두 정답이다. 그러나 전통적인 회귀(Regression) 모델이 사용하는 평균 제곱 오차(MSE) 손실 함수는 이 두 정답의 평균, 즉 기둥을 향해 정면으로 돌진하는 경로를 학습하게 만든다. 이는 치명적인 충돌을 야기한다.22</p>
<h3>4.1  디퓨전 정책(Diffusion Policy): 노이즈로부터 행동을 조각하다</h3>
<p><strong>Diffusion Policy</strong>는 이미지 생성 분야를 평정한 디퓨전 모델(Denoising Diffusion Probabilistic Models)을 로봇 제어에 도입하여 이 문제를 근본적으로 해결했다.23</p>
<ul>
<li><strong>조건부 노이즈 제거(Conditional Denoising):</strong> 정책 <span class="math math-inline">\pi_\theta</span>는 행동을 직접 출력하는 대신, 가우시안 노이즈(Gaussian Noise)로 시작하여 반복적인 정제 과정(Refinement Process)을 거쳐 유효한 행동을 생성한다. 이때 현재의 관측 <span class="math math-inline">o_t</span>가 조건(Condition)으로 작용하여 노이즈 제거 방향을 가이드한다.</li>
<li><strong>스코어 함수(Score Function) 학습:</strong> 디퓨전 모델은 행동 분포의 그라디언트(Gradient), 즉 스코어 함수를 학습한다. 이는 로봇에게 “어떤 행동이 정답인가?“를 묻는 것이 아니라, “현재 상태에서 행동의 확률 밀도가 높아지는 방향은 어디인가?“를 묻는 것이다.22 덕분에 로봇은 여러 가지 유효한 해결책(Mode) 중 하나로 자연스럽게 수렴할 수 있으며, 불가능한 평균값으로 수렴하는 문제를 피할 수 있다.</li>
<li><strong>Receding Horizon Control:</strong> 디퓨전 정책 역시 ACT와 마찬가지로 행동 시퀀스를 예측한다. 이는 동적인 환경 변화에 강인하며, 일관성 있는 장기 계획을 가능하게 한다.25</li>
</ul>
<p>Cheng Chi 등이 제안한 Diffusion Policy는 CNN(ResNet)과 트랜스포머 백본을 모두 지원하며, DDIM(Denoising Diffusion Implicit Models)과 같은 샘플링 가속화 기술을 통해 100Hz 이상의 실시간 제어 주기를 달성했다. 이는 생성형 AI가 느리다는 편견을 깨고, 고속 로봇 제어에 성공적으로 적용된 사례이다.25</p>
<h3>4.2  플로우 매칭(Flow Matching)과 <span class="math math-inline">\pi_0</span>: 차세대 속도 혁명</h3>
<p>디퓨전 모델은 강력하지만, 노이즈를 제거하기 위해 수십 번의 반복 연산(Iteration)이 필요하다는 단점이 있다. 이는 반응 속도가 중요한 로봇 제어에서 병목이 될 수 있다. 이를 극복하기 위해 2024년 말부터 <strong>플로우 매칭(Flow Matching)</strong> 기술이 급부상하고 있다.26</p>
<p>Physical Intelligence 사가 발표한 <strong><span class="math math-inline">\pi_0</span> (Pi-Zero)</strong> 모델은 VLM 백본에 플로우 매칭 헤드를 결합한 구조이다.28</p>
<ul>
<li><strong>직선 경로의 학습:</strong> 디퓨전이 노이즈에서 데이터로 가는 구불구불한 확률적 경로를 역추적한다면, 플로우 매칭은 노이즈 분포와 데이터 분포 사이를 잇는 최적의 직선 경로(Vector Field)를 직접 학습한다. 이는 훨씬 적은 단계(Step)만으로도 고품질의 샘플을 생성할 수 있게 한다.29</li>
<li><strong>VLM과 연속 제어의 통합:</strong> <span class="math math-inline">\pi_0</span>는 인터넷 규모의 데이터로 사전 학습된 VLM의 강력한 의미론적 이해 능력과, 플로우 매칭의 정교한 연속 제어 능력을 하나의 모델로 통합했다. 텍스트 토큰과 연속적인 행동 값을 동시에 처리하는 이 아키텍처는 로봇이 복잡한 언어 명령을 이해하고, 최대 50Hz의 고속으로 정밀한 조작을 수행할 수 있게 한다.31 이는 2026년 현재 비주모터 제어의 가장 최신 SOTA(State-of-the-Art) 기술로 평가받는다.</li>
</ul>
<h2>5.  데이터 혁명: UMI와 범용 로봇 정책의 실현</h2>
<p>아무리 뛰어난 아키텍처라도 데이터가 없으면 무용지물이다. End-to-End 학습, 특히 트랜스포머와 디퓨전 기반의 대형 모델은 막대한 양의 양질의 데이터를 요구한다.</p>
<h3>5.1  UMI (Universal Manipulation Interface): 데이터의 민주화</h3>
<p>기존의 데이터 수집 방식인 원격 조종(Teleoperation)은 고가의 장비와 숙련된 조작자를 필요로 했다. 스탠포드 대학 등에서 개발한 <strong>UMI</strong>는 이러한 진입 장벽을 무너뜨렸다.33</p>
<ul>
<li><strong>하드웨어:</strong> GoPro 카메라와 3D 프린팅된 그리퍼만으로 구성된 휴대용 장치를 사람이 들고 직접 조작 작업을 수행한다.</li>
<li><strong>상대 좌표계와 SLAM:</strong> UMI의 핵심은 절대 좌표가 아닌, 손목 카메라 기준의 상대적 변위(Delta Pose)를 학습하는 것이다. SLAM 기술을 이용해 그리퍼의 궤적을 정밀하게 복원하고, 어안 렌즈를 통해 넓은 시야를 확보한다.</li>
<li><strong>In-the-Wild 데이터 수집:</strong> 실험실이 아닌 실제 가정, 사무실, 야외 등 다양한 환경에서 저렴하게 데이터를 수집할 수 있게 되었다. 이는 로봇 학습 데이터의 양과 다양성을 폭발적으로 증가시키는 기폭제가 되었다.35</li>
</ul>
<h3>5.2  Octo와 Open X-Embodiment</h3>
<p>이러한 데이터 수집의 혁신은 다양한 로봇과 작업 데이터를 하나로 모으는 <strong>Open X-Embodiment</strong> 프로젝트로 이어졌고, 이를 바탕으로 <strong>Octo</strong>와 같은 범용 로봇 정책(Generalist Robot Policy)이 탄생했다.36 Octo는 특정 로봇 하드웨어에 종속되지 않는 유연한 입력/출력 구조를 가지며, 새로운 로봇에 대해서도 적은 양의 데이터로 미세 조정(Fine-tuning)하여 사용할 수 있는 ’로봇을 위한 파운데이션 모델’을 지향한다.38</p>
<h2>6.  End-to-End 접근의 본질적 장단점 및 비교 분석</h2>
<p>End-to-End 비주모터 제어는 분명 매력적이지만, 모듈형 파이프라인을 완전히 대체하기에는 여전히 해결해야 할 과제들이 존재한다.</p>
<h3>6.1  장점 (Pros)</h3>
<ol>
<li><strong>극한의 성능 최적화:</strong> 인지부터 제어까지 모든 파라미터가 작업 성공률이라는 하나의 목표를 위해 공동 최적화(Joint Optimization)된다. 이는 인간이 설계한 특징 추출기가 놓칠 수 있는 미세한 정보까지 활용하게 하여, 숙련된 인간 수준의 조작 능력을 가능케 한다.9</li>
<li><strong>시스템의 단순성과 유연성:</strong> 복잡한 수식 모델링이나 캘리브레이션 과정이 생략된다. 데이터만 있다면 어떤 새로운 작업이나 환경에도 적응할 수 있는 유연성을 제공한다.6</li>
<li><strong>일반화 능력:</strong> 대규모 데이터와 결합된 VLA 모델은 학습하지 않은 물체나 상황에 대해서도 상식적인 추론을 통해 대처하는 능력을 보여준다.</li>
</ol>
<h3>6.2  단점 (Cons) 및 과제</h3>
<ol>
<li><strong>해석 불가능성(Black Box):</strong> 신경망 내부에서 어떤 근거로 특정 행동이 결정되었는지 파악하기 어렵다. 이는 사고 발생 시 원인 규명(Root Cause Analysis)을 불가능하게 만들어, 산업 현장 도입을 가로막는 주된 요인이다.39</li>
<li><strong>안전성(Safety) 보장의 부재:</strong> 모듈형 방식은 “장애물 10cm 이내 접근 금지“와 같은 명시적 제약(Hard Constraint)을 걸 수 있지만, End-to-End 모델은 이를 확률적으로만 학습한다. 학습 데이터 분포를 벗어난 상황(OOD)에서는 예측 불가능하고 위험한 행동을 할 수 있다.41</li>
<li><strong>데이터 의존성:</strong> 높은 성능을 위해서는 막대한 데이터가 필요하며, 데이터의 품질이 곧 성능을 결정한다. 잘못된 데이터(예: 위험한 행동이 포함된 데모)는 치명적인 결과를 초래할 수 있다.</li>
</ol>
<h3>6.3  비교 요약</h3>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>모듈형 파이프라인 (Modular)</strong></th><th><strong>End-to-End 비주모터 제어</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 구조</strong></td><td>인지 <span class="math math-inline">\to</span> 계획 <span class="math math-inline">\to</span> 제어 (직렬)</td><td>관측 <span class="math math-inline">\to</span> 신경망 <span class="math math-inline">\to</span> 행동 (통합)</td></tr>
<tr><td><strong>정보 처리</strong></td><td>모듈 간 정보 손실 발생 (병목)</td><td>원시 정보의 보존 및 추출</td></tr>
<tr><td><strong>최적화</strong></td><td>각 모듈 개별 최적화</td><td>전체 시스템 전역 최적화 (Global Opt.)</td></tr>
<tr><td><strong>해석 가능성</strong></td><td>높음 (각 단계 검증 가능)</td><td>낮음 (내부 동작 불투명)</td></tr>
<tr><td><strong>강인성 (OOD)</strong></td><td>규칙 기반 안전장치 용이</td><td>데이터 분포 의존적, 취약함</td></tr>
<tr><td><strong>적용 분야</strong></td><td>정형화된 산업 자동화, 자율 주행(일부)</td><td>비정형 조작, 가사 로봇, 다목적 로봇</td></tr>
</tbody></table>
<h2>7.  결론: 하이브리드와 안전한 미래를 향하여</h2>
<p>“픽셀에서 토크까지(Pixels to Torques)“라는 비전은 로봇 공학의 오랜 꿈인 모라벡의 역설(Moravec’s Paradox)을 극복할 가장 강력한 도구로 자리 잡았다. 초기 CNN과 공간 소프트맥스를 통한 단순한 시각-운동 협응에서 시작하여, 트랜스포머를 통한 시퀀스 모델링, 디퓨전과 플로우 매칭을 통한 멀티모달 생성 모델링에 이르기까지, 기술은 숨 가쁘게 진화해 왔다.</p>
<p>이제 학계와 산업계의 관심은 단순히 성능을 높이는 것을 넘어, 이 강력한 도구를 어떻게 ‘안전하고 신뢰할 수 있게’ 만드느냐로 옮겨가고 있다. <strong>SafeVLA</strong>와 같이 제약된 강화학습(Constrained RL)을 통해 안전성을 내재화하려는 시도41, 그리고 **<span class="math math-inline">\pi_0</span>**와 같이 언어 모델의 추론 능력과 제어 정책의 정밀함을 통합하려는 시도28는 그 방향성을 명확히 보여준다.</p>
<p>미래의 로봇 제어 시스템은 순수한 End-to-End와 고전적 제어 이론이 결합된 하이브리드 형태가 될 가능성이 높다. 고수준의 인지와 계획은 거대 신경망이 담당하되, 최하위 레벨의 안전과 안정성은 검증된 제어 이론이 담당하는 형태이다. 이러한 융합은 로봇이 실험실을 벗어나 우리의 일상 속에서 안전하게 공존하며 물리적 지능(Physical Intelligence)을 발휘하는 시대를 앞당길 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>SE3-Pose-Nets: Structured Deep Dynamics Models for Visuomotor Planning and Control | Request PDF - ResearchGate, https://www.researchgate.net/publication/320179883_SE3-Pose-Nets_Structured_Deep_Dynamics_Models_for_Visuomotor_Planning_and_Control</li>
<li>End-to-End Training of Deep Visuomotor Policies - ResearchGate, https://www.researchgate.net/publication/274572264_End-to-End_Training_of_Deep_Visuomotor_Policies</li>
<li>Autonomous driving: Modular pipeline Vs. End-to-end and LLMs | by Samer Attrah | Medium, https://medium.com/@samiratra95/autonomous-driving-modular-pipeline-vs-end-to-end-and-llms-642ca7f4ef89</li>
<li>End-to-End Learning for a Low-Cost Robotics Arm - Scholarship Repository @ Florida Tech, https://repository.fit.edu/cgi/viewcontent.cgi?article=2546&amp;context=etd</li>
<li>Bridging the Gap Between Modular and End-to-end Autonomous Driving Systems - UC Berkeley EECS, https://www2.eecs.berkeley.edu/Pubs/TechRpts/2022/EECS-2022-79.pdf</li>
<li>This article will help you clarify the differences in end-to-end architecture of autonomous driving - EEWORLD, https://en.eeworld.com.cn/news/qcdz/eic695754.html</li>
<li>End-to-End Training of Deep Visuomotor Policies - UC Berkeley Robot Learning Lab, https://rll.berkeley.edu/RSS2015-BlueSky-Shakey/Levine-ShakeyWS-2015.pdf</li>
<li>End-to-End Training of Deep Visuomotor Policies - IAS TU Darmstadt, https://www.ias.informatik.tu-darmstadt.de/uploads/Workshops/RSS2015VisualTactileInteraction/Levine-RSS-vtl-2015.pdf</li>
<li>End-to-End Training of Deep Visuomotor Policies - Journal of Machine Learning Research, https://www.jmlr.org/papers/volume17/15-522/15-522.pdf</li>
<li>Deep Spatial Autoencoders for Visuomotor Learning, https://rll.berkeley.edu/dsae/dsae.pdf</li>
<li>little_experiments/diffusion_spatial_softmax.md at main - GitHub, https://github.com/alexander-soare/little_experiments/blob/main/diffusion_spatial_softmax.md</li>
<li>Pre-Training for Robots: Offline RL Enables Learning New Tasks in a Handful of Trials, https://www.roboticsproceedings.org/rss19/p019.pdf</li>
<li>Action Chunking with Transformer (ACT) - Emergent Mind, https://www.emergentmind.com/topics/action-chunking-with-transformer-act</li>
<li>Enhancing Diffusion Policy with Classifier-Free Guidance for Temporal Robotic Tasks - arXiv, https://arxiv.org/html/2510.09786v1</li>
<li>Action Chunking with Transformers - ACT — Open Edge Platform Documentation - Intel, https://docs.openedgeplatform.intel.com/dev/edge-ai-suites/robotics-ai-suite/embodied/developer_tools_tutorials/model_tutorials/model_act.html</li>
<li>Dissecting Action Chunking with Transformers (ACT): Precision Imitation Learning for Robotic Manipulation - phospho, https://blog.phospho.ai/dissecting-action-chunking-with-transformers-act-precision-imitation-learning-for-robotic-manipulation/</li>
<li>Action chunking with Transformers (ACT) robot policy | by Deepthi Karkada - Medium, https://medium.com/@deepkarkada/action-chunking-with-transformers-act-robot-policy-80519fc024bc</li>
<li>Open X-Embodiment: Robotic Learning Datasets and RT-X Models, https://robotics-transformer-x.github.io/</li>
<li>RT-2: New model translates vision and language into action - Google DeepMind, https://deepmind.google/blog/rt-2-new-model-translates-vision-and-language-into-action/</li>
<li>Open X-Embodiment: Robotic Learning Datasets and RT-X Models - arXiv, https://arxiv.org/html/2310.08864v5</li>
<li>[Robotics] RT-2: Vision-Language-Action Models | by Ming-Hao Hsu | Medium, https://medium.com/@amiable_cardinal_crocodile_398/robotics-rt-2-vision-language-action-models-8db0c197d02</li>
<li>Diffusion Policy: How Diffusion Models Are Transforming Robot Learning from Demonstration | by Isaac Kargar, https://kargarisaac.medium.com/diffusion-policy-how-diffusion-models-are-transforming-robot-learning-from-demonstration-32c27ba829cf</li>
<li>Visuomotor Policy Learning via Action Diffusion, https://diffusion-policy.cs.columbia.edu/diffusion_policy_ijrr.pdf</li>
<li>Diffusion Policy, https://diffusion-policy.cs.columbia.edu/</li>
<li>Visuomotor Policy Learning via Action Diffusion - arXiv, https://arxiv.org/html/2303.04137v5</li>
<li>Flow Matching Policy Gradient - Pangram Labs, https://www.pangram.com/history/1c40bb03-58bd-4802-b10e-b87df8f95d92</li>
<li><span class="math math-inline">π_0</span>: A Vision-Language-Action Flow Model for General Robot Control - arXiv, https://arxiv.org/abs/2410.24164</li>
<li>𝜋₀: A Vision-Language-Action Flow Model for General Robot Control - arXiv, https://arxiv.org/html/2410.24164v1</li>
<li>ReinFlow: Fine-tuning Flow Matching Policy with Online Reinforcement Learning - NeurIPS 2025, https://neurips.cc/virtual/2025/poster/119473</li>
<li>Streaming Flow Policy Simplifying diffusion/flow-matching policies by treating action trajectories as flow trajectories Website: https://streaming-flow-policy.github.io - arXiv, https://arxiv.org/html/2505.21851v2</li>
<li>[Paper Review] Pi0, Pi0.5, Pi0-FAST - Tracing the Path of Physical Intelligence (PI), https://bequiet-log.vercel.app/pi-review</li>
<li>π0: A Vision-Language-Action Flow Model for General Robot Control - Physical Intelligence, https://www.physicalintelligence.company/download/pi0.pdf</li>
<li>Universal Manipulation Interface (UMI) - Emergent Mind, https://www.emergentmind.com/topics/universal-manipulation-interface-umi</li>
<li>[Literature Review] Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots - Moonlight, https://www.themoonlight.io/en/review/universal-manipulation-interface-in-the-wild-robot-teaching-without-in-the-wild-robots</li>
<li>Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots - Robotics, https://roboticsproceedings.org/rss20/p045.pdf</li>
<li>An Open-Source Generalist Robot Policy - Octo, https://octo-models.github.io/paper.pdf</li>
<li>Octo: An Open-Source Generalist Robot Policy - Robotics: Science and Systems, https://roboticsconference.org/2024/program/papers/90/</li>
<li>Octo: An Open-Source Generalist Robot Policy - arXiv, https://arxiv.org/html/2405.12213v2</li>
<li>Measuring Interpretability of Neural Policies of Robots with Disentangled Representation - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v229/wang23c/wang23c.pdf</li>
<li>[2407.08065] Towards Interpretable Foundation Models of Robot Behavior: A Task Specific Policy Generation Approach - arXiv, https://arxiv.org/abs/2407.08065</li>
<li>SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning - arXiv, https://arxiv.org/html/2503.03480v1</li>
<li>[Literature Review] SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning - Moonlight, https://www.themoonlight.io/en/review/safevla-towards-safety-alignment-of-vision-language-action-model-via-safe-reinforcement-learning</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>