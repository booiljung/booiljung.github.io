<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.1 로봇 제어 아키텍처의 스펙트럼 (The Spectrum of Architectures)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.1 로봇 제어 아키텍처의 스펙트럼 (The Spectrum of Architectures)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 3. 로봇을 위한 SOTA 기술 지형도</a> / <a href="index.html">3.1 로봇 제어 아키텍처의 스펙트럼 (The Spectrum of Architectures)</a> / <span>3.1 로봇 제어 아키텍처의 스펙트럼 (The Spectrum of Architectures)</span></nav>
                </div>
            </header>
            <article>
                <h1>3.1 로봇 제어 아키텍처의 스펙트럼 (The Spectrum of Architectures)</h1>
<h2>1.  서론: 제어 패러다임의 대전환과 스펙트럼의 확장</h2>
<p>로봇 공학의 역사에서 제어 아키텍처(Control Architecture)의 설계는 단순한 공학적 선택을 넘어, 인공지능이 물리적 세계와 상호작용하는 방식을 규정하는 철학적 기조를 대변해 왔다. 2024년과 2025년을 관통하며 로봇 제어 기술은 전례 없는 변곡점을 맞이하고 있다. 과거에는 명확히 정의된 입출력 인터페이스를 가진 모듈들의 집합인 ’모듈형 아키텍처(Modular Architecture)’와, 입력 센서 데이터로부터 제어 신호까지를 하나의 거대한 함수로 최적화하려는 ’엔드투엔드(End-to-End) 학습’이 양극단의 대립 구도를 형성했다면, 현재의 지형도는 이 두 극단 사이에 존재하는 방대한 ’스펙트럼(Spectrum)’으로 확장되었다.1 이 스펙트럼은 고전적인 제어 이론의 정밀함과 현대적인 딥러닝의 일반화 능력을 융합하려는 다양한 시도들로 채워지고 있으며, 각 접근법은 해석 가능성(Interpretability), 샘플 효율성(Sample Efficiency), 일반화(Generalization), 그리고 안전성(Safety)이라는 핵심 가치들 사이에서 각기 다른 균형점을 제시한다.3</p>
<p>특히 생성형 AI(Generative AI)와 대규모 언어 모델(LLM), 그리고 비전-언어-행동 모델(VLA)의 비약적인 발전은 이 스펙트럼의 중심축을 흔들고 있다. 과거에는 상상하기 힘들었던 ’상식적 추론(Common Sense Reasoning)’이 가능한 로봇 제어 시스템이 등장하면서, 로봇은 단순한 반복 작업 기계에서 벗어나 비정형 환경(Unstructured Environment)에서 인간의 모호한 명령을 이해하고 수행하는 지능형 에이전트로 진화하고 있다.5 본 장에서는 이러한 기술적 맥락을 바탕으로, 고전적 파이프라인부터 최신 파운데이션 모델 기반 아키텍처에 이르기까지 로봇 제어 시스템의 스펙트럼을 심층적으로 해부하고, 2025년 현재 연구와 산업 현장에서 가장 유효한 설계 전략이 무엇인지 탐구한다.</p>
<h2>2.  고전적 모듈형 아키텍처: 명시적 분해와 구조적 안정성</h2>
<p>스펙트럼의 가장 전통적인 끝에는 **모듈형 아키텍처(Modular Architecture)**가 위치한다. 이는 복잡한 로봇 제어 문제를 인지(Perception), 계획(Planning), 제어(Control)라는 독립적이고 명확한 하위 문제로 분해(Decomposition)하여 해결하려는 접근법이다. 이 방식은 수십 년간 로봇 공학의 표준으로 자리 잡아 왔으며, 특히 안전과 검증이 중요한 자율 주행 및 산업 자동화 분야에서 여전히 강력한 지배력을 행사하고 있다.2</p>
<h3>2.1  감지-계획-행동 (Sense-Plan-Act) 사이클의 심층 분석</h3>
<p>모듈형 시스템의 핵심은 <strong>감지-계획-행동(Sense-Plan-Act, SPA)</strong> 사이클이다. 이 구조에서 정보는 일방향 또는 제한적인 피드백 루프를 통해 흐르며, 각 단계는 고유한 입력과 출력을 가지는 블랙박스 또는 화이트박스 모듈로 기능한다.</p>
<ol>
<li>인지(Perception) 모듈: 세계의 구조화</li>
</ol>
<p>인지 모듈의 주된 임무는 고차원의, 노이즈가 섞인 센서 데이터(RGB 이미지, LiDAR 포인트 클라우드, IMU 데이터 등)를 저차원의 의미 있는 상태(State) 정보로 압축하는 것이다.1</p>
<ul>
<li><strong>상태 추정(State Estimation):</strong> 로봇의 위치(Localization)와 주변 환경의 지도(Mapping)를 작성하는 SLAM(Simultaneous Localization and Mapping) 기술이 핵심이다. 최근에는 딥러닝 기반의 시각적 오도메트리(Visual Odometry)와 루프 결합(Loop Closure) 기술이 통합되어 정밀도를 높이고 있다.9</li>
<li><strong>객체 인식 및 추적:</strong> 2024년 이후의 인지 모듈은 단순한 바운딩 박스(Bounding Box) 검출을 넘어, 객체의 3D 포즈, 의미론적 클래스, 그리고 이동 궤적을 실시간으로 추적한다.2 특히 이벤트 기반 카메라(Event-based Camera)의 도입은 고속 주행이나 급격한 조명 변화 환경에서도 마이크로초 단위의 시간 해상도로 강건한 특징 추출을 가능하게 하여, 자율 주행 차량의 인지 성능을 비약적으로 향상시켰다.1</li>
<li><strong>정보의 손실:</strong> 그러나 모듈형 인지의 근본적인 한계는 ‘정보 병목(Information Bottleneck)’ 현상이다. 원본 센서 데이터에 포함된 미세한 텍스처 정보나 비정형적인 환경 단서들이, 사전에 정의된 ‘상태(State)’ 포맷(예: 객체 좌표 리스트)으로 변환되는 과정에서 필연적으로 소실된다.2</li>
</ul>
<ol start="2">
<li>계획(Planning) 모듈: 의사결정의 수학적 최적화</li>
</ol>
<p>인지 모듈이 제공한 ’세계 모델(World Model)’을 바탕으로, 로봇은 목표를 달성하기 위한 최적의 행동 시퀀스를 생성한다.</p>
<ul>
<li><strong>TAMP (Task and Motion Planning):</strong> 현대 로봇 계획의 정점은 이산적인 논리적 계획(Task Planning)과 연속적인 기하학적 계획(Motion Planning)을 결합한 TAMP이다.8 예를 들어 “커피를 타라“는 명령은 “컵으로 이동 -&gt; 컵 집기 -&gt; 정수기로 이동 -&gt; 물 따르기“라는 작업 시퀀스로 분해되고, 각 단계는 다시 충돌 없는 관절 궤적(Trajectory)으로 변환된다. 2024-2025년의 연구들은 불완전한 정보 하에서도 강건하게 동작하는 TAMP 알고리즘과, 계획 단계에서 불가능하다고 판단된 행동을 실시간으로 수정하는 재계획(Replanning) 전략에 집중하고 있다.11</li>
<li><strong>최적화 기반 계획:</strong> 모델 예측 제어(MPC)와 같이 비용 함수(Cost Function)를 최소화하는 궤적을 실시간으로 계산하는 방식이 주류를 이룬다. 이는 로봇의 동역학적 제약과 환경의 장애물을 수학적 제약 조건(Constraints)으로 명시할 수 있어 안전성을 보장하는 데 유리하다.3</li>
</ul>
<ol start="3">
<li>제어(Control) 모듈: 물리적 실행</li>
</ol>
<p>계획된 궤적을 실제 액추에이터가 이해할 수 있는 전압이나 토크 신호로 변환한다. PID 제어기부터 적응형 제어(Adaptive Control), 임피던스 제어(Impedance Control) 등 고전 제어 이론이 가장 활발하게 적용되는 영역이다. 이 단계에서는 수 밀리초(ms) 단위의 실시간성(Real-time) 보장이 필수적이다.2</p>
<h3>2.2  모듈형 아키텍처의 강점과 한계: 트레이드오프 분석</h3>
<p><strong>강점 (Strengths):</strong></p>
<ul>
<li><strong>해석 가능성(Interpretability) 및 디버깅:</strong> 사고 발생 시 원인이 인지 오류인지, 계획 실패인지, 제어 불안정인지 명확히 파악할 수 있다. 이는 시스템 유지보수와 책임 소재 규명에 결정적이다.2</li>
<li><strong>검증 가능성(Verifiability) 및 안전성:</strong> 각 모듈의 입출력이 명시적이므로, 수학적 증명이나 시뮬레이션 기반 스트레스 테스트를 통해 안전성을 검증하기 용이하다. 산업 표준(ISO 26262 등)을 준수해야 하는 시스템에서 필수적이다.4</li>
<li><strong>모듈 재사용성:</strong> 개발된 내비게이션 모듈을 다른 로봇 플랫폼에 이식하거나, 더 좋은 센서로 교체할 때 전체 시스템을 재학습할 필요 없이 해당 모듈만 교체하면 된다.16</li>
</ul>
<p><strong>한계 (Limitations):</strong></p>
<ul>
<li><strong>오차 전파(Compounding Errors):</strong> 상위 모듈(인지)의 작은 오차가 하위 모듈로 갈수록 증폭되어 시스템 전체의 실패를 야기한다. 예를 들어, 인지 모듈이 장애물 위치를 1cm 잘못 추정하면, 계획 모듈은 엉뚱한 경로를 생성하고, 제어 모듈은 충돌을 일으킬 수 있다.9</li>
<li><strong>수동 튜닝의 복잡성:</strong> 각 모듈 간의 인터페이스를 정의하고 매개변수를 튜닝하는 데 막대한 엔지니어링 노력이 든다. 이는 시스템의 확장성을 저해한다.7</li>
<li><strong>비정형 환경 대응력 부족:</strong> 사전에 정의된 상태 표현법으로는 숲속이나 재난 현장 같은 비정형 환경의 모든 변수를 담아낼 수 없다.12</li>
</ul>
<h2>3.  엔드투엔드 학습: 픽셀에서 토크까지의 혁명 (From Pixels to Torques)</h2>
<p>스펙트럼의 반대편에는 <strong>엔드투엔드(End-to-End, E2E) 학습</strong>이 자리 잡고 있다. “픽셀에서 토크까지(From Pixels to Torques)“라는 슬로건으로 대표되는 이 접근법은 중간 단계의 명시적 표현(Intermediate Representations)을 제거하고, 센서 입력에서 제어 출력까지의 매핑을 심층 신경망(Deep Neural Networks)을 통해 직접 학습한다.18</p>
<h3>3.1  엔드투엔드 학습의 두 가지 기둥: 모방과 강화</h3>
<p>E2E 아키텍처는 주로 데이터 주도적인 학습 방법론에 의존하며, 크게 모방 학습(Imitation Learning)과 강화 학습(Reinforcement Learning)으로 나뉜다.</p>
<h4>3.1.1  모방 학습 (Imitation Learning): 전문가의 행동 복제</h4>
<p>모방 학습은 로봇이 인간 전문가의 시연(Demonstration) 데이터를 보고 이를 그대로 흉내 내도록 학습하는 방식이다. 가장 기본적인 형태인 행동 복제(Behavior Cloning, BC)는 상태-행동 쌍(State-Action Pairs)을 지도 학습(Supervised Learning)으로 학습한다.</p>
<ul>
<li><strong>ALOHA 및 Mobile ALOHA의 혁신:</strong> 2024년 스탠포드 대학 연구진이 공개한 Mobile ALOHA 프로젝트는 E2E 모방 학습의 가능성을 극적으로 보여주었다.21 연구진은 저렴한 텔레오퍼레이션(Teleoperation) 장비를 통해 양질의 시연 데이터를 수집하고, 이를 트랜스포머 기반의 정책 네트워크로 학습시켰다. 이 시스템은 RGB 이미지와 로봇 팔의 관절 위치(Joint Positions)를 입력받아, 14 자유도(DoF)의 관절 위치를 직접 출력한다.23 놀랍게도 단 50회의 시연만으로도 요리, 청소, 엘리베이터 탑승과 같은 복잡하고 장기적인 작업을 90% 이상의 성공률로 수행해냈다.24 이는 데이터의 질과 학습 알고리즘의 발전이 하드웨어의 한계를 극복할 수 있음을 시사한다.</li>
</ul>
<h4>3.1.2  강화 학습 (Reinforcement Learning): 시행착오를 통한 최적화</h4>
<p>강화 학습은 로봇이 환경과 상호작용하며 보상(Reward)을 최대화하는 방향으로 정책을 스스로 수정해 나가는 방식이다.</p>
<ul>
<li><strong>심층 강화 학습(Deep RL)의 진화:</strong> 초기에는 단순한 게임 환경에서 주로 사용되었으나, 2024-2025년에는 4족 보행 로봇(Quadruped)의 동적 보행이나 로봇 팔의 조작 작업에 널리 적용되고 있다.25</li>
<li><strong>직접 토크 제어(Direct Torque Control):</strong> 최근 연구들은 위치 제어(Position Control)를 넘어, 관절의 토크(Torque)를 직접 제어하는 정책을 학습하는 데 집중하고 있다.14 토크 제어는 로봇의 유연성(Compliance)을 높이고 에너지 효율적인 동작을 가능하게 하지만, 학습 난이도가 매우 높다. 이를 해결하기 위해 전문가의 시연 데이터를 잠재 행동(Latent Action)으로 압축하여 강화 학습의 탐색 효율을 높이는 기술들이 제안되고 있다.27</li>
<li><strong>심투리얼(Sim-to-Real) 전이:</strong> 현실 세계에서 강화 학습을 수행하는 것은 시간과 비용이 많이 들고 위험하다. 따라서 Isaac Sim과 같은 고정밀 물리 시뮬레이터에서 대규모로 학습한 후, 도메인 무작위화(Domain Randomization) 기술을 통해 현실 로봇에 이식하는 방식이 표준으로 자리 잡았다.28</li>
</ul>
<h3>3.2  아키텍처의 혁신: Diffusion Policy와 Transformer</h3>
<p>E2E 학습의 성능을 비약적으로 끌어올린 두 가지 핵심 기술은 **확산 모델(Diffusion Model)**과 **트랜스포머(Transformer)**이다.</p>
<ol>
<li>확산 정책 (Diffusion Policy): 멀티모달 행동의 표현</li>
</ol>
<p>전통적인 정책 네트워크는 주어진 상태에서 하나의 평균적인 행동을 출력하거나(Deterministic), 가우시안 분포를 가정(Stochastic)했다. 그러나 실제 환경에서는 동일한 상황에서도 여러 가지 유효한 행동이 존재할 수 있다(예: 장애물을 왼쪽으로 피할 수도, 오른쪽으로 피할 수도 있음). 평균적인 행동을 학습하면 장애물 정면으로 향하는 최악의 결과를 낳는다.</p>
<ul>
<li><strong>작동 원리:</strong> 2024년 발표된 Diffusion Policy는 이미지 생성 모델의 원리를 차용하여, 로봇의 행동 생성을 ‘조건부 노이즈 제거(Conditional Denoising)’ 과정으로 모델링했다.30 가우시안 노이즈에서 시작하여, 현재 관측(Observation)을 조건으로 반복적인 디노이징 단계를 거쳐 최적의 행동 궤적을 생성한다.</li>
<li><strong>성능 우위:</strong> 이 방식은 기존의 LSTM-GMM이나 단순 BC 방식 대비 평균 46.9% 이상의 성능 향상을 보였으며, 특히 장기적인 작업(Long-horizon tasks)과 멀티모달 행동 분포를 학습하는 데 탁월한 성능을 입증했다.31 또한, 위치 제어(Position Control) 모드에서도 부드럽고 안정적인 동작을 생성할 수 있어, 속도 제어(Velocity Control)에 의존하던 기존 방식의 한계를 극복했다.31</li>
</ul>
<ol start="2">
<li>Robotic Transformer (RT) 시리즈: 행동의 토큰화</li>
</ol>
<p>구글 딥마인드의 RT 시리즈는 언어 모델의 성공 방정식을 로봇에 그대로 적용했다.</p>
<ul>
<li><strong>RT-1:</strong> 이미지와 언어 명령을 입력받아, 로봇의 행동을 이산적인 토큰(Token) 시퀀스로 출력한다. 트랜스포머 아키텍처를 사용하여 이전 프레임의 정보를 효율적으로 처리하며 실시간 제어를 가능하게 했다.33</li>
<li><strong>RT-2:</strong> 인터넷 규모의 데이터로 학습된 거대 비전-언어 모델(VLM)을 로봇 데이터로 미세 조정(Fine-tuning)했다.35 핵심은 로봇의 행동을 텍스트와 동일한 형태의 ’문자열’로 취급한 것이다. 예를 들어, 로봇 팔의 움직임은 “1 128 91…“과 같은 숫자 토큰의 나열로 표현된다.36 이를 통해 RT-2는 “지친 사람에게 줄 음료수를 골라줘“와 같이 학습 데이터에 없는 추상적인 명령에 대해서도, 웹에서 학습한 지식을 바탕으로 에너지 드링크를 집어드는 ‘창발적(Emergent)’ 능력을 보여주었다.35</li>
</ul>
<h2>4.  스펙트럼의 중원: 하이브리드 및 계층적 아키텍처 (The Middle Ground)</h2>
<p>완전한 모듈형과 완전한 E2E 사이의 중간 지대에서는 두 접근법의 장점을 융합하려는 시도가 활발하다. 2025년 현재, 이 영역은 로봇 제어 아키텍처 연구의 가장 뜨거운 격전지이다.</p>
<h3>4.1  하이브리드 아키텍처: 학습된 인지와 고전적 제어의 결합</h3>
<p>가장 실용적인 접근법은 <strong>“인지는 딥러닝으로, 제어는 고전 제어로”</strong> 구성하는 것이다. 인지 모듈은 딥러닝의 패턴 인식 능력을 활용하여 복잡한 환경을 해석하고, 제어 모듈은 고전 제어의 수학적 보장성을 활용하여 정밀하고 안전한 동작을 수행한다.2</p>
<ul>
<li><strong>사례 분석:</strong> NVIDIA의 연구들은 시각 정보를 통해 “비용 지도(Cost Map)“나 “점유 지도(Occupancy Grid)“를 딥러닝으로 학습하고, 이를 리만 모션 정책(Riemannian Motion Policies, RMP)이나 MPC와 같은 최적화 기반 플래너에 입력으로 제공한다. 이 방식은 인지 오차에 대한 강건함과 동작의 부드러움을 동시에 달성한다.38</li>
</ul>
<h3>4.2  계층적 VLA 아키텍처: 시스템 1과 시스템 2의 통합</h3>
<p>인간의 인지 과정을 모사한 **이중 시스템 이론(Dual Process Theory)**은 로봇 아키텍처 설계에 깊은 영감을 주었다. ’시스템 1’은 빠르고 직관적이며 무의식적인 행동(반사적 동작)을, ’시스템 2’는 느리고 분석적이며 의식적인 추론(계획 수립)을 담당한다.39</p>
<ol>
<li>고수준(High-Level) 추론기: 시스템 2</li>
</ol>
<p>이 계층은 거대 언어 모델(LLM)이나 VLM이 담당한다. 사용자의 모호한 자연어 명령을 해석하고, 이를 구체적인 하위 작업(Sub-goals)이나 웨이포인트, 또는 코드로 변환한다.</p>
<ul>
<li><strong>Code as Policies (CaP):</strong> 구글의 CaP는 LLM에게 로봇 제어용 Python 코드를 작성하게 한다.41 LLM은 <code>detect_object()</code>, <code>move_arm()</code>과 같은 사전에 정의된 고수준 API를 조합하여 논리적인 제어 로직을 생성한다. 이는 LLM의 추론 능력과 모듈형 API의 신뢰성을 결합한 것으로, 복잡한 기하학적 추론이나 산술 연산이 필요한 작업에서 탁월한 성능을 보인다.43</li>
<li><strong>VoxPoser:</strong> LLM과 VLM을 활용하여 3D 공간상에 ’가치 지도(Value Map)’를 생성한다.44 “서랍을 열어라“라는 명령에 대해, LLM은 Python 코드를 생성하여 서랍 손잡이 주변에는 높은 가치를, 장애물 주변에는 낮은 가치를 부여하는 3D 맵을 만든다. 경로 계획기는 이 맵을 따라가기만 하면 된다. 별도의 학습 없이도(Zero-shot) 언어 명령을 물리적 궤적으로 변환하는 혁신적인 방법이다.46</li>
</ul>
<ol start="2">
<li>저수준(Low-Level) 실행기: 시스템 1</li>
</ol>
<p>고수준 모델이 설정한 목표를 달성하기 위해 실제 관절을 제어한다. 빠르고 반응적이어야 하므로, 소규모의 모방 학습 정책이나 강화 학습 모델이 사용된다.</p>
<ul>
<li><strong>HAMSTER 아키텍처:</strong> 2025년 발표된 HAMSTER는 계층적 VLA의 정점을 보여준다.17 고수준 VLA 모델은 입력 이미지 위에 로봇이 이동해야 할 거친 2D 경로(Coarse Path)를 그린다. 저수준의 3D 인식 정책은 이 경로를 가이드 삼아 정밀한 6 자유도 조작을 수행한다. 이 방식은 인터넷 데이터로 학습된 VLA의 일반화 능력과, 로봇 특화 데이터로 학습된 저수준 정책의 정밀함을 동시에 달성하여, 기존 단일 VLA 모델 대비 20% 이상의 성공률 향상을 기록했다.48</li>
<li><strong>Hi-Robot:</strong> 유사하게 Hi-Robot은 고수준 VLM을 통해 사용자와 대화하며 작업을 구체화하고, 저수준 정책으로 빠른 동작을 수행하는 구조를 갖추고 있다.39</li>
</ul>
<h2>5.  파운데이션 모델의 통합과 VLA 모델의 부상</h2>
<p>2024년 이후 로봇 제어 아키텍처의 가장 큰 파도는 **파운데이션 모델(Foundation Models)**의 전면적인 도입이다. 특히 비전-언어-행동(VLA) 모델은 인지와 추론, 행동 생성을 하나의 모델로 통합하려는 야심 찬 시도이다.50</p>
<h3>5.1  OpenVLA와 Octo: 개방형 혁신</h3>
<p>RT-2가 독점적인 모델이었다면, <strong>OpenVLA</strong>와 <strong>Octo</strong>는 오픈 소스 커뮤니티를 통해 VLA 기술을 대중화하고 있다.52</p>
<ul>
<li><strong>Octo:</strong> 80만 개 이상의 로봇 에피소드로 구성된 Open X-Embodiment 데이터셋으로 사전 학습된 트랜스포머 기반 확산 정책이다.53 Octo는 다양한 로봇 형태(Embodiment)와 센서 구성을 지원하며, 새로운 작업에 대해 적은 데이터로도 미세 조정(Fine-tuning)이 가능하도록 설계되었다.54</li>
<li><strong>OpenVLA:</strong> 최근 연구에서는 VLA 모델의 행동 공간을 단순한 위치 제어가 아닌, 토크 제어까지 확장하려는 시도가 이루어지고 있다. OpenVLA의 최신 변형들은 토크 어댑터(Torque Adapter)를 도입하거나 보조 출력으로 토크를 예측하게 하여, 접촉이 많은 작업에서의 성능을 개선하고 있다.52</li>
</ul>
<h3>5.2  행동 공간(Action Space)의 설계</h3>
<p>VLA 모델에서 로봇의 행동을 어떻게 표현할 것인가는 성능을 좌우하는 핵심 요소이다.</p>
<ul>
<li><strong>이산 토큰화(Discrete Tokenization):</strong> RT 시리즈처럼 연속적인 관절 값을 256개의 구간으로 나누어 토큰화하는 방식. 언어 모델과의 통합이 쉽지만, 정밀도가 떨어지고 고빈도 제어(High-frequency Control)에 불리할 수 있다.57</li>
<li><strong>연속적 표현(Continuous Representation):</strong> Diffusion Policy나 Octo와 같이 연속적인 값을 직접 다루는 방식. 정밀한 제어에 유리하며, 최근 VLA 모델들은 점차 이 방식을 채택하는 추세이다.58</li>
</ul>
<h2>6.  아키텍처 비교 분석 및 트레이드오프 (Comparative Analysis)</h2>
<p>로봇 제어 아키텍처의 선택은 “모든 것에 맞는 하나의 정답(Silver Bullet)“이 없는 공학적 트레이드오프의 영역이다. 다음의 표와 분석은 각 아키텍처의 특성을 4가지 핵심 차원에서 비교한다.3</p>
<h3>6.1 표 3.1.1: 로봇 제어 아키텍처 심층 비교 (Comparative Analysis of Architectures)</h3>
<table><thead><tr><th><strong>특성 (Dimension)</strong></th><th><strong>고전적 모듈형 (Modular)</strong></th><th><strong>계층적 VLA (Hierarchical VLA)</strong></th><th><strong>엔드투엔드 (End-to-End E2E)</strong></th></tr></thead><tbody>
<tr><td><strong>데이터 요구량</strong></td><td><strong>낮음</strong> (물리 모델 및 알고리즘 기반)</td><td><strong>중간</strong> (사전 학습 모델 활용 + 소량의 적응 데이터)</td><td><strong>매우 높음</strong> (대규모 도메인 데이터 필수)</td></tr>
<tr><td><strong>일반화 성능</strong></td><td><strong>낮음</strong> (사전 정의된 환경 및 조건에 한정)</td><td><strong>높음</strong> (LLM/VLM의 추론 능력으로 낯선 명령/환경 대응)</td><td><strong>중간~높음</strong> (데이터 분포 내에서는 강력하나, 분포 밖은 취약)</td></tr>
<tr><td><strong>정밀도 (Precision)</strong></td><td><strong>매우 높음</strong> (수학적 최적화 및 고빈도 피드백)</td><td><strong>높음</strong> (저수준 정책이 정밀 제어 담당)</td><td><strong>중간</strong> (토큰화 등으로 인한 정밀도 손실 가능성)</td></tr>
<tr><td><strong>해석 가능성</strong></td><td><strong>높음</strong> (모듈별 입출력 검증 및 디버깅 용이)</td><td><strong>중간</strong> (고수준 계획은 해석 가능, 저수준 실행은 블랙박스)</td><td><strong>낮음</strong> (전체 과정이 거대한 신경망 내부 연산)</td></tr>
<tr><td><strong>안전성 (Safety)</strong></td><td><strong>높음</strong> (명시적 제약 조건 및 안전 필터 적용 용이)</td><td><strong>중간</strong> (계획 단계 검증 가능, 실행 단계 주의 필요)</td><td><strong>낮음</strong> (행동의 안전성 보장이 확률적임)</td></tr>
<tr><td><strong>실행 빈도 (Hz)</strong></td><td><strong>높음</strong> (최적화된 C++ 코드, &gt;100Hz 가능)</td><td><strong>혼합</strong> (고수준: &lt;1Hz, 저수준: &gt;50Hz)</td><td><strong>낮음~중간</strong> (거대 모델 추론으로 인한 지연, 5~10Hz)</td></tr>
<tr><td><strong>대표 기술 및 사례</strong></td><td>ROS Navigation, TAMP, MPC</td><td>HAMSTER, VoxPoser, Code as Policies, Hi-Robot</td><td>RT-2, Mobile ALOHA, Diffusion Policy, Octo</td></tr>
</tbody></table>
<h3>6.2  일반화 대 정밀도 (Generalization vs. Precision)</h3>
<ul>
<li><strong>VLA 모델</strong>은 인터넷 데이터의 힘을 빌려 놀라운 <strong>의미론적 일반화(Semantic Generalization)</strong> 능력을 보여준다. 그러나 “스폰지밥 인형을 집어“라는 명령을 이해하는 능력과, 그 인형을 떨어뜨리지 않고 정밀하게 파지하는 능력은 별개이다. E2E 모델은 종종 이 기하학적 정밀도(Geometric Precision)에서 한계를 보인다.</li>
<li><strong>계층적 아키텍처</strong>는 이 딜레마를 해결하기 위한 ‘분업화’ 전략이다. VLM이 대략적인 “무엇을 해야 하는가“를 결정하고, 저수준 정책이나 고전 제어기가 “어떻게 정확하게 움직일 것인가“를 담당함으로써 두 마리 토끼를 잡는다.47</li>
</ul>
<h3>6.3  샘플 효율성 대 확장성 (Sample Efficiency vs. Scalability)</h3>
<ul>
<li><strong>E2E 모델</strong>은 데이터가 많을수록 성능이 향상되는 **스케일링 법칙(Scaling Laws)**을 따른다. 그러나 로봇 데이터 수집은 비용이 매우 높다.</li>
<li>**하이브리드 모델(VoxPoser 등)**은 별도의 학습 없이도(Zero-shot) 즉시 작동 가능한 극도의 샘플 효율성을 보여준다. 이는 새로운 환경에 로봇을 배치할 때 재학습 비용을 획기적으로 낮춰준다.44</li>
</ul>
<h3>6.4  안전성과 신뢰성 (Safety and Reliability)</h3>
<ul>
<li>산업 현장에서는 여전히 <strong>모듈형 아키텍처</strong>가 선호된다. 안전 필터(Safety Filter)나 가드레일(Guardrails)을 명시적으로 설계하여 로봇이 위험한 영역으로 진입하는 것을 원천적으로 차단할 수 있기 때문이다.15</li>
<li>최신 연구들은 VLA 모델에 **“내부 독백(Inner Monologue)”**이나 <strong>“자기 성찰(Self-Correction)”</strong> 메커니즘을 도입하여, 로봇이 자신의 계획을 실행하기 전에 안전성을 스스로 검토하게 하거나, 실패 시 다른 전략을 수립하도록 유도하고 있다.39</li>
</ul>
<h2>7.  결론: 수렴하는 스펙트럼과 미래의 아키텍처</h2>
<p>로봇 제어 아키텍처의 스펙트럼은 더 이상 양자택일의 전장이 아니다. 2025년의 로봇 공학은 <strong>모듈형 시스템의 구조적 안정성</strong>과 <strong>엔드투엔드 시스템의 유연한 학습 능력</strong>을 결합하는 방향으로 빠르게 수렴하고 있다. 과거의 “픽셀에서 토크까지“라는 급진적인 비전은, 이제 거대 언어 모델이 고수준의 ‘두뇌’ 역할을 하고, 확산 정책과 같은 강력한 학습 기반 제어기가 저수준의 ‘신경계’ 역할을 하며, 그 사이를 명시적인 중간 표현(2D 경로, 가치 지도, 코드 등)이 연결하는 <strong>계층적 파운데이션 모델 아키텍처</strong>로 진화하였다.</p>
<p>이러한 융합은 로봇이 통제된 실험실 환경을 벗어나, 복잡하고 예측 불가능한 현실 세계(Open World)에서 인간과 공존하며 유용한 작업을 수행할 수 있게 하는 핵심 열쇠가 될 것이다. 우리는 바야흐로 로봇이 단순한 자동화 기계를 넘어, 세상을 이해하고 스스로 행동을 계획하는 진정한 의미의 지능형 에이전트로 거듭나는 시대를 목격하고 있다. 이어진 장에서는 이러한 아키텍처들이 실제 로봇 조작(Manipulation)과 내비게이션(Navigation) 작업에 어떻게 구체적으로 구현되고 적용되는지, 최신 기술적 세부 사항들을 다룰 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Efficient and real-time perception: a survey on end-to-end event-based object detection in autonomous driving - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1674421/full</li>
<li>Bridging the Gap Between Modular and End-to-end Autonomous Driving Systems - UC Berkeley EECS, https://www2.eecs.berkeley.edu/Pubs/TechRpts/2022/EECS-2022-79.pdf</li>
<li>Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model - arXiv, https://arxiv.org/html/2512.14031v1</li>
<li>Safe Learning for Contact-Rich Robot Tasks: A Survey from Classical Learning-Based Methods to Safe Foundation Models - arXiv, https://arxiv.org/html/2512.11908v1</li>
<li>A Step Toward World Models: A Survey on Robotic Manipulation - arXiv, https://arxiv.org/html/2511.02097v2</li>
<li>Foundation Model Driven Robotics: A Comprehensive Review - arXiv, https://arxiv.org/html/2507.10087v1</li>
<li>Autonomous driving: Modular pipeline Vs. End-to-end and LLMs | by Samer Attrah | Medium, https://medium.com/@samiratra95/autonomous-driving-modular-pipeline-vs-end-to-end-and-llms-642ca7f4ef89</li>
<li>Multimodal perception-driven decision-making for human-robot interaction: a survey, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1604472/full</li>
<li>Actor-Critic Model Predictive Control: Differentiable Optimization meets Reinforcement Learning - Robotics and Perception Group, https://rpg.ifi.uzh.ch/research_learning.html</li>
<li>Task and Motion Planning for Execution in the Real - Kavraki Lab, https://www.kavrakilab.rice.edu/publications/pan2024-tamper.pdf</li>
<li>[2406.03641] Task and Motion Planning for Execution in the Real - arXiv, https://arxiv.org/abs/2406.03641</li>
<li>[2404.02817] A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches - arXiv, https://arxiv.org/abs/2404.02817</li>
<li>Simultaneous Path Planning and Task Allocation in Dynamic Environments - MDPI, https://www.mdpi.com/2218-6581/14/2/17</li>
<li>Safe Reinforcement Learning Direct Torque Control for Continuous Control Set Permanent Magnet Synchronous Motor Drives | Request PDF - ResearchGate, https://www.researchgate.net/publication/383161621_Safe_Reinforcement_Learning_Direct_Torque_Control_for_Continuous_Control_Set_Permanent_Magnet_Synchronous_Motor_Drives</li>
<li>How Robotics Are Transforming Production Lines in 2025 - Delta Wye Electric, https://deltawye.com/robotics-are-transforming-production-lines/</li>
<li>Designing for Distributed Heterogeneous Modularity: On Software Architecture and Deployment of the MoonBots - arXiv, https://arxiv.org/html/2511.01437v1</li>
<li>HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation, https://hamster-robot.github.io/</li>
<li>Learned perception modules for autonomous aerial vehicle navigation and control - IRoM-Lab - Princeton University, https://irom-lab.princeton.edu/wp-content/uploads/2025/07/Simon_princeton_0181D_15442-1.pdf</li>
<li>(PDF) From Pixels to Torques: Policy Learning with Deep Dynamical Models, https://www.researchgate.net/publication/272195531_From_Pixels_to_Torques_Policy_Learning_with_Deep_Dynamical_Models</li>
<li>From Pixels to Torques with Linear Feedback | Request PDF - ResearchGate, https://www.researchgate.net/publication/381770733_From_Pixels_to_Torques_with_Linear_Feedback</li>
<li>mobile-aloha.pdf, https://mobile-aloha.github.io/resources/mobile-aloha.pdf</li>
<li>Meet the robot that can sauté shrimp - Stanford Report, https://news.stanford.edu/stories/2024/04/meet-robot-that-can-saute-shrimp</li>
<li>A solution achieving the whole process of Mobile Aloha - Hackster.io, https://www.hackster.io/agilexrobotics/a-solution-achieving-the-whole-process-of-mobile-aloha-2e8326</li>
<li>ALOHA mobile robotics: a step towards autonomous collaborative robots, https://www.generationrobots.com/blog/en/aloha-mobile-robotics-a-step-towards-autonomous-collaborative-robots/</li>
<li>Stable Jumping Control Based on Deep Reinforcement Learning for a Locust-Inspired Robot - MDPI, https://www.mdpi.com/2313-7673/9/9/548</li>
<li>Taxonomy and Trends in Reinforcement Learning for Robotics and Control Systems: A Structured Review - arXiv, https://arxiv.org/html/2510.21758v3</li>
<li>Latent Action Priors for Locomotion with Deep Reinforcement Learning - arXiv, https://arxiv.org/html/2410.03246v2</li>
<li>Real-World Robotic Perception and Control Using Synthetic Data - UC Berkeley EECS, https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-104.pdf</li>
<li>Dojo: A Differentiable Physics Engine for Robotics - arXiv, https://arxiv.org/html/2203.00806v5</li>
<li>Diffusion Policy: - Robotics, https://www.roboticsproceedings.org/rss19/p026.pdf</li>
<li>Visuomotor Policy Learning via Action Diffusion - arXiv, https://arxiv.org/html/2303.04137v5</li>
<li>Visuomotor Policy Learning via Action Diffusion, https://diffusion-policy.cs.columbia.edu/diffusion_policy_ijrr.pdf</li>
<li>RT-1: Robotics Transformer for Real-World Control at Scale - ResearchGate, https://www.researchgate.net/publication/372803068_RT-1_Robotics_Transformer_for_Real-World_Control_at_Scale</li>
<li>Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey, https://arxiv.org/html/2508.13073v1</li>
<li>RT-2: New model translates vision and language into action - Google DeepMind, https://deepmind.google/blog/rt-2-new-model-translates-vision-and-language-into-action/</li>
<li>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control - Hugging Face, https://huggingface.co/papers/2307.15818</li>
<li>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v229/zitkovich23a/zitkovich23a.pdf</li>
<li>R²D²: Perception-Guided Task &amp; Motion Planning for Long-Horizon Manipulation | NVIDIA Technical Blog, https://developer.nvidia.com/blog/r2d2-perception-guided-task-amp-motion-planning-for-long-horizon-manipulation/</li>
<li>Hi Robot, https://hi-robot-vla.github.io/</li>
<li>NaVILA: Legged Robot Vision-Language-Action Model for Navigation - arXiv, https://arxiv.org/html/2412.04453v1</li>
<li>Code as Policies: Language Model Programs for Embodied Control, https://code-as-policies.github.io/</li>
<li>[2209.07753] Code as Policies: Language Model Programs for Embodied Control - arXiv, https://arxiv.org/abs/2209.07753</li>
<li>Robots That Write Their Own Code - Google Research, https://research.google/blog/robots-that-write-their-own-code/</li>
<li>VoxPoser, https://voxposer.github.io/</li>
<li>Composable 3D Value Maps for Robotic Manipulation with Language Models - VoxPoser, https://voxposer.github.io/voxposer.pdf</li>
<li>[2307.05973] VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models - arXiv, https://arxiv.org/abs/2307.05973</li>
<li>\method: Hierarchical Action Models for Open-World Robot Manipulation - arXiv, https://arxiv.org/html/2502.05485</li>
<li>\method: Hierarchical Action Models for Open-World Robot Manipulation - arXiv, https://arxiv.org/html/2502.05485v1</li>
<li>Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models - Physical Intelligence, https://www.physicalintelligence.company/download/hirobot.pdf</li>
<li>JeffreyYH/Awesome-Generalist-Robots-via-Foundation-Models: Paper list in the survey paper - GitHub, https://github.com/JeffreyYH/Awesome-Generalist-Robots-via-Foundation-Models</li>
<li>What Foundation Models can Bring for Robot Learning in Manipulation : A Survey - arXiv, https://arxiv.org/abs/2404.18201</li>
<li>[2509.07962] TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models - arXiv, https://arxiv.org/abs/2509.07962</li>
<li>Octo: An Open-Source Generalist Robot Policy, https://octo-models.github.io/</li>
<li>An Open-Source Generalist Robot Policy - Octo, https://octo-models.github.io/paper.pdf</li>
<li>Octo: An Open-Source Generalist Robot Policy - arXiv, https://arxiv.org/html/2405.12213v2</li>
<li>Elucidating the Design Space of Torque-aware Vision-Language-Action Models, https://openreview.net/forum?id=HAmi1X11BO</li>
<li>Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications, https://arxiv.org/html/2510.07077v1</li>
<li>Kinematics-Aware Diffusion Policy with Consistent 3D Observation and Action Space for Whole-Arm Robotic Manipulation - arXiv, https://arxiv.org/html/2512.17568</li>
<li>Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation - arXiv, https://arxiv.org/html/2403.03890v1</li>
<li>Deep Reinforcement and IL for Autonomous Driving: A Review in the CARLA Simulation Environment - MDPI, https://www.mdpi.com/2076-3417/15/16/8972</li>
<li>Agentic LLM-based robotic systems for real-world applications: a review on their agenticness and ethics - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1605405/full</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>