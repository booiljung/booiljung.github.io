<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Chapter 3. 로봇을 위한 SOTA 기술 지형도</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Chapter 3. 로봇을 위한 SOTA 기술 지형도</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">제목: Embodied AI & Modern Control</a> / <a href="index.html">Chapter 3. 로봇을 위한 SOTA 기술 지형도</a> / <span>Chapter 3. 로봇을 위한 SOTA 기술 지형도</span></nav>
                </div>
            </header>
            <article>
                <h1>Chapter 3. 로봇을 위한 SOTA 기술 지형도</h1>
<h2>1.  서론: 물리적 인공지능(Physical AI)으로의 패러다임 전환</h2>
<p>2025년은 로봇 공학의 역사에서 ’물리적 인공지능(Physical AI)’이라는 새로운 패러다임이 확고히 뿌리내린 해로 기록될 것이다.1 과거 수십 년간 로봇 공학을 지배해 온 고전적인 제어 이론과 모듈식 파이프라인—즉, 인식(Perception), 계획(Planning), 제어(Control)가 명확히 분리된 구조—은 거대 언어 모델(LLM)의 추론 능력과 비전-언어-행동(Vision-Language-Action, VLA) 모델의 결합을 통해 급격히 해체되고 재구성되었다. 이 장에서는 2024년 말부터 2025년 상반기에 걸쳐 로봇 기술의 최전선(State-of-the-Art, SOTA)을 정의하는 핵심 기술들을 심층적으로 분석한다.</p>
<p>전통적인 로봇 공학이 사전에 정의된 환경에서 정밀한 반복 작업을 수행하는 데 초점을 맞추었다면, 현재의 물리적 AI는 ’비정형화된 현실 세계(Open-World)’에서의 범용적 적응성을 목표로 한다.4 이는 로봇이 사전에 학습하지 않은 낯선 물체를 조작하고, 모호한 자연어 명령을 문맥에 맞게 해석하며, 시뮬레이션에서 축적한 지식을 현실 세계로 전이(Sim-to-Real Transfer)하는 능력을 요구한다. 이러한 요구사항을 충족시키기 위해 등장한 기술적 계층(Stack)은 데이터 중심(Data-Centric)의 접근법과 생성형 AI(Generative AI) 아키텍처를 기반으로 하고 있다.6</p>
<p>본 장에서는 로봇 파운데이션 모델(RFM)의 진화, 인간의 인지 구조를 모사한 시스템 1과 시스템 2의 하이브리드 아키텍처, 시뮬레이션과 현실의 경계를 허무는 월드 모델(World Models), 그리고 이 모든 학습을 가능케 하는 데이터 엔진과 SOTA 하드웨어 플랫폼의 상호작용을 포괄적으로 다룬다. 특히 2025년에 등장한 <span class="math math-inline">\pi_0</span>(Pi-Zero), GR00T N1, Helix와 같은 구체적인 모델들과 Tesla Optimus Gen 3, Figure 03, 1X NEO 등 최신 하드웨어 플랫폼의 기술적 사양과 설계 철학을 상세히 비교 분석함으로써, 독자들에게 미래 로봇 시스템의 청사진을 제시하고자 한다.</p>
<h2>2.  로봇 파운데이션 모델 (Robot Foundation Models)</h2>
<p>로봇 파운데이션 모델(RFM)은 인터넷 규모의 텍스트, 이미지, 비디오 데이터로 사전 학습된 대형 모델을 로봇의 인지, 추론, 행동 생성에 적용하는 기술을 총칭한다. 2023년의 RT-2가 시각-언어 모델(VLM)을 로봇 제어에 접목하는 가능성을 보여주었다면, 2025년의 RFM은 행동(Action)을 직접적이고 효율적으로 생성하는 VLA(Vision-Language-Action) 모델로의 전환이 완료된 상태이다.4</p>
<h3>2.1  비전-언어-행동(VLA) 모델의 아키텍처 진화</h3>
<p>VLA 모델은 시각적 관측(Observation)과 자연어 명령(Instruction)을 입력으로 받아, 로봇의 관절 제어 신호나 엔드 이펙터(End-effector)의 6자유도(6-DoF) 포즈를 직접 출력한다. 이 과정에서 가장 큰 기술적 난제는 고차원의 연속적인 행동 공간(Continuous Action Space)을 언어 모델의 이산적 토큰(Discrete Token) 구조와 어떻게 통합하느냐 하는 것이었다.</p>
<h4>2.1.1 토큰화(Tokenization)와 연속 제어의 통합</h4>
<p>초기 VLA 모델인 RT-1과 RT-2는 행동을 이산적인 토큰(예: 0~255 사이의 정수)으로 변환하여 언어 모델이 텍스트를 생성하듯 행동을 생성하게 했다.8 그러나 이 방식은 정밀한 제어가 어렵고, 고빈도 제어(High-frequency Control)에 부적합하다는 한계가 있었다. 2025년 SOTA 모델들은 이러한 한계를 극복하기 위해 **확산 정책(Diffusion Policy)**과 <strong>유동 매칭(Flow Matching)</strong> 기술을 도입했다.9</p>
<p>확산 정책은 노이즈로부터 유효한 행동 분포를 생성하는 방식으로, 멀티모달(Multimodal) 행동 분포를 효과적으로 모델링한다. 예를 들어, 컵을 잡는 방법이 여러 가지(위에서 잡기, 옆에서 잡기)일 때, 확산 모델은 이들 각각의 가능성을 모두 포괄하는 분포를 학습하여 상황에 가장 적합한 행동을 생성할 수 있다. 2025년 Physical Intelligence사가 발표한 <span class="math math-inline">\pi_0</span>(Pi-Zero) 모델은 여기서 한 단계 더 나아가 ‘유동 매칭’ 기법을 적용했다.9 유동 매칭은 확률 흐름(Probability Flow)을 직접 학습하여 확산 모델보다 더 적은 추론 단계(Inference Step)로 고품질의 행동 궤적을 생성할 수 있어, 실시간 제어가 필수적인 로봇 시스템에 최적화되어 있다.</p>
<h4>2.1.2 <span class="math math-inline">\pi_0</span> (Pi-Zero): 범용성을 위한 흐름</h4>
<p><span class="math math-inline">\pi_0</span>는 인터넷 규모의 VLM(PaliGemma 등)을 백본으로 사용하되, 이를 로봇의 행동 생성에 특화된 유동 매칭 헤드(Head)와 결합했다.11 이 모델의 가장 큰 특징은 **‘형태 불가지론(Embodiment-Agnostic)’**이다. <span class="math math-inline">\pi_0</span>는 단일 암(Single-arm), 양팔(Dual-arm), 모바일 매니퓰레이터(Mobile Manipulator), 심지어 쿼드러페드(Quadruped) 로봇 등 8종 이상의 서로 다른 하드웨어에서 수집된 데이터를 통합하여 학습했다.12 이는 특정 로봇에 종속되지 않는 ’범용 로봇 뇌’를 구현하려는 시도로, 데이터의 이질성을 극복하기 위해 로봇의 고유한 운동학(Kinematics) 정보를 모델의 입력으로 함께 제공하거나, 행동 공간을 정규화하는 기술이 적용되었다. <span class="math math-inline">\pi_0</span>는 설거지, 빨래 개기, 박스 조립과 같은 복잡한 가사 노동 작업에서 기존의 OpenVLA나 Octo 모델 대비 압도적인 성능을 입증했다.12</p>
<h3>2.2  휴머노이드 특화 모델: GR00T N1</h3>
<p>NVIDIA가 공개한 GR00T(Generalist Robot 00 Technology) N1 모델은 휴머노이드 로봇의 제어에 특화된 VLA 모델이다.13 휴머노이드는 균형 유지와 보행, 양팔 조작이 동시에 이루어져야 하므로 제어 난이도가 극히 높다. GR00T N1은 이를 해결하기 위해 인간의 비디오 데이터와 시뮬레이션 데이터를 대규모로 활용했다.</p>
<table><thead><tr><th><strong>특성</strong></th><th><strong>GR00T N1 (NVIDIA)</strong></th><th><strong>π0 (Physical Intelligence)</strong></th><th><strong>RT-H (Google DeepMind)</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 아키텍처</strong></td><td>Dual-System (VLM + Diffusion Transformer)</td><td>VLM + Flow Matching</td><td>Hierarchical Transformer</td></tr>
<tr><td><strong>주요 입력 데이터</strong></td><td>시뮬레이션(Isaac Lab) + 실제 로봇 + 인간 비디오</td><td>다기종 로봇 데이터 (Open X + 자체 데이터)</td><td>웹 데이터 + 로봇 데이터 + 언어기반 계층 구조</td></tr>
<tr><td><strong>행동 생성 방식</strong></td><td>Diffusion 기반 연속 제어</td><td>Flow Matching 기반 연속 제어</td><td>언어적 추론 후 하위 정책 호출</td></tr>
<tr><td><strong>특화 영역</strong></td><td>휴머노이드 전신 제어, 균형 유지</td><td>다양한 형태의 로봇(Cross-Embodiment), 가사 작업</td><td>긴 호흡의 작업 계획, 추론</td></tr>
<tr><td><strong>공개 여부 (2025)</strong></td><td>일부 공개 (Open Model 지향)</td><td>기술 보고서 및 일부 웨이트 공개</td><td>논문 공개</td></tr>
</tbody></table>
<p>GR00T N1의 핵심은 **‘이중 시스템 아키텍처(Dual-System Architecture)’**이다. 시각-언어 모듈(System 2)이 환경을 해석하고 자연어 명령을 이해하여 상위 수준의 잠재 표현(Latent Representation)을 생성하면, 확산 트랜스포머(System 1)가 이를 바탕으로 실시간 모터 제어 신호를 생성한다. 특히 GR00T는 NVIDIA의 옴니버스(Omniverse) 기반 시뮬레이션 환경인 ’Isaac Lab’에서 생성된 방대한 합성 데이터를 학습에 활용하여, 실제 데이터만으로는 확보하기 어려운 다양한 예외 상황(Edge Cases)에 대한 대응 능력을 키웠다.14</p>
<h3>2.3  물리학적 추론과 RFM-1</h3>
<p>Covariant의 RFM-1(Robotics Foundation Model 1)은 로봇이 단순히 행동을 모방하는 것을 넘어, 물리학적 인과관계를 이해하도록 설계되었다.16 RFM-1은 텍스트, 이미지, 비디오, 그리고 로봇의 센서 데이터(토크, 위치 등)를 모두 토큰화하여 통합 학습하는 디코더 전용(Decoder-only) 트랜스포머 모델이다. 이 모델은 “다음 프레임에 무슨 일이 일어날지“를 예측하는 비디오 생성 능력을 통해, 로봇이 자신의 행동 결과를 시뮬레이션하고 물리적 상식(예: 무거운 물체는 관성이 크다)을 내재화하게 한다. 이는 로봇이 한 번도 본 적 없는 물체를 다룰 때도 “깨질 것 같은 물체는 조심스럽게 다뤄야 한다“는 식의 일반화된 추론을 가능케 한다.</p>
<h2>3.  인지 아키텍처: 시스템 1과 시스템 2의 융합</h2>
<p>로봇 제어 소프트웨어 스택은 2025년에 들어서며 인간의 인지 과정을 모사한 ’시스템 1(직관)’과 ’시스템 2(추론)’의 하이브리드 아키텍처로 수렴하고 있다.18 이는 순수 종단간 학습(End-to-End Learning)이 가진 확장성의 한계와 고전적 모듈형 제어가 가진 유연성의 부족을 동시에 해결하기 위한 전략적 선택이다.</p>
<h3>3.1  이중 처리 이론의 로봇 공학적 구현</h3>
<p>다니엘 카너먼의 이중 정보 처리 이론에서 영감을 받은 이 아키텍처는 로봇의 기능을 시간적 해상도와 추론의 깊이에 따라 두 계층으로 분리한다.</p>
<ul>
<li><strong>시스템 1 (Fast &amp; Reactive):</strong> 척수 반사나 소뇌의 기능에 해당한다. 고빈도(50Hz~200Hz)로 작동하며, 시각적 입력에 대해 즉각적인 운동 명령(Motor Command)을 생성한다. 여기에는 주로 <strong>확산 정책(Diffusion Policy)</strong>, <strong>유동 매칭(Flow Matching)</strong>, 또는 <strong>트랜스포머 기반의 행동 모델</strong>이 사용된다. 시스템 1은 복잡한 논리적 추론보다는 현재 상태에서 최적의 동작을 매끄럽게 생성하는 데 집중하며, 외란에 대해 강건하고 반응이 빠르다.19</li>
<li><strong>시스템 2 (Slow &amp; Deliberative):</strong> 대뇌의 전두엽 기능에 해당한다. 저빈도(0.5Hz~5Hz)로 작동하며, 전체적인 작업의 계획(Planning), 논리적 추론(Reasoning), 오류 수정(Error Recovery)을 담당한다. 주로 **대형 언어 모델(LLM)**이나 **대형 멀티모달 모델(VLM)**이 이 역할을 수행한다. 시스템 2는 사용자의 자연어 명령을 해석하고, 장기적인 목표를 달성하기 위해 시스템 1에게 구체적인 하위 목표(Sub-goal)나 기술(Skill)을 지시한다.19</li>
</ul>
<h3>3.2  Figure AI의 Helix: 하드웨어와 AI의 공진화</h3>
<p>Figure AI의 휴머노이드 로봇에 탑재된 ‘Helix’ 모델은 이러한 이중 구조를 상용 수준으로 구현한 대표적인 사례이다.19</p>
<ol>
<li><strong>관심사의 분리(Separation of Concerns):</strong> Helix는 시스템 1과 시스템 2를 명시적으로 분리하여 개발 및 최적화 효율을 극대화했다. 시스템 2는 인터넷 규모의 데이터로 학습된 VLM을 사용하여 범용적인 객체 인식과 상식적 추론 능력을 제공한다. 반면, 시스템 1은 Figure 로봇의 하드웨어 특성에 맞춰 최적화된 시각-운동 정책(Visuomotor Policy)을 사용하여 정밀한 전신 제어(Whole-body Control)를 수행한다.</li>
<li><strong>비동기 통신(Asynchronous Communication):</strong> 두 시스템은 서로 다른 주파수로 독립적으로 실행되면서 비동기적으로 통신한다. 시스템 1은 200Hz의 제어 루프를 돌면서, 가장 최근에 시스템 2가 생성한 상위 명령(Latent Vector 또는 Language Embedding)을 참조한다. 이를 통해 거대 모델의 느린 추론 속도가 로봇의 빠른 반사 신경을 저해하지 않도록 설계되었다.19</li>
<li><strong>전신 제어의 통합:</strong> Helix는 기존 모델들이 주로 팔 조작(Manipulation)에만 집중했던 것과 달리, 손가락의 파지, 팔의 궤적, 몸통의 균형, 머리의 시선 처리를 포함한 전신 동작을 하나의 통합된 네트워크로 제어한다. 이는 휴머노이드가 걷으면서 물건을 집거나, 쪼그리고 앉아 작업을 수행하는 등 역동적인 동작을 가능하게 한다.</li>
</ol>
<h3>3.3  계층적 범용 정책(Hierarchical Generalist Policies)과 Hi-Robot</h3>
<p>Physical Intelligence의 ‘Hi-Robot’ 연구는 계층적 구조가 어떻게 ‘긴 호흡의 작업(Long-horizon tasks)’ 성능을 향상시키는지 보여준다.21 단일 신경망으로 “부엌을 청소하고 커피를 타오라“는 긴 시퀀스를 처리하는 것은 매우 어렵다. 중간에 작은 실수만 있어도 전체 작업이 실패로 돌아가기 쉽기 때문이다.</p>
<p>Hi-Robot은 VLM을 상위 관리자로 사용하여 복잡한 명령을 “식탁 위의 쓰레기 식별”, “쓰레기통으로 이동”, “물체 파지 및 투기“와 같은 일련의 하위 작업으로 분해한다. 그리고 각 하위 작업은 해당 기능에 특화된 저수준 정책에 의해 실행된다. 만약 로봇이 쓰레기를 집는 데 실패하면, 시스템 2가 이를 인지하고 “다시 집으라“는 명령을 내리거나 “다른 각도에서 접근하라“는 수정된 계획을 수립할 수 있다. 이러한 <strong>‘닫힌 루프(Closed-loop) 추론’</strong> 능력은 로봇의 자율성을 비약적으로 높여주는 핵심 기술이다.18</p>
<h2>4.  월드 모델(World Models): 시뮬레이션과 현실의 경계 붕괴</h2>
<p>“로봇이 행동하기 전에 결과를 상상할 수 있다면 어떨까?” 2025년 로봇 공학의 또 다른 핵심 트렌드는 **월드 모델(World Models)**의 실용화이다.23 월드 모델은 로봇이 자신의 행동에 따른 환경의 변화를 내부적으로 시뮬레이션할 수 있게 해주는 인지 모델로, 전통적인 물리 엔진 기반 시뮬레이터의 한계를 뛰어넘고 있다.</p>
<h3>4.1  생성형 비디오 모델로서의 월드 모델</h3>
<p>1X Technologies는 자사의 로봇 EVE와 NEO를 위한 ’비디오 생성 기반 월드 모델’을 구축하여 이 분야를 선도하고 있다.25</p>
<ul>
<li><strong>미래 예측:</strong> 이 모델은 현재의 카메라 관측(Frame <span class="math math-inline">t</span>)과 로봇의 행동 명령(Action <span class="math math-inline">t</span>)을 입력으로 받아, 미래의 관측(Frame <span class="math math-inline">t+1, t+2,...</span>)을 비디오 형태로 생성한다. 이는 텍스트-비디오 생성 모델(Sora 등)의 로봇 버전이라 할 수 있다.</li>
<li><strong>정책 평가(Policy Evaluation):</strong> 로봇은 실제 행동을 수행하기 전에, 이 월드 모델을 사용하여 가상의 시뮬레이션을 수행한다. 예를 들어, “컵을 세게 쥐면 깨질까?“라는 질문에 대해 월드 모델은 컵이 깨지는 비디오를 생성함으로써 부정적인 결과를 예측하고, 로봇이 더 조심스러운 행동을 선택하도록 유도한다. 이는 수천 번의 시행착오를 실제 환경에서 겪지 않고도 안전하게 학습할 수 있게 해준다.27</li>
<li><strong>압축된 잠재 공간:</strong> 최신 월드 모델은 픽셀 공간(Pixel Space)이 아닌 압축된 잠재 공간(Latent Space)에서 미래를 예측함으로써 연산 효율성을 높이고, 모델 예측 제어(Model Predictive Control, MPC)와 결합하여 실시간 계획 수립을 가능하게 한다.28</li>
</ul>
<h3>4.2  3D Gaussian Splatting (3DGS)과 NeRF의 진화</h3>
<p>환경을 디지털로 복제하여 시뮬레이션에 활용하는 기술 또한 2025년에 큰 도약을 이루었다. NeRF(Neural Radiance Fields)가 3D 장면 재구성을 위한 딥러닝 접근법을 열었다면, **3D Gaussian Splatting (3DGS)**은 이를 실시간 응용이 가능한 수준으로 완성시켰다.30</p>
<ul>
<li><strong>실시간 렌더링:</strong> 3DGS는 장면을 수백만 개의 3D 가우시안 타원체로 표현하여, 기존 NeRF 대비 수십 배 빠른 렌더링 속도를 제공한다. 이는 로봇이 이동하면서 실시간으로 주변 환경의 고정밀 3D 지도를 생성하고(SLAM), 즉석에서 시뮬레이션을 돌려 경로를 계획하는 것을 가능하게 한다.32</li>
<li><strong>물리 정보 주입(Physics Integration):</strong> 단순한 시각적 복제를 넘어, 생성된 3DGS 장면에 물리적 속성(마찰력, 질량, 탄성 등)을 부여하는 연구가 활발하다.33 이는 생성형 AI가 만든 가상 환경에서 로봇이 물체와 상호작용(충돌, 파지)하며 학습할 수 있는 <strong>‘미분 가능한 물리(Differentiable Physics)’</strong> 시뮬레이터의 기반이 된다.34</li>
</ul>
<h3>4.3  자율주행 기술의 전이: 비디오 사전 학습</h3>
<p>테슬라의 FSD(Full Self-Driving) v12가 보여준 종단간 신경망의 성공은 로봇 분야에 강력한 영감을 주었다.36 테슬라는 수백만 대의 차량에서 수집한 주행 영상을 통해 ’도로의 물리학’을 학습한 거대 모델을 구축했고, 이를 옵티머스 로봇에 적용하고 있다. 자율주행 차가 도로의 흐름을 예측하듯, 로봇은 작업 공간의 동적인 변화를 예측한다. 2025년에는 이러한 **‘비디오 사전 학습(Video Pre-training)’**이 로봇 조작(Manipulation) 분야의 표준 학습법으로 자리 잡았으며, 로봇이 명시적인 프로그래밍 없이도 비디오 시청만으로 작업의 절차와 물리적 특성을 이해하는 수준에 도달했다.29</p>
<h2>5.  데이터 중심(Data-Centric) 로봇 공학: 양질의 데이터 확보</h2>
<p>아무리 뛰어난 아키텍처라도 학습할 데이터가 없다면 무용지물이다. 로봇 공학의 오랜 난제였던 데이터 부족 문제는 2025년 ’데이터 엔진(Data Engine)’의 구축과 대규모 오픈 데이터 이니셔티브를 통해 해결 국면에 접어들었다.3</p>
<h3>5.1  Open X-Embodiment와 DROID: 로봇의 ImageNet 모멘트</h3>
<p>2023년 말 시작된 <strong>Open X-Embodiment (OXE)</strong> 프로젝트는 2025년까지 폭발적으로 성장하며 로봇 학습의 기반 데이터셋으로 자리 잡았다.39</p>
<ul>
<li><strong>규모와 다양성:</strong> 전 세계 34개 이상의 연구소가 협력하여 22종 이상의 로봇 형태, 100만 개 이상의 궤적 데이터를 통합했다. 이 데이터셋은 로봇 학습 모델이 특정 하드웨어에 종속되지 않고 범용적인 특징을 학습할 수 있도록 돕는다.</li>
<li><strong>DROID 데이터셋의 고도화:</strong> DROID(Distributed Robot Interaction Dataset)는 통제된 실험실 환경이 아닌 ‘야생(In-the-wild)’ 환경에서의 데이터를 대규모로 제공한다.40 2025년 업데이트된 DROID는 7만 5천 개 이상의 에피소드에 대해 정밀한 카메라 캘리브레이션과 3개의 자연어 주석을 포함하여, VLA 모델이 조명 변화, 배경 잡동사니, 다양한 물체 형태에 강건해지도록 돕는 핵심 자원이 되었다.</li>
</ul>
<h3>5.2  합성 데이터(Synthetic Data)와 Sim-to-Real 파이프라인</h3>
<p>실제 데이터 수집의 높은 비용과 물리적 제약을 극복하기 위해 합성 데이터의 활용 비중이 급격히 증가했다. NVIDIA의 <strong>Isaac Lab</strong>과 <strong>Omniverse</strong> 플랫폼은 합성 데이터 생성의 표준 도구가 되었다.14</p>
<ul>
<li><strong>절차적 생성(Procedural Generation):</strong> 생성형 AI를 활용하여 시뮬레이션 내에 수천 가지의 방 구조, 가구 배치, 물체 텍스처를 자동으로 생성한다.42 로봇은 이 무한한 가상 환경에서 “파란색 컵을 찾아라“와 같은 과제를 반복 수행하며 탐색 및 조작 능력을 기른다.</li>
<li><strong>도메인 무작위화(Domain Randomization):</strong> 시뮬레이션의 물리 파라미터(마찰, 질량)와 시각적 요소(조명, 색상)를 무작위로 변형시켜 학습함으로써, 로봇이 현실 세계를 ’시뮬레이션의 또 다른 변형’으로 인식하게 만든다. 이는 Sim-to-Real 전이 시 발생하는 성능 저하를 최소화한다.43</li>
</ul>
<h3>5.3  텔레오퍼레이션과 데이터 엔진</h3>
<p>데이터 수집을 위한 원격 조작(Teleoperation) 기술도 진보했다. Apple Vision Pro와 같은 공간 컴퓨팅 기기나 ALOHA와 같은 저가형 마스터 암(Master Arm)을 활용하여, 사람이 로봇을 직접 조종하며 고품질의 데모 데이터를 수집하는 방식이 보편화되었다.45 Tesla와 Figure AI는 자사 공장에 배치된 로봇들이 실패한 케이스를 자동으로 선별하고, 이를 사람이 원격 조작으로 수정하여 다시 학습 데이터로 활용하는 **‘자동화된 데이터 엔진’**을 구축했다. 이 선순환 구조는 로봇의 성능을 기하급수적으로 향상시키는 원동력이 되고 있다.3</p>
<h2>6.  SOTA 하드웨어: 범용 휴머노이드의 상용화 경쟁</h2>
<p>2025년은 휴머노이드 로봇이 연구실을 벗어나 실제 산업 현장과 가정으로 진입하기 시작한 원년이다. Tesla, Figure AI, 1X Technologies, 그리고 중국의 유니트리(Unitree) 등이 각기 다른 철학으로 하드웨어 플랫폼을 발전시키고 있다.47</p>
<h3>6.1  Tesla Optimus Gen 3: 수직 계열화의 힘</h3>
<p>테슬라의 옵티머스 Gen 3는 가장 공격적인 양산 목표와 수직 계열화된 기술력을 자랑한다.49</p>
<ul>
<li><strong>하드웨어 내재화:</strong> 배터리, 액추에이터, 센서, 제어 보드 등 모든 핵심 부품을 테슬라가 직접 설계했다. 이는 비용 절감뿐만 아니라, 로봇의 관절 토크 제어와 같은 저수준 성능을 극한으로 끌어올리는 기반이 된다.</li>
<li><strong>AI 추론 칩:</strong> 자율주행을 위해 개발된 FSD 칩(AI Inference Chip)을 로봇에 이식하여, 클라우드 연결 없이도 복잡한 신경망을 로봇 내부에서 실시간으로 처리한다. 이는 통신 지연이나 단절 상황에서도 로봇의 안전을 보장한다.49</li>
<li><strong>인간 수준의 손:</strong> 22자유도(DoF)를 가진 손은 인간의 손을 생체모방하여 설계되었으며, 촉각 센서를 통해 달걀을 깨지 않고 쥐거나 미세한 물체를 조작하는 능력을 갖추었다.50</li>
</ul>
<h3>6.2  Figure 03: VLA 모델을 위한 맞춤형 설계</h3>
<p>Figure AI의 3세대 로봇 Figure 03는 하드웨어와 AI 모델(Helix)이 상호 의존적으로 설계된(Co-designed) 대표적인 사례이다.51</p>
<ul>
<li><strong>센서-모델 최적화:</strong> 로봇의 카메라 배치, 해상도, 프레임 레이트는 Helix 모델의 입력 요구사항에 맞춰 최적화되었다. 불필요한 데이터를 줄이고 추론 효율을 높이기 위함이다.</li>
<li><strong>상업적 내구성:</strong> 물류 창고 투입을 목표로 하여, 케이블이 외부로 노출되지 않는 엑소스켈레톤(Exoskeleton) 디자인을 채택했고, 24시간 가동을 위한 배터리 교체 및 관리 시스템을 갖추었다.46</li>
</ul>
<h3>6.3  1X Technologies NEO: 안전한 소프트 로보틱스</h3>
<p>1X의 NEO는 가정용 로봇 시장을 겨냥하여 안전성을 최우선으로 설계되었다.52</p>
<ul>
<li><strong>기어리스(Gearless) 구동:</strong> 딱딱한 기어박스 대신 고토크 모터와 케이블 구동 방식을 사용하여, 로봇의 팔다리가 유연하게 움직인다. 이는 사람과 충돌했을 때 충격을 흡수하여 부상을 방지하는 ’수동적 안전성(Passive Safety)’을 제공한다.</li>
<li><strong>가벼운 디자인:</strong> 인간과 유사한 체형에 가벼운 소재를 사용하여 가정 내에서의 이동성과 에너지 효율을 높였다.</li>
</ul>
<h3>6.4  하드웨어 비교 요약</h3>
<table><thead><tr><th><strong>특징</strong></th><th><strong>Tesla Optimus Gen 3</strong></th><th><strong>Figure 03</strong></th><th><strong>1X NEO</strong></th><th><strong>Unitree G1</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 철학</strong></td><td>대량 생산, 수직 계열화, FSD 기술 전이</td><td>VLA 모델(Helix) 최적화, 물류 자동화</td><td>가정용 안전성, 소프트 로보틱스</td><td>가성비, 고기동성(연구/교육용)</td></tr>
<tr><td><strong>AI 모델</strong></td><td>End-to-End Neural Net (Tesla AI)</td><td>Helix (System 1/2 Hybrid)</td><td>1X World Model</td><td>외부 모델 탑재 가능 (오픈 플랫폼)</td></tr>
<tr><td><strong>주요 스펙</strong></td><td>22 DoF 손, 커스텀 액추에이터</td><td>200Hz 전신 제어, 상체 조작 특화</td><td>기어리스 구동, 유연한 관절</td><td>360도 라이다, 접이식 구조</td></tr>
<tr><td><strong>타겟 시장</strong></td><td>일반 공장, 가정(장기)</td><td>물류 창고, 제조 현장</td><td>가정, 요양 서비스</td><td>연구소, 순찰, 교육</td></tr>
</tbody></table>
<h2>7.  결론 및 시사점: 범용 로봇을 향한 수렴</h2>
<p>2025년의 로봇 기술 지형도는 명확한 ‘수렴(Convergence)’ 현상을 보여준다. 파편화되어 있던 인식, 계획, 제어 모듈들은 **VLA(비전-언어-행동)**라는 거대 파운데이션 모델로 통합되었고, 빠른 반사 신경(<strong>시스템 1</strong>)과 깊은 추론(<strong>시스템 2</strong>)이 결합된 하이브리드 아키텍처가 범용 로봇 제어의 표준으로 자리 잡았다. 또한, <strong>월드 모델</strong>과 <strong>3DGS</strong> 기술의 발전으로 시뮬레이션은 단순한 가상 환경을 넘어 로봇 학습의 필수불가결한 ’데이터 공장’이 되었다.</p>
<p>이러한 기술적 진보는 로봇이 더 이상 반복적인 작업만 수행하는 ’자동화 기계’가 아니라, 인식하고, 추론하며, 세상과 상호작용하는 진정한 의미의 **‘체화된 지능(Embodied Intelligence)’**으로 거듭나고 있음을 시사한다. 하드웨어의 발전과 데이터 엔진의 확장이 맞물리며, 우리는 이제 ’실험실의 로봇’이 ’우리의 일상’으로 들어오는 역사적인 전환점에 서 있다. 다음 장에서는 이러한 기술들이 실제 물류, 제조, 가정 서비스 분야에서 어떻게 구체적인 가치를 창출하고 있는지 심층적으로 살펴볼 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>What Is NVIDIA’s Three-Computer Solution for Robotics? - NVIDIA Blog, https://blogs.nvidia.com/blog/three-computers-robotics/</li>
<li>AI Factories, Physical AI, and Advances in Models, Agents, and Infrastructure That Shaped 2025 | NVIDIA Technical Blog, https://developer.nvidia.com/blog/ai-factories-physical-ai-and-advances-in-models-agents-and-infrastructure-that-shaped-2025/</li>
<li>Expanding Our Data Engine for Physical AI - Scale AI, https://scale.com/blog/physical-ai</li>
<li>What Foundation Models can Bring for Robot Learning in Manipulation : A Survey - arXiv, https://arxiv.org/abs/2404.18201</li>
<li>[2402.02385] A Survey on Robotics with Foundation Models: toward Embodied AI - arXiv, https://arxiv.org/abs/2402.02385</li>
<li>[2303.10158] Data-centric Artificial Intelligence: A Survey - arXiv, https://arxiv.org/abs/2303.10158</li>
<li>Vision-Language-Action Models: Concepts, Progress, Applications and Challenges - arXiv, https://arxiv.org/abs/2505.04769</li>
<li>RT-2: New model translates vision and language into action - Google DeepMind, https://deepmind.google/blog/rt-2-new-model-translates-vision-and-language-into-action/</li>
<li>𝜋₀: A Vision-Language-Action Flow Model for General Robot Control - arXiv, https://arxiv.org/html/2410.24164v1</li>
<li>[2509.24917] From Code to Action: Hierarchical Learning of Diffusion-VLM Policies - arXiv, https://arxiv.org/abs/2509.24917</li>
<li>Vision-Language-Action (VLA) Models: The AI Brain Behind the Next Generation of Robots &amp; Physical AI | by RAKTIM SINGH | Nov, 2025 | Medium, https://medium.com/@raktims2210/vision-language-action-vla-models-the-ai-brain-behind-the-next-generation-of-robots-physical-bced48e8ae94</li>
<li>π 0 : Our First Generalist Policy - Physical Intelligence, https://www.pi.website/blog/pi0</li>
<li>GR00T N1: An Open Foundation Model for Generalist Humanoid Robots - arXiv, https://arxiv.org/abs/2503.14734</li>
<li>Build Synthetic Data Pipelines to Train Smarter Robots with NVIDIA Isaac Sim, https://developer.nvidia.com/blog/build-synthetic-data-pipelines-to-train-smarter-robots-with-nvidia-isaac-sim/</li>
<li>Welcome to Isaac Lab! - GitHub Pages, https://isaac-sim.github.io/IsaacLab/</li>
<li>Research focus: AI that knows what it doesn’t know - Covariant, https://covariant.ai/insights/research-focus-ai-that-knows-what-it-doesn-t-know/</li>
<li>MODEX 2024: Covariant introduces RFM-1 to give robots human-like ability to reason, https://www.robotics247.com/article/modex_2024_covariant_introduces_rfm_1_to_give_robots_human_like_ability_to_reason</li>
<li>Frontier of General-Purpose Robotics (2025): Research, Data, and Evolving Industry Dynamics - writing, https://www.adampatni.com/posts/robotics_deep_dive/</li>
<li>Helix: A Vision-Language-Action Model for Generalist Humanoid Control - Figure AI, https://www.figure.ai/news/helix</li>
<li>Helix Accelerating Real-World Logistics - Figure AI, https://www.figure.ai/news/helix-logistics</li>
<li>[2502.19417] Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models - arXiv, https://arxiv.org/abs/2502.19417</li>
<li>ICML Poster Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models, https://icml.cc/virtual/2025/poster/44202</li>
<li>[2511.02097] A Step Toward World Models: A Survey on Robotic Manipulation - arXiv, https://arxiv.org/abs/2511.02097</li>
<li>World Models Reading List: The Papers You Actually Need in 2025 | by Graison Thomas, https://medium.com/@graison/world-models-reading-list-the-papers-you-actually-need-in-2025-882f02d758a9</li>
<li>1X World Model, https://www.1x.tech/discover/1x-world-model</li>
<li>[2510.07092] Generative World Modelling for Humanoids: 1X World Model Challenge Technical Report - arXiv, https://arxiv.org/abs/2510.07092</li>
<li>1x-technologies/1xgpt: world modeling challenge for humanoid robots - GitHub, https://github.com/1x-technologies/1xgpt</li>
<li>World Model Predictive Control with Multimodal Adaptation: Towards Artificial Dynamics Intelligence for General Robotics | OpenReview, https://openreview.net/forum?id=F56sPNjq48</li>
<li>Understanding World or Predicting Future? A Comprehensive Survey of World Models, https://arxiv.org/html/2411.14499v3</li>
<li>3D Generative Models and Neural Radiance Fields (NeRFs) in 2025 - Medium, https://medium.com/@thekzgroupllc/3d-generative-models-and-neural-radiance-fields-nerfs-in-2025-570614792180</li>
<li>Revolutionizing Neural Reconstruction and Rendering in gsplat with 3DGUT | NVIDIA Technical Blog, https://developer.nvidia.com/blog/revolutionizing-neural-reconstruction-and-rendering-in-gsplat-with-3dgut/</li>
<li>3D-Vision-World/awesome-NeRF-and-3DGS-SLAM - GitHub, https://github.com/3D-Vision-World/awesome-NeRF-and-3DGS-SLAM</li>
<li>Grounding Creativity in Physics: A Brief Survey of Physical Priors in AIGC - IJCAI, https://www.ijcai.org/proceedings/2025/1176.pdf</li>
<li>Differentiable physics-based system identification for robotic manipulation of elastoplastic materials - -ORCA - Cardiff University, https://orca.cardiff.ac.uk/id/eprint/176713/1/yang-et-al-2025-differentiable-physics-based-system-identification-for-robotic-manipulation-of-elastoplastic-materials.pdf</li>
<li>Differentiable Information Enhanced Model-Based Reinforcement Learning, https://ojs.aaai.org/index.php/AAAI/article/download/34419/36574</li>
<li>Waymo and Tesla’s self-driving systems are more similar than people think - Reddit, https://www.reddit.com/r/SelfDrivingCars/comments/1px7vr0/waymo_and_teslas_selfdriving_systems_are_more/</li>
<li>Autonomous Driving Tech: Who’s Actually Winning in 2025? | by Andrew Kennon | Medium, https://medium.com/@nxfsr568/autonomous-driving-tech-whos-actually-winning-in-2025-4d896e37a884</li>
<li>Artificial Intelligence Index Report 2025 | Stanford HAI, https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf</li>
<li>Open X-Embodiment Dataset - Emergent Mind, https://www.emergentmind.com/topics/open-x-embodiment-dataset</li>
<li>DROID: A Large-Scale In-the-Wild Robot Manipulation Dataset, https://droid-dataset.github.io/</li>
<li>Isaac Sim - Robotics Simulation and Synthetic Data Generation - NVIDIA Developer, https://developer.nvidia.com/isaac/sim</li>
<li>Gemini Robotics: Bringing AI into the Physical World - arXiv, https://arxiv.org/html/2503.20020v1</li>
<li>Sim-to-real Transfer: A Comprehensive Guide for 2025 - Shadecoder, https://www.shadecoder.com/hi/topics/sim-to-real-transfer-a-comprehensive-guide-for-2025</li>
<li>Enabling Real-World Object Manipulation with Sim-to-Real Transfer | by Toyota Research Institute - Medium, https://medium.com/toyotaresearch/enabling-real-world-object-manipulation-with-sim-to-real-transfer-20d4962e029</li>
<li>CoRL 2024 Conference | OpenReview, https://openreview.net/group?id=robot-learning.org/CoRL/2024/Conference</li>
<li>Scaling Helix: a New State of the Art in Humanoid Logistics - Figure AI, https://www.figure.ai/news/scaling-helix-logistics</li>
<li>Humanoid robots: Crossing the chasm from concept to commercial reality, https://www.mckinsey.com/industries/industrials/our-insights/humanoid-robots-crossing-the-chasm-from-concept-to-commercial-reality</li>
<li>Mapping the Humanoid Robot Value Chain - Morgan Stanley Financial Advisors, https://advisor.morganstanley.com/john.howard/documents/field/j/jo/john-howard/The_Humanoid_100_-_Mapping_the_Humanoid_Robot_Value_Chain.pdf</li>
<li>AI &amp; Robotics | Tesla, https://www.tesla.com/AI</li>
<li>Elon Musk’s Optimus Gen 3: A Technical Breakdown of the 2025 AI Revolution - Capitaly.vc, https://www.capitaly.vc/blog/elon-musks-optimus-gen-3-a-technical-breakdown-of-the-2025-ai-revolution</li>
<li>Introducing Figure 03 - Figure AI, https://www.figure.ai/news/introducing-figure-03</li>
<li>Figure.AI Archives - The Robot Report, https://www.therobotreport.com/tag/figure-ai/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>