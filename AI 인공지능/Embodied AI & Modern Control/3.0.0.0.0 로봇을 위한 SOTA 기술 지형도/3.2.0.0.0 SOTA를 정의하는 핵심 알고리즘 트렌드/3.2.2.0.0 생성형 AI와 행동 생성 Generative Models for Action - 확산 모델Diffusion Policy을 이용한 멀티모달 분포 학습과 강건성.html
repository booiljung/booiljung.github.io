<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.2.2 생성형 AI와 행동 생성 (Generative Models for Action): 확산 모델(Diffusion Policy)을 이용한 멀티모달 분포 학습과 강건성</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.2.2 생성형 AI와 행동 생성 (Generative Models for Action): 확산 모델(Diffusion Policy)을 이용한 멀티모달 분포 학습과 강건성</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 3. 로봇을 위한 SOTA 기술 지형도</a> / <a href="index.html">3.2 SOTA를 정의하는 핵심 알고리즘 트렌드</a> / <span>3.2.2 생성형 AI와 행동 생성 (Generative Models for Action): 확산 모델(Diffusion Policy)을 이용한 멀티모달 분포 학습과 강건성</span></nav>
                </div>
            </header>
            <article>
                <h1>3.2.2 생성형 AI와 행동 생성 (Generative Models for Action): 확산 모델(Diffusion Policy)을 이용한 멀티모달 분포 학습과 강건성</h1>
<p>현대 로봇 공학 및 인공지능 연구의 흐름은 결정론적(Deterministic) 함수 근사에서 생성형(Generative) 모델링으로 급격히 이동하고 있다. 특히, 이미지 생성 분야에서 혁명적인 성과를 거둔 확산 모델(Diffusion Models)을 로봇의 행동 제어에 접목하려는 시도는, 기존 모방 학습(Imitation Learning)이 겪던 고질적인 문제들을 해결하는 열쇠로 부상했다. 본 장에서는 생성형 AI가 행동 생성(Action Generation)의 패러다임을 어떻게 변화시키고 있는지, 특히 **확산 정책(Diffusion Policy)**의 이론적 배경과 기술적 아키텍처, 그리고 이를 통해 달성되는 멀티모달(Multimodal) 분포 학습의 정밀성과 시스템의 강건성(Robustness)을 심층적으로 분석한다.</p>
<h2>1.  행동 복제의 한계와 생성형 접근의 필연성</h2>
<p>로봇 학습, 특히 시각 운동 정책(Visuomotor Policy) 학습의 가장 기본적인 형태는 행동 복제(Behavioral Cloning, BC)이다. 이는 전문가의 시연 데이터(Demonstration) <span class="math math-inline">\mathcal{D} = {(O_t, A_t)}</span>를 지도 학습(Supervised Learning)하여, 관측 <span class="math math-inline">O_t</span>가 주어졌을 때 행동 <span class="math math-inline">A_t</span>를 예측하는 정책 <span class="math math-inline">\pi(A_t|O_t)</span>를 근사하는 과정이다.</p>
<h3>1.1  명시적 행동 복제(Explicit BC)와 모드 평균화의 오류</h3>
<p>전통적인 명시적 행동 복제는 정책을 결정론적 함수 <span class="math math-inline">A = \pi_\theta(O)</span>로 정의하고, 예측된 행동과 전문가 행동 간의 평균 제곱 오차(Mean Squared Error, MSE)를 최소화하는 방향으로 학습한다.1<br />
<span class="math math-display">
\mathcal{L}_{MSE} = \| \pi_\theta(O_t) - A_t \|^2
</span><br />
이러한 접근법은 데이터가 단봉(Unimodal) 분포, 즉 특정 상황에서 정답이 하나만 존재하는 경우에는 효과적이다. 그러나 현실 세계의 데이터, 특히 인간의 시연 데이터는 본질적으로 **멀티모달리티(Multimodality)**를 내포한다. 예를 들어, 로봇이 장애물을 마주했을 때 왼쪽으로 피하는 것과 오른쪽으로 피하는 것 모두 유효한 행동이다. 이때 MSE 기반의 학습은 두 유효한 행동의 산술 평균, 즉 ’장애물을 향해 직진’하는 행동을 최적해로 학습하게 된다. 이를 모드 평균화(Mode Averaging) 현상이라 하며, 이는 로봇이 충돌하거나 아무런 의미 없는 행동을 하게 만드는 주된 원인이다.2</p>
<h3>1.2  기존 대안 모델들의 한계</h3>
<p>멀티모달 분포를 다루기 위해 혼합 밀도 네트워크(Mixture Density Networks, MDN)나 LSTM-GMM과 같은 확률적 접근이 시도되었다. 이들은 행동 분포를 여러 가우시안 분포의 가중 합으로 모델링하여 멀티모달리티를 표현하려 했으나, 고차원 행동 공간(High-dimensional Action Space)에서 학습이 불안정하고 특정 모드에 편향되는 경향을 보였다.4 또한, 에너지 기반 모델(Energy-Based Models, EBM)을 활용한 암시적 행동 복제(Implicit Behavioral Cloning, IBC)는 <span class="math math-inline">A = \arg\min_a E_\theta(O, a)</span> 형태로 행동을 생성하여 불연속적인 분포를 표현할 수 있었으나, 훈련 과정에서 분할 함수(Partition Function) 추정의 난해함(Intractability)과 추론 시 최적화 과정의 높은 비용 문제로 인해 실시간 제어에 한계를 드러냈다.4</p>
<p>이러한 맥락에서 등장한 **확산 정책(Diffusion Policy)**은 행동 생성을 조건부 디노이징 확산 과정(Conditional Denoising Diffusion Process)으로 재정의함으로써, 명시적인 분포 가정 없이도 복잡한 행동 분포를 정밀하게 학습하고, 높은 차원의 연속적인 행동 공간에서도 안정적인 성능을 입증하며 새로운 표준(State-of-the-art)을 정립하였다.2</p>
<h2>2.  확산 정책(Diffusion Policy)의 이론적 정식화</h2>
<p>확산 정책은 이미지 생성에서 사용되는 DDPM(Denoising Diffusion Probabilistic Models)을 로봇의 시각 운동 제어 문제에 맞게 변형한 것이다. 핵심 아이디어는 행동 데이터에 노이즈를 점진적으로 주입하여 가우시안 분포로 만드는 순방향 과정(Forward Process)을 정의하고, 이를 역으로 수행하여 노이즈로부터 유효한 행동을 복원하는 역방향 과정(Reverse Process)을 학습하는 것이다.</p>
<h3>2.1  조건부 디노이징 확산 과정 (Conditional Denoising Diffusion Process)</h3>
<p>확산 정책에서 정책 <span class="math math-inline">\pi_\theta</span>는 관측 <span class="math math-inline">O_t</span>를 조건(Condition)으로 받아 <span class="math math-inline">K</span>번의 반복적인 디노이징 단계를 거쳐 행동 <span class="math math-inline">A_t</span>를 생성한다. 초기 상태 <span class="math math-inline">x_K</span>는 표준 정규 분포 <span class="math math-inline">\mathcal{N}(0, I)</span>에서 샘플링된 순수 노이즈이며, 역방향 과정은 다음과 같은 마르코프 체인(Markov Chain)으로 정의된다.3<br />
<span class="math math-display">
x_{k-1} = \alpha_k (x_k - \gamma_k \epsilon_\theta(x_k, k, O_t) + \mathcal{N}(0, \sigma_k^2 I))
</span><br />
여기서 각 변수의 의미는 다음과 같다.</p>
<ul>
<li><strong><span class="math math-inline">x_k</span></strong>: 확산 단계 <span class="math math-inline">k</span>에서의 행동 시퀀스 상태 (노이즈가 포함된 잠재 변수).</li>
<li><strong><span class="math math-inline">\epsilon_\theta(x_k, k, O_t)</span></strong>: 노이즈 예측 신경망(Noise Prediction Network). 현재의 노이즈 상태 <span class="math math-inline">x_k</span>, 확산 단계 <span class="math math-inline">k</span>, 그리고 관측 <span class="math math-inline">O_t</span>를 입력으로 받아 <span class="math math-inline">x_k</span>에 추가된 노이즈를 예측한다.</li>
<li><strong><span class="math math-inline">\alpha_k, \gamma_k, \sigma_k</span></strong>: 노이즈 스케줄(Noise Schedule)에 의해 결정되는 계수들이다. <span class="math math-inline">\alpha_k</span>는 데이터의 스케일을 조절하고, <span class="math math-inline">\sigma_k</span>는 확률적 탐색을 위한 노이즈의 분산을 결정한다.</li>
</ul>
<p>이 과정은 수학적으로 **확률적 랑주뱅 역학(Stochastic Langevin Dynamics)**을 이용한 경사 하강법과 유사하다. 신경망 <span class="math math-inline">\epsilon_\theta</span>는 사실상 조건부 행동 분포의 로그 밀도 함수인 스코어 함수(Score Function) <span class="math math-inline">\nabla_{x} \log p(x | O_t)</span>를 추정하는 역할을 수행한다. 즉, 확산 과정은 에너지 지형(Energy Landscape) 상에서 높은 확률 밀도를 가진 지점(유효한 행동)으로 샘플을 이동시키는 반복적인 최적화 과정으로 해석될 수 있다.2</p>
<h3>2.2  학습 목표 및 손실 함수</h3>
<p>확산 정책의 학습은 복잡한 적분이나 역전파가 필요 없는 간단한 지도 학습 문제로 귀결된다. 학습 데이터셋에서 샘플링한 원본 행동 시퀀스 <span class="math math-inline">x_0</span>에 임의의 타임스텝 <span class="math math-inline">k</span>에 해당하는 가우시안 노이즈 <span class="math math-inline">\epsilon</span>을 섞어 <span class="math math-inline">x_k</span>를 생성한다. 신경망은 이 섞인 노이즈 <span class="math math-inline">\epsilon</span>을 예측하도록 훈련된다.1<br />
<span class="math math-display">
\mathcal{L} = \mathbb{E}_{k, x_0, \epsilon} [ \| \epsilon - \epsilon_\theta(x_k, k, O_t) \|^2 ]
</span><br />
여기서 <span class="math math-inline">x_k = \sqrt{\bar{\alpha}_k} x_0 + \sqrt{1 - \bar{\alpha}_k} \epsilon</span> 이다. (<span class="math math-inline">\bar{\alpha}_k</span>는 누적 노이즈 스케줄 계수).</p>
<p>이 단순한 MSE 손실 함수는 변분 하한(Variational Lower Bound, ELBO)을 최소화하는 것과 수학적으로 동치임이 증명되어 있다. 이는 확산 정책이 별도의 정규화 상수를 계산하거나 복잡한 적대적 학습(Adversarial Training) 없이도 전체 데이터 분포를 안정적으로 학습할 수 있음을 의미한다.3</p>
<h2>3.  아키텍처 및 핵심 기술 메커니즘</h2>
<p>확산 정책이 로봇 제어에서 탁월한 성과를 낼 수 있었던 배경에는 단순히 생성 모델을 차용한 것을 넘어, 로봇 데이터의 특성을 고려한 아키텍처 설계와 제어 전략의 통합이 있었다. Chi et al. (2023)은 이를 위해 <strong>후퇴적 지평 제어</strong>, <strong>시각적 조건화</strong>, 그리고 <strong>행동 표현 방식</strong>의 세 가지 측면에서 중요한 기술적 기여를 제시했다.3</p>
<h3>3.1  후퇴적 지평 제어 (Receding Horizon Control)</h3>
<p>확산 정책은 매 순간 단 하나의 행동(예: 현재 시점의 관절 속도)만을 예측하는 기존 방식과 달리, 미래의 행동 시퀀스 전체를 예측하는 전략을 취한다. 이는 시간적 일관성(Temporal Consistency)을 확보하고 근시안적인(Myopic) 계획을 방지하는 데 필수적이다. 이를 구현하기 위해 다음과 같은 지평(Horizon) 파라미터들이 정의된다.4</p>
<ul>
<li><strong>관측 지평 (Observation Horizon, <span class="math math-inline">T_o</span>):</strong> 정책이 참조하는 과거 관측 데이터의 길이이다. 일반적으로 <span class="math math-inline">T_o=2</span>를 사용하여 현재 상태와 직전 상태를 함께 입력받음으로써 속도와 가속도 정보를 암시적으로 추론하게 한다.</li>
<li><strong>예측 지평 (Prediction Horizon, <span class="math math-inline">T_p</span>):</strong> 모델이 한 번의 추론으로 생성하는 미래 행동 시퀀스의 길이이다. 예를 들어 <span class="math math-inline">T_p=16</span>으로 설정하면, 현재 시점부터 미래 16스텝까지의 행동 궤적을 생성한다. 이는 급격한 행동 변화(Jittering)를 억제하고 부드러운 움직임을 생성하는 핵심 요인이다.</li>
<li><strong>실행 지평 (Execution Horizon, <span class="math math-inline">T_a</span>):</strong> 예측된 <span class="math math-inline">T_p</span> 길이의 시퀀스 중 실제로 로봇이 수행하는 행동의 길이이다. 확산 정책에서는 보통 <span class="math math-inline">T_a &lt; T_p</span> (예: <span class="math math-inline">T_a=8</span>)로 설정한다.</li>
</ul>
<p><strong>후퇴적 지평 제어</strong>의 메커니즘은 다음과 같다: <span class="math math-inline">T_p</span> 길이의 행동을 예측한 후, 앞부분의 <span class="math math-inline">T_a</span>만큼만 실행한다. 그리고 다시 새로운 관측을 받아 전체 궤적을 재계획(Re-planning)한다. 이 방식은 예측 오차를 조기에 수정하고, 환경의 예기치 않은 변화(예: 사람이 물체를 움직임)에 대해 즉각적으로 반응할 수 있는 폐루프(Closed-loop) 제어의 이점을 제공한다.4 긴 예측 지평은 행동의 일관성을, 짧은 실행 지평은 반응성(Responsiveness)을 보장하는 상호보완적인 관계를 형성한다.10</p>
<h3>3.2  신경망 백본과 시각적 조건화 메커니즘</h3>
<p>확산 정책은 고차원 이미지 입력을 효과적으로 조건화(Conditioning)하기 위해 CNN 기반과 Transformer 기반의 두 가지 주요 아키텍처를 제안한다. 각 아키텍처는 시각 정보를 행동 생성 과정에 주입하는 방식에서 차이를 보인다.3</p>
<h4>3.2.1  CNN 기반 확산 정책과 FiLM</h4>
<p>CNN 기반 모델은 1D Temporal U-Net 구조를 채택하여 시간적 차원에서 행동 시퀀스를 처리한다. 이때 시각적 조건화에는 FiLM (Feature-wise Linear Modulation) 기법이 사용된다. FiLM은 시각 인코더(예: ResNet-18)에서 추출된 특징 벡터를 사용하여, U-Net의 각 컨볼루션 계층(ResBlock)의 특징 맵(Feature Map)에 대해 채널별로 아핀 변환(Affine Transformation)을 적용한다.<br />
<span class="math math-display">
\text{FiLM}(h, \gamma(O), \beta(O)) = \gamma(O) \cdot h + \beta(O)
</span><br />
여기서 <span class="math math-inline">h</span>는 중간 특징 맵, <span class="math math-inline">\gamma</span>와 <span class="math math-inline">\beta</span>는 관측 <span class="math math-inline">O</span>로부터 학습된 스케일(Scale)과 시프트(Shift) 파라미터이다. FiLM은 적은 연산량으로도 시각 정보가 행동 생성의 모든 단계에 강력하게 영향을 미치도록 하며, 특히 저주파수 제어 신호와 고주파수 세부 동작을 모두 포착하는 데 효과적이다.2</p>
<h4>3.2.2  Transformer 기반 확산 정책과 Cross-Attention</h4>
<p>Transformer 기반 모델은 행동 시퀀스와 관측 정보를 토큰(Token)화하여 처리한다. 이 구조에서는 교차 어텐션(Cross-Attention) 메커니즘이 핵심 역할을 수행한다. 행동 토큰이 쿼리(Query)로, 시각적 관측 토큰이 키(Key)와 밸류(Value)로 작용하여, 각 디노이징 단계에서 행동 생성에 필요한 시각 정보에 동적으로 주의(Attend)를 기울인다.<br />
<span class="math math-display">
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
</span><br />
이 방식은 긴 시퀀스 간의 의존성을 모델링하는 데 유리하지만, CNN 기반 모델에 비해 학습 데이터가 적을 때 과적합(Overfitting)되거나 학습이 까다로울 수 있다는 한계가 보고되기도 했다.13 그러나 대규모 데이터셋에서는 더 높은 확장성을 보여준다.</p>
<h3>3.3  행동 표현과 공간의 선택</h3>
<p>행동 공간(Action Space)의 선택은 확산 정책의 성능에 지대한 영향을 미친다. 연구 결과에 따르면, <strong>위치 제어(Position Control)</strong> 방식이 속도 제어(Velocity Control)보다 확산 모델과 더 잘 부합하며, 더 높은 정밀도를 보인다.11 이는 위치 정보가 행동의 모드(Mode)를 더 명확하게 구분해 주기 때문이다. 또한, 대부분의 조작 작업에서 <strong>말단 장치 포즈(End-effector Pose)</strong> 공간에서의 학습이 관절(Joint) 공간보다 일반화 성능이 우수한 것으로 나타났다. 다만, 전신 제어(Whole-body Control)나 복잡한 충돌 회피가 필요한 경우 관절 공간 혹은 3D 키포인트 기반의 표현이 필수적일 수 있다.14</p>
<p>행동 데이터의 전처리는 확산 과정의 안정성을 위해 필수적이다. 모든 행동 데이터는 <span class="math math-inline">[-1, 1]</span> 범위로 정규화(Normalization)되어야 하며, 일반적으로 델타 포즈(Delta Pose)보다는 절대 포즈(Absolute Pose)를 사용하는 것이 장기적인 일관성 유지에 유리하다.16</p>
<h2>4.  멀티모달 분포 학습과 시스템 강건성 분석</h2>
<p>확산 정책의 가장 혁신적인 기여는 멀티모달 분포를 명시적으로, 그리고 정확하게 모델링할 수 있다는 점과, 이를 통해 실제 환경의 불확실성에 대해 강력한 강건성을 확보했다는 점이다.</p>
<h3>4.1  멀티모달리티의 보존과 모드 붕괴 방지</h3>
<p>확산 정책은 확률적 생성 모델로서, 훈련 데이터에 존재하는 여러 행동 모드를 ’평균화’하지 않고 모두 보존한다. 추론 시에는 무작위 노이즈 초기값에 따라 학습된 여러 모드 중 하나로 자연스럽게 수렴하게 된다.</p>
<p>4와 4의 비교 실험에 따르면, LSTM-GMM이나 IBC는 훈련 과정에서 특정 모드에 과도하게 편향되거나(Mode Collapse), 두 모드 사이를 오가는 불안정한 거동을 보였다. 반면 확산 정책은 각 롤아웃(Rollout) 내에서 하나의 일관된 모드를 선택하여 수행함이 확인되었다. 예를 들어 ‘Push-T’ 과제에서 T자형 블록을 왼쪽이나 오른쪽 중 한 방향으로 밀어야 할 때, 확산 정책은 명확하게 한쪽 방향을 선택하여 끝까지 수행하는 반면, 다른 모델들은 중간에서 방향을 바꾸거나 주저하는 모습을 보였다.</p>
<h3>4.2  고차원 행동 공간에서의 학습 안정성</h3>
<p>기존의 내재적 행동 복제(IBC)와 같은 에너지 기반 모델은 고차원 공간에서 분할 함수를 추정하기 위해 네거티브 샘플링(Negative Sampling)이 필요했다. 이는 행동 공간의 차원이 커질수록 기하급수적으로 어려워지며 학습 불안정을 초래한다.6 확산 정책은 점수 매칭(Score Matching) 기반의 학습을 통해 이러한 정규화 상수 계산 문제를 우회한다. 따라서 7자유도 로봇 팔과 그리퍼를 포함한 고차원 행동 공간에서도 별도의 하이퍼파라미터 튜닝 없이 안정적인 학습이 가능하다.3</p>
<h3>4.3  외란 및 시각적 방해에 대한 강건성</h3>
<p>확산 정책의 강건성은 두 가지 요인에서 기인한다. 첫째, <strong>후퇴적 지평 제어</strong>를 통한 빈번한 재계획이다. 로봇이 행동을 수행하는 도중 외부 힘에 의해 위치가 바뀌거나 목표 물체가 이동하더라도, 다음 추론 단계에서 즉시 변경된 관측을 반영하여 새로운 궤적을 생성한다. 둘째, <strong>디노이징 과정 자체의 내성</strong>이다. 확산 모델은 본질적으로 노이즈가 섞인 데이터를 복원하도록 훈련되었기 때문에, 센서 노이즈나 부분적인 가림(Occlusion), 조명 변화와 같은 시각적 외란(Visual Distraction)에 대해서도 강인한 성능을 보인다.4 실제 실험에서 확산 정책은 카메라 앞을 손으로 가리거나 로봇을 밀치는 강한 외란 상황에서도 작업을 완수하는 높은 성공률을 기록했다.3</p>
<h2>5.  비교 분석 및 성능 벤치마크</h2>
<p>확산 정책의 우수성을 입증하기 위해, 주요 모방 학습 알고리즘들과의 체계적인 비교 분석을 수행한다. 다음은 1의 연구 결과를 종합한 비교표이다.</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>Explicit BC (MSE)</strong></th><th><strong>LSTM-GMM</strong></th><th><strong>Implicit BC (IBC)</strong></th><th><strong>Diffusion Policy</strong></th></tr></thead><tbody>
<tr><td><strong>분포 모델링</strong></td><td>유니모달 (평균값 수렴)</td><td>멀티모달 (제한적)</td><td>멀티모달 (유연함)</td><td><strong>멀티모달 (매우 유연함, 임의 분포)</strong></td></tr>
<tr><td><strong>고차원 확장성</strong></td><td>우수함</td><td>낮음</td><td>낮음 (학습 불안정)</td><td><strong>우수함 (이미지 생성 수준의 확장성)</strong></td></tr>
<tr><td><strong>학습 안정성</strong></td><td>매우 안정적</td><td>보통</td><td>불안정 (발산 가능성)</td><td><strong>안정적 (Score Matching)</strong></td></tr>
<tr><td><strong>시간적 일관성</strong></td><td>낮음 (고주파수 떨림 발생)</td><td>보통</td><td>보통</td><td><strong>매우 높음 (시퀀스 단위 예측)</strong></td></tr>
<tr><td><strong>추론 속도</strong></td><td>매우 빠름 (&gt;1kHz)</td><td>빠름</td><td>느림 (최적화 필요)</td><td><strong>느림 (반복적 디노이징 필요)</strong></td></tr>
<tr><td><strong>샘플 효율성</strong></td><td>보통</td><td>낮음</td><td>낮음</td><td><strong>높음</strong></td></tr>
<tr><td><strong>강건성</strong></td><td>낮음 (오차 누적 심각)</td><td>낮음</td><td>보통</td><td><strong>매우 높음 (Re-planning &amp; Denoising)</strong></td></tr>
</tbody></table>
<p>데이터에 따르면, 확산 정책은 추론 속도를 제외한 거의 모든 성능 지표에서 기존 방법론을 평균 46.9% 이상 상회하는 결과를 보여주었다.3 추론 속도의 문제는 DDIM(Denoising Diffusion Implicit Models)과 같은 샘플링 가속 기법이나, 최근 제안된 One-step Diffusion Policy 등을 통해 빠르게 개선되고 있는 추세이다.5</p>
<h2>6.  최신 확장 연구 및 발전 방향</h2>
<p>확산 정책의 기본 프레임워크를 넘어, 더욱 고도화된 기능과 효율성을 달성하기 위한 파생 연구들이 활발히 진행되고 있다.</p>
<h3>6.1  일관성 증류와 고속 추론 (Consistency Distillation)</h3>
<p>확산 모델의 느린 추론 속도를 극복하기 위해, 수십 단계의 디노이징 과정을 단 한 번, 혹은 아주 적은 횟수의 단계로 압축하는 연구가 진행 중이다. <strong>One-step Diffusion Policy</strong>나 <strong>Consistency Distillation</strong> 기법은 교사 모델(Teacher Model)의 지식을 학생 모델(Student Model)로 증류하여, 성능 저하를 최소화하면서 추론 속도를 수십 배 향상시킨다.5</p>
<h3>6.2  상태 인식과 계층적 제어 (Statefulness &amp; Hierarchy)</h3>
<p>기본적인 확산 정책은 현재의 관측만을 바탕으로 행동을 생성하는 무상태(Stateless) 정책에 가깝다. <strong>Diff-Control</strong>과 같은 연구는 이전 단계의 행동 정보를 명시적으로 조건화하여 상태 인식(Statefulness)을 강화함으로써, 동적인 작업에서의 성능을 높였다.18 또한, **Hierarchical Diffusion Policy (HDP)**는 상위 레벨에서 목표(Goal)나 중간 지점(Waypoint)을 생성하고, 하위 레벨에서 이를 추종하는 궤적을 생성하는 계층적 구조를 통해 장기 계획(Long-horizon Planning) 능력을 획기적으로 개선하였다.19</p>
<h3>6.3  3D 기구학 인식 (Kinematics Awareness)</h3>
<p>단순한 말단 장치 제어를 넘어, 로봇 팔 전체의 충돌을 방지하고 기구학적 제약을 준수하기 위해 3D 포인트 클라우드와 기구학 정보를 통합하는 연구도 등장했다. <strong>Kinematics-Aware Diffusion Policy</strong>는 로봇의 링크(Link) 정보를 3D 공간상에 투영하여 학습함으로써, 좁은 공간에서의 조작이나 전신 제어 작업에서 물리적 타당성(Feasibility)을 보장한다.14</p>
<h2>7.  결론</h2>
<p>확산 정책(Diffusion Policy)은 로봇 행동 생성 분야의 패러다임을 근본적으로 변화시켰다. 행동을 단순한 입력-출력 매핑이 아닌, 조건부 생성 과정으로 재해석함으로써 멀티모달 분포를 자연스럽게 포착하고, 시간적 일관성을 갖춘 고품질의 궤적을 생성할 수 있게 되었다. 특히 후퇴적 지평 제어와 시각적 조건화 기법의 유기적 결합은 시뮬레이션뿐만 아니라 실제 로봇 환경(Real-world)에서의 복잡한 조작 작업에서도 탁월한 성능과 강건성을 발휘하게 했다.</p>
<p>비록 반복적인 디노이징 과정으로 인한 계산 비용이 존재하지만, 이는 하드웨어의 발전과 알고리즘의 최적화(DDIM, Distillation 등)를 통해 빠르게 극복되고 있다. 확산 정책은 단순한 모방 학습을 넘어, 대규모 데이터셋을 활용한 범용 로봇 기초 모델(Robot Foundation Models) 구축의 핵심 알고리즘으로 자리 잡고 있으며, 향후 비전-언어-행동(Vision-Language-Action, VLA) 모델과의 융합을 통해 로봇의 인지 및 제어 능력을 비약적으로 향상시킬 것으로 전망된다. 따라서 차세대 지능형 로봇 시스템을 설계하는 연구자와 엔지니어에게 확산 정책의 원리와 구현 방식을 이해하는 것은 선택이 아닌 필수가 되었다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>STATISTICAL COMPARISON OF DIFFUSION POLICIES AND …, http://www.diva-portal.org/smash/get/diva2:1978992/FULLTEXT01.pdf</li>
<li>Visuomotor Policy Learning via Action Diffusion - Steven Gong, https://stevengong.co/research-papers/Diffusion-Policy-Visuomotor-Policy-Learning-via-Action-Diffusion</li>
<li>Visuomotor Policy Learning via Action Diffusion, https://diffusion-policy.cs.columbia.edu/diffusion_policy_2023.pdf</li>
<li>Diffusion Policy, https://diffusion-policy.cs.columbia.edu/</li>
<li>Visuomotor policy learning via action diffusion - Semantic Scholar, https://www.semanticscholar.org/paper/Diffusion-policy%3A-Visuomotor-policy-learning-via-Chi-Feng/bdba3bd30a49ea4c5b20b43dbd8f0eb59e9d80e2</li>
<li>Diffusion Policy: - Robotics, https://www.roboticsproceedings.org/rss19/p026.pdf</li>
<li>Visuomotor Policy Learning via Action Diffusion - arXiv, https://arxiv.org/html/2303.04137v5</li>
<li>Diffusion Policy: Visuomotor Policy Learning via Action Diffusion, https://arxiv.org/pdf/2303.04137</li>
<li>State-Action Guided Diffusion Policy - Emergent Mind, https://www.emergentmind.com/topics/state-action-guided-diffusion-policy</li>
<li>Unpacking the Individual Components of Diffusion Policy - arXiv, https://arxiv.org/html/2412.00084v1</li>
<li>Visuomotor Policy Learning via Action Diffusion | by Yutong Zhang, https://medium.com/correll-lab/diffusion-policy-applying-diffusion-model-to-policy-learning-8ee16fc056bb</li>
<li>Different ways to condition the diffusion model - Jonghwa Yim, https://jonhwayim.medium.com/different-ways-to-condition-the-diffusion-model-d2e41eaa01c2</li>
<li>Scaling Diffusion Policy in Transformer to 1 Billion Parameters for …, https://arxiv.org/html/2409.14411v2</li>
<li>Kinematics-Aware Diffusion Policy with Consistent 3D Observation …, https://arxiv.org/html/2512.17568</li>
<li>Kinematics-Aware Diffusion Policy with Consistent 3D Observation …, https://www.researchgate.net/publication/398936740_Kinematics-Aware_Diffusion_Policy_with_Consistent_3D_Observation_and_Action_Space_for_Whole-Arm_Robotic_Manipulation</li>
<li>Diffusion Policy — RoboVerse 0.1.0 documentation, https://roboverse.wiki/roboverse_learn/imitation_learning/diffusion_policy</li>
<li>(PDF) Responsive Noise-Relaying Diffusion Policy - ResearchGate, https://www.researchgate.net/publication/389129843_Responsive_Noise-Relaying_Diffusion_Policy_Responsive_and_Efficient_Visuomotor_Control</li>
<li>A Stateful Diffusion-based Policy for Imitation Learning - Diff-Control, https://diff-control.github.io/static/videos/Diff-Control.pdf</li>
<li>Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task …, https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_Hierarchical_Diffusion_Policy_for_Kinematics-Aware_Multi-Task_Robotic_Manipulation_CVPR_2024_paper.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>