<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:3.2.3 표현 학습의 진화 (Evolution of Representation): 2D CNN을 넘어, 3D 가우시안 스플래팅(Gaussian Splatting)과 암시적 표현(Implicit Representation)으로</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>3.2.3 표현 학습의 진화 (Evolution of Representation): 2D CNN을 넘어, 3D 가우시안 스플래팅(Gaussian Splatting)과 암시적 표현(Implicit Representation)으로</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 3. 로봇을 위한 SOTA 기술 지형도</a> / <a href="index.html">3.2 SOTA를 정의하는 핵심 알고리즘 트렌드</a> / <span>3.2.3 표현 학습의 진화 (Evolution of Representation): 2D CNN을 넘어, 3D 가우시안 스플래팅(Gaussian Splatting)과 암시적 표현(Implicit Representation)으로</span></nav>
                </div>
            </header>
            <article>
                <h1>3.2.3 표현 학습의 진화 (Evolution of Representation): 2D CNN을 넘어, 3D 가우시안 스플래팅(Gaussian Splatting)과 암시적 표현(Implicit Representation)으로</h1>
<h2>1.  서론: 신체성(Embodiment)을 위한 시각 지능의 재정의</h2>
<p>로봇 공학에서 ’본다’는 행위는 단순히 이미지를 분류하거나 객체의 경계를 탐지하는 정적인 작업을 넘어선다. 로봇은 물리적 신체(Embodiment)를 가진 에이전트로서, 3차원 공간을 점유하고, 이동하며, 환경과 물리적으로 상호작용해야 한다. 따라서 로봇을 위한 인공지능, 즉 ’구현된 인공지능(Embodied AI)’의 핵심은 기계가 물리적 세계를 내부적으로 어떻게 표상(Representation)하느냐에 달려 있다.</p>
<p>지난 10여 년간 컴퓨터 비전 분야는 2D 이미지 기반의 합성곱 신경망(Convolutional Neural Network, CNN)과 최근의 비전 트랜스포머(Vision Transformer, ViT)를 통해 비약적인 발전을 이루었다. 그러나 이러한 2D 중심의 표현 방식은 3차원 공간의 깊이(Depth), 기하학적 구조(Geometry), 그리고 동적 변화(Dynamics)를 온전히 담아내지 못한다는 근본적인 한계에 봉착했다.1 로봇이 복잡한 비정형 환경(Unstructured Environment)에서 안전하게 주행하고 정교하게 물체를 조작하기 위해서는 2D 픽셀 평면을 넘어선 새로운 차원의 표현 학습이 필수적이다.</p>
<p>본 장에서는 2D CNN 기반 표현 학습이 로봇 공학적 맥락에서 갖는 구조적 한계를 심층 분석하고, 이를 극복하기 위해 등장한 신경 방사장(Neural Radiance Fields, NeRF)과 같은 암시적 표현(Implicit Representation)의 기술적 진보를 추적한다. 나아가, 2023년 이후 로봇 인식 기술의 패러다임을 송두리째 바꾸고 있는 3D 가우시안 스플래팅(3D Gaussian Splatting, 3DGS)의 수학적 원리와 이를 활용한 최신 SLAM(Simultaneous Localization and Mapping), 내비게이션, 조작(Manipulation) 기술의 현황을 포괄적으로 기술한다. 이는 단순한 시각화를 넘어, 로봇이 공간을 이해하고 기억하며 예측하는 방식의 진화를 의미한다.</p>
<h2>2.  2D 중심 시각 표현의 구조적 한계와 3D 공간 이해의 필요성</h2>
<h3>2.1  2D CNN의 변환 불변성(Translation Invariance)과 공간 정보의 소실</h3>
<p>합성곱 신경망(CNN)은 이미지 내의 객체가 어디에 위치하든 동일한 객체로 인식하도록 설계된 ’변환 불변성’을 핵심 특징으로 한다. 이는 풀링(Pooling) 레이어와 스트라이드(Stride) 연산을 통해 달성되는데, 이 과정에서 정확한 공간적 위치 정보(Spatial Information)가 의도적으로 제거되거나 뭉개진다.2</p>
<p>그러나 로봇 공학, 특히 정밀한 조작이나 경로 계획이 필요한 작업에서 위치 정보의 소실은 치명적이다.</p>
<ul>
<li><strong>의료 영상 분할이나 로봇 수술:</strong> 종양의 위치나 절개 부위의 정확한 좌표가 생명과 직결되는 작업에서 풀링으로 인한 해상도 저하는 허용될 수 없다.2</li>
<li><strong>공간적 의존성(Spatial Dependencies)의 무시:</strong> CNN은 국소적인 수용 영역(Receptive Field)에 집중하는 경향이 있어, 이미지 전체에 걸친 전역적인(Global) 공간 관계나 거리가 먼 객체 간의 상호작용을 파악하는 데 어려움을 겪는다.2 예를 들어, 로봇이 “식탁 위의 컵“을 인식할 때, 식탁의 전체적인 평면 구조와 컵의 상대적 위치를 3차원적으로 파악하지 못하면, 컵을 잡으려다 식탁을 치는 충돌이 발생할 수 있다.</li>
</ul>
<p>또한, 2D CNN 기반 모델은 각 프레임을 독립적으로 처리하는 경향이 강해 시간적 연속성(Temporal Continuity)을 잃기 쉽다. 이는 교통 흐름 예측이나 군중 이동 예측과 같이 시공간적(Spatio-temporal) 상관관계를 모델링해야 하는 작업에서 한계를 드러낸다.3</p>
<h3>2.2  뷰 일관성(View-Consistency)의 부재와 일반화 문제</h3>
<p>로봇 에이전트는 고정된 카메라가 아니라 이동하는 주체다. 로봇이 이동함에 따라 동일한 객체는 시점(Viewpoint)에 따라 전혀 다른 2D 이미지로 투영된다.</p>
<ul>
<li><strong>회전 및 스케일 변화:</strong> 2D CNN은 훈련 데이터에 포함되지 않은 회전된 각도나 스케일의 변화에 대해 취약하다. 이를 극복하기 위해 데이터 증강(Data Augmentation)을 사용하지만, 이는 근본적인 3D 구조 이해가 아닌 데이터 기반의 암기에 가깝다.2</li>
<li><strong>3D 기하학과의 단절:</strong> 기존의 비전 중심(Vision-Centric) 방법론은 2D 의미론적 특징(Semantic Feature) 추출에는 탁월하지만, 3D 공간 관계를 포착하는 데 실패한다.1 로봇이 “보트“와 “물“의 관계를 2D 이미지 상의 픽셀 인접성으로만 학습한다면, 보트가 물 위에 떠 있는 3D 물리적 관계나 부력과 같은 역학적 속성을 추론할 수 없다. 이는 로봇이 낯선 환경에서 인과관계를 추론하고 작업을 수행하는 데 필요한 ‘일반화(Generalization)’ 능력을 저해한다.</li>
</ul>
<p>실제로 대규모 비전-언어 모델(VLM)인 InternViT-6B와 같은 모델조차도, 2D 이미지 이해 능력은 뛰어나지만 3D 공간 추론이 필요한 로봇 조작 작업(예: Franka Kitchen 벤치마크)에서는 작은 모델보다 성능이 떨어지거나 이점을 보이지 못하는 경우가 관찰된다.1 이는 단순히 모델의 크기를 키우는 것만으로는 3D 공간 지능을 달성할 수 없음을 시사한다.</p>
<h3>2.3  구현된 인공지능을 위한 표현 학습의 요구사항</h3>
<p>따라서 로봇을 위한 차세대 표현 학습은 다음과 같은 속성을 만족해야 한다.</p>
<ol>
<li><strong>3D 공간 인식 (3D Awareness):</strong> 2D 투영 이전의 3D 기하학적 구조를 직접적으로 모델링해야 한다.</li>
<li><strong>연속적 표현 (Continuous Representation):</strong> 이산적인 픽셀이나 복셀(Voxel)의 해상도 제약을 넘어, 임의의 해상도로 쿼리 가능한 연속 함수 형태여야 한다.</li>
<li><strong>의미와 기하학의 융합 (Semantic-Geometric Fusion):</strong> 객체의 형상뿐만 아니라 그 기능적 의미(Affordance)를 함께 인코딩해야 한다.</li>
<li><strong>동적 적응성 (Dynamic Adaptability):</strong> 환경의 변화를 실시간으로 반영하고 업데이트할 수 있어야 한다.</li>
</ol>
<p>이러한 요구에 부응하여 등장한 첫 번째 혁신이 바로 ’암시적 표현(Implicit Representation)’이다.</p>
<h2>3.  암시적 표현(Implicit Representation)의 부상: NeRF와 SDF</h2>
<p>데이터를 3D 격자(Voxel)나 포인트 클라우드(Point Cloud)와 같이 명시적인 좌표의 집합으로 저장하는 방식을 ’명시적 표현(Explicit Representation)’이라 한다. 반면, ’암시적 표현’은 3D 형상과 색상 정보를 신경망의 가중치(Weight) 내부에 연속적인 함수 형태로 저장하는 방식이다. 이는 메모리 효율성과 표현의 연속성 측면에서 로봇 공학에 새로운 가능성을 열었다.</p>
<h3>3.1  신경 방사장(Neural Radiance Fields, NeRF)의 원리</h3>
<p>2020년 Mildenhall 등이 제안한 NeRF는 3D 장면을 5차원 좌표 입력을 받는 함수로 정의하여 혁신을 일으켰다.4</p>
<h4>3.1.1  수학적 정식화</h4>
<p>NeRF는 공간 좌표 <span class="math math-inline">\mathbf{x} = (x, y, z)</span>와 보는 방향 <span class="math math-inline">\mathbf{d} = (\theta, \phi)</span>를 입력으로 받아, 해당 위치의 부피 밀도(Volume Density, <span class="math math-inline">\sigma</span>)와 시점 의존적 색상(View-dependent Color, <span class="math math-inline">\mathbf{c} = (r, g, b)</span>)을 출력하는 다층 퍼셉트론(MLP) <span class="math math-inline">F_{\Theta}</span>로 모델링된다.4<br />
<span class="math math-display">
F_{\Theta} : (\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma)
</span><br />
이 함수는 미분 가능한 볼륨 렌더링(Differentiable Volume Rendering) 방정식을 통해 2D 이미지로 합성된다. 카메라 광선 <span class="math math-inline">\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}</span> 상의 <span class="math math-inline">N</span>개 샘플 포인트에 대해, 예상되는 픽셀 색상 <span class="math math-inline">\hat{C}(\mathbf{r})</span>은 다음과 같이 적분된다.4<br />
<span class="math math-display">
\hat{C}(\mathbf{r}) = \sum_{i=1}^{N} T_i (1 - \exp(-\sigma_i \delta_i)) \mathbf{c}_i
</span></p>
<p><span class="math math-display">
\text{where } T_i = \exp\left( -\sum_{j=1}^{i-1} \sigma_j \delta_j \right)
</span></p>
<p>여기서 <span class="math math-inline">T_i</span>는 광선이 <span class="math math-inline">i</span>번째 샘플까지 도달할 투과율(Transmittance)을, <span class="math math-inline">\delta_i</span>는 샘플 간의 거리를 의미한다. NeRF는 렌더링된 이미지와 실제 이미지 간의 광도계 오차(Photometric Error)를 최소화하는 방식으로 네트워크 가중치 <span class="math math-inline">\Theta</span>를 학습한다.6</p>
<h4>3.1.2  로봇 공학적 의의: 연속성과 뷰 합성</h4>
<p>NeRF가 로봇 공학에 제공하는 핵심 가치는 다음과 같다.</p>
<ul>
<li><strong>고해상도 연속 표현:</strong> 복셀 그리드와 달리 해상도가 고정되어 있지 않아, 로봇이 물체에 가까이 다가갈수록 더 세밀한 디테일을 렌더링할 수 있다. 이는 정밀 조작(Fine Manipulation)에 유리하다.7</li>
<li><strong>자기 지도 학습(Self-Supervised Learning):</strong> 3D 레이블(CAD 모델 등) 없이 2D 이미지만으로 3D 구조를 학습할 수 있어, 데이터 수집 비용이 높은 로봇 환경에 적합하다.8</li>
<li><strong>조명 변화 강건성:</strong> 시점 의존적 색상을 모델링하므로, 금속이나 유리 같은 반사 재질(Specular Surface)이 있는 환경에서도 로봇의 위치 추정(Localization) 성능을 유지할 수 있다.9</li>
</ul>
<h3>3.2  로봇 내비게이션을 위한 NeRF 활용 (NeRF-Navigation)</h3>
<p>NeRF가 생성하는 밀도장(Density Field)은 로봇의 충돌 회피 경로 계획에 직접적으로 활용될 수 있다. Adamkiewicz 등의 연구(“NeRF-Navigation”)는 학습된 NeRF 모델 내부에서 로봇의 궤적을 최적화하는 방법을 제안했다.10</p>
<p>이 시스템은 로봇의 상태 <span class="math math-inline">\mathbf{x}_t</span>가 주어졌을 때, NeRF가 예측한 해당 위치의 밀도 값 <span class="math math-inline">\sigma(\mathbf{x}_t)</span>가 임계값 이상이면 충돌로 간주한다. 경로 계획 알고리즘은 이 밀도 값을 비용 함수(Cost Function)에 포함하여, 밀도가 낮은 ’빈 공간(Free Space)’을 통과하는 궤적을 생성한다.</p>
<ul>
<li><strong>장점:</strong> 별도의 점유 격자 지도(Occupancy Grid Map) 생성 과정 없이, 시각적 모델(NeRF)을 그대로 물리적 충돌 모델로 사용한다는 점에서 메모리 효율적이며, “End-to-End” 최적화가 가능하다.</li>
<li><strong>동적 궤적 최적화:</strong> 미분 가능한 NeRF의 특성을 이용하여 경사 하강법(Gradient Descent) 기반으로 궤적을 매끄럽게 수정할 수 있다.11</li>
</ul>
<h3>3.3  부호 거리 함수(SDF)와 iSDF: 표면의 명확성 확보</h3>
<p>NeRF의 부피 밀도는 구름이나 안개처럼 ‘반투명한’ 표현을 허용하므로, 로봇이 물체와 접촉해야 하는 상황에서는 표면의 위치가 모호할 수 있다. 이를 해결하기 위해 부호 거리 함수(Signed Distance Function, SDF)를 학습하는 방식이 도입되었다.14</p>
<p><strong>iSDF (Incremental Signed Distance Fields)</strong> 14는 실시간 RGB-D 스트림으로부터 환경의 SDF를 지속적으로 학습한다. SDF 값 <span class="math math-inline">s</span>는 표면 내부에서 음수(<span class="math math-inline">s&lt;0</span>), 외부에서 양수(<span class="math math-inline">s&gt;0</span>), 표면 위에서 0(<span class="math math-inline">s=0</span>)이 되도록 정의된다.</p>
<ul>
<li><strong>물리적 상호작용:</strong> SDF는 표면까지의 정확한 거리와 법선 벡터(Normal Vector, <span class="math math-inline">\nabla s</span>)를 제공하므로, 로봇 팔이 물체 표면을 따라 이동하거나 힘을 제어하는 작업에 필수적이다.</li>
<li><strong>아이코날(Eikonal) 규제:</strong> 학습 시 <span class="math math-inline">|\nabla s| = 1</span>이라는 아이코날 방정식을 손실 함수에 추가하여, 거리 장(Distance Field)의 물리적 타당성을 보장한다.14</li>
</ul>
<h3>3.4  신경 기술자 필드(Neural Descriptor Fields, NDF): 조작의 일반화</h3>
<p>단순한 형상 복원을 넘어, 물체의 ’의미적 부분(Semantic Parts)’을 파악하고 조작 기술을 전이(Transfer)하기 위해 NDF가 제안되었다.7</p>
<p>NDF는 3D 좌표 <span class="math math-inline">\mathbf{x}</span>를 의미론적 특징 벡터(Descriptor)로 매핑하는 함수 <span class="math math-inline">f(\mathbf{x}|\mathbf{P})</span>를 학습한다. 여기서 핵심은 **SE(3) 등변성(Equivariance)**이다. 물체가 회전하거나 이동해도, 해당 물체의 특정 부분(예: 컵의 손잡이)에 해당하는 특징 벡터의 값은 변하지 않고 좌표 공간과 함께 변환된다.16</p>
<ul>
<li><strong>퓨샷 학습(Few-Shot Learning):</strong> NDF를 사용하면 로봇에게 5~10번의 데모만 보여주어도, 전혀 다른 모양의 컵이나 주전자의 손잡이를 찾아내는 능력을 일반화할 수 있다.15 이는 로봇이 물체의 기하학적 형상과 ’잡을 수 있는 곳(Graspable Region)’이라는 의미적 속성을 연결했기 때문이다.</li>
</ul>
<h3>3.5  암시적 표현의 병목과 한계</h3>
<p>NeRF와 SDF는 로봇 인지 기술을 혁신했으나, 실시간 로봇 시스템, 특히 SLAM과 같이 빠른 반응이 필요한 작업에 적용하기에는 명확한 한계가 존재했다.17</p>
<ol>
<li><strong>느린 렌더링 및 추론 속도:</strong> NeRF는 이미지 한 장을 렌더링하기 위해 픽셀당 수백 번의 MLP 연산(Ray Marching)을 수행해야 한다. 이는 1080p 해상도에서 수 초~수 분이 소요되어, 실시간 주행이나 조작에는 부적합하다.9</li>
<li><strong>데이터 효율성과 망각(Forgetting):</strong> MLP는 새로운 데이터를 학습할 때 이전의 정보를 잊어버리는 ‘파국적 망각(Catastrophic Forgetting)’ 현상이 발생하기 쉽다. 이를 방지하기 위한 리플레이 버퍼(Replay Buffer) 등의 기법은 메모리 사용량을 증가시킨다.19</li>
<li><strong>편집 불가능성(Lack of Editability):</strong> 신경망의 가중치에 정보가 전역적으로 분산되어 있어, 맵의 특정 부분(예: 이동한 의자 하나)만 삭제하거나 수정하는 것이 매우 어렵다.18</li>
</ol>
<p>이러한 배경 속에서, 2023년 등장한 **3D 가우시안 스플래팅(3D Gaussian Splatting, 3DGS)**은 암시적 표현의 고품질 렌더링과 명시적 표현의 실시간성을 결합하며 로봇 공학의 새로운 표준으로 급부상했다.</p>
<h2>4.  명시적 표현의 귀환: 3D 가우시안 스플래팅 (3D Gaussian Splatting)</h2>
<p>3D 가우시안 스플래팅(3DGS)은 장면을 수백만 개의 3D 가우시안 타원체(Ellipsoid)들의 집합으로 표현하는 기술이다. 이는 NeRF와 같은 신경망(MLP)을 사용하지 않고, 최적화 가능한 파라미터를 가진 명시적인 기하학적 원형(Primitive)을 사용하여 렌더링 속도를 획기적으로 개선했다.6</p>
<h3>4.1  3DGS의 수학적 모델</h3>
<p>3DGS는 장면을 구성하는 각 가우시안 <span class="math math-inline">G(\mathbf{x})</span>를 중심 위치(Mean, <span class="math math-inline">\boldsymbol{\mu}</span>)와 공분산 행렬(Covariance Matrix, <span class="math math-inline">\boldsymbol{\Sigma}</span>)로 정의한다.6<br />
<span class="math math-display">
G(\mathbf{x}) = \exp\left( -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) \right)
</span><br />
각 가우시안은 다음의 속성을 학습 가능한 파라미터로 가진다 20:</p>
<ol>
<li><strong>위치 (Position, <span class="math math-inline">\boldsymbol{\mu} \in \mathbb{R}^3</span>):</strong> 3D 공간 상의 중심 좌표.</li>
<li><strong>공분산 (Covariance, <span class="math math-inline">\boldsymbol{\Sigma} \in \mathbb{R}^{3 \times 3}</span>):</strong> 타원체의 크기와 방향을 결정한다. 최적화의 안정성을 위해 <span class="math math-inline">\boldsymbol{\Sigma}</span>는 스케일링 행렬 <span class="math math-inline">\mathbf{S}</span>와 회전 행렬 <span class="math math-inline">\mathbf{R}</span>(쿼터니언으로 표현)로 분해하여 $ \boldsymbol{\Sigma} = \mathbf{R}\mathbf{S}\mathbf{S}^T\mathbf{R}^T $ 형태로 학습된다.</li>
<li><strong>불투명도 (Opacity, $\alpha \in $):</strong> 해당 가우시안의 투명도.</li>
<li><strong>색상 (Color via Spherical Harmonics):</strong> 시점에 따라 변하는 색상을 표현하기 위해 구면 조화 함수(Spherical Harmonics, SH) 계수를 저장한다. 이를 통해 반사광과 같은 뷰 의존적 효과를 정교하게 렌더링한다.</li>
</ol>
<h3>4.2  타일 기반 래스터화(Tile-based Rasterization)와 렌더링 파이프라인</h3>
<p>3DGS의 핵심 혁신은 NeRF의 레이 마칭을 대체하는 <strong>래스터화(Rasterization)</strong> 기반의 렌더링 파이프라인이다. 이 과정은 다음과 같이 진행된다.22</p>
<ol>
<li>
<p>투영 (Projection): 3D 가우시안을 카메라 시점의 2D 이미지 평면으로 투영한다. 이때 3D 공분산 <span class="math math-inline">\boldsymbol{\Sigma}</span>는 2D 공분산 <span class="math math-inline">\boldsymbol{\Sigma}&#39;</span>으로 변환된다.<br />
<span class="math math-display">
\boldsymbol{\Sigma}&#39; = \mathbf{J} \mathbf{W} \boldsymbol{\Sigma} \mathbf{W}^T \mathbf{J}^T
</span><br />
여기서 <span class="math math-inline">\mathbf{W}</span>는 뷰 변환 행렬, <span class="math math-inline">\mathbf{J}</span>는 투영 변환의 야코비안(Jacobian)이다.</p>
</li>
<li>
<p><strong>타일 정렬 (Tile Sorting):</strong> 이미지를 <span class="math math-inline">16 \times 16</span> 픽셀 크기의 타일로 분할한다. 각 타일 내에 존재하는 가우시안들을 깊이(Depth) 순서대로 정렬한다. 3DGS는 GPU의 고속 정렬 알고리즘(Radix Sort)을 활용하여 수백만 개의 가우시안을 실시간으로 정렬한다.</p>
</li>
<li>
<p><strong>알파 블렌딩 (Alpha Blending):</strong> 정렬된 가우시안들을 앞에서 뒤로(Front-to-back) 순서로 누적하여 최종 픽셀 색상을 계산한다. 누적 불투명도가 1에 도달하면 해당 픽셀의 계산을 중단(Early Stopping)하여 속도를 높인다.</p>
</li>
</ol>
<p>이러한 파이프라인 덕분에 3DGS는 1080p 해상도에서 <strong>100 FPS 이상의 렌더링 속도</strong>를 달성하며, 이는 NeRF 대비 수십 배에서 수백 배 빠른 속도이다.9</p>
<h3>4.3  적응형 밀도 제어 (Adaptive Density Control)</h3>
<p>3DGS는 학습 과정에서 가우시안의 개수와 분포를 동적으로 조절한다.20</p>
<ul>
<li><strong>복제 (Clone):</strong> 텍스처가 복잡하거나 디테일이 부족한 영역(Under-reconstructed)에서는 가우시안을 복제하여 밀도를 높인다.</li>
<li><strong>분할 (Split):</strong> 가우시안의 크기가 너무 커서 과도한 영역을 덮는 경우(Over-reconstructed), 이를 두 개의 작은 가우시안으로 분할한다.</li>
<li><strong>가지치기 (Prune):</strong> 불투명도(<span class="math math-inline">\alpha</span>)가 임계값 이하로 떨어지거나 화면 밖으로 벗어난 가우시안은 주기적으로 제거하여 메모리 효율을 유지한다.</li>
</ul>
<p>이러한 적응형 제어는 로봇이 미지의 환경을 탐험할 때, 정보가 부족한 영역을 자동으로 채우고 불필요한 데이터를 정리하는 데 매우 효과적이다.</p>
<h3>4.4  NeRF 대 3DGS: 로봇 공학적 관점의 비교 분석</h3>
<table><thead><tr><th><strong>특성</strong></th><th><strong>NeRF (Neural Radiance Fields)</strong></th><th><strong>3D Gaussian Splatting (3DGS)</strong></th><th><strong>로봇 공학적 함의</strong></th></tr></thead><tbody>
<tr><td><strong>기반 기술</strong></td><td>암시적 (Implicit), MLP 신경망</td><td>명시적 (Explicit), 가우시안 원형</td><td>3DGS는 명시적 좌표를 가지므로 맵의 부분 수정 및 업데이트가 용이함.18</td></tr>
<tr><td><strong>렌더링 방식</strong></td><td>레이 마칭 (Ray Marching)</td><td>래스터화 (Rasterization)</td><td>3DGS는 실시간 시각 피드백 및 고주사율 제어(Control) 가능.20</td></tr>
<tr><td><strong>학습/수렴</strong></td><td>수 시간 소요 (표준 모델)</td><td>수 분 ~ 실시간</td><td>3DGS는 온라인 SLAM 및 즉각적 환경 적응에 유리.9</td></tr>
<tr><td><strong>메모리</strong></td><td>매우 작음 (수 MB)</td><td>큼 (수백 MB ~ 수 GB)</td><td>NeRF는 저장 용량이 제한된 극소형 임베디드 시스템에 유리할 수 있음.</td></tr>
<tr><td><strong>노이즈</strong></td><td>고주파 노이즈에 강인함</td><td>초기 포인트 클라우드 품질에 의존</td><td>3DGS는 희소한 데이터에서 아티팩트(Floater)가 발생할 수 있음.25</td></tr>
</tbody></table>
<p>이러한 비교 우위를 바탕으로 3DGS는 로봇의 SLAM, 내비게이션, 조작 기술에 빠르게 도입되고 있다.</p>
<h2>5.  3DGS 기반 로봇 인식 기술의 최전선: SLAM과 내비게이션</h2>
<h3>5.1  3DGS 기반 SLAM: 고밀도 실사 맵핑의 실현</h3>
<p>기존의 특징점 기반 SLAM(ORB-SLAM 등)은 희소한(Sparse) 맵을 생성하여 로봇이 환경을 온전히 이해하는 데 한계가 있었고, NeRF 기반 SLAM(iMAP, NICE-SLAM)은 느린 속도와 “망각” 문제가 있었다. 3DGS 기반 SLAM은 ’고밀도(Dense) 실사 맵핑’과 ’실시간 트래킹’을 동시에 달성하며 SLAM의 새로운 장을 열었다.26</p>
<h4>5.1.1  SplaTAM: 실루엣 마스크와 명시적 맵핑</h4>
<p><strong>SplaTAM</strong> 28은 단안 RGB-D 카메라를 사용하여 3DGS 기반의 실시간 SLAM을 구현한 선구적인 연구다.</p>
<ul>
<li><strong>작동 원리:</strong> SplaTAM은 카메라 포즈 추적(Tracking)과 맵 업데이트(Mapping)를 명확히 분리한다. 새로운 프레임이 들어오면 기존 가우시안 맵을 현재 시점으로 렌더링하고, 입력 이미지와의 차이(Photometric Error)를 최소화하는 방향으로 카메라 포즈를 최적화한다.</li>
<li><strong>실루엣 마스크 (Silhouette Mask):</strong> 3DGS의 명시적 특성을 활용하여, 현재 맵이 커버하는 영역(실루엣)과 새로운 관측 영역을 구분한다. 이를 통해 이미 맵핑된 영역은 업데이트하고, 새로운 영역에는 가우시안을 추가하는 구조적 맵 확장을 수행한다.</li>
<li><strong>성능:</strong> 기존 NeRF 기반 방식 대비 카메라 포즈 추정 정확도와 맵 재구성 품질(PSNR)에서 2배 이상의 성능 향상을 보였다.28</li>
</ul>
<h4>5.1.2  Splat-SLAM: 광학 흐름과 전역 최적화</h4>
<p><strong>Splat-SLAM</strong> 30은 RGB 카메라만으로 고밀도 맵을 생성하는 데 초점을 맞춘다. 깊이 센서가 없는 상황에서 3D 구조를 복원하기 위해 **전역 최적화(Global Optimization)**에 집중한다.</p>
<ul>
<li><strong>고밀도 광학 흐름 (Dense Optical Flow):</strong> 딥러닝 기반 광학 흐름 모델(RAFT 등)을 사용하여 프레임 간 픽셀 대응 관계를 찾고, 이를 기반으로 초기 포즈와 깊이를 추정한다.</li>
<li><strong>DSPO (Disparity, Scale and Pose Optimization):</strong> 단안 깊이 추정(Monocular Depth Estimation) 네트워크의 출력을 사전 정보(Prior)로 활용하여, 스케일 모호성을 해결하고 포즈 정확도를 높이는 DSPO 계층을 도입했다.31</li>
<li><strong>맵 변형 (Map Deformation):</strong> 루프 결합(Loop Closure)이 감지되면, 포즈 그래프 최적화 결과에 따라 3D 가우시안 맵 전체를 유연하게 변형시켜 전역적인 일관성을 유지한다. 이는 맵이 신경망 가중치에 고정된 NeRF 방식에서는 구현하기 힘든 기능이다.</li>
<li><strong>정량적 성과:</strong> Replica 데이터셋 등에서 NeRF 기반 방법보다 우수한 ATE(Absolute Trajectory Error) 0.57cm를 기록하며, 메모리 사용량도 획기적으로 줄였다.28</li>
</ul>
<h4>5.1.3  대규모 및 협업 SLAM: CoSLAM과 GigaSLAM</h4>
<p>단일 로봇을 넘어, <strong>GigaSLAM</strong> 34은 계층적 희소 복셀(Hierarchical Sparse Voxel) 구조를 도입하여 3DGS를 대규모 실외 환경으로 확장했다. 또한 <strong>CoSLAM (Collaborative SLAM)</strong> 27 연구들은 여러 로봇이 수집한 가우시안 맵을 중앙 서버나 분산 네트워크에서 병합하는 기술을 다룬다. 3DGS 데이터는 포인트 클라우드처럼 병합이 가능하므로, 다중 에이전트 간의 맵 공유와 협업에 유리하다.</p>
<h3>5.2  의미론적 내비게이션과 언어 내장 (Language Embedding)</h3>
<p>로봇이 “빨간색 의자로 가라” 혹은 “커피 머신을 찾아라“와 같은 고수준 명령을 수행하려면, 공간의 기하학적 정보뿐만 아니라 의미론적(Semantic) 정보가 3D 맵에 통합되어야 한다.</p>
<h4>5.2.1  LEGS (Language-Embedded Gaussian Splats)</h4>
<p><strong>LEGS</strong> 21는 3DGS에 거대 언어 모델(LLM)과 비전-언어 모델(VLM, 예: CLIP)의 지식을 주입하여 ’말을 알아듣는 3D 맵’을 구축한다.</p>
<ul>
<li><strong>구조:</strong> 로봇이 환경을 이동하며 이미지를 수집할 때, CLIP과 같은 인코더를 통해 이미지의 픽셀별 또는 패치별 의미 특징(Semantic Feature)을 추출한다. 이 고차원 특징 벡터(예: 512차원)는 3D 가우시안의 추가적인 속성(<span class="math math-inline">\mathbf{f}_k</span>)으로 학습된다.</li>
<li><strong>개방형 어휘(Open-Vocabulary) 쿼리:</strong> 사용자가 텍스트 쿼리를 입력하면, 이를 동일한 임베딩 공간으로 변환한 뒤 3D 가우시안들의 특징 벡터와 코사인 유사도(Cosine Similarity)를 계산한다. 유사도가 높은 가우시안들이 활성화되며 해당 위치를 3D 공간 상에서 즉시 찾아낸다.</li>
<li><strong>성능:</strong> 기존의 NeRF 기반 언어 필드인 LERF(Language Embedded Radiance Fields)와 비교하여, LEGS는 <strong>3.5배 이상 빠른 학습 속도</strong>를 보였다.21 이는 로봇이 낯선 환경에 진입하여 빠르게 맵을 만들고 즉시 임무를 수행해야 하는 상황에서 결정적인 이점이다. 또한 다중 카메라 설정을 통한 증분 번들 조정(Incremental Bundle Adjustment)을 통해 궤적의 정확도와 재구성 품질을 높였다.</li>
</ul>
<h2>6.  동적 상호작용과 조작(Manipulation)의 진화</h2>
<p>로봇 팔을 이용한 조작(Manipulation)은 정적 맵핑보다 훨씬 복잡한 문제를 다룬다. 로봇이 물체를 밀거나 들어 올릴 때 환경이 어떻게 변할지 예측하는 <strong>동역학(Dynamics)</strong> 모델링이 필요하기 때문이다.</p>
<h3>6.1  ManiGaussian: 미래 장면 예측을 통한 동역학 학습</h3>
<p><strong>ManiGaussian</strong> 38은 조작 작업을 위해 3DGS를 확장한 ‘동적 가우시안 스플래팅(Dynamic Gaussian Splatting)’ 프레임워크를 제안했다. 기존의 방법들이 2D 이미지의 의미적 특징에만 의존하여 물리적 상호작용을 놓치는 문제를 해결하고자 했다.</p>
<h4>6.1.1  핵심 메커니즘: 가우시안 월드 모델</h4>
<p>ManiGaussian은 **가우시안 월드 모델(Gaussian World Model)**을 구축하여 환경의 동역학을 학습한다.</p>
<ul>
<li><strong>입력:</strong> 현재 관측된 3DGS 상태와 로봇의 행동(Action, 예: 그리퍼 이동, 잡기).</li>
<li><strong>예측:</strong> 로봇의 행동에 따라 가우시안들이 어떻게 이동하고 변형될지(Deformation)를 예측하여 ’미래의 장면’을 3DGS 형태로 생성한다.</li>
<li><strong>미래 장면 일관성 손실(Future Scene Consistency Loss, <span class="math math-inline">\mathcal{L}_{\text{Dyna}}</span>):</strong> 예측된 미래 장면을 렌더링한 이미지와, 실제로 로봇이 행동을 수행한 후 관측된 이미지 간의 차이를 최소화한다. 이를 통해 모델은 별도의 물리 엔진 없이도 데이터로부터 물리적 인과관계를 스스로 학습한다.38</li>
</ul>
<h4>6.1.2  성과 및 비교</h4>
<p>RLBench 벤치마크에서 10개의 조작 작업(블록 쌓기, 문 열기 등)을 평가한 결과, ManiGaussian은 기존 SOTA(State-of-the-Art) 방식들보다 <strong>평균 13.1% 높은 성공률</strong>을 기록했다.39 특히 “두 개의 블록 정밀하게 쌓기“와 같이 3D 공간 관계와 충돌 예측이 중요한 작업에서 월등한 성능을 보였다. 이는 3DGS가 제공하는 명시적이고 연속적인 공간 표현이 정밀 조작에 효과적임을 입증한다.</p>
<h3>6.2  ManiGaussian++: 양팔 로봇과 다중 물체 상호작용</h3>
<p>최근 연구인 <strong>ManiGaussian++</strong> 42은 이를 양팔 로봇(Bimanual Robot)으로 확장했다.</p>
<ul>
<li><strong>계층적 리더-팔로워(Leader-Follower) 모델:</strong> 한 팔이 물체를 고정(Stabilizing)하고 다른 팔이 조작(Acting)하는 복잡한 상호작용을 모델링하기 위해, 두 팔의 역할을 구분하여 동역학을 학습한다.</li>
<li><strong>성과:</strong> 단일 팔 모델이 실패했던 복잡한 협조 작업에서 20.2%의 성능 향상을 이루었으며, 실환경(Real-world) 실험에서도 60% 이상의 성공률을 달성했다.42</li>
</ul>
<h2>7.  결론 및 전망: 하이브리드 표현과 생성형 월드 모델</h2>
<p>표현 학습의 진화는 2D 픽셀의 패턴 인식(CNN)에서 시작하여, 3D 공간의 연속적 함수(NeRF/Implicit)를 거쳐, 다시 명시적이고 제어 가능한 3D 원형(3DGS/Explicit)으로 회귀하는 나선형 발전 양상을 보이고 있다.</p>
<p>2D CNN은 로봇에게 ’객체를 분류하는 법’을 가르쳤고, NeRF는 ’공간을 상상하고 채우는 법’을 제시했다. 이제 **3D 가우시안 스플래팅(3DGS)**은 로봇에게 **‘실시간으로 공간을 조각하고, 편집하며, 물리적 결과를 예측하는 능력’**을 부여하고 있다.</p>
<ul>
<li><strong>SLAM의 진화:</strong> SplaTAM과 Splat-SLAM은 로봇이 자신의 위치를 파악함과 동시에 사진과 같은 고해상도 3D 지도를 실시간으로 생성하게 했다.</li>
<li><strong>언어와의 결합:</strong> LEGS는 이 지도에 언어적 지능을 불어넣어, 로봇과 인간이 자연어로 공간에 대해 소통할 수 있는 길을 열었다.</li>
<li><strong>행동과의 통합:</strong> ManiGaussian은 시각적 표현이 단순한 관찰을 넘어 행동의 결과를 예측하는 시뮬레이터 역할을 할 수 있음을 증명했다.</li>
</ul>
<p>향후 연구는 3DGS의 메모리 효율성을 개선하고(압축 기술), NeRF의 장점인 위상학적(Topological) 완전성을 결합한 하이브리드 아키텍처로 나아갈 것이다. 또한, 생성형 AI(Generative AI)와 결합하여 로봇이 가상의 행동을 3D 공간에서 시뮬레이션하고 최적의 계획을 수립하는 **‘생성형 월드 모델(Generative World Models)’**이 로봇 제어의 핵심 두뇌로 자리 잡을 것으로 전망된다. 이러한 표현 학습의 혁신은 로봇을 단순한 자동화 기계에서 진정한 의미의 지능형 에이전트로 거듭나게 하는 기술적 토대가 될 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>3D Spatial-Awareness Enables Effective Embodied Representation, https://liner.com/review/spa-3d-spatialawareness-enables-effective-embodied-representation</li>
<li>What are the limitations of CNN in computer vision? - Milvus, https://milvus.io/ai-quick-reference/what-are-the-limitations-of-cnn-in-computer-vision</li>
<li>2D-CNN methods limitations (a) Modelling 2D image Spatial and…, https://www.researchgate.net/figure/D-CNN-methods-limitations-a-Modelling-2D-image-Spatial-and-temporal-dependencies-b_fig2_349016822</li>
<li>NeRFs in Robotics: A Survey - arXiv, https://arxiv.org/html/2405.01333v2</li>
<li>7.6. Neural Radiance Fields for Drones, https://www.roboticsbook.org/S76_drone_learning.html</li>
<li>From Volume Rendering to 3D Gaussian Splatting - arXiv, https://arxiv.org/html/2510.18101v1</li>
<li>Neural Fields in Robotics: A Survey - arXiv, https://arxiv.org/html/2410.20220v1</li>
<li>Neural Descriptor Fields: SE(3)-Equivariant Object Representations …, https://yilundu.github.io/ndf/</li>
<li>Evaluating 3D Reconstruction: A Side-by-Side Comparison of NeRF …, https://www.preprints.org/manuscript/202504.1068</li>
<li>Vision-Only Robot Navigation in a Neural Radiance World, https://pculbertson.github.io/assets/pdf/adamkiewicz2021.pdf</li>
<li>Vision-Only Robot Navigation in a Neural Radiance World, https://neuralfields.cs.brown.edu/paper_261.html</li>
<li>Vision-Only Robot Navigation in a Neural Radiance World - arXiv, https://arxiv.org/pdf/2110.00168</li>
<li>Vision-Only Robot Navigation in a Neural Radiance World, https://www.researchgate.net/publication/358565316_Vision-Only_Robot_Navigation_in_a_Neural_Radiance_World</li>
<li>iSDF: Real-Time Neural Signed Distance Fields for Robot Perception, https://www.researchgate.net/publication/361714160_iSDF_Real-Time_Neural_Signed_Distance_Fields_for_Robot_Perception</li>
<li>Local Neural Descriptor Fields - IEEE Xplore, https://ieeexplore.ieee.org/iel7/10160211/10160212/10160423.pdf</li>
<li>(PDF) Neural Descriptor Fields: SE(3)-Equivariant Object …, https://www.researchgate.net/publication/356919495_Neural_Descriptor_Fields_SE3-Equivariant_Object_Representations_for_Manipulation</li>
<li>A Survey on 3D Gaussian Splatting - arXiv, https://arxiv.org/html/2401.03890v7</li>
<li>Editing Implicit and Explicit Representations of Radiance Fields - arXiv, https://arxiv.org/html/2412.17628v1</li>
<li>dynamic visual neural slam based on local-global encoding, https://discovery.ucl.ac.uk/id/eprint/10215510/1/2403.11776v1.pdf</li>
<li>3D Gaussian Splatting vs NeRF: The End Game of … - PyImageSearch, https://pyimagesearch.com/2024/12/09/3d-gaussian-splatting-vs-nerf-the-end-game-of-3d-reconstruction/</li>
<li>Language-Embedded Gaussian Splats (LEGS) - AUTOLAB, https://autolab.berkeley.edu/assets/publications/media/2024_IROS_LEGS_CR.pdf</li>
<li>3D Gaussian Splatting - Paper Explained, Training NeRFStudio, https://learnopencv.com/3d-gaussian-splatting/</li>
<li>Rendering Gaussian Splats in real-time - Evolve Benchmarking, https://www.evolvebenchmark.com/blog-posts/3d-gaussian-splatting-rendering-gaussian-splats-in-real-time</li>
<li>3D Gaussian Splatting for Real-Time Radiance Field Rendering - Inria, https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/3d_gaussian_splatting_low.pdf</li>
<li>BIP3D: Bridging 2D Images and 3D Perception for Embodied …, https://openaccess.thecvf.com/content/CVPR2025/papers/Lin_BIP3D_Bridging_2D_Images_and_3D_Perception_for_Embodied_Intelligence_CVPR_2025_paper.pdf</li>
<li>3D-Vision-World/awesome-NeRF-and-3DGS-SLAM - GitHub, https://github.com/3D-Vision-World/awesome-NeRF-and-3DGS-SLAM</li>
<li>A Survey on Collaborative SLAM with 3D Gaussian Splatting - arXiv, https://arxiv.org/html/2510.23988v1</li>
<li>Splat-SLAM: RGB-D SLAM with 3D Gaussians - Emergent Mind, https://www.emergentmind.com/papers/2312.02126</li>
<li>Open-Set Semantic Gaussian Splatting SLAM with Expandable…, https://openreview.net/forum?id=E68dgQUzrC</li>
<li>RD-SLAM: Real-Time Dense SLAM Using Gaussian Splatting - MDPI, https://www.mdpi.com/2076-3417/14/17/7767</li>
<li>Splat-SLAM: Globally Optimized RGB-only SLAM with 3D Gaussians, https://openaccess.thecvf.com/content/CVPR2025W/VOCVALC/papers/Sandstrom_Splat-SLAM_Globally_Optimized_RGB-only_SLAM_with_3D_Gaussians_CVPRW_2025_paper.pdf</li>
<li>Splat-SLAM: Globally Optimized RGB-only SLAM with 3D Gaussians, https://openreview.net/forum?id=YKtbklD5MV</li>
<li>Globally Optimized RGB-only SLAM with 3D Gaussians - SciSpace, https://scispace.com/pdf/splat-slam-globally-optimized-rgb-only-slam-with-3d-232hrn1p6h.pdf</li>
<li>Large-Scale Monocular SLAM with Hierachical Gaussian Splats, https://arxiv.org/html/2503.08071v1</li>
<li>Language-Embedded Gaussian Splats (LEGS) - arXiv, https://arxiv.org/html/2409.18108v1</li>
<li>Language-Embedded Gaussian Splats (LEGS) - ResearchGate, https://www.researchgate.net/publication/387425740_Language-Embedded_Gaussian_Splats_LEGS_Incrementally_Building_Room-Scale_Representations_with_a_Mobile_Robot</li>
<li>(PDF) Language-Embedded Gaussian Splats (LEGS) - ResearchGate, https://www.researchgate.net/publication/384365084_Language-Embedded_Gaussian_Splats_LEGS_Incrementally_Building_Room-Scale_Representations_with_a_Mobile_Robot</li>
<li>ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic …, https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05194.pdf</li>
<li>Dynamic Gaussian Splatting for Multi-task Robotic Manipulation, https://chatpaper.com/paper/99386</li>
<li>ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic …, https://guanxinglu.github.io/ManiGaussian/</li>
<li>Dynamic Gaussian Splatting for Multi-task Robotic Manipulation - arXiv, https://arxiv.org/abs/2403.08321</li>
<li>Dynamic Gaussian Splatting for Multi-task Robotic Manipulation, https://www.researchgate.net/publication/384462520_ManiGaussian_Dynamic_Gaussian_Splatting_for_Multi-task_Robotic_Manipulation</li>
<li>General Robotic Bimanual Manipulation with Hierarchical Gaussian …, https://arxiv.org/html/2506.19842v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>