<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:6.1.1 기존 객체 탐지(Object Detection)의 한계: COCO, ImageNet 등 고정된 데이터셋 클래스의 제약과 로봇 현장 적용 시 발생하는 도메인 격차(Domain Gap).</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>6.1.1 기존 객체 탐지(Object Detection)의 한계: COCO, ImageNet 등 고정된 데이터셋 클래스의 제약과 로봇 현장 적용 시 발생하는 도메인 격차(Domain Gap).</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 6. 오픈 보캐블러리와 시맨틱 이해 (Open-Vocabulary & Semantic Understanding)</a> / <a href="index.html">6.1 폐쇄형 집합(Closed-set)의 종말과 개방형 어휘(Open-Vocabulary)의 부상</a> / <span>6.1.1 기존 객체 탐지(Object Detection)의 한계: COCO, ImageNet 등 고정된 데이터셋 클래스의 제약과 로봇 현장 적용 시 발생하는 도메인 격차(Domain Gap).</span></nav>
                </div>
            </header>
            <article>
                <h1>6.1.1 기존 객체 탐지(Object Detection)의 한계: COCO, ImageNet 등 고정된 데이터셋 클래스의 제약과 로봇 현장 적용 시 발생하는 도메인 격차(Domain Gap).</h1>
<h2>1.  서론: 통제된 실험실의 성공과 닫힌 세계(Closed-Set)의 환상</h2>
<p>지난 10여 년간 컴퓨터 비전 분야, 특히 객체 탐지(Object Detection) 기술은 심층 신경망(Deep Neural Networks, DNN)의 발전과 함께 눈부신 성장을 이룩했다. R-CNN(Region-based Convolutional Neural Networks) 계열의 2단계 탐지기에서 시작하여, 속도와 정확도의 균형을 맞춘 YOLO(You Only Look Once), SSD(Single Shot MultiBox Detector), 그리고 최근의 트랜스포머(Transformer) 기반 DETR(Detection Transformer)에 이르기까지, 학술적 벤치마크에서의 성능은 인간의 인지 능력을 상회하는 수준에 도달한 것처럼 보였다. 이러한 기술적 진보는 자율 주행 자동차, 지능형 CCTV, 그리고 로봇 공학 분야에 즉각적이고 혁명적인 변화를 가져올 것으로 기대되었다. 그러나 로봇이 정형화된 데이터셋의 보호를 받는 실험실 환경을 벗어나 복잡하고 예측 불가능한 현실 세계(Open World)로 진입했을 때, 기존의 객체 탐지 패러다임은 명확하고도 치명적인 한계에 봉착했다.</p>
<p>로봇 공학의 관점에서 바라본 기존 객체 탐지 기술의 근본적인 문제는 이 시스템들이 ‘닫힌 세계 가정(Closed-Set Assumption)’ 위에서 구축되었다는 점이다. 이는 탐지기가 학습 과정에서 명시적으로 정의되고 레이블링된 클래스(Seen Classes)만을 인식할 수 있으며, 그 외의 모든 시각적 입력은 배경(Background)으로 처리하거나 억지로 아는 클래스로 끼워 맞추는(Overconfident Misclassification) 구조적 결함을 의미한다. 정해진 공정 라인에서 특정 부품만을 검사하는 산업용 로봇에게는 이러한 가정이 유효할 수 있으나, 가정, 병원, 재난 현장 등 비정형 환경에서 인간과 공존하며 다양한 임무를 수행해야 하는 서비스 로봇이나 자율 주행 로봇에게는 생존과 직결된 치명적인 결함이 된다.</p>
<p>본 장에서는 제미나이(Gemini)와 같은 차세대 대규모 멀티모달 모델(Large Multimodal Models, LMM) 기반의 오픈 어휘(Open-Vocabulary) 탐지 기술이 왜 로봇 공학의 필연적인 대안이 될 수밖에 없는지를 논증하기 위해, 기존 지도 학습(Supervised Learning) 기반 객체 탐지 방법론이 가진 구조적, 데이터적, 환경적 한계를 심층적으로 해부한다. 특히 로봇 현장 적용 시 발생하는 <strong>(1) 고정된 분류 체계(Fixed Taxonomy)와 어휘의 빈곤</strong>, <strong>(2) 롱테일 분포(Long-Tail Distribution)로 인한 데이터 불균형과 희귀 객체 인식 실패</strong>, <strong>(3) 정형화된 시점(Canonical View)과 로봇의 1인칭 시점(Egocentric View) 간의 도메인 격차</strong>, 그리고 <strong>(4) 변화하는 환경 요인(조명, 블러 등)에 대한 강건성 부족</strong> 문제를 상세히 분석한다. 이를 통해 독자는 기존 기술의 한계점을 명확히 인식하고, 새로운 패러다임의 필요성을 절감하게 될 것이다.</p>
<h2>2.  고정된 데이터셋과 분류 체계(Taxonomy)의 구조적 제약</h2>
<h3>2.1  어휘의 빈곤: COCO의 80개 클래스와 현실 세계의 불일치</h3>
<p>현재 객체 탐지 연구의 표준으로 자리 잡은 MS COCO(Microsoft Common Objects in Context) 데이터셋은 약 33만 장의 이미지와 150만 개 이상의 객체 인스턴스를 포함하며, 객체 탐지 알고리즘의 성능을 평가하는 절대적인 척도로 사용되어 왔다. 그러나 COCO 데이터셋이 정의하고 있는 객체 카테고리는 단 80개에 불과하다. 이는 ImageNet 분류 데이터셋(ILSVRC)이 다루는 1,000개의 클래스나, 인간이 일상적으로 인식하고 언어로 표현하는 수만 개의 객체 개념과 비교할 때 턱없이 부족한 수치이다.</p>
<p>이러한 제한된 어휘(Restricted Vocabulary)는 로봇의 환경 인지 능력을 심각하게 저해한다. 예를 들어, 가정용 서비스 로봇에게 “드라이버(Screwdriver)를 가져와“라고 명령했을 때, COCO 데이터셋으로 학습된 최첨단 탐지기조차 드라이버를 인식할 수 없다. 드라이버는 COCO의 80개 클래스(사람, 자전거, 자동차, 개, 고양이, 칫솔 등)에 포함되지 않기 때문이다. 이 경우 로봇은 드라이버를 단순한 배경 잡음으로 처리하거나, 형태적 유사성에 기반하여 ’칫솔(Toothbrush)’이나 ’칼(Knife)’과 같은 엉뚱한 클래스로 오인할 가능성이 높다. 이는 단순한 인식 실패를 넘어, 로봇이 드라이버의 기능적 특성(Affordance)을 잘못 이해하여 부적절한 파지(Grasping) 전략을 수립하게 만들고, 결과적으로 물리적 사고나 임무 실패로 이어질 수 있다.</p>
<p>더욱이 기존의 지도 학습 방식에서는 새로운 클래스를 추가하기 위해 막대한 비용과 시간이 소요된다. 만약 로봇 개발자가 ’드라이버’를 인식시키기 위해 데이터를 추가하고 모델을 재학습시키더라도, 사용자가 다음에 “몽키 스패너“를 요구한다면 또다시 동일한 과정을 반복해야 한다. 현실 세계의 객체는 무한히 다양하며, 고정된 <span class="math math-inline">N</span>개의 클래스로 이를 커버하려는 시도는 근본적으로 불가능한 접근법이다. 아래 표 1은 주요 객체 탐지 데이터셋의 클래스 수와 로봇 환경 간의 극명한 격차를 보여준다.</p>
<table><thead><tr><th><strong>데이터셋</strong></th><th><strong>클래스 수</strong></th><th><strong>주요 특징 및 한계</strong></th><th><strong>로봇 애플리케이션 적용 시 문제점</strong></th></tr></thead><tbody>
<tr><td><strong>PASCAL VOC</strong></td><td>20</td><td>초기 딥러닝 객체 탐지 벤치마크.</td><td>극도로 제한된 어휘로 인해 실질적인 로봇 서비스 시나리오에 적용 불가.</td></tr>
<tr><td><strong>MS COCO</strong></td><td>80</td><td>표준 벤치마크. 문맥 정보 포함.</td><td>일반적인 사물(사람, 차, 동물) 위주로 구성되어, 특정 도구, 부품, 희귀 객체 인식 불가.</td></tr>
<tr><td><strong>ImageNet (CLS)</strong></td><td>1,000</td><td>분류 전용. 다양한 카테고리.</td><td>위치 정보(Bounding Box)가 부재하며, 여전히 롱테일(Long-tail) 객체에 대한 데이터는 부족함.</td></tr>
<tr><td><strong>OpenImages</strong></td><td>~600</td><td>대규모, 계층적 구조 도입.</td><td>어휘 확장은 이루어졌으나, 여전히 사전 정의된 닫힌 집합(Closed Set) 내에서만 동작.</td></tr>
<tr><td><strong>LVIS</strong></td><td>1,203</td><td>롱테일 분포 반영.</td><td>꼬리(Tail) 클래스에 대한 데이터 부족으로 실제 탐지 정확도가 현저히 낮음.</td></tr>
<tr><td><strong>현실 세계 (Real World)</strong></td><td><span class="math math-inline">\infty</span> (무한)</td><td>오픈 월드, 지속적 변화.</td><td><strong>기존 고정 클래스 기반 탐지기로는 근본적 대응 불가능.</strong></td></tr>
</tbody></table>
<h3>2.2  배경 클래스의 역설(The Background Paradox)과 미지의 공포</h3>
<p>지도 학습 기반 탐지기(예: Faster R-CNN, YOLO)의 학습 메커니즘은 ‘배경(Background)’ 클래스에 대한 정의에 의존한다. 이들 모델은 학습된 <span class="math math-inline">N</span>개의 전경(Foreground) 클래스에 속하지 않는 모든 이미지 영역을 배경으로 분류하도록 훈련된다. 즉, 모델에게 세상은 “<span class="math math-inline">N</span>개의 아는 물체“와 “그 외의 무의미한 배경” 두 가지로만 구성된다.</p>
<p>이러한 이분법적 세계관은 ’열린 세계(Open World)’인 로봇 주행 환경에서 심각한 안전 문제를 야기한다. 이를 ‘미지의 공포(Fear of the Unknown)’ 혹은 ‘알려지지 않은 미지(Unknown Unknowns)’ 문제라 칭한다. 예를 들어, 자율 주행 배달 로봇이 도로 주행 중 학습 데이터에 존재하지 않는 ’전동 킥보드를 타고 넘어진 사람’을 마주쳤다고 가정해보자. 이 객체는 ‘보행자(Pedestrian)’ 클래스와도 다르고, ‘차량(Vehicle)’ 클래스와도 시각적 특징이 상이하다. 닫힌 세계 가정 하의 탐지기는 이 낯선 객체를 ’알 수 없는 위험 물체’로 판단하여 회피하는 것이 아니라, 단순히 ’배경(빈 도로)’으로 인식할 확률이 매우 높다. 그 결과 로봇은 장애물이 없는 것으로 판단하고 주행을 지속하여 충돌 사고를 일으킬 수 있다.</p>
<p>연구에 따르면, 최신 탐지기들은 미지의 물체(Unknown Object)에 대해 낮은 신뢰도(Confidence Score)를 출력하는 것이 아니라, 오히려 엉뚱한 클래스로 높은 확신을 가지고 오분류(Overconfident Misclassification)하는 경향이 있다. 사막을 주행하는 로봇이 처음 보는 ’선인장’을 ’교통표지판’이나 ’사람’으로 99%의 확신을 갖고 분류하는 현상이 대표적이다. 이는 심층 신경망이 학습 데이터의 분포(In-Distribution)에 과도하게 최적화되어, 분포 밖 데이터(Out-of-Distribution, OOD)에 대해 예측 불가능한 거동을 보이기 때문이다. 로봇의 판단 로직(Decision Making)이 이러한 잘못된 확신 정보에 기반할 경우, 로봇은 상황을 오판하고 위험한 행동을 실행에 옮기게 된다.</p>
<h3>2.3  아키텍처의 경직성: 분류 헤드(Classification Head)와 Softmax의 족쇄</h3>
<p>전통적인 객체 탐지 모델의 마지막 단계는 고정된 크기 <span class="math math-inline">N</span>을 가진 분류 헤드(Classification Head)와 Softmax 함수로 구성된다. 이 구조는 수학적으로나 의미론적으로 로봇의 유연한 사고를 가로막는 족쇄로 작용한다.</p>
<p>첫째, **상호 배타적 가정(Mutually Exclusive Assumption)**의 문제다. Softmax 함수는 입력된 특징 벡터가 <span class="math math-inline">N</span>개의 클래스 중 오직 하나에만 속할 확률을 계산한다. 그러나 현실 세계의 객체는 다중적이고 계층적인 의미를 가진다. 예를 들어, ’의자’는 동시에 ’가구’이기도 하고, 재질에 따라 ’목재’일 수도 있다. 기존 탐지기는 이러한 의미의 중첩을 허용하지 않으며, ’의자’이면서 ’가구’인 상태를 표현할 수 없다. 이는 로봇이 “가구들을 스캔해“라는 상위 개념의 명령을 수행할 때, 개별 객체(의자, 책상 등)와 상위 개념(가구) 간의 연결 고리를 끊어버리는 결과를 낳는다.</p>
<p>둘째, **의미론적 관계의 부재(Lack of Semantic Relation)**이다. 원-핫 인코딩(One-Hot Encoding)된 벡터 공간에서 ‘고양이’()와 ‘개’() 사이의 거리는 ’고양이’와 ‘자동차’() 사이의 거리와 수학적으로 동일하다(<span class="math math-inline">\sqrt{2}</span>). 즉, 모델은 클래스 간의 의미적 유사성이나 연관성을 전혀 이해하지 못하며, 단순히 레이블을 인덱스로만 암기한다. 이로 인해 ’트럭’을 수만 장 학습한 모델이라도, 시각적으로 매우 유사한 ’버스’를 처음 보았을 때 기존의 지식을 전이(Transfer)하여 추론하지 못하고, 완전히 새로운 객체로 처음부터 다시 배워야 한다. 이러한 제로샷 전이(Zero-Shot Transfer) 능력의 부재는 로봇이 낯선 환경에 적응하는 속도를 현저히 늦추는 요인이 된다.</p>
<h2>3.  어노테이션 병목(Annotation Bottleneck)과 경제성 분석</h2>
<p>기존 객체 탐지 시스템의 한계를 극복하기 위해 단순히 데이터를 더 많이 수집하고 레이블링하면 되지 않을까? 그러나 현실적인 비용과 시간의 제약, 즉 ‘어노테이션 병목(Annotation Bottleneck)’ 현상이 이를 가로막는다.</p>
<h3>3.1  천문학적인 비용과 노동의 비효율성</h3>
<p>객체 탐지 모델을 학습시키기 위해서는 이미지 내의 모든 객체에 대해 정확한 바운딩 박스(Bounding Box)를 그리고 클래스 이름을 태깅하는 작업이 필수적이다. 최근 산업계의 분석에 따르면, 전문적인 품질 관리를 거친 바운딩 박스 하나를 생성하는 데 드는 비용은 약 $0.10에서 $0.50 수준으로 추산된다.</p>
<p>로봇이 마주칠 수 있는 100개의 새로운 카테고리를 추가한다고 가정해보자. 각 카테고리당 강건한 학습을 위해 최소 500장의 이미지가 필요하고, 이미지당 평균 3개의 객체가 존재한다고 할 때, 필요한 총 바운딩 박스의 수는 150,000개이다.<br />
<span class="math math-display">
\text{Total Cost} = 100 \text{ classes} \times 500 \text{ images} \times 3 \text{ boxes} \times \$0.30 \approx \$45,000
</span><br />
단 100개의 클래스를 추가하는 데만 약 45,000달러(한화 약 6천만 원)의 비용이 소요된다. 만약 로봇이 인간 수준의 어휘력(약 2만~3만 단어)을 갖추게 하려면, 그 비용은 수천만 달러 규모로 폭증한다. 비용뿐만 아니라, 수작업 레이블링에 소요되는 시간은 모델의 업데이트 주기를 늦추어, 급변하는 시장의 요구사항이나 새로운 트렌드(예: 신제품 출시)를 로봇이 즉각적으로 학습하는 것을 불가능하게 만든다.</p>
<h3>3.2  파국적 망각(Catastrophic Forgetting)의 딜레마</h3>
<p>비용을 들여 새로운 데이터를 구축했다 하더라도, 기존 모델에 이를 추가 학습시키는 과정에서 또 다른 문제가 발생한다. 바로 ‘파국적 망각(Catastrophic Forgetting)’ 현상이다. 딥러닝 모델은 새로운 지식(예: ‘드론’ 클래스)을 학습할 때, 기존에 학습했던 지식(예: ‘비행기’, ‘새’)을 잊어버리는 경향이 있다. 이를 방지하기 위해서는 새로운 데이터뿐만 아니라 과거의 모든 데이터를 합쳐서 모델 전체를 처음부터 다시 학습(Re-training)해야 한다. 이는 막대한 컴퓨팅 자원과 전력을 소모하며, 엣지 디바이스(Edge Device) 레벨에서 지속적인 학습(Continual Learning)을 수행해야 하는 로봇에게는 적합하지 않은 방법론이다. 결과적으로, 고정된 클래스 기반의 탐지기는 시간이 지날수록 변화하는 환경에 적응하지 못하고 도태될 수밖에 없는 운명을 가진다.</p>
<h2>4.  데이터 분포의 불균형: 롱테일(Long-Tail) 문제와 희귀 객체</h2>
<h3>4.1  지프의 법칙과 롱테일 분포의 현실</h3>
<p>현실 세계의 객체 빈도는 지프의 법칙(Zipf’s Law)을 따르는 전형적인 롱테일(Long-Tail) 분포를 보인다. 이는 소수의 ‘머리(Head)’ 클래스는 매우 빈번하게 등장하여 데이터가 풍부한 반면, 대다수의 ‘꼬리(Tail)’ 클래스는 등장 빈도가 매우 낮아 데이터 수집이 어렵다는 것을 의미한다.</p>
<p>COCO 데이터셋은 인위적으로 클래스 밸런스를 어느 정도 맞춘 데이터셋이지만, 보다 현실 세계를 반영한 LVIS(Large Vocabulary Instance Segmentation) 데이터셋이나 자율주행 데이터셋(nuScenes)을 분석해보면 이러한 불균형이 극명하게 드러난다. LVIS 데이터셋의 경우 1,200여 개의 클래스 중 상당수가 10장 미만의 학습 이미지만을 가지고 있다. 기존의 딥러닝 탐지기들은 데이터가 풍부한 머리 클래스(예: 사람, 자동차, 의자)에 편향되어 학습되며, 데이터가 부족한 꼬리 클래스(예: 휠체어, 유모차, 특정 공구, 희귀 동물 등)에 대해서는 재현율(Recall)과 정밀도(Precision)가 급격히 하락한다.</p>
<h3>4.2  로봇에게 꼬리 클래스가 중요한 이유: 안전과 직결된 희귀 케이스</h3>
<p>일반적인 웹 이미지 검색 서비스에서는 꼬리 클래스를 정확히 맞히지 못해도 사용자 경험에 큰 지장을 주지 않을 수 있다. 그러나 로봇 공학, 특히 안전이 중요한(Safety-Critical) 애플리케이션에서 꼬리 클래스는 생명과 직결되는 경우가 많다.</p>
<ul>
<li><strong>자율주행:</strong> 도로 위의 ’자동차’나 ’성인 보행자’는 데이터가 풍부한 머리 클래스이다. 그러나 ‘유모차를 끄는 사람’, ‘목발을 짚은 노인’, ‘도로 위에 떨어진 화물(Debris)’, ‘전복된 차량’ 등은 전형적인 꼬리 클래스에 속한다. 이러한 희귀 케이스(Corner Cases)를 탐지하지 못하면 치명적인 인명 사고로 이어진다.</li>
<li><strong>서비스 로봇:</strong> 가정용 로봇이 ’컵’이나 ’리모컨’은 잘 찾지만, 사용자가 긴급 상황에서 찾는 ‘천식 흡입기’, ‘인슐린 주사기’, ’지팡이’와 같은 꼬리 객체를 인식하지 못한다면, 그 로봇은 서비스 제공자로서의 본질적 가치를 상실한 것과 다름없다.</li>
</ul>
<p>기존 연구에서는 데이터 리샘플링(Re-sampling)이나 손실 함수 가중치 조절(Re-weighting) 기법을 통해 이러한 불균형을 완화하려 노력했으나, 근본적으로 데이터가 절대적으로 부족한 희귀 클래스에 대한 일반화 성능을 획기적으로 높이지는 못했다. 이는 소수의 샘플만으로도 새로운 개념을 학습하는 퓨샷 학습(Few-Shot Learning)이나 제로샷 학습(Zero-Shot Learning) 능력이 기존 아키텍처에는 부재하기 때문이다.</p>
<h2>5.  로봇 현장 적용 시 발생하는 도메인 격차(Domain Gap)</h2>
<p>객체 탐지 모델이 COCO와 같은 정제된 데이터셋에서 높은 mAP 점수를 기록하더라도, 실제 로봇에 탑재되었을 때 성능이 급락하는 주된 원인은 학습 데이터의 분포(Source Domain)와 실제 운영 환경의 분포(Target Domain)가 상이하여 발생하는 ‘도메인 격차(Domain Gap)’ 때문이다. 로봇 환경에서의 도메인 격차는 크게 시점(Viewpoint), 환경(Environment), 그리고 의미(Semantics)의 세 가지 차원에서 발생한다.</p>
<h3>5.1  정형화된 시점(Canonical View) vs. 로봇의 1인칭 시점(Egocentric View)</h3>
<p>대부분의 인터넷 기반 데이터셋(ImageNet, COCO)은 인간 사진가가 “객체가 잘 보이도록” 의도를 가지고 촬영한 사진들로 구성된다. 이를 **‘정형화된 시점(Canonical View)’**이라 한다. 이 시점에서는 객체가 이미지의 중앙에 위치하고, 적절한 크기이며, 조명도 양호하고, 가려짐(Occlusion)이 최소화되어 있다. 인간은 본능적으로 사물을 가장 잘 알아볼 수 있는 각도에서 사진을 찍기 때문이다.</p>
<p>반면, 로봇(휴머노이드, 매니퓰레이터, 이동 로봇 등)의 카메라는 로봇의 신체에 부착되어 세상을 바라보는 **‘1인칭 시점(Egocentric View)’**을 가진다. Ego4D와 같은 대규모 1인칭 비디오 데이터셋 연구 결과에 따르면, 로봇 시점에서는 다음과 같은 극단적인 시각적 난제가 발생하며, 이는 기존 탐지기의 성능을 무력화시킨다.</p>
<ul>
<li><strong>심각한 가려짐(Heavy Occlusion)과 자기 신체 간섭:</strong> 로봇이 물체를 조작(Manipulation)하기 위해 손을 뻗는 순간, 로봇의 팔이나 손(End-effector)이 카메라 시야를 가려 목표 물체의 상당 부분을 가리게 된다. 기존 탐지기는 물체 전체가 온전히 보이는 상황만 학습했으므로, 로봇 손에 의해 50% 이상 가려진 물체를 인식하지 못하거나, 로봇의 손을 물체의 일부로 오인하는 오류를 범한다.</li>
<li><strong>비정형적 앵글(Non-canonical Angles)과 급격한 시점 변화:</strong> 로봇은 물체를 위에서 수직으로 내려다보거나(Top-down view), 낮은 자세에서 올려다보는 등 인간의 사진 촬영 구도와는 전혀 다른 각도에서 사물을 관찰한다. 정면에서 찍은 ‘머그컵’ 사진만 학습한 모델은 위에서 본 동그라미 형태의 컵 내부를 컵으로 인식하지 못한다. 또한, 로봇의 머리나 몸통의 움직임에 따라 시점이 급격하게 변화하며, 이에 따른 객체의 외형 변화(Deformation)도 심각하다.</li>
<li><strong>객체 절단(Truncation)과 근접 촬영:</strong> 로봇이 작업 대상을 정밀하게 조작하기 위해 물체에 근접하면, 카메라의 화각(Field of View) 제한으로 인해 물체의 일부분만 화면에 잡히게 된다. 코끼리의 다리만 보고 코끼리임을 유추해야 하는 상황과 같다. 전체적인 형상(Shape) 정보에 의존하는 기존 CNN 기반 탐지기들은 이러한 부분적 정보(Partial Observation)만으로는 객체를 식별하는 데 큰 어려움을 겪는다.</li>
</ul>
<p>실제로 Ego4D 벤치마크 결과에 따르면, COCO 데이터셋에서 사전 학습된 Faster R-CNN 모델을 1인칭 비디오 데이터에 적용했을 때 mAP(mean Average Precision)가 39.8%에서 20.1%로 절반 가까이 급락하는 것으로 보고되었다. 이는 시점의 차이가 단순한 성능 하락이 아닌, 모델의 유효성을 파괴하는 수준의 근본적인 문제임을 시사한다.</p>
<h3>5.2  환경적 요인: 조명, 블러, 그리고 동적 변화</h3>
<p>실내외를 오가며 동작하는 로봇은 통제되지 않은 조명과 환경 조건에 노출된다. 이는 정적인 이미지 데이터셋과는 차원이 다른 시각적 노이즈를 유발한다.</p>
<ul>
<li><strong>모션 블러(Motion Blur):</strong> 로봇이 빠르게 회전하거나 주행할 때, 혹은 관찰 대상이 움직일 때 발생하는 모션 블러는 객체의 고주파 성분인 경계선(Edge)과 질감(Texture) 정보를 뭉개뜨린다. 기존 데이터셋은 주로 선명한 이미지만을 선별(Curation)하여 포함하므로, 심한 블러가 낀 이미지는 모델에게 낯선 도메인 밖(OOD) 데이터가 된다. 블러로 인해 특징맵(Feature Map)이 손상되면 탐지기는 객체의 존재 자체를 놓치거나(False Negative), 위치를 부정확하게 추정한다.</li>
<li><strong>조명 변화와 역광:</strong> 역광(Glare), 저조도(Low-light), 그림자 등은 객체의 색상과 형태 정보를 왜곡시킨다. 야간 주행 시 가로등 불빛에 반사된 젖은 노면을 장애물로 오인하거나, 어두운 터널에서 검은색 차량을 배경과 구분하지 못하는 경우가 이에 해당한다. 특히 RGB 센서에 의존하는 탐지기는 조명 변화에 매우 취약하며, 이는 로봇의 24시간 운영을 제한하는 요소가 된다.</li>
<li><strong>객체의 상태 변화(State Change):</strong> 요리 로봇이나 조립 로봇의 경우, 객체의 상태가 지속적으로 변화한다. ’양파’를 인식해야 하지만, 껍질을 벗긴 양파, 반으로 썰린 양파, 잘게 다져진 양파는 시각적으로 완전히 다른 특징을 가진다. 기존 데이터셋은 주로 ’온전한 상태’의 객체만을 레이블링하는 경향이 있어, 조작 과정에서 형태가 변형되는 객체의 상태 추적(State Tracking)에 실패한다.</li>
</ul>
<h3>5.3  Sim-to-Real Gap: 시뮬레이션 데이터의 한계</h3>
<p>데이터 부족 문제를 해결하기 위해 AI2-THOR나 Habitat와 같은 시뮬레이터를 활용하여 합성 데이터(Synthetic Data)를 생성하고 학습에 활용하는 연구가 활발하다. 그러나 시뮬레이션 이미지와 실제 이미지(Real Data) 사이에는 여전히 좁혀지지 않는 ’Sim-to-Real Gap’이 존재한다. 렌더링된 텍스처의 인위성, 물리 엔진의 단순화, 센서 노이즈 모델링의 부정확성 등은 시뮬레이션에서는 완벽하게 동작하던 탐지기가 현실 세계에서는 엉뚱한 물체를 잡으려 허공을 휘젓게 만드는 원인이 된다. 특히 배경(Background)의 복잡도와 조명 반사 특성을 완벽하게 모사하기 어렵기 때문에, 합성 데이터로만 학습된 모델은 실제 환경의 미세한 뉘앙스를 놓치기 쉽다.</p>
<h2>6.  온톨로지(Ontology)와 의미론적 격차: “의자“를 넘어서</h2>
<p>마지막으로, 기존 객체 탐지 시스템은 **‘기능적 의미(Affordance)’**가 아닌 **‘시각적 레이블(Label)’**에 갇혀 있다는 한계가 있다. ImageNet의 WordNet 기반 계층 구조는 생물학적, 어휘적 분류에는 적합할지 몰라도, 로봇의 기능적 임무 수행에는 적합하지 않을 수 있다.</p>
<p>예를 들어, 다리가 아픈 사람을 돕는 로봇에게 “앉을 수 있는 곳을 찾아라“라고 명령했을 때, 기존 탐지기는 ‘의자(Chair)’, ‘소파(Sofa)’, ’벤치(Bench)’라는 학습된 텍스트 레이블에 매몰되어, 이 객체들만을 찾으려 할 것이다. 하지만 주변에 의자가 없다면 튼튼한 ’나무 상자’나 평평한 ‘바위’, 혹은 ’낮은 담장’도 앉을 수 있는 기능을 제공하는 훌륭한 대안이 될 수 있다.</p>
<p>기존의 닫힌 클래스 집합에 갇힌 모델은 이러한 **기능적 대체물(Affordance-based Objects)**을 인식할 수 있는 유연성이 없다. “이것은 상자이지만, 튼튼하고 평평하므로 앉을 수 있다“는 추론은 단순한 시각적 패턴 매칭을 넘어, 객체의 속성과 맥락을 이해하는 고차원적인 인지 능력을 요구한다. 이는 시각 정보와 언어적/상식적 지식이 통합되지 않은 기존 단일 모달(Uni-modal) 탐지기의 근본적인 한계점이다.</p>
<h2>7.  결론: 패러다임 전환의 필연성</h2>
<p>요약하자면, 기존의 지도 학습 기반 객체 탐지 기술은 <strong>(1) 닫힌 어휘(Closed Vocabulary)로 인한 확장성 부재와 미지 객체에 대한 취약성</strong>, <strong>(2) 막대한 어노테이션 비용과 경제적 비효율성</strong>, <strong>(3) 롱테일 분포로 인한 희귀 객체 탐지 실패</strong>, 그리고 <strong>(4) 정형화된 데이터와 거친 로봇 현실 간의 도메인 격차</strong>라는 사중고(四重苦)에 직면해 있다. 이러한 한계들은 단순히 데이터를 더 많이 모으거나 모델의 레이어를 늘리는 양적 팽창만으로는 해결할 수 없는 구조적인 문제들이다.</p>
<p>따라서 로봇이 진정한 의미의 오픈 월드 환경에서 자율성을 확보하고 인간과 자연스럽게 상호작용하기 위해서는, 고정된 클래스의 제약을 넘어 언어와 비전이 통합된(Vision-Language Aligned) **오픈 어휘 객체 탐지(Open-Vocabulary Object Detection)**로의 과감한 패러다임 전환이 필수적이다. 제미나이(Gemini)와 같은 대규모 멀티모달 모델(LMM)은 방대한 웹 데이터를 통해 학습된 범용적인 지식과 언어적 추론 능력을 바탕으로, 본 적 없는 물체를 묘사(Description)만으로 찾아내고, 맥락에 따라 유연하게 객체의 기능을 유추함으로써 이러한 전통적 한계를 극복할 수 있는 열쇠를 제공한다.</p>
<p>이어지는 장에서는 이러한 제미나이 기반의 탐지 기술이 어떻게 앞서 제기된 문제들을 구체적으로 해결하고, 로봇의 인지 능력을 혁신하여 ’보는 로봇’에서 ’이해하는 로봇’으로의 진화를 가능케 하는지 심도 있게 다룰 것이다.</p>
<h3>7.1 [표 2] 기존 객체 탐지(Closed-Set) vs. 제미나이 기반 탐지(Open-Vocabulary) 비교 요약</h3>
<table><thead><tr><th><strong>구분</strong></th><th><strong>기존 객체 탐지 (Traditional OD)</strong></th><th><strong>제미나이 기반 탐지 (Gemini/VLM)</strong></th></tr></thead><tbody>
<tr><td><strong>인식 대상</strong></td><td>학습된 <span class="math math-inline">N</span>개 클래스 (고정, 유한)</td><td>언어로 표현 가능한 모든 객체 (가변, 무한)</td></tr>
<tr><td><strong>미지 객체 대응</strong></td><td>’배경’으로 무시하거나 오분류 (취약)</td><td>’Unknown’으로 인식하거나 유사 개념으로 추론 (유연)</td></tr>
<tr><td><strong>학습 방식</strong></td><td>정답 박스(BBox) 지도 학습 (비용 <span class="math math-inline">\uparrow</span>)</td><td>이미지-텍스트 쌍(Image-Text Pair) 자가 지도 학습 (비용 <span class="math math-inline">\downarrow</span>)</td></tr>
<tr><td><strong>데이터 효율성</strong></td><td>롱테일 클래스 성능 저하 심각 (빈익빈 부익부)</td><td>제로샷/퓨샷 능력으로 희귀 객체 인식 우수</td></tr>
<tr><td><strong>도메인 적응</strong></td><td>별도의 Fine-tuning 필수 (Catastrophic Forgetting 위험)</td><td>강건한 일반화 성능(Zero-shot Transfer)으로 도메인 격차 완화</td></tr>
<tr><td><strong>로봇 상호작용</strong></td><td>단순 레이블 출력 (“Cup”)</td><td>문맥적 이해 및 추론 (“깨지기 쉬운 컵”, “손잡이가 있는 컵”)</td></tr>
<tr><td><strong>시점(Viewpoint)</strong></td><td>정형화된 시점에 과적합 (1인칭 시점 성능 급락)</td><td>다양한 웹 데이터 학습으로 시점 변화에 상대적 강건</td></tr>
</tbody></table>
<h2>8. 참고 자료</h2>
<ol>
<li>The Overlooked Elephant of Object Detection: Open Set, https://openaccess.thecvf.com/content_WACV_2020/papers/Dhamija_The_Overlooked_Elephant_of_Object_Detection_Open_Set_WACV_2020_paper.pdf</li>
<li>Uncertainty for Identifying Open-Set Errors in Visual Object Detection, https://arxiv.org/pdf/2104.01328</li>
<li>Towards Open Vocabulary Learning: A Survey - arXiv, https://arxiv.org/pdf/2306.15880</li>
<li>Small Object Detection in Traffic Scenes for Mobile Robots - MDPI, https://www.mdpi.com/2079-9292/14/13/2614</li>
<li>Comparison of class distributions of the COCO and LVIS datasets …, https://www.researchgate.net/figure/Comparison-of-class-distributions-of-the-COCO-and-LVIS-datasets-LVIS-has-a-much-larger_fig7_363633338</li>
<li>Towards Evidential and Class Separable Open Set Object Detection, https://ojs.aaai.org/index.php/AAAI/article/view/28367/28719</li>
<li>YOLOv4: A Breakthrough in Real-Time Object Detection - arXiv, https://arxiv.org/html/2502.04161v1</li>
<li>(PDF) Improving Classification Performance of Softmax Loss …, https://www.researchgate.net/publication/340915779_Improving_Classification_Performance_of_Softmax_Loss_Function_Based_on_Scalable_Batch-Normalization</li>
<li>4.1. Softmax Regression - Dive into Deep Learning, https://d2l.ai/chapter_linear-classification/softmax-regression.html</li>
<li>Weakly Supervised Open-Vocabulary Object Detection, https://ojs.aaai.org/index.php/AAAI/article/view/28127/28257</li>
<li>Gu 等 - 2021 - Open-Vocabulary Object Detection via Vision and …, <a href="https://www.scribd.com/document/905951630/Gu-%E7%AD%89-2021-Open-Vocabulary-Object-Detection-via-Vision-and-Language-Knowledge-Distillation">https://www.scribd.com/document/905951630/Gu-%E7%AD%89-2021-Open-Vocabulary-Object-Detection-via-Vision-and-Language-Knowledge-Distillation</a></li>
<li>When Does Supervised Training Pay Off? The Hidden Economics of …, https://arxiv.org/html/2510.11302v2</li>
<li>Open-Vocabulary Object Detection Using Captions, https://openaccess.thecvf.com/content/CVPR2021/papers/Zareian_Open-Vocabulary_Object_Detection_Using_Captions_CVPR_2021_paper.pdf</li>
<li>Towards Learning Object Detectors with Limited Data for Industrial …, https://library.oapen.org/bitstream/handle/20.500.12657/100728/towards-learning-object-detectors-with-limited-data-for-industrial-applications.pdf?sequence=1&amp;isAllowed=y</li>
<li>A Long-Tailed Image Classification Method Based on Enhanced …, https://www.mdpi.com/1424-8220/23/15/6694</li>
<li>Exploring Classification Equilibrium in Long-Tailed Object Detection, https://openaccess.thecvf.com/content/ICCV2021/papers/Feng_Exploring_Classification_Equilibrium_in_Long-Tailed_Object_Detection_ICCV_2021_paper.pdf</li>
<li>Towards Long-Tailed 3D Detection, https://proceedings.mlr.press/v205/peri23a/peri23a.pdf</li>
<li>Semi-Supervised and Long-Tailed Object Detection with … - DR-NTU, https://dr.ntu.edu.sg/bitstreams/beffc2b0-994d-4076-883b-4bdcc7a1fb24/download</li>
<li>A Simple Framework for Long-tail Instance Segmentation, https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590715.pdf</li>
<li>Trustworthy Long-Tailed Classification - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Trustworthy_Long-Tailed_Classification_CVPR_2022_paper.pdf</li>
<li>How Important are Data Augmentations to Close the Domain Gap for …, https://elib.dlr.de/220493/2/2410.15766v1_copyright.pdf</li>
<li>From Sharp to Blur: Unsupervised Domain Adaptation for 2D Human …, https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_From_Sharp_to_Blur_Unsupervised_Domain_Adaptation_for_2D_Human_ICCV_2025_paper.pdf</li>
<li>Balancing Domain Gap for Object Instance Detection* - ResearchGate, https://www.researchgate.net/publication/336084310_Balancing_Domain_Gap_for_Object_Instance_Detection/fulltext/5d8d7ef3458515202b6cf158/Balancing-Domain-Gap-for-Object-Instance-Detection.pdf</li>
<li>Generalization between canonical and non-canonical views in …, https://jov.arvojournals.org/article.aspx?articleid=2121074</li>
<li>Generalization between canonical and non-canonical views in …, https://pmc.ncbi.nlm.nih.gov/articles/PMC3586995/</li>
<li>To Overcome Limitations of Computer Vision Datasets, https://escholarship.org/uc/item/9bd2t9hf</li>
<li>Ego4D: Around the World in 3,600 Hours of Egocentric Video, https://ieeexplore.ieee.org/iel8/34/11192800/10611736.pdf</li>
<li>Revisiting 3D Object Detection From an Egocentric Perspective, https://proceedings.neurips.cc/paper/2021/file/db182d2552835bec774847e06406bfa2-Paper.pdf</li>
<li>Is Tracking really more challenging in First Person Egocentric Vision?, https://arxiv.org/html/2507.16015v1</li>
<li>A 4D Egocentric Dataset for Category-Level Human-Object Interaction, https://hoi4d.github.io/HOI4D_cvpr2022.pdf</li>
<li>Learning Egocentric In-Hand Object Segmentation through Weak …, https://arxiv.org/html/2509.26004v1</li>
<li>Real-time Hand Tracking under Occlusion from an Egocentric RGB …, https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/content/OccludedHands_ICCV2017.pdf</li>
<li>Egocentric Hand Track and Object-Based Human Action Recognition, https://www.researchgate.net/publication/332801508_Egocentric_Hand_Track_and_Object-Based_Human_Action_Recognition</li>
<li>Ego4D: Around the World in 3,600 Hours of Egocentric Video, https://www.computer.org/csdl/journal/tp/2025/11/10611736/1YTJvDcduIE</li>
<li>A Modular Object Detection System for Humanoid Robots Using YOLO, https://arxiv.org/html/2510.13625v1</li>
<li>FDC-YOLO: A Blur-Resilient Lightweight Network for Engine Blade …, https://www.mdpi.com/1999-4893/18/11/725</li>
<li>Introspective Perception through Identifying Blur, Light Direction …, https://www.ri.cmu.edu/app/uploads/2022/08/MSR_MTH_Thesis_for_publication.pdf</li>
<li>Egocentric 4D Perception (EGO4D), https://ego4d-data.org/</li>
<li>EGO4D/hands-and-objects - GitHub, https://github.com/EGO4D/hands-and-objects</li>
<li>Investigating Domain Gaps for Indoor 3D Object Detection, https://openreview.net/forum?id=g7xZkiHcGO</li>
<li>A survey of ontology-enabled processes for dependable robot …, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2024.1377897/full</li>
<li>Ontology of Visual Objects - ACL Anthology, https://aclanthology.org/2022.clib-1.14.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>