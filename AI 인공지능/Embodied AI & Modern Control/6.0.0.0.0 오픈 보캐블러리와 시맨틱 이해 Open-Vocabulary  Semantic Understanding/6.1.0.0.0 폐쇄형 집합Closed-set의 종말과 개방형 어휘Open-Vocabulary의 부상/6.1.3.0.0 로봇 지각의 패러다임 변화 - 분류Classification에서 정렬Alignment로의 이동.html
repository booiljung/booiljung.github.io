<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:6.1.3 로봇 지각의 패러다임 변화: '분류(Classification)'에서 '정렬(Alignment)'로의 이동.</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>6.1.3 로봇 지각의 패러다임 변화: '분류(Classification)'에서 '정렬(Alignment)'로의 이동.</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 6. 오픈 보캐블러리와 시맨틱 이해 (Open-Vocabulary & Semantic Understanding)</a> / <a href="index.html">6.1 폐쇄형 집합(Closed-set)의 종말과 개방형 어휘(Open-Vocabulary)의 부상</a> / <span>6.1.3 로봇 지각의 패러다임 변화: '분류(Classification)'에서 '정렬(Alignment)'로의 이동.</span></nav>
                </div>
            </header>
            <article>
                <h1>6.1.3 로봇 지각의 패러다임 변화: ’분류(Classification)’에서 ’정렬(Alignment)’로의 이동.</h1>
<h2>1.  서론: 지각(Perception)의 존재론적 전환</h2>
<p>로봇 공학의 역사에서 ’지각(Perception)’은 오랫동안 ’상태 추정(State Estimation)’과 ’객체 식별(Object Identification)’이라는 두 가지 축을 중심으로 발전해 왔다. 특히 시각적 인지(Visual Perception) 분야에서 지난 10년은 심층 합성곱 신경망(CNN)의 비약적인 발전에 힘입어, 입력된 이미지를 사전에 정의된 이산적인 카테고리(Discrete Categories) 중 하나로 매핑하는 ‘분류(Classification)’ 패러다임이 지배적이었다. 이 시기의 로봇은 “이것은 무엇인가?“라는 질문에 대해, 인간이 미리 정해놓은 <span class="math math-inline">N</span>개의 정답지(Label Set) 중에서 확률적으로 가장 높은 것을 선택하는 방식으로 대답했다. 이는 로봇이 인식할 수 있는 세계를 학습 데이터셋의 클래스 목록(예: COCO 80개 클래스, ImageNet 1000개 클래스)으로 제한하는 ’닫힌 세계(Closed World)’의 가정을 내포하고 있었다.</p>
<p>그러나 2020년대 들어 로봇 지각 시스템은 근본적인 패러다임의 변화를 맞이하고 있다. 이는 대규모 파운데이션 모델(Foundation Models)과 멀티모달 학습(Multimodal Learning)의 등장으로 촉발되었으며, 로봇이 세상을 이해하는 방식을 ’고정된 클래스로의 분류’에서 ’시각 정보와 언어 정보의 의미론적 정렬(Semantic Alignment)’로 이동시켰다. 이제 로봇은 “이것은 컵인가?“라고 묻는 대신, “입력된 시각적 특징은 ’내 약이 들어있는 파란색 서랍’이라는 언어적 설명과 얼마나 유사한가?“를 묻는다. 이 변화는 단순한 알고리즘의 교체를 넘어, 로봇에게 미지(Unknown)의 객체를 인식하고, 자연어 명령의 맥락을 이해하며, 별도의 재학습 없이도 새로운 환경에 적응할 수 있는 <strong>‘개방형 어휘(Open-Vocabulary)’</strong> 능력과 <strong>‘제로샷(Zero-shot)’</strong> 인지 능력을 부여하는 혁명적 전환점이다.</p>
<p>본 장에서는 이러한 패러다임 변화의 수학적, 구조적 원리를 심층적으로 분석한다. Softmax 기반의 고전적 분류기가 가지는 본질적 한계를 고찰하고, 이를 대체하는 대조 학습(Contrastive Learning)과 멀티모달 정렬 아키텍처(CLIP, GLIP, Grounding DINO, OWL-ViT 등)의 작동 메커니즘을 상세히 기술한다. 나아가 이러한 기술이 실제 로봇 시스템(내비게이션, 조작, 엣지 컴퓨팅)에 적용될 때 발생하는 공학적 과제와 해결책을 최신 SOTA(State-of-the-Art) 연구 결과에 기반하여 논한다.</p>
<h2>2.  분류(Classification) 패러다임의 유산과 한계</h2>
<p>로봇 비전의 전통적인 접근 방식은 지도 학습(Supervised Learning)에 기반한 분류 모델이었다. ResNet, EfficientNet, 그리고 초기 YOLO 시리즈와 같은 모델들은 로봇의 눈 역할을 충실히 수행해 왔으나, 비정형 환경(Unstructured Environment)에서 활동해야 하는 현대 로봇의 요구사항을 충족시키기에는 구조적 한계에 봉착했다.</p>
<h3>2.1  Softmax 함수와 상호 배타성의 딜레마</h3>
<p>분류 모델의 최종단에는 언제나 Softmax 함수가 존재한다. 입력 이미지 <span class="math math-inline">x</span>가 주어졌을 때, 모델은 특징 추출기(Backbone)를 통해 고차원 특징 맵을 생성하고, 이를 완전 연결 층(Fully Connected Layer)을 거쳐 클래스별 로짓(Logit) 벡터 <span class="math math-inline">z = [z_1, z_2, \dots, z_N]</span>를 출력한다. Softmax는 이를 확률 분포 <span class="math math-inline">p</span>로 변환한다.<br />
<span class="math math-display">
p_i = \text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{N} e^{z_j}}
</span><br />
이 수식은 로봇 지각에 있어 두 가지 치명적인 전제를 강요한다.</p>
<ol>
<li><strong>상호 배타성(Mutually Exclusive):</strong> Softmax는 모든 클래스 확률의 합이 1이 되도록 강제한다. 이는 특정 객체가 ’A’이면서 동시에 ’B’일 수 없음을 의미한다. 그러나 현실 세계의 객체는 다중 속성을 가진다. “흰색”, “세라믹”, “손잡이가 있는”, “컵“이라는 속성은 동시에 존재할 수 있다. 분류 모델은 이를 “흰색 컵“이라는 하나의 클래스로 정의하거나, 각각의 속성을 별도의 분류기로 학습시켜야 하는 비효율성을 초래한다.</li>
<li><strong>닫힌 집합(Closed Set):</strong> 분모의 <span class="math math-inline">\sum_{j=1}^{N}</span>은 모델이 인식할 수 있는 세계가 <span class="math math-inline">N</span>개의 클래스로 한정됨을 수학적으로 명시한다. 학습 단계에서 정의되지 않은 <span class="math math-inline">N+1</span>번째 객체가 등장하면, 모델은 이를 가장 유사한 기존 클래스로 잘못 분류하거나(Overconfident Error), 배경으로 무시해야 한다.</li>
</ol>
<h3>2.2  재학습 병목(Retraining Bottleneck) 현상</h3>
<p>로봇이 가정 내에서 “텀블러“를 인식해야 하는데, 기존 모델이 “컵“만 학습했다면 어떤 일이 벌어지는가? 전통적인 분류 패러다임에서는 다음의 과정을 거쳐야 한다.</p>
<ol>
<li>“텀블러” 이미지를 수천 장 수집한다.</li>
<li>사람이 일일이 바운딩 박스와 레이블을 주석(Annotation)한다.</li>
<li>기존 데이터셋에 이를 병합하고 모델 구조(출력 노드 수)를 변경한다.</li>
<li>전체 모델을 처음부터 다시 학습(Retraining)하거나 미세 조정(Fine-tuning)한다.</li>
</ol>
<p>이러한 ’재학습 병목’은 로봇의 확장성을 가로막는 가장 큰 장벽이었다. 로봇이 새로운 환경에 배치될 때마다 수주에서 수개월의 재학습 기간이 소요된다면, 진정한 의미의 자율 로봇이라 할 수 없다. 이러한 한계를 극복하기 위해 등장한 것이 바로 시각과 언어를 하나의 공간에 정렬하는 새로운 패러다임이다.</p>
<h2>3.  정렬(Alignment) 기반 지각의 이론적 토대</h2>
<p>정렬 패러다임은 “무엇인가?“라는 존재론적 질문을 “어떤 설명과 일치하는가?“라는 관계론적 질문으로 전환한다. 이 접근법의 핵심은 서로 다른 양상(Modality)인 이미지와 텍스트를 동일한 고차원 벡터 공간(Latent Space)에 매핑하고, 두 벡터 사이의 거리(Distance)나 유사도(Similarity)를 통해 인식을 수행하는 것이다.</p>
<h3>3.1  대조 학습(Contrastive Learning)과 InfoNCE 손실</h3>
<p>이 패러다임의 수학적 엔진은 대조 학습이다. 대조 학습은 정답 레이블을 예측하는 대신, 데이터 간의 관계를 학습한다. 대표적으로 CLIP(Contrastive Language-Image Pre-training) 모델은 수억 쌍의 이미지-텍스트 쌍 <span class="math math-inline">(I_i, T_i)</span>에 대해, 올바른 쌍(Positive Pair)의 임베딩 벡터 내적은 최대화하고, 틀린 쌍(Negative Pair)의 내적은 최소화하도록 학습된다.</p>
<p>이를 구현하는 손실 함수인 InfoNCE(Information Noise Contrastive Estimation) Loss는 다음과 같이 정의된다.<br />
<span class="math math-display">
L_i^{(v \to t)} = -\log \frac{\exp(\langle v_i, t_i \rangle / \tau)}{\sum_{j=1}^{B} \exp(\langle v_i, t_j \rangle / \tau)}
</span><br />
여기서 <span class="math math-inline">v_i, t_i</span>는 각각 <span class="math math-inline">L2</span> 정규화된 이미지 및 텍스트 임베딩 벡터이며, <span class="math math-inline">\langle \cdot, \cdot \rangle</span>은 코사인 유사도(Cosine Similarity)를 나타내는 내적 연산이다. <span class="math math-inline">\tau</span>는 온도 매개변수(Temperature Parameter)로, 분포의 뾰족한 정도를 조절한다.</p>
<p>이 수식에서 주목해야 할 점은 분모의 합이 전체 클래스 <span class="math math-inline">N</span>이 아니라, 현재 미니 배치(Mini-batch) 내의 샘플 개수 <span class="math math-inline">B</span>라는 점이다. 이는 학습의 대상이 고정된 상수가 아니라, 입력 데이터 자체(텍스트)가 됨을 의미한다. 따라서 로봇은 학습되지 않은 새로운 단어(클래스)가 입력되더라도, 텍스트 인코더를 통해 생성된 벡터 <span class="math math-inline">t_{new}</span>와 시각 벡터 <span class="math math-inline">v</span>를 비교함으로써 즉각적인 인식이 가능하다. 이것이 바로 <strong>제로샷(Zero-shot)</strong> 인식의 원리이다.</p>
<h3>3.2  Sigmoid Focal Loss와 개방형 탐지의 수학</h3>
<p>객체 탐지(Object Detection)로 넘어오면 문제는 더욱 복잡해진다. 하나의 이미지 안에 수많은 객체가 존재하고, 이들은 서로 겹치거나 다양한 크기로 나타난다. 기존 DETR이나 YOLO 등의 탐지기는 Softmax Cross-Entropy를 사용하여 각 바운딩 박스의 클래스를 결정했다. 그러나 개방형 어휘 탐지(Open-Vocabulary Object Detection, OVOD)에서는 <strong>Sigmoid Focal Loss</strong>가 표준으로 자리 잡았다.<br />
<span class="math math-display">
L_{FL}(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)
</span><br />
여기서 <span class="math math-inline">p_t</span>는 모델이 예측한 확률(Sigmoid 출력)이다. Sigmoid 함수 <span class="math math-inline">\sigma(x) = 1/(1+e^{-x})</span>를 사용함으로써 각 클래스(또는 텍스트 쿼리)에 대한 확률을 독립적으로 계산한다. 이는 하나의 객체가 “사람”, “남자”, “파란 옷을 입은 사람” 등 여러 텍스트 쿼리와 동시에 정렬될 수 있음을 수학적으로 허용한다.</p>
<p>또한 Focal Term <span class="math math-inline">(1 - p_t)^\gamma</span>는 모델이 이미 잘 정렬된(확신하는) 샘플에 대한 손실 기여도를 낮추어, 학습이 어려운 샘플(Hard Example)이나 희소한 객체(Rare Category)에 집중하게 만든다. 이는 로봇이 수집한 데이터의 롱테일(Long-tail) 분포 문제를 해결하는 데 결정적인 역할을 한다.</p>
<h3>3.3  정렬의 기하학: 구형 공간(Hypersphere)</h3>
<p>정렬 학습이 완료된 임베딩 공간은 초구(Hypersphere) 형태를 띤다. 모든 벡터가 정규화되어 있기 때문에 벡터의 크기는 무시되고 방향만이 의미를 가진다. 이 공간에서 의미론적 연산이 가능해진다.</p>
<ul>
<li><strong>합성성(Compositionality):</strong> <span class="math math-inline">\vec{v}(\text{&quot;빨간색&quot;}) + \vec{v}(\text{&quot;블록&quot;}) \approx \vec{v}(\text{&quot;빨간 블록&quot;})</span></li>
</ul>
<p>로봇은 “빨간색“이라는 시각적 개념과 “블록“이라는 형상적 개념을 각각 학습한 뒤, 이를 결합하여 한 번도 본 적 없는 “빨간 블록“을 인식할 수 있다. 이는 기존 분류 패러다임에서는 불가능했던 추론 능력이다.</p>
<h2>4.  핵심 아키텍처 및 기술 심층 분석</h2>
<p>정렬 패러다임은 이미지 전체 수준(Image-level)의 정렬에서 시작하여, 점차 객체 및 픽셀 수준(Region/Pixel-level)의 정밀한 정렬로 진화해 왔다. 로봇 지각을 위한 최신 SOTA 모델들을 분석한다.</p>
<h3>4.1  파운데이션 모델: CLIP과 ALIGN</h3>
<p>이 변화의 시발점이 된 것은 OpenAI의 CLIP과 Google의 ALIGN이다.</p>
<h4>4.1.1  CLIP (Contrastive Language-Image Pre-training)</h4>
<p>CLIP은 인터넷에서 수집한 4억 개의 이미지-텍스트 쌍(WebImageText)으로 학습되었다.</p>
<ul>
<li><strong>아키텍처:</strong> 듀얼 인코더 구조를 채택했다. 이미지 인코더로는 ResNet-50 또는 ViT(Vision Transformer)를, 텍스트 인코더로는 Transformer를 사용한다.</li>
<li><strong>학습 전략:</strong> 처음부터 끝까지(End-to-End) 대조 손실(Contrastive Loss)로만 학습된다. 생성 모델이나 캡션 모델과 달리, 오직 “이 이미지가 이 텍스트와 짝인가?“만을 판별한다.</li>
<li><strong>로봇 응용:</strong> CLIP은 로봇에게 ’시각적 상식’을 제공한다. 로봇은 CLIP을 통해 “바나나는 노란색이고 구부러져 있다“는 명시적 규칙 대신, 수많은 바나나 이미지와 텍스트의 상관관계를 통해 개념을 익힌다.</li>
</ul>
<h4>4.1.2  ALIGN (Scaling Up with Noisy Text Supervision)</h4>
<p>ALIGN은 데이터 정제의 필요성을 제거하고 규모의 경제를 입증했다.</p>
<ul>
<li><strong>데이터:</strong> 18억 개 이상의 노이즈가 포함된 이미지-Alt 텍스트 쌍을 사용했다. 필터링을 최소화하여 데이터 수집 비용을 극단적으로 낮췄다.</li>
<li><strong>성능:</strong> 노이즈가 많은 데이터라도 그 양이 충분히 많으면(10억 개 이상), 정렬 모델은 강건한(Robust) 특징을 학습할 수 있음을 보여주었다. 이는 로봇이 실제 환경에서 마주치는 불완전한 데이터들을 처리하는 데 있어 중요한 시사점을 준다.</li>
</ul>
<table><thead><tr><th><strong>모델</strong></th><th><strong>CLIP (OpenAI)</strong></th><th><strong>ALIGN (Google)</strong></th></tr></thead><tbody>
<tr><td><strong>학습 데이터</strong></td><td>4억 쌍 (큐레이션됨)</td><td>18억 쌍 (노이즈 포함)</td></tr>
<tr><td><strong>이미지 인코더</strong></td><td>ViT, ResNet</td><td>EfficientNet-L2</td></tr>
<tr><td><strong>텍스트 인코더</strong></td><td>Transformer</td><td>BERT-Large</td></tr>
<tr><td><strong>핵심 기여</strong></td><td>제로샷 전이, 프롬프트 엔지니어링</td><td>데이터 규모에 의한 노이즈 극복</td></tr>
<tr><td><strong>로봇 적합성</strong></td><td>명확한 지시 이해, HRI</td><td>다양한 환경 적응, 강건성</td></tr>
</tbody></table>
<p>(표 6.1.3-1: 대표적인 비전-언어 정렬 파운데이션 모델 비교 )</p>
<h3>4.2  영역-단어 정렬(Region-Word Alignment): GLIP</h3>
<p>CLIP은 이미지 전체를 하나의 벡터로 요약하기 때문에, 로봇이 물체를 집거나(Picking) 피해야 할 때 필요한 정확한 위치 정보(Localization)를 제공하는 데는 한계가 있다. GLIP(Grounded Language-Image Pre-training)은 이를 해결하기 위해 객체 탐지를 ‘구문 접지(Phrase Grounding)’ 문제로 재정의했다.</p>
<ul>
<li><strong>심층 융합(Deep Fusion):</strong> CLIP은 이미지와 텍스트가 마지막에 한 번 만나는 ‘Late Fusion’ 방식인 반면, GLIP은 인코더의 초기 단계부터 크로스 어텐션(Cross Attention)을 통해 정보를 교환하는 ‘Early Fusion’ 방식을 사용한다. 이를 통해 “고양이 옆의 강아지“와 같이 문맥에 의존적인 시각적 특징을 더 잘 추출한다.</li>
<li><strong>손실 함수:</strong> GLIP은 영역-단어 정렬 손실(Region-Word Alignment Loss)을 도입했다. 바운딩 박스 영역 특징 <span class="math math-inline">O</span>와 텍스트 토큰 특징 <span class="math math-inline">W</span> 간의 유사도 점수 <span class="math math-inline">S = O \cdot W^T</span>를 계산하고, 이에 대해 이진 분류를 수행한다.</li>
<li><strong>데이터 효율성:</strong> GLIP은 탐지 데이터(Detection Data)와 접지 데이터(Grounding Data)를 동시에 사용하여, 적은 데이터로도 높은 제로샷 탐지 성능을 달성했다. 로봇은 GLIP을 통해 “왼쪽 테이블 위의 빨간 사과“와 같은 복잡한 지시를 위치 좌표로 변환할 수 있다.</li>
</ul>
<h3>4.3  Grounding DINO: Transformer 기반의 개방형 탐지</h3>
<p>Grounding DINO는 현존하는 가장 강력한 개방형 객체 탐지 모델 중 하나로, DETR(Detection Transformer) 아키텍처인 DINO에 언어적 입력을 통합했다.</p>
<ul>
<li><strong>아키텍처:</strong></li>
</ul>
<ol>
<li><strong>Backbone:</strong> 이미지(Swin Transformer)와 텍스트(BERT) 특징을 추출한다.</li>
<li><strong>Feature Enhancer:</strong> 크로스 모달리티 어텐션을 통해 이미지와 텍스트 특징을 융합한다.</li>
<li><strong>Language-Guided Query Selection:</strong> 텍스트와 가장 관련성이 높은 이미지 영역을 선택하여 디코더의 쿼리(Query)로 초기화한다. 이는 탐지기가 텍스트와 무관한 객체를 탐지하느라 리소스를 낭비하는 것을 방지한다.</li>
<li><strong>Cross-Modality Decoder:</strong> 바운딩 박스를 정제한다.</li>
</ol>
<ul>
<li>
<p><strong>대조적 매칭(Contrastive Matching):</strong> 학습 시 헝가리안 매칭(Hungarian Matching) 알고리즘을 사용할 때, 클래스 일치 여부 대신 언어적 유사도를 비용 함수(Cost Function)에 포함시킨다.<br />
<span class="math math-display">
L_{match} = \lambda_{cls} L_{focal} + \lambda_{L1} L_{1} + \lambda_{GIOU} L_{GIOU}
</span><br />
여기서 <span class="math math-inline">L_{focal}</span>은 예측된 박스와 텍스트 쿼리 간의 정렬 점수에 대한 초점 손실이다.</p>
</li>
<li>
<p><strong>성능:</strong> COCO 및 LVIS 제로샷 벤치마크에서 SOTA를 기록했으며, 특히 텍스트 프롬프트에 민감하게 반응하여 로봇이 사용자의 의도를 정확히 파악하는 데 유리하다.</p>
</li>
</ul>
<h3>4.4  OWL-ViT (Open-World Localization Vision Transformer)</h3>
<p>Google의 OWL-ViT는 표준 ViT를 최소한으로 수정하여 개방형 탐지기로 전환한 모델이다.</p>
<ul>
<li><strong>구조적 단순함:</strong> CLIP으로 사전 학습된 ViT에서 마지막 풀링 레이어(Token Pooling)를 제거하고, 각 이미지 패치 토큰에 직접 경량화된 MLP 헤드를 부착하여 바운딩 박스와 클래스 임베딩을 예측한다. 이는 ViT가 이미 공간적 정보를 잘 보존하고 있다는 발견에 기반한다.</li>
<li><strong>이미지 조건부 탐지(Image-Conditioned Detection):</strong> OWL-ViT의 가장 큰 특징은 텍스트 쿼리뿐만 아니라, 이미지 쿼리(Image Query)를 사용할 수 있다는 점이다. 로봇에게 “나사를 찾아라“라고 말하는 대신, 나사 이미지를 보여주면(One-shot), 로봇은 시각적 유사도를 기반으로 환경 내의 모든 나사를 찾아낸다. 이는 제조 현장이나 물류 창고와 같이 부품의 이름이 모호한 경우에 매우 강력한 도구가 된다.</li>
</ul>
<h3>4.5  YOLO-World: 실시간성의 확보</h3>
<p>트랜스포머 기반 모델들은 높은 정확도를 보이지만 연산량이 많아 실시간 제어가 필요한 로봇에 적용하기 어렵다는 단점이 있다. YOLO-World는 이러한 문제를 해결하기 위해 등장했다.</p>
<ul>
<li><strong>RepVL-PAN:</strong> 비전-언어 경로 통합 네트워크(Vision-Language Path Aggregation Network)를 재파라미터화(Re-parameterization) 가능한 구조로 설계하여, 추론 시에는 언어 인코더를 분리하고 시각적 가중치만을 남겨(Offline Vocabulary) 연산 속도를 비약적으로 높였다.</li>
<li><strong>성능:</strong> 엣지 디바이스에서도 높은 FPS(초당 프레임 수)를 유지하면서 개방형 탐지를 수행할 수 있어, 자율 주행 로봇이나 드론과 같이 빠른 반응 속도가 필수적인 시스템에 적합하다.</li>
</ul>
<h2>5.  로봇 시스템에서의 구현 및 응용</h2>
<p>이론적으로 우수한 정렬 모델들을 실제 로봇 시스템에 통합하는 과정에는 구체적인 공학적 과제들이 존재한다. 여기서는 내비게이션, 조작(Manipulation), 그리고 엣지 컴퓨팅 구현에 대해 다룬다.</p>
<h3>5.1  제로샷 객체 목표 내비게이션 (Zero-Shot Object Goal Navigation, ZS-OGN)</h3>
<p>ZS-OGN은 로봇에게 “소파를 찾아라“와 같은 명령을 내렸을 때, 로봇이 ’소파’에 대한 사전 학습 데이터 없이도 텍스트 임베딩과 시각적 관측의 유사도만을 이용하여 목표를 찾아가는 기술이다.</p>
<ol>
<li><strong>의미론적 지도 작성 (Semantic Mapping):</strong> 로봇은 SLAM(Simultaneous Localization and Mapping)을 수행하면서 동시에 각 위치의 시각 정보를 CLIP이나 GLIP 인코더를 통해 임베딩 벡터로 변환하여 저장한다. 이를 ‘의미론적 특징 지도(Semantic Feature Map)’ 또는 VLM-Map이라 한다. 각 지도 포인트 <span class="math math-inline">P_i</span>는 좌표 <span class="math math-inline">(x, y, z)</span>뿐만 아니라 의미 벡터 <span class="math math-inline">v_i</span>를 포함한다.</li>
<li><strong>탐색 전략:</strong> 로봇은 현재 관측된 임베딩과 목표 텍스트 임베딩 간의 코사인 유사도가 높아지는 방향으로 경로를 계획한다. 최근 연구에서는 PWDN(Potential Weight Distribution Network)과 같은 확률적 업데이트를 통해 센서 노이즈와 인식 불확실성을 보정한다.</li>
<li><strong>이점:</strong> 로봇은 “빨간 소파“를 찾다가 실패하면, 사용자가 “그럼 긴 의자를 찾아봐“라고 명령을 수정했을 때, 지도를 다시 작성할 필요 없이 저장된 임베딩 벡터와 새로운 텍스트 쿼리 간의 유사도만 다시 계산하면 된다.</li>
</ol>
<h3>5.2  언어 조건부 로봇 조작 (Language-Conditioned Robotic Manipulation)</h3>
<p>로봇 팔을 이용한 조작 작업에서 정렬 패러다임은 ‘Pick and Place’ 작업을 혁신했다.</p>
<ul>
<li><strong>VLA (Vision-Language-Action) 모델:</strong> RT-2(Robotic Transformer 2)나 PaLM-E와 같은 최신 모델들은 시각과 언어 입력을 받아 직접 로봇의 제어 명령(Action Token)을 출력한다. 이들은 인터넷 규모의 데이터로 학습된 VLM의 지식을 로봇 제어 데이터와 <strong>정렬</strong>하여 전이시킨다. VLM이 “바나나“를 인식하는 능력을 가졌다면, VLA는 “바나나를 집어라“라는 명령을 수행하기 위해 그리퍼(Gripper)를 어디로 이동시켜야 하는지(Action)를 학습한다.</li>
<li><strong>공간적 정렬:</strong> 단순히 “컵을 집어라“가 아니라 “깨지지 않게 손잡이를 잡아라“와 같은 세밀한 지시를 수행하기 위해서는 3D 공간 정보와 언어 정보의 정렬이 필요하다. 이를 위해 포인트 클라우드(Point Cloud) 기반의 3D 인코더와 언어 모델을 결합하거나, NeRF(Neural Radiance Fields)에 의미론적 필드를 추가하는 LERF(Language Embedded Radiance Fields) 연구가 진행되고 있다.</li>
</ul>
<h3>5.3  엣지(Edge) 디바이스 최적화 및 구현</h3>
<p>대부분의 로봇은 배터리와 컴퓨팅 파워의 제약을 받는 엣지 디바이스(예: NVIDIA Jetson Orin)에서 작동한다. 거대한 Transformer 모델인 OWL-ViT나 Grounding DINO를 그대로 탑재하는 것은 불가능에 가깝다. 이를 해결하기 위한 최적화 기법은 다음과 같다.</p>
<ul>
<li><strong>NanoOWL:</strong> NVIDIA는 OWL-ViT를 엣지 디바이스에 최적화한 NanoOWL 프레임워크를 제안했다. 이는 모델을 ONNX로 변환하고 TensorRT 엔진으로 컴파일하여 추론 속도를 비약적으로 향상시킨다. 연구 결과에 따르면, FP16 정밀도를 적용한 OWL-ViT는 Jetson AGX Orin에서 약 75 FPS의 속도를 달성하여 실시간 처리가 가능함을 입증했다.</li>
<li><strong>양자화 (Quantization):</strong> FP32(32비트 부동소수점) 연산을 FP16이나 INT8로 줄여 메모리 대역폭과 연산량을 절약한다. 특히 INT8 양자화는 약간의 정확도 손실(mAP 하락)을 감수하고 추론 속도를 2배 이상 높일 수 있어, 드론이나 보행 로봇과 같이 전력 효율이 중요한 시스템에 필수적이다.</li>
<li><strong>증류 (Distillation):</strong> 클라우드 상의 거대 모델(Teacher)의 지식을 엣지 상의 작은 모델(Student)로 전이시키는 지식 증류(Knowledge Distillation) 기법도 활발히 적용되고 있다. Grounding DINO의 경우, 클라우드 모델의 예측 결과를 의사 레이블(Pseudo-label)로 사용하여 경량 모델을 학습시키는 방식이 사용된다.</li>
</ul>
<table><thead><tr><th style="text-align: left">모델</th><th style="text-align: left">플랫폼</th><th style="text-align: left">최적화 기법</th><th style="text-align: left">추론 속도 (FPS)</th><th style="text-align: left">제로샷 성능 (LVIS AP)</th></tr></thead><tbody>
<tr><td style="text-align: left"><strong>Grounding DINO 1.5 Edge</strong></td><td style="text-align: left">Jetson Orin</td><td style="text-align: left">TensorRT, FP16</td><td style="text-align: left">~75</td><td style="text-align: left">36.2 (minival)</td></tr>
<tr><td style="text-align: left"><strong>YOLO-World-S</strong></td><td style="text-align: left">Jetson Orin</td><td style="text-align: left">TensorRT, INT8</td><td style="text-align: left">&gt;100</td><td style="text-align: left">High (Speed Trade-off)</td></tr>
<tr><td style="text-align: left"><strong>OWL-ViT Base</strong></td><td style="text-align: left">Jetson Orin</td><td style="text-align: left">NanoOWL</td><td style="text-align: left">~10-20</td><td style="text-align: left">Strong Generalization</td></tr>
</tbody></table>
<p>(표 6.1.3-2: 엣지 디바이스에서의 개방형 어휘 탐지 모델 성능 비교 )</p>
<h2>6.  결론 및 미래 전망: 정렬을 넘어 추론으로</h2>
<p>’분류’에서 ’정렬’로의 패러다임 변화는 로봇을 닫힌 세계(Closed World)에서 열린 세계(Open World)로 해방시켰다. 이제 로봇은 프로그래머가 코드를 수정하지 않아도, 사용자의 자연어 지시를 통해 새로운 객체를 인식하고(Zero-shot), 복잡한 상황을 이해하며(Semantic Understanding), 환경에 유연하게 적응(Adaptability)할 수 있게 되었다.</p>
<p>이러한 변화는 데이터의 역할을 근본적으로 바꾸어 놓았다. 과거의 데이터는 모델을 학습시키기 위한 ’연료’에 불과했으나, 정렬 패러다임에서 데이터(특히 텍스트 쿼리)는 모델을 제어하는 ’인터페이스’가 되었다. 이는 로봇 운용 비용(OPEX)을 획기적으로 낮추는 파급 효과를 가져온다. 사용자는 더 이상 로봇 전문가를 부를 필요 없이, “저기 있는 파란색 상자를 치워줘“라고 말함으로써 로봇의 행동을 수정할 수 있다.</p>
<p>그러나 여전히 과제는 남아 있다. 정렬 모델은 때때로 존재하지 않는 물체를 확신을 가지고 탐지하는 ‘환각(Hallucination)’ 현상을 보인다. 물리적 세계에서 활동하는 로봇에게 이는 충돌이나 파손과 같은 안전 사고로 이어질 수 있다. 따라서 향후 연구는 정렬의 정확도를 높이는 것을 넘어, 모델의 불확실성(Uncertainty)을 정량화하고 안전성(Safety)을 보장하는 방향으로 나아가야 한다.</p>
<p>또한, 현재의 정렬 기술은 주로 정적인 이미지와 텍스트의 매칭에 머물러 있다. 미래의 로봇 지각 시스템은 시간적 흐름과 인과 관계를 이해하는 **‘시간적 정렬(Temporal Alignment)’**과, 로봇의 행동에 따른 물리적 결과를 예측하는 **‘물리적 정렬(Physical Alignment)’**이 결합된 형태, 즉 진정한 의미의 **‘멀티모달 월드 모델(Multimodal World Model)’**로 진화할 것이다. 이것이 바로 로봇 지각의 다음 패러다임이 될 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Vision Language Action Models in Robotic Manipulation: A Systematic Review - arXiv, https://arxiv.org/html/2507.10672v1</li>
<li>Multimodal perception-driven decision-making for human-robot interaction: a survey, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1604472/full</li>
<li>Softmax vs Sigmoid Activation function - GeeksforGeeks, https://www.geeksforgeeks.org/deep-learning/softmax-vs-sigmoid-activation-function/</li>
<li>Towards Evidential and Class Separable Open Set Object Detection, https://ojs.aaai.org/index.php/AAAI/article/view/28367/28719</li>
<li>Learning Transferable Visual Models From Natural Language Supervision - OpenAI, https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf</li>
<li>Contrastive Language-Image Pre-training - Wikipedia, https://en.wikipedia.org/wiki/Contrastive_Language-Image_Pre-training</li>
<li>Analyzing the Impact of Learnable Softmax Temperature in Contrastive Visual-Textual Alignment Systems: Benefits, Drawbacks, and Alternative Approaches | OpenReview, https://openreview.net/forum?id=rx1QNhsNsK</li>
<li>Softmax vs Sigmoid function in Logistic classifier? - Cross Validated - Stats StackExchange, https://stats.stackexchange.com/questions/233658/softmax-vs-sigmoid-function-in-logistic-classifier</li>
<li>A Survey on Open-Vocabulary Detection and Segmentation: Past, Present, and Future, https://arxiv.org/html/2307.09220</li>
<li>Open-Vocabulary Detection &amp; Segmentation, https://faculty.cc.gatech.edu/~zk15/teaching/AY2025_cs8803vlm_fall/L5_OpenVocabulary.pdf</li>
<li>RO-ViT: Region-aware pre-training for open-vocabulary object detection with visi, https://research.google/blog/ro-vit-region-aware-pre-training-for-open-vocabulary-object-detection-with-vision-transformers/</li>
<li>Region-Aware Pretraining for Open-Vocabulary Object Detection With Vision Transformers, https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Region-Aware_Pretraining_for_Open-Vocabulary_Object_Detection_With_Vision_Transformers_CVPR_2023_paper.pdf</li>
<li>Don’t Blind Your VLA: Aligning Visual Representations for OOD Generalization - arXiv, https://arxiv.org/html/2510.25616v1</li>
<li>[PDF] Learning Transferable Visual Models From Natural Language Supervision, https://www.semanticscholar.org/paper/Learning-Transferable-Visual-Models-From-Natural-Radford-Kim/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4</li>
<li>ALIGN: Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision - Microsoft Research, https://www.microsoft.com/en-us/research/video/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/</li>
<li>ALIGN: Scaling Up Visual and Vision-Language Representation …, https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/</li>
<li>Improving Object Detection via Local-global Contrastive Learning - arXiv, https://arxiv.org/html/2410.05058v2</li>
<li>Generative Region-Language Pretraining for Open-Ended Object Detection - arXiv, https://arxiv.org/html/2403.10191v1</li>
<li>Exploring Region-Word Alignment in Built-in Detector for Open-Vocabulary Object Detection, https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Exploring_Region-Word_Alignment_in_Built-in_Detector_for_Open-Vocabulary_Object_Detection_CVPR_2024_paper.pdf</li>
<li>GLIPv2: Unifying Localization and VL Understanding - NeurIPS, https://proceedings.neurips.cc/paper_files/paper/2022/file/ea370419760b421ce12e3082eb2ae1a8-Paper-Conference.pdf</li>
<li>Grounding DINO: Open Vocabulary Object Detection on Videos - PyImageSearch, https://pyimagesearch.com/2025/12/08/grounding-dino-open-vocabulary-object-detection-on-videos/</li>
<li>Marrying DINO with Grounded Pre-Training for Open-Set Object Detection - arXiv, https://arxiv.org/html/2303.05499v5</li>
<li>Fine-Tuning Grounding DINO: Open-Vocabulary Object Detection - Learn OpenCV, https://learnopencv.com/fine-tuning-grounding-dino/</li>
<li>Grounding DINO 1.5: Advance the “Edge” of Open-Set Object Detection - arXiv, https://arxiv.org/html/2405.10300v1</li>
<li>Real-time open-vocabulary perception for mobile robots on edge devices: a systematic analysis of the accuracy-latency trade-off - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC12583037/</li>
<li>Papers Explained 237: OWL ViT - Ritvik Rastogi - Medium, https://ritvik19.medium.com/papers-explained-237-owl-vit-ea58a142de68</li>
<li>Simple Open-Vocabulary Object Detection with Vision Transformers, https://currtain.com/wp-content/uploads/v2.pdf</li>
<li>Understanding popular Zero-Shot and Open-Vocabulary Object Detection models, https://cv-tricks.com/how-to/understanding-popular-zero-shot-and-open-vocabulary-object-detection-models/</li>
<li>Top 5 zero-shot object detection models in 2025 - InteligenAI, https://inteligenai.com/zero-shot-detection-enterprise/</li>
<li>One Map to Find Them All: Real-time Open-Vocabulary Mapping for Zero-shot Multi-Object Navigation - arXiv, https://arxiv.org/html/2409.11764v2</li>
<li>Reliable Semantic Understanding for Real World Zero-shot Object Goal Navigation - arXiv, https://arxiv.org/html/2410.21926v1</li>
<li>GAMap: Zero-Shot Object Goal Navigation with Multi-Scale Geometric-Affordance Guidance, https://arxiv.org/html/2410.23978v1</li>
<li>Language-Driven Zero-Shot Object Navigation via Dynamic Probabilistic Strategy and Large Language Models - IEEE Xplore, https://ieeexplore.ieee.org/iel8/6287639/10820123/11175413.pdf</li>
<li>HiFi-CS: Towards Open Vocabulary Visual Grounding For Robotic Grasping Using Vision-Language Models - arXiv, https://arxiv.org/html/2409.10419v1</li>
<li>Real-time open-vocabulary perception for mobile robots on edge devices: a systematic analysis of the accuracy-latency trade-off - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1693988/full</li>
<li>Distilling Grounding DINO for an Edge-Cloud Collaborative Advanced Driver Assistance System - IEEE Xplore, https://ieeexplore.ieee.org/document/11072408/</li>
<li>Grounding DINO 1.5 Edge - Emergent Mind, https://www.emergentmind.com/topics/grounding-dino-1-5-edge</li>
<li>Object Detection with Multimodal Large Vision-Language Models: An In-depth Review, https://arxiv.org/html/2508.19294v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>