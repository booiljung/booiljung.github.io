<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:6.1 폐쇄형 집합(Closed-set)의 종말과 개방형 어휘(Open-Vocabulary)의 부상</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>6.1 폐쇄형 집합(Closed-set)의 종말과 개방형 어휘(Open-Vocabulary)의 부상</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 6. 오픈 보캐블러리와 시맨틱 이해 (Open-Vocabulary & Semantic Understanding)</a> / <a href="index.html">6.1 폐쇄형 집합(Closed-set)의 종말과 개방형 어휘(Open-Vocabulary)의 부상</a> / <span>6.1 폐쇄형 집합(Closed-set)의 종말과 개방형 어휘(Open-Vocabulary)의 부상</span></nav>
                </div>
            </header>
            <article>
                <h1>6.1 폐쇄형 집합(Closed-set)의 종말과 개방형 어휘(Open-Vocabulary)의 부상</h1>
<h2>1.  서론: 분류학의 종언과 인식의 해방</h2>
<p>로봇 공학의 역사, 특히 구체화된 인공지능(Embodied AI)의 발전 과정에서 2020년대 초반은 ’인식의 해방기’이자 ’고정된 분류학의 종언’으로 기록될 것이다. 지난 반세기 동안 로봇의 시각 지능(Visual Intelligence)을 지배해 온 패러다임은 <strong>폐쇄형 집합(Closed-set)</strong> 가정이었다. 이는 로봇이 인식해야 할 객체의 종류와 속성이 훈련 당시에 이미 고정되어 있으며, 세상의 모든 사물은 사전에 정의된 유한한 정수(Integer) ID 중 하나로 귀결된다는 믿음에 기초한다. 이러한 결정론적 세계관 하에서 로봇은 통제된 공장 환경이나 제한된 실험실 환경에서는 탁월한 성능을 발휘했으나, 무한한 다양성을 지닌 비정형의 현실 세계로 나아가는 순간 무력해질 수밖에 없었다.</p>
<p>전통적인 컴퓨터 비전 모델들은 80개의 클래스를 가진 COCO 데이터셋이나 1,000개의 클래스를 가진 ImageNet 데이터셋을 기반으로 훈련되었다. 그러나 인간의 언어로 기술될 수 있는 시각적 개념은 수만, 수십만 개를 넘어 사실상 무한대에 가깝다. 로봇이 “파란색 뚜껑이 달린 약병“이나 “식탁 모서리에 위태롭게 놓인 컵“을 인식해야 할 때, 폐쇄형 집합 모델은 이를 단지 ’병(Bottle)’이나 ’컵(Cup)’이라는 일반 명사로 축소시키거나, 훈련 데이터에 없는 속성이라는 이유로 배경(Background) 잡음으로 처리해버린다. 이러한 <strong>어휘의 빈곤(Vocabulary Poverty)</strong> 은 로봇이 인간의 복잡한 언어적 명령을 수행하고 환경과 상호작용하는 데 있어 넘을 수 없는 ’유리천장’으로 작용해 왔다.</p>
<p>본 장에서는 오랫동안 로봇의 범용성을 제한해 온 폐쇄형 집합의 구조적 한계를 심층적으로 분석하고, 거대 언어 모델(LLM)과 시각 지능의 결합으로 탄생한 <strong>개방형 어휘(Open-Vocabulary)</strong> 패러다임이 어떻게 로봇의 인식 체계를 혁신하고 있는지 기술한다. 또한 CLIP, GLIP, Grounding DINO와 같은 최신 SOTA(State-of-the-Art) 모델들의 기술적 메커니즘을 해부하고, 이들이 <em>HomeRobot OVMM</em> 이나 <em>OK-Robot</em> 과 같은 실제 로봇 시스템에 통합되어 어떠한 성과를 거두었는지 사례를 통해 검증한다. 우리는 지금 로봇이 단순히 ‘보는(Seeing)’ 단계에서, 언어를 매개로 세상을 ‘이해하고(Understanding)’ ‘연결하는(Grounding)’ 단계로 진화하는 변곡점에 서 있다.</p>
<h2>2.  폐쇄형 집합(Closed-set) 인식의 구조적 한계와 로봇 공학의 딜레마</h2>
<p>로봇이 물리적 세계에서 유의미한 작업을 수행하기 위해 가장 먼저 선행되어야 하는 과정은 지각(Perception)이다. 전통적인 객체 탐지(Object Detection) 및 분할(Segmentation) 모델들은 지도 학습(Supervised Learning) 파이프라인을 따르며, 이는 명확한 정답(Ground Truth)을 제공한다는 장점 이면에 로봇의 확장성을 가로막는 치명적인 구조적 결함을 내포하고 있다.</p>
<h3>2.1  온톨로지의 붕괴와 데이터의 희소성</h3>
<p>전통적인 딥러닝 기반 탐지기, 예컨대 Mask R-CNN이나 YOLO 시리즈는 분류기(Classifier)의 마지막 레이어(Softmax layer)가 고정된 차원의 벡터를 출력하도록 설계된다. 이는 모델이 학습할 수 있는 지식의 범위가 데이터셋 설계자가 정의한 온톨로지(Ontology)에 갇혀 있음을 의미한다.</p>
<p>가정용 서비스 로봇의 시나리오를 고려해 보자. 사용자는 로봇에게 “구급상자 안에 있는 핀셋을 가져와“라고 명령할 수 있다. 그러나 일반적인 범용 데이터셋인 MS-COCO에는 ’핀셋(Tweezers)’이라는 클래스가 존재하지 않는다. 로봇 개발자는 이를 해결하기 위해 핀셋 이미지를 수집하고, 바운딩 박스를 라벨링(Labeling)하여 모델을 미세 조정(Fine-tuning)해야 한다. 하지만 다음 날 사용자가 “아보카도 슬라이서“를 요구한다면? 이 과정은 무한히 반복되어야 하며, 이는 사실상 로봇의 배포와 확장을 불가능하게 만드는 경제적, 시간적 비용을 초래한다.</p>
<p>더욱 심각한 문제는 데이터 분포의 불균형이다. 현실 세계의 객체 분포는 전형적인 <strong>롱테일(Long-tail) 분포</strong>를 따른다.</p>
<ul>
<li><strong>머리(Head) 클래스:</strong> ‘사람’, ‘자동차’, ‘의자’, ‘컵’ 등은 빈번하게 등장하며 데이터가 풍부하다. 기존 모델들은 이러한 클래스에 대해 높은 정확도를 보인다.</li>
<li><strong>꼬리(Tail) 클래스:</strong> ‘청진기’, ‘특정 공구’, ‘희귀 과일’ 등은 등장 빈도가 낮고 데이터 수집이 어렵다.</li>
</ul>
<p>폐쇄형 집합 모델은 데이터가 풍부한 머리 클래스에 편향(Bias)되어 학습되는 경향이 강하다. 꼬리 클래스를 학습시키기 위해 데이터를 추가하거나 재학습을 시도하면, 기존에 잘 인식하던 머리 클래스에 대한 지식을 잊어버리는 <strong>치명적 망각(Catastrophic Forgetting)</strong> 현상이 발생한다. 이는 로봇이 새로운 도구를 배우는 순간 기존 도구의 사용법을 잊어버리는 것과 유사하며, 지속적인 학습(Continual Learning)이 필요한 로봇 시스템에 있어 치명적인 약점이다.</p>
<h3>2.2  위치 정보와 의미론의 괴리</h3>
<p>로봇 조작(Manipulation)은 단순한 분류(Classification)보다 훨씬 높은 수준의 인식을 요구한다. 로봇이 물체를 파지(Grasping)하기 위해서는 물체의 정확한 위치(Localization)와 3차원적 형상, 그리고 그 물체가 무엇인지에 대한 의미론적(Semantic) 이해가 동시에 필요하다.</p>
<p>폐쇄형 집합 탐지기는 바운딩 박스 회귀(Bounding Box Regression)를 통해 위치를 추정하지만, 이 위치 정보는 클래스 라벨이라는 단순한 정수값과만 연결된다. “손잡이가 깨진 컵“이나 “물이 반쯤 찬 컵“과 같은 상태 정보(State Information)는 클래스 ID에 포함되지 않는다. 따라서 로봇은 컵을 잡을 때 손잡이가 깨진 부분을 피해야 한다는 사실을 시각적 인식 단계에서 추론할 수 없다. 이는 로봇의 행동 계획(Motion Planning) 단계에 불충분한 정보를 전달하게 되어 작업 실패로 이어진다.</p>
<p>표 6.1은 폐쇄형 집합 데이터셋과 현실 세계의 다양성 간의 격차를 보여준다.</p>
<table><thead><tr><th><strong>데이터셋 (Dataset)</strong></th><th><strong>클래스 수 (Categories)</strong></th><th><strong>인스턴스 수 (Instances)</strong></th><th><strong>특징 (Characteristics)</strong></th><th><strong>로봇 적용 한계</strong></th></tr></thead><tbody>
<tr><td><strong>MS-COCO</strong></td><td>80</td><td>~1.5M</td><td>일반적인 객체 위주, 높은 품질의 라벨</td><td>빈약한 어휘, 가정/산업 현장의 구체적 객체 부재</td></tr>
<tr><td><strong>LVIS</strong></td><td>~1,203</td><td>~1.2M</td><td>롱테일 분포 반영, 희귀 클래스 포함</td><td>여전히 닫힌 집합, 데이터 불균형 심화</td></tr>
<tr><td><strong>OpenImages</strong></td><td>~600</td><td>~1.7M</td><td>계층적 라벨링, 대규모 데이터</td><td>어휘의 확장은 있으나, 의미론적 관계 이해 부족</td></tr>
<tr><td><strong>현실 세계</strong></td><td><span class="math math-inline">\infty</span></td><td><span class="math math-inline">\infty</span></td><td>무한한 가변성, 복합적 속성, 관계성</td><td>기존 폐쇄형 모델로는 대응 불가능</td></tr>
</tbody></table>
<h3>2.3  실제 로봇 벤치마크에서의 실패 사례 분석</h3>
<p>폐쇄형 집합 모델의 한계는 이론적 영역을 넘어 실제 로봇 벤치마크에서 참혹한 성능 저하로 나타났다. 최근 수행된 <em>HomeRobot Open-Vocabulary Mobile Manipulation (OVMM)</em> 챌린지는 이러한 현실을 적나라하게 보여준다.</p>
<p>이 챌린지는 로봇이 낯선 가정 환경에서 “어떤 물건이든 찾아서(Find), 집어서(Pick), 놓는(Place)” 능력을 평가한다. 초기 베이스라인 연구에서 <strong>Detic</strong>과 같은 폐쇄형 기반의(혹은 과도기적인) 탐지기를 사용했을 때, 시뮬레이션과 실제 환경 모두에서 인식 실패가 전체 미션 실패의 가장 큰 원인을 차지했다.</p>
<ul>
<li><strong>오탐지(False Positive)의 만연:</strong> 훈련 데이터에 없는 객체(Unknown Object)를 마주쳤을 때, 폐쇄형 모델은 이를 ’배경’으로 무시하거나, 가장 외형이 유사한 훈련 클래스로 강제 할당(Misclassification)하는 경향을 보였다. 예를 들어, 로봇이 ’전동 드릴’을 처음 보았을 때 이를 ’헤어 드라이어’로 오인식하여, 부적절한 파지 전략을 수립하고 결국 물체를 떨어뜨리거나 파손시키는 결과를 초래했다.</li>
<li><strong>성공률의 한계:</strong> 베이스라인 시스템의 전체 미션 성공률은 실제 환경에서 20% 내외에 불과했다. 이는 로봇의 하드웨어 제어 능력 부족이 아니라, 지각 시스템이 환경을 올바르게 해석하지 못한 탓이었다.</li>
</ul>
<p>결론적으로, 로봇 공학계는 지도 학습 기반의 폐쇄형 집합 인식이 로봇의 일반화(Generalization)를 가로막는 병목(Bottleneck)임을 인지하게 되었으며, 이는 “무엇이든 인식할 수 있는(Recognize Anything)” 새로운 패러다임에 대한 갈망으로 이어졌다.</p>
<h2>3.  개방형 어휘(Open-Vocabulary)의 이론적 배경과 기술적 진화</h2>
<p>폐쇄형 집합의 종말은 컴퓨터 비전(Vision)과 자연어 처리(NLP)라는 두 거대한 모달리티의 융합에서 시작되었다. 텍스트는 시각적 개념을 설명하는 가장 풍부하고 유연하며, 인류가 축적해 온 지식의 보고이다. 이미지의 특징 공간(Feature Space)을 텍스트의 특징 공간과 정렬(Align)시킬 수 있다면, 로봇은 별도의 시각적 재학습 없이도 텍스트로 묘사된 모든 객체를 인식할 수 있게 된다.</p>
<h3>3.1  CLIP: 시각-언어 정렬의 기초</h3>
<p>OpenAI의 <em>Learning Transferable Visual Models From Natural Language Supervision</em> (CLIP)은 개방형 어휘 인식의 시대를 연 기폭제였다. CLIP의 핵심 혁신은 라벨링된 소규모 데이터셋 대신, 인터넷상에서 수집한 4억 개(400M) 이상의 이미지-텍스트 쌍(Image-Text Pairs)을 활용하여 <strong>대조 학습(Contrastive Learning)</strong> 을 수행한 데 있다.</p>
<p><strong>메커니즘:</strong></p>
<p>CLIP은 이미지 인코더(Image Encoder, 예: ViT 또는 ResNet)와 텍스트 인코더(Text Encoder, 예: Transformer)로 구성된다. 배치(Batch) 크기 <span class="math math-inline">N</span>에 대해, <span class="math math-inline">N</span>개의 이미지 <span class="math math-inline">I_1, \dots, I_N</span>과 이에 대응하는 <span class="math math-inline">N</span>개의 텍스트 캡션 <span class="math math-inline">T_1, \dots, T_N</span>이 입력으로 주어진다. 모델은 이들을 각각의 임베딩 벡터 <span class="math math-inline">\mathbf{v}_i</span>와 <span class="math math-inline">\mathbf{t}_i</span>로 변환한다.</p>
<p>학습의 목표는 정답 쌍인 <span class="math math-inline">(\mathbf{v}_i, \mathbf{t}_i)</span> 간의 코사인 유사도(Cosine Similarity)는 최대화하고, 오답 쌍인 <span class="math math-inline">(\mathbf{v}_i, \mathbf{t}_j) (i \neq j)</span> 간의 유사도는 최소화하는 것이다. 이를 수식으로 표현하면 다음과 같은 대칭적 교차 엔트로피(Symmetric Cross-Entropy) 손실 함수가 된다:<br />
<span class="math math-display">
\mathcal{L}_{img} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{\exp(\langle \mathbf{v}_i, \mathbf{t}_i \rangle / \tau)}{\sum_{j=1}^{N} \exp(\langle \mathbf{v}_i, \mathbf{t}_j \rangle / \tau)}
</span></p>
<p><span class="math math-display">
\mathcal{L}_{text} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{\exp(\langle \mathbf{v}_i, \mathbf{t}_i \rangle / \tau)}{\sum_{j=1}^{N} \exp(\langle \mathbf{v}_j, \mathbf{t}_i \rangle / \tau)}
</span></p>
<p><span class="math math-display">
\mathcal{L}_{CLIP} = \frac{1}{2} (\mathcal{L}_{img} + \mathcal{L}_{text})
</span></p>
<p>여기서 <span class="math math-inline">\tau</span>는 학습 가능한 온도 매개변수(Temperature parameter)로, 소프트맥스 분포의 평탄도(Sharpness)를 조절한다.</p>
<p><strong>로봇 공학적 함의:</strong> CLIP의 가장 큰 기여는 <strong>제로샷(Zero-shot) 전이</strong> 능력이다. 로봇에게 “빨간색 소화기“를 찾으라는 명령이 주어졌을 때, 로봇은 훈련 데이터에서 ’소화기’라는 라벨을 본 적이 없더라도, 텍스트 인코더가 생성한 “A photo of a red fire extinguisher“의 임베딩 벡터와 현재 카메라 입력의 시각적 임베딩을 비교하여 해당 물체를 식별할 수 있다. 이는 로봇이 새로운 환경에 적응하기 위해 필요한 데이터 수집 및 재학습 과정을 제거함으로써 배포 비용을 획기적으로 낮추었다. 또한 CLIP 특징 공간은 기존 지도 학습 모델보다 조명 변화, 가려짐(Occlusion), 스타일 변화 등 분포 변화(Distribution Shift)에 대해 훨씬 강력한 견고함(Robustness)을 보여주었다.</p>
<h3>3.2  분류에서 탐지로: 위치 정보의 부재 극복을 위한 시도</h3>
<p>CLIP은 이미지 전체에 대한 분류(Classification)에는 탁월했으나, 로봇 조작에 필수적인 <strong>위치 정보(Localization)</strong>, 즉 경계 상자(Bounding Box)를 제공하지 못한다는 한계가 있었다. 로봇이 물체를 파지하기 위해서는 ’무엇(What)’인지 아는 것뿐만 아니라 ‘어디에(Where)’ 있는지를 픽셀 수준에서 정확히 알아야 한다. 초기 연구들은 CLIP의 강력한 시맨틱 능력을 기존 탐지기에 이식하거나, 영역 제안(Region Proposal)과 결합하는 방식으로 이 문제를 해결하려 했다.</p>
<h4>3.2.1 OVR-CNN: 캡션을 활용한 약지도 학습</h4>
<p><em>Open-Vocabulary Object Detection Using Captions</em> (OVR-CNN)은 이미지 캡션 데이터를 활용하여 개방형 어휘 탐지를 시도한 선구적 연구이다. 기존 탐지기가 박스 라벨이 있는 소수의 ‘기본 범주(Base Categories)’ 데이터에 의존했다면, OVR-CNN은 인터넷상의 방대한 이미지-캡션 쌍을 활용한다. 이 모델은 시각적 백본(Visual Backbone)과 언어 모델을 사전 학습 단계에서 정렬시킨다. 캡션 내의 단어들이 이미지 내의 어떤 영역과 연관되는지(Word-Region Alignment)를 약지도(Weakly-supervised) 방식으로 학습함으로써, 명시적인 박스 라벨이 없는 ’새로운 범주(Novel Categories)’에 대해서도 탐지가 가능함을 보였다. 이는 비싼 박스 어노테이션 없이도 어휘 확장이 가능함을 입증하며 로봇 학습 데이터의 경제성을 제고했다.</p>
<h4>3.2.2 ViLD: 지식 증류(Knowledge Distillation)를 통한 확장</h4>
<p><em>Open-vocabulary Object Detection via Vision and Language Knowledge Distillation</em> (ViLD)는 사전 학습된 VLM(교사 모델, 예: CLIP)의 지식을 일반적인 2단계(Two-stage) 탐지기(학생 모델, 예: Mask R-CNN)에 주입하는 증류(Distillation) 방식을 제안했다.</p>
<p><strong>ViLD의 작동 원리:</strong></p>
<ol>
<li><strong>영역 제안:</strong> 학생 탐지기의 RPN(Region Proposal Network)이 이미지에서 객체가 있을법한 영역(Proposal)들을 추출한다.</li>
<li><strong>교사 모델 추론:</strong> 추출된 영역 이미지를 잘라내어(Crop) CLIP 이미지 인코더에 입력하고, 해당 영역의 시각적 임베딩을 얻는다.</li>
<li><strong>지식 증류:</strong> 학생 탐지기의 ROI Head가 출력하는 특징 벡터가 CLIP이 생성한 임베딩과 유사해지도록 <span class="math math-inline">L_1</span> 손실 함수 등을 통해 학습시킨다.</li>
<li><strong>추론:</strong> 훈련이 끝나면 학생 모델은 CLIP의 방대한 어휘 지식을 모방할 수 있게 된다. 텍스트 인코더로 생성한 임의의 카테고리 임베딩과 학생 모델의 영역 임베딩 간의 내적(Dot Product)을 통해 분류를 수행한다.</li>
</ol>
<p>ViLD는 LVIS 데이터셋의 희귀 범주(Rare categories)에서 기존 지도 학습 모델을 압도하는 성능을 보여주었으며, 이는 로봇이 낯선 환경에서 희귀 물체를 조작해야 할 때 필수적인 능력을 제공한다.</p>
<h2>4.  객체 탐지의 재정의: 그라운딩(Grounding)과 GLIP</h2>
<p>OVR-CNN과 ViLD가 기존 탐지기 구조에 VLM을 덧붙이는 방식이었다면, 그 다음 단계의 혁신은 객체 탐지 문제 자체를 <strong>구문 그라운딩(Phrase Grounding)</strong> 문제로 재정의하는 것이었다. <em>Grounded Language-Image Pre-training</em> (GLIP)은 이 패러다임의 전환을 대표하는 모델이다.</p>
<h3>4.1  탐지와 그라운딩의 통합</h3>
<p>기존의 객체 탐지기는 클래스 분류를 <span class="math math-inline">N</span>-way Softmax 분류 문제로 취급했다. 즉, 모델은 입력된 영역이 사전에 정의된 <span class="math math-inline">N</span>개의 클래스 중 어디에 속하는지를 확률적으로 예측한다 (<span class="math math-inline">P(c \vert I)</span>).</p>
<p>반면, GLIP은 이를 <strong>이미지 영역과 텍스트 구문 간의 정렬(Alignment) 문제</strong>로 바꾼다. 입력은 이미지와 텍스트 프롬프트(예: “Detect: person, car, dog…”)가 쌍으로 주어진다. 모델은 이미지 내의 각 영역(Region)과 텍스트 내의 각 단어(Token) 간의 유사도를 계산하여 매칭 여부를 결정한다.</p>
<p>수식적으로, 이미지 인코더로부터 얻은 영역 특징을 <span class="math math-inline">O \in \mathbb{R}^{M \times d}</span>, 텍스트 인코더로부터 얻은 단어 특징을 <span class="math math-inline">W \in \mathbb{R}^{L \times d}</span>라고 할 때, 정렬 점수(Alignment Score) 행렬 <span class="math math-inline">S_{ground}</span>는 다음과 같이 계산된다:<br />
<span class="math math-display">
S_{ground} = O W^T
</span><br />
여기서 <span class="math math-inline">S_{ground} \in \mathbb{R}^{M \times L}</span>의 각 원소는 <span class="math math-inline">i</span>번째 영역과 <span class="math math-inline">j</span>번째 단어 사이의 연관성을 나타낸다. 손실 함수는 이 점수 행렬에 대해 시그모이드(Sigmoid) 등을 적용하여 계산된다.</p>
<h3>4.2  GLIP의 혁신성</h3>
<ul>
<li><strong>데이터의 통합:</strong> GLIP은 기존의 탐지 데이터(Detection Data)와 그라운딩 데이터(Grounding Data)를 모두 학습에 활용할 수 있다. 탐지 데이터의 라벨(예: “cat”)을 “c a t“이라는 텍스트 프롬프트로 변환함으로써 두 데이터를 통일된 형식으로 처리한다.</li>
<li><strong>의미론적 풍부함(Semantic Richness):</strong> 단순한 명사뿐만 아니라, “파란 셔츠를 입은 남자“와 같은 서술적 표현(Descriptive Phrase)에 대해서도 그라운딩이 가능하다. 이는 로봇에게 문맥과 관계를 이해하는 능력을 부여한다.</li>
<li><strong>전이 학습:</strong> GLIP은 COCO나 LVIS와 같은 표준 벤치마크에서 제로샷 혹은 퓨샷(Few-shot) 설정만으로도 완전 지도 학습(Fully Supervised) 모델에 필적하거나 능가하는 성능을 보여주었다.</li>
</ul>
<h2>5.  SOTA 아키텍처 분석: Grounding DINO</h2>
<p>현재 시점(2024-2025년 기준)에서 로봇 공학계, 특히 구체화된 AI 분야에서 가장 널리 사용되며 강력한 성능을 보여주는 모델은 <strong>Grounding DINO</strong>이다. <em>Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection</em>  연구는 Transformer 기반의 SOTA 탐지기인 <strong>DINO</strong>와 언어적 그라운딩 능력을 결합하여, 사실상 “무엇이든 탐지하고 분할(Detect and Segment Anything)“하는 능력을 구현했다.</p>
<h3>5.1  아키텍처 심층 분석</h3>
<p>Grounding DINO는 두 가지 핵심 인코더와 이들을 연결하는 융합 모듈로 구성된다.</p>
<ul>
<li><strong>백본(Backbone):</strong> 시각적 특징 추출을 위한 Swin Transformer와 텍스트 특징 추출을 위한 BERT(또는 유사한 언어 모델)를 사용한다.</li>
<li><strong>Feature Enhancer:</strong> 이미지 특징과 텍스트 특징을 단순 결합하는 것을 넘어, 초기 단계부터 깊게 상호작용하도록 설계된 모듈이다. 이는 크로스 어텐션(Cross-Attention) 메커니즘을 사용하여, 텍스트 정보(“고양이”)가 이미지 내의 관련 특징(털, 귀, 꼬리 등)을 강조(Highlight)하도록 유도하고, 반대로 시각 정보가 텍스트의 모호성을 해소하도록 돕는다.</li>
<li><strong>Language-Guided Query Selection:</strong> 기존 DETR 계열 모델들이 객체 쿼리(Object Query)를 무작위로 초기화하거나 학습된 고정 값을 사용하는 것과 달리, Grounding DINO는 입력된 텍스트 정보에 기반하여 쿼리를 초기화한다. 이는 탐색 공간을 효율적으로 좁히고 수렴 속도를 가속화한다.</li>
<li><strong>Cross-Modality Decoder:</strong> 디코더 단계에서도 이미지와 텍스트 특징 간의 상호작용이 지속적으로 이루어지며, 최종적으로 박스 좌표와 해당 박스가 어떤 텍스트 토큰과 매칭되는지를 예측한다.</li>
</ul>
<h3>5.2  로봇 애플리케이션을 위한 핵심 기능: REC</h3>
<p>Grounding DINO는 단순한 객체 탐지를 넘어 <strong>REC(Referring Expression Comprehension)</strong> 능력을 갖추고 있다. 로봇에게 “녹색 의자 아래에 숨겨진 고양이 장난감“이라는 복잡한 자연어 명령이 주어지면, 모델은 다음을 수행한다:</p>
<ol>
<li>‘의자’, ’고양이 장난감’이라는 객체를 식별한다.</li>
<li>’녹색’이라는 속성(Attribute)을 필터링한다.</li>
<li>’아래(under)’라는 공간적 관계(Spatial Relationship)를 해석하여 최종 타겟을 선정한다.</li>
</ol>
<p>이 과정이 단일 모델 내에서 엔드투엔드(End-to-End)로 수행된다는 점은 로봇의 인지 파이프라인을 단순화하고 지연 시간(Latency)을 줄이는 데 크게 기여한다.</p>
<p>표 6.2는 주요 개방형 어휘 모델들의 특성을 비교 요약한 것이다.</p>
<table><thead><tr><th><strong>모델 (Model)</strong></th><th><strong>기반 아키텍처 (Architecture)</strong></th><th><strong>핵심 메커니즘 (Mechanism)</strong></th><th><strong>로봇 적용 이점</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>CLIP</strong></td><td>ViT / ResNet + Transformer</td><td>대조 학습 (Contrastive Learning)</td><td>강력한 제로샷 분류, 높은 강건성</td><td>위치 정보 부재 (탐지 불가)</td></tr>
<tr><td><strong>ViLD</strong></td><td>Mask R-CNN + CLIP</td><td>지식 증류 (Knowledge Distillation)</td><td>기존 탐지기 구조 재활용, 희귀 객체 탐지 우수</td><td>증류 과정의 복잡성</td></tr>
<tr><td><strong>GLIP</strong></td><td>Swin + BERT</td><td>구문 그라운딩 (Phrase Grounding)</td><td>탐지와 그라운딩의 통합, 의미론적 관계 이해</td><td>텍스트 프롬프트 의존성</td></tr>
<tr><td><strong>Grounding DINO</strong></td><td>DINO (Transformer) + BERT</td><td>언어 유도 쿼리 선택, Cross-Modality Fusion</td><td>SOTA 성능, 복잡한 REC 수행 가능, 유연한 입력</td><td>높은 연산 비용</td></tr>
<tr><td><strong>YOLO-World</strong></td><td>YOLO + CLIP</td><td>RepVL-PAN, 오프라인 어휘 계산</td><td><strong>실시간성(Real-time)</strong>, 엣지 디바이스 구동 가능</td><td>정확도와 속도의 트레이드오프</td></tr>
</tbody></table>
<h2>6.  로봇 모바일 조작(Mobile Manipulation)에서의 혁명</h2>
<p>이론적 우수성은 실제 물리 세계에서 작동하는 로봇의 성과로 증명되어야 한다. 개방형 어휘 인식 기술의 도입은 로봇의 모바일 조작(Mobile Manipulation) 분야에서 비약적인 성능 향상을 가져왔다.</p>
<h3>6.1  사례 연구: HomeRobot OVMM 챌린지</h3>
<p><em>HomeRobot Open-Vocabulary Mobile Manipulation Challenge</em> 는 개방형 어휘 로봇의 능력을 검증하는 대표적인 벤치마크이다.</p>
<ul>
<li><strong>과제:</strong> 로봇(Hello Robot Stretch)은 본 적 없는 가정 환경(Unseen Environment)에서 “식탁 위의 컵을 싱크대로 옮겨라“와 같은 자연어 명령을 수행해야 한다.</li>
<li><strong>초기 결과:</strong> 기존의 폐쇄형 탐지기나, 단순한 CLIP 기반 휴리스틱을 사용한 초기 베이스라인의 성공률은 20% 미만이었다. 실패의 대부분은 ’물체 탐지 실패(Object Detection Failure)’에서 기인했다.</li>
<li><strong>개선:</strong> 최근 연구들은 Grounding DINO와 같은 고급 개방형 탐지기를 도입하고, 실패 시 재탐색(Re-perception)하거나 다른 각도에서 다시 보는 등의 능동적 인식(Active Perception) 전략을 결합하여 성공률을 30~40%대까지 끌어올리고 있다. 특히, 텍스트 쿼리를 단순히 “컵“이라고 하는 대신 “투명한 유리컵“과 같이 구체화(Prompt Engineering)했을 때 인식률이 크게 향상됨이 확인되었다.</li>
</ul>
<h3>6.2  시스템 통합: OK-Robot</h3>
<p><em>OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics</em>  연구는 상용 로봇 하드웨어에 개방형 어휘 모델을 통합했을 때의 효과를 정량적으로 입증한 기념비적 연구이다.</p>
<ul>
<li><strong>시스템 구성:</strong> OK-Robot은 3D 공간을 스캔하여 복셀 맵(Voxel Map)을 생성하고, 여기에 <strong>CLIP</strong>, <strong>LangSAM</strong> (Language Segment Anything), <strong>AnyGrasp</strong>와 같은 개방형 지식 모델들을 통합한다.</li>
<li><strong>작동 방식:</strong> 사용자가 “침대 위의 베개를 치워줘“라고 말하면, (1) VLM이 ’베개’의 시각적 특징을 텍스트로부터 추출하고, (2) 3D 맵 상에서 해당 특징과 일치하는 영역을 찾고(Grounding), (3) Grasping 모델이 해당 물체에 적합한 파지 포즈를 생성하여 실행한다.</li>
<li><strong>성과:</strong> 이 시스템은 별도의 추가 학습(Zero-shot) 없이 10개의 낯선 실제 가정 환경에 투입되어 평균 58.5%의 성공률을 기록했다. 환경을 정리하고 적대적 물체(투명하거나 너무 큰 물체)를 제외하면 성공률은 82.4%까지 상승했다. 이는 로봇이 더 이상 특정 환경에 과적합(Overfitting)되지 않고, 범용적인 ’지식’을 가지고 세상과 상호작용할 수 있음을 시사한다.</li>
</ul>
<h2>7.  한계와 남은 과제: 환각, 3D, 그리고 실시간성</h2>
<p>개방형 어휘 모델의 부상은 로봇 공학의 오랜 숙원이었던 ‘인식의 일반화’ 문제를 해결할 열쇠를 제공했지만, 완벽한 해결책은 아니며 여전히 극복해야 할 기술적 난제들이 존재한다.</p>
<h3>7.1  환각(Hallucination)과 신뢰성(Reliability)</h3>
<p>개방형 어휘 모델, 특히 거대 모델들은 때때로 없는 물체를 있다고 확신하거나(False Positive), 텍스트의 미묘한 뉘앙스를 오해하여 엉뚱한 물체를 가리키는 <strong>환각</strong> 문제를 일으킨다. 예를 들어, “사과를 집어“라는 명령에 대해 식탁보에 그려진 사과 그림을 인식하고 집으려 시도할 수 있다. 로봇에게 있어 이러한 오류는 단순한 오답이 아니라 물리적 충돌이나 파손으로 이어지는 안전 문제(Safety Issue)이다.</p>
<h3>7.2  2D 시맨틱과 3D 공간의 불일치</h3>
<p>현재 대부분의 개방형 어휘 모델(CLIP, Grounding DINO 등)은 2D 이미지 기반으로 학습되었다. 그러나 로봇은 3D 세계에서 작동한다. 2D 이미지에서 탐지된 바운딩 박스를 3D 점군(Point Cloud)이나 깊이 지도(Depth Map)로 투영(Back-projection)하는 과정에서 정보 손실, 노이즈, 가려짐에 의한 오류가 발생한다. 2D의 풍부한 시맨틱 정보를 3D 공간 표현(NeRF, Gaussian Splatting 등)에 어떻게 효과적으로 융합(Fusion)할 것인가는 6.4장에서 다룰 <strong>공간 지능(Spatial AI)</strong> 의 핵심 주제이다.</p>
<h3>7.3  실시간성(Real-time)과 연산 비용</h3>
<p>Grounding DINO와 같은 고성능 Transformer 모델은 막대한 연산 자원(GPU VRAM, FLOPs)을 요구한다. 배터리로 구동되는 모바일 로봇의 제한된 엣지(Edge) 컴퓨팅 환경에서 이러한 거대 모델을 실시간(30Hz 이상)으로 구동하는 것은 큰 도전이다. 이를 해결하기 위해 <strong>YOLO-World</strong>와 같이 경량화된 실시간 개방형 탐지기나, 클라우드와 엣지를 연동하는 하이브리드 아키텍처, 또는 작은 모델(Student)로 지식을 증류하는 연구가 활발히 진행 중이다.</p>
<h3>7.4  능동적 인식(Active Perception)의 부재</h3>
<p>현재의 VLM 기반 로봇들은 대부분 수동적(Passive)이다. 즉, 주어진 한 장의 이미지나 비디오에서 최선을 다해 찾으려 노력한다. 그러나 인간은 잘 보이지 않으면 고개를 돌리거나, “이것 말씀이신가요?“라고 되물어본다. 개방형 어휘 인식이 진정한 로봇 지능으로 거듭나기 위해서는, 모호함을 해결하기 위해 스스로 정보를 수집하고 행동하는 <strong>능동적 지각</strong> 능력이 결합되어야 한다.</p>
<h2>8.  결론: 의미론적 상호작용의 시대로</h2>
<p>폐쇄형 집합의 시대에 로봇은 고립된 섬과 같았다. 개발자가 미리 정의해 준 정수(Integer) ID들의 세상에서만 작동할 수 있었기 때문이다. 이제 개방형 어휘의 부상과 함께 로봇은 인간과 동일한 언어적, 시각적 개념을 공유하는 <strong>의미론적 상호작용(Semantic Interaction)</strong> 의 시대로 진입했다.</p>
<p>물체는 더 이상 단순한 ’Class ID 42’가 아니다. 그것은 “투명하고, 약간 금이 갔으며, 식탁 모서리에 위험하게 놓인 내가 좋아하는 머그컵“이다. 이러한 인식의 확장은 로봇이 수행할 수 있는 작업의 범위를 무한히 확장시킨다. 본 장에서 논의한 인식의 혁명은 이어지는 장들에서 다룰 멀티모달 임베딩(6.2절), 분할의 일반화(6.3절), 그리고 공간 지능(6.4절)의 기반이 된다. 우리는 이제 막 ‘무엇이든 보고 이해하는’ 기계의 눈을 뜨게 했다. 남은 과제는 그 눈으로 본 것을 바탕으로 신뢰할 수 있고 안전하게 ‘행동하는’ 방법을 로봇에게 가르치는 일이다.</p>
<h2>9. 참고 문헌 및 심층 분석</h2>
<p>본 장의 기술은 다음의 핵심 연구들을 바탕으로 재구성되었다.</p>
<ul>
<li><strong>제약된 인식의 문제:</strong> 로봇 공학에서 롱테일 분포의 문제는 <em>Limitations of closed-set object detection in robotics long-tail problem</em> 에서 제기된 바와 같이, 희귀 클래스에 대한 탐지 실패가 전체 시스템의 신뢰도를 하락시키는 주원인이었다.</li>
<li><strong>패러다임의 전환:</strong> CLIP은 시각적 개념을 언어적 임베딩으로 변환함으로써 제로샷 인식의 가능성을 열었다. 이는 <em>Simple Open-Vocabulary Object Detection with Vision Transformers</em> (OWL-ViT) 및 ViLD와 같은 연구로 이어져 탐지기로 확장되었다.</li>
<li><strong>통합 모델:</strong> GLIP과 Grounding DINO는 탐지와 그라운딩을 하나의 파이프라인으로 통합하여, 로봇이 복잡한 언어 명령을 시각적 좌표로 변환하는 데 결정적인 기여를 했다.</li>
<li><strong>실제 적용:</strong> <em>HomeRobot</em>  및 <em>OK-Robot</em>  프로젝트는 이러한 이론적 진보가 실제 로봇(Real-world Robot)의 조작 성공률을 어떻게 혁신적으로 개선했는지 보여주는 중요한 사례 연구이다.</li>
</ul>
<h2>10. 참고 자료</h2>
<ol>
<li>Towards Open Vocabulary Learning: A Survey - IEEE Xplore, https://ieeexplore.ieee.org/iel7/34/10550108/10420487.pdf</li>
<li>IEEE PAMI: Towards Open Vocabulary Learning A Survey - Scribd, https://www.scribd.com/document/667333141/IEEE-PAMI-Towards-Open-Vocabulary-Learning-A-Survey</li>
<li>Open-Vocabulary Detection &amp; Segmentation, https://faculty.cc.gatech.edu/~zk15/teaching/AY2025_cs8803vlm_fall/L5_OpenVocabulary.pdf</li>
<li>Benchmarking Object Detectors with COCO: A New Path Forward, https://arxiv.org/html/2403.18819v1</li>
<li>Object Detection: Current and Future Directions - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2015.00029/full</li>
<li>GOAT: GO to Any Thing, https://robots-that-learn.github.io/resources/59_goat_go_to_any_thing.pdf</li>
<li>A Survey on Open-Vocabulary Detection and Segmentation, https://scispace.com/pdf/a-survey-on-open-vocabulary-detection-and-segmentation-past-1vsjgo4g.pdf</li>
<li>Towards Long-Tailed 3D Detection, https://proceedings.mlr.press/v205/peri23a/peri23a.pdf</li>
<li>(PDF) Improving Long-tailed Object Detection with Image-Level …, https://www.researchgate.net/publication/364520236_Improving_Long-tailed_Object_Detection_with_Image-Level_Supervision_by_Multi-Task_Collaborative_Learning</li>
<li>Towards Learning Object Detectors with Limited Data for Industrial …, https://library.oapen.org/bitstream/handle/20.500.12657/100728/towards-learning-object-detectors-with-limited-data-for-industrial-applications.pdf?sequence=1&amp;isAllowed=y</li>
<li>Synthetic Object Compositions for Scalable and Accurate Learning …, https://arxiv.org/html/2510.09110v3</li>
<li>Towards Open-World Mobile Manipulation in Homes - arXiv, https://arxiv.org/html/2407.06939v1</li>
<li>Open-Vocabulary Mobile Manipulation - HomeRobot - AI Habitat, https://aihabitat.org/static/challenge/home_robot_ovmm_2023/ovmm-compressed.pdf</li>
<li>[PDF] HomeRobot: Open-Vocabulary Mobile Manipulation, https://www.semanticscholar.org/paper/HomeRobot%3A-Open-Vocabulary-Mobile-Manipulation-Yenamandra-Ramachandran/3b0c02955e88f5862e61b560c7f70ba8cf235b1d</li>
<li>HomeRobot: Open-Vocabulary Mobile Manipulation - arXiv, https://arxiv.org/html/2306.11565v2</li>
<li>OpenAI CLIP Model Explained: An Engineer’s Guide - Lightly, https://www.lightly.ai/blog/clip-openai</li>
<li>CLIP: Learning Transferable Visual Models From Natural Language …, https://itnext.io/clip-learning-transferable-visual-models-from-natural-language-supervision-29f2817f317f</li>
<li>What Makes CLIP More Robust to Long-Tailed Pre-Training Data? A …, https://proceedings.neurips.cc/paper_files/paper/2024/file/403d7aae69d2f2926dadb35499e1a105-Paper-Conference.pdf</li>
<li>OVIS: Open-Vocabulary Visual Instance Search via Visual-Semantic …, https://ojs.aaai.org/index.php/AAAI/article/view/20070/19829</li>
<li>[2011.10678] Open-Vocabulary Object Detection Using Captions, https://arxiv.org/abs/2011.10678</li>
<li>Simple Open-Vocabulary Object Detection with Vision Transformers, https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700714.pdf</li>
<li>Open-vocabulary Object Detection via Vision and Language … - arXiv, https://arxiv.org/abs/2104.13921</li>
<li>Med-GLIP: Advancing Medical Language-Image Pre-training with …, https://arxiv.org/html/2508.10528v1</li>
<li>[PDF] Grounded Language-Image Pre-training - Semantic Scholar, https://www.semanticscholar.org/paper/Grounded-Language-Image-Pre-training-Li-Zhang/5341b412383c43f4a693ad63ec4489e3ec7688c8</li>
<li>[PDF] Grounding DINO 1.5: Advance the “Edge” of Open-Set Object …, <a href="https://www.semanticscholar.org/paper/Grounding-DINO-1.5%3A-Advance-the-%22Edge%22-of-Open-Set-Ren-Jiang/3abfa0d372bca716f57ca7871e7e3694dacdfae8">https://www.semanticscholar.org/paper/Grounding-DINO-1.5%3A-Advance-the-%22Edge%22-of-Open-Set-Ren-Jiang/3abfa0d372bca716f57ca7871e7e3694dacdfae8</a></li>
<li>[ECCV 2024] Official implementation of the paper “Grounding DINO …, https://github.com/IDEA-Research/GroundingDINO</li>
<li>Marrying DINO with Grounded Pre-Training for Open-Set Object …, https://arxiv.org/abs/2303.05499</li>
<li>Grounding DINO 1.5: Advance the “Edge” of Open-Set Object … - arXiv, https://arxiv.org/abs/2405.10300</li>
<li>Mask-Guided Teacher–Student Learning for Open-Vocabulary …, https://www.mdpi.com/2072-4292/17/19/3385</li>
<li>A Training-Free Guess What Vision Language Model from Snippets …, https://arxiv.org/html/2601.11910v1</li>
<li>Open Vocabulary Mobile Manipulation (OVMM) Challenge | AI Habitat, https://aihabitat.org/challenge/2023_homerobot_ovmm/</li>
<li>arXiv:2407.06939v1 [cs.RO] 9 Jul 2024, https://www.i-newcar.com/uploads/allimg/20241226/2-241226100046108.pdf</li>
<li>OK-Robot: What Really Matters in Integrating Open-Knowledge …, https://arxiv.org/pdf/2401.12202</li>
<li>What Really Matters in Integrating Open-Knowledge Models for …, https://roboticsproceedings.org/rss20/p091.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>