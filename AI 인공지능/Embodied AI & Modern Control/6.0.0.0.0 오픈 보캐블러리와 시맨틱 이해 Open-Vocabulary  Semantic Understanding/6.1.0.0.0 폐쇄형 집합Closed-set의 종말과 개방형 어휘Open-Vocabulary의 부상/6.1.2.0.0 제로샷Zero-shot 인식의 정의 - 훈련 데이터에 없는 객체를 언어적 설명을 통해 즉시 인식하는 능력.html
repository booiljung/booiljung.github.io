<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:6.1.2 제로샷(Zero-shot) 인식의 정의: 훈련 데이터에 없는 객체를 언어적 설명을 통해 즉시 인식하는 능력.</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>6.1.2 제로샷(Zero-shot) 인식의 정의: 훈련 데이터에 없는 객체를 언어적 설명을 통해 즉시 인식하는 능력.</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 6. 오픈 보캐블러리와 시맨틱 이해 (Open-Vocabulary & Semantic Understanding)</a> / <a href="index.html">6.1 폐쇄형 집합(Closed-set)의 종말과 개방형 어휘(Open-Vocabulary)의 부상</a> / <span>6.1.2 제로샷(Zero-shot) 인식의 정의: 훈련 데이터에 없는 객체를 언어적 설명을 통해 즉시 인식하는 능력.</span></nav>
                </div>
            </header>
            <article>
                <h1>6.1.2 제로샷(Zero-shot) 인식의 정의: 훈련 데이터에 없는 객체를 언어적 설명을 통해 즉시 인식하는 능력.</h1>
<h2>1.  서론: 폐쇄형 인식에서 개방형 인식으로의 패러다임 전환</h2>
<p>현대 로봇 공학 및 인공지능 제어 시스템에서 ’인지(Perception)’는 로봇이 물리적 세계와 상호작용하기 위한 가장 기초적이면서도 결정적인 단계이다. 과거 수십 년간 컴퓨터 비전 분야를 지배해 온 지도 학습(Supervised Learning) 기반의 객체 검출기들은 ’폐쇄형 집합(Closed-set)’이라는 근본적인 한계에 갇혀 있었다. 이는 모델이 훈련 과정에서 명시적으로 라벨링(Labeling)된 고정된 범주(예: COCO 데이터셋의 80개 클래스, ImageNet의 1,000개 클래스)만을 인식할 수 있음을 의미한다. 이러한 시스템 하에서는 로봇이 훈련 데이터에 존재하지 않는 새로운 객체(Unseen Object)를 마주했을 때, 이를 단순히 ’배경(Background)’으로 처리하거나 기존에 학습된 엉뚱한 클래스로 오분류하는 치명적인 오류를 범하게 된다.</p>
<p>그러나 로봇이 활동해야 하는 현실 세계(Open World)는 무한한 다양성을 지닌 객체들로 구성되어 있으며, 새로운 제품, 변형된 사물, 그리고 미지의 환경이 끊임없이 등장한다. 이러한 모든 변수에 대해 데이터를 수집하고 라벨링하여 모델을 재학습(Retraining)시키는 것은 비용적, 시간적으로 불가능에 가깝다. 이러한 배경에서 등장한 **제로샷 인식(Zero-shot Recognition)**은 인공지능 인식 시스템의 패러다임을 근본적으로 변화시켰다.</p>
<p>제로샷 인식이란, 모델이 훈련 단계에서 단 한 번도 시각적 예시를 보지 못한(Zero-shot) 범주의 객체를, 오직 **언어적 설명(Linguistic Description)**이나 <strong>속성(Attribute)</strong> 정보만을 통해 즉시 인식하고 분류하는 능력을 정의한다. 이는 인간이 새로운 지식을 습득하는 방식과 유사하다. 인간은 한 번도 본 적 없는 ’오카피(Okapi)’라는 동물을 “얼룩말의 다리와 기린의 목을 가진 동물“이라는 텍스트 설명만으로도 처음 조우했을 때 식별할 수 있다. 이처럼 제로샷 인식은 ’시각적 특징(Visual Features)’과 ‘의미론적 정보(Semantic Information)’ 사이의 연결 고리를 학습함으로써, 로봇이 사전 정의된 데이터셋의 제약을 넘어 개방형 어휘(Open-Vocabulary)를 이해하고 물리적 세계에 적응하도록 만드는 핵심 기술이다.</p>
<p>본 절에서는 제로샷 인식의 이론적 정의와 이를 구현하는 핵심 메커니즘인 시각-언어 모델(Vision-Language Model, VLM)의 작동 원리, 그리고 Grounding DINO, YOLO-World 등 최신 SOTA(State-of-the-Art) 알고리즘들의 기술적 특성을 심층적으로 분석한다. 또한, 이러한 기술이 실제 로봇 시스템에서 어떻게 응용되고 있는지에 대해 포괄적으로 기술한다.</p>
<h2>2.  제로샷 학습(Zero-Shot Learning, ZSL)의 이론적 배경</h2>
<p>제로샷 인식을 이해하기 위해서는 그 기저에 있는 기계 학습 이론인 제로샷 학습(ZSL)의 수학적, 구조적 원리를 파악해야 한다. ZSL은 훈련 데이터와 테스트 데이터의 클래스 집합이 서로 겹치지 않는(Disjoint) 설정에서 작동하며, ’본 적 없는 클래스(Unseen Class)’를 인식하기 위해 ’보조 정보(Auxiliary Information)’를 매개체로 활용한다.</p>
<h3>2.1  의미론적 임베딩 공간 (Semantic Embedding Space)</h3>
<p>제로샷 학습의 핵심은 입력 이미지 공간(Visual Space) <span class="math math-inline">\mathcal{X}</span>와 출력 클래스 공간(Label Space) <span class="math math-inline">\mathcal{Y}</span> 사이를 직접 매핑하는 대신, 이 둘을 공통의 <strong>의미론적 임베딩 공간(Semantic Embedding Space)</strong> <span class="math math-inline">\mathcal{S}</span>로 투영(Projection)하는 것이다.</p>
<p>전통적인 분류기가 <span class="math math-inline">f: \mathcal{X} \rightarrow \mathcal{Y}</span>를 학습하여 이미지 <span class="math math-inline">x</span>가 주어졌을 때 <span class="math math-inline">P(y\vert x)</span>를 최대화하는 클래스 <span class="math math-inline">y</span>를 찾는다면, 제로샷 모델은 시각적 매핑 함수 <span class="math math-inline">\theta(x)</span>와 의미론적 매핑 함수 <span class="math math-inline">\phi(y)</span>를 학습하여, 의미론적 공간 상에서 두 벡터 간의 거리(또는 유사도)를 최적화한다.<br />
<span class="math math-display">
\hat{y} = \arg\max_{y \in \mathcal{Y}_{U}} \text{sim}(\theta(x), \phi(y))
</span><br />
여기서 <span class="math math-inline">\mathcal{Y}_{U}</span>는 훈련 시 보지 못한 클래스들의 집합을 의미하며, <span class="math math-inline">\text{sim}(\cdot, \cdot)</span>은 코사인 유사도(Cosine Similarity) 등의 거리 함수이다. 이 임베딩 공간을 구성하는 방식에 따라 ZSL은 크게 두 가지 접근법으로 나뉜다.</p>
<ol>
<li><strong>속성 기반(Attribute-based) 접근:</strong> 각 클래스를 인간이 정의한 속성 벡터로 표현한다. 예를 들어, ’얼룩말’은 [줄무늬: 있음, 색상: 흑백, 다리: 4개, 서식지: 초원]과 같은 벡터로 정의된다. 모델은 이미지에서 이러한 속성을 추출하도록 학습되며, 새로운 클래스가 주어지면 해당 클래스의 속성 정의와 비교하여 인식을 수행한다. 하지만 이 방식은 모든 클래스에 대해 세밀한 속성을 사람이 직접 정의해야 하는(Annotation Cost) 한계가 있다.</li>
<li><strong>임베딩 기반(Embedding-based) 접근:</strong> 대규모 텍스트 코퍼스(Word2Vec, GloVe, BERT, CLIP Text Encoder)에서 학습된 단어 벡터를 활용한다. ’고양이’와 ’호랑이’라는 단어는 텍스트 공간상에서 가깝게 위치하므로, 모델이 ’고양이’의 시각적 특징을 학습했다면 그와 의미적으로 유사한 ’호랑이’의 시각적 특징도 유추할 수 있게 된다. 현재의 대규모 제로샷 인식 모델들은 대부분 이 방식을 채택하고 있다.</li>
</ol>
<h3>2.2  허브니스(Hubness) 문제와 편향(Bias)</h3>
<p>제로샷 학습 시스템을 설계할 때 직면하는 주요 난제 중 하나는 <strong>허브니스(Hubness)</strong> 문제이다. 고차원 벡터 공간에서는 특정 벡터(Hub)가 다른 많은 벡터들과 높은 유사도를 갖는 현상이 발생한다. 이는 모델이 입력 이미지와 관계없이 특정 클래스(주로 훈련 데이터에 많이 등장했던 클래스나 의미론적으로 일반적인 단어)로 편향되게 예측하는 원인이 된다.</p>
<p>또한, <strong>도메인 이동(Domain Shift)</strong> 문제도 심각하다. 훈련 데이터(Seen Classes)의 시각적 분포와 테스트 데이터(Unseen Classes)의 분포가 다를 경우, 모델은 훈련된 클래스로 편향된 예측을 내놓는 경향이 있다. 이를 해결하기 위해 <strong>일반화된 제로샷 학습(Generalized Zero-Shot Learning, GZSL)</strong> 개념이 도입되었다. GZSL 설정에서는 테스트 단계에서 ’본 클래스’와 ’보지 못한 클래스’가 모두 등장하며, 모델은 이 둘을 편향 없이 구별해야 한다. 이를 위해 최근 연구들은 훈련 시 보지 못한 클래스의 가상 데이터를 생성(Generative Models)하여 학습하거나, 교정된 손실 함수(Calibrated Loss)를 사용하여 본 클래스로의 편향을 억제한다.</p>
<h2>3.  핵심 엔진: 시각-언어 모델 (Vision-Language Models)</h2>
<p>제로샷 인식의 비약적인 발전은 2021년 OpenAI의 CLIP(Contrastive Language-Image Pre-training) 등장과 함께 시작된 대규모 시각-언어 모델(VLM)의 혁명에 기인한다. VLM은 제로샷 인식의 ‘엔진’ 역할을 수행하며, 로봇에게 시각 정보와 언어 정보를 연결하는 능력을 부여한다.</p>
<h3>3.1  CLIP: 자연어 감독 학습의 혁명</h3>
<p>CLIP은 인터넷에서 수집한 4억 개의 이미지-텍스트 쌍(Image-Text Pairs)을 사용하여 훈련된 모델로, 기존의 정제된 라벨(Clean Labels) 대신 **자연어 감독(Natural Language Supervision)**을 활용했다는 점에서 혁신적이다.</p>
<p>CLIP의 아키텍처는 두 개의 독립적인 인코더로 구성된다.</p>
<ul>
<li><strong>이미지 인코더(Image Encoder):</strong> ResNet-50 또는 ViT(Vision Transformer)를 사용하여 이미지를 고차원 특징 벡터로 변환한다.</li>
<li><strong>텍스트 인코더(Text Encoder):</strong> 트랜스포머(Transformer) 기반으로 텍스트(캡션 또는 클래스 이름)를 동일한 차원의 특징 벡터로 변환한다.</li>
</ul>
<p>CLIP의 가장 큰 기여는 제로샷 분류 성능을 지도 학습 모델 수준으로 끌어올렸다는 점이다. 예를 들어, ImageNet 데이터셋을 전혀 학습하지 않은 상태(Zero-shot)에서 ResNet-50과 대등한 정확도를 기록했다. 이는 로봇이 특정한 물체를 인식하기 위해 별도의 데이터 수집과 훈련 과정을 거칠 필요 없이, 단순히 찾고자 하는 물체의 이름을 텍스트로 입력하는 것만으로 인식 기능을 수행할 수 있음을 시사한다.</p>
<h3>3.2  정렬(Alignment) 메커니즘과 대조 학습</h3>
<p>CLIP 학습의 핵심 원리는 <strong>대조 학습(Contrastive Learning)</strong>, 구체적으로 InfoNCE 손실 함수를 통한 정렬(Alignment) 과정에 있다. 배치(Batch) 크기가 <span class="math math-inline">N</span>일 때, <span class="math math-inline">N</span>개의 이미지와 그에 대응하는 <span class="math math-inline">N</span>개의 텍스트 캡션이 존재한다. 이 배치는 <span class="math math-inline">N</span>개의 올바른 쌍(Positive Pairs, 대각선 요소)과 <span class="math math-inline">N^2 - N</span>개의 잘못된 쌍(Negative Pairs, 비대각선 요소)으로 구성된다.</p>
<p>모델의 목표는 올바른 쌍의 임베딩 간 코사인 유사도는 최대화하고, 잘못된 쌍의 유사도는 최소화하는 것이다. 이를 수식으로 표현하면 다음과 같다.<br />
<span class="math math-display">
\mathcal{L}_{i}^{(v \rightarrow t)} = -\log \frac{\exp(\text{sim}(I_i, T_i) / \tau)}{\sum_{j=1}^{N} \exp(\text{sim}(I_i, T_j) / \tau)}
</span><br />
여기서 <span class="math math-inline">I_i</span>는 <span class="math math-inline">i</span>번째 이미지 임베딩, <span class="math math-inline">T_i</span>는 <span class="math math-inline">i</span>번째 텍스트 임베딩, <span class="math math-inline">\tau</span>는 온도(Temperature) 파라미터이다. 이 대칭적인 손실 함수(Symmetric Cross Entropy Loss)를 통해 이미지 공간과 텍스트 공간이 하나의 공통된 의미론적 공간으로 강력하게 정렬된다.</p>
<p>이러한 정렬 메커니즘은 로봇에게 다음과 같은 이점을 제공한다.</p>
<ol>
<li><strong>강건성(Robustness):</strong> 다양한 조명, 앵글, 스타일(실사, 그림 등)의 이미지 변화에도 의미론적 내용이 같다면 유사한 벡터로 매핑된다.</li>
<li><strong>유연성(Flexibility):</strong> “빨간색 컵“을 찾다가 “파란색 텀블러“를 찾아야 할 때, 모델 재학습 없이 텍스트 쿼리만 변경하면 된다.</li>
</ol>
<h2>4.  개방형 어휘 객체 검출 (Open-Vocabulary Object Detection)의 진화</h2>
<p>CLIP은 이미지 전체를 분류하는 데는 탁월하지만, 이미지 내 특정 객체의 위치(Bounding Box)를 찾아내는 ‘객체 검출’ 작업에는 직접적으로 적용하기 어렵다. 로봇 제어, 특히 파지(Grasping)나 조작(Manipulation)을 위해서는 객체의 정확한 위치 정보가 필수적이다. 따라서 CLIP의 강력한 시각-언어 지식을 객체 검출기로 전이(Transfer)하기 위한 연구가 활발히 진행되어 왔다.</p>
<h3>4.1  검출(Detection)과 그라운딩(Grounding)의 융합</h3>
<p>전통적인 객체 검출은 이미지를 입력받아 <code>(Box, Class ID)</code>를 출력한다. 반면, 시각적 그라운딩(Visual Grounding) 또는 구절 그라운딩(Phrase Grounding)은 이미지와 자연어 표현(예: “왼쪽에 있는 남자”)을 입력받아 해당 표현이 지칭하는 영역의 박스를 출력한다.</p>
<p>최신 제로샷 객체 검출 기술인 **개방형 어휘 객체 검출(Open-Vocabulary Object Detection, OVOD)**은 이 두 가지 태스크를 융합하는 방향으로 진화했다. 즉, 객체 검출을 “이미지 내의 모든 객체를 찾아내고, 각 객체를 주어진 임의의 텍스트 쿼리와 매칭하는 문제“로 재정의한 것이다.</p>
<h3>4.2  GLIP: 객체 검출의 구절 그라운딩 재정의</h3>
<p>Microsoft가 제안한 *GLIP (Grounded Language-Image Pre-training)*은 이러한 융합의 선구적인 모델이다. GLIP은 객체 검출을 구절 그라운딩 문제로 변환함으로써, 기존의 제한된 객체 검출 데이터셋(COCO, Objects365)뿐만 아니라 인터넷상의 방대한 이미지-캡션 데이터(구절과 이미지 영역이 매칭된 데이터)를 훈련에 활용할 수 있게 만들었다.</p>
<p>GLIP의 핵심은 <strong>깊은 융합(Deep Fusion)</strong> 아키텍처이다. 기존 모델들이 이미지 인코더와 텍스트 인코더를 독립적으로 실행한 후 마지막 단계에서만 결합했던 것과 달리, GLIP은 인코딩 초기 단계부터 이미지와 텍스트 특징이 서로 상호작용(Cross-Attention)하도록 설계했다. 이를 통해 시각적 특징은 텍스트 문맥에 따라 더욱 정교해지며(Language-Aware Visual Features), 제로샷 검출 성능을 비약적으로 향상시켰다.</p>
<h3>4.3  OWL-ViT &amp; OWLv2: 비전 트랜스포머의 직접적 활용</h3>
<p>Google Research의 *OWL-ViT (Simple Open-Vocabulary Object Detection with Vision Transformers)*는 CLIP 모델의 구조를 최대한 유지하면서 객체 검출 기능을 수행하도록 개조한 모델이다.</p>
<p>OWL-ViT의 전략은 단순하면서도 강력하다. 표준 ViT의 마지막 풀링 레이어(Pooling Layer)를 제거하고, 대신 각 공간 토큰(Spatial Token) 출력에 경량화된 MLP 헤드를 부착하여 바운딩 박스와 클래스 임베딩을 예측하게 했다. 이때 분류 헤드의 가중치는 고정된 클래스 벡터가 아니라, CLIP 텍스트 인코더에서 생성된 텍스트 임베딩으로 동적으로 대체된다.</p>
<p>특히 OWL-ViT는 <strong>원샷(One-shot) 이미지 조건부 검출</strong>이 가능하다는 특징이 있다. 텍스트 쿼리 대신 찾고자 하는 물체의 예시 이미지(Query Image)를 입력하면, 그 이미지의 임베딩을 사용하여 대상 이미지 내에서 유사한 물체들을 모두 검출할 수 있다. 후속작인 <strong>OWLv2</strong>는 웹 스케일의 훈련 데이터와 자기 훈련(Self-training) 기법을 도입하여 희귀 클래스(Rare Classes)에 대한 검출 성능을 대폭 개선하였다.</p>
<h2>5.  최신 SOTA 알고리즘 심층 분석</h2>
<p>2024년과 2025년에 걸쳐 제로샷 객체 검출 기술은 정확도와 속도, 그리고 엣지 디바이스 호환성 측면에서 비약적인 발전을 이루었다. 현재 로봇 시스템 구성에 가장 널리 사용되는 최신 SOTA 모델들을 상세히 분석한다.</p>
<h3>5.1  Grounding DINO: DINO와 그라운딩의 결합</h3>
<p>IDEA Research에서 개발한 <em>Grounding DINO</em>는 트랜스포머 기반의 고성능 객체 검출기인 DINO와 언어적 그라운딩을 결합하여, 개방형 집합 검출(Open-Set Detection) 분야에서 독보적인 성능을 입증했다.</p>
<h4>5.1.1  아키텍처 및 핵심 모듈</h4>
<p>Grounding DINO는 닫힌 집합 검출기인 DINO를 세 가지 핵심 단계를 통해 개방형 인식기로 확장한다.</p>
<ol>
<li><strong>특징 향상기(Feature Enhancer):</strong> 이미지 백본(Swin Transformer 등)과 텍스트 백본(BERT 등)에서 추출된 특징을 초기에 융합하지 않고, 별도의 인코더 레이어를 통해 양방향 교차 주의(Bidirectional Cross-Attention)를 수행한다. 이는 이미지 특징이 텍스트 문맥을 반영하고, 텍스트 특징이 이미지 내용을 반영하도록 하여 특징의 표현력을 극대화한다.</li>
<li><strong>언어 유도 쿼리 선택(Language-Guided Query Selection):</strong> DETR 계열 모델은 학습 가능한 객체 쿼리(Object Query)를 사용한다. Grounding DINO는 입력 텍스트와 가장 연관성이 높은 이미지 특징을 선택하여 쿼리를 초기화하는 방식을 도입했다. 이는 모델이 텍스트 설명에 부합하는 객체에 우선적으로 집중하도록 유도하여 검출 효율을 높인다.</li>
<li><strong>교차 양상 디코더(Cross-Modality Decoder):</strong> 디코딩 단계에서도 이미지 쿼리와 텍스트 특징 간의 교차 주의를 지속 수행하여, 최종적으로 텍스트 프롬프트에 정확히 매칭되는 객체의 바운딩 박스를 정제한다.</li>
</ol>
<h3>5.2  Grounding DINO 1.5 및 1.6: 엣지와 프로(Pro) 모델의 분화</h3>
<p>2024년 5월에 공개된 <strong>Grounding DINO 1.5</strong>와 이후의 1.6 업데이트는 모델을 고성능 서버용인 ’Pro’와 엣지 디바이스용인 ’Edge’로 이원화하여 로봇 응용성을 대폭 강화했다.</p>
<table><thead><tr><th><strong>모델</strong></th><th><strong>주요 특징 및 아키텍처 변화</strong></th><th><strong>타겟 응용 분야</strong></th></tr></thead><tbody>
<tr><td><strong>Grounding DINO 1.5 Pro</strong></td><td>ViT-L 백본 채택, 2천만 개 이상의 고품질 그라운딩 데이터(Grounding-20M) 학습. <strong>초기 융합(Early Fusion)</strong> 설계 유지. 네거티브 샘플링 강화로 환각(Hallucination) 억제.</td><td>고성능 서버, 정밀 분석, 복잡한 추론이 필요한 로봇 관제 시스템</td></tr>
<tr><td><strong>Grounding DINO 1.5 Edge</strong></td><td>EfficientViT 백본 채택. 다중 스케일 특징 융합을 제거하고 고수준 특징만 사용하는 효율적 구조. <strong>텐서RT(TensorRT)</strong> 최적화 지원.</td><td>모바일 로봇, 드론, 임베디드 보드(Jetson Orin 등), 실시간 주행</td></tr>
</tbody></table>
<p>특히 <strong>Grounding DINO 1.5 Edge</strong>는 로봇 공학자들에게 매우 중요한 의미를 갖는다. 기존의 거대 모델들이 높은 연산량으로 인해 로봇 내부(On-board) 컴퓨터에서 구동하기 어려웠던 반면, Edge 모델은 NVIDIA Orin NX와 같은 엣지 보드에서도 높은 FPS로 제로샷 검출을 수행할 수 있게 하여 자율 주행 및 실시간 조작을 가능하게 했다.</p>
<h3>5.3  YOLO-World: 실시간 제로샷 검출을 위한 효율성 혁신</h3>
<p><em>YOLO-World: Real-Time Open-Vocabulary Object Detection</em>은 트랜스포머 기반 모델들의 느린 추론 속도 문제를 해결하기 위해, CNN 기반의 고효율 아키텍처인 YOLO(You Only Look Once)에 개방형 어휘 능력을 주입한 모델이다.</p>
<h4>5.3.1  RepVL-PAN과 프롬프트-디텍트(Prompt-then-Detect) 패러다임</h4>
<p>YOLO-World의 기술적 혁신은 **RepVL-PAN(Re-parameterizable Vision-Language Path Aggregation Network)**에 있다. 이는 시각적 특징 피라미드 네트워크(FPN)의 각 단계에서 언어 정보와 시각 정보를 융합하되, 이를 재파라미터화(Re-parameterization) 가능하도록 설계했다.</p>
<p>가장 결정적인 특징은 <strong>오프라인 어휘(Offline Vocabulary)</strong> 전략이다. 일반적인 VLM은 매 추론마다 텍스트 인코더를 실행해야 하므로 연산 오버헤드가 크다. 반면, YOLO-World는 사용자가 정의한 텍스트 프롬프트(예: “사람”, “장애물”, “문”)를 사전에 인코딩하여 모델의 가중치(Conv Filters)에 통합시킨다. 따라서 실제 추론 시(Online Inference)에는 무거운 텍스트 인코더를 실행할 필요 없이, 단순한 행렬 연산만으로 제로샷 검출이 수행된다. 이로 인해 YOLO-World는 LVIS 데이터셋 제로샷 평가에서 기존 모델 대비 20배 이상의 속도 향상을 달성하며 실시간성을 확보했다.</p>
<h4>5.3.2  YOLO-World v2의 진화</h4>
<p>2024년 중반 이후 업데이트된 <strong>YOLO-World v2</strong>는 작은 객체(Small Object)에 대한 검출 성능을 강화하고, 프롬프트 튜닝 기능을 개선하여 로봇 파지(Grasping)와 같이 정밀한 위치 추정이 필요한 작업에서의 활용도를 높였다.</p>
<h2>6.  로봇 공학적 응용과 엣지 배포</h2>
<p>제로샷 인식 기술은 로봇이 통제된 환경(공장 라인 등)을 벗어나 비정형 환경(가정, 병원, 야외)에서 자율적으로 작동하게 만드는 핵심 동력이다.</p>
<h3>6.1  개방형 어휘 모바일 조작 (Open-Vocabulary Mobile Manipulation, OVMM)</h3>
<p>OVMM은 로봇 공학의 성배와 같은 과제로, 로봇이 미지의 환경에서 임의의 물체를 찾아 조작하는 능력을 말한다. 예를 들어 “주방에 가서 빨간색 머그컵을 찾아 테이블 위에 올려줘“라는 명령을 수행하기 위해서는 네비게이션, 객체 탐색, 파지(Picking), 배치(Placing)의 전 과정이 제로샷으로 이루어져야 한다.</p>
<p>최근의 <strong>HomeRobot</strong> 벤치마크 연구에 따르면, 제로샷 객체 검출기(OWL-ViT, Grounding DINO 등)를 탑재한 로봇은 기존의 지도 학습 기반 로봇보다 월등히 높은 임무 성공률을 보였다. 로봇은 카메라로 입력되는 영상에서 ‘주방’, ‘빨간색 머그컵’, ’테이블’을 실시간으로 인식하고, 해당 객체의 3D 좌표를 추정하여 매니퓰레이터를 제어한다. 이때 제로샷 인식기는 단순한 분류를 넘어, 물체의 기하학적 형태나 파지 가능한 부위(Affordance)를 추론하는 데에도 도움을 줄 수 있다.</p>
<h3>6.2  엣지 디바이스 최적화 및 경량화</h3>
<p>로봇, 특히 배터리로 구동되는 드론이나 사족 보행 로봇은 전력과 연산 자원이 매우 제한적이다. 따라서 거대 VLM 모델을 경량화하여 엣지 디바이스에 배포하는 기술이 필수적이다.</p>
<ul>
<li><strong>NanoOWL:</strong> NVIDIA Jetson 플랫폼을 위해 OWL-ViT를 최적화한 프레임워크로, 텐서RT를 활용하여 추론 속도를 수십 배 향상시켰다.</li>
<li><strong>FastSAM &amp; MobileSAM:</strong> Segment Anything Model(SAM)의 무거운 이미지 인코더를 경량 CNN으로 대체하거나 지식 증류(Knowledge Distillation)를 통해 모바일 환경에서도 제로샷 분할(Segmentation)이 가능하도록 했다.</li>
<li><strong>YOLO-World의 엣지 배포:</strong> 앞서 언급한 오프라인 어휘 전략 덕분에 YOLO-World는 추가적인 최적화 없이도 엣지 디바이스에서 가장 빠른 속도를 보여주며, 동적인 환경에서의 로봇 주행에 가장 적합한 모델로 평가받는다.</li>
</ul>
<h3>6.3  의미론적 지도 작성 (Semantic Mapping)과 네비게이션</h3>
<p>로봇이 공간을 이동하며 수집한 시각 정보에 제로샷 인식기를 적용하면, 공간 전체를 **의미론적 지도(Semantic Map)**로 구성할 수 있다. <strong>VLMaps</strong>나 <strong>VL-Nav</strong>와 같은 기술은 공간의 각 좌표(Voxel)에 CLIP 임베딩을 저장한다. 이렇게 생성된 지도는 자연어 쿼리(Queryable)가 가능하다. 사용자가 “소파가 있는 곳으로 가라“고 명령하면, 로봇은 지도 데이터베이스에서 ’소파’라는 텍스트 임베딩과 가장 유사도가 높은 위치를 검색하여 경로를 생성한다. 이는 로봇이 사전에 정의되지 않은 “창가 쪽의 식물 옆“과 같은 복잡한 위치 설명도 이해하고 이동할 수 있게 한다.</p>
<h2>7.  한계점 및 미래 전망</h2>
<p>제로샷 인식 기술은 혁명적이지만 여전히 해결해야 할 과제들이 존재한다.</p>
<h3>7.1  환각(Hallucination) 현상과 신뢰성</h3>
<p>생성형 모델의 특성상 제로샷 인식기 또한 존재하지 않는 물체를 있다고 확신하거나(Hallucination), 시각적으로 유사한 물체를 오인하는 경우가 발생한다. 예를 들어, 복잡한 패턴의 카펫을 ’군중’으로 인식하거나, 거울에 비친 상을 실체로 인식하는 등의 오류는 로봇의 안전한 제어에 위협이 될 수 있다. 이를 해결하기 위해 신뢰도 점수(Confidence Score) 보정, 다중 센서 융합(Sensor Fusion), 그리고 로봇의 물리적 상호작용을 통한 검증 절차가 연구되고 있다.</p>
<h3>7.2  VLA (Vision-Language-Action) 모델로의 확장</h3>
<p>현재의 기술이 ’인식(Perception)’에 집중되어 있다면, 미래는 ’행동(Action)’까지 포함하는 <strong>VLA 모델</strong>로 진화하고 있다. Google의 **RT-2(Robotic Transformer 2)**나 <strong>OpenVLA</strong>와 같은 모델들은 시각과 언어 입력을 받아 로봇의 관절 제어 명령(Action Token)을 직접 출력한다. 이는 인식과 제어 사이의 경계를 허물고, “서랍을 열어라“라는 명령에 대해 로봇이 서랍을 인식하는 것을 넘어, 서랍을 여는 구체적인 동작 궤적까지 제로샷으로 생성하는 진정한 의미의 범용 로봇 지능(General Purpose Robot Intelligence)을 실현할 것으로 기대된다.</p>
<p>요약하자면, 제로샷 인식은 로봇에게 인간 수준의 유연한 시각적 이해 능력을 부여함으로써, 로봇이 정형화된 공장을 벗어나 예측 불가능한 일상 공간에서 인간과 공존하며 협업할 수 있는 길을 열어주었다. Grounding DINO, YOLO-World 등의 기술적 진보는 이러한 변화를 가속화하고 있으며, 이는 향후 로봇 제어 시스템의 표준이 될 것임이 자명하다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Conceptual comparison of closed-vocabulary vs. open-vocabulary…, https://www.researchgate.net/figure/Conceptual-comparison-of-closed-vocabulary-vs-open-vocabulary-detection-pipelines-in_fig1_397320728</li>
<li>Open-vocabulary vs. Closed-set: Best Practice for Few-shot Object …, https://openreview.net/pdf?id=LDwsvLQTLx</li>
<li>What is Zero Shot Learning in Computer Vision? - Edureka, https://www.edureka.co/blog/what-is-zero-shot-learning/</li>
<li>What Is Zero Shot Learning in Image Classification? [Examples], https://www.v7labs.com/blog/zero-shot-learning-guide</li>
<li>What is Zero-Shot Object Detection? - Hugging Face, https://huggingface.co/tasks/zero-shot-object-detection</li>
<li>Zero-shot Learning Machine Vision Systems Made Simple - UnitX, https://www.unitxlabs.com/zero-shot-learning-machine-vision-system-guide-benefits/</li>
<li>Real-time open-vocabulary perception for mobile robots on edge …, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1693988/full</li>
<li>What Is Zero-Shot Learning - Lyzr, https://www.lyzr.ai/glossaries/zero-shot-learning/</li>
<li>What Is Zero-Shot Learning? | IBM, https://www.ibm.com/think/topics/zero-shot-learning</li>
<li>CLIP: Connecting text and images - OpenAI, https://openai.com/index/clip/</li>
<li>Learning Transferable Visual Models From Natural Language …, https://proceedings.mlr.press/v139/radford21a/radford21a.pdf</li>
<li>Learning Transferable Visual Models From Natural Language …, https://arxiv.org/abs/2103.00020</li>
<li>Understanding CLIP: the Contrastive Language-Image Pre-train, https://medium.com/@anna.ml4fun/understanding-clip-the-contrastive-language-image-pre-train-2994ae6e9b18</li>
<li>Open-Vocabulary Object Detection - Emergent Mind, https://www.emergentmind.com/topics/open-vocabulary-object-detection</li>
<li>OW-OVD: Unified Open World and Open Vocabulary Object Detection, https://openaccess.thecvf.com/content/CVPR2025/papers/Xi_OW-OVD_Unified_Open_World_and_Open_Vocabulary_Object_Detection_CVPR_2025_paper.pdf</li>
<li>Grounded Language-Image Pre-training - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Grounded_Language-Image_Pre-Training_CVPR_2022_paper.pdf</li>
<li>Grounded Language-Image Pre-training - Microsoft Research, https://www.microsoft.com/en-us/research/publication/grounded-language-image-pre-training/?lang=zh-cn</li>
<li>Simple Open-Vocabulary Object Detection with Vision Transformers, https://arxiv.org/abs/2205.06230</li>
<li>Simple Open-Vocabulary Object Detection with Vision Transformers, https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700714.pdf</li>
<li>owl | PDF | Applied Mathematics | Machine Learning - Scribd, https://www.scribd.com/document/981758914/owl</li>
<li>Top 5 zero-shot object detection models in 2025 - InteligenAI, https://inteligenai.com/zero-shot-detection-enterprise/</li>
<li>Marrying DINO with Grounded Pre-Training for Open-Set Object …, https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06319.pdf</li>
<li>Marrying DINO with Grounded Pre-Training for Open-Set Object …, https://arxiv.org/abs/2303.05499</li>
<li>[PDF] Grounding DINO 1.5: Advance the “Edge” of Open-Set Object …, <a href="https://www.semanticscholar.org/paper/Grounding-DINO-1.5%3A-Advance-the-%22Edge%22-of-Open-Set-Ren-Jiang/3abfa0d372bca716f57ca7871e7e3694dacdfae8">https://www.semanticscholar.org/paper/Grounding-DINO-1.5%3A-Advance-the-%22Edge%22-of-Open-Set-Ren-Jiang/3abfa0d372bca716f57ca7871e7e3694dacdfae8</a></li>
<li>Grounding DINO 1.5: Advance the “Edge” of Open-Set Object … - arXiv, https://arxiv.org/html/2405.10300v1</li>
<li>Grounding DINO 1.5: Advance the “Edge” of Open-Set Object … - arXiv, https://arxiv.org/html/2405.10300v2</li>
<li>Grounding DINO 1.5: Pushing the Boundaries of Open-Set Object …, https://www.digitalocean.com/community/tutorials/grounding-dino-1-5-open-set-object-detection</li>
<li>Grounding DINO 1.5 Edge - Emergent Mind, https://www.emergentmind.com/topics/grounding-dino-1-5-edge</li>
<li>YOLO-World: Real-Time Open-Vocabulary Object Detection - arXiv, https://arxiv.org/html/2401.17270v3</li>
<li>YOLO-World: Real-Time Open-Vocabulary Object Detection, https://openaccess.thecvf.com/content/CVPR2024/papers/Cheng_YOLO-World_Real-Time_Open-Vocabulary_Object_Detection_CVPR_2024_paper.pdf</li>
<li>YOLO-World: Real-Time Open-Vocabulary Object Detection - Liner, https://liner.com/review/yoloworld-realtime-openvocabulary-object-detection</li>
<li>paynezhangpayne/YOLO-World - Gitee, https://gitee.com/paynezhangpayne/YOLO-World</li>
<li>Grounding DINO 1.6 - DeepDataSpace, https://deepdataspace.com/blog/6/</li>
<li>Language-Conditioned Open-Vocabulary Mobile Manipulation with …, https://arxiv.org/html/2507.17379v1</li>
<li>HomeRobot: Open-Vocabulary Mobile Manipulation, https://proceedings.mlr.press/v229/yenamandra23a/yenamandra23a.pdf</li>
<li>[PDF] Open-vocabulary Mobile Manipulation in Unseen Dynamic …, https://www.semanticscholar.org/paper/Open-vocabulary-Mobile-Manipulation-in-Unseen-with-Qiu-Ma/41f84f296558fb4e4ab5b577066c5e1fcc67d007</li>
<li>Real-time open-vocabulary perception for mobile robots on edge …, https://pmc.ncbi.nlm.nih.gov/articles/PMC12583037/</li>
<li>Top Zero-Shot Object Detection Models - Roboflow, https://roboflow.com/model-feature/zero-shot-detection</li>
<li>Open-Vocabulary Spatio-Temporal Scene Graph for Robot … - arXiv, https://arxiv.org/pdf/2509.23107</li>
<li>Foundation Models for Robotics: Vision-Language-Action (VLA), https://rohitbandaru.github.io/blog/Foundation-Models-for-Robotics-VLA/</li>
<li>Robot Foundation Models - Emergent Mind, https://www.emergentmind.com/topics/robot-foundation-model</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>