<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:6.6.1 Semantic Goal Navigation: "주방으로 가서 냉장고 찾아" – 지도를 생성하며 목표물을 탐색하는 과정.</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>6.6.1 Semantic Goal Navigation: "주방으로 가서 냉장고 찾아" – 지도를 생성하며 목표물을 탐색하는 과정.</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 6. 오픈 보캐블러리와 시맨틱 이해 (Open-Vocabulary & Semantic Understanding)</a> / <a href="index.html">6.6 사례 연구: 자연어 명령 기반 내비게이션 (Object Goal Navigation)</a> / <span>6.6.1 Semantic Goal Navigation: "주방으로 가서 냉장고 찾아" – 지도를 생성하며 목표물을 탐색하는 과정.</span></nav>
                </div>
            </header>
            <article>
                <h1>6.6.1 Semantic Goal Navigation: “주방으로 가서 냉장고 찾아” – 지도를 생성하며 목표물을 탐색하는 과정.</h1>
<h2>1.  서론: 구현된 인공지능과 의미론적 탐색의 진화</h2>
<p>“주방으로 가서 냉장고를 찾아라“라는 명령은 인간에게는 직관적이고 단순한 과업이지만, 로봇 에이전트에게는 인지(Perception), 추론(Reasoning), 지도 작성(Mapping), 그리고 경로 계획(Planning)이 복합적으로 요구되는 고난도의 문제이다. 이를 학술적으로는 <strong>의미론적 목표 탐색(Semantic Goal Navigation)</strong> 또는 **객체 목표 탐색(Object Goal Navigation, ObjectNav)**이라 정의한다. 이 과업의 핵심은 에이전트가 이전에 방문한 적 없는 미지의 환경(Unseen Environment)에서, “냉장고“라는 목표 객체(Instance)의 시각적 특징을 인식할 뿐만 아니라, “냉장고는 주방에 있다“라는 상식적 사실(Common Sense Knowledge)을 활용하여 주방 구역을 우선적으로 탐색하는 전략을 수립하는 데 있다.</p>
<p>과거의 로봇 내비게이션이 단순히 장애물을 회피하며 지정된 좌표로 이동하는 기하학적 탐색(Geometric Navigation)에 머물렀다면, 2024년과 2025년을 기점으로 발전한 최신 기술들은 거대 언어 모델(LLM)과 시각-언어 모델(VLM)을 결합하여 환경의 ’의미(Semantics)’를 이해하는 방향으로 진화하고 있다. 특히 <strong>CVPR 2024</strong>, <strong>ICRA 2025</strong>, <strong>ICCV 2025</strong> 등 주요 컴퓨터 비전 및 로보틱스 학회에서 발표된 연구들은 닫힌 어휘(Closed-set)의 한계를 넘어, 학습하지 않은 객체나 복잡한 자연어 명령까지 처리할 수 있는 <strong>오픈 보캐블러리(Open-Vocabulary)</strong> 및 <strong>제로샷(Zero-Shot)</strong> 탐색 능력을 보여주고 있다.</p>
<p>본 섹션에서는 사용자가 내린 “주방으로 가서 냉장고 찾아“라는 명령을 수행하기 위해, 로봇이 실시간으로 환경 지도를 생성하고(Mapping), 자신의 위치를 파악하며(Localization), 논리적으로 탐색 경로를 결정하는(Navigation) 일련의 과정을 최신 SOTA(State-of-the-Art) 알고리즘을 중심으로 심층 분석한다.</p>
<h2>2.  의미론적 지도 생성 (Semantic Mapping): 기하학을 넘어선 의미의 투영</h2>
<p>로봇이 “주방“과 “냉장고“를 찾기 위해서는 먼저 자신이 보고 있는 환경을 이해할 수 있는 형태로 저장해야 한다. 전통적인 SLAM(Simultaneous Localization and Mapping)이 공간의 점유 여부(Occupancy)만을 기록했다면, 최신 연구인 <strong>ConceptFusion</strong>이나 <strong>VLMaps</strong>는 공간의 각 지점에 언어적 의미를 내포한 고차원 벡터를 매핑하는 방식을 채택한다.</p>
<h3>2.1  픽셀 정렬 기능 맵과 ConceptFusion의 원리</h3>
<p>2023년 RSS(Robotics: Science and Systems)에서 발표된 <strong>ConceptFusion</strong> 과 그 계보를 잇는 연구들은 사전 학습된 파운데이션 모델(Foundation Model)의 지식을 3D 지도에 직접 투영한다. 이 과정은 다음과 같은 기술적 절차를 따른다.</p>
<ol>
<li>
<p><strong>시각-언어 특징 추출:</strong> 로봇의 RGB 카메라로 입력된 이미지 <span class="math math-inline">I_t</span>에 대해 CLIP(Contrastive Language-Image Pre-training)과 같은 VLM을 사용하여 픽셀 단위 또는 영역 단위의 특징 벡터(Feature Vector) <span class="math math-inline">f_{visual}</span>을 추출한다.</p>
</li>
<li>
<p><strong>3D 투영 및 융합:</strong> 깊이(Depth) 센서 정보를 활용하여 2D 이미지 상의 특징 벡터들을 3D 포인트 클라우드 또는 복셀(Voxel) 그리드 상의 좌표 <span class="math math-inline">p(x, y, z)</span>로 역투영(Back-projection)한다. 이때, 여러 시점에서 관측된 동일 지점의 특징 벡터들은 베이지안 융합(Bayesian Fusion) 등을 통해 점진적으로 정제된다.</p>
</li>
<li>
<p><strong>오픈 셋 쿼리 처리:</strong> 이렇게 생성된 지도는 텍스트 쿼리 <span class="math math-inline">t_{query}</span>(“냉장고”)와 수학적인 연산이 가능하다. 지도상의 특정 지점 <span class="math math-inline">p</span>가 냉장고일 확률(유사도) <span class="math math-inline">S(p)</span>는 특징 벡터 <span class="math math-inline">f_p</span>와 텍스트 임베딩 <span class="math math-inline">t_{query}</span> 간의 코사인 유사도로 계산된다.<br />
<span class="math math-display">
S(p) = \frac{f_p \cdot t_{query}}{\|f_p\| \|t_{query}\|}
</span><br />
이 방식 덕분에 로봇은 “냉장고“라는 라벨을 별도로 학습하지 않아도, 인터넷 규모의 데이터로 학습된 VLM의 지식을 빌려 “냉장고“와 유사한 특징을 가진 지점들을 지도상에서 즉각적으로 식별할 수 있다.</p>
</li>
</ol>
<h3>2.2  VLMaps와 공간적 인덱싱</h3>
<p><strong>VLMaps</strong> 는 이러한 개념을 더욱 확장하여, 로봇이 자연어 명령을 코드로 변환하여 탐색할 수 있는 기반을 제공한다. VLMaps는 환경의 3D 재구성(Reconstruction) 위에 LSeg와 같은 개방형 어휘 세그멘테이션 모델의 출력을 통합한다. 결과적으로 생성된 지도는 단순한 기하학적 장애물 지도가 아니라, “냉장고(Refrigerator)”, “싱크대(Sink)”, “식탁(Dining Table)” 등 수천 가지의 객체 카테고리를 검색할 수 있는 데이터베이스 역할을 수행한다. 특히 “주방“과 같은 추상적인 구역(Region) 개념도, 주방 용품들이 밀집된 영역의 특징 벡터 분포를 분석함으로써 정의될 수 있다.</p>
<h3>2.3  SAM 기반의 정밀 세그멘테이션과 SamSLAM</h3>
<p>Meta가 공개한 <strong>SAM(Segment Anything Model)</strong> 은 지도 작성의 정밀도를 비약적으로 향상시켰다. 기존의 객체 감지 모델이 사전에 정의된 물체만 인식했다면, SAM은 화면 내의 모든 물체를 픽셀 단위로 정교하게 분할(Segmentation)한다. 최신 연구인 <strong>SamSLAM</strong> 이나 <strong>HQ-SAM</strong> 을 활용한 매핑 시스템은 동적인 환경에서 더욱 강력한 성능을 발휘한다. 예를 들어 로봇이 주방을 탐색하는 도중 사람이 지나가더라도, SAM 기반의 알고리즘은 움직이는 객체(사람)와 정적인 객체(냉장고)를 명확히 분리하여 지도에 반영한다. 이를 통해 “냉장고“의 경계선이 명확한 고해상도의 의미론적 지도가 실시간으로 구축된다.</p>
<table><thead><tr><th><strong>기술 명칭</strong></th><th><strong>핵심 메커니즘</strong></th><th><strong>역할 및 기여</strong></th><th><strong>관련 연구</strong></th></tr></thead><tbody>
<tr><td><strong>ConceptFusion</strong></td><td>픽셀 정렬(Pixel-aligned) 특징 융합</td><td>멀티모달(텍스트/이미지/오디오) 쿼리 가능한 3D 지도 생성</td><td></td></tr>
<tr><td><strong>VLMaps</strong></td><td>VLM 임베딩의 격자 지도 통합</td><td>자연어 명령을 공간 좌표로 변환하는 인덱싱 시스템 제공</td><td></td></tr>
<tr><td><strong>SamSLAM</strong></td><td>SAM 마스크 활용 SLAM</td><td>동적 객체 배제 및 정밀한 객체 경계 지도 작성</td><td></td></tr>
<tr><td><strong>SGM</strong></td><td>자기지도 생성형 지도 (Self-supervised)</td><td>관측되지 않은 영역의 의미론적 구조 상상(In-painting)</td><td></td></tr>
</tbody></table>
<h2>3.  인지적 추론과 탐색 정책 (Cognitive Reasoning &amp; Navigation Policy)</h2>
<p>지도가 생성되고 있다 하더라도, “냉장고가 어디에 있는가?“라는 질문에 답하기 위해서는 단순한 인식을 넘어선 **추론(Reasoning)**이 필요하다. 로봇은 현재 거실에 서 있지만, 냉장고를 찾기 위해 침실이 아닌 주방 쪽으로 이동해야 함을 판단해야 한다.</p>
<h3>3.1  CogNav: 인지 프로세스 모델링 (Cognitive Process Modeling)</h3>
<p><strong>ICCV 2025</strong>에서 발표된 <strong>CogNav</strong> 는 인간의 인지 과정을 모사하여 로봇의 탐색 행동을 제어한다. CogNav는 LLM을 두뇌로 활용하여 에이전트의 상태를 정교하게 관리하며, “주방으로 가서 냉장고 찾아“라는 과업을 다음과 같은 인지 상태 전이(Cognitive State Transition)로 해결한다.</p>
<ol>
<li><strong>광역 탐색 (Broad Search):</strong> 초기에는 냉장고의 위치에 대한 정보가 전무하므로, 로봇은 환경의 구조를 파악하기 위해 넓은 범위를 탐색한다. 이때 LLM은 미탐사 지역(Frontier) 중 정보 획득량이 가장 클 것으로 예상되는 곳을 목표로 선정한다.</li>
<li><strong>문맥적 탐색 (Contextual Search):</strong> 로봇이 탐색 도중 ’식탁’이나 ’싱크대’와 같은 랜드마크를 발견하면, LLM은 상식적 추론을 가동한다. “식탁과 싱크대는 주방에 있다. 냉장고는 주방에 있을 확률이 높다.“라는 논리에 따라, 로봇은 해당 랜드마크 주변을 집중적으로 탐색하는 전략으로 전환한다. 이 단계에서 이질적 인지 지도(Heterogeneous Cognitive Map)가 활용되어 에이전트 주변의 공간적 관계를 LLM에 텍스트 프롬프트로 제공한다.</li>
<li><strong>목표물 관측 (Observe Target):</strong> 시야에 냉장고와 유사한 객체가 포착되면 상태는 ‘관측’ 단계로 넘어간다.</li>
<li><strong>후보 검증 (Candidate Verification) 및 확정 (Target Confirmation):</strong> 관측된 물체가 실제 냉장고인지, 아니면 냉장고처럼 생긴 하얀 수납장인지 검증하기 위해 로봇은 다각도로 접근하여 확인한다.</li>
</ol>
<p>CogNav는 이러한 인지적 접근을 통해 기존 SOTA 대비 14% 이상의 성능 향상을 기록했으며, 특히 맹목적인 탐색을 줄이고 효율적인 경로를 생성하는 데 탁월함을 보인다.</p>
<h3>3.2  SEEK: 계층적 확률 계획법</h3>
<p><strong>SEEK (Semantic Reasoning for Object Goal Navigation)</strong> 는 **동적 장면 그래프(Dynamic Scene Graph, DSG)**와 **관계형 의미 네트워크(Relational Semantic Network, RSN)**를 결합하여 확률적인 탐색을 수행한다.</p>
<ul>
<li><strong>RSN의 역할:</strong> RSN은 대규모 데이터셋에서 학습된 지식을 바탕으로 <span class="math math-inline">P(\text{주방} | \text{냉장고})</span>와 같은 조건부 확률을 예측한다. 즉, 목표 객체가 주어졌을 때 그것이 발견될 가능성이 높은 방(Room)의 종류를 추론한다.</li>
<li><strong>계층적 계획 (Hierarchical Planning):</strong> SEEK의 전역 계획기(Global Planner)는 RSN의 확률값을 비용 함수(Cost Function)에 반영하여, 집 안의 여러 방 중에서 ’주방’일 가능성이 높은 미탐사 영역으로 로봇을 우선 유도한다. 방 내부에 진입하면 지역 계획기(Local Planner)가 구체적인 객체 탐색을 수행한다.</li>
</ul>
<h3>3.3  UniGoal: 그래프 매칭 기반 유니버설 탐색</h3>
<p><strong>CVPR 2025</strong>의 <strong>UniGoal</strong> 은 목표가 텍스트(“냉장고”), 이미지, 혹은 복합적인 형태로 주어지더라도 이를 통일된 그래프 표현(Uniform Graph Representation)으로 변환하여 처리한다.</p>
<ul>
<li>로봇은 현재 관측한 환경을 실시간으로 장면 그래프(Scene Graph)로 변환한다. (예: <code>Node:Room(Kitchen) --contains--&gt; Node:Object(Sink)</code>)</li>
<li>목표 그래프(Goal Graph)인 <code>Node:Object(Refrigerator)</code>를 장면 그래프와 매칭시킨다.</li>
<li><strong>매칭 없음 (Zero Matching):</strong> 목표와 관련된 단서가 없으면 반복적인 하위 그래프 탐색을 수행한다.</li>
<li><strong>부분 매칭 (Partial Matching):</strong> 냉장고는 보이지 않지만, 냉장고와 연관된(공동 등장 확률이 높은) ’싱크대’나 ’오븐’이 그래프상에 존재하면, 앵커 쌍 정렬(Anchor Pair Alignment)을 통해 냉장고의 예상 위치 좌표를 투영하여 탐색한다.</li>
</ul>
<h2>4.  탐색 정책 알고리즘: 어디를 볼 것인가? (Where to Look?)</h2>
<p>탐색의 효율성은 “어디를 먼저 볼 것인가“에 달려 있다. 이를 위해 최신 연구들은 프론티어(Frontier) 기반의 잠재 함수를 활용한다.</p>
<h3>4.1  VLFM: 시각-언어 프론티어 지도 (Vision-Language Frontier Maps)</h3>
<p><strong>VLFM</strong> 은 미탐사 영역인 프론티어의 가치를 평가할 때 VLM(CLIP/GLIP)을 활용한다. 로봇의 현재 시야에서 “refrigerator“라는 텍스트 쿼리와 관련된 시각적 특징이 감지되면, 해당 방향의 프론티어에 높은 가중치를 부여한다. VLFM의 탐색 점수 <span class="math math-inline">S(f)</span>는 다음과 같이 계산될 수 있다.<br />
<span class="math math-display">
S(f) = w_{sem} \cdot S_{VL}(f, \text{goal}) + w_{exp} \cdot S_{dist}(f)
</span><br />
여기서 <span class="math math-inline">S_{VL}</span>은 프론티어 방향의 시각-언어 일치도이며, <span class="math math-inline">S_{dist}</span>는 거리 기반 효율성이다. 이를 통해 로봇은 무작위로 탐색하는 대신, “냉장고가 있을 법한 힌트“가 보이는 방향으로 이끌리듯 이동하게 된다.</p>
<h3>4.2  PONI: 잠재 함수망 (Potential Function Networks)</h3>
<p><strong>PONI</strong> 는 지도상의 미탐사 영역에 대해 두 가지 잠재성을 예측하는 네트워크를 학습시킨다.</p>
<ol>
<li>
<p><strong>영역 잠재성 (Area Potential):</strong> 이 프론티어 너머에 얼마나 넓은 미탐사 공간이 있는가?</p>
</li>
<li>
<p><strong>객체 잠재성 (Object Potential):</strong> 이 프론티어 너머에 목표 객체(냉장고)가 존재할 확률은 얼마인가?</p>
</li>
</ol>
<p>PONI는 사전에 수많은 평면도 데이터를 학습하여, 구조적 특징(예: 좁은 문을 지나면 방이 나온다, ’ㄱ’자형 카운터 뒤에는 주방 가전이 있다)을 파악하고 이를 통해 보이지 않는 영역의 가치를 예측한다.</p>
<h2>5.  통합 시나리오 분석: 주방 탐색에서 냉장고 발견까지</h2>
<p>앞서 논의한 기술들이 통합되어 “주방으로 가서 냉장고 찾아“라는 미션을 수행하는 구체적인 과정을 시뮬레이션한다.</p>
<h3>5.1 단계: 초기화 및 광역 탐색 (Initialization &amp; Broad Search)</h3>
<ul>
<li><strong>상황:</strong> 로봇이 거실 한가운데서 시작한다. 주변 지도는 비어 있다.</li>
<li><strong>명령 처리:</strong> “주방(Room) -&gt; 냉장고(Object)“의 계층적 목표가 설정된다.</li>
<li><strong>동작:</strong> 로봇이 제자리 회전하며 초기 파노라마 이미지를 획득한다. <strong>SamSLAM</strong>이 주변의 소파, TV 등을 인식하여 지도로 등록한다. <strong>CogNav</strong>는 현재 위치가 ’거실(Living Room)’임을 인지하고, 목표인 ’주방’이 아니므로 <strong>광역 탐색(Broad Search)</strong> 모드를 활성화한다. <strong>VLFM</strong>은 거실의 닫힌 벽보다는 트여 있는 복도 쪽 프론티어에 높은 점수를 부여하여 로봇을 이동시킨다.</li>
</ul>
<h3>5.2 단계: 랜드마크 발견 및 문맥적 추론 (Contextual Search)</h3>
<ul>
<li><strong>상황:</strong> 복도를 지나던 중 로봇의 카메라에 ’식탁’과 ’전자레인지’가 포착된다.</li>
<li><strong>지도 업데이트:</strong> <strong>ConceptFusion</strong>은 이 객체들을 3D 지도상에 배치하고 의미 벡터를 저장한다.</li>
<li><strong>인지 전환:</strong> <strong>SEEK</strong>의 RSN 모듈은 “전자레인지“와 “식탁“이 “주방“과 높은 확률로 공존함을 알린다. <strong>CogNav</strong>는 상태를 **문맥적 탐색(Contextual Search)**으로 전환한다. 이제 로봇은 집 전체를 탐색하는 것을 멈추고, 전자레인지가 발견된 구역 주변을 집중적으로 스캔하기 시작한다.</li>
<li><strong>경로 수정:</strong> <strong>UniGoal</strong>은 장면 그래프 상의 [전자레인지] 노드를 앵커(Anchor)로 삼아, 냉장고가 있을 법한 위치(예: 전자레인지 옆이나 맞은편 벽)를 추정하고 그곳을 볼 수 있는 위치로 지역 경로(Local Path)를 생성한다.</li>
</ul>
<h3>5.3 단계: 목표 식별 및 접근 (Object Localization)</h3>
<ul>
<li><strong>상황:</strong> 주방 안쪽으로 진입하자 구석에 있는 흰색 대형 물체가 시야에 들어온다.</li>
<li><strong>인식:</strong> 거리가 멀어(5m) 형체가 불분명하지만, <strong>VLFM</strong>의 가치 맵에서 해당 영역의 점수가 급상승한다. 로봇은 해당 물체를 향해 접근한다.</li>
<li><strong>정밀 검증:</strong> 로봇이 2m 거리까지 접근하자 <strong>SAM</strong>이 물체의 윤곽을 정확히 따내고, <strong>VLM</strong>이 이를 “Refrigerator“로 분류한다(신뢰도 0.85). 하지만 옆에 있는 “수납장(Cabinet)“과 헷갈릴 수 있으므로, <strong>CogNav</strong>는 <strong>후보 검증(Candidate Verification)</strong> 상태를 유지하며 로봇을 물체의 정면으로 이동시켜 손잡이나 디스펜서 같은 세부 특징을 확인하도록 한다.</li>
</ul>
<h3>5.4 단계: 목표 확정 및 종료 (Target Confirmation)</h3>
<ul>
<li><strong>상황:</strong> 로봇이 냉장고 정면 1m 지점에 도달한다.</li>
<li><strong>종료:</strong> 시각적 특징이 냉장고임이 확실시되면(신뢰도 &gt; 0.95), 로봇은 <strong>목표 확정(Target Confirmation)</strong> 상태로 전이하고 정지한다. 시스템은 “주방에서 냉장고를 찾았습니다“라는 메시지를 출력하고, 생성된 의미론적 지도를 저장한다. 이 지도는 향후 “냉장고에서 물 꺼내와“와 같은 조작(Manipulation) 작업에 바로 활용될 수 있다.</li>
</ul>
<h2>6.  결론</h2>
<p>“주방으로 가서 냉장고 찾아“라는 시나리오는 현대 로봇 공학이 달성하고자 하는 <strong>Embodied AI</strong>의 정수를 보여준다. 2025년 현재, 이 기술은 기하학적 지도를 그리는 수준을 넘어, <strong>ConceptFusion</strong>과 같은 픽셀 정렬 매핑을 통해 환경의 의미를 세밀하게 기록하고, <strong>CogNav</strong>나 <strong>SEEK</strong>와 같은 LLM 기반 추론 엔진을 통해 인간 수준의 문맥적 탐색 전략을 구사하는 단계에 이르렀다.</p>
<p>특히, 데이터에 전적으로 의존하던 End-to-End 학습 방식에서 벗어나, 지도 작성(Mapping), 추론(Reasoning), 계획(Planning) 모듈이 유기적으로 결합된 **모듈러 아키텍처(Modular Architecture)**가 대세로 자리 잡고 있다. 이는 로봇이 낯선 환경에서도 “냉장고는 주방에 있다“는 상식을 활용하여 탐색 시간을 획기적으로 단축하고, <strong>UniGoal</strong>과 같이 텍스트, 이미지 등 다양한 형태의 목표를 유연하게 처리할 수 있는 범용성을 확보했음을 의미한다. 앞으로의 연구는 이러한 의미론적 지도를 시간의 흐름에 따라 변화하는 동적 환경에 적응시키는 방향으로 더욱 발전할 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>MAG-Nav: Language-Driven Object Navigation Leveraging Memory …, https://arxiv.org/html/2508.05021</li>
<li>Cognitive Process Modeling for Object Goal Navigation with LLMs, https://openaccess.thecvf.com/content/ICCV2025/papers/Cao_CogNav_Cognitive_Process_Modeling_for_Object_Goal_Navigation_with_LLMs_ICCV_2025_paper.pdf</li>
<li>Reliable Semantic Understanding for Real World Zero-shot Object …, https://arxiv.org/html/2410.21926v1</li>
<li>UniGoal: Towards Universal Zero-shot Goal-oriented Navigation, https://cvpr.thecvf.com/virtual/2025/poster/34649</li>
<li>ConceptFusion: Open-set Multimodal 3D Mapping - Robotics, https://www.roboticsproceedings.org/rss19/p066.pdf</li>
<li>ConceptFusion: Open-set Multimodal 3D Mapping - OpenReview, https://openreview.net/pdf/3df88889157197f157155b5bc6074e8c60fb9b8b.pdf</li>
<li>Visual Language Maps for Robot Navigation - SciSpace, https://scispace.com/pdf/visual-language-maps-for-robot-navigation-byf1lxfx.pdf</li>
<li>Multimodal Spatial Language Maps for Robot Navigation and … - arXiv, https://arxiv.org/html/2506.06862v1</li>
<li>Meta AI’s Segment Anything Model (SAM) Explained - Encord, https://encord.com/blog/segment-anything-model-explained/</li>
<li>Segment Anything - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2023/papers/Kirillov_Segment_Anything_ICCV_2023_paper.pdf</li>
<li>A Visual SLAM Based on Segment Anything Model for Dynamic …, https://www.researchgate.net/publication/383758592_SamSLAM_A_Visual_SLAM_Based_on_Segment_Anything_Model_for_Dynamic_Environment</li>
<li>Segment Anything in High Quality - NeurIPS, https://proceedings.neurips.cc/paper_files/paper/2023/file/5f828e38160f31935cfe9f67503ad17c-Paper-Conference.pdf</li>
<li>Self-Supervised Generative Map for Object Goal Navigation, https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Imagine_Before_Go_Self-Supervised_Generative_Map_for_Object_Goal_Navigation_CVPR_2024_paper.pdf</li>
<li>ICCV 2025 Open Access Repository, https://openaccess.thecvf.com/content/ICCV2025/html/Cao_CogNav_Cognitive_Process_Modeling_for_Object_Goal_Navigation_with_LLMs_ICCV_2025_paper.html</li>
<li>SEEK: Semantic Reasoning for Object Goal Navigation in Real …, https://www.roboticsproceedings.org/rss20/p024.pdf</li>
<li>SEEK: Semantic Reasoning for Object Goal Navigation in Real …, https://arxiv.org/html/2405.09822v1</li>
<li>Real-time Vision-Language Navigation with Spatial Reasoning - arXiv, https://arxiv.org/html/2502.00931v3</li>
<li>PONI: Potential Functions for ObjectGoal Navigation with Interaction …, https://vision.cs.utexas.edu/projects/poni/</li>
<li>PONI: Potential Functions for ObjectGoal Navigation With Interaction …, https://openaccess.thecvf.com/content/CVPR2022/papers/Ramakrishnan_PONI_Potential_Functions_for_ObjectGoal_Navigation_With_Interaction-Free_Learning_CVPR_2022_paper.pdf</li>
<li>SEEK: Semantic Reasoning for Object Goal Navigation in Real …, https://api.semanticscholar.org/arXiv:2405.09822</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>