<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:6.6.2 불확실성 하에서의 의사결정: 인식 신뢰도(Confidence Score)를 활용한 탐험 전략.</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>6.6.2 불확실성 하에서의 의사결정: 인식 신뢰도(Confidence Score)를 활용한 탐험 전략.</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 6. 오픈 보캐블러리와 시맨틱 이해 (Open-Vocabulary & Semantic Understanding)</a> / <a href="index.html">6.6 사례 연구: 자연어 명령 기반 내비게이션 (Object Goal Navigation)</a> / <span>6.6.2 불확실성 하에서의 의사결정: 인식 신뢰도(Confidence Score)를 활용한 탐험 전략.</span></nav>
                </div>
            </header>
            <article>
                <h1>6.6.2 불확실성 하에서의 의사결정: 인식 신뢰도(Confidence Score)를 활용한 탐험 전략.</h1>
<h2>1.  서론: 모호한 세계에서의 내비게이션과 인식 신뢰도의 부상</h2>
<p>현대 로보틱스, 특히 체화된 인공지능(Embodied AI) 분야에서 ’객체 목표 내비게이션(Object Goal Navigation, ObjectNav)’은 로봇이 미지의 환경에서 자연어로 지시된 객체(예: “편안한 안락의자를 찾아라”)를 탐색하고 발견하는 핵심 과제로 자리 잡았습니다. 과거의 내비게이션 시스템이 기하학적 지도(Occupancy Grid Map)를 통해 “로봇이 갈 수 있는 빈 공간은 어디인가?“라는 질문에 답하는 데 집중했다면, 현대의 의미론적 내비게이션(Semantic Navigation)은 “목표 객체는 어디에 있을 확률이 높은가?“라는 훨씬 더 복잡하고 확률적인 질문을 던집니다.</p>
<p>이 과정에서 가장 치명적인 문제는 비전-언어 모델(Vision-Language Models, VLMs)의 내재적 불확실성입니다. CLIP이나 GLIP과 같은 최신 VLM은 이미지와 텍스트 간의 연관성을 파악하는 데 탁월한 성능을 보이지만, 이들이 출력하는 코사인 유사도(Cosine Similarity)나 신뢰도 점수(Confidence Score)는 종종 보정되지 않은(Uncalibrated) 값을 가집니다. 예를 들어, 로봇이 흐릿한 물체를 보았을 때 모델은 ’소파’라는 텍스트 프롬프트에 대해 0.7의 유사도를 출력할 수 있지만, ’의자’에 대해서도 0.65를 출력할 수 있습니다. 기존의 결정론적(Deterministic) 방법론들은 이러한 값 중 가장 높은 것만을 취하여 지도를 작성하고 경로를 계획했으나, 이는 긍정 오류(False Positive)로 인한 탐색 실패나 국소 최적해(Local Minima)에 빠지는 결과를 초래했습니다.</p>
<p>본 장에서는 이러한 한계를 극복하기 위해 제안된 <strong>‘불확실성 정보를 활용한 능동적 인식(Uncertainty-Informed Active Perception)’</strong> 패러다임을 심도 있게 다룹니다. 이 전략의 핵심은 의미론적 예측을 단일 점 추정치(Point Estimate)로 다루는 것이 아니라, **신뢰도 점수(Confidence Score)와 그 분산(Variance)**을 통해 불확실성을 정량화하고, 이를 탐험 전략의 핵심 동력으로 삼는 것입니다. 우리는 불확실성이 단순한 노이즈가 아니라, 로봇이 “어디를 더 자세히 보아야 하는가?“를 결정하게 하는 정보 획득의 신호(Signal for Information Gain)임을 논증할 것입니다.</p>
<h2>2.  확률적 센서 모델링: 의미론적 불확실성의 정량화</h2>
<p>불확실성 하에서의 의사결정을 위해서는 먼저 센서 데이터(시각 정보)로부터 불확실성을 추출하고 정량화하는 수학적 모델이 필요합니다. 여기서는 VLM의 출력을 확률 분포로 변환하는 두 가지 주요 접근법, 즉 <strong>프롬프트 앙상블(Prompt Ensembling)</strong> 기법과 <strong>공간적 신뢰도(Spatial Confidence)</strong> 모델링을 분석합니다.</p>
<h3>2.1  인식론적 불확실성(Epistemic Uncertainty)과 프롬프트 앙상블</h3>
<p>일반적인 VLM 기반 시스템(예: VLFM )은 단일 프롬프트(예: “A photo of a [object]”)에 대한 유사도 점수를 그대로 사용합니다. 그러나 이는 모델이 해당 객체에 대해 얼마나 확신하는지, 혹은 모델의 지식이 부족한지를 나타내는 인식론적 불확실성을 포착하지 못합니다.</p>
<p>최근 연구(Bajpai et al., ECMR 2025)에서는 이를 해결하기 위해 대규모 언어 모델(LLM)을 활용한 프롬프트 앙상블 기법을 제안했습니다. 이 방법은 GPT-4와 같은 LLM을 사용하여 목표 객체를 묘사하는 <span class="math math-inline">N</span>개의 다양한 동의어 프롬프트 집합 <span class="math math-inline">\mathcal{P} = {p_1, p_2, \dots, p_N}</span>을 생성합니다.</p>
<ul>
<li><span class="math math-inline">p_1</span>: “A photo of a [target]”</li>
<li><span class="math math-inline">p_2</span>: “There is a [target] in the scene”</li>
<li><span class="math math-inline">p_3</span>: “Looking at a [target]”</li>
<li><span class="math math-inline">p_N</span>: “A [target] is located here”</li>
</ul>
<p>시간 <span class="math math-inline">t</span>에서의 RGB 관측 이미지 <span class="math math-inline">\mathbf{I}_t^{\text{rgb}}</span>에 대해, VLM은 각 프롬프트 <span class="math math-inline">p_i</span>와의 유사도 점수 집합 <span class="math math-inline">\mathcal{S} = {s_1, s_2, \dots, s_N}</span>를 계산합니다. 이때 의미론적 관련성(Semantic Relevance) <span class="math math-inline">S</span>는 정규 분포를 따르는 확률 변수로 모델링됩니다.<br />
<span class="math math-display">
S \sim \mathcal{N}(\mu_S, \sigma_S^2)
</span><br />
여기서 표본 평균 <span class="math math-inline">\mu_S</span>와 표본 분산 <span class="math math-inline">\sigma_S^2</span>는 다음과 같이 계산됩니다:<br />
<span class="math math-display">
\mu_S = \frac{1}{N} \sum_{i=1}^{N} s_i</span><br />
$$</p>
<p><span class="math math-display">
\sigma_S^2 = \frac{1}{N-1} \sum_{i=1}^{N} (s_i - \mu_S)^2
</span></p>
<p>이 수식에서 <span class="math math-inline">\sigma_S^2</span>는 의미론적 불확실성의 척도가 됩니다. 만약 로봇이 명확한 객체를 보고 있다면 모든 프롬프트에 대해 일관되게 높은 점수가 나와 분산이 낮을 것입니다. 반면, 객체가 모호하거나 VLM이 혼란스러워한다면 프롬프트의 미세한 어조 차이에도 점수가 크게 변동하여 높은 분산을 나타낼 것입니다.</p>
<h3>2. 우연적 불확실성(Aleatoric Uncertainty)과 공간적 신뢰도</h3>
<p>카메라 센서의 특성상, 이미지의 중심부에 있는 객체는 주변부에 있는 객체보다 왜곡이 적고 정보량이 풍부합니다. 이러한 데이터 자체의 노이즈, 즉 우연적 불확실성을 반영하기 위해 <strong>공간적 신뢰도(Spatial Confidence)</strong> 개념이 도입됩니다.</p>
<p>관측된 픽셀이 카메라의 광학 축(Optical Axis)과 이루는 각도를 <span class="math math-inline">\theta</span>, 카메라의 수평 시야각(FOV)을 <span class="math math-inline">\theta_{\text{FOV}}</span>라고 할 때, 공간적 신뢰도 가중치 <span class="math math-inline">C_V(\theta)</span>는 코사인 감쇠 함수(Cosine Decay Function)를 사용하여 모델링할 수 있습니다.<br />
<span class="math math-display">
C_V(\theta) = \cos^2 \left( \frac{\theta}{\theta_{\text{FOV}}/2} \cdot \frac{\pi}{2} \right)
</span><br />
이 함수에 따르면, 이미지의 정중앙(<span class="math math-inline">\theta=0</span>)에 위치한 픽셀은 <span class="math math-inline">C_V=1</span>의 최대 신뢰도를 가지며, 시야의 가장자리(<span class="math math-inline">\theta \approx \theta_{\text{FOV}}/2</span>)로 갈수록 신뢰도는 0에 수렴합니다.</p>
<h3>3. 통합 확률 관측 모델</h3>
<p>최종적으로 지도에 업데이트되는 관측 데이터는 단순한 점수가 아니라, 프롬프트 불확실성과 공간적 불확실성이 결합된 가우시안 분포 <span class="math math-inline">(\mu_{Z,t}, \sigma_{Z,t}^2)</span>로 정의됩니다. 투영된 지도 격자 셀 <span class="math math-inline">m</span>에 대한 관측 분산 <span class="math math-inline">\sigma_{Z,t}^2</span>는 공간적 신뢰도에 반비례하도록 설계됩니다.<br />
<span class="math math-display">
\sigma_{Z,t}^2(m) = \frac{\sigma_S^2}{C_V(\theta)} + \sigma_{\text{sensor}}^2
</span><br />
여기서 <span class="math math-inline">\sigma_{\text{sensor}}^2</span>는 센서의 기저 노이즈(Base Noise Floor)입니다. 이 수식은 매우 중요한 함의를 가집니다: <strong>“로봇이 객체를 시야의 중심으로 가져올수록(Active Perception), 관측의 불확실성(<span class="math math-inline">\sigma_{Z,t}^2</span>)은 감소한다.”</strong> 이는 추후 설명할 탐험 전략에서 로봇이 불확실한 객체를 재확인하기 위해 고개를 돌리거나 접근하게 만드는 수학적 유인이 됩니다.</p>
<h2>6.6.2.3 베이지안 기하-의미론적 지도 (Bayesian Geometric-Semantic Map)</h2>
<p>확보된 확률적 관측 데이터는 시간의 흐름에 따라 누적되어야 합니다. 단일 시점의 관측은 노이즈나 일시적인 가림(Occlusion)에 취약하기 때문입니다. 이를 위해 우리는 기하학적 정보(장애물 유무)와 의미론적 정보(목표 객체 존재 확률)를 동시에 유지하는 <strong>확률적 기하-의미론적 지도</strong>를 구축합니다.</p>
<h3>1. 지도의 구조</h3>
<p>이 지도는 두 개의 레이어로 구성된 격자 지도(Grid Map) 형태를 띱니다.</p>
<ol>
<li><strong>기하학적 레이어 (<span class="math math-inline">M_G</span>):</strong> 전통적인 점유 격자 지도(Occupancy Grid)로, 각 셀이 장애물일 확률 <span class="math math-inline">P(O)</span>를 저장합니다.</li>
<li><strong>의미론적 레이어 (<span class="math math-inline">M_S</span>):</strong> 각 셀 <span class="math math-inline">m</span>이 목표 객체와 얼마나 관련이 있는지를 나타내는 신념(Belief) 분포 <span class="math math-inline">\mathcal{N}(\mu_t(m), \sigma_t^2(m))</span>를 저장합니다.</li>
</ol>
<h3>2. 가우시안 신념의 재귀적 베이지안 갱신 (Recursive Bayesian Update)</h3>
<p>시간 <span class="math math-inline">t-1</span>에서의 셀 <span class="math math-inline">m</span>에 대한 신념이 <span class="math math-inline">\mathcal{N}(\mu_{t-1}, \sigma_{t-1}^2)</span>이고, 새로운 관측 <span class="math math-inline">Z_t \sim \mathcal{N}(\mu_{Z,t}, \sigma_{Z,t}^2)</span>가 들어왔을 때, 이를 통합하는 과정은 칼만 필터(Kalman Filter)의 측정 업데이트 단계와 동일한 형식을 따릅니다.</p>
<p><strong>정밀도(Precision) 기반 분산 갱신:</strong><br />
<span class="math math-display">
\frac{1}{\sigma_{t}^2(m)} = \frac{1}{\sigma_{t-1}^2(m)} + \frac{1}{\sigma_{Z,t}^2(m)}
</span><br />
이를 분산에 대해 정리하면:<br />
<span class="math math-display">
\sigma_{t}^2(m) = \frac{\sigma_{t-1}^2(m) \cdot \sigma_{Z,t}^2(m)}{\sigma_{t-1}^2(m) + \sigma_{Z,t}^2(m)}
</span><br />
<strong>분산 가중 평균(Variance-Weighted Mean) 갱신:</strong><br />
<span class="math math-display">
\mu_{t}(m) = \sigma_{t}^2(m) \cdot \left( \frac{\mu_{t-1}(m)}{\sigma_{t-1}^2(m)} + \frac{\mu_{Z,t}}{\sigma_{Z,t}^2(m)} \right)
</span><br />
<strong>수식의 해석 및 시사점:</strong></p>
<ul>
<li><strong>신뢰도 기반 통합:</strong> 기존 지도의 불확실성(<span class="math math-inline">\sigma_{t-1}^2</span>)이 크고 새로운 관측의 불확실성(<span class="math math-inline">\sigma_{Z,t}^2</span>)이 작다면(예: 정면에서 명확하게 봄), 지도는 새로운 관측값 쪽으로 급격하게 업데이트됩니다. 반대로, 기존 지도가 이미 확실한 상태에서 노이즈가 많은 주변부 관측이 들어오면 지도는 거의 변하지 않습니다.</li>
<li><strong>변동성 감소:</strong> 여러 번의 관측이 누적될수록 분산(<span class="math math-inline">\sigma_t^2</span>)은 단조 감소합니다. 이는 로봇이 해당 지역을 여러 번 관측할수록 확신(Confidence)이 증가함을 수학적으로 보장합니다.</li>
</ul>
<h3>3. 대안적 접근: 디리클레(Dirichlet) 분포 기반 지도와의 비교</h3>
<p>가우시안 방식 외에도, 의미론적 클래스(예: 의자, 책상, 침대 등)의 다항 분포를 모델링하기 위해 **디리클레 분포(Dirichlet Distribution)**를 사용하는 방법론(예: Evidential Grid Map)도 존재합니다.</p>
<p>디리클레 기반 접근법은 증거(Evidence) <span class="math math-inline">\mathbf{e}</span>를 농도 파라미터 <span class="math math-inline">\boldsymbol{\alpha}</span>에 누적하는 방식(<span class="math math-inline">\boldsymbol{\alpha}_t = \boldsymbol{\alpha}_{t-1} + \mathbf{e}_t</span>)을 사용합니다.</p>
<ul>
<li><strong>장점:</strong> ’무지(Ignorance)’와 ’상충(Conflict)’을 명확히 구분할 수 있습니다. 예를 들어, 데이터가 없어서 확률이 0.5인 것과, ’의자’일 확률 0.5와 ’책상’일 확률 0.5가 상충하는 상황을 구분합니다.</li>
<li><strong>단점:</strong> 연산량이 많고, 특정 목표 객체 하나만을 찾는 ObjectNav 태스크에서는 가우시안 방식의 ‘관련성 점수(Relevance Score)’ 모델링이 계획(Planning) 단계에서 더 효율적이고 직관적인 보상 함수 설계를 가능하게 합니다.</li>
</ul>
<p>아래 표는 두 가지 지도 작성 방식을 비교 요약한 것입니다.</p>
<table><thead><tr><th><strong>특징</strong></th><th><strong>가우시안 관련성 지도 (Gaussian Relevance Map)</strong></th><th><strong>디리클레 증거 지도 (Dirichlet Evidential Map)</strong></th></tr></thead><tbody>
<tr><td><strong>표현 대상</strong></td><td>목표 객체와의 의미론적 관련성 (단일 연속 변수)</td><td>전체 클래스에 대한 범주형 확률 분포 (다변수)</td></tr>
<tr><td><strong>불확실성 표현</strong></td><td>분산 (<span class="math math-inline">\sigma^2</span>)</td><td>총 증거량 (<span class="math math-inline">\sum \alpha_k</span>) 및 신념 질량</td></tr>
<tr><td><strong>갱신 방식</strong></td><td>분산 가중 평균 (Kalman Filter Style)</td><td>증거 합산 (Evidence Accumulation)</td></tr>
<tr><td><strong>장점</strong></td><td>연산 효율성 높음, MAB 보상 함수와 직접 연동 용이</td><td>다중 클래스 세분화 가능, 무지와 상충의 구분 명확</td></tr>
<tr><td><strong>주요 적용 사례</strong></td><td>특정 목표 탐색 (Target-driven Navigation)</td><td>의미론적 분할 및 전역 지도 작성</td></tr>
</tbody></table>
<hr />
<h2>6.6.2.4 MAB(Multi-Armed Bandit) 기반의 탐험 전략</h2>
<p>확률적 지도가 구축되었다면, 다음 단계는 이 지도를 바탕으로 “어디로 이동할 것인가?“를 결정하는 것입니다. 전통적인 프론티어 탐험(Frontier Exploration)은 단순히 가장 가까운 미지의 영역을 선택하거나, 결정론적 점수가 가장 높은 곳을 선택했습니다. 그러나 불확실성을 고려하는 본 전략에서는 탐험 문제를 <strong>다중 슬롯 머신(Multi-Armed Bandit, MAB)</strong> 문제로 재정의합니다.</p>
<h3>1. 프론티어(Frontier)를 슬롯 머신의 팔(Arm)로 정의</h3>
<p>로봇이 현재 도달 가능한 프론티어(알려진 공간과 미지 공간의 경계)의 집합을 <span class="math math-inline">\mathcal{F} = {f_1, f_2, \dots, f_K}</span>라고 합시다. 각 프론티어 <span class="math math-inline">f_i</span>는 MAB 문제에서의 하나의 ’팔(Arm)’에 해당합니다. 로봇이 특정 프론티어를 선택하여 이동하는 행위는 해당 팔을 당기는(Pulling) 것이며, 이때 얻는 보상은 ‘목표 객체를 발견할 확률’ 또는 ’불확실성의 감소량’입니다.</p>
<h3>2. 불확실성 정보를 반영한 보상 함수 (Reward Function)</h3>
<p>의사결정의 핵심은 각 프론티어 <span class="math math-inline">f_i</span>의 효용(Utility)을 계산하는 보상 함수 <span class="math math-inline">R(f_i)</span>의 설계입니다. 이상적인 보상 함수는 <strong>활용(Exploitation)</strong>, <strong>탐험(Exploration)</strong>, 그리고 <strong>비용(Cost)</strong> 간의 균형을 맞춰야 합니다. 이를 위해 UCB(Upper Confidence Bound) 알고리즘에서 영감을 받은 다음과 같은 선형 결합 형태의 보상 함수를 제안합니다.<br />
<span class="math math-display">
R(f_i) = \underbrace{\lambda_1 \cdot \bar{\mu}_{t}(f_i)}_{\text{활용 (Exploitation)}} + \underbrace{\lambda_2 \cdot \bar{\sigma}_{t}(f_i)}_{\text{탐험 (Exploration)}} - \underbrace{\lambda_3 \cdot C(x_{\text{robot}}, f_i)}_{\text{이동 비용 (Cost)}}
</span><br />
각 항의 의미와 역할은 다음과 같습니다:</p>
<ol>
<li><strong>활용 항 (<span class="math math-inline">\bar{\mu}_{t}(f_i)</span>):</strong> 프론티어 <span class="math math-inline">f_i</span> 주변 영역의 평균 의미론적 관련성입니다. 이 값이 높다는 것은 해당 지역에 목표 객체가 있을 확률이 높다는 뜻이므로, 로봇이 그곳을 우선적으로 방문하도록 유도합니다. (예: “저기에 소파 같은 것이 보인다.”)</li>
<li><strong>탐험 항 (<span class="math math-inline">\bar{\sigma}_{t}(f_i)</span>):</strong> 프론티어 주변의 불확실성(분산)입니다. 이 항은 <strong>‘호기심(Curiosity)’</strong> 보너스로 작용합니다. 만약 어떤 지역의 관련성 점수(<span class="math math-inline">\mu</span>)는 낮지만 불확실성(<span class="math math-inline">\sigma</span>)이 매우 높다면(예: 무언가 있지만 흐릿해서 식별 불가), 이 항 때문에 보상이 높아져 로봇이 확인하러 가게 됩니다. 이는 정보 이론의 ‘정보 이득(Information Gain)’ 또는 ’엔트로피 감소’를 근사한 것입니다.</li>
<li><strong>비용 항 (<span class="math math-inline">C(x_{\text{robot}}, f_i)</span>):</strong> 현재 로봇 위치에서 프론티어까지의 경로 거리(Geodesic Distance)입니다. 아무리 유망한 지역이라도 너무 멀리 있다면, 가까운 유망 지역을 먼저 탐색하는 것이 효율적임을 반영합니다.</li>
</ol>
<h3>3. 탐험 시나리오 분석</h3>
<p>이 보상 함수가 실제 주행에서 어떻게 작동하는지 몇 가지 시나리오를 통해 분석해 봅시다.</p>
<ul>
<li><strong>시나리오 A: “유령(Ghost) 탐지”</strong>
<ul>
<li>상황: 로봇이 먼 거리에 있는 복잡한 패턴의 벽지를 보았는데, VLM이 순간적으로 이를 ’책장’으로 오인하여 높은 점수(<span class="math math-inline">\mu</span>)를 냈지만, 프롬프트 간의 의견 불일치로 분산(<span class="math math-inline">\sigma</span>)도 매우 높은 상태입니다.</li>
<li>행동: 기존의 결정론적 탐험(VLFM 등)은 <span class="math math-inline">\mu</span>만 보고 즉시 “발견했다“고 판단하여 탐색을 종료할 위험이 있습니다. 그러나 본 전략에서는 높은 <span class="math math-inline">\sigma</span>가 유지되는 한, 로봇은 멈추지 않고 해당 지점으로 접근(탐험 항 작용)하여 더 선명한 이미지를 얻으려 합니다. 가까이 가서 확인한 결과 벽지임이 판명되면 <span class="math math-inline">\mu</span>는 낮아지고 <span class="math math-inline">\sigma</span>도 낮아져(확실히 아님), 로봇은 미련 없이 다른 곳으로 떠납니다.</li>
</ul>
</li>
<li><strong>시나리오 B: “등잔 밑이 어둡다”</strong>
<ul>
<li>상황: 로봇 근처에 목표 물체가 있지만 각도 때문에 아주 일부분만 보입니다. <span class="math math-inline">\mu</span>는 낮지만, VLM이 무언가 관련성을 감지하여 <span class="math math-inline">\sigma</span>가 약간 상승했습니다.</li>
<li>행동: 먼 곳의 미지 영역보다 이동 비용(<span class="math math-inline">C</span>)이 매우 작으므로, 작은 <span class="math math-inline">\sigma</span> 상승만으로도 보상 <span class="math math-inline">R</span>이 커져 로봇은 근처의 의심스러운 영역을 먼저 샅샅이 훑게 됩니다.</li>
</ul>
</li>
</ul>
<hr />
<h2>6.6.2.5 능동적 인식(Active Perception)의 구현 및 최적화</h2>
<p>이론적으로 정립된 모델을 실제 로봇 시스템에 구현하기 위해서는 인식, 지도 작성, 계획 모듈이 유기적으로 결합된 파이프라인이 필요합니다.</p>
<h3>1. 시스템 아키텍처 (System Architecture)</h3>
<p>아래 표는 불확실성 기반 능동적 인식 시스템의 주요 모듈과 데이터 흐름을 보여줍니다.</p>
<table><thead><tr><th><strong>모듈 (Module)</strong></th><th><strong>구성 요소 (Component)</strong></th><th><strong>역할 (Function)</strong></th><th><strong>입력 (Inputs)</strong></th><th><strong>출력 (Outputs)</strong></th></tr></thead><tbody>
<tr><td><strong>인식 (Perception)</strong></td><td><strong>VLM (CLIP/GLIP)</strong></td><td>특징 추출 및 점수화</td><td>RGB 이미지, <span class="math math-inline">N</span>개 프롬프트</td><td>유사도 점수 집합 <span class="math math-inline">\{s_i\}</span></td></tr>
<tr><td><strong>인식 (Perception)</strong></td><td><strong>확률적 센서 모델</strong></td><td>불확실성 계산 및 보정</td><td>점수 집합, 깊이(Depth) 맵</td><td>관측 분포 <span class="math math-inline">(\mu_Z, \sigma_Z^2)</span></td></tr>
<tr><td><strong>지도 작성 (Mapping)</strong></td><td><strong>베이지안 갱신기</strong></td><td>시공간적 데이터 융합</td><td>관측 분포, 로봇 위치(Pose)</td><td>의미론적 지도 <span class="math math-inline">M_S(\mu, \sigma)</span></td></tr>
<tr><td><strong>계획 (Planning)</strong></td><td><strong>프론티어 추출기</strong></td><td>후보 탐색지 생성</td><td>기하학적 지도 <span class="math math-inline">M_G</span></td><td>프론티어 목록 <span class="math math-inline">\{f_i\}</span></td></tr>
<tr><td><strong>계획 (Planning)</strong></td><td><strong>MAB 솔버</strong></td><td>최적 목표점 선정</td><td><span class="math math-inline">\{f_i\}</span>, <span class="math math-inline">M_S</span>, <span class="math math-inline">M_G</span></td><td>목표 좌표 <span class="math math-inline">x_{\text{goal}}</span></td></tr>
<tr><td><strong>제어 (Control)</strong></td><td><strong>로컬 플래너</strong></td><td>경로 생성 및 주행</td><td><span class="math math-inline">x_{\text{goal}}</span>, 장애물 정보</td><td>속도 명령 <span class="math math-inline">(v, \omega)</span></td></tr>
</tbody></table>
<h3>2. 신뢰도 게이팅 정지 조건 (Confidence-Gated Stop Condition)</h3>
<p>탐험의 종료 시점, 즉 “목표를 찾았다“고 선언하는 시점을 결정하는 것은 매우 중요합니다. 본 전략에서는 단순한 임계값 넘기기 대신, <strong>이중 검증(Dual Verification)</strong> 메커니즘을 사용합니다.</p>
<p>로봇은 다음 두 조건이 <em>동시에</em> 충족될 때만 탐색을 종료(Stop Action)합니다:</p>
<ol>
<li><strong>높은 관련성:</strong> 지도상에서 가장 높은 평균 점수가 임계값을 초과함 (<span class="math math-inline">\max_m \mu_t(m) &gt; \tau_{\mu}</span>).</li>
<li><strong>낮은 불확실성:</strong> 해당 지점의 분산이 충분히 낮음 (<span class="math math-inline">\sigma_t(m) &lt; \tau_{\sigma}</span>).</li>
</ol>
<p>이 조건은 로봇이 “아마도 이것인 것 같은데?”(높은 <span class="math math-inline">\mu</span>, 높은 <span class="math math-inline">\sigma</span>) 상태에서는 멈추지 않고, “확실히 이것이다!”(높은 <span class="math math-inline">\mu</span>, 낮은 <span class="math math-inline">\sigma</span>) 상태가 될 때까지 능동적으로 정보를 더 수집하도록 강제합니다. 이는 GLIP과 같은 객체 탐지 모델이 제공하는 신뢰도 점수(Confidence Score)를 활용할 때도 동일하게 적용될 수 있습니다.</p>
<h3>3. GLIP 및 Grounding DINO 활용 시 고려사항</h3>
<p>최근 연구에서는 CLIP 기반의 전체 이미지 유사도 대신, GLIP이나 Grounding DINO와 같은 개방형 어휘(Open-Vocabulary) 객체 탐지기를 사용하기도 합니다. 이 모델들은 바운딩 박스(Bounding Box)와 함께 각 객체에 대한 신뢰도 점수(Logit)를 제공합니다. 이 경우, 프롬프트 앙상블 대신 모델이 출력하는 <strong>Logit 자체를 불확실성 지표로 활용</strong>하거나, 박스의 크기와 위치에 따른 <strong>기하학적 불확실성</strong>을 결합하여 지도에 투영하는 방식으로 본 프레임워크를 확장할 수 있습니다. GLIP의 신뢰도 점수는 보통 0~1 사이로 정규화되지만, 이를 확률 분포의 파라미터(예: 베타 분포의 <span class="math math-inline">\alpha, \beta</span>)로 매핑하여 베이지안 업데이트에 활용하는 연구도 진행되고 있습니다.</p>
<hr />
<h2>6.6.2.6 실험적 검증 및 비교 분석</h2>
<p>이 탐험 전략의 유효성은 다양한 시뮬레이션 환경(예: Habitat, AI2-THOR)에서의 실험을 통해 입증되었습니다. 주요 비교 대상(Baseline)과 성능 지표를 분석해 봅시다.</p>
<h3>1. 비교 대상 (Baselines)</h3>
<ul>
<li><strong>Random/Frontier Exploration:</strong> 의미론적 정보 없이 단순히 공간을 넓히는 방식. 가장 낮은 성능을 보입니다.</li>
<li><strong>Greedy Semantic (예: VLFM):</strong> 불확실성(<span class="math math-inline">\sigma</span>)을 무시하고 평균 점수(<span class="math math-inline">\mu</span>)가 가장 높은 곳으로만 이동하는 방식.</li>
<li><strong>Entropy-Based Only:</strong> 객체의 존재 여부(<span class="math math-inline">\mu</span>)보다 정보 획득(<span class="math math-inline">\sigma</span>)에만 치중하는 방식. 목표와 무관하게 복잡한 곳만 찾아다니는 부작용이 있습니다.</li>
</ul>
<h3>2. 성능 평가 결과 요약</h3>
<p>ECMR 2025 등 최신 문헌의 결과를 종합하면, <strong>불확실성 기반 능동적 인식(Uncertainty-Informed Active Perception)</strong> 전략은 다음과 같은 성과를 보입니다.</p>
<ul>
<li><strong>성공률(Success Rate) 증가:</strong> 모호한 객체를 성급하게 목표로 판단하여 실패하는 경우가 줄어들어, Greedy 방식 대비 10~15% 이상의 성공률 향상을 보입니다.</li>
<li><strong>SPL(Success Weighted by Path Length) 개선:</strong> 언뜻 보기에 불확실성 확인을 위해 이리저리 움직이는 것이 비효율적으로 보일 수 있습니다. 그러나 긍정 오류(False Positive)에 속아 맵의 반대편으로 헛걸음하는 경우를 획기적으로 줄여주기 때문에, 전체적인 평균 이동 거리는 오히려 감소하여 높은 SPL을 기록합니다.</li>
<li><strong>강건성(Robustness):</strong> 프롬프트의 미세한 변화나 조명 변화에 대해 훨씬 더 일관된 탐색 경로를 생성합니다.</li>
</ul>
<h2>6.6.2.7 결론 및 향후 전망</h2>
<p>본 절에서는 인식 신뢰도(Confidence Score)를 단순한 참고 지표가 아닌, 탐험 전략의 핵심 변수로 격상시켰습니다. 프롬프트 앙상블을 통한 인식론적 불확실성의 추출, 베이지안 갱신을 통한 시간적 통합, 그리고 MAB 기반의 보상 함수 설계는 로봇이 “자신이 무엇을 모르는지(Knowing what it doesn’t know)“를 인지하고 행동하게 만듭니다.</p>
<p>향후 연구 방향으로는 **컨포멀 예측(Conformal Prediction)**을 도입하여 VLM의 신뢰도 점수를 통계적으로 엄밀하게 보정(Calibrate)하는 연구와, 2D 그리드 맵을 넘어 **3D 의미론적 가우시안 스플래팅(Semantic Gaussian Splatting)**에 불확실성을 통합하여 수직적 공간 정보까지 활용하는 방법론이 유망합니다. 또한, 단순한 로컬 불확실성을 넘어 LLM이 지도 전체의 맥락을 읽고 “부엌에 토스터가 있을 확률이 높다“는 식의 <strong>전역적(Global) 의미론적 추론</strong>을 베이지안 사전 확률(Prior)로 주입하는 계층적 탐험 전략으로의 확장이 기대됩니다.</p>
<p>불확실성은 이제 극복해야 할 장애물이 아니라, 로봇을 지능적인 탐험가로 만드는 가장 강력한 동기 부여제입니다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>GUIDEd Agents: Enhancing Navigation Policies through Task-Specific Uncertainty Abstraction in Localization-Limited Environments - arXiv, 1월 28, 2026에 액세스, https://arxiv.org/html/2410.15178v4</li>
<li>Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual Navigation - arXiv, 1월 28, 2026에 액세스, https://arxiv.org/html/2510.00441v3</li>
<li>Uncertainty-Informed Active Perception for Open Vocabulary Object Goal Navigation - arXiv, 1월 28, 2026에 액세스, https://arxiv.org/html/2506.13367v1</li>
<li>VLFM: Vision-Language Frontier Maps For Zero-Shot Semantic Navigation | PDF - Scribd, 1월 28, 2026에 액세스, https://www.scribd.com/document/859578486/2312-03275v1</li>
<li>(PDF) Uncertainty-Informed Active Perception for Open Vocabulary Object Goal Navigation, 1월 28, 2026에 액세스, https://www.researchgate.net/publication/392736884_Uncertainty-Informed_Active_Perception_for_Open_Vocabulary_Object_Goal_Navigation</li>
<li>Uncertainty-Informed Active Perception for Open Vocabulary Object Goal Navigation, 1월 28, 2026에 액세스, https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/bajpaio2025ecmr.pdf</li>
<li>You’ve Got to Feel It To Believe It: Multi-Modal Bayesian Inference for Semantic and Property Prediction - arXiv, 1월 28, 2026에 액세스, https://arxiv.org/html/2402.05872v4</li>
<li>[Literature Review] Uncertainty-aware Semantic Mapping in Off-road Environments with Dempster-Shafer Theory of Evidence - Moonlight, 1월 28, 2026에 액세스, https://www.themoonlight.io/en/review/uncertainty-aware-semantic-mapping-in-off-road-environments-with-dempster-shafer-theory-of-evidence</li>
<li>Learning Semantic Maps from Natural Language Descriptions - Robotics, 1월 28, 2026에 액세스, https://www.roboticsproceedings.org/rss09/p04.pdf</li>
<li>EvidMTL: Evidential Multi-Task Learning for Uncertainty-Aware Semantic Surface Mapping from Monocular RGB Images - arXiv, 1월 28, 2026에 액세스, https://arxiv.org/html/2503.04441v1</li>
<li>[2506.13367] Uncertainty-Informed Active Perception for Open Vocabulary Object Goal Navigation - arXiv, 1월 28, 2026에 액세스, https://arxiv.org/abs/2506.13367</li>
<li>Learning-Augmented Model-Based Planning for Visual Exploration | Request PDF, 1월 28, 2026에 액세스, https://www.researchgate.net/publication/376506846_Learning-Augmented_Model-Based_Planning_for_Visual_Exploration</li>
<li>Understanding while Exploring: Semantics-driven Active Mapping - arXiv, 1월 28, 2026에 액세스, https://arxiv.org/html/2506.00225v1</li>
<li>Measurement-Driven Planner - Emergent Mind, 1월 28, 2026에 액세스, https://www.emergentmind.com/topics/measurement-driven-planner</li>
<li>A Survey of Object Goal Navigation - ORCA – Online Research @ Cardiff, 1월 28, 2026에 액세스, https://orca.cardiff.ac.uk/id/eprint/167432/1/ObjectGoalNavigationSurveyTASE.pdf</li>
<li>History-Augmented Vision-Language Models for Frontier-Based Zero-Shot Object Navigation This material is based upon work supported by the National Aeronautics and Space Administration (NASA) under award number 80NSSC23K1393, the National Science Foundation under Grant Number CNS-2232048. - arXiv, 1월 28, 2026에 액세스, https://arxiv.org/html/2506.16623v1</li>
<li>Grounding DINO: Open Vocabulary Object Detection on Videos - PyImageSearch, 1월 28, 2026에 액세스, https://pyimagesearch.com/2025/12/08/grounding-dino-open-vocabulary-object-detection-on-videos/</li>
<li>Language-guided Object Picking for Robots - Lund University Publications, 1월 28, 2026에 액세스, https://lup.lub.lu.se/student-papers/record/9161648/file/9161682.pdf</li>
<li>SemNav: A Model-Based Planner for Zero-Shot Object Goal Navigation Using Vision-Foundation Models - arXiv, 1월 28, 2026에 액세스, https://arxiv.org/html/2506.03516v1</li>
<li>UniDetector: Towards Universal Object Detection With Heterogeneous Supervision - IEEE Xplore, 1월 28, 2026에 액세스, https://ieeexplore.ieee.org/iel8/34/11026037/10552883.pdf</li>
<li>Language-Driven Zero-Shot Object Navigation via Dynamic Probabilistic Strategy and Large Language Models - IEEE Xplore, 1월 28, 2026에 액세스, https://ieeexplore.ieee.org/iel8/6287639/10820123/11175413.pdf</li>
<li>Robotic Navigation using Entropy-Based Exploration - Semantic Scholar, 1월 28, 2026에 액세스, https://www.semanticscholar.org/paper/Robotic-Navigation-using-Entropy-Based-Exploration-Usama-Chang/684c1ac1f24fd04cd30e159875bcbcfcbc7ba763</li>
<li>[PDF] ObjectNav Revisited: On Evaluation of Embodied Agents Navigating to Objects, 1월 28, 2026에 액세스, https://www.semanticscholar.org/paper/ObjectNav-Revisited%3A-On-Evaluation-of-Embodied-to-Batra-Gokaslan/19c8c5a32cb0d77cd22ee1201e31306f0cd6100a</li>
<li>CLASH: Collaborative Large-Small Hierarchical Framework for Continuous Vision-and-Language Navigation - arXiv, 1월 28, 2026에 액세스, https://arxiv.org/html/2512.10360v2</li>
<li>Active-SLAM-Paper-List - GitHub, 1월 28, 2026에 액세스, https://github.com/Active-SLAM/Active-SLAM-Paper-List</li>
<li>Entropy-based Exploration Conduction for Multi-step Reasoning - ACL Anthology, 1월 28, 2026에 액세스, https://aclanthology.org/2025.findings-acl.201/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>