<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:6.4 2D 시맨틱을 3D 공간으로: 공간 지능의 확장 (Spatial AI)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>6.4 2D 시맨틱을 3D 공간으로: 공간 지능의 확장 (Spatial AI)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 6. 오픈 보캐블러리와 시맨틱 이해 (Open-Vocabulary & Semantic Understanding)</a> / <a href="index.html">6.4 2D 시맨틱을 3D 공간으로: 공간 지능의 확장 (Spatial AI)</a> / <span>6.4 2D 시맨틱을 3D 공간으로: 공간 지능의 확장 (Spatial AI)</span></nav>
                </div>
            </header>
            <article>
                <h1>6.4 2D 시맨틱을 3D 공간으로: 공간 지능의 확장 (Spatial AI)</h1>
<h2>1.  서론: 기하학적 지도를 넘어 의미론적 공간으로</h2>
<p>로봇 공학의 오랜 역사 속에서 ’지도(Map)’는 로봇이 자신의 위치를 파악하고(Localization) 경로를 계획하는(Planning) 데 필수적인 기하학적 데이터베이스였다. 전통적인 SLAM(Simultaneous Localization and Mapping) 기술은 공간을 점유 그리드(Occupancy Grid)나 포인트 클라우드(Point Cloud) 형태로 표현하여, 로봇에게 “여기는 비어 있고, 저기는 막혀 있다“는 물리적 정보만을 제공했다. 그러나 인간이 인식하는 공간은 단순한 좌표의 집합이 아니다. 우리는 공간을 “소파와 TV 사이”, “주방의 식탁 위”, “빨간색 머그컵이 있는 곳“과 같이 의미론적(Semantic) 맥락으로 이해한다. 로봇이 인간과 공존하며 복잡한 명령을 수행하기 위해서는 이러한 기하학적 공간에 언어적, 시각적 의미를 부여하는 **공간 지능(Spatial AI)**의 확장이 필수적이다.</p>
<p>최근 대규모 시각-언어 모델(Vision-Language Models, VLMs)과 파운데이션 모델(Foundation Models)의 비약적인 발전은 이미지 내의 객체를 식별하고 설명하는 데 있어 인간 수준, 혹은 그 이상의 성능을 보여주고 있다. CLIP(Contrastive Language-Image Pre-training)과 같은 모델은 텍스트와 이미지를 동일한 임베딩 공간에 매핑함으로써, “개방형 어휘(Open-Vocabulary)” 인식을 가능하게 했다. 즉, 사전 정의된 클래스(예: 의자, 책상, 컵)에 국한되지 않고, 학습하지 않은 임의의 객체나 속성(예: “빈티지 스타일의 가죽 의자”)까지도 인식할 수 있게 된 것이다.</p>
<p>본 절에서는 2D 이미지 기반의 파운데이션 모델이 가진 풍부한 시맨틱 정보를 3D 공간 표현으로 승격(Lifting)시키는 최신 방법론들을 심도 있게 다룬다. 2D 시맨틱을 3D로 확장하는 과정은 단순한 데이터의 투영이 아니다. 이는 2D 이미지의 픽셀 단위 정보가 3D 공간의 물리적 좌표와 정합되어야 하며, 시점 변화에 따른 일관성(Consistency)을 유지해야 하고, 로봇의 제한된 컴퓨팅 자원 내에서 효율적으로 쿼리(Query)될 수 있어야 하는 고난도의 최적화 문제이다.</p>
<p>우리는 먼저 암시적 표현(Implicit Representation)인 NeRF(Neural Radiance Fields)에 언어를 결합한 <strong>LERF</strong>를 시작으로, 명시적 표현(Explicit Representation)으로서 실시간 성능을 극대화한 <strong>LangSplat</strong>, 지도 기반의 접근법인 <strong>VLMaps</strong>, 그리고 멀티모달 융합을 통한 <strong>ConceptFusion</strong>과 3D 증류 기법인 <strong>OpenScene</strong>에 이르기까지, 현재 로봇 공학계를 선도하는 핵심 기술들을 수학적 원리와 아키텍처 관점에서 분석한다.</p>
<h2>2.  뉴럴 필드 기반의 언어 임베딩: LERF (Language Embedded Radiance Fields)</h2>
<p><strong>LERF: Language Embedded Radiance Fields</strong> 는 NeRF가 가진 기하학적 복원 능력에 CLIP의 언어 이해 능력을 결합하여, 3D 공간 전체를 텍스트로 검색 가능한 ’언어 필드(Language Field)’로 변환하는 방법론이다. NeRF가 공간의 각 지점 <span class="math math-inline">(x, y, z)</span>와 시점 <span class="math math-inline">(\theta, \phi)</span>에 대해 색상(RGB)과 밀도(<span class="math math-inline">\sigma</span>)를 출력한다면, LERF는 여기에 고차원의 언어 임베딩 벡터(<span class="math math-inline">f_{\text{lang}}</span>)를 추가로 출력하도록 네트워크를 확장한다.</p>
<h3>2.1  볼륨 렌더링을 통한 언어 필드의 최적화</h3>
<p>LERF의 핵심 아이디어는 3D 공간상의 언어 임베딩을 2D 이미지 평면으로 렌더링했을 때, 그 결과가 사전 학습된 CLIP 모델이 해당 2D 뷰에서 추출한 임베딩과 일치하도록 학습시키는 것이다. 이는 별도의 3D 레이블링 데이터 없이 2D 이미지만으로 3D 시맨틱을 학습할 수 있게 한다.</p>
<p>NeRF의 전통적인 볼륨 렌더링 방정식은 광선(Ray) <span class="math math-inline">\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}</span>를 따라 색상을 적분한다. LERF는 이를 확장하여 언어 임베딩을 적분한다.<br />
<span class="math math-display">
\hat{\phi}_{\text{lang}}(\mathbf{r}) = \int_{t_n}^{t_f} T(t)\sigma(t) f_{\text{lang}}(\mathbf{r}(t), s(t)) \, dt
</span><br />
여기서 <span class="math math-inline">T(t) = \exp(-\int_{t_n}^t \sigma(u)du)</span>는 누적 투과율(Transmittance)을 의미하며, <span class="math math-inline">\sigma(t)</span>는 해당 지점의 밀도이다. 중요한 점은 <span class="math math-inline">f_{\text{lang}}</span>이 위치 <span class="math math-inline">\mathbf{r}(t)</span>뿐만 아니라 스케일 파라미터 <span class="math math-inline">s(t)</span>에도 의존한다는 것이다. 이는 CLIP과 같은 시각-언어 모델이 이미지의 국소적인 패치(Local patch)와 전역적인 문맥(Global context)을 다르게 인식하는 특성을 반영하기 위함이다. 카메라로부터 멀어질수록 투영되는 영역(Receptive field)이 커지므로, 광선을 따라 샘플링되는 각 지점의 스케일 <span class="math math-inline">s(t)</span>는 카메라로부터의 거리에 비례하여 증가하도록 모델링된다.<br />
<span class="math math-display">
s(t) = s_{\text{img}} \cdot \frac{t}{f_{xy}}
</span><br />
여기서 <span class="math math-inline">f_{xy}</span>는 카메라의 초점 거리이다. 이러한 멀티 스케일 접근은 LERF가 “작은 컵의 손잡이“부터 “거실 전체“와 같은 다양한 크기의 시맨틱 쿼리를 처리할 수 있게 하는 기반이 된다.</p>
<h3>2.2  손실 함수 및 학습 전략</h3>
<p>LERF의 학습은 기하학적 구조를 학습하는 NeRF 손실과 언어 임베딩을 학습하는 언어 손실의 합으로 구성된다. 기하학적 구조가 먼저 안정화된 후 언어 필드를 학습시키거나 동시에 학습시킬 수 있다. 언어 손실 함수는 렌더링된 언어 임베딩 <span class="math math-inline">\hat{\phi}_{\text{lang}}</span>과 CLIP 이미지 인코더로부터 추출된 피라미드 형태의 다중 스케일 지상실측(Ground Truth) 임베딩 <span class="math math-inline">\phi_{\text{gt}}</span> 간의 코사인 유사도(Cosine Similarity)를 최대화하는 방향으로 정의된다.<br />
<span class="math math-display">
\mathcal{L}_{\text{lang}} = \mathbb{E}_{\mathbf{r}} \left[ 1 - \frac{\hat{\phi}_{\text{lang}}(\mathbf{r}) \cdot \phi_{\text{gt}}(\mathbf{r})}{\|\hat{\phi}_{\text{lang}}(\mathbf{r})\| \|\phi_{\text{gt}}(\mathbf{r})\|} \right]
</span><br />
이때, 단순히 CLIP 임베딩만 학습할 경우 3D 공간상에서 객체의 경계가 불명확해지거나(Blurry), 텍스처가 없는 영역에서 언어 필드가 모호해지는 현상이 발생할 수 있다. 이를 보완하기 위해 LERF는 자기지도학습 모델인 DINO(Distillation of Self-supervised Networks)의 특징을 정규화기(Regularizer)로 활용한다. DINO는 의미론적 분할 능력이 뛰어나 객체와 배경을 명확히 구분하는 경향이 있으므로, 이를 통해 언어 필드의 공간적 응집력을 강화한다.</p>
<h3>2.3  LERF의 한계와 LERF-TOGO: 객체 중심 조작으로의 확장</h3>
<p>LERF는 혁신적인 방법론이지만, 로봇 조작(Manipulation)과 같은 정밀한 작업에 적용하기에는 몇 가지 한계를 보인다. 첫째, LERF의 쿼리 결과인 관련성 맵(Relevancy map)은 종종 객체의 형상과 정확히 일치하지 않고 주변으로 번지는 경향이 있다. 둘째, ’객체(Object)’라는 명시적인 개념이 없고 연속적인 필드로만 존재하기 때문에, “컵의 손잡이“와 같은 부분(Part) 쿼리와 “컵“이라는 전체(Whole) 쿼리를 구분하여 처리하는 데 어려움이 있다.</p>
<p>이를 해결하기 위해 제안된 <strong>LERF-TOGO (Task-Oriented Grasping of Objects)</strong> 는 LERF 위에 명시적인 객체 마스킹 과정을 추가한다.</p>
<ol>
<li><strong>3D 객체 마스크 추출:</strong> DINO 특징을 활용하여 3D 공간상에서 객체와 배경을 분리하는 마스크를 생성한다. DINO는 CLIP보다 객체의 경계를 탐지하는 데 훨씬 유리하다.</li>
<li><strong>조건부 쿼리(Conditional Querying):</strong> 추출된 3D 마스크 내에서만 LERF 쿼리를 수행한다. 이를 통해 배경 노이즈를 제거하고 객체 내부의 특정 부위(예: 손잡이)에 대한 활성화를 명확히 한다.</li>
<li><strong>파지 분포 생성:</strong> 최종적으로 3D 마스크와 정제된 언어 필드 활성화를 기반으로 로봇이 파지해야 할 최적의 위치와 방향(Pose)을 분포 형태로 출력한다.</li>
</ol>
<p>실험 결과, LERF-TOGO는 31개의 물리적 객체에 대한 파지 실험에서 81%의 부품 식별 성공률을 보이며, 순수 LERF 대비 월등한 조작 성능을 입증했다. 이는 2D 시맨틱을 3D로 확장할 때, 순수한 필드 기반 접근법보다 기하학적 제약(마스크)을 결합한 하이브리드 접근법이 로봇의 물리적 상호작용에 더 적합함을 시사한다.</p>
<h2>3.  3D 가우시안 스플래팅과 언어의 결합: LangSplat</h2>
<p>NeRF 기반의 LERF는 높은 렌더링 품질과 연속적인 표현력을 제공하지만, 픽셀마다 수백 번의 신경망 연산을 수행해야 하는 볼륨 렌더링의 특성상 추론 속도가 매우 느리다는 치명적인 단점이 있다. 이는 실시간성이 생명인 로봇 내비게이션이나 AR/VR 애플리케이션에서 큰 걸림돌이 된다. 이를 극복하기 위해 등장한 <strong>LangSplat: 3D Language Gaussian Splatting</strong> 은 최근 3D 비전 분야를 혁신하고 있는 3D Gaussian Splatting(3D-GS) 기술을 언어 필드에 도입하여, 기존 LERF 대비 약 199배 빠른 렌더링 속도를 달성했다.</p>
<h3>3.1  3D 가우시안 스플래팅과 타일 기반 언어 래스터화</h3>
<p>LangSplat은 3D 공간을 수만에서 수백만 개의 3D 가우시안(Gaussian) 타원체들의 집합으로 표현한다. 각 가우시안 <span class="math math-inline">G_i</span>는 다음과 같은 속성을 가진다.</p>
<ul>
<li><strong>위치 (Mean):</strong> <span class="math math-inline">\boldsymbol{\mu}_i \in \mathbb{R}^3</span></li>
<li><strong>공분산 (Covariance):</strong> <span class="math math-inline">\boldsymbol{\Sigma}_i \in \mathbb{R}^{3 \times 3}</span> (회전 및 스케일로 분해됨)</li>
<li><strong>불투명도 (Opacity):</strong> $\alpha_i \in $</li>
<li><strong>색상 (Color):</strong> <span class="math math-inline">\mathbf{c}_i \in \mathbb{R}^3</span> (Spherical Harmonics 계수)</li>
<li><strong>언어 임베딩 (Language Embedding):</strong> <span class="math math-inline">\mathbf{l}_i \in \mathbb{R}^D</span></li>
</ul>
<p>LangSplat의 핵심은 색상을 렌더링하는 것과 동일한 방식으로 언어 임베딩을 2D 화면에 투사(Splatting)하는 것이다. 이를 위해 타일 기반 래스터화(Tile-based Rasterization) 기법을 사용한다. 특정 픽셀 <span class="math math-inline">v</span>에서의 렌더링된 언어 특징 <span class="math math-inline">\mathbf{L}(v)</span>는 해당 픽셀에 투영되는 가우시안들의 언어 임베딩을 깊이 순서(Depth-order)대로 알파 블렌딩(Alpha blending)하여 계산된다.<br />
<span class="math math-display">
\mathbf{L}(v) = \sum_{i \in \mathcal{N}} \mathbf{l}_i \alpha_i&#39; \prod_{j=1}^{i-1} (1 - \alpha_j&#39;)
</span><br />
여기서 <span class="math math-inline">\mathcal{N}</span>은 픽셀 <span class="math math-inline">v</span>에 영향을 주는 가우시안들의 집합이며, <span class="math math-inline">\alpha_i&#39;</span>는 2D로 투영된 가우시안의 불투명도이다. 이 과정은 완전히 병렬화 가능하며, 미분 가능한 래스터라이저를 통해 역전파 학습이 가능하다. NeRF의 값비싼 MLP 연산과 샘플링 과정을 단순한 행렬 연산과 정렬(Sorting)로 대체함으로써 압도적인 속도 향상을 이뤄냈다.</p>
<h3>3.2  장면 별 언어 오토인코더 (Scene-wise Language Autoencoder)</h3>
<p>속도 문제는 해결했지만, 메모리 문제가 남는다. CLIP 임베딩은 보통 512차원 이상의 고차원 벡터이다. 수백만 개의 가우시안 각각에 512차원 벡터를 저장하면 GPU 메모리가 순식간에 고갈된다 (예: 100만 개 가우시안 <span class="math math-inline">\times</span> 512차원 <span class="math math-inline">\times</span> 4바이트 <span class="math math-inline">\approx</span> 2GB, 씬이 커지면 수십 GB 필요).</p>
<p>LangSplat은 이를 해결하기 위해 **장면 특화 오토인코더 (Scene-specific Autoencoder)**를 도입한다.</p>
<ol>
<li>
<p><strong>인코더 (Encoder) <span class="math math-inline">E</span>:</strong> 512차원의 CLIP 특징 <span class="math math-inline">\mathbf{f}*{\text{CLIP}}</span>을 매우 낮은 차원(예: 3차원)의 잠재 벡터 <span class="math math-inline">\mathbf{h} \in \mathbb{R}^d</span>로 압축한다. <span class="math math-display">\mathbf{h} = E(\mathbf{f}*{\text{CLIP}})</span></p>
</li>
<li>
<p><strong>3D 가우시안 학습:</strong> 가우시안들은 고차원 벡터 대신 이 3차원 잠재 벡터 <span class="math math-inline">\mathbf{h}</span>를 속성으로 저장하고 학습한다. 이는 메모리 사용량을 획기적으로(약 1/170) 줄인다.</p>
</li>
<li>
<p><strong>디코더 (Decoder) <span class="math math-inline">\Psi</span>:</strong> 렌더링 시에는 3차원 잠재 벡터를 래스터화한 후, 디코더를 통해 다시 512차원 CLIP 공간으로 복원하여 텍스트 쿼리와 비교한다.<br />
<span class="math math-display">
\hat{\mathbf{f}}_{\text{CLIP}} = \Psi(\mathbf{F}_{\text{latent}})
</span></p>
</li>
</ol>
<p>오토인코더의 학습은 재구성 손실(L1 Loss)과 코사인 유사도 손실을 결합하여 수행된다.<br />
<span class="math math-display">
\mathcal{L}_{\text{ae}} = \|\mathbf{f}_{\text{CLIP}} - \Psi(E(\mathbf{f}_{\text{CLIP}}))\|_1 + \lambda (1 - \cos(\mathbf{f}_{\text{CLIP}}, \Psi(E(\mathbf{f}_{\text{CLIP}}))))
</span><br />
이 전략은 메모리 효율성을 극대화하면서도 언어적 표현력을 유지하는 LangSplat의 핵심 기술이다.</p>
<h3>6.4.3.3 SAM을 이용한 계층적 시맨틱 (Hierarchical Semantics)</h3>
<p>LangSplat은 LERF가 겪었던 ‘포인트 모호성(Point Ambiguity)’ 문제를 해결하기 위해 SAM(Segment Anything Model)을 적극적으로 활용한다. 하나의 3D 점은 시각적 관점에서 여러 의미를 가질 수 있다. 예를 들어, 곰 인형의 코에 해당하는 점은 “코(Nose)”, “머리(Head)”, “인형(Doll)“이라는 의미를 동시에 내포한다. 기존의 포인트 기반 학습은 이러한 계층적 의미를 하나의 벡터에 뭉뚱그려 학습함으로써 경계가 모호해지는 문제가 있었다.</p>
<p>LangSplat은 SAM을 사용하여 학습 이미지로부터 세 가지 레벨의 마스크를 생성한다.</p>
<ul>
<li><strong>Whole Level:</strong> 객체 전체 (예: 곰 인형)</li>
<li><strong>Part Level:</strong> 객체의 주요 부분 (예: 머리)</li>
<li><strong>Sub-part Level:</strong> 객체의 세부 요소 (예: 코)</li>
</ul>
<p>각 계층별로 독립적인 언어 특징 <span class="math math-inline">f_w, f_p, f_s</span>를 학습시킴으로써, 사용자의 쿼리 수준(추상적 vs 구체적)에 따라 적절한 3D 영역을 명확한 경계(Crisp boundaries)와 함께 반환할 수 있다. 실험 결과, LangSplat은 LERF 데이터셋의 3D 객체 위치 추정에서 84.3%의 정확도를 기록하여 LERF(73.6%)를 크게 앞섰으며, 특히 “Waldo 찾기“와 같이 작고 정교한 객체를 찾는 작업에서 탁월한 성능을 보였다.</p>
<h2>6.4.4 그리드 및 맵 기반 접근: VLMaps (Visual Language Maps)</h2>
<p>뉴럴 필드나 가우시안 스플래팅이 3D 공간을 시각적으로 재구성하는 데 중점을 둔다면, 로봇의 자율 주행(Navigation)을 위해서는 보다 구조화된 지도 형태의 표현이 필요하다. <strong>VLMaps</strong> 는 사전 학습된 시각-언어 모델(LSeg, OpenSeg 등)의 픽셀 임베딩을 로봇의 주행 공간인 2차원 그리드 맵(Top-down Grid Map)에 투영하여 생성된다.</p>
<h3>6.4.4.1 맵 생성 및 업데이트 알고리즘</h3>
<p>VLMaps의 생성 과정은 다음과 같다. 로봇이 RGB-D 카메라를 통해 환경을 탐색할 때, RGB 이미지는 VLM(예: LSeg)을 통과하여 픽셀별 고차원 언어 임베딩을 생성한다. 동시에 깊이(Depth) 정보와 로봇의 포즈(Pose) 정보를 사용하여 각 픽셀을 3D 포인트 클라우드로 역투영(Back-projection)한다.<br />
<span class="math math-display">
\mathbf{p}_{\text{world}} = \mathbf{T}_{\text{robot}} \mathbf{K}^{-1} [u, v, d]^T
</span><br />
이렇게 얻어진 3D 포인트들은 세계 좌표계(World Coordinate)상의 <span class="math math-inline">(x, y)</span> 위치에 해당하는 그리드 셀에 매핑된다. 한 셀에 여러 프레임의 데이터가 누적될 수 있으므로, 평균(Average) 연산 등을 통해 업데이트한다.<br />
<span class="math math-display">
M_t[u, v] = \frac{n \cdot M_{t-1}[u, v] + \mathbf{f}_{\text{new}}}{n+1}
</span><br />
최종적으로 생성된 VLMaps는 <span class="math math-inline">H \times W \times C</span> 크기의 텐서(Tensor)가 된다. 여기서 <span class="math math-inline">C</span>는 언어 임베딩의 차원(예: CLIP의 경우 512)이다.</p>
<h3>6.4.4.2 자연어 명령 기반 내비게이션과 코드 생성 (Code as Policies)</h3>
<p>VLMaps의 가장 큰 장점은 LLM(Large Language Models)과의 결합을 통해 복잡한 자연어 명령을 실행 가능한 로봇 제어 코드로 변환할 수 있다는 점이다. 이를 <strong>Code as Policies</strong>라고 부른다.</p>
<p>예를 들어, “소파와 TV 사이로 이동해“라는 명령이 주어졌을 때, 시스템은 다음과 같이 작동한다.</p>
<ol>
<li><strong>랜드마크 인덱싱(Landmark Indexing):</strong> 텍스트 인코더를 통해 ’sofa’와 ’TV’의 임베딩 벡터를 얻고, VLMaps 전체 텐서와 코사인 유사도를 계산하여 두 객체의 위치(좌표)를 찾는다.</li>
<li><strong>LLM 코드 생성:</strong> LLM은 자연어 명령을 해석하여 Python 함수 호출로 변환한다.</li>
</ol>
<pre><code class="language-Python"># 사용자 명령: "소파와 TV 사이로 이동해"
pos_sofa = robot.get_pos('sofa')
pos_tv = robot.get_pos('TV')
target_pos = (pos_sofa + pos_tv) / 2
robot.move_to(target_pos)
</code></pre>
<p>이러한 방식은 “오른쪽으로 3미터 이동 후 주방으로 가라“와 같은 순차적 명령이나, “의자 오른쪽 3미터 지점“과 같은 공간적 관계를 포함한 명령도 처리할 수 있게 한다. 실험 결과, VLMaps는 다중 목표 내비게이션(Multi-goal navigation) 작업에서 기존 방식인 CoW(CLIP on Wheels)나 LM-Nav 대비 최대 29% 높은 성공률을 보였다.</p>
<h3>6.4.4.3 다중 신체(Multi-Embodiment) 장애물 맵</h3>
<p>VLMaps는 단일 맵을 공유하면서도 로봇의 신체적 특성(Embodiment)에 따라 다른 내비게이션 경로를 생성할 수 있다. 예를 들어, 바퀴 달린 로봇(Rover)에게는 ’소파’나 ’테이블’이 장애물이지만, 드론(Drone)에게는 장애물이 아닐 수 있다. VLMaps에서는 쿼리만 바꾸면 즉석에서 장애물 맵(Obstacle Map)을 생성할 수 있다.</p>
<ul>
<li><strong>Rover:</strong> <code>obstacle_list = ['wall', 'chair', 'table', 'sofa']</code></li>
<li><strong>Drone:</strong> <code>obstacle_list = ['wall', 'column']</code></li>
</ul>
<p>각 리스트에 해당하는 객체들의 영역을 맵에서 마스킹(Masking)함으로써, 별도의 재학습 없이 다양한 로봇에 맞는 경로 계획이 가능하다.</p>
<h2>6.4.5 멀티모달 융합과 픽셀 정렬: ConceptFusion</h2>
<p><strong>ConceptFusion</strong> 은 VLMaps보다 더 포괄적이고 개방적인 3D 매핑을 지향한다. 텍스트뿐만 아니라 이미지, 오디오, 클릭(Click) 등 다양한 입력을 쿼리로 사용할 수 있는 <strong>멀티모달 개방형 집합(Open-set Multimodal) 3D 매핑</strong> 기술이다. 기존 SLAM 시스템(예: GradSLAM) 위에 픽셀 정렬(Pixel-aligned) 특징 융합 모듈을 얹은 구조를 가진다.</p>
<h3>6.4.5.1 제로샷 픽셀 정렬 (Zero-shot Pixel Alignment)</h3>
<p>CLIP과 같은 파운데이션 모델은 이미지 전체에 대한 글로벌 특징(Global feature, <span class="math math-inline">\mathbf{f}_G</span>)은 잘 제공하지만, 픽셀 단위의 로컬 특징(Local feature)은 해상도가 낮거나 제공하지 않는 경우가 많다. ConceptFusion은 이를 극복하기 위해 제로샷 픽셀 정렬 기술을 제안한다.</p>
<ol>
<li><strong>로컬 특징 추출:</strong> SAM과 같은 클래스 불가지론적(Class-agnostic) 마스크 생성기를 사용하여 이미지 내의 수많은 영역(Region)을 제안받고, 각 영역을 잘라내어(Crop) CLIP 인코더를 통과시켜 로컬 특징 <span class="math math-inline">\mathbf{f}_L</span>을 얻는다.</li>
<li><strong>픽셀 융합 (Pixel Fusion):</strong> 각 픽셀의 최종 특징 <span class="math math-inline">\mathbf{f}_P</span>는 글로벌 특징 <span class="math math-inline">\mathbf{f}_G</span>와 해당 픽셀이 속한 영역들의 로컬 특징 <span class="math math-inline">\mathbf{f}_L</span>의 가중 합으로 계산된다. 이때 가중치는 로컬 특징과 글로벌 특징 간의 유사도, 그리고 로컬 특징들 간의 유사도를 기반으로 결정된다.</li>
</ol>
<p><span class="math math-display">
\mathbf{f}_{P_i} = w_i \mathbf{f}_G + (1 - w_i) \mathbf{f}_{L_i}
</span></p>
<p>여기서 가중치 <span class="math math-inline">w_i</span>는 유사도 행렬을 통해 계산되며, 이를 통해 픽셀 단위의 정밀한 의미 정보를 생성한다.</p>
<h3>6.4.5.2 베이지안 맵 업데이트</h3>
<p>이렇게 생성된 2D 픽셀 정렬 특징은 3D 공간(포인트 클라우드)으로 투영된다. 3D 포인트 <span class="math math-inline">k</span>에 대한 특징 업데이트는 베이지안 방식을 따르거나 신뢰도 기반 이동 평균(Moving Average)을 사용한다.<br />
<span class="math math-display">
\mathbf{f}_{P_{k,t}} \leftarrow \frac{\bar{c}_k \mathbf{f}_{P_{k,t-1}} + \alpha \mathbf{f}_{P_{u,v,t}}}{\bar{c}_k + \alpha}
</span><br />
여기서 <span class="math math-inline">\bar{c}_k</span>는 포인트 <span class="math math-inline">k</span>에 대한 누적 신뢰도 카운트이며, <span class="math math-inline">\alpha</span>는 현재 관측의 가중치(예: 카메라와의 거리나 각도에 따른 신뢰도)이다. 이 과정을 통해 3D 맵은 시간이 지날수록 노이즈가 제거되고 시맨틱 정보가 강화된다.</p>
<p>ConceptFusion은 텍스트 쿼리뿐만 아니라, “이 소리(오디오 파일)가 나는 물체는?“과 같은 오디오 쿼리나, 사용자가 맵상의 특정 지점을 클릭했을 때 유사한 객체를 찾아주는 클릭 쿼리도 지원한다. 이는 멀티모달 임베딩 공간(ImageBind 등)을 활용한 덕분이다.</p>
<h2>6.4.6 3D 증류와 포인트 클라우드: OpenScene</h2>
<p><strong>OpenScene</strong> 은 2D 이미지 없이 3D 포인트 클라우드 입력만으로도 개방형 어휘 인식을 수행하는 것을 목표로 한다. 이를 위해 2D 파운데이션 모델의 지식을 3D 네트워크로 전이(Transfer)시키는 <strong>3D 증류(Distillation)</strong> 기법을 사용한다.</p>
<h3>6.4.6.1 교사-학생 (Teacher-Student) 학습 구조</h3>
<p>OpenScene의 학습 과정은 다음과 같다.</p>
<ol>
<li><strong>교사 (Teacher, 2D):</strong> OpenSeg나 LSeg와 같은 사전 학습된 2D 세그멘테이션 모델을 사용하여, 3D 씬의 다중 시점 이미지로부터 픽셀별 특징을 추출하고 이를 3D 포인트에 투영하여 융합한다. 이렇게 생성된 ’융합된 2D 특징(<span class="math math-inline">F_{2D}</span>)’은 노이즈가 섞여 있을 수 있지만 풍부한 시맨틱 정보를 담고 있다.</li>
<li><strong>학생 (Student, 3D):</strong> MinkowskiEngine 과 같은 3D Sparse Convolution 네트워크를 사용하여 3D 포인트 클라우드(<span class="math math-inline">P</span>)만을 입력으로 받아 3D 특징(<span class="math math-inline">F_{3D}</span>)을 예측하도록 학습한다.</li>
<li><strong>증류 손실 (Distillation Loss):</strong> 학생 네트워크가 예측한 <span class="math math-inline">F_{3D}</span>가 교사 네트워크가 만든 <span class="math math-inline">F_{2D}</span>와 유사해지도록 손실 함수를 설계한다. 주로 코사인 유사도 손실을 사용한다.</li>
</ol>
<p><span class="math math-display">
\mathcal{L} = 1 - \cos(F_{2D}, F_{3D})
</span></p>
<h3>6.4.6.2 3D 전용 추론 및 성능</h3>
<p>학습이 완료되면, OpenScene은 RGB 이미지 없이 3D 포인트 클라우드(좌표 및 색상)만 입력받아도 각 포인트의 시맨틱 임베딩을 출력할 수 있다. 이 임베딩은 CLIP 텍스트 인코더와 동일한 공간에 존재하므로, 임의의 텍스트 쿼리와 비교하여 제로샷 세그멘테이션이 가능하다.</p>
<p>OpenScene은 ScanNet, Matterport3D 등의 벤치마크에서 기존의 지도 학습(Supervised Learning) 방법론들을 능가하거나 대등한 성능을 보였으며, 특히 학습 데이터에 없는 희귀 클래스(Rare classes) 인식에서 압도적인 성능을 보였다. 이는 자율주행 차량이 LiDAR 센서만으로 주행할 때, 이전에 본 적 없는 장애물(예: 전동 킥보드, 특이한 옷차림의 보행자)을 인식하는 데 매우 유용하다.</p>
<h2>6.4.7 구조적 이해: ConceptGraphs 및 씬 그래프</h2>
<p>지금까지 논의한 기술들이 공간의 각 지점(Point, Voxel, Gaussian)에 의미를 부여하는 것이라면, <strong>ConceptGraphs</strong> 는 여기서 한 걸음 더 나아가 공간을 **객체(Object)**와 **관계(Relation)**로 구조화된 **3D 씬 그래프(Scene Graph)**로 추상화한다.</p>
<h3>6.4.7.1 노드 및 엣지 생성</h3>
<p>ConceptGraphs는 RGB-D 시퀀스를 입력으로 받아 다음과 같은 과정을 거친다.</p>
<ol>
<li><strong>객체 제안 및 융합:</strong> 2D 이미지에서 SAM 등을 이용해 객체 마스크를 추출하고, 이를 3D 공간에 투영하여 3D 객체 인스턴스를 생성한다. 서로 겹치는 인스턴스들은 병합(Merge)하여 하나의 노드(Node)로 만든다.</li>
<li><strong>노드 속성 부여:</strong> 각 노드(객체)에 대해 CLIP 임베딩을 추출하고, VLM(예: LLaVA)을 사용하여 텍스트 캡션(Caption)을 생성한다. 예를 들어, 특정 노드에는 “빨간색 세라믹 머그컵“이라는 캡션과 해당 CLIP 벡터가 저장된다.</li>
<li><strong>엣지 생성 (관계 추론):</strong> 객체 간의 공간적 인접성(Proximity)을 분석하여 “A는 B 위에 있다(on)”, “A는 B 옆에 있다(near)“와 같은 관계 엣지(Edge)를 생성한다.</li>
</ol>
<h3>6.4.7.2 LLM을 이용한 계획 수립</h3>
<p>이렇게 구축된 3D 씬 그래프는 텍스트(JSON 또는 XML) 형태로 변환되어 LLM에 입력될 수 있다. 비정형의 포인트 클라우드 데이터가 LLM이 이해할 수 있는 정형 데이터로 변환된 것이다. 로봇에게 “내 안경을 찾아줘“라고 명령하면, LLM은 씬 그래프를 탐색하여 “안경” 노드를 찾고, 만약 없다면 “책상“이나 “침대 협탁“과 같이 안경이 있을 법한 관계를 가진 노드 주변을 탐색하도록 계획을 수립할 수 있다. ConceptGraphs는 이러한 구조화를 통해 단순 검색을 넘어선 추론(Reasoning) 기반의 공간 지능을 구현한다.</p>
<h2>6.4.8 성능 비교 및 아키텍처 분석</h2>
<p>앞서 살펴본 주요 공간 지능 기술들을 속도, 표현 방식, 주요 응용 분야 측면에서 비교하면 다음 표와 같다.</p>
<table><thead><tr><th><strong>기술 명</strong></th><th><strong>기반 표현 (Representation)</strong></th><th><strong>시맨틱 소스 (Semantic Source)</strong></th><th><strong>추론 속도 (Inference Speed)</strong></th><th><strong>공간 정밀도 (Spatial Precision)</strong></th><th><strong>주요 응용 (Applications)</strong></th><th><strong>메모리 효율</strong></th></tr></thead><tbody>
<tr><td><strong>LERF</strong></td><td>NeRF (Implicit)</td><td>CLIP (Multi-scale)</td><td>느림 (수 초/뷰)</td><td>낮음 (경계 모호)</td><td>3D 시맨틱 검색, 인터랙티브 뷰어</td><td>보통</td></tr>
<tr><td><strong>LangSplat</strong></td><td>3D Gaussian (Explicit)</td><td>CLIP + SAM</td><td><strong>매우 빠름 (실시간)</strong></td><td><strong>높음 (SAM 기반 경계)</strong></td><td>실시간 3D 검색, AR/VR, 로봇 내비게이션</td><td><strong>높음 (오토인코더 압축)</strong></td></tr>
<tr><td><strong>VLMaps</strong></td><td>Grid Map (2.5D)</td><td>LSeg / OpenSeg</td><td>빠름 (경로 계획 시)</td><td>중간 (그리드 해상도 의존)</td><td>장거리 내비게이션, 장애물 회피</td><td>높음</td></tr>
<tr><td><strong>ConceptFusion</strong></td><td>Point Cloud / Map</td><td>CLIP + SAM + Audio</td><td>빠름 (벡터 검색)</td><td>높음 (Pixel-aligned)</td><td>멀티모달 검색, 로봇 조작, 오디오 소스 추적</td><td>낮음 (포인트별 저장)</td></tr>
<tr><td><strong>OpenScene</strong></td><td>Sparse Voxel / Point</td><td>OpenSeg / LSeg</td><td>빠름 (3D Conv)</td><td>높음 (복셀 해상도 의존)</td><td>자율주행, 3D 세그멘테이션 (LiDAR)</td><td>보통</td></tr>
<tr><td><strong>ConceptGraphs</strong></td><td>Scene Graph (Object)</td><td>CLIP + VLM (Caption)</td><td>쿼리 속도 최상</td><td>객체 단위</td><td>고차원 추론, LLM 연동 계획 수립</td><td>매우 높음 (그래프 추상화)</td></tr>
</tbody></table>
<h3>6.4.8.1 암시적 표현 대 명시적 표현의 트레이드오프</h3>
<p>LERF와 같은 암시적 표현은 공간을 연속 함수로 모델링하므로 줌인(Zoom-in) 시에도 해상도가 깨지지 않고 부드러운 표현이 가능하다는 장점이 있다. 그러나 빈 공간(Empty space)에 대해서도 연산을 수행해야 하므로 비효율적이다. 반면, LangSplat이나 ConceptFusion 같은 명시적 표현은 실제로 물체가 존재하는 표면이나 가우시안만을 저장하므로 연산 효율이 뛰어나고 국소적인 업데이트가 용이하다.</p>
<p>로봇 공학, 특히 동적인 환경에서 실시간 상호작용이 필요한 분야에서는 LangSplat과 같은 명시적 표현이나 VLMaps와 같은 지도 기반 접근이 선호되는 추세이다. 반면, 고품질의 시각화나 오프라인 분석이 필요한 경우에는 LERF나 그 파생 기술들이 여전히 유효하다.</p>
<h2>6.4.9 로봇 응용 및 사례 연구</h2>
<p>이러한 공간 지능 기술들은 실제 로봇 시스템에서 혁신적인 변화를 일으키고 있다.</p>
<ol>
<li><strong>개방형 어휘 내비게이션 (Open-Vocabulary Navigation):</strong> VLMaps와 LangSplat을 탑재한 로봇은 “장난감이 어질러진 곳으로 가라“거나 “가장 편안해 보이는 의자를 찾아라“와 같은 주관적이고 추상적인 명령을 수행할 수 있다. 이는 기존의 기하학적 지도에서는 불가능했던 일이다.</li>
<li><strong>의미론적 파지 (Semantic Grasping):</strong> LERF-TOGO는 로봇 팔이 물체의 특정 부위를 인식하고 잡는 데 사용된다. “표백제 병의 손잡이를 잡아라(손에 묻지 않게)“와 같은 명령을 수행할 때, 3D 시맨틱 필드는 손잡이 부분의 정확한 3D 좌표와 방향을 제공한다.</li>
<li><strong>인간-로봇 상호작용 (HRI):</strong> ConceptFusion을 적용한 AR 글래스나 로봇은 사용자가 손가락으로 가리키거나(Click query), 소리(Audio query)를 냄으로써 물체를 지정하고 정보를 얻을 수 있게 해준다.</li>
</ol>
<h2>6.4.10 결론: 의미의 숲을 탐색하는 로봇</h2>
<p>6.4절에서는 2D 시맨틱을 3D 공간으로 확장하여 로봇에게 진정한 의미의 ’공간 지능’을 부여하는 기술들을 살펴보았다. CLIP과 SAM, 그리고 생성형 파운데이션 모델들은 로봇에게 “무엇(What)“을 알려주었고, NeRF, 3DGS, SLAM 기술은 “어디에(Where)“를 알려주었다. 이 둘의 결합은 로봇이 단순히 장애물을 피하는 기계에서, 환경의 맥락을 이해하고 인간의 언어로 소통하며 복잡한 임무를 수행하는 지능형 에이전트로 진화하는 변곡점을 만들었다.</p>
<p>특히 LangSplat과 같은 기술은 이러한 고차원 추론을 실시간으로 가능하게 함으로써 상용화의 가능성을 높였고, ConceptGraphs는 로봇의 지식을 인간과 유사한 구조적 형태로 격상시켰다. 이제 남은 과제는 정적인(Static) 공간에 대한 이해를 넘어, 시간이 흐름에 따라 변화하는 동적 환경(Dynamic Environment)에서도 이러한 시맨틱 일관성을 유지하는 것이다. 이어지는 6.5절에서는 시간적 일관성(Temporal Consistency)을 다루며, 움직이는 물체와 변화하는 환경 속에서의 공간 지능을 논의한다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>LERF-TOGO, 1월 25, 2026에 액세스, https://lerftogo.github.io/</li>
<li>LERF: Language Embedded Radiance Fields - CVF Open Access, 1월 25, 2026에 액세스, https://openaccess.thecvf.com/content/ICCV2023/papers/Kerr_LERF_Language_Embedded_Radiance_Fields_ICCV_2023_paper.pdf</li>
<li>LERF: Language Embedded Radiance Fields, 1월 25, 2026에 액세스, https://www.lerf.io/</li>
<li>Language Embedded Radiance Fields for Zero-Shot Task-Oriented …, 1월 25, 2026에 액세스, https://openreview.net/forum?id=k-Fg8JDQmc</li>
<li>LangSplat: 3D Language Gaussian Splatting - alphaXiv, 1월 25, 2026에 액세스, https://www.alphaxiv.org/overview/2312.16084v2</li>
<li>LangSplat: Turbocharging 3D Language Fields with a Mind-Blowing …, 1월 25, 2026에 액세스, https://syncedreview.com/2024/01/07/langsplat-turbocharging-3d-language-fields-with-a-mind-blowing-199x-speed-boost/</li>
<li>Semantically Consistent Language Gaussian Splatting for 3D Point …, 1월 25, 2026에 액세스, https://arxiv.org/html/2503.21767v2</li>
<li>LangSplatV2: High-dimensional 3D Language Gaussian Splatting …, 1월 25, 2026에 액세스, https://openreview.net/pdf/52fe7ec3599e16e352226d3a31a8369636a3fc7e.pdf</li>
<li>LangSplat: 3D Language Gaussian Splatting - CVF Open Access, 1월 25, 2026에 액세스, https://openaccess.thecvf.com/content/CVPR2024/papers/Qin_LangSplat_3D_Language_Gaussian_Splatting_CVPR_2024_paper.pdf</li>
<li>[Quick Review] LangSplat: 3D Language Gaussian Splatting - Liner, 1월 25, 2026에 액세스, https://liner.com/review/langsplat-3d-language-gaussian-splatting</li>
<li>Vision and Language Navigation in the Real World via Online Visual …, 1월 25, 2026에 액세스, https://www.ccs.neu.edu/home/lsw/papers/corl2023ws-vln.pdf</li>
<li>Visual Language Maps for Robot Navigation, 1월 25, 2026에 액세스, https://vlmaps.github.io/</li>
<li>Visual Language Maps for Robot Navigation - SciSpace, 1월 25, 2026에 액세스, https://scispace.com/pdf/visual-language-maps-for-robot-navigation-byf1lxfx.pdf</li>
<li>Visual language maps for robot navigation - Google Research, 1월 25, 2026에 액세스, https://research.google/blog/visual-language-maps-for-robot-navigation/</li>
<li>ConceptFusion: Open-set Multimodal 3D Mapping - OpenReview, 1월 25, 2026에 액세스, https://openreview.net/pdf/3df88889157197f157155b5bc6074e8c60fb9b8b.pdf</li>
<li>ConceptFusion: Open-set Multimodal 3D Mapping - Semantic Scholar, 1월 25, 2026에 액세스, https://www.semanticscholar.org/paper/ConceptFusion%3A-Open-set-Multimodal-3D-Mapping-Jatavallabhula-Kuwajerwala/5e2bceb56f116e98baf7e418208057bc0e1c1861</li>
<li>ConceptFusion: Open-set Multimodal 3D Mapping - Robotics, 1월 25, 2026에 액세스, https://www.roboticsproceedings.org/rss19/p066.pdf</li>
<li>ConceptFusion: Open-set Multimodal 3D Mapping - ResearchGate, 1월 25, 2026에 액세스, https://www.researchgate.net/publication/368507767_ConceptFusion_Open-set_Multimodal_3D_Mapping</li>
<li>OpenScene: 3D Scene Understanding With Open Vocabularies, 1월 25, 2026에 액세스, https://openaccess.thecvf.com/content/CVPR2023/papers/Peng_OpenScene_3D_Scene_Understanding_With_Open_Vocabularies_CVPR_2023_paper.pdf</li>
<li>Open Vocabulary 3D Scene Understanding via Geometry Guided …, 1월 25, 2026에 액세스, https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02396.pdf</li>
<li>[PDF] OpenScene: 3D Scene Understanding with Open Vocabularies, 1월 25, 2026에 액세스, https://www.semanticscholar.org/paper/OpenScene%3A-3D-Scene-Understanding-with-Open-Peng-Genova/774408d8848b129d93fb67548ec6571d99b31a2d</li>
<li>OpenScene: 3D Scene Understanding with Open Vocabularies, 1월 25, 2026에 액세스, https://pengsongyou.github.io/media/openscene/OpenScene.pdf</li>
<li>NVIDIA/MinkowskiEngine: Minkowski Engine is an auto-diff … - GitHub, 1월 25, 2026에 액세스, https://github.com/NVIDIA/MinkowskiEngine</li>
<li>4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural …, 1월 25, 2026에 액세스, https://openaccess.thecvf.com/content_CVPR_2019/papers/Choy_4D_Spatio-Temporal_ConvNets_Minkowski_Convolutional_Neural_Networks_CVPR_2019_paper.pdf</li>
<li>Open-Vocabulary 3D Scene Graphs for Perception and Planning, 1월 25, 2026에 액세스, https://concept-graphs.github.io/assets/pdf/2023-ConceptGraphs.pdf</li>
<li>Open-Vocabulary 3D Scene Graphs for Perception and Planning, 1월 25, 2026에 액세스, https://www.researchgate.net/publication/382992541_ConceptGraphs_Open-Vocabulary_3D_Scene_Graphs_for_Perception_and_Planning</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>