<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:6.4.3 3D 장면 그래프 (3D Scene Graphs): 객체 간의 관계(Relation)와 속성(Attribute)을 포함한 계층적 공간 이해.</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>6.4.3 3D 장면 그래프 (3D Scene Graphs): 객체 간의 관계(Relation)와 속성(Attribute)을 포함한 계층적 공간 이해.</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 6. 오픈 보캐블러리와 시맨틱 이해 (Open-Vocabulary & Semantic Understanding)</a> / <a href="index.html">6.4 2D 시맨틱을 3D 공간으로: 공간 지능의 확장 (Spatial AI)</a> / <span>6.4.3 3D 장면 그래프 (3D Scene Graphs): 객체 간의 관계(Relation)와 속성(Attribute)을 포함한 계층적 공간 이해.</span></nav>
                </div>
            </header>
            <article>
                <h1>6.4.3 3D 장면 그래프 (3D Scene Graphs): 객체 간의 관계(Relation)와 속성(Attribute)을 포함한 계층적 공간 이해.</h1>
<h2>1.  서론: 로봇 인지(Perception)의 패러다임 전환과 고수준 공간 표현의 필요성</h2>
<p>현대 로봇 공학의 궁극적인 지향점은 인간과 공존하며 복잡한 명령을 수행하고, 예측 불가능한 환경에서도 자율적으로 판단하여 행동하는 지능형 에이전트(Intelligent Agent)를 구현하는 것이다. 이를 위해서는 로봇이 주변 환경을 단순히 물리적인 장애물의 집합으로 인식하는 것을 넘어, 인간이 세상을 이해하는 방식과 유사한 수준의 의미론적(Semantic)이고 관계적(Relational)인 이해가 필수적으로 요구된다. 과거의 로봇 인지 시스템, 특히 SLAM(Simultaneous Localization and Mapping) 기술은 주로 **점유 그리드 지도(Occupancy Grid Map)**나 <strong>밀집 점군(Dense Point Cloud)</strong>, **TSDF(Truncated Signed Distance Field)**와 같은 메트릭(Metric) 표현에 집중해 왔다. 이러한 저수준(Low-level) 표현 방식은 로봇의 자기 위치 추정(Localization)과 충돌 회피(Collision Avoidance)와 같은 기하학적 작업에는 효과적이지만, “부엌에서 냉장고 문을 열고 사과를 가져오라“와 같은 고수준의 태스크를 계획하고 수행하기에는 본질적인 한계를 지닌다. 메트릭 지도는 ’냉장고’가 무엇인지, ’부엌’이라는 공간이 어디서부터 어디까지인지, 그리고 ’문’이 ’열린다’는 상호작용 가능성(Affordance)이 무엇인지에 대한 정보를 명시적으로 포함하지 않기 때문이다.</p>
<p>이러한 배경에서 등장한 **3D 장면 그래프(3D Scene Graphs)**는 로봇의 공간 인지 능력을 획기적으로 격상시키는 새로운 패러다임으로 자리 잡았다. 컴퓨터 그래픽스와 컴퓨터 비전 분야에서 이미지 내 객체들의 관계를 그래프로 표현하던 개념을 3차원 물리 공간으로 확장한 3D 장면 그래프는, 공간을 구성하는 <strong>객체(Objects)</strong>, <strong>구조(Structures)</strong>, <strong>장소(Places)</strong>, <strong>방(Rooms)</strong>, <strong>건물(Buildings)</strong> 등의 엔티티(Entity)를 **노드(Node)**로 정의하고, 이들 사이의 공간적, 논리적, 시간적 관계를 **엣지(Edge)**로 연결하여 표현하는 계층적 데이터 구조이다. 이는 로봇에게 환경에 대한 압축적이고 구조화된 “정신 모델(Mental Model)“을 제공하며, 메트릭 정보의 정확성과 시맨틱 정보의 추상성을 동시에 만족시키는 통합된 표현체계(Unified Representation)로 기능한다.</p>
<p>본 장에서는 로봇을 구성하는 최신 SOTA(State-of-the-Art) 인공지능 기술의 관점에서 3D 장면 그래프의 이론적 배경과 구조, 그리고 이를 실시간으로 구축하고 최적화하는 핵심 알고리즘을 심도 있게 분석한다. 특히 <strong>Kimera</strong>, <strong>Hydra</strong>, <strong>S-Graphs</strong>, <strong>Khronos</strong>, <strong>ConceptGraphs</strong> 등 최근 로봇 학계와 산업계에서 주목받는 주요 프레임워크들의 기술적 원리를 상세히 규명하고, 거대 언어 모델(LLM) 및 비전-언어 모델(VLM)과 결합된 오픈 보캐블러리(Open-Vocabulary) 인식 기술이 어떻게 로봇의 장기 태스크 플래닝(Long-Horizon Task Planning)과 시각적 질의응답(VQA) 능력을 혁신하고 있는지 논한다.</p>
<h2>2.  3D 장면 그래프의 이론적 정의와 계층적 아키텍처</h2>
<p>3D 장면 그래프는 수학적으로 비순환 유향 그래프(Directed Acyclic Graph, DAG) 또는 무향 그래프(Undirected Graph)의 형태를 띠며, <span class="math math-inline">G = (V, E)</span>로 정의된다. 여기서 <span class="math math-inline">V</span>는 환경 내의 다양한 요소를 나타내는 노드의 집합이며, <span class="math math-inline">E</span>는 노드 간의 관계를 나타내는 엣지의 집합이다. 단순한 평면적 그래프 구조와 달리, 3D 장면 그래프는 공간의 물리적 포함 관계와 기능적 위계를 반영하여 다층적인 계층 구조(Hierarchy)를 형성한다는 점이 핵심적인 특징이다.</p>
<h3>2.1  계층적 노드 구조 (Hierarchical Node Structure)</h3>
<p>Armeni 등의 연구 <strong>3D Scene Graph: A Structure for Unified Semantics, 3D Space</strong>에서 제안된 구조를 기반으로, 최신 연구인 <strong>Hydra</strong>와 <strong>HOV-SG</strong> 등은 이를 더욱 세분화하여 일반적으로 5단계 이상의 계층을 정의한다. 각 계층은 서로 다른 추상화 레벨을 가지며, 하위 계층은 상위 계층의 구체적인 근거(Grounding)를 제공하고, 상위 계층은 하위 계층의 정보를 요약(Abstraction)하여 제공한다.</p>
<ol>
<li><strong>Layer 1: 메트릭-시맨틱 메쉬 (Metric-Semantic Mesh)</strong></li>
</ol>
<ul>
<li>가장 하위 레벨에 위치하며, 환경의 기하학적 형상을 나타내는 3D 메쉬(Mesh)의 버텍스(Vertex)와 페이스(Face)들로 구성된다. 각 요소는 시맨틱 라벨(예: 바닥, 벽, 의자)과 연동되어 있다. 이는 SLAM 시스템이 생성한 원본 데이터에 가장 가까운 형태이며, 로봇의 물리적 충돌 계산과 정밀한 조작(Manipulation)을 위한 기초가 된다.</li>
</ul>
<ol start="2">
<li><strong>Layer 2: 객체 (Objects) 및 에이전트 (Agents)</strong></li>
</ol>
<ul>
<li>메쉬의 부분 집합을 의미 단위로 클러스터링하여 생성된 ‘물체’ 단위의 노드이다. 정적 객체뿐만 아니라 환경 내에서 움직이는 사람이나 다른 로봇과 같은 에이전트도 이 계층에 포함된다.</li>
<li><strong>속성(Attribute):</strong> 각 객체 노드는 단순한 클래스 이름뿐만 아니라, 3D 바운딩 박스(Bounding Box), 6자유도 자세(6DoF Pose), 색상, 재질, 형상 정보(Shape Mesh) 등을 속성으로 갖는다. 최근에는 <strong>ConceptGraphs</strong>와 같은 연구를 통해 <strong>CLIP</strong> 임베딩 벡터와 같은 고차원 시각-언어 특징이 속성으로 추가되어, 자연어 기반의 검색을 가능하게 한다.</li>
</ul>
<ol start="3">
<li><strong>Layer 3: 장소 (Places) 및 구조 (Structures)</strong></li>
</ol>
<ul>
<li><strong>장소(Places):</strong> 로봇이 점유하거나 이동할 수 있는 빈 공간(Free Space)을 위상학적으로 표현한 노드이다. 이는 주로 **Generalized Voronoi Graph (GVG)**나 위상 그래프(Topological Graph)의 노드로 표현되며, 로봇의 경로 계획(Path Planning)을 위한 내비게이션 그래프(Navigational Graph)의 역할을 수행한다.</li>
<li><strong>구조(Structures):</strong> 벽(Wall), 바닥(Floor), 천장(Ceiling), 기둥(Pillar) 등 공간의 골격을 형성하는 요소들이 이 계층에 포함된다. 이는 <strong>S-Graphs</strong>와 같이 구조적 제약 조건을 활용하여 SLAM의 정확도를 높이는 데 핵심적인 역할을 한다.</li>
</ul>
<ol start="4">
<li><strong>Layer 4: 방 (Rooms) / 지역 (Regions)</strong></li>
</ol>
<ul>
<li>기능적으로 구분되는 공간의 단위이다. 장소 노드들과 객체 노드들을 포함하며, ‘부엌’, ‘사무실’, ’복도’와 같은 시맨틱 라벨이 부여된다. 방 노드는 로봇에게 공간의 컨텍스트(Context)를 제공하여, “부엌에서는 요리 도구를 찾을 확률이 높다“와 같은 베이지안 추론을 가능하게 한다.</li>
</ul>
<ol start="5">
<li><strong>Layer 5: 층 (Floors) 및 건물 (Buildings)</strong></li>
</ol>
<ul>
<li>다층 건물 환경에서 여러 방을 포함하는 층 노드와, 전체 환경을 아우르는 건물 노드가 최상위에 위치한다. 이 계층은 대규모 환경에서의 탐색 효율성을 높이고, 층간 이동(계단, 엘리베이터 등)을 추상화하여 관리하는 데 사용된다.</li>
</ul>
<h3>2.2  엣지와 관계의 다차원성 (Relational Edges)</h3>
<p>엣지는 노드 간의 관계를 정의하며, 3D 장면 그래프가 단순한 객체 목록(List)을 넘어선 구조적 지식을 표현하게 한다. 엣지는 크게 세 가지 범주로 분류할 수 있다.</p>
<ul>
<li><strong>포함 관계 (Inclusion / Hierarchical Edges):</strong> 상위 계층 노드가 하위 계층 노드를 포함함을 나타낸다 (예: 건물 <span class="math math-inline">\rightarrow</span> 층 <span class="math math-inline">\rightarrow</span> 방 <span class="math math-inline">\rightarrow</span> 객체). 이는 그래프 탐색 범위를 좁히는 쿼리 최적화(Pruning)에 매우 유용하다.</li>
<li><strong>공간적 관계 (Spatial Edges):</strong></li>
<li><strong>근접성(Proximity):</strong> 두 노드 간의 거리가 특정 임계값 이내임을 나타낸다.</li>
<li><strong>위상적 관계(Topological Relation):</strong> <em>on</em>, <em>under</em>, <em>inside</em>, <em>next to</em> 등 객체 간의 상대적 위치를 나타낸다. 예를 들어, “책상은 바닥 <em>위에</em> 있고(on), 모니터는 책상 <em>위에</em> 있다“와 같은 정보는 로봇이 물체를 조작할 때 지지 관계(Support Relation)를 이해하는 데 필수적이다.</li>
<li><strong>가시성(Visibility):</strong> 특정 장소에서 특정 객체가 보이는지를 나타낸다. 이는 능동적 탐색(Active Search) 시 로봇이 어디로 이동해야 물체를 찾을 수 있을지 결정하는 데 사용된다.</li>
<li><strong>시간적 관계 (Temporal Edges):</strong> 동적 환경에서 동일한 객체의 시간 <span class="math math-inline">t</span>와 <span class="math math-inline">t+1</span> 사이의 상태 변화를 연결한다. 이는 <strong>Khronos</strong>와 같은 4D 시공간 매핑 시스템에서 객체의 궤적(Trajectory)을 추적하고, 환경의 장기적인 변화(Long-term Change)를 모델링하는 데 사용된다.</li>
</ul>
<p>아래의 테이블은 3D 장면 그래프를 구성하는 주요 노드 유형과 그들이 가지는 속성 및 관계를 요약한 것이다.</p>
<h3>2.3 표 6.4.3-1 3D 장면 그래프의 계층별 노드 구성 요소 및 관계 정의</h3>
<table><thead><tr><th style="text-align: left">계층 레벨 (Layer)</th><th style="text-align: left">노드 유형 (Node Type)</th><th style="text-align: left">주요 속성 (Attributes)</th><th style="text-align: left">주요 엣지 및 관계 (Edges &amp; Relations)</th></tr></thead><tbody>
<tr><td style="text-align: left"><strong>L5: 건물 (Building)</strong></td><td style="text-align: left">루트 노드, 건물 전체</td><td style="text-align: left">건물 이름, GPS 좌표, 전체 좌표계 원점</td><td style="text-align: left">층 포함 (Has_Floor)</td></tr>
<tr><td style="text-align: left"><strong>L4: 층 (Floor)</strong></td><td style="text-align: left">층 단위 공간</td><td style="text-align: left">층 번호, 높이(Z-level), 평면도(Footprint)</td><td style="text-align: left">방 포함 (Has_Room), 층간 연결 (Stairs/Elevator)</td></tr>
<tr><td style="text-align: left"><strong>L3: 방 (Room)</strong></td><td style="text-align: left">기능적 공간 단위</td><td style="text-align: left">방 유형(Type), 중심 좌표, 다각형 영역(Polygon)</td><td style="text-align: left">인접(Adjacent), 장소/객체 포함 (Contains)</td></tr>
<tr><td style="text-align: left"><strong>L3: 장소 (Place)</strong></td><td style="text-align: left">위상학적 자유 공간</td><td style="text-align: left">위치(Position), 점유 가능성(Traversability)</td><td style="text-align: left">이동 가능(Connected_to), 가시성(Visible_from)</td></tr>
<tr><td style="text-align: left"><strong>L2: 객체 (Object)</strong></td><td style="text-align: left">개별 물체 및 에이전트</td><td style="text-align: left">클래스, 6DoF 자세, 바운딩 박스, <strong>CLIP Feature</strong></td><td style="text-align: left">지지(Supported_by), 근접(Near), 상대 위치(On/In)</td></tr>
<tr><td style="text-align: left"><strong>L1: 메트릭 (Metric)</strong></td><td style="text-align: left">메쉬, 클러스터</td><td style="text-align: left">기하 정보(Vertices/Faces), 시맨틱 라벨</td><td style="text-align: left">소속(Belongs_to_Object)</td></tr>
</tbody></table>
<h2>3.  메트릭-시맨틱 SLAM과 장면 그래프의 기반 구축: Kimera</h2>
<p>3D 장면 그래프를 구축하기 위해서는 먼저 로봇이 자신의 위치를 정확히 파악하고 환경의 3차원 기하 정보를 복원하는 SLAM 과정이 선행되어야 한다. MIT SPARK Lab에서 개발한 <strong>Kimera</strong>는 이러한 메트릭-시맨틱 SLAM의 표준을 제시하며, 3D 장면 그래프 생성을 위한 견고한 토대를 마련하였다. Kimera는 실시간으로 센서 데이터를 처리하여 메트릭 지도와 시맨틱 정보를 결합하고, 이를 상위 레벨의 장면 그래프 모듈로 전달하는 파이프라인의 핵심 역할을 수행한다.</p>
<h3>3.1  Kimera의 모듈식 아키텍처와 데이터 흐름</h3>
<p><strong>Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping</strong> 논문에 따르면, Kimera는 모듈화된 구조를 통해 유연성과 확장성을 확보하였다. 각 모듈은 다음과 같은 기능을 수행한다.</p>
<ol>
<li><strong>Kimera-VIO (Visual-Inertial Odometry):</strong></li>
</ol>
<ul>
<li>카메라의 영상 정보와 IMU(관성 측정 장치)의 가속도 및 각속도 데이터를 융합하여 로봇의 6자유도 자세(Pose)와 속도를 고속으로 추정한다. 이는 그래프의 시간적 일관성을 유지하고 모션 블러(Motion Blur)와 같은 급격한 움직임에서도 강건함을 유지하는 데 필수적이다.</li>
</ul>
<ol start="2">
<li><strong>Kimera-RPGO (Robust Pose Graph Optimization):</strong></li>
</ol>
<ul>
<li>장시간 주행 시 누적되는 위치 오차(Drift)를 보정하기 위해 루프 클로저(Loop Closure)를 감지하고 포즈 그래프를 최적화한다. 특히, 인지적 에일리어싱(Perceptual Aliasing)으로 인해 잘못된 루프 클로저가 발생할 경우 지도가 붕괴되는 것을 방지하기 위해 **PCM (Pairwise Consistency Measurement)**과 같은 아웃라이어 제거 알고리즘을 적용하여 시스템의 신뢰성을 보장한다.</li>
</ul>
<ol start="3">
<li><strong>Kimera-Mesher:</strong></li>
</ol>
<ul>
<li>VIO에서 추출한 희소 특징점(Sparse Feature Points)들을 이용하여 실시간으로 3D 메쉬를 생성한다. 이 단계에서는 빠르고 경량화된 메쉬 생성을 목표로 하며, 로봇의 즉각적인 장애물 회피 등에 사용된다.</li>
</ul>
<ol start="4">
<li><strong>Kimera-Semantics:</strong></li>
</ol>
<ul>
<li>입력된 2D 이미지에 대해 딥러닝 기반의 시맨틱 세그멘테이션(Semantic Segmentation)을 수행하고, 이를 <strong>번들 레이캐스팅(Bundled Raycasting)</strong> 기법을 통해 3D 메쉬에 확률적으로 투영한다. 이를 통해 각 메쉬의 표면(Face)은 ‘의자’, ‘책상’, ’바닥’과 같은 시맨틱 클래스 확률 분포를 갖게 되며, 이는 <strong>Metric-Semantic Mesh</strong>라는 3D 장면 그래프의 최하위 레이어(L1)를 형성한다.</li>
</ul>
<p>Kimera가 생성한 메트릭-시맨틱 메쉬는 단순히 시각적인 정보를 넘어서, 상위 레벨의 ’객체’와 ’방’을 추출하기 위한 원천 데이터(Raw Material)가 된다.</p>
<h2>4.  실시간 동적 장면 그래프 구성 및 최적화: Hydra 시스템</h2>
<p>초기의 3D 장면 그래프 연구들은 이미 스캔이 완료된 정적인 데이터셋(예: Matterport3D)을 오프라인에서 처리하는 데 그쳤다. 그러나 로봇이 실제 현장에서 작동하기 위해서는 미지의 환경을 탐색함과 동시에 실시간으로 그래프를 생성하고 업데이트할 수 있어야 한다. <strong>Hydra: A Real-time Spatial Perception System for 3D Scene Graph Construction and Optimization</strong> 연구는 이러한 문제를 해결한 최초의 실시간 공간 인식 시스템으로, 대규모 3D 데이터를 효율적으로 처리하기 위한 병렬 아키텍처를 제안하였다.</p>
<h3>4.1  Hydra의 프론트엔드: 점진적 그래프 생성 (Incremental Construction)</h3>
<p>Hydra는 데이터 처리의 효율성을 위해 <strong>활성 윈도우(Active Window)</strong> 개념을 도입한다. 로봇 주변의 일정 반경 내 영역만을 활성화하여 고해상도 처리를 수행하고, 윈도우를 벗어난 영역은 비활성화하거나 압축하여 저장한다. 프론트엔드는 센서 데이터 스트림으로부터 그래프의 노드와 엣지를 점진적으로 생성한다.</p>
<ol>
<li><strong>저수준 인지 (Low-level Perception) 및 장소 추출:</strong></li>
</ol>
<ul>
<li>Hydra는 <strong>Voxblox</strong>를 기반으로 TSDF를 구축하고, 이를 통해 장애물과의 거리를 나타내는 **ESDF(Euclidean Signed Distance Field)**를 실시간으로 업데이트한다.</li>
<li>ESDF 정보를 바탕으로 공간의 뼈대(Skeleton)를 추출하여 **GVG(Generalized Voronoi Graph)**를 생성한다. 이 GVG의 노드들은 좁은 통로와 넓은 공간을 위상학적으로 연결하는 ‘장소(Place)’ 노드가 된다. 이는 로봇의 경로 계획을 위한 로드맵 역할을 수행한다.</li>
</ul>
<ol start="2">
<li><strong>중수준 인지 (Mid-level Perception) 및 객체 분할:</strong></li>
</ol>
<ul>
<li>활성 윈도우 내의 메트릭-시맨틱 메쉬 버텍스들에 대해 **유클리드 군집화(Euclidean Clustering)**를 수행하여 개별 객체 인스턴스를 식별한다.</li>
<li>각 군집은 시맨틱 클래스, 중심점(Centroid), 바운딩 박스로 추상화되어 ‘객체(Object)’ 노드로 등록된다. 동일한 시맨틱 라벨을 가진 인접한 버텍스들이 하나의 객체로 통합되는 과정을 거친다.</li>
</ul>
<h3>4.2  Hydra의 백엔드: 전역 일관성 및 방(Room) 탐지</h3>
<p>백엔드는 로컬에서 생성된 정보를 전역 그래프(Global Graph)에 통합하고, 루프 클로저 발생 시 그래프 전체를 최적화하며, 고수준의 추론을 수행한다.</p>
<ol>
<li><strong>방(Room) 탐지 알고리즘:</strong></li>
</ol>
<ul>
<li>Hydra는 기하학적 규칙(예: 사각형 방)에 의존하지 않고, 장소(Place) 노드들의 연결성을 기반으로 방을 식별한다. 이를 위해 네트워크 이론의 <strong>커뮤니티 감지(Community Detection)</strong> 알고리즘인 <strong>Louvain 알고리즘</strong> 등을 위상 그래프에 적용한다.</li>
<li>연결성이 높고 밀집된 장소들의 집합은 하나의 ’방’으로, 이들을 연결하는 병목 구간은 ’문(Door)’이나 ’통로’로 식별된다. 이 방식은 복잡한 구조나 열린 공간(Open Space)에서도 강건하게 방을 구분할 수 있게 한다.</li>
</ul>
<ol start="2">
<li><strong>임베디드 변형 그래프(Embedded Deformation Graph)를 이용한 최적화:</strong></li>
</ol>
<ul>
<li>로봇이 이전에 방문한 장소를 재방문하여 루프 클로저가 발생하면, 포즈 그래프 최적화를 통해 로봇의 궤적이 수정된다. 이때 장면 그래프의 일관성을 유지하기 위해 Hydra는 <strong>임베디드 변형 그래프</strong> 기술을 사용한다.</li>
<li>변형 그래프는 공간 전체를 감싸는 제어 노드(Control Nodes)들로 구성되며, 포즈 그래프의 변화량에 따라 공간을 비선형적으로 변형시킨다. 이 변형은 하위의 메쉬 버텍스부터 상위의 객체, 장소, 방 노드까지 전파되어, 3D 장면 그래프 전체가 기하학적으로 올바른 위치로 정렬되도록 한다. 이는 계층적 자료구조 전체를 실시간으로 최적화하는 최초의 시도로 평가받는다.</li>
</ul>
<h2>5.  상황 인식 그래프와 팩터 그래프 최적화: S-Graphs (Situational Graphs)</h2>
<p>독일 뮌헨 공과대학교(TUM) 등의 연구진이 제안한 **S-Graphs (Situational Graphs)**는 3D 장면 그래프를 단순한 결과물이 아닌, SLAM 자체의 정확도를 높이기 위한 **최적화의 제약 조건(Constraint)**으로 활용한다는 점에서 독창적이다.</p>
<h3>5.1  S-Graphs+의 4계층 팩터 그래프 구조</h3>
<p>**S-Graphs+**는 환경의 구조적 특징인 벽(Wall), 방(Room), 층(Floor)을 로봇의 포즈 추정과 결합하여 하나의 거대한 팩터 그래프(Factor Graph)로 모델링한다.</p>
<ol>
<li><strong>키프레임 레이어 (Keyframes Layer):</strong> 로봇의 궤적을 나타내는 포즈 노드들로, 오도메트리 측정값과 IMU 데이터가 팩터로 연결된다.</li>
<li><strong>벽 레이어 (Walls Layer):</strong> LiDAR나 RGB-D 센서로부터 추출된 평면(Planar) 정보를 기반으로 생성된 벽면 노드들이다. 로봇이 관측한 벽면의 방정식(Plane Equation)이 키프레임 노드와 벽 노드 사이의 제약 조건으로 작용한다. “이 벽은 평평하며 움직이지 않는다“는 가정이 로봇의 위치 오차를 줄이는 강력한 앵커(Anchor) 역할을 한다.</li>
<li><strong>방 레이어 (Rooms Layer):</strong> 닫힌 벽면들의 집합으로 정의되는 방 노드이다. 방 노드는 자신을 구성하는 벽 노드들과 연결되어, 벽들 간의 상대적 위치 관계를 고정한다.</li>
<li><strong>층 레이어 (Floors Layer):</strong> 다층 건물 환경에서 각 층을 나타내며, 층간 이동 시 그래프의 구조를 관리한다.</li>
</ol>
<h3>5.2  Hydra와의 비교 및 이점</h3>
<p>Hydra가 센서 데이터로부터 상향식(Bottom-up)으로 그래프를 구축하는 데 중점을 둔다면, S-Graphs는 구조적 정보를 이용해 하향식(Top-down)으로 SLAM의 성능을 향상시키는 데 중점을 둔다.</p>
<ul>
<li><strong>강건성(Robustness):</strong> 텍스처가 부족한 흰색 복도나 특징점이 적은 환경에서는 시각적 SLAM이 실패하기 쉽다. 그러나 S-Graphs는 벽과 방의 구조적 제약 조건을 활용하므로 이러한 환경에서도 위치를 잃지 않고 강건하게 작동한다.</li>
<li><strong>효율성(Efficiency):</strong> 수만 개의 포인트 클라우드를 최적화 변수로 사용하는 기존 방식과 달리, 소수의 벽과 방 파라미터만을 최적화하므로 계산 효율이 매우 높다. 대규모 환경에서 10배 이상의 데이터 압축 효과를 보이며, 이는 장기 운용 로봇에게 필수적인 특성이다.</li>
<li>최신 연구인 **vS-Graphs (Visual S-Graphs)**는 이를 시각 정보와 결합하여 LiDAR 없이도 RGB-D 카메라만으로 상황 인식 그래프를 생성하고 최적화하는 방향으로 발전하고 있다.</li>
</ul>
<h2>6.  시공간(Spatio-Temporal) 인식과 4D 그래프의 진화: Khronos</h2>
<p>현실 세계는 정지해 있지 않으며, 사람과 물체는 끊임없이 움직인다. 기존의 정적(Static) 장면 그래프는 이러한 변화를 노이즈로 간주하거나 제대로 반영하지 못하는 한계가 있었다. MIT SPARK Lab의 최신 연구 <strong>Khronos</strong>는 단기적 동역학(Short-term Dynamics)과 장기적 변화(Long-term Changes)를 통합적으로 다루는 <strong>시공간 메트릭-시맨틱 SLAM (Spatio-temporal Metric-semantic SLAM, SMS)</strong> 프레임워크를 제안하였다.</p>
<h3>6.1  Khronos의 시간적 인수분해(Factorization)</h3>
<p><strong>Khronos: A Unified Approach for Spatiotemporal Metric-Semantic Perception</strong> 논문은 SMS 문제를 해결하기 위해 시스템을 시간적 스케일에 따라 두 개의 프로세스로 분리(Factorization)하는 접근법을 취한다.</p>
<ol>
<li><strong>고속 프로세스 (Fast Process): 단기 동역학 추적</strong></li>
</ol>
<ul>
<li><strong>활성 윈도우</strong> 내에서 빠르게 움직이는 에이전트(사람, 로봇)나 이동 중인 물체를 실시간으로 추적한다.</li>
<li>기존의 배경 감산(Background Subtraction) 방식과 달리, 3D 공간의 점유 확률 변화를 기반으로 동적 객체를 검출한다.</li>
<li>검출된 동적 객체는 장면 그래프에 일시적인 노드로 추가되며, 시간 <span class="math math-inline">t</span>에서의 위치와 속도 정보를 갖는다. 이는 로봇의 즉각적인 충돌 회피와 단기 경로 예측에 사용된다.</li>
</ul>
<ol start="2">
<li><strong>저수준 프로세스 (Slow Process): 장기 변화 추론 및 전역 4D 최적화</strong></li>
</ol>
<ul>
<li>로봇이 이전에 방문했던 지역을 다시 방문(Revisit)했을 때 발생하는 환경의 변화(예: 의자가 다른 방으로 옮겨짐, 문이 닫힘, 가구 배치가 바뀜)를 감지한다.</li>
<li>Khronos는 이를 <strong>4D 팩터 그래프</strong> 문제로 정식화한다. 시간 축을 포함한 그래프에서, 과거의 지도 상태와 현재의 관측치 사이의 불일치를 분석하여 “객체가 이동했다“는 사실을 추론한다.</li>
<li>객체의 이동은 그래프 상에서 기존 위치의 노드를 비활성화하고 새로운 위치에 노드를 생성하거나, 노드의 속성을 업데이트하는 방식으로 표현된다. 또한 **시간적 엣지(Temporal Edge)**를 통해 동일한 객체의 과거 상태와 현재 상태를 연결하여, 객체의 이력(History)을 추적할 수 있게 한다.</li>
</ul>
<p>이러한 접근은 로봇이 단순히 현재의 상태만을 아는 것이 아니라, 환경이 시간에 따라 어떻게 진화해 왔는지를 이해하는 4차원적 인지 능력을 부여한다. 이는 로봇이 “누가 내 컵을 어디로 옮겼지?“와 같은 과거 지향적 질문에 답하거나, 물체의 이동 패턴을 학습하여 미래를 예측하는 데 활용될 수 있다.</p>
<h2>7.  오픈 보캐블러리(Open-Vocabulary)와 파운데이션 모델의 융합: ConceptGraphs</h2>
<p>전통적인 장면 그래프 생성 방식은 사전에 정의된 폐쇄형 클래스 집합(Closed-set, 예: COCO 데이터셋의 80개 클래스)에 국한되어, 훈련 데이터에 없는 새로운 객체를 인식하지 못하는 한계가 있었다. 그러나 <strong>CLIP (Contrastive Language-Image Pre-training)</strong>, <strong>SAM (Segment Anything Model)</strong>, <strong>DINO</strong>와 같은 거대 파운데이션 모델(Foundation Model)의 등장은 임의의 객체를 인식하고 자연어로 질의할 수 있는 <strong>오픈 보캐블러리 3D 장면 그래프</strong> 시대를 열었다. 이 분야의 선도적인 연구가 바로 <strong>ConceptGraphs</strong>이다.</p>
<h3>7.1  ConceptGraphs의 생성 파이프라인</h3>
<p><strong>ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning</strong> 논문에서 제안한 방식은 2D 파운데이션 모델의 강력한 일반화 성능을 3D 공간으로 전이(Transfer)시키는 과정을 거친다.</p>
<ol>
<li><strong>클래스 불가지론적 2D 분할 (Class-Agnostic 2D Segmentation):</strong></li>
</ol>
<ul>
<li>입력된 RGB 이미지에 대해 <strong>SAM</strong>을 적용하여 이미지 내의 의미 있는 모든 영역(Mask)을 분할한다. 이때 특정 클래스를 지정하지 않고 “모든 물체“를 분할하므로, 훈련되지 않은 희귀한 물체도 검출할 수 있다.</li>
</ul>
<ol start="2">
<li><strong>시각-언어 특징 추출 (Vision-Language Feature Extraction):</strong></li>
</ol>
<ul>
<li>분할된 각 마스크 영역에 대해 <strong>CLIP</strong> 이미지 인코더를 통과시켜 고차원 특징 벡터(Embedding)를 추출한다. 이 벡터는 이미지의 시각적 패턴뿐만 아니라 자연어와의 의미적 연관성을 내포하고 있다.</li>
</ul>
<ol start="3">
<li><strong>3D 투영 및 멀티뷰 연관 (3D Projection &amp; Multi-view Association):</strong></li>
</ol>
<ul>
<li>깊이(Depth) 정보를 이용하여 2D 마스크와 특징 벡터를 3D 포인트 클라우드로 투영한다.</li>
<li>연속된 프레임에서 동일한 물체를 식별하고 통합(Fusion)하기 위해, 3D 공간 상의 중첩도(IoU)와 CLIP 특징 벡터 간의 **코사인 유사도(Cosine Similarity)**를 기반으로 데이터 연관(Data Association)을 수행한다. 이를 통해 여러 시점에서 관측된 정보가 하나의 3D 객체 노드로 통합된다.</li>
</ul>
<ol start="4">
<li><strong>LLM 기반 캡션 생성 및 관계 추론:</strong></li>
</ol>
<ul>
<li>통합된 3D 객체에 대해 VLM을 사용하여 “오래된 갈색 가죽 소파”, “어린이용 파란색 플라스틱 의자“와 같은 상세한 텍스트 캡션을 생성한다.</li>
<li>객체 간의 엣지를 정의하기 위해 LLM(Large Language Model)의 추론 능력을 활용한다. 두 객체의 캡션과 3D 좌표(중심점, 바운딩 박스)를 프롬프트로 입력하면, LLM은 상식(Commonsense)과 기하 정보를 결합하여 “A가 B 위에 놓여 있다(A is on B)”, “A는 B를 가리고 있다(A occludes B)“와 같은 관계를 추론해낸다. 이는 기존의 기하학적 규칙만으로는 파악하기 힘든 기능적, 맥락적 관계를 추출하는 데 탁월하다.</li>
</ul>
<h3>7.2  계층적 오픈 보캐블러리 매핑: HOV-SG</h3>
<p>ConceptGraphs가 객체 수준에 집중했다면, **HOV-SG (Hierarchical Open-Vocabulary 3D Scene Graphs)**는 이를 방(Room)과 층(Floor) 단위로 확장한다.</p>
<ul>
<li>HOV-SG는 3D 포인트 클라우드를 방 단위로 분할한 후, 해당 영역에 포함된 객체들의 특징 벡터를 집계(Aggregation)하여 방의 용도를 추론한다. 예를 들어, ’침대’와 ’옷장’이 있는 방은 ’침실’로, ’책상’과 ’컴퓨터’가 많은 방은 ’사무실’로 라벨링된다.</li>
<li>이를 통해 “2층 회의실에 있는 파란색 의자로 가라“와 같은 복잡한 계층적 자연어 명령을 해석하고 수행할 수 있는 내비게이션 그래프를 구축한다. 로봇은 먼저 ‘2층’ 노드를 찾고, 그 하위의 ‘회의실’ 노드를 탐색한 뒤, 최종적으로 ‘파란색 의자’ 객체 노드로 이동하는 계층적 탐색 전략을 구사할 수 있다.</li>
</ul>
<h2>8.  관계 예측(Relation Prediction) 및 그래프 신경망(GNN)의 활용</h2>
<p>3D 장면 그래프의 품질과 활용성은 노드 간의 엣지, 즉 **관계(Relation)**를 얼마나 정확하고 풍부하게 예측하느냐에 달려 있다. 단순한 거리 기반 휴리스틱은 “의자가 테이블에 ‘속해’ 있다“거나 “컵이 쟁반 ‘위에’ 있다“와 같은 미묘한 물리적, 기능적 관계를 포착하는 데 한계가 있다. 이를 극복하기 위해 최신 연구들은 **그래프 신경망(Graph Neural Networks, GNN)**을 도입하여 데이터로부터 관계적 패턴을 학습한다.</p>
<h3>8.1  GNN 기반 관계 추론 아키텍처</h3>
<p>일반적인 GNN 기반 3D 장면 그래프 생성(3DSSG) 모델의 파이프라인은 다음과 같다.</p>
<ol>
<li><strong>특징 인코딩 (Feature Encoding):</strong></li>
</ol>
<ul>
<li><strong>PointNet</strong>이나 <strong>Point Transformer</strong>와 같은 3D 백본 네트워크를 사용하여 각 객체의 점군으로부터 기하학적 특징 <span class="math math-inline">f_{geo}</span>를 추출한다. 동시에 객체의 클래스 라벨(Word Embedding)로부터 의미적 특징 <span class="math math-inline">f_{sem}</span>을 추출한다.</li>
</ul>
<ol start="2">
<li><strong>그래프 구축 및 메시지 전달 (Message Passing):</strong></li>
</ol>
<ul>
<li>초기에는 모든 객체 쌍을 연결한 완전 그래프(Fully-connected Graph)나 K-NN 그래프를 구성한다.</li>
<li><strong>GCN (Graph Convolutional Network)</strong> 또는 <strong>GAT (Graph Attention Network)</strong> 레이어를 통해 이웃 노드 간에 특징 정보를 교환한다. 이를 통해 각 노드는 자신의 정보뿐만 아니라 주변 이웃의 문맥(Context) 정보를 반영한 새로운 표현(Representation)으로 업데이트된다. 예를 들어, ‘의자’ 노드는 주변에 ‘책상’ 노드가 있다는 정보를 받아들여 자신이 ’사무용 의자’일 확률을 높이거나 ’책상 아래에 있다’는 관계를 예측할 준비를 한다.</li>
</ul>
<ol start="3">
<li><strong>엣지 분류 (Edge Classification):</strong></li>
</ol>
<ul>
<li>메시지 전달 과정을 거친 두 노드 특징 <span class="math math-inline">h_i, h_j</span>를 결합(Concatenation)하여 MLP(Multi-Layer Perceptron) 분류기에 통과시킨다.</li>
<li>MLP는 두 노드 사이의 관계 클래스(예: <em>support</em>, <em>proximity</em>, <em>standing on</em>, <em>none</em>)에 대한 확률 분포를 출력한다.</li>
</ul>
<h3>8.2  최신 연구 동향: 엣지 중심 추론과 대칭성 보존</h3>
<p>최근 연구인 <strong>LEO</strong> 등은 객체 중심(Object-centric)의 메시지 전달뿐만 아니라, <strong>엣지 중심(Edge-centric)의 관계적 추론</strong>을 강조한다. 이는 엣지를 또 다른 노드로 취급하는 <strong>라인 그래프(Line Graph)</strong> 변환을 통해, 관계들 간의 의존성을 학습한다. 예를 들어, “A가 B 위에 있다“는 관계는 “B가 A를 지지한다“는 관계와 밀접하게 연관되어 있다. 엣지 중심 GNN은 이러한 관계의 상호 의존성을 학습하여 예측의 일관성을 높인다.</p>
<p>또한 **ESGNN (Equivariant Scene Graph Neural Network)**은 3D 공간의 회전 및 이동 대칭성(SE(3) Equivariance)을 보존하는 GNN 구조를 제안하였다. 기존의 GNN은 입력 포인트 클라우드가 회전하면 그래프의 특징 값도 변하여 성능이 저하되는 문제가 있었다. ESGNN은 좌표계 변환에 대해 불변(Invariant)하거나 공변(Equivariant)하는 특징을 학습함으로써, 로봇이 어떤 방향에서 환경을 바라보더라도 일관된 장면 그래프를 생성할 수 있게 한다.</p>
<h2>9.  로봇 응용: 태스크 플래닝 및 시각적 질의응답(VQA)</h2>
<p>구축된 3D 장면 그래프는 단순한 지도가 아니라, 로봇의 고차원 인지 및 행동 결정을 지원하는 핵심 데이터베이스로 활용된다.</p>
<h3>9.1  장기 태스크 플래닝 (Long-Horizon Task Planning)</h3>
<p><strong>Taskography</strong>와 같은 벤치마크 연구는 3D 장면 그래프가 대규모 환경에서의 상징적 계획(Symbolic Planning)을 가능하게 함을 실증하였다.</p>
<ul>
<li><strong>시나리오:</strong> 사용자가 “배고프다“고 말했을 때, 로봇은 “부엌 이동 <span class="math math-inline">\rightarrow</span> 냉장고 탐색 <span class="math math-inline">\rightarrow</span> 냉장고 문 열기 <span class="math math-inline">\rightarrow</span> 사과 집기 <span class="math math-inline">\rightarrow</span> 사용자에게 전달“이라는 긴 시퀀스를 계획해야 한다.</li>
<li><strong>활용:</strong> 메트릭 공간에서 이러한 계획을 세우는 것은 계산 복잡도가 매우 높다. 그러나 3D 장면 그래프는 방대한 메트릭 데이터를 ‘부엌’, ‘냉장고’, ’사과’라는 노드와 ‘안에 있다(Inside)’, ’열 수 있다(Openable)’라는 엣지로 추상화한다.</li>
<li>**PDDL(Planning Domain Definition Language)**과 같은 기호적 플래너(Symbolic Planner)는 이 그래프 상에서 탐색(Search)을 수행하여 효율적으로 행동 시퀀스를 생성할 수 있다. <strong>Scrub</strong>과 같은 알고리즘은 태스크와 무관한 노드를 가지치기(Pruning)하여 계획 속도를 획기적으로 높인다.</li>
</ul>
<h3>9.2  시각적 질의응답 (Visual Question Answering, VQA)</h3>
<p><strong>GraphEQA</strong>와 같은 시스템은 3D 장면 그래프를 VLM(Vision-Language Model)의 장기 메모리(Long-term Memory)로 활용한다.</p>
<ul>
<li><strong>문제:</strong> 사용자가 “내 지갑 어디에 뒀어?“라고 물었을 때, 지갑이 현재 로봇의 시야에 없다면 일반적인 VQA 모델은 답할 수 없다.</li>
<li><strong>해법:</strong> 로봇은 지금까지 구축한 3D 장면 그래프 데이터베이스를 조회한다. 그래프 탐색을 통해 ‘지갑’ 속성을 가진 노드를 찾고, 해당 노드의 부모 노드가 ’거실 테이블’이며 관계가 ’위에 있음(On)’이라는 것을 파악한다. 이를 바탕으로 “거실 테이블 위에 있습니다“라고 답변하거나, 해당 위치로 사용자를 안내할 수 있다. 이는 로봇이 시공간적으로 분리된 정보를 연결하여 추론하는 능력을 보여준다.</li>
</ul>
<h2>10.  결론 및 향후 전망</h2>
<p>3D 장면 그래프는 로봇이 복잡한 3차원 물리 세계를 기하학(Geometry), 위상(Topology), 의미(Semantics)가 통합된 형태로 이해하게 하는 핵심 기술이다. <strong>Hydra</strong>와 <strong>S-Graphs</strong>를 통해 실시간성과 구조적 최적화 기술이 성숙 단계에 접어들었으며, <strong>Khronos</strong>를 통해 4차원 시공간 인식으로 확장이 이루어지고 있다. 특히 <strong>ConceptGraphs</strong>와 같이 LLM/VLM 파운데이션 모델과의 결합은 로봇에게 인간 수준의 오픈 보캐블러리 이해 능력과 상식적 추론 능력을 부여하고 있다.</p>
<p>향후 연구는 그래프 생성의 완전 자동화, 불확실성(Uncertainty)의 확률적 표현, 그리고 <strong>뉴로-심볼릭(Neuro-Symbolic) AI</strong>와의 결합을 통해 더욱 정교하고 강건한 공간 지능을 구현하는 방향으로 나아갈 것이다. 3D 장면 그래프는 단순한 데이터 구조가 아니라, 로봇의 인지(Perception)와 행동(Action)을 매개하고 인간과의 자연스러운 상호작용을 가능하게 하는 차세대 로봇 지능의 운영체제(OS)와 같은 역할을 수행할 것이다.</p>
<h2>11. 참고 자료</h2>
<ol>
<li>Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping, https://www.semanticscholar.org/paper/Kimera%3A-an-Open-Source-Library-for-Real-Time-and-Rosinol-Abate/86db02456021ecbd3d813ecd38b52c01a364a16c</li>
<li>Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping - Massachusetts Institute of Technology, https://www.mit.edu/~arosinol/papers/Rosinol20icra-Kimera.pdf</li>
<li>1월 26, 2026에 액세스, https://openaccess.thecvf.com/content_ICCV_2019/papers/Armeni_3D_Scene_Graph_A_Structure_for_Unified_Semantics_3D_Space_ICCV_2019_paper.pdf</li>
<li>Scene graph - Wikipedia, https://en.wikipedia.org/wiki/Scene_graph</li>
<li>Concept-Guided Exploration: Building Persistent, Actionable Scene Graphs - MDPI, https://www.mdpi.com/2076-3417/15/20/11084</li>
<li>[2201.13360] Hydra: A Real-time Spatial Perception System for 3D Scene Graph Construction and Optimization - arXiv, https://arxiv.org/abs/2201.13360</li>
<li>3-D Scene Graph: A Sparse and Semantic Representation of Physical Environments for Intelligent Agents - arXiv, https://arxiv.org/pdf/1908.04929</li>
<li>Scene graph, https://cseweb.ucsd.edu/classes/wi18/cse167-a/lec10.pdf</li>
<li>Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation, https://www.roboticsproceedings.org/rss20/p077.pdf</li>
<li>Hydra: A Real-time Spatial Perception System for 3D Scene Graph Construction and Optimization - DSpace@MIT, https://dspace.mit.edu/bitstream/handle/1721.1/145300/2201.13360.pdf</li>
<li>ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning, https://concept-graphs.github.io/</li>
<li>ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning - IEEE Xplore, https://ieeexplore.ieee.org/iel8/10609961/10609862/10610243.pdf</li>
<li>Robot Localization Using Situational Graphs (S-Graphs) and Building Architectural Plans, https://www.mdpi.com/2218-6581/12/3/65</li>
<li>S-Graphs 2.0 – A Hierarchical-Semantic Optimization and Loop Closure for SLAM - IEEE Xplore, https://ieeexplore.ieee.org/iel8/7083369/11215960/11197654.pdf</li>
<li>Graph-to-3D: End-to-End Generation and Manipulation of 3D Scenes Using Scene Graphs - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2021/papers/Dhamo_Graph-to-3D_End-to-End_Generation_and_Manipulation_of_3D_Scenes_Using_Scene_ICCV_2021_paper.pdf</li>
<li>Khronos: A Unified Approach for Spatio-Temporal Metric-Semantic SLAM in Dynamic Environments - arXiv, https://arxiv.org/html/2402.13817v2</li>
<li>Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping, https://www.researchgate.net/publication/344982175_Kimera_an_Open-Source_Library_for_Real-Time_Metric-Semantic_Localization_and_Mapping</li>
<li>MIT-SPARK/Kimera-VIO: Visual Inertial Odometry with SLAM capabilities and 3D Mesh generation. - GitHub, https://github.com/MIT-SPARK/Kimera-VIO</li>
<li>Hydra: A Spatial Perception Engine for Constructing and Optimizing 3D Scene Graphs in Real-time - DSpace@MIT, https://dspace.mit.edu/handle/1721.1/145096?show=full</li>
<li>Hydra: A Real-time Spatial Perception System for 3D Scene Graph Construction and Optimization - Robotics, https://www.roboticsproceedings.org/rss18/p050.pdf</li>
<li>S-Graphs 2.0 – A Hierarchical-Semantic Optimization and Loop Closure for SLAM - arXiv, https://arxiv.org/abs/2502.18044</li>
<li>[2212.11770] S-Graphs+: Real-time Localization and Mapping leveraging Hierarchical Representations - arXiv, https://arxiv.org/abs/2212.11770</li>
<li>vS-Graphs: Integrating Visual SLAM and Situational Graphs through Multi-level Scene Understanding - arXiv, https://arxiv.org/html/2503.01783v1</li>
<li>snt-arg/visual_sgraphs: vision-driven Situational Graphs (vS-Graphs) - GitHub, https://github.com/snt-arg/visual_sgraphs</li>
<li>Khronos: A Unified Approach for Spatio-Temporal Metric-Semantic SLAM in Dynamic Environments - ResearchGate, https://www.researchgate.net/publication/380152001_Khronos_A_Unified_Approach_for_Spatio-Temporal_Metric-Semantic_SLAM_in_Dynamic_Environments</li>
<li>Khronos: 4D Spatio-temporal Perception for Autonomous Robots …, https://aeroastro.mit.edu/news-impact/khronos-4d-spatio-temporal-perception-for-autonomous-robots/</li>
<li>Open-Vocabulary 3D Scene Graphs for Perception and Planning - ConceptGraphs, https://concept-graphs.github.io/assets/pdf/2023-ConceptGraphs.pdf</li>
<li>Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation, https://hovsg.github.io/</li>
<li>TESGNN: Temporal Equivariant Scene Graph Neural Networks for Efficient and Robust Multi-View 3D Scene Understanding - arXiv, https://arxiv.org/html/2411.10509v2</li>
<li>Object-Centric Representation Learning for Enhanced 3D Semantic Scene Graph Prediction | OpenReview, https://openreview.net/forum?id=LjmXrUsSrg</li>
<li>Edge-Centric Relational Reasoning for 3D Scene Graph Prediction - arXiv, https://arxiv.org/html/2511.15288v1</li>
<li>Graph Neural Networks in Point Clouds: A Survey - MDPI, https://www.mdpi.com/2072-4292/16/14/2518</li>
<li>MLGCN: an ultra efficient graph convolutional neural model for 3D point cloud analysis - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC11449895/</li>
<li>ESGNN: TOWARDS EQUIVARIANT SCENE GRAPH NEURAL NETWORK FOR 3D SCENE UNDERSTANDING, http://people.cs.uchicago.edu/~hytruongson/posters/IEEE_ROMAN_2024_Poster.pdf</li>
<li>Taskography: Evaluating robot task planning over large 3D scene graphs - OpenReview, https://openreview.net/forum?id=nWLt35BU1z_</li>
<li>TASKOGRAPHY: Evaluating robot task planning over large 3D scene graphs, https://montrealrobotics.ca/pdfs/taskography.pdf</li>
<li>GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering, https://arxiv.org/html/2412.14480v2</li>
<li>GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering, https://saumyasaxena.github.io/grapheqa/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>