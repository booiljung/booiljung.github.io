<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:6.4.1 2D-to-3D Lifting: 2D 비전 모델의 의미론적 특징(Feature)을 3D 포인트 클라우드나 복셀(Voxel)에 투영하는 방법.</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>6.4.1 2D-to-3D Lifting: 2D 비전 모델의 의미론적 특징(Feature)을 3D 포인트 클라우드나 복셀(Voxel)에 투영하는 방법.</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 6. 오픈 보캐블러리와 시맨틱 이해 (Open-Vocabulary & Semantic Understanding)</a> / <a href="index.html">6.4 2D 시맨틱을 3D 공간으로: 공간 지능의 확장 (Spatial AI)</a> / <span>6.4.1 2D-to-3D Lifting: 2D 비전 모델의 의미론적 특징(Feature)을 3D 포인트 클라우드나 복셀(Voxel)에 투영하는 방법.</span></nav>
                </div>
            </header>
            <article>
                <h1>6.4.1 2D-to-3D Lifting: 2D 비전 모델의 의미론적 특징(Feature)을 3D 포인트 클라우드나 복셀(Voxel)에 투영하는 방법.</h1>
<h2>1.  서론: 의미론적 3D 공간 이해를 위한 가교, 리프팅(Lifting)</h2>
<h3>1.1  배경 및 정의</h3>
<p>현대 컴퓨터 비전과 로보틱스 분야에서 3D 장면 이해(Scene Understanding)는 단순한 기하학적 재구성(Geometric Reconstruction)을 넘어, 공간 내 객체와 영역의 의미(Semantics)를 파악하는 방향으로 진화하고 있다. 지난 10년 간 합성곱 신경망(CNN)과 비전 트랜스포머(ViT)를 위시한 2D 딥러닝 모델들은 이미지 분류, 객체 탐지, 의미론적 분할(Semantic Segmentation)에서 인간 수준의 성능을 달성했다. 특히 CLIP(Contrastive Language-Image Pre-training)이나 SAM(Segment Anything Model)과 같은 대규모 파운데이션 모델(Foundation Model)의 등장은 제한된 클래스 라벨을 넘어 개방형 어휘(Open-vocabulary) 인식을 가능하게 했다.</p>
<p>그러나 이러한 2D 모델의 “시각적 지능“을 3D 공간으로 확장하는 데는 근본적인 장벽이 존재한다. 3D 데이터(포인트 클라우드, 메쉬, 복셀 등)는 2D 이미지에 비해 데이터 획득이 어렵고, 의미론적 주석(Annotation)이 달린 대규모 데이터셋이 현저히 부족하기 때문이다. 이에 대한 해결책으로 제시된 핵심 방법론이 바로 **2D-to-3D 리프팅(Lifting)**이다.</p>
<p>리프팅은 2D 이미지 공간에서 사전 학습된 모델이 추출한 고차원 의미론적 특징(Semantic Features)—그것이 픽셀 단위의 클래스 로짓(Logits)이든, CLIP의 임베딩 벡터이든—을 3D 기하학적 표현(Representation)으로 역투영(Back-projection)하여 할당하는 과정을 의미한다. 이는 2D의 풍부한 의미론적 정보와 3D의 정확한 공간 정보를 결합하여, 로봇이 “이 앞의 장애물은 1m 거리에 있다“를 넘어 “이 앞 1m 거리에 ‘지우기 쉬운’ ’유리잔’이 있다“라고 이해할 수 있게 만든다.</p>
<h3>1.2  핵심 과제 및 응용 분야</h3>
<p>이 과정은 단순히 2D 이미지를 3D 표면에 텍스처 매핑(Texture Mapping)하는 것보다 훨씬 복잡한 수학적, 계산적 난제를 포함한다.</p>
<ol>
<li><strong>시점 불일치(Multi-view Inconsistency):</strong> 동일한 3D 객체라도 보는 각도에 따라 2D 모델은 다르게 인식할 수 있다(예: 컵의 손잡이가 보일 때와 안 보일 때의 분류 차이).</li>
<li><strong>차원성 및 메모리(Dimensionality &amp; Memory):</strong> RGB 색상은 3채널에 불과하지만, DINO나 CLIP의 특징 벡터는 수백에서 수천 차원에 이른다. 이를 3D 공간 전체에 저장하고 처리하는 것은 막대한 메모리와 연산량을 요구한다.</li>
<li><strong>가려짐 및 앨리어싱(Occlusion &amp; Aliasing):</strong> 2D 픽셀을 3D로 투영할 때, 객체 경계면에서의 미세한 오차가 3D 공간에서는 ’유령 기하학(Ghost Geometry)’이나 ’날리는 픽셀(Flying Pixels)’과 같은 심각한 아티팩트를 유발한다.</li>
</ol>
<p>이러한 리프팅 기술은 자율 주행 자동차의 정밀 시맨틱 매핑(Semantic Mapping), 로봇의 언어 기반 네비게이션(Language-guided Navigation), 그리고 증강 현실(AR)에서의 물리적 객체 상호작용 등 다양한 분야의 기반 기술로 활용된다. 본 장에서는 2D 특징을 3D로 리프팅하기 위한 기하학적 기초부터, 포인트 클라우드, 복셀, 그리고 최신 뉴럴 필드(Neural Fields) 및 3D Gaussian Splatting을 활용한 고급 기법까지 심도 있게 다룬다.</p>
<hr />
<h2>2.  기하학적 기초: 핀홀 카메라 모델과 역투영(Back-projection)</h2>
<p>2D-to-3D 리프팅의 가장 근본적인 연산은 2D 픽셀 좌표계와 3D 월드 좌표계 사이의 상호 변환이다. 이를 이해하기 위해서는 핀홀 카메라 모델과 투영 기하학에 대한 엄밀한 수학적 정의가 선행되어야 한다.</p>
<h3>2.1  핀홀 카메라 모델과 투영 행렬 (Projection Matrix)</h3>
<p>3차원 공간상의 한 점 <span class="math math-inline">\mathbf{P}_w =^T</span>가 2D 이미지 평면의 픽셀 <span class="math math-inline">\mathbf{p} = [u, v]^T</span>로 투영되는 과정은 카메라의 **내부 파라미터(Intrinsics, <span class="math math-inline">K</span>)**와 **외부 파라미터(Extrinsics, $$)**에 의해 결정된다.</p>
<p>동차 좌표계(Homogeneous Coordinates)를 도입하여, 3D 점 <span class="math math-inline">\mathbf{P}_w</span>를 <span class="math math-inline">^T</span>로, 2D 픽셀 <span class="math math-inline">\mathbf{p}</span>를 <span class="math math-inline">[u, v, 1]^T</span>로 표현하면, 투영 과정은 다음과 같은 행렬 곱으로 나타낼 수 있다:<br />
<span class="math math-display">
s \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = \mathbf{K} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}
</span><br />
여기서 <span class="math math-inline">s</span>는 스케일 팩터(Scale Factor)로, 일반적으로 카메라 좌표계에서의 깊이 값 <span class="math math-inline">Z_c</span>에 해당한다. 내부 파라미터 행렬 <span class="math math-inline">\mathbf{K}</span>는 카메라의 광학적 특성을 정의하며 다음과 같다:<br />
<span class="math math-display">
\mathbf{K} = \begin{bmatrix} f_x &amp; 0 &amp; c_x \\ 0 &amp; f_y &amp; c_y \\ 0 &amp; 0 &amp; 1 \end{bmatrix}
</span></p>
<ul>
<li><span class="math math-inline">f_x, f_y</span>: 초점 거리(Focal Length)로, 렌즈 중심에서 이미지 센서까지의 거리를 픽셀 단위로 환산한 값이다. 비정사각 픽셀(Non-square pixels)을 고려하여 x축과 y축을 구분하지만, 현대 디지털 카메라에서는 보통 <span class="math math-inline">f_x \approx f_y</span>이다.</li>
<li><span class="math math-inline">c_x, c_y</span>: 주점(Principal Point)으로, 렌즈의 광축(Optical Axis)이 이미지 센서와 만나는 점의 픽셀 좌표이다. 일반적으로 이미지의 중심 <span class="math math-inline">(W/2, H/2)</span>에 위치한다.</li>
</ul>
<p>외부 파라미터 $$는 월드 좌표계(World Coordinate System)에서 카메라 좌표계(Camera Coordinate System)로의 강체 변환(Rigid Body Transformation)을 나타낸다. <span class="math math-inline">\mathbf{R}</span>은 <span class="math math-inline">3 \times 3</span> 회전 행렬, <span class="math math-inline">\mathbf{t}</span>는 <span class="math math-inline">3 \times 1</span> 이동 벡터이다. 이때 주의할 점은 <span class="math math-inline">\mathbf{t}</span>가 카메라의 월드 좌표상 위치가 아니라, 월드 원점이 카메라 좌표계에서 어디에 있는지를 나타낸다는 것이다. 카메라의 월드 좌표상 위치 <span class="math math-inline">\mathbf{C}*{pos}</span>는 <span class="math math-inline">\mathbf{C}*{pos} = -\mathbf{R}^T \mathbf{t}</span>로 계산된다.</p>
<h3>2.2  2D 픽셀의 3D 역투영 (Back-projection) 공식 유도</h3>
<p>리프팅은 투영의 역과정이다. 2D 픽셀 <span class="math math-inline">(u, v)</span>에 할당된 의미론적 특징(예: ’의자’라는 라벨 또는 임베딩 벡터)을 3D 공간의 특정 위치로 보내야 한다. 그러나 2D 픽셀 하나는 3D 공간상의 무한한 광선(Ray)에 대응되므로, 깊이 정보(Depth, <span class="math math-inline">d</span>) 없이는 3D 위치를 특정할 수 없다.</p>
<p>깊이 센서(LiDAR, RGB-D 카메라) 또는 깊이 추정 네트워크로부터 해당 픽셀의 깊이 값 <span class="math math-inline">d</span> (일반적으로 <span class="math math-inline">Z_c</span>)를 획득했다고 가정할 때, 3D 월드 좌표 <span class="math math-inline">\mathbf{P}_w</span>를 복원하는 <strong>역투영 공식</strong>은 다음과 같이 단계적으로 유도된다.</p>
<ol>
<li><strong>픽셀 좌표 <span class="math math-inline">\rightarrow</span> 정규화된 이미지 평면 (Normalized Image Plane):</strong></li>
</ol>
<p>먼저 픽셀 좌표 <span class="math math-inline">[u, v, 1]^T</span>에 내부 파라미터 행렬의 역행렬 <span class="math math-inline">\mathbf{K}^{-1}</span>을 곱하여, 초점 거리가 1인 정규화된 평면상의 좌표 <span class="math math-inline">[x_n, y_n, 1]^T</span>로 변환한다.<br />
<span class="math math-display">
   \begin{bmatrix} x_n \\ y_n \\ 1 \end{bmatrix} = \mathbf{K}^{-1} \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = \begin{bmatrix} (u - c_x)/f_x \\ (v - c_y)/f_y \\ 1 \end{bmatrix}
</span><br />
이 좌표는 카메라 원점으로부터 해당 픽셀을 향하는 방향 벡터(Direction Vector)로 해석될 수 있다.</p>
<ol start="2">
<li><strong>정규화된 평면 <span class="math math-inline">\rightarrow</span> 카메라 좌표계 (Camera Coordinate System):</strong></li>
</ol>
<p>정규화된 좌표에 깊이 값 <span class="math math-inline">d</span>를 곱하여 카메라 좌표계상의 실제 3D 점 <span class="math math-inline">\mathbf{P}_c</span>를 복원한다. 여기서 <span class="math math-inline">d</span>는 카메라의 Z축 방향 거리(<span class="math math-inline">Z_c</span>)를 의미한다. 만약 <span class="math math-inline">d</span>가 유클리드 거리(Euclidean Distance, Ray Length)라면 방향 벡터를 정규화한 후 곱해야 한다. 일반적인 깊이 맵(Depth Map)은 <span class="math math-inline">Z_c</span>를 저장하므로 다음과 같다:<br />
<span class="math math-display">
   \mathbf{P}_c = d \cdot \begin{bmatrix} x_n \\ y_n \\ 1 \end{bmatrix} = \begin{bmatrix} d \cdot (u - c_x)/f_x \\ d \cdot (v - c_y)/f_y \\ d \end{bmatrix}
</span></p>
<ol start="3">
<li><strong>카메라 좌표계 <span class="math math-inline">\rightarrow</span> 월드 좌표계 (World Coordinate System):</strong></li>
</ol>
<p>마지막으로 외부 파라미터의 역변환을 적용한다. 카메라 좌표계 기준 점 <span class="math math-inline">\mathbf{P}_c</span>와 월드 좌표계 기준 점 <span class="math math-inline">\mathbf{P}_w</span>의 관계는 <span class="math math-inline">\mathbf{P}_c = \mathbf{R} \mathbf{P}_w + \mathbf{t}</span>이다. 이를 <span class="math math-inline">\mathbf{P}_w</span>에 대해 정리하면:<br />
<span class="math math-display">
   \mathbf{P}_w = \mathbf{R}^{-1} (\mathbf{P}_c - \mathbf{t})
</span><br />
회전 행렬의 역행렬은 전치 행렬과 같으므로(<span class="math math-inline">\mathbf{R}^{-1} = \mathbf{R}^T</span>), 최종 식은 다음과 같다:<br />
<span class="math math-display">
   \mathbf{P}_w = \mathbf{R}^T \mathbf{P}_c - \mathbf{R}^T \mathbf{t}
</span><br />
또는 카메라의 월드 위치 <span class="math math-inline">\mathbf{C}_{pos} = -\mathbf{R}^T \mathbf{t}</span>를 사용하여 더 직관적으로 표현하면:<br />
<span class="math math-display">
   \mathbf{P}_w = \mathbf{R}^T \mathbf{P}_c + \mathbf{C}_{pos}
</span><br />
이 수식은 Lift3D , OpenUrban3D , 그리고 다양한 SLAM 시스템에서 2D 특징을 3D 점으로 변환하는 핵심 알고리즘으로 사용된다.</p>
<h3>2.3  레이 캐스팅(Ray Casting)과 가시성(Visibility) 검증</h3>
<p>단순한 역투영만으로는 3D 구조의 복잡성을 완전히 다룰 수 없다. 특히, 특정 시점에서 3D 포인트나 복셀이 다른 물체에 의해 가려져 있는 경우(Occlusion), 2D 특징을 해당 위치에 투영하면 안 된다. 이를 해결하기 위해 <strong>레이 캐스팅(Ray Casting)</strong> 기법이 필수적이다.</p>
<p>레이 캐스팅은 카메라 원점 <span class="math math-inline">\mathbf{O}</span>에서 픽셀 방향으로 광선 <span class="math math-inline">\mathbf{r}(t) = \mathbf{O} + t\mathbf{d}</span>를 발사하여 3D 구조(메쉬, 복셀 등)와의 교차점을 찾는 과정이다.</p>
<ul>
<li><strong>복셀 그리드에서의 레이 순회 (Voxel Traversal):</strong> 복셀 그리드와 같은 이산적인 3D 구조에서는 <strong>Amanatides-Woo 알고리즘</strong> 이 표준으로 사용된다. 이 알고리즘은 광선이 통과하는 복셀들을 순차적으로 방문하며, 각 복셀 경계면(Face)을 통과하는 <span class="math math-inline">t</span> 값을 효율적으로 갱신한다. 이를 통해 광선이 처음으로 만나는 ‘점유된(Occupied)’ 복셀을 정확히 찾아내어, 특징을 해당 복셀의 표면에만 할당할 수 있다. 이는 벽 뒤의 물체에 특징이 투영되는 ‘투영 출혈(Projection Bleeding)’ 현상을 방지한다.</li>
<li><strong>Z-버퍼링을 활용한 가시성 검사:</strong> 이미 3D 포인트 클라우드나 메쉬가 구축되어 있다면, 역으로 3D 점을 현재 카메라 시점으로 재투영(Reprojection)하여 가시성을 확인할 수 있다. 3D 점 <span class="math math-inline">\mathbf{P}_w</span>를 현재 카메라로 투영하여 얻은 깊이 <span class="math math-inline">d_{proj}</span>와, 해당 픽셀의 깊이 센서 측정값 <span class="math math-inline">d_{obs}</span>를 비교한다:</li>
</ul>
<p><span class="math math-display">
| d_{proj} - d_{obs} | &lt; \epsilon
</span></p>
<p>차이가 임계값 <span class="math math-inline">\epsilon</span> 이내라면 해당 점은 가시적인(Visible) 것으로 간주하고 특징을 업데이트한다. 만약 <span class="math math-inline">d_{proj} \gg d_{obs}</span>라면, 해당 점은 다른 물체 뒤에 가려진 것이므로 업데이트에서 제외한다.</p>
<h2>3.  특징 추출(Feature Extraction)과 3D 표현별 리프팅 전략</h2>
<p>역투영을 통해 2D와 3D 사이의 기하학적 연결이 수립되었다면, 다음 단계는 ’어떤 특징’을 ’어떤 3D 표현’에 저장할 것인가를 결정하는 것이다. 리프팅 대상이 되는 특징은 단순한 RGB 색상부터, 객체 클래스 확률(Logits), 그리고 고차원 임베딩 벡터까지 다양하다.</p>
<h3>3.1  리프팅 대상 특징 (What to Lift?)</h3>
<ol>
<li><strong>의미론적 로짓 (Semantic Logits):</strong> Semantic Segmentation 모델(예: DeepLab, Mask2Former)의 출력인 클래스별 확률 분포이다. 일반적으로 <span class="math math-inline">K</span>개의 클래스에 대한 로짓 벡터 <span class="math math-inline">\mathbf{l} \in \mathbb{R}^K</span> 형태를 가진다. 이는 폐쇄형 집합(Closed-set) 인식에 유리하며, 메모리 사용량이 예측 가능하다.</li>
<li><strong>고차원 임베딩 (High-dimensional Embeddings):</strong> CLIP, DINO, ImageBind와 같은 파운데이션 모델의 중간 층(Intermediate Layer) 또는 최종 출력 벡터이다. 이는 개방형 어휘(Open-vocabulary) 인식을 가능하게 하지만, 차원이 매우 높아(예: CLIP ViT-L/14는 768차원) 메모리 효율적인 저장 전략이 필요하다.</li>
<li><strong>객체 인스턴스 ID (Instance IDs):</strong> SAM과 같은 모델이 생성한 인스턴스 마스크 ID이다. 이는 3D 공간에서 객체 단위의 분할을 가능하게 하지만, 시점 간 인스턴스 ID의 일관성(Association)을 유지하는 것이 어렵다.</li>
</ol>
<h3>3.2  포인트 클라우드(Point Cloud) 기반 리프팅</h3>
<p>포인트 클라우드는 <span class="math math-inline">\mathcal{P} = \{\mathbf{p}_i | i=1 \dots N\}</span> 형태의 3D 점 집합이다. 가장 직관적인 리프팅 방식은 각 점 <span class="math math-inline">\mathbf{p}_i</span>에 특징 벡터 <span class="math math-inline">\mathbf{f}_i</span>를 속성으로 추가하는 것이다.</p>
<h4>3.2.1  ConceptFusion: 픽셀 정렬(Pixel-aligned) 특징 융합</h4>
<p>**ConceptFusion **은 포인트 클라우드에 멀티모달(Multimodal) 특징을 융합하는 대표적인 방법론이다. 기존의 CLIP 모델은 이미지 전체에 대한 글로벌 임베딩(Global Feature)은 잘 추출하지만, 픽셀 단위의 로컬 특징(Local Feature)은 약하다는 단점이 있다. ConceptFusion은 이를 해결하기 위해 다음과 같은 융합 전략을 사용한다.</p>
<ul>
<li><strong>로컬-글로벌 특징 혼합:</strong></li>
</ul>
<p>SAM과 같은 클래스 불문(Class-agnostic) 마스크 생성기를 사용하여 이미지 내 객체 영역(Region)을 추출한다. 각 영역에 대해 CLIP 임베딩(로컬 특징, <span class="math math-inline">\mathbf{f}_L</span>)을 추출하고, 전체 이미지 임베딩(글로벌 특징, <span class="math math-inline">\mathbf{f}_G</span>)과 융합한다. 융합 가중치는 각 픽셀과 특징 간의 연관성에 따라 결정된다.<br />
<span class="math math-display">
  \mathbf{f}_{pixel} = w \cdot \mathbf{f}_G + (1-w) \cdot \mathbf{f}_L
</span><br />
여기서 <span class="math math-inline">w</span>는 코사인 유사도(Cosine Similarity)를 기반으로 계산되어, 픽셀이 특정 로컬 객체에 속할 확률을 반영한다.</p>
<ul>
<li><strong>신뢰도 기반 업데이트:</strong></li>
</ul>
<p>포인트 클라우드의 각 점은 여러 시점에서 관측될 수 있다. ConceptFusion은 각 관측의 신뢰도(Confidence)를 고려하여 특징을 누적 평균(Weighted Moving Average)한다. 이미지 중심부에 위치하거나 카메라와 가까운 관측일수록 높은 신뢰도를 부여한다.<br />
<span class="math math-display">
  \mathbf{f}_{3D}^{(new)} = \frac{c_{acc} \mathbf{f}_{3D}^{(old)} + w_{obs} \mathbf{f}_{2D}}{c_{acc} + w_{obs}}, \quad c_{acc} \leftarrow c_{acc} + w_{obs}
</span><br />
이 방식은 노이즈를 효과적으로 제거하고 3D 일관성을 높인다.</p>
<h4>3.2.2  OpenSeg 및 OpenUrban3D</h4>
<p>**OpenUrban3D **와 같은 연구에서는 포인트 클라우드에 2D 오픈 보캐블러리 특징을 투영할 때, <strong>샘플 밸런싱(Sample Balancing)</strong> 전략을 도입한다. 단순히 모든 픽셀을 투영하면 카메라 가까이 있는 객체나 큰 객체에 포인트가 과도하게 할당되는 문제가 발생한다. 이를 방지하기 위해 마스크별로 포인트 수를 균등화하거나, 특징이 희소한 영역에 우선적으로 할당하는 전략을 사용하여 대규모 도시 환경에서도 효율적인 의미론적 매핑을 수행한다.</p>
<h3>3.3  복셀 그리드(Voxel Grid) 기반 리프팅</h3>
<p>복셀 그리드는 3D 공간을 균일한 격자(Grid)로 나눈 구조로, 각 복셀 <span class="math math-inline">V_{ijk}</span>에 특징을 저장한다. 이는 공간의 점유(Occupancy) 정보와 의미론적 정보를 동시에 관리하기에 적합하다.</p>
<h4>3.3.1  TSDF와 의미론적 융합 (SemanticFusion)</h4>
<p>**SemanticFusion **과 같은 시스템은 기하학적 재구성을 위해 TSDF(Truncated Signed Distance Function)를 사용하고, 각 복셀에 확률 분포 벡터를 추가한다. TSDF는 표면까지의 거리를 저장하므로, 표면 근처의 복셀(Zero-crossing)에만 의미론적 특징을 업데이트하여 메모리를 절약할 수 있다.</p>
<ul>
<li><strong>베이지안 업데이트(Bayesian Update)의 적용:</strong> 복셀 단위 리프팅의 핵심은 <strong>베이지안 융합</strong>이다. 이는 단순 평균보다 통계적으로 훨씬 견고하다. 특히 범주형 데이터(Categorical Data)의 경우, 다항 분포(Multinomial Distribution)의 켤레 사전 분포(Conjugate Prior)인 **디리클레 분포(Dirichlet Distribution)**를 활용한다. (상세 내용은 4절에서 다룸)</li>
</ul>
<h4>3.3.2  희소 복셀 구조 (Sparse Voxel Structures)</h4>
<p>복셀 그리드의 가장 큰 단점은 메모리 비효율성이다. 대부분의 3D 공간은 빈 공간(Air)이기 때문이다. 이를 해결하기 위해 **옥트리(Octree)**나 <strong>해시 테이블(Spatial Hash Table)</strong> 기반의 희소 구조가 사용된다.</p>
<ul>
<li>SLIM-VDB : OpenVDB와 같은 희소 자료구조를 활용하여, 데이터가 존재하는 표면 근처의 활성 복셀(Active Voxel)에만 메모리를 할당하고 특징을 리프팅한다. 이를 통해 실시간으로 대규모 환경의 시맨틱 매핑이 가능하다.</li>
</ul>
<h2>4.  특징 융합(Aggregation) 알고리즘: 2D 불확실성의 해결</h2>
<p>2D 모델의 출력은 시점, 조명, 가려짐 등에 따라 불안정하다. 3D 리프팅의 성공 여부는 이러한 다중 시점의 불일치(Inconsistency)를 어떻게 하나의 일관된 3D 표현으로 **융합(Aggregation)**하느냐에 달려 있다.</p>
<h3>4.1  가중 이동 평균 (Weighted Moving Average)</h3>
<p>가장 기본적이고 널리 사용되는 방법이다. 연속적인 특징 벡터(예: CLIP 임베딩, RGB 색상)를 융합할 때 주로 사용된다.<br />
<span class="math math-display">
\mathbf{f}_{3D} \leftarrow \frac{W_{acc} \mathbf{f}_{3D} + w_t \mathbf{f}_{t}}{W_{acc} + w_t}
</span></p>
<ul>
<li><strong>장점:</strong> 구현이 간단하고 계산 비용이 매우 낮다.</li>
<li><strong>단점:</strong> 아웃라이어(Outlier)에 취약하다. 잘못된 2D 예측 하나가 평균을 오염시킬 수 있다.</li>
<li><strong>개선:</strong> 신뢰도(Confidence) <span class="math math-inline">w_t</span>를 정교하게 설계해야 한다. 예를 들어, 카메라와 표면의 법선(Normal)이 이루는 각도가 클수록(비스듬히 볼수록) 가중치를 낮추거나, 2D 세그멘테이션의 엔트로피(Entropy)가 높을수록 가중치를 낮춘다.</li>
</ul>
<h3>4.2  베이지안 융합 (Bayesian Fusion): 디리클레 업데이트</h3>
<p>이산적인 클래스 라벨(Discrete Class Labels)을 융합할 때는 베이지안 프레임워크가 표준이다. 이는 관측된 데이터로부터 사후 확률(Posterior)을 지속적으로 갱신한다.</p>
<h4>4.2.1  디리클레 분포 업데이트 유도</h4>
<p><span class="math math-inline">K</span>개의 클래스에 대한 확률 분포 <span class="math math-inline">\mathbf{p} = [p_1, \dots, p_K]^T</span>를 추정한다고 하자 (<span class="math math-inline">\sum p_k = 1</span>). 이 <span class="math math-inline">\mathbf{p}</span> 자체를 확률 변수로 보고, 그 분포를 모델링하는 것이 디리클레 분포이다.<br />
<span class="math math-display">
Dir(\mathbf{p} | \boldsymbol{\alpha}) = \frac{1}{B(\boldsymbol{\alpha})} \prod_{k=1}^{K} p_k^{\alpha_k - 1}
</span><br />
여기서 <span class="math math-inline">\boldsymbol{\alpha} = [\alpha_1, \dots, \alpha_K]^T</span>는 집중도 파라미터(Concentration Parameter)로, 각 클래스가 관측된 횟수(또는 의사 카운트, Pseudo-count)로 해석될 수 있다.</p>
<p>새로운 관측 <span class="math math-inline">\mathbf{y}_t</span> (원-핫 벡터 또는 확률 분포)가 들어왔을 때, 사후 분포의 파라미터 <span class="math math-inline">\boldsymbol{\alpha}_t</span>는 다음과 같이 단순한 덧셈으로 갱신된다:<br />
<span class="math math-display">
\boldsymbol{\alpha}_t = \boldsymbol{\alpha}_{t-1} + \mathbf{y}_t
</span><br />
이때 각 클래스의 기대 확률(Expected Probability)은 다음과 같다:<br />
<span class="math math-display">
\mathbb{E}[p_k] = \frac{\alpha_k}{\sum_{j=1}^{K} \alpha_j}
</span><br />
<strong>표 6.4.1 융합 알고리즘 비교</strong></p>
<table><thead><tr><th><strong>특징</strong></th><th><strong>가중 이동 평균 (Weighted Avg)</strong></th><th><strong>베이지안 융합 (Bayesian / Dirichlet)</strong></th></tr></thead><tbody>
<tr><td><strong>대상 데이터</strong></td><td>연속적 벡터 (RGB, CLIP 임베딩)</td><td>이산적 범주 (Class Labels, Occupancy)</td></tr>
<tr><td><strong>수학적 원리</strong></td><td>선형 보간 (Linear Interpolation)</td><td>켤레 사전 분포 (Conjugate Prior) 업데이트</td></tr>
<tr><td><strong>장점</strong></td><td>고차원 데이터 처리에 효율적</td><td>불확실성(Uncertainty) 모델링 가능, 노이즈 강인성 우수</td></tr>
<tr><td><strong>단점</strong></td><td>통계적 엄밀성 부족, 아웃라이어 취약</td><td>고차원 벡터에는 적용 어려움, 메모리 사용량 증가 (<span class="math math-inline">\alpha</span> 벡터 저장)</td></tr>
<tr><td><strong>대표 사례</strong></td><td>ConceptFusion , Feature 3DGS</td><td>SemanticFusion , SLIM-VDB</td></tr>
</tbody></table>
<h3>4.3  뉴럴 융합 (Neural Fusion)</h3>
<p>최근 연구에서는 단순한 수식 대신, 신경망을 이용하여 융합하는 방식이 제안되고 있다.</p>
<ul>
<li><strong>GRU/LSTM 기반:</strong> 3D 포인트의 특징을 RNN의 은닉 상태(Hidden State)로 간주하고, 새로운 뷰의 특징이 들어올 때마다 게이트(Gate)를 통해 업데이트한다. 이는 시간적인 문맥(Temporal Context)을 학습할 수 있어, 일시적인 오인식을 효과적으로 무시할 수 있다.</li>
<li><strong>어텐션(Attention) 기반:</strong> 트랜스포머(Transformer)의 어텐션 메커니즘을 사용하여, 현재 3D 점에 투영된 여러 뷰의 특징 중 가장 ‘중요한(Relevant)’ 뷰에 가중치를 동적으로 부여한다.</li>
</ul>
<h2>5.  최신 트렌드: 뉴럴 필드(Neural Fields)와 3D Gaussian Splatting</h2>
<p>명시적인 포인트나 복셀 대신, 신경망 가중치나 3D Gaussian의 집합으로 장면을 표현하는 <strong>암시적(Implicit) 또는 반-명시적(Semi-explicit) 표현</strong> 방식이 리프팅의 새로운 표준으로 자리 잡고 있다.</p>
<h3>5.1  LERF (Language Embedded Radiance Fields)</h3>
<p>LERF는 NeRF(Neural Radiance Fields)에 언어 임베딩 필드(Language Field)를 통합한 선구적인 연구이다.</p>
<ul>
<li><strong>작동 원리:</strong> LERF는 명시적인 역투영을 수행하지 않는다. 대신 <strong>볼륨 렌더링(Volume Rendering)</strong> 과정을 통해 2D 특징과 3D 필드를 일치시킨다. 3D 공간상의 점 <span class="math math-inline">(x, y, z)</span>에 대해 밀도(<span class="math math-inline">\sigma</span>)와 색상(<span class="math math-inline">c</span>)뿐만 아니라 언어 임베딩 벡터(<span class="math math-inline">\mathbf{f}_{lang}</span>)를 출력하는 MLP를 학습시킨다.</li>
</ul>
<p>랜더링된 픽셀의 특징 <span class="math math-inline">\hat{\mathbf{F}}</span>는 광선을 따라 샘플링된 점들의 특징을 가중 합하여 계산된다:<br />
<span class="math math-display">
  \hat{\mathbf{F}}(\mathbf{r}) = \sum_{i=1}^{N} T_i \alpha_i \mathbf{f}_{lang}(\mathbf{r}(t_i))
</span><br />
이 <span class="math math-inline">\hat{\mathbf{F}}</span>가 CLIP 이미지 인코더가 추출한 2D 특징 맵과 유사해지도록 손실 함수를 최소화한다.</p>
<ul>
<li><strong>다중 스케일 감독 (Multi-scale Supervision):</strong> CLIP 특징은 이미지의 해상도(Crop Size)에 따라 민감하게 변한다. LERF는 이를 해결하기 위해 이미지 피라미드를 구축하고, 다양한 스케일의 CLIP 특징을 동시에 학습하여, “신발“과 같은 작은 객체부터 “거실“과 같은 큰 영역까지 계층적으로 검색할 수 있게 한다.</li>
</ul>
<h3>5.2  Feature 3DGS &amp; LangSplat: 속도와 정확도의 혁신</h3>
<p>NeRF 기반 방식은 렌더링 속도가 느리다는 단점이 있다. 이를 극복하기 위해 **3D Gaussian Splatting (3DGS)**을 활용한 리프팅 기술이 등장했다.</p>
<ul>
<li><strong>3DGS의 특징 렌더링 확장:</strong></li>
</ul>
<p>3DGS는 각 Gaussian을 2D 화면에 ’스플래팅(Splatting)’하여 이미지를 만든다. Feature 3DGS는 각 Gaussian <span class="math math-inline">\mathcal{G}_i</span>에 RGB 색상 외에 특징 벡터 <span class="math math-inline">\mathbf{f}_i</span>를 추가 속성으로 부여한다. 렌더링 과정은 색상 렌더링과 동일한 <strong>알파 블렌딩(Alpha Blending)</strong> 공식을 따른다:<br />
<span class="math math-display">
  \mathbf{F}_{pixel} = \sum_{i \in \mathcal{N}} \mathbf{f}_i \alpha_i \prod_{j=1}^{i-1} (1 - \alpha_j)
</span><br />
이 방식은 미분 가능하므로(Differentiable), 2D 특징 맵과의 차이를 역전파하여 각 Gaussian의 특징 벡터 <span class="math math-inline">\mathbf{f}_i</span>를 직접 최적화할 수 있다.</p>
<ul>
<li>LangSplat의 오토인코더 전략 : 수백만 개의 Gaussian 각각에 512차원 CLIP 벡터를 저장하면 GPU 메모리가 부족해진다. LangSplat은 **장면별 오토인코더(Scene-wise Autoencoder)**를 도입하여 이 문제를 해결했다.</li>
</ul>
<ol>
<li>CLIP 특징을 저차원 잠재 벡터(Latent Vector, 예: 3차원)로 압축한다.</li>
<li>3D Gaussian에는 이 저차원 벡터를 저장하고 학습한다.</li>
<li>렌더링 시에는 저차원 특징 맵을 렌더링한 후, 디코더(Decoder)를 통해 다시 고차원 CLIP 특징으로 복원한다. 이 전략은 메모리 사용량을 획득적으로 줄이면서도(LERF 대비 199배 빠른 속도), 명시적인 포인트 구조 덕분에 객체 수준의 편집을 가능하게 한다.</li>
</ol>
<h3>5.3  의미론적 앨리어싱(Aliasing) 문제와 해결</h3>
<p>3DGS나 NeRF와 같은 연속적 표현(Continuous Representation)에 불연속적인 의미론적 라벨(Discrete Labels)을 학습시킬 때, <strong>앨리어싱(Aliasing)</strong> 문제가 발생한다. 객체 경계면에서 색상은 부드럽게 변해도 되지만, 의미론적 라벨(“자동차” vs “도로”)은 급격하게 변해야 하기 때문이다. 고주파 신호인 의미론적 경계를 저주파 필드로 표현하려 할 때 아티팩트가 발생한다. 이를 해결하기 위해 <strong>멀티 스케일 샘플링</strong>이나 <strong>안티 앨리어싱 래스터라이제이션(Anti-aliased Rasterization)</strong> 기법이 적용되며, 최근에는 기하학적 특징(Normal, Depth)을 가이드로 사용하여 의미론적 필드의 경계를 날카롭게 유지하는 연구가 진행되고 있다.</p>
<h2>6.  결론 및 제언</h2>
<p>2D-to-3D 리프팅은 2D AI 모델의 강력한 인식 능력을 3D 물리 공간으로 전이시키는 핵심 기술이다.</p>
<ul>
<li><strong>기하학적 투영</strong>은 핀홀 카메라 모델과 레이 캐스팅을 통해 2D와 3D를 연결하는 기본 토대이다.</li>
<li>**특징 융합(Fusion)**은 베이지안 추론과 신뢰도 기반 가중치를 통해 2D 데이터의 불확실성을 3D 일관성으로 변환한다.</li>
<li><strong>표현(Representation)의 진화</strong>는 포인트/복셀과 같은 명시적 구조에서 NeRF/3DGS와 같은 뉴럴 필드로 이동하고 있으며, 특히 <strong>LangSplat</strong>과 같은 하이브리드 접근법은 실시간성, 메모리 효율성, 정확도라는 세 마리 토끼를 동시에 잡는 방향으로 발전하고 있다.</li>
</ul>
<p>향후 연구는 정적 장면을 넘어 동적 객체(Dynamic Objects)가 존재하는 환경에서의 리프팅, 그리고 생성형 모델(Generative Models)과 결합하여 관측되지 않은 뒷면(Occluded Area)의 의미론적 정보를 상상하여 채워 넣는 <strong>3D 시맨틱 인페인팅(Inpainting)</strong> 분야로 확장될 것이다. 독자들은 본 장에서 다룬 수학적 원리와 알고리즘을 바탕으로, 자신의 애플리케이션(로봇 네비게이션, AR, 디지털 트윈)에 최적화된 리프팅 파이프라인을 설계할 수 있을 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Deep Learning-Based Vision Systems for Robot Semantic Navigation, https://www.mdpi.com/2227-7080/12/9/157</li>
<li>Lifting Foundation Masks for Label-Free Semantic Scene Completion, https://arxiv.org/html/2407.03425v1</li>
<li>SSR-2D: Semantic 3D Scene Reconstruction from 2D Images - arXiv, https://arxiv.org/html/2302.03640v4</li>
<li>PointCLIP: Point Cloud Understanding by CLIP - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_PointCLIP_Point_Cloud_Understanding_by_CLIP_CVPR_2022_paper.pdf</li>
<li>Few-Shot 3D Keypoint Detection with Back-Projected 2D Features, https://repository.kaust.edu.sa/bitstreams/a03e9546-25a7-426e-a489-675f80024fa4/download</li>
<li>Multi-view Consistent 3D Panoptic Scene Understanding, https://ojs.aaai.org/index.php/AAAI/article/view/32598/34753</li>
<li>Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D, https://openaccess.thecvf.com/content/CVPR2024/papers/T_Lift3D_Zero-Shot_Lifting_of_Any_2D_Vision_Model_to_3D_CVPR_2024_paper.pdf</li>
<li>LangSplat: 3D Language Gaussian Splatting - IEEE Xplore, https://ieeexplore.ieee.org/iel8/10654794/10654797/10655933.pdf</li>
<li>Language Embedded 3D Gaussians for Open-Vocabulary Scene …, https://openaccess.thecvf.com/content/CVPR2024/papers/Shi_Language_Embedded_3D_Gaussians_for_Open-Vocabulary_Scene_Understanding_CVPR_2024_paper.pdf</li>
<li>Multi-Scale 3D Gaussian Splatting for Anti-Aliased Rendering, https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_Multi-Scale_3D_Gaussian_Splatting_for_Anti-Aliased_Rendering_CVPR_2024_paper.pdf</li>
<li>Appearance-Semantic Joint Gaussian Representation for 3D … - arXiv, https://arxiv.org/html/2411.19235v2</li>
<li>Computer Vision in Robotics: A Guide to Smarter Automation, https://prudentpartners.in/computer-vision-in-robotics/</li>
<li>Voxel Grid Perception in 3D Scene Analysis - Emergent Mind, https://www.emergentmind.com/topics/voxel-grid-perception</li>
<li>47 3D Motion and Its 2D Projection - Foundations of Computer Vision, https://visionbook.mit.edu/2d_motion_from_3d.html</li>
<li>Camera Calibration and 3D Reconstruction - OpenCV Documentation, https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html</li>
<li>3D Gaussian Splatting for Real Time Radiance Field Rendering …, https://trepo.tuni.fi/bitstream/10024/157725/2/GunesUlas.pdf</li>
<li>Computing the Pixel Coordinates of a 3D Point - Scratchapixel, https://www.scratchapixel.com/lessons/3d-basic-rendering/computing-pixel-coordinates-of-3d-point/mathematics-computing-2d-coordinates-of-3d-points.html</li>
<li>What are Intrinsic and Extrinsic Camera Parameters in Computer …, https://towardsdatascience.com/what-are-intrinsic-and-extrinsic-camera-parameters-in-computer-vision-7071b72fb8ec/</li>
<li>Depth map to 3D point cloud with OpenCV ? : r/computervision, https://www.reddit.com/r/computervision/comments/ln5enw/depth_map_to_3d_point_cloud_with_opencv/</li>
<li>Back projecting a 2D pixel from an image to its corresponding 3D …, https://math.stackexchange.com/questions/4382437/back-projecting-a-2d-pixel-from-an-image-to-its-corresponding-3d-point</li>
<li>Back-projecting a 2d point to a ray edit - OpenCV Q&amp;A Forum, https://answers.opencv.org/question/117354/back-projecting-a-2d-point-to-a-ray/</li>
<li>Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D, https://cseweb.ucsd.edu/~ravir/mukundcvprarxiv.pdf</li>
<li>OpenUrban3D: Annotation-Free Open-Vocabulary Semantic … - arXiv, https://arxiv.org/html/2509.10842v1</li>
<li>Ray casting - Wikipedia, https://en.wikipedia.org/wiki/Ray_casting</li>
<li>Voxelisation Algorithms and Data Structures: A Review - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC8707769/</li>
<li>A Fast Voxel Traversal Algorithm for Ray Tracing, http://www.cse.yorku.ca/~amana/research/grid.pdf</li>
<li>This Tiny Algorithm Can Render BILLIONS of Voxels in Real Time, https://www.youtube.com/watch?v=ztkh1r1ioZo</li>
<li>Panoptic Lifting for 3D Scene Understanding with Neural Fields, https://nihalsid.github.io/panoptic-lifting/static/PanopticLifting.pdf</li>
<li>ConceptFusion: Open-set Multimodal 3D Mapping - ResearchGate, https://www.researchgate.net/publication/368507767_ConceptFusion_Open-set_Multimodal_3D_Mapping</li>
<li>Lifting by Gaussians: A Simple Fast and Flexible Method for 3D …, https://openaccess.thecvf.com/content/WACV2025/papers/Chacko_Lifting_by_Gaussians_A_Simple_Fast_and_Flexible_Method_for_WACV_2025_paper.pdf</li>
<li>ConceptFusion: Open-set Multimodal 3D Mapping, https://concept-fusion.github.io/</li>
<li>2023-ConceptFusion | PDF | Image Segmentation - Scribd, https://www.scribd.com/document/982340016/2023-ConceptFusion</li>
<li>Probabilistic Projective Association and Semantic Guided …, https://cg.cs.tsinghua.edu.cn/papers/ICRA-2019-densemapping.pdf</li>
<li>3d object detection by feature aggregation using point cloud …, https://www.researchgate.net/publication/343401425_3D_OBJECT_DETECTION_BY_FEATURE_AGGREGATION_USING_POINT_CLOUD_INFORMATION_FOR_FACTORY_OF_THE_FUTURE</li>
<li>Bayesian Spatial Kernel Smoothing for Scalable Dense Semantic …, https://robots.engin.umich.edu/publications/ganlu-2020a.pdf</li>
<li>Dynamic Semantic Occupancy Mapping Using 3D Scene Flow and …, https://ieeexplore.ieee.org/iel7/6287639/9668973/09882042.pdf</li>
<li>Dynamic Association of Semantics and Parameter Estimates … - arXiv, https://arxiv.org/html/2601.09158v1</li>
<li>(PDF) SLIM-VDB: A Real-Time 3D Probabilistic Semantic Mapping …, https://www.researchgate.net/publication/398720759_SLIM-VDB_A_Real-Time_3D_Probabilistic_Semantic_Mapping_Framework</li>
<li>Supercharging 3D Gaussian Splatting to Enable Distilled Feature …, https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Feature_3DGS_Supercharging_3D_Gaussian_Splatting_to_Enable_Distilled_Feature_CVPR_2024_paper.pdf</li>
<li>Point Cloud Generation From Multiple Angles of Voxel Grids, https://scispace.com/pdf/point-cloud-generation-from-multiple-angles-of-voxel-grids-4v2r1egz30.pdf</li>
<li>VoxFormer: Sparse Voxel Transformer for Camera-based 3D …, https://3dcompat-dataset.org/workshop/C3DV23/papers/VoxFormer_CVPR.pdf</li>
<li>LERF: Language Embedded Radiance Fields - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2023/papers/Kerr_LERF_Language_Embedded_Radiance_Fields_ICCV_2023_paper.pdf</li>
<li>LERF - nerfstudio, https://docs.nerf.studio/nerfology/methods/lerf.html</li>
<li>Supercharging 3D Gaussian Splatting to Enable Distilled Feature …, https://www.researchgate.net/publication/384173383_Feature_3DGS_Supercharging_3D_Gaussian_Splatting_to_Enable_Distilled_Feature_Fields</li>
<li>LangSplat: 3D Language Gaussian Splatting - alphaXiv, https://www.alphaxiv.org/overview/2312.16084v2</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>