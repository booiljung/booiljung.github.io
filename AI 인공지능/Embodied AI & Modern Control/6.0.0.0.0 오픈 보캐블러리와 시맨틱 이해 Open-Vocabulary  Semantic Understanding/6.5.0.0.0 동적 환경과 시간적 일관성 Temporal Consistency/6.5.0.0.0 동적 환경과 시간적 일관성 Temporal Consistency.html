<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:6.5 동적 환경과 시간적 일관성 (Temporal Consistency)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>6.5 동적 환경과 시간적 일관성 (Temporal Consistency)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 6. 오픈 보캐블러리와 시맨틱 이해 (Open-Vocabulary & Semantic Understanding)</a> / <a href="index.html">6.5 동적 환경과 시간적 일관성 (Temporal Consistency)</a> / <span>6.5 동적 환경과 시간적 일관성 (Temporal Consistency)</span></nav>
                </div>
            </header>
            <article>
                <h1>6.5 동적 환경과 시간적 일관성 (Temporal Consistency)</h1>
<h2>1.  서론: 정적 가정을 넘어선 동적 세계의 본질적 난제</h2>
<p>로봇 공학과 컴퓨터 비전의 역사에서 SLAM(Simultaneous Localization and Mapping)과 3D 재구성(Reconstruction) 기술은 오랜 기간 동안 ’정적 세계(Static World)’라는 강력하고도 편리한 가정 위에서 발전해 왔다. 이 가정 하에서 환경 내의 모든 구조물과 객체는 고정불변이며, 관측되는 기하학적 변화나 광학적 흐름(Optical Flow)은 오직 관찰자(카메라 또는 로봇)의 자아 운동(Ego-motion)에 기인한 것으로 해석된다. 이러한 접근 방식은 건물, 벽, 대형 가구와 같은 고정된 환경을 매핑하는 데에는 탁월한 성능을 보였으나, 인간이 거주하고 상호작용하는 실제 환경—문이 열리고 닫히며, 의자가 옮겨지고, 사람과 반려동물이 끊임없이 이동하는—을 표현하는 데에는 근본적인 한계를 드러낸다. 특히, 단순한 기하학적 형상 복원을 넘어 환경의 의미(Semantics)를 이해하고 추론해야 하는 <strong>개방형 어휘 3D 매핑(Open-Vocabulary 3D Mapping)</strong> 시스템에 있어, 환경의 동적 특성은 시스템의 신뢰성을 위협하는 가장 치명적인 요인으로 작용한다.</p>
<p>동적 환경에서 시간적 일관성(Temporal Consistency)이 결여될 때 발생하는 문제는 단순히 지도의 미관을 해치는 수준에 그치지 않는다. 이는 로봇의 인지(Perception), 추론(Reasoning), 그리고 행동 계획(Action Planning) 전반에 걸쳐 심각한 병리적 현상을 유발한다. 첫째, <strong>의미론적 깜빡임(Semantic Flickering)</strong> 현상이다. 단일 프레임 기반의 시각-언어 모델(Vision-Language Model, VLM)이나 CLIP과 같은 제로샷 분류기는 입력 이미지의 미세한 변화—시점(Viewpoint), 조명(Lighting), 부분적 가림(Partial Occlusion), 모션 블러(Motion Blur)—에도 민감하게 반응하여 프레임마다 서로 다른 클래스 레이블을 출력하는 경향이 있다. 예컨대, 특정 각도에서는 ’사무용 의자’로 정확히 분류되던 객체가, 카메라가 이동함에 따라 다음 프레임에서는 ‘소파’, 그 다음에는 ‘가구’, 심지어는 ’기타 물체’로 분류될 수 있다. 이러한 프레임 간 불일치가 3D 공간으로 투영될 경우, 지도 상의 해당 복셀(Voxel)이나 포인트 클라우드는 시간이 지남에 따라 그 정체성이 끊임없이 요동치게 되며, 이는 로봇이 해당 객체와 상호작용하거나 의미론적 쿼리를 수행하는 것을 불가능하게 만든다.</p>
<p>둘째, **유령 효과(Ghosting) 및 잔상(Artifacts)**의 축적이다. 전통적인 매핑 알고리즘이 이동하는 객체를 정적 배경으로 오인하여 지도에 통합할 경우, 객체가 이미 자리를 뜬 후에도 이전 위치에 그 형상이 ’유령’처럼 남아있는 현상이 발생한다. 이는 내비게이션 경로 계획에 있어 존재하지 않는 장애물을 회피하기 위해 불필요한 경로를 생성하게 하거나, 반대로 현재 위치에 실존하는 객체를 지도상에서 인식하지 못하여 충돌을 유발하는 치명적인 오류로 이어진다. 특히, 수 시간에서 수 일에 걸쳐 환경을 지속적으로 갱신해야 하는 ‘라이프롱 매핑(Lifelong Mapping)’ 시나리오에서, 물체의 재배치(Relocation)를 감지하고 구식 정보(Obsolete Data)를 능동적으로 삭제하거나 갱신하는 능력은 시스템의 생존과 직결된다.</p>
<p>셋째, **인스턴스 추적의 불연속성(Discontinuity of Instance Tracking)**과 <strong>재식별(Re-identification)의 난해함</strong>이다. 폐쇄형 집합(Closed-Set) 기반의 시스템에서는 사전에 정의된 클래스 ID를 통해 객체를 추적할 수 있지만, 개방형 어휘 시스템에서는 임의의 텍스트 쿼리에 대응해야 하므로 고정된 ID 체계를 사용하기 어렵다. 대신 고차원 임베딩 공간에서의 유사도에 의존해야 하는데, 조명 변화나 가림으로 인해 임베딩 벡터가 왜곡될 경우 동일한 객체를 새로운 객체로 오인하여 중복 생성하거나, 서로 다른 객체를 하나로 잘못 병합하는 문제가 빈번하게 발생한다.</p>
<p>본 장에서는 이러한 난제들을 극복하고, 시간의 흐름 속에서도 의미론적, 기하학적, 위상학적 일관성을 유지하는 견고한(Robust) 3D 지도를 구축하기 위한 최신 연구 동향과 기술적 방법론을 심층적으로 분석한다. 우리는 2D 이미지 공간에서의 시공간적 특징 전파(Spatiotemporal Feature Propagation) 기술부터 시작하여, 3D 공간에서의 베이지안 확률 융합(Bayesian Probabilistic Fusion), 동적 객체의 생명주기 관리(Lifecycle Management), 그리고 동적 장면 그래프(Dynamic Scene Graph)를 통한 고수준 추론에 이르기까지, 시스템의 각 계층에서 시간적 일관성이 어떻게 확보되는지 체계적으로 고찰할 것이다.</p>
<h2>2.  2D 시공간 특징 전파 및 추적 메커니즘 (Spatiotemporal Feature Propagation)</h2>
<p>3D 공간의 일관성을 확보하기 위한 첫 번째 방어선은 3D 재구성 이전에, 입력 데이터인 2D 비디오 시퀀스 단계에서부터 시간적 연속성을 강화하는 것이다. 단일 이미지를 독립적으로 처리하는 ‘Bag of Frames’ 접근 방식은 필연적으로 시간적 맥락(Temporal Context)을 무시하게 되므로, 인접 프레임 간의 상관관계를 활용하여 특징(Feature)을 전파하고 평활화(Smoothing)하는 전략이 필수적이다.</p>
<h3>2.1  광학 흐름(Optical Flow) 기반 특징 워핑(Feature Warping)과 보정</h3>
<p>비디오 시맨틱 분할(Video Semantic Segmentation, VSS) 및 개방형 어휘 비디오 처리의 핵심 기술 중 하나는 광학 흐름(Optical Flow)을 활용한 특징 워핑이다. 이 기술은 프레임 <span class="math math-inline">I_t</span>와 <span class="math math-inline">I_{t+k}</span> 사이의 픽셀 단위 움직임 벡터 필드를 추정하고, 이를 기반으로 이전 프레임의 고수준 의미론적 특징(Semantic Features)이나 분할 마스크(Segmentation Masks)를 현재 프레임으로 ’이동(Warping)’시키는 것이다.</p>
<p>전통적으로 광학 흐름은 픽셀의 밝기 불변성(Brightness Constancy) 가정에 의존했으나, 이는 조명 변화나 복잡한 텍스처에서 쉽게 실패한다. 최신 딥러닝 기반 방법론들은 이를 극복하기 위해 **쿼리 기반 흐름(Query-based Flow)**이나 **변형 가능한 어텐션(Deformable Attention)**을 도입하여, 픽셀 값이 아닌 의미론적 특징 자체의 흐름을 학습한다. 예를 들어, FlowLens와 같은 아키텍처는 광학 흐름을 명시적인 가이드로 사용하여 특징을 전파함으로써, 카메라의 물리적 시야(FoV) 밖으로 벗어난 영역이나 일시적으로 가려진 영역에 대한 기억(Memory)을 유지하고, 다시 관측되었을 때 일관성을 회복하도록 돕는다.</p>
<p>개방형 어휘 매핑의 맥락에서, CLIP이나 SAM(Segment Anything Model)과 같은 대형 파운데이션 모델(Foundation Models)은 매우 높은 계산 비용을 요구한다. 모든 프레임에 대해 이러한 무거운 인코더를 실행하는 것은 실시간 시스템에서 불가능에 가깝다. 따라서 키프레임(Keyframe)에서만 고비용의 인코딩을 수행하고, 중간 프레임들은 광학 흐름을 통해 특징을 전파함으로써 계산 효율성과 시간적 일관성을 동시에 달성하는 전략이 주로 사용된다. FlowVid와 같은 연구는 이러한 흐름 기반 전파가 불완전한 흐름 추정(Imperfect Flow Estimation)으로 인해 발생할 수 있는 오류를 ’공간적 조건(Spatial Conditions)’과 결합하여 보정하는 방법을 제안한다.</p>
<p><strong>수식적 모델링과 오차 보정:</strong></p>
<p>프레임 <span class="math math-inline">t</span>에서의 특징 맵을 <span class="math math-inline">F_t</span>, <span class="math math-inline">t</span>에서 <span class="math math-inline">t+k</span>로의 광학 흐름을 <span class="math math-inline">\mathcal{W}*{t \to t+k}</span>라고 할 때, 전파된 특징 <span class="math math-inline">\hat{F}*{t+k}</span>는 다음과 같이 표현될 수 있다.<br />
<span class="math math-display">
\hat{F}_{t+k} = \text{Warp}(F_t, \mathcal{W}_{t \to t+k})
</span><br />
이 과정에서 발생하는 필연적인 오차—객체가 가려짐(Occlusion)이나 드러남(Disocclusion)으로 인해 대응점이 사라지는 현상—를 보정하기 위해, 신뢰도 맵(Confidence Map) <span class="math math-inline">M</span>을 도입하거나, 현재 프레임에서 추출된 잔차(Residual) 특징을 합성하는 방식이 사용된다. 의 연구에서는 OFF-ViNet과 같이 워핑된 특징과 현재 프레임의 특징을 3D 컨볼루션이나 순환 신경망(RNN)으로 결합하여 미래의 픽셀 상태를 예측하고 시간적 불연속성을 채우는 기법을 제안한다. 또한, FlowLens는 ’Clip-Recurrent Transformer’를 통해 과거의 흐름 정보를 잠재 공간(Latent Space)에 축적하여, 단순한 프레임 간 전파를 넘어선 장기적인 시공간적 일관성을 유지한다.</p>
<h3>2.2  분리된 비디오 분할(Decoupled Video Segmentation)과 DEVA</h3>
<p>개방형 어휘 환경에서 시간적 일관성을 유지하는 획기적인 접근법 중 하나는 <strong>분리된 비디오 분할(Decoupled Video Segmentation)</strong>, 대표적으로 <strong>DEVA(Decoupled Video Association)</strong> 프레임워크이다. 기존의 엔드-투-엔드(End-to-End) 비디오 모델들이 특정 데이터셋의 클래스에 과적합(Overfitting)되어 개방형 어휘 확장이 어려웠던 반면, DEVA는 ’무엇을 분할할 것인가(Segmentation/Detection)’와 ’어떻게 추적할 것인가(Propagation/Tracking)’를 구조적으로 완벽히 분리함으로써 이 문제를 해결한다.</p>
<ol>
<li><strong>이미지 레벨 분할 (Task-Specific Image Segmentation):</strong> Grounding DINO, SAM, 또는 범용 이미지 분할 모델을 사용하여 각 프레임에서 독립적으로 객체를 탐지하고 마스크를 생성한다. 이 단계에서는 시간적 정보가 사용되지 않으므로, “빨간 모자를 쓴 사람“이나 “특정 브랜드의 캔“과 같은 복잡한 텍스트 쿼리에 대해 최신 이미지 모델의 높은 정확도를 그대로 활용할 수 있다. 이는 도메인 특화 데이터 없이도 제로샷 성능을 보장하는 핵심이다.</li>
<li><strong>클래스 불가지론적 시간 전파 (Class-Agnostic Temporal Propagation):</strong> XMem과 같은 범용 비디오 객체 분할(VOS) 모델을 사용하여, 앞서 생성된 마스크를 다음 프레임으로 전파한다. 이 전파 모듈은 객체의 클래스가 무엇인지(사람인지, 차인지) 전혀 신경 쓰지 않으며, 오직 픽셀 간의 시공간적 대응 관계와 객체의 영속성(Object Permanence)만을 학습한다.</li>
</ol>
<p>DEVA의 기술적 핵심은 <strong>반-온라인 융합(Semi-online Fusion)</strong> 메커니즘에 있다. 프레임 <span class="math math-inline">t</span>에서 이미지 모델이 제안한 새로운 검출 결과 집합 <span class="math math-inline">\mathcal{D}_t</span>와, 이전 프레임에서 시간 전파 모듈을 통해 예측된 마스크 집합 <span class="math math-inline">\mathcal{M}^{prop}_t</span> 사이의 양방향 매칭(Bi-matching)을 수행하여 일관성을 확보한다.</p>
<ul>
<li><strong>ID 유지 및 계승:</strong> <span class="math math-inline">\mathcal{D}_t</span>의 검출 결과와 <span class="math math-inline">\mathcal{M}^{prop}_t</span>의 전파 마스크 간의 공간적 중첩(IoU)이 높은 쌍은 동일 객체로 간주하여 기존의 트랙 ID를 계승한다. 이를 통해 VLM이 일시적으로 레이블을 잘못 분류하더라도, 추적 모듈에 의해 올바른 ID와 속성이 유지된다.</li>
<li><strong>전파 전용 모드 (Propagation-only Mode):</strong> 전파된 마스크가 현재 이미지 검출 결과와 매칭되지 않더라도, 전파 모델의 신뢰도가 높다면 이를 ’이미지 모델의 일시적 검출 실패(False Negative)’로 간주하여 추적을 강제로 유지한다. 이는 가림(Occlusion)이나 모션 블러 상황에서 객체가 지도에서 사라지는 것을 방지한다.</li>
<li><strong>신규 객체 초기화:</strong> 이미지 검출 결과가 어떤 전파된 마스크와도 매칭되지 않고, 신뢰도가 임계값 이상이라면, 이는 ’새로운 객체의 등장’으로 간주하여 새로운 트랙 ID를 부여하고 메모리 뱅크에 등록한다.</li>
</ul>
<p>또한, DEVA는 장기 추적 시 발생할 수 있는 메모리 누수와 오검출 축적을 방지하기 위해 ’최대 미검출 카운트(max_missed_detection_count)’와 같은 파라미터를 통해 메모리 뱅크를 관리하며, XMem의 장기 메모리(Long-term Memory) 기능을 활용하여 시야에서 완전히 사라졌다가 다시 나타난 객체도 재식별(Re-identification)할 수 있는 능력을 갖추고 있다.</p>
<h3>2.3  전역적 모션 일관성과 정준 공간: OmniMotion</h3>
<p>기존의 광학 흐름이나 추적 기법들이 인접한 프레임 간의 짧은 시간 윈도우 내에서만 작동하거나 가림 현상에 취약했던 반면, <strong>OmniMotion</strong>은 비디오 전체를 아우르는 **전역적 모션 표현(Global Motion Representation)**을 제안함으로써 시간적 일관성의 새로운 차원을 열었다.</p>
<p>OmniMotion은 비디오 전체를 하나의 **준-3D 정준 볼륨(Quasi-3D Canonical Volume, <span class="math math-inline">\mathbf{G}</span>)**으로 모델링한다. 이 정준 공간은 시간 축이 제거된, 객체들의 ’원형(Prototype)’이 존재하는 공간으로 이해할 수 있다. 시스템은 각 비디오 프레임의 로컬 3D 좌표계 <span class="math math-inline">L_i</span>와 정준 공간 <span class="math math-inline">\mathbf{G}</span> 사이의 전단사(Bijective) 매핑 <span class="math math-inline">\mathcal{T}_i: L_i \leftrightarrow \mathbf{G}</span>를 학습한다.</p>
<ul>
<li><strong>가역적 매핑과 추적:</strong> 임의의 프레임 <span class="math math-inline">i</span>에 있는 3D 점 <span class="math math-inline">x_i</span>는 매핑 <span class="math math-inline">\mathcal{T}_i</span>를 통해 정준 공간의 좌표 <span class="math math-inline">u = \mathcal{T}_i(x_i)</span>로 변환된다. 이 <span class="math math-inline">u</span>는 시간 불변(Time-independent) 좌표이므로, 이를 다시 다른 프레임 <span class="math math-inline">j</span>의 역매핑 <span class="math math-inline">\mathcal{T}_j^{-1}</span>을 통해 투영하면 해당 프레임에서의 위치 <span class="math math-inline">x_j = \mathcal{T}_j^{-1}(u)</span>를 정확하게 찾아낼 수 있다.</li>
<li><strong>가림 처리 (Occlusion Handling):</strong> OmniMotion은 정준 공간에 밀도(Density) 정보를 함께 저장한다. 특정 픽셀을 정준 공간으로 매핑할 때, 경로 상의 누적 투과율(Transmittance)을 계산함으로써 해당 지점이 현재 시점에서 가려졌는지 여부를 명시적으로 판단할 수 있다. 이는 기존 방법들이 가려진 객체의 궤적을 잃어버리는 문제를 근본적으로 해결한다.</li>
</ul>
<p>OmniMotion은 개방형 어휘 매핑 시스템에 직접적으로 의미론적 레이블을 제공하지는 않지만, 2D 픽셀 단위의 의미 정보를 3D 공간이나 전체 비디오 시퀀스로 전파할 때 기하학적으로 가장 강력한 **경성 제약 조건(Hard Constraints)**을 제공한다. 즉, OmniMotion을 통해 확보된 전역적으로 일관된 픽셀 궤적(Trajectory)을 따라 CLIP 특징을 평균화하거나 집계(Aggregation)하면, 노이즈가 제거된 매우 견고하고 안정적인 의미론적 표현을 얻을 수 있다. 이는 “모든 것을, 모든 곳에서, 한 번에(Tracking Everything Everywhere All at Once)” 추적한다는 철학을 바탕으로, 의미론적 일관성을 기하학적 일관성 위에 구축하는 전략이다.</p>
<h3>2.4  메모리 유도 트래킹 (Memory-Induced Tracking)과 OV2Seg</h3>
<p>시간적 일관성을 유지하면서도 계산 효율성을 극대화하기 위한 또 다른 접근법으로 <strong>OV2Seg</strong>와 같은 메모리 유도 트랜스포머 아키텍처가 있다. OV2Seg는 복잡한 ’제안-축소-연관(Propose-Reduce-Association)’의 다단계 파이프라인을 단순화하고, **메모리 쿼리(Memory Queries)**라는 개념을 도입하여 장기적인 문맥을 유지한다.</p>
<ul>
<li>
<p><strong>메모리 쿼리 업데이트:</strong> 프레임 <span class="math math-inline">t-1</span>에서의 메모리 쿼리 <span class="math math-inline">Q^{M}*{t-1}</span>은 현재 프레임 <span class="math math-inline">t</span>의 객체 중심 쿼리 <span class="math math-inline">Q^{\*}*{t}</span>와 결합되어 업데이트된다. 이때 업데이트 함수는 다음과 같은 모멘텀(Momentum) 방식을 따른다:<br />
<span class="math math-display">
Q^{M}_{t} = \alpha \cdot s_{obj} \cdot Q^{*}_{t} + (1 - \alpha \cdot s_{obj}) \cdot Q^{M}_{t-1}
</span><br />
여기서 <span class="math math-inline">\alpha</span>는 업데이트 비율을 제어하는 인자(Factor)이고, <span class="math math-inline">s_{obj}</span>는 객체 점수이다. 와 의 실험 결과에 따르면, <span class="math math-inline">\alpha</span> 값을 0.3에서 0.8 사이로 설정했을 때 시스템은 일시적인 인식 오류나 노이즈에 영향을 받지 않으면서도 객체의 외형 변화를 부드럽게 반영하여 가장 안정적인 성능(mAP)을 보였다. 반면 <span class="math math-inline">\alpha=0</span>인 경우(메모리 업데이트 없음) 성능이 급격히 하락하여, 시간적 특징의 누적(Aggregation)이 필수적임을 입증했다.</p>
</li>
<li>
<p><strong>분류 강건성:</strong> 이렇게 누적된 메모리 쿼리는 단일 프레임에서 얻은 특징보다 훨씬 풍부한 정보를 담고 있으므로, CLIP 텍스트 인코더와의 코사인 유사도 매칭 시 훨씬 정확하고 안정적인 분류 결과를 내놓는다. 가림이나 블러로 인해 현재 프레임의 정보가 불확실하더라도, 과거의 정보가 메모리 쿼리에 남아 있어 올바른 분류를 유도한다.</p>
</li>
</ul>
<table><thead><tr><th><strong>특성</strong></th><th><strong>광학 흐름 (Optical Flow)</strong></th><th><strong>DEVA (Decoupled Seg.)</strong></th><th><strong>OmniMotion</strong></th><th><strong>OV2Seg (Memory Query)</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 원리</strong></td><td>픽셀 변위 기반 특징 워핑</td><td>분할과 추적의 모듈화 및 재결합</td><td>전역적 정준 볼륨 매핑</td><td>메모리 쿼리의 모멘텀 업데이트</td></tr>
<tr><td><strong>시간적 범위</strong></td><td>단기 (인접 프레임)</td><td>중장기 (메모리 뱅크 활용)</td><td>전역 (비디오 전체)</td><td>장기 (쿼리 누적)</td></tr>
<tr><td><strong>강점</strong></td><td>계산 속도 빠름, 국소 모션</td><td>개방형 모델 결합 용이, ID 강건성</td><td>가림 해결 탁월, 완벽한 일관성</td><td>엔드-투-엔드 학습, 분류 정확도 향상</td></tr>
<tr><td><strong>한계</strong></td><td>장기 추적 시 드리프트 누적</td><td>VOS 모델 성능 의존</td><td>높은 계산 비용, 최적화 시간</td><td>학습 데이터 의존성</td></tr>
</tbody></table>
<h2>3.  확률적 융합과 3D 의미론적 일관성 (Probabilistic 3D Fusion)</h2>
<p>2D 단계에서의 처리가 완료되면, 수집된 의미 정보(Semantic Information)를 3D 공간—복셀(Voxel), 포인트 클라우드(Point Cloud), 서펠(Surfel), 또는 가우시안 스플랫(Gaussian Splat)—에 통합해야 한다. 이때 가장 큰 문제는 다수의 관측이 서로 상충할 때 어떻게 **진실(Ground Truth)**을 추정하고, 시간적 노이즈를 걸러낼 것인가이다. 베이지안 융합(Bayesian Fusion)과 불확실성 모델링(Uncertainty Modeling)은 이러한 문제를 수학적으로 다루는 표준적인 프레임워크를 제공한다.</p>
<h3>3.1  베이지안 의미론적 갱신 (Bayesian Semantic Update)</h3>
<p>3D 공간의 각 요소(예: 복셀 <span class="math math-inline">v</span>)는 특정 의미 클래스 <span class="math math-inline">c_i</span>에 속할 확률 분포 <span class="math math-inline">P(L_v = c_i | Z_{1:t})</span>를 가진다. 여기서 <span class="math math-inline">Z_{1:t}</span>는 시간 <span class="math math-inline">1</span>부터 <span class="math math-inline">t</span>까지의 모든 관측값 시퀀스이다. 새로운 관측 <span class="math math-inline">z_t</span>가 들어왔을 때, 사후 확률(Posterior)을 갱신하는 과정은 베이즈 정리에 의해 재귀적(Recursive)으로 수행된다.</p>
<p>계산의 효율성과 수치적 안정성을 위해, 확률 대신 <strong>로그 오즈(Log-Odds)</strong> 표기법을 주로 사용한다. 클래스 <span class="math math-inline">c</span>에 대한 로그 오즈 <span class="math math-inline">l_{t}(c)</span>는 다음과 같이 정의된다:<br />
<span class="math math-display">
l_t(c) = \log \frac{P(L_v = c | Z_{1:t})}{1 - P(L_v = c | Z_{1:t})}
</span><br />
베이지안 업데이트 식은 곱셈 연산을 덧셈 연산으로 변환하여 다음과 같이 단순화된다 :<br />
<span class="math math-display">
l_t(c) = l_{t-1}(c) + \log \frac{P(c | z_t)}{1 - P(c | z_t)} - \log \frac{P(c)}{1 - P(c)}
</span><br />
여기서 <span class="math math-inline">P(c | z_t)</span>는 현재 프레임의 VLM(예: CLIP, SegFormer)이 출력한 확률이며, <span class="math math-inline">P(c)</span>는 사전 확률(Prior)이다. 이 방식은 일시적인 오분류(Outlier)가 발생하더라도, 누적된 과거의 관측값들이 ‘관성(Inertia)’ 역할을 하여 급격한 레이블 변화를 막아준다. 예를 들어, 100번의 프레임 동안 ’책상’으로 관측된 복셀이 한 번의 노이즈로 인해 ’의자’로 인식되더라도, 로그 오즈 값은 여전히 ‘책상’ 쪽으로 강하게 편향되어 있어 지도 상의 레이블은 변하지 않는다. <strong>OpenFusion</strong>과 <strong>OpenFusion++</strong> 같은 시스템은 이러한 베이지안 업데이트를 TSDF(Truncated Signed Distance Function) 맵과 결합하여, 기하학적 표면의 재구성뿐만 아니라 표면의 의미론적 속성까지 실시간으로 정제한다. 특히 OpenFusion++는 동적 객체에 대해 이러한 업데이트를 선택적으로 적용하거나, 신뢰도가 낮은 관측을 필터링하는 메커니즘을 추가하여 동적 환경에서의 강건성을 높였다.</p>
<h3>3.2  다중 시점 CLIP 특징 집계 (Multi-view Feature Aggregation)</h3>
<p>개방형 어휘 매핑에서는 단순한 이산적인 클래스 레이블(예: “의자”, “책상”)뿐만 아니라, 고차원 연속 벡터인 CLIP 임베딩 자체를 3D 공간에 저장해야 한다. 이는 나중에 사용자가 “나무로 된 앤티크 의자“와 같이 구체적인 쿼리를 던졌을 때, 저장된 임베딩과의 유사도를 계산하기 위함이다. 이때의 융합 전략은 단순한 평균화(Averaging)를 넘어선다.</p>
<p><strong>ConceptFusion</strong>과 **OVO (Open-Vocabulary Online)**는 다중 시점에서 얻어진 CLIP 특징 벡터들을 효과적으로 압축하고 융합하는 전략을 제안한다.</p>
<ul>
<li><strong>코사인 유사도 기반 가중치 (Cosine Similarity Weighting):</strong> 현재 관측된 특징 벡터가 기존에 저장된 벡터와 유사도가 높을수록(즉, 의미론적 일관성이 있을수록) 가중치를 높여 업데이트한다. 이는 완전히 다른 의미를 가진 노이즈(예: 지나가는 사람에 의해 가려진 경우)가 융합되는 것을 방지한다.</li>
<li><strong>신뢰도 기반 융합 (Confidence-based Fusion):</strong> 객체와의 거리, 관측 각도, 마스크의 품질 등을 고려하여 ’신뢰도 점수’를 산출하고, 이를 융합 가중치로 사용한다. 정면에서 가까이 본 의자의 특징이 측면에서 멀리서 보거나 부분적으로 가려진 특징보다 더 높은 가중치를 갖는다. ConceptFusion은 픽셀 단위의 CLIP 특징을 3D 포인트로 투영할 때 이러한 기하학적 신뢰도를 반영한다.</li>
<li><strong>OVO의 CLIP Merging:</strong> OVO는 단순 평균 대신, 신경망을 사용하여 여러 시점의 디스크립터를 하나의 대표 디스크립터로 융합하는 <strong>CLIP Merging</strong> 기법을 도입했다. OVO는 각 3D 세그먼트에 대해 세 가지 종류의 2D 입력(전체 이미지, 배경을 제외한 마스크 크롭, 배경을 포함한 마스크 크롭)에서 추출한 CLIP 벡터를 융합하여, 시점 변화와 배경 잡음에 강인한 전역적 표현을 학습한다. 이는 단순히 벡터를 더하는 것보다 훨씬 풍부하고 구별력 있는(Discriminative) 표현을 생성한다.</li>
</ul>
<h3>3.3  불확실성 인식 매핑 (Uncertainty-Aware Mapping)과 Bayesian Spatial Kernel Smoothing</h3>
<p>단순히 “이것은 의자다“라고 확신하는 것보다, “이것은 의자일 확률이 높지만, 조명이 어두워서 확실하지 않다“라고 판단하는 것이 로봇에게는 훨씬 유용하다. **Bayesian Spatial Kernel Smoothing (BSKS)**이나 <strong>Evidential Deep Learning</strong>을 적용한 연구들은 3D 지도에 의미 정보와 함께 <strong>불확실성(Uncertainty)</strong> 또는 **증거(Evidence)**의 양을 명시적으로 모델링한다.</p>
<p>이러한 접근법은 <strong>Dempster-Shafer 이론</strong>이나 **디리클레 분포(Dirichlet Distribution)**를 활용하여, 데이터가 희소하거나 관측 노이즈가 심한 영역에서 모델이 “모른다(Unknown)“고 판단할 수 있게 한다. 예를 들어, 조명이 어두워 CLIP의 분류 신뢰도가 낮은 영역이나, 모션 블러가 심한 영역은 ’인식론적 불확실성(Epistemic Uncertainty)’이 높은 상태로 마킹된다. 이렇게 불확실성이 높은 데이터는 베이지안 융합 과정에서 가중치가 낮게 책정되어 지도의 기존 정보를 오염시키지 않으며, 추후 더 명확한 관측이 들어왔을 때 대체된다. 또한, 커널 기반 평활화(Kernel Smoothing)를 통해 공간적으로 인접한 영역의 정보를 참조하여 불확실성을 줄이고, 듬성듬성한(Sparse) 관측을 조밀한(Dense) 지도로 보간(Interpolate)할 때 의미론적 경계가 무너지지 않도록 보호한다.</p>
<h2>4.  동적 객체 처리와 지도 관리 전략 (Dynamic Object Management)</h2>
<p>베이지안 융합이 ’관측 노이즈’를 걸러내는 과정이라면, 동적 객체 처리는 지도의 ‘구조적 변화’—물체의 이동, 생성, 소멸—를 다루는 과정이다. 현실 세계에서는 물체가 끊임없이 이동하며, 이는 단순한 측정 오차가 아닌 물리적 상태의 전이(State Transition)이다. 라이프롱(Lifelong) 매핑 시스템은 이러한 변화를 감지하고 맵을 최신 상태로 유지해야 한다.</p>
<h3>4.1  구식 인덱스 제거 (Removal of Obsolete Indices)와 광선 투사</h3>
<p>가장 직관적이고 빈번한 문제는 ’이동한 물체’의 처리이다. 책상 위에 있던 컵이 치워졌을 때, 로봇의 지도는 컵이 더 이상 그곳에 없음을 인지하고 해당 공간을 ’빈 공간(Free Space)’으로 갱신해야 한다. <strong>DovSG (Dynamic Open-Vocabulary Scene Graphs)</strong> 시스템은 이를 위해 <strong>광선 투사(Ray-Casting) 기반의 점유 확인(Occupancy Check)</strong> 기법을 효과적으로 사용한다.</p>
<ul>
<li><strong>작동 메커니즘:</strong> 로봇이 새로운 RGB-D 프레임을 획득할 때마다, 카메라 원점에서 해당 픽셀 방향으로 3D 지도를 향해 가상의 광선을 투사한다.</li>
<li><strong>불일치 감지:</strong> 만약 지도상에 ’컵’이 있다고 기록된 복셀 위치를 광선이 통과하여, 그 뒤에 있는 ’책상 표면’이나 ’벽’에서 실제 깊이(Depth) 값이 측정된다면(즉, 측정된 깊이 <span class="math math-inline">d_{measured} &gt; d_{map}</span>), 이는 지도상의 ’컵’이 사라졌음을 의미한다.</li>
<li><strong>지도 갱신:</strong> DovSG는 이러한 모순이 발생한 복셀 인덱스를 **구식(Obsolete)**으로 식별한다. 시스템은 해당 복셀의 점유 확률(Occupancy Probability)을 급격히 낮추거나 메모리에서 즉시 제거(Pruning)한다. 반대로, 빈 공간이었던 곳에서 예상보다 짧은 거리에서 깊이가 측정된다면 새로운 물체가 나타난 것으로 간주하여 새로운 복셀을 할당하고 의미 정보를 업데이트한다. 이 과정은 “Remove Obsolete Indices” 모듈을 통해 실시간으로 수행되며, 장기적인 작업(Long-term Tasks)에서 지도가 현실과 동기화되도록 유지하는 핵심 알고리즘이다.</li>
</ul>
<h3>4.2  이중 지도 표현: DualMap</h3>
<p><strong>DualMap</strong>은 동적 환경의 효율적인 내비게이션과 매핑을 위해 지도를 두 개의 계층으로 분리하는 이중 구조 전략을 취한다.</p>
<ol>
<li><strong>구체적 지도 (Concrete Map):</strong> 로봇의 국소적(Local) 관측을 바탕으로 구축되는 고정밀 3D 복셀 또는 포인트 클라우드 맵이다. 여기에는 실시간으로 관측된 객체의 세부 형상, 위치, 의미 정보가 기록된다. DualMap은 <strong>하이브리드 분할(Hybrid Segmentation)</strong> 전략을 사용하여, 빠르고 객체 검출에 강한 <strong>YOLO</strong>와 정밀한 마스크를 생성하는 <strong>FastSAM</strong>을 결합한다. YOLO로 객체의 존재와 클래스를 빠르게 파악하고, FastSAM으로 정확한 영역을 따내어 맵에 투영함으로써 동적 변화를 즉각적으로 반영한다.</li>
<li><strong>추상적 지도 (Abstract Map):</strong> 구체적 지도를 바탕으로 생성된 위상학적(Topological) 또는 객체 중심적(Object-Centric) 지도이다. 이는 전역적인 경로 계획이나 “냉장고로 가라“와 같은 내비게이션 쿼리 처리에 사용된다. 추상적 지도는 구체적 지도보다 갱신 주기가 길지만, 전체적인 환경의 구조(방, 복도, 주요 랜드마크)를 유지한다.</li>
</ol>
<p>DualMap의 핵심 혁신은 **경량화된 객체 내부 검사(Intra-Object Check)**이다. 기존 방법들이 맵 전체에 대해 값비싼 3D 병합(Merging) 연산을 수행했던 것과 달리, DualMap은 새로 관측된 객체와 기존 지도상 객체 간의 중첩 여부와 클래스 일치 여부만을 빠르게 검사한다. 예를 들어, 사용자가 “크래커 상자 찾아줘“라고 했을 때, 로봇은 추상적 지도를 통해 과거의 위치로 이동하지만, 현장에서 구체적 지도를 갱신하여 상자가 이동했음을 감지하면 즉시 탐색 전략을 수정한다. 이는 계산 비용을 획기적으로 줄이면서도 동적 변화에 민첩하게 대응할 수 있게 한다.</p>
<h3>4.3  단기 동적 요소와 장기 정적 요소의 분리</h3>
<p>SLAM 시스템은 종종 움직이는 사람이나 차량과 같은 **단기 동적 요소(Short-term Dynamics)**를 지도의 오차 요인으로 간주하고 제거하려 한다. <strong>DS-SLAM</strong>이나 <strong>DynaSLAM</strong>과 같은 선구적 연구들은 의미론적 분할을 이용해 ‘사람’, ‘차량’ 등 움직일 가능성이 높은 객체를 마스킹하고, 해당 영역의 특징점을 위치 추정(Localization)과 매핑에서 배제한다.</p>
<p>반면, 가구 배치 변경과 같은 **장기 정적 요소(Long-term Static)**는 지도의 영구적인 변화로 받아들여 업데이트한다. 최신 개방형 어휘 매핑에서는 이러한 구분을 텍스트 프롬프트 기반으로 유연하게 설정할 수 있다. 예를 들어, 시스템에게 “사람과 애완동물은 매핑하지 말고, 가구의 위치 변화는 기록하라“는 식의 세밀한 제어가 가능하다. 에서 언급된 바와 같이, 광학 흐름이나 프레임 간 차분(Frame Difference)을 이용한 <strong>움직이는 객체 분할(Moving Object Segmentation, MOS)</strong> 기술은 이러한 단기 동적 요소를 실시간으로 탐지하여 지도에서 배제하는 데 핵심적인 역할을 한다.</p>
<h2>5.  고수준 추론과 동적 장면 그래프 (Dynamic Scene Graphs)</h2>
<p>시간적 일관성의 최상위 단계는 단순한 기하학적/의미론적 일관성을 넘어, 공간의 구조적 관계(Relationship)를 유지하고 갱신하는 것이다. **3D 장면 그래프(3D Scene Graph)**는 공간을 노드(객체, 방, 건물)와 엣지(관계: ‘위에 있다’, ‘안에 있다’ 등)로 표현하는 계층적 구조이다. 이 그래프가 시간축을 따라 진화할 때, 이를 **동적 장면 그래프(Dynamic Scene Graph)**라 부른다.</p>
<h3>5.1  정적 그래프에서 동적 그래프로: Hydra와 DovSG</h3>
<p>초기의 장면 그래프 연구인 <strong>Hydra</strong>는 실시간성을 강조하며 공간을 위상학적으로 추상화하는 데 성공했으나, 주로 정적인 구조에 초점을 맞추었다. 그러나 최신 연구인 <strong>DovSG</strong>나 <strong>Hydra-Multi</strong>는 노드(객체)가 이동하고 관계(엣지)가 변화하는 동적 상황을 모델링한다.</p>
<ul>
<li><strong>노드 속성 및 위상 갱신:</strong> 객체가 이동하면 해당 노드의 위치 속성(Centroid)이 갱신되는 것에 그치지 않고, 연결된 엣지도 재구성된다. 만약 ’컵’이 ’방 A’의 ‘책상’ 위에서 ’방 B’의 ‘싱크대’ 위로 이동하면, 그래프 상에서 ‘컵’ 노드는 ‘방 A’ 노드와의 ‘Contains’ 엣지를 끊고, ‘방 B’ 노드와 새로운 엣지를 형성한다. 또한 ’책상’과의 ‘Supported_by’ 엣지가 사라지고 ’싱크대’와의 새로운 관계가 생성된다.</li>
<li><strong>LLM 기반 시공간 추론:</strong> 동적 장면 그래프는 거대 언어 모델(LLM)과 결합하여 강력한 추론 능력을 제공한다. “내 컵이 어디 갔지?“라는 질문에 대해, LLM은 장면 그래프의 히스토리(History)를 조회하여 “컵은 원래 식탁 위에 있었으나, 10분 전에 로봇에 의해 주방 싱크대로 이동되었습니다“라고 답변할 수 있다. 이는 단순한 위치 찾기를 넘어 사건의 전후 관계(Causality)를 이해하는 수준의 시간적 일관성을 의미한다.</li>
</ul>
<h3>5.2  Hydra와 Open3DSG의 융합</h3>
<p><strong>Open3DSG</strong>와 같은 연구는 Hydra의 위상학적 구조 위에 개방형 어휘(CLIP feature)를 입히는 시도를 하고 있다. 이는 “회의실에 있는 빨간 의자“와 같은 복합적인 속성 쿼리를 가능하게 한다. 동적 환경에서 이러한 그래프를 건강하게 유지하기 위해서는, 객체의 소멸과 생성을 추적하는 **생명주기 관리(Lifecycle Management)**가 필수적이다. 일정 시간 이상 관측되지 않거나 신뢰도가 떨어진 노드는 그래프에서 <strong>가지치기(Pruning)</strong> 당하며, 이는 맵의 데이터 비대화를 막고 검색 효율성을 높인다. Open3DSG는 GNN(Graph Neural Network)을 사용하여 이러한 노드와 엣지의 특징을 정제하고, 2D 이미지에서 추출된 관계 정보를 3D 그래프로 전파(Distillation)하는 학습 과정을 거친다.</p>
<h2>6.  루프 클로저와 전역적 일관성 복원 (Loop Closure &amp; Global Consistency)</h2>
<p>시간적 일관성을 확보하는 데 있어 가장 드라마틱하고 어려운 기술적 개입은 <strong>루프 클로저(Loop Closure)</strong> 시점에 발생한다. 로봇이 긴 복도를 지나 출발점으로 돌아왔을 때, 센서의 누적된 위치 오차(Drift)로 인해 시작 지점의 맵과 현재 관측된 맵이 서로 어긋나 중복되는 현상이 발생한다. 이를 보정하는 과정에서, 기존에 생성된 3D 객체들의 중복과 의미론적 불일치를 어떻게 해결하느냐가 시스템의 성능을 좌우한다.</p>
<h3>6.1  OVO 시스템의 인스턴스 융합 (Instance Fusion)</h3>
<p><strong>OVO (Open-Vocabulary Online)</strong> 시스템은 이 문제를 해결하기 위해 정교한 <strong>인스턴스 융합(Instance Fusion)</strong> 알고리즘을 제안한다. 루프 클로저가 감지되면, OVO는 기하학적 맵(포인트 클라우드)뿐만 아니라 의미론적 객체(Instance)들의 일관성도 함께 복원한다.</p>
<ol>
<li><strong>기하학적 보정 및 전파:</strong> SLAM 백엔드(예: ORB-SLAM)에서 포즈 그래프 최적화(Pose Graph Optimization)가 수행되어 키프레임들의 궤적 <span class="math math-inline">T</span>가 수정된 <span class="math math-inline">T&#39;</span>으로 갱신되면, OVO는 즉시 연관된 모든 3D 랜드마크 포인트들을 새로운 좌표계로 변환한다 (<span class="math math-inline">P&#39; = T&#39; T^{-1} P</span>). 이 과정은 맵 전체의 형태를 바로잡는다.</li>
<li><strong>중복 인스턴스 탐지 기준:</strong> 기하학적 보정 후, 공간상에서 서로 가깝게 위치하게 된 객체 쌍(Pair)들을 전수 조사한다. OVO는 두 인스턴스를 병합할지 결정하기 위해 엄격한 기준을 적용한다 :</li>
</ol>
<ul>
<li><strong>중심점 거리 (Centroid Proximity):</strong> 두 인스턴스의 중심 거리가 <strong>1.5m (150cm)</strong> 이내여야 한다.</li>
<li><strong>의미론적 유사도 (Semantic Similarity):</strong> 두 인스턴스의 CLIP 디스크립터 간 코사인 유사도가 <strong>0.8</strong> 이상이어야 한다. 이는 물리적으로 겹치더라도 의미가 다른 객체(예: 책상과 그 옆의 의자)를 병합하지 않기 위함이다.</li>
<li><strong>기하학적 중첩 (Geometric Overlap):</strong> 한 인스턴스 포인트의 50% 이상이 다른 인스턴스 포인트와 <strong>10cm</strong> 이내에 존재해야 한다.</li>
</ul>
<ol start="3">
<li><strong>병합 및 가지치기 (Merging and Pruning):</strong> 위 조건을 모두 만족하는 두 인스턴스는 하나로 병합된다. 더 오래 관측되었거나 신뢰도가 높은 ID로 통합되며, 중복된 포인트들은 정리된다. 또한, 최적화 과정에서 키프레임이 삭제됨에 따라 연관 포인트가 모두 사라진 ’유령 인스턴스’들은 즉시 맵에서 삭제(Pruning)된다.</li>
</ol>
<p>이러한 사후 처리(Post-processing) 방식은 실시간 추적의 한계를 보완하고, 전역적인 지도 일관성(Global Consistency)을 보장하는 최후의 보루 역할을 한다. OVO의 실험 결과에 따르면, 이러한 루프 클로저 및 융합 기능이 활성화되었을 때, 3D 객체 분할의 정확도(mIoU)와 맵의 깔끔함(Cleanliness)이 비약적으로 향상됨을 보여준다.</p>
<h2>7.  결론 및 향후 전망</h2>
<p>동적 환경에서의 시간적 일관성 확보는 개방형 어휘 3D 매핑이 통제된 실험실 환경을 벗어나 복잡하고 예측 불가능한 실제 세계(Real World)로 진출하기 위한 필수 불가결한 조건이다. 본 장에서 우리는 이 문제를 해결하기 위한 다층적인 기술적 접근법들을 살펴보았다.</p>
<p><strong>표 6.5.1: 시간적 일관성 확보를 위한 기술 계층 요약</strong></p>
<table><thead><tr><th><strong>기술 계층</strong></th><th><strong>주요 난제</strong></th><th><strong>해결 방법론 및 대표 연구</strong></th><th><strong>핵심 메커니즘</strong></th></tr></thead><tbody>
<tr><td><strong>2D 특징 전파</strong></td><td>의미론적 깜빡임, ID 스위칭, 가림</td><td><strong>Optical Flow Warping</strong> (FlowLens ), <strong>DEVA</strong> , <strong>OmniMotion</strong></td><td>픽셀 변위 워핑, 분할/추적의 분리(Decoupling), 전역 정준 공간 매핑</td></tr>
<tr><td><strong>3D 확률적 융합</strong></td><td>관측 불확실성, 센서 노이즈</td><td><strong>Bayesian Update</strong> , <strong>ConceptFusion</strong> , <strong>OpenFusion</strong></td><td>로그-오즈 기반 확률 갱신, 신뢰도 가중치 CLIP 융합, 불확실성 모델링</td></tr>
<tr><td><strong>지도 관리</strong></td><td>이동 물체 잔상(Ghosting), 구조 변경</td><td><strong>DovSG</strong> , <strong>DualMap</strong></td><td>광선 투사를 통한 구식 인덱스 제거, 이중 지도(구체/추상) 운용</td></tr>
<tr><td><strong>전역 일관성</strong></td><td>루프 클로저 시 중복/불일치</td><td><strong>OVO</strong> , <strong>Scene Graphs</strong></td><td>기하/의미/위상 복합 기준 인스턴스 병합, 그래프 가지치기</td></tr>
</tbody></table>
<p>향후 연구는 **4D 시공간 매핑(Spatiotemporal Mapping)**으로 진화할 것이다. 단순히 현재 시점의 ’스냅샷’을 최신 상태로 유지하는 것을 넘어, 시간 축을 포함하여 공간의 변화 이력(History) 전체를 저장하고 쿼리할 수 있는 시스템이 등장할 것이다. 이는 “어제 여기에 있던 열쇠가 지금 어디로 갔지?“와 같은 질문에 답할 수 있는, 진정한 의미의 지능형 공간 지각 시스템의 도래를 예고한다. 또한, 비디오 생성 모델(Video Diffusion Models)이나 4D 가우시안 스플랫(4D Gaussian Splatting)과 같은 최신 생성형 AI 기술이 SLAM에 통합되면서, 누락된 정보를 상상하여 채우거나(Inpainting), 미래의 동적 상황을 예측하는 능력까지 갖춘 매핑 시스템이 연구되고 있다. 이러한 기술적 진보는 로봇이 인간과 공존하며 끊임없이 변화하는 환경을 이해하고 적응하는 데 있어 핵심적인 기반이 될 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Open-Vocabulary 3D Semantic Segmentation with Foundation Models, https://pure.mpg.de/rest/items/item_3620931/component/file_3620932/content</li>
<li>Semantic information based solution for visual SLAM in dynamic …, https://www.researchgate.net/publication/382370775_Semantic_information_based_solution_for_visual_SLAM_in_dynamic_environment</li>
<li>Open-Vocabulary Online Semantic Mapping for SLAM, https://zaguan.unizar.es/record/165592/files/texto_completo.pdf</li>
<li>(PDF) Open-Vocabulary Online Semantic Mapping for SLAM, https://www.researchgate.net/publication/396190179_Open-Vocabulary_Online_Semantic_Mapping_for_SLAM</li>
<li>DualMap: Online Open-Vocabulary Semantic Mapping for Natural …, https://arxiv.org/html/2506.01950v3</li>
<li>DualMap: Online Open-Vocabulary Semantic Mapping for Natural …, https://openreview.net/pdf/b3e55540cd04c1c39538169ea660bf051ae3a100.pdf</li>
<li>Tracking Anything with Decoupled Video Segmentation, https://helios2.mi.parisdescartes.fr/~lomn/Cours/CV/SeqVideo/CollaborativeWork/SOTA/Cheng_Tracking_Anything_with_Decoupled_Video_Segmentation_ICCV_2023_paper.pdf</li>
<li>hkchengrex/Tracking-Anything-with-DEVA: [ICCV 2023 … - GitHub, https://github.com/hkchengrex/Tracking-Anything-with-DEVA</li>
<li>Joint Learning of Video Segmentation and Optical Flow, https://cdn.aaai.org/ojs/6699/6699-13-9928-1-10-20200522.pdf</li>
<li>Seeing Beyond the FoV via Flow-guided Clip-Recurrent Transformer, https://www.researchgate.net/publication/365633867_FlowLens_Seeing_Beyond_the_FoV_via_Flow-guided_Clip-Recurrent_Transformer</li>
<li>Mask Propagation for Efficient Video Semantic Segmentation - NIPS, https://proceedings.neurips.cc/paper_files/paper/2023/file/167bcf2af2cd08fcf75b932022db0311-Paper-Conference.pdf</li>
<li>FlowVid: Taming Imperfect Optical Flows for Consistent Video-to …, https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_FlowVid_Taming_Imperfect_Optical_Flows_for_Consistent_Video-to-Video_Synthesis_CVPR_2024_paper.pdf</li>
<li>(PDF) OFF-ViNet: Optical Flow-Based Feature Warping ViNet for …, https://www.researchgate.net/publication/380139049_OFF-ViNet_Optical_Flow-Based_Feature_Warping_ViNet_for_Video_Saliency_Prediction_Considering_Future_Prediction</li>
<li>(PDF) Tracking Anything with Decoupled Video Segmentation, https://www.researchgate.net/publication/373754069_Tracking_Anything_with_Decoupled_Video_Segmentation</li>
<li>Tracking Everything Everywhere All at Once - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Tracking_Everything_Everywhere_All_at_Once_ICCV_2023_paper.pdf</li>
<li>Tracking Everything Everywhere All at Once, https://omnimotion.github.io/</li>
<li>[PDF] Tracking Everything Everywhere All at Once | Semantic Scholar, https://www.semanticscholar.org/paper/Tracking-Everything-Everywhere-All-at-Once-Wang-Chang/408399600631b7eacf491dae0ac997fb5c13ccc7</li>
<li>[Quick Review] Tracking Everything Everywhere All at Once - Liner, https://liner.com/review/tracking-everything-everywhere-all-at-once</li>
<li>Towards Open-Vocabulary Video Instance Segmentation, https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Towards_Open-Vocabulary_Video_Instance_Segmentation_ICCV_2023_paper.pdf</li>
<li>Overall mean Average Precision (mAP) under different memory …, https://www.researchgate.net/figure/Overall-mean-Average-Precision-mAP-under-different-memory-update-factor-a-on-LV-VIS-and_fig3_377424658</li>
<li>Semantic SLAM: Fusion of Geometry &amp; Semantics - Emergent Mind, https://www.emergentmind.com/topics/semantic-simultaneous-localization-and-mapping-slam</li>
<li>Incremental Semantic Mapping - Emergent Mind, https://www.emergentmind.com/topics/incremental-semantic-mapping</li>
<li>3D Semantic Map Reconstruction for Orchard Environments Using …, https://www.preprints.org/frontend/manuscript/fefa695be008da28b7ac4f0da565fb7b/download_pub</li>
<li>Open-Vocabulary 3D Semantic SLAM - Emergent Mind, https://www.emergentmind.com/topics/open-vocabulary-3d-semantic-slam-40ef505d-f1d0-4bd6-8e09-394a9b8697cd</li>
<li>Real-time Open-Vocabulary 3D Mapping and Queryable Scene …, https://www.semanticscholar.org/paper/Open-Fusion%3A-Real-time-Open-Vocabulary-3D-Mapping-Yamazaki-Hanyu/23cde1c9c8f4fe79e1ba5fc8f9c17bd1967f0a79</li>
<li>Matteo Frosi’s research works | Politecnico di Milano and other places, https://www.researchgate.net/scientific-contributions/Matteo-Frosi-2201120591</li>
<li>Computer Science Apr 2025 - arXiv, https://www.arxiv.org/list/cs/2025-04?skip=8125&amp;show=2000</li>
<li>CORE-3D: CONTEXT-AWARE OPEN-VOCABULARY - OpenReview, https://openreview.net/pdf/9a980b56e8e243213d30d276a3e469eafd2e879e.pdf</li>
<li>Online Open-Vocabulary Mapping with Neural Implicit Representation, https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11877.pdf</li>
<li>Bayesian Fields: Task-driven Open-Set Semantic Gaussian Splatting, https://arxiv.org/html/2503.05949v1</li>
<li>Bayesian Spatial Kernel Smoothing for Scalable Dense Semantic …, https://www.researchgate.net/publication/338497709_Bayesian_Spatial_Kernel_Smoothing_for_Scalable_Dense_Semantic_Mapping</li>
<li>Dynamic Open-Vocabulary 3D Scene Graphs for Long-term … - arXiv, https://arxiv.org/html/2410.11989v1</li>
<li>Dynamic Open-Vocabulary 3D Scene Graphs for Long-term … - arXiv, https://arxiv.org/html/2410.11989v2</li>
<li>Eku127/DualMap: [RAL-25] An online open-vocabulary … - GitHub, https://github.com/Eku127/DualMap</li>
<li>Zero-Shot Video Camouflaged Object Segmentation By Optical Flow …, https://arxiv.org/html/2505.01431v2</li>
<li>Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds …, https://openaccess.thecvf.com/content/CVPR2024/papers/Koch_Open3DSG_Open-Vocabulary_3D_Scene_Graphs_from_Point_Clouds_with_Queryable_CVPR_2024_paper.pdf</li>
<li>3D Dynamic Scene Graphs: Actionable Spatial Perception with …, https://roboticsconference.org/2020/program/papers/79.html</li>
<li>Open-Vocabulary Functional 3D Scene Graphs for Real-World …, https://pure.mpg.de/rest/items/item_3687328/component/file_3687329/content</li>
<li>Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds …, https://darko-project.eu/wp-content/uploads/papers/2024/Open3DSG_Open-Vocabulary_3D_Scene_Graphs_from_Point_Clouds_with_Queryable_Objects_and_Open-Set_Relationships.pdf</li>
<li>Hydra: A Real-time Spatial Perception System for 3D Scene Graph …, https://dspace.mit.edu/bitstream/handle/1721.1/145300/2201.13360.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>