<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:6.5.2 모호성 해결 (Ambiguity Resolution): "저 컵"이 아닌 "왼쪽에 있는 빨간 컵"을 구별하기 위한 문맥 인식.</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>6.5.2 모호성 해결 (Ambiguity Resolution): "저 컵"이 아닌 "왼쪽에 있는 빨간 컵"을 구별하기 위한 문맥 인식.</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 6. 오픈 보캐블러리와 시맨틱 이해 (Open-Vocabulary & Semantic Understanding)</a> / <a href="index.html">6.5 동적 환경과 시간적 일관성 (Temporal Consistency)</a> / <span>6.5.2 모호성 해결 (Ambiguity Resolution): "저 컵"이 아닌 "왼쪽에 있는 빨간 컵"을 구별하기 위한 문맥 인식.</span></nav>
                </div>
            </header>
            <article>
                <h1>6.5.2 모호성 해결 (Ambiguity Resolution): “저 컵“이 아닌 “왼쪽에 있는 빨간 컵“을 구별하기 위한 문맥 인식.</h1>
<h2>1.  서론: 엠바디드 AI의 핵심 난제, 지칭적 모호성</h2>
<p>로봇이 인간과 공존하며 물리적 공간에서 협업을 수행하는 엠바디드 AI(Embodied AI) 분야에서, 자연어 명령을 올바르게 수행하는 것은 기술적 완성도의 정점을 보여주는 지표입니다. 그러나 인간의 언어는 본질적으로 고도로 압축되어 있으며, 효율성을 극대화하기 위해 맥락(Context)에 크게 의존합니다. 인간 대 인간의 소통에서 “저 컵 좀 줘(Give me that cup)“라는 발화는 시선 교환, 공유된 과거의 기억, 현재 수행 중인 작업의 흐름, 그리고 물리적 환경의 구성 등 복합적인 정보를 통해 순식간에 해독됩니다. 반면, 센서를 통해 세상을 데이터로 받아들이는 로봇에게 이러한 발화는 엄청난 불확실성과 모호성(Ambiguity)의 덩어리입니다. 로봇의 시각 시스템에 포착된 수많은 객체 중 어떤 것이 ’저 컵’인지, 화자가 말하는 ’왼쪽’이 화자의 왼쪽인지 로봇의 왼쪽인지, 혹은 ’빨간 컵’이라는 표현이 단순히 색상을 묘사하는 것인지 아니면 다른 컵들과의 대조를 위해 사용된 것인지를 판별하는 과정은 단순한 패턴 인식을 넘어선 고차원적인 추론 능력을 요구합니다.</p>
<p>본 장에서는 제미나이(Gemini)와 같은 대규모 멀티모달 모델(LMM)을 활용하여 로봇의 인지 능력을 설계할 때, 가장 빈번하게 마주치는 ’모호성’을 어떻게 정의하고 기술적으로 해결하는지 심층적으로 분석합니다. 특히, 2024년과 2025년의 최신 연구 동향을 바탕으로, 단순한 시각적 탐지(Detection)를 넘어선 <strong>시각적 접지(Visual Grounding)</strong>, 3D 공간의 구조적 이해를 위한 <strong>3D 씬 그래프(3D Scene Graph)</strong>, 화자의 의도를 역추론하는 <strong>화용론적 추론(Pragmatic Reasoning)</strong>, 그리고 불확실성을 능동적으로 해소하는 <strong>능동적 인식(Active Perception)</strong> 및 <strong>컨포멀 예측(Conformal Prediction)</strong> 기술을 포괄적으로 다룹니다. 이를 통해 “저 컵“이라는 모호한 지시가 “왼쪽에 있는 빨간 컵“이라는 구체적인 물리적 대상으로 변환되는 인지적 파이프라인의 전 과정을 기술합니다.</p>
<h3>1.1  모호성의 유형 분류</h3>
<p>로봇이 직면하는 모호성은 크게 지각적 차원, 공간적 차원, 그리고 화용적 차원으로 나눌 수 있습니다.</p>
<table><thead><tr><th><strong>모호성 유형 (Ambiguity Type)</strong></th><th><strong>정의 및 예시</strong></th><th><strong>기술적 해결 과제</strong></th><th><strong>관련 핵심 기술</strong></th></tr></thead><tbody>
<tr><td><strong>지각적 모호성 (Perceptual)</strong></td><td>시각적 특징의 불명확성 또는 중복. (예: “빨간 컵“이 시야에 두 개 이상 존재하거나, 조명 변화로 인해 색상 식별이 어려운 경우)</td><td>고해상도 특징 추출, 멀티모달 융합, 수량(Cardinality) 인식</td><td>CLIP-Fields, LERF, NGDINO, MuRes</td></tr>
<tr><td><strong>공간적 모호성 (Spatial)</strong></td><td>위치 참조의 기준 프레임(Reference Frame) 불일치. (예: “왼쪽”, “뒤”, “근처“의 기준이 화자/로봇/객체 중 무엇인지 불분명)</td><td>3D 공간 구조화, 위상 관계(Topological Relation) 추론, 시점 변환</td><td>3D Scene Graphs, ConceptGraphs, EGG</td></tr>
<tr><td><strong>화용적 모호성 (Pragmatic)</strong></td><td>발화의 정보 부족 및 의도 생략. (예: 컵이 여러 개인데 단순히 “컵“이라고만 할 때, 화자의 숨겨진 의도 파악 필요)</td><td>화자 의도 역추론, 맥락 기반 함축(Implicature) 해석, 합리적 화행 모델링</td><td>Rational Speech Acts (RSA), HandMeThat</td></tr>
<tr><td><strong>실행적 모호성 (Actionable)</strong></td><td>인식은 되었으나 행동 결정의 불확실성. (예: 타겟을 찾았으나 파지하기 어렵거나 확신이 90% 미만일 때)</td><td>불확실성 정량화, 질문 생성, 능동적 정보 수집</td><td>Conformal Prediction, ActiveVLA, Ask-to-Clarify</td></tr>
</tbody></table>
<p>이러한 모호성을 해결하기 위해서는 단일 모달리티에 의존하는 것이 아니라, 시각, 언어, 행동 데이터가 유기적으로 결합된 문맥 인식(Context Awareness) 시스템이 필수적입니다. 다음 절부터는 각 모호성 유형별로 최신 기술들이 어떻게 이를 극복하고 있는지 상세히 논의합니다.</p>
<h2>2.  지각적 모호성 해결: 고해상도 시각-언어 접지와 3D 의미장</h2>
<p>“빨간 컵“을 식별하기 위한 첫 번째 단계는 로봇이 수집한 시각 데이터(RGB 이미지, 깊이 지도, 포인트 클라우드)와 사용자의 언어적 표현을 매핑하는 **참조 표현 이해(REC, Referring Expression Comprehension)**입니다. 과거의 REC 기술이 2D 이미지 상의 바운딩 박스(Bounding Box) 검출에 머물렀다면, 최신 엠바디드 AI는 로봇이 활동하는 3차원 공간 자체에 언어적 의미를 부여하는 방식으로 진화하고 있습니다.</p>
<h3>2.1  3D 공간의 의미화: CLIP-Fields와 LERF</h3>
<p>로봇이 “빨간 컵“을 찾기 위해서는 현재 카메라에 보이는 장면뿐만 아니라, 이전에 지나쳐온 공간에 대한 기억이 필요합니다. 이를 위해 3D 맵의 각 좌표에 시각적 정보뿐만 아니라 언어적 의미 정보를 함께 저장하는 기술이 등장했습니다. 이는 CLIP(Contrastive Language-Image Pre-training)과 같은 대규모 비전-언어 모델(VLM)의 임베딩을 3D 공간 표현에 통합함으로써 가능해졌습니다.</p>
<p>**CLIP-Fields **는 이러한 접근의 선구적인 연구 중 하나입니다. 로봇이 환경을 주행하며 얻은 RGB-D 데이터를 이용해 3D 포인트 클라우드를 구축할 때, 각 포인트에 해당 위치의 시각적 특징을 CLIP 인코더로 변환한 고차원 벡터를 할당합니다. 사용자가 “빨간 컵“이라고 말하면, 텍스트 인코더가 생성한 “빨간 컵” 벡터와 3D 맵 상의 모든 포인트 벡터 간의 내적(Dot Product)을 계산합니다. 유사도가 높은 영역이 활성화됨으로써 로봇은 별도의 학습 없이도(Zero-shot) “빨간 컵”, “먹을 수 있는 것”, “액체를 담는 도구“와 같은 다양한 자연어 쿼리에 대응할 수 있게 됩니다.</p>
<p>이러한 방식은 **LERF (Language Embedded Radiance Fields) **를 통해 더욱 정교해졌습니다. LERF는 NeRF(Neural Radiance Fields)의 볼륨 렌더링 과정에 언어 필드를 통합합니다. NeRF가 3D 공간의 밀도와 색상을 예측한다면, LERF는 각 3D 지점의 언어적 임베딩을 추가로 예측합니다. 특히 LERF는 이미지의 전체적인 특징뿐만 아니라 다양한 크기의 패치(Multi-scale patches)에서 CLIP 임베딩을 추출하여 학습합니다. 이를 통해 “거실“과 같은 넓은 공간적 개념부터 “전기 콘센트“나 “머그컵의 손잡이” 같은 세밀한 객체까지 계층적으로 이해할 수 있습니다. 2025년 연구 결과에 따르면, LERF 기반의 접근법은 텍스트 쿼리에 대해 3D 공간상에 ’관련성 맵(Relevancy Map)’을 생성하여, 모호한 쿼리(“오래된 물건”)에 대해서도 시각적 속성과 추상적 의미를 결합해 타겟을 찾아내는 데 탁월한 성능을 보입니다.</p>
<h3>2.2  오픈 보캐블러리와 멀티모달 융합: ConceptFusion과 OpenScene</h3>
<p>“저 컵“이 아닌 구체적인 대상을 지칭하기 위해 사용자는 “금속 소리가 나는 컵“이나 “어제 내가 보여준 그 컵“과 같이 시각 외적인 정보를 사용할 수 있습니다. **ConceptFusion **과 **OpenScene **은 이러한 멀티모달 쿼리를 처리하기 위해 제안되었습니다.</p>
<p>ConceptFusion은 픽셀 단위로 정렬된(Pixel-aligned) 오픈셋(Open-set) 특징을 3D 맵에 융합합니다. 기존의 객체 감지기가 사전에 정의된 클래스(예: COCO 데이터셋의 80개 클래스)만 인식할 수 있었던 반면, ConceptFusion은 인터넷 규모의 데이터로 학습된 파운데이션 모델의 지식을 3D 맵으로 전이(Distillation)시킵니다. 이를 통해 로봇은 한 번도 본 적 없는 “찌그러진 펩시 캔“이나 “피카츄 인형” 같은 롱테일(Long-tail) 객체도 텍스트 쿼리로 찾아낼 수 있습니다. 또한, 오디오 인코더(AudioCLIP 등)와 연동하여 소리 기반의 검색이나, 이미지를 예시로 보여주며 “이것과 같은 것“을 찾는 이미지 기반 검색도 지원하여 지각적 모호성을 획기적으로 낮춥니다.</p>
<h3>2.3  극한의 모호성 환경: 드론과 NGDINO</h3>
<p>지상 로봇보다 자유도가 높고 시야가 넓은 드론 환경에서는 지각적 모호성이 극대화됩니다. 드론이 상공에서 내려다볼 때 객체는 매우 작게 보이며(Small-scale), 한 화면에 수많은 유사 객체가 존재할 수 있습니다(High-density). 2025년 발표된 **RefDrone 벤치마크 **는 이러한 드론 시점의 REC 문제를 집중적으로 다룹니다.</p>
<p>이 환경에서 모호성을 해결하기 위한 핵심 기술로 **NGDINO (Number GroundingDINO) **가 제안되었습니다. 기존 모델들은 “사람들“이라는 쿼리에 대해 군중 전체를 하나의 박스로 잡거나 일부만 검출하는 오류를 범하기 쉽습니다. NGDINO는 표현식에 포함된 수량 정보(예: “두 명의 사람”, “모든 차”)를 명시적으로 추정하는 모듈을 도입했습니다. “빨간 컵“이라는 쿼리가 들어왔을 때, 화면에 빨간 컵이 여러 개라면 모델은 텍스트에 내포된 단수/복수 정보를 분석하고, 이를 시각적 탐지 과정의 제약 조건으로 활용합니다. 만약 사용자가 “왼쪽의 컵 하나(the cup)“라고 단수로 지칭했다면, 모델은 가장 돌출되거나 명확한 하나의 대상만을 선택하도록 유도되어 다중 타겟 상황에서의 모호성을 해결합니다.</p>
<p>또한, **MuRes (Multimodal Guided Residual Module) **는 로봇과 인간의 상호작용에서 발생하는 비언어적 신호(제스처, 시선)를 통합하여 지각적 모호성을 줄입니다. 사용자가 “저거“라고 말하며 손가락으로 가리킬 때, 언어 모델만으로는 정보를 얻을 수 없지만, MuRes는 시각적 제스처 특징을 추출하여 언어 특징에 잔차(Residual) 형태로 더해줌으로써, 텍스트가 지칭하는 대상을 시각적 힌트로 강화(Reinforce)합니다. 이는 “저 컵“이라는 모호한 표현이 포인팅 제스처와 결합될 때 어떻게 명확해지는지를 기계적으로 구현한 사례입니다.</p>
<h2>3.  공간적 모호성 해결: 구조적 추론과 3D 씬 그래프</h2>
<p>“왼쪽에 있는 빨간 컵“이라는 문장에서 ’왼쪽’은 절대적인 좌표가 아닌 상대적인 관계를 나타냅니다. 이러한 공간적 관계(Spatial Relation)를 이해하기 위해서는 환경을 단순한 점들의 집합(Point Cloud)이 아닌, 의미 있는 객체들과 그들 간의 관계로 구조화된 **3D 씬 그래프(3D Scene Graph)**로 변환해야 합니다.</p>
<h3>3.1  3D 씬 그래프의 구조와 생성</h3>
<p>3D 씬 그래프는 환경을 노드(Node)와 엣지(Edge)로 추상화한 데이터 구조입니다.</p>
<ul>
<li><strong>노드(Node):</strong> 공간상의 개체를 나타냅니다. 이는 계층적으로 구성되어 건물(Building) <span class="math math-inline">\to</span> 방(Room) <span class="math math-inline">\to</span> 가구(Furniture) <span class="math math-inline">\to</span> 물체(Object) 순으로 연결됩니다. 각 노드는 의미적 라벨(“컵”), 3D 바운딩 박스, 시각적 특징 벡터, 그리고 행동 가능한 어포던스(Affordance) 정보를 포함합니다.</li>
<li><strong>엣지(Edge):</strong> 노드 간의 공간적, 의미적 관계를 나타냅니다. 예를 들어 <code>on(컵, 테이블)</code>, <code>next_to(의자, 침대)</code>, <code>inside(사과, 냉장고)</code>와 같은 위상적(Topological) 관계가 엣지로 정의됩니다.</li>
</ul>
<p>최신 연구인 **ConceptGraphs **는 이러한 씬 그래프를 오픈 보캐블러리 환경으로 확장했습니다. ConceptGraphs의 파이프라인은 다음과 같이 작동하여 공간적 모호성을 해결합니다.</p>
<ol>
<li><strong>객체 분할 및 특징 추출:</strong> RGB-D 입력에서 SAM(Segment Anything Model)을 사용하여 모든 물체 후보를 분할하고, CLIP 또는 DINOv2와 같은 모델로 특징을 추출합니다.</li>
<li><strong>3D 객체 통합:</strong> 다중 시점의 2D 마스크를 3D 공간으로 투영하고 통합하여 개별 객체 인스턴스를 생성합니다.</li>
<li><strong>의미적 캡션 생성:</strong> LLaVA와 같은 멀티모달 LLM을 사용하여 각 3D 객체에 대한 상세한 캡션을 생성합니다. 단순히 “컵“이 아니라 “스타벅스 로고가 있는 흰색 머그컵“과 같이 상세한 설명이 노드 속성으로 저장됩니다.</li>
<li><strong>관계 추론:</strong> LLM(GPT-4 등)을 활용하여 객체들의 좌표와 캡션을 바탕으로 공간적 관계를 추론합니다. “컵의 좌표가 테이블의 상판 좌표 범위 내에 있고 높이가 더 높다“는 기하학적 사실을 LLM이 <code>on(cup, table)</code>이라는 의미적 관계 엣지로 변환합니다.</li>
</ol>
<h3>3.2  공간적 쿼리 처리와 참조 프레임</h3>
<p>ConceptGraphs와 같은 구조화된 표현을 사용하면 “왼쪽에 있는 빨간 컵“이라는 쿼리는 그래프 탐색 문제로 변환됩니다.</p>
<ol>
<li><strong>속성 필터링:</strong> 먼저 캡션 정보를 바탕으로 “빨간색“이고 “컵“인 속성을 가진 노드들을 후보군으로 검색합니다. (예: 후보 {Node_A, Node_B})</li>
<li><strong>관계 검증:</strong> “왼쪽에 있는“이라는 공간적 제약을 검증합니다. 이때 중요한 것은 **참조 프레임(Reference Frame)**의 설정입니다.</li>
</ol>
<ul>
<li><strong>자아 중심(Egocentric):</strong> 로봇의 현재 카메라 뷰를 기준으로 왼쪽을 판단합니다.</li>
<li><strong>타자 중심(Allocentric):</strong> “노트북의 왼쪽“처럼 기준 객체(노트북)가 명시된 경우입니다.</li>
<li><strong>환경 중심(Environment-centric):</strong> “방의 왼쪽 코너“와 같이 절대적 공간 좌표를 기준으로 합니다.</li>
</ul>
<p>Refer360  데이터셋과 관련 연구들은 이러한 참조 프레임의 모호성을 해결하기 위해, 파노라마 이미지와 다중 시점 데이터를 활용하여 “왼쪽“이라는 표현이 화자의 시선 방향과 일치하는지, 아니면 절대적 방위(동서남북)를 의미하는지를 학습합니다. 특히 LLM의 추론 능력을 활용하여 문맥상 가장 적절한 참조 프레임을 선택합니다. 예를 들어 “너의 왼쪽에 있어“라고 하면 로봇(청자) 기준임을, “의자 왼쪽에 있어“라고 하면 객체 기준임을 파악하여 그래프 상에서 해당 엣지(<code>left_of</code>)를 검사합니다.</p>
<h3>3.3  시간적 모호성과 사건 기반 접지 (Event Grounding)</h3>
<p>공간은 시간에 따라 변합니다. “아까 내가 물을 마셨던 그 컵“이라는 표현은 현재의 상태가 아닌 과거의 사건(Event)을 참조합니다. **Event-Grounding Graph (EGG) **는 3D 씬 그래프에 시간 축을 도입하여 이를 해결합니다. EGG는 ’사건(Event)’을 별도의 노드로 정의하고, 이 사건에 참여한 객체(Participants)와 발생한 장소(Location), 시간(Time)을 엣지로 연결합니다. 사용자가 “아까 쓴 컵“을 찾을 때, 로봇은 EGG를 탐색하여 <code>Drinking_Event</code> 노드와 연결된 <code>Cup</code> 노드를 식별함으로써, 현재 시각적으로는 다른 컵들과 구별되지 않더라도 문맥적으로 유일한 컵을 찾아낼 수 있습니다. 이는 공간적 모호성 해결을 시간적 차원으로 확장한 중요한 진보입니다.</p>
<h2>4.  화용적 모호성 해결: 의도 추론과 합리적 화행 (RSA)</h2>
<p>시각적으로 컵을 찾고 공간적 위치를 파악했다 하더라도, 왜 화자가 굳이 “빨간 컵“이라고 말했는지, 혹은 왜 “컵“이라고만 말했는지를 이해하는 것은 별개의 문제입니다. 이는 언어학의 화용론(Pragmatics) 영역으로, 화자의 발화 의도를 파악하여 생략된 정보를 복원하는 과정입니다.</p>
<h3>4.1  그라이스의 격률과 로봇의 추론</h3>
<p>인간의 대화는 폴 그라이스(Paul Grice)가 제안한 대화의 격률(Maxims of Conversation)을 따르는 경향이 있습니다. 그중 **양의 격률(Maxim of Quantity)**은 “필요한 만큼의 정보를 제공하라“는 원칙입니다.</p>
<ul>
<li>만약 테이블 위에 빨간 컵 하나만 있다면, 화자는 굳이 “빨간 컵“이라고 하지 않고 “컵“이라고 말할 것입니다(경제성 원칙).</li>
<li>만약 화자가 “빨간 컵“이라고 말했다면, 로봇은 역으로 “아, 빨간색이 아닌 다른 컵들이 존재하겠구나“라고 추론해야 합니다. 이것이 바로 **함축(Implicature)**의 계산입니다.</li>
</ul>
<h3>4.2  합리적 화행 (RSA, Rational Speech Acts) 프레임워크</h3>
<p>RSA 프레임워크는 이러한 화용적 추론을 베이지안 확률 모델로 정식화하여 로봇에 적용합니다. RSA 모델은 화자(Speaker)와 청자(Listener)가 서로를 재귀적으로 시뮬레이션한다고 가정합니다.</p>
<h4>4.2.1  수학적 정식화</h4>
<ol>
<li><strong>문자적 청자 (Literal Listener, <span class="math math-inline">L_0</span>):</strong></li>
</ol>
<p>가장 기초적인 단계로, 발화 <span class="math math-inline">u</span>의 문자 그대로의 의미(Semantics)를 만족하는 객체 <span class="math math-inline">o</span>를 찾습니다.<br />
<span class="math math-display">
   P_{L_0}(o|u) \propto [[u(o)]] P(o)
</span><br />
여기서 <span class="math math-inline">[[u(o)]]</span>는 객체 <span class="math math-inline">o</span>가 발화 <span class="math math-inline">u</span>의 진리 조건(예: <span class="math math-inline">u</span>=“빨간 컵”, <span class="math math-inline">o</span>=빨간 컵 <span class="math math-inline">\to</span> True)을 만족하면 1, 아니면 0인 지시 함수입니다. <span class="math math-inline">P(o)</span>는 객체의 사전 확률(Prior)입니다. 이 단계에서 로봇은 모든 빨간 컵을 동일한 확률로 인식합니다.</p>
<ol start="2">
<li><strong>화용적 화자 (Pragmatic Speaker, <span class="math math-inline">S_1</span>):</strong></li>
</ol>
<p>화자는 청자(<span class="math math-inline">L_0</span>)가 대상을 헷갈리지 않고 정확히 식별하기를 원하며(정보성), 동시에 짧게 말하기를 원합니다(비용). 화자가 객체 <span class="math math-inline">o</span>를 지칭하기 위해 발화 <span class="math math-inline">u</span>를 선택할 확률은 다음과 같습니다.<br />
<span class="math math-display">
   P_{S_1}(u|o) \propto \exp(\alpha (\log P_{L_0}(o|u) - C(u)))
</span><br />
여기서 <span class="math math-inline">U(u; o) = \log P_{L_0}(o|u) - C(u)</span>는 효용 함수(Utility Function)이며, <span class="math math-inline">\alpha</span>는 화자의 합리성 정도(Rationality Parameter)입니다. <span class="math math-inline">C(u)</span>는 발화의 비용(단어 수 등)입니다. 이 모델에 따르면, 화자는 파란 컵이 옆에 있을 때 “컵“이라고 말하면 <span class="math math-inline">L_0</span>가 혼동할 것(낮은 <span class="math math-inline">P_{L_0}</span>)을 알기에, 비용이 조금 더 들더라도 “빨간 컵“이라고 말할 확률이 높아집니다.</p>
<ol start="3">
<li><strong>화용적 청자 (Pragmatic Listener, <span class="math math-inline">L_1</span>):</strong></li>
</ol>
<p>최종적으로 로봇(청자)은 화자가 <span class="math math-inline">S_1</span> 모델에 따라 행동했을 것이라고 가정하고, 관측된 발화 <span class="math math-inline">u</span>로부터 화자의 의도된 객체 <span class="math math-inline">o</span>를 역추론합니다.<br />
<span class="math math-display">
   P_{L_1}(o|u) \propto P_{S_1}(u|o) P(o)
</span><br />
이 과정을 통해 로봇은 “빨간 컵“이라는 말을 듣고, 단순히 빨간색 속성을 가진 컵을 찾는 것을 넘어, “다른 색깔의 컵들과 구별되는(Contrastive) 빨간 컵“을 타겟으로 확률적으로 확정하게 됩니다.</p>
<h3>4.3  HandMeThat 벤치마크와 물리-사회적 맥락</h3>
<p>RSA 모델의 효용성은 **HandMeThat 벤치마크 **에서 잘 드러납니다. 이 벤치마크는 로봇이 사용자의 모호한 지시(“그거 줘”)를 수행하기 위해 물리적 상태뿐만 아니라 사용자의 행동 궤적(Action Trajectory)을 맥락으로 활용해야 함을 보여줍니다.</p>
<ul>
<li><strong>시나리오:</strong> 사용자가 요리를 하다가 찬장을 열고 안을 들여다본 뒤 닫고, 로봇에게 “그거 좀 줄래?“라고 말합니다.</li>
<li><strong>RSA 적용:</strong></li>
<li><span class="math math-inline">P(o)</span>: 사용자가 찬장을 열었다는 행동은 찬장 안에 있는 물건을 찾으려 했음을 시사합니다(Goal Inference). 하지만 빈손으로 닫았다는 것은 찾는 물건이 없거나 손이 닿지 않았음을 의미합니다. 따라서 찬장과 관련된 물품(예: 소금)의 사전 확률이 높아집니다.</li>
<li><span class="math math-inline">P_{S_1}(u|o)</span>: 사용자가 “그거(it)“라고 대명사를 쓴 것은, 로봇과 사용자가 현재 ’찬장을 탐색한 상황’을 공유하고 있다고 믿기 때문입니다(Joint Attention).</li>
<li><span class="math math-inline">L_1</span> 추론: 로봇은 이러한 맥락을 종합하여, 테이블 위에 있는 여러 물건 중 ‘찬장에 보통 들어있어야 할’ 물건인 소금을 타겟으로 선택합니다.</li>
</ul>
<p>최근 연구 는 이러한 RSA 추론 과정을 LLM의 텍스트 생성 능력이나 학습된 신경망(Learned RSA)으로 근사하여 계산 효율성을 높이고 있습니다. LLM에게 “이 상황에서 화자가 ’저 컵’이라고 했다면 무엇을 가리킬까?“라고 프롬프팅하는 것만으로도 RSA와 유사한 화용적 추론 효과를 얻을 수 있음이 입증되었습니다.</p>
<h2>5.  능동적 모호성 해결: 불확실성 정량화와 능동적 인식</h2>
<p>아무리 뛰어난 시각 모델과 추론 엔진을 사용하더라도, 정보 자체가 차단된 상황(완전한 가려짐, 극심한 노이즈)이나 발화가 논리적으로 불가능한 경우(존재하지 않는 물체를 지칭)에는 모호성을 완전히 제거할 수 없습니다. 이때 로봇은 수동적인 관찰자에서 벗어나 **능동적 행위자(Active Agent)**가 되어야 합니다. 이를 위해 <strong>불확실성의 정량화</strong>, <strong>확인 질문 생성</strong>, <strong>능동적 시각 탐색</strong> 기술이 통합됩니다.</p>
<h3>5.1  컨포멀 예측(Conformal Prediction)을 통한 불확실성 보장</h3>
<p>딥러닝 모델의 출력(Softmax 확률 등)은 종종 과신(Overconfident)되는 경향이 있어, 로봇의 안전한 의사결정 기준으로 삼기 위험합니다. **컨포멀 예측(Conformal Prediction, CP) **은 통계적으로 유효한 불확실성 구간을 제공하는 핵심 기술로 2024-2025년 로봇 학계에서 주목받고 있습니다.</p>
<p>CP는 단일 예측값 대신, 정답을 포함할 확률이 사용자 지정 수준(예: <span class="math math-inline">1-\epsilon = 95\%</span>) 이상임이 수학적으로 보장되는 **예측 집합(Prediction Set, <span class="math math-inline">C(x)</span>)**을 생성합니다.</p>
<ul>
<li><strong>작동 메커니즘:</strong></li>
</ul>
<ol>
<li>보정 데이터셋(Calibration Set)을 사용하여 모델의 불확실성 점수(Non-conformity Score) 분포를 계산하고, 95% 신뢰 수준에 해당하는 임계값(Quantile) <span class="math math-inline">\hat{q}</span>를 도출합니다.</li>
<li>새로운 입력 <span class="math math-inline">x</span>에 대해, 점수가 <span class="math math-inline">\hat{q}</span>를 넘는 모든 후보 객체를 집합 <span class="math math-inline">C(x)</span>에 포함시킵니다.</li>
</ol>
<ul>
<li><strong>모호성 판단 기준:</strong></li>
<li><span class="math math-inline">|C(x)| = 1</span>: 예측 집합에 단 하나의 객체만 존재함. <span class="math math-inline">\to</span> <strong>모호성 없음</strong>. 로봇은 확신을 가지고 행동(Action)을 수행합니다.</li>
<li><span class="math math-inline">|C(x)| &gt; 1</span>: 예측 집합에 여러 객체(예: {컵 A, 컵 B})가 포함됨. <span class="math math-inline">\to</span> <strong>모호함</strong>. 로봇은 어떤 것이 정답인지 확신할 수 없으므로 행동을 멈추고 확인 질문(Clarification)을 생성해야 합니다.</li>
<li><span class="math math-inline">|C(x)| = 0</span>: 예측 집합이 비어 있음. <span class="math math-inline">\to</span> <strong>인식 실패</strong>. 현재 시야에 타겟이 없거나 모델이 전혀 모르는 대상임. 탐색(Search) 모드로 전환합니다.</li>
</ul>
<p>이러한 CP 기반 접근은 로봇이 “모른다는 것을 아는(Know unknowns)” 메타인지 능력을 부여하여, 섣부른 행동으로 인한 실패를 방지하고 인간의 신뢰를 확보하는 데 기여합니다.</p>
<h3>5.2  Ask-to-Clarify: 질문을 통한 모호성 해소</h3>
<p>불확실성이 감지되면(<span class="math math-inline">|C(x)| &gt; 1</span>), 로봇은 사용자에게 질문을 해야 합니다. **Ask-to-Clarify 프레임워크 **는 언제, 무엇을 물을지 결정하는 체계적인 방법을 제시합니다.</p>
<ul>
<li><strong>신호 탐지기(Signal Detector):</strong> 비전-언어 모델의 내부 활성화 값이나 엔트로피, CP의 예측 집합 크기를 모니터링하여 ’행동(Act)’과 ‘질문(Ask)’ 모드 간의 스위칭을 제어합니다.</li>
<li><strong>정보 이득(Information Gain) 기반 질문 생성:</strong> 단순히 “어떤 거요?“라고 묻는 것은 비효율적입니다. 로봇은 후보 집합 <span class="math math-inline">C(x)</span>에 포함된 객체들의 속성을 분석하여 가장 변별력(Discriminative)이 높은 특징을 찾습니다. 예를 들어 후보군이 {왼쪽 빨간 컵, 오른쪽 파란 컵}이라면 ’색상’과 ’위치’가 구별 포인트입니다. 따라서 “빨간색 컵을 말하는 건가요?” 또는 “왼쪽에 있는 건가요?“와 같이 예/아니오로 답할 수 있거나 선택지를 좁혀주는 구체적인 질문을 생성합니다. 이는 사용자의 인지 부하를 줄이고 대화 턴(Turn) 수를 최소화합니다.</li>
</ul>
<h3>5.3  ActiveVLA: 능동적 인식과 뷰포인트 최적화</h3>
<p>질문조차 불가능하거나 사용자의 개입을 최소화해야 할 때, 로봇은 스스로 움직여 정보를 획득해야 합니다. **ActiveVLA (Active Vision-Language-Action) **는 2026년형 최신 모델로, 수동적인 인식(Passive Perception)의 한계를 극복하기 위해 제안되었습니다.</p>
<p>ActiveVLA는 <strong>Coarse-to-Fine (거시에서 미시로)</strong> 전략을 사용합니다.</p>
<ol>
<li><strong>전역 탐색 (Coarse Stage):</strong> 전체 장면을 스캔하여 언어 쿼리와 관련된 영역의 히트맵을 생성합니다. 이때 모호성이 높거나(히트맵이 분산됨) 타겟이 가려져 있다면 불확실성 신호가 발생합니다.</li>
<li><strong>능동적 뷰 선택 (Active View Selection):</strong> 로봇은 정보 이득을 최대화할 수 있는 다음 카메라 위치(Next-Best-View)를 계산합니다. 예를 들어, 컵의 손잡이가 보이는지 확인하기 위해 측면으로 이동하거나, 컵 안의 내용물을 확인하기 위해 위로 이동합니다.</li>
<li><strong>3D 줌인 (Active 3D Zoom-in):</strong> 관심 영역(ROI)에 대해 물리적으로 접근하거나 카메라 줌을 사용하여 해상도를 높입니다. 이를 통해 멀리서는 “작은 점“으로 보이던 객체를 “스타벅스 로고가 있는 컵“으로 명확히 식별하여 지각적 모호성을 해소합니다.</li>
</ol>
<p>실험 결과에 따르면, ActiveVLA는 정적인 카메라를 사용하는 기존 모델 대비 복잡한 조작 작업에서의 성공률을 비약적으로 향상시켰으며, 이는 “움직임(Action)“이 “인식(Perception)“의 일부임을 증명합니다.</p>
<h2>6.  결론 및 향후 전망</h2>
<p>“저 컵“이 아닌 “왼쪽에 있는 빨간 컵“을 구별하는 과정은 단순한 단어 매칭 게임이 아닙니다. 그것은 <strong>1) 고해상도 시각 정보와 언어 의미의 3D 공간상 융합(Visual Grounding)</strong>, <strong>2) 공간적 위상 관계의 구조적 이해(Spatial Reasoning)</strong>, <strong>3) 화자의 의도와 생략된 맥락의 복원(Pragmatic Reasoning)</strong>, 그리고 **4) 불확실성에 대응하는 통계적 보장과 능동적 행동(Active Perception &amp; CP)**이 결합된 총체적인 인지 시스템의 작동 결과입니다.</p>
<p>본 장에서 살펴본 CLIP-Fields, ConceptGraphs, RSA, ActiveVLA 등의 기술들은 로봇이 인간의 모호한 언어를 물리적 실체로 연결하는 ’마지막 1마일’을 연결하고 있습니다. 향후 연구는 이러한 개별 모듈들을 하나의 거대하고 유기적인 <strong>End-to-End 파운데이션 모델</strong>로 통합하는 방향으로 나아갈 것입니다. 로봇은 이제 단순히 명령을 수행하는 기계를 넘어, 인간과 맥락을 공유하고, 의도를 묻고, 스스로 확인하며 협업하는 사회적 파트너(Social Partner)로 진화하고 있습니다. 집필 중인 서적의 본 챕터는 이러한 기술적 진보의 최전선을 독자들에게 보여주는 중요한 이정표가 될 것입니다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>[PDF] CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory, https://www.semanticscholar.org/paper/CLIP-Fields%3A-Weakly-Supervised-Semantic-Fields-for-Shafiullah-Paxton/9cf66efb5ddc0eef574f909fd4e1fa09994c0184</li>
<li>VL-Fields: Towards Language-Grounded Neural Implicit Spatial Representations, https://tsagkas.github.io/vl-fields/</li>
<li>LERF: Language Embedded Radiance Fields - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2023/papers/Kerr_LERF_Language_Embedded_Radiance_Fields_ICCV_2023_paper.pdf</li>
<li>Rethinking Open-Vocabulary Segmentation of Radiance Fields in 3D Space - arXiv, https://arxiv.org/html/2408.07416v1</li>
<li>ConceptFusion: Open-set Multimodal 3D Mapping | alphaXiv, https://www.alphaxiv.org/overview/2302.07241v3</li>
<li>[2302.07241] ConceptFusion: Open-set Multimodal 3D Mapping - arXiv, https://arxiv.org/abs/2302.07241</li>
<li>[PDF] OpenScene: 3D Scene Understanding with Open Vocabularies - Semantic Scholar, https://www.semanticscholar.org/paper/OpenScene%3A-3D-Scene-Understanding-with-Open-Peng-Genova/774408d8848b129d93fb67548ec6571d99b31a2d</li>
<li>OpenSU3D: Open World 3D Scene Understanding using Foundation Models - arXiv, https://arxiv.org/html/2407.14279v2</li>
<li>RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes - arXiv, https://arxiv.org/html/2502.00392v2</li>
<li>[2512.06558] Embodied Referring Expression Comprehension in Human-Robot Interaction, https://arxiv.org/abs/2512.06558</li>
<li>Functional 3D Scene Graphs in AI &amp; Robotics, https://www.emergentmind.com/topics/functional-3d-scene-graphs</li>
<li>FunGraph: Functionality Aware 3D Scene Graphs for Language-Prompted Scene Interaction - arXiv, https://arxiv.org/html/2503.07909v1</li>
<li>3D Dynamic Scene Graphs: Actionable Spatial Perception with Places, Objects, and Humans - Robotics, https://www.roboticsproceedings.org/rss16/p079.pdf</li>
<li>Open-Vocabulary 3D Scene Graphs for Perception and Planning - ConceptGraphs, https://concept-graphs.github.io/assets/pdf/2023-ConceptGraphs.pdf</li>
<li>Event-Grounding Graph: Unified Spatio-Temporal Scene Graph from Robotic Observations - arXiv, https://arxiv.org/html/2510.18697v1</li>
<li>Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog, https://aclanthology.org/2025.emnlp-main.1145/</li>
<li>Learning in the Rational Speech Acts Model - The Stanford Natural Language Processing Group, https://www-nlp.stanford.edu/pubs/monroe2015learning.pdf</li>
<li>learning in the rational speech acts model a dissertation submitted to the department of computer science and the - Will Monroe, https://wmonroeiv.github.io/pubs/dissertation.pdf</li>
<li>(RSA)2: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding - ACL Anthology, https://aclanthology.org/2025.acl-long.1019.pdf</li>
<li>Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog - arXiv, https://arxiv.org/html/2507.14063v1</li>
<li>HandMeThat: Human-Robot Communication in Physical and Social Environments - arXiv, https://arxiv.org/html/2310.03779v1</li>
<li>HandMeThat: Human-Robot Communication in Physical and Social Environments - NeurIPS, https://proceedings.neurips.cc/paper_files/paper/2022/file/4eb33c53ed5b14ce9028309431f565cc-Paper-Datasets_and_Benchmarks.pdf</li>
<li>HandMeThat: Human-Robot Communication in Physical and Social Environments | OpenReview, https://openreview.net/forum?id=nUTemM6v9sv</li>
<li>Conformal Prediction for Semantically-Aware Autonomous Perception in Urban Environments - GitHub, https://raw.githubusercontent.com/mlresearch/v270/main/assets/doula25a/doula25a.pdf</li>
<li>Introspective Planning: Aligning Robots’ Uncertainty with Inherent Task Ambiguity - NIPS, https://proceedings.neurips.cc/paper_files/paper/2024/file/8451a20c5a7e0ee5671dda28f7daf7f3-Paper-Conference.pdf</li>
<li>Conformal prediction under ambiguous ground truth - Google Research, https://research.google/pubs/conformal-prediction-under-ambiguous-ground-truth/</li>
<li>Conformal Prediction for Natural Language Processing: A Survey - MIT Press Direct, https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00715/125278/Conformal-Prediction-for-Natural-Language</li>
<li>Seeing with Partial Certainty: Conformal Prediction for Robotic Scene Recognition in Built Environments - arXiv, https://arxiv.org/html/2501.04947v1</li>
<li>Conformalized Teleoperation: Confidently Mapping Human Inputs to High-Dimensional Robot Actions, https://www.roboticsproceedings.org/rss20/p008.pdf</li>
<li>Asking Follow-Up Clarifications to Resolve Ambiguities in Human-Robot Conversation, https://ieeexplore.ieee.org/document/9889368/</li>
<li>CLAM: Selective Clarification for Ambiguous Questions with Generative Language Models - OpenReview, https://openreview.net/pdf?id=VQWuqgSoVN</li>
<li>A Multimodal Data Collection Framework for Dialogue-Driven Assistive Robotics to Clarify Ambiguities: A Wizard-of-Oz Pilot Study - arXiv, https://arxiv.org/html/2601.16870v1</li>
<li>Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue - arXiv, https://arxiv.org/html/2509.15061v1</li>
<li>ActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation - arXiv, https://arxiv.org/html/2601.08325v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>