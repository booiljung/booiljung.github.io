<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:6.3 무엇이든 분할한다: SAM과 Grounding</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>6.3 무엇이든 분할한다: SAM과 Grounding</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 6. 오픈 보캐블러리와 시맨틱 이해 (Open-Vocabulary & Semantic Understanding)</a> / <a href="index.html">6.3 무엇이든 분할한다: SAM과 Grounding</a> / <span>6.3 무엇이든 분할한다: SAM과 Grounding</span></nav>
                </div>
            </header>
            <article>
                <h1>6.3 무엇이든 분할한다: SAM과 Grounding</h1>
<h2>1. 개방형 어휘 인식과 범용 시각 지능의 결합</h2>
<p>로봇 공학의 역사에서 시각적 인식은 오랜 시간 동안 사전에 정의된 특정 범주의 객체만을 식별하는 폐쇄형 집합(Closed-set)의 굴레에 갇혀 있었다. 공장 자동화 라인이나 통제된 실험실 환경에서는 특정 부품이나 색상을 인식하는 것만으로 충분했으나, 인간과 공존하는 비정형 환경에 투입되는 로봇에게는 완전히 새로운 지각 패러다임이 요구되었다. 이러한 요구에 부응하여 등장한 것이 오픈 보캐블러리(Open-Vocabulary) 인식 기술이며, 그 정점에는 이미지 내의 모든 객체를 정밀하게 분리해낼 수 있는 Segment Anything Model(SAM)과 자연어와 시각 정보를 정교하게 연결하는 Grounding 기술이 존재한다.</p>
<p>전통적인 방식의 이미지 분할(Segmentation) 모델은 학습 단계에서 주석(Annotation)이 달린 데이터셋의 범주를 벗어나지 못했다. 그러나 로봇이 “식탁 위에 있는 저 낡은 도자기 컵을 집어라“라는 명령을 수행하기 위해서는, 학습 데이터에 ’도자기 컵’이나 ’낡은’이라는 형용사가 포함되어 있지 않더라도 해당 객체의 정확한 물리적 경계를 추출해낼 수 있어야 한다. SAM은 이러한 문제를 해결하기 위해 프롬프트 가능(Promptable) 분할이라는 새로운 메커니즘을 도입했으며, 이는 로봇이 특정 좌표, 박스, 또는 텍스트 쿼리를 기반으로 환경 내의 어떤 요소든 즉각적으로 분리해낼 수 있는 ’시각적 파운데이션 모델’의 역할을 수행하게 한다.</p>
<p>하지만 SAM 자체는 객체의 의미(Semantics)를 완벽하게 이해하지 못하는 ‘Class-agnostic’ 모델이라는 한계를 가진다. 이를 보완하는 기술이 바로 Grounding이다. Grounding DINO와 같은 모델은 자연어 문장을 입력받아 이미지 내에서 해당 설명에 부합하는 영역을 바운딩 박스로 특정하며, 이를 SAM의 입력 프롬프트로 활용함으로써 로봇은 비로소 “무엇을(Grounding)” “어느 정도의 범위로(SAM)” 인식해야 하는지를 동시에 파악하게 된다. 이러한 기술적 결합은 로봇의 지각 능력을 단순한 객체 탐지를 넘어선 공간 지능(Spatial Intelligence)의 차원으로 격상시킨다.</p>
<h2>2. Segment Anything Model(SAM): 분할 기술의 새로운 기준</h2>
<h3>2.1 SAM의 아키텍처와 프롬프트 가능 분할 메커니즘</h3>
<p><code>Segment Anything</code> (Kirillov et al., 2023) 논문에서 제안된 SAM은 대규모 데이터 학습을 통해 제로샷(Zero-shot) 일반화 성능을 극대화한 이미지 분할 모델이다. SAM의 아키텍처는 효율적인 실시간 추론을 가능하게 하면서도 복잡한 시각적 맥락을 포착할 수 있도록 세 가지 주요 컴포넌트로 정교하게 설계되었다.</p>
<p>첫 번째는 이미지 인코더(Image Encoder)이다. 이는 강력한 Vision Transformer(ViT) 백본을 사용하여 입력 이미지를 고차원의 특징 임베딩으로 변환한다. 일반적으로 1024x1024 해상도의 이미지를 처리하며, 대규모 이미지 데이터셋에서 Masked Autoencoder(MAE) 방식으로 사전 학습된 ViT-H/16 모델을 주로 사용한다. 이 과정은 상대적으로 연산량이 많지만, 이미지 한 장에 대해 한 번만 수행하면 되므로 이후의 다양한 프롬프트 입력에 대해서는 빠른 반응 속도를 유지할 수 있다.</p>
<p>두 번째는 프롬프트 인코더(Prompt Encoder)이다. SAM은 점(Point), 상자(Box), 텍스트(Text)와 같은 성격이 다른 프롬프트를 동시에 수용한다. 점과 상자는 위치 인코딩(Positional Encoding)을 통해 표현되며, 텍스트 프롬프트는 CLIP(Contrastive Language-Image Pre-training)의 텍스트 인코더를 통해 시각적 임베딩 공간과 정렬된 벡터로 변환된다. 마스크(Mask) 프롬프트 역시 밀집(Dense) 임베딩 형태로 전달되어 이전 단계의 결과물을 정교화하는 데 사용된다.</p>
<p>세 번째는 마스크 디코더(Mask Decoder)이다. 이는 두 층의 Transformer 디코더 구조를 가지며, 이미지 임베딩과 프롬프트 임베딩 사이의 상호 주의집중(Cross-attention)을 수행한다. 특히 SAM은 분할 대상이 모호할 경우(예: 배낭의 끈만 분할할 것인지, 배낭 전체를 분할할 것인지)를 대비하여 단일 프롬프트에 대해 여러 개의 유효한 마스크를 동시에 예측하고, 각 마스크의 신뢰도를 IoU score로 출력하여 최적의 결과를 선택할 수 있게 한다.</p>
<table><thead><tr><th><strong>컴포넌트</strong></th><th><strong>기술적 핵심 사항</strong></th><th><strong>역할 및 특징</strong></th></tr></thead><tbody>
<tr><td>이미지 인코더</td><td>ViT-H/16, MAE Pre-training</td><td>이미지의 전역적 특징 추출 및 임베딩 생성</td></tr>
<tr><td>프롬프트 인코더</td><td>Positional Encoding, CLIP Text Encoder</td><td>사용자 입력을 공간/의미적 벡터로 변환</td></tr>
<tr><td>마스크 디코더</td><td>Light-weight Transformer, Multi-mask output</td><td>실시간 프롬프트 대응 및 마스크 생성 (50ms 이내)</td></tr>
<tr><td>손실 함수</td><td>Focal Loss + Dice Loss (\vert 20:1 \vert)</td><td>픽셀 단위의 정밀한 경계 보존</td></tr>
</tbody></table>
<h3>2.2 SA-1B 데이터 엔진: 10억 개의 마스크가 만드는 지능</h3>
<p>SAM이 기존 모델들과 궤를 달리하는 강력한 일반화 성능을 보유하게 된 배경에는 <code>SA-1B</code> 데이터셋의 존재가 결정적이다. Meta AI 연구팀은 약 1,100만 장의 고해상도 이미지로부터 11억 개 이상의 마스크를 수집하였으며, 이는 기존에 존재하던 최대 규모의 분할 데이터셋보다 마스크 수 기준으로 400배 이상 큰 규모이다. 이러한 방대한 데이터를 구축하기 위해 연구팀은 ’데이터 엔진(Data Engine)’이라 불리는 세 단계의 주석 파이프라인을 구축했다.</p>
<p>초기 단계인 ‘지원-수동(Assisted-Manual)’ 단계에서는 작업자가 SAM의 초기 버전을 사용하여 마스크를 생성하고 이를 수정했다. 이 과정에서 SAM의 성능이 점차 개선됨에 따라 마스크 하나당 소요되는 작업 시간이 34초에서 14초로 단축되었다. 두 번째 ‘반자동(Semi-Automatic)’ 단계에서는 모델이 확실하게 인식하는 객체들의 마스크를 먼저 생성한 뒤, 작업자가 모델이 놓친 모호한 객체들에 집중하여 마스크를 추가함으로써 데이터의 다양성을 높였다. 마지막 ‘완전 자동(Fully Automatic)’ 단계에서는 정교해진 모델이 이미지 전체에 격자 형태의 포인트 프롬프트를 배치하여 모든 가능한 영역을 스스로 분할해냈다.</p>
<p>이러한 데이터 중심 AI(Data-centric AI) 접근법은 SAM이 특정 도메인에 국한되지 않고, 현미경 사진에서부터 위성 영상, 복잡한 로봇 작업장 환경에 이르기까지 어떠한 시각적 입력에 대해서도 강건하게 동작하는 원동력이 되었다. 로봇 공학 관점에서 이는 새로운 환경에 로봇을 투입할 때마다 별도의 레이블링이나 재학습이 필요 없는 제로샷 인식을 실현했다는 점에서 매우 중요한 진전이다.</p>
<h2>3. Grounding DINO: 언어와 시각의 밀결합을 통한 객체 탐지</h2>
<h3>3.1 시각적 접지(Visual Grounding)의 메커니즘</h3>
<p>로봇에게 자연어 명령을 내릴 때, 모델은 “창가 옆에 있는 빨간색 소파“라는 추상적 설명을 이미지 상의 구체적인 픽셀 영역과 연결해야 한다. 이 과정을 시각적 접지(Visual Grounding)라고 한다. <code>Grounding DINO: Marrying DINO with Grounded Pre-training for Open-Set Object Detection</code> (Liu et al., 2023)은 Transformer 기반의 고성능 탐지 모델인 DINO를 확장하여, 언어 정보를 모델의 모든 계층에서 융합하는 방식을 제안했다.</p>
<p>기존의 오픈 보캐블러리 탐지기들이 이미지 특징과 텍스트 특징을 마지막 단계에서 결합하는 방식(Late Fusion)을 취했다면, Grounding DINO는 특징 추출 단계에서부터 두 모달리티(Modality)를 긴밀하게 통합하는 ‘타이트 퓨전(Tight Fusion)’ 방식을 채택했다. 이는 모델이 텍스트 명령을 먼저 읽고, 그 정보를 바탕으로 이미지 내에서 관련성이 높은 시각적 특징을 더 집중적으로 찾게 만드는 인간의 주의 집중 방식과 유사하다.</p>
<h3>3.2 Grounding DINO의 아키텍처 상세</h3>
<p>Grounding DINO의 구조는 크게 이미지와 텍스트를 각각 처리하는 백본, 두 정보를 융합하는 피처 인핸서(Feature Enhancer), 그리고 최종 탐지를 수행하는 디코더로 나뉜다.</p>
<ol>
<li><strong>이미지 및 텍스트 백본:</strong> 이미지 처리를 위해 Swin Transformer를, 텍스트 처리를 위해 BERT를 주로 사용한다. 이들은 각각의 모달리티에서 원시 데이터를 고수준의 특징 벡터로 변환한다.</li>
<li><strong>피처 인핸서(Feature Enhancer):</strong> 이 모듈은 변형 가능한 자기 주의집중(Deformable Self-attention)과 함께 이미지-텍스트 교차 주의집중을 수행한다. 여기서 시각적 특징은 텍스트 프롬프트에 의해 가중치가 조절되며, 반대로 텍스트 특징도 이미지 내의 객체 정보를 반영하여 업데이트된다. 이 과정은 “파란색 모자를 쓴 사람“과 같은 복잡한 속성 조합을 인식하는 데 결정적인 역할을 한다.</li>
<li><strong>언어 유도 쿼리 선택(Language-guided Query Selection):</strong> 디코더가 객체를 찾기 위해 사용하는 ’쿼리’들을 초기화할 때, 입력된 텍스트와 가장 유사도가 높은 이미지 특징 영역을 우선적으로 선택한다. 이는 모델이 배경 정보에 현혹되지 않고 명령받은 대상에 집중하게 만든다.</li>
<li><strong>교차 모달리티 디코더(Cross-modality Decoder):</strong> 마지막 단계에서 디코더는 이미지와 텍스트 특징을 모두 참조하여 바운딩 박스의 위치와 해당 영역이 텍스트 설명과 일치하는 정도(Similarity score)를 계산한다.</li>
</ol>
<p>Grounding DINO는 이러한 설계를 통해 COCO 데이터셋에서 학습 과정에 포함되지 않은 카테고리에 대해서도 52.5 AP라는 놀라운 제로샷 성능을 기록했으며, 이는 GLIP이나 OWL-ViT와 같은 이전 세대 모델들을 크게 상회하는 수치이다.</p>
<h2>4. Grounded-SAM: 인식과 분할의 통합 파이프라인</h2>
<h3>4.1 모듈형 아키텍처의 강력함</h3>
<p>로봇 공학에서 인식 시스템의 완성은 ’무엇인지 아는 것(Classification)’과 ’어디까지인지 아는 것(Segmentation)’의 조화로 이루어진다. SAM은 정밀한 경계를 찾아내지만 객체의 이름을 알지 못하고, Grounding DINO는 텍스트를 통해 객체를 찾지만 대략적인 박스 형태만 제공한다. 이 둘을 체인으로 연결한 <code>Grounded-SAM</code>은 로봇이 자연어 명령만으로 이미지 내의 어떤 객체든 픽셀 단위로 정확하게 분리해낼 수 있는 강력한 파이프라인을 형성한다.</p>
<p>Grounded-SAM의 작동 방식은 직관적이면서도 강력하다. 사용자가 “바닥에 떨어진 사과“라고 입력하면, Grounding DINO가 해당 사과의 위치를 상자(Box) 형태로 찾아낸다. 이 상자 정보는 그대로 SAM의 프롬프트로 전달되며, SAM은 상자 내부의 픽셀들을 분석하여 사과의 정밀한 마스크를 생성한다. 이러한 구조는 각 모델을 독립적으로 최적화하거나 교체할 수 있는 모듈성을 제공한다. 예를 들어, 더 높은 품질의 마스크가 필요하다면 <code>HQ-SAM</code>을 적용할 수 있고, 실시간성이 중요하다면 <code>MobileSAM</code>으로 교체할 수 있다.</p>
<table><thead><tr><th><strong>단계</strong></th><th><strong>모델 역할</strong></th><th><strong>입력 데이터</strong></th><th><strong>출력 데이터</strong></th><th><strong>로봇 제어에서의 활용</strong></th></tr></thead><tbody>
<tr><td>1. 탐지</td><td>Grounding DINO</td><td>RGB 이미지 + 텍스트 프롬프트</td><td>바운딩 박스 (Bbox)</td><td>관심 객체(ROI) 선별 및 위치 파악</td></tr>
<tr><td>2. 분할</td><td>SAM (또는 HQ-SAM)</td><td>RGB 이미지 + 바운딩 박스</td><td>이진 마스크 (Mask)</td><td>정밀 파지 지점 계산 및 장애물 경계 확정</td></tr>
<tr><td>3. 추적</td><td>SAM 2 (선택 사항)</td><td>비디오 프레임 + 초기 마스크</td><td>연속적 마스크 추적</td><td>움직이는 객체 조작 및 동적 회피</td></tr>
</tbody></table>
<h3>4.2 확장된 시각 이해 시스템으로의 발전</h3>
<p>Grounded-SAM은 단순히 탐지와 분할에 그치지 않고, 다양한 파운데이션 모델들과 결합하여 더 고차원적인 시각 지능을 구현한다. <code>BLIP</code>과 결합하면 이미지 내의 객체들을 스스로 설명하고 그 설명에 따라 다시 분할하는 ’자동 주석 시스템’이 되며, <code>RAM(Recognize Anything Model)</code>과 결합하면 이미지 내의 모든 사물에 태그를 달고 분할하는 조밀한 인식이 가능해진다.</p>
<p>특히 로봇의 작업 공간에서 이 파이프라인은 <code>Stable Diffusion</code>과 결합하여 시뮬레이션 데이터를 생성하거나 가상의 시나리오를 편집하는 데 활용될 수 있다. 로봇이 인식한 객체 영역을 SAM으로 마스킹하고, 그 자리에 다른 물체를 생성하여 넣음으로써 로봇은 가상의 환경에서 더 다양한 학습 데이터를 확보할 수 있게 된다. 이러한 확장은 로봇 지능이 단순히 ’보는 것’을 넘어 ‘이해하고 상상하는’ 단계로 나아가고 있음을 시사한다.</p>
<h2>5. 로봇 작업 지능으로의 전이: 파지와 조작</h2>
<h3>5.1 AnyGrasp와 SAM의 시너지</h3>
<p>로봇이 임의의 물체를 집어 올리는 파지(Grasping) 작업은 로봇 공학의 가장 기본적이면서도 어려운 과제 중 하나이다. 특히 훈련 단계에서 본 적 없는 새로운 물체를 다뤄야 할 때, 단순한 객체 탐지만으로는 부족하다. <code>AnyGrasp</code>는 점구름(Point Cloud) 데이터를 기반으로 7자유도(7-DoF)의 파지 포즈를 제안하는 모델로, SAM과 결합했을 때 그 진가가 드러난다.</p>
<p>전통적인 파지 시스템은 전체 장면의 점구름에서 파지 가능한 후보군을 찾는다. 이 과정에서 바닥이나 벽, 혹은 원치 않는 인접 물체를 잡으려는 오류가 발생하기 쉽다. 하지만 SAM을 통해 사용자가 원하는 물체만 픽셀 단위로 분리해내면, 로봇은 해당 마스크 영역에 해당하는 점구름만 필터링하여 AnyGrasp에 입력할 수 있다. 이는 파지 후보군 탐색의 범위를 극적으로 좁혀주며, 정확한 물체의 기하학적 중심과 표면 법선(Normal) 정보를 제공하여 파지 성공률을 획기적으로 높인다.</p>
<h3>5.2 부위 기반 조작(Part-based Manipulation)</h3>
<p>복잡한 물체를 다룰 때 로봇은 물체 전체가 아닌 특정 부위(Part)를 겨냥해야 할 때가 많다. 예를 들어, 가방을 옮기려면 가방 자체가 아니라 ’가방 손잡이’를 잡아야 한다. <code>AnyPart</code> 프레임워크는 이러한 문제를 해결하기 위해 오픈 보캐블러리 파트 분할을 도입했다. Grounding DINO가 ’가방’을 찾으면, SAM 기반의 파트 분할기가 그 안에서 ‘손잡이’ 영역을 다시 세밀하게 찾아낸다.</p>
<p>이러한 단계별 접근 방식은 로봇이 “주전자의 주둥이가 아닌 손잡이를 잡아라“와 같은 정교한 자연어 명령을 물리적 행동으로 변환할 수 있게 한다. 연구 결과에 따르면, 이러한 모듈형 파운데이션 모델의 결합은 추가적인 재학습 없이도 실제 로봇 팔 실험에서 매우 높은 성공률을 보였으며, 이는 로봇이 인간의 언어적 지시를 물리적 제약 조건과 연결하는 강력한 수단을 얻었음을 의미한다.</p>
<h2>6. 공간 지능의 확장: 시맨틱 매핑과 탐색</h2>
<h3>6.1 ConceptFusion: 멀티모달 3D 지도 구축</h3>
<p>로봇이 미지의 공간을 탐색하며 지도를 만들 때, 기하학적 구조만을 기록하는 것은 고차원적인 임무 수행에 한계가 있다. <code>ConceptFusion</code> (Jatavallabhula et al., 2023)은 SAM과 CLIP의 기능을 3D SLAM(Simultaneous Localization and Mapping)과 결합하여, 텍스트, 이미지, 심지어 오디오로도 쿼리가 가능한 오픈 세트 시맨틱 지도를 구축한다.</p>
<p>이 기술의 핵심은 ’픽셀 정렬 특징(Pixel-aligned Features)’의 생성이다. 로봇은 주행하며 수집하는 매 프레임 이미지에 대해 SAM을 실행하여 객체 단위의 영역(Region)들을 나눈다. 각 영역은 CLIP 인코더를 통해 의미론적 임베딩 벡터로 변환되며, 이 벡터들은 3D 공간상의 점(Point)들과 연계되어 저장된다. 결과적으로 로봇이 구축한 지도는 단순한 점들의 집합이 아니라, 각 점이 어떤 의미를 지니는지에 대한 정보를 담고 있는 ’의미론적 신경장’이 된다.</p>
<table><thead><tr><th><strong>지도 구성 요소</strong></th><th><strong>기술적 구현 방식</strong></th><th><strong>제공하는 기능</strong></th></tr></thead><tbody>
<tr><td>기하학적 정보</td><td>Depth Camera, LiDAR, SLAM</td><td>공간의 물리적 경계 및 로봇 위치 파악</td></tr>
<tr><td>시맨틱 정보</td><td>SAM + CLIP + DINO</td><td>객체 단위의 의미론적 벡터 저장</td></tr>
<tr><td>멀티모달 쿼리</td><td>Audio, Text, Image Embeddings</td><td>“여기 소리 나는 곳으로 가줘” 또는 “이 사진과 같은 물건 찾아줘” 기능</td></tr>
<tr><td>공간 추론</td><td>LLM + Scene Graph</td><td>객체 간의 관계(예: 책상 위의 노트북) 이해 및 계획 수립</td></tr>
</tbody></table>
<h3>6.2 시맨틱 내비게이션과 장애물 회피</h3>
<p>SAM과 Grounding의 결합은 로봇의 주행 지능에도 혁신을 가져왔다. 기존의 장애물 회피 시스템은 센서에 걸리는 모든 물체를 ’갈 수 없는 영역’으로 간주했다. 하지만 Grounding 기술을 통해 로봇은 장애물의 정체를 파악할 수 있다. 예를 들어, 로봇은 앞에 놓인 것이 “밟으면 안 되는 유리병“인지, 아니면 “그냥 지나가도 안전한 마른 낙엽“인지를 구분하여 경로를 계획할 수 있다.</p>
<p>또한 <code>Grounded-SAM 2</code>는 동적인 환경에서 더욱 강력한 성능을 발휘한다. SAM 2는 비디오 상에서 객체의 마스크를 실시간으로 추적(Tracking)할 수 있는 능력을 갖추고 있어, 로봇은 움직이는 사람이나 다른 로봇의 경계를 정확히 유지하며 안전 거리를 확보할 수 있다. 이러한 연속적인 시각적 접지는 로봇이 복잡하고 유동적인 환경에서도 일관된 상황 인식을 유지하게 해준다.</p>
<h2>7. 엣지 로봇을 위한 최적화와 경량화 전략</h2>
<h3>7.1 연산량의 장벽과 지식 증류</h3>
<p>SAM과 Grounding DINO는 뛰어난 성능을 자랑하지만, 수억 개의 매개변수를 가진 거대 모델이기 때문에 로봇의 온보드 컴퓨터에서 실시간으로 구동하기에는 많은 제약이 따른다. 특히 SAM의 이미지 인코더인 ViT-H는 고성능 GPU에서도 상당한 연산 시간을 소요한다. 이를 해결하기 위해 로봇 공학계에서는 성능 손실을 최소화하면서 속도를 높인 경량화 모델들이 활발히 연구되고 있다.</p>
<p><code>MobileSAM</code>은 거대한 이미지 인코더를 Tiny-ViT로 대체하여 매개변수 수를 60배 이상 줄이면서도 SAM과 유사한 분할 능력을 유지한다. 또한 <code>NanoSAM</code>은 지식 증류(Knowledge Distillation) 기술을 사용하여 가벼운 ResNet-18 기반 인코더가 SAM의 특징을 학습하도록 설계되었으며, NVIDIA Jetson 플랫폼에서 TensorRT를 통해 실시간 속도를 달성했다. 이러한 경량화는 로봇이 복잡한 인식 과정을 위해 클라우드 서버에 의존하지 않고, 현장에서 즉각적인 판단을 내릴 수 있게 함으로써 통신 지연으로 인한 안전 사고를 방지한다.</p>
<h3>7.2 실시간 제어 루프와 FPS의 중요성</h3>
<p>로봇이 인간과 상호작용하거나 빠른 움직임을 제어하기 위해서는 지각 모듈이 최소 10 FPS 이상의 응답 속도를 보여야 한다. 연구 데이터에 따르면, NVIDIA Jetson AGX Orin 플랫폼에서 최적화된 <code>EfficientViT-SAM</code> 변형 모델들은 정밀도 설정에 따라 약 8ms에서 77ms 사이의 지연 시간을 기록했다. 이는 인식 결과가 제어 루프에 통합되어 물리적 행동으로 이어지기에 충분한 속도이다.</p>
<table><thead><tr><th><strong>모델 변체</strong></th><th><strong>추론 정밀도</strong></th><th><strong>지연 시간(ms)</strong></th><th><strong>mIoU (정확도)</strong></th><th><strong>특징</strong></th></tr></thead><tbody>
<tr><td>EfficientViT-SAM-L0</td><td>FP16 (TensorRT)</td><td>7.88</td><td>상대적 낮음</td><td>초고속 실시간 제어용</td></tr>
<tr><td>EfficientViT-SAM-XL1</td><td>FP32 (TensorRT)</td><td>77.8</td><td>매우 높음</td><td>정밀 조작 및 분석용</td></tr>
<tr><td>NanoSAM</td><td>FP16 (Mixed)</td><td>중간 수준</td><td>안정적</td><td>높은 신뢰도 및 균형 잡힌 성능</td></tr>
</tbody></table>
<h2>8. 미래 전망: 지각을 넘어 행동의 핵심으로</h2>
<h3>8.1 VLA 모델로의 통합</h3>
<p>SAM과 Grounding 기술의 미래는 단순한 시각적 도구를 넘어, 로봇의 지각-언어-행동을 통합하는 VLA(Vision-Language-Action) 모델의 핵심 구성 요소로 통합되는 방향으로 향하고 있다. <code>RT-2</code>와 같은 모델에서 볼 수 있듯이, 로봇은 이제 시각적으로 접지된 정보를 바탕으로 별도의 행동 계획 알고리즘 없이도 직접적인 제어 명령을 생성하기 시작했다. 이 과정에서 SAM이 제공하는 정밀한 객체 경계와 Grounding DINO의 문맥 이해 능력은 LLM이 현실 세계의 물리적 법칙을 이해하고 ’구체화된 지능(Embodied Intelligence)’으로 거듭나게 하는 필수적인 데이터 소스가 된다.</p>
<h3>8.2 논리적 추론과 도구 활용의 결합</h3>
<p>최신 연구들은 <code>Grounded-SAM</code>을 LLM 에이전트와 결합하여, 로봇이 스스로 인식 도구를 선택하고 결과를 검증하는 방식으로 발전시키고 있다. 예를 들어, 로봇이 “냉장고에서 가장 차가운 음료를 가져와라“라는 명령을 받으면, LLM은 먼저 시각적 탐지기를 통해 음료들을 찾고, SAM을 통해 각 음료의 위치를 확정한 뒤, 시뮬레이션이나 센서 데이터를 통해 ’차가움’이라는 속성을 추론하여 최종 행동을 결정한다.</p>
<p>이러한 ‘시각 프로그래밍’ 패러다임에서 SAM과 Grounding은 로봇이 세상을 분해하여 이해할 수 있는 원자적 기능(Primitive)을 제공한다. 지능형 로봇은 이러한 기능들을 조합하여 복잡하고 긴 호흡의 임무(Long-horizon Tasks)를 스스로 설계하고 실행할 수 있게 될 것이다. 결국 “무엇이든 분할“하고 “어떤 말이든 위치를 찾는” 이 기술들은 로봇이 인간의 환경에 적응하는 단계를 넘어, 인간과 진정으로 협력할 수 있는 동반자가 되기 위한 필수적인 시각적 문해력을 완성해주고 있다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>Segment Anything Model (SAM) for Zero-Shot Segmentation, https://www.emergentmind.com/topics/segment-anything-model-sam</li>
<li>Marrying DINO with Grounded Pre-Training for Open-Set Object …, https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06319.pdf</li>
<li>Grounded SAM: Open-Vocab Vision Pipeline - Emergent Mind, https://www.emergentmind.com/topics/groundedsam</li>
<li>Segment Anything Model (SAM) - Ultralytics YOLO Docs, https://docs.ultralytics.com/models/sam/</li>
<li>Real-time open-vocabulary perception for mobile robots … - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1693988/full</li>
<li>Grounded SAM 2: From Open-Set Detection to Segmentation and …, https://pyimagesearch.com/2026/01/19/grounded-sam-2-from-open-set-detection-to-segmentation-and-tracking/</li>
<li>Segment Anything in High Quality - NeurIPS, https://proceedings.neurips.cc/paper_files/paper/2023/file/5f828e38160f31935cfe9f67503ad17c-Paper-Conference.pdf</li>
<li>Segment Anything - CVF Open Access - The Computer Vision …, https://openaccess.thecvf.com/content/ICCV2023/papers/Kirillov_Segment_Anything_ICCV_2023_paper.pdf</li>
<li>SA-1B Dataset: Segmentation Benchmark - Emergent Mind, https://www.emergentmind.com/topics/sa-1b-dataset</li>
<li>Marrying DINO with Grounded Pre-training for Open … - BibSonomy, https://www.bibsonomy.org/bibtex/16696787c57d7eef30bcfe68f587e114f</li>
<li>Grounding DINO for Open-Set Object Detection Debuts, https://deepdataspace.com/en/blog/14/</li>
<li>Grounding DINO: Open Vocabulary Object Detection on Videos, https://pyimagesearch.com/2025/12/08/grounding-dino-open-vocabulary-object-detection-on-videos/</li>
<li>Fine-Tuning Grounding DINO: Open-Vocabulary Object Detection, https://learnopencv.com/fine-tuning-grounding-dino/</li>
<li>Automatic Labeling With GroundingDino | by Lihi Gur Arie, PhD, https://medium.com/data-science/automatic-labeling-of-object-detection-datasets-using-groundingdino-b66c486656fe</li>
<li>Grounded SAM: Assembling Open-World Models for Diverse Visual …, https://arxiv.org/html/2401.14159v1</li>
<li>Open Scene Understanding: Grounded Situation Recognition Meets …, https://openaccess.thecvf.com/content/ICCV2023W/ACVR/papers/Liu_Open_Scene_Understanding_Grounded_Situation_Recognition_Meets_Segment_Anything_for_ICCVW_2023_paper.pdf</li>
<li>Grounded SAM - Assembling Open-World Models For Diverse …, https://www.scribd.com/document/854614172/Grounded-SAM-Assembling-Open-World-Models-for-Diverse-Visual-Tasks</li>
<li>SAM+Grounding Dino+Stable diffusion = Segment ,Detect ,Change, https://medium.com/@amir_shakiba/sam-grounding-dino-stable-diffusion-segment-detect-change-da7926947286</li>
<li>AnyGrasp: Robust and Efficient Grasp Perception in Spatial and …, https://ieeexplore.ieee.org/iel7/8860/4359257/10167687.pdf</li>
<li>ok-robot/ok-robot-manipulation/README.md at main - GitHub, https://github.com/ok-robot/ok-robot/blob/main/ok-robot-manipulation/README.md</li>
<li>Open-Vocabulary Part-Based Grasping - arXiv, https://arxiv.org/html/2406.05951v2</li>
<li>HiFi-CS: Towards Open Vocabulary Visual Grounding For Robotic …, https://arxiv.org/html/2409.10419v1</li>
<li>Open-set Multimodal 3D Mapping - ConceptFusion, https://concept-fusion.github.io/assets/pdf/2023-ConceptFusion.pdf</li>
<li>ConceptFusion: Open-set Multimodal 3D Mapping - Semantic Scholar, https://www.semanticscholar.org/paper/ConceptFusion%3A-Open-set-Multimodal-3D-Mapping-Jatavallabhula-Kuwajerwala/5e2bceb56f116e98baf7e418208057bc0e1c1861</li>
<li>ConceptFusion: Open-set Multimodal 3D Mapping | alphaXiv, https://www.alphaxiv.org/overview/2302.07241v3</li>
<li>Semantic Mapping for Autonomous Navigation and Exploration, https://www.ri.cmu.edu/app/uploads/2021/08/cmuthesis_template.pdf</li>
<li>Small Obstacle Avoidance Based on RGB-D Semantic Segmentation, https://www.researchgate.net/publication/335564903_Small_Obstacle_Avoidance_Based_on_RGB-D_Semantic_Segmentation</li>
<li>Visual Programming for Zero-shot Open-Vocabulary 3D Visual …, https://openaccess.thecvf.com/content/CVPR2024/papers/Yuan_Visual_Programming_for_Zero-shot_Open-Vocabulary_3D_Visual_Grounding_CVPR_2024_paper.pdf</li>
<li>LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large …, https://openreview.net/pdf/554fd4d6bae70c686ab544a4dcfb243fde04a82b.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>