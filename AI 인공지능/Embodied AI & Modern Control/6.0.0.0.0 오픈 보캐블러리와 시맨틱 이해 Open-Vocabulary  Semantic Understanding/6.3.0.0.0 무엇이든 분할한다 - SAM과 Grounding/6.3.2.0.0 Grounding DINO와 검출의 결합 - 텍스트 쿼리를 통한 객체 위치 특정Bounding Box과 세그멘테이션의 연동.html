<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:6.3.2 Grounding DINO와 검출의 결합: 텍스트 쿼리를 통한 객체 위치 특정(Bounding Box)과 세그멘테이션의 연동.</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>6.3.2 Grounding DINO와 검출의 결합: 텍스트 쿼리를 통한 객체 위치 특정(Bounding Box)과 세그멘테이션의 연동.</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 6. 오픈 보캐블러리와 시맨틱 이해 (Open-Vocabulary & Semantic Understanding)</a> / <a href="index.html">6.3 무엇이든 분할한다: SAM과 Grounding</a> / <span>6.3.2 Grounding DINO와 검출의 결합: 텍스트 쿼리를 통한 객체 위치 특정(Bounding Box)과 세그멘테이션의 연동.</span></nav>
                </div>
            </header>
            <article>
                <h1>6.3.2 Grounding DINO와 검출의 결합: 텍스트 쿼리를 통한 객체 위치 특정(Bounding Box)과 세그멘테이션의 연동.</h1>
<p>로봇이 복잡하고 변화무쌍한 실제 환경에서 자율적으로 임무를 수행하기 위해서는 주변 환경의 사물을 단순히 기하학적으로 인식하는 수준을 넘어, 인간의 언어와 시각 정보를 일관성 있게 정렬하여 이해하는 능력이 필수적이다. 전통적인 객체 검출 시스템은 사전에 정의된 유한한 범주의 데이터셋에서 학습된 폐쇄형 집합(Closed-set) 인식에 의존해 왔으며, 이는 훈련 데이터에 포함되지 않은 새로운 객체를 만났을 때 로봇의 대응 능력을 심각하게 제한하는 요소로 작용했다. 이러한 한계를 극복하기 위해 등장한 기술이 바로 개방형 어휘(Open-Vocabulary) 객체 검출이며, 그 중심에는 트랜스포머 기반의 강력한 검출기인 DINO와 언어 기반의 그라운딩(Grounded Pre-training) 기술을 결합한 Grounding DINO가 있다. Grounding DINO는 인간의 자연어 입력을 바탕으로 이미지 내의 임의의 객체를 실시간으로 찾아내고 그 위치를 바운딩 박스(Bounding Box)로 특정하며, 이를 SAM(Segment Anything Model)과 연동함으로써 정교한 픽셀 단위의 세그멘테이션 마스크까지 생성하는 통합적 시각 인지 파이프라인을 구축한다.</p>
<h2>1. Grounding DINO의 아키텍처와 다중 양상 융합의 기술적 정수</h2>
<p>Grounding DINO의 핵심 설계 원칙은 시각 정보와 언어 정보를 모델의 모든 추론 단계에서 긴밀하게 결합하는 ‘Tight Fusion’ 전략에 있다. 단순히 시각 모델의 출력과 언어 모델의 출력을 마지막 단계에서 결합하는 방식(Late Fusion)이 아니라, 특징 추출 초기 단계부터 상호 작용을 유도함으로써 복잡한 지시문이나 세밀한 속성 묘사에 대해서도 높은 강건성을 확보한다.</p>
<h3>1.1 이중 인코더 기반의 특징 추출 및 텍스트 백본</h3>
<p>모델의 아키텍처는 크게 이미지 특징을 추출하는 시각 백본(Vision Backbone)과 텍스트 쿼리를 처리하는 텍스트 백본(Text Backbone)으로 나뉜다. 시각 백본으로는 주로 Swin Transformer가 채택되어 다양한 스케일의 시각 특징 맵을 생성하며, 텍스트 백본으로는 BERT 계열의 사전 학습된 언어 모델을 사용하여 입력된 자연어 문장을 고차원 임베딩 공간으로 투영한다. 이 단계에서 생성된 시각 및 언어 특징들은 서로 독립적인 상태이지만, 이후 진행되는 다단계 융합 과정을 통해 하나의 공통된 의미 공간(Semantic Space)으로 정렬된다.</p>
<h3>1.2 특징 강화 장치(Feature Enhancer)의 다중 양상 상호작용</h3>
<p>Grounding DINO의 성능을 결정짓는 가장 중요한 모듈 중 하나는 ‘목(Neck)’ 부분에 위치한 특징 강화 장치(Feature Enhancer)이다. 이 모듈은 변형 가능한 자기 주의 집중(Deformable Self-attention)과 더불어, 이미지와 텍스트 간의 양방향 상호 주의 집중(Bi-directional Cross-attention) 레이어를 수직으로 쌓아 올린 구조를 가진다. 시각 특징은 텍스트 쿼리에 포함된 단어들과 대조되며 특정 객체의 존재 가능성이 높은 영역을 강조하고, 반대로 텍스트 특징은 이미지 내의 시각적 맥락에 따라 그 의미적 가중치가 조정된다. 이러한 과정은 로봇이 “테이블 위에 놓인 빨간색 머그컵“과 같은 구체적인 지시를 받았을 때, ‘테이블’, ‘위’, ‘빨간색’, ’머그컵’이라는 각 단어 토큰이 이미지의 실제 픽셀 영역과 공간적으로 조응하게 만드는 기술적 토대가 된다.</p>
<h3>1.3 언어 가이드 쿼리 선택(Language-Guided Query Selection)</h3>
<p>기존의 DETR이나 DINO 모델이 고정된 수의 학습 가능한 객체 쿼리(Object Queries)를 사용했던 것과 달리, Grounding DINO는 텍스트 입력의 가이드를 받아 동적으로 쿼리를 초기화하는 방식을 도입한다. 특징 강화 장치를 통과한 이미지 특징들 중 입력된 텍스트 토큰과의 유사도가 가장 높은 상위 <span class="math math-inline">N</span>개의 영역을 선별하여 디코더의 초기 쿼리로 사용하는 것이다. 이 메커니즘은 로봇의 실시간 추론 시 불필요한 영역에 대한 연산을 배제하고, 인간의 지시와 가장 밀접한 관련이 있는 영역에 모델의 어텐션을 집중시킴으로써 제로샷 환경에서의 검출 정확도를 비약적으로 향상시킨다. 다음 표는 Grounding DINO의 주요 구성 요소와 설정을 요약한 것이다.</p>
<table><thead><tr><th><strong>구성 요소</strong></th><th><strong>기술적 특징 및 역할</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td>Image Backbone</td><td>Swin Transformer 기반 다중 스케일 특징 추출</td><td>ResNet 등 교체 가능</td></tr>
<tr><td>Text Backbone</td><td>BERT 기반 텍스트 임베딩 생성</td><td>256 토큰 제한</td></tr>
<tr><td>Feature Enhancer</td><td>Deformable Self-attention + Bi-directional Cross-attention</td><td>연산량의 약 70% 차지</td></tr>
<tr><td>Query Selection</td><td>텍스트 유사도 기반 상위 900개 쿼리 동적 선택</td><td><code>num_queries</code> 파라미터 조절</td></tr>
<tr><td>Decoder</td><td>self, image-to-query, text-to-query attention 순차 수행</td><td>교차 양상 반복 정제</td></tr>
</tbody></table>
<h2>2. 수학적 정식화와 학습 최적화 전략</h2>
<p>Grounding DINO의 강력한 객체 위치 특정 능력은 정교하게 설계된 손실 함수(Loss Function)와 대규모 그라운딩 데이터셋을 통한 사전 학습 과정에서 기인한다. 모델은 시각적 영역(Region)과 언어적 단어(Word) 사이의 정렬을 최적화하기 위해 대조 학습(Contrastive Learning) 프레임워크를 기반으로 한다.</p>
<h3>2.1 서브 문장 레벨의 텍스트 표현(Sub-Sentence Level Representation)</h3>
<p>Grounding DINO의 독창적인 시도 중 하나는 입력된 텍스트 쿼리를 처리할 때 문장 전체를 하나의 벡터로 다루지 않고, 의미 단위인 ‘서브 문장(Sub-sentence)’ 레벨로 분할하여 처리한다는 점이다. 이는 “강아지를 산책시키는 사람“과 같은 긴 지시문에서 ’강아지’와 ’사람’이라는 개별 객체를 명확히 구분하면서도, ’산책시키는’과 같은 관계적 맥락을 유지하기 위함이다. 이를 위해 모델은 블록 대각 주의 집중 마스크(Block-diagonal Attention Mask)를 사용하여 서로 무관한 단어 간의 간섭(Semantic Bleed)을 원천적으로 차단한다. 이러한 구조는 로봇이 복잡한 지시를 수행할 때 발생할 수 있는 의미적 모호성을 제거하는 데 결정적인 역할을 한다.</p>
<h3>2.2 다중 목적 손실 함수의 구성</h3>
<p>모델의 학습은 위치 특정 손실(Localization Loss)과 대조적 그라운딩 손실(Contrastive Grounding Loss)의 가중 합으로 이루어진다. 위치 특정 손실은 예측된 바운딩 박스와 정답 박스 간의 <span class="math math-inline">L_1</span> 거리와 객체의 겹침 정도를 측정하는 GIoU(Generalized Intersection over Union) 손실로 구성된다. 그라운딩 손실은 각 쿼리가 어떤 단어 토큰에 대응되는지를 분류하는 문제로 취급하며, 예측된 유사도 점수에 대해 Focal Loss를 적용하여 클래스 불균형 문제를 완화한다.</p>
<p>전체 학습을 위한 손실 함수 <span class="math math-inline">\mathcal{L}</span>은 다음과 같이 정의된다:<br />
<span class="math math-display">
\mathcal{L} = \lambda_{cls}\mathcal{L}_{cls} + \lambda_{L1}\mathcal{L}_{L1} + \lambda_{giou}\mathcal{L}_{giou}
</span><br />
여기서 <span class="math math-inline">\mathcal{L}*{cls}</span>는 쿼리-텍스트 유사도에 대한 Focal Contrastive Loss를 의미하며, <span class="math math-inline">\mathcal{L}*{L1}</span>과 <span class="math math-inline">\mathcal{L}_{giou}</span>는 바운딩 박스 회귀를 위한 손실이다. 각 손실 항에 적용되는 하이퍼파라미터 가중치는 모델의 수렴 안정성을 위해 세밀하게 조정된다.</p>
<table><thead><tr><th><strong>손실 항목</strong></th><th><strong>수식 및 계산 방식</strong></th><th><strong>가중치 설정 (TAO Toolkit 기준)</strong></th></tr></thead><tbody>
<tr><td>Grounding (Cls)</td><td><span class="math math-inline">\text{Focal Loss on query-text dot products}</span></td><td>2.0</td></tr>
<tr><td>Box L1 Loss</td><td><span class="math math-inline">\sum \vert \hat{b} - b^* \vert_1</span></td><td>5.0</td></tr>
<tr><td>GIoU Loss</td><td><span class="math math-inline">1 - \text{GIoU}(\hat{b}, b^*)</span></td><td>2.0</td></tr>
<tr><td>Aux Loss</td><td>각 디코더 레이어 이후의 보조 손실 계산</td><td>레이어별 동일 적용</td></tr>
</tbody></table>
<p>이와 같은 학습 체계는 Grounding DINO가 단순히 사물의 위치를 잘 찾는 것을 넘어, 그 사물이 무엇인지에 대한 언어적 개념을 시각적 특징과 완벽하게 일치시키도록 만든다.</p>
<h2>3. Grounded SAM: 검출과 세그멘테이션의 유기적 연동</h2>
<p>Grounding DINO는 뛰어난 객체 검출 능력을 가졌으나 출력이 사각형 박스 형태에 국한된다는 한계가 있다. 반면 SAM(Segment Anything Model)은 임의의 프롬프트를 입력받아 정밀한 픽셀 단위 마스크를 생성할 수 있지만, 특정 텍스트 쿼리만으로 객체를 식별하는 기능은 내장되어 있지 않다. 이 두 모델을 결합한 ‘Grounded SAM’ 파이프라인은 텍스트를 통해 객체를 지칭하고(Grounding), 그 위치를 기반으로 정교하게 분할하는(Segmentation) 통합적 인지 과정을 가능하게 한다.</p>
<h3>3.1 워크플로우와 프롬프트 매핑 메커니즘</h3>
<p>Grounded SAM의 연동 과정은 매우 직관적이면서도 강력하다. 사용자가 “파란색 뚜껑의 물병“이라는 텍스트 쿼리를 입력하면, Grounding DINO가 이미지 전체를 스캔하여 해당 물병의 위치를 좌표 쌍 <span class="math math-inline">(x_{min}, y_{min}, x_{max}, y_{max})</span> 형태의 바운딩 박스로 출력한다. 이 바운딩 박스 좌표는 즉시 SAM의 프롬프트 인코더(Prompt Encoder)로 전달된다. SAM은 전달받은 박스 영역을 객체가 존재할 확률이 높은 구역으로 인식하고, 이미지 인코더에서 추출된 특징 맵과 결합하여 해당 영역 내부의 객체 경계선을 정밀하게 추론한다.</p>
<p>이러한 단계적 접근법은 다음과 같은 세 가지 차원의 이점을 제공한다:</p>
<ol>
<li><strong>공간적 인지 정밀도의 극대화:</strong> 단순한 박스 정보만으로는 파악하기 어려운 객체의 비정형적인 형태나 미세한 구멍(Hole)까지 픽셀 단위로 정확하게 분리해낼 수 있다. 이는 로봇이 복잡한 물체를 집어 올리거나 장애물을 회피할 때 필수적인 정보가 된다.</li>
<li><strong>제로샷 인스턴스 세그멘테이션:</strong> 특정 도메인의 마스크 데이터셋으로 모델을 추가 훈련시킬 필요 없이, 사전 학습된 두 파운데이션 모델의 조합만으로 생소한 객체에 대한 세그멘테이션을 즉각적으로 수행할 수 있다.</li>
<li><strong>데이터 어노테이션의 효율화:</strong> 텍스트 명령만으로 이미지 내의 수많은 객체에 대해 박스와 마스크 라벨을 자동으로 생성할 수 있어, 로봇 학습을 위한 대규모 데이터셋 구축 비용을 수만 배 이상 절감할 수 있다.</li>
</ol>
<h3>3.2 Grounded SAM 2로의 진화와 동적 환경 대응</h3>
<p>최근 발표된 SAM 2는 정지 영상뿐만 아니라 동영상에서도 객체를 추적하고 세그멘테이션하는 능력을 갖추고 있다. 이에 따라 Grounded SAM 파이프라인 역시 Grounded SAM 2로 확장되었으며, 이는 로보틱스 분야에서 매우 중요한 진전을 의미한다. 로봇이 이동하며 사물을 관찰할 때 발생하는 시점의 변화나 객체의 가려짐(Occlusion) 상황에서도, Grounding DINO가 특정 프레임에서 검출한 객체를 SAM 2가 스트리밍 메모리 트랜스포머(Streaming Memory Transformer)를 통해 지속적으로 추적하고 마스크를 유지해주기 때문이다. 이는 로봇이 “내가 보고 있는 이 사과를 따라가라“와 같은 동적인 임무를 수행하는 핵심 알고리즘이 된다.</p>
<h2>4. 로보틱스 응용 사례 및 SOTA 벤치마크 분석</h2>
<p>Grounding DINO와 SAM의 결합은 로봇의 시각 지능을 인간 수준의 유연한 인지로 격상시키는 역할을 한다. 특히 Embodied AI의 주요 도전 과제인 정밀 조작, 자율 내비게이션, 그리고 데이터 중심의 학습 과정에서 그 가치가 입증되고 있다.</p>
<h3>4.1 정밀 조작 및 파지 지점 추정 (Grasp Point Estimation)</h3>
<p>로봇 팔이 물체를 안전하게 잡기 위해서는 단순히 물체의 중심점을 아는 것만으로는 부족하다. GraspSAM과 같은 최신 연구들은 Grounding DINO를 통해 식별된 바운딩 박스를 활용하여 객체를 분할하고, 이를 바탕으로 픽셀 단위의 파지 품질 맵(Grasp Quality Map)을 생성한다. 예를 들어, 로봇이 “날카로운 칼날 부위를 피해 손잡이를 잡아라“라는 복잡한 언어 명령을 이해하고 실행하기 위해서는, 객체의 각 부위를 텍스트로 그라운딩하고 해당 부위의 정밀한 마스크를 추출하는 과정이 선행되어야 한다. Grounding DINO와 SAM의 연동은 이러한 고차원적 제어를 위한 시각적 근거를 제공한다.</p>
<h3>4.2 개방형 어휘 기반의 로봇 내비게이션</h3>
<p>자율 이동 로봇이 낯선 환경에서 목표물을 찾아가는 Object Goal Navigation 과제에서도 이 기술은 핵심적인 역할을 수행한다. LOVON(Large Language Model-based Open-Vocabulary Object Navigation) 프레임워크는 로봇의 온보드 카메라로부터 들어오는 영상을 Grounding DINO로 실시간 분석하여, 사용자가 요청한 사물이 시야에 들어오는 즉시 이를 탐지하고 로봇의 이동 경로를 생성한다. 이때 로봇의 거친 움직임으로 발생하는 영상 흔들림이나 블러 문제를 해결하기 위해 라플라시안 분산 필터링(Laplacian Variance Filtering)과 같은 기법이 결합되어 인지 안정성을 보완한다.</p>
<h3>4.3 벤치마크 성능 수치 비교</h3>
<p>Grounding DINO는 다양한 제로샷 전이(Zero-shot Transfer) 벤치마크에서 기존 모델들을 압도하는 성능을 기록했다. 특히 COCO 데이터셋을 전혀 학습하지 않은 상태에서도 50 AP 이상의 높은 정확도를 보여주며, 이는 개방형 어휘 인식 분야의 새로운 표준이 되었다.</p>
<table><thead><tr><th><strong>평가 벤치마크</strong></th><th><strong>Grounding DINO (Swin-T)</strong></th><th><strong>Grounding DINO 1.6 Pro</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td>COCO Zero-shot AP</td><td>48.4</td><td>55.4</td><td>훈련 데이터 미포함 성능</td></tr>
<tr><td>LVIS-minival AP</td><td>27.4</td><td>57.7</td><td>희귀 클래스 인식 능력 포함</td></tr>
<tr><td>ODinW (Mean AP)</td><td>26.1</td><td>-</td><td>35개 다양한 도메인 데이터셋 평균</td></tr>
<tr><td>SegInW (Mean AP)</td><td>48.1</td><td>48.7</td><td>Grounded SAM 기반 분할 성능</td></tr>
</tbody></table>
<p>이러한 수치는 로봇이 별도의 환경별 학습 과정 없이도, 실험실 환경에서 구축된 파운데이션 모델만으로 실제 가정이나 공장 등 복잡한 현장에 즉각 투입될 수 있음을 시사한다.</p>
<h2>5. 결론 및 미래 전망</h2>
<p>Grounding DINO와 검출 기술의 결합, 그리고 SAM과의 연동은 로봇이 세상을 바라보는 방식을 근본적으로 바꾸어 놓았다. 단순히 미리 약속된 기호를 찾는 수준을 넘어, 인간의 유연한 자연어 명령을 픽셀 단위의 물리적 실체로 연결하는 능력은 범용 로봇(Generalist Robot) 시대를 앞당기는 핵심 동력이다. 비록 트랜스포머 기반 모델의 높은 연산량과 실시간 추론 지연 시간이 로봇 하드웨어 배포의 장애물로 남아 있으나, Grounding DINO 1.5 Edge와 같은 경량화 모델과 TensorRT 기반의 하드웨어 가속화 기법들이 이러한 간극을 빠르게 메우고 있다. 향후 이 기술은 시각-언어-행동 모델(VLA)과 더욱 긴밀하게 통합되어, 로봇이 인지한 의미 정보를 직접적인 제어 토크로 변환하는 보다 고차원적인 인공지능 아키텍처의 중추가 될 것이다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Marrying DINO with Grounded Pre-Training for Open-Set Object Detection - arXiv, https://arxiv.org/html/2303.05499v5</li>
<li>Marrying DINO with Grounded Pre-Training for Open-Set Object Detection - arXiv, https://arxiv.org/abs/2303.05499</li>
<li>Grounded SAM: Open-Vocab Vision Pipeline - Emergent Mind, https://www.emergentmind.com/topics/groundedsam</li>
<li>Grounded SAM 2: From Open-Set Detection to Segmentation and Tracking, https://pyimagesearch.com/2026/01/19/grounded-sam-2-from-open-set-detection-to-segmentation-and-tracking/</li>
<li>Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks - arXiv, https://arxiv.org/html/2401.14159v1</li>
<li>Marrying DINO with Grounded Pre-Training for Open-Set Object Detection | OpenReview, https://openreview.net/forum?id=DS5qRs0tQz</li>
<li>Marrying DINO with Grounded Pre-Training for Open-Set Object Detection - European Computer Vision Association, https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06319.pdf</li>
<li>Grounding DINO: Open Vocabulary Object Detection on Videos - PyImageSearch, https://pyimagesearch.com/2025/12/08/grounding-dino-open-vocabulary-object-detection-on-videos/</li>
<li>Enhanced Zero-shot Labels using DINO &amp; Grounded Pre-training - Labellerr, https://www.labellerr.com/blog/grounded-dino-combining-dino-with-grounded-pre-training-for-open-set-object-annotations/</li>
<li>Exploring DINO Family Part2: Grounding DINO for Open-Set Object Detection Debuts, https://deepdataspace.com/en/blog/14/</li>
<li>Grounding DINO: Vision-Language Detection - Emergent Mind, https://www.emergentmind.com/topics/grounding-dino-model</li>
<li>Fine-Tuning Grounding DINO: Open-Vocabulary Object Detection - Learn OpenCV, https://learnopencv.com/fine-tuning-grounding-dino/</li>
<li>Enhanced Grounding DINO: Efficient Cross-Modality Block for Open-Set Object Detection in Remote Sensing - IEEE Xplore, https://ieeexplore.ieee.org/iel8/4609443/10766875/11021309.pdf</li>
<li>Mask Grounding DINO — Tao Toolkit - NVIDIA Documentation, https://docs.nvidia.com/tao/tao-toolkit/text/cv_finetuning/pytorch/instance_segmentation/mask_grounding_dino.html</li>
<li>Grounded-SAM: Revolutionizing Vision with Text Prompts - Viso Suite, https://viso.ai/deep-learning/grounded-sam/</li>
<li>Grounded SAM Instance Segmentation Model: What is, How to Use - Roboflow, https://roboflow.com/model/grounded-sam</li>
<li>Grounded SAM for Bulk Image segmentation Now Available on Dataloop - YouTube, https://www.youtube.com/watch?v=3ug-1cdYl7w</li>
<li>Segmenting satellite images using SAM and Grounding DINO - Medium, https://medium.com/echo-r-d/segmenting-satellite-images-using-sam-and-grounding-dino-ad2afbbe8086</li>
<li>RoboDexVLM: Visual Language Model-Enabled Task Planning and Motion Control for Dexterous Robot Manipulation - arXiv, https://arxiv.org/html/2503.01616v1</li>
<li>LOVON: Legged Open-Vocabulary Object Navigator - arXiv, https://arxiv.org/html/2507.06747v1</li>
<li>Table 5 from Grounding DINO 1.5: Advance the “Edge” of Open-Set Object Detection, <a href="https://www.semanticscholar.org/paper/Grounding-DINO-1.5%3A-Advance-the-%22Edge%22-of-Open-Set-Ren-Jiang/3abfa0d372bca716f57ca7871e7e3694dacdfae8/figure/8">https://www.semanticscholar.org/paper/Grounding-DINO-1.5%3A-Advance-the-%22Edge%22-of-Open-Set-Ren-Jiang/3abfa0d372bca716f57ca7871e7e3694dacdfae8/figure/8</a></li>
<li>Instruct2Subtask: A Language Parsing Framework for Sequential Robotic Manipulation - OpenReview, https://openreview.net/pdf?id=j78YPxJ3ki</li>
<li>GraspSAM: When Segment Anything Model Meets Grasp Detection - arXiv, https://arxiv.org/html/2409.12521v1</li>
<li>IDEA-Research/Grounding-DINO-1.5-API - GitHub, https://github.com/IDEA-Research/Grounding-DINO-1.5-API</li>
<li>Dynamic-DINO: Fine-Grained Mixture of Experts Tuning for Real-time Open-Vocabulary Object Detection - arXiv, https://arxiv.org/html/2507.17436v1</li>
<li>Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks - arXiv, https://arxiv.org/abs/2401.14159</li>
<li>Real-time open-vocabulary perception for mobile robots on edge devices: a systematic analysis of the accuracy-latency trade-off - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1693988/full</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>