<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:6.3.3 Open-Vocabulary Panoptic Segmentation: 배경(Stuff)과 객체(Thing)를 모두 아우르는 전역적 의미 이해.</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>6.3.3 Open-Vocabulary Panoptic Segmentation: 배경(Stuff)과 객체(Thing)를 모두 아우르는 전역적 의미 이해.</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 6. 오픈 보캐블러리와 시맨틱 이해 (Open-Vocabulary & Semantic Understanding)</a> / <a href="index.html">6.3 무엇이든 분할한다: SAM과 Grounding</a> / <span>6.3.3 Open-Vocabulary Panoptic Segmentation: 배경(Stuff)과 객체(Thing)를 모두 아우르는 전역적 의미 이해.</span></nav>
                </div>
            </header>
            <article>
                <h1>6.3.3 Open-Vocabulary Panoptic Segmentation: 배경(Stuff)과 객체(Thing)를 모두 아우르는 전역적 의미 이해.</h1>
<p>로봇이 복잡한 실생활 환경에서 자율적으로 기동하고 인간과 상호작용하기 위해서는 단순히 눈앞의 물체를 탐지하는 수준을 넘어, 장면 전체에 대한 구조적이고 전역적인 의미 이해가 선행되어야 한다. 이를 위해 컴퓨터 비전 분야에서 제시된 가장 포괄적인 형태의 시각 인지 태스크가 바로 파놉틱 세그멘테이션(Panoptic Segmentation)이다. 이 기술은 이미지 내의 모든 픽셀에 대하여 ’무엇인지(Semantic)’와 ’어느 개체에 속하는지(Instance)’를 동시에 판별함으로써, 배경(Stuff)과 객체(Thing)를 하나의 프레임워크 내에서 통합한다. 특히 최근에는 미리 정의된 범주에 국한되지 않고 자연어 프롬프트를 통해 임의의 대상을 분할하는 오픈 보캐블러리(Open-Vocabulary) 능력이 결합되면서, 로봇의 공간 지능은 비약적인 발전을 거듭하고 있다.</p>
<h2>1.  파놉틱 세그멘테이션의 개념적 토대와 진화</h2>
<p>파놉틱(Panoptic)이라는 용어는 모든 것을 본다는 의미의 그리스어에서 유래하였으며, 이는 시각적 장면의 완전한 파싱(Parsing)을 목표로 한다. 전통적인 영상 분할 기법은 셀 수 있는 객체를 구분하는 인스턴스 세그멘테이션과 비정형 영역을 분류하는 시맨틱 세그멘테이션으로 이원화되어 발전해 왔으나, 파놉틱 세그멘테이션은 이 두 가지를 단일한 출력 형태로 통합하여 픽셀 수준의 중복 없는 레이블링을 수행한다.</p>
<h3>1.1  배경(Stuff)과 객체(Thing)의 이분법적 분류</h3>
<p>현대적 시각 이해의 중추를 형성하는 Stuff와 Thing의 구분은 인간의 인지 체계를 반영한 결과이다. 로봇이 조우하는 환경은 기하학적 경계가 명확한 개체들과 그렇지 않은 배경적 요소들로 구성된다.</p>
<ul>
<li><strong>객체(Thing):</strong> 사람, 자동차, 의자, 도구 등과 같이 개별적으로 셀 수 있고 기하학적으로 고정된 형태를 가진 인스턴스들을 의미한다. 이들은 각각 고유한 인스턴스 ID를 부여받아 식별되어야 하며, 로봇 제어 관점에서는 조작(Manipulation)이나 충돌 회피의 대상이 된다.</li>
<li><strong>배경(Stuff):</strong> 하늘, 도로, 잔디, 물과 같이 형태가 비정형적이고 개별 인스턴스로 나누기 어려운 연속적인 영역을 지칭한다. 이러한 영역은 인스턴스 ID 없이 시맨틱 라벨만으로 정의되며, 로봇에게는 주행 가능 영역(Drivable Area)이나 환경의 맥락(Context) 정보를 제공한다.</li>
</ul>
<p>이러한 이분법적 분류는 2001년 에드워드 아델슨(Edward Adelson)에 의해 체계화되었으며, 기계 시각이 단순한 픽셀 인식을 넘어 장면 전체의 맥락을 파악하는 ‘장면 이해(Scene Parsing)’ 단계로 도약하는 계기가 되었다.</p>
<h3>1.2  통합 평가 지표: 파놉틱 품질(PQ)</h3>
<p>파놉틱 세그멘테이션의 성능은 단순히 분류 정확도만으로 측정할 수 없다. 객체를 얼마나 정확하게 분할했는지(Segmentation)와 해당 객체의 카테고리를 얼마나 잘 맞췄는지(Recognition)를 동시에 평가해야 한다. 이를 위해 제안된 파놉틱 품질(Panoptic Quality, PQ) 지표는 다음과 같이 정의된다.<br />
<span class="math math-display">
PQ = \frac{\sum_{(p, g) \in TP} IoU(p, g)}{\vert TP \vert + \frac{1}{2}\vert FP \vert + \frac{1}{2}\vert FN \vert}
</span><br />
이 수식은 세그멘테이션 품질(SQ)과 인식 품질(RQ)의 결합으로 이해될 수 있다. PQ는 각 픽셀이 단 하나의 예측 결과에만 속해야 한다는 비중첩 제약을 강제함으로써, 기존의 독립적인 시맨틱 및 인스턴스 분할 모델들이 가졌던 알고리즘적 중복성과 레이블 충돌 문제를 해결한다.</p>
<h2>2.  오픈 보캐블러리 패러다임의 도래</h2>
<p>전통적인 파놉틱 세그멘테이션 모델은 훈련 단계에서 설정된 고정된 카테고리(Closed-set) 내에서만 동작한다. 그러나 자율 로봇이 마주하는 실제 세계는 수만 가지 이상의 사물과 변화무쌍한 배경으로 가득 차 있다. 훈련 데이터에 존재하지 않았던 ’쓰러진 쓰레기통’이나 ’생소한 형태의 조형물’을 인식하지 못하는 폐쇄형 모델은 로봇의 안전과 직무 수행에 심각한 저해 요소가 된다.</p>
<p>오픈 보캐블러리 파놉틱 세그멘테이션은 거대 시각-언어 모델(Vision-Language Models, VLM)의 지식을 활용하여 훈련 데이터에 없는 임의의 텍스트 설명만으로도 해당 영역을 분할하고 식별하는 능력을 제공한다. 이는 로봇이 “저기 구석에 있는 빨간색 벨벳 소재의 의자를 가져와“와 같이 구체적이고 새로운 지시를 수행할 수 있게 하는 인지적 기반이 된다.</p>
<h3>2.1  CLIP의 역할과 기술적 난제</h3>
<p>대부분의 오픈 보캐블러리 접근법은 이미지와 텍스트를 공통의 임베딩 공간으로 정렬시킨 CLIP(Contrastive Language-Image Pre-training) 모델을 핵심 엔진으로 사용한다. 하지만 CLIP을 파놉틱 세그멘테이션에 직접 적용하는 데에는 몇 가지 병목 현상이 존재한다.</p>
<ol>
<li><strong>도메인 갭(Domain Gap):</strong> CLIP은 자연스러운 전체 이미지에 대해 학습되었으나, 세그멘테이션 마스크로 잘려나간 부분 이미지(Masked Image)는 CLIP의 학습 분포와 큰 차이를 보인다. 이는 마스크 분류 성능의 저하로 이어진다.</li>
<li><strong>공간 해상도 손실:</strong> CLIP의 비전 인코더는 주로 이미지 레벨의 특징 추출에 최적화되어 있어, 세밀한 경계 정보가 필요한 세그멘테이션 태스크에서는 정보 손실이 발생할 수 있다.</li>
<li><strong>연산 효율성:</strong> 이미지 내의 수많은 마스크 후보를 각각 CLIP 인코더에 통과시키는 방식은 로봇의 실시간 처리에 부적합할 만큼 느리다.</li>
</ol>
<p>이러한 한계를 극복하기 위해 최근의 SOTA 모델들은 마스크 제안 생성기(Mask Proposal Generator)와 CLIP의 시맨틱 지식을 효율적으로 융합하는 다양한 아키텍처를 제시하고 있다.</p>
<h2>3.  SOTA 알고리즘 분석 및 아키텍처 혁신</h2>
<p>최신 오픈 보캐블러리 파놉틱 세그멘테이션 기술은 크게 두 가지 흐름으로 나뉜다. 하나는 강력한 마스크 생성 모델과 분류 모델을 정교하게 결합하는 방식이며, 다른 하나는 모든 과정을 단일한 트랜스포머 디코더 내에서 처리하는 통합 방식이다.</p>
<h3>3.1  FC-CLIP: 공유된 고정 컨볼루션 백본의 힘</h3>
<p>FC-CLIP (Frozen Convolutional CLIP)은 기존의 2단계 프레임워크가 가졌던 비효율성을 혁신적으로 개선한 모델이다. 기존 방식들이 마스크 생성을 위한 별도의 백본과 분류를 위한 CLIP 인코더를 각각 유지했던 것과 달리, FC-CLIP은 ConvNeXt 기반의 고정된(Frozen) CLIP 비전 인코더 하나만을 백본으로 공유한다.</p>
<p>FC-CLIP의 기술적 핵심은 다음과 같다.</p>
<ul>
<li><strong>단일 단계 파이프라인:</strong> 이미지를 한 번만 통과시켜 마스크 생성과 오픈 보캐블러리 분류를 동시에 수행함으로써 연산 비용을 대폭 절감한다.</li>
<li><strong>양방향 분류 메커니즘:</strong> 훈련 데이터의 범주(In-vocabulary)와 미지의 범주(Out-of-vocabulary)를 동시에 처리할 수 있는 분류 헤드를 구성하여, 기지의 물체에 대한 정밀도와 미지의 물체에 대한 일반화 성능을 모두 확보한다.</li>
<li><strong>해상도 강건성:</strong> 연구 결과에 따르면, ViT(Vision Transformer) 기반 CLIP은 입력 해상도가 증가함에 따라 성능이 급격히 저하되는 경향이 있으나, ConvNeXt 기반의 컨볼루션 CLIP은 고해상도 이미지에서도 안정적인 성능을 유지한다.</li>
</ul>
<p>FC-CLIP은 COCO 데이터셋만으로 학습한 후 ADE20K에서 제로샷 성능을 평가했을 때 26.8 PQ를 달성하여 이전의 SOTA 모델인 ODISE 등을 큰 폭으로 상회하였다.</p>
<h3>3.2  SEEM: 다중 모달 프롬프트를 통한 범용 분할 interface</h3>
<p>SEEM (Segment Everything Everywhere All at Once)은 파놉틱 세그멘테이션을 포함한 모든 분할 태스크를 하나의 범용 인터페이스로 통합하려는 시도이다. 이 모델은 사용자의 다양한 의도(텍스트, 클릭, 박스, 낙서 등)를 시각-언어 공동 공간(Joint Visual-Semantic Space) 내의 프롬프트로 변환하여 처리한다.</p>
<p>SEEM의 주요 특징은 다음과 같다.</p>
<ul>
<li><strong>범용성(Versatility):</strong> Stuff와 Thing을 구분하지 않고 모든 영역을 쿼리 기반의 트랜스포머 디코더로 예측한다.</li>
<li><strong>상호작용성(Interactivity):</strong> 학습 가능한 메모리 프롬프트를 도입하여 세션의 이력을 유지함으로써, 로봇이 인간의 피드백을 받아 세그멘테이션 결과를 점진적으로 수정할 수 있게 한다.</li>
<li><strong>의미론적 인식:</strong> 모든 마스크에 대해 텍스트 인코더와 연동된 시맨틱 라벨을 부여하여 완전한 오픈 보캐블러리 능력을 구현한다.</li>
</ul>
<h3>3.3  OPSNet과 임베딩 변조 기술</h3>
<p>OPSNet (Open-vocabulary Panoptic Segmentation Network)은 세그멘테이션 쿼리와 CLIP 임베딩 간의 정보 교환을 최적화하는 데 집중한다. 단순히 마스크 영역의 특징을 CLIP에 입력하는 대신, 임베딩 변조(Embedding Modulation) 모듈을 통해 쿼리 임베딩과 시각적 지식을 유기적으로 결합한다.</p>
<table><thead><tr><th><strong>컴포넌트</strong></th><th><strong>기능 및 역할</strong></th></tr></thead><tbody>
<tr><td><strong>Spatial Adapter</strong></td><td>CLIP 이미지 인코더 후단에 위치하여 공간 해상도를 유지하고 미세한 기하학적 특징 보존</td></tr>
<tr><td><strong>Mask Pooling</strong></td><td>예측된 클래스 불가지론적 마스크를 사용하여 CLIP 특징 맵에서 고차원 임베딩 추출</td></tr>
<tr><td><strong>Embedding Modulation</strong></td><td>쿼리 임베딩, CLIP 임베딩, 개념적 시맨틱 간의 정보 교환을 통해 최종 분류 임베딩 생성</td></tr>
<tr><td><strong>Decoupled Supervision</strong></td><td>마스크 감독과 분류 감독을 분리하여 대규모 이미지-텍스트 쌍 데이터의 효율적 활용 가능</td></tr>
</tbody></table>
<p>OPSNet은 ResNet-101 백본을 사용하고도 COCO에서 52.6 PQ라는 높은 성능을 기록하며, 오픈 보캐블러리 환경에서도 강력한 인식 능력을 입증하였다.</p>
<h2>4.  데이터셋 및 벤치마크 성능 비교</h2>
<p>오픈 보캐블러리 성능을 측정하기 위해서는 학습 데이터셋에 포함되지 않은 미지의 클래스가 다수 포함된 벤치마크에서의 평가가 필수적이다. 일반적으로 COCO Panoptic 데이터셋으로 모델을 학습시킨 후, ADE20K(150 또는 847 클래스), Cityscapes, Mapillary Vistas 등에서 제로샷 성능을 측정한다.</p>
<p>다음 표는 주요 SOTA 모델들의 ADE20K 제로샷 파놉틱 성능을 요약한 것이다.</p>
<table><thead><tr><th><strong>모델명</strong></th><th><strong>발표 시점</strong></th><th><strong>학습 데이터</strong></th><th><strong>ADE20K PQ</strong></th><th><strong>ADE20K mIoU</strong></th><th><strong>특징</strong></th></tr></thead><tbody>
<tr><td><strong>MaskCLIP</strong></td><td>ICML 2023</td><td>COCO</td><td>15.1</td><td>23.7</td><td>2단계, CLIP 비주얼 모델 보정</td></tr>
<tr><td><strong>FreeSeg</strong></td><td>CVPR 2023</td><td>COCO</td><td>16.3</td><td>24.6</td><td>적응적 프롬프트 튜닝</td></tr>
<tr><td><strong>ODISE</strong></td><td>CVPR 2023</td><td>COCO</td><td>23.2</td><td>28.4</td><td>확산 모델(Stable Diffusion) 특징 추출 활용</td></tr>
<tr><td><strong>FC-CLIP</strong></td><td>NeurIPS 2023</td><td>COCO</td><td>26.8</td><td>34.1</td><td>공유 CLIP 백본, 압도적 효율성</td></tr>
<tr><td><strong>PosSAM (L)</strong></td><td>Arxiv 2024</td><td>COCO</td><td>28.0</td><td>34.2</td><td>SAM의 공간 지능과 CLIP의 결합</td></tr>
<tr><td><strong>PosSAM (H)</strong></td><td>Arxiv 2024</td><td>COCO</td><td>29.2</td><td>35.1</td><td>현시점 최고 수준의 경계 복원력</td></tr>
</tbody></table>
<p>최근의 PosSAM 모델은 SAM(Segment Anything Model)이 가진 탁월한 클래스 불가지론적 분할 능력을 CLIP의 변별력 있는 특징과 결합하여, 특히 객체의 경계선(Boundary) 품질에서 괄목할만한 성과를 보여주고 있다. SAM은 10억 개 이상의 마스크로 학습되어 ‘무엇이든’ 나눌 수 있지만 ’무엇인지’는 알지 못하는데, 여기에 CLIP의 언어적 지식을 주입함으로써 로봇에게 가장 정교한 시각 지도를 제공하게 된 것이다.</p>
<h2>5.  로봇 공학으로의 확장과 실제적 함의</h2>
<p>오픈 보캐블러리 파놉틱 세그멘테이션은 로봇의 전역적 의미 이해(Global Semantic Understanding)를 가능케 함으로써, 추상적인 자연어 명령을 물리적 행동으로 연결하는 가교 역할을 한다.</p>
<h3>5.1  3D 공간 지능과 시각-언어 증류</h3>
<p>로봇은 2D 이미지가 아닌 3D 공간에서 동작한다. 따라서 2D에서의 오픈 보캐블러리 지식을 3D 점구름(Point Cloud)이나 복셀(Voxel) 공간으로 전이하는 연구가 활발하다. 2D-3D 시각-언어 증류(Vision-Language Distillation) 기술은 카메라 이미지에서 추출한 CLIP 임베딩을 LiDAR 포인트에 투영하여, 3D 공간의 모든 점에 오픈 보캐블러리 시맨틱을 부여한다. 이를 통해 로봇은 처음 보는 장애물도 ’회피해야 할 물체(Thing)’인지 ’지나가도 되는 배경(Stuff)’인지 3D 상에서 즉각적으로 판단할 수 있다.</p>
<h3>5.2  시맨틱 내비게이션과 오픈 씬 그래프</h3>
<p>전역적 의미 이해의 궁극적인 형태는 환경 내의 사물과 공간 간의 관계를 구조화하는 것이다. 오픈 보캐블러리 파놉틱 세그멘테이션 결과는 ’오픈 씬 그래프(Open Scene Graph, OSG)’의 기초 데이터가 된다.</p>
<ul>
<li><strong>배경(Stuff) 정보:</strong> 로봇이 현재 어느 방(Room)에 있는지, 바닥의 재질은 무엇인지 정보를 제공하여 대역적 위치 추정(Localization)을 돕는다.</li>
<li><strong>객체(Thing) 정보:</strong> 특정 물체의 인스턴스 ID와 위치를 기록하여 “주방에 가서 두 번째 찬장에 있는 머그컵을 가져와“와 같은 복잡한 목표물 내비게이션(Object-Goal Navigation)을 가능하게 한다.</li>
</ul>
<p>이러한 계층적 표현은 로봇이 단순히 픽셀을 분류하는 수준을 넘어, 인간과 유사한 방식으로 세상을 ’사물들의 유기적 집합’으로 인지하게 만든다.</p>
<h2>6.  결론: 전역적 의미 이해의 미래</h2>
<p>오픈 보캐블러리 파놉틱 세그멘테이션은 로봇 인지 기술의 정점이라 할 수 있다. Stuff와 Thing을 아우르는 통합적 시각은 로봇에게 환경에 대한 완전한 상황 인식(Situational Awareness)을 제공하며, 오픈 보캐블러리 능력은 로봇의 활동 범위를 무한한 현실 세계로 확장시킨다.</p>
<p>FC-CLIP과 같은 모델을 통해 실시간성이 확보되고, SAM과 같은 파운데이션 모델을 통해 분할의 정교함이 완성되면서, 로봇은 이제 더 이상 고정된 실험실이 아닌 역동적인 일상 환경에서도 신뢰할 수 있는 지능체로 거듭나고 있다. 향후 이 기술은 비디오 일관성(Temporal Consistency) 확보와 4D 공간 인식으로 확장될 것이며, 이는 로봇이 시간의 흐름에 따른 환경의 변화까지도 의미론적으로 이해하는 ’진정한 공간 지능’을 구현하는 핵심 동력이 될 것이다. 로봇이 세상을 ‘보는’ 단계를 넘어 ‘완벽히 이해하는’ 시대가 성큼 다가와 있다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Understanding Panoptic Segmentation Basics - Viso Suite, https://viso.ai/deep-learning/panoptic-segmentation/</li>
<li>Panoptic Segmentation Explained: Everything You Need to Know in 2025 | BasicAI’s Blog, https://www.basic.ai/blog-post/comprehensive-guide-of-panoptic-segmentation</li>
<li>Open Vocabulary Panoptic Segmentation With Retrieval Augmentation - arXiv, https://arxiv.org/html/2601.12779v1</li>
<li>Enhancing Open-Vocabulary Panoptic Segmentation with Semantic-Guided Q-Tuning - ResearchGate, https://www.researchgate.net/publication/397086248_Enhancing_Open-Vocabulary_Panoptic_Segmentation_with_Semantic-Guided_Q-Tuning</li>
<li>Hierarchical Open-vocabulary Universal Image Segmentation - NeurIPS, https://proceedings.neurips.cc/paper_files/paper/2023/file/43663f64775ae439ec52b64305d219d3-Paper-Conference.pdf</li>
<li>[AV Vol. 4] 3D Open-Vocabulary Panoptic Segmentation with 2D–3D Vision-Language Distillation (Waymo ECCV 2024) | by Adam Roberge | deMISTify | Medium, https://medium.com/demistify/av-vol-4-3d-open-vocabulary-panoptic-segmentation-with-2d-3d-vision-language-distillation-waymo-0f65b3e7d3cc</li>
<li>Open-vocabulary Panoptic Segmentation with … - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Open-vocabulary_Panoptic_Segmentation_with_Embedding_Modulation_ICCV_2023_paper.pdf</li>
<li>Open-Vocabulary Panoptic Segmentation MaskCLIP - OpenReview, https://openreview.net/forum?id=zWudXc9343</li>
<li>PosSAM: Panoptic Open-vocabulary Segment Anything - arXiv, https://arxiv.org/html/2403.09620v1</li>
<li>Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models - Jiarui Xu, https://jerryxu.net/ODISE/</li>
<li>Open-Vocabulary Semantic Segmentation With Mask-Adapted CLIP, https://openaccess.thecvf.com/content/CVPR2023/papers/Liang_Open-Vocabulary_Semantic_Segmentation_With_Mask-Adapted_CLIP_CVPR_2023_paper.pdf</li>
<li>Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP - OpenReview, <a href="https://openreview.net/forum?id=83LJRUzXWj&amp;referrer=%5Bthe+profile+of+Ju+He%5D(/profile?id%3D~Ju_He1)">https://openreview.net/forum?id=83LJRUzXWj&amp;referrer=%5Bthe%20profile%20of%20Ju%20He%5D(%2Fprofile%3Fid%3D~Ju_He1)</a></li>
<li>EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation | Proceedings of the AAAI Conference on Artificial Intelligence, https://ojs.aaai.org/index.php/AAAI/article/view/32669</li>
<li>EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation - arXiv, https://arxiv.org/html/2412.08628v1</li>
<li>Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP - NeurIPS, https://papers.neurips.cc/paper_files/paper/2023/file/661caac7729aa7d8c6b8ac0d39ccbc6a-Paper-Conference.pdf</li>
<li>A Simple Framework for Open-Vocabulary Segmentation and Detection - ResearchGate, https://www.researchgate.net/publication/377430901_A_Simple_Framework_for_Open-Vocabulary_Segmentation_and_Detection</li>
<li>Segment Everything Everywhere All at Once - arXiv, https://arxiv.org/html/2304.06718</li>
<li>Segment Everything Everywhere All at Once - NeurIPS, https://proceedings.neurips.cc/paper_files/paper/2023/file/3ef61f7e4afacf9a2c5b71c726172b86-Paper-Conference.pdf</li>
<li>Segment Everything Everywhere All at Once | OpenReview, https://openreview.net/forum?id=UHBrWeFWlL</li>
<li>Segment Everything Everywhere All at Once | Request PDF - ResearchGate, https://www.researchgate.net/publication/370001210_Segment_Everything_Everywhere_All_at_Once</li>
<li>microsoft/X-Decoder: [CVPR 2023] Official Implementation … - GitHub, https://github.com/microsoft/X-Decoder</li>
<li>UX-Decoder/Segment-Everything-Everywhere-All-At-Once - GitHub, https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once</li>
<li>bytedance/fc-clip: [NeurIPS 2023] This repo contains the … - GitHub, https://github.com/bytedance/fc-clip</li>
<li>HAECcity: Open-Vocabulary Scene Understanding of City-Scale Point Clouds with Superpoint Graph Clustering, https://openaccess.thecvf.com/content/CVPR2025W/OpenSUN3D/papers/Rusnak_HAECcity_Open-Vocabulary_Scene_Understanding_of_City-Scale_Point_Clouds_with_Superpoint_CVPRW_2025_paper.pdf</li>
<li>Open Scene Graphs for Object Navigation | PDF - Scribd, https://www.scribd.com/document/817312025/Open-Scene-Graphs-for-Open-World-Object-Goal-Navigation</li>
<li>Language-Grounded Dynamic Scene Graphs for Interactive Object Search with Mobile Manipulation, https://semrob.github.io/docs/rss_semrob2024_cr_paper26.pdf</li>
<li>NeRFs in Robotics: A Survey - arXiv, https://arxiv.org/html/2405.01333v2</li>
<li>Leverage Cross-Attention for End-to-End Open-Vocabulary Panoptic Reconstruction - arXiv, https://arxiv.org/abs/2501.01119</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>