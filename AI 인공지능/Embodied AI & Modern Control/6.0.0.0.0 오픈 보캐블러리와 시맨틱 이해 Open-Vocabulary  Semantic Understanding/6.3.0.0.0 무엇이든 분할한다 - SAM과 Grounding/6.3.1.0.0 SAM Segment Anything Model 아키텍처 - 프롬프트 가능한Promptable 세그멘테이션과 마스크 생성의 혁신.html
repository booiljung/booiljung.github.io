<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:6.3.1 SAM (Segment Anything Model) 아키텍처: 프롬프트 가능한(Promptable) 세그멘테이션과 마스크 생성의 혁신.</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>6.3.1 SAM (Segment Anything Model) 아키텍처: 프롬프트 가능한(Promptable) 세그멘테이션과 마스크 생성의 혁신.</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 6. 오픈 보캐블러리와 시맨틱 이해 (Open-Vocabulary & Semantic Understanding)</a> / <a href="index.html">6.3 무엇이든 분할한다: SAM과 Grounding</a> / <span>6.3.1 SAM (Segment Anything Model) 아키텍처: 프롬프트 가능한(Promptable) 세그멘테이션과 마스크 생성의 혁신.</span></nav>
                </div>
            </header>
            <article>
                <h1>6.3.1 SAM (Segment Anything Model) 아키텍처: 프롬프트 가능한(Promptable) 세그멘테이션과 마스크 생성의 혁신.</h1>
<p>컴퓨터 비전 분야에서 객체 세그멘테이션은 이미지의 각 픽셀에 의미론적 정보를 부여하는 가장 정밀한 지각 과업 중 하나이다. 기존의 세그멘테이션 모델들은 주로 사전에 정의된 특정 범주(Category) 내의 객체만을 인식하도록 학습되었으며, 이는 새로운 환경이나 미지의 객체를 조우해야 하는 로봇에게 치명적인 한계로 작용해 왔다. 이러한 배경에서 Meta AI Research(FAIR)가 발표한 <em>Segment Anything</em> 논문은 컴퓨터 비전의 패러다임을 근본적으로 변화시켰다. Segment Anything Model(이하 SAM)은 특정 데이터셋에 종속되지 않고 ‘무엇이든 분할할 수 있는’ 범용적인 파운데이션 모델(Foundation Model)로 설계되었으며, 이는 특히 로봇의 오픈 보캐블러리 지각 능력과 비정형 환경에서의 상호작용 능력을 비약적으로 향상시키는 결과로 이어졌다.</p>
<h2>1. 프롬프트 가능한 세그멘테이션의 개념적 정의와 비전 파운데이션 모델의 부상</h2>
<p>SAM의 핵심적인 혁신은 자연어 처리(NLP) 분야의 거대 언어 모델(LLM)이 보여준 프롬프트 기반 학습 방식을 비전 분야로 성공적으로 이식한 데 있다. 전통적인 세그멘테이션 모델은 ‘사람’, ’자동차’와 같은 특정 클래스를 예측하도록 훈련되지만, SAM은 ’어떤 프롬프트가 주어지더라도 유효한 마스크를 생성하라’는 과업을 수행한다. 여기서 프롬프트란 점(Point), 경계 박스(Bounding Box), 거친 마스크(Rough Mask), 혹은 자연어 텍스트(Text) 등 사용자의 의도를 전달할 수 있는 모든 형태의 시각적·언어적 입력을 의미한다.</p>
<p>이러한 프롬프트 가능한 세그멘테이션(Promptable Segmentation) 과업은 모델에게 강력한 제로샷(Zero-shot) 일반화 능력을 부여한다. 로봇이 학습 과정에서 한 번도 본 적 없는 기괴한 모양의 주방 도구나 산업용 부품을 마주하더라도, 사용자가 점 하나를 찍거나 박스를 그리는 것만으로 모델은 해당 객체의 정확한 경계를 찾아낼 수 있다. 이는 로봇이 새로운 환경에 투입될 때마다 별도의 데이터 수집과 미세 조정(Fine-tuning)을 거쳐야 했던 과거의 방식에서 벗어나, 즉각적인 현장 배포가 가능한 지능을 갖추게 되었음을 시사한다.</p>
<h2>2. SAM의 3대 핵심 아키텍처 요소</h2>
<p>SAM의 시스템 설계는 과업(Task), 모델(Model), 데이터(Data)라는 세 가지 요소의 유기적인 시너지를 목표로 한다. 특히 모델 아키텍처는 고해상도 이미지를 효율적으로 처리하면서도 사용자의 프롬프트에 실시간으로 반응할 수 있도록 설계되었다. SAM은 크게 이미지 인코더(Image Encoder), 프롬프트 인코더(Prompt Encoder), 마스크 디코더(Mask Decoder)라는 세 가지 하위 모듈로 구성된다.</p>
<h3>2.1 이미지 인코더(Image Encoder): 고차원 시각 임베딩의 생성</h3>
<p>이미지 인코더는 입력된 픽셀 정보를 추상화된 시각적 특징으로 변환하는 역할을 수행하며, SAM 아키텍처에서 가장 연산 집약적인 부분이다. Meta는 강력한 특징 추출 성능을 확보하기 위해 Vision Transformer(ViT) 구조를 채택하였으며, 특히 Masked Autoencoder(MAE) 방식으로 사전 학습된 ViT-H/16 모델을 기반으로 한다.</p>
<p>인코더의 세부 구성을 살펴보면, 1024 \times 1024 해상도의 입력 이미지는 먼저 16 \times 16 크기의 패치(Patch)로 분할된다. 이는 64 \times 64 크기의 공간 그리드를 형성하며, 각 패치는 256 채널의 특징 벡터로 인코딩된다. 표준적인 ViT는 모든 패치 간의 전역적 어텐션(Global Attention)을 계산하므로 연산 비용이 해상도의 제곱에 비례하여 증가하는 문제가 있다. SAM은 이를 해결하기 위해 윈도우 어텐션(Windowed Attention) 메커니즘을 도입하였다. 전체 특징 맵을 작은 윈도우로 나누어 국소적인 정보를 먼저 처리하고, 층이 깊어짐에 따라 드문드문 전역적 어텐션을 수행함으로써 고해상도 처리를 가능케 했다. 이미지 인코더의 출력물인 64 \times 64 \times 256 크기의 이미지 임베딩은 프롬프트가 주어지기 전에 미리 계산될 수 있으며, 이는 후속 과정에서의 실시간 반응성을 보장하는 결정적인 요인이 된다.</p>
<h3>2.2 프롬프트 인코더(Prompt Encoder): 상호작용의 수치화</h3>
<p>프롬프트 인코더는 다양한 형태의 입력을 이미지 임베딩과 결합 가능한 고정된 차원의 벡터로 변환한다. SAM은 입력의 성격에 따라 희소한(Sparse) 프롬프트와 조밀한(Dense) 프롬프트를 다르게 처리한다.</p>
<table><thead><tr><th><strong>프롬프트 유형</strong></th><th><strong>처리 메커니즘</strong></th><th><strong>로봇 공학적 의의</strong></th></tr></thead><tbody>
<tr><td>점 (Points)</td><td>위치 인코딩(Positional Encoding)과 전경/배경 구분을 위한 학습 임베딩의 합</td><td>특정 부품의 파지 지점이나 접촉 위치 지정</td></tr>
<tr><td>경계 박스 (Boxes)</td><td>박스의 좌상단과 우하단 좌표에 대한 위치 인코딩 쌍 생성</td><td>객체 검출 모델과의 연동을 통한 자동 세그멘테이션</td></tr>
<tr><td>텍스트 (Text)</td><td>CLIP의 텍스트 인코더로부터 추출된 임베딩 활용</td><td>자연어 명령을 통한 비정형 객체 식별 및 조작</td></tr>
<tr><td>마스크 (Masks)</td><td>합성곱 레이어를 통한 하향 샘플링 및 이미지 임베딩과의 요소별 합(\vert element-wise sum)</td><td>이전 프레임의 추적 정보나 거친 센서 데이터의 정제</td></tr>
</tbody></table>
<p>희소한 프롬프트는 256 차원의 임베딩 벡터 집합으로 변환되며, 텍스트의 경우 CLIP 모델과의 정렬을 통해 시각적 정보와 언어적 정보가 동일한 의미 공간 내에서 소통할 수 있도록 한다. 조밀한 프롬프트인 마스크는 입력 이미지와 동일한 해상도를 유지하다가 합성곱 층을 거치며 이미지 임베딩과 동일한 <span class="math math-inline">64 \times 64</span> 크기로 압축되어 특징 맵에 직접 가산된다.</p>
<h3>2.3 마스크 디코더(Mask Decoder): 실시간 마스크 생성 및 모호성 해결</h3>
<p>마스크 디코더는 SAM 아키텍처의 화룡점정으로, 이미지 임베딩과 프롬프트 임베딩을 융합하여 최종적인 픽셀 마스크를 출력한다. 디코더는 매우 가볍게 설계되어 웹 브라우저나 CPU 상에서도 수십 밀리초(ms) 내에 구동이 가능하다.</p>
<p>디코더의 핵심 구조는 양방향 트랜스포머(Two-way Transformer) 블록이다. 이 블록은 프롬프트 토큰이 이미지 정보를 참조하는 교차 어텐션(Cross-attention)과 이미지 임베딩이 프롬프트 정보를 참조하는 교차 어텐션을 번갈아 수행하며 정보를 정제한다. 이후 업데이트된 토큰들은 마스크 생성을 위한 하이퍼네트워크(Hypernetwork)로 전달된다.</p>
<p>로봇이 실세계에서 마주하는 가장 큰 시각적 난제 중 하나는 프롬프트의 모호성이다. 예를 들어, 로봇이 “저 물체를 잡아라“라는 명령과 함께 테이블 위의 컵 손잡이 지점을 프롬프트로 받았다면, 모델은 그것이 ’손잡이’만을 의미하는지, ’컵 전체’를 의미하는지 판단해야 한다. SAM은 이를 해결하기 위해 하나의 프롬프트에 대해 최대 세 개의 마스크(부분, 전체, 그리고 더 넓은 맥락)를 동시에 예측하는 멀티 마스크 출력 시스템을 갖추고 있다. 각 마스크에는 모델이 예측한 신뢰도 점수인 IoU 토큰이 부여되어, 로봇이 상황에 따라 가장 적절한 마스크를 선택하거나 사용자에게 되물을 수 있는 근거를 제공한다.</p>
<h2>3. SA-1B 데이터셋과 데이터 엔진의 혁신</h2>
<p>SAM의 놀라운 성능은 단순한 아키텍처의 우수성뿐만 아니라, 11억 개 이상의 마스크를 포함하는 SA-1B 데이터셋의 규모에서 기인한다. Meta는 수동 주석(Annotation)의 한계를 극복하기 위해 모델이 스스로 데이터를 생성하고 인간이 이를 검수하며 다시 모델을 학습시키는 ‘데이터 엔진(Data Engine)’ 기법을 도입하였다.</p>
<p>이 엔진은 세 단계로 운영되었다. 첫 번째는 ’지원형 수동 단계(Assisted-Manual Stage)’로, SAM 초기 버전의 도움을 받아 주석 작업자가 브라우저에서 상호작용하며 마스크를 생성했다. 두 번째는 ’반자동 단계(Semi-Automatic Stage)’로, 모델이 자동으로 찾아낸 객체 외에 작업자가 인식하지 못한 객체들을 추가로 레이블링하여 마스크의 다양성을 극대화했다. 마지막 ’전 자동 단계(Fully Automatic Stage)’에서는 고도화된 SAM이 이미지 전체에 격자 형태의 점을 투사하여 11억 개의 마스크를 자동으로 생성해냈다. 이 과정은 로봇 학습에 필요한 방대한 양의 시뮬레이션 및 실세계 데이터를 확보하는 새로운 표준을 제시했다.</p>
<h2>4. 로봇 제어 및 지각 기술과의 융합: SOTA 사례 연구</h2>
<p>로봇 공학 분야에서 SAM은 단순한 세그멘테이션 도구를 넘어, 로봇의 행동 결정(Decision Making)과 정밀 제어의 핵심 요소로 통합되고 있다.</p>
<h3>4.1 오픈 보캐블러리 파지 및 조작</h3>
<p>로봇이 임의의 물체를 집어 올리기 위해서는 해당 물체의 정확한 경계와 기하학적 형상을 파악해야 한다. SAM은 텍스트나 포인트를 통해 이 정보를 실시간으로 제공한다.</p>
<ul>
<li><strong>Grounded-SAM:</strong> Grounding DINO와 SAM을 결합하여, 자연어 명령(예: “테이블 위의 파란 가위”)으로부터 정확한 경계 박스를 찾고 이를 SAM의 프롬프트로 입력하여 픽셀 단위 마스크를 생성한다.</li>
<li><strong>MOKA(Mark-based Visual Prompting):</strong> Vision-Language Model(VLM)이 이미지에 표시된 시각적 마크를 보고 어포던스를 추론하면, SAM이 해당 지점의 정밀한 마스크를 생성하여 로봇 팔의 궤적을 계획한다.</li>
<li><strong>RoG-SAM:</strong> 언어 기반의 지시문을 객체 국지화와 파지 포즈 예측으로 연결하며, 인코더 어댑터를 통해 SAM을 로봇 조작 과업에 최적화한다.</li>
</ul>
<h3>4.2 D 공간 이해 및 내비게이션</h3>
<p>2D 세그멘테이션 정보를 3D 공간으로 투영함으로써 로봇은 더 정교한 공간 지능을 갖게 된다.</p>
<ul>
<li><strong>SAM3D:</strong> LiDAR 데이터를 2D BEV(Bird’s Eye View) 이미지로 변환한 후 SAM을 적용하여 미지의 장애물을 분할하고, 이를 다시 3D 경계 박스로 복원하여 자율 주행 및 내비게이션의 안정성을 높인다.</li>
<li><strong>Semantic Maps:</strong> 로봇이 이동하며 수집한 RGB-D 데이터에 SAM을 적용하여 객체 중심의 3D 시맨틱 지도를 구축한다. 이는 로봇이 단순히 위치를 기억하는 것이 아니라, “주방 냉장고 옆“과 같은 추상적 공간 개념을 이해하도록 돕는다.</li>
</ul>
<h3>4.3 실시간성 확보를 위한 경량화 전략</h3>
<p>SAM의 이미지 인코더는 연산량이 매우 많아 로봇의 엣지 컴퓨팅 환경에서 직접 구동하기 어려운 경우가 많다. 이를 극복하기 위한 SOTA 경량화 기술들이 지속적으로 발표되고 있다.</p>
<table><thead><tr><th><strong>모델명</strong></th><th><strong>최적화 기법</strong></th><th><strong>성능 특징</strong></th></tr></thead><tbody>
<tr><td>MobileSAM</td><td>지식 증류(Distillation)</td><td>ViT-H 대비 파라미터 1% 수준, 모바일 및 로봇 플랫폼 최적화</td></tr>
<tr><td>NanoSAM</td><td>ResNet-18 기반 인코더</td><td>NVIDIA Jetson 상에서 실시간 세그멘테이션 구현 가능</td></tr>
<tr><td>EfficientViT-SAM</td><td>선형 어텐션(Linear Attention)</td><td><span class="math math-inline">O(N^2)</span>의 어텐션 복잡도를 <span class="math math-inline">O(N)</span>으로 줄여 고해상도 처리 효율화</td></tr>
<tr><td>FastSAM</td><td>YOLOv8-seg 기반 설계</td><td>프롬프트 기반 분할을 객체 검출 속도로 수행</td></tr>
</tbody></table>
<h2>5. 미래 전망: SAM 2와 SAM 3로의 진화</h2>
<p>SAM의 혁신은 정적 이미지에 머물지 않고 동적인 비디오와 더 깊은 언어적 이해로 확장되고 있다. 최근 공개된 SAM 2는 비디오 스트림에서의 객체 추적(Tracking) 기능을 통합하여, 로봇이 이동하거나 물체가 움직이는 상황에서도 객체의 정체성을 유지하며 실시간 분할을 수행할 수 있도록 한다. 이는 로봇 조작 과정에서 물체가 가려지거나(Occlusion) 시야에서 벗어날 때 발생하는 지각 불안정성 문제를 획기적으로 개선한다.</p>
<p>더 나아가 SAM 3(혹은 차세대 모델)는 단순한 명사 중심의 분할을 넘어 “빨간 모자를 쓴 사람이 들고 있는 스마트폰“과 같은 복잡한 관계 중심의 텍스트 프롬프트를 이해하는 오픈 보캐블러리 능력을 더욱 심화시키고 있다. 이러한 기술적 진보는 로봇이 인간과 동일한 수준의 시각적 문해력을 갖추게 됨을 의미하며, Embodied AI가 실험실을 벗어나 복잡한 가정이나 산업 현장에 투입될 수 있는 강력한 기술적 기반이 된다.</p>
<p>결론적으로, SAM 아키텍처는 시각 정보의 단순한 레이블링을 넘어, 프롬프트를 통해 인공지능과 물리적 세계 사이의 동적인 인터페이스를 구축했다. 이는 로봇에게 ’무엇이든 볼 수 있는 눈’을 제공함으로써, 행동을 위한 지각(Perception for Action)이라는 로봇 공학의 궁극적 목표에 한 걸음 더 다가서게 했다. 로봇 공학자들에게 SAM은 더 이상 단순한 세그멘테이션 알고리즘이 아니라, 지능형 시스템을 구축하기 위한 필수적인 인프라이자 사고의 틀로 자리매김하고 있다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Segment Anything - CVF Open Access - The Computer Vision …, https://openaccess.thecvf.com/content/ICCV2023/papers/Kirillov_Segment_Anything_ICCV_2023_paper.pdf</li>
<li>Segment Anything – A Foundation Model for Image Segmentation, https://learnopencv.com/segment-anything/</li>
<li>Segment Anything - Promptable Segmentation of Arbitrary Objects, https://towardsdatascience.com/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d-2/</li>
<li>Segment Anything Model (SAM) — Detailed Explanation | by Chau …, https://chautuankien.medium.com/segment-anything-model-sam-detailed-explanation-21698094cd56</li>
<li>Segment Anything Model (SAM) - Ultralytics YOLO Docs, https://docs.ultralytics.com/models/sam/</li>
<li>SAM - Segment Anything model for promptable pixel … - YouTube, https://www.youtube.com/watch?v=L1TC2PoTu-Q</li>
<li>Segment Anything Model (SAM) - International Journal of Research …, https://rsisinternational.org/journals/ijriss/articles/segment-anything-model-sam/</li>
<li>Segment Anything Model (SAM) for Zero-Shot Segmentation, https://www.emergentmind.com/topics/segment-anything-model-sam</li>
<li>Segment Anything Model (SAM): Intro, Use Cases, V7 Tutorial, https://www.v7labs.com/blog/segment-anything-model-sam</li>
<li>What is Segment Anything Model (SAM)? A Breakdown., https://blog.roboflow.com/segment-anything-breakdown/</li>
<li>(PDF) Principles, applications, and advancements of the Segment …, https://www.researchgate.net/publication/379368000_Principles_applications_and_advancements_of_the_Segment_Anything_Model</li>
<li>SAM-E: Leveraging Visual Foundation Model with Sequence … - arXiv, https://arxiv.org/html/2405.19586v1</li>
<li>A Multi-Step Grasping Framework for Zero-Shot Object Detection in …, https://www.mdpi.com/1424-8220/25/23/7125</li>
<li>What is Segment Anything Model (SAM) &amp; its Uses - Sama, https://www.sama.com/blog/what-is-segment-anything-model-sam-its-uses</li>
<li>Real-time open-vocabulary perception for mobile robots on edge …, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1693988/full</li>
<li>MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based …, https://openreview.net/pdf/1a8c7f6d7a66a90052b0add51f6510d5de277a72.pdf</li>
<li>Inside SAM 3: The Next Generation of Meta’s Segment Anything Model, https://sodevelopment.medium.com/inside-sam-3-the-next-generation-of-metas-segment-anything-model-dfce8eae4c89</li>
<li>Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V, https://arxiv.org/html/2404.10220v2</li>
<li>SAM 2: Segment Anything in Images and Videos - arXiv, https://arxiv.org/html/2408.00714v2</li>
<li>SAM2Act: Integrating Visual Foundation Model with A Memory…, https://openreview.net/forum?id=anSWDvJm8v</li>
<li>Unleashing Potential of Segmenting Ambiguous Objects in SAM, <a href="https://openreview.net/forum?id=vJSNsSFO95&amp;noteId=QSthAlrX3T">https://openreview.net/forum?id=vJSNsSFO95¬eId=QSthAlrX3T</a></li>
<li>Generalist Vision Foundation Models for Medical Imaging - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC10252742/</li>
<li>A Survey on Segment Anything Model (SAM) - arXiv, https://arxiv.org/html/2306.06211v4</li>
<li>SA-1B - Ecosystem Graphs for Foundation Models, https://crfm.stanford.edu/ecosystem-graphs/index.html?asset=SA-1B</li>
<li>SA-1B Dataset: Segmentation Benchmark - Emergent Mind, https://www.emergentmind.com/topics/sa-1b-dataset</li>
<li>facebookresearch/segment-anything - GitHub, https://github.com/facebookresearch/segment-anything</li>
<li>A Real 3D Embodied Dataset for Robotic Active Visual Learning, https://www.researchgate.net/publication/359072048_A_Real_3D_Embodied_Dataset_for_Robotic_Active_Visual_Learning</li>
<li>A Language-Driven Framework for Instance-Level Robotic Grasping …, https://www.researchgate.net/publication/390479398_RoG-SAM_A_Language-Driven_Framework_for_Instance-Level_Robotic_Grasping_Detection</li>
<li>Open-vocabulary Mobile Manipulation in Unseen Dynamic …, https://www.alphaxiv.org/overview/2406.18115v1</li>
<li>SAM3D: zero-shot 3D object detection via the segment anything model, http://scis.scichina.com/en/2024/149101.pdf</li>
<li>SAM 2 in Robotic Surgery: An Empirical Evaluation for Robustness …, https://www.researchgate.net/publication/399380585_SAM_2_in_Robotic_Surgery_An_Empirical_Evaluation_for_Robustness_and_Generalization_in_Surgical_Video_Segmentation</li>
<li>Exploring SAM 3: Meta AI’s new Segment Anything Model - Ultralytics, https://www.ultralytics.com/blog/exploring-sam-3-meta-ais-new-segment-anything-model</li>
<li>SAM2Act: Integrating Visual Foundation Model with A Memory …, https://arxiv.org/html/2501.18564v4</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>