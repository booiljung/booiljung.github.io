<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:5.1 표현의 진화: 이산(Discrete)에서 연속(Continuous)으로</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>5.1 표현의 진화: 이산(Discrete)에서 연속(Continuous)으로</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 5. 뉴럴 3D 표현과 렌더링 (Neural 3D Representations)</a> / <a href="index.html">5.1 표현의 진화: 이산(Discrete)에서 연속(Continuous)으로</a> / <span>5.1 표현의 진화: 이산(Discrete)에서 연속(Continuous)으로</span></nav>
                </div>
            </header>
            <article>
                <h1>5.1 표현의 진화: 이산(Discrete)에서 연속(Continuous)으로</h1>
<h2>1.  서론: 디지털 원자론의 종말과 연속체의 부상</h2>
<p>로봇 공학의 역사는 물리적 세계를 기계가 이해할 수 있는 데이터 구조로 환원하려는 시도의 연속이었다. 초기 로봇 시스템은 현실을 인식하기 위해 공간을 유한한 단위로 쪼개어 저장하는 ’디지털 원자론(Digital Atomism)’에 의존해 왔다. 2차원 이미지를 구성하는 픽셀(Pixel), 3차원 공간을 채우는 복셀(Voxel), 그리고 물체의 표면을 근사하는 포인트 클라우드(Point Cloud)는 모두 현실 세계를 불연속적인 점들의 집합으로 치환하는 이산적 표현(Discrete Representation) 방식들이다. 이러한 접근법은 직관적이고 컴퓨터 메모리 구조에 매핑하기 용이하다는 장점이 있었으나, ’해상도(Resolution)’라는 근본적인 감옥에 갇혀 있었다. 데이터를 정밀하게 표현하려 할수록 저장 용량과 계산 비용이 기하급수적으로 증가하는 차원의 저주(Curse of Dimensionality)는 로봇이 복잡하고 비정형적인 환경에서 고도로 정밀한 작업을 수행하는 것을 가로막는 주된 장벽이었다.</p>
<p>그러나 최근 인공지능과 컴퓨터 비전, 그리고 로봇 제어 이론의 교차점에서 일어나고 있는 패러다임의 전환은 이러한 이산적 세계관을 근본적으로 뒤흔들고 있다. <strong>암시적 신경 표현(Implicit Neural Representations, INR)</strong> 혹은 **신경 필드(Neural Fields)**라 불리는 새로운 방법론의 등장은 데이터를 ’값들의 이산적 나열’이 아닌, 좌표를 입력받아 물리적 속성을 출력하는 ’연속적인 함수(Continuous Function)’로 정의한다.1 이 전환은 단순히 데이터 압축이나 효율성의 문제를 넘어선다. 불연속적인 점들의 집합이었던 로봇의 세계가 미분 가능한(Differentiable) 수학적 필드로 변환됨으로써, 로봇의 인지(Perception), 계획(Planning), 그리고 제어(Control)가 하나의 거대한 최적화 루프 안에서 통합될 수 있는 길이 열린 것이다.4</p>
<p>본 장에서는 로봇 공학의 핵심 데이터 구조가 이산적 형태에서 연속적 함수 형태로 진화하는 과정을 심층적으로 분석한다. 포인트 클라우드와 복셀, 메쉬(Mesh)가 가진 구조적 한계와 그로 인한 로봇 제어의 난제들을 규명하고, 좌표 기반 신경망(Coordinate-based Neural Networks)이 어떻게 ’무한 해상도(Infinite Resolution)’와 ’미분 가능성’이라는 두 가지 혁신적 특성을 제공하는지 이론적으로 고찰한다. 나아가, 이러한 연속적 표현이 로봇의 시스템 식별(System Identification), 궤적 최적화(Trajectory Optimization), 그리고 언어-비전-행동 모델(VLA)과의 통합에 미치는 심대한 영향을 구체적인 연구 사례와 함께 논증할 것이다.</p>
<h2>2.  이산 표현(Discrete Representation)의 시대와 구조적 한계</h2>
<p>지난 수십 년간 로봇 공학자들은 센서로부터 획득한 데이터를 처리하기 위해 다양한 이산적 자료 구조를 개발하고 최적화해 왔다. LiDAR, RGB-D 카메라, 스테레오 비전 등 대부분의 로봇 센서는 본질적으로 이산적인 샘플링 데이터를 출력한다. 이러한 데이터를 있는 그대로, 혹은 약간의 가공을 거쳐 사용하는 방식은 초기 로봇 시스템의 표준이 되었으나, 로봇에게 요구되는 작업의 난이도가 높아짐에 따라 그 한계가 명확히 드러나고 있다.</p>
<h3>2.1  포인트 클라우드(Point Cloud): 연결성의 부재와 희소성</h3>
<p>포인트 클라우드는 3D 공간상에 존재하는 점들의 집합 <span class="math math-inline">P = \{ \mathbf{x}_i \in \mathbb{R}^3 \}_{i=1}^N</span>으로 표현된다. 이는 LiDAR 센서가 레이저를 발사하여 반사되어 돌아오는 시간을 측정함으로써 얻어지는 가장 원초적인 형태의 3D 데이터이다.</p>
<ul>
<li><strong>직관성과 획득의 용이성:</strong> 포인트 클라우드는 센서 데이터와 1:1로 매핑되므로 별도의 복잡한 전처리 없이 즉시 사용할 수 있다는 장점이 있다. 또한, 물체가 존재하지 않는 빈 공간을 명시적으로 표현할 필요가 없으므로, 광활한 실외 환경(자율주행 등)을 표현할 때 메모리 효율적이다.7</li>
<li><strong>연결성 정보의 결여 (Lack of Connectivity):</strong> 포인트 클라우드의 가장 치명적인 약점은 위상학적(Topological) 정보의 부재다. 점들의 집합만으로는 어떤 점들이 동일한 물체의 표면을 구성하는지, 혹은 물체의 내부와 외부가 어디인지 명확히 정의할 수 없다.8 예를 들어, 로봇 팔이 물체를 파지(Grasping)하려 할 때, 포인트 클라우드만으로는 손가락이 물체를 관통했는지, 표면에 닿았는지 판단하기 위해 별도의 복잡한 연산(예: Normal estimation, Surface reconstruction)이 필요하다.</li>
<li><strong>거리 의존적 희소성(Sparsity)과 앨리어싱:</strong> 센서로부터 거리가 멀어질수록 포인트 간의 간격은 넓어진다. 참고 문헌 9에 따르면, 300피트 떨어진 차량은 불과 몇 십 개의 점으로 표현되어 그 형체를 식별하기 어렵게 된다. 또한 전신주나 교통 표지판과 같이 얇은 구조물은 포인트 간격 사이에 위치할 경우 아예 데이터에서 사라지는 앨리어싱(Aliasing) 현상이 발생하여, 자율주행 로봇의 충돌 위험을 높이는 원인이 된다. 이를 보완하기 위해 포인트의 크기를 인위적으로 키우거나 색상을 조정하는 시각적 기법이 사용되기도 하지만, 이는 근본적인 기하학적 정보의 손실을 막지 못한다.</li>
</ul>
<h3>2.2  복셀 그리드(Voxel Grid): 입방체의 감옥과 차원의 저주</h3>
<p>복셀(Voxel)은 2D 이미지의 픽셀을 3차원으로 확장한 개념으로, 공간을 균일한 크기의 격자(Grid)로 분할하고 각 격자에 점유 여부(Occupancy)나 물리적 속성(색상, 밀도 등)을 저장한다.</p>
<ul>
<li><strong>구조적 규칙성과 CNN의 적용:</strong> 복셀 데이터는 규칙적인 3차원 배열 구조를 가지므로, 이미지 처리에 널리 쓰이는 합성곱 신경망(CNN)을 3D로 확장(3D CNN)하여 적용하기에 최적화되어 있다. VoxNet7과 같은 초기 3D 딥러닝 모델들은 복셀 표현을 통해 3D 객체 인식에서 비약적인 성능 향상을 이루었다.</li>
<li><strong>세제곱의 메모리 복잡도 (<span class="math math-inline">O(N^3)</span>):</strong> 복셀 표현의 가장 큰 적은 소위 ’차원의 저주’라 불리는 메모리 비효율성이다. 공간의 해상도를 2배 높이면(예: 격자 크기를 절반으로 줄이면), 필요한 메모리 공간은 <span class="math math-inline">2^3=8</span>배로 증가한다. 이는 고해상도의 정밀한 환경 맵을 유지하는 것을 불가능하게 만들며, 대부분의 로봇 시스템이 매우 거친(Coarse) 해상도의 복셀 맵을 사용할 수밖에 없는 이유가 된다.3</li>
<li><strong>정보의 손실과 양자화 오차:</strong> 하나의 복셀 격자 내에 여러 개의 서로 다른 기하학적 특징이 존재하더라도, 이는 단일 값으로 병합(Binning)된다. 복잡하고 미세한 구조물은 복셀화(Voxelization) 과정에서 뭉개지거나 사라지며, 이는 정밀한 조작 작업에 필요한 기하학적 충실도(Fidelity)를 훼손한다.</li>
<li><strong>빈 공간의 메모리 점유:</strong> 로봇이 활동하는 공간의 90% 이상은 비어 있는 공간(Free Space)이다. 그러나 밀집된(Dense) 복셀 그리드는 물체의 유무와 관계없이 전체 공간에 대해 메모리를 할당해야 한다. 이를 해결하기 위해 옥트리(Octree)나 해시 그리드(Hash Grid)와 같은 희소(Sparse) 자료구조가 도입되었으나, 이는 데이터 접근의 복잡성을 높이고 GPU 병렬 처리를 어렵게 만드는 트레이드오프를 가진다.7</li>
</ul>
<h3>2.3  메쉬(Mesh): 위상학적 경직성</h3>
<p>폴리곤 메쉬(Polygon Mesh)는 정점(Vertex), 엣지(Edge), 면(Face)으로 구성되며, 컴퓨터 그래픽스와 CAD(Computer-Aided Design) 분야의 표준 표현 방식이다.</p>
<ul>
<li><strong>표면 표현의 효율성:</strong> 메쉬는 물체의 표면만을 표현하므로 복셀보다 메모리 효율적이며, 텍스처 매핑을 통해 시각적 디테일을 효과적으로 표현할 수 있다.8</li>
<li><strong>위상 변경의 난이도:</strong> 로봇 시뮬레이션, 특히 변형 가능한 물체(Deformable Object)나 유체와의 상호작용을 다룰 때 메쉬는 한계를 드러낸다. 물체가 찢어지거나, 두 물체가 합쳐지거나, 구멍이 뚫리는 위상학적 변화(Topological Change)를 메쉬로 표현하려면 정점과 연결 정보를 동적으로 재구성(Remeshing)해야 한다. 이는 계산적으로 매우 복잡하며, 미분 불가능한 연산을 포함하는 경우가 많아 딥러닝 기반의 최적화 파이프라인에 통합하기 어렵다.10</li>
</ul>
<h3>2.4  이산 최적화(Discrete Optimization)의 딜레마</h3>
<p>이산 표현의 근본적인 한계는 로봇의 경로 계획 및 제어 단계에서 가장 극명하게 드러난다.</p>
<ul>
<li><strong>미분 불가능성(Non-differentiability):</strong> 이산 표현 기반의 충돌 감지 알고리즘은 일반적으로 이진 값(충돌함/충돌안함)을 반환한다. 이는 “충돌을 피하기 위해 어느 방향으로 얼마나 움직여야 하는지“에 대한 구배(Gradient) 정보를 제공하지 못한다. 따라서 로봇은 경사 하강법과 같은 효율적인 최적화 기법 대신, 무작위 샘플링(RRT, PRM)이나 이산 탐색(A*)과 같은 계산 집약적인 방법에 의존해야 한다.12</li>
<li><strong>최적화 랜드스케이프의 거칠음:</strong> 이산 표현 위에서의 최적화 문제는 탐색 공간이 매끄럽지 않고 수많은 국소 최저점(Local Minima)을 가진다. 이는 로봇의 동작을 생성하는 데 있어 수렴 속도를 늦추고, 생성된 궤적이 부자연스럽거나 최적해와 거리가 먼 결과를 초래한다.14</li>
</ul>
<h2>3.  수학적 전환: 암시적 신경 표현(Implicit Neural Representations)</h2>
<p>이산 표현이 가진 해상도의 한계와 미분의 단절을 극복하기 위해 등장한 개념이 바로 **암시적 신경 표현(INR)**이다. 이는 대상을 이산적인 데이터의 나열로 저장하는 것이 아니라, 대상을 설명하는 **연속 함수(Continuous Function)**를 신경망으로 근사(Approximate)하여 저장하는 방식이다. 데이터가 ’값’에서 ’함수’로 그 존재 양식을 달리하는 것이다.</p>
<h3>3.1  데이터로서의 함수 (Data as a Function)</h3>
<p>암시적 표현의 핵심 철학은 데이터를 입력 좌표 <span class="math math-inline">\mathbf{x}</span>에 대해 특정 물리량 <span class="math math-inline">v</span>를 출력하는 함수 <span class="math math-inline">f_\theta</span>로 모델링하는 것이다.<br />
<span class="math math-display">
v = f_\theta(\mathbf{x}), \quad \mathbf{x} \in \mathbb{R}^n, v \in \mathbb{R}^m
</span><br />
여기서 <span class="math math-inline">\mathbf{x}</span>는 2D 이미지의 픽셀 좌표 <span class="math math-inline">(x, y)</span>, 3D 공간의 좌표 <span class="math math-inline">(x, y, z)</span>, 혹은 시간 축이 포함된 시공간 좌표 <span class="math math-inline">(x, y, z, t)</span>일 수 있다. 출력값 <span class="math math-inline">v</span>는 해당 좌표에서의 RGB 색상, 점유 확률(Occupancy), 부호 거리 값(Signed Distance), 밀도(Density) 등을 나타낸다. 신경망의 가중치 <span class="math math-inline">\theta</span>가 곧 데이터를 저장하는 매체가 된다.1</p>
<ul>
<li><strong>좌표 기반 신경망(Coordinate-based Neural Networks):</strong> INR은 일반적으로 완전 연결 신경망(Fully Connected Network) 혹은 다층 퍼셉트론(MLP)으로 구현된다. 기존의 CNN이 이미지를 픽셀 그리드로 처리했다면, INR은 픽셀의 좌표 자체를 입력으로 받아 그 위치의 색상을 예측하도록 학습된다. 이 단순해 보이는 구조적 변화는 데이터의 해상도 제약을 없애는 혁신적인 결과를 가져왔다.16</li>
</ul>
<h3>3.2  무한 해상도(Infinite Resolution)의 실현</h3>
<p>INR의 가장 강력한 특징은 **해상도 독립성(Resolution Independence)**이다. 신경망은 이산적인 샘플들을 통해 학습되지만, 학습된 결과물은 연속적인 함수이므로 이론적으로 무한한 해상도를 가진다.2</p>
<ul>
<li><strong>임의 해상도 샘플링:</strong> 학습된 신경망 <span class="math math-inline">f_\theta</span>에 임의의 조밀한 좌표를 입력하면, 그에 해당하는 값을 얻을 수 있다. 이는 저해상도 이미지나 희소한 포인트 클라우드로 학습한 모델이라도, 추론 단계에서 더 조밀한 좌표를 입력함으로써 고해상도 이미지를 렌더링하거나 부드러운 표면을 복원할 수 있음을 의미한다.18 이는 의료 영상 분석에서 MRI나 CT 스캔의 해상도를 높이는 초해상도(Super-resolution) 작업이나, 로봇이 거친 센서 데이터로부터 정밀한 3D 지도를 생성하는 데 직접적으로 응용된다.</li>
<li><strong>메모리 압축성과 복잡도 비례성:</strong> 복셀 그리드는 공간의 물리적 크기에 비례하여 메모리를 소모하지만, INR은 형상의 **복잡도(Complexity)**에 비례하여 신경망의 용량(가중치 수)이 결정된다. 예를 들어, 거대한 평평한 벽은 단 몇 개의 뉴런만으로 완벽하게 표현될 수 있다. 이는 대규모 데이터를 콤팩트하게 압축하여 저장하고 전송하는 새로운 매체로서 INR의 가능성을 시사한다. 특히 대역폭이 제한된 로봇 통신 환경에서, 거대한 포인트 클라우드 파일 대신 가벼운 신경망 모델 파일만을 전송하여 3D 정보를 공유하는 시나리오가 가능해진다.2</li>
</ul>
<h3>3.3  스펙트럼 편향(Spectral Bias) 극복과 푸리에 특징</h3>
<p>초기 INR 연구에서 일반적인 MLP(ReLU 활성화 함수 사용)는 고주파(High-frequency) 세부 정보를 학습하지 못하고 이미지를 흐릿하게 표현하는 문제가 있었다. 이는 신경망이 저주파 성분을 먼저 학습하려는 경향인 <strong>스펙트럼 편향(Spectral Bias)</strong> 때문임이 이론적으로 밝혀졌다.21</p>
<ul>
<li>
<p>위치 인코딩(Positional Encoding)과 푸리에 특징: 이를 해결하기 위해 입력 좌표 <span class="math math-inline">\mathbf{x}</span>를 고차원 공간으로 매핑하는 푸리에 특징 매핑(Fourier Feature Mapping)이 필수적으로 도입되었다.<br />
<span class="math math-display">
\gamma(\mathbf{x}) = [\sin(2^0\pi\mathbf{x}), \cos(2^0\pi\mathbf{x}), \dots, \sin(2^{L-1}\pi\mathbf{x}), \cos(2^{L-1}\pi\mathbf{x})]
</span><br />
이러한 매핑은 저차원 좌표 입력을 다양한 주파수 대역으로 분해하여 신경망이 고주파 디테일(예: 텍스처의 질감, 날카로운 모서리)을 효과적으로 학습할 수 있게 했다. 이는 NeRF(Neural Radiance Fields)가 사진과 같은 품질의 렌더링을 달성할 수 있었던 핵심 기술 중 하나이다.21</p>
</li>
<li>
<p><strong>SIREN (Sinusoidal Representation Networks):</strong> 또 다른 접근법으로, 활성화 함수 자체를 주기 함수인 사인(Sine) 함수로 사용하는 SIREN 구조가 제안되었다. SIREN은 신호의 고주파 성분을 잘 모델링할 뿐만 아니라, 함수의 1계, 2계 도함수(Gradients, Laplacians) 정보까지도 잘 보존한다. 이는 미분 방정식을 풀어야 하는 물리 정보 기반 학습(PINN)이나, 표면의 곡률 정보가 중요한 로봇 파지 계획 등에 탁월한 성능을 보인다.24</p>
</li>
</ul>
<h3>3.4 표 5.1.1: 이산 표현과 연속(암시적) 표현의 상세 비교</h3>
<table><thead><tr><th><strong>특성 (Feature)</strong></th><th><strong>이산 표현 (Discrete: Point Cloud, Voxel, Mesh)</strong></th><th><strong>연속/암시적 표현 (Continuous/Implicit: INR, SDF, NeRF)</strong></th></tr></thead><tbody>
<tr><td><strong>기본 단위</strong></td><td>점(Point), 격자(Voxel), 다각형(Polygon)</td><td>신경망 가중치(Weights), 함수(Function)</td></tr>
<tr><td><strong>해상도 (Resolution)</strong></td><td>고정됨 (Fixed), 확대 시 계단 현상(Aliasing) 발생</td><td><strong>무한함 (Infinite)</strong>, 임의 해상도 쿼리 및 샘플링 가능 2</td></tr>
<tr><td><strong>메모리 효율성</strong></td><td>공간 크기/해상도에 따라 급증 (<span class="math math-inline">O(N^3)</span> for Voxels)</td><td>**신호의 복잡도(Complexity)**에 비례, 공간 크기와 무관 3</td></tr>
<tr><td><strong>위상(Topology) 유연성</strong></td><td>메쉬는 위상 변경 어려움, 포인트/복셀은 용이하나 연결성 부족</td><td>위상 제약 없음, 임의의 위상 변화 자연스럽게 표현 가능 11</td></tr>
<tr><td><strong>미분 가능성 (Differentiability)</strong></td><td>대부분 미분 불가능하거나 근사(Approximation) 필요</td><td><strong>완전 미분 가능 (End-to-End Differentiable)</strong> 4</td></tr>
<tr><td><strong>빈 공간 표현</strong></td><td>복셀: 명시적 저장(낭비), 포인트: 표현 안 함(정보 부족)</td><td>함수 값으로 내재적 표현 (예: SDF &gt; 0), 메모리 낭비 없음</td></tr>
<tr><td><strong>주요 응용 분야</strong></td><td>고전적 SLAM, 실시간 장애물 감지, CAD, 게임 엔진</td><td>뉴럴 렌더링, 궤적 최적화, 물리 시뮬레이션, 생성 모델, 의료 영상</td></tr>
<tr><td><strong>한계점</strong></td><td>차원의 저주, 최적화 적용 난해, 데이터 희소성 문제</td><td>학습/추론 속도(최근 개선 중), 치명적 망각(Catastrophic Forgetting)</td></tr>
</tbody></table>
<h2>4.  기하학의 미분 가능한 필드화 (Geometry as a Differentiable Field)</h2>
<p>이산적 표현에서 연속적 표현으로의 진화는 단순히 데이터를 저장하는 방식을 넘어, 로봇이 환경과 상호작용하는 방식을 ’미분 가능한 수학적 문제’로 변환시켰다. 이산적 기하학이 연속적인 장(Field)으로 변환되면서, 로봇은 자신의 행동에 대한 환경의 반응을 미분을 통해 예측할 수 있게 되었다.</p>
<h3>4.1  부호 거리 함수(Signed Distance Functions, SDF)</h3>
<p>SDF는 임의의 점 <span class="math math-inline">\mathbf{x}</span>에서 물체의 표면까지의 최단 거리를 반환하는 함수 <span class="math math-inline">f(\mathbf{x}) = d</span>이다. 여기서 <span class="math math-inline">d</span>의 부호는 점이 물체의 외부(<span class="math math-inline">d&gt;0</span>)에 있는지 내부(<span class="math math-inline">d&lt;0</span>)에 있는지를 나타내며, <span class="math math-inline">d=0</span>인 지점들의 집합이 곧 물체의 표면(Zero-level set)이 된다.</p>
<ul>
<li><strong>DeepSDF와 연속적 형상 복원:</strong> 기존에는 SDF를 복셀 그리드에 이산적으로 저장(TSDF, Truncated SDF)했으나, DeepSDF17와 같은 연구는 이를 신경망으로 근사했다. 신경망 기반 SDF는 공간 전체에서 연속적으로 정의되므로, 로봇은 공간상의 어떤 위치에서든 표면까지의 정확한 거리와 **표면 법선 벡터(Normal Vector, <span class="math math-inline">\mathbf{n} = \nabla f(\mathbf{x})</span>)**를 해석적으로 계산할 수 있다. 법선 벡터는 거리 함수의 구배(Gradient)와 같으므로, 별도의 복잡한 계산 없이 역전파만으로 얻을 수 있다.</li>
<li><strong>충돌 처리의 혁명:</strong> 이산 표현에서는 충돌 여부(Boolean)만 알 수 있었지만, SDF를 사용하면 “충돌을 피하기 위해 어느 방향으로 얼마나 움직여야 하는지“에 대한 명확한 벡터 정보를 얻을 수 있다. 이는 로봇 제어 최적화 문제에서 비용 함수(Cost Function)에 충돌 패널티를 미분 가능한 형태로 포함시킬 수 있게 하여, 최적화 기반의 동작 생성을 가능하게 한다.27</li>
</ul>
<h3>4.2  NeRF: 렌더링의 미분화와 시각적 이해</h3>
<p>NeRF(Neural Radiance Fields)는 3D 장면을 밀도(Density, <span class="math math-inline">\sigma</span>)와 색상(Color, <span class="math math-inline">\mathbf{c}</span>)을 출력하는 함수로 표현하고, 볼륨 렌더링(Volume Rendering) 기법을 통해 2D 이미지를 생성한다.17</p>
<ul>
<li><strong>미분 가능한 렌더링(Differentiable Rendering):</strong> NeRF의 렌더링 과정은 미분 가능한 연산들(적분, 투영 등)로 구성된다. 즉, 렌더링된 이미지와 실제 관측된 이미지 사이의 오차를 계산하고, 이를 역전파(Backpropagation)하여 3D 장면의 기하학적 구조뿐만 아니라, 카메라의 위치나 로봇의 관절 각도(Pose)까지도 최적화할 수 있다.4 이는 “로봇이 본 것과 실제 세상의 차이“를 줄이는 방향으로 로봇의 상태를 스스로 보정할 수 있음을 의미한다.</li>
<li><strong>조명과 뷰 의존성 모델링:</strong> NeRF는 보는 각도에 따라 달라지는 반사광(View-dependent effects)까지 모델링할 수 있다. 이는 로봇이 유리, 금속, 거울 등 기존의 LiDAR나 Depth 카메라가 취약했던 반사 재질이나 투명한 물체를 인식하는 데 큰 도움을 준다. 로봇은 이제 물체의 형상뿐만 아니라 광학적 특성까지 내재적으로 이해할 수 있게 되었다.</li>
</ul>
<h3>4.3  하이브리드 접근: 3D Gaussian Splatting</h3>
<p>순수 Implicit 방식(NeRF)은 학습과 렌더링 속도가 느리다는 단점이 있다. 이를 보완하기 위해 이산적 요소와 연속적 렌더링을 결합한 **3D Gaussian Splatting (3DGS)**이 최근 급부상하고 있다.30</p>
<ul>
<li><strong>빠른 속도와 미분 가능성의 조화:</strong> 3DGS는 장면을 수많은 3D 가우시안 타원체들의 집합으로 표현한다. 각 가우시안은 위치, 크기, 회전, 색상, 투명도 파라미터를 가지며, 이를 2D 화면에 투영(Splatting)하여 이미지를 만든다. 이 과정은 NeRF보다 훨씬 빠르면서도 여전히 완전히 미분 가능하다. 이는 이산적 표현(점 기반)의 효율성과 연속적 표현(Splatting 및 미분)의 장점을 동시에 취한 사례로, 실시간 로봇 제어 루프에 통합될 수 있는 높은 잠재력을 보여준다.31</li>
</ul>
<h2>5.  최적화의 혁명: 구배 기반 로봇 제어 (Gradient-Based Robot Control)</h2>
<p>표현의 진화가 가져온 가장 실질적이고 파괴적인 변화는 로봇 제어 및 계획 문제를 ’최적화 문제’로 풀 수 있게 되었다는 점이다. 이산 표현이 가로막고 있던 ’미분의 단절’이 사라지면서, 픽셀(Pixel)에서 토크(Torque)까지 이어지는 **완전한 미분 가능한 파이프라인(End-to-End Differentiable Pipeline)**이 구축되고 있다.</p>
<h3>5.1  미분 가능한 시뮬레이션과 시스템 식별 (System Identification)</h3>
<p>로봇이 미지의 물체를 조작할 때, 마찰 계수, 탄성, 질량 분포와 같은 물리적 파라미터를 정확히 아는 것은 매우 어렵다. 전통적인 방식은 이를 추정하기 위해 복잡한 수식이나 별도의 센서를 필요로 했다.</p>
<ul>
<li><strong>GradSim과 DiffTactile:</strong> GradSim33이나 DiffTactile35과 같은 프레임워크는 물리 시뮬레이터 자체를 미분 가능하게 만들었다. 이는 시뮬레이션의 결과(로봇의 움직임, 물체의 변형)와 실제 관측 결과(비디오)의 차이를 계산하고, 이 오차를 물리 엔진의 파라미터로 역전파한다. 이를 통해 로봇은 비디오를 보는 것만으로 물체의 질량, 마찰력, 탄성 계수 등을 스스로 학습(System Identification)할 수 있다. 예를 들어, GradSim은 비디오에서 천이나 유연체가 떨어지는 모습을 관찰하여 그 재질의 물성치를 정확하게 찾아낸다.</li>
<li><strong>연속적 접촉 모델링:</strong> 기존의 강체 시뮬레이터는 접촉(Contact)을 불연속적인 이벤트로 처리하여 미분이 불가능했다. 하지만 SDF 기반의 연속적 접촉 모델이나 딥러닝 기반의 유연체(Soft body) 시뮬레이션은 접촉면에서의 힘을 연속 함수로 근사한다. 이를 통해 파지(Grasping)나 조작(Manipulation) 동작을 구배 하강법(Gradient Descent)으로 미세 조정하여 최적의 제어 입력을 찾을 수 있게 되었다.37</li>
</ul>
<h3>5.2  궤적 최적화 (Trajectory Optimization)와 충돌 회피</h3>
<p>SDF 기반의 환경 표현은 로봇의 궤적 계획을 획기적으로 단순화한다.</p>
<ul>
<li><strong>구배 기반 회피(Gradient-based Avoidance):</strong> 장애물을 SDF로 표현하면, 로봇의 모든 링크 표면에서 장애물까지의 거리와 그 기울기(Gradient)를 즉시 알 수 있다. <span class="math math-inline">Cost = \sum ||\text{goal} - \text{current}||^2 + \lambda \sum \text{SDF}(\text{obstacle})</span> 형태의 비용 함수를 정의하면, 충돌을 피하면서 목표 지점으로 이동하는 궤적을 경사 하강법만으로 빠르고 부드럽게 생성할 수 있다.27 이는 샘플링 기반 알고리즘(RRT)이 생성하는 삐뚤빼뚤한 경로와 달리, 물리적으로 실행 가능하고 매끄러운 경로를 보장한다.</li>
<li><strong>구성 공간(Configuration Space) 거리 필드 (CDF):</strong> 최근 연구는 작업 공간(Task Space)뿐만 아니라, 로봇의 관절 각도로 이루어진 구성 공간(C-Space)에서의 거리 필드를 신경망으로 학습하고 있다. CDF를 사용하면 역기구학(IK)을 풀 필요 없이, 관절 공간에서 직접 충돌 없는 구성을 실시간으로 찾을 수 있어 계획 속도를 비약적으로 높일 수 있다.28</li>
</ul>
<h3>5.3  Dr. Robot: 시각에서 제어까지</h3>
<p>’Dr. Robot’4과 같은 최신 연구는 로봇의 외형(Appearance)과 기구학(Kinematics)을 미분 가능한 모델로 통합한 대표적인 사례이다.</p>
<ul>
<li><strong>픽셀 그라디언트 활용:</strong> Dr. Robot은 로봇의 각 링크를 3D Gaussian Splatting으로 모델링하고, 기구학적 체인(Kinematic Chain)을 통해 이들을 연결한다. 로봇이 수행해야 할 작업이 이미지로 주어졌을 때(예: “이 사진처럼 팔을 움직여라”), 현재 로봇의 렌더링 이미지와 목표 이미지 간의 픽셀 차이를 계산하고, 이 오차를 역전파하여 로봇의 **관절 각도(Joint Angles)**를 직접 업데이트한다. 이는 별도의 특징 추출(Keypoint detection)이나 명시적인 위치 추정 없이도, 원본 시각 정보(Raw Pixels)만으로 로봇을 정밀하게 제어할 수 있음을 입증하였다.5 더 나아가 충돌 방지 제약 조건을 SDF 형태로 손실 함수에 추가하여, 시각적 목표를 달성하면서도 자기 충돌(Self-collision)이나 환경 충돌을 피하는 동작을 생성할 수 있다.32</li>
</ul>
<h2>6.  연속 패러다임의 심화: 응용 및 사례 연구</h2>
<p>연속적 표현은 로봇 공학의 난제들을 해결하는 새로운 열쇠가 되고 있다. SLAM, 조작, 그리고 대형 언어 모델과의 통합에 이르기까지 그 응용 범위는 광범위하다.</p>
<h3>6.1  신경망 기반 SLAM (Neural SLAM)</h3>
<p>전통적인 SLAM(동시적 위치 추정 및 지도 작성)은 포인트 클라우드나 복셀 맵을 사용했으나, 최근에는 장면 전체를 신경망 가중치로 저장하는 Neural SLAM이 주목받고 있다.</p>
<ul>
<li><strong>iMAP과 NICE-SLAM:</strong> iMAP은 단일 MLP로 방 전체를 표현하여 메모리를 획기적으로 줄였다(수 MB 수준). 하지만 큰 환경에서는 디테일이 뭉개지는 현상이 있었다. 이를 개선한 NICE-SLAM40은 계층적 특징 그리드(Hierarchical Feature Grid)를 도입하여, 로컬한 세부 정보는 격자에 저장하고 전역적인 구조는 MLP가 학습하도록 했다. 이를 통해 대규모 환경에서도 세밀한 기하학적 구조를 실시간으로 복원하고 추적할 수 있게 되었다.</li>
<li><strong>빈틈없는 매핑(Hole-filling)과 예측:</strong> INR은 관측되지 않은 영역에 대해서도 주변 정보를 바탕으로 그럴듯한(Plausible) 값을 예측하여 채워 넣는 보간(Interpolation) 능력이 뛰어나다. 이는 센서 데이터가 희소하거나 가려진 영역이 많은 환경에서도 로봇이 완전한 지도를 작성하고 탐색 계획을 세우는 데 큰 이점을 제공한다.41</li>
</ul>
<h3>6.2  파지 및 조작 (Grasping &amp; Manipulation)</h3>
<p>물체를 잡고 조작하는 작업에서 물체의 정확한 3D 형상을 아는 것은 필수적이다.</p>
<ul>
<li><strong>GIGA (Grasp Detection via Implicit Geometry and Affordance):</strong> GIGA42는 물체의 형상 복원(Reconstruction)과 파지 가능성(Affordance) 예측을 동시에 수행하는 다중 작업 학습 모델이다. GIGA는 Implicit 표현을 사용하여, 카메라에 보이지 않는 물체의 뒷면 형상을 예측하고, 연속적인 공간 상에서 가장 안정적인 파지 점(Grasp Pose)을 찾아낸다. 불연속적인 파지 후보군을 샘플링하고 검증하는 기존 방식과 달리, 파지 품질 함수(Grasp Quality Function)의 극대점을 구배 상승법(Gradient Ascent)으로 찾는 방식을 사용하여 높은 성공률과 정밀도를 달성했다.</li>
</ul>
<h3>6.3  VLA (Vision-Language-Action) 모델과의 통합</h3>
<p>최근 로봇 공학의 거대 트렌드인 VLA 모델에서도 연속적 표현의 중요성이 부각된다.</p>
<ul>
<li><strong>공간 지능(Spatial Intelligence)의 주입:</strong> LLM이나 VLM은 의미론적(Semantic) 이해에는 뛰어나지만, “컵 손잡이를 3cm 오른쪽으로 잡아“와 같은 정밀한 공간적 명령을 수행하는 데는 한계가 있다. 이를 보완하기 위해 암시적 표현(SDF, NeRF 등)을 통해 추출된 정밀한 3D 기하 정보를 VLA 모델의 입력으로 통합하는 <strong>Spatial Forcing</strong> 기법이 제안되었다.43 이를 통해 VLA 모델은 시각적 임베딩과 공간적 기하 정보를 정렬(Alignment)시켜, 언어 명령에 따른 정교하고 물리적으로 타당한 조작을 수행할 수 있게 되었다.</li>
<li><strong>월드 모델(World Model)로서의 INR:</strong> 로봇이 행동의 결과를 예측하기 위해 사용하는 월드 모델 역시 연속적 표현으로 진화하고 있다. 기존의 2D 이미지 예측 모델 대신, 3D 가우시안이나 NeRF 기반의 월드 모델을 사용하면, 로봇은 자신의 행동이 3D 공간을 어떻게 변화시킬지 더 정확하게 시뮬레이션(Imagining)하고 계획할 수 있다.45</li>
</ul>
<h2>7.  표현의 진화가 가져올 미래와 남겨진 과제</h2>
<h3>7.1  데이터 압축과 전송의 새로운 표준</h3>
<p>INR은 데이터를 함수 파라미터(가중치)로 변환하므로, 향후 3D 비디오나 VR/AR 콘텐츠, 로봇의 지도 데이터 등을 전송할 때 거대한 포인트 클라우드 파일 대신 가벼운 신경망 모델 파일만 전송하는 시대가 올 것이다.3 이는 통신 대역폭이 제한된 수중 로봇이나 우주 탐사 로봇, 혹은 군집 로봇(Swarm Robotics) 간의 정보 공유 효율성을 혁신적으로 높일 수 있다.</p>
<h3>7.2  생성형 모델과의 결합</h3>
<p>연속적 표현은 잠재 공간(Latent Space)에서의 보간(Interpolation)이 자유롭다. 이는 로봇이 한 번도 본 적 없는 물체나 환경을 상상(Generative modeling)하고, 그에 대한 대응책을 시뮬레이션해볼 수 있는 기반이 된다. 예를 들어, 소량의 데이터로 학습된 로봇이 생성 모델을 통해 다양한 가상의 장애물 시나리오를 스스로 생성하고 훈련함으로써, 현실 세계의 예측 불가능한 상황에 적응하는 능력을 키울 수 있다.46</p>
<h3>7.3  해결해야 할 과제</h3>
<ul>
<li><strong>실시간 학습 속도:</strong> 암시적 표현은 추론은 빠를 수 있으나, 새로운 환경을 처음 학습(Training/Fitting)하는 데 여전히 시간이 소요된다. Instant NGP나 3DGS와 같은 기술이 이를 초 단위로 단축시키고 있으나, 동적으로 변화하는 환경에 밀리초(ms) 단위로 적응해야 하는 고속 로봇 제어에는 여전히 도전적인 과제이다.</li>
<li><strong>망각(Catastrophic Forgetting):</strong> 신경망 기반 맵은 새로운 영역을 학습할 때 이전 영역의 정보를 잊어버리는 경향이 있다. 이를 해결하기 위해 리플레이 버퍼(Replay Buffer)를 사용하거나, 공간을 분할하여 여러 개의 작은 신경망에 저장하는 로컬 맵핑(Local Mapping) 기법들이 연구되고 있다.40</li>
<li><strong>불확실성(Uncertainty)의 표현:</strong> 함수 <span class="math math-inline">f(\mathbf{x})</span>가 출력하는 값의 신뢰도를 어떻게 추정할 것인가는 안전한 로봇 제어를 위해 필수적이다. 앙상블 기법이나 베이지안 뉴럴 네트워크를 INR에 적용하여 불확실성을 정량화하려는 시도들이 진행 중이다.</li>
</ul>
<h2>8.  결론: 연속성이 여는 자율성의 새로운 지평</h2>
<p>이산에서 연속으로의 전환은 단순한 기술적 유행이 아니다. 이는 로봇이 세상을 ’조각난 정보의 파편’으로 보던 관점에서, ’연속적이고 미분 가능한 물리적 필드’로 이해하는 관점으로의 근본적인 **인식론적 변화(Epistemological Shift)**를 의미한다.</p>
<p>과거에는 지도 작성(Mapping), 계획(Planning), 제어(Control), 인식(Perception)이 서로 다른 데이터 구조를 사용하는 분리된 모듈로 존재했다. 지도는 복셀로, 계획은 그래프로, 제어는 수식으로 이루어져 있어 이들 간의 통합은 항상 번역의 오류와 정보의 손실을 수반했다. 하지만 암시적 신경 표현(INR)과 미분 가능한 렌더링 기술의 등장은 이 모든 과정을 **‘미분 가능한 손실 함수(Differentiable Loss Function)’**라는 공용어를 통해 하나의 거대한 최적화 루프 안에서 통합하고 있다.</p>
<p>이러한 연속적 표현의 진화는 궁극적으로 로봇이 더욱 복잡하고 비정형적인 현실 세계에서, 인간처럼 유연하고 적응력 있게 행동할 수 있는 **‘일반화된 신체 지능(Generalizable Embodied Intelligence)’**을 구현하는 핵심 토대가 될 것이다. 우리는 지금 픽셀의 시대를 지나, 함수의 시대로 나아가고 있다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>Residual-Based Implicit Neural Representation for Synthetic Aperture Radar Images - MDPI, https://www.mdpi.com/2072-4292/16/23/4471</li>
<li>vsitzmann/awesome-implicit-representations: A curated list of resources on implicit neural representations. - GitHub, https://github.com/vsitzmann/awesome-implicit-representations</li>
<li>Where Do We Stand with Implicit Neural Representations? A Technical and Performance Survey - arXiv, https://arxiv.org/html/2411.03688v1</li>
<li>Differentiable Robot Rendering, https://drrobot.cs.columbia.edu/</li>
<li>Differentiable Robot Rendering - arXiv, https://arxiv.org/html/2410.13851v1</li>
<li>NeRFs in Robotics: A Survey - arXiv, https://arxiv.org/html/2405.01333v2</li>
<li>Beyond the pixel plane: sensing and learning in 3D - The Gradient, https://thegradient.pub/beyond-the-pixel-plane-sensing-and-learning-in-3d/</li>
<li>3D Representation Methods: A Survey - arXiv, https://arxiv.org/html/2410.06475v1</li>
<li>15 Common Challenges in 3D Point Cloud Segmentation and How BasicAI Tackles Them, https://www.basic.ai/blog-post/15-common-challenges-in-3d-point-cloud-segmentation</li>
<li>NTopo: Mesh-free Topology Optimization using Implicit Neural Representations - Computational Robotics Lab, https://crl.ethz.ch/papers/NTopoNeurIPS2021.pdf</li>
<li>learning neural implicit functions as object representations for robotic manipulation - arXiv, https://arxiv.org/pdf/2112.04812</li>
<li>[2305.15376] DeepCollide: Scalable Data-Driven High DoF Configuration Space Modeling using Implicit Neural Representations - arXiv, https://arxiv.org/abs/2305.15376</li>
<li>Fitness Landscape Analysis of Discrete Constrained Optimization, https://scholarworks.wmich.edu/cgi/viewcontent.cgi?article=5275&amp;context=masters_theses</li>
<li>Simultaneous Optimization of Discrete and Continuous Parameters Defining a Robot Morphology and Controller - PubMed, https://pubmed.ncbi.nlm.nih.gov/37224357/</li>
<li>Why is Discrete Optimization “Difficult’? - Operations Research Stack Exchange, https://or.stackexchange.com/questions/7756/why-is-discrete-optimization-difficult</li>
<li>Coordinate-Based Neural Networks - Emergent Mind, https://www.emergentmind.com/topics/coordinate-based-neural-networks</li>
<li>Coordinate-Aware Modulation for Neural Fields - arXiv, https://arxiv.org/html/2311.14993</li>
<li>Dynamical Implicit Neural Representations - arXiv, https://arxiv.org/html/2511.21787v1</li>
<li>An Introduction to Neural Implicit Representations with Use Cases | by Nathalie Hager, https://medium.com/@nathaliemariehager/an-introduction-to-neural-implicit-representations-with-use-cases-ad331ca12907</li>
<li>Deep Learning on Implicit Neural Representations of Shapes - OpenReview, https://openreview.net/forum?id=OoOIW-3uadi</li>
<li>Fourier Feature Positional Encoding - Emergent Mind, https://www.emergentmind.com/topics/positional-encoding-using-fourier-features</li>
<li>INR-Bench: A Unified Benchmark for Implicit Neural Representations in Multi-Domain Regression and Reconstruction - arXiv, https://arxiv.org/html/2510.10188v1</li>
<li>Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains - arXiv, https://arxiv.org/pdf/2006.10739</li>
<li>CoordNet: Data Generation and Visualization Generation for Time-Varying Volumes via a Coordinate-Based Neural Network, https://academicweb.nd.edu/~cwang11/papers/tvcg23-coordnet.pdf</li>
<li>On the accuracy of implicit neural representations for cardiovascular anatomies and hemodynamic fields - arXiv, https://arxiv.org/html/2510.20970v1</li>
<li>Robustifying Generalizable Implicit Shape Networks with a Tunable Non-Parametric Model - NIPS papers, https://papers.nips.cc/paper_files/paper/2023/file/525c95ffca1f57a10e3527d3584f3cf1-Paper-Conference.pdf</li>
<li>Continuous Implicit SDF Based Any-Shape Robot Trajectory Optimization - IEEE Xplore, https://ieeexplore.ieee.org/document/10342104</li>
<li>Configuration Space Distance Fields for Manipulation Planning - Robotics, https://www.roboticsproceedings.org/rss20/p131.pdf</li>
<li>Neural Implicit Representations for Multi-View Surface Reconstruction: A Survey - IEEE Xplore, https://ieeexplore.ieee.org/iel8/2945/4359476/11051009.pdf</li>
<li>A Brief Review on Differentiable Rendering: Recent Advances and Challenges - MDPI, https://www.mdpi.com/2079-9292/13/17/3546</li>
<li>Prof. Robot: Differentiable Robot Rendering Without Static and Self-Collisions - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2025/papers/Ruan_Prof._Robot_Differentiable_Robot_Rendering_Without_Static_and_Self-Collisions_CVPR_2025_paper.pdf</li>
<li>Prof. Robot: Differentiable Robot Rendering Without Static and Self-Collisions - arXiv, https://arxiv.org/html/2503.11269v1</li>
<li>A Review of Differentiable Simulators - IEEE Xplore, https://ieeexplore.ieee.org/iel8/6287639/10380310/10589638.pdf</li>
<li>gradSim: Differentiable simulation for system identification and visuomotor control, https://gradsim.github.io/</li>
<li>DiffTactile: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation - arXiv, https://arxiv.org/html/2403.08716v1</li>
<li>[2403.08716] DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation - arXiv, https://arxiv.org/abs/2403.08716</li>
<li>Leveraging Structure for Efficient and Dexterous Contact-Rich Manipulation - DSpace@MIT, https://dspace.mit.edu/bitstream/handle/1721.1/158946/suh-hjsuh-phd-eecs-2025-thesis.pdf?sequence=1&amp;isAllowed=y</li>
<li>ContactNets: Learning Discontinuous Contact Dynamics with Smooth, Implicit Representations - University of Pennsylvania, https://dair.seas.upenn.edu/assets/pdf/Pfrommer2020.pdf</li>
<li>RobotSDF: Implicit Morphology Modeling for the Robotic Arm - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC11359239/</li>
<li>NICE-SLAM: Neural Implicit Scalable Encoding for SLAM - Songyou Peng, https://pengsongyou.github.io/media/nice-slam/NICE-SLAM.pdf</li>
<li>Structerf-SLAM: Neural Implicit Representation SLAM for Structural Environments - University of Huddersfield Research Portal, https://pure.hud.ac.uk/ws/files/81258831/Structerf_SLAM_Neural_Implicit_Representation_SLAM_for_Structural_Environments.pdf</li>
<li>Synergies Between Affordance and Geometry: 6-DoF Grasp Detection via Implicit Representations - Robotics, https://www.roboticsproceedings.org/rss17/p024.pdf</li>
<li>Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model | OpenReview, https://openreview.net/forum?id=euMVC1DO4k</li>
<li>Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model, https://arxiv.org/html/2510.12276v1</li>
<li>A Step Toward World Models: A Survey on Robotic Manipulation - arXiv, https://arxiv.org/html/2511.02097v1</li>
<li>Semantic World Models - WEIRD Lab, https://weirdlabuw.github.io/swm/static/documents/swm.pdf</li>
<li>FLARE: Robot Learning with Implicit World Modeling - arXiv, https://arxiv.org/html/2505.15659v1</li>
<li>Neural Implicit Representation-based Global Consistent Low-Latency SLAM System - arXiv, https://arxiv.org/html/2311.09525v2</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>