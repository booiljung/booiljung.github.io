<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:5.1.3 암시적 표현(Implicit) vs 명시적 표현(Explicit)의 스펙트럼</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>5.1.3 암시적 표현(Implicit) vs 명시적 표현(Explicit)의 스펙트럼</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 5. 뉴럴 3D 표현과 렌더링 (Neural 3D Representations)</a> / <a href="index.html">5.1 표현의 진화: 이산(Discrete)에서 연속(Continuous)으로</a> / <span>5.1.3 암시적 표현(Implicit) vs 명시적 표현(Explicit)의 스펙트럼</span></nav>
                </div>
            </header>
            <article>
                <h1>5.1.3 암시적 표현(Implicit) vs 명시적 표현(Explicit)의 스펙트럼</h1>
<h2>1.  서론: 이분법적 대립에서 연속적 스펙트럼으로의 전환</h2>
<p>로봇 공학(Robotics)과 3차원 컴퓨터 비전(3D Computer Vision) 분야에서 “공간을 어떻게 표현할 것인가?“라는 질문은 시스템의 설계 철학을 결정짓는 가장 근본적인 문제 중 하나이다. 지난 수십 년간 이 질문에 대한 대답은 크게 두 가지 상반된 패러다임, 즉 **명시적 표현(Explicit Representation)**과 <strong>암시적 표현(Implicit Representation)</strong> 사이의 선택으로 귀결되어 왔다.1 포인트 클라우드(Point Cloud), 메쉬(Mesh), 복셀(Voxel)로 대표되는 명시적 표현은 기하학적 데이터를 이산적(Discrete)이고 직접적인 좌표값의 집합으로 저장하여 직관적인 시각화와 물리 엔진과의 호환성을 제공해 왔다. 반면, 최근 딥러닝의 비약적인 발전과 함께 부상한 암시적 표현은 공간을 신경망(Neural Network)이나 수학적 함수(Function)의 파라미터로 인코딩하여 연속적(Continuous)이고 해상도에 구애받지 않는(Resolution-independent) 새로운 가능성을 열었다.3</p>
<p>그러나 2020년대 중반에 이르러, 이러한 이분법적 분류는 더 이상 유효하지 않게 되었다. Instant-NGP 5, Gaussian Splatting 6, ZeroGrasp 7와 같은 최신 연구들은 두 표현 방식의 장점을 결합한 하이브리드(Hybrid) 아키텍처를 채택하며, 명시성과 암시성 사이의 경계를 허무는 ’스펙트럼(Spectrum)’을 형성하고 있다. 로봇은 이제 단순히 환경을 지도화(Mapping)하는 것을 넘어, 투명한 물체를 파지하거나(Grasping) 8, 미지의 환경에서 충돌 없는 경로를 생성(Path Planning)하기 위해 9 더욱 고도화된 의미론적(Semantic)이고 물리적 추론이 가능한 공간 표현을 요구한다.</p>
<p>본 장에서는 3차원 공간 표현의 수학적 정의와 로봇 공학적 특성을 심층 분석한다. 특히 SDF(Signed Distance Functions)와 NeRF(Neural Radiance Fields)로 대표되는 암시적 표현의 수학적 원리, 그리고 이를 보완하는 해시 인코딩(Hash Encoding) 및 하이브리드 구조의 최신 연구 동향을 포괄적으로 다룬다. 이를 통해 로봇 지각 시스템이 나아가야 할 방향성을 명시적-암시적 스펙트럼의 관점에서 재정립하고자 한다.</p>
<h2>2.  명시적 표현(Explicit Representations): 이산적 확실성의 공학적 토대</h2>
<p>명시적 표현은 3차원 객체의 표면이나 부피를 구성하는 기하학적 요소들을 직접 정의하고 저장하는 방식이다. 이는 “데이터가 곧 형상“인 구조로, 로봇 공학의 전통적인 파이프라인에서 가장 널리 사용되어 왔다.</p>
<h3>2.1  기하학적 정의 및 데이터 구조</h3>
<p>명시적 표현의 가장 큰 특징은 공간상에 점유된 영역을 좌표값들의 집합으로 직접 기술한다는 점이다.</p>
<ul>
<li><strong>폴리곤 메쉬(Polygon Mesh):</strong> 메쉬 <span class="math math-inline">\mathcal{M}</span>은 정점(Vertex)의 집합 <span class="math math-inline">\mathcal{V} = {v_1, \dots, v_N}</span>과 이들을 연결하는 면(Face)의 집합 <span class="math math-inline">\mathcal{F}</span>로 구성된 그래프 구조이다.2 각 정점 <span class="math math-inline">v_i \in \mathbb{R}^3</span>는 정확한 <span class="math math-inline">(x, y, z)</span> 좌표를 가지며, 로봇의 시각화 도구(Rviz 등)나 시뮬레이션 환경(Gazebo, MuJoCo)에서 래스터라이제이션(Rasterization)을 통해 실시간으로 렌더링된다.10</li>
<li><strong>포인트 클라우드(Point Cloud):</strong> LiDAR나 RGB-D 카메라로부터 획득되는 원시 데이터 형태로, 연결성 정보 없이 점들의 집합 <span class="math math-inline">\mathcal{P} = {p_1, \dots, p_N}</span>으로 표현된다. 이는 센서 데이터를 즉각적으로 처리하기에 용이하지만, 표면의 연속성이나 위상(Topology) 정보가 부재하여 물리적 상호작용을 모델링하는 데 한계가 있다.12</li>
<li><strong>복셀(Voxel):</strong> 3차원 공간을 균일한 격자(Grid)로 나누고, 각 격자의 점유 여부(Occupancy)를 저장한다. 이는 공간을 이산화하여 탐색 알고리즘을 적용하기 쉽지만, 해상도가 증가함에 따라 메모리 사용량이 <span class="math math-inline">O(N^3)</span>으로 급격히 증가하는 ‘차원의 저주(Curse of Dimensionality)’ 문제를 안고 있다.4</li>
</ul>
<h3>2.2  로봇 공학적 장점: 속도와 호환성</h3>
<p>명시적 표현이 여전히 로봇 공학의 주류를 차지하는 이유는 실시간성과 호환성 때문이다.</p>
<ol>
<li><strong>실시간 충돌 감지:</strong> 로봇 팔의 경로 계획(Path Planning)에서 충돌 감지는 수 밀리초(ms) 내에 이루어져야 한다. 메쉬 기반의 볼록 껍질(Convex Hull)을 사용한 GJK(Gilbert-Johnson-Keerthi) 알고리즘이나 SAT(Separating Axis Theorem)는 매우 빠른 충돌 검사를 보장한다.9</li>
<li><strong>직관적인 편집 및 제어:</strong> 명시적 모델은 특정 정점이나 면을 직접 조작하여 형상을 변형(Deformation)시키기 용이하다. 이는 템플릿 기반의 파지 계획이나 리깅(Rigging)된 캐릭터 애니메이션 등에서 필수적인 특성이다.1</li>
</ol>
<h3>2.3  한계점: 이산화의 제약</h3>
<p>그러나 명시적 표현은 복잡한 위상 변화(Topological Change)나 불확실한 데이터 처리에 취약하다.</p>
<ul>
<li><strong>위상 고정성:</strong> 메쉬 기반 표현은 정점 간의 연결성(Connectivity)이 고정되어 있어, 물체가 쪼개지거나 합쳐지는 위상 변화를 표현하기 위해서는 복잡하고 비용이 많이 드는 리메싱(Remeshing) 과정이 필요하다.16</li>
<li><strong>미분 불가능성:</strong> 대부분의 명시적 렌더링 및 물리 엔진은 미분 불가능(Non-differentiable)하여, 경사 하강법(Gradient Descent) 기반의 딥러닝 최적화 파이프라인에 직접 통합하기 어렵다. 이는 로봇이 시각적 피드백을 통해 자신의 동작을 학습하는 ‘End-to-End’ 학습을 저해하는 요인이 된다.17</li>
</ul>
<hr />
<h2>3.  암시적 표현(Implicit Representations): 연속 함수 공간으로의 확장</h2>
<p>암시적 표현은 3차원 형상을 기하학적 요소의 집합이 아닌, 공간 전체에 정의된 연속 함수 <span class="math math-inline">f: \mathbb{R}^3 \rightarrow \mathbb{R}^n</span>의 등위면(Level Set)으로 정의한다. 이는 “데이터가 형상을 설명하는 함수“가 되는 구조이다.</p>
<h3>3.1  부호 있는 거리 함수 (Signed Distance Functions, SDF)</h3>
<p>SDF는 로봇 공학, 특히 충돌 회피와 경로 계획에서 가장 강력한 수학적 도구로 자리 잡았다. 임의의 공간 좌표 <span class="math math-inline">\mathbf{x} \in \mathbb{R}^3</span>에 대해 함수 <span class="math math-inline">f(\mathbf{x}) = s</span>는 해당 지점에서 가장 가까운 표면까지의 최단 거리를 반환한다.4</p>
<ul>
<li>
<p>수학적 정의:<br />
<span class="math math-display">
f(\mathbf{x}) = \begin{cases} -d(\mathbf{x}, S) &amp; \text{if } \mathbf{x} \in \text{interior} \\ d(\mathbf{x}, S) &amp; \text{if } \mathbf{x} \in \text{exterior} \\ 0 &amp; \text{if } \mathbf{x} \in S \end{cases}
</span><br />
여기서 <span class="math math-inline">S</span>는 물체의 표면이며, <span class="math math-inline">d(\cdot, \cdot)</span>는 유클리드 거리이다. 표면은 <span class="math math-inline">f(\mathbf{x})=0</span>인 제로 레벨 셋(Zero Level Set)으로 암시적으로 정의된다.</p>
</li>
<li>
<p>아이코날 방정식 (Eikonal Equation): 유효한 SDF는 거의 모든 곳에서 기울기의 크기가 1이어야 한다는 아이코날 조건을 만족해야 한다 19:<br />
<span class="math math-display">
||\nabla f(\mathbf{x})||_2 = 1
</span><br />
이 특성은 로봇 공학에서 매우 중요하다. SDF의 기울기 <span class="math math-inline">\nabla f(\mathbf{x})</span>는 표면 법선(Surface Normal) 벡터와 일치하며, 장애물에서 멀어지는 방향을 미분 가능하게 제공한다. 이는 로봇이 장애물과의 충돌 비용(Collision Cost)을 최적화 함수의 일부로 포함시켜, 부드럽고 안전한 경로를 생성할 수 있게 한다.21</p>
</li>
</ul>
<h3>3.2  뉴럴 레디언스 필드 (Neural Radiance Fields, NeRF)</h3>
<p>2020년 등장한 NeRF는 기하학적 형상뿐만 아니라 장면의 시각적 외형(Radiance)까지 암시적 함수로 모델링하는 혁신을 가져왔다. NeRF는 5차원 입력(공간 좌표 <span class="math math-inline">\mathbf{x}=(x,y,z)</span>, 시점 방향 <span class="math math-inline">\mathbf{d}=(\theta, \phi)</span>)을 받아 부피 밀도(Volume Density) <span class="math math-inline">\sigma</span>와 RGB 색상 <span class="math math-inline">\mathbf{c}</span>를 출력하는 함수 <span class="math math-inline">F_\Theta</span>로 정의된다.23</p>
<h4>3.2.1  볼륨 렌더링 방정식 (Volumetric Rendering Equation)</h4>
<p>NeRF의 핵심은 미분 가능한 볼륨 렌더링(Differentiable Volume Rendering)이다. 카메라 원점 <span class="math math-inline">\mathbf{o}</span>에서 방향 <span class="math math-inline">\mathbf{d}</span>로 쏘아진 광선 <span class="math math-inline">\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}</span>를 따라 색상을 적분하여 픽셀 값을 결정한다. 이는 연속적인 적분을 이산적인 샘플링 합으로 근사하여 계산된다:<br />
<span class="math math-display">
C(\mathbf{r}) \approx \sum_{i=1}^{N} T_i (1 - \exp(-\sigma_i \delta_i)) \mathbf{c}_i
</span><br />
여기서 각 항의 의미는 다음과 같다 24:</p>
<ul>
<li>
<p>투과율 (Transmittance, <span class="math math-inline">T_i</span>): 광선이 <span class="math math-inline">t_i</span> 지점까지 장애물에 부딪히지 않고 도달할 확률.<br />
<span class="math math-display">
T_i = \exp\left(-\sum_{j=1}^{i-1} \sigma_j \delta_j\right)
</span></p>
</li>
<li>
<p><strong>불투명도 (Alpha, <span class="math math-inline">\alpha_i</span>):</strong> 구간 <span class="math math-inline">\delta_i</span> 내에서 광선이 입자와 충돌할 확률. <span class="math math-inline">\alpha_i = 1 - \exp(-\sigma_i \delta_i)</span>.</p>
</li>
<li>
<p><strong>밀도 (<span class="math math-inline">\sigma_i</span>)와 색상 (<span class="math math-inline">\mathbf{c}_i</span>):</strong> 신경망(MLP)이 예측한 해당 좌표의 물리적 속성.</p>
</li>
</ul>
<p>이 방정식은 완전 미분 가능(Fully Differentiable)하므로, 2D 이미지와 3D 장면 표현 사이의 오차를 역전파(Backpropagation)하여 신경망을 학습시킬 수 있다. 이는 로봇이 RGB 이미지만으로 3D 환경을 복원하고 이해할 수 있는 길을 열었다.6</p>
<h3>3.3  점유 네트워크 (Occupancy Networks)</h3>
<p>점유 네트워크는 공간의 특정 지점이 객체 내부에 속할 확률 $o: \mathbb{R}^3 \rightarrow $을 출력하는 이진 분류기(Binary Classifier)로 볼 수 있다.27</p>
<ul>
<li>손실 함수: 주로 이진 교차 엔트로피(Binary Cross Entropy, BCE) 손실을 사용하여 학습된다 28:<br />
<span class="math math-display">
\mathcal{L}_{BCE} = - \frac{1}{N} \sum_{i=1}^{N} [y_i \log(p_i) + (1-y_i)\log(1-p_i)]
</span><br />
SDF와 달리 거리 정보를 직접 제공하지는 않지만, 위상 변화에 매우 강건하며(Robust to topology changes), 닫힌 표면(Watertight Surface)을 보장하는 특성이 있어 물체의 대략적인 형상을 빠르게 추론하는 데 유리하다.30</li>
</ul>
<h3>3.4  비교 분석: 명시적 vs 암시적</h3>
<p>다음 표는 두 표현 방식의 로봇 공학적 특성을 비교 요약한 것이다.</p>
<table><thead><tr><th><strong>특성</strong></th><th><strong>명시적 표현 (Explicit)</strong></th><th><strong>암시적 표현 (Implicit)</strong></th></tr></thead><tbody>
<tr><td><strong>기본 단위</strong></td><td>정점(Vertex), 면(Face), 복셀</td><td>신경망 가중치(<span class="math math-inline">\Theta</span>), 함수 파라미터</td></tr>
<tr><td><strong>수학적 모델</strong></td><td><span class="math math-inline">\mathcal{M} = (\mathcal{V}, \mathcal{E}, \mathcal{F})</span></td><td><span class="math math-inline">f(\mathbf{x}) = 0</span> (Level Set)</td></tr>
<tr><td><strong>메모리 효율</strong></td><td>해상도에 따라 급증 (<span class="math math-inline">O(N^2) \sim O(N^3)</span>)</td><td>해상도 독립적, 모델 크기에 의존 (<span class="math math-inline">O(1)</span>)</td></tr>
<tr><td><strong>위상 변화</strong></td><td>어려움 (Remeshing 필요)</td><td>자연스러움 (함수 값 변화로 자동 처리)</td></tr>
<tr><td><strong>미분 가능성</strong></td><td>대체로 불가능 (Non-differentiable)</td><td>미분 가능 (Differentiable)</td></tr>
<tr><td><strong>충돌 감지</strong></td><td>GJK, SAT (빠름)</td><td>SDF 쿼리 (빠름, 기울기 정보 제공)</td></tr>
<tr><td><strong>주요 한계</strong></td><td>이산화 오차, 메모리 제약</td><td>느린 추론 속도 (Dense Query 필요)</td></tr>
</tbody></table>
<hr />
<h2>4.  스펙트럼의 중심: 하이브리드(Hybrid) 표현과 해시 인코딩</h2>
<p>순수 명시적 표현의 메모리 비효율성과 순수 암시적 표현(Pure MLP)의 느린 학습/추론 속도를 극복하기 위해, 두 방식의 장점을 결합한 하이브리드 표현이 등장하였다. 이는 현대 로봇 지각 시스템의 핵심 기술로 자리 잡고 있다.</p>
<h3>4.1  인스턴트 뉴럴 그래픽스 프리미티브 (Instant-NGP)와 해시 인코딩</h3>
<p>NVIDIA의 Instant-NGP 5는 **다해상도 해시 인코딩(Multi-resolution Hash Encoding)**을 도입하여 NeRF의 학습 속도를 수 시간에서 수 초 단위로 단축시켰다. 이는 3차원 공간을 명시적인 데이터 구조(해시 테이블)에 매핑하되, 그 값을 처리하는 방식은 암시적(MLP)으로 유지하는 하이브리드 접근법이다.31</p>
<h4>4.1.1  수학적 원리: 공간 해싱 (Spatial Hashing)</h4>
<p>Instant-NGP는 입력 좌표 <span class="math math-inline">\mathbf{x} \in \mathbb{R}^d</span>를 <span class="math math-inline">L</span>개의 서로 다른 해상도 레벨로 인코딩한다. 각 레벨 <span class="math math-inline">l</span>에서 좌표는 정수 격자 좌표 <span class="math math-inline">\lfloor \mathbf{x}_l \rfloor</span>로 변환된 후, 해시 함수를 통해 <span class="math math-inline">T</span> 크기의 해시 테이블 인덱스로 매핑된다 31:<br />
<span class="math math-display">
h_l(\mathbf{x}) = \left( \bigoplus_{i=1}^{d} x_i \pi_i \right) \mod T
</span></p>
<ul>
<li><strong>XOR 연산 (<span class="math math-inline">\oplus</span>):</strong> 각 차원의 좌표 값에 큰 소수(Large Prime Number) <span class="math math-inline">\pi_i</span>를 곱한 뒤 비트별 XOR 연산을 수행한다. 이는 공간적으로 인접하지 않은 좌표들이 불규칙하게 섞이도록 하여, 구조적인 편향(Bias) 없이 해시 충돌(Collision)을 분산시킨다.</li>
<li><strong>소수 (<span class="math math-inline">\pi_i</span>):</strong> 차원 간의 상관관계를 제거(Decorrelation)하여 해시 분포를 균일하게 만든다.31</li>
<li><strong>학습 가능한 특징 벡터:</strong> 해시 테이블의 각 항목은 학습 가능한 특징 벡터(Feature Vector)를 저장한다. 충돌이 발생하더라도(다른 위치가 같은 해시 인덱스를 가리킴), 다해상도 레벨과 MLP의 비선형성을 통해 충돌로 인한 모호성을 해소하고 유의미한 특징을 학습한다.</li>
</ul>
<p>이 방식은 <span class="math math-inline">O(1)</span>의 상수 시간 쿼리 접근 속도를 유지하면서도, 희소한(Sparse) 표면 정보만을 효율적으로 저장하여 메모리 사용량을 획기적으로 줄인다. 이는 로봇이 제한된 온보드(On-board) 컴퓨팅 자원으로도 고해상도 3D 환경을 실시간으로 모델링할 수 있게 해 준다.32</p>
<h3>4.2  옥트리 기반 생성 모델: ZeroGrasp (CVPR 2025)</h3>
<p>2025년 CVPR에서 발표된 <strong>ZeroGrasp</strong> 7는 명시적 구조인 옥트리(Octree)와 암시적 생성 모델인 CVAE(Conditional Variational Autoencoder)를 결합한 대표적인 하이브리드 사례이다.</p>
<ul>
<li><strong>구조적 특징:</strong> 입력 포인트 클라우드를 희소한 옥트리 구조로 인코딩하여 메모리 효율을 확보한다. 옥트리는 빈 공간(Empty Space)을 건너뛰고 표면 근처의 정보만을 집중적으로 표현한다.</li>
<li><strong>암시적 추론:</strong> 인코딩된 옥트리 특징은 CVAE를 통과하며, 보이지 않는 부분(Occluded region)의 형상과 그에 따른 6D 파지(Grasp) 자세를 확률적으로 생성한다.</li>
<li><strong>의의:</strong> 이는 명시적 표현의 효율성과 암시적 표현의 추론 능력(Inference/Completion capability)을 결합하여, 불완전한 센서 데이터로부터 물리적으로 타당하고 충돌 없는 파지 계획을 수립하는 SOTA(State-of-the-Art) 성능을 달성했다.</li>
</ul>
<h3>4.3  3D Gaussian Splatting: 미분 가능한 명시적 표현</h3>
<p>3D Gaussian Splatting 6은 수만 개의 3D 가우시안 타원체(Ellipsoid)를 사용하여 장면을 표현한다. 각 가우시안은 위치, 공분산(Covariance), 불투명도, 색상 정보를 가진다.</p>
<ul>
<li><strong>스펙트럼 상의 위치:</strong> 데이터 자체는 가우시안이라는 명시적 파라미터(Explicit Parameters) 집합이지만, 렌더링 과정은 미분 가능한 ‘Splatting’ 방식을 사용하여 암시적 최적화의 이점을 취한다.</li>
<li><strong>로봇 공학적 이점:</strong> 포인트 클라우드처럼 직접 편집이나 조작이 가능하면서도, NeRF급의 포토리얼리스틱한 렌더링 품질을 실시간(&gt;30 FPS)으로 제공한다. 이는 SLAM의 랜드마크 맵으로 활용되거나 동적 장애물 회피를 위한 환경 표현으로 빠르게 채택되고 있다.35</li>
</ul>
<hr />
<h2>5.  로봇 어플리케이션을 위한 스펙트럼 최적화</h2>
<p>로봇의 임무(Task)에 따라 최적의 표현 방식은 달라진다. 최신 연구들은 스펙트럼의 다양한 지점을 전략적으로 활용하고 있다.</p>
<h3>5.1  로봇 조작(Manipulation)과 투명 물체 인식</h3>
<p>유리나 투명 플라스틱과 같은 투명 물체(Transparent Objects)는 기존의 깊이 센서(LiDAR, Depth Camera)로는 표면을 감지하기 어렵다. 빛이 투과하거나 굴절되어 깊이 값이 누락되거나 왜곡되기 때문이다.</p>
<ul>
<li><strong>NeRF의 활용:</strong> NeRF는 시점 의존적(View-dependent)인 래디언스를 모델링하므로, 투명 물체의 굴절과 반사 특성을 학습하여 정확한 기하학적 형상을 복원할 수 있다. <em>Dex-NeRF</em>나 2025년의 투명 물체 파지 연구들 8은 NeRF로 복원된 깊이 맵(Depth Map)을 사용하여 기존 파지 계획 알고리즘(Dex-Net 등)을 투명 물체에 성공적으로 적용했다.</li>
<li><strong>형상 사전 정보(Shape Priors)의 통합:</strong> 데이터가 부족한 상황에서는 사전 학습된 형상 정보를 암시적 표현과 결합하여(Shape-prior-driven completion), 부분적으로 관측된 투명 물체의 전체 형상을 유추하고 안정적인 파지 점을 생성한다.37</li>
</ul>
<h3>5.2  내비게이션 및 실시간 SLAM</h3>
<p>이동 로봇의 자율 주행을 위해서는 광범위한 환경을 효율적으로 지도화해야 한다.</p>
<ul>
<li><strong>iSDF (incremental SDF):</strong> 실시간으로 수집되는 깊이 이미지로부터 SDF를 지속적으로 학습(Online Learning)하는 시스템이다. 신경망을 사용하여 노이즈를 제거하고 누락된 부분을 채워 넣으며(In-painting), 경로 계획에 필요한 충돌 비용(Collision Cost)과 기울기(Gradient) 정보를 즉시 제공한다.38</li>
<li><strong>메모리 효율적 SLAM:</strong> NeRF 기반 SLAM 시스템들은 맵 전체를 저장하는 대신, 신경망 가중치나 해시 테이블만을 저장하여 기가바이트(GB) 단위의 포인트 클라우드 맵을 메가바이트(MB) 단위로 압축한다.33 이는 통신 대역폭이 제한된 로봇 간 협업이나 장기 탐사 임무에 필수적이다.</li>
</ul>
<h3>5.3  충돌 감지 및 경로 계획 (Path Planning)</h3>
<ul>
<li><strong>SDF 기반 최적화:</strong> 로봇의 형상과 장애물을 모두 SDF로 표현하면, 충돌 회피 문제는 최적화 문제 내의 제약 조건(Constraint)으로 자연스럽게 공식화된다. SDF의 미분 가능성을 활용하여, 충돌 비용을 최소화하는 방향으로 궤적(Trajectory)을 최적화할 수 있다. 이는 샘플링 기반 방식(RRT 등)보다 훨씬 부드럽고 에너지 효율적인 경로를 생성한다.22</li>
<li><strong>하이브리드 충돌 검사:</strong> 대략적인 충돌은 옥트리나 바운딩 박스(Bounding Box)와 같은 명시적 계층 구조로 빠르게 필터링하고, 근접 거리에서의 정밀한 충돌은 SDF나 뉴럴 필드를 조회하여 계산하는 하이브리드 전략이 널리 사용된다.14</li>
</ul>
<h3>5.4  기초 모델(Foundation Models)과의 융합: 의미론적 내비게이션</h3>
<p>단순한 기하학적 표현을 넘어, 공간에 의미(Semantics)를 부여하는 연구도 진행 중이다. <strong>CLIP-Fields</strong> 41는 3D 공간의 각 지점(암시적 필드 혹은 복셀)에 CLIP(Contrastive Language-Image Pre-training) 임베딩 벡터를 저장한다. 이를 통해 로봇은 “빨간색 의자 뒤로 가라“와 같은 자연어 명령을 이해하고, 해당 의미론적 특징을 가진 공간 좌표로 이동할 수 있다. 여기서 공간 표현은 해시 인코딩을 사용하여 대규모 환경에서도 빠른 쿼리와 메모리 효율성을 보장한다.</p>
<hr />
<h2>6.  결론: 통합된 3D 지능을 향한 제언</h2>
<p>“암시적 표현 vs 명시적 표현“의 논의는 이제 “어떻게 두 표현을 효과적으로 결합하여 로봇의 인지(Perception)와 행동(Action)을 연결할 것인가?“로 진화하였다. 본 장의 분석을 통해 도출된 핵심 결론은 다음과 같다.</p>
<ol>
<li><strong>스펙트럼적 접근의 필수성:</strong> 완전한 명시적 표현이나 완전한 암시적 표현만으로는 현대 로봇의 복잡한 요구사항(실시간성, 정밀성, 의미론적 이해)을 충족시킬 수 없다. Instant-NGP의 해시 인코딩이나 ZeroGrasp의 옥트리-CVAE 결합과 같이, 두 표현의 장점을 취하는 하이브리드 아키텍처가 표준이 되고 있다.</li>
<li><strong>미분 가능성(Differentiability)의 가치:</strong> 암시적 표현이 가져온 가장 큰 혁신은 기하학적 처리를 미분 가능한 연산으로 변환했다는 점이다. 이는 인지, 계획, 제어가 하나의 손실 함수로 연결되는 ‘End-to-End’ 로봇 학습을 가능하게 한다.</li>
<li><strong>행동 가능한(Actionable) 표현:</strong> 3D 표현은 단순히 보기에 좋은(Visual quality) 것을 넘어, 로봇이 물리적으로 상호작용할 수 있는 정보를 제공해야 한다. SDF의 기울기 정보나 NeRF의 투명 물체 복원 능력은 이러한 ’행동 가능한 지각(Actionable Perception)’의 좋은 예시이다.</li>
</ol>
<p>향후 로봇 시스템 설계자는 이러한 스펙트럼 상의 다양한 기술적 옵션들을 깊이 이해하고, 로봇의 하드웨어 제약과 임무 특성에 맞춰 최적의 공간 표현 전략을 수립해야 할 것이다. 2025년 이후의 로봇 공학은 기하학적 정밀함과 의미론적 풍부함을 동시에 갖춘 통합된 3D 지능(Unified 3D Intelligence)을 향해 나아갈 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Comparing explicit and implicit approaches to fitting our model to the…, https://www.researchgate.net/figure/Comparing-explicit-and-implicit-approaches-to-fitting-our-model-to-the-stereo-and_fig3_7305074</li>
<li>Implicit vs Explicit Modelling - Ursula Ackah - WordPress.com, https://uackahmsc.wordpress.com/2020/12/27/implicit-vs-explicit-modelling/</li>
<li>A curated list of resources on implicit neural representations. - GitHub, https://github.com/vsitzmann/awesome-implicit-representations</li>
<li>DeepSDF: Learning Continuous Signed Distance Functions for …, https://openaccess.thecvf.com/content_CVPR_2019/papers/Park_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation_CVPR_2019_paper.pdf</li>
<li>Compact Neural Graphics Primitives with Learned Hash Probing, https://research.nvidia.com/labs/toronto-ai/compact-ngp/assets/compact-ngp.pdf</li>
<li>NeRF: Neural Radiance Field in 3D Vision: A Comprehensive Review, https://arxiv.org/html/2210.00379v7</li>
<li>Zero-Shot Shape Reconstruction Enabled Robotic Grasping, https://openaccess.thecvf.com/content/CVPR2025/papers/Iwase_ZeroGrasp_Zero-Shot_Shape_Reconstruction_Enabled_Robotic_Grasping_CVPR_2025_paper.pdf</li>
<li>Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent …, https://openreview.net/forum?id=zOjU2vZzhCk</li>
<li>Estimated Informed Anytime Search for Sampling-Based Planning …, https://arxiv.org/html/2508.21549v1</li>
<li>Dive into Neural Explicit-Implicit 3D Representations and Their …, https://pengsongyou.github.io/files/talk_sgp.pdf</li>
<li>Implicit Vs Explicit Finite Element Analysis: When to Use Which, https://blog.technia.com/en/simulation/implicit-vs-explicit-finite-element-analysis</li>
<li>What Is The Best 3D Scene Representation for Robotics … - arXiv, https://arxiv.org/html/2512.03422v1</li>
<li>Campar-Blog-Post/BLOGPOST.md at master - GitHub, https://github.com/cangumeli/Campar-Blog-Post/blob/master/BLOGPOST.md</li>
<li>Real-time Collision Detection between General SDFs, http://www.cad.zju.edu.cn/home/jin/papers/Real_Time_CD_between_SDFs.pdf</li>
<li>2.1 Explicit &amp; Implicit! Surfaces - Hao Li, https://www.hao-li.com/cs599-ss2015/slides/Lecture02.1.pdf</li>
<li>Representation and Rendering of Implicit Surfaces - CGL @ ETHZ, https://cgl.ethz.ch/Downloads/Publications/Dissertations/Sig06.pdf</li>
<li>Coupling Explicit and Implicit Surface Representations for …, https://omidpoursaeed.github.io/pdf/HybridNet.pdf</li>
<li>DeepSDF: Learning Continuous Signed Distance Functions for …, https://graphics.iiitd.edu.in/wp-content/uploads/2021/07/DeepSDF-Learning-Continuous-Signed-Distance-Functions-for-Shape-Representation.pdf</li>
<li>Neural Signed Distance Field (NSDF) - Emergent Mind, https://www.emergentmind.com/topics/neural-signed-distance-field-nsdf</li>
<li>Stabilizing the Optimization of Neural Signed Distance Functions …, https://proceedings.neurips.cc/paper_files/paper/2023/file/2d6336c1c2987e9d1d9894edd593478d-Paper-Conference.pdf</li>
<li>Fast Collision Detection for Robot Manipulator Path, https://homepage.iis.sinica.edu.tw/papers/liu/25878-F.pdf</li>
<li>Differentiable Composite Neural Signed Distance Fields for Robot …, https://arxiv.org/html/2502.02664v1</li>
<li>NeRF: A Volume Rendering Perspective | Yue Yu, https://yconquesty.github.io/blog/ml/nerf/nerf_rendering.html</li>
<li>NeRF Revisited: Fixing Quadrature Instability in Volume Rendering, https://proceedings.neurips.cc/paper_files/paper/2023/file/5301c49207917c5c870131959971851c-Paper-Conference.pdf</li>
<li>Instant-NGP Neural Field Overview - Emergent Mind, https://www.emergentmind.com/topics/instant-ngp-neural-field</li>
<li>A Beginner’s 12-Step Visual Guide to Understanding NeRF - Medium, https://medium.com/data-science/a-12-step-visual-guide-to-understanding-nerf-representing-scenes-as-neural-radiance-fields-24a36aef909a</li>
<li>Occupancy Networks: Learning 3D Reconstruction in Function Space, https://openaccess.thecvf.com/content_CVPR_2019/papers/Mescheder_Occupancy_Networks_Learning_3D_Reconstruction_in_Function_Space_CVPR_2019_paper.pdf</li>
<li>A Practical Guide To Binary Cross-Entropy and Log Loss - Coralogix, https://coralogix.com/ai-blog/understanding-binary-cross-entropy-and-log-loss-for-effective-model-monitoring/</li>
<li>Binary Cross Entropy/Log Loss for Binary Classification, https://www.geeksforgeeks.org/deep-learning/binary-cross-entropy-log-loss-for-binary-classification/</li>
<li>Neural Distance Fields Overview - Emergent Mind, https://www.emergentmind.com/topics/neural-distance-fields-ndfs</li>
<li>Instant Neural Graphics Primitives with a Multiresolution Hash …, https://arxiv.org/pdf/2201.05989</li>
<li>Grid-guided Neural Radiance Fields for Large Urban Scenes, https://city-super.github.io/gridnerf/img/main.pdf</li>
<li>Vision-Based Navigation and Perception for Autonomous Robots, https://www.mdpi.com/2673-4117/6/7/153</li>
<li>Zero-Shot Shape Reconstruction Enabled Robotic Grasping - arXiv, https://arxiv.org/html/2504.10857v1</li>
<li>NeRF-Based Transparent Object Grasping Enhanced by Shape Priors, https://www.researchgate.net/publication/395210419_NeRF-Based_Transparent_Object_Grasping_Enhanced_by_Shape_Priors</li>
<li>NeRF-Based Transparent Object Grasping Enhanced by Shape Priors, https://arxiv.org/html/2504.09868v1</li>
<li>NeRF-Based Transparent Object Grasping Enhanced by Shape Priors, https://arxiv.org/abs/2504.09868</li>
<li>iSDF: Real-Time Neural Signed Distance Fields for Robot Perception, https://www.roboticsproceedings.org/rss18/p012.pdf</li>
<li>SLAM Meets NeRF: A Survey of Implicit SLAM Methods - MDPI, https://www.mdpi.com/2032-6653/15/3/85</li>
<li>arXiv:2502.10734v1 [cs.RO] 15 Feb 2025, https://arxiv.org/pdf/2502.10734</li>
<li>CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory, https://arxiv.org/pdf/2210.05663</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>