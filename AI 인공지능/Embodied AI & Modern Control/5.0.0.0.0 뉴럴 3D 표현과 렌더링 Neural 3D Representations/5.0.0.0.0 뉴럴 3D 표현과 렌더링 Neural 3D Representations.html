<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Chapter 5. 뉴럴 3D 표현과 렌더링 (Neural 3D Representations)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Chapter 5. 뉴럴 3D 표현과 렌더링 (Neural 3D Representations)</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">제목: Embodied AI & Modern Control</a> / <a href="index.html">Chapter 5. 뉴럴 3D 표현과 렌더링 (Neural 3D Representations)</a> / <span>Chapter 5. 뉴럴 3D 표현과 렌더링 (Neural 3D Representations)</span></nav>
                </div>
            </header>
            <article>
                <h1>Chapter 5. 뉴럴 3D 표현과 렌더링 (Neural 3D Representations)</h1>
<h2>1.  서론: 3차원 공간 표현의 패러다임 전환</h2>
<p>컴퓨터 비전과 로보틱스 분야에서 3차원 공간을 디지털 환경으로 옮겨오는 문제는 수십 년간 핵심적인 도전 과제였다. 로봇이 물리적 세계와 상호작용하기 위해서는 환경의 기하학적 구조(Geometry)를 인식하고, 자신의 위치를 파악하며, 미지의 영역을 탐사할 수 있는 지도(Map)가 필수적이다. 전통적인 컴퓨터 비전 시스템은 이러한 3차원 공간을 표현하기 위해 점군(Point Cloud), 복셀(Voxel), 메쉬(Mesh)와 같은 이산적(Discrete)이고 명시적(Explicit)인 데이터 구조에 의존해 왔다.</p>
<p>점군은 LiDAR 센서나 RGB-D 카메라로부터 얻어지는 가장 원시적인 형태의 데이터로, 직관적이고 획득이 용이하다는 장점이 있다. 그러나 점들 사이의 연결성(Connectivity) 정보가 부재하여 표면을 정의하기 어렵고, 확대 시 점들 사이에 빈 공간이 나타나는 해상도 문제가 발생한다. 복셀 그리드는 공간을 정해진 크기의 입방체로 나누어 점유 여부를 저장하는 방식으로, 탐색과 경로 계획에는 유리하지만 해상도가 높아질수록 메모리 사용량이 3제곱으로 증가하는 ’차원의 저주(Curse of Dimensionality)’에 직면한다. 메쉬는 정점(Vertex)과 면(Face)으로 표면을 효율적으로 표현하지만, 위상(Topology)이 복잡한 물체를 표현하거나 실시간으로 구조를 변경하는 데에는 기하학적 연산 비용이 많이 소요된다.</p>
<p>2020년, Neural Radiance Fields (NeRF)의 등장은 이러한 고전적인 3D 표현 방식에 근본적인 변화를 가져왔다. NeRF는 3차원 공간을 이산적인 데이터 포인트들의 집합이 아닌, 연속적인 함수(Continuous Function)로 정의했다. 공간상의 좌표 <span class="math math-inline">(x, y, z)</span>와 바라보는 방향 <span class="math math-inline">(\theta, \phi)</span>을 입력하면 해당 지점의 색상 <span class="math math-inline">(r, g, b)</span>과 밀도 <span class="math math-inline">(\sigma)</span>를 출력하는 이 함수는 거대한 3D 행렬이 아닌, 다층 퍼셉트론(Multilayer Perceptron, MLP)이라는 신경망의 가중치 속에 압축되어 저장된다. 이를 <strong>암시적 뉴럴 표현(Implicit Neural Representation)</strong> 또는 **뉴럴 필드(Neural Field)**라고 부른다.1</p>
<p>암시적 표현은 해상도의 제약 없이 무한히 확대해도 부드러운 기하학적 구조를 유지할 수 있으며, 전체 파이프라인이 미분 가능(Differentiable)하다는 강력한 특징을 가진다. 이는 2D 이미지와 3D 모델 간의 오차를 역전파(Backpropagation)하여, 별도의 3D 레이블 데이터 없이도 2D 이미지만으로 3D 공간을 학습할 수 있음을 의미한다. 이러한 특성은 데이터 획득 비용이 높은 로보틱스 분야, 특히 SLAM(Simultaneous Localization and Mapping)과 로봇 조작(Manipulation) 분야에서 폭발적인 관심을 불러일으켰다.3</p>
<p>그러나 NeRF는 렌더링 속도가 매우 느리고, 로봇이 충돌 감지나 파지(Grasping)를 위해 필요로 하는 명확한 표면 정보를 추출하기 어렵다는 단점이 있었다. 이에 대한 대안으로 표면(Surface) 정보를 명시적으로 학습하는 Signed Distance Function (SDF) 기반의 방법론(VolSDF, NeuS 등)이 발전하였으며, 최근에는 실시간 렌더링과 명시적 조작의 이점을 결합한 **3D Gaussian Splatting (3DGS)**이 등장하여 새로운 혁명을 일으키고 있다.5 3DGS는 공간을 수백만 개의 3D 가우시안 타원체로 표현함으로써, NeRF의 미분 가능성과 포인트 클라우드의 고속 렌더링 장점을 동시에 달성했다.</p>
<p>본 장에서는 현대 로보틱스와 컴퓨터 비전의 핵심 기술로 자리 잡은 뉴럴 3D 표현 기술들을 심층적으로 분석한다. NeRF의 수학적 기초와 볼륨 렌더링 방정식의 유도부터 시작하여, 기하학적 정밀성을 높이기 위한 SDF 기반의 방법론, 그리고 최신 트렌드인 3D Gaussian Splatting의 알고리즘 원리를 상세히 다룬다. 나아가 이러한 표현 방식들이 실제 로봇 시스템의 인지(Perception), 위치 추정(Localization), 그리고 조작(Manipulation)에 어떻게 적용되고 있는지, 각 아키텍처의 성능(정확도, 메모리, 속도)을 비교 분석함으로써 독자에게 3D 공간 지능의 최신 지평을 제시하고자 한다.</p>
<h2>2.  Neural Radiance Fields (NeRF): 연속적 볼륨 렌더링의 수학적 기초</h2>
<p>NeRF는 3차원 장면을 ’방사형 필드(Radiance Field)’로 해석한다. 이는 공간의 모든 지점에서 빛이 어떻게 방출되고 흡수되는지를 모델링하는 것으로, 물리학의 복사 전달(Radiative Transfer) 이론에 뿌리를 두고 있다. NeRF의 혁신성은 이러한 물리적 모델을 딥러닝의 함수 근사(Function Approximation) 능력과 결합한 데에 있다.</p>
<h3>2.1  5차원 플레놉틱 함수와 MLP 아키텍처</h3>
<p>NeRF는 3D 장면을 다음과 같은 5차원 벡터 함수로 정의한다 1:<br />
<span class="math math-display">
F_\Theta : (\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma)
</span><br />
여기서 입력 벡터는 다음과 같다:</p>
<ul>
<li><span class="math math-inline">\mathbf{x} = (x, y, z)</span>: 3차원 공간상의 위치 좌표.</li>
<li><span class="math math-inline">\mathbf{d} = (\theta, \phi)</span>: 해당 위치를 바라보는 시점의 방향 벡터 (단위 벡터).</li>
</ul>
<p>출력 벡터는 다음과 같다:</p>
<ul>
<li><span class="math math-inline">\mathbf{c} = (r, g, b)</span>: 해당 위치에서 해당 방향으로 방출되는 색상 (Radiance).</li>
<li><span class="math math-inline">\sigma</span>: 해당 위치의 부피 밀도 (Volume Density). 빛이 해당 지점을 통과할 때 입자에 부딪혀 차단될 확률 밀도.</li>
</ul>
<p>이 함수 <span class="math math-inline">F_\Theta</span>는 다층 퍼셉트론(MLP) 네트워크의 가중치 <span class="math math-inline">\Theta</span>로 파라미터화된다. NeRF의 아키텍처 설계에는 중요한 물리적 제약 조건이 반영되어 있다. <strong>밀도 <span class="math math-inline">\sigma</span>는 오직 위치 <span class="math math-inline">\mathbf{x}</span>에만 의존해야 하며, 시점 <span class="math math-inline">\mathbf{d}</span>에는 독립적이어야 한다.</strong> 이는 물체의 형상(Geometry)이 관찰자의 위치에 따라 변하지 않는다는 물리적 사실을 반영한다. 반면, <strong>색상 <span class="math math-inline">\mathbf{c}</span>는 위치 <span class="math math-inline">\mathbf{x}</span>와 시점 <span class="math math-inline">\mathbf{d}</span> 모두에 의존한다.</strong> 이는 물체의 표면 재질에 따른 반사(Specular reflection)나 뷰 의존적 효과(View-dependent effects)를 표현하기 위함이다.8</p>
<p>이를 구현하기 위해 NeRF 네트워크는 먼저 위치 <span class="math math-inline">\mathbf{x}</span>만을 입력받아 8개의 완전 연결 층(Fully Connected Layers)을 통과시켜 밀도 <span class="math math-inline">\sigma</span>와 중간 특징 벡터(Feature Vector)를 출력한다. 그 후, 이 특징 벡터에 시점 방향 <span class="math math-inline">\mathbf{d}</span>를 연결(Concatenate)하여 추가적인 1~2개의 레이어를 통과시킨 뒤 최종 색상 <span class="math math-inline">\mathbf{c}</span>를 출력한다.</p>
<h3>2.2  볼륨 렌더링 방정식 (Volume Rendering Equation)</h3>
<p>NeRF가 2D 이미지를 생성하는 과정은 전통적인 볼륨 렌더링(Volume Rendering) 기법을 미분 가능한 형태로 구현한 것이다. 카메라의 원점(Optical Center) <span class="math math-inline">\mathbf{o}</span>에서 이미지 평면의 한 픽셀을 향해 쏘아진 광선(Ray)을 <span class="math math-inline">\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}</span>라고 하자 (<span class="math math-inline">t</span>는 광선을 따른 거리). 이 광선을 따라 들어오는 빛의 총량, 즉 픽셀의 색상 <span class="math math-inline">C(\mathbf{r})</span>는 다음과 같은 적분식으로 표현된다 8:<br />
<span class="math math-display">
C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \cdot \sigma(\mathbf{r}(t)) \cdot \mathbf{c}(\mathbf{r}(t), \mathbf{d}) \, dt
</span><br />
이 식의 각 항은 다음과 같은 물리적 의미를 가진다:</p>
<ol>
<li>
<p><strong><span class="math math-inline">\sigma(\mathbf{r}(t))</span></strong>: 거리 <span class="math math-inline">t</span>에 위치한 입자의 밀도. 클수록 빛이 산란되거나 발광할 확률이 높다.</p>
</li>
<li>
<p><strong><span class="math math-inline">\mathbf{c}(\mathbf{r}(t), \mathbf{d})</span></strong>: 거리 <span class="math math-inline">t</span>에서 광선 방향 <span class="math math-inline">\mathbf{d}</span>로 방출되는 색상(Radiance).</p>
</li>
<li>
<p><span class="math math-inline">T(t)</span> (Transmittance, 투과율): 광선이 카메라 원점(<span class="math math-inline">t_n</span>)에서 출발하여 거리 <span class="math math-inline">t</span>까지 도달하는 동안, 다른 입자에 가로막히지 않고 살아남을 확률이다. 이는 그 사이의 밀도를 적분하여 지수 함수를 취한 것으로 정의된다 9:<br />
<span class="math math-display">
T(t) = \exp \left( - \int_{t_n}^{t} \sigma(\mathbf{r}(s)) \, ds \right)
</span></p>
</li>
</ol>
<p>직관적으로 해석하면, <span class="math math-inline">T(t)</span>는 “앞쪽의 물체들에 의해 가려지지 않을 확률“이며, <span class="math math-inline">\sigma(\mathbf{r}(t))</span>는 “해당 위치에 물체가 존재할 확률“이다. 따라서 <span class="math math-inline">T(t) \cdot \sigma(\mathbf{r}(t))</span>는 “정확히 거리 <span class="math math-inline">t</span>에서 광선이 멈출(terminate) 확률 분포 함수(PDF)“로 볼 수 있다.</p>
<h3>2.3  수치적 적분과 구적법 (Quadrature)</h3>
<p>컴퓨터 상에서 연속적인 적분을 계산하는 것은 불가능하므로, NeRF는 이를 이산적인 합으로 근사한다. 이를 위해 광선을 <span class="math math-inline">N</span>개의 구간(bin)으로 나누고, 각 구간 내에서 무작위로 샘플링된 점 <span class="math math-inline">t_i</span>를 선택한다 (Stratified Sampling). 이산화된 렌더링 방정식은 다음과 같다 7:<br />
<span class="math math-display">
\hat{C}(\mathbf{r}) = \sum_{i=1}^{N} T_i \cdot \alpha_i \cdot \mathbf{c}_i
</span><br />
여기서:</p>
<ul>
<li><span class="math math-inline">\delta_i = t_{i+1} - t_i</span>: 인접한 샘플 사이의 거리.</li>
<li><span class="math math-inline">\alpha_i = 1 - \exp(-\sigma_i \delta_i)</span>: <span class="math math-inline">i</span>번째 구간의 불투명도(Opacity, 알파 값). 밀도 <span class="math math-inline">\sigma_i</span>가 높거나 구간 길이 <span class="math math-inline">\delta_i</span>가 길수록 불투명해진다.</li>
<li><span class="math math-inline">T_i = \exp \left( - \sum_{j=1}^{i-1} \sigma_j \delta_j \right) = \prod_{j=1}^{i-1} (1 - \alpha_j)</span>: <span class="math math-inline">i</span>번째 샘플까지의 누적 투과율.</li>
</ul>
<p>이 식은 전통적인 알파 블렌딩(Alpha Blending) 또는 ‘Over’ 연산자와 동일한 형태를 띤다. 중요한 점은 이 모든 연산 과정이 <span class="math math-inline">\sigma_i</span>와 <span class="math math-inline">\mathbf{c}_i</span>에 대해 미분 가능하다는 것이다. 따라서 렌더링된 픽셀 색상 <span class="math math-inline">\hat{C}(\mathbf{r})</span>와 실제 관측된 이미지의 픽셀 색상 <span class="math math-inline">C_{gt}(\mathbf{r})</span> 사이의 차이(L2 Loss)를 계산하고, 이를 역전파(Backpropagation)하여 MLP의 가중치를 업데이트할 수 있다.</p>
<h3>2.4  위치 인코딩 (Positional Encoding)과 고주파 디테일</h3>
<p>초기 실험에서 연구진들은 MLP가 색상과 기하학적 구조의 급격한 변화(고주파 성분)를 제대로 학습하지 못하고 흐릿하게 표현하는 현상을 발견했다. 이는 딥러닝 네트워크가 저주파 함수(Low-frequency function)를 우선적으로 학습하려는 ’스펙트럼 편향(Spectral Bias)’을 가지기 때문이다.7</p>
<p>이를 극복하기 위해 NeRF는 입력 좌표 <span class="math math-inline">(x, y, z)</span>를 고차원의 주파수 공간으로 매핑하는 <strong>위치 인코딩(Positional Encoding)</strong> 함수 <span class="math math-inline">\gamma(\cdot)</span>를 도입했다.<br />
<span class="math math-display">
\gamma(p) = \left( \sin(2^0 \pi p), \cos(2^0 \pi p), \dots, \sin(2^{L-1} \pi p), \cos(2^{L-1} \pi p) \right)
</span><br />
이 함수는 각 좌표값을 서로 다른 주파수를 가진 사인 및 코사인 함수의 조합으로 변환한다. 위치 인코딩을 통해 MLP는 공간상의 미세한 위치 차이를 입력 차원에서 크게 구분할 수 있게 되었으며, 그 결과 텍스처의 디테일과 날카로운 경계선을 효과적으로 복원할 수 있게 되었다. 일반적으로 위치 좌표 <span class="math math-inline">\mathbf{x}</span>에는 <span class="math math-inline">L=10</span>, 시점 방향 <span class="math math-inline">\mathbf{d}</span>에는 <span class="math math-inline">L=4</span> 정도의 차수가 사용된다.</p>
<h3>2.5  NeRF의 한계와 로보틱스 적용의 난제</h3>
<p>NeRF는 혁신적인 화질을 보여주었지만, 실제 로보틱스 시스템에 적용하기에는 몇 가지 치명적인 한계를 가지고 있었다.</p>
<ol>
<li><strong>극도로 느린 렌더링:</strong> 800x800 해상도의 이미지를 하나 렌더링하기 위해서는 64만 개의 광선을 쏘아야 하고, 각 광선마다 수백 번(예: 거친 샘플링 64회 + 정밀 샘플링 128회)의 MLP 추론이 필요하다. 이는 수십 기가플롭스(GFLOPS)의 연산을 요구하며, 일반적인 GPU에서도 프레임당 수 초에서 수십 초가 소요된다. 이는 실시간 내비게이션이나 SLAM에는 부적합하다.12</li>
<li><strong>기하학적 모호성:</strong> 볼륨 렌더링은 물체의 표면을 명확하게 정의하지 않고 밀도의 분포로 표현한다. 따라서 로봇이 물체를 잡거나 충돌을 회피해야 할 때, 정확한 표면(Surface) 위치를 특정하기 어렵다. 추출된 메쉬(Mesh)에는 종종 노이즈가 많거나 표면이 울퉁불퉁한 아티팩트가 발생한다.13</li>
<li><strong>정적 장면 가정:</strong> 기본 NeRF는 장면이 고정되어 있다고 가정한다. 움직이는 물체가 포함된 영상으로 학습할 경우, 움직임에 의한 잔상(Ghosting)이 기하학적 구조로 잘못 학습되거나 학습이 수렴하지 않는다.15</li>
</ol>
<h2>3.  암시적 표면 표현: SDF를 통한 기하학적 정밀화</h2>
<p>로보틱스와 물리 시뮬레이션에서는 ’밀도 구름’보다는 명확한 ‘표면’ 정보가 훨씬 중요하다. 로봇 팔이 컵을 잡기 위해서는 컵의 정확한 경계면을 알아야 하고, 자율주행 로봇이 장애물을 피하기 위해서는 벽까지의 정확한 거리가 필요하다. 이러한 요구에 맞춰, 뉴럴 필드의 출력값을 밀도 대신 **부호화 거리 함수(Signed Distance Function, SDF)**로 대체하려는 연구가 활발히 진행되었다.</p>
<h3>3.1  Signed Distance Function (SDF)의 정의와 이점</h3>
<p>SDF는 공간상의 임의의 점 <span class="math math-inline">\mathbf{x}</span>에서 가장 가까운 물체 표면 <span class="math math-inline">\partial \Omega</span>까지의 최단 거리를 반환하는 함수 <span class="math math-inline">f(\mathbf{x})</span>이다.16<br />
<span class="math math-display">
f(\mathbf{x}) = s \cdot \min_{\mathbf{y} \in \partial \Omega} \|\mathbf{x} - \mathbf{y}\|
</span><br />
여기서 부호 <span class="math math-inline">s</span>는 점 <span class="math math-inline">\mathbf{x}</span>가 물체 내부에 있으면 음수(-), 외부에 있으면 양수(+), 표면 위에 있으면 0이 된다. SDF는 다음과 같은 강력한 로보틱스적 이점을 가진다 18:</p>
<ul>
<li><strong>명시적 표면 정의:</strong> <span class="math math-inline">f(\mathbf{x}) = 0</span>인 지점들의 집합(Zero-level set)이 곧 물체의 표면이 되므로, Marching Cubes 알고리즘 등을 통해 매우 깨끗한 메쉬를 추출할 수 있다.</li>
<li><strong>충돌 감지 및 회피:</strong> 로봇의 현재 위치 <span class="math math-inline">\mathbf{x}</span>에서의 SDF 값 <span class="math math-inline">f(\mathbf{x})</span>가 로봇의 안전 거리보다 작으면 충돌 위험이 있음을 즉시 알 수 있다. 또한 SDF의 기울기(Gradient) <span class="math math-inline">\nabla f(\mathbf{x})</span>는 표면의 법선 벡터(Normal vector)이자 표면에서 멀어지는 최단 방향을 나타내므로, 충돌 회피 경로를 생성하는 데 유용하게 사용된다 (Sphere Tracing 등).</li>
</ul>
<h3>3.2  VolSDF: SDF를 이용한 볼륨 렌더링</h3>
<p>SDF는 기하학적 표현에는 우수하지만, 이를 2D 이미지와 비교하여 학습시키기 위해서는 다시 렌더링 가능한 형태, 즉 밀도(Density)나 불투명도(Opacity)로 변환해야 한다. <strong>VolSDF</strong>는 이 변환 과정을 확률적으로 정립한 대표적인 모델이다.6</p>
<p>VolSDF는 물체 내부(<span class="math math-inline">f &lt; 0</span>)는 밀도가 높고 외부(<span class="math math-inline">f &gt; 0</span>)는 밀도가 낮으며, 표면(<span class="math math-inline">f=0</span>) 근처에서 밀도가 급격히 변해야 한다는 직관을 수학적으로 모델링했다. 이를 위해 라플라스 분포(Laplace Distribution)의 누적 분포 함수(CDF), <span class="math math-inline">\Psi_\beta</span>를 사용한다.<br />
<span class="math math-display">
\sigma(\mathbf{x}) = \alpha \Psi_\beta(-f(\mathbf{x})) = \begin{cases} \frac{\alpha}{2} \exp\left(\frac{f(\mathbf{x})}{\beta}\right) &amp; \text{if } f(\mathbf{x}) \le 0 \\ \alpha \left(1 - \frac{1}{2} \exp\left(-\frac{f(\mathbf{x})}{\beta}\right)\right) &amp; \text{if } f(\mathbf{x}) &gt; 0 \end{cases}
</span><br />
여기서 <span class="math math-inline">\alpha</span>는 최대 밀도 값이며, <span class="math math-inline">\beta</span>는 밀도 변화의 완만함을 조절하는 학습 가능한 파라미터이다. 학습이 진행됨에 따라 <span class="math math-inline">\beta</span>가 0에 수렴하면, 밀도 함수는 표면 경계에서 급격하게 변하는 지시 함수(Indicator Function)와 유사해진다. VolSDF는 이러한 변환을 통해 NeRF의 볼륨 렌더링 파이프라인을 그대로 사용하면서도, 학습 결과로 고품질의 SDF 필드를 얻을 수 있게 했다.14 이는 기하학적 귀납 편향(Inductive Bias)을 주입하여 NeRF보다 훨씬 매끄럽고 물리적으로 타당한 표면을 복원한다.</p>
<h3>3.3  NeuS: 편향 없는(Unbiased) 렌더링</h3>
<p>VolSDF와 유사한 시기에 제안된 <strong>NeuS</strong>는 기존 볼륨 렌더링 방식이 SDF 표면 복원에 있어 미세한 편향(Bias)을 가질 수 있음을 지적했다. 즉, 렌더링된 이미지 상에서 불투명도가 최대가 되는 지점이 실제 SDF의 0 레벨 셋과 일치하지 않을 수 있다는 것이다.</p>
<p>NeuS는 이를 해결하기 위해 새로운 가중치 계산 함수를 제안했다.21 NeuS는 로지스틱 분포(Logistic Distribution)를 기반으로 하는 S-density 함수 <span class="math math-inline">\phi_s(x)</span>를 정의하고, 이를 이용해 볼륨 렌더링 방정식의 가중치 <span class="math math-inline">w(t) = T(t)\sigma(t)</span>를 재구성했다. 특히 NeuS는 네트워크가 공간적 분산(Variance)을 학습하도록 하는 ’Variance Network’를 도입했다. 학습 초기에는 분산을 크게 하여 전체적인 형상을 잡고(Coarse), 학습이 진행될수록 분산을 줄여 표면을 날카롭게(Fine) 다듬는 전략을 사용한다.21</p>
<p>결과적으로 NeuS는 텍스처가 없는 영역이나 복잡한 얇은 구조물에서도 VolSDF보다 더 정교한 표면 재구성 성능을 보여주며, 이는 정밀한 3D 스캔이나 로봇의 파지 계획을 위한 고정밀 모델 생성에 적합하다.23</p>
<p><strong>[표 5-2] NeRF, VolSDF, NeuS 비교 요약</strong></p>
<table><thead><tr><th><strong>특성</strong></th><th><strong>NeRF</strong></th><th><strong>VolSDF</strong></th><th><strong>NeuS</strong></th></tr></thead><tbody>
<tr><td><strong>기본 표현</strong></td><td>볼륨 밀도 (<span class="math math-inline">\sigma</span>)</td><td>SDF (<span class="math math-inline">f(\mathbf{x})</span>)</td><td>SDF (<span class="math math-inline">f(\mathbf{x})</span>)</td></tr>
<tr><td><strong>밀도 변환</strong></td><td>MLP 직접 출력 (ReLU)</td><td>라플라스 CDF (<span class="math math-inline">\Psi_\beta</span>)</td><td>로지스틱 밀도 기반 (<span class="math math-inline">\phi_s</span>)</td></tr>
<tr><td><strong>표면 품질</strong></td><td>낮음 (노이즈, 구름 형태)</td><td>높음 (평활성 우수)</td><td>매우 높음 (미세 구조 보존)</td></tr>
<tr><td><strong>렌더링 편향</strong></td><td>존재 가능</td><td>밀도 변환에 따른 제어</td><td>Unbiased (설계적 제거)</td></tr>
<tr><td><strong>주요 응용</strong></td><td>뷰 합성 (View Synthesis)</td><td>기하학 복원, SLAM</td><td>고정밀 3D 스캔, 역렌더링</td></tr>
</tbody></table>
<h2>4.  3D Gaussian Splatting (3DGS): 실시간 렌더링과 명시적 제어의 혁명</h2>
<p>NeRF와 SDF 기반 방법론들은 미분 가능성과 기하학적 품질 면에서 큰 진보를 이루었지만, 여전히 ’속도’라는 벽을 넘지 못했다. 실시간 로보틱스 응용을 위해서는 최소 30 FPS 이상의 렌더링 및 추론 속도가 필요하지만, 수백만 번의 MLP 연산은 이를 가로막는 병목이었다. 2023년 SIGGRAPH에서 발표된 **3D Gaussian Splatting (3DGS)**은 MLP를 걷어내고, 3D 가우시안이라는 명시적 기하 원시(Geometric Primitive)를 사용함으로써 이 문제를 획기적으로 해결했다.5</p>
<h3>4.1  명시적 표현으로의 회귀: 왜 가우시안인가?</h3>
<p>3DGS는 공간을 연속적인 함수가 아닌, 수백만 개의 3D 가우시안 타원체(Ellipsoid)들의 집합으로 표현한다. 이는 포인트 클라우드와 유사하지만, 각 점이 단순한 좌표가 아니라 크기, 방향, 투명도 정보를 가진 ’부피가 있는 점’이라는 차이가 있다. 가우시안은 미분 가능하고, 래스터화(Rasterization)가 용이하며, 공간을 빈틈없이 메우기에 적합한 수학적 성질을 가지고 있다.24</p>
<p>각 3D 가우시안은 다음의 파라미터로 정의된다 26:</p>
<ol>
<li><strong>평균 (Mean, <span class="math math-inline">\mu \in \mathbb{R}^3</span>)</strong>: 가우시안의 중심 위치 <span class="math math-inline">(x, y, z)</span>.</li>
<li><strong>공분산 (Covariance, <span class="math math-inline">\Sigma \in \mathbb{R}^{3 \times 3}</span>)</strong>: 가우시안의 크기와 방향(퍼짐 정도)을 결정하는 행렬.</li>
<li><strong>불투명도 (Opacity, $\alpha \in $)</strong>: 가우시안의 투명도.</li>
<li><strong>색상 (Color)</strong>: 시점 의존적 색상을 표현하기 위한 구면 조화 함수(Spherical Harmonics, SH) 계수. (일반적으로 3차 SH까지 사용하여 총 48개의 파라미터).</li>
</ol>
<h3>4.2  수학적 파라미터화와 공분산 분해</h3>
<p>3D 가우시안의 형상은 공분산 행렬 <span class="math math-inline">\Sigma</span>에 의해 결정된다. 물리적으로 유효한 타원체가 되기 위해서는 <span class="math math-inline">\Sigma</span>가 양의 준정부호(Positive Semi-definite) 행렬이어야 한다. 최적화 과정(Gradient Descent)에서 이 조건을 항상 만족시키기는 어렵기 때문에, 3DGS는 <span class="math math-inline">\Sigma</span>를 직접 학습하는 대신 **스케일링(Scaling, <span class="math math-inline">S</span>)**과 **회전(Rotation, <span class="math math-inline">R</span>)**으로 분해하여 표현한다.27<br />
<span class="math math-display">
\Sigma = R S S^T R^T
</span><br />
여기서 <span class="math math-inline">S</span>는 3차원 스케일 벡터 <span class="math math-inline">s</span>를 대각 성분으로 갖는 대각 행렬이며, <span class="math math-inline">R</span>은 4차원 쿼터니언(Quaternion) <span class="math math-inline">q</span>로부터 변환된 3x3 회전 행렬이다. 학습 시에는 <span class="math math-inline">s</span>와 <span class="math math-inline">q</span>를 최적화하며, 이는 항상 유효한 <span class="math math-inline">\Sigma</span>를 생성함을 보장한다.</p>
<h3>4.3  EWA Splatting과 2D 투영</h3>
<p>3D 공간의 가우시안을 2D 이미지 평면에 그리기(Rendering) 위해서는 투영(Projection) 과정이 필요하다. 3DGS는 Zwicker 등이 제안한 EWA(Elliptical Weighted Average) Splatting 알고리즘을 기반으로, 3D 공분산 <span class="math math-inline">\Sigma</span>를 2D 이미지 평면상의 공분산 <span class="math math-inline">\Sigma&#39;</span>으로 투영하는 근사식을 사용한다.12<br />
<span class="math math-display">
\Sigma&#39; = J W \Sigma W^T J^T
</span><br />
여기서:</p>
<ul>
<li><span class="math math-inline">W</span>: 월드 좌표계에서 카메라 좌표계로의 뷰 변환 행렬 (Viewing Transformation Matrix).</li>
<li><span class="math math-inline">J</span>: 투영 변환의 야코비안(Jacobian) 행렬. 핀홀 카메라 모델의 원근 투영에 대한 1차 미분 근사이다.</li>
</ul>
<p><span class="math math-display">
J = \begin{bmatrix} f_x/z &amp; 0 &amp; -f_x x / z^2 \\ 0 &amp; f_y/z &amp; -f_y y / z^2 \end{bmatrix}
</span></p>
<p>이 식을 통해 3D 타원체는 2D 이미지 상의 타원(Splat)으로 변환되며, 이 과정 또한 완전히 미분 가능하다.</p>
<h3>4.4  타일 기반 래스터라이저와 고속 렌더링</h3>
<p>3DGS의 압도적인 속도(1080p 해상도에서 100 FPS 이상)의 비결은 효율적인 **타일 기반 래스터라이저(Tile-based Rasterizer)**에 있다.25 렌더링 과정은 다음과 같다:</p>
<ol>
<li><strong>화면 분할:</strong> 전체 이미지를 16x16 픽셀 크기의 타일(Tile)들로 나눈다.</li>
<li><strong>컬링(Culling):</strong> 각 가우시안이 어떤 타일들과 겹치는지 계산하고, 뷰 프러스텀(View Frustum) 밖의 가우시안은 제거한다.</li>
<li><strong>정렬(Sorting):</strong> 각 타일별로 겹치는 가우시안들을 카메라로부터의 깊이(Depth) 순서대로 정렬한다. GPU의 Radix Sort 알고리즘을 사용하여 수백만 개의 가우시안을 매우 빠르게 정렬한다.</li>
<li><strong>알파 블렌딩(Alpha Blending):</strong> 각 픽셀에 대해, 정렬된 가우시안들의 색상을 앞에서 뒤로(Front-to-back) 누적하여 최종 색상을 계산한다.</li>
</ol>
<p><span class="math math-display">
C = \sum_{i \in \mathcal{N}} c_i \alpha_i \prod_{j=1}^{i-1} (1 - \alpha_j)
</span></p>
<p>이 식은 NeRF의 볼륨 렌더링 식과 동일한 형태이지만, MLP 추론 없이 순수하게 가우시안 파라미터의 조회와 행렬 연산만으로 수행되므로 계산 부하가 현저히 낮다.12</p>
<h3>4.5  적응형 밀도 제어 (Adaptive Density Control)</h3>
<p>3DGS의 또 다른 핵심 기능은 학습 중에 가우시안의 분포를 동적으로 조절하는 능력이다.12 이는 로보틱스 SLAM에서 지도를 확장하거나 정밀화하는 데 매우 유용하다.</p>
<ul>
<li><strong>복제(Densification):</strong> 기하학적 디테일이 부족한 영역(Under-reconstruction)이나 가우시안 하나가 너무 넓은 영역을 커버하는 경우(Over-reconstruction), 해당 가우시안을 분할(Split)하거나 복제(Clone)하여 개수를 늘린다. 이는 위치 그래디언트(Positional Gradient)의 크기를 기준으로 판단한다.</li>
<li><strong>가지치기(Pruning):</strong> 불투명도 <span class="math math-inline">\alpha</span>가 임계값 이하로 떨어져 거의 투명해진 가우시안이나, 화면 밖으로 벗어난 가우시안, 혹은 크기가 비정상적으로 커진 가우시안은 주기적으로 제거한다. 이를 통해 메모리 낭비를 막고 렌더링 효율을 유지한다.</li>
</ul>
<p>이러한 명시적인 제어 기능은 NeRF와 달리 사용자가 지도를 직접 편집(수정, 삭제, 추가)할 수 있게 해 주며, 동적 객체를 추적하거나 제거하는 데에도 큰 이점을 제공한다.</p>
<h2>5.  뉴럴 SLAM (Neural SLAM) 시스템 아키텍처</h2>
<p>뉴럴 3D 표현 기술은 정적인 이미지 합성을 넘어, 로봇이 실시간으로 자신의 위치를 파악하고 지도를 작성하는 SLAM(Simultaneous Localization and Mapping) 시스템의 핵심 백엔드(Backend)로 진화했다. 뉴럴 SLAM은 기존의 특징점(Feature) 기반 SLAM(예: ORB-SLAM)이나 직접(Direct) SLAM(예: DSO)과 달리, 맵 전체를 뉴럴 네트워크나 가우시안 필드로 표현함으로써 고밀도(Dense)의 지도 작성과 완벽한 텍스처 복원을 가능하게 했다.</p>
<h3>5.1  NeRF 기반 SLAM의 진화: iMAP에서 NICE-SLAM으로</h3>
<p>초기 뉴럴 SLAM인 <strong>iMAP</strong> (Implicit Mapping and Positioning)은 단일 MLP를 사용하여 전체 장면을 표현했다.30</p>
<ul>
<li><strong>작동 원리:</strong> RGB-D 입력에 대해 MLP를 실시간으로 학습시키며, 카메라 포즈와 맵(MLP 가우중치)을 교대로 최적화한다.</li>
<li><strong>한계:</strong> 단일 MLP의 용량 한계로 인해, 새로운 방을 탐색하면 이전 방의 정보를 잊어버리는 ‘망각(Forgetting)’ 문제가 심각했다. 또한 전체 맵을 업데이트해야 하므로 계산 비용이 높았다.</li>
</ul>
<p>이를 해결하기 위해 <strong>NICE-SLAM</strong> (Neural Implicit Scalable Encoding)이 제안되었다.33</p>
<ul>
<li><strong>계층적 특징 그리드(Hierarchical Feature Grid):</strong> 공간을 격자(Grid)로 나누고, 각 격자점(Voxel corner)에 특징 벡터(Feature Vector)를 저장했다. MLP는 전역 좌표 대신 이 지역적 특징 벡터를 입력으로 받아 색상과 밀도를 출력한다.</li>
<li><strong>장점:</strong> 국소적인 업데이트가 가능해져 망각 문제가 해결되었고, 대규모 실내 공간으로의 확장이 가능해졌다.</li>
<li><strong>단점:</strong> 여전히 픽셀 단위의 광선 투사와 MLP 연산이 필요하여, 실시간성 확보에 어려움이 있었다 (약 1~2 FPS 수준의 추적 속도).36</li>
</ul>
<h3>5.2  하이브리드 인코딩: Co-SLAM</h3>
<p><strong>Co-SLAM</strong> (Joint Coordinate and Sparse Parametric Encodings)은 NeRF 기반 SLAM의 속도와 정확도를 동시에 잡은 획기적인 아키텍처이다.37</p>
<ol>
<li><strong>해시 그리드 (Hash Grid):</strong> NVIDIA의 Instant-NGP에서 제안된 다중 해상도 해시 테이블을 사용하여 특징 그리드의 검색 속도와 메모리 효율을 극대화했다. 이는 <span class="math math-inline">O(1)</span>의 접근 속도를 보장한다.</li>
<li><strong>원-블롭 인코딩 (One-blob Encoding):</strong> 좌표 기반 인코딩(Coordinate-based network)의 장점(부드러운 표면 완성, Hole filling)과 그리드 기반(빠른 수렴)의 장점을 결합했다.38 이를 통해 관측되지 않은 영역(Unobserved area)에 대해서도 합리적인 표면을 추론할 수 있게 되었다.</li>
<li><strong>성능:</strong> Co-SLAM은 일반 GPU에서 10~17Hz의 실시간 성능을 달성했으며, Replica와 ScanNet 데이터셋에서 기존 방법론들을 능가하는 복원 품질을 보여주었다.</li>
</ol>
<h3>5.3  3DGS 기반 SLAM: SplaTAM</h3>
<p><strong>SplaTAM</strong> (Splat, Track &amp; Map)은 3D Gaussian Splatting을 SLAM의 지도 표현으로 채택한 최초의 성공적인 시스템 중 하나이다.39</p>
<ul>
<li><strong>추적 (Tracking):</strong> 현재의 가우시안 맵을 렌더링하여 얻은 이미지/깊이 맵과, 현재 카메라 입력 간의 차이를 최소화하는 방향으로 카메라 포즈를 최적화한다. 이때 **실루엣 마스크(Silhouette Mask)**를 사용하여, 현재 맵에 존재하지 않는 미탐사 영역이 포즈 추정에 노이즈로 작용하는 것을 방지한다.41</li>
<li>손실 함수: <span class="math math-inline">L_t = \sum_{\mathbf{p} \in S} (L_1(D_{ren}, D_{gt}) + 0.5 L_1(C_{ren}, C_{gt}))</span></li>
<li>깊이(Depth) 정보가 추적에 결정적인 역할을 하며, RGB만 사용할 경우 추적 실패율이 높다.40</li>
<li><strong>매핑 (Mapping):</strong> 새로운 키프레임이 들어오면 가우시안을 추가(Densification)하고, 기존 가우시안들의 파라미터(위치, 색상, 불투명도)를 최적화한다. 불필요한 가우시안은 Pruning을 통해 제거하여 맵의 효율성을 유지한다.</li>
</ul>
<p>SplaTAM의 가장 큰 강점은 **압도적인 렌더링 속도(400 FPS)**와 <strong>높은 추적 정확도</strong>이다. 가우시안 스플래팅의 명시적 특성 덕분에 그래디언트가 포즈 최적화에 더 직접적으로 기여하여, 텍스처가 부족한 환경에서도 cm 단위의 정확한 추적이 가능하다.40</p>
<h3>5.4  성능 벤치마크 및 심층 비교 분석</h3>
<p>다음은 주요 뉴럴 SLAM 알고리즘들을 Replica 데이터셋(고품질 실내 합성 데이터) 기준으로 비교한 결과이다.</p>
<p><strong>[표 5-3] 뉴럴 SLAM 알고리즘 성능 및 리소스 비교 (Replica 데이터셋 기준)</strong></p>
<table><thead><tr><th><strong>알고리즘</strong></th><th><strong>기반 기술 (Representation)</strong></th><th><strong>추적 오차 (ATE RMSE, cm) ↓</strong></th><th><strong>렌더링 품질 (PSNR, dB) ↑</strong></th><th><strong>메모리 사용량 (VRAM)</strong></th><th><strong>특징 및 비고</strong></th></tr></thead><tbody>
<tr><td><strong>iMAP</strong> 31</td><td>Single MLP</td><td>2.00 - 4.90</td><td>Low (~23 dB)</td><td><strong>Very Low (&lt; 1GB)</strong></td><td>초기 모델, 망각 문제 심각, 작은 방에만 적합</td></tr>
<tr><td><strong>NICE-SLAM</strong> 43</td><td>Hierarchical Grid</td><td>1.06 - 2.51</td><td>~24.42</td><td>Moderate (~12GB)</td><td>확장성 개선되었으나 추적 속도가 느림 (1-2 FPS)</td></tr>
<tr><td><strong>Co-SLAM</strong> 43</td><td>Hash Grid + MLP</td><td>0.78 - 1.06</td><td>~30.24</td><td><strong>Low (~4.2GB)</strong></td><td><strong>메모리/속도 균형 우수</strong>, 미관측 영역 완성도 높음</td></tr>
<tr><td><strong>ESLAM</strong> 43</td><td>Feature Plane</td><td>0.62</td><td>~29.08</td><td>Moderate (~17GB)</td><td>축 정렬 평면 사용, 빠른 수렴, 메모리 사용량 높음</td></tr>
<tr><td><strong>SplaTAM</strong> 44</td><td>3D Gaussians</td><td><strong>0.36 - 0.41</strong></td><td><strong>~32 dB</strong></td><td>High (~18.5GB)</td><td><strong>최고 추적 정확도</strong>, 렌더링 최상, 메모리 소모 큼</td></tr>
<tr><td><strong>Point-SLAM</strong> 44</td><td>Neural Point Cloud</td><td>0.52</td><td>~28 dB</td><td>Moderate (~8.5GB)</td><td>포인트 기반, SplaTAM과 유사하나 렌더링 느림</td></tr>
</tbody></table>
<p><strong>심층 분석 및 통찰:</strong></p>
<ol>
<li><strong>정확도 vs 메모리 트레이드오프:</strong></li>
</ol>
<ul>
<li><strong>SplaTAM</strong>은 0.36cm라는 경이로운 추적 정확도를 보여준다. 이는 명시적 가우시안들이 표면의 디테일을 픽셀 단위로 정확히 정렬할 수 있기 때문이다. 그러나 수백만 개의 가우시안 파라미터를 저장해야 하므로 <strong>메모리 사용량이 18.5GB</strong>에 달해, 고사양 GPU가 필수적이다.36</li>
<li>반면 <strong>Co-SLAM</strong>은 해시 그리드의 효율성 덕분에 <strong>4.2GB</strong>의 낮은 메모리로도 훌륭한 성능(ATE ~1cm)을 낸다. 이는 제한된 자원을 가진 로봇(예: 드론, 사족보행 로봇)에 탑재하기에 SplaTAM보다 훨씬 유리하다.</li>
</ul>
<ol start="2">
<li><strong>렌더링 방식의 차이:</strong></li>
</ol>
<ul>
<li>NeRF 기반(Co-SLAM, NICE-SLAM)은 빈 공간(Empty space)에 대해서도 MLP를 쿼리해야 하므로 렌더링 속도에 근본적인 한계가 있다.</li>
<li>3DGS 기반(SplaTAM)은 래스터화 방식을 사용하므로 렌더링 속도가 매우 빠르고, 이는 로봇 원격 제어(Teleoperation)나 증강현실(AR) 시각화에 큰 이점을 준다.</li>
</ul>
<ol start="3">
<li><strong>맵의 완성도 (Map Completion):</strong></li>
</ol>
<ul>
<li>Co-SLAM은 원-블롭 인코딩을 통해 관측되지 않은 벽면이나 바닥을 매끄럽게 채워주는(In-painting) 능력이 뛰어나다.</li>
<li>SplaTAM은 관측된 곳에만 가우시안을 배치하므로, 보지 않은 곳은 구멍(Hole)으로 남거나, 초기화 시 잘못된 깊이 정보로 인해 아티팩트(Floater)가 발생할 수 있다.40</li>
</ul>
<hr />
<h2>6.  로보틱스 응용: 인지, 조작, 내비게이션</h2>
<p>뉴럴 3D 표현은 단순한 지도 작성을 넘어, 로봇이 환경을 이해하고 행동을 결정하는 ’인지-행동 루프(Perception-Action Loop)’의 핵심 요소로 통합되고 있다.</p>
<h3>6.1  내비게이션과 경로 계획 (Navigation &amp; Path Planning)</h3>
<p>로봇이 A지점에서 B지점으로 이동하려면 충돌 없는 경로를 찾아야 한다.</p>
<ul>
<li><strong>NeRF 기반:</strong> 초기 연구(Adamkiewicz et al. 32)는 NeRF의 밀도 함수 <span class="math math-inline">\sigma(\mathbf{x})</span>를 비용 함수(Cost function)로 활용했다. 밀도가 높은 곳은 장애물이므로 높은 비용을 부여하여 최적화 기반 경로 계획을 수행했다. 그러나 NeRF 쿼리 속도 문제로 실시간 회피는 어려웠다.</li>
<li><strong>SDF 기반:</strong> SDF는 표면까지의 거리를 즉시 반환하므로 경로 계획에 훨씬 적합하다. 로봇 반경 <span class="math math-inline">r</span>보다 SDF 값 <span class="math math-inline">f(\mathbf{x})</span>가 큰 영역만을 통과하도록 제약 조건을 걸 수 있다.18</li>
<li><strong>3DGS 기반:</strong> 최근에는 3D 가우시안들을 장애물 구(Sphere) 집합으로 간주하고, 이를 회피하는 경로를 생성하는 연구가 진행 중이다. 가우시안은 명시적 위치 정보를 가지므로 KD-Tree 등으로 빠르게 충돌 검사를 수행할 수 있다.</li>
</ul>
<h3>6.2  로봇 조작(Manipulation)과 3DGS: GaussianGrasper</h3>
<p>로봇 팔 조작(Manipulation) 분야에서 3DGS는 <strong>실시간성</strong>과 <strong>의미론적 이해(Semantics)</strong> 결합의 가능성을 보여주었다. 대표적인 사례인 <strong>GaussianGrasper</strong> 시스템을 살펴보자.48</p>
<ol>
<li><strong>아키텍처:</strong> RGB-D 카메라로 스캔한 환경을 3DGS로 복원한다. 이때 단순한 색상뿐만 아니라, CLIP이나 DINO와 같은 비전 파운데이션 모델의 **언어 임베딩(Language Embedding)**을 가우시안의 속성으로 함께 학습한다. 이를 위해 ‘Efficient Feature Distillation (EFD)’ 모듈을 사용하여 고차원의 언어 특징을 가우시안에 압축하여 저장한다.49</li>
<li><strong>언어 기반 쿼리:</strong> 사용자가 “하얀색 머그컵을 잡아줘“라고 명령하면, 로봇은 쿼리 텍스트와 가우시안들의 임베딩 간의 유사도를 계산하여 대상 물체(머그컵)에 해당하는 가우시안들만 필터링한다.48</li>
<li><strong>충돌 감지 및 파지 계획 (Collision Checking &amp; Grasping):</strong></li>
</ol>
<ul>
<li>복원된 3D 가우시안 지오메트리를 바탕으로 파지 후보(Grasp Pose Candidates)를 생성한다.</li>
<li><strong>충돌 감지:</strong> 3D 가우시안 자체가 기하학적 원시(Primitive)이므로, 로봇 그리퍼(Gripper) 모델과 가우시안 타원체 간의 교차 검사를 통해 충돌 없는(Collision-free) 파지 자세를 선별한다.</li>
<li><strong>노멀 가이드 파지 (Normal-guided Grasp Module):</strong> 가우시안 표면의 법선(Normal) 정보를 렌더링하여, 물리적으로 불안정한 파지(예: 표면에 너무 기울어진 접근)를 필터링한다. 여기에는 <strong>Force-closure</strong> 이론이 적용되어 파지의 안정성을 수학적으로 보장한다.48</li>
</ul>
<ol start="4">
<li><strong>동적 업데이트:</strong> 물체를 잡고 이동시키면, 해당 물체의 가우시안들만 로봇 팔의 운동학(Kinematics)에 맞춰 위치를 업데이트한다. NeRF였다면 전체 맵을 재학습해야 했겠지만, 3DGS는 가우시안의 좌표 변환만으로 즉각적인 맵 갱신이 가능하다.51</li>
</ol>
<h3>6.3  엣지 컴퓨팅과 하드웨어 제약</h3>
<p>실제 로봇은 배터리와 크기 제약으로 인해 NVIDIA Jetson Orin과 같은 임베디드 GPU를 주로 사용한다.</p>
<ul>
<li><strong>메모리 제약:</strong> SplaTAM과 같은 고성능 모델은 18GB 이상의 VRAM을 요구하므로 엣지 디바이스(보통 8~16GB 공유 메모리)에서 구동하기 어렵다. 이를 위해 가우시안의 개수를 제한하거나, 불필요한 가우시안을 공격적으로 제거(Pruning)하는 경량화 연구가 필수적이다.53</li>
<li><strong>실시간성:</strong> 3DGS의 래스터라이저는 엣지 디바이스에서도 충분히 빠르지만(30 FPS 이상), 학습(매핑) 과정은 여전히 부하가 크다. 따라서 SLAM 시스템을 ’추적(Tracking, 엣지 구동)’과 ’매핑(Mapping, 서버 또는 비동기 구동)’으로 분리하거나, 키프레임 기반으로 맵 업데이트 빈도를 조절하는 전략이 사용된다.55</li>
</ul>
<h2>7.  결론 및 향후 전망</h2>
<p>뉴럴 3D 표현 기술은 지난 5년간 숨 가쁜 속도로 발전하며 로보틱스와 컴퓨터 비전의 지형을 바꾸어 놓았다.</p>
<ol>
<li><strong>NeRF</strong>는 연속적 볼륨 표현을 통해 3D 공간을 미분 가능한 데이터로 변환하는 패러다임을 제시했다.</li>
<li><strong>VolSDF/NeuS</strong>는 이를 기하학적으로 정밀화하여 물리적 상호작용이 가능한 수준의 표면 복원 기술로 발전시켰다.</li>
<li>**3D Gaussian Splatting (3DGS)**은 명시적 표현의 장점을 부활시켜, 실시간 렌더링과 동적 조작이 가능한 실용적인 단계로 기술을 끌어올렸다. 특히 SplaTAM과 같은 시스템은 기존 SLAM을 능가하는 정확도를, GaussianGrasper는 언어 지능과 결합된 로봇 조작의 가능성을 입증했다.</li>
</ol>
<p><strong>향후 전망 및 도전 과제:</strong></p>
<ul>
<li><strong>대규모 환경의 효율적 표현:</strong> 도시 단위의 거대한 환경을 가우시안으로 표현하려면 메모리 효율적인 압축 기술과 스트리밍 기술이 필요하다. Octree 구조나 해시 기반 압축이 3DGS에 도입될 것이다.</li>
<li><strong>완전한 동적 장면 이해:</strong> 현재는 정적 배경과 움직이는 물체를 분리하는 수준이지만, 유체(물, 연기)나 천과 같은 변형체(Deformable object)를 물리 법칙에 맞게 실시간으로 시뮬레이션하고 렌더링하는 ’Physics-informed 3DGS’가 발전할 것이다.56</li>
<li><strong>파운데이션 모델과의 융합:</strong> 3D 표현은 더 이상 기하학적 정보만 담지 않을 것이다. LLM/VLM의 지식이 3D 공간에 촘촘히 매핑되어, 로봇이 “냉장고 안에 있는 유통기한 지난 우유를 버려줘“와 같은 고수준의 추론과 행동을 수행할 수 있는 ’공간 지능(Spatial Intelligence)’의 기반이 될 것이다.51</li>
</ul>
<p>이러한 기술적 진보는 로봇이 인간과 같은 공간적 직관을 가지고 세상을 바라보고 이해하게 되는, 진정한 자율성의 시대로 우리를 이끌 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>What is NeRF? - Neural Radiance Fields Explained - AWS, https://aws.amazon.com/what-is/neural-radiance-fields/</li>
<li>Neural radiance field - Wikipedia, https://en.wikipedia.org/wiki/Neural_radiance_field</li>
<li>Neural Implicit Representations for Multi-View Surface Reconstruction: A Survey - IEEE Xplore, https://ieeexplore.ieee.org/iel8/2945/4359476/11051009.pdf</li>
<li>[2405.01333] NeRFs in Robotics: A Survey - arXiv, https://arxiv.org/abs/2405.01333</li>
<li>3D Gaussian Splatting: A Technical Guide to Real-Time Neural Rendering - KIRI Engine, https://www.kiriengine.app/blog/3d-gaussian-splatting-a-technical-guide-to-real-time-neural-rendering</li>
<li>Neural Density-Distance Fields - European Computer Vision Association, https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920053.pdf</li>
<li>45 Radiance Fields - Foundations of Computer Vision - MIT, https://visionbook.mit.edu/nerf.html</li>
<li>How Neural Radiance Fields (NeRF) and Instant Neural Graphics Primitives work, https://theaisummer.com/nerf/</li>
<li>Volume Rendering Digest (for NeRF), https://www.cs.unc.edu/~ronisen/teaching/fall_2022/pdf_lectures/volume_rendering_nerf.pdf</li>
<li>NeRF Revisited: Fixing Quadrature Instability in Volume Rendering - NeurIPS, https://proceedings.neurips.cc/paper_files/paper/2023/file/5301c49207917c5c870131959971851c-Paper-Conference.pdf</li>
<li>NeRF: A Volume Rendering Perspective | Yue Yu, https://yconquesty.github.io/blog/ml/nerf/nerf_rendering.html</li>
<li>3D Gaussian Splatting - Paper Explained, Training NeRFStudio - Learn OpenCV, https://learnopencv.com/3d-gaussian-splatting/</li>
<li>Volume Rendering of Neural Implicit Surfaces - NeurIPS, https://proceedings.neurips.cc/paper_files/paper/2021/file/25e2a30f44898b9f3e978b1786dcd85c-Paper.pdf</li>
<li>[2106.12052] Volume Rendering of Neural Implicit Surfaces - arXiv, https://arxiv.org/abs/2106.12052</li>
<li>Adapting neural radiance fields (NeRFs) to dynamic scenes - Amazon Science, https://www.amazon.science/blog/adapting-neural-radiance-fields-nerfs-to-dynamic-scenes</li>
<li>Signed Distance Function: A Comprehensive Guide for 2025 - Shadecoder, https://www.shadecoder.com/topics/signed-distance-function-a-comprehensive-guide-for-2025</li>
<li>Signed distance function - Wikipedia, https://en.wikipedia.org/wiki/Signed_distance_function</li>
<li>Differentiable Composite Neural Signed Distance Fields for Robot Navigation in Dynamic Indoor Environments - arXiv, https://arxiv.org/html/2502.02664v1</li>
<li>Configuration Space Distance Fields for Manipulation Planning - Robotics, https://www.roboticsproceedings.org/rss20/p131.pdf</li>
<li>[2106.12052] Volume Rendering of Neural Implicit Surfaces - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/2106.12052</li>
<li>NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction - Lingjie Liu, https://lingjie0206.github.io/papers/NeuS/paper.pdf</li>
<li>NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction - NeurIPS, https://proceedings.neurips.cc/paper_files/paper/2021/file/e41e164f7485ec4a28741a2d0ea41c74-Paper.pdf</li>
<li>Learning Unsigned Distance Functions from Multi-view Images with Volume Rendering Priors - arXiv, https://arxiv.org/html/2407.16396v1</li>
<li>Gaussian Splatting: A Deep Dive into Transforming 3D Data for Real-Time Visualization, https://karthick.ai/blog/2024/Gaussian-Splatting/</li>
<li>Gaussian splatting - Wikipedia, https://en.wikipedia.org/wiki/Gaussian_splatting</li>
<li>A Comprehensive Overview of Gaussian Splatting | by Kate Feingold (Yurkova) - Medium, https://medium.com/data-science/a-comprehensive-overview-of-gaussian-splatting-e7d570081362</li>
<li>A Python Engineer’s Introduction to 3D Gaussian Splatting (Part 2) - Medium, https://medium.com/data-science/a-python-engineers-introduction-to-3d-gaussian-splatting-part-2-7e45b270c1df</li>
<li>gaussian_splatting/MATH.md at main - GitHub, https://github.com/joeyan/gaussian_splatting/blob/main/MATH.md</li>
<li>Rendering in 3D Gaussian Splatting - Scthe’s blog, https://www.sctheblog.com/blog/gaussian-splatting/</li>
<li>How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey - Fabio Tosi, https://fabiotosi92.github.io/files/survey-slam.pdf</li>
<li>MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM - arXiv, https://arxiv.org/html/2509.13536v1</li>
<li>[2110.00168] Vision-Only Robot Navigation in a Neural Radiance World - arXiv, https://arxiv.org/abs/2110.00168</li>
<li>NICE-SLAM - Songyou Peng, https://pengsongyou.github.io/nice-slam</li>
<li>NICE-SLAM: Neural Implicit Scalable Encoding for SLAM - Microsoft Research, https://www.microsoft.com/en-us/research/publication/nice-slam-neural-implicit-scalable-encoding-for-slam/</li>
<li>NICE-SLAM: Neural Implicit Scalable Encoding for SLAM - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_NICE-SLAM_Neural_Implicit_Scalable_Encoding_for_SLAM_CVPR_2022_paper.pdf</li>
<li>VPGS-SLAM: Voxel-based Progressive 3D Gaussian SLAM in Large-Scale Scenes - arXiv, https://arxiv.org/html/2505.18992v1</li>
<li>Co-SLAM: Joint Coordinate and Sparse Parametric Encodings for Neural Real-Time SLAM, https://hengyiwang.github.io/projects/CoSLAM.html</li>
<li>Co-SLAM: Joint Coordinate and Sparse Parametric Encodings for Neural Real-Time SLAM - UCL Discovery, https://discovery.ucl.ac.uk/10179946/1/Wang_Co-SLAM_Joint_Coordinate_and_Sparse_Parametric_Encodings_for_Neural_Real-Time_CVPR_2023_paper.pdf</li>
<li>SplaTAM: Splat, Track &amp; Map 3D Gaussians for Dense RGB-D SLAM, https://spla-tam.github.io/</li>
<li>SplaTAM.pdf, https://spla-tam.github.io/assets/SplaTAM.pdf</li>
<li>SplaTAM: Splat, Track &amp; Map 3D Gaussians for Dense RGB-D SLAM spla-tam . github . io, https://ar5iv.labs.arxiv.org/html/2312.02126</li>
<li>Robust Gaussian Splatting SLAM by Leveraging Loop Closure - arXiv, https://arxiv.org/html/2409.20111v1</li>
<li>RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization - arXiv, https://arxiv.org/html/2601.00705v3</li>
<li>Evaluating Modern Approaches in 3D Scene Reconstruction: NeRF vs Gaussian-Based Methods - arXiv, https://arxiv.org/html/2408.04268v2</li>
<li>LEG-SLAM: Language-Enhanced Gaussian Splatting for Real-Time SLAM - arXiv, https://arxiv.org/html/2506.03073v1</li>
<li>Photo-realistic Dense SLAM with Gaussian Splatting | OpenReview, https://openreview.net/forum?id=sbkKi8snqH</li>
<li>Vision-Only Robot Navigation in a Neural Radiance World - Preston Culbertson, https://pculbertson.github.io/assets/pdf/adamkiewicz2021.pdf</li>
<li>GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping - arXiv, https://arxiv.org/html/2403.09637v1</li>
<li>[2403.09637] GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping - arXiv, https://arxiv.org/abs/2403.09637</li>
<li>GaussianGrasper - Yuhang Zheng, https://mrsecant.github.io/GaussianGrasper/</li>
<li>Object-Aware Gaussian Splatting for Robotic Manipulation - OpenReview, https://openreview.net/forum?id=gdRI43hDgo</li>
<li>GraspSplats: Efficient Manipulation with 3D Feature Splatting - arXiv, https://arxiv.org/html/2409.02084v1</li>
<li>(PDF) MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM - ResearchGate, https://www.researchgate.net/publication/395582903_MemGS_Memory-Efficient_Gaussian_Splatting_for_Real-Time_SLAM</li>
<li>No Redundancy, No Stall: Lightweight Streaming 3D Gaussian Splatting for Real-time Rendering - arXiv, https://arxiv.org/html/2507.21572v1</li>
<li>Globally Consistent RGB-D SLAM with 2D Gaussian Splatting, https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/zhong2025arxiv.pdf</li>
<li>Building Robotic Mental Models with NVIDIA Warp and Gaussian Splatting, https://developer.nvidia.com/blog/building-robotic-mental-models-with-nvidia-warp-and-gaussian-splatting/</li>
<li>SLAG: Scalable Language-Augmented Gaussian Splatting - arXiv, https://arxiv.org/html/2505.08124v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>