<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:5.7.3 3D 표현 상의 시맨틱 라벨링(Semantic Labeling)과 쿼리</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>5.7.3 3D 표현 상의 시맨틱 라벨링(Semantic Labeling)과 쿼리</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 5. 뉴럴 3D 표현과 렌더링 (Neural 3D Representations)</a> / <a href="index.html">5.7 로봇 제어와 행동을 위한 뉴럴 표현</a> / <span>5.7.3 3D 표현 상의 시맨틱 라벨링(Semantic Labeling)과 쿼리</span></nav>
                </div>
            </header>
            <article>
                <h1>5.7.3 3D 표현 상의 시맨틱 라벨링(Semantic Labeling)과 쿼리</h1>
<h2>1.  서론: 전인적 3D 장면 이해를 향한 패러다임의 전환</h2>
<p>현대 컴퓨터 비전과 로봇 공학, 그리고 인공지능(AI)의 융합 영역에서 가장 혁신적인 변화 중 하나는 3D 공간을 단순히 기하학적(Geometric) 구조와 광학적(Photometric) 외관으로 재구성하는 것을 넘어, 그 안에 내재된 의미(Semantics)를 포괄적으로 이해하려는 시도이다. 이를 ’전인적 3D 장면 이해(Holistic 3D Scene Understanding)’라 칭하며, 이는 자율 주행 자동차가 도로 위의 물체를 식별하거나, 가사 로봇이 사용자의 자연어 명령을 해석하여 특정 물체를 조작(Manipulation)하는 것과 같은 고차원적인 상호작용의 기반이 된다.</p>
<p>과거의 전통적인 파이프라인은 이러한 목표를 달성하기 위해 다단계의 분절된 접근 방식을 취했다. 즉, 희소한 카메라 포즈를 추정하는 SfM(Structure-from-Motion), 밀집된 기하학적 표면을 복원하는 MVS(Multi-View Stereo), 그리고 2D 이미지 상에서의 시맨틱 세그멘테이션 결과를 3D로 투영하는 과정이 순차적으로 이루어졌다. 그러나 이러한 단계적 접근(Staged approach)은 초기 단계의 작은 오차가 후속 단계로 전파되면서 최종적인 시맨틱 맵의 품질을 심각하게 저하시키는 고질적인 문제를 안고 있었다. 특히, 시점 간의 일관성(Multi-view Consistency) 부족으로 인해 동일한 객체가 보는 각도에 따라 다른 클래스로 인식되거나, 3D 공간 상에서 라벨이 깜빡이는 ‘플리커링(Flickering)’ 현상이 빈번하게 발생하였다.</p>
<p>최근 신경 방사장(Neural Radiance Fields, NeRF)과 3D 가우시안 스플래팅(3D Gaussian Splatting, 3DGS)으로 대변되는 차세대 3D 표현 기술의 등장은 이러한 한계를 극복할 새로운 가능성을 열어주었다. 이들 기술은 기하학적 정보와 색상 정보, 그리고 고차원의 시맨틱 정보를 하나의 통일된 표현(Unified Representation) 안에 융합하여 최적화할 수 있는 프레임워크를 제공한다. 본 절에서는 2D 파운데이션 모델(Foundation Models)의 방대한 지식을 3D 표현으로 증류(Distillation)하여 시맨틱 라벨링을 수행하는 메커니즘과, 구축된 3D 시맨틱 필드(Semantic Field)를 통해 자연어 기반의 오픈 어휘(Open-vocabulary) 쿼리를 수행하는 원리, 그리고 이를 로봇 공학 및 공간 컴퓨팅에 응용하는 최신 연구 동향을 심층적으로 분석한다.</p>
<h2>2.  3D 시맨틱 필드의 이론적 배경과 수학적 정식화</h2>
<p>3D 시맨틱 라벨링의 핵심은 물리적 공간을 점유하는 3D 좌표 <span class="math math-inline">\mathbf{x} = (x, y, z)</span>에 대해, 해당 위치의 의미론적 속성을 나타내는 고차원 특징 벡터(Feature Vector) <span class="math math-inline">\mathbf{f} \in \mathbb{R}^D</span>를 매핑하는 함수를 학습하는 것이다. 이를 ‘시맨틱 필드(Semantic Field)’ 또는 ’특징 필드(Feature Field)’라 정의한다.</p>
<h3>2.1  시맨틱 필드의 수학적 정의</h3>
<p>일반적인 3D 장면 재구성 모델이 공간 좌표 <span class="math math-inline">\mathbf{x}</span>와 관측 방향 <span class="math math-inline">\mathbf{d}</span>를 입력으로 받아 색상 <span class="math math-inline">\mathbf{c}</span>와 밀도 <span class="math math-inline">\sigma</span> (또는 불투명도 <span class="math math-inline">\alpha</span>)를 출력하는 함수 <span class="math math-inline">F_{scene}</span>으로 정의된다면, 시맨틱 필드는 여기에 시맨틱 임베딩 <span class="math math-inline">\mathbf{s}</span>를 추가적인 출력으로 생성한다.<br />
<span class="math math-display">
F_{semantic}(\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma, \mathbf{s})
</span><br />
여기서 <span class="math math-inline">\mathbf{s}</span>는 단순한 클래스 ID(스칼라 값)가 아니라, CLIP(Contrastive Language-Image Pre-training)이나 DINO와 같은 대규모 비전-언어 모델(Vision-Language Model, VLM)의 잠재 공간(Latent Space)과 정렬된 고차원 벡터이다. 예를 들어, CLIP ViT-B/16 모델을 티처(Teacher)로 사용할 경우 <span class="math math-inline">D=512</span>차원, Sentence-BERT를 사용할 경우 <span class="math math-inline">D=768</span>차원의 벡터가 각 3D 위치에 할당된다.</p>
<h3>2.2  볼륨 렌더링을 통한 특징 학습 (Volume Rendering of Features)</h3>
<p>NeRF 기반의 방법론(예: LERF)에서 시맨틱 필드는 기존의 볼륨 렌더링 방정식을 확장하여 학습된다. 카메라 광선 <span class="math math-inline">\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}</span> 상에서 샘플링된 포인트들의 시맨틱 특징 <span class="math math-inline">\mathbf{s}_i</span>는 밀도 <span class="math math-inline">\sigma_i</span>와 투과율 <span class="math math-inline">T_i</span>에 의해 가중 합산되어 2D 이미지 평면 상의 픽셀 특징 <span class="math math-inline">\hat{\mathbf{S}}(\mathbf{r})</span>로 렌더링된다.<br />
<span class="math math-display">
\hat{\mathbf{S}}(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma(t) \mathbf{s}(t) dt \approx \sum_{i=1}^{N} T_i \alpha_i \mathbf{s}_i
</span><br />
여기서 <span class="math math-inline">T_i = \exp(-\sum_{j=1}^{i-1} \sigma_j \delta_j)</span>는 광선이 <span class="math math-inline">i</span>번째 샘플까지 도달할 확률(투과율)을 의미한다. 이 식은 3D 공간 상의 시맨틱 분포가 2D 뷰에서 관측되는 특징 맵과 일치하도록 유도하는 미분 가능한 연결 고리를 제공한다.</p>
<h3>2.3  손실 함수와 코사인 유사도 (Loss Functions &amp; Cosine Similarity)</h3>
<p>시맨틱 필드의 학습은 2D 티처 모델(예: CLIP)이 생성한 특징 맵 <span class="math math-inline">\mathbf{S}_{teacher}</span>와 렌더링된 특징 맵 <span class="math math-inline">\hat{\mathbf{S}}_{rendered}</span> 간의 차이를 최소화하는 방향으로 진행된다. 이때, 고차원 임베딩 벡터의 특성상 유클리드 거리(L2 Loss)보다는 벡터의 방향성을 중시하는 코사인 유사도(Cosine Similarity) 기반의 손실 함수가 주로 사용된다.<br />
<span class="math math-display">
\mathcal{L}_{semantic} = 1 - \frac{\hat{\mathbf{S}}_{rendered} \cdot \mathbf{S}_{teacher}}{\| \hat{\mathbf{S}}_{rendered} \| \| \mathbf{S}_{teacher} \|}
</span><br />
코사인 유사도는 벡터의 크기(Magnitude)에 영향을 받지 않고 오직 방향의 일치도만을 측정하므로, 조명 변화나 스케일 차이에 강인한 시맨틱 정렬을 가능하게 한다. 또한, 특징 필드의 고주파 노이즈를 억제하고 공간적 연속성을 보장하기 위해 DINO 특징을 이용한 정규화(Regularization) 항이 추가되기도 한다.</p>
<h2>3.  암시적 표현(Implicit Representation) 기반의 접근: NeRF와 LERF</h2>
<p>신경 방사장(NeRF)은 3D 공간을 MLP(Multi-Layer Perceptron)의 가중치 형태로 암시적으로 저장하는 방식이다. 이 구조에 언어적 의미를 주입한 대표적인 연구가 LERF(Language Embedded Radiance Fields)이다.</p>
<h3>3.1  LERF의 아키텍처 및 메커니즘</h3>
<p>LERF는 기하학적 재구성을 위한 기본 NeRF 네트워크와 병렬로, 또는 통합된 형태로 언어 필드(Language Field)를 구축한다. LERF의 핵심 혁신은 다중 스케일(Multi-scale)의 CLIP 특징을 3D 필드에 융합하는 방식에 있다. CLIP 모델은 기본적으로 224x224 크기의 이미지를 입력으로 받아 전체적인 문맥을 이해하도록 설계되었기 때문에, 3D 장면의 국소적인 디테일(예: 책상 위의 작은 컵)을 포착하는 데 한계가 있다.</p>
<p>이를 극복하기 위해 LERF는 학습 이미지로부터 다양한 크기의 이미지 작물(Image Crops)을 생성하여 이미지 피라미드(Image Pyramid)를 구성한다. 렌더링 시에는 광선의 샘플링 위치가 이미지 평면에서 차지하는 영역의 크기(Scale)를 고려하여, 적절한 스케일의 CLIP 임베딩을 보간(Interpolate)하여 학습한다.</p>
<ul>
<li><strong>입력:</strong> 공간 좌표 <span class="math math-inline">\mathbf{x}</span> + 스케일 <span class="math math-inline">s</span>.</li>
<li><strong>출력:</strong> <span class="math math-inline">D</span>차원 CLIP 임베딩 벡터.</li>
<li><strong>학습 전략:</strong> 광선 기반의 볼륨 렌더링을 수행하되, 다양한 스케일의 작물에서 추출된 CLIP 특징을 감독 신호(Ground Truth)로 사용.</li>
</ul>
<h3>3.2  LERF의 한계와 도전 과제</h3>
<p>LERF는 오픈 어휘 3D 검색의 가능성을 증명했으나, NeRF의 태생적 한계인 느린 학습 속도와 렌더링 속도를 그대로 답습한다. 특히, 512차원 이상의 고차원 벡터를 픽셀마다 쿼리하고 렌더링하는 연산 비용은 실시간 상호작용을 어렵게 만든다. 또한, MLP의 암시적 표현은 고주파수의 텍스처나 복잡한 형상을 표현하는 데 있어 “스펙트럼 편향(Spectral Bias)” 문제를 겪을 수 있어, 시맨틱 경계가 흐릿해지는 경향이 있다.</p>
<h2>4.  명시적 표현(Explicit Representation)의 혁신: 3D Gaussian Splatting</h2>
<p>3D Gaussian Splatting(3DGS)의 등장은 3D 시맨틱 라벨링의 효율성을 획기적으로 개선하는 계기가 되었다. 3DGS는 3D 공간을 수백만 개의 3D 가우시안 타원체(Ellipsoid)들의 집합으로 표현하며, 각 가우시안은 명시적인 파라미터(위치, 공분산, 불투명도, 색상)를 갖는다. 이를 시맨틱 필드로 확장하는 연구들은 가우시안의 속성에 ’시맨틱 특징 벡터’를 추가하는 방식을 채택한다.</p>
<h3>4.1  Feature 3DGS: 임의 차원 특징의 고속 래스터화</h3>
<p>Feature 3DGS는 3DGS의 렌더링 파이프라인을 수정하여, RGB 색상뿐만 아니라 <span class="math math-inline">N</span>차원의 시맨틱 특징을 실시간으로 래스터화(Rasterization)할 수 있도록 설계되었다.</p>
<ul>
<li>
<p><strong>아키텍처 확장:</strong> 각 3D 가우시안 <span class="math math-inline">G_i</span>는 위치 <span class="math math-inline">\mu_i</span>, 공분산 <span class="math math-inline">\Sigma_i</span>, 불투명도 <span class="math math-inline">\alpha_i</span> 외에 시맨틱 특징 벡터 <span class="math math-inline">\mathbf{f}_i \in \mathbb{R}^N</span>을 추가로 저장한다.</p>
</li>
<li>
<p><strong>병렬 N차원 래스터라이저 (Parallel N-dimensional Rasterizer):</strong> 기존 3DGS의 타일 기반 래스터라이저는 RGB 3채널 처리에 최적화되어 있었다. Feature 3DGS는 이를 CUDA 커널 레벨에서 수정하여, 채널 수에 제한이 없는 특징 벡터를 병렬로 블렌딩할 수 있게 하였다.</p>
</li>
<li>
<p><strong>증류 파이프라인:</strong> SAM(Segment Anything Model)이나 LSeg와 같은 2D 모델에서 추출된 특징 맵을 티처로 사용하여, 렌더링된 특징 필드와 티처 특징 맵 간의 L1 손실 및 코사인 유사도 손실을 최소화한다.<br />
<span class="math math-display">
L = L_{rgb} + \gamma L_{feature}
</span><br />
여기서 <span class="math math-inline">\gamma</span>는 특징 손실의 가중치를 조절하는 하이퍼파라미터이다.</p>
</li>
</ul>
<h3>4.2  LangSplat: 메모리 병목 현상의 해결과 오토인코더</h3>
<p>3DGS에 고차원 언어 특징(예: CLIP 512차원)을 직접 저장하는 것은 심각한 메모리 문제를 야기한다. 예를 들어, 100만 개의 가우시안에 512차원 float16 벡터를 저장하면 약 1GB의 추가 메모리가 필요하며, 이는 장면의 복잡도가 증가할수록 선형적으로 증가하여 VRAM 용량을 초과하게 된다.</p>
<p>LangSplat은 이 문제를 해결하기 위해 ’장면 별 오토인코더(Scene-wise Language Autoencoder)’를 도입했다.</p>
<ul>
<li><strong>인코더 (Encoder):</strong> CLIP의 512차원 특징을 매우 낮은 차원(예: 3차원)의 잠재 코드(Latent Code)로 압축한다. 인코더 구조는 일반적으로 <span class="math math-inline">256 \to 128 \to 64 \to 32 \to 3</span> 차원으로 줄어드는 MLP 레이어로 구성된다.</li>
<li><strong>3D 최적화:</strong> 3D 가우시안은 압축된 3차원 특징 벡터만을 속성으로 저장하고 학습한다. 이는 메모리 사용량을 획기적으로(약 100배 이상) 줄여준다.</li>
<li><strong>디코더 (Decoder):</strong> 렌더링 단계에서는 3차원 특징 맵을 래스터화한 후, 학습된 디코더(<span class="math math-inline">3 \to 16 \to \dots \to 512</span>)를 통해 다시 512차원 CLIP 임베딩으로 복원하여 쿼리에 사용한다.</li>
<li><strong>성능:</strong> 이러한 접근 방식은 LERF 대비 199배 빠른 렌더링 속도(1440x1080 해상도)를 달성하면서도, 명확한 객체 경계를 유지하는 고품질의 언어 필드를 생성한다.</li>
</ul>
<h3>4.3  LangSplatV2와 희소 코딩(Sparse Coding)</h3>
<p>최근 제안된 LangSplatV2는 오토인코더 방식의 단점인 압축 손실(Compression Loss)로 인한 시맨틱 모호성을 해결하기 위해 ‘희소 코딩’ 이론을 접목했다.</p>
<ul>
<li>
<p><strong>글로벌 코드북 (Global Dictionary):</strong> 장면 내에 존재하는 의미론적 원자(Semantic Atoms)들을 대표하는 기저 벡터들의 집합(Dictionary) <span class="math math-inline">S \in \mathbb{R}^{L \times D}</span>를 학습한다.</p>
</li>
<li>
<p><strong>희소 계수 필드 (Sparse Coefficient Field):</strong> 각 가우시안은 전체 특징 벡터 대신, 코드북의 기저 벡터들을 조합할 수 있는 희소한 가중치(Coefficient) <span class="math math-inline">w_i</span>만을 저장한다.</p>
</li>
<li>
<p><strong>수학적 모델:</strong> <span class="math math-inline">i</span>번째 가우시안의 언어 특징 <span class="math math-inline">\mathbf{f}_i</span>는 다음과 같이 표현된다.<br />
<span class="math math-display">
\mathbf{f}_i = w_i S = \sum_{l=1}^{L} w_{i,l} s_l
</span></p>
</li>
<li>
<p><strong>렌더링 혁신:</strong> 렌더링 시에는 희소 계수 <span class="math math-inline">w_i</span>만을 스플래팅한 뒤, 마지막에 코드북 <span class="math math-inline">S</span>와 행렬 곱을 수행하여 고차원 특징을 복원한다. 이는 디코더 신경망을 통과하는 연산 비용조차 제거하여, 400 FPS 이상의 초고속 렌더링을 가능하게 한다.</p>
</li>
</ul>
<h2>5.  오픈 어휘 쿼리(Open-Vocabulary Query) 메커니즘</h2>
<p>구축된 3D 시맨틱 필드는 사용자와 3D 환경 간의 직관적인 인터페이스 역할을 한다. 가장 핵심적인 기능은 ’오픈 어휘 쿼리’로, 사전에 정의되지 않은 임의의 텍스트 설명을 통해 3D 객체를 검색하고 상호작용하는 능력이다.</p>
<h3>5.1  텍스트-3D 정렬 및 검색 (Text-to-3D Alignment &amp; Retrieval)</h3>
<p>사용자가 “오래된 나무 의자(an old wooden chair)“라는 텍스트 쿼리를 입력하면, 시스템은 다음과 같은 과정을 거쳐 해당 객체를 식별한다.</p>
<ol>
<li><strong>텍스트 임베딩:</strong> 입력된 텍스트는 CLIP 텍스트 인코더를 통해 <span class="math math-inline">D</span>차원 벡터 <span class="math math-inline">\mathbf{t}</span>로 변환된다.</li>
<li><strong>관련성 맵(Relevancy Map) 계산:</strong> 3D 뷰포트 상의 각 픽셀(또는 3D 포인트)에서 렌더링된 시맨틱 특징 <span class="math math-inline">\mathbf{s}*{pixel}</span>과 텍스트 임베딩 <span class="math math-inline">\mathbf{t}</span> 간의 코사인 유사도 점수를 계산한다. <span class="math math-display">S*{score} = \frac{\mathbf{s}*{pixel} \cdot \mathbf{t}}{|\mathbf{s}*{pixel}| |\mathbf{t}|}</span></li>
<li><strong>시각화 및 활성화:</strong> 계산된 유사도 점수를 기반으로 히트맵(Heatmap)을 생성하여, 점수가 임계값(Threshold)을 넘는 영역을 하이라이트한다. LangSplat은 여기에 SAM의 마스크 정보를 결합하여, 단순한 점군(Point Cloud)이 아닌 명확한 경계를 가진 객체 단위의 활성화를 수행한다.</li>
</ol>
<h3>2. 계층적 쿼리와 3D 장면 그래프 (Hierarchical Query &amp; 3D Scene Graphs)</h3>
<p>단순한 객체 검색을 넘어, “주방에 있는 컵“이나 “소파 옆의 테이블“과 같이 공간적 관계를 포함한 복합적인 쿼리를 처리하기 위해서는 장면의 구조적 정보가 필요하다. 이를 위해 3D 시맨틱 필드를 ’3D 장면 그래프(3D Scene Graph)’와 통합하는 연구가 진행되고 있다.</p>
<ul>
<li><strong>HOV-SG (Hierarchical Open-Vocabulary Scene Graphs):</strong> 3D 공간을 층(Floor) - 방(Room) - 객체(Object)의 계층 구조로 추상화하고, 각 노드에 오픈 어휘 특징을 임베딩한다. 이를 통해 로봇은 “2층 회의실로 이동해“와 같은 고수준의 명령을 해석하고 수행할 수 있다.</li>
<li><strong>DovSG (Dynamic Open-Vocabulary Scene Graphs):</strong> 환경의 변화를 실시간으로 반영하기 위해 동적 장면 그래프를 제안한다. 이는 인식(Perception), 메모리(Memory), 태스크 계획(Task Planning), 네비게이션(Navigation), 조작(Manipulation)의 5개 모듈로 구성되며, 변화하는 환경 속에서 로봇의 장기적인 작업 수행을 지원한다.</li>
</ul>
<h2>5.7.3.6 로봇 공학 및 4D 응용 (Robotics &amp; 4D Applications)</h2>
<p>3D 시맨틱 라벨링 기술은 로봇이 물리적 세계와 상호작용하는 방식(Embodied AI)을 근본적으로 변화시키고 있다.</p>
<h3>1. 시맨틱 네비게이션 (Semantic Navigation)</h3>
<p>기존의 로봇 네비게이션이 “좌표 (x, y)로 이동“하는 기하학적 이동이었다면, 시맨틱 네비게이션은 “냉장고 앞으로 이동“과 같은 의미적 이동을 가능하게 한다.</p>
<ul>
<li><strong>CLIP-Fields:</strong> 로봇의 RGB-D 센서 데이터와 오도메트리 정보를 결합하여, 공간 좌표를 CLIP/Sentence-BERT 임베딩으로 매핑하는 암시적 모델을 학습한다. 이는 별도의 인간 라벨링 없이 웹 스케일 모델(Web-scale models)의 약한 감독(Weak Supervision)만으로 학습되며, 로봇에게 “내 파란색 물병 어디 있어?“와 같은 질문에 답할 수 있는 공간 메모리를 제공한다.</li>
</ul>
<h3>2. 오픈 어휘 모바일 조작 (Open-Vocabulary Mobile Manipulation, OVMM)</h3>
<p>로봇 팔을 이용한 조작 작업에서 3D 시맨틱 필드는 객체의 정확한 6자유도(6-DoF) 포즈와 파지(Grasping) 지점을 식별하는 데 사용된다.</p>
<ul>
<li><strong>일관성의 중요성:</strong> 2D 이미지 기반 분할은 시점이 변하거나 객체가 가려질 때(Occlusion) 실패하기 쉽다. 반면, 3DGS나 NeRF로 구축된 3D 시맨틱 필드는 다중 뷰 일관성을 보장하므로, 로봇이 카메라를 움직여도 타겟 객체의 ID와 분할 마스크가 안정적으로 유지된다. 이는 조작 성공률을 획기적으로 높이는 요인이 된다.</li>
<li><strong>조작 파이프라인:</strong> 사용자 명령(LLM 해석) <span class="math math-inline">\to</span> 3D 시맨틱 맵 쿼리(위치 식별) <span class="math math-inline">\to</span> 네비게이션 <span class="math math-inline">\to</span> 근접 시 3D 인스턴스 분할(SAM/Feature 3DGS) <span class="math math-inline">\to</span> 파지 계획(Grasp Planning) <span class="math math-inline">\to</span> 실행.</li>
</ul>
<h3>3. 4D 시맨틱 필드와 동적 환경 (4D Semantic Fields)</h3>
<p>현실 세계는 정지해 있지 않다. 4D LangSplat과 같은 최신 연구는 시간(<span class="math math-inline">t</span>) 차원을 추가하여 동적 장면(Dynamic Scenes)에서의 시맨틱 이해를 시도한다. 이는 비디오 캡션이나 객체 추적(Tracking) 모델과 결합되어, “움직이는 사람“이나 “떨어지는 물체“와 같은 시간적 속성을 포함한 쿼리를 가능하게 한다. 이는 로봇이 인간과 협업하거나 변화하는 환경에 적응하는 데 필수적인 기술이다.</p>
<h3>표 5.7.3.1: 3D 시맨틱 라벨링 및 쿼리 모델의 비교 분석</h3>
<p>아래 표는 주요 3D 시맨틱 모델들의 기술적 특성과 성능을 요약한 것이다.</p>
<table><thead><tr><th><strong>모델 (Model)</strong></th><th><strong>기반 표현 (Representation)</strong></th><th><strong>특징 증류 소스 (Teacher)</strong></th><th><strong>차원 처리 및 최적화 전략</strong></th><th><strong>렌더링 속도 (FPS)</strong></th><th><strong>주요 특징 및 응용 분야</strong></th></tr></thead><tbody>
<tr><td><strong>LERF</strong></td><td>NeRF (Implicit)</td><td>CLIP (Multi-scale)</td><td>볼륨 렌더링, 이미지 피라미드 보간</td><td>&lt; 1 FPS (Slow)</td><td>최초의 오픈 어휘 3D 필드, 정밀한 텍스트 쿼리</td></tr>
<tr><td><strong>CLIP-Fields</strong></td><td>Implicit Neural / Voxel</td><td>CLIP, Detic, Sentence-BERT</td><td>MLP 매핑 함수 학습 (<span class="math math-inline">XYZ \to Emb</span>)</td><td>N/A (Offline Query)</td><td>로봇 시맨틱 네비게이션, 약한 지도 학습</td></tr>
<tr><td><strong>Feature 3DGS</strong></td><td>3DGS (Explicit)</td><td>SAM, CLIP-LSeg</td><td>병렬 N차원 래스터라이저</td><td>&gt; 30 FPS (Real-time)</td><td>실시간 편집 및 분할, 임의 차원 특징 지원</td></tr>
<tr><td><strong>LangSplat</strong></td><td>3DGS (Explicit)</td><td>CLIP (Mask-based)</td><td>장면별 오토인코더 (<span class="math math-inline">512 \to 3</span> 압축)</td><td>&gt; 100 FPS</td><td>메모리 효율성 극대화, 명확한 객체 경계</td></tr>
<tr><td><strong>LangSplatV2</strong></td><td>3DGS (Explicit)</td><td>CLIP</td><td>희소 코딩 &amp; 글로벌 코드북</td><td>&gt; 400 FPS</td><td>디코더 제거로 초고속 렌더링, 압축 손실 제거</td></tr>
<tr><td><strong>HOV-SG</strong></td><td>3D Scene Graph</td><td>VLMs (Region-based)</td><td>계층적 그래프 구조 (층-방-객체)</td><td>N/A (Graph Query)</td><td>대규모 건물 단위 네비게이션, 구조적 쿼리</td></tr>
</tbody></table>
<h2>5.7.3.7 결론 및 향후 연구 방향</h2>
<p>3D 표현 상의 시맨틱 라벨링과 쿼리 기술은 기하학적 재구성을 넘어 의미론적 공간 지능(Spatial Intelligence)을 실현하는 핵심 기술로 자리 잡았다. NeRF 기반의 LERF가 개념적 토대를 마련했다면, 3DGS 기반의 Feature 3DGS와 LangSplat 시리즈는 실시간성(Real-time)과 효율성(Efficiency) 문제를 해결하며 로봇 공학 및 AR/VR 분야로의 실질적 적용을 가속화하고 있다.</p>
<p>향후 연구는 다음과 같은 방향으로 전개될 것으로 전망된다.</p>
<ol>
<li><strong>생성형 모델과의 융합:</strong> 단순히 장면을 이해하는 것을 넘어, 언어 명령에 따라 3D 객체를 생성하거나 수정(Editing)하는 생성형 3D AI와의 통합이 심화될 것이다.</li>
<li><strong>온디바이스(On-device) 실행:</strong> 로봇이나 모바일 기기의 제한된 자원에서 구동될 수 있도록, 모델의 경량화와 연산 효율성을 극대화하는 연구(예: 희소 코딩, 양자화)가 지속될 것이다.</li>
<li><strong>지속적 학습(Lifelong Learning):</strong> 변화하는 환경에 적응하여 시맨틱 맵을 실시간으로 업데이트하고, 새로운 객체나 개념을 지속적으로 학습하는 시스템(DovSG 등)의 발전이 요구된다.</li>
</ol>
<p>이러한 기술적 진보는 기계가 인간의 언어로 세상을 이해하고 소통하며, 물리적 세계에서 유용한 작업을 수행하는 진정한 의미의 ‘체화된 인공지능(Embodied AI)’ 시대를 앞당길 것이다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields - arXiv, 1월 23, 2026에 액세스, https://arxiv.org/html/2506.09565v2</li>
<li>Semantic 3D segmentation of 3D Gaussian Splats - TU Delft Repository, 1월 23, 2026에 액세스, https://repository.tudelft.nl/file/File_05053a66-b8c8-43b7-b901-22bb25153658</li>
<li>1 Introduction - arXiv, 1월 23, 2026에 액세스, https://arxiv.org/html/2601.03200v1</li>
<li>3D Gaussian Splatting in Robotics: A Survey - arXiv, 1월 23, 2026에 액세스, https://arxiv.org/html/2410.12262v2</li>
<li>arxiv.org, 1월 23, 2026에 액세스, https://arxiv.org/html/2312.03203v2</li>
<li>CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory, 1월 23, 2026에 액세스, https://mahis.life/clip-fields/</li>
<li>LERF: Language Embedded Radiance Fields, 1월 23, 2026에 액세스, https://www.lerf.io/</li>
<li>CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory - Robotics, 1월 23, 2026에 액세스, https://www.roboticsproceedings.org/rss19/p074.pdf</li>
<li>Volume Rendering - Scratchapixel, 1월 23, 2026에 액세스, https://www.scratchapixel.com/lessons/3d-basic-rendering/volume-rendering-for-developers/volume-rendering-summary-equations.html</li>
<li>LERF - nerfstudio, 1월 23, 2026에 액세스, https://docs.nerf.studio/nerfology/methods/lerf.html</li>
<li>Computer Vision Paper ~ LERF: Language Embedded Radiance Field | by Christian Lin, 1월 23, 2026에 액세스, https://medium.com/@crlc112358/computer-vision-paper-lerf-language-embedded-radiance-field-9578e81b57f9</li>
<li>Cosine Similarity Loss: Theory and Applications - Emergent Mind, 1월 23, 2026에 액세스, https://www.emergentmind.com/topics/cosine-similarity-loss</li>
<li>Cosine similarity - Wikipedia, 1월 23, 2026에 액세스, https://en.wikipedia.org/wiki/Cosine_similarity</li>
<li>LERF: Language Embedded Radiance Fields - CVF Open Access, 1월 23, 2026에 액세스, https://openaccess.thecvf.com/content/ICCV2023/papers/Kerr_LERF_Language_Embedded_Radiance_Fields_ICCV_2023_paper.pdf</li>
<li>NeRF Revisited: Fixing Quadrature Instability in Volume Rendering - arXiv, 1월 23, 2026에 액세스, https://arxiv.org/html/2310.20685v2</li>
<li>Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields, 1월 23, 2026에 액세스, https://feature-3dgs.github.io/</li>
<li>Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields - CVF Open Access, 1월 23, 2026에 액세스, https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Feature_3DGS_Supercharging_3D_Gaussian_Splatting_to_Enable_Distilled_Feature_CVPR_2024_paper.pdf</li>
<li>LangSplat: 3D Language Gaussian Splatting - CVF Open Access, 1월 23, 2026에 액세스, https://openaccess.thecvf.com/content/CVPR2024/papers/Qin_LangSplat_3D_Language_Gaussian_Splatting_CVPR_2024_paper.pdf</li>
<li>LangSplatV2: High-dimensional 3D Language Gaussian … - arXiv, 1월 23, 2026에 액세스, https://arxiv.org/html/2507.07136</li>
<li>minghanqin/LangSplat: Official implementation of the paper … - GitHub, 1월 23, 2026에 액세스, https://github.com/minghanqin/LangSplat</li>
<li>[2312.16084] LangSplat: 3D Language Gaussian Splatting - arXiv, 1월 23, 2026에 액세스, https://arxiv.org/abs/2312.16084</li>
<li>LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+ FPS, 1월 23, 2026에 액세스, https://langsplat-v2.github.io/</li>
<li>Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation, 1월 23, 2026에 액세스, https://www.roboticsproceedings.org/rss20/p077.pdf</li>
<li>[2410.11989] Dynamic Open-Vocabulary 3D Scene Graphs for Long-term Language-Guided Mobile Manipulation - arXiv, 1월 23, 2026에 액세스, https://arxiv.org/abs/2410.11989</li>
<li>Dynamic Open-Vocabulary 3D Scene Graphs for Long-term Language-Guided Mobile Manipulation - arXiv, 1월 23, 2026에 액세스, https://arxiv.org/html/2410.11989v6</li>
<li>Comparison of 2D, 2.5D, and 3D segmentation networks for mandibular canals in CBCT images: a study on public and external datasets - PMC - NIH, 1월 23, 2026에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC12235980/</li>
<li>Language-Conditioned Open-Vocabulary Mobile Manipulation with Pretrained Models - IJCAI, 1월 23, 2026에 액세스, https://www.ijcai.org/proceedings/2025/0976.pdf</li>
<li>Open-vocabulary Mobile Manipulation in Unseen Dynamic Environments with 3D Semantic Maps | PromptLayer, 1월 23, 2026에 액세스, https://www.promptlayer.com/research-papers/open-vocabulary-mobile-manipulation-in-unseen-dynamic-environments-with-3d-semantic-maps</li>
<li>4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large Language Models, 1월 23, 2026에 액세스, https://4d-langsplat.github.io/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>