<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:5.7.1 비주얼 서보잉(Visual Servoing)을 위한 미분 가능한 렌더링 활용</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>5.7.1 비주얼 서보잉(Visual Servoing)을 위한 미분 가능한 렌더링 활용</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 5. 뉴럴 3D 표현과 렌더링 (Neural 3D Representations)</a> / <a href="index.html">5.7 로봇 제어와 행동을 위한 뉴럴 표현</a> / <span>5.7.1 비주얼 서보잉(Visual Servoing)을 위한 미분 가능한 렌더링 활용</span></nav>
                </div>
            </header>
            <article>
                <h1>5.7.1 비주얼 서보잉(Visual Servoing)을 위한 미분 가능한 렌더링 활용</h1>
<h2>1.  서론: 제어 루프 안으로 들어온 그래픽스</h2>
<p>로봇 공학의 역사에서 시각 정보 기반 제어, 즉 비주얼 서보잉(Visual Servoing)은 로봇이 외부 환경을 인지하고 상호작용하는 능력을 결정짓는 중추적인 기술로 발전해 왔다. 초기 로봇 시스템은 사전에 정의된 환경 모델이나 정밀하게 캘리브레이션된 좌표계에 의존했으나, 비전 센서의 도입은 비정형 환경에서의 유연한 동작을 가능하게 했다. 전통적으로 비주얼 서보잉은 이미지 평면 상의 기하학적 특징(점, 선, 모서리 등)을 추적하여 오차를 최소화하는 **이미지 기반 비주얼 서보잉(IBVS, Image-Based Visual Servoing)**과, 3D 공간 상의 위치와 자세를 명시적으로 복원하여 제어하는 **위치 기반 비주얼 서보잉(PBVS, Position-Based Visual Servoing)**으로 양분되어 발전해 왔다.</p>
<p>그러나 이러한 특징 기반(Feature-based) 접근 방식은 “특징 추출(Feature Extraction)“이라는 전처리 단계에 과도하게 의존한다는 근본적인 취약점을 안고 있다. 텍스처가 부족한 환경, 복잡한 조명 변화, 모션 블러(Motion Blur), 혹은 객체 간의 폐색(Occlusion)이 발생하는 상황에서 특징점 매칭은 쉽게 실패하며, 이는 곧 제어 루프의 붕괴로 이어진다. 이를 극복하기 위해 이미지 전체의 픽셀 정보를 활용하는 직접 비주얼 서보잉(Direct Visual Servoing) 혹은 포토메트릭 비주얼 서보잉(Photometric Visual Servoing)이 제안되었으나, 2D 이미지 간의 단순 정합은 3D 공간의 깊이 정보나 복잡한 형상 변화를 완벽하게 반영하지 못해 수렴 영역(Convergence Basin)이 좁다는 한계가 있었다.</p>
<p>이러한 맥락에서 딥러닝과 컴퓨터 그래픽스의 융합으로 탄생한 **미분 가능한 렌더링(Differentiable Rendering, DR)**은 로봇 제어의 새로운 패러다임을 제시한다. 미분 가능한 렌더링은 3D 장면 파라미터(형상, 텍스처, 조명, 카메라 포즈 등)로부터 2D 이미지를 생성하는 전 과정을 미분 가능한 연산 그래프로 구성한다. 이는 로봇이 “현재 내가 어떻게 보이는가(Synthesis)“를 시뮬레이션하고, 실제 관측된 이미지와의 차이를 미분(Analysis)하여, 그 오차를 줄이는 방향으로 로봇의 제어 입력을 직접 최적화할 수 있음을 의미한다. 특히 Neural Radiance Fields (NeRF)와 3D Gaussian Splatting (3DGS)과 같은 혁신적인 장면 표현 방식의 등장은 로봇이 환경을 이해하는 방식을 넘어, 환경을 재현하고 이를 통해 자신의 행동을 최적화하는 ‘Analysis-by-Synthesis’ 제어 루프를 실현 가능하게 만들었다.</p>
<p>본 절에서는 미분 가능한 렌더링이 비주얼 서보잉에 어떻게 통합되는지, 그 수학적 원리와 NeRF 및 3DGS를 활용한 구체적인 방법론, 그리고 이를 통해 구현되는 차세대 로봇 제어 시스템인 ‘Self-Modeling’ 로봇에 대해 심층적으로 다룬다.</p>
<hr />
<h2>2.  미분 가능한 렌더링 기반 제어의 이론적 기초</h2>
<h3>2.1  수학적 정식화 (Mathematical Formulation)</h3>
<p>전통적인 IBVS가 기하학적 특징 <span class="math math-inline">s</span>의 오차를 최소화하는 반면, 미분 가능한 렌더링 기반 서보잉은 렌더링된 이미지 <span class="math math-inline">I_{ren}</span>과 관측된 이미지 <span class="math math-inline">I_{obs}</span> 사이의 포토메트릭 손실(Photometric Loss) <span class="math math-inline">\mathcal{L}</span>을 최소화하는 최적화 문제로 정의된다.</p>
<p>로봇의 상태(State) 또는 카메라의 포즈를 <span class="math math-inline">\xi \in SE(3)</span> (혹은 관절 각도 <span class="math math-inline">q \in \mathbb{R}^n</span>)라고 하고, 3D 장면 파라미터를 <span class="math math-inline">\Theta</span>라고 할 때, 렌더링 함수 <span class="math math-inline">R</span>은 다음과 같이 표현된다:<br />
<span class="math math-display">
I_{ren} = R(\Theta, \xi)
</span><br />
우리의 목표는 손실 함수 <span class="math math-inline">\mathcal{L}(I_{obs}, R(\Theta, \xi))</span>를 최소화하는 최적의 제어 입력 <span class="math math-inline">\xi^*</span>를 찾는 것이다. 이를 위해 경사 하강법(Gradient Descent) 혹은 가우스-뉴턴(Gauss-Newton) 방법을 사용할 수 있으며, 핵심은 손실 함수에 대한 상태 변수 <span class="math math-inline">\xi</span>의 그라디언트 <span class="math math-inline">\nabla_\xi \mathcal{L}</span>를 계산하는 것이다.</p>
<p>체인 룰(Chain Rule)에 의해 이 그라디언트는 다음과 같이 분해된다:<br />
<span class="math math-display">
\frac{\partial \mathcal{L}}{\partial \xi} = \sum_{u,v} \underbrace{\frac{\partial \mathcal{L}}{\partial I_{ren}(u,v)}}_{\text{Pixel-wise Loss Gradient}} \cdot \underbrace{\frac{\partial I_{ren}(u,v)}{\partial \xi}}_{\text{Rendering Jacobian}}
</span><br />
여기서 첫 번째 항은 픽셀 단위의 오차(예: L1, L2, SSIM 등)에 대한 미분이며, 두 번째 항인 **렌더링 야코비안(Rendering Jacobian)**이 바로 미분 가능한 렌더러가 제공해야 하는 핵심 정보이다.</p>
<table><thead><tr><th><strong>구분</strong></th><th><strong>전통적 IBVS</strong></th><th><strong>미분 가능한 렌더링 기반 서보잉</strong></th></tr></thead><tbody>
<tr><td><strong>입력 데이터</strong></td><td>추출된 희소 특징점 (Points, Lines)</td><td>이미지 전체의 밀집 픽셀 (Dense Pixels)</td></tr>
<tr><td><strong>매핑 함수</strong></td><td>투영 기하학 (Projective Geometry)</td><td>미분 가능한 렌더링 함수 (<span class="math math-inline">R(\Theta, \xi)</span>)</td></tr>
<tr><td><strong>야코비안</strong></td><td>상호작용 행렬 (<span class="math math-inline">L_s, J_{img}</span>)</td><td>렌더링 함수의 그라디언트 (<span class="math math-inline">\nabla_\xi R</span>)</td></tr>
<tr><td><strong>최적화 대상</strong></td><td>기하학적 오차 (Geometric Error)</td><td>포토메트릭 오차 (Photometric Error)</td></tr>
<tr><td><strong>주요 장점</strong></td><td>계산 속도가 빠름</td><td>텍스처, 조명 변화, 특징 소실에 강건함</td></tr>
<tr><td><strong>주요 단점</strong></td><td>특징 매칭 실패 시 제어 불능</td><td>높은 계산 비용, 국소 최적해 위험</td></tr>
</tbody></table>
<h3>2.2  포토메트릭 제어와의 차별성</h3>
<p>기존의 직접 비주얼 서보잉(DVS) 역시 픽셀 강도를 사용했으나, 2D 이미지 평면에서의 픽셀 이동(Optical Flow)이나 단순한 평면 호모그래피(Homography) 가정에 의존하는 경우가 많았다. 반면, 미분 가능한 렌더링은 3D 형상(Geometry)과 조명 모델(Lighting Model)을 내재하고 있어, 카메라 시점 변화에 따른 가려짐(Occlusion)이나 원근 효과(Perspective distortion)를 물리적으로 정확하게 예측할 수 있다. 이는 로봇이 큰 변위(Large Displacement)를 이동해야 하거나 복잡한 3D 구조물을 조작해야 할 때 수렴 안정성을 비약적으로 높여준다.</p>
<hr />
<h2>3.  NeRF 기반의 비주얼 서보잉 (NeRF-Based Visual Servoing)</h2>
<p>Neural Radiance Fields (NeRF)는 3D 공간의 점 <span class="math math-inline">(x, y, z)</span>과 보는 방향 <span class="math math-inline">(\theta, \phi)</span>을 입력으로 받아 색상(RGB)과 밀도(Density)를 출력하는 MLP(Multi-Layer Perceptron) 네트워크이다. NeRF는 비주얼 서보잉을 위한 고해상도의 연속적인(Continuous) ‘지도’ 역할을 수행할 수 있다.</p>
<h3>3.1  iNeRF: “Analysis-by-Synthesis“의 구현</h3>
<p>iNeRF (Inverting NeRF)는 NeRF 기반 포즈 추정의 시초격인 연구로, 학습된 NeRF 모델을 고정하고 입력 이미지와 렌더링 이미지의 차이를 최소화하는 카메라 포즈를 역추적하는 방식을 제안했다. 이는 비주얼 서보잉의 “Look-and-Move” 방식과 수학적으로 동일한 구조를 가진다.</p>
<p><strong>알고리즘 흐름:</strong></p>
<ol>
<li><strong>초기화:</strong> 로봇의 현재 카메라 포즈 추정값 <span class="math math-inline">T_0</span>에서 시작한다.</li>
<li><strong>렌더링:</strong> 현재 포즈 <span class="math math-inline">T_k</span>에서 NeRF를 통해 이미지를 렌더링한다. (<span class="math math-inline">\hat{I} = F_\Theta(T_k)</span>)</li>
<li><strong>오차 계산:</strong> 실제 관측된 이미지 <span class="math math-inline">I_{obs}</span>와 렌더링된 이미지 <span class="math math-inline">\hat{I}</span> 사이의 픽셀 차이를 계산한다.</li>
<li><strong>역전파:</strong> 오차를 역전파하여 포즈 업데이트 <span class="math math-inline">\Delta T</span>를 계산한다. 이때 NeRF의 네트워크 가중치 <span class="math math-inline">\Theta</span>는 고정(Freeze)되고, 입력 포즈 <span class="math math-inline">T</span>만이 업데이트된다.</li>
<li><strong>업데이트:</strong> <span class="math math-inline">T_{k+1} \leftarrow T_k \oplus \Delta T</span> (여기서 <span class="math math-inline">\oplus</span>는 리 군(Lie Group) 상의 업데이트 연산).</li>
</ol>
<p>iNeRF의 핵심 기여 중 하나는 <strong>관심 픽셀 샘플링(Interest Point Sampling)</strong> 전략이다. 전체 이미지를 매번 렌더링하는 것은 계산 비용이 매우 높기 때문에, iNeRF는 엣지나 텍스처가 풍부한 영역, 혹은 오차가 큰 영역의 픽셀들을 몬테카를로 방식으로 샘플링하여 그라디언트를 계산한다. 이는 제어 주기를 단축시키면서도 정보량이 풍부한 그라디언트를 확보하는 핵심 기법이다.</p>
<h3>3.2  NeRF-Navigation과 경로 계획의 통합</h3>
<p>NeRF는 단순히 현재 위치를 추정하는 것을 넘어, 로봇의 경로 계획(Path Planning)에도 활용된다. **Adamkiewicz et al.**의 연구에서는 NeRF를 환경 모델로 사용하여 로봇의 동역학적 제약 조건을 만족하면서 충돌을 회피하고 포토메트릭 손실을 최소화하는 궤적 최적화(Trajectory Optimization)를 수행했다.</p>
<p>이 시스템에서 상태 추정 필터(State Estimation Filter)는 렌더링 기반의 포토메트릭 손실과 로봇의 동역학 모델(Dynamics Model) 예측 오차를 결합하여 칼만 필터(Kalman Filter)와 유사한 방식으로 최적의 상태를 추정한다.<br />
<span class="math math-display">
J(\mu_t) = \underbrace{\| I_{obs} - \text{Render}(\text{NeRF}, \mu_t) \|^2}_{\text{Photometric Loss}} + \underbrace{\| \mu_t - f(\mu_{t-1}, u_{t-1}) \|^2}_{\text{Process Model Loss}}
</span><br />
이러한 접근법은 텍스처가 부족하거나 특징점이 없는 영역(Textureless Region)에서도 로봇의 동역학적 예측에 의존하여 강건한 위치 추정을 가능하게 하며, 시각 정보가 다시 확보되면 렌더링 손실을 통해 표류(Drift)를 보정한다.</p>
<h3>3.3  NeRF-IBVS: 하이브리드 접근법</h3>
<p>순수 NeRF 최적화 방식은 초기 포즈가 정답에서 멀 경우 국소 최적해(Local Minima)에 빠질 위험이 있다. 이를 보완하기 위해 <strong>NeRF-IBVS</strong>는 NeRF로부터 얻은 3D 깊이 정보를 사전 지식(Prior)으로 활용하여 전통적인 IBVS의 야코비안 행렬 <span class="math math-inline">L_s</span>를 초기화하거나 보정하는 데 사용한다. 즉, NeRF가 제공하는 정확한 3D 구조 정보를 이용해 IBVS의 상호작용 행렬을 더욱 정교하게 구성함으로써, 대략적인 초기 위치(Coarse Pose)에서 정밀한 목표 위치(Fine Pose)로의 수렴성을 보장한다. 이 방식은 NeRF의 무거운 렌더링 부담을 줄이면서도 3D 정보의 이점을 취하는 실용적인 절충안이다.</p>
<hr />
<h2>4.  3D Gaussian Splatting (3DGS): 실시간 고속 제어의 혁명</h2>
<p>NeRF는 뛰어난 장면 표현력을 가지지만, 볼륨 렌더링(Volume Rendering) 방식의 높은 계산 비용으로 인해 수백 Hz 이상의 제어 루프가 요구되는 고속 비주얼 서보잉에는 적용이 제한적이었다. 2023년 등장한 **3D Gaussian Splatting (3DGS)**은 이러한 한계를 극복하고 실시간 로봇 제어의 새로운 표준으로 부상하고 있다.</p>
<h3>4.1  3DGS의 명시적 표현과 렌더링 미분</h3>
<p>3DGS는 3D 장면을 수백만 개의 가우시안 타원체(3D Gaussians)의 집합으로 명시적(Explicit)으로 표현한다. 각 가우시안 <span class="math math-inline">G_k</span>는 중심 위치 <span class="math math-inline">\mu_k</span>, 공분산 <span class="math math-inline">\Sigma_k</span>, 색상(SH Coefficients) <span class="math math-inline">c_k</span>, 불투명도 <span class="math math-inline">\alpha_k</span>로 정의된다. 3DGS의 가장 큰 강점은 <strong>래스터화(Rasterization)</strong> 기반의 렌더링 파이프라인이 미분 가능하다는 점이며, 이는 광선 투사(Ray Casting) 방식보다 훨씬 빠른 속도(100+ FPS)를 제공한다.</p>
<p>비주얼 서보잉을 위한 카메라 포즈 최적화에서 3DGS의 미분 과정은 다음과 같은 체인 룰을 따른다 :</p>
<p><strong>단계 1: 픽셀 오차에서 투영된 2D 가우시안으로의 역전파</strong></p>
<p>손실 함수 <span class="math math-inline">\mathcal{L}</span>에 대해 렌더링된 픽셀 색상 <span class="math math-inline">C</span>의 미분을 구하고, 이를 블렌딩(Blending) 공식에 따라 각 가우시안의 2D 속성(위치 <span class="math math-inline">\mu^{2D}</span>, 공분산 <span class="math math-inline">\Sigma^{2D}</span>, 불투명도 <span class="math math-inline">\alpha</span>)으로 전파한다.<br />
<span class="math math-display">
\frac{\partial \mathcal{L}}{\partial \mu^{2D}_k}, \quad \frac{\partial \mathcal{L}}{\partial \Sigma^{2D}_k}, \quad \frac{\partial \mathcal{L}}{\partial \alpha_k}
</span><br />
<strong>단계 2: 2D 투영에서 3D 공간 및 카메라 포즈로의 역전파</strong></p>
<p>여기가 비주얼 서보잉의 핵심이다. 3D 가우시안이 2D 화면으로 투영될 때, 카메라의 뷰 행렬 <span class="math math-inline">W</span> (World-to-Camera Transform)가 관여한다. 투영된 2D 공분산 <span class="math math-inline">\Sigma^{2D}</span>는 다음과 같이 근사된다:<br />
<span class="math math-display">
\Sigma^{2D} = J W \Sigma^{3D} W^T J^T
</span><br />
여기서 <span class="math math-inline">J</span>는 투영 변환의 아핀 근사 야코비안이다. 따라서 카메라 포즈(뷰 행렬 <span class="math math-inline">W</span>)에 대한 미분은 다음과 같이 유도된다:<br />
<span class="math math-display">
\frac{\partial \mathcal{L}}{\partial W} = \sum_{k} \left( \frac{\partial \mathcal{L}}{\partial \Sigma^{2D}_k} \cdot \frac{\partial \Sigma^{2D}_k}{\partial W} + \frac{\partial \mathcal{L}}{\partial \mu^{2D}_k} \cdot \frac{\partial \mu^{2D}_k}{\partial W} \right)
</span><br />
이 명시적인 그라디언트 계산은 GPU 상에서 고도로 병렬화되어 수행되므로, 로봇은 실시간으로 자신의 위치 변화에 따른 시각적 변화를 예측하고 제어 입력을 생성할 수 있다.</p>
<h3>4.2  RoboSplat와 동적 조작 (Dynamic Manipulation)</h3>
<p>3DGS는 정적인 환경뿐만 아니라 로봇 팔과 같은 동적 객체(Dynamic Objects/Articulated Bodies)를 모델링하는 데에도 탁월하다. <strong>RoboSplat</strong>과 같은 연구에서는 로봇의 각 링크(Link)에 3D 가우시안 군집을 부착하고, 로봇의 URDF(Unified Robot Description Format)와 순기구학(Forward Kinematics)을 통해 가우시안들을 움직인다.</p>
<p>이 시스템은 일종의 <strong>“Digital Twin”</strong> 역할을 수행한다.</p>
<ul>
<li><strong>입력:</strong> 로봇의 관절 각도 <span class="math math-inline">q</span>와 카메라 포즈.</li>
<li><strong>과정:</strong> 관절 각도 <span class="math math-inline">q</span>에 따라 각 링크의 가우시안들이 이동 <span class="math math-inline">\rightarrow</span> 3DGS 렌더러가 이미지 생성.</li>
<li><strong>제어:</strong> 타겟 이미지(예: 전문가의 데모 영상)와 렌더링된 이미지의 차이를 최소화하는 관절 각도 <span class="math math-inline">q^*</span>를 역전파를 통해 탐색.</li>
</ul>
<p>이는 복잡한 역기구학(Inverse Kinematics)이나 특징점 추출 없이, 오직 시각적 피드백만으로 로봇 팔의 복잡한 조작 작업(Manipulation Task)을 수행할 수 있게 한다.</p>
<h3>4.3  GS-ICP 및 GS-SLAM을 통한 통합 제어</h3>
<p>비주얼 서보잉의 전제 조건은 로봇의 현재 위치 파악이다. 3DGS 기반의 SLAM 시스템인 <strong>GS-SLAM</strong>이나 <strong>GS-ICP</strong>는 맵핑과 추적(Tracking)을 하나의 가우시안 표현으로 통합한다. **GS-ICP (Generalized Iterative Closest Point with Gaussian Splatting)**는 현재 관측된 RGB-D 데이터와 맵 상의 가우시안을 정합(Registration)시킨다. 기존의 점(Point) 기반 ICP와 달리, 가우시안의 타원체 형상(공분산)을 고려하여 평면-평면(Plane-to-Plane) 정합과 유사한 효과를 내므로 노이즈에 강건하고 수렴 속도가 빠르다. 이 과정에서 계산된 포즈 오차는 비주얼 서보잉 제어기의 피드백 신호로 즉시 활용될 수 있다.</p>
<hr />
<h2>5.  미분 가능한 로봇 모델: Dr. Robot (Pixels-to-Torque)</h2>
<p>미분 가능한 렌더링을 로봇 제어의 극한까지 확장한 개념이 바로 **Dr. Robot (Differentiable Rendering of Robots)**이다. 이는 로봇 자체를 미분 가능한 함수로 모델링하여, 픽셀 오차로부터 관절 토크(Torque) 혹은 제어 명령까지 그라디언트가 끊김 없이 흐르도록 설계된 프레임워크이다.</p>
<h3>5.1  Dr. Robot의 3대 미분 가능 컴포넌트</h3>
<p>Dr. Robot은 로봇의 기구학, 형상, 외관을 통합적으로 모델링한다.</p>
<ol>
<li><strong>미분 가능한 순기구학 (Differentiable Forward Kinematics):</strong> 관절 각도 벡터 <span class="math math-inline">q</span>를 입력받아 각 링크의 좌표계 변환 행렬 <span class="math math-inline">T_{link}</span>를 출력한다. 자동 미분 라이브러리(PyTorch, JAX)를 통해 구현되어 <span class="math math-inline">\partial T_{link} / \partial q</span>를 제공한다.</li>
<li><strong>임시적 LBS (Implicit Linear Blend Skinning):</strong> 로봇 표면을 덮는 3D 가우시안들이 뼈대(Skeleton)의 움직임에 따라 어떻게 변형되는지를 정의한다. 기존의 강체(Rigid Body) 가정을 넘어, 로봇 표면의 부드러운 변형이나 관절 부위의 스키닝(Skinning)을 미분 가능하게 처리한다.</li>
<li><strong>포즈 조건부 외관 변형 (Pose-Conditioned Appearance Deformation):</strong> 로봇이 움직일 때 발생하는 그림자, 반사광, 접촉에 의한 미세한 형상 변화 등을 신경망으로 보정한다. 이는 <span class="math math-inline">q</span>에 따라 가우시안의 색상 <span class="math math-inline">c</span>나 불투명도 <span class="math math-inline">\alpha</span>를 동적으로 조정하는 함수 <span class="math math-inline">X(\mu, q)</span>로 구현된다.</li>
</ol>
<h3>5.2  픽셀에서 제어까지 (End-to-End Optimization)</h3>
<p>Dr. Robot 프레임워크 하에서는 “빨간 컵을 집어라“와 같은 고수준의 명령이나 타겟 이미지가 주어졌을 때, 이를 달성하기 위한 관절 각도 <span class="math math-inline">q</span>를 다음과 같은 최적화 문제로 푼다:<br />
<span class="math math-display">
q^* = \underset{q}{\text{argmin}} \mathcal{L}(I_{target}, \text{Render}(q))
</span><br />
여기서 <span class="math math-inline">\mathcal{L}</span>은 타겟 이미지와의 픽셀 차이일 수도 있고, CLIP과 같은 비전-언어 모델(VLM)이 제공하는 의미론적 유사도(Semantic Similarity)일 수도 있다. 중요한 점은 렌더링 과정이 미분 가능하므로, 이미지 오차로부터 직접 관절 제어 명령에 대한 그라디언트 <span class="math math-inline">\nabla_q \mathcal{L}</span>를 얻을 수 있다는 것이다. 이는 기존에 비전 모듈과 제어 모듈이 분리되어 발생했던 오차 누적 문제를 해결하고, 인식과 제어가 완전히 통합된(End-to-End) 시스템을 가능하게 한다.</p>
<hr />
<h2>6.  구현 전략 및 최적화 기법</h2>
<p>실제 환경에서 미분 가능한 렌더링 기반 서보잉을 구현할 때 고려해야 할 실질적인 최적화 전략과 기술적 난제들에 대해 논한다.</p>
<h3>6.1  손실 함수(Loss Function)의 설계</h3>
<p>단순한 픽셀 간의 L2 손실(MSE)은 조명 변화에 취약하고 국소 최적해에 빠지기 쉽다. 따라서 다음과 같은 복합 손실 함수가 권장된다 :<br />
<span class="math math-display">
\mathcal{L}_{total} = \lambda_1 \mathcal{L}_{L1} + \lambda_2 \mathcal{L}_{D-SSIM} + \lambda_3 \mathcal{L}_{perceptual} + \lambda_4 \mathcal{L}_{reg}
</span></p>
<ul>
<li><strong>L1 Loss:</strong> L2보다 아웃라이어(Outlier)에 덜 민감하여 전반적인 색상 정합에 유리하다.</li>
<li><strong>D-SSIM (Structural Dissimilarity):</strong> 픽셀의 절대 밝기보다는 구조적 유사성을 비교하므로 조명 변화에 강건하다.</li>
<li><strong>Perceptual Loss (LPIPS, VGG):</strong> 픽셀 단위 정합이 어려운 큰 변위(Large Displacement) 상황에서 이미지의 고수준 특징을 매칭하여 수렴 범위를 넓혀준다.</li>
<li><strong>Regularization (<span class="math math-inline">\mathcal{L}_{reg}</span>):</strong> 깊이(Depth) 평활도나 로봇의 동작 제한(Joint Limit) 등을 제약 조건으로 추가한다.</li>
</ul>
<h3>6.2  다중 해상도 및 Coarse-to-Fine 전략</h3>
<p>렌더링 기반 최적화는 초기 위치가 정답에서 멀 경우 그라디언트가 올바른 방향을 가리키지 않을 수 있다(Gradient Vanishing/Exploding). 이를 해결하기 위해 <strong>이미지 피라미드(Image Pyramid)</strong> 기법이 필수적이다.</p>
<ul>
<li><strong>저해상도 단계:</strong> 이미지를 축소(Downsampling)하여 블러링(Blurring)된 상태에서 최적화를 시작한다. 이는 이미지의 고주파 노이즈를 제거하고 전역적인(Global) 구조 정합을 유도하여 수렴 영역(Basin of Attraction)을 넓힌다.</li>
<li><strong>고해상도 단계:</strong> 점진적으로 해상도를 높이며 디테일한 포즈를 미세 조정(Fine-tuning)한다. 3DGS의 경우, 초기에는 크기가 큰 가우시안으로 시작하여 점차 가우시안을 작게 분할(Splitting)하는 전략과 병행된다.</li>
</ul>
<h3>6.3  동적 환경과 강건성 확보 (Robustness)</h3>
<p>실제 환경에는 로봇 외에도 움직이는 물체(사람, 다른 로봇)가 존재할 수 있다. 렌더링 모델에 없는 객체는 “Ghosting” 효과를 일으키고 잘못된 그라디언트를 생성한다.</p>
<ul>
<li><strong>M-Estimator:</strong> Huber Loss나 Tukey Biweight와 같은 강건한 손실 함수를 사용하여 오차가 지나치게 큰 픽셀(아웃라이어)의 영향을 줄인다.</li>
<li><strong>Uncertainty-aware Rendering:</strong> 렌더링 과정에서 불확실성(Uncertainty) 맵을 함께 출력하고, 불확실성이 높은 영역(동적 객체나 모델링되지 않은 영역)은 손실 함수 계산에서 마스킹(Masking) 처리한다.</li>
</ul>
<hr />
<h2>7.  응용 사례 (Applications)</h2>
<p>미분 가능한 렌더링은 기존 비주얼 서보잉이 적용되기 어려웠던 극한 환경이나 고난도 작업에서 진가를 발휘한다.</p>
<h3>7.1  수술 로봇 (Surgical Robotics)</h3>
<p>체내 환경은 텍스처가 부족하고 표면이 젖어 있어 난반사(Specular Highlight)가 심하다. 특징점 추출이 매우 어렵기 때문에 전통적인 비주얼 서보잉은 실패하기 쉽다. 미분 가능한 렌더링은 수술 도구(Instrument)의 CAD 모델과 내시경 영상을 직접 비교하여 도구의 6D 포즈를 실시간으로 추정한다. 특히, 연성 조직(Soft Tissue)의 변형을 3DGS의 변형 모델로 실시간 추적하여 정밀한 절개나 봉합 작업을 보조하는 연구가 진행 중이다.</p>
<h3>7.2  드론 및 공중 조작 (Aerial Manipulation)</h3>
<p>드론이 좁은 창문을 통과하거나 공중에서 물체를 낚아채는 작업은 매우 빠르고 정밀한 제어를 요구한다. <strong>Robust6DoF</strong>와 같은 연구는 드론에 탑재된 카메라로 물체의 카테고리 레벨 포즈를 추적하며, 3DGS 기반의 렌더링을 통해 조명 변화나 빠른 움직임에 의한 블러(Blur)에도 강건하게 물체와의 상대 위치를 유지한다. 또한, NeRF 맵 상에서 충돌 없는 경로를 실시간으로 렌더링하며 비행하는 내비게이션 기술도 상용화 단계에 있다.</p>
<h3>7.3  텍스트 기반 로봇 제어 (Language-Conditioned Control)</h3>
<p>최근에는 “Text-to-Robot“과 같이 자연어 명령을 로봇 제어로 연결하는 데 미분 가능한 렌더링이 사용된다. CLIP과 같은 멀티모달 모델은 텍스트와 이미지 사이의 유사도를 계산할 수 있다. Dr. Robot은 렌더링된 이미지와 텍스트 프롬프트 사이의 CLIP 유사도를 최대화하는 방향으로 로봇의 관절을 최적화하여, 별도의 학습 데이터 없이도 “엄지 손가락을 들어라“와 같은 명령을 수행할 수 있음을 보여주었다.</p>
<hr />
<h2>8.  결론 및 향후 전망</h2>
<p>미분 가능한 렌더링 기술의 비주얼 서보잉 도입은 로봇에게 **“상상하는 능력(Synthesis Capability)”**을 부여했다는 점에서 혁명적이다. 로봇은 이제 특징점이라는 중간 매개체 없이, 자신의 시각적 입력을 물리적 상태와 직접 연결하여 해석할 수 있게 되었다.</p>
<ol>
<li><strong>정밀도와 강건성의 비약적 향상:</strong> 텍스처가 없는 벽면, 반사 재질, 복잡한 조명 환경에서도 픽셀 전체 정보를 활용하여 강건한 제어가 가능해졌다.</li>
<li><strong>실시간성 확보:</strong> 3DGS의 등장은 미분 가능한 렌더링을 오프라인 최적화 도구에서 온라인 실시간 제어기(100Hz+)로 격상시켰다.</li>
<li><strong>통합된 지능:</strong> 인식, 모델링, 제어가 하나의 미분 가능한 파이프라인으로 결합됨으로써, 향후 대규모 파운데이션 모델(Foundation Models)과 결합된 ’물리적 지능(Embodied AI)’의 핵심 인터페이스가 될 것이다.</li>
</ol>
<p>향후 연구는 시각 정보뿐만 아니라 물리 엔진(Physics Engine)까지 미분 가능하게 통합하여, 접촉력, 마찰, 중력 등 물리적 상호작용까지 고려한 **물리 기반 비주얼 서보잉(Physically Embodied Visual Servoing)**으로 나아갈 것이다. 이는 로봇이 단순히 모양을 맞추는 것을 넘어, 힘과 상호작용을 이해하고 제어하는 진정한 의미의 자율성을 갖추게 됨을 시사한다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>Visual servoing module overview - ViSP, https://visp.inria.fr/visual-servoing/</li>
<li>Visual servoing, http://www.diag.uniroma1.it/deluca/rob2_en/17_VisualServoing.pdf</li>
<li>Direct Visual Servoing: Vision-Based Estimation and Control Using …, http://www-sop.inria.fr/members/Ezio.Malis/publications/2012/2012-ITRO-Silveira_Malis-Direct_visual_servoing_control_using_nonmetric_information.pdf</li>
<li>Direct Imitation Learning-based Visual Servoing using the Large …, https://arxiv.org/html/2406.09120v1</li>
<li>Section New Results - Inria, https://radar.inria.fr/rapportsactivite/intranet/PDF-2014/resultats-tri-Sophia.pdf</li>
<li>Perception et Robotique - PR | Modélisation, Information &amp; Systèmes, https://mis.u-picardie.fr/equipe/perception-robotique</li>
<li>Differentiable Robot Rendering, https://drrobot.cs.columbia.edu/</li>
<li>See to Do: Teaching Robots to Handle the Real World by Arvind …, https://dev.to/arvindsundararajan/see-to-do-teaching-robots-to-handle-the-real-world-by-arvind-sundararajan-phh</li>
<li>iNeRF: Inverting Neural Radiance Fields for Pose Estimation, https://www.researchgate.net/publication/361807504_iNeRF_Inverting_Neural_Radiance_Fields_for_Pose_Estimation</li>
<li>3D Gaussian Splatting in Robotics: A Survey - arXiv, https://arxiv.org/html/2410.12262v2</li>
<li>Differentiable Volumetric Rendering: Learning Implicit 3D …, https://openaccess.thecvf.com/content_CVPR_2020/papers/Niemeyer_Differentiable_Volumetric_Rendering_Learning_Implicit_3D_Representations_Without_3D_Supervision_CVPR_2020_paper.pdf</li>
<li>Differentiable Rendering for Pose Estimation in Proximity Operations, https://www.researchgate.net/publication/366603148_Differentiable_Rendering_for_Pose_Estimation_in_Proximity_Operations</li>
<li>Vision-Only Robot Navigation in a Neural Radiance World, https://pculbertson.github.io/assets/pdf/adamkiewicz2021.pdf</li>
<li>NeRF-IBVS: Visual Servo Based on NeRF for Visual Localization …, https://proceedings.neurips.cc/paper_files/paper/2023/file/1a57081f257da7b440b8eda72a0b12d4-Paper-Conference.pdf</li>
<li>iNeRF: Inverting Neural Radiance Fields for Pose Estimation, https://neuralfields.cs.brown.edu/paper_96.html</li>
<li>yenchenlin/iNeRF-public - GitHub, https://github.com/yenchenlin/iNeRF-public</li>
<li>Vision-Only Robot Navigation with NeRF | PDF - Scribd, https://www.scribd.com/document/660317344/0-NeRF-Navigation</li>
<li>NeRF-IBVS: Visual Servo Based on NeRF for Visual Localization …, <a href="https://openreview.net/forum?id=9pLaDXX8m3&amp;noteId=fQGujr9nvv">https://openreview.net/forum?id=9pLaDXX8m3¬eId=fQGujr9nvv</a></li>
<li>Novel Demonstration Generation with Gaussian Splatting Enables …, https://www.roboticsproceedings.org/rss21/p146.pdf</li>
<li>gaussian_splatting/MATH.md at main - GitHub, https://github.com/joeyan/gaussian_splatting/blob/main/MATH.md</li>
<li>GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting, https://gs-slam.github.io/</li>
<li>3DGS2: Near Second-order Converging 3D Gaussian Splatting - arXiv, https://arxiv.org/html/2501.13975v1</li>
<li>Joint Optimization of Pose Estimation and 3D Gaussian Splatting, https://arxiv.org/html/2510.26117v1</li>
<li>RD-SLAM: Real-Time Dense SLAM Using Gaussian Splatting - MDPI, https://www.mdpi.com/2076-3417/14/17/7767</li>
<li>RGBD GS-ICP SLAM - European Computer Vision Association, https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05296.pdf</li>
<li>dr-robot.pdf, https://drrobot.cs.columbia.edu/assets/dr-robot.pdf</li>
<li>What is Differentiable Rendering?, https://imaging.cs.cmu.edu/pbr_cvpr2023/index_files/pbr_cvpr2023.pdf</li>
<li>Radiance Field-Based Pose Estimation via Decoupled Optimization …, https://openaccess.thecvf.com/content/WACV2025/papers/Lu_Radiance_Field-Based_Pose_Estimation_via_Decoupled_Optimization_Under_Challenging_Initial_WACV_2025_paper.pdf</li>
<li>Jacobian Approximation for State Estimation in Robotics via …, <a href="https://back.skoltech.ru/storage/app/media/%D0%97%D0%B0%D1%89%D0%B8%D1%82%D1%8B%20PhD/2024/%D0%91%D0%BE%D1%82%D0%B0%D1%88%D0%B5%D0%B2%20%D0%9A%D0%B0%D0%B7%D0%B8%D0%B9%20%D0%A0%D1%83%D1%81%D0%BB%D0%B0%D0%BD%D0%BE%D0%B2%D0%B8%D1%87/thesis.pdf">https://back.skoltech.ru/storage/app/media/%D0%97%D0%B0%D1%89%D0%B8%D1%82%D1%8B%20PhD/2024/%D0%91%D0%BE%D1%82%D0%B0%D1%88%D0%B5%D0%B2%20%D0%9A%D0%B0%D0%B7%D0%B8%D0%B9%20%D0%A0%D1%83%D1%81%D0%BB%D0%B0%D0%BD%D0%BE%D0%B2%D0%B8%D1%87/thesis.pdf</a></li>
<li>Robust Jacobian Estimation for Uncalibrated Visual Servoing, http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_ICRA_2010/data/papers/1516.pdf</li>
<li>Guiding Vision and Touch with FisherRF for 3D Gaussian Splatting, https://arm.stanford.edu/next-best-sense</li>
<li>Multi-Agent Pose Uncertainty: A Differentiable Rendering Cramér …, https://www.researchgate.net/publication/396968195_Multi-Agent_Pose_Uncertainty_A_Differentiable_Rendering_Cramer-Rao_Bound</li>
<li>Differentiable Rendering-based Pose Estimation for Surgical …, https://www.researchgate.net/publication/398060515_Differentiable_Rendering-based_Pose_Estimation_for_Surgical_Robotic_Instruments</li>
<li>Towards Real-World Aerial Vision Guidance With Categorical 6D …, https://www.researchgate.net/publication/390495127_Towards_Real-World_Aerial_Vision_Guidance_with_Categorical_6D_Pose_Tracker</li>
<li>Quadrotor UAV Dynamic Visual Servoing Based on Differential …, https://www.mdpi.com/2076-3417/13/12/7005</li>
<li>Physically Embodied Gaussian Splatting: A Visually Learnt and …, https://proceedings.mlr.press/v270/abou-chakra25a.html</li>
<li>Physically Embodied Gaussian Splatting: A Visually Learnt and …, https://embodied-gaussians.github.io/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>