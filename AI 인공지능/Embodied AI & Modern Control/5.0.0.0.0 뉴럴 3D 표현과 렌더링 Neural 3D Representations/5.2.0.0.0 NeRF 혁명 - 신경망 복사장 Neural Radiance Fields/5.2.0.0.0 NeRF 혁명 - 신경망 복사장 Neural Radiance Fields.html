<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:5.2 NeRF 혁명: 신경망 복사장 (Neural Radiance Fields)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>5.2 NeRF 혁명: 신경망 복사장 (Neural Radiance Fields)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 5. 뉴럴 3D 표현과 렌더링 (Neural 3D Representations)</a> / <a href="index.html">5.2 NeRF 혁명: 신경망 복사장 (Neural Radiance Fields)</a> / <span>5.2 NeRF 혁명: 신경망 복사장 (Neural Radiance Fields)</span></nav>
                </div>
            </header>
            <article>
                <h1>5.2 NeRF 혁명: 신경망 복사장 (Neural Radiance Fields)</h1>
<p>3차원 공간을 기계가 어떻게 인식하고, 기억하고, 표현하는가에 대한 질문은 로봇 공학(Robotics)과 컴퓨터 비전(Computer Vision) 분야에서 오랫동안 가장 본질적이면서도 해결하기 어려운 난제 중 하나였다. 로봇이 물리적 세계와 상호작용하기 위해서는 단순한 2차원 픽셀의 집합을 넘어, 공간의 깊이, 기하학적 구조(Geometry), 그리고 표면의 질감과 조명 효과를 온전하게 이해해야 한다. 과거의 로봇 시스템들은 포인트 클라우드(Point Cloud), 복셀(Voxel), 메쉬(Mesh)와 같은 이산적(Discrete)이고 명시적(Explicit)인 표현 방식에 의존했다. 이러한 방식들은 직관적이지만, 해상도가 높아질수록 메모리 사용량이 기하급수적으로 증가하거나(<span class="math math-inline">O(N^3)</span>), 위상(Topology)의 변화를 표현하는 데 한계가 뚜렷했다.</p>
<p>그러나 2020년, 딥러닝 기반의 새로운 3차원 표현 방식인 **Neural Radiance Fields (NeRF)**가 등장하며, 공간 표현의 패러다임은 이산적 구조체에서 <strong>연속적(Continuous) 함수</strong>로 급격하게 이동하였다.1 NeRF는 3차원 장면을 거대한 포인트 데이터베이스로 저장하는 대신, 다층 퍼셉트론(Multilayer Perceptron, MLP)이라는 신경망의 가중치(Weight) 내에 최적화된 함수 형태로 암시적(Implicit)으로 인코딩한다. 이는 로봇이 복잡한 환경을 MB 단위의 매우 작은 메모리로 저장하면서도, 임의의 시점에서 광학적으로 일관된 고해상도 이미지를 생성할 수 있게 함을 의미한다. 더욱 중요한 것은, NeRF가 <strong>미분 가능한 렌더링(Differentiable Rendering)</strong> 파이프라인을 구축함으로써, 2차원 이미지로부터 3차원 공간 정보를 역전파(Backpropagation)를 통해 스스로 학습할 수 있는 길을 열었다는 점이다.3</p>
<p>본 장에서는 NeRF가 가져온 공간 표현의 혁명을 로봇 공학적 관점에서 심층적으로 분석한다. NeRF의 수학적 원리인 플레놉틱 함수(Plenoptic Function)와 볼륨 렌더링 방정식(Volume Rendering Equation)을 상세히 유도하고, 신경망이 고주파(High-frequency) 디테일을 학습하기 위해 도입한 위치 인코딩(Positional Encoding)의 기작을 규명한다. 또한, 단순한 시각적 복원을 넘어 로봇의 위치 추정(Localization), 경로 계획(Path Planning), 그리고 조작(Manipulation)에 NeRF가 어떻게 응용되고 있는지를 최신 연구 결과들을 바탕으로 기술한다.</p>
<h2>1.  연속적 볼륨 표현과 5차원 플레놉틱 함수</h2>
<p>NeRF의 핵심 철학은 “장면(Scene)은 함수(Function)다“라는 명제로 요약된다. 기존의 방식이 공간을 ’채워진 블록’들의 집합으로 보았다면, NeRF는 공간의 어느 위치에서든 그 지점의 특성을 질의(Query)하면 답을 주는 연속적인 함수로 정의한다. 이 함수는 수학적으로 <strong>5차원 벡터 함수</strong>로 표현된다.<br />
<span class="math math-display">
F_{\Theta} : (\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma)
</span><br />
여기서 입력과 출력은 다음과 같이 정의된다 1:</p>
<ul>
<li><strong>입력 (5D Coordinate):</strong></li>
<li><span class="math math-inline">\mathbf{x} = (x, y, z)</span>: 3차원 유클리드 공간상의 위치 좌표.</li>
<li><span class="math math-inline">\mathbf{d} = (\theta, \phi)</span>: 관찰자가 해당 지점을 바라보는 2차원 시야각(Viewing Direction). 일반적으로 3차원 단위 벡터 <span class="math math-inline">\mathbf{d} = (d_x, d_y, d_z)</span>로 표현하여 입력 차원을 맞춘다.</li>
<li><strong>출력 (Output):</strong></li>
<li><span class="math math-inline">\mathbf{c} = (r, g, b)</span>: 해당 위치에서 방사되는 빛의 색상(Color) 및 강도(Radiance).</li>
<li><span class="math math-inline">\sigma</span>: 해당 위치의 <strong>부피 밀도(Volume Density)</strong>. 광선이 해당 지점의 미소 구간을 통과할 때 입자와 충돌하여 멈출(Terminate) 확률 밀도를 의미한다.</li>
</ul>
<p>이 5차원 함수는 컴퓨터 그래픽스와 비전에서 오랫동안 다루어 온 **플레놉틱 함수(Plenoptic Function)**의 압축된 형태이다. 플레놉틱 함수는 본래 7차원(위치 3, 각도 2, 시간 1, 파장 1)으로 정의되어, “세상의 모든 시공간에서 관찰 가능한 모든 광선“을 기술한다. NeRF는 여기서 시간(정적 장면 가정)과 파장(RGB 3채널로 단순화)을 고정하여 5차원으로 축소한 모델이다.7 이 함수를 신경망 <span class="math math-inline">F_{\Theta}</span>로 근사(Approximate)함으로써, 우리는 기하학적 형상과 표면의 반사 특성을 동시에, 그리고 연속적으로 저장할 수 있게 된다.</p>
<h3>1.1 입력 차원의 분리와 시점 의존성 (View-Dependency)</h3>
<p>NeRF 아키텍처 설계에서 가장 중요한 통찰 중 하나는 밀도 <span class="math math-inline">\sigma</span>와 색상 <span class="math math-inline">\mathbf{c}</span>의 의존성을 분리한 것이다. 물리적 세계의 일관성을 유지하기 위해, 물체의 형상을 나타내는 밀도는 관찰자가 어디서 보느냐에 따라 변해서는 안 된다. 반면, 색상은 조명과 재질의 반사 특성(예: 금속의 반짝임, 유리의 투명함)에 따라 관찰 각도에 의존하여 변할 수 있다.1</p>
<p>NeRF는 이를 구현하기 위해 다음과 같은 제약 조건을 네트워크 구조에 반영한다:</p>
<ol>
<li>밀도(Geometry)의 시점 불변성 (View-Independence):</li>
</ol>
<p>밀도 <span class="math math-inline">\sigma</span>는 오직 위치 <span class="math math-inline">\mathbf{x}</span>의 함수여야 한다. 즉, <span class="math math-inline">\sigma = F_{\sigma}(\mathbf{x})</span>이다. 이는 다각도에서 촬영된 이미지들 사이에서 물체의 형상이 일관되게 유지되도록 강제하는 기하학적 제약 조건(Geometric Constraint)으로 작용한다. 만약 밀도가 시점 <span class="math math-inline">\mathbf{d}</span>에 의존하게 된다면, 네트워크는 형상을 올바르게 학습하는 대신 보는 각도마다 물체의 모양을 바꾸어 가며 정답 이미지를 억지로 맞춰내는 과적합(Overfitting) 상태에 빠질 수 있다.</p>
<ol start="2">
<li>색상(Appearance)의 시점 의존성 (View-Dependence):</li>
</ol>
<p>색상 <span class="math math-inline">\mathbf{c}</span>는 위치 <span class="math math-inline">\mathbf{x}</span>와 시점 <span class="math math-inline">\mathbf{d}</span> 모두의 함수이다. 즉, <span class="math math-inline">\mathbf{c} = F_{color}(\mathbf{x}, \mathbf{d})</span>이다. 이를 통해 NeRF는 단순한 램버시안(Lambertian, 모든 방향으로 동일하게 빛을 반사하는 무광 표면) 재질뿐만 아니라, 보는 각도에 따라 하이라이트(Specular Highlight)의 위치가 이동하는 비-램버시안(Non-Lambertian) 효과를 매우 사실적으로 표현할 수 있다.</p>
<p>아래의 표는 이러한 변수 간의 관계와 물리적 의미를 요약한다.</p>
<table><thead><tr><th><strong>구성 요소</strong></th><th><strong>입력 변수</strong></th><th><strong>출력 변수</strong></th><th><strong>물리적 의미</strong></th><th><strong>로봇 공학적 해석</strong></th></tr></thead><tbody>
<tr><td><strong>기하학적 구조</strong></td><td><span class="math math-inline">\mathbf{x} = (x, y, z)</span></td><td><span class="math math-inline">\sigma</span> (Density)</td><td>공간의 불투명도, 입자 존재 확률</td><td><strong>점유 지도(Occupancy Map)</strong>, 충돌 가능성</td></tr>
<tr><td><strong>표면 외관</strong></td><td><span class="math math-inline">\mathbf{x}, \mathbf{d} = (\theta, \phi)</span></td><td><span class="math math-inline">\mathbf{c}</span> (RGB)</td><td>방사되는 빛의 색상 및 강도</td><td><strong>비전 센서 시뮬레이션</strong>, 객체 인식 특징</td></tr>
</tbody></table>
<p>이러한 분리 구조는 MLP 내부에서 먼저 위치 <span class="math math-inline">\mathbf{x}</span>만을 입력받아 밀도 <span class="math math-inline">\sigma</span>와 특징 벡터(Feature Vector)를 생성하고, 이후 단계에서 시점 <span class="math math-inline">\mathbf{d}</span>를 특징 벡터와 결합(Concatenation)하여 색상을 예측하는 형태로 구현된다. 이는 로봇이 NeRF 모델을 활용할 때, 시점 정보를 무시하고 밀도 정보만을 추출하여 3차원 점유 지도(Occupancy Grid)를 생성하거나 충돌 체크를 수행할 수 있음을 의미하며, 이는 NeRF가 그래픽스 도구를 넘어 로봇의 환경 모델로 기능할 수 있는 기반이 된다.9</p>
<h2>2.  볼륨 렌더링 방정식 (Volume Rendering Equation)의 미분 가능성</h2>
<p>NeRF가 혁명적인 이유는 3차원 표현 방식(MLP)과 2차원 이미지 생성 과정(Rendering)이 완전히 미분 가능(Differentiable)하게 연결되어 있다는 점이다. 전통적인 그래픽스 파이프라인(Rasterization)은 이산적인 픽셀 결정 과정(Z-buffer 등)으로 인해 미분이 불가능하거나 매우 복잡한 근사가 필요했다. 반면, NeRF는 볼륨 렌더링(Volume Rendering) 기법을 사용하여 렌더링 과정을 매끄러운 적분식으로 표현함으로써, 오직 2D 이미지와 카메라 포즈(Pose) 데이터만으로 3차원 장면을 역전파를 통해 학습할 수 있다.1</p>
<h3>2.1 광학적 원리: 복사 전달 (Radiative Transfer)</h3>
<p>NeRF의 렌더링 모델은 물리학의 복사 전달 이론(Radiative Transfer Theory)에 기반한다. 허공에 떠 있는 미세한 입자들의 구름(Cloud of particles)을 상상해 보자. 카메라의 한 픽셀에서 출발한 광선(Ray) <span class="math math-inline">\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}</span>가 이 구름을 통과할 때, 광선은 입자에 부딪혀 흡수되거나, 입자가 발산하는 빛을 추가로 획득한다.</p>
<p>카메라에 도달하는 최종 색상 <span class="math math-inline">C(\mathbf{r})</span>는 광선을 따라 존재하는 모든 지점의 색상과 밀도의 가중 합으로 표현된다.11<br />
<span class="math math-display">
C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t)) \mathbf{c}(\mathbf{r}(t), \mathbf{d}) \, dt
</span><br />
이 적분식의 각 항은 다음과 같은 물리적 의미를 갖는다:</p>
<ol>
<li>
<p><strong>밀도 항 <span class="math math-inline">\sigma(\mathbf{r}(t))</span>:</strong> 위치 <span class="math math-inline">\mathbf{r}(t)</span>에 입자가 존재할 확률 밀도이다. 이 값이 클수록 해당 지점은 불투명하며, 광선이 여기서 멈출 확률이 높다.</p>
</li>
<li>
<p><strong>색상 항 <span class="math math-inline">\mathbf{c}(\mathbf{r}(t), \mathbf{d})</span>:</strong> 해당 위치에서 카메라 방향 <span class="math math-inline">\mathbf{d}</span>로 방출되는 빛의 색상이다.</p>
</li>
<li>
<p><strong>투과율 항 <span class="math math-inline">T(t)</span> (Transmittance):</strong> 가장 중요한 개념으로, 광선이 시작점 <span class="math math-inline">t_n</span>에서 현재 지점 <span class="math math-inline">t</span>까지 도달하는 동안 **“무언가에 부딪혀 차단되지 않고 살아남을 확률”**을 의미한다. 이는 비어-램버트 법칙(Beer-Lambert Law)에 따라 앞선 구간의 밀도 적분값에 대해 지수적으로 감소한다.<br />
<span class="math math-display">
T(t) = \exp \left( - \int_{t_n}^{t} \sigma(\mathbf{r}(s)) \, ds \right)
</span></p>
</li>
</ol>
<p>직관적으로 해석하면, <span class="math math-inline">T(t)</span>는 “앞쪽이 비어 있어야 뒤쪽이 보인다“는 가림(Occlusion) 현상을 수학적으로 모델링한 것이다. 만약 광선 앞쪽에 밀도가 높은 물체(벽 등)가 있다면, 그 지점을 통과한 직후 <span class="math math-inline">T(t)</span>는 0에 수렴하게 되고, 그 뒤에 있는 물체(벽 뒤의 물건)는 적분식에 기여하지 못하게 되어 보이지 않게 된다.</p>
<h3>2.2 수치적 근사: 구적법 (Quadrature)과 이산화</h3>
<p>실제 컴퓨터 연산에서는 연속 적분을 수행할 수 없으므로, NeRF는 광선을 <span class="math math-inline">N</span>개의 구간(bin)으로 나누고 각 구간에서 샘플링한 점들을 이용해 적분식을 이산화된 합으로 근사한다. 이를 **구적법(Quadrature)**이라 한다.1</p>
<p>이산화된 렌더링 방정식은 다음과 같다:<br />
<span class="math math-display">
\hat{C}(\mathbf{r}) = \sum_{i=1}^{N} T_i \alpha_i \mathbf{c}_i
</span><br />
여기서 <span class="math math-inline">\alpha_i</span>는 <span class="math math-inline">i</span>번째 구간에서의 불투명도(Opacity)이며, 다음과 같이 정의된다:<br />
<span class="math math-display">
\alpha_i = 1 - \exp(-\sigma_i \delta_i)
</span></p>
<ul>
<li><span class="math math-inline">\delta_i = t_{i+1} - t_i</span>: 인접한 샘플 포인트 사이의 거리(구간의 길이).</li>
<li><span class="math math-inline">\sigma_i</span>: <span class="math math-inline">i</span>번째 샘플 위치에서의 부피 밀도.</li>
<li><span class="math math-inline">T_i = \prod_{j=1}^{i-1} (1 - \alpha_j)</span>: <span class="math math-inline">i</span>번째 구간까지 도달할 투과율 (누적 투과율).</li>
</ul>
<p>이 식은 <strong>알파 블렌딩(Alpha Compositing)</strong> 공식과 정확히 일치한다. NeRF는 각 구간 내에서 밀도 <span class="math math-inline">\sigma</span>와 색상 <span class="math math-inline">\mathbf{c}</span>가 일정하다는 가정(Piecewise constant assumption) 하에 이 근사식을 유도하였다.11 중요한 점은 <span class="math math-inline">\alpha_i</span>와 <span class="math math-inline">T_i</span>를 구성하는 모든 연산(지수 함수, 곱셈, 뺄셈)이 미분 가능하다는 것이다. 따라서 렌더링된 픽셀 색상 <span class="math math-inline">\hat{C}(\mathbf{r})</span>과 실제 학습 이미지의 픽셀 색상 <span class="math math-inline">C_{gt}(\mathbf{r})</span> 사이의 차이(Loss)를 계산하면, 체인 룰(Chain Rule)을 통해 MLP의 가중치 <span class="math math-inline">\Theta</span>에 대한 그래디언트를 계산하고 모델을 업데이트할 수 있다.<br />
<span class="math math-display">
\mathcal{L} = \sum_{\mathbf{r} \in \mathcal{R}} \|\hat{C}(\mathbf{r}) - C_{gt}(\mathbf{r})\|_2^2
</span><br />
이 단순한 L2 손실 함수(Mean Squared Error)만으로도 NeRF는 복잡한 3차원 형상과 조명 효과를 스스로 학습해낸다. 이는 로봇이 명시적인 3D 레이블(Depth Map 등) 없이도, 카메라로 촬영한 RGB 이미지만 있으면 환경을 3차원으로 복원할 수 있음을 시사한다.</p>
<h2>3.  신경망 아키텍처와 위치 인코딩 (Positional Encoding)</h2>
<p>NeRF가 성공할 수 있었던 기술적 비결 중 하나는 딥러닝 모델이 겪는 고질적인 문제인 **스펙트럼 편향(Spectral Bias)**을 해결한 방식에 있다. 일반적인 MLP는 저주파(Low-frequency) 신호를 학습하는 데에는 능숙하지만, 고주파(High-frequency) 신호, 즉 이미지의 날카로운 경계선이나 미세한 텍스처를 학습하는 데에는 매우 느리거나 실패하는 경향이 있다.15 단순한 <span class="math math-inline">(x, y, z)</span> 좌표를 입력으로 사용할 경우, NeRF가 생성하는 이미지는 초점이 맞지 않은 것처럼 매우 흐릿하게 나타난다.</p>
<h3>3.1 고주파 디테일의 복원: 푸리에 피처 매핑</h3>
<p>이 문제를 해결하기 위해 NeRF는 입력 좌표를 고차원의 주파수 영역으로 매핑하는 <strong>위치 인코딩(Positional Encoding)</strong> 기법을 도입했다. 이는 트랜스포머(Transformer) 모델에서 순서 정보를 주입하기 위해 사용되는 위치 인코딩과 유사한 수식을 사용하지만, 그 목적은 전혀 다르다. NeRF의 위치 인코딩은 좌표 공간의 고주파 성분을 신경망이 쉽게 접근할 수 있는 형태로 변환하여, 네트워크가 고해상도 디테일을 빠르게 학습하도록 돕는 **푸리에 피처 매핑(Fourier Feature Mapping)**의 역할을 한다.1</p>
<p>위치 인코딩 함수 <span class="math math-inline">\gamma(p)</span>는 정규화된 스칼라 좌표값 <span class="math math-inline">p \in [-1, 1]</span>를 서로 다른 주파수를 가진 사인(Sinusoid) 함수의 벡터로 변환한다:<br />
<span class="math math-display">
\gamma(p) = \left( \sin(2^0 \pi p), \cos(2^0 \pi p), \cdots, \sin(2^{L-1} \pi p), \cos(2^{L-1} \pi p) \right)
</span><br />
여기서 <span class="math math-inline">L</span>은 주파수 대역(Frequency Band)의 수를 나타내는 하이퍼파라미터이다. <em>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</em> 1 논문에서는 위치 좌표 <span class="math math-inline">\mathbf{x}</span>와 시점 방향 <span class="math math-inline">\mathbf{d}</span>에 대해 서로 다른 <span class="math math-inline">L</span> 값을 적용하여 차원을 확장한다.</p>
<table><thead><tr><th><strong>입력 변수</strong></th><th><strong>원본 차원</strong></th><th><strong>주파수 대역 (L)</strong></th><th><strong>인코딩 후 차원</strong></th><th><strong>설명</strong></th></tr></thead><tbody>
<tr><td><strong>위치 (<span class="math math-inline">\mathbf{x}</span>)</strong></td><td>3</td><td>10</td><td><span class="math math-inline">3 \times 2 \times 10 = 60</span></td><td>기하학적 형상과 텍스처의 미세 디테일 표현을 위해 높은 주파수 대역 사용</td></tr>
<tr><td><strong>방향 (<span class="math math-inline">\mathbf{d}</span>)</strong></td><td>3</td><td>4</td><td><span class="math math-inline">3 \times 2 \times 4 = 24</span></td><td>색상의 변화는 위치에 비해 상대적으로 부드럽게 변하므로 낮은 주파수 대역 사용</td></tr>
</tbody></table>
<p>이러한 인코딩을 통해 MLP는 저차원 좌표 공간에서의 급격한 변화를 고차원 공간에서의 상대적으로 부드러운 변화로 인식하게 되어, 결과적으로 앨리어싱(Aliasing) 없이 고해상도의 선명한 이미지를 합성할 수 있게 된다. 연구 결과에 따르면 위치 인코딩이 없을 경우, NeRF의 PSNR(Peak Signal-to-Noise Ratio) 성능은 현저히 떨어지며 고주파 텍스처를 전혀 복원하지 못한다.17</p>
<h3>3.2 MLP 네트워크 구조 상세</h3>
<p>NeRF의 MLP <span class="math math-inline">F_{\Theta}</span>는 깊이와 효율성, 그리고 앞서 언급한 시점 의존성 분리를 고려하여 정교하게 설계되었다.1 전체 구조는 크게 두 부분으로 나뉜다.</p>
<ol>
<li><strong>밀도 예측 네트워크 (Density Network):</strong></li>
</ol>
<ul>
<li>입력: 위치 인코딩된 <span class="math math-inline">\gamma(\mathbf{x})</span> (60차원).</li>
<li>구조: 256개의 히든 유닛(Hidden Unit)을 가진 8개의 완전 연결 계층(Fully-Connected Layers). 모든 활성화 함수는 ReLU를 사용한다.</li>
<li><strong>Skip Connection:</strong> 깊은 네트워크 학습 시 발생하는 경사 소실(Gradient Vanishing)을 방지하고, 초기 위치 정보를 네트워크 깊은 곳까지 보존하기 위해 5번째 계층(Layer 4의 출력)에 원본 입력 <span class="math math-inline">\gamma(\mathbf{x})</span>를 다시 연결(Concatenation)한다.</li>
<li>출력: 8번째 계층에서 1차원 부피 밀도 <span class="math math-inline">\sigma</span> (ReLU를 통해 음이 아닌 값으로 보정)와 256차원의 기하학적 특징 벡터(Feature Vector)를 출력한다.</li>
</ul>
<ol start="2">
<li><strong>색상 예측 네트워크 (Color Network):</strong></li>
</ol>
<ul>
<li>입력: 밀도 네트워크의 출력인 256차원 특징 벡터 + 위치 인코딩된 시점 방향 <span class="math math-inline">\gamma(\mathbf{d})</span> (24차원).</li>
<li>구조: 이 두 입력을 연결(Concatenation)한 후, 1개의 추가적인 완전 연결 계층(128 채널)을 통과시킨다.</li>
<li>출력: 최종적으로 3차원 RGB 색상 <span class="math math-inline">\mathbf{c}</span>를 출력한다. 색상 값은 0과 1 사이의 범위를 가져야 하므로 Sigmoid 활성화 함수를 사용한다.</li>
</ul>
<p>이 아키텍처는 밀도 <span class="math math-inline">\sigma</span>가 시점 방향 <span class="math math-inline">\mathbf{d}</span>를 보지 못하게 구조적으로 차단함으로써, 멀티뷰 일관성(Multi-view Consistency)을 강제한다. 즉, 어떤 각도에서 보더라도 물체의 형상은 변하지 않으며, 오직 표면의 색상만이 보는 각도에 따라 달라질 수 있게 된다.</p>
<h3>3.3 계층적 볼륨 샘플링 (Hierarchical Volume Sampling)</h3>
<p>단순히 광선을 따라 균일한 간격(Uniform Sampling)으로 샘플링할 경우, NeRF의 효율성은 크게 떨어진다. 대부분의 3차원 공간은 ’빈 공간(Empty Space)’이기 때문이다. 빈 허공이나 물체 내부에 가려진 영역을 촘촘하게 샘플링하는 것은 계산 자원의 낭비이며, 정작 중요한 물체 표면(Surface) 근처의 샘플 부족으로 렌더링 품질을 저하시킬 수 있다.</p>
<p>이를 해결하기 위해 NeRF는 <strong>Coarse(조다)</strong> 네트워크와 <strong>Fine(정밀)</strong> 네트워크 두 개를 동시에 최적화하는 계층적 샘플링 전략을 사용한다.1</p>
<ol>
<li><strong>Coarse 단계:</strong> 먼저 전체 구간을 균일하게 나누어 <span class="math math-inline">N_c</span>개(보통 64개)의 점을 샘플링하고, 이를 ’Coarse MLP’에 통과시켜 대략적인 밀도 분포를 얻는다.</li>
<li><strong>PDF 생성:</strong> Coarse 네트워크에서 얻은 밀도 값들을 정규화하여, 광선을 따른 **확률 밀도 함수(PDF)**로 변환한다. 밀도가 높은 곳(물체가 있을 확률이 높은 곳)은 높은 가중치를 갖게 된다.</li>
<li><strong>Fine 단계:</strong> 생성된 PDF를 기반으로 **역변환 샘플링(Inverse Transform Sampling)**을 수행하여, <span class="math math-inline">N_f</span>개(보통 128개)의 추가 샘플을 추출한다. 이 샘플들은 물체가 존재할 확률이 높은 영역(표면 근처)에 집중적으로 배치된다.</li>
<li><strong>최종 렌더링:</strong> <span class="math math-inline">N_c + N_f</span>개의 모든 샘플을 사용하여 ’Fine MLP’를 통해 최종 색상을 계산한다.</li>
</ol>
<p>이 과정은 일종의 **중요도 샘플링(Importance Sampling)**으로, 네트워크가 공간의 중요도를 스스로 판단하여 계산 자원을 효율적으로 배분하게 만든다. 최종 손실 함수는 Coarse 네트워크와 Fine 네트워크 각각의 렌더링 결과에 대해 모두 계산되어 합산된다.<br />
<span class="math math-display">
\mathcal{L}_{total} = \mathcal{L}_{coarse} + \mathcal{L}_{fine}
</span></p>
<h2>4.  로봇 공학 및 Embodied AI에의 응용</h2>
<p>NeRF는 단순한 이미지 합성을 위한 그래픽스 도구를 넘어, 로봇이 환경을 이해하고 행동(Action)을 결정하는 데 필요한 핵심적인 지각(Perception) 모델로 자리 잡고 있다. 기존의 SLAM(Simultaneous Localization and Mapping) 시스템이 희소한 특징점(Sparse Feature)이나 노이즈가 많은 포인트 클라우드에 의존했던 것과 달리, NeRF는 환경의 밀도와 색상을 연속적으로 채워진(Dense) 형태로, 그리고 메모리 효율적으로 제공한다.21 이는 로봇이 “어디가 비어 있고, 어디가 막혀 있는지“를 미분 가능한 형태로 알 수 있게 함을 의미한다.</p>
<h3>4.1 NeRF 기반 Neural SLAM</h3>
<p>로봇 자율 주행의 근간인 SLAM 기술은 NeRF와 결합하여 <strong>Neural SLAM</strong>이라는 새로운 영역을 개척했다. 기존 SLAM이 맵을 포인트 클라우드나 복셀로 저장했다면, Neural SLAM은 맵을 신경망 가중치로 저장한다.</p>
<ul>
<li>iMAP 22: 최초의 실시간 NeRF 기반 SLAM 시스템 중 하나로, 단일 MLP를 사용하여 지도를 표현하고 카메라 포즈와 맵 파라미터를 교대로 최적화한다. 트래킹(Tracking) 스레드는 고정된 맵에 대해 현재 카메라 포즈를 최적화하고, 매핑(Mapping) 스레드는 키프레임(Keyframe)들을 사용하여 MLP 가중치를 업데이트한다.</li>
<li>NICE-SLAM 23: iMAP의 단점인 단일 MLP의 용량 한계와 “망각(Forgetting)” 문제를 해결하기 위해, 계층적인 특징 그리드(Feature Grid)를 도입하여 대규모 실내 환경에서도 정밀한 복원을 가능하게 했다.</li>
</ul>
<p>Neural SLAM의 가장 큰 장점은 <strong>밀집된(Dense) 지도</strong>를 생성한다는 것이다. 희소한 점들만으로는 장애물인지 아닌지 모호한 공간이 많지만, NeRF 기반 지도는 공간의 모든 지점에 대해 점유 확률을 제공하므로 로봇의 경로 계획에 훨씬 유리하다.</p>
<h3>4.2 충돌 회피 및 경로 계획 (Trajectory Optimization)</h3>
<p>로봇 내비게이션에서 NeRF의 부피 밀도 <span class="math math-inline">\sigma(\mathbf{x})</span>는 그 자체로 훌륭한 **비용 함수(Cost Function)**가 된다. 로봇이 이동하려는 경로 상의 점 <span class="math math-inline">\mathbf{x}</span>에서 <span class="math math-inline">\sigma(\mathbf{x})</span> 값이 크다면, 그곳은 장애물일 확률이 높으므로 충돌 위험이 크다는 뜻이다.</p>
<p>Adamkiewicz 등 9은 <strong>NeRF-Navigation</strong>이라는 개념을 제안하였다. 이들은 NeRF를 환경 모델로 사용하고, 로봇의 궤적 최적화 문제에 NeRF의 밀도 값을 충돌 페널티(Collision Penalty) 항으로 포함시켰다.<br />
<span class="math math-display">
J(\tau) = J_{smoothness}(\tau) + \lambda \int_{0}^{T} \sigma(\mathbf{x}(t)) \, dt
</span><br />
여기서 <span class="math math-inline">\tau</span>는 로봇의 궤적, <span class="math math-inline">J_{smoothness}</span>는 궤적의 부드러움, 그리고 적분항은 궤적을 따라 누적된 밀도(충돌 비용)를 의미한다. NeRF는 미분 가능하므로, 이 비용 함수의 그래디언트 <span class="math math-inline">\nabla_\tau J</span>를 계산하여 경사 하강법(Gradient Descent)으로 충돌 없는 안전한 경로를 생성할 수 있다. 이 방식은 별도의 메쉬 생성이나 거리 변환(Distance Transform) 과정 없이, 학습된 NeRF 모델을 직접 쿼리하여 경로를 계획하므로, 연기(Smoke)나 덤불 같은 비정형 장애물이 있는 환경에서도 강건하게 작동한다.10</p>
<h3>4.3 로봇 조작 (Manipulation)과 투명 물체 인식</h3>
<p>기존의 심도 센서(LiDAR, RGB-D 카메라)는 유리나 투명한 플라스틱, 거울 같은 반사율이 높은 물체(Specular/Transparent objects)에서 적외선이 투과되거나 난반사되어 데이터가 소실되는 치명적인 약점이 있었다. 이는 로봇이 유리컵을 잡거나(Grasping) 거울이 있는 환경을 이동할 때 큰 사고로 이어질 수 있다.</p>
<p>하지만 NeRF는 시점에 따른 색상 변화를 모델링할 수 있으므로, 투명한 물체의 굴절이나 반사면의 하이라이트 이동까지 학습할 수 있다.1</p>
<ul>
<li>Dex-NeRF 10: NeRF를 이용해 투명한 물체의 형상을 복원하고, 이를 기반으로 로봇 팔의 파지 위치를 계산하는 연구이다. NeRF는 투명한 물체 내부의 밀도 분포까지 기하학적으로 일관성 있게 추정할 수 있어, 심도 카메라가 “보지 못하는” 투명한 물체의 표면을 로봇에게 알려줄 수 있다. 이는 가정용 로봇이나 실험실 자동화 로봇에게 필수적인 능력이다.</li>
</ul>
<h3>4.4 시뮬레이션 및 Sim-to-Real</h3>
<p>Embodied AI의 학습을 위해서는 현실과 유사한 고충실도 시뮬레이터가 필수적이다. NeRF는 실제 환경을 촬영한 소량의 이미지로부터 매우 사실적인 ’디지털 트윈(Digital Twin)’을 생성할 수 있다. 이렇게 생성된 NeRF 환경 내에서 로봇 에이전트를 학습시키면, 합성 데이터와 실제 데이터 간의 차이인 <strong>Sim-to-Real Gap</strong>을 획기적으로 줄일 수 있다. 특히 조명 변화나 복잡한 텍스처가 로봇의 시각 인식 알고리즘에 미치는 영향을 사전에 테스트하거나, 값비싼 로봇 하드웨어 없이도 가상공간에서 강화학습(RL)을 수행하는 데 유용하게 활용된다.25</p>
<h2>5.  한계점과 극복을 위한 발전 (Advanced Variants)</h2>
<p>NeRF는 3차원 표현의 혁명을 일으켰지만, 초기 모델은 로봇 공학의 실시간성과 확장성 요구사항을 충족시키기에는 몇 가지 한계가 있었다. 이를 극복하기 위해 다양한 변형 모델들이 제안되었으며, 이는 5.3장과 5.4장에서 다룰 기술들의 발전 토대가 되었다.</p>
<h3>5.1  렌더링 및 학습 속도의 가속화</h3>
<p>기본 NeRF는 하나의 <span class="math math-inline">800 \times 800</span> 이미지를 렌더링하는 데 수십 초가 소요된다. 이는 로봇의 제어 루프(수십 Hz)에 직접 적용하기 불가능한 속도이다.28</p>
<ul>
<li><strong>FastNeRF / KiloNeRF:</strong> 공간을 작은 격자로 나누고, 각 격자를 담당하는 수천 개의 작은 MLP(Tiny MLP)를 병렬로 실행하여 렌더링 속도를 수백 배 향상시켰다.</li>
<li>Instant-NGP 15: NVIDIA가 제안한 <strong>멀티해상도 해시 인코딩(Multiresolution Hash Encoding)</strong> 기법을 사용하여, 거대한 MLP 대신 작은 MLP와 학습 가능한 해시 테이블을 결합했다. 이를 통해 수 시간 걸리던 학습 시간을 수 초(Seconds) 단위로 단축하고, 실시간 렌더링을 가능하게 했다. 이는 NeRF가 로봇의 실시간 매핑 시스템으로 들어오는 결정적인 계기가 되었다.</li>
</ul>
<h3>5.2  스케일 확장 (City-Scale NeRF)</h3>
<p>기본 NeRF는 하나의 방이나 물체 수준의 제한된 공간(Bounded Scene)에서만 잘 작동했다. 로봇이 도시 전체를 돌아다니기 위해서는 확장성이 필요하다.</p>
<ul>
<li>Block-NeRF 22: 웨이모(Waymo)가 제안한 모델로, 도시 전체를 여러 개의 NeRF 블록으로 나누어 학습하고, 로봇의 위치에 따라 적절한 블록을 동적으로 로딩하여 렌더링하는 방식을 통해 샌프란시스코 전체와 같은 거대 도시 스케일의 복원을 실현했다.</li>
<li>NeRF++ / Mip-NeRF 360 6: 전경(Foreground)과 배경(Background)을 분리하여 모델링하거나, 공간을 비선형적으로 매핑(Contraction)하여 무한한 배경을 유한한 공간 안으로 압축해 넣음으로써, 야외 환경(Unbounded Scene)에서도 고품질 렌더링을 달성했다.</li>
</ul>
<h3>5.3  동적 환경에 대한 적응 (Dynamic Scenes)</h3>
<p>정적 환경을 가정한 NeRF는 움직이는 사람이나 물체가 있는 데이터에서 “고스트(Ghosting)” 아티팩트를 생성한다.</p>
<ul>
<li>D-NeRF / HyperNeRF 31: 시간 <span class="math math-inline">t</span>를 입력 변수로 추가하거나, 정준 공간(Canonical Space)으로의 변형 필드(Deformation Field)를 학습하여 시간이 지남에 따라 움직이는 물체의 형상을 4차원적으로 복원한다. 이는 로봇이 움직이는 사람과 상호작용해야 하는 환경에서 필수적인 기술이다.</li>
</ul>
<h2>6.  결론 및 시사점</h2>
<p>NeRF는 3차원 비전과 그래픽스 분야에 “미분 가능한 연속적 볼륨“이라는 새로운 언어를 제시했다. 이는 단순히 더 예쁜 이미지를 만드는 기술이 아니라, 로봇이 세상을 기억하고 표현하는 방식을 근본적으로 변화시켰다. 이산적인 데이터(픽셀, 포인트)의 한계를 넘어, 신경망 속에 압축된 세상은 로봇에게 더 풍부한 시각적 맥락과 기하학적 이해를 제공한다.</p>
<p>비록 초기 NeRF는 느린 속도와 정적 환경 가정이라는 한계가 있었지만, 해시 인코딩(Instant-NGP)과 같은 가속화 기술과 동적 모델링 기술의 발전으로 빠르게 실용화 단계에 진입하고 있다. 특히 NeRF가 제공하는 밀집된 점유 정보와 미분 가능한 특성은 로봇의 Neural SLAM, 경로 계획, 그리고 조작 지능을 한 단계 도약시키는 핵심 엔진으로 작동하고 있다. NeRF가 촉발한 “뉴럴 필드(Neural Field)“의 흐름은 이어지는 장들에서 다룰 SDF 기반 표현(5.3)과 가우시안 스플래팅(5.4)으로 진화하며, Embodied AI가 물리적 세계를 완벽하게 이해하고 재구성하는 날을 앞당기고 있다.</p>
<table><thead><tr><th><strong>특성</strong></th><th><strong>포인트 클라우드 / 메쉬 (Explicit)</strong></th><th><strong>복셀 그리드 (Discrete Volumetric)</strong></th><th><strong>NeRF (Continuous Implicit)</strong></th></tr></thead><tbody>
<tr><td><strong>표현 방식</strong></td><td>점, 면의 좌표 집합</td><td>3차원 격자의 이산적 값</td><td>신경망 가중치 (<span class="math math-inline">F_{\Theta}</span>)</td></tr>
<tr><td><strong>메모리 효율</strong></td><td>해상도에 따라 선형적 증가</td><td><span class="math math-inline">O(N^3)</span> (매우 비효율적)</td><td><strong>매우 높음</strong> (네트워크 크기에 의존) 1</td></tr>
<tr><td><strong>해상도</strong></td><td>고정됨 (저장된 데이터에 한정)</td><td>격자 크기에 제한됨</td><td><strong>이론적으로 무한</strong> (연속 함수)</td></tr>
<tr><td><strong>미분 가능성</strong></td><td>어려움 (Rasterization 등)</td><td>가능하나 메모리 제약 큼</td><td><strong>완전 미분 가능 (End-to-End 학습)</strong></td></tr>
<tr><td><strong>조명/반사</strong></td><td>텍스처 맵 (제한적)</td><td>제한적</td><td><strong>View-Dependent Radiance (고품질)</strong></td></tr>
<tr><td><strong>렌더링 속도</strong></td><td><strong>빠름 (실시간)</strong></td><td>보통</td><td><strong>느림 (최적화 필요)</strong> 28</td></tr>
<tr><td><strong>로봇 활용</strong></td><td>충돌 감지, 경로 계획 (표준)</td><td>탐색, 점유 지도</td><td><strong>고정밀 시각화, 강건한 SLAM (신기술)</strong></td></tr>
</tbody></table>
<h2>7. 참고 자료</h2>
<ol>
<li>Representing Scenes as Neural Radiance Fields for View Synthesis, https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460392.pdf</li>
<li>Representing Scenes as Neural Radiance Fields for View Synthesis, https://cseweb.ucsd.edu/~ravir/pratul_eccv20.pdf</li>
<li>NeRF View Synthesis - Emergent Mind, https://www.emergentmind.com/topics/neural-radiance-field-nerf-view-synthesis</li>
<li>NeRF: Neural Radiance Field in 3D Vision: A Comprehensive Review, https://arxiv.org/html/2210.00379v6</li>
<li>Representing Scenes as Neural Radiance Fields for View Synthesis, https://www.cs.jhu.edu/~misha/ReadingSeminar/Papers/Mildenhall21.pdf</li>
<li>
<ol>
<li>NeRF: Representing Scenes as Neural Radiance Fields for View …, https://towardsai.net/p/l/1-nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis-the-first-nerf</li>
</ol>
</li>
<li>Representing Scenes as Neural Radiance Fields for View Synthesis, <a href="http://graphics.csie.ncku.edu.tw/2020%20CG/NeRF_present.pdf">http://graphics.csie.ncku.edu.tw/2020%20CG/NeRF_present.pdf</a></li>
<li>Limitations of NERF with pre-trained Vision Features for Few-Shot …, https://www.researchgate.net/publication/392941981_Limitations_of_NERF_with_pre-trained_Vision_Features_for_Few-Shot_3D_Reconstruction</li>
<li>Vision-Only Robot Navigation in a Neural Radiance World - arXiv, https://arxiv.org/pdf/2110.00168</li>
<li>CATNIPS: Collision Avoidance Through Neural Implicit Probabilistic …, https://msl.stanford.edu/papers/chen_catnips_2023.pdf</li>
<li>NeRF Revisited: Fixing Quadrature Instability in Volume Rendering, https://proceedings.neurips.cc/paper_files/paper/2023/file/5301c49207917c5c870131959971851c-Paper-Conference.pdf</li>
<li>NeRF and follow-up works - César Díaz Blanco, https://blancocd.com/notes/NeRF.pdf</li>
<li>NeRF: A Volume Rendering Perspective | Yue Yu, https://yconquesty.github.io/blog/ml/nerf/nerf_rendering.html</li>
<li>NeRF Revisited: Fixing Quadrature Instability in Volume Rendering, https://arxiv.org/html/2310.20685v2</li>
<li>Adaptive Wavelet-Positional Encoding for High-Frequency …, https://ojs.aaai.org/index.php/AAAI/article/view/33132/35287</li>
<li>Learning High-Frequency Functions Made Easy with Sinusoidal …, https://www.researchgate.net/publication/382251274_Learning_High-Frequency_Functions_Made_Easy_with_Sinusoidal_Positional_Encoding</li>
<li>ankurhanda/nerf2D: Adding positional encoding to the … - GitHub, https://github.com/ankurhanda/nerf2D</li>
<li>Neural Radiance Fields (NeRF): A Complete Tutorial - Medium, https://medium.com/@kdwaMachineLearning/neural-radiance-fields-nerf-a-complete-tutorial-b813e3ed4461</li>
<li>Neural Radiance Fields (NERFs) with Pytorch Part 2 - Kaggle, https://www.kaggle.com/code/yousefalbasel/neural-radiance-fields-nerfs-with-pytorch-part-2</li>
<li>The Annotated NeRF Training NeRF on Custom Dataset in Pytorch, https://learnopencv.com/annotated-nerf-pytorch/</li>
<li>NeRFs in Robotics: A Survey - arXiv, https://arxiv.org/html/2405.01333v2</li>
<li>Advancement in Neural Radiance Fields Propulsion for SLAM, https://www.atlantis-press.com/article/126003439.pdf</li>
<li>3D-Vision-World/awesome-NeRF-and-3DGS-SLAM - GitHub, https://github.com/3D-Vision-World/awesome-NeRF-and-3DGS-SLAM</li>
<li>Vision-Only Robot Navigation in a Neural Radiance World, https://pculbertson.github.io/assets/pdf/adamkiewicz2021.pdf</li>
<li>A Survey of Embodied World Models - FIB-LAB, https://fi.ee.tsinghua.edu.cn/public/publications/0940dda4-af15-11f0-9d60-0242ac120002.pdf</li>
<li>A Survey of Embodied AI: From Simulators to Research Tasks - A*OAR, https://oar.a-star.edu.sg/storage/o/o1v7q15k6q/a-survey-of-embodied-ai-from-simulators-to-research-tasks.pdf</li>
<li>Implicit Neural Representations —— from SDF to NeRF - Leheng Li, https://len-li.github.io/assets/pdf/nerf_ad_thu.pdf</li>
<li>EfficientNeRF Efficient Neural Radiance Fields - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Hu_EfficientNeRF__Efficient_Neural_Radiance_Fields_CVPR_2022_paper.pdf</li>
<li>FastNeRF: High-Fidelity Neural Rendering at 200FPS, https://3dvar.com/Garbin2021FastNeRF.pdf</li>
<li>Local Positional Encoding for Multi-Layer Perceptrons, https://gpuopen.com/download/paper1117_CRC.pdf</li>
<li>Neural Deformable Voxel Grid for Fast Optimization of Dynamic …, https://npucvr.github.io/NDVG/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>