<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:5.2.4 초기 NeRF의 한계와 고속화 연구 (Instant-NGP 등)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>5.2.4 초기 NeRF의 한계와 고속화 연구 (Instant-NGP 등)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 5. 뉴럴 3D 표현과 렌더링 (Neural 3D Representations)</a> / <a href="index.html">5.2 NeRF 혁명: 신경망 복사장 (Neural Radiance Fields)</a> / <span>5.2.4 초기 NeRF의 한계와 고속화 연구 (Instant-NGP 등)</span></nav>
                </div>
            </header>
            <article>
                <h1>5.2.4 초기 NeRF의 한계와 고속화 연구 (Instant-NGP 등)</h1>
<h2>1.  서론: 뉴럴 렌더링의 패러다임 시프트와 계산적 병목</h2>
<p>2020년 Mildenhall 등이 제안한 신경 방사형 필드(Neural Radiance Fields, NeRF)는 3차원 컴퓨터 비전 및 그래픽스 학계에 전례 없는 파장을 일으켰다. NeRF는 복잡한 3D 장면을 메쉬(Mesh)나 포인트 클라우드(Point Cloud)와 같은 이산적인(Discrete) 기하학적 형태로 저장하는 대신, 다층 퍼셉트론(Multilayer Perceptron, MLP)이라는 연속적인 함수로 매개변수화하여 표현하는 혁신적인 접근 방식을 제시했다.1 이러한 암시적 신경 표현(Implicit Neural Representation)은 기존의 명시적 표현 방식이 필연적으로 수반했던 해상도의 한계와 양자화 오차(Quantization Error)를 근본적으로 극복할 수 있게 해주었으며, 시점 의존적(View-dependent)인 반사 효과와 미세한 기하학적 세부 사항을 사실적으로 복원하는 데 성공했다.3</p>
<p>그러나 이러한 고품질의 렌더링 결과는 막대한 계산 비용을 대가로 요구했다. 초기 NeRF 모델, 흔히 ’Vanilla NeRF’라 불리는 아키텍처는 단일 장면을 학습시키는 데 고성능 GPU(예: NVIDIA V100) 환경에서도 약 1~2일이 소요되는 극심한 훈련 시간을 보였다.1 더욱이, 학습이 완료된 모델을 이용하여 새로운 시점의 이미지를 합성하는 추론(Inference) 단계에서도 프레임당 수십 초에서 1분 가까이 소요되어, 실시간 상호작용이 필수적인 가상현실(VR), 증강현실(AR), 로보틱스 등의 분야에 즉각적으로 적용하기에는 명백한 한계가 존재했다.5</p>
<p>본 장에서는 초기 NeRF가 가질 수밖에 없었던 구조적, 계산적 병목 현상을 심층적으로 분석하고, 이를 극복하기 위해 등장한 다양한 고속화 연구들을 체계적으로 고찰한다. 특히, 신경망의 크기를 줄이거나 제거하고 하이브리드 자료 구조를 도입하여 학습 및 렌더링 속도를 획기적으로 개선한 **Instant-NGP (Instant Neural Graphics Primitives)**를 중심으로, <strong>Plenoxels</strong>, <strong>DVGO</strong>, <strong>TensoRF</strong>와 같은 주요 기법들의 수학적 원리와 성능 특성을 상세히 논의한다.</p>
<h2>2.  초기 NeRF의 구조적 한계와 계산 비용 분석</h2>
<p>NeRF의 고속화 연구를 온전히 이해하기 위해서는 먼저 NeRF의 렌더링 파이프라인이 왜 그토록 많은 연산을 필요로 하는지에 대한 정량적 분석이 선행되어야 한다. NeRF의 핵심은 <strong>볼륨 렌더링(Volume Rendering)</strong> 방정식의 미분 가능한 구현에 있다.</p>
<h3>2.1  좌표 기반 MLP 쿼리의 비효율성</h3>
<p>NeRF는 3차원 공간상의 위치 <span class="math math-inline">\mathbf{x} = (x, y, z)</span>와 관찰 방향 <span class="math math-inline">\mathbf{d} = (\theta, \phi)</span>를 입력으로 받아, 해당 지점의 체적 밀도(Volume Density) <span class="math math-inline">\sigma</span>와 시점 의존적 색상 <span class="math math-inline">\mathbf{c} = (r, g, b)</span>를 출력하는 함수 <span class="math math-inline">F_{\Theta}</span>를 학습한다.7<br />
<span class="math math-display">
[\sigma, \mathbf{c}] = F_{\Theta}(\gamma(\mathbf{x}), \gamma(\mathbf{d}))
</span><br />
여기서 <span class="math math-inline">\gamma(\cdot)</span>는 고주파 세부 정보를 학습하기 위한 위치 인코딩(Positional Encoding) 함수이다. 하나의 픽셀 색상 <span class="math math-inline">\hat{C}(\mathbf{r})</span>을 결정하기 위해 NeRF는 카메라 원점에서 픽셀을 통과하는 광선(Ray) <span class="math math-inline">\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}</span>를 따라 수백 개의 샘플 점을 추출하고, 각 점에 대해 MLP를 조회(Query)한다.<br />
<span class="math math-display">
\hat{C}(\mathbf{r}) = \sum_{i=1}^{N} T_i (1 - \exp(-\sigma_i \delta_i)) \mathbf{c}_i, \quad T_i = \exp\left(-\sum_{j=1}^{i-1} \sigma_j \delta_j\right)
</span><br />
초기 NeRF 설정에 따르면, 하나의 광선당 Coarse 단계에서 64개, Fine 단계에서 128개, 총 192개의 샘플링이 수행된다.1 800x800 해상도의 이미지를 렌더링한다고 가정할 때, 총 픽셀 수는 64만 개이며, 이는 단 한 프레임을 생성하기 위해 약 <span class="math math-inline">1.2 \times 10^8</span> (1억 2천만) 번의 MLP 연산이 필요함을 의미한다. MLP가 8개의 계층(Layer)과 256개의 은닉 유닛(Hidden Unit)으로 구성되어 있음을 감안하면, 이는 실시간 처리가 불가능한 수준의 연산량(FLOPs)으로 귀결된다.9</p>
<h3>2.2  빈 공간 샘플링과 메모리 대역폭 문제</h3>
<p>더 근본적인 문제는 샘플링의 효율성이다. 실제 3D 장면의 대부분은 물체가 존재하지 않는 ’빈 공간(Empty Space)’이다. 그러나 초기 NeRF는 장면의 기하학적 구조를 사전에 알지 못하므로, 광선을 따라 균일하게 샘플링을 수행하거나 단순한 확률적 분포에 의존해야 했다. 이는 밀도가 0에 수렴하는 허공에 대해 무거운 MLP 연산을 반복적으로 수행하는 막대한 자원 낭비를 초래했다.10 비록 계층적 샘플링(Hierarchical Sampling) 기법이 도입되어 밀도가 높은 영역에 샘플을 집중시키려 했으나, 이를 위해서는 여전히 전체 공간에 대한 대략적인 밀도 추정(Coarse Network 평가)이 선행되어야 하므로 근본적인 해결책이 되지 못했다.12</p>
<p>또한, 딥러닝 모델의 학습 과정에서 발생하는 메모리 접근 패턴 역시 병목의 원인이 된다. 모든 샘플 포인트에 대해 위치 인코딩을 계산하고, MLP의 각 계층을 통과하며 발생하는 중간 활성화 값(Activation)들을 GPU 메모리에 저장하고 불러오는 과정(Memory Traffic)은 연산 속도보다 더 큰 지연을 유발하기도 한다. 특히 고해상도 이미지를 배치(Batch) 단위로 학습할 때 이러한 메모리 대역폭 문제는 심화된다.9</p>
<h2>3.  명시적 자료 구조와 하이브리드 접근법의 대두</h2>
<p>NeRF의 느린 속도를 개선하기 위해 연구자들은 순수 MLP 의존성을 탈피하고, 컴퓨터 그래픽스에서 오랫동안 사용되어 온 <strong>명시적(Explicit)</strong> 자료 구조를 도입하기 시작했다. 이는 공간 정보를 신경망의 가중치(Weight) 속에 암묵적으로 숨기는 대신, 복셀 그리드(Voxel Grid), 옥트리(Octree), 또는 해시 테이블(Hash Table)과 같은 구조에 직접 저장하여 접근 속도를 높이는 전략이다.15</p>
<h3>3.1  Plenoxels: 신경망 없는 방사형 필드</h3>
<p>Plenoxels(Plenoptic Voxels)는 “NeRF의 고품질 렌더링이 정말 신경망 덕분인가?“라는 근원적인 질문을 던졌다. Yu 등(2021)은 신경망을 완전히 제거하고, **희소 복셀 그리드(Sparse Voxel Grid)**와 **구면 조화 함수(Spherical Harmonics, SH)**만으로도 NeRF와 동등한 품질을 낼 수 있음을 입증했다.4</p>
<ul>
<li><strong>작동 원리:</strong> Plenoxels는 3D 공간을 희소 복셀 그리드로 이산화(Discretization)한다. 각 복셀의 꼭지점에는 불투명도(Opacity)와 시점 의존적 색상을 표현하기 위한 구면 조화 함수(SH)의 계수(Coefficients)들이 저장된다.18 임의의 위치에 대한 값은 인접한 8개 복셀 값의 삼선형 보간(Trilinear Interpolation)을 통해 계산된다.</li>
<li><strong>최적화:</strong> 신경망 학습 대신, 렌더링된 이미지와 실제 이미지 간의 오차(MSE)를 줄이는 방향으로 복셀 격자의 계수들을 직접 최적화한다. 이 과정은 미분 가능한 렌더링 파이프라인을 통해 수행되며, Total Variation (TV) 정규화를 통해 인접 복셀 간의 연속성을 보장한다.4</li>
<li><strong>성능:</strong> 신경망 연산이 제거됨으로써 학습 속도는 기존 NeRF 대비 2자리 수(Orders of magnitude) 이상 빨라졌다. 예를 들어, 일반적인 360도 장면을 학습하는 데 약 11분 정도가 소요된다.4 이는 NeRF의 성공 요인이 딥러닝 모델 그 자체가 아니라, ’미분 가능한 볼륨 렌더링’이라는 최적화 프레임워크에 있었음을 시사하는 중요한 발견이다.</li>
</ul>
<h3>3.2  DVGO: 직접 복셀 그리드 최적화 (Direct Voxel Grid Optimization)</h3>
<p>Sun 등(2022)이 제안한 DVGO는 구현의 단순함과 수렴 속도에 초점을 맞춘 하이브리드 접근법이다.16 DVGO는 복잡한 희소(Sparse) 자료 구조 대신 **밀집 복셀 그리드(Dense Voxel Grid)**를 사용하되, 효율적인 관리 기법을 도입했다.</p>
<ul>
<li><strong>구조:</strong> DVGO는 밀도(Density)와 색상(Color)을 분리하여 관리한다. 밀도는 고해상도 밀집 그리드에 직접 저장하고, 색상은 특징 그리드(Feature Grid)에 저장된 특징 벡터를 얕은 MLP(Shallow MLP)에 통과시켜 얻는다.16</li>
<li><strong>Post-Activation Interpolation:</strong> 기존 방식이 그리드 값을 보간한 후 활성화 함수를 적용하는 것과 달리, DVGO는 활성화 함수(예: Softplus)를 통과한 후 보간을 수행하는 방식을 제안하여 저해상도 그리드에서도 선명한 경계면(Sharp Boundary)을 표현할 수 있게 했다.19</li>
<li><strong>Coarse-to-Fine 전략:</strong> 학습 초기에는 저해상도 그리드에서 시작하여 점진적으로 해상도를 높이는 방식을 사용하거나, ’Known Free Space’를 빠르게 스킵하는 효율적인 광선 마칭(Ray Marching) 알고리즘을 적용하여 약 15~20분 내에 학습을 완료한다.12</li>
</ul>
<h2>4.  텐서 분해를 통한 메모리 효율화: TensoRF</h2>
<p>Plenoxels나 DVGO와 같은 그리드 기반 방식은 빠른 속도를 제공하지만, 고해상도(<span class="math math-inline">512^3</span> 이상) 그리드를 메모리에 유지해야 하므로 메모리 소모량이 크다는 단점이 있다(<span class="math math-inline">O(N^3)</span> 복잡도). **TensoRF (Tensorial Radiance Fields)**는 이러한 문제를 해결하기 위해 <strong>텐서 분해(Tensor Decomposition)</strong> 기법을 3D 복원 문제에 도입했다.21</p>
<h3>4.1  3D 장면의 텐서 표현</h3>
<p>TensoRF는 3D 공간을 4차원 텐서(3차원 공간 + 특징 채널)로 간주한다. 고차원 텐서가 종종 저랭크(Low-rank) 성분들의 합으로 근사될 수 있다는 수학적 성질을 이용하여, 거대한 3D 그리드를 작은 벡터와 행렬들의 곱으로 분해하여 저장한다.21</p>
<h3>4.2  CP 분해와 VM 분해</h3>
<p>TensoRF는 두 가지 주요 분해 방식을 비교 제안한다:</p>
<ol>
<li>CP 분해 (CANDECOMP/PARAFAC Decomposition):</li>
</ol>
<p>3D 텐서 <span class="math math-inline">\mathcal{T}</span>를 각 축(x, y, z)에 대응하는 세 벡터의 외적(Outer Product)의 합으로 표현한다.22<br />
<span class="math math-display">
   \mathcal{T} \approx \sum_{r=1}^{R} \mathbf{v}_r^x \circ \mathbf{v}_r^y \circ \mathbf{v}_r^z
</span><br />
이 방식은 메모리 복잡도를 <span class="math math-inline">O(N^3)</span>에서 <span class="math math-inline">O(N)</span>으로 획기적으로 줄여주며, 모델 크기를 4MB 이하로 압축할 수 있게 한다.24 그러나 복잡한 고주파 텍스처를 표현하기 위해서는 매우 많은 수의 성분(<span class="math math-inline">R</span>)이 필요할 수 있다.</p>
<ol start="2">
<li>VM 분해 (Vector-Matrix Decomposition):</li>
</ol>
<p>TensoRF의 저자들이 제안한 독창적인 방식으로, 3D 텐서를 ’벡터와 행렬(평면)의 외적’의 합으로 분해한다.<br />
<span class="math math-display">
   \mathcal{T} \approx \sum_{r=1}^{R_1} \mathbf{v}_r^{z} \circ \mathbf{M}_r^{xy} + \sum_{r=1}^{R_2} \mathbf{v}_r^{y} \circ \mathbf{M}_r^{xz} + \sum_{r=1}^{R_3} \mathbf{v}_r^{x} \circ \mathbf{M}_r^{yz}
</span><br />
이는 3D 공간을 xy, xz, yz 평면에 투영된 이미지들과 각 축 방향의 벡터로 표현하는 것과 유사하다. VM 분해는 메모리 복잡도를 <span class="math math-inline">O(N^2)</span>로 유지하면서도 CP 분해보다 훨씬 높은 표현력을 제공하여, 적은 메모리로도 최고의 렌더링 품질을 달성할 수 있다.21 TensoRF는 약 10~30분의 학습 시간으로 NeRF 이상의 품질과 Plenoxels 이상의 메모리 효율성을 동시에 달성했다.12</p>
<h2>5.  Instant-NGP: 다해상도 해시 인코딩의 혁신</h2>
<p>2022년 NVIDIA 연구진이 발표한 <strong>Instant-NGP</strong>는 NeRF 고속화 연구의 정점(Apex)으로 평가받는다. 이 연구는 ‘즉각적인(Instant)’ 학습이라는 타이틀에 걸맞게, 수 초 내에 학습 결과를 가시화하고 수 분 내에 고품질 수렴에 도달하는 압도적인 성능을 보여주었다.26</p>
<h3>5.1  다해상도 해시 인코딩 (Multiresolution Hash Encoding)</h3>
<p>Instant-NGP의 핵심 기여는 입력 좌표를 신경망에 주입하기 전에 처리하는 <strong>다해상도 해시 인코딩</strong> 기법이다. 기존 NeRF가 사용하는 주파수 기반 위치 인코딩(<span class="math math-inline">\sin, \cos</span>)은 고주파 성분을 잘 포착하지만, 전역적인 공간 정보를 효율적으로 인덱싱하지는 못했다. 반면, Instant-NGP는 학습 가능한 파라미터 테이블(Feature Table)을 사용하여 공간 정보를 직접 저장하되, 이를 해시 테이블로 관리하여 메모리 효율을 극대화했다.11</p>
<ol>
<li>공간의 다해상도 격자화:</li>
</ol>
<p>3D 공간을 <span class="math math-inline">L</span>개의 서로 다른 해상도 레벨(Coarse to Fine)을 가진 격자(Grid)로 나눈다. 가장 거친 레벨은 장면 전체를 대략적으로 파악하고, 가장 미세한 레벨은 cm 단위의 세밀한 기하학적 구조를 담당한다.</p>
<ol start="2">
<li>공간 해싱 (Spatial Hashing):</li>
</ol>
<p>고해상도 격자의 경우, 모든 정점(Vertex)을 저장하려면 막대한 메모리가 필요하다. Instant-NGP는 이를 해결하기 위해 공간 좌표 <span class="math math-inline">\mathbf{x} = (x, y, z)</span>를 고정된 크기 <span class="math math-inline">T</span>의 해시 테이블 인덱스로 매핑한다.11<br />
<span class="math math-display">
   h(\mathbf{x}) = \left( \bigoplus_{i=1}^{d} x_i \pi_i \right) \mod T
</span><br />
여기서 <span class="math math-inline">\oplus</span>는 XOR 연산, <span class="math math-inline">\pi_i</span>는 큰 소수(Prime number)이다. 이를 통해 무한히 넓은 공간이나 매우 미세한 해상도의 좌표도 유한한 메모리 공간(L2 캐시에 들어갈 수 있는 크기)에 매핑할 수 있다.</p>
<ol start="3">
<li>해시 충돌(Collision)의 신경망적 해결:</li>
</ol>
<p>해시 함수의 특성상 서로 다른 공간 좌표가 동일한 테이블 인덱스를 가리키는 충돌이 필연적으로 발생한다. 전통적인 자료구조론에서는 이를 치명적인 문제로 보지만, Instant-NGP는 신경망의 적응 능력을 이용해 이를 해결한다. 여러 해상도의 그리드를 동시에 사용(Multiresolution)하므로, 특정 레벨에서 충돌이 발생하더라도 다른 레벨에서는 충돌하지 않을 확률이 높다. 뒤따르는 MLP는 모든 레벨의 특징 벡터를 연결(Concatenate)한 입력을 받아, 충돌로 인한 노이즈는 무시하고 유의미한 신호(Signal)만을 학습하도록 최적화된다.27</p>
<h3>5.2  Tiny CUDA NN과 하드웨어 최적화</h3>
<p>NVIDIA는 이 알고리즘을 구현하기 위해 <strong>Tiny CUDA NN</strong>이라는 맞춤형 프레임워크를 개발했다.26 이는 전체 MLP가 GPU의 L2 캐시 안에 완전히 들어갈 정도로 작은 크기(Small MLPs)를 유지하면서도, 완전히 융합된(Fully Fused) CUDA 커널을 사용하여 메모리 대역폭 병목을 제거했다. 또한 반정밀도(Half-precision, FP16) 연산과 텐서 코어(Tensor Core)를 적극 활용하여 산술 연산 처리량(Arithmetic Throughput)을 극한으로 끌어올렸다. 그 결과, Instant-NGP는 학습 시작 5초 만에 객체의 형태를 복원하고, 1분 이내에 고화질 렌더링이 가능한 수준에 도달하는 경이로운 속도를 달성했다.12</p>
<h2>6.  종합 비교 분석 및 성능 벤치마크</h2>
<p>다음은 앞서 논의한 주요 고속화 알고리즘들의 성능 특성을 종합적으로 비교한 것이다. 이 데이터는 Synthetic NeRF 데이터셋 및 실제 데이터셋에 대한 벤치마크 결과를 기반으로 재구성되었다.12</p>
<table><thead><tr><th><strong>특성 비교 항목</strong></th><th><strong>Vanilla NeRF</strong></th><th><strong>Plenoxels</strong></th><th><strong>DVGO</strong></th><th><strong>TensoRF (VM)</strong></th><th><strong>Instant-NGP</strong></th></tr></thead><tbody>
<tr><td><strong>표현 방식</strong></td><td>Implicit (Pure MLP)</td><td>Explicit (Sparse Grid)</td><td>Hybrid (Dense Grid)</td><td>Hybrid (Tensor Decomp.)</td><td>Hybrid (Hash Grid)</td></tr>
<tr><td><strong>자료 구조</strong></td><td>Weights only</td><td>Voxel Grid + SH</td><td>Voxel + Feature Grid</td><td>Vector-Matrix Factors</td><td>Hash Table + MLP</td></tr>
<tr><td><strong>학습 시간</strong></td><td>10~20 시간</td><td>11~15 분</td><td>15~20 분</td><td>15~30 분</td><td><strong>&lt; 1~5 분</strong></td></tr>
<tr><td><strong>렌더링 FPS</strong></td><td>&lt; 0.1 FPS</td><td>&gt; 15 FPS</td><td>&gt; 10 FPS</td><td>&gt; 10 FPS</td><td><strong>&gt; 20~60 FPS</strong></td></tr>
<tr><td><strong>메모리(VRAM)</strong></td><td><strong>매우 낮음 (~5MB)</strong></td><td>높음 (&gt;500MB)</td><td>높음 (&gt;500MB)</td><td><strong>매우 낮음 (&lt;4MB)</strong></td><td>낮음 (~50MB)</td></tr>
<tr><td><strong>품질 (PSNR)</strong></td><td>31.0 dB (Baseline)</td><td>31.7 dB</td><td>31.9 dB</td><td><strong>33.1 dB</strong></td><td>32.5 dB</td></tr>
</tbody></table>
<p><strong>분석적 통찰:</strong></p>
<ol>
<li><strong>속도의 절대 강자, Instant-NGP:</strong> 학습과 렌더링 모든 측면에서 가장 빠른 속도를 제공한다. 특히 해시 인코딩 덕분에 제한된 메모리 내에서 무제한(Unbounded) 장면을 효과적으로 표현할 수 있어, SLAM이나 대규모 장면 복원에 가장 적합하다.12</li>
<li><strong>메모리와 품질의 균형, TensoRF:</strong> 모바일 기기나 엣지 디바이스와 같이 저장 공간이 중요한 환경에서는 TensoRF가 가장 우수한 선택지다. VM 분해는 적은 용량으로도 NeRF보다 높은 품질(PSNR)을 보여준다.21</li>
<li><strong>단순성의 가치, Plenoxels/DVGO:</strong> 복잡한 CUDA 커널 최적화 없이 파이토치(PyTorch) 기본 연산만으로도 구현 가능하며, 연구 및 교육 목적으로 높은 가치를 지닌다.29</li>
</ol>
<h2>7.  로보틱스 및 실시간 응용에의 파급 효과</h2>
<p>초기 NeRF의 느린 속도는 로보틱스 분야, 특히 SLAM(Simultaneous Localization and Mapping) 적용에 있어 결정적인 장애물이었다. 로봇이 미지의 환경을 탐색하며 실시간으로 자신의 위치를 추정하고 지도를 작성하기 위해서는 센서 데이터가 입력되는 즉시 맵을 업데이트할 수 있어야 하기 때문이다.</p>
<p>Instant-NGP와 같은 고속화 기술의 등장은 <strong>NeRF-SLAM</strong>이라는 새로운 연구 분야를 개척했다.</p>
<ul>
<li><strong>iMAP</strong>과 <strong>NICE-SLAM</strong>은 NeRF 기반 SLAM의 초기 시도로, MLP를 사용하여 환경을 표현했으나, 키프레임 선정과 네트워크 업데이트 속도 문제로 인해 제한된 공간(Room-scale)에서만 작동하거나 실시간성을 보장하기 어려웠다.30</li>
<li><strong>Instant-NGP 기반 SLAM:</strong> 최근 연구들은 Instant-NGP의 초고속 학습 능력을 활용하여, 로봇이 이동함에 따라 실시간으로 해시 그리드를 업데이트하고 즉각적으로 고해상도 3D 지도를 생성하는 시스템(예: Orbeez-SLAM, NGP-SLAM)을 제안하고 있다.6 이는 로봇이 단순히 장애물 회피를 위한 점유 지도(Occupancy Grid)를 넘어서, 텍스처와 조명 정보가 포함된 사진처럼 생생한(Photorealistic) 지도를 실시간으로 작성할 수 있음을 의미한다.</li>
</ul>
<p>이러한 기술적 진보는 자율 주행, 가사 로봇, 드론 내비게이션 등에서 로봇의 인지 능력을 획기적으로 향상시킬 수 있는 잠재력을 지니고 있다. 로봇은 이제 자신이 ‘본’ 것을 기억하고, 상상(Novel View Synthesis)하며, 복잡한 시맨틱 정보를 포함한 공간 지능(Spatial AI)을 갖추는 단계로 나아가고 있다.32</p>
<h2>8. 결론: 암시적 표현과 명시적 표현의 융합</h2>
<p>제5장 2절 4항에서 살펴본 NeRF의 고속화 연구 흐름은 ’암시적 표현(Implicit)’과 ’명시적 표현(Explicit)’이라는 컴퓨터 그래픽스의 두 가지 거대한 줄기가 융합되는 과정을 보여준다. 초기 NeRF가 완전한 암시적 표현을 통해 품질의 혁신을 이뤘다면, Instant-NGP, Plenoxels, TensoRF는 명시적 자료 구조의 효율성을 다시 도입하여 실용성(Practicality)을 확보했다.</p>
<p>특히 Instant-NGP의 성공은 하드웨어(GPU 캐시 구조)와 소프트웨어(해시 인코딩 알고리즘)의 긴밀한 통합(Co-design)이 가져올 수 있는 성능 향상의 극단을 보여주었다. 이러한 하이브리드 구조는 향후 3D Gaussian Splatting과 같은 차세대 명시적 렌더링 기술의 등장에도 영감을 주었으며34, 정적인 장면을 넘어 동적인(Dynamic) 장면, 대규모 도시 환경, 그리고 생성형 AI(Generative AI)와의 결합으로 나아가는 기술적 토대가 되고 있다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>EfficientNeRF Efficient Neural Radiance Fields - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Hu_EfficientNeRF__Efficient_Neural_Radiance_Fields_CVPR_2022_paper.pdf</li>
<li>How Neural Radiance Fields (NeRF) and Instant Neural Graphics Primitives work, https://theaisummer.com/nerf/</li>
<li>SLAM Meets NeRF: A Survey of Implicit SLAM Methods - MDPI, https://www.mdpi.com/2032-6653/15/3/85</li>
<li>Plenoxels: Radiance Fields Without Neural Networks - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Fridovich-Keil_Plenoxels_Radiance_Fields_Without_Neural_Networks_CVPR_2022_paper.pdf</li>
<li>FastNeRF: high-fidelity neural rendering at 200fps [extended] MICROSOFT - Reddit, https://www.reddit.com/r/AR_MR_XR/comments/qkkpdr/fastnerf_highfidelity_neural_rendering_at_200fps/</li>
<li>A Real-time Monocular Visual SLAM with ORB Features and NeRF-realized Mapping - IEEE Xplore, https://ieeexplore.ieee.org/iel7/10160211/10160212/10160950.pdf</li>
<li>NeRF on Device - Stanford University, https://web.stanford.edu/class/cs231a/prev_projects_2022/final_project__2_.pdf</li>
<li>Neural Radiance Fields from Scratch (Part II) — From Theory to Practice | by Luc Frachon, https://medium.com/@luc.frachon/neural-radiance-fields-from-scratch-part-ii-from-theory-to-practice-66a622e28998</li>
<li>FastNeRF: High-Fidelity Neural Rendering at 200FPS, https://3dvar.com/Garbin2021FastNeRF.pdf</li>
<li>Exploring Neural Radiance Fields for 3D Scene Synthesis - Viso Suite, https://viso.ai/deep-learning/neural-radiance-fields/</li>
<li>[R] Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (Training a NeRF takes 5 seconds!) - Reddit, https://www.reddit.com/r/MachineLearning/comments/s5grvj/r_instant_neural_graphics_primitives_with_a/</li>
<li>NeRF: Neural Radiance Field in 3D Vision: A Comprehensive Review - arXiv, https://arxiv.org/html/2210.00379v6</li>
<li>[2106.05264] NeRF in detail: Learning to sample for view synthesis - arXiv, https://arxiv.org/abs/2106.05264</li>
<li>Rendered using instant NGP, with training time of 5 minutes with 72 pics, GTX 1650ti : r/photogrammetry - Reddit, https://www.reddit.com/r/photogrammetry/comments/wrbxyp/rendered_using_instant_ngp_with_training_time_of/</li>
<li>Neural Radiance Fields for the Real World: A Survey - arXiv, https://arxiv.org/html/2501.13104v1</li>
<li>DVGO - Direct Voxel Grid Optimization, https://sunset1995.github.io/dvgo/</li>
<li>Plenoxels: Radiance Fields without Neural Networks - IEEE Xplore, https://ieeexplore.ieee.org/document/9880358/</li>
<li>Plenoxels: Radiance Fields without Neural Networks - Alex Yu, https://alexyu.net/plenoxels/</li>
<li>[2111.11215] Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction - arXiv, https://arxiv.org/abs/2111.11215</li>
<li>Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction | Request PDF - ResearchGate, https://www.researchgate.net/publication/363908652_Direct_Voxel_Grid_Optimization_Super-fast_Convergence_for_Radiance_Fields_Reconstruction</li>
<li>TensoRF: Tensorial Radiance Fields - Anpei Chen, https://apchenstu.github.io/TensoRF/</li>
<li>Tensor rank decomposition - Wikipedia, https://en.wikipedia.org/wiki/Tensor_rank_decomposition</li>
<li>Understanding Tensors and Tensor Decompositions: Part 2 - Kevadiya NEW, https://www.kevadiya.com/post/understanding-tensors-and-tensor-decompositions-part-2</li>
<li>How Far can we Compress Instant-NGP-Based NeRF? | Request PDF - ResearchGate, https://www.researchgate.net/publication/384437226_How_Far_can_we_Compress_Instant-NGP-Based_NeRF</li>
<li>TensoRF - nerfstudio, https://docs.nerf.studio/nerfology/methods/tensorf.html</li>
<li>NVlabs/instant-ngp: Instant neural graphics primitives: lightning fast NeRF and more, https://github.com/NVlabs/instant-ngp</li>
<li>Instant Neural Graphics Primitives with a Multiresolution Hash Encoding - NVlabs, https://nvlabs.github.io/instant-ngp/</li>
<li>PlenVDB: Memory Efficient VDB-Based Radiance Fields for Fast Training and Rendering - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_PlenVDB_Memory_Efficient_VDB-Based_Radiance_Fields_for_Fast_Training_and_CVPR_2023_paper.pdf</li>
<li>Neural radiance fields based on plenoxels - GitHub, https://github.com/nissmar/Radiance-Fields</li>
<li>3D-Vision-World/awesome-NeRF-and-3DGS-SLAM - GitHub, https://github.com/3D-Vision-World/awesome-NeRF-and-3DGS-SLAM</li>
<li>Fit-NGP: Fitting Object Models to Neural Graphics Primitives - arXiv, https://arxiv.org/html/2401.02357v1</li>
<li>Summary of NeRF-based SLAM methods. | Download Scientific Diagram - ResearchGate, https://www.researchgate.net/figure/Summary-of-NeRF-based-SLAM-methods_tbl1_378482614</li>
<li>Vision-Only Robot Navigation in a Neural Radiance World, https://par.nsf.gov/servlets/purl/10419549</li>
<li>CD-NGP: A Fast Scalable Continual Representation for Dynamic Scenes - arXiv, https://arxiv.org/html/2409.05166v1</li>
<li>HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis, https://arxiv.org/html/2509.17083v2</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>