<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:5.2.3 NeRF의 학습 파이프라인과 뷰 합성(View Synthesis)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>5.2.3 NeRF의 학습 파이프라인과 뷰 합성(View Synthesis)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 5. 뉴럴 3D 표현과 렌더링 (Neural 3D Representations)</a> / <a href="index.html">5.2 NeRF 혁명: 신경망 복사장 (Neural Radiance Fields)</a> / <span>5.2.3 NeRF의 학습 파이프라인과 뷰 합성(View Synthesis)</span></nav>
                </div>
            </header>
            <article>
                <h1>5.2.3 NeRF의 학습 파이프라인과 뷰 합성(View Synthesis)</h1>
<p>현대 로보틱스, 특히 Embodied AI 분야에서 시각적 지각(Visual Perception)은 로봇이 물리적 세계와 상호작용하기 위한 가장 핵심적인 감각이다. 2차원 이미지로부터 3차원 공간을 이해하고 재구성하는 능력은 SLAM(Simultaneous Localization and Mapping), 조작(Manipulation), 그리고 내비게이션(Navigation)의 성패를 좌우한다. 이러한 맥락에서 NeRF(Neural Radiance Fields)가 제시한 “미분 가능한 볼륨 렌더링(Differentiable Volume Rendering)” 기반의 학습 파이프라인은 기존의 명시적(Explicit) 3D 표현 방식이 가졌던 한계를 뛰어넘어, 로봇에게 연속적이고 고해상도의 공간 이해 능력을 부여하는 혁명적인 도구로 자리 잡았다. 본 절에서는 희소한 2D 이미지 집합으로부터 밀도 높은 3D 장면을 학습하는 NeRF의 전체 파이프라인을 수학적, 알고리즘적 관점에서 심층적으로 해부하고, 학습된 모델이 어떻게 새로운 시점의 이미지를 합성하여 로봇의 “상상력“을 구현하는지 분석한다.</p>
<h2>1.  학습 파이프라인의 이론적 토대와 구조</h2>
<p>NeRF의 학습 파이프라인은 전통적인 컴퓨터 비전의 3D 재구성 파이프라인과는 근본적으로 다른 철학을 바탕으로 설계되었다. 전통적인 방식이 특징점 추출(Feature Extraction), 매칭(Matching), 그리고 희소 점군(Sparse Point Cloud) 생성의 단계를 거치는 이산적(Discrete) 접근이었다면, NeRF는 장면 전체를 하나의 연속적인 5차원 함수 <span class="math math-inline">F_\Theta: (\mathbf{x}, \mathbf{d}) \to (\mathbf{c}, \sigma)</span>로 정의하고 이를 신경망으로 근사하는 접근을 취한다.1 이 함수는 3차원 위치 <span class="math math-inline">\mathbf{x} = (x, y, z)</span>와 2차원 시선 방향 <span class="math math-inline">\mathbf{d} = (\theta, \phi)</span>를 입력받아, 해당 지점의 색상 <span class="math math-inline">\mathbf{c} = (r, g, b)</span>와 입자의 밀도 <span class="math math-inline">\sigma</span>를 출력한다.2</p>
<p>이 파이프라인의 가장 큰 특징은 <strong>End-to-End 미분 가능성</strong>이다. 입력 이미지의 픽셀값과 렌더링된 픽셀값 사이의 오차를 역전파(Backpropagation)하여, 3D 형상에 대한 직접적인 정답(Ground Truth) 데이터 없이도 오직 2D 이미지만으로 3D 지오메트리와 텍스처를 동시에 최적화할 수 있다.4 로봇 공학적 관점에서 이는 매우 중요한 의미를 갖는데, 고가의 라이다(LiDAR)나 깊이 센서 없이 일반 RGB 카메라만으로도 환경에 대한 정밀한 3D 모델링이 가능하다는 것을 시사하기 때문이다.</p>
<p>전체 학습 파이프라인은 크게 다음과 같은 순환적 단계로 구성된다:</p>
<ol>
<li><strong>광선 생성(Ray Generation):</strong> 카메라 포즈를 기반으로 이미지 픽셀에 대응하는 광선을 3D 공간으로 투사한다.</li>
<li><strong>계층적 샘플링(Hierarchical Sampling):</strong> 광선을 따라 의미 있는 지점들을 효율적으로 추출한다.</li>
<li><strong>뉴럴 쿼리(Neural Query):</strong> 샘플링된 지점의 좌표와 방향을 MLP(Multi-Layer Perceptron)에 입력하여 밀도와 색상을 예측한다.</li>
<li><strong>볼륨 렌더링(Volume Rendering):</strong> 예측된 값들을 적분하여 2D 픽셀 색상을 합성한다.</li>
<li><strong>손실 계산 및 역전파(Loss Computation &amp; Backpropagation):</strong> 합성된 색상과 실제 관측 색상의 차이를 최소화하는 방향으로 네트워크 가중치를 갱신한다.</li>
</ol>
<p>이 과정은 수십만 번의 반복(Iteration)을 통해 수행되며, 결과적으로 신경망 내부에 장면의 전체적인 형상과 조명 정보가 암묵적(Implicit)으로 저장된다.</p>
<h3>1.1 광선 투사(Ray Casting)와 좌표계 변환</h3>
<p>학습의 시작은 물리적인 카메라 모델을 수학적인 광선(Ray)으로 변환하는 것이다. 로봇의 카메라가 수집한 <span class="math math-inline">N</span>장의 학습 이미지 데이터셋이 주어졌을 때, 각 이미지는 카메라의 고유 파라미터(Intrinsic parameters)와 외부 파라미터(Extrinsic parameters)를 포함한다. 일반적으로 NeRF 학습 전 단계에서 COLMAP 6과 같은 Structure-from-Motion (SfM) 알고리즘을 사용하여 각 이미지의 상대적인 포즈(회전 행렬 <span class="math math-inline">\mathbf{R}</span>과 이동 벡터 <span class="math math-inline">\mathbf{t}</span>)를 추정한다.</p>
<p>카메라 좌표계에서 픽셀 <span class="math math-inline">(u, v)</span>를 통과하는 광선의 방향 벡터 <span class="math math-inline">\mathbf{d}_{cam}</span>은 핀홀 카메라 모델을 따라 다음과 같이 정의된다:<br />
<span class="math math-display">
\mathbf{d}_{cam} = \begin{bmatrix} (u - W/2) / f_x \\ -(v - H/2) / f_y \\ -1 \end{bmatrix}
</span><br />
여기서 <span class="math math-inline">W, H</span>는 이미지의 너비와 높이, <span class="math math-inline">f_x, f_y</span>는 초점 거리이다. <span class="math math-inline">-1</span>은 카메라가 바라보는 축(z축)의 음의 방향을 의미한다. 이를 월드 좌표계의 광선 <span class="math math-inline">\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}</span>로 변환하기 위해 카메라의 포즈 <span class="math math-inline">\mathbf{P} =</span>를 적용한다:<br />
<span class="math math-display">
\mathbf{o} = \mathbf{t}, \quad \mathbf{d} = \mathbf{R} \cdot \frac{\mathbf{d}_{cam}}{\|\mathbf{d}_{cam}\|}
</span><br />
이때 <span class="math math-inline">t</span>는 광선의 시작점 <span class="math math-inline">\mathbf{o}</span>로부터의 거리를 나타내는 매개변수이며, 물리적인 깊이(Depth) 범위 <span class="math math-inline">[t_{near}, t_{far}]</span> 사이에서 정의된다. <span class="math math-inline">t_{near}</span>와 <span class="math math-inline">t_{far}</span>는 장면의 스케일에 따라 설정되며, 로봇이 탐색하는 환경의 경계(Bounding Box)를 결정짓는다. 이 광선 생성 과정은 배치(Batch) 단위로 이루어지며, 학습 시에는 메모리 효율성을 위해 전체 이미지가 아닌 무작위로 선택된 광선들의 집합(Ray Batch)을 사용한다.7 일반적인 배치 크기는 4,096개의 광선이다.8</p>
<h2>2.  심층 신경망 아키텍처와 데이터 흐름</h2>
<p>NeRF의 핵심 연산 엔진인 MLP(Multi-Layer Perceptron)는 단순한 분류(Classification)나 회귀(Regression) 모델과는 다른 독특한 구조적 특징을 가진다. 이 네트워크는 장면의 기하학적 구조(Geometry)와 광학적 특성(Appearance)을 분리하여 학습하도록 설계되었다.2 이는 물리적 세계의 성질을 반영한 것으로, 물체의 밀도(형상)는 관찰자가 어디서 보든 변하지 않지만(View-independent), 색상(반사광)은 보는 각도에 따라 달라지기(View-dependent) 때문이다.</p>
<h3>2.1 MLP 구조 상세 분석</h3>
<p>표준적인 NeRF MLP 아키텍처는 다음과 같은 층(Layer) 구조와 데이터 흐름을 가진다.10</p>
<table><thead><tr><th><strong>단계</strong></th><th><strong>입력 차원</strong></th><th><strong>레이어 구성</strong></th><th><strong>활성화 함수</strong></th><th><strong>출력 및 역할</strong></th></tr></thead><tbody>
<tr><td><strong>입력 인코딩</strong></td><td>3 (x, y, z)</td><td>Positional Encoding (<span class="math math-inline">\gamma_\mathbf{x}</span>)</td><td>-</td><td>60 (고주파 위치 정보)</td></tr>
<tr><td><strong>밀도 백본 1</strong></td><td>60</td><td>FC Layer (256 ch) <span class="math math-inline">\times</span> 4</td><td>ReLU</td><td>중간 특징 벡터</td></tr>
<tr><td><strong>스킵 연결</strong></td><td>60 + 256</td><td>Concatenation (Input + Layer 4)</td><td>-</td><td>그래디언트 소실 방지 및 위치 정보 보존</td></tr>
<tr><td><strong>밀도 백본 2</strong></td><td>316</td><td>FC Layer (256 ch) <span class="math math-inline">\times</span> 4</td><td>ReLU</td><td>기하학적 특징 벡터 (256 ch)</td></tr>
<tr><td><strong>밀도 출력</strong></td><td>256</td><td>FC Layer (1 ch)</td><td>ReLU (or Softplus)</td><td><strong>부피 밀도 (<span class="math math-inline">\sigma</span>)</strong></td></tr>
<tr><td><strong>특징 추출</strong></td><td>256</td><td>FC Layer (256 ch)</td><td>-</td><td>형상 특징 벡터</td></tr>
<tr><td><strong>방향 주입</strong></td><td>256 + 24</td><td>Concatenation (Feat + <span class="math math-inline">\gamma_\mathbf{d}</span>)</td><td>-</td><td>뷰 의존성 주입</td></tr>
<tr><td><strong>색상 처리</strong></td><td>280</td><td>FC Layer (128 ch)</td><td>ReLU</td><td>색상 특징 처리</td></tr>
<tr><td><strong>색상 출력</strong></td><td>128</td><td>FC Layer (3 ch)</td><td>Sigmoid</td><td><strong>RGB 색상 (<span class="math math-inline">\mathbf{c}</span>)</strong></td></tr>
</tbody></table>
<h4>2.1.1  밀도 분기(Density Branch)와 스킵 연결</h4>
<p>네트워크의 앞부분 8개 레이어는 오직 위치 <span class="math math-inline">\mathbf{x}</span>만을 입력으로 받는다.10 이는 앞서 언급한 대로 밀도 <span class="math math-inline">\sigma</span>가 시점과 무관한 속성임을 강제하기 위함이다. 특히 4번째 레이어에서 입력 위치 인코딩 벡터를 다시 연결(Skip Connection)하는 구조는 ResNet과 유사한 역할을 수행한다.11 이는 깊은 네트워크에서도 입력 좌표의 고주파 정보가 소실되지 않고 전달되게 하여, 복잡한 3D 형상을 정밀하게 모델링하는 데 기여한다. 출력된 밀도 값 <span class="math math-inline">\sigma</span>는 ReLU를 통과하여 항상 비음수(Non-negative)가 되도록 보장된다.</p>
<h4>2.1.2  뷰 의존성 주입(View Direction Injection)</h4>
<p>8번째 레이어 이후 네트워크는 분기된다. 하나는 밀도를 출력하고, 다른 하나는 256차원의 특징 벡터(Feature Vector)를 생성한다. 이 특징 벡터는 해당 위치의 기하학적 특성을 요약하고 있다. 여기에 시선 방향 <span class="math math-inline">\mathbf{d}</span>의 위치 인코딩 벡터가 결합(Concatenation)된다.2 이 시점이 바로 NeRF가 램버시안(Lambertian) 표면을 넘어 반사(Specular), 금속성 광택 등 복잡한 조명 효과를 표현할 수 있게 되는 지점이다. 방향 정보가 주입된 후 추가적인 FC 레이어를 거쳐 최종 RGB 색상이 출력되며, Sigmoid 함수를 통해 $$ 사이의 값으로 정규화된다.</p>
<p>이러한 아키텍처 설계는 모델이 “형상(Shape)“과 “외관(Appearance)“을 분리하여 학습하게 만든다. 만약 방향 정보가 네트워크의 초반에 주입되었다면, 모델은 시점에 따라 물체의 형상 자체가 변하는 비물리적인 현상을 학습했을 가능성이 높다 (예: 보는 각도에 따라 물체가 사라지거나 나타남). NeRF의 구조는 이러한 기하학적 일관성(Geometric Consistency)을 구조적으로 강제한다.6</p>
<hr />
<h2>5.2.3.3 계층적 볼륨 샘플링 (Hierarchical Volume Sampling)</h2>
<p>NeRF 학습 파이프라인에서 계산 비용이 가장 높은 단계는 광선을 따라 수백 개의 점을 쿼리하는 과정이다. 3D 공간의 대부분은 빈 공간(Empty Space)이거나 물체 내부의 보이지 않는 영역이다. 따라서 광선 위의 모든 지점을 균일하게 샘플링하는 것은 극도로 비효율적이며, 제한된 샘플 수로 복잡한 물체의 경계면을 표현하려 할 때 앨리어싱(Aliasing) 문제를 일으킬 수 있다. NeRF는 이를 해결하기 위해 <strong>Coarse Network</strong>와 <strong>Fine Network</strong>라는 두 개의 네트워크를 동시에 최적화하는 <strong>계층적 샘플링(Hierarchical Volume Sampling)</strong> 전략을 도입한다.5</p>
<h3>1단계: 층화 샘플링(Stratified Sampling)과 Coarse Network</h3>
<p>먼저, 광선의 전체 구간 <span class="math math-inline">[t_{near}, t_{far}]</span>를 <span class="math math-inline">N_c</span>개의 균일한 빈(Bin)으로 나눈다. 그리고 각 빈 내부에서 무작위로 하나의 샘플 <span class="math math-inline">t_i</span>를 추출한다.<br />
<span class="math math-display">
t_i \sim \mathcal{U} \left[ t_{near} + \frac{i-1}{N_c}(t_{far} - t_{near}), \, t_{near} + \frac{i}{N_c}(t_{far} - t_{near}) \right]
</span><br />
이러한 **층화 샘플링(Stratified Sampling)**은 학습 과정에서 샘플 위치에 무작위성(Jitter)을 부여하여, 네트워크가 이산적인 점이 아닌 연속적인 공간 함수를 학습하도록 유도한다.16 추출된 <span class="math math-inline">N_c</span>개의 샘플(Coarse samples)은 <strong>Coarse Network</strong>에 입력되어 밀도와 색상을 예측한다. Coarse Network의 주된 목적은 고품질 이미지를 생성하는 것보다, 공간상의 **밀도 분포(Density Distribution)**를 대략적으로 파악하는 것이다.</p>
<h3>2단계: 확률 밀도 함수(PDF) 구성과 역변환 샘플링</h3>
<p>Coarse Network의 출력값인 밀도 <span class="math math-inline">\sigma_i</span>를 사용하여 광선을 따른 가중치 <span class="math math-inline">w_i</span>를 계산한다.<br />
<span class="math math-display">
w_i = T_i (1 - \exp(-\sigma_i \delta_i))
</span><br />
여기서 <span class="math math-inline">T_i</span>는 투과율(Transmittance)로, 광선이 <span class="math math-inline">i</span>번째 지점까지 가려지지 않고 도달할 확률이다. 가중치 <span class="math math-inline">w_i</span>는 <span class="math math-inline">i</span>번째 샘플이 최종 픽셀 색상에 기여하는 정도를 나타낸다. 이 가중치들을 정규화하면 광선을 따른 **확률 밀도 함수(PDF, Probability Density Function)**를 얻을 수 있다.17<br />
<span class="math math-display">
\hat{w}_i = \frac{w_i}{\sum_{j=1}^{N_c} w_j}
</span><br />
이 PDF는 “물체가 존재하여 광선의 진행을 막고 색상을 방출할 확률이 높은 위치“를 나타낸다. NeRF는 이 분포를 기반으로 **역변환 샘플링(Inverse Transform Sampling)**을 수행하여 <span class="math math-inline">N_f</span>개의 새로운 샘플(Fine samples)을 추출한다.15</p>
<ol>
<li>정규화된 가중치 <span class="math math-inline">\hat{w}_i</span>의 누적 합(Cumulative Sum)을 계산하여 이산적 누적 분포 함수(CDF)를 생성한다.</li>
<li>$$ 구간에서 균일 분포를 따르는 난수들을 생성한다.</li>
<li>생성된 난수를 CDF의 역함수에 대입하여, 가중치가 높은(밀도가 높은) 영역에 집중된 새로운 샘플 위치 <span class="math math-inline">t&#39;_j</span>들을 얻는다.18</li>
</ol>
<h3>2.2 단계: Fine Network와 최종 렌더링</h3>
<p>최종적으로 Coarse 단계의 샘플 <span class="math math-inline">N_c</span>개와 Fine 단계의 샘플 <span class="math math-inline">N_f</span>개를 합집합(Union)으로 결합하고, 깊이 순서대로 정렬한다.15 총 <span class="math math-inline">N_c + N_f</span>개의 샘플이 <strong>Fine Network</strong>에 입력되어 최종적인 밀도와 색상을 계산한다. 이 방식은 빈 공간에 대한 샘플링을 최소화하고 물체 표면 근처에 샘플을 밀집시킴으로써, 동일한 계산 비용으로 훨씬 높은 해상도의 렌더링 결과를 얻을 수 있게 한다. 로봇이 복잡한 환경에서 작은 물체를 인식하거나 얇은 구조물을 파악해야 할 때, 이러한 계층적 샘플링은 필수적인 역할을 수행한다.20</p>
<h2>3.  미분 가능한 볼륨 렌더링 방정식 (Differentiable Volume Rendering)</h2>
<p>NeRF가 3D 장면을 학습할 수 있는 핵심적인 이유는 볼륨 렌더링 과정이 미분 가능(Differentiable)하기 때문이다. 이는 예측된 픽셀 색상과 실제 색상 간의 오차를 신경망의 가중치로 역전파할 수 있음을 의미한다.</p>
<h3>3.1 연속 적분 방정식</h3>
<p>물리학적으로 광선 <span class="math math-inline">\mathbf{r}(t)</span>를 따라 카메라에 도달하는 빛의 색상 <span class="math math-inline">C(\mathbf{r})</span>은 다음과 같은 적분식으로 표현된다 2:<br />
<span class="math math-display">
C(\mathbf{r}) = \int_{t_{near}}^{t_{far}} T(t) \cdot \sigma(\mathbf{r}(t)) \cdot \mathbf{c}(\mathbf{r}(t), \mathbf{d}) \, dt
</span><br />
여기서 <span class="math math-inline">T(t)</span>는 누적 투과율(Accumulated Transmittance)로, <span class="math math-inline">t_{near}</span>부터 <span class="math math-inline">t</span>까지 광선이 어떤 입자와도 부딪히지 않고 통과할 확률을 나타낸다.<br />
<span class="math math-display">
T(t) = \exp \left( - \int_{t_{near}}^{t} \sigma(\mathbf{r}(s)) \, ds \right)
</span><br />
이 식은 밀도 <span class="math math-inline">\sigma</span>가 높은 영역을 지날수록 투과율 <span class="math math-inline">T(t)</span>가 급격히 0으로 수렴함을 보여준다. 즉, 앞에 있는 불투명한 물체가 뒤에 있는 물체를 가리는 <strong>폐색(Occlusion)</strong> 현상을 수학적으로 모델링한 것이다.</p>
<h3>3.2 이산화(Discretization)와 수치적분</h3>
<p>컴퓨터에서의 처리를 위해 이 연속 적분은 구적법(Quadrature)을 사용하여 이산화된다. 샘플링된 점들 사이의 구간에서 밀도와 색상이 일정하다고 가정하면, 적분식은 다음과 같은 합(Summation)으로 변환된다 10:<br />
<span class="math math-display">
\hat{C}(\mathbf{r}) = \sum_{i=1}^{N} T_i \cdot \alpha_i \cdot \mathbf{c}_i
</span><br />
여기서 <span class="math math-inline">\alpha_i</span>는 <span class="math math-inline">i</span>번째 구간에서의 불투명도(Opacity) 또는 알파 값이다.<br />
<span class="math math-display">
\alpha_i = 1 - \exp(-\sigma_i \delta_i)
</span></p>
<p><span class="math math-display">
T_i = \prod_{j=1}^{i-1} (1 - \alpha_j)
</span></p>
<p><span class="math math-inline">\delta_i = t_{i+1} - t_i</span>는 인접한 샘플 사이의 거리이다. 이 이산화된 식은 컴퓨터 그래픽스에서의 **알파 컴포지팅(Alpha Compositing)**과 정확히 일치한다.22 중요한 점은 <span class="math math-inline">\exp</span> 함수와 곱셈, 덧셈 연산 모두가 미분 가능하다는 것이다. 따라서 최종 렌더링된 색상 <span class="math math-inline">\hat{C}(\mathbf{r})</span>에 대한 손실 함수의 기울기(Gradient)를 계산하면, 연쇄 법칙(Chain Rule)에 의해 각 샘플 지점의 <span class="math math-inline">\sigma_i</span>와 <span class="math math-inline">\mathbf{c}_i</span>에 대한 미분값을 구할 수 있고, 이를 통해 MLP를 학습시킬 수 있다.1</p>
<hr />
<h2>4.  손실 함수와 최적화 전략</h2>
<p>NeRF의 학습은 렌더링된 이미지와 실제 입력 이미지(Ground Truth) 사이의 차이를 최소화하는 최적화 문제이다.</p>
<h3>4.1 광도 일관성 손실 (Photometric Loss)</h3>
<p>가장 기본적이고 널리 사용되는 손실 함수는 픽셀 단위의 제곱 오차(Squared Error), 즉 L2 Loss이다.4<br />
<span class="math math-display">
\mathcal{L}_{total} = \sum_{\mathbf{r} \in \mathcal{R}} \left[ \| \hat{C}_c(\mathbf{r}) - C_{gt}(\mathbf{r}) \|_2^2 + \| \hat{C}_f(\mathbf{r}) - C_{gt}(\mathbf{r}) \|_2^2 \right]
</span><br />
여기서 <span class="math math-inline">\mathcal{R}</span>은 배치(Batch)에 포함된 광선들의 집합이다. 주목할 점은 Coarse Network의 출력 <span class="math math-inline">\hat{C}_c</span>와 Fine Network의 출력 <span class="math math-inline">\hat{C}_f</span> 모두에 대해 손실을 계산한다는 것이다.15 비록 최종 이미지는 Fine Network를 통해 생성되지만, Coarse Network가 정확한 밀도 분포(PDF)를 생성해야 Fine Network가 유효한 샘플링을 할 수 있으므로, 두 네트워크를 동시에 학습시키는 것이 필수적이다.</p>
<h3>4.2 최적화 하이퍼파라미터 및 구현 상세</h3>
<p>성공적인 NeRF 학습을 위해서는 세심한 하이퍼파라미터 설정이 요구된다.</p>
<ul>
<li><strong>배치 크기(Batch Size):</strong> 일반적으로 4,096개의 광선을 하나의 배치로 사용한다.7 이는 GPU 메모리 한계(VRAM)와 학습 안정성 사이의 균형점이다. 배치가 너무 작으면 그라디언트 노이즈가 심해 수렴하지 않고, 너무 크면 메모리 부족(OOM) 현상이 발생한다.</li>
<li><strong>최적화 알고리즘:</strong> Adam 옵티마이저가 표준으로 사용되며, 파라미터는 <span class="math math-inline">\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10^{-7}</span>로 설정된다.</li>
<li><strong>학습률 스케줄링(Learning Rate Scheduling):</strong> 초기 학습률은 보통 <span class="math math-inline">5 \times 10^{-4}</span>에서 시작하여, 학습이 진행됨에 따라 지수적으로 감소(Exponential Decay)시켜 약 <span class="math math-inline">5 \times 10^{-5}</span>까지 낮춘다.2 이는 초기에는 빠르게 해를 탐색하고, 후기에는 미세한 디테일을 조정하기 위함이다.</li>
<li><strong>학습 반복:</strong> 단일 장면을 고품질로 학습하기 위해서는 보통 100k ~ 300k 회의 반복(Iteration)이 필요하다. 이는 최신 GPU에서도 수 시간이 소요되는 작업이며, NeRF의 실시간 적용을 어렵게 만드는 주요 요인이었다(이는 5.2.4절의 고속화 연구 동기가 된다).</li>
</ul>
<hr />
<h2>5.  뷰 합성(View Synthesis): 로봇의 상상력과 시뮬레이션</h2>
<p>학습이 완료된 NeRF 모델은 장면의 모든 정보를 신경망의 가중치(Weight) 형태로 압축하여 저장하고 있다. 이제 임의의 카메라 포즈 <span class="math math-inline">\mathbf{P}_{novel}</span>이 주어졌을 때, 새로운 뷰를 합성하는 과정(Inference)은 학습 파이프라인의 순방향 패스(Forward Pass)와 동일하게 수행된다.</p>
<ol>
<li>목표 시점 <span class="math math-inline">\mathbf{P}_{novel}</span>에서 광선을 생성한다.</li>
<li>Coarse 및 Fine 샘플링을 수행한다.</li>
<li>Fine MLP를 통해 밀도와 색상을 쿼리한다.</li>
<li>볼륨 렌더링을 통해 이미지를 생성한다.</li>
</ol>
<p>이 뷰 합성 능력은 로봇 공학, 특히 <strong>Sim-to-Real</strong>과 <strong>로봇 학습(Robot Learning)</strong> 분야에서 강력한 도구가 된다.</p>
<h3>5.1  데이터 증강 (Data Augmentation)과 일반화</h3>
<p>로봇 조작(Manipulation)이나 파지(Grasping) 작업을 학습시키기 위해서는 방대한 양의 데이터가 필요하다. 실제 로봇을 구동하여 데이터를 수집하는 것은 시간과 비용이 많이 들고 위험할 수 있다. NeRF를 사용하면 소수의 실제 이미지로부터 무한히 다양한 각도의 “가상(Hallucinated)” 뷰를 생성할 수 있다.27</p>
<p>최신 연구인 SPARTN 28이나 NeRF-Aug 29는 이러한 아이디어를 확장하여, 로봇 팔이 물체를 조작하는 데모 영상에 NeRF 기반의 섭동(Perturbation)을 주입한다. 즉, 로봇이 실제로 경험하지 않은 각도에서 물체를 보았을 때의 이미지를 합성하여 학습 데이터에 추가함으로써, 시각적 변화에 강인한 정책(Policy)을 학습시킨다. 이는 RGB-D 카메라 없이 일반 RGB 카메라만으로도 깊이 정보에 준하는 기하학적 이해를 가능하게 하여, 투명하거나 반사되는 물체(유리잔 등)의 파지 성공률을 비약적으로 높였다.28</p>
<h3>5.2  모델 기반 강화학습과 경로 계획</h3>
<p>NeRF는 로봇을 위한 고정밀 “월드 모델(World Model)” 역할을 수행할 수 있다. 로봇은 실제 환경에서 행동하기 전에, NeRF로 구축된 가상 환경 내에서 자신의 위치를 변경해가며 시각적 입력을 예측(Imagination)할 수 있다.31</p>
<p>NeRF-Navigation 31과 같은 연구는 로봇이 NeRF 환경 내에서 충돌 없는 경로를 계획하는 방법을 제안한다. NeRF의 밀도 <span class="math math-inline">\sigma</span> 값은 해당 공간이 비어있는지 물체로 차 있는지(Occupancy)를 나타내는 직접적인 지표가 된다. 따라서 궤적 최적화 과정에서 높은 밀도 영역을 통과하는 경로에 페널티를 부여함으로써, 로봇은 복잡한 3D 환경(예: 정글짐, 좁은 실내)에서도 안전하게 비행하거나 주행할 수 있다.32 또한 CATNIPS 33와 같은 연구는 NeRF를 푸아송 점 과정(Poisson Point Process)으로 해석하여 충돌 확률을 수학적으로 엄밀하게 계산하고, 안전 보장형 경로 계획을 수행한다.</p>
<h3>5.3  포즈 추정의 역문제 (Pose Estimation via Inversion)</h3>
<p>NeRF의 미분 가능성은 이미지 생성(<span class="math math-inline">F(\text{Pose}) \to \text{Image}</span>)뿐만 아니라 그 역과정(<span class="math math-inline">F^{-1}(\text{Image}) \to \text{Pose}</span>)도 가능하게 한다. <strong>iNeRF</strong> 34는 학습된 NeRF 모델을 고정시키고, 관측된 이미지와 렌더링된 이미지의 차이를 줄이는 방향으로 카메라 포즈 <span class="math math-inline">\mathbf{P}</span>를 역전파를 통해 최적화한다. 이는 GPS나 외부 마커 없이도 로봇이 자신이 보고 있는 장면과 기억(NeRF 모델)을 대조하여 현재 위치를 정밀하게 추정할 수 있게 해준다. 이러한 방식은 조명 변화나 계절 변화가 심한 환경에서도 강인한 6-DoF 위치 추정을 가능하게 하며, 시각적 서보잉(Visual Servoing)이나 정밀 도킹 작업에 활용될 수 있다.</p>
<h2>6.  요약 및 결론</h2>
<p>NeRF의 학습 파이프라인과 뷰 합성은 단순한 이미지 생성 기술을 넘어, 로봇에게 공간을 이해하고 시뮬레이션할 수 있는 강력한 인지적 틀을 제공한다. <strong>계층적 샘플링</strong>은 연속적인 3D 공간을 효율적으로 탐색할 수 있게 하며, <strong>위치 인코딩</strong>과 <strong>뷰 의존적 MLP</strong>는 물리적 세계의 복잡한 형상과 광학적 특성을 신경망 내부에 정교하게 압축한다. 그리고 <strong>미분 가능한 볼륨 렌더링</strong>은 2D 관측과 3D 표현 사이의 간극을 수학적으로 완벽하게 연결한다.</p>
<p>이러한 파이프라인을 통해 로봇은 희소한 관측 데이터로부터 풍부한 3D 정보를 복원하고(Reconstruction), 가상의 시점을 생성하여 학습 데이터를 증강하며(Augmentation), 미래의 관측을 예측하여 행동을 계획(Planning)할 수 있게 되었다. 다음 절에서는 이러한 NeRF 파이프라인의 계산적 한계를 극복하고 실시간 로보틱스 응용을 가능하게 한 고속화 기술(Instant-NGP 등)과 최신 변형 모델들에 대해 다룬다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>NeRF – Representing Scenes as Neural Radiance Fields for View Synthesis - Wandb, https://wandb.ai/wandb_fc/articles/reports/NeRF-Representing-Scenes-as-Neural-Radiance-Fields-for-View-Synthesis–Vmlldzo1NDI2ODE1</li>
<li>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis, https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460392.pdf</li>
<li>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis - arXiv, https://arxiv.org/abs/2003.08934</li>
<li>NeRF: Representing scenes as neural radiance fields for view synthesis - Computer Science - University of California San Diego, https://cseweb.ucsd.edu/~ravir/icbs23.pdf</li>
<li>Representing Scenes as Neural Radiance … - Neural Fields: NeRF, https://neuralfields.cs.brown.edu/paper_33.html</li>
<li>NoPe-NeRF: Optimising Neural Radiance Field With No Pose Prior - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.pdf</li>
<li>A General Implicit Framework for Fast NeRF Composition and Rendering - arXiv, https://arxiv.org/html/2308.04669v4</li>
<li>Training your first model - nerfstudio, https://docs.nerf.studio/quickstart/first_nerf.html</li>
<li>Some doubts about the nerf training forward and backward · Issue #614 · NVlabs/instant-ngp, https://github.com/NVlabs/instant-ngp/issues/614</li>
<li>The Annotated NeRF Training NeRF on Custom Dataset in Pytorch, https://learnopencv.com/annotated-nerf-pytorch/</li>
<li>Limitations of NeRF with Pre-trained Vision Features for Few-Shot 3D Reconstruction - arXiv, https://arxiv.org/html/2506.18208v1</li>
<li>NeRF: Rendering Reality with Just Two MLPs | by Bharathsaireddy - Medium, https://medium.com/@bharathsaireddy7161/nerf-rendering-reality-with-just-two-mlps-f25919f09e2d</li>
<li>Injecting 3D Perception of Controllable NeRF-GAN into StyleGAN for Editable Portrait Image Synthesis, https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136770240.pdf</li>
<li>NeRFVS: Neural Radiance Fields for Free View Synthesis via Geometry Scaffolds - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_NeRFVS_Neural_Radiance_Fields_for_Free_View_Synthesis_via_Geometry_CVPR_2023_paper.pdf</li>
<li>Neural Radiance Fields (NeRF): A Complete Tutorial | by Kavishka Abeywardana - Medium, https://medium.com/@kdwaMachineLearning/neural-radiance-fields-nerf-a-complete-tutorial-b813e3ed4461</li>
<li>Improving Neural Radiance Fields Using Near-Surface Sampling With Point Cloud Generation - arXiv, https://arxiv.org/html/2310.04152v2</li>
<li>Object and Scene Reconstruction using Neural Radiance Fields - UC Berkeley EECS, https://www2.eecs.berkeley.edu/Pubs/TechRpts/2023/EECS-2023-128.pdf</li>
<li>NeRF Revisited: Fixing Quadrature Instability in Volume Rendering - NeurIPS, https://proceedings.neurips.cc/paper_files/paper/2023/file/5301c49207917c5c870131959971851c-Paper-Conference.pdf</li>
<li>Inverse Transform Sampling in NeRF - Aditya Mehrotra, https://adityamehrotra.ca/blog/NeRF-Inverse-Transform-Sampling/</li>
<li>Reichman University DDNeRF: Depth Distribution Neural Radiance Fields, https://www.runi.ac.il/media/ky2lwtqf/ddnerf-depth-distribution-neural-radiance-fields-dudi-dadon.pdf</li>
<li>NeXT: Towards High Quality Neural Radiance Fields via Multi-Skip Transformer, https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920069.pdf</li>
<li>A Critical Analysis of NeRF-Based 3D Reconstruction - MDPI, https://www.mdpi.com/2072-4292/15/14/3585</li>
<li>NeRFs in Robotics: A Survey - arXiv, https://arxiv.org/html/2405.01333v2</li>
<li>Notes on NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis by Mildenhall et al | by M(A)C | Medium, https://medium.com/@chengmu/notes-on-nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis-by-mildenhall-et-al-fd88f715fe77</li>
<li>DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision - arXiv, https://arxiv.org/html/2312.16256v2</li>
<li>Grid-guided Neural Radiance Fields for Large Urban Scenes, https://city-super.github.io/gridnerf/img/main.pdf</li>
<li>NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields - DSpace@MIT, https://dspace.mit.edu/bitstream/handle/1721.1/153644/2203.01913v1.pdf?sequence=1&amp;isAllowed=y</li>
<li>NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis, https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_NeRF_in_the_Palm_of_Your_Hand_Corrective_Augmentation_for_CVPR_2023_paper.pdf</li>
<li>NeRF-Aug: Data Augmentation for Robotics with Neural Radiance Fields - arXiv, https://arxiv.org/html/2411.02482v2</li>
<li>NeRF-Aug: Data Augmentation for Robotics with Neural Radiance Fields - arXiv, https://arxiv.org/html/2411.02482v1</li>
<li>Vision-Only Robot Navigation in a Neural Radiance World - IEEE Xplore, https://ieeexplore.ieee.org/ielaam/7083369/9647862/9712211-aam.pdf</li>
<li>Vision-Only Robot Navigation in a Neural Radiance World - Preston Culbertson, https://pculbertson.github.io/assets/pdf/adamkiewicz2021.pdf</li>
<li>CATNIPS: Collision Avoidance Through Neural Implicit Probabilistic Scenes, https://msl.stanford.edu/papers/chen_catnips_2023.pdf</li>
<li>Joint Optimization of Neural Radiance Fields and Continuous Camera Motion from a Monocular Video - arXiv, https://arxiv.org/html/2504.19819v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>