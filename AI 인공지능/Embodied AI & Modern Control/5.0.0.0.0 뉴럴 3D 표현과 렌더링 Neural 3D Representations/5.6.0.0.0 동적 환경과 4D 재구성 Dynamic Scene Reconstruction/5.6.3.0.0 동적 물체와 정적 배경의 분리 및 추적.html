<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:5.6.3 동적 물체와 정적 배경의 분리 및 추적</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>5.6.3 동적 물체와 정적 배경의 분리 및 추적</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 5. 뉴럴 3D 표현과 렌더링 (Neural 3D Representations)</a> / <a href="index.html">5.6 동적 환경과 4D 재구성 (Dynamic Scene Reconstruction)</a> / <span>5.6.3 동적 물체와 정적 배경의 분리 및 추적</span></nav>
                </div>
            </header>
            <article>
                <h1>5.6.3 동적 물체와 정적 배경의 분리 및 추적</h1>
<p>임바디드 AI(Embodied AI)가 연구실의 통제된 환경을 벗어나 실제 세계(Wild World)로 진입할 때 마주하는 가장 거대하고 본질적인 장벽은 바로 ’변화’이다. 고전적인 로봇 공학, 특히 SLAM(Simultaneous Localization and Mapping)과 내비게이션 분야는 수십 년간 “세상은 정지해 있다“는 강력한 전제, 즉 정적 세계 가설(Static World Assumption) 위에서 발전해 왔다. 건물은 움직이지 않고, 가구는 고정되어 있으며, 오직 로봇만이 이동한다는 이 가정은 복잡한 3차원 공간을 수학적으로 모델링하는 데 있어 필수적인 단순화였다.</p>
<p>그러나 현실은 혼돈과 역동성으로 가득 차 있다. 보행자는 예측 불가능한 경로로 걷고, 차량은 고속으로 이동하며, 바람에 흔들리는 나뭇가지나 시시각각 변하는 조명은 센서 데이터에 막대한 노이즈를 유발한다. 로봇이 이러한 환경에서 생존하고 유의미한 과업을 수행하기 위해서는 시각적 입력 내에서 ’변하지 않는 것(Static)’과 ’변하는 것(Dynamic)’을 명확히 구분하는 능력이 선행되어야 한다. 이는 단순히 배경에서 움직이는 물체를 지워버리는(Removal) 소극적인 대처를 넘어, 동적 객체를 정밀하게 분리(Decoupling)하고, 그들의 6자유도(6-DOF) 움직임을 시간 축에 따라 추적(Tracking)하며, 나아가 그 미래 상태를 예측(Prediction)하여 제어 루프에 통합하는 적극적인 인지 과정으로 진화해야 한다.</p>
<p>본 절에서는 최신 SOTA(State-of-the-Art) 기술인 NeRF(Neural Radiance Fields)와 3D Gaussian Splatting(3DGS)이 이 난제를 어떻게 해결하고 있는지 심도 있게 분석한다. 우리는 기하학적 모호성(Ambiguity)을 극복하기 위한 수학적 최적화 전략부터, 로봇의 실시간 제어를 위한 명시적 표현의 진화 과정, 그리고 이를 통해 가능해진 물리적 상호작용의 새로운 지평을 탐구할 것이다.</p>
<h2>1.  정적-동적 분리의 기하학적 난제와 이론적 토대</h2>
<p>동적 환경에서의 3D 재구성은 근본적으로 불량 조건 문제(Ill-posed Problem)에 해당한다. 단안 카메라(Monocular Camera)로 촬영된 비디오 시퀀스에서 특정 픽셀의 변화가 발생했을 때, 이것이 카메라의 움직임(Ego-motion)에 의한 시차(Parallax) 때문인지, 물체 자체의 이동(Object Motion) 때문인지, 아니면 조명이나 반사 특성의 변화(Radiance Change) 때문인지를 단일 프레임 정보만으로는 완벽하게 구분할 수 없기 때문이다.</p>
<p>이를 해결하기 위해 최신 뉴럴 렌더링 연구들은 장면을 정적 배경 필드(Static Field)와 동적 객체 필드(Dynamic Field)로 명시적으로 분해(Factorization)하는 전략을 취한다. 수학적으로 이는 시간 <span class="math math-inline">t</span>에서의 렌더링 결과 <span class="math math-inline">I(t)</span>를 다음과 같은 합성 함수로 모델링하는 것으로 시작된다.<br />
<span class="math math-display">
I(t) = \mathcal{R}\left( \Psi_{static}, \Psi_{dynamic}(t), \mathbf{P}(t) \right)
</span><br />
여기서 <span class="math math-inline">\mathcal{R}</span>은 볼륨 렌더링 또는 래스터화 함수이며, <span class="math math-inline">\Psi_{static}</span>은 시간 불변(Time-invariant)의 정적 장면 표현, <span class="math math-inline">\Psi_{dynamic}(t)</span>는 시간 가변(Time-variant)적인 동적 장면 표현, 그리고 <span class="math math-inline">\mathbf{P}(t)</span>는 카메라의 포즈 행렬이다. 이 식에서 로봇 공학적 핵심 과제는 미지의 <span class="math math-inline">\Psi_{dynamic}(t)</span>를 어떻게 모델링하고, 이것이 <span class="math math-inline">\Psi_{static}</span>과 혼재되지 않도록 어떻게 정칙화(Regularization) 할 것인가에 있다.</p>
<h3>1.1  에피폴라 기하학 위반과 모션 분할</h3>
<p>전통적인 방식과 초기 뉴럴 렌더링 방식은 주로 에피폴라 기하학(Epipolar Geometry) 위반 여부를 통해 동적 객체를 검출했다. 정적 배경의 3차원 점들은 카메라 움직임에 의해 유도된 에피폴라 라인(Epipolar Line) 상에 투영되어야 한다는 강력한 기하학적 제약이 존재한다. 반면, 움직이는 물체는 이 제약을 벗어나게 되므로, 이를 통해 동적 영역 마스크 <span class="math math-inline">M(t)</span>를 생성할 수 있다.</p>
<p>그러나 로봇이 회전만 하거나(Pure Rotation), 물체가 카메라의 광축(Optical Axis)과 나란히 움직이는 축퇴(Degenerate) 상황에서는 이러한 기하학적 제약만으로는 분리가 불가능하다. 또한, 텍스처가 부족한 영역(Texture-less region)이나 반복적인 패턴에서는 광류(Optical Flow) 추정 자체가 실패하기 쉽다.</p>
<p>최신 연구인 <strong>DynIBaR</strong>  등은 이러한 한계를 극복하기 위해 시간적 중복성(Temporal Redundancy)과 데이터 기반의 사전 지식(Data-driven Priors)을 결합한다. 특히 영상 기반 렌더링(IBR, Image-Based Rendering) 방식을 차용하여, 인접한 프레임들로부터 특징(Feature)을 집계할 때 정적 배경은 시간적으로 안정적인 매칭 비용(Matching Cost)을 보이지만 동적 객체는 높은 비용을 유발한다는 점을 역이용한다.</p>
<h3>1.2  강체 변환과 비강체 변형의 이원화 모델링</h3>
<p>로봇 공학 관점에서 동적 물체는 다시 두 가지 범주로 나뉜다. 하나는 자동차나 박스처럼 형태가 변하지 않고 위치와 자세만 변하는 ’강체(Rigid Body)’이고, 다른 하나는 보행자, 옷, 밧줄처럼 형태 자체가 변하는 ’비강체(Non-rigid Body)’이다.</p>
<p>이를 통합적으로 모델링하기 위해 최근의 방법론들은 정규 공간(Canonical Space)으로의 매핑 함수를 학습하는 접근법을 취한다. 즉, 시간 <span class="math math-inline">t</span>에 관측된 점 <span class="math math-inline">\mathbf{x}_t</span>를 기준 시간 <span class="math math-inline">t_0</span>의 점 <span class="math math-inline">\mathbf{x}_0</span>로 역변환하는 변형 필드(Deformation Field) <span class="math math-inline">\mathcal{T}(\mathbf{x}, t)</span>를 신경망으로 근사하는 것이다.<br />
<span class="math math-display">
\mathbf{x}_0 = \mathcal{T}(\mathbf{x}_t, t) = \mathbf{x}_t + \Delta\mathbf{x}(\mathbf{x}_t, t)
</span><br />
이때 정적 배경은 <span class="math math-inline">\mathcal{T}(\mathbf{x}, t) \approx \text{Identity}</span>인 영역으로 정의될 수 있으며, 동적 객체는 <span class="math math-inline">\Delta\mathbf{x}</span>가 유의미한 값을 가지는 영역으로 정의된다. 이 변형 필드가 얼마나 물리적으로 타당한지(Physically Plausible)가 추적 성능을 결정하며, 최신 연구들은 여기에 국소 강체성(Local Rigidity) 제약이나 시공간 평활도(Spatio-temporal Smoothness) 제약을 추가하여 모델의 강건성을 확보한다.</p>
<h2>2.  암시적 표현(Implicit Representations): NeRF 기반의 분리 기술</h2>
<p>NeRF의 등장은 연속적인 볼륨 표현을 통해 미분 가능한 방식으로 장면을 최적화할 수 있는 길을 열었다. 그러나 바닐라 NeRF(Vanilla NeRF)는 정적 장면만을 가정하므로, 동적 장면에서는 움직이는 물체의 잔상이 남는 ’유령 효과(Ghosting Artifacts)’나 배경이 흐릿해지는 문제가 발생한다. 이를 해결하기 위해 제안된 방법론들은 크게 ’강건한 손실 함수 기반의 동시 추정’과 ‘시간 차원 확장을 통한 4D 재구성’ 접근법으로 나뉜다.</p>
<h3>2.1  RoDynRF: 강건한 자세 추정과 필드 분리의 결합</h3>
<p>동적 장면 재구성에서 가장 큰 실질적 장벽은 정확한 카메라 포즈를 얻기 힘들다는 것이다. COLMAP과 같은 SfM(Structure from Motion) 알고리즘은 특징점(Keypoint)들이 정적이라는 가정을 전제로 하기 때문에, 화면의 대부분을 움직이는 물체가 차지하거나 복잡한 동적 텍스처가 있는 경우 포즈 추정에 실패하거나 매우 부정확한 결과를 내놓는다. 이는 로봇이 자신의 위치를 모르는 상태(Localization Failure)에서 맵을 작성해야 하는 딜레마를 야기한다.</p>
<p><strong>RoDynRF (Robust Dynamic Radiance Fields)</strong> 는 이러한 문제를 해결하기 위해 정적/동적 방사형 필드와 카메라 파라미터(포즈 및 초점 거리)를 결합하여 추정(Joint Estimation)하는 프레임워크를 제안한다.</p>
<h4>2.1.1  결합 추정 메커니즘 (Joint Estimation Mechanism)</h4>
<p>RoDynRF는 입력 비디오로부터 다음 세 가지 요소를 동시에 최적화하는 방식을 택한다. 이는 기존의 파이프라인(SfM <span class="math math-inline">\to</span> NeRF 학습)을 타파하고, 포즈 추정 자체를 NeRF 최적화 루프 안에 포함시킨 것이다.</p>
<ol>
<li><strong>정적 방사형 필드 (Static Radiance Field):</strong> 시간 불변의 색상 <span class="math math-inline">\mathbf{c}</span>와 밀도 <span class="math math-inline">\sigma</span>를 모델링한다. 이는 전체 비디오 시퀀스에서 공유되는 배경 정보를 담는다.</li>
<li><strong>동적 방사형 필드 (Dynamic Radiance Field):</strong> 시간 가변적인 구조와 외관을 모델링한다. 변형 필드(Deformation Field)를 통해 <span class="math math-inline">t</span> 시점의 공간을 정규 공간(Canonical Space)으로 매핑한다.</li>
<li><strong>카메라 궤적 (Camera Trajectory):</strong> 외부의 SfM 도움 없이 네트워크가 광도 오차(Photometric Error)를 최소화하는 방향으로 카메라의 6-DOF 포즈를 스스로 정렬(Self-calibration)한다.</li>
</ol>
<h4>2.1.2  강건한 최적화 전략 (Robust Optimization Strategy)</h4>
<p>이 과정에서 핵심은 동적 객체가 정적 배경 복원 시 ’아웃라이어(Outlier)’로 작용하지 않도록, 그리고 반대로 정적 배경이 동적 객체 학습을 방해하지 않도록 하는 것이다. RoDynRF는 강건한 손실 함수(Robust Loss Function)를 도입하여 동적 영역의 픽셀이 정적 필드 최적화에 주는 영향을 적응적으로 낮춘다.<br />
<span class="math math-display">
\mathcal{L}_{total} = \mathcal{L}_{photo} + \lambda_{reg}\mathcal{L}_{reg} + \lambda_{robust}\mathcal{L}_{robust}
</span><br />
여기서 <span class="math math-inline">\mathcal{L}_{robust}</span>는 데이터의 통계적 분포를 고려하여 이상치에 대한 민감도를 줄이는 함수(예: Geman-McClure 또는 적응형 Charbonnier loss)가 사용된다. 이러한 접근은 로봇이 사전 맵이 없는 미지의 동적 환경(Unknown Dynamic Environment)에 진입했을 때, 자신의 위치를 추정함과 동시에 움직이는 장애물을 의미론적으로 분리해낼 수 있는 기반 기술이 된다. 실험 결과, RoDynRF는 COLMAP이 실패하는 데이터셋에서도 안정적인 뷰 합성과 포즈 추정을 수행함을 입증했다.</p>
<h3>2.2  DynIBaR: 시간적 집계를 통한 장기적 일관성 확보</h3>
<p>순수 NeRF 방식은 긴 비디오 시퀀스에서 시간적 일관성을 잃거나, 빠른 움직임으로 인한 모션 블러(Motion Blur)를 텍스처로 잘못 학습하는 경향이 있다. <strong>DynIBaR (Dynamic Image-Based Rendering)</strong> 는 이러한 문제를 해결하기 위해 IBRNet과 같은 영상 기반 렌더링 아이디어를 동적 장면에 확장 적용한 것이다.</p>
<h4>2.2.1  모션 인식 특징 집계 (Motion-Aware Feature Aggregation)</h4>
<p>DynIBaR는 전체 장면 정보를 MLP 가중치에만 암시적으로 압축하는 대신, 렌더링 시점에 인접한 소스 프레임(Source Frames)의 픽셀 정보를 직접 참조하여 특징을 생성한다. 이때 중요한 혁신은 ’정적 픽셀’과 ’동적 픽셀’을 구분하여 참조하는 방식이다.</p>
<ul>
<li><strong>정적 콘텐츠 처리:</strong> 에피폴라 라인(Epipolar Line)을 따라 인접 뷰의 특징을 집계한다. 이는 뷰 의존적 효과(View-dependent effects)를 정확히 렌더링하는 데 유리하다.</li>
<li><strong>동적 콘텐츠 처리:</strong> 4D 모션 필드를 통해 3D 모션 벡터를 추정하고, 시간이 흐름에 따라 이동한 위치에서 특징을 샘플링한다. 즉, 시간 <span class="math math-inline">t</span>의 픽셀을 렌더링하기 위해 시간 <span class="math math-inline">t-1, t+1</span>의 프레임에서 해당 물체가 이동했을 것으로 예상되는 위치의 정보를 가져오는 것이다.</li>
</ul>
<p>이 방식은 로봇이 과거에 관측한 동적 물체의 모습을 현재 시점으로 투영하여 ’가려짐(Occlusion)’이나 ‘새로 드러남(Disocclusion)’ 영역을 추론할 때 매우 유용하다. DynIBaR의 연구 결과는 동적 물체와 정적 배경을 분리할 때, 명시적인 마스크 감독(Supervision) 없이도 3D 모션의 불일치(Discrepancy)와 특징 매칭 비용의 차이만으로도 고품질의 분리가 가능함을 시사한다.</p>
<h3>2.3  시공간 분해의 효율화: HexPlane과 K-Planes</h3>
<p>NeRF 기반 방식의 가장 큰 단점은 학습 속도와 메모리 효율이다. 4D 공간 전체를 고해상도 복셀로 표현하는 것은 메모리 대역폭을 초과하기 쉽다. 이를 개선하기 위해 등장한 <strong>HexPlane</strong> 과 <strong>K-Planes</strong>는 4D 시공간(<span class="math math-inline">x, y, z, t</span>)을 저차원 평면으로 분해(Factorization)하여 표현하는 텐서 분해(Tensor Decomposition) 기법을 도입했다.</p>
<ul>
<li><strong>HexPlane:</strong> 4D 공간을 6개의 평면(<span class="math math-inline">xy, xz, yz, xt, yt, zt</span>)으로 투영하여 특징을 저장한다.</li>
<li><strong>분리 전략:</strong> 정적 배경은 공간 평면인 <span class="math math-inline">xy, xz, yz</span> 평면에서만 주로 표현되고, 동적 변화는 시간 축이 포함된 <span class="math math-inline">xt, yt, zt</span> 평면에서 포착되도록 유도한다.</li>
</ul>
<p>이러한 분해 기법은 로봇의 제한된 온보드 메모리(On-board Memory)에서 동적 맵을 유지하고 업데이트하는 데 있어 필수적인 경량화 솔루션을 제공한다. 특히 <strong>LocalDyGS</strong> 와 같은 최신 연구는 이러한 평면 분해 아이디어를 3D Gaussian Splatting과 결합하여, 동적 장면을 로컬 공간(Seed regions)으로 나누어 처리함으로써 전역적인 복잡도를 낮추고 효율성을 극대화한다.</p>
<h2>3.  명시적 표현(Explicit Representations): 3D Gaussian Splatting 기반의 추적</h2>
<p>2023년 등장한 **3D Gaussian Splatting (3DGS)**은 로봇 공학 관점에서 ’게임 체인저’가 되었다. NeRF가 공간을 ’쿼리’하여 색상을 얻는 암시적 함수라면, 3DGS는 공간을 수백만 개의 3D 가우시안 타원체(Ellipsoid)들의 집합으로 ’구성’하는 명시적 표현이다. 이는 동적 물체를 추적하고 분리하는 데 있어 훨씬 더 직관적이고 강력한 도구를 제공한다. 로봇에게 있어 3DGS는 단순한 렌더링 기술이 아니라, 세상을 구성하는 ’입자(Particle)’를 직접 다룰 수 있게 해주는 인터페이스이다.</p>
<h3>3.1  라그랑주 관점의 전환과 6-DOF 추적</h3>
<p>유체 역학에서 유동을 기술하는 방식이 오일러(Eulerian, 고정 격자 기반)와 라그랑주(Lagrangian, 이동 입자 기반)로 나뉘듯, 3DGS는 동적 장면을 입자 중심의 라그랑주 관점으로 다룬다. <strong>Dynamic 3D Gaussians</strong>  연구는 가우시안 자체가 물리적 공간을 점유하는 입자로서 시간 <span class="math math-inline">t</span>에 따라 이동하고 회전(<span class="math math-inline">R</span>)하며 크기(<span class="math math-inline">S</span>)가 변하는 과정을 모델링한다.</p>
<h4>3.1.1  지속적 동적 뷰 합성을 통한 추적 (Tracking by Persistent Dynamic View Synthesis)</h4>
<p>이 연구의 핵심 통찰은 “완벽한 뷰 합성을 위해서는 가우시안들이 실제 물체의 표면에 붙어 있어야(Stick) 한다“는 것이다. 별도의 3D 레이블이나 광류(Optical Flow) 입력 없이도, 렌더링 손실(Rendering Loss)을 최소화하는 과정에서 가우시안들은 자연스럽게 물체의 움직임을 따라가게 된다.</p>
<ul>
<li><strong>지속성(Persistence) 제약:</strong> 가우시안의 색상(Color), 불투명도(Opacity), 크기(Size)는 시간 불변(Time-invariant)으로 제약하고, 오직 위치(Position)와 회전(Rotation)만 시간 가변(Time-variant)으로 둔다. 이는 가우시안이 매 프레임 ‘새로 생겨나거나 사라지는’ 것이 아니라 ’이동한다’는 물리적 실재성을 강제하는 효과가 있다.</li>
<li><strong>국소 강체성(Local Rigidity) 제약:</strong> 인접한 가우시안들은 서로 비슷한 물리적 움직임을 보일 것이라는 제약을 추가한다. <span class="math math-inline">k</span>-최근접 이웃(k-NN) 가우시안들의 이동 경로가 유사하도록 강제함으로써, 개별 가우시안이 흩어지는 것을 막고 물체(Cluster) 단위의 일관된 움직임을 유도한다.</li>
</ul>
<p>이 결과로 얻어지는 것은 단순한 영상이 아니라, 시간 <span class="math math-inline">t</span>에서의 모든 가우시안의 <strong>6-DOF 포즈 궤적</strong>이다. 이는 로봇이 동적 물체의 속도와 회전각을 직접적으로 알 수 있게 해 주며, 충돌 회피 알고리즘에 즉각적인 입력값으로 사용될 수 있다.</p>
<h3>3.2  Gaussian-Flow: 이중 도메인 변형 모델 (DDDM)</h3>
<p>가우시안의 움직임을 단순히 시간 축에 따른 좌표 변화로만 학습하면, 긴 시퀀스에서 오차가 누적되거나 고주파 떨림을 표현하지 못할 수 있다. <strong>Gaussian-Flow</strong> 는 이를 해결하기 위해 **이중 도메인 변형 모델(Dual-Domain Deformation Model, DDDM)**을 제안한다.</p>
<p>DDDM은 동적 가우시안의 움직임을 ’저주파의 전체적인 이동’과 ’고주파의 미세 변형’으로 분리하여 모델링한다.</p>
<ol>
<li><strong>시간 도메인 (Time Domain) - 다항식 피팅:</strong> 물체의 전체적인 궤적(Trajectory)과 같은 저주파 움직임은 시간 <span class="math math-inline">t</span>에 대한 다항식(Polynomial)으로 근사한다. 이는 로봇이 물체의 이동 경로를 예측할 때 부드러운 곡선 형태의 궤적을 제공한다.</li>
<li><strong>주파수 도메인 (Frequency Domain) - 푸리에 급수 피팅:</strong> 옷자락의 펄럭임, 나뭇잎의 떨림과 같은 고주파의 반복적인 움직임은 푸리에 급수(Fourier Series)로 모델링한다.</li>
</ol>
<p><span class="math math-display">
\Delta \mu(t) = \underbrace{\sum_{k=0}^{K} c_k t^k}_{\text{저주파 이동}} + \underbrace{\sum_{n=1}^{N} (a_n \cos(n\omega t) + b_n \sin(n\omega t))}_{\text{고주파 변형}}
</span></p>
<p>이러한 분해는 로봇이 동적 객체의 ’주요 이동 경로’와 ’미세 움직임’을 구분하여 인식하게 해 준다. 예를 들어, 보행자의 전체적인 이동 경로는 다항식 항에서 추출하고, 보행 시 발생하는 팔다리의 교차 운동은 푸리에 항에서 포착할 수 있다. 또한, 이 방식은 별도의 무거운 신경망(MLP) 없이 가우시안 속성 자체를 수식으로 최적화하므로 학습 속도가 기존 3DGS 대비 5배 이상 빠르며 , 현장 적응형(On-the-fly) 학습이 필요한 로봇에게 큰 이점을 제공한다.</p>
<h3>3.3  4D Gaussian Splatting (4D-GS)과 실시간 렌더링</h3>
<p>로봇의 제어 주기는 보통 수백 Hz에 달하므로, 환경 인식 또한 실시간성을 보장해야 한다. <strong>4D-GS</strong> 는 시공간 구조 인코더(Spatial-Temporal Structure Encoder)와 다중 헤드 가우시안 변형 디코더(Multi-head Gaussian Deformation Decoder)를 결합하여 고해상도 동적 장면을 80 FPS 이상의 초고속으로 렌더링하는 기술이다.</p>
<ul>
<li><strong>HexPlane 인코딩의 활용:</strong> 4D-GS는 앞서 언급한 HexPlane의 4D 복셀 특징 추출 방식을 차용하여, 인접 시공간 정보를 효율적으로 통합한다.</li>
<li><strong>경량 디코더:</strong> 각 가우시안의 변형을 예측하는 MLP를 극도로 경량화하여 연산 부하를 줄였다.</li>
</ul>
<p>이 방식은 정적 배경을 위한 정규 가우시안 세트 하나만 메모리에 유지하고, 매 프레임 변형 필드를 통해 동적 상태를 계산하므로 메모리 효율이 매우 높다. 로봇은 이 컴팩트한 표현을 통해 동적 환경의 ’디지털 트윈’을 실시간으로 내부 시뮬레이션에 활용할 수 있다. 예를 들어, 4D-GS로 복원된 가상 환경 내에서 로봇 팔의 경로 계획을 수립하고 충돌 검사를 수행하는 것이 실시간으로 가능해진다.</p>
<h3>3.4  데이터 기반 비교: NeRF vs 3DGS</h3>
<p>동적 객체 분리 및 추적 관점에서 두 기술군의 성능과 특성을 비교하면 다음과 같다.</p>
<table><thead><tr><th><strong>특징</strong></th><th><strong>NeRF 기반 (RoDynRF, DynIBaR)</strong></th><th><strong>3DGS 기반 (Dynamic 3D Gaussians, Gaussian-Flow)</strong></th></tr></thead><tbody>
<tr><td><strong>기본 표현</strong></td><td>연속적 암시적 필드 (MLP, Voxel)</td><td>이산적 명시적 입자 (Gaussian)</td></tr>
<tr><td><strong>렌더링 방식</strong></td><td>광선 투사 (Ray Marching)</td><td>래스터화 (Rasterization)</td></tr>
<tr><td><strong>분리 메커니즘</strong></td><td>로버스트 손실 함수, 특징 집계, 4D 분해</td><td>국소 강체성 제약, 이중 도메인 변형(DDDM)</td></tr>
<tr><td><strong>추적 방식</strong></td><td>변형 필드(Deformation Field)의 벡터</td><td>가우시안 입자의 6-DOF 포즈 궤적</td></tr>
<tr><td><strong>장점</strong></td><td>긴 영상에서의 시간적 일관성, 배경 복원 품질, 뷰 의존적 효과 표현 우수</td><td><strong>실시간 렌더링/학습 속도</strong>, 물리적 입자성, 명시적 편집 가능성</td></tr>
<tr><td><strong>단점</strong></td><td>느린 학습/추론 속도, 높은 메모리 대역폭 요구</td><td>위상 변화(Topology Change) 표현의 한계(가우시안 개수 고정 시)</td></tr>
<tr><td><strong>로봇 응용</strong></td><td>고정밀 오프라인 지도 작성, 사후 분석</td><td><strong>실시간 충돌 회피</strong>, 온라인 SLAM, 조작(Manipulation) 계획</td></tr>
<tr><td><strong>렌더링 속도</strong></td><td>&lt; 1 FPS (TiNeuVox 등 일부 예외 제외)</td><td>20 ~ 200 FPS</td></tr>
</tbody></table>
<p>이 표에서 볼 수 있듯이, 3DGS 기반 방법론은 압도적인 속도와 명시적 입자 표현 덕분에 실시간 로봇 제어 및 상호작용에 더 적합한 특성을 보인다. 반면 NeRF 기반 방법론은 고정밀 맵핑이나 데이터 생성용 시뮬레이터 구축에 강점이 있다.</p>
<h2>4.  로봇 공학적 통합: 시각적 분리를 넘어 물리적 상호작용으로</h2>
<p>동적 물체와 정적 배경의 분리는 그 자체로 목적이 아니라, 로봇의 안전한 제어(Control)와 상호작용(Interaction)을 위한 수단이다. 최신 연구들은 시각적 추적을 물리적 동역학(Dynamics) 학습으로 연결하여, 로봇이 “이것은 무엇인가?“를 넘어 “이것은 어떻게 움직일 것인가?“를 이해하도록 돕고 있다.</p>
<h3>4.1  동적 SLAM과 내비게이션에서의 역할 전환</h3>
<p>전통적인 SLAM에서 동적 물체는 맵의 정합성을 해치는 ’노이즈’이자 ’제거 대상’이었다. <strong>DOT (Dynamic Object Tracking)</strong> 이나 <strong>RoDyn-SLAM</strong> 과 같은 시스템은 의미론적 분할(Semantic Segmentation)과 기하학적 검증을 결합하여 동적 객체를 마스킹하고, 오직 정적 배경 특징점만을 사용하여 로봇의 위치를 추정하고 지도를 작성했다. 이를 통해 붐비는 환경에서도 로봇의 위치 추정 드리프트(Drift)를 방지할 수 있었다.</p>
<p>그러나 최신 <strong>Embodied AI</strong> 트렌드는 동적 물체를 단순한 회피 대상에서 ’예측 및 상호작용 대상’으로 격상시킨다. DynIBaR나 Dynamic 3D Gaussians가 제공하는 조밀한(Dense) 추적 정보는 **모델 예측 제어(MPC, Model Predictive Control)**의 입력으로 직접 사용된다.</p>
<ul>
<li><strong>예측적 회피 (Predictive Avoidance):</strong> 장애물의 현재 위치뿐만 아니라, 추적된 3D 궤적을 기반으로 미래 위치를 확률적으로 예측(Prediction)하여 경로를 생성한다.</li>
<li><strong>동역학적 윈도우 (Dynamic Window) 확장:</strong> 로봇의 속도 공간(Velocity Space)에서 탐색 가능한 영역을 계산하는 Dynamic Window Approach (DWA)에, 3DGS로 추적된 동적 장애물의 미래 점유 영역을 4D 시공간 제약 조건으로 추가한다. 이를 통해 고속 주행 시에도 동적 장애물과의 충돌을 수학적으로 보장(Safety Guarantee)할 수 있다.</li>
</ul>
<h3>4.2  물리적 상호작용을 위한 입자 기반 동역학 모델링</h3>
<p>가장 진보된 형태의 연구는 3D 가우시안을 물리 시뮬레이션의 ’입자’로 간주하여, 로봇이 물성을 학습하게 하는 것이다. <strong>Graph-Based Neural Dynamics Modeling</strong>  연구는 로봇이 물체와 상호작용하는 비디오로부터 물체의 물리적 특성을 학습하는 프레임워크를 제안한다.</p>
<ol>
<li><strong>입자화 (Particlization):</strong> 3DGS로 복원된 고밀도 가우시안을 다운샘플링(Downsampling)하여, 물체의 움직임을 대표하는 희소한 ’제어 입자(Control Particles)’를 선정한다. 이는 계산 복잡도를 낮추면서도 물체의 기하학적 형상을 유지한다.</li>
<li><strong>그래프 신경망 (GNN) 학습:</strong> 선정된 입자들을 노드(Node)로, 입자 간의 물리적 연결(탄성, 마찰 등)을 엣지(Edge)로 하는 그래프를 구성한다. 그리고 로봇의 행동(Action, 예: 밀기, 잡기)이 가해졌을 때 입자들이 어떻게 이동할지 예측하는 GNN 기반 동역학 모델을 학습한다.</li>
<li><strong>행동 조건 비디오 예측:</strong> 학습된 모델은 로봇이 “이 곰 인형의 팔을 잡아당기면 전체 형상이 어떻게 변할까?“라는 질문에 대해, 미래의 3D 가우시안 분포를 시뮬레이션하여 시각적 답변을 제공한다.</li>
</ol>
<p>이 기술은 유연 물체(Deformable Objects, 예: 옷, 인형, 밧줄, 액체) 조작과 같이 기존의 강체 물리 엔진으로는 모델링하기 어려운 복잡한 작업에서, 로봇이 시각적 피드백만으로 물성을 익히고 조작 계획(Manipulation Planning)을 수립하는 데 결정적인 역할을 한다. 이는 3DGS의 명시적 표현이 단순한 그래픽스 도구를 넘어 ’미분 가능한 물리 엔진(Differentiable Physics Engine)’의 구성 요소로 활용될 수 있음을 시사한다.</p>
<h3>4.3  데이터 중심 AI와 Sim-to-Real</h3>
<p>동적 객체 분리 기술은 데이터 생성 측면에서도 혁신을 가져왔다. 로봇 학습의 가장 큰 병목은 양질의 데이터 부족이다. <strong>EnerVerse</strong> 나 <strong>ExpanDyNeRF</strong> 와 같은 연구는 실제 동적 환경을 4D로 정밀하게 재구성한 뒤, 이를 시뮬레이터에 이식하여 로봇 학습을 위한 무한한 데이터를 생성한다.</p>
<p>정적 배경과 동적 객체가 깔끔하게 분리되어 있기 때문에, 시뮬레이션 상에서 다음과 같은 데이터 증강(Data Augmentation)이 가능해진다.</p>
<ul>
<li><strong>객체 재배치:</strong> 보행자의 이동 경로를 수정하거나 속도를 변경하여 다양한 충돌 시나리오 생성.</li>
<li><strong>배경 교체:</strong> 동일한 동적 객체의 움직임을 다양한 배경(맑은 날, 흐린 날, 다른 도시)에 합성.</li>
<li><strong>카메라 뷰 변경:</strong> 로봇이 가보지 않은 시점(Novel View)에서의 관측 데이터 생성.</li>
</ul>
<p>이러한 <strong>Sim-to-Real</strong> 파이프라인은 로봇이 실제 환경에서 겪을 수 있는 희귀한 케이스(Corner Cases)를 시뮬레이션에서 미리 학습하게 함으로써, 현실 세계에서의 적응력을 비약적으로 높여준다.</p>
<h2>5.  결론 및 요약</h2>
<p>동적 물체와 정적 배경의 분리 및 추적 기술은 로봇 공학에서 더 이상 선택 사항이 아니다. 이는 로봇이 ’정지된 박물관’이 아닌 ’살아있는 세계’에서 활동하기 위한 전제 조건이다. NeRF 기반의 기술들은 기하학적 정밀함과 강건한 포즈 추정을 통해 미지의 환경에 대한 고해상도 지도를 제공하며, 3DGS 기반의 기술들은 실시간 렌더링과 입자 기반의 물리적 제어 가능성을 통해 로봇의 즉각적인 반응과 상호작용을 지원한다.</p>
<p>특히 <strong>5.6.3</strong>절에서 다룬 핵심 기술들의 흐름은 ’관측(Observation)’에서 ‘이해(Understanding)’, 그리고 ’예측(Prediction)’으로 나아가고 있다. 정적 배경은 로봇에게 ’자신의 위치(Localization)’를 알려주는 닻(Anchor)이 되고, 분리된 동적 객체는 로봇에게 ‘주의해야 할 장애물’ 혹은 ’조작해야 할 대상’으로 정의된다. 향후 연구는 이러한 분리와 추적 알고리즘을 고성능 GPU 서버가 아닌 로봇의 엣지 디바이스(Edge Device)에서 실시간으로 수행하며, 단순한 기하학적 분리를 넘어 객체의 의도(Intention)까지 파악하는 의미론적 추론(Semantic Reasoning)과 결합하는 방향으로 발전할 것이다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>DynIBaR: Neural Dynamic Image-Based Rendering | Request PDF - ResearchGate, https://www.researchgate.net/publication/373313468_DynIBaR_Neural_Dynamic_Image-Based_Rendering</li>
<li>DynIBaR: Space-time view synthesis from videos of dynamic scenes - Google Research, https://research.google/blog/dynibar-space-time-view-synthesis-from-videos-of-dynamic-scenes/</li>
<li>RoDynRF: Robust Dynamic Radiance Fields, https://robust-dynrf.github.io/</li>
<li>(PDF) Robust Dynamic Radiance Fields - ResearchGate, https://www.researchgate.net/publication/366902449_Robust_Dynamic_Radiance_Fields</li>
<li>RoDyGS: Robust Dynamic Gaussian Splatting for Casual Videos - arXiv, https://arxiv.org/html/2412.03077v1</li>
<li>DynIBaR: Neural Dynamic Image-Based Rendering - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DynIBaR_Neural_Dynamic_Image-Based_Rendering_CVPR_2023_paper.pdf</li>
<li>Supplementary Mateiral DynIBaR: Neural Dynamic Image-Based …, https://dynibar.github.io/static/pdfs/supp.pdf</li>
<li>[2301.09632] HexPlane: A Fast Representation for Dynamic Scenes - arXiv, https://arxiv.org/abs/2301.09632</li>
<li>HexPlane: A Fast Representation for Dynamic Scenes - Ang Cao, https://caoang327.github.io/HexPlane/</li>
<li>LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling, https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_LocalDyGS_Multi-view_Global_Dynamic_Scene_Modeling_via_Adaptive_Local_Implicit_ICCV_2025_paper.pdf</li>
<li>Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis - Semantic Scholar, https://www.semanticscholar.org/paper/Dynamic-3D-Gaussians%3A-Tracking-by-Persistent-View-Luiten-Kopanas/8ba2c82fe675ede1d5c12fc4f97cf8ca3ebf1ca3</li>
<li>Dynamic 3D Gaussians, https://dynamic3dgaussians.github.io/</li>
<li>[2308.09713] Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis - arXiv, https://arxiv.org/abs/2308.09713</li>
<li>4D Reconstruction with Dynamic 3D Gaussian Particle - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Lin_Gaussian-Flow_4D_Reconstruction_with_Dynamic_3D_Gaussian_Particle_CVPR_2024_paper.pdf</li>
<li>Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle - arXiv, https://arxiv.org/html/2312.03431v1</li>
<li>Gaussian-Flow - NJU-3DV, https://nju-3dv.github.io/projects/Gaussian-Flow/</li>
<li>4D Gaussian Splatting for Real-Time Dynamic Scene Rendering, https://guanjunwu.github.io/4dgs/</li>
<li>4D Gaussian Splatting for Real-Time Dynamic Scene Rendering - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_4D_Gaussian_Splatting_for_Real-Time_Dynamic_Scene_Rendering_CVPR_2024_paper.pdf</li>
<li>Monocular Dynamic Gaussian Splatting is Fast and Brittle but Smooth Motion Helps, https://mmehas.github.io/files/DyGauBench_tmp.pdf</li>
<li>DOT: Dynamic Object Tracking for Visual SLAM, https://elib.dlr.de/146127/1/ICRA21_DOT.pdf</li>
<li>RoDyn-SLAM: Robust Dynamic Dense RGB-D SLAM With Neural Radiance Fields, https://www.researchgate.net/publication/382280847_RoDyn-SLAM_Robust_Dynamic_Dense_RGB-D_SLAM_With_Neural_Radiance_Fields</li>
<li>Motion Prediction Based on Multiple Futures for Dynamic Obstacle Avoidance of Mobile Robots - research.chalmers.se, https://research.chalmers.se/publication/526674/file/526674_Fulltext.pdf</li>
<li>The Dynamic Window Approach to Collision Avoidance 1 Introduction - Carnegie Mellon University’s Robotics Institute, https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf</li>
<li>Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling | OpenReview, https://openreview.net/forum?id=itKJ5uu1gW</li>
<li>EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation - arXiv, https://arxiv.org/html/2501.01895v3</li>
<li>Broadening View Synthesis of Dynamic Scenes from Constrained Monocular Videos - arXiv, https://arxiv.org/html/2512.14406v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>