<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:5.6.1 시간 차원(Time)의 통합: 변형 가능한(Deformable) NeRF</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>5.6.1 시간 차원(Time)의 통합: 변형 가능한(Deformable) NeRF</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 5. 뉴럴 3D 표현과 렌더링 (Neural 3D Representations)</a> / <a href="index.html">5.6 동적 환경과 4D 재구성 (Dynamic Scene Reconstruction)</a> / <span>5.6.1 시간 차원(Time)의 통합: 변형 가능한(Deformable) NeRF</span></nav>
                </div>
            </header>
            <article>
                <h1>5.6.1 시간 차원(Time)의 통합: 변형 가능한(Deformable) NeRF</h1>
<p>정적(Static)인 환경에서의 3차원 복원 기술은 로봇 공학의 인식(Perception) 시스템에서 오랜 기간 주류를 이루어왔다. 그러나 실제 로봇이 상호작용해야 하는 물리적 세계는 본질적으로 동적(Dynamic)이며, 시간의 흐름에 따라 객체의 위치가 변하거나 형상이 변형(Deformation)되는 비정상성(Non-stationarity)을 내포한다. 기존의 표준 Neural Radiance Fields (NeRF)는 정적 장면을 가정하고 학습되기에, 움직이는 물체가 포함된 장면에서는 심각한 아티팩트(Artifact)를 발생시키거나 동적 객체를 단순히 흐릿하게(Blurry) 처리하는 한계를 보였다. 이를 극복하기 위해 시간 차원(<span class="math math-inline">t</span>)을 3차원 공간(<span class="math math-inline">x, y, z</span>)에 통합하여 4차원 시공간(Spatio-temporal)을 모델링하는 기술이 등장하였으며, 이는 ‘Dynamic NeRF’ 또는 ’Deformable NeRF’라는 범주로 정의된다. 본 절에서는 정적 NeRF의 한계를 넘어 시간 가변적인 장면을 신경망으로 표현하기 위한 핵심 방법론인 정준 공간(Canonical Space)과 변형 필드(Deformation Field), 위상 변화(Topological Change) 대응을 위한 고차원 임베딩, 그리고 실시간 로봇 제어를 위한 명시적(Explicit) 표현 학습 기법들을 심도 있게 분석한다.</p>
<h2>1.  동적 장면 표현의 이론적 배경과 정준 공간의 도입</h2>
<p>동적 뷰 합성(Dynamic View Synthesis)의 핵심 문제는 시간 <span class="math math-inline">t</span>에 따라 기하학적 구조(Geometry)와 외형(Appearance)이 변화하는 장면을 어떻게 효율적으로 파라미터화(Parameterization) 할 것인가에 있다. 단순히 시간 <span class="math math-inline">t</span>를 NeRF의 입력으로 추가하여 <span class="math math-inline">(x, y, z, t, \theta, \phi) \rightarrow (c, \sigma)</span>로 매핑하는 방식은 시간적 중복성(Temporal Redundancy)을 활용하지 못하고, 각 시간 프레임마다 독립적인 형상을 학습해야 하는 비효율성을 초래한다. 따라서 현대의 대다수 방법론은 장면의 공통된 구조를 공유하는 ’정준 공간(Canonical Space)’과 시간 흐름에 따른 변화를 기술하는 ’변형 필드(Deformation Field)’로 문제를 분해(Decomposition)하는 접근을 취한다.</p>
<h3>1.1  정준 공간(Canonical Space)과 변형(Deformation)의 이원화</h3>
<p>이 접근법의 핵심은 관찰된 동적 장면을 기준이 되는 하나의 형태, 즉 ‘정준(Canonical)’ 상태와 그 상태로부터의 ’변형(Deformation)’으로 분리하여 학습하는 것이다. 이는 물리학의 라그랑주 관점(Lagrangian perspective)과 유사하게, 물체를 구성하는 입자의 기준 위치를 추적하는 방식이다.</p>
<p><strong>D-NeRF: Neural Radiance Fields for Dynamic Scenes</strong> 는 이러한 이원화 전략을 성공적으로 정립한 선구적인 연구이다. D-NeRF는 전체 학습 과정을 두 개의 주요 네트워크로 분리하여 설계한다.</p>
<ol>
<li>
<p><strong>변형 네트워크(Deformation Network, <span class="math math-inline">\Psi_t</span>):</strong> 특정 시간 <span class="math math-inline">t</span>에서의 관찰 공간(Observation Space) 좌표 <span class="math math-inline">x</span>를 입력받아, 해당 지점이 정준 공간의 어디에 대응되는지를 나타내는 변위(Displacement) <span class="math math-inline">\Delta x</span>를 예측한다.<br />
<span class="math math-display">
\Delta x = \Psi_t(x, t)
</span><br />
즉, 시간 <span class="math math-inline">t</span>의 관찰 공간에 있는 점 <span class="math math-inline">x</span>는 정준 공간의 좌표 <span class="math math-inline">x_{can} = x + \Delta x</span>로 매핑된다. 일반적으로 <span class="math math-inline">t=0</span>인 시점의 상태를 정준 공간으로 설정하여(<span class="math math-inline">\Psi_t(x, 0) = 0</span>), 시간 <span class="math math-inline">t=0</span>이 기준 프레임 역할을 하도록 제약 조건을 건다.</p>
</li>
<li>
<p><strong>정준 네트워크(Canonical Network, <span class="math math-inline">\Psi_x</span>):</strong> 변형 네트워크를 통해 정준 공간으로 매핑된 좌표 <span class="math math-inline">x_{can}</span>와 뷰 방향 <span class="math math-inline">d</span>를 입력받아, 해당 지점의 부피 밀도(<span class="math math-inline">\sigma</span>)와 색상(<span class="math math-inline">c</span>)을 출력한다.<br />
<span class="math math-display">
(c, \sigma) = \Psi_x(x + \Delta x, d)
</span></p>
</li>
</ol>
<p>이러한 구조적 분리는 네트워크가 시간 가변적인 3D 부피 밀도와 방사(Radiance)를 암시적으로(Implicitly) 학습하게 하며, 단순한 뷰 보간(Interpolation)이 아닌 물리적 형상의 변형을 기반으로 한 레이 캐스팅(Ray-casting)을 가능하게 한다. D-NeRF는 단안 카메라(Monocular camera)로 촬영된 희소한 뷰(Sparse view)만으로도 비강체(Non-rigid) 변형을 겪는 동적 장면을 효과적으로 복원할 수 있음을 입증하였다.</p>
<h3>1.2  역방향 와핑(Backward Warping)의 기하학적 의미</h3>
<p>D-NeRF와 유사 계열의 모델들은 주로 <strong>역방향 와핑(Backward Warping)</strong> 방식을 채택한다. 이는 렌더링하고자 하는 시점의 광선(Ray) 상의 샘플 포인트 <span class="math math-inline">x(t)</span>를 기준으로, 이 점이 정준 공간의 어디에서 왔는지를 역추적하는 방식이다.<br />
<span class="math math-display">
x_{canonical} = x(t) + \mathcal{D}(x(t), t)
</span><br />
여기서 <span class="math math-inline">\mathcal{D}</span>는 변형 필드를 의미한다. 역방향 와핑을 사용하는 주된 이유는 볼륨 렌더링(Volume Rendering) 과정에서의 효율성 때문이다. 광선을 따라 샘플링된 점들이 정준 공간의 어느 위치에 해당하는지를 즉시 계산하여 밀도와 색상을 조회(Query)할 수 있기 때문이다. 반면, 정준 공간에서 관찰 공간으로의 순방향 와핑(Forward Warping)은 정준 공간의 점이 현재 시점의 어디로 이동했는지를 찾아야 하므로, 렌더링 시점에 광선과 물체의 교차점을 찾기 위한 별도의 탐색 과정이 필요하여 계산 비용이 높다.</p>
<p>D-NeRF의 실험 결과에 따르면, 이러한 구조는 관절 운동(Articulated motion)이나 인체 포즈와 같은 복잡한 변형을 성공적으로 처리한다. 특히 그림자(Shadow)와 같은 조명 효과도 정준 공간의 표면이 변형됨에 따라 자연스럽게 왜곡되어 렌더링되는 것을 확인할 수 있다. 그러나 D-NeRF는 시간적으로 인접한 프레임 간의 변형이 지나치게 클 경우 변형 필드가 수렴하지 못하는 한계가 있으며, 캡처 프레임 레이트를 높여 이를 보완해야 한다.</p>
<h2>2.  강건한 비강체 변형 모델링: SE(3) 필드와 최적화 전략</h2>
<p>D-NeRF가 제안한 단순 변위(Translation) 기반의 변형 필드는 국소적인 이동을 표현하는 데는 적합하지만, 회전 운동을 포함한 강체 변환(Rigid Transformation)이나 복잡한 비강체 변형을 표현하는 데에는 비효율적이다. 예를 들어, 물체가 제자리에서 회전하는 경우, 회전 중심에서 멀어질수록 변위 벡터의 크기가 선형적으로 증가해야 하며, 이는 MLP가 학습해야 할 함수 공간의 복잡도를 불필요하게 높인다.</p>
<h3>2.1  Nerfies: SE(3) 필드를 통한 회전 운동의 효율적 표현</h3>
<p>Park et al. (2021)이 제안한 <strong>Nerfies: Deformable Neural Radiance Fields</strong>는 스마트폰으로 촬영한 ‘셀피’ 영상과 같은 캐주얼한 데이터셋에서 발생하는 비강체 변형을 다루기 위해 고안되었다. Nerfies는 변형의 표현 방식을 단순 이동 벡터에서 <strong>SE(3) 필드</strong>로 확장하였다.</p>
<p>SE(3) 필드는 3차원 공간의 각 점마다 6자유도의 강체 변환(Rigid transformation)을 예측한다. 이를 수학적으로 표현하기 위해 스크류 이론(Screw Theory)에 기반한 <strong>스크류 축(Screw Axis)</strong> 표현법 <span class="math math-inline">S = (r; v) \in \mathbb{R}^6</span>을 사용한다.</p>
<ul>
<li><span class="math math-inline">r \in \mathfrak{so}(3)</span>: 회전축(axis of rotation)과 회전각(<span class="math math-inline">\theta = \lVert r \rVert</span>)을 나타내는 벡터.</li>
<li><span class="math math-inline">v \in \mathbb{R}^3</span>: 병진 이동(Translation) 성분.</li>
</ul>
<p>스크류 축 <span class="math math-inline">S</span>의 지수 맵(Exponential map)을 통해 <span class="math math-inline">SE(3)</span> 변환 행렬 <span class="math math-inline">e^S</span>를 얻을 수 있으며, 이를 통해 점 <span class="math math-inline">x</span>를 변형시킨다.<br />
<span class="math math-display">
e^S = \begin{pmatrix} e^{[r]_\times} &amp; p \\ 0 &amp; 1 \end{pmatrix} \in SE(3)
</span></p>
<p><span class="math math-display">
x&#39; = e^S x
</span></p>
<p>여기서 <span class="math math-inline">[r]_\times</span>는 벡터 <span class="math math-inline">r</span>의 왜대칭 행렬(Skew-symmetric matrix)이다.</p>
<ul>
<li><strong>SE(3) 필드의 장점:</strong> 회전 운동을 표현할 때, 단순 변위 필드는 회전체 상의 모든 점에 대해 서로 다른 크기와 방향의 벡터를 예측해야 한다. 반면, SE(3) 필드는 회전체 전체에 대해 동일하거나 유사한 스크류 파라미터(회전축과 각도)만 예측하면 되므로, 공간적 연속성이 높고 최적화가 훨씬 용이하다. 이는 특히 사람의 머리 회전이나 팔의 움직임과 같은 관절 운동을 모델링할 때 D-NeRF 대비 월등한 성능을 보여준다.</li>
</ul>
<h3>2.2  탄성 정규화(Elastic Regularization)를 통한 물리적 타당성 확보</h3>
<p>변형 필드는 자유도가 매우 높기 때문에, 최적화 과정에서 국소 최적해(Local minima)에 빠지거나 물리적으로 불가능한 기괴한 변형을 생성할 위험이 있다. Nerfies는 이를 방지하기 위해 탄성 정규화(Elastic Regularization)를 도입하였다. 이는 변형 필드의 야코비안(Jacobian) <span class="math math-inline">J_T(x)</span>의 특이값(Singular values)을 제어하는 방식이다.<br />
<span class="math math-display">
\mathcal{L}_{elastic}(x) = \lVert \log \Sigma - \log I \rVert_F^2
</span><br />
여기서 <span class="math math-inline">\Sigma</span>는 야코비안 <span class="math math-inline">J_T(x)</span>의 특이값 대각 행렬이다. 이 손실 함수는 변형 필드가 국소적으로 강체 변환(회전 및 이동)에 가깝도록 유도하며, 비현실적인 부피 팽창이나 수축을 억제한다. 이는 연속체 역학(Continuum mechanics)에서 물체의 변형 에너지를 최소화하는 것과 유사한 원리로, 시각적 품질뿐만 아니라 기하학적 복원 정확도 또한 향상시킨다.</p>
<h3>2.3  Coarse-to-Fine 최적화: 주파수 윈도잉(Frequency Windowing)</h3>
<p>NeRF는 고주파수 성분을 학습하기 위해 위치 인코딩(Positional Encoding)을 사용하지만, 이는 변형 필드 학습 초기에는 오히려 노이즈에 과적합(Overfitting)되거나 수렴을 방해하는 요인이 될 수 있다. Nerfies는 이를 해결하기 위해 <strong>Coarse-to-Fine 최적화</strong> 전략을 제안한다.</p>
<p>학습 초기에는 위치 인코딩의 저주파 성분만을 활성화하고, 학습이 진행됨에 따라 고주파 성분의 가중치 <span class="math math-inline">\alpha</span>를 점진적으로 증가시키는 윈도우 함수(Window function)를 적용한다.<br />
<span class="math math-display">
\gamma_\alpha(x) = (w_0(\alpha)\sin(x), \dots, w_{m-1}(\alpha)\cos(2^{m-1}x))
</span><br />
여기서 가중치 <span class="math math-inline">w_k(\alpha)</span>는 <span class="math math-inline">\alpha</span>에 따라 부드럽게 0에서 1로 변한다. 이 기법은 초기에는 전체적인 움직임(Coarse motion)을 먼저 학습하여 큰 변형을 잡고, 후반부에는 미세한 표면 변화(Fine details)를 학습하게 함으로써 최적화의 안정성을 크게 높인다.</p>
<h2>3.  위상 변화(Topological Change)의 모델링: HyperNeRF</h2>
<p>D-NeRF나 Nerfies와 같은 변형 기반 방법론은 연속적인(Continuous) 매핑 함수(MLP)를 사용하므로, 공간적 연속성이 깨지는 현상을 표현하는 데 근본적인 한계가 있다. 예를 들어, 입을 다물고 있다가 벌리는 경우(구멍의 생성), 또는 가위로 종이를 자르는 경우(절단)와 같은 **위상 변화(Topological Change)**는 연속적인 변형 필드로는 표현할 수 없다. 변형 필드의 불연속성을 허용하면 미분 불가능한 지점이 생겨 역전파 학습이 불가능해지기 때문이다.</p>
<h3>3.1  레벨 셋(Level-set) 이론과 앰비언트 차원(Ambient Dimension)</h3>
<p>Park et al. (2021)은 <strong>HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields</strong>에서 Level-set method의 아이디어를 차용하여 이 문제를 해결한다. Level-set method는 <span class="math math-inline">N</span>차원에서 변화하는 위상을 <span class="math math-inline">N+1</span>차원 함수의 등위면(Level-set)의 단면(Slice)으로 표현함으로써, 고차원에서는 연속적인 함수임에도 저차원 투영에서는 위상 변화가 자연스럽게 나타나도록 한다.</p>
<p>HyperNeRF는 3차원 공간 <span class="math math-inline">(x, y, z)</span>에 추가적인 <strong>앰비언트 차원(Ambient Dimension)</strong> <span class="math math-inline">w</span>를 도입하여 4차원 이상의 ’하이퍼 스페이스(Hyper-space)’를 구성한다. 3D 공간에서 위상이 변하는 현상은 고차원 공간에 존재하는 고정된 고차원 템플릿(Hyper-space Template)을 서로 다른 각도나 위치에서 슬라이싱(Slicing)한 결과로 해석한다.</p>
<h3>3.2  앰비언트 슬라이싱 표면(Ambient Slicing Surface)</h3>
<p>HyperNeRF 모델은 입력 좌표 <span class="math math-inline">x</span>뿐만 아니라, 학습 가능한 임베딩 벡터 <span class="math math-inline">\omega_i</span>에 의해 결정되는 슬라이싱 표면의 좌표를 추가로 입력받는다.<br />
<span class="math math-display">
(x, d, \omega_i) \xrightarrow{\text{Slicing}} (x, w) \xrightarrow{\text{NeRF}} (c, \sigma)
</span><br />
여기서 앰비언트 좌표 <span class="math math-inline">w</span>는 별도의 MLP에 의해 예측되며, 이는 고차원 템플릿을 자르는 ’칼날’의 위치를 결정한다. 단순한 평면 슬라이싱(Axis-aligned slicing)은 표현력에 한계가 있어, HyperNeRF는 **변형 가능한 슬라이싱 표면(Deformable Slicing Surface)**을 제안한다. 이는 슬라이싱 평면을 구부리거나(bend) 휘게 만들 수 있어, 고차원 템플릿의 정보를 더욱 유연하게 3D 공간으로 가져올 수 있다.</p>
<ul>
<li><strong>수식적 표현:</strong><br />
<span class="math math-display">
w = H(x, \omega_i)
</span><br />
여기서 <span class="math math-inline">H</span>는 변형 가능한 슬라이싱 표면을 정의하는 MLP이다. 최종적으로 NeRF 네트워크는 <span class="math math-inline">(x, w)</span>를 입력으로 받아 밀도와 색상을 출력한다.</li>
</ul>
<p>실험 결과, HyperNeRF는 Nerfies 대비 보간(Interpolation) 오차를 4.1%, 새로운 뷰 합성 오차를 8.6% (LPIPS 기준) 감소시켰으며, 특히 입이 벌어지거나 물체가 쪼개지는 위상 변화가 포함된 장면에서 아티팩트 없는 깨끗한 복원 결과를 보여주었다. 이는 로봇이 물체를 조작하거나 절단하는 작업(Cutting task)을 수행할 때 환경을 정확하게 모델링하는 데 필수적인 기술이다.</p>
<h2>4.  실시간 로봇 제어를 위한 명시적 표현(Explicit Representation)</h2>
<p>초기 변형 가능한 NeRF 모델들(D-NeRF, Nerfies, HyperNeRF)은 순수 MLP 기반의 암시적(Implicit) 표현을 사용하여 메모리 효율은 좋으나, 학습과 렌더링 속도가 매우 느리다는 단점이 있다. D-NeRF의 경우 하나의 장면을 학습하는 데 수십 시간이 소요되기도 한다. 이는 실시간으로 환경을 인식하고 반응해야 하는 로봇 시스템에 적용하기에 치명적인 제약이다. 이를 해결하기 위해 복셀(Voxel), 그리드(Grid), 평면(Plane) 등의 명시적 자료구조를 결합하여 연산 효율성을 극대화하는 연구가 진행되었다.</p>
<h3>4.1  TiNeuVox: 시간 인식 신경 복셀 (Time-Aware Neural Voxels)</h3>
<p>Fang et al. (2022)이 제안한 <strong>TiNeuVox: Fast Dynamic Radiance Fields with Time-Aware Neural Voxels</strong>는 동적 장면을 명시적인 복셀 그리드로 표현하여 학습 속도를 획기적으로 단축했다.</p>
<p>기존의 정적 NeRF 가속화 기법(DirectVoxGO, Plenoxels 등)을 동적 장면에 적용하려면 4D 복셀 그리드(<span class="math math-inline">x, y, z, t</span>)가 필요한데, 이는 메모리 사용량이 기하급수적으로 증가하는 문제가 있다. TiNeuVox는 이를 해결하기 위해 <strong>시간 인식 복셀(Time-Aware Voxels)</strong> 개념을 도입했다.</p>
<ol>
<li><strong>작은 변형 네트워크(Tiny Deformation Network):</strong> 시간 <span class="math math-inline">t</span> 정보를 인코딩하여 공간 좌표 <span class="math math-inline">(x, y, z)</span>를 미세하게 변형시킨 <span class="math math-inline">(x&#39;, y&#39;, z&#39;)</span>를 생성한다.</li>
<li><strong>다중 거리 보간(Multi-distance Interpolation):</strong> 변형된 좌표를 사용하여 복셀 그리드에서 특징(Feature)을 추출한다. 이때 단일 위치가 아닌 여러 거리의 이웃 복셀들을 참조하여, 작은 움직임과 큰 움직임을 모두 포착할 수 있도록 한다.</li>
<li><strong>성능 및 효율성:</strong> TiNeuVox는 D-NeRF 대비 150배, HyperNeRF 대비 192배 빠른 학습 속도(약 8분 소요)를 달성하였다. 또한 메모리 사용량도 8MB 수준으로 억제하여, 로봇의 온보드 컴퓨터에서도 충분히 구동 가능한 효율성을 확보하였다.</li>
</ol>
<h3>4.2  차원 분해(Factorization) 기반 표현: K-Planes 및 HexPlane</h3>
<p>고차원 데이터를 효율적으로 표현하기 위해 텐서 분해(Tensor Decomposition) 기법, 특히 CP 분해(CANDECOMP/PARAFAC)를 4D 시공간에 적용한 연구들이 주목받고 있다. 이들은 4D 볼륨을 저차원 평면들의 조합으로 근사하여 메모리 효율과 학습 속도를 동시에 달성한다.</p>
<h4>4.2.1  K-Planes Factorization</h4>
<p>Fridovich-Keil et al. (2023)은 4D 시공간 볼륨을 <span class="math math-inline">k</span>개의 2차원 평면으로 분해하는 <strong>K-Planes</strong>를 제안했다. 4차원 공간의 경우, 총 <span class="math math-inline">\binom{4}{2} = 6</span>개의 평면(<span class="math math-inline">xy, xz, yz, xt, yt, zt</span>)으로 데이터를 투영한다.</p>
<ul>
<li>
<p><strong>수학적 모델:</strong> 4D 좌표 <span class="math math-inline">q = (x, y, z, t)</span>에 대한 특징값 <span class="math math-inline">f(q)</span>는 각 평면으로 투영된 좌표 <span class="math math-inline">\pi_c(q)</span>에서 조회한 특징 <span class="math math-inline">P_c</span>들의 **하다마드 곱(Hadamard Product)**으로 계산된다.<br />
<span class="math math-display">
f(q) = \prod_{c \in C} \psi(P_c, \pi_c(q))
</span><br />
여기서 <span class="math math-inline">\psi</span>는 쌍선형 보간(Bilinear interpolation) 함수이다.</p>
</li>
<li>
<p><strong>하다마드 곱의 중요성:</strong> 기존의 덧셈 기반 결합 대신 곱셈을 사용함으로써, 특정 평면에서의 값이 0이면 전체 특징이 0이 되는 ‘선택적(Selective)’ 활성화가 가능해진다. 이는 공간적으로 국소화된(Spatially localized) 신호를 표현하는 데 유리하며 렌더링 품질을 향상시킨다.</p>
</li>
<li>
<p><strong>로봇 응용:</strong> 정적 요소(공간 평면)와 동적 요소(시공간 평면)가 자연스럽게 분리되므로, 로봇이 배경과 움직이는 물체를 구분하여 인식하는 데 유용하다.</p>
</li>
</ul>
<h4>4.2.2  HexPlane</h4>
<p>Cao and Johnson (2023)의 <strong>HexPlane</strong> 역시 6개의 평면으로 분해하는 전략을 취한다. HexPlane은 K-Planes와 유사하지만, 특징 융합 및 디코딩 과정에서 미세한 차이를 보이며, 특히 데이터가 희소한(Sparse) 상황에서도 강건한 성능을 보이도록 시간 축의 평활도(Smoothness)를 제어하는 모듈을 포함한다. 이러한 평면 분해 기법들은 4D 데이터를 명시적인 2D 이미지 텍스처 형태로 저장할 수 있게 하여, 표준 CNN 기반의 이미지 압축 및 전송 기술을 그대로 활용할 수 있다는 장점이 있다. 이는 대역폭이 제한된 로봇 통신 환경에서 3D 비디오 데이터를 전송할 때 매우 유리하다.</p>
<table><thead><tr><th><strong>특징</strong></th><th><strong>TiNeuVox</strong></th><th><strong>K-Planes / HexPlane</strong></th></tr></thead><tbody>
<tr><td><strong>자료 구조</strong></td><td>3D Voxel Grid + Deformation</td><td>6개의 2D Feature Planes</td></tr>
<tr><td><strong>시간 모델링</strong></td><td>좌표 변형 &amp; 시간 임베딩</td><td>시공간(<span class="math math-inline">xt, yt, zt</span>) 평면 직접 학습</td></tr>
<tr><td><strong>메모리 효율</strong></td><td>중간 (Voxel 해상도에 의존)</td><td><strong>매우 우수</strong> (평면 분해로 <span class="math math-inline">O(N^3) \to O(N^2)</span>)</td></tr>
<tr><td><strong>학습 속도</strong></td><td>매우 빠름 (수 분)</td><td>매우 빠름 (수 분)</td></tr>
<tr><td><strong>주요 강점</strong></td><td>작은 움직임/큰 움직임 모두 강건</td><td>해석 가능성(Interpretability) 및 압축률</td></tr>
</tbody></table>
<h2>5.  차세대 패러다임: 4D Gaussian Splatting (4DGS)</h2>
<p>NeRF가 연속적인 볼륨 표현(Volumetric Representation)을 사용하는 반면, 최근에는 이산적인 점군(Point Cloud) 기반의 **3D Gaussian Splatting (3DGS)**이 렌더링 속도 면에서 혁신을 일으키고 있다. 이를 동적 장면으로 확장한 <strong>4D Gaussian Splatting</strong> 연구들이 급격히 증가하며 NeRF와 경쟁 및 상호보완 관계를 형성하고 있다.</p>
<h3>5.1  4DGS의 작동 원리 및 NeRF와의 비교</h3>
<p>4DGS는 장면을 수만 개의 3D 가우시안(Gaussian) 타원체로 표현하며, 각 가우시안은 위치, 회전, 크기, 불투명도, 색상(Spherical Harmonics) 정보를 가진다. 동적 장면을 표현하기 위해 4DGS는 <strong>HexPlane</strong>과 유사한 분해 기법을 사용하여 4D 변형 필드를 학습하거나, 가우시안 자체의 파라미터를 시간에 따라 변화시킨다.</p>
<ul>
<li><strong>렌더링 속도:</strong> NeRF는 픽셀당 수백 번의 MLP 연산(Ray-marching)이 필요한 반면, 4DGS는 래스터화(Rasterization) 기반으로 투영하므로 100 FPS 이상의 실시간 렌더링이 가능하다.</li>
<li><strong>메모리 및 저장:</strong> 4DGS는 가우시안의 수가 많아지면 메모리 사용량이 급증할 수 있으나, K-Planes와 같은 분해 기법을 적용하여 시공간 중복성을 제거함으로써 효율성을 높이고 있다.</li>
</ul>
<h3>5.2  로봇 공학적 관점에서의 선택</h3>
<p>로봇의 자율 주행 및 조작을 위해서는 <strong>정확도</strong>와 <strong>속도</strong>의 트레이드오프를 고려해야 한다.</p>
<ul>
<li><strong>NeRF (특히 K-Planes):</strong> 메모리 효율이 매우 높고, 물리적 연속성이 보장된 밀도 필드를 제공하므로 **경로 계획(Path Planning)**이나 <strong>물리 시뮬레이션</strong>에 유리하다. 충돌 검사 시 연속적인 공간 정보를 활용할 수 있다.</li>
<li><strong>4DGS:</strong> 압도적인 렌더링 속도로 인해 **실시간 원격 제어(Teleoperation)**를 위한 시각화나, <strong>고속 SLAM</strong> 시스템의 맵 표현에 유리하다. 그러나 가우시안 타원체 간의 빈 공간이나 아티팩트로 인해 정밀한 물리적 상호작용 계산에는 후처리가 필요할 수 있다.</li>
</ul>
<h2>6.  로봇 공학을 위한 응용: 인식, 제어, 그리고 시뮬레이션</h2>
<p>변형 가능한 NeRF 기술은 단순히 예쁜 이미지를 만드는 것을 넘어, 로봇이 세상을 이해하고 행동하는 방식을 혁신하고 있다.</p>
<h3>6.1  물리적 속성 추정 (NeRF-ysics)</h3>
<p>로봇이 유연한 물체(천, 고무, 음식 등)를 조작(Manipulation)하기 위해서는 형상뿐만 아니라 질량, 마찰력, 강성(Stiffness)과 같은 물리적 속성을 알아야 한다. <strong>NeRF-ysics</strong> 는 NeRF의 기하학적 표현(Occupancy field)을 미분 가능한 물리 시뮬레이터(Differentiable Physics Simulator)와 통합하였다. 비디오 관찰만으로 물체의 움직임을 역추적하여 물리 파라미터를 최적화(System Identification)하며, 이를 통해 로봇은 시각 정보만으로 물체의 단단함이나 미끄러움을 추정할 수 있게 된다. 이는 로봇 그리퍼가 물체를 파지할 때 적절한 힘을 조절하는 데 필수적이다.</p>
<h3>6.2  충돌 회피 및 경로 계획 (4D Reconstruction for Planning)</h3>
<p>자율 주행 로봇이나 드론은 동적 장애물을 실시간으로 회피해야 한다. <strong>DrivingRecon</strong> 이나 <strong>EgoMono4D</strong> 와 같은 최신 연구는 단안 카메라 영상으로부터 대규모 거리 환경의 4D 구조를 복원한다. 변형 가능한 NeRF는 동적 객체(차량, 보행자)의 미래 위치를 예측하고, 이를 3D 점유 그리드(Occupancy Grid) 형태로 로봇의 네비게이션 스택에 제공한다. 또한, NeRF의 미분 가능한 특성을 활용하여, 충돌 확률을 최소화하는 안전한 경로를 경사 하강법(Gradient Descent) 기반으로 최적화할 수 있다.</p>
<h3>6.3  동적 SLAM 및 데이터 증강</h3>
<p>동적 환경에서의 SLAM(Simultaneous Localization and Mapping)은 전통적으로 움직이는 물체를 노이즈로 간주하여 제거하려 했다. 그러나 <strong>K-Planes</strong>나 <strong>TiNeuVox</strong>를 활용한 동적 SLAM은 움직이는 물체까지 맵의 일부로 포함하여, 로봇의 위치 추정(Localization) 강건성을 높인다. 또한, <strong>MUSAL</strong> 과 같은 시스템은 다수의 로봇이 수집한 데이터를 클라우드 상에서 통합하여 작업 현장의 4D 디지털 트윈을 구축한다. 이렇게 구축된 고정밀 4D 환경은 강화학습(RL) 에이전트를 위한 무한한 시뮬레이션 데이터를 생성(Data Augmentation)하는 데 활용되어, 로봇의 학습 효율을 극대화한다.</p>
<h2>7.  결론</h2>
<p>시간 차원의 통합은 정적 세상을 바라보던 NeRF를 동적인 현실 세계를 다루는 강력한 도구로 변모시켰다. D-NeRF와 Nerfies를 통해 정립된 정준 공간과 변형 필드의 개념은 HyperNeRF를 통해 위상 변화의 영역으로 확장되었으며, TiNeuVox와 K-Planes, HexPlane과 같은 명시적 표현법을 통해 로봇 제어 루프에 통합될 수 있는 실시간성을 확보하였다. 나아가 4D Gaussian Splatting과의 융합은 렌더링 속도의 한계를 돌파하고 있다. 이러한 기술들은 로봇이 유연한 물체를 조작하고, 복잡한 인파 속을 주행하며, 물리적 상호작용을 시각 정보만으로 예측하는 ’시각 기반 물리 지능(Visual-Physical Intelligence)’의 핵심 기반이 될 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>DeVRF: Fast Deformable Voxel Radiance Fields for Dynamic Scenes, https://papers.neurips.cc/paper_files/paper/2022/file/eeb57fdf745eb31a3c7ef22c59a4661d-Paper-Conference.pdf</li>
<li>Dynamic NeRF: A Review - arXiv, https://arxiv.org/html/2405.08609v1</li>
<li>[Quick Review] D-NeRF: Neural Radiance Fields for Dynamic Scenes, https://liner.com/review/dnerf-neural-radiance-fields-for-dynamic-scenes</li>
<li>D-NeRF: Neural Radiance Fields for Dynamic Scenes, https://openaccess.thecvf.com/content/CVPR2021/papers/Pumarola_D-NeRF_Neural_Radiance_Fields_for_Dynamic_Scenes_CVPR_2021_paper.pdf</li>
<li>D-NeRF: Neural Radiance Fields for Dynamic Scenes - ResearchGate, https://www.researchgate.net/publication/346510555_D-NeRF_Neural_Radiance_Fields_for_Dynamic_Scenes</li>
<li>Nerfies: Deformable Neural Radiance Fields (Supplementary …, https://openaccess.thecvf.com/content/ICCV2021/supplemental/Park_Nerfies_Deformable_Neural_ICCV_2021_supplemental.pdf</li>
<li>NeRF for Dynamic Scenes: Literature Survey | by Vijendra Singh, https://medium.com/@vijendra1125/nerf-for-dynamic-scenes-8b22433fdfe9</li>
<li>NeRF-Supervision: Learning Dense Object Descriptors from Neural …, https://dspace.mit.edu/bitstream/handle/1721.1/153644/2203.01913v1.pdf?sequence=1&amp;isAllowed=y</li>
<li>Nerfies: Deformable Neural Radiance Fields - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2021/papers/Park_Nerfies_Deformable_Neural_Radiance_Fields_ICCV_2021_paper.pdf</li>
<li>HyperNeRF: A Higher-Dimensional Representation for …, https://hypernerf.github.io/</li>
<li>A Higher-Dimensional Representation for Topologically Varying …, <a href="https://graphics.stanford.edu/courses/cs348n-22-winter/PapersReferenced/HyperNeRF%202106.13228.pdf">https://graphics.stanford.edu/courses/cs348n-22-winter/PapersReferenced/HyperNeRF%202106.13228.pdf</a></li>
<li>A Higher-Dimensional Representation for Topologically Varying …, https://www.researchgate.net/publication/353068452_HyperNeRF_A_Higher-Dimensional_Representation_for_Topologically_Varying_Neural_Radiance_Fields</li>
<li>Fast Dynamic Radiance Fields with Time-Aware Neural Voxels - arXiv, https://arxiv.org/pdf/2205.15285</li>
<li>hustvl/TiNeuVox - Time-Aware Neural Voxels - GitHub, https://github.com/hustvl/TiNeuVox</li>
<li>K-Planes: Explicit Radiance Fields in Space, Time, and Appearance, https://www.researchgate.net/publication/367389203_K-Planes_Explicit_Radiance_Fields_in_Space_Time_and_Appearance</li>
<li>1월 23, 2026에 액세스, https://openaccess.thecvf.com/content/CVPR2023/papers/Fridovich-Keil_K-Planes_Explicit_Radiance_Fields_in_Space_Time_and_Appearance_CVPR_2023_paper.pdf</li>
<li>HexPlane: A Fast Representation for Dynamic Scenes, https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_HexPlane_A_Fast_Representation_for_Dynamic_Scenes_CVPR_2023_paper.pdf</li>
<li>MotionGS: Exploring Explicit Motion Guidance for … - ResearchGate, https://www.researchgate.net/publication/384811708_MotionGS_Exploring_Explicit_Motion_Guidance_for_Deformable_3D_Gaussian_Splatting/fulltext/6708996fdc91726ad3896b2b/MotionGS-Exploring-Explicit-Motion-Guidance-for-Deformable-3D-Gaussian-Splatting.pdf?origin=scientificContributions</li>
<li>Qualitative comparisons of 4DGS and our method on the D-nerf …, https://www.researchgate.net/figure/Qualitative-comparisons-of-4DGS-and-our-method-on-the-D-nerf-dataset_fig2_390038243</li>
<li>4D Gaussian Splatting: Dynamic Scene Rendering - Emergent Mind, https://www.emergentmind.com/topics/4d-gaussian-splatting-4dgs</li>
<li>A Differentiable Pipeline for Enriching NeRF-Represented Objects …, https://neural-implicit-workshop.stanford.edu/assets/pdf/lecleach.pdf</li>
<li>Large 4D Gaussian Reconstruction Model For Autonomous Driving, https://arxiv.org/html/2412.09043v1</li>
<li>Self-Supervised Monocular 4D Scene Reconstruction for Egocentric …, https://openaccess.thecvf.com/content/ICCV2025/papers/Yuan_Self-Supervised_Monocular_4D_Scene_Reconstruction_for_Egocentric_Videos_ICCV_2025_paper.pdf</li>
<li>A review of learning-based dynamics models for robotic manipulation, https://albertboai.com/assets/pdf/2025_scirobotics.adt1497.pdf</li>
<li>Neural Radiance Field Dynamic Scene SLAM Based on Ray … - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC11944775/</li>
<li>MUSAL: towards multisource 4D scene modeling by autonomous …, https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13207/1320704/MUSAL–towards-multisource-4D-scene-modeling-by-autonomous-robot/10.1117/12.3030904.full</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>