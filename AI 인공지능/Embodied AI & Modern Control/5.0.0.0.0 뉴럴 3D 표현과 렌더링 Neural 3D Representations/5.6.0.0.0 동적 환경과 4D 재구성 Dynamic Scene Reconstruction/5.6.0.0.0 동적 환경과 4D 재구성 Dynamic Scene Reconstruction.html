<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:5.6 동적 환경과 4D 재구성 (Dynamic Scene Reconstruction)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>5.6 동적 환경과 4D 재구성 (Dynamic Scene Reconstruction)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 5. 뉴럴 3D 표현과 렌더링 (Neural 3D Representations)</a> / <a href="index.html">5.6 동적 환경과 4D 재구성 (Dynamic Scene Reconstruction)</a> / <span>5.6 동적 환경과 4D 재구성 (Dynamic Scene Reconstruction)</span></nav>
                </div>
            </header>
            <article>
                <h1>5.6 동적 환경과 4D 재구성 (Dynamic Scene Reconstruction)</h1>
<p>로봇이 마주하는 물리적 세계는 결코 정지해 있지 않다. 바람에 흔들리는 나뭇가지, 거리를 활보하는 보행자, 형태가 끊임없이 변하는 유체나 천(Fabric), 그리고 로봇 자신의 상호작용으로 인해 변형되는 물체들은 Embodied AI가 해결해야 할 가장 본질적인 난제 중 하나인 ’시간(Time)’의 차원을 인식 프로세스에 도입할 것을 요구한다. 과거의 3D 비전 시스템이 <span class="math math-inline">x, y, z</span>의 정적인 공간 좌표를 복원하는 데 집중했다면, 현대의 로봇 지능은 시간축 <span class="math math-inline">t</span>를 포함한 4차원 시공간(Spacetime)의 연속적인 흐름을 이해하고 재구성해야 한다. 본 절에서는 정적인 뉴럴 렌더링을 넘어, 동적 환경을 실시간으로 표현하고 재구성하는 최신 기술들의 이론적 배경과 로봇 공학적 응용을 심도 있게 다룬다. 특히 NeRF 기반의 초기 시도들부터, 최근 실시간 렌더링과 제어 루프(Control Loop) 통합이 가능한 4D Gaussian Splatting(4DGS) 기술의 비약적인 발전 과정을 상세히 기술하고, 이것이 로봇의 인식(Perception)과 행동(Action)에 미치는 영향을 분석한다.</p>
<h2>1.  시공간 표현의 난제와 패러다임의 전환</h2>
<p>동적 장면 재구성(Dynamic Scene Reconstruction)은 컴퓨터 비전과 로보틱스 분야에서 오랫동안 ’성배(Holy Grail)’와 같은 도전 과제였다. 정적 장면에서는 다시점 기하학(Multi-view Geometry)의 핵심 가정인 ’광도 일관성(Photometric Consistency)’과 ’기하학적 불변성(Geometric Invariance)’이 성립한다. 즉, 서로 다른 시점에서 바라본 물체의 특징점은 3차원 공간상의 고정된 한 점에 대응된다는 가정이 유효하다. 그러나 동적 환경에서는 카메라가 움직이는 동안 장면 자체도 변화하기 때문에 이 가정이 붕괴된다. 동일한 3차원 좌표점이라 하더라도 시간 <span class="math math-inline">t</span>에 따라 그 위치, 색상, 투명도가 달라질 수 있으며, 이는 재구성 알고리즘에 심각한 모호성(Ambiguity)을 초래한다.</p>
<h3>1.1 차원의 저주와 기하학적 모호성</h3>
<p>수학적으로 동적 장면은 4차원 함수 <span class="math math-inline">F(x, y, z, t, \theta, \phi) \rightarrow (c, \sigma)</span>로 정의된다. 여기서 <span class="math math-inline">(x, y, z)</span>는 공간 좌표, <span class="math math-inline">t</span>는 시간, <span class="math math-inline">(\theta, \phi)</span>는 관측 방향, <span class="math math-inline">(c, \sigma)</span>는 색상과 밀도(혹은 불투명도)를 의미한다. 이 문제는 본질적으로 ’비설정성(Ill-posedness)’을 내포한다. 단안 카메라(Monocular Camera)로 촬영된 동영상에서 물체의 겉보기 변화가 실제 형상의 변형(Geometry Deformation) 때문인지, 조명의 변화(View-dependent Appearance) 때문인지, 아니면 카메라의 움직임(Egomotion) 때문인지를 구분하는 것은 매우 어렵다.</p>
<p>초기 접근법들은 시간 축을 이산적인(discrete) 프레임 단위로 나누어 각 시점마다 독립적인 재구성을 시도했다. 이를테면, 매 프레임마다 SfM(Structure from Motion)이나 MVS(Multi-View Stereo)를 수행하는 방식이다. 그러나 이 방식은 인접한 프레임 간의 시간적 일관성(Temporal Consistency)을 보장하지 못하여 결과물이 떨리거나(Jittering), 로봇의 메모리 제약상 막대한 저장 공간을 요구한다는 치명적인 단점이 있었다. 또한, 빠르게 움직이는 물체에 대해서는 모션 블러(Motion Blur)로 인해 특징점 추출 자체가 불가능한 경우가 빈번했다.</p>
<h3>1.2 NeRF에서 3DGS로의 전환: 명시적 표현의 부상</h3>
<p>딥러닝의 도입, 특히 NeRF(Neural Radiance Fields)의 등장은 이러한 한계를 극복할 새로운 가능성을 제시했다. NeRF는 좌표 기반의 MLP(Multi-Layer Perceptron)를 사용하여 씬을 암시적(Implicit)으로 표현함으로써 고품질의 뷰 합성을 가능케 했다. 동적 NeRF(D-NeRF, Nerfies 등)는 시간 <span class="math math-inline">t</span>를 네트워크의 입력으로 추가하거나 변형 필드를 학습시키는 방식으로 이를 확장했다.</p>
<p>그러나 NeRF 기반 방식은 렌더링 시 광선 투사(Ray-marching)를 위해 픽셀당 수백 번의 네트워크 추론이 필요하여, 수 Hz 미만의 렌더링 속도를 보였다. 이는 실시간성이 생명인 로봇의 제어 루프에 통합하기에는 치명적인 결함이었다. 로봇이 100Hz로 제어 명령을 내릴 때, 비전 시스템이 0.1Hz로 환경을 업데이트한다면 그 로봇은 동적 장애물을 회피할 수 없다.</p>
<p>반면, 2023년 등장한 3D Gaussian Splatting(3DGS)은 씬을 수백만 개의 3D 가우시안 타원체(Ellipsoid) 집합으로 명시적(Explicit)으로 표현한다. 이는 미분 가능한 래스터화(Differentiable Rasterization)를 통해 100 FPS 이상의 렌더링 속도를 보장하며, 로봇이 환경의 기하학적 구조를 즉각적으로 파악하는 데 유리하다. 최근 연구들은 이러한 3DGS의 장점을 동적 환경으로 확장하여, 실시간 4D 재구성 및 변형 물체 조작(Manipulation)에 적용하고 있다. 이제 우리는 NeRF에서 시작된 동적 표현의 이론이 어떻게 3DGS로 계승되고 발전되었는지 구체적인 방법론을 살펴본다.</p>
<h2>2.  변형 필드와 정준 공간 (Deformation Fields &amp; Canonical Space)</h2>
<p>동적 씬을 효율적으로 압축하고 표현하기 위한 핵심 아이디어는 ’정준 공간(Canonical Space)’과 ’변형 필드(Deformation Field)’의 분리다. 이 개념은 비단 컴퓨터 그래픽스뿐만 아니라 로봇이 물체의 고유한 형상(Topology)과 현재의 상태(Pose/Shape)를 구분하여 인지하는 데 중요한 역할을 한다. 로봇이 컵을 잡을 때, 컵의 본질적인 형상은 변하지 않지만 로봇 팔의 위치에 따라 상대적인 위치만 변하는 강체 변환(Rigid Transformation)과, 옷을 갤 때 옷의 형상 자체가 변하는 비강체 변환(Non-rigid Transformation)을 통합적으로 다룰 수 있는 프레임워크가 필요하다.</p>
<h3>2.1 정준 공간 (Canonical Space)의 정의</h3>
<p>정준 공간은 물체나 환경이 변형되지 않은 ’기준 상태(Reference State)’를 의미한다. 예를 들어, 걸어가고 있는 사람을 재구성할 때, 정준 공간에서의 사람은 ’T-포즈’나 ’A-포즈’와 같은 고정된 표준 자세를 취하고 있다. 이 공간 내의 모든 3D 포인트 <span class="math math-inline">x_{can}</span>은 시간 <span class="math math-inline">t</span>에 영향을 받지 않는 고유한 속성(색상 <span class="math math-inline">c</span>, 불투명도 <span class="math math-inline">\alpha</span>, 기본 기하 구조)을 가진다. 3DGS의 맥락에서 보면, 정준 공간은 시간 <span class="math math-inline">t=0</span> 혹은 기준이 되는 시점에서의 3D 가우시안 분포 <span class="math math-inline">G_{can} = { \mu_{can}, \Sigma_{can}, c, \alpha }</span>로 정의된다.</p>
<p>이 접근법의 장점은 메모리 효율성이다. 물체의 텍스처나 세밀한 기하학적 정보는 정준 공간에 한 번만 저장하면 되며, 시간 <span class="math math-inline">t</span>에 따라 변하는 것은 오직 좌표의 이동(displacement)뿐이다. 이는 로봇이 긴 시간 동안 환경을 관측할 때 데이터가 기하급수적으로 늘어나는 것을 방지한다.</p>
<h3>2.2 변형 필드 (Deformation Field)의 메커니즘</h3>
<p>변형 필드는 관측 공간(Observation Space, 시간 <span class="math math-inline">t</span>에서의 실제 공간)의 좌표 <span class="math math-inline">x_t</span>를 정준 공간의 좌표 <span class="math math-inline">x_{can}</span>으로 매핑하거나(Backward Warping), 반대로 정준 공간의 가우시안을 시간 <span class="math math-inline">t</span>에 맞게 이동, 회전, 스케일링하는 함수(Forward Warping)다. 3DGS에서는 주로 Forward Warping을 사용하여 가우시안을 직접 이동시킨다.<br />
<span class="math math-display">
x_t = x_{can} + \Delta x(x_{can}, t)
</span><br />
여기서 <span class="math math-inline">\Delta x</span>는 시간 <span class="math math-inline">t</span>와 위치 <span class="math math-inline">x_{can}</span>에 따라 결정되는 변위 벡터다. 딥러닝 모델은 이 <span class="math math-inline">\Delta x</span>를 예측하도록 학습된다.</p>
<h4>2.2.1  MLP 기반 변형 (Implicit Deformation)</h4>
<p>초기 D-NeRF와 같은 연구에서는 MLP 네트워크 <span class="math math-inline">\Psi_t</span>를 사용하여 변형을 모델링했다.<br />
<span class="math math-display">
(x_{can}, d_{can}) = \Psi_t(x, d, t)
</span><br />
이 방식은 부드러운 변형(Non-rigid deformation)을 표현하는 데 적합하지만, 네트워크의 용량 한계로 인해 고주파수(High-frequency)의 빠른 움직임(예: 펄럭이는 옷깃, 빠르게 회전하는 바퀴)을 포착하는 데 어려움이 있다. 또한, MLP의 깊이가 깊어질수록 추론 속도가 느려져 실시간 로봇 제어에 병목이 된다.</p>
<h4>2.2.2  그리드 및 평면 기반 변형 (Explicit/Hybrid Deformation)</h4>
<p>최신 4DGS 연구들(4D-GS, HexPlane, K-Planes 등)은 MLP의 느린 속도를 극복하기 위해 명시적인 특징 격자(Feature Grid)나 평면 분해(Plane Factorization)를 사용하여 변형 필드를 가속화한다. 이들은 4차원 시공간 텐서를 저차원 평면으로 분해함으로써 연산량과 메모리 사용량을 획기적으로 줄인다.</p>
<p>HexPlane이나 K-Planes는 4차원 시공간 <span class="math math-inline">(x, y, z, t)</span>를 6개의 2차원 평면 (<span class="math math-inline">xy, xz, yz, xt, yt, zt</span>)으로 분해하여 표현한다. 이를 통해 메모리 사용량을 <span class="math math-inline">O(N^4)</span>에서 <span class="math math-inline">O(N^2)</span> 수준으로 줄이면서도, MLP보다 훨씬 빠른 학습과 추론 속도를 달성한다. 각 평면에서 이중선형 보간(Bilinear Interpolation)을 통해 얻은 특징 벡터들은 다음과 같이 결합된다 :<br />
<span class="math math-display">
Feature(x, y, z, t) = \bigoplus_{plane \in \{xy, \dots, zt\}} P_{plane}(proj_{plane}(x, y, z, t))
</span><br />
여기서 <span class="math math-inline">\bigoplus</span>는 Hadamard Product(요소별 곱) 혹은 Concatenation(결합) 연산을 의미한다. 이렇게 얻어진 특징 벡터(Feature Vector)는 매우 작은 MLP(Tiny MLP)를 거쳐 가우시안의 위치 변화량(<span class="math math-inline">\Delta \mu</span>), 회전 변화량(<span class="math math-inline">\Delta r</span>), 스케일 변화량(<span class="math math-inline">\Delta s</span>)을 출력한다.</p>
<h2>3.  위상 변화와 4D 표현의 한계 극복 (Handling Topology Changes)</h2>
<p>변형 필드 방식은 물체가 휘어지거나 늘어나는 연속적인 변형(Diffeomorphic deformation)은 잘 표현하지만, 물체가 찢어지거나, 입이 벌어지거나, 새로운 물체가 등장하는 것과 같은 ’위상 변화(Topological Change)’를 다루는 데는 근본적인 한계가 있다. 연속 함수인 변형 필드는 공간의 불연속적인 절단(Discontinuity)을 표현할 수 없기 때문이다. 예를 들어, 사람이 입을 다물고 있다가 벌릴 때, 입술 사이의 공간은 ’없음’에서 ’있음’으로 위상이 변하는데, 단순한 좌표 이동만으로는 이를 자연스럽게 표현하기 어렵다.</p>
<h3>3.1 HyperNeRF: 고차원 슬라이싱 (Level-Set Methods)</h3>
<p>이 문제를 해결하기 위해 제시된 획기적인 개념이 HyperNeRF이다. HyperNeRF는 3차원 공간을 더 높은 차원(Hyperspace, 예를 들어 4차원 이상의 공간)에 존재하는 고차원 형상의 ’슬라이스(Slice)’로 간주한다.</p>
<p>위상수학(Topology)에서 레벨 셋(Level-set) 방식은 3차원의 복잡한 형상 변화를 고차원 함수의 등고선 변화로 매끄럽게 표현할 수 있다. 비유하자면, 2차원 평면에서 두 개의 원이 합쳐져 하나의 원이 되는 과정은 2차원 내에서는 위상적 불연속이 발생하지만, 3차원 공간상의 ‘바지(Pants)’ 모양 도형을 수평으로 자르며 내려오면 두 개의 원(다리 부분)이 자연스럽게 하나(허리 부분)로 합쳐지는 연속적인 슬라이싱 과정으로 표현된다.</p>
<p>HyperNeRF는 이를 뉴럴 렌더링에 적용하여, 씬을 표현하는 네트워크에 ‘주변 차원(Ambient Dimension)’ 좌표 <span class="math math-inline">w</span>를 추가 입력으로 넣는다.<br />
<span class="math math-display">
F(x, y, z, t, w) \rightarrow (c, \sigma)
</span><br />
여기서 변형 필드는 물체의 기하학적 위치를 조정하고, 주변 차원 <span class="math math-inline">w</span>는 물체의 위상(Topology)을 결정하는 슬라이싱 평면의 위치를 조정한다. 연구 결과에 따르면, HyperNeRF는 입을 벌리거나 표정이 급격하게 변하는 사람의 얼굴과 같은 위상 변화를 아티팩트 없이 재구성할 수 있으며, 기존 Nerfies 대비 보간(Interpolation) 오차를 8.6% 감소시켰다.</p>
<h3>3.2 DGS에서의 위상 처리 (Topology in 4DGS)</h3>
<p>3DGS 기반 방법론에서도 위상 변화는 중요한 이슈다. 3DGS는 포인트 클라우드 기반이므로 위상 변화에 더 유연할 것 같지만, 가우시안들이 연결성(Connectivity) 정보 없이 독립적으로 존재하기 때문에 오히려 시간적 일관성이 깨지고 ’플로터(Floater)’라 불리는 노이즈가 발생하기 쉽다.</p>
<p>초기 4DGS 연구들은 가우시안의 투명도(Opacity, <span class="math math-inline">\alpha</span>)를 시간 <span class="math math-inline">t</span>에 따라 조절함으로써 가우시안을 끄거나 켜는 방식으로 위상 변화를 근사했다. 즉, 특정 시점에서 가우시안의 <span class="math math-inline">\alpha</span>가 0이 되면 해당 가우시안은 사라진 것으로 간주된다.</p>
<p>그러나 최근의 Topo4D와 같은 연구는 단순히 가우시안을 숨기는 것을 넘어, 가우시안 간의 연결성(Connectivity)과 매니폴드(Manifold) 구조를 보존하면서 위상을 변경하는 기법을 제안한다. Topo4D는 가우시안 초기화 시점에 3D 메쉬(Mesh)의 정점(Vertex)과 가우시안을 일대일 대응시키고, 메쉬의 엣지 정보를 통해 인접 가우시안 간의 거리와 회전을 제약한다. 이는 로봇이 3D 메쉬를 추출하여 물리 시뮬레이션에 활용해야 할 때, 구멍이 뚫리거나 비정상적인 기하 구조가 생성되는 것을 방지한다. 실험 결과, Topo4D는 기존 방식 대비 기하학적 오차를 크게 줄이면서도 8K 해상도의 텍스처를 유지하는 고품질 4D 아바타 생성을 가능하게 했다.</p>
<h2>4.  실시간 4D 렌더링을 위한 4D Gaussian Splatting 기술</h2>
<p>로봇 공학 관점에서 4DGS가 가지는 가장 큰 의의는 **실시간성(Real-time Performance)**과 **명시적 기하 정보(Explicit Geometry)**의 결합이다. 기존 NeRF 기반 방식이 GPU 클러스터에서도 실시간 렌더링이 어려웠던 반면, 4DGS는 단일 GPU(예: RTX 3090, 4090)에서 80 FPS 이상의 고해상도 렌더링과 30분 이내의 학습 속도를 달성했다. 이는 로봇이 센서 데이터를 수집하고 즉각적으로 환경 모델을 업데이트할 수 있음을 시사한다.</p>
<h3>4.1 DGS의 핵심 아키텍처</h3>
<p>일반적인 4DGS 프레임워크는 크게 두 부분으로 구성된다: (1) 3D 가우시안들의 초기 집합(Canonical Gaussians), (2) 시간 <span class="math math-inline">t</span>에 따른 가우시안 속성의 변화를 예측하는 변형 네트워크(Deformation Network).</p>
<ol>
<li>
<p><strong>가우시안 초기화:</strong> SfM(Structure from Motion)으로 얻은 포인트 클라우드나 무작위 초기화를 통해 3D 가우시안의 위치(<span class="math math-inline">\mu</span>), 공분산(<span class="math math-inline">\Sigma</span>), 색상(SH Coefficients), 불투명도(<span class="math math-inline">\alpha</span>)를 설정한다. 이 초기 가우시안들은 <span class="math math-inline">t=0</span> 시점의 형상을 나타낸다.</p>
</li>
<li>
<p><strong>시공간 인코딩 (Spatiotemporal Encoding):</strong> 입력된 시간 <span class="math math-inline">t</span>와 가우시안의 위치 <span class="math math-inline">x</span>를 결합하여 특징 벡터를 추출한다. 이때 앞서 언급한 HexPlane이나 4D Hash Encoding(Grid4D)과 같은 효율적인 구조를 사용하여 4차원 정보를 압축 저장한다. Grid4D는 다중 해상도 해시 테이블(Multi-resolution Hash Table)을 사용하여 공간과 시간의 지역적 특징을 빠르게 조회(Query)한다.</p>
</li>
<li>
<p><strong>속성 디코딩 (Attribute Decoding):</strong> 추출된 특징 벡터 <span class="math math-inline">h</span>를 경량 MLP에 통과시켜 가우시안의 변화량(<span class="math math-inline">\Delta \mu, \Delta r, \Delta s, \Delta \alpha</span>)을 계산한다.<br />
<span class="math math-display">
(\Delta \mu, \Delta r, \Delta s) = \text{MLP}(h)
</span><br />
최종적으로 시간 <span class="math math-inline">t</span>에서의 가우시안 <span class="math math-inline">G_t</span>는 초기 가우시안 <span class="math math-inline">G_0</span>에 변화량을 더하여 생성된다: <span class="math math-inline">G_t = G_0 + \Delta G</span>. 여기서 중요한 점은, 모든 가우시안에 대해 거대한 MLP를 통과시키는 것이 아니라, 공간적으로 인접한 가우시안들이 특징 격자(Grid)를 공유함으로써 연산 효율을 높인다는 것이다.</p>
</li>
<li>
<p><strong>미분 가능한 스플래팅 (Differentiable Splatting):</strong> 변형된 <span class="math math-inline">G_t</span>를 2D 이미지 평면에 투영하고, 픽셀별 색상을 계산한다. 이 과정은 전체 파이프라인에 대해 미분 가능하므로, 렌더링된 이미지와 실제 이미지 간의 오차(Photometric Loss)를 역전파하여 가우시안 속성과 변형 네트워크를 동시에 최적화할 수 있다.</p>
</li>
</ol>
<h3>4.2 성능 비교: NeRF vs. 4DGS</h3>
<p>아래 테이블은 주요 동적 씬 재구성 알고리즘들의 성능을 D-NeRF 데이터셋 기준으로 비교한 것이다. 여기서 주목할 점은 4D-GS 계열이 NeRF 계열과 유사하거나 더 높은 화질(PSNR)을 보여주면서도, 렌더링 속도(FPS) 면에서 수십 배에서 수백 배의 성능 향상을 이뤘다는 것이다.</p>
<table><thead><tr><th><strong>알고리즘</strong></th><th><strong>기반 기술</strong></th><th><strong>PSNR (↑)</strong></th><th><strong>SSIM (↑)</strong></th><th><strong>LPIPS (↓)</strong></th><th><strong>학습 시간</strong></th><th><strong>렌더링 FPS (↑)</strong></th></tr></thead><tbody>
<tr><td>D-NeRF [Pumarola et al.]</td><td>NeRF+MLP</td><td>31.14</td><td>0.976</td><td>0.046</td><td>~20시간</td><td>&lt; 1</td></tr>
<tr><td>TiNeuVox [Fang et al.]</td><td>NeRF+Voxel</td><td>32.67</td><td>0.970</td><td>0.040</td><td>28분</td><td>1.5</td></tr>
<tr><td>HexPlane [Cao et al.]</td><td>NeRF+Plane</td><td>31.04</td><td>0.970</td><td>0.040</td><td>11분</td><td>2.5</td></tr>
<tr><td>K-Planes [Fridovich-Keil et al.]</td><td>NeRF+Plane</td><td>31.61</td><td>0.970</td><td>-</td><td>52분</td><td>0.97</td></tr>
<tr><td><strong>4D-GS</strong></td><td><strong>GS+Plane</strong></td><td><strong>34.05</strong></td><td><strong>0.980</strong></td><td><strong>0.020</strong></td><td><strong>8분</strong></td><td><strong>82+</strong></td></tr>
<tr><td>SP-GS</td><td>GS+Superpoint</td><td>34.20</td><td>0.985</td><td>0.018</td><td>10분</td><td>227</td></tr>
</tbody></table>
<p>이러한 속도의 차이는 로봇 공학에서 결정적이다. 1 FPS 미만의 속도는 오프라인 분석용으로는 적합할지 몰라도, 로봇이 실시간으로 동적 장애물을 회피하거나 상호작용하기에는 턱없이 부족하다. 80 FPS 이상의 속도는 카메라의 입력 속도(보통 30~60 FPS)를 상회하므로, 로봇은 지연(Latency) 없이 환경의 변화를 인식하고 대응할 수 있다.</p>
<h2>5.  효율성의 극대화: 시공간 분해와 압축 (Factorization &amp; Compression)</h2>
<p>로봇 시스템, 특히 모바일 로봇이나 드론과 같이 엣지 컴퓨팅(Edge Computing) 환경에서 동작하는 시스템은 메모리와 연산 자원이 제한적이다. 4DGS가 빠르긴 하지만, 수백만 개의 가우시안과 4차원 그리드를 저장하는 것은 여전히 부담이 될 수 있다. 따라서 시공간 정보를 효율적으로 분해하고 압축하는 기술이 필수적이다.</p>
<h3>5.1 텐서 분해 (Tensor Decomposition)</h3>
<p>HexPlane과 K-Planes가 사용하는 평면 분해 기법은 텐서 분해 이론에 기반한다. 4차원 텐서를 저차원 평면들의 외적(Outer Product)으로 근사함으로써, 모델의 파라미터 수를 <span class="math math-inline">O(N^4)</span>에서 <span class="math math-inline">O(N^2)</span>로 줄인다. 최근 연구인 Tensor4D는 이를 더욱 발전시켜 CP 분해(CANDECOMP/PARAFAC Decomposition)나 VM 분해(Vector-Matrix Decomposition)를 적용하여 압축 효율을 극대화했다. 이는 로봇의 온보드 메모리에 고해상도 4D 맵을 적재할 수 있게 해 준다.</p>
<h3>5.2 Grid4D와 해시 인코딩</h3>
<p>Grid4D는 Instant-NGP에서 제안된 다중 해상도 해시 인코딩(Multi-resolution Hash Encoding)을 4차원으로 확장했다. 공간 <span class="math math-inline">(x, y, z)</span>와 시간 <span class="math math-inline">t</span>를 결합한 해시 테이블을 사용하여, 빈 공간(Empty Space)에는 메모리를 할당하지 않고 정보가 밀집된 표면 부근에만 해시 엔트리를 집중시킨다. 이를 통해 전체 모델 크기를 수십 MB 수준으로 줄이면서도 4K 해상도의 렌더링 품질을 유지할 수 있다.</p>
<h3>5.3 Superpoint 기반 압축</h3>
<p>SP-GS(Superpoint Gaussian Splatting)는 인접한 가우시안들을 ’슈퍼포인트(Superpoint)’라는 단위로 묶어서 관리한다. 개별 가우시안마다 변형을 예측하는 대신, 슈퍼포인트 단위로 변형을 예측하고 내부의 가우시안들은 국소적인 강체 변환을 따른다고 가정한다. 이 방식은 연산량을 크게 줄여 200 FPS 이상의 초고속 렌더링을 가능하게 하며, 로봇의 SLAM 시스템과 결합하여 대규모 환경을 실시간으로 매핑하는 데 유리하다.</p>
<h2>6.  로봇 인식을 위한 동적 장면의 활용 (Perception &amp; Navigation)</h2>
<p>4D 재구성 기술은 단순한 시각화(Visualization)를 넘어, 로봇이 환경을 이해하고 행동을 계획하는 데 직접적인 정보를 제공한다. Embodied AI는 이 ’살아있는 지도’를 통해 무엇을 할 수 있는가?</p>
<h3>6.1  동적 장애물 회피와 내비게이션 (Navigation &amp; Obstacle Avoidance)</h3>
<p>전통적인 SLAM(Simultaneous Localization and Mapping)은 동적 물체를 아웃라이어(Outlier)로 취급하여 맵에서 제거하려 노력했다. 그러나 자율주행이나 서비스 로봇 환경에서는 동적 물체(보행자, 차량)가 오히려 가장 중요한 피해야 할 대상이다.</p>
<ul>
<li><strong>충돌 검사 (Collision Checking):</strong> 3DGS는 씬을 명시적인 타원체(Ellipsoid)들의 집합으로 표현하므로, 로봇의 충돌 모델(예: 로봇 팔이나 차체를 감싸는 타원체)과 환경 가우시안 간의 교차 검사(Intersection Test)를 수학적으로 빠르게 수행할 수 있다. 두 타원체의 교차 여부는 대수적인 부등식으로 판별 가능하므로, NeRF와 같이 공간상의 밀도(Density)를 일일이 적분해야 하는 방식에 비해 계산 비용이 훨씬 저렴하다.</li>
<li><strong>Splat-Nav:</strong> 최근 연구인 Splat-Nav는 3DGS 맵 상에서 실시간으로 경로를 계획하고 위치를 추정하는 프레임워크를 제안했다. 이는 25Hz 이상의 속도로 로봇의 위치(Pose)를 추정하면서 동시에 장애물을 회피하는 경로를 생성한다. Splat-Nav는 가우시안 맵을 기반으로 안전한 비행 복도(Safe Flight Corridor)를 생성하고, 이를 통해 드론이 복잡한 환경에서도 충돌 없이 고속 비행을 할 수 있음을 보였다.</li>
<li><strong>CoDa-4DGS (자율주행):</strong> 자율주행 시나리오에서는 배경(도로, 건물)은 정적이지만 차량과 보행자는 동적이다. CoDa-4DGS는 2D 파운데이션 모델(Segmentation Model, 예: SAM)을 활용하여 의미론적(Semantic) 정보를 4DGS에 주입하고, 동적 물체와 정적 배경을 분리하여 학습한다. 이를 통해 로봇은 움직이는 물체의 미래 궤적을 예측하거나, 동적 물체를 제거한 깨끗한 정적 맵을 구축하여 장기적인 위치 추정(Localization)에 활용할 수 있다.</li>
</ul>
<h3>6.2  동적 SLAM과 맵 유지보수</h3>
<p>로봇이 장기간 운영되기 위해서는 환경의 변화(가구 재배치, 물건 이동)를 맵에 반영해야 한다. 정적인 3DGS 맵은 시간이 지나면 현실과 괴리(Drift)가 발생한다. 동적 3DGS 기술은 ’망각(Forgetting)’과 ‘생성(Densification)’ 과정을 통해 맵을 업데이트한다.</p>
<p>최신 연구들은 베이지안 필터링(Bayesian Filtering)이나 칼만 필터(Kalman Filter)의 개념을 3DGS에 도입하여, 기존 위치에 있던 가우시안의 신뢰도(Confidence)가 떨어지면(새로운 관측과 불일치하면) 이를 제거(Pruning)하고, 새로운 위치에 가우시안을 생성함으로써 최신 상태의 4D 맵을 유지한다. 이는 ’Lifelong SLAM’의 핵심 기술로, 로봇이 며칠, 몇 달 동안 동일한 공간에서 활동할 수 있게 하는 기반이 된다.</p>
<h2>7.  로봇 조작과 상호작용 (Manipulation &amp; Interaction)</h2>
<p>로봇 매니퓰레이션 분야에서 천(Cloth), 케이블(Cable), 유동체와 같은 비정형 물체(Deformable Object)를 다루는 것은 난제 중의 난제였다. 물체의 형상이 접촉에 의해 시시각각 변하기 때문에, 사전에 정의된 CAD 모델을 사용할 수 없기 때문이다. 4DGS는 비정형 물체의 상태 추정(State Estimation)에 혁신을 가져왔다.</p>
<h3>7.1 실시간 형상 추적 및 물리 시뮬레이션</h3>
<p>4DGS 기반의 추적기(Tracker)는 비정형 물체의 표면을 가우시안으로 덮고, 로봇 팔이 물체를 누르거나 당길 때 발생하는 변형을 실시간으로 업데이트한다. 이는 기존의 FEM(Finite Element Method) 기반 물리 시뮬레이션보다 계산이 빠르며, RGB-D 카메라 입력만으로도 현재 물체의 형상을 높은 정확도로 복원한다.</p>
<ul>
<li><strong>RoboSplatter:</strong> RoboSplatter와 같은 프레임워크는 3DGS 렌더링을 MuJoCo나 Isaac Sim과 같은 물리 시뮬레이터와 통합한다. 로봇은 가상 환경에서 4DGS로 표현된 물체를 조작하는 강화학습(Reinforcement Learning)을 수행하고, 이를 실제 환경(Sim-to-Real)으로 전이한다. 4DGS는 텍스처와 형상을 동시에 제공하므로, 시각 기반 강화학습(Visual RL) 에이전트에게 풍부한 상태 정보를 제공하여 학습 효율을 높인다.</li>
<li><strong>물리적 일관성 (Physical Consistency):</strong> 단순한 시각적 복원을 넘어, 물리학 법칙을 강제하는 연구도 진행되고 있다. PhysGaussian이나 EvoGS와 같은 연구는 가우시안에 질량, 탄성 계수와 같은 물리적 속성을 부여하고, 뉴턴의 운동 법칙을 만족하도록 변형 필드를 학습시킨다. 이는 시각적으로 그럴듯할 뿐만 아니라 물리적으로도 타당한 4D 재구성을 가능하게 하며, 로봇이 물체의 무게나 재질을 추정하는 데 도움을 준다.</li>
</ul>
<h2>8.  결론 및 미래 전망</h2>
<p>동적 환경의 4D 재구성은 로봇이 정적인 관찰자에서 벗어나, 변화하는 세계의 흐름을 이해하고 그 안에서 능동적으로 행동하는 주체가 되기 위한 필수적인 기술이다. 본 절에서 살펴본 바와 같이, D-NeRF에서 시작된 시공간 표현 연구는 3D Gaussian Splatting의 등장으로 실시간성이라는 강력한 무기를 얻게 되었다.</p>
<p>HexPlane과 같은 효율적인 인코딩 기법은 4차원 데이터의 처리 비용을 획기적으로 낮추었으며, Topo4D와 같은 위상 처리 기술은 기하학적 정밀도를 높였다. 무엇보다 CoDa-4DGS, Splat-Nav와 같은 로봇 전용 응용 연구들은 이 기술이 실험실을 넘어 자율주행차와 서비스 로봇의 핵심 모듈로 자리 잡고 있음을 보여준다.</p>
<p>앞으로의 연구는 <strong>(1) 물리 엔진과 렌더링 엔진의 완전한 통합</strong>, <strong>(2) 생성형 AI(Generative AI)를 활용한 4D 환각(Hallucination) 및 예측</strong>, <strong>(3) 신경망 처리 장치(NPU)에 최적화된 하드웨어 가속</strong> 방향으로 나아갈 것이다. 궁극적으로 로봇은 4D 재구성을 통해 과거를 기억하고, 현재를 인식하며, 미래를 예측하는 ’시간을 다루는 지능’을 갖추게 될 것이다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>A Survey on 3D Gaussian Splatting - arXiv, https://arxiv.org/html/2401.03890v7</li>
<li>Gaussian-UDSR: Real-Time Unbounded Dynamic Scene … - MDPI, https://www.mdpi.com/2076-3417/15/11/6262</li>
<li>Spacetime Gaussian Feature Splatting for Real-Time Dynamic View …, https://arxiv.org/html/2312.16812v1</li>
<li>D-NeRF: neural radiance fields for dynamic scenes - UPCommons, https://upcommons.upc.edu/entities/publication/516d565e-392e-4dea-93dc-c69245e15456</li>
<li>D-NeRF: Neural Radiance Fields for Dynamic Scenes, https://www.albertpumarola.com/research/D-NeRF/index.html</li>
<li>3D Gaussian Splatting as a New Era: A Survey - Semantic Scholar, https://www.semanticscholar.org/paper/3D-Gaussian-Splatting-as-a-New-Era%3A-A-Survey-Fei-Xu/0f76333fd1a5c0751ba02a0f6add9a49f3caa78c</li>
<li>4D Gaussian Splatting for Real-Time Dynamic Scene Rendering, https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_4D_Gaussian_Splatting_for_Real-Time_Dynamic_Scene_Rendering_CVPR_2024_paper.pdf</li>
<li>A Comparative Analysis with HexPlane - OpenReview, https://openreview.net/notes/edits/attachment?id=qLdiviJXaZ&amp;name=pdf</li>
<li>Exploring Explicit Representations in 4D - OpenReview, https://openreview.net/pdf/15080a6e6de5b14367a5124ca49d1de5f59827a7.pdf</li>
<li>HexPlane: A Fast Representation for Dynamic Scenes - ResearchGate, https://www.researchgate.net/publication/373324104_HexPlane_A_Fast_Representation_for_Dynamic_Scenes</li>
<li>Dynamic-Static Decomposition with Gaussian Splatting for Distractor …, https://arxiv.org/html/2503.13176v2</li>
<li>A Higher-Dimensional Representation for Topologically Varying …, <a href="https://graphics.stanford.edu/courses/cs348n-22-winter/PapersReferenced/HyperNeRF%202106.13228.pdf">https://graphics.stanford.edu/courses/cs348n-22-winter/PapersReferenced/HyperNeRF%202106.13228.pdf</a></li>
<li>Topology-Preserving Gaussian Splatting for High-fidelity 4D Head …, https://www.researchgate.net/publication/385423769_Topo4D_Topology-Preserving_Gaussian_Splatting_for_High-fidelity_4D_Head_Capture</li>
<li>HyperNeRF: A Higher-Dimensional Representation … - Neural Fields, https://neuralfields.cs.brown.edu/paper_211.html</li>
<li>HyperNeRF: A Higher-Dimensional Representation for …, https://hypernerf.github.io/</li>
<li>A Higher-Dimensional Representation for Topologically Varying …, https://www.researchgate.net/publication/353068452_HyperNeRF_A_Higher-Dimensional_Representation_for_Topologically_Varying_Neural_Radiance_Fields</li>
<li>TagSplat: Topology-Aware Gaussian Splatting for Dynamic Mesh …, https://arxiv.org/html/2512.01329v1</li>
<li>Topo4D: Topology-Preserving Gaussian Splatting for High-Fidelity …, https://xuanchenli.github.io/Topo4D/</li>
<li>4D Gaussian Splatting for Real-Time Dynamic Scene Rendering, https://guanjunwu.github.io/4dgs/</li>
<li>4D Gaussian Splatting for Real-Time Dynamic Scene Rendering, https://www.scribd.com/document/947599161/4D-Gaussian-Splatting-for-Real-Time-Dynamic-Scene-Rendering</li>
<li>Grid4D: 4D Decomposed Hash Encoding for High-fidelity Dynamic …, https://liner.com/review/grid4d-4d-decomposed-hash-encoding-for-highfidelity-dynamic-gaussian-splatting</li>
<li>Swift4D: Adaptive divide-and-conquer Gaussian Splatting for … - arXiv, https://arxiv.org/html/2503.12307v1</li>
<li>Dynamic-Static Decomposition with Gaussian Splatting for Distractor …, https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_DeGauss_Dynamic-Static_Decomposition_with_Gaussian_Splatting_for_Distractor-free_3D_Reconstruction_ICCV_2025_paper.pdf</li>
<li>Superpoint Gaussian Splatting for Real-Time High-Fidelity Dynamic …, https://raw.githubusercontent.com/mlresearch/v235/main/assets/wan24f/wan24f.pdf</li>
<li>Grid4D: 4D Decomposed Hash Encoding for High-Fidelity Dynamic…, <a href="https://openreview.net/forum?id=eyfYC19gOd&amp;noteId=h4RkcAsyf7">https://openreview.net/forum?id=eyfYC19gOd¬eId=h4RkcAsyf7</a></li>
<li>Safe Real-Time Robot Navigation in Gaussian Splatting Maps - arXiv, https://arxiv.org/html/2403.02751v1</li>
<li>Safe Real-Time Robot Navigation in Gaussian Splatting Maps, https://www.semanticscholar.org/paper/Splat-Nav%3A-Safe-Real-Time-Robot-Navigation-in-Maps-Chen-Shorinwa/7834b542860d9117de299472aa2839cad43b5a94</li>
<li>CoDa-4DGS: Dynamic Gaussian Splatting with Context and …, https://openaccess.thecvf.com/content/ICCV2025/papers/Song_CoDa-4DGS_Dynamic_Gaussian_Splatting_with_Context_and_Deformation_Awareness_for_ICCV_2025_paper.pdf</li>
<li>CoDa-4DGS: Dynamic Gaussian Splatting with Context … - Rui Song, https://rruisong.github.io/publications/CoDa-4DGS/</li>
<li>CoDa-4DGS: Dynamic Gaussian Splatting with Context and … - arXiv, https://arxiv.org/abs/2503.06744</li>
<li>MrNeRF’s Awesome-3D-Gaussian-Splatting-Paper-List, https://mrnerf.github.io/awesome-3D-gaussian-splatting/</li>
<li>Gaussian splatting vs. photogrammetry vs. NeRFs - Teleport by Varjo, https://teleport.varjo.com/blog/photogrammetry-vs-nerfs-gaussian-splatting-pros-and-cons</li>
<li>Towards a Generative 3D World Engine for Embodied Intelligence, https://arxiv.org/html/2506.10600v1</li>
<li>Lee-JaeWon/2025-Arxiv-Paper-List-Gaussian-Splatting - GitHub, https://github.com/Lee-JaeWon/2025-Arxiv-Paper-List-Gaussian-Splatting</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>