<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:5.5.3 SplatAM: 3D Gaussian Splatting 기반의 실시간 SLAM</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>5.5.3 SplatAM: 3D Gaussian Splatting 기반의 실시간 SLAM</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 5. 뉴럴 3D 표현과 렌더링 (Neural 3D Representations)</a> / <a href="index.html">5.5 Neural SLAM: 밀도 높은 지도의 실시간 작성</a> / <span>5.5.3 SplatAM: 3D Gaussian Splatting 기반의 실시간 SLAM</span></nav>
                </div>
            </header>
            <article>
                <h1>5.5.3 SplatAM: 3D Gaussian Splatting 기반의 실시간 SLAM</h1>
<p>로봇이 미지의 환경에서 자율적으로 행동하기 위해서는 자신이 어디에 있는지 파악하는 위치 추정(Localization)과 주변 환경을 지도로 작성하는 매핑(Mapping)이 동시에 이루어지는 SLAM(Simultaneous Localization and Mapping) 기술이 필수적이다. 앞서 살펴본<code>iMAP</code>이나 <code>NICE-SLAM</code>과 같은 초기 뉴럴 SLAM(Neural SLAM) 접근법들은 환경을 신경망의 가중치(Weights)나 특징 그리드(Feature Grid)로 암시적(Implicit)으로 표현함으로써, 기존의 이산적(Discrete) 표현 방식이 갖는 한계를 극복하고 연속적인(Continuous) 맵을 생성하는 데 성공했다. 그러나 이러한 볼륨 렌더링(Volume Rendering) 기반의 방식은 광선(Ray) 하나당 수백 번의 신경망 쿼리나 보간(Interpolation) 연산을 수행해야 하는 ‘레이 마칭(Ray-marching)’ 과정의 계산 복잡도로 인해 실시간성을 확보하는 데 근본적인 병목이 존재했다. 또한, 전체 장면을 하나의 네트워크에 압축하거나 고정된 해상도의 그리드에 의존함에 따라 발생하는 디테일 손실(Over-smoothing)과 치명적인 망각(Catastrophic Forgetting) 문제는 로봇의 정밀한 상호작용을 저해하는 요인이었다.</p>
<p>이러한 기술적 난제 속에서 등장한 <strong>“SplaTAM: Splat, Track &amp; Map 3D Gaussians for Dense RGB-D SLAM”</strong> 1은 2024년 CVPR에서 발표되며 덴스 SLAM(Dense SLAM)의 패러다임을 완전히 전환시켰다. SplaTAM은 명시적(Explicit)인 볼륨 표현 방식인 **3D Gaussian Splatting (3DGS)**을 SLAM 파이프라인의 핵심 백본으로 채택한 최초의 연구 중 하나이다.1 이 방법론은 3D 공간을 무수히 많은 ‘3D 가우시안(Gaussian)’ 타원체들의 집합으로 모델링하며, 미분 가능한 래스터화(Differentiable Rasterization) 기술을 통해 사진과 구분이 어려운 고해상도 렌더링을 400 FPS 이상의 압도적인 속도로 수행한다.1 본 절에서는 SplaTAM이 어떻게 기존의 암시적 표현의 한계를 극복하고 명시적 표현의 이점을 극대화했는지, 그리고 그 핵심 메커니즘인 ‘Splat, Track, &amp; Map’ 파이프라인과 실루엣 기반 최적화 전략을 심층적으로 분석한다.</p>
<h2>1.  명시적 볼륨 표현의 부활과 3D Gaussian Splatting</h2>
<p>전통적인 SLAM은 포인트 클라우드(Point Cloud), 서펠(Surfel), 또는 복셀(Voxel)과 같은 명시적이고 이산적인 표현을 주로 사용해 왔다. 이러한 방식은 맵의 국소적 수정(Local Update)이 용이하고 연산이 효율적이라는 장점이 있었으나, 빈 공간(Empty Space)에 대한 정보 부족이나 표면의 불연속성, 그리고 무엇보다 미분 불가능성(Non-differentiability)으로 인해 최신 딥러닝 기반 최적화 기법을 적용하는 데 한계가 있었다. 반면, NeRF(Neural Radiance Fields)로 대표되는 암시적 표현은 미분 가능성을 통해 종단간(End-to-End) 학습을 가능하게 했으나, 추론 속도가 느리고 맵 수정이 어렵다는 단점이 있었다.</p>
<p>SplaTAM은 이 두 가지 접근법의 장점을 융합한다. SplaTAM이 채택한 3D 가우시안은 기하학적으로는 포인트 클라우드와 유사하게 공간상에 분포하는 비정형 입자(Unstructured Particles)들이지만, 각 입자가 단순한 점이 아니라 수학적으로 정의된 부피, 방향성, 불투명도, 색상 정보를 가진 ’미분 가능한 볼륨’으로 기능한다.</p>
<h3>1.1 D 가우시안의 수학적 정의와 렌더링</h3>
<p>SplaTAM에서 환경을 구성하는 기본 단위인 3D 가우시안 <span class="math math-inline">G</span>는 월드 좌표계에서의 중심 위치(Mean) <span class="math math-inline">\mu \in \mathbb{R}^3</span>, 3x3 공분산 행렬(Covariance Matrix) <span class="math math-inline">\Sigma</span>, 불투명도(Opacity) $\alpha \in $, 그리고 시점 의존적 색상을 표현하는 구면 조화 함수(Spherical Harmonics, SH) 계수 <span class="math math-inline">C</span>로 정의된다.3 특정 위치 <span class="math math-inline">x</span>에서의 가우시안 값은 다음과 같이 계산된다.<br />
<span class="math math-display">
G(x) = e^{-\frac{1}{2}(x - \mu)^T \Sigma^{-1} (x - \mu)}
</span><br />
여기서 공분산 행렬 <span class="math math-inline">\Sigma</span>는 가우시안의 크기와 방향을 결정하며, 최적화 과정에서 양의 정부호(Positive Semi-definite) 성질을 유지하기 위해 스케일링 벡터 <span class="math math-inline">S \in \mathbb{R}^3</span>와 회전 쿼터니언 <span class="math math-inline">q \in \mathbb{R}^4</span>으로부터 유도된 회전 행렬 <span class="math math-inline">R</span>로 분해하여 표현한다.4<br />
<span class="math math-display">
\Sigma = R S S^T R^T
</span><br />
이러한 표현 방식은 로봇이 마주하는 복잡한 기하학적 구조를 매우 유연하게 모델링할 수 있게 한다. 예를 들어, 넓은 벽면은 납작하게 퍼진 타원체로, 얇은 전선이나 테이블 다리는 길쭉한 타원체로 표현됨으로써, 고정된 복셀 그리드보다 훨씬 적은 수의 파라미터로 고해상도의 표면을 근사할 수 있다.</p>
<p>무엇보다 중요한 점은 이러한 3D 가우시안들이 **타일 기반 래스터화(Tile-based Rasterization)**를 통해 2D 이미지 평면으로 투영된다는 것이다. 레이 마칭이 픽셀마다 광선을 쏘아 수백 번의 샘플링을 하는 것과 달리, 래스터화는 3D 가우시안들을 정렬(Sorting)한 후 화면에 ‘뿌리는(Splatting)’ 방식으로 작동하며, 이 모든 과정이 미분 가능하도록 설계되어 있다. 이는 렌더링 속도를 수백 배 향상시킬 뿐만 아니라, 렌더링된 이미지와 실제 관측 이미지 간의 차이(Loss)를 역전파(Backpropagation)하여 가우시안의 파라미터와 카메라 포즈를 동시에 최적화할 수 있는 길을 열어주었다.5</p>
<h2>2.  SplaTAM의 시스템 아키텍처: Splat, Track, and Map</h2>
<p>SplaTAM의 전체 파이프라인은 크게 <strong>트래킹(Tracking)</strong>, <strong>덴시피케이션(Densification)</strong>, **매핑(Mapping)**의 세 단계가 유기적으로 순환하는 구조를 갖는다. 시스템은 단일 RGB-D 카메라로부터 들어오는 연속적인 프레임을 입력으로 받아, 실시간으로 카메라의 6자유도(6-DoF) 포즈 <span class="math math-inline">T_t \in SE(3)</span>를 추정하고 가우시안 맵 <span class="math math-inline">\mathcal{M}</span>을 동적으로 확장 및 수정한다.6</p>
<h3>2.1  카메라 트래킹 (Camera Tracking): 미분 가능한 렌더링을 통한 포즈 최적화</h3>
<p>트래킹 단계의 목표는 현재 구축된 맵 <span class="math math-inline">\mathcal{M}</span>을 고정(Frozen)한 상태에서, 현재 입력 프레임 <span class="math math-inline">I_t</span> (RGB) 및 <span class="math math-inline">D_t</span> (Depth)와 가장 잘 일치하는 카메라 포즈 <span class="math math-inline">T_t</span>를 찾아내는 것이다. SplaTAM은 특징점(Feature Point) 매칭에 의존하는 전통적인 방식 대신, 전체 이미지의 픽셀 정보를 활용하는 **직접 방법(Direct Method)**을 사용한다.</p>
<p>초기 포즈 <span class="math math-inline">T_{init}</span> (일반적으로 등속 운동 모델이나 이전 프레임 포즈 사용 8)에서 시작하여, 현재 맵을 렌더링한 이미지 <span class="math math-inline">\hat{I}_t</span>와 <span class="math math-inline">\hat{D}_t</span>를 생성한다. 그 후, 렌더링된 결과와 실제 입력 간의 광도 오차(Photometric Error)와 기하학적 오차(Geometric Error)를 최소화하는 방향으로 포즈 파라미터를 업데이트한다.</p>
<p>트래킹을 위한 손실 함수 <span class="math math-inline">L_{track}</span>은 다음과 같이 정의된다.9<br />
<span class="math math-display">
L_{track} = L_{rgb}(I_t, \hat{I}_t) + \lambda_{depth} L_{depth}(D_t, \hat{D}_t)
</span><br />
여기서 <span class="math math-inline">L_{rgb}</span>는 L1 손실과 SSIM(Structural Similarity Index Measure) 손실의 조합으로 구성되며 11, <span class="math math-inline">L_{depth}</span>는 깊이 맵 간의 L1 손실이다. <span class="math math-inline">\lambda_{depth}</span>는 깊이 정보의 가중치를 조절하는 하이퍼파라미터이다.<br />
<span class="math math-display">
L_{rgb} = (1 - \lambda_{SSIM}) \mathcal{L}_1 + \lambda_{SSIM} \mathcal{L}_{D-SSIM}
</span><br />
SplaTAM의 트래킹이 강력한 이유는 3DGS의 빠른 렌더링 속도 덕분에 프레임당 수십 회 이상의 반복 최적화(Iteration)를 수행해도 실시간성을 유지할 수 있기 때문이다. 이는 텍스처가 부족한 벽면(Texture-less region)이나 급격한 조명 변화, 모션 블러가 심한 상황에서도 강인한 추적 성능을 보장하며, 실제로 SplaTAM은 이러한 도전적인 시나리오에서 기존 방법론 대비 2배 이상의 정확도를 입증했다.1</p>
<h3>2.2  실루엣 기반 맵 확장 (Silhouette-Guided Map Expansion)</h3>
<p>SplaTAM의 가장 독창적이고 핵심적인 기여는 **실루엣 마스크(Silhouette Mask)**를 활용한 맵 확장 메커니즘이다.1 기존의 포인트 기반 SLAM은 새로운 점을 언제 추가해야 할지 결정하기 위해 복잡한 투영 검사나 휴리스틱을 사용해야 했으며, 이는 종종 연산 비용 증가나 맵의 불필요한 중복으로 이어졌다.</p>
<p>SplaTAM은 3DGS 렌더링 과정에서 부산물로 얻어지는 <strong>알파 마스크(Alpha Mask)</strong>, 즉 누적 불투명도(Accumulated Opacity)를 활용하여 이 문제를 우아하게 해결한다. 현재 추정된 카메라 포즈에서 맵을 렌더링했을 때, 픽셀 <span class="math math-inline">(u, v)</span>에서의 누적 불투명도 <span class="math math-inline">A_{rend}(u, v)</span>가 1에 가깝다는 것은 해당 광선 방향에 이미 맵이 충분히 존재하여 장면을 설명하고 있음을 의미한다. 반대로 <span class="math math-inline">A_{rend}</span>가 0에 가깝다면, 해당 영역은 아직 매핑되지 않았거나 빈 공간(Free Space)임을 뜻한다.</p>
<p>이를 바탕으로 실루엣 마스크 <span class="math math-inline">M_{sil}</span>은 다음과 같이 정의된다.3<br />
<span class="math math-display">
M_{sil}(u, v) = \mathbb{1}(A_{rend}(u, v) &lt; \tau_{\alpha})
</span><br />
여기서 <span class="math math-inline">\tau_{\alpha}</span>는 임계값(Threshold)이다. SplaTAM은 입력 깊이 맵 <span class="math math-inline">D_{in}</span>이 유효하면서 동시에 실루엣 마스크 <span class="math math-inline">M_{sil}</span>이 활성화된 영역, 즉 ’유효한 깊이 정보가 있지만 현재 맵에는 없는 영역’에 대해서만 새로운 가우시안을 추가(Densification)한다.</p>
<p>새로운 가우시안의 중심 위치 <span class="math math-inline">\mu_{new}</span>는 해당 픽셀의 깊이 값을 이용한 역투영(Unprojection)을 통해 3D 공간상에 초기화된다.<br />
<span class="math math-display">
\mu_{new} = T_t^{-1} \cdot \pi^{-1}(u, v, D_t(u, v))
</span><br />
이러한 <strong>실루엣 가이드 접근법</strong>은 다음과 같은 결정적인 이점을 제공한다 1:</p>
<ol>
<li><strong>효율적 자원 할당:</strong> 이미 맵이 존재하는 곳에 불필요하게 가우시안을 덧칠하는 것을 방지하여 메모리 효율성을 높인다.</li>
<li><strong>명시적 미탐사 영역 인지:</strong> 로봇은 렌더링된 알파 값을 통해 자신이 모르는 영역을 명확히 구분할 수 있으며, 이는 능동적 탐사(Active Exploration)를 위한 정보로 활용될 수 있다.</li>
<li><strong>구조화된 맵 확장:</strong> 카메라가 이동함에 따라 맵이 시야의 확장에 맞춰 자연스럽게 성장(Grow)하도록 유도한다.</li>
</ol>
<h3>2.3  매핑: 가우시안 파라미터의 동적 최적화</h3>
<p>새로운 가우시안이 추가된 후, 매핑 단계에서는 맵의 전역적 일관성(Global Consistency)과 국소적 정밀도를 높이기 위해 가우시안들의 파라미터(<span class="math math-inline">\mu, \Sigma, \alpha, C</span>)를 최적화한다. 이때 현재 프레임만을 사용하는 것이 아니라, 과거의 관측을 대표하는 **키프레임(Keyframe)**들을 함께 윈도우(Window)에 포함시켜 최적화를 수행함으로써, 이전에 학습한 정보를 잊어버리는 망각 문제를 완화한다.12</p>
<p>매핑 손실 함수 <span class="math math-inline">L_{map}</span>은 트래킹 손실과 유사하지만, 최적화 대상이 카메라 포즈가 아닌 맵의 파라미터라는 점에서 차이가 있다. 또한, 매핑 단계에서는 등방성 정규화(Isotropic Regularization) 항과 같은 추가적인 제약 조건을 포함하여 가우시안의 형상이 지나치게 길어지거나(Needle-shape) 비정상적으로 커지는 것을 방지한다.12<br />
<span class="math math-display">
L_{map} = \sum_{k \in \mathcal{K}} \left( L_{rgb}(I_k, \hat{I}_k) + \lambda_{depth} L_{depth}(D_k, \hat{D}_k) \right) + \lambda_{reg} L_{reg}
</span><br />
여기서 <span class="math math-inline">\mathcal{K}</span>는 현재 프레임과 시야가 겹치는 키프레임들의 집합이다. SplaTAM은 최적화 과정에서 불투명도가 임계값 이하로 떨어지거나(<span class="math math-inline">\alpha &lt; \epsilon</span>), 크기가 너무 커져 기하학적 의미를 상실한 가우시안들을 주기적으로 제거(Pruning)하는 관리 전략을 병행하여 맵의 품질을 유지한다.12</p>
<h2>3.  성능 평가 및 비교 우위: 정확도와 속도의 혁신</h2>
<p>SplaTAM은 공개된 벤치마크 데이터셋에서 기존의 SOTA(State-of-the-Art) 뉴럴 SLAM 방법론들을 압도하는 성능을 보여주며, 명시적 표현의 우수성을 입증했다.</p>
<h3>3.1 트래킹 및 매핑 정확도 분석</h3>
<p>Replica, ScanNet, TUM-RGBD 데이터셋을 대상으로 한 평가에서 SplaTAM은 <code>iMAP</code>, <code>NICE-SLAM</code>, <code>Point-SLAM</code> 등과 비교하여 현저히 낮은 궤적 오차(Trajectory Error)를 기록했다.</p>
<ul>
<li><strong>Replica 데이터셋:</strong> 실내 환경을 시뮬레이션한 Replica 데이터셋에서 SplaTAM은 평균 ATE(Absolute Trajectory Error) RMSE <strong>0.36cm</strong>를 기록했다.15 이는 NICE-SLAM(1.06cm) 대비 약 3배, Point-SLAM(0.52cm) 대비 약 1.5배 더 정밀한 수치이다. 특히 텍스처가 없는 단색 벽면이나 복잡한 구조물이 있는 환경에서도 가우시안의 유연한 형상 모델링 덕분에 추적 실패 없이 안정적인 성능을 보여주었다.</li>
<li><strong>TUM-RGBD 데이터셋:</strong> 실제 센서 노이즈와 모션 블러가 포함된 TUM 데이터셋에서도 SplaTAM은 강력한 성능을 발휘했다. 기존 방법론들이 빠른 카메라 움직임에서 트래킹을 놓치는(Lost) 경우가 빈번했던 반면, SplaTAM은 렌더링 기반의 넓은 수렴 영역(Basin of Attraction) 덕분에 강인한 추적을 유지했다.1</li>
</ul>
<h3>3.2 렌더링 품질과 속도의 비약적 향상</h3>
<p>SplaTAM의 가장 독보적인 경쟁력은 렌더링 성능에 있다. 3DGS의 래스터화 엔진을 활용함으로써, 학습된 맵을 렌더링할 때 <strong>400 FPS</strong> (NVIDIA RTX 3090/4090 기준) 이상의 속도를 달성한다.1 이는 iMAP이나 NICE-SLAM과 같은 레이 마칭 기반 방법들이 10 FPS 미만의 속도에 머무르는 것과 비교하면 수십 배에서 수백 배 빠른 속도이다.</p>
<p>이러한 고속 렌더링 능력은 단순히 시각화를 넘어, 로봇의 경로 계획(Path Planning)이나 가상 현실(VR/AR) 애플리케이션에서 실시간 상호작용을 가능하게 하는 핵심 요소이다. 또한, 렌더링 품질 지표인 PSNR(Peak Signal-to-Noise Ratio)에서도 34dB 이상을 기록하며 Point-SLAM과 대등하거나 더 우수한 고해상도 복원 능력을 보여주었다.15</p>
<h3>3.3 데이터 테이블: 주요 뉴럴 SLAM 방법론의 정량적 비교</h3>
<p>아래의 표는 SplaTAM과 주요 경쟁 모델들의 성능 및 시스템 특성을 종합적으로 비교한 것이다.15</p>
<table><thead><tr><th><strong>비교 항목 (Comparison Metric)</strong></th><th><strong>NICE-SLAM</strong></th><th><strong>Point-SLAM</strong></th><th><strong>SplaTAM</strong></th></tr></thead><tbody>
<tr><td><strong>장면 표현 (Representation)</strong></td><td>계층적 그리드 (Implicit)</td><td>뉴럴 포인트 클라우드</td><td><strong>3D Gaussians (Explicit)</strong></td></tr>
<tr><td><strong>렌더링 방식 (Rendering)</strong></td><td>Volumetric Ray-marching</td><td>Ray-marching</td><td><strong>Rasterization</strong></td></tr>
<tr><td><strong>트래킹 정확도 (ATE RMSE)</strong></td><td>1.06 cm</td><td>0.52 cm</td><td><strong>0.36 cm (Best)</strong></td></tr>
<tr><td><strong>렌더링 속도 (Rendering FPS)</strong></td><td>&lt; 10 FPS</td><td>&lt; 10 FPS</td><td><strong>&gt; 400 FPS</strong></td></tr>
<tr><td><strong>메모리 사용량 (VRAM)</strong></td><td>낮음 (Grid 공유)</td><td>중간</td><td><strong>높음 (~11-15 GB)</strong></td></tr>
<tr><td><strong>최적화 속도 (Tracking freq.)</strong></td><td>~2 Hz</td><td>&lt; 1 Hz</td><td><strong>~1 Hz (설정에 따라 상이)</strong></td></tr>
<tr><td><strong>빈 공간 인지 (Free Space)</strong></td><td>암시적 (Occupancy)</td><td>제한적</td><td><strong>명시적 (Alpha Mask)</strong></td></tr>
</tbody></table>
<p>주: 렌더링 속도와 SLAM 시스템의 전체 루프 속도는 구분이 필요하다. SplaTAM은 렌더링은 매우 빠르지만, 매 프레임 역전파를 통한 최적화를 수행하므로 전체 시스템의 처리 속도(Tracking Frequency)는 하드웨어와 최적화 반복 횟수에 따라 1~5 Hz 수준으로 제한될 수 있다.18</p>
<h2>4.  기술적 한계와 극복 과제</h2>
<p>SplaTAM은 혁신적인 성과를 거두었으나, 로봇 상용화를 위해 해결해야 할 몇 가지 기술적 과제들이 남아있다.</p>
<ol>
<li><strong>시스템 구동 속도와 실시간성:</strong> SplaTAM의 렌더링은 빠르지만, 트래킹과 매핑을 위한 역전파 최적화 과정은 여전히 많은 연산을 요구한다. 특히 파이토치(PyTorch) 기반의 구현은 오버헤드가 있어, 전체 SLAM 루프는 1~2 FPS 수준으로 구동되는 경우가 많다.17 이는 고속으로 이동하는 드론이나 자율주행차에 적용하기에는 부족한 속도이며, 이를 개선하기 위해 최근 <code>FlashSLAM</code> 8이나 <code>GS-ICP SLAM</code> 19과 같이 최적화 과정을 경량화하거나 CUDA 커널을 직접 최적화하는 연구들이 진행되고 있다.</li>
<li><strong>높은 메모리 요구량과 하드웨어 제약:</strong> 수백만 개의 3D 가우시안을 유지하고 최적화하기 위해서는 대용량의 VRAM(약 11~15GB)이 필요하다.16 이는 RTX 3090/4090과 같은 데스크톱 GPU에서는 문제가 되지 않으나, Jetson Orin과 같은 엣지 디바이스나 임베디드 시스템에서는 메모리 부족(OOM)이나 급격한 프레임 저하를 유발한다. 모바일 로봇을 위해서는 가우시안의 수를 줄이거나 압축하는 경량화 기술이 필수적이다.</li>
<li><strong>동적 환경에 대한 취약성:</strong> SplaTAM은 기본적으로 정적 환경(Static World)을 가정한다. 사람이 걸어다니거나 물체가 이동하는 동적 환경에서는 움직이는 객체를 맵의 일부로 잘못 인식하여 ‘고스트(Ghost)’ 잔상을 남기거나 트래킹이 불안정해지는 문제가 발생한다.20 이를 해결하기 위해 동적 객체를 의미론적으로 분할(Segmentation)하거나 별도의 동적 가우시안 필드로 모델링하는 <code>SGS-SLAM</code> 21 등의 후속 연구가 활발히 이루어지고 있다.</li>
</ol>
<h2>5.  결론: 로봇을 위한 새로운 시각적 기억</h2>
<p>“SplaTAM: Splat, Track &amp; Map 3D Gaussians for Dense RGB-D SLAM“은 뉴럴 SLAM 분야에서 명시적 표현의 가치를 재발견하고, 미분 가능한 래스터화를 통해 정밀도와 효율성이라는 두 마리 토끼를 동시에 잡을 수 있음을 증명했다. 특히 실루엣 기반의 맵 확장 전략은 로봇이 미지의 공간을 탐사하고 이해하는 방식에 대한 새로운 통찰을 제공한다.</p>
<p>SplaTAM은 단순한 3D 재구성을 넘어, 로봇이 환경을 ’기억’하는 방식을 근본적으로 변화시키고 있다. 3D 가우시안은 기하학적 정보뿐만 아니라 의미론적 정보(Semantic Information)를 결합하기에도 용이하여, 향후 **시맨틱 SLAM(Semantic SLAM)**이나 **오픈 보캐블러리 3D 맵핑(Open-Vocabulary 3D Mapping)**으로의 확장이 매우 기대된다. 하드웨어 가속 기술의 발전과 알고리즘의 최적화가 병행된다면, SplaTAM 기반의 기술은 차세대 Embodied AI가 인간 수준의 시공간 인지 능력을 갖추는 데 핵심적인 역할을 수행할 것이다. 이는 로봇이 단순히 공간을 점유하는 것을 넘어, 공간의 구조와 의미를 깊이 있게 이해하고 상호작용하는 진정한 지능형 에이전트로 거듭나는 기반이 될 것이다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>SplaTAM.pdf, https://spla-tam.github.io/assets/SplaTAM.pdf</li>
<li>Jonathon Luiten’s research while affiliated with Carnegie Mellon University and other places, https://www.researchgate.net/scientific-contributions/Jonathon-Luiten-2136918250</li>
<li>SplaTAM: Volumetric 3D Gaussian SLAM - Emergent Mind, https://www.emergentmind.com/topics/splatam</li>
<li>Communication-Efficient Active Reconstruction using Self-Organizing Gaussian Mixture Models - Carnegie Mellon University Robotics Institute, https://www.ri.cmu.edu/app/uploads/2024/12/thesis.pdf</li>
<li>[2312.02126] SplaTAM: Splat, Track &amp; Map 3D Gaussians for Dense …, https://ar5iv.labs.arxiv.org/html/2312.02126</li>
<li>[Quick Review] SplaTAM: Splat, Track &amp; Map 3D Gaussians for Dense RGB-D SLAM - Liner, https://liner.com/review/splatam-splat-track-map-3d-gaussians-for-dense-rgbd-slam</li>
<li>SplaTAM: Splat, Track &amp; Map 3D Gaussians for Dense RGB-D, https://www.alphaxiv.org/overview/2312.02126v3</li>
<li>Accelerated RGB-D SLAM for Real-Time 3D Scene Reconstruction with Gaussian Splatting flashslam.github.io - arXiv, https://arxiv.org/html/2412.00682v1</li>
<li>Uncertainty-Aware Neural Implicit SLAM for Real-Time Dense Indoor Scene Reconstruction, https://www.dfki.de/fileadmin/user_upload/import/15654_Wang2025_WACV.pdf</li>
<li>Splat-SLAM: Globally Optimized RGB-only SLAM with 3D Gaussians - arXiv, https://arxiv.org/html/2405.16544v1</li>
<li>GS4: Generalizable Sparse Splatting Semantic SLAM - arXiv, https://arxiv.org/html/2506.06517v3</li>
<li>A 3D Gaussian Splatting-Based Simultaneous Localization and Mapping with Related Keyframe Optimization - MDPI, https://www.mdpi.com/2076-3417/15/3/1320</li>
<li>Splat-SLAM: Globally Optimized RGB-only SLAM with 3D Gaussians - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2025W/VOCVALC/papers/Sandstrom_Splat-SLAM_Globally_Optimized_RGB-only_SLAM_with_3D_Gaussians_CVPRW_2025_paper.pdf</li>
<li>Large-scale compact 3D Gaussian splatting SLAM - Optica Publishing Group, https://opg.optica.org/ao/abstract.cfm?uri=ao-64-31-9185</li>
<li>Evaluating Modern Approaches in 3D Scene Reconstruction: NeRF vs Gaussian-Based Methods - arXiv, https://arxiv.org/html/2408.04268v2</li>
<li>Supplementary Meterial of MonoGS++: Fast and Accurate Monocular RGB Gaussian SLAM - BMVA Archive, https://bmva-archive.org.uk/bmvc/2024/papers/Paper_133/supplementary133.pdf</li>
<li>LEG-SLAM: Language-Enhanced Gaussian Splatting for Real-Time SLAM - arXiv, https://arxiv.org/html/2506.03073v1</li>
<li>MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM - arXiv, https://arxiv.org/html/2509.13536v1</li>
<li>Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline - arXiv, https://arxiv.org/html/2508.04597v1</li>
<li>Embracing Dynamics: Dynamics-aware 4D Gaussian Splatting SLAM - arXiv, https://arxiv.org/html/2504.04844v1</li>
<li>SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM - arXiv, https://arxiv.org/html/2403.07494v4</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>