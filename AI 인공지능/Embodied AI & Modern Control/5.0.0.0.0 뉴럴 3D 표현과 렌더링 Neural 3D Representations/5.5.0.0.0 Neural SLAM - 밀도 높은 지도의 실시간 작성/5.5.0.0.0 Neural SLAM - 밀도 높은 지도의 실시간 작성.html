<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:5.5 Neural SLAM: 밀도 높은 지도의 실시간 작성</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>5.5 Neural SLAM: 밀도 높은 지도의 실시간 작성</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 5. 뉴럴 3D 표현과 렌더링 (Neural 3D Representations)</a> / <a href="index.html">5.5 Neural SLAM: 밀도 높은 지도의 실시간 작성</a> / <span>5.5 Neural SLAM: 밀도 높은 지도의 실시간 작성</span></nav>
                </div>
            </header>
            <article>
                <h1>5.5 Neural SLAM: 밀도 높은 지도의 실시간 작성</h1>
<h2>1.  서론: 공간 지능과 밀집 지도의 새로운 패러다임</h2>
<p>로봇 공학 및 컴퓨터 비전의 역사에서 SLAM(Simultaneous Localization and Mapping)은 에이전트가 미지의 환경을 탐험하며 자신의 위치를 파악하고 지도를 작성하는 근본적인 문제로 정의되어 왔다. 지난 수십 년간 SLAM 기술은 칼만 필터(Kalman Filter) 기반의 초기 접근법에서 시작하여, 특징점(Feature Point)을 활용한 희소 지도(Sparse Map) 작성 기술인 ORB-SLAM, 그리고 서펠(Surfel)이나 복셀(Voxel)을 활용한 밀집 지도(Dense Map) 작성 기술인 KinectFusion 등으로 진화해 왔다. 그러나 전통적인 기하학적 접근 방식들은 각기 다른 한계점을 지니고 있었다. 특징점 기반 방식은 계산 효율성이 뛰어나고 위치 추정이 정교하지만, 환경의 물리적 형상을 점들의 집합으로만 표현하기 때문에 로봇의 경로 계획(Path Planning)이나 물리적 상호작용에는 부적합했다. 반면, 초기 밀집 SLAM 방식들은 환경을 상세하게 표현할 수 있었으나, 방대한 메모리 소모와 누적되는 오차를 수정하기 위한 루프 결합(Loop Closure) 시의 연산 비용 문제로 인해 대규모 환경으로의 확장에 어려움을 겪었다.1</p>
<p>최근 인공지능 기술의 비약적인 발전, 특히 **뉴럴 임플리싯 표현(Neural Implicit Representation)**과 **3D Gaussian Splatting (3DGS)**의 등장은 SLAM 분야에 혁명적인 변화를 가져왔다. 이 새로운 패러다임인 <strong>Neural SLAM</strong>은 3D 장면을 이산적인 데이터 포인트의 집합이 아닌, 연속적인 함수(Continuous Function)나 미분 가능한 볼류메트릭 표현(Differentiable Volumetric Representation)으로 모델링한다. 이는 마치 인간이 세상을 기억할 때 픽셀 단위로 저장하지 않고 추상화된 개념과 구조로 기억하는 것과 유사하다. Neural SLAM은 딥러닝 모델, 특히 다층 퍼셉트론(MLP)이나 학습 가능한 특징 그리드(Feature Grid)를 맵핑의 매개체로 사용하여, 메모리 효율성을 극대화하면서도 전례 없는 수준의 사진 사실적(Photorealistic) 렌더링 품질과 정밀한 기하학적 복원을 가능하게 했다.</p>
<p>본 장에서는 Neural SLAM의 기술적 진보 과정을 심층적으로 다룬다. 먼저 전체 장면을 단일 신경망에 압축하려 했던 초기 시도인 iMAP과 그 한계점인 ‘치명적 망각(Catastrophic Forgetting)’ 현상을 분석한다.3 이어 이를 극복하기 위해 등장한 NICE-SLAM, Co-SLAM, ESLAM 등 하이브리드 특징 그리드 기반의 시스템들이 어떻게 국소적 업데이트(Local Update)와 대규모 확장성을 확보했는지 살펴본다.5 마지막으로, 최근 렌더링 속도와 품질의 혁신을 이끌고 있는 3D Gaussian Splatting 기반의 SplaTAM, GS-SLAM 등의 시스템을 상세히 분석하고, 이들이 어떻게 실시간성과 정밀도라는 두 마리 토끼를 잡았는지 논의한다.8 또한, 로봇 내비게이션을 위한 충돌 회피, 동적 환경에서의 강건성 확보, 그리고 장기적 맵핑을 위한 루프 결합 전략 등 실용적인 응용을 위한 핵심 이슈들을 포괄적으로 다룬다.</p>
<h2>2.  뉴럴 임플리싯 SLAM의 태동과 단일 MLP의 도전</h2>
<h3>2.1  iMAP: 실시간 Implicit Mapping and Positioning의 구조와 원리</h3>
<p>Neural SLAM의 시대를 연 선구적인 연구는 **iMAP (Implicit Mapping and Positioning)**이다. iMAP은 업계 최초로 휴대용 RGB-D 카메라만을 사용하여 실시간으로 장면의 3D 구조와 카메라의 6자유도(6-DoF) 궤적을 동시에 학습하는 시스템을 제안했다.3 iMAP의 가장 큰 특징은 **단일 다층 퍼셉트론(Single MLP)**이 전체 장면의 유일한 표현체라는 점이다.</p>
<p>iMAP의 아키텍처는 NeRF(Neural Radiance Fields)의 개념을 실시간 SLAM에 맞게 경량화한 것이다. 좌표 기반(Coordinate-based) 네트워크인 이 MLP는 3D 좌표 <span class="math math-inline">\mathbf{x} = (x, y, z)</span>를 입력으로 받아, 해당 위치의 색상 <span class="math math-inline">\mathbf{c} = (r, g, b)</span>와 부피 밀도 <span class="math math-inline">\sigma</span> (또는 점유 확률)를 출력한다. iMAP은 사전 학습된 데이터(Prior Data) 없이, 로봇이나 카메라가 환경을 탐사하는 동안 실시간으로 들어오는 비디오 스트림만을 이용하여 네트워크 가중치를 지속적으로 업데이트한다.10</p>
<p><strong>iMAP의 핵심 메커니즘:</strong></p>
<ol>
<li><strong>추적(Tracking):</strong> 현재 프레임의 카메라 포즈를 추정하기 위해, 장면을 표현하는 MLP의 파라미터(Network Weights)를 고정한 상태(Freeze)로 둔다. 그런 다음, 현재 추정된 포즈에서 렌더링한 이미지와 실제 입력된 카메라 이미지 간의 차이, 즉 광도 손실(Photometric Loss)과 기하학적 손실(Geometric Loss)을 계산한다. 이 손실 값을 역전파(Backpropagation)하여 카메라의 포즈 파라미터만을 최적화한다.</li>
<li><strong>맵핑(Mapping):</strong> 추적과는 별도의 병렬 스레드(Thread)에서 수행된다. 시스템은 전체 프레임 중 정보량이 풍부한 **키프레임(Keyframe)**들을 선별하여 유지한다. 맵핑 스레드에서는 이 키프레임들에 대해 MLP의 가중치와 키프레임들의 카메라 포즈를 동시에 최적화(Joint Optimization)한다. 이는 기존 Bundle Adjustment와 유사하지만, 최적화의 대상이 3D 포인트가 아닌 신경망의 가중치라는 점에서 차이가 있다.3</li>
<li><strong>능동적 샘플링(Active Sampling):</strong> 실시간 성능을 보장하기 위해 전체 이미지를 매번 렌더링하는 것은 불가능하다. iMAP은 렌더링 오차가 크거나 정보량이 많은 픽셀들을 확률적으로 선택하여 학습에 사용하는 능동적 샘플링 전략을 도입했다. 이를 통해 계산 비용을 줄이면서도 맵의 디테일을 향상시켰다.4</li>
</ol>
<h3>2.2  단일 MLP의 구조적 한계: 치명적 망각(Catastrophic Forgetting)</h3>
<p>iMAP은 혁신적인 시도였으나, **치명적 망각(Catastrophic Forgetting)**이라는 인공신경망의 고질적인 문제에 직면했다.12 이는 단일 MLP가 전체 장면의 정보를 전역적(Global)으로 공유하는 가중치에 저장하기 때문에 발생한다.</p>
<p>카메라가 새로운 영역(예: 거실)으로 이동하여 새로운 정보를 학습하면, 역전파 과정에서 네트워크의 모든 가중치가 업데이트된다. 이 과정에서 이전에 학습했던 영역(예: 부엌)에 대한 정보가 덮어씌워지거나 손상되는 현상이 발생한다. 즉, 새로운 기억이 과거의 기억을 지워버리는 것이다. iMAP은 이를 완화하기 위해 전체 키프레임 중 일부를 무작위로 추출하여 지속적으로 재학습시키는 <strong>리플레이 버퍼(Replay Buffer)</strong> 전략을 사용했다.3 그러나 장면의 규모가 커질수록 리플레이 버퍼가 커져야 하고, 최적화해야 할 키프레임 수가 증가함에 따라 실시간 처리가 불가능해지는 한계가 있었다. 결과적으로 iMAP은 작은 방 규모의 환경에서는 우수한 성능을 보였으나, 다중 공간이 연결된 아파트나 대형 사무실 같은 환경에서는 이전 맵이 뭉개지거나(Smoothing) 사라지는 현상을 보였다.5</p>
<h2>3.  확장성을 위한 하이브리드 아키텍처: 그리드와 신경망의 결합</h2>
<p>단일 MLP의 한계를 극복하기 위해 연구자들은 순수 임플리싯 모델 대신, 공간을 명시적으로 분할하는 **특징 그리드(Feature Grid)**와 얕은 신경망(Shallow MLP)을 결합한 <strong>하이브리드 표현(Hybrid Representation)</strong> 방식에 주목했다. 이 접근법은 공간적 지역성(Locality)을 보장하여 치명적 망각 문제를 해결하고, 대규모 환경으로의 확장성을 확보하는 데 결정적인 기여를 했다.</p>
<h3>3.1  NICE-SLAM: 계층적 특징 그리드(Hierarchical Feature Grid)</h3>
<p>**NICE-SLAM (Neural Implicit Scalable Encoding for SLAM)**은 iMAP의 전역 업데이트 문제를 해결하기 위해 계층적 특징 그리드 구조를 제안했다.5 NICE-SLAM은 3D 공간을 미세(Fine), 중간(Middle), 거친(Coarse) 레벨의 다중 해상도 복셀 그리드(Voxel Grid)로 나눈다. 각 복셀의 모서리에는 학습 가능한 잠재 특징 벡터(Latent Feature Vector)가 저장된다.</p>
<p><strong>작동 원리 및 장점:</strong></p>
<ul>
<li><strong>국소적 업데이트(Local Updates):</strong> 특정 3D 좌표의 점유 값이나 색상을 계산할 때, 해당 좌표를 포함하는 복셀의 특징 벡터들만 조회하여 얕은 MLP에 입력한다. 따라서 새로운 영역을 학습할 때 역전파는 해당 영역의 복셀 특징들만 업데이트하며, 멀리 떨어져 있는 다른 영역의 특징에는 영향을 주지 않는다. 이를 통해 <strong>치명적 망각 문제를 근본적으로 방지</strong>한다.5</li>
<li><strong>상세한 기하학적 복원:</strong> 계층적 구조 덕분에 NICE-SLAM은 iMAP보다 훨씬 더 정밀한 기하학적 디테일을 표현할 수 있다. 미세 레벨 그리드는 고주파수의 세부 형상을, 거친 레벨 그리드는 전체적인 구조적 일관성을 담당한다. 실험 결과, NICE-SLAM은 대규모 실내 환경에서 iMAP 대비 훨씬 선명한 맵을 생성하며, 텍스처가 부족한 영역에서도 기하학적 사전 정보(Prior)를 통해 형상을 합리적으로 추론하는 것으로 나타났다.16</li>
</ul>
<h3>3.2  Co-SLAM: 해시 그리드와 One-Blob 인코딩의 융합</h3>
<p><strong>Co-SLAM</strong>은 NVIDIA의 Instant-NGP에서 제안된 <strong>해시 그리드(Hash Grid)</strong> 기술을 SLAM에 접목하여 처리 속도와 메모리 효율을 획기적으로 개선했다.6 기존의 복셀 그리드는 빈 공간(Free Space)에도 메모리를 할당해야 하는 비효율성이 있었으나, 해시 그리드는 유효한 표면 근처의 정보만을 해시 테이블에 매핑하여 저장하므로 메모리 사용량을 대폭 줄일 수 있다.</p>
<ul>
<li><strong>Joint Coordinate &amp; Parametric Encoding:</strong> Co-SLAM은 단순히 해시 그리드 특징만을 사용하는 것이 아니라, 좌표 기반 인코딩(Coordinate-based encoding)과 파라메트릭 인코딩(Parametric encoding)을 결합했다. 이를 통해 해시 충돌(Hash Collision)로 인한 노이즈를 억제하고, 관측되지 않은 영역(Unobserved Areas)에 대해서도 부드러운 표면을 생성하는 <strong>구멍 메우기(Hole Filling)</strong> 능력을 확보했다. 특히 <strong>One-Blob 인코딩</strong> 기법을 사용하여 표면의 일관성(Coherence)을 높였다.6</li>
<li><strong>실시간 전역 최적화:</strong> Co-SLAM은 약 10-17Hz의 높은 처리 속도를 자랑하며, iMAP이나 NICE-SLAM이 일부 키프레임만을 선택하여 최적화하는 것과 달리, 모든 키프레임을 대상으로 하는 **전역 번들 조정(Global Bundle Adjustment)**을 실시간으로 수행할 수 있는 효율성을 입증했다.18</li>
</ul>
<h3>3.3  ESLAM: 축 정렬 특징 평면(Axis-Aligned Feature Planes)을 통한 효율화</h3>
<p>**ESLAM (Efficient Dense SLAM)**은 3D 공간을 복셀 대신 3개의 직교하는 2차원 특징 평면(Tri-planes)으로 투영하여 표현하는 방식을 채택했다.7 3D 좌표 <span class="math math-inline">(x, y, z)</span>는 XY, YZ, XZ 평면에 투영되어 각각의 특징 벡터를 가져오고, 이들을 결합하여 MLP에 입력한다.</p>
<ul>
<li><strong>메모리 및 연산 효율:</strong> 3D 데이터를 2D 평면으로 압축함으로써 메모리 사용량을 <span class="math math-inline">O(N^3)</span>에서 <span class="math math-inline">O(N^2)</span>로 줄였다. 이는 대규모 환경 맵핑 시 메모리 증가 폭을 억제하는 데 매우 유리하다.</li>
<li><strong>SDF 기반 정확도 향상:</strong> ESLAM은 T-SDF(Truncated Signed Distance Field)를 직접 학습하여 표면 재구성의 정확도를 높였다. 실험 결과, ESLAM은 기존 방식 대비 50% 이상의 정확도 향상과 10배 빠른 처리 속도를 달성하며, 별도의 사전 학습 없이도 고품질의 맵핑이 가능함을 보였다.7</li>
</ul>
<p><strong>[표 5.5.1] 주요 Neural Implicit SLAM 아키텍처 비교</strong></p>
<table><thead><tr><th><strong>특성</strong></th><th><strong>iMAP</strong></th><th><strong>NICE-SLAM</strong></th><th><strong>Co-SLAM</strong></th><th><strong>ESLAM</strong></th></tr></thead><tbody>
<tr><td><strong>장면 표현</strong></td><td>단일 MLP (Global)</td><td>계층적 복셀 그리드 (Local)</td><td>다중 해상도 해시 그리드 (Hash)</td><td>Tri-planes (Feature Planes)</td></tr>
<tr><td><strong>업데이트 방식</strong></td><td>전역적 업데이트</td><td>국소적 업데이트</td><td>국소적 업데이트</td><td>국소적/평면 업데이트</td></tr>
<tr><td><strong>망각 문제</strong></td><td>심각 (Catastrophic Forgetting)</td><td>해결됨 (Local isolation)</td><td>해결됨</td><td>해결됨</td></tr>
<tr><td><strong>장점</strong></td><td>메모리 효율적 (작은 모델)</td><td>대규모 확장성, 디테일</td><td>빠른 수렴, 실시간 전역 최적화</td><td>높은 정확도, 메모리 효율성</td></tr>
<tr><td><strong>단점</strong></td><td>확장성 부족, 느린 수렴</td><td>느린 추적 속도</td><td>해시 충돌 가능성</td><td>투영 시 정보 손실 가능성</td></tr>
</tbody></table>
<h2>4.  3D Gaussian Splatting 기반 SLAM: 명시적 표현의 귀환</h2>
<p>2023년 등장한 **3D Gaussian Splatting (3DGS)**은 렌더링 속도와 품질 면에서 기존의 NeRF 방식을 압도하며 SLAM 분야에 새로운 흐름을 만들었다. 3DGS는 3D 공간을 무수히 많은 3D 가우시안(타원체)으로 표현하며, 이를 2D 화면에 투영(Splatting)하여 이미지를 생성한다. 이는 본질적으로 포인트 클라우드와 유사한 <strong>명시적(Explicit) 볼류메트릭 표현</strong> 방식이지만, 미분 가능한 렌더링(Differentiable Rendering)을 통해 신경망처럼 최적화가 가능하다는 점이 혁신적이다.8</p>
<h3>4.1  SplaTAM: Splat, Track &amp; Map</h3>
<p><strong>SplaTAM</strong>은 3DGS 기술을 SLAM에 성공적으로 도입한 초기 연구 중 하나이다. SplaTAM은 단일 RGB-D 카메라 입력을 받아 명시적인 3D 가우시안 맵을 실시간으로 생성한다.8</p>
<ul>
<li><strong>실루엣 마스크(Silhouette Mask) 기반 추적:</strong> SplaTAM은 추적의 강건성을 높이기 위해 실루엣 마스크를 도입했다. 맵핑 초기 단계나 탐사되지 않은 영역에서는 가우시안 맵이 불완전할 수밖에 없다. SplaTAM은 렌더링된 가우시안의 불투명도(Opacity)를 기반으로 유효한 맵 영역과 미탐사 영역을 구분하는 마스크를 생성한다. 추적 시 이 마스크를 적용하여, 맵이 존재하지 않는 허공이나 불확실한 영역에서의 오차를 무시하고 신뢰할 수 있는 영역의 정보만을 사용하여 카메라 포즈를 최적화한다. 이는 추적 실패를 방지하고 최적화의 안정성을 크게 높인다.8</li>
<li><strong>구조적 맵 확장(Structured Map Expansion):</strong> 기존 Neural SLAM들이 고정된 그리드나 네트워크 용량에 제한받던 것과 달리, SplaTAM은 새로운 영역이 관측될 때마다 가우시안을 동적으로 추가(Densification)하는 전략을 사용한다. 이는 포인트 클라우드를 확장하는 것과 유사하여 맵의 크기에 제한이 없으며, 필요한 곳에 집중적으로 자원을 할당할 수 있다.22</li>
<li><strong>성능:</strong> SplaTAM은 기존 Neural SLAM 대비 카메라 포즈 추정 정확도와 맵 품질에서 최대 2배 이상의 성능 향상을 보였으며, 특히 흐릿한 이미지(Motion Blur)나 텍스처가 부족한 환경에서도 강건한 성능을 입증했다.8</li>
</ul>
<h3>4.2  GS-SLAM: 적응형 확장과 Coarse-to-Fine 추적</h3>
<p><strong>GS-SLAM</strong>은 3DGS의 효율성을 극대화하여 렌더링 속도와 맵핑 품질의 균형을 맞춘 시스템이다. GS-SLAM은 맵핑 과정에서 가우시안의 생성과 소멸을 동적으로 관리하는 **적응형 확장 전략(Adaptive Expansion Strategy)**을 핵심으로 한다.9</p>
<ul>
<li><strong>적응형 맵 관리:</strong> 기하학적 구조가 복잡하거나 디테일이 필요한 곳에는 가우시안을 분할(Split)하거나 복제(Clone)하여 밀도를 높이고, 불필요하거나 노이즈로 판명된 가우시안은 과감히 제거(Prune)한다. 이는 전체 가우시안의 수를 최적의 수준으로 유지하여 메모리 낭비를 막고 렌더링 속도를 보장한다.9</li>
<li><strong>Coarse-to-Fine 추적:</strong> 실시간성을 확보하기 위해 다단계 추적 전략을 사용한다. 초기에는 저해상도 이미지나 맵의 일부 가우시안만을 사용하여 대략적인 포즈를 빠르게 추정하고, 이후 전체 해상도와 모든 가우시안을 사용하여 미세 조정을 수행한다. 이를 통해 빠르면서도 정밀한 위치 추정이 가능하다.9</li>
<li><strong>압도적인 렌더링 속도:</strong> GS-SLAM은 평균 386 FPS라는 경이로운 렌더링 속도를 달성했다. 비록 SLAM 시스템 전체(추적, 맵핑 포함)의 구동 속도는 약 8~10 FPS 수준이지만, 맵의 시각화 및 상호작용 측면에서는 타의 추종을 불허한다.9</li>
</ul>
<h2>5.  핵심 알고리즘 심층 분석: 추적과 맵핑의 메커니즘</h2>
<p>Neural SLAM 시스템이 실시간으로 고밀도 지도를 작성하기 위해 사용하는 핵심 기술 요소들을 수학적 원리와 함께 심층 분석한다.</p>
<h3>5.1  추적(Tracking): 미분 가능한 렌더링과 손실 함수</h3>
<p>Neural SLAM의 추적은 전통적인 특징점 매칭(Feature Matching) 방식과 달리, 현재 맵에서 렌더링된 이미지와 실제 입력 이미지 간의 차이를 픽셀 단위로 최소화하는 <strong>Direct Method</strong> 방식을 따른다. 이 과정은 전적으로 미분 가능한 렌더링 파이프라인 덕분에 가능하다.</p>
<p>최적화하고자 하는 카메라 포즈를 <span class="math math-inline">\mathbf{T} \in SE(3)</span>라고 할 때, 손실 함수 <span class="math math-inline">\mathcal{L}</span>은 광도 손실 <span class="math math-inline">\mathcal{L}*{photo}</span>와 기하학적 손실 <span class="math math-inline">\mathcal{L}*{geo}</span>의 가중 합으로 정의된다.24</p>
<ol>
<li>광도 손실 (Photometric Loss):</li>
</ol>
<p>렌더링된 RGB 이미지 <span class="math math-inline">I_{ren}(\mathbf{T})</span>와 실제 관측된 이미지 <span class="math math-inline">I_{gt}</span> 간의 차이를 최소화한다. 일반적으로 L1 또는 L2 Norm을 사용한다.<br />
<span class="math math-display">
   \mathcal{L}_{photo} = \sum_{\mathbf{p} \in \Omega} \| I_{ren}(\mathbf{p}, \mathbf{T}) - I_{gt}(\mathbf{p}) \|_1
</span><br />
여기서 <span class="math math-inline">\Omega</span>는 샘플링된 픽셀의 집합이다.</p>
<ol start="2">
<li>기하학적 손실 (Geometric Loss):</li>
</ol>
<p>RGB-D 카메라의 경우, 깊이(Depth) 정보가 함께 제공되므로 이를 적극 활용한다. 렌더링된 깊이 <span class="math math-inline">D_{ren}(\mathbf{T})</span>와 센서 깊이 <span class="math math-inline">D_{gt}</span> 간의 차이를 줄여 맵의 기하학적 정합성을 높인다.<br />
<span class="math math-display">
   \mathcal{L}_{geo} = \sum_{\mathbf{p} \in \Omega} \| D_{ren}(\mathbf{p}, \mathbf{T}) - D_{gt}(\mathbf{p}) \|_1
</span><br />
특히 Co-SLAM과 같은 시스템에서는 깊이 정보의 불확실성을 고려하여 오차가 큰 영역의 가중치를 낮추거나, L1 loss를 사용하여 아웃라이어(Outlier)에 강건하게 만든다.25</p>
<ol start="3">
<li>최적화 과정:</li>
</ol>
<p>NeRF나 3DGS는 미분 가능하므로, 위 손실 함수 <span class="math math-inline">\mathcal{L}</span>에 대해 카메라 포즈 파라미터 <span class="math math-inline">\mathbf{T}</span>로 편미분을 수행하여 그라디언트 <span class="math math-inline">\frac{\partial \mathcal{L}}{\partial \mathbf{T}}</span>를 구한다. 이를 Adam이나 SGD 같은 옵티마이저를 통해 역전파함으로써 포즈를 반복적으로 갱신한다. 3DGS 기반의 SplaTAM은 가우시안 파라미터를 고정한 채 카메라 포즈에 대해서만 그라디언트를 흘려보내는 방식으로 매우 빠른 수렴 속도를 달성했다.22</p>
<h3>5.2  맵핑(Mapping): 능동적 관리와 키프레임 전략</h3>
<p>맵핑은 환경 모델(신경망 가중치 또는 가우시안 파라미터)을 업데이트하여 새로운 관측을 반영하는 과정이다.</p>
<ul>
<li>키프레임 선정(Keyframe Selection):</li>
</ul>
<p>실시간 SLAM에서 모든 프레임을 학습에 사용하는 것은 불가능하다. 따라서 정보량이 많은 프레임을 선별하는 것이 중요하다.</p>
<ul>
<li>
<p><strong>iMAP:</strong> 현재 맵으로 렌더링했을 때 손실(Loss)이 큰 프레임, 즉 현재 맵이 잘 설명하지 못하는 프레임을 키프레임으로 선정하여 정보 획득(Information Gain)을 최대화한다.3</p>
</li>
<li>
<p><strong>NICE-SLAM / Co-SLAM:</strong> 현재 뷰와 시각적 중첩(Visual Overlap)이 있는 키프레임들을 선택한다. 이는 국소적인 번들 조정(Local Bundle Adjustment)을 수행할 때 연산 효율을 높이고, 불필요한 영역의 업데이트를 방지하여 망각 현상을 줄이는 데 기여한다.14 NICE-SLAM은 iMAP과 달리 전역 리플레이 버퍼에 덜 의존하며, 국소적 업데이트 특성을 활용해 중첩된 키프레임만으로도 안정적인 맵핑을 수행한다.</p>
</li>
<li>
<p>밀도 제어(Densification &amp; Pruning):</p>
</li>
</ul>
<p>3DGS 기반 SLAM에서 가장 중요한 맵핑 전략이다. 가우시안 맵은 고정된 해상도가 없으므로, 학습 과정에서 가우시안의 위치 그라디언트(Positional Gradient)가 큰 영역(즉, 구조적 변화가 심한 곳)에 가우시안을 복제하거나 분할한다. 반면, 불투명도(Opacity)가 임계값 이하로 떨어지거나 크기가 지나치게 커진 가우시안은 제거하여 맵을 정제(Clean-up)한다.26 이는 맵의 품질을 유지하면서도 메모리 폭발을 막는 핵심 기법이다.</p>
<h2>6.  장기적 일관성과 루프 결합(Loop Closure)의 난제</h2>
<p>전통적인 SLAM 시스템에서 **루프 결합(Loop Closure)**은 로봇이 이전에 방문했던 장소를 재방문했을 때, 누적된 위치 오차(Drift)를 수정하여 지도의 전역적 일관성(Global Consistency)을 확보하는 필수적인 기능이다. 그러나 Neural SLAM, 특히 임플리싯 표현을 사용하는 방식에서는 루프 결합이 기술적 난제로 남아 있다.</p>
<h3>6.1  Neural Representation에서의 루프 결합 문제</h3>
<p>신경망 가중치에 맵이 ‘녹아있는’ 임플리싯 방식에서는, 루프 결합 시 발생하는 기하학적 수정(Deformation)을 적용하기가 매우 어렵다. 전통적인 방식은 포즈 그래프 최적화(Pose Graph Optimization) 후 3D 포인트들의 위치를 수정하면 되지만, 신경망은 입력 좌표와 출력 간의 비선형적인 관계 때문에 맵의 특정 부분만을 비틀거나 이동시키는 것이 불가능에 가깝다. 단순히 포즈만 수정하고 재학습을 시키면 시간이 오래 걸릴 뿐만 아니라, 이미 학습된 맵의 구조가 깨질 위험이 있다.16 NICE-SLAM이나 iMAP은 이러한 이유로 초기 버전에서 루프 결합 기능을 포함하지 않았다.20</p>
<h3>6.2  최신 해결책: 서브맵(Sub-map)과 변형 가능한 표현</h3>
<p>이를 해결하기 위해 최신 연구들은 전체 맵을 하나의 네트워크나 그리드로 표현하는 대신, 여러 개의 독립적인 **서브맵(Sub-map)**으로 나누어 관리하는 방식을 도입하고 있다.</p>
<ul>
<li><strong>Loopy-SLAM:</strong> 이 시스템은 Neural Point Cloud 기반의 서브맵 전략을 사용한다. 로봇이 이동함에 따라 일정 크기의 서브맵을 생성하고, 각 서브맵은 독립적인 로컬 좌표계를 가진다. 전역 장소 인식(Global Place Recognition)을 통해 루프가 감지되면, 포즈 그래프 최적화를 수행하여 서브맵들의 상대적 위치(Rigid Transformation)만을 수정한다.27 서브맵 내부의 신경망 특징은 수정할 필요 없이 위치만 재정렬되므로 효율적이다.</li>
<li><strong>3DGS의 유연성:</strong> 3D Gaussian Splatting은 가우시안 점들의 집합이므로, 포인트 클라우드와 유사하게 공간적 변형이 자유롭다. 따라서 3DGS 기반 SLAM은 상대적으로 루프 결합 적용이 용이하며, 가우시안의 위치를 직접 수정함으로써 맵의 오차를 보정할 수 있는 잠재력을 가지고 있다.30</li>
</ul>
<h2>7.  로봇 내비게이션 및 동적 환경 대응</h2>
<p>Neural SLAM이 생성한 지도는 단순히 시각적인 복원을 넘어, 로봇의 실제 주행과 상호작용에 활용되어야 한다.</p>
<h3>7.1  충돌 회피와 SDF의 우수성</h3>
<p>로봇 내비게이션을 위해서는 장애물과의 거리를 아는 것이 필수적이다. NeRF 기반의 <strong>밀도(Density)</strong> 필드는 공간의 점유 여부(Occupancy)만을 확률적으로 알려주기 때문에, 장애물까지의 정확한 거리나 충돌 회피를 위한 그라디언트(Gradient)를 계산하기 어렵다. 반면, ESLAM이나 Co-SLAM이 사용하는 <strong>SDF(Signed Distance Function)</strong> 표현은 표면까지의 최단 거리를 부호 있는 값으로 제공한다. SDF는 공간 전체에서 미분 가능하므로, 로봇의 경로 계획(Path Planning) 알고리즘이 충돌 비용(Collision Cost)의 그라디언트를 직접 계산하여 장애물을 부드럽게 회피하는 궤적을 생성하는 데 매우 유리하다.31 이는 Neural SLAM이 단순한 시각화를 넘어 물리적 상호작용을 위한 기반 기술로 자리 잡는 데 중요한 요소이다.</p>
<h3>7.2  동적 환경(Dynamic Environments)의 극복</h3>
<p>대부분의 Neural SLAM은 환경이 정적(Static)이라고 가정한다. 그러나 현실 세계에는 사람이나 반려동물 같은 동적 객체가 존재한다. 이러한 객체는 맵핑 과정에서 ‘유령(Ghost)’ 같은 아티팩트를 남기거나 카메라 추적을 방해한다. 이를 해결하기 위해 Robust NeRF나 마스킹 기법이 연구되고 있다. 예를 들어, 렌더링 오차가 큰 영역(동적 객체일 확률이 높음)을 학습에서 배제하거나, 광학 흐름(Optical Flow) 및 시맨틱 세그멘테이션(Semantic Segmentation)을 활용하여 동적 객체를 사전에 마스킹(Masking)하는 방식이 적용되고 있다.21</p>
<h2>8.  성능 평가 및 벤치마크 분석</h2>
<p>Neural SLAM 시스템들의 성능은 주로 궤적 오차(ATE RMSE), 처리 속도(FPS), 메모리 효율성으로 평가된다. Replica, ScanNet, TUM RGB-D 데이터셋이 표준 벤치마크로 활용된다.</p>
<h3>8.1  추적 정확도 (ATE RMSE) 비교</h3>
<p>Replica 데이터셋을 기준으로 한 주요 모델들의 궤적 오차 비교는 다음과 같다.8 (단위: cm, 낮을수록 우수)</p>
<p><strong>[표 5.5.2] Replica 데이터셋에서의 ATE RMSE 성능 비교</strong></p>
<table><thead><tr><th><strong>모델</strong></th><th><strong>ATE RMSE (cm)</strong></th><th><strong>분석 및 특징</strong></th></tr></thead><tbody>
<tr><td><strong>iMAP</strong></td><td>2.58 - 5.0+</td><td>초기 모델로 오차가 큼. 망각 문제와 낮은 해상도로 인해 정밀한 추적 실패.</td></tr>
<tr><td><strong>NICE-SLAM</strong></td><td>1.69</td><td>iMAP 대비 대폭 개선. 계층적 그리드를 통해 기하학적 정확도 확보.</td></tr>
<tr><td><strong>Co-SLAM</strong></td><td>0.86 - 1.4</td><td>해시 그리드와 전역 최적화 덕분에 매우 우수한 추적 성능.</td></tr>
<tr><td><strong>ESLAM</strong></td><td>0.63 - 0.7</td><td>Tri-plane 기반의 효율적 표현으로 1cm 미만의 정밀도 달성.</td></tr>
<tr><td><strong>Point-SLAM</strong></td><td>0.53</td><td>Neural Point Cloud 방식. 표면 근처의 집중적 표현으로 정확도 높음.</td></tr>
<tr><td><strong>GS-SLAM</strong></td><td><strong>0.50</strong></td><td>3DGS 기반. 명시적 표현과 3DGS의 최적화 효율성으로 매우 높은 정확도.37</td></tr>
<tr><td><strong>SplaTAM</strong></td><td><strong>0.36</strong></td><td><strong>SOTA (State-of-the-art).</strong> 실루엣 마스크와 명시적 가우시안 관리로 오차를 극소화.8</td></tr>
</tbody></table>
<p>분석 결과:</p>
<p>3DGS 기반의 SplaTAM과 GS-SLAM이 기존 NeRF 기반(iMAP, NICE-SLAM) 방식보다 월등히 낮은 오차율을 기록하고 있다. 이는 명시적인 가우시안 표현이 국소적인 최적화에 유리하고, 그라디언트 전파가 직접적이기 때문이다. 또한, SplaTAM의 실루엣 마스크 전략이 미탐사 영역에서의 오차를 효과적으로 차단했음을 시사한다. 하이브리드 방식인 Co-SLAM과 ESLAM 역시 1cm 미만의 높은 정밀도를 보여주며, 특히 메모리가 제한된 환경에서는 3DGS 방식보다 효율적인 대안이 될 수 있다.</p>
<h3>8.2  처리 속도 및 메모리 효율성</h3>
<ul>
<li><strong>속도(FPS):</strong> iMAP은 추적에 2~5 FPS 수준으로 느리지만, Co-SLAM은 10~17 FPS로 안정적인 실시간성을 확보했다.19 GS-SLAM의 경우 렌더링 속도는 386 FPS에 달하나, 전체 SLAM 파이프라인(추적+맵핑)은 약 8~10 FPS로 구동된다. 이는 맵핑(가우시안 추가/삭제) 과정의 연산 부하 때문이다.24</li>
<li><strong>메모리:</strong> iMAP은 단일 MLP를 사용하므로 메모리 사용량이 극히 적다(&lt;1MB). 반면, NICE-SLAM이나 3DGS 기반 방식은 맵의 크기에 비례하여 메모리 사용량이 증가한다. 특히 SplaTAM과 GS-SLAM은 수백만 개의 가우시안을 저장해야 하므로 VRAM 소모가 크며, 대규모 환경에서는 수 기가바이트(GB) 이상의 메모리를 요구할 수 있다.40</li>
</ul>
<h2>9.  결론 및 미래 전망</h2>
<p>Neural SLAM은 딥러닝의 강력한 표현력과 전통적인 SLAM의 정교함을 결합하여, 로봇이 공간을 인식하는 방식을 근본적으로 혁신하고 있다. iMAP에서 시작된 ’단일 신경망 맵핑’의 아이디어는 ’치명적 망각’이라는 난관을 넘어, NICE-SLAM과 Co-SLAM의 ’하이브리드 아키텍처’를 통해 확장성과 실용성을 확보했다. 그리고 최근 <strong>3D Gaussian Splatting</strong>의 도입은 실시간성과 사진 품질의 렌더링이라는 두 가지 이상적인 목표를 동시에 달성하는 쾌거를 이루었다.</p>
<p>특히 SplaTAM과 GS-SLAM이 보여준 밀도 높은 지도의 실시간 작성 능력은 자율 주행, AR/VR, 로봇 조작(Manipulation) 등 다양한 응용 분야에서 파급력을 가질 것이다. 향후 연구는 동적 환경에 대한 강건성 확보, 엣지 디바이스 구동을 위한 모델 경량화, 그리고 시맨틱 정보(Semantic Information)와의 결합을 통한 ‘의미론적 3D 지도’ 작성으로 나아갈 것이다. 궁극적으로 Neural SLAM은 로봇에게 단순한 기하학적 정보를 넘어, 공간의 의미와 맥락까지 이해하는 진정한 **공간 지능(Spatial Intelligence)**을 부여하는 핵심 기술이 될 것이다.</p>
<h2>10. 참고 자료</h2>
<ol>
<li>Real-Time Tracking SLAM System - Emergent Mind, https://www.emergentmind.com/topics/real-time-tracking-slam-system</li>
<li>Role of Deep Learning in Loop Closure Detection for Visual and Lidar SLAM: A Survey - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC7916334/</li>
<li>iMAP: Implicit Mapping and Positioning in Real-Time - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2021/papers/Sucar_iMAP_Implicit_Mapping_and_Positioning_in_Real-Time_ICCV_2021_paper.pdf</li>
<li>iMAP: Implicit Mapping and Positioning in Real-Time - Edgar Sucar, https://edgarsucar.github.io/iMAP/</li>
<li>poster_nice (2) - Songyou Peng, https://pengsongyou.github.io/media/nice-slam/poster_nice-slam.pdf</li>
<li>Co-SLAM: Joint Coordinate and Sparse Parametric Encodings for Neural Real-Time SLAM - UCL Discovery, https://discovery.ucl.ac.uk/10179946/1/Wang_Co-SLAM_Joint_Coordinate_and_Sparse_Parametric_Encodings_for_Neural_Real-Time_CVPR_2023_paper.pdf</li>
<li>ESLAM: Efficient Dense SLAM System Based on Hybrid Representation of Signed Distance Fields, https://openaccess.thecvf.com/content/CVPR2023/papers/Johari_ESLAM_Efficient_Dense_SLAM_System_Based_on_Hybrid_Representation_of_CVPR_2023_paper.pdf</li>
<li>SplaTAM.pdf, https://spla-tam.github.io/assets/SplaTAM.pdf</li>
<li>GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting, https://gs-slam.github.io/</li>
<li>[2103.12352] iMAP: Implicit Mapping and Positioning in Real-Time - arXiv, https://arxiv.org/abs/2103.12352</li>
<li>Geethika CS391R Presentation - iMAP, https://www.cs.utexas.edu/~yukez/cs391r_fall2023/slides/pre_09-07_Geethika.pdf</li>
<li>FeatureSLAM: Feature-enriched 3D gaussian splatting SLAM in real time - arXiv, https://arxiv.org/html/2601.05738v1</li>
<li>Region sampling NeRF-SLAM based on Kolmogorov-Arnold network | PLOS One, https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0325024</li>
<li>NICE-SLAM: Neural Implicit Scalable Encoding for SLAM - Songyou Peng, https://pengsongyou.github.io/media/nice-slam/NICE-SLAM.pdf</li>
<li>Overcoming Catastrophic Forgetting by Incremental Moment Matching - NIPS, https://papers.nips.cc/paper/7051-overcoming-catastrophic-forgetting-by-incremental-moment-matching</li>
<li>NICE-SLAM - Songyou Peng, https://pengsongyou.github.io/nice-slam</li>
<li>cvg/nice-slam: [CVPR’22] NICE-SLAM: Neural Implicit Scalable Encoding for SLAM - GitHub, https://github.com/cvg/nice-slam</li>
<li>Co-SLAM: Joint Coordinate and Sparse Parametric Encodings for Neural Real-Time SLAM, https://hengyiwang.github.io/projects/CoSLAM.html</li>
<li>Co-SLAM: Joint Coordinate and Sparse Parametric Encodings for Neural Real-Time SLAM - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Co-SLAM_Joint_Coordinate_and_Sparse_Parametric_Encodings_for_Neural_Real-Time_CVPR_2023_paper.pdf</li>
<li>NICE-SLAM: Neural Implicit Scalable Encoding for SLAM - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_NICE-SLAM_Neural_Implicit_Scalable_Encoding_for_SLAM_CVPR_2022_paper.pdf</li>
<li>Adaptively Structured Gaussian SLAM with Uncertainty Prediction in Dynamic Environments, https://arxiv.org/html/2505.22335v1</li>
<li>SplaTAM: Splat, Track &amp; Map 3D Gaussians for Dense RGB-D SLAM, https://spla-tam.github.io/</li>
<li>GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_GS-SLAM_Dense_Visual_SLAM_with_3D_Gaussian_Splatting_CVPR_2024_paper.pdf</li>
<li>GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting - arXiv, https://arxiv.org/html/2311.11700v4</li>
<li>SLAM Meets NeRF: A Survey of Implicit SLAM Methods - MDPI, https://www.mdpi.com/2032-6653/15/3/85</li>
<li>GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting - IEEE Xplore, https://ieeexplore.ieee.org/document/10657581/</li>
<li>Loopy-SLAM: Dense Neural SLAM with Loop Closures - arXiv, https://arxiv.org/html/2402.09944v2</li>
<li>[Quick Review] Loopy-SLAM: Dense Neural SLAM with Loop Closures - Liner, https://liner.com/review/loopyslam-dense-neural-slam-with-loop-closures</li>
<li>Loopy-SLAM: Dense Neural SLAM with Loop Closures - GitHub Pages, https://notchla.github.io/Loopy-SLAM/</li>
<li>Open-Set Semantic Gaussian Splatting SLAM with Expandable Representation, https://openreview.net/forum?id=E68dgQUzrC</li>
<li>Differentiable Composite Neural Signed Distance Fields for Robot Navigation in Dynamic Indoor Environments - arXiv, https://arxiv.org/html/2502.02664v1</li>
<li>Predicted Composite Signed-Distance Fields for Real-Time Motion Planning in Dynamic Environments - Ioannis Havoutis, https://ihavoutis.github.io/publications/2021/ICAPS2021_Finean.pdf</li>
<li>[2502.02664] Differentiable Composite Neural Signed Distance Fields for Robot Navigation in Dynamic Indoor Environments - arXiv, https://arxiv.org/abs/2502.02664</li>
<li>Embracing Dynamics: Dynamics-aware 4D Gaussian Splatting SLAM - arXiv, https://arxiv.org/html/2504.04844v1</li>
<li>JingwenWang95/neural_slam_eval: Neural SLAM Evaluation Benchmark. [CVPR’23] Co-SLAM - GitHub, https://github.com/JingwenWang95/neural_slam_eval</li>
<li>arXiv:2403.12550v2 [cs.CV] 22 Mar 2024, <a href="https://arxiv.org/pdf/2403.12550">https://arxiv.org/pdf/2403.12550?</a></li>
<li>Gaussian Splatting SLAM - arXiv, https://arxiv.org/html/2312.06741v2</li>
<li>GS LAM: Gaussian Semantic Splatting SLAM - GitHub Pages, https://cslinzhang.github.io/home/files/GS3LAM.pdf</li>
<li>RD-SLAM: Real-Time Dense SLAM Using Gaussian Splatting - MDPI, https://www.mdpi.com/2076-3417/14/17/7767</li>
<li>Splat-SLAM: Globally Optimized RGB-only SLAM with 3D Gaussians - arXiv, https://arxiv.org/html/2405.16544v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>