<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:5.3 기하학적 정밀함: SDF와 점유 네트워크 (Occupancy Networks)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>5.3 기하학적 정밀함: SDF와 점유 네트워크 (Occupancy Networks)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 5. 뉴럴 3D 표현과 렌더링 (Neural 3D Representations)</a> / <a href="index.html">5.3 기하학적 정밀함: SDF와 점유 네트워크 (Occupancy Networks)</a> / <span>5.3 기하학적 정밀함: SDF와 점유 네트워크 (Occupancy Networks)</span></nav>
                </div>
            </header>
            <article>
                <h1>5.3 기하학적 정밀함: SDF와 점유 네트워크 (Occupancy Networks)</h1>
<p>로봇 공학, 특히 물리적 신체를 가지고 환경과 상호작용하는 Embodied AI의 영역에서, 3차원 공간을 어떻게 표현(Representation)하고 이해할 것인가의 문제는 단순한 인지(Perception)의 차원을 넘어 행동(Action)의 성패를 좌우하는 핵심 기제이다. 로봇이 복잡한 환경에서 장애물을 회피하거나, 미지의 물체를 안정적으로 파지(Grasping)하고, 나아가 물리적 시뮬레이션을 통해 미래를 예측하기 위해서는 대상을 시각적으로 인식하는 것을 넘어, 그 형상의 기하학적 본질을 수학적으로 정밀하게 모델링해야 한다.</p>
<p>본 장에서는 3D 비전과 로봇 제어의 패러다임을 이산적(Discrete) 데이터 처리에서 연속적(Continuous) 함수 공간의 해석으로 전환시킨 두 가지 기념비적인 연구, <strong>DeepSDF</strong>와 <strong>Occupancy Networks</strong>를 중심으로, 암시적 신경 표현(Implicit Neural Representations)이 가져온 기하학적 혁명을 심층적으로 분석한다. 이 기술들은 뉴럴 네트워크가 단순히 데이터를 분류하는 것을 넘어, 기하학적 형상 그 자체를 내재화된 함수의 형태로 기억하고 생성할 수 있음을 증명하였으며, 이는 현대 로봇 공학이 추구하는 ’일반 범용 로봇(Generalist Robot)’을 위한 공간 지능의 토대가 되고 있다.</p>
<h2>1.  이산적 표현의 한계와 함수 공간으로의 전환</h2>
<p>전통적인 로봇 공학 및 컴퓨터 그래픽스 분야에서 3차원 형상을 다루는 방식은 오랜 기간 동안 명시적(Explicit)이고 이산적인 표현법에 의존해 왔다. 대표적으로 복셀(Voxel), 포인트 클라우드(Point Cloud), 그리고 메쉬(Mesh)가 있다. 이들은 각각의 장점을 가지고 있음에도 불구하고, 고도화된 AI 로봇이 요구하는 정밀성과 효율성을 동시에 만족시키기에는 본질적인 한계에 직면했다.1</p>
<h3>1.1 복셀 그리드와 차원의 저주</h3>
<p>복셀 그리드는 3차원 공간을 규칙적인 격자(Grid)로 분할하여 각 격자에 점유 여부나 밀도 값을 할당하는 방식이다. 이 방식은 2차원 이미지 처리에 최적화된 합성곱 신경망(CNN)을 3차원으로 확장하기 용이하다는 장점이 있어 초기 3D 딥러닝 연구에서 널리 사용되었다. 그러나 복셀 표현은 해상도가 증가함에 따라 메모리 사용량이 세제곱(<span class="math math-inline">O(N^3)</span>)으로 급증하는 ‘차원의 저주(Curse of Dimensionality)’ 문제를 안고 있다.2</p>
<p>로봇이 정교한 조작 작업을 수행하기 위해 밀리미터(mm) 단위의 해상도가 필요한 경우, 복셀 방식은 감당할 수 없는 수준의 메모리와 연산량을 요구한다. 예를 들어 <span class="math math-inline">512^3</span> 해상도의 그리드만 해도 수억 개의 데이터 포인트가 필요하며, 이는 대부분의 빈 공간(Empty Space)을 표현하는 데 낭비된다. 옥트리(Octree)와 같은 계층적 자료구조를 통해 이를 완화하려는 시도가 있었으나, 구현의 복잡성과 CNN 적용의 난해함은 여전한 과제로 남았다. 또한, 복셀은 본질적으로 축에 정렬된(Axis-aligned) 구조를 가지므로, ‘맨해튼 월드(Manhattan World)’ 편향을 유발하여 곡면이나 사선 형태의 표면을 계단식으로 거칠게 표현하는 앨리어싱(Aliasing) 문제를 피할 수 없다.2</p>
<h3>1.2 포인트 클라우드와 위상 정보의 부재</h3>
<p>포인트 클라우드는 LiDAR나 RGB-D 카메라와 같은 센서로부터 획득되는 가장 원초적인 형태의 데이터이다. 이는 3차원 표면을 점들의 집합으로 표현하므로 복셀에 비해 메모리 효율적이며, 센서 데이터와의 정합성이 높다. PointNet과 같은 선구적인 연구들은 포인트 클라우드를 직접 딥러닝의 입력으로 사용하여 객체 인식 및 분할에서 괄목할 만한 성과를 거두었다.</p>
<p>그러나 포인트 클라우드는 기하학적 연결성(Topology) 정보를 포함하지 않는다.2 점과 점 사이의 공간이 비어 있는지, 아니면 표면으로 연결되어 있는지에 대한 정보가 없기 때문에, 로봇이 물체의 ’안’과 ’밖’을 명확히 구분해야 하는 충돌 감지나 물리 시뮬레이션에 직접 적용하기 어렵다. 또한, 센서 노이즈에 민감하고 샘플링 밀도에 따라 형상 정보의 품질이 크게 달라지는 단점이 있다.</p>
<h3>1.3 메쉬와 위상의 고정성</h3>
<p>메쉬는 꼭짓점(Vertex), 모서리(Edge), 면(Face)으로 구성되어 표면의 기하학적 정보와 위상 정보를 모두 포함하는 표현 방식이다. 그래픽스 렌더링 파이프라인과의 호환성이 뛰어나지만, 딥러닝 모델을 통해 메쉬를 생성하는 것은 매우 까다롭다. 대부분의 메쉬 생성 네트워크는 구(Sphere)와 같은 템플릿 메쉬를 변형(Deform)하는 방식을 취하는데, 이는 생성 가능한 객체의 위상이 템플릿의 위상(Topology)에 종속됨을 의미한다.2 예를 들어, 구 형태의 템플릿으로는 도넛 모양(Torus)이나 구멍이 뚫린 복잡한 의자 등받이와 같은 형상을 자연스럽게 표현할 수 없다. 이는 다양한 형태의 미지 객체를 다뤄야 하는 로봇에게 치명적인 제약이 된다.</p>
<h3>1.4 암시적 신경 표현(INR)의 등장</h3>
<p>이러한 배경에서 등장한 **암시적 신경 표현(Implicit Neural Representation, INR)**은 3D 형상을 이산적인 데이터 포인트의 집합이 아닌, 연속적인 함수(Continuous Function)로 정의하는 혁신적인 접근법이다. 이 패러다임에서 신경망은 3차원 좌표 <span class="math math-inline">x \in \mathbb{R}^3</span>를 입력받아, 해당 위치의 기하학적 속성(점유 확률, 거리 등)을 출력하는 함수 근사기(Function Approximator)로 작동한다.4<br />
<span class="math math-display">
f_\theta : \mathbb{R}^3 \to \mathbb{R}^n
</span><br />
이 방식은 이론적으로 **무한한 해상도(Infinite Resolution)**를 가진다.6 해상도는 데이터 저장 방식에 의해 결정되는 것이 아니라, 사용자가 함수에 질의(Query)하는 샘플링 밀도에 따라 결정되기 때문이다. 따라서 메모리 효율성이 극대화되며, 복잡한 위상을 가진 객체라도 별도의 템플릿 없이 자연스럽게 표현할 수 있다. 이 흐름을 주도한 두 가지 핵심 기술이 바로 <strong>Occupancy Networks</strong>와 <strong>DeepSDF</strong>이다.</p>
<h2>2.  Occupancy Networks: 확률적 점유의 연속 함수</h2>
<p>2019년 CVPR에서 Mescheder 등이 제안한 <code>Occupancy Networks: Learning 3D Reconstruction in Function Space</code>는 3D 기하학을 딥러닝 분류기(Classifier)의 **연속적 결정 경계(Continuous Decision Boundary)**로 해석하는 새로운 시각을 제시했다.6</p>
<h3>2.1 점유 함수(Occupancy Function)의 정의</h3>
<p>Occupancy Network의 핵심 아이디어는 3차원 공간상의 임의의 점 <span class="math math-inline">p \in \mathbb{R}^3</span>가 물체의 내부에 속하는지 여부를 나타내는 이진 함수인 점유 함수(Occupancy Function) <span class="math math-inline">o: \mathbb{R}^3 \to {0, 1}</span>를 신경망으로 근사하는 것이다.8<br />
<span class="math math-display">
o(p) = \begin{cases} 1 &amp; \text{if } p \in \mathcal{S} \\ 0 &amp; \text{if } p \notin \mathcal{S} \end{cases}
</span><br />
여기서 <span class="math math-inline">\mathcal{S}</span>는 물체가 차지하는 체적을 의미한다. 신경망 <span class="math math-inline">f_\theta</span>는 입력 좌표 <span class="math math-inline">p</span>와 관측 조건 <span class="math math-inline">x</span>(예: 이미지, 포인트 클라우드)를 받아 0과 1 사이의 점유 확률을 출력한다.6<br />
<span class="math math-display">
f_\theta : \mathbb{R}^3 \times \mathcal{X} \to 
</span><br />
물체의 표면(Surface)은 이 점유 확률이 임계값 <span class="math math-inline">\tau</span>(통상 0.5)가 되는 등위면(Iso-surface)으로 정의된다.<br />
<span class="math math-display">
\mathcal{S}_{\text{surface}} = \{ p \in \mathbb{R}^3 \mid f_\theta(p, x) = \tau \}
</span><br />
이러한 접근은 3D 재구성(Reconstruction) 문제를 복잡한 기하학적 생성 문제가 아닌, 공간상의 점들에 대한 <strong>이진 분류(Binary Classification)</strong> 문제로 단순화시킨다. 이는 학습의 안정성을 크게 높이고, 기존 분류기 모델들의 강력한 성능을 3D 도메인으로 전이할 수 있게 한다.2</p>
<h3>2.2 네트워크 아키텍처와 조건부 배치 정규화(CBN)</h3>
<p>Occupancy Networks는 입력 데이터의 형태에 구애받지 않고 유연하게 대응할 수 있는 인코더-디코더(Encoder-Decoder) 구조를 채택한다.</p>
<ol>
<li><strong>인코더(Encoder):</strong> 관측된 데이터 <span class="math math-inline">x</span>를 잠재 벡터(Latent Vector) <span class="math math-inline">c</span>로 압축한다. 입력 데이터의 종류에 따라 다양한 신경망이 사용된다.6</li>
</ol>
<ul>
<li><strong>단일 이미지:</strong> ResNet-18과 같은 2D CNN을 사용하여 이미지 특징을 추출한다.</li>
<li><strong>포인트 클라우드:</strong> PointNet을 사용하여 점들의 순서에 불변(Order-invariant)한 전역 특징을 추출한다.</li>
<li><strong>복셀 그리드:</strong> 3D CNN을 사용하여 공간적 특징을 압축한다.</li>
</ul>
<ol start="2">
<li><strong>디코더(Decoder):</strong> 좌표 <span class="math math-inline">p</span>와 잠재 벡터 <span class="math math-inline">c</span>를 입력받아 점유 확률을 예측한다. 여기서 단순한 연결(Concatenation) 대신 **조건부 배치 정규화(Conditional Batch Normalization, CBN)**를 사용하는 것이 성능 향상의 핵심 요인이다.6</li>
</ol>
<p>일반적인 배치 정규화(Batch Normalization)가 미니 배치의 평균과 분산을 이용하여 정규화를 수행하는 반면, CBN은 정규화 파라미터인 스케일 <span class="math math-inline">\gamma</span>와 시프트 <span class="math math-inline">\beta</span>를 잠재 벡터 <span class="math math-inline">c</span>의 함수로 생성한다.<br />
<span class="math math-display">
\hat{h}_l = \frac{h_l - \mu}{\sigma}, \quad h_{l+1} = \gamma_l(c) \cdot \hat{h}_l + \beta_l(c)
</span><br />
여기서 <span class="math math-inline">h_l</span>은 <span class="math math-inline">l</span>번째 층의 활성화 값이다. <span class="math math-inline">\gamma_l(c)</span>와 <span class="math math-inline">\beta_l(c)</span>는 다층 퍼셉트론(MLP)을 통해 학습된다. 이 방식은 네트워크가 입력 조건(관측된 형상 정보)에 따라 기하학적 추론의 공간적 분포를 동적으로 조절할 수 있게 하여, 형상의 세밀한 디테일을 복원하는 데 탁월한 성능을 보인다.6 전체 디코더는 5개의 ResNet 블록으로 구성되며, 이는 깊은 네트워크에서도 기울기 소실 없이 안정적인 학습을 가능하게 한다.</p>
<h3>2.3 학습 목적 함수 (Loss Function)</h3>
<p>Occupancy Networks의 학습은 미니 배치 <span class="math math-inline">\mathcal{B}</span> 내에서 샘플링된 점들에 대한 예측 점유 확률과 실제 점유 값(Ground Truth) 사이의 차이를 최소화하는 방향으로 진행된다. 손실 함수 <span class="math math-inline">\mathcal{L}</span>은 변분 오토인코더(VAE)와 유사한 형태를 띠거나, 단순한 재구성 오차 최소화를 목표로 한다.2<br />
<span class="math math-display">
\mathcal{L}(\theta, \psi) = \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left( \sum_{j=1}^{K} \mathcal{L}_{\text{BCE}}(f_\theta(p_{ij}, z_i), o_{ij}) + \text{KL}(q_\psi(z|x_i) \parallel p_0(z)) \right)
</span></p>
<ul>
<li><span class="math math-inline">p_{ij}</span>: <span class="math math-inline">i</span>번째 객체의 공간상에서 무작위로 샘플링된 <span class="math math-inline">K</span>개의 점 (예: <span class="math math-inline">K=2048</span>).</li>
<li><span class="math math-inline">o_{ij}</span>: 해당 점의 실제 점유 여부 (0 또는 1).</li>
<li><span class="math math-inline">z_i</span>: 인코더 <span class="math math-inline">q_\psi</span>가 추론한 잠재 코드.</li>
<li><span class="math math-inline">\mathcal{L}_{\text{BCE}}</span>: 이진 교차 엔트로피(Binary Cross-Entropy) 손실 함수.</li>
<li><span class="math math-inline">\text{KL}</span>: 잠재 공간의 분포를 정규 분포 <span class="math math-inline">p_0(z)</span>로 정규화하기 위한 Kullback-Leibler 발산 항.</li>
</ul>
<p><strong>샘플링 전략의 중요성:</strong> 공간 전체를 균일하게 샘플링할 경우, 대부분의 점은 물체 외부의 빈 공간에 위치하게 되어 학습의 비효율성을 초래한다. 따라서 Occupancy Networks는 물체의 경계 상자(Bounding Box) 내부에서 균일하게 샘플링하는 방식과, 물체 표면 근처에서 더 밀집되게 샘플링하는 방식을 혼합하여 사용한다. 이는 표면의 디테일을 놓치지 않으면서도 전체적인 형상을 학습하는 데 필수적이다.6</p>
<h3>2.4 MISE: 다해상도 등위면 추출</h3>
<p>함수 형태의 출력 결과를 시각화하거나 로봇 시뮬레이터에서 사용하기 위해서는 결국 메쉬(Mesh) 형태로의 변환이 필요하다. 가장 일반적인 방법은 Marching Cubes 알고리즘을 사용하는 것이지만, 고해상도 그리드(예: <span class="math math-inline">512^3</span>) 전체에 대해 신경망을 평가하는 것은 연산 비용이 매우 높다. 이를 해결하기 위해 <strong>MISE (Multiresolution IsoSurface Extraction)</strong> 알고리즘이 도입되었다.6</p>
<ol>
<li><strong>초기 평가:</strong> 낮은 해상도(예: <span class="math math-inline">32^3</span>)의 그리드에서 점유 확률을 계산한다.</li>
<li><strong>활성 복셀 식별:</strong> 점유 확률이 임계값(0.5)을 가로지르는, 즉 표면이 존재할 것으로 예상되는 복셀(Active Voxel)을 식별한다.</li>
<li><strong>세분화(Subdivision):</strong> 활성 복셀을 8개의 하위 복셀로 나누고, 해당 영역에 대해서만 더 높은 해상도로 점유 확률을 재평가한다.</li>
<li><strong>반복:</strong> 목표 해상도에 도달할 때까지 이 과정을 반복한다.</li>
<li><strong>표면 추출:</strong> 최종적으로 활성화된 고해상도 그리드에 대해 Marching Cubes를 적용하여 메쉬를 생성한다.</li>
</ol>
<p>MISE 알고리즘은 옥트리(Octree) 구조와 유사하게 표면이 존재하는 영역에 자원을 집중함으로써, 불필요한 빈 공간에 대한 연산을 획기적으로 줄여 고해상도 재구성을 실시간에 준하는 속도로 가능하게 했다.6</p>
<h2>3.  DeepSDF: 기하학적 거리의 정밀한 측정</h2>
<p>Occupancy Networks가 공간의 ’안과 밖’이라는 위상적 상태(Topological State)에 집중했다면, Park 등이 2019년 CVPR에서 발표한 <code>DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation</code>은 표면까지의 ’거리(Metric)’라는 더 풍부한 기하학적 정보를 학습하는 데 초점을 맞추었다.1</p>
<h3>3.1 부호화 거리 함수(SDF)의 신경망 내재화</h3>
<p>SDF(Signed Distance Function)는 공간상의 점 <span class="math math-inline">x</span>에서 가장 가까운 표면까지의 유클리드 거리 <span class="math math-inline">s</span>를 반환하는 함수이다. 이때 거리의 부호(Sign)는 점이 물체 외부에 있으면 양수(+), 내부에 있으면 음수(-)로 정의된다.1<br />
<span class="math math-display">
SDF(x) = s, \quad \text{where } |s| = \min_{y \in \mathcal{S}_{\text{surface}}} \|x - y\|_2
</span><br />
DeepSDF는 이 수학적 함수를 다층 퍼셉트론(MLP) <span class="math math-inline">f_\theta</span>로 근사한다.<br />
<span class="math math-display">
f_\theta(z, x) \approx SDF(x)
</span><br />
여기서 <span class="math math-inline">z</span>는 특정 형상을 나타내는 잠재 코드(Latent Code)이다. DeepSDF의 혁신성은 단일 신경망으로 하나의 객체가 아닌, 전체 형상 클래스(Class of Shapes)를 표현할 수 있다는 점에 있다.1 잠재 코드 <span class="math math-inline">z</span>가 변화함에 따라 네트워크는 의자에서 책상으로, 또는 컵의 손잡이가 있는 형태에서 없는 형태로 연속적으로 변화하는 기하학적 장(Field)을 생성한다. 이는 로봇이 범주 수준(Category-level)의 지식을 가지고 미지의 물체를 추론하는 데 강력한 도구가 된다.14</p>
<h3>3.2 오토 디코더(Auto-Decoder) 프레임워크</h3>
<p>일반적인 생성 모델들이 인코더-디코더 구조를 사용하여 입력 데이터로부터 잠재 코드를 추출하는 것과 달리, DeepSDF는 **오토 디코더(Auto-Decoder)**라는 독창적인 학습 프레임워크를 제안했다.11 저자들은 인코더 네트워크가 완벽하지 않아 발생하는 잠재 코드의 오차를 배제하고, 형상 그 자체를 가장 잘 표현하는 잠재 공간(Latent Space)을 직접 학습하고자 했다.</p>
<p>학습 과정에서 네트워크 파라미터 <span class="math math-inline">\theta</span>와 각 학습 데이터 <span class="math math-inline">i</span>에 대응하는 잠재 코드 <span class="math math-inline">z_i</span>가 동시에 최적화된다.<br />
<span class="math math-display">
\min_{\theta, \{z_i\}_{i=1}^N} \sum_{i=1}^N \left( \sum_{j=1}^K \mathcal{L}(f_\theta(z_i, x_{ij}), s_{ij}) + \frac{1}{\sigma^2} \|z_i\|_2^2 \right)
</span></p>
<ul>
<li><span class="math math-inline">s_{ij}</span>: 점 <span class="math math-inline">x_{ij}</span>에서의 실제 SDF 값.</li>
<li><span class="math math-inline">\|z_i\|_2^2</span>: 잠재 코드가 원점 주변에 밀집되도록 유도하는 정규화 항(Prior). <span class="math math-inline">z_i</span>는 초기에 <span class="math math-inline">N(0, 0.01^2)</span> 분포로 무작위 초기화된다.13</li>
</ul>
<p>이 방식은 인코더가 없기 때문에, 추론(Inference) 단계에서도 최적화 과정이 필요하다. 즉, 새로운 관측 데이터(예: 부분적인 포인트 클라우드)가 주어졌을 때, 네트워크 가중치 <span class="math math-inline">\theta</span>는 고정한 상태에서 해당 데이터를 가장 잘 설명하는 잠재 코드 <span class="math math-inline">\hat{z}</span>를 찾기 위해 역전파(Back-propagation)를 수행한다.13<br />
<span class="math math-display">
\hat{z} = \operatorname*{argmin}_z \sum_{(x, s) \in X_{\text{obs}}} \mathcal{L}(f_\theta(z, x), s) + \frac{1}{\sigma^2} \|z\|_2^2
</span><br />
이러한 ‘추론을 통한 최적화(Optimization-at-Inference)’ 접근법은 노이즈가 많은 데이터나 일부가 가려진(Occluded) 데이터에 대해 매우 강건하며, 데이터의 결측 부분을 자연스럽게 채워 넣는 형상 완성(Shape Completion) 능력에서 타의 추종을 불허하는 성능을 보여주었다.11</p>
<h3>3.3 클램핑된 L1 손실 함수 (Clamped L1 Loss)</h3>
<p>SDF 회귀 학습에서 중요한 기술적 디테일은 손실 함수의 설계이다. 물체 표면에서 멀리 떨어진 공간의 정확한 거리 값은 형상을 복원하는 데 있어 상대적으로 중요도가 낮음에도 불구하고, 그 크기가 크기 때문에 L2 손실 함수 등에서 오차를 지배(Dominate)하는 경향이 있다. 이를 방지하고 표면 근처의 정밀한 기하학적 세부 사항에 집중하기 위해 DeepSDF는 <strong>Clamped L1 Loss</strong>를 도입했다.17<br />
<span class="math math-display">
\mathcal{L}(f_\theta(z, x), s) = | \operatorname{clamp}(f_\theta(z, x), \delta) - \operatorname{clamp}(s, \delta) |
</span><br />
여기서 <span class="math math-inline">\operatorname{clamp}(x, \delta) = \min(\delta, \max(-\delta, x))</span> 함수는 예측값과 실제값을 <span class="math math-inline">[-\delta, \delta]</span> 범위로 제한한다. <span class="math math-inline">\delta</span>는 표면 근처의 집중 영역을 정의하는 파라미터이다. 이 손실 함수는 네트워크가 표면의 디테일을 결정하는 0-레벨 셋(Zero-level set) 근처의 정확도를 우선적으로 학습하도록 유도한다. 이는 로봇이 물체를 파지하거나 충돌을 회피할 때 가장 중요한 정보가 바로 표면과의 근접 거리라는 점과 일맥상통한다.</p>
<h2>4.  비교 분석: 위상적 유연성, 정밀도, 그리고 효율성</h2>
<p>DeepSDF와 Occupancy Networks는 모두 암시적 표현을 통해 기존의 이산적 방법론들을 압도하는 성능을 보여주었다. 두 방법론과 기존 방식들의 특성을 비교 분석하면 다음과 같다.</p>
<p>기하학적 정밀도와 위상적 자유도:</p>
<p>DeepSDF는 Chamfer Distance(CD) 지표에서 기존의 최신 모델들(AtlasNet, OGN)을 큰 격차로 따돌리며 SOTA 성능을 기록했다.14 특히 위상적으로 복잡한 물체, 예를 들어 얇은 구조물이나 구멍이 많은 형상을 표현할 때 메쉬 기반 방법인 AtlasNet은 패치(Patch)들이 겹치거나 찢어지는 현상이 발생하는 반면, DeepSDF와 Occupancy Networks는 이를 매끄럽고 연속적인 표면으로 완벽하게 복원한다.6 이는 로봇이 복잡한 구조의 물체를 조작할 때 물리적 상호작용의 신뢰성을 보장하는 데 필수적인 특성이다.</p>
<p>메모리 효율성:</p>
<p>Occupancy Networks는 단 6백만(6M) 개의 파라미터만으로 ShapeNet 데이터셋의 수천 가지 형상을 고해상도로 표현할 수 있음을 증명했다.6 이는 동등한 해상도의 복셀 그리드가 기가바이트(GB) 단위의 메모리를 요구하는 것과 대조적이다. 이러한 압축 효율성은 로봇의 제한된 온보드(On-board) 메모리에 거대한 환경 지도를 저장할 수 있는 가능성을 열어주었다.</p>
<p>추론 시간과 실시간성:</p>
<p>Occupancy Networks는 인코더-디코더 구조를 사용하므로, 입력 데이터가 주어지면 한 번의 순전파(Forward pass)만으로 3D 형상을 추론할 수 있어 실시간 처리에 유리하다. 반면, DeepSDF의 오토 디코더 방식은 추론 시마다 경사 하강법을 통한 최적화 과정이 필요하므로 상대적으로 시간이 더 소요된다.19 그러나 최근 연구들(Meta-SDF 등)은 메타 러닝(Meta-learning)을 통해 최적화의 초기화 지점을 학습하거나, 인코더를 결합하여 초기 추정치를 제공함으로써 DeepSDF의 추론 속도를 획기적으로 개선하고 있다.19</p>
<table><thead><tr><th><strong>특성</strong></th><th><strong>Occupancy Networks</strong></th><th><strong>DeepSDF</strong></th><th><strong>복셀/포인트/메쉬</strong></th></tr></thead><tbody>
<tr><td><strong>출력 값</strong></td><td>점유 확률 $o \in $</td><td>부호화 거리 <span class="math math-inline">s \in \mathbb{R}</span></td><td>이산적 값/좌표</td></tr>
<tr><td><strong>표면 정의</strong></td><td>결정 경계 (<span class="math math-inline">o=0.5</span>)</td><td>제로 레벨 셋 (<span class="math math-inline">s=0</span>)</td><td>명시적 면/점</td></tr>
<tr><td><strong>위상 유연성</strong></td><td>높음 (임의의 위상)</td><td>높음 (임의의 위상)</td><td>낮음 (템플릿 종속)</td></tr>
<tr><td><strong>학습 방식</strong></td><td>Encoder-Decoder</td><td>Auto-Decoder (주로)</td><td>다양함</td></tr>
<tr><td><strong>로봇 응용</strong></td><td>점유 지도, 대략적 파지</td><td>물리 시뮬레이션, 정밀 충돌</td><td>시각화, 단순 인식</td></tr>
</tbody></table>
<h2>5.  로봇 제어를 위한 기하학: 미분 가능한 물리와 상호작용</h2>
<p>로봇 공학 관점에서 암시적 표현, 특히 SDF가 가지는 가장 강력한 이점은 바로 **미분 가능성(Differentiability)**과 **물리적 의미(Physical Meaning)**에 있다. 이는 로봇의 인지(Perception)와 제어(Control) 사이의 장벽을 허무는 가교 역할을 한다.</p>
<h3>5.1 미분 가능한 충돌 비용과 궤적 최적화</h3>
<p>로봇 팔이나 모바일 로봇의 경로 계획(Path Planning)에서 충돌 회피는 가장 중요한 제약 조건이다. 기존의 충돌 감지 알고리즘(GJK 등)은 “충돌했다/하지 않았다“라는 이산적인 결과만을 제공하거나, 충돌 깊이를 계산하는 데 많은 비용이 소요되었다. 이는 경사 하강법 기반의 궤적 최적화(Trajectory Optimization)를 어렵게 만든다.</p>
<p>반면, 신경망으로 학습된 SDF <span class="math math-inline">f_\theta(x)</span>는 공간상의 모든 점 <span class="math math-inline">x</span>에 대해 표면까지의 거리뿐만 아니라, 표면 법선 벡터(Surface Normal)에 해당하는 기울기(Gradient) <span class="math math-inline">\nabla_x f_\theta(x)</span>를 해석적으로 제공한다.12<br />
<span class="math math-display">
n(x) = \frac{\nabla_x f_\theta(x)}{\| \nabla_x f_\theta(x) \|}
</span><br />
이 기울기 벡터는 장애물로부터 가장 빠르게 멀어지는 방향을 지시한다. 따라서 CHOMP, STOMP와 같은 최적화 기반 모션 플래너나 모델 예측 제어(MPC) 프레임워크에 SDF를 통합하면, 충돌 비용 함수(Cost Function)를 미분 가능하게 정의할 수 있다.20<br />
<span class="math math-display">
\mathcal{J}_{\text{collision}}(\xi) = \int_{0}^{T} \max(0, \epsilon - f_\theta(\xi(t))) \| \nabla_\xi f_\theta(\xi(t)) \| dt
</span><br />
여기서 <span class="math math-inline">\xi(t)</span>는 로봇의 궤적이다. 이 비용 함수는 로봇이 장애물 안전거리 <span class="math math-inline">\epsilon</span> 이내로 진입했을 때, SDF의 기울기를 따라 궤적을 밀어내는 힘(Force)을 발생시킨다. 최신 연구인 <strong>SDF-SC</strong> 프레임워크는 로봇 팔의 각 링크를 복합 SDF(Composite SDF)로 모델링하여, 자기 충돌(Self-collision) 및 환경과의 충돌을 밀리초(ms) 단위로 감지하고 회피하는 실시간 성능을 입증했다.22 이는 복잡한 다관절 로봇이 좁은 공간에서 인간과 협업하거나 빠르게 움직일 때 안전을 보장하는 핵심 기술이 된다.</p>
<h3>5.2 물리 시뮬레이션의 안정화: SDF-Sim</h3>
<p>가상 환경에서의 물리 시뮬레이션은 로봇 학습(Sim-to-Real)의 필수 요소이다. 그러나 메쉬 기반의 물리 엔진은 물체 간의 접촉(Contact)을 처리할 때 수치적 불안정성이나 ’터널링 효과(Tunneling Effect, 빠른 물체가 얇은 벽을 뚫고 지나가는 현상)’를 겪기 쉽다.</p>
<p>DeepMind의 <strong>SDF-Sim</strong> 연구는 강체(Rigid Body) 시뮬레이션의 기본 표현을 메쉬에서 SDF로 대체함으로써 이러한 문제를 해결했다.23 SDF 기반 접촉 처리는 두 물체가 겹쳤을 때 침투 깊이(Penetration Depth)와 접촉 법선(Contact Normal)을 SDF 값과 그 기울기로부터 즉시 산출할 수 있게 한다.<br />
<span class="math math-display">
\text{Penetration}(x) = \max(0, -f_\theta(x))
</span><br />
이러한 방식은 수백 개의 객체가 쏟아지거나 충돌하는 대규모 장면에서도 메쉬 기반 방식보다 훨씬 안정적이고 빠른 시뮬레이션을 가능하게 한다.23 또한, 표면의 마찰력이나 반발력을 계산할 때 필요한 기하학적 정보가 연속 함수로부터 직접 유도되므로, 시뮬레이션의 물리적 정합성(Fidelity)이 크게 향상된다.</p>
<h2>6.  파지(Grasping)와 조작을 위한 형상 완성</h2>
<p>로봇이 물체를 잡거나 조작하기 위해서는 보이는 부분뿐만 아니라, 보이지 않는 뒷면의 형상까지 추론하는 능력이 필요하다. 이를 **형상 완성(Shape Completion)**이라 하며, Occupancy Networks와 DeepSDF는 이 분야에서 독보적인 성능을 발휘한다.</p>
<h3>6.1 가려진 영역의 상상과 파지 계획</h3>
<p>센서 데이터(예: 단일 시점의 Depth 이미지)는 필연적으로 가려짐(Occlusion)을 포함한다. 로봇이 컵의 손잡이만 보고 전체 컵의 무게 중심을 추정하거나, 책장에 꽂힌 책의 두께를 가늠해야 할 때, 암시적 표현 모델은 학습된 형상의 사전 지식(Prior)을 바탕으로 누락된 정보를 채워 넣는다.1</p>
<p>최근의 <strong>GraspNet</strong> 변형 연구들은 로봇이 관측한 불완전한 포인트 클라우드를 Occupancy Network에 입력하여, 전체 물체의 점유 지도를 복원하고 이를 바탕으로 6-DoF 파지 포즈(Grasp Pose)를 생성한다.24 특히 물건들이 어지럽게 쌓여 있는 클러터(Clutter) 환경에서, 이 방식은 물체 간의 경계를 명확히 하고 그리퍼(Gripper)가 진입할 공간을 정확히 찾아냄으로써 파지 성공률을 획기적으로 높였다.26 또한, 로봇의 배치 계획(Placement Planning) 연구에서는 이동 로봇이 물체에 접근하기 위한 최적의 위치를 산출할 때, 환경 전체의 점유 맵을 활용하여 충돌 없는 접근 경로와 조작 공간을 동시에 확보하는 전략을 사용한다.27</p>
<h3>6.2 투명 물체와 난반사 재질의 극복: Dex-NeRF와 BundleSDF</h3>
<p>전통적인 심도 카메라는 유리나 투명 플라스틱과 같은 투명 물체(Transparent Objects)를 감지하지 못하거나 심한 노이즈를 발생시킨다. 이는 로봇 조작의 오랜 난제였다. 그러나 NeRF(5.2장 참조)와 SDF가 결합된 최신 연구들은 RGB 이미지만으로 투명 물체의 형상을 복원해내고 있다.</p>
<p><strong>Dex-NeRF</strong>와 <strong>Evo-NeRF</strong>는 투명 물체의 형상을 기하학적 밀도 필드나 SDF로 학습하여, 깊이 센서 없이도 물체의 정확한 표면을 추정하고 성공적인 파지를 수행함을 보여주었다.28 NVIDIA의 <strong>BundleSDF</strong>는 여기서 한 걸음 더 나아가, 움직이는 물체의 6-DoF 포즈를 추적(Tracking)함과 동시에 물체의 형상을 SDF로 실시간 재구성하는 시스템을 제안했다.30 이는 로봇이 미지의 물체를 손에 들고 돌려보면서 실시간으로 그 형상을 학습하고, 정교한 조작을 수행할 수 있는 ’능동적 지각(Active Perception)’의 가능성을 열었다.</p>
<h2>7.  실시간 매핑과 SLAM: iSDF와 HIO-SDF</h2>
<p>로봇이 미지의 환경을 탐험하며 지도를 작성하는 SLAM(Simultaneous Localization and Mapping) 분야에서도 암시적 표현은 기존의 복셀 기반 TSDF(Truncated Signed Distance Field) 방식을 빠르게 대체하고 있다.</p>
<h3>7.1 연속적 지도 작성 (Continuous Mapping)</h3>
<p>기존의 Voxblox나 KinectFusion과 같은 시스템은 공간을 고정된 크기의 복셀로 나누어 관리했다. 이는 메모리 비효율적일 뿐만 아니라, 지도의 해상도가 미리 결정되어야 한다는 제약이 있었다.31 반면, **iSDF (Incremental SDF)**와 같은 시스템은 로봇이 수집하는 RGB-D 스트림을 실시간으로 신경망에 학습시켜(Online Training), 맵 전체를 하나의 연속 함수로 유지한다.32</p>
<p>이 접근법은 다음과 같은 혁신적인 이점을 제공한다:</p>
<ol>
<li><strong>압축률:</strong> 거대한 환경의 기하학적 정보를 상대적으로 작은 크기의 신경망 가중치(Weights)로 압축하여 저장한다.</li>
<li><strong>다해상도 쿼리:</strong> 로봇의 주행을 위한 거친 경로 계획에는 저해상도로, 정밀한 조작이 필요한 영역에는 고해상도로 지도를 질의할 수 있다.</li>
<li><strong>예측적 채움(Predictive Hole-filling):</strong> 관측되지 않은 영역에 대해서도 신경망이 학습한 기하학적 연속성을 바탕으로 합리적인 거리 값을 예측하여, 경로 계획의 안정성을 높인다.</li>
</ol>
<h3>7.2 계층적 하이브리드 접근: HIO-SDF</h3>
<p>순수 신경망 기반 SLAM의 가장 큰 약점은 ’망각(Catastrophic Forgetting)’이다. 로봇이 새로운 방을 학습하는 동안 이전 방의 정보를 잊어버리는 현상이다. 이를 해결하기 위해 <strong>HIO-SDF</strong>와 같은 하이브리드 방식이 제안되었다.32</p>
<p>HIO-SDF는 거친(Coarse) 복셀 그리드와 미세한(Fine) 로컬 신경망을 계층적으로 결합한다. 전체적인 환경의 구조는 복셀이 관리하여 망각을 방지하고, 각 복셀 내부의 정밀한 표면 정보는 신경망이 학습하는 방식이다. 실험 결과, 이 방식은 기존의 연속적 표현 대비 46% 낮은 오차율을 기록하며 대규모 환경에서의 실용성을 입증했다.32 또한 <strong>NUE</strong>와 같은 시스템은 NeRF의 불확실성(Uncertainty) 추정을 활용하여 로봇이 정보가 부족한 영역을 능동적으로 탐색(Active Exploration)하도록 유도함으로써, 미지의 환경을 더 빠르고 완전하게 매핑하는 전략을 보여주었다.34</p>
<h2>8.  결론: 암시적 기하학, 로봇에게 촉각을 부여하다</h2>
<p>DeepSDF와 Occupancy Networks로 대변되는 암시적 신경 표현은 로봇이 세상을 바라보는 해상도의 한계를 철폐했다. 3D 데이터를 ’불연속적인 점들의 나열’에서 ’연속적인 함수적 이해’로 승격시킨 이 변화는 단순한 시각적 정밀함의 향상을 넘어선다.</p>
<p>이제 로봇은 세상을 미분 가능한 물리 공간으로 인식한다. 충돌 위험을 기울기(Gradient)로 감지하고, 보이지 않는 물체의 뒷면을 확률적으로 추론하며, 거대한 공간을 신경망이라는 뇌 속에 압축하여 기억한다. 이는 로봇에게 ’보는 눈’을 넘어, 대상의 형상을 수학적으로 만지고 느끼는 **‘기하학적 촉각’**을 부여한 것과 같다.</p>
<p>물론 실시간 학습 속도와 추론 비용은 여전히 극복해야 할 과제이다. 그러나 NVIDIA의 <strong>FoundationPose</strong>와 같이 대규모 합성 데이터와 결합된 파운데이션 모델(Foundation Model)들이 등장하면서 30, 범용적인 객체 이해 능력은 급속도로 발전하고 있다. 이러한 기하학적 정밀함은 이어지는 5.4장에서 다룰 <strong>3D Gaussian Splatting</strong>과 같은 차세대 렌더링 기술과 결합하여, 로봇이 현실과 가상을 구분할 수 없을 만큼 정교한 ‘디지털 트윈’ 안에서 학습하고 진화하는 미래를 앞당기고 있다. 암시적 기하학은 Embodied AI가 물리적 세계의 복잡성을 정복하기 위해 반드시 거쳐야 할 필연적인 진화의 단계이다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>[1901.05103] DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation - arXiv, https://arxiv.org/abs/1901.05103</li>
<li>Occupancy Networks: Learning 3D Reconstruction in Function Space - Andreas Geiger, https://www.cvlibs.net/publications/Mescheder2019CVPR_slides.pdf</li>
<li>Neural Implicit Representations for 3D Shapes and Scenes | Kaduri’s blog, https://omrikaduri.github.io/2022/06/18/Using-Neural-Implicit-Representations-for-Shape-and-Scenes.html</li>
<li>Implicit Representation Methods - Emergent Mind, https://www.emergentmind.com/topics/implicit-representation-methods</li>
<li>Occupancy Networks: Learning 3D Reconstruction in Function Space - YouTube, https://www.youtube.com/watch?v=w1Qo3bOiPaE</li>
<li>Occupancy Networks: Learning 3D Reconstruction in Function Space - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2019/papers/Mescheder_Occupancy_Networks_Learning_3D_Reconstruction_in_Function_Space_CVPR_2019_paper.pdf</li>
<li>[1812.03828] Occupancy Networks: Learning 3D Reconstruction in Function Space - arXiv, https://arxiv.org/abs/1812.03828</li>
<li>Unlocking the Future of 3D Shape Representation: Dive into Occupancy Networks and Implicit Functions | Medium, https://medium.com/@deepyachowdary/demystifying-occupancy-networks-3923363b2b51</li>
<li>Occupancy Network, https://rlschuller.github.io/onet_site/</li>
<li>[Quick Review] Occupancy Networks: Learning 3D Reconstruction in Function Space - Liner, https://liner.com/review/occupancy-networks-learning-3d-reconstruction-in-function-space</li>
<li>DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2019/papers/Park_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation_CVPR_2019_paper.pdf</li>
<li>Local Optimization for Robust Signed Distance Field Collision - Miles Macklin, https://mmacklin.com/sdfcontact.pdf</li>
<li>Paper Summary: DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation | by Karan Uppal, https://karan3-zoh.medium.com/paper-summary-deepsdf-learning-continuous-signed-distance-functions-for-shape-representation-147af4740485</li>
<li>[Quick Review] DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation - Liner, https://liner.com/review/deepsdf-learning-continuous-signed-distance-functions-for-shape-representation</li>
<li>DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation, https://graphics.iiitd.edu.in/wp-content/uploads/2021/07/DeepSDF-Learning-Continuous-Signed-Distance-Functions-for-Shape-Representation.pdf</li>
<li>DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation, https://www.researchgate.net/publication/330439562_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation</li>
<li>Curriculum DeepSDF - Yueqi Duan, <a href="https://duanyueqi.github.io/ECCV20_Curriculum%20DeepSDF.pdf">https://duanyueqi.github.io/ECCV20_Curriculum%20DeepSDF.pdf</a></li>
<li>[1901.05103] DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation - ar5iv, https://ar5iv.labs.arxiv.org/html/1901.05103</li>
<li>MetaSDF: Meta-learning Signed Distance Functions - NIPS papers, https://papers.neurips.cc/paper_files/paper/2020/file/731c83db8d2ff01bdc000083fd3c3740-Paper.pdf</li>
<li>Differentiable Composite Neural Signed Distance Fields for Robot Navigation in Dynamic Indoor Environments - arXiv, https://arxiv.org/html/2502.02664v1</li>
<li>Collision-Free Motion Generation Based on Stochastic Optimization and Composite Signed Distance Field Networks of Articulated Robot - IEEE Xplore, http://ieeexplore.ieee.org/document/10238810/</li>
<li>Efficient Collision Detection Framework for Enhancing Collision-Free Robot Motion - arXiv, https://arxiv.org/html/2409.14955v1</li>
<li>Learning rigid-body simulators over implicit shapes for large-scale scenes and vision - NeurIPS, https://proceedings.neurips.cc/paper_files/paper/2024/file/e3abc125ecacb71786cefb9f67b08c5d-Paper-Conference.pdf</li>
<li>Multi-Group Tri-plane Based Local Occupancy Estimation for Object Grasping | OpenReview, https://openreview.net/forum?id=fOCjpnU2ux</li>
<li>Local Occupancy-Enhanced Object Grasping with Multiple Triplanar Projection - arXiv, https://arxiv.org/html/2407.15771v1</li>
<li>Learning Any-View 6DoF Robotic Grasping in Cluttered Scenes via Neural Surface Rendering, https://www.roboticsproceedings.org/rss20/p046.pdf</li>
<li>Planning Robot Placement for Object Grasping This work was supported by the Bonn-Aachen International Center for Information Technology (b-it). - arXiv, https://arxiv.org/html/2405.16692v1</li>
<li>Neural Fields in Robotics: A Survey - arXiv, https://arxiv.org/html/2410.20220v1</li>
<li>Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects - UC Berkeley EECS, https://www2.eecs.berkeley.edu/Pubs/TechRpts/2022/EECS-2022-266.pdf</li>
<li>R²D²: Building AI-based 3D Robot Perception and Mapping with NVIDIA Research, https://developer.nvidia.com/blog/r2d2-building-ai-based-3d-robot-perception-and-mapping-with-nvidia-research/</li>
<li>Truncated Signed Distance Fields Applied To Robotics - Diva-Portal.org, https://www.diva-portal.org/smash/get/diva2:1136113/FULLTEXT01.pdf</li>
<li>HIO-SDF: Hierarchical Incremental Online Signed Distance Fields - IEEE Xplore, https://ieeexplore.ieee.org/document/10610367/</li>
<li>HIO-SDF: Hierarchical Incremental Online Signed Distance Fields - arXiv, https://arxiv.org/html/2310.09463v2</li>
<li>Enhancing Exploratory Capability of Visual Navigation Using Uncertainty of Implicit Scene Representation - arXiv, https://arxiv.org/html/2411.03487v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>