<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:5.4 실시간 렌더링의 게임 체인저: 3D Gaussian Splatting (3DGS)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>5.4 실시간 렌더링의 게임 체인저: 3D Gaussian Splatting (3DGS)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 5. 뉴럴 3D 표현과 렌더링 (Neural 3D Representations)</a> / <a href="index.html">5.4 실시간 렌더링의 게임 체인저: 3D Gaussian Splatting (3DGS)</a> / <span>5.4 실시간 렌더링의 게임 체인저: 3D Gaussian Splatting (3DGS)</span></nav>
                </div>
            </header>
            <article>
                <h1>5.4 실시간 렌더링의 게임 체인저: 3D Gaussian Splatting (3DGS)</h1>
<h2>1.  서론: 명시적 표현과 미분 가능 렌더링의 융합</h2>
<p>지난 수십 년간 컴퓨터 그래픽스와 비전 분야는 3차원 장면을 어떻게 하면 더 효율적이고 사실적으로 복원하고 렌더링할 것인가라는 근본적인 질문에 매달려 왔다. 전통적인 **Structure from Motion (SfM)**과 <strong>Multi-View Stereo (MVS)</strong> 기술은 2차원 이미지 집합으로부터 3차원 포인트 클라우드나 메쉬(Mesh)를 생성하는 데 성공했지만, 텍스처가 없는 영역이나 복잡한 조명 환경에서는 여전히 한계를 보였다. 이러한 상황에서 2020년 등장한 **Neural Radiance Fields (NeRF)**는 장면을 연속적인 볼륨 함수(Volumetric Function)로 정의하고, 다층 퍼셉트론(MLP)이라는 신경망 속에 색상과 밀도 정보를 암시적(Implicit)으로 인코딩함으로써 사진과 구별할 수 없는 수준의 신규 뷰 합성(Novel View Synthesis, NVS)을 가능하게 했다. NeRF는 3D 복원 분야의 패러다임을 완전히 뒤바꾸어 놓았으나, 실용적인 관점에서 치명적인 단점을 안고 있었다. 바로 <strong>속도</strong>였다.</p>
<p>NeRF의 볼륨 렌더링 방식인 광선 투사(Ray Marching)는 하나의 픽셀 색상을 결정하기 위해 광선(Ray)을 쏘고, 그 광선 위에서 수백 번의 샘플링을 수행하여 신경망을 통과시켜야 한다. 고해상도 이미지를 생성하기 위해서는 수십억 번의 신경망 연산이 필요하며, 이는 실시간 렌더링을 불가능하게 만드는 주요 병목으로 작용했다. 수많은 후속 연구들이 해시 그리드(Hash Grid)나 베이킹(Baking) 기술을 통해 속도를 개선하려 시도했지만, 근본적인 광선 투사 방식의 계산 비용을 완전히 제거할 수는 없었다.</p>
<p>2023년 SIGGRAPH에서 Bernhard Kerbl 등이 발표한 **3D Gaussian Splatting (3DGS)**은 이러한 딜레마를 해결하기 위해 신경망 기반의 암시적 표현을 버리고, 다시 **명시적(Explicit)인 비정형 프리미티브(Unstructured Primitives)**로 회귀하는 대담한 접근을 취했다. 3DGS는 장면을 수백만 개의 <strong>3D 가우시안(3D Gaussians)</strong> 입자들로 표현한다. 이 가우시안들은 단순한 점이 아니라, 위치, 크기, 회전, 불투명도, 그리고 보는 방향에 따른 색상 정보를 가진 3차원 타원체들이다.</p>
<p>이 기술이 ’게임 체인저’라 불리는 이유는 단순히 표현 방식의 변화 때문만이 아니다. 3DGS는 전통적인 그래픽스 파이프라인의 <strong>래스터화(Rasterization)</strong> 기술을 미분 가능하게(Differentiable) 재설계함으로써, 딥러닝의 강력한 최적화 능력(Gradient Descent)과 GPU 하드웨어 래스터라이저의 압도적인 렌더링 속도를 결합했기 때문이다. 그 결과, 3DGS는 NeRF와 대등하거나 더 뛰어난 1080p 고해상도 이미지를 100 FPS 이상의 속도로 렌더링할 수 있게 되었으며, 학습 시간 또한 수 시간에서 수 분 단위로 단축시켰다. 본 장에서는 3DGS의 수학적 기초부터 고속 렌더링 엔진의 작동 원리, 그리고 로보틱스 SLAM 및 압축 기술로의 확장까지 이 혁신적인 기술의 모든 측면을 심도 있게 분석한다.</p>
<hr />
<h2>2.  3D 가우시안의 수학적 정식화 (Mathematical Formulation)</h2>
<p>3DGS의 핵심은 복잡한 3차원 장면을 수많은 가우시안 함수들의 집합으로 근사하는 것이다. 이는 마치 점묘화(Pointillism) 기법과 유사하지만, 각 점이 고정된 크기의 원이 아니라 방향성과 크기가 자유로운 부드러운 타원체라는 점에서 차이가 있다. 이 섹션에서는 3D 가우시안이 어떻게 정의되고, 2D 화면으로 투영되며, 색상을 표현하는지 수학적으로 살펴본다.</p>
<h3>2.1  3D 가우시안의 정의 및 공분산 행렬 분해</h3>
<p>3차원 공간상의 위치 <span class="math math-inline">x \in \mathbb{R}^3</span>에서, 하나의 3D 가우시안 <span class="math math-inline">G(x)</span>는 중심점(Mean) <span class="math math-inline">\mu</span>와 공분산 행렬(Covariance Matrix) <span class="math math-inline">\Sigma</span>에 의해 다음과 같이 정의되는 확률 밀도 함수 형태를 띤다.<br />
<span class="math math-display">
G(x) = e^{-\frac{1}{2}(x - \mu)^T \Sigma^{-1} (x - \mu)}
</span><br />
여기서 <span class="math math-inline">\mu</span>는 가우시안 입자의 월드 좌표계 위치를 나타내며, <span class="math math-inline">\Sigma</span>는 입자의 퍼짐 정도와 방향성을 결정하는 <span class="math math-inline">3 \times 3</span> 대칭 행렬이다. 이 가우시안은 중심에서 가장 진하고, 중심에서 멀어질수록 부드럽게 사라지는(Fall-off) 형태를 가진다.</p>
<h4>2.1.1 공분산 행렬의 물리적 유효성 확보</h4>
<p>최적화(학습) 과정에서 가장 까다로운 부분은 공분산 행렬 <span class="math math-inline">\Sigma</span>를 학습하는 것이다. 공분산 행렬이 물리적으로 유효한 타원체를 나타내기 위해서는 반드시 <strong>양의 준정부호(Positive Semi-Definite, PSD)</strong> 성질을 만족해야 한다. 만약 경사 하강법(Gradient Descent)을 통해 <span class="math math-inline">\Sigma</span>의 원소들을 직접 업데이트한다면, 업데이트 과정에서 이 PSD 조건이 깨져 수학적으로 유효하지 않은 행렬이 생성될 위험이 매우 높다.</p>
<p>이를 해결하기 위해 3DGS 저자들은 공분산 행렬 <span class="math math-inline">\Sigma</span>를 직접 최적화하는 대신, 이를 기하학적으로 더 직관적인 **스케일링(Scaling)**과 <strong>회전(Rotation)</strong> 성분으로 분해하여 파라미터화하는 전략을 택했다.<br />
<span class="math math-display">
\Sigma = R S S^T R^T
</span><br />
이 식에서 각 성분은 다음과 같이 정의된다.</p>
<ul>
<li><strong>스케일링 행렬 (<span class="math math-inline">S</span>):</strong> 3D 벡터 <span class="math math-inline">s \in \mathbb{R}^3</span>를 대각 성분으로 갖는 대각 행렬이다. <span class="math math-inline">S = \text{diag}(s)</span>. 여기서 <span class="math math-inline">s</span>는 타원체의 주축(Major axes) 길이를 나타낸다. 음수 스케일을 방지하기 위해 학습 시에는 활성화 함수(예: Exponential)를 거쳐 처리된다.</li>
<li><strong>회전 행렬 (<span class="math math-inline">R</span>):</strong> 3차원 회전을 나타내는 <span class="math math-inline">3 \times 3</span> 행렬이다. 최적화 효율성을 위해 내부적으로는 4차원 <strong>쿼터니언(Quaternion, <span class="math math-inline">q \in \mathbb{R}^4</span>)</strong> 형태로 저장 및 학습되며, 렌더링 시점에 회전 행렬로 변환된다. 쿼터니언은 짐벌 락(Gimbal Lock) 문제가 없고 정규화가 용이하여 3D 회전 최적화에 적합하다.</li>
</ul>
<p>이러한 <span class="math math-inline">RSS^TR^T</span> 분해 방식을 사용하면, 스케일 <span class="math math-inline">s</span>와 쿼터니언 <span class="math math-inline">q</span>를 아무리 자유롭게 업데이트하더라도 결과적으로 구성되는 <span class="math math-inline">\Sigma</span>는 항상 양의 준정부호임이 보장된다. 이는 최적화의 안정성을 확보하는 결정적인 수학적 테크닉이다.</p>
<h3>2.2  EWA Splatting과 2D 투영 (Projection to 2D)</h3>
<p>우리가 보는 화면은 2차원이다. 따라서 3차원 공간에 존재하는 타원체(3D Gaussian)를 카메라가 보고 있는 2차원 평면으로 투영(Projection)해야 한다. 3DGS는 <strong>EWA(Elliptical Weighted Average) Splatting</strong> 알고리즘에 기반하여, 3D 가우시안이 2D 화면에 투영되었을 때 여전히 2D 가우시안의 형태를 유지한다는 성질을 이용한다.</p>
<p>월드 좌표계의 가우시안을 카메라 좌표계로 변환하고, 이를 다시 이미지 평면으로 투영하는 과정은 비선형적이다. 하지만 가우시안의 중심점 근처에서는 이를 선형 변환으로 근사(Local Affine Approximation)할 수 있다. 이때 2D 화면상에서의 공분산 행렬 <span class="math math-inline">\Sigma&#39;</span>은 다음과 같이 계산된다.<br />
<span class="math math-display">
\Sigma&#39; = J W \Sigma W^T J^T
</span><br />
이 수식의 구성 요소는 다음과 같다.</p>
<ul>
<li><strong><span class="math math-inline">W</span> (Viewing Transformation):</strong> 월드 좌표계를 카메라 좌표계로 변환하는 <span class="math math-inline">3 \times 3</span> 회전/이동 행렬이다.</li>
<li><strong><span class="math math-inline">J</span> (Jacobian of Projective Transformation):</strong> 투영 변환의 야코비안 행렬이다. 원근 투영(Perspective Projection)은 거리에 따라 크기가 달라지는 비선형 변환이므로, 가우시안 중심점에서의 미분값인 야코비안을 사용하여 이를 선형화한다.</li>
</ul>
<p>이 <span class="math math-inline">\Sigma&#39;</span> 행렬은 2D 화면상에 그려질 타원의 모양과 방향을 결정한다. 만약 <span class="math math-inline">\Sigma&#39;</span>의 행렬식(Determinant)이 너무 작거나 특정 고유값이 너무 크다면, 이는 가우시안이 화면상에서 너무 작거나 기형적으로 보인다는 것을 의미하므로, 렌더링 시 앨리어싱(Aliasing)을 방지하기 위해 최소 크기를 제한하는 등의 보정 처리가 수행된다.</p>
<p>중요한 점은 이 모든 변환 과정(<span class="math math-inline">\Sigma \to \Sigma&#39;</span>)이 미분 가능하다는 것이다. 즉, 2D 이미지 상에서 계산된 픽셀 색상 오차를 역전파(Backpropagation)하여, 야코비안 <span class="math math-inline">J</span>와 뷰 변환 <span class="math math-inline">W</span>를 거쳐 원래의 3D 속성인 <span class="math math-inline">s</span> (스케일), <span class="math math-inline">q</span> (회전), <span class="math math-inline">\mu</span> (위치)에 대한 그라디언트를 계산하고 업데이트할 수 있다.</p>
<h3>2.3  뷰 의존적 색상: 구면 조화 함수 (Spherical Harmonics)</h3>
<p>현실 세계의 물체는 보는 각도에 따라 색상이 달라진다. 금속의 반짝임이나 젖은 표면의 난반사 같은 현상이 대표적이다. NeRF는 이를 위해 뷰 방향 벡터(View Direction)를 MLP의 입력으로 넣어 색상을 예측했다. 3DGS는 명시적 표현의 장점을 살려 **구면 조화 함수(Spherical Harmonics, SH)**를 사용하여 이러한 뷰 의존적 색상(View-Dependent Appearance)을 모델링한다.</p>
<p>각 3D 가우시안은 색상 정보를 SH 계수(Coefficients) 형태로 저장한다.</p>
<ul>
<li><strong>0차 SH (DC 성분):</strong> 모든 방향에서 동일하게 보이는 기본 색상(Diffuse Color)을 나타낸다.</li>
<li><strong>고차 SH (AC 성분):</strong> 보는 방향에 따라 변하는 고주파 색상 정보(Specular Highlights, Texture changes)를 나타낸다. 일반적으로 3차(Degree 3) SH까지 사용하며, 이는 총 16개의 기저 함수(Basis functions)로 구성된다.</li>
</ul>
<p>RGB 채널 각각에 대해 16개의 계수가 필요하므로, 하나의 가우시안은 총 <span class="math math-inline">16 \times 3 = 48</span>개의 부동소수점 값을 색상 정보로 가진다. 렌더링 시에는 현재 카메라의 뷰 방향 벡터를 입력으로 하여 이 SH 계수들의 선형 결합을 계산함으로써, 해당 방향에서 보이는 정확한 RGB 색상을 실시간으로 얻어낸다.</p>
<p>SH 사용은 렌더링 품질을 높이는 핵심 요소이지만, 동시에 <strong>메모리 사용량 증가의 주범</strong>이기도 하다. 위치나 스케일 정보에 비해 색상 정보가 차지하는 비중이 매우 크기 때문이다(약 75% 이상). 따라서 후술할 압축 기술(LightGaussian 등)에서는 이 SH 계수의 차수를 낮추거나 양자화하는 것이 주요 최적화 대상이 된다.</p>
<h2>3.  고속 래스터화 파이프라인 (The Rasterization Pipeline)</h2>
<p>3DGS가 기존 NeRF와 차별화되는 가장 큰 특징은 <strong>미분 가능한 타일 기반 래스터화(Differentiable Tile-based Rasterization)</strong> 엔진이다. 이 엔진은 수백만 개의 가우시안을 GPU의 병렬 처리 능력을 극대화하여 순식간에 화면에 그려낸다. 그 과정은 크게 전처리, 정렬, 그리고 렌더링의 3단계로 나뉜다.</p>
<h3>3.1  전처리 및 컬링 (Preprocessing and Culling)</h3>
<p>렌더링 파이프라인의 첫 단계는 전체 가우시안 풀(Pool)에서 현재 카메라 시점에 기여하지 않는 가우시안들을 빠르게 걸러내는 것이다.</p>
<ol>
<li><strong>뷰 프러스텀 컬링 (View Frustum Culling):</strong> 가우시안의 위치 <span class="math math-inline">\mu</span>와 공분산 <span class="math math-inline">\Sigma</span>를 기반으로 99% 신뢰 구간(Confidence Interval)에 해당하는 타원체를 정의한다. 이 타원체가 카메라의 뷰 프러스텀(시야각)과 겹치지 않으면 렌더링 대상에서 즉시 제외한다.</li>
<li><strong>가드 밴드 (Guard Band):</strong> 화면 가장자리에 걸친 가우시안이나, 투영 시 깊이(Depth) 값이 너무 작거나(Near plane) 먼(Far plane) 가우시안들도 기하학적 불안정성을 유발할 수 있으므로 제거한다.</li>
<li><strong>2D 투영:</strong> 살아남은 가우시안들에 대해 앞서 설명한 <span class="math math-inline">JW \Sigma W^T J^T</span> 공식을 적용하여 2D 화면상의 공분산 <span class="math math-inline">\Sigma&#39;</span>, 위치, 깊이 등을 계산한다.</li>
</ol>
<h3>3.2  GPU 기수 정렬 (GPU Radix Sort)</h3>
<p>투영된 가우시안들을 올바른 순서로 합성하기 위해서는 **깊이 정렬(Depth Sorting)**이 필수적이다. 3DGS는 여기서 GPU 하드웨어에 최적화된 **기수 정렬(Radix Sort)**을 사용한다.</p>
<ol>
<li><strong>타일 분할:</strong> 화면을 <span class="math math-inline">16 \times 16</span> 픽셀 크기의 타일(Tile)들로 나눈다.</li>
<li><strong>인스턴스 생성:</strong> 하나의 가우시안이 여러 타일에 걸쳐 있을 수 있다. 각 가우시안이 덮고 있는 타일의 개수만큼 인스턴스를 복제한다.</li>
<li><strong>키(Key) 생성:</strong> 각 인스턴스에 대해 64비트 정렬 키를 생성한다.</li>
</ol>
<ul>
<li><strong>상위 32비트:</strong> 타일 ID (Tile ID). 이를 통해 같은 타일에 속한 가우시안끼리 먼저 묶인다.</li>
<li><strong>하위 32비트:</strong> 뷰 공간 깊이 (View-space Depth). 이를 통해 같은 타일 내에서 깊이 순으로 정렬된다.</li>
</ul>
<ol start="4">
<li><strong>정렬 수행:</strong> NVIDIA CUB 라이브러리 등의 고속 Radix Sort를 사용하여 수백만 개의 키를 한 번에 정렬한다. 이 과정은 픽셀 단위가 아닌 가우시안 단위로 수행되므로 NeRF의 광선 샘플링 정렬보다 훨씬 효율적이다.</li>
</ol>
<h3>3.3  타일 기반 블렌딩 및 스레드 병렬화</h3>
<p>정렬이 완료되면 각 타일별로 가우시안 리스트(List)가 생성된다. 이제 GPU의 각 스레드 블록(Thread Block)이 하나의 타일을 맡아 픽셀 색상을 계산한다.<br />
<span class="math math-display">
C(x) = \sum_{i \in \mathcal{N}} c_i \alpha_i \prod_{j=1}^{i-1} (1 - \alpha_j)
</span></p>
<ul>
<li><span class="math math-inline">c_i</span>: <span class="math math-inline">i</span>번째 가우시안의 색상 (SH로 계산됨).</li>
<li><span class="math math-inline">\alpha_i</span>: <span class="math math-inline">i</span>번째 가우시안의 불투명도. 2D 가우시안의 중심으로부터의 거리와 학습된 투명도 파라미터의 곱으로 결정된다.</li>
<li><span class="math math-inline">T_i = \prod (1-\alpha_j)</span>: 투과율(Transmittance). 현재 가우시안 도달 전까지의 누적 투명도이다.</li>
</ul>
<p>각 스레드는 할당된 픽셀에 대해 타일 리스트의 가우시안을 <strong>앞에서 뒤로(Front-to-back)</strong> 순서대로 순회하며 색상을 누적한다. 여기서 중요한 최적화 기법이 적용된다.</p>
<ul>
<li><strong>조기 종료 (Early Stopping):</strong> 색상을 누적하다가 투과율 <span class="math math-inline">T_i</span>가 거의 0에 가까워지면(즉, 픽셀이 불투명해져서 뒤쪽이 더 이상 보이지 않으면), 해당 픽셀에 대한 연산을 즉시 중단한다. 이는 불필요한 연산을 획기적으로 줄여준다.</li>
<li><strong>공유 메모리 활용:</strong> 타일 내의 가우시안 정보(위치, 색상 등)를 GPU의 빠른 공유 메모리(Shared Memory)에 로드하여, 동일 타일 내의 모든 픽셀 스레드들이 데이터를 재사용하도록 한다.</li>
</ul>
<p>이러한 파이프라인 덕분에 3DGS는 복잡한 장면에서도 100 FPS 이상의 압도적인 렌더링 성능을 발휘하며, 이는 3D 렌더링의 역사에서 소프트웨어 래스터라이저가 하드웨어 가속 없이 달성한 가장 놀라운 성과 중 하나로 평가받는다.</p>
<h2>4.  적응형 밀도 제어 및 최적화 (Adaptive Density Control)</h2>
<p>3DGS가 높은 품질을 달성하는 비결은 고정된 개수의 가우시안을 사용하는 것이 아니라, 학습 과정에서 가우시안의 분포와 개수를 **동적으로 조절(Adaptive Density Control)**한다는 점에 있다. 초기에는 SfM으로 얻은 희소한 포인트 클라우드(Sparse Point Cloud)에서 시작하지만, 학습이 진행되면서 가우시안은 스스로 빈 공간을 채우거나 세밀한 부분을 묘사하기 위해 분열한다.</p>
<h3>4.1  복제(Clone)와 분할(Split)</h3>
<p>주기적으로(예: 100 이터레이션마다) 각 가우시안의 상태를 점검하여 밀도를 조절한다. 판단 기준은 주로 **뷰 공간 위치 그라디언트(View-space Position Gradient)**의 크기다. 그라디언트가 크다는 것은 해당 가우시안이 묘사하는 영역의 기하학적 구조가 아직 충분히 수렴하지 않았거나, 오차가 크다는 것을 의미한다.</p>
<ol>
<li><strong>Under-reconstruction (빈 공간 채우기):</strong></li>
</ol>
<ul>
<li><strong>상황:</strong> 가우시안의 분산(Scale)이 작으면서 위치 그라디언트가 큰 경우. 이는 묘사해야 할 빈 공간은 넓은데 작은 가우시안 하나가 고군분투하고 있는 상황이다.</li>
<li><strong>조치:</strong> 해당 가우시안을 복제(Clone)하여 같은 위치에 하나 더 만든다. 이후 최적화 과정에서 두 가우시안은 서로 다른 위치로 이동하여 빈 공간을 메우게 된다.</li>
</ul>
<ol start="2">
<li><strong>Over-reconstruction (세밀화):</strong></li>
</ol>
<ul>
<li><strong>상황:</strong> 가우시안의 분산이 크면서 위치 그라디언트가 큰 경우. 이는 복잡한 디테일이 있는 넓은 영역을 큰 가우시안 하나가 뭉뚱그려 표현하고 있어 오차가 발생하는 상황이다.</li>
<li><strong>조치:</strong> 큰 가우시안을 두 개의 작은 가우시안으로 분할(Split)한다. 분할된 가우시안들은 스케일이 작아지며, 원래의 큰 가우시안이 차지하던 영역의 세부적인 텍스처와 구조를 더 정밀하게 표현할 수 있게 된다.</li>
</ul>
<h3>4.2  가지치기 (Pruning)와 불투명도 초기화</h3>
<p>무작정 가우시안을 늘리기만 하면 메모리가 폭발하고 렌더링 속도가 저하된다. 따라서 불필요한 가우시안을 제거하는 과정이 병행된다.</p>
<ul>
<li><strong>낮은 불투명도 제거:</strong> 불투명도 <span class="math math-inline">\alpha</span>가 특정 임계값(예: 0.005) 이하로 떨어진 가우시안은 시각적으로 거의 기여하지 않으므로 제거한다.</li>
<li><strong>화면 밖/거대 가우시안 제거:</strong> 뷰 프러스텀을 크게 벗어나거나, 스케일이 비정상적으로 커져서 화면 전체를 덮어버리는(Artifact 유발) 가우시안들도 제거 대상이다.</li>
<li><strong>불투명도 재설정 (Opacity Reset):</strong> 학습 중간에 주기적으로 모든 가우시안의 불투명도를 0에 가까운 값으로 낮춘다. 이는 가우시안들이 불필요하게 겹쳐서 로컬 미니마(Local Minima)에 빠지는 것을 방지하고, 정말 필요한 가우시안만 살아남도록 유도하여 가우시안의 배치를 전역적으로 재조정하는 효과를 준다.</li>
</ul>
<p>이러한 적응형 밀도 제어 덕분에 3DGS는 초기 포인트 클라우드의 품질에 크게 의존하지 않고도, 디테일이 필요한 곳에는 많은 가우시안을, 평탄한 곳에는 적은 가우시안을 배치하는 효율적인 장면 표현을 스스로 학습해낸다.</p>
<h2>5.  정적 장면을 넘어: SLAM 및 로보틱스 응용</h2>
<p>3DGS의 명시적 표현과 빠른 업데이트 속도는 로보틱스, 특히 <strong>SLAM (Simultaneous Localization and Mapping)</strong> 분야에 즉각적인 혁신을 가져왔다. 기존 NeRF 기반 SLAM 방식들은 맵(Map)을 업데이트하기 위해 거대한 신경망 전체를 다시 학습(Fine-tuning)시켜야 했기에 실시간성이 떨어졌다. 반면 3DGS는 맵의 특정 영역에 해당하는 가우시안들만 선택적으로 업데이트하거나 추가할 수 있어 실시간 SLAM에 이상적이다.</p>
<p>여기서는 3DGS를 SLAM에 적용한 대표적인 두 가지 접근법인 <strong>SplaTAM</strong>과 <strong>GS-SLAM</strong>을 상세히 비교 분석한다.</p>
<h3>5.1  SplaTAM: 실루엣 기반의 강건한 추적 및 매핑</h3>
<p>**SplaTAM (Splat, Track &amp; Map)**은 단일 RGB-D 카메라를 사용하여 고밀도 3D 재구성을 수행하는 선구적인 시스템이다. 이 연구의 핵심은 3D 가우시안을 유일한 맵 표현 방식으로 사용하면서, 동시에 카메라 추적(Tracking)과 지도 작성(Mapping)을 분리하여 최적화하는 것이다.</p>
<ul>
<li><strong>실루엣 마스크 (Silhouette Masking):</strong> SplaTAM은 렌더링된 깊이 맵과 센서 입력 깊이 맵의 차이를 이용하여 ’실루엣 마스크’를 생성한다. 이 마스크는 현재 맵에 존재하는 유효한 구조와, 아직 매핑되지 않은 빈 공간을 명확히 구분해준다. 이를 통해 이미 맵핑된 영역을 중복 업데이트하는 것을 방지하고, 새로운 영역에 대해서만 효율적으로 가우시안을 추가(Densification)할 수 있다.</li>
<li><strong>손실 함수 (Loss Function):</strong> 카메라 포즈 추적과 맵 업데이트 모두에 대해 **광도 손실(Photometric Loss)**과 **기하학적 손실(Depth/Geometric Loss)**을 결합하여 사용한다. 단순히 색상만 맞추는 것이 아니라 깊이 정보까지 정렬함으로써 텍스처가 부족한 영역에서도 추적 실패를 방지한다.</li>
<li><strong>성과:</strong> SplaTAM은 기존의 암시적 표현 기반 SLAM인 NICE-SLAM이나 Point-SLAM 대비 카메라 포즈 추정 정확도와 맵 복원 품질에서 최대 2배 이상의 성능 향상을 입증했다. 특히 엣지(Edge)가 뚜렷하고 복잡한 실내 환경에서 그 강점을 발휘한다.</li>
</ul>
<h3>5.2  GS-SLAM: 전체 장면 확장을 위한 적응형 전략</h3>
<p><strong>GS-SLAM</strong>은 3DGS를 이용해 단순히 물체를 복원하는 것을 넘어, 전체 실내 공간과 같은 대규모 장면을 실시간으로 재구성하는 데 초점을 맞춘다.</p>
<ul>
<li><strong>적응형 확장 전략 (Adaptive Expansion Strategy):</strong> GS-SLAM은 새로운 기하학적 구조가 관측될 때 가우시안을 적극적으로 추가하고, 노이즈가 많은 가우시안은 과감히 삭제하는 동적 관리 전략을 사용한다. 이는 맵이 계속 확장되는 SLAM 환경에서 메모리 효율과 맵 품질을 유지하는 데 필수적이다.</li>
<li><strong>Coarse-to-Fine 추적:</strong> 매 프레임마다 모든 가우시안을 사용하여 카메라 포즈를 계산하는 것은 비효율적이다. GS-SLAM은 먼저 저해상도 혹은 일부 대표 가우시안만을 사용하여 대략적인(Coarse) 포즈를 추정한 뒤, 이를 초기값으로 하여 전체 가우시안을 사용하는 정밀(Fine) 최적화를 수행한다. 이 계층적 접근은 추적 속도를 높이면서도 급격한 움직임에 대한 강건성(Robustness)을 확보해준다.</li>
</ul>
<h3>5.3  비교 및 동적 환경의 도전 과제</h3>
<p>두 시스템을 비교해보면, <strong>SplaTAM</strong>은 실루엣 마스크를 통한 정밀한 밀도 제어로 추적 정확도(Tracking Accuracy) 면에서 우위를 보이는 반면, <strong>GS-SLAM</strong>은 빠른 렌더링과 전체 장면 처리에 좀 더 최적화된 경향을 보인다.</p>
<table><thead><tr><th><strong>특성</strong></th><th><strong>SplaTAM</strong></th><th><strong>GS-SLAM</strong></th></tr></thead><tbody>
<tr><td><strong>주요 기여</strong></td><td>실루엣 기반 밀도 제어, 정확한 추적</td><td>적응형 확장, Coarse-to-Fine 추적</td></tr>
<tr><td><strong>입력 데이터</strong></td><td>RGB-D</td><td>RGB-D (일부 Monocular 확장)</td></tr>
<tr><td><strong>장점</strong></td><td>높은 포즈 추정 정확도, 맵 품질 우수</td><td>빠른 처리 속도, 대규모 장면 확장성</td></tr>
<tr><td><strong>단점</strong></td><td>상대적으로 느린 최적화 속도</td><td>텍스처가 적은 곳에서 추적 불안정 가능성</td></tr>
</tbody></table>
<p>동적 환경 (Dynamic Environments)의 한계:</p>
<p>현재 대부분의 3DGS SLAM 시스템은 환경이 정적(Static)이라고 가정한다. 사람이 걸어 다니거나 가구가 옮겨지는 동적 환경에서는 움직이는 물체가 맵에 잔상(Ghosting artifacts)으로 남거나, 이를 정적 구조물로 착각하여 카메라 추적이 실패(Drift)하는 문제가 발생한다.</p>
<p>최근 연구들은 이를 해결하기 위해 MobileSAM과 같은 실시간 세그멘테이션 모델을 통합하여 동적 객체를 마스킹(Masking)하고 추적에서 배제하거나, 동적 객체만을 위한 별도의 가우시안 필드를 생성하여 움직임을 모델링하는 방향으로 발전하고 있다. 이는 로봇이 사람과 공존하는 실제 환경에서 3DGS SLAM을 사용하기 위한 필수적인 과제다.</p>
<h2>6.  압축 및 경량화: 기가바이트에서 메가바이트로</h2>
<p>3DGS의 가장 큰 아킬레스건은 <strong>메모리 사용량</strong>이다. NeRF(예: Instant-NGP)가 수십 MB 수준의 가중치 파일만으로 장면을 표현할 수 있는 반면, 3DGS는 수백만 개의 가우시안에 대한 위치, 회전, 스케일, 그리고 고차원의 SH 계수를 모두 저장해야 하므로 장면당 수백 MB에서 수 GB의 저장 공간과 VRAM을 요구한다. 이는 제한된 자원을 가진 모바일 기기나 엣지 디바이스에 3DGS를 배포하는 데 큰 걸림돌이 된다.</p>
<p>이 문제를 해결하기 위해 다양한 압축 기법들이 제안되었으며, 대표적으로 <strong>LightGaussian</strong>과 <strong>Compact3D</strong>가 있다.</p>
<h3>6.1  LightGaussian: 가지치기와 증류(Distillation)</h3>
<p><strong>LightGaussian</strong>은 불필요한 가우시안을 제거하고 정보를 압축하여 모델 크기를 획기적으로 줄이는 프레임워크다.</p>
<ul>
<li><strong>전역 중요도(Global Significance) 기반 가지치기:</strong> 단순히 불투명도가 낮은 가우시안을 지우는 것을 넘어, 각 가우시안이 렌더링된 이미지 품질에 기여하는 정도(Impact)를 계산한다. 기여도가 낮은 가우시안은 과감히 제거(Pruning)하여 전체 가우시안의 개수를 줄인다.</li>
<li><strong>SH 증류 (SH Distillation):</strong> 데이터 용량의 75% 이상을 차지하는 고차 SH 계수를 효율적으로 압축한다. 복잡한 고차 SH를 저차 SH나 더 적은 파라미터로 근사(Distill)하되, 렌더링 결과의 차이를 최소화하도록 학습시킨다.</li>
<li><strong>벡터 양자화 (Vector Quantization):</strong> 남은 파라미터들에 대해 양자화를 수행한다. 이때 위치(Position)나 0차 SH와 같이 시각적 품질에 치명적인 속성은 정밀도를 유지(FP16)하고, 덜 중요한 속성들은 적극적으로 양자화하여 코드북(Codebook) 형태로 저장한다.</li>
</ul>
<h3>6.2  Compact3D: K-Means 클러스터링과 코드북</h3>
<p><strong>Compact3D</strong>는 데이터 중복성을 제거하는 데 초점을 맞춘다.</p>
<ul>
<li><strong>K-Means 클러스터링:</strong> 수백만 개의 가우시안이 가진 색상과 공분산 행렬을 분석해보면 서로 유사한 값을 가진 경우가 많다. Compact3D는 이를 K-Means 알고리즘으로 클러스터링하여 소수의 대표값(Centroids)만을 남기고, 각 가우시안은 해당 값의 인덱스(Index)만 저장하도록 한다.</li>
<li><strong>인덱스 압축:</strong> 저장된 인덱스들을 정렬하고 런 렝스 인코딩(Run-Length Encoding)과 같은 기법을 추가 적용하여 압축률을 극대화한다.</li>
<li><strong>결과:</strong> 이 방법을 통해 3DGS의 저장 용량을 원본 대비 <strong>40~50배</strong>까지 줄이면서도, 렌더링 속도 저하는 거의 없이 원본과 유사한 화질을 유지할 수 있다.</li>
</ul>
<h3>6.3  NeRF vs 3DGS 메모리 및 성능 비교</h3>
<p>다음 표는 NeRF(Instant-NGP)와 원본 3DGS, 그리고 압축된 3DGS의 성능 특성을 비교한 것이다.</p>
<table><thead><tr><th><strong>특성</strong></th><th><strong>NeRF (Instant-NGP)</strong></th><th><strong>3DGS (Original)</strong></th><th><strong>Compressed 3DGS (LightGaussian/Compact3D)</strong></th></tr></thead><tbody>
<tr><td><strong>저장 용량</strong></td><td>작음 (~수십 MB)</td><td><strong>매우 큼 (수백 MB ~ GB)</strong></td><td><strong>중간 (~수십 MB)</strong></td></tr>
<tr><td><strong>렌더링 속도</strong></td><td>빠름 (단, 해상도 비례 비용)</td><td><strong>매우 빠름 (100+ FPS)</strong></td><td><strong>매우 빠름</strong> (압축 해제 오버헤드 미미)</td></tr>
<tr><td><strong>학습 속도</strong></td><td>매우 빠름 (수 분)</td><td><strong>매우 빠름 (수 분)</strong></td><td>빠름 (압축 과정 추가 소요)</td></tr>
<tr><td><strong>표현 방식</strong></td><td>암시적 (Neural Weights)</td><td>명시적 (Point Attributes)</td><td>명시적 + 양자화(Quantized)</td></tr>
<tr><td><strong>편집 용이성</strong></td><td>어려움</td><td>쉬움 (직접 조작 가능)</td><td>보통 (디코딩 필요)</td></tr>
</tbody></table>
<p>압축 기술의 발전으로 3DGS는 이제 고용량이라는 단점을 극복하고, 웹이나 모바일 환경에서도 스트리밍 가능한 가벼운 3D 포맷으로 진화하고 있다.</p>
<h2>7.  아티팩트(Artifacts)와 품질 관리</h2>
<p>3DGS는 뛰어나지만 완벽하지는 않다. 특유의 최적화 방식과 렌더링 알고리즘으로 인해 발생하는 몇 가지 고유한 시각적 아티팩트들이 존재한다. 이를 이해하고 해결하는 것은 고품질 콘텐츠 제작을 위해 필수적이다.</p>
<ol>
<li><strong>Needle Artifacts (바늘 모양 아티팩트):</strong></li>
</ol>
<ul>
<li><strong>현상:</strong> 특정 뷰에서는 정상적으로 보이지만, 시점을 조금만 돌리면 가우시안들이 길쭉하게 늘어난 바늘이나 창(Spear)처럼 보이는 현상이다.</li>
<li><strong>원인:</strong> 주로 학습 데이터의 카메라 뷰가 제한적이거나 초기 포인트 클라우드가 희소할 때 발생한다. 최적화 과정에서 가우시안이 카메라 광선 방향으로 길게 늘어지면(Ray stretching), 해당 뷰에서는 투영된 모양이 정상적인 원으로 보이지만, 측면에서 보면 길쭉한 타원체가 되기 때문이다.</li>
<li><strong>해결:</strong> <strong>Pixel-GS</strong>나 <strong>eRank-GS</strong> 같은 최신 연구들은 가우시안의 형상 비율(Aspect Ratio)에 제약(Regularization)을 걸거나, 초기화 및 성장 전략을 개선하여 가우시안이 지나치게 이방성(Anisotropic)을 띠지 않도록 억제한다.</li>
</ul>
<ol start="2">
<li><strong>Popping Artifacts (팝핑 현상):</strong></li>
</ol>
<ul>
<li><strong>현상:</strong> 카메라가 움직일 때 가우시안들의 렌더링 순서가 바뀌면서 화면의 일부가 깜빡이거나(Flickering), 갑자기 나타났다 사라지는 현상이다.</li>
<li><strong>원인:</strong> 3DGS는 타일 기반 정렬을 사용하는데, 뷰가 변경됨에 따라 가우시안의 깊이 값(<span class="math math-inline">z</span>)이 미세하게 변하고, 이로 인해 정렬 순서가 급격히 뒤바뀔 수 있다. 특히 서로 겹쳐 있는 반투명한 가우시안들 사이에서 이 문제가 두드러진다.</li>
<li><strong>해결:</strong> <strong>StopThePop</strong>과 같은 연구는 픽셀 단위의 깊이 정렬을 개선하거나, 뷰 변경에 따른 급격한 가시성 변화를 억제하는 스무딩 필터를 적용하여 시간적 일관성(Temporal Consistency)을 확보하려 한다.</li>
</ul>
<ol start="3">
<li><strong>Floaters (부유물):</strong></li>
</ol>
<ul>
<li><strong>현상:</strong> 카메라 바로 앞이나 빈 허공에 뜬금없이 나타나는 작은 가우시안 덩어리들이다.</li>
<li><strong>원인:</strong> SfM 초기화가 잘못되었거나, 텍스처가 없는 하늘/벽면 영역을 억지로 설명하기 위해 가우시안이 잘못 배치된 경우다.</li>
<li><strong>해결:</strong> 깊이 기반 정규화(Depth Regularization)를 통해 센서 깊이 값과 일치하지 않는 가우시안을 억제하거나, 학습 시 배경을 무작위 색상으로 변경하여 배경과 객체를 명확히 분리하도록 유도한다.</li>
</ul>
<h2>8.  결론 및 미래 전망</h2>
<p>3D Gaussian Splatting은 명시적 기하학(Explicit Geometry)의 직관성과 제어 가능성, 그리고 딥러닝의 최적화 능력을 완벽하게 결합하여 실시간 렌더링의 새로운 표준을 제시했다. NeRF가 “존재 가능한 모든 뷰를 신경망 속에 압축“하려는 시도였다면, 3DGS는 “3D 공간에 지능적인 물감을 뿌려(Splatting)” 장면을 재구성하는 방식이다.</p>
<p>이러한 패러다임의 변화는 단순한 속도 향상을 넘어선다. 명시적 표현 덕분에 우리는 이제 학습된 3D 장면을 직접 수정하고, 편집하고, 물리 엔진과 상호작용하게 만들 수 있다. 로보틱스 분야에서는 이미 3DGS가 실시간 SLAM의 핵심 기술로 자리 잡아가고 있으며, VR/AR 분야에서는 고품질의 실사 3D 콘텐츠를 모바일 기기에서 실시간으로 보여줄 수 있는 유일한 대안으로 주목받고 있다.</p>
<p>앞으로의 3DGS는 4D 동적 장면(Dynamic Scene) 처리 능력의 고도화, 생성형 AI(Generative AI)와의 결합을 통한 Text-to-3D 생성, 그리고 극한의 압축 효율을 달성하는 방향으로 발전할 것이다. 독자들은 본 장을 통해 단순히 기술의 원리를 이해하는 것을 넘어, 이 기술이 열어갈 ’실감형 디지털 트윈’과 ’메타버스’의 미래를 가늠해 볼 수 있을 것이다. 3DGS는 단순한 렌더링 알고리즘이 아니라, 현실 세계를 디지털 공간으로 옮겨오는 가장 효율적이고 강력한 운송 수단이기 때문이다.</p>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>