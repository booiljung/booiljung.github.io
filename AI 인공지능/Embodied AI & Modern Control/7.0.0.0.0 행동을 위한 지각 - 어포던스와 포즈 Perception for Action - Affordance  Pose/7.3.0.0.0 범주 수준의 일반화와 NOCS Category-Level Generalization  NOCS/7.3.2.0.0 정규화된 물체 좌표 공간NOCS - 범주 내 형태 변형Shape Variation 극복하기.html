<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.3.2 정규화된 물체 좌표 공간(NOCS): 범주 내 형태 변형(Shape Variation) 극복하기</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.3.2 정규화된 물체 좌표 공간(NOCS): 범주 내 형태 변형(Shape Variation) 극복하기</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <a href="index.html">7.3 범주 수준의 일반화와 NOCS (Category-Level Generalization & NOCS)</a> / <span>7.3.2 정규화된 물체 좌표 공간(NOCS): 범주 내 형태 변형(Shape Variation) 극복하기</span></nav>
                </div>
            </header>
            <article>
                <h1>7.3.2 정규화된 물체 좌표 공간(NOCS): 범주 내 형태 변형(Shape Variation) 극복하기</h1>
<h3>0.1 서론: 구체적인 사물에서 추상적인 범주로의 도약</h3>
<p>인공지능이 탑재된 로봇, 즉 엠바디드 AI(Embodied AI)가 통제된 실험실 환경을 벗어나 비정형화된 현실 세계(Unstructured Real World)에서 유의미한 작업을 수행하기 위해 가장 시급하게 해결해야 할 난제는 물체의 ’다양성’을 인지하고 다루는 능력이다. 전통적인 인스턴스 수준(Instance-Level)의 6D 포즈 추정 기술은 사전에 완벽한 3D CAD 모델이 확보된 특정 물체—가령 공장의 특정 부품이나 실험실의 표준화된 상자—에 대해서만 작동하도록 설계되었다. 이러한 접근 방식은 로봇의 작업 범위를 극도로 제한한다. 인간의 생활 공간에 존재하는 물체들은 같은 ’머그컵’이라는 범주(Category)에 속하더라도 그 형태, 크기, 비율, 텍스처, 그리고 위상적 구조(Topological Structure)가 무한에 가까운 변형(Variation)을 가지기 때문이다.</p>
<p>로봇이 “식탁 위의 머그컵을 집어라“라는 명령을 수행할 때, 그 머그컵은 로봇이 학습 데이터에서 한 번도 본 적 없는(Unseen) 새로운 디자인일 가능성이 높다. 손잡이가 길 수도 있고, 몸통이 뚱뚱할 수도 있으며, 높이가 매우 낮을 수도 있다. 이러한 **범주 내 형태 변형(Intra-Category Shape Variation)**을 극복하고, 처음 보는 물체에 대해서도 파지(Grasping)와 조작(Manipulation)이 가능한 6차원 포즈(3D 위치 및 3D 회전)와 물리적 크기(Scale)를 추론해내는 것은 로봇 지각(Perception) 시스템의 성배와도 같다.</p>
<p>2019년 CVPR에서 발표된 기념비적인 연구인 <strong>“Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation”</strong> 은 이 문제에 대해 **정규화된 물체 좌표 공간(Normalized Object Coordinate Space, 이하 NOCS)**이라는 혁신적인 기하학적 해법을 제시하였다. 본 절에서는 NOCS가 어떻게 기하학적으로 상이한 물체들을 하나의 통일된 좌표계로 투영하여 형태 변형 문제를 극복하는지, 그 수학적 원리와 신경망 아키텍처, 그리고 최적화 기법을 심층적으로 분석한다. 또한, NOCS가 갖는 본질적인 한계와 이를 극복하기 위한 후속 연구들의 이론적 토대까지 포괄적으로 논의한다.</p>
<h2>1.  정규화된 물체 좌표 공간(NOCS)의 기하학적 정의와 위상학적 의미</h2>
<h3>1.1  표준 공간(Canonical Space)의 필요성과 정의</h3>
<p>전통적인 포즈 추정에서 물체의 6D 포즈는 카메라 좌표계(Camera Coordinate System) <span class="math math-inline">C</span>에 대한 물체 좌표계(Object Coordinate System) <span class="math math-inline">O</span>의 변환 행렬 <span class="math math-inline">T \in SE(3)</span>로 정의된다. 특정 인스턴스(예: 특정 브랜드의 전동 드릴)의 경우, CAD 모델의 원점과 축이 고정되어 있으므로 <span class="math math-inline">O</span>는 유일하게 정의된다. 그러나 범주 수준에서는 문제가 복잡해진다. 서로 다른 모양을 가진 두 개의 머그컵에 대해, 어디를 원점으로 잡고 어떤 방향을 축으로 설정해야 두 물체가 ’같은 포즈’에 있다고 말할 수 있는가? 기준이 없으면 학습 자체가 불가능하다.</p>
<p>NOCS는 이 질문에 대해 모든 물체 인스턴스를 포괄하는 **공유된 표준 공간(Shared Canonical Space)**을 정의함으로써 답한다. 이 공간은 3차원 단위 큐브(Unit Cube)로 정의되며, 수학적으로는 <span class="math math-inline">\{x, y, z\} \in ^3</span>의 영역을 갖는다.</p>
<p>이 단위 큐브는 단순한 공간이 아니라, 해당 범주의 모든 물체가 기하학적으로 정렬(Align)되어야 하는 목표 공간이다. NOCS의 핵심 전제는 “같은 범주에 속하는 물체들은 위상적으로 유사한 구조를 가지며, 적절한 아핀 변환(Affine Transformation)을 통해 하나의 표준 공간으로 매핑될 수 있다“는 것이다.</p>
<h3>1.2  NOCS 맵(Map)과 정규화 변환식</h3>
<p>NOCS의 핵심 아이디어는 입력 이미지의 각 픽셀이 단순히 색상(RGB) 정보나 깊이(Depth) 정보를 담는 것을 넘어, 해당 픽셀이 가리키는 물체 표면의 점이 <strong>표준화된 3D 공간 상에서 어디에 위치하는지</strong>를 예측하는 것이다. 이를 **NOCS 맵(NOCS Map)**이라 한다. NOCS 맵은 입력 RGB 이미지와 동일한 해상도를 가지며, 각 픽셀 위치 <span class="math math-inline">(u, v)</span>에는 해당 지점의 객체 표면 좌표 <span class="math math-inline">(x_{nocs}, y_{nocs}, z_{nocs})</span>가 저장된다.</p>
<p>어떤 범주에 속한 물체 인스턴스 <span class="math math-inline">i</span>가 주어졌을 때, 카메라 좌표계 상의 3차원 점 <span class="math math-inline">P_{camera}</span>는 다음과 같은 유사 변환(Similarity Transformation)을 통해 NOCS 좌표 <span class="math math-inline">P_{nocs}</span>로 매핑된다.<br />
<span class="math math-display">
P_{nocs} = S^{-1} R^{-1} (P_{camera} - T) + \mathbf{c}
</span><br />
여기서:</p>
<ul>
<li><span class="math math-inline">P_{camera} \in \mathbb{R}^3</span>: 카메라 좌표계에서의 3차원 점 위치 (깊이 센서로부터 역투영됨).</li>
<li><span class="math math-inline">T \in \mathbb{R}^3</span>: 물체의 중심 위치 (Translation).</li>
<li><span class="math math-inline">R \in SO(3)</span>: 물체의 회전 행렬 (Rotation).</li>
<li><span class="math math-inline">S \in \mathbb{R}^{3 \times 3}</span>: 물체의 크기 행렬 (Scale). NOCS의 기본 설정에서는 등방성 스케일(Isotropic Scale) <span class="math math-inline">s \in \mathbb{R}</span>을 사용하여 <span class="math math-inline">S = sI</span>로 단순화되기도 하지만, 형태 변형이 심한 경우 축별 스케일링이 고려될 수 있다.</li>
<li><span class="math math-inline">\mathbf{c}</span>: 단위 큐브의 중심 오프셋, 일반적으로 <span class="math math-inline">(0.5, 0.5, 0.5)</span>로 설정된다.</li>
</ul>
<p>이 정의에 따르면, 모든 물체는 크기와 회전이 정규화되어 단위 큐브 안에 꽉 차게(Tight Bound) 정렬된다. 예를 들어, 모든 ‘카메라’ 범주의 물체는 렌즈가 NOCS의 +X 축을 향하고, 셔터 버튼이 +Y 축을 향하며, 뷰파인더가 -X 축을 향하도록 정렬된다. ’병(Bottle)’의 경우 병목이 +Y 축(또는 정의에 따라 +Z 축)을 향하도록 정렬되며, 병의 바닥면은 <span class="math math-inline">y=0</span> 평면 근처에, 병뚜껑은 <span class="math math-inline">y=1</span> 평면 근처에 매핑된다.</p>
<h3>1.3  형태 변형의 밀도 함수적 해석</h3>
<p>이러한 정규화 과정은 **형태 변형(Shape Variation)**을 <strong>좌표의 밀도 함수(Density Function)의 차이</strong>로 치환하는 효과를 갖는다.</p>
<ul>
<li><strong>키가 큰 병:</strong> NOCS 공간 내에서 Y축 방향으로 더 많은 픽셀이 매핑되거나, Y축 좌표의 변화율이 완만하게 분포할 것이다.</li>
<li><strong>뚱뚱한 병:</strong> X, Z축 방향으로 넓게 퍼진 좌표 분포를 가질 것이다.</li>
</ul>
<p>하지만 중요한 것은 두 병 모두 <span class="math math-inline">^3</span> 공간 내에 존재하며, 바닥면은 <span class="math math-inline">y \approx 0</span>이고 입구는 <span class="math math-inline">y \approx 1</span>이라는 **위상적 일관성(Topological Consistency)**을 유지한다는 점이다. 이것이 NOCS가 보지 못한 물체(Unseen Objects)에 대해 일반화할 수 있는 강력한 기하학적 근거가 된다. 딥러닝 네트워크는 개별 물체의 절대적인 크기나 위치를 외우는 대신, 물체의 <strong>상대적인 기하학적 구조(Relative Geometric Structure)</strong>, 즉 “병의 목은 몸통보다 위에 있고 좁다“와 같은 관계성을 NOCS 좌표 예측을 통해 학습하게 된다.</p>
<h2>2.  NOCS 예측을 위한 심층 신경망 아키텍처</h2>
<p>NOCS 맵을 추론하는 것은 본질적으로 2D 이미지의 각 픽셀에 대해 3D 좌표 <span class="math math-inline">(r, g, b)</span> 값을 회귀(Regression)하는 문제로 볼 수 있다. 이를 구현하기 위해 연구팀은 객체 검출 및 분할 분야의 표준인 <strong>Mask R-CNN</strong>  아키텍처를 기반으로 하되, 이를 3차원 좌표 예측에 특화되도록 확장한 구조를 제안했다.</p>
<h3>2.1  백본(Backbone)과 특징 피라미드(FPN)의 역할</h3>
<p>입력 RGB 이미지는 ResNet-50 또는 ResNet-101 기반의 백본 네트워크를 통과하며 특징(Feature)을 추출한다. 로봇이 마주하는 환경에서는 물체의 크기가 거리에 따라 극적으로 변하기 때문에, 단일 스케일의 특징만으로는 부족하다. 이를 보완하기 위해 <strong>FPN (Feature Pyramid Network)</strong> 구조가 필수적으로 사용된다.</p>
<p>FPN은 상위 레이어의 풍부한 의미론적(Semantic) 정보와 하위 레이어의 정밀한 공간적(Spatial) 정보를 측면 연결(Lateral Connection)과 업샘플링을 통해 결합한다. 이를 통해 아주 작은 물체(멀리 있는 병)부터 큰 물체(가까이 있는 노트북)까지 모든 스케일에서 강건한 특징 맵을 생성할 수 있다. 이는 NOCS 맵의 픽셀 단위 정밀도를 보장하는 데 결정적인 역할을 한다.</p>
<h3>2.2  ROI Align과 다중 헤드(Multi-Head) 설계</h3>
<p>Region Proposal Network (RPN)이 물체가 존재할 확률이 높은 영역(Region of Interest, ROI)을 제안하면, <strong>ROI Align</strong>  연산을 통해 해당 영역의 특징을 고정된 크기(예: <span class="math math-inline">14 \times 14</span> 또는 <span class="math math-inline">28 \times 28</span>)의 특징 맵으로 추출한다. 기존의 ROI Pooling은 좌표의 양자화(Quantization) 과정에서 미세한 위치 정보가 손실되는 문제가 있었으나, ROI Align은 쌍선형 보간(Bilinear Interpolation)을 사용하여 부동 소수점 단위의 정확한 특징 추출을 가능하게 한다. 이는 3D 좌표 예측과 같은 정밀한 작업(Dense Regression Task)에서 필수적인 요소이다.</p>
<p>추출된 특징 맵은 서로 다른 역할을 수행하는 병렬적인 헤드(Head)들로 분기된다 :</p>
<ol>
<li><strong>Class Head (분류):</strong> 물체의 범주(Label)를 예측한다 (Softmax, <span class="math math-inline">C</span>개 클래스 + 배경).</li>
<li><strong>Box Head (검출):</strong> 2D 바운딩 박스 좌표 4개 <span class="math math-inline">(x, y, w, h)</span>를 예측한다.</li>
<li><strong>Mask Head (분할):</strong> 물체의 실루엣을 나타내는 이진 마스크(Binary Mask)를 예측한다.</li>
<li><strong>NOCS Head (좌표):</strong> 물체 픽셀별 <span class="math math-inline">(x, y, z)</span> NOCS 좌표를 예측한다.</li>
</ol>
<h3>2.3  좌표 회귀(Regression) 대 분류(Classification)의 딜레마</h3>
<p>NOCS 연구 과정에서 발견된 가장 중요한 공학적 통찰 중 하나는, 연속적인 3D 좌표 값을 직접 회귀(Direct Regression, 예: MSE Loss 사용)하는 것보다, 좌표 공간을 이산적인 구간(Bin)으로 나누어 <strong>분류(Classification)</strong> 문제로 접근하는 것이 학습 안정성과 최종 성능 면에서 월등히 우수하다는 점이다.</p>
<p>직접 회귀 방식은 0과 1 사이의 실수 값을 예측해야 하는데, 이는 신경망의 출력 범위가 넓고 최적화가 불안정해지기 쉽다. 반면, 분류 기반 접근은 다음과 같이 설계된다:</p>
<ol>
<li><strong>Binning (구간화):</strong> $$의 좌표 범위를 <span class="math math-inline">N</span>개의 구간(Bin)으로 나눈다. 실험적으로 <span class="math math-inline">N=32</span>가 가장 좋은 성능을 보였다.</li>
<li><strong>Classification (구간 분류):</strong> 각 픽셀의 <span class="math math-inline">x, y, z</span> 좌표가 32개의 Bin 중 어디에 속하는지를 예측한다. 이는 Cross Entropy Loss를 사용하여 학습된다.</li>
<li><strong>Offset Regression (미세 조정):</strong> 해당 Bin의 중심으로부터의 미세한 오차(Offset) <span class="math math-inline">\Delta</span>를 예측한다. 이는 Smooth L1 Loss를 사용한다.</li>
</ol>
<p>이러한 <strong>“Bin Classification + Offset Regression”</strong> 전략은 3D 공간의 비선형성을 완화하여 수렴 속도를 높일 뿐만 아니라, 특정 축 방향의 대칭성이나 모호성을 확률 분포(Softmax 확률값)로 표현할 수 있게 하여 네트워크가 불확실성을 효과적으로 학습하는 데 도움을 준다. 예를 들어, 대칭적인 물체의 경우 좌표의 확률 분포가 여러 Bin에 걸쳐 나타날 수 있으며, 이는 네트워크가 대칭성을 인지하고 있음을 시사한다.</p>
<h2>3. 손실 함수(Loss Function)의 설계와 대칭성(Symmetry) 처리</h2>
<p>네트워크 학습을 위한 총 손실 함수 <span class="math math-inline">L_{total}</span>은 각 헤드의 손실 함수들의 가중 합으로 정의된다 :<br />
<span class="math math-display">
L_{total} = L_{class} + L_{box} + L_{mask} + \lambda L_{nocs}
</span><br />
여기서 <span class="math math-inline">L_{class}</span>, <span class="math math-inline">L_{box}</span>, <span class="math math-inline">L_{mask}</span>는 기존 Mask R-CNN의 손실 함수와 동일하며, 핵심은 <span class="math math-inline">L_{nocs}</span>의 설계에 있다.</p>
<h3>3.1 NOCS 손실 함수의 구조</h3>
<p>앞서 설명한 분류 기반 접근법에 따라, <span class="math math-inline">L_{nocs}</span>는 다시 분류 손실과 회귀 손실로 나뉜다.<br />
<span class="math math-display">
L_{nocs} = \sum_{k \in \{x, y, z\}} (L_{bin}^k + L_{offset}^k)
</span></p>
<ul>
<li><strong><span class="math math-inline">L_{bin}^k</span>:</strong> <span class="math math-inline">k</span>축 좌표가 속한 Bin을 맞추는 Softmax Cross Entropy Loss.</li>
<li><strong><span class="math math-inline">L_{offset}^k</span>:</strong> 선택된 Bin 내에서의 위치 오차를 줄이는 Smooth L1 Loss.</li>
</ul>
<p>이 구조는 객체 감지 모델인 YOLO나 SSD에서 바운딩 박스를 예측할 때 앵커(Anchor) 박스를 사용하는 원리와 유사하다. Bin이 일종의 3D 앵커 역할을 수행하여 탐색 공간을 제한하고 학습을 가이드하는 것이다.</p>
<h3>3.2 대칭 물체의 기하학적 모호성 해결</h3>
<p>범주 수준 포즈 추정에서 가장 치명적인 난제는 물체의 **대칭성(Symmetry)**이다. 인스턴스 수준 포즈 추정에서는 텍스처(Texture)가 고유하기 때문에 대칭 물체라도 미세한 흠집이나 무늬를 통해 방향을 구분할 수 있지만, 범주 수준에서는 형상 그 자체가 대칭인 경우가 많다.</p>
<p>예를 들어, 손잡이가 보이지 않는 각도에서의 머그컵, 원통형 병, 둥근 그릇(Bowl) 등은 회전 대칭성(Rotational Symmetry)을 가진다. 대칭축을 중심으로 회전한 병은 RGB 이미지 상에서(겉보기에) 동일해 보이지만, NOCS 좌표계 상에서는 X, Z 좌표값이 회전에 따라 계속 변한다. 만약 네트워크에게 특정 각도의 NOCS 좌표를 정답(Ground Truth)으로 강제하면, 시각적으로 구분이 불가능한 입력(회전된 병들)에 대해 서로 다른 정답을 요구하는 모순이 발생한다. 이 경우 네트워크는 학습에 실패하거나, 가능한 모든 좌표값의 평균인 회색(<span class="math math-inline">0.5, 0.5, 0.5</span>)으로 수렴해버리는 현상이 발생한다.</p>
<h3>3.3 대칭 인식 손실 함수 (Symmetry-Aware Loss)</h3>
<p>NOCS 연구진은 이를 해결하기 위해 대칭성 종류에 따라 특화된 손실 함수를 제안했다. 이는 기하학적 사전 지식(Geometric Prior)을 손실 함수에 주입하는 것이다.</p>
<table><thead><tr><th><strong>대칭 유형</strong></th><th><strong>예시 물체</strong></th><th><strong>손실 함수 전략 (Lsym)</strong></th></tr></thead><tbody>
<tr><td><strong>비대칭 (Asymmetric)</strong></td><td>카메라, 노트북, 신발</td><td>일반적인 <span class="math math-inline">L_{bin} + L_{offset}</span> 사용. 유일한 정답 존재.</td></tr>
<tr><td><strong>이산 대칭 (Discrete)</strong></td><td>직육면체 상자 (180°), 책</td><td>가능한 대칭 변환군 <span class="math math-inline">G_{sym}</span>을 정의하고, 변환된 NOCS 맵 중 예측값과 가장 차이가 적은 것을 선택하여 손실 계산. <span class="math math-display">L_{sym} = \min_{R \in G_{sym}} \text{Loss}(P_{pred}, R \cdot P_{gt})</span></td></tr>
<tr><td><strong>연속 대칭 (Continuous)</strong></td><td>병, 캔, 그릇 (Bowl)</td><td>대칭축(예: Y축)을 중심으로 한 임의의 회전 <span class="math math-inline">\theta</span>에 대해 불변이어야 함. 이론적으로는 축 방향 좌표(<span class="math math-inline">y</span>)와 축으로부터의 거리(<span class="math math-inline">r = \sqrt{x^2+z^2}</span>)만 비교해야 함. 실제 구현에서는 <span class="math math-inline">N</span>개의 이산화된 회전 각도(예: 60단계) 중 최소 손실을 선택하는 방식으로 근사화.</td></tr>
</tbody></table>
<p>구체적으로 연속 대칭 물체(예: 병)에 대해서는 대칭축(Y축)을 기준으로 <span class="math math-inline">P_{gt}</span>를 <span class="math math-inline">0^\circ</span>부터 <span class="math math-inline">360^\circ</span>까지 일정 간격으로 회전시킨 여러 후보군 <span class="math math-inline">\tilde{P}_{gt}^\theta</span>를 생성한다. 그 후 네트워크가 예측한 <span class="math math-inline">P_{pred}</span>와 가장 가까운 후보군과의 차이를 손실로 사용한다.</p>
<p>$$L_{continuous} = \min_{\theta \in 이는 로봇이 원통형 물체나 접시 등을 다룰 때 발생할 수 있는 포즈의 모호성을 해소하는 결정적인 기술적 진보이다.</p>
<h2>4. 2D-3D 대응과 포즈 최적화: Umeyama 알고리즘과 강건한 추정</h2>
<p>네트워크가 추론 과정을 마치면, 우리는 입력 이미지의 물체 영역에 대해 두 가지 형태의 점 구름(Point Cloud) 데이터를 확보하게 된다.</p>
<ol>
<li><strong>관측된 점 구름 (Observed Point Cloud, <span class="math math-inline">P_{obs}</span>):</strong> 깊이(Depth) 이미지의 픽셀값 <span class="math math-inline">Z</span>와 카메라 내부 파라미터(Intrinsics) <span class="math math-inline">K</span>를 사용하여 3차원으로 역투영(Back-projection)한 점들. 이는 카메라 좌표계 상에 존재하며, 실제 물리적 크기(Metric Scale)를 가진다.</li>
<li><strong>예측된 NOCS 점 구름 (Predicted NOCS Point Cloud, <span class="math math-inline">P_{nocs}</span>):</strong> 네트워크가 예측한 NOCS 맵의 RGB 값(각 채널이 <span class="math math-inline">x, y, z</span> 좌표에 대응)을 추출한 점들. 이는 <span class="math math-inline">^3</span> 공간에 존재하며 정규화된 크기를 가진다.</li>
</ol>
<p>이제 남은 과제는 정규화된 공간의 점들 <span class="math math-inline">P_{nocs}</span>를 어떻게 변환해야 관측된 현실 공간의 점들 <span class="math math-inline">P_{obs}</span>와 정확하게 겹쳐지는지를 찾아내는 것이다. 즉, <span class="math math-inline">P_{nocs}</span>를 회전(<span class="math math-inline">R</span>), 이동(<span class="math math-inline">T</span>), 스케일링(<span class="math math-inline">s</span>)하여 <span class="math math-inline">P_{obs}</span>에 정합(Registration)시키는 최적의 파라미터 <span class="math math-inline">{s, R, T}</span>를 구하는 문제이다. 이는 컴퓨터 비전에서 <strong>절대 방향 문제(Absolute Orientation Problem)</strong> 또는 <strong>7자유도 유사 변환 추정(7-DoF Similarity Transformation Estimation)</strong> 문제로 알려져 있다.</p>
<h3>4.1 Umeyama 알고리즘: 닫힌 해(Closed-form Solution)의 유도</h3>
<p>NOCS 논문에서는 이 문제를 해결하기 위해 <strong>Umeyama 알고리즘</strong> 을 채택하였다. Umeyama 알고리즘은 두 점 집합 <span class="math math-inline">X = \{x_i\}</span> (<span class="math math-inline">P_{nocs}</span>)와 <span class="math math-inline">Y = \{y_i\}</span> (<span class="math math-inline">P_{obs}</span>) 사이의 평균 제곱 오차(MSE)를 최소화하는 변환을 SVD(특이값 분해)를 통해 해석적으로 한 번에 계산해낸다.</p>
<p>최적화 목표 함수는 다음과 같다:<br />
<span class="math math-display">
\min_{s, R, T} \sum_{i=1}^{N} \Vert y_i - (s R x_i + T) \Vert^2
</span><br />
이 문제는 반복적인 최적화(Iterative Optimization) 없이 다음과 같은 절차로 빠르고 정확하게 풀린다 :</p>
<ol>
<li>
<p><strong>중심화(Centering):</strong> 두 점 집합의 무게 중심(Centroid) <span class="math math-inline">\mu_x, \mu_y</span>를 계산하고, 각 점을 중심 기준으로 이동시켜 이동 성분(Translation)을 일시적으로 제거한다.<br />
<span class="math math-display">
x&#39;_i = x_i - \mu_x, \quad y&#39;_i = y_i - \mu_y
</span></p>
</li>
<li>
<p><strong>공분산 행렬(Covariance Matrix) 계산:</strong> 두 중심화된 점 집합 간의 교차 공분산 행렬을 구한다.<br />
<span class="math math-display">
\Sigma_{xy} = \frac{1}{N} \sum_{i=1}^{N} y&#39;_i {x&#39;_i}^T
</span></p>
</li>
<li>
<p><strong>특이값 분해(SVD):</strong> 공분산 행렬을 분해하여 회전 성분을 추출할 준비를 한다.<br />
<span class="math math-display">
\Sigma_{xy} = U D V^T
</span></p>
</li>
<li>
<p><strong>회전 행렬(Rotation) 도출:</strong><br />
<span class="math math-display">
R = U S V^T, \quad S = \text{diag}(1, 1, \det(U V^T))
</span><br />
여기서 행렬 <span class="math math-inline">S</span>는 반사(Reflection)를 방지하기 위한 보정 행렬이다. 만약 <span class="math math-inline">\det(U V^T) = -1</span>이라면 이는 회전이 아니라 거울 대칭 변환이 포함되었다는 의미이므로, 마지막 특이값을 -1로 반전시켜 올바른 회전 행렬 <span class="math math-inline">R \in SO(3)</span>을 보장한다.</p>
</li>
<li>
<p><strong>스케일(Scale) 및 이동(Translation) 도출:</strong> 회전 행렬이 구해지면 스케일과 이동은 간단히 계산된다.<br />
<span class="math math-display">
s = \frac{\text{tr}(D)}{\sum \Vert x&#39;_i \Vert^2 / N}, \quad T = \mu_y - s R \mu_x
</span></p>
</li>
</ol>
<p>이 과정을 통해 로봇은 물체의 6D 포즈뿐만 아니라 물리적 크기(<span class="math math-inline">s</span>)까지 동시에 획득하게 된다. 이는 로봇 팔이 물체를 잡을 때 그리퍼(Gripper)를 얼마나 벌려야 하는지(Gripper Aperture)를 결정하는 데 필수적인 정보이다.</p>
<h3>4.2 RANSAC을 통한 아웃라이어 제거와 강건함 확보</h3>
<p>이론적으로 Umeyama 알고리즘은 완벽하지만, 현실 데이터에는 항상 노이즈가 존재한다. 특히 딥러닝 네트워크가 예측한 NOCS 맵은 물체의 경계 부분이나 가려진(Occluded) 영역에서 심각한 오차를 보일 수 있다. 또한 깊이 센서 자체의 노이즈(Sensor Noise)도 무시할 수 없다. 이러한 **아웃라이어(Outlier)**들이 최소자승법(Least Squares) 기반인 Umeyama 알고리즘에 포함되면, 오차의 제곱합을 최소화하려는 성질 때문에 전체 포즈 추정이 크게 틀어지게 된다.</p>
<p>이를 방지하기 위해 <strong>RANSAC (Random Sample Consensus)</strong>  알고리즘이 Umeyama 알고리즘의 래퍼(Wrapper)로 적용된다.</p>
<ol>
<li><strong>샘플링:</strong> 전체 점 대응 쌍 중에서 무작위로 소수(예: 3개 또는 4개)의 샘플을 선택한다.</li>
<li><strong>가설 수립:</strong> 선택된 샘플만으로 Umeyama 알고리즘을 수행하여 임시 포즈(Hypothesis Pose)를 계산한다.</li>
<li><strong>검증 (Voting):</strong> 계산된 임시 포즈를 전체 점에 적용했을 때, 예측 위치와 실제 위치의 오차가 임계값(Threshold, 예: 1cm 또는 10% 오차) 이내인 점들(Inlier)의 개수를 센다.</li>
<li><strong>반복:</strong> 이 과정을 수백 번(예: <span class="math math-inline">K=100</span>) 반복하여 인라이어가 가장 많이 포함되는 최적의 가설을 선택한다.</li>
<li><strong>정밀화 (Refinement):</strong> 선택된 최적 가설의 인라이어 점들만을 모두 사용하여 최종적으로 Umeyama 알고리즘을 한 번 더 수행함으로써, 노이즈가 제거된 정밀한 포즈를 얻는다.</li>
</ol>
<p>이러한 RANSAC-Umeyama 파이프라인 덕분에 NOCS 기반 방법론은 심한 클러터(Clutter) 환경이나 부분적으로 가려진 물체에 대해서도 매우 높은 강건성을 보여준다. 실험 결과에 따르면, 3D IoU 50% 기준 88.4%의 mAP를 달성하여 기존 SOTA 모델들을 압도하는 성능을 입증하였다.</p>
<h2>5. 범주 내 형태 변형(Intra-Class Shape Variation)의 극복 전략과 한계</h2>
<p>NOCS가 기존 방법론 대비 혁신적인 이유는 ’형태 변형’을 명시적으로 다루기 때문이다. 이 섹션에서는 NOCS가 어떻게 다양한 형태를 하나의 공간으로 수렴시키는지, 그리고 그 과정에서 발생하는 의미론적 한계와 이를 극복하기 위한 최신 연구 동향(SOCS 등)을 심층 분석한다.</p>
<h3>5.1 아핀 변환(Affine Transformation)으로서의 형태 변형 모델링</h3>
<p>NOCS는 물체의 범주 내 형태 변형을 일종의 <strong>아핀 변환(Affine Transformation)</strong>, 특히 각 축 방향으로의 스케일링(Scaling) 차이로 해석한다.</p>
<p>예를 들어, ‘머그컵’ 범주를 생각해보자. 표준 머그컵이 NOCS 공간의 <span class="math math-inline">(0.2, 0, 0.2)</span>에서 <span class="math math-inline">(0.8, 0.6, 0.8)</span>까지 점유한다고 가정하자. 만약 현실 세계에서 키가 아주 큰 ’텀블러형 머그컵’이 입력되었다면, 이는 표준 머그컵이 Y축(높이) 방향으로 늘어난(Stretched) 형태이다. NOCS 네트워크는 이 긴 머그컵의 상단 픽셀들을 여전히 NOCS 공간의 <span class="math math-inline">y \approx 0.6</span> 근처로 매핑하도록 학습된다. 즉, 네트워크는 물체의 픽셀들이 <strong>상대적으로 어디에 위치하는지</strong>를 학습한다.</p>
<p>결과적으로, Umeyama 알고리즘이 계산하는 **스케일 인자 <span class="math math-inline">s</span>**와 변환 행렬은 단순히 물체와 카메라 간의 거리에 따른 원근법적 크기뿐만 아니라, <strong>범주 표준 형상 대비 해당 인스턴스의 실제 기하학적 변형 비율</strong>을 내포하게 된다. 이는 로봇에게 “이 물체는 머그컵인데, 보통 머그컵보다 높이가 1.5배 높다“라는 정보를 수치적으로 제공하는 것과 같다.</p>
<h3>5.2 합성 데이터(Synthetic Data)와 도메인 적응: CAMERA 데이터셋</h3>
<p>수천, 수만 가지의 형태 변형을 학습하기 위해서는 막대한 양의 데이터가 필요하다. 현실 세계에서 서로 다른 디자인의 머그컵 1,000개를 구해서 정밀하게 3D 스캔하고, 각 픽셀마다 NOCS 좌표를 일일이 어노테이션하는 것은 비용적으로나 시간적으로 불가능에 가깝다.</p>
<p>이 문제를 해결하기 위해 NOCS 연구진은 <strong>CAMERA (Context-Aware MixEd ReAlity)</strong> 데이터셋 생성 기법을 개발하였다.</p>
<ol>
<li><strong>ShapeNetCore 활용:</strong> ShapeNet과 같은 대규모 3D 모델 리포지토리에서 6개 범주(병, 그릇, 카메라, 캔, 노트북, 머그)에 해당하는 수천 개의 3D CAD 모델을 확보한다. 이들은 이미 다양한 형태 변형을 포함하고 있다.</li>
<li><strong>현실 배경 합성 (Context-Aware Compositing):</strong> 단순히 검은 배경에 물체를 띄우는 것이 아니라, 실제 실내 환경 데이터셋(Real275 등)의 배경 이미지를 사용한다.</li>
<li><strong>물리적 타당성 (Physical Plausibility):</strong> 평면 검출(Plane Detection) 알고리즘을 사용하여 책상이나 바닥과 같은 평면을 찾고, 그 위에 가상 물체를 자연스럽게 배치한다. 물체가 공중에 떠 있거나 바닥을 뚫고 들어가는 비현실적인 상황을 방지한다.</li>
<li><strong>조명 및 렌더링:</strong> 주변 환경의 조명 조건을 추정하여 가상 물체에 그림자와 반사 효과를 적용함으로써, 실제 이미지와 가상 물체 간의 이질감(Domain Gap)을 최소화한다.</li>
<li><strong>자동 어노테이션:</strong> 렌더링 엔진을 통해 생성되므로, 픽셀별 NOCS 좌표, 인스턴스 마스크, 6D 포즈 등의 정답(Ground Truth)을 오차 없이 완벽하게 자동 생성할 수 있다.</li>
</ol>
<p>이렇게 생성된 30만 장 이상의 CAMERA 데이터셋은 네트워크가 “다양한 형태“를 “동일한 NOCS 공간“으로 매핑하는 기하학적 규칙을 학습하는 데 핵심적인 역할을 했다. 실제 데이터(Real-World Data)가 부족한 상황에서, 합성 데이터로 기하학적 추론 능력을 기르고 소량의 실제 데이터로 미세 조정(Fine-tuning)하는 <strong>Sim-to-Real</strong> 전략은 이제 엠바디드 AI 학습의 표준 패러다임이 되었다.</p>
<h3>5.3 NOCS의 한계: 의미론적 불일치(Semantic Misalignment)</h3>
<p>초기 NOCS 연구는 큰 성공을 거두었지만, 극심한 형태 변형에 대해서는 근본적인 한계를 드러냈다. NOCS 공간은 기본적으로 물체들을 강체 변환(Rigid Transformation)과 전체적인 스케일링(Global Scaling)을 통해 정렬하여 생성된다. 하지만 <strong>비선형적인 형태 변형</strong>을 가진 물체들 사이에서는 이러한 강체 정렬이 **의미론적 불일치(Semantic Misalignment)**를 유발한다.</p>
<p>가장 대표적인 예가 ‘카메라’ 범주이다.</p>
<ul>
<li><strong>DSLR 카메라:</strong> 렌즈가 몸통의 중앙에 위치하고 크기가 크다.</li>
<li><strong>컴팩트 카메라:</strong> 렌즈가 몸통의 한쪽 구석에 위치하고 크기가 작다.</li>
</ul>
<p>이 두 카메라를 NOCS의 단위 큐브에 억지로 끼워 맞추기 위해 전체 크기만 조절하여 정렬하면, DSLR의 렌즈 중심 좌표와 컴팩트 카메라의 렌즈 중심 좌표가 NOCS 공간 상에서 서로 다른 위치에 매핑된다. 즉, NOCS 좌표 <span class="math math-inline">(0.5, 0.5, 0.5)</span>가 어떤 카메라에서는 렌즈 중심을 가리키지만, 다른 카메라에서는 바디 케이스를 가리키게 되는 것이다.</p>
<p>이러한 **의미론적 비일관성(Semantic Incoherence)**은 로봇이 “카메라의 렌즈를 잡아라“와 같이 특정 부품(Part)을 조작해야 할 때 치명적인 오류를 낳는다. 네트워크는 혼란에 빠져 중간값으로 예측하게 되고, 결과적으로 포즈 추정의 정확도가 떨어진다.</p>
<h3>5.4 해결책의 모색: SOCS와 변형 가능한 정렬</h3>
<p>이 문제를 극복하기 위해 제안된 개념이 <strong>SOCS (Semantically-aware Object Coordinate Space)</strong> 이다. SOCS는 NOCS 공간을 정의할 때 강체 정렬 대신 <strong>변형 가능한(Deformable) 정렬</strong>을 사용한다.</p>
<ol>
<li><strong>의미론적 키포인트(Semantic Keypoints) 정의:</strong> 렌즈 중심, 셔터 버튼, 밑면 모서리 등 의미적으로 동일한 지점들을 정의한다.</li>
<li><strong>비강체 워핑(Non-rigid Warping):</strong> 모든 학습용 3D 모델들을 워핑하여, 이 키포인트들이 표준 공간 상의 동일한 좌표에 위치하도록 형상을 찌그러뜨리거나 늘린다.</li>
<li><strong>일관된 학습:</strong> 이렇게 생성된 SOCS 데이터로 학습하면, 네트워크는 카메라의 형태가 아무리 달라도 렌즈 중심은 항상 <span class="math math-inline">(x_L, y_L, z_L)</span> 좌표를 갖는다는 일관된 규칙을 학습할 수 있다.</li>
</ol>
<p>이 외에도 <strong>DualPoseNet</strong> 과 같이 암시적(Implicit) 포즈 디코더를 추가하여 입력 점 구름을 표준 포즈로 복원(Reconstruction)하는 보조 작업을 수행하게 하거나, <strong>SPD (Shape Prior Deformation)</strong> 와 같이 범주별 평균 형상(Mean Shape)을 변형시켜 입력에 맞추는 방식 등이 연구되고 있다. 이러한 기술들은 NOCS가 가진 ’강체 정렬의 한계’를 넘어, 형태가 급격히 변하는 물체들에 대해서도 로봇이 부품 수준의 정밀한 이해와 조작을 가능하게 하는 방향으로 진화하고 있다.</p>
<h3>결론</h3>
<p>NOCS(Normalized Object Coordinate Space)는 로봇이 사물을 인식하는 방식을 “메모리에 저장된 A를 찾는다“는 템플릿 매칭(Template Matching) 방식에서, “A라는 범주의 기하학적 구조를 이해하고, 관측된 물체를 그 구조에 투영한다“는 추론(Reasoning) 방식으로 격상시켰다.</p>
<p>7.3.2절에서 살펴본 NOCS의 핵심 기여는 다음과 같다:</p>
<ol>
<li><strong>표준화(Normalization):</strong> 모든 물체를 단위 큐브라는 공통의 좌표계로 투영하여, 형태가 다른 물체들을 수학적으로 비교 가능하게 만들었다.</li>
<li><strong>데이터 중심(Data-Centric):</strong> CAMERA와 같은 맥락 인식 합성 데이터 생성 기술을 통해, 현실 데이터의 부족함을 극복하고 형태 변형에 대한 일반화 성능을 확보했다.</li>
<li><strong>기하학과 딥러닝의 결합:</strong> 딥러닝의 강력한 특징 추출 능력으로 NOCS 맵(확률적 예측)을 만들고, 이를 Umeyama 알고리즘(결정론적 기하학)으로 해석하여 정밀한 6D 포즈를 도출하는 하이브리드 파이프라인을 정립했다.</li>
</ol>
<p>이 기술은 단순히 물체의 위치를 파악하는 것을 넘어, 이어지는 7.5절의 파지(Grasping) 계획과 17장의 시각-언어-행동 모델(VLA)이 작동하기 위한 물리적 좌표계(Grounding Interface)를 제공한다. 로봇이 “저 긴 병을 잡아서 컵에 따라줘“라는 추상적인 언어 명령을 수행하기 위해서는, ’병’과 ’컵’이라는 의미적 기호가 NOCS와 같은 정밀한 기하학적 좌표 공간 위에 안착(Grounding)되어야만 한다. 따라서 NOCS는 소프트웨어 2.0 시대의 엠바디드 AI에 있어 지각(Perception)과 행동(Action)을 매개하는 가장 중요한 기하학적 플랫폼이라 할 수 있다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>normalized-object-coordinate-space-for-category-level-6d- …, 1월 31, 2026에 액세스, https://scispace.com/pdf/normalized-object-coordinate-space-for-category-level-6d-21rkz8vogv.pdf</li>
<li>Normalized Object Coordinate Space for Category-Level 6D Object …, 1월 31, 2026에 액세스, https://geometry.stanford.edu/projects/NOCS_CVPR2019/pub/NOCS_CVPR2019.pdf</li>
<li>Normalized Object Coordinate Space for Category-Level 6D … - ar5iv, 1월 31, 2026에 액세스, https://ar5iv.labs.arxiv.org/html/1901.02970</li>
<li>Attention-Based Mask R-CNN Enhancement for Infrared Image …, 1월 31, 2026에 액세스, https://www.mdpi.com/2073-8994/17/7/1099</li>
<li>What is Mask R-CNN? - sky engine ai, 1월 31, 2026에 액세스, https://www.skyengine.ai/blog/what-is-mask-r-cnn</li>
<li>How Mask R-CNN Works? | ArcGIS API for Python - Esri Developer, 1월 31, 2026에 액세스, https://developers.arcgis.com/python/latest/guide/how-maskrcnn-works/</li>
<li>Normalized Object Coordinate Space for Category-Level 6D Object …, 1월 31, 2026에 액세스, https://geometry.stanford.edu/projects/NOCS_CVPR2019/</li>
<li>VladimirYugay/category-level-6D-pose-estimation - GitHub, 1월 31, 2026에 액세스, https://github.com/VladimirYugay/category-level-6D-pose-estimation</li>
<li>Offset Bin Classification Network for Accurate Object Detection, 1월 31, 2026에 액세스, https://openaccess.thecvf.com/content_CVPR_2020/papers/Qiu_Offset_Bin_Classification_Network_for_Accurate_Object_Detection_CVPR_2020_paper.pdf</li>
<li>Aligning point patterns with Kabsch–Umeyama algorithm, 1월 31, 2026에 액세스, https://zpl.fi/aligning-point-patterns-with-kabsch-umeyama-algorithm/</li>
<li>Kabsch algorithm - Wikipedia, 1월 31, 2026에 액세스, https://en.wikipedia.org/wiki/Kabsch_algorithm</li>
<li>The Geometry Of Kernel Canonical Correlation Analysis - Microsoft, 1월 31, 2026에 액세스, https://www.microsoft.com/en-us/research/wp-content/uploads/2003/01/kussgrae03.pdf</li>
<li>pose estimation for robotic disassembly using ransac with line features, 1월 31, 2026에 액세스, https://open.clemson.edu/all_theses/1126/</li>
<li>An Accurate and Robust Method for Absolute Pose Estimation with …, 1월 31, 2026에 액세스, https://www.mdpi.com/1424-8220/22/15/5925</li>
<li>Random sample consensus - Wikipedia, 1월 31, 2026에 액세스, https://en.wikipedia.org/wiki/Random_sample_consensus</li>
<li>SupeRANSAC: One RANSAC to Rule Them All - arXiv, 1월 31, 2026에 액세스, https://arxiv.org/html/2506.04803v1</li>
<li>ICCV 2023 Open Access Repository, 1월 31, 2026에 액세스, https://openaccess.thecvf.com/content/ICCV2023/html/Wan_SOCS_Semantically-Aware_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_ICCV_2023_paper.html</li>
<li>arXiv:2303.10346v1 [cs.CV] 18 Mar 2023, 1월 31, 2026에 액세스, https://arxiv.org/pdf/2303.10346</li>
<li>SOCS: Semantically-Aware Object Coordinate Space for Category …, 1월 31, 2026에 액세스, https://openaccess.thecvf.com/content/ICCV2023/papers/Wan_SOCS_Semantically-Aware_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_ICCV_2023_paper.pdf</li>
<li>DualPoseNet: Category-Level 6D Object Pose and Size Estimation …, 1월 31, 2026에 액세스, https://openaccess.thecvf.com/content/ICCV2021/papers/Lin_DualPoseNet_Category-Level_6D_Object_Pose_and_Size_Estimation_Using_Dual_ICCV_2021_paper.pdf</li>
<li>SSP-Pose: Symmetry-Aware Shape Prior Deformation for … - arXiv, 1월 31, 2026에 액세스, https://arxiv.org/pdf/2208.06661</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>