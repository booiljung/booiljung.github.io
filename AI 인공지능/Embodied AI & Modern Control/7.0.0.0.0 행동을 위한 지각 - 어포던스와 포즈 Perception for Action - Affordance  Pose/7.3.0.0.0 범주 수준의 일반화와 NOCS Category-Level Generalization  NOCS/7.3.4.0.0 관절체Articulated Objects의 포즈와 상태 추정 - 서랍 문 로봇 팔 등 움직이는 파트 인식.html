<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.3.4 관절체(Articulated Objects)의 포즈와 상태 추정: 서랍, 문, 로봇 팔 등 움직이는 파트 인식</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.3.4 관절체(Articulated Objects)의 포즈와 상태 추정: 서랍, 문, 로봇 팔 등 움직이는 파트 인식</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <a href="index.html">7.3 범주 수준의 일반화와 NOCS (Category-Level Generalization & NOCS)</a> / <span>7.3.4 관절체(Articulated Objects)의 포즈와 상태 추정: 서랍, 문, 로봇 팔 등 움직이는 파트 인식</span></nav>
                </div>
            </header>
            <article>
                <h1>7.3.4 관절체(Articulated Objects)의 포즈와 상태 추정: 서랍, 문, 로봇 팔 등 움직이는 파트 인식</h1>
<h2>1.  서론: 정적 세계를 넘어 동적 상호작용을 위한 인식</h2>
<p>컴퓨터 비전과 로보틱스 분야의 오랜 난제 중 하나는 정적인 환경에 대한 수동적인 관찰을 넘어, 물리적인 상호작용이 가능한 동적 객체를 어떻게 이해하고 표현할 것인가에 대한 문제였다. 우리가 일상생활에서 마주하는 대부분의 기능성 객체—가구, 가전제품, 공구, 그리고 로봇 그 자체—는 단일한 강체(Rigid Object)가 아니라, 여러 개의 강체 부품(Rigid Parts)이 관절(Joint)이라는 기구학적 구속 조건(Kinematic Constraints)으로 연결된 **관절체(Articulated Objects)**의 형태를 띤다. 서랍을 열거나, 노트북을 펼치거나, 가위질을 하는 행위는 모두 객체의 부품 간 상대적인 포즈 변화를 유발하며, 이러한 조작(Manipulation)을 성공적으로 수행하기 위해서는 객체의 현재 포즈뿐만 아니라 기구학적 구조와 관절의 상태(State)를 정확히 추정하는 능력이 필수적이다.</p>
<p>관절체 인식(Articulated Object Perception)은 강체 인식보다 훨씬 높은 차원의 복잡성을 내포한다. 강체의 경우 6자유도(6-DoF) 포즈, 즉 3차원 회전과 이동만을 추정하면 되지만, 관절체는 이를 구성하는 각 부품의 6-DoF 포즈는 물론, 부품 간을 연결하는 관절의 종류(회전, 이동 등), 관절의 축(Axis), 그리고 현재의 관절 각도나 이동 거리와 같은 상태 변수(Joint State)를 모두 추정해야 하기 때문이다. 또한, 특정 부품의 움직임은 연결된 다른 부품의 포즈에 영향을 미치므로, 이를 독립적인 강체들의 집합으로만 해석해서는 안 되며, <strong>운동학적 체인(Kinematic Chain)</strong> 또는 트리 구조 내에서 상호 의존성을 고려한 통합적 추론이 요구된다.</p>
<p>최근 딥러닝 기술의 발전은 이 분야에 혁신적인 방법론들을 제시하고 있다. 특히 사전에 정확한 3D CAD 모델이 주어지지 않은 상태에서도 범주(Category) 수준의 일반화된 특징을 학습하여 낯선 객체의 구조를 파악하는 <strong>카테고리 수준(Category-Level) 포즈 추정</strong> 기술이 비약적으로 발전하였다. 이는 로봇이 사전 정보가 없는 비정형 환경(Unstructured Environment)에서 “서랍장“이라는 일반적인 개념만으로 새로운 서랍장을 인식하고 조작할 수 있게 됨을 의미한다.</p>
<p>본 장에서는 관절체의 포즈와 상태 추정을 위한 최신 기술적 접근법들을 심도 있게 다룬다. 기하학적 정규화(Normalization)를 통해 형상의 다양성을 극복하는 ANCSH 프레임워크부터, 시계열 데이터의 연속성을 활용하는 CAPTRA 트래킹 시스템, 상호작용을 통해 디지털 트윈을 생성하는 Ditto, 그리고 불확실성을 확률적으로 모델링하는 최신 확산 모델(Diffusion Model) 기반의 Diff-Art까지, 기술의 진화 과정을 추적하고 각 방법론의 핵심 원리와 한계, 그리고 상호 보완적인 관계를 분석한다.</p>
<h2>2.  관절체의 기구학적 모델링과 이론적 배경</h2>
<p>관절체 인식을 기술적으로 논하기 위해서는 먼저 대상을 수학적으로 정의하고 모델링하는 체계가 필요하다. 로보틱스와 컴퓨터 비전에서 관절체는 위상학적 그래프 구조와 기하학적 변환의 결합으로 표현된다.</p>
<h3>2.1  운동학적 트리와 상태 공간 (Kinematic Tree and State Space)</h3>
<p>관절체 <span class="math math-inline">O</span>는 <span class="math math-inline">K</span>개의 강체 부품 <span class="math math-inline">{P_1, P_2, \dots, P_K}</span>와 이들을 연결하는 <span class="math math-inline">J</span>개의 관절 <span class="math math-inline">{J_1, J_2, \dots, J_J}</span>로 구성된 운동학적 트리(Kinematic Tree)로 모델링된다. 이 트리 구조에서 각 부품은 노드(Node)가 되며, 관절은 두 부품 간의 상대적인 운동을 허용하는 엣지(Edge) 역할을 한다. 일반적으로 하나의 ’베이스 부품(Base Part)’이 존재하며, 이는 전체 객체의 전역적 이동과 회전을 담당하고, 나머지 ’움직이는 부품(Mobile/Child Parts)’들은 관절을 통해 부모 부품에 종속된다.</p>
<p>각 부품 <span class="math math-inline">P_k</span>의 포즈는 특수 유클리드 군(Special Euclidean Group) <span class="math math-inline">SE(3)</span>에 속하는 변환 행렬 <span class="math math-inline">T_k =</span>로 표현된다. 여기서 <span class="math math-inline">R_k \in SO(3)</span>는 회전 행렬, <span class="math math-inline">t_k \in \mathbb{R}^3</span>는 이동 벡터이다. 그러나 관절체의 핵심은 이들 포즈 <span class="math math-inline">T_k</span>가 독립적이지 않다는 점이다. 부모 부품 <span class="math math-inline">P_{parent}</span>와 자식 부품 <span class="math math-inline">P_{child}</span>가 관절 <span class="math math-inline">J</span>로 연결되어 있다면, 자식 부품의 포즈는 부모 부품의 포즈와 관절의 상태 변수 <span class="math math-inline">\theta</span>에 의해 결정된다.<br />
<span class="math math-display">
T_{child} = T_{parent} \cdot T_{joint}(\theta)
</span><br />
여기서 <span class="math math-inline">T_{joint}(\theta)</span>는 관절의 종류와 파라미터에 따라 정의되는 로컬 변환 행렬이다.</p>
<h3>2.2  관절 유형과 파라미터화 (Joint Types and Parameterization)</h3>
<p>현실 세계의 대부분의 관절체는 다음과 같은 기본적인 관절 유형의 조합으로 설명된다.</p>
<ol>
<li><strong>회전 관절 (Revolute/Hinge Joint):</strong></li>
</ol>
<ul>
<li><strong>예시:</strong> 방문, 노트북 화면, 가위, 전자레인지 도어.</li>
<li><strong>자유도:</strong> 1-DoF.</li>
<li><strong>파라미터:</strong> 회전 축의 방향 벡터 <span class="math math-inline">u \in \mathbb{R}^3</span> (<span class="math math-inline">||u||=1</span>), 회전 축 상의 한 점(피벗) <span class="math math-inline">q \in \mathbb{R}^3</span>, 그리고 상태 변수인 회전 각도 <span class="math math-inline">\theta \in \mathbb{R}</span>.</li>
<li><strong>운동 방정식:</strong> 로드리게스 회전 공식(Rodrigues’ Rotation Formula) 등을 사용하여 축 <span class="math math-inline">u</span>를 중심으로 <span class="math math-inline">\theta</span>만큼 회전하는 변환을 기술한다.</li>
</ul>
<ol start="2">
<li><strong>이동 관절 (Prismatic/Slider Joint):</strong></li>
</ol>
<ul>
<li><strong>예시:</strong> 서랍, 미닫이문, 엘리베이터 도어.</li>
<li><strong>자유도:</strong> 1-DoF.</li>
<li><strong>파라미터:</strong> 이동 축의 방향 벡터 <span class="math math-inline">u \in \mathbb{R}^3</span>, 그리고 상태 변수인 이동 거리 <span class="math math-inline">d \in \mathbb{R}</span>.</li>
<li><strong>운동 방정식:</strong> 축 <span class="math math-inline">u</span> 방향으로 <span class="math math-inline">d</span>만큼의 선형 이동을 기술한다.</li>
</ul>
<ol start="3">
<li><strong>나사 관절 (Screw Joint):</strong></li>
</ol>
<ul>
<li><strong>예시:</strong> 병뚜껑, 립스틱, 회전 의자(높이 조절).</li>
<li><strong>특징:</strong> 회전과 이동이 결합된 형태이다. 피치(Pitch) 파라미터가 회전과 이동의 비율을 결정하거나, 두 자유도가 독립적일 수도 있다.</li>
</ul>
<p>이러한 기구학적 파라미터를 추정하는 것은 단순한 3D 형상 복원을 넘어, 로봇이 물리적으로 개입하여 객체의 상태를 변화시킬 수 있는 ’행동 가능성(Actionability)’을 확보하는 것을 의미한다.</p>
<h2>3.  카테고리 수준의 포즈 추정: 정규화 좌표계 접근법</h2>
<p>인스턴스 수준(Instance-Level)의 포즈 추정은 미리 스캔된 3D CAD 모델과 입력 이미지를 매칭하는 방식으로 이루어진다. 하지만, 세상에 존재하는 수많은 종류의 서랍장을 모두 미리 스캔해두는 것은 불가능하다. 이에 따라, 같은 범주(Category)에 속하는 객체들이 공유하는 구조적 공통점을 학습하여, 처음 보는 인스턴스에 대해서도 포즈를 추정하는 <strong>카테고리 수준 포즈 추정</strong>이 연구의 주류가 되었다.</p>
<h3>3.1  ANCSH: 계층적 정규화 좌표계의 도입</h3>
<p>**ANCSH (Articulation-aware Normalized Coordinate Space Hierarchy)**는 카테고리 수준 관절체 포즈 추정의 획기적인 전환점이 된 연구이다. 강체 포즈 추정을 위해 제안되었던 NOCS(Normalized Object Coordinate Space) 개념을 관절체로 확장하여, 부품 단위의 정규화와 전체 객체 단위의 정규화를 계층적으로 연결했다.</p>
<h4>3.1.1  NPCS와 NAOCS</h4>
<p>ANCSH는 두 가지 수준의 좌표계를 정의한다.</p>
<ul>
<li><strong>NPCS (Normalized Part Coordinate Space):</strong> 각 강체 부품(예: 서랍 하나, 몸체 하나)을 단위 큐브(Unit Cube) 내로 스케일링하고 정렬한 좌표계이다. 이는 각 부품의 고유한 형상 정보를 인코딩한다.</li>
<li><strong>NAOCS (Normalized Articulated Object Coordinate Space):</strong> 관절체 전체를 아우르는 정규화된 좌표계이다. 모든 부품이 ’정지 상태(Rest State, 관절 각도/변위가 0인 상태)’에 있을 때의 배치를 기준으로 정의된다.</li>
</ul>
<h4>3.1.2  네트워크 아키텍처 및 추론 과정</h4>
<p>ANCSH는 PointNet++ 기반의 딥러닝 네트워크를 사용하여 단일 깊이 영상(Depth Image)으로부터 다음 요소들을 예측한다.</p>
<ol>
<li><strong>부품 분할 (Segmentation):</strong> 입력 포인트 클라우드의 각 점이 어느 부품에 속하는지 분류한다.</li>
<li><strong>NPCS 좌표 회귀:</strong> 각 점이 해당 부품의 NPCS 상에서 어디에 위치하는지 예측한다. 이는 2D 픽셀을 3D 정준(Canonical) 좌표로 매핑하는 것과 같다.</li>
<li><strong>관절 파라미터 투표 (Voting):</strong> 각 점이 자신이 속한 부품과 연결된 관절의 파라미터(축 방향, 피벗 포인트 등)를 투표(Voting) 방식으로 예측한다.</li>
</ol>
<h4>3.1.3  최적화 및 운동학적 제약</h4>
<p>네트워크의 출력만으로는 노이즈로 인해 부품 간의 연결이 어긋날 수 있다(예: 문이 문틀에서 떨어져 보이는 현상). ANCSH는 이를 보정하기 위해 **유도된 관절 제약(Induced Joint Constraints)**을 사용한 최적화 단계를 수행한다. 예측된 관절 축을 기준으로 부품들이 물리적으로 타당하게 연결되도록 부품의 포즈와 스케일을 조정함으로써, 시각적으로나 물리적으로 일관된 결과를 도출한다.</p>
<h3>3.2  CAPTRA: 트래킹을 통한 연속성 확보</h3>
<p>정적 이미지에서의 추정은 매 프레임 독립적으로 수행되므로 시간적 일관성(Temporal Consistency)이 부족하고 떨림(Jitter) 현상이 발생할 수 있다. **CAPTRA (Category-level Pose Tracking for Rigid and Articulated Objects)**는 이를 해결하기 위해 시계열 트래킹 프레임워크를 제안했다.</p>
<h4>3.2.1  포즈 정규화 및 Delta Pose 예측</h4>
<p>CAPTRA의 핵심 아이디어는 **좌표계 정규화(Canonicalization)**를 트래킹 루프에 통합하는 것이다.</p>
<ul>
<li><strong>입력 정규화:</strong> 이전 프레임(<span class="math math-inline">t-1</span>)에서 추정된 포즈 <span class="math math-inline">T_{t-1}</span>를 사용하여 현재 프레임(<span class="math math-inline">t</span>)의 포인트 클라우드를 정준 좌표계로 역변환한다. 이렇게 하면 네트워크는 객체가 어디에 있든 항상 원점 근처에 정렬된 포인트 클라우드를 입력으로 받게 되어 학습 난이도가 대폭 낮아진다.</li>
<li><strong>Delta Pose 예측:</strong> 네트워크는 전체 포즈를 처음부터 예측하는 대신, 이전 프레임 대비 미세한 포즈 변화량(Residual Pose / Delta Pose) <span class="math math-inline">\Delta T</span>만을 예측한다. 이는 <span class="math math-inline">T_t = T_{t-1} \cdot \Delta T</span> 형태로 업데이트된다.</li>
</ul>
<h4>3.2.2  회전과 좌표의 분리 (Disentanglement)</h4>
<p>CAPTRA는 CoordinateNet과 RotationNet이라는 두 개의 전문화된 네트워크를 사용한다. CoordinateNet은 형상의 크기와 중심 위치를 추정하고, RotationNet은 회전 변화를 전담한다. 특히 관절체 트래킹에서는 부모 부품의 움직임이 자식 부품으로 전파되므로, 계층적 구조를 따라 순차적으로 트래킹을 수행하여 오차 누적을 방지한다. 실험 결과, CAPTRA는 9-DoF(6-DoF 포즈 + 3-DoF 스케일) 트래킹에서 실시간성(약 12 FPS)과 높은 정확도를 동시에 달성하였다.</p>
<h2>4.  생성형 모델과 확률적 접근의 부상 (2024-2025)</h2>
<p>2024년 이후의 연구 흐름은 결정론적(Deterministic) 회귀 모델의 한계를 극복하기 위해 확률적 생성 모델, 특히 **확산 모델(Diffusion Model)**과 <strong>다양체(Manifold)</strong> 이론을 도입하는 방향으로 전개되고 있다. 이는 대칭성으로 인한 모호함(Ambiguity)이나 심한 가려짐(Occlusion) 상황에서 더욱 강건한 성능을 보장한다.</p>
<h3>4.1  Diff-Art: 조건부 확산 모델 기반 추정</h3>
<p>**Diff-Art (2025)**는 관절체 포즈 추정 문제를 조건부 데이터 분포에서의 샘플링 문제로 재정의했다.</p>
<h4>4.1.1  포즈 모호성과 다중 가설 (Multi-Hypothesis)</h4>
<p>대칭적인 물체(예: 원형 손잡이, 대칭형 캐비닛)나 일부만 보이는 물체는 유일한 정답 포즈를 정의하기 어렵다. 기존 회귀 모델은 가능한 여러 정답의 평균을 출력하여 엉뚱한 결과를 내놓는 경우가 많았다. Diff-Art는 확산 모델의 특성을 활용하여, 관측 데이터(Observation)라는 조건 하에 가능한 여러 포즈 분포를 학습한다. 추론 시에는 노이즈로부터 시작하여 이 분포를 따르는 구체적인 포즈 샘플을 생성해낸다.</p>
<h4>4.1.2  운동학적 트리를 반영한 확산 과정</h4>
<p>단순히 강체 포즈를 생성하는 것을 넘어, Diff-Art는 관절체의 계층 구조를 확산 과정에 반영한다. 베이스 부품의 포즈를 먼저 생성하고, 이를 조건(Condition)으로 하여 자식 부품의 포즈와 관절 상태를 순차적으로 생성한다. 이러한 <strong>Joint-Centric Modeling</strong>은 생성된 결과물이 물리적 연결성을 위배하지 않도록 보장한다. 실험적으로 Diff-Art는 합성 데이터뿐만 아니라 실제 환경(Real-World) 데이터에서도 SOTA(State-of-the-Art) 성능을 달성하며, 특히 불확실성이 높은 상황에서 탁월한 강건성을 입증했다.</p>
<h3>4.2  PPF-Tracker: SE(3) 다양체와 리 대수</h3>
<p>**PPF-Tracker (2025)**는 딥러닝과 고전적인 기하학적 특징인 **PPF (Point Pair Features)**를 결합한 하이브리드 접근법이다.</p>
<h4>4.2.1  가중치 포인트 쌍 (Weighted PPF)</h4>
<p>모든 포인트 쌍을 동등하게 처리하는 기존 방식과 달리, PPF-Tracker는 표면 법선(Normal)의 각도 차이 등을 분석하여 기하학적으로 유의미한 정보(예: 모서리, 굴곡)를 담고 있는 포인트 쌍에 더 높은 가중치를 부여한다. 이는 데이터 효율성을 높이고 노이즈에 대한 저항력을 키운다.</p>
<h4>4.2.2  Lie Algebra (<span class="math math-inline">\mathfrak{se}(3)</span>) 최적화</h4>
<p>가장 큰 이론적 기여는 포즈 추적을 유클리드 공간이 아닌 **SE(3) 리 군(Lie Group)**의 접공간인 <strong>리 대수(Lie Algebra, <span class="math math-inline">\mathfrak{se}(3)</span>)</strong> 상에서 수행한다는 점이다. 3차원 회전과 이동을 선형 공간인 리 대수로 매핑하여 최적화를 수행함으로써, 회전 행렬의 직교성(Orthogonality) 제약을 자연스럽게 만족시키고 수렴 속도와 정확도를 비약적으로 향상시켰다. 비교 실험에서 PPF-Tracker는 ANCSH나 ContactArt 대비 회전 오차를 크게 줄이며 실시간 트래킹 성능을 보여주었다.</p>
<h2>5.  상호작용과 디지털 트윈: 능동적 인식으로의 전환</h2>
<p>관절체 인식은 단순히 ’보는 것’에서 멈추지 않고, 로봇이 환경과 상호작용하며 정보를 획득하고 검증하는 **능동적 인식(Active Perception)**의 단계로 나아가고 있다.</p>
<h3>5.1  Ditto: 상호작용을 통한 디지털 트윈 생성</h3>
<p>**Ditto (Building Digital Twins of Articulated Objects from Interaction)**는 로봇이 객체를 조작하는 전후의 관측 데이터를 비교하여 객체의 3D 형상과 운동학적 모델을 동시에 복원하는 혁신적인 방법론이다.</p>
<h4>5.1.1  입력 및 처리 구조</h4>
<p>Ditto는 상호작용 전(<span class="math math-inline">T_{before}</span>)과 후(<span class="math math-inline">T_{after}</span>)의 포인트 클라우드 쌍을 입력으로 받는다. 이 두 시점의 데이터를 비교 분석하여, 공간상에서 위치가 변한 부분(Mobile Part)과 변하지 않은 부분(Static Part)을 식별한다.</p>
<h4>5.1.2  암시적 신경 표현 (Implicit Neural Representation)</h4>
<p>Ditto는 복잡한 형상(예: 서랍 내부의 빈 공간, 얇은 문)을 표현하기 위해, 3D 공간의 점유 확률(Occupancy)을 예측하는 암시적 신경망(Implicit Neural Network)을 사용한다. 이는 해상도 제한이 있는 복셀(Voxel)이나 위상 제약이 있는 메쉬(Mesh)보다 정교한 형상 복원이 가능하다. 결과적으로 Ditto는 시뮬레이션 환경에 즉시 로딩하여 물리 시뮬레이션을 돌릴 수 있는 수준의 **디지털 트윈(Digital Twin)**을 생성해낸다.</p>
<h3>5.2  ContactArt: 조작을 위한 접촉 프라이어</h3>
<p><strong>ContactArt</strong>는 관절체를 인식할 때, 사람이나 로봇 손이 어디에 닿아야 하는지(Contact Map)를 함께 학습한다.</p>
<ul>
<li>단순히 “이것은 서랍이다“라고 인식하는 것을 넘어, “이 서랍을 열기 위해서는 손잡이의 이 부분을 잡아야 한다“는 <strong>상호작용 유도성(Affordance)</strong> 정보를 제공한다.</li>
<li>대규모 손-객체 상호작용 데이터셋을 통해 학습된 이 모델은, 로봇이 관절체를 조작할 때 발생할 수 있는 물리적 충돌이나 파지 실패를 줄이는 데 결정적인 역할을 한다.</li>
</ul>
<h2>6.  데이터셋 생태계와 시뮬레이션</h2>
<p>딥러닝 모델의 성능은 데이터의 질과 양에 비례한다. 관절체 연구를 위한 데이터셋과 시뮬레이터는 단순한 형상 라이브러리에서 물리 기반의 상호작용 환경으로 진화하고 있다.</p>
<h3>6.1  SAPIEN과 PartNet-Mobility</h3>
<p><strong>SAPIEN</strong>은 관절체 조작 연구를 위해 특화된 물리 시뮬레이터로, PhysX 엔진을 기반으로 하여 정확한 강체 역학, 관절 구속, 마찰 등을 시뮬레이션한다. 이와 함께 제공되는 <strong>PartNet-Mobility</strong> 데이터셋은 2,346개의 관절체 모델에 대해 부품 분할, 관절 축, 운동 범위 등이 상세히 어노테이션되어 있어, 현재까지도 관절체 연구의 표준 벤치마크(Standard Benchmark)로 활용되고 있다.</p>
<h3>6.2  차세대 데이터셋: PartNeXt와 GAPartNet</h3>
<p>최근 연구들은 기존 데이터셋의 한계—텍스처 부족, 단순한 구조—를 극복하기 위해 더욱 정교한 데이터셋을 구축하고 있다.</p>
<h4>6.2.1  GAPartNet: 일반화 가능하고 조작 가능한 부품</h4>
<p>**GAPartNet (2023)**은 **“GAPart (Generalizable Actionable Part)”**라는 개념을 도입했다.</p>
<ul>
<li><strong>개념:</strong> ‘손잡이(Handle)’, ‘버튼(Button)’, ’노브(Knob)’와 같은 부품들은 그것이 어떤 물체에 달려 있든 유사한 조작 방식(Actionability)을 공유한다.</li>
<li><strong>목적:</strong> 객체의 전체 카테고리(예: 냉장고 vs 세탁기)가 달라도, GAPart 단위의 학습을 통해 조작 기술을 전이(Transfer)시킬 수 있도록 돕는다. 이는 도메인 일반화(Domain Generalization) 성능을 극대화한다.</li>
</ul>
<h4>6.2.2  PartNeXt: 고품질 텍스처와 미세 계층 구조</h4>
<p>**PartNeXt (2025)**는 기존 PartNet의 텍스처 부재와 낮은 메쉬 품질 문제를 해결한 차세대 데이터셋이다.</p>
<ul>
<li><strong>특징:</strong> 23,519개의 고해상도 텍스처 모델과 35만 개 이상의 부품 어노테이션을 포함한다.</li>
<li><strong>영향:</strong> 시각적 텍스처 정보가 풍부해짐에 따라, 기하학적 정보(Depth)뿐만 아니라 RGB 정보를 적극적으로 활용하는 비전 모델(VLM 등)의 학습이 가능해졌으며, 이는 Sim-to-Real 성능 향상으로 직결된다.</li>
</ul>
<table><thead><tr><th><strong>데이터셋</strong></th><th><strong>주요 특징</strong></th><th><strong>데이터 규모</strong></th><th><strong>핵심 용도</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>PartNet-Mobility</strong></td><td>물리 속성(질량, 마찰), 운동 범위 포함</td><td>2K+ 모델</td><td>강화학습, 기초 인식</td><td>SAPIEN 기반 표준</td></tr>
<tr><td><strong>GAPartNet</strong></td><td>Actionable Part 정의, 도메인 일반화</td><td>1K+ 객체, 8K+ 부품</td><td>로봇 조작, 기능성 학습</td><td>‘부품’ 중심 접근</td></tr>
<tr><td><strong>PartNeXt</strong></td><td>고품질 텍스처, 미세 계층 구조</td><td>23K+ 모델</td><td>RGB 기반 인식, VLM</td><td>PartNet의 진화형</td></tr>
<tr><td><strong>ContactArt</strong></td><td>손-객체 상호작용, 접촉 맵</td><td>상호작용 데이터</td><td>파지(Grasping), 접촉 계획</td><td>상호작용 프라이어</td></tr>
</tbody></table>
<h2>7.  Sim-to-Real: 가상에서 현실로의 전이</h2>
<p>시뮬레이션에서 학습된 모델을 실제 로봇에 적용할 때 발생하는 <strong>Sim-to-Real Gap</strong>은 여전히 큰 도전 과제이다.</p>
<h3>7.1  도메인 적응 (Domain Adaptation) 및 랜덤화</h3>
<p>현실의 센서 데이터는 노이즈가 심하고, 물체의 재질(투명, 반사)에 따라 깊이 정보가 소실되기도 한다. 이를 극복하기 위해 연구자들은 시뮬레이션에서 도메인 랜덤화(Domain Randomization)를 수행한다. Diff-Art나 PPF-Tracker와 같은 최신 모델들은 훈련 시에 인위적인 노이즈, 가려짐, 시점 변화를 주입하여 현실 데이터에 대한 강건성을 확보한다.</p>
<h3>7.2  능동적 감지 (Active Sensing)</h3>
<p>단일 시점의 관측만으로는 구조를 완벽히 파악하기 어렵다. 로봇 팔에 부착된 카메라(Eye-in-Hand)를 이동시키며 여러 각도에서 객체를 관찰하거나, 직접 살짝 건드려보는(Interaction) 행위를 통해 불확실성을 줄이는 능동적 감지 기술이 필수적으로 통합되고 있다.</p>
<h2>8.  결론 및 향후 전망</h2>
<p>관절체 포즈 및 상태 추정 기술은 정적인 3D 객체 인식을 넘어, 로봇이 세상과 물리적으로 소통하기 위한 **동적 이해(Dynamic Understanding)**의 핵심 기술로 자리 잡았다.</p>
<ol>
<li><strong>기술의 고도화:</strong> ANCSH로 시작된 카테고리 수준 인식은 CAPTRA의 실시간 트래킹, Diff-Art의 생성형 모델링, PPF-Tracker의 수학적 최적화를 거치며 비약적으로 발전했다. 이제는 단순한 형상 인식을 넘어 운동학적 제약과 불확실성을 정교하게 다루는 단계에 진입했다.</li>
<li><strong>데이터의 질적 전환:</strong> PartNet-Mobility에서 PartNeXt와 GAPartNet으로 이어지는 데이터셋의 진화는 ’단순 형상’에서 ’텍스처와 기능성(Actionability)’으로의 중심 이동을 보여준다. 이는 시각-언어 모델(VLM)과 같은 거대 모델(Foundation Model)이 관절체 인식에 개입할 수 있는 길을 열어주고 있다.</li>
<li><strong>능동적 상호작용:</strong> Ditto와 ContactArt의 사례는 인식이 수동적인 관찰이 아니라 능동적인 과정임을 시사한다. 미래의 로봇은 “보이는 대로 믿는” 것이 아니라, “상호작용하며 확신하는” 방식으로 관절체를 이해하게 될 것이다.</li>
</ol>
<p>이러한 기술적 진보는 가사 도우미 로봇이 낯선 주방 가구를 자유자재로 다루거나, 산업용 로봇이 비정형 부품을 조립하고, 재난 현장에서 로봇이 잔해(관절체 형태의)를 치우는 등 복잡하고 고도화된 태스크를 수행하는 데 필수적인 기반이 될 것이다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>
<p>Category-Level Articulated Object Pose Estimation - Guibas Lab, https://geometry.stanford.edu/lgl_2024/papers/lwygas-claope-20/lwygas-claope-20.pdf</p>
</li>
<li>
<p>Survey on Modeling of Articulated Objects - arXiv, https://arxiv.org/html/2403.14937v1</p>
</li>
<li>
<p>R2-Art: Category-Level Articulation Pose Estimation from Single …, https://hfut-liuliu.com/assets/pdf/R2Art.pdf</p>
</li>
<li>
<p>Category-level Articulation Estimation from a Single Point Cloud …, https://www.cvl.iis.u-tokyo.ac.jp/data/uploads/papers/Fu_ArticulationEstimation_ICRA2024.pdf</p>
</li>
<li>
<p>CAtegory-level Pose Tracking for Rigid and Articulated Objects from …, https://ieeexplore.ieee.org/iel7/9709627/9709628/09711093.pdf</p>
</li>
<li>
<p>CAtegory-level Pose Tracking for Rigid and Articulated Objects from …, https://geometry.stanford.edu/lgl_2024/papers/wwzqdfcsg-ccptraopc-21/wwzqdfcsg-ccptraopc-21.pdf</p>
</li>
<li>
<p>Exploring Category-level Articulated Object Pose Tracking on SE(3 …, https://arxiv.org/html/2511.05996v1</p>
</li>
<li>
<p>KineDiff3D: Kinematic-Aware Diffusion for Category-Level … - arXiv, https://arxiv.org/html/2510.17137v1</p>
</li>
<li>
<p>SPLART: Articulation Estimation and Part-Level Reconstruction with …, https://ttic.edu/ripl/assets/publications/lin25.pdf</p>
</li>
<li>
<p>SAPIEN: A SimulAted Part-Based Interactive ENvironment, https://openaccess.thecvf.com/content_CVPR_2020/papers/Xiang_SAPIEN_A_SimulAted_Part-Based_Interactive_ENvironment_CVPR_2020_paper.pdf</p>
</li>
<li>
<p>GAPartNet: Cross-Category Domain-Generalizable Object …, https://openaccess.thecvf.com/content/CVPR2023/papers/Geng_GAPartNet_Cross-Category_Domain-Generalizable_Object_Perception_and_Manipulation_via_Generalizable_and_CVPR_2023_paper.pdf</p>
</li>
<li>
<p>A Multi-Step CNN-Based Estimation of Aircraft Landing Gear Angles, https://www.mdpi.com/1424-8220/21/24/8440</p>
</li>
<li>
<p>CAtegory-level Pose Tracking for Rigid and Articulated Objects from …, https://arxiv.org/abs/2104.03437</p>
</li>
<li>
<p>Diff-Art: Category-level Articulation Pose Estimation via Conditional …, https://www.computer.org/csdl/proceedings-article/icme/2025/11209596/2beC4MA4Ilq</p>
</li>
<li>
<p>Ditto: Building Digital Twins of Articulated Objects from Interaction, https://ut-austin-rpl.github.io/Ditto/</p>
</li>
<li>
<p>Ditto: Building Digital Twins of Articulated Objects from Interaction, https://ieeexplore.ieee.org/iel7/9878378/9878366/09879272.pdf</p>
</li>
<li>
<p>Ditto: Building Digital Twins of Articulated Objects From Interaction, https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_Ditto_Building_Digital_Twins_of_Articulated_Objects_From_Interaction_CVPR_2022_paper.pdf</p>
</li>
<li>
<p>Learning 3D Interaction Priors for Category-level Articulated Object …, https://arxiv.org/html/2305.01618v2</p>
</li>
<li>
<p>Natural Language Guided 3D Hand-Object Contact Modeling … - arXiv, https://arxiv.org/html/2407.12727v1</p>
</li>
<li>
<p>PartNet-Mobility - Dataset - LDM, https://service.tib.eu/ldmservice/dataset/partnet-mobility</p>
</li>
<li>
<p>Daily Papers - Hugging Face, <a href="https://huggingface.co/papers?q=depth-aware+part+association">https://huggingface.co/papers?q=depth-aware%20part%20association</a></p>
</li>
<li>
<p>PartNeXt: A Next-Generation Dataset for Fine-Grained and…, <a href="https://openreview.net/forum?id=J0PmRFMZXm&amp;referrer=%5Bthe+profile+of+Lan+Xu%5D(/profile?id%3D~Lan_Xu2)">https://openreview.net/forum?id=J0PmRFMZXm&amp;referrer=%5Bthe%20profile%20of%20Lan%20Xu%5D(%2Fprofile%3Fid%3D~Lan_Xu2)</a></p>
</li>
<li>
<p>A Next-Generation Dataset for Fine-Grained and Hierarchical 3D …, https://www.alphaxiv.org/overview/2510.20155v1</p>
</li>
<li>
<p>ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via …, https://openaccess.thecvf.com/content/CVPR2025/papers/Li_ManipTrans_Efficient_Dexterous_Bimanual_Manipulation_Transfer_via_Residual_Learning_CVPR_2025_paper.pdf</p>
</li>
<li>
<p>Sim-to-Real RL for Generalizable Articulated Object Manipulation …, https://arxiv.org/html/2502.14457v1</p>
</li>
<li>
<p>Ditto in the House: Building Articulation Models of Indoor Scenes …, https://ieeexplore.ieee.org/iel7/10160211/10160212/10161431.pdf</p>
</li>
<li></li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>