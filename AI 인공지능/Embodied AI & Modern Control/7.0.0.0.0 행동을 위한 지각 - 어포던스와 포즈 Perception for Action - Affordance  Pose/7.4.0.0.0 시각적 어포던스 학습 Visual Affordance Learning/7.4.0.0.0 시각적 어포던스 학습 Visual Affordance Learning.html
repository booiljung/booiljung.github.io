<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.4 시각적 어포던스 학습 (Visual Affordance Learning)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.4 시각적 어포던스 학습 (Visual Affordance Learning)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <a href="index.html">7.4 시각적 어포던스 학습 (Visual Affordance Learning)</a> / <span>7.4 시각적 어포던스 학습 (Visual Affordance Learning)</span></nav>
                </div>
            </header>
            <article>
                <h1>7.4 시각적 어포던스 학습 (Visual Affordance Learning)</h1>
<h2>1.  서론: 행동 가능성의 시각적 접지 (Visual Grounding of Action Possibilities)</h2>
<p>로봇 공학(Robotics)과 구체화된 인공지능(Embodied AI)의 융합 영역에서, 시각적 인식(Perception)은 단순히 환경을 묘사하는 수동적인 과정이 아니라 행동을 유발하고 안내하는 능동적인 기제로 재정의된다. 제임스 깁슨(James J. Gibson)이 1979년 그의 저서 <em>The Ecological Approach to Visual Perception</em>에서 제창한 <strong>어포던스(Affordance)</strong> 개념은 이러한 패러다임 전환의 핵심에 있다. 깁슨은 어포던스를 “환경이 동물에게 제공하는 행동의 기회(Action possibilities provided by the environment)“로 정의했다. 이는 물체의 물리적 속성(질감, 형상, 무게)이 관찰자의 행동 능력(Action Capabilities)과 결합하여 창출되는 관계적 속성이다.</p>
<p>전통적인 컴퓨터 비전 시스템이 “이 물체는 컵이다(This is a cup)“라는 의미론적 범주화(Semantic Categorization)에 집중했다면, 로봇을 위한 시각적 어포던스 학습은 “이 컵의 손잡이를 잡을 수 있다(Graspable)”, “이 컵에 액체를 담을 수 있다(Containable)”, “이 컵을 밀어서 치울 수 있다(Pushable)“와 같은 기능적 속성을 추론하는 것을 목표로 한다. 신경과학적 관점에서 이는 뇌의 복측 경로(Ventral Stream, ‘What’ pathway)와 배측 경로(Dorsal Stream, ‘Where/How’ pathway)의 차이로 설명될 수 있다. 어포던스 학습은 주로 배측 경로의 기능, 즉 물체의 정체성보다는 물체와 상호작용하기 위한 공간적 위치와 조작 방식을 처리하는 시각-운동 매핑(Visuomotor Mapping) 과정을 모델링하는 것이다.</p>
<p>최근 로봇 조작(Robotic Manipulation) 분야에서 어포던스 학습은 딥러닝 기술의 발전과 함께 비약적인 성장을 이루었다. 초기 연구가 2D 이미지 내에서의 픽셀 단위 레이블링(Pixel-wise Labeling)에 머물렀다면, 최신 SOTA(State-of-the-Art) 기술들은 3D 공간 정보, 시간적 동역학(Temporal Dynamics), 그리고 대규모 언어 모델(LLM) 및 생성형 모델(Generative Models)의 지식을 통합하여 더욱 정교하고 일반화된 행동 가능성을 추론하고 있다. 특히 2024년과 2025년에는 확산 모델(Diffusion Models)과 플로우 매칭(Flow Matching)과 같은 생성형 접근법이 도입되면서, 데이터 분포를 단순히 분류하는 것을 넘어 유효한 상호작용 궤적을 생성(Generate)하는 방향으로 기술이 진화하고 있다.</p>
<p>본 장에서는 시각적 어포던스 학습의 기술적 진화를 심층적으로 분석한다. 먼저 2D 이미지 기반의 감독 학습(Supervised Learning) 모델인 AffordanceNet 등을 통해 어포던스 분할의 기초를 다지고, 인간의 행동 비디오를 활용한 약지도 학습(Weakly Supervised Learning) 및 상호작용 핫스팟(Interaction Hotspots) 기술을 살펴본다. 이어 시뮬레이션 환경에서의 능동적 상호작용을 통한 3D 어포던스 학습(Where2Act 등)과 최신 생성형 AI 기반의 개방형 어휘(Open-Vocabulary) 어포던스 추론 기술인 DAG 및 플로우 매칭 정책을 상세히 기술한다.</p>
<h2>2.  2D 어포던스 분할과 픽셀 수준의 기능적 이해</h2>
<p>딥러닝 초기 단계에서 시각적 어포던스 학습은 의미론적 분할(Semantic Segmentation) 문제의 확장으로 접근되었다. 이는 입력된 이미지의 각 픽셀에 대해 해당 위치가 제공하는 행동적 기능을 분류하는 과업이다. 물체 전체를 하나의 클래스로 인식하는 객체 검출(Object Detection)과 달리, 어포던스 분할은 물체의 부분(Part) 수준에서 기능을 이해해야 하므로 더욱 세밀한 특징 추출 능력이 요구된다.</p>
<h3>2.1  AffordanceNet: 객체 검출과 어포던스의 동시 추론</h3>
<p>이 분야의 선구적인 연구인 <strong>AffordanceNet</strong> 은 객체 검출과 어포던스 분할을 하나의 엔드투엔드(End-to-End) 네트워크로 통합한 아키텍처를 제안했다. 기존 방법론들이 객체 검출 후 별도의 후처리 과정을 통해 어포던스를 추론하거나 CRF(Conditional Random Field)와 같은 확률 모델을 추가적으로 사용했던 것과 달리, AffordanceNet은 150ms의 빠른 추론 속도로 실시간 로봇 애플리케이션에 적용 가능한 성능을 입증했다.</p>
<p><strong>아키텍처 및 메커니즘</strong></p>
<p>AffordanceNet의 구조는 크게 두 가지 핵심 브랜치로 구성된다.</p>
<ol>
<li><strong>객체 검출 브랜치(Object Detection Branch)</strong>: VGG-16이나 ResNet과 같은 심층 합성곱 신경망(CNN)을 백본으로 사용하여 이미지 내의 관심 영역(RoI)을 추출하고, 객체의 범주(Category)와 바운딩 박스(Bounding Box)를 예측한다. 이는 Faster R-CNN의 RPN(Region Proposal Network) 구조를 차용하여 다양한 크기와 비율의 물체를 강건하게 검출한다.</li>
<li><strong>어포던스 분할 브랜치(Affordance Detection Branch)</strong>: 검출된 객체의 RoI 내부에서 각 픽셀의 어포던스 레이블을 예측한다. 이 과정에서 발생하는 핵심적인 기술적 난제는 RoI 풀링(Pooling) 과정에서 손실되는 공간 해상도를 어떻게 복원하느냐는 것이다. AffordanceNet은 이를 해결하기 위해 <strong>역합성곱(Deconvolutional) 레이어</strong> 시퀀스를 도입했다. 이를 통해 저해상도 특징 맵을 다시 고해상도로 업샘플링(Upsampling)하여, 컵의 얇은 손잡이나 병의 입구와 같은 미세한 구조의 어포던스를 정밀하게 분할한다.</li>
</ol>
<p><strong>다중 작업 손실 함수 (Multi-task Loss Function)</strong></p>
<p>학습의 안정성과 성능 극대화를 위해 AffordanceNet은 다중 작업 손실 함수를 사용한다. 전체 손실 함수 <span class="math math-inline">L</span>은 다음과 같이 정의된다.<br />
<span class="math math-display">
L = L_{cls} + L_{loc} + L_{mask}
</span><br />
여기서 <span class="math math-inline">L_{cls}</span>는 객체 범주 분류를 위한 교차 엔트로피 손실, <span class="math math-inline">L_{loc}</span>는 바운딩 박스 회귀를 위한 Smooth L1 손실, 그리고 <span class="math math-inline">L_{mask}</span>는 픽셀 단위 어포던스 분류를 위한 다중 클래스 교차 엔트로피 손실이다. 이러한 다중 작업 학습 전략은 모델이 물체의 전역적인 형태 정보(Global Context)와 국소적인 기능 정보(Local Context)를 상호 보완적으로 학습하도록 유도한다.</p>
<p><strong>실험 결과 및 한계</strong> IIT-AFF 데이터셋을 이용한 실험에서 AffordanceNet은 기존의 SOTA 방법론들을 큰 차이로 능가하는 성능을 보였다. 그러나 2D 이미지 기반 접근법은 근본적인 한계를 지닌다. 첫째, 3D 공간 정보의 부재로 인해 로봇 팔의 6자유도(6DoF) 접근 경로를 계획하는 데 필요한 깊이(Depth) 및 법선 벡터(Normal Vector) 정보를 직접적으로 제공하지 못한다. 둘째, ’담다(Contain)’와 같이 물체 내부 공간에 작용하는 어포던스는 특정 시점(Viewpoint)에서는 관측되지 않을 수 있어(Self-occlusion), 단일 시점 2D 이미지만으로는 추론이 불가능하거나 부정확할 수 있다.</p>
<h3>2.2  데이터셋의 진화: IIT-AFF에서 3D AffordanceNet으로</h3>
<p>데이터셋은 어포던스 학습 모델의 성능과 일반화 능력을 결정짓는 핵심 요소다. <strong>IIT-AFF</strong> 데이터셋은 초기 연구를 이끌었으나, 정적인 이미지와 제한된 배경 복잡도로 인해 실제 로봇 환경의 동적 특성을 반영하기 어려웠다. 이를 극복하기 위해 등장한 <strong>3D AffordanceNet</strong> 은 23,000개의 3D 형상(Shape)에 대해 18가지 시각적 어포던스 카테고리(예: Grasp, Lift, Lay, Sit, Support 등)를 주석화했다.</p>
<p>3D AffordanceNet은 포인트 클라우드(Point Cloud) 데이터를 기반으로 하여, 시점에 구애받지 않는 전방향(Omni-directional) 어포던스 학습을 가능하게 했다. 이는 PointNet++나 DGCNN과 같은 3D 딥러닝 아키텍처가 어포던스 학습에 본격적으로 도입되는 계기가 되었으며, 2D 픽셀 인식에서 3D 기하학적 이해로 연구의 흐름을 전환하는 데 기여했다. 특히 벤치마크 실험 결과, ’Contain’과 같이 오목한(Concave) 형상에 의존하는 어포던스는 회전 불변성(Rotation Invariance)을 확보하기 어려워 여전히 도전적인 과제임이 밝혀졌다.</p>
<h2>3.  비디오 기반 학습: 수동적 관찰에서 미래 행동 예측으로</h2>
<p>정적인 이미지 데이터셋은 레이블링 비용이 높고, 실제 상호작용의 동역학을 포착하지 못한다는 단점이 있다. 이에 대한 대안으로, 인간이 도구를 사용하는 비디오(Human Demonstration Video)를 관찰하여 어포던스를 학습하는 방법론이 대두되었다. 이는 “사람이 이 물체를 어떻게 다루는가?“를 모방함으로써 명시적인 레이블 없이도 기능적 영역을 학습하는 약지도 학습(Weakly Supervised Learning) 접근법이다.</p>
<h3>3.1  상호작용 핫스팟 (Interaction Hotspots)</h3>
<p><strong>Interaction Hotspots</strong>  연구는 비디오에서 관찰된 인간의 행동을 통해 물체의 “상호작용 가능한 영역“을 히트맵(Heatmap) 형태로 예측한다. 이 접근법의 핵심은 어포던스를 물체의 고정된 속성이 아니라, **미래 행동의 예측(Anticipation)**으로 해석한다는 점이다.</p>
<p><strong>작동 메커니즘</strong></p>
<ol>
<li><strong>약지도 학습 신호 생성</strong>: 비디오 프레임에서 인간의 손(Hand)과 물체(Object)가 접촉하는 시점의 손 위치를 추출한다. 이 위치 정보는 해당 물체의 “기능적 핫스팟“을 나타내는 정답(Ground Truth)으로 활용된다.</li>
<li><strong>소프트 어텐션(Soft-Attention) 모듈</strong>: 이미지 내에서 특정 영역에 가중치를 부여하는 어텐션 메커니즘을 사용하여, 물체 전체가 아닌 상호작용이 집중되는 국소 영역(Hotspot)을 자동으로 학습한다.</li>
<li><strong>예측 및 전이</strong>: 학습된 모델은 새로운 정적 이미지가 주어졌을 때, 해당 물체가 현재 사용되고 있지 않더라도(Passive State), 미래에 상호작용이 일어날 가능성이 높은 위치를 공간적 확률 분포(Spatial Probability Distribution)로 출력한다.</li>
</ol>
<p>이 기술은 OPRA(Object Part Affordance) 데이터셋이나 EPIC-KITCHENS와 같은 1인칭 시점(Egocentric) 비디오 데이터셋을 통해 검증되었다. 실험 결과, 핫스팟 예측 모델은 명시적인 부품 레이블(Part Label) 없이도 “칼의 손잡이“나 “전기 스위치“와 같은 기능적 부위를 정확히 찾아낼 수 있었다. 또한, 학습 과정에서 보지 못한 새로운 범주의 물체(Novel Object Categories)에 대해서도 시각적 유사성을 바탕으로 합리적인 핫스팟을 제안하는 일반화 능력을 보였다. 이는 로봇이 낯선 환경에서도 인간의 행동 패턴을 기반으로 탐색 전략을 수립할 수 있음을 시사한다.</p>
<h3>3.2  VRB: Vision-Robotics Bridge</h3>
<p><strong>Vision-Robotics Bridge (VRB)</strong> 는 인간 중심의 비디오 데이터(Ego4D, EPIC-KITCHENS)를 로봇의 물리적 제어(Control)로 직접 연결하는 가교 역할을 수행하는 프레임워크다. 기존의 핫스팟 연구가 “어디를 만질 것인가?“에 집중했다면, VRB는 “접촉 후 어떻게 움직일 것인가?“까지 확장하여 <strong>실행 가능한(Actionable)</strong> 정보를 추출한다.</p>
<p><strong>인간-무관(Human-Agnostic) 표현 학습</strong> VRB의 가장 큰 특징은 인간의 손이 등장하는 비디오에서 학습하지만, 추론 시에는 로봇이 마주할 정적인 장면(손이 없는 상태)에서 어포던스를 예측한다는 점이다. 이를 위해 VRB는 인간의 손을 인페인팅(Inpainting)하거나 마스킹하는 대신, 대규모 데이터셋으로부터 **상호작용 발생 전의 장면(Pre-interaction Frame)**과 <strong>상호작용 후의 변화(Post-interaction Dynamics)</strong> 사이의 관계를 학습한다.</p>
<p><strong>모델 출력 및 로봇 적용</strong></p>
<p>VRB 모델은 입력 이미지에 대해 다음과 같은 정보를 출력한다.</p>
<ol>
<li><strong>접촉 히트맵(Contact Heatmap)</strong>: 물체 표면에서 파지(Grasp)하거나 접촉해야 할 지점의 확률 분포.</li>
<li><strong>궤적 트랜스포머(Trajectory Transformer)</strong>: 접촉 시점 이후 손목(Wrist)이 이동해야 할 3D 웨이포인트(Waypoints) 시퀀스.</li>
</ol>
<p>이러한 출력값은 로봇의 모션 플래닝(Motion Planning) 알고리즘의 초기 궤적(Seed Trajectory)으로 직접 사용될 수 있다. Franka Emika Panda 로봇을 이용한 실세계 실험에서, VRB는 ‘서랍 열기’, ‘물건 집기’, ‘문 열기’ 등 10가지 이상의 다양한 작업에서 높은 성공률을 보였다. 특히 인터넷상의 야생(In-the-wild) 비디오를 학습 데이터로 사용함으로써, 시뮬레이션 환경에 국한된 기존 방법론보다 조명 변화나 배경 복잡도에 훨씬 강건한 성능을 입증했다. 이는 로봇이 인터넷 규모의 데이터를 통해 상식적인 물리 지식(Common Sense Physics)을 습득할 수 있음을 보여주는 중요한 사례다.</p>
<h2>4.  능동적 지각과 3D 관절 객체 조작 (Active Perception &amp; 3D Articulated Objects)</h2>
<p>로봇이 단순히 물체를 집어 올리는(Pick-and-Place) 것을 넘어, 서랍을 열거나 노트북을 펼치고, 레버를 당기는 등의 복잡한 작업을 수행하기 위해서는 물체의 **관절 구조(Articulation Structure)**와 **운동학적 제약(Kinematic Constraints)**을 이해해야 한다. 이러한 작업에서는 2D 이미지의 픽셀 정보만으로는 불충분하며, 3D 공간상에서의 정확한 힘의 방향과 회전축에 대한 추론이 필수적이다.</p>
<h3>4.1  Where2Act: 픽셀에서 물리적 행동으로</h3>
<p><strong>Where2Act</strong> 는 3D 관절 객체에 대해 <strong>“어디서(Where)”</strong> 상호작용하고 <strong>“어떻게(How)”</strong> 힘을 가해야 하는지를 통합적으로 학습하는 프레임워크를 제안했다. 이 연구는 어포던스를 단순히 의미론적 영역이 아닌, **행동 가능성(Actionability)**의 관점에서 정량화한다.</p>
<p><strong>시스템 아키텍처 및 학습 파이프라인</strong></p>
<p>Where2Act는 3D 포인트 클라우드 또는 깊이 이미지를 입력으로 받아 다음과 같은 정보를 예측한다.</p>
<ol>
<li><strong>행동 가능성 점수(Actionability Score)</strong>: 각 3D 포인트가 특정 기본 동작(Primitive Action, 예: 밀기 Pushing, 당기기 Pulling)에 대해 유효한 상호작용 지점인지를 나타내는  사이의 확률값.</li>
<li><strong>행동 제안(Action Proposal)</strong>: 유효한 포인트에 대해 구체적인 조작 파라미터, 즉 그리퍼의 접근 방향(Orientation)과 힘을 가해야 할 벡터를 회귀(Regression)한다.</li>
</ol>
<p>학습 데이터 수집을 위해 현실 세계에서 모든 관절 물체를 조작해보는 것은 비용적으로 불가능하다. Where2Act는 이를 극복하기 위해 <strong>SAPIEN</strong>과 같은 물리 시뮬레이터 내에서 대규모의 상호작용 데이터를 생성하는 <strong>Sim-to-Real</strong> 전략을 채택했다. 시뮬레이션 상에서 로봇은 수천 개의 다양한 관절 객체(캐비닛, 서랍, 창문 등)와 무작위로 상호작용(Online Exploration)하며, 어떤 지점을 어떤 방향으로 당겼을 때 물체의 상태 변화(State Change)가 발생하는지를 기록한다.</p>
<p><strong>기술적 성과 및 비교 우위</strong> Where2Act는 비디오 기반의 VRB 방식이 2D 이미지 평면에 투영된 궤적을 다루는 것과 달리, 3D 공간에서의 구체적인 6DoF 조작 파라미터를 제공한다는 점에서 차별화된다. 실험 결과, 좁은 틈새로 그리퍼를 넣어 레버를 당기거나, 특정 각도로 문을 여는 정밀한 작업에서 Where2Act는 비디오 기반 방식보다 월등히 높은 성공률을 기록했다. 이는 시각적 정보만으로 물체의 보이지 않는 관절 축(Joint Axis)과 운동 범위를 암묵적으로 추론하는 능력을 모델이 학습했음을 시사한다. 또한, 학습에 사용되지 않은 새로운 범주의 관절 물체에 대해서도 기하학적 유사성을 기반으로 올바른 조작 전략을 생성하는 범주 간 일반화(Cross-Category Generalization) 능력을 입증했다.</p>
<h2>5.  생성형 모델 기반 어포던스: 확산 모델과 플로우 매칭</h2>
<p>2024년과 2025년을 기점으로, 컴퓨터 비전 및 자연어 처리 분야를 휩쓴 <strong>생성형 AI(Generative AI)</strong> 기술이 로봇 어포던스 학습의 지평을 확장하고 있다. 특히 텍스트-이미지(Text-to-Image) 생성 모델인 확산 모델(Diffusion Models)이 인터넷상의 방대한 데이터로부터 학습한 “상식적인 지식“을 로봇의 물리적 추론에 전이(Transfer)하려는 시도가 활발하다.</p>
<h3>5.1  DAG: 확산 모델의 잠재적 지식 해방</h3>
<p><strong>DAG (Diffusion-based Affordance Grounding)</strong> 는 사전 학습된 대규모 확산 모델(Stable Diffusion 등)의 동결된(Frozen) 내부 표현을 활용하여 3D 어포던스를 추론하는 혁신적인 프레임워크다.</p>
<p><strong>핵심 가설 및 방법론</strong></p>
<p>DAG의 핵심 가설은 “사람이 의자에 앉아 있는 이미지를 생성할 수 있는 확산 모델은 이미 ’의자’와 ’앉다’라는 행동 사이의 기하학적, 의미론적 관계를 내부적으로 이해하고 있다“는 것이다. 이를 활용하기 위해 DAG는 다음과 같은 과정을 거친다.</p>
<ol>
<li><strong>지식 추출(Knowledge Extraction)</strong>: 확산 모델의 U-Net 구조 내에 존재하는 중간 특징 맵(Intermediate Feature Maps)과 교차 주의 집중 맵(Cross-Attention Maps)을 추출한다. 이 맵들은 텍스트 프롬프트(예: “A handle to open”)와 시각적 영역 간의 강한 상관관계를 포함하고 있다.</li>
<li><strong>어포던스 블록(Affordance Block)</strong>: 추출된 고차원 시각적 임베딩과 텍스트 토큰을 융합하여 밀집된(Dense) 3D 어포던스 예측을 수행한다.</li>
<li><strong>멀티 소스 디코더(Multi-Source Decoder)</strong>: 2D 이미지 특징을 3D 포인트 클라우드와 정렬(Alignment)하여, 3D 공간상에서의 정확한 어포던스 마스크를 생성한다.</li>
</ol>
<p><strong>개방형 어휘(Open-Vocabulary) 성능</strong> DAG의 가장 큰 장점은 <strong>개방형 어휘</strong> 능력이다. 별도의 추가 학습(Retraining) 없이도 자연어 명령어를 변경함으로써 새로운 어포던스를 즉시 탐지할 수 있다. 예를 들어, 학습 데이터에 존재하지 않았던 “모자를 걸 수 있는 뾰족한 부분“이라는 명령어가 주어져도, 확산 모델의 언어 이해 능력을 통해 옷걸이의 끝부분을 정확히 찾아낼 수 있다. 실험 결과, DAG는 기존의 3D 어포던스 모델들보다 미세한 부품 탐지 및 낯선 물체에 대한 일반화 성능에서 압도적인 우위를 보였다.</p>
<h3>5.2  플로우 매칭(Flow Matching) 기반 조작 정책</h3>
<p>확산 모델의 생성 과정(SDE: Stochastic Differential Equation)을 로봇의 행동 생성에 적용한 것이 Diffusion Policy라면, 이를 더욱 결정론적이고 효율적인 ODE(Ordinary Differential Equation) 풀이 과정으로 개선한 것이 <strong>플로우 매칭(Flow Matching)</strong> 기술이다.</p>
<p><strong>어포던스 유도 플로우 매칭 (Affordance-guided Flow Matching)</strong></p>
<p>이 방법론은 시각적 어포던스 학습과 로봇의 궤적 생성을 단일 프레임워크로 통합한다.</p>
<ol>
<li><strong>프롬프트 튜닝(Prompt Tuning)</strong>: 비전 파운데이션 모델(VFM)에 작업과 관련된 텍스트 프롬프트를 주입하여, 이미지 내에서 조작에 필요한 핵심 특징(Affordance Feature)을 추출하도록 미세 조정(Fine-tuning)한다. 이는 전체 모델을 재학습하는 것보다 훨씬 효율적이다(Parameter-Efficient).</li>
<li><strong>플로우 벡터 필드(Flow Vector Field) 학습</strong>: 추출된 어포던스 정보를 조건(Condition)으로 하여, 랜덤한 노이즈 분포를 로봇의 최적 행동 궤적 분포로 매끄럽게 변환(Flow)하는 벡터 필드를 학습한다.</li>
</ol>
<p><strong>기술적 이점</strong> 플로우 매칭 기반 정책은 기존의 확산 정책(Diffusion Policy) 대비 <strong>추론 속도가 빠르고 학습 안정성이 높다</strong>. 노이즈 제거 단계를 수십 번 거쳐야 하는 확산 모델과 달리, 플로우 매칭은 ODE 솔버를 통해 더 적은 단계로 최적의 경로를 생성할 수 있다. 또한, 생성된 궤적이 어포던스 영역(예: 컵의 손잡이)으로 자연스럽게 수렴하도록 유도함으로써 동작의 정확도와 안전성을 동시에 확보한다. Honda Research Institute EU의 연구에 따르면, 플로우 매칭 정책은 다양한 조작 작업 벤치마크(Push-T, Franka Kitchen)에서 행동 복제(Behavior Cloning) 및 기본 확산 정책보다 우수한 일반화 성능과 빠른 추론 속도를 기록했다.</p>
<h2>6.  자기지도 학습과 놀이로부터의 발견</h2>
<p>데이터 레이블링의 병목 현상을 해결하기 위해, 로봇이 스스로 환경과 상호작용하며 어포던스를 학습하는 <strong>자기지도 학습(Self-Supervised Learning)</strong> 접근법도 중요한 진전을 이루었다.</p>
<h3>6.1  Inpaint2Learn: 인페인팅을 통한 기능적 추론</h3>
<p><strong>Inpaint2Learn</strong> 은 이미지 인페인팅(Inpainting) 작업을 대리 과제(Pretext Task)로 사용하여 어포던스를 학습하는 독창적인 방법론이다.</p>
<ul>
<li><strong>아이디어</strong>: 물체의 일부분(예: 컵의 손잡이)을 이미지에서 지우고, 모델이 이를 복원하도록 학습시킨다. 손잡이를 그럴듯하게 복원해낸다는 것은 해당 위치에 손잡이가 있어야 함을, 즉 ’잡을 수 있는 기능’이 존재함을 모델이 이해하고 있다는 방증이다.</li>
<li><strong>응용</strong>: 이렇게 학습된 모델은 사람의 손과 물체의 상호작용 관계를 예측하거나, 빈 공간에 어떤 물체가 놓여야 자연스러운지를 추론하는(Context-aware Placement) 데 활용된다. 이는 별도의 사람이 레이블링한 데이터 없이도 시각적 문맥(Visual Context)만으로 어포던스를 학습할 수 있음을 보여준다.</li>
</ul>
<h3>6.2  VAPO: 놀이(Play) 데이터 활용</h3>
<p><strong>VAPO (Visual Affordance-guided Policy Optimization)</strong> 는 로봇이 목적 없이 환경과 상호작용하는 ‘놀이(Play)’ 데이터로부터 어포던스를 추출한다.</p>
<ul>
<li><strong>과정</strong>: 로봇이 원격 조종(Teleoperation)이나 랜덤 탐색을 통해 다양한 물체를 건드려본다. 이 과정에서 물체가 움직이거나 상태가 변하는 영역을 ’행동 가능한 영역’으로 스스로 레이블링한다.</li>
<li><strong>강화학습 가속화</strong>: 이렇게 학습된 어포던스 맵은 강화학습(RL)의 탐색(Exploration) 효율을 극대화하는 데 사용된다. 로봇은 아무 곳이나 무작위로 탐색하는 대신, 어포던스 맵이 활성화된 영역(Actionable Region)을 집중적으로 탐색함으로써 정책 학습 속도를 베이스라인 대비 4배 이상 가속화했다. 이는 어포던스가 지각(Perception)을 넘어 제어(Control)의 효율성을 높이는 중요한 사전 지식(Prior)임을 입증한다.</li>
</ul>
<h2>7.  종합 비교 및 성능 분석</h2>
<p>본 장에서 다룬 주요 어포던스 학습 방법론들을 성능, 입력 데이터, 출력 형태, 그리고 한계점 측면에서 비교 분석한다.</p>
<table><thead><tr><th><strong>구분</strong></th><th><strong>주요 방법론</strong></th><th><strong>입력 데이터</strong></th><th><strong>출력 형태</strong></th><th><strong>핵심 특징 및 장점</strong></th><th><strong>한계점</strong></th></tr></thead><tbody>
<tr><td><strong>2D 지도 학습</strong></td><td><strong>AffordanceNet</strong></td><td>RGB 이미지, BBox</td><td>픽셀 단위 클래스 마스크</td><td>객체 검출과 어포던스 동시 수행, 빠른 추론(150ms)</td><td>3D 기하 정보 부재, 시점 의존적(Viewpoint Dependent)</td></tr>
<tr><td><strong>비디오 기반</strong></td><td><strong>VRB</strong></td><td>인간 행동 비디오 (Ego/Exo)</td><td>접촉 히트맵, 3D 궤적</td><td>대규모 데이터 활용, 야생 환경(In-the-wild) 강건성</td><td>정밀한 6DoF 조작 부족, 로봇 신체 구조 차이(Embodiment Gap)</td></tr>
<tr><td><strong>3D 시뮬레이션</strong></td><td><strong>Where2Act</strong></td><td>3D 포인트 클라우드</td><td>행동 가능성 점수, 6D 포즈 제안</td><td>관절 객체 조작 특화, 물리적 상호작용 이해</td><td>시뮬레이션 데이터 의존(Sim-to-Real Gap), 텍스처 정보 활용 미흡</td></tr>
<tr><td><strong>생성형 모델</strong></td><td><strong>DAG</strong></td><td>RGB, 텍스트 프롬프트</td><td>밀집 어포던스 맵</td><td>개방형 어휘(Open-Vocabulary) 처리, 강력한 사전 지식 활용</td><td>높은 계산 비용, 실시간성 부족 가능성</td></tr>
<tr><td><strong>정책 통합</strong></td><td><strong>Flow Matching</strong></td><td>RGB, 언어 명령어</td><td>연속 궤적 (Trajectory)</td><td>빠른 추론, 높은 학습 안정성, 어포던스 기반 유도</td><td>복잡한 다단계 작업(Long-horizon)에서의 검증 필요</td></tr>
</tbody></table>
<p><strong>성능 비교</strong>: 최신 벤치마크 결과에 따르면, <strong>DAG</strong>와 같은 생성형 기반 모델들이 기존의 감독 학습 모델(AffordanceNet)이나 비디오 기반 모델(VRB)보다 낯선 물체(Unseen Objects)에 대한 일반화 성능(Zero-shot Generalization)에서 우위를 점하고 있다. 특히 언어 명령어를 통한 유연한 어포던스 지정 능력은 로봇이 가정이나 사무실과 같은 비구조화된 환경에서 사용자의 다양한 요구를 수행하는 데 필수적이다. 반면, <strong>Where2Act</strong>와 같은 3D 기반 모델은 서랍 열기와 같이 물리적 제약이 강한 작업에서 여전히 독보적인 정밀도를 보여준다. 이는 하나의 만능 모델보다는 작업의 성격에 따라 적절한 어포던스 표현 방식을 선택하거나, 이들을 하이브리드로 결합하는 전략이 필요함을 시사한다.</p>
<h2>8.  결론 및 향후 전망</h2>
<p>시각적 어포던스 학습은 2D 이미지의 영역 분할에서 시작하여, 인간 행동 모방을 통한 핫스팟 예측, 3D 물리 시뮬레이션을 통한 능동적 상호작용 학습, 그리고 생성형 AI를 활용한 개방형 어휘 추론으로 진화해왔다.</p>
<ol>
<li><strong>정밀함과 일반화의 균형</strong>: 초기 모델들이 특정 클래스의 물체에 대해 정밀한 마스크를 생성하는 데 집중했다면, 최신 모델들은 인터넷 규모의 지식을 활용하여 처음 보는 물체와 환경에서도 “어떻게 행동할지“를 추론하는 일반화 능력에 초점을 맞추고 있다.</li>
<li><strong>생성형 행동 지능</strong>: 확산 모델과 플로우 매칭 기술의 도입은 어포던스 인식을 단순한 분류 문제가 아닌, 행동 궤적의 생성 문제로 전환시켰다. 이는 로봇이 불확실한 환경에서 여러 가지 가능한 행동 대안을 생성하고, 상황에 가장 적합한 행동을 선택할 수 있게 한다.</li>
<li><strong>언어와의 결합</strong>: VLM(Vision-Language Model)과의 통합은 로봇이 “조심스럽게 들어라”, “손잡이를 꽉 잡아라“와 같은 부사적(Adverbial) 지시나 추상적인 맥락을 이해하고 이를 물리적 행동으로 변환(Grounding)하는 능력을 비약적으로 향상시킬 것이다.</li>
</ol>
<p>향후 연구는 **양손 조작(Bimanual Manipulation)**을 위한 협응 어포던스, 동적으로 움직이는 물체에 대한 실시간 어포던스 추적, 그리고 촉각(Tactile) 정보와 시각 정보를 결합한 멀티모달 어포던스 학습으로 확장될 전망이다. 이러한 기술적 진보는 로봇이 인간의 생활 공간에 자연스럽게 통합되어, 진정한 의미의 가사 도우미나 협동 로봇으로 거듭나는 데 핵심적인 기여를 할 것이다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>Affordances from Human Videos as a Versatile Representation for Robotics, https://robo-affordances.github.io/resources/vrb_paper.pdf</li>
<li>Perceiving Possibilities for Action: On the Necessity of Calibration and Perceptual Learning for the Visual Guidance of Action | Request PDF - ResearchGate, https://www.researchgate.net/publication/7703762_Perceiving_possibilities_for_action_On_the_necessity_of_calibration_and_perceptual_learning_for_the_visual_guidance_of_action</li>
<li>Is there a lower visual field advantage for object affordances? A registered report - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC11529120/</li>
<li>Affordance - Wikipedia, https://en.wikipedia.org/wiki/Affordance</li>
<li>Computational mechanisms underlying cortical responses to the affordance properties of visual scenes - Research journals - PLOS, https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006111</li>
<li>Diffusion models for robotic manipulation: a survey - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1606247/full</li>
<li>Affordance-based Robot Manipulation with Flow Matching - arXiv, https://arxiv.org/html/2409.01083v5</li>
<li>AffordanceNet: an end-to-end deep learning approach for object affordance detection, https://research.monash.edu/en/publications/affordancenet-an-end-to-end-deep-learning-approach-for-object-aff/</li>
<li>[1709.07326] AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection - arXiv, https://arxiv.org/abs/1709.07326</li>
<li>AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection - Computer Science, https://www.csc.liv.ac.uk/~anguyen/assets/pdfs/2018_ICRA_AffordanceNet.pdf</li>
<li>3D AffordanceNet: A Benchmark for Visual Object Affordance Understanding - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2021/papers/Deng_3D_AffordanceNet_A_Benchmark_for_Visual_Object_Affordance_Understanding_CVPR_2021_paper.pdf</li>
<li>Gorilla-Lab-SCUT/AffordanceNet - GitHub, https://github.com/Gorilla-Lab-SCUT/AffordanceNet</li>
<li>A Deep Learning Approach to Object Affordance Segmentation - IEEE Xplore, https://ieeexplore.ieee.org/document/9054167</li>
<li>Grounded Human-Object Interaction Hotspots From Video - CVF Open Access, https://openaccess.thecvf.com/content_ICCV_2019/papers/Nagarajan_Grounded_Human-Object_Interaction_Hotspots_From_Video_ICCV_2019_paper.pdf</li>
<li>Grounded Human-Object Interaction Hotspots From Video, https://www.cs.utexas.edu/~grauman/papers/hotspots-iccv2019.pdf</li>
<li>Vision-Robotics Bridge, https://vision-robotics-bridge.github.io/</li>
<li>A Review of Video-based Learning Approaches for Robot Manipulation - arXiv, https://arxiv.org/html/2402.07127v3</li>
<li>Affordances From Human Videos as a Versatile Representation for Robotics - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Bahl_Affordances_From_Human_Videos_as_a_Versatile_Representation_for_Robotics_CVPR_2023_paper.pdf</li>
<li>arXiv:2503.07135v2 [cs.RO] 27 Mar 2025 - mediaTUM, https://mediatum.ub.tum.de/doc/1779995/fznwy7bayq5zamupl9bg5toqy.pdf</li>
<li>Where2Act: From Pixels to Actions for Articulated 3D Objects - GitHub, https://github.com/daerduoCarey/where2act</li>
<li>Where2Act: From Pixels to Actions for Articulated 3D Objects - Stanford Computer Science, https://cs.stanford.edu/~kaichun/where2act/</li>
<li>Where2Act: From Pixels to Actions for Articulated 3D Objects | Request PDF - ResearchGate, https://www.researchgate.net/publication/358994417_Where2Act_From_Pixels_to_Actions_for_Articulated_3D_Objects</li>
<li>[2101.02692] Where2Act: From Pixels to Actions for Articulated 3D Objects - arXiv, https://arxiv.org/abs/2101.02692</li>
<li>RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation - arXiv, https://arxiv.org/html/2407.04689v1</li>
<li>DAG: Unleash the Potential of Diffusion Model for Open-Vocabulary 3D Affordance Grounding - arXiv, https://arxiv.org/html/2508.01651v1</li>
<li>DAG: Unleash the Potential of Diffusion Model for Open-Vocabulary 3D Affordance Grounding - ResearchGate, https://www.researchgate.net/publication/394292841_DAG_Unleash_the_Potential_of_Diffusion_Model_for_Open-Vocabulary_3D_Affordance_Grounding</li>
<li>DAG: Unleash the Potential of Diffusion Model for Open-Vocabulary 3D Affordance Grounding - Zhenhao Zhang, https://zhangzhh.cn/DAG_page/</li>
<li>Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions | Request PDF - ResearchGate, https://www.researchgate.net/publication/394512560_Grounding_3D_Object_Affordance_with_Language_Instructions_Visual_Observations_and_Interactions</li>
<li>[Literature Review] Affordance-based Robot Manipulation with Flow Matching, https://www.themoonlight.io/en/review/affordance-based-robot-manipulation-with-flow-matching</li>
<li>Affordance-based Robot Manipulation with Flow Matching - arXiv, https://arxiv.org/html/2409.01083v3</li>
<li>[Quick Review] Affordance-based Robot Manipulation with Flow Matching - Liner, https://liner.com/review/affordancebased-robot-manipulation-with-flow-matching</li>
<li>Inpaint2Learn: A Self-Supervised Framework for Affordance Learning - CVF Open Access, https://openaccess.thecvf.com/content/WACV2022/papers/Zhang_Inpaint2Learn_A_Self-Supervised_Framework_for_Affordance_Learning_WACV_2022_paper.pdf</li>
<li>[2203.00352] Affordance Learning from Play for Sample-Efficient Policy Learning - arXiv, https://arxiv.org/abs/2203.00352</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>