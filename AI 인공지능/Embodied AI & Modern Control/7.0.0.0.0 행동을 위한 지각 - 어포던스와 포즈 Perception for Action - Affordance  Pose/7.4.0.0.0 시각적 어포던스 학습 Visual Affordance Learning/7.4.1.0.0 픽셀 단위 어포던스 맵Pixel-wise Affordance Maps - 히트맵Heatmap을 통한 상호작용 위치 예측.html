<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.4.1 픽셀 단위 어포던스 맵(Pixel-wise Affordance Maps): 히트맵(Heatmap)을 통한 상호작용 위치 예측</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.4.1 픽셀 단위 어포던스 맵(Pixel-wise Affordance Maps): 히트맵(Heatmap)을 통한 상호작용 위치 예측</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <a href="index.html">7.4 시각적 어포던스 학습 (Visual Affordance Learning)</a> / <span>7.4.1 픽셀 단위 어포던스 맵(Pixel-wise Affordance Maps): 히트맵(Heatmap)을 통한 상호작용 위치 예측</span></nav>
                </div>
            </header>
            <article>
                <h1>7.4.1 픽셀 단위 어포던스 맵(Pixel-wise Affordance Maps): 히트맵(Heatmap)을 통한 상호작용 위치 예측</h1>
<h3>0.1 서론: 행동을 위한 정밀 지각의 필요성</h3>
<p>Embodied AI의 핵심은 로봇이 환경을 수동적으로 관찰하는 것을 넘어, 물리적 세계와 능동적으로 상호작용하는 데 있다. 제임스 깁슨(J.J. Gibson)이 제창한 생태학적 심리학의 개념인 어포던스(Affordance)는 환경이 행위 주체(Agent)에게 제공하는 ’행동의 가능성’을 의미한다. 로봇 공학의 관점에서, 특히 복잡한 비정형 물체를 조작해야 하는 매니퓰레이션(Manipulation) 작업에서 어포던스는 추상적인 개념이 아닌 구체적인 좌표와 영역으로 환원되어야 한다. 물체가 ’무엇(What)’인지를 식별하는 객체 인식(Object Recognition)이나, 물체가 공간상의 ’어디(Where)’에 위치하는지를 파악하는 객체 탐지(Object Detection)만으로는 로봇이 그 물체를 ‘어떻게(How)’ 다뤄야 하는지에 대한 정보를 완전히 얻을 수 없다.</p>
<p>컵을 예로 들면, 컵의 몸체는 ‘액체를 담는(Contain)’ 기능을 제공하지만, 손잡이는 ‘잡는(Grasp)’ 기능을 제공한다. 로봇이 컵을 옮기려 한다면 손잡이의 정확한 픽셀 위치를 파악해야 하며, 물을 따르려 한다면 컵의 입구 위치를 알아야 한다. 이처럼 하나의 객체 내에서도 부위(Part)별로 기능이 달라지며, 이를 정밀하게 구분하기 위해 등장한 개념이 바로 **픽셀 단위 어포던스 맵(Pixel-wise Affordance Map)**이다. 최신 SOTA(State-of-the-Art) 기술은 이 어포던스 맵을 단순한 이진 마스크(Binary Mask)를 넘어, 상호작용 성공 확률을 연속적인 분포로 표현하는 <strong>히트맵(Heatmap)</strong> 형태로 생성하여 제어의 불확실성을 최소화하는 방향으로 진화하고 있다. 본 절에서는 픽셀 단위 어포던스 맵의 정의와 수학적 모델링, 이를 구현하는 딥러닝 아키텍처의 진화, 그리고 로봇 제어 시스템과의 통합 메커니즘을 심층적으로 다룬다.</p>
<h3>0.2  픽셀 단위 어포던스 맵의 개념적 정의와 분류</h3>
<h4>0.2.1  시맨틱 세그멘테이션과 어포던스 맵의 차별성</h4>
<p>컴퓨터 비전 분야에서 이미지 세그멘테이션(Image Segmentation)은 이미지를 구성하는 픽셀들을 특정 기준에 따라 분류하는 기술이다. 픽셀 단위 어포던스 맵은 형식적으로는 세그멘테이션 작업의 일종으로 보일 수 있으나, 그 내재적 의미와 목표는 전통적인 시맨틱 세그멘테이션(Semantic Segmentation)과 명확히 구분된다.</p>
<p><strong>시맨틱 세그멘테이션</strong>은 이미지 내의 모든 픽셀을 ‘사람’, ‘자동차’, ‘도로’, ’하늘’과 같은 의미론적 범주(Category)로 분류한다. 이때 동일한 클래스에 속하는 객체들은 서로 구분되지 않으며(Instance-agnostic), 각 픽셀은 상호 배타적인(Mutually Exclusive) 하나의 클래스 레이블만을 갖는다.</p>
<p>반면, <strong>픽셀 단위 어포던스 맵</strong>은 픽셀의 레이블이 명사가 아닌 동사(Verb)적 속성, 즉 ’기능(Functionality)’을 나타낸다. 예를 들어, ’의자’라는 객체의 픽셀들은 시맨틱 세그멘테이션에서는 모두 ’Chair’로 분류되지만, 어포던스 맵에서는 등받이가 ‘기댈 수 있는(Support)’, 좌석이 ‘앉을 수 있는(Sit-able)’ 영역으로 세분화된다. 더 나아가, 어포던스 속성은 상호 배타적이지 않다. 컵의 내부는 ‘담을 수 있는(Contain)’ 영역인 동시에, 설거지를 할 때는 ‘닦을 수 있는’ 영역이 될 수 있다. 따라서 어포던스 맵은 다중 레이블(Multi-label) 문제로 접근해야 하며, 각 픽셀이 여러 행동 가능성을 동시에 내포할 수 있음을 전제한다.</p>
<p>**인스턴스 세그멘테이션(Instance Segmentation)**과의 관계도 중요하다. 로봇이 여러 개의 사과 중 특정 사과를 집어야 하는 상황을 고려할 때, 단순히 ’사과’라는 클래스만 아는 것으로는 부족하며 개별 객체(Instance)의 구분이 필수적이다. 따라서 현대의 어포던스 감지 네트워크는 객체 탐지(Detection)를 선행하여 개별 인스턴스를 격리(Locality)한 후, 해당 인스턴스 내부에서 픽셀 단위의 기능성을 예측하는 계층적 구조를 띠는 경우가 많다.</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>시맨틱 세그멘테이션 (Semantic Segmentation)</strong></th><th><strong>인스턴스 세그멘테이션 (Instance Segmentation)</strong></th><th><strong>픽셀 단위 어포던스 맵 (Pixel-wise Affordance Map)</strong></th></tr></thead><tbody>
<tr><td><strong>기본 단위</strong></td><td>픽셀 (Pixel)</td><td>객체 (Object/Instance)</td><td>픽셀 (Pixel within Instance)</td></tr>
<tr><td><strong>레이블 성격</strong></td><td>범주 (Noun, Category)</td><td>개체 식별자 (ID) + 범주</td><td>기능/행동 (Verb, Functionality)</td></tr>
<tr><td><strong>레이블 관계</strong></td><td>상호 배타적 (One class per pixel)</td><td>상호 배타적</td><td><strong>다중 레이블 (Multi-label)</strong> 허용</td></tr>
<tr><td><strong>주요 목적</strong></td><td>장면 이해 (Scene Understanding)</td><td>객체 계수 및 분리</td><td><strong>로봇 상호작용 (Interaction)</strong> 및 조작</td></tr>
<tr><td><strong>출력 형태</strong></td><td>단일 채널 클래스 맵</td><td>인스턴스 마스크</td><td><strong>다채널 확률 히트맵 (Heatmap)</strong></td></tr>
</tbody></table>
<h4>0.2.2  히트맵(Heatmap) 표현의 우수성</h4>
<p>로봇 제어, 특히 파지(Grasping)나 조작(Manipulation)을 위해서는 “여기가 손잡이인가?“라는 이진(Binary) 질문보다, “여기가 손잡이로서 가장 적합한 위치인가?“라는 확률적 질문에 대한 답이 더 유용하다. 이진 마스크는 경계가 명확하지만, 마스크 내부의 모든 픽셀을 동등하게 취급한다. 그러나 실제 물리적 상호작용에서는 손잡이의 가장자리보다는 중심부를 잡는 것이 안정적이다.</p>
<p><strong>히트맵</strong>은 각 픽셀 <span class="math math-inline">p</span>에 대해 특정 행동 <span class="math math-inline">a</span>가 성공할 확률 <span class="math math-inline">P(a|p)</span>를 <span class="math math-inline">0</span>과 <span class="math math-inline">1</span> 사이의 실수 값(Continuous Value)으로 표현한다. 이 방식은 다음과 같은 이점을 제공한다:</p>
<ol>
<li><strong>최적 위치 선정(Optimal Localization):</strong> 히트맵의 피크(Peak) 지점을 찾음으로써 가장 안정적인 파지 점을 즉각적으로 도출할 수 있다.</li>
<li><strong>불확실성 모델링(Uncertainty Modeling):</strong> 히트맵의 퍼짐(Spread) 정도는 해당 어포던스의 공간적 관용도(Tolerance)를 나타낸다. 넓게 퍼진 히트맵은 대략적인 위치 제어도 허용됨을, 좁은 히트맵은 정밀한 제어가 필요함을 시사한다.</li>
<li><strong>미분 가능성(Differentiability):</strong> 연속적인 값을 가지므로, 경사 하강법(Gradient Descent) 기반의 최적화 기법을 적용하여 로봇 팔의 경로 계획(Trajectory Planning)에 직접적인 비용 함수(Cost Function)로 통합할 수 있다.</li>
</ol>
<h3>0.3  어포던스 히트맵의 수학적 모델링</h3>
<p>어포던스 맵을 생성하고 학습시키기 위해서는 정밀한 수학적 정식화가 필요하다. 이는 정답 데이터(Ground Truth)를 생성하는 과정과 네트워크의 예측 오차를 계산하는 손실 함수(Loss Function) 설계로 나뉜다.</p>
<h4>0.3.1  가우시안 커널을 이용한 그라운드 트루스 생성</h4>
<p>일반적으로 어포던스 데이터셋은 상호작용의 중심점(Keypoint)이나 폴리곤 영역으로 레이블링된다. 이를 신경망이 학습할 수 있는 밀집 히트맵(Dense Heatmap)으로 변환하기 위해 2D 가우시안 커널(Gaussian Kernel)이 표준적으로 사용된다.</p>
<p>특정 어포던스 클래스 <span class="math math-inline">c</span>에 대한 정답 키포인트가 <span class="math math-inline">p_k = (x_k, y_k)</span>로 주어졌을 때, 이미지 좌표 <span class="math math-inline">(x, y)</span>에서의 히트맵 값 <span class="math math-inline">H_c(x, y)</span>는 다음과 같이 정의된다:<br />
<span class="math math-display">
H_c(x, y) = \max_{k \in K_c} \exp \left( - \frac{(x - x_k)^2 + (y - y_k)^2}{2\sigma^2} \right)
</span><br />
여기서 <span class="math math-inline">K_c</span>는 클래스 <span class="math math-inline">c</span>에 속하는 모든 키포인트의 집합이며, <span class="math math-inline">\sigma</span>는 어포던스 영역의 유효 반경을 결정하는 표준편차 파라미터이다. <span class="math math-inline">\sigma</span> 값은 객체의 크기나 상호작용의 정밀도 요구사항에 따라 적응적으로(Adaptive) 설정될 수 있다. 예를 들어, 작은 버튼을 누르는 행동(‘Push’)은 작은 <span class="math math-inline">\sigma</span>를 사용하여 좁고 뾰족한 분포를 만들고, 넓은 테이블을 닦는 행동(‘Wipe’)은 큰 <span class="math math-inline">\sigma</span>를 사용하여 완만한 분포를 생성한다.</p>
<p>이러한 가우시안 스플래싱(Gaussian Splatting) 방식은 픽셀 위치 <span class="math math-inline">(x, y)</span>가 정답 위치 <span class="math math-inline">p_k</span>에서 멀어질수록 상호작용 가능성이 지수적으로 감소함을 모델링한다. 이는 로봇 제어 시 발생할 수 있는 위치 오차를 허용하는 확률적 버퍼(Probabilistic Buffer) 역할을 수행한다.</p>
<h4>0.3.2  손실 함수(Loss Function)의 설계</h4>
<p>히트맵 예측 모델을 학습시키기 위한 손실 함수는 크게 분류 기반 접근과 회귀/분포 기반 접근으로 나뉜다.</p>
<ol>
<li><strong>픽셀별 교차 엔트로피(Pixel-wise Cross-Entropy):</strong></li>
</ol>
<p>어포던스 맵을 다중 클래스 분류 문제로 취급할 때 사용된다. 전체 픽셀 수 <span class="math math-inline">N</span>과 클래스 수 <span class="math math-inline">C</span>에 대하여, 손실 함수 <span class="math math-inline">L_{CE}</span>는 다음과 같다:<br />
<span class="math math-display">
   L_{CE} = - \frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} \left[ y_{i,c} \log(\hat{y}_{i,c}) + (1 - y_{i,c}) \log(1 - \hat{y}_{i,c}) \right]
</span><br />
여기서 <span class="math math-inline">y_{i,c}</span>는 정답 레이블(0 또는 1), <span class="math math-inline">\hat{y}_{i,c}</span>는 모델의 예측 확률이다. 이 방식은 <strong>AffordanceNet</strong>과 같이 명확한 경계를 가진 마스크를 예측할 때 주로 사용되며, 각 클래스 채널별로 이진 교차 엔트로피(Binary Cross-Entropy)를 적용하여 다중 레이블 문제를 해결한다.</p>
<ol start="2">
<li><strong>KL 발산(Kullback-Leibler Divergence):</strong></li>
</ol>
<p>히트맵을 픽셀들의 공간적 확률 분포로 간주할 때 적합하다. 예측 분포 <span class="math math-inline">Q</span>가 정답 분포 <span class="math math-inline">P</span>를 얼마나 잘 근사하는지를 측정한다.<br />
<span class="math math-display">
   L_{KL} = \sum_{x} P(x) \log \left( \frac{P(x)}{Q(x)} \right)
</span><br />
<strong>Demo2Vec</strong> 연구에서는 이 KL 발산을 사용하여, 모델이 단순히 피크 위치뿐만 아니라 어포던스 영역의 전반적인 형상(Shape)과 불확실성 분포까지 학습하도록 유도한다. 이는 특히 비디오 데이터로부터 인간의 모호한 동작 의도를 학습할 때 효과적이다.</p>
<ol start="3">
<li><strong>MSE (Mean Squared Error) 및 적응형 손실:</strong> 회귀 문제로 접근하여 픽셀 값 자체의 차이를 최소화한다. 일부 연구에서는 가우시안 분포의 꼬리(Tail) 부분에서 발생하는 미세한 오차가 학습을 방해하는 것을 막기 위해, 정답 히트맵의 값이 일정 수준 이상인 영역(관심 영역)에 더 큰 가중치를 부여하는 가중 MSE(Weighted MSE)를 사용하기도 한다.</li>
</ol>
<h3>0.4  주요 아키텍처 및 알고리즘 심층 분석</h3>
<p>픽셀 단위 어포던스 예측 기술은 초기 CNN 기반 모델에서 비디오 추론 모델, 그리고 최근의 생성형 AI 및 파운데이션 모델 기반으로 급격히 발전해왔다.</p>
<h4>0.4.1  AffordanceNet: 엔드투엔드 탐지 및 세그멘테이션</h4>
<p><strong>AffordanceNet</strong>은 정지 이미지에서 객체 탐지와 픽셀 단위 어포던스 예측을 동시에 수행하는 선구적인 아키텍처이다. Mask R-CNN의 성공적인 구조를 차용하되, 로봇 조작에 필요한 정밀도를 확보하기 위해 몇 가지 결정적인 수정을 가했다.</p>
<ul>
<li>
<p><strong>아키텍처 개요:</strong> VGG16 또는 ResNet 백본을 통해 특징을 추출하고, RPN(Region Proposal Network)으로 객체 후보 영역(RoI)을 제안한다. 이후 두 개의 분기(Branch)로 나뉘는데, 하나는 객체 분류 및 바운딩 박스 회귀를 수행하고, 다른 하나는 어포던스 맵을 생성한다.</p>
</li>
<li>
<p><strong>고해상도 복원을 위한 디컨볼루션(Deconvolution):</strong> 기존 Mask R-CNN은 14x14 또는 28x28의 매우 작은 마스크를 예측한 뒤 이를 업샘플링한다. 이는 일반적인 객체 분할에는 충분하지만, 로봇이 잡아야 할 미세한 부위를 식별하기에는 해상도가 턱없이 부족하다. AffordanceNet은 일련의 디컨볼루션 층(Deconvolutional Layers)을 도입하여 RoI 특징 맵을 244x244 이상의 고해상도로 복원함으로써 픽셀 단위의 정교한 경계를 생성한다.</p>
</li>
<li>
<p><strong>다중 작업 손실(Multi-task Loss):</strong> 네트워크는 다음 세 가지 손실의 가중 합을 최소화하도록 학습된다:<br />
<span class="math math-display">
L = L_{cls} + L_{box} + L_{mask}
</span><br />
여기서 <span class="math math-inline">L_{mask}</span>는 다중 클래스 어포던스 분류를 위한 손실이다. 이러한 동시 학습 전략은 객체의 의미론적 정보(Category)가 어포던스 예측을 돕고, 반대로 어포던스 정보가 객체 인식을 돕는 상호 보완적 효과를 낳는다.</p>
</li>
</ul>
<h4>0.4.2  Demo2Vec: 인간 시연 비디오로부터의 어포던스 추론</h4>
<p>정지 이미지만으로는 도구의 동적인 사용법을 유추하기 어렵다. <strong>Demo2Vec</strong>은 “백문이 불여일견“이라는 아이디어에 착안하여, 인간의 시연(Demonstration) 비디오를 관찰함으로써 정지 이미지 내의 어포던스를 예측하는 프레임워크를 제안했다.</p>
<ul>
<li><strong>메커니즘:</strong> 시스템은 두 가지 입력을 받는다. 하나는 인간이 도구를 사용하는 비디오 클립(Demonstration Video)이고, 다른 하나는 로봇이 상호작용해야 할 타겟 이미지(Target Image)이다.</li>
<li><strong>시공간 특징 추출:</strong> 비디오 프레임들은 CNN을 통과한 후 <strong>ConvLSTM (Convolutional LSTM)</strong> 계층으로 입력되어 시간적 특징(Temporal Dynamics)을 추출한다. 이를 통해 “칼을 앞뒤로 움직이는 동작“과 같은 사용 패턴이 벡터화된다.</li>
<li><strong>어포던스 전이(Transfer):</strong> 추출된 데모 임베딩(Demonstration Embedding)은 타겟 이미지의 시각적 특징과 결합(Concatenate)되어, 디코더를 통해 타겟 물체 위의 히트맵으로 변환된다. 이 과정에서 KL 발산 손실을 사용하여 예측된 히트맵 분포가 정답 분포와 일치하도록 학습한다.</li>
</ul>
<h4>0.4.3  YOLOA (YOLO Affordance): 실시간성과 LLM의 융합</h4>
<p>2025년 말에 등장한 <strong>YOLOA</strong>는 실시간 객체 탐지 모델인 YOLO 아키텍처에 어포던스 예측을 통합하고, 거대 언어 모델(LLM)을 보조적 추론 도구로 활용하는 최신 접근법이다.</p>
<ul>
<li><strong>What-Where-How 통합:</strong> YOLOA는 객체 인식(What), 위치 탐지(Where), 어포던스 예측(How)을 단일 패스(Single-pass) 네트워크에서 처리하여 초당 80프레임(FPS) 이상의 처리 속도를 달성한다. 이는 동적인 환경에서 움직이는 물체를 조작해야 하는 로봇에게 필수적인 성능이다.</li>
<li><strong>LLM 어댑터(Adapter)를 통한 지식 주입:</strong> 시각 정보만으로는 어포던스가 모호한 경우(예: 처음 보는 형태의 주전자), LLM이 보유한 방대한 상식적 지식(World Knowledge)을 활용한다. LLM 어댑터는 “주전자는 손잡이를 잡아야 하고, 주둥이로 물을 따른다“는 텍스트 기반의 의미론적 사전 지식(Semantic Prior)을 시각적 특징 맵에 주입한다. 구체적으로, LLM은 ‘어포던스 게이트(Affordance Gate)’ 메커니즘을 통해 어포던스 분기(Branch)가 집중해야 할 공간적 영역을 가이드한다. 이는 시각-언어 모델(VLM)의 장점을 경량화된 실시간 탐지 모델에 이식한 성공적인 사례이다.</li>
</ul>
<h4>0.4.4  Robo-ABC: 생성형 AI와 의미론적 대응</h4>
<p><strong>Robo-ABC</strong>는 학습 데이터에 없는 새로운 카테고리의 물체(Novel Object)에 대해서도 어포던스를 예측할 수 있는 제로샷(Zero-shot) 일반화 능력을 보여준다. 이 모델은 **확산 모델(Diffusion Model)**의 내부 표현력을 활용한다.</p>
<ul>
<li><strong>어포던스 메모리(Affordance Memory):</strong> 인간의 비디오나 기존 데이터셋에서 추출한 상호작용 경험(접촉 위치, 손의 포즈 등)을 메모리 뱅크에 저장한다. 이는 로봇에게 일종의 ’경험적 지식’을 부여한다.</li>
<li><strong>의미론적 대응(Semantic Correspondence):</strong> 로봇이 낯선 물체(Target)를 마주했을 때, 메모리에서 시각적/기능적으로 가장 유사한 물체(Source)를 검색한다. 그리고 <strong>Stable Diffusion</strong>과 같은 사전 학습된 이미지 생성 모델의 특징 맵(Feature Map)을 추출하여 두 이미지 간의 **코사인 유사도(Cosine Similarity)**를 픽셀 단위로 계산한다.</li>
<li><strong>확산 특징(Diffusion Features)의 발견:</strong> 놀랍게도 이미지 생성을 위해 학습된 확산 모델의 U-Net 내부 특징들은, 서로 다른 카테고리의 물체 간에도 의미론적으로 대응되는 부위(예: 컵의 손잡이와 주전자의 손잡이)를 매우 높은 정확도로 매칭시킨다는 사실이 밝혀졌다. Robo-ABC는 이를 이용하여 별도의 추가 학습 없이(Training-free) Source 물체의 파지 점을 Target 물체의 대응되는 위치로 전이(Transfer)시킨다.</li>
</ul>
<h3>0.5  데이터셋 및 벤치마크</h3>
<p>어포던스 모델의 성능은 고품질의 픽셀 단위 어노테이션 데이터셋에 의존한다.</p>
<h4>0.5.1  UMD Part Affordance Dataset</h4>
<p><strong>UMD 데이터셋</strong>은 픽셀 단위 어포던스 연구의 표준 벤치마크이다.</p>
<ul>
<li><strong>규모 및 구성:</strong> 17개 카테고리의 105개 도구(Tool)에 대해 약 30,000장의 RGB-D 이미지를 제공한다.</li>
<li><strong>클래스 정의:</strong> 7가지 핵심 행동 어포던스(Affordance Classes)를 정의하고 있다: <strong>Grasp(잡기), Cut(자르기), Scoop(프기), Contain(담기), Pound(두드리기), Support(받치기), Wrap-grasp(감싸 쥐기)</strong>.</li>
<li><strong>특징:</strong> 각 픽셀은 다중 레이블을 가질 수 있으며, 회전판을 이용해 물체의 360도 뷰를 제공하여 3D 구조 학습을 용이하게 한다.</li>
</ul>
<h4>0.5.2  IIT-AFF Dataset</h4>
<p><strong>IIT-AFF 데이터셋</strong>은 실험실 환경이 아닌 실제 환경의 복잡성(Clutter)을 반영하는 데 중점을 두었다. ImageNet 데이터셋의 일부와 자체 수집한 로봇 환경 이미지를 결합하여 구축되었으며, 배경이 복잡하고 물체가 가려진(Occluded) 상황에서의 어포던스 감지 성능을 평가하는 데 사용된다. 또한 바운딩 박스와 픽셀 마스크를 모두 제공하여 <strong>AffordanceNet</strong>과 같은 엔드투엔드 모델 학습에 최적화되어 있다.</p>
<h3>0.6  로봇 제어를 위한 통합 (Implementation for Control)</h3>
<p>생성된 히트맵은 그 자체로는 이미지 상의 확률 분포일 뿐이다. 이를 로봇이 실행 가능한 물리적 동작으로 변환하기 위해 다음과 같은 후처리 및 통합 과정이 수행된다.</p>
<h4>0.6.1  2D 히트맵에서 3D 공간 좌표로의 변환</h4>
<p>히트맵 <span class="math math-inline">H(u, v)</span>에서 확률이 최대인 픽셀 좌표 <span class="math math-inline">(u^*, v^*)</span>를 추출한 후, 이를 로봇의 작업 공간(Workspace) 좌표계로 변환해야 한다. 이를 위해 RGB-D 카메라의 깊이 정보(Depth Map) <span class="math math-inline">Z</span>와 카메라 내부 파라미터(Intrinsic Matrix) <span class="math math-inline">K</span>를 이용한 역투영(Back-projection)이 수행된다.<br />
<span class="math math-display">
P_{world} = R_{cam} \cdot \left( Z(u^*, v^*) \cdot K^{-1} \cdot \begin{bmatrix} u^* \\ v^* \\ 1 \end{bmatrix} \right) + T_{cam}
</span><br />
여기서 <span class="math math-inline">R_{cam}</span>과 <span class="math math-inline">T_{cam}</span>은 카메라의 외부 파라미터(Extrinsic Parameters)이다.</p>
<h4>0.6.2  자세 추정 및 접근 벡터 생성</h4>
<p>단순한 위치(<span class="math math-inline">x, y, z</span>)뿐만 아니라, 그리퍼가 어떤 각도로 접근해야 하는지(Orientation)가 중요하다. 어포던스 히트맵의 그라디언트(Gradient) 방향이나, 해당 영역 포인트 클라우드의 표면 법선(Surface Normal) 벡터를 분석하여 접근 벡터(Approach Vector)를 생성한다. 예를 들어, ‘Cut’ 어포던스가 활성화된 칼날 부위에 대해서는 칼날의 길이 방향과 수직인 방향으로 그리퍼를 정렬시킨다.</p>
<h4>0.6.3  시각-언어-행동(VLA) 모델과의 연계</h4>
<p>최근의 연구 흐름은 정적인 어포던스 맵을 넘어, 언어 명령에 따라 동적으로 변화하는 어포던스를 다룬다. 사용자가 “칼을 들어라“라고 명령하면 ‘Grasp’ 히트맵이 활성화되고, “사과를 깎아라“라고 명령하면 칼과 사과의 접촉면에 대한 ‘Cut’ 히트맵이 활성화되어야 한다. 앞서 언급한 <strong>YOLOA</strong>의 LLM 어댑터나 <strong>Robo-ABC</strong>의 텍스트 기반 검색 기능은 이러한 시각-언어-행동(VLA, Vision-Language-Action) 모델의 핵심 구성 요소로 작동하며, 픽셀 단위 어포던스 맵을 로봇의 고수준 추론(Reasoning)과 저수준 제어(Control)를 잇는 인터페이스로 활용하고 있다.</p>
<h3>0.7 결론</h3>
<p>픽셀 단위 어포던스 맵은 로봇에게 “무엇을 할 수 있는가“에 대한 직관적이고 정밀한 시각적 가이드를 제공한다. 가우시안 히트맵을 통한 확률적 표현은 로봇 제어의 불확실성을 수학적으로 포용하게 해주며, <strong>AffordanceNet</strong>에서 시작된 아키텍처는 <strong>YOLOA</strong>와 같은 실시간 모델과 <strong>Robo-ABC</strong>와 같은 생성형 일반화 모델로 진화하며 그 성능과 범용성을 비약적으로 확장시켰다. 이제 어포던스 맵은 단순한 인식을 넘어, 행동을 위한 지각(Perception for Action)을 실현하는 Embodied AI의 중추적인 기술 요소로서, 복잡하고 비정형적인 현실 세계에서 로봇이 유연하게 작업할 수 있는 기반을 마련하고 있다.</p>
<h2>1. 참고 자료</h2>
<ol>
<li>Visual Affordance Prediction: Survey and Reproducibility - arXiv, https://arxiv.org/html/2505.05074</li>
<li>Interactive Affordance Map Building for a Robotic Task, https://robotics.usc.edu/publications/downloads/pub/870/</li>
<li>YOLOA: Real-Time Affordance Detection via LLM Adapter - arXiv, https://arxiv.org/html/2512.03418</li>
<li>Semantic Segmentation: A Practical Guide - Lightly AI, https://www.lightly.ai/blog/semantic-segmentation-a-practical-guide</li>
<li>Instance vs Semantic Segmentation: Understanding the Difference, https://keylabs.ai/blog/instance-vs-semantic-segmentation-understanding-the-difference/</li>
<li>Semantic segmentation: Complete guide [Updated 2024], https://www.superannotate.com/blog/guide-to-semantic-segmentation</li>
<li>One-Shot Object Affordance Detection in the Wild - arXiv, https://arxiv.org/pdf/2108.03658</li>
<li>Multi-label Affordance Mapping from Egocentric Vision, https://openaccess.thecvf.com/content/ICCV2023/papers/Mur-Labadia_Multi-label_Affordance_Mapping_from_Egocentric_Vision_ICCV_2023_paper.pdf</li>
<li>Semantic vs. Instance Segmentation in Microscopy | AI Essentials, https://mediacy.com/blog/ai-essentials-semantic-vs-instance-segmentation-microscopy/</li>
<li>Human-like Object Manipulation Based on Object Affordance …, https://cares.blogs.auckland.ac.nz/files/2020/09/ROMAN2020_Tech8_Paper.pdf</li>
<li>AffordanceNet - Computer Science, https://www.csc.liv.ac.uk/~anguyen/assets/pdfs/2018_ICRA_AffordanceNet.pdf</li>
<li>HIH: Towards More Accurate Face Alignment via Heatmap in … - arXiv, https://arxiv.org/pdf/2104.03100</li>
<li>More than A Point: Capturing Uncertainty with Adaptive Affordance …, https://arxiv.org/html/2510.10912v1</li>
<li>More than A Point: Capturing Uncertainty with Adaptive Affordance …, https://www.researchgate.net/publication/396459665_More_than_A_Point_Capturing_Uncertainty_with_Adaptive_Affordance_Heatmaps_for_Spatial_Grounding_in_Robotic_Tasks</li>
<li>Reinforcement Learning with Visual Affordances and Manipulability …, https://arxiv.org/pdf/2508.13151</li>
<li>Fast &amp; Accurate Gaussian Kernel Density Estimation, https://idl.cs.washington.edu/files/2021-FastKDE-VIS.pdf</li>
<li>Optimizing gaussian heatmap generation - Stack Overflow, https://stackoverflow.com/questions/74666177/optimizing-gaussian-heatmap-generation</li>
<li>Gaussian Kernel - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/gaussian-kernel/</li>
<li>What Is Cross-Entropy Loss Function? - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/what-is-cross-entropy-loss-function/</li>
<li>Cross-Entropy Loss Function in Machine Learning - DataCamp, https://www.datacamp.com/tutorial/the-cross-entropy-loss-function-in-machine-learning</li>
<li>Demo2Vec: Reasoning Object Affordances from Online Videos, https://scispace.com/pdf/demo2vec-reasoning-object-affordances-from-online-videos-3m70pa3q90.pdf</li>
<li>Demo2Vec: Reasoning Object Affordances From Online Videos, https://openaccess.thecvf.com/content_cvpr_2018/papers/Fang_Demo2Vec_Reasoning_Object_CVPR_2018_paper.pdf</li>
<li>Heatmap Distribution Matching for Human Pose Estimation - NIPS, https://proceedings.neurips.cc/paper_files/paper/2022/file/999fcab97007ebef0cda9949550b4a9e-Paper-Conference.pdf</li>
<li>AffordanceNet: An End-to-End Deep Learning Approach for Object …, https://www.researchgate.net/publication/319977466_AffordanceNet_An_End-to-End_Deep_Learning_Approach_for_Object_Affordance_Detection</li>
<li>AffordanceNet: Deep Object Affordance Detection - Emergent Mind, https://www.emergentmind.com/papers/1709.07326</li>
<li>Demo2Vec: Reasoning Object Affordances from Online Videos, https://svl.stanford.edu/assets/publications/pdfs/demo2vec-cvpr2018.pdf</li>
<li>“Detecting and predicting visual affordance of objects in a given …, https://scholarworks.sjsu.edu/etd_projects/990/</li>
<li>YOLOA: Real-Time Affordance Detection via LLM Adapter - arXiv, https://arxiv.org/html/2512.03418v1</li>
<li>YOLOA: Unified Affordance Detection - Emergent Mind, https://www.emergentmind.com/topics/yolo-affordance-yoloa</li>
<li>YOLOA: Real-Time Affordance Detection via LLM Adapter - arXiv, https://arxiv.org/abs/2512.03418</li>
<li>Robo-ABC: Affordance Generalization Beyond Categories … - arXiv, https://arxiv.org/abs/2401.07487</li>
<li>RAM: Retrieval-Based Affordance Transfer for Generalizable Zero- …, https://arxiv.org/html/2407.04689v1</li>
<li>Robo-ABC : Affordance Generalization Beyond Categories via …, https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05748.pdf</li>
<li>RAM: Retrieval-Based Affordance Transfer for - arXiv, <a href="https://arxiv.org/pdf/2407.04689">https://arxiv.org/pdf/2407.04689?</a></li>
<li>Robo-ABC: Affordance Generalization Beyond Categories … - arXiv, https://arxiv.org/html/2401.07487v1</li>
<li>Learning Precise Affordances from Egocentric Videos for Robotic …, https://arxiv.org/html/2408.10123v2</li>
<li>RGB-D Part Affordance Dataset - Austin Myers, http://www.amyers.umiacs.io/part-affordance-dataset/</li>
<li>Affordance of Object Parts from Geometric Features - People @EECS, https://people.eecs.berkeley.edu/~kanazawa/papers/affordanceRSSworkshop2014.pdf</li>
<li>Learning to Segment Object Affordances on Synthetic Data for Task- …, https://vbn.aau.dk/ws/portalfiles/portal/519309597/0544.pdf</li>
<li>Training-Free Affordance Labeling and Exploration using Subspace …, https://ieeexplore.ieee.org/iel8/6287639/6514899/10979846.pdf</li>
<li>IIT-AFF Dataset - Google, https://sites.google.com/site/iitaffdataset/</li>
<li>Examples of affordance detection results by our proposed method …, https://www.researchgate.net/figure/Examples-of-affordance-detection-results-by-our-proposed-method-on-the-IIT-AFF-dataset_fig5_374003529</li>
<li>WenQu-NEU/Affordance-Grounding-Dataset - GitHub, https://github.com/WenQu-NEU/Affordance-Grounding-Dataset</li>
<li>Affogato: Learning Open-Vocabulary Affordance Grounding with …, https://arxiv.org/html/2506.12009v1</li>
<li>Recognising Affordances in Predicted Futures to Plan with … - arXiv, https://arxiv.org/pdf/2206.10920</li>
<li>Simultaneous Localization and Affordance Prediction of Tasks from …, https://arxiv.org/html/2407.13856v2</li>
<li>One-Shot Open Affordance Learning with Foundation Models, https://www.researchgate.net/publication/384169480_One-Shot_Open_Affordance_Learning_with_Foundation_Models</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>