<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.4.4 도구 사용을 위한 어포던스(Tool Affordance): 기능적 부위(Functional Part) 식별과 그립 방향</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.4.4 도구 사용을 위한 어포던스(Tool Affordance): 기능적 부위(Functional Part) 식별과 그립 방향</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <a href="index.html">7.4 시각적 어포던스 학습 (Visual Affordance Learning)</a> / <span>7.4.4 도구 사용을 위한 어포던스(Tool Affordance): 기능적 부위(Functional Part) 식별과 그립 방향</span></nav>
                </div>
            </header>
            <article>
                <h1>7.4.4 도구 사용을 위한 어포던스(Tool Affordance): 기능적 부위(Functional Part) 식별과 그립 방향</h1>
<p>로봇이 인간의 도구를 사용하여 물리적 작업을 수행하는 능력은 단순한 물체 파지(Grasping)를 넘어선 고차원적인 인지 및 조작 능력을 요구한다. 도구 사용(Tool Use)의 맥락에서 어포던스(Affordance)는 물체의 물리적 속성과 로봇의 행동 능력 사이의 관계를 규명하는 핵심 기제이다. 특히 도구는 본질적으로 ’파지 부위(Handle)’와 ’기능적 부위(Functional Part)’라는 이질적인 속성을 가진 다중 부위로 구성되어 있다. 로봇이 망치를 들어 못을 박거나 칼을 들어 사과를 깎기 위해서는 도구의 전체 형상뿐만 아니라, 도구가 환경에 작용을 가하는 기능적 부위를 정확히 식별하고, 해당 기능을 발현시키기 위한 구체적인 파지 위치와 그립 방향(Grasp Orientation)을 결정해야 한다. 본 절에서는 도구의 기능적 부위 식별을 위한 픽셀 및 포인트 클라우드 수준의 밀집 표현(Dense Representation)부터, 기하학적 제약 조건을 명시적으로 다루는 키포인트 기반 방법론, 그리고 대규모 언어-비전 모델(VLM)을 활용한 최신 제로샷(Zero-shot) 추론 기법까지 포괄적으로 다룬다.</p>
<h2>1.  도구 어포던스와 태스크 지향적 파지의 이론적 토대</h2>
<p>제임스 깁슨(James J. Gibson)이 주창한 어포던스 개념은 로봇 공학에서 물체가 제공하는 행동 가능성으로 해석된다. 도구 사용에 있어 이는 단순한 파지 가능성(Graspability)과는 구별되는 ’태스크 지향적 파지(Task-Oriented Grasping, TOG)’의 문제로 귀결된다. 일반적인 파지가 물체를 떨어뜨리지 않고 안정적으로 들어 올리는 것(Stability)을 목표로 한다면, 태스크 지향적 파지는 파지 이후의 조작 행위가 성공적으로 수행될 수 있는지를 평가한다. 예를 들어, 로봇이 컵을 사용하여 물을 따르는 태스크를 수행할 때, 컵의 입구를 막는 파지는 안정적일지라도 기능적으로는 실패한 파지이다. 따라서 도구 어포던스 인지는 **무엇(What)**을 할 수 있는가에 대한 의미론적 이해와, <strong>어디를(Where)</strong>, <strong>어떻게(How)</strong> 잡아야 하는가에 대한 기하학적 추론이 결합되어야 한다.</p>
<p>도구의 기능적 부위 식별은 이러한 태스크 지향적 파지의 선결 조건이다. 대부분의 도구는 사용자가 힘을 가하는 핸들(Handle)과 대상 물체에 힘을 전달하는 작용부(End-effector)로 명확히 구분된다. 로봇은 시각 정보로부터 이 두 영역을 분할하고, 태스크의 요구사항(예: “자르기“를 위해 칼날을 자유롭게 두어야 함)에 맞춰 파지 자세를 계획해야 한다. 최근 연구들은 이러한 도구 어포던스를 학습하기 위해 인간의 시연(Demonstration)을 모방하거나 , 시뮬레이션 상에서 자가 지도 학습(Self-Supervised Learning)을 통해 도구와 효과 간의 인과관계를 학습하는 방향으로 진화하고 있다. 특히 *GIFT (Generalizable Interaction-aware Functional Tool Affordances)*와 같은 프레임워크는 라벨링 된 데이터 없이도 로봇이 상호작용을 통해 절차적으로 생성된 도구들의 어포던스를 발견하도록 유도한다.</p>
<h2>2.  고밀도 시각 표현 기반의 기능적 부위 식별</h2>
<p>컴퓨터 비전 기술의 발전은 이미지나 3D 포인트 클라우드의 각 요소에 의미론적 라벨을 부여함으로써 도구의 기능적 부위를 식별하는 것을 가능하게 했다. 이는 주로 픽셀 수준(Pixel-level) 또는 포인트 수준(Point-level)의 세그멘테이션(Segmentation) 문제로 정의된다.</p>
<h3>2.1 D 이미지 기반의 다중 어포던스 검출: AffordanceNet</h3>
<p>초기 연구는 2D RGB 이미지를 입력으로 받아 객체 검출과 어포던스 분할을 동시에 수행하는 방식에 집중했다. 대표적인 연구인 <em>AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection</em>은 Mask R-CNN 아키텍처를 확장하여 객체 내부의 픽셀들을 다중 어포던스 클래스(예: grasp, cut, contain, support)로 분류한다.</p>
<p>이 아키텍처는 크게 두 가지 분기(Branch)로 구성된다. 첫 번째 분기는 객체의 위치를 찾는 바운딩 박스를 회귀하고 객체 범주를 분류한다. 두 번째 분기는 제안된 관심 영역(RoI) 내에서 각 픽셀의 어포던스 라벨을 예측한다. 여기서 중요한 기술적 난제는 동일한 범주의 도구라도 시점에 따라 형상 크기가 달라진다는 점이다. 이를 해결하기 위해 <em>AffordanceNet</em>은 <strong>견고한 리사이징(Robust Resizing)</strong> 전략을 도입했다. 이는 RoIAlign을 통해 추출된 특징 맵을 고정된 크기의 마스크로 변환할 때 발생하는 정보 왜곡을 최소화하며, 다양한 크기의 도구 부위를 일관된 해상도로 처리할 수 있게 한다. 또한, 단일 픽셀이 여러 어포던스를 가질 수 있는 상황(예: 컵의 손잡이는 ’잡기’와 ’걸기’가 모두 가능)을 고려하여 다중 라벨 분류 손실 함수를 적용한다. 그러나 2D 기반 접근법은 깊이(Depth) 정보의 부재로 인해 3D 공간상에서의 정확한 파지 위치와 접근 벡터를 산출하는 데 한계가 있으며, 손에 의해 가려진 물체(Hand-Occluded Object)의 경우 어포던스 영역 추론이 어렵다는 단점이 존재한다.</p>
<h3>2.2 D 포인트 클라우드 기반의 기하학적 분할</h3>
<p>로봇 조작의 본질은 3D 공간에서의 상호작용이므로, 2D 이미지를 넘어 3D 포인트 클라우드 상에서 직접 어포던스를 예측하는 연구가 주류를 형성하고 있다. <em>PointNet++</em> 나 <em>DGCNN (Dynamic Graph CNN)</em> 과 같은 3D 딥러닝 아키텍처가 주로 활용된다.</p>
<p><em>PointNet++</em> 기반의 방법론들은 입력된 포인트 클라우드 집합 <span class="math math-inline">P = {x_1,..., x_n}</span>에 대해 계층적인 특징 학습(Hierarchical Feature Learning)을 수행한다. 각 포인트는 <span class="math math-inline">(x, y, z)</span> 좌표와 법선 벡터(Normal Vector) 등의 기하학적 정보를 포함하며, 네트워크는 각 포인트가 속한 기능적 부위(예: 머그컵의 손잡이, 병의 입구, 드릴의 트리거)를 분류한다. 이러한 방식은 물체의 전반적인 형상 정보뿐만 아니라 국소적인 기하학적 특징(Local Geometric Features)을 모두 활용할 수 있다는 장점이 있다. 특히 <em>3D AffordanceNet</em> 벤치마크를 통해 다양한 도구에 대한 포인트 수준의 어포던스 예측 성능이 검증되었다.</p>
<p>그러나 기존의 <em>PointNet++</em> 기반 모델들은 블랙박스(Black-box) 형태로 작동하여, 왜 특정 부위를 ’파지 가능’으로 판단했는지에 대한 근거를 제공하지 못하는 해석 가능성(Interpretability)의 문제가 있었다. 이를 해결하기 위해 최근 연구에서는 **프로토타입 학습(Prototypical Learning)**을 3D 포인트 클라우드에 적용하고 있다. <em>Interpretable Affordance Detection on 3D Point Clouds with Probabilistic Prototypes</em> 연구는 입력된 포인트 클라우드의 특징을 사전에 학습된 ’어포던스 프로토타입(Affordance Prototypes)’과 비교하여, “이 부분은 학습된 손잡이 프로토타입과 유사하므로 파지 가능하다“는 식의 사례 기반 추론(Case-based Reasoning)을 수행한다. 이는 로봇의 판단 근거를 시각화할 수 있게 하여 인간-로봇 협업 시 신뢰성을 높이는 데 기여한다.</p>
<h3>2.3 부분 관측과 노이즈 대응</h3>
<p>실제 작업 환경에서는 센서의 시야각 제한이나 다른 물체에 의한 가림(Occlusion)으로 인해 불완전한(Incomplete) 포인트 클라우드만 획득되는 경우가 빈번하다. 이를 보완하기 위해 <strong>DEP-Net</strong>과 같은 연구는 2D 이미지의 깊이 맵 정보를 활용하여 누락된 기하학적 정보를 복원하고 포인트 클라우드 특징 표현을 강화하는 방법을 제안했다. 또한, 생성형 모델을 활용하여 부분 관측된 포인트 클라우드로부터 전체 형상을 복원(Completion)한 후 어포던스를 예측하거나, 확산 모델(Diffusion Model)을 이용하여 노이즈가 섞인 관측 데이터로부터 강건한 파지 포즈를 생성하는 접근법도 시도되고 있다.</p>
<table><thead><tr><th><strong>특징</strong></th><th><strong>2D 기반 (AffordanceNet 등)</strong></th><th><strong>3D 기반 (PointNet++, DGCNN)</strong></th></tr></thead><tbody>
<tr><td><strong>입력 데이터</strong></td><td>RGB 이미지</td><td>3D 포인트 클라우드</td></tr>
<tr><td><strong>공간 정보</strong></td><td>2D 픽셀 좌표 (깊이 정보 부재)</td><td>3D 좌표 (<span class="math math-inline">x, y, z</span>) 및 법선 벡터</td></tr>
<tr><td><strong>장점</strong></td><td>높은 텍스처 인식, 빠른 추론 속도</td><td>정확한 기하학적 형상 이해, 6-DoF 파지 계획 용이</td></tr>
<tr><td><strong>단점</strong></td><td>3D 파지 변환 시 오차 발생, 가림에 취약</td><td>높은 계산 비용, 데이터 노이즈에 민감</td></tr>
<tr><td><strong>주요 응용</strong></td><td>객체 인식, 대략적인 기능 부위 탐색</td><td>정밀 파지 계획, 조작 동작 생성</td></tr>
</tbody></table>
<h2>3.  기하학적 제약 조건과 키포인트 기반의 범주 수준 조작 (Category-Level Manipulation)</h2>
<p>밀집된(Dense) 픽셀이나 포인트 단위의 예측은 정밀하지만, 계산 비용이 높고 로봇의 제어 파이프라인에 직접 통합하기에는 추상화 수준이 낮다는 단점이 있다. 이에 대한 대안으로 물체의 형상을 소수의 의미론적 3D 키포인트(Semantic 3D Keypoints)로 압축하여 표현하고, 이를 기반으로 조작을 계획하는 방법론이 주목받고 있다.</p>
<h3>3.1 kPAM: 키포인트 어포던스 매니퓰레이션 프레임워크</h3>
<p><em>kPAM (Keypoint Affordance Manipulation)</em> 프레임워크는 물체를 6-DoF 포즈(Pose)로 표현하는 전통적인 템플릿 매칭 방식에서 탈피하여, 의미론적으로 정의된 3D 키포인트들의 집합으로 표현한다. 이 방식은 범주 내 형상 변이(Intra-category shape variation)가 큰 물체들을 다룰 때 특히 강력하다. 예를 들어, 모든 머그컵이 동일한 크기와 모양을 갖지 않더라도, ‘윗면 중심(<span class="math math-inline">p_{top\_center}</span>)’, ‘밑면 중심(<span class="math math-inline">p_{bottom\_center}</span>)’, ’손잡이 중심(<span class="math math-inline">p_{handle\_center}</span>)’과 같은 키포인트는 위상학적으로 일관되게 존재한다.</p>
<p>kPAM의 핵심 강점은 조작 목표를 키포인트에 대한 기하학적 비용(Cost)과 제약 조건(Constraint)으로 명시적으로 정의할 수 있다는 점이다. 로봇의 행동 <span class="math math-inline">T_{action} \in SE(3)</span>은 변환된 키포인트 <span class="math math-inline">T_{action}p</span>가 목표 상태의 기하학적 조건을 만족하도록 최적화 문제를 통해 도출된다.</p>
<h4>3.1.1 최적화 수식 및 제약 조건의 정식화</h4>
<p>kPAM에서 로봇의 조작 계획은 다음과 같은 비용 함수 최소화 문제로 정식화된다.<br />
<span class="math math-display">
\min_{T_{action}} \sum_{i} w_i C_i(T_{action} p_i) \quad \text{subject to} \quad G(T_{action} p) \leq 0
</span><br />
여기서 <span class="math math-inline">p_i</span>는 <span class="math math-inline">i</span>번째 키포인트, <span class="math math-inline">C_i</span>는 해당 키포인트에 대한 비용 함수, <span class="math math-inline">G</span>는 기하학적 제약 조건이다. 예를 들어, 머그컵을 랙(Rack)의 고리에 거는 태스크의 경우, 다음과 같은 제약 조건이 설정될 수 있다.</p>
<ol>
<li>
<p><strong>위치 제약 (Position Constraint)</strong>: 손잡이의 중심점이 랙의 고리 위치(<span class="math math-inline">p_{target}</span>)와 일치해야 한다.<br />
<span class="math math-display">
\| T_{action} p_{handle\_center} - p_{target} \| &lt; \epsilon
</span></p>
</li>
<li>
<p><strong>방향 제약 (Orientation Constraint)</strong>: 컵이 바로 서 있어야 하므로, 컵의 수직 축 벡터가 월드 좌표계의 z축과 정렬되어야 한다. 이는 두 키포인트(윗면 중심, 밑면 중심)의 상대적 위치 벡터를 통해 표현된다.<br />
<span class="math math-display">
\frac{T_{action} p_{top} - T_{action} p_{bottom}}{\| T_{action} p_{top} - T_{action} p_{bottom} \|} \cdot \vec{z}_{world} \approx 1
</span></p>
</li>
</ol>
<p>이러한 수식 기반의 제약 조건은 해석 가능(Interpretable)하고 유연(Flexible)하며, 딥러닝 모델이 직접 파지 포즈를 출력하는 방식(End-to-End)에 비해 물리적 타당성을 보장하기 쉽다.</p>
<h4>3.1.2 자동화된 키포인트 선택 (ATK)</h4>
<p>kPAM의 초기 버전에서는 어떤 키포인트가 해당 태스크에 중요한지를 사람이 사전에 정의해야 한다는 한계가 있었다. <em>ATK (Automatic Task-oriented Keypoint Selection)</em> 알고리즘은 이를 자동화하여, 태스크 수행 결과와 키포인트 간의 상관관계를 분석함으로써 최적의 키포인트 집합을 데이터로부터 학습한다. 이는 시뮬레이션상의 전문가 정책(Expert Policy)이나 인간 시연 데이터로부터, 태스크 성공을 가장 잘 예측할 수 있는(Predictive of optimal behavior) 키포인트들을 선별한다. 선정된 키포인트들은 로봇의 시각적 피드백 제어(Visual Servoing)를 위한 추적 대상으로 사용되어, 실시간으로 변화하는 환경에서도 강건한 조작을 가능하게 한다.</p>
<h2>4.  언어-비전 모델(VLM)을 활용한 제로샷 의미론적 추론</h2>
<p>최근 거대 언어 모델(LLM)과 비전-언어 모델(VLM)의 등장은 사전에 학습되지 않은(Unseen) 새로운 도구와 범주에 대해서도 기능적 부위를 식별할 수 있는 <strong>제로샷(Zero-Shot)</strong> 및 <strong>퓨샷(Few-Shot)</strong> 능력을 로봇에게 부여했다. 이는 대규모 데이터 수집 및 라벨링 비용을 획기적으로 낮추는 패러다임의 전환을 의미한다.</p>
<h3>4.1 PartSLIP 및 PartSLIP++: 2D-3D 지식 전이</h3>
<p>*PartSLIP (Part Segmentation via Language-Image Pre-training)*은 GLIP과 같은 2D 오픈 어휘 객체 검출기(Open-Vocabulary Detector)의 강력한 의미론적 인식 능력을 3D 포인트 클라우드로 확장한 연구이다. 작동 원리는 다음과 같다. 먼저 3D 포인트 클라우드를 여러 시점(Multi-view)에서 2D 이미지로 렌더링한다. 그 후 GLIP을 사용하여 자연어 프롬프트(예: “handle”, “blade”, “switch”)에 해당하는 부위를 2D 상에서 검출한다. 마지막으로 검출된 2D 바운딩 박스들을 3D 공간으로 역투영(Back-projection)하고 통합하여 3D 파트 세그멘테이션을 완성한다.</p>
<p>후속 연구인 *PartSLIP++*는 초기 모델의 바운딩 박스 기반 분할이 가진 정밀도 한계를 극복하기 위해 제안되었다. *PartSLIP++*는 SAM (Segment Anything Model)을 도입하여 픽셀 단위의 정밀한 2D 인스턴스 마스크를 생성한다. 또한, 단순한 투영 대신 수정된 EM(Expectation-Maximization) 알고리즘을 적용하여, 2D 마스크와 3D 인스턴스 간의 최적 매칭을 수행한다. 이 과정에서 3D 인스턴스 분할을 관측되지 않은 잠재 변수(Latent Variable)로 간주하고, 이를 반복적으로 정제함으로써 기존 대비 월등히 높은 분할 정확도를 달성했다.</p>
<h3>4.2 ShapeGrasp: 기하학적 분해와 상식 추론의 결합</h3>
<p><em>ShapeGrasp</em>는 물체를 기본적인 볼록 형상(Convex Shapes)으로 분해하고, 이를 그래프 구조로 표현한 뒤 LLM의 상식 추론 능력을 활용하여 각 부위의 역할을 유추하는 방법론이다. 시각적 정보만으로는 “이 둥근 부분이 손잡이인가?“를 판단하기 어려울 때, LLM의 방대한 텍스트 지식을 활용한다.</p>
<ol>
<li><strong>기하학적 분해</strong>: 물체를 기하학적으로 단순화된 부분(Part)들로 분해하고, 이들 간의 연결 관계를 그래프로 생성한다.</li>
<li><strong>의미론적 추론</strong>: LLM에게 물체의 이름(예: “Watering Can”)과 태스크(예: “pour water”)를 입력으로 주면, LLM은 각 기하학적 부위가 ’몸통(Container)’인지 ’손잡이(Graspable)’인지를 추론한다.</li>
<li><strong>적합성 평가</strong>: 각 부위의 기하학적 속성(크기, 위치)과 추론된 의미를 결합하여 해당 태스크 수행에 가장 적합한 파지 부위를 선택한다. 이 방식은 92%의 경우에서 올바른 파트를 선택하는 높은 정확도를 보였으며, 시각적 특징과 언어적 지식을 구조적으로 결합했다는 점에서 의의가 크다.</li>
</ol>
<h3>4.3 RTAGrasp 및 GRIM: 기억 및 생성 모델 기반 접근</h3>
<p><em>RTAGrasp</em>는 인간의 도구 사용 비디오로부터 구축된 ’기억(Memory)’을 활용한다. 로봇은 새로운 물체를 마주했을 때, 기억 저장소에서 가장 유사한 인간의 사용 사례를 검색(Retrieve)하고, 비전 파운데이션 모델을 통해 인간의 파지 자세를 현재 물체에 전이(Transfer) 및 정렬(Align)시킨다. 더 나아가 *GRIM (Grasp Re-alignment via Iterative Matching)*은 비디오 생성 모델(Video Generation Models, VGMs)을 활용하여 파지 제안을 시각적으로 검증하고 재정렬하는 훈련 없는(Training-free) 프레임워크를 제안했다. 이는 생성형 AI가 단순한 이미지 생성을 넘어 로봇의 물리적 추론 및 계획 단계에 깊숙이 개입할 수 있음을 시사한다. <em>ZeroDexGrasp</em> 또한 다중 모달 LLM을 활용하여 태스크 지향적인 솜씨 있는 파지(Dexterous Grasping)를 제로샷으로 생성하는 프레임워크로, 의미론적 추론과 접촉 기반 최적화를 결합하였다.</p>
<h2>5.  그립 방향 결정과 최적화 기반 계획</h2>
<p>기능적 부위가 식별되었다면, 다음 단계는 해당 부위를 <strong>어떻게(How)</strong> 잡을 것인가를 결정하는 것이다. 도구 사용에 있어 파지 자세(Grasp Pose), 특히 접근 벡터(Approach Vector)와 손목의 회전(Orientation)은 도구의 기능 발휘 여부를 결정짓는 결정적 변수이다.</p>
<h3>5.1 태스크 렌치 공간(TWS)과 신경망 점수 함수</h3>
<p>전통적으로 태스크 지향적 파지는 <strong>태스크 렌치 공간(Task Wrench Space, TWS)</strong> 분석을 통해 접근되었다. 이는 특정 작업을 수행하기 위해 도구가 환경에 가해야 하는 힘과 토크의 집합을 정의하고, 특정 파지 자세가 이 렌치 공간을 생성할 수 있는지를 기계적으로 평가하는 방식이다. 최근에는 이를 데이터 기반으로 학습된 **신경망 점수 함수(Neural Score Function)**로 대체하는 추세이다. 딥러닝 모델은 입력된 파지 <span class="math math-inline">g</span>와 태스크 <span class="math math-inline">T</span>에 대해 두 가지 점수를 예측한다.</p>
<ol>
<li>
<p><strong>파지 품질 점수 (<span class="math math-inline">Q_{grasp}</span>)</strong>: 물체를 안정적으로 잡을 수 있는가?</p>
</li>
<li>
<p><strong>태스크 적합성 점수 (<span class="math math-inline">Q_{task}</span>)</strong>: 해당 파지가 태스크 수행을 방해하지 않는가? (예: 컵의 입구를 가리지 않음) 최종적인 파지 선택은 이 두 점수의 가중 합을 최대화하는 최적화 문제로 해결된다.<br />
<span class="math math-display">
g^* = \arg\max_{g} (\alpha Q_{grasp}(g) + \beta Q_{task}(g, T))
</span></p>
</li>
</ol>
<h3>5.2 방향 맵(Orientation Map)과 복셀 기반 표현</h3>
<p>도구 사용 시 파지 방향은 매우 강력한 기하학적 제약 조건을 따른다. 드릴을 사용할 때는 손잡이를 잡되 비트가 작업면을 향해야 하며, 프라이팬을 잡을 때는 손목이 수평을 유지해야 한다. 이를 계산적으로 구현하기 위해 3D 공간을 복셀(Voxel)로 표현하고, 각 복셀에 대해 다양한 맵을 생성하여 최적의 접근 경로를 계획하는 연구가 진행되었다.</p>
<ul>
<li><strong>공간 비용 맵 (Spatial Cost Map)</strong>: 충돌을 회피하고 도구에 접근하기 용이한 영역을 표시.</li>
<li><strong>방향 맵 (Orientation Map)</strong>: 각 위치에서 요구되는 엔드 이펙터의 최적 방향을 벡터 필드(Vector Field)로 표현.</li>
<li><strong>그리퍼 맵 (Gripper Map)</strong>: 그리퍼의 개폐 여부 및 형상 적합성 표시. GPT-4o와 같은 대규모 모델이 자연어 지시를 해석하여 이러한 맵 생성에 필요한 파라미터(예: “수직으로 접근하라”, “손잡이의 뒤쪽을 잡아라”)를 추출하고, 이를 기반으로 복셀 맵을 계산하는 방식이 제안되었다.</li>
</ul>
<h3>5.3 확률론적 접근과 학습된 제약 조건의 전이</h3>
<p>도구의 포즈나 형상에 대한 불확실성이 존재하는 실세계 환경에서는 확률론적(Probabilistic) 접근이 필수적이다. 베이지안 최적화(Bayesian Optimization)를 사용하여 이전에 학습된 기하학적 제약 조건을 새로운 태스크로 전이(Transfer)하거나 , 입자 필터(Particle Filter)를 통해 불확실성을 반복적으로 감소시키는 폐루프(Closed-loop) 제어 전략이 유효하다. 이는 로봇이 한 번의 시도로 완벽한 파지를 수행하려 하기보다, 시각적/촉각적 피드백을 통해 파지 위치를 미세 조정하며 기능적 부위를 찾아가도록 유도한다.</p>
<h3>5.4 양손 조작(Dual-Arm Manipulation)과 어포던스</h3>
<p>복잡한 도구 사용이나 조립 작업의 경우, 하나의 팔로는 불충분할 수 있다. <em>DualAfford</em>와 같은 연구는 두 개의 그리퍼가 동시에 상호작용해야 하는 상황에서의 어포던스를 다룬다. 이때 문제는 단순히 두 개의 파지점을 찾는 것이 아니라, 두 그리퍼 간의 역할 분담(Role Assignment)과 협응(Coordination)을 최적화하는 것이다. 예를 들어, 한 손은 물체를 고정(Support)하고 다른 한 손은 도구를 조작(Manipulate)하는 경우, 각 손의 어포던스는 서로에 의해 조건부(Conditional)로 결정된다. 이를 위해 두 그리퍼의 동작 공간을 분리하면서도 상호 연결된 하위 작업으로 분해하여 학습하는 접근법이 제안되었다.</p>
<h3>5.5 결론</h3>
<p>도구 어포던스를 위한 기능적 부위 식별과 그립 방향 결정 기술은 로봇이 정해진 동작을 반복하는 단순 기계에서, 환경의 물리적 의미를 이해하고 도구를 능동적으로 활용하는 지능형 에이전트로 진화하는 데 있어 핵심적인 기술이다. <em>kPAM</em>과 같은 키포인트 기반 방법론은 기하학적 정밀함과 제약 조건의 명시적 제어를 가능하게 했으며, <em>AffordanceNet</em> 및 <em>PointNet++</em> 기반 모델들은 복잡한 형상에 대한 일반화 능력을 입증했다. 최근에는 <em>PartSLIP</em>, <em>ShapeGrasp</em>, <em>RTAGrasp</em> 등 LLM과 VLM의 방대한 상식 지식을 로봇의 물리적 추론과 결합하려는 시도가 급격히 증가하고 있다. 이러한 기술적 흐름은 로봇이 사전 데이터 없이도 낯선 도구의 기능을 직관적으로 파악하고, 인간과 유사한 수준의 솜씨 있는 조작(Dexterous Manipulation)을 수행할 수 있는 미래를 앞당기고 있다. 향후 연구는 이러한 인지적 추론을 동적인 힘 제어(Force Control) 및 촉각 피드백(Tactile Feedback)과 실시간으로 통합하여, 불확실성이 높은 비정형 환경에서도 강건하게 도구를 조작할 수 있는 완전한 행동-인지 루프(Perception-Action Loop)를 구축하는 방향으로 나아갈 것이다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Developing Intelligent Robots that Grasp Affordance - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2022.951293/full</li>
<li>Affordance Detection for Task-Specific Grasping Using Deep Learning, https://www.cs.columbia.edu/~allen/S19/Student_Papers/kragic_affordance_grasp_planning.pdf</li>
<li>AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary …, https://arxiv.org/html/2503.00778v1</li>
<li>TOSC: Task-Oriented Shape Completion for Open-World Dexterous …, https://arxiv.org/html/2601.05499v1</li>
<li>3D Affordance Keypoint Detection for Robotic Manipulation - arXiv, https://arxiv.org/html/2511.22195v1</li>
<li>RTAGrasp: Learning Task-Oriented Grasping from Human Videos …, https://arxiv.org/html/2409.16033v1</li>
<li>Learning tool affordances without labels, https://robotics.utoronto.ca/2021/07/07/learning-tool-affordances-without-labels/</li>
<li>Learning of Tool Affordances for autonomous tool manipulation, https://www.researchgate.net/publication/254050903_Learning_of_Tool_Affordances_for_autonomous_tool_manipulation</li>
<li>AffordanceNet - Computer Science, https://www.csc.liv.ac.uk/~anguyen/assets/pdfs/2018_ICRA_AffordanceNet.pdf</li>
<li>AffordanceNet: An End-to-End Deep Learning Approach for Object …, https://www.researchgate.net/publication/319977466_AffordanceNet_An_End-to-End_Deep_Learning_Approach_for_Object_Affordance_Detection</li>
<li>Affordance Segmentation of Hand-Occluded Containers from …, https://openaccess.thecvf.com/content/ICCV2023W/ACVR/papers/Apicella_Affordance_Segmentation_of_Hand-Occluded_Containers_from_Exocentric_Images_ICCVW_2023_paper.pdf</li>
<li>Semantic Segmentation of Transmission Corridor 3D Point Clouds …, https://www.mdpi.com/2079-9292/12/13/2829</li>
<li>Open-Vocabulary Affordance Detection in 3D Point Clouds, https://www.csc.liv.ac.uk/~anguyen/assets/pdfs/2023_OpenAD.pdf</li>
<li>Interpretable Affordance Detection on 3D Point Clouds with … - arXiv, https://arxiv.org/html/2504.18355v1</li>
<li>3D AffordanceNet: A Benchmark for Visual Object Affordance …, https://openaccess.thecvf.com/content/CVPR2021/papers/Deng_3D_AffordanceNet_A_Benchmark_for_Visual_Object_Affordance_Understanding_CVPR_2021_paper.pdf</li>
<li>[2504.18355] Interpretable Affordance Detection on 3D Point Clouds …, https://arxiv.org/abs/2504.18355</li>
<li>An Optimization of PointNet++ for Incomplete Point Clouds Using …, https://openreview.net/pdf?id=IAInRggylB</li>
<li>Language-Conditioned Affordance-Pose Detection in 3D Point Clouds, https://ieeexplore.ieee.org/iel8/10609961/10609862/10610008.pdf</li>
<li>kPAM: KeyPoint Affordances for Category-Level Robotic Manipulation, https://arxiv.org/pdf/1903.06684</li>
<li>kPAM: KeyPoint Affordances for Category-Level Robotic Manipulation, https://www.emergentmind.com/papers/1903.06684</li>
<li>kPAM-SC: Generalizable Manipulation Planning using KeyPoint …, https://dspace.mit.edu/bitstream/handle/1721.1/143985/Gao20.pdf?sequence=2&amp;isAllowed=y</li>
<li>kPAM-SC: Generalizable Manipulation Planning using KeyPoint …, https://groups.csail.mit.edu/robotics-center/public_papers/Gao20.pdf</li>
<li>KPAM: KeyPoint Affordances for Category-Level Robotic Manipulation, https://www.researchgate.net/publication/358679767_KPAM_KeyPoint_Affordances_for_Category-Level_Robotic_Manipulation</li>
<li>kPAM: KeyPoint Affordances for Category-Level Robotic Manipulation, https://arxiv.org/abs/1903.06684</li>
<li>Automatic Task-driven Keypoint Selection for Robust Policy Learning, https://arxiv.org/abs/2506.13867</li>
<li>PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via …, https://ieeexplore.ieee.org/document/10203074/</li>
<li>PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds … - Liner, https://liner.com/review/partslip-lowshot-part-segmentation-for-3d-point-clouds-via-pretrained</li>
<li>Low-Shot Part Segmentation for 3D Point Clouds via Pretrained …, https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_PartSLIP_Low-Shot_Part_Segmentation_for_3D_Point_Clouds_via_Pretrained_CVPR_2023_paper.pdf</li>
<li>PartSLIP++: Enhancing Low-Shot 3D Part Segmentation via Multi …, https://arxiv.org/abs/2312.03015</li>
<li>PartSLIP++: Enhancing Low-Shot 3D Part Segmentation via Multi …, https://arxiv.org/html/2312.03015v1</li>
<li>ShapeGrasp: Zero-Shot Task-Oriented Grasping with Large … - arXiv, https://arxiv.org/html/2403.18062v1</li>
<li>ShapeGrasp: Zero-Shot Task-Oriented Grasping with Large …, https://simonstepputtis.com/publication/shapegrasp_iros2024/</li>
<li>Task-Oriented Grasping with Conditioning on Generative Examples, https://arxiv.org/html/2506.15607v2</li>
<li>ZeroDexGrasp: Zero-Shot Task-Oriented Dexterous Grasp Synthesis …, https://arxiv.org/html/2511.13327v1</li>
<li>Task-Based Grasp Quality Measures for Grasp Synthesis - USF RPAL, https://rpal.cse.usf.edu/products/2015_iros_lin.pdf</li>
<li>Learning task-oriented grasping for tool manipulation from simulated …, https://openreview.net/pdf/0b34559dbf63c44bc0f0ed06806433138edf0d69.pdf</li>
<li>Learning task-oriented grasping for tool manipulation from simulated …, https://rpl.cs.utexas.edu/publications/papers/fang-ijrr19-learning.pdf</li>
<li>Manipulability-Aware Task-Oriented Grasp Planning and Motion …, https://www.mdpi.com/2079-9292/13/24/5025</li>
<li>Task-Aware Robot Affordance-Centric Diffusion Policy Learned …, https://ieeexplore.ieee.org/iel8/7083369/11125679/11124589.pdf</li>
<li>Learning Geometric Constraints in Task and Motion Planning - arXiv, https://arxiv.org/pdf/2201.09612</li>
<li>Learning Geometric Constraints in Task and Motion Planning - arXiv, https://arxiv.org/abs/2201.09612</li>
<li>A Probabilistic Planning Framework for Planar Grasping Under …, https://www.ri.cmu.edu/app/uploads/2017/06/main_revised_v2.pdf</li>
<li>DualAfford: Learning Collaborative Visual Affordance for Dual …, https://hyperplane-lab.github.io/DualAfford/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>