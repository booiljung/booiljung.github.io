<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.4.2 3D 어포던스와 접촉 예측(Contact Prediction): 포인트 클라우드와 임플리싯 함수(Implicit Functions) 기반 접근</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.4.2 3D 어포던스와 접촉 예측(Contact Prediction): 포인트 클라우드와 임플리싯 함수(Implicit Functions) 기반 접근</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <a href="index.html">7.4 시각적 어포던스 학습 (Visual Affordance Learning)</a> / <span>7.4.2 3D 어포던스와 접촉 예측(Contact Prediction): 포인트 클라우드와 임플리싯 함수(Implicit Functions) 기반 접근</span></nav>
                </div>
            </header>
            <article>
                <h1>7.4.2 3D 어포던스와 접촉 예측(Contact Prediction): 포인트 클라우드와 임플리싯 함수(Implicit Functions) 기반 접근</h1>
<p>로봇 공학에서 ’지각(Perception)’이 단순히 환경을 관찰하고 분류하는 수동적인 과정을 넘어, 물리적 세계를 변화시키기 위한 능동적 개입의 전제 조건으로 재정의되고 있다. 특히 로봇 매니퓰레이션(Manipulation) 분야에서 물체가 ’무엇인가(Semantic Classification)’를 아는 것보다 더욱 중요한 것은, 그 물체와 ’어떻게 상호작용할 수 있는가(Action Possibility)’를 이해하는 것이다. 제임스 깁슨(James Gibson)이 주창한 생태심리학적 개념인 **어포던스(Affordance)**는 로봇 공학에서 3차원 기하학 정보와 결합하여 **3D 어포던스(3D Affordance)**라는 새로운 기술적 패러다임으로 진화하였다.</p>
<p>2D 이미지 기반의 어포던스 분석이 픽셀 단위의 기능적 영역을 식별하는 데 초점을 맞추었다면, 3D 어포던스는 로봇의 엔드 이펙터(End-effector)가 물리적으로 접촉해야 하는 정확한 6자유도(6-DoF) 포즈와 접촉 표면의 기하학적 특성을 예측하는 문제로 귀결된다. 이는 물체의 표면 법선 벡터(Normal Vector), 곡률(Curvature), 마찰 계수(Friction), 그리고 충돌 가능성(Collision) 등을 종합적으로 고려해야 하는 고차원의 최적화 문제이다.</p>
<p>본 절에서는 이러한 3D 어포던스와 접촉 예측을 구현하는 두 가지 지배적인 데이터 표현 방식인 <strong>포인트 클라우드(Point Cloud)</strong> 기반의 명시적(Explicit) 접근법과 <strong>임플리싯 함수(Implicit Functions)</strong> 기반의 암시적(Implicit) 접근법을 심층적으로 분석한다. 각 접근법이 취하는 수학적 모델링, 신경망 아키텍처, 그리고 실제 로봇 시스템에서의 효용성과 한계를 최신 SOTA(State-of-the-Art) 연구 결과들을 바탕으로 기술한다.</p>
<h2>1.  3D 기하 표현과 상호작용의 본질적 관계</h2>
<p>로봇이 환경과 물리적 접촉(Physical Contact)을 형성하기 위해서는 대상 물체의 형상을 디지털 공간에 표현해야 한다. 이 표현(Representation) 방식은 로봇의 인지 능력과 계산 효율성, 그리고 행동의 정밀도를 결정짓는 핵심 변수이다. 3D 공간 데이터를 처리하는 방식은 크게 데이터를 이산적인 점들의 집합으로 다루는 명시적 방식과, 공간 전체를 연속적인 함수로 정의하는 암시적 방식으로 양분된다.</p>
<p>과거의 로봇 파지(Grasping) 연구가 주로 사전에 모델링된 물체 데이터베이스(CAD Models)에 의존하거나 단순한 기하학적 원형(Primitives)을 피팅하는 방식이었다면, 딥러닝 시대의 3D 어포던스 연구는 센서로부터 획득된 불완전하고 노이즈가 섞인 데이터로부터 직접적으로 ’파지 가능한 영역(Graspable Region)’이나 ’조작 가능한 부위(Actionable Part)’를 추론하는 방향으로 발전했다.</p>
<h3>1.1  명시적 표현(Explicit Representation)의 특성</h3>
<p>명시적 표현은 3D 공간상의 점유(Occupancy)된 위치를 직접적으로 나열한다. 가장 대표적인 형태인 포인트 클라우드(Point Cloud)는 <span class="math math-inline">P = {p_i \in \mathbb{R}^3 \vert i=1, \dots, N}</span>의 집합으로 정의된다.</p>
<ul>
<li><strong>직관성</strong>: LiDAR나 RGB-D 카메라(Depth Camera)의 원시 데이터(Raw Data)와 가장 유사하여 전처리 과정이 최소화된다.</li>
<li><strong>희소성(Sparsity)</strong>: 물체의 표면에만 데이터가 존재하므로 메모리 효율이 높지만, 위상학적(Topological) 연결 정보가 부족하다.</li>
<li><strong>이산성(Discreteness)</strong>: 데이터가 연속적이지 않아 미분 가능한 표면 처리에 한계가 있으며, 해상도에 종속적이다.</li>
</ul>
<h3>1.2  암시적 표현(Implicit Representation)의 특성</h3>
<p>암시적 표현은 3D 형상을 공간상의 좌표 <span class="math math-inline">x \in \mathbb{R}^3</span>를 입력으로 받아 해당 위치의 속성(예: 거리, 점유 확률)을 출력하는 함수 <span class="math math-inline">f(x)</span>의 등위면(Level Set)으로 정의한다. 최근에는 신경망이 이 함수 <span class="math math-inline">f</span>를 근사하는 **뉴럴 필드(Neural Fields)**가 주류를 이룬다.</p>
<ul>
<li><strong>연속성(Continuity)</strong>: 이론적으로 무한한 해상도를 가지며, 임의의 위치에서 값을 쿼리(Query)할 수 있다. 이는 로봇 제어에 필요한 미분 가능한 성질(Differentiability)을 제공한다.</li>
<li><strong>전역적 일관성</strong>: 전체 형상을 하나의 함수로 압축하여 표현하므로, 관측되지 않은 부분(Occlusion)에 대한 보완(Completion) 능력이 뛰어나다.</li>
<li><strong>계산 비용</strong>: 형상을 복원하거나 접촉점을 찾기 위해 다수의 쿼리 연산이나 최적화 과정(Ray Marching 등)이 필요하여 실시간성에 제약이 있을 수 있다.</li>
</ul>
<h2>2.  포인트 클라우드 기반의 어포던스와 접촉 예측</h2>
<p>포인트 클라우드는 로봇 비전 시스템에서 가장 널리 사용되는 데이터 형식이다. 이는 센서 데이터와의 직접적인 대응성 덕분에 실시간 파이프라인 구축에 유리하며, 최근 딥러닝 아키텍처의 발전으로 불규칙하고 정렬되지 않은(Unordered) 점들의 집합으로부터 고수준의 의미론적, 기하학적 특징을 추출하는 능력이 비약적으로 향상되었다.</p>
<h3>2.1  포인트 클라우드 처리 아키텍처의 진화와 어포던스</h3>
<p>포인트 클라우드 기반의 3D 어포던스 학습은 각 점 <span class="math math-inline">p_i</span>가 특정 행동을 지원하는지 여부를 판별하는 <strong>점 단위 세분화(Point-wise Segmentation)</strong> 문제 또는 각 점을 중심으로 하는 로컬 프레임에서의 <strong>포즈 회귀(Pose Regression)</strong> 문제로 정식화된다.</p>
<h4>2.1.1  PointNet++와 로컬 기하학의 중요성</h4>
<p>초기의 <strong>PointNet</strong> 은 모든 점을 독립적으로 처리한 후 전역적 특징(Global Feature)을 맥스 풀링(Max Pooling)으로 추출하는 방식을 취했다. 그러나 어포던스는 물체의 전체 형상보다는 손잡이, 뚜껑의 가장자리, 버튼의 표면과 같은 **국소적 기하학(Local Geometry)**에 크게 의존한다. 이를 해결하기 위해 등장한 **PointNet++**는 계층적 구조를 도입하여 다양한 반경(Radius) 내의 점들을 그룹화하고 특징을 추출함으로써 로컬 구조와 전역 구조를 동시에 학습한다.</p>
<p>PointNet++의 핵심 연산인 <strong>설정 추상화(Set Abstraction)</strong> 계층은 다음과 같이 정의된다:</p>
<ol>
<li><strong>샘플링(Sampling)</strong>: Farthest Point Sampling (FPS) 알고리즘을 사용하여 대표점들을 선별한다.</li>
<li><strong>그룹화(Grouping)</strong>: 각 대표점 주변의 이웃 점들을 모아 로컬 영역을 형성한다.</li>
<li><strong>PointNet</strong>: 각 로컬 영역에 PointNet을 적용하여 로컬 특징 벡터를 추출한다.</li>
</ol>
<p>이러한 구조는 로봇이 물체의 ’손잡이’와 같은 기능적 부위(Functional Part)를 식별하고, 해당 부위의 곡률이나 크기에 맞는 그리퍼의 개방 폭(Width)을 예측하는 데 결정적인 역할을 한다. 최신 연구인 <strong>Contact-GraspNet</strong>이나 <strong>AnyGrasp</strong> 등의 시스템은 대부분 PointNet++를 백본(Backbone) 네트워크로 채택하고 있다.</p>
<h4>2.1.2  트랜스포머(Transformer)와 어텐션 메커니즘의 도입</h4>
<p>최근에는 자연어 처리 분야에서 성공을 거둔 트랜스포머 아키텍처를 3D 포인트 클라우드에 적용하려는 시도가 늘고 있다. <strong>Point Transformer</strong>나 <strong>Point-BERT</strong>와 같은 모델들은 점들 간의 관계를 어텐션(Attention) 메커니즘으로 모델링하여, 중요한 기하학적 특징에 가중치를 부여한다. 이는 복잡한 클러터(Clutter) 환경에서 로봇이 파지해야 할 대상 물체와 배경을 명확히 구분하고, 파지 안정성이 높은 지점에 집중(Attend)하게 함으로써 어포던스 예측의 정확도를 높인다.</p>
<h3>2.2  심층 분석: Contact-GraspNet - 6-DoF 파지 포즈의 밀도 높은 예측</h3>
<p><strong>Contact-GraspNet</strong> 은 포인트 클라우드 기반 접촉 예측의 패러다임을 바꾼 획기적인 연구이다. 기존의 파지 검출 방식이 후보군을 샘플링하고 순위를 매기는 다단계 방식이었던 반면, Contact-GraspNet은 입력 포인트 클라우드의 모든 점을 잠재적인 접촉점(Contact Point)으로 간주하고, 각 점에 대해 6-DoF 파지 포즈를 직접 회귀하는 종단간(End-to-End) 방식을 제안했다.</p>
<h4>2.2.1  네트워크 구조와 추론 메커니즘</h4>
<p>Contact-GraspNet은 부분적인(Partial) 포인트 클라우드 <span class="math math-inline">P \in \mathbb{R}^{N \times 3}</span>를 입력으로 받는다. 이 네트워크는 PointNet++ 기반의 U-Net 구조를 가지며, 각 점 <span class="math math-inline">p_i</span>에 대해 다음과 같은 파지 파라미터를 예측한다:</p>
<ul>
<li><strong>파지 점수(Grasp Score, <span class="math math-inline">s_i</span>)</strong>: 해당 점이 안정적인 파지 접촉점이 될 확률 ($s_i \in $).</li>
<li><strong>파지 방향(Grasp Orientation, <span class="math math-inline">R_i \in SO(3)</span>)</strong>: 로봇 그리퍼의 접근 벡터(Approach Vector)와 기준선(Baseline) 간의 회전을 쿼터니언(Quaternion) 또는 6D 회전 표현법으로 예측.</li>
<li><strong>그리퍼 폭(Grasp Width, <span class="math math-inline">w_i \in \mathbb{R}</span>)</strong>: 물체의 크기에 따른 그리퍼의 개방 정도.</li>
</ul>
<p>이 모델의 가장 큰 특징은 관측되지 않은 물체의 뒷면이나 가려진 부분에 대해서도 파지 가능한 포즈를 추론할 수 있다는 점이다. 이는 네트워크가 수십억 개의 파지 데이터를 포함하는 <strong>GraspNet-1Billion</strong>이나 <strong>Acronym</strong> 데이터셋을 통해 다양한 물체의 기하학적 형상을 내재적으로 학습했기 때문이다.</p>
<h4>2.2.2  손실 함수(Loss Function)와 학습 전략</h4>
<p>Contact-GraspNet의 학습은 예측된 파지 포즈와 실제 성공 가능한 파지 포즈(Ground Truth) 간의 차이를 최소화하는 방향으로 이루어진다. 전체 손실 함수 <span class="math math-inline">\mathcal{L}_{total}</span>은 다음과 같이 구성된다:<br />
<span class="math math-display">
\mathcal{L}_{total} = \mathcal{L}_{score} + \mathbb{I}_{positive} (\lambda_1 \mathcal{L}_{rot} + \lambda_2 \mathcal{L}_{width} + \lambda_3 \mathcal{L}_{trans})
</span></p>
<ul>
<li><span class="math math-inline">\mathcal{L}_{score}</span>: 이진 크로스 엔트로피(Binary Cross Entropy) 손실을 사용하여 파지 성공 확률을 학습한다.</li>
<li><span class="math math-inline">\mathcal{L}_{rot}</span>: 예측된 회전 <span class="math math-inline">R_{pred}</span>와 정답 회전 <span class="math math-inline">R_{gt}</span> 간의 거리를 최소화한다. 대칭 물체의 경우 불필요한 회전 오차를 무시하도록 설계된다.</li>
<li><span class="math math-inline">\mathcal{L}_{width}</span>: 그리퍼 폭에 대한 L1 또는 L2 손실이다.</li>
<li><span class="math math-inline">\mathcal{L}_{trans}</span>: 접촉점의 위치 오차를 줄이기 위한 항으로, 예측된 접촉점이 물체 표면 근처에 위치하도록 강제한다.</li>
</ul>
<p>추론 시에는 예측된 점수 <span class="math math-inline">s_i</span>가 특정 임계값(예: 0.23) 이상인 점들을 필터링하고, 비최대 억제(Non-Maximum Suppression, NMS)를 적용하여 중복된 파지 포즈를 제거하고 최적의 파지 포즈를 선정한다.</p>
<h3>2.3  AnyGrasp: 시간적 일관성과 동적 파지</h3>
<p>정적인 장면에서의 파지 검출을 넘어, 동적인 환경이나 연속적인 조작을 위해서는 시간적 일관성(Temporal Consistency)이 중요하다. <strong>AnyGrasp</strong> 는 Contact-GraspNet의 아키텍처를 확장하여 시공간적(Spatio-temporal) 도메인에서의 강인한 파지 인식을 구현했다.</p>
<p>AnyGrasp의 핵심 기여는 다음과 같다:</p>
<ol>
<li><strong>시간적 연관성(Temporal Association)</strong>: 연속된 프레임 간의 파지 포즈 대응 관계를 추적하여, 물체가 움직이거나 카메라 시점이 변하더라도 안정적인 파지 타겟을 유지한다.</li>
<li><strong>질량 중심(Center of Mass, CoM) 인식</strong>: 단순히 표면 기하학만 고려하는 것이 아니라, 물체의 대략적인 질량 중심을 추정하여 파지 시 발생할 수 있는 모멘트(Moment)를 최소화하는 파지 점을 우선순위에 둔다. 이는 물리적 안정성을 크게 향상시킨다.</li>
<li><strong>장애물 인식 및 회피</strong>: 파지 공간 내에 그리퍼가 진입할 수 있는 공간이 충분한지를 검사하여 실행 불가능한 파지 후보를 사전에 제거한다.</li>
</ol>
<p>실험 결과, AnyGrasp는 300개 이상의 미지 물체(Unseen Objects)가 쌓여 있는 복잡한 빈 피킹(Bin-picking) 시나리오에서 93.3%의 높은 성공률을 기록하며 인간 수준의 수행 능력을 보여주었다.</p>
<h3>2.4  해석 가능한 어포던스: 프로토타입 기반 학습 (Prototypical Learning)</h3>
<p>기존의 포인트 클라우드 네트워크들이 ’블랙박스’로 동작하여 의사결정의 근거를 알기 어렵다는 점은 안전이 중요한 로봇 공학에서 큰 약점이다. 이를 해결하기 위해 **프로토타입 기반 학습(Prototypical Learning)**이 3D 어포던스에 도입되었다.</p>
<p>이 접근법은 네트워크가 입력 포인트 클라우드의 특징을 사전에 학습된 ’어포던스 프로토타입(Affordance Prototype)’들과 비교하는 방식이다. 예를 들어, 네트워크는 입력된 컵의 손잡이 부분을 보고 “이 부분은 내가 알고 있는 ‘잡기(Graspable)’ 프로토타입과 90% 유사하다“라고 판단한다. 이러한 사례 기반 추론(Case-based Reasoning) 방식은 로봇이 왜 해당 지점을 선택했는지 시각적으로 설명할 수 있게 해주며(Explainability), 소량의 데이터로도 새로운 범주의 물체에 적응하는 퓨샷 학습(Few-shot Learning) 성능을 향상시킨다.</p>
<h3>2.5  포인트 클라우드 접근법의 한계</h3>
<p>포인트 클라우드 기반 방식은 다음과 같은 본질적인 한계를 가진다:</p>
<ul>
<li><strong>해상도 의존성</strong>: 센서의 해상도나 거리에 따라 포인트 밀도가 달라지며, 밀도가 낮은 영역에서는 파지 예측의 신뢰도가 급격히 떨어진다.</li>
<li><strong>불완전한 기하 정보</strong>: 표면의 법선 벡터나 곡률을 점들의 이웃 관계로부터 추정해야 하므로 노이즈에 민감하며, 미분 가능한 정밀 제어에 필요한 연속적인 표면 정보를 제공하지 못한다.</li>
<li><strong>위상학적 모호성</strong>: 점들의 집합만으로는 물체의 내부와 외부를 명확히 구분하기 어렵다(Watertight Mesh가 아님).</li>
</ul>
<p>이러한 한계를 극복하기 위해 연속적인 공간 표현을 사용하는 임플리싯 함수 기반 접근법이 대두되었다.</p>
<h2>3.  암시적 표현의 혁명: 뉴럴 필드와 연속적 어포던스</h2>
<p>임플리싯 함수(Implicit Function) 표현은 3D 공간을 이산적인 점들의 집합이 아닌, 연속적인 필드(Field)로 정의한다. 이는 3D 딥러닝에서 <strong>뉴럴 필드(Neural Fields)</strong> 또는 **좌표 기반 신경망(Coordinate-based Neural Networks)**으로 구현되며, 로봇이 물체의 형상과 어포던스를 해상도의 제약 없이 정밀하게 이해할 수 있는 수학적 토대를 제공한다.</p>
<h3>3.1  임플리싯 함수(Implicit Functions)의 수학적 기초</h3>
<p>임플리싯 표현은 일반적으로 다음과 같은 함수 <span class="math math-inline">f_\theta</span>로 모델링된다:<br />
<span class="math math-display">
f_\theta : \mathbb{R}^3 \rightarrow \mathbb{R}^n
</span><br />
여기서 입력은 3D 좌표 <span class="math math-inline">x = (x, y, z)</span>이며, 출력은 해당 위치의 기하학적 또는 의미론적 속성이다. 로봇 공학에서 주로 사용되는 형태는 다음과 같다:</p>
<ol>
<li><strong>점유 네트워크(Occupancy Networks)</strong>: <span class="math math-inline">f(x) \in \{0, 1\}</span>. 해당 좌표가 물체 내부인지 외부인지를 나타내는 이진 값을 출력한다.</li>
<li><strong>부호 거리 함수(Signed Distance Functions, SDF)</strong>: <span class="math math-inline">f(x) = s \in \mathbb{R}</span>. 해당 좌표에서 가장 가까운 물체 표면까지의 거리를 출력하며, 부호(Sign)는 내부/외부를 나타낸다. 표면은 <span class="math math-inline">f(x)=0</span>인 등위면(Zero Level Set)으로 정의된다.</li>
<li><strong>뉴럴 래디언스 필드(NeRF)</strong>: <span class="math math-inline">f(x, d) \rightarrow (c, \sigma)</span>. 위치와 시점 방향(View Direction)을 입력으로 받아 색상(Color)과 밀도(Density)를 출력하여, 사실적인 렌더링을 가능하게 한다.</li>
</ol>
<p>이러한 연속적 표현은 미분 가능(Differentiable)하다는 강력한 장점을 가진다. 즉, 로봇의 제어 입력에 대한 그래디언트(Gradient)를 계산하여, 파지 위치를 최적화하거나 충돌을 회피하는 궤적을 생성하는 데 직접적으로 활용될 수 있다.</p>
<h3>3.2  Neural Descriptor Fields (NDF): 카테고리 수준의 일반화</h3>
<p><strong>Neural Descriptor Fields (NDF)</strong> 는 임플리싯 함수를 로봇 조작에 적용하여 물체 범주 내의 일반화(Intra-category Generalization) 문제를 해결한 획기적인 연구이다. NDF는 단순히 공간의 점유 여부만을 출력하는 것이 아니라, 해당 위치의 기하학적 문맥(Geometric Context)을 함축하는 고차원 **기술자(Descriptor)**를 출력한다.</p>
<h4>3.2.1  NDF의 작동 원리</h4>
<p>NDF는 물체의 부분 포인트 클라우드 <span class="math math-inline">P</span>를 조건(Condition)으로 받아, 3D 좌표 <span class="math math-inline">x</span>를 기술자 벡터 <span class="math math-inline">z</span>로 매핑한다.<br />
<span class="math math-display">
f(x \vert P) \rightarrow z \in \mathbb{R}^d
</span><br />
NDF의 가장 핵심적인 특성은 **SE(3) 등변성(Equivariance)**이다. 만약 물체 <span class="math math-inline">P</span>가 <span class="math math-inline">T \in SE(3)</span>만큼 회전하고 이동하여 <span class="math math-inline">TP</span>가 되었다면, 변환된 좌표 <span class="math math-inline">Tx</span>에서의 기술자 값은 변환 전 좌표 <span class="math math-inline">x</span>에서의 기술자 값과 동일해야 한다(혹은 예측 가능한 방식으로 변환되어야 한다).<br />
<span class="math math-display">
f(Tx \vert TP) \approx f(x \vert P)
</span><br />
이러한 등변성은 로봇이 한 번도 본 적 없는 포즈(Unseen Pose)로 놓인 물체를 조작할 때 강력한 위력을 발휘한다.</p>
<h4>3.2.2  원샷(One-shot) 학습과 매니퓰레이션</h4>
<p>NDF를 활용한 로봇 조작은 **데모 기반 학습(Demonstration-based Learning)**으로 이루어진다.</p>
<ol>
<li><strong>데모 단계</strong>: 사용자가 특정 물체(예: 머그컵 A)의 손잡이를 잡는 시범을 보인다. 이때 로봇은 파지 위치 <span class="math math-inline">x_{grasp}</span>에서의 기술자 <span class="math math-inline">z_{target} = f(x_{grasp} \vert P_A)</span>를 기억한다.</li>
<li><strong>실행 단계</strong>: 새로운 물체(예: 머그컵 B)가 임의의 포즈로 놓여 있다. 로봇은 머그컵 B의 주변 공간을 탐색(Optimization)하여, 기억해 둔 <span class="math math-inline">z_{target}</span>과 가장 유사한 기술자 값을 가지는 위치 <span class="math math-inline">x^*</span>를 찾는다.</li>
</ol>
<p><span class="math math-display">
x^* = \underset{x}{\text{argmin}} \| f(x \vert P_B) - z_{target} \|
</span></p>
<p>이 과정은 명시적인 키포인트 검출이나 6D 포즈 추정 없이도, 기하학적 유사성에 기반하여 자연스럽게 파지 위치를 찾아낸다. 실험 결과, NDF는 단 5~10회의 데모만으로도 훈련되지 않은 새로운 물체 인스턴스에 대해 높은 성공률로 조작을 수행함을 입증했다.</p>
<h3>3.3  GIGA: 기하학과 어포던스의 시너지 (Synergy)</h3>
<p><strong>GIGA (Grasp detection via Implicit Geometry and Affordance)</strong> 는 임플리싯 표현을 활용하여 3D 형상 재구성(Reconstruction)과 파지 검출(Grasp Detection)을 동시에 수행하는 멀티태스크 학습 프레임워크이다. GIGA의 핵심 가설은 “물체의 형상을 정확히 이해하는 것이 파지 성능을 높이는 데 기여한다“는 것이다.</p>
<h4>3.3.1  구조적 특징: TSDF와 특징 그리드</h4>
<p>GIGA는 깊이 카메라로부터 융합된 <strong>TSDF(Truncated Signed Distance Function)</strong> 복셀 그리드를 입력으로 받는다. 3D CNN을 통해 이를 처리하여 **구조화된 특징 그리드(Structured Feature Grid)**를 생성한다. 임의의 3D 좌표 <span class="math math-inline">x</span>에 대해, GIGA는 이 특징 그리드로부터 삼선형 보간(Trilinear Interpolation)을 통해 로컬 특징을 추출하고, 이를 두 개의 병렬 헤드(Head)로 보낸다:</p>
<ol>
<li><strong>Geometry Head</strong>: 해당 위치의 점유 확률(Occupancy)을 예측하여 물체의 형상을 재구성한다.</li>
<li><strong>Affordance Head</strong>: 해당 위치에서의 파지 품질(Quality), 방향(Orientation), 폭(Width)을 예측한다.</li>
</ol>
<h4>3.3.2  시너지 효과와 손실 함수</h4>
<p>GIGA는 기하학적 재구성 손실 <span class="math math-inline">\mathcal{L}*{geo}</span>와 파지 검출 손실 <span class="math math-inline">\mathcal{L}*{grasp}</span>를 동시에 최적화한다.<br />
<span class="math math-display">
\mathcal{L}_{total} = \mathcal{L}_{grasp} + \lambda \mathcal{L}_{geo}
</span><br />
연구 결과, 기하학적 재구성을 보조 작업(Auxiliary Task)으로 함께 학습했을 때 파지 검출 성능이 비약적으로 향상되었다. 특히 부분적으로 가려진(Occluded) 물체의 경우, 임플리싯 네트워크가 보이지 않는 뒷면의 형상을 채워 넣음으로써(Shape Completion), 더욱 안정적이고 물리적으로 타당한 파지 점을 제안할 수 있다. 이는 로봇이 불완전한 시각 정보만으로도 “저 컵의 뒷면에 손잡이가 있을 것이다“라고 추론하며 파지 계획을 세우는 것과 유사하다.</p>
<h3>3.4  Neural Grasp Distance Fields (NGDF)와 궤적 최적화</h3>
<p><strong>NGDF (Neural Grasp Distance Fields)</strong> 는 파지 문제를 거리 함수 학습 문제로 변환한다. NGDF는 로봇의 엔드 이펙터 포즈 <span class="math math-inline">T_{ee}</span>를 입력으로 받아, ’유효한 파지 매니폴드(Valid Grasp Manifold)’까지의 거리 <span class="math math-inline">d(T_{ee})</span>를 출력한다.</p>
<p>이 거리 함수는 미분 가능하기 때문에, 파지 계획(Grasp Planning)을 최적화 문제로 풀 수 있다. 즉, 로봇의 궤적 생성기(Trajectory Optimizer)에 <span class="math math-inline">d(T_{ee})</span>를 비용 함수(Cost Function)로 추가하여, 장애물을 회피하면서 자연스럽게 파지 위치로 수렴하는 궤적을 생성한다. 이는 이산적인 파지 후보군을 생성하고 역운동학(IK)을 통해 도달 가능성을 검증하는 기존의 순차적 방식보다 훨씬 유연하고 매끄러운 동작을 생성한다.</p>
<h2>4.  하이브리드 아키텍처와 최신 표현 기술</h2>
<p>최근 연구 동향은 포인트 클라우드의 효율성과 임플리싯 함수의 정밀함, 그리고 대규모 언어 모델(LLM)의 의미론적 추론 능력을 결합하는 하이브리드 방식으로 나아가고 있다.</p>
<h3>4.1  3D Gaussian Splatting과 어포던스: GaussianGrasper</h3>
<p>2023년 등장한 **3D Gaussian Splatting (3DGS)**은 NeRF의 렌더링 품질을 유지하면서도 실시간 학습과 렌더링을 가능하게 한 혁신적인 기술이다. <strong>GaussianGrasper</strong> 는 이 기술을 로봇 조작에 접목시킨 선구적인 연구이다.</p>
<h4>4.1.1  명시적-암시적 하이브리드 표현</h4>
<p>GaussianGrasper는 장면을 수만 개의 **3D 가우시안 타원체(Gaussian Ellipsoids)**로 표현한다. 각 가우시안은 위치, 공분산(크기 및 방향), 불투명도, 색상 정보를 가진다. 이는 포인트 클라우드처럼 명시적(Explicit)이면서도, 렌더링 시에는 미분 가능한 스플래팅(Splatting)을 통해 연속적인(Implicit-like) 이미지를 생성한다.</p>
<h4>4.1.2  언어 지시와 특징 증류(Feature Distillation)</h4>
<p>GaussianGrasper의 핵심은 시각적 정보뿐만 아니라 언어적 의미를 3D 공간에 주입한다는 점이다. CLIP이나 DINO와 같은 비전-언어 모델(VLM)의 특징을 가우시안의 속성으로 함께 학습시킨다(Efficient Feature Distillation, EFD). 이를 통해 사용자가 “드라이버의 손잡이를 잡아라(Grasp the handle of the screwdriver)“라고 자연어로 명령하면, 로봇은 3D 공간상에서 ’손잡이’에 해당하는 가우시안들을 활성화하고, 해당 영역의 기하학적 정보(Normal, Depth)를 바탕으로 충돌 없는 정밀한 파지 포즈를 생성한다.</p>
<p>이 방식은 NeRF 기반 방식보다 추론 속도가 월등히 빠르며, 포인트 클라우드보다 훨씬 풍부한 표면 세부 정보를 제공하여 마찰력이 필요한 정밀 파지에 유리하다.</p>
<h3>4.2  시각과 촉각의 통합: NeuralTouch</h3>
<p>시각적 어포던스만으로는 접촉 시점의 미세한 물리적 상호작용을 완벽히 제어하기 어렵다. <strong>NeuralTouch</strong> 는 NDF 기반의 시각적 추론에 <strong>촉각(Tactile)</strong> 센싱을 결합한 하이브리드 프레임워크이다.</p>
<ol>
<li><strong>Coarse Phase (시각)</strong>: NDF를 사용하여 물체의 대략적인 파지 포즈와 특징을 매칭한다. 이는 시각 정보에 기반한 전역적인 접근이다.</li>
<li><strong>Fine Phase (촉각)</strong>: 로봇이 물체에 접근하여 접촉하는 순간, 촉각 센서(예: DIGIT, GelSight)로부터 얻은 국소적인 변형 정보를 바탕으로 파지 포즈를 미세 조정한다.</li>
</ol>
<p>이때 NDF의 기술자(Descriptor)가 시각과 촉각이라는 이질적인 모달리티(Modality)를 연결하는 공통의 ‘언어’ 역할을 한다. 즉, 촉각 센서로 느껴지는 표면 곡률 정보가 NDF 상의 특정 위치 기술자와 일치하는지를 확인함으로써, 로봇은 눈을 감고도 손끝의 감각만으로 “내가 지금 손잡이의 정중앙을 잡고 있구나“를 인지하고 제어할 수 있게 된다.</p>
<h3>4.3  확산 모델 기반의 어포던스 생성 (Diffusion Policies)</h3>
<p>생성형 AI의 발전으로 확산 모델(Diffusion Model)이 로봇 행동 생성에도 적용되고 있다. <strong>ADP3 (Affordance-guided 3D Diffusion Policy)</strong> 는 포인트 클라우드 입력에 어포던스 히트맵을 조건부(Conditioning)로 주입하여 6-DoF 행동 궤적을 생성한다.</p>
<p>기존의 결정론적(Deterministic) 정책이 단 하나의 ‘최적’ 파지 포즈만을 예측하려 했다면, 확산 모델은 파지 가능한 모든 포즈의 **분포(Distribution)**를 학습한다. 이는 다중 모드(Multi-modal) 특성을 가진 실제 환경(예: 컵을 잡는 방법은 손잡이를 잡을 수도, 몸통을 잡을 수도 있음)에서 훨씬 유연하고 강인한 대응을 가능하게 한다. 특히 ADP3는 시각적 허위 상관관계(Spurious Correlation)에 빠지기 쉬운 순수 딥러닝 정책의 약점을 어포던스 정보를 통해 보정하여, 낯선 환경에서도 높은 일반화 성능을 보여준다.</p>
<h2>5.  비교 분석 및 시스템 구현 전략</h2>
<p>지금까지 살펴본 기술들을 실제 로봇 시스템에 적용하기 위해서는 각 방법론의 특성을 비교하고 적절한 선택을 해야 한다.</p>
<h3>5.1  성능 및 특성 비교</h3>
<table><thead><tr><th><strong>특성</strong></th><th><strong>포인트 클라우드 (Explicit)</strong></th><th><strong>임플리싯 함수 (Implicit)</strong></th><th><strong>하이브리드 (Gaussian Splatting 등)</strong></th></tr></thead><tbody>
<tr><td><strong>대표 모델</strong></td><td>Contact-GraspNet, AnyGrasp</td><td>NDF, GIGA, DeepSDF</td><td>GaussianGrasper, NeuralTouch</td></tr>
<tr><td><strong>입력 데이터</strong></td><td>RGB-D Point Cloud</td><td>Point Cloud, Voxel Grid</td><td>RGB Images, Sparse Point Cloud</td></tr>
<tr><td><strong>공간 해상도</strong></td><td>센서 의존적 (거리 비례)</td><td>이론적 무한 (연속적)</td><td>가변적 (가우시안 밀도)</td></tr>
<tr><td><strong>추론 속도</strong></td><td>매우 빠름 (&lt;100ms)</td><td>느림 (최적화/쿼리 필요)</td><td>빠름 (실시간 렌더링 가능)</td></tr>
<tr><td><strong>메모리 효율</strong></td><td>높음 (표면 점만 저장)</td><td>중간 (네트워크 파라미터)</td><td>낮음 (대량의 가우시안 파라미터)</td></tr>
<tr><td><strong>미분 가능성</strong></td><td>제한적</td><td>완전 미분 가능</td><td>미분 가능 (Splatting)</td></tr>
<tr><td><strong>주요 강점</strong></td><td>실시간성, 성숙된 생태계</td><td>형상 보완, 카테고리 일반화</td><td>텍스처/기하 통합, 언어 접지</td></tr>
<tr><td><strong>주요 약점</strong></td><td>노이즈 민감, 위상 부재</td><td>학습/추론 시간 소요</td><td>높은 VRAM 요구량</td></tr>
</tbody></table>
<h3>5.2  Sim-to-Real 전이 전략</h3>
<p>시뮬레이션에서 학습된 3D 어포던스 모델을 실제 로봇에 적용할 때 가장 큰 장벽은 <strong>Sim-to-Real Gap</strong>이다.</p>
<ul>
<li><strong>포인트 클라우드</strong>: 도메인 무작위화(Domain Randomization)와 센서 노이즈 모델링(Depth Noise Simulation)이 필수적이다. 가상 깊이 카메라에 현실적인 노이즈 패턴(Kinect, RealSense 패턴 등)을 주입하여 학습시킨다.</li>
<li><strong>임플리싯 함수</strong>: 기하학적 일관성에 의존하므로 포인트 클라우드 방식보다는 도메인 격차에 강인한 편이다. NDF의 경우 시뮬레이션 데이터만으로 학습해도 실제 물체에 잘 작동하는 경향이 있다. 그러나 실제 환경의 복잡한 배경(Background Clutter)을 처리하기 위해 세그멘테이션 전처리가 중요한 경우가 많다.</li>
</ul>
<h3>5.3  데이터셋의 역할</h3>
<p>이러한 모델들의 성능은 데이터셋의 품질과 규모에 비례한다.</p>
<ul>
<li><strong>Acronym</strong>: 수천 개의 물체와 수백만 개의 파지 포즈를 포함하며, 물체의 메쉬(Mesh)와 물리적 파지 성공 여부를 포함한다.</li>
<li><strong>GraspNet-1Billion</strong>: 190개의 물체에 대해 10억 개 이상의 파지 라벨을 제공하며, 클러터 환경에서의 강인한 학습을 위한 표준 벤치마크로 자리 잡았다.</li>
<li><strong>PartNet-Mobility</strong>: 관절이 있는 물체(Articulated Objects)의 어포던스를 학습하기 위한 데이터셋으로, FlowBot3D와 같은 동적 어포던스 연구에 활용된다.</li>
</ul>
<h2>6.  결론 및 향후 전망</h2>
<p>3D 어포던스와 접촉 예측 기술은 <strong>‘이산적인 점(Point)에서 연속적인 장(Field)으로’</strong>, 그리고 <strong>‘순수 기하학(Geometry-only)에서 의미론적 기하학(Semantic Geometry)으로’</strong> 진화하고 있다.</p>
<ol>
<li>**포인트 클라우드 기반 접근(Contact-GraspNet 등)**은 직접적이고 효율적인 처리가 가능하여 현재 산업 현장과 실시간 물류 시스템에서 가장 강력하고 실용적인 솔루션으로 자리 잡았다. 로컬 특징 추출 능력의 고도화로 복잡한 환경에서의 파지 성공률은 이미 인간 수준에 근접했다.</li>
<li>**임플리싯 함수 기반 접근(NDF, GIGA 등)**은 해상도의 제약을 넘어서고, 물체의 가려진 부분을 추론하며, 카테고리 수준의 일반화를 가능하게 함으로써 로봇에게 낯선 물체에도 유연하게 대처할 수 있는 ’지능적 적응력’을 부여했다.</li>
<li><strong>미래의 방향</strong>은 3D Gaussian Splatting과 같이 명시적 장점과 암시적 장점을 결합한 하이브리드 표현이 주류가 될 것이며, 여기에 거대 언어 모델(LLM/VLM)의 상식 추론 능력을 3D 물리 공간에 접지(Grounding)시키는 연구가 가속화될 것이다.</li>
</ol>
<p>결국 로봇에게 3D 어포던스를 가르친다는 것은, 로봇이 세상을 단순히 관찰하는 것을 넘어, 자신의 신체(Embodiment)를 통해 세상과 어떻게 물리적으로 소통하고 변화시킬 수 있는지를 본질적으로 이해시키는 과정이다. 이것이 바로 Embodied AI가 실험실을 벗어나 복잡한 현실 세계(Wild World)로 나아가기 위한 가장 중요한 교두보이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Learning Spatial Affordances From 3D Point Clouds for Mapping …, https://research.usc.edu.au/view/pdfCoverPage?instCode=61USC_INST&amp;filePid=13270851890002621&amp;download=true</li>
<li>[2504.18355] Interpretable Affordance Detection on 3D Point Clouds …, https://arxiv.org/abs/2504.18355</li>
<li>Generalizable 3D Affordance Learning with Cross-Modal Consistency, https://openaccess.thecvf.com/content/CVPR2025/papers/Lu_GEAL_Generalizable_3D_Affordance_Learning_with_Cross-Modal_Consistency_CVPR_2025_paper.pdf</li>
<li>arXiv:2303.10437v1 [cs.CV] 18 Mar 2023, https://arxiv.org/pdf/2303.10437</li>
<li>Grasp Transfer based on Self-Aligning Implicit Representations of …, https://research.chalmers.se/publication/537251/file/537251_Fulltext.pdf</li>
<li>Enriching Point Clouds with Implicit Representations for 3D … - MDPI, https://www.mdpi.com/2072-4292/15/1/61</li>
<li>Neural Fields in Robotics: A Survey - arXiv, https://arxiv.org/html/2410.20220v1</li>
<li>Learning Implicit Representations for Grasps of Multiple Robotic …, https://proceedings.mlr.press/v205/khargonkar23a/khargonkar23a.pdf</li>
<li>learning continuous grasping function with - OpenReview, https://openreview.net/pdf/fe8b905f6e06714a0cf8a556d38ee543661d5b31.pdf</li>
<li>Neural Grasp Distance Fields for Robot Manipulation, https://par.nsf.gov/servlets/purl/10433352</li>
<li>PointNet: Deep Learning on Point Sets for 3D Classification and …, https://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf</li>
<li>PointNet++ - A Point-based Architecture for 3D Point Cloud - Medium, https://medium.com/@nikitamalviya/pointnet-a-point-based-architecture-for-3d-point-cloud-data-32c1e7a16b84</li>
<li>Research on PointPillars Algorithm Based on Feature-Enhanced …, https://www.mdpi.com/2079-9292/13/7/1233</li>
<li>ContactHandover: Contact-Guided Robot-to-Human Object Handover, https://arxiv.org/html/2404.01402v2</li>
<li>AnyGrasp: Robust and Efficient Grasp Perception in Spatial and …, https://ieeexplore.ieee.org/iel7/8860/4359257/10167687.pdf</li>
<li>Grasping Field: Implicit Human Grasps - Emergent Mind, https://www.emergentmind.com/papers/2008.04451</li>
<li>Scene-level Sequential Affordance Reasoning on 3D Gaussian …, https://arxiv.org/html/2507.23772v1</li>
<li>Object and Contact Point Tracking in Demonstrations Using 3D …, https://arxiv.org/html/2411.03555v1</li>
<li>Contact-graspnet: Efficient 6-dof grasp generation in cluttered scenes, https://elib.dlr.de/145798/1/Contact-GraspNet.pdf</li>
<li>GraspNet通用物体抓取, https://graspnet.net/</li>
<li>AnyGrasp: Robust and Efficient Grasp Perception in Spatial and …, https://ieeexplore.ieee.org/document/10167687/</li>
<li>Any Grasp | PDF | Matrix (Mathematics) | Euclidean Vector - Scribd, https://www.scribd.com/document/864942958/AnyGrasp</li>
<li>MIT Open Access Articles Neural Descriptor Fields: SE(3), https://dspace.mit.edu/bitstream/handle/1721.1/150404/2112.05124.pdf?sequence=2&amp;isAllowed=y</li>
<li>Learning Grasp Generators from Partial Point Clouds with Diffusion …, https://arxiv.org/html/2412.08398v1</li>
<li>Neural Grasp Distance Fields for Robot Manipulation - arXiv, https://arxiv.org/html/2211.02647v3</li>
<li>Neural Descriptor Fields: SE(3)-Equivariant Object … - Neural Fields, https://neuralfields.cs.brown.edu/paper_310.html</li>
<li>Local Neural Descriptor Fields - IEEE Xplore, https://ieeexplore.ieee.org/iel7/10160211/10160212/10160423.pdf</li>
<li>1월 31, 2026에 액세스, [https://sites.google.com/view/rpl-giga2021/#:<sub>:text=Model%20architecture%20of%20Grasp%20detection,aggregated%20into%202D%20feature%20grids.](https://sites.google.com/view/rpl-giga2021/#:</sub>:text=Model architecture of Grasp detection, <a href="https://sites.google.com/view/rpl-giga2021/#:~:text=Model%20architecture%20of%20Grasp%20detection,aggregated%20into%202D%20feature%20grids.">https://sites.google.com/view/rpl-giga2021/#:~:text=Model%20architecture%20of%20Grasp%20detection,aggregated%20into%202D%20feature%20grids.</a></li>
<li>GIGA, https://sites.google.com/view/rpl-giga2021/</li>
<li>6-DoF Grasp Detection via Implicit Representations - Robotics, https://www.roboticsproceedings.org/rss17/p024.pdf</li>
<li>CenterGrasp: Object-Aware Implicit Representation Learning for …, https://arxiv.org/html/2312.08240v2</li>
<li>AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary …, https://www.researchgate.net/publication/398057884_AffordGrasp_In-Context_Affordance_Reasoning_for_Open-Vocabulary_Task-Oriented_Grasping_in_Clutter</li>
<li>GaussianGrasper - Yuhang Zheng, https://mrsecant.github.io/GaussianGrasper/</li>
<li>3D Language Gaussian Splatting for Open-vocabulary Robotic …, https://arxiv.org/html/2403.09637v1</li>
<li>3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians, https://arxiv.org/html/2504.11218v2</li>
<li>Neural Descriptor Fields: SE(3)-Equivariant Object Representations …, https://www.researchgate.net/publication/361957561_Neural_Descriptor_Fields_SE3-Equivariant_Object_Representations_for_Manipulation</li>
<li>NeuralTouch: Leveraging Implicit Neural Descriptor for Precise Sim …, https://openreview.net/pdf/6d0162252e0c965aa16c01fc8060132074e1b4f8.pdf</li>
<li>Neural Descriptors for Precise Sim-to-Real Tactile Robot Control, https://arxiv.org/html/2510.20390v1</li>
<li>Neural Descriptors for Precise Sim-to-Real Tactile Robot Control, https://arxiv.org/pdf/2510.20390</li>
<li>Neural Contact Fields: Tracking Extrinsic Contact with Tactile Sensing, https://www.researchgate.net/publication/372117022_Neural_Contact_Fields_Tracking_Extrinsic_Contact_with_Tactile_Sensing</li>
<li>ADP3: Affordance-Guided Generalizable Visuomotor Policies …, https://repository.tudelft.nl/file/File_1b32febe-1662-4e1c-9212-e1e8ed20054d?preview=1</li>
<li>Robotics Dexterous Grasping: The Methods Based on Point Cloud …, https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2021.658280/full</li>
<li>Enriching Point Clouds with Implicit Representations for 3D …, https://3d.bk.tudelft.nl/liangliang/publications/2023/implicit/implicit.pdf</li>
<li>GraspClutter6D: A Large-scale Real-world Dataset for Robust … - arXiv, https://arxiv.org/pdf/2504.06866</li>
<li>FlowBot3D: Learning 3D Articulation Flow to Manipulate … - arXiv, https://arxiv.org/html/2205.04382v6</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>