<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.4.3 인간 비디오로부터의 어포던스 추출: HOI(Human-Object Interaction) 분석과 전이</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.4.3 인간 비디오로부터의 어포던스 추출: HOI(Human-Object Interaction) 분석과 전이</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <a href="index.html">7.4 시각적 어포던스 학습 (Visual Affordance Learning)</a> / <span>7.4.3 인간 비디오로부터의 어포던스 추출: HOI(Human-Object Interaction) 분석과 전이</span></nav>
                </div>
            </header>
            <article>
                <h1>7.4.3 인간 비디오로부터의 어포던스 추출: HOI(Human-Object Interaction) 분석과 전이</h1>
<h2>1.  서론: 행동 가능한 시각 정보와 신체 불일치(Embodiment Gap)의 극복</h2>
<p>로봇 공학이 직면한 가장 근본적인 난제 중 하나는 ’데이터의 부재’이다. 컴퓨터 비전이나 자연어 처리(NLP) 분야가 웹 스케일의 방대한 데이터셋을 통해 비약적인 발전을 이룩한 것과 대조적으로, 로봇 학습을 위한 양질의 상호작용 데이터는 여전히 희소하다. 전통적으로 로봇을 학습시키는 방식은 인간이 직접 로봇을 원격 조작(Teleoperation)하여 데이터를 수집하거나, 물리 시뮬레이터 내부에서 수많은 시행착오를 거치는 강화학습(Reinforcement Learning)에 의존해 왔다. 그러나 이러한 방식은 확장성(Scalability) 측면에서 치명적인 한계를 가진다. 원격 조작은 시간과 비용이 많이 소요되며, 시뮬레이션은 현실 세계의 복잡하고 미묘한 물리적 특성—마찰, 변형, 유체 역학 등—을 완벽하게 모사하지 못해 ‘Sim-to-Real’ 격차를 발생시킨다.</p>
<p>이러한 상황에서 연구자들은 인터넷상에 무한히 존재하는 <strong>인간 비디오(Human Video)</strong> 데이터에 주목하기 시작했다. 유튜브(YouTube)의 “How-to” 영상, 브이로그(Vlog), 그리고 최근 구축된 Ego4D나 EPIC-Kitchens와 같은 대규모 1인칭 시점(Egocentric) 데이터셋은 인간이 세상과 어떻게 상호작용하는지를 보여주는 거대한 지식의 보고이다. 인간은 별도의 명시적인 지시 없이도 타인의 행동을 관찰하는 것만으로 새로운 도구의 사용법을 익히고, 물체의 조작 방식을 유추할 수 있다. 이와 유사하게, 로봇이 인간의 비디오를 보고 물리적 상호작용의 원리를 학습할 수 있다면, 데이터 기근 문제를 해결하고 범용 로봇(General Purpose Robot)을 향한 중요한 도약이 가능해진다.</p>
<p>하지만 인간의 행동을 로봇에게 그대로 적용하는 것은 단순한 모방 이상의 문제를 내포한다. 이를 <strong>신체 불일치(Embodiment Gap)</strong> 문제라고 정의한다. 인간의 손은 수십 개의 관절과 정교한 촉각 센서, 유연한 피부를 가진 반면, 로봇의 그리퍼는 제한된 자유도(DoF)와 딱딱한 표면을 가진 경우가 많다. 따라서 인간 비디오의 픽셀 정보를 로봇의 관절 토크(Torque)나 속도 명령으로 직접 매핑하는 것은 불가능에 가깝다. 이 간극을 메우기 위해서는 픽셀과 제어 명령 사이를 연결하는 중간 매개체, 즉 <strong>행동 가능한 표현(Actionable Representation)</strong> 이 필요하다.</p>
<p>이 지점에서 제임스 깁슨(J.J. Gibson)이 제창한 <strong>어포던스(Affordance)</strong> 개념이 현대적 로봇 학습의 맥락에서 재정의된다. 깁슨의 생태심리학적 정의가 “환경이 유기체에게 제공하는 행동의 가능성“이라는 추상적인 개념이었다면, 컴퓨터 비전과 로봇 공학에서의 시각적 어포던스(Visual Affordance)는 계산 가능하고 실행 가능한 구체적인 데이터 구조로 변환된다. 2024년과 2025년의 최신 연구들은 어포던스를 단순한 의미론적 라벨(예: “앉을 수 있음”, “잡을 수 있음”)로 분류하는 것을 넘어, <strong>“어디를(Where)”, “어떻게(How)”, “어떤 궤적(Trajectory)으로”</strong> 상호작용해야 하는지에 대한 기하학적이고 동역학적인 정보로 추출하는 데 집중하고 있다.</p>
<p>본 장에서는 인간 비디오로부터 이러한 고수준의 어포던스를 추출하고, 이를 로봇의 제어 정책으로 전이(Transfer)하는 최신 방법론들을 심층적으로 분석한다. 정적인 이미지 분석을 넘어선 시공간적(Spatio-Temporal) HOI 탐지 기술, 가림(Occlusion)과 깊이 모호성(Depth Ambiguity)을 극복하기 위한 3D 기하학적 재구성(Reconstruction) 기법, 그리고 서로 다른 신체 구조를 가진 에이전트 간의 기술 전이(Skill Transfer)를 가능하게 하는 크로스-엠바디먼트 알고리즘 등을 포괄적으로 다룬다.</p>
<h2>2.  시공간적 HOI 탐지: 비디오 이해의 진화와 튜브렛 토큰</h2>
<p>인간과 객체의 상호작용(Human-Object Interaction, HOI)은 본질적으로 동적인 과정이다. “컵을 집다(Pick up)”, “물을 따르다(Pour)”, “칼로 썰다(Cut)“와 같은 행위는 단일 프레임의 정지 영상만으로는 그 의도와 상태를 완벽하게 파악하기 어렵다. 예를 들어, 손이 컵에 닿아 있는 정지 영상만으로는 컵을 집어 올리는 중인지, 내려놓는 중인지, 아니면 단순히 손을 대고 있는 것인지 구별할 수 없다. 따라서 최신 HOI 탐지 기술은 시간 축(Temporal Axis)을 포함한 비디오 전체의 맥락을 이해하는 방향으로 진화하고 있다.</p>
<h3>2.1  튜브렛 토큰(Tubelet Tokens)과 시공간적 추상화</h3>
<p>기존의 비디오 비전 트랜스포머(Video Vision Transformer)들은 비디오를 3D 패치(공간 <span class="math math-inline">H \times W</span> + 시간 <span class="math math-inline">T</span>)로 균일하게 분할하여 처리하였다. 그러나 이러한 방식은 계산 비용이 높을 뿐만 아니라, 상호작용의 핵심이 되는 객체나 사람의 움직임을 효과적으로 추적하지 못한다는 단점이 있다. 상호작용은 비디오의 전체 영역에서 발생하는 것이 아니라, 사람의 손과 객체 주변이라는 국소적인 영역에서 시간의 흐름에 따라 연속적으로 발생하기 때문이다.</p>
<p>이를 해결하기 위해 제안된 개념이 <strong>튜브렛 토큰(Tubelet Tokens)</strong> 이다. 튜브렛(Tubelet)은 시간과 공간을 동시에 아우르는 ‘튜브(Tube)’ 형태의 영역을 하나의 토큰으로 추상화한 것이다.</p>
<ul>
<li><strong>생성 메커니즘:</strong> TUTOR(Tubelet Transformer)와 같은 모델은 비디오 내에서 의미론적으로 연관된 패치들을 시공간적으로 연결(Link)하고 집계(Agglomerate)하여 튜브렛을 형성한다. 이는 고정된 그리드(Grid) 방식의 토큰화가 아니라, 움직이는 객체의 궤적을 따라가는 동적인 토큰화를 가능하게 한다.</li>
<li><strong>구조적 장점:</strong></li>
<li><strong>압축성(Compactness):</strong> 배경과 같이 상호작용과 무관한 영역의 정보를 줄이고, 핵심적인 행위가 일어나는 영역에 정보를 집중시킨다.</li>
<li><strong>표현력(Expressiveness):</strong> 시간적 연속성을 내재하고 있어, “접근(Approach) -&gt; 접촉(Contact) -&gt; 조작(Manipulation)“으로 이어지는 상호작용의 단계를 하나의 튜브렛 내에서 혹은 튜브렛 간의 관계로 모델링할 수 있다.</li>
<li><strong>성능 비교:</strong> ST-HOI 와 같은 기존의 베이스라인 모델들은 프레임별 탐지 결과를 후처리(Post-processing)를 통해 연결하거나, 단순한 3D CNN을 사용하여 시간 정보를 융합하였다. 반면, 튜브렛 기반 접근 방식은 학습 단계에서부터 시공간적 구조를 직접 학습하므로, 복잡한 동작이나 가림이 심한 상황에서도 HOI 탐지의 정확도(mAP)를 비약적으로 향상시켰다.</li>
</ul>
<h3>2.2  시간적 어텐션(Temporal Attention)과 강건성(Robustness)</h3>
<p>실제 환경(In-the-wild)의 비디오는 실험실 환경과 달리 예측 불가능한 변수들로 가득 차 있다. 카메라의 급격한 움직임, 조명의 변화, 모션 블러(Motion Blur), 그리고 무엇보다 빈번한 가림(Occlusion) 현상이 HOI 분석을 방해한다. 특히 1인칭 시점(Egocentric View) 비디오에서는 조작하는 손이 대상 객체를 가리는 ’Self-Occlusion’이 필연적으로 발생한다.</p>
<p>이러한 문제를 극복하기 위해 최신 연구들은 <strong>장기 시간 어텐션(Long-term Temporal Attention)</strong> 과 <strong>기억(Memory)</strong> 메커니즘을 도입하고 있다.</p>
<ul>
<li><strong>기억 기반 복원:</strong> 현재 프레임 <span class="math math-inline">t</span>에서 객체가 손에 가려져 보이지 않더라도, 과거 프레임 <span class="math math-inline">t-k</span> (손이 닿기 전)의 정보를 참조하여 객체의 전체 형상과 위치를 추론한다. 이는 트랜스포머의 Cross-Attention 메커니즘을 시간 축으로 확장하여 구현된다. 모델은 현재의 가려진 쿼리(Query)를 과거의 선명한 키(Key)/값(Value)과 매칭시켜 정보를 복원한다.</li>
<li><strong>RoHOI 벤치마크와 강건성 평가:</strong> 2025년에 제안된 RoHOI(Robustness Benchmark for HOI)는 기존 모델들이 깨끗한(Clean) 데이터셋에서는 높은 성능을 보이지만, 실제 환경의 노이즈에는 취약하다는 점을 지적했다. RoHOI는 20가지 유형의 영상 부패(Corruption)—가우시안 노이즈, 날씨 효과, 디지털 압축 손실 등—를 적용하여 모델의 회복력을 평가한다. 이를 통해 제안된 SAMPL(Sample-Augmented Model Perturbation Learning)과 같은 기법은 확산 모델(Diffusion Model)을 이용해 다양한 악조건의 이미지를 생성하고 이를 학습에 포함시킴으로써, 환경 변화에 강인한 HOI 탐지 모델을 구축하는 방법을 제시한다.</li>
</ul>
<h3>2.3  청각-시각 멀티모달 통합(Audio-Visual Integration)</h3>
<p>시각 정보만으로는 상호작용의 종류를 명확히 구분하기 어려운 경우가 있다. 예를 들어, 손으로 물체를 ‘두드리는’ 것인지 ‘쓰다듬는’ 것인지는 시각적으로 유사할 수 있지만, 발생하는 소리는 완전히 다르다. 2024-2025년의 연구 동향 중 하나는 청각 신호(Audio)를 HOI 탐지의 보조적인, 혹은 핵심적인 단서로 활용하는 것이다.</p>
<ul>
<li><strong>EPIC-SOUNDS:</strong> 이 벤치마크는 비디오 내의 소리를 통해 상호작용을 인식하고, 그 발생 구간(Temporal Localization)을 탐지하는 과제를 제시한다.</li>
<li><strong>상호 보완성:</strong> 시각 정보가 가림이나 조명으로 인해 손실되었을 때, 오디오 신호는 상호작용의 발생 여부와 강도(Magnitude)를 추정하는 데 결정적인 역할을 한다. 딥러닝 모델은 스펙트로그램(Spectrogram)으로 변환된 오디오 특징과 비디오 특징을 융합(Fusion)하여, “칼질 소리“나 “물 따르는 소리“와 같은 청각적 이벤트와 시각적 모션을 결합한 멀티모달 HOI 이해를 수행한다.</li>
</ul>
<h2>3.  3D 기하학과 기능적 부위의 추출: 픽셀에서 복셀로</h2>
<p>2D 이미지 평면에서의 HOI 탐지는 “무엇이(What)” 일어나고 있는지를 알려주지만, 로봇이 실제로 물리적 개입을 하기 위해서는 “어디에(Where)” 3차원적으로 위치하는지에 대한 공간 정보가 필수적이다. 따라서 2D 비디오로부터 3D 손-객체 기하학(Geometry)을 재구성(Reconstruction)하는 기술은 어포던스 전이의 핵심 교두보 역할을 한다.</p>
<h3>3.1  MCC-HO: 검색 증강 재구성(Retrieval-Augmented Reconstruction)</h3>
<p>단안(Monocular) RGB 비디오에서 3D 객체를 복원하는 것은 ’깊이 모호성(Depth Ambiguity)’과 ’보이지 않는 면(Invisible Surface)’의 문제 때문에 매우 어렵다. 특히 손에 파지된 물체는 손가락에 의해 주요 부위가 가려지기 때문에, 전통적인 사진 측량(Photogrammetry) 방식으로는 온전한 3D 모델을 얻을 수 없다.</p>
<p>2024년 발표된 <strong>MCC-HO (Multiview Compressive Coding for Hand-Object)</strong> 는 이러한 한계를 극복하기 위해, 딥러닝 기반의 추론과 데이터베이스 검색을 결합한 하이브리드 접근법을 제안했다.</p>
<table><thead><tr><th><strong>단계</strong></th><th><strong>프로세스 상세</strong></th><th><strong>역할 및 의의</strong></th></tr></thead><tbody>
<tr><td><strong>1. 기하 추정</strong></td><td>입력 이미지와 추정된 3D 손 포즈를 트랜스포머 기반의 MCC 모델에 입력. 쿼리 포인트 <span class="math math-inline">Q(x,y,z)</span>에 대한 점유(Occupancy)와 색상(RGB)을 예측.</td><td>CO3D 등 대규모 3D 데이터셋에서 학습된 ’형상 사전(Shape Prior)’을 활용하여, 가려진 부분이나 보이지 않는 뒷면의 형상을 확률적으로 추론.</td></tr>
<tr><td><strong>2. 검색 증강 (RAR)</strong></td><td>1단계에서 예측된 불완전한 포인트 클라우드를 쿼리로 사용하여, 3D 모델 라이브러리에서 가장 유사한 완성형 3D 메시(Mesh)를 검색(Retrieval).</td><td>불확실한 추론 결과를 고품질의 3D 자산(Asset)으로 대체하거나 보정하여, 로봇이 파지 계획(Grasp Planning)을 수립할 수 있는 완전한 기하 정보 제공.</td></tr>
<tr><td><strong>3. 시간적 정렬</strong></td><td>비디오의 연속 프레임에서 복원된 객체들이 시간적 일관성을 갖도록 정렬. DINOv2와 같은 시각적 특징을 이용해 강체 변환(Rigid Transform) 최적화.</td><td>프레임마다 물체의 크기나 모양이 달라지는 ‘떨림(Jitter)’ 현상을 방지하고, 안정적인 6-DoF 포즈 궤적 생성.</td></tr>
</tbody></table>
<p>이 방법론은 “보이는 것만 복원하는” 기존의 방식에서 벗어나, “보이지 않는 부분은 지식(검색)으로 채우는” 패러다임의 전환을 보여준다. 이는 인간이 물체의 일부분만 보고도 전체 모양을 유추하는 인지 과정과 유사하다.</p>
<h3>3.2  암시적 표현(Implicit Representation)과 접촉 모델링</h3>
<p>복잡한 위상(Topology)을 가진 물체나, 손과 물체가 밀착된 상호작용을 정밀하게 표현하기 위해 메시(Mesh)나 포인트 클라우드 대신 <strong>SDF(Signed Distance Function)</strong> 와 같은 암시적 표현(Implicit Representation)이 주류로 자리 잡고 있다.</p>
<ul>
<li><strong>HOISDF &amp; CenterGrasp:</strong> 이 연구들은 손과 물체의 SDF를 결합하여 전역적인 상호작용 필드(Interaction Field)를 생성한다.. 특히 CenterGrasp는 객체의 중심점을 기준으로 3D 형상뿐만 아니라, 6-DoF 파지(Grasp) 가능성까지 동시에 추론하는 <strong>SGDF(Shape and Grasp Distance Function)</strong> 디코더를 제안하였다. 이는 형상 복원과 파지 감지를 별개의 태스크가 아닌, 상호 보완적인 태스크로 통합했다는 점에서 의의가 있다.</li>
<li><strong>명시적 접촉 학습 (Learning Explicit Contact):</strong>  연구는 단순히 3D 형상만을 복원하는 것이 아니라, 손과 물체의 접촉 영역 자체를 명시적으로 예측하고 이를 3D 재구성의 강력한 제약 조건(Prior)으로 활용한다. 그래프 기반 트랜스포머(Graph-based Transformer)를 사용하여 손의 각 정점(Vertex)과 물체 표면 간의 접촉 확률을 추정하고, 이 정보를 확산(Diffuse)시켜 암시적 형상 복원의 정확도를 높인다. 이는 “접촉이 일어나는 곳에 표면이 반드시 존재한다“는 물리적 사실을 수학적 손실 함수(Loss Function)인 <span class="math math-inline">L_{contact}</span>로 구현하여, 가림이 심한 영역의 복원 성능을 획기적으로 개선하였다.</li>
</ul>
<h3>3.3  야생 데이터셋 구축: Open3DHOI와 PICO-db</h3>
<p>이러한 기술 발전의 배경에는 데이터셋의 진화가 있다. 과거의 데이터셋들이 실내 스튜디오나 제한된 물체군에 머물렀다면, 2025년 공개된 데이터셋들은 ’야생(In-the-wild)’과 ’개방형 어휘(Open-Vocabulary)’를 지향한다.</p>
<ul>
<li>Open3DHOI : 2,500개 이상의 이미지에 대해 고품질 3D 주석을 포함하며, 133개 범주의 객체와 120개 범주의 상호작용을 다룬다. 특히 3D 가우시안 스플래팅(Gaussian Splatting)을 활용한 새로운 최적화 도구를 통해, 2D 이미지로부터 정교한 3D 접촉 관계를 반자동으로 주석화하는 파이프라인을 구축했다.</li>
<li>PICO-db : Vision Foundation Model을 활용해 이미지에서 3D 객체를 검색하고, 적은 횟수의 클릭만으로 신체와 객체 간의 3D 접촉 대응(Correspondence)을 주석화하는 방법론을 제시했다. 이는 데이터 구축 비용을 획기적으로 낮추면서도, 로봇 학습에 필수적인 ’접촉 정보’를 대량으로 확보할 수 있는 길을 열었다.</li>
</ul>
<h2>4.  기능적 어포던스와 파지 전략: 무엇을, 어디를, 왜 잡는가?</h2>
<p>로봇이 물체를 잡을 때, 단순히 물리적으로 안정적인 파지(Stability)만을 고려해서는 안 된다. 물체의 사용 목적(Task)에 부합하는 파지, 즉 <strong>작업 지향적 파지(Task-Oriented Grasping, TOG)</strong> 가 필요하다. 칼은 날이 아니라 손잡이를 잡아야 ’썬다’는 기능을 수행할 수 있고, 머그컵은 몸통을 잡으면 ’건네주기’는 가능하지만 ’마시기’는 어렵다.</p>
<h3>4.1  Aff-Grasp: 기능적 부위의 발견과 활용</h3>
<p><strong>Aff-Grasp</strong> 프레임워크는 이러한 ’기능적 어포던스(Functional Affordance)’를 픽셀 수준에서 찾아내고 이를 로봇의 파지 전략으로 연결한다.</p>
<ul>
<li><strong>기능적 포인트(Functional Point) 국소화:</strong> 도구(Tool)와 객체(Object)의 상호작용 비디오(예: 드라이버로 나사 조이기)에서, 도구가 객체와 접촉하는 지점이 바로 ’기능이 발현되는 지점’이다. Aff-Grasp는 비디오의 나레이션과 어포던스 데이터베이스를 활용해 해당 상호작용 클립을 검색하고, 도구가 객체를 가리기 직전의 <strong>접촉 전 프레임(Pre-contact frame)</strong> 을 찾아내어 기능적 부위를 마스킹한다.</li>
<li><strong>개방형 어휘 탐지:</strong> CLIP이나 SAM(Segment Anything Model)과 같은 대규모 비전 모델을 활용하여, 미리 학습되지 않은 새로운 객체나 “누를 수 있는(Pressable)”, “비틀 수 있는(Twistable)“과 같은 추상적인 어포던스에 대해서도 텍스트 프롬프트를 통해 해당 부위를 세그멘테이션한다.</li>
<li><strong>실행 전략:</strong> 로봇은 탐지된 기능적 부위를 피해서 잡거나(예: 칼날을 피해 손잡이를 잡음), 혹은 기능적 부위를 도구로 타격하는 등의 구체적인 동작 시퀀스를 생성한다. 실험 결과, Aff-Grasp는 95.5%의 높은 어포던스 예측 성공률을 보이며 복잡한 도구 사용 작업에서 뛰어난 성능을 입증했다.</li>
</ul>
<h3>4.2  모션 기반 파트 분할(Motion-based Part Discovery)</h3>
<p>정적인 이미지에서는 하나의 덩어리로 보이는 물체라도, 실제로는 여러 개의 움직이는 부품(Articulated Parts)으로 구성된 경우가 많다. 서랍, 가위, 안경, 노트북 등이 이에 해당한다. 로봇이 이러한 물체를 조작하려면 각 부품의 운동학적 구조(Kinematic Structure)를 이해해야 한다.</p>
<p>최신 연구들은 비디오 내의 <strong>모션 단서(Motion Cues)</strong> 를 활용하여, 감독(Supervision) 없이도 물체의 움직이는 부품을 분할하는 방법을 제안한다.</p>
<ul>
<li><strong>모션 플럭스(Motion Flux)와 궤적 군집화:</strong> 비디오에서 픽셀이나 특징점(Keypoint)의 장기 궤적(Long-term Trajectory)을 추적한다. 같은 강체(Rigid Body)에 속한 점들은 3D 공간에서 동일한 운동 변환을 겪으므로, 궤적들의 상관관계를 분석하여 군집화(Clustering)하면 물체의 부품을 분할할 수 있다.</li>
<li><strong>관절 파라미터 추정:</strong> 분할된 부품들의 상대적인 움직임을 분석하여, 회전 관절(Revolute Joint)인지 병진 관절(Prismatic Joint)인지 판별하고, 관절 축(Axis)과 가동 범위(Range of Motion)를 추정한다. 이는 로봇이 서랍을 열거나 문을 당길 때 필요한 힘의 방향과 궤적을 계획하는 데 필수적인 정보다.</li>
</ul>
<h2>5.  크로스-엠바디먼트 전이 방법론: 인간에서 로봇으로</h2>
<p>인간 비디오에서 추출한 어포던스를 로봇에게 적용하는 과정에서 가장 큰 장벽은 <strong>형태학적 차이(Morphological Discrepancy)</strong> 이다. 인간의 5손가락 동작을 2지 그리퍼나 흡착형 그리퍼(Suction Gripper)를 가진 로봇에게 어떻게 매핑할 것인가?</p>
<h3>5.1  VRB (Vision-Robotics Bridge): 형태 불가지론적(Agnostic) 전이</h3>
<p>카네기 멜론 대학(CMU) 연구진이 제안한 <strong>VRB</strong>는 이 문제를 해결하기 위해, 인간의 신체 동작을 그대로 모방하는 것을 포기하고 <strong>환경 중심적(Environment-centric)</strong> 인 접근을 취한다.</p>
<ul>
<li><strong>핵심 철학:</strong> “로봇이 인간의 팔 관절을 어떻게 움직이는지는 중요하지 않다. 중요한 것은 물체의 ’어디’를 건드려서 ’어떤 방향’으로 변화시켰느냐는 것이다.”</li>
<li><strong>어포던스 표현 (<span class="math math-inline">c, \tau</span>):</strong> VRB는 어포던스를 두 가지 핵심 요소로 정의한다.</li>
</ul>
<ol>
<li><strong>접촉 지점 (<span class="math math-inline">c</span>):</strong> 상호작용이 시작되는 픽셀 좌표. (Where)</li>
<li><strong>접촉 후 궤적 (<span class="math math-inline">\tau</span>):</strong> 접촉 후 손목이 이동하는 3D 벡터 또는 웨이포인트. (How)</li>
</ol>
<ul>
<li><strong>학습 파이프라인:</strong></li>
</ul>
<ol>
<li>Ego4D 등의 대규모 데이터셋에서 손-객체 접촉 순간을 탐지한다.</li>
<li>카메라의 움직임(Egomotion)을 보정한 후, 접촉 시점의 손 위치를 <strong>인간이 없는 초기 프레임(Human-agnostic Frame)</strong> 으로 역투영(Back-projection)한다. 이는 로봇이 실제 환경에서 보게 될 장면(사람이 없는 상태)과 훈련 데이터를 일치시키기 위함이다.</li>
<li>모델은 정적 이미지를 입력받아 접촉 히트맵과 궤적을 예측하도록 학습된다.</li>
</ol>
<ul>
<li><strong>다양한 로봇 학습 패러다임으로의 통합:</strong> 추출된 <span class="math math-inline">(c, \tau)</span> 정보는 모방 학습(Imitation Learning)의 초기화 정책으로 사용되거나, 강화학습(RL)의 탐색 공간을 줄여주는 가이드, 혹은 목표 조건(Goal-conditioned) 학습의 보조 정보로 활용된다. VRB는 서로 다른 그리퍼를 가진 로봇들에서도 높은 성공률을 보이며, 형태 불가지론적 전이의 가능성을 입증했다.</li>
</ul>
<h3>5.2  RTAGrasp: 검색(Retrieval), 전이(Transfer), 정렬(Alignment)</h3>
<p><strong>RTAGrasp</strong> 는 생성 모델 대신 방대한 데이터베이스를 활용하는 비모수적(Non-parametric) 접근 방식을 취한다.</p>
<table><thead><tr><th><strong>단계</strong></th><th><strong>설명</strong></th><th><strong>기술적 요소</strong></th></tr></thead><tbody>
<tr><td><strong>1. 메모리 구축 (Memory)</strong></td><td>인간 비디오에서 성공적인 파지 사례를 추출하여 (객체 이미지, 2D 파지점, 3D 접근 벡터, 텍스트 설명) 형태의 DB 구축.</td><td>HOI 탐지, 손 궤적 추적, GPT-4V를 이용한 텍스트 생성.</td></tr>
<tr><td><strong>2. 검색 (Retrieval)</strong></td><td>로봇에게 태스크(예: “컵 건네주기”)가 주어지면, 현재 관측된 객체와 가장 유사한 인간 파지 사례를 메모리에서 검색.</td><td>멀티모달 임베딩 유사도 계산, 의미론적 매칭.</td></tr>
<tr><td><strong>3. 전이 (Transfer)</strong></td><td>검색된 인간의 파지점을 현재 로봇 시야의 객체로 전이.</td><td>DINOv2와 같은 강건한 특징 매칭(Feature Matching)을 통해 객체의 포즈나 크기가 달라도 대응점을 찾음.</td></tr>
<tr><td><strong>4. 정렬 (Alignment)</strong></td><td>전이된 파지 제약조건(위치, 방향)을 로봇 그리퍼의 운동학적 제약에 맞춰 최적화.</td><td>파지 후보군(Candidates) 샘플링 및 점수화(Scoring).</td></tr>
</tbody></table>
<p>이 방식의 가장 큰 장점은 <strong>Training-free</strong>라는 점이다. 별도의 복잡한 정책 네트워크를 학습시키지 않고도, 단 한 번의 검색과 전이만으로 새로운 물체에 대한 작업 지향적 파지를 수행할 수 있다.</p>
<h3>5.3  리타겟팅(Retargeting)과 어포던스 동등성</h3>
<p>인간의 손(Hand)을 로봇의 손(Hand/Gripper)으로 직접 매핑하는 <strong>리타겟팅</strong> 기술도 정교해지고 있다.</p>
<ul>
<li><strong>기구학적 최적화:</strong> 단순한 관절 각도 매핑(Joint-to-Joint)은 신체 구조 차이로 인해 실패하기 쉽다. 최신 연구 는 <strong>‘손끝 간의 상대적 위치(Relative position among fingertips)’</strong> 와 <strong>‘손끝의 방향(Fingertip orientations)’</strong> 을 유지하는 것이 물체 조작 성공의 핵심임을 밝혀냈다. 즉, 엄지와 검지의 간격이나 접촉면의 각도를 보존하는 방향으로 최적화 문제를 푼다.</li>
<li><strong>어포던스 동등성(Affordance Equivalence):</strong> 서로 다른 로봇이라도 동일한 물체에 대해 동일한 ’효과(Effect)’를 낼 수 있다면, 이들은 잠재 공간(Latent Space)에서 공유된 어포던스 표현을 갖는다고 가정한다. 이를 통해 A 로봇(인간)의 경험을 B 로봇으로 전이하거나, 다수의 로봇 데이터를 통합하여 거대 행동 모델(VLA)을 학습시키는 연구가 활발히 진행 중이다.</li>
</ul>
<hr />
<h2>6.  생성형 모델과 미래의 HOI: 확산과 맘바(Mamba)</h2>
<p>어포던스 추출의 최전선은 이제 분석(Analysis)을 넘어 생성(Generation)의 영역으로 확장되고 있다. 로봇이 “어떻게 잡을까?“를 고민할 때, 가능한 상호작용의 미래를 시뮬레이션해 보는 것이다.</p>
<h3>6.1  확산 모델(Diffusion Model) 기반 합성</h3>
<p><strong>HOI-Diff</strong> 와 <strong>CHOIS</strong> 는 텍스트 프롬프트나 초기 상태를 입력받아, 사실적인 3D 인간-객체 상호작용 모션을 생성한다.</p>
<ul>
<li><strong>기전:</strong> 확산 모델은 노이즈로부터 점진적으로 신호를 복원하며, 이 과정에서 물리적 제약조건(발이 땅에 닿아야 함, 손이 물체를 관통하지 않아야 함)을 가이던스(Guidance)로 주어 물리적으로 타당한 동작을 생성한다.</li>
<li><strong>활용:</strong> 생성된 데이터는 로봇 학습을 위한 합성 데이터(Synthetic Data)로 사용되거나, 로봇이 수행할 동작의 참조 궤적(Reference Trajectory)으로 활용된다.</li>
</ul>
<h3>6.2  차세대 아키텍처: 맘바(Mamba)와 하이브리드 모델</h3>
<p>2025년에는 트랜스포머의 높은 계산 비용(이차 복잡도)을 극복하기 위해 <strong>맘바(Mamba)</strong> 와 같은 상태 공간 모델(State Space Model, SSM)이 HOI 비디오 생성 및 이해에 도입되고 있다. MambaVision 과 같은 하이브리드 아키텍처는 선형 복잡도로 긴 시퀀스의 비디오를 처리할 수 있어, 장기적인 상호작용(Long-horizon Interaction)을 모델링하는 데 유리하다. 이는 로봇이 수 분 이상의 긴 작업을 계획하고 수행하는 데 필수적인 기술적 토대가 될 것이다.</p>
<h3>6.3  결론: 보고, 이해하고, 행동하라</h3>
<p>인간 비디오로부터 어포던스를 추출하는 기술은 로봇 공학의 패러다임을 ’직접 교시(Teaching)’에서 ’관찰 학습(Observational Learning)’으로 전환시키고 있다. 2024-2025년의 연구 성과들은 2D 픽셀 정보를 3D 공간의 행동 가능한 지식으로 변환하는 파이프라인을 완성해 나가고 있다. <strong>VRB</strong>와 같은 형태 불가지론적 표현은 로봇 하드웨어의 제약을 뛰어넘게 해주었고, <strong>MCC-HO</strong>와 <strong>Implicit Representation</strong>은 보이지 않는 정보를 추론하는 능력을 부여했다. 또한 <strong>Open3DHOI</strong>와 같은 야생 데이터셋과 <strong>RTAGrasp</strong> 같은 검색 기반 방법론은 실험실 밖의 복잡한 세상으로 로봇을 이끌고 있다.</p>
<p>이제 로봇은 수십억 명의 인간 스승을 갖게 되었다. 비디오 속의 인간이 문을 열고, 요리를 하고, 도구를 사용하는 매 순간이 로봇에게는 배움의 기회이다. 향후 연구는 이러한 시각적 어포던스를 촉각(Tactile) 및 힘(Force) 정보와 결합하여, ‘보고 잡는’ 수준을 넘어 물체의 물성을 ‘느끼고 조작하는’ 단계로 나아갈 것이다. 이것이 바로 진정한 의미의 체화된 지능(Embodied Intelligence)을 향한 길이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Affordances From Human Videos as a Versatile Representation for Robotics - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Bahl_Affordances_From_Human_Videos_as_a_Versatile_Representation_for_Robotics_CVPR_2023_paper.pdf</li>
<li>Visual Affordance Prediction: Survey and Reproducibility - arXiv, https://arxiv.org/html/2505.05074</li>
<li>Affordances from Human Videos as a Versatile Representation for Robotics: VRB, https://robo-affordances.github.io/</li>
<li>[2206.01908] Video-based Human-Object Interaction Detection from Tubelet Tokens - arXiv, https://arxiv.org/abs/2206.01908</li>
<li>Human-object interaction prediction in videos through gaze following, https://elib.dlr.de/197480/1/2306.03597.pdf</li>
<li>Video-based Human-Object Interaction Detection from Tubelet Tokens - NIPS, https://proceedings.neurips.cc/paper_files/paper/2022/file/9415416201aa201902d1743c7e65787b-Paper-Conference.pdf</li>
<li>Video-based Human-Object Interaction Detection from Tubelet Tokens - OpenReview, https://openreview.net/forum?id=kADW_LsENM</li>
<li>Human–object interaction prediction in videos through gaze following, https://evm7.github.io/HOIGaze-page/</li>
<li>RoHOI: Robustness Benchmark for Human-Object Interaction Detection - arXiv, https://arxiv.org/html/2507.09111v3</li>
<li>Second Joint Egocentric Vision (EgoVis) Workshop, https://egovis.github.io/cvpr25/</li>
<li>Reconstructing Hand-Held Objects in 3D from Images and Videos - arXiv, https://arxiv.org/html/2404.06507v3</li>
<li>Reconstructing Hand-Held Objects in 3D from Images and Videos, https://arxiv.org/html/2404.06507v4</li>
<li>arXiv:2404.06507v2 [cs.CV] 10 Apr 2024 - Caltech Authors, https://authors.library.caltech.edu/records/0n803-rzq26/files/2404.06507v2.pdf?download=1</li>
<li>ECCV Poster Coarse-to-Fine Implicit Representation Learning for 3D Hand-Object Reconstruction from a Single RGB-D Image, https://eccv.ecva.net/virtual/2024/poster/965</li>
<li>Coarse-to-Fine Implicit Representation Learning for 3D Hand-Object Reconstruction from a Single RGB-D Image, https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06748.pdf</li>
<li>CVPR 2024: Constraining 3D Hand-Object Pose Estimation with Global Signed Distance Fields (HOISDF) - YouTube, https://www.youtube.com/watch?v=tDn6NL6HHnw</li>
<li>CenterGrasp: Implicit Representation for Shape Reconstruction and 6-DoF Grasp Estimation [RA-L 2024] - YouTube, https://www.youtube.com/watch?v=j9EXZ0KdA4I</li>
<li>CenterGrasp: Object-Aware Implicit Representation Learning for Simultaneous Shape Reconstruction and 6-DoF Grasp Estimation - arXiv, https://arxiv.org/html/2312.08240v2</li>
<li>Learning Explicit Contact for Implicit Reconstruction of Hand-Held Objects from Monocular Images - AAAI Publications, https://ojs.aaai.org/index.php/AAAI/article/view/27995/28007</li>
<li>[2503.15898] Reconstructing In-the-Wild Open-Vocabulary Human-Object Interactions - arXiv, https://arxiv.org/abs/2503.15898</li>
<li>CVPR Poster PICO: Reconstructing 3D People In Contact with Objects, https://cvpr.thecvf.com/virtual/2025/poster/34142</li>
<li>Learning Precise Affordances from Egocentric Videos for Robotic Manipulation - arXiv, https://arxiv.org/html/2408.10123v1</li>
<li>Learning Precise Affordances from Egocentric Videos for Robotic Manipulation - arXiv, https://arxiv.org/html/2408.10123v2</li>
<li>Learning Precise Affordances from Egocentric Videos for Robotic Manipulation - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Learning_Precise_Affordances_from_Egocentric_Videos_for_Robotic_Manipulation_ICCV_2025_paper.pdf</li>
<li>Daily Papers - Hugging Face, <a href="https://huggingface.co/papers?q=actionable+affordance+regions">https://huggingface.co/papers?q=actionable%20affordance%20regions</a></li>
<li>Discovery and recognition of motion primitives in human activities | PLOS One, https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0214499</li>
<li>Object Discovery in Videos as Foreground Motion Clustering - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2019/papers/Xie_Object_Discovery_in_Videos_as_Foreground_Motion_Clustering_CVPR_2019_paper.pdf</li>
<li>Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos, https://arxiv.org/html/2506.08334v1</li>
<li>Unsupervised articulated object modeling via conditional view synthesis - arXiv, https://arxiv.org/html/2406.16623v1</li>
<li>Articulated Object Understanding from a Single Video Sequence - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2025W/gDT-IV/papers/Artykov_Articulated_Object_Understanding_from_a_Single_Video_Sequence_ICCVW_2025_paper.pdf</li>
<li>Vision-Robotics Bridge, https://vision-robotics-bridge.github.io/</li>
<li>Affordances from Human Videos as a Versatile Representation for …, https://robo-affordances.github.io/resources/vrb_paper.pdf</li>
<li>RTAGrasp: Learning Task-Oriented Grasping from Human Videos via Retrieval, Transfer, and Alignment - arXiv, https://arxiv.org/html/2409.16033v1</li>
<li>[Literature Review] RTAGrasp: Learning Task-Oriented Grasping from Human Videos via Retrieval, Transfer, and Alignment - Moonlight, https://www.themoonlight.io/en/review/rtagrasp-learning-task-oriented-grasping-from-human-videos-via-retrieval-transfer-and-alignment</li>
<li>Analyzing Key Objectives in Human-to-Robot Retargeting for Dexterous Manipulation - arXiv, https://arxiv.org/abs/2506.09384</li>
<li>Analyzing Key Objectives in Human-to-Robot Retargeting for Dexterous Manipulation - OpenReview, https://openreview.net/pdf?id=ojCJehWjy7</li>
<li>Analyzing Key Objectives in Human-to-Robot Retargeting for Dexterous Manipulation - arXiv, https://arxiv.org/html/2506.09384v1</li>
<li>Cross-Embodied Affordance Transfer through Learning Affordance Equivalences - arXiv, https://arxiv.org/html/2404.15648v2</li>
<li>Cross-Embodiment Transfer in Robotics - Emergent Mind, https://www.emergentmind.com/topics/cross-embodiment-transfer</li>
<li>Emergence of Human to Robot Transfer in Vision-Language-Action Models - arXiv, https://arxiv.org/html/2512.22414v1</li>
<li>HOI-Diff: Text-Driven Synthesis of 3D Human-Object Interactions using Diffusion Models, https://research.google/pubs/hoi-diff-text-driven-synthesis-of-3d-human-object-interactions-using-diffusion-models/</li>
<li>[2312.06553] HOI-Diff: Text-Driven Synthesis of 3D Human-Object Interactions using Diffusion Models - arXiv, https://arxiv.org/abs/2312.06553</li>
<li>[2312.03913] Controllable Human-Object Interaction Synthesis - arXiv, https://arxiv.org/abs/2312.03913</li>
<li>End-to-End Multi-Modal Diffusion Mamba - arXiv, https://arxiv.org/html/2510.13253v1</li>
<li>Diffusion Transformer-to-Mamba Distillation for High-Resolution Image Generation - BMVA Archive, https://bmva-archive.org.uk/bmvc/2025/assets/papers/Paper_931/paper.pdf</li>
<li>MambaVision: A Hybrid Mamba-Transformer Vision Backbone - Research at NVIDIA, https://research.nvidia.com/publication/2025-06_mambavision-hybrid-mamba-transformer-vision-backbone</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>