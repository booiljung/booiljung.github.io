<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">제목: Embodied AI & Modern Control</a> / <a href="index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <span>Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</span></nav>
                </div>
            </header>
            <article>
                <h1>Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance &amp; Pose)</h1>
<h2>1.  서론: 인식을 넘어 행동으로의 진화 (Introduction: Evolution from Recognition to Action)</h2>
<p>로봇 공학의 궁극적인 지향점은 인간의 개입 없이 물리적 세계에서 유의미한 작업을 수행하는 완전한 자율성(Autonomy)을 확보하는 것이다. 이러한 자율성의 핵심에는 로봇이 환경을 어떻게 이해하고 해석하는가에 대한 질문, 즉 ’지각(Perception)’의 문제가 자리 잡고 있다. 전통적인 컴퓨터 비전(Computer Vision)과 인공지능(AI) 연구는 오랫동안 지각을 ’인식(Recognition)’과 동의어로 간주해 왔다. 이미지 분류(Image Classification), 객체 탐지(Object Detection), 의미론적 분할(Semantic Segmentation) 등은 모두 입력된 시각 데이터에 의미론적 라벨(Label)을 부여하는 것을 목표로 한다. 즉, “이것은 컵인가, 병인가?”, “책상은 어디에 있는가?“와 같은 서술적 지식(Descriptive Knowledge)을 획득하는 데 집중해 온 것이다.</p>
<p>그러나 로봇은 수동적인 관찰자가 아니다. 로봇은 환경에 물리력을 행사하여 상태를 변화시키는 행위자(Agent)이다. 컵이 ’컵’이라는 사실을 아는 것(Knowing That)만으로는 로봇이 그 컵을 안전하게 파지하여 물을 따르는 작업(Knowing How)을 수행할 수 없다. 로봇이 물리적 상호작용을 수행하기 위해서는 대상의 의미론적 범주를 넘어, 구체적인 행동 가능성과 기하학적 실행 좌표를 파악해야 한다. 이러한 패러다임의 전환을 우리는 **‘행동을 위한 지각(Perception for Action)’**이라 부른다.</p>
<p>이 장에서는 행동을 위한 지각을 구성하는 두 가지 핵심 축인 **어포던스(Affordance)**와 **6D 포즈(6D Pose)**를 심도 있게 다룬다. 어포던스는 “무엇을 할 수 있는가“에 대한 기능적 가능성을 제시하며, 6D 포즈는 “어디에서 어떻게 그 행동을 실행할 것인가“에 대한 정밀한 기하학적 매개변수를 제공한다. 최근 2024년과 2025년에 걸쳐 급격히 발전한 파운데이션 모델(Foundation Models), 확산 모델(Diffusion Models), 그리고 대규모 자아 중심 비디오(Egocentric Video) 학습 기법들은 이 두 분야에 혁명적인 변화를 가져왔다.</p>
<p>본 장의 구성은 다음과 같다. 먼저 7.2절에서는 제임스 J. 깁슨(James J. Gibson)의 생태 심리학적 기원에서 출발하여 로봇 공학적으로 재해석된 어포던스 이론의 기초를 다진다. 7.3절에서는 최신 시각적 어포던스 학습(Visual Affordance Learning) 기법들을 분석하며, 특히 인간의 행동 비디오로부터 미세한 접촉점과 기능적 부위를 학습하는 GAT(Geometry-guided Affordance Transformer)와 같은 방법론을 상세히 설명한다. 7.4절에서는 로봇 조작의 필수 요소인 6D 물체 포즈 추정 기술을 다루며, 미지 물체(Novel Object)에 대해 제로샷(Zero-shot) 추론이 가능한 FoundationPose의 아키텍처와 혁신성을 심층 분석한다. 이어지는 7.5절에서는 이러한 지각 정보를 바탕으로 실제 파지(Grasping)를 생성하는 과정을 다루며, 생성형 AI를 접목한 GraspGen과 같은 최신 확산 기반 파지 생성 프레임워크를 소개한다. 마지막으로 7.6절에서는 이 모든 요소를 통합하여 복잡한 비정형 환경에서도 강인하게 작동하는 로봇 조작 파이프라인의 구축 전략을 제시한다.</p>
<h2>2.  어포던스 이론: 생태학적 관점과 로봇 공학적 형식화 (Affordance Theory: Ecological Views and Robotic Formalization)</h2>
<h3>2.1  깁슨의 생태학적 접근 (Gibson’s Ecological Approach)</h3>
<p>어포던스(Affordance)라는 용어는 1966년 미국의 지각 심리학자 제임스 J. 깁슨(James J. Gibson)에 의해 처음 제안되었으며, 1979년 그의 저서 <em>The Ecological Approach to Visual Perception</em>을 통해 체계화되었다. 깁슨은 당시 주류였던 정보 처리적 접근(Information Processing Approach)—즉, 지각을 감각 입력으로부터 내부적인 정신 모델을 구성하는 연산 과정으로 보는 관점—에 반기를 들었다. 그는 동물이 환경을 지각할 때, 물체의 물리적 속성(크기, 색상, 형상)을 분석한 뒤 그것의 의미를 추론하는 것이 아니라, 환경이 제공하는 행동의 가능성(Action Possibilities)을 <strong>직접적으로(Directly)</strong> 지각한다고 주장했다.</p>
<blockquote>
<p>“환경의 어포던스는 그것이 동물에게 제공하는 것(what it offers), 준비하는 것(what it provides or furnishes), 좋든 나쁘든 간에…” — J.J. Gibson (1979)</p>
</blockquote>
<p>깁슨 이론의 핵심은 **상호보완성(Complementarity)**과 **상대성(Relativity)**에 있다. 어포던스는 환경 내의 객관적인 물리적 구조와 관찰자(동물 또는 로봇)의 신체적 능력 사이의 관계 속에 존재한다. 예를 들어, 높이가 50cm인 평평한 바위는 성인 인간에게는 ‘앉을 수 있는(Sit-able)’ 어포던스를 제공하지만, 작은 곤충에게는 ‘착륙할 수 있는(Land-able)’ 표면이 되며, 거대한 코끼리에게는 단순히 ‘밟고 지나갈 수 있는(Step-on-able)’ 지형일 뿐이다.</p>
<p>이러한 관점은 로봇 공학, 특히 자율 에이전트 설계에 심오한 시사점을 제공한다. 로봇이 세상을 이해한다는 것은 3D CAD 모델을 완벽하게 복원하는 것이 아니라, 자신의 엔드 이펙터(End-effector)와 센서 능력을 고려했을 때 환경과 어떻게 상호작용할 수 있는지를 파악하는 과정이어야 함을 의미한다.</p>
<h3>2.2  로봇 공학적 형식화와 논쟁 (Formalization and Debates in Robotics)</h3>
<p>심리학적 개념이었던 어포던스는 로봇 공학에 도입되면서 공학적으로 구현 가능한 형태로 재정의되어야 했다. 2000년대 초반부터 Sahin, Chemero, Turvey 등 여러 연구자들은 어포던스를 형식화하기 위해 다양한 정의를 내놓았다.</p>
<p><strong>주요 형식화 접근:</strong></p>
<ul>
<li><strong>Turvey (1992):</strong> 어포던스를 환경의 물리적 속성(Disposition)으로 정의하며, 동물의 효과기(Effector) 속성과 짝을 이룬다고 보았다.</li>
<li><strong>Sahin et al. (2007):</strong> 로봇 공학적 관점에서 어포던스를 <code>(환경(Environment), 행위자(Agent), 행동(Behavior), 효과(Effect))</code>의 관계 튜플로 정의했다. 이는 로봇이 특정 환경 <span class="math math-inline">E</span>에서 행동 <span class="math math-inline">B</span>를 수행했을 때 예상되는 효과 <span class="math math-inline">E_{ff}</span>를 학습하는 문제로 치환될 수 있음을 시사한다.</li>
<li><strong>현대의 딥러닝 기반 접근 (2020s~):</strong> 최근의 연구들은 철학적 논쟁보다는 실용적인 구현에 집중한다. 어포던스는 주로 **“입력 이미지 내에서 특정 행동의 성공 확률이 높은 픽셀 영역”**으로 정의된다. 이는 픽셀 단위의 의미론적 분할(Segmentation) 또는 히트맵(Heatmap) 회귀(Regression) 문제로 다루어지며, 로봇의 시각-운동 정책(Visuomotor Policy) 학습의 핵심 입력값으로 사용된다.</li>
</ul>
<h3>2.3  행동 중심 지각으로의 전환 (Shift to Action-Oriented Perception)</h3>
<p>로봇 공학에서 어포던스 이론의 도입은 ’재귀적(Reactive) 제어’에서 ’인지적(Cognitive) 제어’로의 가교 역할을 수행했다. Brooks의 포섭 구조(Subsumption Architecture) 와 같은 초기 행동 기반 로봇 공학(Behavior-based Robotics)은 명시적인 표상(Representation) 없이 감각과 행동을 직접 연결하려 했으나, 복잡한 작업을 수행하는 데 한계가 있었다. 반면, 어포던스 기반 접근은 환경의 정보를 ’행동에 유용한 형태’로 압축하여 표상함으로써, 최소한의 연산으로도 환경의 의미를 파악하고 적응적인 행동을 생성할 수 있게 한다. 이는 “지각은 경제적이다(Perception is economical)“라는 깁슨의 명제와 일치한다.</p>
<h2>3.  시각적 어포던스 학습의 최전선 (Frontiers in Visual Affordance Learning)</h2>
<p>2024년과 2025년은 시각적 어포던스 학습(Visual Affordance Learning)에 있어 획기적인 발전이 이루어진 시기이다. 대규모 비전-언어 모델(VLM), 생성형 확산 모델, 그리고 인간 행동 데이터의 대량 학습이 결합되면서, 기존의 단순한 ’잡기 가능 영역 탐지’를 넘어 ‘도구 사용’, ‘기능적 조작’, ‘언어 명령 기반 어포던스 추론’ 등으로 연구의 범위가 확장되었다.</p>
<h3>3.1  어포던스 표현 방식의 진화 (Evolution of Affordance Representations)</h3>
<p>로봇이 시각 정보로부터 어포던스를 추출하여 표현하는 방식은 기술의 발전에 따라 고도화되었다. 다음 표는 주요 어포던스 표현 방식의 특징과 장단점을 비교한 것이다.</p>
<table><thead><tr><th><strong>표현 방식 (Representation)</strong></th><th><strong>설명 (Description)</strong></th><th><strong>장점 (Pros)</strong></th><th><strong>단점 (Cons)</strong></th><th><strong>대표 연구 사례</strong></th></tr></thead><tbody>
<tr><td><strong>의미론적 마스크 (Semantic Masks)</strong></td><td>이미지 내 픽셀을 ‘잡기 가능’, ‘밀기 가능’ 등의 클래스로 분류 (Segmentation)</td><td>시각적으로 직관적이며, 2D CNN/Transformer 모델 활용 용이</td><td>깊이 정보 부족으로 3D 실행 시 투영 오차 발생 가능</td><td></td></tr>
<tr><td><strong>히트맵 및 키포인트 (Heatmaps &amp; Keypoints)</strong></td><td>행동 성공 확률 분포를 가우시안 히트맵으로 표현하거나, 최적의 파지점을 키포인트로 검출</td><td>파지 중심점 등 정밀한 위치 지정 가능, 불확실성 표현 용이</td><td>물체의 전체적인 형상 정보나 방향(Orientation) 정보 누락 가능</td><td></td></tr>
<tr><td><strong>3D 어포던스 필드 (3D Affordance Fields)</strong></td><td>포인트 클라우드, NeRF, 또는 3D 복셀 공간에 어포던스 점수 부여</td><td>6D 포즈 및 로봇 모션 계획(Motion Planning)과 직접 연동 가능</td><td>높은 계산 비용, 고품질 3D 데이터 확보 및 레이블링의 어려움</td><td></td></tr>
<tr><td><strong>언어 조건부 어포던스 (Language-Conditioned)</strong></td><td>텍스트 명령(예: “칼로 썰어라”)에 따라 동적으로 활성화되는 어포던스 영역 예측</td><td>과업(Task)의 맥락을 반영한 유연한 어포던스 추론 가능</td><td>VLM의 추론 능력에 의존적이며, 실시간 처리 속도 이슈 존재</td><td></td></tr>
</tbody></table>
<h3>3.2  인간 비디오를 통한 정밀 어포던스 학습: GAT와 Aff-Grasp (Learning from Human Videos)</h3>
<p>어포던스 데이터셋 구축의 가장 큰 난제는 “어디를 잡아야 하는가“에 대한 정답(Ground Truth)을 사람이 일일이 주석 달기 어렵다는 점이다. 이는 주관적이고 노동 집약적인 작업이다. 이를 해결하기 위해 2025년의 최신 연구들은 <strong>자아 중심 비디오(Egocentric Video)</strong>—예: Ego4D, EPIC-KITCHENS—를 활용하여 인간의 자연스러운 상호작용으로부터 어포던스를 자율 학습하는 방식(Self-Supervised Learning)으로 전환하고 있다.</p>
<p>GAT (Geometry-guided Affordance Transformer)  Li 등(2025)이 제안한 GAT 프레임워크는 인간의 도구 사용 비디오에서 ’잡기(Graspable)’와 ‘기능(Functional)’ 어포던스를 분리하여 학습하는 혁신적인 파이프라인을 제시했다.</p>
<ol>
<li><strong>데이터 수집 및 호모그래피 매핑:</strong></li>
</ol>
<ul>
<li>인간이 도구를 사용하는 비디오 클립에서 손과 물체가 접촉하는 순간을 포착한다.</li>
<li><strong>잡기 어포던스:</strong> 손이 물체를 감싸는 영역을 식별한다.</li>
<li><strong>기능적 어포던스:</strong> 도구(예: 칼)가 대상 물체(예: 도마 위의 당근)와 상호작용하는 지점(예: 칼날)을 식별한다.</li>
<li>중요한 점은 접촉 전(Pre-contact) 프레임으로 접촉점들을 <strong>호모그래피(Homography)</strong> 변환을 통해 역투영하여, 로봇이 물체에 접근하기 전 상태에서의 어포던스를 학습하도록 한다는 것이다.</li>
</ul>
<ol start="2">
<li><strong>기하학적 유도 및 모델 구조:</strong></li>
</ol>
<ul>
<li>단순 2D 이미지만을 사용하는 것이 아니라, 깊이(Depth) 정보를 활용하여 3D 공간상의 기하학적 일관성을 유지한다.</li>
<li>모델의 백본(Backbone)으로는 <strong>DINOv2</strong>와 같은 대규모 시각 파운데이션 모델을 사용하며, **LoRA (Low-Rank Adaptation)**를 통해 효율적으로 미세 조정(Fine-tuning)한다.</li>
<li>추출된 핵심 포인트(Keypoints)를 프롬프트로 사용하여 **SAM (Segment Anything Model)**을 구동, 정밀한 픽셀 단위의 어포던스 마스크를 생성한다.</li>
</ul>
<p>이러한 접근 방식은 기존 모델들이 단순히 ’물체의 중심’을 잡는 경향이 있었던 것과 달리, 칼의 손잡이를 잡고(Grasp), 칼날을 이용하여(Function) 자르는 복합적인 행위 지식을 로봇에게 전수할 수 있게 한다. GAT는 기존 SOTA 대비 mIoU를 13.8% 향상시켰다.</p>
<h3>3.3  확산 모델 기반 어포던스 생성: AffordDex (Diffusion-based Affordance Generation)</h3>
<p>전통적인 회귀(Regression) 기반 모델들은 어포던스의 분포가 여러 곳에 존재할 때(Multi-modal), 그 중간 지점을 예측하는 ’평균화의 오류(Averaging Error)’를 범하기 쉽다. 예를 들어 컵의 손잡이가 좌우에 모두 있을 수 있다면, 회귀 모델은 컵의 몸통 중앙을 예측해버릴 수 있다. **확산 모델(Diffusion Models)**은 노이즈로부터 데이터 분포를 생성해내는 능력을 통해 이러한 다봉성(Multimodality) 문제를 효과적으로 해결한다.</p>
<p>AffordDex  2025년 발표된 AffordDex는 인간과 유사한(Anthropomorphic) 덱스터러스 파지(Dexterous Grasping)를 위해 2단계 확산 프레임워크를 제안한다.</p>
<ul>
<li><strong>1단계: 인간 동작 사전 학습 (Human Motion Priors):</strong> 대규모 인간 손 데이터셋(OakInk2 등)을 사용하여, 손이 물체에 자연스럽게 접근하고 형상을 취하는 분포를 확산 모델로 학습한다. 이는 로봇 손이 기괴하거나 물리적으로 불가능한 자세를 취하는 것을 방지한다.</li>
<li><strong>2단계: 부정적 어포던스 인지 (Negative Affordance Awareness):</strong> VLM을 활용하여 ‘만지면 안 되는 영역’(예: 칼날, 뜨거운 표면, 깨지기 쉬운 장식)을 **부정적 어포던스(Negative Affordance)**로 정의하고 분할한다. 확산 모델의 역과정(Reverse Process)에서 이러한 영역을 회피하도록 그라디언트 가이던스(Gradient Guidance)를 제공한다.</li>
</ul>
<p>AffordDex는 본 적 없는 물체(Unseen Objects)와 범주(Novel Categories)에 대해서도 인간처럼 자연스럽고 기능적으로 올바른 파지 자세를 생성하며, 기존 방법론 대비 월등한 성공률과 인간 유사성(Human-likeness)을 입증했다.</p>
<h3>3.4  VLM과 개방형 어휘 어포던스 (VLMs and Open-Vocabulary Affordance)</h3>
<p>고정된 클래스 라벨(Closed-set)의 한계를 넘어서기 위해, **비전-언어 모델(VLM)**의 추론 능력을 어포던스에 접목하는 연구가 활발하다.</p>
<ul>
<li><strong>CoA-VLA (Chain-of-Affordance):</strong> “식물을 옮겨라“라는 명령을 받으면, VLM은 “식물을 옮기기 위해서는 화분을 잡아야 한다”, “화분의 윗부분은 식물이 있어 잡기 부적절하다”, “화분의 몸통이나 바닥을 잡아야 한다“는 식의 **추론 체인(Chain-of-Thought)**을 생성한다. 이 텍스트 추론 결과는 시각적 그라운딩(Visual Grounding) 모델과 결합되어 이미지 상의 구체적인 좌표로 변환된다.</li>
<li>이러한 접근은 로봇이 “손잡이가 부러진 컵은 몸통을 잡아라“와 같이 상황에 따라 유동적으로 변하는 어포던스(Dynamic Affordance)를 이해할 수 있게 해준다.</li>
</ul>
<h2>4.  6D 물체 포즈 추정: 행동을 위한 정밀 기하학 (6D Object Pose Estimation: Precise Geometry for Action)</h2>
<p>어포던스가 로봇에게 “무엇을 할 것인가“와 “어디를 공략할 것인가“에 대한 전략적 지도를 제공한다면, **6D 포즈 추정(6D Pose Estimation)**은 그 지점의 정확한 3차원 좌표와 회전 정보를 제공하는 전술적 실행 도구이다. 6D 포즈는 카메라 좌표계 <span class="math math-inline">C</span>에 대한 물체 좌표계 <span class="math math-inline">O</span>의 변환 행렬 <span class="math math-inline">T_{CO} = \in SE(3)</span>를 추정하는 문제로, 3자유도의 회전(Rotation)과 3자유도의 이동(Translation)을 포함한다.</p>
<h3>4.1  6D 포즈 추정의 난제와 분류 (Challenges and Categories)</h3>
<p>6D 포즈 추정은 다음과 같은 이유로 컴퓨터 비전에서 가장 도전적인 문제 중 하나로 꼽힌다:</p>
<ol>
<li><strong>가려짐(Occlusion):</strong> 빈 픽킹(Bin Picking) 시나리오에서 물체들은 서로 겹쳐져 있어 전체 형상의 일부만 관측된다.</li>
<li><strong>대칭성(Symmetry):</strong> 원통형 컵이나 대칭적인 산업 부품은 시각적으로 구분이 불가능한 여러 개의 유효한 회전 포즈를 가질 수 있어, 모델 학습 시 손실 함수(Loss Function) 설계가 까다롭다.</li>
<li><strong>질감 부족(Texture-less):</strong> 표면에 특징적인 무늬가 없는 금속 부품이나 플라스틱 제품은 기존의 특징 매칭(Feature Matching) 기법을 무력화시킨다.</li>
</ol>
<p>연구의 흐름은 다루는 물체의 범위에 따라 다음과 같이 분류된다:</p>
<ul>
<li><strong>인스턴스 레벨(Instance-level):</strong> 학습 단계에서 이미 본 특정 물체(CAD 모델 보유)의 포즈를 추정.</li>
<li><strong>카테고리 레벨(Category-level):</strong> ’머그컵’이라는 범주에 속하는 다양한 컵들의 정규화된 좌표계(NOCS) 상의 포즈를 추정.</li>
<li><strong>제로샷/미지 물체(Zero-shot/Novel Object):</strong> 학습 시 보지 못한 완전히 새로운 물체의 포즈를 추정. 2024-2025년 연구의 핵심 트렌드이다.</li>
</ul>
<h3>4.2  FoundationPose: 통합 파운데이션 모델의 아키텍처와 혁신 (FoundationPose: Architecture and Innovation)</h3>
<p>2024년 NVIDIA 연구팀이 발표한 <strong>FoundationPose</strong> 는 6D 포즈 추정 분야의 ’GPT 모멘트’라 불릴 만한 혁신을 가져왔다. 이 모델은 단일 통합 프레임워크 내에서 모델 기반(Model-based)과 모델 프리(Model-free) 설정을 모두 지원하며, 미지 물체에 대해서도 별도의 미세 조정(Fine-tuning) 없이 즉각적인 추론이 가능하다.</p>
<h4>4.2.1 핵심 아키텍처 및 작동 원리</h4>
<p>FoundationPose의 아키텍처는 크게 <strong>새로운 뷰 합성(Novel View Synthesis)</strong>, <strong>포즈 정제(Pose Refinement)</strong>, **포즈 선택(Pose Selection)**의 세 단계로 구성된다.</p>
<ol>
<li><strong>신경 필드를 이용한 뷰 합성 (Neural Fields for View Synthesis):</strong></li>
</ol>
<ul>
<li>CAD 모델이 없는 미지 물체(Model-free setup)의 경우, 소수의 참조 이미지(Reference Images)만으로 물체의 형상을 이해해야 한다.</li>
<li>FoundationPose는 **객체 중심 신경 필드(Object-centric Neural Field)**를 활용하여, 참조 이미지로부터 임의의 시점에 대한 RGB-D 이미지를 렌더링한다. 이는 NeRF(Neural Radiance Fields)와 유사한 개념으로, 딥러닝 내부에서 ‘Render-and-Compare’ 방식을 구현한 것이다.</li>
<li>이를 통해 다운스트림 포즈 추정 모듈은 입력이 실제 CAD 렌더링인지, 신경 필드 렌더링인지 구분할 필요 없이 동일하게 작동할 수 있다(Unified Framework).</li>
</ul>
<ol start="2">
<li><strong>트랜스포머 기반 반복 정제 (Transformer-based Iterative Refinement):</strong></li>
</ol>
<ul>
<li>초기 포즈 가설(Initial Pose Hypothesis)을 등간격으로 생성하거나 거칠게 추정한 뒤, 이를 정제 네트워크(Refinement Network)에 입력한다.</li>
<li>이 네트워크는 <strong>트랜스포머(Transformer)</strong> 아키텍처를 기반으로 하며, 입력 이미지와 렌더링된 이미지(현재 포즈 가설 기반) 간의 차이를 분석하여 포즈 업데이트 값(<span class="math math-inline">\Delta P</span>)을 예측한다.</li>
<li>특히, 외관이 유사한 방해물(Distractor)과 실제 타겟 물체를 구분하기 위해 <strong>대조 학습(Contrastive Learning)</strong> 손실 함수가 적용되었다. 이는 특징 공간(Feature Space) 상에서 타겟 물체의 임베딩을 명확히 분리해내는 데 기여한다.</li>
</ul>
<ol start="3">
<li><strong>점수 기반 포즈 선택 (Score-based Pose Selection):</strong></li>
</ol>
<ul>
<li>정제 과정을 거친 여러 포즈 후보들 중 가장 정확한 하나를 선택하기 위해 별도의 점수 네트워크(Score Network)를 사용한다.</li>
<li>이 네트워크는 계층적 비교(Hierarchical Comparison)와 포즈 순위 인코더(Pose Ranking Encoder)를 통해 각 포즈의 신뢰도를 산출한다. 이는 대칭 물체나 심한 가려짐 상황에서 발생할 수 있는 포즈의 모호성(Ambiguity)을 해결하는 결정적인 역할을 한다.</li>
</ul>
<h4>4.2.2 성능 및 산업적 의의</h4>
<p>FoundationPose는 BOP(Benchmark for 6D Object Pose Estimation) 리더보드에서 미지 물체 포즈 추정 부문 압도적 1위를 차지했다. 이는 로봇이 새로운 제품이 매일 쏟아져 나오는 물류 센터나, 정리되지 않은 가정 환경에서도 별도의 재학습 없이 즉시 투입될 수 있음을 의미한다. NVIDIA는 이를 Isaac ROS 패키지로 공개하여 상용 로봇 개발자들이 즉시 활용할 수 있도록 지원하고 있다.</p>
<h3>4.3  산업용 데이터셋의 진화: XYZ-IBD (Evolution of Industrial Datasets)</h3>
<p>학술적 연구와 산업 현장 간의 격차(Domain Gap)를 줄이기 위해 데이터셋 또한 진화하고 있다. 2025년 공개된 <strong>XYZ-IBD 데이터셋</strong> 은 기존의 YCB-Video와 같은 가정용 물체 데이터셋이 포화 상태(Saturation)에 이르렀음을 지적하며, 극한의 산업 환경을 모사한다.</p>
<ul>
<li><strong>특징:</strong> 반사 재질(Reflective Materials), 금속성 부품, 형태가 유사한 물체들의 고밀도 적재(Dense Clutter) 상황을 포함한다.</li>
<li><strong>정밀도:</strong> 27만 개 이상의 객체 인스턴스에 대해 밀리미터(mm) 수준의 정밀한 6D 포즈 주석을 제공한다.</li>
<li>이러한 데이터셋은 FoundationPose와 같은 모델이 “쉬운 문제“에 과적합(Overfitting)되지 않고, 실제 공장의 빈 픽킹 문제와 같은 “어려운 문제“를 해결할 수 있는지 검증하는 척도가 된다.</li>
</ul>
<h2>5.  파지 생성: 기하학과 어포던스의 결합 (Grasp Generation: Merging Geometry and Affordance)</h2>
<p>6D 포즈 추정이 물체의 ’위치’를 알려준다면, 파지 생성(Grasp Generation)은 그 물체와 어떻게 ’접촉’할 것인지를 결정한다. 이는 인식된 정보를 물리적 행동으로 변환하는 최종 관문이다.</p>
<h3>5.1  접촉 기반 파지넷 (Contact-GraspNet)</h3>
<p><strong>Contact-GraspNet</strong> 은 2021년 발표 이후 로봇 파지 분야의 표준(Standard)으로 자리 잡은 방법론이다.</p>
<ul>
<li><strong>접근 방식:</strong> 물체의 3D 메쉬(Mesh)를 복원하거나 6D 포즈를 먼저 추정하는 대신, 관측된 포인트 클라우드(Point Cloud)의 각 점을 잠재적인 접촉점(Contact Point)으로 간주한다.</li>
<li><strong>네트워크 구조:</strong> PointNet++와 같은 포인트 기반 인코더를 사용하여, 각 점에 대해 6-DoF 파지 포즈(회전 및 접근 벡터), 그리퍼 폭(Width), 그리고 파지 성공 확률(Confidence)을 직접 회귀(Regress)한다.</li>
<li><strong>장점:</strong> 부분적으로만 관측된(Partially Observed) 물체에 대해서도 강인하게 작동하며, 복잡한 클러터 환경에서도 충돌 없는 파지를 생성할 수 있다. 하지만 “어디를 잡아야 하는가“에 대한 의미론적 판단보다는 물리적 안정성(Stability)에 초점을 맞춘다.</li>
</ul>
<h3>5.2  확산 모델 기반 생성형 파지: GraspGen (Generative Grasping with Diffusion)</h3>
<p>2025년, NVIDIA와 워싱턴 대학 연구팀은 파지 생성을 <strong>SE(3) 공간에서의 조건부 생성 모델링</strong> 문제로 재정의한 <strong>GraspGen</strong> 을 발표했다.</p>
<h4>5.2.1 GraspGen의 핵심 혁신: On-Generator Training</h4>
<p>GraspGen은 단순히 성공한 파지 데이터의 분포를 학습하는 것을 넘어, 생성자(Generator)와 판별자(Discriminator)의 상호작용을 통해 성능을 극대화한다.</p>
<ol>
<li><strong>SE(3) 확산 프로세스:</strong> 가우시안 노이즈로 구성된 무작위 6D 포즈들의 집합에서 시작하여, 물체의 포인트 클라우드 조건(Condition) 하에 역확산(Reverse Diffusion) 과정을 거쳐 유효한 파지 포즈 분포로 수렴시킨다. 이는 단 하나의 ’최적해’가 아니라, 상황에 따라 선택 가능한 다양한 파지 해법(Diverse Solutions)을 제공한다.</li>
<li><strong>On-Generator Training:</strong> 기존의 GAN이나 확산 모델 훈련과 달리, GraspGen은 생성자가 만들어낸 파지 샘플들을 시뮬레이션 상에서 검증(Ground Truth Evaluation)하고, 그 결과를 판별자(Discriminator) 학습에 다시 피드백한다. 즉, 모델이 스스로 생성한 데이터의 성공/실패 여부를 학습함으로써, 자신의 생성 능력 한계 내에서 가장 정확한 평가 기준을 수립하게 된다.</li>
<li><strong>성과:</strong> 5,300만 개 이상의 대규모 파지 데이터셋으로 학습된 GraspGen은 FetchBench 벤치마크에서 기존 SOTA 대비 17% 향상된 성능을 기록했으며, 메모리 효율성 또한 21배 개선되었다.</li>
</ol>
<h3>5.3  태스크 지향적 파지 파이프라인: Aff-Grasp (Task-Oriented Grasping Pipeline)</h3>
<p>진정한 의미의 ’행동을 위한 지각’은 단순히 물체를 떨어뜨리지 않고 잡는 것(Stable Grasp)을 넘어, 이후의 조작 작업(Task)을 고려한 파지(Task-oriented Grasp)를 수행하는 것이다. Li 등(2025)이 제안한 <strong>Aff-Grasp</strong> 은 앞서 논의한 기술들을 하나의 유기적인 파이프라인으로 통합한 모범 사례이다.</p>
<p><strong>Aff-Grasp 파이프라인의 단계:</strong></p>
<ol>
<li><strong>어포던스 감지 (Affordance Detection):</strong> GAT 모델을 통해 입력 이미지에서 ’잡기 가능 영역(Graspable Region)’과 ’기능적 영역(Functional Region)’의 마스크를 생성한다. 예를 들어, 머그컵의 경우 손잡이는 잡기 영역, 입이 닿는 부분은 기능 영역이 된다.</li>
<li><strong>3D 공간 투영 (3D Projection):</strong> 깊이 이미지를 이용하여 2D 마스크를 3D 포인트 클라우드 상의 영역으로 매핑한다.</li>
<li><strong>영역 제한 파지 샘플링 (Constrained Grasp Sampling):</strong> Contact-GraspNet이나 GraspGen과 같은 파지 생성기가 전체 물체가 아닌, ‘잡기 가능 영역’ 내에서만 파지 후보를 생성하도록 제한(Conditioning)한다.</li>
<li><strong>행동 실행 (Execution):</strong> 선택된 파지 포즈로 물체를 잡은 후, 기능적 영역이 작업 대상(예: 사람의 입, 또 다른 물체)을 향하도록 로봇 팔의 경로를 계획(Motion Planning)한다.</li>
</ol>
<p>이 파이프라인은 기하학적 파지 모델의 ’물리적 안정성’과 의미론적 어포던스 모델의 ’목적 지향성’을 결합하여, 로봇이 인간과 협업하거나 도구를 사용하는 고차원적인 작업을 수행할 수 있게 한다. 실험 결과, Aff-Grasp는 복잡한 클러터 환경에서도 77.1%의 높은 작업 성공률을 달성했다.</p>
<h2>6.  통합적 고찰: 지각-행동 루프의 완성 (Integrated Discussion: Closing the Perception-Action Loop)</h2>
<p>지금까지 살펴본 어포던스와 6D 포즈, 그리고 파지 생성 기술은 개별적으로 발전해 왔으나, 2025년 현재 하나의 거대한 **‘지각-행동 루프(Perception-Action Loop)’**로 수렴하고 있다. 이 통합의 과정에서 나타나는 주요한 트렌드와 시사점은 다음과 같다.</p>
<h3>6.1  데이터 중심의 패러다임 전환 (Data-Driven Paradigm Shift)</h3>
<p>과거에는 로봇의 지각 알고리즘을 수작업으로 설계(Hand-crafted features)했으나, 이제는 **합성 데이터(Synthetic Data)**와 **인간 데모 비디오(Human Demos)**가 그 자리를 대체하고 있다.</p>
<ul>
<li><strong>합성 데이터의 힘:</strong> FoundationPose와 GraspGen은 시뮬레이션에서 생성된 수천만, 수억 개의 데이터를 학습한다. 물리 엔진을 통해 생성된 이 데이터들은 로봇에게 ’기하학적 직관’을 심어주어, 현실 세계의 미지 물체에 대해서도 일반화(Generalization)할 수 있는 능력을 부여한다.</li>
<li><strong>인간 지식의 전이:</strong> AffordDex와 GAT는 유튜브나 Ego4D와 같은 인간 행동 영상에서 ‘무엇이 잡을 수 있는 것인가’, ’어떻게 도구를 쓰는가’에 대한 암묵적 지식(Tacit Knowledge)을 배운다. 이는 로봇에게 명시적인 규칙을 코딩하지 않고도 인간의 상식을 전이(Transfer)시키는 가장 효율적인 방법이다.</li>
</ul>
<h3>6.2  의미론과 기하학의 실시간 융합 (Real-time Fusion of Semantics and Geometry)</h3>
<p>과거에는 의미론적 인식(Vision)과 기하학적 제어(Control)가 분리되어 있었다. 비전 팀이 물체를 찾으면, 제어 팀이 좌표를 받아 움직이는 식이었다. 그러나 **6D 어포던스(6D Affordance)**의 개념은 이 경계를 허물고 있다. VLM이 생성한 “뜨거우니 조심하라“는 텍스트 경고는 AffordDex 내에서 즉시 ’파지 금지 영역(Negative Affordance)’이라는 기하학적 제약 조건으로 변환되어 로봇 손의 움직임을 제어한다. 즉, 고수준의 인지(Cognition)가 저수준의 모터 제어(Motor Control)에 실시간으로 개입하는 것이다.</p>
<h3>6.3  엣지 컴퓨팅과 경량화 (Edge Computing and Efficiency)</h3>
<p>이러한 거대 모델들을 실제 로봇에 탑재하기 위해서는 경량화가 필수적이다. FoundationPose는 TensorRT 최적화를 통해 실시간 추적을 지원하며 , LiteFat 과 같은 연구는 자원이 제한된 임베디드 로봇을 위한 경량화된 그래프 학습 모델을 제안한다. 향후 로봇 시스템은 클라우드 상의 초거대 AI(VLM/LLM)가 전략을 수립하고, 로봇 엣지단의 고속 비전 모델(Pose/Grasp)이 전술을 실행하는 하이브리드 구조로 발전할 것이다.</p>
<h2>7.  결론 (Conclusion)</h2>
<p>제7장에서는 로봇이 물리적 세계와 상호작용하기 위해 필수적인 ‘행동을 위한 지각’ 체계를 탐구하였다. 깁슨의 생태학적 통찰에서 시작된 어포던스는 이제 딥러닝과 확산 모델을 통해 픽셀 단위의 정밀한 행동 지도로 구현되고 있다. 동시에, 6D 포즈 추정 기술은 파운데이션 모델의 도입으로 사전에 학습하지 않은 미지의 물체까지 즉각적으로 다룰 수 있는 범용성을 확보하였다.</p>
<p>우리는 <strong>FoundationPose</strong>를 통해 로봇이 새로운 물체를 만났을 때 즉시 그 위치와 자세를 파악하는 법을, <strong>GraspGen</strong>과 <strong>AffordDex</strong>를 통해 가장 안정적이고 인간다운 파지 자세를 생성하는 법을, 그리고 <strong>GAT</strong>와 <strong>Aff-Grasp</strong>를 통해 도구의 기능적 의미를 이해하고 목적에 맞게 사용하는 법을 살펴보았다. 이 기술들의 결합은 로봇을 단순한 자동화 기계에서, 환경을 이해하고 능동적으로 변화시키는 지능형 에이전트로 진화시키고 있다.</p>
<p>하지만 여전히 해결해야 할 과제들은 남아 있다. 투명하거나 반사가 심한 물체에 대한 인식률 개선, 복잡한 비정형 물체(Deformable Objects, 예: 옷, 케이블)의 조작, 그리고 시각 정보가 차단된 상황에서의 촉각(Tactile) 기반 어포던스 보정 등은 차세대 연구자들의 몫이다.</p>
<p>다음 장(Chapter 8)에서는 이러한 지각 정보를 바탕으로 로봇 팔의 구체적인 궤적을 생성하고, 물체와의 접촉 시 발생하는 힘을 제어하는 **‘동작 계획 및 제어(Motion Planning &amp; Interaction Control)’**에 대해 다룰 것이다. 지각이 행동의 가능성을 열었다면, 제어는 그 가능성을 현실로 만든다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>CoA-VLA: Improving Vision-Language-Action Models via Visual-Text Chain-of-Affordance - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2025/papers/Li_CoA-VLA_Improving_Vision-Language-Action_Models_via_Visual-Text_Chain-of-Affordance_ICCV_2025_paper.pdf</li>
<li>Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors - arXiv, https://arxiv.org/html/2508.08896v1</li>
<li>FoundationPose: Unified 6D Pose Estimation and Tracking … - NVlabs, https://nvlabs.github.io/FoundationPose/</li>
<li>Learning Precise Affordances from Egocentric Videos for Robotic Manipulation - arXiv, https://arxiv.org/html/2408.10123v2</li>
<li>Editorial: Computational models of affordance for robotics - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC9583360/</li>
<li>Affordances in Psychology, Neuroscience, and Robotics - Research Explorer - The University of Manchester, https://research.manchester.ac.uk/files/76585946/77602011.pdf</li>
<li>Learning Predictive Features in Affordance-based Robotic Perception Systems - Association for the Advancement of Artificial Intelligence (AAAI), https://cdn.aaai.org/Workshops/2006/WS-06-03/WS06-03-010.pdf</li>
<li>Gibson’s affordances and Turing’s theory of computation - LSE Research Online, https://eprints.lse.ac.uk/2606/1/Affordances_and_Computation_APA_style_(LSERO).pdf</li>
<li>One-Shot Open Affordance Learning with Foundation Models - ResearchGate, https://www.researchgate.net/publication/384169480_One-Shot_Open_Affordance_Learning_with_Foundation_Models</li>
<li>Video Generation Models in Robotics: Applications, Research Challenges, Future Directions, https://arxiv.org/html/2601.07823v1</li>
<li>Affordance-Guided Reinforcement Learning via Visual Prompting - ResearchGate, https://www.researchgate.net/publication/398060538_Affordance-Guided_Reinforcement_Learning_via_Visual_Prompting</li>
<li>Enhancing task-oriented robotic grasping via 3D affordance grounding from vision-language models - ResearchGate, https://www.researchgate.net/publication/399157968_Enhancing_task-oriented_robotic_grasping_via_3D_affordance_grounding_from_vision-language_models</li>
<li>Generalizable Coarse-to-Fine Robot Manipulation via Language-Aligned 3D Keypoints, https://openreview.net/forum?id=WXFfMLyB6y</li>
<li>Aff-Grasp - Gen Li, https://reagan1311.github.io/affgrasp/</li>
<li>Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Model - arXiv, https://arxiv.org/html/2508.17922v1</li>
<li>Learning Precise Affordances from Egocentric Videos for Robotic Manipulation - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Learning_Precise_Affordances_from_Egocentric_Videos_for_Robotic_Manipulation_ICCV_2025_paper.pdf</li>
<li>[2408.10123] Learning Precise Affordances from Egocentric Videos for Robotic Manipulation - arXiv, https://arxiv.org/abs/2408.10123</li>
<li>Paper page - Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors - Hugging Face, https://huggingface.co/papers/2508.08896</li>
<li>Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors, https://www.researchgate.net/publication/394458091_Towards_Affordance-Aware_Robotic_Dexterous_Grasping_with_Human-like_Priors</li>
<li>Towards Affordance-Aware Robotic Dexterous Grasping with Human-like\n Priors - Liner, https://liner.com/review/towards-affordanceaware-robotic-dexterous-grasping-with-humanlike-priors</li>
<li>6D Object Pose Estimation, https://mahmoud-ali-fcis.github.io/6D-Object-Pose-Estimation/</li>
<li>A Survey of 6DoF Object Pose Estimation Methods for Different Application Scenarios, https://www.mdpi.com/1424-8220/24/4/1076</li>
<li>XYZ-IBD: Benchmarking Robust 6D Object Pose Estimation under Real-World Industrial Complexity | OpenReview, https://openreview.net/forum?id=TW8ps0YirX</li>
<li>FoundationPose is a unified foundation model for 6D object pose estimation and tracking of objects. - NVIDIA NGC Catalog, https://catalog.ngc.nvidia.com/orgs/nvidia/teams/isaac/models/foundationpose</li>
<li>FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Wen_FoundationPose_Unified_6D_Pose_Estimation_and_Tracking_of_Novel_Objects_CVPR_2024_paper.pdf</li>
<li>FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects - Jan Kautz, https://www.jankautz.com/publications/FoundationPose_CVPR24.pdf</li>
<li>FoundationPose Model Card - NGC Catalog - NVIDIA, https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/foundationpose</li>
<li>[CVPR 2024 Highlight] FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects - GitHub, https://github.com/NVlabs/FoundationPose</li>
<li>NVlabs/contact_graspnet: Efficient 6-DoF Grasp Generation in Cluttered Scenes - GitHub, https://github.com/NVlabs/contact_graspnet</li>
<li>Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes | Request PDF, https://www.researchgate.net/publication/355434800_Contact-GraspNet_Efficient_6-DoF_Grasp_Generation_in_Cluttered_Scenes</li>
<li>Contact-graspnet: Efficient 6-dof grasp generation in cluttered scenes, https://elib.dlr.de/145798/1/Contact-GraspNet.pdf</li>
<li>GraspGen: A Diffusion-based Framework for 6-DOF Grasping with On-Generator Training - arXiv, https://arxiv.org/html/2507.13097v1</li>
<li>NVIDIA AI Releases GraspGen: A Diffusion-Based Framework for 6-DOF Grasping in Robotics - MarkTechPost, https://www.marktechpost.com/2025/07/26/nvidia-ai-releases-graspgen-a-diffusion-based-framework-for-6-dof-grasping-in-robotics/</li>
<li>AffordDP: Generalizable Diffusion Policy with Transferable Affordance - arXiv, https://arxiv.org/html/2412.03142v2</li>
<li>GraspGen: A Diffusion-based Framework for 6-DOF Grasping with On-Generator Training - ChatPaper, https://chatpaper.com/paper/164696</li>
<li>Official repo for GraspGen: A Diffusion-based Framework for 6-DOF Grasping - GitHub, https://github.com/NVlabs/GraspGen</li>
<li>Paper page - Learning Precise Affordances from Egocentric Videos for Robotic Manipulation, https://huggingface.co/papers/2408.10123</li>
<li>IROS 2025 Program | Wednesday October 22, 2025, https://ras.papercept.net/conferences/conferences/IROS25/program/IROS25_ContentListWeb_2.html</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>