<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.5 파지 검출과 생성 (Grasp Detection & Generation)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.5 파지 검출과 생성 (Grasp Detection & Generation)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <a href="index.html">7.5 파지 검출과 생성 (Grasp Detection & Generation)</a> / <span>7.5 파지 검출과 생성 (Grasp Detection & Generation)</span></nav>
                </div>
            </header>
            <article>
                <h1>7.5 파지 검출과 생성 (Grasp Detection &amp; Generation)</h1>
<h2>1.  서론 및 개요 (Introduction and Overview)</h2>
<p>로봇 조작(Robotic Manipulation)의 영역에서 물체를 안정적으로 파지(Grasping)하는 능력은 가장 기초적이면서도 난이도가 높은 문제로 간주된다. 인간에게는 직관적이고 본능적인 이 행위가 로봇에게는 고차원의 인지, 계획, 제어 능력을 동시에 요구하는 복합적인 과제이기 때문이다. 파지 시스템은 로봇이 시각 센서(Visual Sensor) 등을 통해 환경을 인식하고, 인식된 정보로부터 물체를 성공적으로 들어 올리거나 조작하기 위한 로봇 손(End-effector)의 위치(Position)와 자세(Orientation)를 결정하는 일련의 과정을 포함한다. 과거의 파지 연구가 사전에 정의된 CAD 모델과 정확한 위치 정보를 가정한 분석적(Analytical) 방법에 의존했다면, 최근의 연구 흐름은 RGB-D 센서나 LiDAR 등에서 획득한 불완전하고 노이즈가 포함된 점군(Point Cloud) 데이터로부터 딥러닝(Deep Learning) 모델을 이용해 파지 자세를 직접 추론하는 데이터 기반(Data-driven) 방법으로 급격히 이동하고 있다.</p>
<p>본 절에서는 로봇 파지 기술의 핵심인 파지 검출(Grasp Detection)과 파지 생성(Grasp Generation)의 개념을 정립하고, 이를 구현하기 위한 다양한 접근 방식들을 심도 있게 분석한다. 기하학적 알고리즘(Geometric Algorithms)에서부터 최신의 심층 생성 모델(Deep Generative Models), 그리고 대규모 데이터셋(Large-scale Datasets)의 구축과 활용에 이르기까지 기술의 진화 과정을 상세히 기술하며, 각 방법론이 갖는 수학적 배경과 실질적인 구현 전략을 논의한다.</p>
<h3>1.1  파지 검출과 생성의 정의 및 패러다임 변화</h3>
<p>학술적으로 파지 검출(Detection)과 생성(Generation)은 혼용되어 사용되기도 하지만, 문제 해결을 위한 접근 방식과 출력의 성격에서 분명한 차이를 보인다.</p>
<ul>
<li><strong>파지 검출(Grasp Detection)</strong>: 이 접근법은 컴퓨터 비전(Computer Vision)의 객체 검출(Object Detection) 문제와 유사한 맥락에서 발전하였다. 입력 이미지나 3D 점군 데이터에서 파지가 가능한 후보 영역(Region of Interest)을 식별하고, 해당 영역에 대한 파지 설정(Configuration)을 분류(Classification)하거나 회귀(Regression)하는 방식이다. 초기 연구는 2차원 평면(Planar) 상에서의 파지 사각형(Oriented Rectangle)을 검출하는 데 집중하였으나(<span class="math math-inline">g = \{x, y, \theta, h, w\}</span>), 점차 3차원 공간상의 6자유도(6-DoF) 파지 자세를 검출하는 것으로 발전하였다. 파지 검출 시스템은 일반적으로 다수의 파지 후보군(Candidates)을 샘플링한 후, 딥러닝 모델을 통해 각 후보의 품질(Quality)을 점수화(Scoring)하여 최적의 파지를 선택하는 ‘가설 및 검증(Hypothesis and Test)’ 구조를 가진다.</li>
<li><strong>파지 생성(Grasp Generation)</strong>: 파지 생성은 주어진 환경이나 객체에 대해 실행 가능한 파지 자세의 확률 분포(Probability Distribution)를 모델링하고, 이 분포로부터 파지 자세를 직접 샘플링(Sampling)하거나 합성(Synthesis)하는 접근법이다. 이는 단순히 유한한 후보군 중에서 선택하는 것을 넘어, 무한하고 연속적인 공간에서 최적의 해를 찾아내는 생성적 관점을 취한다. 최근 생성적 적대 신경망(GAN), 변분 오토인코더(VAE), 그리고 확산 모델(Diffusion Models) 등의 생성형 AI 기술이 도입되면서, 비정형 물체나 혼잡한 환경에서도 다양하고 안정적인 파지를 생성해내는 연구가 활발히 진행되고 있다.</li>
</ul>
<h3>1.2  6-DoF 파지의 수학적 표현과 공간적 자유도</h3>
<p>초기 파지 연구는 카메라가 물체를 수직으로 내려다보는 상황(Top-down view)을 가정한 3자유도(3-DoF: <span class="math math-inline">(x, y, \theta)</span>) 파지에 집중했다. 이는 물체가 평평한 바닥에 놓여 있고 서로 겹쳐 있지 않은 단순한 환경에서는 효과적이다. 그러나 현실 세계의 복잡한 비정형 환경, 특히 물체가 무질서하게 쌓여 있는 클러터(Clutter) 환경이나 측면 파지가 필수적인 선반 정리(Bin Picking) 등의 작업에서는 6자유도(6-DoF) 파지 기술이 필수적이다.</p>
<p>6-DoF 파지는 3차원 공간에서의 위치와 회전을 모두 고려해야 하므로, 탐색 공간이 기하급수적으로 넓어진다. 이를 수학적으로 표현하면, <span class="math math-inline">SE(3)</span> (Special Euclidean group) 공간에서 로봇 손의 포즈 <span class="math math-inline">g \in SE(3)</span>를 결정하는 문제가 된다. 일반적으로 평행 그리퍼(Parallel-jaw gripper)를 가정한 6-DoF 파지 <span class="math math-inline">g</span>는 다음과 같이 정의된다:<br />
<span class="math math-display">
g = (R, t, w) \in SO(3) \times \mathbb{R}^3 \times \mathbb{R}
</span><br />
여기서 <span class="math math-inline">R</span>은 그리퍼의 회전 행렬, <span class="math math-inline">t</span>는 그리퍼의 중심 위치, <span class="math math-inline">w</span>는 그리퍼의 개폐 폭(Width)을 의미한다. 이러한 고차원 공간에서의 최적화 문제는 카메라 시점에 따른 폐색(Occlusion), 조명 변화, 깊이 센서의 노이즈 등 다양한 불확실성을 처리해야 하는 난제를 안고 있다. 본 절에서는 이러한 6-DoF 파지 문제를 해결하기 위한 기하학적 방법론부터 최신 딥러닝 기반 방법론까지 순차적으로 다룬다.</p>
<h2>2.  기하학적 파지 후보 생성 (Geometric Grasp Candidate Generation)</h2>
<p>딥러닝 모델이 파지 품질을 평가하기 위해서는 먼저 파지가 가능할 것으로 예상되는 후보(Candidate)들을 효율적으로 생성해야 한다. 이를 위한 가장 고전적이면서도 여전히 강력한 방법은 물체의 형상 정보, 특히 표면의 곡률(Curvature)과 법선 벡터(Normal Vector)를 분석하여 기하학적으로 타당한 파지 자세를 도출하는 것이다. 이 방식은 별도의 학습 데이터 없이도 3D 점군 데이터만 있으면 즉시 적용 가능하다는 장점이 있으며, 딥러닝 모델의 학습 데이터를 자동 생성(Auto-labeling)하는 데에도 핵심적인 역할을 수행한다.</p>
<h3>2.1  GPG (Grasp Pose Generator) 알고리즘 상세 분석</h3>
<p>GPG(Grasp Pose Generator)는 기하학적 제약 조건을 기반으로 3D 점군에서 6-DoF 파지 후보를 생성하는 대표적인 알고리즘이다. GPG는 머신러닝 요소를 배제하고 순수하게 점군 데이터의 기하학적 특성만을 이용하여 파지 가능한 위치를 탐색하므로, 데이터 편향(Bias) 없는 후보 생성이 가능하다.</p>
<h4>2.1.1  표면 법선 추정 및 전처리 (Surface Normal Estimation)</h4>
<p>GPG의 첫 단계는 입력된 점군 <span class="math math-inline">P = {p_1, p_2,..., p_N}</span>에 대해 각 점의 표면 법선(Surface Normal) <span class="math math-inline">n_i</span>를 추정하는 것이다. 이는 주성분 분석(PCA) 등을 통해 인접한 점들의 평면을 추정함으로써 계산된다. 특히 단일 시점(Single-view) 깊이 센서를 사용하는 경우, 법선 벡터의 방향이 모호해질 수 있는데, GPG는 카메라의 원점 <span class="math math-inline">O_{cam}</span> 정보를 활용하여 법선 벡터의 방향이 항상 카메라를 향하도록 정렬(Re-orientation)한다. 이는 물체의 표면이 센서에 보일 수 있는 방향, 즉 접근 가능한 방향임을 보장하는 중요한 전처리 과정이다.</p>
<h4>2.1.2  작업 공간 내 샘플링 전략 (Sampling Strategy)</h4>
<p>전체 공간을 무작위로 탐색하는 것은 계산 효율성 측면에서 불가능에 가깝다. 따라서 GPG는 사용자가 지정한 작업 공간(Workspace) <span class="math math-inline">W = [x_{min}, x_{max}] \times [y_{min}, y_{max}] \times [z_{min}, z_{max}]</span> 내에서 표면 점들을 중심으로 파지 후보를 샘플링한다. 샘플링된 각 점 <span class="math math-inline">p_s</span>는 파지 접촉점 후보가 되며, 해당 점의 법선 벡터 <span class="math math-inline">n_s</span>는 그리퍼의 접근 방향(Approach Vector)을 결정하는 초기 단서로 활용된다. 로컬 좌표계를 형성하기 위해, 법선 벡터에 수직인 임의의 축을 선택하여 그리퍼의 회전각을 다양하게 샘플링함으로써 하나의 접촉점에서도 다수의 파지 후보를 생성한다.</p>
<h4>2.1.3  기하학적 파지 조건 검사 (Geometric Feasibility Check)</h4>
<p>샘플링된 각 파지 후보 <span class="math math-inline">g_i</span>에 대해 로봇 손(그리퍼)의 물리적 형상을 고려한 엄격한 검사가 수행된다. 2지(2-finger) 평행 그리퍼를 가정할 때, 다음의 조건들이 순차적으로 충족되어야 한다.</p>
<ul>
<li><strong>충돌 검사(Collision Check)</strong>: 그리퍼의 손가락(Finger) 영역과 손바닥(Base) 영역, 그리고 접근 경로(Approach path)에 물체나 바닥면의 점군이 존재하는지 확인한다. 이를 위해 그리퍼 형상을 단순화한 바운딩 박스(Bounding Box) 모델을 사용하여 점군과의 간섭 여부를 고속으로 연산한다.</li>
<li><strong>접촉 검색(Contact Search)</strong>: 충돌이 없는 파지 자세에서 그리퍼를 닫았을 때, 반대편 손가락이 물체 표면과 만나는지 확인한다. 이는 그리퍼의 닫힘 방향(Closing Direction)을 따라 레이 캐스팅(Ray Casting)을 수행하거나, 그리퍼 내부 영역(Closing Region)에 포함된 점들의 분포를 분석하여 수행한다.</li>
</ul>
<h3>2.2  반대향 파지(Antipodal Grasp) 조건과 이론적 배경</h3>
<p>기하학적 파지 생성의 이론적 토대는 ‘반대향 파지(Antipodal Grasp)’ 개념에 있다. 이는 마찰이 있는 접촉(Frictional Contact) 모델 하에서, 두 손가락이 물체 표면과 접촉했을 때 가해지는 힘이 서로 상쇄되어 외력에 대해 물체를 안정적으로 고정할 수 있는 상태를 의미한다.</p>
<p>수학적으로, 두 접촉점 <span class="math math-inline">c_1, c_2</span>와 해당 지점에서의 표면 법선 <span class="math math-inline">n_1, n_2</span>가 주어졌을 때, 파지가 안정적이기 위해서는 두 접촉점을 잇는 선분 <span class="math math-inline">c_1 - c_2</span>가 각 접촉점의 마찰 원뿔(Friction Cone) 내에 포함되어야 한다. 이를 단순화된 기하학적 조건으로 표현하면 다음과 같다:</p>
<ol>
<li>
<p><strong>법선의 대칭성</strong>: 두 법선 벡터가 서로 반대 방향을 향해야 한다. (<span class="math math-inline">n_1 \cdot n_2 \approx -1</span>)</p>
</li>
<li>
<p><strong>힘의 정렬</strong>: 두 접촉점을 잇는 벡터가 법선 벡터와 일직선상에 가까워야 한다.<br />
<span class="math math-display">
\frac{c_2 - c_1}{||c_2 - c_1||} \cdot n_1 \approx -1, \quad \frac{c_1 - c_2}{||c_1 - c_2||} \cdot n_2 \approx -1
</span></p>
</li>
</ol>
<p>GPG 알고리즘은 이러한 조건들을 만족하는 파지 자세만을 선별하여 최종 후보군으로 출력한다. 이러한 방식은 딥러닝 모델의 학습 데이터셋 구축 시 ’Positive Sample’을 자동으로 생성하는 데 매우 유용하게 사용된다. 그러나 GPG는 점군 데이터의 노이즈나 부분적인 소실(Missing data)에 매우 민감하며, 물체의 무게 중심이나 재질에 따른 마찰 계수 변화와 같은 물리적 특성을 완벽히 반영하지 못한다는 한계가 있다. 따라서 현대의 시스템에서는 GPG와 같은 기하학적 생성기를 초기 후보 생성 단계(Proposal Stage)로 사용하고, 딥러닝 모델을 이용해 최종 품질을 평가하는 하이브리드(Hybrid) 방식을 주로 채택한다.</p>
<h2>3.  식별 모델 기반 파지 검출 (Discriminative Grasp Detection)</h2>
<p>식별 모델 기반 접근법은 생성된 수많은 파지 후보(Grasp Candidates)에 대해 ‘성공’ 또는 ‘실패’ 확률을 예측하거나, 연속적인 파지 품질 점수(Grasp Quality Score)를 부여하는 방식이다. 이는 파지 문제를 이진 분류(Binary Classification) 또는 회귀(Regression) 문제로 모델링하여 해결한다.</p>
<h3>3.1  GPD (Grasp Pose Detection): 다채널 뷰 기반 접근</h3>
<p>GPD(Grasp Pose Detection)는 3D 점군 데이터로부터 6-DoF 파지 자세를 검출하는 선구적인 연구로서, 불완전한 센서 데이터에서도 강인한 성능을 보임을 입증하였다. GPD의 핵심 아이디어는 3차원의 복잡한 기하 정보를 CNN(Convolutional Neural Network)이 처리하기 쉬운 2D 이미지 형태의 특징(Feature)으로 변환하는 것이다.</p>
<h4>3.1.1  파지 이미지(Grasp Image) 표현 및 생성</h4>
<p>GPD는 파지 후보 영역의 로컬 점군을 그리퍼 좌표계로 정규화(Normalization)한 후, 이를 투영하여 다채널 이미지(Multi-channel Image)를 생성한다. 이를 ’파지 이미지’라고 부르며, 다음과 같은 구성을 갖는다 :</p>
<ul>
<li><strong>채널 구성</strong>: 단순히 깊이(Depth) 값만을 사용하는 것이 아니라, 표면 법선(Normal) 정보와 그림자(Shadow) 정보 등을 포함하여 정보량을 극대화한다.</li>
<li><strong>다중 시점(Multi-view) 통합</strong>: 단일 시점에서의 정보 부족을 보완하기 위해, 그리퍼의 접근 방향을 기준으로 여러 시점(예: 정면, 좌측, 우측)에서 관찰된 이미지를 결합한다.</li>
<li><strong>15-채널 표현</strong>: 높은 정밀도를 위해 3개의 시점 각각에 대해 5개의 채널(표면 법선 x, y, z + 깊이 + 그림자 여부 등)을 할당하여 총 15개의 채널을 구성한다. 이는 CNN이 물체의 3D 곡면 특성을 입체적으로 학습할 수 있게 한다.</li>
</ul>
<h4>3.1.2  CNN 아키텍처 및 학습</h4>
<p>생성된 파지 이미지는 LeNet을 변형한 경량 CNN이나 ResNet과 같은 심층 신경망에 입력되어 해당 파지가 성공할지 실패할지를 판별한다. GPD는 수백만 개의 합성 데이터와 실제 데이터로 학습되며, 훈련 과정에서 데이터 증강(Data Augmentation)을 통해 다양한 물체 크기와 위치 변화에 대한 일반화 성능을 확보한다. 추론 단계에서는 OpenVINO나 TensorRT와 같은 라이브러리와 결합하여 수천 개의 후보를 실시간으로 평가할 수 있도록 최적화된다.</p>
<h3>3.2  PointNetGPD: 점군 직접 처리(Point-based) 방식</h3>
<p>GPD와 같이 점군을 이미지로 변환(Voxelization 또는 Projection)하는 방식은 3D 공간의 기하학적 정보를 이산화(Discretization)하는 과정에서 필연적으로 정보 손실을 발생시키며, 변환 과정 자체의 계산 비용이 높다는 단점이 있다. PointNetGPD는 이러한 한계를 극복하기 위해 Raw Point Cloud를 딥러닝 모델의 입력으로 직접 사용하는 방식을 제안하였다.</p>
<h4>3.2.1  그리퍼 내부 점군 추출 (Cropping &amp; Transformation)</h4>
<p>PointNetGPD는 먼저 전체 점군에서 파지 후보의 그리퍼 영역(Closing Area) 내부에 위치한 점들만을 추출(Crop)한다. 추출된 점군 <span class="math math-inline">P_{crop}</span>은 그리퍼의 중심을 원점으로 하는 로컬 좌표계로 변환된다. 이 과정은 파지 자세의 절대적인 위치나 회전 변화에 불변(Invariant)하는 특징을 추출하기 위해 필수적이다. 즉, 물체가 어디에 있든, 어떤 각도로 놓여 있든, 그리퍼 내부의 점 분포가 동일하다면 동일한 파지 품질을 예측하도록 유도한다.</p>
<h4>3.2.2  PointNet 아키텍처 활용 및 특징 추출</h4>
<p>PointNet [Qi et al.]은 점들의 입력 순서에 무관하게(Permutation Invariant) 3D 형상을 인식할 수 있는 신경망 구조이다. PointNetGPD는 PointNet을 인코더로 사용하여 <span class="math math-inline">P_{crop}</span>으로부터 전역 특징 벡터(Global Feature Vector)를 추출한다.</p>
<p>수식으로 표현하면, 입력 점군 집합 <span class="math math-inline">S = {x_1, \dots, x_n}</span>에 대해 PointNetGPD의 특징 추출 과정은 다음과 같다:<br />
<span class="math math-display">
f(S) \approx g(h(x_1), h(x_2), \dots, h(x_n))
</span><br />
여기서 <span class="math math-inline">h(\cdot)</span>는 점별 특징을 추출하는 공유된 MLP(Multi-Layer Perceptron) 층이며, <span class="math math-inline">g(\cdot)</span>는 MaxPooling과 같은 대칭 함수(Symmetric Function)로서 점들의 순서와 관계없이 전체 형상을 요약하는 역할을 한다. 추출된 전역 특징은 다시 MLP를 통과하여 최종적인 파지 품질 점수(Grasp Score)를 출력한다.</p>
<h4>3.2.3  GPD와의 성능 비교 및 분석</h4>
<p>연구 결과에 따르면 PointNetGPD는 GPD에 비해 특히 클러터(Clutter) 환경에서 더 우수한 성능을 보여준다. 점군 데이터를 직접 처리함으로써 물체 표면의 미세한 곡률이나 접촉면의 기하학적 구조를 손실 없이 포착할 수 있기 때문이다. 또한, 이미지 변환 과정이 생략되므로 추론 속도 면에서도 이점을 가지며, 가려짐(Occlusion)이 심한 환경에서도 국소적인(Local) 기하 정보만으로 파지 가능성을 판단하는 데 강인함을 보인다.</p>
<table><thead><tr><th><strong>알고리즘</strong></th><th><strong>입력 데이터 형태</strong></th><th><strong>특징 추출 방식</strong></th><th><strong>장점</strong></th><th><strong>단점</strong></th></tr></thead><tbody>
<tr><td><strong>GPD</strong></td><td>2D Multi-channel Image</td><td>CNN (2D Conv)</td><td>성숙한 CNN 기술 활용, 시각적 해석 용이</td><td>투영 과정에서의 정보 손실, 시점 의존성</td></tr>
<tr><td><strong>PointNetGPD</strong></td><td>3D Raw Point Cloud</td><td>PointNet (MLP + MaxPool)</td><td>3D 기하 정보 보존, 변환 불변성(Invariant)</td><td>점군 밀도에 따른 성능 변화, 국소 특징 의존</td></tr>
</tbody></table>
<h2>4.  심층 생성 모델을 이용한 파지 합성 (Deep Generative Models for Grasp Synthesis)</h2>
<p>최근에는 파지 후보를 샘플링(Sampling)하고 이를 하나씩 평가(Evaluating)하는 2단계(Two-stage) 방식의 계산 비효율성을 개선하기 위해, 파지 자세를 직접 생성(Generate)하거나 회귀하는 종단간(End-to-End) 학습 모델들이 등장하였다. 이들은 입력 데이터로부터 파지 자세의 확률 분포를 학습하고, 이 분포로부터 최적의 파지를 직접 추출함으로써 실시간성과 정확도를 동시에 추구한다.</p>
<h3>4.1  6-DoF GraspNet: 변분 오토인코더(VAE) 기반 접근</h3>
<p>6-DoF GraspNet 은 변분 오토인코더(VAE, Variational Autoencoder)를 사용하여 파지 자세의 다양성을 확보하는 대표적인 방법론이다.</p>
<h4>4.1.1  VAE를 이용한 파지 샘플링 (Grasp Sampling)</h4>
<p>이 모델은 입력 점군 <span class="math math-inline">X</span>가 주어졌을 때, 성공적인 파지 자세 <span class="math math-inline">g</span>의 조건부 분포 <span class="math math-inline">P(g|X)</span>를 모델링한다. VAE 구조를 채택함으로써, 잠재 변수(Latent Variable) <span class="math math-inline">z</span>를 표준 정규 분포 <span class="math math-inline">N(0, I)</span>에서 샘플링하고, 이를 디코더(Generator)에 통과시켜 다양한 파지 자세 <span class="math math-inline">g</span>를 생성할 수 있다.<br />
<span class="math math-display">
g = G(z, X), \quad z \sim \mathcal{N}(0, I)
</span><br />
이 방식은 결정론적(Deterministic) 모델이 단 하나의 평균적인 파지 자세만을 출력하는 것과 달리, 잠재 공간(Latent Space)의 탐색을 통해 단일 입력에 대해서도 수많은 유효한 파지 후보를 생성할 수 있어, 불확실성이 높은 환경에서 유리하다.</p>
<h4>4.1.2  평가 및 그래디언트 기반 정제 (Evaluator &amp; Refinement)</h4>
<p>생성된 파지 자세 <span class="math math-inline">g</span>는 별도로 학습된 평가 네트워크(Evaluator Network) <span class="math math-inline">E(g, X)</span>에 의해 성공 확률이 예측된다. 6-DoF GraspNet의 독창적인 메커니즘은 이 평가 네트워크의 그래디언트(Gradient)를 이용하여 생성된 파지 자세를 수정(Refine)한다는 점이다.<br />
<span class="math math-display">
g_{new} = g_{old} + \alpha \cdot \nabla_g E(g, X)
</span><br />
여기서 <span class="math math-inline">\alpha</span>는 학습률(Step size)이다. 이 과정은 생성된 초기 파지 자세를 물체의 표면 쪽으로 이동시키거나, 미세한 충돌을 회피하여 파지 품질이 극대화되는 방향으로 자세를 최적화한다. 이는 일종의 랑주뱅 역학(Langevin Dynamics)과 유사한 원리로, 초기 샘플이 다소 부정확하더라도 사후 보정을 통해 성공률을 획기적으로 높인다.</p>
<h3>4.2  Contact-GraspNet: 접촉점 중심의 표현 축소</h3>
<p>기존의 6-DoF 파지 자세 <span class="math math-inline">(R, t)</span>를 직접 예측하는 것은 탐색 공간이 너무 넓어 학습이 어렵다. Contact-GraspNet 은 파지 문제를 “접촉점(Contact Point)” 중심으로 재정의하여 차원을 축소하고 효율성을 극대화하였다.</p>
<h4>4.2.1  파지 표현의 차원 축소 (6-DoF <span class="math math-inline">\rightarrow</span> 4-DoF)</h4>
<p>Contact-GraspNet은 관측된 점군 데이터의 각 점을 잠재적인 파지 접촉점 <span class="math math-inline">c \in \mathbb{R}^3</span>으로 간주한다. 평행 그리퍼의 경우, 파지 중심 위치 <span class="math math-inline">t_g</span>는 접촉점 <span class="math math-inline">c</span>, 그리퍼 폭 <span class="math math-inline">w</span>, 그리고 접근 방향에 의해 기하학적으로 결정될 수 있다. 따라서 네트워크는 접촉점에서의 3-DoF 회전 <span class="math math-inline">R_g</span>와 그리퍼 폭 <span class="math math-inline">w</span>만을 예측하면 된다.<br />
<span class="math math-display">
t_g = c + \frac{w}{2} \cdot \mathbf{b} + d \cdot \mathbf{a}
</span><br />
여기서 <span class="math math-inline">\mathbf{a}</span>는 접근 벡터(Approach vector), <span class="math math-inline">\mathbf{b}</span>는 베이스라인 벡터(Baseline vector), <span class="math math-inline">d</span>는 그리퍼 깊이(Depth)이다. 이와 같이 문제의 자유도를 4-DoF로 줄임으로써 학습 수렴 속도와 추론 정확도를 크게 향상시켰다.</p>
<h4>4.2.2  아키텍처 및 다중 손실 함수 (Loss Functions)</h4>
<p>Contact-GraspNet은 PointNet++를 백본(Backbone)으로 사용하여 점군 내의 각 점에 대해 파지 가능성(Grasp Logit), 회전, 폭을 밀집(Dense) 예측한다. 손실 함수는 다음과 같이 구성된다:<br />
<span class="math math-display">
\mathcal{L} = \mathcal{L}_{class} + \lambda_1 \mathcal{L}_{rot} + \lambda_2 \mathcal{L}_{width}
</span></p>
<ul>
<li><span class="math math-inline">\mathcal{L}_{class}</span>: 해당 점이 파지 가능한지 여부를 판단하는 이진 분류 손실(Binary Cross Entropy).</li>
<li><span class="math math-inline">\mathcal{L}_{rot}</span>: 성공적인 파지 데이터셋과의 회전 오차를 줄이는 회귀 손실. 주로 코사인 유사도(Cosine Similarity)나 쿼터니언(Quaternion) 거리 등을 사용한다.</li>
<li><span class="math math-inline">\mathcal{L}_{width}</span>: 그리퍼 폭에 대한 L2 손실.</li>
</ul>
<p>Contact-GraspNet은 1,700만 개의 시뮬레이션 파지 데이터(ACRONYM)를 학습하여, 실제 환경(Real-world)의 보지 못한 물체(Unseen Objects)에 대해서도 90% 이상의 높은 성공률을 달성하며, 복잡한 파이프라인 없이 단일 패스(Single-pass)로 전체 장면의 파지를 생성할 수 있는 효율성을 입증하였다.</p>
<h2>5.  최신 확산 모델 기반 파지 (Diffusion-based Grasping)</h2>
<p>생성형 AI의 패러다임이 GAN과 VAE에서 확산 모델(Diffusion Models)로 이동함에 따라, 로봇 파지 분야에서도 이를 도입하려는 시도가 급증하고 있다. 확산 모델은 노이즈로부터 데이터를 복원하는 과정을 학습하여 복잡한 다봉(Multi-modal) 분포를 정밀하게 모델링할 수 있는 강점이 있다.</p>
<h3>5.1  SE(3) 공간에서의 확산 (Diffusion on SE(3) Manifold)</h3>
<p><em>GraspDiff</em> 와 같은 연구는 파지 자세 생성 문제를 조건부 확산 과정(Conditional Diffusion Process)으로 정의한다. 일반적인 이미지 생성 확산 모델이 유클리드 공간(픽셀 값)에서 작동한다면, 파지 확산 모델은 회전과 이동이 결합된 비선형 다양체인 <span class="math math-inline">SE(3)</span> 공간에서 정의되어야 한다.</p>
<h4>5.1.1  전방 및 역방향 과정 (Forward &amp; Reverse Process)</h4>
<p>확산 과정은 파지 자세 <span class="math math-inline">g_0</span>에 점진적으로 가우시안 노이즈(Gaussian Noise)를 주입하여 완전한 노이즈 상태 <span class="math math-inline">g_T</span>로 만드는 전방 과정(Forward Process)과, 이를 역으로 추정하여 노이즈를 제거하는 역방향 과정(Reverse Process)으로 구성된다.</p>
<p><span class="math math-display">q(g_t | g_{t-1}) = \mathcal{N}(g_t; \sqrt{1-\beta_t}g_{t-1}, \beta_t I)</span>역방향 과정에서는 신경망이 조건부 확률 분포 <span class="math math-inline">p_\theta(g_{t-1} | g_t, X)</span>를 학습하여, 노이즈가 섞인 파지 자세 <span class="math math-inline">g_t</span>와 물체 정보 <span class="math math-inline">X</span>를 입력받아 노이즈 성분을 예측하고 제거한다.<br />
<span class="math math-display">
p_\theta(g_{t-1} | g_t, X) = \mathcal{N}(g_{t-1}; \mu_\theta(g_t, t, X), \Sigma_\theta(g_t, t, X))
</span><br />
<span class="math math-inline">SE(3)</span> 공간에서의 노이즈 주입과 제거는 리 대수(Lie Algebra) <span class="math math-inline">\mathfrak{se}(3)</span> 상에서의 연산을 통해 이루어지며, 이는 회전 행렬의 직교성(Orthogonality)을 보존하기 위함이다.</p>
<h4>2. 스코어 매칭과 랑주뱅 역학 (Score Matching &amp; Langevin Dynamics)</h4>
<p>확산 모델 기반 파지 생성기는 스코어 매칭(Score Matching) 기법을 사용하여 데이터 분포의 그라디언트, 즉 스코어 함수(Score function, <span class="math math-inline">\nabla_g \log p(g|X)</span>)를 학습한다. 추론(Inference) 시에는 랑주뱅 역학(Langevin Dynamics)을 이용하여 무작위 노이즈 상태에서 시작해 데이터 분포의 밀도가 높은 영역(즉, 물리적으로 안정적인 파지 자세)으로 궤적을 이동시킨다.<br />
<span class="math math-display">
g_{t-1} \leftarrow g_t + \frac{\sigma_t^2}{2} \nabla_g \log p(g_t|X) + \sigma_t \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
</span><br />
이러한 방식은 VAE가 생성하는 흐릿하거나 평균화된 결과, 또는 GAN이 겪는 모드 붕괴(Mode Collapse) 문제 없이 매우 다양하고 정밀한 파지 자세를 생성할 수 있다는 장점이 있다. 특히 손과 물체의 상호작용(Hand-Object Interaction)을 고려한 정교한 파지 생성이나, 텍스트 명령에 따른 특정 부위 파지 생성 등에 탁월한 성능을 보인다.</p>
<h2>7.5.6 데이터셋 및 벤치마크 (Datasets and Benchmarks)</h2>
<p>데이터 기반(Data-driven) 파지 알고리즘의 성능은 학습 데이터의 양, 다양성, 그리고 라벨의 정확도에 절대적으로 의존한다. 과거에는 Cornell Grasp Dataset과 같이 수백 장의 이미지에 2D 사각형 라벨을 수동으로 붙인 소규모 데이터셋이 주류였으나, 딥러닝 모델의 크기가 커짐에 따라 10억 개 이상의 파지 라벨을 포함하는 대규모 3D 데이터셋이 표준으로 자리 잡았다.</p>
<h3>7.5.6.1 GraspNet-1Billion: 범용 파지 데이터의 표준</h3>
<p>GraspNet-1Billion 은 현재 학계 및 산업계에서 가장 널리 사용되는 대규모 벤치마크이다. 이 데이터셋은 실제 환경에서의 데이터 취득과 물리적 검증을 결합하여 구축되었다.</p>
<ul>
<li><strong>규모 및 구성</strong>: 190개의 클러터 장면(Cluttered Scenes)을 포함하며, 각 장면은 Kinect Azure와 RealSense D435 두 종류의 카메라로 촬영된 97,280장의 RGB-D 이미지를 제공한다. 총 88개의 다양한 물체(가정용품, 도구, 3D 프린팅 객체 등)가 포함되어 있다.</li>
<li><strong>라벨링 (Annotation)</strong>: 각 장면에 대해 약 11억(1.1 Billion) 개 이상의 6-DoF 파지 라벨을 제공한다. 이는 단순히 성공/실패의 이진 라벨이 아니라, 파지의 안정성을 정량화한 것이다.</li>
<li><strong>평가 지표 (Metric)</strong>: 파지 성공률을 마찰 계수 <span class="math math-inline">\mu</span>와 연동하여 평가한다. 점수 <span class="math math-inline">s</span>는 <span class="math math-inline">s = 1.1 - \mu</span>로 정의되는데, 이는 더 낮은 마찰 계수에서도 미끄러지지 않고 성공하는 파지일수록 더 높은 점수를 부여한다는 의미이다. 즉, 파지의 기하학적 견고성(Robustness)을 평가하는 지표이다.</li>
</ul>
<h3>7.5.6.2 ACRONYM: 시뮬레이션 기반 대규모 데이터셋</h3>
<p>ACRONYM 은 물리 시뮬레이션(NVIDIA FleX)을 통해 생성된 대규모 파지 데이터셋이다. 실제 로봇을 이용한 데이터 수집의 비용과 시간 한계를 극복하기 위해 고안되었다.</p>
<ul>
<li><strong>특징</strong>: ShapeNet 등에서 가져온 8,872개의 물체 메쉬(Mesh)와 262개의 카테고리에 대해 약 1,770만(17.7M) 개의 평행 그리퍼 파지 자세를 포함한다.</li>
<li><strong>장점</strong>: 실제 환경에서 수집하기 어려운 매우 정밀한 파지 라벨을 제공하며, 물체의 형상이 완벽하게 알려진 상태(Full State)에서의 파지 계획(Grasp Planning) 학습에 적합하다.</li>
<li><strong>활용 전략</strong>: Contact-GraspNet과 같은 최신 모델들은 ACRONYM 데이터셋으로 사전 학습(Pre-training)하여 일반적인 파지 형상을 학습한 후, GraspNet-1Billion과 같은 실제 데이터로 미세 조정(Fine-tuning)하는 ‘Sim-to-Real’ 전략을 사용하여 높은 성능을 달성한다.</li>
</ul>
<h3>7.5.6.3 특수 목적 데이터셋: SuctionNet 및 TransCG</h3>
<p>파지 방식의 다양화를 위해 평행 그리퍼 외의 다른 메커니즘을 위한 데이터셋도 등장하고 있다.</p>
<ul>
<li><strong>SuctionNet-1Billion</strong>: 진공 흡착(Suction) 그리퍼를 위한 데이터셋으로, 물체 표면의 법선과 곡률을 기반으로 흡착 가능 여부(Seal label)와 중력 및 외력에 대한 저항성(Wrench label)을 제공한다. 물류 자동화 분야에서 박스나 비정형 물체를 처리하는 데 필수적인 데이터이다.</li>
<li><strong>TransCG</strong>: 투명 물체(Transparent Objects)는 깊이 센서에서 노이즈를 유발하여 파지가 매우 어렵다. TransCG는 투명 물체의 깊이 완료(Depth Completion)와 파지를 위한 대규모 실세계 데이터셋으로, 5만 장 이상의 이미지를 포함한다.</li>
</ul>
<table><thead><tr><th><strong>데이터셋 명</strong></th><th><strong>데이터 소스</strong></th><th><strong>파지 라벨 수</strong></th><th><strong>물체 수</strong></th><th><strong>특징 및 용도</strong></th></tr></thead><tbody>
<tr><td><strong>GraspNet-1Billion</strong></td><td>Real RGB-D</td><td>1.1 Billion</td><td>88</td><td>클러터 환경, 밀집 라벨, 카메라 노이즈 강인성 평가</td></tr>
<tr><td><strong>ACRONYM</strong></td><td>Simulation</td><td>17.7 Million</td><td>8,872</td><td>물리 시뮬레이션 검증, 다양한 물체 카테고리, 사전 학습용</td></tr>
<tr><td><strong>Jacquard</strong></td><td>Simulation</td><td>1.1 Million</td><td>11,000</td><td>2D/3D 파지, 평면 파지 위주, 초기 딥러닝 연구용</td></tr>
<tr><td><strong>SuctionNet</strong></td><td>Real RGB-D</td><td>-</td><td>88</td><td>흡착(Suction) 파지 전용, 물류 자동화 연구</td></tr>
<tr><td><strong>TransCG</strong></td><td>Real RGB-D</td><td>-</td><td>51</td><td>투명 물체 파지 및 깊이 복원 연구</td></tr>
</tbody></table>
<h2>7.5.7 미래 동향: VLA 및 범용 에이전트 (Future Trends: VLA and Generalist Agents)</h2>
<p>파지 기술의 미래는 단순한 기하학적 안정성을 확보하는 ’물리적 파지’를 넘어, 인간의 언어 명령을 이해하고 상황에 맞는 논리적인 판단을 수행하는 ’의미론적 파지(Semantic Grasping)’와 범용 에이전트(Generalist Agent)로 진화하고 있다.</p>
<h3>7.5.7.1 비전-언어-행동(VLA) 모델의 통합</h3>
<p>VLA(Vision-Language-Action) 모델은 거대 언어 모델(LLM)의 추론 능력과 비전 모델(VLM)의 인식 능력을 로봇의 제어(Action)와 결합한 형태이다. 예를 들어, “파란색 블록을 집어서 빨간색 그릇에 담아줘“라는 명령을 내리면, VLA 모델은 다음과 같은 과정을 수행한다.</p>
<ol>
<li><strong>인식(Perception)</strong>: 시각 정보에서 ’파란색 블록’과 ’빨간색 그릇’을 식별(Detection)한다.</li>
<li><strong>생성(Generation)</strong>: 블록을 집기 위한 최적의 6-DoF 파지 자세를 생성한다. 이때, 그릇에 담기 편한 방향으로 집어야 한다는 제약 조건을 고려한다.</li>
<li><strong>행동(Action)</strong>: 로봇 팔의 궤적을 생성하고 제어 신호를 보낸다.</li>
</ol>
<p>구글의 Gemini Robotics나 RT-2와 같은 모델이 대표적이며, 이들은 인터넷 규모의 방대한 데이터로 학습되어 처음 보는 물체(Open-vocabulary objects)나 낯선 환경에서도 놀라운 일반화(Generalization) 성능을 보여준다.</p>
<h3>7.5.7.2 어포던스(Affordance)와 도구 사용</h3>
<p>VLA 시대의 파지는 “어떻게 잡을 것인가(How to grasp)“에 집중했던 기존 연구를 넘어 “무엇을, 왜, 어디를 잡을 것인가(What, Why, Where to grasp)“를 핵심 질문으로 삼는다. 이는 물체의 기능적 속성인 어포던스(Affordance, 행동 유도성)를 학습하는 것과 직결된다. 예를 들어, 칼을 건네줄 때는 칼날이 아닌 손잡이를 잡아야 하고, 머그컵을 설거지할 때는 손잡이가 아닌 몸통을 잡아야 할 수도 있다. 이러한 과제 지향적 파지(Task-oriented Grasping)는 기하학적 파지 생성 알고리즘과 거대 모델의 상식적 추론 능력이 결합되는 지점에서 해결책을 찾고 있다.</p>
<h2>7.5.8 결론 (Conclusion)</h2>
<p>본 7.5장에서는 로봇 파지 기술의 핵심인 파지 검출과 생성의 제반 기술을 상세히 다루었다. 기하학적 조건과 마찰 원뿔 이론에 기반한 GPG 알고리즘부터, 딥러닝을 활용하여 불완전한 데이터에서 특징을 추출하는 GPD와 PointNetGPD, 그리고 최신 생성 모델인 GraspNet과 Diffusion 모델에 이르기까지, 파지 기술은 데이터의 불확실성을 극복하고 파지 성공률을 극대화하는 방향으로 끊임없이 발전해 왔다. 특히 GraspNet-1Billion과 같은 대규모 데이터셋의 등장은 이러한 데이터 기반 방법론의 비약적인 성장을 견인한 핵심 동력이었다. 향후 파지 기술은 단순한 ‘잡기’ 기능을 넘어, 인간의 의도를 이해하고 도구의 의미를 파악하여 복잡한 작업을 자율적으로 수행하는 인지적 파지(Cognitive Grasping)로 발전할 것으로 전망된다. 이러한 기술적 진보는 가정용 로봇, 물류 자동화, 그리고 스마트 팩토리 등 다양한 분야에서 로봇의 실용성을 획기적으로 높이는 기폭제가 될 것이다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>Review of Deep Learning Methods in Robotic Grasp Detection - MDPI, 2월 1, 2026에 액세스, https://www.mdpi.com/2414-4088/2/3/57</li>
<li>Robotic Grasping from Classical to Modern: A Survey - ResearchGate, 2월 1, 2026에 액세스, https://www.researchgate.net/publication/358458001_Robotic_Grasping_from_Classical_to_Modern_A_Survey</li>
<li>Robotics Dexterous Grasping: The Methods Based on Point Cloud and Deep Learning - NIH, 2월 1, 2026에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC8221534/</li>
<li>A review of robotic grasp detection technology | Robotica | Cambridge Core, 2월 1, 2026에 액세스, https://www.cambridge.org/core/journals/robotica/article/review-of-robotic-grasp-detection-technology/DAC7618B53607EFC16A50099713FD080</li>
<li>Context-Aware Grasp Generation in Cluttered Scenes - Diva-portal.org, 2월 1, 2026에 액세스, http://www.diva-portal.org/smash/get/diva2:1648882/FULLTEXT01.pdf</li>
<li>[2503.03464] Generative Artificial Intelligence in Robotic Manipulation: A Survey - arXiv, 2월 1, 2026에 액세스, https://arxiv.org/abs/2503.03464</li>
<li>GraspDiff: Grasping Generation for Hand-Object Interaction With Multimodal Guided Diffusion - IEEE Computer Society, 2월 1, 2026에 액세스, https://www.computer.org/csdl/journal/tg/2025/09/10689328/20tjl998uDC</li>
<li>6-DoF Grasp Detection Method Based on Vision Language Guidance - MDPI, 2월 1, 2026에 액세스, https://www.mdpi.com/2227-9717/13/5/1598</li>
<li>Fo D- 6t su bo R sd ra w oT ni gn ip sa rG re gn iF- itl u M en ec S tic ilp xE hti w re tt ul C gn id na ts re dn U - Aaltodoc, 2월 1, 2026에 액세스, https://aaltodoc.aalto.fi/bitstreams/a4f51a0f-25c6-4a1b-a429-f2c2bd73e36e/download</li>
<li>Contact-graspnet: Efficient 6-dof grasp generation in cluttered scenes, 2월 1, 2026에 액세스, https://elib.dlr.de/145798/1/Contact-GraspNet.pdf</li>
<li>grasp-pose-generator - PyPI, 2월 1, 2026에 액세스, https://pypi.org/project/grasp-pose-generator/</li>
<li>[1501.03100] Using Geometry to Detect Grasps in 3D Point Clouds - arXiv, 2월 1, 2026에 액세스, https://arxiv.org/abs/1501.03100</li>
<li>Grasp the Graph (GtG) 2.0: Ensemble of Graph Neural Networks for High‑Precision Grasp Pose Detection in Clutter - arXiv, 2월 1, 2026에 액세스, https://arxiv.org/html/2505.02664v2</li>
<li>atenpas/gpg: Generate grasp pose candidates in point clouds - GitHub, 2월 1, 2026에 액세스, https://github.com/atenpas/gpg</li>
<li>atenpas/gpd: Detect 6-DOF grasp poses in point clouds - GitHub, 2월 1, 2026에 액세스, https://github.com/atenpas/gpd</li>
<li>PointNetGPD is an end-to-end grasp evaluation model to address the challenging problem of localizing robot grasp configurations directly from the point cloud. - GitHub, 2월 1, 2026에 액세스, https://github.com/lianghongzhuo/PointNetGPD</li>
<li>GraspNet-1Billion: A Large-Scale Benchmark for General Object Grasping - CVF Open Access, 2월 1, 2026에 액세스, https://openaccess.thecvf.com/content_CVPR_2020/papers/Fang_GraspNet-1Billion_A_Large-Scale_Benchmark_for_General_Object_Grasping_CVPR_2020_paper.pdf</li>
<li>Using Geometry to Detect Grasp Poses in 3D Point Clouds - Khoury College of Computer Sciences, 2월 1, 2026에 액세스, http://www.ccs.neu.edu/home/atp/publications/grasp_poses_isrr2015.pdf</li>
<li>High Precision Grasp Pose Detection in Dense Clutter - Khoury College of Computer Sciences, 2월 1, 2026에 액세스, https://www.khoury.northeastern.edu/home/mgualti/2016-Gualtieri-HighPrecisionGraspPoseDetectionInDenseClutter.pdf</li>
<li>(PDF) Grasp the Graph (GtG) 2.0: Ensemble of GNNs for High-Precision Grasp Pose Detection in Clutter - ResearchGate, 2월 1, 2026에 액세스, https://www.researchgate.net/publication/391461616_Grasp_the_Graph_GtG_20_Ensemble_of_GNNs_for_High-Precision_Grasp_Pose_Detection_in_Clutter</li>
<li>(PDF) Grasp Pose Detection in Point Clouds - ResearchGate, 2월 1, 2026에 액세스, https://www.researchgate.net/publication/318107041_Grasp_Pose_Detection_in_Point_Clouds</li>
<li>Review Paper: PointNetGPD- Detecting Grasp Configuration from Point Sets | by Isaac Kargar | Aidrivers Ltd. | Medium, 2월 1, 2026에 액세스, https://medium.com/aidrivers/review-paper-pointnetgpd-detecting-grasp-configuration-from-point-sets-6e4528b90885</li>
<li>PointNetGPD: Detecting Grasp Configurations from Point Sets - arXiv, 2월 1, 2026에 액세스, https://arxiv.org/pdf/1809.06267</li>
<li>PointNet++GPD:6-DOF grasping pose detection method based on object point cloud, 2월 1, 2026에 액세스, https://ieeexplore.ieee.org/document/10011999/</li>
<li>An efficient pose classification method for robotic grasping. - Cobot, 2월 1, 2026에 액세스, https://collaborative-robot.org/articles/1-7</li>
<li>6-DOF GraspNet: Variational Grasp Generation for Object Manipulation - CVF Open Access, 2월 1, 2026에 액세스, https://openaccess.thecvf.com/content_ICCV_2019/papers/Mousavian_6-DOF_GraspNet_Variational_Grasp_Generation_for_Object_Manipulation_ICCV_2019_paper.pdf</li>
<li>jsll/pytorch_6dof-graspnet - GitHub, 2월 1, 2026에 액세스, https://github.com/jsll/pytorch_6dof-graspnet</li>
<li>NVlabs/6dof-graspnet: Implementation of 6-DoF GraspNet … - GitHub, 2월 1, 2026에 액세스, https://github.com/NVlabs/6dof-graspnet</li>
<li>[paper-review] 6-DOF GraspNet: Variational Grasp Generation for Object Manipulation, 2월 1, 2026에 액세스, https://joonhyung-lee.github.io/blog/2023/6dof-graspnet/</li>
<li>AnyDexGrasp: Learning General Dexterous Grasping for Any Hands with Human-level Learning Efficiency - GraspNet, 2월 1, 2026에 액세스, https://graspnet.net/anydexgrasp/assets/files/AnyDexGrasp.pdf</li>
<li>Diffusion models for robotic manipulation: a survey - Frontiers, 2월 1, 2026에 액세스, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1606247/full</li>
<li>Diffusion models for robotic manipulation: a survey - PubMed, 2월 1, 2026에 액세스, https://pubmed.ncbi.nlm.nih.gov/40995149/</li>
<li>Variational Shape Inference for Grasp Diffusion on SE⁢(3) - arXiv, 2월 1, 2026에 액세스, https://arxiv.org/html/2508.17482v1</li>
<li>GraspDiff: Grasping Generation for Hand-Object Interaction With Multimodal Guided Diffusion | Request PDF - ResearchGate, 2월 1, 2026에 액세스, https://www.researchgate.net/publication/384268438_GraspDiff_Grasping_Generation_for_Hand-Object_Interaction_With_Multimodal_Guided_Diffusion</li>
<li>Variational Shape Inference for Grasp Diffusion on SE(3) - IEEE Xplore, 2월 1, 2026에 액세스, https://ieeexplore.ieee.org/iel8/7083369/11293803/11302796.pdf</li>
<li>EquiGraspFlow: SE(3)-Equivariant 6-DoF Grasp Pose Generative Flows - GitHub, 2월 1, 2026에 액세스, https://raw.githubusercontent.com/mlresearch/v270/main/assets/lim25a/lim25a.pdf</li>
<li>GraspDiffusion: Synthesizing Realistic Whole-body Hand-Object Interaction - arXiv, 2월 1, 2026에 액세스, https://arxiv.org/html/2410.13911v3</li>
<li>GraspDiff: Grasping Generation for Hand-Object Interaction With Multimodal Guided Diffusion - PubMed, 2월 1, 2026에 액세스, https://pubmed.ncbi.nlm.nih.gov/39312427/</li>
<li>GraspNet通用物体抓取, 2월 1, 2026에 액세스, https://graspnet.net/</li>
<li>ACRONYM: A Large-Scale Grasp Dataset Based on Simulation | Request PDF, 2월 1, 2026에 액세스, https://www.researchgate.net/publication/355429221_ACRONYM_A_Large-Scale_Grasp_Dataset_Based_on_Simulation</li>
<li>ACRONYM: A Large-Scale Grasp Dataset Based on Simulation, 2월 1, 2026에 액세스, https://sites.google.com/view/graspdataset</li>
<li>NVlabs/acronym: This repository contains a sample of the grasping dataset and tools to visualize grasps, generate random scenes, and render observations. The two sample files are in the HDF5 format. - GitHub, 2월 1, 2026에 액세스, https://github.com/NVlabs/acronym</li>
<li>ACRONYM: A Large-Scale Grasp Dataset Based on Simulation - IEEE Xplore, 2월 1, 2026에 액세스, https://ieeexplore.ieee.org/document/9560844</li>
<li>SuctionNet-1Billion - GraspNet, 2월 1, 2026에 액세스, https://graspnet.net/suction</li>
<li>10 Open Challenges Steering the Future of Vision-Language-Action Models - arXiv, 2월 1, 2026에 액세스, https://arxiv.org/html/2511.05936v1</li>
<li>Vision-Language-Action Models: Concepts, Progress, Applications and Challenges - arXiv, 2월 1, 2026에 액세스, https://arxiv.org/html/2505.04769v1</li>
<li>Fall 2025 GRASP on Robotics - Jie Tan, Google DeepMind - YouTube, 2월 1, 2026에 액세스, https://www.youtube.com/watch?v=fXiKQk-IzqQ</li>
<li>Sim-Grasp: Learning 6-DOF Grasp Policies for Cluttered Environments Using a Synthetic Benchmark, 2월 1, 2026에 액세스, https://ntrs.nasa.gov/api/citations/20250002252/downloads/Sim_Grasp_final_1.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>