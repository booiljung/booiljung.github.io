<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.5.3 어포던스 기반 파지(Affordance-guided Grasping): "잡을 수 있는 곳" vs "잡아야 하는 곳" (Task-Oriented)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.5.3 어포던스 기반 파지(Affordance-guided Grasping): "잡을 수 있는 곳" vs "잡아야 하는 곳" (Task-Oriented)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <a href="index.html">7.5 파지 검출과 생성 (Grasp Detection & Generation)</a> / <span>7.5.3 어포던스 기반 파지(Affordance-guided Grasping): "잡을 수 있는 곳" vs "잡아야 하는 곳" (Task-Oriented)</span></nav>
                </div>
            </header>
            <article>
                <h1>7.5.3 어포던스 기반 파지(Affordance-guided Grasping): “잡을 수 있는 곳” vs “잡아야 하는 곳” (Task-Oriented)</h1>
<p>로봇이 실생활 환경에서 인간과 유사한 수준의 조작 능력을 갖추기 위해서는 단순히 물체를 떨어뜨리지 않고 들어 올리는 기하학적 파지를 넘어, 수행하려는 작업의 목적에 부합하는 파지 지점을 결정하는 능력이 필수적이다. 전통적인 로봇 파지 연구는 물체의 외형과 깊이 정보를 바탕으로 물리적 안정성(Stability)을 극대화하는 지점을 찾는 데 집중해 왔다. 그러나 이러한 방식은 칼의 날 부분을 잡아 전달하거나, 물을 따라야 하는 컵의 입구 부분을 움켜쥐는 등 후속 작업을 불가능하게 만드는 오류를 범하기 쉽다. 이러한 한계를 극복하기 위해 등장한 개념이 바로 어포던스 기반 파지(Affordance-guided Grasping)이며, 이는 “잡을 수 있는 곳(Can Grasp)“이라는 기하학적 허용 범위와 “잡아야 하는 곳(Should Grasp)“이라는 기능적 요구 사항 사이의 정교한 정렬을 요구한다.</p>
<h2>1. 어포던스 이론의 로봇 공학적 전사</h2>
<p>어포던스(Affordance)는 심리학자 제임스 깁슨(James Gibson)이 제안한 개념으로, 환경이 유기체에게 제공하는 행동 가능성을 의미한다. 로봇 공학에서 어포던스는 물체의 시각적 특징과 로봇이 수행 가능한 행동 사이의 매핑으로 정의된다. 예를 들어, 주전자의 ’손잡이’는 ’파지 가능성’을 제공하고, ’주둥이’는 ’액체 배출 가능성’을 제공한다. 로봇이 이러한 어포던스를 이해한다는 것은 단순히 객체를 인식하는 수준을 넘어, 객체의 부위별 기능을 파악하고 이를 작업 계획에 통합할 수 있음을 뜻한다.</p>
<p>작업 지향적 파지(Task-Oriented Grasping, TOG)는 이러한 어포던스 이해를 파지 포즈 생성 과정의 핵심 제약 조건으로 활용한다. 이는 물체의 기하학적 구조만을 고려하는 작업 불가지론적 파지(Task-Agnostic Grasping, TAG)와 대조되는 개념이다. 로봇 지능의 진화에 따라 어포던스 기반 파지는 고정된 카테고리의 물체를 다루는 수준에서 벗어나, 한 번도 본 적 없는 물체에 대해서도 자연어 지시문을 바탕으로 적절한 파지 부위를 추론하는 오픈 보캐블러리(Open-Vocabulary) 단계로 진입하고 있다.</p>
<h2>2. 시각적 어포던스 인지의 기술적 변천</h2>
<p>로봇이 사물의 어포던스를 인지하기 위해서는 이미지나 포인트 클라우드 데이터에서 의미 있는 부위를 분할하고 레이블링하는 과정이 선행되어야 한다. 초기 딥러닝 기반 어포던스 연구는 주로 픽셀 단위의 분할(Pixel-wise Segmentation) 모델에 의존했다.</p>
<h3>2.1 AffordanceNet의 구조와 멀티태스크 학습</h3>
<p><code>AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection</code>은 RGB 이미지에서 객체 탐지와 어포던스 분할을 동시에 수행하는 선구적인 아키텍처를 제시했다. 이 모델은 백본 네트워크를 통해 추출된 특징 맵을 두 개의 분기로 나눈다. 첫 번째 분기는 객체의 위치와 클래스를 식별하고, 두 번째 분기는 객체 내부의 각 픽셀에 대해 ‘grasp’, ‘cut’, ‘contain’, ‘pour’ 등 다중 클래스 어포던스 레이블을 할당한다.</p>
<p>이 과정에서 정밀한 마스크 생성을 위해 연속적인 Deconvolution 레이어가 사용되며, ROI Align 기술을 통해 특징 손실을 최소화한다. AffordanceNet의 학습을 최적화하는 멀티태스크 손실 함수는 다음과 같이 구성된다.<br />
<span class="math math-display">
L = L_{cls} + L_{box} + L_{aff}
</span><br />
여기서 <span class="math math-inline">L_{aff}</span>는 픽셀별 다중 클래스 분류 성능을 결정짓는 핵심 요소로, 로봇이 물체의 어느 부위가 특정 행동에 적합한지를 수치적으로 판단할 수 있게 한다.</p>
<table><thead><tr><th><strong>기술적 특징</strong></th><th><strong>AffordanceNet의 기여</strong></th></tr></thead><tbody>
<tr><td>아키텍처</td><td>객체 탐지 분기와 어포던스 분할 분기의 통합 (End-to-End)</td></tr>
<tr><td>업샘플링</td><td>7x7 특징 맵을 244x244 고해상도 마스크로 복원하는 3단계 Deconvolution</td></tr>
<tr><td>스코어링</td><td>Softmax 레이어를 통한 픽셀 단위 다중 어포던스 확률 계산</td></tr>
<tr><td>성능</td><td>이미지당 약 150ms의 추론 속도로 실시간 로봇 제어 가능</td></tr>
</tbody></table>
<h3>2.2 포인트 클라우드 기반의 3D 어포던스 접지</h3>
<p>최근의 SOTA 기술은 2D 이미지를 넘어 3D 포인트 클라우드에서 직접 어포던스를 추론하는 방향으로 나아가고 있다. <code>3D-AffordanceNet</code>과 같은 데이터셋은 물체의 표면 포인트마다 어포던스 점수를 부여하여, 로봇이 3차원 공간에서 파지 포즈를 직접 계산할 수 있도록 돕는다. 특히 프로토타입 학습(Prototypical Learning)을 적용한 모델들은 블랙박스 구조의 신경망과 달리, 입력된 포인트 클라우드가 기존에 학습된 어떤 전형적인 어포던스 형태와 유사한지를 비교함으로써 추론 결과에 대한 해석 가능성을 제공한다.</p>
<h2>3. 작업 지향적 파지(TOG)와 기하학적 파지(TAG)의 정렬</h2>
<p>작업 지향적 파지 시스템의 핵심 문제는 생성된 수많은 파지 후보군 중에서 현재의 작업 문맥에 가장 적합한 것을 선별하는 것이다. 이는 기하학적 안정성 점수(<span class="math math-inline">S_{geo}</span>)와 어포던스 적합성 점수(<span class="math math-inline">S_{aff}</span>)를 결합하는 최적화 문제로 요약된다.</p>
<h3>3.1 “잡을 수 있는 곳“의 기하학적 제약</h3>
<p>기하학적 파지는 로봇 그리퍼가 물체와 접촉했을 때 미끄러지지 않고 견고하게 고정될 수 있는 위치를 찾는 과정이다. <code>Contact-GraspNet</code>은 깊이 정보를 바탕으로 6-DoF 파지 포즈를 직접 생성하며, 힘 폐쇄(Force-closure) 조건을 만족하는 수많은 후보를 제안한다. 이 단계에서 로봇은 “어디를 잡아야 물리적으로 안전한가“에 대한 해답을 얻는다. 하지만 이는 “칼날을 잡아도 안정적이라면 잡겠다“는 위험한 결론에 도달할 수 있다.</p>
<h3>3.2 “잡아야 하는 곳“의 기능적 선택</h3>
<p>기능적 선택은 어포던스 맵이나 시맨틱 세그멘테이션 결과를 필터로 활용하여 이루어진다. <code>OVAL-Grasp</code>와 같은 최신 프레임워크는 이 과정을 히트맵(Heatmap) 기반의 스코어링 시스템으로 구현했다.</p>
<ol>
<li><strong>사용자 지시문 해석:</strong> “드라이버를 건네줘“라는 명령이 들어오면 LLM은 ’손잡이’를 파지 부위로, ’비트(날)’를 피해야 할 부위로 규정한다.</li>
<li><strong>어포던스 히트맵 생성 (<span class="math math-inline">H</span>):</strong> VLM은 이미지에서 해당 부위들을 찾아내어, 손잡이 영역에는 양(+)의 값을, 날 영역에는 음(-)의 값을 부여한 히트맵을 생성한다.</li>
<li><strong>통합 스코어링:</strong> 생성된 파지 후보 <span class="math math-inline">g</span>의 접촉점 <span class="math math-inline">(x, y)</span>가 히트맵 <span class="math math-inline">H</span>에서 갖는 값을 바탕으로 최종 순위를 결정한다.</li>
</ol>
<table><thead><tr><th><strong>파지 결정 인자</strong></th><th><strong>수식적 표현 및 의미</strong></th></tr></thead><tbody>
<tr><td>기하학적 안정성 (<span class="math math-inline">S_{geo}</span>)</td><td>그리퍼와 물체 표면의 접촉 일치성 및 마찰력 한계</td></tr>
<tr><td>어포던스 일치성 (<span class="math math-inline">S_{aff}</span>)</td><td>히트맵 <span class="math math-inline">H</span>에서의 픽셀 값 투영 (<span class="math math-inline">S_{aff} = H(x, y)</span>)</td></tr>
<tr><td>접근 방향 제약 (<span class="math math-inline">S_{z}</span>)</td><td>그리퍼의 접근 벡터와 물체의 기능적 축 사이의 정렬도</td></tr>
<tr><td>물리적 페널티 (<span class="math math-inline">C</span>)</td><td>토크(<span class="math math-inline">\tau</span>), 미끄러짐(<span class="math math-inline">s</span>), 법선 정렬(<span class="math math-inline">\alpha</span>)의 가중 합</td></tr>
</tbody></table>
<p>최종 파지 포즈 <span class="math math-inline">g^*</span>는 다음과 같이 정의된 목적 함수를 최대화하는 지점으로 결정된다.<br />
<span class="math math-display">
\begin{equation}
    g^* = \arg\max_{g} \left[ \underbrace{w_1 S_{geo}(g) + w_2 H(proj(g)) + w_3 S_{z}(g)}_{S_{total}} - \lambda C(\tau, s, \alpha) \right]
\end{equation}
</span><br />
여기서 <span class="math math-inline">\xi</span>는 파지 이후 수행될 궤적을 의미하며, <span class="math math-inline">proj</span>는 3D 공간의 파지 지점을 2D 히트맵으로 투영하는 함수다.</p>
<h2>4. 비전-언어 모델(VLM)을 통한 문맥 의존적 추론</h2>
<p>최신 SOTA 모델들의 가장 큰 변화는 어포던스 인지가 고정된 레이블 학습에서 유연한 문맥 추론으로 진화했다는 점이다. <code>AffordGrasp</code> 및 <code>VCoT-Grasp</code>는 거대 모델의 추론 능력을 직접 파지 파이프라인에 이식했다.</p>
<h3>4.1 암시적 명령과 In-context Reasoning</h3>
<p><code>AffordGrasp</code>는 사용자의 명시적인 지시(예: “머그잔의 손잡이를 잡아”)뿐만 아니라 암시적인 의도(예: “커피를 마시고 싶어”)를 이해하는 인문맥 어포던스 추론(In-context Affordance Reasoning) 모듈을 탑재했다. 이 모듈은 VLM을 사용하여 장면 내의 객체들 사이의 관계를 파악하고, 현재 상황에서 가장 적절한 ’작업’이 무엇인지를 스스로 유추한다. 이후 해당 작업에 필요한 어포던스 부위를 시각적으로 접지(Grounding)하여 파지 지점을 결정한다.</p>
<h3>4.2 Visual Chain-of-Thought (VCoT)</h3>
<p><code>VCoT-Grasp</code>는 인간이 복잡한 장면을 볼 때 중요한 부분에 시선을 고정하고 세부 사항을 파악하는 방식에서 영감을 얻었다. 이 모델은 단 한 번의 추론으로 파지 포즈를 내뱉는 대신, 다음과 같은 다단계 추론 과정을 거친다.</p>
<ol>
<li><strong>객체 식별 및 바운딩 박스 예측:</strong> 지시문에 부합하는 대상을 탐지한다.</li>
<li><strong>시각적 집중(Visual Zoom-in):</strong> 탐지된 객체 영역을 크롭하여 고해상도로 다시 입력한다.</li>
<li><strong>세부 어포던스 분석:</strong> 크롭된 이미지 내에서 파지할 구체적인 위치와 각도를 계산한다.</li>
</ol>
<p>이러한 ‘이미지로 생각하기’ 방식은 복잡하게 뒤섞인 환경(Cluttered Scenes)에서 타겟 물체를 오인하거나 엉뚱한 부위를 잡을 확률을 획기적으로 낮추어, 기존 모델 대비 미학습 물체에 대한 일반화 성능을 30% 이상 향상시켰다.</p>
<h2>5. 기하학적 분해와 심볼릭 지식의 결합</h2>
<p>신경망 기반의 추론 외에도 물체의 구조적 특성을 기하학적으로 해석하여 어포던스를 도출하는 연구도 활발하다. <code>ShapeGrasp</code>는 물체를 단순한 포인트의 집합이 아니라, 상호 연결된 볼록 기하 구조(Convex Primitives)의 그래프로 이해한다.</p>
<h3>5.1 볼록 분해와 형태 그래프(Shape Graph)</h3>
<p>로봇이 마주하는 복잡한 객체는 CoACD 알고리즘 등을 통해 여러 개의 볼록한 부품으로 분해된다. 각 부품은 형태 그래프의 노드가 되며, 부품 간의 인접성은 에지로 표현된다. LLM은 이 그래프 구조와 물체 이름을 입력받아 각 노드에 시맨틱 레이블을 부여한다. 예를 들어, T자 형태로 결합된 두 원기둥 중 긴 쪽에는 ’handle’을, 짧고 뭉툭한 쪽에는 ‘head’ 레이블을 할당하는 식이다.</p>
<p>이러한 방식의 장점은 시각 데이터가 불완전하거나 가려짐(Occlusion)이 발생하더라도, 물체의 전반적인 구조적 연결성을 통해 보이지 않는 부분의 어포던스를 상식적으로 추론할 수 있다는 점이다. 또한 <code>SemanticCSG</code>로 확장될 경우, 로봇은 물체를 구성하는 기본 입체 도형들의 불리언 연산(합집합, 교집합 등) 조합으로 물체를 재구성하여 더욱 강건한 3차원 어포던스 지도를 생성한다.</p>
<h2>6. 생성형 모델 기반의 고차원 파지 합성</h2>
<p>최근의 제어 기술은 확산 모델(Diffusion Models)을 파지 포즈 생성의 중추로 활용하고 있다. 이는 파지 공간의 다봉성(Multimodality)을 효과적으로 모델링할 수 있기 때문이다.</p>
<h3>6.1 HGDiffuser와 확산 기반 제약 조건 통합</h3>
<p><code>HGDiffuser (Human-guided Grasp Diffuser)</code>는 인간의 파지 데이터를 학습한 후, 이를 로봇의 파지 생성 과정에서 가이드로 활용한다. 기존의 2단계 방식(샘플링 후 필터링)은 광활한 6-DoF 공간에서 무작위 샘플링을 수행하므로 효율성이 낮았다. 반면 HGDiffuser는 확산 모델의 점수 기반 샘플링 과정에 인간의 파지 제약 조건을 직접 주입한다.</p>
<p>로봇은 노이즈가 섞인 초기 파지 상태에서 시작하여, “손잡이 영역에 접촉해야 한다“와 “작업 수행을 위해 손목 각도는 수평을 유지해야 한다“는 두 가지 점수 함수의 유도를 받으며 점진적으로 최적의 파지 포즈로 수렴한다. 이러한 방식은 추론 시간을 1초대에서 0.19초대로 단축시키며 실시간 조작의 가능성을 열었다.</p>
<h3>6.2 Affordance Dexterous Grasp (AffordDexGrasp)</h3>
<p>다지 로봇 손(Dexterous Hand)의 경우, 단순히 파지 위치뿐만 아니라 수십 개의 관절 각도를 결정해야 하므로 복잡도가 비약적으로 상승한다. <code>AffordDexGrasp</code> 프레임워크는 이를 해결하기 위해 흐름 매칭(Flow Matching) 기법을 도입했다.</p>
<ul>
<li><strong>Affordance Flow Matching (AFM):</strong> 언어 지시문에 따라 물체 표면의 잠재적인 파지 가능 영역을 확률 분포 형태로 생성한다.</li>
<li><strong>Grasp Flow Matching (GFM):</strong> 생성된 어포던스 분포를 경계 조건으로 삼아, 로봇 손의 관절 궤적을 목표 파지 형상으로 ‘밀어내는’ 벡터장을 형성한다.</li>
</ul>
<p>이 모델은 카테고리에 무관한(Category-agnostic) 의미론적 속성을 학습함으로써, 학습 데이터에 포함되지 않은 새로운 형태의 도구에 대해서도 즉각적으로 기능적 파지를 수행할 수 있는 능력을 보여주었다.</p>
<h2>7. 도구 사용 및 동적 환경에서의 물리적 정렬</h2>
<p>어포던스 기반 파지의 궁극적인 목표 중 하나는 도구 사용(Tool Use)이다. 도구를 사용하는 작업에서는 파지의 안정성뿐만 아니라, 도구를 통해 전달되는 힘의 효율성과 작업의 동역학적 특성이 고려되어야 한다.</p>
<h3>7.1 iTuP: 물리 제약 기반의 도구 사용 계획</h3>
<p><code>iTuP (inverse Tool-use Planning)</code> 프레임워크는 물리 시뮬레이션과 VLM의 추론을 결합하여 도구 사용에 최적화된 파지를 찾는다. 이 시스템은 단순히 ’어디를 잡느냐’를 넘어 ’어떻게 힘을 쓸 것이냐’를 계산한다. 예를 들어 망치질을 할 때, 로봇은 타격 지점에서 발생하는 충격력이 그리퍼의 조(Jaw) 방향으로 어떻게 분산되는지를 분석한다.</p>
<table><thead><tr><th><strong>물리적 페널티 항목</strong></th><th><strong>계산 메커니즘 및 목적</strong></th></tr></thead><tbody>
<tr><td>상호작용 토크 (<span class="math math-inline">\hat{\tau}</span>)</td><td>파지점과 도구의 무게중심, 작업 작용점 사이의 모멘트 계산. 최소화 시 그리퍼 모터 부하 감소</td></tr>
<tr><td>미끄러짐 마진 (<span class="math math-inline">\hat{s}</span>)</td><td>접촉면의 법선력과 마찰 계수를 고려한 최대 허용 전단력 계산. 도구 이탈 방지</td></tr>
<tr><td>법선 정렬 (<span class="math math-inline">\hat{\alpha}</span>)</td><td>그리퍼의 손가락 표면 법선과 도구 표면의 일치도. 접촉 면적 극대화</td></tr>
</tbody></table>
<p>이러한 물리적 페널티는 VLM이 제안한 어포던스 영역 내에서 최종적인 파지 지점을 미세 조정(Fine-tuning)하는 기준으로 사용된다. 결과적으로 로봇은 “잡아야 하는 곳” 중에서도 “가장 힘을 잘 쓸 수 있는 곳“을 선택하게 된다.</p>
<h2>8. 데이터 중심의 어포던스 학습과 벤치마크</h2>
<p>어포던스 지능의 고도화는 고품질의 대규모 데이터셋 구축에 크게 의존하고 있다. 특히 시각-언어-행동의 연결 고리를 강화하기 위한 새로운 데이터 플랫폼들이 등장하고 있다.</p>
<h3>8.1 RAGNet과 RoboAfford++</h3>
<p><code>RAGNet</code>은 27만 개 이상의 이미지와 2.6만 개의 추론 기반 지시문을 결합한 대규모 벤치마크다. 이 데이터셋의 특징은 “망치를 집어줘” 같은 단순 명령이 아니라, “못을 박고 싶은데 어떤 도구를 어떻게 잡아야 할까?“와 같은 상황적 추론을 요구하는 지시문들로 구성되어 있다는 점이다.</p>
<p>반면 <code>RoboAfford++</code>는 조작(Manipulation)뿐만 아니라 이동(Navigation)을 위한 공간적 어포던스(Spatial Affordance)까지 통합했다. 로봇이 물체를 어디에 배치할 수 있는지, 혹은 어디로 이동할 수 있는지에 대한 정보를 87만 장의 이미지와 200만 개의 QA 쌍으로 제공하여, 파지 이후의 단계까지 고려한 통합 지능 학습을 지원한다.</p>
<h3>8.2 에고센트릭 비전을 통한 무감독 학습</h3>
<p>인간이 도구를 사용하는 1인칭 시점 영상(Egocentric Videos)은 어포던스 학습의 보고다. 최신 연구들은 <code>Epic-Kitchens</code>나 <code>Ego4d</code> 같은 데이터셋에서 인간의 손과 물체의 상호작용 지점을 자동으로 추출하여 학습 데이터를 생성한다. <code>Aff-Grasp</code> 시스템은 이러한 영상에서 인간이 잡는 부위(Graspable)와 실제로 도구의 기능이 발현되는 부위(Functional)를 분리하여 인식함으로써, 로봇이 인간의 행동 양식을 모방하면서도 기계적인 제약 조건을 준수하도록 유도한다.</p>
<table><thead><tr><th><strong>데이터셋 명칭</strong></th><th><strong>규모 및 데이터 형식</strong></th><th><strong>핵심 기능</strong></th></tr></thead><tbody>
<tr><td>RAGNet</td><td>273k Images / 26k Instructions</td><td>인간과 유사한 추론 기반 어포던스 세그멘테이션</td></tr>
<tr><td>RoboAfford++</td><td>870k Images / 2M QAs</td><td>객체 및 공간 어포던스의 통합, 다중 모달 학습</td></tr>
<tr><td>VCoT-GraspSet</td><td>167k Synthetic / 1.36M Grasps</td><td>Visual Chain-of-Thought를 위한 단계별 바운딩 박스 포함</td></tr>
<tr><td>PartNet-Mobility</td><td>2.3k Articulated Models</td><td>가동 부위(문, 서랍 등)에 대한 세밀한 부품 레이블링</td></tr>
</tbody></table>
<h2>9. 씬(Scene) 레벨의 복합 어포던스 이해</h2>
<p>실제 환경에서 로봇은 단일 물체가 아닌, 수많은 물체가 뒤섞인 환경을 마주한다. 이 경우 어포던스 인지는 타겟 물체 식별, 장애물 회피, 그리고 파지 가능성 판단이 얽힌 복합적인 문제가 된다.</p>
<p><code>FreeGrasp</code>와 같은 시스템은 VLM의 세계 지식(World Knowledge)을 활용하여 장면 내 물체들의 공간적 배치를 분석한다. 예를 들어, 잡고자 하는 컵이 다른 접시들에 둘러싸여 있다면, VLM은 직접적인 파지가 불가능함을 판단하고 “접시를 먼저 옮긴 후 컵을 잡는다“는 계층적인 행동 계획을 세운다. 이 과정에서 ’방해물’이라는 부정적 어포던스와 ’목표물’이라는 긍정적 어포던스가 동적으로 상호작용하며 최종 파지 전략을 결정짓는다.</p>
<p>또한, <code>AffordGrasp</code>는 가려짐이 심한 환경에서 다각도 관찰을 통해 정보를 보충하는 능동적 지각(Active Perception)과 결합하여, 부분적인 포인트 클라우드만으로도 전체 물체의 기능적 구조를 복원하고 최적의 파지 지점을 찾아낸다.</p>
<h2>10. 결론 및 미래 전망</h2>
<p>어포던스 기반 파지는 로봇을 단순한 기계 장치에서 주변 환경의 의미와 기능을 이해하는 인지체로 진화시키는 핵심 기술이다. “잡을 수 있는 곳“에 대한 기하학적 확신이 로봇의 신체적 한계를 규정한다면, “잡아야 하는 곳“에 대한 어포던스 추론은 로봇의 지능적 목적성을 부여한다.</p>
<p>향후 이 분야는 다음과 같은 방향으로 더욱 고도화될 것이다. 첫째, 시각과 언어를 넘어 촉각(Tactile) 및 오디오 정보를 통합하여, 물체를 잡는 순간의 재질감이나 소리를 통해 파지의 적절성을 실시간으로 수정하는 멀티모달 폐루프 시스템이 주류가 될 것이다. 둘째, 세계 모델(World Models)과의 결합을 통해 파지 이후에 발생할 물리적 변화를 가상으로 시뮬레이션하고, 그 결과가 작업 성공에 미칠 영향을 미리 예측하여 파지 포즈를 최적화하는 능력이 강화될 것이다.</p>
<p>결국 어포던스 기반 파지 기술의 발전은 로봇이 인간의 생활 공간에 자연스럽게 녹아들어, 도구를 자유자재로 다루며 복잡한 가사 노동이나 산업 현장의 조립 업무를 수행할 수 있는 진정한 의미의 Embodied AI를 실현하는 토대가 될 것이다.</p>
<h2>11. 참고 자료</h2>
<ol>
<li>AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary Task-Oriented Grasping in Clutter - arXiv, https://arxiv.org/html/2503.00778v1</li>
<li>AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary Task-Oriented Grasping in Clutter - GitHub Pages, https://eqcy.github.io/affordgrasp/static/paper/IROS2025_AffordGrasp.pdf</li>
<li>Visual Affordances: Enabling Robots to Understand Object Functionality - arXiv, https://arxiv.org/html/2505.05074v1</li>
<li>Learning Precise Affordances from Egocentric Videos for Robotic Manipulation - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Learning_Precise_Affordances_from_Egocentric_Videos_for_Robotic_Manipulation_ICCV_2025_paper.pdf</li>
<li>Learning Precise Affordances from Egocentric Videos for Robotic Manipulation - arXiv, https://arxiv.org/html/2408.10123v1</li>
<li>HGDiffuser: Efficient Task-Oriented Grasp Generation via Human-Guided Grasp Diffusion Models - arXiv, https://arxiv.org/html/2503.00508v1</li>
<li>Manipulability-Aware Task-Oriented Grasp Planning and Motion Control with Application in a Seven-DoF Redundant Dual-Arm Robot - MDPI, https://www.mdpi.com/2079-9292/13/24/5025</li>
<li>OVAL-Grasp: Open-Vocabulary Affordance Localization for Task Oriented Grasping - arXiv, https://arxiv.org/html/2511.20841v1</li>
<li>arXiv:1709.07326v3 [cs.CV] 4 Mar 2018 - Computer Science, https://www.csc.liv.ac.uk/~anguyen/assets/pdfs/2018_ICRA_AffordanceNet.pdf</li>
<li>AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection, https://www.researchgate.net/publication/319977466_AffordanceNet_An_End-to-End_Deep_Learning_Approach_for_Object_Affordance_Detection</li>
<li>[1709.07326] AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection - arXiv, https://arxiv.org/abs/1709.07326</li>
<li>AffordanceNet: Deep Object Affordance Detection - Emergent Mind, https://www.emergentmind.com/papers/1709.07326</li>
<li>Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention - arXiv, https://arxiv.org/html/2405.18295v2</li>
<li>Interpretable Affordance Detection on 3D Point Clouds with Probabilistic Prototypes - arXiv, https://arxiv.org/html/2504.18355v1</li>
<li>PartAfford: Part-level Affordance Discovery from Cross-category 3D Objects, https://geometry.stanford.edu/voli/papers/4.pdf</li>
<li>(PDF) GRIM: Task-Oriented Grasping with Conditioning on Generative Examples, https://www.researchgate.net/publication/392839004_GRIM_Task-Oriented_Grasping_with_Conditioning_on_Generative_Examples</li>
<li>Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes | Request PDF, https://www.researchgate.net/publication/355434800_Contact-GraspNet_Efficient_6-DoF_Grasp_Generation_in_Cluttered_Scenes</li>
<li>6-DoF Grasp Learning in Partially Observable Cluttered Scenes, https://elib.dlr.de/148348/1/thesis.pdf</li>
<li>Physics-Constrained Robot Grasp Planning for Dynamic Tool Use - arXiv, https://arxiv.org/html/2505.01399v2</li>
<li>Open-Vocabulary Affordance Localization for Task Oriented Grasping - arXiv, https://arxiv.org/pdf/2511.20841</li>
<li>(PDF) OVAL-Grasp: Open-Vocabulary Affordance Localization for Task Oriented Grasping, https://www.researchgate.net/publication/398026550_OVAL-Grasp_Open-Vocabulary_Affordance_Localization_for_Task_Oriented_Grasping</li>
<li>[2503.00778] AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary Task-Oriented Grasping in Clutter - arXiv, https://arxiv.org/abs/2503.00778</li>
<li>[Papierüberprüfung] VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought Reasoning for Language-driven Grasp Generation - Moonlight, https://www.themoonlight.io/de/review/vcot-grasp-grasp-foundation-models-with-visual-chain-of-thought-reasoning-for-language-driven-grasp-generation</li>
<li>VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought Reasoning for Language-driven Grasp Generation - arXiv, https://arxiv.org/html/2510.05827v1</li>
<li>VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought Reasoning for Language-driven Grasp Generation - arXiv, <a href="https://arxiv.org/pdf/2510.05827">https://arxiv.org/pdf/2510.05827?</a></li>
<li>[2510.05827] VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought Reasoning for Language-driven Grasp Generation - arXiv, https://arxiv.org/abs/2510.05827</li>
<li>3D Understanding and Zero-Shot Task-Oriented Grasping via …, https://www.ri.cmu.edu/app/uploads/2025/05/MSR_thesis_samuel_li.pdf</li>
<li>Geometric Shape Reasoning for Zero-Shot Task-Oriented Grasping - OpenReview, https://openreview.net/pdf?id=nwjsY2OIJO</li>
<li>AffordDexGrasp: Open-set Language-guided Dexterous Grasp with Generalizable-Instructive Affordance - arXiv, https://arxiv.org/html/2503.07360v1</li>
<li>AffordDexGrasp: Open-set Language-guided Dexterous Grasp with Generalizable-Instructive Affordance, https://openaccess.thecvf.com/content/ICCV2025/papers/Wei_AffordDexGrasp_Open-set_Language-guided_Dexterous_Grasp_with_Generalizable-Instructive_Affordance_ICCV_2025_paper.pdf</li>
<li>FoundationGrasp: Generalizable Task-Oriented Grasping with Foundation Models - arXiv, https://arxiv.org/html/2404.10399v2</li>
<li>RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping, https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_RAGNet_Large-scale_Reasoning-based_Affordance_Segmentation_Benchmark_towards_General_Grasping_ICCV_2025_paper.pdf</li>
<li>RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation - arXiv, https://arxiv.org/html/2511.12436v1</li>
<li>RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation - ResearchGate, https://www.researchgate.net/publication/397700201_RoboAfford_A_Generative_AI-Enhanced_Dataset_for_Multimodal_Affordance_Learning_in_Robotic_Manipulation_and_Navigation</li>
<li>PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding - arXiv, https://arxiv.org/html/2510.20155v1</li>
<li>Paper page - Free-form language-based robotic reasoning and grasping - Hugging Face, https://huggingface.co/papers/2503.13082</li>
<li>Enhancing task-oriented robotic grasping via 3D affordance grounding from vision-language models - ResearchGate, https://www.researchgate.net/publication/399157968_Enhancing_task-oriented_robotic_grasping_via_3D_affordance_grounding_from_vision-language_models</li>
<li>LieGrasPFormer: Point Transformer-based 6-DOF Grasp Detection with Lie Algebra Grasp Representation - mediaTUM, https://mediatum.ub.tum.de/doc/1720412/d0rg0kp16l5shyo6ag683v2hi.lieGrasPFormer.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>