<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.5.4 촉각(Tactile) 정보를 예상하는 시각 지능: 미끄러짐과 재질 추정</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.5.4 촉각(Tactile) 정보를 예상하는 시각 지능: 미끄러짐과 재질 추정</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <a href="index.html">7.5 파지 검출과 생성 (Grasp Detection & Generation)</a> / <span>7.5.4 촉각(Tactile) 정보를 예상하는 시각 지능: 미끄러짐과 재질 추정</span></nav>
                </div>
            </header>
            <article>
                <h1>7.5.4 촉각(Tactile) 정보를 예상하는 시각 지능: 미끄러짐과 재질 추정</h1>
<p>로봇이 인간과 유사한 수준의 정밀한 조작 능력을 갖추기 위해서는 단순히 물체를 식별하고 위치를 파악하는 시각적 능력을 넘어, 접촉 시 발생할 물리적 상호작용을 예측하는 고도의 인지 체계가 필수적이다. Embodied AI의 관점에서 촉각은 물체와의 직접적인 소통 채널이며, 시각은 이러한 접촉이 일어나기 전 혹은 접촉 중에 발생할 물리적 변화를 사전에 시뮬레이션하는 예측 기제로서 기능한다. 특히 “7.5.4 촉각(Tactile) 정보를 예상하는 시각 지능: 미끄러짐과 재질 추정“에서는 로봇이 물체를 만지기도 전에 외형만으로 그 물체의 마찰력, 강성, 질감을 유추하고, 파지 과정에서 발생할 수 있는 미끄러짐을 실시간으로 예견하는 최신 SOTA(State-of-the-Art) 기술들을 다룬다.</p>
<h2>1. 시각과 촉각의 교차 모달성: 인지적 연결의 기원</h2>
<p>인간의 뇌는 시각적 관찰을 통해 대상의 촉각적 성질을 유추하는 데 탁월한 능력을 갖추고 있다. 차가운 금속의 질감이나 부드러운 직물의 느낌은 직접 만져보지 않아도 시각적 텍스처와 빛의 반사 패턴을 통해 뇌 내에서 재구성된다. 이러한 ’시각-촉각 교차 모달성(Visuo-Tactile Cross-modality)’은 로봇 지능에서도 핵심적인 연구 분야로 부상하였다. 로봇에게 이러한 능력을 부여하는 것은 조작의 효율성을 극대화하기 위함이다. 만약 로봇이 물체를 잡기 전에 그것이 미끄러운 유리인지, 혹은 마찰력이 큰 고무인지 미리 안다면, 파지력(Grasping Force)을 최적으로 설정하여 에너지 소모를 줄이고 물체의 파손을 방지할 수 있다.</p>
<p>최근의 연구인 “Connecting Touch and Vision via Cross-Modal Prediction“은 이러한 연결을 자가 지도 학습(Self-supervised Learning) 프레임워크로 구현하였다. 이 연구는 로봇 팔에 고해상도 시각 기반 촉각 센서인 GelSight를 장착하고 외부 웹캠을 통해 조작 과정을 촬영하여 대규모의 대응 데이터셋을 구축하였다. 핵심 아이디어는 시각적 특징으로부터 촉각 신호를 생성(Vision-to-Touch)하고, 반대로 촉각 신호로부터 시각적 장면을 상상(Touch-to-Vision)하는 양방향 예측 모델을 구축하는 것이다. 이는 로봇이 ‘눈으로 느끼고’ ‘손으로 보는’ 통합적 지각 시스템을 갖추게 됨을 의미한다.</p>
<h2>2. 비전 기반 촉각 센싱 기술의 비약적 발전</h2>
<p>시각 지능이 촉각 정보를 예측하기 위해서는 먼저 촉각 정보를 시각적 데이터 형태로 변환하여 처리할 수 있는 하드웨어적 토대가 필요하다. GelSight로 대표되는 비전 기반 촉각 센서(Vision-based Tactile Sensor)는 조명 시스템과 카메라, 그리고 유연한 멤브레인을 결합하여 접촉면의 미세한 변형을 고해상도 이미지로 캡처한다. 이러한 센서는 전통적인 압력 센서 어레이가 제공하지 못하는 표면의 3D 기하학적 구조, 마커의 흐름장(Flow Field), 그리고 미세한 텍스처 정보를 제공한다.</p>
<p>GelSight 외에도 TacTip과 같은 센서들은 생체 모방형 구조를 채택하여 인간의 손가락 끝과 유사한 감도를 구현하였다. “Design and development of a robust vision-based tactile sensor“에서는 이러한 센서들이 제공하는 데이터가 단순한 압력 분포를 넘어, 물체의 형상(Shape), 질감(Texture), 경도(Hardness)를 파악하는 데 결정적인 피드백을 제공함을 강조한다. 특히 최신 SOTA 모델인 Tac3D는 3D 변형량과 접촉력 분포를 동시에 측정하여 물체의 마찰 계수 분포를 재구성하는 능력을 갖추고 있다.</p>
<h2>3. Vision-to-Touch: 시각으로부터 촉각적 반응의 예측</h2>
<p>시각 정보를 촉각 신호로 변환하는 기술의 핵심 과제는 두 모달리티 간의 거대한 스케일 차이를 극복하는 것이다. 시각 센서는 전체 장면을 광범위하게 포착하지만, 촉각은 매우 국소적인 영역의 고밀도 정보를 다룬다. “VisGel” 프로젝트에서는 이 간극을 메우기 위해 조건부 생성적 적대 신경망(Conditional GAN) 아키텍처를 제안하였다. 생성자 <span class="math math-inline">G</span>는 현재의 시각 프레임 <span class="math math-inline">x</span>와 로봇의 접촉 위치 정보를 입력받아 해당 부위에서 발생할 촉각 이미지 <span class="math math-inline">y</span>를 예측한다.</p>
<p>이 과정에서 사용되는 손실 함수는 다음과 같이 정의된다.<br />
<span class="math math-display">
G^* = \text{arg min}_G \max_D \mathcal{L}_{GAN}(G, D) + \lambda\mathcal{L}_1(G)
</span><br />
여기서 <span class="math math-inline">\mathcal{L}_{GAN}</span>은 판별자를 속여 실제 촉각 데이터와 구별할 수 없는 사실적인 이미지를 생성하도록 유도하며, <span class="math math-inline">\mathcal{L}_1</span> 손실은 픽셀 수준에서의 정확도를 보장한다. 특히 “VisGel” 연구진은 데이터의 60% 이상이 공중에서의 움직임이나 특징 없는 평면 접촉인 점을 고려하여, 지면 진리(Ground Truth)의 변형 정도에 비례하여 손실 함수에 가중치를 부여하는 데이터 리밸런싱 전략을 사용하여 모델이 복잡한 질감을 더 잘 학습하도록 유도하였다. 다음 표는 시각 정보를 바탕으로 예측되는 주요 촉각 파라미터와 그에 대응하는 학습 지표를 정리한 것이다.</p>
<table><thead><tr><th><strong>예측 대상 (Tactile Property)</strong></th><th><strong>시각적 입력 (Visual Input)</strong></th><th><strong>수식적 표현/지표 (Mathematical Metric)</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td>접촉 시점 (Moment of Contact)</td><td>로봇 팔의 궤적 및 거리</td><td><span class="math math-inline">\vert t_l - t^{gt}_l \vert + \vert t_r - t^{gt}_r \vert</span></td><td>시계열 큐 활용</td></tr>
<tr><td>마커 변형 (Marker Deformation)</td><td>텍스처 및 압착 정도</td><td>Average <span class="math math-inline">L_2</span> distance of markers</td><td>흐름장 분석</td></tr>
<tr><td>표면 거칠기 (Surface Roughness)</td><td>반사광 및 그림자 패턴</td><td><span class="math math-inline">\text{Ra (Arithmetic Mean Roughness)} \vert \text{KF}</span></td><td><span class="math math-inline">Ra</span>는 산술 평균 거칠기</td></tr>
<tr><td>접촉 위치 및 스케일</td><td>픽셀 좌표 및 심도 정보</td><td><span class="math math-inline">r \cdot (d_{max} - d_{min}) + d_{min}</span></td><td>컷오프 임계값 설정</td></tr>
</tbody></table>
<h2>4. 재질 추정: 시각적 단서를 통한 물리적 통찰</h2>
<p>재질 추정(Material Estimation)은 로봇이 물체를 효과적으로 조작하기 위한 사전 지식을 구축하는 과정이다. 물체의 외형에서 추출된 정보는 단순히 시각적인 즐거움을 넘어 물체의 질량, 강성, 그리고 마찰 특성을 암시한다. 최근의 SOTA 기술은 시각-언어 모델(Vision-Language Models, VLMs)을 활용하여 이러한 물리적 추론 능력을 극대화하고 있다. “TLV-Link(Touch-Language-Vision Representation Learning through Curriculum Linking)“와 같은 모델은 촉각, 언어, 시각 세 가지 모달리티를 연결하여 로봇이 “부드럽다” 혹은 “거칠다“와 같은 언어적 개념을 실제 물리적 데이터와 매핑하도록 학습한다.</p>
<p>물리적 특성 추정에서 가장 중요한 요소 중 하나는 마찰 계수(Coefficient of Friction, COF)의 예측이다. “Slip Prediction Using Visual Information“에서는 지형이나 물체의 표면 이미지를 입력받아 CNN을 통해 재질 클래스를 분류하고, 각 클래스에 연계된 마찰 확률 분포 함수(Probability Distribution Functions)를 결합하여 마찰력을 추정한다. 이러한 방식은 특히 자율 주행 로봇이나 보행 로봇이 빙판, 젖은 타일, 혹은 모래밭과 같은 다양한 환경에서 미끄러짐을 예견하고 이동 경로를 수정하는 데 결정적인 역할을 한다.</p>
<p>또한, 강성(Stiffness) 추정은 로봇이 물체를 잡을 때 가해야 하는 적정 힘을 결정하는 데 필수적이다. “Active gathering of frictional properties from objects” 연구에 따르면, 로봇은 비전 데이터를 통해 물체의 초기 형태를 파악하고, 비정밀 조작(Non-prehensile Pushing)이나 당기기(Pulling) 동작을 통해 발생하는 미세한 변형을 관찰함으로써 물체의 질량 중심과 강성을 역으로 추론할 수 있다. 이러한 예측 과정은 베이지안 필터링(Bayesian Filtering)과 그래프 신경망(GNN)을 결합한 “Dual Differentiable Filtering” 프레임워크를 통해 시간 가변적인 물리 속성을 정밀하게 추적한다.</p>
<h2>5. 미끄러짐(Slip)의 동역학적 예측과 제어 메커니즘</h2>
<p>미끄러짐은 파지 안정성을 해치는 가장 위협적인 요소이며, 이를 사전에 예측하고 방지하는 것은 로봇 조작 기술의 정점이다. 미끄러짐 감지는 보통 두 단계로 나뉜다. 첫 번째는 물체가 실제로 크게 움직이기 전 발생하는 ’전조 미끄러짐(Incipient Slip)’의 포착이고, 두 번째는 시각 정보를 기반으로 현재의 파지 상태가 미래에 미끄러짐으로 이어질지 예견하는 것이다.</p>
<p>“WACSAN(Weight Allocation and Cross-Self Attention Network)” 모델은 시각과 촉각 데이터를 통합하여 미끄러짐을 감지하는 최신 SOTA 아키텍처 중 하나다. 이 모델은 특징 맵(Feature Map) 내의 위치별로 서로 다른 가중치를 할당하는데, 특히 물체와 집게(Gripper)가 접촉하는 경계 부위(Edges)에 높은 가중치를 부여한다. 미끄러짐이 시작될 때 가장 먼저 변화가 일어나는 곳이 바로 이 경계 부위이기 때문이다. 미끄러짐 감지는 전형적인 이진 분류 문제로 취급되며, 모델 학습에는 이진 교차 엔트로피(Binary Cross-Entropy) 손실 함수가 널리 사용된다.</p>
<p>미끄러짐의 물리적 모델링에서는 물체의 자세, 속도, 그리고 지면의 특성이 복합적으로 작용한다. 휠-레그 로봇(Wheeled-legged Robot)의 사례를 보면, 시각 정보를 기반으로 추정된 로봇의 포즈와 실제 이동 경로 사이의 편차를 분석하여 휠의 슬립량을 수치화한다. 만약 이러한 미끄러짐을 모델링에서 제외할 경우, 로봇의 운동 방정식은 5%에서 11.5% 사이의 심각한 오차를 발생시킨다. 이를 방지하기 위해 로봇은 예측된 미끄러짐 값을 실시간 궤적 최적화(Trajectory Optimization) 파이프라인에 입력하여, 슬립이 발생하기 전 가속도를 조절하거나 파지력을 강화하는 선제적 대응을 수행한다.</p>
<h2>6. SOTA 아키텍처: ViTacFormer와 트랜스포머 기반의 융합</h2>
<p>최근 로봇 지능의 트렌드는 CNN 중심의 아키텍처에서 트랜스포머(Transformer) 및 주의 집중 메커니즘(Attention Mechanism)으로 급격히 이동하고 있다. “ViTacFormer“는 고해상도 시각 정보와 촉각 신호를 공유된 잠재 공간에서 융합하는 최첨단 프레임워크다. 이 모델의 핵심은 ’자기회귀적 촉각 예측 헤드(Autoregressive Tactile Prediction Head)’로, 현재의 시각-촉각 상태를 기반으로 미래에 발생할 접촉 신호를 미리 예측한다.</p>
<p>ViTacFormer의 학습 과정은 ’커리큘럼 학습(Curriculum Learning)’을 채택하여 안정성을 확보한다. 초기 단계에서는 지면 진리(Ground-truth) 촉각 신호를 사용하여 학습하고, 점진적으로 모델이 스스로 예측한 촉각 신호(Predicted Tactile Signals)를 사용하여 정책을 갱신하도록 전이한다. 이러한 방식은 로봇이 시각적으로 가려진(Occluded) 상황에서도 과거의 촉각 경험과 현재의 시각적 맥락을 결합하여 정밀한 조작을 지속할 수 있게 한다.</p>
<p>또한 “SaTA(Spatially-anchored Tactile Awareness)” 모델은 로봇 손의 순운동학(Forward Kinematics) 정보를 활용하여 촉각 특징을 공간적으로 고정(Anchoring)시킨다. 이를 통해 로봇은 단순히 접촉 여부를 아는 것을 넘어, 물체의 기하학적 구조를 로봇 손의 좌표계 내에서 정확히 추론할 수 있다. 이러한 기술은 USB-C 포트 삽입이나 전구 교체와 같이 서브 밀리미터(Sub-millimeter) 단위의 정밀도가 요구되는 작업에서 기존 시각 전용 모델 대비 30% 이상의 성공률 향상을 입증하였다.</p>
<h2>7. 데이터 효율성과 가상 환경에서의 촉각 시뮬레이션</h2>
<p>고성능 시각-촉각 지능을 구축하는 데 있어 가장 큰 병목 현상은 실제 로봇 데이터를 수집하는 비용이다. “VisGel“과 같은 데이터셋은 수천 번의 접촉 시퀀스를 포함하지만, 여전히 일반적인 컴퓨터 비전 데이터셋에 비하면 규모가 작다. 이를 극복하기 위해 Nvidia Flex나 PyFlex와 같은 물리 엔진을 기반으로 한 촉각 시뮬레이션 환경이 적극적으로 활용되고 있다.</p>
<p>“ViTacGen“은 이러한 시뮬레이션의 이점을 극대화한 프레임워크로, 시각 영상으로부터 표준화된 촉각 표현인 ’접촉 심도 이미지(Contact Depth Images)’를 생성한다. 이 기술의 놀라운 점은 실제 촉각 센서가 없는 로봇 시스템에서도 시각 지능만으로 생성된 ’가상 촉각’을 활용하여 강화학습 정책을 수행할 수 있다는 것이다. 이는 로봇 지능의 제로샷(Zero-shot) 배포 가능성을 열어주며, 하드웨어 제약을 소프트웨어적 예측으로 극복하는 SOTA 기술의 정수를 보여준다.</p>
<p>또한, 센서 간의 미세한 공정 차이로 인한 성능 저하를 막기 위해 ‘센서 불변적 촉각 표현(Sensor-Invariant Tactile Representations, SITR)’ 기술이 도입되었다. 트랜스포머 기반 아키텍처를 사용하여 다양한 센서 디자인에서 수집된 시뮬레이션 데이터를 학습함으로써, 모델은 특정 하드웨어에 종속되지 않는 범용적인 촉각 특징을 추출하게 된다. 이는 미래의 로봇이 어떤 종류의 촉각 센서를 장착하더라도 사전에 학습된 시각-촉각 지능을 즉각적으로 활용할 수 있음을 의미한다.</p>
<h2>8. 지능형 제어와의 통합: VLA 모델로의 진화</h2>
<p>예측된 촉각 정보는 로봇의 최종 행동(Action)으로 연결될 때 진정한 가치를 발휘한다. 최신 연구들은 시각-촉각 정보를 언어와 결합한 ‘시각-촉각-언어-행동(Vision-Tactile-Language-Action, VTLA)’ 모델로 진화하고 있다. VTLA 모델은 “조금 더 세게 쥐어라” 혹은 “미끄러짐이 감지되면 즉시 멈춰라“와 같은 고수준의 언어 지시사항을 시각적 관찰 및 촉각적 예측 신호와 동기화한다.</p>
<p>특히 “DPO(Direct Preference Optimization)“를 로봇 작업에 적용하여, 분류 기반의 다음 토큰 예측 손실과 연속적인 제어 작업 사이의 간극을 메우는 시도가 성공을 거두었다. 이러한 통합 모델은 처음 보는 물체에 대해서도 90% 이상의 파지 및 조작 성공률을 보이며, 심지어 시뮬레이션에서 학습된 정책이 실제 환경에서도 별도의 튜닝 없이 작동하는 뛰어난 Sim-to-Real 성능을 입증하였다.</p>
<table><thead><tr><th><strong>기술 명칭</strong></th><th><strong>핵심 메커니즘</strong></th><th><strong>주요 성과/특징</strong></th></tr></thead><tbody>
<tr><td>ViTacFormer</td><td>Cross-attention + Autoregressive head</td><td>미래 촉각 상태 예견, 정밀 조작 50% 향상</td></tr>
<tr><td>WACSAN</td><td>Weight Allocation + Cross-Self Attention</td><td>경계 영역 가중치 부여를 통한 미끄러짐 감지 최적화</td></tr>
<tr><td>ViTacGen</td><td>Vision-to-Touch Generation</td><td>시각 데이터만으로 촉각 심도 이미지 생성 및 RL 적용</td></tr>
<tr><td>VTLA</td><td>Multi-modal Language Grounding</td><td>언어 지시와 시각-촉각 피드백의 통합 제어</td></tr>
<tr><td>FXT-ZNN</td><td>Fixed-time Adaptive Estimation</td><td>마찰 모델 없이 joint friction 정밀 추정</td></tr>
</tbody></table>
<h2>9. 결론: 촉각을 예견하는 시각의 미래</h2>
<p>로봇의 시각 지능이 촉각 정보를 예상한다는 것은, 로봇이 물리 법칙에 대한 ’직관’을 갖추게 됨을 의미한다. 재질을 추정하고 미끄러짐을 예견하는 능력은 로봇이 단순히 정해진 궤적을 따라가는 기계에서 벗어나, 환경의 미묘한 변화에 적응하고 지능적으로 대처하는 Embodied AI로 진화하기 위한 필수 전제 조건이다.</p>
<p>앞으로의 연구 방향은 더욱 복잡한 비정형 환경에서의 강건성 확보와, 인간의 언어적 피드백을 실시간 촉각 제어 루프에 통합하는 방향으로 흐를 것이다. 특히 기초 모델(Foundation Models)의 발전과 더불어, 로봇은 수억 번의 시각-촉각 상호작용 데이터를 통해 물리학적 상식을 스스로 터득하고, 이를 바탕으로 인간과 유사하거나 혹은 그 이상의 정교한 조작 지능을 실현하게 될 것이다. 시각 지능과 촉각 지능의 경계가 허물어지는 이 지점에서 로봇은 비로소 세상을 진정으로 ‘느끼며’ ‘이해하는’ 존재가 된다.</p>
<h2>10. 참고 자료</h2>
<ol>
<li>Connecting Touch and Vision via Cross-Modal … - VisGel - MIT, https://visgel.csail.mit.edu/visgel-paper.pdf</li>
<li>Aligning Visual-Tactile Fusion with Contrastive Representations - arXiv, https://arxiv.org/html/2506.20757v1</li>
<li>(PDF) Connecting Touch and Vision via Cross-Modal Prediction, https://www.researchgate.net/publication/333815643_Connecting_Touch_and_Vision_via_Cross-Modal_Prediction</li>
<li>Self-Supervised Visuo-Tactile Pretraining to Locate and Follow …, https://www.researchgate.net/publication/372791436_Self-Supervised_Visuo-Tactile_Pretraining_to_Locate_and_Follow_Garment_Features</li>
<li>Design and development of a robust vision-based tactile sensor, <a href="https://orca.cardiff.ac.uk/149803/3/Design%20and%20development%20of%20a%20robust%20vision-based%20tactile%20sensor.pdf">https://orca.cardiff.ac.uk/149803/3/Design%20and%20development%20of%20a%20robust%20vision-based%20tactile%20sensor.pdf</a></li>
<li>Robotic Perception with a Large Tactile-Vision-Language Model for …, https://arxiv.org/html/2506.19303v1</li>
<li>Connecting Touch and Vision via Cross-Modal Prediction, https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Connecting_Touch_and_Vision_via_Cross-Modal_Prediction_CVPR_2019_paper.pdf</li>
<li>Connecting Touch and Vision via Cross-Modal Prediction - ar5iv, https://ar5iv.labs.arxiv.org/html/1906.06322</li>
<li>Generating Visual Scenes from Touch - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Generating_Visual_Scenes_from_Touch_ICCV_2023_paper.pdf</li>
<li>Multi-Robot Assembly of Deformable Linear Objects Using … - arXiv, https://arxiv.org/html/2506.22034v1</li>
<li>Tac3D: A Novel Vision-based Tactile Sensor for Measuring Forces …, https://arxiv.org/pdf/2202.06211</li>
<li>Sim-to-Real Deep Tactile Policies for Dexterous Robotic Manipulation., https://research-information.bris.ac.uk/ws/portalfiles/portal/460810853/LIN_PhD_Thesis_UoB_final_submission.pdf</li>
<li>Slip Detection for Grasp Stabilization With a Multifingered Tactile …, https://www.researchgate.net/publication/346824107_Slip_Detection_for_Grasp_Stabilization_With_a_Multifingered_Tactile_Robot_Hand</li>
<li>Predicting human tactile smoothness/roughness perception … - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC12660992/</li>
<li>Visuo-Tactile based Predictive Cross Modal Perception for Object …, https://arxiv.org/abs/2405.12634</li>
<li>Touch100k: A Large-Scale Touch-Language-Vision Dataset … - arXiv, https://arxiv.org/html/2406.03813v1</li>
<li>Octopi-X: Robotic Perception with a Large Tactile-Vision-Language …, https://openreview.net/notes/edits/attachment?id=M0yZxmzySE&amp;name=pdf</li>
<li>Estimating indoor tile friction coefficient using visual information, https://academic.oup.com/jcde/article/12/1/331/7954144</li>
<li>Slip Prediction Using Visual Information - ResearchGate, https://www.researchgate.net/publication/221344445_Slip_Prediction_Using_Visual_Information</li>
<li>Vision-based dynamic modeling of wheeled-legged robot …, https://www.cambridge.org/core/journals/robotica/article/visionbased-dynamic-modeling-of-wheeledlegged-robot-considering-slippage-using-gibbsappell-formulation/8D2B32C348A8DB8F5D876C9B0728E672</li>
<li>Predictive Visuo-Tactile Interactive Perception Framework for Object …, https://ieeexplore.ieee.org/iel8/8860/10778592/10847911.pdf</li>
<li>Predictive Visuo-Tactile Interactive Perception Framework for Object …, https://arxiv.org/html/2411.09020v1</li>
<li>Visual–Tactile Slip Detection via Weight Allocation and Cross-Self …, https://www.researchgate.net/publication/386125055_Visual-tactile_Slip_Detection_via_Weight_Allocation_and_Cross-Self_Attention_Net</li>
<li>Mechatronics - University of Washington, https://faculty.washington.edu/chx/sample_publication/hu-grasping-journal-2024/paper.pdf</li>
<li>Bioinspired trajectory modulation for effective slip control in robot …, https://pmc.ncbi.nlm.nih.gov/articles/PMC12283357/</li>
<li>Estimating Friction Using Incipient Slip Sensing During a …, http://bdml.stanford.edu/oldweb/touch/publications/tremblay_icra93.pdf</li>
<li>Universal slip detection of robotic hand with tactile sensing - Frontiers, https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2025.1478758/full</li>
<li>ROBOTIC MANIPULATION BASED ON VISUAL AND … - RUA, https://rua.ua.es/dspace/bitstream/10045/118217/1/tesis_brayan_impata.pdf</li>
<li>IICAIET 2024 ABSTRCT BOOK, https://iicaiet.ieeesabah.org/IICAIET2024ABSTRACTBOOK.pdf</li>
<li>Proactive slip control by learned slip model and trajectory adaptation, https://proceedings.mlr.press/v205/nazari23a/nazari23a.pdf</li>
<li>VITACFORMER: LEARNING CROSS-MODAL REPRE - OpenReview, https://openreview.net/pdf/d40ed28859e9726177a3006275da2c792b051818.pdf</li>
<li>Learning Cross-Modal Representation for Visuo-Tactile Dexterous …, https://arxiv.org/abs/2506.15953</li>
<li>Learning Cross-Modal Representation for Visuo-Tactile Dexterous …, <a href="https://openreview.net/forum?id=YiIqzkYRhj&amp;noteId=wI1E0l5hOm">https://openreview.net/forum?id=YiIqzkYRhj¬eId=wI1E0l5hOm</a></li>
<li>机器人相关2025_6_23 - arXiv每日学术速递, http://arxivdaily.com/thread/68666</li>
<li>Spatially-anchored Tactile Awareness for Robust Dexterous … - arXiv, https://arxiv.org/html/2510.14647v1</li>
<li>TextToucher: Fine-Grained Text-to-Touch Generation, https://ojs.aaai.org/index.php/AAAI/article/view/32802/34957</li>
<li>Multi-Modal Representation Learning with Tactile Data, https://engineering.purdue.edu/cdesign/wp/wp-content/uploads/2025/02/Multi-Modal_Representation_Learning_with_Tactile_Data.pdf</li>
<li>Learning from Human-Collected Vision and Touch - NeurIPS, https://proceedings.neurips.cc/paper_files/paper/2022/file/354892587fe39b17c2b727af02abff4a-Paper-Datasets_and_Benchmarks.pdf</li>
<li>Visual-Tactile Multimodality for Following Deformable Linear Objects …, https://arxiv.org/pdf/2204.00117</li>
<li>ViTacGen: Robotic Pushing with Vision-to-Touch Generation - arXiv, https://arxiv.org/html/2510.14117v1</li>
<li>ViTacGen: Robotic Pushing with Vision-to-Touch Generation, https://www.researchgate.net/publication/396540995_ViTacGen_Robotic_Pushing_with_Vision-to-Touch_Generation</li>
<li>Sensor-Invariant Tactile Representation - arXiv, https://arxiv.org/html/2502.19638v2</li>
<li>VTLA: Vision-Tactile-Language-Action Model with Preference … - arXiv, https://arxiv.org/html/2505.09577v1</li>
<li>OmniVTLA: Vision-Tactile-Language-Action Model with Semantic- …, https://arxiv.org/html/2508.08706v1</li>
<li>Joint Friction Dynamic Estimation for Robotic Finger Using Novel …, https://ieeexplore.ieee.org/iel8/6287639/10820123/11129059.pdf</li>
<li>How Well do Latent Safety Filters Understand Partially Observable …, https://www.researchgate.net/publication/396331239_What_You_Don’t_Know_Can_Hurt_You_How_Well_do_Latent_Safety_Filters_Understand_Partially_Observable_Safety_Constraints</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>