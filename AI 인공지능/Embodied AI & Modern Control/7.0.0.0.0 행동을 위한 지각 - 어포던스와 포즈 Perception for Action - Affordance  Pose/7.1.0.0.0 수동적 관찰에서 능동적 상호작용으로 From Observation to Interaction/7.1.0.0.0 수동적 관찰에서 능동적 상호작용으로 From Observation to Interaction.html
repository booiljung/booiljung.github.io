<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.1 수동적 관찰에서 능동적 상호작용으로 (From Observation to Interaction)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.1 수동적 관찰에서 능동적 상호작용으로 (From Observation to Interaction)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <a href="index.html">7.1 수동적 관찰에서 능동적 상호작용으로 (From Observation to Interaction)</a> / <span>7.1 수동적 관찰에서 능동적 상호작용으로 (From Observation to Interaction)</span></nav>
                </div>
            </header>
            <article>
                <h1>7.1 수동적 관찰에서 능동적 상호작용으로 (From Observation to Interaction)</h1>
<h2>1.  정적 컴퓨터 비전의 한계와 ‘관찰자’ 패러다임의 종말</h2>
<p>지난 수십 년간 인공지능(AI), 특히 컴퓨터 비전(Computer Vision) 분야는 놀라운 발전을 거듭해 왔다. ImageNet이나 COCO와 같은 대규모 정적 데이터셋을 기반으로 훈련된 심층 신경망(Deep Neural Networks)은 객체 분류 및 감지 작업에서 인간을 능가하는 성능을 보여주기도 했다. 그러나 이러한 성공은 로봇 공학이라는 물리적 실체(Embodiment)가 존재하는 환경으로 넘어오는 순간, 근본적인 한계에 직면하게 된다. 전통적인 컴퓨터 비전은 세상을 ’관찰(Observation)’의 대상으로만 간주하며, 시스템은 카메라를 통해 들어오는 시각 정보를 수동적으로 받아들여 처리하는 ’관찰자(Spectator)’의 역할에 머물러 있었다.</p>
<p>이러한 수동적 관찰(Passive Observation) 패러다임은 구조화되지 않은 비정형 환경(Unstructured Environment)에서 로봇이 작업을 수행할 때 치명적인 취약점을 노출한다. 로봇은 고정된 데이터셋의 분포를 벗어나는 상황, 즉 ‘롱테일(Long Tail)’ 사건들에 끊임없이 직면하게 되며 , 시각적 정보만으로는 해결할 수 없는 물리적 모호성(Ambiguity)에 부딪힌다. 본 절에서는 정적 컴퓨터 비전이 로봇 애플리케이션에서 실패하는 구조적 원인을 분석하고, 이를 극복하기 위해 능동적 상호작용(Active Interaction)으로의 패러다임 전환이 왜 필연적인지 논한다.</p>
<h3>1.1 데이터셋 편향과 시점의 불일치 (Viewpoint Bias and Distribution Shift)</h3>
<p>딥러닝 기반의 시각 인식 모델은 훈련 데이터의 분포에 극도로 의존적이다. 그러나 인터넷에서 수집된 대다수의 이미지 데이터셋은 인간 사진가가 촬영한 ‘표준적(Canonical)’ 시점을 따르는 경향이 있다. 예를 들어, 의자는 정면이나 45도 측면에서, 컵은 손잡이가 보이거나 위에서 비스듬히 내려다보는 각도에서 촬영된 이미지가 지배적이다. 반면, 로봇은 바닥을 기어 다니거나(청소 로봇), 천장에 매달리거나(산업용 로봇), 혹은 넘어진 물체를 아주 가까이에서 바라보는 등 매우 비표준적인 시점(Non-canonical viewpoints)에서 세상을 마주하게 된다.</p>
<p>이러한 **시점의 불일치(Viewpoint Mismatch)**는 모델의 성능을 급격히 저하시킨다. 정적 모델은 물체를 인식하기 위해 학습된 특정 특징(feature)들이 보이지 않거나 왜곡될 경우, 이를 보정할 능력이 없다. 예를 들어, 머그컵의 손잡이가 뒤쪽으로 돌아가 원통형 몸체만 보이는 경우, 수동적 관찰자는 이를 단순한 원기둥이나 캔으로 오인할 가능성이 높다. 능동적 상호작용이 배제된 상태에서는, 로봇이 스스로 움직여 손잡이를 확인하거나 물체를 회전시켜 보는 ’인식론적 행동(Epistemic Action)’을 취할 수 없으므로, 이러한 불확실성은 해결되지 않은 채 남게 된다.</p>
<p>또한, 현실 세계의 조명 조건, 날씨, 반사 등의 환경적 요인은 정적 데이터셋이 포괄할 수 없는 무한한 변이(Variance)를 가진다. 자율 주행 차량이나 야외 로봇은 안개, 비, 혹은 강한 역광과 같은 ‘적대적(Adversarial)’ 조건에 빈번히 노출되며, 이러한 상황에서 훈련 데이터의 편향(Bias)은 치명적인 오작동으로 이어진다. 수동적 시스템은 입력된 데이터의 품질이 나쁘면 결과물의 품질도 나빠질 수밖에 없는 개방 루프(Open-loop) 구조의 한계를 벗어나지 못한다.</p>
<h3>1.2 기하학적 불확실성과 완전성의 환상 (Geometric Uncertainty and the Illusion of Completeness)</h3>
<p>수동적 시각 시스템의 또 다른 결정적인 한계는 **폐색(Occlusion)**과 **깊이 모호성(Depth Ambiguity)**을 해결할 수 없다는 점이다. 단일 시점, 혹은 고정된 스테레오 시점에서 3차원 세상을 2차원 평면으로 투영할 때 정보의 손실은 불가피하다. 물체의 뒷면, 다른 물체에 가려진 부분, 혹은 깊은 내부 구조는 수동적 센서로는 원리적으로 관측이 불가능하다.</p>
<p>그러나 많은 정적 비전 알고리즘들은 이러한 불완전한 정보를 바탕으로 ‘완전한’ 객체를 추론하려고 시도한다. 이를 **완전성의 환상(Illusion of Completeness)**이라 부를 수 있다. 예를 들어, 로봇이 테이블 위의 상자를 인식했을 때, 수동적 시스템은 이를 온전한 육면체로 가정하고 파지(Grasping) 계획을 세운다. 하지만 그 상자의 뒷면이 뚫려 있거나, 내용물이 무거워 무게 중심이 쏠려 있다면, 시각적 정보에만 의존한 파지는 실패할 수밖에 없다.</p>
<p>특히 투명하거나(Transparent) 반사성이 강한(Specular) 물체, 혹은 텍스처가 없는(Textureless) 물체는 수동적 RGB-D 센서(LiDAR, ToF 등)의 작동 원리를 위배한다. 투명한 유리는 빛을 투과시키고, 거울은 빛을 정반사시켜 센서로 돌아오지 않게 하거나 엉뚱한 곳에서 온 빛을 센서로 보내 ’유령 기하(Ghost Geometry)’를 생성한다. 정적 시스템에서 유리컵은 종종 깊이 정보가 없는 ’구멍(Hole)’으로 나타나거나 배경과 합쳐져 인식되지 않는다. 로봇이 이러한 물체와 상호작용하려면, 단순히 보는 것을 넘어 능동적으로 센서를 움직여 반사광의 이동(Specular Flow)을 추적하거나, 직접 접촉하여 존재를 확인하는 절차가 필수적이다.</p>
<h3>1.3 체화된 인지(Embodied Cognition)의 부재</h3>
<p>인지과학의 관점에서 볼 때, 수동적 관찰은 인간이나 동물의 지각 방식과도 근본적으로 다르다. <strong>체화된 인지(Embodied Cognition)</strong> 이론에 따르면, 지각은 뇌가 감각 입력을 수동적으로 처리하여 세상에 대한 내부 표상(Internal Representation)을 만드는 과정이 아니라, 신체가 환경과 상호작용하며 능동적으로 정보를 추출하는 과정이다.</p>
<p>J.J. Gibson의 생태학적 심리학(Ecological Psychology)은 우리가 세상을 기하학적 형태가 아닌 **행동 유도성(Affordance)**의 관점에서 지각한다고 주장한다. 우리는 컵을 ’지름 8cm의 원통’으로 보는 것이 아니라 ’손으로 잡을 수 있는 것(Graspable)’으로, 의자를 ’다리가 네 개 달린 물체’가 아니라 ’앉을 수 있는 것(Sittable)’으로 인식한다. 이러한 어포던스는 관찰자의 신체적 능력과 의도에 따라 정의된다.</p>
<p>수동적 관찰자는 이러한 ’의도(Intention)’와 ’상호작용(Interaction)’이 결여되어 있다. 연구에 따르면, 학습자가 단순히 타인의 행동을 관찰할 때와 직접 행동에 참여할 때의 시선 처리와 인지 과정은 현격히 다르다. 수동적 관찰 시에는 결과(예: 그려진 선)에 시선이 머무르지만, 능동적 수행 시에는 예측적 시선(Predictive Gaze)이 행동을 앞서간다. 이는 로봇 시스템에서도 마찬가지로, 능동적 상호작용 없이는 환경에 대한 깊이 있는 이해나 인과관계의 파악이 불가능함을 시사한다.</p>
<p>결국, 로봇이 정적이고 수동적인 ’관찰자’에서 벗어나 환경을 탐색하고 조작하는 ’행위자(Actor)’로 거듭나야만 비로소 복잡한 현실 세계의 불확실성을 통제할 수 있게 된다. 이것이 바로 **상호작용적 지각(Interactive Perception, IP)**이 필요한 이유이다.</p>
<h2>2.  상호작용적 지각(Interactive Perception)의 이론적 토대</h2>
<p>상호작용적 지각(IP)은 로봇이 환경과의 물리적 상호작용을 통해 감각 신호의 규칙성을 발견하고, 이를 통해 지각 능력을 향상시키는 일련의 과정을 의미한다. 이는 로봇 공학, 제어 이론, 정보 이론이 교차하는 지점에서 탄생한 개념으로, 지각(Perception)과 행동(Action)을 분리된 모듈이 아닌 하나의 통합된 루프(Loop)로 재정의한다.</p>
<h3>2.1 지각-행동 루프(Perception-Action Loop)의 재구성</h3>
<p>전통적인 로봇 제어 아키텍처는 **감지-생각-행동(Sense-Think-Act)**의 선형적 패러다임을 따랐다. 여기서 지각은 행동 이전에 완료되어야 하는 전제 조건이었다. 그러나 IP 패러다임에서는 지각과 행동이 순환적으로 서로를 강화한다.</p>
<ul>
<li><strong>수동적 지각 (Open Loop):</strong> $ \text{World} \rightarrow \text{Sensors} \rightarrow \text{Perception} \rightarrow \text{Belief} $</li>
<li><strong>상호작용적 지각 (Closed Loop):</strong> $ \text{World} \rightarrow \text{Sensors} \rightarrow \text{Perception} \rightarrow \text{Controller} \rightarrow \text{Actuators} \rightarrow \text{World Change} \rightarrow \text{Sensors…} $</li>
</ul>
<p>이 루프의 핵심은 로봇이 수행하는 행동이 단순한 작업 수행(Pragmatic Action)을 넘어, 정보를 획득하기 위한 **인식론적 행동(Epistemic Action)**을 포함한다는 것이다. 예를 들어, 문을 여는 것이 작업 수행 행동이라면, 문이 잠겨 있는지 확인하기 위해 살짝 밀어보는 것은 인식론적 행동이다. IP는 로봇이 불확실성을 줄이기 위해 능동적으로 실험을 설계하고 수행하는 ’과학자’와 같은 역할을 부여한다.</p>
<h3>2.2 정보 이론적 정식화: 불확실성과 상호 정보량</h3>
<p>IP의 목표는 수학적으로 <strong>불확실성(Uncertainty)의 최소화</strong>로 표현될 수 있다. 로봇이 추정하고자 하는 세계의 상태(예: 물체의 위치, 무게, 마찰계수)를 확률 변수 <span class="math math-inline">X</span>라고 하고, 센서를 통해 얻는 관측값을 <span class="math math-inline">Z</span>라고 하자. 이때 상태 <span class="math math-inline">X</span>에 대한 불확실성은 <strong>엔트로피(Entropy)</strong> <span class="math math-inline">H(X)</span>로 정의된다.</p>
<p>로봇이 특정 행동 <span class="math math-inline">a</span>를 취했을 때 얻게 될 관측값 <span class="math math-inline">Z</span>가 상태 <span class="math math-inline">X</span>에 대한 불확실성을 얼마나 줄여주는지는 <strong>상호 정보량(Mutual Information, MI)</strong>, 또는 <strong>정보 이득(Information Gain)</strong> <span class="math math-inline">I(X; Z|a)</span>로 계산된다.<br />
<span class="math math-display">
I(X; Z|a) = H(X) - H(X|Z, a)
</span><br />
여기서 <span class="math math-inline">H(X|Z, a)</span>는 행동 <span class="math math-inline">a</span>를 통해 <span class="math math-inline">Z</span>를 관측한 후 남은 조건부 엔트로피이다. IP 시스템의 제어기는 이 상호 정보량을 최대화하는 최적의 행동 <span class="math math-inline">a^*</span>를 찾아내야 한다.<br />
<span class="math math-display">
a^* = \operatorname*{argmax}_{a \in \mathcal{A}} \mathbb{E}_{z \sim P(z|a)} [ I(X; Z) ]
</span><br />
즉, 로봇은 현재 자신의 믿음(Belief State)을 가장 크게 갱신할 수 있는, 다시 말해 가장 ‘놀라운(Surprising)’ 정보를 줄 것으로 예상되는 행동을 선택해야 한다. 이는 베이지안 추론(Bayesian Inference)의 틀 안에서, 사전 확률(Prior)과 우도(Likelihood)를 통해 사후 확률(Posterior)을 갱신해 나가는 과정과 일치한다. 로봇은 행동을 통해 감각 채널의 신호 대 잡음비(SNR)를 높이고, 결과적으로 지각의 정확도를 향상시킨다.</p>
<h3>2.3 예측적 처리와 능동적 추론 (Predictive Processing and Active Inference)</h3>
<p>IP는 뇌과학의 <strong>예측적 처리(Predictive Processing)</strong> 이론과도 밀접하게 연관된다. 이 이론에 따르면, 뇌는 끊임없이 감각 입력을 예측하고, 실제 입력과 예측 사이의 오차(Prediction Error)를 최소화하는 방향으로 작동한다. 로봇 공학에서 이는 **능동적 추론(Active Inference)**으로 구현된다. 로봇은 자신의 내부 모델(World Model)을 기반으로 행동의 결과를 예측하고, 예측 오차를 줄이기 위해 내부 모델을 수정하거나(지각), 예측과 일치하도록 환경을 변화시키는 행동을 취한다(행동).</p>
<p>이러한 이론적 배경은 로봇이 왜 단순히 ’보는 것’을 넘어 ‘움직여야’ 하는지를 설명해준다. 움직임은 예측 오차를 능동적으로 생성하고 검증하는 수단이며, 이를 통해 로봇은 정적 데이터로는 얻을 수 없는 인과적 구조(Causal Structure)를 학습하게 된다.</p>
<h2>3.  능동 시각(Active Vision): 보기 위해 움직이다</h2>
<p>상호작용적 지각의 가장 일차적인 형태는 센서의 위치와 파라미터를 제어하여 더 나은 시각 정보를 획득하는 **능동 시각(Active Vision)**이다. 이는 카메라를 단순히 수동적인 기록 장치가 아니라, 탐색을 위한 도구로 활용하는 것이다.</p>
<h3>3.1 차선 시점 계획 (Next-Best-View, NBV)</h3>
<p>능동 시각의 핵심 문제는 “어디를 보아야 불확실성이 가장 많이 줄어들까?“라는 질문, 즉 <strong>NBV(Next-Best-View) 계획</strong> 문제로 귀결된다. 이는 3차원 재구성(Reconstruction), 객체 인식, 그리고 파지(Grasping) 계획에서 필수적인 요소이다.</p>
<p>로봇이 복잡한 클러터(Clutter) 환경에서 물체를 파지하려 할 때, 단일 시점에서는 다른 물체에 가려져 파지 가능 지점(Grasp affordance)을 찾기 어려운 경우가 많다. NBV 알고리즘은 현재의 3차원 지도(예: Voxel Grid, OctoMap)에서 ’미지 영역(Unknown space)’과 ’표면(Surface)’의 경계를 분석하고, 가려진 영역을 가장 많이 드러낼 수 있는 카메라의 위치를 계산한다.</p>
<p>최근의 연구들은 기하학적 휴리스틱을 넘어, 딥러닝 기반의 NBV 계획을 도입하고 있다. 예를 들어, 심층 신경망을 이용해 현재의 깊이 이미지(Depth Image)로부터 6-DoF(자유도) 파지 성공 확률을 예측하고, 동시에 불확실성이 높은 영역을 식별하여 다음 시점을 제안하는 방식이다. 이러한 방식은 시뮬레이션 환경에서 대규모로 학습된 후 실세계로 전이(Sim-to-Real)되어, 정적 카메라 시스템 대비 월등히 높은 파지 성공률을 달성한다. 특히 손잡이와 같은 주요 특징이 가려진 경우, 로봇이 스스로 카메라를 움직여 ’표준적 시점’을 찾아내는 능력은 인식 성능을 극적으로 향상시킨다.</p>
<h3>3.2 신경 방사장(NeRF)과 신경 파지성 필드(Neural Graspness Field)</h3>
<p>최근 컴퓨터 비전의 혁명이라 불리는 <strong>신경 방사장(NeRF, Neural Radiance Fields)</strong> 기술은 능동 시각과 결합하여 강력한 시너지를 내고 있다. NeRF는 장면을 연속적인 함수로 표현하여, 임의의 시점에서의 이미지를 합성할 수 있게 한다. 하지만 기본적으로 NeRF는 정적인 장면 복원에 초점이 맞춰져 있다.</p>
<p>로봇 공학자들은 이를 확장하여 **신경 파지성 필드(Active Neural Graspness Field, ActiveNGF)**와 같은 개념을 제안했다. 이 시스템은 단순히 색상(RGB)과 밀도(Density)만을 학습하는 것이 아니라, 3차원 공간의 모든 점에 대해 ’파지 적합성(Graspness)’을 예측하는 별도의 분기(Branch)를 신경망에 추가한다.</p>
<ul>
<li><strong>작동 원리:</strong> 로봇이 새로운 시점으로 이동하며 이미지를 수집할 때마다, NeRF 모델은 실시간으로 업데이트된다. 이때 시스템은 현재 모델의 ‘파지성’ 예측이 불확실하거나 뷰 간의 일관성이 떨어지는(Inconsistency) 영역을 계산한다.</li>
<li><strong>능동 루프:</strong> 로봇은 이 불확실성을 최소화하는 방향(Information Gain이 최대화되는 방향)으로 카메라를 이동시킨다. 즉, 단순히 ‘예쁜’ 3D 모델을 만드는 것이 아니라, ’잡을 수 있는 부위’를 확실하게 파악하기 위해 능동적으로 탐색한다.</li>
</ul>
<p>실험 결과, 이러한 능동적 접근 방식은 단일 시점 기반 방식이나 무작위 탐색 방식에 비해 복잡한 클러터 환경에서의 파지 성공률을 12% 이상 향상시키는 것으로 보고되었다.</p>
<h3>3.3 투명 및 반사 물체에 대한 능동적 대응</h3>
<p>앞서 언급한 투명하거나 반사되는 물체의 인식 실패 문제 역시 능동 시각을 통해 완화될 수 있다. 정적 이미지에서는 투명한 유리컵이 배경과 구분되지 않지만, 카메라가 움직이면 **광학적 흐름(Optical Flow)**과 **반사광(Specular Highlight)**의 움직임에서 차이가 발생한다. 유리의 굴절로 인해 배경이 왜곡되어 움직이거나, 표면의 반사광이 카메라의 움직임에 따라 표면을 타고 흐르는 현상이 관측된다.</p>
<p><strong>Evo-NeRF</strong>와 같은 최신 연구들은 이러한 동적 정보를 활용하여 투명 물체의 형상을 복원한다. 로봇이 카메라를 움직이며 연속적인 이미지를 획득하면, 시스템은 다중 시점 일관성(Multi-view Consistency)을 활용하여 굴절과 반사를 모델링하고, 깊이 센서가 놓친 형상을 채워 넣는다. 이는 정적 깊이 센서가 0%에 가까운 성공률을 보이는 투명 물체 파지 작업을 90% 이상의 성공률로 끌어올리는 핵심 기술이다.</p>
<h2>4.  촉각적 상호작용: 만져서 알아내다 (Interaction for Inference)</h2>
<p>시각 정보는 아무리 능동적으로 수집해도 표면적인 속성(색상, 텍스처, 형상)에 국한된다. 그러나 로봇이 물체를 조작하기 위해 필수적인 물리적 속성—<strong>질량(Mass), 마찰계수(Friction), 강성(Stiffness), 무게중심(Center of Mass, CoM)</strong>—은 시각적으로는 ‘잠재된(Latent)’ 속성들이다. 이를 알아내기 위해서는 물리적 접촉, 즉 <strong>촉각적 상호작용</strong>이 필수적이다.</p>
<h3>4.1 물리적 속성 추정의 역학</h3>
<p>질량이나 마찰은 정적인 상태에서는 관측 불가능하다. 뉴턴의 제2법칙(<span class="math math-inline">F=ma</span>)에 따라, 질량을 알기 위해서는 힘(<span class="math math-inline">F</span>)을 가하고 그에 따른 가속도(<span class="math math-inline">a</span>)를 관측해야 한다. 마찰계수를 알기 위해서는 물체를 밀어서 미끄러짐이 발생하는 임계점을 찾아야 한다. 이처럼 물리적 속성 추정은 본질적으로 **개입(Intervention)**을 요구한다.</p>
<ul>
<li><strong>탐색적 밀기(Exploratory Push):</strong> 로봇은 물체의 물리적 속성을 파악하기 위해 ’찌르기(Poking)’나 ’밀기(Pushing)’와 같은 저수준의 상호작용을 수행한다. 물체의 반응(이동 거리, 회전 각도)은 물리적 파라미터의 함수이다. 예를 들어, 무게중심을 벗어난 지점을 밀면 물체는 회전할 것이고, 무게중심을 밀면 병진 운동을 할 것이다.</li>
<li><strong>정보 이득 기반 행동 선택:</strong> 무작위로 미는 것은 효율적이지 않다. 로봇은 현재의 불확실성을 가장 크게 줄일 수 있는 접촉 지점과 힘의 방향을 계산해야 한다. 이를 위해 <strong>N-스텝 정보 이득(N-step Information Gain)</strong> 접근법이 사용된다. 로봇은 시뮬레이션 상에서 여러 후보 행동을 미리 수행해보고(Lookahead), 파라미터 추정의 분산(Variance)을 가장 크게 줄일 것으로 예측되는 행동을 선택한다.</li>
</ul>
<h3>4.2 시각-촉각 융합(Visuo-Tactile Fusion)과 이중 미분 필터</h3>
<p>시각은 전역적이고 거시적인 정보를 제공하는 반면, 촉각은 국소적이고 정밀한 접촉 정보를 제공한다. 최신 IP 프레임워크는 이 두 모달리티를 <strong>이중 미분 필터(Dual Differentiable Filter, DDF)</strong> 아키텍처를 통해 융합한다.</p>
<p>이 구조는 상태 추정(State Estimation)과 파라미터 추정(Parameter Estimation)을 두 개의 상호 의존적인 필터로 분리하여 처리한다.</p>
<ol>
<li><strong>상태 필터:</strong> 물체의 위치, 속도 등 동적 상태를 추정한다.</li>
<li><strong>파라미터 필터:</strong> 질량, 마찰계수 등 정적 속성을 추정한다.</li>
</ol>
<p>중요한 점은 이 필터가 **미분 가능(Differentiable)**하다는 것이다. 즉, 관측된 오차(예: 예측보다 물체가 덜 미끄러짐)를 역전파(Backpropagation)하여, 물체의 질량이나 마찰계수와 같은 잠재 변수를 경사 하강법(Gradient Descent)으로 업데이트할 수 있다. 여기에 **그래프 신경망(GNN)**을 결합하여 물체와 로봇 간의 상호작용 동역학을 학습하면, 복잡한 비선형 마찰이나 불규칙한 형상의 물체에 대해서도 정밀한 물성 추정이 가능하다.</p>
<p>실험 결과에 따르면, 이러한 능동적 시각-촉각 융합 프레임워크는 단순히 시각 정보에만 의존하거나 수동적인 데이터 수집을 하는 경우보다 물성 추정의 정확도를 획기적으로 높이며, 이는 결과적으로 파지 안정성과 조작 성공률의 상승으로 이어진다.</p>
<h2>5.  조작을 통한 지각 (Action for Perception)</h2>
<p>능동적 상호작용의 가장 고차원적인 형태는 인식을 돕기 위해 환경 자체를 변화시키는 **조작(Manipulation)**이다. 이는 “인식을 위해 행동한다(Action for Perception)“는 개념으로 요약된다.</p>
<h3>5.1 싱귤레이션(Singulation)과 클러터 제거</h3>
<p>’빈 피킹(Bin Picking)’과 같이 물체들이 무질서하게 쌓여 있는 환경은 시각 인식의 악몽과도 같다. 물체들이 서로 겹쳐 있어 경계가 불분명하기 때문에(Segmentation failure), 최신 딥러닝 모델조차도 개별 물체를 인식하는 데 실패하곤 한다.</p>
<p>이때 IP 시스템은 인식 알고리즘을 개선하는 대신, 물리적 상황을 단순화하는 전략을 택한다. 로봇은 뭉쳐 있는 물체 더미를 손가락이나 도구로 흩어놓는 <strong>싱귤레이션(Singulation)</strong> 동작을 수행한다. 물체들이 물리적으로 분리되면, 시각적 폐색이 사라지고 인식 난이도가 급격히 낮아진다. 이는 복잡한 계산 문제를 물리적 에너지 소비로 치환하여 해결하는 체화된 지능의 전형적인 예이다.</p>
<h3>5.2 상호작용을 통한 범주 수준 자세 추정 (Category-Level Pose Estimation &amp; NOCS)</h3>
<p>새로운 물체(Unseen Object)를 다룰 때, 로봇은 정확한 3D CAD 모델 없이도 그 물체의 자세를 파악해야 한다. 이를 위해 **범주 수준 자세 추정(Category-Level Pose Estimation)**이 사용되며, 모든 물체를 표준화된 좌표계인 **NOCS(Normalized Object Coordinate Space)**로 매핑한다.</p>
<p>하지만 대칭적인 물체(예: 뚜껑이 있는 병, 텍스처 없는 컵)는 정적인 시각 정보만으로는 자세의 모호성이 남는다(예: 병의 앞뒤 구분 불가). 이때 로봇은 <strong>원샷 능동 지각(One-Shot Active Perception)</strong> 전략을 사용한다.</p>
<ul>
<li><strong>전략:</strong> 로봇이 물체를 잡고 회전시키거나 들어 올린다.</li>
<li><strong>효과:</strong> 물체의 움직임에 따른 실루엣 변화나 미세한 텍스처의 이동을 관찰함으로써 회전 대칭성을 해소하고, NOCS 공간상의 정확한 좌표를 확정한다.</li>
<li><strong>의의:</strong> 이를 통해 로봇은 이전에 본 적 없는 물체라도 단 한 번의 상호작용을 통해 범주 모델에 등록(Enrollment)하고, 이후 정밀한 조작을 수행할 수 있게 된다. 이는 지속 학습(Continual Learning)의 관점에서도 중요한 기술이다.</li>
</ul>
<h3>5.3 투명 물체의 촉각적 검증</h3>
<p>앞서 투명 물체의 시각적 한계를 언급했지만, 물리적 상호작용은 가장 확실한 검증 수단(Ground Truth)을 제공한다. 시각 시스템이 투명한 물체의 깊이를 확신하지 못할 때, 로봇은 **가드 이동(Guarded Move)**을 수행한다. 힘 센서를 켠 상태로 조심스럽게 접근하다가 접촉이 감지되면 즉시 멈추는 것이다.</p>
<p>이때 획득된 접촉 좌표(<span class="math math-inline">x, y, z</span>)는 시각적 깊이 지도의 ’구멍’을 메우거나 보정하는 데 사용된다. 나아가, 젤사이트(GelSight)와 같은 고해상도 촉각 센서를 활용하면 투명한 표면의 미세한 곡률이나 흠집까지도 감지하여, 시각적으로는 보이지 않는 물체의 3차원 형상을 역으로 재구성할 수도 있다. <strong>GraspView</strong>와 같은 최신 파이프라인은 RGB 정보와 이러한 능동적 탐색 전략을 결합하여, 깊이 센서 없이도 복잡한 배경 속의 투명 물체를 높은 성공률로 파지함을 증명하였다.</p>
<h2>6.  체화된 지능을 위한 알고리즘 아키텍처</h2>
<p>관찰에서 상호작용으로의 전환은 로봇 소프트웨어 아키텍처의 근본적인 변화를 요구한다. 정적 데이터셋에 대한 지도 학습(Supervised Learning)만으로는 부족하며, 시계열 데이터와 인과관계를 다루는 강화학습(RL)과 생성 모델이 중심이 된다.</p>
<h3>6.1 강화학습(RL)과 시뮬레이션 기반 학습</h3>
<p>능동적 지각 정책(Policy)은 주로 **강화학습(Reinforcement Learning)**을 통해 학습된다. 이때 보상 함수(Reward Function)는 단순히 작업의 성공 여부뿐만 아니라, **정보 획득량(Information Gain)**이나 **내재적 동기(Intrinsic Motivation)**를 포함하도록 설계된다.</p>
<ul>
<li><strong>Sim-to-Real:</strong> 수만 번의 시행착오가 필요한 RL을 실제 로봇에서 수행하는 것은 불가능하므로, 고충실도 시뮬레이터에서의 학습이 필수적이다. 도메인 무작위화(Domain Randomization)를 통해 마찰, 조명, 텍스처 등을 다양화함으로써, 시뮬레이션에서 학습된 능동적 탐색 정책이 실제 세계의 불확실성에도 강건하게 작동하도록 한다.</li>
</ul>
<h3>6.2 월드 모델(World Models)과 비디오 생성 AI</h3>
<p>최근 생성형 AI의 발전은 **월드 모델(World Models)**이라는 새로운 가능성을 열었다. 로봇은 상호작용의 결과를 실제로 수행하기 전에 ’상상’할 수 있다. 비디오 생성 모델(Diffusion Models 등)을 활용하여, “내가 이 컵을 밀면 어떻게 될까?“에 대한 미래 비디오를 생성하고, 그 결과가 유익하다면(예: 가려진 물체가 드러남) 실제로 행동을 수행한다. 이는 **모델 예측 제어(Model Predictive Control, MPC)**와 생성형 AI가 결합된 형태로, 로봇에게 물리적 상식과 예측 능력을 부여한다.</p>
<h3>6.3 비전-언어-행동(VLA) 모델의 통합</h3>
<p><strong>VLA(Vision-Language-Action)</strong> 모델(예: RT-2, Octo)은 대규모 언어 모델(LLM)의 추론 능력과 로봇의 제어 능력을 통합한다. 이들은 “무거운 물체를 찾아라“와 같은 추상적인 명령을 받으면, 단순히 이미지를 분석하는 것을 넘어, 물체를 들어보거나 밀어보는 탐색적 행동 시퀀스를 스스로 생성해낸다. VLA 모델은 방대한 로봇 상호작용 데이터셋에서 학습되므로, 별도의 명시적인 프로그래밍 없이도 ’무게를 확인하려면 들어봐야 한다’는 체화된 지식을 암묵적으로 습득하고 실행한다.</p>
<h2>7.  결론: 상호작용, 지능의 새로운 지평</h2>
<p>7.1절에서 우리는 수동적 관찰의 한계와 이를 극복하기 위한 능동적 상호작용으로의 여정을 살펴보았다. 데이터셋이라는 고정된 무대에서 벗어나, 로봇은 이제 환경이라는 역동적인 시스템 속으로 뛰어들고 있다.</p>
<ul>
<li><strong>한계의 극복:</strong> 능동적 움직임은 폐색을 해결하고, 다중 시점 일관성은 투명성을 뚫어보며, 물리적 접촉은 보이지 않는 물성을 드러낸다.</li>
<li><strong>패러다임의 통합:</strong> 컴퓨터 비전, 제어 이론, 기계 학습이 ’체화된 AI(Embodied AI)’라는 하나의 목표 아래 통합되고 있다. 지각은 행동을 위해 존재하며, 행동은 지각을 돕기 위해 존재한다.</li>
</ul>
<p>미래의 로봇은 단순히 세상을 ‘보는’ 기계가 아니라, 끊임없이 세상을 찌르고, 밀고, 살펴보며 그 구조를 ‘이해하는’ 존재가 될 것이다. 이러한 상호작용적 지능이야말로 실험실을 벗어나 야생(in the Wild)의 복잡성 속에서 생존하고 작동할 수 있는 로봇을 만드는 열쇠이다.</p>
<h3>7.1 표 7.1: 수동적 관찰과 상호작용적 지각의 비교</h3>
<table><thead><tr><th><strong>특징</strong></th><th><strong>수동적 관찰 (Passive Observation)</strong></th><th><strong>상호작용적 지각 (Interactive Perception)</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 입력</strong></td><td>정적 이미지, 고정된 데이터셋 (ImageNet 등)</td><td>연속적인 감각 데이터 스트림 + 모터 신호</td></tr>
<tr><td><strong>에이전트 역할</strong></td><td>관찰자 (Spectator)</td><td>행위자/실험자 (Actor/Experimenter)</td></tr>
<tr><td><strong>폐색 (Occlusion)</strong></td><td>해결 불가 (사전 지식에 의존한 추측)</td><td>해결 가능 (이동하여 가려진 곳 확인)</td></tr>
<tr><td><strong>모호성 (Ambiguity)</strong></td><td>높음 (크기, 착시, 3D 구조 모호성)</td><td>낮음 (운동 시차 및 상호작용으로 해소)</td></tr>
<tr><td><strong>속성 추정</strong></td><td>시각적 외관 (텍스처, 색상)에 국한</td><td>물리적/관성적 속성 (질량, 마찰, 강성) 추정 가능</td></tr>
<tr><td><strong>주요 실패 요인</strong></td><td>투명/반사 물체, 비표준적 시점, 조명 변화</td><td>상호작용의 위험성(충돌/파손), 계산 지연</td></tr>
<tr><td><strong>수학적 목표</strong></td><td>분류 정확도 <span class="math math-inline">P(Y|X</span> 최대화</td><td>정보 이득(Information Gain) <span class="math math-inline">I(X;Z)</span> 최대화</td></tr>
<tr><td><strong>핵심 아키텍처</strong></td><td>CNN, Vision Transformer (ViT)</td><td>RL, 이중 미분 필터(DDF), 월드 모델, VLA</td></tr>
</tbody></table>
<h3>7.2 표 7.2: 인식론적 행동(Epistemic Actions)의 분류</h3>
<table><thead><tr><th><strong>행동 유형</strong></th><th><strong>목표 (Goal)</strong></th><th><strong>예시 시나리오</strong></th></tr></thead><tbody>
<tr><td><strong>능동 시각 (Active Vision)</strong></td><td>기하학적 불확실성 감소</td><td>파지 전 머그컵 손잡이를 확인하기 위해 카메라 이동</td></tr>
<tr><td><strong>탐색적 조작 (Exploratory Manipulation)</strong></td><td>잠재된 물리적 속성 추정</td><td>상자를 밀어서(Push) 마찰계수와 질량 추정</td></tr>
<tr><td><strong>싱귤레이션 (Singulation)</strong></td><td>객체 분할(Segmentation) 성능 향상</td><td>뭉쳐 있는 사과 더미를 손가락으로 흩뜨려 개별 인식</td></tr>
<tr><td><strong>모호성 해소 (Disambiguation)</strong></td><td>범주 및 자세(Pose) 확정</td><td>대칭적인 공구를 회전시켜 고유한 마킹 확인</td></tr>
<tr><td><strong>검증 (Verification)</strong></td><td>감각 가설의 물리적 확인</td><td>투명한 유리벽의 존재를 확인하기 위해 조심스럽게 접촉</td></tr>
</tbody></table>
<h2>8. 참고 자료</h2>
<ol>
<li>What are the current major limitations of computer vision? - Milvus, https://milvus.io/ai-quick-reference/what-are-the-current-major-limitations-of-computer-vision</li>
<li>A large-scale bias-controlled dataset for pushing the limits of object recognition models - ObjectNet, https://objectnet.dev/objectnet-a-large-scale-bias-controlled-dataset-for-pushing-the-limits-of-object-recognition-models.pdf</li>
<li>Passive Observation versus Active Participation - Lifestyle → Sustainability Directory, https://lifestyle.sustainability-directory.com/area/passive-observation-versus-active-participation/</li>
<li>Data issues in most available computer vision datasets - Anyverse, https://anyverse.ai/data-issues-computer-vision-datasets-need-solve/</li>
<li>Active Perception: Interactive Manipulation for Improving Object Detection - Stanford Computer Science, https://cs.stanford.edu/~quocle/activevision.pdf</li>
<li>Occlusion Handling in Generic Object Detection: A Review - arXiv, https://arxiv.org/pdf/2101.08845</li>
<li>Evolving NeRF for Sequential Robot Grasping of Transparent Objects - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v205/kerr23a/kerr23a.pdf</li>
<li>MVTrans: Multi-View Perception of Transparent Objects - IEEE Xplore, https://ieeexplore.ieee.org/iel7/10160211/10160212/10161089.pdf</li>
<li>Tactile-Filter: Interactive Tactile Perception for Part Mating - Robotics, https://www.roboticsproceedings.org/rss19/p079.pdf</li>
<li>Embodied cognition - Wikipedia, https://en.wikipedia.org/wiki/Embodied_cognition</li>
<li>Building an Affordances Map With Interactive Perception - PMC - PubMed Central - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC9127723/</li>
<li>Predictive Eye Movements Characterize Active, Not Passive, Participation in the Collective Embodied Learning of a Scientific Concept - MDPI, https://www.mdpi.com/2076-3417/13/15/8627</li>
<li>Embodied Cognition: Thinking with the Body - Structural Learning, https://www.structural-learning.com/post/embodied-cognition</li>
<li>Interactive Perception: Leveraging Action in Perception and Perception in Action | IEEE Journals &amp; Magazine | IEEE Xplore, https://ieeexplore.ieee.org/document/8007233/</li>
<li>Increase in Mutual Information During Interaction with the Environment Contributes to Perception - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC7514849/</li>
<li>Information Gain and Mutual Information for Machine Learning - MachineLearningMastery.com, https://machinelearningmastery.com/information-gain-and-mutual-information/</li>
<li>Information Gain and Mutual Information for Machine Learning | by Amit Yadav | Biased-Algorithms | Medium, https://medium.com/biased-algorithms/information-gain-and-mutual-information-for-machine-learning-060a79f32981</li>
<li>Active Perception for Grasp Detection via Neural Graspness Field - NIPS, https://proceedings.neurips.cc/paper_files/paper/2024/file/4364fef031fdf7bfd9d1c9c56b287084-Paper-Conference.pdf</li>
<li>Aiding Grasp Synthesis for Novel Objects Using Heuristic-Based and Data-Driven Active Vision Methods - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2021.696587/full</li>
<li>ASGrasp: Generalizable Transparent Object Reconstruction and 6-DoF Grasp Detection from RGB-D Active Stereo Camera - He Wang, https://pku-epic.github.io/ASGrasp/</li>
<li>Predictive Visuo-Tactile Interactive Perception Framework for Object Properties Inference - IEEE Xplore, https://ieeexplore.ieee.org/iel8/8860/10778592/10847911.pdf</li>
<li>[2411.09020] Predictive Visuo-Tactile Interactive Perception Framework for Object Properties Inference - arXiv, https://arxiv.org/abs/2411.09020</li>
<li>Predictive Visuo-Tactile Interactive Perception Framework for Object Properties Inference, https://ieeexplore.ieee.org/document/10847911/</li>
<li>Predictive Visuo-Tactile Interactive Perception Framework for Object Properties Inference - IEEE Xplore, https://ieeexplore.ieee.org/iel8/8860/4359257/10847911.pdf</li>
<li>INTUITIVE, https://usercontent.one/wp/www.intuitive-itn.eu/wp-content/uploads/2024/05/INTUITIVE-Deliverable-5.3-BMW3.pdf</li>
<li>Predictive Visuo-Tactile Interactive Perception Framework for Object Properties Inference - Pure, https://pure.tue.nl/ws/portalfiles/portal/345475565/2411.09020v1.pdf</li>
<li>Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation - Stanford University, https://geometry.stanford.edu/projects/NOCS_CVPR2019/</li>
<li>Category-Level 6D Object Pose Estimation in the Wild: A Semi-Supervised Learning Approach and A New Dataset - NeurIPS, https://proceedings.neurips.cc/paper_files/paper/2022/file/afe99e55be23b3523818da1fefa33494-Paper-Conference.pdf</li>
<li>You Only Look at One: Category-Level Object Representations for Pose Estimation From a Single Example, https://proceedings.mlr.press/v205/goodwin23a/goodwin23a.pdf</li>
<li>You Only Look at One: Category-Level Object Representations for Pose Estimation From a Single Example | OpenReview, https://openreview.net/forum?id=lb7B5Rw7tjw</li>
<li>[2305.12626] You Only Look at One: Category-Level Object Representations for Pose Estimation From a Single Example - arXiv, https://arxiv.org/abs/2305.12626</li>
<li>Visual-tactile Fusion for Transparent Object Grasping in Complex Backgrounds - arXiv, https://arxiv.org/pdf/2211.16693</li>
<li>New progress in transparent object grasping, https://www.sigs.tsinghua.edu.cn/en/2023/0720/c1303a89376/page.htm</li>
<li>[2511.04199] GraspView: Active Perception Scoring and Best-View Optimization for Robotic Grasping in Cluttered Environments - arXiv, https://arxiv.org/abs/2511.04199</li>
<li>Deep Learning - Robotics and Perception Group, https://rpg.ifi.uzh.ch/research_learning.html</li>
<li>[2512.01188] Real-World Reinforcement Learning of Active Perception Behaviors - arXiv, https://arxiv.org/abs/2512.01188</li>
<li>RealD²iff: Bridging Real-World Gap in Robot Manipulation via Depth Diffusion - arXiv, https://arxiv.org/html/2511.22505v1</li>
<li>Video Generation Models in Robotics: Applications, Research Challenges, Future Directions, https://arxiv.org/html/2601.07823v1</li>
<li>Motion Planning and Control with Environmental Uncertainties for Humanoid Robot - MDPI, https://www.mdpi.com/1424-8220/24/23/7652</li>
<li>I-FailSense: Towards General Robotic Failure Detection with Vision-Language Models, https://arxiv.org/html/2509.16072v2</li>
<li>ActiveUMI: Robotic Manipulation with Active Perception from Robot‑Free Human Demonstrations - arXiv, https://arxiv.org/html/2510.01607v1</li>
<li>Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications - IEEE Xplore, https://ieeexplore.ieee.org/iel8/6287639/10820123/11164279.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>