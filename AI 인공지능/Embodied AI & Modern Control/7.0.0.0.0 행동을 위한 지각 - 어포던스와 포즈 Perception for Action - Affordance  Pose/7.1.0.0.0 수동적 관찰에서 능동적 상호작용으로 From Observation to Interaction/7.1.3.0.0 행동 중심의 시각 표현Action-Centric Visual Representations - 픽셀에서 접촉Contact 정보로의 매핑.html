<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.1.3 행동 중심의 시각 표현(Action-Centric Visual Representations): 픽셀에서 접촉(Contact) 정보로의 매핑</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.1.3 행동 중심의 시각 표현(Action-Centric Visual Representations): 픽셀에서 접촉(Contact) 정보로의 매핑</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <a href="index.html">7.1 수동적 관찰에서 능동적 상호작용으로 (From Observation to Interaction)</a> / <span>7.1.3 행동 중심의 시각 표현(Action-Centric Visual Representations): 픽셀에서 접촉(Contact) 정보로의 매핑</span></nav>
                </div>
            </header>
            <article>
                <h1>7.1.3 행동 중심의 시각 표현(Action-Centric Visual Representations): 픽셀에서 접촉(Contact) 정보로의 매핑</h1>
<h2>1.  서론: 시각적 인식의 패러다임 전환과 물리적 접촉의 본질</h2>
<p>로봇 공학, 특히 조작(Manipulation) 분야에서 시각적 인식(Visual Perception)의 역할은 과거의 수동적인 ’관찰자’에서 능동적인 ’행위자’를 위한 인터페이스로 급격하게 진화하고 있다. 전통적인 컴퓨터 비전 파이프라인이 이미지 내의 객체를 분류(Classification)하고, 경계 상자(Bounding Box)를 검출하거나, 의미론적 분할(Semantic Segmentation)을 수행하여 “이것이 무엇인가(What is this?)“를 답하는 데 주력했다면, **행동 중심의 시각 표현(Action-Centric Visual Representations)**은 “이것을 어떻게 조작할 것인가(How to manipulate this?)“라는 질문에 답하기 위해 픽셀 정보를 물리적 접촉 정보로 직접 매핑하는 것을 목표로 한다.</p>
<p>로봇이 환경과 상호작용하기 위한 가장 근본적인 행위는 ’접촉(Contact)’이다. 접촉은 로봇의 말단 장치(End-effector)가 환경 객체의 표면에 물리적인 힘을 가하는 지점이자, 조작의 의도가 실현되는 경계면이다. 그러나 원시적인 RGB-D 픽셀 데이터와 6차원 공간(SE(3))에서의 물리적 접촉 행동 사이에는 거대한 추상화의 간극(Abstraction Gap)이 존재한다. 초기 연구들은 객체의 6D 포즈를 명시적으로 추정한 후 모델 기반의 계획(Model-based Planning)을 수립하는 방식으로 이 간극을 메우려 했으나, 이는 사전에 모델링되지 않은 미지의 객체(Novel Objects), 형상이 자유롭게 변하는 유연체(Deformable Objects), 그리고 객체들이 복잡하게 얽혀 있는 클러터(Clutter) 환경에서는 극심한 한계를 드러낸다.</p>
<p>이에 따라 최근의 연구 흐름은 중간 단계의 명시적 포즈 추정 없이, 시각적 관찰을 곧바로 ‘행동 가능한(Actionable)’ 어포던스(Affordance) 맵이나 접촉 확률 분포로 변환하는 데 집중하고 있다. 이는 제임스 깁슨(J.J. Gibson)의 어포던스 이론을 계산적으로 구현하는 과정으로 해석될 수 있으며, 픽셀 하나하나가 잠재적인 파지점(Grasp Point), 밀기점(Push Point), 또는 조작의 기준점(Keypoint)으로서의 확률적 가치를 지니게 된다. 본 절에서는 이러한 행동 중심 시각 표현이 구현되는 구체적인 방법론들을 <strong>밀집 객체 기술자(Dense Object Descriptors)</strong>, <strong>명시적 접촉 합성(Explicit Contact Synthesis)</strong>, <strong>암시적 기하 표현(Implicit Geometric Representations)</strong>, 그리고 **위상적 대응(Topological Correspondence)**의 네 가지 관점에서 심층적으로 분석한다. 또한, 이러한 표현들이 어떻게 시뮬레이션과 실제 환경의 간극(Sim-to-Real Gap)을 극복하고, 궁극적으로 거대 언어 모델(LLM) 및 비전-언어-행동 모델(VLA)과 같은 파운데이션 모델(Foundation Models)과 결합하여 ’물리적 지능(Physical Intelligence)’으로 확장되는지 논의한다.</p>
<h2>2.  밀집 객체 기술자(Dense Object Descriptors): 픽셀 공간의 의미론적 기하학</h2>
<p>로봇이 “컵의 손잡이를 잡으라“는 명령을 수행하기 위해서는, 컵이라는 객체를 인식하는 것을 넘어 ’손잡이’라는 구체적인 부분(Part)을 다양한 시점과 조명 변화, 심지어는 객체의 형상 변화 속에서도 일관되게 식별할 수 있어야 한다. **밀집 객체 기술자(Dense Object Descriptors, DONs)**는 이미지의 모든 픽셀을 고차원 특징 공간(Feature Space)으로 매핑하여, 서로 다른 이미지 상의 픽셀이라도 동일한 물리적 지점에 해당하면 특징 공간 상에서 서로 가까운 거리에 위치하도록 학습하는 방법론이다.</p>
<h3>2.1  자기지도 학습(Self-Supervised Learning) 기반의 대응성 확보</h3>
<p>DONs의 가장 큰 특징은 사람이 일일이 픽셀에 레이블을 지정하는 고비용의 과정 없이, <strong>자기지도 학습</strong>을 통해 픽셀 간의 대응 관계를 학습한다는 점이다. 이는 로봇이 스스로 데이터를 수집하고 학습하는 자율성을 부여하는 핵심 기제이다.</p>
<ol>
<li><strong>데이터 수집 파이프라인</strong>: 로봇은 정적인 객체 주위를 이동하거나, 로봇 팔에 부착된 카메라(Eye-in-hand)를 통해 객체를 다양한 각도에서 촬영한 RGB-D 비디오를 수집한다. 이때, 카메라의 포즈 정보(Extrinsic Parameters)와 깊이(Depth) 정보를 결합하여 3D 재구성(Reconstruction)을 수행한다. 최근에는 NeRF(Neural Radiance Fields)와 같은 신경 렌더링 기술을 활용하여 더욱 정밀한 3D 밀도 필드(Density Field)를 생성하고, 이를 통해 가려짐(Occlusion)이 있는 상황에서도 강건한 대응 관계를 추출하는 연구가 진행되고 있다.</li>
<li><strong>대응 쌍(Correspondence Pair) 생성</strong>: 3D 재구성 모델은 서로 다른 시점의 2D 이미지들을 연결하는 매개체가 된다. 시점 <span class="math math-inline">A</span>의 이미지 <span class="math math-inline">I_a</span>에 있는 픽셀 <span class="math math-inline">u_a</span>가 3D 공간상의 점 <span class="math math-inline">P</span>에 투영되고, 이 점 <span class="math math-inline">P</span>가 다시 시점 <span class="math math-inline">B</span>의 이미지 <span class="math math-inline">I_b</span>에 있는 픽셀 <span class="math math-inline">u_b</span>로 투영된다면, 쌍 <span class="math math-inline">(u_a, u_b)</span>는 물리적으로 동일한 지점을 가리키는 <strong>양성 쌍(Positive Pair)</strong>, 즉 ‘매치(Match)’ 관계가 된다. 반면, 서로 다른 3D 지점으로 투영되는 픽셀 쌍은 <strong>음성 쌍(Negative Pair)</strong>, 즉 ‘비매치(Non-match)’ 관계가 된다.</li>
</ol>
<h3>2.2  픽셀 단위 대조 손실(Pixel-wise Contrastive Loss)의 수학적 원리</h3>
<p>DONs의 학습은 샴 네트워크(Siamese Network) 구조 하에서 **픽셀 단위의 대조 손실(Pixel-wise Contrastive Loss)**을 최적화하는 과정이다. 이 손실 함수는 특징 공간(Descriptor Space)에서의 거리가 실제 물리적 거리를 반영하도록 강제한다.</p>
<p>이미지 쌍 <span class="math math-inline">(I_a, I_b)</span>에 대해, 매핑 함수 <span class="math math-inline">f(\cdot)</span>(일반적으로 ResNet 등의 완전 합성곱 신경망)가 출력한 기술자 간의 손실 함수 <span class="math math-inline">L</span>은 다음과 같이 정의된다:<br />
<span class="math math-display">
L(I_a, I_b) = L_{matches} + L_{non-matches}
</span><br />
**매치 손실(Match Loss)**은 대응되는 픽셀 간의 거리를 최소화한다:<br />
<span class="math math-display">
L_{matches} = \frac{1}{N_{matches}} \sum_{(u_a, u_b) \in M} \| f(I_a)(u_a) - f(I_b)(u_b) \|^2_2
</span><br />
**비매치 손실(Non-match Loss)**은 대응되지 않는 픽셀 간의 거리가 일정 마진(Margin) <span class="math math-inline">M</span> 이상이 되도록 한다:<br />
<span class="math math-display">
L_{non-matches} = \frac{1}{N_{non-matches}} \sum_{(u_a, u_b) \notin M} \max(0, M - \| f(I_a)(u_a) - f(I_b)(u_b) \|^2_2)
</span><br />
여기서 중요한 점은 **하드 네거티브 마이닝(Hard Negative Mining)**의 역할이다. 학습 초기에는 무작위로 음성 쌍을 선택해도 쉽게 구분이 가능하지만, 학습이 진행될수록 물리적으로 인접해 있거나 텍스처가 유사한 서로 다른 지점들을 구분하는 것이 중요해진다. 따라서, 가장 구분이 어려운(Hard) 음성 쌍들에 더 높은 가중치를 부여하거나 이들을 집중적으로 샘플링함으로써 기술자의 변별력(Discriminative Power)을 극대화한다.</p>
<h3>2.3  Sim-to-Real 전이와 SRDONs</h3>
<p>시뮬레이션 데이터만으로 학습된 기술자를 실제 환경에 적용할 때 발생하는 ’도메인 격차(Domain Gap)’는 주요한 난제이다. 이를 해결하기 위해 제안된 <strong>Sim-to-Real Dense Object Nets (SRDONs)</strong> 은 시뮬레이션 이미지와 실제 이미지 사이의 픽셀 일관성을 유지하는 통합 특징 공간을 학습한다.</p>
<p>SRDONs는 <strong>객체 대 객체 매칭(Object-to-Object Matching)</strong> 기법을 도입하여, 서로 다른 도메인(Sim vs Real)과 서로 다른 배경(Scene)에서 촬영된 이미지 쌍에 대해서도 대응 관계를 학습한다. 이는 GraspNet과 같은 대규모 공개 데이터셋을 활용하여 실제 데이터의 다양성을 확보하고, 도메인 무작위화(Domain Randomization)를 통해 텍스처나 조명 변화에 불변하는 특징을 추출함으로써 가능해진다. 실험적으로 SRDONs는 실제 데이터에 대한 추가 학습(Fine-tuning) 없이도 미지의 객체에 대해 높은 수준의 조작 성능을 달성하여, 행동 중심 시각 표현의 범용성을 입증하였다.</p>
<h3>2.4  로봇 조작을 위한 기술자 공간의 활용</h3>
<p>학습된 3차원 기술자 공간은 RGB 색상 공간으로 시각화할 수 있다. 이때 객체의 동일한 물리적 부위는 시점이 바뀌거나 객체가 변형되어도 항상 같은 색상으로 표현된다. 이러한 특성은 다음과 같은 조작 태스크를 가능하게 한다:</p>
<ul>
<li><strong>원샷 모방 학습(One-shot Imitation Learning)</strong>: 사용자가 시연 영상에서 컵의 손잡이를 한 번만 지정하면, 로봇은 새로운 장면에서 컵의 위치나 방향이 바뀌어도 기술자 매칭을 통해 손잡이의 픽셀 좌표를 즉시 찾아내어 파지할 수 있다.</li>
<li><strong>비정형 물체 조작</strong>: 로프나 옷감과 같은 유연체는 6D 포즈로 정의할 수 없다. 그러나 DONs는 유연체의 위상적 구조를 보존하므로, “로프의 끝 매듭“이나 “모자의 챙“과 같은 특징점을 강건하게 추적할 수 있다. 이는 로봇이 로프의 매듭을 묶거나(Knot Tying) 옷을 개는 등의 복잡한 조작을 수행하는 기반이 된다.</li>
</ul>
<h2>3.  명시적 접촉 합성: Contact-GraspNet의 점군 기반 접근</h2>
<p>밀집 기술자가 객체의 ’특정 부위’를 식별하는 데 초점을 맞춘다면, <strong>Contact-GraspNet</strong>은 객체 표면에서 ’안정적인 파지가 가능한 접촉점’을 명시적으로 탐색하고 생성하는 데 집중한다. 이는 특히 클러터 환경이나 미지의 객체가 산재한 비정형 환경에서 6자유도(6-DoF) 파지 포즈를 생성하는 문제를 획기적으로 단순화한다.</p>
<h3>3.1  6-DoF 파지 문제의 재정의: 샘플링에서 접촉 예측으로</h3>
<p>기존의 파지 생성 방법들은 6차원 공간 전체에서 파지 후보를 샘플링하거나(Grasp Sampling), 복잡한 비용 함수를 최적화하는 방식을 사용했다. 이는 탐색 공간이 방대하여 계산 효율이 낮고, 지역 최적해(Local Minima)에 빠지기 쉽다는 단점이 있었다.</p>
<p>Contact-GraspNet은 이 문제를 **“관측된 점군(Point Cloud) 상의 점들을 잠재적인 접촉점(Potential Contact Points)으로 분류하는 문제”**로 재정의한다. 즉, 파지 포즈 <span class="math math-inline">g \in SE(3)</span>의 6개 자유도를 접촉점 <span class="math math-inline">c \in \mathbb{R}^3</span>(3-DoF)와, 해당 접촉점에서의 파지 방향 및 폭(3+1 DoF)으로 분해하여 차원을 축소한다. 이는 “어디를 잡을 것인가(위치)“를 먼저 결정하고, “어떻게 잡을 것인가(자세)“를 종속적으로 추론하는 계층적 접근 방식이다.</p>
<h3>3.2  PointNet++ 기반의 아키텍처와 헤드 설계</h3>
<p>Contact-GraspNet은 원시 깊이(Depth) 데이터로부터 변환된 3D 점군을 입력으로 받아, <strong>PointNet++</strong>  아키텍처를 통해 각 점의 국소적(Local) 및 전역적(Global) 기하 특징을 추출한다. 네트워크는 입력된 <span class="math math-inline">N</span>개의 점 각각에 대해 다중 헤드(Multi-head) 출력을 생성한다 :</p>
<ol>
<li><strong>접촉 점수(Contact Confidence, <span class="math math-inline">s</span>)</strong>: 해당 점이 성공적인 파지를 위한 접촉점일 확률(0~1 사이의 값). 이는 이진 분류 문제로 학습된다.</li>
<li><strong>파지 회전(Grasp Rotation, <span class="math math-inline">R_g</span>)</strong>: 그리퍼의 접근 벡터(Approach Vector, <span class="math math-inline">a</span>)와 베이스라인 벡터(Baseline Vector, <span class="math math-inline">b</span>). 네트워크는 3차원 벡터 <span class="math math-inline">z_1, z_2</span>를 예측한 후, 그람-슈미트(Gram-Schmidt) 직교화를 통해 유효한 회전 행렬을 복원한다.</li>
<li><strong>파지 폭(Grasp Width, <span class="math math-inline">w</span>)</strong>: 그리퍼가 벌려야 할 최적의 너비.</li>
<li><strong>오프셋(Offset, <span class="math math-inline">d</span>)</strong>: 관측된 표면 점(접촉점)과 그리퍼의 중심(TCP) 사이의 거리. 실제 파지 시 그리퍼가 물체와 충돌하지 않고 깊숙이 접근할 수 있도록 한다.</li>
</ol>
<p>이러한 구조는 각 3D 점이 자신의 로컬 기하학적 형상(예: 곡률, 평탄도)을 바탕으로 파지 가능성을 독립적으로 “투표(Vote)“하는 것과 같다. 따라서 장면 전체의 복잡도와 무관하게, 국소적인 파지 가능성을 신속하게 추론할 수 있다.</p>
<h3>3.3  클러터 환경에서의 강건성과 일반화 능력</h3>
<p>Contact-GraspNet은 1,700만 개 이상의 시뮬레이션 파지 데이터를 학습하여, 물체들이 서로 겹쳐 있거나 부분적으로 가려진(Occluded) 클러터 환경에서도 탁월한 성능을 발휘한다. 학습 과정에서는 물리 시뮬레이션을 통해 검증된 성공적인 파지(Positive Grasps)만을 정답 데이터로 활용하며, 가려지지 않은 표면 점들만을 이용하여 충돌 없는(Collision-free) 파지 포즈를 생성하도록 유도한다.</p>
<p>실험 결과에 따르면, Contact-GraspNet은 학습하지 않은 새로운 객체(Unseen Objects)와 복잡한 클러터 환경에서 90% 이상의 파지 성공률을 기록했으며, 이는 기존 최신 연구(SOTA) 대비 실패율을 절반으로 줄인 성과이다. 이는 픽셀(또는 3D 점) 자체를 접촉 후보로 매핑하는 전략이 센서 노이즈나 데이터의 불완전성에 대해 강력한 내성(Tolerance)을 가짐을 시사한다.</p>
<h3>3.4  세그먼트 애니씽(Segment Anything)과의 통합</h3>
<p>최근에는 Contact-GraspNet이나 <strong>AnyGrasp</strong> 와 같은 모델들이 **Segment Anything Model (SAM)**과 같은 거대 비전 모델과 결합하여 ‘개방형 어휘(Open-Vocabulary)’ 조작으로 확장되고 있다. SAM을 통해 사용자 명령(예: “빨간색 머그컵”)에 해당하는 객체 마스크를 추출하고, AnyGrasp가 해당 마스크 영역 내의 점군에 대해서만 파지 포즈를 생성함으로써, 의미론적 인식과 기하학적 조작을 결합하는 방식이다. 이는 단순한 ’파지’를 넘어 ’의도된 대상의 파지’를 가능케 하는 중요한 진전이다.</p>
<h2>4.  암시적 기하 표현과 전체론적 파지: CenterGrasp</h2>
<p>Contact-GraspNet이 표면의 점들에 집중한다면, <strong>CenterGrasp</strong> 는 **암시적 표현(Implicit Representation)**을 활용하여 객체의 ’보이지 않는 형상’까지 추론하고, 이를 바탕으로 더욱 정교한 파지 계획을 수립한다. 이는 단일 시점(Single-view) 관측의 한계를 극복하기 위한 접근이다.</p>
<h3>4.1  객체 인식(Object-Aware) 암시적 표현</h3>
<p>CenterGrasp는 객체를 **중심점(Center Point)**과 그에 대응하는 **잠재 코드(Latent Code)**로 표현한다. 입력 이미지는 인코더를 거쳐 객체의 중심점 히트맵, 6D 포즈, 그리고 형상 잠재 벡터(Latent Shape Vector)를 예측한다. 이 잠재 벡터는 객체의 전체적인 3D 형상 정보를 압축하고 있으며, 디코더를 통해 3D 공간 상의 임의의 점에 대한 속성을 질의(Query)할 수 있게 한다.</p>
<p>이는 장면 전체를 하나의 점군으로 처리하는 기존 방식과 달리, 개별 객체 단위로 정보를 분리하여 처리하는 ‘객체 인식(Object-Aware)’ 특성을 갖는다. 따라서 인접한 물체 간의 간섭을 명확히 구분하고, 특정 객체에 대한 전체론적(Holistic)인 조작 계획이 가능하다.</p>
<h3>4.2  형상 및 파지 거리 함수(SGDF)</h3>
<p>CenterGrasp의 핵심은 **형상 및 파지 거리 함수(Shape and Grasp Distance Function, SGDF)**이다. 이는 DeepSDF의 개념을 확장하여, 신경망 <span class="math math-inline">f_\theta(x, z)</span>가 3D 좌표 <span class="math math-inline">x</span>와 잠재 코드 <span class="math math-inline">z</span>를 입력받아 두 가지 값을 동시에 출력하도록 설계되었다:</p>
<ol>
<li><strong>형상 거리(Shape Distance)</strong>: 해당 점 <span class="math math-inline">x</span>에서 객체 표면까지의 부호 거리(Signed Distance). <span class="math math-inline">f(x) &lt; 0</span>이면 객체 내부, <span class="math math-inline">f(x) &gt; 0</span>이면 외부를 의미하며, 이를 통해 객체의 3D 형상을 완벽하게 복원할 수 있다.</li>
<li><strong>파지 거리(Grasp Distance)</strong>: 해당 점 <span class="math math-inline">x</span>가 유효한 파지 포즈 매니폴드(Grasp Pose Manifold)에 얼마나 가까운지를 나타내는 값. 이 값이 작을수록 안정적인 파지가 가능한 위치임을 의미한다.</li>
</ol>
<p>SGDF 디코더는 학습된 잠재 공간을 통해 관측되지 않은 객체의 뒷면 형상을 “상상(Hallucinate)“하고, 그 위에서 물리적으로 타당한 접촉점을 찾아낸다. 이는 단순히 보이는 표면만을 고려하는 명시적 방법론들이 놓칠 수 있는, 무게 중심을 고려한 파지나 충돌 회피 전략을 수립하는 데 결정적인 이점을 제공한다. 실험 결과, CenterGrasp는 형상 복원 오차를 38.5mm 줄이고 파지 성공률을 33%포인트 향상시키는 등 SOTA 성능을 입증하였다.</p>
<h2>5.  변형 물체 조작을 위한 밀집 시각 대응: UniGarmentManip</h2>
<p>강체(Rigid Object)와 달리, 옷이나 천과 같은 **변형 물체(Deformable Objects)**는 무한한 자유도를 가지며, 중력과 외력에 의해 형상이 극적으로 변한다. 이러한 물체에 대해 ’접촉점’을 정의하는 것은 기하학적 위치만으로는 불가능하며, 위상적(Topological) 구조에 대한 이해가 필수적이다. <strong>UniGarmentManip</strong> 은 이러한 난제를 해결하기 위해 **밀집 시각 대응(Dense Visual Correspondence)**과 <strong>스켈레톤(Skeleton)</strong> 기반의 표현을 결합한 통합 프레임워크를 제시한다.</p>
<h3>5.1  스켈레톤 포인트(Skeleton Points)와 구조적 앵커링</h3>
<p>옷을 개거나 걸기 위해서는 옷이 구겨져 있더라도 ‘왼쪽 소매 끝’, ‘목 칼라’, ’밑단 모서리’와 같은 기능적 지점을 정확히 찾아내야 한다. UniGarmentManip은 이를 위해 **카테고리 수준의 조작(Category-Level Manipulation)**을 목표로 한다. 즉, 서로 다른 디자인과 크기의 티셔츠라도 위상적으로는 동일한 구조(몸통 하나, 소매 둘)를 공유한다는 점에 착안한다.</p>
<p>이 프레임워크는 변형된 옷의 관측 <span class="math math-inline">O</span>와 펼쳐진 상태의 정준 형상(Canonical Shape) <span class="math math-inline">\bar{O}</span> 사이, 혹은 서로 다른 두 옷 <span class="math math-inline">O</span>와 <span class="math math-inline">O&#39;</span> 사이의 픽셀/포인트 단위 대응 관계를 학습한다. 그러나 픽셀 단위의 직접 매핑은 변형이 심할 경우 학습이 어렵다. 이를 극복하기 위해 <strong>스켈레톤 포인트</strong>라는 중간 표현을 도입한다. 스켈레톤 포인트는 옷의 구조를 정의하는 핵심 키포인트들의 집합으로, 희소(Sparse)하고, 서로 구별 가능(Distinct)하며, 순서가 있는(Ordered) 특징을 갖는다.</p>
<h3>5.2  시뮬레이션 기반의 포인트 트레이싱(Point Tracing)과 InfoNCE 손실</h3>
<p>학습 데이터 생성을 위해 물리 시뮬레이터(예: PyFlex)를 활용한다. 시뮬레이션 상에서 옷을 펼친 상태(Flattened State)에서 스켈레톤 포인트들을 정의하고, 옷이 무작위로 구겨지거나 변형될 때 이 포인트들이 3D 공간 상에서 어디로 이동하는지를 <strong>포인트 트레이싱(Point Tracing)</strong> 기법으로 추적하여 지상참(Ground Truth) 대응 관계를 생성한다.</p>
<p>네트워크(주로 PointNet++ 기반) 학습에는 <strong>InfoNCE(Information Noise Contrastive Estimation)</strong> 손실 함수가 사용된다. 이는 양성 쌍(Positive Pair)과 다수의 음성 쌍(Negative Pair)을 동시에 고려하여 학습 효율을 극대화하는 방식이다.<br />
<span class="math math-display">
\mathcal{L}_{NCE} = - \log \frac{\exp(f(p) \cdot f(p&#39;) / \tau)}{\sum_{i=1}^{K} \exp(f(p) \cdot f(p&#39;_i) / \tau)}
</span><br />
여기서 <span class="math math-inline">(p, p&#39;)</span>는 구조적으로 동일한 지점에 해당하는 양성 쌍(예: 변형 전후의 동일한 소매 끝점)이며, <span class="math math-inline">\{p&#39;_i\}_{i=1}^K</span>는 다른 지점들(음성 샘플)이다. <span class="math math-inline">\tau</span>는 온도 매개변수이다. 연구에서는 배치(Batch) 당 32개의 옷 쌍을 사용하고, 각 쌍마다 20개의 양성 쌍과 150개의 음성 쌍을 샘플링하여, 네트워크가 미세한 변형 차이와 구조적 유사성을 동시에 학습하도록 한다.</p>
<h3>5.3  교차 변형 및 교차 객체 일반화</h3>
<p>이러한 학습을 통해 UniGarmentManip은 <strong>교차 변형(Cross-Deformation)</strong> 및 <strong>교차 객체(Cross-Object)</strong> 대응성을 확보한다. 로봇은 한 번의 시연(Demonstration)만 보면, 새로운 옷이 아무리 구겨져 있어도 시연된 접촉 지점(예: “어깨를 잡아서 들어라”)에 해당하는 픽셀을 정확히 찾아내어 조작 행동을 생성할 수 있다. 이는 픽셀 정보를 옷의 ’구조적 뼈대’에 매핑함으로써, 변형에도 불변하는(Invariant) 접촉 목표점을 생성하는 강력한 메커니즘이다.</p>
<h2>6.  시각적 어포던스와 범용 조작 정책 (UMPNet)</h2>
<p>픽셀에서 접촉으로의 매핑은 단순히 기하학적 위치를 찾는 것을 넘어, 그 점이 제공하는 **행동 유도성(Affordance)**을 이해하는 것으로 확장된다. 어포던스 맵(Affordance Map)은 이미지의 각 픽셀에 대해 “이곳을 누르면 문이 열림”, “이곳을 잡으면 들어 올릴 수 있음“과 같은 조작 성공 확률을 할당한 것이다.</p>
<h3>6.1  접촉 맵(Contact Map) 예측과 3단계 조작 정책</h3>
<p><strong>Universal Manipulation Policy (UMPNet)</strong> 은 다양한 종류의 관절 객체(Articulated Objects: 문, 서랍, 냉장고 등)를 다루기 위해, 시각적 입력으로부터 조작 가능한 접촉 맵을 직접 예측하고 이를 기반으로 정책을 수립한다.</p>
<p>UMPNet은 조작 과정을 **접근(Approach) - 파지(Grasp) - 조작(Operate)**의 3단계로 분리하고, 각 단계에 맞는 범용 정책을 학습한다. 여기서 핵심은 **접촉점 예측(Contact Point Prediction)**이 전체 정책의 시발점이라는 것이다. 로봇은 RGB-D 이미지를 입력받아, 조작의 대상이 되는 객체의 기능적 부위(예: 문의 손잡이, 서랍의 앞판)에 높은 값을 부여하는 히트맵(Heatmap)을 출력한다. 이 접촉 맵은 로봇의 운동학적 제약 조건과 물리적 상호작용의 결과를 함축적으로 포함한다.</p>
<h3>6.2  범용성(Universality)과 일반화</h3>
<p>실험 결과, 접촉 위치를 명시적으로 예측하고 이를 정책에 반영하는 것(Contact-based Policy)이 무작위 접촉이나 단순한 엔드-투-엔드 학습보다 월등히 높은 조작 성공률을 보임이 입증되었다. UMPNet은 문이 열리는 방식(미닫이 vs 여닫이)이나 손잡이의 형태(레버형 vs 노브형)가 달라도, ’기능적 접촉점’을 일관되게 찾아냄으로써 범용적인 조작 능력을 보여주었다. 이는 행동 중심 시각 표현이 다양한 메커니즘을 가진 객체들 사이에서도 일반화될 수 있는 공통의 ‘조작 언어’ 역할을 수행함을 의미한다.</p>
<h2>7.  결론 및 미래 전망: 파운데이션 모델과 물리적 지능의 결합</h2>
<p>지금까지 살펴본 바와 같이, 행동 중심의 시각 표현은 원시 픽셀 데이터를 로봇이 물리적으로 개입할 수 있는 접촉 정보로 변환하는 다양한 메커니즘을 발전시켜 왔다. 아래 표는 논의된 주요 방법론들을 요약하여 비교한 것이다.</p>
<table><thead><tr><th><strong>방법론 (Methodology)</strong></th><th><strong>핵심 표현 (Core Representation)</strong></th><th><strong>주요 메커니즘 (Mechanism)</strong></th><th><strong>대상 객체 (Target Object)</strong></th><th><strong>특징 (Key Feature)</strong></th></tr></thead><tbody>
<tr><td><strong>DONs</strong></td><td>밀집 기술자 (Dense Descriptors)</td><td>픽셀 단위 대조 학습 (Pixel-wise Contrastive Learning)</td><td>강체, 유연체 (Rigid/Deformable)</td><td>특정 지점 식별, 시점 불변성, One-shot 전이</td></tr>
<tr><td><strong>Contact-GraspNet</strong></td><td>점군 기반 접촉점 (Point-wise Contact)</td><td>6-DoF 파지를 접촉점+방향+폭으로 분해</td><td>클러터 환경, 미지 객체</td><td>명시적 접촉 예측, 높은 강건성, 실시간 추론</td></tr>
<tr><td><strong>CenterGrasp</strong></td><td>암시적 거리 함수 (Implicit Function, SGDF)</td><td>객체 중심(Center) 임베딩 및 형상 복원</td><td>강체, 관절 물체</td><td>형상-파지 통합 학습, 객체 인식, 비관측 영역 추론</td></tr>
<tr><td><strong>UniGarmentManip</strong></td><td>밀집 시각 대응 (Dense Visual Correspondence)</td><td>스켈레톤 포인트, InfoNCE 손실</td><td>변형 물체 (옷, 천)</td><td>위상적 구조 보존, 교차 형상/변형 대응, 카테고리 일반화</td></tr>
<tr><td><strong>UMPNet</strong></td><td>어포던스/접촉 맵 (Affordance Map)</td><td>범용 정책 학습, 접촉 위치 예측</td><td>관절 물체 (Articulated)</td><td>다양한 메커니즘(문, 서랍 등)에 대한 일반화</td></tr>
</tbody></table>
<h3>7.1  파운데이션 모델 시대의 물리적 지능(Physical Intelligence)</h3>
<p>현재 이 분야는 거대 언어 모델(LLM) 및 **비전-언어-행동 모델(VLA, Vision-Language-Action Models)**과 같은 파운데이션 모델과의 결합으로 새로운 국면을 맞이하고 있다. 파운데이션 모델은 인터넷 규모의 데이터를 통해 습득한 상식적 추론 능력(Common Sense Reasoning)을 로봇의 물리적 조작에 주입하고 있다.</p>
<p>예를 들어, <strong>CronusVLA</strong> 와 같은 연구는 객체 중심의 표현을 사전 학습된 VLA에 주입하여 조작 효율을 높이고 있으며, <strong>EgoScaler</strong> 는 인간의 에고센트릭 비디오로부터 6-DoF 궤적을 추출하여 로봇의 행동 표현으로 변환한다. NVIDIA의 <strong>Cosmos</strong> 와 같은 물리 AI(Physical AI)를 위한 세계 파운데이션 모델(World Foundation Models)은 비디오 생성과 물리 시뮬레이션을 결합하여 로봇이 미래의 물리적 상호작용 결과를 예측하고 계획하는 능력을 획기적으로 향상시킬 것으로 기대된다.</p>
<p>또한, **안전한 학습(Safe Learning)**에 대한 연구 는 접촉이 풍부한(Contact-Rich) 작업에서 파운데이션 모델이 어떻게 안전성을 보장하며 탐색할 수 있을지를 탐구하고 있다. 미래의 행동 중심 시각 표현은 단순한 기하학적 적합성(Geometric Fit)을 넘어, 물체의 물리적 속성(마찰, 재질, 무게), 의미론적 기능(Semantic Function), 그리고 작업의 맥락(Context)까지 아우르는 진정한 의미의 **“물리적 지능”**의 핵심 인터페이스로 진화할 것이다. 이는 픽셀에서 접촉으로의 매핑이 로봇 공학의 성배(Holy Grail)인 완전 자율 범용 조작(Fully Autonomous General Manipulation)을 향한 가장 확실한 가교임을 시사한다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>A Survey on Efficient Vision-Language-Action Models - arXiv, https://arxiv.org/html/2510.24795v1</li>
<li>Object-Centric Action-Enhanced Representations for Robot Visuo-Motor Policy Learning, https://arxiv.org/html/2505.20962v1</li>
<li>CVPR Poster UniGarmentManip: A Unified Framework for Category-Level Garment Manipulation via Dense Visual Correspondence, https://cvpr.thecvf.com/virtual/2024/poster/30566</li>
<li>UniGarmentManip: A Unified Framework for Category-Level Garment Manipulation via Dense Visual Correspondence - GitHub Pages, https://deformable-workshop.github.io/icra2024/spotlight/02_10_wdo_wu_unigarmentmanip.pdf</li>
<li>Learning Sim-to-Real Dense Object Descriptors for Robotic Manipulation - IEEE Xplore, https://ieeexplore.ieee.org/document/10161477/</li>
<li>Learning dense visual object descriptor by and for robotic manipulation, https://www.cs.utexas.edu/~yukez/cs391r_fall2021/slides/pre_09-21_Gabriel.pdf</li>
<li>Learning Dense Visual Object Descriptors By and For Robotic Manipulation - YouTube, https://www.youtube.com/watch?v=L5UW1VapKNE</li>
<li>NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields - DSpace@MIT, https://dspace.mit.edu/bitstream/handle/1721.1/153644/2203.01913v1.pdf?sequence=1&amp;isAllowed=y</li>
<li>Dense Object Nets for Robotic Manipulation - Emergent Mind, https://www.emergentmind.com/papers/1806.08756</li>
<li>Learning Dense Object Descriptors from Multiple Views for Low-shot Category Generalization - NeurIPS, https://proceedings.neurips.cc/paper_files/paper/2022/file/517a0884c56008f8bf9d5912ca771d71-Paper-Conference.pdf</li>
<li>Dense Object Nets and Descriptors for Robotic Manipulation - Seita’s Place, https://danieltakeshi.github.io/2019/11/09/paper-set-descriptors/</li>
<li>More On Dense Object Nets and Descriptors: Applications to Rope Manipulation and Kit Assembly - Seita’s Place, https://danieltakeshi.github.io/2020/02/09/descriptors-2/</li>
<li>Learning Dense Visual Descriptors using Image Augmentations for Robot Manipulation Tasks, https://proceedings.mlr.press/v205/graf23a/graf23a.pdf</li>
<li>Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes | Request PDF, https://www.researchgate.net/publication/355434800_Contact-GraspNet_Efficient_6-DoF_Grasp_Generation_in_Cluttered_Scenes</li>
<li>Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes - ResearchGate, https://www.researchgate.net/publication/350457061_Contact-GraspNet_Efficient_6-DoF_Grasp_Generation_in_Cluttered_Scenes</li>
<li>Contact-graspnet: Efficient 6-dof grasp generation in cluttered scenes, https://elib.dlr.de/145798/1/Contact-GraspNet.pdf</li>
<li>Converting Point Completion to Geometry Feature to Enhance 6-DoF Grasp - arXiv, https://arxiv.org/html/2504.16320v2</li>
<li>AnyPlace: Learning Generalized Object Placement for Robot Manipulation - arXiv, https://arxiv.org/html/2502.04531v1</li>
<li>Open-Vocabulary Part-Based Grasping - arXiv, https://arxiv.org/html/2406.05951v2</li>
<li>Demonstrating OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics, https://roboticsproceedings.org/rss20/p091.pdf</li>
<li>CenterGrasp: Object-Aware Implicit Representation Learning for Simultaneous Shape Reconstruction and 6-DoF Grasp Estimation | Request PDF - ResearchGate, https://www.researchgate.net/publication/379856469_CenterGrasp_Object-Aware_Implicit_Representation_Learning_for_Simultaneous_Shape_Reconstruction_and_6-DoF_Grasp_Estimation</li>
<li>CenterGrasp: Object-Aware Implicit Representation Learning for Simultaneous Shape Reconstruction and 6-DoF Grasp Estimation - arXiv, https://arxiv.org/html/2312.08240v2</li>
<li>CenterGrasp: Object-Aware Implicit Representation Learning for Simultaneous Shape Reconstruction and 6-DoF Grasp Estimation - arXiv, https://arxiv.org/html/2312.08240v1</li>
<li>CenterArt: Joint Shape Reconstruction and 6-DoF Grasp Estimation of Articulated Objects, https://arxiv.org/html/2404.14968v1</li>
<li>UniGarmentManip: A Unified Framework for Category-Level Garment Manipulation via Dense Visual Correspondence | Request PDF - ResearchGate, https://www.researchgate.net/publication/384161945_UniGarmentManip_A_Unified_Framework_for_Category-Level_Garment_Manipulation_via_Dense_Visual_Correspondence</li>
<li>UniGarmentManip: A Unified Framework for Category-Level Garment Manipulation via Dense Visual Correspondence - arXiv, https://arxiv.org/html/2405.06903v1</li>
<li>DexGarmentLab: Dexterous Garment Manipulation Environment with Generalizable Policy, https://arxiv.org/html/2505.11032v1</li>
<li>[Literature Review] UniGarmentManip: A Unified Framework for Category-Level Garment Manipulation via Dense Visual Correspondence - Moonlight, https://www.themoonlight.io/en/review/unigarmentmanip-a-unified-framework-for-category-level-garment-manipulation-via-dense-visual-correspondence</li>
<li>DexGarmentLab: Dexterous Garment Manipulation Environment with Generalizable Policy, https://arxiv.org/html/2505.11032v3</li>
<li>RMDO 2024: UniGarmentManip: A Unified Framework for Category-Level Garment Manipulation via Dense… - YouTube, https://www.youtube.com/watch?v=N5NYt-XJDOs</li>
<li>UniGarmentManip: A Unified Framework for Category-Level Garment Manipulation via Dense Visual Correspondence, https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_UniGarmentManip_A_Unified_Framework_for_Category-Level_Garment_Manipulation_via_Dense_CVPR_2024_paper.pdf</li>
<li>UMPNet: Universal Manipulation Policy Network for Articulated Objects - ResearchGate, https://www.researchgate.net/publication/357814286_UMPNet_Universal_Manipulation_Policy_Network_for_Articulated_Objects</li>
<li>UniDoorManip: Learning Universal Door Manipulation Policy Over Large-scale and Diverse Door Manipulation Environments - arXiv, https://arxiv.org/html/2403.02604v1</li>
<li>HACMan: Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation, https://proceedings.mlr.press/v229/zhou23a/zhou23a.pdf</li>
<li>Foundation Models for Robotics: Vision-Language-Action (VLA) | Rohit Bandaru, https://rohitbandaru.github.io/blog/Foundation-Models-for-Robotics-VLA/</li>
<li>NVIDIA Cosmos - Physical AI with World Foundation Models, https://www.nvidia.com/en-us/ai/cosmos/</li>
<li>Safe Learning for Contact-Rich Robot Tasks: A Survey from Classical Learning-Based Methods to Safe Foundation Models - arXiv, https://arxiv.org/html/2512.11908v1</li>
<li>(PDF) Safe Learning for Contact-Rich Robot Tasks: A Survey From Classical Learning-Based Methods to Safe Foundation Models - ResearchGate, https://www.researchgate.net/publication/398287994_Safe_Learning_for_Contact-Rich_Robot_Tasks_A_Survey_From_Classical_Learning-Based_Methods_to_Safe_Foundation_Models</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>