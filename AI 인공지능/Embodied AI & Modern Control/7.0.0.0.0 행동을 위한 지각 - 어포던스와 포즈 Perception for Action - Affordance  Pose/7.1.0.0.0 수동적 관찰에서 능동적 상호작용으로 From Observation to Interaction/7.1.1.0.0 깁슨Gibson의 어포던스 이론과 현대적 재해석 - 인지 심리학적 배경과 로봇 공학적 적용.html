<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.1.1 깁슨(Gibson)의 어포던스 이론과 현대적 재해석: 인지 심리학적 배경과 로봇 공학적 적용</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.1.1 깁슨(Gibson)의 어포던스 이론과 현대적 재해석: 인지 심리학적 배경과 로봇 공학적 적용</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <a href="index.html">7.1 수동적 관찰에서 능동적 상호작용으로 (From Observation to Interaction)</a> / <span>7.1.1 깁슨(Gibson)의 어포던스 이론과 현대적 재해석: 인지 심리학적 배경과 로봇 공학적 적용</span></nav>
                </div>
            </header>
            <article>
                <h1>7.1.1 깁슨(Gibson)의 어포던스 이론과 현대적 재해석: 인지 심리학적 배경과 로봇 공학적 적용</h1>
<h2>1.  서론: 인지과학의 코페르니쿠스적 전환과 로봇 공학의 새로운 지평</h2>
<h3>1.1  표상주의(Representationalism)의 한계와 생태학적 접근의 태동</h3>
<p>20세기 중반까지 인지심리학과 초기 인공지능(AI) 연구를 지배했던 패러다임은 세상을 내부적으로 재구성하려는 ’표상주의(Representationalism)’와 이를 기호로 연산하려는 ’계산주의(Computationalism)’였다. 이 관점에서 지각(Perception)은 감각 데이터를 통해 외부 세계의 3차원 모델을 머릿속에 구축하는 과정이었으며, 행동(Action)은 이 모델을 바탕으로 계획(Planning)된 결과의 집행에 불과했다. 그러나 이러한 접근법은 로봇 공학이 통제된 실험실을 벗어나 복잡하고 비정형화된 현실 세계(Unstructured Environments)로 진입하자마자 심각한 난관에 봉착했다. 로봇이 행동하기 위해 고려해야 할 변수의 수가 기하급수적으로 폭발하는 ’프레임 문제(Frame Problem)’와, 추상적인 기호가 물리적 실체와 어떻게 연결되는지를 설명하지 못하는 ’기호 접지 문제(Symbol Grounding Problem)’가 그것이다.</p>
<p>이러한 맥락에서 제임스 깁슨(James J. Gibson)이 1979년 저서 <em>The Ecological Approach to Visual Perception</em>에서 제창한 ’생태학적 심리학(Ecological Psychology)’과 ‘어포던스(Affordance)’ 이론은 인지과학의 코페르니쿠스적 전환을 유발했다. 깁슨은 정보가 동물의 머릿속에 있는 것이 아니라 환경에 내재되어 있으며, 지각은 복잡한 내부 연산을 거쳐 구축되는 것이 아니라 환경의 불변량(Invariants)을 직접 탐지(Direct Perception)하는 과정이라고 주장했다. 이는 로봇이 세상을 ’해석’하려 들지 않고, 세상이 제공하는 ’행동 가능성’을 직접 지각함으로써 즉각적이고 적응적인 행동을 생성할 수 있는 이론적 토대를 제공한다.</p>
<h3>1.2  현대 로봇 공학에서의 어포던스 재조명</h3>
<p>오늘날 딥러닝(Deep Learning)과 4E 인지(Embodied, Embedded, Enacted, Extended Cognition) 과학의 발전은 깁슨의 이론을 단순한 심리학적 가설에서 구체적인 공학적 방법론으로 격상시켰다. 특히 시각적 어포던스 학습(Visual Affordance Learning)은 로봇이 “이 물체는 컵이다“라고 분류하는 것을 넘어, “이 물체는 손잡이를 통해 잡을 수 있고, 빈 공간에 액체를 담을 수 있다“는 기능적 의미를 픽셀 단위에서 직접 학습하게 한다. 본 장에서는 깁슨의 원류적 이론부터 시작하여, 이를 둘러싼 심리학적/철학적 논쟁, 그리고 현대 로봇 공학에서의 수학적 형식화와 최신 딥러닝 모델(Where2Act, VRB, Robo-ABC 등)에 이르는 진화 과정을 포괄적이고 심층적으로 분석한다.</p>
<h2>2.  제임스 깁슨의 생태학적 심리학: 원류와 본질</h2>
<h3>2.1  어포던스의 정의: 주관과 객관의 이분법을 넘어서</h3>
<p>깁슨은 어포던스를 다음과 같이 정의했다: “환경의 어포던스는 환경이 동물에게 제공하는 것(what it offers), 준비해 주는 것(what it provides or furnishes)이며, 이는 유익할 수도 있고 해로울 수도 있다”. 이 정의의 가장 급진적인 측면은 전통적인 이원론적 세계관—물리적 실재(객관)와 현상학적 경험(주관)—을 거부한다는 점이다.</p>
<p>깁슨에 따르면, 어포던스는 “객관적 속성도 아니고 주관적 속성도 아니며, 혹은 그 둘 다“이다. 예를 들어, ’앉을 수 있음(Sittability)’이라는 어포던스는 의자의 물리적 높이(객관적 사실)와 행위자의 다리 길이(신체적 사실) 사이의 <strong>관계(Relation)</strong> 속에 존재한다. 따라서 어포던스는 “환경의 사실이면서 동시에 행동의 사실(fact of the environment and a fact of behavior)“이라는 이중적 위상을 갖는다.</p>
<table><thead><tr><th><strong>차원</strong></th><th><strong>물리적 속성 (Physical Property)</strong></th><th><strong>현상학적 속성 (Phenomenal Quality)</strong></th><th><strong>어포던스 (Affordance)</strong></th></tr></thead><tbody>
<tr><td><strong>존재의 근거</strong></td><td>관찰자와 무관한 물질적 실체</td><td>관찰자의 내적 심리 상태</td><td><strong>행위자-환경 시스템 (Agent-Environment System)</strong></td></tr>
<tr><td><strong>측정 척도</strong></td><td>절대적 단위 (미터, 킬로그램 등)</td><td>주관적 척도 (좋음, 싫음 등)</td><td><strong>생태적 척도 (Body-scaled Metrics)</strong></td></tr>
<tr><td><strong>인식 기제</strong></td><td>추론 및 계산 (Inference)</td><td>내성 (Introspection)</td><td><strong>직접 지각 (Direct Perception)</strong></td></tr>
<tr><td><strong>로봇 공학적 함의</strong></td><td>CAD 모델, 절대 좌표계</td><td>사용자 선호도 모델링</td><td><strong>지각-행동 루프, 상호작용 가능성 탐지</strong></td></tr>
</tbody></table>
<h3>2.2  직접 지각(Direct Perception)과 생태학적 광학(Ecological Optics)</h3>
<p>깁슨의 이론에서 어포던스는 추론되는 것이 아니라 **직접 지각(Direct Perception)**된다. 그는 빛이 환경의 표면에서 반사되어 관찰자의 눈에 도달할 때 형성되는 <strong>‘광학적 배열(Optic Array)’</strong> 안에 행동에 필요한 정보가 이미 구조화되어 있다고 보았다.</p>
<ul>
<li><strong>광학적 흐름(Optic Flow):</strong> 관찰자가 이동할 때 망막상의 이미지는 변하지만, 그 변화 속에는 변하지 않는 수학적 불변량(Invariants)이 존재한다. 예를 들어, 착륙하는 비행기 조종사에게 활주로의 중심점은 팽창하는 흐름의 중심(Focus of Expansion)으로서 ‘충돌 가능성’ 혹은 ’착륙 가능성’을 직접적으로 지시한다.</li>
<li><strong>의미의 직접성:</strong> 로봇 공학적 관점에서 이는 센서 데이터(RGB-D 이미지 등)로부터 복잡한 3D 재구성이나 물체 인식(Object Recognition) 과정을 거치지 않고도, 곧바로 제어 신호(Control Signal)를 추출할 수 있음을 시사한다. 이는 현대의 ‘End-to-End Visuomotor Policy’ 학습의 이론적 선구라 할 수 있다.</li>
</ul>
<h3>2.3  환경과 동물의 상호보완성 (Complementarity)</h3>
<p>어포던스는 “동물과 환경의 상호보완성을 함축한다(implies the complementarity of the animal and the environment)”. 환경의 틈새(Niche)는 동물의 생활 방식(Way of life)을 규정하고, 역으로 동물의 신체 구조와 행동 양식은 환경 내에서 무엇이 어포던스가 되는지를 결정한다.</p>
<ul>
<li><strong>로봇 공학적 적용:</strong> 이는 범용 로봇(General Purpose Robot)을 설계할 때, 로봇의 하드웨어(Gripper 형태, 팔의 자유도)에 따라 지각 시스템이 탐지해야 할 어포던스가 달라져야 함을 의미한다. 진공 흡착 그리퍼를 장착한 로봇에게 ’평평한 표면’은 잡을 수 있는 어포던스이지만, 평행 그리퍼를 장착한 로봇에게는 잡을 수 없는 대상일 수 있다.</li>
</ul>
<h2>3.  어포던스의 존재론적 논쟁과 정립: 심리학에서 공학으로</h2>
<p>깁슨의 사후, 그의 이론을 어떻게 해석하고 형식화할 것인가를 두고 생태 심리학계 내부에서, 그리고 이를 수용하려는 인지과학 및 공학계에서 활발한 논쟁이 벌어졌다. 이 논쟁은 로봇의 지식 표현(Knowledge Representation) 방식을 결정하는 데 중요한 시사점을 제공한다.</p>
<h3>3.1  성향적 관점 (Dispositional Account): Turvey의 해석</h3>
<p>터비(Turvey, 1992)는 어포던스를 환경 내의 객체가 가진 **잠재적 성향(Disposition)**으로 해석했다.</p>
<ul>
<li><strong>정의:</strong> 성향(Disposition)은 적절한 조건(Effectivities, 행위자의 능력)이 충족될 때 발현되는 속성이다. 예를 들어, 소금의 ’용해성’은 물을 만나기 전에도 소금에 내재된 속성이듯, 어포던스는 동물이 있든 없든 환경에 실재한다.</li>
<li><strong>공학적 함의:</strong> 이 관점은 로봇이 환경에 진입하기 전에 환경 맵(Map)에 어포던스 정보를 미리 태깅(Tagging)하거나, 객체 데이터베이스에 기능적 속성을 저장해두는 방식(예: Semantic Map)과 연결된다.</li>
</ul>
<h3>3.2  관계적 관점 (Relational Account): Chemero의 해석</h3>
<p>케메로(Chemero, 2003)는 어포던스가 ’속성’이 아니라 **** 간의 <strong>관계(Relation)</strong> 그 자체라고 주장했다.</p>
<ul>
<li><strong>정의:</strong> <span class="math math-inline">Affordance = Relation(Ability, Situation)</span>. 어포던스는 고정된 속성이 아니며, 동적인 상황 속에서 창발한다. 예를 들어, 계단은 성인에게는 ‘오를 수 있는(climbable)’ 곳이지만, 기어 다니는 유아에게는 ‘기어오를 수 있는’ 곳이거나 혹은 장애물일 수 있다.</li>
<li><strong>공학적 함의:</strong> 이는 로봇이 자신의 현재 상태(배터리 잔량, 관절 가동 범위, 파지한 도구 등)에 따라 환경을 다르게 지각해야 함을 의미한다. 현대 로봇 공학, 특히 강화학습(RL) 기반 접근법은 이 관계적 관점을 강하게 지지한다.</li>
</ul>
<h3>3.3  노먼의 재해석: 지각된 어포던스 (Perceived Affordance)</h3>
<p>도널드 노먼(Donald Norman)은 HCI(Human-Computer Interaction) 관점에서 깁슨의 이론을 변용하여 <strong>‘지각된 어포던스’</strong> 개념을 제시했다. 그는 물리적 가능성(Real Affordance)과 사용자가 인지하는 가능성(Perceived Affordance)을 구분했다.</p>
<p>[표 1] Gaver의 어포던스 매트릭스에 기반한 노먼과 깁슨의 관점 통합</p>
<table><thead><tr><th><strong>구분</strong></th><th><strong>지각 정보 있음 (Perceptible Information)</strong></th><th><strong>지각 정보 없음 (No Perceptible Information)</strong></th></tr></thead><tbody>
<tr><td><strong>행동 가능함 (Action Possible)</strong></td><td><strong>지각된 어포던스 (Perceptible Affordance)</strong> (예: 손잡이가 달린 문)</td><td><strong>숨겨진 어포던스 (Hidden Affordance)</strong> (예: 눌러서 여는 비밀 문)</td></tr>
<tr><td><strong>행동 불가능함 (Action Impossible)</strong></td><td><strong>거짓 어포던스 (False Affordance)</strong> (예: 그려진 문 손잡이)</td><td><strong>올바른 거부 (Correct Rejection)</strong> (예: 벽)</td></tr>
</tbody></table>
<ul>
<li><strong>로봇 공학적 중요성:</strong> 로봇의 시각 시스템은 ’거짓 어포던스’에 속지 않아야 하며(예: 유리창을 통로로 착각하지 않음), ’숨겨진 어포던스’를 찾아내기 위해 능동적 탐색(Active Exploration)을 수행해야 한다.</li>
</ul>
<h2>4.  현대 인지과학과의 융합: 4E 인지와 능동적 추론</h2>
<h3>4.1  신체화된 인지(Embodied Cognition)와 어포던스</h3>
<p>4E 인지 과학은 인지가 뇌 속에 갇힌 연산이 아니라, 신체(Embodied)를 통해 환경에 착근(Embedded)하고, 행위를 통해(Enacted) 환경으로 확장(Extended)되는 과정이라고 본다. 여기서 어포던스는 인지 시스템이 환경과 결합하는 <strong>인터페이스</strong> 역할을 한다. 로봇 공학에서 이는 ’지각-판단-제어’의 직렬적 파이프라인 대신, 지각과 행동이 밀접하게 결합된 <strong>감각운동 협응(Sensorimotor Coordination)</strong> 구조를 채택해야 함을 시사한다.</p>
<h3>4.2  능동적 추론(Active Inference)과 자유 에너지 원리</h3>
<p>칼 프리스턴(Karl Friston)의 **자유 에너지 원리(Free Energy Principle)**와 **능동적 추론(Active Inference)**은 깁슨의 어포던스 이론을 확률론적, 신경과학적 기반 위에 통합한다.</p>
<ul>
<li>
<p><strong>기본 원리:</strong> 생물학적 에이전트는 생존을 위해 자신의 내부 모델(Generative Model)과 감각 입력 간의 차이, 즉 ‘놀라움(Surprise)’ 또는 ’변분 자유 에너지(Variational Free Energy)’를 최소화하려 한다.<br />
<span class="math math-display">
F \approx -\ln P(o|m) + D_{KL}[Q(s)||P(s|o)]
</span><br />
(여기서 <span class="math math-inline">F</span>는 자유 에너지, <span class="math math-inline">o</span>는 관측, <span class="math math-inline">m</span>은 모델, <span class="math math-inline">s</span>는 상태, <span class="math math-inline">Q</span>는 근사 사후 확률이다.)</p>
</li>
<li>
<p><strong>어포던스의 재해석:</strong> 능동적 추론에서 어포던스는 에이전트가 선호하는 결과(Prior Preferences, Goals)를 달성하기 위해 선택할 수 있는 **정책(Policy)에 대한 사전 믿음(Prior Beliefs)**으로 해석된다. 즉, “컵을 잡을 수 있다“는 어포던스는, 내가 컵을 잡는 행동을 했을 때 예상되는 감각 입력(손의 촉각, 시각적 겹침 등)이 나의 예측 모델과 일치할 확률이 높다는 것을 의미한다.</p>
</li>
<li>
<p><strong>행동 유발성:</strong> 깁슨이 말한 “행동을 유발하는 가치(Invitation)“는 수학적으로 **예상 자유 에너지(Expected Free Energy)**의 최소화 과정으로 설명된다. 로봇은 목표 상태(예: 물체를 잡은 상태)를 실현된 현실로 가정하고, 현재 감각과의 불일치(Prediction Error)를 해소하기 위해 반사적으로 행동을 생성한다.</p>
</li>
</ul>
<h2>5.  로봇 공학적 형식화(Formalization): 계산 가능한 모델로의 전환</h2>
<p>심리학적 개념인 어포던스를 알고리즘으로 구현하기 위해서는 엄밀한 수학적 형식화가 필요하다. 로봇 공학자들은 이를 위해 다양한 튜플(Tuple) 및 관계형 모델을 제안했다.</p>
<h3>5.1  어포던스의 3가지 관점 (Perspectives)</h3>
<p>Sahin et al. (2007)은 어포던스 연구를 세 가지 관점으로 분류했다.</p>
<ol>
<li><strong>에이전트 관점 (Agent Perspective):</strong> “나는 무엇을 할 수 있는가?” (예: 나의 그리퍼로 이 공을 잡을 수 있는가?)</li>
<li><strong>환경 관점 (Environmental Perspective):</strong> “이 환경은 무엇을 제공하는가?” (예: 이 바닥은 주행 가능한가?)</li>
<li><strong>관찰자 관점 (Observer Perspective):</strong> “저 에이전트가 저 객체와 상호작용하면 무슨 일이 일어나는가?” (예: 인간이 문을 여는 것을 보고 문 여는 법을 학습)</li>
</ol>
<h3>5.2  Sahin의 형식화: <code>(Effect, (Entity, Behavior))</code></h3>
<p>가장 널리 받아들여지는 형식화 중 하나는 Sahin이 제안한 모델이다.<br />
<span class="math math-display">
Affordance = (Effect, (Entity, Behavior))
</span></p>
<ul>
<li>
<p><strong>Entity (객체):</strong> 로봇의 센서로 지각된 환경의 부분 (예: 파란색 공).</p>
</li>
<li>
<p><strong>Behavior (행동):</strong> 로봇이 수행할 수 있는 운동 프리미티브 (예: Lift).</p>
</li>
<li>
<p><strong>Effect (효과):</strong> 행동 수행 결과로 발생하는 상태 변화 (예: 공이 바닥에서 떨어짐).</p>
</li>
</ul>
<p>이 형식화는 어포던스를 단순한 ’가능성’이 아니라, <strong>행동과 그 결과에 대한 예측 모델</strong>로 정의한다. 이는 로봇이 행동의 결과를 예측(Forward Model)하거나, 원하는 결과를 얻기 위해 행동을 계획(Inverse Model)하는 데 직접적으로 사용된다.</p>
<h3>5.3  기호 접지(Symbol Grounding)와 어포던스</h3>
<p>어포던스 학습은 로봇에게 “컵“이라는 기호가 단순한 텍스트가 아니라 <code>[Graspable, Pourable, Contain-able]</code>과 같은 행동 가능성의 집합으로 ’접지(Grounding)’되게 한다. 이는 로봇이 언어적 명령(예: “물 좀 줘”)을 수행할 때, 주변의 물체들 중에서 ‘물을 담을 수 있는(Contain-able)’ 어포던스를 가진 물체(컵)를 찾아내고, ‘잡을 수 있는(Graspable)’ 부위를 파악하여 행동하게 하는 인지적 연결고리를 제공한다.</p>
<h2>6.  딥러닝 기반의 시각적 어포던스 학습 (Visual Affordance Learning)</h2>
<p>전통적인 로봇 비전이 물체의 **범주(Category)**와 **포즈(Pose)**를 추정하는 데 집중했다면, 최신 딥러닝 기반 어포던스 학습은 픽셀 단위에서 **행동 가능성(Actionability)**을 예측하는 데 초점을 맞춘다.</p>
<h3>6.1  의미론적 분할 vs. 어포던스 분할: 기술적 차이와 손실 함수</h3>
<p>**어포던스 분할(Affordance Segmentation)**은 겉보기에 **의미론적 분할(Semantic Segmentation)**과 유사해 보이지만, 근본적인 차이점이 존재한다.</p>
<p><strong>[표 2] 의미론적 분할과 어포던스 분할의 비교 분석</strong></p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>의미론적 분할 (Semantic Segmentation)</strong></th><th><strong>어포던스 분할 (Affordance Segmentation)</strong></th></tr></thead><tbody>
<tr><td><strong>목표</strong></td><td>“이것은 무엇인가?” (What is it?)</td><td>“이것으로 무엇을 할 수 있는가?” (What can I do?)</td></tr>
<tr><td><strong>레이블 특성</strong></td><td>상호 배타적 (Mutually Exclusive) (한 픽셀은 ’책상’이면서 ’의자’일 수 없음)</td><td><strong>다중 레이블 (Multi-label)</strong> (한 픽셀이 ’잡을 수 있음’과 ’밀 수 있음’을 동시에 가질 수 있음)</td></tr>
<tr><td><strong>세분화 수준</strong></td><td>주로 객체 전체 (Whole Object)</td><td><strong>객체의 부분 (Object Parts)</strong> (칼날: 자르기 / 손잡이: 잡기)</td></tr>
<tr><td><strong>손실 함수</strong></td><td>Categorical Cross-Entropy (Softmax)</td><td><strong>Binary Cross-Entropy per Class (Sigmoid) / Multinomial Loss</strong></td></tr>
<tr><td><strong>일반화</strong></td><td>훈련된 클래스 내에서만 작동</td><td><strong>새로운 클래스(Unseen Class)에도 기능적 유사성으로 적용 가능</strong></td></tr>
</tbody></table>
<p>특히 손실 함수 설계에서, 어포던스 분할은 각 픽셀이 여러 기능을 동시에 가질 수 있음을 반영하기 위해, 클래스별로 독립적인 **이진 교차 엔트로피(Binary Cross-Entropy)**의 합을 사용하거나, AffordanceNet과 같이 다중 속성을 처리할 수 있는 특수한 **다항 손실(Multinomial Cross Entropy)**을 적용한다.</p>
<h3>6.2  사례 연구 1: AffordanceNet - 객체 감지와 기능성 인식의 통합</h3>
<p>Do et al. (2018)이 제안한 <strong>AffordanceNet</strong>은 딥러닝을 이용해 객체 감지와 어포던스 분할을 동시에 수행하는 선구적인 모델이다.</p>
<ul>
<li><strong>아키텍처:</strong> VGG16 기반의 백본(Backbone)과 RPN(Region Proposal Network)을 사용하여 객체의 위치(RoI)를 먼저 찾는다.</li>
<li><strong>RoIAlign의 활용:</strong> 기존의 RoIPool링이 가진 양자화 오차를 해결하기 위해 <strong>RoIAlign</strong>을 도입하여, 픽셀 단위의 정밀한 어포던스 마스크를 생성한다.</li>
<li><strong>성과:</strong> 이 모델은 로봇에게 단순한 Bounding Box를 넘어, 물체의 ’어느 부위’를 잡아야 하는지(Grasp), ’어느 부위’가 위험한지(Cut)를 픽셀 수준에서 알려줌으로써 정교한 조작을 가능하게 했다.</li>
</ul>
<h3>6.3  사례 연구 2: Where2Act - 상호작용을 통한 행동 가능성 발견</h3>
<p>스탠포드 대학의 Mo et al. (2021)이 제안한 <strong>Where2Act</strong>는 정적인 데이터셋 학습의 한계를 넘어, 시뮬레이션 상의 상호작용을 통해 어포던스를 학습하는 혁신적인 프레임워크다.</p>
<ul>
<li><strong>핵심 질문:</strong> “이 물체가 무엇인가?“가 아니라 **“어디를(Where) 어떻게(How) 조작하면 움직이는가?”**를 묻는다.</li>
<li><strong>네트워크 구조:</strong></li>
</ul>
<ol>
<li><strong>입력:</strong> 3D 포인트 클라우드 (Articulated Object).</li>
<li><strong>모듈 구성:</strong></li>
</ol>
<ul>
<li><strong>Actionability Module (<span class="math math-inline">D_a</span>):</strong> 각 픽셀(포인트)이 상호작용 가능한지 점수화.</li>
<li><strong>Proposal Module (<span class="math math-inline">D_r</span>):</strong> 해당 지점에서 가능한 6-DoF 그리퍼의 방향(Orientation)을 제안 (가우시안 노이즈 샘플링 기반의 생성 모델).</li>
<li><strong>Critic Module (<span class="math math-inline">D_s</span>):</strong> 제안된 행동의 성공 확률을 평가.</li>
<li><strong>학습 과정:</strong> SAPIEN 시뮬레이터에서 로봇이 수만 번의 무작위 상호작용(밀기, 당기기)을 시도하고, 물리적 변화(예: 문이 열림)가 발생하면 이를 성공 신호로 사용하여 **자기지도학습(Self-Supervised Learning)**을 수행한다.</li>
<li><strong>의의:</strong> 이 모델은 학습 과정에서 본 적 없는 새로운 범주(Unseen Category)의 물체에 대해서도, 기하학적 특징(틈새, 돌출부 등)에 기반해 손잡이를 찾아내고 문을 여는 놀라운 일반화 성능을 보여주었다. 이는 깁슨이 주장한 **‘불변량의 직접 지각’**이 신경망을 통해 구현된 강력한 사례다.</li>
</ul>
<h2>7.  대규모 데이터와 인간-로봇 지식 전이: 인터넷 비디오의 활용</h2>
<p>로봇이 직접 상호작용하며 데이터를 모으는 것은 시간이 많이 걸린다. 이를 극복하기 위해 최근 연구들은 인터넷상의 방대한 인간 행동 비디오(YouTube, Ego4D 등)를 어포던스 학습의 원천으로 삼고 있다.</p>
<h3>7.1  VRB (Vision-Robotics Bridge): 인간 비디오에서 로봇 행동으로</h3>
<p>CMU와 Meta AI가 제안한 <strong>VRB</strong>는 인간의 1인칭 비디오로부터 ’행동 가능한 정보’를 추출하여 로봇에게 전이하는 프레임워크다.</p>
<ul>
<li><strong>어포던스 표현의 추상화:</strong> 인간의 손과 로봇의 그리퍼는 형태(Morphology)가 다르다. 따라서 VRB는 손 자체를 학습하는 대신, **접촉 지점(Contact Point)**과 **접촉 후 궤적(Post-contact Trajectory)**이라는 <strong>에이전트 불가지론적(Agent-agnostic)</strong> 정보를 추출한다.</li>
<li><strong>학습 파이프라인:</strong></li>
</ul>
<ol>
<li>비디오에서 손이 물체와 접촉하는 순간을 탐지한다.</li>
<li>인간이 등장하기 전의 깨끗한 배경 프레임(Human-agnostic frame)에 접촉 히트맵과 궤적을 투영하여 학습시킨다.</li>
<li>이렇게 하면 로봇은 사람 없이 물체만 있는 상황에서도 “사람이 저기를 잡고 저렇게 움직였다“는 ‘유령(Ghost)’ 어포던스를 지각할 수 있게 된다.</li>
</ol>
<ul>
<li><strong>응용:</strong> VRB는 로봇이 처음 보는 환경에서도 의미 있는 탐색(Exploration)을 할 수 있도록 돕거나, 모방 학습(Imitation Learning)의 데이터 효율성을 극대화하는 데 사용된다.</li>
</ul>
<h3>7.2  Robo-ABC: 의미론적 대응과 제로샷 일반화</h3>
<p><strong>Robo-ABC</strong>는 ’본 적 없는 물체’를 다루는 로봇의 능력을 인간 수준의 직관으로 끌어올리려는 시도다.</p>
<ul>
<li><strong>핵심 아이디어: 의미론적 대응(Semantic Correspondence).</strong> 인간은 처음 보는 주전자를 마주쳐도, 기존에 알던 컵의 손잡이와 주전자의 손잡이가 ’같은 기능’을 한다는 것을 직관적으로 안다.</li>
<li><strong>기술적 구현:</strong></li>
<li><strong>어포던스 메모리(Affordance Memory):</strong> 인간 비디오에서 추출한 다양한 물체의 접촉 정보를 메모리에 저장한다.</li>
<li><strong>확산 특징(Diffusion Features - DIFT):</strong> Stable Diffusion과 같은 거대 생성 모델 내부의 특징 맵(Feature Map)은 시각적 외형이 달라도 의미적으로 유사한 부분(예: 고양이 다리와 강아지 다리)끼리 높은 코사인 유사도를 갖는 창발적(Emergent) 특성이 있다.</li>
<li><strong>작동 방식:</strong> 로봇이 새로운 물체를 보면, 1) 메모리에서 의미적으로 가장 유사한 물체를 검색하고, 2) DIFT를 이용해 메모리 속 물체의 접촉 지점을 현재 물체의 대응 지점으로 매핑(Mapping)한다.</li>
<li><strong>결과:</strong> 이를 통해 로봇은 별도의 추가 학습 없이도(Zero-shot), 낯선 물체의 올바른 부위를 잡아 조작할 수 있게 된다.</li>
</ul>
<h2>8.  결론 및 향후 전망: 행동하는 지능을 향하여</h2>
<p>본 장에서 살펴본 바와 같이, 깁슨의 어포던스 이론은 20세기 중반의 심리학적 가설에서 출발하여, 21세기 로봇 공학의 핵심적인 인지 아키텍처로 진화했다.</p>
<ol>
<li><strong>인식론적 전환의 완성:</strong> ’관계적 실재’로서의 어포던스 개념은 로봇을 관찰자(Observer)에서 참여자(Participant)로 재정의했다. 로봇은 이제 세상을 3차원 지도로 복원하려 애쓰는 대신, 자신의 신체와 환경이 빚어내는 행동 가능성의 춤(Dance of Affordance)을 직접 지각한다.</li>
<li><strong>기술적 융합과 진보:</strong> Sahin 등의 초기 형식화 노력은 모호했던 개념을 알고리즘의 영역으로 끌어들였고, Where2Act와 같은 딥러닝 모델은 이를 픽셀 단위의 정밀한 제어 신호로 변환해냈다. 나아가 VRB와 Robo-ABC는 인터넷상의 방대한 인간 경험을 로봇의 인지 자원으로 흡수하는 길을 열었다.</li>
<li><strong>향후 과제:</strong> 앞으로의 연구는 시각적 어포던스를 넘어, 언어 모델(LLM)과 결합된 <strong>언어-시각 어포던스(Language-Vision Affordance)</strong>, 그리고 로봇이 도구를 사용하여 새로운 어포던스를 창출하는 <strong>도구 사용(Tool Use)</strong> 및 <strong>창발적 어포던스(Emergent Affordance)</strong> 연구로 확장될 것이다.</li>
</ol>
<p>결론적으로, “지각은 행동을 위한 것이다(Perception is for Action)“라는 깁슨의 통찰은 현대 AI와 로봇 공학에서 **“데이터는 행동 가능성으로 변환될 때 비로소 지능이 된다”**는 공학적 원칙으로 재탄생하였다. 이는 미래의 범용 로봇이 인간과 공존하며 복잡한 환경에 적응하기 위한 가장 근본적이고 필수적인 토대가 될 것이다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>Embodied Affordance Grounding using Semantic Simulations and …, https://www.diva-portal.org/smash/get/diva2:1733092/FULLTEXT01.pdf</li>
<li>Affordance Equivalences in Robotics: A Formalism - Frontiers, https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2018.00026/full</li>
<li>Gibson’s affordances: A journey to the ontological no-man’s land., https://affordancetheory.medium.com/gibsons-affordances-a-journey-to-the-ontological-no-man-s-land-bfd88d07e9a7</li>
<li>Affordances in Psychology, Neuroscience, and Robotics, https://research.manchester.ac.uk/files/76585946/77602011.pdf</li>
<li>Gibson’s affordances and Turing’s theory of computation, https://eprints.lse.ac.uk/2606/1/Affordances_and_Computation_APA_style_(LSERO).pdf</li>
<li>Affordance Research in Developmental Robotics: A Survey, https://ieeexplore.ieee.org/iel7/7274989/7422051/07582380.pdf</li>
<li>An intentional analysis of Gibson’s ecological approach to visual …, https://cspeech.ucd.ie/Fred/docs/HeftAffordances.pdf</li>
<li>(PDF) Affordances as a Framework for Robot Control - ResearchGate, https://www.researchgate.net/publication/253040979_Affordances_as_a_Framework_for_Robot_Control</li>
<li>Building Affordance Relations for Robotic Agents - A Review - IJCAI, https://www.ijcai.org/proceedings/2021/0590.pdf</li>
<li>Affordances, Architecture and the Action Possibilities of Learning …, https://www.mdpi.com/2075-5309/12/1/76</li>
<li>The History and Philosophy of Ecological Psychology - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC6280920/</li>
<li>Inference of affordances and active motor control in simulated agents, https://pmc.ncbi.nlm.nih.gov/articles/PMC9405427/</li>
<li>The anatomy of choice: active inference and agency - Frontiers, https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2013.00598/full</li>
<li>Developing Intelligent Robots that Grasp Affordance - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC9294137/</li>
<li>The Active Inference Approach to Ecological Perception - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2018.00021/full</li>
<li>Perceptual and Semantic Processing in Cognitive Robots - MDPI, https://www.mdpi.com/2079-9292/10/18/2216</li>
<li>Learning to Segment Affordances - CVF Open Access, https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w14/Luddecke_Learning_to_Segment_ICCV_2017_paper.pdf</li>
<li>Context-based Affordance Segmentation from 2D Images for Robot …, https://eckerlab.org/pdf/affordance_segmentation_preprint.pdf</li>
<li>AffordanceNet - Computer Science, https://www.csc.liv.ac.uk/~anguyen/assets/pdfs/2018_ICRA_AffordanceNet.pdf</li>
<li>LEARNING COLLABORATIVE VISUAL AFFORDANCE FOR DUAL …, https://openreview.net/pdf/3d748072189f631dccc0d79e113d3909f32f78b5.pdf</li>
<li>Where2Act: From Pixels to Actions for Articulated 3D Objects …, https://openaccess.thecvf.com/content/ICCV2021/supplemental/Mo_Where2Act_From_Pixels_ICCV_2021_supplemental.pdf</li>
<li>Where2Act - Stanford Computer Science - Stanford University, https://cs.stanford.edu/~kaichun/where2act/</li>
<li>Where2Act: From Pixels to Actions for Articulated 3D Objects, https://geometry.stanford.edu/lgl_2024/papers/mgmgt-wfpaa3o-21/mgmgt-wfpaa3o-21.pdf</li>
<li>Affordances From Human Videos as a Versatile Representation for …, https://openaccess.thecvf.com/content/CVPR2023/papers/Bahl_Affordances_From_Human_Videos_as_a_Versatile_Representation_for_Robotics_CVPR_2023_paper.pdf</li>
<li>Affordances from Human Videos as a Versatile Representation for …, https://robo-affordances.github.io/</li>
<li>shikharbahl/vrb - GitHub, https://github.com/shikharbahl/vrb</li>
<li>Robo-ABC : Affordance Generalization Beyond Categories via …, https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05748.pdf</li>
<li>Robo-ABC: Affordance Generalization Beyond Categories via …, https://www.researchgate.net/publication/385893552_Robo-ABC_Affordance_Generalization_Beyond_Categories_via_Semantic_Correspondence_for_Robot_Manipulation</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>