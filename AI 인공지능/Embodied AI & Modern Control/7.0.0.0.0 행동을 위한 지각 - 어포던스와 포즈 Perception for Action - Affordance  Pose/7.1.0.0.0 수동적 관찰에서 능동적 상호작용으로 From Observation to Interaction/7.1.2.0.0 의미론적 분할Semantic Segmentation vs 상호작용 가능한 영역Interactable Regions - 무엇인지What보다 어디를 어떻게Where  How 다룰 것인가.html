<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.1.2 의미론적 분할(Semantic Segmentation) vs. 상호작용 가능한 영역(Interactable Regions): 무엇인지(What)보다 어디를 어떻게(Where & How) 다룰 것인가</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.1.2 의미론적 분할(Semantic Segmentation) vs. 상호작용 가능한 영역(Interactable Regions): 무엇인지(What)보다 어디를 어떻게(Where & How) 다룰 것인가</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <a href="index.html">7.1 수동적 관찰에서 능동적 상호작용으로 (From Observation to Interaction)</a> / <span>7.1.2 의미론적 분할(Semantic Segmentation) vs. 상호작용 가능한 영역(Interactable Regions): 무엇인지(What)보다 어디를 어떻게(Where & How) 다룰 것인가</span></nav>
                </div>
            </header>
            <article>
                <h1>7.1.2 의미론적 분할(Semantic Segmentation) vs. 상호작용 가능한 영역(Interactable Regions): 무엇인지(What)보다 어디를 어떻게(Where &amp; How) 다룰 것인가</h1>
<h2>1. 서론: 인지의 패러다임 전환과 로봇 조작의 본질</h2>
<p>현대 로봇 공학, 특히 자율 조작(Autonomous Manipulation) 분야에서 시각적 인지(Visual Perception) 시스템은 단순한 관찰자를 넘어 행위자(Actor)로서의 역할을 요구받고 있다. 지난 10년간 딥러닝의 비약적인 발전은 컴퓨터 비전 분야에서 **의미론적 분할(Semantic Segmentation)**의 황금기를 열었다. 이미지 내의 모든 픽셀에 ‘컵’, ‘의자’, ’책상’과 같은 의미론적 레이블(Semantic Label)을 부여하는 이 기술은 자율 주행이나 이미지 검색과 같은 분야에서는 혁명적인 변화를 가져왔다. 그러나 물리적 세계와 직접 부딪히며 물체를 조작해야 하는 로봇에게 있어, “이것은 무엇인가?(What is this?)“라는 질문에 대한 답은 문제 해결의 절반, 혹은 그 이하에 불과하다.</p>
<p>로봇이 실제 환경에서 유의미한 작업을 수행하기 위해서는 객체의 명칭을 넘어선, **상호작용 가능한 영역(Interactable Regions)**에 대한 이해가 필수적이다. 이는 “이 객체의 <strong>어디를(Where)</strong> 만져야 하며, <strong>어떻게(How)</strong> 힘을 가해야 기능적인 상호작용이 일어나는가?“라는 질문으로 귀결된다. 본 장에서는 의미론적 분할이 로봇 조작 태스크에서 갖는 한계를 심층적으로 분석하고, 이를 극복하기 위해 등장한 <strong>시각적 어포던스(Visual Affordance)</strong> 및 <strong>상호작용 영역 탐지(Interactable Region Detection)</strong> 기술을 상세히 기술한다. 특히 <strong>Where2Act</strong>, <strong>Contact-GraspNet</strong>, <strong>Interaction Hotspots</strong> 등 최신 연구 사례를 통해, 인지 시스템이 정적인 객체 레이블링에서 동적인 행동 가능성 예측으로 어떻게 진화하고 있는지, 그리고 이러한 기술적 진보가 로봇의 범용성(Generalization)과 자율성에 어떤 영향을 미치는지 논의한다.</p>
<h2>2.  의미론적 분할(Semantic Segmentation)의 공학적 효용과 한계</h2>
<h3>2.1  의미론적 분할의 정의와 현주소</h3>
<p>의미론적 분할은 입력 이미지의 각 픽셀을 사전에 정의된 클래스 집합(<span class="math math-inline">C = \{c_1, c_2,..., c_N\}</span>) 중 하나로 분류하는 밀집 예측(Dense Prediction) 과제이다. Mask2Former, DeepLab, U-Net과 같은 최신 아키텍처들은 객체의 경계를 매우 정교하게 구분해내며, 이는 로봇이 작업 공간 내의 객체 분포를 파악하는 데 기초적인 정보를 제공한다.</p>
<p>로봇 시스템에서 의미론적 분할은 주로 다음과 같은 역할을 수행해왔다:</p>
<ol>
<li><strong>객체 식별 및 위치 추정:</strong> 작업 공간에 있는 물체가 ’망치’인지 ’드라이버’인지 식별하여 작업 계획(Task Planning)의 기초 단계를 수립한다.</li>
<li><strong>장애물 회피:</strong> ‘테이블’, ‘벽’, ‘바닥’ 등 배경 요소를 분리하여 충돌 없는 경로를 생성한다.</li>
<li><strong>관심 영역(RoI) 필터링:</strong> 전체 이미지에서 조작 대상이 되는 물체의 픽셀만을 마스킹하여 후속 처리 알고리즘(예: 포즈 추정)의 연산 부하를 줄인다.</li>
</ol>
<h3>2.2  “무엇(What)“의 함정: 조작 관점에서의 구조적 결함</h3>
<p>그러나 ’이름을 아는 것’과 ’다룰 줄 아는 것’은 별개의 문제이다. 로봇 조작, 특히 파지(Grasping)나 도구 사용(Tool Use)과 같은 접촉 기반 작업(Contact-rich Manipulation)에서 의미론적 분할은 다음과 같은 구조적 한계에 봉착한다.</p>
<h4>2.2.1  기능적 모호성(Functional Ambiguity)과 어포던스의 부재</h4>
<p>의미론적 레이블은 인간의 언어적 분류 체계(Taxonomy)를 따른다. 그러나 이 분류 체계는 로봇의 물리적 상호작용 가능성을 보장하지 않는다. 예를 들어, 의미론적 분할 모델이 어떤 객체를 ’컵(Cup)’으로 완벽하게 분할했다고 가정해보자. 이 모델은 컵의 몸체, 손잡이, 바닥을 모두 ’컵’이라는 하나의 클래스로 묶거나, 기껏해야 ’손잡이(Handle)’라는 부품 레이블을 제공할 뿐이다. 하지만 로봇이 ‘물을 따르기 위해’ 컵을 잡아야 한다면, 컵의 윗부분(Rim)이나 바닥을 잡아서는 안 되며, 반드시 손잡이나 몸통의 무게중심을 잡아야 한다. 반면, ‘설거지를 하기 위해’ 컵을 뒤집어야 한다면 잡는 위치는 달라진다. 의미론적 레이블은 이러한 **태스크 의존적 기능성(Task-dependent Functionality)**을 내포하지 않는다. 의 연구에서 지적하듯, ’손잡이’라는 레이블이 붙은 영역이라도 장식용으로 얇게 제작되어 로봇 그리퍼의 악력을 견딜 수 없는 경우, 이는 의미론적으로는 정답이지만 물리적으로는 오답인 ’파지 불가 영역’이 된다.</p>
<h4>2.2.2  미세 객체 및 경계 처리의 실패 (Scale Imbalance)</h4>
<p>딥러닝 기반의 분할 모델은 일반적으로 이미지 내에서 넓은 면적을 차지하는 객체(Large Objects)에 편향되어 학습되는 경향이 있다. 이는 <strong>스케일 불균형(Scale Imbalance)</strong> 문제로 이어진다. 로봇 조작 환경에서 가장 중요한 상호작용 부위는 전선의 플러그, 컵의 얇은 테두리, 작은 나사못 등 매우 미세한 영역인 경우가 많다. 과 의 연구에 따르면, 기존의 의미론적 분할 모델이나 경량화된 모델들은 이러한 미세한 경계나 작은 부품을 배경과 혼동하거나, 해상도 저하로 인해 뭉개버리는(Smoothing) 현상을 보인다. 로봇이 컵의 손잡이와 같은 ’얇은 구조물(Thin Structures)’을 정확히 파지하기 위해서는 픽셀 단위의 정밀한 기하학적 정보가 필요하지만, 의미론적 손실 함수(Cross-Entropy Loss)는 대다수의 배경 픽셀에 의해 희석되어 이러한 미세 영역의 중요도를 간과하게 만든다.</p>
<h4>2.2.3  인스턴스 내 변산성(Intra-class Variance) 무시</h4>
<p>같은 ‘의자’ 클래스라도 사무용 의자, 안락의자, 스툴은 상호작용 방식이 전혀 다르다. 어떤 의자는 바퀴가 달려 있어 밀 수 있고(Pushable), 어떤 의자는 고정되어 있어 들어 올려야(Liftable) 한다. 의미론적 분할은 이들을 모두 ’Chair’로 라벨링함으로써, 개별 인스턴스가 가진 고유한 물리적 속성과 기하학적 제약을 제거해버린다. 의 연구에서는 바닥(Floor)으로 분류된 픽셀 중 실제로 로봇이 주행 가능한(Walkable) 영역은 79%에 불과하며, 나머지는 장애물이나 경사로 인해 이동이 불가능함을 보였다. 이는 의미론적 범주화가 물리적 행동 가능성을 과도하게 단순화함을 시사한다.</p>
<h2>3.  상호작용 가능한 영역(Interactable Regions): “어디서(Where)“의 재정의</h2>
<p>상호작용 가능한 영역 탐지는 객체의 사회적 명칭을 버리고, 로봇이 물리적인 힘을 가했을 때 의도한 변화를 일으킬 수 있는 **행동 유도성 표면(Affordance Landscape)**을 찾는 과정이다. 이는 제임스 깁슨(J.J. Gibson)의 생태학적 시각 이론을 공학적으로 구현한 것으로, **시각적 어포던스 분할(Visual Affordance Segmentation)**이라고도 불린다.</p>
<h3>3.1  의미론적 분할 vs. 어포던스 분할: 개념적 대조</h3>
<p>다음 표는 두 접근 방식의 근본적인 차이를 요약한 것이다.</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>의미론적 분할 (Semantic Segmentation)</strong></th><th><strong>어포던스 분할 (Affordance Segmentation)</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 질문</strong></td><td>“이것의 이름은 무엇인가?” (Labeling)</td><td>“이것으로 무엇을 할 수 있는가?” (Actionability)</td></tr>
<tr><td><strong>출력 단위</strong></td><td>객체 클래스 (예: 컵, 칼, 책)</td><td>행동 가능 영역 (예: 잡을 수 있는 곳, 자를 수 있는 날)</td></tr>
<tr><td><strong>기준</strong></td><td>인간의 언어적 분류 체계 (Taxonomy)</td><td>로봇/에이전트의 행동 능력 (Capability)</td></tr>
<tr><td><strong>일반화 범위</strong></td><td>훈련된 클래스(Closed-set)에 한정</td><td>유사한 기하학적 형상을 가진 미지의 물체(Open-set)로 확장 가능</td></tr>
<tr><td><strong>데이터 요구</strong></td><td>수동 어노테이션된 이미지 (고비용)</td><td>인간 시연 비디오, 시뮬레이션 상호작용 로그 (자기지도학습 가능)</td></tr>
<tr><td><strong>예시</strong></td><td>컵 전체를 ’Cup’으로 마스킹</td><td>컵 손잡이를 ‘Graspable’, 입구를 ’Pourable’로 마스킹</td></tr>
</tbody></table>
<p>이 표에서 알 수 있듯이, 어포던스 분할은 객체의 전체적인 형상보다는 **국소적인 기하학적 특징(Local Geometry)**에 주목한다. 컵의 전체 모양이 기이하더라도, 손잡이 부분의 곡률(Curvature)이 그리퍼가 감쌀 수 있는 형태라면 그곳은 ‘Graspable’ 영역이 된다.</p>
<h3>3.2  상호작용 영역의 다층적 구조와 우선순위</h3>
<p>상호작용 가능한 영역은 단일하지 않으며, 상황과 태스크에 따라 계층적으로 정의된다. 과 의 연구를 종합하면, 상호작용 영역은 다음과 같은 3단계 필터링을 거쳐 결정된다.</p>
<ol>
<li><strong>물리적 가능 영역 (Physically Feasible Region):</strong> 로봇의 운동학적(Kinematic) 제약 내에서 도달 가능하고, 충돌 없이 그리퍼를 위치시킬 수 있는 모든 표면. 이는 순수한 기하학적 연산(Collision Checking, Reachability Analysis)의 영역이다.</li>
<li><strong>기능적 유효 영역 (Functionally Effective Region):</strong> 물리적으로 잡을 수는 있지만, 해당 행동이 목적을 달성할 수 있는 영역. 예를 들어, 칼날을 잡으면 손이 베이거나 칼을 사용할 수 없으므로, 칼자루만이 기능적 유효 영역이 된다. AffordanceNet 과 같은 연구는 객체 내에서 이러한 다중 어포던스 클래스를 구분하여 분할한다.</li>
<li><strong>의미론적/사회적 제약 영역 (Semantically Constrained Region):</strong> 에서 제안하는 ‘의미론적 제약(Semantic Constraints)’ 개념이다. 예를 들어, 뜨거운 물이 담긴 컵을 옮길 때는 쏟지 않기 위해 컵의 상단을 잡아서는 안 되며, 전자기기 위로 물 컵을 옮겨서도 안 된다. 이러한 제약은 순수 기하학이나 어포던스를 넘어선 상위 레벨의 추론을 요구한다.</li>
</ol>
<h2>4.  실행 가능한 동작(Actionability): “어떻게(How)“의 구현</h2>
<p>“어디를(Where)” 찾아내는 것이 2D 이미지 평면이나 3D 공간상의 위치(<span class="math math-inline">x, y, z</span>)를 특정하는 것이라면, “어떻게(How)“는 그 위치에서 로봇이 수행해야 할 구체적인 동작 파라미터를 결정하는 것이다. 이를 **Actionability(실행 가능성)**라고 하며, 최신 연구들은 단순한 영역 마스킹을 넘어 픽셀 단위의 행동 파라미터 회귀(Regression)로 나아가고 있다.</p>
<h3>4.1  6-DoF 포즈와 접근 벡터 (Approach Vectors)</h3>
<p>로봇 팔(Manipulator)이 특정 지점 <span class="math math-inline">p</span>와 성공적으로 상호작용하기 위해서는 단순한 위치 좌표 이상의 정보, 즉 <strong>6자유도(6-DoF) 포즈</strong>가 필요하다. Contact-GraspNet 과 같은 연구는 각 상호작용 점에 대해 다음 요소들을 예측한다.</p>
<ul>
<li><strong>접촉점(Contact Point, <span class="math math-inline">c</span>):</strong> 그리퍼와 물체 표면이 만나는 정확한 3D 좌표.</li>
<li><strong>접근 벡터(Approach Vector, <span class="math math-inline">a</span>):</strong> 그리퍼가 물체 표면에 다가가는 방향. 일반적으로 표면 법선(Surface Normal)과 관련이 깊지만, 주변 장애물 회피를 위해 조정될 수 있다.</li>
<li><strong>대향 벡터(Opposition Vector / Baseline Vector, <span class="math math-inline">b</span>):</strong> 2핑거 그리퍼의 경우, 두 손가락이 닫히는 축의 방향. 이는 물체의 국소적 폭(Width)이나 형상에 따라 결정된다.</li>
<li><strong>그리퍼 파라미터:</strong> 그리퍼 개방 너비(Opening Width)와 파지력(Grasping Force).</li>
</ul>
<p><span class="math math-display">
Grasp = (c, a, b, w) \in \mathbb{R}^3 \times \mathbb{R}^3 \times \mathbb{R}^3 \times \mathbb{R}
</span></p>
<p>이러한 벡터들은 의미론적 레이블에는 전혀 포함되어 있지 않은 정보들이다. “손잡이“라는 단어는 “어느 방향에서 접근해야 하는가“를 알려주지 않지만, 어포던스 모델은 이를 수학적 벡터로 출력한다.</p>
<h3>4.2  성공 확률 예측 (Actionability Score)</h3>
<p>모든 기하학적으로 타당해 보이는 지점이 실제로 성공적인 조작을 보장하지는 않는다. 마찰 계수, 무게 중심, 보이지 않는 뒷면의 형상 등이 변수가 된다. <strong>Where2Act</strong>  연구는 각 픽셀마다 **상호작용 성공 확률(Actionability Score)**을 예측하는 네트워크를 제안한다. <span class="math math-display">S_{action}(p, \phi) \rightarrow </span> 여기서 <span class="math math-inline">p</span>는 픽셀, <span class="math math-inline">\phi</span>는 행동 파라미터이다. 이 점수가 높은 영역(Hotspot)만이 최종적으로 실행 단계(Execution)로 넘어간다. 이는 로봇이 불확실한 환경에서 무의미한 시도(Trial-and-Error)를 줄이고, 성공 확률이 가장 높은 동작을 우선적으로 계획하게 해준다.</p>
<h2>4. 심층 분석: 주요 연구 사례 및 방법론 (Methodological Deep Dive)</h2>
<p>이 섹션에서는 “What“에서 “Where &amp; How“로의 전환을 이끈 대표적인 최신 연구 방법론들을 상세히 분석한다. 각 연구는 데이터의 형태, 학습 방식, 그리고 출력의 차원에서 서로 다른 접근법을 취하고 있다.</p>
<h3>4.1 Where2Act: 픽셀에서 행동으로 (From Pixels to Actions)</h3>
<p>스탠포드 대학교 연구진(Mo et al., ICCV 2021)이 제안한 <strong>Where2Act</strong>는 관절이 있는 3D 객체(Articulated Objects, 예: 서랍, 문, 노트북)와의 상호작용에 초점을 맞춘다. 이 연구는 “로봇이 물체의 카테고리를 몰라도(Category-agnostic) 상호작용할 수 있는가?“라는 질문에 긍정적인 답을 제시한다.</p>
<ul>
<li>
<p><strong>핵심 철학 및 아키텍처:</strong></p>
<p>로봇은 ’냉장고’라는 개념을 이해하지 못해도, ’틈새’나 ’핸들’과 같은 국소적 기하학(Local Geometry) 정보만으로 문을 열 수 있다. Where2Act는 3D 포인트 클라우드 또는 깊이 이미지(Depth Image)를 입력받아 PointNet++ 백본을 통해 특징을 추출한다. 이후 세 가지 디코딩 브랜치(Branch)를 통해 결과를 출력한다.</p>
<ol>
<li><strong>Actionability Scoring Module:</strong> 각 포인트가 상호작용 가능한지 여부를 이진 분류 점수로 예측한다. (Where)</li>
<li><strong>Action Proposal Module:</strong> 해당 포인트에서 성공적인 상호작용을 일으키는 6-DoF 궤적 파라미터(예: 당기는 방향)를 제안한다. (How)</li>
<li><strong>Action Scoring Module:</strong> 제안된 파라미터로 행동했을 때의 성공 확률을 예측한다.</li>
</ol>
</li>
<li>
<p><strong>시뮬레이션 기반 학습과 적응형 샘플링:</strong> 이 모델을 학습시키기 위해 연구진은 SAPIEN 시뮬레이터 를 활용했다. 수천 개의 관절 객체(Articulated Objects)에 대해 로봇이 무작위로, 혹은 학습된 정책에 따라 상호작용(Pushing, Pulling)을 시도하고, 물리 엔진이 반환하는 결과(문이 열렸는가? 물체가 이동했는가?)를 정답 레이블로 사용한다. 특히 <strong>온라인 적응형 샘플링(Online Adaptive Sampling)</strong> 전략이 핵심이다. 초기에는 무작위 탐색으로 시작하지만, 학습이 진행될수록 모델이 ’가능성이 높다’고 예측한 영역을 집중적으로 탐색하여 긍정 샘플(Positive Sample)의 비율을 높인다. 이는 데이터 효율성을 극대화하며, 희소한 상호작용 영역(예: 작은 손잡이)을 효과적으로 학습하게 한다.</p>
</li>
</ul>
<h3>4.2 Contact-GraspNet: 6-DoF 파지 생성의 기하학</h3>
<p>NVIDIA와 워싱턴 대학교의 연구(Sundermeyer et al., ICRA 2021)인 <strong>Contact-GraspNet</strong>은 복잡한 클러터(Clutter) 환경에서 미지의 물체(Unknown Objects)를 잡는 문제에 대해 “형상 완성 없이 접촉만으로 충분하다“는 가설을 증명했다.</p>
<ul>
<li>
<p><strong>기존 방식과의 차별점:</strong></p>
<p>과거의 파지 연구들은 불완전한 깊이 이미지(Depth Image)로부터 물체의 전체 3D 형상을 복원(Shape Completion)하거나 3D Bounding Box를 추정한 뒤 파지 점을 찾으려 했다. 그러나 Contact-GraspNet은 관측된 포인트 클라우드 자체만을 입력으로 사용한다. “로봇 그리퍼가 닿는 그 지점의 국소적 곡률과 공간만이 중요하다“는 전제를 바탕으로 한다.</p>
</li>
<li>
<p><strong>네트워크 구조 및 손실 함수:</strong> PointNet++를 사용하여 씬(Scene) 전체의 포인트 클라우드(약 20,000개 포인트)를 인코딩한다. 각 포인트는 잠재적인 ‘접촉점(Contact Point)’ 후보가 된다. 네트워크 헤드는 각 포인트에 대해 파지 성공 확률(<span class="math math-inline">s</span>)과 파지 설정(Orientation <span class="math math-inline">r</span>, Width <span class="math math-inline">w</span>)을 회귀한다. 학습을 위한 손실 함수는 다음과 같이 구성된다 :<br />
<span class="math math-display">
L = \alpha L_{bce} + \beta L_{add-s} + \gamma L_{width}
</span></p>
<ul>
<li><span class="math math-inline">L_{bce}</span>: 파지 성공 여부에 대한 이진 교차 엔트로피(Binary Cross Entropy).</li>
<li><span class="math math-inline">L_{add-s}</span>: 대칭성을 고려한 파지 자세 거리 오차(Average Distance of Minimum Symmetry). 그리퍼는 180도 회전 대칭이므로, 이를 고려하여 불필요한 회전 오차 학습을 방지한다.</li>
<li><span class="math math-inline">L_{width}</span>: 그리퍼 너비에 대한 예측 오차.</li>
</ul>
</li>
<li>
<p><strong>성능과 시사점:</strong></p>
<p>이 모델은 훈련 데이터에 없던 새로운 물체들에 대해서도 90% 이상의 파지 성공률을 기록했다. 이는 의미론적 분류(“이것은 무엇인가”)를 완전히 배제하고, 철저히 기하학적 접촉(“어디를 잡을 수 있는가”)에 집중했을 때 오히려 범용성(Generalization)이 높아짐을 보여주는 강력한 사례이다.</p>
</li>
</ul>
<h3>4.3 Interaction Hotspots: 비디오로부터 배우는 인간의 행동</h3>
<p>UT Austin과 Facebook AI Research의 연구(Nagarajan et al., ICCV 2019)는 시뮬레이션이 아닌, <strong>실제 인간의 비디오 데이터</strong>를 통해 어포던스를 학습하는 방법을 제시한다.</p>
<ul>
<li>
<p><strong>문제 의식: 정적 이미지의 한계:</strong></p>
<p>이미지 속의 물체는 가만히 있다. “이 컵은 따를 수 있다(Pourable)“는 속성은 컵이 기울여지는 동적인 과정에서만 드러난다. 정적인 이미지에 수동으로 ’여기가 따르는 곳’이라고 라벨링하는 것은 주관적이며, 행동의 맥락을 담지 못한다.</p>
</li>
<li>
<p><strong>방법론: 약한 지도 학습(Weakly Supervised Learning)과 예측(Anticipation):</strong></p>
<p>연구진은 EPIC-Kitchens, OPRA와 같은 대규모 인간 행동 비디오 데이터셋을 활용했다. 훈련 시에는 픽셀 단위의 라벨 없이, 비디오 클립 전체에 대한 행동 라벨(예: “남자가 컵을 열고 있다”)만 주어진다.</p>
<p>모델은 비디오에서 ’손이 물체와 접촉하는 순간’과 ’물체가 움직이는 방식’을 관찰하여, 역으로 “아무도 만지지 않고 있는 정적 상태의 컵” 이미지에서 미래에 손이 닿을 영역(Hotspot)을 예측하도록 학습된다. 이를 <strong>Anticipation Module</strong>이라고 한다.</p>
</li>
<li>
<p><strong>결과 - Interaction Hotspots:</strong></p>
<p>결과물은 이미지 위에 겹쳐진 히트맵(Heatmap)이다. 이 히트맵은 단순히 ’손잡이’를 가리키는 것이 아니라, ’사람들이 실제로 잡는 부위’를 가리킨다. 예를 들어, 무거운 프라이팬의 경우 손잡이의 끝부분보다는 무게중심에 가까운 안쪽을 잡는 경향이 있는데, Hotspot 모델은 이러한 인간의 암묵적 지식(Tacit Knowledge)까지 학습하여 시각화한다. 이는 로봇에게 “사람처럼 행동하는 법“을 가르치는 중요한 단서가 된다.</p>
</li>
</ul>
<h3>4.4 AffordanceNet: 두 세계의 결합</h3>
<p><strong>AffordanceNet</strong> (Do et al., ICRA 2018)은 의미론적 분할의 강점(객체 탐지)과 어포던스 감지의 강점(기능 부위 인식)을 결합한 초기 대표 연구이다.</p>
<ul>
<li>
<p><strong>접근법: Object Detection + Affordance Segmentation:</strong></p>
<p>Mask R-CNN과 유사한 2단계(Two-stage) 접근법을 사용한다. 먼저 이미지에서 객체의 바운딩 박스를 찾고(Object Detection), 그 RoI(Region of Interest) 내부에서 픽셀 단위로 어포던스 클래스를 예측한다.</p>
</li>
<li>
<p><strong>다중 라벨링:</strong> 하나의 객체에 여러 어포던스가 공존함을 명시적으로 다룬다. 컵은 <code>contain</code> (내부)과 <code>grasp</code> (손잡이) 라벨을 동시에 갖는다.</p>
</li>
<li>
<p><strong>의의:</strong> 기하학적 정보만으로는 알기 힘든 ’기능적 의미’를 학습한다는 점에서 Contact-GraspNet과 상호 보완적이다. 예를 들어, 칼날 부분은 기하학적으로는 얇아서 잡기 좋지만(Graspable), AffordanceNet은 학습 데이터를 통해 그곳이 <code>cut</code> 기능을 하는 곳이지 <code>grasp</code> 하는 곳이 아님을 구분할 수 있다.</p>
</li>
</ul>
<h2>5. 최신 연구 동향 (2024-2025): 개방형 어휘와 파운데이션 모델의 융합</h2>
<p>“Where &amp; How“의 패러다임은 2024년과 2025년을 기점으로 거대 언어 모델(LLM) 및 비전-언어 모델(VLM)과 결합하여 <strong>개방형 어휘(Open-Vocabulary) 어포던스</strong>라는 새로운 차원으로 확장되고 있다.</p>
<h3>5.1 CLIP과 SAM을 활용한 언어 기반 그라운딩 (Language-guided Grounding)</h3>
<p>기존 어포던스 모델은 ‘잡기’, ‘밀기’ 등 사전에 정의된 소수의 행동만 학습했다. 그러나 로봇이 가정 환경과 같은 비정형 공간에서 작동하려면 무한한 종류의 지시를 이해해야 한다. 의 **OVA-Fields (Open-Vocabulary Affordance Fields)**와 의 텍스트 가이드 파지 연구는 CLIP(Contrastive Language-Image Pre-training)과 SAM(Segment Anything Model)을 활용한다. 사용자가 “바나나 껍질을 벗기기 위해 잡아야 할 곳(Where to grasp to peel)“이라고 자연어로 명령하면, VLM은 텍스트 임베딩과 이미지 특징을 대조하여 의미론적으로 가장 적합한 영역을 찾아낸다. 이후 SAM이 해당 영역의 정밀한 마스크를 생성하고, Contact-GraspNet이 최종적인 6-DoF 포즈를 계산하는 파이프라인이 주류로 자리 잡고 있다. 이러한 접근은 “무엇(What)“에 해당하는 언어적 이해가 “어디를(Where)“에 해당하는 공간적 탐색을 가이드하는 형태로, 의미론적 분할과 어포던스 탐지의 이상적인 결합을 보여준다.</p>
<h3>5.2 생성형 AI와 어포던스 상상 (Generative Affordance)</h3>
<p>확산 모델(Diffusion Model)의 등장으로 로봇은 이제 데이터를 단순히 분류하는 것을 넘어, 상호작용을 **생성(Generate)**하고 **상상(Imagine)**하는 단계에 진입했다. 의 <strong>Affordance Diffusion</strong> 연구는 객체 이미지가 주어졌을 때, 인간의 손이나 로봇 그리퍼가 그 객체와 상호작용하는 이미지를 생성해낸다. 이 생성된 이미지는 역으로 로봇에게 “어떻게 접근해야 하는가(How)“에 대한 강력한 시각적 가이드(Visual Planning)로 작용한다. 데이터가 부족한 희귀 물체에 대해서도 로봇이 생성형 모델을 통해 상호작용 방식을 ’환각(Hallucinate)’하여 해결책을 찾아내는 것이다.</p>
<h2>6. 사례 연구 및 비교 분석 (Comparative Analysis)</h2>
<p>다음 표는 동일한 객체에 대해 의미론적 분할과 상호작용 영역 예측 모델이 어떻게 다른 출력을 생성하는지 구체적으로 비교한 것이다.</p>
<h3>표 7.1.2-1: 의미론적 분할 vs. 상호작용 영역 예측 출력 비교</h3>
<table><thead><tr><th><strong>입력 객체</strong></th><th><strong>시나리오/태스크</strong></th><th><strong>의미론적 분할 출력 (What)</strong></th><th><strong>상호작용 영역/어포던스 출력 (Where &amp; How)</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>주전자 (Teapot)</strong></td><td>물 따르기</td><td><strong>Class:</strong> Teapot   <strong>Mask:</strong> 주전자 전체 픽셀 (손잡이, 주둥이 포함)</td><td><strong>Action:</strong> Grasping for Pouring   <strong>Region:</strong> 손잡이 표면 (Heatmap)   <strong>Vector:</strong> 수직 접근, 45도 틸팅 허용   <strong>Constraint:</strong> 주둥이와 뚜껑은 파지 금지 영역으로 설정</td><td>의미론적 분할은 주전자를 인식하지만, 뜨거운 몸통을 잡는 오류를 범할 수 있음. 어포던스 모델은 ‘따르기’ 기능을 위해 손잡이를 특정함.</td></tr>
<tr><td><strong>가위 (Scissors)</strong></td><td>건네주기 (Hand-over)</td><td><strong>Class:</strong> Scissors   <strong>Mask:</strong> 가위 전체</td><td><strong>Action:</strong> Safe Grasping   <strong>Region:</strong> 손잡이 고리(Ring) 부분   <strong>Avoid:</strong> 칼날(Blade) 부분 (Negative Affordance)</td><td>사회적 맥락(안전)이 반영된 어포던스. 날 부분은 기하학적으로 얇아 잡기 쉽지만(Graspable), 의미론적으로 금지됨.</td></tr>
<tr><td><strong>전원 플러그</strong></td><td>콘센트 꽂기</td><td><strong>Class:</strong> Cable / Wire   (작아서 배경으로 무시될 가능성 높음 )</td><td><strong>Action:</strong> Insert   <strong>Region:</strong> 플러그 헤드(Head)의 평면   <strong>Pose:</strong> 핀 방향과 일치하는 6-DoF 정밀 포즈</td><td>스케일 불균형 문제의 대표 사례. 어포던스 모델은 미세한 ‘헤드’ 부분의 기하학적 특징(평면)을 찾아냄.</td></tr>
<tr><td><strong>문 (Door)</strong></td><td>문 열기</td><td><strong>Class:</strong> Door</td><td><strong>Action:</strong> Pull / Push   <strong>Region:</strong> 문 손잡이(Handle) 주변   <strong>Trajectory:</strong> 손잡이를 축으로 하는 호(Arc) 운동 경로</td><td>Where2Act  사례. 문 전체가 아닌, 상호작용을 유발하는 국소 부위(트리거)에 집중.</td></tr>
</tbody></table>
<h2>7. 결론: 통합된 인지 시스템을 향하여</h2>
<p>본 장의 논의를 종합하면, 로봇 조작을 위한 시각 지능은 “이것은 무엇인가(Classification)“라는 수동적 관찰에서 “이것으로 무엇을 할 수 있는가(Affordance)“라는 능동적 행동 예측으로 진화하고 있음을 알 수 있다. 의미론적 분할과 상호작용 영역 탐지는 대립적인 기술이 아니라, 상호 보완적인 관계에 있다.</p>
<ol>
<li><strong>의미론적 분할은 문맥(Context)을 제공한다:</strong> 이는 로봇에게 “이것이 칼이다“라는 사실을 알려줌으로써, 기하학적으로는 잡기 좋아 보일지라도 날 부분을 잡으면 안 된다는 상위 레벨의 판단 근거를 제공한다.</li>
<li><strong>상호작용 영역 탐지는 실천(Action)을 가능케 한다:</strong> 이는 추상적인 명칭을 구체적인 <span class="math math-inline">xyz</span> 좌표와 힘 벡터로 변환하여, 물리 세계에서의 변화를 이끌어낸다. Contact-GraspNet과 Where2Act는 의미론적 정보 없이도 강력한 조작이 가능함을 증명했으나, 복잡한 사회적 규칙이 얽힌 환경에서는 한계가 있다.</li>
</ol>
<p>따라서 미래의 로봇 인지 시스템은 <strong>“의미론적으로 이해하고(Semantically Understand), 기하학적으로 실행하는(Geometrically Act)”</strong> 하이브리드 구조로 나아가야 한다. 최근의 VLM과 Open-Vocabulary 연구들은 이러한 통합이 가속화되고 있음을 보여준다. 개발자는 자신의 로봇이 수행할 태스크가 단순한 분류인지, 아니면 정교한 물리적 조작인지에 따라 “What“과 “Where &amp; How“의 비중을 적절히 조절하여 인지 파이프라인을 설계해야 할 것이다. 이것이 진정한 의미의 **체화된 지능(Embodied AI)**을 구현하는 길이다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>Segmenting Object Affordances: Reproducibility and Sensitivity to …, 1월 28, 2026에 액세스, https://arxiv.org/html/2409.01814v1</li>
<li>Where2Act: From Pixels to Actions for Articulated 3D Objects, 1월 28, 2026에 액세스, https://geometry.stanford.edu/lgl_2024/papers/mgmgt-wfpaa3o-21/mgmgt-wfpaa3o-21.pdf</li>
<li>Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered …, 1월 28, 2026에 액세스, https://www.researchgate.net/publication/355434800_Contact-GraspNet_Efficient_6-DoF_Grasp_Generation_in_Cluttered_Scenes</li>
<li>Mobile-Seed: Joint Semantic Segmentation and Boundary Detection …, 1월 28, 2026에 액세스, https://arxiv.org/html/2311.12651v3</li>
<li>(PDF) FineGrasp: Towards Robust Grasping for Delicate Objects, 1월 28, 2026에 액세스, https://www.researchgate.net/publication/393512480_FineGrasp_Towards_Robust_Grasping_for_Delicate_Objects</li>
<li>A Multi-Scale CNN for Affordance Segmentation in RGB Images, 1월 28, 2026에 액세스, https://web.engr.oregonstate.edu/~sinisa/research/publications/eccv16_affordance.pdf</li>
<li>Affordance Segmentation of Hand-Occluded Containers from …, 1월 28, 2026에 액세스, https://openaccess.thecvf.com/content/ICCV2023W/ACVR/papers/Apicella_Affordance_Segmentation_of_Hand-Occluded_Containers_from_Exocentric_Images_ICCVW_2023_paper.pdf</li>
<li>Semantically Safe Robot Manipulation - OpenReview, 1월 28, 2026에 액세스, https://openreview.net/pdf/383695475801189fa3ad4a80f07f6abf51a49585.pdf</li>
<li>Visual Affordance Prediction: Survey and Reproducibility - arXiv, 1월 28, 2026에 액세스, https://arxiv.org/html/2505.05074</li>
<li>AffordanceNet - Computer Science, 1월 28, 2026에 액세스, https://www.csc.liv.ac.uk/~anguyen/assets/pdfs/2018_ICRA_AffordanceNet.pdf</li>
<li>Contact-graspnet: Efficient 6-dof grasp generation in cluttered scenes, 1월 28, 2026에 액세스, https://elib.dlr.de/145798/1/Contact-GraspNet.pdf</li>
<li>Where2Act: From Pixels to Actions for Articulated 3D Objects, 1월 28, 2026에 액세스, https://www.researchgate.net/publication/348321194_Where2Act_From_Pixels_to_Actions_for_Articulated_3D_Objects</li>
<li>Language-guided Object Picking for Robots, 1월 28, 2026에 액세스, https://lup.lub.lu.se/student-papers/record/9161648/file/9161682.pdf</li>
<li>Efficient 6-DoF Grasp Generation in Cluttered Scenes, 1월 28, 2026에 액세스, https://www.semanticscholar.org/paper/Contact-GraspNet%3A-Efficient-6-DoF-Grasp-Generation-Sundermeyer-Mousavian/d85d16b003955c6996fafacea7f3c075c531225f</li>
<li>Joint Hand Motion and Interaction Hotspots Prediction From …, 1월 28, 2026에 액세스, https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Joint_Hand_Motion_and_Interaction_Hotspots_Prediction_From_Egocentric_Videos_CVPR_2022_paper.pdf</li>
<li>PEAR: phrase-based hand-object interaction anticipation, 1월 28, 2026에 액세스, http://scis.scichina.com/en/2025/150209.pdf</li>
<li>Grounded Human-Object Interaction Hotspots From Video, 1월 28, 2026에 액세스, https://www.cs.utexas.edu/~grauman/papers/hotspots-iccv2019.pdf</li>
<li>[PDF] Grounded Human-Object Interaction Hotspots From Video, 1월 28, 2026에 액세스, https://www.semanticscholar.org/paper/Grounded-Human-Object-Interaction-Hotspots-From-Nagarajan-Feichtenhofer/316a16485bf9ad67a6a07888f8e0d24604d96b76</li>
<li>OVA-Fields: Weakly Supervised Open-Vocabulary Affordance Fields …, 1월 28, 2026에 액세스, https://openaccess.thecvf.com/content/ICCV2025/papers/Su_OVA-Fields_Weakly_Supervised_Open-Vocabulary_Affordance_Fields_for_Robot_Operational_Part_ICCV_2025_paper.pdf</li>
<li>Affordance Diffusion: Synthesizing Hand-Object Interactions, 1월 28, 2026에 액세스, https://www.researchgate.net/publication/373313200_Affordance_Diffusion_Synthesizing_Hand-Object_Interactions</li>
<li>Learning to Segment Affordances - CVF Open Access, 1월 28, 2026에 액세스, https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w14/Luddecke_Learning_to_Segment_ICCV_2017_paper.pdf</li>
<li>Where2Act: From Pixels to Actions for Articulated 3D Objects, 1월 28, 2026에 액세스, https://www.researchgate.net/publication/358994417_Where2Act_From_Pixels_to_Actions_for_Articulated_3D_Objects</li>
<li>3D Understanding and Zero-Shot Task-Oriented Grasping via …, 1월 28, 2026에 액세스, https://www.ri.cmu.edu/app/uploads/2025/05/MSR_thesis_samuel_li.pdf</li>
<li>What does CLIP know about peeling a banana? - Semantic Scholar, 1월 28, 2026에 액세스, https://www.semanticscholar.org/paper/What-does-CLIP-know-about-peeling-a-banana-Cuttano-Rosi/abbd3a2dcbc0c40b125a8cdaed836556e483842d</li>
<li>Understanding 3D Object Interaction from a Single Image, 1월 28, 2026에 액세스, https://www.researchgate.net/publication/377430395_Understanding_3D_Object_Interaction_from_a_Single_Image</li>
<li>One-Shot Open Affordance Learning with Foundation Models, 1월 28, 2026에 액세스, https://www.researchgate.net/publication/384169480_One-Shot_Open_Affordance_Learning_with_Foundation_Models</li>
<li>PASG: A Closed-Loop Framework for Automated Geometric … - arXiv, 1월 28, 2026에 액세스, https://arxiv.org/html/2508.05976v1</li>
<li>Detecting Transitions from Stability to Instability in Robotic Grasping …, 1월 28, 2026에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC11314830/</li>
<li>Learning of 2D Grasping Strategies from Box-Based 3D Object …, 1월 28, 2026에 액세스, https://www.roboticsproceedings.org/rss05/p2.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>