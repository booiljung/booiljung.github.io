<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.2.1 인스턴스 레벨(Instance-Level) 포즈 추정: 키포인트(Keypoint) 기반 방식과 PVNet</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.2.1 인스턴스 레벨(Instance-Level) 포즈 추정: 키포인트(Keypoint) 기반 방식과 PVNet</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <a href="index.html">7.2 6D 물체 포즈 추정의 진화 (Evolution of 6D Object Pose Estimation)</a> / <span>7.2.1 인스턴스 레벨(Instance-Level) 포즈 추정: 키포인트(Keypoint) 기반 방식과 PVNet</span></nav>
                </div>
            </header>
            <article>
                <h1>7.2.1 인스턴스 레벨(Instance-Level) 포즈 추정: 키포인트(Keypoint) 기반 방식과 PVNet</h1>
<h2>1.  서론: 6DoF 포즈 추정의 진화와 키포인트 방식의 대두</h2>
<p>컴퓨터 비전과 로보틱스 분야에서 3차원 객체의 6자유도(6DoF: 6 Degrees of Freedom) 포즈를 정밀하게 추정하는 기술은 기계가 세상을 이해하고 물리적으로 상호작용하는 데 있어 필수 불가결한 요소로 자리 잡았다. 6DoF 포즈 추정은 임의의 3차원 공간에 존재하는 객체 좌표계(Object Coordinate System)에서 카메라 좌표계(Camera Coordinate System)로의 강체 변환(Rigid Transformation)을 정의하는 3차원 회전(Rotation, <span class="math math-inline">\mathbf{R} \in SO(3)</span>)과 3차원 이동(Translation, <span class="math math-inline">\mathbf{t} \in \mathbb{R}^3</span>)을 규명하는 문제로 정의된다. 이는 로봇 팔이 물체를 정확히 파지(Grasping)하거나, 증강 현실(AR) 환경에서 가상의 콘텐츠를 실물 객체 위에 정합하고, 자율 주행 차량이 도로 위의 장애물을 3차원적으로 인식하는 등 다양한 응용 분야의 기반 기술이 된다.</p>
<p>딥러닝 기술이 도입되기 이전, 포즈 추정은 주로 SIFT, SURF와 같은 핸드크래프트(Hand-crafted) 지역 특징점(Local Features)을 추출하고, 이를 3차원 모델의 점들과 매칭한 뒤 PnP(Perspective-n-Point) 알고리즘을 통해 포즈를 역산하는 방식에 의존했다. 이러한 방식은 텍스처가 풍부한 객체에 대해서는 비교적 정확한 성능을 보였으나, 산업용 부품이나 가정용 식기류와 같이 텍스처가 부족한(Texture-less) 객체, 또는 조명 변화가 심하거나 객체 간 가림(Occlusion)이 발생하는 복잡한 환경에서는 특징점 매칭이 실패하여 포즈 추정이 불가능해지는 근본적인 한계를 지니고 있었다.</p>
<p>합성곱 신경망(CNN)의 등장은 이러한 한계를 극복하는 새로운 돌파구를 제시했다. 초기 딥러닝 기반 접근법은 입력 이미지로부터 6DoF 파라미터를 직접 회귀(Direct Regression)하는 방식을 취했다. 대표적으로 PoseCNN은 CNN을 통해 객체의 중심점과 회전 쿼터니언(Quaternion)을 직접 예측하려 시도했다. 그러나 3차원 회전 공간의 비선형성(Non-linearity)으로 인해 이미지 픽셀 공간과 6D 포즈 공간 사이의 매핑을 학습하는 것은 매우 난해하며, 일반화 성능을 확보하기 어렵다는 단점이 드러났다.</p>
<p>이에 대한 대안으로 부상한 것이 바로 <strong>키포인트 기반(Keypoint-based) 방식</strong>이다. 이 접근법은 문제를 두 단계로 분할한다. 첫 번째 단계에서는 딥러닝 모델이 객체의 3차원 형상을 대표하는 2D 키포인트(예: 3차원 바운딩 박스의 8개 모서리 투영점)를 탐지하고, 두 번째 단계에서는 탐지된 2D 키포인트와 미리 알고 있는 3D 키포인트 간의 대응 관계(2D-3D Correspondences)를 이용하여 PnP 알고리즘으로 포즈를 계산한다. 키포인트 기반 방식은 기하학적 제약 조건(Geometric Constraints)을 명시적으로 활용한다는 점에서 직접 회귀 방식보다 높은 정확도를 보였으며, BB8, YOLO-6D 등의 연구를 통해 그 유효성이 입증되었다.</p>
<p>하지만 초기 키포인트 방식들 역시 여전히 한계를 안고 있었다. 대부분의 방식이 키포인트의 2D 좌표를 직접 회귀(Coordinate Regression)하거나 히트맵(Heatmap)을 통해 예측했기 때문에, 키포인트가 다른 물체에 의해 가려지거나(Occluded) 이미지 화면 밖으로 잘려나간(Truncated) 경우, 해당 지점의 시각적 특징이 소실되어 정확한 위치를 예측할 수 없었다.</p>
<p>본 장에서는 이러한 기존 키포인트 기반 방식의 한계를 근본적으로 해결하기 위해 제안된 **PVNet(Pixel-wise Voting Network)**을 심도 있게 분석한다. PVNet은 키포인트의 절대 좌표를 예측하는 대신, 객체 영역 내의 모든 픽셀이 키포인트의 방향을 가리키는 벡터 필드(Vector Field)를 학습함으로써, ’보이지 않는 부분을 보이는 부분으로부터 추론’하는 밀집 예측(Dense Prediction)의 힘을 보여주었다. 이 혁신적인 접근법은 가림과 절단에 매우 강건한 성능을 보이며 인스턴스 레벨 포즈 추정의 새로운 패러다임을 열었다.</p>
<h2>2.  키포인트 기반 포즈 추정의 이론적 프레임워크</h2>
<h3>2.1  문제 정의 및 PnP 알고리즘의 역할</h3>
<p>키포인트 기반 포즈 추정의 핵심은 3차원 공간상에 미리 정의된 객체의 특징점(Control Points) 집합 <span class="math math-inline">\{\mathbf{X}_k \in \mathbb{R}^3\}_{k=1}^K</span>과 이에 대응하는 이미지 평면상의 2D 투영점 집합 <span class="math math-inline">\{\mathbf{x}_k \in \mathbb{R}^2\}_{k=1}^K</span> 사이의 관계를 규명하는 것이다. 여기서 <span class="math math-inline">K</span>는 키포인트의 개수를 의미하며, 일반적으로 8개 이상의 점이 사용된다. 이들의 투영 관계는 핀홀 카메라 모델에 의해 다음과 같은 수식으로 정의된다.<br />
<span class="math math-display">
s \tilde{\mathbf{x}}_k = \mathbf{K} (\mathbf{R} \mathbf{X}_k + \mathbf{t})
</span><br />
여기서 <span class="math math-inline">\tilde{\mathbf{x}}_k</span>는 2D 좌표 <span class="math math-inline">\mathbf{x}_k</span>의 동차 좌표(Homogeneous Coordinate) 표현인 <span class="math math-inline">[u, v, 1]^T</span>이고, <span class="math math-inline">\mathbf{K}</span>는 카메라의 초점 거리와 주점을 포함하는 내부 파라미터 행렬(Intrinsic Matrix), <span class="math math-inline">s</span>는 스케일 팩터(깊이 정보)이다. 딥러닝 네트워크는 입력 이미지 <span class="math math-inline">\mathbf{I}</span>로부터 2D 좌표 <span class="math math-inline">{\mathbf{x}_k}</span>를 추정하는 함수 <span class="math math-inline">f(\mathbf{I})</span>의 역할을 수행한다.</p>
<p>이렇게 얻어진 2D-3D 대응쌍 <span class="math math-inline">\{(\mathbf{x}_k, \mathbf{X}_k)\}</span> 집합을 이용하여 회전 행렬 <span class="math math-inline">\mathbf{R}</span>과 이동 벡터 <span class="math math-inline">\mathbf{t}</span>를 구하는 최적화 문제를 <strong>PnP(Perspective-n-Point) 문제</strong>라고 한다. PnP 문제는 다음의 재투영 오차(Reprojection Error)를 최소화하는 해를 찾는 것으로 귀결된다.<br />
<span class="math math-display">
\min_{\mathbf{R}, \mathbf{t}} \sum_{k=1}^{K} | | \mathbf{x}_k - \pi(\mathbf{R} \mathbf{X}_k + \mathbf{t}) ||^2
</span><br />
여기서 <span class="math math-inline">\pi(\cdot)</span>는 3D 점을 2D 이미지 평면으로 투영하는 함수이다. 전통적으로 EPnP(Efficient PnP) 알고리즘이나 DLT(Direct Linear Transformation)와 같은 알고리즘이 초기 해를 구하는 데 사용되며, 이후 Levenberg-Marquardt(LM) 알고리즘과 같은 비선형 최적화 기법을 통해 정밀도를 높이는(Refinement) 과정을 거친다.</p>
<h3>2.2  키포인트 선택 전략: 바운딩 박스 대 표면 점</h3>
<p>키포인트 기반 방식에서 “어떤 점을 키포인트로 선택할 것인가“는 성능에 지대한 영향을 미친다.</p>
<ul>
<li><strong>3D 바운딩 박스 모서리 (Bounding Box Corners)</strong>: BB8, YOLO-6D와 같은 초기 연구들은 3D 바운딩 박스의 8개 꼭짓점을 키포인트로 정의했다. 이 방식은 객체의 전체적인 크기와 방향을 포괄할 수 있다는 장점이 있으나, 꼭짓점이 실제 객체의 표면이 아닌 허공에 위치하는 경우가 많다는 치명적인 단점이 있다. 텍스처나 시각적 특징이 없는 허공의 좌표를 이미지 픽셀 정보만으로 추정하는 것은 네트워크 학습을 어렵게 하고 위치 오차를 증가시키는 원인이 된다.</li>
<li><strong>최원점 샘플링 (Farthest Point Sampling, FPS)</strong>: PVNet 연구진은 이러한 문제를 해결하기 위해 FPS 알고리즘을 사용하여 객체 표면(Mesh Surface) 상의 점들을 키포인트로 선택할 것을 제안했다. FPS는 서로 가장 멀리 떨어져 있는 점들을 순차적으로 선택하여 객체의 형상을 고르게 커버하도록 한다. 실험 결과, 객체 표면상의 키포인트를 사용할 때 분산이 훨씬 작게 나타났으며, 이는 픽셀들이 자신의 위치와 가까운 표면상의 점을 예측하는 것이 기하학적으로 더 용이하고 시각적 증거(Visual Evidence)가 명확하기 때문이다.</li>
</ul>
<h3>2.3  희소 회귀(Sparse Regression) 방식의 한계와 도전 과제</h3>
<p>BB8이나 YOLO-6D와 같은 방식은 CNN의 마지막 레이어에서 키포인트의 <span class="math math-inline">(u, v)</span> 좌표를 직접 출력하는 회귀(Regression) 방식을 택했다. 이 접근법은 다음과 같은 상황에서 한계를 드러낸다.</p>
<table><thead><tr><th><strong>문제점</strong></th><th><strong>설명</strong></th><th><strong>결과</strong></th></tr></thead><tbody>
<tr><td><strong>가림(Occlusion)</strong></td><td>로봇 작업 공간이나 어수선한 배경(Clutter)에서 객체가 다른 물체에 의해 가려짐.</td><td>가려진 키포인트 위치의 시각적 정보(Feature)가 소실되어 네트워크가 허구의 값을 예측하거나 큰 오차 발생.</td></tr>
<tr><td><strong>절단(Truncation)</strong></td><td>객체가 카메라 시야각 경계에 걸쳐 있어 일부 키포인트가 이미지 밖으로 나감.</td><td>이미지 내에 존재하지 않는 좌표를 예측해야 하므로 정의되지 않은 영역에 대한 추론 강요로 성능 급락.</td></tr>
<tr><td><strong>정보의 비효율적 활용</strong></td><td>객체는 수천 개의 픽셀로 구성되나, 희소 회귀는 소수의 점 위치 정보만 학습에 사용.</td><td>객체 전체의 형상(Geometry)이나 표면 텍스처 정보를 충분히 활용하지 못해 강건성 저하.</td></tr>
</tbody></table>
<p>이러한 문제를 극복하기 위해서는 ‘지역적(Local)’ 특징에만 의존하는 좌표 회귀를 넘어, 객체의 ‘전역적(Global)’ 정보를 활용하여 키포인트 위치를 추론할 수 있는 새로운 표현형(Representation)이 필요했다. 이것이 PVNet이 제안한 <strong>픽셀 단위 투표(Pixel-wise Voting)</strong> 메커니즘의 출발점이다.</p>
<h2>3.  PVNet: 픽셀 단위 투표 네트워크의 아키텍처 및 핵심 원리</h2>
<p>PVNet은 CVPR 2019에서 Sida Peng 등에 의해 제안된 프레임워크로, 6DoF 포즈 추정의 난제인 가림과 절단 문제를 <strong>벡터 필드(Vector Field)</strong> 개념을 도입하여 획기적으로 해결했다.</p>
<h3>3.1  벡터 필드 표현(Vector-Field Representation)</h3>
<p>PVNet은 키포인트의 절대 좌표를 직접 예측하는 대신, 객체 마스크 내의 각 픽셀 <span class="math math-inline">p</span>에서 키포인트 <span class="math math-inline">\mathbf{x}_k</span>를 향하는 <strong>단위 벡터(Unit Vector)</strong> <span class="math math-inline">\mathbf{v}_k(p)</span>를 학습한다.<br />
<span class="math math-display">
\mathbf{v}_k(p) = \frac{\mathbf{x}_k - p}{||\mathbf{x}_k - p||_2}
</span><br />
여기서 <span class="math math-inline">p</span>는 이미지 상의 픽셀 좌표이다. 이 벡터 필드 표현은 다음과 같은 기하학적 및 학습적 강점을 가진다.</p>
<ul>
<li><strong>기하학적 불변성(Geometric Invariance)</strong>: 벡터 필드는 객체의 지역적 외형(Local Appearance)과 키포인트 간의 상대적 방향 관계를 나타낸다. 따라서 객체의 일부가 가려지더라도, 가려지지 않고 남아 있는 픽셀들이 가리키는 벡터들의 연장선이 교차하는 지점은 여전히 유효한 키포인트 위치를 지시한다. 이는 ’보이는 부분이 보이지 않는 부분을 예측한다’는 철학을 구현한 것이다.</li>
<li><strong>밀집 정보 활용(Dense Supervision)</strong>: 객체 영역 내의 모든 픽셀이 학습에 참여하므로, 희소 좌표 회귀보다 훨씬 풍부한 지도 신호(Supervision Signal)를 제공한다. 이는 네트워크가 객체의 전체적인 형상을 이해하고, 국소적인 노이즈나 가림에 영향을 덜 받도록 돕는다.</li>
<li><strong>스케일 강건성</strong>: 초기 연구에서는 오프셋(Offset, <span class="math math-inline">\mathbf{x}_k - p</span>)을 직접 예측하기도 했으나, 오프셋은 객체의 크기나 거리에 따라 스케일이 변하여 학습이 어렵다. 반면 단위 벡터는 방향 정보만을 담고 있어 스케일 변화에 강건하다.</li>
</ul>
<h3>3.2  네트워크 아키텍처 (Network Architecture)</h3>
<p>PVNet의 아키텍처는 세그멘테이션(Segmentation)과 벡터 필드 예측을 동시에 수행하는 멀티태스크 학습 구조를 가진다.</p>
<ul>
<li><strong>백본(Backbone)</strong>: ResNet-18을 기본으로 하되, 얕은 층(Low-level)의 공간 해상도를 유지하기 위해 팽창 컨볼루션(Dilated Convolution)을 적용하거나 일부 풀링(Pooling) 레이어를 제거하는 변형을 가한다. 이는 작은 객체나 정교한 키포인트 위치 추정에 필수적인 공간 정보 손실을 최소화하기 위함이다.</li>
<li><strong>출력 헤드(Output Heads)</strong>: 백본의 특징 맵(Feature Map)은 두 개의 병렬 분기(Branch)로 나뉜다.</li>
</ul>
<ol>
<li><strong>시맨틱 세그멘테이션(Semantic Segmentation) 분기</strong>: 각 픽셀이 특정 객체 클래스에 속하는지 여부를 판별한다. 출력 텐서의 형태는 <span class="math math-inline">H \times W \times (C+1)</span>이며, 여기서 <span class="math math-inline">C</span>는 객체 클래스 수이고 <span class="math math-inline">+1</span>은 배경 클래스이다.</li>
<li><strong>벡터 필드 예측(Vector Field Prediction) 분기</strong>: 각 픽셀마다 <span class="math math-inline">K</span>개의 키포인트를 향하는 단위 벡터의 <span class="math math-inline">(x, y)</span> 성분을 예측한다. 출력 텐서의 형태는 <span class="math math-inline">H \times W \times (C \times K \times 2)</span>이다.</li>
</ol>
<h3>3.3  학습 전략 및 손실 함수 (Training Strategy &amp; Loss Function)</h3>
<p>네트워크 학습을 위한 손실 함수는 벡터 필드 회귀 손실(<span class="math math-inline">L_{vec}</span>)과 세그멘테이션 손실(<span class="math math-inline">L_{seg}</span>)의 가중 합으로 구성된다.<br />
<span class="math math-display">
L = L_{vec} + \lambda L_{seg}
</span></p>
<ul>
<li>
<p><strong>벡터 필드 손실 (<span class="math math-inline">L_{vec}</span>)</strong>: 객체 영역에 속한 픽셀 <span class="math math-inline">p</span>에 대해서만, 예측된 벡터 <span class="math math-inline">\tilde{\mathbf{v}}_k(p)</span>와 정답 벡터 <span class="math math-inline">\mathbf{v}_k(p)</span> 사이의 차이를 Smooth L1 Loss로 계산한다.<br />
<span class="math math-display">
L_{vec} = \sum_{k=1}^K \sum_{p \in \mathcal{O}} \text{SmoothL1}(\tilde{\mathbf{v}}_k(p) - \mathbf{v}_k(p))
</span><br />
여기서 <span class="math math-inline">\mathcal{O}</span>는 객체 마스크에 해당하는 픽셀들의 집합이다.</p>
</li>
<li>
<p><strong>세그멘테이션 손실 (<span class="math math-inline">L_{seg}</span>)</strong>: 일반적인 픽셀 단위 Cross-entropy Loss를 사용한다.</p>
</li>
</ul>
<p>**데이터 증강(Data Augmentation)**은 PVNet의 성능, 특히 가림에 대한 강건성을 확보하는 데 결정적인 역할을 한다. ‘Cut-and-Paste’ 전략을 사용하여, 학습 이미지에서 객체의 일부를 무작위로 잘라내거나(Truncation), 다른 물체 이미지를 객체 위에 덧붙여(Occlusion) 인위적인 가림 상황을 시뮬레이션한다. 또한, 배경 이미지 교체, 색상 변화(Color Jittering), 회전 등을 통해 다양한 조명 환경과 배경 복잡도에 대응하도록 학습한다.</p>
<h2>4.  추론 메커니즘: 픽셀에서 포즈까지</h2>
<p>PVNet의 추론 과정은 네트워크가 예측한 벡터 필드로부터 실제 키포인트 좌표를 복원하고, 이를 바탕으로 최종 6DoF 포즈를 계산하는 단계로 이루어진다. 이 과정에서 <strong>RANSAC 기반 투표</strong>와 <strong>불확실성 기반 PnP</strong>가 핵심적인 역할을 수행한다.</p>
<h3>4.1  RANSAC 기반 투표 메커니즘 (Voting-based Keypoint Localization)</h3>
<p>네트워크 출력인 벡터 필드로부터 키포인트 좌표를 복원하는 과정은 Hough Voting과 유사한 개념을 차용하되, 딥러닝으로 학습된 정밀한 방향 벡터를 사용한다는 점에서 차별화된다.</p>
<ol>
<li>
<p><strong>가설 생성(Hypothesis Generation)</strong>: 세그멘테이션 결과로 얻어진 객체 마스크 내에서 두 픽셀 <span class="math math-inline">p_i, p_j</span>를 무작위로 선택한다. 두 픽셀에서의 예측 벡터 <span class="math math-inline">\mathbf{v}_k(p_i), \mathbf{v}*k(p_j)</span>가 가리키는 직선의 교차점을 키포인트 후보(Hypothesis) <span class="math math-inline">h*{k, m}</span>으로 삼는다. 이 과정을 <span class="math math-inline">N</span>번 반복하여 다수의 후보군을 생성한다.</p>
</li>
<li>
<p><strong>투표 및 점수 계산(Voting Score)</strong>: 생성된 각 후보 <span class="math math-inline">h_{k, m}</span>에 대해, 객체 내 모든 픽셀들이 투표를 진행한다. 픽셀 <span class="math math-inline">p</span>의 예측 벡터 <span class="math math-inline">\mathbf{v}*k(p)</span>가 후보점 <span class="math math-inline">h*{k, m}</span>을 정확히 향할수록 높은 점수를 부여한다. 점수 <span class="math math-inline">w_{k, m}</span>은 다음과 같이 정의된다.<br />
<span class="math math-display">
w_{k, m} = \sum_{p \in \mathcal{O}} \mathbb{I} \left( \frac{(h_{k, m} - p)^T}{| | h_{k, m} - p | |} \cdot \mathbf{v}_k(p) \ge \theta \right)
</span><br />
여기서 <span class="math math-inline">\mathbb{I}</span>는 지시 함수(Indicator Function), <span class="math math-inline">\theta</span>는 임계값(Threshold, 보통 0.99)이다. 즉, 예측된 방향과 후보점을 향한 실제 방향의 코사인 유사도가 0.99 이상인(거의 일치하는) 픽셀들의 개수가 해당 후보의 신뢰도(Confidence)가 된다.</p>
</li>
</ol>
<p>이 방식은 아웃라이어(Outlier)에 매우 강건하다. 배경 픽셀이나 가려진 영역의 픽셀이 잘못된 벡터를 예측하더라도, 다수의 정상 픽셀들이 형성하는 교차점이 지배적인 투표를 받게 되므로 노이즈가 자연스럽게 필터링된다.</p>
<h3>4.2  확률 분포 추정과 불확실성(Uncertainty) 모델링</h3>
<p>PVNet의 가장 큰 기술적 진보 중 하나는 키포인트 위치를 단순한 점(Point)이 아닌 **공간적 확률 분포(Spatial Probability Distribution)**로 해석했다는 점이다. 투표 결과로 얻어진 후보군과 그 가중치(점수)를 이용해 각 키포인트 위치의 평균(Mean, <span class="math math-inline">\mu_k</span>)과 공분산(Covariance, <span class="math math-inline">\Sigma_k</span>)을 추정할 수 있다.<br />
<span class="math math-display">
\mu_k = \frac{\sum_{m} w_{k, m} h_{k, m}}{\sum_{m} w_{k, m}}
</span></p>
<p><span class="math math-display">
\Sigma_k = \frac{\sum_{m} w_{k, m} (h_{k, m} - \mu_k)(h_{k, m} - \mu_k)^T}{\sum_{m} w_{k, m}}
</span></p>
<p>여기서 공분산 행렬 <span class="math math-inline">\Sigma_k</span>는 해당 키포인트 추정의 **불확실성(Uncertainty)**을 정량적으로 대변한다. 예를 들어, 객체의 모서리 부분처럼 특징이 명확한 지점은 벡터들이 한 점으로 정밀하게 수렴하여 분산이 작게(타원이 작게) 추정된다. 반면, 텍스처가 없는 평면 내부의 점이나 심하게 가려진 점은 벡터들의 교차점이 넓게 퍼지게 되어 분산이 크게(타원이 크고 길게) 추정된다. 이러한 불확실성 정보는 후술할 PnP 단계에서 결정적인 가중치로 작용한다.</p>
<h3>4.3  불확실성 기반 PnP (Uncertainty-driven PnP)</h3>
<p>전통적인 EPnP 알고리즘은 모든 2D-3D 대응쌍을 동등한 신뢰도로 취급한다. 그러나 PVNet의 결과에서 볼 수 있듯이, 키포인트마다 추정의 신뢰도가 다르다. 이를 무시하고 모든 점에 동일한 가중치를 부여하면, 불확실한 키포인트(오차가 큰 점)가 전체 포즈 추정 결과를 왜곡시킬 수 있다.</p>
<p>PVNet은 이를 해결하기 위해 **불확실성 기반 PnP(Uncertainty-driven PnP)**를 제안했다. 이는 각 키포인트의 공분산 행렬 <span class="math math-inline">\Sigma_k</span>의 역행렬(Mahalanobis distance)을 가중치로 사용하여 재투영 오차를 최소화하는 방식이다.<br />
<span class="math math-display">
\min_{\mathbf{R}, \mathbf{t}} \sum_{k=1}^{K} (\tilde{\mathbf{x}}_k - \mu_k)^T \Sigma_k^{-1} (\tilde{\mathbf{x}}_k - \mu_k)
</span><br />
여기서 <span class="math math-inline">\tilde{\mathbf{x}}_k = \pi(\mathbf{R} \mathbf{X}_k + \mathbf{t})</span>는 현재 추정된 포즈에 의한 3D 키포인트의 투영점이다. 이 식의 기하학적 의미는 직관적이다. 분산이 큰(불확실한) 키포인트 방향으로는 오차를 어느 정도 허용하되(<span class="math math-inline">\Sigma_k^{-1}</span>가 작음), 분산이 작은(확실한) 키포인트 방향으로는 오차를 엄격하게 제한(<span class="math math-inline">\Sigma_k^{-1}</span>가 큼)하여 최적의 포즈를 찾는 것이다. 이는 베이지안 관점에서 관측 데이터의 우도(Likelihood)를 최대화하는 것과 동일하며, 특히 가림이 심해 특정 키포인트의 위치가 불분명할 때 전체 포즈가 틀어지는 것을 방지하는 강력한 정규화(Regularization) 효과를 발휘한다.</p>
<p>초기값 설정(Initialization)을 위해, 공분산 행렬의 대각합(Trace)이 가장 작은, 즉 가장 신뢰할 수 있는 4개의 키포인트를 선택하여 EPnP를 수행한 후, Levenberg-Marquardt 알고리즘으로 위 목적 함수를 최적화한다.</p>
<h2>5.  성능 평가 및 비교 분석</h2>
<p>PVNet은 LINEMOD, Occlusion LINEMOD, YCB-Video 등 주요 벤치마크 데이터셋에서 당대 최고의 성능(State-of-the-Art)을 기록했다. 특히 가림이 있는 상황에서의 성능 향상이 두드러졌다.</p>
<h3>5.1  데이터셋 및 평가 지표 (Datasets &amp; Metrics)</h3>
<ul>
<li><strong>LINEMOD</strong>: 13개의 텍스처가 부족한 가정용 객체로 구성된 표준 벤치마크이다. 조명 변화와 배경 복잡도가 존재한다.</li>
<li><strong>Occlusion LINEMOD</strong>: LINEMOD의 부분집합으로, 객체들이 서로 심하게 가려진 장면을 포함한다. 가림에 대한 강건성을 평가하는 데 최적화되어 있다.</li>
<li><strong>평가 지표</strong>:</li>
<li><strong>ADD Metric</strong>: 추정된 포즈로 변환된 모델의 3D 점들과 정답 포즈로 변환된 점들 간의 평균 거리가 모델 지름의 10% 이내인 경우를 정답으로 간주한다.</li>
<li><strong>ADD-S Metric</strong>: 대칭 객체(Symmetric Object)의 경우, 가장 가까운 점(Closest Point) 간의 거리를 평균하여 평가한다.</li>
<li><strong>2D Projection Metric</strong>: 3D 모델 점들을 2D 이미지로 투영했을 때의 평균 픽셀 거리가 5픽셀 이내인 경우를 정답으로 한다.</li>
</ul>
<h3>5.2  정량적 비교 분석 (Quantitative Comparison)</h3>
<p>다음 표는 LINEMOD 및 Occlusion LINEMOD 데이터셋에서의 성능 비교를 요약한 것이다.</p>
<table><thead><tr><th><strong>방법론 (Method)</strong></th><th><strong>접근 방식 (Approach)</strong></th><th><strong>LINEMOD ADD (%)</strong></th><th><strong>Occlusion LINEMOD ADD (%)</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td>**BB8 **</td><td>BBox 키포인트 회귀 + Refinement</td><td>62.7</td><td>-</td><td>가림에 취약, 추가 정제 필요</td></tr>
<tr><td>**Tekin et al. **</td><td>YOLO 기반 좌표 회귀</td><td>55.95</td><td>-</td><td>단일 단계, 빠른 속도이나 정확도 낮음</td></tr>
<tr><td>**PoseCNN **</td><td>직접 포즈 회귀 + Hough</td><td>62.7</td><td>24.9</td><td>회전 추정의 한계, 깊이 정보 미사용 시 성능 저하</td></tr>
<tr><td>**DeepIM **</td><td>포즈 정제 (Refinement)</td><td>88.6</td><td>-</td><td>초기 포즈 필요, 연산량 과다</td></tr>
<tr><td>**PVNet (Ours) **</td><td><strong>벡터 투표 + 불확실성 PnP</strong></td><td><strong>86.27</strong></td><td><strong>40.77</strong></td><td><strong>압도적 가림 강건성, 실시간 성능</strong></td></tr>
</tbody></table>
<ul>
<li><strong>LINEMOD</strong>: PVNet은 ADD 메트릭에서 86.27%를 달성하여, 기존 SOTA였던 방식들을 상회했다. 특히 DeepIM과 같은 무거운 정제 네트워크를 사용하지 않고도 높은 정확도를 달성했다는 점이 주목할 만하다.</li>
<li><strong>Occlusion LINEMOD</strong>: 객체의 상당 부분이 가려진 이 데이터셋에서 PVNet의 진가가 드러난다. PoseCNN이 24.9%, 다른 키포인트 방식들이 한 자릿수 성공률을 보일 때, PVNet은 40.77%라는 획기적인 성능을 기록했다. 이는 픽셀 단위 투표가 부분적인 정보만으로도 전체 형상을 복원하는 데 얼마나 효과적인지를 증명한다.</li>
</ul>
<h3>5.3  정성적 분석 및 가림 강건성 (Qualitative Analysis)</h3>
<p>PVNet의 벡터 필드 시각화 결과를 보면, 객체의 일부가 가려져 있어도 나머지 보이는 부분의 벡터들이 정확하게 가려진 키포인트 위치를 지시하는 것을 확인할 수 있다. 예를 들어, 고양이(Cat) 객체의 머리 부분이 다른 물체에 가려져 있어도, 몸통 부분의 픽셀들이 머리 중심을 향해 일관되게 벡터를 예측함으로써 정확한 위치 추정이 가능하다.</p>
<p>또한, 불확실성 PnP를 통해 추정된 공분산 타원(Covariance Ellipse)을 살펴보면, 가려지거나 텍스처가 부족한 부분의 키포인트는 타원이 크고 길게 형성되어 불확실성이 높음을 스스로 인지하고 있음을 알 수 있다. 이는 PnP 과정에서 해당 키포인트의 영향력을 낮추어 전체적인 포즈 오차를 줄이는 데 기여한다.</p>
<h3>5.4  효율성 및 계산 비용 (Efficiency)</h3>
<p>PVNet은 ResNet-18이라는 비교적 가벼운 백본을 사용하여 실시간 추론이 가능하다. 640x480 해상도 이미지 기준으로 약 25 FPS(Frames Per Second)의 속도로 동작하며, 이는 로봇 조작이나 AR과 같은 실시간 애플리케이션에 적합한 수준이다. 특히 RANSAC 투표 과정은 GPU 병렬 처리를 통해 최적화될 수 있어 계산 비용을 효과적으로 억제할 수 있다.</p>
<h2>6.  한계점 및 최신 확장 연구 (Advanced Variants &amp; Limitations)</h2>
<p>PVNet이 혁신적인 성과를 거두었지만, 몇 가지 한계점 또한 존재하며 이는 후속 연구들의 모티브가 되었다.</p>
<h3>6.1  텍스처 부재(Texture-less) 객체에 대한 한계와 개선</h3>
<p>PVNet은 기본적으로 RGB 정보를 기반으로 벡터를 추론한다. 따라서 객체 표면에 텍스처가 전혀 없고 색상이 균일한 경우(예: 흰색 컵, 산업용 금속 부품), 픽셀의 지역적 특징만으로는 키포인트 방향을 유추하기 어려워 벡터 필드가 무작위로 발산하는 경향이 있다. 이를 보완하기 위해 <strong>DR_PVNet</strong>과 같은 연구는 엣지 정보를 강화하거나 깊이(Depth) 정보를 융합하는 RGB-D PVNet 형태로 발전하였다.</p>
<h3>6.2  CoS-PVNet: 복잡한 장면을 위한 확장</h3>
<p>**CoS-PVNet (Complex Scenes Pixel-wise Voting Network)**은 PVNet의 구조를 개선하여 더욱 복잡한 장면에서의 강건성을 높인 연구이다. CoS-PVNet은 다음과 같은 개선 사항을 도입했다.</p>
<ul>
<li><strong>글로벌 어텐션 메커니즘 (Global Attention Mechanism)</strong>: 객체 전체의 맥락(Context) 정보를 파악하여, 가려진 부분에 대한 추론 능력을 강화한다.</li>
<li><strong>가중치 자가 학습 (Weight Self-learning)</strong>: 학습 과정에서 각 픽셀의 중요도를 스스로 조절하여, 노이즈가 심하거나 가려진 픽셀의 영향력을 줄이고 신뢰할 수 있는 픽셀에 집중하도록 한다.</li>
</ul>
<h3>6.3  최신 트렌드와의 비교: FoundationPose 등</h3>
<p>최근에는 대규모 데이터로 사전 학습된 트랜스포머(Transformer) 기반의 모델들, 예를 들어 <strong>FoundationPose</strong> 등이 등장하여 PVNet보다 더 높은 정확도와 일반화 성능을 보여주고 있다. 이러한 최신 모델들은 인스턴스 특화 학습 없이도 새로운 객체에 바로 적용 가능한 제로샷(Zero-shot) 능력을 갖추고 있으나, PVNet은 여전히 경량화된 모델 크기와 실시간성, 그리고 명확한 기하학적 해석 가능성 덕분에 특정 산업 현장이나 임베디드 시스템에서 유효한 솔루션으로 평가받는다.</p>
<h2>7.  로봇 조작(Robotic Manipulation) 응용 사례</h2>
<p>PVNet의 강력한 가림 강건성은 로봇 조작, 특히 **빈 피킹(Bin Picking)**과 같은 작업에서 빛을 발한다. 빈 피킹은 상자 안에 무작위로 쌓여 있는 물체들을 로봇이 하나씩 집어내는 작업으로, 물체들끼리 서로 겹치고 가려지는 현상이 필연적으로 발생한다.</p>
<ul>
<li><strong>파지(Grasping) 파이프라인</strong>: PVNet을 통해 가려진 객체의 6D 포즈를 추정한 후, 이 포즈를 기반으로 미리 정의된 파지 점(Grasp Point)으로 로봇 팔을 유도한다. 실제 실험 결과, PVNet을 적용한 로봇 시스템은 가림이 심한 환경에서도 96%의 파지 위치 선정(Localization) 성공률과 88%의 파지 성공률을 기록했다.</li>
<li><strong>투명 물체 및 난반사 물체</strong>: 최근 연구에서는 PVNet의 세그멘테이션과 투표 메커니즘을 투명한 물체(유리컵 등)에 적용하여, 깊이 센서가 감지하지 못하는 투명 물체의 포즈를 RGB 이미지만으로 추정하는 데 성공하기도 했다.</li>
</ul>
<h2>8. 결론</h2>
<p>PVNet은 6DoF 포즈 추정 분야에서 **‘밀집 픽셀 투표(Dense Pixel-wise Voting)’**라는 새로운 패러다임을 제시했다. 기존의 희소 회귀 방식이 갖던 가림과 절단에 대한 취약성을 벡터 필드와 RANSAC 투표를 통해 극복했으며, 딥러닝 예측의 불확실성을 기하학적 최적화(PnP)에 통합함으로써 정밀도를 극대화했다. 비록 텍스처 부재 객체 처리나 대칭성 문제 등 일부 한계가 존재하지만, 이를 보완하는 CoS-PVNet 등의 후속 연구를 통해 기술은 지속적으로 진화하고 있다. 본 서적의 독자들은 PVNet의 설계를 통해 딥러닝과 전통적인 기하학(Geometry)이 어떻게 상호 보완적으로 결합하여 컴퓨터 비전의 난제를 해결하는지에 대한 중요한 통찰을 얻을 수 있을 것이다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>PVNet: Pixel-Wise Voting Network for 6DoF … - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2019/papers/Peng_PVNet_Pixel-Wise_Voting_Network_for_6DoF_Pose_Estimation_CVPR_2019_paper.pdf</li>
<li>A Survey of 6DoF Object Pose Estimation Methods for Different …, https://www.researchgate.net/publication/378057489_A_Survey_of_6DoF_Object_Pose_Estimation_Methods_for_Different_Application_Scenarios</li>
<li>CS 231a Project Final Report 6D Pose Estimation using PVNet, https://web.stanford.edu/class/cs231a/prev_projects_2022/CS_231a_Project_Final_Report.pdf</li>
<li>6D Object Pose Estimation using Keypoints and Part Affinity Fields, https://www.ais.uni-bonn.de/papers/RCS_2021_Zappel.pdf</li>
<li>Instance-Adaptive and Geometric-Aware Keypoint Learning for …, https://arxiv.org/html/2403.19527v1</li>
<li>DR_PVNet: An Improved PVNet Network for Weakly Textured Object …, https://ieeexplore.ieee.org/iel8/6287639/10820123/10855419.pdf</li>
<li>PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation, https://zju3dv.github.io/pvnet/</li>
<li>Leveraging Feature Uncertainty in the PnP Problem - BMVA Archive, https://bmva-archive.org.uk/bmvc/2014/files/paper065.pdf</li>
<li>Where Unseen Model-based 6D Pose Estimation Meets Occlusion, https://arxiv.org/pdf/2511.15874</li>
<li>What’s the difference b/w RANSAC-based voting &amp; Hough voting to …, https://stackoverflow.com/questions/67333996/whats-the-difference-b-w-ransac-based-voting-hough-voting-to-estimate-vector</li>
<li>PVNet: 6DoF Pose Estimation via Pixel-Wise Voting | PDF - Scribd, https://www.scribd.com/document/733811521/Peng-PVNet-Pixel-Wise-Voting-Network-for-6DoF-Pose-Estimation-CVPR-2019-Paper</li>
<li>(PDF) PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation, https://www.researchgate.net/publication/330035033_PVNet_Pixel-wise_Voting_Network_for_6DoF_Pose_Estimation</li>
<li>RSCS6D: Keypoint Extraction-Based 6D Pose Estimation - MDPI, https://www.mdpi.com/2076-3417/15/12/6729</li>
<li>A Robust CoS-PVNet Pose Estimation Network in Complex Scenarios, https://www.mdpi.com/2079-9292/13/11/2089</li>
<li>Targeted Hard Sample Synthesis Based on Estimated Pose … - arXiv, https://arxiv.org/html/2412.04279v1</li>
<li>[1802.00520] Real-world Multi-object, Multi-grasp Detection - arXiv, https://arxiv.org/abs/1802.00520</li>
<li>6-DOF GraspNet: Variational Grasp Generation for Object …, https://openaccess.thecvf.com/content_ICCV_2019/papers/Mousavian_6-DOF_GraspNet_Variational_Grasp_Generation_for_Object_Manipulation_ICCV_2019_paper.pdf</li>
<li>Results of grasping experiment (successful rate [%]). 12 trials for…, https://www.researchgate.net/figure/Results-of-grasping-experiment-successful-rate-12-trials-for-each-object-in-every_tbl1_365104008</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>