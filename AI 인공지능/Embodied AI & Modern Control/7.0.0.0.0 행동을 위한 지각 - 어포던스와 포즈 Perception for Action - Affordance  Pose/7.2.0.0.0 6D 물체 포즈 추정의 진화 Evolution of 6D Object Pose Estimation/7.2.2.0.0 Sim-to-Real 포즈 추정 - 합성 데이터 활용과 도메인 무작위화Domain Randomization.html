<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.2.2 Sim-to-Real 포즈 추정: 합성 데이터 활용과 도메인 무작위화(Domain Randomization)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.2.2 Sim-to-Real 포즈 추정: 합성 데이터 활용과 도메인 무작위화(Domain Randomization)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <a href="index.html">7.2 6D 물체 포즈 추정의 진화 (Evolution of 6D Object Pose Estimation)</a> / <span>7.2.2 Sim-to-Real 포즈 추정: 합성 데이터 활용과 도메인 무작위화(Domain Randomization)</span></nav>
                </div>
            </header>
            <article>
                <h1>7.2.2 Sim-to-Real 포즈 추정: 합성 데이터 활용과 도메인 무작위화(Domain Randomization)</h1>
<h2>1.  서론: 데이터 가용성의 역설과 합성 데이터의 부상</h2>
<p>현대 로보틱스, 특히 비정형 환경에서의 빈 피킹(Bin-picking)이나 정밀 조립(Assembly)과 같은 조작(Manipulation) 작업에 있어, 객체의 6자유도(6-DoF) 포즈—3차원 공간상의 위치(Translation, <span class="math math-inline">\mathbf{t} \in \mathbb{R}^3</span>)와 자세(Rotation, <span class="math math-inline">\mathbf{R} \in SO(3)</span>)—를 정확히 추정하는 것은 시스템의 성공을 좌우하는 전제 조건이다. 딥러닝 기반의 컴퓨터 비전 기술은 지난 10년간 비약적인 발전을 이루었으나, 이러한 발전은 대규모의 정제가 잘 된(Well-curated) 데이터셋의 존재를 전제로 한다. 2차원 객체 검출(Object Detection)이나 분류(Classification) 작업의 경우, 인터넷상의 방대한 이미지를 활용하거나 크라우드 소싱을 통해 비교적 저렴한 비용으로 바운딩 박스(Bounding Box) 라벨링을 수행할 수 있다. 그러나 6D 포즈 추정의 영역으로 들어오면 상황은 급변한다.</p>
<p>6D 포즈 라벨링은 인간의 직관만으로는 불가능에 가깝다. 이미지 내의 특정 픽셀이 3차원 공간상에서 카메라 좌표계 기준으로 정확히 몇 밀리미터 떨어져 있는지, 혹은 객체가 쿼터니언(Quaternion) 기준으로 얼마나 회전해 있는지를 사람이 육안으로 판별하여 라벨링하는 것은 현실적으로 불가능하다. 이를 해결하기 위해 실제 환경에서 데이터를 수집하려면, 고가의 모션 캡처 시스템을 구축하거나, 객체와 카메라에 마커(Marker)를 부착한 후 정밀하게 캘리브레이션된 환경에서 촬영을 진행해야 한다. 이러한 과정은 막대한 비용과 시간을 요구할 뿐만 아니라, 환경이 조금만 바뀌어도 데이터를 다시 수집해야 하는 확장성(Scalability)의 문제를 야기한다.</p>
<p>이러한 ‘데이터 가용성의 역설’—가장 데이터가 필요한 고난도 작업일수록 데이터 확보가 가장 어렵다는 모순—을 해결하기 위한 돌파구로 **합성 데이터(Synthetic Data)**가 주목받고 있다. 시뮬레이션 환경에서는 물리 엔진과 렌더링 엔진을 통해 객체의 위치, 조명, 카메라 앵글, 가려짐(Occlusion) 등을 완벽하게 제어할 수 있으며, 이에 따른 픽셀 단위의 완벽한 정답(Ground Truth) 라벨을 비용 없이 자동으로 생성할 수 있다.</p>
<p>하지만 합성 데이터로 훈련된 모델을 실제 세계(Real World)에 배포했을 때, 성능이 급격히 저하되는 현상이 필연적으로 발생한다. 이를 <strong>현실 간극(Reality Gap)</strong> 또는 도메인 격차(Domain Gap)라고 한다. 이는 시뮬레이터가 생성한 이미지가 실제 카메라 센서가 포착하는 이미지의 텍스처 디테일, 조명 반사, 색상 분포, 센서 노이즈 등을 완벽하게 모사하지 못하기 때문에 발생한다.</p>
<p>본 장에서는 이러한 현실 간극을 극복하고, 시뮬레이션에서 학습된 지능을 실제 로봇 시스템으로 전이(Transfer)하기 위한 핵심 방법론인 **도메인 무작위화(Domain Randomization, DR)**와 <strong>사실적 렌더링(Photorealistic Rendering)</strong>, 그리고 최근의 <strong>생성형 AI(Generative AI)</strong> 기반 텍스처링 기술을 심층적으로 다룬다.</p>
<h2>2.  도메인 무작위화(Domain Randomization)의 이론적 프레임워크</h2>
<h3>2.1  현실 간극의 해체: 외형적 간극과 내용적 간극</h3>
<p>성공적인 Sim-to-Real 전이를 위해서는 먼저 현실 간극의 본질을 이해해야 한다. NVIDIA의 연구진들은 이를 크게 두 가지로 분류하여 접근한다.</p>
<ul>
<li><strong>외형적 간극 (Appearance Gap):</strong> 픽셀 수준에서의 차이를 의미한다. 시뮬레이터의 렌더링 엔진(Rasterization vs Ray-tracing)이 실제 광학 현상을 단순화하면서 발생하는 차이, 재질(Material) 표현의 한계, 카메라 센서의 노이즈, 모션 블러 등이 이에 해당한다.</li>
<li><strong>내용적 간극 (Content Gap):</strong> 장면의 구성과 관련된 차이이다. 실제 환경에 존재하는 다양한 배경 잡동사니, 객체의 다양한 배치 형태, 조명의 복잡한 구성 등이 시뮬레이션 환경에 부재함으로써 발생한다.</li>
</ul>
<p>도메인 무작위화(DR)는 이 두 가지 간극을 메우기 위해 시뮬레이션 환경을 실제와 똑같이 만드는 ’모사(Simulation)’의 접근 대신, 시뮬레이션 환경의 파라미터를 극단적으로 다양화하여 실제 환경을 수많은 무작위 변이 중 하나로 포섭하려는 ’일반화(Generalization)’의 접근을 취한다.</p>
<h3>2.2  수학적 정식화: 교란 변수의 주변화(Marginalization)</h3>
<p>도메인 무작위화의 핵심 원리는 신경망이 학습 과정에서 **교란 변수(Nuisance Variables)**에 과적합되는 것을 방지하고, <strong>도메인 불변(Domain-Invariant)</strong> 특징을 학습하도록 유도하는 것이다. 여기서 교란 변수란 조명, 배경, 객체의 색상 등 포즈 추정이라는 본질적 태스크와는 인과관계가 없는 시각적 요소를 의미한다.</p>
<p>수학적으로 이를 표현하면, 우리는 최적의 포즈 추정 모델 파라미터 <span class="math math-inline">\theta^*</span>를 찾고자 한다. 소스 도메인(시뮬레이션)의 파라미터 공간 <span class="math math-inline">\xi \in \Xi</span> (예: 조명 강도, 텍스처 종류, 카메라 위치 등)에 대해 정의된 확률 분포 <span class="math math-inline">P(\xi)</span>가 있을 때, DR은 다음의 기대 손실(Expected Loss)을 최소화하는 최적화 문제로 정의된다.<br />
<span class="math math-display">
\theta^* = \arg\min_{\theta} \mathbb{E}_{\xi \sim P(\xi)} [ \mathcal{L}(f_\theta(x_\xi), y_\xi) ]
</span><br />
여기서 <span class="math math-inline">x_\xi</span>는 파라미터 <span class="math math-inline">\xi</span>로 생성된 합성 이미지, <span class="math math-inline">y_\xi</span>는 해당 이미지의 정답 포즈 라벨, <span class="math math-inline">f_\theta</span>는 신경망 모델, <span class="math math-inline">\mathcal{L}</span>은 손실 함수(예: ADD-S Loss)이다.</p>
<p>이 식의 함의는 명확하다. 모델이 단일한 시뮬레이션 환경(<span class="math math-inline">\xi_{fixed}</span>)에 최적화되는 것이 아니라, 분포 <span class="math math-inline">P(\xi)</span> 전체에 대해 평균적으로 낮은 손실을 갖도록 강제하는 것이다. 이를 통해 모델은 <span class="math math-inline">\xi</span>의 변화(조명이 바뀌거나 배경이 바뀌는 등)에 민감하지 않은(Invariant) 특징, 즉 객체의 기하학적 형상(Geometry)이나 엣지(Edge) 정보에 집중하게 된다. 통계적으로 이는 포즈 추정 확률 분포 <span class="math math-inline">P(Y|X, \xi)</span>에서 <span class="math math-inline">\xi</span>를 적분하여 소거(Marginalize out)하는 과정과 같다.<br />
<span class="math math-display">
P(Y|X) = \int P(Y|X, \xi) P(\xi) d\xi
</span><br />
만약 우리가 설정한 분포 <span class="math math-inline">P(\xi)</span>의 서포트(Support)가 충분히 넓어서 실제 환경의 파라미터 <span class="math math-inline">\xi_{real}</span>이 그 안에 포함된다면(<span class="math math-inline">\xi_{real} \in \text{supp}(P(\xi))</span>), 시뮬레이션에서 학습된 모델은 실제 환경에서도 이론적으로 작동해야 한다.</p>
<h3>2.3  교란 변수와 불변성 학습의 메커니즘</h3>
<p>DR이 작동하는 이유는 신경망이 ’게으른 학습자(Lazy Learner)’라는 특성을 갖기 때문이다. 신경망은 가장 쉬운 특징(Shortcut)을 찾아 학습하려는 경향이 있다. 예를 들어, 특정 부품이 시뮬레이션 상에서 항상 회색 바닥 위에만 놓여 있다면, 네트워크는 부품의 형상을 배우는 대신 ’회색 배경 위의 픽셀 덩어리’를 찾는 단순한 규칙을 학습할 수 있다. 실제 환경에서 바닥 색이 바뀌면 이 모델은 실패한다.</p>
<p>DR은 이러한 ’가짜 상관관계(Spurious Correlations)’를 파괴한다. 배경색을 빨강, 파랑, 노랑, 심지어 노이즈 패턴으로 무작위화하면, 배경색은 더 이상 포즈를 예측하는 데 유용한 정보(Predictive Feature)가 아니게 된다. 네트워크는 손실 함수를 최소화하기 위해 유일하게 변하지 않는 정보, 즉 객체의 <strong>3D 형상과 실루엣</strong>에 집중할 수밖에 없게 된다.</p>
<table><thead><tr><th><strong>변수 유형 (Variable Type)</strong></th><th><strong>예시 (6D Pose Estimation Context)</strong></th><th><strong>DR의 전략적 역할</strong></th></tr></thead><tbody>
<tr><td><strong>의미적 변수 (Semantic Variable)</strong></td><td>객체의 3D 메쉬 형상, 엣지, 키포인트 위치</td><td><strong>보존 (Preserve):</strong> 모델이 반드시 학습해야 할 불변의 정보.</td></tr>
<tr><td><strong>교란 변수 (Nuisance Variable)</strong></td><td>조명 색상/위치, 배경 텍스처, 바닥 재질, 카메라 센서 노이즈</td><td><strong>무작위화 (Randomize):</strong> 정보 가치를 제거하여 모델이 이를 무시하도록 강제.</td></tr>
<tr><td><strong>물리적 파라미터 (Physical Parameter)</strong></td><td>객체 질량, 마찰 계수 (물리 시뮬레이션 시)</td><td><strong>다양화 (Diversify):</strong> 객체가 쌓이는 다양한 포즈(Pile configurations)를 생성하여 기하학적 다양성 확보.</td></tr>
</tbody></table>
<p>[표 7.2.2-1] 도메인 무작위화에서의 변수 분류 및 역할과 전략</p>
<h2>3.  합성 데이터 생성 파이프라인의 구축: 도구와 전략</h2>
<p>효과적인 DR을 구현하기 위해서는 고도화된 합성 데이터 생성 파이프라인이 필수적이다. 과거에는 단순한 OpenGL 기반의 렌더러가 사용되었으나, 최근에는 물리 기반 렌더링(PBR)과 레이 트레이싱을 지원하는 엔진들이 주류를 이루고 있다.</p>
<h3>3.1  주요 시뮬레이션 플랫폼 비교</h3>
<p>현재 로보틱스 및 컴퓨터 비전 연구에서 가장 널리 활용되는 두 가지 주요 플랫폼은 <strong>BlenderProc</strong>과 <strong>NVIDIA Isaac Sim</strong>이다.</p>
<ul>
<li>BlenderProc :</li>
<li><strong>개요:</strong> 오픈 소스 3D 제작 툴인 Blender를 기반으로 DLR(독일 항공우주 센터)에서 개발한 파이프라인이다. Python 스크립트를 통해 Blender의 기능을 모듈화하여 제어한다.</li>
<li><strong>특징:</strong> Blender의 강력한 ‘Cycles’ 렌더러를 사용하여 고품질의 레이 트레이싱 이미지를 생성한다. 특히 BOP(Benchmark for 6D Object Pose Estimation) 포맷을 네이티브로 지원하여 학계의 표준 데이터셋 생성에 최적화되어 있다.</li>
<li><strong>물리 시뮬레이션:</strong> PyBullet을 내부적으로 사용하여 객체를 컨테이너에 쏟아붓는 ‘Physics Drop’ 시뮬레이션을 수행한다. 이는 객체들이 겹치지 않고 자연스럽게 쌓여 있는(Cluttered) 상태를 생성하는 데 필수적이다. 공중에 떠 있는 객체보다 바닥에 쌓인 객체가 물리적으로 타당한 포즈 분포를 제공하기 때문이다.</li>
<li><strong>작동 방식:</strong> 주로 오프라인(Offline) 방식으로 데이터를 미리 대량 생성하여 HDD에 저장한 후 학습에 사용한다.</li>
<li>NVIDIA Isaac Sim &amp; Omniverse Replicator :</li>
<li><strong>개요:</strong> NVIDIA의 Omniverse 플랫폼 기반으로 구축된 로보틱스 전용 시뮬레이터이다. ’Replicator’라는 합성 데이터 생성 모듈(SDG)을 내장하고 있다.</li>
<li><strong>특징:</strong> USD(Universal Scene Description) 포맷을 기반으로 하며, RTX GPU 가속을 통한 실시간 레이 트레이싱과 경로 추적(Path Tracing)을 지원한다.</li>
<li><strong>On-the-fly 무작위화:</strong> Isaac Sim의 가장 강력한 기능은 학습 루프 내에서 데이터를 실시간으로 생성하는 것이다. 데이터를 디스크에 저장하지 않고, GPU 메모리상에서 매 프레임마다 조명, 텍스처, 포즈를 무작위로 변경하여 파이토치(PyTorch) 텐서로 직접 주입한다. 이는 저장 공간의 제약을 없애고, 사실상 무한한 변이(Infinite Variations)를 가진 데이터셋을 모델에 제공할 수 있게 한다.</li>
</ul>
<table><thead><tr><th><strong>특징</strong></th><th><strong>BlenderProc</strong></th><th><strong>NVIDIA Isaac Sim (Replicator)</strong></th></tr></thead><tbody>
<tr><td><strong>기반 엔진</strong></td><td>Blender (Cycles Renderer)</td><td>Omniverse (RTX Renderer)</td></tr>
<tr><td><strong>렌더링 품질</strong></td><td>최상 (느린 Ray-tracing)</td><td>높음 (실시간 Ray-tracing)</td></tr>
<tr><td><strong>데이터 생성 방식</strong></td><td>Offline (디스크 저장 후 로드)</td><td>Online (On-the-fly 생성 및 주입)</td></tr>
<tr><td><strong>물리 엔진</strong></td><td>Bullet Physics</td><td>PhysX 5</td></tr>
<tr><td><strong>주요 활용</strong></td><td>학술 연구, 정적 데이터셋 구축 (BOP)</td><td>산업용 로봇 학습, 대규모 실시간 강화학습</td></tr>
<tr><td><strong>접근성</strong></td><td>오픈 소스, Python 기반</td><td>NVIDIA 하드웨어 필요, Python/USD 기반</td></tr>
</tbody></table>
<p>[표 7.2.2-2] 주요 합성 데이터 생성 플랫폼 비교</p>
<h3>3.2  데이터 생성의 핵심 단계</h3>
<p>빈 피킹을 위한 6D 포즈 추정 데이터 생성은 다음과 같은 정교한 절차를 따른다.</p>
<ol>
<li><strong>3D 자산 준비 (Asset Preparation):</strong> 타겟 객체의 정밀한 3D CAD 모델(.obj,.ply)을 확보한다. 물리 시뮬레이션의 정확성을 위해 객체의 질량 중심(Center of Mass), 관성 모멘트, 마찰 계수와 같은 물리적 속성을 정의하고, 시각적 사실성을 위해 UV 매핑과 PBR 재질(Albedo, Roughness, Metallic)을 적용한다.</li>
<li><strong>장면 구성 및 물리적 배치 (Physics-Aware Placement):</strong></li>
</ol>
<ul>
<li>단순히 객체를 공간에 무작위로 배치하면 객체끼리 겹치거나(Interpenetration) 공중에 떠 있는 물리적으로 불가능한 상황이 발생한다.</li>
<li><strong>Drop Simulation:</strong> 객체들을 가상의 빈(Bin) 위에서 중력을 적용해 낙하시킨다. 물리 엔진이 충돌 처리를 수행하여 객체들이 자연스럽게 안착하고 쌓이도록 한다. 이는 실제 산업 현장의 부품 적재 상태와 가장 유사한 분포를 만들어낸다.</li>
</ul>
<ol start="3">
<li><strong>도메인 무작위화 적용 (Randomization Layer):</strong> 물리적으로 안착된 객체들의 포즈는 유지한 채, 배경, 조명, 텍스처, 카메라 위치 등을 무작위로 변경한다.</li>
<li><strong>자동 어노테이션 (Auto-Annotation):</strong> 렌더링된 RGB 이미지와 완벽하게 동기화된 6D 포즈(<span class="math math-inline">R, t</span>), 깊이 맵(Depth), 인스턴스 분할 마스크(Segmentation Mask), 표면 법선(Surface Normal) 등을 추출한다.</li>
</ol>
<h2>4.  사례 연구: NVIDIA DOPE와 하이브리드 데이터 전략</h2>
<p>Sim-to-Real 6D 포즈 추정의 가장 성공적인 사례 중 하나는 NVIDIA의 **DOPE (Deep Object Pose Estimation)**이다. DOPE 연구진은 순수하게 합성 데이터만을 사용하여 실제 YCB 객체(미국 식료품 등)의 6D 포즈를 추정하는 데 성공했으며, 이 과정에서 발견한 ’데이터 혼합 전략’은 이후 연구들에 큰 영향을 미쳤다.</p>
<h3>4.1  DOPE의 아키텍처와 추론 방식</h3>
<p>DOPE는 VGG-19를 백본(Backbone)으로 사용하는 원-샷(One-shot) 딥러닝 네트워크이다. 이 네트워크는 이미지 전체를 입력받아 객체의 3D 바운딩 박스 8개 꼭짓점과 중심점의 2D 투영 좌표에 대한 **신뢰 맵(Belief Map)**과, 각 꼭짓점이 중심점과 연결되는 방향을 나타내는 **벡터 필드(Vector Field)**를 출력한다.</p>
<ul>
<li><strong>PnP 알고리즘의 활용:</strong> 네트워크가 3D 좌표를 직접 회귀(Regression)하는 대신 2D 키포인트를 찾고, 이를 PnP(Perspective-n-Point) 알고리즘을 통해 6D 포즈로 변환하는 방식을 택했다. 이는 카메라 내부 파라미터(Intrinsics)가 변경되어도 네트워크를 재학습할 필요가 없게 만드는 큰 장점을 제공한다.</li>
</ul>
<h3>4.2  하이브리드 데이터 전략: 1:1 비율의 비밀</h3>
<p>DOPE 연구의 핵심 통찰은 <strong>도메인 무작위화(DR) 데이터</strong>와 <strong>사실적(Photorealistic) 데이터</strong>를 결합했을 때의 시너지 효과이다. 연구진은 실험을 통해 두 데이터를 약 <strong>1:1 비율</strong>로 혼합하여 학습했을 때 실제 환경에서의 성능이 가장 뛰어남을 입증했다.</p>
<ul>
<li><strong>도메인 무작위화 데이터 (DR Data):</strong></li>
<li><strong>구성:</strong> 배경으로 임의의 패턴이나 실제 사진(Flickr 등)을 사용하고, 객체의 텍스처를 제거하거나 강한 색상으로 변경한다. 조명은 비현실적으로 강렬하거나 색상이 있는 조명을 사용한다. 또한, ’Flying Distractors’라고 불리는 기하학적 도형(구, 육면체 등)을 객체 주변 공중에 띄워 가려짐(Occlusion)에 대한 강건성을 학습시킨다.</li>
<li><strong>역할:</strong> 모델이 텍스처, 조명, 특정 배경과 같은 교란 변수에 의존하지 않도록 ’강건성(Robustness)’을 부여한다. “무엇을 무시해야 하는지“를 가르친다.</li>
<li><strong>사실적 데이터 (Photorealistic Data):</strong></li>
<li><strong>구성:</strong> Unreal Engine 4 (UE4)와 NVIDIA의 ‘Falling Things’ 데이터셋을 활용하여 실제 부엌이나 거실과 같은 고품질 3D 환경을 구축한다. 물리 기반 렌더링(PBR)을 통해 사실적인 그림자, 반사(Reflection), 상호 반사(Inter-reflection)를 구현한다.</li>
<li><strong>역할:</strong> 모델에게 실제 세계의 물리적, 광학적 특성을 학습시킨다. 그림자가 어떻게 지는지, 물체끼리 겹쳤을 때 경계면이 어떻게 보이는지 등 ’정밀도(Precision)’와 ’내용적 현실성’을 부여한다. “실제가 어떻게 생겼는지“를 가르친다.</li>
</ul>
<p>이 두 데이터의 결합은 DR이 제공하는 넓은 커버리지(High Variance)와 사실적 데이터가 제공하는 분포의 정확성(High Fidelity)을 모두 취하는 전략이다. 실험 결과, DR 데이터만 사용했을 때는 실제 이미지의 미묘한 텍스처 차이에 취약했고, 사실적 데이터만 사용했을 때는 시뮬레이터가 완벽하게 모사하지 못한 조명 조건에서 성능이 떨어졌다. 두 데이터를 결합함으로써 DOPE는 실제 데이터 한 장 없이도 YCB 데이터셋에서 SOTA 성능을 달성할 수 있었다.</p>
<h2>5.  구조적 도메인 무작위화 (Structured Domain Randomization, SDR)</h2>
<p>전통적인 도메인 무작위화(Vanilla DR)는 파라미터를 균등 분포(Uniform Distribution)에서 무작위로 샘플링한다. 이는 구현이 쉽고 탐색 공간이 넓다는 장점이 있지만, 물리적으로 불가능하거나 문맥에 전혀 맞지 않는 장면(예: 자동차가 하늘에 떠 있거나, 냉장고가 숲속에 있는 장면)을 생성한다. 이러한 데이터는 학습 효율을 떨어뜨릴 수 있다. 이를 개선하기 위해 **구조적 도메인 무작위화(SDR)**가 제안되었다.</p>
<h3>5.1  SDR의 핵심 원리: 제약 조건이 있는 무작위성</h3>
<p>SDR은 무작위성을 유지하되, 장면의 구조(Structure)와 문맥(Context)을 보존하는 확률적 제약 조건을 둔다.</p>
<ul>
<li><strong>위치 및 관계 제약 (Positional &amp; Relational Constraints):</strong> 객체는 중력의 영향을 받아 바닥면이나 다른 객체 위에 위치해야 한다. 자율주행 데이터의 경우 “자동차는 도로 위에, 보행자는 인도 위에“라는 문맥 제약이 적용된다. 빈 피킹에서는 “부품은 빈(Bin) 내부에 위치해야 한다“는 제약이 이에 해당한다.</li>
<li><strong>전역 파라미터와 종속성 (Global Parameters &amp; Dependency):</strong> 조명의 위치가 바뀌면 그림자의 방향도 물리적으로 일관성 있게 바뀌어야 한다. SDR은 장면 전체를 관장하는 전역 파라미터(예: 시간대 - 낮/밤)를 설정하고, 하위 요소들(하늘 색상, 조명 강도, 그림자 길이)이 이 전역 파라미터에 종속적으로 결정되도록 한다.</li>
<li><strong>확률적 배치 (Probabilistic Placement):</strong> 객체의 배치를 완전 무작위가 아닌, 조건부 확률 <span class="math math-inline">P(\text{Object} | \text{Context})</span>에 따라 결정한다.</li>
</ul>
<h3>5.2  6D 포즈 추정을 위한 SDR 파라미터 설정</h3>
<p>성공적인 6D 포즈 추정을 위해 SDR에서 무작위화해야 할 구체적인 파라미터들은 다음과 같다.</p>
<ul>
<li><strong>조명 (Lighting):</strong></li>
<li>광원의 개수: 1~5개 사이 무작위.</li>
<li>광원의 종류: 점 광원(Point), 직사광(Directional), 환경광(Ambient) 혼합.</li>
<li>위치: 상반구(Upper Hemisphere) 상에서 무작위 샘플링.</li>
<li>색상 및 강도: 백색광을 중심으로 약간의 색온도 변화 부여.</li>
<li><strong>텍스처 및 재질 (Material):</strong></li>
<li>객체 텍스처: 실제 객체의 텍스처를 유지하되, 색조(Hue), 채도(Saturation), 명도(Value)를 HSV 공간에서 소폭(<span class="math math-inline">\pm 10\% \sim 20\%</span>) 변동시켜 색상 보정(Color Calibration) 오류에 대비한다.</li>
<li>배경 텍스처: PBR 재질(금속, 나무, 플라스틱 등)을 무작위로 바닥과 벽면에 적용하거나, 실제 산업 현장 이미지를 투영한다.</li>
<li>반사율(Specularity) 및 거칠기(Roughness): 금속 부품의 경우 반사율을 무작위화하여 난반사와 정반사 상황을 모두 학습시킨다.</li>
<li><strong>카메라 및 광학 (Camera &amp; Optics):</strong></li>
<li>위치: 빈을 바라보는 작업 반경 내의 구면 좌표계(<span class="math math-inline">r, \theta, \phi</span>)에서 샘플링.</li>
<li>노이즈: 가우시안 노이즈(Gaussian Noise), 소금-후추 노이즈(Salt-and-Pepper), 모션 블러(Motion Blur) 추가.</li>
</ul>
<p>SDR을 적용하면 모델은 비현실적인 데이터에 낭비하는 용량을 줄이고, 실제 환경과 유사한 구조 내에서의 변동성을 학습하여 데이터 효율성(Sample Efficiency)과 수렴 속도를 높일 수 있다.</p>
<h2>6.  생성형 AI와 텍스처 합성: 생성형 도메인 무작위화</h2>
<p>최근 생성형 AI, 특히 **확산 모델(Diffusion Models)**과 <strong>ControlNet</strong> 의 등장은 합성 데이터 생성의 패러다임을 ’절차적 생성(Procedural Generation)’에서 ’생성적 합성(Generative Synthesis)’으로 전환시키고 있다. 이를 **생성형 도메인 무작위화(Generative Domain Randomization)**라고 부른다.</p>
<h3>6.1  텍스처 간극(Texture Gap)과 생성형 AI의 역할</h3>
<p>기존의 DR은 사전에 준비된 텍스처 이미지 라이브러리(예: Describable Textures Dataset)에서 이미지를 골라 객체에 매핑하는 방식이었다. 이는 텍스처의 다양성이 라이브러리 크기에 제한된다는 단점이 있다. 또한, 3D 메쉬와 텍스처의 매핑이 부자연스러울 수 있다. 확산 모델은 텍스트 프롬프트만으로 무한에 가까운 다양한 텍스처를 생성할 수 있어 이러한 한계를 극복한다.</p>
<h3>6.2  ControlNet을 활용한 기하학적 보존 텍스처링</h3>
<p>Sim-to-Real 포즈 추정 데이터 생성에서 가장 중요한 것은 <strong>이미지의 텍스처는 바꾸되, 6D 포즈 정답(Ground Truth)인 객체의 형상과 엣지는 절대 변형되면 안 된다</strong>는 것이다. 일반적인 Image-to-Image 변환(예: StyleGAN)은 스타일을 바꾸면서 객체의 모양을 왜곡시킬 위험이 있다.</p>
<p><strong>ControlNet</strong>은 이러한 문제를 해결하는 핵심 기술이다.</p>
<ul>
<li><strong>작동 원리:</strong> ControlNet은 사전 학습된 Stable Diffusion 모델의 인코더 블록을 복제(Trainable Copy)하고, 원본 블록은 동결(Locked Copy)한다. 두 블록은 ‘Zero Convolution’ 레이어로 연결된다. 이 구조는 초기 학습 단계에서 모델이 원본의 생성 능력을 잃지 않도록 보호하면서, 추가적인 조건(Condition)을 학습하게 한다.</li>
<li><strong>Depth/Canny Conditioning:</strong> 시뮬레이터에서 렌더링된 **깊이 맵(Depth Map)**이나 <strong>Canny Edge 맵</strong>을 ControlNet의 조건으로 입력한다. 이렇게 하면 확산 모델은 깊이 맵이 지시하는 3차원 구조와 엣지를 엄격하게 따르면서, 텍스트 프롬프트(예: “rusty metal gear part on oil-stained concrete floor”)에 맞는 텍스처와 조명 효과만을 생성해낸다.</li>
</ul>
<h3>6.3  스타일 전이(Style Transfer)를 통한 Sim-to-Real 적응</h3>
<p>이 기술은 실제 작업 환경의 데이터를 소량만 수집하여(라벨 불필요) 확산 모델을 미세 조정(Fine-tuning, 예: DreamBooth, LoRA)함으로써 더욱 강력해진다.</p>
<ol>
<li>실제 공장 바닥이나 조명 환경을 촬영한 이미지 10~20장으로 확산 모델에게 ’타겟 도메인의 스타일’을 학습시킨다.</li>
<li>시뮬레이터에서 렌더링된 단순한(Color-coded) 이미지와 깊이 맵을 입력으로 넣는다.</li>
<li>학습된 확산 모델이 시뮬레이션의 기하학적 구조는 유지한 채, 실제 공장의 조명과 텍스처 화풍을 입힌 이미지를 생성한다.</li>
<li>이 이미지는 시뮬레이터에서 생성한 정확한 6D 포즈 라벨을 그대로 사용할 수 있다.</li>
</ol>
<p>연구 결과 에 따르면, 이러한 생성형 텍스처링 방식을 사용했을 때, 기존의 무작위 텍스처 매핑보다 실제 데이터에 대한 포즈 추정 정확도가 유의미하게 향상되었음이 보고되었다.</p>
<h2>7.  자가 학습(Self-Training)과 반복적 적응 루프</h2>
<p>아무리 정교한 시뮬레이션과 생성형 AI를 사용하더라도 현실 간극을 완전히 0으로 만들 수는 없다. 따라서 최근에는 시뮬레이션에서 훈련된 모델(Teacher)을 사용하여 라벨이 없는 실제 데이터(Unlabeled Real Data)에 가상 라벨(Pseudo-label)을 부여하고, 이를 통해 모델(Student)을 재학습시키는 <strong>반복적 자가 학습(Iterative Self-Training)</strong> 기법이 표준적으로 사용된다.</p>
<h3>7.1 자가 학습 파이프라인</h3>
<ol>
<li><strong>초기 교사 모델 학습 (Warm-up):</strong> 앞서 논의한 DR, PBR, 생성형 AI 데이터를 활용하여 초기 모델을 시뮬레이션 데이터로만 훈련한다.</li>
<li><strong>가상 라벨링 (Pseudo-Labeling):</strong> 훈련된 교사 모델로 실제 환경의 비디오 스트림이나 이미지에서 객체의 포즈를 추론한다.</li>
<li><strong>신뢰도 검증 (Reliability Check &amp; Filtering):</strong> 추론된 포즈가 맞는지 틀린지 정답이 없으므로 검증이 필요하다.</li>
</ol>
<ul>
<li><strong>Render-and-Compare:</strong> 추론된 포즈 (<span class="math math-inline">R_{pred}, t_{pred}</span>)를 사용하여 3D CAD 모델을 렌더링한다. 이 렌더링된 이미지(마스크 또는 엣지)와 실제 이미지를 픽셀 단위로 비교(IoU, Edge Chamfer Distance 등)한다. 일치도가 높으면 해당 추론을 신뢰할 수 있는 가상 라벨로 채택한다.</li>
</ul>
<ol start="4">
<li><strong>학생 모델 재학습 (Student Re-training):</strong> 선별된 가상 라벨 데이터와 원본 합성 데이터를 혼합하여 새로운 학생 모델을 훈련한다. 이때 학생 모델에는 더 강한 데이터 증강(Augmentation)을 적용하여 강건성을 높인다.</li>
<li><strong>반복 (Iteration):</strong> 학생 모델은 다시 교사 모델이 되어 더 정확한 가상 라벨을 생성한다. 이 과정을 반복할수록 모델은 실제 도메인의 데이터 분포에 점진적으로 적응하게 된다.</li>
</ol>
<p>이 방식은 Chen et al. 의 연구에서 로봇 빈 피킹 성공률을 20% 가까이 향상시키는 결과를 보여주었으며, 수작업 라벨링 없이도 실제 데이터의 특징을 흡수할 수 있는 가장 강력한 방법론 중 하나이다.</p>
<h2>8.  결론</h2>
<p>6D 포즈 추정을 위한 Sim-to-Real 기술은 단순한 데이터 부족 문제를 넘어, 로봇 지능이 가상 세계의 무한한 경험을 물리 세계로 확장하는 핵심 메커니즘으로 진화했다.</p>
<p>본 장의 논의를 요약하면 다음과 같다.</p>
<ol>
<li><strong>데이터 전략의 다변화:</strong> 합성 데이터는 더 이상 ‘가짜’ 데이터가 아니다. **도메인 무작위화(DR)**는 모델에게 환경 변화에 대한 강건성(Invariance)을 부여하고, **사실적 렌더링(PBR)**은 정밀한 시각적 특징을 학습시키며, 이 둘의 하이브리드 전략(DOPE)이 가장 효과적임이 입증되었다.</li>
<li><strong>무작위화의 지능화:</strong> 무작위성은 통제되어야 한다. **구조적 도메인 무작위화(SDR)**를 통해 물리적, 문맥적 정합성을 유지해야 학습 효율을 높일 수 있다.</li>
<li><strong>생성형 기술의 도입:</strong> <strong>ControlNet</strong>과 같은 확산 모델 기반 텍스처링은 기하학적 정보를 보존하면서도 텍스처의 다양성을 비약적으로 높여 외형적 간극을 메우는 새로운 표준이 되고 있다.</li>
<li><strong>현실 적응의 자동화:</strong> <strong>자가 학습(Self-Training)</strong> 루프를 통해 모델은 사람의 개입 없이도 스스로 실제 데이터에 적응하며 진화한다.</li>
</ol>
<p>결론적으로, 성공적인 Sim-to-Real 6D 포즈 추정 시스템을 구축하기 위해서는 단일 기법에 의존하기보다, 고품질 시뮬레이터(Isaac Sim, BlenderProc)와 생성형 AI를 결합한 데이터 파이프라인, 그리고 자가 학습을 통한 지속적인 적응 전략을 통합적으로 운용해야 한다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for …, https://www.researchgate.net/publication/359971156_Sim-to-Real_6D_Object_Pose_Estimation_via_Iterative_Self-training_for_Robotic_Bin-picking</li>
<li>bridging the reality gap in 6D pose estimation for robotic grasping, https://pmc.ncbi.nlm.nih.gov/articles/PMC10565011/</li>
<li>Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for …, https://www.researchgate.net/publication/364641663_Sim-to-Real_6D_Object_Pose_Estimation_via_Iterative_Self-training_for_Robotic_Bin_Picking</li>
<li>BlenderProc: Reducing the Reality Gap with Photorealistic Rendering, https://www.semanticscholar.org/paper/BlenderProc%3A-Reducing-the-Reality-Gap-with-Denninger-Sundermeyer/87d2a16af38839f4970d5e75882ce293a3b45854</li>
<li>NVIDIA 6-DoF pose estimation trained on synthetic data, https://www.therobotreport.com/nvidia-grasping-system-synthetic-data/</li>
<li>DEMONSTRATION OF OBJECT RECOGNITION USING DOPE …, https://trepo.tuni.fi/bitstream/10024/137710/2/LinnosmaaEssi.pdf</li>
<li>Generating Images with Physics-Based Rendering for an Industrial …, https://www.mdpi.com/1424-8220/21/23/7901</li>
<li>Domain Randomization With Replicator, https://docs.nvidia.com/learning/physical-ai/getting-started-with-isaac-sim/latest/synthetic-data-generation-for-perception-model-training-in-isaac-sim/03-domain-randomization-with-replicator.html</li>
<li>Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for …, https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136990526.pdf</li>
<li>Domain Randomization in Machine Learning - Emergent Mind, https://www.emergentmind.com/topics/domain-randomization</li>
<li>Learning Domain Randomization Distributions for Training Robust …, https://arxiv.org/pdf/1906.00410</li>
<li>Nuisance variable - Wikipedia, https://en.wikipedia.org/wiki/Nuisance_variable</li>
<li>Provable Sim-to-Real Transfer via Offline Domain Randomization, https://arxiv.org/html/2506.10133v1</li>
<li>UNDERSTANDING DOMAIN RANDOMIZATION FOR SIM-TO-REAL …, https://openreview.net/pdf?id=T8vZHIRTrY</li>
<li>Domain Randomization for Sim2Real Transfer | Lil’Log, https://lilianweng.github.io/posts/2019-05-05-domain-randomization/</li>
<li>Domain Randomization: future of robust modeling | by Urwa Muaz, https://medium.com/data-science/domain-randomization-c7942ed66583</li>
<li>Domain Randomization Techniques - Emergent Mind, https://www.emergentmind.com/topics/domain-randomization-techniques</li>
<li>6D Object Pose Estimation Using Synthetic Training Data and Deep …, https://documentserver.uhasselt.be/bitstream/1942/41337/2/ed75ec02-f900-453a-99f6-d45891578162.pdf</li>
<li>Benchmark for 6D Object Pose Estimation (BOP) Scene Replication, https://dlr-rm.github.io/BlenderProc/examples/datasets/bop_scene_replication/README.html</li>
<li>From Hours to Minutes: A BlenderProc Toolkit for 6DOF Pose …, https://medium.com/@armando_genis/from-hours-to-minutes-a-blenderproc-toolkit-for-6dof-pose-estimation-datasets-efficientpose-dope-4c3bef7ef16a</li>
<li>Generate Synthetic Data for Deep Object Pose Estimation Training …, https://developer.nvidia.com/blog/generate-synthetic-data-for-deep-object-pose-estimation-training-with-nvidia-isaac-ros/</li>
<li>[isaacsim.replicator.domain_randomization] Isaac Sim … - NVIDIA, https://docs.isaacsim.omniverse.nvidia.com/4.5.0/py/source/extensions/isaacsim.replicator.domain_randomization/docs/index.html</li>
<li>Online Generation - Isaac Sim Documentation - NVIDIA, https://docs.isaacsim.omniverse.nvidia.com/4.5.0/replicator_tutorials/tutorial_replicator_online_generation.html</li>
<li>A fast monocular 6D pose estimation method for textureless objects …, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2024.1424036/full</li>
<li>Deep Object Pose Estimation for Semantic Robotic Grasping of …, http://proceedings.mlr.press/v87/tremblay18a/tremblay18a.pdf</li>
<li>Real world 3D object pose estimation and the Sim2Real gap - CS230, http://cs230.stanford.edu/projects_fall_2021/reports/103058918.pdf</li>
<li>DOPE-Plus: Enhancements in Feature Extraction and Data …, https://deeprob.org/w25/assets/projects/reports/DOPE-Plus/ROB599_Final_report.pdf</li>
<li>Structured Domain Randomization Makes Deep Learning More …, https://developer.nvidia.com/blog/structured-domain-randomization-makes-deep-learning-more-accessible/</li>
<li>(PDF) Structured Domain Randomization: Bridging the Reality Gap …, https://www.researchgate.net/publication/328494394_Structured_Domain_Randomization_Bridging_the_Reality_Gap_by_Context-Aware_Synthetic_Data</li>
<li>Structured Domain Randomization - Emergent Mind, https://www.emergentmind.com/topics/structured-domain-randomization</li>
<li>Deep Learning-Based 6-DoF Object Pose Estimation … - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC10748156/</li>
<li>Reviewing 6D Pose Estimation: Model Strengths, Limitations, and …, https://www.mdpi.com/2076-3417/15/6/3284</li>
<li>ControlNet – Achieving Superior Image Generation Results, https://learnopencv.com/controlnet/</li>
<li>Evaluating Text-to-Image Diffusion Models for Texturing Synthetic Data, https://arxiv.org/html/2411.10164v1</li>
<li>Generative Modeling for Sim-to-Real Transfer - Emergent Mind, https://www.emergentmind.com/topics/generative-modeling-for-simulation-to-real-transfer</li>
<li>ControlNet: The Breakthrough That Finally Gave Generative Models …, https://medium.com/@asadbukhari886/controlnet-the-breakthrough-that-finally-gave-generative-models-spatial-control-42cfc133e3a4</li>
<li>Stable-Sim2Real: Exploring Simulation of Real-Captured 3D Data …, https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth_ICCV_2025_paper.pdf</li>
<li>Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for …, https://www.semanticscholar.org/paper/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Chen-Cao/0d9d5d34d287db5e46ca628c95cf1b78fb04f2eb</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>