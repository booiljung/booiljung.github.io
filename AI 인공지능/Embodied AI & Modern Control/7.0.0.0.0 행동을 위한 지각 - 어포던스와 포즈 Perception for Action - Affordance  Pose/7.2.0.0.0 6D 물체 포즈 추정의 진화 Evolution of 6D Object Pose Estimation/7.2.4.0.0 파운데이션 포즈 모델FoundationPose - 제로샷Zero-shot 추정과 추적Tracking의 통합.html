<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.2.4 파운데이션 포즈 모델(FoundationPose): 제로샷(Zero-shot) 추정과 추적(Tracking)의 통합</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.2.4 파운데이션 포즈 모델(FoundationPose): 제로샷(Zero-shot) 추정과 추적(Tracking)의 통합</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <a href="index.html">7.2 6D 물체 포즈 추정의 진화 (Evolution of 6D Object Pose Estimation)</a> / <span>7.2.4 파운데이션 포즈 모델(FoundationPose): 제로샷(Zero-shot) 추정과 추적(Tracking)의 통합</span></nav>
                </div>
            </header>
            <article>
                <h1>7.2.4 파운데이션 포즈 모델(FoundationPose): 제로샷(Zero-shot) 추정과 추적(Tracking)의 통합</h1>
<p>로봇이 통제되지 않은 비정형 환경에서 범용적인 작업을 수행하기 위해서는 사전에 학습되지 않은 새로운 물체(Novel Object)를 즉각적으로 인식하고 그 6차원(6D) 자세를 정밀하게 추정하는 능력이 필수적이다. 기존의 인스턴스 레벨(Instance-Level) 포즈 추정 기술들은 특정 물체에 대해 장시간의 학습 과정을 요구하거나, 3D CAD 모델의 유무에 따라 상이한 알고리즘을 적용해야 하는 구조적 한계를 가지고 있었다. 이러한 배경에서 등장한 <strong>FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects</strong> 는 거대 언어 모델(LLM), 확산 모델(Diffusion Model), 그리고 트랜스포머(Transformer) 아키텍처를 융합하여, 단일 통합 프레임워크 내에서 제로샷(Zero-shot) 추정과 실시간 추적(Tracking)을 동시에 달성한 혁신적인 연구로 평가받는다. 본 절에서는 FoundationPose가 어떻게 기존의 모델 기반(Model-based) 접근과 비모델 기반(Model-free) 접근의 경계를 허물고, 로봇 지각 시스템의 새로운 파운데이션 모델(Foundation Model)로 자리 잡았는지 심층적으로 분석한다.</p>
<h2>1.  파운데이션 모델로의 패러다임 전환</h2>
<p>과거의 6D 포즈 추정 연구들은 제한된 데이터셋(예: LINEMOD, YCB-Video)에 포함된 소수의 객체에 과적합(Overfitting)되는 경향이 있었다. 이는 로봇이 공장 자동화와 같은 통제된 환경을 벗어나 가정이나 물류 창고와 같은 개방형 환경(Open-world)으로 진출하는 데 있어 가장 큰 걸림돌이었다. FoundationPose는 이러한 “롱테일(Long-tail)” 물체 문제를 해결하기 위해, 추론 단계에서 별도의 미세 조정(Fine-tuning) 없이 즉시 적용 가능한(Instant Test-time Application) 구조를 제안한다.</p>
<p>이 접근 방식의 핵심은 방대한 합성 데이터를 통한 강력한 일반화(Generalization) 능력 확보에 있다. 컴퓨터 비전 분야에서 CLIP이나 SAM(Segment Anything Model)이 보여준 제로샷 성능을 6D 포즈 추정 분야로 확장한 것으로, 이는 로봇이 처음 보는 물체라 할지라도 그 형상과 텍스처의 일반적인 특징을 이해하고 포즈를 추론할 수 있음을 의미한다.</p>
<h3>1.1  통합 프레임워크의 철학: 표현의 가교(Bridging Representations)</h3>
<p>FoundationPose의 가장 독창적인 기여는 3D CAD 모델이 주어지는 상황(Model-based)과 소수의 참조 이미지만 주어지는 상황(Model-free)을 하나의 파이프라인으로 통합했다는 점이다. 기존 연구들은 CAD 모델이 있을 때는 렌더링 기반의 최적화를, 이미지만 있을 때는 특징점 매칭이나 뷰포인트 분류를 사용하는 등 이분법적인 방법론을 취했다. 그러나 FoundationPose는 <strong>뉴럴 임플리시트 표현(Neural Implicit Representation)</strong>, 구체적으로는 객체 중심의 뉴럴 필드(Object-centric Neural Field)를 매개체로 사용하여 이 두 가지 입력을 동등한 수준의 중간 표현(Intermediate Representation)으로 변환한다.</p>
<p>이러한 통합은 하위(Downstream) 네트워크인 포즈 정제(Refinement) 및 점수(Score) 네트워크가 입력 데이터의 원천(Source)에 구애받지 않고 작동할 수 있게 한다. 즉, 네트워크 입장에서는 입력이 정교한 CAD 모델에서 렌더링된 것인지, 아니면 참조 이미지로부터 재구성된 뉴럴 필드에서 합성된 것인지 구분할 필요가 없다. 이는 시스템의 유연성을 극대화하며, 로봇 시스템 통합 관점에서 유지보수 및 확장성을 비약적으로 향상시킨다.</p>
<h2>2.  LLM과 생성형 AI를 활용한 데이터 파이프라인 구축</h2>
<p>강력한 일반화 성능을 달성하기 위해서는 데이터의 양과 질, 그리고 다양성이 전제되어야 한다. 하지만 현실 세계에서 정밀한 6D 포즈 라벨이 부착된 대규모 데이터를 확보하는 것은 비용적으로 불가능에 가깝다. FoundationPose는 이를 극복하기 위해 대규모 언어 모델(LLM)과 이미지 생성 모델을 결합한 새로운 합성 데이터 생성 파이프라인을 도입했다.</p>
<h3>2.1  LLM 기반 의미론적 텍스처 증강</h3>
<p>기존의 합성 데이터 생성 방식은 3D 형상(Shape) 데이터베이스는 풍부하게 활용할 수 있었으나, 텍스처(Texture)의 다양성 부족으로 인해 실제 환경과의 도메인 격차(Sim-to-Real Gap)를 좁히는 데 한계가 있었다. 단순히 랜덤한 색상을 입히거나 노이즈를 추가하는 수준으로는 복잡한 현실 세계의 재질감을 모사하기 어렵다.</p>
<p>FoundationPose는 이 문제를 해결하기 위해 다음과 같은 3단계 프로세스를 정립했다:</p>
<ol>
<li><strong>의미론적 이해(Semantic Understanding):</strong> 먼저 3D 모델(예: Objaverse, Google Scanned Objects)의 메타데이터나 카테고리 태그를 LLM(예: ChatGPT)에 입력하여 해당 객체의 의미론적 특성을 파악한다.</li>
<li><strong>프롬프트 엔지니어링(Prompt Generation):</strong> LLM에게 해당 물체에 적용될 수 있는 현실적이고 다양한 재질, 표면 상태, 스타일(예: “녹슨 금속 질감”, “오래된 나무 표면”, “화려한 패턴의 세라믹”)에 대한 텍스트 프롬프트를 생성하도록 요청한다. 이는 인간 엔지니어가 일일이 상상하기 힘든 수천, 수만 가지의 창의적인 텍스처 조합을 가능하게 한다.</li>
<li><strong>텍스처 합성(Texture Synthesis):</strong> 생성된 텍스트 프롬프트는 텍스트-투-이미지(Text-to-Image) 확산 모델에 입력되어 고해상도의 텍스처 맵을 생성한다. 이렇게 생성된 텍스처는 3D 모델의 UV 맵에 투영되어, 기하학적 형상은 유지하되 외관이 완전히 새로운 학습 데이터를 무한히 생성해낸다.</li>
</ol>
<h3>2.2  물리 기반 렌더링과 도메인 무작위화</h3>
<p>증강된 3D 자산들은 NVIDIA Isaac Sim과 같은 물리 시뮬레이터 내에서 배치되며, 레이 트레이싱(Ray Tracing) 기반의 물리 기반 렌더링(PBR)을 통해 학습 이미지가 생성된다. 이 과정에서 조명, 카메라 포즈, 배경 이미지, 방해물(Distractor)의 배치 등을 극도로 무작위화(Domain Randomization)하여 네트워크가 특정 환경적 요인이 아닌 물체 본연의 기하학적 특징에 집중하도록 유도한다.</p>
<p>이러한 과정을 통해 구축된 데이터셋은 약 540만 장의 이미지와 4만 1천 개 이상의 3D 모델을 포함하며, 이는 FoundationPose가 별도의 실세계 데이터 학습 없이도 SOTA 성능을 달성하게 하는 근간이 되었다.</p>
<h2>3.  심층 아키텍처 분석: 트랜스포머와 대조 학습</h2>
<p>FoundationPose의 추론 엔진은 CNN(Convolutional Neural Network)의 지역적 특징 추출 능력과 트랜스포머(Transformer)의 전역적 문맥 이해 능력을 결합한 하이브리드 구조를 채택하고 있다.</p>
<h3>3.1  포즈 가설 생성 (Pose Hypothesis Generation)</h3>
<p>추론의 첫 단계는 대략적인 포즈 가설을 생성하는 것이다. 입력 이미지에서 대상 물체의 2D 영역을 식별하기 위해 Mask R-CNN이나 CNOS 와 같은 2D 검출기를 사용한다. 검출된 2D 바운딩 박스와 깊이 정보를 바탕으로 이동(Translation) 성분을 초기화하고, 회전(Rotation) 성분은 SO(3) 공간을 균일하게 커버하도록 미리 정의된 회전군(Rotation Group)을 사용하여 다수의 가설을 샘플링한다. 이는 국소 최적해(Local Minima)에 빠지는 것을 방지하기 위한 전역 탐색(Global Search) 전략이다.</p>
<h3>3.2  포즈 정제 네트워크 (Refinement Network)</h3>
<p>생성된 초기 가설들은 정제 네트워크를 통해 반복적으로 수정된다. 이 과정은 “렌더링 후 비교(Render-and-Compare)” 전략을 따른다.</p>
<ul>
<li><strong>입력 처리:</strong> 현재 추정된 포즈 <span class="math math-inline">P_k</span>를 사용하여 3D 모델(또는 뉴럴 필드)을 렌더링한다. 렌더링된 RGB-D 이미지와 실제 관측된 RGB-D 이미지는 채널 방향으로 연결(Concatenate)되어 네트워크에 입력된다.</li>
<li><strong>분리된 업데이트(Disentangled Update):</strong> 네트워크는 현재 포즈와 정답 포즈 사이의 차이인 <span class="math math-inline">\Delta P</span>를 예측하는데, 이때 이동 업데이트 <span class="math math-inline">\Delta t</span>와 회전 업데이트 <span class="math math-inline">\Delta R</span>을 서로 분리하여 예측하는 구조를 가진다.</li>
<li>회전 업데이트: <span class="math math-inline">R_{new} = \Delta R \cdot R_{old}</span></li>
<li>이동 업데이트: <span class="math math-inline">t_{new} = t_{old} + \Delta t</span></li>
<li>이러한 분리된 접근은 회전 변화가 이동 추정에 미치는 비선형적인 영향을 최소화하여 학습의 수렴 속도와 안정성을 높인다.</li>
<li><strong>손실 함수:</strong> 정제 네트워크는 예측된 변위와 실제 변위 사이의 L2 손실을 최소화하도록 학습된다.</li>
</ul>
<h3>3.3  포즈 선택 네트워크와 계층적 비교 (Hierarchical Comparison)</h3>
<p>FoundationPose의 핵심 혁신 중 하나는 여러 포즈 가설 중 최적의 하나를 선택하는 <strong>점수 네트워크(Score Network)</strong> 의 설계에 있다. 이 네트워크는 트랜스포머의 셀프 어텐션(Self-Attention) 메커니즘을 활용하여 개별 포즈의 절대적 품질뿐만 아니라, 포즈 후보군 간의 상대적 우열을 평가한다.</p>
<ul>
<li>
<p><strong>트랜스포머 인코더:</strong> <span class="math math-inline">K</span>개의 포즈 가설에 대해 각각 추출된 특징 벡터 <span class="math math-inline">F \in \mathbb{R}^{K \times D}</span>는 트랜스포머 인코더에 입력된다. 셀프 어텐션 층은 모든 가설 간의 상호 정보를 교환(Mix)하여, 특정 가설이 전체 후보군 내에서 얼마나 기하학적으로 타당한지를 전체적인 맥락 속에서 판단하게 한다.</p>
</li>
<li>
<p><strong>포즈 조건부 삼중항 손실(Pose-Conditioned Triplet Loss):</strong> 학습 시에는 정답 포즈에 가까운 ’양성 샘플(Positive Sample)’과 먼 ’음성 샘플(Negative Sample)’을 정의하고, 양성 샘플의 점수가 음성 샘플보다 일정 마진(Margin) 이상 높게 나오도록 유도하는 대조 학습(Contrastive Learning)을 수행한다.<br />
<span class="math math-display">
L(i^+, i^-) = \max(S(i^-) - S(i^+) + \alpha, 0)
</span><br />
여기서 <span class="math math-inline">S(\cdot)</span>는 예측된 점수, <span class="math math-inline">\alpha</span>는 마진이다. FoundationPose는 입력 이미지가 포즈 가설에 따라 다르게 크롭(Crop)되어 앵커(Anchor)가 고정되지 않는 특성을 고려하여, 동적인 앵커를 사용하는 변형된 삼중항 손실을 제안했다. 이는 대칭성이 있는 물체나 유사한 뷰포인트가 많은 경우 발생할 수 있는 모호성(Ambiguity)을 해결하는 데 결정적인 역할을 한다.</p>
</li>
</ul>
<h2>4.  모델 프리(Model-free) 설정: 뉴럴 필드를 통한 온보딩</h2>
<p>CAD 모델이 없는 경우, FoundationPose는 소수의 참조 이미지를 사용하여 물체를 ’온보딩(Onboarding)’한다. 이 과정에서 <strong>Instant-NGP</strong> 와 같은 고속 NeRF 기술이 활용된다.</p>
<ul>
<li><strong>즉각적인 학습:</strong> 참조 이미지와 그에 따른 카메라 포즈가 주어지면, 해시 인코딩(Hash Encoding) 기반의 뉴럴 필드를 수 초에서 수 분 이내에 학습시킨다. 이렇게 학습된 뉴럴 필드는 임의의 시점에 대한 RGB 및 깊이 이미지를 렌더링할 수 있는 기능을 제공하며, 이는 기존의 메시(Mesh) 기반 렌더링 파이프라인을 완벽하게 대체한다.</li>
<li><strong>피드 포워드 vs 최적화:</strong> FoundationPose 자체는 사전 학습된 피드 포워드(Feed-forward) 네트워크이지만, 모델 프리 설정을 위한 뉴럴 필드 구축 과정은 테스트 시간 최적화(Test-time Optimization) 과정을 포함한다. 그러나 Instant-NGP의 효율성 덕분에 이 과정은 실제 로봇 운영 시나리오에서 허용 가능한 수준의 지연 시간 내에 수행된다.</li>
</ul>
<h2>5.  실험적 검증 및 벤치마크 분석</h2>
<p>FoundationPose는 BOP(Benchmark for 6D Object Pose Estimation) 챌린지를 비롯한 다양한 데이터셋에서 기존 SOTA 방법론들을 압도하는 성능을 입증했다.</p>
<h3>5.1  BOP 벤치마크 성과</h3>
<p>YCB-Video, T-LESS, LM-O 등 BOP의 주요 데이터셋에서 FoundationPose는 모델 기반 및 비모델 기반 설정 모두에서 최상위권의 성능을 기록했다.</p>
<table><thead><tr><th><strong>데이터셋</strong></th><th><strong>특성</strong></th><th><strong>기존 방법론 대비 성과</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>YCB-Video</strong></td><td>텍스처, 조명 변화</td><td><strong>MegaPose, CosyPose 능가</strong></td><td>합성 데이터만으로 학습했음에도 정답(GT)에 준하는 정확도 달성.</td></tr>
<tr><td><strong>T-LESS</strong></td><td>텍스처 없음, 대칭성</td><td><strong>1위 (Top-1)</strong></td><td>기하학적 특징만으로 대칭성 모호함을 효과적으로 해결.</td></tr>
<tr><td><strong>LM-O</strong></td><td>심한 가려짐</td><td><strong>경쟁 우위</strong></td><td>부분적인 가시성만으로 전체 포즈를 복원하는 강건성 입증.</td></tr>
</tbody></table>
<p>특히 T-LESS와 같이 텍스처가 없고 대칭성이 강한 산업용 부품 데이터셋에서의 높은 성능은 트랜스포머 기반의 점수 네트워크가 전역적 문맥을 고려하여 모호함을 해소하는 능력이 탁월함을 시사한다. 최신 벤치마크 결과에 따르면, FoundationPose는 AR(Average Recall) 지표에서 0.889 (YCB-V), 0.834 (T-LESS) 등을 기록하며 인스턴스별로 특화된(Instance-specific) 모델들과 대등하거나 그 이상의 성능을 보여주었다.</p>
<h3>5.2  로봇 파지(Grasping) 및 실세계 조작</h3>
<p>FoundationPose의 우수성은 단순한 시각적 추정을 넘어 실제 로봇의 물리적 상호작용 능력 향상으로 이어진다.</p>
<ul>
<li><strong>파지 성공률:</strong> 다양한 형태와 재질의 물체(투명 물체 포함)에 대해 수행된 실제 로봇 파지 실험에서 77.5%에서 최대 95.3%에 이르는 높은 성공률이 보고되었다. 특히 물체의 질량 중심(Center of Gravity)을 고려해야 하는 정밀 파지 작업에서도 안정적인 6D 포즈를 제공함으로써, 단순한 위치 추정을 넘어선 물리적 이해 기반의 조작을 가능하게 했다.</li>
<li><strong>추적 성능:</strong> 실시간 추적 시나리오에서 FoundationPose는 물체가 움직이거나 손에 의해 가려지는 상황에서도 강건하게 포즈를 유지한다. RTX 4090 GPU 기준 초당 수백 프레임의 추적 속도를, 엣지 디바이스인 Jetson Orin에서도 실시간성(30Hz 이상)을 확보하여 로봇 제어 루프에 직접 통합될 수 있음을 증명했다.</li>
</ul>
<h2>6.  결론 및 향후 전망</h2>
<p>FoundationPose는 6D 물체 포즈 추정 분야에서 ’파운데이션 모델’의 개념을 성공적으로 구현한 이정표적인 연구이다. 이 모델은 다음과 같은 기술적 진보를 통해 로봇 지각 시스템의 수준을 한 단계 끌어올렸다.</p>
<ol>
<li><strong>통합성:</strong> CAD 모델과 참조 이미지를 아우르는 단일 파이프라인을 통해 다양한 응용 시나리오에 유연하게 대응한다.</li>
<li><strong>일반화:</strong> LLM과 생성형 AI를 활용한 대규모 합성 데이터 학습을 통해, 본 적 없는 물체에 대해서도 제로샷으로 높은 성능을 보장한다.</li>
<li><strong>실용성:</strong> 복잡한 재학습 과정 없이 테스트 시간에 즉시 적용 가능한 구조와 엣지 디바이스에서의 실시간 구동 능력은 상용 로봇 시스템으로의 배포를 가속화한다.</li>
</ol>
<p>향후 FoundationPose는 반사나 투명 재질에 대한 한계를 극복하기 위해 멀티모달 센서 융합 기술과 결합하거나, 로봇의 능동적 지각(Active Perception) 행동과 연계되어 더욱 지능적인 형태로 진화할 것으로 전망된다. 이는 로봇이 미지의 환경에서 스스로 물체를 이해하고 조작하는 ’일반 범용 로봇(Generalist Robot)’의 실현을 앞당기는 핵심 기술이 될 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Unified 6D Pose Estimation and Tracking of Novel Objects - arXiv, https://arxiv.org/html/2312.08344v1</li>
<li>FoundationPose: Unified 6D Pose Estimation and Tracking … - NVlabs, https://nvlabs.github.io/FoundationPose/</li>
<li>FoundationPose: The Superpower of Seeing in 3D for Robots and AR!, https://medium.com/@elmo92/foundationpose-the-superpower-of-seeing-in-3d-for-robots-and-ar-8c3fc706e3bc</li>
<li>FoundPose: Unseen Object Pose Estimation with Foundation Features, https://www.researchgate.net/publication/385348345_FoundPose_Unseen_Object_Pose_Estimation_with_Foundation_Features</li>
<li>Unified 6D Pose Estimation and Tracking of Novel Objects, https://openaccess.thecvf.com/content/CVPR2024/papers/Wen_FoundationPose_Unified_6D_Pose_Estimation_and_Tracking_of_Novel_Objects_CVPR_2024_paper.pdf</li>
<li>UA-Pose: Uncertainty-Aware 6D Object Pose Estimation and … - arXiv, https://arxiv.org/html/2506.07996v1</li>
<li>[CVPR 2024 Highlight] FoundationPose: Unified 6D Pose Estimation …, https://github.com/NVlabs/FoundationPose</li>
<li>FoundationPose - NVIDIA NGC Catalog, https://catalog.ngc.nvidia.com/orgs/nvidia/teams/isaac/models/foundationpose</li>
<li>FoundationPose Model Card - NGC Catalog - NVIDIA, https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/foundationpose</li>
<li>Object Pose Estimation leveraging foundation model with geometric …, https://www.researchgate.net/publication/397700227_OPFormer_Object_Pose_Estimation_leveraging_foundation_model_with_geometric_encoding</li>
<li>BOP-Distrib: Revisiting 6D Pose Estimation Benchmarks for … - arXiv, https://arxiv.org/html/2408.17297v4</li>
<li>Leaderboards - BOP: Benchmark for 6D Object Pose Estimation, https://bop.felk.cvut.cz/leaderboards/</li>
<li>Results of grasping experiment (successful rate [%]). 12 trials for…, https://www.researchgate.net/figure/Results-of-grasping-experiment-successful-rate-12-trials-for-each-object-in-every_tbl1_365104008</li>
<li>Grasping Pose Estimation for Robots Based on Convolutional …, https://www.mdpi.com/2075-1702/11/10/974</li>
<li>Foundation Feature-Driven Online End-Effector Pose Estimation, https://arxiv.org/html/2503.14051v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>