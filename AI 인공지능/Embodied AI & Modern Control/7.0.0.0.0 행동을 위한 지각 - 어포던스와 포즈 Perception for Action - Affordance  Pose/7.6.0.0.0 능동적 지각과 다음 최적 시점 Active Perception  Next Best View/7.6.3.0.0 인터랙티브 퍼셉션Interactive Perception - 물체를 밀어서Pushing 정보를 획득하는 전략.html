<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.6.3 인터랙티브 퍼셉션(Interactive Perception): 물체를 밀어서(Pushing) 정보를 획득하는 전략</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.6.3 인터랙티브 퍼셉션(Interactive Perception): 물체를 밀어서(Pushing) 정보를 획득하는 전략</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <a href="index.html">7.6 능동적 지각과 다음 최적 시점 (Active Perception & Next Best View)</a> / <span>7.6.3 인터랙티브 퍼셉션(Interactive Perception): 물체를 밀어서(Pushing) 정보를 획득하는 전략</span></nav>
                </div>
            </header>
            <article>
                <h1>7.6.3 인터랙티브 퍼셉션(Interactive Perception): 물체를 밀어서(Pushing) 정보를 획득하는 전략</h1>
<p>로보틱스 분야, 특히 비정형 환경(Unstructured Environment)에서의 조작(Manipulation) 작업에 있어 ’지각(Perception)’의 패러다임은 지난 수십 년간 수동적인 관찰자(Passive Observer)에서 능동적인 행위자(Active Agent)로 급격히 진화해왔다. 고전적인 컴퓨터 비전(Computer Vision) 시스템이 정지된 이미지 내에서 패턴을 분석하고 정보를 추출하는 데 주력했다면, 현대의 로봇 지각 시스템은 환경과 물리적으로 상호작용함으로써 지각의 불확실성을 능동적으로 줄이고 숨겨진 정보를 드러내는 전략을 취한다. 본 절에서는 이러한 인터랙티브 퍼셉션(Interactive Perception, IP)의 핵심 동작 원리(Primitive)인 ’밀기(Pushing)’를 중심으로, 로봇이 어떻게 물리적 접촉을 통해 정보를 획득하고, 이를 정보 이론(Information Theory)과 최신 강화학습(Reinforcement Learning) 관점에서 어떻게 모델링하는지 심층적으로 기술한다. 특히 2024년과 2025년에 발표된 최신 연구 성과들을 바탕으로, 단순한 위치 변경을 넘어선 인지적 도구로서의 밀기 전략을 상세히 분석한다.</p>
<h2>1.  인터랙티브 퍼셉션의 철학적 배경과 정의</h2>
<p>인터랙티브 퍼셉션은 “지각을 용이하게 하기 위해 환경과 상호작용하는 과정“으로 정의된다. 이는 로봇이 환경을 수동적으로 감지하는 ’수동적 지각(Passive Perception)’이나, 센서의 파라미터(위치, 줌, 초점 등)만을 변경하여 더 나은 시점을 확보하려는 ’능동적 지각(Active Perception)’과는 구별되는 개념이다. 능동적 지각이 센서의 상태를 제어하여 정보 획득을 최적화하는 데 반해, 인터랙티브 퍼셉션은 환경의 상태(State) 자체를 변경시킨다는 점에서 근본적인 차이가 있다.</p>
<h3>1.1  생물학적 영감과 행동-지각 루프</h3>
<p>인터랙티브 퍼셉션의 이론적 근거는 생물학적 시스템의 인지 발달 과정에서 찾을 수 있다. 헬드(Held)와 하인(Hein)의 유명한 새끼 고양이 실험(1963)은 시각적 발달이 단순히 시각 자극을 수용하는 것이 아니라, 자가 생성된 움직임(Self-produced movement)과 그에 따른 시각적 피드백 사이의 관계를 학습하는 데에 달려 있음을 입증했다. 이 실험에서 능동적으로 움직이며 시각 정보를 획득한 고양이는 정상적인 깊이 지각 능력을 발달시킨 반면, 수동적으로 이동된 고양이는 그렇지 못했다. 이는 로봇 공학에서도 동일하게 적용된다. 로봇이 세상의 구조를 이해하기 위해서는 단순히 카메라로 세상을 바라보는 것을 넘어, 세상을 ‘건드려보고(Poke)’, ‘밀어보고(Push)’, 그 반응을 관찰해야 한다.</p>
<h3>1.2  정적 비전의 한계와 상호작용의 필요성</h3>
<p>가정, 물류 창고, 재난 현장과 같은 비정형 환경에서는 물체들이 무질서하게 쌓여 있거나(Clutter), 특정 물체가 다른 물체에 의해 가려지는 폐색(Occlusion) 현상이 빈번하게 발생한다. 정적 카메라 시스템은 이러한 상황에서 다음과 같은 본질적인 한계에 직면한다.</p>
<ul>
<li><strong>폐색(Occlusion):</strong> 카메라 시야에서 가려진 물체의 존재, 형상, 포즈를 파악할 수 없다. 이는 로봇이 파지(Grasping) 계획을 세우는 데 있어 치명적인 정보 부재를 야기한다.</li>
<li><strong>시각적 모호성(Visual Ambiguity):</strong> 텍스처가 없거나 색상이 유사한 물체들이 겹쳐 있을 경우, 이를 개별 물체로 분할(Segmentation)하기 어렵다. 예를 들어, 흰 테이블 위에 놓인 흰색 컵이나, 서로 겹쳐 있는 동일한 색상의 상자들은 시각 정보만으로는 경계를 구분하기가 불가능에 가깝다.</li>
<li><strong>물리적 속성 불가지성(Unobservability of Physical Properties):</strong> 물체의 질량(Mass), 마찰 계수(Friction Coefficient), 무게 중심(Center of Mass), 강성(Stiffness) 등은 시각 정보만으로는 추론이 불가능하거나 매우 부정확하다. 이러한 속성들은 물체를 안정적으로 조작하기 위해 필수적인 정보이다.</li>
</ul>
<p>이러한 상황에서 ‘밀기(Pushing)’ 동작은 가장 기본적이면서도 강력한 정보 획득 전략이 된다. 밀기 동작은 파지(Grasping)와 달리 복잡한 기하학적 조건을 요구하지 않으며(예: 손가락이 들어갈 공간이 필요 없음), 물체의 위치를 변경하여 폐색을 해소하거나, 물체의 움직임을 유발하여 물리적 속성을 드러내는 풍부한 감각 신호(Sensory Signal)를 생성한다.</p>
<h3>1.3  능동적 지각과 인터랙티브 퍼셉션의 비교 분석</h3>
<p>학계에서는 능동적 지각과 인터랙티브 퍼셉션을 구분하거나 혼용하여 사용하기도 하지만, 본 서적에서는 Bohg et al.(2017)의 분류를 따라 이 둘을 명확히 구분한다. 다음 표는 두 접근 방식의 차이를 구조적으로 요약한 것이다.</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>능동적 지각 (Active Perception)</strong></th><th><strong>인터랙티브 퍼셉션 (Interactive Perception)</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 행위</strong></td><td>센서 이동 (Move Sensor), 파라미터 조절</td><td>환경 조작 (Manipulate Environment), 물리적 접촉</td></tr>
<tr><td><strong>환경 상태</strong></td><td>불변 (Static Environment) 가정</td><td>가변 (Dynamic Environment) 유도</td></tr>
<tr><td><strong>주요 목적</strong></td><td>시점(Viewpoint) 최적화, 사각지대 해소</td><td>폐색 제거, 물체 분리(Singulation), 속성 식별</td></tr>
<tr><td><strong>정보 획득 원리</strong></td><td>관측 위치 변경을 통한 기하학적 정보 획득</td><td>물리적 상호작용에 의한 동역학적 신호 생성</td></tr>
<tr><td><strong>불확실성 처리</strong></td><td>3D 형상 및 포즈의 불확실성 감소</td><td>물리적 속성 및 가려진 상태의 불확실성 감소</td></tr>
<tr><td><strong>대표 알고리즘</strong></td><td>Next-Best-View (NBV), Frontier Exploration</td><td>Push-for-Info, Interactive Segmentation</td></tr>
<tr><td><strong>적용 사례</strong></td><td>3D 재구성, SLAM, 자율 주행 탐색</td><td>빈 피킹(Bin Picking), 도구 사용, 미지 물체 탐색</td></tr>
</tbody></table>
<p>이 표에서 볼 수 있듯이, 인터랙티브 퍼셉션은 능동적 지각을 포함하는 더 넓은 개념으로 볼 수도 있으나, **물리적 접촉(Physical Contact)**을 통해 환경을 변화시킨다는 점이 핵심적인 차별점이다.</p>
<h2>2.  정보 이론적 정식화 (Information Theoretic Formulation)</h2>
<p>로봇이 무작위로 물체를 미는 것이 아니라, ‘정보를 얻기 위해’ 의도적으로 행동하려면, 정보의 가치를 정량화할 수 있는 수학적 프레임워크가 필요하다. 이를 위해 정보 이론(Information Theory)의 섀넌 엔트로피(Shannon Entropy)와 상호 정보량(Mutual Information) 개념이 로봇의 의사결정 과정에 도입된다. 로봇은 행동 <span class="math math-inline">a_t</span>를 수행함으로써 관측 <span class="math math-inline">z_{t+1}</span>을 획득하고, 이를 통해 환경에 대한 믿음(Belief State) <span class="math math-inline">b_t</span>를 갱신한다. 이 과정에서 로봇의 목표는 작업 성공뿐만 아니라, 환경에 대한 정보 이득(Information Gain)을 최대화하는 행동을 선택하는 것이다.</p>
<h3>2.1  상태 불확실성의 엔트로피 표현</h3>
<p>환경의 상태 <span class="math math-inline">S</span>에 대한 불확실성은 확률 분포 <span class="math math-inline">P(S)</span>의 엔트로피 <span class="math math-inline">H(S)</span>로 표현된다. 여기서 상태 <span class="math math-inline">S</span>는 문제의 정의에 따라 다양하게 설정될 수 있다. 예를 들어, 점유 그리드 맵(Occupancy Grid Map)의 각 셀(Cell)의 상태(점유됨/비어있음/미지), 물체의 6자유도 포즈(6D Pose), 또는 물체의 물리적 파라미터 등이 될 수 있다.<br />
<span class="math math-display">
H(S) = - \sum_{s \in S} P(s) \log P(s)
</span><br />
연속적인 공간(Continuous Space)에서의 엔트로피(Differential Entropy)는 적분 형태로 표현된다.<br />
<span class="math math-display">
H(S) = - \int P(s) \ln P(s) ds
</span><br />
엔트로피 값이 높다는 것은 해당 상태에 대한 확률 분포가 넓게 퍼져 있어, 로봇이 현재 상태를 정확히 알지 못함을 의미한다. 반대로 엔트로피가 낮으면 분포가 특정 값에 집중되어 있어 확신(Confidence)이 높음을 뜻한다.</p>
<h3>2.2  정보 이득(Information Gain)과 행동 선택</h3>
<p>로봇이 특정 밀기 행동 <span class="math math-inline">A</span>를 수행하고 센서 관측 <span class="math math-inline">Z</span>를 얻었을 때, 획득할 것으로 기대되는 정보량, 즉 정보 이득(IG)은 행동 전의 엔트로피와 행동 후의 조건부 엔트로피(Conditional Entropy)의 차이로 정의된다. 이는 행동 <span class="math math-inline">A</span>를 통해 감소시킬 수 있는 불확실성의 양을 나타낸다.<br />
<span class="math math-display">
IG(S, A) = H(S) - H(S|Z, A)
</span><br />
그러나 로봇은 행동을 수행하기 전에는 실제 관측 <span class="math math-inline">Z</span>를 알 수 없으므로, 가능한 모든 관측 <span class="math math-inline">z</span>에 대한 기대치(Expectation)를 계산해야 한다. 이를 기대 정보 이득(Expected Information Gain)이라 한다.<br />
<span class="math math-display">
E = H(S) - E_{z \sim P(Z|S, A)}
</span></p>
<p><span class="math math-display">
E = H(S) - \sum_{z} P(z|A) H(S|z)
</span></p>
<p>따라서, 정보 획득을 목적으로 하는 로봇의 정책(Policy) <span class="math math-inline">\pi</span>는 가능한 행동 후보군 <span class="math math-inline">\mathcal{A} = \{a_1, a_2, \dots, a_n\}</span> 중에서 기대 정보 이득을 최대화하는 행동 <span class="math math-inline">a^*</span>를 선택하는 최적화 문제로 귀결된다.<br />
<span class="math math-display">
a^* = \operatorname*{argmax}_{a \in \mathcal{A}} E
</span></p>
<h3>2.3  상호 정보량(Mutual Information)의 활용</h3>
<p>정보 이득은 상호 정보량(Mutual Information, MI) <span class="math math-inline">I(S; Z)</span>와 수학적으로 동치이다. 상호 정보량은 변수 <span class="math math-inline">S</span>와 <span class="math math-inline">Z</span> 사이의 통계적 의존성을 측정하며, 관측 <span class="math math-inline">Z</span>를 알게 됨으로써 상태 <span class="math math-inline">S</span>에 대해 얻게 되는 정보의 양을 의미한다.<br />
<span class="math math-display">
I(S; Z) = H(S) - H(S|Z) = H(Z) - H(Z|S)
</span><br />
이 식의 두 번째 변형인 <span class="math math-inline">H(Z) - H(Z|S)</span>는 계산상 유용한 통찰을 제공한다. <span class="math math-inline">H(Z)</span>를 최대화한다는 것은 다양한 관측 결과가 나올 수 있는, 즉 예측하기 어려운 결과를 초래하는 행동을 선택한다는 것을 의미하며(호기심과 연결), <span class="math math-inline">H(Z|S)</span>를 최소화한다는 것은 상태 <span class="math math-inline">S</span>가 주어졌을 때 관측 <span class="math math-inline">Z</span>의 노이즈가 적은, 즉 신뢰할 수 있는 센서 데이터를 얻을 수 있는 행동을 선택함을 의미한다.</p>
<h3>2.4  점유 그리드 맵에서의 적용 (Occupancy Grid Mapping)</h3>
<p>밀기 동작을 통한 공간 탐색은 종종 3차원 공간을 2차원 또는 3차원 그리드로 분할한 점유 그리드 맵(Occupancy Grid Map) 상에서 모델링된다. 계산의 복잡도를 줄이기 위해 각 그리드 셀 <span class="math math-inline">c_i</span>는 서로 독립적인(Independent) 확률변수로 가정하는 것이 일반적이다. 이 경우 전체 맵 <span class="math math-inline">M</span>의 엔트로피는 개별 셀 엔트로피의 합으로 근사된다.<br />
<span class="math math-display">
H(M) \approx \sum_{i=1}^{N} H(c_i)
</span><br />
각 셀 <span class="math math-inline">c_i</span>는 이진 상태(점유됨/비어있음)를 가지므로, 베르누이 분포의 엔트로피 공식을 따른다.<br />
<span class="math math-display">
H(c_i) = - [p(c_i) \log p(c_i) + (1-p(c_i)) \log (1-p(c_i))]
</span><br />
여기서 <span class="math math-inline">p(c_i)</span>는 셀 <span class="math math-inline">i</span>가 점유되어 있을 확률이다. <span class="math math-inline">p(c_i) = 0.5</span>일 때(전혀 모를 때) 엔트로피가 최대가 되며, 0 또는 1에 가까워질수록 엔트로피는 0으로 수렴한다. 로봇이 물체를 밀어서 숨겨진 공간(Unknown space)을 드러내거나 물체를 치우면, 레이캐스팅(Ray-casting)을 통해 해당 영역의 셀 확률이 갱신(Update)되고 전체 맵의 엔트로피가 감소한다. 최근 연구에서는 이러한 독립적 셀 가정(Independence Assumption)이 구조적 정보(물체의 연속성 등)를 무시한다는 한계를 극복하기 위해, 딥러닝 기반의 보정된 신경망(Calibrated Neural-Accelerated Belief Updates)이나 가우시안 프로세스(Gaussian Process)를 사용하여 셀 간의 상관관계를 반영하고 맵의 불확실성을 더 정확하게 추정하는 기법들이 제안되고 있다.</p>
<h2>3.  폐색 제거와 객체 분할을 위한 밀기 전략 (Pushing for Occlusion Removal and Singulation)</h2>
<p>비정형 환경에서 로봇 파지(Grasping)의 실패 원인 중 가장 큰 비중을 차지하는 것은 클러터(Clutter)로 인한 공간 부족과 폐색이다. 여러 물체가 겹쳐 있거나 인접해 있을 때, 로봇 그리퍼(Gripper)의 핑거(Finger)가 진입할 공간이 없거나(Collision), 비전 시스템이 개별 물체를 구분하지 못해 잘못된 파지 자세(Grasp Pose)를 생성하게 된다. 이를 해결하기 위한 전략이 ’싱귤레이션(Singulation)’이며, 이는 밀기 동작을 통해 뭉쳐 있는 물체들을 서로 분리시키는 작업을 의미한다.</p>
<h3>3.1  비선형 밀기 정책 (Nonlinear Pushing Policy)</h3>
<p>Won et al.(2019)의 연구 “Object Singulation by Nonlinear Pushing for Robotic Grasping“은 복잡하게 쌓인 물체들을 효과적으로 분리하기 위해 비선형(Nonlinear) 밀기 경로를 제안하였다. 기존의 직선 밀기(Linear Pushing)는 물체를 단순히 이동시키는 데 그치지만, 비선형 경로는 물체의 회전을 유도하거나 주변 물체와의 얽힘을 푸는 데 더 유리할 수 있다.</p>
<ul>
<li>
<p><strong>알고리즘:</strong> 이 연구에서는 밀기 동작을 생성하기 위해 “Push Proposal Network“를 사용한다. 이 네트워크는 현재의 깊이 이미지(Depth Image)와 분할 마스크를 입력받아, 물체들을 최대한 분산시킬 수 있는 밀기 시작점과 곡선 경로를 출력한다.</p>
</li>
<li>
<p><strong>보상 함수 설계:</strong> 강화학습을 위한 보상 함수 <span class="math math-inline">R</span>은 물체들 사이의 거리 증가와 곂침(Overlap) 영역의 감소, 그리고 작업 공간 밖으로 물체가 나가는 것을 방지하는 페널티 항으로 구성된다.</p>
<p><span class="math math-display">R = \Delta (\text{Separation}) - \text{Penalty}(\text{Out of Bounds})</span></p>
<p>실험 결과, 비선형 밀기는 직선 밀기에 비해 더 적은 횟수의 시도로 높은 싱귤레이션 성공률을 달성함을 보였다.</p>
</li>
</ul>
<h3>2. 푸시-그라스프 시너지 (Push-Grasp Synergy)</h3>
<p>과거에는 밀기와 파지를 별개의 작업으로 간주했으나, 최신 연구들은 이를 상호 보완적인 시너지(Synergy) 관계로 모델링한다. 즉, 밀기는 그 자체로 목적이 아니라, 파지 확률(Graspability)을 높이기 위한 전처리 과정(Pre-condition)으로 수행된다.</p>
<ul>
<li><strong>VPG (Visual Pushing and Grasping):</strong> Zeng et al.(2018)은 DQN(Deep Q-Network)을 사용하여 밀기와 파지의 Q-value 맵을 동시에 학습하는 프레임워크를 제안했다. 여기서 밀기 네트워크는 “파지 성공률을 높이는 위치“를 찾아내도록 학습된다. 즉, 밀기 행동의 가치 <span class="math math-inline">Q_{push}(s, a)</span>는 미래에 기대되는 최대 파지 보상 <span class="math math-inline">\max_{a&#39;} Q_{grasp}(s&#39;, a&#39;)</span>과 연결된다.</li>
<li><strong>Equivariant PushGrasp (EPG):</strong> 2025년 Hu et al.이 제안한 “Equivariant PushGrasp” 네트워크는 기하학적 대칭성을 활용하여 학습 효율을 극대화하였다. 이 연구는 로봇의 조작 작업이 회전 대칭성(Rotational Symmetry)을 가진다는 점에 착안하여, SE(2) 등변성(Equivariance)을 보장하는 신경망을 설계했다. 이는 데이터 증강(Augmentation) 없이도 회전된 모든 상황에 대한 밀기-파지 관계를 추론할 수 있게 하여, 샘플 효율성을 획기적으로 개선했다.</li>
</ul>
<h3>3. 분할을 위한 상호작용 (Interactive Segmentation)</h3>
<p>시각적 분할(Visual Segmentation)이 불확실할 때, 로봇은 의심되는 영역을 밀어봄으로써 물리적 객체의 경계를 확인한다. 이를 “Interactive Segmentation“이라 하며, 다음과 같은 단계로 진행된다.</p>
<ol>
<li><strong>가설 생성 (Hypothesis Generation):</strong> 로봇은 현재의 시각 정보를 바탕으로 객체 경계에 대한 가설을 세운다. 예를 들어, 색상이 유사한 두 영역이 하나의 물체인지, 인접한 두 물체인지에 대한 확률적 모델을 생성한다.</li>
<li><strong>행동 선택 (Action Selection):</strong> 가설의 불확실성이 가장 높은 영역(예: 두 영역의 경계선)을 타겟으로 하여 밀기 동작을 수행한다. 이때 정보 이득을 최대화하는 방향으로 밀기 벡터를 결정한다.</li>
<li><strong>모션 분석 및 검증 (Motion Analysis &amp; Verification):</strong> 밀기 후, 두 영역이 함께 움직이면(Rigid Motion Consistency) 하나의 물체로 판단하고, 서로 다른 방향이나 속도로 움직이면 별개의 물체로 판단하여 분할 맵을 갱신한다.</li>
</ol>
<h2>7.6.3.4 물리적 속성 추정을 위한 탐색적 밀기 (Exploratory Pushing)</h2>
<p>시각 정보만으로는 알 수 없는 물체의 질량(Mass), 마찰 계수(Friction Coefficient), 무게 중심(Center of Mass, COM), 강성(Stiffness) 등은 정확한 조작을 위해 필수적이다. 로봇은 “탐색적 작용(Exploratory Action)“으로서의 밀기를 수행하여 이러한 파라미터를 식별(System Identification)한다.</p>
<h3>1. 물리 엔진 기반 추론 (Physics-based Inference)</h3>
<p>물체의 움직임은 뉴턴-오일러 방정식과 같은 물리 법칙을 따르므로, 로봇은 관측된 움직임(변위, 회전)과 입력된 힘 사이의 관계를 역으로 추적하여 물리 파라미터를 추정할 수 있다.</p>
<ul>
<li><strong>준정적 가정(Quasi-static Assumption):</strong> 저속으로 밀 때 관성력을 무시하고 마찰력과 미는 힘의 평형만을 고려하는 모델이다. 이는 계산을 단순화하지만, 동적 효과가 큰 상황에서는 오차가 발생한다. Mason의 투표 정리(Voting Theorem)에 따르면, 물체를 밀 때 회전 방향은 접촉점과 마찰 중심(Center of Friction) 간의 관계에 의해 결정되므로, 이를 통해 마찰 중심의 위치를 좁혀나갈 수 있다.</li>
<li><strong>확률적 물리 모델:</strong> 미지의 마찰 분포나 바닥 표면의 거칠기 등을 확률 변수로 모델링하고, 베이지안 필터(Bayesian Filter)나 입자 필터(Particle Filter)를 통해 파라미터의 사후 분포(Posterior)를 추정한다.</li>
</ul>
<h3>2. 데이터 기반 학습과 비디오 예측 (Data-driven Learning &amp; Video Prediction)</h3>
<p>복잡한 형상이나 액체, 입자상 물질(Granular Media)과 같이 명시적인 물리 모델을 수립하기 어려운 대상에 대해서는 딥러닝 기반의 접근이 효과적이다.</p>
<ul>
<li><strong>전방 모델(Forward Model):</strong> 현재 상태 <span class="math math-inline">s_t</span>와 행동 <span class="math math-inline">a_t</span>가 주어졌을 때, 다음 상태 <span class="math math-inline">s_{t+1}</span>을 예측하는 신경망 모델 <span class="math math-inline">f(s_t, a_t) \rightarrow s_{t+1}</span>을 학습한다. 이 모델의 잠재 공간(Latent Space)에는 물체의 물리적 속성이 임베딩(Embedding)된다. 로봇은 이 모델을 사용하여 다양한 밀기 행동의 결과를 시뮬레이션(상상)하고, 원하는 결과를 얻기 위한 최적의 행동을 계획할 수 있다.</li>
<li><strong>비디오 예측:</strong> 픽셀 공간에서 미래의 이미지를 직접 생성하는 모델이다. 이는 물체의 형태나 텍스처 변화, 유체의 흐름까지 예측할 수 있어 비강체(Deformable Object) 조작에 유리하다.</li>
</ul>
<h2>7.6.3.5 내재적 동기와 호기심 기반 탐색 (Intrinsic Motivation and Curiosity)</h2>
<p>2025년 최신 연구 동향 중 가장 주목할 만한 변화는 로봇에게 외부의 구체적인 목표(Extrinsic Reward, 예: “물체를 옮겨라”) 대신, 자신의 지식을 확장하려는 “내재적 동기(Intrinsic Motivation)“를 부여하여 스스로 환경을 탐색하게 하는 것이다.</p>
<h3>1. “Push, See, Predict” 프레임워크</h3>
<p>Konstantaropoulos et al.(2025)이 제안한 “Push, See, Predict” 연구는 호기심(Curiosity) 기반의 자가지도 학습(Self-Supervised Learning)을 통해 로봇이 물체 중심의 지각(Object-Centric Perception) 능력을 창발적으로 획득함을 보였다.</p>
<ul>
<li>
<p><strong>월드 모델 (World Model):</strong> 로봇은 자신의 행동에 따른 환경 변화를 예측하는 월드 모델을 학습한다. 이 모델은 잠재 공간(Latent Space) 상에서 작동하며, 물체의 영속성(Object Permanence)과 물리 법칙을 내재화한다.</p>
</li>
<li>
<p><strong>내재적 보상 (Intrinsic Reward):</strong> 로봇의 보상 함수는 외부 작업의 성공 여부가 아니라, 월드 모델의 <strong>예측 오차(Prediction Error)</strong> 또는 **예측 불확실성(Predictive Uncertainty)**에 비례하여 설정된다. 즉, 로봇은 “결과를 예측하기 어려운” 상황을 일부러 찾아다니며 행동한다.<br />
<span class="math math-display">
R_{int} \propto \| \hat{s}_{t+1} - s_{t+1} \|^2
</span><br />
또는 베이지안 관점에서 정보 이득(KL-Divergence)을 보상으로 사용한다.<br />
<span class="math math-display">
R_{int} = D_{KL}(P(\theta|D \cup {d_{new}}) || P(\theta|D))
</span></p>
</li>
<li>
<p><strong>학습 과정:</strong> 로봇은 초기에는 무작위로 밀기 동작을 수행하지만, 점차 물체가 구르거나 충돌하는 복잡한 상황, 즉 자신의 예측 모델이 틀리는 상황을 의도적으로 생성하여 데이터를 수집한다. 이를 통해 별도의 인간 레이블링 없이도 정교한 물리적 직관을 학습하게 된다.</p>
</li>
</ul>
<h3>2. 학습 진척도(Learning Progress)와 “Noisy TV” 문제</h3>
<p>단순히 예측 오차만을 보상으로 사용하면, 로봇이 예측 불가능한 화이트 노이즈(예: 고장 난 TV 화면)만 계속 바라보는 “Noisy TV Problem“에 빠질 수 있다. 이를 방지하기 위해, 오차의 절대량이 아닌 **오차의 감소율(Learning Progress)**을 보상으로 사용하는 기법들이 적용된다. 즉, “내가 지금 배우고 있는가?“가 탐색의 동기가 되며, 더 이상 배울 것이 없는(예측이 완벽하거나 아예 불가능한) 영역은 탐색을 멈추게 된다.</p>
<h2>7.6.3.6 딥 강화학습을 이용한 밀기 전략 구현 (Deep RL Implementation)</h2>
<p>실제 시스템 구현에서 밀기 전략은 주로 심층 강화학습(Deep Reinforcement Learning, DRL)을 통해 학습된다. 이 섹션에서는 구현을 위한 구체적인 아키텍처와 보상 함수 설계를 다룬다.</p>
<h3>1. 상태 및 행동 공간 정의 (State and Action Space)</h3>
<ul>
<li><strong>상태 공간 (<span class="math math-inline">S</span>):</strong> 일반적으로 RGB-D 카메라에서 얻은 컬러 이미지와 깊이 이미지가 사용된다. 이를 높이 맵(Height Map)으로 변환하여 3D 구조 정보를 보존하거나, 객체별 분할 마스크를 추가 채널로 입력하기도 한다. 최근에는 시각적 어포던스(Visual Affordance) 맵 형태로 가공하여 “어디를 밀 수 있는가“에 대한 픽셀별 확률을 입력으로 사용한다.</li>
<li><strong>행동 공간 (<span class="math math-inline">A</span>):</strong> 밀기의 시작점 <span class="math math-inline">(x, y)</span>, 밀기 방향 <span class="math math-inline">\theta</span>, 밀기 거리 <span class="math math-inline">l</span>로 정의된다. 행동 공간이 연속적일 경우 DDPG(Deep Deterministic Policy Gradient)나 SAC(Soft Actor-Critic) 알고리즘이 사용되며, 픽셀 단위로 이산화(Discretize)할 경우 FCN(Fully Convolutional Network) 기반의 Q-learning이 사용된다.</li>
</ul>
<h3>2. 보상 함수 엔지니어링 (Reward Function Engineering)</h3>
<p>밀기 동작의 효율성은 보상 함수의 설계에 크게 좌우된다.</p>
<ul>
<li>
<p><strong>싱귤레이션 목적:</strong> 물체 분리가 목표일 때 보상은 클러스터링 품질의 변화량으로 정의된다.<br />
<span class="math math-display">
R = \alpha \cdot \Delta (\text{Number of Singulated Objects}) + \beta \cdot \Delta (\text{Max Cluster Size})
</span><br />
물체 간의 거리가 멀어지거나, 가장 큰 뭉텅이(Cluster)의 크기가 줄어들수록 양의 보상을 받는다.</p>
</li>
<li>
<p><strong>정보 획득 목적:</strong> 맵 작성이나 탐색이 목표일 때, 엔트로피 감소량을 직접적인 보상으로 사용한다.<br />
<span class="math math-display">
R = IG(Map) - \text{Cost}(Motion)
</span><br />
여기서 <span class="math math-inline">\text{Cost}(Motion)</span>은 이동 거리나 시간에 대한 비용으로, 불필요한 움직임을 억제한다.</p>
</li>
<li>
<p><strong>희소 보상 문제 해결 (Sparse Reward Problem):</strong> 파지 성공과 같이 최종 목표 달성 시에만 보상을 주면 학습이 매우 더디다. 따라서 밀기 동작이 파지 가능성(Graspability Score)을 얼마나 높였는지를 보조 보상(Auxiliary Reward)으로 제공하는 방식이 널리 사용된다. 예를 들어, 파지 네트워크 <span class="math math-inline">Q_{grasp}</span>가 예측한 최고 파지 점수가 밀기 후에 상승했다면, 그 상승분 <span class="math math-inline">Q_{grasp}(s&#39;) - Q_{grasp}(s)</span>를 밀기 행동의 보상으로 지급한다.</p>
</li>
</ul>
<h3>3. 마스킹과 어텐션 메커니즘 (Masking and Attention)</h3>
<p>효율적인 학습을 위해 전체 이미지를 처리하는 대신, 관심 영역(ROI)에 집중하는 어텐션 메커니즘이나 이미지 마스킹(Image Masking) 기법이 도입된다. 이는 학습 속도를 높이고 불필요한 배경 정보(예: 빈 테이블 바닥)에 의한 노이즈를 줄이는 효과가 있다. 마스킹을 적용한 DRL 에이전트는 물체가 존재하는 영역 주변으로 탐색을 제한함으로써, 탐색 공간을 효율적으로 좁힐 수 있다.</p>
<h2>7.6.3.7 결론 및 향후 전망</h2>
<p>인터랙티브 퍼셉션, 특히 밀기를 통한 정보 획득 전략은 로봇이 비정형 환경의 복잡성과 불확실성을 극복하는 핵심 열쇠이다. 2024-2025년의 최신 연구들은 단순한 동작 수행을 넘어, 정보 이론적 원리에 기반한 능동적 탐색과 내재적 동기에 의한 자율 학습으로 진화하고 있다. 향후 기술적 발전은 다음과 같은 방향으로 전개될 것으로 전망된다.</p>
<ol>
<li><strong>멀티모달 융합의 심화 (Deep Multimodal Fusion):</strong> 시각적 밀기뿐만 아니라, 접촉 순간의 고해상도 촉각(Tactile) 정보를 결합하여 물체의 성질을 더 정밀하게 파악하는 연구가 가속화되고 있다. 시각은 전체적인 맥락을, 촉각은 국소적인 물성을 담당하여 상호 보완적인 정보 이득을 추구한다.</li>
<li><strong>월드 모델 기반의 상상과 계획 (World Models and Imagination):</strong> 실제 밀기를 수행하기 전에, 학습된 월드 모델 내에서 결과를 시뮬레이션(상상)하고 최적의 행동을 계획하는 모델 기반 강화학습(Model-Based RL)이 주류가 될 것이다. 이는 실제 환경에서의 시행착오 비용을 획기적으로 줄여준다.</li>
<li><strong>대규모 언어 모델(LLM)과의 결합:</strong> 로봇의 밀기 전략이 단순한 물리적 목적을 넘어, “저 빨간 컵 뒤에 무엇이 있는지 확인해 줘“와 같은 고수준의 언어 명령을 이해하고 수행하는 형태로 발전하고 있다. LLM의 상식 추론 능력이 IP 전략의 고수준 계획(High-level Planning)을 담당하는 하이브리드 아키텍처가 등장하고 있다.</li>
</ol>
<p>결론적으로, “밀기“는 로봇 공학에서 단순한 물리적 이동을 넘어, 로봇이 세상과 대화하고, 질문하고, 이해하는 “인지적 행위“로 재정의되고 있다. 이러한 능동적 상호작용 능력은 차세대 지능형 로봇이 갖추어야 할 필수적인 소양이며, 완전 자율 조작(Fully Autonomous Manipulation)을 향한 중요한 디딤돌이 될 것이다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>Interactive Perception: Leveraging Action in Perception and Perception in Action - IEEE Xplore, 2월 2, 2026에 액세스, https://ieeexplore.ieee.org/ielaam/8860/8113882/8007233-aam.pdf</li>
<li>Let’s Push Things Forward: A Survey on Robot Pushing - PMC - NIH, 2월 2, 2026에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC7805815/</li>
<li>Revisiting active perception - PMC - PubMed Central, 2월 2, 2026에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC6954017/</li>
<li>MANIP: A Modular Architecture for Integrating Interactive Perception for Robot Manipulation - Berkeley AUTOLab, 2월 2, 2026에 액세스, https://autolab.berkeley.edu/assets/publications/media/IROS24_MANIP_CR.pdf</li>
<li>An information-theoretic on-line update principle for perception-action coupling - arXiv, 2월 2, 2026에 액세스, https://arxiv.org/pdf/1804.05906</li>
<li>Tactile Robotics: An Outlook - arXiv, 2월 2, 2026에 액세스, https://arxiv.org/html/2508.11261v1</li>
<li>Clearing a Pile of Unknown Objects using Interactive Perception, 2월 2, 2026에 액세스, https://www.ri.cmu.edu/pub_files/2013/3/ICRA13_1691_FI.pdf</li>
<li>Predictive Visuo-Tactile Interactive Perception Framework for Object Properties Inference, 2월 2, 2026에 액세스, https://arxiv.org/html/2411.09020v1</li>
<li>Embodied AI: A Survey on the Evolution from Perceptive to Behavioral Intelligence, 2월 2, 2026에 액세스, https://www.researchgate.net/publication/395884929_Embodied_AI_A_Survey_on_the_Evolution_from_Perceptive_to_Behavioral_Intelligence</li>
<li>A Survey of Decision-Theoretic Approaches for Robotic Environmental Monitoring, 2월 2, 2026에 액세스, https://www.emerald.com/ftrob/article/11/4/225/1324867/A-Survey-of-Decision-Theoretic-Approaches-for</li>
<li>Effective Exploration for MAVs Based on the Expected Information Gain - MDPI, 2월 2, 2026에 액세스, https://www.mdpi.com/2504-446X/2/1/9</li>
<li>Information Gain and Mutual Information for Machine Learning | by Amit Yadav | Biased-Algorithms | Medium, 2월 2, 2026에 액세스, https://medium.com/biased-algorithms/information-gain-and-mutual-information-for-machine-learning-060a79f32981</li>
<li>Information Gain and Mutual Information for Machine Learning - MachineLearningMastery.com, 2월 2, 2026에 액세스, https://machinelearningmastery.com/information-gain-and-mutual-information/</li>
<li>Mutual information - Wikipedia, 2월 2, 2026에 액세스, https://en.wikipedia.org/wiki/Mutual_information</li>
<li>Autonomous exploration by expected information gain from probabilistic occupancy grid mapping | Request PDF - ResearchGate, 2월 2, 2026에 액세스, https://www.researchgate.net/publication/313951202_Autonomous_exploration_by_expected_information_gain_from_probabilistic_occupancy_grid_mapping</li>
<li>Exploration for autonomous 3D voxel mapping of static indoor environments with depth cameras and 2D odometry, 2월 2, 2026에 액세스, https://elib.dlr.de/96335/1/DLR_Masterthesis_Thomas_Wohlfahrt_Final.pdf</li>
<li>Map Space Belief Prediction for Manipulation-Enhanced Mapping - arXiv, 2월 2, 2026에 액세스, https://arxiv.org/html/2502.20606v1</li>
<li>Object Singulation by Nonlinear Pushing for Robotic Grasping | Request PDF, 2월 2, 2026에 액세스, https://www.researchgate.net/publication/338941773_Object_Singulation_by_Nonlinear_Pushing_for_Robotic_Grasping</li>
<li>Approaching Collaborative Manipulation by Pushing-Grasping Fusion - IEEE Xplore, 2월 2, 2026에 액세스, https://ieeexplore.ieee.org/iel8/6287639/10820123/11016017.pdf</li>
<li>Push-Grasp Policy Learning Using Equivariant Models and Grasp Score Optimization - arXiv, 2월 2, 2026에 액세스, https://arxiv.org/html/2504.03053v1</li>
<li>Review of Deep Reinforcement Learning-based Object Grasping: Techniques, Open Challenges and Recommendations - IEEE Xplore, 2월 2, 2026에 액세스, https://ieeexplore.ieee.org/iel7/6287639/6514899/09210095.pdf</li>
<li>Push-Grasp Policy Learning Using Equivariant Models and Grasp …, 2월 2, 2026에 액세스, https://equipushgrasp.github.io/</li>
<li>10: Robotics 2 - Chapter 4 - Active Perception - YouTube, 2월 2, 2026에 액세스, https://www.youtube.com/watch?v=psqLcaG7Gjc</li>
<li>Interactive segmentation for manipulation in unstructured environments - ResearchGate, 2월 2, 2026에 액세스, https://www.researchgate.net/publication/221076270_Interactive_segmentation_for_manipulation_in_unstructured_environments</li>
<li>Guided pushing for object singulation - WashU Medicine Research Profiles, 2월 2, 2026에 액세스, https://profiles.wustl.edu/en/publications/guided-pushing-for-object-singulation/</li>
<li>Interactive Perception for Robotic Manipulation of Liquids, Grains, and Doughs - UC Berkeley EECS, 2월 2, 2026에 액세스, https://www2.eecs.berkeley.edu/Pubs/TechRpts/2021/EECS-2021-174.pdf</li>
<li>Push, See, Predict: Emergent Perception Through Intrinsically Motivated Play - OpenReview, 2월 2, 2026에 액세스, https://openreview.net/forum?id=yHeeQNv7uU</li>
<li>Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots, 2월 2, 2026에 액세스, https://www.researchgate.net/publication/234412726_Active_Learning_of_Inverse_Models_with_Intrinsically_Motivated_Goal_Exploration_in_Robots</li>
<li>A model-centered introduction to curiosity - bioRxiv, 2월 2, 2026에 액세스, https://www.biorxiv.org/content/10.1101/2025.11.19.689250v1.full.pdf</li>
<li>NATIONAL TECHNICAL UNIVERSITY OF ATHENS Emergent, 2월 2, 2026에 액세스, https://dspace.lib.ntua.gr/xmlui/bitstream/handle/123456789/62990/KONSTANTAROPOULOS_ORESTIS_THESIS.pdf?sequence=1&amp;isAllowed=y</li>
<li>Push, See, Predict: Emergent Perception Through Intrinsically Motivated Play - OpenReview, 2월 2, 2026에 액세스, https://openreview.net/pdf/938829cb3f6f5283c33684072c58d9e7b98c84d6.pdf</li>
<li>VidBot: Learning Generalizable 3D Actions from In-the-Wild 2D Human Videos for Zero-Shot Robotic Manipulation - CVPR 2026, 2월 2, 2026에 액세스, https://cvpr.thecvf.com/virtual/2025/poster/32561</li>
<li>A Survey on Deep Reinforcement Learning Algorithms for Robotic Manipulation - PMC - NIH, 2월 2, 2026에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC10098871/</li>
<li>Rapid-Learning Collaborative Pushing and Grasping via Deep Reinforcement Learning and Image Masking - MDPI, 2월 2, 2026에 액세스, https://www.mdpi.com/2076-3417/14/19/9018</li>
<li>Robotics and Perception Group, 2월 2, 2026에 액세스, https://rpg.ifi.uzh.ch/</li>
<li>Expanding robot perception | MIT News | Massachusetts Institute of Technology, 2월 2, 2026에 액세스, https://news.mit.edu/2025/expanding-robot-perception-luca-carlone-0128</li>
<li>Predictive Visuo-Tactile Interactive Perception Framework for Object Properties Inference, 2월 2, 2026에 액세스, https://www.researchgate.net/publication/385851425_Predictive_Visuo-Tactile_Interactive_Perception_Framework_for_Object_Properties_Inference</li>
<li>WoW: Towards a World-omniscient World-model Through Embodied Interaction - arXiv, 2월 2, 2026에 액세스, https://arxiv.org/html/2509.22642v1</li>
<li>Transactable World Models - Dexterity AI, 2월 2, 2026에 액세스, https://www.dexterity.ai/blog/transactable-world-models</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>