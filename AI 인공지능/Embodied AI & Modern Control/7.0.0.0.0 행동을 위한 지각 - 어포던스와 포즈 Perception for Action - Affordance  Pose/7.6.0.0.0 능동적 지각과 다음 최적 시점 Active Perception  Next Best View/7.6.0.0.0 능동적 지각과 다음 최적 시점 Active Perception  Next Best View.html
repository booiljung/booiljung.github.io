<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.6 능동적 지각과 다음 최적 시점 (Active Perception & Next Best View)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.6 능동적 지각과 다음 최적 시점 (Active Perception & Next Best View)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <a href="index.html">7.6 능동적 지각과 다음 최적 시점 (Active Perception & Next Best View)</a> / <span>7.6 능동적 지각과 다음 최적 시점 (Active Perception & Next Best View)</span></nav>
                </div>
            </header>
            <article>
                <h1>7.6 능동적 지각과 다음 최적 시점 (Active Perception &amp; Next Best View)</h1>
<h2>1.  서론: 인지적 에이전트와 능동적 정보 획득의 철학</h2>
<p>로봇 공학 및 인공지능(AI) 시스템의 발전사에서 ’지각(Perception)’을 바라보는 관점은 지난 반세기에 걸쳐 근본적인 패러다임의 전환을 겪었다. 초기의 컴퓨터 비전 시스템은 정지된 카메라로부터 수동적으로 입력되는 이미지 데이터를 분석하여 패턴을 분류하거나 기하학적 구조를 복원하는 ’수동적 관찰자(Passive Observer)’의 역할에 국한되었다. 그러나 현실 세계는 본질적으로 역동적이고(Dynamic), 부분적으로만 관측 가능하며(Partially Observable), 불확실성(Uncertainty)으로 가득 차 있다. 이러한 환경에서 고정된 단일 시점(Viewpoint)이 제공하는 정보는 필연적으로 불완전할 수밖에 없으며, 이는 로봇이 복잡한 작업을 수행하는 데 있어 치명적인 한계로 작용한다. 이러한 배경에서 등장한 개념이 바로 **능동적 지각(Active Perception)**이다.</p>
<h3>1.1  능동적 지각의 정의와 역사적 기원</h3>
<p>능동적 지각은 시스템이 환경과 상호작용하며 센서의 파라미터(초점, 줌, 조리개 등)나 물리적 위치(이동, 회전)를 조절함으로써 불확실성을 능동적으로 최소화하고 태스크 수행 능력을 극대화하는 지능적 행동 양식을 의미한다. 1988년 Ruzena Bajcsy 교수는 이 개념을 공식화하며, 지각을 단순한 감각 데이터의 수동적 처리가 아닌, “무엇을 볼 것인가(What to see)“와 “어떻게 볼 것인가(How to see)“를 결정하는 제어 가능한 데이터 획득 과정으로 재정의하였다. 이는 지각 시스템을 정보의 수동적 수신자에서 인지적 에이전트의 핵심적인 의사결정 주체로 격상시키는 계기가 되었다.</p>
<p>현대적 관점에서 능동적 지각은 감각 데이터 수집(Sensing)과 행동 제어(Control)가 결합된 <strong>폐루프(Closed-loop) 시스템</strong>으로 정의된다. 즉, 현재의 관찰이 다음 행동을 결정하고, 그 행동이 다시 새로운 관찰을 생성하여 시스템의 내부 상태(State)를 갱신하는 순환 구조를 가진다. 이러한 순환은 에이전트가 환경에 대한 이해도를 점진적으로 높여가는 과정을 수학적으로 모델링 가능하게 한다.</p>
<p>이러한 접근 방식은 생물학적 시스템의 발달 심리학과도 깊은 연관이 있다. 1963년 Held와 Hein이 수행한 ‘아기 고양이 회전목마(Kitten Carousel)’ 실험은 감각 입력과 운동 출력 간의 능동적 상호작용이 인지 능력 발달에 필수적임을 증명한 기념비적 연구다. 실험에서 스스로 움직이며 시각 정보를 능동적으로 획득한 고양이는 정상적인 깊이 지각 능력을 발달시켰으나, 동일한 시각적 자극을 받았더라도 수동적으로 이동된 고양이는 공간 지각 능력을 갖추지 못했다. 이는 지능이 신체(Body)와 환경(Environment)의 상호작용 속에서 발현된다는 **체화된 인공지능(Embodied AI)**의 철학적 기반이 되며, 오늘날 로봇 공학이 추구하는 지능형 에이전트 설계의 핵심 원리로 작용하고 있다.</p>
<h3>1.2  다음 최적 시점(Next Best View) 문제의 본질</h3>
<p>능동적 지각을 공학적으로 구현하는 데 있어 가장 핵심적인 질문은 “현재의 불확실한 정보를 바탕으로, 정보를 최대화하기 위해 다음에 어디를 보아야 하는가?“이다. 이를 <strong>다음 최적 시점(Next Best View, NBV)</strong> 문제라 한다. NBV는 3차원 환경 재구성(3D Reconstruction), 자율 주행(Autonomous Driving), 로봇 조작(Manipulation), 그리고 미지의 환경 탐사(Exploration) 등 다양한 응용 분야에서 필수적인 기술적 난제이다.</p>
<p>예를 들어, 가정용 서비스 로봇이 “냉장고에서 컵을 꺼내오라“는 명령을 수행하기 위해서는 복잡한 실내 환경에서 자신의 위치를 파악하고(Localization), 장애물을 피해 냉장고로 이동하며(Navigation), 냉장고 문을 열고 가려진 물체들 사이에서 컵을 식별(Object Detection)한 뒤, 안정적인 파지(Grasping)를 위해 최적의 접근 각도를 찾아야 한다. 이 모든 과정이 연속적인 NBV 결정 과정의 집합이다.</p>
<p>과거의 NBV 연구는 주로 기하학적 구멍(Geometric holes)을 메우기 위해 미탐사 영역과의 경계(Frontier)를 찾아가는 기하학적 접근에 치중했으나, 최근에는 의미론적 이해(Semantic Understanding)와 딥러닝 기반의 불확실성 추정(Uncertainty Estimation)을 결합한 <strong>정보 이론적(Information-Theoretic) 접근</strong>이 주류를 이루고 있다. 특히 신경 복사장(NeRF)이나 3D 가우시안 스플래팅(3DGS)과 같은 혁신적인 3D 표현 방식의 등장은 NBV 알고리즘이 다루어야 할 상태 공간과 정보 이득의 정의를 근본적으로 변화시키고 있다.</p>
<h2>2.  정보 이론적 기초와 불확실성 정량화</h2>
<p>능동적 지각의 핵심 메커니즘은 ’불확실성’을 수학적으로 정량화하고, 이를 가장 효과적으로 감소시킬 수 있는 행동을 최적화(Optimization)하는 것이다. 이를 위해 확률론과 통계학, 그리고 정보 이론(Information Theory)의 도구들이 광범위하게 사용된다.</p>
<h3>2.1  샤논 엔트로피와 정보 이득 (Information Gain)</h3>
<p>가장 근본적인 불확실성의 척도는 1948년 클로드 섀넌(Claude Shannon)이 제안한 **엔트로피(Entropy)**이다. 로봇의 확률적 지도(Probabilistic Map) <span class="math math-inline">M</span>에 대하여, 각 상태(예: 공간의 점유 여부)가 확률 변수 <span class="math math-inline">X</span>로 표현될 때, 엔트로피 <span class="math math-inline">H(X)</span>는 시스템의 무질서도 혹은 정보의 부족량을 나타낸다.<br />
<span class="math math-display">
H(X) = - \sum_{x \in \mathcal{X}} p(x) \log p(x)
</span><br />
여기서 <span class="math math-inline">p(x)</span>는 상태 <span class="math math-inline">x</span>가 발생할 확률이다. 능동적 지각에서 로봇의 목표는 관측 <span class="math math-inline">Z</span>를 통해 이 엔트로피를 최소화하는 것이다. 즉, 관측 전의 불확실성(사전 엔트로피)과 관측 후의 불확실성(사후 엔트로피)의 차이를 극대화해야 하며, 이 차이를 <strong>상호 정보량(Mutual Information, MI)</strong> 또는 **정보 이득(Information Gain, IG)**이라 정의한다.<br />
<span class="math math-display">
IG(Z; X) = I(X; Z) = H(X) - H(X|Z)
= \sum_{z \in \mathcal{Z}} \sum_{x \in \mathcal{X}} p(x, z) \log \left( \frac{p(x, z)}{p(x)p(z)} \right)
</span><br />
NBV 알고리즘은 가능한 후보 행동 집합 <span class="math math-inline">\mathcal{A}</span> 중에서 기대 정보 이득(Expected Information Gain)을 최대화하는 최적의 행동 <span class="math math-inline">a^*</span>를 선택하는 최적화 문제로 귀결된다.<br />
<span class="math math-display">
a^* = \underset{a \in \mathcal{A}}{\text{argmax}} \ \mathbb{E}_{Z} [ I(X; Z_a) ]
</span><br />
여기서 <span class="math math-inline">Z_a</span>는 행동 <span class="math math-inline">a</span>를 취했을 때 예상되는 관측값이다. 하지만 고차원 상태 공간(예: 수백만 개의 복셀이나 신경망의 파라미터)에서 상호 정보량을 직접 계산하는 것은 계산 복잡도가 매우 높아(<span class="math math-inline">O(N^2)</span> 이상), 실시간 처리가 필요한 로봇 시스템에는 큰 부담이 된다. 따라서 코시-슈바르츠 이차 상호 정보량(CSQMI)과 같은 효율적인 근사법이나, 맵의 구조적 특성을 이용한 고속화 알고리즘(FCMI, FSMI)들이 제안되어 왔다.</p>
<h3>2.2  딥러닝 시대의 불확실성: 피셔 정보(Fisher Information)</h3>
<p>최근 딥러닝 기반의 3D 복원 모델이 주류가 되면서, 이산적인 확률 분포(Discrete Probability Distribution)를 다루는 샤논 엔트로피 대신, 연속적인 파라미터 공간에서의 정보를 다루는 **피셔 정보(Fisher Information)**가 주목받고 있다. 피셔 정보 행렬(Fisher Information Matrix, FIM) <span class="math math-inline">\mathcal{I}(\theta)</span>는 관측 데이터 <span class="math math-inline">Z</span>가 모델 파라미터 <span class="math math-inline">\theta</span>에 대해 얼마나 많은 정보를 제공하는지를 나타내는 척도이다.<br />
<span class="math math-display">
\mathcal{I}(\theta) = \mathbb{E}_{Z \sim p(z|\theta)} \left[ \left( \nabla_\theta \log p(Z|\theta) \right) \left( \nabla_\theta \log p(Z|\theta) \right)^T \right]
</span><br />
피셔 정보는 모델의 로그 우도 함수(Log-likelihood)의 기울기(Gradient, 또는 Score)의 공분산으로 정의된다. 이는 직관적으로 “특정 뷰에서 얻은 데이터가 모델 파라미터를 업데이트하는 데 얼마나 크게 기여하는가?“를 의미한다. 기울기가 크다는 것은 해당 데이터가 모델에 큰 변화를 줄 수 있다는 것, 즉 정보량이 많다는 것을 시사한다.</p>
<p>최신 3D Gaussian Splatting 기반 연구(FisherRF 등)에서는 피셔 정보를 활용하여 렌더링된 픽셀이 가우시안 파라미터(위치, 색상, 불투명도 등)의 불확실성을 얼마나 감소시키는지를 정량화한다. 피셔 정보는 샤논 엔트로피에 비해 미분 가능한 신경망 구조에 직접 적용하기 용이하며, 특히 대각 행렬 근사(Diagonal Approximation)를 통해 수백만 개의 파라미터를 실시간으로 처리할 수 있다는 장점이 있다.</p>
<h3>2.3  불확실성의 이원론: Aleatoric vs. Epistemic</h3>
<p>능동적 지각 시스템을 설계할 때는 불확실성의 원인을 명확히 구분하는 것이 중요하다.</p>
<ol>
<li><strong>우연적 불확실성 (Aleatoric Uncertainty):</strong> 데이터 자체에 내재된 무작위성이나 노이즈(센서의 잡음, 모션 블러, 조명 변화 등)에 기인한다. 이는 데이터의 양을 늘려도 근본적으로 제거되지 않는다.</li>
<li><strong>인식론적 불확실성 (Epistemic Uncertainty):</strong> 모델의 지식 부족에 기인한다. 즉, 모델이 해당 영역을 충분히 관측하지 못했거나 학습하지 못했기 때문에 발생한다.</li>
</ol>
<p>NBV 알고리즘의 목표는 <strong>인식론적 불확실성</strong>을 줄이는 것이다. ActiveNeRF 와 같은 연구에서는 앙상블(Ensemble) 모델이나 베이지안 추론을 통해 모델의 인식론적 불확실성을 추정하고, 이를 기반으로 아직 탐사되지 않은 영역(Unobserved Regions)을 관측할 수 있는 뷰를 선택한다. 반면, 우연적 불확실성이 높은 영역(예: 텍스처가 없는 벽이나 반사가 심한 표면)은 정보 이득이 낮다고 판단하여 회피하거나 다른 센서(예: 촉각)를 사용하도록 유도해야 한다.</p>
<h2>3.  고전적 방법론: 복셀 그리드와 레이 캐스팅</h2>
<p>딥러닝이 도입되기 이전, 로봇 공학에서 능동적 지각은 주로 공간을 이산적인 격자로 나누는 <strong>복셀 그리드(Voxel Grid)</strong> 방식을 통해 구현되었다. 이 접근법은 직관적이고 구현이 용이하여 여전히 많은 상용 로봇 시스템의 기초가 되고 있다.</p>
<h3>3.1  옥토맵(OctoMap)과 엔트로피 맵</h3>
<p>옥토맵(OctoMap)은 3차원 공간을 계층적인 트리 구조인 옥트리(Octree)로 표현하여 메모리 효율성을 극대화한 확률적 지도 표현 방식이다. 각 노드(복셀)는 ‘점유됨(Occupied)’, ‘비어있음(Free)’, 또는 ’미지(Unknown)’의 상태 확률을 가진다. NBV 알고리즘은 이 맵을 바탕으로 후보 시점들에서 가상의 레이(Ray)를 발사하는 <strong>레이 캐스팅(Ray-casting)</strong> 시뮬레이션을 수행한다.</p>
<p>정보 이득은 맵의 전체 엔트로피 감소량으로 계산된다. 각 복셀 <span class="math math-inline">v</span>의 엔트로피는 다음과 같다:<br />
<span class="math math-display">
H(v) = - \left( P(v) \log P(v) + (1-P(v)) \log (1-P(v)) \right)
</span><br />
가상의 레이가 ‘미지’ 상태의 복셀들을 통과하다가 장애물에 부딪히거나 최대 거리에 도달하면, 해당 레이는 통과한 미지 영역을 ‘비어있음’ 또는 ’점유됨’으로 확정 지을 잠재력을 가진다. 따라서 많은 수의 미지 복셀을 통과하는 뷰가 높은 정보 이득을 갖게 된다.</p>
<h3>3.2  경계 기반 탐색 (Frontier-based Exploration)</h3>
<p>복셀 기반 탐색의 가장 대표적인 전략은 **프론티어(Frontier)**를 찾아가는 것이다. 프론티어는 ’알려진 자유 공간(Known Free Space)’과 ‘미탐사 공간(Unknown Space)’ 사이의 경계면으로 정의된다. 로봇이 프론티어로 이동하여 센서를 작동시키면 필연적으로 미탐사 영역에 대한 새로운 정보를 얻게 된다.</p>
<p>최근의 연구들은 단순한 프론티어 탐색을 넘어, 다음과 같은 요소들을 통합하여 탐색 효율을 높이고 있다:</p>
<ul>
<li><strong>이동 비용(Travel Cost):</strong> 현재 위치에서 프론티어까지의 이동 거리나 에너지 소모량을 고려한 유틸리티 함수 최적화.</li>
<li><strong>센서 모델:</strong> 센서의 시야각(FOV)과 유효 사거리를 고려하여 한 번의 스캔으로 가장 넓은 프론티어 영역을 커버할 수 있는 방향(Orientation) 설정.</li>
<li><strong>의미론적 정보:</strong> 단순한 공간 점유 여부가 아니라, 특정 객체(예: 문, 창문)가 있을 확률이 높은 곳을 우선적으로 탐색.</li>
</ul>
<h3>3.3  고전적 방법의 한계</h3>
<p>복셀 기반 방식은 다음과 같은 본질적인 한계를 지닌다 :</p>
<ol>
<li><strong>메모리 및 계산 비용의 상충:</strong> 정밀한 환경 표현을 위해 복셀 해상도를 높이면 메모리 사용량과 레이 캐스팅 연산량이 3제곱(<span class="math math-inline">O(N^3)</span>)으로 증가한다.</li>
<li><strong>이산화 오차(Discretization Artifacts):</strong> 격자 구조로 인해 경사면이나 곡면과 같은 정밀한 형상을 표현하는 데 한계가 있으며, 이는 로봇 조작과 같이 정밀도가 요구되는 작업에 부적합하다.</li>
<li><strong>의미 정보의 부재:</strong> 대부분의 고전적 방법은 기하학적 정보(점유 여부)만을 다루므로, “어떤 물체인가?“에 대한 고차원적인 추론이 불가능하다.</li>
</ol>
<h2>4.  신경 복사장(NeRF) 시대의 능동적 지각</h2>
<p>2020년 신경 복사장(Neural Radiance Fields, NeRF)의 등장은 3D 비전 분야에 혁명을 일으켰으며, 능동적 지각 연구에도 새로운 패러다임을 제시하였다. NeRF는 3D 공간을 연속적인 함수(MLP 신경망)로 표현하여 복셀 방식의 해상도 문제를 해결하고, 사진과 같은 사실적인(Photorealistic) 뷰 합성을 가능하게 했다.</p>
<h3>4.1  NeRF의 한계와 ActiveNeRF의 등장</h3>
<p>기본적인 NeRF는 고품질의 3D 복원을 위해 수십에서 수백 장의 밀도 높은 이미지(Dense views)를 필요로 한다. 만약 뷰가 부족한(Sparse view) 상황이라면, NeRF는 보지 못한 영역에서 심각한 아티팩트(Artifact)를 생성하거나 기하학적 구조가 붕괴되는 현상을 보인다.</p>
<p><strong>ActiveNeRF</strong> 는 이러한 문제를 해결하기 위해, 적은 수의 초기 이미지로 NeRF를 학습시키면서 추가로 필요한 이미지를 능동적으로 선택하는 프레임워크이다. ActiveNeRF의 핵심 혁신은 NeRF 모델 자체가 자신의 예측에 대한 불확실성을 출력하도록 설계된 점이다.</p>
<ul>
<li><strong>불확실성 모듈의 통합:</strong> 기존 NeRF가 색상(RGB)과 밀도(Density)만을 출력했다면, ActiveNeRF는 렌더링된 픽셀의 불확실성(분산, <span class="math math-inline">\beta</span>)을 예측하는 추가적인 헤드(Head)를 가진다.</li>
<li><strong>능동 학습 루프:</strong></li>
</ul>
<ol>
<li>초기 소수 이미지로 모델 학습.</li>
<li>후보 뷰 집합(Candidate Pool)에서 가상 렌더링 수행 및 불확실성 맵(Uncertainty Map) 생성.</li>
<li>불확실성이 가장 높은(즉, 정보 이득이 가장 클 것으로 예상되는) 뷰를 선택하여 로봇을 이동시키거나 데이터를 획득.</li>
<li>새로운 데이터를 포함하여 모델 재학습.</li>
</ol>
<ul>
<li><strong>성과:</strong> 실험 결과 ActiveNeRF는 무작위로 뷰를 선택하는 것보다 훨씬 적은 수의 이미지로도 고품질의 3D 복원을 달성하였으며, 특히 텍스처가 부족하거나 구조가 복잡한 영역을 집중적으로 학습하는 경향을 보였다.</li>
</ul>
<h3>4.2  Neural Visibility Field (NVF): 가려짐의 명시적 처리</h3>
<p>ActiveNeRF가 픽셀 단위의 불확실성에 집중했다면, <strong>Neural Visibility Field (NVF)</strong> 는 ’가시성(Visibility)’을 불확실성의 핵심 원천으로 간주한다. NeRF의 주요 실패 요인 중 하나는 특정 뷰에서 가려져 보이지 않았던(Occluded) 영역을 렌더링할 때 발생하는 환각(Hallucination)이다.</p>
<p>NVF는 공간상의 각 점이 카메라 위치에서 관측 가능한지를 나타내는 가시성 필드를 별도로 학습한다.</p>
<ul>
<li><strong>가시성 엔트로피 (Visibility Entropy):</strong> 관측되지 않은 영역(Unobserved regions)은 높은 엔트로피를 가지며, 이는 곧 높은 불확실성을 의미한다.</li>
<li><strong>베이지안 합성:</strong> NVF는 위치 기반(Position-based)의 필드 불확실성을 레이 기반(Ray-based)의 관측 불확실성으로 변환하기 위해 베이지안 네트워크를 활용한다. 이를 통해 로봇은 단순히 텍스처가 복잡한 곳이 아니라, 실제로 가려져서 정보가 없는 영역을 해소할 수 있는 뷰를 NBV로 선택한다.</li>
</ul>
<h3>4.3  앙상블 기법과 잠재 코드 불확실성</h3>
<p>불확실성을 추정하는 또 다른 접근법은 **딥 앙상블(Deep Ensemble)**이다. 여러 개의 NeRF 모델을 서로 다른 초기값으로 학습시키면, 데이터가 충분한 영역에서는 모델들의 예측이 일치하지만, 데이터가 부족한 영역에서는 예측이 크게 엇갈린다(높은 분산). 이 분산을 불확실성의 척도로 사용하여 NBV를 결정할 수 있다. 그러나 앙상블 방식은 여러 개의 모델을 학습시켜야 하므로 계산 비용이 매우 높다는 단점이 있다. 이에 대한 대안으로 모델의 가중치가 아닌, 입력 조건(Conditioning)에 해당하는 잠재 코드(Latent Code)의 분포를 통해 불확실성을 추정하는 연구들도 진행되고 있다.</p>
<h2>5.  차세대 3D 표현론: 3D Gaussian Splatting과 능동적 감각</h2>
<p>2023년 등장한 **3D Gaussian Splatting (3DGS)**은 3D 장면을 수백만 개의 3D 가우시안 타원체(Ellipsoid)로 표현하는 기술이다. 3DGS는 NeRF 수준의 고화질 렌더링을 제공하면서도, 포인트 클라우드와 유사한 명시적(Explicit) 특성을 가져 실시간 렌더링과 편집이 가능하다. 이러한 특성은 실시간 의사결정이 필수적인 로봇의 능동적 지각에 이상적인 플랫폼을 제공한다.</p>
<h3>5.1  FisherRF: 효율적인 피셔 정보 계산</h3>
<p><strong>FisherRF</strong> 는 3DGS 모델의 파라미터에 대한 피셔 정보(Fisher Information)를 계산하여 정보 이득을 정량화하는 최신 기법이다.</p>
<ul>
<li><strong>작동 원리:</strong> 3DGS는 명시적인 가우시안 파라미터(위치 <span class="math math-inline">\mu</span>, 공분산 <span class="math math-inline">\Sigma</span>, 색상 <span class="math math-inline">c</span>, 불투명도 <span class="math math-inline">\alpha</span>)를 가진다. 렌더링된 이미지가 이 파라미터들에 대해 가지는 민감도(Gradient)를 계산하면, 어떤 뷰가 모델을 업데이트하는 데 가장 큰 영향을 미치는지 알 수 있다.</li>
<li><strong>대각 근사(Diagonal Approximation):</strong> 전체 파라미터에 대한 피셔 정보 행렬(Hessian의 근사)은 크기가 너무 커서 계산이 불가능하다. FisherRF는 이를 대각 행렬이나 블록 대각 행렬로 근사하여 실시간으로 정보 이득을 계산한다. 이는 기존의 샤논 엔트로피 기반 방법보다 계산 효율이 월등히 높다.</li>
<li><strong>렌더링 없는 평가:</strong> FisherRF의 가장 큰 장점은 후보 뷰에서 실제로 이미지를 렌더링하거나 무거운 추론을 돌리지 않고도, 현재 모델의 그라디언트 정보만으로 정보량을 예측할 수 있다는 점이다.</li>
</ul>
<h3>5.2  OUGS: 객체 중심의 물리적 불확실성</h3>
<p><strong>OUGS (Object-aware Uncertainty for Gaussian Splatting)</strong> 는 단순한 장면 복원을 넘어, 로봇이 조작해야 할 특정 물체(Object-centric)에 집중하는 능동적 지각 프레임워크다.</p>
<ul>
<li><strong>물리적 파라미터 불확실성:</strong> NeRF와 달리 3DGS의 파라미터는 물리적 의미(위치, 크기)를 가진다. OUGS는 가우시안의 위치와 스케일 파라미터의 공분산을 렌더링 자코비안(Jacobian)을 통해 전파하여 픽셀 단위의 불확실성을 유도한다. 이는 “이 픽셀의 색상이 불확실하다“는 모호한 정보 대신, “이 물체의 경계면 위치가 불확실하다“는 구체적이고 물리적인 정보를 제공한다.</li>
<li><strong>의미론적 마스킹:</strong> 의미론적 분할(Semantic Segmentation) 마스크를 통합하여, 배경의 불필요한 정보(Clutter)나 노이즈에 의한 불확실성을 무시하고, 관심 물체(Target Object)의 완성도를 높이는 뷰를 우선적으로 선택한다. 이는 로봇이 복잡한 배경 속에서 특정 물체를 파지해야 할 때 매우 유용한 전략이다.</li>
</ul>
<h3>5.3  Next Best Sense: 시각과 촉각의 능동적 융합</h3>
<p>스탠포드 대학 연구팀이 제안한 <strong>“Next Best Sense”</strong> 는 능동적 지각을 시각(Vision)에서 촉각(Touch)으로 확장한 선도적인 연구다.</p>
<ul>
<li><strong>깊이 불확실성(Depth Uncertainty):</strong> 기존 FisherRF가 주로 색상 정보의 불확실성에 의존했다면, 이 연구는 깊이(Depth) 정보의 불확실성을 피셔 정보로 인코딩한다. 로봇 조작에 있어 형상(Geometry)의 정확도는 색상보다 훨씬 중요하기 때문이다.</li>
<li><strong>능동적 촉각 탐색:</strong> 거울이나 투명한 유리, 혹은 텍스처가 없는 매끄러운 표면은 시각 정보만으로는 깊이를 추정하기 어렵다(높은 불확실성). Next Best Sense는 이러한 영역을 식별하고, 로봇 팔을 뻗어 직접 접촉(Touch)함으로써 깊이 정보를 획득하도록 유도한다.</li>
<li><strong>국소적 스무딩과 SAM:</strong> SAM(Segment Anything Model)을 이용해 물체 마스크를 추출하고, 터치로 얻은 정확한 깊이 정보를 주변 가우시안에 전파(Smoothing)하여 시각적 결함을 보정한다. 이는 능동적 지각이 단순한 ’보기’를 넘어 ’만져서 확인하기’라는 인간적인 탐색 방식으로 진화하고 있음을 보여준다.</li>
</ul>
<h2>6.  강화학습(RL) 기반 능동적 지각 정책</h2>
<p>정보 이론 기반의 NBV 접근법은 수학적으로 견고하지만, 매 단계마다 복잡한 최적화 계산을 수행해야 하며(Online Optimization), 국소 최적해(Local Minimum)에 빠질 위험이 있다. 이에 대한 대안으로 **심층 강화학습(Deep Reinforcement Learning, DRL)**을 통해 최적의 뷰 선택 정책(Policy)을 사전에 학습하는 연구들이 진행되고 있다.</p>
<h3>6.1  부분 관측 마르코프 결정 과정 (POMDP)</h3>
<p>로봇의 탐색 문제는 전형적인 **부분 관측 마르코프 결정 과정(POMDP)**이다. 로봇은 전체 환경 상태 <span class="math math-inline">S</span>를 알지 못하고, 제한된 관측 <span class="math math-inline">O</span>만을 통해 현재 상태를 추정(Belief State)하고 행동 <span class="math math-inline">A</span>를 결정해야 한다. 강화학습은 로봇이 시뮬레이션 환경에서 수만 번의 시행착오를 거치며, 어떤 관측 패턴이 나타났을 때 어디를 보아야 보상(Reward)을 최대화할 수 있는지를 스스로 깨닫게 한다.</p>
<h3>6.2  AAWR: 비대칭 이점 가중 회귀</h3>
<p>2025년 NeurIPS에 발표된 <strong>AAWR (Asymmetric Advantage Weighted Regression)</strong> 은 부분 관측 상황에서 능동적 지각 행동을 효율적으로 학습하는 획기적인 알고리즘이다.</p>
<ul>
<li><strong>비대칭 학습(Asymmetric Learning) 구조:</strong></li>
<li><strong>학습 시 (Training Time):</strong> 시뮬레이터는 전지전능하다. 따라서 가치 함수(Critic) 학습에는 로봇의 센서 데이터뿐만 아니라, 물체의 정확한 3D 위치, 가려진 영역의 정보, 전지적 시점의 카메라 영상 등 **‘특권 정보(Privileged Information)’**를 제공한다.</li>
<li><strong>실행 시 (Test Time):</strong> 로봇의 행동을 결정하는 정책(Policy)은 오직 로봇에 장착된 센서(Wrist Camera, Proprioception) 데이터만을 입력으로 받는다.</li>
<li><strong>작동 원리:</strong> 특권 정보를 가진 비평가(Critic)는 정책이 현재 수행한 행동이 전체 정보를 다 알고 있는 관점에서 얼마나 훌륭한지(Advantage)를 정확하게 평가할 수 있다. 정책은 이 정확한 피드백을 바탕으로 학습되므로, 실행 시에는 특권 정보 없이도 마치 전체를 꿰뚫어 보는 것처럼 효율적으로 정보를 탐색하는 행동(예: 서랍을 열어보거나 장애물을 치워 시야를 확보하는 행동)을 창발적으로 수행하게 된다.</li>
<li><strong>보상 설계:</strong> 탐색 에피소드가 끝날 때 물체를 성공적으로 찾거나 조작했을 때 **희소 보상(Sparse Reward)**을 줌으로써, 로봇이 불필요한 움직임을 최소화하고 목표 지향적인 탐색을 하도록 유도한다.</li>
</ul>
<h3>6.3  ObjectNav와 구현된 AI</h3>
<p><strong>ObjectNav (Object Goal Navigation)</strong> 태스크는 “부엌으로 가서 컵을 찾아라“와 같은 고수준 명령을 수행하는 것으로, 능동적 지각의 종합 예술이라 할 수 있다. 최근 연구들은 거대 엔드-투-엔드(End-to-End) 모델보다는 <strong>모듈형 강화학습(Modular RL)</strong> 접근법이 더 우수함을 보여준다.</p>
<ul>
<li><strong>모듈화:</strong> 지각(Mapping), 전역 계획(Global Policy), 지역 제어(Local Policy)를 분리하고, 각 모듈을 강화학습으로 최적화한다.</li>
<li><strong>능동적 시각 정책:</strong> 단순히 이동하는 것뿐만 아니라, 카메라의 고개(Tilt/Pan)를 능동적으로 제어하여 바닥의 장애물을 확인하거나 높은 선반 위를 살피는 행동을 학습한다. 이는 기존의 수동적인 SLAM 기반 내비게이션과 확연히 구분되는 지점이다.</li>
</ul>
<h2>7.  응용 사례: 폐색 환경에서의 로봇 파지 (Active Grasping)</h2>
<p>능동적 지각 기술이 가장 실용적으로 적용되는 분야는 물류 창고나 가정 환경에서의 로봇 파지(Grasping)이다. 잡동사니가 무질서하게 쌓여있는 환경(Cluttered Environment)에서는 물체들이 서로를 가리고 있어(Occlusion), 단일 시점에서는 안전한 파지 점(Grasp Pose)을 찾기 어렵다.</p>
<h3>7.1  GraspView와 Render-and-Score 전략</h3>
<p><strong>GraspView</strong> 는 RGB 카메라만 사용하여, 고가의 깊이 센서 없이도 복잡한 환경에서 정밀한 파지를 수행하는 파이프라인을 제안한다.</p>
<ul>
<li><strong>Render-and-Score (렌더링 후 점수화):</strong></li>
</ul>
<ol>
<li>현재까지 획득한 이미지로 3D 장면을 재구성(Reconstruction)한다.</li>
<li>로봇이 도달 가능한 여러 후보 시점에서 가상의 이미지를 렌더링한다.</li>
<li>각 가상 이미지에 대해 “이 뷰가 현재 가려진 영역을 얼마나 드러내는가?” 또는 “이 뷰에서 파지 성공 확률이 얼마나 높게 예측되는가?“를 점수화(Score)한다.</li>
<li>가장 높은 점수를 받은 뷰로 실제 카메라를 이동시켜 추가 관측을 수행한다.</li>
</ol>
<ul>
<li><strong>성과:</strong> 이 방식은 투명한 물체나 반사 재질의 물체(Transparents &amp; Specular objects)처럼 깊이 센서(LiDAR, RealSense 등)가 실패하는 상황에서도 RGB 기반의 3D 복원과 능동적 뷰 선택을 통해 우수한 파지 성능을 입증하였다.</li>
</ul>
<h3>7.2  VISO-Grasp: 거대 모델 기반의 공간 추론</h3>
<p><strong>VISO-Grasp</strong> 는 최신 거대 언어 모델(LLM)과 비전-언어 모델(VLM)의 추론 능력을 능동적 지각에 도입하였다.</p>
<ul>
<li><strong>상식적 추론 (Commonsense Reasoning):</strong> “열쇠는 보통 노트북 근처나 책 더미 아래에 숨겨져 있을 가능성이 높다“와 같은 상식적인 지식을 활용하여, 무작위 탐색이 아니라 가능성이 높은 영역을 우선적으로 탐색한다.</li>
<li><strong>VLM 기반 불확실성 융합:</strong> 여러 뷰에서 얻은 정보를 통합할 때, VLM이 예측한 의미론적 정보의 불확실성을 가중치로 사용한다. 이를 통해 인식 오류가 맵 전체를 오염시키는 것을 방지하고, 더 신뢰성 있는 파지 계획을 수립한다.</li>
</ul>
<h3>7.3  RF-Grasp: 가시광선을 넘어서</h3>
<p><strong>RF-Grasp</strong> 는 시각 정보의 근본적 한계를 극복하기 위해 <strong>전파(Radio Frequency, RF)</strong> 신호를 활용하는 혁신적인 시스템이다. RF 신호는 종이 상자나 플라스틱 덮개와 같은 비금속 물체를 투과할 수 있다.</p>
<ul>
<li><strong>RF-Visual 융합:</strong> 손목에 부착된 RF 리더를 통해 시각적으로 완전히 가려진(Fully Occluded) 물체의 위치를 대략적으로 파악(RF-Search)한 후, 시각 시스템을 해당 위치로 유도하여 정밀한 파지를 수행한다.</li>
<li><strong>의의:</strong> 이는 능동적 지각이 가시광선 영역에만 머물지 않고, 다양한 센서 모달리티를 능동적으로 활용하여 ‘보이지 않는 것을 보는’ 능력으로 확장될 수 있음을 보여준다.</li>
</ul>
<h2>8.  결론 및 향후 전망</h2>
<p>능동적 지각과 다음 최적 시점(NBV) 기술은 로봇이 단순히 정해진 루틴을 반복하는 자동화 기계(Automation Machine)에서, 비정형 환경 속에서 스스로 정보를 획득하고 판단하는 진정한 의미의 **자율 에이전트(Autonomous Agent)**로 진화하기 위한 필수 불가결한 요소이다.</p>
<h3>8.1  기술적 비교 요약</h3>
<p>아래 표는 본 장에서 다룬 주요 기술적 접근법들을 비교 분석한 것이다.</p>
<table><thead><tr><th><strong>구분</strong></th><th><strong>고전적 방법 (Voxel/Ray-casting)</strong></th><th><strong>NeRF 기반 (ActiveNeRF, NVF)</strong></th><th><strong>3DGS 기반 (FisherRF, OUGS)</strong></th><th><strong>강화학습 기반 (AAWR)</strong></th></tr></thead><tbody>
<tr><td><strong>환경 표현</strong></td><td>복셀 그리드 (OctoMap)</td><td>MLP 신경망 (Implicit)</td><td>3D 가우시안 (Explicit)</td><td>잠재 벡터 또는 신경망 정책</td></tr>
<tr><td><strong>불확실성 척도</strong></td><td>샤논 엔트로피 (점유 확률)</td><td>앙상블 분산, 잠재 코드 불확실성</td><td>피셔 정보, 물리적 공분산</td><td>가치 함수(Q-value)의 분산</td></tr>
<tr><td><strong>NBV 기준</strong></td><td>정보 이득(IG), 프론티어</td><td>픽셀/가시성 불확실성 감소</td><td>피셔 정보량, 객체 인식 점수</td><td>누적 보상(Reward) 최대화</td></tr>
<tr><td><strong>계산 비용</strong></td><td>높음 (레이 캐스팅 <span class="math math-inline">O(N^3)</span>)</td><td>매우 높음 (반복적 학습 필요)</td><td>낮음 (실시간 렌더링, 대각 근사)</td><td>학습 시 높음, 실행 시 매우 낮음</td></tr>
<tr><td><strong>장점</strong></td><td>직관적, 구현 용이, 안전성 보장</td><td>고해상도, 사진 같은 품질</td><td>실시간성, 물리적 해석 가능</td><td>복잡하고 창발적인 전략 학습</td></tr>
<tr><td><strong>단점</strong></td><td>메모리 효율 저하, 의미 정보 부재</td><td>느린 속도, 많은 데이터 필요</td><td>초기화 민감, 대용량 VRAM</td><td>학습 불안정, Sim-to-Real 격차</td></tr>
</tbody></table>
<h3>8.2  미래 연구 방향 (Future Directions)</h3>
<p>향후 능동적 지각 연구는 다음과 같은 방향으로 발전할 것으로 전망된다.</p>
<ol>
<li><strong>파운데이션 모델과의 심층 통합:</strong> GPT-4V, SAM, Gemini와 같은 멀티모달 파운데이션 모델(Multimodal Foundation Models)이 로봇의 ‘두뇌’ 역할을 하며, 단순한 기하학적 탐색을 넘어 “냉장고 안에 유통기한이 지난 우유가 있는지 확인해줘“와 같은 고차원적인 의미론적 탐색(Semantic Exploration)을 수행하게 될 것이다.</li>
<li><strong>온디바이스 실시간 적응 (On-device Adaptation):</strong> 3DGS의 빠른 학습 속도를 활용하여, 로봇이 작동하는 동안 실시간으로 환경 모델을 업데이트하고(Continual Learning), 물체가 움직이거나 환경이 변하는 동적 상황(Dynamic Scene)에 즉각적으로 적응하는 능동적 지각 시스템이 보편화될 것이다.</li>
<li><strong>인간-로봇 상호작용적 지각 (Interactive Perception):</strong> 로봇이 자신의 불확실성을 인지하고, 스스로 해결할 수 없을 때 인간에게 도움을 요청하거나(Active Query), 인간의 의도를 더 잘 파악하기 위해 능동적으로 시점을 변경하고 질문하는 사회적 지능(Social Intelligence)을 갖추게 될 것이다.</li>
</ol>
<p>결론적으로, 능동적 지각은 ’지능’이 정적인 데이터 처리가 아니라, 환경과의 끊임없는 상호작용 속에서 불확실성을 줄여나가는 동적인 과정임을 시사한다. 미래의 로봇은 단순히 보는 기계가 아니라, <strong>알기 위해 움직이고, 이해하기 위해 행동하는 존재</strong>가 될 것이다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>Active Perception Behaviors in AI - Emergent Mind, https://www.emergentmind.com/topics/active-perception-behaviors</li>
<li>Embodied intelligence: Recent advances and future perspectives - The Innovation, https://www.the-innovation.org/data/article/informatics/preview/pdf/TII-2025-0015.pdf</li>
<li>(PDF) Real-World Reinforcement Learning of Active Perception Behaviors - ResearchGate, https://www.researchgate.net/publication/398225997_Real-World_Reinforcement_Learning_of_Active_Perception_Behaviors</li>
<li>A Call for Embodied AI - arXiv, https://arxiv.org/html/2402.03824v3</li>
<li>Artificial cognition vs. artificial intelligence for next-generation autonomous robotic agents - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC10995397/</li>
<li>[2110.06766] Next-Best-View Estimation based on Deep Reinforcement Learning for Active Object Classification - arXiv, https://arxiv.org/abs/2110.06766</li>
<li>Next Best Sense: Guiding Vision and Touch with FisherRF for 3D Gaussian Splatting - arXiv, https://arxiv.org/html/2410.04680v4</li>
<li>Active Vision and Exploration - Robotics and Perception Group, https://rpg.ifi.uzh.ch/research_active_vision.html</li>
<li>Neural Visibility Field for Uncertainty-Driven Active Mapping - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Xue_Neural_Visibility_Field_for_Uncertainty-Driven_Active_Mapping_CVPR_2024_paper.pdf</li>
<li>ActiveNeRF: Learning where to See with Uncertainty Estimation - European Computer Vision Association, https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136930225.pdf</li>
<li>OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS - arXiv, https://arxiv.org/html/2511.09397v1</li>
<li>22.11. Information Theory — Dive into Deep Learning 1.0.3 documentation, https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html</li>
<li>Decoding collective communications using information theory tools | Journal of The Royal Society Interface, https://royalsocietypublishing.org/rsif/article/17/164/20190563/35998/Decoding-collective-communications-using</li>
<li>Information-Theoretic Planning with Trajectory Optimization for Dense 3D Mapping - Robotics, https://roboticsproceedings.org/rss11/p03.pdf</li>
<li>An Efficient and Continuous Approach to Information-Theoretic Exploration, https://eems.mit.edu/wp-content/uploads/2020/03/2020_icra_fcmi.pdf</li>
<li>An Information Theoretic Interpretation to Deep Neural Networks - MDPI, https://www.mdpi.com/1099-4300/24/1/135</li>
<li>FisherRF: Active View Selection and Uncertainty Quantification for Radiance Fields using Fisher Information - Wen Jiang, https://jiangwenpl.github.io/FisherRF/</li>
<li>Next Best Sense: Guiding Vision and Touch with FisherRF for 3D Gaussian Splatting - arXiv, https://arxiv.org/html/2410.04680v3</li>
<li>POp-GS: Next Best View in 3D-Gaussian Splatting with P-Optimality, https://openaccess.thecvf.com/content/CVPR2025/papers/Wilson_POp-GS_Next_Best_View_in_3D-Gaussian_Splatting_with_P-Optimality_CVPR_2025_paper.pdf</li>
<li>Accurate Uncertainty Estimation and Decomposition in Ensemble Learning - NeurIPS, http://papers.neurips.cc/paper/9097-accurate-uncertainty-estimation-and-decomposition-in-ensemble-learning.pdf</li>
<li>FSMI: Fast computation of Shannon mutual information for information-theoretic mapping, https://eems.mit.edu/wp-content/uploads/2020/08/2020_ijrr_fsmi.pdf</li>
<li>octomap/octomap/include/octomap/OccupancyOcTreeBase.h at devel · OctoMap/octomap - GitHub, https://github.com/OctoMap/octomap/blob/devel/octomap/include/octomap/OccupancyOcTreeBase.h</li>
<li>An Information Gain Formulation for Active Volumetric 3D Reconstruction - Robotics and Perception Group, https://rpg.ifi.uzh.ch/docs/ICRA16_Isler.pdf</li>
<li>A comparison of volumetric information gain metrics for active 3D object reconstruction, https://rpg.ifi.uzh.ch/docs/AURO17_Delmerico.pdf</li>
<li>Active Mapping Via Gradient Ascent Optimization of Shannon Mutual Information Over Continuous SE(3) Trajectories - Nikolay A. Atanasov, https://natanaso.github.io/ref/Asgharivaskasi_DifferentiableActiveMapping_IROS22.pdf</li>
<li>An Information Gain Formulation for Active Volumetric 3D Reconstruction - ResearchGate, https://www.researchgate.net/publication/318268391_An_Information_Gain_Formulation_for_Active_Volumetric_3D_Reconstruction</li>
<li>Uncertainty-aware Active Learning of NeRF-based Object Models for Robot Manipulators using Visual and Re-orientation Actions - arXiv, https://arxiv.org/html/2404.01812v1</li>
<li>[2209.08546] ActiveNeRF: Learning where to See with Uncertainty Estimation - arXiv, https://arxiv.org/abs/2209.08546</li>
<li>ActiveNeRF: Learning Where to See with Uncertainty Estimation - ResearchGate, https://www.researchgate.net/publication/365009379_ActiveNeRF_Learning_Where_to_See_with_Uncertainty_Estimation</li>
<li>POp-GS: Next Best View in 3D-Gaussian Splatting with P-Optimality, http://openaccess.thecvf.com/content/CVPR2025/papers/Wilson_POp-GS_Next_Best_View_in_3D-Gaussian_Splatting_with_P-Optimality_CVPR_2025_paper.pdf</li>
<li>OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS - ResearchGate, https://www.researchgate.net/publication/397556253_OUGS_Active_View_Selection_via_Object-aware_Uncertainty_Estimation_in_3DGS</li>
<li>Lee-JaeWon/2025-Arxiv-Paper-List-Gaussian-Splatting - GitHub, https://github.com/Lee-JaeWon/2025-Arxiv-Paper-List-Gaussian-Splatting</li>
<li>OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS - arXiv, https://arxiv.org/abs/2511.09397</li>
<li>Next Best Sense: Guiding Vision and Touch with FisherRF for 3D Gaussian Splatting, https://arm.stanford.edu/next-best-sense</li>
<li>Next Best Sense: Guiding Vision and Touch with FisherRF for 3D Gaussian Splatting - arXiv, https://arxiv.org/html/2410.04680v1</li>
<li>Learning Active Perception via Self‑Evolving Preference Optimization for GUI Grounding, https://arxiv.org/html/2509.04243v1</li>
<li>Real-World Reinforcement Learning of Active Perception Behaviors - OpenReview, https://openreview.net/pdf/9898464c3583495f1ef4b902b09d34126b164835.pdf</li>
<li>Real-World Reinforcement Learning of Active Perception Behaviors - NeurIPS, https://neurips.cc/virtual/2025/136079</li>
<li>Real-World Reinforcement Learning of Active Perception Behaviors - arXiv, https://arxiv.org/html/2512.01188v1</li>
<li>AAWR - GitHub Pages, https://penn-pal-lab.github.io/aawr/</li>
<li>Robotic Grasping of Fully-Occluded Objects using RF Perception - MIT, https://www.mit.edu/~fadel/papers/RFGrasp-paper.pdf</li>
<li>GraspView: Active Perception Scoring and Best-View Optimization for Robotic Grasping in Cluttered Environments - arXiv, https://arxiv.org/html/2511.04199v1</li>
<li>GraspView: Active Perception Scoring and Best-View Optimization for Robotic Grasping in Cluttered Environments | Request PDF - ResearchGate, https://www.researchgate.net/publication/397366344_GraspView_Active_Perception_Scoring_and_Best-View_Optimization_for_Robotic_Grasping_in_Cluttered_Environments</li>
<li>[2511.04199] GraspView: Active Perception Scoring and Best-View Optimization for Robotic Grasping in Cluttered Environments - arXiv, https://arxiv.org/abs/2511.04199</li>
<li>(PDF) Multi-View Picking: Next-best-view Reaching for Improved Grasping in Clutter, https://www.researchgate.net/publication/335143712_Multi-View_Picking_Next-best-view_Reaching_for_Improved_Grasping_in_Clutter</li>
<li>Grasp the Invisibility by Vision-Language guided Active View Planning - Dynamic Automata Lab, https://dyalab.mines.edu/2025/icra-workshop/2.pdf</li>
<li>Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting - arXiv, https://arxiv.org/html/2512.22771v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>