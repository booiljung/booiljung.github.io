<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.6.1 불확실성 추정(Uncertainty Estimation): 로봇은 자신이 무엇을 모르는지 아는가?</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.6.1 불확실성 추정(Uncertainty Estimation): 로봇은 자신이 무엇을 모르는지 아는가?</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <a href="index.html">7.6 능동적 지각과 다음 최적 시점 (Active Perception & Next Best View)</a> / <span>7.6.1 불확실성 추정(Uncertainty Estimation): 로봇은 자신이 무엇을 모르는지 아는가?</span></nav>
                </div>
            </header>
            <article>
                <h1>7.6.1 불확실성 추정(Uncertainty Estimation): 로봇은 자신이 무엇을 모르는지 아는가?</h1>
<h2>1.  서론: 기계적 확신을 넘어선 인지적 겸손함</h2>
<p>현대 로봇 공학에서 “지능(Intelligence)“의 정의는 단순히 주어진 작업을 높은 정확도로 수행하는 것을 넘어섰다. 진정한 지능은 자신의 한계를 인지하는 능력, 즉 메타 인지(Meta-cognition)를 포함한다. 딥러닝(Deep Learning) 기술의 비약적인 발전으로 로봇의 시각적 인지 능력은 인간의 수준에 근접하거나 특정 태스크에서는 이를 상회하고 있다. 그러나 전통적인 결정론적(Deterministic) 신경망 모델은 치명적인 결함을 가지고 있다. 그것은 바로 “잘못된 확신(Overconfidence)“이다. 99%의 정확도를 가진 모델이라 하더라도, 학습하지 않은 데이터나 모호한 상황에 직면했을 때 “모른다“라고 답하는 대신, 틀린 답을 100%의 확신을 가지고 출력하는 경향이 있다.</p>
<p>자율주행 자동차가 도로 위의 낯선 장애물을 ’그림자’라고 99% 확신하여 주행하거나, 의료 수술 로봇이 본 적 없는 병변 조직을 정상 조직으로 오판하여 절개하는 시나리오는 상상만으로도 끔찍하다. 이처럼 안전이 중요한(Safety-critical) 고위험 도메인에서 로봇이 자신의 예측에 대해 얼마나 확신하는지, 혹은 얼마나 불확실한지를 정량화하는 능력은 선택이 아닌 필수적인 생존 요건이 되었다. 불확실성 추정(Uncertainty Estimation)은 로봇에게 ’겸손함’을 가르치는 기술이다. 이는 로봇이 자신의 판단이 틀릴 수 있음을 수학적으로 인지하고, 불확실성이 높을 때 인간에게 제어권을 넘기거나(Hand-over), 추가적인 정보를 얻기 위해 능동적으로 탐색(Active Exploration)하도록 유도하는 의사결정의 기초가 된다.</p>
<p>본 장에서는 로봇 지각(Perception) 시스템이 직면하는 불확실성의 근원적 유형을 수학적, 철학적으로 분해하고, 이를 정량화하기 위한 최신 딥러닝 방법론—베이지안 신경망(Bayesian Neural Networks)의 근사법부터 최신 증거적 딥러닝(Evidential Deep Learning)까지—을 심층적으로 분석한다. 나아가, 추정된 불확실성을 단순히 수동적인 모니터링 지표로 사용하는 것을 넘어, 로봇이 능동적으로 정보를 획득하게 만드는 **능동적 인식(Active Perception)**의 트리거(Trigger) 메커니즘과, 최근 3D 비전의 핵심인 <strong>NeRF(Neural Radiance Fields)</strong> 및 <strong>3D Gaussian Splatting</strong>에서의 불확실성 처리 기법까지 포괄적으로 다룬다.</p>
<h2>2.  불확실성의 이원론적 구조: Aleatoric과 Epistemic의 분해</h2>
<p>불확실성을 다루기 위한 첫 번째 단계는 “왜 불확실한가?“에 대한 원인을 규명하는 것이다. 학계와 산업계에서는 이를 크게 **Aleatoric Uncertainty(데이터/우연적 불확실성)**와 **Epistemic Uncertainty(모델/인식적 불확실성)**로 구분한다. 이 두 가지는 로봇이 환경에 대응하는 전략(Policy)을 수립하는 데 있어 전혀 다른 함의를 갖는다.</p>
<h3>2.1  Aleatoric Uncertainty (우연적 불확실성)</h3>
<p>Aleatoric이라는 용어는 주사위 놀이를 뜻하는 라틴어 <em>aleator</em>에서 유래했다. 이는 데이터 생성 과정 자체에 내재된 무작위성(Randomness)이나 노이즈에서 기인하며, 아무리 많은 데이터를 수집하고 모델을 고도화해도 원칙적으로 줄일 수 없는(Irreducible) 불확실성으로 정의된다.</p>
<ul>
<li><strong>로봇 공학적 현상:</strong> 라이다(LiDAR) 센서가 비나 안개에 의해 난반사를 일으키는 경우, 카메라 이미지에 심한 모션 블러(Motion Blur)가 발생한 경우, 혹은 깊이를 가늠할 수 없는 물웅덩이 표면과 같은 상황이 이에 해당한다. 이 경우 로봇이 아무리 더 많은 훈련 데이터를 학습하더라도, 해당 순간의 센서 데이터 자체가 가진 모호함은 사라지지 않는다.</li>
<li><strong>세부 유형:</strong></li>
<li><strong>Homoscedastic Uncertainty (등분산성):</strong> 입력 데이터와 무관하게 모든 데이터 포인트에서 노이즈 레벨이 일정하다고 가정한다. 주로 멀티태스크 학습(Multi-task Learning)에서 각 태스크 간의 손실 함수(Loss Function) 가중치를 조절하는 데 사용된다.</li>
<li><strong>Heteroscedastic Uncertainty (이분산성):</strong> 입력 데이터에 따라 노이즈 레벨이 달라진다고 가정한다. 로봇 공학에서는 이분산성 모델링이 훨씬 중요하다. 예를 들어, 맑은 하늘 영역(저노이즈)과 텍스처가 복잡한 덤불 영역(고노이즈)을 구분하여, 덤불 영역에서의 뎁스(Depth) 추정 불확실성을 더 높게 예측해야 하기 때문이다.</li>
</ul>
<h3>2.2  Epistemic Uncertainty (인식적 불확실성)</h3>
<p>Epistemic은 지식을 뜻하는 그리스어 <em>episteme</em>에서 유래했다. 이는 “지식의 부재“에서 오는 불확실성으로, 모델이 훈련 데이터에서 보지 못한 영역(Out-of-Distribution, OOD)에 대해 가지는 무지를 나타낸다.</p>
<ul>
<li><strong>특성:</strong> 이론적으로 더 많은 데이터를 수집하여 학습하거나, 모델의 구조(Architecture)를 개선하면 줄일 수 있는(Reducible) 불확실성이다.</li>
<li><strong>로봇 공학적 현상:</strong> 공장 바닥(평지)에서만 훈련된 로봇이 자갈밭이나 경사로를 처음 마주했을 때, 또는 낮에만 주행한 자율주행차가 밤거리를 주행할 때 발생하는 불확실성이다. 이때 모델은 입력 패턴을 해석할 수 있는 내부 표현(Representation)을 갖추지 못했으므로, 높은 인식적 불확실성을 출력해야 한다.</li>
<li><strong>전략적 함의:</strong> Epistemic Uncertainty가 높다는 것은 로봇에게 “이곳은 내가 모르는 영역이다“라는 강력한 신호다. 이는 로봇이 속도를 줄이거나(Safety), 해당 영역을 더 자세히 탐색(Exploration)하거나, 인간에게 도움을 요청하는 근거가 된다.</li>
</ul>
<h3>2.3  수학적 통합 및 예측 불확실성</h3>
<p>실제 로봇의 추론 과정에서 관측되는 전체 불확실성(Predictive Uncertainty)은 이 두 가지 성분의 합으로 표현된다. 회귀(Regression) 문제에서 입력 <span class="math math-inline">x</span>에 대한 출력 <span class="math math-inline">y</span>의 분산 <span class="math math-inline">Var(y|x)</span>은 다음과 같이 분해될 수 있다:</p>
<p><span class="math math-display">\underbrace{Var(y|x)}_{\text{Total Predictive Uncertainty}} \approx \underbrace{\sigma^2_a(x)}_{\text{Aleatoric (Data Noise)}} + \underbrace{\sigma^2_e(x)}_{\text{Epistemic (Model Variance)}}</span></p>
<p>여기서 <span class="math math-inline">\sigma^2_a(x)</span>는 데이터 자체의 노이즈를 예측하도록 학습된 항이며, <span class="math math-inline">\sigma^2_e(x)</span>는 모델 파라미터의 분포 혹은 앙상블 모델 간의 예측 불일치도에서 유도된다. 의 연구에 따르면, 로봇 지각 모델을 학습시킬 때 이 두 가지를 명확히 구분하는 것은 경로 계획(Planning) 단계에서 결정적인 차이를 만든다. Aleatoric 불확실성이 높은 구역(예: 시야가 차단된 안개 지역)은 ’위험 구역’으로 간주하여 회피해야 하지만, Epistemic 불확실성이 높은 구역(예: 가보지 않은 골목)은 정보를 얻기 위해 ’탐험 대상’이 되기 때문이다.</p>
<hr />
<h2>7.6.1.3 불확실성 정량화(UQ) 방법론: 베이지안에서 증거적 딥러닝까지</h2>
<p>로봇 시스템, 특히 제한된 배터리와 연산 능력을 가진 임베디드(Embedded) 환경에서 불확실성을 추정하는 것은 정확도와 계산 비용(Computational Cost) 사이의 끊임없는 트레이드오프(Trade-off) 문제이다. 최근 연구들은 이 문제를 해결하기 위해 몬테카를로 샘플링부터 단일 패스(Single-pass) 추정까지 다양한 스펙트럼의 기술을 제안하고 있다.</p>
<h3>1. 몬테카를로 드롭아웃 (MC Dropout): 가장 널리 쓰이는 베이지안 근사</h3>
<p>Gal &amp; Ghahramani(2016)에 의해 대중화된 MC Dropout은 베이지안 신경망(Bayesian Neural Network, BNN)의 복잡한 연산을 드롭아웃(Dropout)을 통해 근사하는 실용적인 방법이다.</p>
<ul>
<li><strong>메커니즘:</strong> 학습 시에만 활성화하던 드롭아웃을 추론(Inference) 단계에서도 켜둔다. 동일한 입력 이미지에 대해 <span class="math math-inline">T</span>번의 순전파(Forward Pass)를 수행하면, 매번 랜덤하게 다른 뉴런이 비활성화되므로 서로 다른 <span class="math math-inline">T</span>개의 예측값이 생성된다.
<ul>
<li><strong>예측값의 평균:</strong> 최종 예측값.</li>
<li><strong>예측값의 분산:</strong> Epistemic Uncertainty (모델의 불확실성).</li>
</ul>
</li>
<li><strong>장점:</strong> 모델 구조를 변경할 필요가 없고 구현이 매우 간단하다. 기존에 학습된 모델(Pre-trained Model)에도 드롭아웃 레이어만 있다면 즉시 적용 가능하다. 이는 뎁스 추정(Depth Estimation)부터 객체 인식까지 다양한 로봇 태스크에 범용적으로 적용된다.</li>
<li><strong>한계 및 실시간성 이슈:</strong>
<ul>
<li><strong>연산 비용:</strong> <span class="math math-inline">T</span>번의 반복 추론이 필요하므로, <span class="math math-inline">T=10</span>일 경우 이론적으로 추론 속도가 1/10로 감소한다. 30Hz 이상의 실시간 처리가 필요한 자율주행이나 드론 제어에서 이는 치명적인 병목이 될 수 있다.</li>
<li><strong>벤치마크 결과:</strong> 의 엣지 디바이스(CPU 환경) 실험에 따르면, 베이스라인 모델이 21 FPS를 기록한 반면, MC Dropout은 7 FPS 미만의 속도를 보여 실시간 처리에 부적합함이 드러났다.</li>
<li><strong>모드 붕괴(Mode Collapse):</strong> 드롭아웃으로 생성된 앙상블은 단일 모델의 파라미터 공간 내에서만 변동하므로, 분포의 다양성이 충분하지 않아 불확실성을 과소평가(Underestimate)하는 경향이 있다.</li>
</ul>
</li>
</ul>
<h3>2. 딥 앙상블 (Deep Ensembles): 정확도의 골드 스탠다드</h3>
<p>서로 다른 랜덤 시드(Random Seed)로 초기화된 <span class="math math-inline">M</span>개의 독립적인 신경망을 학습시키고, 이들의 예측 분포를 결합하는 방식이다.</p>
<ul>
<li><strong>메커니즘:</strong> 각 모델은 Aleatoric uncertainty(분산 <span class="math math-inline">\sigma^2</span>)를 출력하도록 학습된다(Negative Log-Likelihood Loss 사용). 최종적으로 <span class="math math-inline">M</span>개 모델 예측의 평균을 구하고, “모델 간 예측의 분산(Epistemic)“과 “개별 모델이 예측한 분산의 평균(Aleatoric)“을 합하여 전체 불확실성을 도출한다.</li>
<li><strong>성능 우위:</strong> 현재까지 알려진 방법 중 가장 신뢰성(Calibration)이 높고 OOD 탐지 성능이 우수하다. 와 의 연구에서는 6D Pose Estimation과 같은 복잡한 회귀 문제에서 딥 앙상블이 가장 잘 보정된(Calibrated) 불확실성을 제공함을 입증했다.</li>
<li><strong>치명적 단점:</strong> <span class="math math-inline">M</span>개의 모델을 메모리에 로드하고 실행해야 하므로 메모리 사용량과 연산량이 정확히 <span class="math math-inline">M</span>배 증가한다. 리소스가 제한적인 모바일 로봇에는 적용하기 어렵지만, 서버 기반의 클라우드 로보틱스나 오프라인 맵핑 분석에는 적합하다.</li>
</ul>
<h3>3. 증거적 딥러닝 (Evidential Deep Learning, EDL): 효율성의 혁명</h3>
<p>최근 로봇 공학계에서 가장 주목받는 기법으로, 단 한 번의 순전파(Single Forward Pass)만으로 Aleatoric과 Epistemic 불확실성을 동시에 분리해낸다.</p>
<ul>
<li><strong>이론적 배경: 주관적 논리(Subjective Logic):</strong> EDL은 뎀스터-쉐퍼 이론(Dempster-Shafer Theory)에 기반한다. 분류(Classification) 문제의 경우, 모델은 클래스 확률(Softmax output)을 직접 예측하는 것이 아니라, 그 확률 분포를 생성하는 **디리클레 분포(Dirichlet Distribution)**의 파라미터(<span class="math math-inline">\alpha</span>)를 예측한다. 이는 “확률에 대한 확률(Second-order probability)“을 다루는 것이다.</li>
<li><strong>증거(Evidence)와 불확실성의 관계:</strong>
<ul>
<li>모델의 출력(Logit)이 클수록 해당 클래스에 대한 ’증거(<span class="math math-inline">e</span>)’가 많은 것으로 간주한다 (<span class="math math-inline">\alpha = e + 1</span>).</li>
<li>모든 클래스에 대한 증거가 부족하면(<span class="math math-inline">e \approx 0</span>), 디리클레 분포는 평평(Flat)해지며, 이는 높은 <strong>Epistemic Uncertainty</strong>를 의미한다.</li>
<li>증거가 상충하면(예: 클래스 A와 B의 증거가 모두 높음), <strong>Aleatoric Uncertainty</strong>가 높게 측정된다.</li>
</ul>
</li>
<li><strong>성능 및 효율성:</strong>
<ul>
<li><strong>속도:</strong> MC Dropout이나 앙상블과 달리 추가적인 샘플링이 필요 없어 실시간성이 보장된다.  실험에서 EDL은 베이스라인 모델과 거의 차이 없는 19 FPS를 기록하며 실시간성을 입증했다.</li>
<li><strong>OOD 탐지:</strong> 훈련 데이터와 다른 분포의 데이터(OOD)가 입력되면 증거(Evidence) 값이 0에 수렴하도록 유도하는 정규화 항(KL Divergence term)을 손실 함수에 포함시킨다. 이를 통해 의 연구에서는 MNIST로 훈련된 모델이 NotMNIST 데이터셋에 대해 앙상블 방식보다 6배 더 뛰어난 OOD 탐지 성능을 보임을 확인했다.</li>
<li><strong>최신 확장:</strong> 초기에는 분류 문제에 주로 사용되었으나, 최근에는 연속적인 값을 다루는 회귀 문제(Deep Evidential Regression)로 확장되어 물체 감지, 뎁스 추정, 로봇 팔 제어 등 다양한 로봇 작업에 적용되고 있다.</li>
</ul>
</li>
</ul>
<h3>4. 불확실성 정량화 방법론 비교 요약</h3>
<p>다음 표는 로봇 공학 관점에서 주요 불확실성 추정 방법론의 특성을 비교 분석한 것이다.</p>
<table><thead><tr><th><strong>특성</strong></th><th><strong>MC Dropout</strong></th><th><strong>Deep Ensembles</strong></th><th><strong>Evidential Deep Learning (EDL)</strong></th></tr></thead><tbody>
<tr><td><strong>기반 이론</strong></td><td>베이지안 근사 (Variational Inference)</td><td>빈도주의적/베이지안 혼합</td><td>뎀스터-쉐퍼 이론 / 주관적 논리</td></tr>
<tr><td><strong>추론 횟수 (Latency)</strong></td><td>다수 (<span class="math math-inline">T \ge 10</span>)</td><td>다수 (<span class="math math-inline">M \ge 5</span>)</td><td><strong>단일 (1회)</strong></td></tr>
<tr><td><strong>메모리 비용</strong></td><td>낮음 (단일 모델)</td><td><strong>높음 (다중 모델 로드)</strong></td><td>낮음 (단일 모델)</td></tr>
<tr><td><strong>불확실성 품질 (Calibration)</strong></td><td>중간 (과소평가 경향)</td><td><strong>최상 (Gold Standard)</strong></td><td>우수 (데이터/Loss 의존적)</td></tr>
<tr><td><strong>OOD 탐지 성능</strong></td><td>양호</td><td>우수</td><td><strong>매우 우수 (이론적 보장)</strong></td></tr>
<tr><td><strong>실시간성 (FPS)</strong></td><td>낮음 (Edge 부적합, &lt;7 FPS)</td><td>낮음</td><td><strong>높음 (Real-time, ~19 FPS)</strong></td></tr>
<tr><td><strong>주요 적용처</strong></td><td>오프라인 분석, 의료 영상</td><td>고성능 서버 기반 AI</td><td>자율주행, 모바일 로봇, 드론</td></tr>
</tbody></table>
<hr />
<h2>7.6.1.4 능동적 인식(Active Perception): 불확실성을 탐험의 동력으로</h2>
<p>로봇이 자신이 무엇을 모르는지 알게 되었다면, 그 다음 단계는 무엇인가? 수동적인 AI는 단순히 “신뢰도가 낮음“이라는 경고를 띄우고 멈추지만, 능동적인 로봇(Active Agent)은 그 불확실성을 줄이기 위해 행동한다. 이것이 바로 **능동적 인식(Active Perception)**의 핵심이다. 로봇은 불확실성을 최소화하는 방향으로 센서를 움직이거나, 이동하거나, 물체를 조작한다.</p>
<h3>1. 불확실성 기반 트리거 메커니즘 (Uncertainty as a Trigger)</h3>
<p>로봇은 항상 고비용의 정밀 스캔이나 복잡한 경로 계획을 수행할 수 없다. 에너지 효율성을 위해 평소에는 저비용의 빠른 인식(Deterministic Perception)을 수행하다가, 추정된 불확실성이 허용 범위를 넘을 때만 고비용의 능동적 행동을 트리거하는 전략이 필요하다.</p>
<ul>
<li><strong>ACP vs. DCP:</strong> 의 연구에서는 로봇이 익숙한 자극(낮은 불확실성)에 대해서는 ’결정적 조건 인식(DCP, Determined Condition Perception)’을, 낯선 자극(높은 불확실성)에 대해서는 ’능동적 조건 인식(ACP, Active Condition Perception)’을 수행하도록 전환하는 메커니즘을 제안했다.</li>
<li><strong>엔트로피 기반 임계값 (Thresholding):</strong> 트리거의 기준으로는 주로 정보 이론의 **샤논 엔트로피(Shannon Entropy)**가 사용된다. 엔트로피가 높다는 것은 예측 분포가 평평(Flat)하다는 뜻이며, 이는 곧 높은 불확실성을 의미한다. 의 연구들은 엔트로피 값에 동적 임계값을 적용하여 이상 징후(Anomaly)를 감지하거나 능동적 학습(Active Learning) 데이터를 선별하는 데 활용했다.</li>
</ul>
<h3>2. 정보 이득(Information Gain)의 최대화와 루프</h3>
<p>능동적 인식의 목표는 미래의 관측을 통해 얻을 수 있는 **정보 이득(Information Gain, IG)**을 최대화하는 행동 <span class="math math-inline">a^*</span>를 선택하는 것이다. 이를 수식으로 표현하면 다음과 같다:</p>
<p><span class="math math-display">IG(a, \xi) = H(\xi) - E_{z|a}[H(\xi|z, a)]</span></p>
<p>여기서 <span class="math math-inline">H(\xi)</span>는 현재 상태의 엔트로피, 두 번째 항은 행동 <span class="math math-inline">a</span>를 취해 관측 <span class="math math-inline">z</span>를 얻었을 때 예상되는 사후 엔트로피(Posterior Entropy)의 기댓값이다. 즉, 행동 전후의 불확실성 감소량이 곧 정보 이득이다.</p>
<ul>
<li><strong>Next-Best-View (NBV) Planning:</strong> 3D 재구성이나 탐색 구조 로봇에서 로봇은 현재 시점의 불확실성 맵(Uncertainty Map)을 기반으로, 아직 보지 못한 영역(Epistemic Uncertainty가 높은 영역)이나 형상이 모호한 영역을 가장 잘 관측할 수 있는 다음 카메라 위치를 계산한다. 의 연구에서는 게임 이론을 접목하여 정보 이득 추정 오차를 최소화하는 NBV 알고리즘을 통해 정보 획득량을 7% 증가시키고 추정 오차를 42% 감소시켰다.</li>
</ul>
<h3>2.4  실제 적용 사례: 촉각 탐색과 물체 네비게이션</h3>
<ul>
<li><strong>U-GRAPH (촉각 기반 회전 탐색):</strong> 의 연구에서는 로봇 팔이 물체를 잡았을 때, 무게 중심(CoM) 추정의 불확실성이 높으면 베이지안 신경망(BNN)을 통해 정보 이득이 가장 큰 회전 각도를 계산한다. 이후 물체를 해당 각도로 회전시켜 추가적인 힘/토크 센서 데이터를 수집함으로써 불확실성을 줄이는 능동적 루프를 구현했다.</li>
<li><strong>Object Goal Navigation (ObjectNav):</strong> “침실로 가서 안경을 찾아라“와 같은 명령을 수행할 때, 로봇은 시각적 정보뿐만 아니라 언어 모델(VLM)의 의미론적 불확실성(Semantic Uncertainty)을 모델링해야 한다. 의 연구는 프롬프트의 모호함이나 환경 인식의 불확실성을 고려한 다중 무장 강도(Multi-Armed Bandit) 기반의 탐험 계획을 통해, 불확실한 영역을 우선적으로 탐색하거나 반대로 확실한 단서를 따라가는 전략적 탐색을 보여주었다.</li>
</ul>
<hr />
<h2>3.  고차원 공간 지각에서의 불확실성: 6D Pose와 3D Vision</h2>
<p>단순한 2D 이미지 분류를 넘어, 로봇이 물체를 조작(Manipulation)하거나 3차원 공간을 이해해야 하는 고차원 문제에서 불확실성 추정은 더욱 복잡하고 필수적인 과제가 된다. 특히 6D Pose Estimation과 NeRF/3D Gaussian Splatting 분야에서 혁신적인 연구들이 2024-2025년에 집중적으로 발표되었다.</p>
<h3>3.1  6D 물체 자세 추정 (6D Pose Estimation)의 불확실성</h3>
<p>물체의 3차원 위치(x, y, z)와 회전(Roll, Pitch, Yaw)을 추정하는 6D Pose Estimation은 로봇 팔 조작의 핵심이다. 그러나 금속 부품처럼 반사율이 높거나(Textureless), 컵이나 나사처럼 대칭성을 가진 물체는 시각적으로 모호하여 자세 추정이 매우 어렵다.</p>
<ul>
<li><strong>대칭성과 불확실성 모델링:</strong> 대칭 물체는 여러 각도에서 동일하게 보이므로, 단일 정답(Unimodal)을 예측하는 모델은 평균값으로 수렴하여 엉뚱한 자세를 예측하는 문제가 발생한다. 이를 해결하기 위해 <strong>Bingham Distribution</strong>이나 <strong>Fisher Distribution</strong>과 같은 방향성 통계 분포를 사용하여 회전 불확실성을 모델링해야 한다.</li>
<li><strong>UA-Pose (Uncertainty-Aware Pose Estimation):</strong> 에서 제안된 UA-Pose는 부분적으로만 관측된(Occluded) 물체에 대해 ‘메모리 풀(Memory Pool)’ 전략을 도입했다. 신뢰도(Confidence)가 높은 자세 추정값은 메모리에 저장하고, 신뢰도가 낮은 경우(불확실성이 높은 경우) 메모리 풀에 저장된 이전 관측값들을 활용해 온라인으로 물체의 형상을 완성(Completion)해 나간다. 이는 불확실성을 단순히 필터링하는 것이 아니라, 데이터 축적의 기준으로 활용한 사례다.</li>
<li><strong>SLUE (S-Lemma Uncertainty Estimation):</strong> 의 연구는 6D Pose의 불확실성을 <strong>볼록 최적화(Convex Optimization)</strong> 기법인 S-Lemma를 사용하여 타원체(Ellipsoid) 형태로 정량화했다. 딥러닝 모델의 출력이 흔히 가지는 ‘블랙박스’ 성격을 극복하고, 기하학적 제약 조건을 통해 “참값이 이 타원체 안에 있을 확률“을 수학적으로 보장(Guarantee)한다. 이는 로봇 팔이 물체를 잡을 때 충돌을 방지하는 안전 마진(Safety Margin)을 설정하는 데 있어 딥러닝 기반의 불확실성보다 훨씬 신뢰할 수 있는 경계를 제공한다.</li>
</ul>
<h3>3.2  NeRF 및 3D Gaussian Splatting (3DGS)의 불확실성</h3>
<p>최근 3D 재구성의 패러다임을 바꾼 NeRF(Neural Radiance Fields)와 3DGS에서도 불확실성 추정은 핫 토픽이다. 이들은 새로운 시점(Novel View)을 합성할 때, 학습 데이터가 없는 각도에서 어떤 색상이나 형태를 보여줘야 할지 결정해야 한다.</p>
<ul>
<li><strong>View-Dependent Uncertainty:</strong> 3DGS는 보는 각도에 따라 색상이 달라지는 성질(Spherical Harmonics)을 갖는다. 는 3D Gaussian 각각에 대해 ’불확실성 속성’을 추가하여, 특정 각도에서만 불확실한 경우(예: 반사광)와 모든 각도에서 불확실한 경우(예: 가려진 영역)를 구분하여 모델링했다. 이를 통해 렌더링 시 불확실한 영역을 흐리게 처리하거나 로봇에게 “더 가까이 가서 보라“는 신호를 줄 수 있다.</li>
<li><strong>PUP 3D-GS (Principled Uncertainty Pruning):</strong> 는 모델 압축(Compression)을 위해 불확실성 개념을 도입했다. 학습 데이터에 대한 재구성 오차의 2차 미분값(Hessian)을 근사한 **피셔 정보(Fisher Information)**를 민감도 점수(Sensitivity Score)로 사용한다. 즉, “이 Gaussian을 제거했을 때 전체 에러가 얼마나 증가할 것인가(얼마나 불확실해지는가?)“를 수치화하여, 정보량이 적은 Gaussian을 과감히 제거(Pruning)함으로써 렌더링 속도를 3배 이상 높이면서도 품질을 유지했다. 이는 불확실성 추정이 단순히 안전뿐만 아니라 효율성 증대에도 기여함을 보여준다.</li>
</ul>
<h3>3.3  로봇 네비게이션 및 자율주행에서의 응용</h3>
<ul>
<li><strong>MaskVal:</strong> 의 연구는 복잡한 앙상블 대신, 포즈 추정치(Pose Estimate)를 다시 렌더링하여 원본 이미지의 인스턴스 세그멘테이션 마스크와 비교하는 단순한 방법(MaskVal)이 실제 로봇 조작 태스크에서 더 정확한 불확실성 지표가 됨을 보였다.</li>
<li><strong>리스크 인지 제어 (Risk-Aware Control):</strong> 의 연구들은 자율주행 레이싱이나 교차로 주행 시, 도로 곡률 추정의 불확실성을 시스템 역학(Dynamics)에 통합하여, 불확실성이 높을수록 속도를 줄이거나 안전 거리를 확보하는 모델 예측 제어(MPC) 기법을 제안했다.</li>
</ul>
<hr />
<h2>4.  결론: 로봇, 무지를 자각하고 진화하다</h2>
<p>“로봇은 자신이 무엇을 모르는지 아는가?“라는 질문에 대한 답은 이제 “예, 그리고 그것을 줄이기 위해 행동합니다“로 진화하고 있다. 초기 로봇 공학이 결정론적 세계관에서 완벽한 제어를 꿈꿨다면, 현대의 인지 로봇 공학은 확률론적 세계관 위에서 불확실성을 관리하는 법을 배우고 있다.</p>
<p>MC Dropout과 딥 앙상블과 같은 초기 기법들은 높은 계산 비용으로 인해 실시간 로봇 적용에 한계가 있었으나, **증거적 딥러닝(Evidential Deep Learning)**과 같은 단일 패스 기법의 등장은 엣지 디바이스에서도 신뢰할 수 있는 불확실성 추정을 가능하게 했다. 더 나아가, 추정된 불확실성은 더 이상 제거해야 할 노이즈가 아니라, 로봇의 호기심을 유발하고(Exploration), 안전한 동작을 유도하며(Safety), 효율적인 데이터 수집을 가능하게 하는(Active Learning) 핵심적인 **정보원(Information Source)**으로 재정의되고 있다.</p>
<p>앞으로의 연구는 텍스트, 이미지, 촉각, 3D 공간 정보를 아우르는 <strong>멀티모달(Multimodal) 불확실성 통합</strong>과, 인간의 의도와 같은 추상적인 개념의 불확실성을 다루는 방향으로 나아갈 것이다. 로봇이 자신의 무지를 정확히 자각하고 표현할 때, 비로소 인간은 로봇을 ’블랙박스’가 아닌 신뢰할 수 있는 파트너로 받아들이고 복잡한 현실 세계를 함께 살아갈 수 있을 것이다.</p>
<h2>5. 참고 자료</h2>
<ol>
<li>From Aleatoric to Epistemic: Exploring Uncertainty Quantification …, https://arxiv.org/html/2501.03282v1</li>
<li>Bayesian Deep Neural Networks for Supervised Learning of Single- …, https://ieeexplore.ieee.org/document/9681190/</li>
<li>Bayesian Neural Networks versus deep ensembles for uncertainty …, https://arxiv.org/abs/2509.19180</li>
<li>Multimodal perception-driven decision-making for human-robot …, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1604472/full</li>
<li>Epistemic Uncertainty in State Estimation and Belief Space Planning …, https://openreview.net/pdf?id=CPZaavSwXg</li>
<li>Introduction to Uncertainty in Machine Learning Models, https://blog.paperspace.com/aleatoric-and-epistemic-uncertainty-in-machine-learning/</li>
<li>Aleatoric and Epistemic Uncertainty Estimation in Autonomous Off- …, https://symposium.foragerone.com/speak-up-2023/presentations/58236</li>
<li>A Deeper Look into Aleatoric and Epistemic Uncertainty …, https://www.researchgate.net/publication/362899652_A_Deeper_Look_into_Aleatoric_and_Epistemic_Uncertainty_Disentanglement</li>
<li>Aleatory or Epistemic? Does It Matter? | Request PDF - ResearchGate, https://www.researchgate.net/publication/222422822_Aleatory_or_Epistemic_Does_It_Matter</li>
<li>Exploring Uncertainty and Movement in Categorical Perception …, https://meclab.w3.uvm.edu/papers/2016_PPSN_Powell.pdf</li>
<li>Active Inference with Dynamic Planning and Information Gain in …, https://www.mdpi.com/1099-4300/27/8/846</li>
<li>Applying Monte Carlo Dropout to Quantify the Uncertainty of Skip …, https://www.mdpi.com/2079-9292/12/6/1453</li>
<li>Divergent Ensemble Networks: Enhancing Uncertainty Estimation …, https://arxiv.org/html/2412.01193v3</li>
<li>[PDF] Improving evidential deep learning via multi-task learning, https://www.semanticscholar.org/paper/Improving-evidential-deep-learning-via-multi-task-Oh-Shin/34148f8d11b814739e524bd834d5fe1f4b911d4e</li>
<li>Final Evaluation and Conclusion - Netidee, https://www.netidee.at/increasing-trustworthiness-edge-ai-adding-uncertainty-estimation-object-detection/final-evaluation</li>
<li>Deep Ensemble Bayesian Active Learning : Addressing the Mode …, https://arxiv.org/abs/1811.03897</li>
<li>Evidential Deep Learning for Uncertainty Quantification and Out-of …, https://arxiv.org/html/2501.05656v1</li>
<li>Evidential Deep Learning: Enhancing Predictive Uncertainty … - arXiv, https://arxiv.org/html/2309.13207v2</li>
<li>Uncertainty Quantification with Deep Ensembles for 6D Object Pose …, https://isprs-annals.copernicus.org/articles/X-2-2024/223/2024/</li>
<li>Uncertainty Quantification with Deep Ensembles for 6D Object Pose …, https://arxiv.org/html/2403.07741v1</li>
<li>Evidential Uncertainty Estimation for Multi-Modal Trajectory Prediction, https://arxiv.org/html/2503.05274v1</li>
<li>Uncertainty Estimation by Flexible Evidential Deep Learning - NeurIPS, https://neurips.cc/virtual/2025/poster/118400</li>
<li>Improved Evidential Deep Learning via a Mixture of Dirichlet … - arXiv, https://arxiv.org/html/2402.06160v1</li>
<li>The Unreasonable Effectiveness of Deep Evidential Regression, https://ojs.aaai.org/index.php/AAAI/article/view/26096/25868</li>
<li>A Comprehensive Survey on Evidential Deep Learning and Its …, https://arxiv.org/pdf/2409.04720</li>
<li>An Active Perception Game for Robust Information Gathering - arXiv, https://arxiv.org/html/2404.00769v2</li>
<li>An Active Perception Game for Robust Autonomous Exploration - arXiv, https://arxiv.org/html/2404.00769v1</li>
<li>Active sensing with predictive coding and uncertainty minimization, https://pmc.ncbi.nlm.nih.gov/articles/PMC11240181/</li>
<li>An Entropy-Based Approach for Anomaly Detection in Activities of …, https://pmc.ncbi.nlm.nih.gov/articles/PMC7517444/</li>
<li>An Entropy-Based Network Anomaly Detection Method - MDPI, https://www.mdpi.com/1099-4300/17/4/2367</li>
<li>Entropy-Based Active Learning for Object Detection With …, https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Entropy-Based_Active_Learning_for_Object_Detection_With_Progressive_Diversity_Constraint_CVPR_2022_paper.pdf</li>
<li>An Active Perception Game for Robust Information Gathering - arXiv, https://arxiv.org/abs/2404.00769</li>
<li>An Active Perception Approach for Estimating the Center of Mass of …, https://arxiv.org/html/2502.02663v1</li>
<li>Uncertainty-Informed Active Perception for Open Vocabulary Object …, https://arxiv.org/html/2506.13367v2</li>
<li>Uncertainty-Informed Active Perception for Open Vocabulary Object …, https://arxiv.org/html/2506.13367v1</li>
<li>UA-Pose: Uncertainty-Aware 6D Object Pose Estimation and … - arXiv, https://arxiv.org/html/2506.07996v1</li>
<li>Uncertainty-Driven 6D Pose Estimation of Objects and Scenes From …, https://openaccess.thecvf.com/content_cvpr_2016/papers/Brachmann_Uncertainty-Driven_6D_Pose_CVPR_2016_paper.pdf</li>
<li>6D Pose Estimation for Textureless Objects on RGB Frames using …, https://arxiv.org/abs/2210.11554</li>
<li>UA-Pose: Uncertainty-Aware 6D Object Pose Estimation and Online …, https://cvpr.thecvf.com/virtual/2025/poster/34276</li>
<li>UA-Pose: Uncertainty-Aware 6D Object Pose Estimation and Online …, https://www.researchgate.net/publication/394512127_UA-Pose_Uncertainty-Aware_6D_Object_Pose_Estimation_and_Online_Object_Completion_with_Partial_References</li>
<li>Uncertainty Quantification for Visual Object Pose Estimation - arXiv, https://arxiv.org/abs/2511.21666</li>
<li>View-Dependent Uncertainty Estimation of 3D Gaussian Splatting, https://arxiv.org/html/2504.07370v1</li>
<li>Uncertainty Estimation for Novel Views in Gaussian Splatting … - arXiv, https://arxiv.org/html/2508.02443v1</li>
<li>PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting, https://cvpr.thecvf.com/virtual/2025/poster/33353</li>
<li>PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting, https://pup3dgs.github.io/</li>
<li>PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting, https://www.computer.org/csdl/proceedings-article/cvpr/2025/436400f949/299d3EdY9DW</li>
<li>Simple but Effective Uncertainty Quantification for 6D Pose Estimation, https://ieeexplore.ieee.org/document/10711451/</li>
<li>Uncertainty-Aware Perception-Based Control for Autonomous Racing, https://arxiv.org/html/2508.02494</li>
<li>Misbehavior Detection in Uncertainty Aware Object Level Cooperative, https://rosap.ntl.bts.gov/view/dot/87356/dot_87356_DS1.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>