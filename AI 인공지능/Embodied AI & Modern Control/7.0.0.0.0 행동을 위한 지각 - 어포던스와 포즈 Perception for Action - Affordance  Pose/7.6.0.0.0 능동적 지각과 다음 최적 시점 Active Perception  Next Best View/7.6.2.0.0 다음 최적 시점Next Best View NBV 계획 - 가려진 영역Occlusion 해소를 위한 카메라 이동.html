<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.6.2 다음 최적 시점(Next Best View, NBV) 계획: 가려진 영역(Occlusion) 해소를 위한 카메라 이동</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.6.2 다음 최적 시점(Next Best View, NBV) 계획: 가려진 영역(Occlusion) 해소를 위한 카메라 이동</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 7. 행동을 위한 지각: 어포던스와 포즈 (Perception for Action: Affordance & Pose)</a> / <a href="index.html">7.6 능동적 지각과 다음 최적 시점 (Active Perception & Next Best View)</a> / <span>7.6.2 다음 최적 시점(Next Best View, NBV) 계획: 가려진 영역(Occlusion) 해소를 위한 카메라 이동</span></nav>
                </div>
            </header>
            <article>
                <h1>7.6.2 다음 최적 시점(Next Best View, NBV) 계획: 가려진 영역(Occlusion) 해소를 위한 카메라 이동</h1>
<p>로봇이 정해진 궤적을 따라 수동적으로 데이터를 수집하던 시대는 종말을 고했다. 엠바디드 AI(Embodied AI)의 핵심은 신체(Body)를 가진 지능이 환경과 물리적으로 상호작용하며 정보의 불확실성을 능동적으로 줄여나가는 데 있다. 특히 시각적 지각(Visual Perception)에 있어 가장 치명적인 적인 ’가려진 영역(Occlusion)’을 해소하는 능력은 로봇의 자율성을 결정짓는 척도가 된다. 로봇 팔이 복잡한 선반 뒤의 물건을 집어 올리거나, 드론이 숲의 캐노피 아래 숨겨진 실종자를 수색하는 상황에서, 로봇은 “현재 내가 무엇을 모르고 있는가?“를 인지하고, “어디를 보아야 그 불확실성을 가장 크게 줄일 수 있는가?“를 스스로 결정해야 한다. 이것이 바로 다음 최적 시점(Next Best View, NBV) 계획의 본질이다. 본 절에서는 고전적인 정보 이론(Information Theory) 기반의 기하학적 접근법부터, 최신 뉴럴 래디언스 필드(NeRF) 및 3D 가우시안 스플래팅(3D Gaussian Splatting)을 활용한 SOTA(State-of-the-Art) 기술, 그리고 강화학습(RL) 기반의 능동적 탐색 전략까지, 가려진 영역 해소를 위한 기술적 진보를 심도 있게 다룬다.</p>
<h2>1.  능동적 지각과 정보 불확실성의 정량화</h2>
<p>NBV 문제는 근본적으로 정보 획득(Information Acquisition)의 최적화 문제로 귀결된다. 로봇이 환경에 대해 가지는 지식은 불완전하며, 센서 데이터에는 잡음이 섞여 있다. 따라서 로봇은 현재의 맵(Map) 상태에서 엔트로피(Entropy)를 계산하고, 후보 시점(Candidate View)들 중 예상되는 정보 이득(Information Gain, IG)이 가장 높은 시점을 선택해야 한다.</p>
<h3>1.1  섀넌 엔트로피와 확률적 복셀 맵</h3>
<p>3차원 공간을 표현하는 가장 일반적인 방식은 공간을 균일한 격자(Grid)로 나누는 복셀(Voxel) 맵이나, 이를 계층적으로 압축한 옥토맵(OctoMap)을 사용하는 것이다. 각 복셀 <span class="math math-inline">v</span>는 해당 공간이 장애물로 채워져 있을 확률 <span class="math math-inline">P(v)</span>를 가진 확률 변수로 모델링된다. 이때, 정보 이론의 창시자 클로드 섀넌(Claude Shannon)이 정의한 엔트로피 <span class="math math-inline">H(v)</span>는 해당 복셀의 불확실성을 나타내는 척도로 사용된다.</p>
<p>이진 분류(Binary Classification) 문제, 즉 복셀이 ‘점유됨(Occupied)’ 혹은 ‘비어있음(Free)’ 중 하나라고 가정할 때, 섀넌 엔트로피는 다음과 같이 정의된다:<br />
<span class="math math-display">
H(v) = - \left( P(v) \log_2 P(v) + (1 - P(v)) \log_2 (1 - P(v)) \right)
</span><br />
이 수식의 거동을 분석해보면, <span class="math math-inline">P(v) = 0.5</span>일 때, 즉 해당 공간에 무엇이 있는지 전혀 알 수 없는 상태일 때 엔트로피는 최대값 <span class="math math-inline">1.0</span>을 가진다. 반면, 센서 관측을 통해 <span class="math math-inline">P(v)</span>가 <span class="math math-inline">0</span>이나 <span class="math math-inline">1</span>에 가까워질수록 불확실성은 해소되며 엔트로피는 <span class="math math-inline">0</span>으로 수렴한다. 따라서 NBV 계획의 목표는 전체 맵 공간 <span class="math math-inline">M</span>에 대한 총 엔트로피를 최소화하는 시점 <span class="math math-inline">V_{next}</span>를 찾는 것이다.</p>
<h3>1.2  정보 이득(Information Gain)과 상호 정보량(Mutual Information)</h3>
<p>단순히 엔트로피가 높은 곳을 보는 것만으로는 부족하다. 그곳이 실제로 센서에 의해 관측 가능한지(Visible)를 고려해야 한다. 이를 위해 정보 이득(IG) 개념이 도입된다. 현재 맵 상태를 <span class="math math-inline">M</span>, 특정 시점 <span class="math math-inline">c</span>에서 획득할 것으로 예상되는 관측 데이터를 <span class="math math-inline">z</span>라고 할 때, 시점 <span class="math math-inline">c</span>의 효용성(Utility)은 관측 전후의 엔트로피 감소량, 즉 상호 정보량(Mutual Information)으로 정의된다.<br />
<span class="math math-display">
IG(c) = I(M; Z) = H(M) - E_z \left[ H(M \vert z) \right]
</span><br />
여기서 <span class="math math-inline">H(M \vert z)</span>는 관측 <span class="math math-inline">z</span>가 주어졌을 때의 사후 엔트로피(Posterior Entropy)이다. 하지만 실제 관측값 <span class="math math-inline">z</span>는 미래의 데이터이므로 알 수 없다. 따라서 로봇은 현재 가지고 있는 맵을 바탕으로 가상의 센서 모델을 돌려 <span class="math math-inline">z</span>를 예측해야 한다. 이를 위해 레이 캐스팅(Ray Casting) 기법이 필수적으로 사용된다.</p>
<h3>1.3  레이 캐스팅의 계산 복잡도와 최적화</h3>
<p>레이 캐스팅은 센서 원점에서 시야각(FOV) 내의 각 픽셀 방향으로 가상의 광선을 투사하여, 광선이 통과하는 모든 복셀의 상태를 검사하는 과정이다. 전통적인 NBV 알고리즘은 수백 개의 후보 시점에 대해 각각 레이 캐스팅을 수행하여 예상되는 IG를 계산한다.</p>
<p>그러나 이 방식은 <span class="math math-inline">O(N_{rays} \times D_{depth})</span>의 높은 계산 복잡도를 가진다. 특히 고해상도 3D 재구성이나 라이다(LiDAR) 센서와 같이 데이터 양이 방대한 경우, 레이 캐스팅 기반의 IG 계산은 실시간 로봇 제어의 병목(Bottleneck)이 된다. 예를 들어, <span class="math math-inline">640 \times 480</span> 해상도의 깊이 카메라를 시뮬레이션하기 위해서는 시점당 30만 번 이상의 광선 추적 연산이 필요하다.</p>
<p>이를 해결하기 위해 최신 연구에서는 <strong>투영 기반(Projection-based) NBV 계획</strong>이 제안되었다. 이 기법은 복잡한 복셀 구조를 타원체(Ellipsoid)와 같은 단순한 기하학적 형태로 근사(Approximation)하고, 이를 이미지 평면에 투영하여 커버리지를 계산한다. 연구 결과에 따르면, 투영 기반 방법은 레이 캐스팅 대비 10배 이상의 계산 속도 향상을 달성하면서도 유사한 수준의 커버리지를 보장한다. 이는 특히 빠른 기동이 필요한 드론이나 소형 로봇의 NBV 계획에 있어 중요한 기술적 진보다.</p>
<table><thead><tr><th><strong>기법 (Method)</strong></th><th><strong>원리 (Mechanism)</strong></th><th><strong>장점 (Pros)</strong></th><th><strong>단점 (Cons)</strong></th><th><strong>계산 복잡도</strong></th></tr></thead><tbody>
<tr><td><strong>레이 캐스팅 (Ray Casting)</strong></td><td>가상 광선을 투사하여 복셀 단위로 엔트로피 감소량 적분</td><td>정확한 가시성(Visibility) 및 정보 이득 계산 가능</td><td>계산 비용이 매우 높음, 실시간성 저하</td><td><span class="math math-inline">O(N_{cands} \cdot W \cdot H \cdot D)</span></td></tr>
<tr><td><strong>프론티어 탐색 (Frontier-based)</strong></td><td>탐색된 영역과 미탐색 영역의 경계(Frontier) 추출</td><td>구현이 간단하고 탐색 속도가 빠름</td><td>3D 형상 정보 및 가려진 영역 해소 미흡</td><td><span class="math math-inline">O(N_{voxels})</span></td></tr>
<tr><td><strong>투영 기반 (Projection-based)</strong></td><td>복셀 클러스터를 타원체로 근사 후 이미지 평면 투영</td><td>레이 캐스팅 대비 획기적인 속도 (10x), 효율적</td><td>미세한 형상 정보 손실 가능성</td><td><span class="math math-inline">O(N_{objects} \cdot N_{cands})</span></td></tr>
</tbody></table>
<h2>2.  기하학적 탐색에서 학습 기반 탐색으로의 진화</h2>
<p>초기의 NBV 연구는 기하학적 휴리스틱에 의존했다. 대표적인 **프론티어 기반 탐색(Frontier-based Exploration)**은 이미 아는 공간(Free Space)과 모르는 공간(Unknown Space)의 경계인 ’프론티어’로 로봇을 이동시키는 전략이다. 야마우치(Yamauchi)가 제안한 이래로 이 방법은 2D 맵 작성의 표준이 되었으나, 3D 공간의 복잡한 가려짐(Occlusion)을 해결하는 데는 한계가 명확하다. 단순히 미탐색 영역으로 가는 것과, 물체의 형상을 온전히 이해하기 위해 특정 각도로 이동하는 것은 다르기 때문이다.</p>
<p>현대의 SOTA 기술은 이러한 기하학적 접근을 넘어, <strong>데이터 기반(Data-driven)</strong> 및 <strong>학습 기반(Learning-based)</strong> 접근으로 이동하고 있다. 이는 로봇이 사전에 학습된 물체의 형상적 특징(Shape Prior)을 활용하거나, 딥러닝 모델 자체가 내포하는 불확실성을 지표로 삼아 탐색 효율을 극대화하는 방식이다.</p>
<h2>3.  뉴럴 래디언스 필드(NeRF)와 능동적 매핑 (Active NeRF)</h2>
<p>좌표 기반 신경망(Coordinate-based Neural Network)을 이용해 3D 장면을 암시적(Implicit)으로 표현하는 NeRF(Neural Radiance Fields)의 등장은 로봇 시각 시스템에 혁명을 가져왔다. NeRF는 3D 공간의 각 점 <span class="math math-inline">(x, y, z)</span>와 시선 방향 <span class="math math-inline">(\theta, \phi)</span>을 입력받아 색상(RGB)과 밀도(Density)를 출력하는 함수 <span class="math math-inline">F_\Theta</span>를 학습한다.<br />
<span class="math math-display">
F_\Theta(x, y, z, \theta, \phi) \rightarrow (c, \sigma)
</span><br />
기존의 복셀 맵이나 포인트 클라우드가 이산적(Discrete)이고 메모리 집약적인 반면, NeRF는 연속적(Continuous)이고 컴팩트한 표현이 가능하다. 그러나 로봇공학 관점에서 NeRF의 가장 큰 매력은 **불확실성 추정(Uncertainty Estimation)**과의 결합 가능성이다.</p>
<h3>3.1  NeRF의 불확실성 추정 메커니즘</h3>
<p>NBV 계획을 위해서는 현재 NeRF 모델이 렌더링한 이미지가 얼마나 신뢰할 수 있는지, 즉 불확실성이 어디서 높은지를 알아야 한다. <strong>ActiveNeRF</strong> 는 이를 위해 기존 NeRF의 색상 출력 <span class="math math-inline">c</span>를 단일 값이 아닌 가우시안 확률 분포 <span class="math math-inline">N(\mu, \sigma^2)</span>로 모델링한다.<br />
<span class="math math-display">
c(r) \sim N(\mu(r), \sigma^2(r))
</span><br />
여기서 분산 <span class="math math-inline">\sigma^2(r)</span>은 해당 광선(Ray)이 통과하는 영역에 대한 모델의 불확실성을 나타낸다. 학습 데이터가 충분한 영역에서는 분산이 작아지고, 관측되지 않은 가려진 영역에서는 분산이 커진다. NBV 알고리즘은 이 분산, 즉 불확실성이 가장 높은 시점을 다음 목표로 선정하여, 모델이 ‘가장 헷갈려하는’ 부분을 우선적으로 학습하게 한다.</p>
<p>이 외에도 <strong>앙상블(Ensemble)</strong> 기법이 사용된다. 여러 개의 NeRF 모델을 동시에 학습시키고, 동일한 시점에 대해 모델들이 출력하는 결과의 차이(Variance)를 불확실성으로 간주한다. 모델들이 서로 다른 예측을 내놓는다는 것은 해당 영역에 대한 정보가 부족하다는 강력한 증거가 되기 때문이다.</p>
<h3>3.2  맵리스(Mapless) 탐색: NeU-NBV</h3>
<p>전통적인 방식은 명시적인 맵(Occupancy Grid 등)을 유지하며 탐색을 수행하지만, <strong>NeU-NBV (Next Best View Planning Using Uncertainty Estimation)</strong> 는 명시적인 맵 없이(Mapless) 수집된 이미지들만으로 다음 시점을 결정하는 혁신적인 프레임워크를 제안한다.</p>
<p>NeU-NBV는 <strong>이미지 기반 뉴럴 렌더링(Image-based Neural Rendering)</strong> 기술을 활용한다. 이는 2D 이미지들로부터 특징(Feature)을 추출하여 3D 정보를 추론하는 방식으로, 매번 처음부터 학습해야 하는 NeRF의 단점인 긴 학습 시간을 극복한다. NeU-NBV의 핵심은 렌더링 과정에서 발생하는 불확실성을 엔트로피 기반으로 정량화하고, 이를 탐색의 가이드로 사용하는 것이다.</p>
<p>구체적으로, 인코더 네트워크는 수집된 이미지들로부터 특징 볼륨(Feature Volume)을 생성하고, 쿼리 시점(Query View)에서의 픽셀별 불확실성을 예측한다.<br />
<span class="math math-display">
U(v) = \frac{1}{N_{pixels}} \sum_{p \in v} H(p)
</span><br />
실험 결과, 불확실성 기반의 가이드를 따르는 NeU-NBV는 무작위 탐색이나 단순 커버리지 기반 탐색보다 훨씬 적은 수의 이미지로도 높은 품질(PSNR, SSIM)의 3D 장면 복원을 달성함이 입증되었다. 이는 로봇이 미지의 환경에서 사전 정보 없이도 효율적인 탐색이 가능함을 시사한다.</p>
<h2>4.  3D 가우시안 스플래팅(3DGS)과 피셔 정보(Fisher Information)</h2>
<p>NeRF의 느린 학습 및 렌더링 속도는 실시간 로봇 제어에 걸림돌이 되어왔다. 이에 대한 대안으로 등장한 **3D 가우시안 스플래팅(3D Gaussian Splatting, 3DGS)**은 3D 공간을 수만 개의 타원체(3D Gaussians)로 표현하여 실시간 렌더링과 빠른 학습 속도를 보장한다.</p>
<h3>4.1  3DGS를 위한 FisherRF</h3>
<p>3DGS 환경에서의 NBV 계획을 위한 SOTA 기술로는 <strong>FisherRF</strong> 가 주목받고 있다. FisherRF는 정보 이론의 <strong>피셔 정보(Fisher Information)</strong> 개념을 도입하여, 관측 데이터가 모델 파라미터 업데이트에 얼마나 기여할지를 예측한다.</p>
<p>피셔 정보 행렬(Fisher Information Matrix, FIM) <span class="math math-inline">I(\theta)</span>는 로그 우도 함수(Log-Likelihood Function)의 2차 도함수(Hessian)의 기댓값으로 정의된다.<br />
<span class="math math-display">
I(\theta) = - E \left[ \frac{\partial^2}{\partial \theta^2} \log P(X \vert \theta) \right]
</span><br />
직관적으로, 손실 함수(Loss Function)의 곡률(Curvature)이 큰 영역은 파라미터 변화에 민감하므로 정보량이 많은 곳이며(불확실성이 낮음), 곡률이 평평한 곳은 데이터가 부족하여 불확실성이 높은 곳이다. FisherRF는 후보 시점들 중에서 FIM의 값을 최대화하는, 즉 모델 파라미터를 가장 크게 업데이트할 수 있는 시점을 선택한다.</p>
<h3>4.2  깊이 불확실성(Depth Uncertainty)의 통합</h3>
<p>FisherRF의 강력함은 단순한 색상(RGB) 정보뿐만 아니라 <strong>깊이(Depth) 정보의 불확실성</strong>을 통합한 데 있다. 텍스처가 없는 단조로운 벽면이나 반사 재질의 경우, RGB 정보만으로는 형상을 파악하기 어렵다. FisherRF는 깊이 예측의 불확실성을 FIM 계산에 포함시킴으로써, 기하학적 구조가 불명확한 영역을 우선적으로 탐색하도록 유도한다. 또한, 거대한 FIM을 직접 계산하는 대신, 헤시안 행렬의 대각 성분(Diagonal Approximation)만을 근사하여 사용하는 기법을 통해 온라인 실시간 처리가 가능하도록 최적화되었다. 이는 로봇이 움직이는 동안 실시간으로 3DGS 맵을 업데이트하고 다음 경로를 수정할 수 있게 한다.</p>
<h2>5.  행동 중심의 NBV: 어포던스와 ACE-NBV</h2>
<p>지금까지의 NBV가 ‘보기 위한(Seeing)’ 것이었다면, 로봇 공학의 궁극적인 목표는 ‘행동하기 위한(Doing)’ 지각이다. 물체를 파지(Grasping)하거나 조작(Manipulation)하기 위해서는 단순히 겉모습을 복원하는 것을 넘어, 로봇 손이 접근할 수 있는 부위의 형상을 정확히 파악해야 한다.</p>
<h3>5.1  ACE-NBV: 어포던스 유도형 시점 계획</h3>
<p><strong>ACE-NBV (Affordance-driven Next-Best-View)</strong> 는 이러한 ’행동을 위한 지각’을 구현한 대표적인 연구다. 이 정책은 물체의 가려진 영역이 파지(Grasp) 가능한지 여부를 판단하기 위해 최적의 시점을 찾는다.</p>
<p>핵심 메커니즘은 **새로운 시점 상상(Novel View Imagery)**이다. ACE-NBV는 현재 관측된 데이터로부터 가려진 시점에서의 물체 형상과 어포던스(Affordance) 맵을 ’상상(Imagination)’해낸다. 그리고 상상된 어포던스의 신뢰도가 가장 높은, 혹은 파지 성공 확률을 가장 명확하게 판단할 수 있는 시점으로 이동한다. 예를 들어, 머그컵의 손잡이가 가려져 있다면, 3D 재구성 알고리즘은 컵의 바닥을 보러 갈 수도 있지만, ACE-NBV는 손잡이의 위치를 추론하여 그곳을 확인할 수 있는 측면으로 이동한다. 이는 지각과 행동을 분리하지 않고, 행동의 성공률을 목적 함수(Objective Function)로 삼아 지각 과정을 제어한다는 점에서 진정한 의미의 Embodied AI라 할 수 있다.</p>
<h3>5.2  인터랙티브 퍼셉션과의 연계</h3>
<p>NBV는 카메라 이동만으로는 해결할 수 없는 가려짐(예: 물체의 바닥면, 서랍 속의 물건)을 해결하기 위해 **인터랙티브 퍼셉션(Interactive Perception)**과 결합된다. Chapter 7.6.3에서 상술하겠지만, 최신 연구들은 NBV 계획에 ’밀기(Pushing)’나 ‘뒤집기(Flipping)’ 같은 물리적 상호작용을 포함시킨다. 로봇은 시각적 정보 이득과 물리적 조작 비용을 동시에 고려하여, 카메라를 움직일지 아니면 물체를 건드릴지를 결정한다.</p>
<h2>6.  강화학습(Deep RL) 기반의 NBV 정책 학습</h2>
<p>고전적인 NBV 알고리즘(정보 이득 기반)은 대부분 탐욕적(Greedy)이다. 즉, 당장 눈앞의 불확실성을 줄이는 데 최적화되어 있어, 전체 탐색 경로의 효율성이나 장기적인 목표 달성에는 취약할 수 있다. 이를 극복하기 위해 **심층 강화학습(Deep Reinforcement Learning, DRL)**을 적용하여 최적의 탐색 정책(Policy)을 학습하려는 시도가 활발하다.</p>
<h3>6.1  MDP 정식화 및 상태 공간 정의</h3>
<p>RL 기반 NBV 문제는 **마르코프 결정 과정(Markov Decision Process, MDP)**으로 정식화된다.</p>
<ul>
<li><strong>상태 공간 (State Space, <span class="math math-inline">S</span>):</strong> 로봇이 인식하는 환경 정보와 자신의 상태를 포함한다. 일반적으로 현재까지 구축된 3D 맵(OctoMap, TSDF 등), 로봇의 현재 6D 포즈, 그리고 과거 방문했던 시점들의 이력(History)이 포함된다. 고차원의 3D 데이터를 효율적으로 처리하기 위해 3D CNN(3D Convolutional Neural Networks)이나 PointNet과 같은 인코더가 사용되어 맵 정보를 압축된 잠재 벡터(Latent Vector)로 변환한다.</li>
<li><strong>행동 공간 (Action Space, <span class="math math-inline">A</span>):</strong> 로봇 카메라의 다음 6D 포즈(위치 <span class="math math-inline">x, y, z</span> + 자세 <span class="math math-inline">\phi, \theta, \psi</span>)를 정의한다. 이산적(Discrete) 행동 공간을 사용하는 경우 미리 정의된 후보 시점들 중 하나를 선택하며, 연속적(Continuous) 행동 공간을 사용하는 경우 SAC(Soft Actor-Critic)나 PPO(Proximal Policy Optimization)와 같은 알고리즘을 통해 구체적인 이동 명령을 생성한다.</li>
<li><strong>보상 함수 (Reward Function, <span class="math math-inline">R</span>):</strong> 에이전트의 행동을 유도하는 핵심 요소다. 보상 <span class="math math-inline">R_t</span>는 일반적으로 정보 이득(커버리지 증가량, 엔트로피 감소량)과 이동 비용(에너지 소모, 시간)의 가중 합으로 정의된다.</li>
</ul>
<p><span class="math math-display">
R_t = w_1 \cdot \Delta Coverage - w_2 \cdot Distance(p_t, p_{t+1}) - w_3 \cdot Time
</span></p>
<p>여기서 <span class="math math-inline">\Delta Coverage</span>는 이번 행동으로 새로 밝혀진 영역의 크기를, <span class="math math-inline">Distance</span>는 이동 거리를 의미한다. 때로는 물체 인식 성공이나 파지 성공과 같은 희소 보상(Sparse Reward) 문제를 해결하기 위해, 중간 단계의 탐색에 대한 보상(Shaped Reward)을 설계하는 것이 중요하다.</p>
<h3>6.2  학습된 정책의 일반화와 Sim-to-Real</h3>
<p>DRL 기반 방법의 가장 큰 강점은 <strong>일반화(Generalization)</strong> 능력이다. ShapeNet이나 Replica와 같은 대규모 3D 데이터셋을 이용한 시뮬레이션 환경에서 수만 번의 에피소드를 통해 학습된 정책은, 한 번도 본 적 없는 물체나 환경에서도 효율적인 탐색 경로를 생성할 수 있다.</p>
<p>연구 결과에 따르면, RL 에이전트는 단순한 휴리스틱 알고리즘보다 더 적은 이동 거리와 더 적은 수의 이미지로도 높은 품질의 3D 재구성을 달성한다. 특히, RL 에이전트는 “먼저 높이 올라가서 전체적인 윤곽을 파악한 뒤(Global Scan), 복잡한 세부 영역으로 내려가 정밀 관측을 수행하는(Local Inspection)” 등 인간 전문가와 유사한 고수준의 탐색 전략을 스스로 창발(Emergence)시키는 경향을 보인다. 이는 로봇이 단순히 알고리즘을 따르는 기계가 아니라, 경험을 통해 지능적으로 환경을 이해하는 존재로 진화하고 있음을 보여준다.</p>
<h2>7.  결론 및 미래 전망: 인지적 로봇을 향한 여정</h2>
<p>NBV 계획 기술은 ‘보이지 않는 것을 보기 위한’ 단순한 기하학적 계산에서, 불확실성을 능동적으로 관리하고 행동과 지각을 통합하는 고도의 인지 과정으로 진화했다.</p>
<ol>
<li><strong>표현의 혁신:</strong> 복셀 그리드와 같은 이산적 표현의 한계를 넘어, NeRF와 3DGS 같은 연속적이고 미분 가능한 표현(Differentiable Representation)이 NBV의 중심이 되고 있다. 이에 따라 Fisher Information과 같은 고급 통계적 불확실성 지표가 도입되었다.</li>
<li><strong>행동과의 통합:</strong> 지각은 더 이상 수동적인 관찰이 아니다. ACE-NBV와 같이 파지(Grasping) 성능을 높이기 위한 시점 계획이나, 물리적 상호작용(Pushing)을 포함한 능동적 탐색이 주류가 되고 있다.</li>
<li><strong>데이터 중심의 지능:</strong> 핸드크래프트 휴리스틱에서 벗어나, 강화학습(RL)을 통해 다양한 환경에 적응 가능한 일반화된 탐색 정책을 학습하는 방향으로 나아가고 있다.</li>
</ol>
<p>향후 NBV 기술은 거대 언어 모델(LLM) 및 비전-언어 모델(VLM)과 결합하여, “책상 위의 빨간 머그컵을 찾아서 가져와“와 같은 의미론적 명령(Semantic Goal)을 수행하기 위한 고수준의 추론(Reasoning) 과정으로 확장될 것이다. 이는 로봇이 인간의 언어를 이해하고, 그 목적을 달성하기 위해 스스로 필요한 시각 정보를 능동적으로 수집하는 진정한 의미의 지능형 에이전트로 거듭나는 길이다.</p>
<hr />
<p><strong>[표 7.6.2-1] 주요 NBV 계획 알고리즘 비교</strong></p>
<table><thead><tr><th><strong>구분</strong></th><th><strong>대표 알고리즘</strong></th><th><strong>맵 표현 방식</strong></th><th><strong>불확실성 지표</strong></th><th><strong>특징 및 한계</strong></th></tr></thead><tbody>
<tr><td><strong>기하학적 접근</strong></td><td>Frontier-based</td><td>Occupancy Grid</td><td>Frontier (Boundary)</td><td>구현 용이, 3D 가려짐 해소 미흡</td></tr>
<tr><td></td><td>Ray Casting IG</td><td>OctoMap</td><td>Shannon Entropy</td><td>정확한 IG 계산, 높은 계산 비용</td></tr>
<tr><td><strong>뉴럴 필드 기반</strong></td><td>ActiveNeRF</td><td>NeRF (MLP)</td><td>Color Variance (Gaussian)</td><td>고품질 렌더링, 긴 학습 시간</td></tr>
<tr><td></td><td>NeU-NBV</td><td>Image Features</td><td>Pixel-wise Entropy</td><td>Mapless 탐색, 빠른 추론, 일반화 가능</td></tr>
<tr><td><strong>SOTA (3DGS)</strong></td><td>FisherRF</td><td>3D Gaussians</td><td>Fisher Information (Hessian)</td><td>RGB+Depth 불확실성 통합, 실시간성 우수</td></tr>
<tr><td><strong>행동 중심 (RL)</strong></td><td>ACE-NBV</td><td>Point Cloud/TSDF</td><td>Grasp Affordance</td><td>파지 성공률 최적화, 조작 작업 특화</td></tr>
</tbody></table>
<h2>8. 참고 자료</h2>
<ol>
<li>Enhancing UAV Search Under Occlusion Using Next Best View …, https://ieeexplore.ieee.org/iel8/4609443/11278119/11271526.pdf</li>
<li>Pathwise Information Gain with Map Predictions for Indoor Robot …, https://arxiv.org/html/2503.07504v1</li>
<li>An Information Gain Formulation for Active Volumetric 3D …, https://rpg.ifi.uzh.ch/docs/ICRA16_Isler.pdf</li>
<li>A Next-Best-View Method for Complex 3D Environment Exploration …, https://www.mdpi.com/2076-3417/15/14/7757</li>
<li>Boundary Exploration of Next Best View Policy in 3D Robotic Scanning, https://arxiv.org/html/2412.10444v1</li>
<li>Efficient Sparse Voxel Octrees - Research at NVIDIA, https://research.nvidia.com/sites/default/files/pubs/2010-02_Efficient-Sparse-Voxel/laine2010tr1_paper.pdf</li>
<li>Entropy in machine learning — applications, examples, alternatives, https://nebius.com/blog/posts/entropy-in-machine-learning</li>
<li>Entropy and Information Gain in Machine Learning: Explained Simply, https://aighost.co.uk/entropy-and-information-gain-in-machine-learning-explained-simply/</li>
<li>FSMI: Fast computation of Shannon mutual information for information, https://eems.mit.edu/wp-content/uploads/2020/08/2020_ijrr_fsmi.pdf</li>
<li>Information gain (decision tree) - Grokipedia, https://grokipedia.com/page/Information_gain_(decision_tree)</li>
<li>A comparison of volumetric information gain metrics for active 3D …, https://www.researchgate.net/publication/316362928_A_comparison_of_volumetric_information_gain_metrics_for_active_3D_object_reconstruction</li>
<li>A comparison of volumetric information gain metrics for active 3D …, https://rpg.ifi.uzh.ch/docs/AURO17_Delmerico.pdf</li>
<li>An Efficient Projection-Based Next-best-view Planning Framework …, https://arxiv.org/html/2409.12096v1</li>
<li>Ray Casting in a Voxel Block Grid — Open3D 0.15.1 documentation, https://www.open3d.org/docs/0.15.1/tutorial/t_reconstruction_system/ray_casting.html</li>
<li>Adaptive Perspective Ray Casting, https://cfcs.pku.edu.cn/baoquan/docs/20180622110253095591.pdf</li>
<li>An Overview of the Fast Voxel Traversal Algorithm - GitHub, https://github.com/cgyurgyik/fast-voxel-traversal-algorithm/blob/master/overview/FastVoxelTraversalOverview.md</li>
<li>PB-NBV: Efficient Projection-Based Next-Best-View Planning … - arXiv, https://arxiv.org/html/2501.10663v2</li>
<li>Frontier Exploration and Decision-Making in Robotics - GitHub, https://github.com/Brandonio-c/Frontier-Exploration</li>
<li>Robotic Exploration of Unknown 2D Environment Using a Frontier …, https://scispace.com/pdf/robotic-exploration-of-unknown-2d-environment-using-a-4dkw3yftpq.pdf</li>
<li>Frontier-Based Exploration for Autonomous Robot | Awabot, https://awabot.com/en/autonomous-exploration-method-frontiers/</li>
<li>Two-Dimensional Frontier-Based Viewpoint Generation for … - MDPI, https://www.mdpi.com/1424-8220/19/6/1460</li>
<li>Online Next-Best-View Planner for 3D-Exploration and Inspection …, https://arxiv.org/pdf/2203.10113</li>
<li>ActiveNeRF: Learning Where to See with Uncertainty Estimation, https://www.researchgate.net/publication/365009379_ActiveNeRF_Learning_Where_to_See_with_Uncertainty_Estimation</li>
<li>ActiveNeRF: Learning where to See with Uncertainty Estimation, https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136930225.pdf</li>
<li>ActiveNeuS: Active 3D Reconstruction using Neural Implicit Surface …, https://arxiv.org/html/2405.02568v1</li>
<li>Neural Visibility Field for Uncertainty-Driven Active Mapping - arXiv, https://arxiv.org/html/2406.06948v2</li>
<li>Neural Visibility Field for Uncertainty-Driven Active Mapping, https://openaccess.thecvf.com/content/CVPR2024/papers/Xue_Neural_Visibility_Field_for_Uncertainty-Driven_Active_Mapping_CVPR_2024_paper.pdf</li>
<li>Active View Planning for Radiance Fields, https://imrss2022.github.io/contributions/lin.pdf</li>
<li>NeU-NBV: Next Best View Planning Using Uncertainty Estimation in …, https://arxiv.org/pdf/2303.01284</li>
<li>NeU-NBV: Next Best View Planning Using Uncertainty Estimation in …, https://ieeexplore.ieee.org/document/10342226/</li>
<li>Differentiable Uncertainty Quantification of Radiance Field Models, https://arxiv.org/html/2503.14665v2</li>
<li>NeU-NBV: Next Best View Planning Using Uncertainty Estimation in …, https://www.researchgate.net/publication/368935283_NeU-NBV_Next_Best_View_Planning_Using_Uncertainty_Estimation_in_Image-Based_Neural_Rendering</li>
<li>NeU-NBV: Next Best View Planning Using Uncertainty Estimation in …, https://ieeexplore.ieee.org/iel7/10341341/10341342/10342226.pdf</li>
<li>[2303.01284] NeU-NBV: Next Best View Planning Using Uncertainty …, https://arxiv.org/abs/2303.01284</li>
<li>NeU-NBV: Next Best View Planning Using Uncertainty Estimation in …, https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/jin2023iros.pdf</li>
<li>Next Best Sense: Guiding Vision and Touch with FisherRF for 3D …, https://arm.stanford.edu/next-best-sense</li>
<li>POp-GS: Next Best View in 3D-Gaussian Splatting with P-Optimality, https://openaccess.thecvf.com/content/CVPR2025/papers/Wilson_POp-GS_Next_Best_View_in_3D-Gaussian_Splatting_with_P-Optimality_CVPR_2025_paper.pdf</li>
<li>Guiding Vision and Touch with FisherRF for 3D Gaussian Splatting, https://www.researchgate.net/publication/384700101_Next_Best_Sense_Guiding_Vision_and_Touch_with_FisherRF_for_3D_Gaussian_Splatting</li>
<li>Guiding Vision and Touch with FisherRF for 3D Gaussian Splatting, https://www.themoonlight.io/en/review/next-best-sense-guiding-vision-and-touch-with-fisherrf-for-3d-gaussian-splatting</li>
<li>Active View Selection and Mapping with Radiance Fields using …, https://arxiv.org/html/2311.17874v2</li>
<li>Fisher Matrix for Beginners - arXiv, https://arxiv.org/html/2510.09683v1</li>
<li>Fisher Matrix for Beginners - UC Davis, https://wittman.physics.ucdavis.edu/Fisher-matrix-guide.pdf</li>
<li>FisherRF: Active View Selection and Mapping with Radiance Fields …, https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02130.pdf</li>
<li>ICML Poster Fishers for Free? Approximating the Fisher Information …, https://icml.cc/virtual/2025/poster/44175</li>
<li>Affordance-Driven Next-Best-View Planning for Robotic Grasping …, https://openreview.net/forum?id=IeKC9khX5jD</li>
<li>Next-Best-View Estimation based on Deep Reinforcement Learning …, https://arxiv.org/abs/2110.06766</li>
<li>A Survey of Research and Applications of Optimal Path Planning …, https://www.itm-conferences.org/articles/itmconf/pdf/2025/04/itmconf_iwadi2024_01003.pdf</li>
<li>Deep Reinforcement Learning for Next-Best-View Planning in …, https://www.hrl.uni-bonn.de/publications/zeng22icra.pdf</li>
<li>Online Next-Best-View Planner for 3D-Exploration and Inspection …, https://www.researchgate.net/publication/358180878_Online_Next-Best-View_Planner_for_3D-Exploration_and_Inspection_With_a_Mobile_Manipulator_Robot</li>
<li>Next Best View Planning via Reinforcement Learning for Scanning …, https://www.semanticscholar.org/paper/Next-Best-View-Planning-via-Reinforcement-Learning-Potapova-Artemov/c60c3308644d6b6d44e1122db3edfefe4061856a</li>
<li>VIN-NBV: A View Introspection Network for Next-Best- … - arXiv, https://arxiv.org/html/2505.06219v1</li>
<li>17.1. Markov Decision Process (MDP) - Dive into Deep Learning, https://d2l.ai/chapter_reinforcement-learning/mdp.html</li>
<li>State-space decomposition for Reinforcement Learning, https://www.imperial.ac.uk/media/imperial-college/faculty-of-engineering/computing/public/distinguished-projects/2021-ug-projects/State-space-decomposition-for-Reinforcement-Learning.pdf</li>
<li>MDP - Markov Decision Process - Machine Learning, https://gradml.mit.edu/reinforcement/mdp/</li>
<li>“Navigating the World of Reinforcement Learning: MDPs, Q- …, https://medium.com/@siddharthapramanik771/navigating-the-world-of-reinforcement-learning-mdps-q-learning-dqn-and-policy-gradients-b63f93003907</li>
<li>A Reinforcement Learning Approach to View Planning for … - MDPI, https://www.mdpi.com/1424-8220/21/6/2030</li>
<li>Guide to Reward Functions in Reinforcement Fine-Tuning - Predibase, https://predibase.com/blog/reward-functions-reinforcement-fine-tuning</li>
<li>Reinforcement-Learning-Based Path Planning: A Reward Function …, https://www.mdpi.com/2076-3417/14/17/7654</li>
<li>
<ol start="12">
<li>Modelling and abstraction for MDPs - Open Textbooks @ UQ, https://uq.pressbooks.pub/mastering-reinforcement-learning/chapter/modelling-abstraction-mdps/</li>
</ol>
</li>
<li>Reinforcement Learning with MDP for revenues optimization, https://stackoverflow.com/questions/50737705/reinforcement-learning-with-mdp-for-revenues-optimization</li>
<li>Design of Reward Function on Reinforcement Learning for … - arXiv, https://arxiv.org/html/2503.16559v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>