<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:1.4 인지-판단-행동 루프 (The Perception-Action-Cognition Loop)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>1.4 인지-판단-행동 루프 (The Perception-Action-Cognition Loop)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 1. Embodied AI의 태동: 뇌를 가진 신체</a> / <a href="index.html">1.4 인지-판단-행동 루프 (The Perception-Action-Cognition Loop)</a> / <span>1.4 인지-판단-행동 루프 (The Perception-Action-Cognition Loop)</span></nav>
                </div>
            </header>
            <article>
                <h1>1.4 인지-판단-행동 루프 (The Perception-Action-Cognition Loop)</h1>
<p>인공지능(Artificial Intelligence)이 비트(bit)의 세계에서 벗어나 원자(atom)의 세계로 진입할 때, 즉 ’신체’를 갖게 되었을 때 가장 본질적으로 변화하는 것은 데이터의 처리 방식이 아니라 존재 양식 그 자체이다. 인터넷상의 AI(Internet AI)에게 데이터는 정적이고 수동적인 입력값이지만, 로봇에게 데이터는 자신의 행동에 의해 끊임없이 변화하는 유동적인 실체이다. 로봇이 환경을 인식하고(Perception), 그에 기반하여 판단을 내리고(Cognition/Reasoning), 물리적 세계에 힘을 가해 행동(Action)하면, 그 행동은 다시 환경을 변화시켜 새로운 인식을 만들어낸다. 이 순환적인 구조, 즉 **인지-판단-행동 루프(The Perception-Action-Cognition Loop)**야말로 엠바디드 AI(Embodied AI)를 정의하는 심장이자 지능이 발현되는 최소 단위이다.1</p>
<p>과거의 고전적 제어 이론이나 초기 로봇 공학에서는 이 루프를 명확히 구분된 모듈의 집합으로 보았으나, 최신 SOTA(State-of-the-Art) 연구들은 이 경계를 허물고 통합된 신경망 구조 안에서 루프를 내재화하는 방향으로 진화하고 있다. 본 절에서는 이 루프가 역사적으로 어떻게 진화해 왔으며, 최신 거대 언어 모델(LLM)과 비전-언어-행동 모델(VLA)이 이 루프의 ’인지’와 ‘판단’ 영역을 어떻게 혁명적으로 재구성하고 있는지, 그리고 물리적 제어의 ‘행동’ 영역과 어떻게 유기적으로 결합하는지 심층적으로 분석한다. 특히, 로드니 브룩스(Rodney Brooks)의 포섭 구조(Subsumption Architecture)에서 시작된 신체성 논의가 현대의 트랜스포머(Transformer) 기반 파운데이션 모델과 만나 어떻게 재해석되고 있는지, 그리고 그 과정에서 발생하는 ’주파수 불일치(Frequency Mismatch)’와 같은 공학적 난제들이 어떻게 해결되고 있는지를 면밀히 추적한다.3</p>
<h2>1.  고전적 패러다임의 해체: SPA에서 End-to-End로의 전환</h2>
<p>로봇 지능을 구현하는 아키텍처는 데이터가 루프를 통과하는 방식에 따라 크게 두 가지의 상반된 철학으로 발전해 왔다. 하나는 순차적이고 명시적인 <strong>Sense-Plan-Act(SPA)</strong> 패러다임이며, 다른 하나는 데이터 주도의 <strong>End-to-End Learning</strong> 패러다임이다. 이 두 가지 접근 방식의 대립과 융합은 현대 로봇 공학의 역사를 관통하는 핵심 주제이다.</p>
<h3>1.1 Sense-Plan-Act (SPA): 명시적 모듈화와 그 한계</h3>
<p>초기 로봇 공학의 지배적인 패러다임이었던 SPA 아키텍처는 인간의 인지 과정을 논리적으로 분해하여 구현하려는 시도였다. 이 구조에서 로봇은 세 가지 독립적인 단계를 순차적으로 수행한다.4</p>
<ol>
<li><strong>Sense (감지)</strong>: 센서 데이터를 수집하여 노이즈를 제거하고, 이를 바탕으로 로봇의 현재 상태와 환경 지도를 생성한다(State Estimation). 이 단계에서는 카메라, 라이다(LiDAR), IMU 등의 센서 데이터가 융합되어 로봇의 월드 좌표계(World Coordinate) 상의 위치와 주변 물체의 기하학적 형상을 복원한다.</li>
<li><strong>Plan (계획)</strong>: 생성된 상태 정보를 바탕으로 목표를 달성하기 위한 최적의 경로와 행동 시퀀스를 계산한다(Task and Motion Planning). 여기에는 A* 알고리즘이나 RRT(Rapidly-exploring Random Tree)와 같은 경로 계획 알고리즘, 그리고 STRIPS와 같은 기호적 작업 계획(Symbolic Task Planning)이 포함된다.</li>
<li><strong>Act (행동)</strong>: 계획된 경로를 따라 모터를 제어하여 물리적 움직임을 수행한다(Control). PID 제어기나 MPC(Model Predictive Control)가 이 단계에서 작동하여 목표 궤적을 추종하기 위한 토크(Torque)를 계산한다.</li>
</ol>
<p>이 접근법은 각 모듈을 독립적으로 개발하고 검증할 수 있다는 장점이 있어, 제어 이론과 결합하여 정형화된 환경(예: 공장 자동화 라인)에서 큰 성공을 거두었다. 공장과 같이 변수가 통제된 환경에서는 ‘Sense’ 단계의 불확실성이 낮고, ‘Plan’ 단계의 계산 복잡도가 관리 가능하기 때문이다.</p>
<p>그러나 비정형 환경(Unstructured Environment)에서는 SPA 아키텍처가 치명적인 한계를 드러냈다. 첫째, 오차의 전파(Error Propagation) 문제이다. ‘Sense’ 단계에서 발생한 작은 상태 추정 오차가 ‘Plan’ 단계로 넘어가면서 증폭되었고, ‘Act’ 단계에서 실행될 때는 이미 현실과 동떨어진 계획이 되기 일쑤였다. 예를 들어, 머그컵의 위치를 1cm 잘못 인식했다면, 계획 모듈은 완벽한 궤적을 생성하더라도 그리퍼는 허공을 쥐게 된다.</p>
<p>둘째, 계산 병목(Computational Bottleneck) 현상이다. 각 단계가 직렬로 연결되어 있어 전체 루프의 속도가 가장 느린 모듈(주로 Planning)에 의해 결정된다. 동적인 환경에서 로봇이 실시간으로 반응해야 할 때, 복잡한 경로 계획을 매번 처음부터 다시 계산하는 것은 시스템의 응답성을 심각하게 저하시킨다.6</p>
<p>셋째, 세계 모델링의 어려움이다. 비정형 환경의 모든 요소를 사전에 정의된 기호나 기하학적 모델로 표현하는 것은 불가능에 가깝다. 부드러운 천, 흐르는 물, 찌그러진 캔과 같은 객체는 전통적인 강체(Rigid Body) 역학 기반의 상태 추정기로는 표현하기 어렵다.</p>
<h3>1.2 End-to-End Visuomotor Policy: 픽셀에서 토크까지 (Pixels to Torques)</h3>
<p>딥러닝의 부상과 함께 등장한 End-to-End 학습은 SPA의 복잡한 파이프라인을 하나의 거대한 신경망으로 대체하려는 시도이다. 이는 **“Pixels to Torques”**라는 문구로 요약되며, 로봇의 카메라로 들어온 원본 이미지(Raw Pixel)를 신경망에 입력하면, 중간의 명시적인 상태 추정이나 계획 단계 없이 곧바로 관절의 모터 명령(Torque/Velocity)이 출력되는 방식이다.7</p>
<p>이 방식의 핵심은 <strong>특징 표현(Representation)의 학습</strong>에 있다. SPA에서는 엔지니어가 수작업으로 정의한 특징(Hand-crafted Features, 예: 모서리, 물체의 6D 포즈)을 사용했지만, End-to-End 모델은 데이터로부터 작업 수행에 필요한 특징을 스스로 추출한다. 이는 리치 서튼(Rich Sutton)이 주창한 **“The Bitter Lesson”**이 로봇 공학에서도 유효함을 증명하는 듯했다. 즉, 인간의 사전 지식을 주입하여 시스템을 설계하는 것보다, 거대한 데이터와 컴퓨팅 자원을 활용하여 기계가 스스로 특징을 찾게 하는 것이 장기적으로 훨씬 더 우월한 성능을 낸다는 것이다.7</p>
<p>SOTA 로봇 학습 연구, 예를 들어 구글의 RT-1(Robotic Transformer 1)이나 스탠포드의 ALOHA 프로젝트 등은 대부분 이러한 End-to-End 방식을 채택하고 있다. 이들은 수만 개 이상의 로봇 데모 데이터를 통해 이미지와 행동 간의 매핑을 학습한다. 이 방식은 명시적인 물체 인식 단계를 거치지 않기 때문에, 찌그러진 캔이나 투명한 병과 같이 전통적인 컴퓨터 비전이 어려워하는 대상에 대해서도 강인한 조작 능력을 보여준다. 로봇은 “이것이 캔이다“라고 인식하는 것이 아니라, “이러한 시각적 패턴이 들어오면 그리퍼를 닫아야 한다“는 행동 유도성(Affordance)을 직접 학습하기 때문이다.6</p>
<table><thead><tr><th><strong>특징</strong></th><th><strong>Sense-Plan-Act (SPA)</strong></th><th><strong>End-to-End Visuomotor Policy</strong></th></tr></thead><tbody>
<tr><td><strong>데이터 흐름</strong></td><td>직렬, 모듈 간 명시적 인터페이스</td><td>단일 신경망을 통한 통합 처리</td></tr>
<tr><td><strong>상태 표현</strong></td><td>기호적, 기하학적 (Symbolic/Geometric)</td><td>잠재 벡터, 뉴럴 임베딩 (Latent/Neural)</td></tr>
<tr><td><strong>장점</strong></td><td>설명 가능성(Explainability), 모듈별 디버깅 용이</td><td>비정형 환경 강인성, 특징 자동 추출</td></tr>
<tr><td><strong>단점</strong></td><td>오차 누적, 느린 반응 속도, 모델링 한계</td><td>데이터 요구량 과다, 해석 불가능성(Black-box)</td></tr>
<tr><td><strong>대표 기술</strong></td><td>SLAM, RRT*, MPC</td><td>CNN/ViT Policy, Imitation Learning</td></tr>
</tbody></table>
<p>그러나 순수한 End-to-End 방식(특히 단순한 행동 복제, BC) 역시 한계를 가진다. 학습 데이터 분포에서 벗어난 상황(Out-of-Distribution)에 취약하며, 장기적인 추론(Reasoning)이나 다단계 작업(Multi-step Task)을 수행하는 데 필요한 ’기억’과 ’상식’이 부족하다. 단순히 전문가의 행동을 모방하는 것만으로는 “냉장고 문을 열었는데 우유가 없으면 어떻게 해야 하는가?“와 같은 인과적 추론이 필요한 상황에 대처할 수 없다. 여기서 현대 로봇 지능의 새로운 돌파구가 열린다. 바로 ’인지’와 ‘행동’ 사이에 강력한 <strong>‘인지(Cognition)’</strong> 엔진으로서의 파운데이션 모델(Foundation Models)의 도입이다.</p>
<h2>2.  현대적 루프의 재구성: VLA와 인지적 개입 (Cognitive Intervention)</h2>
<p>2020년대 중반, 로봇 공학은 거대 언어 모델(LLM)과 멀티모달 모델(LMM)을 로봇의 제어 루프에 통합하는 <strong>VLA(Vision-Language-Action)</strong> 모델의 시대로 진입했다. 이는 단순한 반사 신경(Reflex) 수준의 End-to-End를 넘어, 고차원적인 추론이 가능한 루프를 형성한다. VLA 모델은 인터넷상의 방대한 텍스트와 이미지 데이터로부터 학습한 ’일반 상식’을 로봇의 물리적 제어에 전이(Transfer)시킴으로써, 이전에는 불가능했던 일반화(Generalization) 능력을 보여준다.8</p>
<h3>2.1 Cognition의 부활: LLM을 추론 코어로 활용</h3>
<p>과거의 로봇은 “물병을 집어라“라는 명령을 이해하기 위해 자연어 처리 모듈이 명령어를 “Pick(Bottle)“과 같은 미리 정의된 기호로 번역해야 했다. 하지만 VLA 모델은 자연어 명령과 시각 정보를 동시에 처리하여, 로봇의 행동을 생성한다. 구글의 **RT-2(Robotic Transformer 2)**나 <strong>PaLM-E</strong>와 같은 모델은 인터넷 규모의 데이터로 학습된 상식을 로봇 제어에 활용한다. 예를 들어, “피곤한 사람에게 줄 음료를 골라라“는 명령을 받았을 때, RT-2는 명시적인 코딩 없이도 에너지 드링크나 커피를 선택할 수 있다. 이는 LLM이 가진 의미론적 추론 능력이 물리적 행동 선택에 직접적으로 개입함을 의미한다.10</p>
<p>RT-2의 아키텍처는 이 루프가 어떻게 통합되는지를 명확히 보여준다. RT-2는 거대 비전-언어 모델(VLM)을 기반으로 하되, 로봇의 행동을 텍스트와 동일한 형태의 ’토큰(Token)’으로 취급한다. 이를 **액션 토큰화(Action Tokenization)**라고 한다. 로봇 팔의 6자유도(x, y, z, roll, pitch, yaw)와 그리퍼의 개폐 상태를 0에서 255 사이의 정수로 이산화(Discretization)하여, 이를 마치 언어의 단어처럼 사전에 등록한다.11<br />
<span class="math math-display">
A_t = \text{VLA}(I_t, \text{Text\_Instruction}, H_{t-1})
</span><br />
여기서 <span class="math math-inline">A_t</span>는 현재 시점의 행동 토큰 시퀀스, <span class="math math-inline">I_t</span>는 현재 관측된 이미지, <span class="math math-inline">H_{t-1}</span>은 이전까지의 대화 및 행동 컨텍스트이다. 거대 언어 모델은 “The cat sat on the…” 뒤에 “mat“이 올 것을 예측하듯이, 현재의 이미지와 명령어 뒤에 올 “로봇 팔의 다음 위치 토큰“을 예측한다. 이 방식은 인지(Vision), 언어적 이해(Instruction), 그리고 행동 생성(Action)이 단일한 Transformer 네트워크의 Forward Pass 한 번으로 이루어짐을 의미한다. 이는 로봇 제어 문제를 ’다음 토큰 예측(Next Token Prediction)’이라는 범용적인 시퀀스 모델링 문제로 환원시킨 혁신적인 접근이다.</p>
<h3>2.2 PaLM-E: 멀티모달 문장으로서의 상태 (State as a Multimodal Sentence)</h3>
<p>**PaLM-E(Pathways Language Model - Embodied)**는 한 단계 더 나아가, 로봇의 센서 데이터(이미지, 상태 벡터 등)를 언어 모델의 임베딩 공간(Embedding Space)에 직접 주입(Inject)한다.12 기존의 VLM들이 이미지를 캡션으로 변환하거나 별도의 인코더를 통해 느슨하게 결합했던 것과 달리, PaLM-E는 비전 트랜스포머(ViT)를 통과한 이미지 임베딩 벡터를 언어 토큰 임베딩과 동일한 차원으로 투영(Project)하여, 텍스트 토큰 사이에 끼워 넣는다.</p>
<p>PaLM-E에서 입력은 텍스트와 연속적인 센서 데이터가 혼합된 **‘멀티모달 문장(Multimodal Sentence)’**으로 구성된다.</p>
<blockquote>
<p><strong>User:</strong> “저기 [Image_Embedding]에 있는 물건들 중에서 깨지기 쉬운 것을 조심해서 왼쪽 구석으로 옮겨줘.”</p>
</blockquote>
<p>이때 <code>[Image_Embedding]</code>은 단순한 파일 경로가 아니라, 로봇의 카메라로 본 장면을 압축한 수백 개의 벡터 시퀀스이다. PaLM-E는 이 입력을 받아 고수준의 계획을 수립하거나(Planning), 직접적인 저수준 제어 명령을 생성하기도 한다. 실험 결과, PaLM-E는 단일 이미지뿐만 아니라 여러 장의 연속된 이미지나 포인트 클라우드 데이터까지도 처리할 수 있는 것으로 나타났다. 이는 로봇의 인지-판단-행동 루프가 언어적 추론(Semantic Reasoning)과 물리적 인식(Physical Perception)이 얽힌(Entangled) 형태로 구동됨을 시사한다. 로봇은 단순히 “보이는 대로 행동“하는 것이 아니라, “이해한 대로 행동“하게 된다. 언어 모델의 추론 능력 덕분에, 로봇은 이전에 본 적 없는 물체(Zero-shot)에 대해서도 그 속성을 추론하고 적절한 파지(Grasping) 전략을 수립할 수 있다.6</p>
<p>이러한 <strong>Co-fine-tuning</strong> 전략, 즉 인터넷의 일반 데이터와 로봇의 물리적 데이터를 함께 학습시키는 전략은 로봇에게 ’상식’을 부여한다. “스펀지는 푹신하니까 세게 쥐어도 된다“거나 “유리잔은 조심해야 한다“는 지식은 로봇 데이터셋에는 거의 없지만, 웹 텍스트 데이터에는 풍부하기 때문이다. VLA 모델은 이 두 세계를 연결함으로써 로봇 지능의 새로운 지평을 열었다.</p>
<h2>3.  주파수 불일치 문제와 계층적 루프 (Hierarchy in the Loop)</h2>
<p>인지-판단-행동 루프를 설계할 때 가장 실질적이고 치명적인 공학적 난관은 <strong>주파수(Frequency)의 불일치</strong>이다. 인간의 뇌가 빠른 반사 신경(척수 레벨)과 느린 숙고(대뇌 피질 레벨)를 병행하듯, 로봇 시스템 역시 서로 다른 시간 스케일의 처리가 필요하다.16</p>
<h3>3.1 System 1 vs. System 2: 듀얼 프로세스 이론의 적용</h3>
<p>노벨상 수상자 대니얼 카너먼(Daniel Kahneman)의 “생각에 관한 생각“에서 제시된 **System 1(빠르고 직관적)**과 **System 2(느리고 논리적)**의 구분은 현대 엠바디드 AI 아키텍처, 특히 VLA 모델을 실제 로봇에 적용할 때 핵심적인 설계 원칙이 된다.18</p>
<ul>
<li><strong>System 2 (High-Level Cognition):</strong> VLM이나 LLM이 담당한다. 복잡한 자연어 명령을 해석하고, 장기적인 계획을 수립하며, 예외 상황에 대처하거나 과거의 실패로부터 전략을 수정한다. 이 과정은 수십억 개의 파라미터를 가진 거대 모델을 구동해야 하므로 막대한 연산량을 요구하며, 추론 속도가 느리다. 예를 들어, RT-2-PaLI-X-55B 모델의 경우 추론 속도가 1~3Hz에 불과하다.10 이는 로봇이 1초에 최대 3번만 의사결정을 내릴 수 있음을 의미한다.</li>
<li><strong>System 1 (Low-Level Control):</strong> 모터 제어 정책(Policy)이 담당한다. 실시간으로 변하는 관절의 위치, 속도, 토크를 제어해야 하며, 외부의 물리적 외란(Disturbance)에 즉각적으로 반응해야 한다. 안정적인 물리 제어를 위해서는 최소 50Hz에서 1000Hz 이상의 제어 주기가 요구된다. 이 영역에서는 Diffusion Policy나 경량화된 Visuomotor Policy, 또는 고전적인 PID 제어기가 주로 사용된다.16</li>
</ul>
<p>이 극심한 주파수 차이(1Hz vs 100Hz)를 해결하지 못하면, 로봇은 생각하느라 멈칫거리거나(Stuttering), 빠르게 움직이다가 상황 변화를 놓치고 충돌하게 된다.</p>
<h3>3.2 HiRT와 계층적 제어의 해결책</h3>
<p>거대 모델(VLM)을 100Hz 이상의 고주파수 제어 루프에 직접 사용하는 것은 현재의 하드웨어(GPU/TPU) 성능으로는 불가능하며, 에너지 효율 측면에서도 비합리적이다. 따라서 최신 연구는 <strong>계층적(Hierarchical) 구조</strong>를 통해 이 간극을 메운다.</p>
<p>**HiRT(Hierarchical Robot Transformer)**와 같은 연구는 느린 주기의 고수준 VLM과 빠른 주기의 저수준 정책을 결합하는 프레임워크를 제안한다.17</p>
<p>HiRT 구조에서 고수준의 VLM은 전체적인 작업의 맥락을 파악하여 저수준 정책에 **“지침(Guidance)”**이나 **“중간 목표(Sub-goal)”**를 전달한다. 이 지침은 텍스트일 수도 있고, 잠재 벡터(Latent Vector)나 웨이포인트(Waypoint)일 수도 있다. 저수준 정책은 이 지침을 조건(Condition)으로 받아 빠른 주기의 관절 제어 명령을 생성한다.</p>
<p>예를 들어, VLM은 1초에 한 번씩 “파란 컵을 향해 접근하라“는 의미를 담은 잠재 벡터를 생성하고, 저수준 제어기(예: Transformer Policy or Diffusion Policy)는 이 벡터를 참조하여 0.01초마다 모터의 전류값을 미세 조정한다. VLM은 일종의 <strong>‘메타 컨트롤러(Meta-Controller)’</strong> 역할을 수행하며, 저수준 제어기는 <strong>‘실행기(Executor)’</strong> 역할을 한다.</p>
<table><thead><tr><th><strong>구분</strong></th><th><strong>고수준 루프 (High-Level Loop)</strong></th><th><strong>저수준 루프 (Low-Level Loop)</strong></th></tr></thead><tbody>
<tr><td><strong>담당 모델</strong></td><td>VLA (RT-2, PaLM-E), LLM</td><td>Diffusion Policy, ACT, PID</td></tr>
<tr><td><strong>주파수</strong></td><td>0.5Hz ~ 5Hz (느림)</td><td>50Hz ~ 1000Hz (빠름)</td></tr>
<tr><td><strong>역할</strong></td><td>추론, 계획, 예외 처리, 의미 파악</td><td>동작 생성, 외란 제거, 관절 제어</td></tr>
<tr><td><strong>입력</strong></td><td>이미지, 텍스트 명령, 긴 컨텍스트</td><td>현재 관절 상태, VLM의 지침 벡터</td></tr>
<tr><td><strong>비유</strong></td><td>대뇌 피질 (System 2)</td><td>소뇌 및 척수 (System 1)</td></tr>
</tbody></table>
<p>이러한 계층적 구조는 **RTC(Robot Transformer Controller)**와 같은 연구에서도 확인되는데, 이는 실시간으로 작업의 진행 상황을 모니터링하다가 중요한 이벤트(예: 물체 파지 성공)가 발생했을 때만 고수준 VLM을 호출하여 연산 자원을 효율적으로 사용한다.9 이로써 로봇은 거대 모델의 지능과 반사 신경의 민첩성을 동시에 확보할 수 있게 된다.</p>
<h2>4.  생성형 행동과 확산 정책 (Generative Action with Diffusion Policy)</h2>
<p>전통적인 로봇 제어에서 ’행동(Action)’은 주어진 상태에 대한 결정론적인 출력값(Deterministic Output)이었다. 그러나 최신 AI, 특히 생성형 AI(Generative AI)의 발전은 행동 생성 과정을 근본적으로 변화시키고 있다. 이제 행동은 단순히 계산되는 것이 아니라, **‘생성(Generation)’**되는 것이다. 이 변화의 중심에는 **확산 모델(Diffusion Models)**이 있다.</p>
<h3>4.1 왜 행동을 ’생성’해야 하는가?</h3>
<p>복잡한 작업에서 최적의 행동은 하나가 아니다. 장애물을 피해서 컵을 잡는 방법은 수천 가지가 넘는다(Multimodality). 기존의 회귀(Regression) 기반 모델(예: MSE Loss를 사용하는 BC)은 이러한 여러 가능한 행동들의 ’평균’을 학습하려는 경향이 있다. 장애물을 왼쪽으로 피하는 경로와 오른쪽으로 피하는 경로의 평균을 취하면, 로봇은 장애물 정면으로 돌진하게 된다. 이는 다봉(Multi-modal) 분포를 단봉(Uni-modal)으로 강제로 압축할 때 발생하는 대표적인 문제이다.22</p>
<p>확산 모델은 노이즈로부터 데이터를 복원하는 과정을 학습함으로써 복잡한 확률 분포를 정밀하게 모델링할 수 있다. 이를 로봇 제어에 적용한 것이 <strong>Diffusion Policy</strong>이다. Diffusion Policy는 로봇의 행동 궤적(Trajectory)을 이미지 생성하듯이 생성한다. 랜덤한 노이즈(Gaussian Noise)에서 시작하여, 현재의 관측(Observation)을 조건(Condition)으로 하여 점진적으로 노이즈를 제거(Denoising)해 나감으로써, 물리적으로 실행 가능한 부드러운 행동 궤적을 만들어낸다.19</p>
<h3>4.2 Action-Guided Perception: 행동이 인식을 이끈다</h3>
<p>더 나아가, 최근의 연구들은 행동 생성을 위한 노이즈 제거 과정이 역으로 인지적 특징을 강화하는 데 사용될 수 있음을 보여준다. **DP-AG(Action-Guided Diffusion Policy)**와 같은 모델은 ’인지’가 ’행동’을 결정하는 단방향 흐름을 넘어, ’행동’이 ’인지’를 형성하는 양방향 상호작용을 구현한다.24</p>
<p>DP-AG에서는 행동을 생성하는 과정에서 발생하는 그라디언트(Gradient), 구체적으로는 확산 모델의 노이즈 예측 오차에 대한 Vector-Jacobian Product(VJP)를 역전파하여 시각적 인코더(Visual Encoder)를 업데이트한다. 이는 **“내가 어떤 행동을 하려느냐에 따라 무엇을 봐야 할지가 달라진다”**는 원리를 수학적으로 구현한 것이다. 컵을 잡으려 할 때는 컵의 손잡이가 중요하게 인식되고, 컵을 밀려 할 때는 컵의 바닥면이 중요하게 인식되도록 인지 모듈(Representation Learning)이 행동 의도에 의해 동적으로 조절된다.</p>
<p>또한, <strong>Cycle-Consistent Contrastive Loss</strong>를 도입하여, 정적인 이미지 특징과 동적으로 진화하는 행동 생성 과정이 서로 정합성(Consistency)을 유지하도록 강제한다.26 이는 생물학적 시스템에서 운동 피질(Motor Cortex)과 감각 피질(Sensory Cortex)이 긴밀하게 연결되어 서로 정보를 주고받는 메커니즘을 모방한 것으로 볼 수 있다. 이러한 접근은 로봇이 복잡한 조작 작업(Contact-Rich Manipulation)을 수행할 때 훨씬 더 높은 성공률과 적응성을 보여준다.</p>
<h2>5.  루프의 완성: 세계 모델(World Models)과 예측</h2>
<p>단순히 현재를 보고 반응하는 것(Reactive)을 넘어, 고지능 로봇의 핵심은 **‘미래를 시뮬레이션’**하는 능력에 있다. 이것이 바로 **세계 모델(World Models)**이 인지-판단-행동 루프에 필수적으로 통합되어야 하는 이유이다. 세계 모델은 로봇 내부에 존재하는 ’세상의 축소판’이다.27</p>
<h3>5.1 예측이 곧 이해다 (Prediction is Understanding)</h3>
<p>세계 모델을 탑재한 로봇은 행동하기 전에 마음속으로 행동의 결과를 상상한다. 이를 **DayDreaming(백일몽)**이라고 부른다. <strong>DayDreamer</strong>와 같은 알고리즘은 실제 환경에서의 상호작용 데이터를 통해 환경의 동역학(Dynamics)을 학습한다.29<br />
<span class="math math-display">
S_{t+1} \approx M_{\text{World}}(S_t, A_t)
</span><br />
여기서 <span class="math math-inline">S_t</span>는 현재 상태, <span class="math math-inline">A_t</span>는 행동, <span class="math math-inline">M_{\text{World}}</span>는 로봇이 학습한 세계 모델이다. 로봇은 이 모델을 사용하여 실제 팔을 움직이지 않고도 수천, 수만 번의 가상 시행착오를 겪으며 정책을 최적화할 수 있다. 이는 실제 세계에서의 위험하고 느리고 비용이 많이 드는 탐색 과정을 획기적으로 줄여준다.</p>
<p>얀 르쿤(Yann LeCun)이 제안한 <strong>JEPA(Joint Embedding Predictive Architecture)</strong> 역시 이러한 맥락에서 중요하다. JEPA는 비디오의 다음 프레임을 픽셀 단위로 예측하는 것이 아니라, 추상적인 **의미 공간(Latent Space)**에서의 미래 상태를 예측한다.30 픽셀 단위의 예측(Generative World Model, 예: Sora)은 나무가 흔들리는 잎사귀 하나하나까지 예측하느라 불필요한 연산을 소모하지만, JEPA와 같은 의미론적 세계 모델은 “내가 차를 밀면 차가 움직일 것이다“라는 핵심적인 인과관계(Causality)에 집중한다.</p>
<h3>5.2 세계 모델의 역할과 분류</h3>
<p>세계 모델은 크게 두 가지 기능을 수행한다.28</p>
<ol>
<li><strong>Look-ahead Planning:</strong> 미래를 예측하여 최적의 경로를 계획한다. 모델 예측 제어(MPC)와 결합하여 장애물을 회피하거나 동적인 물체를 포착하는 데 사용된다.</li>
<li><strong>Representation Learning:</strong> 미래를 잘 예측하기 위해 현재의 상태를 가장 효율적으로 압축하고 표현하는 방법을 배운다. 이는 로봇이 시각적 입력에서 불필요한 노이즈를 제거하고 본질적인 정보(Task-relevant features)만을 추출하도록 돕는다.</li>
</ol>
<p>최근에는 비디오 생성 모델(Video Generation Models)을 세계 모델로 활용하려는 시도도 늘어나고 있다. 로봇이 “내가 이 컵을 밀면 어떻게 될까?“라고 물으면, 생성형 AI가 컵이 밀려서 넘어지는 짧은 비디오를 생성해 보여주고, 로봇은 이를 바탕으로 행동의 위험성을 판단하는 식이다.27 이는 <strong>UniSim</strong>이나 <strong>RoboGen</strong>과 같은 연구로 이어지며, 시뮬레이션과 현실의 경계를 허물고 있다.</p>
<h2>6.  행동을 위한 지각: 어포던스와 그라운딩 (Affordance &amp; Grounding)</h2>
<p>루프의 ‘Perception(지각)’ 단계 역시 수동적인 관찰에서 **능동적 지각(Active Perception)**으로 진화하고 있다. 로봇은 정보를 더 잘 얻기 위해 행동한다. 머리를 움직여 가려진 물체를 보거나, 물체를 들어 올려 무게를 가늠하는 것이 그 예이다.32</p>
<h3>6.1 SayCan: 언어적 계획의 물리적 접지</h3>
<p><strong>SayCan</strong> 프로젝트는 언어 모델의 추론이 물리적 현실에 **그라운딩(Grounding)**되어야 함을 보여준 대표적인 사례이다.33 LLM은 “음료수를 쏟았다“는 상황에 대해 “걸레로 닦는다”, “진공청소기로 빤다”, “새 음료수를 산다” 등 다양한 해결책을 제시할 수 있다. 그러나 로봇 앞에 걸레는 없고 청소기만 있다면? 또는 그 청소기가 액체를 빨아들이면 고장 나는 모델이라면? LLM만으로는 이를 알 수 없다.</p>
<p>SayCan은 LLM이 제안하는 행동의 ’언어적 유용성(Task-Grounding)’과 로봇이 현재 상황에서 해당 행동을 수행할 수 있는 ’물리적 가능성(World-Grounding)’을 결합하여 최종 행동을 선택한다.35</p>
<p><span class="math math-display">P(\text{Skill} \vert \text{Instruction}, \text{State}) \propto P_{\text{LLM}}(\text{Skill} \vert \text{Instruction}) \times P_{\text{V}}(\text{Success} \vert \text{Skill}, \text{State})</span></p>
<p>이 수식에서 <span class="math math-inline">P_{\text{LLM}}</span>은 언어 모델이 계산한 행동의 적합도이고, <span class="math math-inline">P_{\text{V}}</span>는 시각적 인식에 기반한 어포던스(Affordance) 확률이다. 가치 함수(Value Function)로 학습된 <span class="math math-inline">P_{\text{V}}</span>는 현재 로봇의 눈앞에 있는 물체들과 로봇의 능력을 고려할 때 해당 스킬이 성공할 확률을 나타낸다. 예를 들어 “사과를 집어라“라는 스킬은 사과가 보이지 않으면 <span class="math math-inline">P_{\text{V}}</span>가 0에 가까워진다.</p>
<p>따라서 로봇의 인지 루프는 단순히 대상을 식별(Object Detection)하는 것을 넘어, **“내가 저것으로 무엇을 할 수 있는가?”**를 끊임없이 평가하는 과정이 된다. 이는 제임스 깁슨(James Gibson)의 생태학적 심리학(Ecological Psychology) 이론이 최신 AI 기술을 통해 로봇 공학적으로 구현된 것이다.</p>
<h2>1.4.7 요약 및 시사점</h2>
<p>1.4절에서 살펴본 인지-판단-행동 루프의 진화는 엠바디드 AI가 단순한 기계 제어를 넘어 인지적 주체로 거듭나는 과정을 보여준다.</p>
<ul>
<li><strong>구조적 통합 (Integration):</strong> 분절된 SPA 모듈은 거대한 신경망(End-to-End, VLA)으로 통합되었으며, 데이터는 이 루프를 끊김 없이(Seamless) 흐른다. 이는 모라벡의 역설을 극복하기 위한 필연적인 진화이다.</li>
<li><strong>언어의 개입 (Language grounding):</strong> LLM/VLM의 도입으로 ‘판단’ 단계에 인간의 언어적 지식과 추론 능력이 주입되었으며, 이는 로봇의 범용성과 해석 가능성을 비약적으로 높였다.</li>
<li><strong>시간적 계층 (Temporal Hierarchy):</strong> System 1(빠른 제어, Diffusion Policy)과 System 2(느린 추론, VLA)의 계층적 결합은 고지능과 민첩성을 동시에 달성하기 위한 필수 아키텍처로 자리 잡았다.</li>
<li><strong>예측적 본질 (Predictive Nature):</strong> 세계 모델의 도입으로 루프는 현재에 갇히지 않고 미래를 시뮬레이션(DayDreaming)하며 최적의 행동을 선택하는 방향으로 나아가고 있다.</li>
</ul>
<p>결론적으로 현대의 로봇 지능은 **“세상을 이해하고(World Model), 언어로 추론하며(VLA), 물리적으로 상호작용하는(Policy) 거대한 순환계”**로 정의될 수 있다. 이 루프의 완결성(Closure)과 효율성, 그리고 현실 세계와의 정합성(Alignment)이야말로 로봇이 실험실을 벗어나 복잡한 현실 세계로 나아갈 수 있는지를 결정짓는 척도가 될 것이다. 다음 절에서는 이 루프를 지탱하는 세 가지 기술적 기둥인 데이터(Data), 시뮬레이션(Simulation), 그리고 하드웨어(Hardware)에 대해 구체적으로 살펴본다.36</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>Perception, Action, and Cognition - ResearchGate, 1월 1, 2026에 액세스, https://www.researchgate.net/profile/Snehlata-Jaswal/publication/317014624_Perception_action_and_cognition/links/591efab8a6fdcc233fd4d4b8/Perception-action-and-cognition.pdf</li>
<li>DIM C-BRAINS, 1월 1, 2026에 액세스, https://dim-cbrains.fr/en/network/166/team</li>
<li>Multi-agent Embodied AI: Advances and Future Directions - arXiv, 1월 1, 2026에 액세스, https://arxiv.org/html/2505.05108v1</li>
<li>Integrating Open-Ended Learning in the Sense-Plan-Act Robot Control Paradigm, 1월 1, 2026에 액세스, https://www.goal-robots.eu/wp-content/uploads/2021/06/2020_ODDI_ECAI_2020.pdf</li>
<li>Sensing, Planning and Acting – Path to Building Intelligent Robotic Systems - CodeRobo.AI, 1월 1, 2026에 액세스, https://www.coderobo.ai/blogs/sensing-planning-and-acting-path-to-building-intelligent-robotic-systems/</li>
<li>The State of Robot Learning. A partially observed, semi-stochastic… | by Vincent Vanhoucke, 1월 1, 2026에 액세스, https://vanhoucke.medium.com/the-state-of-robot-learning-639dafffbcf8</li>
<li>Is end to end the answer to Robotics as well? - Reddit, 1월 1, 2026에 액세스, https://www.reddit.com/r/robotics/comments/1mmitl1/is_end_to_end_the_answer_to_robotics_as_well/</li>
<li>Vision Language Action Models in Robotic Manipulation: A Systematic Review - arXiv, 1월 1, 2026에 액세스, https://arxiv.org/html/2507.10672v1</li>
<li>Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey, 1월 1, 2026에 액세스, https://arxiv.org/html/2508.13073v1</li>
<li>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, 1월 1, 2026에 액세스, https://robotics-transformer2.github.io/assets/rt2.pdf</li>
<li>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, 1월 1, 2026에 액세스, https://www.cs.utexas.edu/~yukez/cs391r_fall2023/slides/pre_10-24_Ming.pdf</li>
<li>Mind to Hand: Purposeful Robotic Control via Embodied Reasoning - arXiv, 1월 1, 2026에 액세스, https://arxiv.org/html/2512.08580v1</li>
<li>arXiv:2303.03378v1 [cs.LG] 6 Mar 2023, 1월 1, 2026에 액세스, https://arxiv.org/pdf/2303.03378</li>
<li>Agentic LLM-based robotic systems for real-world applications: a review on their agenticness and ethics - Frontiers, 1월 1, 2026에 액세스, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1605405/full</li>
<li>[PDF] PaLM-E: An Embodied Multimodal Language Model | Semantic Scholar, 1월 1, 2026에 액세스, https://www.semanticscholar.org/paper/PaLM-E%3A-An-Embodied-Multimodal-Language-Model-Driess-Xia/38fe8f324d2162e63a967a9ac6648974fc4c66f3</li>
<li>Learning-Based Joint Control with Hierarchical Reinforcement Learning and On-Device Execution - IEEE Xplore, 1월 1, 2026에 액세스, https://ieeexplore.ieee.org/iel8/7083369/7339444/11206440.pdf</li>
<li>[2410.05273] HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers - arXiv, 1월 1, 2026에 액세스, https://arxiv.org/abs/2410.05273</li>
<li>A Dual Process VLA: Efficient Robotic Manipulation Leveraging VLM - arXiv, 1월 1, 2026에 액세스, https://arxiv.org/html/2410.15549v1</li>
<li>Consistency Policy Accelerated Visuomotor Policies via Consistency Distillation - arXiv, 1월 1, 2026에 액세스, https://arxiv.org/html/2405.07503v2</li>
<li>HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers - arXiv, 1월 1, 2026에 액세스, https://arxiv.org/html/2410.05273v3</li>
<li>(PDF) HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers, 1월 1, 2026에 액세스, https://www.researchgate.net/publication/384769482_HiRT_Enhancing_Robotic_Control_with_Hierarchical_Robot_Transformers</li>
<li>STATISTICAL COMPARISON OF DIFFUSION POLICIES AND EXPLICIT BEHAVIOR CLONING FOR ROBOTIC MANIPULATION - DiVA portal, 1월 1, 2026에 액세스, http://www.diva-portal.org/smash/get/diva2:1978992/FULLTEXT01.pdf</li>
<li>Visuomotor Policy Learning via Action Diffusion, 1월 1, 2026에 액세스, https://diffusion-policy.cs.columbia.edu/diffusion_policy_2023.pdf</li>
<li>Diffusion-Driven Perception–Action Interplay for Adaptive Policies, 1월 1, 2026에 액세스, https://jingwang18.github.io/dp-ag.github.io/</li>
<li>[2509.25822] Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for Adaptive Policies - arXiv, 1월 1, 2026에 액세스, https://arxiv.org/abs/2509.25822</li>
<li>Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for Adaptive Policies, 1월 1, 2026에 액세스, https://arxiv.org/html/2509.25822v1</li>
<li>A Step Toward World Models: A Survey on Robotic Manipulation - arXiv, 1월 1, 2026에 액세스, https://arxiv.org/html/2511.02097v2</li>
<li>Understanding World or Predicting Future? A Comprehensive Survey of World Models, 1월 1, 2026에 액세스, https://fi.ee.tsinghua.edu.cn/~dingjingtao/papers/WorldModel.pdf</li>
<li>From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity - arXiv, 1월 1, 2026에 액세스, https://arxiv.org/html/2508.19172v3</li>
<li>Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning - arXiv, 1월 1, 2026에 액세스, https://arxiv.org/html/2508.10399v1</li>
<li>[2411.14499] Understanding World or Predicting Future? A Comprehensive Survey of World Models - arXiv, 1월 1, 2026에 액세스, https://arxiv.org/abs/2411.14499</li>
<li>Publications | Embodiment of perception - Weizmann Institute of Science, 1월 1, 2026에 액세스, https://www.weizmann.ac.il/brain-sciences/labs/ahissar/publications</li>
<li>SayCan: Grounding Language in Robotic Affordances, 1월 1, 2026에 액세스, https://say-can.github.io/</li>
<li>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances - SayCan, 1월 1, 2026에 액세스, https://say-can.github.io/assets/palm_saycan.pdf</li>
<li>Towards Helpful Robots: Grounding Language in Robotic Affordances - Google Research, 1월 1, 2026에 액세스, https://research.google/blog/towards-helpful-robots-grounding-language-in-robotic-affordances/</li>
<li>A review of embodied intelligence systems: a three-layer framework integrating multimodal perception, world modeling, and structured strategies, 1월 1, 2026에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC12631203/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>