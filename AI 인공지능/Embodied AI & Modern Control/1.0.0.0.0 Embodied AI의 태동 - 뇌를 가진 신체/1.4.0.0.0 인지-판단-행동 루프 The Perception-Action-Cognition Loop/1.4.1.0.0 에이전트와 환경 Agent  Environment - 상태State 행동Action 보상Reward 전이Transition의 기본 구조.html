<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:1.4.1 에이전트와 환경 (Agent & Environment): 상태(State), 행동(Action), 보상(Reward), 전이(Transition)의 기본 구조.</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>1.4.1 에이전트와 환경 (Agent & Environment): 상태(State), 행동(Action), 보상(Reward), 전이(Transition)의 기본 구조.</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 1. Embodied AI의 태동: 뇌를 가진 신체</a> / <a href="index.html">1.4 인지-판단-행동 루프 (The Perception-Action-Cognition Loop)</a> / <span>1.4.1 에이전트와 환경 (Agent & Environment): 상태(State), 행동(Action), 보상(Reward), 전이(Transition)의 기본 구조.</span></nav>
                </div>
            </header>
            <article>
                <h1>1.4.1 에이전트와 환경 (Agent &amp; Environment): 상태(State), 행동(Action), 보상(Reward), 전이(Transition)의 기본 구조.</h1>
<p>엠바디드 AI(Embodied AI)의 핵심은 추상적인 알고리즘이 물리적 실체인 ’신체’를 통해 가혹하고 불확실한 현실 세계(Real World)와 상호작용한다는 점에 있다. 인터넷 상의 데이터셋을 학습하는 AI와 달리, 로봇은 자신의 행동을 통해 새로운 데이터를 생성하고, 그 행동의 결과로 환경을 변화시킨다. 이 역동적인 상호작용을 수학적으로 모델링하기 위해 로봇 공학은 강화학습(Reinforcement Learning, RL)의 근간이 되는 마르코프 결정 과정(Markov Decision Process, MDP)과 그 확장인 부분 관측 마르코프 결정 과정(Partially Observable MDP, POMDP)을 차용한다.1</p>
<p>하지만 최신 로봇 연구, 특히 거대 언어 모델(LLM)과 비전-언어-행동 모델(VLA), 디퓨전 정책(Diffusion Policy), 그리고 월드 모델(World Models)의 등장은 고전적인 MDP의 4요소—상태(<span class="math math-inline">S</span>), 행동(<span class="math math-inline">A</span>), 보상(<span class="math math-inline">R</span>), 전이(<span class="math math-inline">P</span>)—의 정의와 경계를 근본적으로 재구성하고 있다. 본 절에서는 최신 SOTA(State-of-the-Art) 기술들이 이 기본 구조를 어떻게 재해석하고 있는지, 그리고 에이전트와 환경 사이의 상호작용 루프가 현대적 관점에서 어떻게 진화하고 있는지 심층적으로 분석한다.</p>
<h2>1.  에이전트와 환경의 경계: 이원론을 넘어서</h2>
<p>전통적인 제어 이론에서 에이전트(Agent)와 환경(Environment)의 구분은 명확했다. 제어기(Controller)는 에이전트이고, 로봇의 기구학적 신체(Mechanism)를 포함한 나머지는 환경으로 간주되었다. 그러나 현대 엠바디드 AI에서 이 경계는 “지능이 어디까지 확장되는가?“라는 질문과 맞물려 유동적으로 변하고 있다.</p>
<h3>1.1  에이전트 (The Agent): 인지하는 두뇌에서 추론하는 신체로</h3>
<p>에이전트는 환경으로부터 감각 정보(Observation)를 받아들이고, 내부 상태(State)를 갱신하며, 목적 함수(Objective Function)를 최대화하기 위해 행동(Action)을 결정하는 주체이다. 과거의 에이전트가 <code>Sense-Plan-Act</code> 파이프라인에 갇힌 알고리즘의 집합이었다면, 현대의 에이전트는 거대한 신경망, 즉 파운데이션 모델(Foundation Model) 그 자체로 진화하고 있다.</p>
<ul>
<li><strong>소프트웨어 2.0 에이전트:</strong> RT-2(Robotic Transformer 2)나 OpenVLA와 같은 최신 VLA 모델에서 에이전트는 시각적 픽셀과 자연어 명령을 입력받아 로봇의 관절 제어 신호를 직접적인 ’토큰(Token)’으로 출력한다.3 여기서 에이전트는 단순히 제어 명령을 내리는 계산기가 아니라, 인터넷 규모의 지식을 바탕으로 상황을 추론(Reasoning)하고 의미론적 맥락(Semantic Context)을 이해하는 ‘두뇌’ 역할을 수행한다.</li>
<li><strong>신체적 자아(Physical Self)의 확장:</strong> 에이전트는 환경 속에 고립된 존재가 아니라, 자신의 신체 상태(Proprioception)를 통해 환경과 연결된다. 최신 연구에서는 로봇이 자신의 팔 길이, 관절의 유연성, 모터의 토크 한계 등을 명시적인 파라미터가 아닌, 학습된 잠재 벡터(Latent Vector) 형태로 인지한다. 이는 에이전트가 자신의 신체조차도 환경의 일부로서 관찰하고 학습해야 함을 시사한다.5</li>
</ul>
<h3>1.2  환경 (The Environment): 확률적이고 적대적인 현실</h3>
<p>환경은 에이전트가 통제할 수 없는 모든 외재적 요소를 총칭한다. 로봇에게 환경은 중력, 마찰력, 조명의 변화, 움직이는 장애물, 그리고 상호작용해야 할 객체들의 집합이다.</p>
<ul>
<li><strong>비결정론적(Stochastic) 특성:</strong> 시뮬레이션과 달리 현실 세계는 본질적으로 확률적이다. 바닥의 미세한 굴곡, 기어의 백래시(Backlash), 센서의 노이즈, 통신 지연 등은 에이전트가 동일한 행동 <span class="math math-inline">A</span>를 취하더라도 결과 상태 <span class="math math-inline">S&#39;</span>이 매번 달라지게 만든다. MDP는 이러한 불확실성을 전이 확률 <span class="math math-inline">P(S&#39; \vert S, A)</span>로 모델링한다.1</li>
<li><strong>부분 관측성(Partial Observability):</strong> 에이전트는 환경의 모든 정보를 알 수 있는 ’전지적 시점(God’s Eye View)’을 갖지 못한다. 오직 제한된 센서를 통해서만 환경을 엿볼 수 있으며, 이는 로봇 문제를 MDP가 아닌 POMDP로 정의해야 하는 결정적인 이유가 된다. 최신 연구들은 이러한 관측의 한계를 극복하기 위해 메모리 기반의 트랜스포머(Transformer)나 순환 신경망(RNN)을 사용하여 과거의 관측들을 통합(Integrate)하려 시도한다.6</li>
</ul>
<h2>2.  상태 (State, <span class="math math-inline">S</span>)와 관측 (Observation, <span class="math math-inline">O</span>): 인식의 불완전성과 잠재 표현</h2>
<p>상태(State)는 특정 시점 <span class="math math-inline">t</span>에서의 세계에 대한 모든 정보의 집합을 의미하며, 마르코프 성질(Markov Property)을 만족해야 한다. 즉, 현재 상태 <span class="math math-inline">S_t</span>는 미래의 상태 <span class="math math-inline">S_{t+1}</span>을 예측하는 데 필요한 충분통계량(Sufficient Statistic)이어야 한다.<br />
<span class="math math-display">
P(S_{t+1} \vert S_t, A_t) = P(S_{t+1} \vert S_t, A_t, S_{t-1}, A_{t-1}, \dots, S_0, A_0)
</span><br />
그러나 로봇 공학의 현실에서 에이전트는 진정한 상태 <span class="math math-inline">S_t</span>에 접근할 수 없으며, 불완전하고 노이즈가 섞인 관측 <span class="math math-inline">O_t</span>만을 얻는다. 따라서 현대 로봇 AI의 핵심 도전 과제는 “어떻게 관측 <span class="math math-inline">O_t</span>로부터 유용한 상태 표현(State Representation)을 학습할 것인가?“로 귀결된다.</p>
<h3>2.1  관측의 다중 모달리티 (Multimodal Observations)</h3>
<p>로봇은 다양한 센서를 통해 세계를 감지한다. 각각의 모달리티는 서로 다른 물리적 속성을 포착하며, SOTA 기술들은 이들을 융합(Sensor Fusion)하여 강건한 상태 추정을 수행한다.</p>
<table><thead><tr><th><strong>모달리티 (Modality)</strong></th><th><strong>특성 및 데이터 구조</strong></th><th><strong>SOTA 활용 기술 및 역할</strong></th></tr></thead><tbody>
<tr><td><strong>RGB-D 비전</strong></td><td>고차원(<span class="math math-inline">H \times W \times 4</span>), 텍스처 및 깊이 정보 포함. 조명 변화에 민감.</td><td><strong>VINS-RGBD, CNN/ViT:</strong> 시각적 SLAM 및 객체 인식의 주류. RT-2와 같은 VLA 모델의 핵심 입력으로, 의미론적 정보(Semantic)와 기하학적 정보(Geometric)를 동시에 제공함.4</td></tr>
<tr><td><strong>LiDAR</strong></td><td>희소한(Sparse) 3D 포인트 클라우드. 거리 측정 정확도 우수, 텍스처 부재.</td><td><strong>PointNet, VoxelNet:</strong> 자율 주행 및 실내 내비게이션에서 장애물 회피와 맵 생성(Mapping)에 필수적.9</td></tr>
<tr><td><strong>고유수용감각 (Proprioception)</strong></td><td>저차원 벡터 (관절 각도 <span class="math math-inline">q</span>, 각속도 <span class="math math-inline">\dot{q}</span>, 토크 <span class="math math-inline">\tau</span>). 로봇의 신체 상태.</td><td><strong>State Encoder:</strong> 로봇 제어의 가장 기초적인 정보. 시각 정보가 차단되거나 부정확할 때 로봇의 자세를 유지하는 데 사용됨.5</td></tr>
<tr><td><strong>촉각 (Tactile)</strong></td><td>접촉면의 힘 분포, 진동, 텍스처. 국소적(Local) 정보.</td><td><strong>GelSight, Visio-Tactile:</strong> 물체 조작(Manipulation) 시 미끄러짐 감지나 재질 파악에 사용. 시각 정보의 사각지대를 보완.10</td></tr>
<tr><td><strong>언어 (Language)</strong></td><td>자연어 명령 및 설명. 추상적이고 고차원적인 문맥 정보.</td><td><strong>LLM Embeddings:</strong> “사과를 집어라“와 같은 목표(Goal)를 상태 공간에 주입하여, 에이전트의 행동을 조건화(Conditioning)함.3</td></tr>
</tbody></table>
<h3>2.2  잠재 상태 (Latent State)와 표현 학습 (Representation Learning)</h3>
<p>고전적인 로봇 공학에서는 칼만 필터(Kalman Filter)나 파티클 필터(Particle Filter)를 사용하여 <span class="math math-inline">P(S_t \vert O_{1:t})</span>를 베이즈 추론으로 계산했다. 그러나 이미지와 같은 고차원 데이터에 대해 이러한 명시적 추론은 불가능에 가깝다.</p>
<p>최신 딥러닝 기반 접근법, 특히 **표현 학습(Representation Learning)**은 관측 <span class="math math-inline">O_t</span>를 저차원의 잠재 벡터 <span class="math math-inline">z_t</span>로 압축(Compression)하는 방식을 취한다.</p>
<ol>
<li><strong>비전 인코더의 진화:</strong> 초기에는 ResNet과 같은 CNN 기반 인코더가 주류였으나, 최근에는 ViT(Vision Transformer)가 표준으로 자리 잡았다. 특히 OpenVLA는 공간적 정밀도를 위한 <strong>DINOv2</strong>와 의미론적 이해를 위한 <strong>SigLIP</strong>의 특징(Feature)을 융합하여 사용한다.4 이는 로봇이 단순히 물체의 위치뿐만 아니라, “그것이 무엇이며 어떻게 조작해야 하는지“에 대한 의미론적 상태(Semantic State)를 이해하게 한다.</li>
<li><strong>시각적 고유수용감각 (Visual Proprioception):</strong> 저가형 로봇이나 소프트 로봇은 정확한 엔코더가 없을 수 있다. 이 경우 외부 카메라의 이미지만으로 로봇의 관절 상태를 역추적하는 기술이 연구되고 있다. 이는 에이전트가 자신의 신체를 ’환경의 일부’로서 시각적으로 재인식하는 과정이며, 저비용 하드웨어의 한계를 소프트웨어로 극복하는 사례이다.5</li>
</ol>
<h3>2.3  POMDP와 신념 상태 (Belief State)의 현대적 구현</h3>
<p>POMDP를 해결하기 위해서는 현재의 관측뿐만 아니라 과거의 관측 역사(History) <span class="math math-inline">H_t = (O_1, A_1, \dots, O_t)</span>를 고려해야 한다. 이 역사를 요약한 것을 신념 상태(Belief State) <span class="math math-inline">b_t</span>라고 한다.2</p>
<ul>
<li><strong>LSTM에서 트랜스포머로:</strong> 과거에는 DRQN(Deep Recurrent Q-Network)과 같이 LSTM을 사용하여 <span class="math math-inline">H_t</span>를 은닉 상태(Hidden State)로 압축했다.6 그러나 LSTM은 긴 시퀀스에서 정보를 잊어버리는(Catastrophic Forgetting) 문제가 있었다.</li>
<li><strong>Decision Transformer (DT):</strong> 최신 연구는 트랜스포머의 어텐션(Attention) 메커니즘을 활용하여 긴 역사 속에서 현재 결정에 필요한 정보만을 선택적으로 조회(Query)한다. <strong>Decision Transformer</strong>나 <strong>Trajectory Transformer</strong>는 상태, 행동, 보상의 시퀀스를 마치 자연어 문장처럼 처리하며, 이를 통해 명시적인 신념 상태 추정 없이도 부분 관측 환경에서 최적의 행동을 도출한다.13 이는 “상태 추정” 문제를 “시퀀스 모델링” 문제로 치환한 패러다임의 전환이다.</li>
<li><strong>Belief State Transformer (BST):</strong> 일부 연구에서는 트랜스포머가 명시적으로 신념 상태 분포를 출력하도록 유도하여, 불확실성이 높은 상황에서의 계획(Planning) 능력을 향상시킨다.15</li>
</ul>
<h2>3.  행동 (Action, <span class="math math-inline">A</span>): 연속적 제어에서 의미론적 토큰으로</h2>
<p>행동 <span class="math math-inline">A_t</span>는 에이전트가 환경에 물리적 영향력을 행사하는 유일한 수단이다. 로봇 공학에서 행동 공간(Action Space)의 정의는 제어의 정밀도, 학습의 난이도, 그리고 에이전트의 일반화 능력에 지대한 영향을 미친다.</p>
<h3>3.1  행동 공간의 스펙트럼: 연속 대 이산 (Continuous vs. Discrete)</h3>
<p>강화학습 알고리즘은 행동 공간이 연속적인지 이산적인지에 따라 크게 달라진다.</p>
<h4>3.1.1  연속 행동 공간 (Continuous Action Space)과 저수준 제어</h4>
<p>전통적인 로봇 제어와 연속적인 강화학습(예: Soft Actor-Critic, PPO)은 실수 벡터 <span class="math math-inline">\mathbf{a} \in \mathbb{R}^n</span>를 출력한다.</p>
<ul>
<li><strong>토크 제어 (Torque Control):</strong> 관절 모터에 인가할 토크(<span class="math math-inline">\tau</span>)를 직접 계산한다. 이는 로봇의 동역학을 완벽하게 제어할 수 있어 유연성(Compliance)이 필요한 작업(예: 계란 집기, 사람과의 악수)에 유리하다.16 그러나 중력 보상이나 마찰력 등을 에이전트가 모두 학습해야 하므로 수렴이 어렵고, Sim-to-Real 간극에 매우 취약하다.</li>
<li><strong>위치/속도 제어 (Position/Velocity Control):</strong> 목표 위치(<span class="math math-inline">q_{des}</span>)나 속도(<span class="math math-inline">\dot{q}_{des}</span>)를 출력하면, 하위의 PID 제어기가 이를 추종한다. 학습이 안정적이고 쉬우나, 접촉이 많은 상황에서 로봇이 뻣뻣하게(Stiff) 반응하여 파손의 위험이 있다.</li>
<li><strong>하이브리드 및 가변 임피던스 제어:</strong> 최근에는 신경망이 위치 명령뿐만 아니라 PD 제어기의 게인(Gain, <span class="math math-inline">K_p, K_d</span>)까지 실시간으로 조절하는 방식이 SOTA로 부상했다. 이를 통해 로봇은 자유 공간에서는 빠르게 움직이고(높은 강성), 물체와 접촉할 때는 부드럽게(낮은 강성) 변하는 적응형 행동을 학습할 수 있다.18</li>
</ul>
<h4>3.1.2  행동의 토큰화 (Action Tokenization)와 VLA 모델</h4>
<p>LLM을 로봇 제어에 도입하면서 발생한 가장 큰 혁신은 연속적인 물리적 행동을 이산적인 **토큰(Token)**으로 변환했다는 점이다.</p>
<ul>
<li><strong>이산화 (Discretization):</strong> <strong>RT-2</strong>와 <strong>OpenVLA</strong>는 관절의 변위나 엔드 이펙터의 위치 좌표를 256개의 구간(Bin)으로 나누어 정수(Integer)로 표현한다.3</li>
<li>예: <code>Action: &lt;Stop&gt; &lt;Lift: 128&gt; &lt;Gripper: 255&gt;</code></li>
<li><strong>언어와의 통합:</strong> 이렇게 토큰화된 행동은 자연어 단어와 동일한 임베딩 공간에서 처리된다. 즉, VLA 모델에게 “파란 블록을 집어라“라는 명령을 수행하는 것은 “나는 학교에 간다“라는 문장을 완성하는 것과 수학적으로 동일한 <strong>다음 토큰 예측(Next-Token Prediction)</strong> 문제가 된다.20</li>
<li><strong>장점:</strong> 이 방식은 연속 공간에서 발생하는 평균화 문제(Mode Averaging)—예를 들어 장애물 왼쪽과 오른쪽 사이의 평균인 중간으로 가서 충돌하는 문제—를 해결한다. 이산적 분포는 명확하게 하나의 모드(Mode)를 선택할 수 있기 때문이다.22</li>
</ul>
<h3>3.2  시간적 추상화: 행동 청킹 (Action Chunking)과 디퓨전 정책</h3>
<p>전통적 MDP는 매 스텝(<span class="math math-inline">t</span>)마다 하나의 행동(<span class="math math-inline">A_t</span>)을 결정한다. 그러나 100Hz 이상의 고주파 제어가 필요한 로봇에서 거대 모델을 매번 추론하는 것은 불가능하다. 이에 대한 SOTA 해결책은 시간적 차원에서 행동을 묶는 **행동 청킹(Action Chunking)**이다.</p>
<ul>
<li><strong>디퓨전 정책 (Diffusion Policy):</strong> 이미지 생성에 쓰이는 디퓨전 모델(Diffusion Model)을 행동 생성에 적용한 혁신적인 기법이다. 가우시안 노이즈로부터 시작하여, 조건부 관측(Observation) 하에서 노이즈를 제거(Denoising)해 나가는 과정을 통해 미래 <span class="math math-inline">k</span>개의 행동 시퀀스 <span class="math math-inline">\mathbf{a}_{t:t+k}</span>를 한꺼번에 생성한다.23</li>
<li><strong>일관성과 부드러움:</strong> 단일 스텝 정책이 갖는 떨림(Jittering) 현상을 제거하고, 궤적(Trajectory) 수준의 일관성을 보장한다.</li>
<li><strong>멀티모달 분포 학습:</strong> 디퓨전 모델은 여러 가지 가능한 해결책(예: 컵을 위로 잡기 vs 옆으로 잡기)의 분포를 정확하게 모델링할 수 있어, 복잡한 조작 작업에서 탁월한 성능을 보인다.</li>
<li><strong>계층적 구조 (Hierarchical Structure):</strong> **Reactive Diffusion Policy (RDP)**와 같은 연구는 느린 주기의 ’청킹 정책(Slow Policy)’이 전체적인 궤적을 계획하고, 빠른 주기의 ’반응형 제어기(Fast Policy)’가 촉각 피드백 등을 반영하여 실시간으로 궤적을 수정하는 이중 루프 구조를 제안한다.10 이는 인간의 인지(Cognition)와 반사(Reflex) 체계를 모방한 것이다.</li>
</ul>
<h2>4.  보상 (Reward, <span class="math math-inline">R</span>): 가치 판단의 기준과 자동화된 설계</h2>
<p>보상 <span class="math math-inline">R_t = R(S_t, A_t, S_{t+1})</span>은 에이전트의 행동이 얼마나 바람직한지를 나타내는 스칼라 신호이다. 강화학습의 목적은 누적 보상의 합(Return)을 최대화하는 것이지만, 로봇 공학에서 적절한 보상 함수를 설계하는 것(Reward Engineering)은 악명 높게 어렵다.</p>
<h3>4.1  희소 보상(Sparse) 대 조밀한 보상(Dense)의 딜레마</h3>
<ul>
<li><strong>희소 보상 (Sparse Reward):</strong> “성공 시 +1, 실패 시 0”. 가장 직관적이고 편향이 없지만, 에이전트가 우연히 성공하기 전까지는 아무런 피드백을 받지 못하므로 학습 효율(Sample Efficiency)이 극도로 낮다.24 복잡한 미로 찾기나 정밀 조립 작업에서는 사실상 학습이 불가능할 수 있다.</li>
<li><strong>조밀한 보상 (Dense Reward):</strong> “목표물에 가까워질수록 +점수”. 학습을 가이드(Guide)하여 속도를 높이지만, **보상 해킹(Reward Hacking)**의 부작용이 크다. 예를 들어, 청소 로봇에게 “먼지가 적을수록 보상“을 주면, 먼지를 카펫 아래로 숨기거나 센서를 가리는 행동을 학습할 수도 있다. 또는 목표에 다가가는 보상만 주면, 목표 바로 앞에서 멈춰서 진동하며 거리 보상을 무한히 챙기는 국소 최적해(Local Optima)에 빠지기도 한다.26</li>
</ul>
<h3>4.2  생성형 AI를 통한 보상 설계의 혁명: Eureka</h3>
<p>최신 SOTA 연구는 보상 함수를 사람이 직접 코딩하는 방식에서 탈피하여, LLM이 이를 자동으로 생성하고 최적화하는 방식으로 전환하고 있다.</p>
<ul>
<li><strong>Eureka (Evolutionary Reward via LLM):</strong> GPT-4와 같은 고성능 LLM을 사용하여 로봇 작업에 대한 보상 함수 코드(Python)를 진화적으로 탐색하는 알고리즘이다.28</li>
</ul>
<ol>
<li><strong>코드 생성:</strong> LLM이 자연어 과제 설명(“펜을 손가락 위에서 돌려라”)을 입력받아 후보 보상 함수 코드를 작성한다.</li>
<li><strong>시뮬레이션 검증:</strong> 생성된 보상 함수로 강화학습 에이전트를 학습시키고 성능을 측정한다.</li>
<li><strong>피드백 루프:</strong> 결과 성능과 에이전트의 행동 로그를 다시 LLM에 피드백으로 제공한다. LLM은 이를 바탕으로 보상 함수를 수정하고 개선한다.</li>
</ol>
<ul>
<li>
<p><strong>결과:</strong> Eureka는 펜 돌리기(Pen Spinning)와 같이 인간 전문가도 보상 함수를 짜기 어려운 고난이도 덱스터러스(Dexterous) 조작 작업에서 인간 수준을 능가하는 보상 함수를 발견해냈다.29</p>
</li>
<li>
<p><strong>자기 정렬 (Self-Alignment)과 안전:</strong> 보상 설계의 자동화는 안전성 문제를 동반한다. 최근 연구는 LLM이 생성한 보상 함수가 의도치 않은 위험 행동을 유발하지 않는지 스스로 검증하고, 제약 조건(Constraint)을 코드로 추가하는 자기 정렬 기법을 포함하고 있다.30</p>
</li>
</ul>
<h3>4.3  내재적 동기 (Intrinsic Motivation)와 비지도 탐색</h3>
<p>외부 보상이 없는 상황에서도 에이전트가 스스로 학습하게 만드는 <strong>내재적 보상</strong> 연구도 활발하다.</p>
<ul>
<li><strong>호기심 (Curiosity):</strong> 에이전트가 미래 상태를 예측하고, 그 예측이 틀렸을 때(즉, 새로운 것을 발견했을 때) 큰 보상을 준다. 이는 에이전트가 환경의 불확실성을 줄이는 방향으로 탐색하게 유도한다.</li>
<li><strong>기술 발견 (Skill Discovery):</strong> 계층적 강화학습(Hierarchical RL)이나 비지도 학습을 통해 에이전트가 “걷기”, “잡기“와 같은 원시 기술(Primitive Skills)을 사전 학습하고, 이를 조합하여 복잡한 보상을 달성하게 한다.31</li>
</ul>
<h2>5.  전이 (Transition, <span class="math math-inline">P</span>)와 월드 모델 (World Models): 시뮬레이션과 상상</h2>
<p>전이 확률 <span class="math math-inline">P(S_{t+1} \vert S_t, A_t)</span>는 에이전트의 행동에 대해 세계가 어떻게 반응하는지를 기술한다. 이는 뉴턴 역학(<span class="math math-inline">F=ma</span>)과 같은 물리 법칙, 물체 간의 충돌 및 접촉, 그리고 환경의 동적인 변화를 모두 포함한다.</p>
<h3>5.1  모델 기반(Model-Based) RL과 월드 모델의 부상</h3>
<p>전통적인 <strong>모델 프리(Model-Free) RL</strong>은 <span class="math math-inline">P</span>를 명시적으로 배우지 않고 시행착오(Trial and Error)를 통해 정책을 학습했다. 이는 편향이 적지만 엄청난 양의 데이터(Sample Inefficiency)를 요구한다. 반면, 최신 SOTA는 <span class="math math-inline">P</span>를 신경망으로 근사하여 에이전트 내부에 **월드 모델(World Model)**을 구축하는 방향으로 나아가고 있다.</p>
<ul>
<li><strong>DreamerV3:</strong> 입력된 고차원 이미지 관측을 저차원 잠재 상태(Latent State)로 인코딩하고, 이 잠재 공간(Latent Space)에서의 전이 역학(Transition Dynamics)을 학습한다.32</li>
<li><strong>상상 속의 학습 (Learning in Imagination):</strong> 에이전트는 실제 환경과 상호작용하지 않고도, 자신의 월드 모델 내부에서 수천 번의 가상 시뮬레이션을 수행(Dreaming)하며 미래를 예측하고 정책을 최적화한다.</li>
<li><strong>범용성:</strong> DreamerV3는 아타리 게임부터 로봇 팔 제어, 마인크래프트 탐험에 이르기까지 별도의 하이퍼파라미터 튜닝 없이 다양한 도메인에서 SOTA 성능을 달성했다. 이는 전이 모델 학습이 데이터 효율성과 일반화 능력의 핵심임을 입증한다.33</li>
</ul>
<h3>5.2  물리 엔진의 진화: 미분 가능한 시뮬레이션 (Differentiable Physics)</h3>
<p>전이 모델 <span class="math math-inline">P</span>를 신경망으로 학습하는 것(Learned Model)과 달리, 물리 엔진 자체를 미분 가능하게 만들어 최적화에 활용하는 접근법도 있다.</p>
<ul>
<li><strong>Brax와 DiffMPM:</strong> MuJoCo와 같은 기존 시뮬레이터는 미분이 불가능하여 그래디언트(Gradient)를 전파할 수 없었다. 반면 Google의 <strong>Brax</strong>나 <strong>DiffMPM</strong>과 같은 미분 가능한 물리 엔진은 시뮬레이션 결과 오차에 대한 미분값을 계산하여, 정책 신경망이나 물리 파라미터를 역전파(Backpropagation)로 직접 업데이트할 수 있게 한다.34</li>
<li><strong>분석적 제어와 학습의 결합:</strong> 이는 딥러닝의 학습 능력과 물리 법칙의 정확성을 결합하여, 수백만 번의 반복이 필요한 학습 과정을 수천 번 수준으로 획기적으로 단축시킨다.35</li>
</ul>
<h3>5.3  Sim-to-Real: 가상과 현실의 간극 극복</h3>
<p>전이 모델(시뮬레이터 또는 월드 모델)인 <span class="math math-inline">P_{sim}</span>과 실제 현실의 <span class="math math-inline">P_{real}</span> 사이에는 필연적인 오차가 존재한다. 이를 <strong>Sim-to-Real Gap</strong>이라 한다.</p>
<ul>
<li><strong>도메인 무작위화 (Domain Randomization):</strong> 시뮬레이션의 물리 파라미터(마찰 계수, 질량, 모터 감쇠 등)와 시각적 요소(조명, 텍스처)를 광범위하게 무작위화하여 학습시킨다. 에이전트는 다양한 <span class="math math-inline">P</span>의 분포에 노출됨으로써, 특정 물리 조건에 과적합(Overfitting)되지 않고 현실 세계의 불확실성에 강건한(Robust) 정책을 학습하게 된다.37</li>
<li><strong>잔차 학습 (Residual Physics):</strong> <span class="math math-inline">P_{real} \approx P_{sim} + \delta(S, A)</span> 형태로 모델링하여, 물리 엔진이 설명하지 못하는 잔차(Residual) 부분 <span class="math math-inline">\delta</span>만을 신경망이 학습하도록 한다. 이는 모델 기반 제어의 안정성과 학습 기반 방법의 유연성을 동시에 확보하는 전략이다.39</li>
</ul>
<h2>6. 결론: 통합된 인지-행동 루프</h2>
<p>본 절에서 살펴본 바와 같이, 에이전트와 환경을 구성하는 4가지 요소—상태, 행동, 보상, 전이—는 현대 엠바디드 AI 기술에 힘입어 극적인 진화를 겪고 있다.</p>
<ol>
<li><strong>상태(<span class="math math-inline">S</span>):</strong> 고정된 물리량 측정에서 트랜스포머 기반의 **시공간적 맥락 통합(Context Integration)**과 <strong>잠재 표현 학습</strong>으로 진화했다.</li>
<li><strong>행동(<span class="math math-inline">A</span>):</strong> 단순한 제어 신호 출력에서 LLM 기반의 <strong>의미론적 토큰 생성</strong>과 디퓨전 모델 기반의 **전체 궤적 생성(Chunking)**으로 확장되었다.</li>
<li><strong>보상(<span class="math math-inline">R</span>):</strong> 인간의 수동 설계를 넘어 LLM에 의한 <strong>자동 생성(Generative Reward)</strong> 및 자기 정렬로 고도화되었다.</li>
<li><strong>전이(<span class="math math-inline">P</span>):</strong> 블랙박스 환경을 넘어, 에이전트가 스스로 구축한 <strong>월드 모델</strong> 내에서의 상상과 계획이 가능해졌다.</li>
</ol>
<p>이러한 변화는 기존의 로봇 공학이 고수해온 <code>Sense-Plan-Act</code>의 엄격한 모듈성을 허물고, 인지와 행동이 하나의 거대한 신경망 안에서 유기적으로 융합되는 <strong>엔드투엔드(End-to-End)</strong> 시대를 예고한다. 이는 단순히 제어 성능의 향상을 넘어, 로봇이 진정한 의미의 ’지능’을 가지고 환경과 상호작용하는 새로운 패러다임의 시작점이다. 이어지는 절에서는 이러한 구성 요소들이 기존 파이프라인의 한계를 어떻게 극복하고 있는지 구체적으로 논의한다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Chapter 1: Markov Decision Processes and Partially Observable Markov Decision Processes | by Wang Haitao | Deep Reinforcement Learning Notebook | Medium, https://medium.com/deep-reinforcement-learning-notebook/chapter-1-markov-decision-processes-and-partially-observable-markov-decision-processes-18cbc8c55311</li>
<li>Partially observable Markov decision process - Wikipedia, https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process</li>
<li>RT-2: Vision-Language-Action Models, https://robotics-transformer2.github.io/</li>
<li>OpenVLA: Open Source VLA for Robotics - Emergent Mind, https://www.emergentmind.com/topics/openvla</li>
<li>Latent Representations for Visual Proprioception in Inexpensive Robots - arXiv, https://arxiv.org/html/2504.14634v1</li>
<li>Paper review: Deep Transformer Q Networks | by Aritra Chakrabarty - Medium, https://medium.com/correll-lab/deep-transformer-q-networks-a-paper-analysis-e7efd9379e5f</li>
<li>Rethinking Transformers in Solving POMDPs - arXiv, https://arxiv.org/html/2405.17358v1</li>
<li>RGBD-Inertial Trajectory Estimation and Mapping for Ground Robots - MDPI, https://www.mdpi.com/1424-8220/19/10/2251</li>
<li>How LiDAR &amp; RGB-D Cameras Compare and Work Together in AMRs - Orbbec, https://www.orbbec.com/blog/how-lidar-and-rgbd-cameras-compare-and-work-together/</li>
<li>Reactive Diffusion Policy, https://reactive-diffusion-policy.github.io/</li>
<li>Reactive Diffusion Policy - Robotics, https://www.roboticsproceedings.org/rss21/p052.pdf</li>
<li>Vision Language Action Models in Robotic Manipulation: A Systematic Review - arXiv, https://arxiv.org/html/2507.10672v1</li>
<li>Online Decision Transformer - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v162/zheng22c/zheng22c.pdf</li>
<li>Balancing Context Length and Mixing Times for Reinforcement Learning at Scale - NIPS papers, https://proceedings.neurips.cc/paper_files/paper/2024/file/92f79f493ca2d6c0ba04c3af76bb3368-Paper-Conference.pdf</li>
<li>[Quick Review] The Belief State Transformer - Liner, https://liner.com/review/the-belief-state-transformer</li>
<li>Action Space Design – Reinforcement Learning for Robot Motor Skills » Lamarr-Blog, https://lamarr-institute.org/blog/action-space-design-reinforcement-learning/</li>
<li>Position Control vs Velocity Control vs Torque Control - Robotics Stack Exchange, https://robotics.stackexchange.com/questions/10052/position-control-vs-velocity-control-vs-torque-control</li>
<li>Continuous-Discrete Reinforcement Learning for Hybrid Control in Robotics, http://proceedings.mlr.press/v100/neunert20a/neunert20a.pdf</li>
<li>DecAP : Decaying Action Priors for Accelerated Imitation Learning of Torque-Based Legged Locomotion Policies - arXiv, https://arxiv.org/html/2310.05714v3</li>
<li>Learning-Deep-Learning/paper_notes/rt2.md at master - GitHub, https://github.com/patrick-llgc/Learning-Deep-Learning/blob/master/paper_notes/rt2.md</li>
<li>Vision Language Action Models (VLA) &amp; Policies for Robots - Learn OpenCV, https://learnopencv.com/vision-language-action-models-lerobot-policy/</li>
<li>Discrete vs Continuous action space : r/reinforcementlearning - Reddit, https://www.reddit.com/r/reinforcementlearning/comments/rsc59y/discrete_vs_continuous_action_space/</li>
<li>Streaming Flow Policy Simplifying diffusion/flow-matching policies by treating action trajectories as flow trajectories Website: https://streaming-flow-policy.github.io - arXiv, https://arxiv.org/html/2505.21851v2</li>
<li>What are the key points in reward function design in deep reinforcement learning?, https://www.tencentcloud.com/techpedia/107500?ref=luxalgo.com</li>
<li>Real-World DRL: 5 Essential Reward Functions for Modeling Objectives and Constraints, https://medium.com/@zhonghong9998/real-world-drl-5-essential-reward-functions-for-modeling-objectives-and-constraints-e742325d4747</li>
<li>What is the reward function in reinforcement learning? - Milvus, https://milvus.io/ai-quick-reference/what-is-the-reward-function-in-reinforcement-learning</li>
<li>What are the pros and cons of sparse and dense rewards in reinforcement learning?, https://ai.stackexchange.com/questions/23012/what-are-the-pros-and-cons-of-sparse-and-dense-rewards-in-reinforcement-learning</li>
<li>Eureka: Human-Level Reward Design via Coding Large Language Models - arXiv, https://arxiv.org/abs/2310.12931</li>
<li>Eureka | Human-Level Reward Design via Coding Large Language Models, https://eureka-research.github.io/</li>
<li>Learning Reward for Robot Skills Using Large Language Models via Self-Alignment - arXiv, https://arxiv.org/html/2405.07162v2</li>
<li>Awesome Model-Based Reinforcement Learning - GitHub, https://github.com/opendilab/awesome-model-based-RL</li>
<li>The Evolution of Imagination: A Deep Dive into DreamerV3 and its Conquest of Minecraft, https://findingtheta.com/blog/the-evolution-of-imagination-a-deep-dive-into-dreamerv3-and-its-conquest-of-minecraft</li>
<li>World Models: The Next Leap Beyond LLMs | by Graison Thomas | Medium, https://medium.com/@graison/world-models-the-next-leap-beyond-llms-012504a9c1e7</li>
<li>Differentiable Simulations and Interactive World Models - Oden Institute, <a href="https://oden.utexas.edu/news-and-events/events/2164---Krishna%20Kumar/">https://oden.utexas.edu/news-and-events/events/2164—Krishna%20Kumar/</a></li>
<li>Differentiable Information Enhanced Model-Based Reinforcement Learning, https://ojs.aaai.org/index.php/AAAI/article/download/34419/36574</li>
<li>Differentiable Information Enhanced Model-Based Reinforcement Learning - arXiv, https://arxiv.org/html/2503.01178v1</li>
<li>Sim-to-real via latent prediction: Transferring visual non-prehensile manipulation policies, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2022.1067502/full</li>
<li>Sim and Real: Better Together - NeurIPS, https://proceedings.neurips.cc/paper_files/paper/2021/file/36f4d832825380f102846560a5104c90-Paper.pdf</li>
<li>Dojo: A Differentiable Physics Engine for Robotics - arXiv, https://arxiv.org/html/2203.00806v5</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>