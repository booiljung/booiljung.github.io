<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:1.4.3 엔드투엔드(End-to-End) 접근법의 부상: 픽셀(Pixel)에서 토크(Torque)까지, 신경망 하나로 연결하려는 시도.</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>1.4.3 엔드투엔드(End-to-End) 접근법의 부상: 픽셀(Pixel)에서 토크(Torque)까지, 신경망 하나로 연결하려는 시도.</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 1. Embodied AI의 태동: 뇌를 가진 신체</a> / <a href="index.html">1.4 인지-판단-행동 루프 (The Perception-Action-Cognition Loop)</a> / <span>1.4.3 엔드투엔드(End-to-End) 접근법의 부상: 픽셀(Pixel)에서 토크(Torque)까지, 신경망 하나로 연결하려는 시도.</span></nav>
                </div>
            </header>
            <article>
                <h1>1.4.3 엔드투엔드(End-to-End) 접근법의 부상: 픽셀(Pixel)에서 토크(Torque)까지, 신경망 하나로 연결하려는 시도.</h1>
<p>로봇 공학의 장대한 역사 속에서 2010년대 중반은 단순한 기술적 진보를 넘어선, 인식론적 패러다임의 거대한 전환점(Turning Point)으로 기록된다. 이는 수십 년간 로봇 공학계를 지배해 온 환원주의적 설계 철학, 즉 복잡한 문제를 인지(Perception), 계획(Planning), 제어(Control)라는 독립된 모듈로 쪼개어 해결하려는 ‘분할 정복(Divide and Conquer)’ 전략에 대한 근본적인 도전이었다. 이 도전의 최전선에는 <strong>엔드투엔드(End-to-End) 학습</strong>이라는 개념이 자리 잡고 있었다. 카메라 렌즈를 통해 들어오는 원시적인 광자(Photon)의 배열, 즉 픽셀(Pixel) 정보를 그 어떠한 명시적 중간 단계 없이 로봇 관절의 모터를 구동하는 물리적 힘, 즉 토크(Torque)로 직결시키려는 이 시도는 당시로서는 무모해 보일 정도로 대담한 발상이었다.</p>
<p>본 장에서는 ’픽셀에서 토크까지(From Pixels to Torques)’라는 슬로건 아래 전개된 이 혁신적인 흐름을 심층적으로 분석한다. 우리는 먼저 전통적인 ‘감지-계획-행동(Sense-Plan-Act)’ 파이프라인이 직면했던 구조적 한계와 정보 이론적 병목 현상을 진단하고, 1980년대 후반 신경망의 태동기부터 시작된 엔드투엔드 제어의 역사적 맥락을 되짚어볼 것이다. 나아가 2015년 서게이 레빈(Sergey Levine) 등을 필두로 한 연구진이 딥러닝(Deep Learning)을 무기로 이 난제를 어떻게 돌파했는지, 그 기술적 아키텍처와 학습 알고리즘을 해부한다. 마지막으로, 이 접근법이 로봇에게 ’암묵적 지식(Tacit Knowledge)’을 학습할 능력을 부여함으로써 얻어낸 성취와, 여전히 해결되지 않은 ’블랙박스(Black Box)’의 위험성이라는 이중적 면모를 비판적으로 고찰할 것이다.</p>
<h2>1.  분절된 지능의 한계: 모듈러 파이프라인과 정보의 병목</h2>
<p>엔드투엔드 접근법의 혁신성을 온전히 이해하기 위해서는, 그것이 해체하고자 했던 기존 체제, 즉 고전적 로봇 제어 아키텍처의 철학적 기반과 그 한계를 명확히 규명해야 한다. 로봇 공학의 태동기부터 엔지니어들은 복잡성을 관리하기 위해 시스템을 기능별로 분리하는 모듈러 디자인(Modular Design)을 채택해 왔다.</p>
<h3>1.1  감지-계획-행동(Sense-Plan-Act) 사이클의 구조적 모순</h3>
<p><strong>감지-계획-행동(SPA)</strong> 사이클은 현대 로봇 공학의 근간을 이루는 프레임워크다.1 이 구조 하에서 로봇의 지능은 선형적이고 순차적인 정보 처리 과정으로 정의된다.</p>
<ol>
<li><strong>감지(Sense):</strong> 카메라인 LiDAR와 같은 센서를 통해 외부 환경의 데이터를 수집하고, 이를 ’상태(State)’라는 수학적 표현으로 변환한다. 이 과정은 주로 컴퓨터 비전(Computer Vision)이나 상태 추정(State Estimation) 알고리즘이 담당한다.3</li>
<li><strong>계획(Think/Plan):</strong> 추정된 상태 정보를 바탕으로, 로봇이 목표를 달성하기 위해 수행해야 할 최적의 행동 시퀀스나 경로를 생성한다. 여기에는 경로 계획(Path Planning)이나 작업 계획(Task Planning) 알고리즘이 사용된다.4</li>
<li><strong>행동(Act):</strong> 생성된 계획을 실행하기 위해 하위 레벨 제어기(Low-level Controller)가 각 관절의 모터에 전기적 신호를 보낸다. 주로 PID 제어기나 임피던스 제어기가 이 역할을 맡는다.1</li>
</ol>
<p>이러한 모듈화는 각 하위 시스템을 독립적으로 개발, 테스트, 디버깅할 수 있다는 공학적 편의성을 제공했다. 그러나 이 방식은 각 단계 사이의 경계면(Interface)에서 심각한 **정보 소실(Information Loss)**을 유발한다.6 예를 들어, 고해상도 이미지에 담긴 풍부한 텍스처, 미묘한 조명의 변화, 물체 표면의 마찰 계수와 같은 정보들은 ‘감지’ 단계에서 ’물체의 3차원 좌표(x, y, z)’와 같은 기하학적 정보로 압축되는 과정에서 대부분 폐기된다. ‘계획’ 모듈은 이렇게 희석된 정보만을 바탕으로 판단을 내려야 하며, 이는 복잡하고 비정형적인 환경에서 로봇의 적응력을 떨어뜨리는 주원인이 된다.</p>
<h3>1.2  상징 접지 문제(Symbol Grounding Problem)와 의미의 부재</h3>
<p>모듈러 접근법의 더 깊은 이론적 난제는 인지과학과 인공지능 철학에서 오랫동안 논의되어 온 **상징 접지 문제(Symbol Grounding Problem)**와 맞닿아 있다.7</p>
<p>전통적인 로봇 시스템 내부에서 처리되는 정보들은 0과 1로 이루어진 기호(Symbol)들의 집합이다. 문제는 이 기호들이 실제 물리 세계의 대상과 본질적으로 연결(Grounding)되어 있지 않다는 점이다. 프로그래머가 “컵“이라는 기호에 컵의 3D 모델을 연결해 줄 수는 있지만, 로봇에게 “컵“은 그저 “잡을 수 있는 대상“이라는 추상적 속성일 뿐, “미끄러운 표면을 가졌으므로 꽉 쥐어야 하지만 깨질 수 있으니 조심해야 하는 대상“이라는 감각 운동적(Sensorimotor) 의미를 내포하지 못한다.10</p>
<p>스티븐 하나드(Stevan Harnad)가 제기한 이 문제는 로봇이 사전에 정의되지 않은 낯선 환경에 놓였을 때 치명적인 오류로 이어진다. 로봇은 자신이 조작하는 기호의 물리적 의미를 모르기 때문에, 인식 모듈에서 발생한 작은 오차가 계획 모듈과 행동 모듈을 거치며 증폭되는 <strong>오류 전파(Error Propagation)</strong> 현상을 막을 수 없다.11 예를 들어, 비전 시스템이 컵의 위치를 5mm 잘못 추정했다면, 로봇 팔은 허공을 움켜쥐거나 컵을 쳐서 떨어뜨리게 된다. 이때 제어 모듈은 자신의 행동이 왜 실패했는지 알 수 없다. 인식 모듈이 넘겨준 좌표대로 정확히 움직였기 때문이다. 이러한 책임의 단절은 시스템 전체의 학습과 개선을 가로막는 거대한 장벽이었다.</p>
<h3>1.3  표: 모듈러 파이프라인과 정보 흐름의 단절</h3>
<p>아래 표는 전통적인 SPA 파이프라인에서 단계별로 정보가 어떻게 축소되고 왜곡될 수 있는지를 보여준다.</p>
<table><thead><tr><th><strong>단계 (Module)</strong></th><th><strong>입력 데이터 (Input)</strong></th><th><strong>출력 데이터 (Output)</strong></th><th><strong>정보 처리 특성 및 손실 (Information Loss)</strong></th></tr></thead><tbody>
<tr><td><strong>감지 (Sense)</strong></td><td>Raw RGB 이미지 (예: 1920x1080x3 픽셀), LiDAR 포인트 클라우드</td><td>물체 클래스, 6D 포즈 (위치+자세), 경계 상자(Bounding Box)</td><td><strong>극심한 추상화:</strong> 텍스처, 재질, 조명, 미세한 물리적 속성(마찰 등)이 제거되고 기하학적 좌표만 남음.</td></tr>
<tr><td><strong>계획 (Plan)</strong></td><td>물체의 6D 포즈, 로봇의 현재 관절 각도, 장애물 지도</td><td>관절 공간의 궤적 (Trajectory), 웨이포인트(Waypoints)</td><td><strong>동역학 무시:</strong> 로봇 팔의 관성이나 물체의 무게중심 변화 등 동적인 요소를 고려하지 않고 기구학적(Kinematic) 경로만 생성하는 경우가 많음.</td></tr>
<tr><td><strong>행동 (Act)</strong></td><td>목표 관절 각도/속도</td><td>모터 전압/전류 (Motor Torque)</td><td><strong>블라인드 실행:</strong> 상위 모듈의 명령을 맹목적으로 추종. 실제 환경과의 접촉에서 발생하는 예상치 못한 반발력 등에 유연하게 대처하기 어려움.</td></tr>
</tbody></table>
<p>이러한 배경 속에서, 연구자들은 “만약 이 모든 과정을 하나의 함수로 합칠 수 있다면 어떨까?“라는 질문을 던지기 시작했다. 로봇의 눈(센서)과 근육(모터)을 잇는 신경망을 만들어, 그 사이의 복잡한 정보 처리를 데이터에 기반한 학습에 맡겨보자는 발상이었다.</p>
<h2>2.  태동기의 선구자들: ALVINN과 신경망 제어의 여명</h2>
<p>엔드투엔드 제어의 개념이 2010년대 딥러닝 붐과 함께 갑자기 등장한 것은 아니다. 그 뿌리는 인공지능 연구의 역사 깊은 곳, 특히 **연결주의(Connectionism)**가 부상하던 1980년대 후반으로 거슬러 올라간다.</p>
<h3>2.1  ALVINN: 도로를 읽는 신경망의 눈</h3>
<p>1989년, 카네기 멜론 대학(CMU)의 딘 포머로(Dean Pomerleau)가 개발한 **ALVINN(Autonomous Land Vehicle In a Neural Network)**은 현대적 엔드투엔드 자율주행 시스템의 직계 조상이라 할 수 있다.13 당시 자율주행 연구의 주류는 이미지에서 차선(Lane Marker)을 검출하기 위해 소벨 필터(Sobel Filter)나 허프 변환(Hough Transform)과 같은 핸드크래프트(Hand-crafted) 알고리즘을 사용하는 것이었다. 그러나 이러한 방식은 차선이 지워지거나, 그림자가 지거나, 비포장도로와 같이 차선이 없는 환경에서는 무용지물이었다.</p>
<p>ALVINN은 급진적인 대안을 제시했다. 포머로는 3층 구조의 단순한 역전파(Back-propagation) 신경망을 설계했다.</p>
<ul>
<li><strong>입력층:</strong> 30x32 해상도의 저화질 비디오 카메라 입력(Retina)과 레이저 거리 측정기 데이터.</li>
<li><strong>은닉층:</strong> 29개의 유닛으로 구성된 단일 은닉층.</li>
<li><strong>출력층:</strong> 차량의 조향 방향(Steering Curvature)을 나타내는 45개의 유닛.</li>
</ul>
<p>ALVINN은 사람이 운전하는 모습을 관찰하며(Learning from Demonstration), 입력 이미지 패턴과 사람의 조향 명령 사이의 관계를 스스로 학습했다. 놀랍게도, 단 5분 정도의 훈련만으로 ALVINN은 포장도로뿐만 아니라 비포장도로나 차선이 없는 도로에서도 주행하는 법을 터득했다.13</p>
<h3>2.2  특징 추출의 자동화와 그 의의</h3>
<p>ALVINN의 가장 큰 성취는 **특징 추출(Feature Extraction)**의 자동화였다. 기존 시스템에서는 엔지니어가 “도로란 무엇인가(예: 평행한 두 개의 선)“를 정의해야 했다면, ALVINN은 데이터로부터 “도로처럼 보이는 패턴“을 스스로 찾아냈다. 연구 분석 결과, ALVINN의 은닉층 뉴런들은 다양한 너비와 위치를 가진 도로의 가장자리에 반응하는 필터(Filter)로 진화해 있었다.14 이는 신경망이 환경의 가변성에 적응할 수 있는 유연한 내부 표현(Representation)을 구축할 수 있음을 증명한 사례였다.</p>
<p>그러나 1990년대 초반, 컴퓨팅 파워의 부족과 데이터 수집의 어려움, 그리고 소위 ‘사라지는 기울기(Vanishing Gradient)’ 문제 등으로 인해 신경망 연구는 두 번째 겨울(AI Winter)을 맞이했고, ALVINN의 유산은 잠시 잊히는 듯했다.16 로봇 공학은 다시금 정교한 수학적 모델링과 확률론적 로보틱스(Probabilistic Robotics)가 지배하는 시대로 회귀했다.</p>
<h2>3.  딥러닝과 로보틱스의 재결합: 픽셀에서 토크까지</h2>
<p>2010년대, GPU(Graphics Processing Unit)의 비약적인 발전과 대용량 데이터셋의 등장으로 딥러닝이 부활하면서 로봇 공학계에도 다시금 변화의 바람이 불었다. 특히 2015년에서 2016년 사이, UC 버클리의 서게이 레빈(Sergey Levine), 첼시 핀(Chelsea Finn), 피터 아빌(Pieter Abbeel) 등의 연구진이 발표한 일련의 논문들은 **“End-to-End Training of Deep Visuomotor Policies”**라는 제목으로 이 접근법을 로봇 조작(Manipulation) 분야의 주류 담론으로 끌어올렸다.17</p>
<h3>3.1  픽셀에서 토크로(From Pixels to Torques): 패러다임의 정의</h3>
<p>레빈 연구진이 주창한 이 접근법의 핵심은 로봇의 제어 정책(Policy, <span class="math math-inline">\pi</span>)을 입력 관측(Observation, <span class="math math-inline">o_t</span>)에서 행동(Action, <span class="math math-inline">u_t</span>)으로 가는 단일한 매핑 함수로 정의하고, 이를 깊은 신경망(Deep Neural Network)으로 구현하는 것이었다.</p>
<ul>
<li><strong>입력(Pixels):</strong> 로봇의 관절 각도와 같은 고유 수용 감각(Proprioception) 정보와 함께, 카메라로부터 들어오는 원시 RGB 이미지를 직접 입력받는다. 이는 물체의 위치를 별도로 추정하는 단계가 생략됨을 의미한다.</li>
<li><strong>출력(Torques):</strong> 로봇의 행동은 단순히 손끝의 위치(Task-space coordinate)를 이동시키는 것이 아니라, 각 관절 모터에 인가할 토크 값(Joint Torques)을 직접 출력한다.20</li>
</ul>
<p>왜 위치(Position)나 속도(Velocity)가 아닌 토크인가?</p>
<p>전통적인 산업용 로봇은 위치 제어(Position Control)를 사용한다. 이는 “A지점에서 B지점으로 가라“는 명령을 내리면, 강력한 모터가 오차 없이 그 위치로 이동하는 방식이다. 그러나 이 방식은 딱딱하다(Stiff). 로봇이 환경과 접촉할 때 유연하게 대처하지 못한다. 반면, **토크 제어(Torque Control)**는 로봇이 가하는 힘을 제어하므로, 환경에 순응(Compliance)할 수 있다. 예를 들어 병뚜껑을 돌려 닫는 작업을 할 때, 위치 제어 로봇은 나사산이 조금만 어긋나도 병을 부수거나 멈춰버리지만, 토크 제어 로봇은 저항이 느껴지면 부드럽게 힘을 조절하여 나사산을 따라갈 수 있다.5 엔드투엔드 방식은 시각 정보로부터 이러한 미세한 힘의 조절까지 학습하려 시도했다.</p>
<h3>3.2  기술적 아키텍처: 공간적 정보를 보존하는 CNN</h3>
<p>연구진은 로봇 제어에 특화된 새로운 형태의 합성곱 신경망(CNN) 아키텍처를 제안했다. 당시 컴퓨터 비전 분야를 휩쓸던 AlexNet이나 VGGNet과 같은 분류(Classification) 모델들과는 결정적인 차이가 있었다.24</p>
<ol>
<li><strong>풀링 레이어(Pooling Layer)의 제거:</strong> 이미지 분류 작업에서는 “고양이가 이미지의 어디에 있든 고양이다“라는 위치 불변성(Translation Invariance)이 중요하다. 이를 위해 풀링 연산을 통해 공간 정보를 압축한다. 그러나 로봇 제어에서는 물체의 <strong>정확한 위치</strong>가 생명이다. 따라서 연구진은 풀링 레이어를 제거하여 픽셀의 공간적 정보를 최대한 보존했다.</li>
<li><strong>공간적 소프트맥스(Spatial Softmax):</strong> 일반적인 CNN은 마지막에 완전 연결 층(Fully Connected Layer)으로 넘어가기 위해 특징 맵(Feature Map)을 1차원 벡터로 평탄화(Flatten)한다. 이 과정에서 공간 정보가 완전히 파괴된다. 레빈 팀은 대신 **공간적 소프트맥스(Spatial Soft-Argmax)**를 도입했다. 이는 각 특징 채널(Feature Channel)에서 가장 활성화된 지점의 <span class="math math-inline">(x, y)</span> 좌표를 기댓값(Expectation) 형태로 추출한다.</li>
</ol>
<ul>
<li>이 기법은 신경망이 이미지 내부의 주요 지점(Keypoints), 예를 들어 로봇의 손끝, 물체의 모서리, 목표 지점 등을 스스로 찾아내고 그 좌표를 제어기에 전달하도록 유도한다.17</li>
</ul>
<h3>3.3  학습 전략: 유도된 정책 탐색(Guided Policy Search, GPS)</h3>
<p>수십만 개의 파라미터를 가진 신경망을 처음부터(From Scratch) 강화학습만으로 훈련시키는 것은 데이터 효율성 측면에서 재앙에 가까웠다. 로봇 팔을 수백만 번 휘두르게 할 수는 없었기 때문이다. 이를 해결하기 위해 <strong>유도된 정책 탐색(Guided Policy Search)</strong> 알고리즘이 도입되었다.17</p>
<p>이 방법은 학습 과정을 두 단계로 분리한다:</p>
<ol>
<li><strong>궤적 최적화(Trajectory Optimization) - 교사(Teacher):</strong> 훈련 초기에는 로봇에게 환경의 모든 정보(Full State)를 제공한다. 이를 바탕으로 최적 제어(Optimal Control) 알고리즘(예: LQR, iLQG)을 실행하여 과제를 성공적으로 수행하는 이상적인 궤적을 생성한다. 이것이 ’정답지’가 된다.</li>
<li><strong>지도 학습(Supervised Learning) - 학생(Student):</strong> 신경망 정책(Policy)은 이제 오직 카메라 이미지와 관절 각도(Partial Observation)만을 보고, 교사가 생성해 낸 이상적인 궤적의 토크 값을 모방(Imitation)하도록 학습된다.</li>
</ol>
<p>이 과정을 반복함으로써, 로봇은 복잡한 수학적 계산 없이도 시각 정보만으로 숙련된 제어기의 행동을 흉내 낼 수 있게 된다. 이는 마치 장인이 도제에게 손을 잡고 움직임을 가르치는 것과 유사하다.</p>
<h3>3.4  실험적 성과: 조작 기술의 진보</h3>
<p>이 접근법은 기존의 방식으로는 매우 까다로웠던 작업들에서 놀라운 성과를 보여주었다.24</p>
<ul>
<li><strong>옷걸이 걸기(Placing a hanger):</strong> 옷걸이는 형태가 복잡하고 얇아서 기존의 비전 시스템으로는 인식하기 매우 어렵다.</li>
<li><strong>장난감 망치의 갈고리로 못 빼기:</strong> 도구의 끝부분을 정확히 못 머리 아래로 밀어 넣어야 하는 정밀한 작업이다.</li>
<li><strong>병뚜껑 닫기:</strong> 나사산을 맞추기 위해 섬세한 힘 조절과 시각적 피드백의 긴밀한 루프가 필요하다.</li>
</ul>
<p>이러한 실험들은 엔드투엔드 신경망이 단순히 물체의 위치를 파악하는 것을 넘어, 물체와의 **접촉 역학(Contact Dynamics)**을 시각적 정보와 연관 지어 학습할 수 있음을 증명했다.</p>
<h2>4.  엔드투엔드 접근법이 가져온 혁신과 이점</h2>
<p>“픽셀에서 토크까지” 연결하려는 시도는 로봇 공학의 고질적인 문제들에 대한 새로운 해법을 제시했다.</p>
<h3>4.1  카메라 보정(Calibration)의 종말</h3>
<p>전통적인 로봇 시스템 구축에서 가장 고통스러운 작업 중 하나는 카메라 보정(Calibration)이었다. 카메라의 렌즈 왜곡을 펴고, 카메라 좌표계와 로봇 좌표계를 완벽하게 일치시키는 변환 행렬을 구해야 했다. 만약 카메라가 진동으로 1mm만 틀어져도, 로봇의 손끝은 1mm만큼 빗나갔다.</p>
<p>엔드투엔드 신경망은 이러한 명시적인 보정 절차를 제거했다. 신경망은 이미지상의 픽셀 패턴과 모터의 움직임 사이의 관계를 직접 학습하므로, 카메라가 어디에 달려있든, 렌즈가 얼마나 왜곡되었든 상관없이 그 상태 그대로의 매핑 함수를 찾아낸다. 이는 로봇 시스템의 설치와 유지보수 비용을 획기적으로 낮출 수 있는 가능성을 열었다.24</p>
<h3>4.2  목적 지향적 인지(Goal-Driven Perception)</h3>
<p>기존의 모듈러 비전 시스템은 “모든 것을 완벽하게 인식“하려 노력했다. 컵을 잡는 작업에서도 컵의 무늬, 탁자의 색깔, 뒤에 있는 벽지 등을 모두 처리하느라 연산 자원을 소모했다.</p>
<p>반면, 엔드투엔드 정책은 **목적 지향적(Goal-Driven)**이다. 신경망은 오직 ’성공적인 토크 생성’에 기여하는 시각적 특징에만 가중치를 부여한다. 컵을 잡는 데 컵의 손잡이 위치가 중요하다면 그것에 집중하고, 컵의 색깔이 중요하지 않다면 무시하는 법을 배운다. 즉, 로봇의 ’눈’이 ’손’의 목적에 종속되어 최적화되는 것이다.17 이는 생물학적 진화 과정에서 동물의 시각 시스템이 생존과 행동에 필요한 정보만을 선택적으로 처리하도록 발달한 것과 일맥상통한다.</p>
<h3>4.3  암묵적 지식(Tacit Knowledge)의 획득</h3>
<p>마이클 폴라니(Michael Polanyi)가 말했듯, “우리는 말할 수 있는 것보다 더 많이 알고 있다.” 자전거를 타는 법이나 달걀을 깨지 않게 쥐는 법은 수식이나 언어로 명확히 기술하기 어렵다. 이를 암묵적 지식이라 한다.</p>
<p>기존의 로봇 프로그래밍은 모든 것을 명시적 규칙(Explicit Rules)으로 기술해야 했기에 한계가 있었다. 그러나 엔드투엔드 학습은 데이터(성공/실패의 경험)를 통해 이러한 암묵적 노하우를 신경망의 가중치(Weights) 속에 분포된 형태로 저장한다. 병뚜껑을 돌릴 때 느껴지는 미세한 저항감에 어떻게 반응해야 하는지, 로봇은 텍스트가 아닌 감각과 행동의 연결 고리로 기억하게 된다.</p>
<h2>5.  빛과 그림자: 해결되지 않은 도전과제들</h2>
<p>엔드투엔드 접근법은 로봇 학습(Robot Learning)이라는 분야를 폭발적으로 성장시켰지만, 동시에 ’블랙박스’가 가진 본질적인 위험성과 한계도 드러냈다.</p>
<h3>5.1  인과적 혼동(Causal Confusion)과 안전 문제</h3>
<p>신경망은 데이터 간의 상관관계(Correlation)를 학습할 뿐, 인과관계(Causality)를 이해하지 못한다. 이로 인해 **인과적 혼동(Causal Confusion)**이라는 현상이 발생한다.27</p>
<p>예를 들어, 자율주행 데이터에서 “브레이크 등(Brake light)이 켜짐“과 “차가 멈춤“이 항상 같이 나타난다면, 에이전트는 “앞에 장애물이 있어서 멈추는 것“이라는 인과관계를 배우는 대신, “대시보드에 브레이크 표시등이 켜지면 멈춰야 한다“는 잘못된 규칙을 학습할 수 있다. 이는 모듈러 방식에서는 발생하지 않는 오류 유형이다.</p>
<p>또한, 로봇이 실패했을 때 “왜 실패했는지” 알 수 없다는 점(Lack of Interpretability)은 실제 산업 현장 도입의 가장 큰 걸림돌이다. 모듈러 방식에서는 “인식 모듈이 컵을 10cm 옆에 있는 것으로 착각했다“고 진단할 수 있지만, 엔드투엔드 신경망에서는 “수만 개의 뉴런 중 일부의 활성화 패턴이 이상했다“는 식의 무의미한 분석만 가능하다.31</p>
<h3>5.2  데이터 효율성(Sample Efficiency)과 일반화(Generalization)</h3>
<p>엔드투엔드 모델은 수만 개 이상의 파라미터를 학습해야 하므로 막대한 양의 데이터가 필요하다. 시뮬레이션에서 수억 번의 훈련을 거치는 것은 가능하지만, 물리적인 로봇을 그만큼 구동하는 것은 하드웨어의 마모와 시간 비용 때문에 불가능에 가깝다.20</p>
<p>또한, 훈련 데이터 분포에서 조금만 벗어나도 성능이 급격히 저하되는 일반화 문제도 심각하다. 특정 조명 조건과 배경에서 학습된 모델은 책상보 색깔만 바뀌어도 컵을 인식하지 못할 수 있다.34 이를 해결하기 위해 도메인 랜덤화(Domain Randomization)와 같은 기법이 연구되고 있지만, 여전히 완전한 해결책은 요원하다.</p>
<h3>5.3  표: 기술적 접근법의 진화 비교</h3>
<p>아래 표는 비주얼 서보잉부터 최신 엔드투엔드 학습까지 로봇 비주얼 제어 기술의 발전 단계를 요약한다.</p>
<table><thead><tr><th><strong>특징</strong></th><th><strong>비주얼 서보잉 (Visual Servoing)</strong></th><th><strong>초기 신경망 제어 (ALVINN, 1989)</strong></th><th><strong>딥 비주얼 모터 정책 (Levine et al., 2015)</strong></th><th><strong>최신 동향 (VLA, Diffusion Policy)</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 기술</strong></td><td>제어 이론, 특징 추적 (Feature Tracking)</td><td>얕은 신경망 (MLP), 지도 학습</td><td>심층 CNN, 유도된 정책 탐색 (GPS)</td><td>트랜스포머 (Transformers), 생성형 모델</td></tr>
<tr><td><strong>입력 형태</strong></td><td>수동으로 지정한 특징점 (Hand-crafted Features)</td><td>저해상도 이미지 (30x32)</td><td>고해상도 RGB 이미지, 관절 상태</td><td>이미지, 텍스트, 비디오 등 멀티모달</td></tr>
<tr><td><strong>특징 추출</strong></td><td>인간 엔지니어가 설계</td><td>신경망이 일부 학습 (Edge Detection 등)</td><td>심층 특징 학습 (Deep Feature Learning)</td><td>대규모 데이터 사전 학습 (Pre-training)</td></tr>
<tr><td><strong>제어 출력</strong></td><td>속도 (Velocity) 명령</td><td>조향 곡률 (Curvature)</td><td><strong>관절 토크 (Torque)</strong></td><td>위치/속도/토크의 시퀀스 (Action Chunking)</td></tr>
<tr><td><strong>주요 한계</strong></td><td>특징점 지정 필요, 보정(Calibration) 민감</td><td>제한된 용량, 간단한 주행만 가능</td><td>높은 데이터 요구량, 일반화 부족</td><td>막대한 계산 비용, 추론 속도</td></tr>
</tbody></table>
<h2>6.  결론: 통합된 신체를 향한 여정</h2>
<p>“픽셀에서 토크까지“라는 슬로건은 로봇 공학을 기계 제어의 영역에서 인공지능 학습의 영역으로 확장시킨 결정적인 계기였다. 엔드투엔드 접근법은 로봇의 신체와 두뇌를 분리할 수 없는 하나의 유기체로 통합하려는 시도였으며, 이는 **체화된 인공지능(Embodied AI)**이라는 거대한 연구 흐름의 시발점이 되었다.</p>
<p>물론 2020년대 현재, 순수한 엔드투엔드 방식만이 유일한 정답은 아니다. 최근에는 거대 언어 모델(LLM)이나 비전-언어 모델(VLM)이 상위 수준의 추론과 계획을 담당하고, 엔드투엔드 정책이 하위 수준의 기민한 동작을 담당하는 계층적(Hierarchical) 구조가 새로운 표준으로 떠오르고 있다.35 그러나 “인지와 행동은 분리될 수 없다“는 엔드투엔드의 핵심 철학은 여전히 유효하다. 로봇은 이제 프로그래머가 입력한 코드대로 움직이는 기계를 넘어, 스스로 세상을 보고, 느끼고, 부딪히며 그 경험을 자신의 신경망에 새기는 학습하는 존재로 진화하고 있다. 픽셀에서 토크로 흐르는 정보의 고속도로 위에서, 로봇은 비로소 진정한 의미의 ’행동’을 배우기 시작한 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Sense-Think-Act Cycle in Real-Time Autonomous AI Agents - Aciano Technologies, https://aciano.net/blog/autonomous-ai-sense-think-act/</li>
<li>Integrating Open-Ended Learning in the Sense-Plan-Act Robot Control Paradigm, https://www.goal-robots.eu/wp-content/uploads/2021/06/2020_ODDI_ECAI_2020.pdf</li>
<li>SystemsEngineering - Intelligent Motion Lab, https://motion.cs.illinois.edu/RoboticSystems/SystemsEngineering.html</li>
<li>The State of Robot Learning. A partially observed, semi-stochastic… | by Vincent Vanhoucke, https://vanhoucke.medium.com/the-state-of-robot-learning-639dafffbcf8</li>
<li>End-Effector Force and Joint Torque Estimation of a 7-DoF Robotic Manipulator Using Deep Learning - MDPI, https://www.mdpi.com/2079-9292/10/23/2963</li>
<li>ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge - arXiv, https://arxiv.org/html/2512.20276v1</li>
<li>An Algorithmic Information-Theoretic Perspective on the Symbol Grounding Problem - arXiv, https://www.arxiv.org/abs/2510.05153</li>
<li>Symbol grounding problem - Wikipedia, https://en.wikipedia.org/wiki/Symbol_grounding_problem</li>
<li>Approaching the Symbol Grounding Problem with Probabilistic Graphical Models | AI Magazine - AAAI Publications, https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2384</li>
<li>The Difficulties in Symbol Grounding Problem and the Direction for Solving It - MDPI, https://www.mdpi.com/2409-9287/7/5/108</li>
<li>Planning with Uncertainty in Position: an Optimal Planner - Carnegie Mellon University Robotics Institute, https://www.ri.cmu.edu/pub_files/pub4/gonzalez_juan_pablo_2004_1/gonzalez_juan_pablo_2004_1.pdf</li>
<li>Error Propagation on the Euclidean Group With Applications to Manipulator Kinematics - Johns Hopkins University, https://rpk.lcsr.jhu.edu/wp-content/uploads/2014/08/Wang06.pdf</li>
<li>Efficient Training of Artificial Neural Networks for Autonomous Navigation - PubMed, https://pubmed.ncbi.nlm.nih.gov/31141866/</li>
<li>ALVINN: An Autonomous Land Vehicle in a Neural Network, https://papers.neurips.cc/paper_files/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf</li>
<li>arXiv:1912.05440v1 [cs.CV] 11 Dec 2019, https://arxiv.org/pdf/1912.05440</li>
<li>A Brief History of Deep Learning - Dataversity, https://www.dataversity.net/articles/brief-history-deep-learning/</li>
<li>End-to-End Training of Deep Visuomotor Policies - CCC, https://cra.org/ccc/great-innovative-idea-end-to-end-training-of-deep-visuomotor-policies/</li>
<li>End-to-End Training of Deep Visuomotor Policies - UC Berkeley Robot Learning Lab, https://rll.berkeley.edu/RSS2015-BlueSky-Shakey/Levine-ShakeyWS-2015.pdf</li>
<li>[1504.00702] End-to-End Training of Deep Visuomotor Policies - arXiv, https://arxiv.org/abs/1504.00702</li>
<li>Learning Fast and Precise Pixel-to-Torque Control - IEEE Xplore, https://ieeexplore.ieee.org/iel7/100/9794949/09675140.pdf</li>
<li>From Pixels to Torques: Policy Learning with Deep Dynamical Models - DiVA portal, https://www.diva-portal.org/smash/get/diva2:866120/FULLTEXT01.pdf</li>
<li>[1502.02251] From Pixels to Torques: Policy Learning with Deep Dynamical Models - arXiv, https://arxiv.org/abs/1502.02251</li>
<li>Ch. 2 - Let’s get you a robot - Robotic Manipulation, https://manipulation.csail.mit.edu/robot.html</li>
<li>End-to-End Training of Deep Visuomotor Policies - Journal of Machine Learning Research, https://www.jmlr.org/papers/volume17/15-522/15-522.pdf</li>
<li>End-to-End Training of Deep Visuomotor Policies - IAS TU Darmstadt, https://www.ias.informatik.tu-darmstadt.de/uploads/Workshops/RSS2015VisualTactileInteraction/Levine-RSS-vtl-2015.pdf</li>
<li>End-to-End Training of Deep Visuomotor Policies - Semantic Scholar, https://pdfs.semanticscholar.org/038d/09889d7d388a8e58b1deff0dc1783228e976.pdf</li>
<li>Imitating with Sequential Masks: Alleviating Causal Confusion in Autonomous Driving - J-Stage, https://www.jstage.jst.go.jp/article/jaciii/28/4/28_882/_article/-char/en</li>
<li>Exploring the Causality of End-to-End Autonomous Driving - arXiv, https://arxiv.org/html/2407.06546v1</li>
<li>CIVIL: Causal and Intuitive Visual Imitation Learning - arXiv, https://arxiv.org/html/2504.17959v1</li>
<li>Object-Aware Regularization for Addressing Causal Confusion in Imitation Learning, https://proceedings.neurips.cc/paper_files/paper/2021/file/17a3120e4e5fbdc3cb5b5f946809b06a-Paper.pdf</li>
<li>Enhancing interpretability and accuracy of AI models in healthcare: a comprehensive review on challenges and future directions - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC11638409/</li>
<li>End-to-end Autonomous Driving: Challenges and Frontiers - arXiv, https://arxiv.org/html/2306.16927v3</li>
<li>Causal Robot Learning for Manipulation, https://www.ri.cmu.edu/app/uploads/2024/07/tabitha-edith-lee-phd-thesis-causal-robot-learning-for-manipulation.pdf</li>
<li>ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation - arXiv, https://arxiv.org/html/2509.20841v1</li>
<li>Foundation Model Driven Robotics: A Comprehensive Review - arXiv, https://arxiv.org/html/2507.10087v1</li>
<li>RoboHorizon: An LLM-Assisted Multi-View World Model for Long-Horizon Robotic Manipulation - arXiv, https://arxiv.org/html/2501.06605v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>