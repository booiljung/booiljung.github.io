<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Chapter 1. Embodied AI의 태동: 뇌를 가진 신체</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Chapter 1. Embodied AI의 태동: 뇌를 가진 신체</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">제목: Embodied AI & Modern Control</a> / <a href="index.html">Chapter 1. Embodied AI의 태동: 뇌를 가진 신체</a> / <span>Chapter 1. Embodied AI의 태동: 뇌를 가진 신체</span></nav>
                </div>
            </header>
            <article>
                <h1>Chapter 1. Embodied AI의 태동: 뇌를 가진 신체</h1>
<h2>1.  서론: 지능의 코페르니쿠스적 전환</h2>
<p>인공지능(Artificial Intelligence, AI)의 역사는 수십 년간 ’지능’을 순수한 연산과 기호 조작의 과정으로 정의해 온 이원론적 사고에 지배되어 왔다. 1956년 다트머스 회의 이후, 초기 AI 연구자들은 인간의 지적 능력을 체스 게임이나 수학 정리 증명과 같은 고도로 추상화된 논리적 과제 해결 능력과 동일시했다. 그들은 “기계가 생각할 수 있는가?“라는 질문에 답하기 위해 논리적 추론 엔진을 구축하는 데 집중했으며, 신체는 단지 뇌의 명령을 수행하는 수동적인 말단 장치(peripherals)에 불과하다고 여겼다. 그러나 이러한 접근 방식, 즉 ’신체 없는 지능(Disembodied Intelligence)’은 현실 세계의 복잡성과 불확실성이라는 거대한 벽 앞에서 한계를 드러냈다. 이제 우리는 지능이 단순히 뇌 내부의 정보 처리가 아니라, 물리적 신체를 통해 환경과 끊임없이 상호작용하는 과정에서 창발(emergence)한다는 사실을 깨닫고 있다. 이것이 바로 ’Embodied AI(체화된 인공지능)’의 태동이며, 본 장에서는 뇌를 가진 신체로서의 AI가 가지는 철학적, 기술적, 역사적 의미를 심도 있게 탐구한다.</p>
<h3>1.1  ’통 속의 뇌’에서 ’세상 속의 에이전트’로</h3>
<p>우리가 흔히 접하는 현대의 AI, 특히 2010년대 이후 급부상한 딥러닝 기반의 이미지 인식 모델이나 거대 언어 모델(LLM)은 ’인터넷 AI(Internet AI)’로 분류될 수 있다.1 인터넷 AI는 웹에서 수집된 수십억 장의 정적인 이미지(ImageNet, COCO 등)나 텍스트 데이터를 학습하여 패턴을 인식하고 분류하는 데 초인적인 성능을 보인다. 그러나 이러한 모델들은 본질적으로 데카르트적 이원론에 갇힌 “통 속의 뇌(Brain in a Jar)“와 같다.2</p>
<p>인터넷 AI는 자신이 처리하는 데이터가 현실 세계에서 어떤 물리적 속성을 가지는지 체감하지 못한다. 예를 들어, LLM은 “사과를 깎는 법“에 대해 매우 상세한 텍스트를 생성할 수 있지만, 실제로 칼을 쥐었을 때의 그립감, 사과 껍질의 저항감, 칼날이 미끄러질 때의 위험을 전혀 ’이해’하지 못한다. 그들에게 사과는 픽셀의 집합이거나 텍스트 토큰의 나열일 뿐, 물리적 실체가 아니다.1 이러한 한계는 ‘환각(Hallucination)’ 현상을 야기하거나, 실제 물리적 세계에서의 인과관계 추론 실패로 이어진다.</p>
<p>반면, Embodied AI는 에이전트(Agent)가 가상 시뮬레이터 또는 실제 물리적 환경 내에서 능동적으로 행동(Act)하고, 그 행동의 결과를 다시 감각(Sense) 입력으로 받아들이는 폐루프(closed-loop) 시스템을 지향한다.1 이는 지능이 환경과의 감각-운동 결합(sensorimotor coupling)을 통해 형성된다는 인지과학의 ‘체화된 인지(Embodied Cognition)’ 이론을 공학적으로 구현하려는 시도이다.3 갓 태어난 아기가 세상을 배우는 방식을 상기해 보십시오. 아기는 정적인 그림 카드를 수동적으로 바라보며 물리 법칙을 배우지 않는다. 손을 뻗어 물건을 잡고, 입에 넣어 맛을 보고, 떨어뜨려 소리를 들으며 자신의 행동이 환경에 미치는 영향을 능동적으로 탐색한다.3 Embodied AI는 이러한 학습 방식을 인공지능에 적용하여, 정적인 패턴 인식 기계를 넘어 동적인 행위자로서의 진화를 목표로 한다.</p>
<h3>1.2  로봇 공학의 캄브리아기 대폭발</h3>
<p>지구 역사상 약 5억 4천만 년 전, 캄브리아기 대폭발(Cambrian Explosion)이라 불리는 생명체의 급격한 다양성 증가가 발생했다. 많은 진화생물학자는 이 사건의 주요 원인 중 하나로 ’눈(Eye)’의 탄생과 이동 능력의 발달을 꼽는다. 환경을 시각적으로 감지하고, 포식자를 피하거나 먹이를 쫓아 능동적으로 움직일 수 있는 능력은 생존 경쟁과 진화의 핵심 동력이었다.</p>
<p>현재 로봇 공학 및 AI 분야에서도 이와 유사한 현상이 목격되고 있다.6 과거의 로봇이 사전에 엄격하게 프로그래밍된 규칙에 따라 제한된 동작만을 반복 수행하는 산업용 기계였다면, 현재의 로봇은 딥러닝(Deep Learning)과 강화학습(Reinforcement Learning)이라는 새로운 신경계를 장착하고 시각적 인식 능력과 학습 능력을 갖추기 시작했다. 엔비디아(NVIDIA)의 젠슨 황(Jensen Huang)을 비롯한 AI 선구자들은 우리가 현재 ‘로봇 공학의 캄브리아기 대폭발’ 시기에 진입했다고 평가한다.6 이는 단순한 기술적 진보를 넘어, 기계가 물리적 세계를 이해하고 조작하는 방식의 근본적인 변화를 의미한다. 저렴해진 센서, 강력한 GPU 연산 능력, 그리고 방대한 데이터를 처리할 수 있는 신경망 아키텍처의 결합은 기계가 수동적인 도구에서 자율적인 존재로 거듭나는 진화적 도약의 발판을 마련했다.</p>
<h2>2.  철학적 난제와 이론적 토대: 왜 로봇은 어려운가?</h2>
<p>인간에게는 너무나 쉬운 걷기나 물건 집기 같은 행동이 로봇에게는 왜 그토록 어려운 일까? Embodied AI의 필요성과 난이도를 이해하기 위해서는 이 질문에 대한 철학적, 이론적 배경을 살펴볼 필요가 있다. 이는 기술적 문제 해결을 넘어 지능의 본질에 대한 깊은 통찰을 제공한다.</p>
<h3>2.1  모라벡의 역설 (Moravec’s Paradox): 진화의 시간표</h3>
<p>1980년대 한스 모라벡(Hans Moravec), 로드니 브룩스(Rodney Brooks), 마빈 민스키(Marvin Minsky) 등이 제기한 ’모라벡의 역설’은 초기 AI 연구의 낙관론을 무너뜨린 중요한 통찰이다. 이 역설의 핵심은 “성인 수준의 체스 게임이나 수학적 증명과 같은 고도의 추론 능력은 컴퓨터로 구현하기 쉽지만, 한 살짜리 아기 수준의 지각이나 운동 능력은 구현하기가 극도로 어렵다“는 것이다.9</p>
<p>이 역설은 진화론적 관점에서 명쾌하게 설명된다. 인류의 진화 역사를 1년으로 환산한다면, 감각 운동 기술(sensorimotor skills)은 1년 내내 생존을 위해 갈고 닦아온 기능이다. 우리는 수억 년에 걸쳐 시각 정보를 처리하고, 울퉁불퉁한 지형에서 균형을 잡으며, 복잡한 물체를 조작하는 능력을 최적화했다. 따라서 인간의 뇌는 이러한 처리에 막대한 자원을 할애하도록 설계되어 있으며, 우리는 이를 ’무의식적’이고 ‘쉬운’ 것으로 느낀다.10 반면, 추상적인 논리, 수학, 언어와 같은 능력은 진화의 역사에서 불과 몇 분 전에 등장한 최신 기능이다. 우리는 이러한 작업을 수행할 때 의식적인 노력이 필요하고 어렵다고 느끼지만, 컴퓨터에게는 이것이 단순히 명시적인 규칙을 따르는 연산 과정일 뿐이기에 오히려 쉽다.</p>
<p>모라벡은 인간의 망막이 수행하는 이미지 처리 연산량만 해도 당시 슈퍼컴퓨터의 능력을 상회한다고 지적했다. 에포크 AI(Epoch AI)의 분석에 따르면, 뇌의 학습에 필요한 연산량은 생애 동안 약 <span class="math math-inline">10^{23}</span>에서 <span class="math math-inline">10^{25}</span> FLOPs에 달할 것으로 추정되며, 이는 단순히 가중치를 최적화하는 것을 넘어 진화 과정 전체를 최적화 프로세스로 보아야 함을 시사한다.11 결국, Embodied AI 연구는 인류가 수억 년 동안 해결해 온 진화적 과제를 압축적으로 재현하려는 시도이며, 이것이 바로 로봇 공학이 그토록 어려운 이유다.</p>
<h3>2.2  기호 접지 문제 (The Symbol Grounding Problem): 의미의 부재</h3>
<p>Embodied AI가 해결해야 할 또 다른 핵심 과제는 인지과학자 스티븐 하나드(Stevan Harnad)가 1990년에 정식화한 ’기호 접지 문제(Symbol Grounding Problem)’이다.12 이는 “컴퓨터 시스템 내부의 기호(symbol)가 어떻게 현실 세계의 의미(meaning)와 연결될 수 있는가?“에 대한 질문이다.</p>
<p>전통적인 AI 시스템에서 ’사과’라는 단어는 시스템 내부에서 ‘과일이다’, ‘빨갛다’, ’먹을 수 있다’와 같은 다른 기호들과의 관계(syntax)로 정의될 뿐이다. 하지만 그 기호들이 가리키는 실제 물리적 대상(semantics)과는 아무런 연결 고리가 없다. 이는 존 서을(John Searle)의 ‘중국어 방(Chinese Room)’ 논변과 직결된다. 중국어를 전혀 모르는 사람이 방 안에 갇혀서, 완벽한 중국어 규칙서(프로그램)에 따라 중국어 질문에 대한 답을 조합하여 내보낸다면, 밖에서 보기에는 그 사람이 중국어를 이해하는 것처럼 보일 것이다. 그러나 실제로 그 사람은 기호의 형태만 조작할 뿐, 그 기호가 무엇을 의미하는지 전혀 모른다.12</p>
<p>Embodied AI는 이 문제를 해결할 수 있는 유일한 열쇠로 간주된다. 로봇이 카메라(센서)를 통해 사과를 ‘보고’, 로봇 팔(액추에이터)을 뻗어 사과를 ‘잡고’, 무게와 질감을 ‘느끼는’ 물리적 상호작용을 수행할 때, 비로소 ’사과’라는 기호는 물리적 실체에 ’접지(grounded)’된다.15 최근 연구에 따르면, 이러한 감각-운동 경험의 부재가 현재 거대 언어 모델(LLM)이 겪는 환각 현상의 근본 원인 중 하나일 수 있으며, 언어 모델을 로봇의 신체에 연결함으로써 진정한 의미 이해에 도달할 수 있다는 주장이 힘을 얻고 있다.2</p>
<h3>2.3  감각 운동 부수성 이론 (Sensorimotor Contingency Theory): 지각은 행위다</h3>
<p>오래건(O’Regan)과 노에(Noë)가 2001년 제안한 ’감각 운동 부수성 이론(Sensorimotor Contingency Theory)’은 지각(Perception)을 수동적인 입력 처리가 아닌, 능동적인 행위의 일종으로 재정의한다.17 이 이론에 따르면, 본다는 것은 눈앞의 장면을 사진처럼 찍어 뇌 속에 내부 모델(internal representation)을 만드는 과정이 아니다. 대신, 지각은 “내가 눈을 움직이거나 고개를 돌리면(운동), 시각 입력이 어떻게 변할 것인가(감각)?“에 대한 법칙, 즉 ’부수성(Contingency)’을 숙달하는 과정이다.</p>
<p>예를 들어, 우리가 손에 쥔 스펀지의 부드러움을 느낄 때, 그것은 단순히 피부의 압력 센서가 보내는 신호가 아니다. 손가락으로 눌렀을 때(운동) 들어가는 깊이에 따라 변하는 압력의 변화율(감각) 사이의 관계를 파악하는 것이다.19 시각적 의식 또한 마찬가지이다. 우리가 고양이를 볼 때 고양이의 뒷모습을 보지 못하지만, 한 발짝 옆으로 움직이면 뒷모습이 보일 것임을 암묵적으로 알고 있기 때문에 고양이를 입체적인 존재로 인식한다.20</p>
<p>이 이론은 로봇 공학, 특히 능동적 지각(Active Perception) 분야에 심오한 영향을 미쳤습니다. 루제나 바크시(Ruzena Bajcsy) 교수가 주창한 능동적 지각은 “에이전트가 왜(Why) 감지하려는지 알고, 무엇을(What) 감지할지 선택하며, 어떻게(How) 감지할지 결정하는 과정“이다.21 로봇이 환경을 이해하려면 단순히 고해상도 카메라를 장착하는 것만으로는 부족한다. 로봇은 자신의 행동이 감각 입력에 어떤 변화를 가져오는지 끊임없이 탐색하고, 불확실성을 줄이기 위해 능동적으로 시점을 변경하거나 물체를 조작해야 한다.23 이는 정적인 데이터셋을 학습하는 인터넷 AI와 가장 극명하게 대비되는 Embodied AI만의 특징이다.</p>
<h2>3.  패러다임의 충돌: 인터넷 AI vs. 체화된 AI</h2>
<p>인터넷 AI와 Embodied AI의 구분은 단순히 적용 분야의 차이가 아니라, 데이터를 다루는 방식, 학습의 본질, 그리고 지능을 바라보는 관점의 근본적인 차이에서 기인한다.</p>
<h3>3.1  정적 데이터셋 vs. 동적 상호작용 스트림</h3>
<p>인터넷 AI는 기본적으로 ’큐레이션 된 과거’를 학습한다. ImageNet의 사진들은 사진사가 의도와 구도를 가지고 찍은, 잘 정돈되고 중심이 맞춰진(center-biased) 시각 정보인다. 반면, Embodied AI가 마주하는 데이터는 ’날 것의 현재’이자 ’동적인 스트림’이다.1 로봇이 이동하며 획득하는 시각 정보는 끊임없이 흔들리고(motion blur), 물체에 의해 가려지며(occlusion), 조명 조건이 급격히 변한다. 무엇보다 가장 큰 차이는 에이전트의 행동이 다음 데이터에 직접적인 영향을 미친다는 점이다(Egocentric Perception). 로봇이 고개를 돌리면 시야가 바뀌고, 물건을 밀면 물건의 위치가 바뀝니다. 이러한 데이터의 비정상성(non-stationarity)은 기존의 지도 학습(Supervised Learning) 방법론을 그대로 적용하기 어렵게 만든다.</p>
<table><thead><tr><th><strong>특징 비교</strong></th><th><strong>인터넷 AI (Internet AI)</strong></th><th><strong>체화된 AI (Embodied AI)</strong></th></tr></thead><tbody>
<tr><td><strong>데이터 원천</strong></td><td>정적 데이터셋 (이미지, 텍스트, 비디오)</td><td>환경과의 동적 상호작용에서 발생하는 실시간 센서 데이터 스트림</td></tr>
<tr><td><strong>지각 관점</strong></td><td>3인칭 관점 (Observer, Disembodied)</td><td>1인칭 관점 (Agent, Egocentric, Embodied)</td></tr>
<tr><td><strong>학습 목표</strong></td><td>패턴 인식, 분류, 생성, 번역</td><td>행동 결정, 물리적 조작, 생존, 목표 달성</td></tr>
<tr><td><strong>상호작용성</strong></td><td>수동적 (데이터를 ‘봄’)</td><td>능동적 (환경을 ‘변화시킴’, Causality 발생)</td></tr>
<tr><td><strong>검증 환경</strong></td><td>정적 벤치마크 (Test Set, IID 가정)</td><td>물리적 시뮬레이터 또는 현실 세계 (Dynamic, Non-IID)</td></tr>
<tr><td><strong>주요 한계</strong></td><td>환각, 물리적 상식 부족, 접지 실패</td><td>높은 데이터 수집 비용, Reality Gap, 안전 문제</td></tr>
</tbody></table>
<p>[표 1. 인터넷 AI와 체화된 AI의 심층 비교 요약 1]</p>
<h3>3.2  상관관계(Correlation)를 넘어 인과관계(Causality)로</h3>
<p>인터넷 AI는 데이터 간의 통계적 상관관계를 학습하는 데 탁월한다. 예를 들어, “비가 오면 우산을 쓴다“는 수많은 텍스트와 이미지를 학습한 모델은 ’비’와 ‘우산’ 사이의 강력한 상관관계를 파악한다. 그러나 이 모델은 우산을 펴는 행위가 비를 오게 만들지는 않는다는 인과관계를 명확히 이해하지 못할 수 있다.24</p>
<p>Embodied AI는 물리적 세계에서 직접 행동하며 인과관계를 체득할 수 있는 환경을 제공한다. 로봇이 테이블 위의 컵을 미는 행동(원인)을 했을 때 컵이 바닥으로 떨어져 깨지는 현상(결과)을 관찰하면, 이는 단순한 통계적 연관성을 넘어선 물리적 인과관계로 학습된다. 이러한 ’물리적 상식(Physical Common Sense)’은 AI가 안전하게 현실 세계에서 작동하기 위해 필수적이다. 인터넷 AI가 2D 이미지 속에서 컵이 공중에 떠 있는 초현실적인 그림을 생성할 수 있다면, Embodied AI는 중력이라는 물리 법칙 내에서 컵이 지지대 없이는 떨어질 수밖에 없음을 “몸으로” 배웁니다.2</p>
<h3>3.3  파운데이션 모델의 역할과 한계</h3>
<p>최근에는 LLM과 같은 파운데이션 모델(Foundation Models)을 로봇에 적용하려는 ‘Vision-Language-Action (VLA)’ 모델 연구가 활발한다.2 이러한 모델들은 인터넷 규모의 방대한 지식을 가지고 있어, 로봇에게 “부엌에서 마실 것 좀 가져와“와 같은 추상적이고 고수준의 명령을 이해시키고 계획(Planning)을 수립하는 데 매우 유용한다.</p>
<p>하지만 이 모델들은 여전히 “뇌만 있는(Brains in a jar)” 상태에서 출발했기에 한계가 명확한다.2 그들은 “냉장고 문을 연다“는 개념은 알지만, 실제 냉장고 문을 열기 위해 얼마나 큰 토크(Torque)가 필요한지, 손잡이가 미끄러울 때 어떻게 그립을 조정해야 하는지와 같은 ’암묵적 지식(Tacit Knowledge)’은 가지고 있지 않는다. 따라서 현대의 Embodied AI 연구는 인터넷 AI가 가진 방대한 의미적 지식(Semantic Knowledge)과 로봇이 가진 구체적인 물리적 경험(Physical Experience)을 어떻게 결합할 것인가에 집중하고 있다.26 이는 기호 접지 문제를 해결하고, AI가 진정한 의미에서 세상을 이해하도록 만드는 과정이다.</p>
<h2>4.  방법론의 진화: 고전 제어에서 딥러닝까지</h2>
<p>Embodied AI의 역사는 “로봇을 어떻게 제어할 것인가?“에 대한 방법론의 변천사로 볼 수 있다. 이는 크게 수학적 모델링에 기반한 고전적 제어(Classical Control)에서 데이터에 기반한 학습(Learning)으로의 전환으로 요약된다.</p>
<h3>4.1  고전적 제어와 모델 기반 접근: 정교함과 취약성</h3>
<p>초기 로봇 공학은 정교한 수학적 모델링에 기반했다. 연구자들은 로봇의 링크와 관절에 대한 운동학(kinematics) 및 동역학(dynamics) 방정식을 직접 유도하고, 환경의 지도를 미리 작성한 뒤, 최적의 경로를 계산하는 알고리즘을 사용했다.29 PID 제어기, 모델 예측 제어(MPC), 그리고 SLAM(Simultaneous Localization and Mapping)과 같은 기술들이 이 시기를 대표한다.</p>
<p>이러한 접근 방식은 통제된 환경(예: 자동차 공장의 조립 라인)에서는 매우 효과적이고 정밀했다. 그러나 ’모델링 오차’나 예측하지 못한 ’외란(disturbance)’에 매우 취약하다는 치명적인 단점이 있었다. 복잡한 가정 환경이나 야외와 같은 비정형 환경을 완벽하게 수학적으로 모델링하는 것은 불가능에 가까웠기 때문이다. 이 시기의 AI는 주로 ’인지-계획-실행(Sense-Plan-Act)’이라는 순차적 파이프라인을 따랐는데, ‘인지’ 단계에서 완벽한 세계 모델을 만드는 것이 병목이 되어 전체 시스템의 실패로 이어지곤 했다.29</p>
<h3>4.2  학습 기반 접근의 부상: 데이터가 모델을 대체하다</h3>
<p>2010년대 이후 딥러닝의 부상과 함께 로봇 공학에도 혁명이 일어났습니다. 연구자들은 복잡한 수식을 사람이 직접 설계하는 대신, 방대한 데이터를 신경망에 통과시켜 로봇의 행동 정책(Policy)을 직접 학습시키는 방법을 모색하기 시작했다.31</p>
<ol>
<li><strong>딥 강화학습 (Deep Reinforcement Learning, DRL):</strong> 2015년경 DeepMind가 Atari 게임과 바둑(AlphaGo)에서 보여준 DRL의 성공은 로봇 제어에도 적용되었다. 로봇은 시행착오(trial and error)를 통해 보상을 최대화하는 행동을 스스로 찾아냅니다. 이는 사전에 수학적으로 정의하기 힘든 복잡한 동작(예: 물체의 불규칙한 파지, 험지 보행)을 학습하는 데 효과적이었다.29</li>
<li><strong>모방 학습 (Imitation Learning):</strong> 인간의 시연(demonstration) 데이터를 통해 로봇을 학습시키는 방법이다. 이는 탐색 비용이 높고 안전 문제가 있는 강화학습의 단점을 보완하며, 인간의 노하우를 로봇에게 효율적으로 전수하는 방법으로 자리 잡았다.34</li>
</ol>
<p>이러한 흐름은 **‘End-to-End Visuomotor Policy’**라는 개념으로 구체화되었다. 즉, 카메라의 픽셀 입력을 받아 모터의 토크 명령을 직접 출력하는 하나의 거대한 신경망을 학습시키는 것이다. 이는 중간 단계의 특징 추출(feature extraction)이나 상태 추정(state estimation) 과정을 생략함으로써, 사람이 설계한 모델의 편향을 제거하고 데이터 자체에서 최적의 해결책을 찾도록 한다.34</p>
<h2>5.  가상과 현실의 가교: 시뮬레이션과 Sim2Real</h2>
<p>Embodied AI의 급격한 발전은 하드웨어의 발전뿐만 아니라, 가상 환경에서 로봇을 학습시킬 수 있는 고성능 시뮬레이터와 이를 현실로 옮기는 기술(Sim-to-Real)의 발전에 힘입은 바 큽니다.</p>
<h3>5.1  시뮬레이터: 디지털 훈련장 (The Digital Dojo)</h3>
<p>현실 세계에서 로봇을 학습시키는 것은 비용이 많이 들고, 위험하며, 시간이 오래 걸립니다. 로봇이 걷는 법을 배우기 위해 수천 번 넘어져야 한다면, 하드웨어 수리비용과 시간은 감당하기 어려울 것이다. 따라서 연구자들은 물리 법칙이 적용되는 가상 세계, 즉 시뮬레이터를 구축하여 AI 에이전트를 훈련시키기 시작했다.</p>
<table><thead><tr><th><strong>시뮬레이터</strong></th><th><strong>주요 특징 및 강점</strong></th><th><strong>대표 활용 분야</strong></th><th><strong>기반 기술</strong></th></tr></thead><tbody>
<tr><td><strong>Habitat</strong></td><td>초고속 렌더링(10,000+ FPS), 사실적인 3D 환경(Matterport3D 등 활용)</td><td>실내 내비게이션 (PointGoal Nav), 시각 탐색</td><td>C++ / Python, Vulkan</td></tr>
<tr><td><strong>AI2-THOR</strong></td><td>풍부한 객체 상호작용 (열기, 닫기, 켜기, 깨기 등), 물리적 상식 학습</td><td>상호작용적 내비게이션, 가사 도우미 로봇</td><td>Unity 3D</td></tr>
<tr><td><strong>Isaac Gym</strong></td><td>GPU 기반 병렬 물리 시뮬레이션, 수만 개의 에이전트 동시 학습</td><td>보행 로봇 제어, 로봇 팔 조작 (Manipulation)</td><td>NVIDIA PhysX, GPU</td></tr>
<tr><td><strong>VirtualHome</strong></td><td>인간의 가사 활동을 프로그램 형태로 정의, 다중 에이전트 및 인간 행동 모방</td><td>고수준 작업 계획, 인간-로봇 상호작용</td><td>Unity 3D</td></tr>
</tbody></table>
<p>[표 2. 주요 Embodied AI 시뮬레이터 비교 26]</p>
<ul>
<li><strong>Habitat:</strong> Facebook Reality Labs(현 Meta)에서 개발한 Habitat는 속도에 최적화되어 있다. 기존 시뮬레이터들이 그래픽 렌더링 속도 때문에 학습 병목을 겪었던 반면, Habitat는 초당 수천 프레임 이상의 속도로 에이전트를 훈련시킬 수 있어 강화학습의 데이터 효율성을 극대화했다.37</li>
<li><strong>AI2-THOR &amp; VirtualHome:</strong> 단순한 이동을 넘어 물체와의 상호작용에 초점을 맞춘다. 특히 VirtualHome은 “전자레인지에 컵을 넣고 돌린다“와 같은 복잡한 가사 활동을 스크립트 형태로 시뮬레이션하여, 인간의 행동 패턴을 이해하고 모방하는 연구에 활용된다.40</li>
<li><strong>Isaac Gym:</strong> NVIDIA의 Isaac Gym은 물리 엔진 자체를 GPU에서 구동함으로써, 데이터를 CPU와 GPU 사이에서 복사하는 병목을 제거했다. 이를 통해 데스크톱 한 대에서 수만 대의 로봇을 동시에 시뮬레이션하는 것과 같은 효과를 내며 학습 속도를 비약적으로 높였다.26</li>
</ul>
<h3>5.2  Sim-to-Real: 가상에서 현실로의 도약과 Reality Gap</h3>
<p>시뮬레이션에서 완벽하게 작동하는 정책이 현실에서는 실패하는 경우가 많은데, 이를 ’Reality Gap’이라고 한다. 시뮬레이션은 마찰, 공기 저항, 센서 노이즈, 조명 변화 등을 완벽하게 모사할 수 없기 때문이다. 이를 극복하기 위한 핵심 기술이 바로 **‘도메인 무작위화(Domain Randomization)’**이다.</p>
<p>도메인 무작위화의 기본 아이디어는 “충분히 다양한 시뮬레이션 환경은 현실을 포함한다“는 것이다. 시뮬레이션 내에서 바닥의 마찰 계수, 물체의 질량, 조명의 밝기, 카메라의 위치 등을 비현실적일 정도로 다양하게 무작위화하여 로봇을 훈련시킨다. 이렇게 극도로 다양한 환경에 적응한 로봇은 현실 세계를 ’또 하나의 변형된 시뮬레이션 환경’으로 인식하여 강건하게 작동할 수 있다. OpenAI가 로봇 손(Dactyl)으로 루빅스 큐브를 맞추는 프로젝트에서 사용한 **ADR(Automatic Domain Randomization)**은 이 기술의 정점을 보여주었다. 로봇의 성능이 향상됨에 따라 환경의 난이도와 무작위성 범위를 자동으로 넓혀가는 이 방식은 ‘창발적 메타 러닝(Emergent Meta-Learning)’ 효과를 유도하여, 로봇이 갑작스러운 외란(예: 고무장갑을 끼우거나 인형으로 방해함)에도 적응할 수 있게 만들었다.43</p>
<p>또한, **“Learning to Fly by Crashing”**과 같은 연구는 시뮬레이션 대신 드론을 실제로 수천 번 충돌시키며 얻은 ’부정적 데이터(Negative Data)’를 학습에 활용하여, 복잡한 환경에서도 충돌을 회피하는 강건한 정책을 만드는 방법을 제안하기도 했다.35 이는 시뮬레이션과 현실 데이터 수집 사이의 균형을 찾는 또 다른 접근 방식이다.</p>
<h2>6.  주요 연구 사례 및 이정표 (Milestones in Embodied AI)</h2>
<p>Embodied AI 분야는 지난 10년간 몇 가지 기념비적인 연구들을 통해 비약적으로 발전해 왔다. 이 연구들은 각각 제어, 대규모 학습, 그리고 언어 모델과의 결합이라는 측면에서 중요한 변곡점을 만들었다.</p>
<h3>6.1  End-to-End Visuomotor Policies (Levine et al., 2016)</h3>
<p>UC 버클리의 세르게이 레빈(Sergey Levine) 교수 팀은 딥러닝을 이용해 이미지 픽셀로부터 로봇의 관절 토크를 직접 제어하는 ’Visuomotor Policy’를 제안했다.34 이 연구는 복잡한 특징 공학(Feature Engineering) 없이도 로봇이 블록 끼우기, 병뚜껑 닫기, 옷걸이 걸기 등 정교한 조작 작업을 수행할 수 있음을 보여주었다. 특히, <strong>“Guided Policy Search (GPS)”</strong> 방법을 통해, 로봇의 상태를 모두 알고 있는 ‘교사(Teacher)’ 알고리즘(최적 제어)이 비전 기반의 ‘학생(Student)’ 신경망을 가르치는 방식을 사용하여 학습 효율을 극대화했다. 이는 로봇 제어의 패러다임을 ’설계’에서 ’데이터 기반 학습’으로 완전히 전환시킨 시발점이 되었다.</p>
<h3>6.2  QT-Opt: 확장 가능한 강화학습 (Scalable Deep RL)</h3>
<p>구글은 <strong>QT-Opt (Q-function Target Optimization)</strong> 알고리즘을 통해 비전 기반 로봇 조작의 성능과 규모를 한 단계 끌어올렸습니다.45 기존의 연속 제어 강화학습 알고리즘(예: DDPG)이 불안정했던 반면, QT-Opt는 대규모 오프폴리시(Off-policy) 학습을 도입했다. 7대의 로봇이 4개월간 수집한 58만 건의 실제 파지(grasping) 데이터를 학습했으며, 특히 **Cross-Entropy Method (CEM)**을 사용하여 Q-함수를 최대화하는 행동을 찾아내는 방식이 주효했다.</p>
<p>결과는 놀라웠습니다. 훈련 과정에서 한 번도 보지 못한 새로운 물체에 대해서도 96% 이상의 파지 성공률을 달성했다.45 이는 로봇 학습에서도 ’데이터의 규모(Scale)’가 성능을 결정짓는 핵심 요소임을 입증했으며, 강화학습이 실험실을 벗어나 실제 복잡한 환경에서 작동할 수 있는 가능성을 보여주었다.</p>
<h3>6.3  SayCan: 언어 모델을 물리 세계에 접지하다 (2022)</h3>
<p>구글의 SayCan (“Do As I Can, Not As I Say”) 프로젝트는 거대 언어 모델(LLM)의 추론 능력과 로봇의 물리적 수행 능력(Affordance)을 결합한 획기적인 시도이다.48</p>
<p>기존의 LLM은 “커피를 쏟았어“라는 말을 들으면 “청소해라”, “스펀지를 가져와라” 등 그럴듯한 계획을 내놓지만, 로봇이 현재 부엌에 있는지, 스펀지가 어디 있는지 알지 못한다. SayCan은 LLM이 제안하는 행동(Say)의 확률과, 로봇의 가치 함수(Value Function)가 평가한 해당 행동의 실행 성공 확률(Can)을 곱하여 최종 행동을 결정한다.</p>
<p>예를 들어, LLM이 “걸레를 가져와“라고 제안하더라도, 로봇이 현재 위치에서 걸레를 찾거나 가져올 확률(Value)이 낮다면 이를 실행하지 않고, 대신 성공 확률이 높은 “휴지를 가져와“를 선택하는 식이다. 실험 결과, SayCan은 101개의 쿼리에 대해 84%의 계획 성공률과 74%의 실행 성공률을 기록했다.48 이는 LLM의 고질적인 환각 문제를 로봇의 물리적 현실 감각(Grounding)으로 보정한 모범 사례이며, 기호 접지 문제에 대한 실용적인 해결책을 제시했다.</p>
<h3>6.4  RT-1: 로봇을 위한 트랜스포머 (Robotics Transformer)</h3>
<p>RT-1은 자연어 처리 분야를 평정한 트랜스포머(Transformer) 아키텍처를 로봇 제어에 그대로 가져온 ‘Vision-Language-Action (VLA)’ 모델이다.51 RT-1은 이미지와 자연어 명령을 입력으로 받아 로봇의 행동을 불연속적인 토큰(Action Token)으로 변환하여 출력한다.</p>
<p>구글은 13만 건 이상의 에피소드를 포함한 대규모 데이터셋으로 RT-1을 학습시켰습니다. 특히, EDR(Everyday Robot) 데이터뿐만 아니라 다른 로봇(Kuka)의 데이터나 시뮬레이션 데이터까지 혼합하여 학습시켰을 때 성능이 오히려 향상되는 결과를 보여주었다.52 이는 로봇 데이터에도 ’다양성’이 중요하며, 서로 다른 형태(morphology)를 가진 로봇의 데이터도 트랜스포머가 효과적으로 통합할 수 있음을 시사한다. RT-1은 학습하지 않은 새로운 명령이나 물체에 대해서도 일반화된 성능을 보여주며, 로봇 제어 모델도 데이터와 모델의 크기를 키우면 범용적인 능력을 가질 수 있다는 ’Scaling Law’의 가능성을 확인시켜 주었다.</p>
<h3>6.5  월드 모델 (World Models): 꿈꾸는 에이전트</h3>
<p>데이비드 하(David Ha)와 위르겐 슈미트후버(Jürgen Schmidhuber)가 제안한 월드 모델은 에이전트가 환경에 대한 내부 모델(Mental Model)을 학습하는 방법에 초점을 맞춘다.54 에이전트는 VAE(Variational Autoencoder)를 통해 시각 정보를 압축하고, RNN(Recurrent Neural Network)을 통해 자신의 행동에 따라 압축된 환경 정보가 어떻게 변할지를 예측하는 생성 모델(Generative Model)을 학습한다.</p>
<p>가장 흥미로운 점은 에이전트가 실제 환경이 아닌, 이 월드 모델이 생성해낸 ‘꿈(Dream)’ 속에서 가상의 훈련을 수행할 수 있다는 것이다. 이는 인간이 잠을 자며 기억을 정리하거나, 머릿속으로 시뮬레이션을 돌려보며 행동을 계획하는 것과 유사한다. 월드 모델은 샘플 효율성을 극대화할 뿐만 아니라, 로봇이 예기치 못한 상황에 대처하는 능력을 키워주는 강력한 도구로 자리 잡았다.</p>
<h2>7.  결론: AGI를 향한 여정</h2>
<p>Embodied AI는 단순히 로봇을 더 잘 움직이게 하는 기술이 아니다. 그것은 진정한 의미의 지능, 즉 인공 일반 지능(AGI)으로 나아가는 필수적인 관문이다. 인터넷상의 텍스트만 학습한 AI는 결코 ‘사과의 맛’, ‘얼음의 차가움’, 혹은 ’넘어질 때의 아픔’을 진정으로 이해할 수 없다. 오직 물리적 신체를 통해 세상과 부딪히며 배우는 Embodied AI만이 기호와 의미를 연결하고, 물리적 인과관계를 체득하며, 상식적인 추론을 할 수 있는 온전한 지능을 가질 수 있다.3</p>
<p>우리는 지금 시뮬레이터, 파운데이션 모델, 대규모 데이터셋이라는 강력한 도구들이 갖추어진 ’로봇 공학의 캄브리아기’의 초입에 서 있다. 연구의 패러다임은 정적인 인식(Passive Perception)에서 동적인 행동(Active Action)으로 이동했다. 물론 해결해야 할 과제는 여전히 산적해 있다. 긴 시간 지평을 가진 계획 수립(Long-horizon planning), 낯선 환경에서의 강건한 일반화(Generalization), 그리고 인간과의 자연스럽고 안전한 상호작용 등은 여전히 난제로 남아 있다.56 하지만 ’뇌를 가진 신체’를 구현하려는 이 여정은, 기계가 단순한 도구를 넘어 물리적 세계에서 우리와 공존하고 협력하는 동반자로 거듭나는 미래를 여는 열쇠가 될 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Overview of Embodied Artificial Intelligence | by Luis Bermudez | machinevision - Medium, https://medium.com/machinevision/overview-of-embodied-artificial-intelligence-b7f19d18022</li>
<li>Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents via Active Querying - arXiv, https://arxiv.org/html/2512.15776v1</li>
<li>A Call for Embodied AI - arXiv, https://arxiv.org/html/2402.03824v3</li>
<li>(PDF) Frontier Exploration of the Fusion of Embodied Intelligence and Large Language Models - ResearchGate, https://www.researchgate.net/publication/394372026_Frontier_Exploration_of_the_Fusion_of_Embodied_Intelligence_and_Large_Language_Models</li>
<li>Embodied cognition - Wikipedia, https://en.wikipedia.org/wiki/Embodied_cognition</li>
<li>
<ol start="243">
<li>Warfare in the Parallel Cambrian Age - Mad Scientist Laboratory, https://madsciblog.tradoc.army.mil/243-warfare-in-the-parallel-cambrian-age/</li>
</ol>
</li>
<li>A “Cambrian Explosion” in Robotics? | Science and Culture Today, https://scienceandculture.com/2015/09/a_cambrian_expl/</li>
<li>Top Three Trends In Robotics: The Cambrian Explosion Is Happening - Forbes, https://www.forbes.com/councils/forbesbusinesscouncil/2021/09/30/top-three-trends-in-robotics-the-cambrian-explosion-is-happening/</li>
<li>Moravec Paradox: Why Is Robotics Lagging Way Behind AI? - Appventurez, https://www.appventurez.com/blog/moravec-paradox-in-robotics-and-ai</li>
<li>Moravec’s paradox - Wikipedia, <a href="https://en.wikipedia.org/wiki/Moravec&#x27;s_paradox">https://en.wikipedia.org/wiki/Moravec%27s_paradox</a></li>
<li>Moravec’s paradox and its implications - Epoch AI, https://epoch.ai/gradient-updates/moravec-s-paradox</li>
<li>The Difficulties in Symbol Grounding Problem and the Direction for Solving It - MDPI, https://www.mdpi.com/2409-9287/7/5/108</li>
<li>Toward a General Solution to the Symbol Grounding Problem: Combining Learning and Computer Vision, https://cdn.aaai.org/Symposia/Fall/1993/FS-93-04/FS93-04-033.pdf</li>
<li>Which symbol grounding problem should we try to solve? - arXiv, https://arxiv.org/pdf/2507.21080</li>
<li>Symbol ungrounding: what the successes (and failures) of large language models reveal about human cognition - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC11529626/</li>
<li>Symbol Grounding: A Bridge from Artificial Life to Artificial Intelligence - Evan Thompson, https://evanthompson.me/wp-content/uploads/2012/11/thompson-symbolgrounding.pdf</li>
<li>Socializing Sensorimotor Contingencies - Frontiers, https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2021.624610/full</li>
<li>A sensorimotor account of vision and visual consciousness - J. Kevin O’Regan, http://nivea.psycho.univ-paris5.fr/OREGAN-NOE-BBS/ORegan;Noe.BBS.pdf</li>
<li>Using enactive robotics to think outside of the problem-solving box: How sensorimotor contingencies constrain the forms of emergent autononomous habits, https://pmc.ncbi.nlm.nih.gov/articles/PMC9810814/</li>
<li>On the Brain-Basis of Visual Consciousness: A Sensorimotor Account, <a href="http://ondrej.becev.cz/upload//No%C3%AB-and-J.-Kevin-O%E2%80%99Regan-On-the-Brain-Basis-of-Visual-Consciousness.pdf">http://ondrej.becev.cz/upload//No%C3%AB-and-J.-Kevin-O%E2%80%99Regan-On-the-Brain-Basis-of-Visual-Consciousness.pdf</a></li>
<li>Revisiting active perception - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC6954017/</li>
<li>Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding, https://arxiv.org/html/2512.05774v1</li>
<li>Editorial: Active Vision and Perception in Human-Robot Collaboration - Frontiers, https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2022.848065/full</li>
<li>AI Embodiment Through 6G: Shaping the Future of AGI - ResearchGate, https://www.researchgate.net/publication/379415224_AI_Embodiment_Through_6G_Shaping_the_Future_of_AGI</li>
<li>A Survey of Embodied AI: From Simulators to Research Tasks - arXiv, https://arxiv.org/pdf/2103.04918</li>
<li>Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI - arXiv, https://arxiv.org/html/2407.06886v1</li>
<li>A Survey on Vision-Language-Action Models for Embodied AI - arXiv, https://arxiv.org/html/2405.14093v5</li>
<li>RT-1: Robotics Transformer for Real-World Control at Scale - ResearchGate, https://www.researchgate.net/publication/372803068_RT-1_Robotics_Transformer_for_Real-World_Control_at_Scale</li>
<li>adam-maj/robotics: A deep dive on the history of robotics and the future of humanoids - GitHub, https://github.com/adam-maj/robotics</li>
<li>Reinforcement Learning in Robotics: A Survey, https://www.ri.cmu.edu/pub_files/2013/7/Kober_IJRR_2013.pdf</li>
<li>AI Watch Historical Evolution of Artificial Intelligence - JRC Publications Repository, https://publications.jrc.ec.europa.eu/repository/bitstream/JRC120469/jrc120469_historical_evolution_of_ai-v1.1.pdf</li>
<li>From Control Loops to Cognitive Systems: How AI Is Reprogramming the Future of Robotics | by Aarav Bedi | Oct, 2025 | Medium, https://medium.com/@aaravbedi/from-control-loops-to-cognitive-systems-how-ai-is-reprogramming-the-future-of-robotics-aa6b85ecceba</li>
<li>Reinforcement Learning with Foundation Priors: Let the Embodied Agent Efficiently Learn on Its Own - arXiv, https://arxiv.org/html/2310.02635v4</li>
<li>End-to-End Training of Deep Visuomotor Policies - ResearchGate, https://www.researchgate.net/publication/274572264_End-to-End_Training_of_Deep_Visuomotor_Policies</li>
<li>Learning to Fly by Crashing | Request PDF - ResearchGate, https://www.researchgate.net/publication/316270191_Learning_to_Fly_by_Crashing</li>
<li>End-to-End Training of Deep Visuomotor Policies - IAS TU Darmstadt, https://www.ias.informatik.tu-darmstadt.de/uploads/Workshops/RSS2015VisualTactileInteraction/Levine-RSS-vtl-2015.pdf</li>
<li>A Brief History of Embodied Artificial Intelligence, and its Outlook, https://cacm.acm.org/blogcacm/a-brief-history-of-embodied-artificial-intelligence-and-its-future-outlook/</li>
<li>ROS-X-Habitat: Bridging the ROS Ecosystem with Embodied AI - UBC Library Open Collections, https://open.library.ubc.ca/media/stream/pdf/24/1.0412641/3</li>
<li>Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI, https://arxiv.org/html/2407.06886v6</li>
<li>VirtualHome: Simulating Household Activities Via Programs | Request PDF - ResearchGate, https://www.researchgate.net/publication/341070802_VirtualHome_Simulating_Household_Activities_Via_Programs</li>
<li>Habitat: A Platform for Embodied AI Research - CVF Open Access, https://openaccess.thecvf.com/content_ICCV_2019/papers/Savva_Habitat_A_Platform_for_Embodied_AI_Research_ICCV_2019_paper.pdf</li>
<li>VirtualHome: Building Socially Intelligent Agents via Simulation Xavier Puig Fernández, https://www.xavierpuigf.com/documents/xavier_puig_phd_tesis.pdf</li>
<li>[1910.07113] Solving Rubik’s Cube with a Robot Hand - arXiv, https://arxiv.org/abs/1910.07113</li>
<li>Timeline &amp; Review of OpenAI’s Robotic Hand Project | Exxact Blog, https://www.exxactcorp.com/blog/Deep-Learning/timeline-review-of-openai-s-robotic-hand-project</li>
<li>[Paper Notes 1]QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation | by Flood Sung | IntelligentUnit | Medium, https://medium.com/intelligentunit/paper-notes-1-qt-opt-scalable-deep-reinforcement-learning-for-vision-based-robotic-manipulation-9b4fd22c50b1</li>
<li>Quantile QT-Opt for Risk-Aware Vision-Based Robotic Grasping, https://roboticsproceedings.org/rss16/p075.pdf</li>
<li>QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation, https://www.pair.toronto.edu/csc2621-w20/assets/slides/lec5_qtopt.pdf</li>
<li>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances - arXiv, https://arxiv.org/pdf/2204.01691</li>
<li>[Robotics] Do As I Can, Not As I Say: Grounding Language in Robotic Affordances - Medium, https://medium.com/@amiable_cardinal_crocodile_398/robotics-do-as-i-can-not-as-i-say-grounding-language-in-robotic-affordances-e6d1b74035fd</li>
<li>SayCan: Grounding Language in Robotic Affordances, https://say-can.github.io/</li>
<li>RT-1: Robotics Transformer for Real-World Control at Scale - R Discovery, https://discovery.researcher.life/article/rt-1-robotics-transformer-for-real-world-control-at-scale/a5a1b925066134a18933ad552f843acf</li>
<li>RT-1: Robotics Transformer for real-world control at scale - Google Research, https://research.google/blog/rt-1-robotics-transformer-for-real-world-control-at-scale/</li>
<li>RT-1: ROBOTICS TRANSFORMER FOR REAL-WORLD CONTROL AT SCALE, https://robotics-transformer1.github.io/assets/rt1.pdf</li>
<li>[1803.10122] World Models - arXiv, https://arxiv.org/abs/1803.10122</li>
<li>World Models - Department of Computer Science and Technology |, https://www.cl.cam.ac.uk/~ey204/teaching/ACS/R244_2022_2023/papers/ha_arXiv_2018.pdf</li>
<li>ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning, https://research.nvidia.com/publication/2025-12_thinkact-vision-language-action-reasoning-reinforced-visual-latent-planning</li>
<li>Practical Insights on Grasp Strategies for Mobile Manipulation in the Wild - arXiv, https://arxiv.org/html/2504.12512v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>