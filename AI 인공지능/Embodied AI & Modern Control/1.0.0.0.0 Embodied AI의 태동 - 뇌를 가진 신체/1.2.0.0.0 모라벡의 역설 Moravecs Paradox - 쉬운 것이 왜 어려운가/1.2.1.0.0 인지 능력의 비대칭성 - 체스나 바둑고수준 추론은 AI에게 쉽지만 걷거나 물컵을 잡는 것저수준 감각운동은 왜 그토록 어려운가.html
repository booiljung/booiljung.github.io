<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:1.2.1 인지 능력의 비대칭성: 체스나 바둑(고수준 추론)은 AI에게 쉽지만, 걷거나 물컵을 잡는 것(저수준 감각운동)은 왜 그토록 어려운가?</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>1.2.1 인지 능력의 비대칭성: 체스나 바둑(고수준 추론)은 AI에게 쉽지만, 걷거나 물컵을 잡는 것(저수준 감각운동)은 왜 그토록 어려운가?</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 1. Embodied AI의 태동: 뇌를 가진 신체</a> / <a href="index.html">1.2 모라벡의 역설 (Moravec's Paradox): 쉬운 것이 왜 어려운가?</a> / <span>1.2.1 인지 능력의 비대칭성: 체스나 바둑(고수준 추론)은 AI에게 쉽지만, 걷거나 물컵을 잡는 것(저수준 감각운동)은 왜 그토록 어려운가?</span></nav>
                </div>
            </header>
            <article>
                <h1>1.2.1 인지 능력의 비대칭성: 체스나 바둑(고수준 추론)은 AI에게 쉽지만, 걷거나 물컵을 잡는 것(저수준 감각운동)은 왜 그토록 어려운가?</h1>
<p>인공지능 연구의 역사는 인간의 직관이 얼마나 자주, 그리고 얼마나 극적으로 빗나갔는지를 보여주는 실패와 재발견의 기록이다. 1950년대 다트머스 회의를 기점으로 인공지능이라는 학문이 태동했을 때, 당대의 석학들은 “지능“의 정수를 논리적 추론, 수학적 정리 증명, 그리고 체스와 같은 전략 게임에서 찾았다. 그들은 이러한 고등 사고 능력이 인간을 동물과 구분 짓는 가장 고차원적인 기능이기에, 기계로 구현하기 가장 어려울 것이라 예측했다. 반면 걷기, 물건 집기, 시각적 인식과 같은 감각운동(Sensorimotor) 능력은 동물들도 수행하는 저차원적인 기능이므로, 컴퓨터에게 가르치는 것은 여름방학 과제 정도의 난이도일 것이라 치부했다. 그러나 반세기가 지난 지금, 현실은 정반대로 드러났다. 기계는 인간이 수천 년간 연마해 온 바둑의 심오한 전략을 단 며칠 만에 정복했지만, 빨래를 개거나 어질러진 방을 정리하는, 인간에게는 너무나 사소한 과제 앞에서 여전히 고전하고 있다. 이 장에서는 소위 ’모라벡의 역설(Moravec’s Paradox)’이라 불리는 이 인지 능력의 비대칭성을 진화생물학, 계산 복잡도 이론, 그리고 로봇 공학의 물리적 상호작용 관점에서 심층적으로 해부한다.</p>
<h2>1. 초기 인공지능의 오판과 역설의 발견</h2>
<p>인공지능 분야의 선구자들은 지능을 ’기호 조작(Symbol Manipulation)’의 문제로 정의했다. 그들은 세계를 명제와 논리식으로 표현하고, 이를 연산함으로써 지능을 구현할 수 있다고 믿었다. 이러한 믿음은 1966년 MIT 인공지능 연구소의 “The Summer Vision Project“에서 극명하게 드러났다. 당시 시모어 페퍼트(Seymour Papert)는 학부생들을 고용하여 여름방학 동안 컴퓨터에 ’시각’을 부여하는 시스템을 개발하도록 지시했다. 카메라로 입력된 2차원 배열의 픽셀 정보를 분석하여 전경과 배경을 분리하고 물체를 식별하는 과제는, 당시 연구자들에게는 체스 전략을 짜는 것보다 훨씬 단순한 하위 문제로 보였다.1</p>
<p>그러나 그 여름은 끝나지 않았다. 컴퓨터 비전(Computer Vision)은 이후 50년 넘게 인공지능의 가장 난해한 분야 중 하나로 남았다. 1980년대에 이르러 한스 모라벡(Hans Moravec), 로드니 브룩스(Rodney Brooks), 마빈 민스키(Marvin Minsky) 등은 이 기이한 현상을 다음과 같이 정식화했다.</p>
<blockquote>
<p>“지능 검사나 체스에서 성인 수준의 성능을 발휘하도록 컴퓨터를 만드는 것은 비교적 쉽다. 그러나 지각(perception)과 이동(mobility) 능력에서 한 살짜리 아기 수준의 능력을 갖추게 하는 것은 어렵거나 불가능하다.“1</p>
</blockquote>
<p>이 역설은 우리가 ’어럽다’고 느끼는 것과 ‘계산적으로 복잡한’ 것 사이의 불일치를 드러낸다. 인간은 의식적인 노력이 필요한 논리적 사고를 어렵다고 느끼지만, 컴퓨터에게 그것은 명확한 규칙에 따른 단순 연산일 뿐이다. 반면, 우리가 ’쉽다’고 느끼는, 의식조차 하지 않는 걷기나 보기는 사실 뇌의 막대한 자원을 소모하는 고도로 복잡한 연산의 결과물이다. 스티븐 핑커(Steven Pinker)가 <em>The Language Instinct</em>에서 언급했듯, “35년간의 AI 연구가 주는 주된 교훈은 어려운 문제는 쉽고, 쉬운 문제는 어렵다는 것“이다.1</p>
<h2>2. 진화적 관점: 빙산의 거대한 몸체</h2>
<p>모라벡의 역설을 이해하는 첫 번째 열쇠는 진화의 시간 척도(Time Scale)에 있다. 모라벡은 그의 저서 <em>Mind Children</em>에서 인간의 지능을 빙산에 비유했다. 수면 위로 드러난 빙산의 일각은 우리가 의식적으로 수행하는 논리, 언어, 추론과 같은 고등 인지 능력이다. 그러나 이를 떠받치고 있는 거대한 몸체는 수면 아래 잠겨 있는 무의식적인 감각운동 지능이다.2</p>
<h3>2.1 진화의 최적화와 시간적 격차</h3>
<p>지구상의 생명체는 수십억 년 동안 중력, 마찰, 관성이라는 가혹한 물리 법칙 속에서 생존해 왔다. 포식자로부터 도망치기 위해 복잡한 지형을 빠르게 이동하고, 먹이를 잡기 위해 3차원 공간에서 물체의 궤적을 예측하며, 자신의 신체를 정밀하게 제어하는 능력은 생존과 직결된 문제였다. 자연선택(Natural Selection)은 이러한 감각운동 시스템을 극한으로 최적화했다. 인간의 뇌 구조를 살펴보면, 시각 처리에 관여하는 대뇌피질 영역은 전체의 30% 이상을 차지하며, 정밀한 운동 제어를 담당하는 소뇌(Cerebellum)는 뇌 전체 부피의 10%에 불과하지만 전체 뉴런 수의 50% 이상(약 690억 개 중 500억 개 이상)을 보유하고 있다.6 이는 ’걷기’나 ’잡기’와 같은 행위가 얼마나 막대한 계산 자원을 요구하는지를 생물학적으로 방증한다.</p>
<p>반면, 추상적인 추론이나 체스와 같은 논리적 사고는 진화의 역사에서 지극히 최근에 등장했다. 호모 사피엔스가 언어를 사용하고 복잡한 사회적 상호작용을 시작한 것은 기껏해야 10만 년 전후의 일이다. 진화의 관점에서 10만 년은 새로운 전용 하드웨어를 설계하고 최적화하기에는 턱없이 부족한 시간이다. 따라서 인간의 고수준 추론 능력은 감각운동을 위해 진화된 범용 신경망의 일부를 임시로 전용하여 수행하는, 일종의 “가장 얇은 껍질(thinnest veneer)“에 불과하다.2 컴퓨터 과학자들에게 이는 역설적이게도 희소식이었다. 인간이 의식적으로, 그리고 서툴게 수행하는 논리적 사고는 그 과정이 명시적이고 단계적이며 규칙 기반으로 설명 가능하기 때문에, 이를 알고리즘으로 역설계(Reverse Engineering)하는 것이 수월했던 것이다.6</p>
<h3>2.2 정보 처리 대역폭의 불균형</h3>
<p>인간의 정보 처리 대역폭(Bandwidth)에 대한 연구는 이 격차를 수치적으로 명확히 보여준다. 다양한 연구 결과들을 종합해 볼 때, 인간의 감각 기관(망막, 달팽이관, 피부 감각 수용체 등)은 초당 약 1,100만 비트(11 Mbps) 이상의 정보를 뇌로 쏟아붓는다. 특히 시각 정보만으로도 약 1,000만 비트에 달하는 데이터가 유입된다.9 뇌의 무의식적인 영역은 이 거대한 데이터의 홍수를 실시간으로 처리하여 3차원 공간을 재구성하고 근육을 조정한다.</p>
<p>그러나 인간의 ’의식(Consciousness)’이 처리할 수 있는 정보의 양은 초당 40~50 비트에 불과하다.9 우리가 “체스를 둔다“거나 “수학 문제를 푼다“라고 할 때, 우리는 이 50 비트의 극도로 좁은 대역폭을 사용하여 직렬적(Serial)으로 사고한다. 컴퓨터는 본질적으로 직렬 처리에 특화되어 있으며, 인간의 50 비트 의식적 사고보다 수억 배 빠른 속도로 논리 연산을 수행할 수 있다. 반면 로봇이 커피 컵을 잡는 행위는 1,100만 비트의 병렬적(Parallel) 데이터 스트림을 실시간으로 처리해야 하는 과제다. 인간은 이를 무의식적으로 수행하기 때문에 그 난이도를 인지하지 못하지만(0에 가까운 인지 부하), 컴퓨터에게는 엄청난 연산 부하를 일으키는 과제가 된다. 민스키가 지적했듯, “우리는 우리의 마음이 가장 잘하는 일에 대해서는 가장 알지 못한다(We’re least aware of what our minds do best)”.1</p>
<h2>3. 계산 복잡도와 상태 공간의 본질적 차이</h2>
<p>“체스는 어렵고 걷기는 쉽다“는 인간의 착각은 문제 공간(Problem Space)의 성격을 오해한 데서 비롯된다. 체스나 바둑과 같은 보드 게임과 현실 세계의 로봇 제어는 ’상태 공간(State Space)’의 차원과 성격이 근본적으로 다르다.</p>
<h3>3.1 닫힌 세계(Closed World) vs. 열린 세계(Open World)</h3>
<p>체스와 바둑은 전형적인 <strong>닫힌 세계</strong>의 문제다.</p>
<ol>
<li><strong>이산적 상태(Discrete States):</strong> 말의 위치는 A3, B4와 같이 명확한 좌표로 정의된다. 말이 칸의 경계선에 걸쳐 있는 애매한 상태는 규칙상 존재하지 않는다.</li>
<li><strong>완전 정보(Perfect Information):</strong> 보드 위의 모든 정보는 공개되어 있다. 상대의 말이 숨겨져 있거나(Fog of War), 주사위 굴리기와 같은 우연적 요소가 없다.</li>
<li><strong>결정론적 전이(Deterministic Transition):</strong> 특정 수를 두면 게임의 상태는 규칙에 따라 정확하고 예외 없이 변한다.</li>
</ol>
<p>바둑의 경우 가능한 상태의 수가 <span class="math math-inline">10^{170}</span>에 달해 우주의 원자 수(<span class="math math-inline">10^{80}</span>)보다 많다고 하지만12, 이는 유한한 집합이다. 거대하지만 경계가 명확한 이 공간에서 컴퓨터는 몬테카를로 트리 탐색(MCTS)과 심층 신경망을 결합하여 최적의 경로를 찾아낼 수 있다. AlphaGo의 승리는 본질적으로 ‘검색(Search)’ 알고리즘의 승리였다.12</p>
<p>반면, 로봇이 현실 세계에서 걷거나 물체를 잡는 것은 <strong>열린 세계</strong>의 문제다.</p>
<ol>
<li><strong>연속적 상태(Continuous States):</strong> 로봇 팔의 관절 각도, 물체의 위치, 표면의 거칠기 등은 실수(Real Number)로 표현되는 연속적인 값이다. 가능한 상태의 수는 무한대다. <span class="math math-inline">10^{170}</span>이 아무리 큰 수라 할지라도 무한대 앞에서는 0이나 다름없다.15</li>
<li><strong>불완전 정보(Imperfect Information):</strong> 센서는 항상 노이즈를 포함하며, 물체는 다른 물체에 가려져(Occlusion) 보이지 않을 수 있다. 로봇은 제한된 센서 데이터만으로 세계의 전체 상태를 확률적으로 추론(State Estimation)해야 한다.</li>
<li><strong>비결정론적 역학(Nondeterministic Dynamics):</strong> 현실 세계는 마찰, 공기 저항, 유체의 흐름, 재질의 변형 등 수많은 물리적 변수가 작용한다. 로봇이 발을 내디뎠을 때 바닥이 미끄러울지, 모래처럼 무너질지 완벽하게 예측하는 것은 불가능하다.</li>
</ol>
<h3>3.2 프레임 문제와 차원의 저주</h3>
<p>고전적 AI가 직면했던 **프레임 문제(The Frame Problem)**는 현실 세계의 복잡성을 단적으로 보여준다. 체스에서는 말을 움직이면 그 말의 위치만 변하고 나머지 말들은 그대로다. 즉, 행동의 결과가 국소적(Local)이다. 그러나 현실 세계에서는 로봇 팔이 움직이면 공기의 흐름이 바뀌고, 그림자가 이동하며, 자칫하면 옆에 있던 물건을 쳐서 떨어뜨릴 수도 있다. 어떤 행동을 했을 때 ’변하는 것’과 ’변하지 않는 것’을 모두 기술하려면 무한한 수의 논리식이 필요하다.</p>
<p>이러한 현실의 무한성은 **차원의 저주(Curse of Dimensionality)**로 이어진다. 자유도(Degrees of Freedom)가 하나 늘어날 때마다 탐색해야 할 공간은 지수적으로 폭증한다. 바둑판은 361개의 교차점이라는 고정된 차원을 갖지만, 유연한 신체를 가진 로봇이 변형 가능한 물체를 다루는 상황은 사실상 무한 차원의 제어 문제를 야기한다. 따라서 로봇 공학에서 걷기나 잡기는 단순한 ’이동’이 아니라, 매 순간 변화하는 불확실한 환경 속에서 최적의 제어 입력을 찾아내야 하는 고난도 비선형 최적화(Nonlinear Optimization) 문제가 된다.15</p>
<h2>4. 감각운동의 심연: 물리적 상호작용의 기술적 난관</h2>
<p>모라벡의 역설이 제기된 지 40년이 지났음에도 불구하고, 왜 여전히 로봇은 빨래를 개는 데 서툰가? 이 질문에 답하기 위해서는 저수준 감각운동 제어에서 발생하는 구체적인 물리적, 광학적 난관들을 들여다봐야 한다.</p>
<h3>4.1 투명성과 반사의 악몽: 깊이 인식의 한계</h3>
<p>인간의 시각 시스템은 투명한 유리잔이나 반짝이는 금속 숟가락을 보았을 때, 뇌가 즉시 재질의 특성을 보정하여 정확한 위치와 형상을 인식한다. 그러나 로봇에게 투명하거나 반사가 심한 물체(Transparent and Specular Objects)는 인식 시스템을 마비시키는 “악몽“과도 같다.19</p>
<p>현재 대부분의 로봇은 3D 공간을 인식하기 위해 LiDAR나 RGB-D(깊이) 카메라를 사용한다. 이 중 ToF(Time-of-Flight) 방식이나 구조광(Structured Light) 방식은 센서가 적외선을 쏘고 물체에 반사되어 돌아오는 시간을 측정하여 거리를 계산한다. 문제는 유리나 플라스틱 같은 투명한 물체는 적외선을 그대로 통과시키고, 거울 같은 반사체는 빛을 엉뚱한 방향으로 튀겨버린다는 점이다. 이는 <strong>다중 경로 간섭(Multipath Interference)</strong> 현상을 초래한다. 센서의 한 픽셀에 도달한 빛이 물체 표면에서 직접 반사된 것인지, 아니면 물체를 통과하여 뒷벽을 맞고 돌아온 것인지, 혹은 주변의 다른 물체에 난반사되어 들어온 것인지 구분할 수 없게 된다.20</p>
<p>결과적으로 로봇의 눈(Depth Sensor)에는 유리컵이 아예 존재하지 않는 투명인간으로 보이거나, 혹은 컵의 형상이 찌그러지고 구멍이 뚫린 노이즈 덩어리로 인식된다. 이러한 부정확한 깊이 정보는 로봇이 허공에 손을 휘젓거나 컵을 깨뜨리는 원인이 된다. 최근에는 딥러닝을 이용해 RGB 이미지에서 투명 물체의 굴절을 추론하거나(예: ClearGrasp), 합성 데이터를 통해 이러한 왜곡을 보정하려는 시도가 이어지고 있으나24, 여전히 실시간 처리에는 막대한 연산 비용이 소요된다.</p>
<h3>4.2 마찰의 불확실성과 미끄러짐(Slip)</h3>
<p>물체를 잡는(Grasping) 행위의 성공 여부는 손가락 끝과 물체 표면 사이의 마찰력에 달려 있다. 인간은 피부에 분포한 약 2,000개의 기계수용체(Mechanoreceptor)를 통해 접촉 순간의 미세한 진동과 피부의 늘어남을 감지한다. 놀랍게도 인간은 물체를 들어 올리기 시작한 지 0.1초(100ms) 이내에 물체의 무게와 마찰 계수를 추정하고, 미끄러짐(Incipient Slip)이 발생하기 직전에 반사적으로 악력을 조절한다.27</p>
<p>반면 로봇에게 마찰력은 ‘접촉하기 전까지는 알 수 없는’ 미지의 변수다. 시각 정보만으로는 물체 표면이 젖어 있는지, 기름이 묻어 있는지, 혹은 먼지가 쌓여 있는지 알 수 없다.28 또한, 기존의 로봇 그리퍼(Gripper)에 부착된 힘 센서는 인간의 피부만큼 민감하지 않거나, 신호 처리 지연(Latency)으로 인해 실시간 미끄러짐 제어에 실패하는 경우가 많다. 마찰 계수를 모르는 상태에서 로봇은 안전을 위해 과도한 힘으로 물체를 잡게 되는데, 이는 연약한 물체(딸기, 얇은 유리잔)를 파손시키는 결과를 낳는다.29 최근에는 시각과 촉각 정보를 융합(Visuo-haptic fusion)하여 마찰을 추정하려는 연구가 진행되고 있으나, 이는 여전히 어려운 과제다.28</p>
<h3>4.3 변형 가능한 물체(Deformable Objects)와 무한 자유도</h3>
<p>가장 극단적인 난이도 격차는 빨래 개기에서 나타난다. 체스 말이나 컵과 같은 강체(Rigid Body)는 6개의 자유도(위치 <span class="math math-inline">x, y, z</span> + 회전 <span class="math math-inline">roll, pitch, yaw</span>)만 알면 그 상태를 완벽하게 정의할 수 있다. 그러나 수건, 옷, 케이블, 음식과 같은 **변형 가능한 물체(Deformable Objects)**는 접히고, 구겨지고, 늘어남에 따라 무한에 가까운 자유도를 가진다.32</p>
<p>로봇이 수건을 반으로 접으려면, 수건의 각 부분이 중력과 마찰력에 의해 어떻게 휘어질지를 예측해야 한다. 이를 시뮬레이션하기 위해서는 유한요소법(FEM)과 같은 복잡한 물리 모델이 필요하지만, 이는 실시간 제어에 적용하기에는 계산량이 너무 많다. 또한, 옷감의 현재 상태(어디가 꼬여 있고 어디가 겹쳐 있는지)를 시각적으로 파악하는 것조차 ‘셀프 오클루전(Self-occlusion, 자기 가림)’ 현상 때문에 매우 어렵다. 2010년대의 한 연구에서는 로봇이 수건 하나를 개는 데 20분이 걸리기도 했다. 이는 고수준 추론이 아닌, 물체의 위상 기하학적 상태(Topological State)를 인식하고 조작하는 저수준 감각운동 지능의 부재를 보여준다.35</p>
<h3>4.4 유체 역학의 카오스: 슬로싱(Sloshing)</h3>
<p>물컵을 들고 이동하는 단순한 과제에도 심오한 물리학이 숨어 있다. 컵을 가속하면 관성에 의해 액체 표면이 기울어지고 출렁이는 ‘슬로싱(Sloshing)’ 현상이 발생한다. 인간은 팔의 임피던스(Impedance, 강성)를 유동적으로 조절하여 이 진동을 상쇄하며, 컵의 기울기를 미세하게 조정하여 쏟아짐을 방지한다. 이 과정은 뇌의 피드포워드(Feedforward) 모델과 시각적 피드백이 결합된 고도의 제어 루프다.</p>
<p>로봇이 이를 수행하려면 컵 내부 액체의 동역학을 모델링해야 하는데, 액체의 움직임은 나비에-스토크스 방정식(Navier-Stokes equations)을 따르는 비선형 시스템이다. 컵의 형상, 액체의 점성, 로봇의 가속도가 복잡하게 상호작용하여 혼돈(Chaos)에 가까운 거동을 보인다.37 이를 억제하기 위해 투입 성형(Input Shaping) 기법이나 동적 최적화(Dynamic Optimization) 알고리즘이 사용되지만, 이는 여전히 로봇의 이동 속도를 제한하는 주된 요인이다.39</p>
<h2>5. 폴라니의 역설: 말할 수 없는 지식의 장벽</h2>
<p>모라벡의 역설은 인식론적 차원에서 ’폴라니의 역설(Polanyi’s Paradox)’과 깊이 연결되어 있다. 헝가리 출신의 철학자 마이클 폴라니(Michael Polanyi)는 1966년 그의 저서 <em>The Tacit Dimension</em>에서 인간 지식의 본질에 대해 다음과 같은 유명한 명제를 남겼다.</p>
<blockquote>
<p>“우리는 우리가 말할 수 있는 것보다 더 많이 알고 있다(We know more than we can tell).“42</p>
</blockquote>
<p>이 통찰은 왜 전통적인 프로그래밍 방식이 로봇 공학에서 실패했는지를 명쾌하게 설명한다. 인간의 지식은 크게 두 가지로 나뉜다.</p>
<ol>
<li><strong>형식지(Explicit Knowledge):</strong> 언어나 기호로 명확하게 기술할 수 있는 지식. 체스의 규칙, 수학 공식, 요리 레시피 등이 이에 해당한다.</li>
<li><strong>암묵지(Tacit Knowledge):</strong> 경험과 신체화를 통해 습득했지만, 언어로 설명하기 어려운 지식. 자전거의 균형을 잡는 법, 어머니의 얼굴을 식별하는 미묘한 특징, 병아리 감별사의 직관 등이 이에 속한다.</li>
</ol>
<p>초기 AI 연구자들은 지능을 형식지의 집합으로 보았다. 체스는 규칙이 명확한 형식지였으므로 프로그래머가 코드로 옮기는 것이 가능했다(Rule-based AI). 그러나 걷기나 잡기는 대표적인 암묵지다. 우리는 어떻게 걷는지 ’설명’할 수 없다. “무릎을 굽히고 발을 뻗는다“라고 말할 수는 있지만, 그 과정에서 수백 개의 근육이 어떤 순서로, 어느 정도의 강도로 수축하고 이완하는지, 무게 중심을 어떻게 이동시키는지에 대한 수치적 규칙은 우리 자신도 모른다. 설명할 수 없으니 프로그래밍할 수도 없는 것이다.42</p>
<p>이 장벽을 넘기 위해 AI는 인간이 규칙을 입력하는 방식(Symbolic AI)에서, 데이터로부터 스스로 규칙을 학습하는 방식(Machine Learning/Deep Learning)으로 패러다임을 전환했다. AlphaGo는 바둑의 암묵적인 패턴을 기보 데이터로부터 학습했다. 그러나 로봇 공학에서는 이조차 쉽지 않다. 인터넷에는 텍스트와 이미지는 넘쳐나지만, 로봇이 컵을 잡을 때 느껴지는 미세한 힘과 미끄러짐에 대한 ’촉각 데이터’나, 복잡한 조작 작업에 대한 ’행동 데이터’는 극도로 희소하기 때문이다. 이를 ’데이터의 기근’이라 부르며, 최근에는 시뮬레이션에서 데이터를 생성하여 학습시키는 방법(Sim-to-Real)이 대안으로 떠오르고 있다.16</p>
<h2>6. 최신 AI의 동향: LLM과 신체성의 간극</h2>
<p>2020년대 중반, 거대언어모델(LLM)과 멀티모달 모델(LMM/VLM)의 비약적인 발전은 모라벡의 역설을 새로운 국면으로 이끌었다. GPT-4, Gemini와 같은 모델은 언어적 이해, 코딩, 복잡한 추론 능력에서 인간 수준에 도달하거나 이를 상회했다. 일부 벤치마크(MMLU, GPQA)에서는 박사급 전문가 수준의 지식을 보여준다.45 그렇다면 이 고도화된 지능이 로봇의 감각운동 문제를 해결해 줄 수 있을까?</p>
<p>현재의 연구 결과는 “아직은 아니다“라고 답한다. 최신 벤치마크(RoboBench, ResponsibleRobotBench)에 따르면, LLM 기반의 로봇 에이전트는 고수준의 계획(High-level Planning)에는 능숙하지만, 저수준의 제어(Low-level Control)에서는 여전히 실패율이 높다.46</p>
<p>예를 들어, “목이 마르니 컵에 물을 담아 가져다줘“라는 명령을 받으면, LLM은 다음과 같이 완벽한 계획을 세운다.</p>
<ol>
<li>주방으로 이동한다.</li>
<li>찬장에서 컵을 찾는다.</li>
<li>정수기로 이동한다.</li>
<li>컵에 물을 받는다.</li>
<li>사용자에게 가져온다.</li>
</ol>
<p>그러나 막상 ‘2. 찬장에서 컵을 찾는다’ 단계에서 컵이 다른 그릇 뒤에 숨겨져 있거나(Occlusion), ‘4. 컵에 물을 받는다’ 단계에서 컵의 손잡이 방향에 맞춰 손목을 정밀하게 비틀어야 할 때(Affordance Prediction), 언어 모델의 추상적인 지식은 무용지물이 된다. 텍스트로 학습한 “컵을 잡는다“는 개념과, 실제 물리 세계에서 컵을 잡기 위해 필요한 관절 토크(Torque) 제어 사이에는 거대한 간극(Gap)이 존재한다. 이를 **접지 문제(Grounding Problem)**라고 한다. LLM은 ’사과’라는 단어의 의미는 알지만, 사과의 무게감, 껍질의 질감, 깨물었을 때의 저항감을 신체적으로 알지 못한다. 즉, “뇌(Brain)는 비대해졌지만 몸(Body)은 여전히 둔한” 상태인 것이다.48</p>
<p>이를 해결하기 위해 구글의 RT(Robotics Transformer) 시리즈나 GraspVLA와 같은 시각-언어-행동(Vision-Language-Action) 모델이 개발되고 있다. 이들은 인터넷상의 방대한 의미론적 데이터와 로봇의 물리적 행동 데이터를 결합하여 학습함으로써, 언어적 명령을 물리적 행동으로 직접 변환하려 시도하고 있다. 특히 GraspVLA와 같은 모델은 수십억 프레임의 합성 데이터(Synthetic Data)를 사용하여 학습함으로써 데이터 부족 문제를 극복하고 일반화 성능을 높이려 하고 있다.25 그러나 여전히 시뮬레이션과 현실 사이의 물리적 차이(Sim-to-Real Gap)와 돌발 상황에 대한 대처 능력 부족은 해결해야 할 과제다.</p>
<h2>7. 결론: 지능에 대한 겸손함과 미래</h2>
<p>결론적으로, 1.2.1장에서 살펴본 인지 능력의 비대칭성은 인간 지능의 본질에 대한 재평가를 요구한다. 우리가 ’지능적’이라고 칭송했던 체스나 수학은 인공적인 규칙 위에서 작동하는 제한된 게임에 불과했다. 반면, 우리가 하찮게 여겼던 걷기, 잡기, 느끼기는 수십억 년의 진화가 빚어낸, 현존하는 어떤 슈퍼컴퓨터로도 완벽히 모사할 수 없는 기적적인 계산의 결과물이다.</p>
<p>표 1은 이러한 비대칭성을 요약하여 보여준다.</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>고수준 추론 (체스, 바둑, 수학)</strong></th><th><strong>저수준 감각운동 (걷기, 잡기)</strong></th></tr></thead><tbody>
<tr><td><strong>문제의 성격</strong></td><td>닫힌 세계 (Closed World)</td><td>열린 세계 (Open World)</td></tr>
<tr><td><strong>정보 가용성</strong></td><td>완전 정보 (Perfect Information)</td><td>불완전 정보 (Noisy, Partial)</td></tr>
<tr><td><strong>상태 공간</strong></td><td>이산적, 유한함 (<span class="math math-inline">10^{170}</span>)</td><td>연속적, 사실상 무한대</td></tr>
<tr><td><strong>진화의 역사</strong></td><td>최근 (&lt; 10만 년)</td><td>매우 오래됨 (&gt; 10억 년)</td></tr>
<tr><td><strong>인간의 처리 방식</strong></td><td>의식적, 직렬 처리 (50 bps)</td><td>무의식적, 병렬 처리 (11 Mbps)</td></tr>
<tr><td><strong>주요 난관</strong></td><td>검색 공간의 크기 (탐색 문제)</td><td>불확실성, 비선형 역학, 모델링 불가</td></tr>
<tr><td><strong>AI 달성 현황</strong></td><td>초인적 수준 (Superhuman)</td><td>유아 수준 미달 (Sub-human)</td></tr>
</tbody></table>
<p>모라벡의 역설은 우리에게 기술적 도전 과제뿐만 아니라 철학적 겸손함을 가르쳐준다. 진정한 범용 인공지능(AGI)으로 나아가기 위해서는 뇌를 만드는 것만큼이나 몸을 이해하고 제어하는 것이 중요하며, 어쩌면 후자가 훨씬 더 도달하기 어려운 목표일지도 모른다. 인공지능 연구의 최전선은 이제 서버실의 GPU 클러스터를 넘어, 거친 현실 세계와 부딪히고 넘어지며 물리적 상호작용을 배우는 로봇의 손끝과 발끝으로 확장되고 있다. 빨래를 개는 로봇이 체스를 두는 로봇보다 더 위대한 기술적 성취로 인정받는 날, 우리는 비로소 지능의 전모를 이해했다고 말할 수 있을 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Moravec’s paradox - Wikipedia, <a href="https://en.wikipedia.org/wiki/Moravec&#x27;s_paradox">https://en.wikipedia.org/wiki/Moravec%27s_paradox</a></li>
<li>Moravec’s Paradox (1988) : r/agi - Reddit, https://www.reddit.com/r/agi/comments/1paa5hd/moravecs_paradox_1988/</li>
<li>Moravec’s Paradox from the 4E Cognitive Psychology View | by Carlos E. Perez - Medium, https://medium.com/intuitionmachine/moravecs-paradox-from-the-4e-cognitive-psychology-view-539359b432fa</li>
<li>Superhumanism: According to Hans Moravec, by 2040 Robots Will Become as Smart as We Are. And Then They’ll Displace Us as the Dominant Form of Life on Earth. But He Isn’t Worried - Elon University, https://www.elon.edu/u/imagining/expert_predictions/superhumanism-according-to-hans-moravec-by-2040-robots-will-become-as-smart-as-we-are-and-then-theyll-displace-us-as-the-dominant-form-of-life-on-earth-but-he-isnt-worried-the-robots-will-love-8/</li>
<li>Moravec, H. (1990) Mind Children | PDF | Artificial Intelligence - Scribd, https://www.scribd.com/document/678843543/8-Moravec-H-1990-Mind-Children</li>
<li>MORAVEC’S PARADOX: CONSIDERATION IN THE CONTEXT OF TWO BRAIN HEMISPHERE FUNCTIONS, https://vsrotenberg.rjews.com/moravecs-paradox.pdf</li>
<li>Moravec’s paradox and its implications — EA Forum, https://forum.effectivealtruism.org/posts/dTjtgyq6KyYdgyrxc/moravec-s-paradox-and-its-implications</li>
<li>Why Can’t Machines Learn Simple Tasks? - Mind Matters, https://mindmatters.ai/2018/08/why-cant-machines-learn-simple-tasks/</li>
<li>Information flow in sensory systems and conscious perception - ResearchGate, https://www.researchgate.net/figure/Information-flow-in-sensory-systems-and-conscious-perception_tbl1_238400531</li>
<li>The Unbearable Slowness of Being: Why do we live at 10 bits/s? - arXiv, https://arxiv.org/html/2408.10234v2</li>
<li>Flow and intuition: a systems neuroscience comparison - Oxford Academic, https://academic.oup.com/nc/article/2025/1/niae040/7942876</li>
<li>Not just in your opinion. The search space is exponentially larger in go because… | Hacker News, https://news.ycombinator.com/item?id=10981943</li>
<li>Does AlphaGo actually play Go? Concerning the State Space of Artificial Intelligence - arXiv, https://arxiv.org/pdf/1912.10005</li>
<li>Game complexity - Wikipedia, https://en.wikipedia.org/wiki/Game_complexity</li>
<li>State space sampling of feasible motions for high-performance mobile robot navigation in complex environments, https://www.ri.cmu.edu/pub_files/pub4/howard_thomas_2008_1/howard_thomas_2008_1.pdf</li>
<li>Sample-efficient Reinforcement Learning via Difference Models - UNM CS, https://www.cs.unm.edu/amprg/Workshops/MLPC18/submissions/paper_9.pdf</li>
<li>Monte-Carlo Tree Search vs. Model-Predictive Controller: A Track-Following Example - OpenReview, https://openreview.net/references/pdf?id=SyiF5-23Z</li>
<li>Monte-Carlo Tree Search vs. Model-Predictive Controller: A Lane-Following Example - OpenReview, https://openreview.net/pdf?id=SyiF5-23Z</li>
<li>Transparent, Reflective Objects Now Within Grasp of Robots, https://www.ri.cmu.edu/transparent-reflective-objects-now-within-grasp-of-robots/</li>
<li>What is Multi-path Interference in ToF cameras? - YouTube, https://www.youtube.com/watch?v=t-M7V0iJGJw</li>
<li>SPUMIC: SIMULTANEOUS PHASE UNWRAPPING AND MULTIPATH INTERFERENCE CANCELLATION IN TIME-OF-FLIGHT CAMERAS USING SPECTRAL METHODS A - Microsoft, https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/06607553.pdf</li>
<li>TN06-Multipath interference of indirect Time of Flight (iToF) - Goermicro Industrial Line, https://industry.goermicro.com/blog/tech-briefs/multipath-interference-of-indirect-time-of-flight-itof.html</li>
<li>What is multipath interference? How to minimize it in Time-of-Flight cameras? - e-con Systems, https://www.e-consystems.com/blog/camera/technology/what-is-multipath-interference-how-to-minimize-it-in-time-of-flight-cameras/</li>
<li>Review of Deep Reinforcement Learning-based Object Grasping: Techniques, Open Challenges and Recommendations - IEEE Xplore, https://ieeexplore.ieee.org/iel7/6287639/6514899/09210095.pdf</li>
<li>GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data, https://arxiv.org/html/2505.03233v3</li>
<li>Resolving Multi-Path Interference in Compressive Time-of-Flight Depth Imaging with a Multi-Tap Macro-Pixel Computational CMOS Image Sensor, https://pmc.ncbi.nlm.nih.gov/articles/PMC9003367/</li>
<li>Real-time Friction Estimation for Grip Force Control - Contactile, https://contactile.com/wp-content/uploads/2022/01/KhamisEtAl2021_ICRA2021_Manuscript_preprint.pdf</li>
<li>Probabilistic Surface Friction Estimation Based on Visual and Haptic Measurements, https://ieeexplore.ieee.org/document/9364673/</li>
<li>Grasping Force Control of Multi-Fingered Robotic Hands through Tactile Sensing for Object Stabilization - MDPI, https://www.mdpi.com/1424-8220/20/4/1050</li>
<li>Friction-Scaled Vibrotactile Feedback for Real-Time Slip Detection in Manipulation using Robotic Sixth Finger - arXiv, https://arxiv.org/html/2503.15447v1</li>
<li>Detecting Transitions from Stability to Instability in Robotic Grasping Based on Tactile Perception - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC11314830/</li>
<li>Grasping Deformable Objects in Industry Application: A Comprehensive Review of Robotic Manipulation - IEEE Xplore, https://ieeexplore.ieee.org/iel8/6287639/10820123/10892136.pdf</li>
<li>Editorial: Robotic grasping and manipulation of deformable objects - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2022.1108038/full</li>
<li>Challenges and Outlook in Robotic Manipulation of Deformable Objects, https://arm.robotics.umich.edu/download.php?p=103</li>
<li>Why AI Can Beat Chess Champions But Can’t Fold a Piece of Fabric - Moravec’s Paradox, https://www.youtube.com/shorts/O6T2ALcsFJI</li>
<li>If we have really achieved AI, why we don’t have a clothing folding machine? - Reddit, https://www.reddit.com/r/ArtificialInteligence/comments/1c0hk6q/if_we_have_really_achieved_ai_why_we_dont_have_a/</li>
<li>A Plug-in Feed-forward Control for Sloshing Suppression in Robotic Teleoperation Tasks - IRIS Unimore, https://www.iris.unimore.it/retrieve/e31e124d-88ba-987f-e053-3705fe0a095a/HarmonicFilter4Sloshing_IROS2018_final.pdf</li>
<li>A Dynamic Optimization Approach for Sloshing Free Transport of Liquid Filled Containers using an Industrial Robot - IEEE Xplore, https://ieeexplore.ieee.org/document/8968144/</li>
<li>Robotic handling of liquids with spilling avoidance: a constraint-based control approach - POLITesi, https://www.politesi.polimi.it/bitstream/10589/136702/3/2017_10_Maderna.pdf</li>
<li>A Plug-In Feed-Forward Control for Sloshing Suppression in Robotic Teleoperation Tasks, https://ieeexplore.ieee.org/document/8593962/</li>
<li>(PDF) Manipulating Liquids with Robots: a Sloshing-Free Solution - ResearchGate, https://www.researchgate.net/publication/326201131_Manipulating_Liquids_with_Robots_a_Sloshing-Free_Solution</li>
<li>Polanyi’s Paradox and the Shape of Employment Growth - NBER, https://www.nber.org/system/files/working_papers/w20485/w20485.pdf</li>
<li>Polanyi’s Paradox: Will humans maintain any advantage over machines?, https://philosophicaldisquisitions.blogspot.com/2015/10/polanyis-paradox-will-humans-maintain.html</li>
<li>Moravec’s Paradox: Why AI Cannot Replace Humans - LatentView, https://www.latentview.com/blog/moravecs-paradox-why-ai-cannot-replace-humans/</li>
<li>LLM Benchmarks 2025 - Complete Evaluation Suite, https://llm-stats.com/benchmarks</li>
<li>A Survey of Robot Intelligence with Large Language Models - MDPI, https://www.mdpi.com/2076-3417/14/19/8868</li>
<li>ResponsibleRobotBench: Benchmarking Responsible Robot Manipulation using Multi-modal Large Language Models - arXiv, https://arxiv.org/html/2512.04308v1</li>
<li>A collection of benchmarks for emboided robotic agent based on LLMs | by Yanan Chen, https://medium.com/@yananchen1116/a-collection-of-benchmarks-for-emboided-robotic-agent-based-on-llms-40482ecef189</li>
<li>Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain - arXiv, https://arxiv.org/html/2510.17801v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>