<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:1.2.3 로봇 공학의 난제: 불확실성(Uncertainty), 비정형성(Unstructured Environment), 그리고 실시간성(Real-time Constraint).</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>1.2.3 로봇 공학의 난제: 불확실성(Uncertainty), 비정형성(Unstructured Environment), 그리고 실시간성(Real-time Constraint).</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 1. Embodied AI의 태동: 뇌를 가진 신체</a> / <a href="index.html">1.2 모라벡의 역설 (Moravec's Paradox): 쉬운 것이 왜 어려운가?</a> / <span>1.2.3 로봇 공학의 난제: 불확실성(Uncertainty), 비정형성(Unstructured Environment), 그리고 실시간성(Real-time Constraint).</span></nav>
                </div>
            </header>
            <article>
                <h1>1.2.3 로봇 공학의 난제: 불확실성(Uncertainty), 비정형성(Unstructured Environment), 그리고 실시간성(Real-time Constraint).</h1>
<p>인터넷이라는 가상의 공간에서 획득한 지능을 물리적 실체를 가진 로봇(Embodied AI)으로 이식하는 과정은 단순한 플랫폼의 변화가 아닌, 인식론적 세계관의 근본적인 전환을 의미한다. 데이터 센터 내의 인공지능이 노이즈가 통제되고 정제된 데이터셋 위에서 확률적 추론을 수행한다면, 로봇은 엔트로피가 끊임없이 증가하는 물리 세계의 무질서함 속에서 생존과 과제 수행이라는 이중의 목표를 달성해야 한다. 모라벡의 역설(Moravec’s Paradox)이 시사하듯, 인간에게는 무의식적이고 자연스러운 감각운동(Sensorimotor) 능력이 로봇에게 난제로 다가오는 이유는 물리 세계가 가진 세 가지 본질적인 속성, 즉 <strong>불확실성(Uncertainty)</strong>, <strong>비정형성(Unstructured Environment)</strong>, 그리고 <strong>실시간성(Real-time Constraint)</strong> 때문이다.1 이 세 가지 요소는 독립적으로 존재하는 것이 아니라 서로 얽혀 로봇의 인지-판단-행동 루프(Perception-Action-Cognition Loop)를 끊임없이 위협한다. 본 절에서는 이 세 가지 난제가 현대 로봇 공학에서 구체적으로 어떻게 정의되며, 이를 극복하기 위해 SOTA(State-of-the-Art) 기술들이 어떠한 방식으로 수학적 모델링과 인공지능 아키텍처를 진화시키고 있는지 심층적으로 분석한다.</p>
<h2>1.  불확실성(Uncertainty): 확률적 엔트로피와의 투쟁</h2>
<p>로봇 공학에서 불확실성은 제거해야 할 노이즈가 아니라, 로봇이 세상을 이해하고 행동하는 데 있어 전제로 삼아야 할 상수이다. 시뮬레이션 환경에서의 상태(State)는 결정론적(Deterministic)이지만, 현실 세계의 상태는 언제나 관측 불가능한 히든 스테이트(Hidden State)를 포함하며, 로봇은 불완전한 센서 데이터에 의존해 이 히든 스테이트를 추정해야 한다. 로봇이 마주하는 불확실성은 그 근원과 성격에 따라 크게 두 가지 차원으로 분류되며, 이를 구분하여 대응하는 것이 현대 제어 및 학습 이론의 핵심이다.2</p>
<h3>1.1  불확실성의 분류학: Aleatoric과 Epistemic의 이분법</h3>
<p>확률론적 로봇 공학(Probabilistic Robotics)과 최신 딥러닝 기반 방법론들은 불확실성을 **Aleatoric Uncertainty(우연적 불확실성)**와 **Epistemic Uncertainty(인식적 불확실성)**로 구분하여 접근한다.4 이 두 불확실성의 구분은 로봇이 “데이터를 더 모을 것인가” 아니면 “보수적으로 행동할 것인가“를 결정하는 기준이 된다.</p>
<p><strong>Aleatoric Uncertainty</strong>는 데이터 자체에 내재된 무작위성에서 기인한다. 라이다(LiDAR) 센서가 가진 고유의 측정 오차, 지면의 미세한 굴곡으로 인한 바퀴의 미끄러짐, 바람에 의한 드론의 흔들림 등이 이에 해당한다. 이는 본질적인 물리적 현상이므로 데이터를 무한히 수집한다 해도 제거할 수 없다. 따라서 로봇은 이를 확률 분포(Probability Distribution)로 모델링하여, 특정 행동이 초래할 결과의 분산(Variance)을 예측하고 리스크를 관리해야 한다.6 반면, <strong>Epistemic Uncertainty</strong>는 모델의 지식 부족에서 기인한다. 로봇이 학습 데이터에 존재하지 않았던 새로운 물체(Out-of-Distribution, OOD)를 마주치거나, 한 번도 가보지 못한 지형에 진입했을 때 발생한다. 이는 이론적으로 데이터를 더 수집하거나 탐험(Exploration)을 통해 지식을 확장함으로써 감소시킬 수 있는 불확실성이다.7</p>
<p>아래 표는 이 두 가지 불확실성이 로봇 공학적 관점에서 어떻게 대조되는지를 보여준다.</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>Aleatoric Uncertainty (데이터 내재적)</strong></th><th><strong>Epistemic Uncertainty (모델 지식적)</strong></th></tr></thead><tbody>
<tr><td><strong>근원</strong></td><td>센서 노이즈, 환경의 동역학적 확률성(Stochasticity)</td><td>학습 데이터의 부족, 모델의 표현력 한계, OOD 상황</td></tr>
<tr><td><strong>해결 가능성</strong></td><td><strong>불가능 (Irreducible)</strong>. 분포 추정으로 대응.</td><td><strong>가능 (Reducible)</strong>. 추가 학습 및 능동적 탐색으로 감소.</td></tr>
<tr><td><strong>로봇의 대응</strong></td><td>강건 제어(Robust Control), 확률적 경로 계획</td><td>능동 학습(Active Learning), 안전 정지(Fail-safe)</td></tr>
<tr><td><strong>수학적 접근</strong></td><td><span class="math math-inline">p(y \vert x)</span>의 분산 최적화, Diffusion Models</td><td>베이지안 뉴럴 네트워크(BNN), 앙상블(Ensemble)</td></tr>
</tbody></table>
<p>과거의 고전적 제어 이론은 주로 Aleatoric Uncertainty를 가우시안 노이즈(Gaussian Noise)로 가정하고 칼만 필터(Kalman Filter)나 입자 필터(Particle Filter)를 통해 상태를 추정하는 데 집중했다. 그러나 딥러닝이 로봇의 두뇌가 된 현재, 훈련되지 않은 상황에서의 오작동을 방지하기 위한 Epistemic Uncertainty의 정량화가 자율 주행과 조작(Manipulation)의 안전성을 담보하는 핵심 기술로 부상했다.9</p>
<h3>1.2  부분 관측 마르코프 결정 과정(POMDP)의 부활과 한계</h3>
<p>이론적으로 불확실한 환경에서의 로봇 의사결정 문제는 **부분 관측 마르코프 결정 과정(POMDP, Partially Observable Markov Decision Process)**으로 정식화된다. 로봇은 현재의 상태 <span class="math math-inline">s_t</span>를 직접 알 수 없고, 오직 관측 <span class="math math-inline">o_t</span>만을 통해 현재 상태에 대한 믿음(Belief State) <span class="math math-inline">b_t</span>를 업데이트하며 행동 <span class="math math-inline">a_t</span>를 결정한다.10</p>
<p>POMDP는 불확실성을 수학적으로 엄밀하게 다루는 프레임워크를 제공하지만, 상태 공간이 커질수록 계산 복잡도가 기하급수적으로 증가하는 ’차원의 저주(Curse of Dimensionality)’를 안고 있다. 특히 고차원 이미지 센서와 다관절 로봇 팔을 사용하는 현대 로봇 시스템에서 POMDP를 실시간으로 정확하게 푸는 것은 계산적으로 불가능(Intractable)에 가깝다. 이에 따라 최근의 연구는 POMDP의 엄밀함을 유지하면서도 딥러닝의 근사(Approximation) 능력을 활용하는 방향으로 선회하고 있다. 예를 들어, <strong>Dec-POMDP</strong>와 같은 멀티 에이전트 확장 모델은 로봇 군집 제어에서 불확실성을 분산 처리하는 데 사용되지만, 여전히 복잡한 시나리오에서는 근사 해법이 필수적이다.11</p>
<h3>1.3  생성형 AI 기반 불확실성 대응: Diffusion Policy</h3>
<p>전통적인 정책(Policy) 네트워크는 주어진 상태에 대해 하나의 최적 행동(Deterministic Action)을 출력하거나, 단순한 단봉(Unimodal) 가우시안 분포를 가정했다. 그러나 로봇이 컵을 잡는 상황을 생각해보자. 손잡이를 잡을 수도 있고, 몸통을 잡을 수도 있다. 이 두 가지 유효한 행동의 평균(손잡이와 몸통 사이의 허공)을 취하는 것은 실패로 귀결된다. 이러한 **멀티모달 불확실성(Multimodal Uncertainty)**을 해결하기 위해 등장한 것이 바로 <strong>Diffusion Policy</strong>이다.12</p>
<p><strong>Diffusion Policy: Visuomotor Policy Learning via Action Diffusion</strong>14 논문에서 제안된 이 방법론은 로봇의 행동 생성을 조건부 노이즈 제거 과정(Conditional Denoising Process)으로 재정의한다. 이미지 <span class="math math-inline">I</span>가 주어졌을 때, 행동 <span class="math math-inline">A</span>를 직접 예측하는 대신, 가우시안 노이즈로부터 시작하여 점진적으로 노이즈를 제거해가며 유효한 행동 궤적을 생성한다.</p>
<ul>
<li><strong>멀티모달리티의 보존:</strong> 확산 모델은 임의의 복잡한 분포를 표현할 수 있어, 컵의 손잡이를 잡는 분포와 몸통을 잡는 분포를 모두 학습하고, 추론 시에는 그중 하나의 명확한 모드(Mode)로 수렴한다. 이는 기존의 MSE(Mean Squared Error) 기반 학습이 범하는 ’평균화의 오류’를 근본적으로 해결한다.</li>
<li><strong>시간적 일관성(Temporal Consistency):</strong> Diffusion Policy는 단일 시점의 행동이 아닌, 미래의 행동 궤적 전체를 한 번에 생성(Receding Horizon Control)함으로써 동작의 부드러움과 일관성을 보장한다. 이는 급격한 외란에도 로봇이 급발진하지 않고 안정적인 궤적으로 복귀하는 데 기여한다.15</li>
</ul>
<h3>1.4  세계 모델(World Models): 상상 속에서의 불확실성 탐색</h3>
<p>로봇이 불확실한 환경에서 안전하게 학습하기 위해서는 실제 세계에서 시행착오를 겪기 전에, 자신의 행동이 초래할 결과를 예측해봐야 한다. 이를 가능하게 하는 기술이 **세계 모델(World Models)**이다. <strong>DreamerV3</strong>17와 같은 최신 모델 기반 강화학습(Model-Based RL) 알고리즘은 환경의 동역학(Dynamics)을 잠재 공간(Latent Space)에 내재화한다.</p>
<p>로봇은 관측된 이미지로부터 잠재 상태(Latent State)를 추론하고, 이 상태에서 가상의 행동을 수행했을 때 다음 상태가 어떻게 변할지를 확률적으로 예측한다. 이때 세계 모델은 예측의 불확실성을 <strong>KL 발산(Kullback-Leibler Divergence)</strong> 등을 통해 정량화한다.</p>
<ul>
<li><strong>불확실성 인식 탐험:</strong> 로봇은 세계 모델의 예측 불확실성이 높은 영역(Epistemic Uncertainty가 큰 영역)을 능동적으로 탐험하여 모델을 개선하거나, 반대로 위험도가 높은 상황에서는 보수적인 행동을 취하도록 학습된다.</li>
<li><strong>WIMLE</strong>와 같은 연구는 세계 모델의 예측 불확실성을 강화학습의 보상 함수나 손실 함수에 가중치로 반영하여, 모델이 부정확한 상황에서는 학습을 억제하고 신뢰할 수 있는 경험에 집중하도록 유도한다.19</li>
<li>최근에는 트랜스포머(Transformer) 아키텍처를 도입한 <strong>TransDreamer</strong>와 같은 모델이 등장하여, 장기적인 기억(Long-term Memory)을 유지함으로써 시간이 지남에 따라 누적되는 불확실성을 효과적으로 관리하고 있다.20</li>
</ul>
<h3>1.5  안전을 위한 이론적 보증: Conformal Prediction</h3>
<p>딥러닝 모델, 특히 로봇의 인지-판단 루프에 사용되는 블랙박스 모델의 불확실성을 수학적으로 엄밀하게 보증하는 것은 안전과 직결된다. 단순히 “불확실하다“라고 말하는 것을 넘어, “95%의 확률로 로봇의 다음 위치는 이 영역 안에 있다“라는 통계적 확신이 필요하다. 이에 대한 해답으로 **Conformal Prediction(CP)**이 주목받고 있다.21</p>
<p>Conformal Prediction은 모델의 내부 구조나 데이터 분포에 대한 가정(Distribution-free) 없이, 보정 데이터셋(Calibration Set)을 사용하여 통계적으로 유효한 **예측 구간(Prediction Set)**을 생성한다.</p>
<ul>
<li><strong>Learning-Augmented MPC와의 결합:</strong> <strong>PlanCP</strong>21나 적응형 CP 기반 제어22 연구에서는 CP가 제공하는 불확실성 영역을 모델 예측 제어(MPC)의 제약 조건(Constraint)으로 통합한다. 예측이 확실할 때는 좁은 안전 마진으로 민첩하게 움직이고, 예측이 불확실할 때는 넓은 안전 마진을 확보하여 충돌을 방지한다. 이는 학습 기반 제어기의 성능과 제어 이론의 안전성을 결합하는 강력한 도구로 자리 잡고 있다.24</li>
</ul>
<h2>2.  비정형성(Unstructured Environment): 열린 세계(Open World)로의 진입</h2>
<p>공장의 조립 라인이나 물류 창고는 대표적인 ’정형화된 환경(Structured Environment)’이다. 이곳에서는 조명이 일정하고, 바닥은 평평하며, 로봇이 조작해야 할 물체는 사전에 정의된 CAD 모델과 정확히 일치한다. 그러나 로봇이 가정, 사무실, 재난 현장, 야외와 같은 ’비정형 환경(Unstructured Environment)’으로 나아가는 순간, 무한한 변동성(Variability)이라는 거대한 장벽에 부딪힌다.25 비정형성은 단순한 환경의 복잡함을 넘어, 사전 지식(Prior Knowledge)만으로는 결코 정의할 수 없는 **개방형 어휘(Open-Vocabulary)**와 <strong>예측 불가능한 물리적 상호작용</strong>의 문제를 내포한다.</p>
<h3>2.1  비정형 환경이 던지는 세 가지 난제</h3>
<ol>
<li><strong>지각의 모호성(Perceptual Ambiguity)과 폐색(Occlusion):</strong> 비정형 환경은 정리되어 있지 않다. 물체들은 서로 겹쳐져 있고(Occlusion), 투명하거나 반사되는 재질은 깊이 센서(Depth Sensor)를 교란하며, 조명 조건은 시시각각 변한다. “냉장고 뒤쪽의 케첩을 꺼내라“는 단순한 작업도 시각적으로는 고난도의 퍼즐이 된다.27</li>
<li><strong>의미론적 무한성(Semantic Diversity):</strong> 공장 로봇은 “나사 A“와 “나사 B“만 구분하면 되지만, 가정용 로봇은 “약간 찌그러진 캔”, “반쯤 먹다 남은 사과”, “아이가 그린 그림” 등 수천만 가지의 변형된 객체를 인식해야 한다. 또한 “지저분한 식탁을 치워라“와 같은 추상적인 명령을 구체적인 행동 시퀀스로 변환하는 것은 기호학적 접지(Symbol Grounding)의 극한을 요구한다.29</li>
<li><strong>지형 및 물리적 상호작용의 비예측성:</strong> 공사 현장이나 야지(Off-road)에서는 지면의 마찰 계수, 경도, 평탄도를 미리 알 수 없다. 로봇이 밟았을 때 무너지는 지형인지, 단단한 바위인지 구분하는 것은 시각 정보만으로는 불가능하며, 이는 이동 로봇의 주행 안정성을 심각하게 위협한다.30</li>
</ol>
<h3>2.2  SOTA 해결책: 파운데이션 모델과 3D 표현의 혁명</h3>
<p>비정형성을 극복하기 위해 현대 로봇 공학은 규칙 기반(Rule-based) 접근을 폐기하고, 인터넷 규모의 데이터를 학습한 파운데이션 모델(Foundation Models)과 신경망 기반의 3D 표현 기술을 도입하는 ‘데이터 중심(Data-Centric)’ 패러다임으로 전환하고 있다.</p>
<h4>2.2.1  VLA (Vision-Language-Action) 모델: 일반 범용 로봇의 두뇌</h4>
<p>**RT-2 (Robotic Transformer 2)**32, <strong>OpenVLA</strong>34, <strong>Octo</strong>36와 같은 <strong>Vision-Language-Action (VLA)</strong> 모델들은 비정형 환경에서의 일반화 능력을 획기적으로 향상시켰다. 이들은 거대 언어 모델(LLM)이 가진 추론 능력과 시각-언어 모델(VLM)의 인식 능력을 로봇의 행동 제어와 결합한 것이다.</p>
<ul>
<li><strong>인터넷 지식의 전이(Web Knowledge Transfer):</strong> RT-2는 인터넷상의 수십억 개의 이미지와 텍스트를 통해 학습된 상식(Web Knowledge)을 로봇 제어에 전이한다. 예를 들어, 학습 데이터에 없었던 “슈퍼맨 피규어“를 집어 달라는 명령을 받으면, 인터넷 데이터에서 습득한 슈퍼맨의 시각적 특징과 “집다(Pick)“라는 행동 토큰을 연결하여 작업을 수행한다. 이는 비정형 환경의 미지 객체(Unseen Object)를 다루는 데 있어 필수적인 능력이다.38</li>
<li><strong>Open-Vocabulary 조작:</strong> 기존 로봇이 사전에 정의된 ID를 가진 물체만 인식했다면, VLA 모델은 자연어로 묘사된 임의의 물체나 속성을 이해한다. “가장 오른쪽의 빨간 물체”, “투명한 컵 조심해서“와 같은 자연어 제약 조건을 행동 계획에 반영할 수 있다.40</li>
<li><strong>Octo: Generalist Policy:</strong> Octo와 같은 모델은 다양한 로봇 형태(Embodiment)와 카메라 구성을 아우르는 통합된 표현을 학습한다. 트랜스포머 아키텍처를 기반으로 서로 다른 로봇의 데이터를 하나의 모델에 학습시킴으로써, 새로운 환경이나 로봇에 대한 적응성(Adaptability)을 극대화한다.37</li>
</ul>
<h4>2.2.2  Neural Fields와 3D Gaussian Splatting: 비정형 공간의 연속적 이해</h4>
<p>비정형 환경을 정밀하게 탐색하고 조작하기 위해서는 공간에 대한 정확한 3D 이해가 필요하다. 포인트 클라우드(Point Cloud)나 복셀(Voxel)과 같은 이산적(Discrete) 표현은 메모리 효율이 낮고 해상도의 제약이 있다. 최근 **NeRF(Neural Radiance Fields)**와 <strong>3D Gaussian Splatting (3DGS)</strong> 기술은 로봇의 공간 지각 능력에 혁명을 일으키고 있다.43</p>
<ul>
<li><strong>연속적이고 밀도 높은 표현:</strong> Neural Fields는 공간을 연속적인 함수(MLP 등)로 표현하여 해상도의 제약 없이 정밀한 형상과 텍스처를 복원한다. 이는 복잡한 비정형 물체의 6-DoF 파지(Grasping) 자세를 추정하는 데 유리하다.44</li>
<li><strong>실시간 SLAM 및 내비게이션:</strong> 특히 3DGS는 가우시안 타원체(Ellipsoid)를 통해 장면을 표현하고 이를 래스터화(Rasterization)하여 렌더링 속도가 매우 빠르다. 이는 로봇이 실시간으로 주변 환경의 3D 지도를 생성(SLAM)하고, 이를 바탕으로 충돌 없는 경로를 생성하는 데 적합하다. <strong>SplaTAM</strong>이나 <strong>GS-SLAM</strong>과 같은 최신 연구들은 3DGS를 활용하여 복잡하고 어수선한 실내 환경에서도 정밀한 지도를 실시간으로 구축함을 증명했다.45</li>
<li><strong>GLOVER</strong>와 같은 연구는 LLM의 상식 추론 능력과 3D 어포던스(Affordance) 정보를 결합하여, 비정형 물체의 어느 부분을 잡아야 하는지(예: 칼날이 아닌 손잡이)를 추론하고 파지 자세를 생성한다.29</li>
</ul>
<h4>2.2.3  Active Perception (능동적 지각): 불확실성을 줄이는 행동</h4>
<p>비정형 환경의 불확실성을 줄이기 위해 로봇은 수동적인 관찰자를 넘어 능동적인 행위자가 되어야 한다. <strong>Active Perception</strong>은 로봇이 정보를 더 잘 얻기 위해 카메라 위치를 옮기거나, 물체를 건드려보는 행위를 포함한다.46</p>
<ul>
<li><strong>Next-Best-View (NBV) 계획:</strong> 냉장고 안의 물건이 가려져 있을 때, 로봇은 현재의 3D 지도에서 불확실성(Entropy)이 가장 높은 영역을 식별하고, 정보 이득(Information Gain)이 최대화되는 다음 카메라 위치를 계산하여 이동한다. 최근 연구에서는 Neural Fields의 불확실성 맵을 기반으로 NBV를 실시간으로 최적화하는 기법들이 제안되고 있다.47</li>
<li><strong>상호작용적 지각(Interactive Perception):</strong> 단순히 보는 것을 넘어, 로봇 팔로 앞의 물건을 치워 가려진 물체를 드러내거나(Decluttering), 물체를 밀어보며 물리적 속성을 파악하는 등의 물리적 상호작용을 통해 지각의 성능을 높인다.47</li>
</ul>
<h2>3.  실시간성(Real-time Constraint): 1kHz 루프의 압박과 시간의 비가역성</h2>
<p>로봇 공학이 인터넷 AI와 가장 극명하게 차별화되는 지점은 바로 시간의 비가역성과 엄격한 제약 조건이다. 챗봇의 응답이 1초 늦어지는 것은 사용자 경험(UX)의 저하에 그치지만, 보행 로봇의 자세 제어 신호가 0.01초 늦어지면 로봇은 넘어진다. 물리 세계는 로봇의 연산을 기다려주지 않으며, 모든 물리적 행동은 정해진 시간 내에 완료되어야만 유효하다. 이것이 바로 <strong>Hard Real-time Constraint</strong>이다.49</p>
<h3>3.1  제어 주파수와 지연 시간(Latency)의 딜레마</h3>
<p>로봇 시스템은 물리적 하드웨어의 대역폭(Bandwidth)에 맞춰 제어 주기를 유지해야 한다. 일반적으로 로봇 팔이나 다족 보행 로봇의 하위 제어기(Low-level Controller)는 <strong>1kHz (0.001초)</strong> 이상의 주파수로 동작해야 안정적인 제어가 가능하다. 그러나 시각 인식이나 고수준 추론을 담당하는 거대 AI 모델은 추론에 수십~수백 밀리초(ms)가 소요된다.</p>
<ul>
<li><strong>VLA 모델의 속도 한계:</strong> RT-2와 같은 거대 모델은 추론 속도가 1~3Hz(초당 1~3회)에 불과하다. 1kHz로 동작해야 하는 모터 제어 루프에 3Hz의 AI 모델을 직접 연결하는 것은 불가능하다. 만약 로봇이 미끄러지는 순간 VLA 모델의 추론을 기다린다면, 로봇은 이미 바닥에 넘어진 후일 것이다.32</li>
<li><strong>Sim-to-Real Gap과 Latency:</strong> 시뮬레이션에서는 시간의 흐름을 멈추고 복잡한 연산을 수행할 수 있지만, 현실에서는 연산 시간 동안에도 로봇과 환경의 상태가 변한다. 이 ’관측-행동 지연(Observation-Action Latency)’은 시스템을 불안정하게 만드는 주원인이며, 이를 해결하지 못하면 시뮬레이션에서 완벽했던 정책도 현실에서는 진동하거나 발산한다.52</li>
</ul>
<h3>3.2  계층적 제어(Hierarchical Control): System 1과 System 2의 조화</h3>
<p>이러한 시간 스케일의 불일치를 해결하기 위해 로봇 공학은 인간의 인지 구조를 모방한 <strong>계층적 제어(Hierarchical Control)</strong> 아키텍처를 채택하고 있다. 이는 인지심리학자 대니얼 카너먼이 제시한 **System 1 (직관적, 빠른 반응)**과 <strong>System 2 (분석적, 느린 추론)</strong> 이론과 유사하다.53</p>
<table><thead><tr><th><strong>계층</strong></th><th><strong>역할</strong></th><th><strong>주기 (Frequency)</strong></th><th><strong>기술 요소 (SOTA)</strong></th></tr></thead><tbody>
<tr><td><strong>High-Level (System 2)</strong></td><td>장기 계획, 의미론적 추론, 작업 순서 결정. “냉장고에서 사과를 꺼내라”</td><td>1Hz ~ 10Hz</td><td>VLM (RT-2, OpenVLA), LLM Planner, LeVERB-VL</td></tr>
<tr><td><strong>Mid-Level</strong></td><td>궤적 생성, 장애물 회피, 모델 예측 제어. 손 끝의 6D 궤적 생성.</td><td>50Hz ~ 100Hz</td><td>Learning-Augmented MPC, Diffusion Policy, System 2의 잠재 명령 해석</td></tr>
<tr><td><strong>Low-Level (System 1)</strong></td><td>전신 제어(WBC), 관절 토크 계산, 균형 유지. 외란에 대한 즉각적 반사.</td><td>500Hz ~ 2kHz</td><td>Convex Optimization, Whole-Body Control, Proprioceptive RL, LeVERB-A</td></tr>
</tbody></table>
<h4>3.2.1  Whole-Body Control (WBC)과 강화학습의 결합</h4>
<p>휴머노이드와 같이 자유도가 높은(High-DoF) 로봇의 균형을 실시간으로 잡기 위해 **Whole-Body Control (WBC)**이 필수적이다. 전통적인 WBC는 로봇의 동역학 모델을 기반으로 최적화 문제(Quadratic Programming)를 풀어 토크를 계산하는데, 최근에는 이를 강화학습(RL)과 결합하는 시도가 주를 이룬다.55</p>
<ul>
<li><strong>계층적 RL:</strong> 상위 정책(RL Policy)은 50Hz 정도로 목표 관절 위치나 속도, 혹은 ‘걷기’, ’점프’와 같은 잠재 명령(Latent Command)을 출력한다. 하위의 고속 제어기(WBC 또는 PD 제어기)는 이를 1kHz 이상의 속도로 추종하며 실제 모터 토크를 생성한다.</li>
<li><strong>LeVERB</strong>와 같은 최신 연구는 VLM(System 2)이 생성한 “잠재 동사(Latent Verbs)“를 저수준 제어기(System 1)가 해석하여 전신 동작으로 변환하는 구조를 제안했다. 이를 통해 언어적 지시를 따르면서도 물리적 균형을 잃지 않는 강건한 제어가 가능해졌다.53</li>
</ul>
<h4>3.2.2  Sim-to-Real: 가상에서 현실로의 초고속 전이</h4>
<p>실시간 제어 정책을 학습하기 위해 강화학습을 현실에서 직접 수행하는 것은 위험하고 시간이 많이 걸린다. 따라서 시뮬레이션에서 학습된 정책을 현실로 이식하는 <strong>Sim-to-Real Transfer</strong>가 핵심 기술이다.</p>
<ul>
<li><strong>VIRAL (Vision-based Sim-to-Real):</strong> 최근 발표된 VIRAL 프레임워크는 대규모 분산 강화학습과 시각적 도메인 무작위화(Domain Randomization)를 통해, 휴머노이드 로봇이 시각 정보만으로 복잡한 조작 및 보행(Loco-manipulation)을 수행할 수 있음을 보였다. 시뮬레이션에서 수천 개의 GPU를 사용하여 학습된 정책은 현실 세계의 100Hz 제어 루프 내에서 지연 없이 동작하며, 전문가 수준의 원격 조작 성능에 근접했다.58</li>
<li><strong>RialTo:</strong> “On-the-fly” 프로그래밍 개념을 도입하여, 로봇이 새로운 환경을 스캔하고 시뮬레이션에서 빠르게 강화학습을 수행한 뒤, 이를 즉시 현실에 배포하는 파이프라인을 제시했다. 이는 실시간성을 갖춘 정책을 현장에서 즉석으로 생성해내는 가능성을 보여준다.59</li>
</ul>
<h4>3.2.3  Learning-Augmented MPC: 최적 제어의 가속화</h4>
<p>모델 예측 제어(MPC)는 미래의 상태를 예측하여 최적의 제어 입력을 찾지만, 매 틱(Tick)마다 최적화 문제를 풀어야 하므로 계산 비용이 매우 높다. 이를 해결하기 위해 <strong>Learning-Augmented MPC</strong>는 무거운 최적화 과정을 신경망으로 근사(Approximation)하거나, 불확실한 동역학 모델만을 가우시안 프로세스(GP) 등으로 학습하여 보정한다.60</p>
<ul>
<li><strong>정책 근사(Policy Approximation):</strong> 복잡한 MPC의 입출력 관계를 신경망으로 학습시켜, 실행 시에는 최적화 풀이 대신 신경망의 순전파(Forward Pass)만으로 제어 입력을 생성한다. 이를 통해 수 ms 이내의 연산 시간을 달성하면서도 MPC의 안정성을 유지할 수 있다. 최근 연구에서는 임베디드 마이크로컨트롤러에서도 1kHz 제어가 가능한 수준의 경량화와 속도를 달성했다.60</li>
<li><strong>Meta-Learning Augmented MPC:</strong> 쿼드콥터와 같이 외란에 민감한 시스템을 위해, 메타 러닝(Meta-Learning)을 통해 환경 변화(바람, 하중 변화 등)에 빠르게 적응하는 잔차(Residual) 모델을 학습하고 이를 MPC에 통합하는 연구도 진행되고 있다. 이는 실시간 적응성과 안전성을 동시에 확보하는 방안이다.62</li>
</ul>
<h4>3.2.4  뉴로모픽 컴퓨팅(Neuromorphic Computing): 하드웨어 레벨의 혁신</h4>
<p>소프트웨어 최적화를 넘어, 하드웨어 자체의 지연 시간을 극복하기 위해 폰 노이만 구조를 벗어난 <strong>뉴로모픽(Neuromorphic)</strong> 컴퓨팅의 도입이 가속화되고 있다. **이벤트 카메라(Event Camera)**와 **스파이킹 신경망(SNN)**은 데이터가 변화하는 순간에만 신호를 처리(Event-based)하므로, 극도로 낮은 지연 시간(Latency)과 전력 소모를 보여준다. 드론의 고속 회피 기동이나 로봇 팔의 반사적 파지(Reflexive Grasping)와 같이 마이크로초(<span class="math math-inline">\mu s</span>) 단위의 반응이 생명인 분야에서 뉴로모픽 기술은 기존 딥러닝의 한계를 넘어서는 대안으로 부상하고 있다.64</p>
<h2>4.  결론: 세 가지 난제의 교차점과 공진화</h2>
<p>로봇 공학의 세 가지 난제인 <strong>불확실성</strong>, <strong>비정형성</strong>, <strong>실시간성</strong>은 개별적으로 해결될 수 있는 독립된 문제가 아니다. 비정형 환경은 로봇의 인식 불확실성을 증폭시키고, 증폭된 불확실성을 해결하기 위한 복잡한 확률적 추론 모델(World Models, Diffusion Policy)은 계산 비용을 높여 실시간성을 저해한다. 반대로 실시간성을 확보하기 위해 모델을 단순화하면 불확실성과 비정형성에 대한 대응 능력이 떨어진다.</p>
<p>현대의 SOTA 기술들은 이 세 가지 제약 조건 사이의 트레이드오프(Trade-off)를 깨기 위해 다각도로 진화하고 있다.</p>
<ul>
<li><strong>불확실성</strong>에 대응하기 위해 단순한 예측을 넘어 생성형 모델(Diffusion)과 상상하는 모델(World Models)이 도입되었으며, Conformal Prediction을 통해 안전을 수학적으로 보증하려 한다.</li>
<li><strong>비정형성</strong>을 극복하기 위해 인터넷 규모의 지식을 담은 VLA 모델과 연속적 공간 표현인 Neural Fields가 로봇의 인지 능력을 확장하고 있다.</li>
<li><strong>실시간성</strong>을 준수하기 위해 인간의 신경계를 닮은 계층적 제어(System 1 &amp; 2) 구조와 Sim-to-Real, Learning-Augmented MPC와 같은 기술이 알고리즘과 하드웨어의 간극을 메우고 있다.</li>
</ul>
<p>결국 Embodied AI의 발전은 이 난제들을 우회하는 것이 아니라, 새로운 알고리즘과 하드웨어의 공진화(Co-evolution)를 통해 제약의 임계치를 정면으로 돌파하고 넓혀가는 과정이다. 우리는 지금 모라벡의 역설이 지적했던 ’쉬운 것의 어려움’을 극복하고, 걷고, 보고, 이해하며, 예측 불가능한 세상 속에서 유연하게 반응하는 로봇 지능의 탄생을 목격하고 있다.</p>
<h2>5. 참고 자료</h2>
<ol>
<li>Moravec’s paradox - Wikipedia, <a href="https://en.wikipedia.org/wiki/Moravec&#x27;s_paradox">https://en.wikipedia.org/wiki/Moravec%27s_paradox</a></li>
<li>Contingent Task and Motion Planning under Uncertainty for Human–Robot Interactions, https://www.mdpi.com/2076-3417/10/5/1665</li>
<li>Identifying Uncertainty in Self-Adaptive Robotics with Large Language Models - arXiv, https://arxiv.org/html/2504.20684v2</li>
<li>Is it practical and useful to distinguish aleatoric and epistemic uncertainty? - UC Berkeley Statistics, https://www.stat.berkeley.edu/~aldous/Real_World/ale_epi.html</li>
<li>Epistemic and aleatoric uncertainty: The role of context, https://statmodeling.stat.columbia.edu/2022/02/03/epistemic-and-aleatoric-uncertainty/</li>
<li>Lecture 14: Aleatoric vs. Epistemic Uncertainty, https://www.seas.upenn.edu/~obastani/cis7000/spring2024/docs/lecture14.pdf</li>
<li>Aleatoric Uncertainty From AI-Based 6D Object Pose Predictors for Object-Relative State Estimation - IEEE Xplore, https://ieeexplore.ieee.org/iel8/7083369/11125679/11152309.pdf</li>
<li>From Aleatoric to Epistemic: Exploring Uncertainty Quantification Techniques in Artificial Intelligence - arXiv, https://arxiv.org/html/2501.03282v1</li>
<li>arxiv.org, https://arxiv.org/html/2512.05927v1</li>
<li>Planning under Uncertainty for Robotic Tasks with Mixed Observability - AdaComp@NUS, https://adacomp.org/wp-content/uploads/2016/01/ijrr10.pdf</li>
<li>Helping robots handle uncertainty | MIT News | Massachusetts Institute of Technology, https://news.mit.edu/2015/algorithm-helps-robots-handle-uncertainty-0602</li>
<li>Diff-Dagger: Uncertainty Estimation With Diffusion Policy for Robotic Manipulation - IEEE Xplore, http://ieeexplore.ieee.org/iel8/11127273/11127223/11127730.pdf</li>
<li>Cheng_chi Diffusion Policy Visuomotor Policy Learning via Action Diffusion 2023 | Sukai Huang, https://sino-huang.github.io/posts/cheng_chi-diffusion-policy-visuomotor-policy-learning-via-action-diffusion-2023/</li>
<li>Visuomotor Policy Learning via Action Diffusion, https://diffusion-policy.cs.columbia.edu/diffusion_policy_ijrr.pdf</li>
<li>[2410.14868] Diff-DAgger: Uncertainty Estimation with Diffusion Policy for Robotic Manipulation - arXiv, https://arxiv.org/abs/2410.14868</li>
<li>Visuomotor Policy Learning via Action Diffusion - arXiv, https://arxiv.org/html/2303.04137v5</li>
<li>World Models | Rohit Bandaru, https://rohitbandaru.github.io/blog/World-Models/</li>
<li>danijar/dreamerv3: Mastering Diverse Domains through World Models - GitHub, https://github.com/danijar/dreamerv3</li>
<li>WIMLE: Uncertainty‑Aware World Models with IMLE for Sample‑Efficient Continuous Control, https://openreview.net/forum?id=mzLOnTb3WH</li>
<li>TransDreamerV3: Implanting Transformer In DreamerV3 - arXiv, https://arxiv.org/html/2506.17103</li>
<li>Conformal Prediction for Uncertainty-Aware Planning with Diffusion Dynamics Model - NeurIPS, https://proceedings.neurips.cc/paper_files/paper/2023/file/fe318a2b6c699808019a456b706cd845-Supplemental-Conference.pdf</li>
<li>Safety-Critical Control with Uncertainty Quantification using Adaptive Conformal Prediction - IEEE Xplore, https://ieeexplore.ieee.org/iel8/10644130/10644150/10644391.pdf</li>
<li>[2407.03569] Safety-Critical Control with Uncertainty Quantification using Adaptive Conformal Prediction - arXiv, https://arxiv.org/abs/2407.03569</li>
<li>Safe Planning in Dynamic Environments Using Conformal Prediction | George J. Pappas, https://www.georgejpappas.org/wp-content/uploads/2023/08/Safe_Planning_in_Dynamic_Environments_Using_Conformal_Prediction.pdf</li>
<li>Challenges and Solutions for Autonomous Ground Robot Scene Understanding and Navigation in Unstructured Outdoor Environments: A Review - MDPI, https://www.mdpi.com/2076-3417/13/17/9877</li>
<li>The Future of Robotics in Unstructured Environments - SK Godelius, https://godelius.com/en/the-future-of-robotics-in-unstructured-environments/</li>
<li>Mobile Manipulation in Unstructured Environments - ROAM Lab, https://roam.me.columbia.edu/sites/default/files/content/papers/ram_2011.pdf</li>
<li>An improved YOLO v4 used for grape detection in unstructured environment - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC10374324/</li>
<li>GLOVER: Generalizable Open-Vocabulary Affordance Reasoning for Task-Oriented Grasping, https://corl25-genpriors.github.io/Papers/22_GLOVER_Generalizable_Open_V.pdf</li>
<li>Bench2FreeAD: A Benchmark for End-to-end Navigation in Unstructured Robotic Environments - arXiv, https://arxiv.org/html/2503.12180v2</li>
<li>Humanoid Trajectory Planning Method for Intelligent Vehicles in Unstructured Environment Based on R-CAM Network - IEEE Xplore, https://ieeexplore.ieee.org/iel8/25/11303018/11054310.pdf</li>
<li>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, https://www.cs.utexas.edu/~yukez/cs391r_fall2023/slides/pre_10-24_Ming.pdf</li>
<li>RT-2: Vision-Language-Action Models, https://robotics-transformer2.github.io/</li>
<li>OpenVLA: An open-source vision-language-action model for robotic manipulation. - GitHub, https://github.com/openvla/openvla</li>
<li>OpenVLA: An Open-Source Vision-Language-Action Model, https://proceedings.mlr.press/v270/kim25c.html</li>
<li>Octo: An Open-Source Generalist Robot Policy, https://octo-models.github.io/</li>
<li>Octo: An Open-Source Generalist Robot Policy - arXiv, https://arxiv.org/html/2405.12213v2</li>
<li>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control | by Faryal Saud | Medium, https://medium.com/@faryal.saud/rt-2-vision-language-action-models-transfer-web-knowledge-to-robotic-control-8cffbd038781</li>
<li>RT-2: New model translates vision and language into action - Google DeepMind, https://deepmind.google/blog/rt-2-new-model-translates-vision-and-language-into-action/</li>
<li>arxiv.org, https://arxiv.org/abs/2511.19315</li>
<li>Language-Conditioned Object Detection and Manipulation - Carnegie Mellon University Robotics Institute, https://www.ri.cmu.edu/app/uploads/2023/09/Jain_Ayush_Thesis.pdf</li>
<li>[2405.12213] Octo: An Open-Source Generalist Robot Policy - arXiv, https://arxiv.org/abs/2405.12213</li>
<li>3D Gaussian Splatting in Robotics: A Survey - arXiv, https://arxiv.org/html/2410.12262v2</li>
<li>Neural Fields in Robotics: A Survey - arXiv, https://arxiv.org/html/2410.20220v1</li>
<li>3D-Vision-World/awesome-NeRF-and-3DGS-SLAM - GitHub, https://github.com/3D-Vision-World/awesome-NeRF-and-3DGS-SLAM</li>
<li>Efficient Manipulation-Enhanced Semantic Mapping With Uncertainty-Informed Action Selection - arXiv, https://arxiv.org/html/2506.02286v2</li>
<li>Efficient Manipulation-Enhanced Semantic Mapping With Uncertainty-Informed Action Selection - arXiv, https://arxiv.org/html/2506.02286v1</li>
<li>Active Perception for Grasp Detection via Neural Graspness Field - OpenReview, <a href="https://openreview.net/forum?id=6FYh6gxzPf&amp;referrer=%5Bthe+profile+of+Di+Huang%5D(/profile?id%3D~Di_Huang4)">https://openreview.net/forum?id=6FYh6gxzPf&amp;referrer=%5Bthe%20profile%20of%20Di%20Huang%5D(%2Fprofile%3Fid%3D~Di_Huang4)</a></li>
<li>Differences between hard real-time, soft real-time, and firm real-time? - Stack Overflow, https://stackoverflow.com/questions/17308956/differences-between-hard-real-time-soft-real-time-and-firm-real-time</li>
<li>Real-time computing - Wikipedia, https://en.wikipedia.org/wiki/Real-time_computing</li>
<li>RT-2, Robotic Transformer 2 Review | gracefullight.dev, https://gracefullight.dev/en/2025/08/24/rt-2-review/</li>
<li>Revealing the Challenges of Sim-to-Real Transfer in Model-Based Reinforcement Learning via Latent Space Modeling - arXiv, https://arxiv.org/html/2506.12735v1</li>
<li>LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction | alphaXiv, https://www.alphaxiv.org/overview/2506.13751v2</li>
<li>Robotic computing system and embodied AI evolution: an algorithm-hardware co-design perspective - Journal of Semiconductors, https://www.jos.ac.cn/en/article/doi/10.1088/1674-4926/25020034</li>
<li>Gait Generation and Motion Implementation of Humanoid Robots Based on Hierarchical Whole-Body Control - MDPI, https://www.mdpi.com/2079-9292/14/23/4714</li>
<li>Mobile-TeleVision: Predictive Motion Priors for Humanoid Whole-Body Control, https://mobile-tv.github.io/resources/pmp.pdf</li>
<li>LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction - People @EECS, <a href="https://people.eecs.berkeley.edu/~sastry/pubs/Pdfs%20of%202025/XueLeVERB2025.pdf">https://people.eecs.berkeley.edu/~sastry/pubs/Pdfs%20of%202025/XueLeVERB2025.pdf</a></li>
<li>VIRAL: Visual Sim-to-Real at Scale for Humanoid Loco-Manipulation - arXiv, https://arxiv.org/html/2511.15200v2</li>
<li>Precision home robots learn with real-to-sim-to-real | MIT News, https://news.mit.edu/2024/precision-home-robotics-real-sim-real-0731</li>
<li>Learning-based Approximate Model Predictive Control for an Impact Wrench Tool - arXiv, https://arxiv.org/pdf/2512.16624</li>
<li>Contents - arXiv, https://arxiv.org/html/2512.05008</li>
<li>[Literature Review] Meta-Learning Augmented MPC for Disturbance-Aware Motion Planning and Control of Quadrotors - Moonlight, https://www.themoonlight.io/en/review/meta-learning-augmented-mpc-for-disturbance-aware-motion-planning-and-control-of-quadrotors</li>
<li>Meta-Learning Augmented MPC for Disturbance-Aware Motion Planning and Control of Quadrotors - arXiv, https://arxiv.org/pdf/2410.06325</li>
<li>Real-Time Evolution and Deployment of Neuromorphic Computing at the Edge - OSTI.GOV, https://www.osti.gov/servlets/purl/1844889</li>
<li>Real-Time Neuromorphic Navigation: Guiding Physical Robots with Event-Based Sensing and Task-Specific Reconfigurable Autonomy Stack - arXiv, https://arxiv.org/html/2503.09636v1</li>
<li>The edge of intelligence: How neuromorphic computing is changing AI - Vertiv, https://www.vertiv.com/fr-ca/about/news-and-insights/articles/educational-articles/the-edge-of-intelligence–how-neuromorphic-computing-is-changing-ai/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>