<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:1.5.1 데이터 (Data): 인터넷 스케일의 비디오 데이터와 로봇 조작 데이터셋의 등장.</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>1.5.1 데이터 (Data): 인터넷 스케일의 비디오 데이터와 로봇 조작 데이터셋의 등장.</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 1. Embodied AI의 태동: 뇌를 가진 신체</a> / <a href="index.html">1.5 현대 로봇 지능의 세 가지 기둥 (The Three Pillars)</a> / <span>1.5.1 데이터 (Data): 인터넷 스케일의 비디오 데이터와 로봇 조작 데이터셋의 등장.</span></nav>
                </div>
            </header>
            <article>
                <h1>1.5.1 데이터 (Data): 인터넷 스케일의 비디오 데이터와 로봇 조작 데이터셋의 등장.</h1>
<h2>1. 서론: 모라벡의 역설을 넘어서는 데이터 중심(Data-Centric)의 지평</h2>
<p>인공지능(AI)과 로봇 공학의 융합 과정에서 우리는 오랜 기간 동안 한스 모라벡(Hans Moravec), 로드니 브룩스(Rodney Brooks), 마빈 민스키(Marvin Minsky) 등이 1980년대에 제기한 ’모라벡의 역설(Moravec’s Paradox)’에 갇혀 있었다. “컴퓨터에게 체스나 바둑과 같은 성인 수준의 지능 검사를 수행하게 하는 것은 비교적 쉬우나, 한 살배기 아기 수준의 지각(Perception)과 운동(Mobility) 능력을 부여하는 것은 어렵거나 불가능하다“는 이 역설은 수십 년간 로봇 공학자들을 괴롭혀 온 난제였다.1 인간에게는 수백만 년의 진화를 통해 무의식적으로 내재화된 ‘걷기’, ‘물건 집기’, ’얼굴 인식하기’와 같은 본능적인 감각 운동 능력이, 명시적인 규칙과 수식을 통해 세상을 모델링하려던 기존의 로봇 제어 이론(Control Theory)으로는 구현하기 극도로 어려웠기 때문이다.1</p>
<p>그러나 2020년대에 접어들며 이 견고했던 역설의 벽에 균열이 가기 시작했다. 그 변화의 중심에는 더 정교한 제어 알고리즘이 아닌, 압도적인 규모의 **데이터(Data)**가 자리 잡고 있다. 대규모 언어 모델(LLM)이 인터넷 전체의 텍스트 데이터를 학습하여 인간 수준의 언어 능력을 보여준 것처럼, 로봇 공학계 또한 ’모델 중심(Model-Centric)’에서 ’데이터 중심(Data-Centric)’으로의 거대한 패러다임 전환을 맞이하고 있다.3 이는 단순히 데이터를 많이 모으는 양적 팽창을 넘어, 로봇이 세상을 이해하고 상호작용하는 방식을 근본적으로 재정의하는 질적 도약이다.</p>
<p>본 장에서는 이러한 변화를 이끄는 두 가지 핵심 축인 **인터넷 스케일의 비디오 데이터(Internet-Scale Video Data)**와 **대규모 로봇 조작 데이터셋(Large-Scale Robot Manipulation Datasets)**을 심도 있게 분석한다. 우리는 인간의 일상이 담긴 수억 시간의 비디오 데이터가 어떻게 로봇에게 ’시각적 상식(Visual Commonsense)’과 ’행동의 우선순위(Behavioral Prior)’를 제공하는지, 그리고 전 세계의 연구실이 협력하여 구축한 로봇 조작 데이터가 어떻게 물리적 제어의 정교함을 완성해 가는지 살펴볼 것이다. 이 두 데이터의 결합은 로봇이 실험실이라는 닫힌 세계(Closed World)를 벗어나, 예측 불가능한 현실 세계(Open World)로 나아가는 결정적인 열쇠가 되고 있다.</p>
<h2>2.  인류의 시각적 경험을 학습하다: 인터넷 스케일의 비디오 데이터</h2>
<p>로봇 학습의 가장 큰 병목 현상은 ’데이터의 희소성’이다. 물리적인 로봇을 구동하여 데이터를 수집하는 것은 시간적, 공간적, 비용적 제약이 따르며, 장비의 마모와 안전 문제로 인해 확장이 어렵다. 반면, 인터넷상에는 인간이 촬영하고 업로드한 방대한 양의 비디오 데이터가 존재한다. 유튜브(YouTube)와 같은 플랫폼에 축적된 이 ’비디오 텍스트’는 인간이 도구를 어떻게 사용하는지, 복잡한 환경에서 어떻게 길을 찾는지, 물체와 어떻게 상호작용하는지를 보여주는 거대한 지식의 보고이다.5 최근 연구들은 이러한 수동적(Passive) 인간 비디오 데이터를 로봇의 능동적(Active) 행동 지능으로 전이(Transfer)시키는 데 주력하고 있다.</p>
<h3>2.1  에고센트릭(Egocentric) 비전의 부상: 로봇의 시점에서 세상을 보다</h3>
<p>초기의 컴퓨터 비전 연구는 영화나 TV 쇼와 같은 3인칭 시점(Third-person view)의 영상에 집중했다. 그러나 로봇의 카메라는 대개 로봇의 머리나 손(End-effector)에 장착되어 세상을 1인칭 시점(First-person view, Egocentric)으로 바라본다. 따라서 인간이 웨어러블 카메라를 착용하고 촬영한 에고센트릭 비디오는 로봇의 시각 정보처리와 가장 유사한 형태의 학습 재료를 제공한다.</p>
<h4>2.1.1  Ego4D: 전례 없는 규모와 다양성의 확보</h4>
<p><strong>Ego4D(Egocentric 4D Live Perception)</strong> 프로젝트는 이러한 에고센트릭 데이터의 결정판이라 할 수 있다. 전 세계 9개국, 74개 지역의 931명 참가자가 수집한 3,670시간 분량의 이 비디오 데이터셋은 기존 데이터셋 대비 20배 이상의 규모를 자랑한다.7</p>
<ul>
<li><strong>다양성(Diversity)의 힘:</strong> Ego4D는 특정 실험실이나 서구권 문화에 국한되지 않고, 가사 노동, 여가 활동, 직장 업무, 사회적 상호작용 등 전 지구적인 인간 활동을 포괄한다. 이는 로봇이 특정 조명이나 인테리어에 과적합(Overfitting)되지 않고, 다양한 환경 조건(Visual Perturbation)에 강인한 시각적 표현(Representation)을 학습하는 데 필수적이다.9</li>
<li><strong>지능적 벤치마크:</strong> 단순한 객체 인식을 넘어, Ego4D는 ‘과거의 일화적 기억(Episodic Memory)’, ‘현재의 손-물체 상호작용(Hands and Objects)’, ‘미래의 행동 예측(Forecasting)’ 등 로봇 지능의 핵심 요소를 평가할 수 있는 벤치마크를 제공한다. 예를 들어, 로봇은 영상을 통해 “내가 마지막으로 열쇠를 어디에 두었지?“라는 질문에 답하거나(Querying), “다음에는 소금을 넣어야 한다“는 요리 과정을 예측하는 능력을 기를 수 있다.9</li>
</ul>
<h4>2.1.2  Epic-Kitchens: 미세 조작(Fine-grained Manipulation)의 정수</h4>
<p>Ego4D가 광범위한 일상을 다룬다면, <strong>Epic-Kitchens</strong>는 ’주방’이라는 고도로 복잡한 작업 공간에 집중한다. 주방은 자르기(Cut), 섞기(Mix), 씻기(Wash), 굽기(Bake) 등 도구를 사용하는 복잡한 조작(Manipulation)이 빈번하게 일어나는 공간이다.11</p>
<ul>
<li><strong>행동의 문법(Verb-Noun Pairs):</strong> Epic-Kitchens-100은 100시간 분량의 영상 속에 수만 건의 행동 세그먼트(Action Segments)를 포함하며, 각 행동은 “당근을(Noun) 잡다(Verb)”, “칼로(Noun) 자르다(Verb)“와 같이 동사와 명사의 조합으로 정밀하게 주석 처리되어 있다. 이는 로봇이 ’자르다’라는 행위가 칼, 가위, 톱 등 다양한 도구와 결합될 수 있음을 학습하고, 대상 물체의 물성(Hardness, Texture)에 따라 조작 방식이 달라져야 함을 이해하는 데 도움을 준다.13</li>
<li><strong>비지도 도메인 적응:</strong> 참가자들의 실제 집에서 촬영된 이 데이터는 규격화된 실험실 환경과 달리 조명, 식기, 구조가 제각각이다. 이러한 비정형성(Unstructured nature)은 로봇이 훈련되지 않은 새로운 주방 환경에서도 식기 세척기를 식별하고 조작할 수 있는 일반화 능력을 키우는 데 결정적인 역할을 한다.15</li>
</ul>
<h3>2.2  물리적 상식과 인과관계의 학습: Something-Something</h3>
<p>로봇이 세상을 이해한다는 것은 정지된 사물의 이름을 아는 것을 넘어, 사물 간의 물리적 상호작용과 그로 인한 상태 변화(State Change)를 예측하는 것을 의미한다. <strong>Something-Something V2</strong> 데이터셋은 이러한 ’물리적 상식(Physical Common Sense)’을 주입하기 위해 설계되었다.16</p>
<ul>
<li><strong>템플릿 기반의 물리 행동:</strong> 약 22만 개의 비디오 클립으로 구성된 이 데이터셋은 174개의 클래스를 통해 구체적인 물리적 역학을 묘사한다. 예를 들어 “무언가를 무언가 위에 올려놓기(Putting something on something)”, “무언가를 엎지르기(Spilling something)”, “무언가를 찢어서 둘로 나누기(Tearing something into two pieces)“와 같은 템플릿은 특정 물체(예: 빨간 컵)에 종속되지 않는 범용적인 물리 법칙을 나타낸다.18</li>
<li><strong>상태 변화의 시각화:</strong> 정지 영상 기반의 ImageNet과 달리, Something-Something은 행동의 시간적 흐름(Temporal Dynamics)을 강조한다. 로봇은 이 데이터를 통해 “밀다(Push)“라는 행동이 물체의 위치를 이동시킨다는 인과 관계(Causality)를 학습하고, “떨어뜨리다(Drop)“라는 행동이 중력에 의해 물체를 아래로 이동시킨다는 물리적 직관을 신경망 내부에 형성하게 된다.20</li>
</ul>
<h3>2.3  픽셀에서 행동으로: 비디오를 로봇의 두뇌로 변환하는 알고리즘</h3>
<p>인터넷 비디오 데이터가 아무리 풍부해도, 그것은 로봇의 관절을 제어하는 ‘행동(Action)’ 레이블(Label)이 없는 단순한 픽셀의 나열일 뿐이다. 따라서 이 픽셀 데이터로부터 로봇이 사용할 수 있는 유의미한 표현(Representation)이나 보상 함수(Reward Function)를 추출하는 기술이 필수적이다.</p>
<h4>2.3.1  VIP (Value-Implicit Pre-training): 시각적 보상 함수의 발견</h4>
<p><strong>VIP</strong>는 인간의 비디오를 “목표를 향해 나아가는 최적의 경로“라고 가정하고, 이를 통해 로봇 학습에 필수적인 보상 함수를 자동으로 생성하는 혁신적인 방법론이다.21</p>
<ul>
<li><strong>임베딩 거리 기반 보상:</strong> VIP는 이중 목표 조건부 가치 함수(Dual Goal-Conditioned Value Function) 목표를 사용하여 비디오의 각 프레임을 임베딩 공간에 매핑한다. 이때 목표 상태(Goal State)에 가까운 프레임일수록 임베딩 거리가 가까워지도록 학습된다. 결과적으로, 로봇은 현재 자신의 카메라 이미지와 목표 이미지 사이의 거리를 계산함으로써 “내가 목표에 얼마나 가까워졌는가?“에 대한 조밀한 보상(Dense Reward)을 얻을 수 있다.23</li>
<li><strong>제로샷 강화학습:</strong> VIP로 사전 학습된 표현은 별도의 미세 조정(Fine-tuning) 없이도 다양한 로봇 작업에 즉시 적용 가능한 시각적 보상 함수로 기능한다. 이는 수동으로 보상 함수를 설계해야 했던 강화학습(RL)의 난제를 해결하고, 로봇이 20개 미만의 적은 데모만으로도 새로운 작업을 학습하는 퓨샷(Few-shot) 능력을 가능하게 했다.21</li>
</ul>
<h4>2.3.2  R3M과 VPT: 표현 학습과 역동역학 모델</h4>
<p>**R3M(Representation for Robot Manipulation)**은 Ego4D와 같은 대규모 비디오 데이터에서 시간 대조 학습(Time Contrastive Learning)과 언어 정렬(Language Alignment)을 결합하여 로봇 조작에 특화된 시각 인코더를 학습한다. 이는 로봇이 비디오의 시간적 순서와 작업의 진행 상황을 이해하도록 돕는다.24</p>
<p>한편, OpenAI의 **VPT(Video PreTraining)**는 마인크래프트 게임 환경에서 “인터넷 비디오를 보고 행동을 배우는” 가능성을 입증했다. 연구진은 소량의 레이블 데이터로 **역동역학 모델(Inverse Dynamics Model, IDM)**을 학습시킨 후, 이를 이용해 7만 시간 분량의 유튜브 비디오에 가상의 행동 레이블(Pseudo-label)을 달았다. 이렇게 생성된 데이터로 학습된 에이전트는 다이아몬드 곡괭이를 제작하는 것과 같이 인간도 20분 이상 걸리는 복잡한 순차적 작업(Sequential Decision Making)을 수행해냈다.26 이는 로봇 공학에서도 인터넷 비디오를 통해 복잡한 도구 사용법이나 조립 절차와 같은 행동 우선순위(Behavioral Prior)를 학습할 수 있음을 시사한다.27</p>
<h2>3.  물리적 세계의 경험을 집대성하다: 대규모 로봇 조작 데이터셋</h2>
<p>인터넷 비디오 데이터가 로봇에게 ’보는 법’과 ’작업의 흐름’을 가르친다면, 실제 로봇이 수집한 데이터는 물체와 접촉했을 때의 반발력, 마찰, 미끄러짐과 같은 ’촉각적 현실’과 정교한 ’제어(Control)’를 가르친다. 최근 로봇 학계는 개별 연구실 단위의 파편화된 데이터 수집을 넘어, 전 지구적인 협력을 통해 데이터를 통합하는 ’로봇 공학의 이미지넷(ImageNet) 모멘트’를 만들어내고 있다.</p>
<h3>3.1  Open X-Embodiment: 로봇 데이터의 통합과 RT-X</h3>
<p><strong>Open X-Embodiment</strong> 프로젝트는 구글 딥마인드(Google DeepMind)를 주축으로 전 세계 21개 기관, 34개 로봇 연구실이 협력하여 구축한 역사적인 데이터셋이다. 이는 로봇 학습의 고질적인 문제였던 데이터의 파편화를 해결하고, 범용 로봇 모델(Generalist Robot Model)의 가능성을 타진하기 위해 시작되었다.28</p>
<h4>3.1.1  데이터셋의 구조적 특성과 규모</h4>
<p>Open X-Embodiment 데이터셋은 22종의 서로 다른 로봇 형태(Embodiment)에서 수집된 100만 개 이상의 실제 로봇 궤적(Trajectory)을 포함한다. 여기에는 527개의 기술(Skill)과 160,266개의 작업(Task)이 포함되어 있어, 단일 연구실에서는 도저히 달성할 수 없는 규모와 다양성을 확보했다.28</p>
<table><thead><tr><th><strong>특성</strong></th><th><strong>Open X-Embodiment 통계</strong></th></tr></thead><tbody>
<tr><td><strong>총 궤적 수</strong></td><td>1,000,000+ (1M+)</td></tr>
<tr><td><strong>참여 기관</strong></td><td>21개 기관 (34개 연구실)</td></tr>
<tr><td><strong>로봇 종류</strong></td><td>22종 (Franka, UR5, WidowX, Quadruped 등)</td></tr>
<tr><td><strong>기술(Skills)</strong></td><td>527종</td></tr>
<tr><td><strong>작업(Tasks)</strong></td><td>160,266종</td></tr>
<tr><td><strong>데이터 포맷</strong></td><td>RLDS (Reinforcement Learning Datasets) 표준화</td></tr>
</tbody></table>
<p>이 데이터셋은 단일 팔(Single Arm), 양팔(Bi-manual), 사족 보행 로봇(Quadruped) 등 운동학적 구조(Kinematics)가 상이한 로봇들의 데이터를 <strong>RLDS</strong> 포맷으로 표준화하여 통합했다. 이는 연구자들이 기종에 상관없이 데이터를 로드하고 학습에 활용할 수 있는 기반을 마련했다.29</p>
<h4>3.1.2  교차 체화 전이(Cross-Embodiment Transfer)와 RT-X</h4>
<p>Open X 프로젝트의 가장 중요한 발견은 <strong>RT-X</strong> 모델을 통해 입증된 ‘긍정적 전이(Positive Transfer)’ 현상이다. 기존 통념으로는 형태가 다른 로봇의 데이터를 섞어서 학습하면 서로 간섭(Interference)이 일어나 성능이 저하될 것이라 우려했다. 그러나 <strong>RT-1-X</strong>와 <strong>RT-2-X</strong> 모델의 실험 결과는 정반대였다.</p>
<ul>
<li><strong>큰 데이터가 작은 데이터를 돕는다:</strong> 여러 로봇의 데이터를 함께 학습한 RT-X 모델은 특정 로봇 데이터만으로 학습한 모델보다 더 높은 성능을 보였다. 특히 데이터가 부족한 소규모 로봇 플랫폼의 경우, 대규모 데이터셋과 함께 학습되었을 때 성능이 50% 이상 향상되는 획기적인 결과를 보였다.28</li>
<li><strong>강인함(Robustness)의 증대:</strong> 다양한 배경, 조명, 물체가 포함된 Open X 데이터로 학습된 모델은 학습하지 않은 새로운 환경(Unseen Environment)에서도 높은 성공률을 유지했다. 이는 데이터의 다양성이 로봇의 일반화 능력과 직결됨을 증명한다.</li>
</ul>
<h3>3.2  RT-1과 RT-2: 언어 모델에서 행동 모델로의 진화</h3>
<p>구글의 **RT-1(Robotics Transformer 1)**과 <strong>RT-2</strong>는 이러한 대규모 데이터를 처리하기 위해 트랜스포머(Transformer) 아키텍처를 로봇 제어에 도입한 대표적인 사례다.</p>
<h4>3.2.1  RT-1: 행동의 토큰화(Tokenization)</h4>
<p>RT-1은 13만 개의 에피소드로 구성된 대규모 실세계 로봇 데이터셋(Everyday Robots)으로 학습되었다. RT-1의 핵심 혁신은 로봇의 입출력을 언어 모델처럼 **토큰(Token)**으로 처리했다는 점이다.30</p>
<ul>
<li><strong>입력과 출력:</strong> 로봇의 카메라 이미지는 EfficientNet 기반의 비전 트랜스포머(ViT)를 통해 토큰화되고, 자연어 명령어(“콜라 캔을 집어라”) 또한 텍스트 토큰으로 변환된다. 중요한 것은 출력인 로봇의 행동(팔의 7자유도 움직임, 그리퍼 개폐) 또한 0에서 255 사이의 이산적인(Discrete) 토큰으로 변환된다는 점이다.</li>
<li><strong>실시간 제어:</strong> 이를 통해 로봇 제어 문제는 “이전 토큰들이 주어졌을 때 다음 행동 토큰을 예측하는 문제“로 치환된다. RT-1은 3Hz의 제어 주기를 달성하여 실시간 로봇 제어가 가능함을 입증했다.28</li>
</ul>
<h4>3.2.2  RT-2: VLA(Vision-Language-Action) 모델과 창발적 추론</h4>
<p>RT-2는 여기서 한 발 더 나아가, 인터넷 스케일의 비전-언어 데이터로 사전 학습된 거대 모델(VLM, 예: PaLI-X, PaLM-E)을 로봇 데이터로 미세 조정(Fine-tuning)하는 <strong>VLA(Vision-Language-Action)</strong> 접근 방식을 취했다.32</p>
<ul>
<li><strong>창발적 추론(Emergent Reasoning):</strong> RT-2는 로봇 데이터셋에는 존재하지 않는 놀라운 추론 능력을 보여주었다. 예를 들어 “피곤한 사람에게 줄 음료를 골라라“라는 명령을 받으면, 로봇은 ’피곤함’과 ‘에너지 드링크’ 사이의 의미론적 관계를 인터넷 데이터에서 유추하여 에너지 드링크를 집어 든다.32</li>
<li><strong>추론의 사슬(Chain of Thought):</strong> 심지어 “망치가 없으니 주변에 있는 물건 중 망치 대용으로 쓸 만한 것을 집어서 못을 박아라“와 같은 복잡한 다단계 추론도 수행한다. 로봇은 ’돌(Rock)’이 단단하고 무겁다는 속성을 파악하여 이를 망치 대신 선택한다. 이는 로봇이 단순히 동작을 모방(Mimicry)하는 수준을 넘어, 언어적 의미와 물리적 속성을 통합적으로 이해(Understanding)하고 있음을 시사한다.33</li>
</ul>
<h3>3.3  데이터 수집의 민주화: Bridge Data V2와 RoboTurk</h3>
<p>Open X와 같은 거대 프로젝트가 ’규모’를 담당한다면, 학계에서는 누구나 쉽게 데이터를 수집하고 활용할 수 있도록 하는 ’접근성(Accessibility)’과 ‘품질(Quality)’ 향상 노력이 병행되고 있다.</p>
<h4>3.3.1  Bridge Data V2: 저가형 하드웨어의 표준화</h4>
<p><strong>Bridge Data V2</strong>는 WidowX 250이라는 비교적 저렴한(약 $3,600) 로봇 팔을 사용하여 수집된 데이터셋이다. 24개의 다양한 환경(장난감 주방, 세탁실, 테이블 등)에서 6만 96개의 궤적을 포함한다.35</p>
<ul>
<li><strong>일반화 테스트베드:</strong> 고가의 산업용 로봇 없이도 누구나 데이터 수집에 참여할 수 있는 표준 플랫폼을 제시함으로써, 데이터 중심 로봇 연구의 진입 장벽을 획기적으로 낮췄다. Bridge Data V2는 다양한 기술(Skills)과 환경이 포함되어 있어, 로봇 학습 알고리즘의 일반화 성능을 테스트하는 표준 벤치마크로 자리 잡았다.37</li>
<li><strong>스케일링 법칙의 확인:</strong> 연구 결과, Bridge Data V2의 데이터 양과 모델의 크기를 늘릴수록 성능이 비례하여 향상되는 현상이 관찰되었다. 이는 로봇 분야에서도 ’데이터는 다다익선(The more, the better)’이라는 명제가 유효함을 확인시켜 주었다.36</li>
</ul>
<h4>3.3.2  RoboTurk: 크라우드소싱을 통한 품질과 난이도 확보</h4>
<p>데이터의 양만큼 중요한 것이 품질, 특히 작업의 복잡도다. 스탠퍼드 대학에서 개발한 <strong>RoboTurk</strong>는 스마트폰이나 웹 브라우저를 통해 원격지의 일반인들이 로봇을 실시간으로 조종하여 데이터를 수집할 수 있게 하는 플랫폼이다.38</p>
<ul>
<li><strong>인터페이스의 혁신:</strong> 전용 컨트롤러 없이 아이폰(iPhone)의 모션 센서나 브라우저 인터페이스를 사용하므로, 수천 킬로미터 떨어진 곳의 사용자도 쉽게 로봇을 가르칠 수 있다. 이는 숙련된 연구원만이 데이터를 생성할 수 있다는 제약을 타파했다.40</li>
<li><strong>장기 작업(Long-horizon Tasks)의 해결:</strong> RoboTurk는 ‘세탁물 정리(Laundry Layout)’, ’타워 쌓기(Tower Creation)’와 같이 여러 단계의 순차적 판단이 필요한 고난이도 작업 데이터를 수집하는 데 큰 기여를 했다. 54명의 사용자가 단 22시간의 시스템 가동으로 1,000개 이상의 성공적인 데모를 수집하는 등 크라우드소싱의 효율성을 입증했다.38</li>
</ul>
<h3>3.4  분산 데이터 수집의 미래: DROID</h3>
<p>가장 최근에 등장한 **DROID(Distributed Robot Interaction Dataset)**는 이러한 흐름의 정점을 보여준다. 단일 기관이 아닌 다수의 기관이 동일한 하드웨어와 데이터 수집 프로토콜을 공유하며 분산적으로 데이터를 축적했다. DROID 데이터로 학습된 정책은 기존 데이터셋보다 외부 분포(Out-of-Distribution, OOD) 환경에서 22% 더 높은 성공률을 보였으며, 이는 로봇 학습에 있어 데이터의 지리적, 환경적 다양성이 얼마나 중요한지를 다시 한번 강조한다.42</p>
<h2>4.  심층 분석: 데이터가 로봇의 인지 구조를 어떻게 재편하는가?</h2>
<p>지금까지 살펴본 인터넷 스케일의 비디오 데이터와 대규모 로봇 조작 데이터의 등장은 단순한 아카이브의 확장이 아니다. 이는 로봇이 세상을 인지하고 행동을 생성하는 <strong>’내부 메커니즘’의 근본적인 변화</strong>를 의미한다.</p>
<h3>4.1  암묵적 지식(Tacit Knowledge)의 내재화</h3>
<p>폴라니의 역설(Polanyi’s Paradox)에 따르면, “우리는 우리가 말할 수 있는 것보다 더 많은 것을 알고 있다.” 자전거를 타는 법이나 사람의 얼굴을 알아보는 법을 말로 완벽하게 설명하기란 불가능하다. 과거의 로봇 공학(GOFAI)은 이러한 암묵적 지식을 명시적인 코드(If-Then 규칙)로 변환하려다 실패했다. 그러나 대규모 데이터를 학습한 현대의 신경망 모델은 수만 번의 비디오와 조작 궤적 속에 녹아 있는 암묵적 패턴을 스스로 학습하여 가중치(Weight) 형태로 내재화한다. 이는 모라벡의 역설이 지적했던 ’쉬운 문제(Easy Problems)’들을 데이터의 힘으로 정복해 나가는 과정이다.43</p>
<h3>4.2  시각-언어-행동의 삼위일체 (Multimodal Alignment)</h3>
<p>Ego4D나 Open X 데이터셋은 시각 정보(Pixel)와 언어 정보(Text), 그리고 물리적 행동(Action)이 긴밀하게 연결되어 있다. 이는 로봇에게 <strong>그라운딩(Grounding)</strong> 능력을 부여한다.</p>
<ul>
<li><strong>의미의 물리적 실체화:</strong> 로봇은 사전적 정의를 통해 “부드럽게(Gently)“라는 단어를 배우는 것이 아니다. 수천 개의 “부드럽게 놓기” 비디오와 그때 기록된 힘 센서 데이터, 그리고 감속되는 팔의 궤적 패턴을 통해 “부드럽게“라는 추상적 개념을 물리적 제어 신호로 변환하는 법을 배운다.</li>
<li><strong>지식의 전이:</strong> 인터넷 텍스트에서 학습한 논리적 추론 능력(예: “비가 오면 우산이 필요하다”)과 로봇 데이터에서 학습한 조작 능력(예: “우산을 펼치는 동작”)이 결합되어, VLA 모델은 한 번도 배우지 않은 상황(“비가 오니 로봇이 우산을 챙겨준다”)에서도 적절히 행동할 수 있게 된다.</li>
</ul>
<h3>4.3  시뮬레이션의 한계와 리얼 월드 데이터의 가치</h3>
<p>과거에는 데이터 부족을 해결하기 위해 시뮬레이션(Sim2Real)에 의존했다. 그러나 시뮬레이션은 현실의 복잡한 마찰, 변형, 광학적 특성(Reality Gap)을 완벽히 모사할 수 없다. Bridge Data V2나 DROID와 같은 리얼 월드 데이터셋의 대두는 **“노이즈가 포함된 불완전한 실제 데이터가 깨끗한 시뮬레이션 데이터보다 가치 있다”**는 새로운 합의를 형성하고 있다.44 실제 데이터에 포함된 빛의 반사, 카메라의 흔들림, 로봇의 미세한 떨림 등 ‘현실의 노이즈’ 자체가 로봇에게는 중요한 학습 대상이 되기 때문이다.</p>
<h2>5. 결론 및 요약</h2>
<p>“1.5.1 데이터” 장에서 우리는 로봇 공학이 맞이한 데이터 중심의 혁명을 목격했다. 인류의 시각적 경험이 담긴 <strong>인터넷 스케일의 비디오 데이터</strong>는 로봇에게 세상을 보는 눈과 행동의 인과관계를 가르치는 교과서가 되었고, <strong>대규모 로봇 조작 데이터셋</strong>은 로봇에게 정교한 물리적 기술을 전수하는 실습실이 되었다.</p>
<p>이러한 변화의 핵심 시사점은 다음과 같다:</p>
<ol>
<li><strong>데이터 스케일링 법칙의 입증:</strong> 로봇 공학에서도 데이터의 양과 다양성이 지능의 성능을 결정짓는 핵심 요소임이 확인되었다. Open X와 RT-X의 성공은 서로 다른 로봇 데이터의 통합이 긍정적 전이를 일으킴을 증명했다.</li>
<li><strong>범용 로봇(Generalist Robot)의 가능성:</strong> 과거 특정 작업에 특화된 전용 로봇에서, 다양한 작업을 수행하고 새로운 환경에 적응하는 범용 로봇으로의 진화가 가속화되고 있다. 이는 VLA 모델과 같은 거대 파운데이션 모델(Foundation Model)의 등장으로 더욱 구체화되고 있다.</li>
<li><strong>오픈 사이언스와 협력의 필수성:</strong> 데이터의 규모가 개별 연구실의 역량을 넘어섬에 따라, Ego4D, Open X, DROID와 같은 범국가적 협력 프로젝트와 RoboTurk와 같은 크라우드소싱 플랫폼이 로봇 연구의 표준으로 자리 잡았다.</li>
</ol>
<p>결국, 데이터는 로봇이라는 하드웨어에 지능이라는 생명을 불어넣는 혈액과도 같다. 우리는 지금 기계가 데이터를 통해 인간의 암묵적 지식을 흡수하고, 물리적으로 세상과 소통하기 시작한 역사적인 변곡점 위에 서 있다. 다음 장에서는 이러한 데이터를 실제로 처리하고 학습하여 행동을 생성하는 구체적인 **모델 아키텍처(Transformer, Diffusion Models)**와 학습 방법론에 대해 논의할 것이다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Moravec’s paradox - Wikipedia, <a href="https://en.wikipedia.org/wiki/Moravec&#x27;s_paradox">https://en.wikipedia.org/wiki/Moravec%27s_paradox</a></li>
<li>What is embodied artificial intelligence and why it matters to ITU?, <a href="https://www.itu.int/en/ITU-T/Workshops-and-Seminars/2025/1010/Documents/Wei%20Kai.pdf">https://www.itu.int/en/ITU-T/Workshops-and-Seminars/2025/1010/Documents/Wei%20Kai.pdf</a></li>
<li>[2303.10158] Data-centric Artificial Intelligence: A Survey - arXiv, https://arxiv.org/abs/2303.10158</li>
<li>Data-Centric and Model-Centric AI: Twin Drivers of Compact and Robust Industry 4.0 Solutions - MDPI, https://www.mdpi.com/2076-3417/13/5/2753</li>
<li>Visual Pretraining for Robotic Manipulation | by Michael Atkin - Medium, https://medium.com/@mjatkin/visual-pretraining-for-robotic-manipulation-4d1cab9ff642</li>
<li>Exploring Video Pre-Training (VPT): OpenAI’s Latest Innovation - AiXiA, https://aix-ia.com/exploring-video-pre-training-vpt-openais-latest-innovation/</li>
<li>Egocentric 4D Perception (EGO4D), https://ego4d-data.org/</li>
<li>Ego4D: Around the World in 3,600 Hours of Egocentric Video - IEEE Xplore, https://ieeexplore.ieee.org/iel8/34/11192800/10611736.pdf</li>
<li>[2110.07058] Ego4D: Around the World in 3,000 Hours of Egocentric Video - arXiv, https://arxiv.org/abs/2110.07058</li>
<li>v2.1 Update - Ego4D, https://ego4d-data.org/docs/updates/</li>
<li>A Review of Video-based Learning Approaches for Robot Manipulation - arXiv, https://arxiv.org/html/2402.07127v3</li>
<li>EPIC-KITCHENS Dataset, https://epic-kitchens.github.io/</li>
<li>Leaderboards Open for the 2024 Challenge - EPIC-KITCHENS Dataset, https://epic-kitchens.github.io/2024</li>
<li>EPIC-KITCHENS-100 - Datasets - data.bris - University of Bristol, https://data.bris.ac.uk/data/dataset/2g1n6qdydwa9u22shpxqzp0t8m</li>
<li>epic-kitchens/epic-kitchens-100-annotations: :plate_with_cutlery: Annotations for the public release of the EPIC-KITCHENS-100 dataset - GitHub, https://github.com/epic-kitchens/epic-kitchens-100-annotations</li>
<li>Something-Something V1 &amp; V2 - Dataset - LDM, https://service.tib.eu/ldmservice/dataset/something-something-v1—v2</li>
<li>[1706.04261] The “something something” video database for learning and evaluating visual common sense - arXiv, https://arxiv.org/abs/1706.04261</li>
<li>HuggingFaceM4/something_something_v2 · Datasets at Hugging Face, https://huggingface.co/datasets/HuggingFaceM4/something_something_v2</li>
<li>Something-Something v. 2 Dataset - Qualcomm, https://www.qualcomm.com/developer/software/something-something-v-2-dataset</li>
<li>Brief Review — The “Something Something” Video Database for Learning and Evaluating Visual Common Sense - Sik-Ho Tsang, https://sh-tsang.medium.com/brief-review-the-something-something-video-database-for-learning-and-evaluating-visual-common-e851dd3015a6</li>
<li>VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training (ICLR 2023, SpotLight) - Google Sites, https://sites.google.com/view/vip-rl</li>
<li>VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training | Dinesh Jayaraman, https://www.seas.upenn.edu/~dineshj/publication/ma-2023-vip/</li>
<li>[2210.00030] VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training - arXiv, https://arxiv.org/abs/2210.00030</li>
<li>R3M: A Universal Visual Representation for Robot Manipulation - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v205/nair23a/nair23a.pdf</li>
<li>(PDF) R3M: A Universal Visual Representation for Robot Manipulation - ResearchGate, https://www.researchgate.net/publication/359436519_R3M_A_Universal_Visual_Representation_for_Robot_Manipulation</li>
<li>Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos - OpenAI, https://cdn.openai.com/vpt/Paper.pdf</li>
<li>Learning to play Minecraft with Video PreTraining - OpenAI, https://openai.com/index/vpt/</li>
<li>Open X-Embodiment: Robotic Learning Datasets and RT-X Models, https://robotics-transformer-x.github.io/</li>
<li>google-deepmind/open_x_embodiment - GitHub, https://github.com/google-deepmind/open_x_embodiment</li>
<li>RT-1: Robotics Transformer for real-world control at scale - Google Research, https://research.google/blog/rt-1-robotics-transformer-for-real-world-control-at-scale/</li>
<li>RT-1: ROBOTICS TRANSFORMER FOR REAL-WORLD CONTROL AT SCALE, https://robotics-transformer1.github.io/assets/rt1.pdf</li>
<li>RT-2: Vision-Language-Action Models, https://robotics-transformer2.github.io/</li>
<li>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control - arXiv, https://arxiv.org/abs/2307.15818</li>
<li>RT-2: New model translates vision and language into action - Google DeepMind, https://deepmind.google/blog/rt-2-new-model-translates-vision-and-language-into-action/</li>
<li>BridgeData V2: A Dataset for Robot Learning at Scale, https://rail-berkeley.github.io/bridgedata/</li>
<li>BridgeData V2: A Dataset for Robot Learning at Scale - arXiv, https://arxiv.org/html/2308.12952v3</li>
<li>Bridge Data: Boosting Generalization of Robotic Skills with Cross-Domain Datasets, https://bair.berkeley.edu/blog/2021/11/18/bridge-data/</li>
<li>The RoboTurk Real Robot Dataset, https://roboturk.stanford.edu/dataset_real.html</li>
<li>ROBOTURK Explained: A Scalable Path to Training Smarter Robots - Labellerr, https://www.labellerr.com/blog/roboturk-explained-a-scalable-path-to-training-smarter-robots/</li>
<li>[1811.02790] RoboTurk: A Crowdsourcing Platform for Robotic Skill Learning through Imitation - arXiv, https://arxiv.org/abs/1811.02790</li>
<li>Scaling Robot Supervision to Hundreds of Hours with RoboTurk: Robotic Manipulation Dataset through Human Reasoning and Dexterity, https://rpl.cs.utexas.edu/publications/papers/mandlekar-iros19-scaling.pdf</li>
<li>DROID: A Large-Scale In-the-Wild Robot Manipulation Dataset, https://droid-dataset.github.io/</li>
<li>A Theory-Based AI Automation Exposure Index: Applying Moravec’s Paradox to the US Labor Market - arXiv, https://arxiv.org/html/2510.13369v1</li>
<li>Zero-Shot Visual Generalization in Robot Manipulation - arXiv, https://arxiv.org/html/2505.11719v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>