<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:1.5.3 파운데이션 모델 (Foundation Models): 사전 학습된(Pre-trained) 거대 모델이 로봇에게 제공하는 상식(Common Sense)과 추론 능력.</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>1.5.3 파운데이션 모델 (Foundation Models): 사전 학습된(Pre-trained) 거대 모델이 로봇에게 제공하는 상식(Common Sense)과 추론 능력.</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 1. Embodied AI의 태동: 뇌를 가진 신체</a> / <a href="index.html">1.5 현대 로봇 지능의 세 가지 기둥 (The Three Pillars)</a> / <span>1.5.3 파운데이션 모델 (Foundation Models): 사전 학습된(Pre-trained) 거대 모델이 로봇에게 제공하는 상식(Common Sense)과 추론 능력.</span></nav>
                </div>
            </header>
            <article>
                <h1>1.5.3 파운데이션 모델 (Foundation Models): 사전 학습된(Pre-trained) 거대 모델이 로봇에게 제공하는 상식(Common Sense)과 추론 능력.</h1>
<p>현대 로봇 공학은 기계적 제어의 정밀함을 넘어 인지적 유연성(Cognitive Flexibility)을 확보하는 단계로 진입하고 있다. 그 중심에는 인공지능 분야의 패러다임을 송두리째 바꾼 **파운데이션 모델(Foundation Models)**이 존재한다. 과거의 로봇이 사전에 정의된 규칙과 닫힌 환경(Closed Environment) 내에서만 작동하는 ’자동화 기계’였다면, 파운데이션 모델을 탑재한 로봇은 비정형 환경(Unstructured Environment)에서 상황을 해석하고, 인과관계를 추론하며, 스스로 행동을 계획하는 ’지능형 에이전트’로 진화하고 있다.1</p>
<p>본 절에서는 파운데이션 모델이 어떻게 로봇에게 인간 수준의 ’상식(Common Sense)’과 ‘추론(Reasoning)’ 능력을 부여하는지 심층적으로 분석한다. 대규모 언어 모델(Large Language Models, LLMs)과 비전-언어 모델(Vision-Language Models, VLMs), 그리고 행동까지 포함하는 비전-언어-행동 모델(Vision-Language-Action Models, VLAs)에 이르기까지, 최신 SOTA(State-of-the-Art) 기술들이 로봇의 인지-판단-제어 루프(Loop)를 어떻게 재정의하고 있는지 논한다. 또한, <em>SayCan</em>, <em>PaLM-E</em>, <em>RT-2</em>, <em>Voyager</em> 등 기념비적인 연구들을 통해 이론적 배경과 실증적 효용성을 검토하고, Embodied AI(체화된 인공지능)가 직면한 기술적 난제와 미래 전망을 제시한다.</p>
<h2>1.  로봇 지능의 새로운 지평: 파운데이션 모델의 정의와 역할</h2>
<h3>1.1  파운데이션 모델의 개념적 정의와 로봇 공학적 함의</h3>
<p>파운데이션 모델은 “광범위한 데이터(Broad Data)를 사용하여 대규모로 학습(주로 자기 지도 학습, Self-Supervised Learning)되며, 다양한 다운스트림(Downstream) 작업에 적응할 수 있는 모델“로 정의된다.1 이는 BERT, GPT-3, CLIP 등에서 시작된 개념으로, 특정 작업을 위해 라벨링된 데이터셋을 구축하고 모델을 처음부터 훈련시키던 기존의 지도 학습(Supervised Learning) 패러다임과는 근본적으로 다르다.</p>
<p>로봇 공학의 관점에서 파운데이션 모델의 도입은 ’전문가(Specialist)’에서 ’범용가(Generalist)’로의 전환을 의미한다. 기존 로봇 학습(Robot Learning)은 ‘컵 잡기’, ‘문 열기’ 등 개별 작업마다 별도의 데이터 수집과 모델 훈련을 요구했다. 이는 데이터의 희소성(Data Scarcity)과 높은 비용 문제를 야기했으며, 학습하지 않은 새로운 상황에 대한 적응력을 떨어뜨리는 주원인이었다.1 반면, 파운데이션 모델은 인터넷상의 방대한 텍스트, 이미지, 비디오 데이터를 통해 학습된 일반적인 지식을 로봇에게 전이(Transfer)한다. 이를 통해 로봇은 별도의 훈련 없이도(Zero-shot) 또는 소량의 예시만으로(Few-shot) 새로운 물체를 인식하고 복잡한 명령을 수행할 수 있게 된다.6</p>
<h3>1.2  Embodied AI의 세 가지 기둥 (The Three Pillars)</h3>
<p>파운데이션 모델 기반의 Embodied AI 시스템은 다음의 세 가지 핵심 요소, 즉 데이터(Data), 시뮬레이션(Simulation), 그리고 파운데이션 모델 그 자체의 유기적인 결합으로 지탱된다.1</p>
<table><thead><tr><th><strong>요소 (Pillar)</strong></th><th><strong>역할 및 기능</strong></th><th><strong>로봇 공학적 의의</strong></th></tr></thead><tbody>
<tr><td><strong>데이터 (Data)</strong></td><td>인터넷 규모의 비전-언어 데이터 및 로봇의 물리적 상호작용 데이터</td><td>로봇 데이터의 희소성을 웹 데이터의 풍부함으로 보완. 멀티모달 데이터(시각, 언어, 행동)의 정렬(Alignment)을 통해 상식 학습.1</td></tr>
<tr><td><strong>시뮬레이션 (Simulation)</strong></td><td>물리 엔진 기반의 가상 환경 (Digital Twin)</td><td>현실 세계에서의 위험하고 비용이 많이 드는 데이터 수집을 대체. Sim-to-Real 전이 학습의 토대 마련.3</td></tr>
<tr><td><strong>파운데이션 모델</strong></td><td>인지, 추론, 계획, 제어를 통합하는 중추 신경망</td><td>맥락 이해(Context Awareness), 상식적 추론, 장기 계획 수립 능력 제공. 인지-행동 루프의 핵심 엔진.10</td></tr>
</tbody></table>
<p>이 세 가지 기둥은 상호 보완적이다. 시뮬레이션은 파운데이션 모델을 훈련시키거나 검증하기 위한 안전한 데이터를 제공하고, 파운데이션 모델은 시뮬레이션 에이전트에게 지능을 부여하며, 데이터는 이 모든 과정의 연료가 된다. 특히 최근 연구는 ’의사결정 결합 월드 모델(Decision-Coupled World Models)’과 같은 개념을 통해 시뮬레이션과 생성형 모델의 경계를 허물고 있다.12</p>
<h2>2.  상식(Common Sense)의 주입: 의미론적 추론과 세계 지식</h2>
<p>로봇에게 있어 가장 어려운 과제는 인간에게는 너무나 당연한 ’상식’을 이해하는 것이다. “물을 쏟았으니 닦아라“라는 명령을 수행하기 위해 로봇은 ‘물은 액체이다’, ‘액체를 닦으려면 흡수성이 있는 도구가 필요하다’, ‘스펀지는 흡수성이 있다’, ’망치는 흡수성이 없다’와 같은 일련의 배경 지식을 가지고 있어야 한다. 이를 모라벡의 역설(Moravec’s Paradox)이라 하며, 파운데이션 모델은 텍스트 데이터에 내재된 방대한 세계 지식(World Knowledge)을 통해 이 난제를 해결한다.</p>
<h3>2.1  의미론적 추론 (Semantic Reasoning)</h3>
<p>대규모 언어 모델(LLM)은 텍스트 간의 통계적 연관성을 학습함으로써 물체의 속성, 도구의 기능, 사회적 규범 등을 내재화한다. 이는 로봇이 명시적인 지시 없이도 상황에 맞는 적절한 행동을 유추할 수 있게 한다.</p>
<ul>
<li><strong>속성 및 기능 추론:</strong> RT-2(Robotic Transformer 2)와 같은 모델은 “피곤한 사람을 위한 음료를 가져다줘“라는 명령에 대해, ’피곤함’이라는 상태와 ’에너지 드링크’의 각성 효과를 연결하여 적절한 물체를 선택한다.13 또한, “망치가 없을 때 무엇을 쓸 수 있는가?“라는 질문에 대해 주변의 ’돌멩이’를 집어 드는 창발적(Emergent) 능력을 보여준다. 이는 로봇이 물체의 겉모습(Visual)뿐만 아니라 그 물체가 가진 물리적 속성과 기능적 행동 가능성(Affordance)을 이해하고 있음을 시사한다.15</li>
<li><strong>사회적 맥락 이해:</strong> 로봇은 인간과 공존하는 공간에서 사회적 규범을 준수해야 한다. 파운데이션 모델은 “칼을 사람에게 건네줄 때는 손잡이 쪽을 향하게 해야 한다“거나 “사람이 대화 중일 때는 방해하지 말고 기다려야 한다“는 식의 사회적 상식을 자연어 데이터로부터 학습하여 로봇의 행동 정책(Policy)에 반영할 수 있다.16</li>
</ul>
<h3>2.2  생각의 사슬 (Chain of Thought) 기반 계획 수립</h3>
<p>복잡한 작업(Long-horizon Tasks)을 수행하기 위해 로봇은 최종 목표를 하위 단계들로 분해해야 한다. 파운데이션 모델의 ‘생각의 사슬(Chain of Thought, CoT)’ 프롬프팅 기법은 로봇의 계획(Planning) 능력을 비약적으로 향상시킨다.6</p>
<p>로봇은 CoT를 통해 다음과 같은 논리적 사고 과정을 거쳐 행동을 생성한다:</p>
<ol>
<li><strong>상황 인식:</strong> “현재 식탁 위에 과자 부스러기가 흩어져 있다.”</li>
<li><strong>목표 설정:</strong> “부스러기를 치워야 한다.”</li>
<li><strong>도구 검색:</strong> “치우기 위해서는 빗자루나 진공청소, 또는 스펀지가 필요하다. 주변을 보니 스펀지가 있다.”</li>
<li><strong>행동 계획:</strong> “스펀지를 집는다” <span class="math math-inline">\rightarrow</span> “부스러기가 있는 곳으로 이동한다” <span class="math math-inline">\rightarrow</span> “닦는다” <span class="math math-inline">\rightarrow</span> “스펀지를 헹군다”.</li>
<li><strong>실행:</strong> 각 단계별 제어 명령 생성.</li>
</ol>
<p>RT-2와 같은 모델은 이 모든 과정을 하나의 신경망 내부에서 처리하며, 자연어로 된 사고 과정을 먼저 출력한 후 실제 행동 토큰(Action Token)을 생성한다. 실험 결과, CoT를 적용한 로봇은 그렇지 않은 경우보다 다단계 추론이 필요한 작업에서 월등히 높은 성공률을 보였다.13</p>
<h2>3.  그라운딩(Grounding): 언어와 물리 세계의 연결</h2>
<p>파운데이션 모델이 로봇 공학에 적용될 때 직면하는 가장 큰 도전은 ‘그라운딩(Grounding, 실현 가능성)’ 문제이다. 언어 모델이 아무리 그럴듯한 계획을 내놓더라도, 그것이 실제 물리적 환경에서 로봇이 수행할 수 없는 행동이라면 무용지물이다. 예를 들어, 언어 모델은 “날아서 창문을 닦아라“라고 할 수 있지만, 바퀴 달린 로봇에게는 불가능한 명령이다. 따라서 언어적 지식을 로봇의 물리적 신체(Embodiment) 및 환경과 연결하는 기술이 필수적이다.</p>
<h3>3.1  SayCan: 언어적 유창함과 물리적 가능성의 결합</h3>
<p>구글 리서치(Google Research)가 발표한 논문 <em>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</em> 19에서 제안된 <strong>SayCan</strong>은 이 문제를 해결하기 위한 초기 파운데이션 모델 기반 프레임워크의 전형을 보여준다. SayCan은 LLM의 의미론적 확률과 로봇의 행동 가능성 확률을 수학적으로 결합한다.20</p>
<p>SayCan의 의사결정 수식은 다음과 같이 표현된다:<br />
<span class="math math-display">
\text{Score}(a) = P_{\text{LLM}}(\text{narrative} \vert i) \cdot P_{\text{VBF}}(\text{success} \vert s, a)
</span><br />
여기서 각 항의 의미는 다음과 같다.</p>
<ul>
<li><span class="math math-inline">i</span>: 사용자의 자연어 명령어 (예: “음료수를 쏟았어”).</li>
<li><span class="math math-inline">a</span>: 로봇이 수행 가능한 기술(Skill) 집합 중 하나 (예: “걸레 찾기”, “청소기 가져오기”, “춤추기”).</li>
<li><span class="math math-inline">P_{\text{LLM}}(\text{narrative} \vert i)</span>: <strong>(Say)</strong> 언어 모델이 판단하기에 명령어 <span class="math math-inline">i</span>를 수행하기 위해 행동 <span class="math math-inline">a</span>가 얼마나 적절한지를 나타내는 확률. 예를 들어, 음료수를 쏟았을 때 “걸레 찾기“는 높은 확률을, “춤추기“는 낮은 확률을 가질 것이다.</li>
<li><span class="math math-inline">P_{\text{VBF}}(\text{success} \vert s, a)</span>: <strong>(Can)</strong> 현재 로봇의 상태 <span class="math math-inline">s</span>에서 행동 <span class="math math-inline">a</span>를 성공적으로 수행할 수 있는지를 나타내는 확률. 이는 강화 학습(RL)으로 학습된 가치 함수(Value Function)를 통해 계산된다. 만약 로봇 근처에 걸레가 없다면 이 확률은 0에 수렴한다.</li>
</ul>
<p>SayCan은 이 두 확률의 곱(Product)을 통해, ‘말이 되면서도(Say)’ 동시에 ‘실현 가능한(Can)’ 행동을 선택한다. 이는 언어 모델의 환각을 물리적 현실(Reality)로 억제하는 안전장치 역할을 하며, 로봇이 자신의 능력 범위 내에서 최적의 해답을 찾도록 유도한다.</p>
<h3>3.2  PaLM-E: 체화된 다중 모달 언어 모델 (Embodied Multimodal Language Model)</h3>
<p>SayCan이 언어 모델과 로봇 제어기를 별도의 모듈로 다루었다면, <strong>PaLM-E</strong> (<em>PaLM-E: An Embodied Multimodal Language Model</em> 23)는 이를 하나의 거대 단일 모델로 통합하는 엔드투엔드(End-to-End) 접근 방식을 취한다. PaLM-E는 텍스트뿐만 아니라 이미지, 로봇의 상태 벡터(State Vector), 3D 장면 표현 등 연속적인(Continuous) 센서 데이터를 언어 토큰과 동일한 임베딩 공간으로 투영(Project)하여 처리한다.</p>
<ul>
<li><strong>다중 모달 문장 (Multimodal Sentence):</strong> PaLM-E의 입력은 텍스트와 센서 데이터가 혼합된 형태이다. 예를 들어, <code>Q: &lt;Image_Embedding&gt; 이 사진에서 로봇 팔이 잡아야 할 물체는?</code>과 같은 형태의 입력이 주어진다. ViT(Vision Transformer)가 이미지를 인코딩하여 언어 모델(PaLM)의 입력 공간에 주입하면, LLM은 이를 마치 단어처럼 처리하여 텍스트 답변이나 제어 명령을 생성한다.25</li>
<li><strong>양성 전이 (Positive Transfer):</strong> PaLM-E의 가장 큰 발견 중 하나는 ’전이 학습’의 효과이다. 인터넷상의 일반적인 비전-언어 데이터(VQA 등)로 학습된 지식이 로봇 조작 작업의 성능을 향상시키며, 반대로 로봇 데이터를 학습하는 것이 모델의 일반적인 언어 능력을 저하시키지 않고 오히려 풍부하게 만든다.24</li>
<li><strong>다양한 구현체 지원:</strong> PaLM-E는 단일 모델로 로봇 팔(Manipulator), 모바일 로봇, 드론 등 다양한 하드웨어를 제어할 수 있는 범용성을 보여준다.</li>
</ul>
<p>PaLM-E 562B(5620억 파라미터) 모델은 로봇 모델에서도 스케일링 법칙(Scaling Laws)이 유효함을 입증하였다. 모델의 크기가 커질수록 물리적 추론 능력과 일반화 능력이 비선형적으로 향상되는 현상이 관찰되었다.23</p>
<h2>4.  비전-언어-행동 모델 (Vision-Language-Action Models, VLA)</h2>
<p>PaLM-E가 고수준의 계획이나 텍스트 답변을 생성하는 데 강점이 있다면, 구글 딥마인드(Google DeepMind)의 **RT-2 (Robotic Transformer 2)**는 파운데이션 모델이 직접 로봇의 관절 제어 신호를 출력하도록 설계된 VLA(Vision-Language-Action) 모델이다.14</p>
<h3>4.1  행동을 언어처럼 (Actions as Language)</h3>
<p>RT-2의 혁신적인 아이디어는 로봇의 물리적 행동(Action)을 자연어 텍스트와 동일한 ’토큰(Token)’으로 취급한다는 점이다. 일반적으로 로봇 제어 신호는 연속적인 실수값(예: 모터 전압, 목표 좌표 <span class="math math-inline">x, y, z</span>)이지만, RT-2는 이를 이산화(Discretization)하여 정수 토큰으로 변환한다.</p>
<ul>
<li><strong>토큰화 메커니즘:</strong> 예를 들어, 로봇 팔의 6자유도 움직임과 그리퍼의 개폐를 포함한 행동 공간을 256개의 구간(Bin)으로 나눈다. 그러면 “팔을 앞으로 뻗어라“라는 연속적인 동작은 <code>[12, 128, 55, 1,...]</code>와 같은 일련의 숫자 토큰 시퀀스로 표현된다.</li>
<li><strong>통합 학습:</strong> 이렇게 변환된 행동 토큰은 텍스트 토큰과 구조적으로 동일하므로, 기존의 VLM(Vision-Language Model) 아키텍처(PaLI-X, PaLM-E 기반)를 수정 없이 그대로 사용하여 학습할 수 있다. 모델은 인터넷 텍스트를 읽듯이 로봇의 행동 궤적을 ‘읽고’ 생성한다.13</li>
</ul>
<h3>4.2  공동 미세 조정 (Co-fine-tuning)과 창발적 능력</h3>
<p>RT-2는 대규모 웹 데이터(VQA, 캡션 등)와 로봇의 주행 데이터(RT-X 데이터셋 등)를 섞어서 공동 미세 조정(Co-fine-tuning)을 수행한다. 이 과정에서 웹 데이터의 일반 상식과 로봇 데이터의 물리적 제어 능력이 융합된다.</p>
<ul>
<li><strong>새로운 객체 및 명령에 대한 적응:</strong> RT-2는 훈련 데이터에 한 번도 등장하지 않은 물체(예: ‘장난감 공룡’, ‘특이한 깃발’)를 조작하거나, “가장 작은 물건을 집어라”, “숫자 1 위에 사과를 올려라“와 같은 추상적이고 관계적인 명령을 수행할 수 있다.14</li>
<li><strong>추론과 제어의 일원화:</strong> 별도의 추론 모듈 없이, 단일 신경망이 시각 정보를 해석하고(Vision), 사용자의 의도를 파악하며(Language), 즉시 제어 신호를 생성(Action)한다. 이는 처리 속도와 시스템 복잡도 측면에서 큰 이점을 제공한다.</li>
</ul>
<p>아래 표는 주요 파운데이션 모델 기반 로봇 시스템의 특징을 비교한 것이다.</p>
<table><thead><tr><th><strong>모델</strong></th><th><strong>기반 모델 (Backbone)</strong></th><th><strong>접근 방식</strong></th><th><strong>출력 형태 (Output)</strong></th><th><strong>주요 특징</strong></th></tr></thead><tbody>
<tr><td><strong>SayCan</strong> 19</td><td>LLM (PaLM, FLAN) + RL Value Function</td><td>파이프라인 (Modular)</td><td>기술(Skill) 선택</td><td>언어적 확률과 물리적 성공 확률의 결합. 고수준 계획 수립에 집중.</td></tr>
<tr><td><strong>PaLM-E</strong> 23</td><td>LLM (PaLM) + ViT</td><td>End-to-End Multimodal</td><td>텍스트 (Plan or Answer)</td><td>연속적 센서 데이터를 언어 공간에 임베딩. 다중 모달 추론 및 양성 전이 효과.</td></tr>
<tr><td><strong>RT-2</strong> 14</td><td>VLM (PaLI-X, PaLM-E)</td><td><strong>VLA</strong> (Vision-Language-Action)</td><td><strong>행동 토큰 (Action Tokens)</strong></td><td>행동을 언어 토큰화하여 통합 학습. 웹 지식을 로봇 제어로 직접 전이.</td></tr>
</tbody></table>
<h2>5.  시뮬레이션 환경에서의 자율 진화: 평생 학습(Lifelong Learning) 에이전트</h2>
<p>물리적 로봇뿐만 아니라, 가상 세계(Metaverse/Simulation) 또한 파운데이션 모델의 강력한 실험장이다. 특히 마인크래프트(Minecraft)와 같은 개방형(Open-ended) 샌드박스 게임은 무한에 가까운 상태 공간과 작업 종류를 제공하여, 에이전트의 일반화 능력과 장기 계획 능력을 테스트하는 데 최적화되어 있다. 이 분야의 기념비적 연구인 <strong>Voyager</strong>를 살펴본다.30</p>
<h3>5.1  Voyager: 코드로 세상을 탐험하는 에이전트</h3>
<p>Voyager는 GPT-4를 기반으로 구축된 최초의 LLM 기반 평생 학습 에이전트이다. 기존의 강화 학습(RL) 에이전트들이 픽셀 입력을 받아 조이스틱 조작과 같은 저수준 행동을 출력하며 수백만 번의 시행착오(Trial and Error)를 겪어야 했던 것과 달리, Voyager는 **실행 가능한 코드(Executable Code)**를 행동의 단위로 사용한다.</p>
<p>Voyager의 아키텍처는 다음 세 가지 핵심 모듈로 구성된다 33:</p>
<ol>
<li><strong>자동 커리큘럼 (Automatic Curriculum):</strong> 에이전트의 현재 상태(소지 아이템, 주변 지형 등)를 분석하여, GPT-4가 스스로 “다음에 무엇을 배울지“를 결정한다. 예를 들어, “나무를 캤으니 이제 돌 곡괭이를 만들어보자“라거나, “사막에 있으니 선인장을 채집하자“는 식으로 탐험의 난이도를 점진적으로 높여가는 커리큘럼을 생성한다.33</li>
<li><strong>반복적 프롬프팅 (Iterative Prompting) 및 코드 생성:</strong> 특정 작업을 수행하기 위해 에이전트는 Mineflayer API를 활용한 제어 코드를 작성한다. 만약 코드 실행 중 오류가 발생하거나 목표를 달성하지 못하면, GPT-4는 에러 메시지와 환경의 피드백을 분석하여 코드를 수정(Self-correction)한다.</li>
<li><strong>기술 라이브러리 (Skill Library):</strong> 성공적으로 검증된 코드는 기술 라이브러리에 벡터 데이터베이스 형태로 저장된다. 이후 유사한 상황(예: “좀비와 싸우는 법“과 “거미와 싸우는 법“의 유사성)이 발생하면, 에이전트는 저장된 기술을 검색(Retrieval)하고 조합하여 더 복잡한 행동을 수행한다.</li>
</ol>
<h3>5.2  성과와 시사점</h3>
<p>Voyager는 인간의 개입 없이 스스로 맵을 탐험하며, 기존 SOTA 에이전트 대비 3.3배 더 많은 고유 아이템을 발견하고, 2.3배 더 긴 거리를 이동했다. 특히, 마인크래프트의 복잡한 기술 트리(Tech Tree)를 따라 나무 도구에서 다이아몬드 도구까지 발전하는 속도가 기존 모델보다 15.3배 빨랐다.33</p>
<p>Voyager의 사례는 파운데이션 모델이 단순히 정적인 지식 베이스가 아니라, 환경과 상호작용하며 새로운 기술을 습득하고 축적해 나가는 ’능동적 학습자’가 될 수 있음을 보여준다. 이는 향후 로봇이 공장에서 출하된 이후에도 가정이나 산업 현장에서 스스로 새로운 작업을 배우는 ’평생 학습(Lifelong Learning) 로봇’으로 발전할 가능성을 시사한다.</p>
<h2>6.  기술적 도전과제와 미래 전망</h2>
<p>파운데이션 모델이 로봇 공학에 가져온 혁신은 명확하지만, 상용화와 완전한 Embodied AGI(체화된 일반 인공지능) 실현을 위해서는 여전히 해결해야 할 난제들이 존재한다.7</p>
<h3>6.1  추론 지연과 실시간 제어 (Latency &amp; Real-time Control)</h3>
<p>거대 모델(수백억~수천억 파라미터)은 추론에 막대한 연산 자원을 소모한다. 로봇 제어, 특히 이족 보행이나 드론 비행과 같이 동역학(Dynamics)이 빠른 시스템은 수 밀리초(ms) 단위의 제어 주파수(Control Frequency)를 요구한다. 클라우드 기반 LLM/VLM은 네트워크 지연(Latency) 문제로 인해 이러한 긴급 상황에 대응하기 어렵다.</p>
<p>이를 해결하기 위해 모델을 경량화하는 지식 증류(Distillation), 엣지 디바이스(Edge Device)에서의 추론 최적화, 또는 **계층적 제어 구조(Hierarchical Control)**가 연구되고 있다.8 계층적 구조란 고수준의 느린 계획(Plan)은 거대 모델이 담당하고, 저수준의 빠른 반사 신경(Reflex)은 가벼운 정책 네트워크가 담당하는 방식이다.2</p>
<h3>6.2  안전성(Safety)과 환각(Hallucination)</h3>
<p>LLM의 고질적인 문제인 환각(Hallucination)은 물리 세계에서 심각한 안전 문제를 야기할 수 있다. 텍스트 생성에서의 오류는 단순히 잘못된 정보 전달에 그치지만, 로봇의 환각은 기물 파손이나 인명 피해로 직결된다.34 “칼을 던져줘“라는 위험한 명령을 수행하거나, 없는 물체를 집으려고 허공을 휘젓는 행동이 그 예이다.</p>
<p>따라서 파운데이션 모델의 출력에 대해 엄격한 물리적/윤리적 제약 조건을 거는 안전 가드레일(Safety Rails) 기술, 불확실성 추정(Uncertainty Estimation), 그리고 인간의 피드백을 통한 강화 학습(RLHF)의 로봇 버전이 필수적으로 요구된다.11</p>
<h3>6.3  데이터의 모달리티 확장</h3>
<p>현재의 파운데이션 모델은 시각과 언어 데이터에 편중되어 있다. 그러나 로봇이 물리 세계를 온전히 이해하고 조작하기 위해서는 <strong>촉각(Touch/Haptic)</strong>, <strong>힘(Force)</strong>, <strong>소리(Audio)</strong>, <strong>열(Thermal)</strong> 등 다양한 감각 정보가 필요하다.10 예를 들어, 설거지를 할 때 접시가 미끄러운지, 물이 얼마나 뜨거운지는 시각만으로는 알 수 없다. 향후 연구는 이러한 이질적인 감각 데이터를 통합하는 **다중 감각 파운데이션 모델(Multisensory Foundation Models)**로 나아가야 한다.</p>
<h3>6.4  데이터 부족과 Sim-to-Real의 고도화</h3>
<p>로봇 데이터는 여전히 부족하다. Google의 RT-X 프로젝트와 같이 여러 기관이 협력하여 로봇 데이터를 공유하는 움직임이 있지만 29, 웹 데이터의 규모에는 미치지 못한다. 따라서 시뮬레이션(Digital Twin)의 물리적 충실도(Fidelity)를 높여 가상 데이터를 생성하고, 이를 현실로 전이하는 Sim-to-Real 기술의 중요성은 더욱 커질 것이다.3 또한, 생성형 AI를 활용하여 시뮬레이션 환경 자체를 자동으로 생성하는 기술도 유망한 분야이다.</p>
<h2>7.  결론</h2>
<p>파운데이션 모델은 로봇 공학의 오랜 꿈이었던 ’일반 목적의 로봇(General Purpose Robot)’을 향한 거대한 도약이다. 사전 학습된 거대 모델은 로봇에게 ’무엇을 해야 하는가(What)’에 대한 상식과 ’어떻게 해야 하는가(How)’에 대한 추론 능력을 동시에 제공함으로써, 로봇을 단순한 기계 장치에서 지적인 파트너로 격상시켰다.</p>
<p>SayCan의 그라운딩, PaLM-E의 다중 모달 통합, RT-2의 행동 토큰화 기술은 로봇이 닫힌 공장을 벗어나 인간의 복잡하고 예측 불가능한 일상 공간으로 들어오기 위한 초석을 다졌다. 비록 실시간성과 안전성이라는 높은 벽이 남아있지만, 데이터, 시뮬레이션, 그리고 파운데이션 모델이라는 세 가지 기둥의 결합은 로봇 지능의 진화 속도를 가속화하고 있다. 이는 곧 Embodied AI가 지향하는 최종 목표, 즉 물리적 신체를 가지고 인간과 교감하며 세상을 변화시키는 **체화된 일반 인공지능(Embodied AGI)**의 실현을 앞당기는 핵심 동력이 될 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>(PDF) Design and Development of Embodied AI Based on …, https://www.researchgate.net/publication/393690304_Design_and_Development_of_Embodied_AI_Based_on_Foundation_Models</li>
<li>Foundation Control Model for General Embodied Intelligence, https://www.ri.cmu.edu/app/uploads/2025/05/MSR_Thesis-1.pdf</li>
<li>Embodied AI Explained: Principles, Applications, and Future …, https://lamarr-institute.org/blog/embodied-ai-explained/</li>
<li>On the Opportunities and Risks of Foundation Models, https://crfm.stanford.edu/assets/report.pdf</li>
<li>Foundation Models in Robotics: Applications, Challenges, and the …, https://arxiv.org/html/2312.07843v1</li>
<li>Real-world robot applications of foundation models: a review, https://www.tandfonline.com/doi/full/10.1080/01691864.2024.2408593</li>
<li>Robot Learning in the Era of Foundation Models: A Survey, https://www.researchgate.net/publication/375925817_Robot_Learning_in_the_Era_of_Foundation_Models_A_Survey</li>
<li>How Embodied Intelligence Is Redefining Industrial Operations …, https://www.techaheadcorp.com/blog/how-embodied-intelligence-redefining-industrial-operation/</li>
<li>Grow AI Virtual Cells: Three Data Pillars and Closed-Loop Learning, https://guomics.com/wp-content/uploads/2025/03/AIVC3pillars-20250306_1752v13.pdf</li>
<li>Will embodied AI create robotic coworkers? - McKinsey, https://www.mckinsey.com/industries/industrials/our-insights/will-embodied-ai-create-robotic-coworkers</li>
<li>Human Centric General Physical Intelligence for Agile … - arXiv, https://arxiv.org/html/2508.11960v2</li>
<li>A Comprehensive Survey on World Models for Embodied AI - arXiv, https://arxiv.org/html/2510.16732v1</li>
<li>RT-2: Vision-Language-Action Models, https://robotics-transformer2.github.io/</li>
<li>(PDF) RT-2: Vision-Language-Action Models Transfer Web …, https://www.researchgate.net/publication/372784419_RT-2_Vision-Language-Action_Models_Transfer_Web_Knowledge_to_Robotic_Control</li>
<li>[2307.15818] RT-2: Vision-Language-Action Models Transfer Web …, https://arxiv.org/abs/2307.15818</li>
<li>Foundation Model Driven Robotics: A Comprehensive Review - arXiv, https://arxiv.org/html/2507.10087v1</li>
<li>Physical Intelligence for Humanist Robots - Perceiving Systems Blog, https://perceiving-systems.blog/en/post/towards-humanist-robots</li>
<li>RT-2: Vision-Language-Action Models Transfer Web Knowledge to …, https://robotics-transformer2.github.io/assets/rt2.pdf</li>
<li>Do As I Can, Not As I Say: Grounding Language in Robotic … - arXiv, https://arxiv.org/abs/2204.01691</li>
<li>SayCanPay: Heuristic Planning with Large Language Models using …, https://arxiv.org/abs/2308.12682</li>
<li>Author Interview: SayCan - Do As I Can, Not As I Say … - YouTube, https://www.youtube.com/watch?v=X4S8F3bwuuw</li>
<li>SayCan: Grounding Language in Robotic Affordances, https://say-can.github.io/</li>
<li>arXiv:2303.03378v1 [cs.LG] 6 Mar 2023, https://arxiv.org/abs/2303.03378</li>
<li>PaLM-E: An Embodied Multimodal Language Model, https://proceedings.mlr.press/v202/driess23a/driess23a.pdf</li>
<li>[PDF] PaLM-E: An Embodied Multimodal Language Model, https://www.semanticscholar.org/paper/PaLM-E%3A-An-Embodied-Multimodal-Language-Model-Driess-Xia/38fe8f324d2162e63a967a9ac6648974fc4c66f3</li>
<li>PaLM-E - An Embodied Multimodal Language Model - Google AI Blog, https://www.scribd.com/document/839307669/PaLM-E-An-embodied-multimodal-language-model-Google-AI-Blog</li>
<li>[Quick Review] PaLM-E: An Embodied Multimodal Language Model, https://liner.com/review/palme-embodied-multimodal-language-model</li>
<li>RT 2 | PDF | Robotics | Computing - Scribd, https://www.scribd.com/document/724547947/rt2</li>
<li>RT-2: Vision-Language-Action Models Transfer Web Knowledge to …, https://www.cs.utexas.edu/~yukez/cs391r_fall2023/slides/pre_10-24_Ming.pdf</li>
<li>MineDojo/Voyager: An Open-Ended Embodied Agent with … - GitHub, https://github.com/MineDojo/Voyager</li>
<li>Voyager: An Open-Ended Embodied Agent with Large Language …, https://openreview.net/forum?id=ehfRiF0R3a</li>
<li>Voyager: An Open-Ended Embodied Agent with Large Language …, https://arxiv.org/abs/2305.16291</li>
<li>Voyager | An Open-Ended Embodied Agent with Large Language …, https://voyager.minedojo.org/</li>
<li>Towards Safe and Trustworthy Embodied AI: Foundations, Status …, https://openreview.net/pdf/a3b0eb5349f3c0dd92e21b43b04037add70c669a.pdf</li>
<li>Future Directions Workshop on Embodied Intelligence, <a href="https://basicresearch.defense.gov/Portals/61/Documents/future-directions/Future%20Directions%20on%20Embodied%20Intelligence%20workshop%20report_clean.pdf?ver=eAR-1vdgCheJ069HKJoaag%3D%3D">https://basicresearch.defense.gov/Portals/61/Documents/future-directions/Future%20Directions%20on%20Embodied%20Intelligence%20workshop%20report_clean.pdf?ver=eAR-1vdgCheJ069HKJoaag%3D%3D</a></li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>