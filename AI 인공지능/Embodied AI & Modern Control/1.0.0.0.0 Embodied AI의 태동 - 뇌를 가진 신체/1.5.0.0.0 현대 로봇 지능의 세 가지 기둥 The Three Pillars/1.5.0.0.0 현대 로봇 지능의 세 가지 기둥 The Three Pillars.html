<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:1.5 현대 로봇 지능의 세 가지 기둥 (The Three Pillars)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>1.5 현대 로봇 지능의 세 가지 기둥 (The Three Pillars)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 1. Embodied AI의 태동: 뇌를 가진 신체</a> / <a href="index.html">1.5 현대 로봇 지능의 세 가지 기둥 (The Three Pillars)</a> / <span>1.5 현대 로봇 지능의 세 가지 기둥 (The Three Pillars)</span></nav>
                </div>
            </header>
            <article>
                <h1>1.5 현대 로봇 지능의 세 가지 기둥 (The Three Pillars)</h1>
<h2>1.  서론: 체화된 지능(Embodied AI)의 도래와 패러다임의 전환</h2>
<p>인공지능(AI)의 역사는 물리적 실체가 없는 알고리즘의 발전 과정과, 물리적 세계와 상호작용하는 기계 장치의 발전 과정이 서로 수렴해 온 역사라고 볼 수 있다. 과거의 로봇 공학(Robotics)은 정해진 환경에서 사전에 프로그래밍된 반복 작업을 수행하는 ’자동화(Automation)’에 초점을 맞추었다. 공장 조립 라인의 로봇 팔은 정밀하고 강력했지만, 자신이 다루는 부품이 무엇인지 이해하거나 예기치 못한 장애물에 대처하는 능력은 전무했다. 반면, 컴퓨터 비전이나 자연어 처리와 같은 인공지능 분야는 디지털 세계 내의 데이터 처리에 집중해 왔다. 이 두 흐름이 만나 탄생한 것이 바로 **‘체화된 지능(Embodied AI)’**이다.1</p>
<p>체화된 지능은 지능이 단순히 뇌(소프트웨어)에만 머무르는 것이 아니라, 신체(하드웨어)를 통해 환경과 상호작용하는 과정에서 발현된다는 철학을 바탕으로 한다. 현대 로봇 지능의 핵심은 이 상호작용의 루프를 얼마나 자율적이고 지능적으로 수행하느냐에 달려 있다. 우리는 이 복잡한 지능형 로봇 시스템을 이해하기 위해, 이를 지탱하는 세 가지 핵심 기둥(The Three Pillars)인 <strong>인지(Perception)</strong>, <strong>추론(Reasoning)</strong>, **행동(Action)**으로 나누어 분석한다.1</p>
<p>이 세 가지 기둥은 독립적인 모듈이 아니다. 과거의 ‘Sense-Plan-Act’ 패러다임에서는 이들이 순차적으로 실행되는 별개의 단계였으나, 현대의 딥러닝 기반 로봇 공학에서는 이 경계가 희미해지고 있다. 인지는 행동을 유도하고, 행동은 다시 새로운 인지를 낳으며, 추론은 이 전체 과정을 조율하고 기억과 미래 예측을 연결한다. 특히 2023년 이후 대규모 언어 모델(LLM)과 비전-언어-행동(VLA) 모델의 폭발적인 발전은 로봇이 세상을 이해하고 조작하는 방식에 근본적인 혁명을 일으키고 있다.4 본 장에서는 이 세 기둥 각각의 최신 기술적 진보와 그들이 통합되어 만들어내는 시너지를 방대한 문헌과 최신 연구 결과를 바탕으로 심층적으로 탐구한다.</p>
<h2>2.  제1의 기둥: 인지 (Perception) — 세계의 재구성과 의미의 발견</h2>
<p>로봇에게 있어 인지(Perception)란, 센서를 통해 들어오는 무질서한 데이터(광자, 음파, 압력 등)를 해석 가능한 정보로 변환하는 과정이다. 이는 로봇이 “내가 어디에 있는가?(Localization)”, “내 주변은 어떻게 생겼는가?(Mapping)”, “저 물체는 무엇인가?(Semantic Understanding)“라는 질문에 답하는 능력이다.2 현대 로봇 인지는 단순히 2차원 이미지를 분류하는 것을 넘어, 3차원 공간의 기하학적 구조를 복원하고 그 안에 내재된 의미론적 정보를 추출하는 방향으로 진화하고 있다.</p>
<h3>2.1  기하학적 인지의 혁명: 암시적 표현에서 명시적 표현으로</h3>
<p>로봇이 물리적 공간에서 이동하고 상호작용하기 위해서는 환경의 3차원 지도를 생성하고 자신의 위치를 파악하는 SLAM(Simultaneous Localization and Mapping) 기술이 필수적이다. 전통적인 SLAM은 특징점(Feature Point) 기반의 희소(Sparse) 지도를 사용했으나, 이는 환경의 구체적인 형태를 파악하는 데 한계가 있었다. 최근 딥러닝과 컴퓨터 그래픽스의 결합은 ’신경 3D 표현(Neural 3D Representations)’이라는 새로운 지평을 열었다.5</p>
<h4>2.1.1 신경방사장(NeRF)의 등장과 한계</h4>
<p>2020년 등장한 신경방사장(NeRF: Neural Radiance Fields)은 3차원 공간을 다층 퍼셉트론(MLP)과 같은 신경망의 가중치로 암시적(Implicit)으로 표현하는 방식을 제안했다. NeRF는 입력된 3차원 좌표와 시점 방향에 대해 색상과 밀도를 출력하는 함수로 장면을 모델링하여, 사진과 구별할 수 없을 정도의 고해상도 렌더링을 가능하게 했다.7 이는 로봇이 카메라 이미지만으로 주변 환경을 완벽하게 복원할 수 있다는 가능성을 보여주었다.</p>
<p>그러나 NeRF는 로봇 공학적 관점에서 치명적인 단점을 가지고 있었다. 첫째, 학습과 렌더링 속도가 매우 느려 실시간성이 중요한 자율 주행이나 로봇 조작에 적용하기 어려웠다. 둘째, 전체 장면이 하나의 신경망에 압축되어 있어, 환경의 변화(예: 물체의 이동)가 발생했을 때 부분적인 업데이트가 불가능하고 전체를 재학습해야 했다.7</p>
<h4>2.1.2 차원 가우시안 스플래팅(3DGS): 로봇 인지의 새로운 표준</h4>
<p>이러한 NeRF의 한계를 극복하기 위해 등장한 **3차원 가우시안 스플래팅(3D Gaussian Splatting, 3DGS)**은 2023년 이후 로봇 인지 분야를 휩쓸고 있다. 3DGS는 장면을 신경망 속에 숨기는 대신, 수백만 개의 3차원 가우시안(Gaussian) 타원체들의 집합으로 명시적(Explicit)으로 표현한다.8 각 가우시안은 위치, 크기, 회전, 색상, 불투명도 정보를 가지고 있으며, 이를 2차원 화면에 투영(Splatting)하여 이미지를 생성한다.</p>
<p>3DGS가 로봇 공학에 가져온 혁신은 다음과 같다:</p>
<ol>
<li><strong>실시간성:</strong> 타일 기반 래스터화(Tile-based Rasterizer) 기술을 통해 100 FPS 이상의 렌더링 속도를 제공한다. 이는 로봇이 움직이는 동안 즉각적으로 주변 환경의 시각적 피드백을 생성할 수 있음을 의미한다.8</li>
<li><strong>명시적 구조:</strong> 포인트 클라우드와 유사하게 가우시안들이 공간상에 분포하므로, 특정 영역의 가우시안을 이동시키거나 삭제하는 등의 국소적 수정이 가능하다. 이는 동적인 환경에서 움직이는 물체를 추적하거나 지도를 업데이트하는 데 유리하다.9</li>
<li><strong>학습 효율:</strong> NeRF에 비해 학습 속도가 수십 배 빠르며, 로봇이 탐험과 동시에 지도를 학습하는 온라인 SLAM에 적합하다.</li>
</ol>
<h4>2.1.3 차세대 SLAM: SplaTAM과 협업 매핑</h4>
<p>3DGS 기술은 SLAM 시스템과 결합하여 <strong>SplaTAM</strong>과 같은 차세대 매핑 시스템을 탄생시켰다.10 SplaTAM은 RGB-D 센서 데이터를 활용하여 3D 가우시안 지도를 실시간으로 생성하고 추적한다. 기존의 메쉬(Mesh)나 복셀(Voxel) 기반 방식과 달리, SplaTAM은 실루엣 마스크를 활용하여 장면의 밀도를 우아하게 포착하고, 새로운 가우시안을 추가함으로써 구조적으로 지도를 확장한다. 실험 결과, SplaTAM은 카메라 위치 추정 및 지도 생성 성능에서 기존 방식 대비 최대 2배의 성능 향상을 보여주었다.10</p>
<p>더 나아가 다수의 로봇이 협업하여 지도를 작성하는 <strong>협업 가우시안 스플래팅 SLAM (Collaborative Gaussian Splatting SLAM)</strong> 연구도 활발하다. 이는 여러 로봇이 각자 수집한 가우시안 하위 지도(Sub-map)를 중앙 서버나 분산 네트워크를 통해 통합하는 기술이다. 이 과정에서 각 로봇의 위치 추정 오차를 줄이고, 통신 대역폭을 효율적으로 관리하며, 서로 다른 시점에서 본 장면의 일관성을 유지하는 것이 핵심 과제이다.9 이러한 기술은 재난 현장이나 대규모 물류 창고와 같이 단일 로봇으로는 커버하기 어려운 환경에서 필수적이다.</p>
<table><thead><tr><th><strong>특성</strong></th><th><strong>NeRF (Neural Radiance Fields)</strong></th><th><strong>3D Gaussian Splatting (3DGS)</strong></th><th><strong>SplaTAM (Splat, Track &amp; Map)</strong></th></tr></thead><tbody>
<tr><td><strong>표현 방식</strong></td><td>암시적 (Implicit MLP)</td><td>명시적 (Explicit Gaussians)</td><td>명시적 가우시안 + 밀도 마스크</td></tr>
<tr><td><strong>렌더링 속도</strong></td><td>느림 (수 초~수 분/프레임)</td><td>매우 빠름 (실시간, &gt;100fps)</td><td>실시간</td></tr>
<tr><td><strong>지도 업데이트</strong></td><td>전체 재학습 필요 (어려움)</td><td>국소적 수정 가능</td><td>온라인으로 가우시안 추가/수정 용이</td></tr>
<tr><td><strong>주요 용도</strong></td><td>고품질 오프라인 시각화</td><td>실시간 렌더링, 동적 장면</td><td>실시간 로봇 SLAM, 네비게이션</td></tr>
<tr><td><strong>로봇 적합성</strong></td><td>낮음 (높은 연산 비용)</td><td>높음 (빠른 속도, 수정 용이)</td><td>매우 높음 (정확한 포즈 추정 및 매핑)</td></tr>
</tbody></table>
<h3>2.2  의미론적 인지: 언어로 세상을 검색하다</h3>
<p>로봇이 단순히 공간의 형태(Geometry)만 파악해서는 인간과 소통하거나 복잡한 임무를 수행할 수 없다. “컵을 가져와“라는 명령을 수행하려면 ’컵’이라는 객체의 의미(Semantics)와 위치를 알아야 한다. 과거에는 YOLO와 같은 객체 인식기가 제한된 클래스(약 80개)만을 분류했지만, 최근에는 거대 언어-비전 모델(VLM)을 활용하여 <strong>개방형 어휘(Open-Vocabulary)</strong> 인식이 가능해졌다.14</p>
<h4>2.2.1 LERF: 언어 임베디드 방사장</h4>
<p>**LERF(Language Embedded Radiance Fields)**는 기하학적 복원을 넘어 의미론적 정보를 3차원 공간에 심는 획기적인 기술이다.15 LERF는 CLIP(Contrastive Language-Image Pre-training)과 같은 사전 학습된 VLM의 임베딩 벡터를 NeRF나 3DGS와 같은 3D 장면에 함께 학습시킨다.</p>
<p>이 기술의 핵심은 다음과 같다:</p>
<ol>
<li><strong>밀집된 언어 필드(Dense Language Field):</strong> 공간의 모든 지점(X, Y, Z)이 텍스트 임베딩과 연관된다. 즉, 3차원 공간 자체가 검색 가능한 데이터베이스가 된다.</li>
<li><strong>제로샷(Zero-Shot) 쿼리:</strong> 로봇에게 별도의 학습 없이 “전기(Electricity)“라는 추상적인 단어를 입력하면, LERF는 전원 콘센트나 전자기기의 위치를 활성화(Activation) 맵으로 보여준다. “오래된 컵”, “빨간색 손잡이“와 같은 구체적인 자연어 묘사도 즉각적으로 이해한다.16</li>
<li><strong>롱테일(Long-Tail) 객체 인식:</strong> 학습 데이터에 거의 등장하지 않는 희귀한 객체(예: “월리(Waldo)”)나 특정 상품명까지도 인식할 수 있다. 이는 CLIP이 인터넷상의 방대한 이미지-텍스트 쌍으로 학습되었기 때문이다.</li>
<li><strong>다중 스케일(Multi-scale) 분석:</strong> LERF는 다양한 크기의 이미지 패치를 분석하여, 객체의 전체적인 모습뿐만 아니라 “컵의 손잡이“와 같은 세부적인 부분(Part-level) 쿼리도 처리할 수 있다.17</li>
</ol>
<p>로봇 팔 시스템인 <strong>LERF-TOGO</strong>는 이를 활용하여 “손잡이를 잡아(Grasp the handle)“와 같은 언어 명령을 수행한다. 3D 공간상에서 ’손잡이’에 해당하는 부분의 의미론적 분포를 파악하고, 그에 맞는 그립 포즈를 생성함으로써 언어 명령을 물리적 행동으로 연결한다.17 이는 인지가 단순한 ’봄(Seeing)’을 넘어 ’이해함(Understanding)’으로 진화했음을 보여준다.</p>
<h3>2.3  인지의 난제: 편향과 투명성</h3>
<p>현대 로봇 인지가 비약적으로 발전했지만, 여전히 해결해야 할 과제들이 존재한다. 학습 데이터에 내재된 편향(Bias)은 로봇의 인식 능력에 영향을 미칠 수 있다. 예를 들어, 서구권 가정 환경 데이터로 주로 학습된 로봇은 아시아나 아프리카의 가정 환경에서 물체를 제대로 인식하지 못할 수 있다.2 또한, 딥러닝 기반의 인지 모델은 ’블랙박스’와 같아서, 로봇이 왜 특정 물체를 컵으로 인식했는지, 혹은 왜 장애물을 보지 못했는지에 대한 설명 가능성(Explainability)과 투명성(Transparency)이 부족하다. 이는 안전이 중요한 로봇 애플리케이션에서 신뢰성 문제로 직결된다.2</p>
<h2>3.  제2의 기둥: 추론 (Reasoning) — 로봇의 인공 전두엽</h2>
<p>인지된 정보는 행동으로 옮겨지기 전에 반드시 처리와 판단의 과정을 거쳐야 한다. 이것이 바로 **추론(Reasoning)**이다. 과거의 로봇은 유한 상태 기계(Finite State Machine)나 사전에 정의된 규칙(Rule-based System)에 따라 움직였다. 그러나 현대 로봇은 **대규모 언어 모델(LLM)**을 두뇌로 활용하여 불확실한 상황에서 일반 상식(Common Sense)을 기반으로 유연하게 계획을 수립하고 문제를 해결한다.2</p>
<h3>3.1  LLM: 로봇의 가상 전두엽 (Virtual Prefrontal Cortex)</h3>
<p>신경과학적으로 인간의 전두엽(Prefrontal Cortex, PFC)은 고차원적인 인지 기능, 즉 계획 수립, 의사결정, 사회적 행동 조절, 복잡한 목표의 하위 목표 분해 등을 담당한다. 흥미롭게도 최근 연구들은 LLM이 로봇 아키텍처 내에서 이와 유사한 역할을 수행하고 있음을 시사한다.20</p>
<p>LLM 기반의 로봇 추론 시스템은 다음과 같은 뇌의 기능적 구조를 모방한다:</p>
<ul>
<li><strong>배외측 전두엽(DLPFC) 역할 - 작업 분해:</strong> 사용자가 “아침 식사를 준비해줘“라는 모호하고 높은 수준의 명령을 내리면, LLM은 이를 “주방으로 이동 -&gt; 냉장고 열기 -&gt; 계란과 우유 꺼내기 -&gt; 조리 도구 준비 -&gt; 요리하기 -&gt; 서빙하기“와 같은 구체적이고 순차적인 하위 작업(Sub-goal)으로 분해한다. 이는 전두엽의 계획 기능과 정확히 일치한다.23</li>
<li><strong>시스템 1과 시스템 2의 통합:</strong> 노벨상 수상자 대니얼 카너먼이 제시한 이중 과정 이론(Dual Process Theory)에 따르면, 인간의 사고는 빠르고 직관적인 ’시스템 1’과 느리고 논리적인 ’시스템 2’로 나뉜다. 로봇 공학에서 <strong>시스템 1</strong>은 감각 운동 제어(Sensorimotor Control)와 같은 즉각적인 반응을 담당하며(예: 넘어지려 할 때 균형 잡기), 이는 주로 소형 신경망이나 고전 제어 이론이 맡는다. 반면, <strong>시스템 2</strong>는 LLM이 담당하여 장기적인 계획과 논리적 추론, 예외 상황 처리를 수행한다.21 이 두 시스템의 유기적인 결합은 로봇이 민첩하면서도 똑똑하게 행동할 수 있게 한다.</li>
<li><strong>기능적 네트워크:</strong> 최신 연구에 따르면 LLM 내부에서도 인간 뇌의 기능적 네트워크와 유사한 패턴이 발견되며, 특정 뉴런 그룹을 마스킹하면 추론 능력이 손상되는 현상이 관찰된다.24 이는 LLM이 단순한 통계적 앵무새가 아니라, 복잡한 인지 기능을 수행하는 구조적 기반을 갖추고 있음을 시사한다.</li>
</ul>
<h3>3.2  추론 아키텍처: 생각의 사슬과 코드 생성</h3>
<p>LLM을 로봇 제어에 활용하는 방식은 크게 두 가지로 나뉜다: 자연어 추론을 통한 계획 수립과 실행 가능한 코드 생성이다.</p>
<h4>3.2.1 생각의 사슬 (Chain of Thought, CoT)</h4>
<p>LLM에게 단순히 답을 요구하는 대신, “단계별로 생각해보라“고 유도하는 ‘생각의 사슬(Chain of Thought)’ 기법은 로봇의 문제 해결 능력을 비약적으로 향상시켰다. 예를 들어, 로봇에게 “책상 위에 엎질러진 물을 닦아라“라고 명령했을 때, CoT를 적용한 로봇은 다음과 같이 추론한다:</p>
<ol>
<li>
<p>“물을 닦으려면 흡수할 수 있는 도구가 필요하다.”</p>
</li>
<li>
<p>“주변을 둘러보니 스펀지와 휴지가 있다.”</p>
</li>
<li>
<p>“물 양이 많으므로 스펀지가 더 적합하다.”</p>
</li>
<li>
<p>“먼저 스펀지를 집어서 물이 있는 곳으로 이동해야 한다.”</p>
</li>
</ol>
<p>이러한 논리적 비약 없는 단계적 추론은 복잡한 상황에서 로봇의 성공률을 높인다.25</p>
<h4>3.2.2 코드로서의 정책 (Code as Policies)</h4>
<p>자연어로 생성된 계획은 로봇이 이해하기에 여전히 모호할 수 있다. 구글의 <strong>“Code as Policies (CaP)”</strong> 연구는 LLM에게 자연어 대신 파이썬(Python) 코드를 생성하게 함으로써 이 문제를 해결했다.26</p>
<ul>
<li><strong>재귀적 정의와 논리 구조:</strong> 프로그래밍 언어는 반복문(<code>for</code>), 조건문(<code>if</code>), 함수 호출을 명확하게 정의할 수 있다. LLM은 “빨간 블록을 모두 파란 상자에 넣어“라는 명령을 <code>for block in red_blocks: robot.pick(block); robot.place(blue_box)</code>와 같은 코드로 변환한다.</li>
<li><strong>도구 사용(Tool Use):</strong> 로봇이 가진 다양한 기능(물체 인식 API, 네비게이션 API, 파지 API)을 미리 정의된 함수 라이브러리로 제공하면, LLM은 이를 적재적소에 호출하는 ‘프로그래머’ 역할을 수행한다.</li>
<li><strong>일반화 능력:</strong> 학습 데이터에 없는 낯선 명령이라도, 이미 알고 있는 코드 블록들의 조합(Composition)을 통해 새로운 해결책을 생성해낼 수 있다. 이는 기존의 모방 학습(Imitation Learning)이 데이터 분포를 벗어난 상황에서 실패하는 것과 대조된다.</li>
</ul>
<h3>3.3  기억과 맥락: 시간적 연속성의 확보</h3>
<p>진정한 지능은 과거를 기억하고 현재의 맥락(Context)을 유지하는 데서 온다. 로봇이 “아까 그 컵 다시 가져와“라는 명령을 이해하려면 ’아까 그 컵’이 무엇인지, 어디에 두었는지를 기억해야 한다.</p>
<ul>
<li><strong>검색 증강 생성(RAG):</strong> 현대 로봇 시스템은 벡터 데이터베이스(Vector Database)를 장기 기억 저장소로 활용한다. 로봇의 경험(이미지, 대화, 행동 로그)은 벡터로 변환되어 저장되며, 필요할 때 유사도 검색을 통해 관련 기억을 소환한다(RAG: Retrieval-Augmented Generation).3</li>
<li><strong>맥락 유지:</strong> 헬스케어 로봇의 경우, 환자의 당뇨 수치 변화나 지난주 의사의 처방 내용을 기억하고 이를 현재의 식단 추천 추론에 반영한다.3 이는 로봇이 단발성 명령 수행 기계가 아니라, 사용자와의 관계와 역사를 이해하는 파트너로 진화하고 있음을 의미한다.</li>
</ul>
<h3>3.4  안전과 윤리: 추론의 가드레일</h3>
<p>강력한 추론 능력은 양날의 검이다. LLM은 때때로 없는 사실을 지어내는 ’환각(Hallucination)’을 겪을 수 있으며, 이는 물리적 로봇에게 치명적인 사고로 이어질 수 있다(예: 독성 물질을 음료로 착각). 따라서 로봇 추론 시스템에는 반드시 안전 검증(Safety Verification) 단계가 필요하다.</p>
<p>최근 연구들은 ’윤리적 지침 튜닝(Ethical Instruction Tuning)’이나 ’인간 개입 신뢰 모델링(Human-in-the-loop Trust Modeling)’을 통해 로봇의 계획이 안전 규범과 윤리적 가이드라인(예: EU AI Act)을 준수하는지 감시한다.18 또한, 로봇의 추론 과정을 시각화하고 설명 가능한 인터페이스를 제공함으로써, 사용자가 로봇의 의도를 파악하고 필요시 개입할 수 있도록 하는 것이 필수적이다.</p>
<h2>4.  제3의 기둥: 행동 (Action) — 물리적 지능의 실현</h2>
<p>인지와 추론이 아무리 뛰어나도, 물리적 세계에 변화를 주지 못하면 로봇은 단순한 컴퓨터에 불과하다. 제3의 기둥인 **행동(Action)**은 로봇이 계획한 바를 실제 모터의 움직임(Torque, Velocity, Position)으로 변환하는 단계이다. 최근 이 분야는 범용 로봇 모델(Generalist Robot Models)의 등장과 시뮬레이션-현실 전이(Sim-to-Real) 기술의 비약적 발전으로 인해 “물리적 지능(Physical Intelligence)“이라는 새로운 차원으로 도약하고 있다.22</p>
<h3>4.1  비전-언어-행동(VLA) 모델: 통합된 신경망</h3>
<p>과거에는 비전 시스템과 제어 시스템이 분리되어 있었다. 그러나 최근에는 거대 언어 모델의 성공 방식을 로봇 제어에 적용한 <strong>VLA(Vision-Language-Action)</strong> 모델이 주류로 부상하고 있다.4 VLA 모델은 이미지와 텍스트를 입력으로 받아, 텍스트 토큰뿐만 아니라 로봇의 팔 관절 각도나 그리퍼의 개폐를 의미하는 **‘행동 토큰(Action Token)’**을 직접 출력한다.</p>
<h4>4.1.1 RT-2와 범용 지식의 전이</h4>
<p>구글 딥마인드의 **RT-2(Robotic Transformer 2)**는 VLA 모델의 효시이다. RT-2는 인터넷의 방대한 텍스트와 이미지로 사전 학습된 대규모 모델을 기반으로, 로봇의 행동 데이터를 미세 조정(Fine-tuning)하여 만들어졌다.25</p>
<p>RT-2의 가장 큰 혁신은 **‘지식 전이(Knowledge Transfer)’**이다. 로봇 데이터셋에 없는 물체나 개념이라도, 웹 데이터에서 학습한 지식을 통해 다룰 수 있다.</p>
<ul>
<li><strong>사례:</strong> “슈퍼맨 피규어를 집어줘“라는 명령에 대해, RT-2는 로봇 훈련 데이터에 슈퍼맨이 없어도 웹 지식을 통해 슈퍼맨의 특징을 파악하고 정확히 집어낸다.</li>
<li><strong>추론과 행동의 결합:</strong> “피곤한 사람을 위한 음료를 골라줘“라는 명령에 대해, RT-2는 에너지 드링크를 식별(추론)하고 이를 집어 드는(행동) 과정을 하나의 신경망 패스(Pass)로 처리한다.25</li>
</ul>
<h4>4.1.2 Octo: 오픈소스 범용 로봇 정책</h4>
<p>RT-2가 폐쇄적인 모델이라면, <strong>Octo</strong>는 오픈소스 진영에서 개발된 트랜스포머 기반 확산(Diffusion) 정책 모델이다.30 80만 개 이상의 로봇 에피소드를 포함하는 ‘Open X-Embodiment’ 데이터셋으로 학습된 Octo는 다양한 로봇 하드웨어(Embodiment)를 아우르는 범용성을 목표로 한다.</p>
<ul>
<li><strong>유연한 아키텍처:</strong> Octo는 자연어 명령, 목표 이미지, 관찰 기록 등 다양한 입력을 토큰화하여 처리한다.</li>
<li><strong>확산 디코더(Diffusion Decoder):</strong> 단순한 결정론적 행동 출력 대신, 확산 모델을 사용하여 멀티모달(Multi-modal) 행동 분포를 생성한다. 이는 불확실한 상황에서 로봇이 여러 가지 가능한 행동 중 최적의 경로를 선택하거나, 교착 상태에서 벗어나는 데 유리하다.</li>
<li><strong>파인튜닝 효율성:</strong> 새로운 로봇 팔이나 센서 구성에 대해, 적은 양의 데이터(약 100개 데모)만으로도 빠르게 적응할 수 있음이 입증되었다.32</li>
</ul>
<table><thead><tr><th><strong>모델</strong></th><th><strong>아키텍처 유형</strong></th><th><strong>행동 생성 방식</strong></th><th><strong>주요 특징</strong></th><th><strong>접근성</strong></th></tr></thead><tbody>
<tr><td><strong>RT-2</strong></td><td>Transformer (VLA)</td><td>이산적 토큰 (Categorical)</td><td>웹 지식 전이, 추론-행동 통합</td><td>비공개 (Google)</td></tr>
<tr><td><strong>Octo</strong></td><td>Transformer + Diffusion</td><td>연속적 분포 (Diffusion)</td><td>다양한 하드웨어 지원, 효율적 파인튜닝</td><td>오픈소스</td></tr>
<tr><td><strong>OpenVLA</strong></td><td>LLaMA 2 기반 VLA</td><td>이산적 토큰</td><td>소비자용 GPU 구동 가능, 빠른 속도</td><td>오픈소스</td></tr>
<tr><td><strong>π0 (PI0)</strong></td><td>VLM + Flow Matching</td><td>연속적 흐름 (Flow)</td><td>장기 작업(Long-horizon) 수행 능력 탁월</td><td>연구 단계</td></tr>
</tbody></table>
<p>[표 1.5.3.1] 주요 범용 로봇 행동 모델 비교 34</p>
<h3>4.2  강화학습과 Sim-to-Real: 가상에서 획득한 초인적 기술</h3>
<p>LLM이나 VLA 모델이 ‘상식적인’ 행동을 잘한다면, 곡예에 가까운 고난도 동작(예: 파쿠르, 펜 돌리기)은 여전히 강화학습(Reinforcement Learning, RL)의 영역이다. 그러나 현실 세계에서 수만 번 넘어지며 배우는 것은 불가능하므로, 시뮬레이션에서 학습한 후 현실로 옮기는 <strong>Sim-to-Real</strong> 기술이 핵심이다.35</p>
<h4>4.2.1 Eureka: LLM이 설계하는 보상 함수</h4>
<p>강화학습의 가장 큰 병목은 로봇에게 무엇이 잘한 행동인지 알려주는 ’보상 함수(Reward Function)’를 설계하는 것이다. 이는 고도의 전문 지식과 수많은 시행착오를 요구한다. NVIDIA의 Eureka는 LLM(GPT-4)을 활용하여 이 과정을 자동화했다.37</p>
<p>Eureka는 인간이 작성한 단순한 목표(예: “펜을 돌려라”)를 입력받아, 복잡한 보상 함수 코드를 생성한다. 그리고 시뮬레이션 결과를 피드백 받아 코드를 반복적으로 수정하고 개선한다. 놀랍게도 Eureka가 생성한 보상 함수는 인간 전문가가 설계한 것보다 더 효율적이었으며, 이를 통해 로봇 손이 펜을 고속으로 회전시키는 ‘펜 스피닝’ 기술을 습득하는 데 성공했다.38</p>
<h4>4.2.2 DrEureka: 도메인 무작위화의 자동화와 요가볼 실험</h4>
<p>Eureka에서 한 단계 진보한 **DrEureka (Domain Randomization Eureka)**는 보상 함수뿐만 아니라 시뮬레이션 환경의 물리 파라미터(마찰력, 질량, 댐핑 등)를 설정하는 도메인 무작위화(Domain Randomization, DR) 과정까지 LLM에게 위임했다.39</p>
<p>DrEureka의 성능을 증명한 결정적인 사례는 “요가볼 위에서 걷는 4족 보행 로봇” 실험이다.</p>
<ul>
<li><strong>도전 과제:</strong> 요가볼 위에서 균형을 잡고 걷는 것은 바닥의 탄성, 공의 공기압, 마찰력 등 미세한 물리적 상호작용이 끊임없이 변하는 극한의 난이도를 가진다.</li>
<li><strong>해결책:</strong> DrEureka는 LLM을 이용해 로봇이 다양한 물리적 조건에서 견딜 수 있도록 시뮬레이션 환경을 스스로 튜닝했다. LLM은 “로봇이 공 위에서 너무 미끄러진다“는 텍스트 로그를 분석하여 “마찰력 계수의 범위를 높여라“와 같은 구체적인 파라미터 조정을 수행했다.</li>
<li><strong>결과:</strong> 시뮬레이션에서만 훈련된 로봇 정책을 현실의 4족 보행 로봇에 이식했을 때, 로봇은 요가볼 위에서 수 분간 균형을 유지하고, 전진 및 후진을 수행했으며, 심지어 사람이 공을 발로 차거나 공기가 빠지는 상황에서도 넘어지지 않고 적응했다.40</li>
</ul>
<p>이 실험은 LLM이 단순히 언어를 처리하는 것을 넘어, 물리적 세계의 역학(Dynamics)을 이해하고 이를 바탕으로 강건한(Robust) 제어 정책을 생성할 수 있는 **‘물리적 지능의 코치’**가 될 수 있음을 시사한다.</p>
<h3>4.3  행동의 과제: 데이터와 정교함</h3>
<p>VLA 모델과 Sim-to-Real 기술은 비약적으로 발전했지만, 여전히 한계는 존재한다. VLA 모델은 웹 데이터의 힘을 빌려 범용성은 높지만, 바늘 구멍에 실을 꿰는 것과 같은 초정밀 조작에는 약한 모습을 보인다. 반면 RL 기반 방식은 정밀하지만 범용성이 떨어진다. 이 두 접근 방식을 결합하여, VLA가 상위 수준의 계획을 세우고 RL이 하위 수준의 정교한 제어를 담당하게 하거나, RL 데이터로 VLA를 미세 조정하는 하이브리드 접근법이 활발히 연구되고 있다.22 또한, ‘지식 격리(Knowledge Insulation)’ 기술을 통해 VLA의 방대한 지식과 로봇의 정밀한 운동 제어 능력(Motor Cortex)을 효율적으로 분리하고 통합하는 아키텍처도 제안되고 있다.22</p>
<h2>5.  결론: 세 기둥의 융합과 미래 (Synthesis &amp; Future)</h2>
<p>지금까지 우리는 현대 로봇 지능을 지탱하는 세 가지 기둥인 인지, 추론, 행동을 각각 깊이 있게 살펴보았다.</p>
<ol>
<li>**인지(Perception)**는 3D Gaussian Splatting과 LERF와 같은 기술을 통해, 세상을 실시간으로 3차원적이고 의미론적으로 재구성하는 ’눈’이 되었다. 로봇은 이제 무엇이 어디에 있는지뿐만 아니라, 그것이 무엇을 의미하는지 이해한다.</li>
<li>**추론(Reasoning)**은 LLM을 통해 인간의 모호한 명령을 논리적 작업으로 분해하고, 과거를 기억하며 미래를 계획하는 ’전두엽’이 되었다. Code as Policies와 같은 기법은 자연어와 기계어 사이의 간극을 메웠다.</li>
<li>**행동(Action)**은 VLA 모델과 DrEureka와 같은 AI 기반 Sim-to-Real 기술을 통해, 가상의 지능을 물리적 세계의 움직임으로 구현하는 ’손과 발’이 되었다.</li>
</ol>
<h3>5.1  통합의 흐름: 모듈식에서 종단간 학습으로</h3>
<p>가장 뚜렷한 추세는 이 세 기둥의 **통합(Integration)**이다. 과거의 ‘Sense-Plan-Act’ 파이프라인은 각 단계가 명시적인 인터페이스로 분리되어 있었고, 한 단계의 오류가 다음 단계로 전파되는 문제가 있었다. 그러나 RT-2나 Octo와 같은 최신 모델들은 인지부터 행동까지를 하나의 거대한 신경망으로 처리하는 종단간(End-to-End) 학습을 지향한다. 이는 데이터의 손실을 최소화하고 시스템 전체의 최적화를 가능하게 한다.</p>
<p>동시에, 거대 모델의 무거움과 실시간 제어의 기민함을 조화시키기 위해, **계층적 아키텍처(Hierarchical Architecture)**도 주목받고 있다. LLM이 느리지만 똑똑한 ’시스템 2’로서 전략을 수립하면, 경량화된 정책 네트워크가 빠르고 반사적인 ’시스템 1’로서 즉각적인 제어를 담당하는 방식이다.21</p>
<h3>5.2  범용 휴머노이드를 향하여</h3>
<p>이러한 기술적 진보는 결국 **범용 휴머노이드 로봇(General-Purpose Humanoid Robot)**의 실현으로 귀결된다. Agility Robotics의 Digit이나 Tesla의 Optimus와 같은 로봇들은 이미 이 세 가지 기둥을 통합하여 물류 창고나 가정에서 다양한 임무를 수행하는 실험을 진행 중이다.19 로봇은 더 이상 반복 작업만 하는 기계가 아니라, 인간과 언어로 소통하고, 낯선 환경을 탐험하며, 도구를 사용하여 문제를 해결하는 지적인 동반자로 진화하고 있다.</p>
<p>물론 해결해야 할 과제는 여전히 산적해 있다. 3DGS의 대규모 맵 관리 문제, LLM의 환각과 안전성 문제, Sim-to-Real의 정교함 한계, 그리고 이 모든 것을 로봇 내부(On-board)의 제한된 컴퓨팅 자원으로 구동해야 하는 에너지 효율성 문제 등이 그것이다.3 그러나 ’인지-추론-행동’이라는 세 기둥의 견고한 발전은 이러한 문제들을 하나씩 해결해 나가는 토대가 될 것이다. 바야흐로 로봇 공학은 기계 공학을 넘어, 인지 과학과 인공지능이 융합된 **물리적 AI(Physical AI)**의 시대로 진입하고 있다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Perception, Reasoning, Action: the New Frontier of Embodied AI - IRIS Unimore, https://iris.unimore.it/retrieve/handle/11380/1271185/406005/Thesis_Landi_Revised.pdf</li>
<li>Perception, Reasoning, Action: The AI Trilogy for the Modern Era - Startupsgurukul, https://startupsgurukul.com/blog/2024/01/07/perception-reasoning-action-the-ai-trilogy-for-the-modern-era/</li>
<li>The Core Pillars of Artificial Intelligence: Perception, Reasoning, Memory, and Action — Insights and a Healthcare Case Study | by Giulio Sistilli | Medium, https://medium.com/@giulio.sistilli/the-core-pillars-of-artificial-intelligence-perception-reasoning-memory-and-action-insights-82fe8926b5a5</li>
<li>Vision-Language-Action Models: Concepts, Progress, Applications and Challenges - arXiv, https://arxiv.org/html/2505.04769v1</li>
<li>Structure from Collision | CVF Open Access, https://openaccess.thecvf.com/content/CVPR2025/papers/Kaneko_Structure_from_Collision_CVPR_2025_paper.pdf</li>
<li>From 2D to 3D Cognition: A Brief Survey of General World Models - arXiv, https://arxiv.org/html/2506.20134v1</li>
<li>AE-NeRF: Augmenting Event-Based Neural Radiance Fields for Non-ideal Conditions and Larger Scenes, https://ojs.aaai.org/index.php/AAAI/article/view/32299/34454</li>
<li>A Survey on Robust Rendering, Restoration, and Enhancement for 3D Low-Level Vision - arXiv, https://arxiv.org/html/2506.16262v2</li>
<li>A Survey on Collaborative SLAM with 3D Gaussian Splatting - ResearchGate, https://www.researchgate.net/publication/397006354_A_Survey_on_Collaborative_SLAM_with_3D_Gaussian_Splatting</li>
<li>SplaTAM: Splat, Track &amp; Map 3D Gaussians for Dense RGB-D SLAM, https://spla-tam.github.io/</li>
<li>SplaTAM.pdf, https://spla-tam.github.io/assets/SplaTAM.pdf</li>
<li>Visual SLAM with 3D Gaussian Splatting and Generalized ICP - IEEE Xplore, https://ieeexplore.ieee.org/document/11086047/</li>
<li>A Survey on Collaborative SLAM with 3D Gaussian Splatting - arXiv, https://arxiv.org/html/2510.23988v1</li>
<li>JeffreyYH/Awesome-Generalist-Robots-via-Foundation-Models: Paper list in the survey paper - GitHub, https://github.com/JeffreyYH/Awesome-Generalist-Robots-via-Foundation-Models</li>
<li>LERF: Language Embedded Radiance Fields, https://www.lerf.io/</li>
<li>LERF: Language Embedded Radiance Fields - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2023/papers/Kerr_LERF_Language_Embedded_Radiance_Fields_ICCV_2023_paper.pdf</li>
<li>Language Embedded Radiance Fields for Zero-Shot Task-Oriented Grasping - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v229/rashid23a/rashid23a.pdf</li>
<li>Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision - arXiv, https://arxiv.org/html/2504.02477v3</li>
<li>AI in Robotics: Computer Vision, NLP &amp; Machine Learning - Roboflow Blog, https://blog.roboflow.com/ai-in-robotics/</li>
<li>An AI Agent for cell-type specific brain computer interfaces - bioRxiv, https://www.biorxiv.org/content/10.1101/2025.09.11.675660v1.full.pdf</li>
<li>(PDF) A Dual Process VLA: Efficient Robotic Manipulation Leveraging VLM - ResearchGate, https://www.researchgate.net/publication/385108368_A_Dual_Process_VLA_Efficient_Robotic_Manipulation_Leveraging_VLM</li>
<li>VLAs that Train Fast, Run Fast, and Generalize Better - Physical Intelligence, https://www.physicalintelligence.company/research/knowledge_insulation</li>
<li>HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective for Disentangling Complex Manipulation - arXiv, https://arxiv.org/html/2506.08296v1</li>
<li>Brain-Inspired Exploration of Functional Networks and Key Neurons in Large Language Models - arXiv, https://arxiv.org/html/2502.20408v1</li>
<li>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, https://openreview.net/forum?id=XMQgwiJ7KSX</li>
<li>Robotic Control via Embodied Chain-of-Thought Reasoning - arXiv, https://arxiv.org/html/2407.08693v2</li>
<li>[Robotics] Code as Policies: Language Model Programs for Embodied Control - Medium, https://medium.com/@amiable_cardinal_crocodile_398/robotics-code-as-policies-language-model-programs-for-embodied-control-ec64b3165bf2</li>
<li>Physical Artificial Intelligence for Powering the Next Revolution in Robotics, https://asmedigitalcollection.asme.org/computingengineering/article/25/12/120809/1225298/Physical-Artificial-Intelligence-for-Powering-the</li>
<li>A Survey of Robot Intelligence with Large Language Models - MDPI, https://www.mdpi.com/2076-3417/14/19/8868</li>
<li>Octo: An Open-Source Generalist Robot Policy, https://www.roboticsproceedings.org/rss20/p090.pdf</li>
<li>Octo: An Open-Source Generalist Robot Policy, https://octo-models.github.io/</li>
<li>An Open-Source Generalist Robot Policy - Octo, https://octo-models.github.io/paper.pdf</li>
<li>Octo] General-purpose robot trained on a large robotics dataset | AI-SCHOLAR, https://ai-scholar.tech/en/articles/large-language-models/octo-generalist-robot</li>
<li>Comparing 5 Pioneering Robotics Foundation Models for ML-Based Control | by Genki Sano (Co-founder &amp; CTO, Telexistence Inc.) | Medium, https://medium.com/@genki-sano/a-practical-comparison-of-five-leading-ml-based-robotics-control-approaches-49e1977dd3ec</li>
<li>Deep Reinforcement Learning of Mobile Robot Navigation in Dynamic Environment: A Review - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC12158180/</li>
<li>Taxonomy and Trends in Reinforcement Learning for Robotics and Control Systems: A Structured Review - arXiv, https://arxiv.org/html/2510.21758v3</li>
<li>Eureka! NVIDIA Research Breakthrough Puts New Spin on Robot Learning | NVIDIA Blog, https://blogs.nvidia.com/blog/eureka-robotics-research/</li>
<li>Eureka | Human-Level Reward Design via Coding Large Language Models, https://eureka-research.github.io/</li>
<li>DrEureka: Language Model Guided Sim-To-Real Transfer - Robotics, https://www.roboticsproceedings.org/rss20/p094.pdf</li>
<li>DrEureka: Language Model Guided Sim-To-Real Transfer eureka-research.github.io/dr-eureka - arXiv, https://arxiv.org/html/2406.01967v1</li>
<li>Simulation to Reality: Robots Now Train Themselves with the Power of LLM (DrEureka), https://www.analyticsvidhya.com/blog/2024/05/sim-to-real-robots-now-train-themselves-dreureka/</li>
<li>Language Model Guided Sim-To-Real Transfer - DrEureka, https://eureka-research.github.io/dr-eureka/</li>
<li>Effective Tuning Strategies for Generalist Robot Manipulation Policies - arXiv, https://arxiv.org/html/2410.01220v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>