<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:1.1.2 정적 데이터셋의 한계: 고정된 이미지/텍스트(Static Data) 학습의 한계와 상호작용(Interaction)을 통한 인과관계 학습의 필요성.</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>1.1.2 정적 데이터셋의 한계: 고정된 이미지/텍스트(Static Data) 학습의 한계와 상호작용(Interaction)을 통한 인과관계 학습의 필요성.</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 1. Embodied AI의 태동: 뇌를 가진 신체</a> / <a href="index.html">1.1 인터넷 AI에서 엠바디드 AI로 (From Internet AI to Embodied AI)</a> / <span>1.1.2 정적 데이터셋의 한계: 고정된 이미지/텍스트(Static Data) 학습의 한계와 상호작용(Interaction)을 통한 인과관계 학습의 필요성.</span></nav>
                </div>
            </header>
            <article>
                <h1>1.1.2 정적 데이터셋의 한계: 고정된 이미지/텍스트(Static Data) 학습의 한계와 상호작용(Interaction)을 통한 인과관계 학습의 필요성.</h1>
<h2>1.  서론: 관찰자 패러다임의 붕괴와 행위자 패러다임의 부상</h2>
<p>인공지능(AI) 연구의 역사는 데이터의 역사와 궤를 같이한다. 지난 10년 동안 우리는 소위 ’스케일링 법칙(Scaling Laws)’이 지배하는 시대를 목격했다. 더 많은 텍스트, 더 많은 이미지, 그리고 더 거대한 컴퓨팅 자원을 투입하면 모델의 성능이 멱법칙(power-law)에 따라 향상된다는 이 경험적 믿음은 거대언어모델(LLM)과 시각-언어 모델(VLM)의 눈부신 성공을 이끌었다. 인터넷이라는 거대한 디지털 아카이브에서 수집된 수조 개의 토큰과 이미지는 AI에게 인간의 언어를 유창하게 구사하고, 복잡한 패턴을 인식하는 능력을 부여했다. 그러나 이러한 성공의 이면에는 근본적이고 치명적인 인식론적 한계가 도사리고 있다. 그것은 바로 현재의 AI가 철저히 ’수동적인 관찰자(Passive Observer)’에 머물러 있다는 점이다.</p>
<p>우리가 학습시키는 정적 데이터셋(Static Datasets)—고정된 텍스트 코퍼스와 스냅샷 이미지들—은 물리적 실체의 ’그림자’일 뿐 실체 그 자체가 아니다. 플라톤의 동굴 비유를 빌리자면, 현재의 생성형 AI 모델들은 동굴 벽에 비친 그림자들의 패턴을 기가 막히게 예측하는 수감자와 같다. 그들은 “불“이라는 단어와 “뜨거움“이라는 단어가 통계적으로 자주 공기(co-occurrence)한다는 사실은 알지만, 실제로 불에 손을 대었을 때 느껴지는 감각적 고통과 물리적 손상의 인과관계를 이해하지 못한다. 이것은 단순한 데이터 양의 문제가 아니다. 이는 데이터의 ’질적 본성’에 관한 문제이다. 정적 데이터는 상관관계(Correlation)의 지도만을 제공할 뿐, 인과관계(Causality)의 구조를 제공하지 못한다.</p>
<p>본 보고서는 정적 데이터셋에 의존하는 현대 AI 모델들이 직면한 구조적 한계를 심층적으로 분석하고, 왜 진정한 지능—특히 물리 세계와 상호작용하는 로봇 지능—이 되기 위해서는 수동적 관찰을 넘어 능동적 상호작용(Active Interaction)으로 나아가야 하는지를 논증한다. 우리는 먼저 정적 데이터가 야기하는 ’허위 상관관계(Spurious Correlation)’와 ’단순성 편향(Simplicity Bias)’의 병리학을 해부하고, 주디아 펄(Judea Pearl)의 인과 계층 이론을 통해 왜 관찰만으로는 인과성을 도출할 수 없는지를 이론적으로 규명할 것이다. 나아가, 최근 발표된 시각-언어 모델들의 물리 추론 실패 사례(ConserveBench 등)를 통해 이러한 이론적 한계가 실제 모델의 성능에 어떻게 나타나는지를 실증적으로 검토한다.</p>
<p>결론적으로, 우리는 지능의 본질이 ’패턴 매칭’이 아닌 ’물리적 실재와의 인과적 상호작용’에 있음을 역설한다. 얀 르쿤(Yann LeCun)이 주창한 ’월드 모델(World Model)’과 로드니 브룩스(Rodney Brooks)의 ‘물리적 기반(Physical Grounding)’ 개념을 통합하여, 미래의 AI는 인터넷의 텍스트를 읽는 것을 넘어, 로봇의 신체를 통해 세상을 밀고, 당기고, 떨어뜨리며 배우는 ’체화된 지능(Embodied Intelligence)’으로 진화해야 함을 제안한다.</p>
<h2>2.  정적 데이터의 병리학: 허위 상관관계와 단순성 편향의 늪</h2>
<h3>2.1  단순성 편향(Simplicity Bias)과 오컴의 면도날의 역설</h3>
<p>딥러닝 모델은 본질적으로 손실 함수(Loss Function)를 최소화하는 방향으로 최적화되는 수학적 기계이다. 이 과정에서 모델은 데이터 내에 존재하는 가장 ‘쉬운’ 특징을 찾아내어 정답과 연결하려는 경향을 보인다. 이를 ’단순성 편향(Simplicity Bias)’이라 한다.1 문제는 정적 데이터셋 내에서 가장 쉽게 발견되는 특징이 종종 우리가 의도한 본질적 속성이 아니라, 우연히 겹친 배경이나 맥락적 요소라는 점이다.</p>
<p>예를 들어, 개(dog)를 분류하도록 훈련된 신경망을 생각해보자. 훈련 데이터셋의 개 사진들이 대부분 야외의 푸른 잔디밭에서 촬영되었다면, 모델은 ’다리 네 개’나 ’귀의 모양’과 같은 복잡한 기하학적 형태를 학습하는 대신, 단순히 이미지 하단의 녹색 텍스처(잔디)를 ’개’라고 판단하는 지름길(Shortcut)을 택한다. 이는 오컴의 면도날 원칙에 따르면 합리적인 선택이다. 잔디 텍스처는 개의 형태보다 정보량이 적고 학습하기 쉽기 때문이다. 그러나 이러한 모델은 실내 카펫 위에 있는 개를 보거나, 잔디밭 위에 있는 고양이를 볼 때 치명적인 오류를 범하게 된다.1</p>
<p>노스캐롤라이나 주립대의 연구진은 이러한 현상이 AI가 “개의 목걸이“와 같은 비본질적 특징에 과도하게 의존하게 만든다는 것을 입증했다. 개를 식별할 때 털이나 귀의 모양보다 목걸이가 더 단순한 특징이기 때문에, AI는 목걸이를 쓴 고양이를 개로 오인하는 오류를 범한다.2 정적 데이터셋은 이러한 ’선택 편향(Selection Bias)’을 내재하고 있으며, 모델이 스스로 데이터에 개입하여 변수를 통제할 수 없기 때문에, 모델은 영원히 이 허위 상관관계의 감옥에 갇히게 된다.</p>
<h3>2.2  텍스처 편향(Texture Bias) 대 형상 편향(Shape Bias): 인간 시각과의 괴리</h3>
<p>컴퓨터 비전 분야에서 정적 데이터 학습의 한계를 가장 극명하게 보여주는 사례는 ’텍스처 편향’이다. 인간의 시각 시스템은 강력한 ’형상 편향(Shape Bias)’을 가지고 있다. 우리는 고양이가 털로 덮여 있든, 금속으로 조각되었든, 연필로 스케치되었든 간에 그 기하학적 윤곽(실루엣)을 통해 그것이 고양이임을 인식한다. 반면, ImageNet과 같은 정적 이미지 데이터셋으로 훈련된 표준 합성곱 신경망(CNN)은 압도적인 ’텍스처 편향’을 보인다.3</p>
<p>Geirhos 등의 기념비적인 연구는 ‘단서 충돌(Cue-Conflict)’ 실험을 통해 이를 증명했다. 연구진은 코끼리의 피부 질감(텍스처)을 가진 고양이 형상의 이미지를 생성하여 AI와 인간에게 제시했다. 인간은 이를 “고양이“라고 분류했다. 그러나 최첨단 딥러닝 모델들은 이를 “코끼리“라고 분류했다.4 이는 모델이 객체의 전체적인 구조나 물리적 경계를 이해하는 것이 아니라, 국소적인 텍스처 패치들의 통계적 집합(Bag of local features)으로 대상을 인식하고 있음을 시사한다.</p>
<p>로봇 공학의 관점에서 이는 재앙에 가깝다. 가정용 로봇이 컵을 집어 들기 위해서는 컵의 표면 무늬가 아니라 컵의 ’손잡이’라는 기하학적 구조를 인식해야 한다. 만약 로봇이 텍스처에 의존한다면, 꽃무늬가 그려진 컵을 보았을 때 그것을 컵이 아닌 ’꽃’으로 인식하거나, 컵과 동일한 무늬가 있는 식탁보를 컵의 일부로 착각하여 허공을 잡으려 할 것이다.3 정적 이미지는 객체의 3차원적 본질과 물리적 경계를 평면으로 압축해버리기 때문에, 모델은 텍스처라는 피상적 정보에 매몰될 수밖에 없다.</p>
<h3>2.3  의료 및 안전 분야에서의 치명적 오류: 인과성 부재의 대가</h3>
<p>정적 데이터의 허위 상관관계는 단순한 분류 오류를 넘어 생명과 직결되는 영역에서도 심각한 문제를 야기한다. 딥러닝 기반 폐렴 진단 모델에 대한 연구는 이를 극명하게 보여준다. 여러 병원에서 수집된 흉부 X-ray 데이터셋으로 훈련된 AI 모델이 의사보다 높은 정확도를 기록했다. 그러나 정밀 분석 결과, 이 모델은 폐의 병변을 보고 폐렴을 진단한 것이 아니었다. 특정 병원에서는 중증 환자를 촬영할 때 어깨 부위에 금속 토큰(마커)을 부착하는 관행이 있었는데, AI는 이 ’금속 토큰’의 존재 여부를 폐렴의 가장 강력한 예측 변수로 학습한 것이다.5</p>
<p>이는 전형적인 ’상관관계와 인과관계의 혼동’이다. 금속 토큰은 폐렴의 결과도 아니고 원인도 아니지만, 데이터셋 내에서 강력한 상관관계를 맺고 있었다. 이 모델을 금속 토큰을 사용하지 않는 다른 병원에 도입했을 때(분포 변화, Distribution Shift), 성능은 처참하게 하락했다.5 만약 이 AI가 정적 이미지를 학습하는 것을 넘어, 직접 의사에게 “이 토큰을 치우면 진단 결과가 달라집니까?“라고 묻거나(개입), 토큰을 제거한 이미지를 생성해볼 수 있었다면 이러한 오류는 즉각 수정되었을 것이다. 그러나 정적 데이터셋은 이러한 ‘반사실적(Counterfactual)’ 검증을 허용하지 않는다.</p>
<p><strong>표 1: 정적 데이터셋 학습의 주요 편향과 실패 사례</strong></p>
<table><thead><tr><th><strong>편향 유형 (Bias Type)</strong></th><th><strong>메커니즘 (Mechanism)</strong></th><th><strong>사례 (Example)</strong></th><th><strong>결과 (Consequence)</strong></th></tr></thead><tbody>
<tr><td><strong>단순성 편향 (Simplicity Bias)</strong></td><td>모델은 손실을 줄이는 가장 간단한 특징(색상, 배경)에 의존함</td><td>녹색 배경(잔디)을 ’개’로 인식 1</td><td>배경이 바뀐 환경(실내)에서 인식 실패</td></tr>
<tr><td><strong>텍스처 편향 (Texture Bias)</strong></td><td>객체의 전체 형상보다 국소적인 질감 패턴에 과적합됨</td><td>코끼리 피부 질감을 가진 고양이 형상을 ’코끼리’로 분류 4</td><td>조명 변화나 노이즈에 취약, 물리적 조작(Grasping) 실패</td></tr>
<tr><td><strong>허위 상관관계 (Spurious Correlation)</strong></td><td>인과적 연관성이 없는 배경 요소를 핵심 특징으로 오인</td><td>X-ray의 금속 토큰을 폐렴의 지표로 학습 5</td><td>새로운 환경(병원) 적용 시 성능 붕괴 (OOD Failure)</td></tr>
</tbody></table>
<h2>3.  인과적 추론의 부재: 펄(Pearl)의 인과 사다리와 관찰의 한계</h2>
<p>정적 데이터 학습의 한계를 근원적으로 이해하기 위해서는 통계학적 관점을 넘어 인과론적 관점이 필요하다. 튜링상 수상자인 주디아 펄(Judea Pearl)은 지능이 다루는 정보를 세 가지 계층으로 분류한 ‘인과 사다리(Ladder of Causation)’ 이론을 제시했다. 이 이론에 따르면, 현재의 정적 데이터 기반 AI는 사다리의 가장 낮은 단계에 머물러 있다.</p>
<h3>3.1  1단계: 연관성(Association) - “보는 것(Seeing)“의 한계</h3>
<p>사다리의 첫 번째 단계는 ’연관성’이다. 이는 <span class="math math-inline">P(y|x)</span>, 즉 “x를 보았을 때 y일 확률은 얼마인가?“라는 질문을 다룬다. “기압계의 눈금이 떨어지면 비가 올 확률이 높다“와 같은 통계적 규칙이 여기에 해당한다. 현재의 딥러닝, 머신러닝, 그리고 대부분의 정적 데이터셋 학습은 이 단계에 갇혀 있다. 정적 데이터는 변수들 사이의 상관관계(Correlation)만을 담고 있기 때문이다.6</p>
<p>연관성 수준의 지능은 관찰된 패턴이 유지되는 한 매우 강력하다. 그러나 패턴의 원인이 바뀌거나, 환경이 변화하면 무력해진다. 기압계의 눈금이 떨어지는 것을 보고 비를 예측하는 AI는 훌륭하지만, 누군가 장난으로 기압계 바늘을 강제로 내렸을 때도 비가 올 것이라고 예측한다면 그것은 지능의 실패다. 정적 데이터만으로는 바늘이 내려간 것이 자연적 기압 변화 때문인지, 누군가의 조작 때문인지 구별할 수 없다.</p>
<h3>3.2  2단계: 개입(Intervention) - “하는 것(Doing)“의 필요성</h3>
<p>두 번째 단계는 ’개입’이다. 이는 <span class="math math-inline">P(y|do(x))</span>, 즉 “내가 x를 강제로 실행했을 때 y는 어떻게 되는가?“라는 질문을 다룬다. “내가 두통약을 먹으면 머리가 안 아플까?”, “내가 로봇 팔로 이 물체를 밀면 어디로 굴러갈까?“와 같은 질문들이다. 이 단계부터는 수동적 관찰자가 아닌 능동적 행위자가 필요하다.6</p>
<p>로봇 공학에서 2단계의 중요성은 절대적이다. 정적 비디오를 통해 ’사람이 브레이크 등을 켜면 차가 멈춘다’는 연관성을 학습한 로봇(1단계)은, 자신이 브레이크 등을 켜면(개입) 차가 멈출 것이라고 잘못 추론할 수 있다. 그러나 실제로는 브레이크 페달을 밟는 행위(원인)가 등 점등과 차량 정지(결과)를 동시에 유발하는 교란 요인(Confounder) 구조를 갖는다. 로봇이 직접 페달을 밟아보거나, 등만 켜보는 ’개입’을 수행하지 않고서는 이러한 인과 구조를 결코 파악할 수 없다.8 정적 데이터셋에는 <span class="math math-inline">do(x)</span> 연산자가 존재하지 않는다. 오직 <span class="math math-inline">observe(x)</span>만이 존재할 뿐이다.</p>
<h3>3.3  3단계: 반사실(Counterfactuals) - “상상하는 것(Imagining)“의 영역</h3>
<p>마지막 단계는 ’반사실적 추론’이다. “만약 내가 그때 약을 먹지 않았더라면, 지금 여전히 아플까?“와 같이 과거의 사건을 재구성하고 대안적 현실을 시뮬레이션하는 능력이다. 이는 인간 지능의 정수이자, 과학적 사고의 핵심이다. 반사실적 추론을 위해서는 세상의 작동 원리에 대한 견고한 인과 모델(Causal Model)이 필요하다.6</p>
<p>정적 데이터셋으로 학습된 모델은 반사실적 질문에 대해 그럴듯한 텍스트를 생성할 수는 있지만, 그것은 언어적 패턴에 기반한 흉내내기일 뿐 논리적 추론이 아니다. 펄은 “순수한 관찰 데이터(정적 데이터)만으로는 결코 인과적 혹은 반사실적 질문에 답할 수 없다“는 것을 수학적으로 증명했다.10</p>
<h3>3.4  로봇 공학에서의 교란 변수(Confounding Variable) 문제</h3>
<p>로봇이 정적 데이터를 통해 학습할 때 직면하는 가장 큰 기술적 장벽은 ‘식별 불가능성(Unidentifiability)’ 문제다. 데이터 내에 숨겨진 교란 변수(Confounder)가 존재할 때, 관찰만으로는 진정한 인과관계를 파악하는 것이 수학적으로 불가능하다.11</p>
<p>예를 들어, 인간 전문가가 로봇에게 이동 경로를 시연하는 데이터셋을 생각해보자. 전문가는 바닥에 쏟아진 물(미끄러운 구간)을 보고 본능적으로 피해 간다. 그러나 로봇의 카메라에는 물이 투명해서 잘 보이지 않을 수 있다(Unobserved Confounder). 로봇은 “이 지점에서는 왼쪽으로 꺾어야 한다“는 잘못된 규칙을 학습한다. 만약 로봇이 상호작용을 통해 직접 그 위를 걸어보고 미끄러지는 경험(물리적 피드백)을 하지 않는다면, 로봇은 왜 전문가가 그곳을 피했는지 영원히 이해할 수 없다.13 이처럼 정적 데이터 학습은 교란 변수의 영향력을 제거할 수 없기에, 환경 변화에 취약한 편향된 정책(Biased Policy)을 양산한다.</p>
<h2>4.  시각-언어 모델(VLM)의 물리적 추론 실패: 보존 개념의 부재</h2>
<p>최근 등장한 시각-언어 모델(VLM)들은 수십억 개의 이미지와 텍스트 쌍으로 훈련되었음에도 불구하고, 유치원생 수준의 물리적 직관조차 갖추지 못했음이 드러나고 있다. 특히 장 피아제(Jean Piaget)의 발달 심리학에서 차용한 ‘보존(Conservation)’ 과제에서의 실패는 정적 데이터 학습의 한계를 적나라하게 보여준다.</p>
<h3>4.1  ConserveBench와 물리적 문맹의 실체</h3>
<p>2024년 발표된 ConserveBench 연구는 GPT-4V, Gemini 등의 최신 VLM들을 대상으로 물리적 보존 법칙에 대한 이해도를 평가했다. 이 벤치마크는 물을 옮겨 담거나 점토의 모양을 바꾸는 영상을 보여주고 두 가지 유형의 질문을 던진다.15</p>
<ol>
<li><strong>변환적 질문(Transformational Questions):</strong> “물이 컵 A에서 컵 B로 이동했나요?”, “점토가 납작해졌나요?”</li>
<li><strong>정량적 질문(Quantitative Questions):</strong> “물의 양은 이전과 동일한가요?”, “점토의 질량은 변했나요?”</li>
</ol>
<p>실험 결과, VLM들은 변환적 질문에는 80% 이상의 높은 정답률을 보였다. 이는 비디오 내의 시각적 변화(픽셀의 이동, 모양의 변형)를 추적하는 패턴 매칭 능력은 탁월함을 의미한다. 그러나 정량적 질문에 대해서는 정답률이 50% 미만, 즉 무작위 추측(Chance level) 수준으로 급락했다.16</p>
<p>표 2: 보존 과제(Conservation Tasks)에서의 VLM 성능과 인간 아동 비교 15</p>
<table><thead><tr><th><strong>과제 유형 (Task Dimension)</strong></th><th><strong>질문 유형 (Question Type)</strong></th><th><strong>VLM 성능 (GPT-4V 등)</strong></th><th><strong>인간 아동 (구체적 조작기)</strong></th><th><strong>시사점 (Implication)</strong></th></tr></thead><tbody>
<tr><td><strong>액체 부피 (Liquid Volume)</strong></td><td>변환 (“물이 옮겨졌는가?”)</td><td><strong>높음</strong> (High)</td><td>높음</td><td>모델은 시각적 광학 흐름(Optical Flow)은 인식함</td></tr>
<tr><td></td><td><strong>보존 (“양이 같은가?”)</strong></td><td><strong>매우 낮음</strong> (Failure)</td><td>높음</td><td><strong>모델은 높이 변화를 양의 변화로 착각함 (보존 불변성 부재)</strong></td></tr>
<tr><td><strong>고체 질량 (Solid Mass)</strong></td><td>변환 (“모양이 변했는가?”)</td><td><strong>높음</strong> (High)</td><td>높음</td><td>모델은 형상 변화를 텍스트로 기술 가능</td></tr>
<tr><td></td><td><strong>보존 (“질량이 같은가?”)</strong></td><td><strong>매우 낮음</strong> (Failure)</td><td>높음</td><td><strong>모델은 ’넓어짐=커짐’이라는 단순 시각적 휴리스틱에 의존</strong></td></tr>
</tbody></table>
<h3>4.2  시각적 환각과 논리적 괴리</h3>
<p>더욱 충격적인 것은, 모델들이 “물의 높이가 높아졌으므로 양이 늘어났다“와 같은 피아제 전조작기(Pre-operational stage) 아동들이 범하는 오류를 그대로 답습한다는 점이다. 심지어 어떤 경우에는 “물을 쏟지 않았고 그릇만 바꾸었다“고 상황을 정확히 설명하면서도(변환 인지), 결론에서는 “따라서 양이 변했다“라고 답하는 논리적 모순을 보이기도 한다.15</p>
<p>이는 VLM이 ’물리 엔진’을 내재화한 것이 아니라, 텍스트와 이미지 사이의 표면적 통계 확률만을 학습했기 때문이다. 인터넷상의 텍스트 데이터에는 “물을 컵에 따르는” 묘사는 많지만, 그 과정에서 “부피가 보존된다“는 당연한 물리 법칙을 매번 명시적으로 설명하는 텍스트는 드물다. 따라서 정적 데이터만 학습한 모델은 ’쏟음(Pouring)’이라는 행위와 ’보존(Conservation)’이라는 불변량(Invariant) 사이의 인과적 고리를 형성하지 못한 것이다.18</p>
<h3>4.3  텍스트는 물리를 가르칠 수 없다</h3>
<p>얀 르쿤은 이러한 현상을 두고 “텍스트는 저대역폭(Low-bandwidth) 정보이기 때문에 물리 세계의 풍부함을 담을 수 없다“고 비판했다.19 텍스트는 인간의 경험을 추상화한 기호일 뿐이다. “무겁다“라는 단어는 텍스트 임베딩 공간에서 “가볍다“의 반대말로서 존재하지만, 실제 근육에 가해지는 부하(Torque)나 피부가 눌리는 압력(Pressure)으로서의 의미는 결여되어 있다. 정적 데이터 학습은 기호(Symbol)들 간의 관계는 가르칠 수 있지만, 그 기호가 지칭하는 물리적 실체(Grounding)는 가르칠 수 없다. 이것이 바로 로드니 브룩스가 지적한 “그라운딩(Grounding) 없는 AI의 공허함“이다.20</p>
<h2>5.  상호작용을 통한 해결: 개입(Intervention)과 능동적 지각</h2>
<p>정적 데이터의 한계를 극복하는 유일한 길은 AI를 수동적 관찰자에서 능동적 행위자로 전환하는 것이다. 상호작용(Interaction)은 단순한 데이터 수집 방식의 변화가 아니라, 인과관계를 발견하고 물리적 실재를 검증하는 인식론적 도구이다.</p>
<h3>5.1  인과적 개입을 통한 허위 상관관계 타파</h3>
<p>앞서 언급한 펄의 2단계(개입)는 상호작용을 통해 구현된다. 로봇이 환경에 개입한다는 것은 곧 자연계에서 무작위 대조군 실험(RCT)을 수행하는 것과 같다. 로봇이 특정 변수의 값을 강제로 변경(Intervention)하면, 그 변수로 들어오는 모든 인과적 영향력은 차단되고, 오직 그 변수가 밖으로 미치는 영향력만이 남게 된다.</p>
<p>예를 들어, “개 목걸이” 편향을 가진 로봇이 있다고 가정하자. 이 로봇이 상호작용을 통해 개의 목에서 목걸이를 제거(개입: <span class="math math-inline">do(Collar=Removed)</span>)해보는 순간, 진실이 드러난다. 목걸이가 없어져도 대상이 여전히 짖고 꼬리를 흔든다면, “목걸이=개“라는 가설은 즉시 기각된다. 반대로 고양이에게 목걸이를 채워보고 여전히 야옹거리는 것을 확인하면 “목걸이<span class="math math-inline">\neq</span>개“임을 확신하게 된다.2 정적 데이터에서 수천 장의 사진을 보는 것보다, 단 한 번의 물리적 개입이 인과관계를 파악하는 데 훨씬 더 강력한 정보량(Information Gain)을 제공한다.</p>
<h3>5.2  능동적 지각(Active Perception): 움직임이 정보를 만든다</h3>
<p>정적 컴퓨터 비전은 ‘주어진’ 이미지를 해석하는 수동적 과정이지만, 로봇 공학의 ’능동적 지각’은 정보를 획득하기 위해 센서를 제어하는 능동적 과정이다. 스탠포드 대학교의 연구는 로봇이 물체를 인식할 때 단순히 쳐다보는 것이 아니라, 물체를 잡고 돌려보거나(Manipulation), 머리를 움직여 시점을 바꿈으로써(Exploration) 인식률을 획기적으로 높일 수 있음을 보여주었다.22</p>
<p>정적 이미지에서 컵의 손잡이가 가려져 있다면(Occlusion), AI는 그것이 컵인지 원통인지 추측해야 한다. 그러나 능동적 로봇은 “손잡이가 있을 법한 곳“으로 카메라를 이동시킨다. 이는 불확실성을 능동적으로 줄여나가는 과정이다. 움직임에 따라 발생하는 시차(Motion Parallax)는 대상이 2차원 그림인지 3차원 물체인지를 즉각적으로 알려준다. 정적 데이터에서는 수학적으로 해결 불가능한(Ill-posed) 역문제들이, 상호작용을 통한 시간적 연속 데이터에서는 해결 가능한 문제가 된다.23</p>
<h3>5.3  탐색적 행동(Exploratory Actions): 보이지 않는 속성의 발견</h3>
<p>시각은 본질적으로 표면의 정보만을 제공한다. 질량, 마찰력, 강성(Stiffness), 무게중심과 같은 물체의 내재적 속성은 눈에 보이지 않는다. 정적 비디오만으로는 상자가 무거운지 가벼운지, 스펀지가 딱딱한지 부드러운지 알 수 없다. 이를 알아내기 위해 로봇은 ’탐색적 행동’을 수행해야 한다.25</p>
<p>Matej Hoffmann 등의 연구에 따르면, 로봇은 다음과 같은 일련의 행동을 통해 물성을 파악한다 27:</p>
<ul>
<li><strong>누르기(Poke/Squeeze):</strong> 물체의 강성(Stiffness)과 탄성을 측정. 시각적으로 동일해 보이는 진짜 바나나와 플라스틱 모형 바나나를 구별하는 유일한 방법이다.</li>
<li><strong>들어올리기(Lift):</strong> 관절의 토크 센서를 통해 질량(Mass)을 측정.</li>
<li><strong>문지르기(Rub):</strong> 진동과 저항력을 통해 표면 마찰 계수와 재질을 파악.</li>
</ul>
<p>이러한 상호작용 데이터는 정적 시각 데이터와 결합되어 ’멀티모달 임베딩’을 형성한다. 로봇은 수천 번의 상호작용을 통해 “금속처럼 생긴 것은 무겁고 차갑다“는 시각-촉각 상관관계를 학습하게 되며, 이는 단순한 텍스처 매칭을 넘어선 깊은 물리적 이해로 이어진다.29</p>
<h2>6.  메커니즘: 자기지도 학습과 멀티모달 융합</h2>
<p>그렇다면 상호작용은 구체적으로 어떤 알고리즘적 메커니즘을 통해 학습 효율을 높이는가? 핵심은 ’자기지도 학습(Self-Supervised Learning, SSL)’과 ’감각 융합(Sensory Fusion)’에 있다.</p>
<h3>6.1  자기지도 학습: 세상 그 자체가 정답지다</h3>
<p>정적 데이터셋 학습의 가장 큰 병목은 사람이 일일이 라벨(Label)을 붙여야 한다는 점이다(Supervised Learning). 그러나 로봇이 상호작용할 때, 물리 법칙은 그 자체로 완벽하고 즉각적인 피드백을 제공한다. 로봇이 “내가 앞으로 가면 벽에 부딪힐까?“라고 예측하고 실제로 움직였을 때, 충돌 센서가 울린다면 그것이 바로 정답(Label)이 된다. 이를 ’자기지도 학습’이라 한다.31</p>
<p>카네기멜론 대학(CMU)의 간디(Gandhi) 등은 드론을 11,500번 고의로 충돌시키는 실험을 통해 이 개념을 입증했다. 드론은 정적 데이터나 사람의 라벨링 없이, 오직 “충돌하면 나쁘다“는 신호만을 가지고 비행을 학습했다. 흥미로운 점은, 이 드론이 정적 데이터 기반의 깊이(Depth) 센서가 감지하지 못하는 ’유리벽’을 피하는 법을 배웠다는 것이다. 깊이 카메라는 유리를 투과하지만, 실제 충돌(상호작용)은 유리가 장애물임을 알려주었기 때문이다. 드론은 유리의 희미한 반사광을 ’위험 신호’로 연관 짓는 법을 스스로 깨우쳤다. 이는 상호작용 없이는 절대 얻을 수 없는 지식이다.33</p>
<p>표 3: 자기지도 상호작용 학습 대 정적 지도 학습 비교 33</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>정적 지도 학습 (Static Supervised)</strong></th><th><strong>상호작용 자기지도 학습 (Interactive Self-Supervised)</strong></th></tr></thead><tbody>
<tr><td><strong>데이터 소스</strong></td><td>사람이 라벨링한 이미지/깊이 맵</td><td>로봇의 실제 주행 및 충돌 경험 (11.5k Crashes)</td></tr>
<tr><td><strong>투명한 물체(유리)</strong></td><td><strong>실패</strong> (깊이 센서가 투과함)</td><td><strong>성공</strong> (충돌 경험을 통해 반사광을 장애물로 학습)</td></tr>
<tr><td><strong>복잡한 환경(의자)</strong></td><td>낮은 성능 (복잡한 형상 인식 실패)</td><td><strong>높은 성능</strong> (실제 통과 가능 여부를 몸으로 학습)</td></tr>
<tr><td><strong>학습 신호</strong></td><td>인간의 주관적 판단 (Annotation)</td><td>물리적 현실의 객관적 피드백 (IMU 충격 신호)</td></tr>
</tbody></table>
<h3>6.2  시각-촉각 융합(Visuo-Tactile Fusion)과 교차 예측</h3>
<p>인간은 눈을 감고도 물건을 집을 수 있고, 보는 것만으로도 물체의 거칠기를 느낄 수 있다. 이는 시각과 촉각이 뇌 속에서 긴밀하게 연결되어 있기 때문이다. 로봇 공학에서도 이러한 ‘교차 양상(Cross-modal)’ 학습이 필수적이다.</p>
<p>최신 연구들은 ‘교차 양상 예측(Cross-modal Prediction)’ 모델을 제안한다. 이는 시각 정보를 입력받아 미래의 촉각 신호를 예측하거나, 촉각 정보를 통해 보이지 않는 시각적 형상을 유추하는 방식이다.34</p>
<ul>
<li><strong>Vision-to-Touch:</strong> 로봇이 컵을 잡기 전에 시각 정보만으로 표면의 마찰력을 예측하여 그립 힘(Grip Force)을 조절한다. 정적 데이터로는 불가능한 ’예측 제어’가 가능해진다.</li>
<li><strong>Touch-to-Vision:</strong> 어두운 곳이나 연기 속에서 로봇이 손끝의 감각만으로 물체의 3D 형상을 마음속으로 그려낸다(Reconstruction).</li>
</ul>
<p>이러한 융합은 정보의 풍부함을 극대화한다. 시각은 전역적(Global) 정보를, 촉각은 국소적(Local) 정보를 제공하며 상호 보완한다. 이 두 감각의 일치(Alignment)를 학습함으로써 로봇은 더욱 강건한(Robust) 세상의 모델을 구축하게 된다.36</p>
<h2>7.  결론: 체화된 지능(Embodied Intelligence)을 향하여</h2>
<p>우리는 지금 ’모라벡의 역설(Moravec’s Paradox)’을 재확인하고 있다. 1980년대 한스 모라벡은 “체스나 수학 같은 고등 추론은 컴퓨터에게 쉽지만, 걷거나 물체를 집는 아이들의 감각운동 능력은 매우 어렵다“고 통찰했다.38 오늘날 LLM은 시를 쓰고 코드를 짜지만(고등 추론), 컵 하나를 제대로 집지 못하거나 물의 양이 보존된다는 사실을 이해하지 못한다. 이는 지능의 뿌리가 추상적 기호가 아닌, 물리적 신체를 통한 세상과의 상호작용에 있음을 시사한다.</p>
<p>정적 데이터셋은 AI에게 세상의 ’스냅샷’을 보여주었지만, 세상의 ’작동 원리’를 가르치는 데는 실패했다. 정적 데이터의 한계—허위 상관관계, 텍스처 편향, 인과적 불확실성—는 데이터의 양을 늘린다고 해결될 문제가 아니다. 이는 관찰자(Observer)에서 행위자(Actor)로의 패러다임 전환을 통해서만 극복 가능하다.</p>
<p>얀 르쿤이 제창한 ’월드 모델(World Model)’은 이러한 전환의 청사진을 제시한다. 차세대 AI는 다음 단어를 예측하는 것이 아니라, “내가 이 행동을 하면 세상이 어떻게 변할까?“를 예측하는 시뮬레이터를 머릿속에 가져야 한다.19 그리고 그 시뮬레이터는 오직 상호작용을 통해서만 정교해질 수 있다. 로봇이 직접 세상을 밀어보고, 넘어지고, 부딪히며 얻는 ’체화된 데이터(Embodied Data)’야말로 인과관계를 학습하고 진정한 의미의 지능을 구축하는 열쇠이다.</p>
<p>이제 우리는 “데이터를 얼마나 많이 모을 것인가“가 아니라, “어떻게 세상과 더 똑똑하게 상호작용할 것인가“를 물어야 한다. 정적 데이터의 시대가 저물고, 상호작용적 학습(Interactive Learning)의 시대가 열리고 있다. 이것이 바로 고정된 이미지의 한계를 넘어 인과관계의 실체로 나아가는 유일한 길이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>The Clever Hans Mirage: A Comprehensive Survey on Spurious Correlations in Machine Learning - arXiv, https://arxiv.org/html/2402.12715v3</li>
<li>New Technique Overcomes Spurious Correlations Problem in AI - NC State News, https://news.ncsu.edu/2025/03/ai-spurious-correlations/</li>
<li>Explorations in Texture Learning - arXiv, https://arxiv.org/html/2403.09543v1</li>
<li>[1811.12231] ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness - arXiv, https://arxiv.org/abs/1811.12231</li>
<li>Spurious Correlations in Machine Learning: A Survey - arXiv, https://arxiv.org/html/2402.12715v2</li>
<li>State-of-the-art in AI #1: causality, hypotheticals, and robots with free will &amp; capacity for evil (UPDATED) - Dominic Cummings’s Blog, https://dominiccummings.com/2018/05/21/technology-the-state-of-the-art-in-ai-causality-and-hypotheticals/</li>
<li>The Ladder of Causation: Climbing up in the world of Causal Inference (2/15) - Medium, https://medium.com/causal-inference/the-ladder-of-causation-climbing-up-in-the-world-of-causal-inference-2-15-7539f92c280d</li>
<li>Leveraging Causal Graphical Models For Robotics - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v164/stocking22a/stocking22a.pdf</li>
<li>Causal Effects via the Do-operator | Towards Data Science, https://towardsdatascience.com/causal-effects-via-the-do-operator-5415aefc834a/</li>
<li>Judea Pearl, AI, and Causality: What Role Do Statisticians Play? - Amstat News, https://magazine.amstat.org/blog/2023/09/01/judeapearl/</li>
<li>[D] Causality research in ML is a scam (warning: controversial) : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/qs7g4t/d_causality_research_in_ml_is_a_scam_warning/</li>
<li>An Introduction to Causal Inference Methods for Observational Human-Robot Interaction Research - arXiv, https://arxiv.org/pdf/2310.20468</li>
<li>Causal Reinforcement Learning using Observational and Interventional Data - arXiv, https://arxiv.org/pdf/2106.14421</li>
<li>Deep causal learning for robotic intelligence - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC9992986/</li>
<li>Vision Language Models Know Law of Conservation without Understanding More-or-Less, https://arxiv.org/html/2410.00332v2</li>
<li>Vision Language Models Know Law of Conservation without Understanding More-or-Less, https://arxiv.org/html/2410.00332v5</li>
<li>Vision Language Models Cannot Reason About Physical Transformation - OpenReview, https://openreview.net/forum?id=iTK8BZ8i3J</li>
<li>Understanding the Limits of Vision Language Models Through the Lens of the Binding Problem - NIPS papers, https://proceedings.neurips.cc/paper_files/paper/2024/file/cdcc6d47c1627350014a3076112ab824-Paper-Conference.pdf</li>
<li>Language models impress but miss real-world understanding - Digital Watch Observatory, https://dig.watch/updates/language-models-impress-but-miss-real-world-understanding</li>
<li>Rodney Brooks: The Truth About Humanoid Robots and AI Hype - Reddit, https://www.reddit.com/r/robotics/comments/1npckqx/rodney_brooks_the_truth_about_humanoid_robots_and/</li>
<li>The Myth Buster: Rodney Brooks Breaks Down the Hype Around AI for Newsweek ‘AI Impact’, https://www.robust.ai/blog/newsweekaiseries</li>
<li>Active Perception: Interactive Manipulation for Improving Object Detection - Stanford Computer Science, https://cs.stanford.edu/~quocle/activevision.pdf</li>
<li>Active Visual Perception: Opportunities and Challenges - arXiv, https://arxiv.org/html/2512.03687v1</li>
<li>Self-supervised visual learning from interactions with objects - arXiv, https://arxiv.org/html/2407.06704v1</li>
<li>Interactive Learning of Physical Object Properties Through Robot Manipulation and Database of Object Measurements - arXiv, https://arxiv.org/html/2404.07344v2</li>
<li>[2404.07344] Interactive Learning of Physical Object Properties Through Robot Manipulation and Database of Object Measurements - arXiv, https://arxiv.org/abs/2404.07344</li>
<li>Interactive Learning of Physical Object Properties Through Robot Manipulation and Database of Object Measurements | Request PDF - ResearchGate, https://www.researchgate.net/publication/387422417_Interactive_Learning_of_Physical_Object_Properties_Through_Robot_Manipulation_and_Database_of_Object_Measurements</li>
<li>Interactive Learning of Physical Object Properties Through Robot Manipulation and Database of Object Measurements - arXiv, https://arxiv.org/html/2404.07344v1</li>
<li>Learning Object Properties Using Robot Proprioception via Differentiable Robot-Object Interaction - Chao Liu, https://chaoliu.tech/publication/chen-diff-proprioception-icra-2025/chen-diff-proprioception-icra-2025.pdf</li>
<li>A review of learning-based dynamics models for robotic manipulation - Bo Ai, https://albertboai.com/assets/pdf/2025_scirobotics.adt1497.pdf</li>
<li>What Is Self-Supervised Learning? - IBM, https://www.ibm.com/think/topics/self-supervised-learning</li>
<li>Breaking Down Self-Supervised Learning: Concepts, Comparisons, and Examples - Wandb, https://wandb.ai/mostafaibrahim17/ml-articles/reports/Breaking-Down-Self-Supervised-Learning-Concepts-Comparisons-and-Examples–Vmlldzo2MzgwNjIx</li>
<li>Self-Supervised Robot Learning, https://www.ri.cmu.edu/app/uploads/2019/06/dgandhi_thesis.pdf</li>
<li>Bridging vision and touch: advancing robotic interaction prediction with self-supervised multimodal learning - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC11472251/</li>
<li>Adaptive visual–tactile fusion recognition for robotic operation of multi-material system - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC10318164/</li>
<li>See, Hear, and Feel: Smart Sensory Fusion for Robotic Manipulation, https://par.nsf.gov/servlets/purl/10429045</li>
<li>Adaptive Visuo-Tactile Fusion with Predictive Force Attention for Dexterous Manipulation - arXiv, https://arxiv.org/html/2505.13982v1</li>
<li>Moravec’s paradox - Wikipedia, <a href="https://en.wikipedia.org/wiki/Moravec&#x27;s_paradox">https://en.wikipedia.org/wiki/Moravec%27s_paradox</a></li>
<li>Moravec’s Paradox from the 4E Cognitive Psychology View | by Carlos E. Perez - Medium, https://medium.com/intuitionmachine/moravecs-paradox-from-the-4e-cognitive-psychology-view-539359b432fa</li>
<li>World Models vs. Word Models: Why Yann LeCun Believes LLMs Will Be Obsolete - Medium, https://medium.com/state-of-the-art-technology/world-models-vs-word-models-why-lecun-believes-llms-will-be-obsolete-23795e729cfa</li>
<li>Probabilities of causation: Bounds and identification - FTP Directory Listing, https://ftp.cs.ucla.edu/pub/stat_ser/r271-A.pdf</li>
<li>Quantifying Shape and Texture Biases for Enhancing Transfer Learning in Convolutional Neural Networks - MDPI, https://www.mdpi.com/2624-6120/5/4/40</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>