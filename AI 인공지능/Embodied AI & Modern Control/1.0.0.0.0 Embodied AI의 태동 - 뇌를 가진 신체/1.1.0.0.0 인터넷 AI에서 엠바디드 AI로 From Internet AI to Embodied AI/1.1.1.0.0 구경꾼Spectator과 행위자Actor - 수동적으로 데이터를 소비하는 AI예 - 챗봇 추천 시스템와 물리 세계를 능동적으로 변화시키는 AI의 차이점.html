<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:1.1.1 구경꾼(Spectator)과 행위자(Actor): 수동적으로 데이터를 소비하는 AI(예: 챗봇, 추천 시스템)와 물리 세계를 능동적으로 변화시키는 AI의 차이점.</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>1.1.1 구경꾼(Spectator)과 행위자(Actor): 수동적으로 데이터를 소비하는 AI(예: 챗봇, 추천 시스템)와 물리 세계를 능동적으로 변화시키는 AI의 차이점.</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 1. Embodied AI의 태동: 뇌를 가진 신체</a> / <a href="index.html">1.1 인터넷 AI에서 엠바디드 AI로 (From Internet AI to Embodied AI)</a> / <span>1.1.1 구경꾼(Spectator)과 행위자(Actor): 수동적으로 데이터를 소비하는 AI(예: 챗봇, 추천 시스템)와 물리 세계를 능동적으로 변화시키는 AI의 차이점.</span></nav>
                </div>
            </header>
            <article>
                <h1>1.1.1 구경꾼(Spectator)과 행위자(Actor): 수동적으로 데이터를 소비하는 AI(예: 챗봇, 추천 시스템)와 물리 세계를 능동적으로 변화시키는 AI의 차이점.</h1>
<h2>1. 서론: 실존 양식의 전환—디지털 동굴의 수감자에서 광장의 모험가로</h2>
<p>인공지능(Artificial Intelligence)의 역사는 지능의 원천을 어디에 두느냐에 따라 크게 두 시기로 구분될 수 있다. 초기부터 최근까지 주류를 이루었던 첫 번째 시기는 지능을 거대한 데이터의 패턴을 인식하는 통계적 연산 능력으로 정의했다. 이 시기의 인공지능은 우리가 흔히 ’인터넷 AI(Internet AI)’라 부르는 것으로, 웹상에 축적된 텍스트와 이미지라는 정적인 그림자를 학습하며 세계를 파악하려 했다.1 그러나 이제 우리는 두 번째 시기, 즉 ’체화된 AI(Embodied AI)’로의 거대한 패러다임 전환을 목격하고 있다. 이 새로운 흐름은 지능을 물리적 신체를 가지고 환경과 상호작용하며 생존하고 과업을 수행하는 능력으로 재정의한다.3</p>
<p>이러한 기술적 전이는 철학적이고 연극적인 은유를 통해 가장 명징하게 드러난다. 바로 ’구경꾼(Spectator)’에서 ’행위자(Actor)’로의 전환이다.5 구경꾼으로서의 AI는 무대(세계) 밖의 어둠 속에 앉아 무대 위에서 벌어지는 일들을 수동적으로 관찰한다. 그들은 데이터가 자신에게 주어지기를 기다리며, 데이터의 생성 과정에 개입할 수 없다. 반면, 행위자로서의 AI는 무대 위로 올라가 직접 조명을 받으며 소품을 만지고, 다른 배우들과 대화하며, 서사를 만들어낸다. 그들은 움직임을 통해 자신의 시야를 능동적으로 바꾸고, 행동을 통해 인과관계를 검증한다.5</p>
<p>본 장에서는 이 두 가지 실존 양식, 즉 수동적 데이터 소비자인 구경꾼 AI와 능동적 물리 세계 변화자인 행위자 AI의 차이를 심층적으로 분석한다. 우리는 데이터의 원천, 학습 방법론, 인과 추론의 수학적 기초, 그리고 언어와 의미의 접지(Grounding)라는 다각적인 렌즈를 통해, 왜 차세대 인공지능이 필연적으로 행위자가 되어야만 하는지를 논증할 것이다. 이는 단순히 알고리즘의 개선을 넘어, 지능이 세계 속에 존재(Being-in-the-world)하는 방식에 대한 근본적인 성찰을 요구한다.</p>
<h2>2.  구경꾼(Spectator): 인터넷 AI와 표상적 지능의 한계</h2>
<h3>2.1  정적 데이터셋과 제3자 시점의 딜레마</h3>
<p>지난 10년간 딥러닝의 폭발적인 성장을 견인한 ’인터넷 AI’는 본질적으로 육체가 없는(disembodied) 지능이다.1 이들은 ImageNet과 같은 대규모 이미지 데이터셋이나 Common Crawl과 같은 텍스트 말뭉치(Corpus)를 통해 학습한다. 이러한 데이터는 AI가 직접 경험한 세계가 아니라, 인간이 경험하고 기록한 세계의 ’표상(Representation)’이다.</p>
<p>구경꾼 AI의 가장 큰 특징은 **데이터의 수동성(Passivity)**이다. 인터넷 AI에게 데이터는 외부에서 주어지는 고정불변의 진리다. 예를 들어, 인터넷 AI가 학습하는 사진 데이터셋은 사진작가(인간)라는 제3의 행위자에 의해 이미 앵글, 조명, 구도가 결정된 결과물이다.4 AI는 고양이가 소파 뒤에 숨어 있을 때, 고개를 돌려 숨겨진 부분을 확인할 수 없다. 단지 픽셀의 통계적 패턴을 통해 “이것은 소파 뒤의 고양이일 확률이 높다“고 추론할 뿐이다. 이는 전지적 시점(Third-person view)을 흉내 내지만, 실제로는 큐레이션 된 데이터의 편향(Bias) 안에 갇힌 시야다.</p>
<p>이러한 학습 방식은 필연적으로 **‘인과적 불투명성(Causal Opacity)’**을 낳는다. 구경꾼 AI는 “바람이 불면 나뭇가지가 흔들린다“는 사실과 “나뭇가지가 흔들리면 바람이 분다“는 사실을 구별하기 어렵다. 정적인 데이터 안에서 두 사건은 항상 높은 상관관계(Correlation)를 보이며 동시에 등장하기 때문이다.8 물리적 개입(Intervention)이 불가능한 구경꾼에게 인과는 단지 강력한 상관관계의 다른 이름일 뿐이다.</p>
<h3>2.2  관객의 역설: 참여 없는 인식</h3>
<p>연극학에서 관객(Spectator)은 무대 위 사건에 정서적으로 참여하지만, 물리적으로는 개입하지 않는 존재로 정의된다.6 AI의 맥락에서 이러한 ’관객성’은 지능의 한계를 설정한다. Denis Diderot의 ’배우의 역설(Paradox of the Actor)’이 배우가 감정을 느끼지 않으면서 감정을 표현하는 이중성을 지적했다면7, AI의 ’구경꾼의 역설’은 <strong>“세계를 보지만 세계에 닿을 수 없는”</strong> 인지적 괴리를 의미한다.</p>
<p>최근의 대형 언어 모델(LLM)이 보여주는 환각(Hallucination) 현상은 이 구경꾼적 한계의 직접적인 증거다. 모델은 수억 개의 문장을 읽었기에 유창하게 말을 할 수 있지만, 그 말이 가리키는 물리적 실체와는 단 한 번도 상호작용해 본 적이 없다. 이는 마치 수영 교본을 수천 권 읽었지만 한 번도 물에 들어가 본 적 없는 사람과 같다. 그는 물의 유체 역학에 대해 완벽하게 서술할 수 있을지 모르나, 실제로 물에 빠졌을 때 생존할 수 있는 ’지능’을 가졌다고 보기는 어렵다.9 구경꾼 AI에게 세계는 기호(Symbol)들의 집합일 뿐, 물리적 저항과 반작용이 존재하는 실재(Reality)가 아니다.</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>인터넷 AI (구경꾼, Spectator)</strong></th><th><strong>체화된 AI (행위자, Actor)</strong></th></tr></thead><tbody>
<tr><td><strong>데이터 원천 (Data Source)</strong></td><td>인터넷에서 큐레이션 된 정적 데이터셋 (이미지, 비디오, 텍스트)</td><td>물리적 환경과의 동적 상호작용에서 발생하는 실시간 감각 데이터</td></tr>
<tr><td><strong>학습 방식 (Learning Method)</strong></td><td>수동적 관찰 (Passive Observation), 패턴 인식</td><td>능동적 탐색 (Active Exploration), 시행착오, 상호작용</td></tr>
<tr><td><strong>시각적 관점 (Perspective)</strong></td><td>제3자 시점 (Third-person view), 전지적 관찰자 시점</td><td>1인칭 자기중심 시점 (Egocentric view), 제한된 시야</td></tr>
<tr><td><strong>인과성 인식 (Causality)</strong></td><td>상관관계 (Correlation) 중심, 연관성 학습</td><td>개입 (Intervention) 및 반사실적 사고를 통한 인과관계 학습</td></tr>
<tr><td><strong>데이터셋 속성</strong></td><td>정적 (Static), i.i.d (Independent and Identically Distributed) 가정</td><td>동적 (Dynamic), 시간적 상관성 존재, 비(non)-i.i.d</td></tr>
<tr><td><strong>주요 응용</strong></td><td>이미지 분류, 번역, 챗봇, 추천 시스템</td><td>자율주행, 로봇 조작, 내비게이션, 스마트 시티 운영</td></tr>
</tbody></table>
<p>1</p>
<h2>3.  행위자(Actor): 체화된 AI와 인지적 혁명</h2>
<h3>3.1  에고센트릭(Egocentric) 지각과 신체성</h3>
<p>’체화된 AI’로의 전환은 지능을 구경꾼의 안락의자에서 일으켜 세워 거칠고 불확실한 현실의 광장으로 내보내는 것이다. 행위자 AI는 <strong>에고센트릭(Egocentric) 지각</strong>을 가진다.1 이는 지능이 세계를 자신의 신체(Body)를 중심으로 인식함을 의미한다. 로봇의 카메라는 로봇의 머리가 향하는 곳만을 볼 수 있고, 로봇의 팔은 닿을 수 있는 거리의 물체만을 조작할 수 있다.</p>
<p>이러한 제약은 역설적으로 지능 발달의 필수 조건이 된다. 제한된 시야는 에이전트에게 **“더 잘 보기 위해 움직여야 한다”**는 동기를 부여한다. 구경꾼 AI에게 “가려짐(Occlusion)“은 데이터의 결함이지만, 행위자 AI에게는 고개를 돌리거나 몸을 비틀어 해결해야 할 과제(Task)가 된다.11 신체는 세상을 지각하는 필터이자, 세상에 영향을 미치는 도구가 된다. 지능은 이 신체적 제약을 극복하고 최적의 행동을 찾아내는 과정에서 창발한다.</p>
<h3>3.2  능동적 지각(Active Perception)과 루프의 완성</h3>
<p>행위자 AI의 핵심 작동 원리는 **감각-운동 루프(Sensorimotor Loop)**의 완결성에 있다. 수동적 지각(Passive Perception)이 환경에서 들어오는 정보를 일방적으로 수용하는 것이라면, **능동적 지각(Active Perception)**은 인식의 주체가 정보를 획득하기 위해 환경을 조작하거나 자신의 상태를 변화시키는 과정이다.13</p>
<p>예를 들어, 자율주행차(행위자)는 교차로에서 시야가 확보되지 않을 때, 조금씩 앞으로 전진하며(“Creeping”) 센서의 사각지대를 지운다. 또한 로봇 팔은 물체의 재질을 알기 위해 눈으로 보는 것에 그치지 않고, 손가락으로 표면을 긁어보거나 두드려보는 촉각적 탐색을 수행한다.15 여기서 행동(Action)은 단순히 결과물을 내놓는 출력이 아니라, 새로운 감각 입력(Input)을 생성하는 인식의 과정이다.</p>
<ul>
<li><strong>시각적 탐색(Visual Exploration)</strong>: 정보 이득(Information Gain)을 최대화하기 위해 시선을 이동.</li>
<li><strong>상호작용적 조작(Interactive Manipulation)</strong>: 물체의 물리적 속성(무게, 마찰 등)을 파악하기 위해 물체를 들어 올리거나 흔듦.</li>
<li><strong>인식론적 행동(Epistemic Action)</strong>: 목표 달성을 위한 직접적 행동이 아니라, 정보를 얻기 위한 행동(예: 지도를 보기 위해 멈추는 행위).</li>
</ul>
<p>이러한 능동성은 구경꾼 AI가 겪는 데이터 부족 문제를 근본적으로 해결한다. 행위자는 필요한 데이터가 없으면 스스로 움직여 데이터를 만들어낸다.</p>
<h2>4.  경험은 언어를 접지한다: 의미의 기원과 세계의 범위</h2>
<h3>4.1  “Experience Grounds Language“와 의미의 접지</h3>
<p>Yonatan Bisk 등은 그들의 영향력 있는 논문 “Experience Grounds Language“에서 현재의 NLP(자연어 처리) 연구가 직면한 한계를 지적하며, 언어 모델이 진정한 의미(Meaning)를 획득하기 위해서는 물리적 세계에 **접지(Grounding)**되어야 한다고 주장했다.9</p>
<p>접지란 추상적인 기호(Symbol)를 구체적인 감각(Sensation)이나 물리적 상태(Physical State)와 연결하는 과정이다. 구경꾼 AI인 LLM에게 “사과“는 “빨갛다”, “과일”, “맛있다“와 같은 다른 단어 벡터들과의 수학적 거리로 정의된다. 그러나 행위자 AI에게 “사과“는 둥근 시각적 패턴(Vision), 표면의 매끄러운 촉감(Touch), 들어 올릴 때 느껴지는 무게감(Proprioception), 그리고 베어 물었을 때의 미각적 보상(Reward)이 결합된 총체적 경험이다. Bisk는 의미가 텍스트의 통계적 분포에서 나오는 것이 아니라, 사람이나 에이전트가 의사소통을 위해 세계와 상호작용하는 과정에서 발생한다고 보았다.17</p>
<h3>4.2  세계의 범위(World Scope): 5단계 지능 모델</h3>
<p>Bisk 등은 AI가 다루는 세계의 복잡성과 상호작용 수준에 따라 지능의 단계를 5가지 ’세계 범위(World Scope)’로 분류했다.9 이 분류는 구경꾼에서 행위자로의 진화를 명확히 보여준다.</p>
<table><thead><tr><th><strong>범위 (Scope)</strong></th><th><strong>정의</strong></th><th><strong>데이터 소스</strong></th><th><strong>AI 유형</strong></th><th><strong>특징 및 한계</strong></th></tr></thead><tbody>
<tr><td><strong>WS1: Corpus</strong></td><td>말뭉치 (과거의 기록)</td><td>텍스트</td><td>n-gram, 초기 RNN</td><td>언어의 통계적 규칙 학습, 의미 부재.</td></tr>
<tr><td><strong>WS2: Internet</strong></td><td>인터넷 (현재의 NLP)</td><td>웹 텍스트, 이미지</td><td>BERT, GPT-4, LLaMA</td><td><strong>구경꾼(Spectator)의 정점.</strong> 방대한 지식을 가지지만 물리적 실체와 연결되지 않음.</td></tr>
<tr><td><strong>WS3: Perception</strong></td><td>지각 (멀티모달)</td><td>비디오, 오디오</td><td>멀티모달 LLM (Gemini 등)</td><td>시각과 청각을 통해 세계를 ‘보지만’, 여전히 개입할 수는 없음. 수동적 관찰자.</td></tr>
<tr><td><strong>WS4: Embodiment</strong></td><td><strong>체화 (행위자)</strong></td><td>물리 시뮬레이션, 로봇</td><td><strong>Embodied AI Agents</strong></td><td><strong>행위자(Actor)의 시작.</strong> 행동을 통해 세계를 변화시키고 인과관계를 학습.</td></tr>
<tr><td><strong>WS5: Social</strong></td><td>사회 (상호작용)</td><td>다중 에이전트 환경</td><td>Social Robots, Human-AI Team</td><td>타자의 의도를 파악하고 협력. 사회적 맥락에서의 행위자.</td></tr>
</tbody></table>
<p>9</p>
<p>현재 우리는 WS2와 WS3의 경계에서 WS4로 나아가고 있다. Bisk의 주장에 따르면, WS2(인터넷) 수준의 AI는 아무리 데이터가 많아져도 “라디오를 듣고 언어를 배우는 것“과 같아서, 언어가 지시하는 물리적 맥락을 완전히 이해할 수 없다.9 WS4(체화) 단계로 진입해야만 AI는 “무겁다“라는 단어의 의미를 모터의 부하(torque) 증가와 배터리 소모라는 신체적 경험을 통해 진정으로 ’이해’할 수 있게 된다.</p>
<h2>5.  인과관계의 사다리: 보는 것(Seeing)과 하는 것(Doing)</h2>
<h3>5.1  Judea Pearl의 인과성 계층</h3>
<p>튜링상 수상자인 Judea Pearl은 그의 저서 『The Book of Why』와 관련 연구들을 통해 인과 추론의 능력을 세 단계의 사다리로 비유했다.19 이 사다리 이론은 구경꾼 AI와 행위자 AI의 지적 능력을 구분하는 가장 강력한 이론적 틀을 제공한다.</p>
<ol>
<li><strong>1층: 관찰 (Association / Seeing)</strong></li>
</ol>
<ul>
<li>질문: “내가 <span class="math math-inline">X</span>를 본다면, <span class="math math-inline">Y</span>의 확률은 얼마인가?” (<span class="math math-inline">P(y|x)</span>)</li>
<li>활동: 관찰, 탐지, 예측.</li>
<li>주체: <strong>구경꾼 (Spectator)</strong>. 부엉이처럼 관찰하지만 개입하지 않는다. 딥러닝 기반의 상관관계 분석이 여기에 해당한다.</li>
</ul>
<ol start="2">
<li><strong>2층: 개입 (Intervention / Doing)</strong></li>
</ol>
<ul>
<li>질문: “내가 <span class="math math-inline">X</span>를 한다면, <span class="math math-inline">Y</span>는 어떻게 될 것인가?” (<span class="math math-inline">P(y|do(x))</span>)</li>
<li>활동: 실험, 행동, 정책 결정.</li>
<li>주체: <strong>행위자 (Actor)</strong>. 환경을 조작하여 결과를 확인한다.</li>
</ul>
<ol start="3">
<li><strong>3층: 반사실적 사고 (Counterfactuals / Imagining)</strong></li>
</ol>
<ul>
<li>질문: “내가 그때 <span class="math math-inline">X</span>를 하지 않았더라면, <span class="math math-inline">Y</span>는 어떻게 되었을까?” (<span class="math math-inline">P(y_x|x&#39;, y&#39;)</span>)</li>
<li>활동: 상상, 후회, 원인 분석.</li>
<li>주체: 고도로 발달한 행위자 혹은 인간.</li>
</ul>
<h3>5.2  보는 것과 하는 것의 수학적 불일치</h3>
<p>구경꾼 AI는 1층에 머물러 있다. 그들은 <span class="math math-inline">P(y|x)</span>를 아주 정확하게 계산할 수 있다. 예를 들어, “아이스크림 판매량(<span class="math math-inline">X</span>)이 높으면 익사 사고(<span class="math math-inline">Y</span>)도 많다“는 데이터를 학습한다. 그러나 여기서 구경꾼 AI는 치명적인 오류를 범할 수 있다. 만약 아이스크림 판매를 금지하면(<span class="math math-inline">do(X=0)</span>), 익사 사고가 줄어들 것이라고 잘못 예측할 수 있는 것이다.</p>
<p>행위자 AI는 2층의 능력, 즉 <strong>do-calculus</strong>를 수행할 수 있다. 로봇이 직접 아이스크림 판매를 중단시키는 행동을 시뮬레이션해본다(<span class="math math-inline">do(X)</span>). 그 결과 익사 사고(<span class="math math-inline">Y</span>)에 변화가 없음을 확인한다. 이를 통해 두 변수 사이에 인과관계가 없으며, 실제로는 ’여름의 더위(<span class="math math-inline">Z</span>)’라는 교란 변수(Confounder)가 양쪽에 영향을 미쳤음을 깨닫는다.8</p>
<p>수식으로 표현하면, 구경꾼의 확률 <span class="math math-inline">P(Y|X)</span>는 다음과 같이 단순히 조건부 확률로 정의된다:<br />
<span class="math math-display">
P(Y|X) = \frac{P(X, Y)}{P(X)}
</span><br />
하지만 행위자의 확률 <span class="math math-inline">P(Y|do(X))</span>는 <span class="math math-inline">X</span>로 들어오는 모든 인과적 화살표를 끊어내고(graph surgery), <span class="math math-inline">X</span>를 강제로 고정한 상태에서의 확률을 의미한다.[20]<br />
<span class="math math-display">
P(Y|do(X)) \neq P(Y|X)
</span><br />
이 부등식(<span class="math math-inline">\neq</span>)이 바로 구경꾼과 행위자의 결정적 차이다. 구경꾼은 관찰된 분포에 편향되어 있지만, 행위자는 개입을 통해 그 편향을 깨뜨리고 진정한 인과 구조를 파악할 수 있다.</p>
<h3>5.3  데이터 생성의 주체로서의 행위자</h3>
<p>행위자 AI는 단순히 데이터를 소비하는 것이 아니라, 자신의 행동을 통해 <strong>반사실적 데이터를 생성</strong>할 잠재력을 가진다. 로봇 팔이 컵을 밀어 떨어뜨리는 실험은 “컵을 밀지 않았더라면 떨어지지 않았을 것“이라는 반사실적 추론을 검증하는 물리적 과정이다. 이는 정적 데이터셋만으로는 절대 도달할 수 없는 지식의 영역이다. Pearl은 기계가 진정한 지능을 갖추기 위해서는 반드시 이 ’개입’의 단계를 통과해야 한다고 역설했다.22</p>
<h2>6.  능동적 추론(Active Inference): 불확실성에 맞서는 행위자의 동기</h2>
<h3>6.1  자유 에너지 원리(Free Energy Principle)와 행위의 목적</h3>
<p>행위자 AI가 끊임없이 움직이고 탐색해야 하는 내재적 동기는 무엇인가? Karl Friston의 <strong>능동적 추론(Active Inference)</strong> 이론은 이를 생물학적이고 물리학적인 원리로 설명한다.24 이 이론에 따르면, 모든 지능체(인간, 동물, 그리고 고도화된 AI)의 궁극적인 목표는 자신이 가진 ’세계에 대한 모델’과 실제 ‘감각 입력’ 사이의 괴리, 즉 **변분 자유 에너지(Variational Free Energy)**를 최소화하는 것이다.</p>
<p>자유 에너지를, 혹은 간단히 말해 ’놀라움(Surprise)’이나 ’예측 오차(Prediction Error)’를 최소화하는 방법은 두 가지가 있다26:</p>
<ol>
<li><strong>지각적 추론 (Perceptual Inference)</strong>: 들어오는 감각 데이터에 맞춰 나의 내부 모델(믿음)을 수정한다. (세계를 이해함)</li>
<li><strong>능동적 추론 (Active Inference)</strong>: 나의 내부 모델(예측)에 맞춰 세계를 변화시키도록 행동한다. (세계에 개입함)</li>
</ol>
<p>구경꾼 AI는 첫 번째 방법인 지각적 추론만을 수행한다. 데이터가 들어오면 가중치를 수정하여 모델을 데이터에 맞춘다. 그러나 행위자 AI는 두 번째 방법, 즉 행동을 통해 감각 입력을 능동적으로 선택하고 조작한다.</p>
<h3>6.2  탐험(Exploration)과 활용(Exploitation)의 통합</h3>
<p>능동적 추론은 행위자의 행동을 두 가지 가치로 설명한다.28</p>
<ul>
<li><strong>실용적 가치 (Pragmatic Value)</strong>: 목표를 달성하기 위한 행동. (예: 배터리를 충전하기 위해 도킹 스테이션으로 이동)</li>
<li><strong>인식론적 가치 (Epistemic Value)</strong>: 불확실성을 줄이기 위한 행동. (예: 어두운 방에서 스위치를 켜는 행위)</li>
</ul>
<p>기존의 강화학습(Reinforcement Learning)이 주로 외재적 보상(Reward)을 최대화하는 데 초점을 맞췄다면, 능동적 추론은 보상이 없더라도 정보 그 자체를 얻기 위한 행동(호기심)을 수학적으로 유도해낸다.30 행위자 AI는 “내가 모르는 것이 무엇인지 알기 위해” 움직인다. 로봇이 낯선 물체를 집어 이리저리 돌려보는 것은 누가 시켜서가 아니라, 그 물체의 3차원 구조에 대한 자신의 예측 오차를 줄이기 위한 본능적(수학적) 행동이다. 이는 구경꾼 AI가 수동적으로 레이블링 된 데이터를 기다리는 것과 극명하게 대조된다.</p>
<h2>7.  시뮬레이션과 현실: 행위자를 위한 훈련장</h2>
<h3>7.1  체화된 AI 시뮬레이터의 생태계</h3>
<p>행위자 AI를 물리적 현실에서 바로 학습시키는 것은 비용이 많이 들고 위험하다. 수만 번의 시행착오(Trial and Error)가 필요한 강화학습을 실제 로봇으로 수행하면 기계가 파손되거나 안전사고가 발생할 수 있다. 따라서 ’시뮬레이션’은 구경꾼 AI가 행위자 AI로 진화하는 인큐베이터 역할을 한다.1</p>
<p>최근 등장한 고성능 시뮬레이터들은 단순한 3D 게임 화면을 넘어, 물리 법칙(중력, 마찰, 충돌)을 정교하게 모사한다. 이를 통해 에이전트는 가상 공간에서 수백만 번의 ’행위’를 수행하며 인과관계를 학습한다.</p>
<p><strong>표 1. 주요 체화된 AI (Embodied AI) 시뮬레이터 비교</strong></p>
<table><thead><tr><th><strong>시뮬레이터 (Simulator)</strong></th><th><strong>기반 엔진 (Engine)</strong></th><th><strong>주요 특징 및 환경 구성</strong></th><th><strong>지원 작업 (Tasks)</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>AI2-THOR</strong></td><td>Unity 3D</td><td>상호작용 가능한 가정 환경 (120개 룸). 물체 열기/닫기, 켜기/끄기 등 물리적 상태 변화 가능.</td><td>Visual Navigation, Interaction, Manipulation</td><td>물체 조작에 특화됨. 1</td></tr>
<tr><td><strong>Habitat</strong></td><td>자체 엔진</td><td>실제 환경의 고해상도 3D 스캔(Matterport3D 등) 기반. 초고속 렌더링 (10,000+ FPS).</td><td>PointGoal Navigation, Embodied QA</td><td>사실적 시각 정보와 속도에 강점. 32</td></tr>
<tr><td><strong>DeepMind Lab</strong></td><td>Quake III</td><td>공상과학적 미로 환경. 시각적 복잡성보다는 인지적 과제 중심.</td><td>Reinforcement Learning, Maze Navigation</td><td>강화학습 연구용. 1</td></tr>
<tr><td><strong>iGibson</strong></td><td>PyBullet</td><td>상호작용 가능한 대규모 환경. 로봇의 물리적 조작과 센서 시뮬레이션 통합.</td><td>Robotic Manipulation, Social Navigation</td><td>촉각, 열화상 등 멀티모달 센서 지원.</td></tr>
</tbody></table>
<p>1</p>
<p>이러한 시뮬레이터에서 AI는 단순한 분류(Classification)가 아니라, <strong>시각적 내비게이션(Visual Navigation)</strong>, **체화된 질의응답(Embodied QA)**과 같은 복합적인 과제를 수행한다. 예를 들어, “냉장고에 사과가 있나?“라는 질문에 답하기 위해, AI는 주방으로 이동(Navigation)하고, 냉장고 문을 여는 물리적 조작(Manipulation)을 수행한 뒤, 시각적으로 확인(Perception)해야 한다. 이는 WS4(체화) 단계의 전형적인 과업이다.</p>
<h3>7.2  Sim-to-Real: 현실 격차(Reality Gap)의 극복</h3>
<p>시뮬레이션에서 훈련된 행위자를 현실(Real World)로 옮길 때 **‘현실 격차(Reality Gap)’**라는 거대한 장벽에 부딪힌다.33 시뮬레이션은 필연적으로 현실의 복잡성을 단순화한 모델이기 때문이다. 시뮬레이션에서는 마찰 계수가 일정하지만, 현실에서는 바닥의 먼지 하나 때문에 로봇이 미끄러질 수 있다.</p>
<ul>
<li><strong>도메인 무작위화(Domain Randomization)</strong>: 시뮬레이션의 질감, 조명, 물리 파라미터를 극단적으로 다양하게 무작위화하여 학습시킨다. 이렇게 하면 에이전트는 현실 세계를 ‘시뮬레이션의 또 다른 변주’ 중 하나로 인식하게 되어 강건성(Robustness)을 갖게 된다.</li>
<li><strong>적응형 학습</strong>: 현실 세계에서 행동하며 발생하는 예측 오차를 통해 시뮬레이션 모델을 실시간으로 미세 조정(Fine-tuning)한다.</li>
</ul>
<p>구경꾼 AI에게는 이러한 ’격차’의 개념이 희미하다. 그들에게는 훈련 데이터셋과 테스트 데이터셋의 분포 차이(Distribution Shift)만이 문제 될 뿐, 물리 법칙의 불일치는 고려 대상이 아니다. 그러나 행위자 AI에게 Reality Gap의 극복은 생존의 문제다.</p>
<h3>7.3  사회적 상호작용으로의 확장</h3>
<p>마지막으로, 행위자 AI는 물리적 환경뿐만 아니라 사회적 환경에서도 행위자(Social Actor)가 되어야 한다.34 인간과 로봇이 공존하는 공간에서, AI는 인간의 행동을 관찰하는 것(구경꾼)을 넘어, 인간의 의도를 추론하고 협력적 행동을 해야 한다. “도와줘“라는 말을 듣고 단순히 텍스트를 분석하는 것이 아니라, 상대방이 들고 있는 무거운 짐을 인식하고 다가가서 들어주는 행동을 하는 것, 이것이 진정한 **사회적 접지(Social Grounding)**이다.35 이는 Bisk가 말한 최상위 단계인 WS5(Social)에 해당한다.</p>
<h2>8. 결론: 구경꾼의 시대를 넘어</h2>
<p>우리는 지금 ’인터넷 AI’라는 거대한 도서관에서 나와, ’체화된 AI’라는 미지의 대지로 나아가는 문턱에 서 있다. 구경꾼(Spectator)으로서의 AI는 인류가 축적한 지식의 패턴을 읽어내는 데 탁월한 능력을 보여주었다. 그들은 셰익스피어의 희곡을 분석하고37, 수백만 장의 사진을 분류할 수 있다. 그러나 그들은 자신이 읽는 단어의 무게를 느끼지 못하고, 자신이 분류하는 사물을 만질 수 없다. 그들은 동굴 벽에 비친 그림자를 보고 세상이라 믿는 플라톤의 수감자와 같다.</p>
<p>행위자(Actor)로서의 AI는 이 한계를 뛰어넘는다. 그들은 신체라는 매개를 통해 세상과 직접 충돌한다. 그들은 행동을 통해 불확실성을 지우고(능동적 추론), 개입을 통해 인과관계를 검증하며(인과적 추론), 경험을 통해 언어에 생명을 불어넣는다(접지). 이 전환은 단순히 AI가 로봇 몸체를 갖게 된다는 공학적 사실 이상의 의미를 가진다. 그것은 AI가 <strong>’관찰자’에서 ’참여자’로</strong>, <strong>’객체’에서 ’주체’로</strong> 진화함을 의미한다.</p>
<p>진정한 인공 일반 지능(AGI)은 정적인 데이터센터의 서버 랙 사이에서 태어나는 것이 아니라, 로봇이 넘어지고 다시 일어서며 세상을 배우는 그 역동적인 상호작용의 현장에서 탄생할 것이다.2 이제 AI는 무대 밖의 어둠에서 걸어 나와, 무대 위에서 자신의 서사를 쓰는 행위자가 되어야 한다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>A Survey of Embodied AI: From Simulators to Research Tasks - arXiv, https://arxiv.org/pdf/2103.04918</li>
<li>A Call for Embodied AI - arXiv, https://arxiv.org/html/2402.03824v4</li>
<li>Embodied AI-Driven Operation of Smart Cities: A Concise Review - SciSpace, https://scispace.com/pdf/embodied-ai-driven-operation-of-smart-cities-a-concise-22m7z045dg.pdf</li>
<li>(PDF) Frontier Exploration of the Fusion of Embodied Intelligence and Large Language Models - ResearchGate, https://www.researchgate.net/publication/394372026_Frontier_Exploration_of_the_Fusion_of_Embodied_Intelligence_and_Large_Language_Models</li>
<li>Creative Action at a Distance: A Conceptual Framework for Embodied Performance With Robotic Actors - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2021.662182/full</li>
<li>Performing the Post-Anthropocene | TDR | Cambridge Core, https://www.cambridge.org/core/journals/the-drama-review/article/performing-the-postanthropocene/B9DE2559209DA4FFED282EED03E73620</li>
<li>The (AI) Actor’s Paradox - Acting Everyday, https://actingeveryday.com/blog/ai-actors-paradox/</li>
<li>Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering | Request PDF - ResearchGate, https://www.researchgate.net/publication/371415070_Cross-Modal_Causal_Relational_Reasoning_for_Event-Level_Visual_Question_Answering</li>
<li>Experience Grounds Language - DSpace@MIT, https://dspace.mit.edu/bitstream/handle/1721.1/142865/2020.emnlp-main.703.pdf?sequence=2&amp;isAllowed=y</li>
<li>Comparison: Internet AI vs. embodied AI | Download Scientific Diagram - ResearchGate, https://www.researchgate.net/figure/Comparison-Internet-AI-vs-embodied-AI_tbl1_394372026</li>
<li>(PDF) Fast and Robust Multi-Person 3D Pose Estimation From Multiple Views, https://www.researchgate.net/publication/338511900_Fast_and_Robust_Multi-Person_3D_Pose_Estimation_From_Multiple_Views</li>
<li>Modeling the Relationship Between Gesture Motion and Meaning - Enlighten Theses, https://theses.gla.ac.uk/83292/1/2022SaundPhD.pdf</li>
<li>Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI - arXiv, https://arxiv.org/html/2407.06886v1</li>
<li>What Foundation Models can Bring for Robot Learning in Manipulation : A Survey - arXiv, https://arxiv.org/html/2404.18201v4</li>
<li>Multi-Modal Perception with Vision, Language, and Touch for Robot Manipulation - UC Berkeley EECS, https://www2.eecs.berkeley.edu/Pubs/TechRpts/2025/EECS-2025-68.pdf</li>
<li>Experience Grounds Language | Request PDF - ResearchGate, https://www.researchgate.net/publication/340826958_Experience_Grounds_Language</li>
<li>Experience Grounds Language - Yonatan Bisk, https://yonatanbisk.com/egl.html</li>
<li>Grounding ‘Grounding’ in NLP - CMU School of Computer Science, https://www.cs.cmu.edu/~awb/papers/2021.findings-acl.375.pdf</li>
<li>The Book of Why - UCLA Cognitive Systems Laboratory (Experimental), https://bayes.cs.ucla.edu/WHY/errata-pages-PearlMackenzie_BookofWhy_Final.pdf</li>
<li>Data Analysis for Economics, https://www.sea-stat.com/wp-content/uploads/2021/10/data-analysis-for-economics-a-modern-introduction.pdf</li>
<li>1 THE MECHANIZATION OF CAUSAL INFERENCE: A “mini” Turing, http://web.cs.ucla.edu/~kaoru/aaai-turing-mini-test-july2012-bw.pdf</li>
<li>A World Model framework with digital twin for scheduling demand-side resources under systemic complexity - ResearchGate, https://www.researchgate.net/publication/399008749_A_World_Model_framework_with_digital_twin_for_scheduling_demand-side_resources_under_systemic_complexity</li>
<li>Demonstrative Learning for Human-Agent Knowledge Transfer | Request PDF - ResearchGate, https://www.researchgate.net/publication/385671026_Demonstrative_Learning_for_Human-Agent_Knowledge_Transfer</li>
<li>Generating meaning: active inference and the scope and limits of passive AI - Figshare, https://sussex.figshare.com/articles/journal_contribution/Generating_meaning_active_inference_and_the_scope_and_limits_of_passive_AI/28789265</li>
<li>Simulating Active Inference Processes by Message Passing - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2019.00020/full</li>
<li>Predictive Processing and the Representation Wars - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC6566209/</li>
<li>How Active Inference Could Help Revolutionise Robotics - PMC - PubMed Central - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC8946999/</li>
<li>Active digital twins via active inference - Dipartimento di Matematica, https://www.mate.polimi.it/biblioteca/add/qmox/41-2025.pdf</li>
<li>Active Digital Twins via Active Inference - arXiv, https://arxiv.org/html/2506.14453v1</li>
<li>Causality in the human niche: lessons for machine learning - arXiv, https://arxiv.org/html/2506.13803v1</li>
<li>[PDF] A Survey of Embodied AI: From Simulators to Research Tasks | Semantic Scholar, https://www.semanticscholar.org/paper/A-Survey-of-Embodied-AI%3A-From-Simulators-to-Tasks-Duan-Yu/9c404d02aefd850ac3d5a8bdc5860738e6cd2b04</li>
<li>(PDF) Embodied Multi-Agent Systems: A Review - ResearchGate, https://www.researchgate.net/publication/392742472_Embodied_Multi-Agent_Systems_A_Review</li>
<li>How Will Internet AI Crossover to the Physical World? - Duality AI, https://www.duality.ai/blog/embodied-ai</li>
<li>Shared worlds, shared minds: Strategies to develop physically and socially embedded AI, https://pmc.ncbi.nlm.nih.gov/articles/PMC12420807/</li>
<li>Social Sensorimotor Contingencies - DiVA portal, http://www.diva-portal.org/smash/get/diva2:920840/FULLTEXT01.pdf</li>
<li>An Active-Inference Approach to Second-Person Neuroscience - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC11539477/</li>
<li>How AI Changes Theatrical Publics - Alexa Alice Joubin, https://ajoubin.org/wp-content/uploads/2024/02/Joubin-AI-theatre.pdf</li>
<li>Performing/Watching Artificial Intelligence on Stage - UniTo, https://iris.unito.it/retrieve/handle/2318/1793939/774110/26-98-PB.pdf</li>
<li>Embodied Multi-Agent Systems: A Review - IEEE/CAA Journal of Automatica Sinica, https://www.ieee-jas.net/article/doi/10.1109/JAS.2025.125552</li>
<li>Multi-modal Active Perception for Autonomously Selecting Landing Sites on Icy Moons, https://ntrs.nasa.gov/api/citations/20170011648/downloads/20170011648.pdf</li>
<li>Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI, https://arxiv.org/html/2407.06886v4</li>
<li>Active inference and robot control: a case study - UCL Discovery, https://discovery.ucl.ac.uk/1520078/1/20160616.full.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>