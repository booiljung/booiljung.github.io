<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:1.1.3 엠바디드 AI의 정의: "지능은 환경과의 상호작용을 통해 발현된다"는 정의와 범위 (가상 에이전트 vs 물리 로봇).</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>1.1.3 엠바디드 AI의 정의: "지능은 환경과의 상호작용을 통해 발현된다"는 정의와 범위 (가상 에이전트 vs 물리 로봇).</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 1. Embodied AI의 태동: 뇌를 가진 신체</a> / <a href="index.html">1.1 인터넷 AI에서 엠바디드 AI로 (From Internet AI to Embodied AI)</a> / <span>1.1.3 엠바디드 AI의 정의: "지능은 환경과의 상호작용을 통해 발현된다"는 정의와 범위 (가상 에이전트 vs 물리 로봇).</span></nav>
                </div>
            </header>
            <article>
                <h1>1.1.3 엠바디드 AI의 정의: “지능은 환경과의 상호작용을 통해 발현된다“는 정의와 범위 (가상 에이전트 vs 물리 로봇).</h1>
<h2>1.  서론: 지능의 패러다임 전환과 엠바디먼트의 부상</h2>
<p>인공지능(Artificial Intelligence, AI)의 역사는 ’지능이란 무엇인가?’라는 본질적인 질문에 대한 답을 찾아가는 과정이었다. 초기 인공지능 연구, 이른바 ’고전적 AI(Good Old-Fashioned AI, GOFAI)’는 지능을 기호(Symbol)의 조작과 논리적 연산 능력으로 정의했다. 1950년대 앨런 튜링(Alan Turing)이 제안한 이미테이션 게임부터 1990년대의 체스 컴퓨터 딥블루에 이르기까지, 주류 AI는 “뇌는 컴퓨터와 같고, 마음은 소프트웨어와 같다“는 전산주의(Computationalism)에 입각해 있었다. 이 관점에서 신체는 단지 뇌의 명령을 수행하는 수동적인 입출력 장치(Peripheral Device)에 불과했으며, 지능은 신체와 분리되어 존재할 수 있는 순수한 알고리즘으로 간주되었다.1</p>
<p>그러나 이러한 접근법은 체스와 같은 닫힌 세계(Closed World)에서는 성공적이었으나, 컵을 잡거나 복잡한 거리를 걷는 것과 같은 물리적 세계의 동적인 문제들 앞에서는 무력함을 드러냈다. 모라벡의 역설(Moravec’s Paradox)이 시사하듯, 인간에게 어려운 고차원적 추론은 컴퓨터에게 쉽고, 인간에게 쉬운 감각-운동(Sensorimotor) 능력은 컴퓨터에게 지극히 어렵다는 사실이 밝혀졌다. 여기서 ’엠바디드 AI(Embodied AI, 체화된 인공지능)’라는 새로운 패러다임이 태동한다. 엠바디드 AI는 데카르트의 “나는 생각한다, 고로 존재한다(Cogito, ergo sum)“는 명제를 거부하고, “나는 움직인다, 고로 이해한다(Moveo, ergo intelligo)“는 철학적 기조 위에 선다.</p>
<p>엠바디드 AI의 핵심 정의는 **“지능은 뇌(Brain), 신체(Body), 그리고 환경(Environment)의 동적인 상호작용(Interaction)을 통해 창발(Emerge)한다”**는 것이다.1 이는 지능이 고립된 연산 장치 내부의 코드가 아니라, 에이전트가 물리적 세계에 존재하며(Being-in-the-world) 겪는 체험의 산물임을 의미한다. 본 보고서는 이러한 정의의 이론적 배경과 현대적 의미를 심층 분석하고, 그 범위가 물리적 로봇(Physical Robots)을 넘어 가상 에이전트(Virtual Agents)로 확장되는 논쟁적 지점과 최신 기술 동향(Sim-to-Real, Embodied Foundation Models)을 포괄적으로 고찰한다.</p>
<h2>2.  이론적 배경: 엠바디먼트 가설 (The Embodiment Hypothesis)</h2>
<p>엠바디드 AI의 정의는 단순한 공학적 필요에 의해 탄생한 것이 아니라, 인지과학, 심리학, 철학의 깊은 뿌리를 가지고 있다. 이를 이해하기 위해서는 ’엠바디먼트 가설(Embodiment Hypothesis)’의 다층적인 의미를 살펴볼 필요가 있다.</p>
<h3>2.1  인지과학과 현상학적 기원</h3>
<p>전통적 인지주의는 인간의 마음을 외부 세계를 내부적으로 표상(Representation)하고 조작하는 정보 처리 시스템으로 보았다. 그러나 마르틴 하이데거(Martin Heidegger)와 모리스 메를로-퐁티(Maurice Merleau-Ponty)와 같은 현상학자들은 인간이 세계를 추상적으로 분석하기 이전에, 신체를 통해 세계와 관계 맺고 있음을 강조했다.6 메를로-퐁티는 “신체는 세계에 닻을 내리는 지점“이라고 표현하며, 우리의 인지 능력 자체가 신체적 구조와 감각 경험에 의해 형성된다고 주장했다.</p>
<p>이러한 철학적 통찰은 인공지능 연구에 지대한 영향을 미쳤다. 프란시스코 바렐라(Francisco Varela) 등의 연구자들은 ‘체화된 인지(Embodied Cognition)’ 이론을 통해, 인지는 독립된 뇌의 산물이 아니라 신체 활동의 결과물임을 역설했다. 즉, 우리가 ’위’와 ’아래’라는 개념을 이해하는 것은 중력이라는 물리적 환경 속에서 직립 보행하는 신체를 가지고 있기 때문이며, ’뜨거움’을 이해하는 것은 열을 감지하고 회피하는 감각-운동 시스템을 가지고 있기 때문이다.2</p>
<h3>2.2  로드니 브룩스: 재현 없는 지능 (Intelligence without Representation)</h3>
<p>1991년, MIT의 로봇 공학자 로드니 브룩스(Rodney Brooks)는 그의 기념비적인 논문 “재현 없는 지능(Intelligence without representation)” 3을 통해 고전적 AI의 정설을 정면으로 반박했다. 그는 복잡한 세계 모델(World Model)을 구축하고 기호 논리로 계획을 수립하는 ‘Sense-Model-Plan-Act’ 패러다임이 실제 로봇에게는 비효율적이며 불가능에 가깝다고 비판했다.</p>
<p>브룩스는 대신 **“세계 자체가 최고의 모델(The world is its own best model)”**이라고 선언했다. 그는 로봇이 내부적으로 복잡한 지도를 그리는 대신, 환경의 신호에 즉각적으로 반응하는 포섭 구조(Subsumption Architecture)를 제안했다. 예를 들어, 로봇이 장애물을 피하기 위해 장애물의 3D 좌표를 계산하는 것이 아니라, 근접 센서가 감지되면 바퀴를 반대로 회전시키는 단순한 반사 행동(Reflex)을 층층이 쌓아 올림으로써 복잡한 지능적 행동이 ’창발’될 수 있음을 보였다.3 이는 엠바디드 AI가 추상적 연산이 아닌, 환경과의 직접적인 물리적 결합(Coupling)을 통해 정의되어야 함을 강력하게 시사한다.</p>
<h3>2.3  린다 스미스: 발달심리학이 주는 6가지 교훈</h3>
<p>심리학자 린다 스미스(Linda Smith)와 마이클 개서(Michael Gasser)는 2005년 연구 11에서 아기의 지능 발달 과정을 분석하여 엠바디드 AI 설계를 위한 6가지 핵심 교훈을 제시했다. 이들의 연구는 “지능은 상호작용에서 온다“는 정의를 구체적인 메커니즘으로 설명해 준다.</p>
<table><thead><tr><th><strong>교훈 (Lesson)</strong></th><th><strong>내용 및 엠바디드 AI 함의</strong></th></tr></thead><tbody>
<tr><td><strong>1. 다중 감각 경험</strong> (Multimodal Experience)</td><td>아기의 경험은 시각, 청각, 촉각이 시간적으로 동기화(time-locked)되어 있다. 물체를 흔들 때(고유수용감각) 소리가 나고(청각) 물체가 움직이는 모습(시각)이 동시에 입력되며, 이 상관관계 속에서 ’물체’라는 개념이 형성된다. AI 또한 단일 모달리티가 아닌 다중 센서 융합을 통해 학습해야 함을 시사한다.</td></tr>
<tr><td><strong>2. 점진적 발달</strong> (Incremental Development)</td><td>지능은 완성된 채로 주어지지 않는다. 아기의 제한된 시력과 운동 능력은 오히려 정보의 과부하를 막고 학습을 돕는 제약 조건으로 작용한다. AI 에이전트 역시 간단한 신체와 환경에서 시작해 점차 복잡도를 높여가는 커리큘럼 학습(Curriculum Learning)이 필요하다.</td></tr>
<tr><td><strong>3. 물리적 세계의 규칙성</strong> (Physical Regularities)</td><td>중력, 관성, 공간의 연속성 등 물리적 세계가 제공하는 강력한 제약과 규칙성은 학습의 비계(Scaffolding) 역할을 한다. 지능은 뇌 속에만 있는 것이 아니라, 환경 구조 속에 분산되어 있다.</td></tr>
<tr><td><strong>4. 탐색을 통한 학습</strong> (Exploration)</td><td>아기는 수동적인 관찰자가 아니다. 옹알이를 하거나 장난감을 떨어뜨리는 등 능동적 탐색(Active Exploration)을 통해 스스로 학습 데이터를 생성한다. 이는 엠바디드 AI의 ‘호기심 기반 탐색’ 연구로 이어진다.</td></tr>
<tr><td><strong>5. 사회적 상호작용</strong> (Social Interaction)</td><td>양육자와의 상호작용은 무엇이 중요한 정보인지 알려주는 가이드 역할을 한다. 엠바디드 AI는 인간과의 협업 및 사회적 신호(Social Cues) 인식을 포함해야 한다.</td></tr>
<tr><td><strong>6. 언어의 상징적 학습</strong> (Symbolic Learning)</td><td>언어는 추상적인 기호가 아니라, 물리적, 사회적 맥락 속에 접지(Grounded)된 상태로 습득된다. 이는 현대의 비전-언어 모델(VLM)이 지향하는 바와 일치한다.</td></tr>
</tbody></table>
<p>이러한 이론적 배경을 종합하면, 엠바디드 AI는 단순히 로봇 하드웨어를 제어하는 기술이 아니다. 그것은 <strong>신체적 경험과 환경적 제약을 통해 정보를 구조화하고, 세상의 의미를 구성해 나가는 구성주의적(Constructivist) 지능 시스템</strong>으로 정의된다.</p>
<h2>3.  정의의 심층 분석: 인터넷 AI vs. 엠바디드 AI</h2>
<p>엠바디드 AI의 본질을 명확히 하기 위해서는 현재 AI 기술의 주류를 이루고 있는 ’인터넷 AI(Internet AI)’와의 대조가 필수적이다. 대형언어모델(LLM)이나 생성형 이미지 모델로 대표되는 인터넷 AI는 놀라운 성능을 보여주지만, 엠바디드 AI와는 근본적으로 다른 데이터 철학과 존재론적 기반을 가지고 있다.14</p>
<h3>3.1  I.I.D. 가정의 붕괴와 비정상성 (Non-Stationarity)</h3>
<p>기계학습(Machine Learning) 이론의 대부분은 훈련 데이터가 ’독립적이고 동일한 분포(Independent and Identically Distributed, I.I.D.)’를 따른다는 가정에 기초한다. 인터넷 AI가 학습하는 정적 데이터셋(Static Datasets)은 이 가정을 어느 정도 만족시킨다. 예를 들어, ImageNet의 개와 고양이 사진들은 서로 독립적으로 촬영되었으며, 순서가 바뀌어도 학습에 큰 영향을 주지 않는다.17</p>
<p>그러나 엠바디드 AI가 마주하는 현실 세계는 I.I.D. 가정이 완전히 붕괴되는 공간이다.19</p>
<ul>
<li><strong>시간적 종속성(Temporal Dependency):</strong> 엠바디드 에이전트의 현재 상태는 과거의 상태와 행동에 의해 결정된다. 로봇이 <span class="math math-inline">t</span> 시점에 취한 행동 <span class="math math-inline">a_t</span>는 <span class="math math-inline">t+1</span> 시점의 관측 <span class="math math-inline">o_{t+1}</span>에 직접적인 인과적 영향을 미친다. 데이터는 독립적이지 않으며(Non-Independent), 강력한 시간적 상관관계를 가진다.</li>
<li><strong>분포의 변화(Distribution Shift):</strong> 로봇이 새로운 방으로 이동하거나, 조명 조건이 바뀌거나, 도구를 사용하여 환경을 조작하면 입력 데이터의 통계적 분포가 급격하게 변한다(Non-Identical). 인터넷 AI는 이러한 분포 변화(Out-of-Distribution, OOD)에 취약하지만, 엠바디드 AI는 변화하는 분포에 지속적으로 적응(Adaptation)해야 한다.1</li>
</ul>
<h3>3.2  관찰자(Spectator) 대 행위자(Actor)</h3>
<p>인터넷 AI는 ’관찰자’의 입장에서 세상을 배운다. 인터넷상의 이미지는 누군가에 의해 이미 촬영되고 선별된 ’박제된 과거’이다. 반면, 엠바디드 AI는 ’행위자’로서 환경에 개입한다.</p>
<ul>
<li><strong>인터넷 AI:</strong> “이 이미지에 사과가 있는가?” (분류 문제)</li>
<li><strong>엠바디드 AI:</strong> “이 사과를 집으려면 내 손을 어떻게 움직여야 하며, 내가 건드리면 사과는 굴러떨어질 것인가?” (인과적 추론 및 제어 문제)</li>
</ul>
<p>14의 비교에 따르면, 인터넷 AI는 의미론적 이해(Semantic Understanding)에 강점이 있지만 물리적 인과관계 파악에 약점이 있다. 반면 엠바디드 AI는 물리적 상호작용을 통해 실질적인 지능을 발현하지만, 초기에는 의미론적 지식이 부족할 수 있다. 최근의 연구(PaLM-E, RT-2)는 이 두 영역을 통합하려는 시도다.</p>
<h3>3.3  심볼 그라운딩 문제 (Symbol Grounding Problem)</h3>
<p>엠바디드 AI의 정의에서 가장 중요한 철학적 난제 중 하나는 ’심볼 그라운딩’이다. LLM은 “사과는 빨갛고 둥글다“라는 문장을 생성할 수 있지만, ’빨갛다’는 시각적 경험이나 ’둥글다’는 촉각적 경험을 알지 못한다. 이는 마치 중국어를 모르는 사람이 중국어 사전만 보고 중국어 문장을 조합하는 것과 같다(Searle의 ‘중국어 방’ 논증).</p>
<p>엠바디드 AI는 이 문제를 해결한다. 로봇이 사과를 보고(Vision), 만지고(Touch), 들어 올리는(Proprioception) 감각-운동 경험을 통해 “사과“라는 기호(Symbol)는 물리적 실체에 접지(Grounding)된다.21 따라서 엠바디드 AI의 지능은 텍스트 통계 확률이 아니라, <strong>물리적 세계와의 직접적인 접촉 경험에 기반한 참된 이해</strong>를 지향한다.</p>
<table><thead><tr><th><strong>구분</strong></th><th><strong>인터넷 AI (Internet AI)</strong></th><th><strong>엠바디드 AI (Embodied AI)</strong></th></tr></thead><tbody>
<tr><td><strong>데이터 원천</strong></td><td>정적 데이터셋 (Web Text, Images)</td><td>동적 환경과의 상호작용 (Interaction)</td></tr>
<tr><td><strong>데이터 속성</strong></td><td>I.I.D. 가정 (독립적, 정적 분포)</td><td>Non-I.I.D. (시간적 종속성, 동적 분포)</td></tr>
<tr><td><strong>에이전트 역할</strong></td><td>수동적 관찰자 (Passive Spectator)</td><td>능동적 행위자 (Active Actor)</td></tr>
<tr><td><strong>학습 목표</strong></td><td>패턴 인식 (Pattern Recognition)</td><td>행동 결정 및 적응 (Decision Making &amp; Adaptation)</td></tr>
<tr><td><strong>지식의 기반</strong></td><td>기호 간의 통계적 관계</td><td>감각-운동 경험에의 접지 (Grounding)</td></tr>
<tr><td><strong>대표 모델</strong></td><td>BERT, GPT-4, Stable Diffusion</td><td>RT-2, PaLM-E, Humanoid Control Policies</td></tr>
</tbody></table>
<h2>4.  능동적 지각(Active Perception): “본다는 것은 행동하는 것이다”</h2>
<p>엠바디드 AI의 정의를 구성하는 또 다른 핵심 축은 ’능동적 지각(Active Perception)’이다. 1988년 루제나 바즈시(Ruzena Bajcsy) 교수가 주창한 이 개념은 인식을 수동적인 입력 처리가 아닌, 지능적인 제어 전략의 일환으로 재정의했다.23</p>
<h3>4.1  수동적 비전에서 능동적 비전으로</h3>
<p>기존 컴퓨터 비전(Computer Vision)은 주어진 정지 영상에서 정보를 추출하는 데 집중했다. 그러나 엠바디드 에이전트에게 시각은 ’탐색’의 도구다.</p>
<ul>
<li><strong>가려짐(Occlusion) 해결:</strong> 로봇은 컵의 손잡이가 보이지 않을 때, 딥러닝 모델로 보이지 않는 부분을 상상하는 것에 그치지 않고, 직접 몸을 움직여 컵의 뒷면을 확인한다.25</li>
<li><strong>불확실성 감소:</strong> 에이전트는 현재의 정보가 부족하다고 판단되면, 정보 이득(Information Gain)을 최대화할 수 있는 ’다음 최적 시점(Next Best View, NBV)’을 계산하고 카메라를 이동시킨다.27</li>
<li><strong>어포던스(Affordance) 인지:</strong> 제임스 깁슨(James Gibson)의 생태학적 심리학에서 유래한 개념으로, 엠바디드 AI는 물체를 “무엇(What)“으로 인식하는 것을 넘어 “어떻게(How) 상호작용할 수 있는가“로 인식한다. 의자는 ‘앉을 수 있는 것’, 컵은 ’잡을 수 있는 것’으로 지각되며, 이는 신체 구조에 따라 달라진다.22</li>
</ul>
<h3>4.2  인지-행동 루프 (Perception-Action Loop)</h3>
<p>따라서 엠바디드 AI에서 인식(Perception)과 행동(Action)은 분리된 모듈이 아니다. 인식은 행동을 유도하고, 행동은 새로운 인식을 낳는다. 이 순환 고리(Perception-Action Loop) 자체가 지능의 작동 방식이다. 최근 연구들은 이를 ‘호기심(Curiosity)’ 기반의 강화학습(RL)으로 구현하여, 에이전트가 외부 보상이 없더라도 새로운 상태를 경험하기 위해 스스로 탐색하도록 유도한다.27 이는 바즈시가 말한 “왜(Why), 어떻게(How), 무엇을(What) 볼 것인가를 스스로 결정하는 시스템“으로서의 엠바디드 AI 정의를 기술적으로 실현한 것이다.</p>
<h2>5.  엠바디드 AI의 범위 논쟁: 가상 에이전트 vs. 물리 로봇</h2>
<p>“지능은 환경과의 상호작용을 통해 발현된다“는 정의에 동의하더라도, ’환경’과 ’신체’의 범위를 어디까지로 설정할 것인가는 학계의 뜨거운 논쟁 주제다. 특히 3D 시뮬레이션 속의 **가상 에이전트(Virtual Agent)**를 진정한 엠바디드 AI로 볼 수 있는가에 대한 논의는 이 분야의 정체성을 규정하는 중요한 이슈다.</p>
<h3>5.1  물리 로봇 (Physical Robots): 진정한 엠바디먼트 (True Embodiment)</h3>
<p>전통적인 로봇 공학자들과 일부 인지과학자들은 물리적 실체(Physical Body)를 가진 로봇만이 진정한 엠바디드 AI라고 주장한다.30</p>
<ol>
<li><strong>현실의 복잡성(Real-World Complexity):</strong> 현실 세계는 시뮬레이션으로 완벽히 복제할 수 없는 무한한 변수(노이즈, 마찰력, 조명 변화, 통신 지연 등)를 가진다. 로봇 공학자들은 알고리즘이 ‘깨끗한’ 시뮬레이션이 아니라, ‘지저분한’ 현실의 저항을 극복하는 과정에서 진정한 지능이 나온다고 믿는다.</li>
<li><strong>물리적 실재감(Physical Presence):</strong> 인간-로봇 상호작용(HRI) 연구에 따르면, 사람들은 화면 속의 가상 에이전트보다 물리적으로 공존하는 로봇에게 더 높은 신뢰감과 사회적 반응을 보인다.30 이는 엠바디드 지능의 사회적 측면이 물리적 신체에 의존함을 시사한다.</li>
<li><strong>위험 관리(Risk Management):</strong> 물리 로봇의 행동은 돌이킬 수 없는 결과(기물 파손, 상해)를 초래한다. 이러한 실질적 위험(Real Risk)을 인지하고 안전하게 행동하는 능력은 시뮬레이션만으로는 학습하기 어렵다.32</li>
</ol>
<h3>5.2  가상 에이전트 (Virtual Agents): 확장된 엠바디먼트 (Extended Embodiment)</h3>
<p>반면, Facebook AI Research(FAIR), Allen Institute for AI(AI2) 등 현대 AI 연구의 주류는 가상 에이전트를 엠바디드 AI의 핵심 범주로 포함시킨다. 이들은 엠바디드 AI를 **“가상 로봇을 위한 AI(AI for virtual robots)”**로 정의하기도 한다.21</p>
<ol>
<li><strong>소프트 엠바디먼트 (Soft Embodiment):</strong> 가상 에이전트라도 물리 엔진(Physics Engine)이 적용된 3D 환경 내에서 충돌, 중력, 마찰을 경험하며 상호작용한다면, 이는 엠바디드 지능의 요건을 충족한다. 에이전트는 ’God-view(전지적 시점)’가 아닌 ’Egocentric view(1인칭 시점)’로 세상을 보며, 자신의 행동으로 환경 상태를 바꾼다.</li>
<li><strong>데이터 기근 해결 (Data Scarcity):</strong> 물리 로봇은 학습 속도가 느리고(Real-time), 데이터 수집 비용이 매우 높다. 반면 시뮬레이터는 수천 배 빠른 속도로 병렬 학습이 가능하며, 수백만 번의 시행착오를 통해 강화학습(RL) 모델을 훈련시킬 수 있다.29</li>
<li><strong>안전한 학습:</strong> 파손 위험 없이 위험한 시나리오(예: 화재 현장, 드론 충돌)를 무한히 반복 학습할 수 있다.</li>
</ol>
<h3>5.3  시뮬레이터 플랫폼의 진화</h3>
<p>가상 에이전트 연구를 가능케 한 것은 고성능 엠바디드 시뮬레이터의 발전이다. 이들은 단순한 게임 엔진을 넘어, 물리적 정확도와 사실적인 렌더링을 제공하며 엠바디드 AI 연구의 표준 플랫폼으로 자리 잡았다.</p>
<table><thead><tr><th><strong>시뮬레이터</strong></th><th><strong>주요 특징 및 연구 초점</strong></th><th><strong>관련 연구</strong></th></tr></thead><tbody>
<tr><td><strong>Habitat (Meta)</strong></td><td>초고속 렌더링(10,000+ FPS), 대규모 실내 공간 내비게이션(Navigation) 특화. 사실적인 3D 스캔 데이터(HM3D) 활용.</td><td>29</td></tr>
<tr><td><strong>AI2-THOR (Allen Inst.)</strong></td><td>상호작용 가능한 객체(Interactivity) 중심. 냉장고를 열거나 물을 트는 등의 물리적 조작(Manipulation) 및 상식 추론 연구에 적합.</td><td>37</td></tr>
<tr><td><strong>OmniGibson (Stanford)</strong></td><td>물리적 시뮬레이션의 정확도 강조. 로봇 팔 제어 및 재질(Material) 특성 반영. Real-to-Sim 전이 연구에 활용.</td><td>41</td></tr>
<tr><td><strong>ThreeDWorld (TDW)</strong></td><td>고충실도 물리 엔진. 유체, 연성체(Soft body) 시뮬레이션 및 소리(Audio) 시뮬레이션 지원. 다중 모달리티 연구.</td><td>43</td></tr>
</tbody></table>
<h3>5.4  통합적 관점: Sim-to-Real과 디지털 트윈</h3>
<p>현대 엠바디드 AI의 정의는 물리와 가상의 이분법을 넘어, **‘Sim-to-Real(가상에서 현실로의 전이)’**이라는 통합적 프레임워크로 나아가고 있다. 가상 에이전트는 물리 로봇의 ‘정신적 리허설’ 공간으로 기능한다.</p>
<ul>
<li><strong>Real-to-Sim-to-Real:</strong> 최근 Gaussian Splatting이나 NeRF(Neural Radiance Fields)와 같은 기술은 스마트폰으로 촬영한 현실 공간을 즉시 고해상도 3D 시뮬레이션으로 변환한다(EmbodiedSplat 44). 로봇은 이 ‘디지털 트윈’ 공간에서 안전하게 학습한 후, 학습된 정책(Policy)을 그대로 현실 공간의 물리 로봇에 이식한다.</li>
<li><strong>하이브리드 정의:</strong> 따라서 1.1.3 섹션에서의 정의는 다음과 같이 정리될 수 있다. “엠바디드 AI는 물리적 혹은 물리적으로 시뮬레이션된 신체를 가지고 환경과 상호작용하는 시스템이며, 가상 에이전트는 물리적 현실로의 전이 가능성을 전제로 할 때 그 범주에 포함된다.”</li>
</ul>
<h2>6.  최신 기술 동향과 정의의 확장: 엠바디드 파운데이션 모델</h2>
<p>2023년 이후, 대형언어모델(LLM)과 비전-언어 모델(VLM)의 등장은 엠바디드 AI의 정의를 ’동작 제어(Motor Control)’에서 ’인지적 추론(Cognitive Reasoning)’의 영역으로 확장시키고 있다.</p>
<h3>6.1  PaLM-E와 RT-2: 인터넷 지식과 물리적 행동의 결합</h3>
<p>과거의 엠바디드 AI는 “문을 여는 방법“을 수천 번의 시행착오(RL)로 배웠다면, 최신 엠바디드 파운데이션 모델은 인터넷상의 방대한 지식을 통해 “문손잡이는 돌리는 것“이라는 상식을 이미 알고 시작한다.</p>
<ul>
<li><strong>PaLM-E (Embodied Multimodal Language Model):</strong> 구글이 발표한 PaLM-E는 로봇의 센서 데이터(이미지, 관절 상태 등)를 언어 모델의 입력 토큰으로 직접 주입한다.47 즉, 로봇의 시각 정보가 “사과가 식탁 위에 있다“는 텍스트와 동등한 수준의 정보로 처리되어, “사과를 가져와“라는 명령에 대해 물리적 맥락을 고려한 계획을 수립할 수 있다.</li>
<li><strong>RT-2 (Robotic Transformer 2):</strong> 비전-언어 모델(VLM)을 로봇의 행동 데이터와 함께 미세 조정(Fine-tuning)한 모델이다.50 RT-2는 이미지와 텍스트를 입력받아 로봇 팔의 좌표 변화(<span class="math math-inline">\Delta pos_x, \Delta pos_y</span>)를 텍스트 토큰처럼 출력한다. 놀라운 점은 학습 데이터에 없던 물체(예: “연예인 사진이 붙은 병”)를 보고도 인터넷 지식을 활용해 올바르게 반응하는 ’창발적 능력(Emergent Capability)’을 보인다는 것이다.</li>
</ul>
<p>이러한 모델들은 엠바디드 AI가 단순한 ’반사 신경’을 넘어, <strong>일반 상식(Common Sense)과 의미론적 추론 능력을 갖춘 물리적 에이전트</strong>로 진화하고 있음을 보여준다.29</p>
<h3>6.2  엠바디드 월드 모델 (Embodied World Models)</h3>
<p>최근 연구는 에이전트가 단순히 입력에 반응하는 것을 넘어, 자신의 행동이 미래를 어떻게 바꿀지 예측하는 ’내부 모델(Internal World Model)’을 구축하는 방향으로 나아가고 있다.53 이는 로드니 브룩스가 비판했던 ’재현(Representation)’의 부활이라기보다는, 신경과학의 ‘예측적 부호화(Predictive Coding)’ 이론에 가깝다. 에이전트는 물리적 상호작용을 통해 세상의 인과관계를 학습하고, 이를 바탕으로 시뮬레이션(상상)을 통해 최적의 행동을 계획한다.</p>
<h2>7.  결론</h2>
<p>본 보고서는 엠바디드 AI의 정의와 범위를 다각도로 분석하였다. 서적의 “1.1.3 엠바디드 AI의 정의” 섹션에 포함되어야 할 핵심 결론은 다음과 같다.</p>
<ol>
<li><strong>정의의 본질:</strong> 엠바디드 AI는 지능을 뇌 속에 갇힌 연산이 아니라, **신체(Body)-환경(Environment)-상호작용(Interaction)**의 삼각 구도 속에서 발현되는 동적인 과정으로 정의한다. 지능은 세상의 복잡성을 신체적 상호작용을 통해 단순화하고, 의미를 구성해 나가는 능력이다.</li>
<li><strong>데이터 관점의 차별성:</strong> 정적이고 독립적인(I.I.D.) 데이터를 학습하는 인터넷 AI와 달리, 엠바디드 AI는 자신의 행동이 미래의 데이터를 결정하는 인과적 루프 속에서 비정상적(Non-Stationary) 데이터 분포에 적응해야 한다.</li>
<li><strong>범위의 포괄성:</strong> 엠바디드 AI의 범위는 물리적 로봇을 핵심으로 하되, 고충실도 시뮬레이션 속의 가상 에이전트를 포함한다. 특히 최근의 Sim-to-Real 기술과 디지털 트윈의 발전은 가상과 현실의 경계를 허물며 두 영역을 통합하고 있다.</li>
<li><strong>미래 전망:</strong> 파운데이션 모델과의 결합을 통해 엠바디드 AI는 감각-운동 지능을 넘어, 인간 수준의 상식과 언어 능력을 갖춘 **범용 엠바디드 에이전트(General-Purpose Embodied Agent)**로 진화하고 있다.</li>
</ol>
<p>이러한 정의는 AI 연구가 단순히 ’더 똑똑한 챗봇’을 만드는 것을 넘어, 인간과 함께 물리적 공간을 공유하고 협력할 수 있는 존재를 만드는 단계로 진입했음을 시사한다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning - arXiv, https://arxiv.org/html/2508.10399v1</li>
<li>Embodied Cognition - Stanford Encyclopedia of Philosophy, https://plato.stanford.edu/entries/embodied-cognition/</li>
<li>Intelligence without Representation: A Historical Perspective - MDPI, https://www.mdpi.com/2079-8954/8/3/31</li>
<li>A Call for Embodied AI - arXiv, https://arxiv.org/html/2402.03824v3</li>
<li>Embodied Intelligence: The Key to Unblocking Generalized Artificial Intelligence - arXiv, https://arxiv.org/html/2505.06897v1</li>
<li>Active Vision for Embodied Agents Using Reinforcement Learning - ediss.sub.hamburg, https://ediss.sub.uni-hamburg.de/bitstream/ediss/10952/1/thesis-MengdiLi-20240605-final-signed.pdf</li>
<li>Embodied cognition - Wikipedia, https://en.wikipedia.org/wiki/Embodied_cognition</li>
<li>Intelligence without representation* - People, https://people.csail.mit.edu/brooks/papers/representation.pdf</li>
<li>Rodney Brooks - Wikipedia, https://en.wikipedia.org/wiki/Rodney_Brooks</li>
<li>Is there a future for AI without representation? - arXiv, https://arxiv.org/pdf/2503.18955</li>
<li>A Call for Embodied AI - arXiv, https://arxiv.org/html/2402.03824v4</li>
<li>The development of embodied cognition: six lessons from babies - PubMed - NIH, https://pubmed.ncbi.nlm.nih.gov/15811218/</li>
<li>The Development of Embodied Cognition: Six Lessons from Babies - ResearchGate, https://www.researchgate.net/publication/7925616_The_Development_of_Embodied_Cognition_Six_Lessons_from_Babies</li>
<li>Comparison: Internet AI vs. embodied AI | Download Scientific Diagram - ResearchGate, https://www.researchgate.net/figure/Comparison-Internet-AI-vs-embodied-AI_tbl1_394372026</li>
<li>AI That Moves, Adapts, and Learns: The Future of Embodied Intelligence | Columbia AI, https://ai.columbia.edu/news/ai-moves-adapts-and-learns-future-embodied-intelligence</li>
<li>Embodied AI: Intelligence in the Physical World - MG Super Labs, https://mgsl.in/blogs/news/embodied-ai-intelligence-in-the-physical-world</li>
<li>Understanding identical and independent distribution (IID) in Machine Learning | by Yash Khasbage | Data Science at Microsoft | Medium, https://medium.com/data-science-at-microsoft/understanding-identical-and-independent-distribution-iid-in-machine-learning-dc2e98b6609d</li>
<li>Non-IID Thinking, Informatics, and Learning - IEEE Xplore, https://ieeexplore.ieee.org/iel7/9670/9896783/09896785.pdf</li>
<li>Effective Non-IID Degree Estimation for Robust Federated Learning in Healthcare Datasets, https://pmc.ncbi.nlm.nih.gov/articles/PMC12290147/</li>
<li>A review on different techniques used to combat the non-iid and heterogeneous nature of data in FL - arXiv, https://arxiv.org/html/2401.00809v1</li>
<li>Overview of Embodied Artificial Intelligence | by Luis Bermudez | machinevision - Medium, https://medium.com/machinevision/overview-of-embodied-artificial-intelligence-b7f19d18022</li>
<li>Overview of Robot Perception - UT Austin Computer Science, https://www.cs.utexas.edu/~yukez/cs391r_fall2021/slides/lecture_robot_perception.pdf</li>
<li>Revisiting active perception - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC6954017/</li>
<li>(PDF) Revisiting Active Perception - ResearchGate, https://www.researchgate.net/publication/301842249_Revisiting_Active_Perception</li>
<li>Active perception - ResearchGate, https://www.researchgate.net/profile/Yiannis-Aloimonos/publication/228083826_Active_Perception/links/00b49528e6f94c813e000000/Active-Perception.pdf</li>
<li>ACTIVE PERCEPTION - University of Pennsylvania, https://repository.upenn.edu/bitstreams/14686d28-7a8a-4554-beee-991eec61cbdd/download</li>
<li>Embodied AI Agents: Modeling the World - arXiv, https://arxiv.org/html/2506.22355v1</li>
<li>Awesome Embodied Navigation: Concept, Paradigm and State-of-the-arts - GitHub, https://github.com/Franky-X/Awesome-Embodied-Navigation</li>
<li>Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI - arXiv, https://arxiv.org/html/2407.06886v1</li>
<li>Physical vs. Virtual Agent Embodiment and Effects on Social Interaction - DiVA portal, https://www.diva-portal.org/smash/get/diva2:1051747/FULLTEXT02.pdf</li>
<li>The Dark Side of Embodiment Teaming Up With Robots VS Disembodied Agents, https://roboticsproceedings.org/rss16/p010.pdf</li>
<li>Embodied AI: Emerging Risks and Opportunities for Policy Action - arXiv, https://arxiv.org/html/2509.00117</li>
<li>[1904.01201] Habitat: A Platform for Embodied AI Research - arXiv, https://arxiv.org/abs/1904.01201</li>
<li>[Quick Review] Habitat: A Platform for Embodied AI Research - Liner, https://liner.com/review/habitat-platform-for-embodied-ai-research</li>
<li>Habitat: A Platform for Embodied AI Research - Erik Wijmans, https://wijmans.xyz/publication/ai-habitat/</li>
<li>Habitat: A Platform for Embodied AI Research - CVF Open Access, https://openaccess.thecvf.com/content_ICCV_2019/papers/Savva_Habitat_A_Platform_for_Embodied_AI_Research_ICCV_2019_paper.pdf</li>
<li>1월 1, 2026에 액세스, [https://prior.allenai.org/projects/ai2thor#:<sub>:text=The%20House%20Of%20inteRactions%20(AI2,with%20objects%20to%20perform%20tasks.](https://prior.allenai.org/projects/ai2thor#:</sub>:text=The House Of inteRactions (AI2, [https://prior.allenai.org/projects/ai2thor#:<sub>:text=The%20House%20Of%20inteRactions%20(AI2,with%20objects%20to%20perform%20tasks.](https://prior.allenai.org/projects/ai2thor#:</sub>:text=The House Of inteRactions (AI2,with objects to perform tasks.)</li>
<li>AI2-THOR: An Interactive 3D Environment for Visual AI, https://prior.allenai.org/projects/ai2thor</li>
<li>allenai/ai2thor: An open-source platform for Visual AI. - GitHub, https://github.com/allenai/ai2thor</li>
<li>AI2-THOR: An Interactive 3D Environment for Visual AI - arXiv, https://arxiv.org/pdf/1712.05474</li>
<li>[1808.10654] Gibson Env: Real-World Perception for Embodied Agents - arXiv, https://arxiv.org/abs/1808.10654</li>
<li>AI2-THOR, https://ai2thor.allenai.org/</li>
<li>Digital twins to embodied artificial intelligence: review and perspective - OAE Publishing Inc., https://www.oaepublish.com/articles/ir.2025.11</li>
<li>[2509.17430] EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device - arXiv, https://arxiv.org/abs/2509.17430</li>
<li>EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device - arXiv, https://arxiv.org/html/2509.17430v1</li>
<li>EmbodiedSplat - Gunjan Chhablani, https://gchhablani.github.io/embodied-splat/</li>
<li>PaLM-E: An embodied multimodal language model - Google Research, https://research.google/blog/palm-e-an-embodied-multimodal-language-model/</li>
<li>arXiv:2303.03378v1 [cs.LG] 6 Mar 2023, https://arxiv.org/pdf/2303.03378</li>
<li>PaLM-E: An Embodied Multimodal Language Model, https://proceedings.mlr.press/v202/driess23a.html</li>
<li>The compounding impact of Artificial Intelligence and Robotics on the future of learning: Focus on RobotGPTs | ITCILO, https://www.itcilo.org/compounding-impact-artificial-intelligence-and-robotics-future-learning-focus-robotgpts</li>
<li>RT-2: New model translates vision and language into action - Google DeepMind, https://deepmind.google/blog/rt-2-new-model-translates-vision-and-language-into-action/</li>
<li>PaLM-E: An Embodied Multimodal Language Model, https://faculty.cc.gatech.edu/~zk15/teaching/AY2024_cs8803vlm_fall/slides/L21_PaLM-E.pdf</li>
<li>A Comprehensive Survey on World Models for Embodied AI - arXiv, https://arxiv.org/html/2510.16732v1</li>
<li>Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI, https://arxiv.org/html/2407.06886v8</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>