<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:1.1 인터넷 AI에서 엠바디드 AI로 (From Internet AI to Embodied AI)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>1.1 인터넷 AI에서 엠바디드 AI로 (From Internet AI to Embodied AI)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 1. Embodied AI의 태동: 뇌를 가진 신체</a> / <a href="index.html">1.1 인터넷 AI에서 엠바디드 AI로 (From Internet AI to Embodied AI)</a> / <span>1.1 인터넷 AI에서 엠바디드 AI로 (From Internet AI to Embodied AI)</span></nav>
                </div>
            </header>
            <article>
                <h1>1.1 인터넷 AI에서 엠바디드 AI로 (From Internet AI to Embodied AI)</h1>
<p>인류가 인공지능(Artificial Intelligence, AI)을 탐구해 온 역사는 지능의 본질이 무엇인가에 대한 철학적 논쟁과 궤를 같이한다. 초기 AI 연구가 논리 기호의 조작을 통한 순수한 정신 활동의 모사에 집중했다면, 최근의 흐름은 지능이 물리적 신체를 통해 환경과 상호작용하는 과정에서 발현된다는 ’신체성(Embodiment)’의 재발견으로 요약될 수 있다. 우리는 지금 ’인터넷 AI(Internet AI)’라는 거대한 디지털 정보 처리 시스템에서, 물리 세계의 불확실성 속에서 생존하고 행동하는 ’엠바디드 AI(Embodied AI, 체화된 인공지능)’로의 거대한 패러다임 전환(Paradigm Shift)을 목격하고 있다. 이 절에서는 지난 10년간 AI의 눈부신 발전을 이끌어온 인터넷 AI의 성과와 본질적인 한계를 분석하고, 왜 로봇 지능이 필연적으로 엠바디드 AI로 나아가야 하는지, 그리고 이 두 가지 상이한 지능의 형태가 최신 파운데이션 모델(Foundation Model) 기술을 통해 어떻게 융합되고 있는지를 기술적, 수학적, 철학적 관점에서 심도 있게 고찰한다.</p>
<h2>1.  관찰자 패러다임: 인터넷 AI의 본질과 정적 데이터의 세계</h2>
<h3>1.1 디지털 공간의 ‘통 속의 뇌’ (The Brain in a Vat)</h3>
<p>2010년대 이후 AI의 폭발적인 성장은 인터넷 AI가 주도했다. 1 컴퓨터 비전(Computer Vision) 분야의 <em>ImageNet</em> 혁명부터, 자연어 처리(NLP) 분야의 <em>Transformer</em> 아키텍처, 그리고 최근의 거대 언어 모델(Large Language Model, LLM)에 이르기까지, 이들은 모두 인터넷이라는 가상 공간에 축적된 방대한 데이터를 기반으로 한다. 인터넷 AI의 핵심은 ’정적 데이터(Static Data)’에 있다. 웹 크롤링을 통해 수집된 수십억 장의 이미지와 텍스트, 비디오는 과거의 기록이며, AI는 이 고정된 데이터 분포(Distribution) 안에서 통계적 규칙성을 찾아내고 패턴을 인식하는 데 최적화되어 있다.</p>
<p>이러한 접근 방식은 철학자 힐러리 퍼트남(Hilary Putnam)이 제안한 사고 실험인 ’통 속의 뇌(Brain in a Vat)’를 연상시킨다. 인터넷 AI는 외부 세계와 물리적으로 단절된 채, 입력되는 전기 신호(데이터)만을 처리하는 고립된 지능이다. 예를 들어, <em>GPT-4</em>와 같은 LLM은 ’사과’라는 단어가 ‘빨갛다’, ‘맛있다’, ’과일이다’라는 다른 기호들과 통계적으로 어떻게 연결되는지는 완벽하게 계산해 낸다. 그러나 이 모델은 사과를 한 입 베어 물었을 때의 아삭한 식감, 손에 쥐었을 때 느껴지는 매끄러운 감촉, 혹은 사과를 놓쳤을 때 바닥으로 떨어지는 중력 가속도의 물리적 현상을 결코 ’경험’하지 못한다. 3</p>
<p>이것이 인터넷 AI가 가진 태생적 한계인 ’비접지성(Ungroundedness)’이다. 인터넷 AI에게 세계는 기호(Symbol)들의 나열일 뿐이며, 그 기호가 지시하는 물리적 실체(Physical Entity)와는 연결되어 있지 않다. 따라서 인터넷 AI는 세계를 이해하는 것이 아니라, 세계에 대한 텍스트적 ’설명’을 학습한 것에 불과하다. 이는 정보의 검색, 요약, 번역과 같은 작업에서는 탁월한 성능을 발휘하지만, 물리적 세계에 개입하여 변화를 만들어내야 하는 로봇에게는 치명적인 결핍이 된다.</p>
<h3>1.2 수동적 지각(Passive Perception)과 I.I.D. 가정의 함정</h3>
<p>인터넷 AI의 또 다른 결정적인 특징은 ’수동적 지각(Passive Perception)’이다. 4 인터넷 AI 모델은 데이터가 자신에게 주어지기를 기다린다. 훈련 데이터셋은 이미 인간에 의해 수집되고, 정제되고, 라벨링 된 상태로 제공된다. 모델은 이 데이터를 수동적으로 섭취하며 신경망의 가중치(Weight)를 업데이트한다. 이 과정에서 에이전트는 데이터를 선택하거나 획득하는 과정에 개입하지 못한다.</p>
<p>이러한 수동성은 통계학의 강력하지만 위험한 가정인 <strong>I.I.D. (Independent and Identically Distributed)</strong> 가정에 의존한다. 7 즉, 훈련 데이터와 테스트 데이터가 동일한 확률 분포에서 독립적으로 추출되었다고 가정하는 것이다.<br />
<span class="math math-display">
P_{train}(x, y) \approx P_{test}(x, y)
</span><br />
예를 들어, <em>COCO</em> 데이터셋으로 훈련된 객체 인식 모델은 인터넷에 올라온 ‘잘 찍힌(Canonical Viewpoint)’ 고양이 사진에는 뛰어난 성능을 보이지만, 로봇이 실제 환경에서 마주치는 흐려지거나, 가려지거나(Occlusion), 기괴한 조명 아래의 고양이(Out-of-Distribution)를 인식하는 데는 처참하게 실패한다. 인터넷 데이터셋은 인간 사진가가 의도를 가지고 피사체를 중심으로 촬영한 ‘편향된’ 데이터이기 때문이다.</p>
<p>현실 세계는 I.I.D.가 아니다. 로봇이 고개를 돌리는 순간 시각 정보의 분포는 급격하게 변하며, 조명은 시간에 따라 달라지고, 물체의 위치는 로봇의 작용에 의해 끊임없이 이동한다. 인터넷 AI가 전제로 하는 ’정적이고 고정된 세계’는 현실에 존재하지 않는다. 이러한 데이터 분포의 불일치(Distribution Shift)는 인터넷 AI를 물리 세계로 가져올 때 발생하는 ’현실 격차(Reality Gap)’의 근본적인 원인이 된다. 10 인터넷 AI는 자신이 훈련된 분포 밖의 데이터(Out-of-Distribution, OOD)를 마주했을 때, 자신의 무지를 인지하지 못하고 자신 있게 틀린 답을 내놓는 환각(Hallucination) 현상을 보인다. 13 이는 로봇 제어에 있어 심각한 안전 문제를 야기할 수 있다.</p>
<h3>1.3 심볼 그라운딩 문제 (The Symbol Grounding Problem)</h3>
<p>인터넷 AI의 한계를 논할 때 빠질 수 없는 것이 인지과학자 스티븐 하르나드(Stevan Harnad)가 1990년에 제기한 ’심볼 그라운딩 문제(The Symbol Grounding Problem)’이다. 14 하르나드는 기호(Symbol)가 어떻게 의미(Meaning)를 갖게 되는지를 묻는다.</p>
<p>존 설(John Searle)의 ‘중국어 방(Chinese Room)’ 사고 실험을 떠올려보자. 중국어를 전혀 모르는 사람이 방 안에 갇혀서, 중국어 질문이 들어오면 매뉴얼에 적힌 규칙대로 중국어 답변을 내보낸다. 방 밖의 사람은 안에 있는 사람이 중국어를 이해한다고 생각하겠지만, 그는 단지 기호의 형태(Syntax)를 조작했을 뿐 의미(Semantics)를 파악한 것은 아니다. 현재의 LLM은 거대하고 정교한 중국어 방과 같다. 그들은 수조 개의 파라미터로 기호 간의 통계적 관계를 계산하지만, 그 기호가 가리키는 물리적 대상과는 단절되어 있다.</p>
<p>인터넷 AI에서는 ’사과’라는 기호가 ’Apple’이라는 또 다른 기호로, 다시 픽셀들의 집합이라는 기호로 순환 참조될 뿐, 물리적 실체에 닻을 내리지(Grounding) 못한다. 이를 **기호적 순환(Symbolic Circularity)**이라 한다. 반면, 엠바디드 AI는 이 문제를 정면으로 돌파한다. 로봇이 사과를 보고(Vision), 잡고(Touch), 들어 올리는(Proprioception) 감각 운동(Sensorimotor) 경험을 통해 ’사과’라는 기호는 비로소 물리적 실체와 결합하여 ’의미’를 획득한다. 17 이것이 바로 엠바디드 AI가 진정한 지능으로 나아가는 필수 관문이다. 의미는 사전 정의된 딕셔너리에 있는 것이 아니라, 에이전트와 환경의 상호작용 속에 존재하기 때문이다.</p>
<h2>2.  행위자 패러다임: 엠바디드 AI와 능동적 상호작용</h2>
<h3>2.1 신체를 가진 지능 (Intelligence with a Body)</h3>
<p>엠바디드 AI(Embodied AI)는 지능이 신체를 통해 환경과 상호작용하는 과정에서 창발한다고 본다. 1 이는 “지능은 신체적 활동을 위해 진화했다“는 ’신체성 가설(The Embodiment Hypothesis)’에 뿌리를 두고 있다. 엠바디드 에이전트(로봇, 자율주행차, 가상 인간 등)는 단순히 데이터를 처리하는 서버가 아니라, 환경을 감지하고(Perceive), 행동하고(Act), 그 행동의 결과로 변화된 환경을 다시 감지하는 폐루프(Closed-loop) 안에 존재한다.</p>
<p>이 패러다임에서 AI는 ’관찰자(Spectator)’에서 ’행위자(Actor)’로 변모한다. 20 관찰자로서의 AI가 “이 이미지는 무엇인가?”(Classification)를 묻는다면, 행위자로서의 AI는 “이 상황에서 나는 무엇을 할 수 있는가?”(Affordance)를 묻는다. 엠바디드 AI는 자신의 센서를 능동적으로 제어하여 불확실성을 줄이고 정보를 획득하는 **능동적 지각(Active Perception)**을 수행한다. 4 예를 들어, 컵의 손잡이가 보이지 않을 때 인터넷 AI는 “손잡이 없음“이라고 판단하거나 보이지 않는 부분을 환각으로 채워 넣을 수 있지만, 엠바디드 AI는 몸을 기울이거나 팔을 뻗어 컵을 돌려보면서 손잡이의 유무를 물리적으로 검증한다. 이러한 능동성은 지능 시스템이 불완전한 정보(Partial Observability)와 불확실한 환경을 극복하는 핵심 기제이다.</p>
<h3>2.2 시뮬레이터와 물리적 상호작용: 동적 데이터의 원천</h3>
<p>인터넷 AI가 정적인 데이터셋(<em>ImageNet</em>, <em>Wikipedia</em>)을 먹고 자랐다면, 엠바디드 AI는 시뮬레이터와 물리적 상호작용을 통해 생성되는 동적 데이터(Dynamic Data)를 먹고 자란다. 1 <em>Habitat</em>, <em>Isaac Gym</em>, <em>MuJoCo</em>, <em>ThreeDWorld</em>와 같은 고성능 물리 시뮬레이터는 엠바디드 AI를 위한 새로운 ’학교’이자 데이터 공장이다.</p>
<p>시뮬레이션 환경에서는 에이전트가 초당 수천 번의 시행착오를 겪으며 물리 법칙(중력, 마찰, 충돌)을 체득할 수 있다. 여기서 생성되는 데이터는 에이전트의 상태(State, <span class="math math-inline">s</span>), 행동(Action, <span class="math math-inline">a</span>), 그리고 보상(Reward, <span class="math math-inline">r</span>)이 결합된 시계열 궤적(Trajectory, <span class="math math-inline">\tau</span>) 형태를 띤다.<br />
<span class="math math-display">
\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots, s_T, a_T, r_T)
</span><br />
이러한 데이터는 인터넷 데이터와 달리 강력한 인과관계(Causality)를 내포한다. “컵을 밀었더니(Action) 떨어져서 깨졌다(State Change)“는 인과적 사슬은 정적인 이미지에서는 결코 배울 수 없는 정보이다. 12 엠바디드 AI는 이 인과관계를 학습함으로써 단순한 패턴 매칭을 넘어 미래를 예측하고 계획(Planning)하는 능력을 갖추게 된다. 이는 “데이터는 행동의 부산물“이라는 관점으로의 전환을 의미하며, 데이터의 양보다 데이터의 질과 다양성(Diversity)이 학습에 더 큰 영향을 미친다. 24</p>
<h3>2.3 비정상성(Non-stationarity)과 마르코프 결정 과정(MDP)</h3>
<p>수학적으로 엠바디드 AI 문제는 인터넷 AI의 지도 학습(Supervised Learning)과는 본질적으로 다른 프레임워크를 요구한다. 에이전트의 행동은 다음 상태의 분포를 변화시킨다. 즉, 데이터 분포가 시간에 따라, 그리고 에이전트의 정책(Policy)에 따라 끊임없이 변하는 **비정상성(Non-stationarity)**을 가진다. 7</p>
<p>인터넷 AI의 학습이 고정된 데이터셋 <span class="math math-inline">\mathcal{D}</span>에 대해 손실 함수 <span class="math math-inline">\mathcal{L}</span>을 최소화하는 최적화 문제라면:<br />
<span class="math math-display">
\theta^* = \arg\min_\theta \mathbb{E}_{(x,y) \sim \mathcal{D}} [\mathcal{L}(f_\theta(x), y)]
</span><br />
엠바디드 AI는 주로 **마르코프 결정 과정(Markov Decision Process, MDP)**이나 **부분 관찰 마르코프 결정 과정(POMDP)**으로 정식화된다. 여기서 목표는 누적 보상의 기댓값을 최대화하는 최적의 정책 <span class="math math-inline">\pi^*(a \vert s)</span>를 찾는 것이다.<br />
<span class="math math-display">
\pi^* = \arg\max_\pi \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \right]
</span><br />
여기서 가장 큰 차이점은 **분포의 의존성(Dependency of Distribution)**이다. 엠바디드 에이전트가 학습을 통해 정책 <span class="math math-inline">\pi</span>를 업데이트하면, 그 에이전트가 방문하는 상태의 분포 <span class="math math-inline">P^\pi(s)</span> 자체가 변한다. 이를 <strong>Covariate Shift</strong> 또는 <strong>Distributional Shift</strong>라고 한다. 25 예를 들어, 자율주행차가 도로의 중앙을 잘 유지하도록 학습되면, 더 이상 도로 가장자리의 데이터를 수집하지 않게 된다. 나중에 차가 외란에 의해 실수로 가장자리에 갔을 때, 학습 데이터가 부족하여 복구하지 못하는 치명적인 문제가 발생할 수 있다. 따라서 엠바디드 AI는 이러한 비정상성을 다루기 위해 **DAgger (Dataset Aggregation)**와 같은 모방 학습 알고리즘이나, 탐험(Exploration)을 강조하는 **강화학습(Reinforcement Learning)**을 필수적으로 사용해야 한다. 26</p>
<h2>3.  인터넷 AI와 엠바디드 AI의 대전환 비교 분석</h2>
<p>인터넷 AI에서 엠바디드 AI로의 전환은 단순한 응용 분야의 확장이 아니라, 인공지능을 바라보는 관점과 방법론의 근본적인 변화이다. 이를 5가지 핵심 차원에서 비교 분석한다.</p>
<table><thead><tr><th><strong>비교 차원 (Dimension)</strong></th><th><strong>인터넷 AI (Internet AI)</strong></th><th><strong>엠바디드 AI (Embodied AI)</strong></th></tr></thead><tbody>
<tr><td><strong>존재 양식 (Mode of Being)</strong></td><td><strong>관찰자 (Spectator)</strong> 스크린 너머의 세계를 분석하고 설명함.</td><td><strong>행위자 (Actor)</strong> 물리 세계 안에서 상호작용하고 개입함.</td></tr>
<tr><td><strong>데이터 원천 (Data Source)</strong></td><td><strong>정적 큐레이션 (Static Curation)</strong> 인터넷에서 수집된 과거 데이터 (ImageNet, WebText).</td><td><strong>동적 상호작용 (Dynamic Interaction)</strong> 시뮬레이션 및 현실 세계에서의 실시간 경험.</td></tr>
<tr><td><strong>학습 가정 (Assumption)</strong></td><td><strong>I.I.D. 가정</strong> 훈련과 테스트 데이터 분포가 동일하고 독립적임.</td><td><strong>비정상성 (Non-stationarity)</strong> 에이전트의 행동이 미래 데이터 분포를 변화시킴.</td></tr>
<tr><td><strong>지각 방식 (Perception)</strong></td><td><strong>수동적 지각 (Passive Perception)</strong> 주어진 이미지를 분류하거나 해석함.</td><td><strong>능동적 지각 (Active Perception)</strong> 정보 획득을 위해 시점을 변경하거나 환경을 조작함.</td></tr>
<tr><td><strong>접지 (Grounding)</strong></td><td><strong>약한 접지 (Weak Grounding)</strong> 텍스트와 이미지 간의 연관성 학습.</td><td><strong>감각운동 접지 (Sensorimotor Grounding)</strong> 기호와 물리적 실체/행동의 결합.</td></tr>
<tr><td><strong>핵심 목표 (Objective)</strong></td><td><strong>패턴 인식 (Pattern Recognition)</strong> 정확도(Accuracy) 및 손실 최소화.</td><td><strong>행동 제어 (Action Control)</strong> 성공률(Success Rate) 및 누적 보상 최대화.</td></tr>
</tbody></table>
<h3>3.1  데이터의 경제학: 큐레이션에서 생성으로</h3>
<p>인터넷 AI 시대에는 “데이터가 곧 석유“였다. 더 많은 데이터를 긁어모으고 정제하는 것이 경쟁력이었다. 그러나 엠바디드 AI 시대에는 데이터가 단순히 주어지는 자원이 아니라, 에이전트가 행동을 통해 능동적으로 생성해야 하는 산물이다. 로봇이 환경과 상호작용할수록 데이터는 무한히 생성되지만, 이 데이터는 노이즈가 많고, 획득 비용이 비싸며(로봇 하드웨어 마모, 시간 소모, 전력 소비), 위험할 수 있다. 따라서 엠바디드 AI는 데이터를 무작정 많이 모으는 것보다, 현재의 지식 격차를 줄일 수 있는 **가치 있는 데이터(High-value data)**를 스스로 탐색하고 선별적으로 수집하는 능력(Curiosity-driven Exploration)이 중요하다. 24</p>
<h3>3.2  의미의 획득: 패턴에서 물리적 접지로</h3>
<p>인터넷 AI는 “사과는 빨갛다“는 문장과 빨간 사과 이미지를 매칭하며 의미를 학습한다. 이는 기호와 기호 사이의 관계일 뿐이다. 그러나 엠바디드 AI는 로봇 팔로 사과를 쥐었을 때의 압력 분포, 들어 올릴 때 필요한 토크(Torque)의 양, 떨어뜨렸을 때의 충격음 등을 통해 다중 감각적(Multimodal)이고 물리적인 의미를 획득한다. 18 이는 AI가 텍스트나 픽셀의 패턴을 넘어 제임스 깁슨(James Gibson)이 주창한 <strong>어포던스(Affordance)</strong>—환경이 행위자에게 제공하는 행동의 가능성—를 이해하게 함을 의미한다. “의자“는 단순히 “다리가 4개인 물체“가 아니라 “앉을 수 있는(Sittable) 대상“으로 인식된다.</p>
<h3>3.3  현실 격차(Reality Gap)와 Sim-to-Real</h3>
<p>인터넷 AI는 디지털 세계에서 태어나 디지털 세계에서 작동하므로 환경 불일치 문제가 적다. 반면, 엠바디드 AI는 시뮬레이터에서 학습하고 현실 세계에 배포되는 경우가 많다. 이때 시뮬레이션의 물리 법칙, 렌더링 품질, 센서 노이즈가 현실과 완벽히 일치하지 않아 발생하는 **현실 격차(Reality Gap)**는 엠바디드 AI의 가장 큰 기술적 장벽이다. 10 이를 극복하기 위해 물리 파라미터(마찰계수, 질량 등)를 다양하게 변화시켜 학습하는 도메인 무작위화(Domain Randomization), 현실 데이터를 이용해 시뮬레이터를 보정하는 시스템 식별(System Identification), 그리고 시각적 차이를 줄이는 도메인 적응(Domain Adaptation) 기술이 필수적으로 요구된다.</p>
<h2>4.  가교(Bridge): 파운데이션 모델이 로봇을 만났을 때</h2>
<p>2020년대 중반, 인터넷 AI의 정점인 <strong>파운데이션 모델(Foundation Models)</strong>—LLM과 비전-언어 모델(VLM)—이 엠바디드 AI와 결합하면서 역사적인 융합이 일어나고 있다. 19 이는 인터넷 AI가 쌓아 올린 방대한 ’상식(Common Sense)’과 ’언어 능력’을 엠바디드 AI의 ’신체(Body)’와 연결하려는 시도이다. 이 융합은 기존 로봇 공학의 고질적인 문제였던 일반화(Generalization)의 한계를 극복할 열쇠로 여겨진다.</p>
<p>과거의 로봇은 “물 컵을 가져와“라는 명령을 수행하기 위해 “물 컵“이 무엇인지(인식), “가져오다“가 어떤 동작 시퀀스인지(계획 및 제어)를 일일이 프로그래밍해야 했다. 그러나 인터넷 AI(LLM)는 이미 “물 컵은 주로 부엌에 있다”, “물을 쏟지 않으려면 컵을 수직으로 들어야 한다”, “유리는 깨지기 쉬우므로 조심해야 한다“는 상식을 가지고 있다. 이 상식을 로봇의 제어 루프에 주입함으로써, 우리는 훈련 데이터에 없는 새로운 명령과 상황에도 대응할 수 있는 **일반 범용 로봇(Generalist Robot)**의 가능성을 보게 되었다.</p>
<h3>4.1 상징적인 전환점: 최신 SOTA 연구 사례</h3>
<p>이 융합을 이끄는 기념비적인 연구들은 인터넷 AI의 추론 능력을 엠바디드 AI의 행동 공간으로 변환하는 다양한 방법론을 제시한다.</p>
<h4>4.1.1  <em>SayCan</em>: 언어 모델의 꿈을 현실에 접지하다</h4>
<p>Google Research의 <em>SayCan</em> 30은 “Do As I Can, Not As I Say“라는 논문 제목처럼, LLM의 계획 능력과 로봇의 수행 능력(Affordance)을 결합한 초기 대표작이다.</p>
<ul>
<li><strong>작동 원리</strong>: 사용자가 “커피를 쏟았어, 도와줘“라고 말하면, LLM은 “스펀지를 찾는다”, “닦는다”, “진공청소기를 가져온다” 등의 후보 행동을 생성한다. 이때 로봇의 가치 함수(Value Function, <span class="math math-inline">V(s, a)</span>)가 각 행동이 현재 상태에서 물리적으로 실행 가능한지(Can)를 평가한다. 실행 불가능한 환각(예: “진공청소기로 물을 빤다”)은 낮은 점수를 받아 제거되고, 현실적이고 유용한 행동이 선택된다.</li>
<li><strong>의의</strong>: 인터넷 AI(LLM)의 고수준 추론 능력을 사용하되, 이를 로봇의 물리적 가능성(Affordance)에 확률적으로 접지(Grounding)시킴으로써 환각 문제를 완화하고 유의미한 행동 계획을 생성했다.</li>
</ul>
<h4>4.1.2  <em>PaLM-E</em>: 체화된 다중모달 언어 모델</h4>
<p><em>PaLM-E</em> 33는 ’Embodied’의 E를 모델 이름에 명시하며, 로봇의 센서 데이터(이미지, 상태 벡터)를 언어 토큰과 동등하게 취급하여 LLM에 입력으로 넣는 방식을 제안했다.</p>
<ul>
<li><strong>작동 원리</strong>: 시각적 관찰(Visual Observation)과 로봇 상태(State)를 인코딩하여 텍스트 프롬프트와 함께 거대 언어 모델(<em>PaLM</em>)에 주입한다. 모델은 “서랍을 열고 칩 과자를 꺼내라“는 명령에 대해, 텍스트뿐만 아니라 현재 보이는 이미지 정보를 바탕으로 다음 행동 계획을 텍스트로 출력한다. “Multi-modal sentence completion” 문제로 로봇 제어를 재정의한 것이다.</li>
<li><strong>의의</strong>: 인터넷 AI 모델을 별도의 수정 없이 로봇의 ’두뇌’로 사용하면서, 멀티모달 입력을 통해 끊김 없는(End-to-End) 추론을 가능하게 했다. 이는 인터넷 AI의 일반화 능력이 로봇 도메인으로 긍정적으로 전이(Transfer)될 수 있음을 증명했다.</li>
</ul>
<h4>4.1.3  <em>RT-2</em>: 비전-언어-행동(VLA) 모델의 탄생</h4>
<p><em>RT-2</em> (Robotic Transformer 2) 36는 패러다임 전환의 정점이다. 기존 VLM이 이미지와 텍스트를 입력받아 텍스트를 내뱉었다면, <em>RT-2</em>는 텍스트 대신 **로봇의 행동(Action Token)**을 직접 출력한다.</p>
<ul>
<li><strong>작동 원리</strong>: 인터넷 규모의 이미지-텍스트 데이터로 VLM을 사전 학습한 후, 로봇의 궤적 데이터(이미지-행동 쌍)를 파인 튜닝(Fine-tuning)한다. 이때 로봇의 행동(팔의 관절 각도, 그리퍼 개폐)을 1에서 256 사이의 정수로 이산화(Discretize)하여 텍스트 토큰처럼 취급한다.</li>
<li><strong>의의</strong>: 이를 통해 <strong>VLA (Vision-Language-Action)</strong> 모델이라는 새로운 카테고리가 탄생했다. <em>RT-2</em>는 학습하지 않은 물체나 복잡한 추론이 필요한 명령(예: “슈퍼맨 피규어를 멸종된 동물 쪽으로 옮겨라”)에 대해서도 인터넷 데이터를 통해 얻은 지식을 활용하여 성공적으로 행동을 생성했다. 이는 인터넷 AI의 ’지식’과 엠바디드 AI의 ’행동’이 하나의 신경망 가중치 안에서 완전히 통합될 수 있음을 보여준다.</li>
</ul>
<h4>4.1.4  <em>Voyager</em>: 평생 학습하는 에이전트와 코드 생성</h4>
<p><em>Voyager</em> 40는 <em>Minecraft</em>라는 개방형 게임 환경에서 LLM(<em>GPT-4</em>)을 이용해 스스로 탐험하고, 스킬을 배우고, 새로운 발견을 하는 에이전트이다.</p>
<ul>
<li><strong>작동 원리</strong>: <em>Voyager</em>는 로봇의 제어 신호 대신, 실행 가능한 코드(Code)를 작성하여 행동을 수행한다. 실행 결과(오류 메시지, 성공 여부, 인벤토리 변화)를 피드백으로 받아 코드를 수정하고, 성공한 스킬은 라이브러리에 저장하여 나중에 더 복잡한 작업을 할 때 재사용한다.</li>
<li><strong>의의</strong>: 이는 **코드 생성(Code Generation)**이 엠바디드 AI의 강력한 행동 공간(Action Space)이 될 수 있음을 보여주며, 인간의 개입 없이 스스로 커리큘럼을 짜고 학습하는 ’평생 학습(Lifelong Learning)’과 ’오픈 엔디드 학습(Open-Ended Learning)’의 가능성을 제시했다. 40</li>
</ul>
<h2>5.  기술적 도전과 미래: 완전한 엠바디드 지능을 향하여</h2>
<p>인터넷 AI에서 엠바디드 AI로의 전환은 이제 막 시작되었다. 파운데이션 모델의 도입으로 로봇은 더 똑똑해졌지만, 여전히 해결해야 할 난제들이 산적해 있다.</p>
<p>첫째, **데이터의 비대칭성(Data Asymmetry)**이다. 인터넷 텍스트 데이터는 수조 토큰에 달하지만, 양질의 로봇 행동 데이터는 기껏해야 수백만 프레임 수준이다. 12 로봇 데이터는 수집하기 어렵고, 포맷이 제각각이며, 공유가 어렵다. 이를 해결하기 위해 전 세계 로봇 연구실이 데이터를 공유하는 ‘Open X-Embodiment’ 노력이나, 비디오 생성 AI(<em>Sora</em>, <em>Gen-2</em>)를 이용해 물리적으로 그럴듯한 합성 데이터를 생성하고 이를 학습에 활용하는 ‘월드 모델(World Model)’ 접근법 19이 활발히 연구되고 있다. 월드 모델은 에이전트가 “내가 이 행동을 하면 세상이 어떻게 변할까?“를 상상(Simulation)할 수 있게 함으로써, 실제 상호작용 없이도 학습 효율을 극대화한다.</p>
<p>둘째, **실시간성(Real-time)과 안전(Safety)**이다. 챗봇이 답변을 1초 늦게 하거나 틀린 말을 하는 것은 허용되지만, 자율주행차나 산업용 로봇이 1초 늦게 반응하거나 오작동하는 것은 재앙이다. 10 수십억 파라미터의 거대 모델을 엣지 디바이스(Edge Device)에서 실시간으로 구동하기 위한 경량화 기술(Quantization, Distillation)과, 모델의 불확실성을 엄격하게 통제하고 이론적으로 보증하는 안전 제어(Safe Control) 기술이 필수적이다.</p>
<p>셋째, <strong>진정한 물리적 이해</strong>이다. 현재의 VLA 모델들은 여전히 통계적 상관관계에 크게 의존한다. 로봇이 진정으로 물리학(마찰, 탄성, 유체 역학)을 직관적으로 이해하고, 낯선 도구를 즉석에서 사용하여 문제를 해결하는 ‘맥가이버’ 수준의 지능에 도달하려면, 단순한 모방 학습(Imitation Learning)을 넘어선 인과적 추론(Causal Reasoning)과 심층 강화학습(Deep RL)의 고도화가 필요하다. 43</p>
<h2>6. 결론: 뇌를 가진 신체, 세계로 나아가다</h2>
<p>우리는 지금 ’인터넷 AI’라는 거인의 어깨 위에서 ’엠바디드 AI’라는 새로운 거인이 일어서는 광경을 목격하고 있다. 인터넷 AI가 디지털 세계의 정보를 집대성하여 지능의 가능성을 증명했다면, 엠바디드 AI는 그 지능을 물리 세계로 확장하여 인류의 삶을 실질적으로 보조하고 노동을 혁신할 것이다.</p>
<p>1.1절에서 살펴본 인터넷 AI와 엠바디드 AI의 본질적 차이, 그리고 파운데이션 모델을 통한 융합의 흐름은, 이어지는 챕터들에서 다룰 제어 이론, 인지 과학, 3D 비전, 그리고 최신 로봇 학습 알고리즘들을 이해하는 나침반이 될 것이다. 이제 우리는 모니터 뒤의 안전한 관찰자에서 벗어나, 거칠고 불확실하지만 무한한 가능성으로 가득 찬 물리적 현실의 세계로, 즉 ’엠바디드 AI’의 세계로 발을 내디뎌야 한다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Overview of Embodied Artificial Intelligence | by Luis Bermudez | machinevision - Medium, https://medium.com/machinevision/overview-of-embodied-artificial-intelligence-b7f19d18022</li>
<li>Strategic Analysis: Transitioning from an AI-Centric to an Embodied Robot-Centric Economy, https://johnrector.me/2025/11/16/strategic-analysis-transitioning-from-an-ai-centric-to-an-embodied-robot-centric-economy/</li>
<li>Beyond Digital: Why Embodied AI Is the Essential Next Frontier - Anshad Ameenza, https://anshadameenza.com/blog/technology/embodied-ai-beyond-digital-frontier/</li>
<li>1월 1, 2026에 액세스, [https://arxiv.org/html/2512.03687v1#:<sub>:text=Unlike%20passive%20systems%20that%20rely,may%20not%20provide%20sufficient%20information.](https://arxiv.org/html/2512.03687v1#:</sub>:text=Unlike passive systems that rely, <a href="https://arxiv.org/html/2512.03687v1#:~:text=Unlike%20passive%20systems%20that%20rely,may%20not%20provide%20sufficient%20information.">https://arxiv.org/html/2512.03687v1#:~:text=Unlike%20passive%20systems%20that%20rely,may%20not%20provide%20sufficient%20information.</a></li>
<li>Active Visual Perception: Opportunities and Challenges - arXiv, https://arxiv.org/html/2512.03687v1</li>
<li>Active Perception: Interactive Manipulation for Improving Object Detection - Stanford Computer Science, https://cs.stanford.edu/~quocle/activevision.pdf</li>
<li>Embodied Multi-Agent Systems: A Review - IEEE/CAA Journal of Automatica Sinica, https://www.ieee-jas.net/article/doi/10.1109/JAS.2025.125552</li>
<li>Towards Continual Reinforcement Learning: A Review and Perspectives - Journal of Artificial Intelligence Research, https://jair.org/index.php/jair/article/download/13673/26878/32800</li>
<li>Capability Inversion: The Turing Test Meets Information Design - National Bureau of Economic Research, https://www.nber.org/system/files/working_papers/w33893/w33893.pdf</li>
<li>Digital twins to embodied artificial intelligence: review and perspective - OAE Publishing Inc., https://www.oaepublish.com/articles/ir.2025.11</li>
<li>Embodied AI: Emerging Risks and Opportunities for Policy Action - arXiv, https://arxiv.org/html/2509.00117</li>
<li>The Value of Data in Embodied Artificial Intelligence - Communications of the ACM, https://cacm.acm.org/blogcacm/the-value-of-data-in-embodied-artificial-intelligence/</li>
<li>Hallucination (artificial intelligence) - Wikipedia, https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)</li>
<li>The Difficulties in Symbol Grounding Problem and the Direction for Solving It - MDPI, https://www.mdpi.com/2409-9287/7/5/108</li>
<li>The physical symbol grounding problem - University of Southampton Web Archive, https://web-archive.southampton.ac.uk/cogprints.org/3055/1/csr.pdf</li>
<li>A test of indirect grounding of abstract concepts using multimodal distributional semantics, https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2022.906181/full</li>
<li>Symbol ungrounding: what the successes (and failures) of large language models reveal about human cognition - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC11529626/</li>
<li>From Affordances to Abstract Words: The Flexibility of Sensorimotor Grounding - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC8534254/</li>
<li>Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning - arXiv, https://arxiv.org/html/2508.10399v1</li>
<li>Conceiving, developing and representing a conversational-actor agent Acting paradigm and theatrical dialog 1. Summary A convers - Manufacture.ch, <a href="https://www.manufacture.ch/download/docs/2dkc8s7e.pdf/Project%20Chatbot%20in%20English.pdf">https://www.manufacture.ch/download/docs/2dkc8s7e.pdf/Project%20Chatbot%20in%20English.pdf</a></li>
<li>AI as intermediary in modern-day ritual: An immersive, interactive production of the roller disco musical Xanadu at UCLA - arXiv, https://arxiv.org/html/2511.06195v1</li>
<li>Active Haptic Perception in Robots: A Review - Frontiers, https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2019.00053/full</li>
<li>Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI - arXiv, https://arxiv.org/html/2407.06886v1</li>
<li>Active learning in robotics - Thomas A. Berrueta, https://tberrueta.github.io/assets/pdfs/TaylorMechatronics2021.pdf</li>
<li>Continual Learning: Applications and the Road Forward - OpenReview, https://openreview.net/pdf?id=axBIMcGZn9</li>
<li>Active Reinforcement Learning Vs. Passive Reinforcement Learning - Toloka AI, https://toloka.ai/blog/active-reinforcement-learning-vs-passive-reinforcement-learning/</li>
<li>Active Comparison Based Learning Incorporating User Uncertainty and Noise - Carnegie Mellon University Robotics Institute, https://www.ri.cmu.edu/pub_files/2016/6/claus.pdf</li>
<li>On measuring grounding and generalizing grounding problems - arXiv, https://arxiv.org/html/2512.06205v1</li>
<li>Will embodied AI create robotic coworkers? - McKinsey, https://www.mckinsey.com/industries/industrials/our-insights/will-embodied-ai-create-robotic-coworkers</li>
<li>A Survey of Robotic Language Grounding: Tradeoffs between Symbols and Embeddings - IJCAI, https://www.ijcai.org/proceedings/2024/0885.pdf</li>
<li>SayCan: Grounding Language in Robotic Affordances, https://say-can.github.io/</li>
<li>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances - Semantic Scholar, https://www.semanticscholar.org/paper/Do-As-I-Can%2C-Not-As-I-Say%3A-Grounding-Language-in-Ahn-Brohan/cb5e3f085caefd1f3d5e08637ab55d39e61234fc</li>
<li>[2303.03378] PaLM-E: An Embodied Multimodal Language Model - arXiv, https://arxiv.org/abs/2303.03378</li>
<li>PaLM - Wikipedia, https://en.wikipedia.org/wiki/PaLM</li>
<li>Slowly Explained: PaLM-E: An Embodied Multimodal Language Model - YouTube, https://www.youtube.com/watch?v=-5RtZ4eN_qs</li>
<li>Vision-language-action model - Wikipedia, https://en.wikipedia.org/wiki/Vision-language-action_model</li>
<li>RT-2: New model translates vision and language into action - Google DeepMind, https://deepmind.google/blog/rt-2-new-model-translates-vision-and-language-into-action/</li>
<li>RT-2: Vision-Language-Action Models, https://robotics-transformer2.github.io/</li>
<li>kyegomez/RT-2: Democratization of RT-2 “RT-2: New model translates vision and language into action” - GitHub, https://github.com/kyegomez/RT-2</li>
<li>Voyager: An Open-Ended Embodied Agent with Large Language Models - arXiv, https://arxiv.org/abs/2305.16291</li>
<li>Voyager | An Open-Ended Embodied Agent with Large Language Models, https://voyager.minedojo.org/</li>
<li>Voyager: An Open-Ended Embodied Agent with Large Language Models, https://www.semanticscholar.org/paper/Voyager%3A-An-Open-Ended-Embodied-Agent-with-Large-Wang-Xie/f197bf0fc2f228483f6af3285000d54d8d97f9eb</li>
<li>Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI | alphaXiv, https://www.alphaxiv.org/overview/2407.06886v8</li>
<li>A Comprehensive Survey on World Models for Embodied AI - arXiv, https://arxiv.org/html/2510.16732v1</li>
<li>[2406.00765] The Embodied World Model Based on LLM with Visual Information and Prediction-Oriented Prompts - arXiv, https://arxiv.org/abs/2406.00765</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>