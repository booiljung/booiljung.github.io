<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:1.3 신체성 가설 (The Embodiment Hypothesis)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>1.3 신체성 가설 (The Embodiment Hypothesis)</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 1. Embodied AI의 태동: 뇌를 가진 신체</a> / <a href="index.html">1.3 신체성 가설 (The Embodiment Hypothesis)</a> / <span>1.3 신체성 가설 (The Embodiment Hypothesis)</span></nav>
                </div>
            </header>
            <article>
                <h1>1.3 신체성 가설 (The Embodiment Hypothesis)</h1>
<p>인공지능 연구의 역사는 ’지능(Intelligence)’의 본질을 규명하고 이를 공학적으로 재현하려는 시도의 연속이었다. 초기의 고전적 인공지능(Classical AI)은 지능을 물리적 실체와 독립된 기호(Symbol)의 조작 과정으로 간주했다. 이른바 ‘통 속의 뇌(Brain in a Vat)’ 모델이라 불리는 이 접근법은, 적절한 입력과 논리적 연산 규칙만 주어진다면 신체가 없는 컴퓨터도 인간과 동일한 지능을 가질 수 있다는 기능주의적 전제를 바탕으로 했다. 그러나 현대 인공지능, 특히 딥러닝과 로보틱스의 융합 연구가 심화됨에 따라 이러한 전제는 근본적인 도전에 직면했다. 텍스트와 이미지 데이터의 통계적 패턴을 학습한 거대 모델들이 놀라운 성능을 보여주면서도, 물리적 세계의 인과율과 상식적 추론에서 빈번히 실패하는 현상이 관찰되었기 때문이다.</p>
<p>이 지점에서 **신체성 가설(The Embodiment Hypothesis)**은 지능이 추상적인 정보 처리의 결과가 아니라, 물리적 신체를 가진 에이전트가 환경과 실시간으로 상호작용하는 감각운동(Sensorimotor) 활동의 창발적 산물이라고 주장한다.1 본 장에서는 인지과학적 기원인 린다 스미스(Linda Smith)와 마이클 가서(Michael Gasser)의 연구로부터 시작하여, 심볼 그라운딩 문제(Symbol Grounding Problem)가 현대의 벡터 그라운딩(Vector Grounding) 문제로 재해석되는 과정, 그리고 최신 VLA(Vision-Language-Action) 모델과 물리적 지능(Physical Intelligence)을 구현하기 위한 세계 모델(World Models)에 이르기까지 신체성 가설이 현대 인공지능 및 제어 기술에 미치는 영향을 심층적으로 분석한다.</p>
<h2>1.  인지과학적 기원과 현대적 재해석: 아기로부터 배우는 교훈</h2>
<p>신체성 가설의 핵심은 지능이 물리적, 사회적, 언어적 세계에 근거(Grounding)를 두고 있어야 한다는 점이다. 2005년 스미스와 가서는 “The Development of Embodied Cognition: Six Lessons from Babies“라는 기념비적인 논문을 통해 인간의 유아기 발달 과정에서 신체가 인지 능력 형성에 미치는 결정적인 역할을 여섯 가지 원칙으로 정리하였다.1 이 원칙들은 단순한 아동 발달 심리학의 이론을 넘어, 오늘날 구글의 RT-2나 피지컬 인텔리전스(Physical Intelligence)의 <span class="math math-inline">\pi_0</span>와 같은 최신 로봇 파운데이션 모델(Foundation Models)을 설계하는 데 있어 필수적인 공학적 청사진을 제공한다.7</p>
<h3>1.1 교훈 1: 다중 감각적 경험 (Be Multi-modal)</h3>
<p>인간의 지능은 단일 감각에 의존하여 형성되지 않는다. 아기는 대상을 인식할 때 시각적 정보뿐만 아니라 청각, 촉각, 후각, 그리고 자신의 신체 위치를 감지하는 고유수용성 감각(Proprioception)을 통합적으로 활용한다. ’사과’라는 개념은 붉은 색(시각), 아삭한 소리(청각), 매끄러운 표면(촉각), 그리고 손으로 쥐었을 때의 무게감(고유수용감각)이 시간적으로 동기화되어 결합된 다중 감각적 표상이다. 이러한 통합된 경험은 단일 양상(Modality)의 정보가 가질 수 있는 모호성을 제거하고, 대상에 대한 보다 강건한(Robust) 개념을 형성하게 한다.</p>
<p>현대 인공지능에서 이는 멀티모달 학습(Multimodal Learning)의 이론적 토대가 되었다. 텍스트 데이터만으로 학습된 언어 모델(LLM)은 ’사과’라는 단어와 관련된 수많은 텍스트 패턴을 알지만, 사과의 물리적 실체성을 경험하지 못한다. 반면, CLIP(Contrastive Language-Image Pretraining)과 같은 모델이나 PaLM-E와 같은 구현체는 텍스트 토큰, 시각적 임베딩, 그리고 로봇의 상태 벡터를 동일한 잠재 공간(Latent Space)에 투영함으로써 이러한 다중 감각적 통합을 시도한다.9 특히 최신 VLA 모델들은 시각 정보와 언어 정보, 그리고 로봇의 관절 제어 신호를 하나의 트랜스포머 아키텍처 내에서 처리함으로써, 감각(Perception)과 행위(Action)가 분리되지 않은 통합된 지능을 구현하려 한다.</p>
<h3>1.2 교훈 2: 점진적 발달 (Be Incremental)</h3>
<p>아기는 태어나자마자 복잡한 운동 제어를 수행하거나 추상적인 사고를 하지 않는다. 기어가기, 앉기, 서기, 걷기와 같은 신체적 발달 단계를 거치며, 각 단계에서 획득한 감각운동 기술을 바탕으로 다음 단계의 더 복잡한 기술을 습득한다. 이러한 점진적 발달 과정은 제한된 인지 자원을 효율적으로 관리하면서도 복잡한 환경에 적응할 수 있게 하는 핵심 전략이다.</p>
<p>로봇 공학 및 인공지능 학습 이론에서 이는 커리큘럼 러닝(Curriculum Learning)이나 최근 주목받고 있는 **기술 증분 학습(Skill-Incremental Learning)**과 직접적으로 연결된다.12 기존의 딥러닝 모델, 특히 정적인 데이터셋에 과적합된 모델들은 새로운 데이터를 학습할 때 이전 지식을 급격히 잃어버리는 ‘치명적 망각(Catastrophic Forgetting)’ 문제를 겪는다. 반면, 신체화된 에이전트는 물리적 환경과의 지속적인 상호작용 속에서 기존의 감각운동 스키마(Schema) 위에 새로운 기술을 쌓아 올리는 구조를 가져야 한다. 최근 iManip과 같은 연구는 로봇이 물체를 조작하는 기술을 점진적으로 학습하면서도 이전 기술을 유지하는 방법론을 제안하며, 이는 아기의 발달 과정에서 영감을 받은 공학적 해법이라 할 수 있다.13</p>
<h3>1.3 교훈 3: 물리적 상호작용 (Be Physical)</h3>
<p>지능은 진공 속에서 작동하는 알고리즘이 아니라, 중력, 마찰, 관성 등 물리적 법칙이 지배하는 세계에서 신체를 움직여 문제를 해결하는 과정에서 발달한다. 단순히 세상을 관찰하는 것(Passive Observation)과 내가 세상에 개입하여 변화를 일으키는 것(Active Intervention)은 질적으로 다른 데이터를 생성한다. 아기는 물체를 떨어뜨려 보며 중력을 배우고, 벽에 부딪혀 보며 공간의 경계를 인식한다.</p>
<p>이 교훈은 오늘날 **‘인터넷 AI(Internet AI)’**와 **‘신체화된 AI(Embodied AI)’**를 구분 짓는 가장 결정적인 기준이 된다.14 인터넷 AI는 정제된 데이터셋(이미지, 텍스트)을 수동적으로 학습하는 반면, 신체화된 AI는 환경과의 동적인 상호작용을 통해 자신의 학습 데이터를 스스로 생성한다. 물리적 신체를 가진다는 것은 에이전트의 인지 과정에 물리적 제약(Constraints)을 부과하는 동시에, 그 제약을 활용하여 환경을 이해할 수 있는 강력한 도구(Leverage)를 제공하는 것이다. Duan 등(2022)의 연구는 시뮬레이터와 물리적 로봇이 정적 학습의 한계를 넘어 동적 경험을 통해 진화하는 과정을 보여준다.16</p>
<h3>1.4 교훈 4: 탐험 (Explore)</h3>
<p>아기는 끊임없이 움직이고, 물건을 입에 넣고, 소리를 지르며 세상을 탐험한다. 이러한 탐험적 행위는 외부에서 보기에 뚜렷한 목적이 없어 보일지라도(Goal-free), 실제로는 세계의 인과관계(Causality)와 자신의 신체적 능력(Affordance)을 학습하는 매우 효율적인 데이터 수집 과정이다.</p>
<p>로봇 공학에서 이는 **능동적 인식(Active Perception)**과 내재적 동기(Intrinsic Motivation)에 의한 탐색 알고리즘으로 구현된다.17 고정된 카메라로 수동적으로 이미지를 받아들이는 로봇은 시야가 가려지거나 조명이 어두우면 인식에 실패한다. 그러나 탐험하는 로봇은 고개를 돌려 가려진 부분을 확인하고, 물체를 조작하여 보이지 않는 뒷면을 확인한다. 스미스와 가서의 연구는 이러한 탐험이 뇌의 신경망 구조 자체를 환경에 최적화된 형태로 조직화한다고 주장하며, 이는 최근 얀 르쿤이 제안한 자가지도 학습(Self-Supervised Learning) 기반의 세계 모델 학습과 맥락을 같이한다.20</p>
<h3>1.5 교훈 5: 사회적 상호작용 (Be Social)</h3>
<p>인간의 지능은 고립된 개체가 아니라 타인과의 상호작용 속에서 발달한다. 아기는 양육자의 시선을 따라가는 공동 주의(Joint Attention)를 통해 중요한 정보에 집중하는 법을 배우고, 타인의 행동을 모방(Imitation)함으로써 시행착오를 줄이며 복잡한 기술을 습득한다.</p>
<p>최근 휴머노이드 로봇 연구에서 인간의 시연(Demonstration)을 보고 배우는 모방 학습(Imitation Learning)이나, 인간의 피드백을 통한 강화학습(RLHF: Reinforcement Learning from Human Feedback)이 강조되는 이유는 바로 이러한 사회적 신체성 때문이다.21 특히, 로봇이 인간 파트너의 관점을 이해하지 못하면 협업에 실패할 확률이 급격히 증가한다는 연구 결과(Baek et al., 2025)는 사회적 신체성이 단순한 기능 추가가 아니라 지능형 에이전트의 필수 조건임을 시사한다.22 지능은 개인의 뇌 속에 갇힌 것이 아니라, 사회적 맥락과 공유된 환경 속에 분산되어(Distributed) 존재한다.</p>
<h3>1.6 교훈 6: 언어 학습 (Learn a Language)</h3>
<p>언어는 신체적 경험을 추상화하고 타인과 공유하는 강력한 도구다. 그러나 언어는 기호 그 자체로 존재하는 것이 아니라, 앞서 언급한 물리적, 사회적 경험에 기반(Grounding)할 때 비로소 의미를 갖는다. 아기는 “공“이라는 단어를 배우기 전에 공을 만지고, 던지고, 굴리는 수많은 감각운동 경험을 축적한다. 이러한 경험적 토대 위에 언어라는 상징적 라벨이 붙는 것이다.</p>
<p>Bisk 등(2020)의 “Experience Grounds Language“는 이 원칙을 AI에 적용하여, 언어 모델이 신체 없는 ’통 속의 뇌’가 되지 않기 위해 반드시 물리적 경험(World Scope 5)과 연결되어야 함을 주장한다.23 최신 VLA 모델들은 로봇의 행동 토큰과 언어 토큰을 동일한 시퀀스로 처리함으로써, 언어가 추상적 기호가 아닌 물리적 행위를 유발하고 설명하는 매개체로서 기능하도록 한다. 이는 언어 능력이 신체적 상호작용 능력과 분리될 수 없음을 보여주는 공학적 증거이다.</p>
<p><strong>표 1.3.1 스미스와 가서의 6가지 교훈과 최신 AI/로봇 기술의 대응</strong></p>
<table><thead><tr><th><strong>교훈 (Lessons from Babies)</strong></th><th><strong>핵심 개념 (Core Concept)</strong></th><th><strong>최신 AI/로봇 구현 기술 (SOTA Implementation)</strong></th></tr></thead><tbody>
<tr><td><strong>1. 다중 감각적 경험</strong></td><td>시각, 청각, 촉각, 고유수용감각의 통합</td><td><strong>PaLM-E, CLIP, Audio-Visual Transformers</strong> (멀티모달 임베딩 융합)</td></tr>
<tr><td><strong>2. 점진적 발달</strong></td><td>쉬운 기술에서 복잡한 기술로 단계적 학습</td><td><strong>Curriculum Learning, Skill-Incremental Learning (iManip)</strong></td></tr>
<tr><td><strong>3. 물리적 상호작용</strong></td><td>환경에 개입하여 인과관계 학습</td><td><strong>Embodied AI, Sim-to-Real Transfer, Interactive Perception</strong></td></tr>
<tr><td><strong>4. 탐험</strong></td><td>내재적 동기에 의한 능동적 정보 수집</td><td><strong>Active Vision, Intrinsic Motivation RL, Exploration Policies</strong></td></tr>
<tr><td><strong>5. 사회적 상호작용</strong></td><td>모방, 공동 주의, 피드백 학습</td><td><strong>Imitation Learning (Behavior Cloning), RLHF, Human-Robot Collaboration</strong></td></tr>
<tr><td><strong>6. 언어 학습</strong></td><td>물리적 경험에 근거한 기호 습득</td><td><strong>VLA Models (RT-2, OpenVLA), Grounded Language Understanding</strong></td></tr>
</tbody></table>
<h2>2.  심볼 그라운딩에서 벡터 그라운딩으로 (From Symbol to Vector Grounding)</h2>
<p>신체성 가설이 AI 연구의 전면에 재등장하게 된 배경에는 1990년 스티븐 하나드(Stevan Harnad)가 제기한 **‘심볼 그라운딩 문제(Symbol Grounding Problem)’**가 있다.25 이는 형식적 기호 시스템(Formal Symbol System)이 그 기호가 지칭하는 실제 세계의 대상이나 의미와 어떻게 내재적으로(Intrinsically) 연결될 수 있는가에 대한 철학적이고 공학적인 난제이다.</p>
<h3>2.1 고전적 심볼 그라운딩과 중국어 방 논변</h3>
<p>존 설(John Searle)의 ‘중국어 방(Chinese Room)’ 사고실험은 기호를 규칙에 따라 문법적으로 조작(Syntactic Manipulation)하는 것만으로는 의미를 이해(Semantic Understanding)했다고 볼 수 없음을 논증한다.26 설령 어떤 시스템이 중국어 질문에 대해 완벽한 중국어 답변을 생성한다 하더라도, 그 시스템 내부에는 중국어 단어가 지칭하는 대상에 대한 경험적 내용이 결여되어 있다면 그것은 진정한 이해가 아니다.</p>
<p>하나드는 이를 해결하기 위해 기호가 에이전트의 감각(Sensory) 및 운동(Motor) 능력과 직접적으로 연결되어야 한다고 주장했다.28 즉, ’사과’라는 기호의 의미는 사전에 정의된 텍스트 설명이나 다른 기호들과의 관계(예: “사과는 과일이다”)에 의해 정의되는 것이 아니라, 에이전트가 사과를 보고(빨갛고 둥글다), 만지고(매끄럽다), 씹어보는(달콤하고 아삭하다) 과정에서 추출된 **감각운동 불변량(Sensorimotor Invariants)**에 의해 획득되어야 한다. 기호 체계는 바닥(Ground)인 물리적 세계에 닻을 내려야(Anchor) 비로소 의미를 가진다.</p>
<h3>2.2 LLM 시대의 벡터 그라운딩 문제 (The Vector Grounding Problem)</h3>
<p>현대의 LLM은 이산적인 기호(Symbol) 대신 고차원 벡터 공간(Vector Space)에서의 연산을 수행한다. 수십억 개의 파라미터를 가진 신경망은 단어와 문장 사이의 통계적 확률 분포를 학습하여 인간 수준의 텍스트 생성 능력을 보여준다. 그러나 최근 연구자들은 심볼 그라운딩 문제를 **‘벡터 그라운딩 문제(The Vector Grounding Problem)’**로 재정의하며 여전히 같은 질문을 던지고 있다.11</p>
<p>“GPT-4는 사과를 진정으로 이해하는가, 아니면 ’사과’라는 벡터와 코사인 유사도가 높은 다른 벡터들을 인출할 뿐인가?”</p>
<p>Mollo와 Millière(2023)는 LLM이 <strong>지시적 그라운딩(Referential Grounding)</strong>, 즉 언어적 항목이 실제 세계의 대상과 연결되는 능력을 결여하고 있다고 지적한다.30 텍스트 데이터만으로 학습된 임베딩 벡터는 단어 간의 관계(Relational Grounding)는 완벽하게 학습했을지 몰라도, 그 단어가 지칭하는 물리적 속성(질량, 마찰, 탄성 등)과는 유리되어 있다. 이는 마치 사과의 맛을 글로만 배운 사람이 사과의 맛을 설명하는 것과 같다.</p>
<p>이러한 그라운딩의 부재는 **물리적 상식의 오류(Physical Commonsense Failure)**로 나타난다. 예를 들어, Gemini 2.5와 같은 최신 모델조차 “길이 대 직경 비율이 10:1을 넘는 금속 부품을 선반 가공할 때 발생할 수 있는 문제“에 대해 텍스트적으로는 ’휨(Deflection)’이나 ’떨림(Chatter)’을 언급할 수 있지만, 실제 도면이나 물리적 상황이 주어졌을 때는 그 위험성을 인지하지 못하고 잘못된 공정 계획을 수립하는 경우가 보고된다.32 또한, 로봇 제어 상황에서 컵을 90도 회전시키는 것과 뒤집는 행위의 물리적 결과 차이를 혼동하여, 컵 안의 내용물을 쏟아버리는 환각(Hallucination) 현상이 발생하기도 한다.33 이는 언어 모델이 물리적 세계의 인과율을 체화(Embody)하지 못하고 통계적 연관성만으로 추론하기 때문에 발생하는 필연적인 한계다.</p>
<h3>2.3 인터넷 AI의 한계와 감각운동 유관성 (Sensorimotor Contingencies)</h3>
<p>Duan 등(2022)은 기존의 데이터셋 기반 AI를 **‘인터넷 AI(Internet AI)’**로 분류하며, 이를 정적이고 수동적인 관찰자로 규정했다.14 인터넷 AI에게 “냉장고를 열어라“라는 문장은 수많은 냉장고 이미지와 텍스트 설명의 집합이다. 반면, **‘신체화된 AI(Embodied AI)’**에게 이 문장은 “냉장고 손잡이에 접근하여(Approach), 손잡이를 쥐고(Grasp), 내 몸의 균형을 유지하며 팔을 뒤로 당겨(Pull), 문이 열리는 궤적에 맞춰 내 몸을 이동하라“는 복잡한 감각운동 제어의 시퀀스를 의미한다.</p>
<p>이 차이는 오레건(O’Regan)과 노에(Noë)가 제안한 <strong>감각운동 유관성(Sensorimotor Contingencies)</strong> 이론으로 설명될 수 있다.29 이 이론에 따르면, 지각(Perception)은 수동적인 입력 처리가 아니라, “내가 이렇게 움직이면 감각 입력이 저렇게 변할 것이다“라는 법칙을 암묵적으로 이해하는 것이다. 예를 들어, 우리가 부드러운 스펀지를 본다는 것은 시각적 정보뿐만 아니라, “이것을 누르면 쑥 들어갈 것이다“라는 촉각-운동 유관성을 동시에 활성화하는 과정이다. 진정한 의미의 그라운딩은 기호나 벡터를 이러한 감각운동 유관성과 연결하는 과정이며, 이를 위해서는 에이전트가 물리적 세계에서 직접 행동하고 그 결과를 피드백 받는 루프(Action-Perception Loop) 안에 존재해야 한다.</p>
<h2>3.  능동적 인식과 상호작용적 인식 (Active and Interactive Perception)</h2>
<p>신체성 가설은 ’인식(Perception)’을 수동적인 정보 수용 과정이 아니라, 행동을 통해 정보를 획득하고 불확실성을 줄여나가는 능동적 탐색 과정으로 재정의한다. 이는 제임스 깁슨(J.J. Gibson)의 생태학적 심리학과 어포던스(Affordance) 이론을 현대 로봇 공학의 제어 이론과 결합한 것이다.</p>
<h3>3.1 능동적 비전 (Active Vision)</h3>
<p>고정된 카메라를 사용하는 전통적인 컴퓨터 비전(Passive Vision)은 폐색(Occlusion), 조명 변화, 스케일 모호성, 시점 의존성 등의 문제에 취약하다. 그러나 신체를 가진 에이전트는 카메라(눈)를 움직여 이러한 문제를 능동적으로 해결할 수 있다.17</p>
<ul>
<li><strong>시점 변경을 통한 정보 획득:</strong> 물체의 뒷면이 보이지 않을 때 고개를 돌리거나 드론을 이동시켜 가려진 정보를 획득한다. Bajcsy(1988)는 이를 “모델링을 위한 움직임“이 아닌 “보기를 위한 움직임“으로 정의했다.</li>
<li><strong>운동 시차(Motion Parallax) 활용:</strong> 머리를 움직임으로써 발생하는 시각적 흐름(Optical Flow)의 차이를 이용해 깊이(Depth) 정보를 단안 카메라로도 정확하게 파악한다.</li>
<li><strong>주의 집중(Attention) 제어:</strong> 시각적 주의 메커니즘을 통해 정보량이 많은 영역(예: 사람의 얼굴, 조작하려는 물체의 손잡이)에 해상도와 처리 자원을 집중시킨다.</li>
</ul>
<p>최신 연구에서는 심층 강화학습(Deep RL)을 활용하여 로봇이 물체를 파지(Grasping)하기 가장 좋은 시점(Next-Best-View)을 스스로 학습하는 프레임워크가 제안되고 있다. 예를 들어, 신경 파지장(Neural Graspness Field)과 같은 모델은 장면을 점진적으로 스캔하며 파지 성공 확률이 가장 높은 뷰포인트를 찾아내도록 카메라 이동 경로를 계획한다.18 이는 인식이 행동을 유발할 뿐만 아니라, 행동이 인식의 성능을 결정한다는 상호 의존성을 보여준다.</p>
<h3>3.2 상호작용적 인식 (Interactive Perception)</h3>
<p>능동적 비전이 센서의 위치를 바꾸는 것이라면, **상호작용적 인식(Interactive Perception)**은 환경 자체를 물리적으로 조작하여 정보를 획득하는 것이다.39 Bohg 등(2017)은 로봇이 환경을 수동적으로 관찰하는 것을 넘어, 물리적 상호작용을 통해 인식의 모호성을 해소해야 한다고 주장했다.</p>
<ul>
<li><strong>물리적 분리(Singulation):</strong> 테이블 위에 여러 물체가 겹쳐져 있어 개별 인식이 어려울 때, 로봇 팔로 물체들을 휘저어 흩뜨려 놓음으로써(Pushing/Stirring) 개별 물체의 경계를 명확히 한다.</li>
<li><strong>물성 확인:</strong> 물체의 강성(Stiffness)이나 무게를 알기 위해 눌러보거나(Poking) 들어본다. 시각 정보만으로는 알 수 없는 물리적 속성을 행동을 통해 드러내는(Reveal) 것이다.</li>
<li><strong>상태 변경:</strong> 상자 안의 내용물을 확인하기 위해 뚜껑을 열거나, 옷의 형태를 파악하기 위해 구겨진 옷을 펼치는 행위 등이 포함된다.</li>
</ul>
<p>버클리 대학의 AUTOLab에서 제안한 <strong>MANIP 아키텍처</strong>는 이러한 개념을 시스템 레벨에서 구현한 대표적 사례이다.40 MANIP은 로봇이 현재 시스템 상태(State)에 대한 확신도(Confidence)가 낮을 때, ‘상호작용적 인식’ 서브 정책을 실행하여 환경을 인식하기 쉬운 상태로 변경하거나 모호성을 해소하는 전략을 취한다. 예를 들어, 케이블이 꼬여 있어 그 구조를 파악하기 어려울 때, 무작위로 당겨보는 것이 아니라 꼬임을 풀 수 있는 방향으로 조작을 가하며 인식을 개선한다. 수학적으로 이는 행동 <span class="math math-inline">a</span>를 통해 관측 <span class="math math-inline">o</span>의 엔트로피 <span class="math math-inline">H(o|a)</span>를 최소화하거나, 상태 추정의 정보 이득(Information Gain)을 최대화하는 과정으로 모델링된다.22</p>
<p><strong>표 1.3.2 인식 패러다임의 비교</strong></p>
<table><thead><tr><th><strong>구분</strong></th><th><strong>수동적 인식 (Passive Perception)</strong></th><th><strong>능동적 인식 (Active Perception)</strong></th><th><strong>상호작용적 인식 (Interactive Perception)</strong></th></tr></thead><tbody>
<tr><td><strong>행위 주체</strong></td><td>관찰자 (Observer)</td><td>이동하는 관찰자 (Moving Observer)</td><td>조작자 (Manipulator)</td></tr>
<tr><td><strong>센서 제어</strong></td><td>고정 (Static)</td><td>이동 가능 (Mobile/Pan-Tilt)</td><td>이동 가능 + 환경 조작</td></tr>
<tr><td><strong>환경 상태</strong></td><td>변경 없음</td><td>변경 없음</td><td><strong>변경됨 (Forceful Interaction)</strong></td></tr>
<tr><td><strong>정보 획득</strong></td><td>주어진 데이터 처리</td><td>시점 변경을 통한 정보 획득</td><td>물리적 반응을 통한 숨겨진 속성 획득</td></tr>
<tr><td><strong>대표 예시</strong></td><td>ImageNet 분류, CCTV 감시</td><td>SLAM, Next-Best-View Planning</td><td>겹친 물체 헤치기, 눌러서 강성 확인</td></tr>
</tbody></table>
<hr />
<h2>4.  파운데이션 모델의 신체화: VLA 모델의 진화</h2>
<p>신체성 가설의 최신 구현체는 거대 언어 모델(LLM)과 비전 모델, 그리고 로봇 제어(Action)를 통합한 <strong>VLA(Vision-Language-Action)</strong> 모델이다.43 이는 스미스와 가서의 “Six Lessons” 중 ‘다중 감각적 경험’, ‘물리적 상호작용’, ’언어 학습’을 하나의 거대한 신경망 안에서 융합하려는 시도이며, 인공지능이 텍스트의 세계를 넘어 물리적 세계로 진입하는 중요한 전환점이다.</p>
<h3>4.1 PaLM-E: 신체화된 멀티모달 언어 모델</h3>
<p>구글의 PaLM-E는 LLM(PaLM)에 시각적 정보와 로봇의 상태 정보를 ‘토큰’ 형태로 주입(Injecting)하는 방식을 제안하며, “Embodied Language Model“이라는 개념을 정립했다.9</p>
<p>PaLM-E의 핵심 아이디어는 이질적인 모달리티(Modality)를 동일한 임베딩 공간으로 매핑하는 것이다. 이미지나 센서 데이터는 ViT(Vision Transformer)와 같은 인코더를 통해 벡터화된 후, 텍스트 토큰과 동일한 차원을 갖도록 선형 투영(Linear Projection)된다. 이를 통해 모델은 다음과 같은 **멀티모달 문장(Multimodal Sentences)**을 처리할 수 있게 된다.</p>
<blockquote>
<p>“입력: &lt;이미지_임베딩_1&gt; 상황에서 내가 &lt;로봇_상태_임베딩&gt;라면, 서랍을 열기 위해 어떻게 해야 하는가?”</p>
</blockquote>
<p>PaLM-E의 가장 큰 성과는 **긍정적 전이(Positive Transfer)**를 입증한 것이다. 인터넷 규모의 텍스트와 이미지로 학습된 일반 상식(General Commonsense)이 로봇 작업에 전이되어, 학습하지 않은 작업(Zero-shot)이나 처음 보는 물체에 대해서도 추론이 가능해진 것이다. 그러나 PaLM-E는 주로 고수준의 계획(High-level Planning)이나 텍스트 답변을 생성하는 데 강점이 있었으며, 초당 수십 회의 정밀한 제어 신호를 생성하는 저수준 제어(Low-level Control)는 여전히 별도의 정책 네트워크에 위임하는 경향이 있었다.</p>
<h3>4.2 RT-2: 행동을 언어처럼 (Actions as Tokens)</h3>
<p>RT-2(Robotic Transformer 2)는 LLM이 직접 로봇의 저수준 행동을 출력하도록 설계된 진정한 의미의 VLA 모델이다.43 RT-2는 로봇의 행동(End-effector의 위치 <span class="math math-inline">x, y, z</span>, 회전 <span class="math math-inline">roll, pitch, yaw</span>, 그리퍼 개폐 등)을 텍스트와 같은 ’토큰’으로 취급하는 혁신적인 접근을 취했다.</p>
<ul>
<li><strong>행동의 토큰화 (Action Tokenization):</strong> 로봇의 행동 공간을 256개의 구간(Bin)으로 이산화(Discretization)한다. 예를 들어, 로봇 팔을 앞으로 10cm 뻗는 동작은 “Action_Token_128“과 같은 형태로 변환된다. 이를 통해 언어 모델은 “사과를” 다음에 “집어라“라는 단어를 예측하듯, 현재 상태 이미지 다음에 적절한 행동 토큰을 자기회귀(Autoregressive) 방식으로 예측하게 된다.</li>
<li><strong>공동 학습 (Co-training):</strong> 웹 데이터(VQA, 캡션 등)와 로봇 궤적 데이터(Bridge Data 등)를 함께 학습함으로써, 시각적 인식과 의미적 추론, 그리고 물리적 행동 생성을 동시에 수행한다.</li>
</ul>
<p>RT-2는 “Superman toy를 집어라“와 같이 훈련 데이터에 없는 물체나 명령에 대해서도 웹 지식을 활용하여 성공적으로 작업을 수행했다. 그러나 행동을 이산적인 토큰으로 표현함으로써 발생하는 정밀도 저하(Quantization Error)와, 자기회귀적 생성 방식의 느린 추론 속도는 고빈도(High-frequency) 제어가 필요한 동적 조작(Dynamic Manipulation)에서 한계로 작용했다.49</p>
<h3>4.3 VLA 모델의 한계와 도전</h3>
<p>RT-2와 OpenVLA 등 기존 VLA 모델들은 다음과 같은 구조적 한계에 직면해 있다.</p>
<ol>
<li><strong>나이브 토큰화의 문제:</strong> 연속적인 물리적 행동을 이산적인 토큰으로 강제 변환함에 따라, 미세한 힘 조절이나 부드러운 궤적 생성이 어렵다.</li>
<li><strong>시간적 상관관계의 무시:</strong> 언어 모델은 토큰 간의 인과관계를 학습하지만, 로봇 제어에서는 인접한 시간 단계(Timestep) 간의 행동이 매우 높은 상관관계를 가진다(Smoothness). 독립적인 토큰 예측 방식은 이러한 연속성을 간과하여 떨림(Jittering) 현상을 유발할 수 있다.</li>
<li><strong>데이터 부족:</strong> 텍스트 데이터에 비해 로봇 데이터는 턱없이 부족하며, 특히 다양한 형태(Morphology)를 가진 로봇들의 데이터를 통합하는 것은 큰 도전이다.</li>
</ol>
<h2>5.  물리적 지능의 최전선: <span class="math math-inline">\pi_0</span>와 흐름 매칭 (Physical Intelligence and Flow Matching)</h2>
<p>2024년 말, 피지컬 인텔리전스(Physical Intelligence) 사는 기존 VLA의 한계를 극복하기 위해 <strong><span class="math math-inline">\pi_0</span> (Pi-Zero)</strong> 모델을 발표했다.50 이 모델은 신체성 가설의 관점에서 VLA 모델이 어떻게 물리적 세계의 연속성(Continuity)과 복잡성을 다루어야 하는지에 대한 새로운 해법을 제시한다.</p>
<h3>5.1 흐름 매칭 (Flow Matching) 기반의 행동 생성</h3>
<p><span class="math math-inline">\pi_0</span>는 행동을 이산적 토큰으로 변환하는 대신, 연속적인 행동 공간을 직접 다루기 위해 <strong>흐름 매칭(Flow Matching)</strong> 기법을 도입했다. 흐름 매칭은 디퓨전 모델(Diffusion Model)의 일반화된 형태로, 노이즈로부터 데이터 분포로 가는 최적의 경로(Vector Field)를 학습한다.</p>
<ul>
<li><strong>하이브리드 아키텍처:</strong> <span class="math math-inline">\pi_0</span>는 VLM 백본(PaliGemma 3B 등)이 텍스트와 이미지를 처리하여 문맥 정보를 추출하고, 이를 조건(Conditioning)으로 하여 별도의 <strong>행동 전문가(Action Expert)</strong> 모듈이 흐름 매칭을 통해 연속적인 행동 궤적(Trajectory)을 생성한다.54</li>
<li><strong>고빈도 및 동적 제어:</strong> 기존 VLA가 3~5Hz 수준의 제어에 머물렀던 반면, <span class="math math-inline">\pi_0</span>는 최대 50Hz의 제어 주기를 지원한다. 이를 통해 빨래 개기, 탁구 치기, 상자 조립하기, 계란 꺼내기와 같이 순간적인 힘 조절과 빠른 반응이 필요한 고난도의 동적 조작(Dexterous Manipulation)이 가능해졌다. 이는 신체성 가설에서 강조하는 ’미세한 감각운동 조율’을 대규모 파운데이션 모델 레벨에서 구현한 것이다.</li>
</ul>
<h3>5.2 교차 신체 (Cross-Embodiment) 학습</h3>
<p><span class="math math-inline">\pi_0</span>는 단일 로봇이 아닌, 8종 이상의 서로 다른 로봇(단팔, 양팔, 휴머노이드, 모바일 매니퓰레이터 등) 데이터를 통합 학습했다.52 이는 형태가 다른 신체들 사이에서도 통용되는 보편적인 물리적 상호작용의 원리(Physics of Interaction)를 모델이 학습했음을 시사한다. 예를 들어, “물체를 집어 올린다“는 행위의 추상적인 개념은 그리퍼의 모양이 달라도 중력을 이겨내고 마찰력을 유지해야 한다는 물리적 본질은 동일하기 때문이다.</p>
<p><strong>표 1.3.3 주요 VLA 모델 아키텍처 비교</strong></p>
<table><thead><tr><th><strong>모델</strong></th><th><strong>행동 표현 방식 (Action Representation)</strong></th><th><strong>생성 방식 (Generation Method)</strong></th><th><strong>제어 빈도</strong></th><th><strong>특징</strong></th></tr></thead><tbody>
<tr><td><strong>RT-2</strong></td><td>이산적 토큰 (Discretized Tokens)</td><td>자기회귀 (Autoregressive Next-Token Prediction)</td><td>저빈도 (~5Hz)</td><td>웹 데이터와의 Co-training, 높은 일반화 성능</td></tr>
<tr><td><strong>OpenVLA</strong></td><td>이산적 토큰</td><td>자기회귀</td><td>저빈도</td><td>오픈소스, Llama/Prismatic 기반</td></tr>
<tr><td><strong><span class="math math-inline">\pi_0</span> (Pi0)</strong></td><td><strong>연속적 벡터 (Continuous Vectors)</strong></td><td><strong>흐름 매칭 (Flow Matching / Diffusion)</strong></td><td><strong>고빈도 (50Hz)</strong></td><td>동적 조작 가능, 교차 신체 학습, 하이브리드 구조</td></tr>
</tbody></table>
<h2>6.  물리적 세계에서의 추론: 신체화된 사고 사슬 (Embodied Chain-of-Thought)</h2>
<p>LLM의 핵심 능력 중 하나인 ’사고 사슬(Chain-of-Thought, CoT)’을 로봇에 적용하려는 시도 또한 신체성 가설의 관점에서 중요하다. 순수한 텍스트 기반 CoT는 물리적 상황과 괴리된 환각을 일으킬 수 있다. 따라서 로봇의 CoT는 반드시 **신체화된 사고 사슬(Embodied Chain-of-Thought, ECoT)**이어야 한다.55</p>
<h3>6.1 ECoT의 구조와 필요성</h3>
<p>ECoT는 행동을 생성하기 전에 (1) 현재 관측된 상황에 대한 설명, (2) 작업의 하위 목표(Sub-goal) 설정, (3) 물체의 위치(Bounding Box) 및 상태 파악, (4) 자신의 팔 위치(End-effector state) 확인 등의 과정을 언어적으로, 그리고 시각적으로 명시화한다.</p>
<blockquote>
<p><strong>ECoT 예시:</strong></p>
<ul>
<li><strong>명령:</strong> “드라이버를 집어라.”</li>
<li><strong>관측:</strong> “테이블 위에 빨간색 망치와 노란색 드라이버가 있다.”</li>
<li><strong>그라운딩:</strong> “드라이버의 위치는 [bbox: 200, 300, 250, 400]이다. 내 그리퍼는 현재 [x:0.5, y:0.2]에 있다.”</li>
<li><strong>계획:</strong> “먼저 그리퍼를 드라이버 위로 이동(Move)시킨 후, Z축으로 하강(Descend)하여 파지(Grasp)해야 한다.”</li>
<li><strong>행동:</strong> [Action Vector Generation]</li>
</ul>
</blockquote>
<p>OpenVLA 기반의 연구 결과, ECoT를 적용했을 때 로봇의 작업 성공률이 28% 향상되었다.55 이는 “생각하고 행동하는(Think before you act)” 메커니즘이 물리적 수행 능력을 강화함을 입증한다. 또한, 로봇이 실패했을 때, 단순히 행동이 틀렸는지 아니면 인식(예: 망치를 드라이버로 오인) 단계에서 오류가 있었는지 추론 과정을 통해 투명하게 디버깅할 수 있다는 장점이 있다.58</p>
<h3>6.2 협업 로봇에서의 그라운딩 오류와 해결</h3>
<p>Baek 등(2025)은 인간과 로봇, 또는 로봇과 로봇이 협업할 때 발생하는 실패의 상당수가 ‘통신적 그라운딩(Communicative Grounding)’ 오류에서 기인함을 밝혔다.22 “그거 집어줘“라고 했을 때, 로봇이 자신의 시야(Egocentric View)에서만 ’그거’를 해석하면 협업은 실패한다. 이를 ’특권적 정보 편향(Privileged Information Bias)’이라 한다. ECoT는 로봇이 자신의 인식 상태와 불확실성을 언어로 표현하고, 파트너에게 “파란색 블록을 말하는 것인가?“라고 되물음(Active Querying)으로써 이러한 그라운딩 격차를 줄이는 데 기여할 수 있다.</p>
<h2>7.  세계 모델과 신체성의 미래 (World Models and Future Directions)</h2>
<p>마지막으로, 신체화된 지능이 나아가야 할 방향은 **세계 모델(World Models)**의 통합이다. 얀 르쿤(Yann LeCun)은 진정한 자율 지능(Autonomous Intelligence)을 위해 LLM과 같은 자기회귀적 생성 모델이 아닌, <strong>JEPA(Joint Embedding Predictive Architecture)</strong> 기반의 세계 모델을 제안한다.20</p>
<h3>7.1 생성 모델 vs. 예측 모델</h3>
<p>현재의 생성형 AI(Video Generative Models like Sora)는 픽셀 단위로 미래의 모든 세부 사항을 생성하려 한다. 이는 계산 비용이 매우 높으며, 바람에 흔들리는 나뭇잎과 같이 행동과 무관한 디테일(Aleatoric Uncertainty)까지 예측하느라 정작 중요한 물리적 인과관계를 놓칠 수 있다.</p>
<p>반면, 르쿤의 가설에 따르면 지능적인 에이전트는 입력의 모든 세부 사항을 예측할 필요가 없다. 대신 추상화된 **잠재 공간(Representation Space)**에서 자신의 행동(<span class="math math-inline">a</span>)이 상태(<span class="math math-inline">s</span>)를 어떻게 변화(<span class="math math-inline">s&#39;</span>)시킬지에 대한 본질적인 동역학(Dynamics)만 예측하면 된다. 이는 “이 컵을 놓으면 바닥에 떨어져 깨질 것이다“라는 결과를 예측하는 것이지, 깨진 유리 조각의 정확한 위치를 픽셀 단위로 그려내는 것이 아니다.</p>
<h3>7.2 시뮬레이터와 물리적 지능의 결합</h3>
<p>신체성 가설 관점에서 세계 모델은 에이전트의 ‘내적 시뮬레이터(Internal Simulator)’ 역할을 한다. 에이전트는 실제 행동을 수행하기 전에(Pre-act), 세계 모델을 통해 다양한 행동의 결과를 상상(Simulation)하고 계획(Planning)할 수 있다.59 이는 시행착오에 따른 비용이 큰 물리적 로봇에게 필수적인 능력이다. 최근 연구들은 인터넷 규모의 비디오 데이터를 통해 일반적인 물리 법칙을 학습하고, 이를 로봇의 제어 정책과 결합하여 ’일반화된 세계 모델’을 구축하는 방향으로 나아가고 있다.62</p>
<h2>8. 결론: 기호에서 행위로, 데이터에서 경험으로</h2>
<p>신체성 가설은 인공지능이 ’통 속의 뇌’에서 벗어나 ’세상 속의 신체’로 나아가야 함을 역설한다. 스미스와 가서가 제시한 여섯 가지 교훈—다중 감각, 점진적 발달, 물리적 상호작용, 탐험, 사회성, 언어 학습—은 20년이 지난 지금, VLA 모델과 <span class="math math-inline">\pi_0</span>, ECoT와 같은 최첨단 기술을 통해 구체화되고 있다. 심볼 그라운딩 문제는 벡터 그라운딩 문제로 진화하였으며, 이를 해결하기 위해 단순한 텍스트 학습을 넘어선 감각운동 통합(Sensorimotor Integration)과 능동적 상호작용이 필수적임이 증명되고 있다. 미래의 인공지능은 텍스트를 처리하는 거대 언어 모델을 넘어, 물리적 세계를 이해하고 조작하며 인간과 공존하는 **물리적 지능(Physical Intelligence)**으로 진화할 것이다. 이는 단순히 로봇을 더 잘 움직이게 하는 기술이 아니라, 진정한 의미의 인공 일반 지능(AGI)에 도달하기 위한 필수 불가결한 과정이다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>The development of embodied cognition: six lessons from babies - PubMed - NIH, https://pubmed.ncbi.nlm.nih.gov/15811218/</li>
<li>Embodied cognition - Wikipedia, https://en.wikipedia.org/wiki/Embodied_cognition</li>
<li>A Brief History of Embodied Artificial Intelligence, and its Outlook, https://cacm.acm.org/blogcacm/a-brief-history-of-embodied-artificial-intelligence-and-its-future-outlook/</li>
<li>The Development of Embodied Cognition: Six Lessons from Babies - ResearchGate, https://www.researchgate.net/publication/7925616_The_Development_of_Embodied_Cognition_Six_Lessons_from_Babies</li>
<li>The Development of Embodied Cognition: Six Lessons from Babies, https://www.cogsci.msu.edu/DSS/2010-2011/Smith/6lessons.pdf</li>
<li>The Development of Embodied Cognition: Six Lessons from Babies - Luis M. Rocha, https://casci.binghamton.edu/publications/embrob/papers/smith.html</li>
<li>The Development of Embodied Cognition: Six Lessons from Babies, https://vision.cs.utexas.edu/381V-fall2017/slides/Lin_paper.pdf</li>
<li>A Call for Embodied AI - arXiv, https://arxiv.org/html/2402.03824v3?ref=breakthroughpursuit.com</li>
<li>PaLM-E: An Embodied Multimodal Language Model | Request PDF - ResearchGate, https://www.researchgate.net/publication/369035918_PaLM-E_An_Embodied_Multimodal_Language_Model</li>
<li>PaLM-E: An embodied multimodal language model - Google Research, https://research.google/blog/palm-e-an-embodied-multimodal-language-model/</li>
<li>Do Multimodal Large Language Models and Humans Ground Language Similarly? - University of California San Diego, https://pages.ucsd.edu/~bkbergen/papers/2024_Jones_Trott_Bergen_TACL.pdf</li>
<li>Personalizing Humanoid Robot Behavior through Incremental Learning from Natural Interactions, https://h2t.iar.kit.edu/pdf/Weberruss2025.pdf</li>
<li>iManip: Skill-Incremental Learning for Robotic Manipulation - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2025/papers/Zheng_iManip_Skill-Incremental_Learning_for_Robotic_Manipulation_ICCV_2025_paper.pdf</li>
<li>A Call for Embodied AI - arXiv, https://arxiv.org/html/2402.03824v4</li>
<li>AI Embodiment Through 6G: Shaping the Future of AGI - ResearchGate, https://www.researchgate.net/publication/379415224_AI_Embodiment_Through_6G_Shaping_the_Future_of_AGI</li>
<li>A Call for Embodied AI - arXiv, https://arxiv.org/html/2402.03824v3</li>
<li>Revisiting active perception - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC6954017/</li>
<li>Active Perception for Grasp Detection via Neural Graspness Field - NIPS papers, https://proceedings.neurips.cc/paper_files/paper/2024/file/4364fef031fdf7bfd9d1c9c56b287084-Paper-Conference.pdf</li>
<li>Learning to Poke by Poking: Experiential Learning of Intuitive Physics - NIPS papers, http://papers.neurips.cc/paper/6113-learning-to-poke-by-poking-experiential-learning-of-intuitive-physics.pdf</li>
<li>V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning, https://arxiv.org/html/2506.09985v1</li>
<li>Incao S., Mazzola C., Belgiovine G., Sciutti A., A Roadmap for Embodied and Social Grounding in LLMs, Robophilosophy Conference, 2024, Aarhus. - arXiv, https://arxiv.org/html/2409.16900v1</li>
<li>Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents via Active Querying - arXiv, https://arxiv.org/html/2512.15776v1</li>
<li>Experience Grounds Language - DSpace@MIT, https://dspace.mit.edu/handle/1721.1/142865</li>
<li>Experience Grounds Language - ACL Anthology, https://aclanthology.org/2020.emnlp-main.703.pdf</li>
<li>Which symbol grounding problem should we try to solve? - arXiv, https://arxiv.org/pdf/2507.21080</li>
<li>The Difficulties in Symbol Grounding Problem and the Direction for Solving It - MDPI, https://www.mdpi.com/2409-9287/7/5/108</li>
<li>The Vector Grounding Problem - arXiv, https://arxiv.org/html/2304.01481v3</li>
<li>The Symbol Grounding Problem - arXiv, https://arxiv.org/html/cs/9906002</li>
<li>Symbol‐grounding Problem | Request PDF - ResearchGate, https://www.researchgate.net/publication/228005845_Symbol-grounding_Problem</li>
<li>No Qualia? No Meaning (and no AGI)! - Article (Preprint v1) by Marco Masi | Qeios, https://www.qeios.com/read/DN232Y</li>
<li>A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem - ResearchGate, https://www.researchgate.net/publication/398557132_A_Categorical_Analysis_of_Large_Language_Models_and_Why_LLMs_Circumvent_the_Symbol_Grounding_Problem</li>
<li>Frontier AI Models Still Fail at Basic Physical Tasks: A Manufacturing Case Study, https://www.lesswrong.com/posts/r3NeiHAEWyToers4F/frontier-ai-models-still-fail-at-basic-physical-tasks-a</li>
<li>STEER: Flexible Robotic Manipulation via Dense Language Grounding - arXiv, https://arxiv.org/html/2411.03409v1</li>
<li>Agentic LLM-based robotic systems for real-world applications: a review on their agenticness and ethics - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1605405/full</li>
<li>Overview of Embodied Artificial Intelligence | by Luis Bermudez | machinevision - Medium, https://medium.com/machinevision/overview-of-embodied-artificial-intelligence-b7f19d18022</li>
<li>Modeling arbitrarily applicable relational responding with the non-axiomatic reasoning system: a Machine Psychology approach - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC12497618/</li>
<li>Wanting to Be Understood Explains the Meta-Problem of Consciousness - arXiv, https://arxiv.org/html/2506.12086v1</li>
<li>Sensing &amp; Perception Archives - Carnegie Mellon University Robotics Institute, https://www.ri.cmu.edu/research-topic/sensing-perception/</li>
<li>Research – Intelligent Robotics and Vision Lab at the University of Texas at Dallas, https://labs.utdallas.edu/irvl/research/</li>
<li>MANIP: A Modular Architecture for Integrating Interactive Perception for Robot Manipulation - Berkeley AUTOLab, https://autolab.berkeley.edu/assets/publications/media/IROS24_MANIP_CR.pdf</li>
<li>Multi-Modal Perception with Vision, Language, and Touch for Robot Manipulation - UC Berkeley EECS, https://www2.eecs.berkeley.edu/Pubs/TechRpts/2025/EECS-2025-68.pdf</li>
<li>Deep Learning for Active Robotic Perception - SciTePress, https://www.scitepress.org/Papers/2023/122958/122958.pdf</li>
<li>VLA Models: Vision-Language-Action for Robotics (2025) | RoboCloud Hub, https://robocloud-dashboard.vercel.app/learn/blog/vla-models-robotics-2025</li>
<li>Vision-Language-Action Models: Concepts, Progress, Applications and Challenges - arXiv, https://arxiv.org/abs/2505.04769</li>
<li>jonyzhang2023/awesome-embodied-vla-va-vln - GitHub, https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln</li>
<li>Exploring Embodied Multimodal Large Models: Development, Datasets, and Future Directions - arXiv, <a href="https://arxiv.org/pdf/2502.15336">https://arxiv.org/pdf/2502.15336?</a></li>
<li>[PDF] PaLM-E: An Embodied Multimodal Language Model | Semantic Scholar, https://www.semanticscholar.org/paper/PaLM-E%3A-An-Embodied-Multimodal-Language-Model-Driess-Xia/38fe8f324d2162e63a967a9ac6648974fc4c66f3</li>
<li>Vision-Language-Action Models: Concepts, Progress, Applications and Challenges - arXiv, https://arxiv.org/html/2505.04769v1</li>
<li>[Paper Review] Pi0, Pi0.5, Pi0-FAST - Tracing the Path of Physical Intelligence (PI), https://bequiet-log.vercel.app/pi-review</li>
<li>allenzren/open-pi-zero: Re-implementation of pi0 vision-language-action (VLA) model from Physical Intelligence - GitHub, https://github.com/allenzren/open-pi-zero</li>
<li>π₀ (Pi0) - Hugging Face, https://huggingface.co/docs/lerobot/pi0</li>
<li>π0: A Vision-Language-Action Flow Model for General Robot Control - Physical Intelligence, https://www.physicalintelligence.company/download/pi0.pdf</li>
<li>π 0 : Our First Generalist Policy - Physical Intelligence, https://www.physicalintelligence.company/blog/pi0</li>
<li>𝜋₀: A Vision-Language-Action Flow Model for General Robot Control - arXiv, https://arxiv.org/html/2410.24164v1</li>
<li>Robotic Control via Embodied Chain-of-Thought Reasoning, https://embodied-cot.github.io/</li>
<li>Embodied Chain of Thought: A robotic policy that reason to solve the task. - GitHub, https://github.com/MichalZawalski/embodied-CoT</li>
<li>Training Strategies for Efficient Embodied Reasoning - arXiv, https://arxiv.org/html/2505.08243v2</li>
<li>Robotic Control via Embodied Chain-of-Thought Reasoning - arXiv, https://arxiv.org/html/2407.08693v1</li>
<li>Embodied AI Agents: Modeling the World - arXiv, https://arxiv.org/html/2506.22355v1</li>
<li>Comparative Analysis: Yann LeCun’s JEPA vs. Li Fei-Fei’s Spatial &amp; Embodied。Intelligence | Schemes and Mind Maps Artificial Intelligence | Docsity, https://www.docsity.com/en/docs/comparative-analysis-yann-lecuns-jepa-vs-li-fei-feis-spatial-and-embodiedintelligence/12942291/</li>
<li>World Models vs. Large Language Models and the Potential to Converge - Medium, https://medium.com/@d.incecushman/world-models-vs-large-language-models-and-the-potential-to-co-f1db3d947122</li>
<li>3D Vision-Language-Action Generative World Model (MIT, UCLA, UMass, etc.) - Reddit, https://www.reddit.com/r/singularity/comments/1bfsysa/3d_visionlanguageaction_generative_world_model/</li>
<li>A Step Toward World Models: A Survey on Robotic Manipulation - arXiv, https://arxiv.org/html/2511.02097v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>