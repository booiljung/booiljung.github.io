<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:1.3.2 감각운동 유관성 (Sensorimotor Contingencies): "보는 것은 곧 행동하는 것이다." 지각(Perception)과 행동(Action)의 불가분의 관계.</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>1.3.2 감각운동 유관성 (Sensorimotor Contingencies): "보는 것은 곧 행동하는 것이다." 지각(Perception)과 행동(Action)의 불가분의 관계.</h1>
                    <nav class="breadcrumbs"><a href="../../../../index.html">Home</a> / <a href="../../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../../index.html">제목: Embodied AI & Modern Control</a> / <a href="../index.html">Chapter 1. Embodied AI의 태동: 뇌를 가진 신체</a> / <a href="index.html">1.3 신체성 가설 (The Embodiment Hypothesis)</a> / <span>1.3.2 감각운동 유관성 (Sensorimotor Contingencies): "보는 것은 곧 행동하는 것이다." 지각(Perception)과 행동(Action)의 불가분의 관계.</span></nav>
                </div>
            </header>
            <article>
                <h1>1.3.2 감각운동 유관성 (Sensorimotor Contingencies): “보는 것은 곧 행동하는 것이다.” 지각(Perception)과 행동(Action)의 불가분의 관계.</h1>
<h2>1.  서론: 수동적 수신자에서 능동적 탐험가로의 전환</h2>
<p>우리가 “본다(seeing)“라고 말할 때, 우리는 흔히 눈이라는 광학 장치를 통해 외부 세계의 이미지를 받아들이고, 이를 뇌라는 중앙 처리 장치에서 분석하여 3차원 모델로 재구성하는 과정을 떠올린다. 이러한 전통적인 시각적 인지 모형, 즉 ‘스냅샷(snapshot)’ 가설은 초기 인공지능과 컴퓨터 비전 연구를 지배해왔다. 이 관점에 따르면, 로봇에게 시각을 부여하는 문제는 카메라로 입력된 정적 이미지에서 에지(edge)를 검출하고, 객체를 분할(segmentation)하며, 레이블을 분류(classification)하는 과정으로 환원된다. 그러나 이러한 접근 방식은 정적인 인터넷상의 이미지를 분류하는 데에는 성공했을지 모르나, 끊임없이 변화하는 물리적 세계와 상호작용해야 하는 로봇에게는 치명적인 한계를 드러냈다.</p>
<p>이 한계를 극복하기 위해 등장한 개념이 바로 **감각운동 유관성(Sensorimotor Contingencies, SMCs)**이다. J. Kevin O’Regan과 Alva Noë가 제창한 이 급진적인 이론은 “지각은 뇌 안에서 일어나는 수동적인 처리가 아니라, 유기체가 환경과 상호작용하는 능동적인 기술(Skill)“이라고 정의한다.1 즉, 보는 것은 단순히 눈을 통해 들어온 빛을 해석하는 과정이 아니라, 눈을 움직이고, 고개를 돌리고, 몸을 기울였을 때 감각 입력이 어떻게 변하는지에 대한 법칙(Law)을 마스터하는 과정이다. 이 관점에서 로봇은 더 이상 데이터를 수동적으로 처리하는 관찰자가 아니라, 자신의 행동을 통해 감각 정보를 능동적으로 변화시키고 조작하는 탐험가가 된다.</p>
<p>본 장에서는 엠바디드 AI(Embodied AI)의 철학적, 기술적 기반이 되는 감각운동 유관성 이론을 심도 있게 분석하고, 이것이 현대 로봇 공학의 슬램(SLAM), 능동적 지각(Active Perception), 그리고 최신 SOTA(State-of-the-Art) 기술인 월드 모델(World Models)과 JEPA(Joint-Embedding Predictive Architecture)에 어떻게 구현되고 있는지 논한다. 우리는 왜 진정한 인공지능에게 신체가 필수적인지, 그리고 최신 AI가 어떻게 정적인 데이터 학습을 넘어 동적인 감각운동 법칙을 학습하는 방향으로 진화하고 있는지 탐구할 것이다.</p>
<h2>2.  내부 재현(Internal Representation)의 신화와 그 해체</h2>
<h3>2.1  “거대한 환상(Grand Illusion)”</h3>
<p>전통적인 계산주의 마음 이론(Computational Theory of Mind)은 뇌를 정보를 처리하여 상세한 내부 모델을 만드는 기계로 보았다. 우리는 눈을 뜨면 즉시 전방의 모든 풍경을 고해상도로 인식한다고 느낀다. 그러나 신경과학적 사실은 이와 다르다. 인간의 망막에서 고해상도 색상 정보를 처리할 수 있는 중심와(fovea)는 시야의 극히 일부(약 1~2도)에 불과하며, 나머지 주변 시야는 흑백에 가까운 저해상도 정보만을 제공한다. 그럼에도 불구하고 우리가 세상을 연속적이고 풍부한 디테일로 가득 찬 것으로 경험하는 이유는 무엇인가?</p>
<p>O’Regan과 Noë는 이를 **“거대한 환상(Grand Illusion)”**이라 칭하며, 뇌 속에 완벽한 3차원 지도가 존재한다는 가정을 거부한다.2 대신, 그들은 **“세계 자체가 외부 기억(Outside Memory)으로 작용한다”**는 대안을 제시한다. 로봇이나 인간은 모든 시각적 정보를 뇌(메모리)에 저장할 필요가 없다. 단지 필요할 때마다 눈을 움직여(saccade) 해당 정보를 ’샘플링’할 수 있는 능력과 그 가능성만 있으면 된다. 즉, 우리가 느끼는 시각적 풍요로움은 현재 뇌에 활성화된 정보의 양에서 오는 것이 아니라, 우리가 언제든 접근할 수 있다는 **잠재적 접근 가능성(Accessibility)**에 대한 확신에서 온다.</p>
<h3>2.2  변화맹(Change Blindness)과 로봇 시각의 효율성</h3>
<p>이 이론을 뒷받침하는 강력한 증거가 바로 <strong>변화맹(Change Blindness)</strong> 현상이다. 실험자가 피험자의 눈이 움직이는 순간(saccade)이나 화면이 깜빡이는 순간에 큰 물체를 사라지게 하거나 색깔을 바꾸면, 피험자는 이를 전혀 알아차리지 못하는 경우가 빈번하다.1 만약 뇌가 화면 전체의 상세한 사본을 유지하고 있다면 이러한 변화를 놓칠 리 없다. 이는 인간이 세상을 픽셀 단위로 저장하지 않으며, 현재의 작업(Task)과 관련된 감각운동 정보에만 주의(Attention)를 기울인다는 것을 시사한다.</p>
<p>로봇 공학적 관점에서 이는 엄청난 계산 효율성의 기회를 제공한다. 모든 프레임을 픽셀 단위로 완벽하게 복원하거나 추적하려는 시도는 불필요하며 비효율적이다. 대신 로봇은 현재 수행 중인 행동(예: 문고리 잡기)에 필요한 감각운동 유관성(손의 위치와 문고리의 상대적 거리 변화)에만 집중하면 된다. 이는 최근의 주의 기반(Attention-based) 비전 모델들이 전체 이미지가 아닌 중요한 패치(Patch)나 토큰(Token)에 집중하는 방식과 일맥상통한다.5</p>
<h2>3.  감각운동 유관성(SMC)의 구조와 원리</h2>
<p>감각운동 유관성은 감각 입력(<span class="math math-inline">S</span>), 행위자의 행동(<span class="math math-inline">A</span>), 그리고 환경(<span class="math math-inline">E</span>) 사이의 법칙적 관계(<span class="math math-inline">f</span>)로 정의된다. 이를 수식으로 표현하면 다음과 같다.<br />
<span class="math math-display">
S_{t+1} = f(S_t, A_t, E)
</span><br />
여기서 핵심은 특정 시점의 감각 입력 <span class="math math-inline">S_t</span> 그 자체가 아니라, 행동 <span class="math math-inline">A_t</span>를 취했을 때 감각 <span class="math math-inline">S</span>가 어떻게 변하는지에 대한 **변화의 규칙성(Regularity)**이다.1</p>
<h3>3.1  감각적 유관성 vs. 지각적 유관성</h3>
<p>SMC는 크게 두 가지 층위로 나뉘며, 로봇은 이 두 가지를 구분하여 학습해야 한다.7</p>
<table><thead><tr><th><strong>구분</strong></th><th><strong>정의</strong></th><th><strong>예시</strong></th><th><strong>로봇 공학적 함의</strong></th></tr></thead><tbody>
<tr><td><strong>감각적 유관성</strong> (Sensory SMCs)</td><td>시각 기관(센서) 자체의 물리적 특성에 의해 결정되는 법칙. 대상의 속성과는 무관하다.</td><td>눈을 깜빡이면 어두워진다. 눈을 누르면 시야가 흔들린다. 카메라 렌즈의 왜곡.</td><td><strong>신체 스키마(Body Schema):</strong> 로봇은 자신의 센서 노이즈, 모터의 한계, 카메라의 왜곡 보정 등을 학습하며 ’자아’의 경계를 확립한다.</td></tr>
<tr><td><strong>지각적 유관성</strong> (Perceptual SMCs)</td><td>대상의 속성(모양, 색상, 위치)에 의해 결정되는 법칙.</td><td>정육면체를 볼 때 고개를 옆으로 돌리면 면의 모양이 왜곡되지만 위상학적 구조는 유지된다.</td><td><strong>객체 인식(Object Recognition):</strong> 물체란 ’정적인 속성’이 아니라, 내가 다가가면 커지고, 돌리면 다른 면이 보이는 ’변화의 법칙 덩어리’이다.</td></tr>
</tbody></table>
<h3>3.2  포르쉐(Porsche) 비유와 감각의 질(Qualia)</h3>
<p>O’Regan은 감각 경험의 본질을 설명하기 위해 **“포르쉐 운전”**의 비유를 사용한다.9 포르쉐를 운전한다는 느낌(Feel)은 차가 고속으로 달릴 때뿐만 아니라, 신호 대기 중 정지해 있을 때조차 존재한다. 이는 운전자가 “내가 지금 가속 페달을 밟으면 차가 폭발적으로 튀어 나갈 것이다” 혹은 “핸들을 꺾으면 차체가 단단하게 반응할 것이다“라는 **잠재적 반응성(Potential Contingency)**을 인지하고 있기 때문이다.</p>
<p>로봇에게 있어서도 마찬가지다. 로봇이 ’벽’을 인식한다는 것은 단순히 깊이 센서(LiDAR)에 평평한 점들이 찍히는 것을 의미하지 않는다. 그것은 “내가 전진하면 충돌 신호가 발생할 것(Future Collision)“이고, “더 이상 전진할 수 없다(Unpassable)“는 행동적 제약을 예측하는 상태를 의미한다. 즉, 인식은 현재의 감각 입력뿐만 아니라 **미래의 가능한 감각-행동 시나리오(Counterfactual Richness)**를 포함한다.6</p>
<h2>4.  능동적 지각(Active Perception)과 예측적 처리</h2>
<p>SMC 이론은 현대 신경과학의 <strong>능동적 추론(Active Inference)</strong> 및 <strong>예측적 부호화(Predictive Coding)</strong> 프레임워크와 결합하여 로봇 제어의 새로운 패러다임을 제시한다.</p>
<h3>4.1  수동적 감지 대 능동적 지각</h3>
<p>수동적 감지(Passive Sensing)는 고정된 센서로 데이터를 수집하는 방식이다. 반면, **능동적 지각(Active Perception)**은 불확실성을 줄이거나 목표를 달성하기 위해 센서의 파라미터를 조절하거나 신체를 이동시키는 제어 루프를 포함한다.10</p>
<ul>
<li><strong>Held와 Hein의 Kitten Carousel 실험:</strong> 1963년의 이 고전적 실험은 신체성의 중요성을 극명하게 보여준다. 스스로 걸으며 시각 정보를 얻은 능동적인 고양이는 정상적인 깊이 지각을 발달시켰지만, 곤돌라에 타고 수동적으로 움직이며 동일한 시각적 자극을 받은 고양이는 시각 장애를 겪었다.12</li>
<li><strong>로봇 공학적 교훈:</strong> 고정된 데이터셋(ImageNet 등)으로 학습한 AI 모델이 실제 로봇에 이식되었을 때 성능이 저하되는 이유가 여기에 있다. 데이터셋에는 “내가 움직였기 때문에 이미지가 변했다“라는 인과적 연결 고리가 결여되어 있다. 로봇이 진정한 시각을 갖기 위해서는 자신의 모터 명령(Efference Copy)과 시각적 변화 사이의 상관관계를 학습하는 과정이 필수적이다.</li>
</ul>
<h3>4.2  자유 에너지 원리(Free Energy Principle)와 SMC</h3>
<p>Karl Friston의 자유 에너지 원리에 따르면, 지능형 에이전트는 자신의 예측 모델과 실제 감각 입력 사이의 괴리(Surprise)를 최소화하려는 경향이 있다.6 이를 수식으로 나타내면 변분 자유 에너지 <span class="math math-inline">F</span>를 최소화하는 문제로 귀결된다.<br />
<span class="math math-display">
F \approx -\ln p(o \vert m)
</span><br />
여기서 <span class="math math-inline">o</span>는 관측(Sensory observation), <span class="math math-inline">m</span>은 에이전트의 내부 모델이다. 로봇은 두 가지 방법으로 이 <span class="math math-inline">F</span>를 줄일 수 있다.</p>
<ol>
<li><strong>지각적 학습(Perceptual Learning):</strong> 내부 모델 <span class="math math-inline">m</span>을 수정하여 관측 <span class="math math-inline">o</span>를 더 잘 설명하도록 한다. (세계관의 수정)</li>
<li><strong>능동적 추론(Active Inference):</strong> 행동 <span class="math math-inline">a</span>를 취하여 관측 <span class="math math-inline">o</span>를 자신의 예측(선호하는 상태)에 맞추도록 환경을 변화시킨다. (세계의 수정)</li>
</ol>
<p>이 프레임워크에서 SMC를 마스터한다는 것은, 행동 <span class="math math-inline">a</span>가 관측 <span class="math math-inline">o</span>에 미칠 영향을 정확히 예측하는 <strong>생성 모델(Generative Model)</strong> <span class="math math-inline">P(o \vert s, a)</span>를 학습하는 것과 동치이다.13 로봇이 팔을 뻗었을 때(proprioception) 시야에서 손이 어떻게 보일지(vision) 예측하고, 그 예측이 빗나갔을 때(Prediction Error) 즉각적으로 궤적을 수정하는 과정 자체가 바로 ’지각’이며 ’행동’이다.</p>
<h2>5.  로봇을 위한 SOTA 인공지능: SMC의 공학적 구현</h2>
<p>SMC 이론은 단순한 철학적 논의를 넘어, 최신 딥러닝 기반 로봇 제어 기술의 핵심 아키텍처로 자리 잡고 있다. 특히 **월드 모델(World Models)**과 <strong>자기지도 학습(Self-Supervised Learning)</strong> 분야에서 SMC의 원리는 명시적으로 구현된다.</p>
<h3>5.1  월드 모델(World Models): 내부 시뮬레이터로서의 SMC</h3>
<p>David Ha와 Jürgen Schmidhuber가 제안한 월드 모델15은 에이전트가 환경의 압축된 시공간 표현(Spatio-temporal representation)을 학습하고, 이를 바탕으로 행동의 결과를 ’상상(Dreaming)’해볼 수 있게 한다. 이 모델은 크게 세 부분으로 구성된다.</p>
<ol>
<li><strong>비전 모델 (V, VAE):</strong> 고차원의 시각적 입력 <span class="math math-inline">O_t</span>를 저차원의 잠재 벡터 <span class="math math-inline">z_t</span>로 압축한다. 이는 픽셀 공간이 아닌 특징 공간(Feature Space)에서 SMC를 학습하기 위함이다.</li>
<li><strong>메모리 모델 (M, RNN/LSTM):</strong> 현재 상태 <span class="math math-inline">z_t</span>와 행동 <span class="math math-inline">a_t</span>를 입력받아 다음 상태 <span class="math math-inline">z_{t+1}</span>의 확률 분포를 예측한다. 이 <span class="math math-inline">P(z_{t+1} \vert z_t, a_t)</span>가 바로 <strong>학습된 SMC</strong>이다.</li>
<li><strong>컨트롤러 (C):</strong> <span class="math math-inline">z_t</span>와 <span class="math math-inline">h_t</span>(히든 스테이트)를 기반으로 최적의 행동 <span class="math math-inline">a_t</span>를 결정한다.</li>
</ol>
<p>이 구조에서 로봇은 물리적 세계에서 위험한 시행착오를 겪기 전에, 자신의 내부 SMC 모델(꿈) 속에서 수천 번의 시뮬레이션을 수행하며 정책을 최적화할 수 있다. 이는 인간이 머릿속으로 리허설을 하는 것과 유사하며, SMC 이론이 말하는 “미래의 감각운동 가능성 예측“을 구체화한 것이다.</p>
<h3>5.2  JEPA (Joint-Embedding Predictive Architecture)</h3>
<p>Yann LeCun은 생성형 AI가 픽셀 단위의 예측(Generative Reconstruction)에 집중하는 것을 비판하며, <strong>JEPA 아키텍처</strong>를 제안했다.18</p>
<ul>
<li><strong>픽셀 생성의 비효율성:</strong> 잔디밭을 걷는 로봇에게 중요한 정보는 “앞에 잔디가 있다“는 사실이지, 잔디 잎 하나하나의 정확한 위치가 아니다. 픽셀 단위로 미래를 예측하는 모델(Video Generation)은 불필요한 디테일을 처리하느라 막대한 계산 자원을 소모한다.</li>
<li><strong>추상적 표현 공간에서의 예측:</strong> JEPA는 입력 <span class="math math-inline">x</span>와 <span class="math math-inline">y</span>를 각각 인코더를 통해 잠재 표현 <span class="math math-inline">s_x, s_y</span>로 변환하고, 예측기(Predictor)가 잠재 공간 내에서 <span class="math math-inline">s_y</span>를 예측하도록 학습한다.</li>
<li><strong>I-JEPA (Image-JEPA):</strong> 이미지의 가려진 부분을 문맥을 통해 예측한다.</li>
<li><strong>V-JEPA (Video-JEPA):</strong> 비디오의 시간적 전개를 예측하여 물리 법칙(중력, 관성, 충돌 등)을 학습한다.21</li>
</ul>
<p>V-JEPA는 **시공간적 SMC(Spatiotemporal SMC)**를 학습하는 가장 진보된 형태이다. 로봇은 이 모델을 통해 자신의 행동이 환경의 *의미론적 상태(Semantic State)*를 어떻게 변화시킬지 학습한다. O’Regan이 “세부 사항을 뇌에 저장하지 않는다“고 했던 것처럼, JEPA는 예측 불가능한 세부 사항(노이즈)은 버리고, 행동과 인과적으로 연결된 특징만을 학습하여 효율적인 세계 모델을 구축한다.</p>
<h3>5.3  시각-운동 정책(Visuomotor Policy) 학습과 Diffusion Models</h3>
<p>최근 로봇 제어의 SOTA는 딥러닝 모델이 카메라 이미지에서 직접 모터 명령을 출력하는 <strong>End-to-End Visuomotor Policy</strong> 학습이다.22</p>
<ul>
<li><strong>RT-1, RT-2 (Robotic Transformers):</strong> 구글의 RT 시리즈는 언어 모델의 트랜스포머 아키텍처를 차용하여, 시각 토큰과 언어 토큰을 입력받아 행동 토큰(Action Token)을 생성한다.24 여기서 ’행동’은 또 다른 형태의 언어처럼 취급되며, 로봇은 수많은 데모 데이터를 통해 “이미지 <span class="math math-inline">A</span> 상황에서 텍스트 명령 <span class="math math-inline">B</span>가 주어지면 행동 <span class="math math-inline">C</span>가 뒤따른다“는 확률적 유관성을 학습한다.</li>
<li><strong>Diffusion Policy:</strong> 이미지 생성 모델에 쓰이던 확산 모델(Diffusion Model)을 로봇 행동 생성에 적용한 사례다. 이는 복잡하고 멀티모달(Multimodal)한 행동 분포를 표현하는 데 탁월하며, 로봇이 불확실한 상황에서 여러 가지 가능한 SMC 시나리오 중 하나를 부드럽게 선택할 수 있게 한다.</li>
</ul>
<h2>6.  지각과 행동의 불가분성: 심볼 그라운딩 문제의 해결책</h2>
<p>1.3.1절에서 제기된 심볼 그라운딩 문제(Symbol Grounding Problem)는 “기호(Symbol)가 어떻게 물리적 의미를 갖는가?“에 대한 질문이었다. SMC는 이에 대해 **“의미는 상호작용 속에 있다”**는 답을 제시한다.</p>
<h3>6.1  어포던스(Affordance)로서의 객체 인식</h3>
<p>SMC 관점에서 ’컵(Cup)’이라는 심볼은 3D CAD 모델이나 사전적 정의가 아니다. 그것은 “손을 뻗어 잡을 수 있음(Graspable)”, “액체를 담을 수 있음(Containable)”, “기울이면 쏟아짐(Spillable)“과 같은 일련의 **감각운동 잠재성(Sensorimotor Potentialities)**의 집합이다.25 로봇이 컵을 인식한다는 것은 컵의 픽셀을 분류하는 것이 아니라, 컵에 대해 수행할 수 있는 행동의 레퍼토리(Repertoire)와 그 결과를 예측할 수 있다는 뜻이다.</p>
<p>J.J. Gibson의 생태학적 심리학(Ecological Psychology)에서 유래한 어포던스(Affordance) 개념은 SMC 이론과 완벽하게 조응한다. 로봇에게 있어 지각은 ’무엇(What)’을 아는 것이 아니라, ‘어떻게(How)’ 상호작용할 수 있는지를 아는 것이다.</p>
<h3>6.2  광학 흐름(Optical Flow)과 자가 운동</h3>
<p>정적인 이미지 한 장에는 속도나 힘에 대한 정보가 없다. 그러나 연속된 프레임 간의 픽셀 이동인 **광학 흐름(Optical Flow)**은 로봇의 자가 운동(Egomotion)과 물체의 움직임을 분리해내는 핵심적인 감각운동 단서(Cue)가 된다.26</p>
<ul>
<li><strong>시각적 안정성(Visual Stability):</strong> 우리가 눈을 빠르게 움직여도 세상이 흔들려 보이지 않는 이유는, 뇌가 눈의 움직임 신호(Efference Copy)를 미리 받아 예상되는 시각적 변화를 상쇄(Cancel out)하기 때문이다.</li>
<li><strong>로봇 구현:</strong> 로봇의 SLAM(Simultaneous Localization and Mapping) 알고리즘은 카메라가 이동할 때 특징점들이 어떻게 이동할지를 예측하고, 관측된 오차를 통해 자신의 위치를 역추적한다. 이는 “나의 움직임이 시각적 입력을 변화시킨다“는 SMC의 기본 공식을 수학적으로 구현한 것이다. 최신의 <strong>Neural 3D Video Processing</strong> 기술은 이 흐름(Flow) 정보를 3차원 공간으로 확장하여, 동적인 환경에서도 강인한 인식을 가능하게 한다.26</li>
</ul>
<h2>7.  결론: 뇌가 아닌 몸으로 배우는 지능</h2>
<p>본 절을 통해 우리는 지각(Perception)과 행동(Action)이 서로 분리된 모듈이 아니라, 동전의 양면처럼 얽혀 있는 단일한 프로세스임을 확인했다. “보는 것은 곧 행동하는 것이다“라는 명제는 다음과 같은 엠바디드 AI의 핵심 원칙들을 도출해낸다.</p>
<ol>
<li><strong>지각은 예측이다 (Perception as Prediction):</strong> 로봇은 수동적으로 데이터를 받아들이지 않고, 자신의 행동이 초래할 감각적 변화를 끊임없이 예측(Active Inference)하며 세상을 이해한다.</li>
<li><strong>세계는 외부 기억이다 (World as Outside Memory):</strong> 복잡한 내부 3D 모델을 유지하는 대신, 로봇은 필요할 때마다 환경과 상호작용하여 정보를 추출하는 감각운동 기술(Skill)을 마스터해야 한다. 이는 계산 비용을 획기적으로 줄이고 실시간성을 확보하는 열쇠다.</li>
<li><strong>지능은 신체에서 나온다 (Intelligence from Embodiment):</strong> 센서의 위치, 팔의 길이, 모터의 반응 속도와 같은 신체적 제약(Constraints) 자체가 지능을 구성하는 감각운동 유관성의 기본 법칙을 형성한다. 신체가 다르면 지능의 형태도 달라진다.</li>
</ol>
<p>이러한 이해는 “뇌만 있으면 지능이 생긴다“는 고전적 AI의 망상을 깨트린다. 인터넷상의 텍스트만 학습한 거대 언어 모델(LLM)이 아무리 유창하게 말을 하더라도, 그것은 진정한 의미에서 ’사과’의 맛이나 무게를 알 수 없다. 왜냐하면 그들에게는 사과를 씹거나 들어 올릴 때 발생하는 감각운동 유관성의 경험이 결여되어 있기 때문이다.</p>
<p>진정한 일반 인공지능(AGI)으로 나아가기 위해서는, 지능이 추상적인 기호 처리가 아니라 구체적인 신체적 상호작용에 뿌리를 두고 있음을 인정해야 한다. 이제 우리는 이 감각운동 유관성이 어떻게 더 고차원적인 인지 기능으로 확장되는지, 그리고 왜 신체가 지능의 필수 전제 조건이 되는지 다음 절 **“1.3.3 지능의 전제 조건”**에서 더욱 깊이 있게 다루게 될 것이다. SOTA 로봇 기술은 이제 정적인 데이터셋 학습을 넘어, 환경과의 동적 상호작용을 통해 세상의 인과 구조를 스스로 깨우치는 에이전트를 향해 나아가고 있다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Sensorimotor theory of consciousness - Scholarpedia, http://www.scholarpedia.org/article/Sensorimotor_theory_of_consciousness</li>
<li>On the Brain-Basis of Visual Consciousness: A Sensorimotor Account, <a href="http://ondrej.becev.cz/upload//No%C3%AB-and-J.-Kevin-O%E2%80%99Regan-On-the-Brain-Basis-of-Visual-Consciousness.pdf">http://ondrej.becev.cz/upload//No%C3%AB-and-J.-Kevin-O%E2%80%99Regan-On-the-Brain-Basis-of-Visual-Consciousness.pdf</a></li>
<li>A sensorimotor account of vision and visual consciousness - PubMed, https://pubmed.ncbi.nlm.nih.gov/12239892/</li>
<li>Kevin O’Regan: The sensorimotor approach to understanding “feel” in humans and robots, https://www.youtube.com/watch?v=w4BsWmxu3dI</li>
<li>[2005.09161] Spatiotemporal Attacks for Embodied Agents - arXiv, https://arxiv.org/abs/2005.09161</li>
<li>The cybernetic Bayesian brain: from interoceptive inference to sensorimotor contingencies - Figshare, https://sussex.figshare.com/articles/chapter/The_cybernetic_Bayesian_brain_from_interoceptive_inference_to_sensorimotor_contingencies/23419385/1/files/41137964.pdf</li>
<li>A sensorimotor account of vision and visual … - J. Kevin O’Regan, http://nivea.psycho.univ-paris5.fr/OREGAN-NOE-BBS/ORegan;Noe.BBS.pdf</li>
<li>A sensorimotor account of vision and visual consciousness - J. Kevin O’Regan, <a href="http://nivea.psycho.univ-paris5.fr/BBS/VisCons.html%20-%20copie">http://nivea.psycho.univ-paris5.fr/BBS/VisCons.html%20-%20copie</a></li>
<li>What it is like to see: A sensorimotor theory of perceptual experience - J. Kevin O’Regan, http://nivea.psycho.univ-paris5.fr/Synthese/MyinFinal.html</li>
<li>Active touch for robust perception under position uncertainty - IEEE Xplore, https://ieeexplore.ieee.org/document/6630996/</li>
<li>Active tactile perception - Scholarpedia, http://www.scholarpedia.org/article/Active_tactile_perception</li>
<li>Overview of Robot Perception - UT Austin Computer Science, https://www.cs.utexas.edu/~yukez/cs391r_fall2020/slides/lecture_robot_perception.pdf</li>
<li>Active inference and robot control: a case study - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC5046960/</li>
<li>The Active Inference Approach to Ecological Perception: General Information Dynamics for Natural and Artificial Embodied Cognition - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2018.00021/full</li>
<li>[1803.10122] World Models - arXiv, https://arxiv.org/abs/1803.10122</li>
<li>World Models, https://worldmodels.github.io/</li>
<li>Recurrent World Models Facilitate Policy Evolution - NIPS papers, https://papers.nips.cc/paper/7512-recurrent-world-models-facilitate-policy-evolution</li>
<li>Deep Dive into Yann LeCun’s JEPA - Rohit Bandaru, https://rohitbandaru.github.io/blog/JEPA-Deep-Dive/</li>
<li>A Path Towards Autonomous Machine Intelligence Version 0.9.2, 2022-06-27 - OpenReview, https://openreview.net/pdf?id=BZ5a1r-kVsf</li>
<li>Yann LeCun | Self-Supervised Learning, JEPA, World Models, and the future of AI, https://www.youtube.com/watch?v=yUmDRxV0krg</li>
<li>Beyond Next-Token Prediction: Yann LeCun’s JEPA and the Quest for AI Common Sense — Where Everything Is an Abstraction | by İlyurek Kılıç - Medium, https://medium.com/@ilyurek/beyond-next-token-prediction-yann-lecuns-jepa-and-the-quest-for-ai-common-sense-where-92150bed9dfd</li>
<li>View Synthesis for Visuomotor Policy Learning - DSpace@MIT, https://dspace.mit.edu/handle/1721.1/152632</li>
<li>Sensorimotor Robot Policy Training using Reinforcement Learning - DiVA portal, https://www.diva-portal.org/smash/get/diva2:1208897/FULLTEXT01.pdf</li>
<li>Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning - arXiv, https://arxiv.org/html/2508.10399v1</li>
<li>SEAL: Self-supervised Embodied Active Learning using Exploration and 3D Consistency, https://proceedings.neurips.cc/paper/2021/file/6d0c932802f6953f70eb20931645fa40-Paper.pdf</li>
<li>Visuomotor Policy Learning via Generating Flow in 3D Space - arXiv, https://arxiv.org/html/2509.18676</li>
<li>Socializing Sensorimotor Contingencies - Frontiers, https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2021.624610/full</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>