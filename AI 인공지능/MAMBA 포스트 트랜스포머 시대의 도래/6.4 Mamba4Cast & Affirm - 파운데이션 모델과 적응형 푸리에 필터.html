<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:6.4 Mamba4Cast & Affirm - 파운데이션 모델과 적응형 푸리에 필터</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>6.4 Mamba4Cast & Affirm - 파운데이션 모델과 적응형 푸리에 필터</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">MAMBA - 포스트 트랜스포머 시대의 도래</a> / <span>6.4 Mamba4Cast & Affirm - 파운데이션 모델과 적응형 푸리에 필터</span></nav>
                </div>
            </header>
            <article>
                <h1>6.4 Mamba4Cast &amp; Affirm - 파운데이션 모델과 적응형 푸리에 필터</h1>
<p>현대 시계열 예측(Time Series Forecasting) 기술은 트랜스포머(Transformer) 아키텍처가 주도해 온 지난 몇 년간의 성과를 바탕으로 새로운 전환점을 맞이하고 있다. 트랜스포머는 전역적인 문맥(Global Context)을 포착하는 데 탁월한 능력을 보였으나, 입력 시퀀스의 길이가 길어질수록 연산 비용과 메모리 사용량이 2차 함수적(<span class="math math-inline">O(L^2)</span>)으로 증가한다는 근본적인 한계에 직면해 있다. 이는 금융 데이터의 초고빈도 거래나 기상 데이터의 장기 기후 예측과 같이 방대한 과거 데이터를 필요로 하는 도메인에서 심각한 병목 현상을 초래한다. 이러한 배경 속에서, 선형 복잡도(<span class="math math-inline">O(L)</span>)를 유지하면서도 장기 의존성(Long-term Dependency)을 효과적으로 모델링할 수 있는 상태 공간 모델(State Space Models, SSM), 특히 Mamba 아키텍처가 차세대 시계열 모델링의 핵심 대안으로 급부상하고 있다.1</p>
<p>본 절에서는 Mamba 아키텍처를 기반으로 시계열 데이터의 본질적인 특성을 포착하고 예측 성능을 극대화하기 위해 고안된 두 가지 혁신적인 모델, <strong>Mamba4Cast</strong>와 <strong>Affirm</strong>을 심층적으로 분석한다. Mamba4Cast는 실제 데이터 없이 오직 합성 데이터(Synthetic Data)만으로 학습된 제로샷(Zero-shot) 파운데이션 모델로서, 데이터 희소성 문제를 해결하고 범용적인 예측 능력을 확보하는 데 중점을 둔다. 반면, Affirm(Adaptive Fourier Filter Interactive Mamba)은 주파수 영역(Frequency Domain)에서의 적응형 필터링과 Mamba의 상호작용 메커니즘을 결합하여, 노이즈가 많은 실제 데이터 환경에서 강건한 장기 예측 성능을 달성하는 것을 목표로 한다. 이 두 모델은 ’데이터 중심(Data-centric)의 일반화’와 ’모델 중심(Model-centric)의 정밀화’라는 상이한 접근 방식을 취하고 있으며, 이를 통해 시계열 예측 분야의 기술적 지평을 확장하고 있다.</p>
<h2>1.  시계열 예측의 새로운 패러다임: Mamba와 선형 복잡도</h2>
<p>본격적인 모델 분석에 앞서, Mamba 아키텍처가 시계열 예측에서 갖는 함의를 이해할 필요가 있다. 기존의 순환 신경망(RNN)은 순차적인 정보 처리에 효율적이었으나 장기 기억 소실 문제와 병렬 처리의 불가능함이라는 단점을 가졌다. 반면 트랜스포머는 병렬 처리가 가능하고 장기 의존성을 잘 포착하지만, 긴 시퀀스에 대한 계산 효율성이 떨어진다. Mamba는 이 두 가지 접근법의 장점을 결합한 선택적 상태 공간 모델(Selective SSM)을 기반으로 한다.3</p>
<p>Mamba의 핵심은 입력 데이터의 내용에 따라 상태 공간의 파라미터를 동적으로 변화시키는 ’선택 메커니즘(Selection Mechanism)’에 있다. 이는 시계열 데이터에서 무의미한 노이즈를 필터링하고, 예측에 중요한 변곡점이나 추세 정보를 선택적으로 기억하여 장기간 유지할 수 있게 한다. 특히 Mamba2 아키텍처는 행렬 곱셈 연산에 최적화된 구조(SSD: Structured State Space Duality)를 도입하여 학습 및 추론 속도를 획기적으로 개선하였다.1 이러한 특성은 수천, 수만 타임스텝 이상의 과거 데이터를 참조해야 하는 장기 시계열 예측 과제에서 트랜스포머 대비 압도적인 효율성을 제공한다.</p>
<h3>1.1 표 6.4.1 트랜스포머와 Mamba 아키텍처의 시계열 예측 특성 비교</h3>
<table><thead><tr><th><strong>특징</strong></th><th><strong>트랜스포머 (Transformers)</strong></th><th><strong>맘바 (Mamba/SSM)</strong></th><th><strong>시계열 예측에서의 함의</strong></th></tr></thead><tbody>
<tr><td><strong>계산 복잡도</strong></td><td><span class="math math-inline">O(L^2)</span> (2차 함수적)</td><td><span class="math math-inline">O(L)</span> (선형)</td><td>Mamba는 매우 긴 과거 데이터(Look-back window)를 효율적으로 처리 가능.3</td></tr>
<tr><td><strong>메커니즘</strong></td><td>자기 어텐션 (Self-Attention)</td><td>선택적 상태 공간 (Selective SSM)</td><td>Mamba는 노이즈가 많은 시계열에서 중요한 신호만 선별적으로 기억하는 데 유리함.</td></tr>
<tr><td><strong>추론 방식</strong></td><td>KV 캐시 필요, 메모리 증가</td><td>고정된 크기의 상태(State) 유지</td><td>실시간 예측 및 엣지 디바이스 배포 시 Mamba가 메모리 효율성 측면에서 우월함.</td></tr>
<tr><td><strong>주요 모델</strong></td><td>PatchTST, iTransformer, Chronos</td><td>Mamba4Cast, Affirm, TimeMachine</td><td>트랜스포머는 전역 문맥 파악에, Mamba는 효율적이고 연속적인 동역학 모델링에 강점.</td></tr>
</tbody></table>
<h2>2.  Mamba4Cast: 합성 데이터 기반의 제로샷 파운데이션 모델</h2>
<p>Mamba4Cast는 “시계열 예측을 위한 파운데이션 모델이 반드시 실제 데이터를 학습해야 하는가?“라는 근본적인 질문에 답을 제시한다. 기존의 시계열 파운데이션 모델들(예: Chronos, Moirai)이 수백만 개의 실제 시계열 데이터를 수집하여 학습한 것과 달리, Mamba4Cast는 전적으로 합성 데이터(Synthetic Data)만을 사용하여 학습되었음에도 불구하고 실제 데이터에 대해 놀라운 제로샷 성능을 보여준다.1 이는 데이터 프라이버시 문제, 도메인 특화 데이터의 부족, 그리고 데이터 수집 비용 문제를 해결할 수 있는 획기적인 접근법이다.</p>
<h3>2.1  아키텍처 심층 분석: 효율성을 위한 설계</h3>
<p>Mamba4Cast의 아키텍처는 약 2,700만(27M) 개의 파라미터를 가지며, 이는 Chronos-Mini(20M)보다 크고 Chronos-Small(46M)보다 작은 규모이다. 그러나 단일 패스(Single-pass) 예측 메커니즘을 통해 훨씬 빠른 추론 속도를 달성한다.1</p>
<ol>
<li>입력 전처리 및 임베딩 (Input Pre-processing &amp; Embedding)</li>
</ol>
<p>입력된 시계열 데이터는 먼저 Min-Max Scaler를 통해 정규화되며, 시간적 맥락을 제공하기 위해 월, 일, 요일, 시간 등의 타임스탬프 정보가 사인/코사인 기반의 위치 임베딩(Positional Embedding)으로 변환된다. 이 스케일링된 값과 시간 특징(Time features)은 결합되어 초기 112차원의 벡터를 형성한다.7</p>
<ol start="2">
<li>다중 스케일 인과적 합성곱 (Multi-scale Causal Convolutions)</li>
</ol>
<p>단순한 선형 투영 대신, Mamba4Cast는 4개의 인과적 1D 합성곱(Causal Conv1d) 계층을 스택으로 쌓아 입력 토큰을 처리한다. 각 계층은 커널 크기 5를 가지며, 팽창(Dilation) 비율이 1, 2, 4, 8로 기하급수적으로 증가한다.</p>
<ul>
<li><strong>의의:</strong> 팽창된 합성곱은 모델이 단기적인 변동(High frequency)부터 장기적인 추세(Low frequency)까지 다양한 시간적 수용 영역(Receptive Field)을 동시에 확보하게 한다. 이는 시계열 데이터의 다중 스케일 특성을 초기에 효과적으로 포착하는 역할을 한다.</li>
<li>이후 인셉션(Inception) 계층이 각기 다른 스케일에서 추출된 특징들을 통합하고, 최종적으로 1024차원의 임베딩 공간으로 투영한다.1</li>
</ul>
<ol start="3">
<li>Mamba2 인코더 블록 (Mamba2 Encoder Blocks)</li>
</ol>
<p>핵심 연산은 2개의 Mamba2 블록에서 이루어진다. 여기서 상태 확장 계수(State Expansion Factor, N)는 128, 블록 확장 계수(Block Expansion Factor, E)는 2로 설정된다.</p>
<ul>
<li><strong>메커니즘:</strong> Mamba2 블록은 입력 시퀀스를 처리하며 내부 상태 <span class="math math-inline">h_t</span>를 갱신한다. <span class="math math-inline">h_t = A h_{t-1} + B x_t</span>와 같은 상태 공간 방정식의 이산화된 형태를 따르며, 선택적 메커니즘을 통해 <span class="math math-inline">B</span>와 <span class="math math-inline">C</span> 파라미터가 입력에 따라 조절된다. 이를 통해 모델은 시계열의 동적 패턴을 학습하고, 불필요한 정보는 망각하며 중요한 정보는 장기 기억으로 보존한다.5</li>
</ul>
<ol start="4">
<li>단일 패스 예측 헤드 (Single-Pass Forecasting Head)</li>
</ol>
<p>Mamba4Cast의 가장 큰 기술적 차별점은 예측 방식에 있다. 기존의 언어 모델 기반 시계열 예측기(예: Chronos)는 자기회귀(Auto-regressive) 방식을 사용하여 미래 시점을 하나씩 순차적으로 예측한다. 이는 예측 구간(Horizon)이 길어질수록 추론 시간이 선형적으로 증가하는 단점이 있다.</p>
<ul>
<li><strong>혁신:</strong> Mamba4Cast는 전체 예측 구간을 단 한 번의 순전파(Forward pass)로 생성한다. 출력층의 선형 투영(Linear Projection) 레이어는 인코더의 잠재 표현을 곧바로 전체 예측 구간에 대한 점 예측(Point Forecast) 벡터로 변환한다.</li>
<li><strong>결과:</strong> 이러한 구조는 추론 속도를 획기적으로 향상시킨다. 실험 결과, Mamba4Cast는 배치 크기가 크고 예측 구간이 길수록 자기회귀 모델 대비 압도적인 속도 우위를 보였다.8</li>
</ul>
<h3>2.2  합성 데이터 생성 전략: 일반화를 위한 사전(Prior) 설계</h3>
<p>Mamba4Cast의 제로샷 성능은 모델 구조보다는 학습 데이터의 설계에서 기인한다고 볼 수 있다. 연구진은 실제 데이터의 패턴을 모사할 수 있는 두 가지 수학적 프레임워크를 결합하여 학습 데이터를 생성했다.5</p>
<ol>
<li>가우시안 프로세스(GP) 커널 (70% 비중)</li>
</ol>
<p>이 방식은 Chronos의 ‘KernelSynth’ 전략을 계승한다. 가우시안 프로세스는 커널 함수(Kernel Function)를 통해 데이터 포인트 간의 공분산을 정의함으로써 시계열을 생성한다.</p>
<ul>
<li><strong>구성:</strong> 선형(Linear), 주기(Periodic), RBF(Radial Basis Function) 등의 기본 커널들을 무작위로 결합(덧셈 또는 곱셈)하여 복잡한 시계열 패턴을 만들어낸다. 예를 들어, 선형 커널과 주기 커널의 합은 ’상승하는 계절성 데이터’를 모사한다.</li>
<li><strong>효과:</strong> GP 기반 데이터는 수학적으로 무한히 다양한 패턴을 생성할 수 있어, 모델이 특정 도메인에 과적합되지 않고 시계열의 일반적인 구조적 성질(연속성, 주기성, 추세성 등)을 학습하게 한다.5</li>
</ul>
<ol start="2">
<li>FPFN (ForecastPFN) Prior (30% 비중)</li>
</ol>
<p>나머지 30%는 Dooley et al. (2023)이 제안한 FPFN(Prior-data Fitted Networks) 방법론을 따른다.</p>
<ul>
<li><strong>구조적 분해:</strong> FPFN은 시계열을 <span class="math math-inline">y_t = \text{Trend}_t + \text{Seasonality}_t + \text{Noise}_t</span>와 같이 명시적인 구성 요소의 합으로 정의하여 생성한다. 이는 통계적 시계열 분석(STL Decomposition 등)에서 가정하는 전형적인 구조를 반영한다.</li>
<li><strong>역할:</strong> GP가 수학적 유연성을 제공한다면, FPFN은 현실 세계의 시계열 데이터가 갖는 전형적인 ’거시적 구조’를 모델이 익히도록 돕는다. 이러한 혼합 전략은 실제 데이터의 복잡성과 정형성을 동시에 커버하는 학습 환경을 제공한다.5</li>
</ul>
<h3>2.3  성능 평가 및 비교</h3>
<p>Mamba4Cast는 17개의 다양한 도메인(금융, 교통, 헬스케어 등) 데이터셋에서 제로샷 평가를 수행하였다. 결과적으로 Chronos-Small과 같은 트랜스포머 기반 파운데이션 모델과 대등하거나 일부 데이터셋에서 더 우수한 MASE(Mean Absolute Scaled Error) 성능을 기록하였다.5 특히 추론 효율성 측면에서, 긴 시퀀스 예측 시 트랜스포머 모델들이 겪는 병목 현상 없이 일정한 속도를 유지함으로써 실시간 애플리케이션에 대한 적합성을 입증하였다. 이는 ’합성 데이터만으로 학습된 선형 복잡도 모델’이 실제 세계의 복잡한 예측 문제를 해결할 수 있다는 강력한 증거가 된다.</p>
<h2>3.  Affirm: 적응형 푸리에 필터와 상호작용적 Mamba</h2>
<p>Mamba4Cast가 범용성과 효율성에 초점을 맞췄다면, **Affirm (Adaptive Fourier Filter Interactive Mamba)**은 시계열 데이터의 본질적인 어려움인 ’노이즈(Noise)’와 ’복잡한 시간 의존성’을 정밀하게 다루기 위해 고안되었다. 저장대학교 연구진이 제안한 이 모델은 시간 영역(Time Domain)의 정보를 주파수 영역(Frequency Domain)으로 변환하여 처리함으로써, 기존 트랜스포머 모델들이 겪는 노이즈 민감성과 과적합 문제를 구조적으로 해결한다.12</p>
<h3>3.1  주파수 영역 분석의 필요성</h3>
<p>시계열 데이터, 특히 센서 데이터나 주식 가격과 같은 장기 시계열은 신호(Signal)와 잡음(Noise)이 혼재되어 있다. 시간 영역에서 이를 처리할 경우, 모델은 무작위적인 변동(Random fluctuation)까지 학습하려다 과적합(Overfitting)되거나, 반대로 중요한 미세 변동을 노이즈로 치부해버릴 위험이 있다. Affirm은 푸리에 변환(Fourier Transform)을 통해 시계열을 주파수 성분으로 분해함으로써, 주기적인 패턴(저주파)과 급격한 노이즈(고주파)를 명확히 분리하고 각각에 맞는 처리를 수행한다.</p>
<h3>3.2  적응형 푸리에 필터 블록 (AFFB)의 수학적 원리</h3>
<p>Affirm의 핵심 구성 요소인 **AFFB (Adaptive Fourier Filter Block)**는 자기 어텐션 메커니즘을 대체하는 경량화된 연산자이다. 이 블록은 데이터를 주파수 영역으로 변환한 후, 학습 가능한 임계값을 사용하여 유의미한 주파수 성분만을 선별적으로 통과시킨다.13</p>
<ol>
<li>이산 푸리에 변환 (DFT) 및 스펙트럼 생성</li>
</ol>
<p>입력 시계열 <span class="math math-inline">x[n]</span> (길이 <span class="math math-inline">N</span>)은 먼저 고속 푸리에 변환(FFT)을 통해 주파수 영역의 표현 <span class="math math-inline">X[k]</span>로 변환된다.<br />
<span class="math math-display">
   X[k] = \sum_{n=0}^{N-1} x[n] W_N^{kn}, \quad W_N = e^{-j(2\pi/N)}
</span><br />
이 과정은 시간 축에서의 복잡한 대역적(Global) 상관관계를 주파수 축에서의 요소별 연산으로 단순화시켜, 계산 복잡도를 <span class="math math-inline">O(N^2)</span>에서 <span class="math math-inline">O(N \log N)</span>으로 낮춘다.</p>
<ol start="2">
<li>학습 가능한 적응형 임계값 (Learnable Adaptive Thresholding)</li>
</ol>
<p>AFFB는 고정된 필터(예: LPF, HPF)를 사용하는 대신, 데이터의 특성에 맞춰 스스로 임계값을 학습하는 적응형 필터링을 수행한다.</p>
<ul>
<li>
<p>적응형 저역 통과 필터 (Adaptive LPF): 시계열의 추세(Trend)와 주요 주기성을 보존하기 위해, 고주파 노이즈를 제거한다. 학습 가능한 임계값 <span class="math math-inline">\theta_{high}</span>를 사용하여 주파수 크기 <span class="math math-inline">|F|</span>가 이보다 작은 성분만을 통과시킨다.<br />
<span class="math math-display">
   X_{Filter}^{high} = X_F \odot \mathbb{I}(|F| \leq \theta_{high})
</span><br />
여기서 <span class="math math-inline">\odot</span>은 요소별 곱(Hadamard product)이며, <span class="math math-inline">\mathbb{I}(\cdot)</span>는 조건이 참일 때 1, 거짓일 때 0을 반환하는 지시 함수이다. <span class="math math-inline">\theta_{high}</span>는 역전파를 통해 최적의 값으로 자동 조정된다.</p>
</li>
<li>
<p><strong>적응형 고역 통과 필터 (Adaptive HPF):</strong> 반대로 저주파 성분을 제어하여 시계열의 국소적인 급격한 변화나 이벤트 정보를 포착한다.</p>
</li>
</ul>
<ol start="3">
<li>주파수 간 상호작용 및 복원</li>
</ol>
<p>필터링된 주파수 성분들은 단순히 더해지는 것이 아니라, <span class="math math-inline">1 \times 1</span> 합성곱 레이어(선형 변환)와 활성화 함수(ReLU)를 거치며 상호작용한다.<br />
<span class="math math-display">
   X_{mixed} = X_{Global} + M(X_{Filter}^{high}) + M(X_{Filter}^{low})
</span><br />
이 <span class="math math-inline">X_{mixed}</span>는 역 푸리에 변환(IFFT)을 통해 다시 시간 영역의 데이터 <span class="math math-inline">X_T</span>로 복원된다. 이 과정에서 시계열은 노이즈가 제거되고(De-noised), 주요 특징이 강조된(Refined) 형태로 재구성되어 다음 단계인 Mamba 블록으로 전달된다.13</p>
<h3>3.3  상호작용적 이중 Mamba 블록 (IDMB)의 구조와 기제</h3>
<p>시간 영역으로 복원된 특징은 **IDMB (Interactive Dual Mamba Block)**에서 처리된다. IDMB는 단일 Mamba 레이어를 사용하는 대신, 서로 다른 입도(Granularity)를 가진 정보를 병렬로 처리하고 상호 교환하는 이중 분기 구조를 채택하여 모델의 표현력을 극대화한다.13</p>
<ol>
<li>이중 분기 설계 (Dual Branch Architecture)</li>
</ol>
<p>IDMB는 입력 특징을 두 개의 경로로 분리하여 처리한다.</p>
<ul>
<li><strong>국소 분기 (Local Branch):</strong> <span class="math math-inline">2 \times 2</span> 깊이별 합성곱(Depth-wise Conv) 필터를 사용하여 미세하고 국소적인 시간 패턴을 포착한다. 이는 시계열의 순간적인 변동이나 짧은 주기의 패턴을 학습하는 데 특화된다.</li>
<li><strong>전역 분기 (Global Branch):</strong> <span class="math math-inline">4 \times 4</span> 깊이별 합성곱 필터를 사용하여 보다 넓은 범위의 전역적인 맥락을 파악한다. 이는 장기적인 추세나 거시적인 패턴을 학습한다.</li>
</ul>
<ol start="2">
<li>상호 변조 메커니즘 (Cross-Modulation Mechanism)</li>
</ol>
<p>각 분기는 독립적인 SSM(Mamba) 레이어를 통과한 후, 서로의 출력을 변조(Modulate)한다. 즉, 국소 정보가 전역 정보를, 전역 정보가 국소 정보를 제어하는 게이트 역할을 수행한다.<br />
<span class="math math-display">
   H_1 = \sigma(h_{2\times2}) \odot h_{4\times4} \odot \sigma(Linear(X_T))
   \\
   H_2 = \sigma(h_{4\times4}) \odot h_{2\times2} \odot \sigma(Linear(X_T))
</span><br />
이러한 상호작용은 모델이 숲(전역)과 나무(국소)를 동시에 고려하게 하며, 정보의 손실 없이 다중 스케일의 특징을 통합할 수 있게 한다. 최종적으로 두 분기의 결과는 합쳐져(<span class="math math-inline">H_1 + H_2</span>) 최종 예측을 위한 특징 벡터가 된다.</p>
<h3>3.4  Affirm의 성능 우위와 강건성</h3>
<p>Affirm은 ETT(Electricity Transformer Temperature), Electricity, Exchange 등 8개의 주요 벤치마크 데이터셋에서 실험되었으며, PatchTST, iTransformer, TimeMixer 등 최신 모델들을 상회하는 성능(MSE/MAE 기준)을 기록하였다.12</p>
<ul>
<li><strong>노이즈 강건성:</strong> 특히 노이즈 비율이 높은 데이터셋이나 작은 규모의 데이터셋에서 성능 저하가 적었다. 이는 AFFB의 주파수 기반 필터링이 실제 데이터의 불확실성을 효과적으로 제어함을 시사한다.</li>
<li><strong>효율성:</strong> Affirm은 트랜스포머 기반 모델 대비 훨씬 적은 메모리와 계산 자원을 사용한다. 이는 Mamba의 선형 복잡도와 FFT의 효율적인 연산(<span class="math math-inline">O(N \log N)</span>)이 결합된 결과로, 긴 시퀀스를 학습할 때 학습 시간이 단축되고 탄소 발자국이 감소하는 효과를 가져온다.</li>
</ul>
<h2>4.  비교 분석 및 시사점</h2>
<p>Mamba4Cast와 Affirm은 모두 Mamba 아키텍처를 기반으로 하지만, 시계열 예측 문제를 해결하는 접근 방식에서 뚜렷한 차이를 보인다.</p>
<h3>4.1 표 6.4.2 Mamba4Cast와 Affirm의 비교 분석</h3>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>Mamba4Cast</strong></th><th><strong>Affirm</strong></th></tr></thead><tbody>
<tr><td><strong>모델의 지향점</strong></td><td><strong>범용성 (Generalization)</strong>: 훈련받지 않은 데이터에 대한 즉각적 예측</td><td><strong>정밀성 (Precision)</strong>: 노이즈가 많은 실제 데이터에서의 정확한 예측</td></tr>
<tr><td><strong>학습 데이터</strong></td><td>100% 합성 데이터 (GP 커널 + FPFN Prior)</td><td>실제 벤치마크 데이터 (Supervised Learning)</td></tr>
<tr><td><strong>핵심 기술</strong></td><td><strong>Single-pass Forecasting</strong>: 반복 생성 없는 고속 추론</td><td><strong>Adaptive Fourier Filter</strong>: 주파수 영역에서의 노이즈 제거</td></tr>
<tr><td><strong>아키텍처 특징</strong></td><td>Mamba2 + Multi-scale Dilated Conv</td><td>Dual Interactive Mamba + Frequency Gating</td></tr>
<tr><td><strong>주요 강점</strong></td><td>데이터 수집 비용 ‘0’, 압도적인 추론 속도</td><td>신호 처리 이론과 딥러닝의 결합을 통한 강건성 확보</td></tr>
</tbody></table>
<p>이 두 모델은 시계열 예측의 미래가 단순한 ’모델 크기 키우기’에 있지 않음을 보여준다. Mamba4Cast는 **“데이터의 다양성”**이 모델 파라미터보다 중요할 수 있음을, Affirm은 **“도메인 지식(신호 처리)의 통합”**이 단순한 딥러닝 레이어 적층보다 효과적일 수 있음을 증명한다. 특히 Mamba 아키텍처의 도입으로 인해 시계열 예측 모델은 과거 데이터의 길이 제약에서 벗어나게 되었으며, 이는 향후 수년 분량의 고해상도 데이터를 입력으로 받아 초장기 미래를 예측하는 모델의 등장을 예고한다.</p>
<p>결론적으로, 6.4절에서 살펴본 두 모델은 각각 ’데이터 중심 AI’와 ’모델 중심 AI’의 진화를 대변한다. 향후 연구는 Mamba4Cast와 같이 합성 데이터로 사전 학습된 강력한 백본(Backbone)에, Affirm과 같은 정교한 주파수 도메인 필터링 기술을 파인튜닝 단계에서 결합하는 하이브리드 형태로 나아갈 가능성이 높다. 이는 범용적인 지식과 도메인 특화된 정밀함을 동시에 갖춘 이상적인 시계열 예측 시스템의 청사진을 제시한다.</p>
<h2>5. 참고 자료</h2>
<ol>
<li>Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models - OpenReview, https://openreview.net/pdf?id=YBOQ5HnzI6</li>
<li>Mamba time series forecasting with uncertainty quantification - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC12281171/</li>
<li>Mamba vs Transformers: Efficiency, Scale, and the Future of AI - Michiel Horstman - Medium, https://michielh.medium.com/mamba-vs-transformers-efficiency-scale-and-the-future-of-ai-d7a8dedb4018</li>
<li>Mamba Models a possible replacement for Transformers? - SciPy Proceedings, https://proceedings.scipy.org/articles/XHDR4700</li>
<li>(PDF) Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models, https://www.researchgate.net/publication/384929159_Mamba4Cast_Efficient_Zero-Shot_Time_Series_Forecasting_with_State_Space_Models</li>
<li>TSMamba : Mamba model for Time Series Forecasting | by Mehul Gupta | Data Science in Your Pocket | Medium, https://medium.com/data-science-in-your-pocket/tsmamba-mamba-model-for-time-series-forecasting-c9eeb0d0d23c</li>
<li>Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models - arXiv, https://arxiv.org/abs/2410.09385</li>
<li>Mamba4Cast, a zero-shot time series forecasting model, achieves competitive performance and faster inference than transformer-based models by leveraging Mamba architecture and synthetic training data to generalize robustly without fine-tuning. - GitHub, https://github.com/automl/Mamba4Cast</li>
<li>Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models - arXiv, https://arxiv.org/html/2410.09385v1</li>
<li>ForecastPFN: Synthetically-Trained Zero-Shot Forecasting - NIPS papers, https://papers.nips.cc/paper_files/paper/2023/file/0731f0e65559059eb9cd9d6f44ce2dd8-Paper-Conference.pdf</li>
<li>Moirai 2.0: When Less Is More for Time Series Forecasting - arXiv, https://arxiv.org/html/2511.11698v1</li>
<li>Affirm Interactive Mamba With Adaptive Fourier Filters For Long-Term Time Series Forecasting - Scribd, https://www.scribd.com/document/888484753/Affirm-Interactive-Mamba-With-Adaptive-Fourier-Filters-for-Long-term-Time-Series-Forecasting</li>
<li>Affirm: Interactive Mamba with Adaptive Fourier Filters for Long-term Time Series Forecasting - AAAI Publications, https://ojs.aaai.org/index.php/AAAI/article/view/35463/37618</li>
<li>(PDF) Affirm: Interactive Mamba with Adaptive Fourier Filters for Long-term Time Series Forecasting - ResearchGate, https://www.researchgate.net/publication/390723354_Affirm_Interactive_Mamba_with_Adaptive_Fourier_Filters_for_Long-term_Time_Series_Forecasting</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>