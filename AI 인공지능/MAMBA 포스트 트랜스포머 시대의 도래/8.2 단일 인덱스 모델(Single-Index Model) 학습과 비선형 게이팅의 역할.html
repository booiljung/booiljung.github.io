<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:8.2 단일 인덱스 모델(Single-Index Model) 학습과 비선형 게이팅의 역할</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>8.2 단일 인덱스 모델(Single-Index Model) 학습과 비선형 게이팅의 역할</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">MAMBA - 포스트 트랜스포머 시대의 도래</a> / <span>8.2 단일 인덱스 모델(Single-Index Model) 학습과 비선형 게이팅의 역할</span></nav>
                </div>
            </header>
            <article>
                <h1>8.2 단일 인덱스 모델(Single-Index Model) 학습과 비선형 게이팅의 역할</h1>
<h3>0.1  서론: 인컨텍스트 러닝의 이론적 난제와 단일 인덱스 모델의 도입</h3>
<p>2020년대 중반을 기점으로 거대 언어 모델(Large Language Models, LLMs)의 패러다임은 트랜스포머(Transformer)의 독주 체제에서 효율성을 강조한 상태 공간 모델(State Space Models, SSMs)과의 경쟁 체제로 전환되었다. 특히 Mamba 아키텍처는 선형 시간 복잡도(Linear Time Complexity)를 유지하면서도 트랜스포머에 필적하는, 혹은 특정 과업에서는 이를 능가하는 성능을 보여주며 ‘포스트 트랜스포머’ 시대의 선두 주자로 자리매김했다. 그러나 Mamba의 이러한 경험적 성공에도 불구하고, 그 내부에서 작동하는 학습 메커니즘—특히 가중치 업데이트 없이 문맥 내에서 새로운 과업을 수행하는 인컨텍스트 러닝(In-Context Learning, ICL)의 원리—에 대한 이론적 규명은 트랜스포머에 비해 상대적으로 미진했던 것이 사실이다.1</p>
<p>트랜스포머의 경우, 어텐션 메커니즘이 경사 하강법(Gradient Descent)의 한 단계를 근사(approximate)한다는 이론적 해석이 주류를 이루며 그 동작 원리가 어느 정도 파악되었다. 반면, 순환 신경망(RNN)의 진화형이자 연속적인 신호 처리에 뿌리를 둔 SSM, 그중에서도 선택적 상태 공간 모델(Selective SSM)인 Mamba가 어떻게 문맥 정보를 활용하여 즉각적으로 ’학습’하는지에 대한 질문은 여전히 베일에 싸여 있었다. 단순한 패턴 매칭인가, 아니면 더 고차원적인 특징 공간의 재구성인가? 이 질문에 답하기 위해 본 장에서는 고차원 통계 학습 이론의 핵심 도구인 <strong>단일 인덱스 모델(Single-Index Model, SIM)</strong> 을 분석의 틀로 도입한다.4</p>
<p>단일 인덱스 모델은 입력 데이터가 고차원 공간에 존재하더라도, 출력값은 알 수 없는 미지의 특징 벡터(feature vector)에 사영(projection)된 값의 비선형 변환으로 결정된다고 가정한다. 이는 현대 딥러닝 모델이 고차원 데이터 속에서 저차원의 내재적 구조(intrinsic structure)를 찾아내는 과정을 수학적으로 가장 정밀하게 모사하는 프레임워크로 평가받는다.6 우리가 주목하는 것은 Mamba가 이러한 SIM 구조를 문맥 내에서 학습할 수 있다는 사실 그 자체가 아니라, 그 과정에서 <strong>비선형 게이팅(Non-linear Gating)</strong> 이 수행하는 결정적인 역할이다.</p>
<p>기존의 선형 어텐션(Linear Attention) 모델들이 연산 효율성을 위해 비선형성을 제거하면서 표현력(Expressivity)의 한계, 즉 ’커널 체제(Kernel Regime)’에 갇혔던 것과 달리, Mamba는 게이팅 메커니즘을 통해 이러한 한계를 돌파한다. 본 절에서는 Mamba가 어떻게 선형 어텐션의 한계를 극복하고, 문맥 예제들로부터 과업에 관련된 특징(relevant features)을 직접 추출하여 학습하는 <strong>테스트 타임 특징 학습(Test-Time Feature Learning)</strong> 을 구현하는지를 심층적으로 분석한다. 우리는 Mamba의 비선형 게이팅이 단순한 정보의 흐름 제어를 넘어, 데이터의 유효성을 평가하고 아웃라이어를 억제하며, 결과적으로 모델이 최적의 통계적 학습 속도에 도달하게 하는 핵심 기제임을 수학적, 통계적 관점에서 논증할 것이다.5</p>
<h3>0.2  단일 인덱스 모델(SIM)의 수학적 정의와 이론적 배경</h3>
<p>Mamba의 학습 동역학을 분석하기 위해서는 먼저 분석의 대상이 되는 과업, 즉 단일 인덱스 모델의 구조를 엄밀하게 정의할 필요가 있다. 통계학 및 기계학습 이론에서 단일 인덱스 모델은 고차원 회귀 분석의 차원의 저주(Curse of Dimensionality)를 극복하기 위한 준모수적(semi-parametric) 모델로 오랫동안 연구되어 왔다.6</p>
<h4>0.2.1 SIM의 기본 구조</h4>
<p><span class="math math-inline">d</span>차원의 입력 벡터 <span class="math math-inline">x \in \mathbb{R}^d</span>와 스칼라 출력 <span class="math math-inline">y \in \mathbb{R}</span> 사이의 관계가 다음과 같이 주어진다고 가정하자:<br />
<span class="math math-display">
y = g_*(\langle \beta, x \rangle) + \zeta
</span><br />
여기서 각 구성 요소의 의미는 다음과 같다:</p>
<ul>
<li><strong><span class="math math-inline">\beta \in \mathbb{R}^d</span> (Index Parameter):</strong> 모델이 학습해야 할 핵심 파라미터로, 미지의 특징 벡터(feature vector) 또는 인덱스 벡터이다. 본 분석에서는 <span class="math math-inline">\beta</span>가 <span class="math math-inline">r</span>차원 부분 공간 <span class="math math-inline">S_r</span>의 단위 구면(unit sphere)에서 균등하게 추출된다고 가정한다. 이는 실제 데이터가 고차원(<span class="math math-inline">d</span>)이지만, 레이블을 결정하는 본질적인 정보는 저차원(<span class="math math-inline">r \ll d</span>) 부분 공간에 존재함을 의미한다.5</li>
<li><strong><span class="math math-inline">g_\*: \mathbb{R} \to \mathbb{R}</span> (Link Function):</strong> 미지의 비선형 링크 함수이다. 일반적인 선형 회귀가 <span class="math math-inline">g_*</span>를 항등 함수(identity function)로 가정하는 것과 달리, SIM은 <span class="math math-inline">g_*</span>가 다항식(polynomial)이나 시그모이드(sigmoid)와 같은 비선형 함수일 수 있음을 허용한다. 이는 모델이 단순한 선형 결합 이상의 복잡한 관계를 학습해야 함을 시사한다.7</li>
<li><strong><span class="math math-inline">x \sim \mathcal{N}(0, I_d)</span>:</strong> 입력 데이터는 표준 정규 분포를 따른다고 가정한다. 이는 이론적 분석의 편의를 위한 것이지만, 최근 연구들은 이러한 가정이 구면 대칭(spherically symmetric) 분포 등으로 완화될 수 있음을 보이고 있다.5</li>
<li><strong><span class="math math-inline">\zeta</span>:</strong> 관측 오차를 나타내는 노이즈 항이다.</li>
</ul>
<h4>0.2.2 인컨텍스트 러닝(ICL) 시나리오에서의 SIM</h4>
<p>전통적인 통계학에서는 고정된 데이터셋을 통해 <span class="math math-inline">\beta</span>와 <span class="math math-inline">g_*</span>를 추정하지만, ICL 설정에서는 모델이 ‘프롬프트(Prompt)’ 내에 주어진 제한된 예제들만으로 이들을 즉석에서 추정해야 한다. 프롬프트 <span class="math math-inline">P</span>는 <span class="math math-inline">N</span>개의 문맥 예제와 하나의 쿼리로 구성된다:<br />
<span class="math math-display">
P = \{(x_1, y_1), (x_2, y_2), \dots, (x_N, y_N), x_{query}\}
</span><br />
Mamba 모델의 목표는 이 프롬프트를 순차적으로 처리하여, 마지막 쿼리 <span class="math math-inline">x_{query}</span>에 대한 정답 <span class="math math-inline">y_{query}</span>를 예측하는 것이다. 이때 모델은 사전에 <span class="math math-inline">\beta</span>나 <span class="math math-inline">g_*</span>에 대한 정보를 가지고 있지 않으며(사전 학습된 가중치에 고정되어 있지 않음), 오직 문맥 <span class="math math-inline">(x_i, y_i)</span> 쌍들 사이의 관계를 분석하여 암묵적으로 <span class="math math-inline">\beta</span>를 찾아내고 <span class="math math-inline">g_*</span>를 근사해야 한다. 이것이 바로 <strong>테스트 타임 특징 학습</strong>의 본질이다.5</p>
<p>이 과업이 중요한 이유는, 만약 Mamba가 SIM을 효과적으로 학습할 수 있다면, 이는 Mamba가 단순히 훈련 데이터에 있는 패턴을 기억해서 출력하는 것이 아니라, 새로운 데이터 분포의 기저에 있는 <strong>구조(structure)</strong> 를 파악하고 적응(adapt)할 수 있는 지능적 능력을 갖추었음을 증명하기 때문이다.</p>
<h3>0.3  선형 어텐션의 한계: 커널 체제(Kernel Regime)의 덫</h3>
<p>Mamba의 성취를 명확히 이해하기 위해서는 비교 대상인 선형 트랜스포머(Linear Transformer)가 왜 이 문제에서 한계를 보이는지 분석해야 한다. 선형 트랜스포머는 기존 트랜스포머의 <span class="math math-inline">O(N^2)</span> 복잡도를 해결하기 위해 소프트맥스(Softmax)를 제거하고 <span class="math math-inline">K, V</span>의 곱을 먼저 계산하는 방식을 취한다. 그러나 이러한 구조적 변화는 모델의 학습 능력을 <strong>커널 회귀(Kernel Regression)</strong> 수준으로 제한하는 결과를 초래한다.5</p>
<h4>0.3.1 커널 방법론으로서의 선형 어텐션</h4>
<p>이론적 분석에 따르면, 선형 어텐션 메커니즘을 사용하는 모델이 인컨텍스트 러닝을 수행할 때, 그 예측값 <span class="math math-inline">\hat{y}</span>는 훈련 예제들의 선형 결합으로 표현된다. 이는 수학적으로 커널 리지 회귀(Kernel Ridge Regression)와 동치이다.<br />
<span class="math math-display">
\hat{y}_{query} = \sum_{i=1}^{N} \alpha_i K(x_i, x_{query})
</span><br />
여기서 커널 함수 <span class="math math-inline">K(x, x&#39;)</span>는 모델의 임베딩 층에 의해 결정되는 고정된 특징 맵 <span class="math math-inline">\phi(\cdot)</span>의 내적 <span class="math math-inline">\langle \phi(x), \phi(x&#39;) \rangle</span>으로 정의된다. 커널 체제 하에서 모델은 입력 데이터를 고정된 고차원 공간으로 매핑한 후, 그 공간 내에서 선형적인 해를 찾는다.</p>
<p>문제는 SIM과 같이 타겟 함수가 입력의 특정 방향(<span class="math math-inline">\beta</span>)에 의존하는 비선형 함수일 때 발생한다. 커널 방법론은 데이터로부터 특징 표현 자체를 학습하는 것이 아니라, 미리 정의된 특징들의 조합만으로 문제를 해결하려 한다. 따라서 타겟 함수 <span class="math math-inline">g_*</span>를 근사하기 위해서는 특징 맵의 차원이 매우 커져야 하며, 이는 <strong>차원의 저주</strong>로 이어진다. 구체적으로, 선형 트랜스포머가 <span class="math math-inline">g_*</span>를 학습하기 위해 필요한 샘플 수(Sample Complexity)는 입력 차원 <span class="math math-inline">d</span>와 링크 함수의 <strong>정보 지수(Information Exponent, <span class="math math-inline">k</span>)</strong> 에 따라 <span class="math math-inline">O(d^k)</span>로 급격히 증가한다.12</p>
<ul>
<li><strong>정보 지수(Information Exponent):</strong> 타겟 함수를 Hermite 다항식으로 전개했을 때, 계수가 0이 아닌 첫 번째 차수를 의미한다. 만약 <span class="math math-inline">g_*</span>가 복잡한 고차 다항식의 성질을 가진다면, 선형 트랜스포머는 이를 학습하기 위해 비현실적으로 많은 문맥 예제를 필요로 하게 된다.14</li>
</ul>
<p>결국 선형 트랜스포머는 계산 효율성을 얻는 대신, 데이터의 숨겨진 구조(<span class="math math-inline">\beta</span>)를 능동적으로 찾아내는 ‘특징 학습’ 능력을 상실하고, ‘게으른 학습(Lazy Learning)’ 혹은 커널 체제의 한계에 갇히게 된다. 이는 아웃라이어에 대한 취약성으로도 연결되는데, 선형 모델은 입력의 스케일에 민감하여 노이즈가 섞인 예제가 전체 예측을 크게 왜곡시킬 수 있기 때문이다.1</p>
<h3>0.4  Mamba 아키텍처와 비선형 게이팅의 메커니즘</h3>
<p>Mamba는 선형 트랜스포머와 유사한 순환적 구조를 가지면서도, <strong>선택적 상태 공간 모델(Selective SSM)</strong> 이라는 독창적인 설계를 통해 커널 체제의 한계를 극복한다. 그 핵심에는 입력에 따라 동적으로 변하는 파라미터, 즉 <strong>비선형 게이팅(Non-linear Gating)</strong> 이 있다.</p>
<h4>0.4.1 Mamba의 이산화(Discretization)와 게이팅 수식</h4>
<p>Mamba의 연속 시간 상태 공간 모델은 다음과 같은 미분 방정식으로 표현된다:<br />
<span class="math math-display">
h&#39;(t) = A h(t) + B x(t)
</span></p>
<p><span class="math math-display">
y(t) = C h(t)
</span></p>
<p>이를 이산 시간(discrete time)으로 변환하여 디지털 시퀀스 데이터를 처리하기 위해, Mamba는 <strong>Zero-Order Hold (ZOH)</strong> 방식을 사용하며, 이때 시간 간격(step size) <span class="math math-inline">\Delta</span>가 핵심적인 게이팅 파라미터로 등장한다. Mamba의 차별점은 이 <span class="math math-inline">\Delta</span>가 고정된 상수가 아니라, 입력 <span class="math math-inline">x_t</span>에 의존하는 함수라는 점이다.17<br />
<span class="math math-display">
\Delta_t = \text{Softplus}(\text{Linear}(x_t)) \\ \bar{A}_t = \exp(\Delta_t A) \\ \bar{B}_t = (\Delta_t A)^{-1} (\exp(\Delta_t A) - I) \cdot \Delta_t B
</span><br />
이산화된 업데이트 식은 다음과 같다:<br />
<span class="math math-display">
h_t = \bar{A}_t h_{t-1} + \bar{B}_t x_t
</span></p>
<p><span class="math math-display">
y_t = C h_t
</span></p>
<p>여기서 <span class="math math-inline">\bar{A}_t</span>와 <span class="math math-inline">\bar{B}_t</span>는 모두 현재 입력 <span class="math math-inline">x_t</span>의 함수인 <span class="math math-inline">\Delta_t</span>에 의해 결정된다. 이것이 바로 Mamba의 <strong>선택성(Selectivity)</strong> 이다. 게이트 값 <span class="math math-inline">\Delta_t</span>가 크면 현재 입력 <span class="math math-inline">x_t</span>의 정보를 은닉 상태 <span class="math math-inline">h_t</span>에 많이 반영하고 이전 상태 <span class="math math-inline">h_{t-1}</span>의 영향력을 조절한다. 반대로 <span class="math math-inline">\Delta_t</span>가 작으면 현재 입력을 무시(skip)하고 이전 기억을 보존한다. 이 메커니즘은 단순한 선형 순환식(LTI, Linear Time Invariant)을 <strong>시변(Time-Variant)</strong> 비선형 시스템으로 변환시킨다.18</p>
<h4>0.4.2 Mamba의 ICL을 위한 그래디언트 누적 가설</h4>
<p>Sushma et al. (2024)의 연구는 Mamba가 이러한 구조를 활용하여 어떻게 SIM을 학습하는지에 대한 통찰을 제공한다. 그들의 분석에 따르면, Mamba의 은닉 상태 <span class="math math-inline">h_t</span>는 암묵적인 선형 회귀 문제의 <strong>그래디언트(Gradient)</strong> 를 누적하는 메모리 역할을 수행한다.17</p>
<p>단일 인덱스 모델의 손실 함수 <span class="math math-inline">L(w) = \frac{1}{2} \sum (w^\top x_i - y_i)^2</span>를 최소화하는 과정에서, 그래디언트는 <span class="math math-inline">\nabla L \propto \sum (w^\top x_i - y_i) x_i</span> 형태를 띤다. Mamba의 업데이트 식 <span class="math math-inline">h_t = \bar{A}_t h_{t-1} + \bar{B}_t x_t</span>는 적절한 파라미터 설정을 통해 이 그래디언트의 누적합을 <span class="math math-inline">h_t</span>에 저장하는 과정과 수학적으로 동치임이 증명되었다.</p>
<ul>
<li><strong>상태 <span class="math math-inline">h_t</span>:</strong> 누적된 그래디언트 정보 (<span class="math math-inline">\sum y_i x_i</span> 등 공분산 통계량)</li>
<li><strong>게이팅 <span class="math math-inline">\Delta_t</span>:</strong> 각 샘플의 학습률(Learning Rate) 또는 가중치. 입력의 중요도에 따라 그래디언트 반영 비율을 조절.</li>
<li><strong>출력 <span class="math math-inline">y_t</span>:</strong> 누적된 그래디언트(업데이트된 가중치 <span class="math math-inline">w_{new}</span>)를 사용하여 쿼리에 대한 예측 수행.</li>
</ul>
<p>이러한 해석은 Mamba가 트랜스포머와 달리 전체 문맥을 한 번에 보지 못함에도 불구하고, 순차적인 업데이트만으로도 전역적인 최적해에 근접할 수 있는 이유를 설명한다. Mamba는 일종의 <strong>온라인 경사 하강법(Online Gradient Descent)</strong> 혹은 <strong>미니배치 경사 하강법</strong>을 내부적으로 에뮬레이션하고 있는 것이다.20</p>
<h3>0.5  비선형 게이팅의 이중 역할: 선택(Selection)과 억제(Suppression)</h3>
<p>단일 인덱스 모델 학습의 관점에서 Mamba의 비선형 게이팅은 단순한 ‘연산’ 그 이상의 의미를 갖는다. 이론적 분석과 실험 결과들은 게이팅이 크게 두 가지 핵심적인 역할을 수행함으로써 모델의 성능과 강건성을 보장함을 보여준다: <strong>(1) 정보적 예제의 선택</strong>과 <strong>(2) 아웃라이어의 억제</strong>이다.1</p>
<h4>0.5.1  정보적 예제의 선택 (Feature Selection via Gating)</h4>
<p>SIM 학습의 핵심은 수많은 입력 차원 중에서 레이블 <span class="math math-inline">y</span>와 상관관계가 높은 <span class="math math-inline">\beta</span> 방향을 찾아내는 것이다. 모든 문맥 예제가 동일한 정보량을 가지지는 않는다. 어떤 예제는 <span class="math math-inline">\beta</span> 방향의 성분이 강하여 학습에 유용하지만, 어떤 예제는 노이즈가 많거나 <span class="math math-inline">\beta</span>와 직교하여 정보가가 낮을 수 있다.</p>
<p>선형 어텐션은 모든 예제에 대해 전역적인 가중치를 적용하는 경향이 있어 이러한 국소적 중요도를 포착하기 어렵다. 반면, Mamba의 게이팅 <span class="math math-inline">\Delta_t(x_t)</span>는 입력 <span class="math math-inline">x_t</span>의 내용에 따라 즉각적으로 반응한다. 연구 결과에 따르면, 학습된 Mamba 모델의 게이트는 <span class="math math-inline">\beta</span> 방향과 정렬된(aligned) 입력, 즉 정보량이 풍부한 예제에 대해 더 큰 값을 출력하여 은닉 상태에 강하게 기록되도록 한다.22 이는 마치 사람이 중요한 정보를 들을 때만 주의를 기울이고 나머지는 흘려듣는 것과 유사한 <strong>정보 필터링(Information Filtering)</strong> 과정이다.</p>
<p>수학적으로 이는 경사 하강법에서의 <strong>적응형 학습률(Adaptive Learning Rate)</strong> 과 유사하다. 정보가 확실한 샘플에 대해서는 큰 스텝으로 파라미터를 업데이트하고, 불확실한 샘플에 대해서는 보수적으로 업데이트함으로써, Mamba는 적은 수의 문맥 예제로도 <span class="math math-inline">\beta</span> 방향을 빠르게 수렴시킬 수 있다. 이것이 바로 Mamba가 <span class="math math-inline">O(d)</span> 수준의 낮은 표본 복잡도를 달성하는 비결 중 하나이다.</p>
<h4>0.5.2  아웃라이어 억제와 강건성 (Robustness via Suppression)</h4>
<p>더욱 결정적인 차이는 <strong>아웃라이어(Outliers)</strong> 처리에서 드러난다. 실제 데이터나 프롬프트에는 종종 분포를 벗어나는 이상치나 적대적 노이즈(adversarial noise)가 포함될 수 있다. 선형 모델의 치명적인 약점은 이러한 아웃라이어의 영향이 선형적으로 전파된다는 것이다.</p>
<p>선형 트랜스포머에서 하나의 거대한 노이즈 입력 <span class="math math-inline">x_{outlier}</span>는 가중합 연산을 통해 전체 어텐션 결과를 왜곡시킨다. 이를 ’전파된 오차’라고 하며, 일정 비율 이상의 아웃라이어가 존재하면 선형 모델은 붕괴(collapse)한다.</p>
<p>하지만 Mamba의 게이팅 함수는 주로 Sigmoid, Tanh, 또는 Softplus와 같은 포화(saturation) 비선형 함수를 포함한다.<br />
<span class="math math-display">
\text{Gate}(x) = \sigma(W x + b)
</span><br />
만약 <span class="math math-inline">x</span>가 정상 범위를 벗어난 아웃라이어라면, 학습된 게이팅 메커니즘은 이를 감지하여 게이트 값을 0에 가깝게 닫아버릴 수 있다(<span class="math math-inline">\Delta_t \to 0</span>). 즉, 아웃라이어가 은닉 상태 <span class="math math-inline">h_t</span>를 오염시키는 것을 물리적으로 차단(Block) 하는 것이다.1</p>
<p>이론적 분석에 따르면, Mamba는 선형 트랜스포머가 견딜 수 있는 임계치를 훨씬 초과하는 아웃라이어 비율 하에서도 정확한 예측 성능을 유지한다. 비록 노이즈를 걸러내기 위해 더 많은 훈련 반복(iterations)이 필요할 수는 있지만, 최종적으로 수렴하는 해의 품질과 일반화 성능은 선형 모델보다 월등히 우수하다.1 이는 Mamba가 ‘깨끗한’ 실험실 환경뿐만 아니라, 노이즈가 만연한 현실 세계의 데이터 스트림을 처리하는 데 적합한 아키텍처임을 시사한다.</p>
<h3>0.6  커널의 장벽을 넘어서: 테스트 타임 특징 학습과 생성 지수</h3>
<p>Mamba가 단일 인덱스 모델 학습에서 보여주는 가장 놀라운 이론적 성취는 <strong>커널 체제를 탈피하여 테스트 타임 특징 학습을 구현했다</strong>는 점이다. 이를 정량적으로 설명하기 위해 최신 통계 학습 이론인 <strong>생성 지수(Generative Exponent)</strong> 개념을 도입한다.</p>
<h4>0.6.1 특징 학습 vs. 커널 학습</h4>
<p>앞서 언급했듯, 커널 방법론(선형 트랜스포머)은 고정된 특징 공간에서 해를 찾는다. 반면, 특징 학습(Feature Learning)은 데이터에 맞춰 특징 공간 자체를 변형한다. Mamba는 비선형 게이팅을 통해 입력 데이터 <span class="math math-inline">x</span>와 레이블 <span class="math math-inline">y</span>의 상호작용을 비선형적으로 변환함으로써, 고정된 커널의 제약을 벗어난다.</p>
<p>연구 결과, Mamba는 프롬프트에 주어진 예제들을 통해 미지의 특징 벡터 <span class="math math-inline">\beta</span>를 자신의 은닉 상태 내에 명시적으로(explicitly) 혹은 암묵적으로(implicitly) 구성해낸다는 것이 밝혀졌다. 이는 모델이 훈련 단계(Pre-training)에서 <span class="math math-inline">\beta</span>를 학습하는 것이 아니라, 추론 단계(Test-time)에서 새로운 <span class="math math-inline">\beta</span>를 찾아낸다는 점에서 <strong>테스트 타임 특징 학습</strong>으로 명명된다.5</p>
<h4>0.6.2 생성 지수(Generative Exponent)와 표본 복잡도의 혁명</h4>
<p>SIM 학습의 난이도는 링크 함수 <span class="math math-inline">g_*</span>의 복잡도에 달려 있다. 기존 이론에서 표본 복잡도는 <strong>정보 지수(Information Exponent, <span class="math math-inline">k</span>)</strong> 에 지배된다(<span class="math math-inline">n \gtrsim d^k</span>). <span class="math math-inline">k</span>가 크면 학습은 사실상 불가능해진다.</p>
<p>그러나 최근 연구들은 Mamba와 같이 특징 학습이 가능한 모델들의 표본 복잡도가 정보 지수가 아닌 <strong>생성 지수(Generative Exponent, <span class="math math-inline">k^*</span></strong>)에 의해 결정됨을 증명했다.2<br />
<span class="math math-display">
k^* := \min_{T \in L^2} \text{IE}(T \circ g_*)
</span><br />
생성 지수는 타겟 함수 <span class="math math-inline">g_*</span>에 임의의 <span class="math math-inline">L^2</span> 변환 <span class="math math-inline">T</span>를 적용했을 때 얻을 수 있는 최소의 정보 지수를 의미한다. Mamba의 비선형 게이팅은 입력과 출력에 비선형 변환을 가하는 효과가 있으며, 이는 수학적으로 최적의 변환 <span class="math math-inline">T</span>를 찾아 적용하는 것과 유사하다.</p>
<p>예를 들어, 어떤 함수가 대칭적이어서 정보 지수가 높더라도(예: <span class="math math-inline">y=x^2</span>, <span class="math math-inline">k=2</span>), 게이팅을 통해 비대칭성을 유도하거나 변환을 가하면 더 낮은 차수의 성질을 갖게 만들어(<span class="math math-inline">k^* &lt; k</span>) 학습을 용이하게 할 수 있다.</p>
<p>이 결과는 실로 혁명적이다. 선형 트랜스포머가 <span class="math math-inline">O(d^k)</span>의 샘플을 요구하며 허덕일 때, Mamba는 비선형 게이팅을 활용하여 이를 <span class="math math-inline">O(d)</span> 또는 생성 지수에 비례하는 수준으로 획기적으로 낮춘다. 이는 Mamba가 소프트맥스 어텐션을 사용하는 일반 트랜스포머나 이상적인 비선형 트랜스포머가 달성하는 <strong>정보 이론적 최적 속도(Near Information-Theoretically Optimal Rate)</strong> 에 근접했음을 의미한다.2</p>
<table><thead><tr><th><strong>특성</strong></th><th><strong>선형 트랜스포머 (Linear Transformer)</strong></th><th><strong>Mamba (Selective SSM)</strong></th><th><strong>일반 트랜스포머 (Softmax Transformer)</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 연산</strong></td><td>선형화된 어텐션 (<span class="math math-inline">KV</span> 곱)</td><td><strong>비선형 게이팅 (<span class="math math-inline">\Delta_t</span>) 기반 순환</strong></td><td>소프트맥스 어텐션</td></tr>
<tr><td><strong>학습 체제</strong></td><td>커널 회귀 (Kernel Regression)</td><td><strong>테스트 타임 특징 학습 (Feature Learning)</strong></td><td>경사 하강법 / 특징 학습</td></tr>
<tr><td><strong>표본 복잡도</strong></td><td><span class="math math-inline">O(d^k)</span> (정보 지수 의존)</td><td><strong><span class="math math-inline">O(d)</span> ~ <span class="math math-inline">O(d^{k^\*})</span> (생성 지수 의존)</strong></td><td><span class="math math-inline">O(d)</span> (최적에 근접)</td></tr>
<tr><td><strong>아웃라이어 강건성</strong></td><td>낮음 (선형 전파로 붕괴)</td><td><strong>높음 (게이팅에 의한 차단)</strong></td><td>중간~높음</td></tr>
<tr><td><strong>추론 속도</strong></td><td><span class="math math-inline">O(N)</span></td><td><strong><span class="math math-inline">O(N)</span></strong></td><td><span class="math math-inline">O(N^2)</span></td></tr>
</tbody></table>
<p><strong>표 8.2.1</strong> 단일 인덱스 모델 학습 관점에서의 아키텍처별 특성 비교. Mamba는 선형 트랜스포머의 효율성과 일반 트랜스포머의 학습 능력을 동시에 달성한다.</p>
<h3>0.7  결론 및 시사점: 포스트 트랜스포머 시대를 향한 제언</h3>
<p>본 절의 심층 분석을 통해 우리는 Mamba의 인컨텍스트 러닝 능력이 단순한 우연이나 경험적 최적화의 산물이 아님을 확인했다. 단일 인덱스 모델(SIM)이라는 이론적 렌즈를 통해 들여다본 Mamba는, <strong>비선형 게이팅</strong>이라는 강력한 기제를 통해 선형 모델의 한계를 돌파하고 고차원 데이터의 본질적 구조를 학습하는 <strong>동적 시스템(Dynamic System)</strong> 이었다.</p>
<p>우리가 도출한 주요 결론은 다음과 같다:</p>
<ol>
<li><strong>비선형성은 선택이 아닌 필수다:</strong> 효율적인 시계열 모델링을 위해 선형 복잡도를 추구하더라도, 정보의 가치를 판단하고 선별하기 위한 최소한의 비선형성(게이팅)은 필수불가결하다. Mamba의 성공은 순수 선형 어텐션 모델들이 왜 실패했는지에 대한 명확한 해답을 제공한다.25</li>
<li><strong>게이팅은 최적화 알고리즘이다:</strong> Mamba의 게이팅 메커니즘은 단순한 신호 제어 장치가 아니라, 입력 데이터의 중요도에 따라 학습률을 조절하고 노이즈를 필터링하는 <strong>정교한 최적화 알고리즘(Optimizer)</strong> 의 역할을 수행한다. 이는 Mamba가 ICL 상황에서 경사 하강법을 에뮬레이션할 수 있는 물리적 기반이 된다.20</li>
<li><strong>효율성과 표현력의 딜레마 해결:</strong> Mamba는 생성 지수에 기반한 효율적인 표본 복잡도를 달성함으로써, 계산 효율성(Linear Compute)과 통계적 효율성(Optimal Sample Complexity)이 반드시 상충 관계(Trade-off)에 있는 것은 아님을 증명했다. 이는 더 적은 자원으로 더 똑똑하게 학습하는 차세대 AI 모델의 청사진을 제시한다.2</li>
</ol>
<p>결국 Mamba의 비선형 게이팅과 단일 인덱스 모델 학습 능력에 대한 규명은, 트랜스포머 이후의 아키텍처가 나아가야 할 방향을 가리키고 있다. 그것은 무작정 모델의 크기를 키우거나 문맥을 늘리는 것이 아니라, 데이터의 홍수 속에서 <strong>‘무엇을 기억하고 무엇을 버릴지’</strong> 를 스스로 결정할 수 있는 지능적인 선택 메커니즘을 설계하는 것이다. 2025년 현재, Mamba는 그 가능성을 이론과 실제 양면에서 가장 강력하게 증명하고 있는 모델이다.</p>
<h2>1. 참고 자료</h2>
<ol>
<li>Understanding Mamba in In-Context Learning with Outliers: A Theoretical Generalization Analysis - OpenReview, https://openreview.net/pdf?id=DHyGZHBZci</li>
<li>arxiv.org, https://arxiv.org/html/2510.12026v1</li>
<li>Mamaba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning, https://chatpaper.com/paper/199851</li>
<li>[2510.12026] Mamba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning - arXiv, https://arxiv.org/abs/2510.12026</li>
<li>[Literature Review] Mamba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning - Moonlight, https://www.themoonlight.io/en/review/mamba-can-learn-low-dimensional-targets-in-context-via-test-time-feature-learning</li>
<li>On Learning High Dimensional Structured Single Index Models, https://ojs.aaai.org/index.php/AAAI/article/view/10835/10694</li>
<li>Learning Single-Index Models in High Dimensions - Robert Nowak - University of Wisconsin–Madison, https://nowak.ece.wisc.edu/single_index_arxiv.pdf</li>
<li>Mamba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning - arXiv, https://arxiv.org/pdf/2510.12026</li>
<li>Inference In High-dimensional Single-Index Models Under Symmetric Designs - Journal of Machine Learning Research, https://jmlr.csail.mit.edu/papers/volume22/19-744/19-744.pdf</li>
<li>Learning Single-Index Models with Shallow Neural Networks, https://proceedings.neurips.cc/paper_files/paper/2022/file/3fb6c52aeb11e09053c16eabee74dd7b-Paper-Conference.pdf</li>
<li>Mamba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning, https://openreview.net/forum?id=3KPsog4mpy</li>
<li>Fundamental Limits of Learning Single-Index Models under Structured Data - OpenReview, https://openreview.net/pdf?id=17mGBDbqR8</li>
<li>Nonlinear feature learning of neural networks with gradient descent: Information theoretic optimality and in-context learning - CIRM, https://www.cirm-math.fr/RepOrga/3003/Slides/Taiji_Suzuki_Presentation_LOL24.pdf</li>
<li>[Quick Review] Neural network learns low-dimensional polynomials, https://liner.com/review/neural-network-learns-lowdimensional-polynomials-with-sgd-near-the-informationtheoretic</li>
<li>Learning single-index models with neural networks - Denny Wu, https://dennywu1.github.io/single_index_SGD.pdf</li>
<li>Can Mamba Learn In Context with Outliers? A Theoretical …, https://openreview.net/forum?id=tswBfpkwHn</li>
<li>state-space models can learn in-context by - arXiv, https://arxiv.org/abs/2410.11687</li>
<li>HOW MAMBA IN-CONTEXT LEARNS MARKOV CHAINS - OpenReview, https://openreview.net/pdf?id=kmK3WSCOCT</li>
<li>state-space models can learn in-context by - arXiv, <a href="https://arxiv.org/pdf/2410.11687">https://arxiv.org/pdf/2410.11687?</a></li>
<li>Trained Mamba Emulates Online Gradient Descent in In-Context Linear Regression, https://neurips.cc/virtual/2025/poster/118683</li>
<li>Taiji Suzuki’s research works | RIKEN and other places - ResearchGate, https://www.researchgate.net/scientific-contributions/Taiji-Suzuki-2106269234</li>
<li>Can Mamba Learn In Context with Outliers? A Theoretical Generalization Analysis - arXiv, https://arxiv.org/html/2510.00399v1</li>
<li>Theoretical Analysis of the Selection Mechanism in Mamba: Training Dynamics and Generalization - What Can(’t) Transformers Do?, https://transformerstheory.github.io/pdf/30_shandirasegaran_et_al.pdf</li>
<li>Computational-Statistical Gaps in Gaussian Single-Index Models - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v247/damian24a/damian24a.pdf</li>
<li>Demystify Mamba in Vision: A Linear Attention Perspective - arXiv, https://arxiv.org/pdf/2405.16605</li>
<li>ReGLA: Refining Gated Linear Attention - arXiv, https://arxiv.org/html/2502.01578v1</li>
<li>State-space models can learn in-context by gradient descent - arXiv, https://arxiv.org/html/2410.11687v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>