<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:6.2 [Video] VideoMamba & Dual Branch - 시공간 모델링과 게이트 클래스 토큰 퓨전(GCTF)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>6.2 [Video] VideoMamba & Dual Branch - 시공간 모델링과 게이트 클래스 토큰 퓨전(GCTF)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">MAMBA - 포스트 트랜스포머 시대의 도래</a> / <span>6.2 [Video] VideoMamba & Dual Branch - 시공간 모델링과 게이트 클래스 토큰 퓨전(GCTF)</span></nav>
                </div>
            </header>
            <article>
                <h1>6.2 [Video] VideoMamba &amp; Dual Branch - 시공간 모델링과 게이트 클래스 토큰 퓨전(GCTF)</h1>
<h2>1.  서론: 비디오 이해 기술의 진화와 새로운 패러다임의 필요성</h2>
<p>현대 컴퓨터 비전 분야에서 비디오 이해(Video Understanding)는 정지된 이미지 처리 기술을 넘어선 가장 복잡하고 도전적인 과제 중 하나로 자리 잡고 있다. 비디오 데이터는 정적인 공간 정보(Spatial Information)와 시간의 흐름에 따라 변화하는 동적 정보(Temporal Information)가 고차원으로 결합된 4차원적 특성을 지니며, 이를 기계가 인지하고 해석하도록 만드는 것은 단순한 사물 인식을 넘어선 인과관계 추론, 행동 예측, 그리고 상황 인식을 요구한다.</p>
<p>지난 10년간 비디오 이해를 위한 딥러닝 아키텍처는 눈부신 발전을 거듭해왔다. 초기에는 2D CNN(Convolutional Neural Networks)을 프레임 단위로 적용하고 그 결과를 풀링(Pooling)하거나 RNN(Recurrent Neural Networks)과 결합하는 방식이 주류를 이루었다. 이는 이미지 처리에 특화된 2D CNN의 강력한 특징 추출 능력을 활용할 수 있다는 장점이 있었으나, 프레임 간의 정교한 시간적 상관관계를 모델링하는 데에는 한계가 있었다.1 이를 극복하기 위해 등장한 3D CNN(C3D, I3D 등)은 시간 축으로 커널을 확장하여 시공간 특징을 동시에 학습하려 시도했으나, 연산량의 급격한 증가와 국소적인 수용 영역(Receptive Field)으로 인해 장기적인 의존성(Long-range Dependency)을 포착하는 데 여전히 어려움을 겪었다.3</p>
<p>이후 자연어 처리(NLP) 분야를 석권한 트랜스포머(Transformer) 아키텍처가 비디오 도메인으로 이식되면서 비디오 트랜스포머(Video Transformer, 예: ViViT, TimeSformer) 시대가 도래했다. 셀프 어텐션(Self-Attention) 메커니즘은 입력 시퀀스 전체를 조망할 수 있는 전역적 수용 영역을 제공함으로써 3D CNN의 국소성 문제를 해결했다. 그러나 트랜스포머는 입력 토큰 수의 제곱(<span class="math math-inline">O(N^2)</span>)에 비례하는 연산 복잡도(Quadratic Complexity)라는 치명적인 병목을 안고 있다.2 비디오 데이터는 픽셀 수와 프레임 수가 곱해져 데이터의 양이 기하급수적으로 늘어나기 때문에, 고해상도나 긴 길이의 비디오를 처리할 때 트랜스포머의 계산 비용은 감당하기 어려운 수준에 도달한다. 이는 제한된 자원을 가진 엣지 디바이스나 실시간 감시 시스템에서의 활용을 가로막는 주요 원인이 되었다.</p>
<p>이러한 배경 속에서 최근 상태 공간 모델(State Space Models, SSM), 그중에서도 Mamba 아키텍처가 비디오 이해 분야의 새로운 대안으로 급부상하고 있다. VideoMamba는 Mamba의 선형 복잡도(<span class="math math-inline">O(N)</span>) 특성을 비디오 도메인에 적용하여, 트랜스포머 수준의 모델링 능력을 유지하면서도 연산 효율성을 획기적으로 개선하였다.1 나아가, 복잡한 비디오 내의 공간 정보와 시간 정보를 더욱 정교하게 분리하고 통합하기 위해 듀얼 브랜치(Dual Branch) 구조와 게이트 클래스 토큰 퓨전(Gated Class Token Fusion, GCTF)이라는 고도화된 메커니즘이 제안되었다.4 본 보고서는 VideoMamba의 핵심 원리부터 시작하여, 이를 확장한 Dual Branch 아키텍처와 GCTF 기술을 심층적으로 분석하고, 이러한 기술이 폭력 감지와 같은 실제 응용 분야에서 어떠한 혁신을 가져오는지 상세히 기술한다.</p>
<h2>2.  이론적 배경: 상태 공간 모델(SSM)과 비디오 모델링의 난제</h2>
<h3>2.1  비디오 모델링의 본질적 어려움</h3>
<p>비디오 데이터 처리가 어려운 근본적인 이유는 ’이중의 딜레마’에 있다. 첫째는 ’국소적 중복성(Local Redundancy)’이다. 인접한 프레임 간에는 픽셀 값의 변화가 크지 않아 정보의 중복이 매우 높다. 둘째는 ’전역적 의존성(Global Dependency)’이다. 비디오의 문맥을 이해하기 위해서는 수 초, 혹은 수 분 떨어진 프레임 간의 인과관계를 파악해야 한다. 기존 CNN은 국소적 중복성을 처리하는 데 효율적이지만 전역적 의존성 파악에 약하고, 트랜스포머는 전역적 의존성은 잘 파악하지만 국소적 중복성을 처리하는 데 과도한 연산을 소모한다. VideoMamba는 이 두 가지 문제를 동시에 해결하기 위해 SSM을 도입한다.3</p>
<h3>2.2  상태 공간 모델(SSM)의 수학적 기초</h3>
<p>SSM은 제어 이론과 신호 처리 분야에서 오랫동안 사용되어 온 연속 시스템 모델링 기법에 뿌리를 두고 있다. 1차원 입력 함수 <span class="math math-inline">x(t) \in \mathbb{R}^L</span>를 은닉 상태 <span class="math math-inline">h(t) \in \mathbb{R}^N</span>를 통해 출력 <span class="math math-inline">y(t) \in \mathbb{R}^L</span>로 매핑하는 과정은 다음과 같은 선형 상미분 방정식(ODE)으로 표현된다.<br />
<span class="math math-display">
h&#39;(t) = \mathbf{A}h(t) + \mathbf{B}x(t)
</span></p>
<p><span class="math math-display">
y(t) = \mathbf{C}h(t)
</span></p>
<p>여기서 <span class="math math-inline">\mathbf{A} \in \mathbb{R}^{N \times N}</span>는 시스템의 상태가 시간에 따라 어떻게 진화하는지를 결정하는 진화 행렬(Evolution Matrix)이며, <span class="math math-inline">\mathbf{B} \in \mathbb{R}^{N \times 1}</span>와 <span class="math math-inline">\mathbf{C} \in \mathbb{R}^{1 \times N}</span>는 각각 입력을 은닉 상태로, 은닉 상태를 출력으로 투영하는 행렬이다.2</p>
<p>딥러닝, 특히 디지털 데이터 처리에 이를 적용하기 위해서는 연속 시스템을 이산화(Discretization)해야 한다. 시간 스텝 <span class="math math-inline">\Delta</span>를 도입하여 영차 홀드(Zero-Order Hold, ZOH) 방식을 적용하면, 연속 파라미터 <span class="math math-inline">(\mathbf{A}, \mathbf{B})</span>는 다음과 같이 이산 파라미터 <span class="math math-inline">(\overline{\mathbf{A}}, \overline{\mathbf{B}})</span>로 변환된다.<br />
<span class="math math-display">
\overline{\mathbf{A}} = \exp(\Delta \mathbf{A})
</span></p>
<p><span class="math math-display">
\overline{\mathbf{B}} = (\Delta \mathbf{A})^{-1}(\exp(\Delta \mathbf{A}) - \mathbf{I}) \cdot \Delta \mathbf{B}
</span></p>
<p>이산화된 식은 다음과 같은 점화식(Recurrence) 형태로 다시 쓸 수 있다.<br />
<span class="math math-display">
h_t = \overline{\mathbf{A}}h_{t-1} + \overline{\mathbf{B}}x_t
</span></p>
<p><span class="math math-display">
y_t = \mathbf{C}h_t
</span></p>
<p>이 형태는 RNN(Recurrent Neural Network)과 유사해 보이지만, SSM은 학습 시에 합성곱(Convolution) 형태로 병렬화하여 계산할 수 있다는 결정적인 장점을 가진다. 그러나 고전적인 SSM(예: LSSL, S4)은 파라미터 <span class="math math-inline">\mathbf{A}, \mathbf{B}, \mathbf{C}</span>가 시간이나 입력에 관계없이 고정된(Time-Invariant) 선형 시스템이라는 한계가 있었다. 이는 입력 데이터의 문맥에 따라 동적으로 반응해야 하는 복잡한 비디오 이해 작업에는 적합하지 않았다.2</p>
<h3>2.3  Mamba의 혁신: 선택적 스캔(Selective Scan)</h3>
<p>Mamba 아키텍처는 기존 SSM의 시불변(Time-Invariant) 제약을 깨고, 파라미터 <span class="math math-inline">\mathbf{B}, \mathbf{C}, \Delta</span>를 입력 <span class="math math-inline">x_t</span>에 의존하는 함수로 만들었다. 이를 ’선택적 SSM(Selective SSM)’이라고 한다. 즉, <span class="math math-inline">B_t, C_t, \Delta_t</span>가 매 시점마다 입력 데이터에 따라 변화한다.<br />
<span class="math math-display">
\overline{\mathbf{B}}_t, \overline{\mathbf{C}}_t, \Delta_t = \text{Linear}(x_t)
</span><br />
이러한 변화는 모델이 입력 시퀀스 내에서 중요한 정보는 기억하고(선택), 불필요한 정보는 무시하거나 잊어버릴 수 있는(여과) 능력을 부여한다. 이는 트랜스포머의 어텐션 메커니즘과 유사한 효과를 내지만, 어텐션이 모든 토큰 쌍의 관계를 계산하여 메모리 맵을 생성해야 하는 것과 달리, Mamba는 순차적인 상태 갱신을 통해 이를 수행하므로 선형 복잡도 <span class="math math-inline">O(N)</span>을 유지한다. Mamba는 GPU 하드웨어에 최적화된 병렬 스캔 알고리즘(Parallel Scan Algorithm)을 통해 이러한 순차적 계산을 매우 빠르게 수행한다.1</p>
<h2>3.  VideoMamba: 3D 비디오를 위한 1D SSM의 확장</h2>
<p>VideoMamba는 1D 시퀀스 모델링에 특화된 Mamba를 3D 비디오 데이터로 확장한 최초의 시도 중 하나이다. 비디오 데이터를 Mamba가 처리할 수 있는 형태로 변환하고, 비디오 고유의 시공간적 특성을 포착하기 위해 VideoMamba는 다음과 같은 핵심 구성 요소를 도입한다.</p>
<h3>3.1  3D 패치 임베딩과 직렬화 (3D Patch Embedding &amp; Serialization)</h3>
<p>비디오 <span class="math math-inline">X \in \mathbb{R}^{3 \times T \times H \times W}</span>는 채널(3), 시간(T), 높이(H), 너비(W)로 구성된 4차원 텐서이다. VideoMamba는 이를 처리하기 위해 먼저 ViT(Vision Transformer)에서 사용하는 패치 임베딩을 3차원으로 확장한 ’튜브릿 임베딩(Tubelet Embedding)’을 수행한다.</p>
<p>비디오를 크기 <span class="math math-inline">t \times h \times w</span>의 겹치지 않는 3D 패치로 나눈 후, 각 패치를 3D Convolution(커널 크기 및 스트라이드 <span class="math math-inline">t \times h \times w</span>)을 통해 <span class="math math-inline">D</span> 차원의 벡터로 투영한다. 결과적으로 비디오는 <span class="math math-inline">L</span>개의 토큰 시퀀스로 변환된다.4<br />
<span class="math math-display">
L = \frac{T}{t} \times \frac{H}{h} \times \frac{W}{w}
</span><br />
변환된 토큰들은 3차원 격자 구조를 가지지만, Mamba에 입력하기 위해서는 1차원 시퀀스로 평탄화(Flattening)되어야 한다. 이때 단순히 순서대로 나열하는 것만으로는 공간적 인접성과 시간적 연속성을 동시에 보존하기 어렵다. VideoMamba는 이를 해결하기 위해 공간 축과 시간 축을 아우르는 스캔 전략을 사용한다.</p>
<h3>3.2  양방향 시공간 SSM (Bidirectional Spatio-Temporal SSM)</h3>
<p>단방향 모델링은 비디오의 문맥을 파악하는 데 불충분하다. 예를 들어, 어떤 동작의 의미는 그 동작의 시작뿐만 아니라 끝을 보아야 정확히 알 수 있는 경우가 많다. 따라서 VideoMamba는 양방향(Bidirectional) Mamba 블록을 핵심 빌딩 블록으로 사용한다.</p>
<p>VideoMamba 블록 내에서 입력 시퀀스는 두 개의 병렬적인 SSM 헤드로 분기되어, 하나는 정방향(Forward)으로, 다른 하나는 역방향(Backward)으로 시퀀스를 스캔한다.<br />
<span class="math math-display">
Z_{forward} = \text{SSM}_{forward}(X_{seq})
</span></p>
<p><span class="math math-display">
Z_{backward} = \text{SSM}_{backward}(\text{Flip}(X_{seq}))
</span></p>
<p><span class="math math-display">
Z_{out} = Z_{forward} + \text{Flip}(Z_{backward})
</span></p>
<p>여기서 중요한 것은 ’어떤 순서로 스캔할 것인가’이다. VideoMamba 연구진은 ‘공간 우선(Spatial-first)’ 스캔과 ‘시간 우선(Temporal-first)’ 스캔을 실험했다.</p>
<ul>
<li><strong>공간 우선 스캔:</strong> 각 프레임 내의 공간 토큰들을 먼저 훑고(행 우선 혹은 열 우선), 다음 프레임으로 넘어가는 방식이다. 이는 ImageNet 등 대규모 2D 이미지 데이터셋으로 사전 학습된 모델의 지식을 전이하는 데 유리하다. 프레임 단위의 공간적 특징을 먼저 견고하게 학습한 후 시간적 연결성을 파악하기 때문이다.2</li>
<li><strong>시간 우선 스캔:</strong> 동일한 공간적 위치에 있는 픽셀들을 시간 축을 따라 먼저 훑는 방식이다.</li>
</ul>
<p>단일 VideoMamba 모델에서는 일반적으로 2D 사전 학습 가중치의 효율적 활용을 위해 공간 우선 스캔이 선호된다.</p>
<h3>3.3  효율성 및 확장성 분석</h3>
<p>VideoMamba의 가장 큰 장점은 압도적인 효율성이다. 실험 결과에 따르면, 64프레임의 비디오를 처리할 때 VideoMamba는 대표적인 비디오 트랜스포머인 TimeSformer 대비 GPU 메모리 사용량을 40배 절감하고, 처리 속도(Throughput)는 6배 향상시켰다.3 이는 <span class="math math-inline">O(N^2)</span>의 복잡도를 가진 어텐션 연산을 <span class="math math-inline">O(N)</span>의 선택적 스캔으로 대체했기 때문이다.</p>
<p>또한, VideoMamba는 ‘자기 증류(Self-Distillation)’ 기법을 도입하여, 방대한 비디오 데이터셋으로 사전 학습하지 않아도 시각적 도메인에서 모델 크기에 따른 성능 확장이 가능함을 보였다. 작은 모델(Teacher)이 학습한 특징을 큰 모델(Student)이 모사하도록 유도함으로써, SSM 기반 모델이 데이터 과적합(Overfitting) 되는 경향을 억제하고 안정적인 학습을 가능하게 했다.3</p>
<h2>4.  Dual Branch VideoMamba: 시공간 분리 및 전문화 전략</h2>
<p>단일 스트림의 VideoMamba도 우수한 성능을 보이지만, 폭력 감지(Violence Detection)와 같이 미세한 공간적 단서(흉기, 표정)와 급격한 시간적 변화(가격, 넘어짐)를 동시에 정밀하게 포착해야 하는 작업에서는 한계가 있을 수 있다. 하나의 1D 시퀀스 내에 공간과 시간 정보를 혼재시키면 정보 간의 간섭이 발생하거나, 모델이 어느 한쪽 정보에 편향될 수 있기 때문이다. 이를 해결하기 위해 제안된 것이 <strong>Dual Branch VideoMamba</strong>이다.4</p>
<h3>4.1  듀얼 브랜치 아키텍처의 설계 철학</h3>
<p>Dual Branch VideoMamba는 인간의 시각 처리 시스템이 ’What 경로(물체 인식)’와 ’Where/How 경로(움직임 인식)’로 나뉘어 있는 것에서 영감을 받았다. 이 모델은 두 개의 독립적인 VideoMamba 파이프라인을 병렬로 배치하여 각각 공간 정보와 시간 정보를 전담하도록 설계되었다.</p>
<ol>
<li><strong>Branch-1 (Spatial-First Scanning):</strong> 이 브랜치는 공간적 특징 추출에 특화된다. 입력 토큰을 공간 축을 우선적으로 스캔하여, 각 프레임 내의 객체, 사람의 자세, 배경의 디테일 등을 정밀하게 모델링한다. 이는 폭력 상황에서 ‘누가’, ‘무엇을’ 가지고 있는지를 파악하는 데 핵심적인 역할을 한다.</li>
<li><strong>Branch-2 (Temporal-First Scanning):</strong> 이 브랜치는 시간적 역동성 포착에 집중한다. 시간 축을 따라 토큰을 스캔하여 프레임 간의 움직임 변화, 속도, 가속도, 동작의 순서 등을 모델링한다. 이는 ’어떤 행동’이 발생했는지, 그 행동이 얼마나 ’격렬한지’를 판단하는 데 필수적이다.4</li>
</ol>
<h3>4.2  크로핑 모듈 (Cropping Module)</h3>
<p>폭력 감지 작업의 특성상, 전체 화면의 90% 이상을 차지하는 배경 정보는 노이즈가 될 수 있다. Dual Branch VideoMamba는 입력 단계에서 사람 감지(Human Detection) 알고리즘을 통해 사람이 존재하는 영역을 식별하고, 해당 영역을 중심으로 영상을 크로핑하여 각 브랜치에 입력한다. 이는 모델이 배경의 변화가 아닌 사람의 행동 자체에 집중하도록 강제하는 주의 집중(Attention) 효과를 낸다.4</p>
<h2>5.  게이트 클래스 토큰 퓨전 (Gated Class Token Fusion, GCTF)</h2>
<p>Dual Branch 구조의 핵심 과제는 “서로 다른 관점에서 추출된 두 정보를 어떻게 효과적으로 결합할 것인가“이다. 기존의 많은 멀티스트림 모델들은 네트워크의 마지막 단에서 특징을 합치는 ‘Late Fusion’ 방식을 사용했다. 그러나 이는 층(Layer)을 거치면서 손실될 수 있는 중간 단계의 상호보완적 정보를 활용하지 못한다는 단점이 있다. Senadeera 등(2025)이 제안한 GCTF는 네트워크의 <strong>모든 레이어</strong>에서 지속적으로 정보를 교환하고 융합하는 메커니즘이다.4</p>
<h3>5.1  GCTF의 수학적 정식화</h3>
<p>GCTF는 각 VideoMamba 블록의 출력단에 위치하며, 두 브랜치의 클래스 토큰(Class Token, CLS)을 교환한다. 클래스 토큰은 해당 레이어까지 처리된 시퀀스의 전체적인 의미 정보를 압축하고 있는 벡터이다.</p>
<p><span class="math math-inline">l</span>번째 레이어에서의 퓨전 과정은 다음과 같다.</p>
<p>먼저, 두 브랜치의 출력 CLS 토큰을 정의한다.</p>
<ul>
<li><span class="math math-inline">CLS^1_l \in \mathbb{R}^d</span>: 공간 우선 브랜치(Branch-1)의 출력 CLS 토큰.</li>
<li><span class="math math-inline">CLS^2_l \in \mathbb{R}^d</span>: 시간 우선 브랜치(Branch-2)의 출력 CLS 토큰.</li>
</ul>
<p>단순히 두 토큰을 더하거나 연결하는 대신, 학습 가능한 파라미터 벡터 <span class="math math-inline">\sigma_l \in \mathbb{R}^d</span>를 도입하여 정보의 혼합 비율을 동적으로 조절한다. 게이트 값 <span class="math math-inline">\sigma&#39;_l</span>은 시그모이드 함수를 통해 0과 1 사이의 값으로 정규화된다.</p>
<p><span class="math math-display">\sigma&#39;_l = \text{Sigmoid}(\sigma_l)</span></p>
<p>이 게이트 값을 이용하여 두 브랜치의 정보를 융합한 새로운 CLS 토큰 <span class="math math-inline">CLS^{fused}_2(l)</span>을 생성한다. 연구에 따르면, 주로 공간 정보를 시간 브랜치에 주입하여 시간적 추론을 보강하는 형태가 효과적이다.<br />
<span class="math math-display">
CLS^{fused}_2(l) = \sigma&#39;_l \odot CLS^2_l + (1 - \sigma&#39;_l) \odot CLS^1_l
</span><br />
여기서 <span class="math math-inline">\odot</span>은 요소별 곱(Element-wise Multiplication)을 의미한다.</p>
<h3>5.2 GCTF 메커니즘의 심층 분석 및 작동 원리</h3>
<p>이 수식은 단순해 보이지만 매우 깊은 함의를 지닌다.</p>
<ol>
<li><strong>채널별 적응적 선택 (Channel-wise Adaptive Selection):</strong> <span class="math math-inline">\sigma&#39;_l</span>은 벡터이므로, <span class="math math-inline">d</span>개의 채널 각각에 대해 서로 다른 융합 비율을 적용할 수 있다. 즉, 모델은 학습을 통해 어떤 특징 차원에서는 자신의 정보(시간 정보, <span class="math math-inline">CLS^2_l</span>)를 유지하는 것이 유리하고, 어떤 차원에서는 상대방의 정보(공간 정보, <span class="math math-inline">CLS^1_l</span>)를 가져오는 것이 유리한지를 스스로 판단한다. 예를 들어, 움직임이 모호한 상황에서는 공간적 특징(예: 손에 쥔 물체)의 비중을 높여 상황 판단을 돕는다.4</li>
<li><strong>연속적 상호작용 (Continuous Interaction):</strong> 퓨전이 단 한 번 일어나는 것이 아니라 <span class="math math-inline">L</span>개의 모든 블록 사이에서 반복적으로 발생한다. 이는 초기 레이어(저수준 특징)부터 깊은 레이어(고수준 의미론적 특징)까지 지속적으로 정보가 교정(Rectify)되고 정제(Refine)됨을 의미한다. 시간 브랜치는 공간 브랜치의 도움을 받아 매 단계마다 자신의 특징 표현을 개선할 수 있다. 이는 한쪽 모달리티에 조기에 과적합되는 것을 방지하고, 두 정보가 유기적으로 얽힌 풍부한 표현(Representation)을 학습하게 한다.5</li>
<li><strong>학습 가능한 게이트:</strong> <span class="math math-inline">\sigma_l</span>은 역전파(Backpropagation)를 통해 학습되는 파라미터이다. 따라서 데이터셋의 특성에 따라 최적의 융합 전략이 자동으로 수립된다. 정적인 퓨전 방식(단순 합, 평균)과는 달리 데이터 주도적인(Data-driven) 유연성을 제공한다.</li>
</ol>
<h3>5.3 최종 퓨전 블록 (Final Fusion Block)</h3>
<p>모든 레이어를 거친 후, 마지막 단계에서는 두 브랜치의 최종 CLS 토큰들을 연결(Concatenation)하여 최종 특징 벡터 <span class="math math-inline">Z</span>를 생성하고, 이를 완전 연결 층(Fully Connected Layer)에 통과시켜 최종 분류 확률을 계산한다.<br />
<span class="math math-display">
Z = \text{Concat}(CLS^1_{final}, CLS^2_{final})
</span></p>
<p><span class="math math-display">
P = \text{Softmax}(\text{Linear}(Z))
</span></p>
<h2>6. 실험 결과 및 성능 분석</h2>
<p>Dual Branch VideoMamba와 GCTF의 효용성은 감시 카메라 환경의 폭력 감지 벤치마크에서 입증되었다. RWF-2000, RLVS, VioPeru 등 다양한 데이터셋을 통합한 대규모 실험 환경에서 모델의 성능이 평가되었다.4</p>
<h3>6.1 비교 실험 결과</h3>
<p>다음은 주요 비디오 이해 아키텍처들과의 성능 비교 결과이다.</p>
<table><thead><tr><th><strong>모델 아키텍처</strong></th><th><strong>파라미터 수 (Params, M)</strong></th><th><strong>연산량 (FLOPs, G)</strong></th><th><strong>Top-1 정확도 (%)</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>SlowFast (3D CNN)</strong></td><td>60</td><td>234</td><td>76.37</td><td>CNN 기반 베이스라인</td></tr>
<tr><td><strong>VideoSwin-B (Transformer)</strong></td><td>88</td><td>282</td><td>80.47</td><td>윈도우 기반 어텐션</td></tr>
<tr><td><strong>UniFormer-V2 (Hybrid)</strong></td><td>354</td><td>6108</td><td>92.51</td><td>CNN과 트랜스포머 결합</td></tr>
<tr><td><strong>VideoMamba (SSM)</strong></td><td>74</td><td>806</td><td>94.39</td><td>단일 브랜치 모델</td></tr>
<tr><td><strong>CUE-Net (Hybrid)</strong></td><td>354</td><td>5826</td><td>95.91</td><td>기존 SOTA (무거운 연산)</td></tr>
<tr><td><strong>Dual Branch VideoMamba</strong></td><td><strong>154.3</strong></td><td><strong>1612</strong></td><td><strong>96.37</strong></td><td><strong>본 아키텍처 (GCTF 적용)</strong></td></tr>
</tbody></table>
<p><strong>결과 분석 및 통찰:</strong></p>
<ol>
<li><strong>SOTA 달성:</strong> Dual Branch VideoMamba는 96.37%의 정확도를 기록하며 기존 최고 성능 모델인 CUE-Net을 능가했다. 이는 순수 SSM 기반 모델이 복잡한 하이브리드(CNN+Transformer) 모델보다 더 우수한 표현력을 가질 수 있음을 증명한다.</li>
<li><strong>압도적인 효율성:</strong> CUE-Net과 비교했을 때, 파라미터 수는 절반 이하(43%), 연산량(FLOPs)은 1/4 수준(27%)에 불과하다. 1612 GFLOPs라는 수치는 여전히 절대적으로 작지 않지만, 경쟁 모델인 5800~6100 GFLOPs 대역의 모델들과 비교하면 혁신적인 경량화를 이룬 것이다. 이는 GCTF를 통한 효율적인 정보 통합 덕분에 더 적은 연산으로도 더 높은 정확도를 달성할 수 있음을 보여준다.4</li>
<li><strong>단일 vs 듀얼:</strong> 단일 VideoMamba(94.39%) 대비 Dual Branch 모델(96.37%)은 약 2%의 성능 향상을 보였다. 파라미터 수가 2배로 증가했지만, 정확도 향상 폭이 이를 상회하는 가치를 제공한다. 특히 폭력 감지와 같이 오탐(False Positive)이나 미탐(False Negative)이 치명적인 보안 응용 분야에서는 이러한 정확도 향상이 매우 중요하다.</li>
</ol>
<h3>6.2 절제 연구 (Ablation Study)</h3>
<p>GCTF 메커니즘의 기여도를 확인하기 위한 실험에서도 흥미로운 결과가 도출되었다.</p>
<ul>
<li><strong>GCTF 제거 시:</strong> 두 브랜치를 독립적으로 학습시키고 마지막에만 결합(Late Fusion)했을 경우, GCTF 적용 모델 대비 정확도가 0.7% ~ 1.0% 포인트 하락했다. 이는 중간 레이어에서의 지속적인 상호작용이 학습 효율과 최종 성능에 결정적인 영향을 미친다는 것을 입증한다.</li>
<li><strong>브랜치 제거 시:</strong> 공간 브랜치나 시간 브랜치 중 하나만 사용했을 때의 성능은 듀얼 브랜치 결합 모델보다 현저히 낮았다. 이는 폭력 감지 작업에서 시공간 정보가 상호 의존적이며 불가분의 관계임을 시사한다.9</li>
</ul>
<h3>6.3 시각화 분석 (Visualization)</h3>
<p>클래스 활성화 맵(Class Activation Map, CAM)을 통한 시각화 분석에서, Dual Branch VideoMamba는 폭력이 발생하는 핵심 영역(예: 타격하는 손, 흉기, 넘어지는 사람)에 정확히 초점을 맞추는 것으로 나타났다. 반면 단일 브랜치 모델이나 기존 CNN 모델은 배경의 움직임이나 관련 없는 객체에 주의가 분산되는 경향을 보였다. GCTF가 모델로 하여금 ’어디를 봐야 하는지(Spatial)’와 ’언제 봐야 하는지(Temporal)’를 서로 가이드해주며 주의 집중력을 강화시킨 결과로 해석된다.4</p>
<h2>7. 결론 및 향후 전망</h2>
<p>VideoMamba는 선형 복잡도를 가진 상태 공간 모델(SSM)을 비디오 도메인에 성공적으로 안착시키며, 트랜스포머가 지배하던 비디오 이해 분야에 새로운 가능성을 제시했다. 3D 패치 임베딩과 양방향 시공간 스캔을 통해 비디오의 4차원적 특성을 1D 시퀀스 모델링으로 효율적으로 변환하였으며, 이를 통해 고해상도 장기 비디오 처리의 연산 장벽을 허물었다.</p>
<p>특히 본 보고서에서 중점적으로 다룬 <strong>Dual Branch VideoMamba와 GCTF</strong> 기술은 단순한 아키텍처 확장을 넘어선 질적인 도약을 보여준다.</p>
<ol>
<li><strong>구조적 혁신:</strong> 공간과 시간 정보를 물리적으로 분리된 브랜치에서 전문적으로 처리하게 함으로써 각 모달리티의 특징 추출 능력을 극대화했다.</li>
<li><strong>융합의 고도화:</strong> GCTF를 통해 분리된 정보를 레이어 단위에서 적응적으로, 그리고 연속적으로 재결합함으로써 시공간 정보의 시너지를 창출했다. 이는 ‘분리 후 통합(Divide and Conquer)’ 전략의 모범적인 사례라 할 수 있다.</li>
<li><strong>실용적 가치:</strong> 높은 정확도와 낮은 연산 비용을 동시에 달성함으로써, 실시간 지능형 감시 시스템(Intelligent Surveillance System) 등 실제 산업 현장에서의 상용화 가능성을 크게 높였다.</li>
</ol>
<p>향후 연구는 이러한 SSM 기반 비디오 모델을 텍스트, 오디오 등 다른 모달리티와 결합한 멀티모달(Multimodal) 모델로 확장하거나, 생성형 AI(Generative AI)에 적용하여 비디오 생성 및 편집 분야로 응용 범위를 넓히는 방향으로 진행될 것으로 전망된다. VideoMamba와 GCTF는 그 여정에서 중요한 이정표이자 강력한 베이스라인이 될 것이다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>VideoMamba: State Space Model for Efficient Video Understanding, 12월 25, 2025에 액세스, https://arxiv.org/abs/2403.06977</li>
<li>VideoMamba: Spatio-Temporal Selective State Space Model, 12월 25, 2025에 액세스, https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03565.pdf</li>
<li>VideoMamba: State Space Model for Efficient Video Understanding, 12월 25, 2025에 액세스, https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03773.pdf</li>
<li>(PDF) Dual Branch VideoMamba with Gated Class Token Fusion for …, 12월 25, 2025에 액세스, https://www.researchgate.net/publication/392405511_Dual_Branch_VideoMamba_with_Gated_Class_Token_Fusion_for_Violence_Detection</li>
<li>Dual Branch VideoMamba with Gated Class Token Fusion for … - arXiv, 12월 25, 2025에 액세스, https://arxiv.org/html/2506.03162v2</li>
<li>VideoMamba: State Space Model for Efficient Video Understanding, 12월 25, 2025에 액세스, https://arxiv.org/html/2403.06977v2</li>
<li>VideoMamba: State Space Model for Efficient Video Understanding, 12월 25, 2025에 액세스, https://huggingface.co/blog/vladbogo/video-mamba</li>
<li>VideoMamba: State Space Model for Efficient Video Understanding, 12월 25, 2025에 액세스, https://arxiv.org/html/2403.06977v1</li>
<li>Dual-Branch Modeling Architecture - Emergent Mind, 12월 25, 2025에 액세스, https://www.emergentmind.com/topics/dual-branch-modeling-architecture</li>
<li>CUE-Net: Violence Detection Video Analytics with Spatial Cropping …, 12월 25, 2025에 액세스, https://www.researchgate.net/publication/384419834_CUE-Net_Violence_Detection_Video_Analytics_with_Spatial_Cropping_Enhanced_UniformerV2_and_Modified_Efficient_Additive_Attention</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>