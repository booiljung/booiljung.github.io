<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:5.1 SSM과 선형 어텐션(Linear Attention)의 수학적 연결 (SSD 프레임워크)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>5.1 SSM과 선형 어텐션(Linear Attention)의 수학적 연결 (SSD 프레임워크)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">MAMBA - 포스트 트랜스포머 시대의 도래</a> / <span>5.1 SSM과 선형 어텐션(Linear Attention)의 수학적 연결 (SSD 프레임워크)</span></nav>
                </div>
            </header>
            <article>
                <h1>5.1 SSM과 선형 어텐션(Linear Attention)의 수학적 연결 (SSD 프레임워크)</h1>
<p>인공지능, 특히 시퀀스 모델링(Sequence Modeling) 분야는 오랜 기간 두 가지의 상반된 패러다임이 지배해 왔다. 하나는 입력 시퀀스의 모든 요소가 서로 직접 상호작용하는 <strong>어텐션(Attention)</strong> 기반의 트랜스포머(Transformer)이고, 다른 하나는 시간을 따라 은닉 상태(Hidden State)를 갱신하며 정보를 압축 전달하는 <strong>상태 공간 모델(State Space Model, SSM)</strong> 또는 순환 신경망(RNN)이다. 이 두 접근 방식은 각각 학습 효율성과 추론 효율성이라는 서로 다른 장점을 점유하며 양립 불가능한 것으로 여겨져 왔다. 트랜스포머는 <span class="math math-inline">O(T^2)</span>의 비용으로 전체 문맥을 파악하는 데 탁월했으나 긴 시퀀스에서의 추론이 느렸고, SSM은 <span class="math math-inline">O(T)</span>의 선형 복잡도로 빠른 추론이 가능했으나 학습 시 GPU 병렬화가 어렵거나 정보의 장기 의존성(Long-range Dependency) 포착에 한계를 보였다.1</p>
<p>본 절에서는 Mamba-2 아키텍처의 이론적 기반이 되는 <strong>구조적 상태 공간 쌍대성(Structured State Space Duality, SSD)</strong> 프레임워크를 통해 이 두 가지 패러다임이 수학적으로 동치임을 증명한다. SSD는 단순한 수식의 변형을 넘어, SSM이 ’구조화된 마스크를 가진 선형 어텐션(Structured Masked Attention)’의 특수한 형태임을 규명하고, 이를 통해 트랜스포머의 시스템적 최적화 기법들을 SSM에 이식할 수 있는 이론적 토대를 제공한다. 이 논의는 시퀀스 변환(Sequence Transformation)을 바라보는 관점을 근본적으로 재정립하는 과정이다.</p>
<h2>1.  시퀀스 변환의 이중적 관점: 순환과 행렬</h2>
<p>시퀀스 모델의 본질은 입력 시퀀스 <span class="math math-inline">x \in \mathbb{R}^{T \times P}</span>를 출력 시퀀스 <span class="math math-inline">y \in \mathbb{R}^{T \times P}</span>로 매핑하는 변환 함수 <span class="math math-inline">\Phi</span>를 학습하는 것이다. 심층 신경망의 역사에서 이 변환 <span class="math math-inline">\Phi</span>를 구현하는 방식은 크게 ’상태의 순환’과 ’행렬의 곱’이라는 두 가지 수학적 형태로 발전해 왔다.3 SSD 프레임워크는 이 두 가지 형태가 **반분리 행렬(Semiseparable Matrix)**이라는 수학적 객체를 매개로 하여 사실상 동일한 연산임을 보인다.</p>
<h3>1.1  순환적 관점 (The Recurrent View)</h3>
<p>전통적인 SSM, 그리고 Mamba-1에서 사용된 선택적(Selective) SSM은 연속 시간 시스템의 이산화(Discretization)를 통해 얻어진 순환식(Recurrence)으로 정의된다. 이는 시스템의 현재 상태가 바로 직전의 상태와 현재의 입력에 의해 결정된다는 마르코프적(Markovian) 성질에 기반한다. 시간 <span class="math math-inline">t</span>에서의 입력 <span class="math math-inline">x_t \in \mathbb{R}^P</span>, 은닉 상태 <span class="math math-inline">h_t \in \mathbb{R}^N</span>, 출력 <span class="math math-inline">y_t \in \mathbb{R}^P</span>에 대해 SSM은 다음과 같은 선형 점화식(Linear Recurrence)을 따른다.<br />
<span class="math math-display">
\begin{aligned} h_t &amp;= A_t h_{t-1} + B_t x_t \\ y_t &amp;= C_t^\top h_t \end{aligned}
</span><br />
여기서 <span class="math math-inline">A_t \in \mathbb{R}^{N \times N}</span>은 상태 전이 행렬(State Transition Matrix), <span class="math math-inline">B_t \in \mathbb{R}^{N \times P}</span>는 입력 행렬, <span class="math math-inline">C_t \in \mathbb{R}^{N \times P}</span>는 출력 행렬을 의미하며, <span class="math math-inline">N</span>은 상태 차원(State Dimension)이다.5 이 식은 시변(Time-varying) 파라미터를 허용하는 선택적 SSM의 일반형이다.</p>
<p>순환적 관점의 가장 큰 특징은 **상태(State)**의 존재다. 모델은 과거의 모든 정보를 <span class="math math-inline">N</span>차원의 고정된 크기의 벡터 <span class="math math-inline">h_t</span>로 압축하여 저장한다. 이는 추론 시 이전 시점의 상태 <span class="math math-inline">h_{t-1}</span>과 현재의 입력 <span class="math math-inline">x_t</span>만 있으면 즉시 다음 출력을 계산할 수 있게 하여, 시퀀스 길이에 무관한 상수 시간 <span class="math math-inline">O(1)</span>의 추론 속도와 상수 메모리 사용량을 보장한다. 그러나 학습 시에는 <span class="math math-inline">t=0</span>부터 <span class="math math-inline">t=T</span>까지 순차적으로 계산을 수행해야 하므로, GPU와 같은 병렬 처리 하드웨어의 성능을 온전히 활용하기 어렵다는 단점이 존재한다.2</p>
<h3>1.2  행렬 변환 관점 (The Matrix Transformation View)</h3>
<p>반면, 트랜스포머의 어텐션 메커니즘은 상태를 명시적으로 유지하지 않고 시퀀스 전체를 한 번에 처리하는 전역적(Global) 접근 방식을 취한다. 시퀀스 길이 <span class="math math-inline">T</span>에 대해 전체 입력 <span class="math math-inline">X \in \mathbb{R}^{T \times P}</span>를 출력 <span class="math math-inline">Y \in \mathbb{R}^{T \times P}</span>로 변환하는 과정은 거대한 “토큰 믹싱(Token Mixing)” 행렬 <span class="math math-inline">M \in \mathbb{R}^{T \times T}</span>과의 행렬 곱으로 표현된다.<br />
<span class="math math-display">
Y = M X
</span><br />
어텐션 메커니즘에서 믹싱 행렬 <span class="math math-inline">M</span>은 쿼리(<span class="math math-inline">Q</span>)와 키(<span class="math math-inline">K</span>)의 내적에 소프트맥스(Softmax) 함수를 적용하여 생성된다 (<span class="math math-inline">M = \text{softmax}(QK^\top)</span>). 언어 모델링과 같이 미래의 정보를 참조할 수 없는 인과적(Causal) 설정에서는 <span class="math math-inline">M</span>이 하삼각 행렬(Lower Triangular Matrix)의 형태를 띤다. 즉, <span class="math math-inline">i &lt; j</span> 인 경우 <span class="math math-inline">M_{ij} = 0</span>이 되어 미래의 토큰이 현재에 영향을 주지 못하게 한다.2</p>
<p>행렬 변환 관점은 전체 계산을 행렬 곱(Matrix Multiplication, MatMul)으로 표현하므로 GPU의 텐서 코어(Tensor Core)를 활용한 병렬 처리에 최적화되어 있다. 그러나 시퀀스 길이 <span class="math math-inline">T</span>가 길어질수록 <span class="math math-inline">M</span> 행렬의 크기가 <span class="math math-inline">T \times T</span>로 급격히 커지며, 이를 메모리에 올리거나 연산하는 비용이 <span class="math math-inline">O(T^2)</span>로 증가한다는 치명적인 한계가 있다.</p>
<p>Dao와 Gu는 SSM의 순환식을 시간 축에 대해 전개(Unrolling)함으로써, SSM 역시 <span class="math math-inline">Y=MX</span> 형태의 행렬 변환으로 정확히 표현될 수 있음을 보였다. 즉, 순환 모델을 풀어서 쓰면 특정한 구조를 가진 행렬 연산이 된다는 것이다.1 이 연결고리가 바로 SSD 프레임워크의 출발점이다.</p>
<h2>2.  반분리 행렬(Semiseparable Matrix)과 구조적 등가성</h2>
<p>SSM과 어텐션의 수학적 연결을 이해하기 위한 핵심 도구는 **반분리 행렬(Semiseparable Matrix)**이다. SSD 프레임워크는 모든 선형 상태 공간 모델이 반분리 행렬 구조를 가진 행렬 변환과 동치임을 증명하며, 이는 SSM의 효율성을 설명하는 수학적 근거가 된다.5</p>
<h3>2.1  SSM의 행렬화 유도</h3>
<p>SSM의 점화식 <span class="math math-inline">h_t = A_t h_{t-1} + B_t x_t</span>를 <span class="math math-inline">t=0</span>부터 순차적으로 대입하여 전개해 보자. 초기 상태 <span class="math math-inline">h_{-1}=0</span>이라 가정할 때, 각 시간 단계의 상태는 다음과 같이 표현된다.<br />
<span class="math math-display">
\begin{aligned} h_0 &amp;= B_0 x_0 \\ h_1 &amp;= A_1 h_0 + B_1 x_1 = A_1 B_0 x_0 + B_1 x_1 \\ h_2 &amp;= A_2 h_1 + B_2 x_2 = A_2 (A_1 B_0 x_0 + B_1 x_1) + B_2 x_2 = A_2 A_1 B_0 x_0 + A_2 B_1 x_1 + B_2 x_2 \\ &amp;\vdots \\ h_t &amp;= \sum_{s=0}^t \left( \prod_{k=s+1}^t A_k \right) B_s x_s \end{aligned}
</span><br />
출력 <span class="math math-inline">y_t = C_t^\top h_t</span>를 구하기 위해 위 식을 대입하면, <span class="math math-inline">y_t</span>는 과거의 모든 입력 <span class="math math-inline">x_s (s \le t)</span>에 대한 선형 결합으로 표현된다. 이를 전체 시퀀스 변환 <span class="math math-inline">Y=MX</span>로 나타낼 때, 행렬 <span class="math math-inline">M</span>의 <span class="math math-inline">(i, j)</span> 번째 원소(여기서 <span class="math math-inline">i</span>는 출력 시점, <span class="math math-inline">j</span>는 입력 시점)는 다음과 같은 닫힌 형태(Closed Form)를 갖는다.<br />
<span class="math math-display">
M_{ij} =  \begin{cases}  C_i^\top A_{i:j}^\times B_j &amp; \text{if } i \ge j \\ 0 &amp; \text{if } i &lt; j \end{cases}
</span><br />
여기서 <span class="math math-inline">A_{i:j}^\times</span>는 <span class="math math-inline">A_i</span>부터 <span class="math math-inline">A_{j+1}</span>까지의 행렬 곱 <span class="math math-inline">A_i A_{i-1} \dots A_{j+1}</span>을 의미한다 (<span class="math math-inline">i=j</span>일 때 <span class="math math-inline">A_{i:i}^\times = I</span>).5 이 행렬 <span class="math math-inline">M</span>을 구체적으로 나열하면 다음과 같은 하삼각 행렬이 된다.<br />
<span class="math math-display">
M = \begin{pmatrix} C_0^\top B_0 &amp; 0 &amp; 0 &amp; \dots &amp; 0 \\ C_1^\top A_1 B_0 &amp; C_1^\top B_1 &amp; 0 &amp; \dots &amp; 0 \\ C_2^\top A_2 A_1 B_0 &amp; C_2^\top A_2 B_1 &amp; C_2^\top B_2 &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ C_{T-1}^\top A_{T-1:1}^\times B_0 &amp; C_{T-1}^\top A_{T-1:2}^\times B_1 &amp; \dots &amp; \dots &amp; C_{T-1}^\top B_{T-1} \end{pmatrix}
</span></p>
<h3>2.2  반분리 구조의 정의와 랭크 성질</h3>
<p>위와 같은 구조를 선형대수학에서는 **반분리 행렬(Semiseparable Matrix)**이라 한다. 구체적으로, <span class="math math-inline">N</span>-반분리 행렬(<span class="math math-inline">N</span>-Semiseparable Matrix) 또는 <span class="math math-inline">N</span>-순차적 반분리 행렬(<span class="math math-inline">N</span>-Sequentially Semiseparable, SSS)은 그 하삼각 부분(대각 성분 포함)에 포함된 모든 부분행렬(Submatrix)의 랭크(Rank)가 최대 <span class="math math-inline">N</span>인 행렬로 정의된다.5</p>
<p>이 정의는 SSM의 상태 차원 <span class="math math-inline">N</span>과 직접적으로 연결된다.</p>
<ul>
<li><strong>정보 압축의 의미:</strong> SSM은 과거의 무한한 정보를 <span class="math math-inline">N</span>차원의 상태 벡터 <span class="math math-inline">h_t</span>로 압축하여 전달한다. 행렬 관점에서 이는 입력 시퀀스의 과거 구간과 출력 시퀀스의 미래 구간을 연결하는 정보의 통로가 <span class="math math-inline">N</span>차원으로 제한됨을 의미한다. 즉, 행렬 <span class="math math-inline">M</span>의 임의의 오프-다이아고날(Off-diagonal) 블록은 랭크가 <span class="math math-inline">N</span>을 넘을 수 없다.</li>
<li><strong>파라미터 효율성:</strong> 일반적인 <span class="math math-inline">T \times T</span> 행렬은 <span class="math math-inline">T^2</span>개의 자유도를 가지지만, <span class="math math-inline">N</span>-반분리 행렬은 <span class="math math-inline">O(TN)</span>개의 파라미터만으로 표현 가능하다. 이는 SSM이 긴 시퀀스를 선형 시간(<span class="math math-inline">O(T)</span>)에 처리할 수 있는 수학적 근거가 된다.10</li>
</ul>
<p>SSD 프레임워크는 “상태 차원이 <span class="math math-inline">N</span>인 모든 선형 시불변/시변 SSM은 <span class="math math-inline">N</span>-반분리 행렬 변환과 동치이며, 역으로 <span class="math math-inline">N</span>-반분리 행렬은 <span class="math math-inline">N</span>차원 SSM으로 표현 가능하다“는 정리를 통해 두 모델의 등가성을 확립한다.8</p>
<h2>3.  SSD의 핵심: 스칼라 구조와 선형 어텐션의 유도</h2>
<p>Mamba-2가 제안하는 구조적 상태 공간 쌍대성(SSD)의 가장 혁신적인 지점은 행렬 <span class="math math-inline">A</span>의 구조에 제약을 가함으로써 SSM을 **선형 어텐션(Linear Attention)**의 형태로 완벽하게 변환하는 것이다. 구체적으로 Mamba-2는 상태 전이 행렬 <span class="math math-inline">A_t</span>를 대각 행렬(Diagonal Matrix) 중에서도 가장 단순한 형태인 <strong>스칼라-항등 행렬(Scalar-times-Identity)</strong> 구조로 제한한다.3</p>
<h3>3.1  스칼라 구조 (Scalar Structured SSM)</h3>
<p>Mamba-2의 SSD 계층에서는 <span class="math math-inline">A_t = a_t I</span> (여기서 <span class="math math-inline">a_t \in \mathbb{R}</span>)로 설정한다. 즉, 상태 벡터의 모든 차원(<span class="math math-inline">N</span>)이 동일한 감쇠(Decay) 비율 <span class="math math-inline">a_t</span>를 공유한다고 가정한다. 이 제약 조건 하에서 행렬 <span class="math math-inline">A</span>는 교환 법칙(Commutativity)이 성립하므로, 행렬 곱 <span class="math math-inline">A_{i:j}^\times</span>는 스칼라들의 곱 <span class="math math-inline">a_{i:j}^\times = a_i \dots a_{j+1}</span>에 항등 행렬 <span class="math math-inline">I</span>를 곱한 형태가 된다. 따라서 행렬 <span class="math math-inline">M</span>의 원소 <span class="math math-inline">M_{ij}</span>는 다음과 같이 벡터와 스칼라의 곱으로 분해될 수 있다.<br />
<span class="math math-display">
M_{ij} = C_i^\top (a_{i:j}^\times I) B_j = (a_{i:j}^\times) (C_i^\top B_j)
</span><br />
이 식은 두 개의 독립적인 항의 원소별 곱(Element-wise Product)으로 표현된다.</p>
<ol>
<li><strong>스칼라 마스크 항 (<span class="math math-inline">L_{ij}</span>):</strong> <span class="math math-inline">L_{ij} = a_{i:j}^\times = a_i \times \dots \times a_{j+1}</span>. 이는 시퀀스 위치 간의 거리에 따른 정보 감쇠(Decay)를 나타내며, 어텐션에서의 마스크(Mask) 역할을 한다.</li>
<li><strong>토큰 상호작용 항 (<span class="math math-inline">P_{ij}</span>):</strong> <span class="math math-inline">P_{ij} = C_i^\top B_j</span>. 이는 입력 <span class="math math-inline">B_j</span>와 출력 투영 <span class="math math-inline">C_i</span> 간의 내적으로, 어텐션에서의 쿼리(<span class="math math-inline">Q</span>)와 키(<span class="math math-inline">K</span>)의 내적(<span class="math math-inline">QK^\top</span>)과 정확히 대응된다.</li>
</ol>
<p>이러한 분해는 전체 믹싱 행렬 <span class="math math-inline">M</span>을 두 행렬의 아다마르 곱(Hadamard Product, <span class="math math-inline">\circ</span>)으로 표현할 수 있게 한다.5<br />
<span class="math math-display">
M = L \circ (C B^\top)
</span><br />
여기서 <span class="math math-inline">L</span>은 **1-반분리 행렬(1-Semiseparable Matrix)**이 된다. <span class="math math-inline">A_t</span>가 스칼라이므로 구조적으로는 랭크가 1인 반분리 행렬의 특성을 가지지만, 실제 데이터(<span class="math math-inline">B, C</span>)는 <span class="math math-inline">N</span>차원 벡터 공간에서 상호작용하므로 모델의 표현력은 유지된다. 이를 “헤드 차원(Head Dimension)” <span class="math math-inline">P</span>로 확장하여 해석하면, <span class="math math-inline">N</span>차원의 상태 공간을 가지면서도 계산적으로는 1-반분리 구조의 효율성을 취할 수 있게 된다.3</p>
<h4>3.1.1  선형 어텐션과의 매핑 (The Mapping)</h4>
<p>위에서 유도한 <span class="math math-inline">M = L \circ (C B^\top)</span> 식을 사용하여 전체 출력 <span class="math math-inline">Y</span>를 구하면 다음과 같다.<br />
<span class="math math-display">
Y = (L \circ (C B^\top)) X
</span><br />
이 식은 **선형 어텐션(Linear Attention)**의 일반적인 형태와 놀랍도록 유사하다. Katharopoulos 등(2020)이 제안한 선형 어텐션은 소프트맥스를 커널 특징 맵(Feature Map) <span class="math math-inline">\phi(\cdot)</span>으로 대체하여 <span class="math math-inline">Y = (\phi(Q) \phi(K)^\top) V</span>로 계산한다. Mamba-2의 SSD 수식에서 각 항을 어텐션의 용어로 치환하면 다음과 같은 1:1 대응 관계가 성립한다.2</p>
<table><thead><tr><th><strong>SSM 파라미터</strong></th><th><strong>어텐션 파라미터</strong></th><th><strong>역할 및 의미</strong></th></tr></thead><tbody>
<tr><td><strong><span class="math math-inline">B</span></strong> (Input Matrix)</td><td><strong><span class="math math-inline">K</span></strong> (Key)</td><td>입력을 기억 공간으로 투영 (Write Key)</td></tr>
<tr><td><strong><span class="math math-inline">C</span></strong> (Output Matrix)</td><td><strong><span class="math math-inline">Q</span></strong> (Query)</td><td>기억 공간에서 정보를 조회 (Read Query)</td></tr>
<tr><td><strong><span class="math math-inline">X</span></strong> (Input Sequence)</td><td><strong><span class="math math-inline">V</span></strong> (Value)</td><td>실제 전달되는 값 (Content)</td></tr>
<tr><td><strong><span class="math math-inline">A</span></strong> (State Matrix)</td><td><strong>Positional Mask</strong></td><td>위치 간 정보 전달의 감쇠율 및 인과성 제어</td></tr>
<tr><td><strong><span class="math math-inline">L</span></strong> (1-SS Matrix)</td><td><strong>Attention Mask</strong></td><td>구조화된 마스크 (<span class="math math-inline">M_{ij}</span>의 가중치 결정)</td></tr>
</tbody></table>
<p>따라서 Mamba-2의 SSD 계층은 **“구조화된 마스크를 가진 선형 어텐션(Structured Masked Attention)”**으로 재정의될 수 있다.5<br />
<span class="math math-display">
Y = \text{SSM}(A, B, C)(X) \iff Y = \text{LinearAttention}(Q=C, K=B, V=X, \text{Mask}=L)
</span><br />
이러한 쌍대성은 SSM이 더 이상 RNN의 아류가 아니라, 어텐션 메커니즘의 특수한(그리고 효율적인) 형태임을 수학적으로 증명한다. 차이점은 기존의 어텐션이 <span class="math math-inline">softmax(QK^\top)</span>와 같이 데이터에 의존적인 비선형 정규화를 사용하여 <span class="math math-inline">O(T^2)</span> 비용을 유발하는 반면, SSM은 구조화된 1-반분리 마스크 <span class="math math-inline">L</span>을 사용하여 <span class="math math-inline">O(T)</span> 비용으로 유사한 효과를 낸다는 점이다.</p>
<h3>3.2  텐서 축약(Tensor Contraction)과 계산 경로의 쌍대성</h3>
<p>SSD 프레임워크에서 밝혀진 수학적 동치성은 단순히 수식이 비슷하다는 것을 넘어, 동일한 연산에 대해 결합 법칙(Associativity)을 적용하여 계산 순서(Reduction Order)를 바꿀 수 있음을 의미한다. 이는 <strong>텐서 축약(Tensor Contraction)</strong> 관점에서 분석할 때 명확히 드러난다.5</p>
<p>우리가 계산하고자 하는 시퀀스 변환은 다음 4개 텐서의 축약(Contraction)이다 (배치 차원과 헤드 차원은 표기의 단순화를 위해 생략).<br />
<span class="math math-display">
Y = \text{contract}(Q, K, V, L)
</span><br />
이 연산은 행렬 곱의 결합 법칙에 의해 두 가지 서로 다른 경로로 계산될 수 있으며, 각 경로는 서로 다른 시간/공간 복잡도를 가진다.</p>
<h4>3.2.1  어텐션 모드 (Quadratic Mode): <span class="math math-inline">O(T^2)</span></h4>
<p>이 모드에서는 먼저 <span class="math math-inline">Q</span>와 <span class="math math-inline">K</span>를 축약하고, 마스크 <span class="math math-inline">L</span>을 적용한 뒤 마지막에 <span class="math math-inline">V</span>와 곱한다.</p>
<ol>
<li><strong>Gram Matrix 생성:</strong> <span class="math math-inline">G = Q K^\top \in \mathbb{R}^{T \times T}</span>. 모든 토큰 쌍 간의 유사도를 계산한다.</li>
<li><strong>마스킹(Masking):</strong> <span class="math math-inline">M = G \circ L \in \mathbb{R}^{T \times T}</span>. 인과적 마스크와 감쇠율(Decay)을 적용한다.</li>
<li><strong>값 적용(Mixing):</strong> <span class="math math-inline">Y = M V \in \mathbb{R}^{T \times D}</span>.</li>
</ol>
<p>이 방식은 <span class="math math-inline">T \times T</span> 크기의 거대한 어텐션 행렬 <span class="math math-inline">M</span>을 명시적으로 생성(Materialize)하므로 <span class="math math-inline">O(T^2)</span>의 연산량과 메모리를 사용한다. 이는 시퀀스가 짧을 때 효율적이며, GPU의 텐서 코어(Matrix Multiply Units)를 최대로 활용할 수 있다는 장점이 있다.11</p>
<h4>3.2.2  SSM 모드 (Linear Mode): <span class="math math-inline">O(T)</span></h4>
<p>이 모드에서는 먼저 <span class="math math-inline">K</span>, <span class="math math-inline">V</span>, <span class="math math-inline">L</span>을 이용하여 상태(State)를 축약하고, 마지막에 <span class="math math-inline">Q</span>와 결합한다.</p>
<ol>
<li>
<p><strong>상태 확장:</strong> <span class="math math-inline">Z = K^\top V</span> (개념적 표현). 입력과 키를 결합한다.</p>
</li>
<li>
<p>순환(Recurrence): <span class="math math-inline">L</span>이 1-반분리 행렬이라는 특성을 이용하여, 순환식을 통해 은닉 상태 <span class="math math-inline">H</span>를 갱신한다. 이는 선형 어텐션의 순환 형태와 일치한다.<br />
<span class="math math-display">
H_t = a_t H_{t-1} + K_t V_t^\top \quad (\text{여기서 } H_t \in \mathbb{R}^{D \times N})
</span><br />
<strong>조회(Querying):</strong> <span class="math math-inline">Y = Q \cdot H</span>. 현재 상태에서 쿼리를 통해 출력을 추출한다.</p>
</li>
</ol>
<p>이 방식은 <span class="math math-inline">T \times T</span> 행렬을 생성하지 않고 <span class="math math-inline">O(TN)</span>의 선형 복잡도로 계산을 수행한다. SSD 프레임워크는 <span class="math math-inline">L</span> 행렬이 1-반분리 구조일 때, 이 선형 모드 계산이 정확히 스칼라 SSM의 순환식과 일치함을 보였다.12</p>
<p><strong>표 5.1 SSD 프레임워크에서의 계산 모드 비교</strong></p>
<table><thead><tr><th><strong>특성</strong></th><th><strong>어텐션 모드 (Quadratic)</strong></th><th><strong>SSM 모드 (Linear)</strong></th></tr></thead><tbody>
<tr><td><strong>계산 순서</strong></td><td><span class="math math-inline">(Q K^\top) \to \text{Mask} \to V</span></td><td><span class="math math-inline">(K V^\top \text{ with Recurrence}) \to Q</span></td></tr>
<tr><td><strong>복잡도 (시간/메모리)</strong></td><td><span class="math math-inline">O(T^2)</span></td><td><span class="math math-inline">O(T)</span></td></tr>
<tr><td><strong>핵심 연산</strong></td><td>행렬 곱 (MatMul)</td><td>누적 합 (CumSum) / 스캔 (Scan)</td></tr>
<tr><td><strong>하드웨어 효율성</strong></td><td>텐서 코어 활용 높음 (High FLOPs)</td><td>메모리 대역폭 의존적 (IO-bound)</td></tr>
<tr><td><strong>병렬화</strong></td><td>전체 시퀀스 병렬화 용이</td><td>순차적 의존성으로 병렬화 난해</td></tr>
<tr><td><strong>Mamba-2 활용</strong></td><td>짧은 시퀀스 또는 블록 내부 계산</td><td>긴 시퀀스 또는 블록 간 상태 전달</td></tr>
</tbody></table>
<h2>4.  SSD 알고리즘: 블록 분해를 통한 하이브리드 가속</h2>
<p>이론적 동치성은 실제 시스템 구현에서 강력한 무기가 된다. Mamba-2는 순수 SSM 모드나 순수 어텐션 모드 중 하나만 택하는 것이 아니라, 두 모드의 장점을 결합한 <strong>블록 분해(Block Decomposition)</strong> 알고리즘을 통해 학습 속도를 비약적으로 향상시킨다.11</p>
<p>이 알고리즘은 전체 시퀀스 길이 <span class="math math-inline">T</span>를 길이가 <span class="math math-inline">Q</span>인 블록(청크, Chunk)들로 나눈다. 전체 반분리 행렬 <span class="math math-inline">M</span>은 다음과 같이 블록 행렬로 분할된다.<br />
<span class="math math-display">
M = \begin{pmatrix} M_{00} &amp; 0 &amp; \dots &amp; 0 \\ M_{10} &amp; M_{11} &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ M_{K0} &amp; M_{K1} &amp; \dots &amp; M_{KK} \end{pmatrix}
</span><br />
여기서 대각 블록(<span class="math math-inline">M_{ii}</span>)과 비대각 블록(<span class="math math-inline">M_{ij}, i &gt; j</span>)을 처리하는 방식이 다르다.</p>
<ol>
<li>
<p><strong>대각 블록 (Diagonal Blocks - Intra-chunk):</strong> <span class="math math-inline">M_{ii}</span>는 각 블록 내부의 연산을 나타낸다. 블록 크기 <span class="math math-inline">Q</span>는 작으므로(예: 64, 128), 이 부분은 **이차 어텐션 모드(Quadratic Attention Mode)**로 계산한다. 즉, 작은 <span class="math math-inline">Q \times Q</span> 어텐션 행렬을 생성하여 고속 행렬 곱(Tensor Cores)을 활용한다. 이는 Mamba-1의 순차적 스캔이 가지는 비효율성을 제거한다.11</p>
</li>
<li>
<p>비대각 블록 (Off-Diagonal Blocks - Inter-chunk): <span class="math math-inline">M_{ij}</span>는 이전 블록들이 현재 블록에 미치는 영향을 나타낸다. 반분리 행렬의 성질에 따라, 이 비대각 블록들은 낮은 랭크(Low-rank)를 가지며 다음과 같이 분해된다.<br />
<span class="math math-display">
M_{ij} = L_{ij} (C^{(i)} (B^{(j)})^\top)
</span><br />
이 부분은 **선형 SSM 모드(Linear SSM Mode)**로 처리한다. 즉, 각 블록의 끝에서 요약된 상태(Summary State)를 다음 블록으로 넘겨주는 방식(State Passing)을 사용한다.11</p>
</li>
</ol>
<h3>4.1  Mamba-2의 4단계 SSD 알고리즘</h3>
<p>구체적으로 Mamba-2의 연산은 다음 4단계로 수행된다.</p>
<ol>
<li><strong>블록 내 출력 계산 (MatMul):</strong> 각 청크 내에서 어텐션(<span class="math math-inline">L \circ QK^\top</span>)을 수행하여 로컬 출력을 계산한다. 이 단계는 모든 청크에서 병렬로 수행된다.</li>
<li><strong>블록 상태 갱신 (MatMul):</strong> 각 청크의 입력을 요약하여 해당 청크의 최종 은닉 상태를 계산한다. 이는 선형 투영 연산이다.</li>
<li><strong>블록 간 순환 (Scan):</strong> 청크 간의 상태를 순환식으로 연결하여, 이전 모든 청크의 정보를 현재 청크의 초기 상태로 전파한다. 이 단계는 시퀀스 길이가 <span class="math math-inline">T/Q</span>로 줄어든 상태에서 수행되므로 매우 빠르다.</li>
<li><strong>최종 출력 합성 (MatMul):</strong> 전파받은 초기 상태를 현재 청크의 출력에 더해준다.</li>
</ol>
<p>이 하이브리드 접근법은 순수 SSM의 순차적 병목을 해소(청크 단위 병렬화)하고, 순수 어텐션의 메모리 비용을 절감(청크 간 상태 압축)하여 Mamba-1 대비 2-8배의 학습 속도 향상을 달성했다. 이는 **텐서 병렬화(Tensor Parallelism)**와 **시퀀스 병렬화(Sequence Parallelism)**를 SSM에 자연스럽게 적용할 수 있게 해 준다.1</p>
<h2>5.  선형 어텐션의 역사와 Mamba-2의 위치</h2>
<p>Mamba-2의 SSD는 Katharopoulos 등(2020)이 제안한 선형 어텐션의 현대적이고 정교화된 버전으로 볼 수 있다. Katharopoulos의 선형 어텐션은 커널 함수 <span class="math math-inline">\phi(\cdot)</span>를 사용하여 <span class="math math-inline">softmax(QK^\top)</span>를 <span class="math math-inline">\phi(Q)\phi(K)^\top</span>로 근사했다. 이는 <span class="math math-inline">O(T)</span> 복잡도를 달성했지만, 단순한 누적 합(Cumulative Sum) 구조로 인해 정보의 망각(Forgetting)을 효과적으로 모델링하지 못했고, 결과적으로 언어 모델링 성능이 떨어졌다.5</p>
<p>Mamba-2는 이 구조에 <strong>데이터 의존적 감쇠(Data-dependent Decay)</strong>, 즉 행렬 <span class="math math-inline">L</span>을 도입했다.</p>
<ul>
<li><strong>기존 선형 어텐션:</strong> <span class="math math-inline">h_t = h_{t-1} + \phi(K_t)\phi(V_t)^\top</span> (모든 과거 정보를 동일하게 유지하거나 단순 지수 감쇠)</li>
<li><strong>Mamba-2 (SSD):</strong> <span class="math math-inline">h_t = a_t h_{t-1} + K_t V_t^\top</span> (입력 내용에 따라 감쇠율 <span class="math math-inline">a_t</span>가 동적으로 변화)</li>
</ul>
<p>이 <span class="math math-inline">a_t</span>의 도입은 모델이 불필요한 정보를 선택적으로 삭제(Forget)하고 중요한 정보에 집중할 수 있게 해 주며, 이것이 Mamba-2가 기존 선형 어텐션들의 성능 한계를 극복하고 트랜스포머와 대등한 성능을 내는 핵심 요인이다.17 또한, RetNet이나 Gated Linear Attention (GLA)과 같은 최신 모델들도 SSD 프레임워크 내에서 특수한 형태의 감쇠 구조(<span class="math math-inline">L</span> 행렬)를 가진 변형들로 통합 설명될 수 있다.6</p>
<h2>6.  결론: 통합된 시각</h2>
<p>5.1절에서 살펴본 구조적 상태 공간 쌍대성(SSD)은 Mamba-2 아키텍처를 지탱하는 가장 중요한 이론적 기둥이다. SSD는 다음 세 가지 측면에서 시퀀스 모델링의 새로운 지평을 열었다.</p>
<ol>
<li><strong>이론적 통합:</strong> <span class="math math-inline">A_t = a_t I</span> 제약 조건을 통해 SSM을 1-반분리 행렬 연산으로 변환시킴으로써, SSM이 본질적으로 ’구조화된 마스크를 가진 선형 어텐션’과 동치임을 증명했다. 이는 SSM과 어텐션을 **“행렬 믹서(Matrix Mixer)”**라는 단일 스펙트럼 상의 변형들로 이해할 수 있게 한다.20</li>
<li><strong>시스템적 혁신:</strong> 텐서 축약의 순서를 자유롭게 변경할 수 있는 쌍대성을 이용하여, 텐서 코어에 최적화된 블록 분해 알고리즘을 도출했다. 이는 SSM의 학습 속도를 획기적으로 개선하여 대규모 모델 학습의 길을 열었다.</li>
<li><strong>설계의 확장성:</strong> 반분리 행렬 <span class="math math-inline">L</span>의 구조를 조작함으로써, 대각 SSM(Multi-head Pattern)이나 양방향 SSM(Bidirectional Hydra)과 같은 다양한 파생 모델을 설계할 수 있는 수학적 도구를 제공한다.</li>
</ol>
<p>결국 Mamba-2의 SSD 프레임워크는 “상태를 가질 것인가, 행렬을 가질 것인가?“라는 오랜 질문에 대해 **“둘은 같다. 상황에 따라 더 효율적인 형태를 선택하라”**는 명쾌한 해답을 제시한 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Transformers are SSMs: Generalized Models and Efficient … - arXiv, https://arxiv.org/abs/2405.21060</li>
<li>Mamba2: The Hardware-Algorithm Co-Design That Unified Attention …, https://medium.com/@danieljsmit/mamba2-the-hardware-algorithm-co-design-that-unified-attention-and-state-space-models-77856d2ac4f4</li>
<li>State Space Duality (Mamba-2) Part I - The Model | Tri Dao, https://tridao.me/blog/2024/mamba2-part1-model/</li>
<li>Understanding State Space Models (SSMs) like LSSL, H3 … - Tinkerd, https://tinkerd.net/blog/machine-learning/state-space-models/</li>
<li>State Space Duality (Mamba-2) Part II - The Theory | Tri Dao, https://tridao.me/blog/2024/mamba2-part2-theory/</li>
<li>State Space Duality (Mamba-2) Part I - The Model | Goomba Lab, https://goombalab.github.io/blog/2024/mamba2-part1-model/</li>
<li>Mamba Explained - The Gradient, https://thegradient.pub/mamba-explained/</li>
<li>1 Introduction - arXiv, https://arxiv.org/html/2510.04944v2</li>
<li>Mamba-2: The ’Transform’ation of Mamba | by Utsavtiwari - Medium, https://medium.com/@utsavtiwari9936/mamba-2-the-transformation-of-mamba-125096294c51</li>
<li>Mamba 2 | PDF | Matrix (Mathematics) | Tensor - Scribd, https://www.scribd.com/document/748683510/mamba2</li>
<li>Mamba-2: Algorithms and Systems, https://pli.princeton.edu/blog/2024/mamba-2-algorithms-and-systems</li>
<li>On Structured State-Space Duality - arXiv, https://www.arxiv.org/pdf/2510.04944</li>
<li>State Space Duality (Mamba-2) Part II - The Theory | Goomba Lab, https://goombalab.github.io/blog/2024/mamba2-part2-theory/</li>
<li>State Space Duality (Mamba-2) Part III - The Algorithm | Tri Dao, https://tridao.me/blog/2024/mamba2-part3-algorithm/</li>
<li>Transformers are SSMs: Generalized Models and Efficient …, https://openreview.net/pdf/54bf495d93336f1f195f264c1b6c2805169b3492.pdf</li>
<li>Demystify Mamba in Vision: A Linear Attention Perspective - arXiv, https://arxiv.org/pdf/2405.16605</li>
<li>IMPROVING MAMBA2 WITH DELTA RULE - Jan Kautz, https://jankautz.com/publications/GatedDeltaNet_ICLR25.pdf</li>
<li>What’s Next for Mamba? Towards More Expressive Recurrent …, https://sustcsonglin.github.io/assets/pdf/talk_250117.pdf</li>
<li>Attention, State Space Models, and Recurrent Neural Networks, https://www.research-collection.ethz.ch/server/api/core/bitstreams/c067c17d-d985-4ddf-bc08-b32314f2afc0/content</li>
<li>Bidirectional State Space Models Through Generalized Matrix Mixers, https://arxiv.org/pdf/2407.09941</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>