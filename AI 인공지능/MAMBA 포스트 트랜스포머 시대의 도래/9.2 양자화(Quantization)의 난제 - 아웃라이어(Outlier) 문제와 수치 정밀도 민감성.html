<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:9.2 양자화(Quantization)의 난제 - 아웃라이어(Outlier) 문제와 수치 정밀도 민감성</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>9.2 양자화(Quantization)의 난제 - 아웃라이어(Outlier) 문제와 수치 정밀도 민감성</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">MAMBA - 포스트 트랜스포머 시대의 도래</a> / <span>9.2 양자화(Quantization)의 난제 - 아웃라이어(Outlier) 문제와 수치 정밀도 민감성</span></nav>
                </div>
            </header>
            <article>
                <h1>9.2 양자화(Quantization)의 난제 - 아웃라이어(Outlier) 문제와 수치 정밀도 민감성</h1>
<h2>1.  서론: 상태 공간 모델(SSM)이 직면한 정밀도의 역설</h2>
<p>인공지능 아키텍처의 패러다임이 트랜스포머(Transformer)에서 Mamba와 같은 선택적 상태 공간 모델(Selective State Space Model, SSM)로 확장됨에 따라, 모델의 효율성을 극대화하기 위한 경량화 기술의 중요성은 그 어느 때보다 높아졌다. Mamba 아키텍처는 선형 시간 복잡도(Linear Time Complexity)와 추론 시 일정한 메모리 요구량(Constant Memory Requirement)이라는 강력한 이점을 제공하며, 긴 시퀀스 처리에서 트랜스포머가 겪는 병목 현상을 해결할 대안으로 자리 잡았다. 그러나 이러한 구조적 우위가 실제 하드웨어, 특히 엣지 디바이스나 대규모 클라우드 서빙 환경에서의 ’비용 효율성’으로 직결되기 위해서는 모델 압축, 그중에서도 양자화(Quantization) 과정이 필수적이다.</p>
<p>하지만 Mamba 모델을 저비트(Low-bit) 정수 연산으로 변환하는 과정은 기존의 CNN이나 트랜스포머 모델에서 겪었던 것과는 질적으로 다른 난관에 봉착한다. 기존 트랜스포머 모델이 주로 어텐션 메커니즘(Attention Mechanism)의 <span class="math math-inline">KV</span> 캐시 메모리 대역폭 문제와 연산 비용을 줄이는 데 집중했다면, Mamba는 순환 신경망(RNN)의 특성을 계승한 재귀적(Recurrent) 구조와 데이터 의존적(Data-dependent) 파라미터들이 만들어내는 복잡한 수치 역학(Numerical Dynamics)을 다워야 한다.</p>
<p>본 절에서는 Mamba 아키텍처가 양자화될 때 발생하는 고유한 문제점들을 심층적으로 분석한다. 우리는 이 문제를 크게 두 가지 핵심 축으로 정의한다. 첫째는 활성화 값(Activation)과 가중치(Weight) 분포에서 나타나는 극단적인 ’아웃라이어(Outlier)’의 존재와 그들의 구조적 지속성이다. 둘째는 연속 시간 시스템을 이산화(Discretization)하는 과정에서 파생되는 파라미터 <span class="math math-inline">\Delta</span>와 순환 상태(Recurrent State) <span class="math math-inline">h_t</span>가 가지는 ’수치 정밀도에 대한 극도의 민감성(Numerical Precision Sensitivity)’이다. 이 두 요소는 Mamba의 성능을 결정짓는 핵심 기제인 ‘선택적 스캔(Selective Scan)’ 연산 내부에서 상호 작용하며, 단순한 양자화 오차를 치명적인 성능 붕괴(Performance Collapse)로 증폭시키는 원인으로 작용한다.</p>
<p>우리는 최신 연구 결과들1을 바탕으로 Mamba의 데이터 분포 특성을 통계적으로 규명하고, 왜 기존의 SmoothQuant나 GPTQ와 같은 범용 양자화 기법들이 SSM 환경에서 실패하거나 불충분한지를 논증한다. 또한, Mamba-2에서 새롭게 도입된 SSD(Structured State Space Duality) 알고리즘이 양자화 안정성에 미치는 영향과, 이것이 시사하는 새로운 경량화 전략의 방향성을 제시한다. 이는 단순한 공학적 최적화의 문제를 넘어, 연속 신호를 디지털 비트로 근사하는 과정에서 발생하는 정보 이론적 한계를 탐구하는 과정이기도 하다.</p>
<h2>2.  Mamba 활성화 아웃라이어의 해부학: 채널 지속성과 증폭</h2>
<p>양자화의 핵심은 연속적인 실수 값을 제한된 정수 그리드(Grid)로 매핑하는 것이다. 이 과정에서 정보 손실을 최소화하기 위해서는 데이터의 분포가 좁고 균일할수록 유리하다. 그러나 Mamba 모델의 내부 텐서들은 이러한 이상적인 분포와는 거리가 먼, 매우 독특하고 극단적인 형태를 띤다.</p>
<h3>2.1  헤비 테일(Heavy-tailed) 분포와 채널 지속성(Channel Persistence)</h3>
<p>Mamba 모델의 선형 투영 레이어(Linear Projection Layers), 특히 게이트 투영(Gate Projection)과 출력 투영(Output Projection)을 통과하는 활성화 값들은 정규 분포를 따르지 않고 극심한 ‘헤비 테일’ 분포를 보인다.1 트랜스포머 모델(LLM)에서도 특정 토큰에 값이 집중되는 아웃라이어 현상이 보고된 바 있으나, Mamba의 아웃라이어는 그 성격이 판이하다.</p>
<p>트랜스포머의 아웃라이어가 주로 특정 단어(Token)나 문장 부호 등 입력 데이터의 의미론적 특성에 따라 간헐적으로 발생하는 ‘토큰 단위(Token-wise)’ 아웃라이어라면, Mamba의 아웃라이어는 특정 ’채널(Channel)’에 고정되어 시퀀스 전체에 걸쳐 지속적으로 나타나는 ’채널 지속성(Channel Persistence)’을 보인다.6</p>
<table><thead><tr><th><strong>특성 비교</strong></th><th><strong>트랜스포머 (Attention-based)</strong></th><th><strong>Mamba (SSM-based)</strong></th></tr></thead><tbody>
<tr><td><strong>아웃라이어 위치</strong></td><td>특정 토큰 (Token-wise)</td><td>특정 채널 (Channel-wise)</td></tr>
<tr><td><strong>발생 빈도</strong></td><td>입력에 따라 간헐적/가변적</td><td>시퀀스 전체에 걸쳐 지속적 (Persistence)</td></tr>
<tr><td><strong>주요 원인</strong></td><td>Layernorm, Attention Score 집중</td><td>SSM의 상태 역학(Dynamics), Gating 메커니즘</td></tr>
<tr><td><strong>양자화 영향</strong></td><td>토큰별 Clipping으로 완화 가능</td><td>전체 텐서의 Dynamic Range를 왜곡시킴</td></tr>
<tr><td><strong>SmoothQuant 적용</strong></td><td>효과적 (가중치로 난이도 이전 용이)</td><td>제한적 (동적 파라미터 및 채널 편차 심화)</td></tr>
</tbody></table>
<p><strong>[표 9-1] 트랜스포머와 Mamba 모델의 아웃라이어 특성 비교 분석</strong></p>
<p>이러한 채널 지속성은 Mamba의 상태 공간 모델이 채널별로 독립적인 역학(Independent Channel Dynamics)을 학습하도록 설계된 구조적 특성에서 기인한다. SSM은 각 채널이 서로 다른 주파수 대역이나 시간 척도(Time Scale)의 정보를 포착하도록 유도한다. 예를 들어, 어떤 채널은 문맥의 장기 의존성을 유지하기 위해 매우 느리게 감쇠(Decay)하는 신호를 처리하고, 어떤 채널은 즉각적인 반응을 처리한다. 장기 기억을 담당하는 채널은 필연적으로 신호가 누적되는 경향이 있어, 다른 채널에 비해 활성화 값의 절대 크기(Magnitude)가 수십 배에서 수백 배까지 커지게 된다.2</p>
<p>이 현상은 양자화에 치명적이다. 만약 텐서 전체(Per-tensor)를 기준으로 스케일링 팩터(Scaling Factor)를 결정한다면, 소수의 거대 아웃라이어 채널을 표현하기 위해 스케일이 지나치게 커지게 된다. 그 결과, 정상 범주에 있는 대다수 채널의 값들은 0 근처의 매우 좁은 정수 범위로 매핑되어, 사실상 정보가 소멸되는 ’양자화 붕괴(Quantization Collapse)’가 발생한다. 반대로 채널별(Per-channel) 양자화를 수행하려 해도, Mamba의 연산 구조상 하드웨어 효율성을 저해하거나 구현 복잡도를 크게 높이는 딜레마에 빠지게 된다.</p>
<h3>2.2  비선형 게이팅(Non-linear Gating)에 의한 아웃라이어 생성</h3>
<p>Mamba 아키텍처의 또 다른 특징인 GLU(Gated Linear Unit) 유사 구조는 아웃라이어를 생성하는 직접적인 원인이 된다. Mamba 블록 내부에서 입력 <span class="math math-inline">X</span>는 두 갈래로 나뉘어 하나는 SSM 경로를, 다른 하나는 게이트 경로를 통과한다. 마지막에 이 둘은 요소별 곱(Element-wise Multiplication)을 통해 결합된다(<span class="math math-inline">Y = O \odot G</span>).<br />
<span class="math math-display">
Y = \text{Output\_Proj}(\text{SSM}(X) \odot \sigma(\text{Gate\_Proj}(X)))
</span><br />
여기서 <span class="math math-inline">\sigma</span>는 SiLU(Sigmoid Linear Unit)와 같은 활성화 함수다. 연구에 따르면, 게이트 투영(Gate Projection) 레이어의 출력값에서 비정상적으로 큰 값들이 관측된다.1 게이팅 메커니즘은 본질적으로 정보의 흐름을 선택적으로 차단하거나 통과시키는 역할을 하는데, 통과시켜야 할 정보에 대해서는 값을 증폭시키는 경향이 있다.</p>
<p>특히 Mamba는 트랜스포머와 달리 Softmax와 같은 정규화 기제(Normalization Mechanism)가 토큰 혼합(Token Mixing) 과정에 존재하지 않는다. 트랜스포머의 어텐션 맵은 합이 1이 되도록 강제되지만, Mamba의 SSM은 선형 재귀(Linear Recurrence)를 통해 값이 제한 없이 커질 수 있는 구조다. 이러한 비유계(Unbounded) 특성이 게이트 메커니즘과 결합되면서, 특정 채널의 활성화 값이 제어되지 않고 폭발적으로 증가하는 아웃라이어를 만들어낸다.</p>
<h3>2.3  PScan(Parallel Scan) 연산과 오차 증폭 메커니즘</h3>
<p>Mamba의 추론 속도를 보장하는 핵심 알고리즘인 병렬 스캔(Parallel Scan, PScan)은 역설적으로 양자화 오차를 증폭시키는 촉매제가 된다. PScan은 순차적인 재귀 연산을 병렬로 처리하기 위해 결합 법칙(Associative Property)을 이용하는데, 이 과정에서 입력의 작은 변동이 출력에 큰 영향을 미친다.</p>
<p>연구 결과1는 PScan 연산이 입력된 활성화 아웃라이어를 단순히 전달하는 것이 아니라 ’증폭(Amplify)’시킨다는 사실을 입증했다. 수학적으로 <span class="math math-inline">h_t = A_t h_{t-1} + B_t x_t</span>의 재귀 식을 병렬 스캔으로 풀면, 각 시점의 출력은 이전 모든 시점의 입력과 상태 전이 행렬의 누적곱(Cumulative Product)의 선형 결합으로 표현된다.</p>
<p>만약 특정 채널의 <span class="math math-inline">A_t</span> 값이 1에 가깝거나 1보다 크고(발산하는 시스템), 입력 <span class="math math-inline">x_t</span>에 양자화 오차로 인한 노이즈가 포함되어 있다면, PScan 과정에서 이 노이즈는 기하급수적으로 증폭될 수 있다. 특히 Mamba 모델은 긴 문맥을 학습하기 위해 일부 채널의 <span class="math math-inline">A</span> 파라미터를 1에 매우 가깝게 설정하는데, 이는 해당 채널이 ’적분기(Integrator)’처럼 동작하게 만든다. 적분기는 입력의 DC 성분(여기서는 양자화 편향 오차)을 시간축을 따라 누적시키므로, 시퀀스가 길어질수록 출력값의 정밀도는 급격히 떨어진다. 이는 Mamba-2에서 SSD 알고리즘을 도입하며 수치적 안정성을 개선하려 한 배경이기도 하다.9</p>
<h2>3.  재귀적 상태(Recurrent State)의 수치 정밀도 민감성</h2>
<p>트랜스포머 모델이 <span class="math math-inline">KV</span> 캐시의 용량 문제와 싸운다면, Mamba는 순환 상태 <span class="math math-inline">h_t</span>의 ‘정밀도(Precision)’ 문제와 싸운다. Mamba의 상태 <span class="math math-inline">h_t</span>는 과거의 모든 정보를 압축하여 담고 있는 메모리 셀이다. 이 상태 벡터가 양자화로 인해 손상되면, 모델은 과거의 문맥을 잊어버리거나 왜곡된 기억을 바탕으로 다음 토큰을 생성하게 된다.</p>
<h3>3.1  무한 임펄스 응답(IIR) 시스템의 양자화 잡음 순환</h3>
<p>Mamba는 신호 처리 관점에서 볼 때, 입력 의존적인 계수를 가진 시변(Time-variant) 무한 임펄스 응답(IIR) 필터로 모델링할 수 있다. IIR 필터의 가장 큰 특징은 피드백 루프(Feedback Loop)가 존재한다는 점이다. 피드백 루프가 있는 시스템에서 양자화 오차는 단순히 출력단에 노이즈를 더하는 것이 아니라, 시스템 내부를 순환하며 지속적인 영향을 미친다.10</p>
<p>시점 <span class="math math-inline">t</span>에서의 이상적인 상태 업데이트가 다음과 같다고 가정하자.<br />
<span class="math math-display">
h_t = A_t h_{t-1} + B_t x_t
</span><br />
양자화된 시스템에서는 각 변수와 연산 결과에 양자화 연산자 <span class="math math-inline">Q(\cdot)</span>가 적용된다.<br />
<span class="math math-display">
\hat{h}_t = Q(Q(A_t) \hat{h}_{t-1} + Q(B_t) Q(x_t))
</span><br />
여기서 발생하는 오차 <span class="math math-inline">e_t = h_t - \hat{h}_t</span>는 다음과 같은 차분 방정식(Difference Equation)을 따른다.<br />
<span class="math math-display">
e_t \approx A_t e_{t-1} + \epsilon_{quant}
</span><br />
여기서 <span class="math math-inline">\epsilon_{quant}</span>는 현재 단계에서 발생한 라운딩 노이즈(Rounding Noise)다. 이 식은 현재의 오차 <span class="math math-inline">e_t</span>가 과거의 오차 <span class="math math-inline">e_{t-1}</span>에 시스템 행렬 <span class="math math-inline">A_t</span>가 곱해진 형태로 전파됨을 보여준다. 만약 <span class="math math-inline">|A_t| &lt; 1</span>이라면 과거의 오차는 자연스럽게 소멸(Decay)되겠지만, Mamba는 장기 기억을 위해 <span class="math math-inline">|A_t| \approx 1</span>인 채널을 다수 포함하고 있다. 이 경우, 양자화 노이즈 <span class="math math-inline">\epsilon_{quant}</span>는 사라지지 않고 랜덤 워크(Random Walk)하거나 특정 값으로 편향되어 누적된다.12</p>
<p>수천 토큰 이상의 긴 시퀀스를 처리할 때, 이러한 오차 누적은 ‘상태 표류(State Drift)’ 현상을 유발한다. 초기 토큰에서 발생한 미세한 양자화 오차가 시퀀스 끝부분에 이르러서는 전혀 다른 문맥 해석을 낳게 되는 것이다. 이는 엣지 디바이스에서 Mamba 모델을 장시간 구동할 때 성능이 점진적으로 저하되는 원인이 될 수 있다.</p>
<h2>4.  이산화 파라미터 <span class="math math-inline">\Delta</span>의 치명적 민감도</h2>
<p>Mamba 양자화에서 가장 다루기 까다로운 변수는 바로 시간 스케일 파라미터 <span class="math math-inline">\Delta</span>이다. <span class="math math-inline">\Delta</span>는 연속 시간 시스템(Continuous-time System)의 파라미터 <span class="math math-inline">A, B</span>를 이산 시간 시스템(Discrete-time System)의 <span class="math math-inline">\bar{A}, \bar{B}</span>로 변환하는 역할을 한다.</p>
<p>이산화 공식, 특히 ZOH(Zero-Order Hold) 방식을 적용하면 <span class="math math-inline">\bar{A}</span>는 다음과 같이 정의된다.<br />
<span class="math math-display">
\bar{A} = \exp(\Delta A)
</span><br />
이 지수 함수(Exponential Function) 관계가 바로 수치 정밀도 민감성의 근원이다.4 <span class="math math-inline">\Delta</span> 값의 아주 미세한 변화는 지수 함수를 통해 <span class="math math-inline">\bar{A}</span> 값에 비선형적이고 거대한 변화를 일으킨다.</p>
<p><span class="math math-inline">\Delta</span>는 입력 <span class="math math-inline">x_t</span>에 의존하는 동적 파라미터로, <span class="math math-inline">\Delta_t = \text{Softplus}(\text{Parameter} + \text{Input})</span>의 형태로 계산된다. 만약 <span class="math math-inline">\Delta</span>를 INT8과 같은 저비트로 양자화한다면 어떻게 될까?</p>
<ol>
<li>
<p><strong>해상도 부족에 의한 에일리어싱:</strong> <span class="math math-inline">\Delta</span>가 가질 수 있는 값의 단계가 듬성듬성해지면, 모델이 표현할 수 있는 ’시간의 흐름’이 불연속적으로 변한다. 이는 마치 오디오 샘플링 레이트를 낮추는 것과 유사하여, 고주파 정보를 잃어버리거나 신호 왜곡(Aliasing)을 일으킨다.</p>
</li>
<li>
<p>Softplus 함수의 양자화 절벽: <span class="math math-inline">\Delta</span>를 생성하는 Softplus 함수는 0 근처에서 부드러운 곡선을 그리지만, 양자화된 정수 도메인에서는 계단 함수가 된다. 특히 <span class="math math-inline">\Delta</span> 값이 작을 때(즉, 정보를 오래 기억해야 할 때) 양자화 오차가 발생하면, <span class="math math-inline">\exp(\Delta A)</span>의 값이 급격히 변하여 기억해야 할 정보를 망각하거나, 반대로 잊어야 할 정보를 과도하게 유지하는 결과(Numerical Explosion)를 초래한다.13</p>
</li>
</ol>
<p>연구 결과에 따르면, 가중치 <span class="math math-inline">W</span>나 입력 <span class="math math-inline">x</span>를 4비트로 줄이는 것보다, <span class="math math-inline">\Delta</span> 파라미터의 정밀도를 8비트 이하로 줄였을 때 모델의 성능 하락폭이 훨씬 크다. 이는 <span class="math math-inline">\Delta</span>가 단순한 가중치가 아니라, 시스템의 ’물리적 동작 속도’를 결정하는 제어 변수이기 때문이다. 따라서 <span class="math math-inline">\Delta</span>에 대해서는 더 높은 정밀도(FP16 또는 BF16)를 유지하거나, 비선형성을 고려한 특수 양자화 테이블을 사용하는 것이 필수적이다.4</p>
<h3>4.1  동적 가중치 <span class="math math-inline">B, C</span>와 양자화의 어려움</h3>
<p>기존 RNN이나 CNN은 가중치가 고정(Static)되어 있어 오프라인에서 미리 최적의 양자화 파라미터를 찾을 수 있었다. 그러나 Mamba의 <span class="math math-inline">B</span>와 <span class="math math-inline">C</span> 행렬은 입력 <span class="math math-inline">x_t</span>의 함수이므로, 매 시점마다 값이 변하는 ’동적 가중치(Dynamic Weight)’다.<br />
<span class="math math-display">
B_t = \text{Linear}_B(x_t), \quad C_t = \text{Linear}_C(h_t)
</span><br />
이는 양자화 관점에서 두 가지 난제를 제시한다.</p>
<p>첫째, 동적 범위의 가변성이다. 어떤 시점에서는 <span class="math math-inline">B_t</span>의 값이 매우 작을 수 있고, 어떤 시점에서는 매우 클 수 있다. 이를 고정된 스케일로 양자화하면, 값이 작은 구간에서는 정밀도가 떨어지고(Underflow), 값이 큰 구간에서는 잘림(Clipping)이 발생한다. 이를 해결하기 위해 실행 시간에 매번 스케일을 계산하는 동적 양자화(Dynamic Quantization)를 사용할 수 있지만, 이는 추론 속도를 저하시키는 오버헤드가 된다.</p>
<p>둘째, 입력 의존적 오차 주입이다. 입력 <span class="math math-inline">x_t</span>에 노이즈가 있을 경우, 이 노이즈는 <span class="math math-inline">B_t</span> 행렬 자체를 변형시킨다. 즉, 입력의 오차가 시스템의 파라미터를 바꿔버리는 셈이다. 이는 입력 오차가 단순히 더해지는 구조(Additive Noise)보다 훨씬 복잡한 증폭 효과(Multiplicative Noise)를 낳는다.</p>
<h2>5.  Mamba-2와 SSD: 구조적 변화와 새로운 안정성 이슈</h2>
<p>후속 아키텍처인 Mamba-2는 ‘구조적 상태 공간 이원성(Structured State Space Duality, SSD)’ 이론을 바탕으로, 순차적인 스캔 연산을 행렬 곱셈(Matrix Multiplication) 형태로 변환하여 GPU 텐서 코어 활용도를 극대화했다. 이는 추론 속도를 비약적으로 높였지만, 양자화 관점에서는 새로운 양상의 불안정성을 드러냈다.</p>
<h3>5.1  SSD 알고리즘의 수치적 불안정성과 ‘세그먼트 합(Segsum)’</h3>
<p>SSD 알고리즘은 긴 시퀀스를 청크(Chunk)로 나누고, 각 청크 내부를 행렬 곱으로 처리한 뒤, 청크 간의 상태를 전달하는 방식을 취한다. 이론적으로는 기존의 선택적 스캔과 수학적으로 등가여야 한다. 그러나 저정밀도 연산 환경에서는 이 등가성이 깨진다.</p>
<p>Mamba-2의 SSD 구현 과정에서 연구진은 초기 알고리즘이 훈련 중 발산(NaN 발생)하는 문제를 발견했다.16 이는 SSD 계산의 핵심인 1-Semiseparable 행렬을 분해하는 과정에서 발생했다.</p>
<ol>
<li>
<p><strong>누적곱 비율(Ratio of Cumulative Products)의 위험성:</strong> SSD 행렬의 원소는 누적곱의 비율(<span class="math math-inline">\prod a_i / \prod a_j</span>)로 표현될 수 있다. 그러나 <span class="math math-inline">a_i</span>가 1보다 작을 경우, 시퀀스가 길어지면 분모와 분자가 모두 0에 수렴하는 언더플로우(Underflow)가 발생한다. 반대로 1보다 크면 오버플로우가 발생한다. 이를 나눗셈하는 과정에서 수치적 정밀도가 극도로 떨어지게 된다.</p>
</li>
<li>
<p>로그 공간 뺄셈과 치명적 소거(Catastrophic Cancellation): 이를 해결하기 위해 로그 공간에서 뺄셈(<span class="math math-inline">\sum \log a_i - \sum \log a_j</span>)으로 변환할 수 있다. 하지만 비슷한 크기의 두 큰 수의 차이를 구할 때 유효 숫자가 급격히 줄어드는 ‘치명적 소거’ 현상이 발생한다. FP32에서도 발생하는 이 문제는, 표현 범위가 좁은 INT8이나 FP8 환경에서는 즉각적인 모델 붕괴로 이어진다.</p>
</li>
</ol>
<p>따라서 Mamba-2를 양자화하기 위해서는 SSD의 ‘Segsum(Segment Sum)’ 연산에 대해 수치적으로 안정화된 특수 커널을 사용해야 하며, 이 부분만큼은 고정밀도(FP32) 누적기(Accumulator)를 유지해야 하는 제약이 생긴다. 이는 저비트 양자화를 통한 가속 효과를 일부 상쇄시키는 요인이 된다.</p>
<h3>5.2  채널 순서 보존(Channel Order Preserving)과 정렬 기반 양자화</h3>
<p>Mamba-2의 긍정적인 발견은 SSD 구조가 입력과 출력 사이의 채널 순서를 보존한다는 점이다. 즉, 입력 <span class="math math-inline">x</span>에서 값이 컸던 채널은 출력 <span class="math math-inline">y</span>에서도 여전히 값이 크고, 상태 <span class="math math-inline">h</span>에서도 활성화될 확률이 높다는 ’채널 지속성’이 더욱 강화되었다.7</p>
<p>Mamba-1이 다소 무작위적인 아웃라이어 분포를 보였다면, Mamba-2는 아웃라이어의 위치가 구조적으로 예측 가능하다. 이는 Quamba2와 같은 최신 연구에서 제안하는 ‘정렬 및 클러스터링(Sort-and-Cluster)’ 기법의 근거가 된다. 아웃라이어 채널들을 미리 식별하여 서로 인접하게 재배열한 뒤, 그룹별로 다른 스케일 파라미터를 적용하면 양자화 오차를 효과적으로 격리할 수 있다. 예를 들어, 값이 큰 채널 그룹은 더 넓은 범위를 커버하는 스케일을, 값이 작은 채널 그룹은 더 정밀한 스케일을 적용함으로써 전체적인 신호 대 잡음비(SNR)를 개선하는 것이다.</p>
<p>하지만 이러한 전처리 과정은 오프라인 단계에서 수행되어야 하며, 입력 데이터의 분포가 학습 데이터와 크게 달라질 경우(Distribution Shift) 효과가 반감될 수 있다는 한계가 여전히 존재한다.</p>
<h2>6.  비교 분석: Transformer 대 Mamba 양자화 난이도</h2>
<p>요약하자면, Mamba의 양자화는 트랜스포머의 양자화보다 구조적으로 더 복잡하고 민감하다. 다음 표는 두 아키텍처의 양자화 난제를 비교 분석한 것이다.</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>Transformer (Attention)</strong></th><th><strong>Mamba (Selective SSM)</strong></th><th><strong>양자화 난이도 (SSM)</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 연산</strong></td><td>행렬 곱 (<span class="math math-inline">QK^T</span>), Softmax</td><td>병렬 스캔 (Prefix Sum), 재귀</td><td><strong>매우 높음</strong> (오차 누적)</td></tr>
<tr><td><strong>오차 전파</strong></td><td>층(Layer) 간 전파, 토큰 간 평균화</td><td>시간(Time) 축 누적 및 증폭</td><td><strong>매우 높음</strong> (State Drift)</td></tr>
<tr><td><strong>파라미터 특성</strong></td><td>대부분 정적(Static) 가중치</td><td>입력 의존적(Dynamic) 파라미터 (<span class="math math-inline">B, C, \Delta</span>)</td><td><strong>높음</strong> (실시간 스케일링 필요)</td></tr>
<tr><td><strong>민감도</strong></td><td><span class="math math-inline">KV</span> Cache 용량</td><td><span class="math math-inline">\Delta</span> 파라미터, Recurrent State <span class="math math-inline">h_t</span></td><td><strong>극도로 높음</strong> (시스템 역학 변화)</td></tr>
<tr><td><strong>아웃라이어</strong></td><td>토큰 단위 (Token-wise)</td><td>채널 단위 (Channel-wise)</td><td><strong>높음</strong> (전역적 영향)</td></tr>
<tr><td><strong>해결책</strong></td><td>SmoothQuant, KV Cache Quant</td><td>MambaQuant, Quamba2, Hybrid Precision</td><td><strong>복잡함</strong> (맞춤형 알고리즘 필요)</td></tr>
</tbody></table>
<p><strong>[표 9-2] Transformer와 Mamba 아키텍처의 양자화 난이도 및 특성 비교</strong></p>
<h2>7.  소결: 수치적 지뢰밭을 건너기 위한 전략</h2>
<p>Mamba의 효율성을 엣지 디바이스에서 온전히 구현하기 위해서는 양자화가 필수적이지만, 앞서 살펴본 바와 같이 ’아웃라이어의 채널 지속성’과 ’재귀적 수치 민감성’이라는 두 가지 거대한 장벽이 가로막고 있다. 일반적인 양자화 기법을 무비판적으로 적용할 경우, Mamba는 긴 문맥을 기억하지 못하거나 엉뚱한 출력을 내놓는 등 트랜스포머보다 훨씬 급격한 성능 저하를 겪는다.</p>
<p>따라서 9.3절에서 다루게 될 최신 경량화 기법들은 이러한 Mamba 고유의 병리적 현상을 치료하기 위한 ‘외과 수술적’ 접근법을 취한다. 아웃라이어를 격리하기 위한 분리형 스케일(Decoupled Scale), 채널 간 분산을 평탄화하기 위한 회전 변환(Rotation), 그리고 가장 민감한 파라미터만을 골라내어 고정밀도로 유지하는 혼합 정밀도(Mixed Precision) 전략 등이 그것이다. Mamba의 잠재력을 실현하는 길은 단순한 비트 수 줄이기가 아니라, 모델 내부의 흐름을 이해하고 수치적 안정성을 보존하는 정교한 엔지니어링에 달려 있다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation Methods | alphaXiv, https://www.alphaxiv.org/overview/2501.13484v1</li>
<li>[2410.13229] Quamba: A Post-Training Quantization Recipe for Selective State Space Models - arXiv, https://arxiv.org/abs/2410.13229</li>
<li>MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation Methods - arXiv, https://arxiv.org/html/2501.13484v1</li>
<li>Quamba: A Post-Training Quantization Recipe for Selective State Space Models - arXiv, https://arxiv.org/html/2410.13229v1</li>
<li>[Quick Review] MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation Methods - Liner, https://liner.com/review/mambaquant-quantizing-the-mamba-family-with-variance-aligned-rotation-methods</li>
<li>Mamba-PTQ: Outlier Channels in Recurrent Large Language Models - OpenReview, https://openreview.net/pdf?id=h3hCUxBi9M</li>
<li>Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models - GitHub, https://raw.githubusercontent.com/mlresearch/v267/main/assets/chiang25a/chiang25a.pdf</li>
<li>Understanding and Minimising Outlier Features in Transformer Training - OpenReview, https://openreview.net/forum?id=npJQ6qS4bg</li>
<li>Mamba-2: Algorithms and Systems | Princeton Language and Intelligence, https://pli.princeton.edu/blog/2024/mamba-2-algorithms-and-systems</li>
<li>What are common pitfalls when designing IIR filters, and how do you fix them? - HackMD, https://hackmd.io/@ampheo/what-are-common-pitfalls-when-designing-iir-filters-and-how-do-you-fix-them</li>
<li>Chapter Six. Infinite Impulse Response Filters, https://www.profdong.com/elc4351_spring2016/ChapterSix.pdf</li>
<li>Late Breaking Results: AFS: Improving Accuracy of Quantized Mamba via Aggressive Forgetting Strategy, https://ieeexplore.ieee.org/document/10993001/</li>
<li>eMamba: Efficient Acceleration Framework for Mamba Models in Edge Computing - arXiv, https://arxiv.org/html/2508.10370v1</li>
<li>question about Parameter+delta? · Issue #410 · state-spaces/mamba - GitHub, https://github.com/state-spaces/mamba/issues/410</li>
<li>Q4’24: Technology Update – Low Precision and Model Optimization - OpenVINO™ Blog, https://blog.openvino.ai/blog-posts/q424-technology-update—low-precision-and-model-optimization</li>
<li>State Space Duality (Mamba-2) Part III - The Algorithm | Tri Dao, https://tridao.me/blog/2024/mamba2-part3-algorithm/</li>
<li>Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models - arXiv, https://arxiv.org/html/2503.22879v2</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>