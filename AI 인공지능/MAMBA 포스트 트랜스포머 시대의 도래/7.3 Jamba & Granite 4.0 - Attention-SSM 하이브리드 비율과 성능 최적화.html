<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.3 Jamba & Granite 4.0 - Attention-SSM 하이브리드 비율과 성능 최적화</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.3 Jamba & Granite 4.0 - Attention-SSM 하이브리드 비율과 성능 최적화</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">MAMBA - 포스트 트랜스포머 시대의 도래</a> / <span>7.3 Jamba & Granite 4.0 - Attention-SSM 하이브리드 비율과 성능 최적화</span></nav>
                </div>
            </header>
            <article>
                <h1>7.3 Jamba &amp; Granite 4.0 - Attention-SSM 하이브리드 비율과 성능 최적화</h1>
<p>거대 언어 모델(LLM)의 발전사에서 트랜스포머(Transformer) 아키텍처는 자연어 처리의 패러다임을 정립한 독보적인 존재였다. 그러나 모델의 규모가 커지고 처리해야 할 문맥(Context)의 길이가 수만 토큰을 넘어 수백만 토큰 단위로 확장됨에 따라, 트랜스포머의 근원적인 한계인 ‘2차 복잡도(Quadratic Complexity)’ 문제가 수면 위로 부상했다. 시퀀스 길이가 길어질수록 연산량과 메모리 사용량이 기하급수적으로 증가하는 이 문제는 특히 긴 문맥을 실시간으로 처리해야 하는 엔터프라이즈 환경에서 심각한 병목을 초래한다. 이에 대한 해답으로 등장한 것이 바로 상태 공간 모델(State Space Model, SSM)과 트랜스포머를 결합한 ’하이브리드 아키텍처(Hybrid Architecture)’이다. 본 장에서는 하이브리드 모델의 상용화를 이끈 두 가지 대표 사례, AI21 Labs의 <strong>Jamba</strong>와 IBM의 <strong>Granite 4.0</strong>을 중심으로, 이들이 채택한 Attention과 SSM(Mamba)의 배합 비율(Ratio) 전략과 이를 통한 성능 최적화 메커니즘을 심층적으로 분석한다.</p>
<h2>1.  하이브리드 아키텍처의 대두: 2차 복잡도의 탈피와 딜레마</h2>
<p>트랜스포머 아키텍처의 핵심인 셀프 어텐션(Self-Attention) 메커니즘은 입력 시퀀스 내의 모든 토큰 간 상호작용을 계산하여 전역적인(Global) 문맥을 포착하는 데 탁월한 성능을 발휘한다. 그러나 이는 <span class="math math-inline">O(N^2)</span>의 연산 복잡도를 수반하며, 특히 추론(Inference) 단계에서 과거의 정보를 저장하는 KV 캐시(Key-Value Cache)의 크기가 시퀀스 길이에 비례하여 선형적으로, 연산량은 제곱으로 증가하는 문제를 야기한다.1 이는 긴 문맥을 처리할 때 GPU 메모리 용량을 급속도로 소진시키고, 처리량(Throughput)을 저하시키는 주된 원인이다.</p>
<p>반면, Mamba로 대표되는 SSM 계열 아키텍처는 순환 신경망(RNN)과 유사하게 고정된 크기의 은닉 상태(Hidden State)를 통해 정보를 압축하여 전달함으로써 <span class="math math-inline">O(N)</span>의 선형 복잡도를 달성한다. 이는 시퀀스 길이가 아무리 길어져도 메모리 사용량이 거의 일정하게 유지됨을 의미한다. 그러나 순수 SSM은 긴 시퀀스 내의 특정 정보를 정밀하게 복원해야 하는 ’연상 기억(Associative Recall)’이나 ‘니들 인 어 헤이스택(Needle-in-a-Haystack)’ 작업에서 트랜스포머에 비해 성능이 저하되는 경향이 있다.3</p>
<p>따라서 현대의 포스트 트랜스포머 모델 설계자들은 이 두 아키텍처의 장점만을 취합하는 하이브리드 전략을 택했다. 핵심은 **“전체 레이어 중 얼마만큼을 값비싼 어텐션 레이어로 할당하고, 나머지를 효율적인 SSM 레이어로 대체할 것인가?”**라는 ’비율(Ratio)’의 최적화 문제로 귀결된다. Jamba와 Granite 4.0은 이 질문에 대해 각각 독자적이면서도 수렴하는 해답을 제시한다.</p>
<h2>2.  Jamba: 1:7 비율의 발견과 블록 단위 최적화</h2>
<p>AI21 Labs가 공개한 Jamba는 프로덕션 레벨에서 Attention-SSM 하이브리드 아키텍처를 성공적으로 구현하고 대규모로 확장한 선구적인 모델이다. Jamba의 설계 철학은 트랜스포머의 성능을 유지하면서도 단일 GPU에서 처리 가능한 문맥 길이를 극대화하는 데 초점을 맞추고 있다.</p>
<h3>2.1  Jamba 블록(Jamba Block)과 1:7 비율의 결정</h3>
<p>Jamba 아키텍처의 기본 단위는 ’Jamba 블록’이다. 이 블록은 총 8개의 레이어로 구성되는데, AI21 Labs의 연구진은 다양한 실험을 통해 <strong>1:7의 Attention-to-Mamba 비율</strong>을 최적점으로 도출했다.5</p>
<ul>
<li><strong>구조적 배치:</strong> 하나의 Jamba 블록 내에서 7개의 레이어는 Mamba 레이어로 구성되며, 단 1개의 레이어만이 트랜스포머의 어텐션 레이어로 할당된다. 즉, 전체 연산의 87.5%는 선형 복잡도를 가진 Mamba가 담당하고, 12.5%만이 2차 복잡도를 가진 어텐션이 담당한다.</li>
<li><strong>비율 선정의 근거 (Ablation Study):</strong> 초기 연구 단계에서 연구진은 1:3 비율(어텐션 25%)과 1:7 비율(어텐션 12.5%)을 비교하는 소거 연구(Ablation Study)를 수행했다. 놀랍게도 두 구성 간의 언어 모델링 성능(Perplexity, Loss) 차이는 거의 없는 것으로 나타났다.6 성능 저하가 없다면 연산 효율성이 월등히 높은 Mamba 레이어의 비중을 높이는 것이 유리하므로, 최종적으로 1:7 비율이 채택되었다.</li>
<li><strong>의의:</strong> 이는 거대 언어 모델이 문맥을 이해하는 데 있어 모든 레이어에서 전역적인 어텐션을 수행할 필요는 없으며, 간헐적인 어텐션(Sparse Attention Layers)만으로도 긴 문맥의 장기 의존성(Long-range Dependency)을 충분히 유지할 수 있음을 시사한다.5</li>
</ul>
<h3>2.2  Mamba-1의 채택과 ‘State Size’ 역설</h3>
<p>Jamba 개발 당시 이미 Mamba-2에 대한 논의가 있었음에도 불구하고, Jamba는 <strong>Mamba-1</strong> 아키텍처를 기반으로 설계되었다. 이는 하이브리드 아키텍처 설계 시 발생할 수 있는 흥미로운 기술적 역설을 보여준다.</p>
<ul>
<li><strong>Mamba-2의 특징:</strong> Mamba-2는 텐서 병렬화(Tensor Parallelism)를 위해 상태 공간 모델을 행렬 곱셈(Matrix Multiplication) 형태로 변환하며, 이를 통해 상태 차원(State Dimension, <span class="math math-inline">N</span>)을 크게 확장할 수 있는 장점이 있다.7</li>
<li><strong>하이브리드 환경에서의 효용성 감소:</strong> 그러나 Jamba 연구진은 하이브리드 구조 내에 ’어텐션 레이어’가 존재할 경우, Mamba-2가 제공하는 확장된 상태 차원의 이점이 희석된다는 사실을 발견했다. 어텐션 레이어가 이미 강력한 전역적 기억 장치 역할을 수행하기 때문에, Mamba 레이어까지 과도하게 큰 상태를 유지할 필요가 없어진 것이다.5</li>
<li><strong>결론:</strong> 결과적으로 Jamba 아키텍처에서는 Mamba-1의 선택적 스캔(Selective Scan) 메커니즘과 어텐션의 조합이 Mamba-2 조합보다 성능 대 효율성 측면에서 우수하다는 결론에 도달했다.5</li>
</ul>
<h3>2.3  MoE(Mixture-of-Experts)와의 삼중 결합</h3>
<p>Jamba는 Attention과 SSM의 하이브리드에 더해, 전문가 혼합(MoE) 모델을 결합하여 효율성을 극대화했다.</p>
<table><thead><tr><th><strong>특성</strong></th><th><strong>세부 사항</strong></th></tr></thead><tbody>
<tr><td><strong>전체 파라미터 (Total Params)</strong></td><td>52B (520억 개)</td></tr>
<tr><td><strong>활성 파라미터 (Active Params)</strong></td><td>12B (120억 개)</td></tr>
<tr><td><strong>MoE 주기</strong></td><td>매 2개 레이어마다 MoE 적용 (Ratio <span class="math math-inline">e=2</span>) 8</td></tr>
<tr><td><strong>전문가 구성</strong></td><td>총 16개 전문가 중 상위 2개(Top-2) 라우팅 8</td></tr>
</tbody></table>
<p>이러한 삼중 하이브리드(Attention-SSM-MoE) 구조 덕분에 Jamba는 52B 규모의 모델임에도 불구하고 추론 시에는 12B 모델 수준의 연산 자원만을 소모한다. 이는 단일 80GB A100 GPU에서 <strong>140K 토큰</strong>이라는 방대한 문맥을 처리할 수 있게 하는 핵심 동력이 된다.9 또한, 긴 문맥 처리 시 순수 트랜스포머 기반의 동급 모델(예: Mixtral 8x7B) 대비 **3배 이상의 처리량(Throughput)**을 기록했다.9</p>
<h3>2.4  안정성 최적화: RMSNorm과 Loss Spike 방지</h3>
<p>대규모 하이브리드 모델 학습의 난제 중 하나는 서로 다른 성격의 레이어가 혼재됨에 따라 발생하는 학습 불안정성이다. Jamba 연구진은 학습 중 Mamba 레이어 내부의 활성화 값(Activation)이 급격히 튀어 오르는 ‘Loss Spike’ 현상을 관찰했다. 이를 해결하기 위해 Mamba 레이어 내부의 활성화 경로에 <strong>RMSNorm</strong>을 추가하는 구조적 변경을 단행했다.8 이 미세한 엔지니어링 최적화는 대규모 스케일링에서 모델이 발산하지 않고 수렴하도록 만드는 결정적인 역할을 했다.</p>
<h2>3.  Granite 4.0: Mamba-2 기반의 9:1 비율과 엔터프라이즈 효율성</h2>
<p>IBM이 발표한 Granite 4.0은 Jamba의 접근법을 계승하면서도, 엔터프라이즈 환경에서의 비용 절감과 최신 기술인 <strong>Mamba-2</strong>를 적극 도입하여 또 다른 최적점을 제시한다. Granite 4.0은 “더 적은 자원으로 더 높은 성능(Betting on Efficiency over Scale)“이라는 기조 아래 설계되었다.10</p>
<h3>3.1  9:1 비율과 ‘Mamba Majority’ 아키텍처</h3>
<p>Granite 4.0은 전체 레이어 구성에서 **9:1의 비율(9 Mamba blocks per 1 Transformer block)**을 공식화했다.11 이는 전체의 90%를 SSM 레이어로 채우고, 단 10%만을 어텐션 레이어로 유지하는 과감한 구성이다.</p>
<ul>
<li><strong>선형 확장성(Linear Scaling)의 극대화:</strong> 90%의 레이어가 선형 복잡도를 가짐으로써, Granite 4.0은 문맥 길이가 증가하더라도 메모리 사용량과 추론 시간이 거의 일정하게 유지된다. IBM의 벤치마크에 따르면, 이러한 구조는 긴 입력과 다중 동시 배치(Concurrent Batches) 처리 시 기존 트랜스포머 모델 대비 <strong>70% 이상의 RAM 절감 효과</strong>를 제공한다.11</li>
<li><strong>Transformer Precision:</strong> 10%의 희소한 어텐션 레이어는 Mamba가 놓칠 수 있는 국소적 의존성(Local Dependencies)과 인컨텍스트 러닝(In-Context Learning) 능력을 보완하는 정밀 타격 역할을 수행한다. IBM은 이를 “Mamba의 속도와 Transformer의 정밀함의 결혼(Marrying the speed of Mamba with the precision of Transformer)“이라고 표현한다.13</li>
</ul>
<h3>3.2  Mamba-2 도입과 하드웨어 친화적 설계 (SSD)</h3>
<p>Jamba와 달리 Granite 4.0은 <strong>Mamba-2</strong>를 전면적으로 채택했다. Mamba-2는 ‘구조화된 상태 공간 쌍대성(Structured State Space Duality, SSD)’ 이론을 바탕으로 SSM의 순차적 연산을 행렬 곱셈(Matrix Multiplication)으로 변환할 수 있게 한다.4</p>
<ul>
<li><strong>텐서 코어(Tensor Core) 활용:</strong> Mamba-1이 맞춤형 CUDA 커널(Selective Scan)에 의존하여 하드웨어 최적화가 까다로웠다면, Mamba-2는 GPU의 텐서 코어가 가장 효율적으로 처리하는 행렬 연산을 주축으로 한다. 이는 학습 및 추론 속도를 비약적으로 향상시키며, 다양한 하드웨어 플랫폼에서의 호환성을 높여준다.</li>
<li><strong>비용 효율성:</strong> 엔터프라이즈 고객에게 중요한 것은 값비싼 H100 GPU뿐만 아니라, 보급형 GPU에서도 모델이 원활히 구동되는 것이다. Granite 4.0의 Mamba-2 기반 설계는 이러한 비용 효율성 요구를 충족시킨다.11</li>
</ul>
<h3>3.3  NoPE(No Positional Encoding) 전략과 일반화</h3>
<p>Granite 4.0의 또 다른 기술적 특징은 <strong>위치 인코딩(Positional Encoding)을 사용하지 않는(NoPE)</strong> 설계를 도입했다는 점이다.3</p>
<ul>
<li><strong>배경:</strong> 트랜스포머는 순서 정보가 없는 집합 연산이므로 위치 인코딩(RoPE 등)이 필수적이다. 그러나 이는 모델이 학습하지 않은 길이 이상의 문맥을 처리할 때 성능 저하(Length Extrapolation 문제)를 일으키는 원인이 되기도 한다.</li>
<li><strong>해결책:</strong> SSM은 그 자체로 순차적(Recurrent) 특성을 가지므로 위치 정보가 내재되어 있다. Granite 4.0은 어텐션 레이어에서도 명시적인 위치 인코딩을 제거함으로써, 학습된 길이보다 훨씬 긴 초장문 문맥(Ultra-long Context)에 대해서도 유연하게 일반화할 수 있는 능력을 확보했다.</li>
</ul>
<h3>3.4  모델 라인업과 MoE 세분화</h3>
<p>Granite 4.0은 크기에 따라 밀집(Dense) 모델과 하이브리드 MoE 모델을 구분하여 제공한다. 특히 Tiny 모델의 MoE 구성은 매우 효율적이다.</p>
<table><thead><tr><th><strong>모델명</strong></th><th><strong>전체 파라미터</strong></th><th><strong>활성 파라미터</strong></th><th><strong>아키텍처 특징</strong></th></tr></thead><tbody>
<tr><td><strong>Granite-4.0-H-Tiny</strong></td><td>7B</td><td><strong>1B</strong></td><td>Hybrid MoE, 미세 입자 전문가 라우팅 13</td></tr>
<tr><td><strong>Granite-4.0-H-Small</strong></td><td>32B</td><td><strong>9B</strong></td><td>Hybrid MoE 14</td></tr>
<tr><td><strong>Granite-4.0-H-Micro</strong></td><td>3B</td><td>3B</td><td>Dense Hybrid (MoE 미사용) 15</td></tr>
</tbody></table>
<p>Tiny 모델의 경우, 70억 개의 파라미터를 가지고 있지만 추론 시에는 단 10억 개(1B)만 활성화된다. 이는 모바일이나 에지 디바이스에서도 7B급 지능을 1B급 속도로 구동할 수 있음을 의미한다.</p>
<h2>4.  Attention-SSM 비율 최적화의 공학적 함의</h2>
<p>Jamba(1:7)와 Granite 4.0(9:1)의 사례, 그리고 NVIDIA의 Hymba(1:5)16와 같은 후속 연구들을 종합해 볼 때, 하이브리드 아키텍처의 최적 비율에 대한 몇 가지 공학적 통찰을 도출할 수 있다.</p>
<h3>4.1  황금 비율(Golden Ratio)의 수렴</h3>
<p>학계와 산업계의 실험 결과, Attention 대 SSM의 비율은 대략 <strong>1:3에서 1:10 사이</strong>로 수렴하고 있다.17</p>
<ul>
<li><strong>성능 포화(Saturation):</strong> 어텐션 레이어의 비율을 높일수록 초기에는 성능이 급격히 향상되지만, 일정 비율(약 10~20%)을 넘어가면 성능 향상폭이 둔화된다. 반면 연산 비용은 선형적 혹은 기하급수적으로 증가한다.</li>
<li><strong>스위트 스팟(Sweet Spot):</strong> Jamba의 1:7(약 12.5% Attention)과 Granite의 9:1(10% Attention)은 이 성능 포화 지점 직전에 위치하여 가성비를 극대화한 ’스위트 스팟’이다. 이는 “Attention은 필요하지만, 모든 곳에 필요하지는 않다(Attention is needed, but not all you need)“는 명제를 증명한다.</li>
</ul>
<h3>4.2  KV 캐시 병목의 해소와 RAG 최적화</h3>
<p>이러한 비율 최적화가 가져오는 가장 큰 이점은 <strong>KV 캐시 메모리의 획기적 감소</strong>이다.</p>
<ul>
<li><strong>메모리 절감:</strong> 256K 토큰 길이에서 Jamba는 순수 트랜스포머 대비 KV 캐시 용량을 <strong>10배(10x) 이상</strong> 줄였다.18 Granite 4.0 역시 70% 이상의 메모리 절감을 보고했다.</li>
<li><strong>RAG 및 에이전트 워크플로우:</strong> 검색 증강 생성(RAG) 시스템이나 복잡한 에이전트 시스템은 수십 개의 문서를 동시에 참조하거나(Multi-hop Reasoning), 긴 대화 기록을 유지해야 한다. 하이브리드 모델은 이러한 작업에서 메모리 부족(OOM) 없이 더 많은 문서를 문맥에 올릴 수 있어, 기업용 AI 시스템의 중추로 자리 잡기에 최적화되어 있다.19</li>
</ul>
<h3>4.3  니들 인 어 헤이스택(Needle-in-a-Haystack) 성능</h3>
<p>하이브리드 아키텍처의 성공 여부를 가르는 리트머스 시험지는 긴 문맥 속에서 미세한 정보를 찾아내는 ‘니들 인 어 헤이스택’ 테스트이다.</p>
<ul>
<li><strong>순수 SSM의 한계 극복:</strong> 순수 Mamba 모델은 상태 압축 과정에서 초기 정보가 소실되는 경향이 있어 이 테스트에 취약하다. 그러나 Jamba와 Granite 4.0은 소수의 어텐션 레이어가 ‘장기 기억의 앵커(Anchor)’ 역할을 수행함으로써 이 문제를 완벽히 해결했다.</li>
<li><strong>RULER 벤치마크:</strong> Jamba-1.5는 256K 길이의 RULER 벤치마크에서 유효 문맥 길이(Effective Length)를 100% 달성하며, 희소한 어텐션 배치만으로도 정보 손실 없는 회상이 가능함을 입증했다.18 이는 어텐션 레이어가 정보의 ’저장소’가 아니라, SSM이 압축한 정보들 사이를 연결하는 ‘인덱스’ 역할을 수행함을 시사한다.</li>
</ul>
<h2>5.  결론: 구성 가능한 하이브리드(Composable Hybrid) 시대로</h2>
<p>Jamba와 Granite 4.0은 포스트 트랜스포머 시대가 단순한 아키텍처의 교체가 아니라, 서로 다른 특성을 가진 아키텍처의 **최적 배합(Optimal Mixture)**으로 나아가고 있음을 보여준다. Jamba는 1:7 비율과 Mamba-1의 조합을 통해 긴 문맥 처리의 품질을 증명했고, Granite 4.0은 9:1 비율과 Mamba-2를 통해 엔터프라이즈급 효율성과 비용 절감을 실현했다.</p>
<p>이러한 하이브리드 아키텍처의 등장은 LLM의 개발 방향이 ’무조건적인 거대화’에서 ’목적에 맞는 효율화’로 전환되고 있음을 의미한다. 특히 KV 캐시의 제약에서 벗어난 이들 모델은 수백만 토큰을 실시간으로 다루는 차세대 에이전트, 온디바이스 AI, 그리고 대규모 RAG 시스템의 표준 인프라로 자리 잡을 것으로 전망된다. 향후 연구는 고정된 비율을 넘어, 입력 데이터의 복잡도에 따라 동적으로 어텐션 비율을 조절하거나(Dynamic Routing), 레이어별 역할을 더욱 세분화하는 방향으로 진화할 것이다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Meet Bamba, IBM’s new attention-state space model - IBM Research, https://research.ibm.com/blog/bamba-ssm-transformer-model</li>
<li>Mamba: Linear-Time Sequence Modeling with Selective State Spaces - arXiv, https://arxiv.org/pdf/2312.00752</li>
<li>IBM Granite 4.0 Deep Dive: Hybrid Mamba-Transformer Architecture - Skywork.ai, https://skywork.ai/blog/ibm-granite-4-0-deep-dive-hybrid-mamba-transformer-architecture/</li>
<li>NVIDIA NeMo Accelerates LLM Innovation with Hybrid State Space Model Support, https://developer.nvidia.com/blog/nvidia-nemo-accelerates-llm-innovation-with-hybrid-state-space-model-support/</li>
<li>Architectural Evolution in Large Language Models: A Deep Dive into Jamba’s Hybrid Transformer-Mamba Design - Greg Robison, https://gregrobison.medium.com/architectural-evolution-in-large-language-models-a-deep-dive-into-jambas-hybrid-transformer-mamba-c3efa8ca8cae</li>
<li>Jamba: A Hybrid Transformer-Mamba Language Model - arXiv, https://arxiv.org/html/2403.19887v1</li>
<li>From Mamba to Mamba-2 - Data Artificer and code:Breaker, https://n1o.github.io/posts/from-mamba-to-mamba2/</li>
<li>Jamba: A Hybrid Transformer-Mamba Language Model, https://arxiv.org/pdf/2403.19887</li>
<li>AI21 Labs Unveils Jamba: The First Production-Grade Mamba-Based AI Model - Maginative, https://www.maginative.com/article/ai21-labs-unveils-jamba-the-first-production-grade-mamba-based-ai-model/</li>
<li>Granite 4.0 bets big on small models - IBM, https://www.ibm.com/think/news/granite-4-bets-big-on-small-models</li>
<li>New IBM Granite 4 Models to Reduce AI Costs with Inference-Efficient Hybrid Mamba-2 Architecture - InfoQ, https://www.infoq.com/news/2025/11/ibm-granite-mamba2-enterprise/</li>
<li>IBM Granite 4: Deep Dive Into the Hybrid Mamba/Transformer LLM Family | by Sai Dheeraj Gummadi | Data Science in Your Pocket | Medium, https://medium.com/data-science-in-your-pocket/ibm-granite-4-deep-dive-into-the-hybrid-mamba-transformer-llm-family-c5d01978f27a</li>
<li>IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models, https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/</li>
<li>IBM Granite 4.0: Hyper-efficient, High Performance Hybrid Models for Enterprise, https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models</li>
<li>Granite 4.0 - IBM, https://www.ibm.com/granite/docs/models/granite</li>
<li>Hymba Hybrid-Head Architecture Boosts Small Language Model Performance, https://developer.nvidia.com/blog/hymba-hybrid-head-architecture-boosts-small-language-model-performance/</li>
<li>On the Tradeoffs of SSMs and Transformers | Goomba Lab, https://goombalab.github.io/blog/2025/tradeoffs/</li>
<li>Jamba-1.5: Hybrid Transformer-Mamba Models at Scale - arXiv, https://arxiv.org/pdf/2408.12570</li>
<li>Jamba mini 1.6 actually outperformed GPT-40 for our RAG support bot - Reddit, https://www.reddit.com/r/LocalLLaMA/comments/1kk66rj/jamba_mini_16_actually_outperformed_gpt40_for_our/</li>
<li>Jamba-1.5: Hybrid Transformer-Mamba Models at Scale - arXiv, https://arxiv.org/html/2408.12570v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>