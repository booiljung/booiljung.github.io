<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:8.1 테스트 타임 특징 학습(Test-Time Feature Learning) - Mamba의 ICL 메커니즘 규명</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>8.1 테스트 타임 특징 학습(Test-Time Feature Learning) - Mamba의 ICL 메커니즘 규명</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">MAMBA - 포스트 트랜스포머 시대의 도래</a> / <span>8.1 테스트 타임 특징 학습(Test-Time Feature Learning) - Mamba의 ICL 메커니즘 규명</span></nav>
                </div>
            </header>
            <article>
                <h1>8.1 테스트 타임 특징 학습(Test-Time Feature Learning) - Mamba의 ICL 메커니즘 규명</h1>
<h2>1.  서론: 상태 공간 모델의 르네상스와 ICL의 역설</h2>
<p>현대 인공지능 연구의 흐름에서 거대 언어 모델(Large Language Models, LLM)이 보여주는 인컨텍스트 학습(In-Context Learning, ICL) 능력은 기계 학습의 패러다임을 근본적으로 변화시킨 현상으로 간주된다. 모델이 가중치 업데이트 없이 프롬프트에 제시된 입출력 예시(Input-Output Examples)만으로 새로운 태스크의 규칙을 추론하고 적용하는 이 능력은, 초기에는 트랜스포머(Transformer) 아키텍처의 전유물로 여겨졌다.1 트랜스포머의 어텐션 메커니즘(Attention Mechanism)은 주어진 문맥(Context) 내의 모든 토큰 간 관계를 전역적으로 계산함으로써, 데이터 간의 상관관계를 파악하고 이를 기반으로 예측을 수행하는 강력한 귀납적 편향(Inductive Bias)을 제공한다.</p>
<p>그러나 트랜스포머의 이차적(Quadratic) 계산 복잡도는 긴 시퀀스를 처리하는 데 있어 치명적인 병목으로 작용하였고, 이는 선형 시간(Linear Time) 복잡도를 가지면서도 긴 문맥을 효과적으로 모델링할 수 있는 대안적 아키텍처에 대한 탐구를 촉발하였다. 그 정점에 서 있는 것이 바로 Mamba로 대표되는 선택적 상태 공간 모델(Selective State Space Models, SSM)이다.3 Mamba는 순환 신경망(Recurrent Neural Networks, RNN)의 효율성과 트랜스포머의 성능을 결합한 하이브리드적 특성을 보이며, 특히 언어 모델링, 유전체 분석 등 장기 의존성(Long-Range Dependency)이 중요한 분야에서 탁월한 성과를 입증하였다.</p>
<p>여기서 흥미로운 이론적 역설이 발생한다. 전통적인 순환 모델은 정보를 고정된 크기의 은닉 상태(Hidden State)에 압축하여 전달해야 하므로, 과거의 모든 정보를 명시적으로 참조할 수 있는 트랜스포머에 비해 ICL 수행에 구조적으로 불리할 것이라는 통념이 존재했다. 그러나 최근의 실증적 연구와 이론적 분석들은 Mamba가 트랜스포머에 필적하거나, 특정 조건(예: 아웃라이어가 많은 데이터, 매우 긴 문맥)에서는 오히려 이를 능가하는 ICL 능력을 보유하고 있음을 보여준다.2</p>
<p>본 장에서는 이러한 현상의 기저에 깔린 메커니즘을 **‘테스트 타임 특징 학습(Test-Time Feature Learning)’**이라는 개념을 중심으로 규명하고자 한다. 이는 모델이 단순히 훈련 데이터의 통계를 기억하는 것을 넘어, 추론 시점(Test-Time)에 주어지는 문맥 데이터로부터 문제의 본질적인 구조(Feature)를 능동적으로 학습하고 추출한다는 것을 의미한다.1 우리는 Mamba의 비선형 게이팅(Nonlinear Gating) 메커니즘이 어떻게 고차원 데이터 공간에서 유의미한 저차원 부분 공간(Subspace)을 식별하고, 이를 통해 ’차원의 저주’를 극복하며 효율적인 ICL을 수행하는지 심층적으로 분석할 것이다.</p>
<h2>2.  이론적 배경: 단일 인덱스 모델과 학습의 난이도</h2>
<p>Mamba의 ICL 메커니즘을 수학적으로 엄밀하게 분석하기 위해서는, 모델이 해결해야 할 목표 함수(Target Function)의 구조를 명확히 정의할 필요가 있다. 본 분석에서는 고차원 데이터 내의 저차원 구조를 설명하는 데 표준적으로 사용되는 **단일 인덱스 모델(Single-Index Model, SIM)**을 프레임워크로 채택한다.8</p>
<h3>2.1  단일 인덱스 모델 (Single-Index Model)</h3>
<p><span class="math math-inline">d</span>차원의 입력 벡터 <span class="math math-inline">x \in \mathbb{R}^d</span>와 스칼라 출력 <span class="math math-inline">y \in \mathbb{R}</span>가 주어졌을 때, 데이터 생성 과정이 다음과 같다고 가정하자.<br />
<span class="math math-display">
y = g_*(\langle \beta, x \rangle) + \epsilon
</span><br />
여기서 <span class="math math-inline">\beta \in \mathbb{R}^d</span>는 미지의 <strong>특징 벡터(Feature Vector)</strong> 또는 인덱스 벡터이며, <span class="math math-inline">g_*: \mathbb{R} \rightarrow \mathbb{R}</span>는 미지의 비선형 **링크 함수(Link Function)**이다. <span class="math math-inline">\epsilon</span>은 가우시안 노이즈 <span class="math math-inline">\mathcal{N}(0, \sigma^2)</span>를 따른다. 이 모델의 핵심은 고차원 입력 <span class="math math-inline">x</span>가 오직 1차원 부분 공간(방향 <span class="math math-inline">\beta</span>)으로 투영된 값 <span class="math math-inline">\langle \beta, x \rangle</span>을 통해서만 출력 <span class="math math-inline">y</span>에 영향을 미친다는 점이다.10</p>
<p>따라서 이 태스크를 해결하는 학습 알고리즘의 목표는 두 가지로 요약된다.</p>
<ol>
<li><strong>특징 학습(Feature Learning):</strong> 고차원 공간 <span class="math math-inline">\mathbb{R}^d</span>에서 유의미한 방향 <span class="math math-inline">\beta</span>를 찾아내는 것.</li>
<li><strong>함수 근사(Function Approximation):</strong> 투영된 값에 적용되는 비선형 함수 <span class="math math-inline">g_*</span>를 추정하는 것.</li>
</ol>
<h3>2.2  정보 지수(Information Exponent)와 게으른 학습(Lazy Learning)의 한계</h3>
<p>기존의 선형 어텐션(Linear Attention) 트랜스포머나 커널 릿지 회귀(Kernel Ridge Regression, KRR)와 같은 알고리즘들은 소위 <strong>‘게으른 학습(Lazy Learning)’</strong> 영역에서 작동하는 것으로 알려져 있다. 이 영역에서의 학습 난이도는 기저 함수 <span class="math math-inline">g_*</span>의 **정보 지수(Information Exponent)**에 의해 결정된다.11</p>
<p>정보 지수 <span class="math math-inline">k</span>는 링크 함수 <span class="math math-inline">g_*</span>를 에르미트 다항식(Hermite Polynomials) 기반으로 직교 전개했을 때, 0이 아닌 첫 번째 계수가 등장하는 차수로 정의된다.<br />
<span class="math math-display">
g_(z) = \sum_{j=0}^{\infty} \mu_j h_j(z), \quad k := \min { j &gt; 0 : \mu_j \neq 0 }
</span><br />
여기서 <span class="math math-inline">h_j</span>는 <span class="math math-inline">j</span>차 에르미트 다항식이다.</p>
<p>Ben Arous et al. (2021) 등의 연구에 따르면, 회전 불변(Rotationally Invariant) 커널을 사용하는 알고리즘이나 선형 트랜스포머가 이 함수를 학습하기 위해 필요한 샘플 수(Sample Complexity) <span class="math math-inline">N</span>은 다음과 같이 입력 차원 <span class="math math-inline">d</span>와 정보 지수 <span class="math math-inline">k</span>에 의존한다.13<br />
<span class="math math-display">
N \gtrsim d^k
</span><br />
이는 만약 <span class="math math-inline">g_*</span>가 고주파 성분을 포함하거나 대칭적인 구조(예: <span class="math math-inline">g_*(z) = z^2</span>, 이 경우 <span class="math math-inline">k=2</span>)를 가질 경우, 필요한 문맥 예시의 수가 차원의 지수승으로 폭증함을 의미한다. 이를 **‘차원의 저주(Curse of Dimensionality)’**라 하며, 선형 트랜스포머가 복잡한 비선형 함수를 ICL로 학습하는 데 근본적인 한계가 있음을 시사한다.7</p>
<h3>2.3  생성 지수(Generative Exponent)와 특징 학습의 도약</h3>
<p>반면, Mamba와 같은 모델이 보여주는 <strong>테스트 타임 특징 학습</strong> 능력은 이러한 한계를 뛰어넘는다. 최신 연구 결과들은 Mamba의 ICL 표본 복잡도가 정보 지수 <span class="math math-inline">k</span>가 아닌 <strong>생성 지수(Generative Exponent)</strong> <span class="math math-inline">k_{gen}</span>에 의존함을 밝혀냈다.7</p>
<p>생성 지수는 기저 함수 <span class="math math-inline">g_*</span>에 임의의 비선형 변환 <span class="math math-inline">T</span>를 적용했을 때 얻을 수 있는 최소의 정보 지수로 정의된다.<br />
<span class="math math-display">
k_{gen} := \min_{T} \text{IE}(T \circ g_)
</span><br />
대부분의 실용적인 함수에 대해 생성 지수 <span class="math math-inline">k_{gen}</span>은 정보 지수 <span class="math math-inline">k</span>보다 작거나 같으며, 많은 경우 1 또는 2의 낮은 값을 가진다. 예를 들어, <span class="math math-inline">g_*(z) = z^2</span>인 경우 정보 지수는 2이지만, 적절한 변환을 통해 학습 난이도를 낮출 수 있다.</p>
<p>이러한 이론적 발견은 Mamba가 단순히 입력 데이터 간의 유사도(Similarity)를 측정하는 커널 머신으로 동작하는 것이 아니라, 문맥 예시를 통해 데이터의 **기저 특징 공간(Intrinsic Feature Space)**을 능동적으로 학습하고 있음을 증명한다. 즉, Mamba는 입력 데이터를 특징 벡터 <span class="math math-inline">\beta</span>가 존재하는 저차원 공간으로 투영함으로써, 고차원 공간에서의 탐색 문제를 저차원 공간에서의 학습 문제로 환원시킨다.9</p>
<h2>3.  Mamba 아키텍처의 해부: ICL을 가능케 하는 구조적 요인</h2>
<p>Mamba가 어떻게 이론적으로 우월한 특징 학습을 수행하는지 이해하기 위해, 그 아키텍처의 핵심 요소들을 ICL 관점에서 재해석한다.</p>
<h3>3.1  선택적 상태 공간 모델 (Selective SSM)</h3>
<p>기존의 구조화된 상태 공간 모델(S4 등)은 선형 시간 불변(Linear Time Invariant, LTI) 시스템에 기반을 두었다.<br />
<span class="math math-display">
h_t = A h_{t-1} + B x_t
</span></p>
<p><span class="math math-display">
y_t = C h_t
</span></p>
<p>여기서 행렬 <span class="math math-inline">A, B, C</span>는 시간 <span class="math math-inline">t</span>에 관계없이 고정되어 있어, 입력 데이터의 내용(Context)에 따라 동적으로 적응해야 하는 ICL 태스크에는 부적합했다.</p>
<p>Mamba는 이를 극복하기 위해 **선택 메커니즘(Selection Mechanism)**을 도입하였다.16 이는 시스템의 파라미터가 입력 <span class="math math-inline">x_t</span>의 함수가 되도록 만든다.<br />
<span class="math math-display">
B_t = s_B(x_t), \quad C_t = s_C(x_t), \quad \Delta_t = s_\Delta(x_t)
</span></p>
<p><span class="math math-display">
\bar{A}_t = \exp(\Delta_t A), \quad \bar{B}_t = (\Delta_t A)^{-1}(\exp(\Delta_t A) - I) \cdot \Delta_t B_t
</span></p>
<p><span class="math math-display">
h_t = \bar{A}_t h_{t-1} + \bar{B}_t x_t
</span></p>
<p>이러한 **입력 의존성(Input-Dependence)**은 Mamba가 매 시점마다 문맥 정보를 얼마나 기억할지(<span class="math math-inline">\bar{A}_t</span>), 그리고 새로운 입력을 얼마나 받아들일지(<span class="math math-inline">\bar{B}_t</span>)를 결정하게 한다. 이는 ICL에서 필수적인 **‘문맥에 따른 적응(Adaptation)’**을 가능하게 하는 물리적 기반이다.</p>
<h3>3.2  입력 임베딩과 에르미트 확장의 구현</h3>
<p>Mamba가 비선형 함수를 학습할 수 있는 또 다른 이유는 입력 임베딩 단계에서의 설계에 있다. Oh et al. (2025)은 Mamba의 입력 임베딩이 상수항, 선형항뿐만 아니라 2차 에르미트 항(Hermite degree-2 terms)을 포함하도록 구성될 수 있음을 보였다.7<br />
<span class="math math-display">
\phi(x) = [1, x, H_2(x)]^\top
</span><br />
이러한 임베딩은 모델이 입력 데이터의 2차 모멘트 정보를 명시적으로 활용할 수 있게 하며, 이는 공분산 구조를 학습하는 데 결정적인 역할을 한다. 단일 레이어 Mamba라 할지라도 이러한 풍부한 임베딩과 비선형 게이팅이 결합되면, 다층 퍼셉트론(MLP)이 수행하는 것과 유사한 비선형 변환을 문맥 내에서 수행할 수 있다.9</p>
<h2>4.  메커니즘 1: 암묵적 경사 하강법(Implicit Gradient Descent) 에뮬레이션</h2>
<p>Mamba의 은닉 상태 업데이트 과정은 최적화 이론의 관점에서 **온라인 경사 하강법(Online Gradient Descent, OGD)**을 에뮬레이션하는 과정으로 해석될 수 있다. 이는 트랜스포머의 어텐션이 경사 하강법의 한 단계를 근사한다는 기존 연구를 SSM으로 확장한 것이다.17</p>
<h3>4.1  은닉 상태와 기울기 누적의 등가성</h3>
<p>선형 회귀 문제에서 손실 함수 <span class="math math-inline">\mathcal{L}(w) = \frac{1}{2} \sum (w^\top x_t - y_t)^2</span>를 최소화하는 과정을 고려하자. 표준적인 SGD 업데이트는 다음과 같다.<br />
<span class="math math-display">
w_{t+1} = w_t - \eta (w_t^\top x_t - y_t) x_t
</span><br />
Tian et al. (2024)과 Jiang et al. (2025)의 연구는 Mamba의 순환 식을 적절히 재구성하면 위와 같은 가중치 업데이트 식과 수학적으로 동형임을 증명하였다.17</p>
<p>구체적으로, Mamba의 은닉 상태 <span class="math math-inline">h_t</span>는 현재까지 관측된 데이터에 대한 기울기(Gradient)의 누적합 또는 **충분 통계량(Sufficient Statistics)**인 공분산 행렬 <span class="math math-inline">\sum x_i x_i^\top</span>과 교차 공분산 벡터 <span class="math math-inline">\sum x_i y_i</span>를 인코딩한다.<br />
<span class="math math-display">
h_t \approx \left[ \sum_{i=1}^t x_i x_i^\top, \sum_{i=1}^t x_i y_i \right]
</span><br />
Mamba의 게이팅 메커니즘은 여기서 학습률 <span class="math math-inline">\eta</span>와 유사한 역할을 수행하여, 과거 정보의 반영 비율을 조절한다. 특히 슬라이딩 윈도우 게이팅(Sliding Window Gating) 기법을 통해, Mamba는 단순한 온라인 SGD(배치 크기 1)보다 안정적인 미니배치 SGD 효과를 낼 수 있음이 밝혀졌다.17</p>
<h3>4.2  특징 벡터 <span class="math math-inline">\beta</span>로의 수렴 (Convergence to <span class="math math-inline">\beta</span>)</h3>
<p>이러한 암묵적 최적화 과정을 통해, Mamba의 은닉 상태는 시퀀스가 진행됨에 따라 실제 특징 벡터 <span class="math math-inline">\beta</span>와 정렬(Alignment)되는 방향으로 진화한다. 실험적으로, 문맥 길이가 길어질수록 Mamba 내부의 은닉 상태 벡터와 정답 <span class="math math-inline">\beta</span> 벡터 간의 코사인 유사도가 1에 수렴하는 현상이 관찰되었다.18</p>
<blockquote>
<p><strong>핵심 통찰:</strong> Mamba는 사전 학습된 고정 가중치를 가지고 있지만, 추론 시점에는 입력된 문맥에 따라 은닉 상태를 동적으로 업데이트함으로써 마치 모델 파라미터를 실시간으로 훈련하는 것과 같은 효과(Test-Time Training)를 낸다. 이때 은닉 상태는 학습된 모델의 ‘일시적 파라미터’ 저장소 역할을 한다.</p>
</blockquote>
<h2>5.  메커니즘 2: 비선형 게이팅을 통한 특징 선택과 차원 축소</h2>
<p>Mamba가 단순한 선형 회귀 이상의 복잡한 ICL을 수행할 수 있는 비결은 **비선형 게이팅(Nonlinear Gating)**에 있다. 이는 모델이 문맥 내에서 유용한 정보를 선별하고, 특징 공간의 차원을 효과적으로 축소하는 데 핵심적인 기여를 한다.</p>
<h3>5.1  이중 메커니즘: 선형 어텐션과 게이팅의 협업</h3>
<p>최신 이론적 분석은 Mamba의 ICL이 두 가지 메커니즘의 상호작용으로 이루어짐을 시사한다.6</p>
<ol>
<li><strong>선형 어텐션(Linear Attention) 역할 - 정보 검색:</strong> Mamba의 일부 연산 구조는 선형 어텐션과 유사하게 작동한다. 이는 현재 쿼리(Query)와 유사한 패턴을 가진 과거의 문맥 예시를 검색(Retrieve)하고 가중치를 부여하는 역할을 한다. 이 과정은 커널 방법론의 평활화(Laplacian Smoothing)와 유사하며, 데이터의 국소적인 경향성을 파악한다.1</li>
<li><strong>비선형 게이팅(Nonlinear Gating) 역할 - 특징 선택 및 아웃라이어 제거:</strong> 이것이 Mamba의 차별점이다. 선형 어텐션만으로는 모든 문맥 정보를 합산하게 되어 노이즈에 취약해진다. Mamba의 게이팅 층은 입력값의 크기나 패턴에 따라 특정 정보의 흐름을 **‘차단(Saturate)’**하거나 **‘증폭(Amplify)’**한다.</li>
</ol>
<h3>5.2  테스트 타임 특징 학습의 수학적 명제</h3>
<p>Mamba의 비선형 게이팅은 고차원 입력 <span class="math math-inline">x</span>를 기저 특징 <span class="math math-inline">\beta</span>와 관련된 성분만을 남기고 나머지는 억제하는 <strong>프로젝션(Projection)</strong> 연산을 근사한다. Oh et al. (2025)은 이를 다음과 같은 명제로 정식화하였다.8</p>
<blockquote>
<p>명제 (Mamba의 테스트 타임 특징 학습):</p>
<p>적절히 사전 학습된 Mamba 모델에 대해, 문맥 길이 <span class="math math-inline">N</span>이 <span class="math math-inline">N = \tilde{\Omega}(r \cdot \text{polylog}(d))</span> (여기서 <span class="math math-inline">r</span>은 내재 차원)를 만족하면, Mamba의 출력은 높은 확률로 다음과 같이 근사된다.<br />
<span class="math math-display">
\text{Mamba}(Z; \gamma^*) \approx \mathcal{P}(\langle \beta, x \rangle)
</span><br />
여기서 <span class="math math-inline">\mathcal{P}</span>는 특징 방향 <span class="math math-inline">\beta</span>로의 투영 및 비선형 변환을 포함하는 연산이다.</p>
</blockquote>
<p>이 명제는 Mamba가 전체 차원 <span class="math math-inline">d</span>가 아닌 유효 차원 <span class="math math-inline">r</span>에 비례하는 샘플만으로도 학습이 가능함을 의미하며, 이는 <span class="math math-inline">d^k</span>의 샘플을 요구하는 선형 트랜스포머/커널 베이스라인을 압도하는 효율성이다.7</p>
<h3>5.3  표본 복잡도 비교 분석</h3>
<p>다음 표는 Mamba와 다른 모델들의 ICL 표본 복잡도를 비교한 것이다.</p>
<table><thead><tr><th><strong>모델 아키텍처</strong></th><th><strong>ICL 메커니즘</strong></th><th><strong>표본 복잡도 (Sample Complexity)</strong></th><th><strong>주요 의존성</strong></th></tr></thead><tbody>
<tr><td><strong>선형 트랜스포머</strong></td><td>커널 릿지 회귀 (Lazy Learning)</td><td><span class="math math-inline">O(d^k)</span></td><td>정보 지수 <span class="math math-inline">k</span>, 주변 차원 <span class="math math-inline">d</span></td></tr>
<tr><td><strong>Softmax 트랜스포머</strong></td><td>경사 하강법 / 특징 학습</td><td><span class="math math-inline">O(d \cdot r)</span> 혹은 <span class="math math-inline">O(r^2)</span></td><td>내재 차원 <span class="math math-inline">r</span>, 생성 지수</td></tr>
<tr><td><strong>Mamba</strong></td><td><strong>테스트 타임 특징 학습</strong></td><td><strong><span class="math math-inline">O(r \cdot \text{polylog}(d))</span></strong></td><td><strong>내재 차원 <span class="math math-inline">r</span>, 생성 지수 <span class="math math-inline">k_{gen}</span></strong></td></tr>
</tbody></table>
<p>위 표에서 볼 수 있듯이, Mamba는 비선형 트랜스포머(Softmax Attention)와 유사한 수준의 효율적인 표본 복잡도를 가지면서도, 선형 시간 추론이라는 계산적 이점을 유지한다.7 이는 Mamba가 구조적으로는 RNN에 가깝지만, 기능적으로는 심층 비선형 모델의 학습 능력을 갖추었음을 시사한다.</p>
<h2>6.  메커니즘 3: 아웃라이어에 대한 강건성 (Robustness)</h2>
<p>실제 데이터 환경에서 ICL의 성능을 저해하는 주된 요인은 데이터에 포함된 이상치(Outlier)이다. Mamba의 ICL 메커니즘은 이러한 아웃라이어에 대해 탁월한 강건성을 보인다.</p>
<h3>6.1  아웃라이어 억제 원리</h3>
<p>Park et al. (2025)의 연구에 따르면, 선형 트랜스포머는 아웃라이어가 포함된 예시의 비율 <span class="math math-inline">\alpha</span>가 0.5를 넘어가면(<span class="math math-inline">\alpha &gt; 1/2</span>) 일반화 성능이 붕괴한다. 이는 선형 어텐션이 모든 토큰의 영향을 평균적으로 반영하기 때문이다.6</p>
<p>반면, Mamba는 <span class="math math-inline">\alpha</span>가 1에 근접하는 극한의 상황(대부분의 예시가 오염된 상황)에서도 정확한 예측을 유지할 수 있다. 그 원리는 다음과 같다.</p>
<ol>
<li><strong>게이팅의 포화(Saturation):</strong> Mamba의 게이팅 함수(예: Sigmoid, Softplus)는 입력값이 특정 범위를 벗어나는 큰 값을 가질 경우(아웃라이어), 출력이 0 또는 1로 포화된다.</li>
<li><strong>선별적 업데이트:</strong> 학습된 Mamba 모델은 아웃라이어를 포함한 토큰이 들어올 때 게이트 값을 0에 가깝게 닫아버림으로써(<span class="math math-inline">B_t \approx 0</span> 또는 <span class="math math-inline">\Delta_t \approx 0</span>), 해당 정보가 은닉 상태 <span class="math math-inline">h_t</span>를 오염시키는 것을 방지한다.19</li>
</ol>
<h3>6.2  유도 헤드(Induction Head)와의 유사성</h3>
<p>이러한 메커니즘은 트랜스포머의 ’유도 헤드(Induction Head)’가 수행하는 역할과 유사하다. 유도 헤드는 이전 문맥에서 현재 토큰과 일치하는 패턴을 찾아 그 다음 토큰을 복사하는 기능을 한다. Mamba의 Corollary 1 분석에 따르면, 선형 어텐션과 게이팅의 조합은 이러한 유도 헤드의 기능을 선형 시간 복잡도 내에서 구현하며, 특히 노이즈가 많은 환경에서 더 정교한 필터링을 수행한다.6</p>
<h2>7.  Mamba의 ICL: 한계와 미래 전망</h2>
<p>Mamba의 ICL 메커니즘은 효율성과 성능의 균형을 맞춘 혁신적인 접근법이지만, 여전히 몇 가지 한계와 연구 과제가 존재한다.</p>
<h3>7.1  순차적 처리의 본질적 제약</h3>
<p>Mamba는 본질적으로 순차적(Sequential) 모델이다. 따라서 트랜스포머처럼 문맥 전체를 동시에 조망(Global View)하여 문맥의 끝부분에 있는 정보와 앞부분의 정보를 즉각적으로 연결하는 능력은 상대적으로 제한될 수 있다. 비록 양방향(Bidirectional) Mamba 등의 변형이 제안되고 있으나, 인과적(Causal) 추론이 아닌 양방향 문맥 이해가 필수적인 태스크(예: 마스킹 된 언어 모델링)에서는 트랜스포머가 여전히 우위를 점할 수 있다.</p>
<h3>7.2  최적화 난이도</h3>
<p>비선형 게이팅은 강력한 표현력을 제공하지만, 동시에 손실 함수의 지형(Landscape)을 비볼록(Non-convex)하고 복잡하게 만든다. 이론적 분석에 따르면 Mamba는 선형 트랜스포머보다 수렴하기 위해 더 많은 훈련 반복(Training Iterations)이 필요할 수 있다.6 그러나 일단 수렴하고 나면, 그 결과물은 훨씬 더 강건하다.</p>
<h3>7.3  결론: 적응형 필터로서의 Mamba</h3>
<p>결론적으로, Mamba의 ICL 메커니즘은 **‘비선형 게이팅을 통한 테스트 타임 특징 학습’**으로 정의된다. Mamba는 정적인 파라미터에 의존하지 않고, 문맥에 따라 동적으로 변하는 상태 공간을 통해 기저 함수의 특징을 실시간으로 학습한다. 이는 Mamba를 단순한 시계열 예측 모델이 아닌, **‘적응형 필터(Adaptive Filter)’**이자 **‘메타 학습자(Meta-Learner)’**로 재정의하게 한다.</p>
<p>본 장에서 규명한 Mamba의 ICL 원리는 향후 더 효율적인 거대 모델을 설계하는 데 중요한 이론적 지침을 제공한다. 특히 주변 차원이 아닌 내재 차원에 의존하는 Mamba의 특성은, 데이터의 양과 차원이 폭발적으로 증가하는 현대 AI 환경에서 자원의 효율적 활용을 위한 핵심적인 열쇠가 될 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Mamba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning, https://arxiv.org/html/2510.12026v1</li>
<li>[PDF] Is Mamba Capable of In-Context Learning? - Semantic Scholar, https://www.semanticscholar.org/paper/Is-Mamba-Capable-of-In-Context-Learning-Grazzi-Siems/57a6c75ebb987ea29a1f904de23f72451e095032</li>
<li>Mamba Explained - The Gradient, https://thegradient.pub/mamba-explained/</li>
<li>Mamba: Linear-Time Sequence Modeling with Selective State Spaces - arXiv, https://arxiv.org/pdf/2312.00752</li>
<li>[R] Is Mamba Capable of In-Context Learning? : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/1ak3jpn/r_is_mamba_capable_of_incontext_learning/</li>
<li>Can Mamba Learn In Context with Outliers? A Theoretical Generalization Analysis - arXiv, https://arxiv.org/html/2510.00399v1</li>
<li>[Literature Review] Mamba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning - Moonlight, https://www.themoonlight.io/en/review/mamba-can-learn-low-dimensional-targets-in-context-via-test-time-feature-learning</li>
<li>Mamba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning - arXiv, https://arxiv.org/pdf/2510.12026</li>
<li>MAMBA CAN LEARN LOW-DIMENSIONAL TARGETS IN-CONTEXT VIA TEST-TIME FEATURE LEARNING - OpenReview, https://openreview.net/pdf/b872836f3f5ba22e086f394ffe799c2dd9c8c920.pdf</li>
<li>[2510.12026] Mamba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning - arXiv, https://arxiv.org/abs/2510.12026</li>
<li>Learning sum of diverse features: computational hardness and efficient gradient-based training for ridge combinations, https://proceedings.mlr.press/v247/oko24a/oko24a.pdf</li>
<li>From Information to Generative Exponent: Learning Rate Induces, https://arxiv.org/html/2510.21020v1</li>
<li>Neural network learns low-dimensional polynomials with SGD near the information-theoretic limit - arXiv, <a href="https://arxiv.org/pdf/2406.01581">https://arxiv.org/pdf/2406.01581?</a></li>
<li>Learning single-index models with neural networks - Denny Wu, https://dennywu1.github.io/single_index_SGD.pdf</li>
<li>The Generative Leap: Sharp Sample Complexity for Efficiently Learning Gaussian Multi-Index Models - arXiv, https://arxiv.org/pdf/2506.05500</li>
<li>Mamba: Linear-Time Sequence Modeling with Selective State Spaces - Arxiv Dives, https://ghost.oxen.ai/mamba-linear-time-sequence-modeling-with-selective-state-spaces-arxiv-dives/</li>
<li>state-space models can learn in-context by - arXiv, https://arxiv.org/abs/2410.11687</li>
<li>Trained Mamba Emulates Online Gradient Descent in In … - arXiv, https://arxiv.org/pdf/2509.23779</li>
<li>Can Mamba Learn In Context with Outliers? A Theoretical …, https://openreview.net/forum?id=tswBfpkwHn</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>