<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:6.1 [Vision] Vision Mamba (Vim) & VSSD - 시각적 상태 공간 쌍대성과 양방향 처리</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>6.1 [Vision] Vision Mamba (Vim) & VSSD - 시각적 상태 공간 쌍대성과 양방향 처리</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">MAMBA - 포스트 트랜스포머 시대의 도래</a> / <span>6.1 [Vision] Vision Mamba (Vim) & VSSD - 시각적 상태 공간 쌍대성과 양방향 처리</span></nav>
                </div>
            </header>
            <article>
                <h1>6.1 [Vision] Vision Mamba (Vim) &amp; VSSD - 시각적 상태 공간 쌍대성과 양방향 처리</h1>
<h2>1.  서론: 시각적 표현 학습의 새로운 패러다임</h2>
<p>컴퓨터 비전(Computer Vision)의 역사는 데이터를 어떻게 효율적으로 처리하고, 그 안에 내재된 특징(Feature)을 어떻게 추출할 것인가에 대한 끊임없는 탐구의 과정이었다. 지난 10년 동안 합성곱 신경망(Convolutional Neural Networks, CNN)과 비전 트랜스포머(Vision Transformers, ViT)는 이 분야를 지배하는 양대 산맥으로 자리 잡았다. CNN은 국소적 수용 영역(Local Receptive Field)과 이동 불변성(Translation Invariance)이라는 강력한 귀납적 편향(Inductive Bias)을 통해 이미지의 공간적 계층 구조를 효과적으로 학습했다. 반면, ViT는 자기 주의(Self-Attention) 메커니즘을 통해 이미지 전체를 아우르는 전역적 수용 영역(Global Receptive Field)을 확보함으로써, 대규모 데이터셋과 모델 크기에서 탁월한 확장성(Scalability)을 증명했다.1</p>
<p>그러나 이 두 아키텍처는 명확한 한계를 지닌다. CNN은 전역적 문맥(Global Context)을 파악하기 위해 네트워크를 깊게 쌓아야 하며, 이는 계산 비용의 증가와 최적화의 어려움을 초래한다. 더 심각한 문제는 ViT에 있다. 자기 주의 메커니즘은 입력 토큰 수 <span class="math math-inline">N</span>에 대해 <span class="math math-inline">O(N^2)</span>의 이차적 계산 복잡도(Quadratic Complexity)를 가진다. 이는 이미지 해상도가 증가함에 따라 연산량과 메모리 요구량이 기하급수적으로 폭증함을 의미하며, 4K 해상도 이상의 고해상도 이미지 처리나 긴 비디오 시퀀스 분석, 그리고 3D 의료 영상 처리와 같은 분야에서 치명적인 병목 현상을 유발한다.3</p>
<p>이러한 배경에서 상태 공간 모델(State Space Models, SSMs), 특히 Mamba 아키텍처의 등장은 시각적 표현 학습에 있어 혁명적인 전환점을 제시한다. Mamba는 순환 신경망(RNN)의 선형적 추론 효율성(<span class="math math-inline">O(N)</span>)과 트랜스포머의 병렬 학습 능력, 그리고 전역적 문맥 모델링 능력을 동시에 갖춘 모델이다. 텍스트와 같은 1차원 시퀀스 데이터에서 트랜스포머에 준하는 성능을 입증한 Mamba는 이제 **Vision Mamba (Vim)**와 **VSSD (Visual State Space Duality)**라는 형태로 진화하여 2차원 시각 데이터의 복잡성을 정복하려 하고 있다. 본 장에서는 비전 태스크를 위한 SSM의 적용 원리, Vim의 양방향 처리 메커니즘, 그리고 최신 Mamba-2 이론을 바탕으로 한 VSSD의 시각적 상태 공간 쌍대성 및 하드웨어 가속 원리에 대해 심층적으로 분석한다.</p>
<h2>2.  상태 공간 모델(SSM)의 기초와 시각적 데이터의 불일치</h2>
<p>Mamba 아키텍처를 이해하기 위해서는 그 뿌리가 되는 상태 공간 모델(SSM)의 수학적 기초와, 이를 이미지라는 2차원 데이터에 적용할 때 발생하는 근본적인 구조적 불일치를 먼저 파악해야 한다.</p>
<h3>2.1  연속 시스템의 이산화와 선택적 스캔(Selective Scan)</h3>
<p>SSM은 본래 제어 이론에서 연속적인 시간(Continuous-time) 상의 시스템을 모델링하기 위해 개발되었다. 현대적인 딥러닝 기반 SSM은 입력을 잠재 상태(Latent State) <span class="math math-inline">h(t)</span>를 통해 출력 <span class="math math-inline">y(t)</span>로 매핑하는 1차 미분 방정식으로 정의된다.5<br />
<span class="math math-display">
h&#39;(t) = \mathbf{A}h(t) + \mathbf{B}x(t)
</span></p>
<p><span class="math math-display">
y(t) = \mathbf{C}h(t)
</span></p>
<p>여기서 <span class="math math-inline">h(t) \in \mathbb{R}^N</span>은 시스템의 숨겨진 상태를 나타내며, <span class="math math-inline">\mathbf{A} \in \mathbb{R}^{N \times N}</span>는 상태 전이 행렬(Evolution Matrix), <span class="math math-inline">\mathbf{B} \in \mathbb{R}^{N \times 1}</span>와 <span class="math math-inline">\mathbf{C} \in \mathbb{R}^{1 \times N}</span>는 각각 입력 및 출력 투영 행렬이다.</p>
<p>컴퓨터와 같은 디지털 시스템에서 이를 처리하기 위해서는 연속 시스템을 이산화(Discretization)해야 한다. 주로 Zero-Order Hold (ZOH) 방식을 사용하여 샘플링 시간 간격 <span class="math math-inline">\Delta</span>에 대해 파라미터를 변환하면 다음과 같은 이산 점화식(Recurrence)이 도출된다.6<br />
<span class="math math-display">
h_t = \bar{\mathbf{A}}h_{t-1} + \bar{\mathbf{B}}x_t
</span></p>
<p><span class="math math-display">
y_t = \mathbf{C}h_t
</span></p>
<p>여기서 이산화된 행렬은 <span class="math math-inline">\bar{\mathbf{A}} = \exp(\Delta \mathbf{A})</span>이고 <span class="math math-inline">\bar{\mathbf{B}} = (\Delta \mathbf{A})^{-1}(\exp(\Delta \mathbf{A}) - \mathbf{I}) \cdot \Delta \mathbf{B}</span>로 정의된다. 이 식은 RNN과 매우 유사한 형태를 띠며, <span class="math math-inline">t</span> 시점의 상태가 이전 시점 <span class="math math-inline">t-1</span>의 상태에 의존하는 순차적(Sequential) 특성을 가진다.</p>
<p>기존의 LTI(Linear Time-Invariant) SSM과 달리, Mamba의 핵심 혁신은 행렬 <span class="math math-inline">\mathbf{B}, \mathbf{C}, \Delta</span>를 입력 <span class="math math-inline">x_t</span>에 대한 함수로 만드는 **선택적 스캔 메커니즘(Selective Scan Mechanism, S6)**을 도입한 것이다.3 이는 모델이 입력 내용에 따라 정보를 선택적으로 기억하거나 망각할 수 있게 하여, 불필요한 정보를 걸러내고 중요한 문맥을 장기간 유지할 수 있게 한다. 이는 선형 시간 복잡도를 유지하면서도 복잡한 데이터 의존성을 모델링할 수 있는 핵심 기제이다.</p>
<h3>2.2  1차원 인과성과 2차원 비인과성의 충돌</h3>
<p>Mamba는 텍스트나 오디오와 같은 1차원 시계열 데이터에서 탁월한 성능을 발휘한다. 그러나 이미지는 본질적으로 <strong>비인과적(Non-causal)</strong> 데이터라는 점이 문제의 핵심이다.9 텍스트는 단어의 순서가 명확하고 미래의 단어가 과거에 영향을 주지 않는 인과적 구조를 가지지만, 이미지는 2차원 평면 위에 픽셀들이 공간적으로 배치되어 있으며, 특정 픽셀의 의미는 상하좌우 모든 방향의 픽셀들에 의해 결정된다.</p>
<p>Vision Mamba 계열의 모델들은 이미지를 처리하기 위해 이미지를 <span class="math math-inline">P \times P</span> 크기의 패치(Patch)로 분할하고, 이를 1차원 시퀀스로 평탄화(Flattening)하는 방식을 취한다.5 이 과정에서 두 가지 심각한 문제가 발생한다.</p>
<ol>
<li><strong>공간적 인접성의 파괴:</strong> 2차원 공간상에서 수직으로 인접해 있던 패치들이 1차원 시퀀스로 나열될 때는 서로 멀리 떨어진 위치에 배치된다. 이는 모델이 수직 방향의 로컬 컨텍스트를 학습하는 것을 어렵게 만든다.9</li>
<li><strong>단방향 처리의 한계:</strong> 기본적인 SSM은 <span class="math math-inline">t</span> 시점의 출력을 계산하기 위해 <span class="math math-inline">t</span> 이전의 정보만을 사용한다. 그러나 이미지의 좌상단에서 우하단으로 스캔할 경우, 우하단에 위치한 패치들은 좌상단의 정보를 참조할 수 있지만, 반대로 좌상단의 패치들은 우하단의 정보를 참조할 수 없다. 이는 전역적 문맥 이해를 저해하며, 특히 객체 검출이나 세분화(Segmentation)와 같은 밀집 예측(Dense Prediction) 과제에서 성능 저하의 주된 원인이 된다.4</li>
</ol>
<p>따라서 시각적 데이터를 위한 SSM, 즉 Vision Mamba를 설계하는 데 있어 핵심 과제는 <strong>선형 복잡도의 효율성을 유지하면서 어떻게 비인과적인 2차원 공간 정보를 효과적으로 통합할 것인가</strong>에 집중된다.</p>
<h2>3.  Vision Mamba (Vim): 양방향 SSM을 통한 효율적 시각 표현</h2>
<p>**Vision Mamba (Vim)**는 이러한 문제를 해결하기 위해 고안된 최초의 순수 SSM 기반 비전 백본 중 하나이다. Vim은 트랜스포머의 자기 주의 메커니즘을 완전히 제거하고, 그 자리를 **양방향 상태 공간 모델(Bidirectional SSM)**로 대체함으로써 고해상도 이미지 처리 효율성을 극대화했다.3</p>
<h3>3.1  양방향 Mamba 블록 (Bidirectional Mamba Block)의 아키텍처</h3>
<p>Vim의 아키텍처는 ViT와 유사하게 패치 임베딩 단계에서 시작하지만, 인코더 블록 내부의 동작 방식은 완전히 다르다. Vim 블록의 핵심은 입력 시퀀스를 두 개의 독립적인 방향으로 스캔하여 융합하는 것이다.5</p>
<ol>
<li><strong>시퀀스 생성 및 투영:</strong> 입력 이미지는 패치 토큰 시퀀스 <span class="math math-inline">T</span>로 변환된 후, 정규화(Normalization)와 선형 투영(Linear Projection)을 거쳐 두 개의 경로(<span class="math math-inline">x</span>와 <span class="math math-inline">z</span>)로 나뉜다.</li>
<li><strong>1D 합성곱 (Conv1d):</strong> SSM에 들어가기 전, 시퀀스는 1D 합성곱 레이어를 통과한다. 이는 국소적인 특징(Local Dependency)을 포착하고, SSM의 초기 상태를 부드럽게 만들어주는 역할을 한다.7</li>
<li><strong>양방향 선택적 스캔 (Bidirectional Selective Scan):</strong></li>
</ol>
<ul>
<li><strong>Forward Scan:</strong> 시퀀스를 <span class="math math-inline">t=1</span>부터 <span class="math math-inline">N</span>까지 순차적으로 처리한다. 이는 이미지의 좌상단에서 우하단으로 흐르는 문맥을 모델링하며, 이전 토큰들의 정보가 누적되어 현재 토큰에 전달된다.</li>
<li><strong>Backward Scan:</strong> 시퀀스를 <span class="math math-inline">t=N</span>부터 <span class="math math-inline">1</span>까지 역순으로 처리한다. 이는 물리적으로 이미지를 뒤집어서 처리하는 것과 같으며, 결과적으로 이미지의 우하단(미래) 정보가 좌상단(과거) 토큰의 상태 업데이트에 반영되도록 한다.5</li>
</ul>
<ol start="4">
<li><strong>게이팅 및 융합:</strong> 정방향과 역방향 스캔의 출력은 각각의 게이트(Gating) 가지를 통과한 후, 요소별 곱(Element-wise Product) 또는 합을 통해 융합된다. 마지막으로 잔차 연결(Residual Connection)을 통해 정보의 흐름을 원활하게 한다.</li>
</ol>
<p>이러한 양방향 설계는 각 토큰이 이미지 전체의 정보를 “과거“와 “미래“라는 두 가지 시간축을 통해 간접적으로 모두 참조할 수 있게 해준다. 이는 <span class="math math-inline">O(N)</span>의 복잡도를 유지하면서도 전역적 수용 영역을 확보하는 Vim만의 독창적인 방식이다.3</p>
<h3>3.2  위치 임베딩(Positional Embedding)의 필수성과 논쟁</h3>
<p>Vim은 ViT와 마찬가지로 입력 패치 시퀀스에 학습 가능한 절대 위치 임베딩(Absolute Positional Embedding)을 추가한다.<br />
<span class="math math-display">
z_i = \text{Linear}(x_i) + E_{pos}[i]
</span><br />
여기서 흥미로운 학술적 논쟁이 발생한다. “SSM은 본질적으로 순환 신경망(RNN)과 유사하게 순서 정보를 내재적으로 처리하는데, 굳이 명시적인 위치 임베딩이 필요한가?“라는 의문이다.4</p>
<p>연구 결과에 따르면, SSM의 상태 <span class="math math-inline">h_t</span>가 순서 정보를 암묵적으로 포함하는 것은 사실이다. 그러나 상태가 전이됨에 따라 초기 위치의 정보는 점차 희석되는 경향(Forget Gate의 영향)이 있다. 또한, 시각적 객체의 인식에 있어서는 상대적인 순서뿐만 아니라 절대적인 좌표 정보가 매우 중요하다. 예를 들어, 하늘은 주로 이미지 상단에, 땅은 하단에 위치한다는 공간적 편향(Spatial Bias)을 학습하기 위해서는 절대 위치 정보가 필수적이다.8</p>
<p>실험적으로도 위치 임베딩을 제거했을 때, 이미지 분류(Classification)보다는 객체 검출(Detection)이나 세분화(Segmentation)와 같은 위치 민감도가 높은 태스크에서 성능 저하가 두드러지게 나타났다.4 이는 Vim에서 위치 임베딩이 단순한 보조 수단이 아니라, 비인과적 2D 공간을 1D 시퀀스로 해석하는 과정에서 발생하는 정보 손실을 보정하는 핵심 장치임을 시사한다. 다만, 최근 Vim-F와 같은 변형 연구에서는 주파수 도메인 정보를 활용하여 위치 임베딩 없이도 성능을 유지할 수 있는 가능성을 제시하기도 했다.14</p>
<h3>3.3  Vim의 성능 및 효율성 분석</h3>
<p>Vim은 ImageNet-1K 분류, COCO 객체 검출, ADE20k 의미론적 분할 등 주요 벤치마크에서 기존의 ViT 모델(DeiT 등)을 압도하는 효율성을 보여준다.8</p>
<ul>
<li><strong>메모리 효율성:</strong> 입력 해상도가 <span class="math math-inline">1248 \times 1248</span>일 때, Vim은 DeiT 대비 GPU 메모리 사용량을 약 <strong>86.8%</strong> 절감한다. 이는 ViT가 <span class="math math-inline">O(N^2)</span> 크기의 어텐션 맵(Attention Map)을 저장해야 하는 반면, Vim은 고정된 크기의 상태(State)만을 유지하면 되기 때문이다.8</li>
<li><strong>추론 속도:</strong> 동일한 고해상도 조건에서 Vim은 DeiT보다 약 <strong>2.8배</strong> 빠른 추론 속도를 보인다. 시퀀스 길이가 길어질수록 이 격차는 더욱 벌어지며, 이는 Vim이 고해상도 이미지 처리에 특화된 백본임을 증명한다.15</li>
<li><strong>정확도:</strong> ImageNet Top-1 정확도에서 Vim-Small은 81.6%를 기록하여, 유사한 파라미터 수를 가진 DeiT-Small(79.8%)을 상회한다. 이는 SSM이 이미지의 장기 의존성(Long-range Dependency)을 효과적으로 학습하고 있음을 방증한다.13</li>
</ul>
<h2>4.  VSSD (Visual State Space Duality): 비인과적 이중성과 텐서 코어 가속</h2>
<p>Vim이 Mamba-1(S6) 기반의 양방향 스캔을 통해 비전 태스크를 해결했다면, **VSSD (Visual State Space Duality)**는 Mamba-2에서 정립된 <strong>상태 공간 쌍대성(State Space Duality, SSD)</strong> 이론을 시각 데이터에 맞게 확장하여 구조적 한계를 극복하고 하드웨어 효율성을 극대화한 최신 아키텍처이다.9</p>
<h3>4.1  Mamba-2의 SSD 이론과 텐서 코어(Tensor Core)</h3>
<p>Mamba-2는 기존 SSM의 순차적 처리 방식(Recurrence)과 트랜스포머의 주의 메커니즘(Attention) 간의 수학적 이중성(Duality)을 규명했다. 핵심 아이디어는 상태 전이 행렬 <span class="math math-inline">\mathbf{A}</span>를 대각 행렬(Diagonal Matrix) 또는 스칼라 구조로 제한하는 것이다. 이렇게 하면 SSM의 재귀적 연산을 대규모 행렬 곱셈(Matrix Multiplication) 형태로 변환할 수 있다.16<br />
<span class="math math-display">
\mathbf{Y} = \text{SSD}(\mathbf{X}) = \text{Mask}(\mathbf{Q}\mathbf{K}^\top) \cdot \mathbf{V}
</span><br />
여기서 <span class="math math-inline">\text{Mask}</span>는 1-semiseparable 행렬 구조를 가지며, 이는 SSM의 순차적 정보 전파 특성을 행렬 형태로 표현한 것이다. 이러한 SSD 형태의 가장 큰 장점은 GPU의 **텐서 코어(Tensor Core)**를 활용할 수 있다는 점이다. 기존 Mamba-1의 선택적 스캔은 순차적 의존성 때문에 병렬 처리에 한계가 있었으나, SSD는 이를 행렬 곱 연산으로 치환하여 최신 GPU 하드웨어에서 연산 처리량(Throughput)을 비약적으로 높일 수 있다.18</p>
<h3>4.2  VSSD의 핵심: 비인과적 SSD (Non-Causal SSD)</h3>
<p>Mamba-2의 SSD는 기본적으로 <strong>인과적(Causal)</strong> 마스킹을 전제로 한다. 즉, 마스크 행렬이 하삼각 행렬(Lower Triangular Matrix) 형태를 띠어, <span class="math math-inline">t</span> 시점의 출력은 <span class="math math-inline">t</span> 이전의 입력에만 영향을 받는다. 이는 앞서 지적했듯이 비인과적인 이미지 데이터에는 적합하지 않다. VSSD는 이 문제를 해결하기 위해 **NC-SSD (Non-Causal State Space Duality)**를 제안한다.9</p>
<p>NC-SSD는 기존 SSD의 인과적 제약을 제거하기 위해 다음과 같은 수학적, 구조적 변형을 가한다.</p>
<ol>
<li><strong>비인과적 마스킹 및 통합:</strong> VSSD는 인과적 마스크를 제거하거나, 이를 전역적 상호작용이 가능한 형태로 변형한다. 구체적으로는 은닉 상태와 토큰 간의 상호작용의 절대적 크기(Magnitude) 정보를 일부 폐기하고, 상대적인 가중치(Relative Weights)를 보존하는 전략을 취한다. 이를 통해 <span class="math math-inline">t</span> 시점의 토큰이 <span class="math math-inline">t</span> 이후의 미래 토큰 정보에도 접근할 수 있는 수학적 경로를 연다.9</li>
<li><strong>다중 스캔 전략의 행렬 통합:</strong> Vim과 유사하게 2D 공간 정보를 보존하기 위해 다중 스캔(Multi-scan) 전략을 사용하지만, 이를 물리적인 순회(Loop)가 아닌 행렬 연산 내부로 통합한다. 2D 평면을 다양한 방향(가로, 세로, 대각선 등)으로 횡단하는 스캔 결과를 비인과적 행렬 연산을 통해 한 번에 계산함으로써, 2D 공간상에서 인접하지만 1D 시퀀스에서는 멀어진 토큰들 간의 관계(Spatial Continuity)를 효과적으로 복원한다.10</li>
</ol>
<p>이러한 접근 방식은 Mamba-2의 하드웨어 친화적인 특성을 유지하면서도, 비전 트랜스포머의 전역적 주의 메커니즘과 유사한 효과를 선형 복잡도 내에서 구현해낸다.</p>
<h3>4.3  VSSD의 성능 우위와 훈련 가속</h3>
<p>VSSD 모델은 NC-SSD 블록을 계층적으로 쌓아 올린 구조를 가지며, 실험 결과에서 다음과 같은 압도적인 성능을 보였다.</p>
<ul>
<li><strong>정확도:</strong> ImageNet-1K 데이터셋에서 VSSD는 유사한 파라미터 수를 가진 VMamba 대비 약 <strong>1%</strong> 높은 Top-1 정확도를 기록했다. 이는 SSM 기반 모델이 CNN(ConvNeXt)이나 ViT(Swin Transformer)와 대등하거나 그 이상의 성능을 낼 수 있음을 증명한다.9</li>
<li><strong>훈련 속도:</strong> VSSD는 인과적 제약을 제거하고 행렬 연산을 최적화함으로써, 기존의 Mamba-1 기반 모델이나 양방향 스캔 방식(Bi-SSD) 대비 훈련 속도를 <strong>20% ~ 50%</strong> 가속화했다. 이는 대규모 데이터셋 학습 시 시간과 비용을 획기적으로 절감할 수 있는 요소이다.9</li>
<li><strong>추론 지연 시간(Latency):</strong> VSSD는 동일한 연산량(FLOPs)을 가진 다른 모델들에 비해 실제 추론 속도가 더 빠르다. 이는 메모리 대역폭(Memory Bandwidth)의 효율적인 사용과 텐서 코어 활용 덕분이며, 실시간성이 요구되는 자율주행이나 로보틱스 분야에서 큰 강점이 된다.16</li>
</ul>
<h2>5.  아키텍처 비교 및 심층 분석: Vim vs. VMamba vs. VSSD</h2>
<p>SSM 기반 비전 모델들은 각기 다른 방식으로 1D-2D 불일치 문제를 해결해왔다. 다음 표는 주요 모델들의 핵심 메커니즘과 특성을 비교 분석한 것이다.</p>
<h3>5.1 [표 6.1.1] Vision Mamba 계열 모델과 기존 모델 비교 분석</h3>
<table><thead><tr><th><strong>특성</strong></th><th><strong>ViT (DeiT/Swin)</strong></th><th><strong>Vim</strong></th><th><strong>VMamba</strong></th><th><strong>VSSD</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 연산</strong></td><td>Self-Attention</td><td>Bidirectional SSM</td><td>Cross-Scan SSM (SS2D)</td><td><strong>Non-Causal SSD</strong></td></tr>
<tr><td><strong>계산 복잡도</strong></td><td><span class="math math-inline">O(N^2)</span> (Quadratic)</td><td><span class="math math-inline">O(N)</span> (Linear)</td><td><span class="math math-inline">O(N)</span> (Linear)</td><td><strong><span class="math math-inline">O(N)</span> (Linear)</strong></td></tr>
<tr><td><strong>스캔 방식</strong></td><td>전역적 (Global)</td><td>2방향 (정방향/역방향)</td><td>4방향 (좌우/우좌/상하/하상)</td><td><strong>다중 스캔 통합 행렬 연산</strong></td></tr>
<tr><td><strong>인과성</strong></td><td>Non-Causal</td><td>Pseudo Non-Causal</td><td>Pseudo Non-Causal</td><td><strong>Inherently Non-Causal</strong></td></tr>
<tr><td><strong>하드웨어 효율</strong></td><td>낮음 (메모리 병목)</td><td>중간 (SRAM 의존)</td><td>중간 (복잡한 스캔 로직)</td><td><strong>최상 (Tensor Core 활용)</strong></td></tr>
<tr><td><strong>위치 임베딩</strong></td><td>필수 (Absolute/Relative)</td><td>필수 (Absolute)</td><td>불필요 (스캔이 위치 정보 포함)</td><td><strong>선택적 / 덜 민감함</strong></td></tr>
<tr><td><strong>주요 강점</strong></td><td>검증된 성능, 생태계</td><td>단순성, 높은 효율성</td><td>2D 구조 보존 우수</td><td><strong>학습/추론 속도, 성능 균형</strong></td></tr>
</tbody></table>
<h3>5.2  스캔 전략의 진화: 1D에서 행렬로</h3>
<p>초기 <strong>Vim</strong>은 1D 시퀀스에 대한 정방향/역방향 스캔을 통해 2D 이미지를 처리했다. 이는 구현이 간단하고 효율적이지만, 수직 방향의 인접성을 직접적으로 포착하는 데에는 한계가 있었다. 이를 개선하기 위해 등장한 <strong>VMamba</strong>는 4방향(좌우, 우좌, 상하, 하상) 크로스 스캔(Cross-Scan Module, CSM)을 도입했다.7 이는 공간적 관계를 더욱 촘촘하게 학습할 수 있게 했으나, 스캔 경로가 늘어남에 따라 계산 비용과 구현 복잡도가 증가했다.</p>
<p><strong>VSSD</strong>는 여기서 한 단계 더 나아가, 물리적인 스캔 루프(Loop)를 도는 대신 상태 공간 쌍대성(SSD)을 이용해 전역적 상호작용을 행렬 연산으로 한 번에 처리하는 방식을 택했다. 이는 모델이 2D 구조를 명시적으로 순회하지 않고도, 수학적인 비인과적 연산을 통해 모든 토큰이 서로의 정보에 즉각적으로 접근할 수 있게 만듦을 의미한다. 이는 ’순차적 처리’라는 SSM의 본질을 유지하면서도 ’병렬 처리’라는 트랜스포머의 장점을 흡수한 형태이다.10</p>
<h3>5.3  하드웨어 효율성과 유효 수용 영역(ERF)</h3>
<p>하드웨어 관점에서 Vim과 VMamba는 순차적인 의존성 때문에 병렬 처리에 제약이 있으며, GPU의 SRAM과 HBM(High Bandwidth Memory) 사이의 데이터 이동(I/O)이 병목이 될 수 있다. 반면, VSSD는 주요 연산을 대규모 행렬 곱셈으로 변환했기 때문에 I/O 오버헤드를 최소화하고 연산 집약적인 텐서 코어를 풀 가동할 수 있다.19</p>
<p>또한, <strong>유효 수용 영역(Effective Receptive Field, ERF)</strong> 분석에 따르면, VSSD는 트랜스포머와 유사하게 초기 레이어부터 전역적인 ERF를 형성하는 반면, CNN은 깊은 레이어에 도달해야만 전역 정보를 볼 수 있다. Vim과 VMamba는 그 중간 형태로, 스캔 방향에 따라 ERF가 십자형이나 대각선 형태로 나타나는 특징을 보인다.22 VSSD의 ERF는 더 넓고 고르게 분포하여 이미지 전체의 문맥을 균형 있게 파악함을 시사한다.</p>
<h2>6.  응용 및 확장성: 고해상도와 다차원 데이터로의 도약</h2>
<p>Vim과 VSSD가 제시한 아키텍처는 단순한 이미지 분류를 넘어, <span class="math math-inline">O(N^2)</span> 복잡도 때문에 ViT 적용이 어려웠던 고해상도 및 다차원 데이터 처리 분야에서 혁신적인 성과를 내고 있다.</p>
<h3>6.1  밀집 예측 (Object Detection &amp; Segmentation)</h3>
<p>객체 검출과 의미론적 분할에서는 입력 해상도가 높을수록 작은 객체나 정교한 경계를 잘 찾아낼 수 있다. Vim과 VSSD는 선형 복잡도 덕분에 고해상도 피처 맵을 유지하면서도 전역 문맥을 파악할 수 있다.</p>
<ul>
<li><strong>Vim:</strong> COCO 객체 검출 태스크에서 ResNet-50 기반 백본 대비 더 높은 mAP(mean Average Precision)를 달성했으며, 특히 마스크 R-CNN과 결합했을 때 인스턴스 분할 성능이 우수했다.8</li>
<li><strong>VSSD:</strong> VM-RTDETR과 같은 실시간 객체 검출 모델의 백본으로 사용되어, 복잡한 환경(예: 양돈 농가 모니터링)에서의 탐지 정확도와 속도를 동시에 향상시켰다. VSSD는 작은 객체에 대한 민감도와 배경 노이즈에 대한 강건성(Robustness)이 뛰어남이 입증되었다.23</li>
</ul>
<h3>6.2  의료 영상 (Medical Imaging)</h3>
<p>의료 영상(CT, MRI, X-ray)은 해상도가 매우 높고(<span class="math math-inline">4096 \times 4096</span> 이상), 3D 볼륨 데이터인 경우가 많아 연산 비용 문제로 ViT 적용이 제한적이었다.</p>
<ul>
<li><strong>Vim 적용:</strong> 흉부 X-ray를 이용한 결핵 탐지에서 Vision Mamba는 ResNet-50과 DenseNet-121을 능가하는 94.32%의 정확도를 기록했다. 놀라운 점은 GPU 메모리 사용량이 ViT 대비 80%나 적어, 리소스가 제한적인 의료 현장(Edge Device)에서의 배포 가능성을 열었다는 것이다.25</li>
<li><strong>VSSD 확장:</strong> VMS2-UNet과 같은 파생 모델은 의료 영상 분할(Segmentation)에서 SOTA(State-of-the-Art)를 기록했다. 3D 스캔 데이터를 긴 시퀀스로 간주하여 처리할 때, VSSD의 비인과적 처리 능력은 장기 의존성(예: 장기의 시작과 끝)을 놓치지 않고 파악하는 데 핵심적인 역할을 한다.20</li>
</ul>
<h3>6.3  원격 탐사 (Remote Sensing)</h3>
<p>위성 이미지는 단일 이미지가 10억 개 이상의 픽셀로 구성될 수 있는 초대형 데이터이다. 기존 ViT는 이를 작은 타일로 잘라서 처리(Tiling)해야 했기에 타일 경계에서 정보 손실이 발생했다.</p>
<ul>
<li><strong>초고해상도 처리:</strong> Vim과 VSSD는 선형 복잡도를 통해 이러한 초대형 이미지를 타일링 없이 통째로 처리하거나, 훨씬 더 큰 윈도우 크기로 처리할 수 있는 가능성을 제시했다. 이는 지형지물의 연속성을 파악하거나 광범위한 지역의 변화 탐지(Change Detection)를 수행하는 데 있어 비약적인 성능 향상을 가져왔다.18</li>
</ul>
<h2>7.  결론 및 향후 전망</h2>
<p>Vision Mamba(Vim)와 VSSD는 컴퓨터 비전 분야에서 **“Self-Attention 없이도 효율적인 전역적 모델링이 가능하다”**는 명제를 증명했다. 이들은 단순한 ViT의 대안이 아니라, 효율성(Efficiency)과 성능(Performance)의 트레이드오프를 근본적으로 재정의하는 새로운 아키텍처이다.</p>
<ol>
<li><strong>Vim</strong>은 1D 시퀀스 모델인 Mamba를 양방향 스캔과 위치 임베딩을 통해 2D 비전 영역으로 성공적으로 이식한 첫 번째 이정표였다. 이는 비전 백본으로서 SSM의 가능성을 열었으며, 특히 고해상도 이미지에서의 메모리 효율성을 입증했다.</li>
<li><strong>VSSD</strong>는 Mamba-2의 이론적 진보인 ’상태 공간 쌍대성’을 비전 데이터의 비인과적 특성에 맞게 수정(Non-Causal SSD)함으로써, 구조적 연속성을 보존하고 텐서 코어 활용도를 극대화했다. 이는 SSM이 단순한 RNN의 변형이 아니라, Attention의 표현력과 CNN의 효율성을 수학적으로 통합한 차세대 아키텍처임을 시사한다.</li>
</ol>
<p>향후 연구는 VSSD의 비인과적 처리 방식을 비디오(3D)나 4D 시공간 데이터로 확장하거나, CNN 및 트랜스포머와의 하이브리드 설계를 통해 각 아키텍처의 귀납적 편향을 최적으로 결합하는 방향으로 나아갈 것으로 전망된다.1 또한, VSSD의 학습 안정성을 높이기 위한 정규화 기법이나, 위치 임베딩을 완전히 제거할 수 있는 새로운 주파수 도메인 스캔 방식에 대한 연구도 지속될 것이다.14 결론적으로 Vision Mamba와 VSSD는 고해상도, 초장기 의존성 처리가 필요한 차세대 AI 시스템의 핵심 백본으로 자리 잡을 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>MambaVision: A Hybrid Mamba-Transformer Vision Backbone, https://openaccess.thecvf.com/content/CVPR2025/papers/Hatamizadeh_MambaVision_A_Hybrid_Mamba-Transformer_Vision_Backbone_CVPR_2025_paper.pdf</li>
<li>Vision Mamba for Permeability Prediction of Porous Media - arXiv, https://arxiv.org/html/2510.14516v1</li>
<li>Vision Mamba: Efficient Visual Representation Learning with …, https://huggingface.co/blog/mikelabs/vision-mamba-efficient-visual-representation-learn</li>
<li>Vision Mamba: Like a Vision Transformer but Better, https://towardsdatascience.com/vision-mamba-like-a-vision-transformer-but-better-3b2660c35848/</li>
<li>Vision Mamba (ViM) Backbone - Emergent Mind, https://www.emergentmind.com/topics/vision-mamba-vim</li>
<li>A Survey on Mamba Architecture for Vision Applications - arXiv, https://arxiv.org/pdf/2502.07161</li>
<li>A Survey on Visual Mamba - MDPI, https://www.mdpi.com/2076-3417/14/13/5683</li>
<li>Vision Mamba: Efficient Visual Representation Learning with … - arXiv, https://arxiv.org/pdf/2401.09417</li>
<li>VSSD: Vision Mamba with Non-Causal State Space Duality - arXiv, https://arxiv.org/html/2407.18559v2</li>
<li>VSSD: Vision Mamba with Non-Casual State Space Duality - arXiv, https://arxiv.org/html/2407.18559v1</li>
<li>Adventurer: Optimizing Vision Mamba Architecture Designs for … - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC12574601/</li>
<li>Efficient Visual Representation with Bidirectional State Space Models, https://medium.com/@sorush.gm/mamba-vision-efficient-visual-representation-with-bidirectional-state-space-models-1086cd89f5bf</li>
<li>Vision Mamba: Efficient Visual Representation Learning with … - Liner, https://liner.com/review/vision-mamba-efficient-visual-representation-learning-with-bidirectional-state-space</li>
<li>Vim-F: Visual State Space Model Benefiting from Learning in … - arXiv, https://arxiv.org/html/2405.18679v1</li>
<li>Daily Papers - Hugging Face, <a href="https://huggingface.co/papers?q=Vision+Mamba">https://huggingface.co/papers?q=Vision%20Mamba</a></li>
<li>Vision Mamba towards Real-time Dense Visual Perception with Non …, https://arxiv.org/html/2511.12671v1</li>
<li>(Structured State Space Duality.) State space duality… - ResearchGate, https://www.researchgate.net/figure/Structured-State-Space-Duality-State-space-duality-describes-the-close-relationship_fig1_381109265</li>
<li>Mamba for Remote Sensing: Architectures, Hybrid Paradigms, and …, https://www.preprints.org/manuscript/202512.0078/v1</li>
<li>Recurrent Architectures and Hardware Efficient Implementations for …, https://epub.jku.at/download/pdf/12767121.pdf</li>
<li>[PDF] VSSD: Vision Mamba with Non-Causal State Space Duality, https://www.semanticscholar.org/paper/VSSD%3A-Vision-Mamba-with-Non-Causal-State-Space-Shi-Dong/0da8568dc1b3dfc781c51881c082a83f731bc89f</li>
<li>Appendix for VSSD: Vision Mamba with Non-Causal State Space …, https://openaccess.thecvf.com/content/ICCV2025/supplemental/Shi_VSSD_Vision_Mamba_ICCV_2025_supplemental.pdf</li>
<li>VMamba: Visual State Space Model - arXiv, https://arxiv.org/html/2401.10166v3</li>
<li>VM-RTDETR: Advancing DETR with Vision State-Space Duality and …, https://pmc.ncbi.nlm.nih.gov/articles/PMC12649245/</li>
<li>VM-RTDETR: Advancing DETR with Vision State-Space Duality and …, https://www.mdpi.com/2076-2615/15/22/3328</li>
<li>Vision Mamba for efficient Tuberculosis Detection based on Chest X …, https://www.researchgate.net/publication/394944876_Vision_Mamba_for_efficient_Tuberculosis_Detection_based_on_Chest_X-Rays_A_comparative_study_with_CNN_and_Vision_transformers</li>
<li>Mamba for Remote Sensing: Architectures, Hybrid Paradigms, and …, https://www.preprints.org/manuscript/202512.0078/v1/download</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>