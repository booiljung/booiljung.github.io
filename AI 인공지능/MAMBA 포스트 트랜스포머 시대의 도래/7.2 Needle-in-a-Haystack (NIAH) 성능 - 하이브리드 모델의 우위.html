<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:7.2 Needle-in-a-Haystack (NIAH) 성능 - 하이브리드 모델의 우위</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>7.2 Needle-in-a-Haystack (NIAH) 성능 - 하이브리드 모델의 우위</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">MAMBA - 포스트 트랜스포머 시대의 도래</a> / <span>7.2 Needle-in-a-Haystack (NIAH) 성능 - 하이브리드 모델의 우위</span></nav>
                </div>
            </header>
            <article>
                <h1>7.2 Needle-in-a-Haystack (NIAH) 성능 - 하이브리드 모델의 우위</h1>
<p>트랜스포머(Transformer) 아키텍처의 등장은 자연어 처리(NLP) 분야에 혁명을 가져왔으나, 동시에 ’문맥의 길이(Context Length)’와 ‘연산 효율성(Computational Efficiency)’ 사이의 딜레마라는 난제를 남겼다. 모델이 처리해야 할 정보의 양이 늘어날수록, 즉 ’건초더미(Haystack)’가 거대해질수록 그 안에 숨겨진 ’바늘(Needle)’을 찾아내는 비용은 기하급수적으로 증가한다. 어텐션(Attention) 메커니즘의 2차 시간 복잡도(<span class="math math-inline">O(N^2)</span>)는 긴 문맥 처리를 물리적으로 제약하는 병목이었다. 이에 대한 대안으로 선형 복잡도(<span class="math math-inline">O(N)</span>)를 가진 상태 공간 모델(State Space Models, SSM)이 주목받았으나, 이들 역시 기억의 압축 과정에서 발생하는 정보 손실이라는 본질적 한계에 부딪혔다. 본 7.2절에서는 포스트 트랜스포머 시대의 핵심 쟁점인 ‘Needle-in-a-Haystack (NIAH)’ 성능을 중심으로, 순수 트랜스포머와 순수 SSM의 한계를 분석하고, 왜 이 둘을 결합한 <strong>하이브리드(Hybrid) 아키텍처</strong>가 긴 문맥 처리와 정밀한 정보 검색의 최적 해법으로 부상했는지에 대해 심층적으로 논의한다. Jamba, StripedHyena, Hymba, Samba 등 최신 하이브리드 모델들의 기술적 메커니즘과 벤치마크 데이터를 토대로, 이들이 어떻게 기억의 정확성과 연산의 효율성이라는 두 마리 토끼를 잡았는지 규명할 것이다.</p>
<h2>1.  서론: 문맥의 확장과 기억의 본질</h2>
<p>인공지능, 특히 거대언어모델(LLM)의 발전사는 ’기억 용량의 확장’과 궤를 같이한다. 초기 RNN(Recurrent Neural Networks)이 겪었던 ‘장기 의존성(Long-term Dependency)’ 문제를 해결한 트랜스포머는 모든 토큰 간의 관계를 직접 계산함으로써 탁월한 성능을 보였으나, 입력 시퀀스가 길어질수록 연산량과 메모리 사용량이 폭발하는 구조적 한계를 가지고 있었다. 2023년 이후 등장한 Mamba와 같은 SSM 기반 모델들은 이러한 연산 효율성 문제를 해결하며 ’무한한 문맥’의 가능성을 제시했지만, 곧바로 새로운 질문에 직면하게 되었다. “효율적으로 처리된 수만, 수십만 토큰의 정보 중에서, 모델은 과연 아주 작은 세부 사항 하나를 정확하게 기억해낼 수 있는가?”</p>
<p>이 질문에 답하기 위해 고안된 것이 바로 ‘Needle-in-a-Haystack (NIAH)’ 테스트이다. NIAH는 단순히 긴 텍스트를 읽는 능력을 넘어, 방대한 정보의 바다(Haystack) 속에 무작위로 삽입된 특정 정보(Needle, 예: 패스키, 특정 숫자, 무의미한 문장 등)를 손실 없이 정확하게 회수(Recall)해내는 능력을 측정한다.1 이는 모델의 ’문해력’보다는 ’기억력’과 ’검색 능력’을 극한으로 시험하는 벤치마크이며, RAG(Retrieval-Augmented Generation) 시스템이나 긴 문서 분석과 같은 실제 애플리케이션의 성능을 가늠하는 가장 확실한 지표로 자리 잡았다.3</p>
<p>최근의 연구 결과들은 순수 SSM 모델들이 텍스트의 전체적인 맥락을 파악하는 데는 능숙하지만, NIAH와 같이 높은 정밀도를 요구하는 작업에서는 트랜스포머에 비해 현저히 떨어지는 성능을 보임을 시사한다.5 반면, 트랜스포머의 어텐션 메커니즘을 전략적으로 혼합한 하이브리드 모델들은 순수 SSM의 효율성을 유지하면서도 트랜스포머에 버금가거나 이를 능가하는 검색 성능을 입증하고 있다. 본 절에서는 이러한 현상의 기저에 깔린 이론적 원리와 아키텍처 설계의 묘미를 파헤친다.</p>
<h2>2.  순수 아키텍처의 한계와 딜레마</h2>
<p>하이브리드 모델의 우위를 논하기 위해서는 먼저 순수 트랜스포머와 순수 SSM이 NIAH 테스트에서 겪는 각각의 한계를 명확히 이해해야 한다. 이는 단순히 성능 수치의 차이가 아니라, 정보를 처리하고 저장하는 근본적인 메커니즘의 차이에서 기인한다.</p>
<h3>2.1 ) 트랜스포머: 완벽한 기억의 대가</h3>
<p>트랜스포머의 어텐션 메커니즘은 본질적으로 ‘내용 기반 주소 지정(Content-based Addressing)’ 시스템이다. 쿼리(Query)가 주어지면, 과거의 모든 키(Key)와 비교하여 관련성이 높은 밸류(Value)를 추출한다.</p>
<ul>
<li><strong>메커니즘의 강점:</strong> 모든 과거 토큰에 대해 직접적인 접근(Random Access)이 가능하다. 이론적으로 문맥 윈도우 내에 있는 정보라면, 그것이 아무리 사소하고 오래된 정보라 할지라도 어텐션 점수(Attention Score)를 통해 정확하게 소환할 수 있다. 이는 NIAH 테스트에서 트랜스포머가 전통적으로 강력한 모습을 보이는 이유다.7</li>
<li><strong>구조적 한계:</strong> 이 완벽한 접근성은 막대한 비용을 요구한다. 모든 과거 상태를 KV Cache(Key-Value Cache)라는 형태로 메모리에 상주시켜야 하며, 문맥 길이가 <span class="math math-inline">L</span>일 때 메모리 요구량은 <span class="math math-inline">O(L)</span>로 선형 증가하지만, 연산량은 <span class="math math-inline">O(L^2)</span>로 증가한다(최적화된 어텐션 제외 시). 100K 토큰 이상의 초장문 처리에 있어 KV Cache는 수십 GB에서 수백 GB의 GPU 메모리를 점유하게 되며, 이는 추론 속도(Latency)와 처리량(Throughput)을 심각하게 저하시키는 병목이 된다.8</li>
</ul>
<p>즉, 트랜스포머는 NIAH 성능은 완벽하지만, 그 성능을 유지하기 위한 비용이 감당하기 어려운 수준으로 치솟는다는 치명적인 단점을 가진다.</p>
<h3>2.2 ) 순수 SSM (Mamba): 압축과 망각의 불가피성</h3>
<p>Mamba로 대표되는 SSM은 순환 신경망(RNN)과 유사하게, 이전 시점의 정보를 고정된 크기의 은닉 상태(Hidden State, <span class="math math-inline">h_t</span>)로 압축하여 다음 시점으로 전달한다.</p>
<ul>
<li><strong>메커니즘의 강점:</strong> 문맥 길이에 상관없이 상태의 크기가 고정되어 있어 메모리 사용량이 일정하고(<span class="math math-inline">O(1)</span>), 추론 시 연산 비용이 문맥 길이에 대해 선형(<span class="math math-inline">O(N)</span>)이다. 이는 수십만 토큰의 처리를 가능하게 하는 핵심 동력이다.10</li>
<li><strong>구조적 한계 (상태 압축 손실):</strong> 고정된 크기의 상태 벡터에 무한한 길이의 정보를 압축해 넣어야 한다. Mamba는 입력에 따라 정보를 선별적으로 저장하는 ’선택 메커니즘(Selection Mechanism)’을 도입하여 이를 개선했으나, 정보 이론적으로 무손실 압축은 불가능하다. NIAH 테스트에서 요구하는 ’니들(Needle)’은 종종 문맥적 개연성이 전혀 없는 무작위 숫자나 패스키인 경우가 많은데, SSM은 이러한 ‘예측 불가능한’ 정보를 노이즈로 간주하여 필터링하거나, 새로운 정보가 들어옴에 따라 점차적으로 상태에서 희석시키는 경향이 있다.5</li>
</ul>
<p>특히 6와 6의 연구에 따르면, Mamba-2와 같은 최신 SSM조차도 시퀀스 길이가 2K를 넘어가면 패스키 검색(Passkey Retrieval) 정확도가 급격히 하락하는 현상이 관찰되었다. 이는 순수 SSM이 ’의미(Gist)’는 기억하지만 ’정밀한 데이터(Exact Data)’를 장기간 보존하는 데에는 태생적인 한계가 있음을 시사한다.</p>
<h3>2.3 ) 이론적 분석: Gather-and-Aggregate (G&amp;A) 가설</h3>
<p>최근 연구 13는 이러한 현상을 ‘Gather-and-Aggregate’ 메커니즘으로 설명한다. 트랜스포머와 SSM 모두 문맥 정보를 수집(Gather)하고 통합(Aggregate)하는 과정을 거치지만, 그 방식에 결정적인 차이가 있다.</p>
<ul>
<li><strong>트랜스포머:</strong> 특정 어텐션 헤드가 명시적인 ‘수집 헤드(Gather Head)’ 역할을 하여 필요한 정보를 날카롭게(Sharp) 추출한다. 이는 불연속적이고 희소한 정보 검색에 최적화되어 있다.</li>
<li><strong>SSM:</strong> 정보 수집 기능이 상태 전이 행렬에 녹아 있어, 훨씬 더 부드러운(Smoother) 주의 집중 패턴을 보인다. 이는 연속적인 신호 처리나 언어 모델링에는 유리할 수 있으나, NIAH와 같이 핀포인트 정확도가 필요한 작업에서는 정보의 경계가 흐릿해지는 원인이 된다.</li>
</ul>
<p>결국, “효율적인 압축(SSM)“과 “정밀한 검색(Attention)“은 상충 관계(Trade-off)에 있으며, 단일 아키텍처만으로는 이 두 가지 요구사항을 동시에 만족시키기 어렵다는 결론에 도달한다. 이것이 하이브리드 모델이 필연적으로 등장할 수밖에 없었던 배경이다.</p>
<h2>3.  하이브리드 아키텍처의 설계 철학 및 메커니즘</h2>
<p>하이브리드 모델은 SSM의 효율적인 ’장기 문맥 압축 능력’과 어텐션의 ’정밀한 회상 능력’을 결합하여 시너지를 극대화하는 전략을 취한다. 이는 인지과학적으로 인간의 기억 구조인 ’장기 기억(Long-term Memory)’과 ’작업 기억(Working Memory)’의 협력 모델과도 유사하다. SSM 레이어는 텍스트의 방대한 배경과 흐름을 처리하여 압축된 상태로 유지하고, 간헐적으로 배치된 어텐션 레이어는 그중에서 결정적인 세부 사항을 명확하게 끄집어내는 역할을 수행한다.</p>
<p>현재 학계와 산업계에서 주목받고 있는 주요 하이브리드 아키텍처들의 구체적인 설계 방식과 NIAH 성능 향상 기제를 상세히 분석해 본다.</p>
<h3>3.1 ) Jamba (AI21 Labs): 1:7 비율의 최적화</h3>
<p>Jamba는 하이브리드 아키텍처의 가장 성공적인 상용화 사례 중 하나로, Mamba 레이어와 트랜스포머 레이어를 <strong>7:1</strong>의 비율로 혼합한 구조를 채택했다.14</p>
<ul>
<li><strong>구조적 특징:</strong> Jamba 블록은 7개의 Mamba 레이어가 연속으로 이어진 후 1개의 어텐션 레이어가 배치되는 형태를 띤다. 이는 전체 레이어의 1/8만이 어텐션 연산을 수행함을 의미한다. 또한, 매 2개 레이어마다 MoE(Mixture-of-Experts)를 적용하여 총 파라미터 수를 늘리면서도 추론 시 활성 파라미터는 낮게 유지하는 전략을 병행했다.15</li>
<li><strong>NIAH 성능 메커니즘:</strong></li>
<li><strong>글로벌 문맥 동기화:</strong> 연속된 Mamba 레이어들이 국소적인 정보와 장기 문맥을 효율적으로 압축 처리하며 진행하다가, 어텐션 레이어가 등장하는 시점에 전체 문맥을 다시 한번 ’조망(Overview)’한다. 이 어텐션 레이어는 SSM의 상태 벡터가 놓쳤거나 희석시켰을 수 있는 구체적인 정보를 KV Cache에서 직접 조회하여 복원하는 ‘앵커(Anchor)’ 역할을 한다.</li>
<li><strong>메모리 효율성:</strong> 전체를 어텐션으로 구성했을 때보다 KV Cache의 크기를 획기적으로 줄일 수 있다. Jamba-1.5-Large 모델의 경우, 256K 문맥 길이를 처리할 때 KV Cache는 9GB에 불과하다. 이는 동급의 Llama-3.1-70B(80GB) 대비 약 9배, Mixtral-8x22B(56GB) 대비 6배 이상 작은 수치다.9 작은 캐시는 더 빠른 검색과 더 긴 문맥 확장을 가능하게 한다.</li>
<li><strong>비율의 최적화:</strong> 연구진은 1:3 비율과 1:7 비율 간의 성능 차이가 미미함을 확인하고, 연산 효율성이 더 높은 1:7 비율을 채택했다.14 이는 NIAH 성능을 유지하기 위해 어텐션이 ’모든 곳’에 있을 필요는 없으며, 적절한 간격으로 배치되기만 하면 충분함을 시사한다.</li>
</ul>
<h3>3.2 ) StripedHyena (Together AI): 신호 처리와 25%의 법칙</h3>
<p>StripedHyena는 신호 처리 이론에 기반한 합성곱 연산(Hyena Operator)과 어텐션을 결합한 독창적인 하이브리드 모델이다.18</p>
<ul>
<li><strong>구조적 특징:</strong> Hyena 블록은 긴 합성곱(Long Convolution)을 사용하여 데이터를 처리하는데, 이는 수학적으로 SSM과 등가이면서도 병렬 처리에 유리하다. StripedHyena는 이 Hyena 연산자와 어텐션 헤드를 혼합하여 사용한다.</li>
<li><strong>최적 비율의 발견:</strong> StripedHyena 연구진은 컴퓨팅 예산(Compute Budget) 대비 최적의 성능을 내는 하이브리드 비율(Scaling Law)을 탐구한 결과, 어텐션 레이어가 전체의 약 **25%**를 차지할 때(즉, SSM:Attention = 3:1) 성능이 극대화된다는 것을 발견했다.18 이는 Jamba의 1:7(약 12.5%)보다 높은 비율로, 좀 더 공격적인 어텐션 개입이 리콜 성능에 긍정적인 영향을 미칠 수 있음을 시사한다.</li>
<li><strong>NIAH 성능:</strong> StripedHyena-7B는 Llama-2와 같은 순수 트랜스포머 모델과 동등한 수준의 장기 문맥 요약 성능을 보이면서도, 훈련 및 추론 속도 면에서 30~100% 이상의 효율성을 달성했다.18 특히 긴 문맥에서의 ’패스키 검색’과 같은 정밀 작업에서 하이브리드 구조는 순수 Hyena(0% Attention) 모델의 약점을 완벽하게 보완한다.</li>
</ul>
<h3>3.3 ) Hymba (NVIDIA): 병렬 하이브리드 헤드와 메타 토큰</h3>
<p>Hymba는 레이어를 번갈아 배치하는 기존의 방식(Sequential Hybrid)을 넘어, 동일한 레이어 내에서 어텐션 헤드와 SSM 헤드를 병렬로 배치하는 <strong>병렬 하이브리드 헤드(Parallel Hybrid Head)</strong> 구조를 제안했다.8</p>
<ul>
<li><strong>구조적 특징:</strong> 입력 토큰은 어텐션 헤드와 SSM 헤드로 동시에 들어가 처리된 후 통합된다.</li>
<li><strong>어텐션 헤드:</strong> “고해상도 회상(High-resolution Recall)“을 전담한다. 즉, NIAH와 같이 핀포인트 정보가 필요할 때 작동한다.</li>
<li><strong>SSM 헤드:</strong> “효율적인 문맥 요약(Context Summarization)“을 전담한다. 전체적인 배경 정보를 처리한다.</li>
<li><strong>메타 토큰(Meta Tokens):</strong> Hymba는 프롬프트 앞에 ’학습 가능한 메타 토큰’을 추가하여 캐시를 초기화한다. 이 토큰들은 어텐션 메커니즘이 모든 토큰을 강제로 저장해야 하는 부담(Forced-to-attend burden)을 덜어주고, 중요한 정보의 위치나 유형을 미리 인코딩하여 검색 효율을 높인다.24 이는 마치 책의 ’목차’나 ’색인’을 미리 머릿속에 넣어두는 것과 같은 효과를 낸다.</li>
<li><strong>NIAH 성능:</strong> Hymba-1.5B 모델은 훨씬 작은 크기임에도 불구하고, NIAH 테스트 및 장기 문맥 추론 작업에서 Llama-3.2-3B 등 더 큰 모델을 능가하는 성능을 보였다.8 특히 SSM 헤드가 문맥을 요약해 줌으로써, 어텐션 헤드는 ’크로스 레이어 KV 공유’와 같은 기술을 통해 캐시 크기를 극단적으로(11.67배) 줄일 수 있었다.</li>
</ul>
<h3>3.4 ) Samba (Microsoft): 슬라이딩 윈도우와 무한의 확장</h3>
<p>Samba는 슬라이딩 윈도우 어텐션(Sliding Window Attention, SWA)과 Mamba를 결합한 모델이다.12</p>
<ul>
<li><strong>구조적 특징:</strong> SWA는 국소적인 정보(윈도우 내부)에 대해서는 완벽한 어텐션을 수행하지만, 윈도우 밖의 정보는 보지 못한다. Mamba는 전역적인 정보를 상태 벡터로 압축한다. Samba는 이 둘을 결합하여, SWA가 놓치는 장기 의존성을 Mamba가 보완하고, Mamba가 놓치는 정밀한 국소 정보를 SWA가 잡아내는 상호 보완적 구조를 완성했다.</li>
<li><strong>NIAH 성능:</strong> 12의 연구에 따르면, 순수 SWA 모델(Mistral 등)은 윈도우 크기를 벗어나는 순간 패스키 검색 정확도가 30% 수준으로 급락하지만, Samba는 256K 길이까지 거의 100%에 가까운 검색 정확도를 유지했다. 이는 Mamba 레이어가 윈도우 밖의 정보를 성공적으로 전달하고 있음을 증명하며, 하이브리드 모델이 ’무한 문맥’으로의 확장에 있어 가장 현실적인 대안임을 보여준다.</li>
</ul>
<h2>4.  심층 분석: NIAH 벤치마크와 데이터가 말하는 진실</h2>
<p>하이브리드 모델의 우위는 단순한 이론적 추측이 아니라, 가혹한 벤치마크 테스트를 통해 입증된 사실이다. 다양한 NIAH 변형 테스트와 그 결과를 상세히 분석해 본다.</p>
<h3>4.1 ) 단일 니들 검색 (Single-Needle Retrieval)</h3>
<p>가장 기본적인 형태의 NIAH 테스트로, 거대한 텍스트 중 특정 위치에 숨겨진 하나의 정보를 찾는 과제이다.</p>
<ul>
<li><strong>순수 Mamba vs. 하이브리드:</strong> 6와 6의 실험 결과, Mamba-2 모델은 시퀀스 길이가 2K~4K를 넘어가면 성능이 급격히 저하되는 경향을 보였다. 반면, Jamba와 Samba는 128K, 256K에 달하는 길이에서도 ‘전 영역 녹색(All-Green)’ 히트맵을 기록하며 완벽한 회수율을 보였다.26</li>
<li><strong>위치 편향(Position Bias):</strong> 트랜스포머 모델들이 종종 겪는 ’Lost in the Middle(중간 부분의 정보를 망각하는 현상)’에 대해서도 하이브리드 모델은 강건한 모습을 보인다. Jamba의 경우, 어텐션 레이어가 주기적으로 문맥을 환기시켜 줌으로써 문맥의 시작, 중간, 끝 어느 위치에 있는 니들도 균일하게 찾아냈다.</li>
</ul>
<h3>4.2 ) 패스키 검색 (Passkey Retrieval)</h3>
<p>패스키 검색은 의미적 연결고리가 없는 무작위 숫자열을 찾아내는 작업으로, 모델의 ’기계적 기억력’을 측정하는 가장 엄격한 테스트다.</p>
<ul>
<li><strong>데이터 분석:</strong> 12의 연구에서 Samba 1.7B 모델은 256K 길이의 패스키 검색에서 완벽한 성능을 보인 반면, 순수 Mamba 기반 모델들은 문맥 길이가 길어질수록 재현율이 떨어져 0%에 수렴하는 경우도 발생했다.</li>
<li><strong>의미:</strong> 이는 SSM의 상태 압축이 ’의미 없는 데이터’를 노이즈로 처리하여 삭제해 버리는 특성이 있음을 시사한다. 하이브리드 모델의 어텐션 레이어는 이러한 무작위 데이터까지도 KV Cache에 원본 그대로 보존하기 때문에 이 문제를 해결할 수 있다. 즉, 하이브리드 모델은 **“이해할 수 없는 정보라도 기억할 수 있는 능력”**을 갖추었다.</li>
</ul>
<h3>4.3 ) 훈련 길이 외삽 (Extrapolation)</h3>
<p>모델을 짧은 문맥(예: 4K)으로 훈련시킨 후, 훈련 때보다 훨씬 긴 문맥(예: 128K)에서 테스트하는 능력이다.</p>
<ul>
<li><strong>Samba와 Jamba의 성과:</strong> Samba 모델은 4K 길이로 미세조정(Fine-tuning)되었음에도 불구하고, 256K 길이의 패스키 검색을 성공적으로 수행했다.12 Jamba 역시 훈련된 길이를 넘어서는 문맥에서도 안정적인 성능을 유지했다.</li>
<li><strong>원인 분석:</strong> 이는 SSM의 순환적 특성이 길이 확장에 유리한 귀납적 편향(Inductive Bias)을 제공하고, 간헐적인 어텐션이 정보의 정합성을 잡아주기 때문이다. 순수 트랜스포머는 위치 임베딩(Positional Embedding)의 한계로 인해 외삽에 어려움을 겪는 경우가 많으나, 하이브리드 모델은 SSM의 유연성을 통해 이 문제를 완화한다.</li>
</ul>
<h2>5.  효율성과 성능의 교차점: 경제적 관점</h2>
<p>NIAH 성능을 달성하는 것만큼 중요한 것은 ‘얼마나 저렴하고 빠르게’ 달성하느냐이다. 하이브리드 모델의 진정한 가치는 바로 이 지점에 있다.</p>
<h3>5.1 ) KV Cache 메모리 절감</h3>
<p>다음은 주요 모델들의 256K 문맥 처리 시 KV Cache 메모리 요구량을 비교한 표이다.9</p>
<table><thead><tr><th><strong>모델 (Model)</strong></th><th><strong>총 파라미터 (Params)</strong></th><th><strong>활성 파라미터 (Active)</strong></th><th><strong>KV Cache (256K, 16bit)</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>Llama-3.1-70B</strong></td><td>70B</td><td>70B</td><td><strong>80GB</strong></td><td>단일 GPU 구동 불가</td></tr>
<tr><td><strong>Mixtral-8x22B</strong></td><td>141B</td><td>39B</td><td><strong>56GB</strong></td><td>매우 높은 메모리 요구</td></tr>
<tr><td><strong>Mistral-Large-2</strong></td><td>123B</td><td>123B</td><td><strong>88GB</strong></td><td>막대한 리소스 소모</td></tr>
<tr><td><strong>Jamba-1.5-Large</strong></td><td>398B</td><td>94B</td><td><strong>9GB</strong></td><td><strong>획기적 절감 (약 1/9)</strong></td></tr>
<tr><td><strong>Jamba-1.5-Mini</strong></td><td>52B</td><td>12B</td><td><strong>4GB</strong></td><td>소비자용 GPU 구동 가능</td></tr>
</tbody></table>
<p>위 표에서 알 수 있듯이, Jamba와 같은 하이브리드 모델은 순수 트랜스포머 기반 모델 대비 KV Cache 크기를 1/10 수준으로 줄였다. 이는 80GB 메모리를 가진 A100 GPU 한 장에서 256K 길이의 문맥을 처리할 수 있느냐 없느냐를 가르는 결정적인 차이이다. 순수 트랜스포머는 메모리 부족(OOM)으로 인해 실행조차 불가능한 환경에서, 하이브리드 모델은 여유롭게 작동한다.</p>
<h3>5.2 ) 처리량 (Throughput) 향상</h3>
<p>긴 문맥에서의 추론 속도 역시 하이브리드 모델이 압도적이다. StripedHyena는 32K~128K 길이 시퀀스 훈련 시 트랜스포머 대비 30%에서 100% 더 빠른 속도를 기록했다.18 Jamba 역시 긴 문맥에서 Mixtral-8x7B 대비 3배 높은 처리량을 보여준다.28 이는 SSM 레이어가 <span class="math math-inline">O(N)</span> 복잡도로 빠르게 입력을 처리해주기 때문에, 전체적인 연산 부담이 크게 줄어들기 때문이다.</p>
<h3>5.3 ) 비용 효율성 (Cost Effectiveness)</h3>
<p>결국 NIAH 성능을 실제 비즈니스에 적용할 때 가장 중요한 것은 비용이다. RAG 시스템에서 수십 개의 문서를 검색하고 답변을 생성할 때, 하이브리드 모델은 더 적은 GPU로 더 많은 요청을 처리할 수 있다. 이는 클라우드 서비스 비용 절감으로 직결되며, 하이브리드 모델이 ’포스트 트랜스포머 시대’의 경제적 표준이 될 수 있음을 시사한다.</p>
<h2>6.  향후 전망 및 결론: 하이브리드, 필연적 진화</h2>
<p>지금까지 7.2절에서는 NIAH 테스트를 통해 하이브리드 모델의 우수성을 다각도로 검증했다. 결론적으로, <strong>하이브리드 아키텍처는 선택이 아닌 필연</strong>이다.</p>
<p>순수 SSM(Mamba)은 트랜스포머의 비효율성을 해결할 강력한 대안으로 등장했지만, ’압축’이라는 본질적 특성으로 인해 ’정밀 검색(NIAH)’이라는 마지막 관문을 넘지 못했다. 반면, 트랜스포머는 정밀 검색의 제왕이지만, 그 왕관의 무게(연산 비용)를 견디기 힘들어하고 있다. Jamba, StripedHyena, Hymba, Samba 등 본 절에서 살펴본 하이브리드 모델들은 이 두 아키텍처의 장점만을 취하여 <strong>“무한한 문맥을 효율적으로 압축하면서도, 결정적인 순간에는 완벽하게 기억해내는”</strong> 이상적인 LLM의 모습을 구현해냈다.</p>
<h3>6.1 하이브리드 모델의 시사점 및 미래 방향</h3>
<ol>
<li><strong>어텐션의 재정의:</strong> 어텐션은 더 이상 ’모든 곳’에 있을 필요가 없다. 전체 레이어의 10~20% 수준, 혹은 병렬 헤드의 일부로서 존재할 때 가장 효율적이다. 어텐션은 이제 ’기본 연산 장치’가 아니라, SSM을 보조하는 ‘고정밀 스캐너’ 역할을 하게 될 것이다.</li>
<li><strong>맞춤형 아키텍처의 부상:</strong> 작업의 성격에 따라 어텐션과 SSM의 비율을 동적으로 조절하거나(예: 검색 집약적 작업에는 어텐션 비중 증가), Hymba와 같이 메타 토큰을 활용하여 제어하는 방식이 더욱 고도화될 것이다.</li>
<li><strong>하드웨어와의 공진화:</strong> 하이브리드 모델은 GPU의 병렬 처리 능력(어텐션 활용)과 메모리 대역폭 효율성(SSM 활용)을 모두 만족시키는 구조다. 향후 AI 반도체는 이러한 하이브리드 연산 흐름에 최적화된 형태로 발전할 가능성이 높다.</li>
</ol>
<p>포스트 트랜스포머 시대는 단일 아키텍처가 지배하는 시대가 아니다. 그것은 상호 보완적인 기술들이 결합하여 한계를 돌파하는 ’융합의 시대’이며, NIAH 성능에서 보여준 하이브리드 모델의 압도적인 우위는 그 시대가 이미 도래했음을 알리는 가장 확실한 신호탄이다. 우리는 이제 거대한 건초더미 속에서 바늘을 찾는 것을 두려워할 필요가 없다. 하이브리드 모델이라는 강력한 자석을 손에 넣었기 때문이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Needle In A Haystack Evaluation - OpenCompass Docs, https://opencompass.readthedocs.io/en/latest/advanced_guides/needleinahaystack_eval.html</li>
<li>The Needle in the Haystack Test and How Gemini Pro Solves It | Google Cloud Blog, https://cloud.google.com/blog/products/ai-machine-learning/the-needle-in-the-haystack-test-and-how-gemini-pro-solves-it</li>
<li>The Needle In a Haystack Test: Evaluating the Performance of LLM RAG Systems - Arize AI, https://arize.com/blog-course/the-needle-in-a-haystack-test-evaluating-the-performance-of-llm-rag-systems/</li>
<li>The Needle In a Haystack Test | Towards Data Science, https://towardsdatascience.com/the-needle-in-a-haystack-test-a94974c1ad38/</li>
<li>The Mamba Revolution: How State Space Models Are Challenging Transformers - Medium, https://medium.com/@aftab001x/the-mamba-revolution-how-state-space-models-are-challenging-transformers-4ad3b276b9a8</li>
<li>Gated Delta Networks: Improving Mamba2 with Delta Rule - OpenReview, <a href="https://openreview.net/forum?id=r8H7xhYPwz&amp;noteId=U0uk5A0VlT">https://openreview.net/forum?id=r8H7xhYPwz¬eId=U0uk5A0VlT</a></li>
<li>[D] What Are the Fundamental Drawbacks of Mamba Compared to Transformers? - Reddit, https://www.reddit.com/r/MachineLearning/comments/1ayog60/d_what_are_the_fundamental_drawbacks_of_mamba/</li>
<li>Hymba: A Hybrid-head Architecture for Small Language Models - OpenReview, https://openreview.net/forum?id=A1ztozypga</li>
<li>Jamba-1.5: Hybrid Transformer-Mamba Models at Scale - arXiv, https://arxiv.org/pdf/2408.12570</li>
<li>Mamba: Linear-Time Sequence Modeling with Selective State Spaces - arXiv, https://arxiv.org/html/2312.00752v2</li>
<li>Mamba: Linear-Time Sequence Modeling with Selective State Spaces - arXiv, https://arxiv.org/pdf/2312.00752</li>
<li>Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling - arXiv, https://arxiv.org/html/2406.07522v1</li>
<li>Understanding the Skill Gap in Recurrent Language Models: The Role of the Gather-and-Aggregate Mechanism | OpenReview, https://openreview.net/forum?id=hWYisuBbp7</li>
<li>Jamba: A Hybrid Transformer-Mamba Language Model - arXiv, https://arxiv.org/html/2403.19887v1</li>
<li>Jamba: A Hybrid Transformer-Mamba Language Model with Mixture-of-Experts - Medium, https://medium.com/@sulbha.jindal/jamba-a-hybrid-transformer-mamba-language-model-with-mixture-of-experts-506281f2398e</li>
<li>Breaking Down Jamba: How Mixing Attention and State Spaces Makes a Smarter LLM, https://hackernoon.com/breaking-down-jamba-how-mixing-attention-and-state-spaces-makes-a-smarter-llm</li>
<li>Jamba-1.5: Hybrid Transformer-Mamba Models at Scale - arXiv, https://arxiv.org/html/2408.12570v1</li>
<li>Paving the way to efficient architectures: StripedHyena-7B, open source models offering a glimpse into a world beyond Transformers - Together AI, https://www.together.ai/blog/stripedhyena-7b</li>
<li>togethercomputer/StripedHyena-Nous-7B - Hugging Face, https://huggingface.co/togethercomputer/StripedHyena-Nous-7B</li>
<li>Mechanistic Design and Scaling of Hybrid Architectures - arXiv, https://arxiv.org/html/2403.17844v2</li>
<li>A Systematic Analysis of Hybrid Linear Attention - arXiv, https://arxiv.org/html/2507.06457v1</li>
<li>Architectures for longer sequences and efficient inference: StripedHyena | hessian.AI, https://hessian.ai/architectures-for-longer-sequences-and-efficient-inference-stripedhyena/</li>
<li>Hymba Hybrid-Head Architecture Boosts Small Language Model Performance, https://developer.nvidia.com/blog/hymba-hybrid-head-architecture-boosts-small-language-model-performance/</li>
<li>[2411.13676] Hymba: A Hybrid-head Architecture for Small Language Models - arXiv, https://arxiv.org/abs/2411.13676</li>
<li>Hymba: A Hybrid-head Architecture for Small Language Models - arXiv, https://arxiv.org/html/2411.13676v1</li>
<li>How Hybrid AI Models Balance Memory and Efficiency, https://textmodels.tech/how-hybrid-ai-models-balance-memory-and-efficiency</li>
<li>Jamba: A Hybrid Transformer-Mamba Language Model - arXiv, https://arxiv.org/pdf/2403.19887</li>
<li>Architectural Evolution in Large Language Models: A Deep Dive into Jamba’s Hybrid Transformer-Mamba Design - Greg Robison, https://gregrobison.medium.com/architectural-evolution-in-large-language-models-a-deep-dive-into-jambas-hybrid-transformer-mamba-c3efa8ca8cae</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>