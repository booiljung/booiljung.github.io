<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:일반화 가능한 로봇 공학을 위한 오픈소스 모바일 매니퓰레이터 'YOR' 심층 분석 보고서</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>일반화 가능한 로봇 공학을 위한 오픈소스 모바일 매니퓰레이터 'YOR' 심층 분석 보고서</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">로봇 인공지능 (Robot AI)</a> / <span>일반화 가능한 로봇 공학을 위한 오픈소스 모바일 매니퓰레이터 'YOR' 심층 분석 보고서</span></nav>
                </div>
            </header>
            <article>
                <h1>일반화 가능한 로봇 공학을 위한 오픈소스 모바일 매니퓰레이터 ‘YOR’ 심층 분석 보고서</h1>
<h2>1.  서론: 체화된 인공지능(Embodied AI)의 하드웨어 병목 현상과 패러다임의 전환</h2>
<p>최근 로봇 학습(Robot Learning) 분야는 대규모 데이터 기반의 정책 학습과 파운데이션 모델(Foundation Models)의 도입으로 인해 비약적인 발전을 이루고 있다. 특히 거대 언어 모델(LLM)과 비전-언어-행동 모델(Vision-Language-Action Models, VLA)의 발전은 로봇이 시각적 환경을 이해하고 자연어 명령을 수행하는 능력을 인간 수준에 근접하게 끌어올리고 있다. 체화된 인공지능(Embodied AI) 연구의 흐름은 통제된 실험실 환경 내에서의 단일 작업 학습을 넘어, 일상적이고 비정형화된 인간 중심 환경(Human-centered environments)에서 다양한 작업을 수행할 수 있는 일반화(Generalization) 역량을 확보하는 방향으로 나아가고 있다. 이러한 학계와 산업계의 폭발적인 관심은 액추에이터와 정밀 센서 등 하드웨어 부품의 상품화 및 비용 하락과 맞물려, 저비용 로봇 플랫폼의 폭발적인 성장을 촉진하는 핵심 동력이 되었다.</p>
<p>그러나 예산이 제한된 연구 환경에서 실내 공간의 복잡한 모바일 조작(Mobile Manipulation) 작업을 완벽히 수행하기 위한 ’최적의 폼팩터(Form Factor)’가 무엇인지에 대해서는 여전히 명확한 해답이 제시되지 않은 상태였다. 기존의 상용 모바일 매니퓰레이터들은 연구실 환경을 벗어나 실제 인간 생활 공간에 배포하기에는 극복하기 힘든 경제적, 기술적 장벽이 존재했다. 예를 들어 휴머노이드(Humanoid) 로봇이나 4족 보행 로봇(Quadrupeds)에 매니퓰레이터를 결합한 형태는 전신 제어(Whole-body control)와 동적 균형 유지(Dynamic balancing) 측면에서 극도로 높은 기계적, 제어공학적 복잡성을 요구한다. RBY1-A와 같이 뛰어난 성능을 제공하는 상용 고성능 플랫폼은 기능적으로 우수하나 그 가격이 10만 달러를 상회하여 대규모 데이터 수집 및 다중 에이전트 연구 확장을 가로막는 절대적인 장애물로 작용해 왔다.</p>
<p>반대로, 최근 급부상한 Mobile ALOHA, Tidybot++, XLeRobot과 같은 저비용 플랫폼들은 예산 장벽을 낮추는 데에는 성공했으나, 기구학적 한계로 인해 내비게이션 역량이 크게 제한되거나, 수직 작업 공간(Vertical workspace) 제어 기능이 부재하고, 페이로드(Payload) 및 팔의 도달 거리가 부족하여 범용적인 가사 작업이나 복잡한 3차원 공간 조작에 적용하기에는 치명적인 제약이 따랐다.</p>
<p>이러한 하드웨어의 양극화 현상과 연구 접근성 불평등 문제를 해결하기 위해, 뉴욕대학교(NYU)와 UC 버클리(UC Berkeley) 등의 연합 연구진은 ’YOR(Your Own Mobile Manipulator for Generalizable Robotics)’라는 혁신적인 플랫폼을 개발하여 전면 공개하였다. YOR는 상용 기성품(Off-the-shelf) 컴포넌트를 조화롭게 통합하여 10,000달러(USD) 미만의 자재명세서(BOM) 비용으로 구축할 수 있는 완전한 오픈소스 기반의 저비용, 고효율 모바일 매니퓰레이터다.</p>
<p>본 심층 분석 보고서는 YOR 플랫폼의 기계적 하드웨어 아키텍처부터 시작하여, 기구학적 전신 제어 및 자율 주행 알고리즘, VQ-BeT 기반의 다중 모달 정책 학습 프레임워크, 그리고 로봇 범용성 모델(Robot Utility Models, RUMs)과의 연계성을 상세히 분석한다. 나아가 이 플랫폼이 체화된 인공지능 연구 커뮤니티에 제공하는 오픈소스 생태계의 가치와 기술적 파급 효과, 2차적 및 3차적 시사점을 종합적으로 도출하여 제시한다.</p>
<h2>2.  YOR 하드웨어 아키텍처 및 기구학적 설계 분석</h2>
<p>YOR의 하드웨어 설계는 ’확장 가능한 모바일 조작 연구를 위한 가장 실용적인 폼팩터의 도출’이라는 명확한 엔지니어링 목표 아래 진행되었다. 이 시스템은 크게 전방향 모바일 베이스(Omnidirectional base), 텔레스코픽 수직 리프트(Telescopic vertical lift), 그리고 양팔 매니퓰레이터(Bimanual arms)의 세 가지 핵심 모듈로 구성된다. 설계의 모든 측면에서 비용 효율성, 제어 가능성, 그리고 안전성이 우선적으로 고려되었다.</p>
<h3>2.1  저비용 및 극단적 모듈화 원칙 (Low-Cost and Extreme Modularity)</h3>
<p>로봇 하드웨어의 진정한 민주화를 달성하기 위해 YOR는 독자적인 맞춤형 금속 가공이나 값비싼 특수 부품의 사용을 철저히 배제하고, 상용 기성품(COTS: Commercial Off-The-Shelf)을 적극적으로 활용하였다. 전체 플랫폼의 자재명세서(BOM) 비용은 약 9,250달러 수준으로 최적화되었으며, 이는 기존 상용 양팔 모바일 매니퓰레이터의 1/10 수준에 불과하다.</p>
<p>모듈형 설계 접근법은 플랫폼의 생명주기를 연장하고 연구 목적에 따른 변형을 용이하게 한다. 예를 들어, 특정 연구팀이 시각-관성 SLAM 대신 라이다(LiDAR) 기반의 내비게이션을 연구하고자 한다면 컴퓨팅 유닛과 센서 모듈만을 독립적으로 교체할 수 있다.</p>
<table><thead><tr><th><strong>서브시스템 (Subsystem)</strong></th><th><strong>구성 요소 및 설명</strong></th><th><strong>예상 비용 (USD)</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>Bimanual Arms</strong></td><td>AgileX PiPER 6-DoF 로봇 암 2기 및 맞춤형 그리퍼</td><td>$5,100</td><td>전체 시스템 예산의 과반을 차지하는 핵심 구동부</td></tr>
<tr><td><strong>Mobile Base</strong></td><td>REV MAXSwerve 휠 모듈 4기, 베이스 프레임, 모터</td><td>$3,000</td><td>전방향 홀로노믹 이동 지원</td></tr>
<tr><td><strong>Vertical Lift</strong></td><td>FLEXISPOT E6 스탠딩 데스크 텔레스코픽 액추에이터</td><td>$200</td><td>수직 도달 범위 확장</td></tr>
<tr><td><strong>Electronics</strong></td><td>전력 분배 보드, 배터리, 배선, Pico 제어기 등</td><td>$700</td><td>저중심 설계를 위해 베이스에 밀집 배치</td></tr>
<tr><td><strong>Base Compute</strong></td><td>Raspberry Pi 5 (16GB RAM) 및 쿨링 시스템</td><td>$250</td><td>하위 레벨 제어 및 통신 중계</td></tr>
<tr><td><strong>기본 시스템 총합</strong></td><td><strong>필수 구동 및 제어 시스템 통합 비용</strong></td><td><strong>$9,250</strong></td><td>1만 달러 미만의 목표 달성</td></tr>
<tr><td><strong>SLAM (Optional)</strong></td><td>NVIDIA reComputer ORIN NX (<span class="math math-inline">1,050) + ZED 2i (</span>500)</td><td>$1,550</td><td>온보드 자율주행 연구를 위한 선택 사양</td></tr>
</tbody></table>
<h3>2.2  전방향 스워브 드라이브 베이스 (Omnidirectional Swerve Drive Base)</h3>
<p>복잡하고 좁은 가구 배치가 일반적인 실내 환경에서 정밀한 모바일 조작을 수행하기 위해서는 로봇 베이스의 기동성이 작업의 성공을 좌우한다. 기존의 저가형 로봇들이 주로 채택해 온 차동 구동(Differential drive) 방식은 병진 운동과 회전 운동이 결합되어 나타나는 비홀로노믹(Non-holonomic) 제약 조건을 수반한다. 이러한 제약은 로봇이 옆으로 평행 이동(Crab-walk)을 할 수 없게 만들어, 테이블 앞의 좁은 공간에서 팔을 뻗기 위해 로봇 전체가 앞뒤로 복잡한 곡선 궤적(Dubins path)을 그리며 기동해야 하는 문제를 발생시킨다.</p>
<p>YOR는 이 문제를 근본적으로 해결하기 위해 4개의 독립 조향 구동 휠 모듈(REV MAXSwerve)을 결합한 스워브 드라이브 아키텍처를 도입했다. 이 아키텍처는 휠의 구동 속도와 조향 각도를 각각 독립적인 모터로 제어함으로써 로봇의 병진 운동(Translation)과 회전 운동(Rotation)을 기구학적으로 완벽히 분리(Decoupled)한다. 그 결과, 로봇은 엔드 이펙터(End-effector)의 오리엔테이션을 목표물에 고정한 상태 그대로 어떤 방향으로든 부드럽게 미끄러지듯 이동할 수 있다.</p>
<p>베이스의 물리적 풋프린트(Footprint)는 43 × 34.5 cm로 인간의 어깨너비보다 좁아, 기존의 거대한 모바일 매니퓰레이터들이 진입하지 못하는 좁은 복도나 어수선한 실내 공간에서도 원활한 기동이 가능하다. 또한, 무거운 배터리와 전력 변환 장치, 메인 컴퓨팅 유닛 등 중량이 큰 컴포넌트들을 베이스 하단에 고밀도로 밀집 배치(Dense packing)하여 로봇의 무게중심(Center-of-mass)을 극단적으로 낮추었다. 이러한 저중심 설계는 로봇이 수직 리프트를 최고 높이로 올리고 두 팔을 전방으로 뻗어 무거운 물체를 들어 올릴 때 발생할 수 있는 피칭(Pitching) 모멘트에 의한 전복 위험을 수동적으로(Passively) 억제하여, 복잡한 동적 밸런싱 제어 알고리즘 없이도 시스템의 기계적 안정성을 보장한다.</p>
<h3>2.3  산업적 강성을 지닌 텔레스코픽 수직 리프트 (Telescopic Vertical Lift)</h3>
<p>Mobile ALOHA나 Tidybot++와 같은 폼팩터들이 지니는 가장 큰 형태적 한계는 고정된 베이스 높이로 인한 수직 작업 공간의 부재였다. 가정 환경에서 로봇은 바닥에 떨어진 쓰레기를 줍는 동작(지면 레벨)부터 높은 찬장이나 전자레인지에 물건을 넣는 동작(오버헤드 레벨)까지 광범위한 높이의 상호작용을 요구받는다.</p>
<p>YOR는 이러한 3차원 공간 제약을 타파하기 위해 상업용 전동 스탠딩 데스크에 널리 사용되는 텔레스코픽 리프트 칼럼(FLEXISPOT E6)을 동체 기둥으로 채택하였다. 이 엑추에이터는 저렴하면서도 이미 산업적으로 10,000회 이상의 높은 부하 주기(Load cycles)에 대해 신뢰성이 검증된 부품이다. 리프트는 63.5cm의 스트로크(Stroke) 길이를 제공하며, 로봇의 어깨에 해당하는 마운팅 포인트의 높이를 지면 기준 0.6m에서 최대 1.24m까지 무단 변속으로 조절할 수 있게 해준다.</p>
<p>가장 중요한 엔지니어링 포인트는 이 리프트가 무거운 양팔 매니퓰레이터 어셈블리가 측면으로 뻗었을 때 발생하는 강력한 오프셋 하중(Offset loads) 상태에서도 굽힘이나 유격 없이 높은 기계적 강성을 유지한다는 점이다. 리프트의 제어는 BTS7960 모터 드라이버와 결합된 Raspberry Pi Pico를 통해 이루어지며, 독립적인 리프트 컨트롤러로서 메인 컴퓨터의 연산 부하를 분산시킨다.</p>
<h3>2.4  유연 제어 기반의 양팔 매니퓰레이터 (Compliant Bimanual Arms)</h3>
<p>로봇이 인간과 동일한 환경 도구를 사용하고 양손 조작(Bimanual manipulation)을 수행하기 위해서는 인간의 팔에 준하는 도달 범위와 다자유도(DoF), 그리고 환경과의 접촉 시 파손을 막아주는 유연성(Compliance)이 필수적이다. YOR는 이를 구현하기 위해 AgileX Robotics에서 개발한 6자유도 PiPER 로봇 암 두 대를 통합하였다.</p>
<p>PiPER 암은 고가의 산업용 협동 로봇(Cobots)에 비해 훨씬 저렴한 대당 약 2,500달러의 가격표를 가지면서도, 연구 목적에 충분한 뛰어난 페이로드-대-중량비(Payload-to-weight ratio)와 정밀도를 제공한다. 알루미늄 합금 및 폴리머 쉘로 제작되어 자체 중량은 4.2kg에 불과하지만 1.5kg의 안정적인 유효 하중을 다룰 수 있다.</p>
<table><thead><tr><th><strong>규격 파라미터 (Specification)</strong></th><th><strong>상세 수치 및 특성</strong></th><th><strong>비고 및 로봇 공학적 의미</strong></th></tr></thead><tbody>
<tr><td><strong>자유도 (Degrees of Freedom)</strong></td><td>6 DoF (각 팔당)</td><td>3차원 공간에서 위치(x,y,z)와 방향(roll,pitch,yaw)을 완전 제어</td></tr>
<tr><td><strong>자체 중량 (Weight)</strong></td><td>4.2 kg</td><td>로봇 베이스의 무게중심 상승을 억제하여 전복 방지</td></tr>
<tr><td><strong>유효 하중 (Payload Capacity)</strong></td><td>1.5 kg</td><td>일상적인 가사 작업(식기 이동, 문 열기 등) 수행에 적합</td></tr>
<tr><td><strong>도달 거리 (Reach / Working radius)</strong></td><td>626 mm</td><td>인간의 팔 길이와 유사하여 인체공학적 작업 공간 모방 가능</td></tr>
<tr><td><strong>반복 정밀도 (Repeatability)</strong></td><td>± 0.1 mm</td><td>핀 포인트 타겟팅 및 미세 조립 등 고정밀 조작을 지원</td></tr>
<tr><td><strong>구동 전압 및 통신 인터페이스</strong></td><td>DC 24V / CAN 통신</td><td>베이스의 중앙 전력 분배 보드와 직관적 연결 및 실시간 제어</td></tr>
<tr><td><strong>작동 온도 (Operating Temperature)</strong></td><td>-20℃ ~ 50℃</td><td>극한의 냉동 창고부터 고온 환경까지 다양한 야외/실내 환경 적응</td></tr>
</tbody></table>
<p>가장 주목해야 할 PiPER 암의 특성은 **유연 제어(Compliant actuation)**의 지원이다. 로봇이 테이블 위를 닦거나 서랍을 여는 등의 환경 접촉 집약적 작업(Contact-rich tasks)을 수행할 때, 강체 기반의 엄격한 위치 제어(Rigid position control)는 환경 모델링에 단 몇 밀리미터의 오차만 있어도 관절 모터의 과부하나 물체의 파손을 초래한다. PiPER 암에 내장된 유연 제어 로직은 기계적 스프링과 댐퍼처럼 물리적 충격과 반발력을 수동적으로 흡수하고 적응하도록 설계되었다. 이러한 특성은 인간 중심 환경에서의 절대적인 안전성을 보장하며, 제어 알고리즘이 환경의 기하학적 불확실성을 일일이 계산하여 지나치게 보수적인(느리고 방어적인) 궤적 계획을 세워야 하는 연산 부하를 크게 경감시킨다. 엔드 이펙터로는 물체를 견고하게 파지하기 위해 설계된 맞춤형 평행 개폐식 각도 그리퍼(Custom angular jaw grippers)가 장착된다.</p>
<h2>3.  전신 제어 시스템 및 실시간 기구학적 보상 (Whole-Body Control &amp; Kinematic Compensation)</h2>
<p>다수의 자유도(베이스 3 DoF + 리프트 1 DoF + 양팔 12 DoF = 총 16 DoF)를 지닌 시스템을 유기적으로 통제하는 것은 제어 공학의 난제다. 단순히 팔과 다리를 순차적으로 움직이는 ‘이동 후 조작(Move-then-manipulate)’ 방식은 작업 속도를 심각하게 저하시키고 동적 환경에 대한 대응을 불가능하게 한다. YOR의 제어 아키텍처는 베이스와 매니퓰레이터 간의 ’기밀한 제어 통합(Tight integration)’을 통해 이동과 조작이 동시에 이루어지는 진정한 의미의 **전신 제어(Whole-body coordination)**를 실현하였다.</p>
<h3>3.1  베이스-암 동역학 결합 및 기구학적 보상 로직</h3>
<p>모바일 로봇이 물건을 집어 들기 위해 다가가면서 동시에 팔을 뻗는 동작을 상상해 보라. 베이스가 전진(Translation)하거나 회전(Rotation)하면, 베이스 좌표계에 부착된 로봇 암의 끝단(End-effector) 역시 월드 좌표계(World frame)에서 원치 않는 방향으로 이탈하게 된다. YOR의 제어 스택은 이 문제를 해결하기 위해 고주기의 실시간 기구학적 보상(Kinematic compensation) 알고리즘을 구동한다.</p>
<p>이 알고리즘은 베이스의 실시간 오도메트리(Odometry) 변화량을 지속적으로 모니터링하여, 월드 좌표계 상에서 엔드 이펙터의 위치가 고정(Locked)되도록 역기구학(Inverse Kinematics, IK)을 초당 수백 회 재계산하여 로봇 암의 관절 각도를 보정한다. 이 보상 시스템의 성능은 매우 탁월하여, 베이스가 초당 수십 센티미터를 병진 및 회전 기동하는 악조건 속에서도 엔드 이펙터의 월드 좌표계 기준 유지 오차를 12~16mm 이하로 제한하는 정밀도를 입증하였다. 이는 동적 환경에서 이동 중에도 흔들림 없이 물컵을 유지하거나 문손잡이를 잡고 부드럽게 뒤로 후진하며 문을 여는 등 고도의 연속적 동적 상호작용을 가능하게 하는 핵심 기술 기반이다.</p>
<h3>3.2  계층적 관절 강성 제어 프레임워크 (Hierarchical Joint Stiffness Controller)</h3>
<p>유연하고 안전한 조작을 구현하기 위해 YOR의 팔 제어는 작업 공간(Task-space)과 관절 공간(Joint-space) 제어를 모두 아우르는 계층적 구조를 취한다. 제어 루프의 핵심은 다음과 같은 목적 함수를 따르는 관절 강성 제어기(Joint stiffness controller)이다.<br />
<span class="math math-display">
\tau = K_p(q_{target} - q_{measured}) - K_d \dot{q} + \tau_{gravity}
</span><br />
여기서 <span class="math math-inline">\tau</span>는 관절에 인가되는 토크 명령, <span class="math math-inline">q_{measured}</span>는 현재 측정된 관절 위치, <span class="math math-inline">q_{target}</span>은 목표 위치, <span class="math math-inline">K_p</span>는 강성(Stiffness) 계수, <span class="math math-inline">K_d</span>는 감쇠(Damping) 계수를 나타낸다. <span class="math math-inline">\tau_{gravity}</span>는 로봇 팔 자체의 중력과 페이로드 하중을 상쇄하기 위해 실시간으로 계산되는 피드포워드 중력 보상(Feedforward gravity compensation) 항이다.</p>
<p>이 방정식은 기계 시스템이 외부 저항에 어떻게 반응할지를 결정한다. 강성 계수(<span class="math math-inline">K_p</span>)를 조절함으로써 핀 포인트 작업 시에는 팔을 단단하게 고정하고, 인간과 부딪힐 위험이 있는 작업 시에는 팔이 쉽게 밀리도록 유연성을 부여할 수 있다. 이 제어 스택은 두 개의 레이어로 나뉜다. 하위 레벨(Low-level) 컨트롤러는 200Hz의 초고속 주기로 동작하며 <strong>Ruckig</strong> 라이브러리를 사용해 저크(Jerk, 가속도의 변화량)가 제한된 부드러운 관절 궤적을 실시간 생성한다. 반면 상위 레벨(High-level) 컨트롤러는 전체 작업의 목표 궤적을 계획하고 <strong>Mink</strong> 라이브러리를 활용해 작업 공간 좌표를 관절 공간의 <span class="math math-inline">q_{target}</span>으로 변환하는 역기구학 연산을 수행한다. 이 계층적 분리를 통해 시스템은 통신 지연이나 상위 계획기의 연산 병목 현상이 발생하더라도 하위 제어기가 부드러운 움직임과 안정성을 스스로 담보할 수 있다.</p>
<h2>4.  직관적 원격 조작 인프라 및 고품질 다중 뷰 데이터 파이프라인</h2>
<p>모방 학습(Imitation Learning) 기반의 로봇 지능 연구에 있어, 기계 학습 알고리즘의 우수성만큼이나 중요한 것이 학습 데이터의 질과 양이다. 질 높은 전문가 시연(Expert demonstrations) 데이터를 수집하기 위해서는 운영자가 로봇을 조종할 때 느끼는 인지적, 신체적 피로도를 최소화하고 직관적인 조작이 가능한 원격 조작(Teleoperation) 시스템의 구축이 필수적이다.</p>
<p>기존의 Mobile ALOHA 등은 로봇의 뒷면에 동일한 구조의 리더 암(Leader arm)을 부착하여 조작자가 이를 물리적으로 움직이는 관절 공간 매핑(Joint-space mapping) 방식을 사용했다. 이 방식은 매우 직관적이라는 장점이 있으나, 무거운 로봇 뒤에서 조작자가 지속적으로 보행하며 물리적 힘을 가해야 하므로 체력 소모가 극심하고, 수직 리프트와 같은 추가 자유도를 동시에 제어하는 데 한계가 있었다. 반면 JoyLo와 같은 조이스틱 기반 시스템은 이동 조작의 직관성이 떨어졌다.</p>
<h3>4.1  VR 컨트롤러 기반의 오프보드 포즈 트래킹 (Offboard Pose Tracking)</h3>
<p>YOR는 하드웨어 결합 없이 소프트웨어적으로 완벽히 동기화되는 원격 조작 시스템을 구현하기 위해, 상용 가상현실(VR) 기기인 <strong>Meta Quest 3 및 3S 컨트롤러</strong>를 오프보드 포즈 트래킹 인터페이스로 활용하였다.</p>
<p>조작자가 VR 컨트롤러를 쥐고 허공에서 손을 움직이면, 컨트롤러의 6자유도 포즈가 캘리브레이션을 거쳐 로봇 양팔의 엔드 이펙터(EE) 목표 포즈로 실시간 리타겟팅(Retargeting)된다. 이 방식의 가장 큰 혁신은 조종 제어의 인체공학적 디커플링(Decoupling)에 있다. 로봇의 수직 리프트를 조종하기 위해 인간 운영자가 무릎을 굽히고 펴는 스쿼트 동작을 반복하도록 강제하는 대신, 컨트롤러의 아날로그 그립 버튼을 누르는 압력에 비례하여 상승/하강(Up/down) 속도 명령을 인가하도록 분리하였다. 또한 YOR 로봇 본체의 폭이 좁은 슬림한 프로파일(Slender profile) 덕분에 조작자는 로봇의 바로 뒤에 선 상태로 작업 공간을 향한 직접적인 가시선(Direct line-of-sight)을 유지할 수 있다. 이는 번거로운 HMD 착용이나 모니터를 통한 카메라 스트리밍에 의존할 필요 없이 육안으로 직접 확인하며 정밀한 제어를 수행할 수 있게 하여, 네트워크 지연(Latency)에 의한 조작 피로도를 완전히 제거한다.</p>
<h3>4.2  30Hz 주기의 멀티모달 데이터 동기화 파이프라인</h3>
<p>데이터 수집 모드에 돌입하면 시스템은 강력한 동기화 파이프라인을 가동한다. 원격 조작을 통해 수행되는 100건 이상의 전문가 시연 에피소드는 정확히 30Hz의 주기로 기록된다. 이 데이터셋은 멀티모달(Multimodal) 특성을 지니며, 로봇의 내부 상태를 나타내는 **고유수용성 감각 데이터(Proprioceptive data)**와 외부 환경을 나타내는 **시각 데이터(Visual data)**로 구성된다. 고유수용성 데이터에는 양팔 엔드 이펙터의 6D 포즈, 조향 및 구동 모터의 엔코더 정보, 리프트의 높이, 베이스의 실시간 오도메트리가 포함된다. 시각 데이터는 환경의 거시적 맥락을 파악하기 위해 로봇 헤드에 장착된 ZED 2i 스테레오 카메라에서 캡처한 256x256 해상도의 RGB 이미지 다발과, 미세한 조작을 위한 시각 피드백을 제공하기 위해 양쪽 손목 위치에 부착된 소형 카메라(iPhone 등을 활용)에서 획득한 손목 뷰 이미지가 포함된다. 이렇게 수집된 고해상도, 다중 뷰 데이터는 추후 시각-언어-행동 파운데이션 모델 학습의 원천 소스로 활용된다.</p>
<h2>5.  시각-관성 SLAM 및 자율 주행 인프라 (Perception &amp; Autonomous Navigation)</h2>
<p>저비용 모바일 로봇이 연구실을 벗어나 실제 가정 환경이나 동적인 공공장소에 배치될 때 직면하는 가장 큰 기술적 한계는 로컬리제이션(Localization, 자기 위치 추정)의 드리프트 현상과 정적 지도의 한계다. YOR는 값비싼 3D LiDAR 대신 저렴한 온보드 스테레오 카메라와 고도로 최적화된 탐색 알고리즘을 결합하여 강건한 자율 주행 성능을 구현하였다.</p>
<h3>5.1  실시간 고밀도 복셀 매핑 (Dense Voxel Mapping)</h3>
<p>YOR의 자율 주행 스택의 눈 역할을 하는 것은 전면 상단에 장착된 <strong>ZED 2i 스테레오 카메라</strong>이다. 시각-관성 SLAM(Visual-Inertial SLAM, VINS) 알고리즘은 ZED 2i에서 생성된 프레임 단위의 뎁스(Depth) 포인트 클라우드 데이터와 초고주기 IMU 센서 데이터를 수학적으로 융합(Sensor fusion)하여 로봇의 3D 궤적을 추정한다.</p>
<p>이 추정된 궤적을 바탕으로 로봇은 주변 환경에 대한 월드 좌표계 기준의 ’고밀도 복셀 맵(Dense voxel map)’을 실시간으로 구축한다. 맵핑 시스템은 0.02m(2cm) 크기의 초정밀 복셀 해상도를 사용하며, 로봇의 주행에 방해가 되는 지형지물을 식별하기 위해 지면 층(Floor band)을 0.05m로 분할하여 인식한다. 또한, 충돌 회피의 안전 마진을 보장하기 위해 맵 상의 모든 장애물에 대해 0.3m 반경의 인플레이션(Robot inflation radius)을 적용하는 보수적인 장애물 모델링을 수행한다.</p>
<table><thead><tr><th><strong>자율 주행 파라미터 영역</strong></th><th><strong>주요 설정 변수 (Parameter)</strong></th><th><strong>수치 및 단위</strong></th><th><strong>의미론적 해석</strong></th></tr></thead><tbody>
<tr><td><strong>Mapping Params</strong></td><td>글로벌 맵 복셀 크기 (Voxel size)</td><td>0.02 m</td><td>미세한 장애물 인식 및 정밀한 맵 해상도 유지</td></tr>
<tr><td><strong>Mapping Params</strong></td><td>지면 분할 대역 (Floor band)</td><td>0.05 m</td><td>높이가 낮은 문지방과 실제 벽/장애물을 구분</td></tr>
<tr><td><strong>Mapping Params</strong></td><td>로봇 인플레이션 반경 (Inflation radius)</td><td>0.3 m</td><td>충돌 회피를 위한 소프트웨어적 안전 버퍼 공간</td></tr>
<tr><td><strong>Controller Params</strong></td><td>최대 병진 속도 (Velocity max)</td><td>0.35 m/s</td><td>인간-로봇 상호작용 환경에서의 안전 주행 속도 제한</td></tr>
<tr><td><strong>Controller Params</strong></td><td>최대 회전 각속도 (Omega max)</td><td>1.0 rad/s</td><td>좁은 공간에서의 기민한 선회 역량 보장</td></tr>
<tr><td><strong>Controller Params</strong></td><td>위치 허용 오차 (Position Tolerance)</td><td>0.015 m</td><td>조작을 위한 베이스 정차 시의 1.5cm 초정밀 위치 제어</td></tr>
<tr><td><strong>Controller Params</strong></td><td>요 허용 오차 (Yaw Tolerance)</td><td>0.03 rad</td><td>조작 대상 방향 정렬 시의 미세 오차 허용 범위</td></tr>
</tbody></table>
<h3>5.2  서브세컨드(Sub-second) 단위의 동적 경로 재탐색</h3>
<p>정적 기반으로 사전에 구축된 지도(Static maps)는 보행자가 돌아다니거나 의자 위치가 수시로 바뀌는 인간 중심 환경에서 순식간에 무용지물이 된다. YOR 내비게이션 스택의 진정한 가치는 예기치 않은 동적 장애물에 대한 폭발적인 반응성에 있다.</p>
<p>로봇의 내부 제어 인터페이스에는 뎁스 카메라에 감지된 데이터가 적색(장애물) 복셀과 녹색(계획된 경로) 복셀로 실시간 투영된다. 보행 중인 사람과 같이 이전에 지도에 없던 장애물이 감지되면, 로봇은 기존의 최단 거리 경로가 차단되었음을 인지하고 즉각적으로 경로 재탐색(Replanning)을 트리거한다. 이 과정은 가중치 기반 A*(Weighted A*) 탐색 알고리즘과 로컬 궤적 추종을 위한 순수 추적(Pure pursuit tracking) 기법을 결합하여 수행된다. 가장 인상적인 점은 장애물 감지부터 대안 경로 도출 및 회피 기동 실행까지 걸리는 지연 시간(Latency)이 단 **1초 미만(Sub-second)**이라는 사실이다. 로봇은 목표 지점 도달이라는 전역적 임무를 잃지 않으면서도 장애물과의 안전거리 클리어런스를 유지하며 부드럽게 대안 궤적을 추종한다. 이러한 높은 반응성은 상용 최고급 플랫폼에서나 기대할 수 있었던 기능으로, 1/10 비용의 플랫폼에서 이 정도의 자율성과 강건성을 입증한 것은 괄목할 만한 성과다.</p>
<h2>6.  정책 학습(Policy Learning)과 VQ-BeT 기반 모방 학습 프레임워크</h2>
<p>정교한 하드웨어와 내비게이션 인프라 위에서, YOR 플랫폼의 궁극적인 임무는 복잡한 시각-운동 제어(Visuomotor control) 정책을 학습하고 추론하는 것이다. 연구진은 로봇의 모방 학습(Imitation Learning) 파이프라인 성능을 검증하기 위해 최신 알고리즘인 <strong>VQ-BeT(Vector Quantized Behavior Transformer)</strong> 아키텍처를 도입하였다.</p>
<h3>6.1  왜 VQ-BeT인가? 모달리티 병합과 추론 속도의 최적화</h3>
<p>전통적인 행동 복제(Behavior Cloning, BC) 모델들은 평균 제곱 오차(MSE)를 최소화하는 방식으로 훈련된다. 그러나 동일한 장애물을 마주했을 때 ’왼쪽으로 회피할 것인가, 오른쪽으로 회피할 것인가’와 같이 전문가 시연의 의사결정이 다봉 분포(Multimodal distribution)를 띨 경우, 전통적인 모델은 두 행동의 평균(장애물 정면 돌파)을 취해버리는 치명적인 결함이 있다. 이를 해결하기 위해 최근 디퓨전 정책(Diffusion Policies)이 각광받고 있으나, 디퓨전 모델은 추론 과정에서 반복적인 노이즈 제거 연산을 수행해야 하므로 엣지 디바이스(Edge devices)에서 실시간으로 구동하기에는 추론 지연(Inference latency)이 크다는 한계가 존재한다.</p>
<p>VQ-BeT는 연속적인 행동 공간(Continuous actions)을 계층적 벡터 양자화(Hierarchical vector quantization) 모듈을 통해 이산적인 토큰(Tokens)으로 변환(Tokenizing)하여 이 문제를 우회한다. 변환된 토큰들은 거대 언어 모델이 단어를 예측하듯 트랜스포머에 의해 확률적으로 예측되므로, 다중 모달리티 행동 패턴을 완벽히 포착하면서도 디퓨전 정책 대비 무려 <strong>5배 빠른 추론 속도</strong>를 달성한다. 이는 즉각적인 반응이 필수적인 YOR와 같은 모바일 매니퓰레이터 시스템에 최적화된 선택이다.</p>
<h3>6.2  모델 아키텍처의 세부 구조와 작업 성공률</h3>
<p>YOR에 탑재된 정책 학습 네트워크 아키텍처는 시각 정보와 고유수용성 정보를 융합하는 강력한 멀티모달 인코더-디코더 구조를 지닌다. 먼저, 자기지도 학습(Self-supervised learning) 방식으로 사전 학습된 공유형 ResNet-50 인코더를 통해 다중 뷰 이미지(ZED 카메라의 헤드 뷰 및 양쪽 손목 카메라 뷰)로부터 고차원 시각적 특징(Visual features) 텐서를 추출한다. 동시에 로봇 관절 각도, 베이스 속도 등의 고유수용성 데이터는 선형 프로젝션(Linear up-projection) 과정을 거쳐 시각 텐서와 병합(Concatenation)된다. 마지막으로, 트랜스포머 디코더(Transformer decoder) 블록이 과거 일정 프레임 동안의 관측창(Window of past observations) 시계열 데이터를 분석하여, 다음 스텝에 취해야 할 이산화된 행동 토큰을 실시간으로 추론해 낸다.</p>
<p>이러한 고강도의 딥러닝 추론 과정은 로봇 내부의 라즈베리 파이 5 단독으로는 연산 부하를 감당하기 어렵다. 따라서 네트워크 소켓 통신(Commlink/ZMQ 라이브러리)을 이용해 로컬 환경의 고성능 원격 GPU 서버에서 딥러닝 추론을 수행하고, 도출된 액션 명령만을 온보드 라즈베리 파이로 실시간 스트리밍하는 하이브리드 아키텍처를 채택하였다.</p>
<p>이 학습 파이프라인의 실효성을 검증하기 위해 연구진은 재활용품 분류, 바닥의 쓰레기를 주워 통에 넣기 등 ’물건 집기, 수직 상승, 이동, 내려놓기(Pick-carry-place)’가 연쇄적으로 결합된 복합 태스크를 평가하였다. 실험 결과, 훈련된 정책은 10회의 테스트 중 9회를 완벽히 성공하며 <strong>90%의 경이로운 작업 성공률</strong>을 달성하였다. 주목할 점은 발생한 10%의 실패 원인이다. 분석 결과 이는 모델의 물체 파지(Grasping)나 팔 궤적 생성 실패가 아니라, 작업 과정에서 카메라의 시야 가림(Occlusion)이 발생하여 시각-관성 SLAM의 오도메트리에 누적된 드리프트 오차가 원인인 것으로 판명되었다. 이는 VQ-BeT 정책 학습 프레임워크 자체의 신뢰성은 극한에 달해 있으며, 오히려 플랫폼의 인식(Perception) 스택만 조금 더 보강된다면 완벽에 가까운 자율 작업이 가능함을 시사한다.</p>
<h2>7.  로봇 범용성 모델(RUMs)과의 시너지 및 제로샷(Zero-Shot) 일반화</h2>
<p>YOR 프로젝트의 진정한 파괴력은 단순한 저비용 하드웨어의 공개를 넘어, 이것이 체화된 AI 커뮤니티의 최대 화두인 <strong>‘로봇 범용성 모델(Robot Utility Models, RUMs)’</strong> 구축을 위한 대규모 데이터 수집 기지로 작동한다는 점에 있다. RUMs 프로젝트는 자연어 처리 영역의 LLM이나 비전 모델들이 겪었던 패러다임 전환과 동일한 궤적을 지향한다. 즉, 새로운 환경을 만날 때마다 로봇 모델을 일일이 파인튜닝(Fine-tuning)하는 기존의 관행을 타파하고, 충분한 스케일의 사전 학습만으로 완전히 새로운 환경(Novel environments)과 물체(Unseen objects)에 대해 즉각적인 제로샷 배포(Zero-shot deployment)를 달성하는 것이다.</p>
<h3>7.1  대규모 다변화 데이터 수집과 정책 일반화의 상관관계</h3>
<p>NYU 연구진은 RUMs 프레임워크의 가능성을 실증하기 위해 수납장 문 열기, 서랍 열기, 냅킨 줍기, 종이 가방 줍기, 쓰러진 물건 방향 똑바로 세우기(Reorienting)라는 일상적인 5가지 유틸리티 태스크를 정의하였다. 이를 위해 YOR 및 유사 기종(Hello Robot Stretch, UFactory xArm 등)을 활용하여 작업당 약 1,000번, 총 5,000건 이상의 시연 데이터를 36개의 서로 다른 환경에서 수집하였다. 수집된 다중 모달 데이터는 6D 포즈 행동 어노테이션과 함께 정책 모델 학습에 투입되었다.</p>
<p>결과는 학계에 큰 충격을 던졌다. 학습에 단 한 번도 노출되지 않은 뉴욕, 저지시티, 피츠버그 등지의 실제 가정집, 소매점, 회의장 등 25개의 완전히 낯선 환경(Unseen environments)에 로봇을 투입하여 테스트한 결과, 평균 <strong>90%의 압도적인 제로샷 일반화 성공률</strong>을 기록한 것이다. 특히 이 실험 과정에서 중요한 이론적 함의가 도출되었다. 연구진이 학습 데이터의 변수를 통제하며 모델의 성능 변화를 추적한 결과, 훈련 알고리즘의 복잡성이나 정책 클래스의 종류보다 **‘훈련 데이터의 질과 다양성(Diversity)’**이 제로샷 일반화 성능을 결정짓는 가장 압도적인 요인임이 밝혀졌다. 서랍 열기와 같이 기하학적 다양성이 적은 태스크는 균일한 환경 데이터만으로도 학습이 가능하지만, 쓰러진 물체 방향 틀기(Reorientation)와 같이 다양한 시각적 노이즈가 존재하는 태스크에서는 다양한 배경에서 수집된 다변화 데이터(Diverse dataset)로 훈련된 정책만이 우수한 성능을 유지하며, 균일 환경 모델은 성능이 50%나 급감하는 현상이 목격되었다.</p>
<h3>7.2  mLLM 기반의 로봇 자기 성찰(Introspection) 및 자율 회복 루프</h3>
<p>RUMs 시스템이 달성한 높은 성공률 이면에는 단순히 강건한 궤적 생성 외에도 획기적인 ’자율 회복 프레임워크’가 존재한다. 모바일 로봇이 작업을 1차로 시도한 후, 로봇의 카메라 시각 정보(Observation 요약본)는 로봇 외부에 위치한 멀티모달 대형 언어 모델(mLLM) 검증기(Verifier)로 전송된다.</p>
<p>이 mLLM 검증기는 로봇의 관측 이미지를 인간의 시각으로 분석하여 “수납장 문이 실제로 열렸는가?”, “물건이 제대로 파지되었는가?“를 평가한다. 만약 검증기가 작업 실패(Failure)로 판정하면, 로봇은 작업을 포기하는 대신 시스템을 새로운 초기 상태로 리셋하고 목표물을 향해 위치를 재조정한 뒤 자동으로 재시도(Retrying)하는 자기 성찰(Introspection) 기반의 페일세이프(Failsafe) 루프를 실행한다. 이는 모바일 매니퓰레이터가 예외 상황에 대처하는 능력을 인간 수준으로 끌어올리는 중요한 도약이다.</p>
<h2>8.  상용 및 오픈소스 모바일 매니퓰레이터 폼팩터 비교 분석</h2>
<p>YOR의 설계적 우수성을 객관적으로 평가하기 위해서는 현재 학계 및 로보틱스 시장에서 두각을 나타내고 있는 타 폼팩터들과의 직접적인 비교가 요구된다. 각 플랫폼은 비용, 조작성, 내비게이션 역량 측면에서 뚜렷한 트레이드오프(Trade-off)를 지닌다.</p>
<table><thead><tr><th><strong>플랫폼 명칭 (Platform)</strong></th><th><strong>구동 베이스 및 내비게이션 특성</strong></th><th><strong>팔 구조 및 상체 자유도 (DoF)</strong></th><th><strong>수직 공간 도달 및 조작 한계점 평가</strong></th><th><strong>예상 획득 비용 수준</strong></th></tr></thead><tbody>
<tr><td><strong>Mobile ALOHA</strong></td><td>대형 비홀로노믹(Non-holonomic) 베이스. 회전 반경 제약 큼</td><td>리더-팔로워 결합 양팔 (12 DoF)</td><td>조종은 직관적이나 수직 리프트가 없어 높낮이가 다른 작업 공간에 대응 불가. 풋프린트가 커 협소 구역 진입 불가.</td><td>고비용 (약 $32K 이상)</td></tr>
<tr><td><strong>Hello Robot Stretch</strong></td><td>초소형 단일 기둥형 홀로노믹 베이스. 기동성 우수</td><td>1 DoF 슬라이딩 암 + 그리퍼</td><td>신뢰성이 매우 높으나, 단일 팔로 인해 양손 조작(Bimanual)이 필수적인 복잡한 가사 노동 수행이 원천적으로 불가능함.</td><td>중가 (약 $20K)</td></tr>
<tr><td><strong>Tidybot++</strong></td><td>차동/홀로노믹 혼합 베이스</td><td>단일 로봇 암</td><td>스마트폰 기반 포즈 트래킹으로 데이터 수집이 용이하나, 수직 제어권이 없어 지면 물체 조작 등 한계 뚜렷.</td><td>연구실 커스텀 수준</td></tr>
<tr><td><strong>XLeRobot</strong></td><td>소형 바퀴형 모바일 베이스</td><td>소형 저가 암 2기</td><td>가격 접근성은 가장 높으나 페이로드 용량과 내구성, 도달 거리(Reach)가 지나치게 짧아 실제 환경의 물건 조작이 어려움.</td><td>극초저가 ($1K 이하)</td></tr>
<tr><td><strong>RBY1-A</strong></td><td>전방향 베이스 결합형 고성능 휴머노이드 상체</td><td>산업용 양팔 (20 DoF)</td><td>YOR와 지향하는 바가 동일하며 하드웨어 스펙이 압도적이나, 예산이 제한된 연구 기관에서 대량 도입하기 불가능함.</td><td>극고비용 ($100K 이상)</td></tr>
<tr><td><strong>YOR (본 연구)</strong></td><td>초소형 스워브 드라이브. 전방향(Omnidirectional) 홀로노믹 구동</td><td>유연 제어 양팔 (상체 13 DoF)</td><td>리프트를 통해 0.6~1.24m 수직 조작 완벽 대응. 기동성, 양손 조작, 비용을 모두 타협점 없이 달성한 최적 폼팩터.</td><td>저비용 (BOM $9.25K)</td></tr>
</tbody></table>
<p>비교표에서 드러나듯, YOR는 기존 시스템들이 가지는 양극단의 약점을 영리하게 피하고 있다. 휴머노이드 보행 제어의 복잡성을 가중시키지 않고 4륜 스워브 드라이브를 채택하여 최고 수준의 기동성을 확보했으며, 값비싼 산업용 매니퓰레이터 대신 1.5kg 페이로드와 ±0.1mm 오차를 보장하는 PiPER 암을 도입하여 양손 조작 능력을 포기하지 않았다. 수직 공간의 자유도는 상용 스탠딩 데스크 부품을 이식하여 해결했다. 이처럼 검증된 상용 컴포넌트들을 창의적으로 ’통합’한 설계 접근법은 모바일 매니퓰레이터 개발의 새로운 패러다임을 제시한다.</p>
<h2>9.  오픈소스 생태계 구축 및 확장 가능한 소프트웨어 인프라</h2>
<p>YOR가 체화된 AI 역사에서 중요한 변곡점을 형성하는 이유는 이 플랫폼이 완벽한 <strong>오픈소스(Open Source) 자산</strong>으로 대중에 환원되었기 때문이다. 과거 로보틱스 하드웨어의 설계 도면과 소프트웨어 통합 노하우는 거대 테크 기업이나 폐쇄적인 연구 기관의 전유물이었다. 이러한 독점은 전 세계 연구자들이 파운데이션 모델을 훈련시킬 대규모 분산 데이터를 확보하는 데 가장 큰 병목 현상이었다.</p>
<h3>9.1  체계적인 공개 지식 인프라와 조립 가이드</h3>
<p>연구진은 공식 프로젝트 웹사이트(yourownrobot.ai)와 전용 빌드 가이드 문서(build.yourownrobot.ai), 그리고 GitHub 저장소(aha-robot 등)를 통해 누구나 로봇을 처음부터 끝까지 복제할 수 있는 방대한 데이터를 투명하게 공개하였다.</p>
<p>공개된 기술 문서의 깊이는 단순한 부품 목록 수준을 넘어선다. 첫째, 모든 3D 프린팅 및 CNC 가공 부품의 오리지널 CAD 파일과 나사 하나까지 포함된 수량, 규격, 재질, 예상 단가가 명시된 극세사 수준의 자재명세서(BOM)가 스프레드시트 형태로 제공된다. 둘째, 기계공학적 지식이 부족한 컴퓨터 공학 기반의 AI 연구자들도 쉽게 로봇을 제작할 수 있도록 단계별(Step-by-step) 조립 튜토리얼을 제공한다. 이는 기초 베이스 프레임 조립(Base Assembly)부터 내부 전장 부품 및 모터 컨트롤러 통합, 복잡한 전력 및 통신 배선(Wiring and Final Assembly), 6자유도 양팔 모듈의 마운팅 작업, 마지막으로 Pico 마이크로컨트롤러를 이용한 리프트 제어기 펌웨어 셋업(Pico Lift Controller Setup)에 이르기까지 하드웨어 구축의 A to Z를 포괄한다.</p>
<h3>9.2  ROS 2 통합과 시뮬레이션 환경으로의 확장</h3>
<p>모듈형 하드웨어에 생명력을 불어넣는 소프트웨어 스택 역시 개방형 생태계의 철학을 따른다. YOR의 하위 제어 시스템과 센서 드라이버는 현대 로봇 공학의 사실상(De facto) 표준인 ROS 2(Robot Operating System 2) 아키텍처에 긴밀히 통합될 수 있도록 설계되었다. ROS 2의 채택은 다중 노드(Node) 간의 실시간 통신 안정성을 보장할 뿐만 아니라, 글로벌 커뮤니티에서 개발된 수많은 최신 내비게이션, 시각 처리 패키지를 YOR 플랫폼에 플러그 앤 플레이 방식으로 이식할 수 있음을 의미한다.</p>
<p>더 나아가 YOR의 기구학적 모델은 가상 환경 기반의 심-투-리얼(Sim-to-Real) 강화학습(Reinforcement Learning) 훈련을 위해 고성능 시뮬레이터와 결합될 잠재력을 지닌다. 특히, 최근 엔비디아(NVIDIA) GPU 병렬 처리 기술을 활용해 수만 개의 로봇 에이전트를 동시에 초고속 시뮬레이션 및 렌더링할 수 있는 <strong>ManiSkill3</strong>와 같은 프레임워크와의 통합은 매우 유망한 발전 방향이다. 물리 엔진(예: Isaac Gym, MuJoCo 등) 상에서 YOR의 디지털 트윈(Digital twin)을 구현하여 수백만 번의 반복 학습을 수행하고 그 결과 정책을 현실의 YOR 하드웨어로 전이(Transfer)시키는 파이프라인이 구축된다면, 로봇 학습 속도는 지금보다 수십 배 가속화될 것이다.</p>
<p>이러한 개방성은 결과적으로 생태계의 선순환을 창출한다. 특정 연구실에서 새로운 유형의 햅틱(Tactile) 그립퍼를 설계하여 GitHub에 공유하면 지구 반대편의 연구실에서 이를 3D 프린터로 즉시 출력해 장착할 수 있다. 동일한 하드웨어 스펙을 지닌 수천 대의 YOR 로봇이 전 세계 가정과 연구소에 배치되어 표준화된 데이터 형식으로 조작 데이터를 축적하기 시작한다면, 이 클라우드 데이터는 자연어 처리 분야의 ’커먼 크롤(Common Crawl)’과 같은 역할을 수행하며 궁극적인 거대 로봇 파운데이션 모델의 탄생을 이끌 것이다.</p>
<h2>10.  기술적 한계점 및 향후 발전 전망</h2>
<p>YOR는 비용 효율성과 성능 간의 탁월한 균형을 입증했으나, 상용 기성품 조합이라는 태생적 한계와 예산 제약으로 인해 일부 기술적 과제를 남겨두고 있으며, 이는 학계의 후속 연구를 통해 극복되어야 할 영역이다.</p>
<p>첫째, <strong>페이로드(Payload) 및 액추에이터 토크의 한계</strong>이다. PiPER 로봇 암이 제공하는 1.5kg의 유효 하중은 빈 컵을 옮기거나 냅킨을 줍는 일상적인 환경에는 적합하나 , 무거운 주물 냄비를 조작하거나 강하게 닫혀 있는 서랍의 손잡이를 힘주어 당겨야 하는 등 순간적으로 높은 토크와 반발력이 작용하는 환경에서는 기어박스 및 모터의 물리적 한계가 노출될 수 있다. 모터의 소형화 및 출력 밀도(Power density) 향상 기술이 도입되어 플랫폼 단가 상승 없이 하중 한계를 끌어올리는 것이 향후 과제다.</p>
<p>둘째, <strong>정밀한 양방향 햅틱(Tactile) 피드백의 부재</strong>이다. 현재 장착된 맞춤형 평행 개폐식 각도 그리퍼(Angular jaw grippers)는 원통형 문손잡이나 불규칙한 형상의 일상 객체를 단단히 파지하는 데 있어 형태적 제약이 따른다. 더욱 치명적인 것은 엔드 이펙터 끝단에 정밀한 6축 힘-토크(Force-torque) 센서나 고해상도 촉각 센서가 결여되어 있다는 점이다. 이로 인해 로봇은 문이 뻑뻑하여 덜 열렸는지, 물체가 미끄러지고 있는지를 물리적 접촉감이 아닌 시각 센서의 오차 추정에 의존해 판단해야 하며, 이는 투명한 유리잔이나 반사재질 물체 조작 시 실패율을 높이는 원인이 된다.</p>
<p>셋째, <strong>시각-관성 SLAM의 오도메트리 드리프트(Drift)</strong> 현상이다. 실험 실패 사례에서 집중적으로 보고된 바와 같이, ZED 2i 카메라 하나에 의존하는 로컬리제이션 시스템은 좁은 수납장 안을 들여다보거나 두 팔이 렌즈 앞을 교차하며 시야 가림(Occlusion)을 유발할 때 특징점(Feature) 추출에 실패하여 치명적인 위치 인식 오류를 범한다. 이를 해결하기 위해 향후 베이스 휠의 엔코더 오도메트리와 초저가 소형 LiDAR 스캐너 데이터를 융합하는 다중 센서 칼만 필터(Kalman Filter) 아키텍처의 도입이 필수적일 것으로 판단된다.</p>
<h2>11.  결론</h2>
<p>YOR(Your Own Mobile Manipulator for Generalizable Robotics)는 하드웨어의 설계적, 경제적 복잡성이라는 체화된 인공지능 연구의 가장 거대한 병목을 획기적으로 낮춘 혁신적이고 기념비적인 플랫폼이다. 전통적인 차동 구동의 한계를 깬 4륜 전방향 스워브 드라이브 모빌리티, 광범위한 3차원 작업 공간을 보장하는 상용 텔레스코픽 수직 리프트, 그리고 환경 적응적 유연 제어(Compliant actuation)를 지원하는 고신뢰성 양팔 매니퓰레이터의 결합은 1만 달러(BOM 기준 9,250달러) 미만이라는 극단적인 예산 제약 속에서도 모바일 조작 로봇이 갖춰야 할 모든 필수적 기구학 및 동역학 특성을 타협 없이 구현해 내는 데 성공하였다.</p>
<p>나아가, YOR는 단순히 저렴한 기계 장치를 제안하는 데 그치지 않았다. 인간의 인지 부하를 극한으로 낮춘 VR 디바이스 기반의 디커플링 원격 조작 인터페이스부터, 12~16mm 오차 이내로 엔드 이펙터를 월드 좌표계에 고정하는 고주기 기구학적 보상 로직, 1초 미만(Sub-second)의 반응성으로 동적 장애물을 회피하는 고밀도 복셀 내비게이션, 그리고 추론 속도와 멀티모달리티를 모두 잡은 VQ-BeT 기반의 첨단 정책 학습 알고리즘까지, 로보틱스의 모든 스택을 통합한 엔드투엔드(End-to-end) 솔루션을 오픈소스로 공개함으로써 로봇 공학 연구의 새로운 표준을 선포하였다.</p>
<p>특히 로봇 범용성 모델(Robot Utility Models, RUMs) 프로젝트를 통해 실증된 90%에 달하는 낯선 환경에서의 제로샷 일반화(Zero-shot generalization) 성공률은, YOR와 같이 접근성이 뛰어나고 표준화된 플랫폼이 전 세계 연구 기관에 보급되어 대규모 다변화(Diverse) 데이터 수집의 전초기지로 작용할 때 달성될 수 있는 기술적 스케일링의 파괴력을 명확히 보여준다. 플랫폼의 모든 CAD 도면, 자재명세서, 배선 정보, ROS 2 기반 제어 스택이 투명하게 대중에 공유되는 개방형 지식 생태계는 향후 이 플랫폼을 끊임없이 진화시킬 것이다. 궁극적으로 YOR 플랫폼의 등장은 통제된 실험실 환경에 머물러 있던 모바일 매니퓰레이터를 현실 세계의 거실과 주방, 공공장소로 이끌어내어, 비정형적 환경 속에서 인간과 자연스럽게 상호작용하며 복잡한 물리적 임무를 자율적으로 수행하는 진정한 의미의 범용 로봇(General-purpose robots) 시대의 도래를 비약적으로 앞당기는 기술적 기폭제가 될 것이다.</p>
<h2>12. 참고 자료</h2>
<ol>
<li>(PDF) YOR: Your Own Mobile Manipulator for Generalizable Robotics - ResearchGate, https://www.researchgate.net/publication/400705357_YOR_Your_Own_Mobile_Manipulator_for_Generalizable_Robotics</li>
<li>Soumith Chintala’s research works | Meta and other places - ResearchGate, https://www.researchgate.net/scientific-contributions/Soumith-Chintala-74905280</li>
<li>Robot Utility Models: General Policies for Zero-Shot Deployment in New Environments, https://ieeexplore.ieee.org/document/11127857/</li>
<li>机器学习2026_2_12 - arXiv每日学术速递, https://www.arxivdaily.com/thread/76536</li>
<li>YOR: Your Own Mobile Manipulator for Generalizable Robotics, https://www.emergentmind.com/papers/2602.11150</li>
<li>arxiv.org, https://arxiv.org/html/2602.11150v1</li>
<li>YOR: Your Own Mobile Manipulator for Generalizable Robotics - Emergent Mind, https://www.emergentmind.com/videos/yor-affordable-mobile-manipulator-9e1509cb</li>
<li>YOR Docs: Meet YOR, https://build.yourownrobot.ai</li>
<li>YOR Docs: Meet YOR, https://build.yourownrobot.ai/</li>
<li>PiPER - Agilex Robotics, https://global.agilex.ai/products/piper</li>
<li>AgileX Piper - 6 Axis Robotic Arm - MG Super Labs, https://mgsl.in/products/piper-6-axis-robotic-arm</li>
<li>AgileX PiPER Robotic Arm in Dubai UAE - Ednex Automation, https://www.ednexautomation.ai/agilex/piper/</li>
<li>YOR: Your Own Mobile Manipulator for Generalizable Robotics - YouTube, https://www.youtube.com/watch?v=NTxHaj24TyY</li>
<li>[Papierüberprüfung] YOR: Your Own Mobile Manipulator for Generalizable Robotics, https://www.themoonlight.io/de/review/yor-your-own-mobile-manipulator-for-generalizable-robotics</li>
<li>Behavior Generation with Latent Actions - arXiv.org, https://arxiv.org/html/2403.03181v1</li>
<li>BAKU: An Efficient Transformer for Multi-Task Policy Learning - NIPS, https://proceedings.neurips.cc/paper_files/paper/2024/file/ff887781480973bd3cb6026feb378d1e-Paper-Conference.pdf</li>
<li>jayLEE0301/vq_bet_official: Official code for “Behavior Generation with Latent Actions” (ICML 2024 Spotlight) - GitHub, https://github.com/jayLEE0301/vq_bet_official</li>
<li>(PDF) Robot Utility Models: General Policies for Zero-Shot Deployment in New Environments - ResearchGate, https://www.researchgate.net/publication/383911788_Robot_Utility_Models_General_Policies_for_Zero-Shot_Deployment_in_New_Environments</li>
<li>Robot Utility Models: General Policies for Zero-Shot Deployment in New Environments, https://arxiv.org/html/2409.05865v1</li>
<li>Robot Utility Models: General Policies for Zero-Shot Deployment in New Environments, https://www.alphaxiv.org/overview/2409.05865v1</li>
<li>Robot Utility Models: General Policies for Zero-Shot Deployment in New Environments, https://robotutilitymodels.com/</li>
<li>Download and Deploy: Building Robots That Work Anywhere - NYU Center for Data Science, https://nyudatascience.medium.com/download-and-deploy-building-robots-that-work-anywhere-28277ff548c5</li>
<li>PiPER ROBOTIC ARM - Quick start user manual, https://static.generation-robots.com/media/agilex-piper-user-manual.pdf</li>
<li>Human-Assisted-Manufacturing Model Library - DTIC, https://apps.dtic.mil/sti/tr/pdf/ADA564714.pdf</li>
<li>Development of an Educational Robotic Platform with Dual Control Interfaces and Intelligent Computer Vision-Based Interaction, https://www.mas.bg.ac.rs/_media/istrazivanje/fme/vol54/1/8_d.p._thinh_et_al.pdf</li>
<li>AIS 2025 PROCEEDINGS - Óbudai Egyetem, <a href="https://ais.amk.uni-obuda.hu/proceedings/2025/AIS%202025%20Proceedings.pdf">https://ais.amk.uni-obuda.hu/proceedings/2025/AIS%202025%20Proceedings.pdf</a></li>
<li>Integration of ABB Robot Manipulators and Robot Operating System for Industrial Automation - ResearchGate, https://www.researchgate.net/publication/369927628_Integration_of_ABB_Robot_Manipulators_and_Robot_Operating_System_for_Industrial_Automation</li>
<li>When control meets large language models: From words to dynamics - ResearchGate, https://www.researchgate.net/publication/400415763_When_control_meets_large_language_models_From_words_to_dynamics</li>
<li>HBRC ROS2 reference robot? - Google Groups, https://groups.google.com/g/hbrobotics/c/mbdN5HRLhpI</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>