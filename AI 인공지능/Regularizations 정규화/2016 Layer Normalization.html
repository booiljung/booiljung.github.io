<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:레이어 정규화(Layer Normalization, 트랜스포머에서의 활용, 2016)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>레이어 정규화(Layer Normalization, 트랜스포머에서의 활용, 2016)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">정규화 (Regularizations)</a> / <span>레이어 정규화(Layer Normalization, 트랜스포머에서의 활용, 2016)</span></nav>
                </div>
            </header>
            <article>
                <h1>레이어 정규화(Layer Normalization, 트랜스포머에서의 활용, 2016)</h1>
<h2>1.  심층 신경망 학습의 안정화를 위한 정규화의 필요성</h2>
<p>심층 신경망(Deep Neural Networks, DNNs)은 현대 인공지능 분야에서 전례 없는 성공을 거두었으나, 그 구조적 깊이는 학습 과정에 있어 본질적인 불안정성을 야기한다. 모델의 층이 깊어질수록 학습은 더욱 어려워지며, 이를 해결하기 위한 다양한 기법 중 정규화(Normalization)는 가장 핵심적인 역할을 수행한다. 본 장에서는 심층 신경망 학습의 근본적인 어려움을 살펴보고, 이를 해결하기 위한 정규화 기법의 필요성을 논한다.</p>
<h3>1.1  심층 신경망 학습의 어려움: 기울기 소실/폭주 문제</h3>
<p>심층 신경망의 학습은 역전파(backpropagation) 알고리즘을 통해 출력층에서 입력층 방향으로 그래디언트(gradient)를 전달하여 가중치를 업데이트하는 방식으로 이루어진다. 그러나 네트워크의 깊이가 증가함에 따라, 이 그래디언트 값은 층을 거치면서 지수적으로 감소하거나(기울기 소실, vanishing gradient) 증가하는(기울기 폭주, exploding gradient) 경향을 보인다.1</p>
<p>기울기 소실 문제는 하위 레이어(입력층에 가까운 층)의 가중치가 거의 업데이트되지 않아 사실상 학습이 정체되는 현상을 유발한다. 반대로 기울기 폭주 문제는 가중치 업데이트가 너무 과도하게 일어나 학습 과정을 불안정하게 만들고, 결국 모델이 최적의 해로 수렴하는 것을 방해한다. 이러한 그래디언트의 불안정성은 심층 신경망의 학습을 저해하는 가장 근본적인 원인 중 하나로, 학습 속도를 현저히 저하시키고 때로는 학습 자체를 불가능하게 만든다.3</p>
<h3>1.2  내부 공변량 변화(Internal Covariate Shift)의 개념과 문제점</h3>
<p>심층 신경망 학습을 어렵게 만드는 또 다른 주요 요인은 ’내부 공변량 변화(Internal Covariate Shift, ICS)’라는 현상이다. 2015년 Ioffe와 Szegedy에 의해 처음 명명된 이 현상은, 학습 과정에서 신경망의 파라미터가 반복적으로 업데이트됨에 따라 각 은닉층(hidden layer)으로 들어오는 입력 데이터의 분포가 계속해서 변하는 것을 의미한다.5</p>
<p>심층 신경망에서 한 층의 출력은 다음 층의 입력이 된다. 학습이 진행되면서 이전 층의 가중치가 변경되면, 해당 층의 출력 분포 역시 변화한다. 결과적으로 다음 층은 매번 다른 분포의 입력을 받게 되는데, 이는 마치 ‘움직이는 과녁을 맞추려는’ 것과 같은 상황을 연출한다.7 각 층은 이전 층으로부터 전달되는 불안정한 입력 분포에 끊임없이 적응해야만 하므로, 학습 과정 전체가 비효율적으로 변한다.2</p>
<p>이러한 내부 공변량 변화는 다음과 같은 구체적인 문제들을 야기한다:</p>
<ol>
<li><strong>학습 속도 저하</strong>: 각 층이 변화하는 입력 분포에 적응하기 위해 추가적인 학습 노력을 기울여야 하므로 전체적인 수렴 속도가 느려진다.6</li>
<li><strong>낮은 학습률(Learning Rate) 강제</strong>: 입력 분포가 불안정하면 그래디언트의 방향과 크기가 예측 불가능하게 변동할 수 있다. 이로 인해 학습이 발산하는 것을 막기 위해 어쩔 수 없이 매우 작은 학습률을 사용해야 하며, 이는 학습 시간을 더욱 길게 만든다.3</li>
<li><strong>신중한 가중치 초기화 요구</strong>: 모델이 포화(saturating) 비선형 활성화 함수(예: Sigmoid, Tanh)를 사용할 경우, 입력 분포가 활성화 함수의 비선형적이고 그래디언트가 0에 가까운 영역으로 밀려나면 기울기 소실 문제가 심화된다. 이를 피하기 위해 매우 신중한 가중치 초기화 전략이 요구된다.6</li>
</ol>
<h3>1.3  정규화 기법의 등장 배경과 목표</h3>
<p>내부 공변량 변화 문제를 완화하고 심층 신경망 학습을 안정화시키기 위한 직접적인 해결책으로 다양한 정규화 기법이 제안되었다.2 정규화의 핵심 목표는 각 층에 들어오는 입력의 통계적 속성(주로 평균과 분산)을 일정하게 유지시켜 주는 것이다. 즉, 입력 데이터의 분포를 평균이 0이고 분산이 1인 표준 정규분포와 유사하게 만들어, 각 층이 보다 안정적이고 일관된 환경에서 학습할 수 있도록 돕는다.3</p>
<p>정규화를 통해 얻을 수 있는 이점은 다음과 같다:</p>
<ul>
<li><strong>학습 안정화 및 수렴 속도 향상</strong>: ICS를 줄임으로써 모델은 더 높은 학습률을 안정적으로 사용할 수 있게 되어 학습 수렴 속도가 크게 향상된다.6</li>
<li><strong>가중치 초기화 의존성 감소</strong>: 입력 분포가 정규화를 통해 제어되므로, 가중치 초기값에 대한 민감도가 낮아져 학습이 더 강건해진다.8</li>
<li><strong>규제(Regularization) 효과</strong>: 일부 정규화 기법은 학습 과정에 약간의 노이즈를 주입하는 효과가 있어, 모델의 일반화 성능을 높이고 과적합(overfitting)을 방지하는 부수적인 이점을 제공한다.8</li>
</ul>
<p>초기 연구에서는 정규화의 성공 요인을 주로 내부 공변량 변화의 해결에서 찾았다.6 그러나 후속 연구들은 정규화가 단순히 입력 분포를 고정시키는 것을 넘어, 손실 함수(loss function)의 최적화 환경(optimization landscape) 자체를 더 매끄럽게(smoother) 만들어 준다는 점을 지적했다. 즉, 정규화는 그래디언트의 크기와 방향을 더 예측 가능하게 만들어 경사 하강법 기반의 옵티마이저가 더 안정적이고 빠르게 최적점에 도달하도록 돕는, 보다 근본적인 메커니즘을 제공한다. 따라서 ICS는 정규화 기법의 등장을 촉발한 역사적 동기로 이해하되, 그 실제 효과는 최적화 환경 개선이라는 더 넓은 맥락에서 파악하는 것이 중요하다.</p>
<h2>2.  배치 정규화(Batch Normalization): 선구적 접근법과 본질적 한계</h2>
<p>배치 정규화(Batch Normalization, BN)는 2015년 Ioffe와 Szegedy에 의해 제안된 기법으로, 심층 신경망 학습의 패러다임을 바꾼 선구적인 접근법이다.6 이 기법은 내부 공변량 변화 문제를 효과적으로 해결함으로써 학습 속도를 획기적으로 개선하였고, 이후 수많은 심층 신경망 모델의 표준 구성 요소로 자리 잡았다. 그러나 그 강력한 성능에도 불구하고, 배치 정규화는 특정 상황에서 명확한 한계를 드러냈으며, 이는 후속 정규화 기법들의 등장을 촉발하는 계기가 되었다.</p>
<h3>2.1  배치 정규화의 동작 원리 및 수식</h3>
<p>배치 정규화의 핵심 아이디어는 학습 과정에서 데이터를 미니 배치(mini-batch) 단위로 처리할 때, 각 미니 배치 내에서 각 특성(feature) 또는 채널(channel)별로 데이터의 평균과 분산을 계산하여 정규화를 수행하는 것이다.8 이 과정은 활성화 함수를 통과하기 전의 선형 변환 결과에 적용된다.</p>
<p>배치 정규화의 연산 과정은 다음과 같은 4단계로 구성된다:</p>
<ol>
<li>
<p><strong>미니 배치 통계량 계산</strong>: 현재 처리 중인 미니 배치 <span class="math math-inline">\mathcal{B} = \{x_1, \dots, x_m\}</span>에 대해, 각 특성 차원별로 평균(<span class="math math-inline">\mu_{\mathcal{B}}</span>)과 분산(<span class="math math-inline">\sigma_{\mathcal{B}}^2</span>)을 계산한다.12<br />
<span class="math math-display">
\mu_{\mathcal{B}} = \frac{1}{m} \sum_{i=1}^{m} x_i
</span></p>
<p><span class="math math-display">
\sigma_{\mathcal{B}}^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_{\mathcal{B}})^2
</span></p>
</li>
</ol>
<p><strong>정규화</strong>: 계산된 평균과 분산을 사용하여 각 입력 데이터 <span class="math math-inline">x_i</span>를 평균 0, 분산 1을 갖도록 정규화한다. 이때 분모가 0이 되는 것을 방지하기 위해 아주 작은 양수 <span class="math math-inline">\epsilon</span>을 더해준다.8<br />
<span class="math math-display">
   \hat{x}_i = \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}}
</span><br />
<strong>스케일 및 시프트 변환</strong>: 모든 입력을 단순히 표준 정규분포로 만들어버리면, 모델이 학습해야 할 데이터의 고유한 분포 특성이 사라져 표현력이 저하될 수 있다. 이를 방지하기 위해, 학습 가능한 파라미터인 스케일(scale) 파라미터 <span class="math math-inline">\gamma</span>와 시프트(shift) 파라미터 <span class="math math-inline">\beta</span>를 도입하여 아핀 변환(affine transformation)을 수행한다. 이 파라미터들은 역전파를 통해 다른 모델 파라미터와 함께 학습된다.8<br />
<span class="math math-display">
   y_i = \gamma \hat{x}_i + \beta
</span><br />
만약 모델이 <span class="math math-inline">\gamma = \sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}</span>이고 <span class="math math-inline">\beta = \mu_{\mathcal{B}}</span>라고 학습한다면, 정규화의 효과를 상쇄하고 원래의 입력을 복원할 수도 있다. 이는 모델에게 정규화의 강도를 스스로 조절할 수 있는 유연성을 부여한다.</p>
<h3>2.2  배치 정규화의 장점: 학습 속도 개선 및 규제 효과</h3>
<p>배치 정규화는 심층 신경망 학습에 다음과 같은 긍정적인 효과를 가져온다:</p>
<ul>
<li><strong>학습 안정화 및 수렴 속도 향상</strong>: 내부 공변량 변화를 효과적으로 억제하여 각 층의 입력 분포를 안정시킨다. 이로 인해 모델은 더 높은 학습률을 사용하여 더 빠르고 안정적으로 수렴할 수 있다.6</li>
<li><strong>가중치 초기화 민감도 감소</strong>: 각 층의 출력이 정규화되므로, 가중치 초기값의 스케일에 덜 민감해진다. 이는 하이퍼파라미터 튜닝의 부담을 줄여준다.8</li>
<li><strong>규제 효과</strong>: 각 미니 배치는 전체 데이터셋의 일부이므로, 미니 배치의 평균과 분산은 약간의 노이즈를 포함한다. 이 노이즈가 학습 과정에 주입되어 모델이 특정 미니 배치에 과적합되는 것을 방지하는 규제 효과를 낳는다. 이로 인해 드롭아웃(Dropout)과 같은 다른 규제 기법의 필요성이 줄어들기도 한다.6</li>
</ul>
<h3>2.3  배치 정규화의 한계점</h3>
<p>그 강력한 성능에도 불구하고, 배치 정규화는 그 작동 방식에 내재된 몇 가지 근본적인 한계를 가지고 있다.</p>
<h4>2.3.1  미니 배치 크기에 대한 의존성</h4>
<p>배치 정규화의 가장 큰 한계는 성능이 미니 배치 크기에 크게 의존한다는 점이다.12 이 기법의 핵심 가정은 ’미니 배치의 통계량(평균, 분산)이 전체 데이터셋의 통계량을 잘 근사할 수 있다’는 것이다. 이는 대수의 법칙과 중심극한정리에 기반한 통계적 가정이다.15</p>
<p>그러나 미니 배치 크기가 매우 작아지면(예: 1, 2, 4 등), 이 가정이 깨지기 시작한다. 작은 수의 샘플로 계산된 평균과 분산은 노이즈가 심하고 변동성이 커서 전체 데이터의 분포를 대표하지 못한다.11 극단적으로 배치 크기가 1이면 분산은 항상 0이 되어 정규화가 불가능해진다. 이러한 불안정한 통계량은 오히려 학습에 악영향을 미칠 수 있다.17 이 문제는 특히 거대한 모델(예: 고해상도 이미지를 처리하는 CNN, 대규모 언어 모델)을 학습시킬 때 심각해진다. GPU 메모리 제약으로 인해 큰 배치 크기를 사용하기 어려워, 배치 정규화의 효과를 온전히 누리기 어렵기 때문이다.16</p>
<h4>2.3.2  순환 신경망(RNN) 적용의 어려움</h4>
<p>배치 정규화는 순환 신경망(Recurrent Neural Networks, RNN)과 같은 시퀀스 데이터 모델에 적용하기 매우 까다롭다.17 RNN은 가변적인 길이의 시퀀스를 처리하는데, 이는 배치 정규화의 기본 가정을 흔든다. 예를 들어, 한 미니 배치에 길이가 다른 여러 문장이 포함되어 있을 경우, 특정 타임스텝(time step)에서는 일부 문장만 데이터를 가지고 있고 나머지는 패딩(padding) 처리된다. 이 상황에서 해당 타임스텝의 ’배치 통계량’을 어떻게 의미 있게 정의하고 계산할 것인지가 모호해진다.</p>
<p>각 타임스텝마다 통계치가 다를 수 있으므로, 모든 타임스텝에 대해 별도의 통계량을 계산하고 저장해야 한다.12 이는 모델을 매우 복잡하게 만들고, 특히 긴 시퀀스에 대해서는 비효율적이다. 이러한 구조적 비호환성 때문에 배치 정규화는 RNN 계열 모델에서 널리 사용되지 못했다.4</p>
<h4>2.3.3  학습과 추론 시의 불일치</h4>
<p>배치 정규화는 학습(training) 시와 추론(inference) 시의 동작 방식이 다르다는 문제점을 안고 있다.13 학습 시에는 현재 미니 배치의 평균과 분산을 사용하여 정규화를 수행한다. 하지만 추론 시에는 일반적으로 단일 데이터 샘플을 처리하므로 ’미니 배치’라는 개념이 존재하지 않는다.</p>
<p>이 문제를 해결하기 위해, 학습 과정 동안 각 층에서 계산된 미니 배치들의 평균과 분산에 대한 이동 평균(moving average)을 계산하여 저장해 둔다. 그리고 추론 시에는 이 저장된 전역 통계량(global statistics)을 사용하여 정규화를 수행한다.8 이 방식은 학습과 추론 간의 계산 불일치를 야기하며, 구현상의 복잡성을 증가시키는 요인이 된다.</p>
<p>이러한 한계점들은 단순한 기술적 제약을 넘어선다. 이는 ’배치’라는 축을 기준으로 통계를 계산하는 배치 정규화의 근본적인 설계와, 시퀀스 데이터나 소규모 배치 학습 환경과 같은 특정 문제 영역의 특성 간의 근본적인 불일치에서 비롯된 필연적인 결과이다. 따라서 이러한 문제를 해결하기 위해서는 ’배치’라는 축에서 벗어나 새로운 차원의 정규화 방식을 모색할 필요가 있었고, 이는 레이어 정규화의 탄생으로 이어지는 강력한 동기가 되었다.</p>
<h2>3.  레이어 정규화(Layer Normalization)의 탄생</h2>
<p>배치 정규화가 제시한 한계, 특히 순환 신경망(RNN)과 같은 시퀀스 모델에서의 적용 어려움은 새로운 정규화 기법의 필요성을 대두시켰다. 이러한 배경 속에서 2016년, Geoffrey Hinton 연구팀(Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton)은 “Layer Normalization“이라는 논문을 통해 배치 정규화의 근본적인 문제들을 해결하는 혁신적인 대안을 제시했다.19</p>
<h3>3.1  배치 정규화의 한계 극복을 위한 제안</h3>
<p>레이어 정규화(Layer Normalization, LN)는 배치 정규화의 주요 단점들을 정면으로 겨냥하여 설계되었다. 개발자들은 배치 크기에 대한 의존성, RNN 적용의 어려움, 그리고 학습과 추론 단계에서의 계산 불일치 문제를 해결하는 것을 명확한 목표로 삼았다.15 이들의 접근 방식은 정규화를 위한 통계량을 계산하는 ’단위’를 근본적으로 바꾸는 것이었다.</p>
<h3>3.2  핵심 아이디어: 배치로부터의 독립성 확보</h3>
<p>배치 정규화가 미니 배치 내 여러 샘플에 걸쳐 각 특성별로 통계를 계산하는, 즉 샘플 간(inter-sample) 정규화 방식이었던 것과 대조적으로, 레이어 정규화는 <strong>단 하나의 데이터 샘플 내에서</strong> 모든 뉴런(또는 모든 특성 차원)에 걸쳐 통계를 계산한다.8 이는 정규화의 범위를 ‘배치’ 차원에서 ‘레이어’ 차원으로 옮긴 것으로, 샘플 내(intra-sample) 정규화 방식이라 할 수 있다.</p>
<p>이러한 발상의 전환은 정규화 과정을 미니 배치로부터 완전히 독립시켰다. 각 데이터 샘플은 다른 샘플의 존재 여부나 미니 배치의 크기에 전혀 영향을 받지 않고 독립적으로 정규화된다.8 이로 인해 배치 정규화가 겪었던 문제들이 자연스럽게 해결되었다.</p>
<h3>3.3  레이어 정규화의 장점 및 단점 요약</h3>
<p>레이어 정규화는 배치 정규화의 한계를 극복하며 다음과 같은 뚜렷한 장점을 제공한다.</p>
<p><strong>장점</strong>:</p>
<ul>
<li><strong>배치 크기 독립성</strong>: 정규화가 개별 샘플 단위로 이루어지므로 미니 배치 크기에 전혀 영향을 받지 않는다. 이는 배치 크기가 1인 온라인 학습(online learning)이나 메모리 제약으로 인해 매우 작은 미니 배치를 사용해야 하는 시나리오에서 매우 효과적이다.8</li>
<li><strong>시퀀스 모델 적용 용이성</strong>: 각 타임스텝의 출력이 독립적으로 정규화될 수 있으므로, 가변 길이의 시퀀스를 다루는 RNN이나 트랜스포머(Transformer)와 같은 모델에 직관적이고 효과적으로 적용할 수 있다. 이는 자연어 처리 분야에서 레이어 정규화가 표준으로 자리 잡게 된 결정적인 이유이다.4</li>
<li><strong>학습 및 추론 과정의 일관성</strong>: 학습 시와 추론 시에 동일한 계산 방식을 사용한다. 미니 배치의 이동 평균을 추적하고 저장할 필요가 없어 구현이 더 간단하며, 추론 시에 추가적인 계산 오버헤드가 발생하지 않는다.8</li>
<li><strong>학습 안정화 효과</strong>: 배치 정규화와 마찬가지로, 내부 공변량 변화를 줄여 학습을 안정시키고, 수렴 속도를 높이며, 일반화 성능을 개선하는 효과를 제공한다.8</li>
</ul>
<p><strong>단점</strong>:</p>
<ul>
<li><strong>계산 오버헤드</strong>: 각 샘플과 각 층마다 평균과 분산을 계산해야 하므로, 배치 정규화에 비해 추가적인 계산 오버헤드가 발생할 수 있다.10</li>
<li><strong>CNN에서의 제한적 성능</strong>: 합성곱 신경망(CNN)과 같은 모델에서는 각 채널(channel)이 독립적이고 특수한 의미(예: 특정 색상, 질감, 패턴 감지)를 갖는 경우가 많다. 레이어 정규화는 이러한 채널들을 구분 없이 모두 함께 정규화하기 때문에, 채널별로 정규화를 수행하는 배치 정규화가 더 적합하고 좋은 성능을 보이는 경향이 있다.24</li>
</ul>
<p>결론적으로, 레이어 정규화는 배치 정규화의 대안으로서, 특히 배치 크기에 제약이 있거나 순차적인 데이터를 다루는 환경에서 그 진가를 발휘하는 강력한 도구이다.</p>
<h2>4.  레이어 정규화의 수학적 원리 및 계산 과정</h2>
<p>레이어 정규화의 핵심은 정규화 통계량(평균과 분산)을 계산하는 단위를 ’배치’에서 ’단일 샘플의 레이어’로 전환하는 것이다. 이 과정은 수학적으로 명확하게 정의되며, 3단계의 계산 절차를 통해 이루어진다.</p>
<h3>4.1  단계별 계산 절차</h3>
<p>특정 <span class="math math-inline">l</span>번째 은닉층에 대한 레이어 정규화 과정을 가정하자. 이 층은 <span class="math math-inline">H</span>개의 뉴런(hidden units)을 가지고 있으며, 활성화 함수에 들어가기 전의 값, 즉 가중치와 입력의 선형 결합 결과인 벡터 <span class="math math-inline">\mathbf{a}^l</span>에 대해 정규화가 수행된다.19</p>
<h4>4.1.1  평균(Mean) 및 분산(Variance) 계산</h4>
<p>첫 번째 단계는 단일 데이터 샘플에 대해, 해당 레이어에 속한 <span class="math math-inline">H</span>개 뉴런의 출력값 <span class="math math-inline">\{a_1^l, a_2^l, \dots, a_H^l\}</span> 전체의 평균 <span class="math math-inline">\mu^l</span>와 분산 <span class="math math-inline">(\sigma^l)^2</span>을 계산하는 것이다.8</p>
<ul>
<li><strong>평균 계산</strong>:<br />
<span class="math math-display">
\mu^l = \frac{1}{H} \sum_{i=1}^{H} a_i^l
</span><br />
<strong>분산 계산</strong>:<br />
<span class="math math-display">
(\sigma^l)^2 = \frac{1}{H} \sum_{i=1}^{H} (a_i^l - \mu^l)^2
</span><br />
이 계산은 미니 배치 내의 다른 샘플과는 무관하게, 오직 현재 처리 중인 단일 샘플의 <span class="math math-inline">l</span>번째 층 내부에서만 이루어진다.</li>
</ul>
<h4>4.1.2  정규화(Normalization)</h4>
<p>두 번째 단계에서는 위에서 계산한 평균과 분산을 사용하여 각 뉴런의 입력값 <span class="math math-inline">a_i^l</span>을 정규화한다. 이 과정은 데이터를 평균 0, 분산 1을 갖는 분포로 변환한다. 분모가 0이 되는 수치적 불안정성을 방지하기 위해 작은 상수 <span class="math math-inline">\epsilon</span>이 분산에 더해진다.8</p>
<ul>
<li><strong>정규화된 값 계산</strong>:<br />
<span class="math math-display">
\hat{a}_i^l = \frac{a_i^l - \mu^l}{\sqrt{(\sigma^l)^2 + \epsilon}}
</span></li>
</ul>
<h4>4.1.3  스케일 및 시프트(Scale and Shift) 변환</h4>
<p>마지막으로, 정규화를 통해 손실될 수 있는 레이어의 표현력을 복원하기 위해 아핀 변환을 적용한다. 이 단계에서는 각 뉴런에 대해 학습 가능한(trainable) 파라미터인 게인(gain, <span class="math math-inline">\gamma</span>)과 바이어스(bias, <span class="math math-inline">\beta</span>)를 도입한다. 이 파라미터들은 모델의 다른 가중치들과 마찬가지로 역전파 과정에서 학습된다.8</p>
<ul>
<li><strong>최종 출력 계산</strong>:<br />
<span class="math math-display">
h_i^l = f(\gamma_i \hat{a}_i^l + \beta_i)
</span><br />
여기서 <span class="math math-inline">h_i^l</span>은 최종적으로 다음 층으로 전달되는 활성화 값이며, <span class="math math-inline">f(\cdot)</span>는 활성화 함수이다. <span class="math math-inline">\gamma</span>와 <span class="math math-inline">\beta</span>는 <span class="math math-inline">H</span>차원의 벡터로, 각 뉴런마다 고유한 스케일과 시프트 값을 적용할 수 있게 한다.</li>
</ul>
<h3>4.2  수식을 통한 상세 해설 및 구현 예시</h3>
<p>트랜스포머 모델에서의 레이어 정규화 적용을 구체적인 예시로 살펴보자. 트랜스포머에서 데이터는 보통 $$ 형태의 3차원 텐서로 표현된다. 레이어 정규화는 이 텐서의 마지막 차원, 즉 <code>Embedding Dimension</code>에 대해 적용된다.</p>
<p>예를 들어, “I am a student“라는 문장이 4개의 토큰으로 분리되고, 각 토큰이 512차원의 임베딩 벡터로 변환되었다고 가정하자. 이 때 입력 텐서의 형태는 $$가 된다. 레이어 정규화는 배치 내의 각 문장, 그리고 문장 내의 각 토큰에 대해 <strong>독립적으로</strong> 수행된다.</p>
<p>한 토큰의 512차원 임베딩 벡터 <span class="math math-inline">\mathbf{a} \in \mathbb{R}^{512}</span>에 대해 레이어 정규화가 적용되는 과정은 다음과 같다:</p>
<ol>
<li><strong>평균 계산</strong>: 512개 차원의 값들의 평균 <span class="math math-inline">\mu</span>를 계산한다.<br />
<span class="math math-display">
\mu = \frac{1}{512} \sum_{i=1}^{512} a_i
</span><br />
<strong>분산 계산</strong>: 512개 차원의 값들의 분산 <span class="math math-inline">\sigma^2</span>을 계산한다.<br />
<span class="math math-display">
\sigma^2 = \frac{1}{512} \sum_{i=1}^{512} (a_i - \mu)^2
</span><br />
<strong>정규화</strong>: 각 차원의 값 <span class="math math-inline">a_i</span>를 정규화한다.<br />
<span class="math math-display">
\hat{a}_i = \frac{a_i - \mu}{\sqrt{\sigma^2 + \epsilon}}
</span><br />
<strong>스케일 및 시프트</strong>: 학습 가능한 512차원 벡터 <span class="math math-inline">\boldsymbol{\gamma}</span>와 <span class="math math-inline">\boldsymbol{\beta}</span>를 사용하여 최종 출력을 계산한다.<br />
<span class="math math-display">
h_i = \gamma_i \hat{a}_i + \beta_i
</span><br />
이 과정은 “I”, “am”, “a”, “student” 각각의 512차원 벡터에 대해 개별적으로, 그리고 배치 내의 모든 문장에 대해 독립적으로 반복된다. 이처럼 레이어 정규화는 정규화의 범위를 명확히 ’샘플 내 특성 전체’로 한정함으로써 배치 의존성을 완전히 제거하고 시퀀스 데이터 처리에 이상적인 특성을 갖게 된다.</li>
</ol>
<h2>5.  주요 정규화 기법 비교 분석</h2>
<p>심층 신경망의 학습을 안정화시키기 위해 제안된 정규화 기법들은 다양하지만, 그 핵심적인 차이는 ’어떤 데이터 집합을 기준으로 평균과 분산을 계산하는가’에 있다. 이 기준, 즉 정규화 축(axis)에 따라 각 기법의 특성과 적합한 적용 분야가 결정된다. 본 장에서는 배치, 레이어, 인스턴스, 그룹 정규화를 중심으로 그 차이점을 시각적으로 이해하고, 체계적으로 비교 분석한다.</p>
<h3>5.1  정규화 축(Axis)에 따른 분류: 시각적 이해</h3>
<p>컴퓨터 비전 분야에서 주로 사용되는 4차원 텐서 $$를 기준으로 각 정규화 기법의 동작 방식을 시각화하면 그 차이를 명확히 이해할 수 있다. 여기서 <span class="math math-inline">N</span>은 배치 크기, <span class="math math-inline">C</span>는 채널 수, <span class="math math-inline">H</span>는 높이, <span class="math math-inline">W</span>는 너비를 의미한다.11</p>
<ul>
<li><strong>배치 정규화 (Batch Normalization)</strong>: 배치 차원(<span class="math math-inline">N</span>)과 공간 차원(<span class="math math-inline">H</span>, <span class="math math-inline">W</span>)에 걸쳐 있는 모든 데이터를 사용하여 각 채널(<span class="math math-inline">C</span>)에 대해 독립적으로 평균과 분산을 계산한다. 즉, 정규화는 $$ 차원에서 이루어지며, 서로 다른 채널은 서로 다른 통계량을 갖는다. 이는 미니 배치 내의 모든 샘플에서 동일한 채널(예: 모든 이미지의 <span class="math math-inline">R</span> 채널)이 유사한 분포를 가질 것이라는 가정에 기반한다.</li>
<li><strong>레이어 정규화 (Layer Normalization)</strong>: 배치 차원(<span class="math math-inline">N</span>)과는 독립적으로, 단일 데이터 샘플 내의 모든 채널(<span class="math math-inline">C</span>)과 공간 차원(<span class="math math-inline">H</span>, <span class="math math-inline">W</span>)에 걸쳐 평균과 분산을 계산한다. 정규화는 $$ 차원에서 이루어지며, 각 샘플은 자신만의 고유한 통계량을 갖는다.</li>
<li><strong>인스턴스 정규화 (Instance Normalization)</strong>: 배치 차원(<span class="math math-inline">N</span>)과 채널 차원(<span class="math math-inline">C</span>) 모두에 독립적으로, 단일 데이터 샘플의 단일 채널 내에서만 평균과 분산을 계산한다. 정규화는 공간 차원(<span class="math math-inline">H</span>, <span class="math math-inline">W</span>)에서만 이루어진다. 이는 각 이미지의 각 채널이 고유한 스타일 정보를 담고 있다는 가정에 기반하며, 주로 스타일 전송(style transfer)과 같은 태스크에 사용된다.25</li>
<li><strong>그룹 정규화 (Group Normalization)</strong>: 레이어 정규화와 인스턴스 정규화의 절충안이다. 채널(<span class="math math-inline">C</span>)을 여러 개의 그룹(<span class="math math-inline">G</span>)으로 나눈 뒤, 각 그룹 내에서 인스턴스 정규화와 유사하게 정규화를 수행한다. 즉, 정규화는 $$ 차원에서 이루어진다. 이는 채널 간에 어느 정도의 연관성이 있다는 가정하에, 배치 크기가 작을 때 배치 정규화의 불안정성을 해결하기 위해 제안되었다.11</li>
</ul>
<h3>5.2  배치, 레이어, 인스턴스, 그룹 정규화 비교</h3>
<p>네 가지 주요 정규화 기법의 핵심적인 차이점을 아래 표를 통해 체계적으로 정리할 수 있다.</p>
<p><strong>Table 1: 주요 정규화 기법 비교</strong></p>
<table><thead><tr><th>특징 (Feature)</th><th>배치 정규화 (Batch Norm)</th><th>레이어 정규화 (Layer Norm)</th><th>인스턴스 정규화 (Instance Norm)</th><th>그룹 정규화 (Group Norm)</th></tr></thead><tbody>
<tr><td><strong>정규화 통계 계산 단위</strong></td><td>미니 배치 내 동일 채널의 모든 픽셀</td><td>단일 샘플의 모든 채널/특성</td><td>단일 샘플의 단일 채널</td><td>단일 샘플의 채널 그룹</td></tr>
<tr><td><strong>정규화 차원 (CNN 기준)</strong></td><td>$$에 걸쳐 평균/분산 계산</td><td>$$에 걸쳐 평균/분산 계산</td><td>$$에 걸쳐 평균/분산 계산</td><td>$$에 걸쳐 평균/분산 계산</td></tr>
<tr><td><strong>배치 크기 의존성</strong></td><td>높음 (High) 12</td><td>없음 (None) 8</td><td>없음 (None) 11</td><td>없음 (None) 11</td></tr>
<tr><td><strong>주요 적용 분야</strong></td><td>CNN, Feed-forward 13</td><td>RNN, Transformers 19</td><td>Style Transfer, GANs 25</td><td>CNN (소규모 배치) 28</td></tr>
<tr><td><strong>학습/추론 동작</strong></td><td>다름 (Different) 8</td><td>동일 (Same) 8</td><td>동일 (Same) 11</td><td>동일 (Same) 27</td></tr>
<tr><td><strong>핵심 아이디어</strong></td><td>샘플 간(inter-sample) 정규화</td><td>샘플 내(intra-sample) 정규화</td><td>샘플 내 채널별 정규화</td><td>샘플 내 채널 그룹별 정규화</td></tr>
</tbody></table>
<h3>5.3. 각 기법의 적합한 적용 분야</h3>
<p>각 정규화 기법은 고유한 특성으로 인해 특정 분야에서 더 나은 성능을 보인다.</p>
<ul>
<li><strong>배치 정규화 (BN)</strong>: 충분히 큰 배치 사이즈를 확보할 수 있는 환경에서 가장 강력한 성능을 보인다. 특히 이미지 분류, 객체 탐지 등 전통적인 컴퓨터 비전 태스크에서 사용되는 CNN 아키텍처의 표준 구성 요소로 널리 사용된다.13</li>
<li><strong>레이어 정규화 (LN)</strong>: 배치 크기가 가변적이거나 매우 작은 경우, 그리고 데이터가 순차적 특성을 갖는 경우에 이상적이다. 이로 인해 RNN, LSTM, 그리고 현대 자연어 처리의 근간을 이루는 트랜스포머 아키텍처에서 거의 필수적으로 사용된다.22</li>
<li><strong>인스턴스 정규화 (IN)</strong>: 이미지의 전반적인 색상, 대비 등 스타일 관련 정보를 제거하고 콘텐츠 자체에 집중해야 하는 태스크에 특화되어 있다. 대표적으로, 한 이미지의 콘텐츠에 다른 이미지의 스타일을 입히는 스타일 전송(style transfer) 분야에서 핵심적인 역할을 한다.25</li>
<li><strong>그룹 정규화 (GN)</strong>: 배치 크기가 작아 배치 정규화의 성능이 저하되는 컴퓨터 비전 태스크에서 효과적인 대안으로 사용된다. GPU 메모리 제약 등으로 인해 큰 배치를 사용하기 어려운 고해상도 이미지 처리나 비디오 분석 등에서 그 유용성이 입증되었다.28</li>
</ul>
<p>이처럼 각 정규화 기법은 저마다의 철학과 장단점을 가지고 있으며, 해결하고자 하는 문제의 특성과 학습 환경을 고려하여 가장 적합한 방법을 선택하는 것이 중요하다.</p>
<h2>6. 트랜스포머 아키텍처와 레이어 정규화</h2>
<p>2017년 “Attention Is All You Need” 논문을 통해 등장한 트랜스포머(Transformer) 아키텍처는 자연어 처리(NLP) 분야에 혁명을 일으켰고, 오늘날 대규모 언어 모델(LLM)의 근간을 이루고 있다. 이 트랜스포머의 성공적인 학습과 안정적인 성능 뒤에는 레이어 정규화가 결정적인 역할을 하고 있다. 본 장에서는 트랜스포머에서 레이어 정규화가 필수적인 이유와 그 위치에 따른 아키텍처 변형, 그리고 최신 동향을 심도 있게 분석한다.</p>
<h3>6.1. 트랜스포머에서 레이어 정규화가 필수적인 이유</h3>
<p>트랜스포머가 배치 정규화 대신 레이어 정규화를 채택한 것은 필연적인 선택이었다. 그 이유는 다음과 같다.</p>
<ol>
<li><strong>가변 길이 시퀀스 처리</strong>: 트랜스포머는 문장이나 문서와 같이 길이가 다양한 시퀀스 데이터를 처리하도록 설계되었다. 미니 배치 내에 서로 다른 길이의 시퀀스가 존재할 경우, 배치 정규화에서처럼 타임스텝별로 일관된 통계량을 계산하기가 매우 어렵다. 반면, 레이어 정규화는 각 시퀀스의 각 토큰에 대해 독립적으로 정규화를 수행하므로 배치 크기나 시퀀스 길이에 영향을 받지 않아 이 문제에 대한 자연스러운 해결책이 된다.30</li>
<li><strong>깊은 네트워크의 학습 안정화</strong>: 트랜스포머는 인코더와 디코더 블록을 수십, 수백 개까지 깊게 쌓는 구조를 가진다. 이렇게 깊은 네트워크에서는 그래디언트가 소실되거나 폭주할 위험이 매우 크다. 레이어 정규화는 각 블록의 입출력 분포를 안정시켜 그래디언트 흐름을 원활하게 하고, 깊은 모델의 학습을 가능하게 하는 핵심적인 장치이다.31</li>
</ol>
<h3>6.2. Post-LN (Post-Layer Normalization): 원본 트랜스포머의 접근법</h3>
<p>원본 트랜스포머 논문에서 제안된 구조는 <strong>Post-LN</strong> 방식으로, 각 서브 레이어(셀프 어텐션 또는 피드포워드 네트워크)의 출력이 잔차 연결(residual connection)과 더해진 <em>후에</em> 레이어 정규화가 적용된다.31 이를 수식으로 표현하면 다음과 같다.<br />
<span class="math math-display">
output = \text{LayerNorm}(x + \text{Sublayer}(x))
</span><br />
이 구조는 직관적이지만, 심각한 학습 불안정성 문제를 야기한다. 이론적 분석에 따르면, Post-LN 구조에서는 층이 깊어질수록 잔차 연결을 통해 전달되는 신호의 크기가 누적되어 상위 레이어의 활성화 값이 기하급수적으로 커질 수 있다.31 이로 인해 학습 초기 단계에서 출력 레이어에 가까운 파라미터들의 그래디언트가 매우 커져 학습이 발산하기 쉽다.31</p>
<p>이러한 불안정성을 해결하기 위해 Post-LN 트랜스포머는 **학습률 워밍업(learning rate warm-up)**이라는 특별한 훈련 기법을 필수적으로 사용해야 한다. 이는 학습 초기에 매우 작은 학습률로 시작하여 수천 스텝에 걸쳐 점진적으로 목표 학습률까지 증가시키는 방식이다.34 워밍업은 큰 그래디언트로 인한 학습 초기의 발산을 막아주지만, 최적의 워밍업 스텝 수와 최대 학습률을 찾아야 하는 추가적인 하이퍼파라미터 튜닝 부담을 안겨주고 전체 학습 시간을 지연시키는 단점이 있다.31</p>
<h3>6.3. Pre-LN (Pre-Layer Normalization): 학습 안정성 개선</h3>
<p>Post-LN의 불안정성 문제를 해결하기 위해 제안된 구조가 <strong>Pre-LN</strong> 방식이다. 이 구조에서는 레이어 정규화가 서브 레이어에 입력이 들어가기 <strong>전에</strong> 적용된다.31 수식은 다음과 같다.<br />
<span class="math math-display">
output = x + \text{Sublayer}(\text{LayerNorm}(x))
</span><br />
Pre-LN 구조에서는 서브 레이어로 들어가는 입력이 항상 정규화되므로, 서브 레이어의 출력 값 크기가 제어된다. 이 제어된 출력이 잔차 연결 <span class="math math-inline">x</span>에 더해지기 때문에, Post-LN에서와 같은 신호의 누적 및 폭주 현상이 발생하지 않는다. 그 결과, Pre-LN 구조는 학습 초기부터 그래디언트가 폭발하거나 소실되지 않고 잘 제어되는(well-behaved) 특성을 보인다.31</p>
<p>이러한 뛰어난 학습 안정성 덕분에 Pre-LN 트랜스포머는 학습률 워밍업 단계를 생략하거나 대폭 축소할 수 있다.31 이는 하이퍼파라미터 튜닝의 복잡성을 줄이고 학습 속도를 크게 향상시키는 실질적인 이점으로 이어진다. BERT, GPT-2, GPT-3를 비롯한 수많은 현대 대규모 언어 모델들이 이 Pre-LN 방식을 채택하여 안정적이고 효율적인 학습을 달성했다.35</p>
<h3>6.4. 최신 동향: Peri-LN 및 기타 변형 구조</h3>
<p>Pre-LN과 Post-LN 사이의 논쟁은 트랜스포머 아키텍처의 최적화 동역학(optimization dynamics)에 대한 깊은 탐구로 이어졌다. 이는 단순히 레이어의 순서를 바꾸는 문제가 아니라, 잔차 연결과 정규화가 상호작용하여 만들어내는 신호 전파와 그래디언트 흐름의 근본적인 특성에 관한 것이다. Post-LN은 신호를 과도하게 증폭시켜 불안정성을 낳고, Pre-LN은 신호를 안정시키는 대신 때로는 과도하게 억제하여 학습 신호를 약화시킬 수 있다는 우려가 제기되었다.</p>
<p>이러한 ’안정성’과 ‘표현력(신호 강도)’ 사이의 트레이드오프를 해결하기 위한 새로운 시도로 **Peri-LN (Peripheral-Layer Normalization)**이 등장했다.37 이 구조는 서브 레이어의 입력과 출력 양쪽에 정규화를 적용하는 방식이다.<br />
<span class="math math-display">
output = x + \text{Norm}(\text{Sublayer}(\text{Norm}(x)))
</span><br />
Peri-LN은 서브 레이어의 입력을 정규화하여 안정성을 확보(Pre-LN의 장점)하는 동시에, 서브 레이어의 출력 또한 정규화하여 잔차 연결과 결합될 때 발생할 수 있는 미세한 불안정성까지 제어하려는 시도이다. 이론적 분석에 따르면, Peri-LN은 Pre-LN의 그래디언트 소실 경향과 Post-LN의 활성화 값 폭주 경향 사이에서 이상적인 균형을 찾아 더 안정적인 학습 동역학을 제공할 수 있다.37 Google의 Gemma, Allen AI의 OLMo와 같은 최신 오픈소스 LLM들이 이 구조를 채택하기 시작했으며, 이는 대규모 모델 학습의 막대한 비용 때문에 이러한 미세한 아키텍처 차이가 실제 성능과 효율에 지대한 영향을 미친다는 것을 방증한다. 이처럼 최적의 정규화 배치 전략에 대한 탐구는 여전히 활발히 진행 중인 연구 분야이다.</p>
<h2>7. 레이어 정규화의 변형 및 최신 연구 동향</h2>
<p>레이어 정규화는 트랜스포머의 성공에 핵심적인 역할을 했지만, 연구자들은 여기서 멈추지 않고 그 효율성과 성능을 더욱 개선하기 위한 다양한 변형 기법들을 제안해왔다. 또한, 레이어 정규화의 작동 원리에 대한 이론적 탐구도 계속해서 이루어지고 있다.</p>
<h3>7.1. RMSNorm (Root Mean Square Layer Normalization): 효율성 증대</h3>
<p>2019년에 제안된 RMSNorm은 레이어 정규화의 계산 효율성을 높이는 데 초점을 맞춘 변형 기법이다.39 연구자들은 레이어 정규화의 두 가지 주요 연산, 즉 평균을 이용한 재중심화(re-centering)와 분산을 이용한 재조정(re-scaling) 중에서, 재중심화 과정이 모델 성능에 미치는 영향이 미미할 수 있다는 가설을 세웠다.</p>
<p>이에 따라 RMSNorm은 평균을 빼는 재중심화 과정을 과감히 생략하고, 오직 재조정 연산만을 수행한다. 이때 표준편차 대신 제곱평균제곱근(Root Mean Square, RMS) 값을 사용한다. 입력 벡터 <span class="math math-inline">\mathbf{a}</span>에 대한 RMSNorm의 계산은 다음과 같다.<br />
<span class="math math-display">
\text{RMS}(\mathbf{a}) = \sqrt{\frac{1}{H} \sum_{i=1}^{H} a_i^2 + \epsilon}
</span></p>
<p><span class="math math-display">
\bar{a}_i = \frac{a_i}{\text{RMS}(\mathbf{a})} \cdot g_i
</span></p>
<p>여기서 <span class="math math-inline">g_i</span>는 학습 가능한 게인(gain) 파라미터이다. 바이어스 <span class="math math-inline">\beta</span>는 사용되지 않는다.</p>
<p>이처럼 계산 과정이 단순화됨에 따라, RMSNorm은 기존 레이어 정규화에 비해 다양한 모델에서 7%에서 64%까지 실행 시간을 단축시키면서도 거의 동등한 성능을 달성했다.[39] 이 결과는 재중심화가 레이어 정규화의 핵심적인 성공 요인이 아닐 수 있음을 시사하며, Llama와 같은 최신 대규모 언어 모델에서 표준으로 채택될 만큼 그 효율성과 성능을 인정받고 있다.</p>
<h3>7.2. 조건부 레이어 정규화(Conditional Layer Normalization, CLN)</h3>
<p>표준 레이어 정규화는 모든 입력 데이터에 대해 동일한 스케일(<span class="math math-inline">\gamma</span>) 및 시프트(<span class="math math-inline">\beta</span>) 파라미터를 적용한다. 그러나 멀티태스크 학습(multi-task learning)이나 특정 조건에 따라 모델의 동작을 다르게 하고 싶은 경우, 이러한 고정된 파라미터는 유연성을 저해할 수 있다.</p>
<p>조건부 레이어 정규화(Conditional Layer Normalization, CLN)는 이러한 문제를 해결하기 위해 제안되었다.[40] CLN은 외부로부터 주어진 조건(예: 태스크 종류를 나타내는 ID, 언어 코드 등)에 따라 <span class="math math-inline">\gamma</span>와 <span class="math math-inline">\beta</span> 파라미터를 동적으로 생성하거나 변형한다. 예를 들어, 태스크 ID <span class="math math-inline">t</span>에 따라 별도의 <span class="math math-inline">\gamma_t</span>와 <span class="math math-inline">\beta_t</span>를 학습하여 기존 파라미터에 더해주는 방식이 있다.<br />
<span class="math math-display">
y = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot (\gamma + \gamma_t) + (\beta + \beta_t)
</span><br />
이러한 방식을 통해 모델은 각 태스크의 특성에 맞게 내부 표현을 미세하게 조정할 수 있게 된다. CLN은 멀티태스크 환경에서 단일 모델이 여러 과제를 더 효과적으로 수행하도록 돕고, 전반적인 성능을 향상시키는 데 기여할 수 있다.[40]</p>
<h3>7.3. 레이어 정규화의 비선형성에 대한 이론적 탐구</h3>
<p>전통적으로 레이어 정규화는 학습을 안정시키는 보조적인 역할로 인식되어 왔다. 그러나 최근 연구에서는 레이어 정규화가 단순히 안정화 장치를 넘어, 네트워크의 표현 능력 자체에 기여하는 능동적인 구성 요소일 수 있다는 이론적 근거를 제시하고 있다.[41]</p>
<p>이 연구에 따르면, 레이어 정규화 연산은 그 자체로 비선형적(non-linear) 특성을 가진다. 입력 벡터의 크기(norm)에 따라 출력의 스케일이 변하기 때문이다. 이러한 비선형성이 선형 변환(예: 완전 연결 레이어)과 결합될 때, 전체 네트워크는 더 복잡한 함수를 근사할 수 있는 표현력을 얻게 된다. 즉, 레이어 정규화는 활성화 함수와 유사하게 네트워크의 표현력을 증가시키는 역할을 할 수 있다는 것이다.[41] 이와 같은 이론적 탐구는 레이어 정규화의 역할을 재조명하고, 향후 새로운 신경망 아키텍처를 설계하는 데 중요한 통찰을 제공할 수 있다.</p>
<h2>8. 결론: 레이어 정규화의 의의와 미래 전망</h2>
<p>레이어 정규화는 심층 신경망, 특히 트랜스포머 아키텍처의 발전에 있어 이정표적인 기여를 한 핵심 기술이다. 배치 정규화의 근본적인 한계를 극복하며 등장한 이 기법은 현대 인공지능 모델, 특히 대규모 언어 모델의 안정적인 학습을 가능하게 한 일등공신이라 할 수 있다.</p>
<h3>8.1. 레이어 정규화의 핵심 기여 요약</h3>
<p>레이어 정규화의 가장 큰 의의는 정규화의 패러다임을 ’배치 의존성’에서 ’배치 독립성’으로 전환시킨 데 있다. 이로 인해 다음과 같은 핵심적인 기여를 이루었다.</p>
<ul>
<li><strong>시퀀스 모델의 학습 혁신</strong>: 배치 정규화가 적용되기 어려웠던 RNN, LSTM과 같은 순환 신경망과 트랜스포머 아키텍처에 효과적인 정규화 수단을 제공했다. 이는 가변 길이 시퀀스 데이터를 안정적으로 처리할 수 있는 길을 열어주었고, 자연어 처리 분야의 폭발적인 성장을 견인했다.</li>
<li><strong>학습 환경의 유연성 확보</strong>: 미니 배치 크기에 구애받지 않으므로, GPU 메모리 제약이 심한 대규모 모델 학습이나 배치 크기가 1인 온라인 학습 등 다양한 환경에서 강건하고 안정적인 성능을 보장했다.</li>
<li><strong>구현의 단순성과 일관성</strong>: 학습과 추론 시에 동일한 연산을 수행하여 모델 구현을 단순화하고, 두 단계 간의 불일치로 인해 발생할 수 있는 잠재적 문제를 원천적으로 차단했다.</li>
</ul>
<h3>8.2. 대규모 언어 모델(LLM) 시대에서의 중요성</h3>
<p>오늘날 ChatGPT, Llama, Gemma 등 세상을 바꾸고 있는 대규모 언어 모델(LLM)들은 거의 예외 없이 트랜스포머 아키텍처에 기반을 두고 있다. 수백, 수천억 개의 파라미터를 가진 이 거대 모델들의 안정적인 학습은 레이어 정규화(및 그 효율적인 변형인 RMSNorm) 없이는 사실상 불가능하다.[42]</p>
<p>특히, Pre-LN과 같은 구조적 개선은 그래디언트 흐름을 안정시켜 학습률 워밍업과 같은 복잡한 튜닝 과정을 간소화했다. 이는 LLM을 학습하는 데 필요한 막대한 양의 계산 자원과 시간을 절감하는 데 실질적으로 기여했으며, LLM 기술의 발전을 가속화하는 중요한 요인이 되었다.</p>
<h3>8.3. 향후 연구 방향 및 과제</h3>
<p>레이어 정규화는 이미 성숙한 기술처럼 보이지만, 그 최적화와 이론적 탐구는 여전히 현재 진행형이다.</p>
<ul>
<li><strong>최적의 아키텍처 탐구</strong>: Post-LN, Pre-LN에 이어 Peri-LN과 같은 새로운 정규화 배치 전략이 등장한 것은, 트랜스포머 내에서 정규화와 다른 구성 요소(특히 잔차 연결) 간의 최적 상호작용을 찾는 연구가 계속되고 있음을 보여준다. 이는 모델의 성능과 학습 효율을 극한까지 끌어올리기 위한 중요한 과제이다.</li>
<li><strong>이론적 이해의 심화</strong>: 레이어 정규화가 최적화 환경에 미치는 영향, 그리고 그 자체의 비선형적 특성이 모델의 표현력에 어떻게 기여하는지에 대한 이론적 분석은 아직 초기 단계에 있다. 이에 대한 더 깊은 이해는 단순히 기존 모델을 개선하는 것을 넘어, 완전히 새로운 개념의 신경망 아키텍처를 설계하는 데 중요한 영감을 제공할 것이다.</li>
<li><strong>효율성과 성능의 조화</strong>: RMSNorm의 성공 사례에서 보듯, 기존 정규화 기법의 핵심적인 요소만을 남겨 계산 효율성을 극대화하려는 노력은 계속될 것이다. 특히 모델의 크기가 계속해서 커지는 추세 속에서, 성능 저하 없이 연산량을 줄이는 새로운 정규화 기법에 대한 수요는 더욱 증가할 것이다.</li>
</ul>
<p>결론적으로, 레이어 정규화는 심층 신경망 학습의 안정성을 확보하는 핵심 도구이자, 현대 AI 모델의 성능을 뒷받침하는 근간 기술이다. 앞으로도 아키텍처, 이론, 효율성 측면에서 지속적인 연구와 발전을 통해 인공지능 기술의 새로운 지평을 열어갈 것으로 기대된다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>RNN(순환신경망), LSTM(장단기메모리) 실습 : 주가분석, 영화리뷰분석 - 갬짱의 개발기록, 8월 26, 2025에 액세스, https://linda284.tistory.com/7</li>
<li>Internal Covariant Shift Problem in Deep Learning - GeeksforGeeks, 8월 26, 2025에 액세스, https://www.geeksforgeeks.org/deep-learning/internal-covariant-shift-problem-in-deep-learning/</li>
<li>Normalization - Gwig’s Research Blog - 티스토리, 8월 26, 2025에 액세스, https://ankle96.tistory.com/55</li>
<li>Layer Normalization vs. Batch Normalization: What’s the Difference? - Coursera, 8월 26, 2025에 액세스, https://www.coursera.org/articles/layer-normalization-vs-batch-normalization</li>
<li>Internal covariate shift - Machine Learning Glossary, 8월 26, 2025에 액세스, https://machinelearning.wtf/terms/internal-covariate-shift/</li>
<li>[1502.03167] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift - arXiv, 8월 26, 2025에 액세스, https://arxiv.org/abs/1502.03167</li>
<li>Understanding Covariate Shift &amp; Normalization | by Michael DSouza - Medium, 8월 26, 2025에 액세스, https://medium.com/@mikedsouza/understanding-covariate-shift-batch-normalization-0b0a4f7ba8d7</li>
<li>Batch vs. Layer Normalization - 언제, 그리고 왜 쓰는 건가요? - velog, 8월 26, 2025에 액세스, <a href="https://velog.io/@jmnsb/Batch-vs.-Layer-Normalization-%EC%96%B8%EC%A0%9C-%EC%93%B0%EB%8A%94-%EA%B1%B4%EA%B0%80%EC%9A%94-RMS-Norm.-Shuffling-BN%EA%B9%8C%EC%A7%80">https://velog.io/@jmnsb/Batch-vs.-Layer-Normalization-%EC%96%B8%EC%A0%9C-%EC%93%B0%EB%8A%94-%EA%B1%B4%EA%B0%80%EC%9A%94-RMS-Norm.-Shuffling-BN%EA%B9%8C%EC%A7%80</a></li>
<li>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift - Proceedings of Machine Learning Research, 8월 26, 2025에 액세스, https://proceedings.mlr.press/v37/ioffe15.html</li>
<li>Batch Normalization VS Layer Normalization - 주홍색 코딩 - 티스토리, 8월 26, 2025에 액세스, https://kwonkai.tistory.com/144</li>
<li>The Different Types of Normalizations in Deep Learning | by DZ - Medium, 8월 26, 2025에 액세스, https://dzdata.medium.com/the-different-types-of-normalizations-in-deep-learning-03eece7fa789</li>
<li>배치 정규화(Batch Normalization) - 코딩일기 - 티스토리, 8월 26, 2025에 액세스, https://daje0601.tistory.com/306</li>
<li>Build Better Deep Learning Models with Batch and Layer Normalization - Pinecone, 8월 26, 2025에 액세스, https://www.pinecone.io/learn/batch-layer-normalization/</li>
<li>Visualizing What Batch Normalization Is and Its Advantages : r/datascience - Reddit, 8월 26, 2025에 액세스, https://www.reddit.com/r/datascience/comments/1aihddg/visualizing_what_batch_normalization_is_and_its/</li>
<li>배치 정규화, 레이어 정규화 - 일단 시작하는 블로그, 8월 26, 2025에 액세스, https://hajinnote.tistory.com/45</li>
<li>Normalization - Dev_Log, 8월 26, 2025에 액세스, https://ooni.oopy.io/927deb13-096b-4531-8bfc-f9f708b6a262</li>
<li>[딥러닝 기본] Deep Learning 학습최적화 개선 - 욱이의 냉철한 공부, 8월 26, 2025에 액세스, https://warm-uk.tistory.com/52</li>
<li>Batch Normalizations - 배치정규화, 8월 26, 2025에 액세스, https://kh-kim.github.io/nlp_with_deep_learning_blog/docs/1-14-regularizations/05-batch_normalization/</li>
<li>Layer Normalization - Department of Computer Science, University …, 8월 26, 2025에 액세스, https://www.cs.utoronto.ca/~hinton/absps/LayerNormalization.pdf</li>
<li>[1607.06450] Layer Normalization - arXiv, 8월 26, 2025에 액세스, https://arxiv.org/abs/1607.06450</li>
<li>[개념정리] Layer Normalization - Innov_AI_te - 티스토리, 8월 26, 2025에 액세스, <a href="https://jaylala.tistory.com/entry/%EA%B0%9C%EB%85%90%EC%A0%95%EB%A6%AC-Layer-Normalization">https://jaylala.tistory.com/entry/%EA%B0%9C%EB%85%90%EC%A0%95%EB%A6%AC-Layer-Normalization</a></li>
<li>Batch Normalization vs Layer Normalization | by Amit Yadav | Biased-Algorithms - Medium, 8월 26, 2025에 액세스, https://medium.com/biased-algorithms/batch-normalization-vs-layer-normalization-c44472883bf2</li>
<li>Batch Normalization vs Layer Normalization - Data Science Stack Exchange, 8월 26, 2025에 액세스, https://datascience.stackexchange.com/questions/126476/batch-normalization-vs-layer-normalization</li>
<li>Mastering Layer Normalization: Enhancing Neural Networks for Optimal Performance, 8월 26, 2025에 액세스, https://www.lunartech.ai/blog/mastering-layer-normalization-enhancing-neural-networks-for-optimal-performance</li>
<li>Normalization Strategies: Batch vs Layer vs Instance vs Group Norm …, 8월 26, 2025에 액세스, https://isaac-the-man.dev/posts/normalization-strategies/</li>
<li>Instance Normalization vs Batch Normalization - GeeksforGeeks, 8월 26, 2025에 액세스, https://www.geeksforgeeks.org/deep-learning/instance-normalization-vs-batch-normalization/</li>
<li>Batch Normalization, Layer Normalization 비교 - Day to_day - 티스토리, 8월 26, 2025에 액세스, https://day-to-day.tistory.com/60</li>
<li>딥러닝 Normalization (Batch Normalization, Layer Normalization, Instance Normalization, Group Normalization 비교) | Luna AI Blog, 8월 26, 2025에 액세스, https://lunaleee.github.io/posts/normalization/</li>
<li>정규화(Normalization) - Jeongwooyeol’s Blog - 티스토리, 8월 26, 2025에 액세스, https://jeongwooyeol0106.tistory.com/26</li>
<li>Layer Normalization in Transformer | by Sachinsoni - Medium, 8월 26, 2025에 액세스, https://medium.com/@sachinsoni600517/layer-normalization-in-transformer-1a2efbff8b85</li>
<li>On Layer Normalization in the Transformer Architecture - arXiv, 8월 26, 2025에 액세스, https://arxiv.org/pdf/2002.04745</li>
<li>Annotated Transformer: LayerNorm Explained - newline, 8월 26, 2025에 액세스, https://www.newline.co/@zaoyang/annotated-transformer-layernorm-explained–a0e93a57</li>
<li>On Layer Normalization in the Transformer Architecture - OpenReview, 8월 26, 2025에 액세스, https://openreview.net/forum?id=B1x8anVFPr</li>
<li>Transformer (deep learning architecture) - Wikipedia, 8월 26, 2025에 액세스, https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)</li>
<li>Normalization of token embeddings in BERT encoder blocks - Stack Overflow, 8월 26, 2025에 액세스, https://stackoverflow.com/questions/79178041/normalization-of-token-embeddings-in-bert-encoder-blocks</li>
<li>What is the role of Layer Normalization in GPT models? - Educative.io, 8월 26, 2025에 액세스, https://www.educative.io/answers/what-is-the-role-of-layer-normalization-in-gpt-models</li>
<li>[2502.02732] Peri-LN: Revisiting Normalization Layer in the Transformer Architecture - arXiv, 8월 26, 2025에 액세스, https://arxiv.org/abs/2502.02732</li>
<li>Peri-LN: Revisiting Layer Normalization in the Transformer Architecture - arXiv, 8월 26, 2025에 액세스, https://arxiv.org/html/2502.02732v1</li>
<li>[1910.07467] Root Mean Square Layer Normalization - arXiv, 8월 26, 2025에 액세스, https://arxiv.org/abs/1910.07467</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>