<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:인스턴스 정규화 (Instance Normalization, 2016)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>인스턴스 정규화 (Instance Normalization, 2016)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">정규화 (Regularizations)</a> / <span>인스턴스 정규화 (Instance Normalization, 2016)</span></nav>
                </div>
            </header>
            <article>
                <h1>인스턴스 정규화 (Instance Normalization, 2016)</h1>
<h2>1.  개념과 동기</h2>
<h3>1.1  딥러닝에서의 정규화 필요성: 내부 공변량 이동(Internal Covariate Shift)</h3>
<p>심층 신경망(Deep Neural Networks)의 학습 과정은 본질적으로 불안정성을 내포한다. 각 층의 파라미터가 업데이트됨에 따라, 후속 층으로 전달되는 입력 데이터의 분포가 계속해서 변화하는 현상이 발생하는데, 이를 내부 공변량 이동(Internal Covariate Shift, ICS)이라 칭한다.1 이러한 분포의 불안정성은 모델이 특정 분포에 적응하기 어렵게 만들어 학습 속도를 현저히 저하시키고, 가중치 초기값 설정에 매우 민감하게 반응하는 문제를 야기한다.1</p>
<p>이러한 근본적인 문제를 해결하기 위한 선구적인 방법론으로 배치 정규화(Batch Normalization, BN)가 제안되었다.1 BN은 학습 데이터의 미니배치(mini-batch) 단위로 각 피처 채널의 평균과 분산을 계산하고, 이를 이용해 해당 층의 입력을 정규화한다. 이 과정을 통해 활성화 값의 분포를 일정하게 유지함으로써 내부 공변량 이동을 효과적으로 완화하고, 결과적으로 학습 과정을 안정시키며 수렴 속도를 가속하는 데 크게 기여했다. BN은 심층 신경망, 특히 ResNet과 같은 깊은 컨볼루션 신경망(CNN)의 성공적인 학습을 가능하게 한 핵심 기술 중 하나로 자리매김했다.</p>
<h3>1.2  인스턴스 정규화의 탄생: 빠른 스타일 전이를 위한 핵심 요소(The Missing Ingredient)</h3>
<p>인스턴스 정규화(Instance Normalization, IN)는 내부 공변량 이동이라는 일반적인 문제 해결을 넘어, ’빠른 스타일 전이(Fast Stylization)’라는 특정 생성 과업의 성능을 극대화하려는 과정에서 발견되었다. Dmitry Ulyanov 등의 2016년 논문 “Instance Normalization: The Missing Ingredient for Fast Stylization“은 IN의 등장을 알리는 중요한 이정표가 되었다.4</p>
<p>해당 연구에서는 기존의 빠른 스타일 전이 생성 모델 아키텍처에서 배치 정규화(BN) 레이어를 인스턴스 정규화(IN) 레이어로 교체하는 매우 단순한 변경만으로 생성되는 이미지의 시각적 품질이 극적으로 향상되는 현상을 발견했다.4 이는 IN이 단순히 BN의 또 다른 변종이 아니라, 이미지의 ’스타일’이라는 고유한 속성을 다루는 생성 모델링 분야에서 본질적으로 중요한 역할을 수행하는 핵심 구성 요소(missing ingredient)임을 시사했다. 이 발견은 정규화 기법이 단순히 학습 안정화를 위한 도구를 넘어, 특정 과업의 목적에 맞게 정보의 흐름을 제어하는 중요한 역할을 할 수 있음을 보여주었다. 즉, 일반적인 문제 해결을 목표로 한 BN과 달리, IN은 특정 응용 분야의 필요가 새로운 일반화 가능한 기술의 발견으로 이어질 수 있음을 보여주는 중요한 사례가 되었다.</p>
<h3>1.3  핵심 아이디어: 인스턴스별 통계량을 이용한 스타일 정보 제거</h3>
<p>인스턴스 정규화의 핵심적인 직관은 스타일 전이의 결과물이 원본 콘텐츠 이미지의 고유한 시각적 특성, 특히 명암(contrast)과 같은 스타일에 의존해서는 안 된다는 가정에서 출발한다.5 생성 모델은 콘텐츠의 구조적 정보는 보존하되, 기존의 스타일은 제거하고 새로운 스타일을 입힐 수 있어야 한다.</p>
<p>이를 구현하기 위해 IN은 각 데이터 샘플(인스턴스)과 각 피처 채널에 대해 독립적으로 평균과 분산을 계산하여 정규화를 수행한다. 이 연산은 미니배치 내의 다른 이미지로부터 어떠한 영향도 받지 않기 때문에, 각 이미지 고유의 명암 대비, 색상 분포와 같은 ’인스턴스별 스타일 정보’를 효과적으로 제거하는 역할을 한다.5 이 과정은 마치 이미지의 스타일을 ’표백’하여 콘텐츠의 골격만 남기는 것과 같다.</p>
<p>이처럼 스타일 정보를 의도적으로 폐기함으로써, 생성 네트워크는 새로운 스타일을 입히는 본연의 작업에 더욱 집중할 수 있게 되어 학습 과정이 단순화되고 용이해진다.5 이러한 접근 방식은 정규화가 단순히 데이터 분포를 안정시키는 역할을 넘어, 네트워크가 학습해야 할 정보의 종류를 능동적으로 제어하는 ‘정보 게이트(information gate)’ 역할을 할 수 있다는 새로운 패러다임을 제시했다. BN이 배치의 전반적인 통계 정보를 보존하여 일반화 성능을 높이는 데 초점을 맞춘다면, IN은 의도적으로 각 인스턴스의 개별 스타일 정보를 버림으로써 생성 과업의 유연성을 극대화하는 방향으로 작동한다.</p>
<h2>2.  인스턴스 정규화의 수학적 원리</h2>
<h3>2.1  입력 텐서의 구조와 정규화 차원 정의</h3>
<p>컨볼루션 신경망(CNN)에서 처리되는 데이터는 일반적으로 4차원 텐서(tensor)의 형태를 가진다. 입력 텐서 <span class="math math-inline">x</span>는 <span class="math math-inline">(N, C, H, W)</span>의 차원으로 정의되며, 각 차원은 다음을 의미한다 7:</p>
<ul>
<li><span class="math math-inline">N</span>: 배치 크기(Batch size), 즉 한 번의 연산에 사용되는 데이터 샘플의 수.</li>
<li><span class="math math-inline">C</span>: 채널 수(Number of channels), 예를 들어 RGB 이미지의 경우 3개의 채널을 가진다.</li>
<li><span class="math math-inline">H</span>: 피처맵의 높이(Height).</li>
<li><span class="math math-inline">W</span>: 피처맵의 너비(Width).</li>
</ul>
<p>인스턴스 정규화는 이 4차원 텐서 내에서 특정 차원들을 기준으로 통계량을 계산하여 정규화를 수행한다.</p>
<h3>2.2  평균 및 분산 계산: 채널 및 인스턴스 단위 연산</h3>
<p>인스턴스 정규화의 가장 큰 특징은 각 배치 샘플(<span class="math math-inline">n \in \{1,..., N\}</span>)과 각 채널(<span class="math math-inline">c \in \{1,..., C\}</span>)에 대해 통계량을 독립적으로 계산한다는 점이다. 즉, 평균과 분산은 오직 공간 차원(<span class="math math-inline">H, W</span>)에 대해서만 계산된다.5</p>
<p>배치 내 <span class="math math-inline">n</span>번째 샘플의 <span class="math math-inline">c</span>번째 채널에 대한 평균 <span class="math math-inline">\mu_{nc}</span>는 해당 채널의 모든 픽셀 값의 합을 픽셀의 총 개수(<span class="math math-inline">H \times W</span>)로 나누어 계산한다.<br />
<span class="math math-display">
\mu_{nc}(x) = \frac{1}{HW} \sum_{h=1}^{H} \sum_{w=1}^{W} x_{nchw}
</span><br />
5</p>
<p>마찬가지로, 분산 <span class="math math-inline">\sigma_{nc}^2</span>는 해당 채널의 각 픽셀 값에서 위에서 계산한 평균 <span class="math math-inline">\mu_{nc}</span>를 뺀 값의 제곱의 평균으로 계산된다.<br />
<span class="math math-display">
\sigma_{nc}^2(x) = \frac{1}{HW} \sum_{h=1}^{H} \sum_{w=1}^{W} (x_{nchw} - \mu_{nc}(x))^2
</span><br />
5</p>
<p>이러한 계산 방식은 각 채널 내에서 공간적으로(<span class="math math-inline">H, W</span> 축) 통계량이 의미 있다는 암묵적인 가정을 내포한다. 즉, 한 채널의 피처맵 내 모든 픽셀들은 동일한 ‘스타일’ 분포를 공유한다고 가정하는 것이다. 이는 텍스처나 색감과 같은 저수준(low-level) 스타일 정보에는 효과적일 수 있으나, 객체의 서로 다른 부분을 나타내는 고수준(high-level) 의미 정보가 한 채널에 섞여 있을 경우, 이들을 함께 평균 내는 과정에서 중요한 정보를 파괴할 위험이 있다. 이러한 한계는 이후 SPADE나 Attentive Normalization과 같이 공간적으로 변화하거나 의미론적으로 분할된 영역에 따라 정규화를 수행하는 더 정교한 기법들의 등장을 촉발했다.11</p>
<h3>2.3  정규화 수식과 학습 가능한 아핀 변환(Affine Transformation)</h3>
<p>계산된 평균과 분산을 사용하여 각 픽셀 값 <span class="math math-inline">x_{nchw}</span>는 다음과 같이 정규화된다. 이 과정에서 분모가 0이 되는 것을 방지하기 위해 아주 작은 양수 <span class="math math-inline">\epsilon</span>을 더해준다.<br />
<span class="math math-display">
\hat{x}_{nchw} = \frac{x_{nchw} - \mu_{nc}(x)}{\sqrt{\sigma_{nc}^2(x) + \epsilon}}
</span><br />
정규화 과정은 데이터의 분포를 평균 0, 분산 1로 표준화하지만, 이로 인해 모델의 표현력이 제한될 수 있다. 이를 보완하기 위해, 학습 가능한(learnable) 아핀 변환 파라미터인 스케일(scale) <span class="math math-inline">\gamma_c</span>와 이동(shift) <span class="math math-inline">\beta_c</span>를 도입한다. 이 파라미터들은 채널별로 존재하며, 역전파를 통해 학습된다.9 최종적인 인스턴스 정규화 연산은 다음과 같이 정의된다.<br />
<span class="math math-display">
\text{IN}(x_{nchw}) = \gamma_c \hat{x}_{nchw} + \beta_c = \gamma_c \left( \frac{x_{nchw} - \mu_{nc}(x)}{\sqrt{\sigma_{nc}^2(x) + \epsilon}} \right) + \beta_c
</span><br />
5</p>
<p>PyTorch와 같은 딥러닝 프레임워크에서는 <code>affine=True</code> 옵션을 통해 이 아핀 변환의 사용 여부를 결정할 수 있다.14</p>
<p>배치 정규화와는 달리, 인스턴스 정규화는 훈련(training) 시와 추론(inference) 시의 동작이 기본적으로 동일하다.6 BN은 추론 시에 훈련 데이터셋 전체의 통계량을 근사하기 위해 이동 평균(moving average)으로 계산된 값을 사용해야 하는 복잡성이 있지만, IN은 항상 현재 입력된 인스턴스 자체에서 통계량을 직접 계산하므로 이러한 불일치가 발생하지 않는다. 이 특성은 IN이 온라인 학습이나 단일 이미지 처리와 같은 실시간 응용 분야에 더 자연스럽고 간편하게 적용될 수 있음을 의미한다.</p>
<h2>3.  정규화 기법 비교 분석: IN vs. BN, LN, GN</h2>
<p>인스턴스 정규화의 특징을 명확히 이해하기 위해서는 다른 주요 정규화 기법들과의 비교가 필수적이다. 각 기법은 정규화를 수행하는 차원과 단위가 다르며, 이는 각자의 장단점과 주요 적용 분야를 결정한다.</p>
<h3>3.1  배치 정규화(Batch Normalization, BN)와의 비교</h3>
<ul>
<li><strong>연산 단위:</strong> 가장 큰 차이점은 통계량을 계산하는 범위에 있다. BN은 각 채널(<span class="math math-inline">c</span>)에 대해 배치 차원(<span class="math math-inline">N</span>)과 공간 차원(<span class="math math-inline">H, W</span>)을 모두 포함하여 평균과 분산을 계산한다. 즉, 미니배치 내의 모든 샘플들이 특정 채널의 통계량 계산에 함께 사용된다. 반면, IN은 각 샘플(<span class="math math-inline">n</span>)과 각 채널(<span class="math math-inline">c</span>)의 쌍에 대해 독립적으로, 오직 공간 차원(<span class="math math-inline">H, W</span>)만을 사용하여 통계량을 계산한다.7</li>
<li><strong>배치 의존성:</strong> 이러한 연산 단위의 차이는 배치 크기에 대한 의존성으로 직결된다. BN의 성능은 배치 크기에 크게 의존하며, 일반적으로 배치 크기가 클수록 전체 데이터셋의 통계량을 더 정확하게 추정하여 안정적인 성능을 보인다. 배치 크기가 매우 작을 경우(예: 2 또는 4), 통계량 추정이 불안정해져 성능이 저하될 수 있다. 반면, IN은 각 샘플을 개별적으로 처리하므로 배치 크기에 전혀 의존하지 않으며, 배치 크기가 1인 경우에도 일관된 성능을 유지한다.16</li>
<li><strong>효과 및 적용 분야:</strong> BN은 배치 내 샘플들 간의 상호작용을 통해 학습을 안정화하고, 일종의 규제(regularization) 효과를 제공하여 모델의 일반화 성능을 높인다. 이 때문에 주로 이미지 분류, 객체 탐지와 같은 판별 모델(discriminative models)에서 표준적으로 사용된다.1 IN은 개별 샘플의 스타일 정보를 효과적으로 제거하는 특성 때문에, 이미지의 시각적 스타일을 변환하거나 생성하는 스타일 전이, 이미지-이미지 변환과 같은 생성 모델(generative models)에서 탁월한 성능을 보인다.16</li>
</ul>
<h3>3.2  계층 정규화(Layer Normalization, LN)와의 비교</h3>
<ul>
<li><strong>연산 단위:</strong> LN은 각 샘플(<span class="math math-inline">n</span>)에 대해, 모든 채널(<span class="math math-inline">C</span>)과 공간 차원(<span class="math math-inline">H, W</span>)을 아우르는 단일 평균과 분산을 계산한다. 즉, 정규화가 샘플 단위로 이루어지며, 한 샘플 내의 모든 뉴런(또는 피처)들이 동일한 통계량으로 정규화된다.10</li>
<li><strong>차이점:</strong> IN이 각 채널을 독립적인 단위로 보고 채널별 스타일을 정규화하는 반면, LN은 모든 채널을 하나의 단위로 묶어 정규화한다. 이로 인해 LN은 채널 간의 상대적인 크기 관계를 변화시키는 반면, IN은 이를 유지한다.</li>
<li><strong>적용 분야:</strong> LN은 배치 크기에 독립적이며 시퀀스 데이터의 길이에 가변적인 특성 때문에 순환 신경망(RNN)이나 트랜스포머(Transformer)와 같은 자연어 처리(NLP) 모델에서 널리 사용된다.19 IN은 주로 CNN 기반의 이미지 생성 과업에 특화되어 있다.</li>
</ul>
<h3>3.3  그룹 정규화(Group Normalization, GN)와의 관계</h3>
<ul>
<li><strong>개념:</strong> GN은 LN과 IN의 장점을 절충하기 위해 제안된 기법이다. GN은 전체 채널(<span class="math math-inline">C</span>)을 사전에 정의된 개수(<span class="math math-inline">G</span>)의 그룹으로 나눈다. 그리고 각 샘플(<span class="math math-inline">n</span>) 내에서, 각 채널 그룹별로 공간 차원(<span class="math math-inline">H, W</span>)에 대한 통계량을 계산한다.10</li>
<li><strong>통합적 관점:</strong> GN은 정규화 기법들을 하나의 통합된 프레임워크로 이해할 수 있는 관점을 제공한다. 만약 그룹의 수 <span class="math math-inline">G</span>가 1이라면, 모든 채널이 하나의 그룹에 속하게 되므로 LN과 동일하게 동작한다. 반대로, 그룹의 수 <span class="math math-inline">G</span>가 전체 채널 수 <span class="math math-inline">C</span>와 같다면, 각 채널이 개별적인 그룹이 되므로 IN과 동일하게 동작한다.22 이 관계는 BN을 제외한 정규화 기법들이 정규화 축을 어떻게 설정하는지에 대한 연속적인 스펙트럼 상에 위치함을 보여준다.</li>
</ul>
<h3>3.4  종합 비교표</h3>
<p>아래 표는 네 가지 주요 정규화 기법의 핵심적인 차이점을 요약하여 보여준다.</p>
<table><thead><tr><th>특징 (Feature)</th><th>배치 정규화 (Batch Normalization)</th><th>계층 정규화 (Layer Normalization)</th><th>인스턴스 정규화 (Instance Normalization)</th><th>그룹 정규화 (Group Normalization)</th></tr></thead><tbody>
<tr><td><strong>정규화 축</strong></td><td>(N, H, W)</td><td>(C, H, W)</td><td>(H, W)</td><td>(C/G, H, W)</td></tr>
<tr><td><strong>평균/분산 계산 단위</strong></td><td>채널별 (Per-channel)</td><td>샘플별 (Per-sample)</td><td>채널 &amp; 샘플별 (Per-channel, per-sample)</td><td>그룹 &amp; 샘플별 (Per-group, per-sample)</td></tr>
<tr><td><strong>배치 크기 의존성</strong></td><td>높음 (High)</td><td>없음 (None)</td><td>없음 (None)</td><td>없음 (None)</td></tr>
<tr><td><strong>핵심 특징</strong></td><td>학습 안정화, 빠른 수렴</td><td>배치 크기 무관, RNN/Transformer에 적합</td><td>인스턴스별 스타일 정보 제거</td><td>LN과 IN의 절충안, 작은 배치에서도 안정적</td></tr>
<tr><td><strong>주요 적용 분야</strong></td><td>이미지 분류, 객체 탐지</td><td>RNN, NLP, Transformer</td><td>스타일 전이, 이미지 생성</td><td>객체 탐지, 분할 (작은 배치)</td></tr>
<tr><td><strong>평균 계산 수식</strong></td><td><span class="math math-inline">\mu_c = \frac{1}{NHW}\sum_{n,h,w} x_{nchw}</span></td><td><span class="math math-inline">\mu_n = \frac{1}{CHW}\sum_{c,h,w} x_{nchw}</span></td><td><span class="math math-inline">\mu_{nc} = \frac{1}{HW}\sum_{h,w} x_{nchw}</span></td><td><span class="math math-inline">\mu_{ng} = \frac{1}{\vert C_g \vert HW}\sum_{c \in C_g, h,w} x_{nchw}</span></td></tr>
</tbody></table>
<p>이러한 비교 분석을 통해, 정규화 기법의 선택이 근본적으로 ’인스턴스 간의 변동성(inter-instance variation)’을 어떻게 다룰 것인가의 문제와 직결됨을 알 수 있다. BN은 이 변동성을 학습에 유용한 정보(규제 효과)로 활용하는 반면, IN은 이를 ’스타일’이라는 노이즈로 간주하고 제거한다.6 특정 과업에서 스타일 정보가 유용한 신호인지(예: 날씨 분류에서 이미지의 전반적인 밝기) 아니면 방해 요소인지(예: 스타일 전이)에 따라 최적의 정규화 기법이 달라진다. Batch-Instance Normalization(BIN)과 같은 후속 연구는 바로 이 트레이드오프를 학습하려는 시도에서 출발했다.7</p>
<p>더 나아가, 정규화 기법의 발전은 딥러닝 아키텍처의 발전과 밀접한 상호 관계를 맺으며 진화해왔다. BN이 ResNet과 같은 깊은 CNN의 학습을 가능하게 했고, LN이 트랜스포머의 성공에 결정적인 역할을 했듯이, IN과 그 변종들은 StyleGAN과 같은 고품질 이미지 생성 모델의 핵심 구성 요소가 되었다. 이는 정규화가 단순히 기존 아키텍처의 성능을 개선하는 보조 도구를 넘어, 새로운 아키텍처의 설계와 성공을 가능하게 하는 핵심적인 ’인에이블러(enabler)’임을 명확히 보여준다.</p>
<h2>4.  스타일 전이(Style Transfer)의 핵심 메커니즘</h2>
<p>인스턴스 정규화는 스타일 전이 분야에서 그 진가를 발휘하며 널리 알려졌다. IN과 이를 기반으로 한 적응형 인스턴스 정규화(AdaIN)는 실시간으로 고품질의 스타일 전이를 가능하게 하는 혁신적인 메커니즘을 제공했다.</p>
<h3>4.1  IN을 통한 콘텐츠와 스타일의 분리</h3>
<p>Gatys 등의 선구적인 연구를 통해, CNN이 이미지를 처리할 때 깊은 층의 피처맵은 이미지의 ‘콘텐츠(content)’ 즉, 공간적 구조와 객체의 형태를 인코딩하고, 피처맵의 통계량(Gram matrix와 같은 2차 통계량)은 이미지의 ‘스타일(style)’ 즉, 텍스처, 색감, 붓 터치 등을 인코딩한다는 사실이 밝혀졌다.9</p>
<p>인스턴스 정규화는 바로 이 원리에 기반하여 작동한다. IN은 각 피처맵의 평균(1차 통계량)과 분산(2차 통계량의 일부)을 정규화함으로써 스타일과 관련된 정보를 효과적으로 제거한다. 이 과정은 콘텐츠의 핵심적인 공간 구조는 그대로 유지하면서, 원본 이미지의 고유한 스타일만을 ’표백(whitening)’하는 효과를 가져온다.7 이렇게 스타일이 제거된 피처맵은 새로운 스타일을 입히기 위한 깨끗한 캔버스 역할을 하게 된다.</p>
<h3>4.2  적응형 인스턴스 정규화(Adaptive Instance Normalization, AdaIN)</h3>
<h4>4.2.1  개념</h4>
<p>AdaIN은 IN의 개념을 한 단계 발전시킨 기법이다. 단순히 콘텐츠 이미지의 스타일을 제거하는 데 그치지 않고, 그 자리에 목표하는 스타일 이미지의 통계량(평균과 분산)을 직접 주입한다.9 즉, 콘텐츠 피처맵을 정규화하여 스타일을 제거한 뒤, 스타일 피처맵의 평균과 분산을 사용하여 역정규화(denormalization)함으로써 콘텐츠의 구조에 새로운 스타일을 입히는 것이다.</p>
<h4>4.2.2  AdaIN의 수학적 정의와 작동 방식</h4>
<p>콘텐츠 이미지의 피처맵을 <span class="math math-inline">x</span>, 스타일 이미지의 피처맵을 <span class="math math-inline">y</span>라고 할 때, AdaIN 연산은 다음과 같이 정의된다.<br />
<span class="math math-display">
\text{AdaIN}(x, y) = \sigma(y) \left( \frac{x - \mu(x)}{\sigma(x)} \right) + \mu(y)
</span><br />
여기서 <span class="math math-inline">\mu(x)</span>와 <span class="math math-inline">\sigma(x)</span>는 콘텐츠 피처맵 <span class="math math-inline">x</span>의 채널별 평균과 표준편차이며, <span class="math math-inline">\mu(y)</span>와 <span class="math math-inline">\sigma(y)</span>는 스타일 피처맵 <span class="math math-inline">y</span>의 채널별 평균과 표준편차다.9</p>
<p>이 수식은 두 단계로 나누어 이해할 수 있다.</p>
<ol>
<li><strong>콘텐츠 스타일 제거:</strong> 괄호 안의 <span class="math math-inline">\frac{x - \mu(x)}{\sigma(x)}</span> 부분은 표준적인 인스턴스 정규화 연산으로, 콘텐츠 피처맵 <span class="math math-inline">x</span>에서 고유의 스타일(평균과 분산)을 제거한다.</li>
<li><strong>새로운 스타일 주입:</strong> 제거된 자리에 스타일 피처맵 <span class="math math-inline">y</span>의 표준편차 <span class="math math-inline">\sigma(y)</span>를 곱하고 평균 <span class="math math-inline">\mu(y)</span>를 더함으로써, <span class="math math-inline">y</span>의 스타일을 <span class="math math-inline">x</span>의 구조에 입힌다.</li>
</ol>
<p>주목할 점은 AdaIN에는 IN과 달리 학습 가능한 아핀 변환 파라미터 <span class="math math-inline">\gamma</span>와 <span class="math math-inline">\beta</span>가 없다는 것이다. 대신, 스타일 피처맵의 통계량인 <span class="math math-inline">\sigma(y)</span>와 <span class="math math-inline">\mu(y)</span>가 동적으로 <span class="math math-inline">\gamma</span>와 <span class="math math-inline">\beta</span>의 역할을 수행한다. 이는 스타일 정보가 네트워크의 고정된 가중치에 저장되는 것이 아니라, 외부에서 제공되는 스타일 이미지로부터 ‘적응적으로(adaptively)’ 계산되어 주입됨을 의미한다.9 이 아이디어는 스타일 정보를 네트워크 파라미터로부터 분리(decouple)하는 혁신적인 발상이었다. 기존의 빠른 스타일 전이 모델은 특정 스타일을 네트워크 가중치 자체에 인코딩해야 했기 때문에 하나의 모델이 하나의 스타일만 생성할 수 있었지만, AdaIN은 네트워크가 ’스타일을 이미지로 렌더링하는 일반적인 방법’만 학습하게 함으로써 모델의 유연성과 확장성을 극적으로 향상시켰다.</p>
<h4>4.2.3  임의 스타일 전이(Arbitrary Style Transfer)의 실시간 구현</h4>
<p>AdaIN의 가장 큰 기여는 사전 정의된 한정된 스타일에 얽매이지 않고, 어떤 스타일 이미지든 실시간으로 적용할 수 있는 ’임의 스타일 전이’를 가능하게 했다는 점이다.9 이는 다음과 같은 Encoder-AdaIN-Decoder 구조를 통해 구현된다.</p>
<ol>
<li><strong>인코더(Encoder):</strong> 사전 학습된 VGG-19와 같은 이미지 분류 모델의 초기 레이어들을 인코더로 사용한다. 콘텐츠 이미지와 스타일 이미지를 각각 인코더에 통과시켜 깊은 피처맵을 추출한다.</li>
<li><strong>AdaIN 레이어:</strong> 추출된 두 피처맵을 AdaIN 레이어에 입력하여, 콘텐츠 피처맵의 통계량을 스타일 피처맵의 통계량으로 교체한다. 이를 통해 스타일이 변환된 목표 피처맵 <span class="math math-inline">t</span>를 생성한다.</li>
<li><strong>디코더(Decoder):</strong> 랜덤하게 초기화된 디코더 네트워크는 목표 피처맵 <span class="math math-inline">t</span>를 입력받아 최종적인 스타일 전이 이미지를 생성하도록 학습된다. 디코더는 인코더와 대칭적인 구조를 가지는 경우가 많다.</li>
</ol>
<p>이 구조는 반복적인 최적화 과정이 필요한 기존 방식에 비해 수백 배에서 수천 배 빠른 속도를 자랑하며, 실시간 비디오 스타일 전이와 같은 응용을 가능하게 했다.26</p>
<h3>4.3  조건부 인스턴스 정규화(Conditional Instance Normalization, CIN)</h3>
<p>AdaIN이 임의의 연속적인 스타일을 다루는 데 초점을 맞춘 반면, 조건부 인스턴스 정규화(CIN)는 여러 개의 고정된(discrete) 스타일을 하나의 네트워크에서 효율적으로 처리하기 위해 제안되었다.29 CIN은 각 스타일에 대해 별도의 아핀 변환 파라미터 쌍(<span class="math math-inline">\gamma_s, \beta_s</span>)을 학습한다. 그리고 스타일 전이 시, 원하는 스타일에 해당하는 파라미터 쌍을 선택하여 표준 IN 연산 후에 적용한다.<br />
<span class="math math-display">
\text{CIN}(x, s) = \gamma_s \left( \frac{x - \mu(x)}{\sqrt{\sigma^2(x) + \epsilon}} \right) + \beta_s
</span><br />
이를 통해 단일 네트워크가 수십 개의 다른 스타일을 학습하고, 스타일 간의 보간(interpolation)까지 가능하게 만들어 다중 스타일 전이 모델을 효율적으로 구현할 수 있게 했다.29</p>
<h2>5.  인스턴스 정규화의 확장과 변주</h2>
<p>인스턴스 정규화의 성공은 ’조건부 정규화(conditional normalization)’라는 새로운 연구 방향을 열었다. 이는 정규화 과정을 외부 정보에 따라 동적으로 변조함으로써, 네트워크가 더 풍부하고 제어 가능한 표현을 학습하도록 하는 기법들의 등장을 촉발했다.</p>
<h3>5.1  배치-인스턴스 정규화(Batch-Instance Normalization, BIN)</h3>
<ul>
<li>
<p><strong>개념:</strong> BIN은 BN이 가진 스타일 보존 특성과 IN이 가진 스타일 제거 특성을 하나의 모듈 안에서 결합하려는 시도다. 이는 특정 과업에서 스타일 정보가 유용할 수도, 혹은 방해가 될 수도 있다는 문제의식에서 출발했다.7</p>
</li>
<li>
<p><strong>수식 및 작동 방식:</strong> BIN은 BN의 출력과 IN의 출력을 학습 가능한 파라미터 <span class="math math-inline">\rho</span>를 이용해 가중합한다.<br />
<span class="math math-display">
y = (\rho \cdot \text{BN}(x) + (1 - \rho) \cdot \text{IN}(x)) \cdot \gamma + \beta
</span><br />
여기서 <span class="math math-inline">\rho</span>는 채널별로 학습되는 스칼라 값으로, 0과 1 사이의 값을 갖는 게이트(gate) 역할을 한다.7 만약 특정 채널의 스타일 정보가 과업 수행에 중요하다면, 모델은 <span class="math math-inline">\rho</span>를 1에 가깝게 학습하여 BN의 결과를 주로 사용하게 된다. 반대로 스타일 정보가 불필요하거나 방해가 된다면, <span class="math math-inline">\rho</span>를 0에 가깝게 학습하여 IN을 통해 스타일을 제거한다. 이처럼 BIN은 데이터와 과업에 맞춰 각 피처 채널에 대한 최적의 정규화 전략을 적응적으로 학습한다.7</p>
</li>
</ul>
<h3>5.2  공간 적응형 정규화(Spatially-Adaptive Normalization, SPADE)</h3>
<ul>
<li>
<p><strong>개념:</strong> SPADE는 의미론적 분할 맵(semantic segmentation map)과 같은 공간적 정보를 활용하여 정규화를 수행하는 혁신적인 방법이다. 기존의 정규화 기법들이 의미론적 레이아웃 정보를 ‘씻어내는(wash away)’ 문제를 해결하기 위해 제안되었다.12</p>
</li>
<li>
<p><strong>작동 방식:</strong> SPADE는 먼저 표준적인 정규화(예: BN)를 통해 피처맵의 스타일 정보를 제거한다. 그 후, 입력으로 주어진 의미론적 분할 맵을 두 개의 작은 컨볼루션 네트워크에 통과시켜 공간적으로 변화하는(spatially-varying) 아핀 변환 파라미터 <span class="math math-inline">\gamma(m)</span>와 <span class="math math-inline">\beta(m)</span>를 생성한다. 이 파라미터들은 스칼라나 벡터가 아닌, 원본 피처맵과 동일한 공간적 해상도를 갖는 텐서다. 최종적으로 이 텐서들을 정규화된 활성화 값에 원소별로(element-wise) 곱하고 더한다.<br />
<span class="math math-display">
y = \gamma(m) \odot \frac{x - \mu(x)}{\sigma(x)} + \beta(m)
</span><br />
이 방식을 통해, 특정 픽셀이 어떤 의미론적 클래스(예: 하늘, 나무, 건물)에 속하는지에 따라 다른 방식으로 정규화가 변조된다. 결과적으로 의미론적 레이아웃을 매우 충실하게 따르는 고품질의 사실적인 이미지를 생성할 수 있다.12</p>
</li>
</ul>
<h3>5.3  어텐션 기반 정규화(Attentive Normalization, AN)</h3>
<ul>
<li><strong>개념:</strong> AN은 외부의 의미론적 맵 대신, 입력 피처맵 자체의 내부적인 의미론적 유사도에 기반하여 정규화를 수행한다. 이는 자기-어텐션(self-attention) 메커니즘의 아이디어를 정규화에 접목한 것이다.11</li>
<li><strong>작동 방식:</strong> AN은 먼저 입력 피처맵으로부터 어텐션 맵(또는 소프트한 의미론적 레이아웃)을 예측한다. 이 맵은 피처맵 내의 각 픽셀이 몇 개의 사전 정의된 의미론적 ‘지역’ 중 어디에 속할 확률이 높은지를 나타낸다. 그 후, 각 지역에 속한 픽셀들끼리만 모아서 독립적으로 인스턴스 정규화를 수행한다. 이를 통해 의미적으로 연관성이 높은, 그러나 공간적으로는 멀리 떨어져 있는 픽셀들이 동일한 정규화 통계량을 공유하게 되어, 이미지 전체의 일관성이 향상된다. 이 방식은 자기-어텐션 GAN과 같이 막대한 계산 비용을 요구하는 메커니즘 없이도 효과적으로 장거리 의존성(long-range dependency)을 모델링할 수 있는 장점이 있다.11</li>
</ul>
<h3>5.4  적응형 계층-인스턴스 정규화(Adaptive Layer-Instance Normalization, AdaLIN)</h3>
<ul>
<li>
<p><strong>개념:</strong> U-GAT-IT이라는 이미지-이미지 변환 모델에서 제안된 AdaLIN은 IN과 LN을 학습 가능한 파라미터를 통해 동적으로 결합한다.35</p>
</li>
<li>
<p><strong>작동 방식:</strong> AdaLIN은 IN과 LN의 결과를 가중합하는 방식으로 작동하며, 이 가중치(<span class="math math-inline">\rho</span>)는 네트워크의 파라미터로 학습된다.<br />
<span class="math math-display">
y = \rho \cdot \text{IN}(x) + (1 - \rho) \cdot \text{LN}(x)
</span><br />
이는 이미지의 형태(shape)와 질감(texture) 변화를 보다 유연하게 제어하기 위함이다. 실험적으로 IN은 콘텐츠의 세부적인 형태를 보존하는 경향이 강하고, LN은 스타일을 더 전역적으로 강하게 전이시키는 경향이 있다. AdaLIN은 데이터셋의 특성에 맞춰 두 정규화의 장점을 적절히 조합하여, 형태 변환이 큰 과업(예: 고양이→개)과 텍스처 변환이 중요한 과업(예: 사진→반 고흐 화풍) 모두에서 뛰어난 성능을 보이도록 한다.37</p>
</li>
</ul>
<p>이러한 다양한 변종들은 ’정규화’의 역할이 학습 안정화를 위한 수동적인 장치에서, 외부 조건에 따라 동적으로 변조되는 능동적인 ’생성 연산자(generative operator)’로 진화했음을 보여준다. 또한, IN이 전체 피처맵을 하나의 단위로 취급한 것에서 SPADE가 의미론적 영역을, AN이 동적 어텐션 영역을 단위로 취급하는 변화는, 이미지 생성 과업이 점점 더 정교해짐에 따라 ‘전역적 스타일’ 제어에서 ‘지역적/의미론적’ 제어로 패러다임이 이동하고 있음을 시사한다.</p>
<h2>6.  생성 모델에서의 응용과 발전: StyleGAN을 중심으로</h2>
<p>인스턴스 정규화와 그로부터 파생된 아이디어들은 현대 생성 모델, 특히 StyleGAN 계열의 아키텍처에 깊숙이 통합되어 고품질 이미지 생성의 핵심적인 역할을 수행했다. StyleGAN의 발전 과정은 IN 기반 조건부 정규화의 가능성과 한계를 동시에 보여주는 중요한 사례 연구다.</p>
<h3>6.1  StyleGAN에서의 AdaIN: 계층별 스타일 제어</h3>
<p>StyleGAN은 기존 GAN 아키텍처와 달리, 잠재 벡터 <span class="math math-inline">z</span>를 직접 생성기에 주입하지 않는다. 대신, 8개의 완전 연결 계층으로 구성된 매핑 네트워크(mapping network)를 통해 <span class="math math-inline">z</span>를 비선형적으로 변환하여 중간 잠재 벡터 <span class="math math-inline">w</span>를 생성한다.38 이 <span class="math math-inline">w</span> 벡터는 스타일 정보를 담고 있으며, 생성 과정 전반을 제어하는 핵심적인 역할을 한다.</p>
<p>생성기의 각 컨볼루션 블록에는 AdaIN 레이어가 포함되어 있다. 중간 잠재 벡터 <span class="math math-inline">w</span>는 학습 가능한 아핀 변환(affine transformation)을 거쳐 각 AdaIN 레이어에 필요한 스케일(<span class="math math-inline">y_s</span>)과 바이어스(<span class="math math-inline">y_b</span>) 파라미터를 생성한다. 이 파라미터들이 정규화된 피처맵에 적용되어 스타일을 주입한다.38<br />
<span class="math math-display">
\text{AdaIN}(x_i, y) = y_{s,i} \left( \frac{x_i - \mu_i}{\sigma_i} \right) + y_{b,i}
</span><br />
StyleGAN은 4x4의 낮은 해상도에서 시작하여 1024x1024의 높은 해상도까지 점진적으로 이미지를 생성하는 구조를 가진다. 각 해상도 레벨에 서로 다른 <span class="math math-inline">w</span> 벡터(또는 동일한 <span class="math math-inline">w</span>에서 파생된 파라미터)를 주입함으로써, 이미지의 다양한 수준의 속성을 제어할 수 있다. 예를 들어, 낮은 해상도(coarse, 4x4 ~ 8x8)의 레이어에 주입된 스타일은 자세, 머리 모양, 얼굴 형태와 같은 전역적인 구조를 제어한다. 중간 해상도(middle, 16x16 ~ 32x32) 레이어는 표정, 눈 모양과 같은 더 세부적인 특징을 제어하며, 높은 해상도(fine, 64x64 ~ 1024x1024) 레이어는 색상, 머리카락 질감, 피부 톤과 같은 미세한 스타일을 제어한다.38</p>
<h3>6.2  StyleGAN2의 혁신: AdaIN의 한계와 가중치 복조(Weight Demodulation)로의 전환</h3>
<p>StyleGAN은 매우 사실적인 이미지를 생성했지만, 생성된 이미지에서 종종 물방울 모양의 특징적인 아티팩트(droplet-like artifacts)가 발견되는 문제가 있었다.40 StyleGAN2의 연구진은 이 문제의 근본 원인이 AdaIN의 인스턴스 정규화 단계에 있다고 분석했다.</p>
<p>IN은 각 피처맵의 평균과 분산을 채널별로 독립적으로 정규화하기 때문에, 서로 다른 채널에 있는 피처들의 상대적인 크기(magnitude) 정보를 파괴하는 경향이 있다. 연구진은 생성기가 이러한 정규화의 제약을 우회하여 특정 정보를 전달하기 위해, 의도적으로 특정 위치에 매우 강한 신호 스파이크(spike)를 만들어 해당 피처맵의 통계량을 지배하게 만드는 ’꼼수(hack)’를 학습한다고 추론했다. 이 비정상적인 스파이크가 시각적으로 물방울 아티팩트로 나타난다는 것이다.42</p>
<p>이 문제를 해결하기 위해 StyleGAN2는 AdaIN을 ’가중치 변조(weight modulation)’와 ’가중치 복조(weight demodulation)’라는 새로운 메커니즘으로 완전히 대체했다.44</p>
<ol>
<li>
<p><strong>가중치 변조 (Weight Modulation):</strong> 스타일 벡터 <span class="math math-inline">s_i</span> (중간 잠재 벡터 <span class="math math-inline">w</span>로부터 파생됨)를 사용하여 컨볼루션 필터의 가중치 <span class="math math-inline">w_{ijk}</span>를 직접 스케일링한다. 이는 피처맵이 아닌 가중치에 스타일 정보를 직접 곱하는 방식이다.<br />
<span class="math math-display">
w&#39;_{ijk} = s_i \cdot w_{ijk}
</span></p>
</li>
<li>
<p><strong>가중치 복조 (Weight Demodulation):</strong> 변조된 가중치는 출력 피처맵의 크기를 크게 증폭시킬 수 있다. 이를 방지하기 위해, 변조된 가중치 <span class="math math-inline">w&#39;_{ijk}</span>를 각 출력 채널 <span class="math math-inline">j</span>에 대해 L2-norm으로 나누어 정규화(복조)한다. 이는 출력 활성화의 표준편차가 대략 1이 되도록 보장하는 효과가 있다.<br />
<span class="math math-display">
w&#39;&#39;_{ijk} = \frac{w&#39;_{ijk}}{\sqrt{\sum_{i,k} (w&#39;_{ijk})^2 + \epsilon}}
</span></p>
</li>
</ol>
<p>이 새로운 방식은 출력 피처맵의 통계량을 직접 조작하는 대신, 컨볼루션 가중치를 조작하여 유사한 효과를 얻는다. 가장 중요한 차이점은 이 과정이 피처맵 간의 상대적인 크기 정보를 보존한다는 것이다.48 결과적으로 생성기가 ’스파이크 생성’과 같은 꼼수를 학습할 필요가 없어지면서 물방울 아티팩트 문제가 근본적으로 해결되었고, 이미지 품질 또한 향상되었다. 이 전환은 정규화 레이어가 때로는 정보의 ’병목(bottleneck)’으로 작용할 수 있으며, 모델 설계 시 어떤 정보를 보존하고 어떤 정보를 버릴 것인지에 대한 신중한 고려가 필요함을 보여주는 사례다.</p>
<h3>6.3  생성 모델에서의 정규화 기법 최신 동향</h3>
<p>StyleGAN 이후, 생성 모델 분야는 확산 모델(Diffusion Models)이라는 새로운 패러다임으로 빠르게 전환되고 있다.50 확산 모델은 노이즈를 점진적으로 제거하는 과정을 학습하여 이미지를 생성하며, 기존 GAN과는 다른 방식으로 작동한다. 이에 따라 전통적인 정규화 기법의 역할 또한 변화하고 있다.</p>
<p>최신 연구 동향을 살펴보면, 정규화 기법은 단순히 생성 모델의 구성 요소를 넘어 다양한 문제 해결에 적용되고 있다. 예를 들어, 분포 외(Out-of-Distribution, OOD) 탐지 성능을 높이기 위해 훈련 시에만 피처 정규화를 적용하는 연구 53, 그래프 신경망(GNN)의 특성에 맞는 새로운 적응형 정규화 기법(GRANOLA)을 제안하는 연구 54, 제한된 데이터 환경에서 GAN의 학습 안정성을 높이기 위한 새로운 정규화(CHAIN)를 제안하는 연구 55 등이 발표되고 있다. 또한, 정규화 흐름(Normalizing Flows)과 같은 확률 모델 기반의 생성 모델 연구도 활발히 진행되며, 정규화의 개념을 모델 아키텍처의 중심으로 가져오고 있다.51 이는 정규화가 여전히 딥러닝 연구의 활발한 분야이며, 새로운 모델과 과업에 맞춰 계속해서 진화하고 있음을 보여준다.</p>
<h2>7.  결론: 인스턴스 정규화의 의의와 전망</h2>
<h3>7.1  장단점 요약 및 핵심 기여</h3>
<p>인스턴스 정규화는 딥러닝, 특히 생성 모델링 분야에 상당한 영향을 미친 중요한 기술이다. 그 장단점과 핵심 기여는 다음과 같이 요약할 수 있다.</p>
<ul>
<li><strong>장점:</strong></li>
<li><strong>배치 크기 독립성:</strong> 배치 크기에 의존하지 않으므로, 배치 크기가 작거나 1인 경우에도 안정적으로 작동한다. 이는 온라인 학습이나 고해상도 이미지 처리 등 큰 배치를 사용하기 어려운 환경에서 큰 이점을 제공한다.16</li>
<li><strong>효과적인 스타일-콘텐츠 분리:</strong> 개별 인스턴스의 통계량을 정규화함으로써 이미지의 스타일 정보를 효과적으로 제거하고 콘텐츠 구조를 보존한다. 이 특성은 스타일 전이 및 이미지 생성 과업에서 탁월한 성능의 기반이 되었다.16</li>
<li><strong>실시간 임의 스타일 전이 구현:</strong> AdaIN으로의 발전은 사전 학습 없이 임의의 스타일을 실시간으로 적용하는 것을 가능하게 하여, 관련 응용 분야에 혁신을 가져왔다.9</li>
<li><strong>단점:</strong></li>
<li><strong>판별 과업에서의 성능 저하:</strong> 스타일 정보를 의도적으로 제거하기 때문에, 질감이나 색상과 같은 스타일 정보가 중요한 단서가 되는 이미지 분류와 같은 판별 과업에서는 BN보다 성능이 저하될 수 있다.6</li>
<li><strong>제한된 적용 분야:</strong> 주로 이미지 생성 및 스타일 변환과 관련된 특정 작업에 유용성이 집중되어 있다.16</li>
<li><strong>아티팩트 유발 가능성:</strong> StyleGAN2의 사례에서 볼 수 있듯이, 특정 네트워크 아키텍처와 결합될 때 정보 병목 현상을 일으켜 의도치 않은 아티팩트를 유발할 수 있다.42</li>
<li><strong>핵심 기여:</strong> 인스턴스 정규화의 가장 중요한 기여는 정규화를 단순히 학습 안정화를 위한 도구가 아닌, 외부 조건에 따라 피처를 동적으로 변조하는 ’조건부 피처 변조(conditional feature modulation)’의 핵심 메커니즘으로 사용하는 새로운 패러다임을 열었다는 점이다. 이 아이디어는 현대 생성 모델 아키텍처에서 ’제어 가능성(controllability)’을 구현하는 핵심적인 설계 원리로 자리 잡았다. AdaIN, SPADE, 그리고 StyleGAN의 스타일 주입 메커니즘은 모두 IN의 이러한 철학을 계승하고 발전시킨 결과물이다.</li>
</ul>
<h3>7.2  향후 연구 방향과 미래 전망</h3>
<p>인스턴스 정규화가 제시한 아이디어는 계속해서 진화하며 새로운 연구 방향을 제시하고 있다.</p>
<ul>
<li><strong>적용 분야의 확장:</strong> IN의 아이디어는 초기 스타일 전이를 넘어 도메인 적응(domain adaptation), 이미지-이미지 변환(image-to-image translation), 이미지 조화(image harmonization) 등 다양한 컴퓨터 비전 분야로 성공적으로 확장되고 있다.7 이는 스타일 정보를 제어하는 능력이 광범위한 응용 가능성을 가짐을 보여준다.</li>
<li><strong>학습 기반 정규화(Learning-to-Normalize):</strong> BIN이나 Exemplar Normalization 61과 같이, 주어진 데이터와 과업에 가장 적합한 정규화 전략 자체를 학습하려는 연구가 활발히 진행 중이다. 이는 ’모든 문제에 최적인 단 하나의 정규화 기법은 없다’는 인식에 기반한다. 미래에는 모델이 아키텍처 내에서 여러 정규화 기법을 동적으로 선택하거나 합성하는 메타-학습(meta-learning) 기반의 접근법이 더욱 중요해질 것이다.</li>
<li><strong>정규화의 역할 재정의:</strong> 정규화 없는 네트워크(Normalizer-Free Networks) 62의 등장은 정규화가 심층 신경망 학습에 필수적인지에 대한 근본적인 질문을 던지고 있다. 이는 정규화의 이점이 다른 아키텍처 설계나 최적화 기법으로 대체될 수 있음을 시사한다. 향후에는 특정 과업과 데이터의 특성에 맞춰 정규화의 필요성 자체를 동적으로 결정하거나, 정규화의 강도를 조절하는 더욱 유연한 아키텍처가 등장할 것으로 예상된다.</li>
</ul>
<p>결론적으로, 인스턴스 정규화는 딥러닝 역사에서 단순한 기술적 발전을 넘어, 모델의 ’제어 가능성’이라는 새로운 차원을 연 중요한 변곡점이었다. 그 유산은 오늘날의 정교한 생성 모델에 깊이 새겨져 있으며, 앞으로도 더욱 지능적이고 적응적인 인공지능 모델을 향한 연구에 지속적인 영감을 제공할 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift - Google Research, http://research.google.com/pubs/archive/43442.pdf</li>
<li>BCN: Batch Channel Normalization for Image Classification - arXiv, https://arxiv.org/html/2312.00596v1</li>
<li>Instance Normalization: A CV Perspective - Number Analytics, https://www.numberanalytics.com/blog/instance-normalization-computer-vision-perspective</li>
<li>[1607.08022] Instance Normalization: The Missing Ingredient for Fast Stylization - arXiv, https://arxiv.org/abs/1607.08022</li>
<li>Instance Normalization: The Missing Ingredient for Fast Stylization, http://arxiv.org/pdf/1607.08022</li>
<li>Instance Normalisation vs Batch normalisation - Stack Overflow, https://stackoverflow.com/questions/45463778/instance-normalisation-vs-batch-normalisation</li>
<li>Batch-Instance Normalization for Adaptively Style-Invariant Neural Networks - NIPS, https://proceedings.neurips.cc/paper_files/paper/2018/file/018b59ce1fd616d874afad0f44ba338d-Paper.pdf</li>
<li>Batch-Instance Normalization for Adaptively Style-Invariant … - NIPS, http://papers.neurips.cc/paper/7522-batch-instance-normalization-for-adaptively-style-invariant-neural-networks.pdf</li>
<li>Arbitrary Style Transfer in Real-Time With Adaptive Instance Normalization - CVF Open Access, https://openaccess.thecvf.com/content_ICCV_2017/papers/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.pdf</li>
<li>[CNN] (6) 정규화 레이어(Normalization Layer) - Simply Guide, https://simply-guide.com/cnn/6_Normalization-Layer/</li>
<li>Attentive Normalization for Conditional Image Generation - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Attentive_Normalization_for_Conditional_Image_Generation_CVPR_2020_paper.pdf</li>
<li>Semantic Image Synthesis with Spatially-Adaptive Normalization - NVlabs, https://nvlabs.github.io/SPADE/</li>
<li>배치 정규화(Batch Normalization)의 비밀: 원리, 구현, 활용, https://hichoe95.tistory.com/133</li>
<li>InstanceNorm2d — PyTorch 2.7 documentation, https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html</li>
<li>Instance Normalization vs Batch Normalization - GeeksforGeeks, https://www.geeksforgeeks.org/deep-learning/instance-normalization-vs-batch-normalization/</li>
<li>BatchNormalization, LayerNormalization 및 InstanceNormalization …, https://thingswell.tistory.com/m/289</li>
<li>딥러닝 Normalization (Batch Normalization, Layer Normalization …, https://lunaleee.github.io/posts/normalization/</li>
<li>Batch Normalization in Machine Vision: A Beginner’s Guide - UnitX, https://www.unitxlabs.com/resources/batch-normalization-machine-vision-guide/</li>
<li>Normalization / Batch Normalization / Layer Normalization - Innov_AI_te - 티스토리, https://jaylala.tistory.com/entry/Normalization-Batch-Normalization-Layer-Normalization</li>
<li>[머신러닝/딥러닝] 8. Normalization - Son’s Notation, https://sonsnotation.blogspot.com/2020/11/8-normalization.html</li>
<li>[논문 요약] Layer Normalization - velog, <a href="https://velog.io/@roqkfwk7lnam/%EB%85%BC%EB%AC%B8-%EC%9A%94%EC%95%BD-Layer-Normalization">https://velog.io/@roqkfwk7lnam/%EB%85%BC%EB%AC%B8-%EC%9A%94%EC%95%BD-Layer-Normalization</a></li>
<li>[논문 읽기] Group Normalization(2018) - 딥러닝 공부방 - 티스토리, https://deep-learning-study.tistory.com/726</li>
<li>Introduction to Deep Learning Normalization - Subinium의 코딩일지, https://subinium.github.io/introduction-to-normalization/</li>
<li>Normalization Methods in Deep Learning - Ahmad Badary, https://ahmedbadary.github.io/work_files/research/dl/concepts/norm_methods</li>
<li>Group Normalization - Lunit Tech Blog, https://bloglunit.wordpress.com/2018/04/12/group-normalization/</li>
<li>Adaptive Instance Normalization Style Transfer, https://web.eecs.umich.edu/~justincj/teaching/eecs442/projects/WI2021/pdfs/055.pdf</li>
<li>Brief Review — AdaIN: Arbitrary Style Transfer in Real-time with …, https://sh-tsang.medium.com/brief-review-adain-arbitrary-style-transfer-in-real-time-with-adaptive-instance-normalization-70d30cf95ef7</li>
<li>[1703.06868] Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization, https://arxiv.org/abs/1703.06868</li>
<li>LPAdaIN: Light Progressive Attention Adaptive Instance Normalization Model for Style Transfer - MDPI, https://www.mdpi.com/2079-9292/11/18/2929</li>
<li>UNSEEN STYLE TRANSFER BASED ON A CONDI, https://mm.cs.uec.ac.jp/pub/conf17/170424yanai_0.pdf</li>
<li>[1903.07291] Semantic Image Synthesis with Spatially-Adaptive Normalization - arXiv, https://arxiv.org/abs/1903.07291</li>
<li>(PDF) GauGAN: semantic image synthesis with spatially adaptive normalization, https://www.researchgate.net/publication/334714551_GauGAN_semantic_image_synthesis_with_spatially_adaptive_normalization</li>
<li>Spatially Adaptive Instance Normalization (SPADE) - Artificial Intelligence, https://schneppat.com/spatially-adaptive-instance-normalization_spade.html</li>
<li>Attentive Normalization for Conditional Image Generation - Jiaya Jia, https://jiaya.me/file/papers/attentivenorm_cvpr20.pdf</li>
<li>Lornatang/UGATIT-PyTorch - GitHub, https://github.com/Lornatang/UGATIT-PyTorch</li>
<li>[1907.10830] U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation - arXiv, https://arxiv.org/abs/1907.10830</li>
<li>Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation - SciSpace, https://scispace.com/pdf/u-gat-it-unsupervised-generative-attentional-networks-with-3efibwv0kf.pdf</li>
<li>StyleGAN - Style Generative Adversarial Networks - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/stylegan-style-generative-adversarial-networks/</li>
<li>StyleGAN: A Gentle Introduction. Generative Adversarial Networks have… - AI Mind, https://pub.aimind.so/stylegan-a-step-by-step-introduction-ff995c99a884</li>
<li>Understanding the StyleGAN and StyleGAN2 Architecture | by Prem Chandra Singh | Analytics Vidhya | Medium, https://medium.com/analytics-vidhya/understanding-the-stylegan-and-stylegan2-architecture-add9e992747d</li>
<li>Understanding StyleGAN1 - Paperspace Blog, https://blog.paperspace.com/understanding-stylegan/</li>
<li>Paper Review - StyleGANv2 | Vines’ Log, https://vinesmsuic.github.io/paper-stylegan2/</li>
<li>The Evolution of StyleGAN: Introduction - Paperspace Blog, https://blog.paperspace.com/evolution-of-stylegan/</li>
<li>Improved StyleGAN2, https://jakebarkovitch.com/assets/pdf/improved_stylegan.pdf</li>
<li>Understanding StyleGAN2 - Paperspace Blog, https://blog.paperspace.com/understanding-stylegan2/</li>
<li>StyleGANs and StyleGAN2 | Saturn Cloud, https://saturncloud.io/glossary/stylegans-and-stylegan2/</li>
<li>GAN — StyleGAN &amp; StyleGAN2. Do you know your style? Most GAN …, https://jonathan-hui.medium.com/gan-stylegan-stylegan2-479bdf256299</li>
<li>StyleGAN vs StyleGAN2 vs StyleGAN2-ADA vs StyleGAN3 - Lambdanalytique, https://lambdanalytique.com/2022/07/01/stylegan-vs-stylegan2-vs-stylegan2-ada-vs-stylegan3/</li>
<li>[D] Why we must use weight demodulation in stylegan2 : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/mnwkhr/d_why_we_must_use_weight_demodulation_in_stylegan2/</li>
<li>arXiv:2502.11974v1 [cs.CV] 17 Feb 2025, <a href="https://arxiv.org/pdf/2502.11974">https://arxiv.org/pdf/2502.11974?</a></li>
<li>Visual Generative Modeling: What’s After Diffusion? | CVPR 2025 Workshop, https://vgm-cvpr.github.io/</li>
<li>CVPR Poster Unveiling Differences in Generative Models: A Scalable Differential Clustering Approach, https://cvpr.thecvf.com/virtual/2025/poster/34419</li>
<li>T2FNorm: Train-time Feature Normalization for OOD Detection in Image Classification - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024W/TCV2024/papers/Regmi_T2FNorm_Train-time_Feature_Normalization_for_OOD_Detection_in_Image_Classification_CVPRW_2024_paper.pdf</li>
<li>GRANOLA: Adaptive Normalization for Graph Neural Networks - arXiv, https://arxiv.org/pdf/2404.13344</li>
<li>CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Ni_CHAIN_Enhancing_Generalization_in_Data-Efficient_GANs_via_lipsCHitz_continuity_constrAIned_CVPR_2024_paper.pdf</li>
<li>Normalizing Flows are Capable Generative Models - arXiv, https://arxiv.org/html/2412.06329v1</li>
<li>[2412.06329] Normalizing Flows are Capable Generative Models - arXiv, https://arxiv.org/abs/2412.06329</li>
<li>Instance Normalization (IN) - Artificial Intelligence, https://schneppat.com/instance-normalization_in.html</li>
<li>Image-to-Image Translation - Papers With Code, https://paperswithcode.com/task/image-to-image-translation</li>
<li>Domain Adaptive Image-to-Image Translation - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Domain_Adaptive_Image-to-Image_Translation_CVPR_2020_paper.pdf</li>
<li>Exemplar Normalization for Learning Deep Representation - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Exemplar_Normalization_for_Learning_Deep_Representation_CVPR_2020_paper.pdf</li>
<li>High-Performance Large-Scale Image Recognition Without Normalization - arXiv, https://arxiv.org/abs/2102.06171</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>