<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:CenterNet (2019) 객체 탐지의 새로운 지평을 연 Anchor-Free 패러다임</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>CenterNet (2019) 객체 탐지의 새로운 지평을 연 Anchor-Free 패러다임</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>CenterNet (2019) 객체 탐지의 새로운 지평을 연 Anchor-Free 패러다임</span></nav>
                </div>
            </header>
            <article>
                <h1>CenterNet (2019) 객체 탐지의 새로운 지평을 연 Anchor-Free 패러다임</h1>
<h2>1.  객체 탐지의 패러다임 전환, CenterNet</h2>
<p>객체 탐지(Object Detection)는 컴퓨터 비전 분야의 핵심적인 과업으로, 이미지나 비디오 내에 존재하는 객체의 종류를 식별하고 그 위치를 정확히 찾아내는 것을 목표로 한다. 지난 십수 년간 딥러닝 기술의 발전과 함께 객체 탐지 기술은 비약적인 성장을 이루었으며, 자율 주행, 의료 영상 분석, 지능형 감시 시스템 등 다양한 산업 분야에서 핵심적인 역할을 수행하고 있다. 이러한 발전의 중심에는 ’Anchor’라는 개념을 기반으로 한 탐지기들이 있었다.</p>
<h3>1.1  Anchor 기반 탐지기의 현황과 한계</h3>
<p>YOLO(You Only Look Once) 시리즈와 Faster R-CNN과 같은 전통적인 고성능 객체 탐지기들은 대부분 ‘Anchor 기반(Anchor-based)’ 접근법을 채택하고 있다.1 이 방식의 핵심은 이미지 위에 다양한 크기와 종횡비를 가진 사전 정의된 참조 상자, 즉 ’Anchor Box’의 집합을 미리 설정하고, 이 상자들을 기준으로 객체의 실제 위치와 크기를 미세 조정(refinement)하는 것이다.3 특징 맵(feature map)의 각 위치에서 여러 개의 Anchor Box를 생성하고, 각 Anchor Box에 대해 객체가 존재하는지 여부를 분류(classification)하고, 존재한다면 그 위치를 더 정확하게 회귀(regression)하는 방식으로 작동한다.</p>
<p>이러한 Anchor 기반 방식은 객체 탐지 성능을 크게 향상시켰지만, 그 구조적 특성으로 인해 몇 가지 본질적인 한계를 내포하고 있다.</p>
<p>첫째, <strong>하이퍼파라미터에 대한 높은 의존성</strong>이다. Anchor Box의 크기, 종횡비, 개수 등은 모델의 성능에 직접적인 영향을 미치는 중요한 하이퍼파라미터다. 이는 데이터셋의 특성(예: 객체의 평균적인 크기나 형태)에 따라 민감하게 반응하므로, 새로운 데이터셋에 모델을 적용할 때마다 번거로운 튜닝 과정을 거쳐야 한다.5 최적의 Anchor 설정을 찾지 못하면, 특히 비정형적인 크기나 형태를 가진 객체에 대한 탐지 성능이 저하될 수 있다.7</p>
<p>둘째, <strong>계산의 비효율성</strong>이다. 고해상도 이미지나 높은 재현율(recall)을 요구하는 시나리오에서는 수십만 개에 달하는 방대한 양의 Anchor Box를 생성하고 처리해야 한다.9 각 Anchor Box에 대해 독립적인 분류와 회귀 연산을 수행하는 것은 상당한 계산 비용을 유발하며, 이는 특히 실시간 처리가 중요한 응용 분야에서 추론 속도의 병목 현상을 야기한다.3</p>
<p>셋째, <strong>정/부 표본의 극심한 불균형(Class Imbalance)</strong> 문제다. 이미지 위에 생성된 수많은 Anchor Box 중 실제 객체(Ground Truth)와 유의미하게 겹치는 긍정 표본(positive sample)은 극소수에 불과하며, 대부분은 배경에 해당하는 부정 표본(negative sample)이다.9 이러한 극심한 데이터 불균형은 모델의 학습 과정을 불안정하게 만들고, 배경을 객체로 오인하는 경향을 높인다. 이 문제를 완화하기 위해 Focal Loss와 같은 특수한 손실 함수 설계가 필수적으로 요구된다.8</p>
<h3>1.2  Anchor-Free 접근법의 대두와 CenterNet의 등장</h3>
<p>Anchor 기반 탐지기의 내재적 한계를 극복하기 위한 대안으로 ‘Anchor-Free’ 패러다임이 부상했다. 이 접근법은 사전 정의된 Anchor Box의 개념을 완전히 배제하고, 객체를 보다 근본적인 기하학적 요소, 예컨대 중심점(center point)이나 모서리점(corner points)과 같은 핵심점(keypoint)으로 표현하여 직접 예측하는 방식으로 전환했다.10 이는 복잡한 하이퍼파라미터 튜닝과 방대한 후보군 처리 과정을 생략함으로써 탐지 파이프라인을 근본적으로 단순화하고 효율성을 높이는 것을 목표로 한다.</p>
<p>이러한 패러다임 전환의 흐름 속에서 2019년 4월, 컴퓨터 비전 학계에 흥미로운 사건이 발생했다. 거의 동시에 발표된 두 편의 영향력 있는 논문이 공교롭게도 동일하게 ’CenterNet’이라는 이름을 사용한 것이다.12 이 두 논문은 객체의 ’중심’을 활용한다는 공통된 아이디어를 공유했지만, 그 철학과 구현 방식에서는 명확한 차이를 보였다. 이로 인해 학계와 산업계에서는 한동안 두 모델 간의 개념적 혼동이 발생하기도 했다.</p>
<p>이러한 배경 속에서 본 안내서는 객체 탐지 분야의 중요한 이정표가 된 두 ‘CenterNet’ 모델을 명확히 구분하고, 특히 Xingyi Zhou 등이 제안한 ‘Objects as Points’ 패러다임의 CenterNet을 중심으로 그 철학, 아키텍처, 학습 방법, 그리고 성능을 심층적으로 분석하고자 한다. 이 모델이 제시한 극도의 단순성과 효율성, 그리고 놀라운 확장성이 어떻게 객체 탐지 분야의 연구 방향에 지대한 영향을 미쳤는지 조명하는 것이 본 안내서의 핵심 목표다. 두 모델의 동시 등장은 단순한 우연이 아니라, 객체 탐지 분야가 Anchor라는 복잡한 매개체를 버리고 객체의 본질적인 표현(keypoint)으로 회귀하려는 거대한 흐름의 필연적 결과물이었다. 한 갈래는 기존 keypoint 기반 탐지기(CornerNet)의 문제를 해결하려는 진화적 접근이었고, 다른 한 갈래는 문제 자체를 ’객체는 점이다’라는 제1원칙에서부터 재정의하려는 혁명적 접근이었다. 이 두 흐름을 명확히 이해하는 것은 현대 객체 탐지 기술의 지형을 파악하는 데 필수적이다.</p>
<h2>2.  두 개의 CenterNet: 개념적 분기점</h2>
<p>’CenterNet’이라는 이름으로 발표된 두 주요 연구는 객체의 중심 정보를 활용한다는 공통분모를 가지지만, 그 목표와 접근 방식에서 근본적인 차이를 보인다. 이 두 모델을 명확히 구분하는 것은 CenterNet의 기술적 기여를 정확히 이해하기 위한 첫걸음이다.</p>
<h3>2.1  CenterNet: Objects as Points (Zhou et al.)</h3>
<p>Xingyi Zhou, Dequan Wang, Philipp Krähenbühl이 발표한 이 논문은 객체 탐지 문제를 완전히 새로운 관점에서 접근한다.10</p>
<ul>
<li><strong>핵심 철학</strong>: 이 모델의 철학은 “객체는 곧 점이다(Objects as Points)“라는 한 문장으로 요약된다. 즉, 각 객체를 복잡한 경계 상자가 아닌, 그 상자의 단일 ’중심점(center point)’으로 모델링한다.3 객체의 크기(너비와 높이), 3D 위치, 방향, 심지어 인간 자세(pose)와 같은 다른 모든 속성들은 이 중심점의 특징(feature)으로부터 직접 회귀하여 예측된다.3 이로써 객체 탐지라는 복잡한 과업이 표준적인 핵심점 추정(keypoint estimation) 문제로 재정의된다.</li>
<li><strong>기반 모델</strong>: 이 모델은 특정 선행 모델에 의존하지 않는 독자적인 Anchor-Free 프레임워크다. 기존의 Anchor 기반 1-단계 탐지기들이 가진 비효율성과 복잡성을 제거하고, 더 단순하면서도 빠르고 정확한 대안을 제시하는 것을 목표로 한다.3</li>
<li><strong>객체 표현</strong>: 객체는 클래스별 히트맵(heatmap) 상의 한 점으로 표현된다. 이 점의 위치가 객체의 중심을 나타내며, 해당 위치에서 별도의 예측 헤드(prediction head)를 통해 객체의 크기(width, height)와 다운샘플링으로 인한 위치 오차를 보정하기 위한 미세 오프셋(offset)을 회귀한다.14</li>
<li><strong>주요 혁신</strong>: 이 모델의 가장 혁신적인 기여는 <strong>비최대 억제(Non-Maximum Suppression, NMS) 후처리 과정의 완전한 제거</strong>다. 전통적인 탐지기들은 동일 객체에 대한 다수의 중복된 경계 상자를 제거하기 위해 NMS라는 별도의 알고리즘을 필요로 한다. 반면, 이 CenterNet은 예측된 히트맵에서 지역 최댓값(local peak)을 찾는 것만으로 자연스럽게 중복 탐지를 걸러낸다. 이는 추론 파이프라인을 극도로 단순화하고 속도를 획기적으로 향상시키는 결과를 가져왔다.14</li>
</ul>
<h3>2.2  CenterNet: Keypoint Triplets for Object Detection (Duan et al.)</h3>
<p>Kaiwen Duan 등이 발표한 이 논문은 기존의 keypoint 기반 탐지기인 CornerNet을 개선하는 데 초점을 맞춘다.18</p>
<ul>
<li>
<p><strong>핵심 철학</strong>: 이 모델은 객체를 좌상단(top-left)과 우하단(bottom-right) 모서리점 쌍으로만 표현할 때 발생하는 문제를 해결하기 위해, 여기에 ’중심점’을 추가하여 ’핵심점 삼중주(Keypoint Triplet)’로 객체를 표현한다.20 이 추가적인 중심점 정보는 제안된 경계 상자가 실제로 객체를 포함하는지를 검증하는 역할을 하여, 탐지의 정밀도(Precision)와 재현율(Recall)을 모두 향상시킨다.22</p>
</li>
<li>
<p><strong>기반 모델</strong>: 이 모델은 <strong>CornerNet의 직접적인 확장판</strong>이다. CornerNet은 Anchor를 제거하는 데 성공했지만, 객체의 내부 정보를 전혀 고려하지 않고 오직 경계 정보(모서리)에만 의존하기 때문에, 실제 객체가 없는 영역을 객체로 오인하는 오탐지(False Positives)가 많이 발생하는 단점이 있었다.21 Duan 등의 CenterNet은 이 문제를 해결하는 것을 주요 동기로 삼는다.</p>
</li>
<li>
<p><strong>객체 표현</strong>: 객체는 세 종류의 히트맵으로 표현된다: 좌상단 모서리 히트맵, 우하단 모서리 히트맵, 그리고 중심점 히트맵. 이 세 가지 정보를 조합하여 최종 객체를 탐지한다.20</p>
</li>
<li>
<p><strong>주요 혁신</strong>: 이 모델은 두 가지 특화된 풀링(pooling) 모듈을 제안한다. <strong>Center Pooling</strong>은 객체 중심 영역의 시각적 패턴을 더 잘 포착하도록 돕고, <strong>Cascade Corner Pooling</strong>은 모서리점이 객체 내부의 정보를 참고할 수 있도록 특징 표현을 강화한다.20 추론 시에는 먼저 모서리 쌍을 통해 후보 경계 상자를 생성한 뒤, 해당 상자의 중앙 영역 내에 같은 클래스의 중심점이 존재하는지를 확인하는</p>
</li>
</ul>
<p><strong>검증(verification) 단계</strong>를 거친다. 이 과정을 통해 CornerNet의 주요 약점이었던 오탐지를 효과적으로 줄일 수 있다.21</p>
<p>이 두 모델의 핵심적인 차이점은 다음 표와 같이 요약할 수 있다. 본 안내서의 나머지 부분에서는 주로 더 근본적인 패러다임 전환을 제시한 Zhou 등의 ‘Objects as Points’ 모델에 초점을 맞추어 심층 분석을 진행할 것이다.</p>
<table><thead><tr><th>특징 (Feature)</th><th>CenterNet (Objects as Points)</th><th>CenterNet (Keypoint Triplets)</th></tr></thead><tbody>
<tr><td><strong>객체 표현 방식</strong></td><td>단일 중심점 (Single Center Point)</td><td>핵심점 삼중주 (Top-Left, Bottom-Right, Center)</td></tr>
<tr><td><strong>기반 모델</strong></td><td>독자적 Anchor-Free 프레임워크</td><td>CornerNet의 확장</td></tr>
<tr><td><strong>핵심 혁신</strong></td><td>NMS-Free 추론, 극도의 단순성</td><td>Center/Cascade Corner Pooling, Triplet 검증 로직</td></tr>
<tr><td><strong>추론 로직</strong></td><td>히트맵 피크 추출 후 속성 회귀</td><td>코너 페어링 후 중심점 존재 여부로 검증</td></tr>
<tr><td><strong>주요 목표</strong></td><td>속도-정확도 트레이드오프 최적화</td><td>CornerNet의 오탐지 감소 및 정확도 향상</td></tr>
</tbody></table>
<h2>3.  CenterNet (Objects as Points): 핵심 철학 및 아키텍처</h2>
<p>CenterNet (Objects as Points)은 객체 탐지에 대한 근본적인 관점의 전환을 제시한다. 이 모델의 아키텍처는 이러한 철학을 구현하기 위해 극도로 단순하면서도 효율적으로 설계되었다.</p>
<h3>3.1  객체를 점으로: Anchor-Free 접근법의 정수</h3>
<p>전통적인 객체 탐지기는 본질적으로 수많은 후보 영역(Anchor Box)에 대한 이미지 분류 문제로 귀결된다. 이는 간접적이고 비효율적인 접근 방식이다. CenterNet은 이 패러다임을 완전히 뒤집어, 객체 탐지를 직접적인 핵심점 추정(keypoint estimation) 문제로 재정의한다.3</p>
<p>이 접근법의 핵심은 각 객체가 오직 하나의 ‘긍정적(positive)’ 중심점에만 할당된다는 점이다. Anchor 기반 방식에서는 하나의 실제 객체(ground truth)에 여러 개의 Anchor Box가 긍정 표본으로 할당될 수 있어, 필연적으로 중복된 예측이 발생하고 이를 제거하기 위한 NMS 과정이 필수적이다. 반면 CenterNet에서는 학습 시 각 객체에 대해 단 하나의 중심점 위치만 긍정 표본으로 간주하고, 나머지 모든 위치는 부정 표본(background)으로 처리한다.3 이로 인해 네트워크는 자연스럽게 각 객체에 대해 단일하고 강한 응답(peak)을 생성하도록 학습되며, 이는 NMS가 불필요해지는 근본적인 이유가 된다.</p>
<p>이러한 접근 방식은 전체 네트워크를 시작부터 끝까지 미분 가능한(end-to-end differentiable) 구조로 만들며, 중간에 복잡한 알고리즘적 후처리(NMS)나 수동적인 하이퍼파라미터 설정(Anchor 디자인)을 배제한다. 그 결과, CenterNet은 구조적으로 더 단순하고, 학습이 안정적이며, 추론 속도가 매우 빠른 탐지기로 탄생하게 되었다.10</p>
<h3>3.2  네트워크 구조: 백본과 예측 헤드</h3>
<p>CenterNet의 아키텍처는 크게 두 부분으로 나뉜다: 입력 이미지로부터 고수준의 의미론적 특징(semantic feature)을 추출하는 ’백본 네트워크’와, 이 특징 맵을 바탕으로 최종 예측 결과를 생성하는 ’예측 헤드’다.</p>
<h4>3.2.1 백본 네트워크 (Backbone Networks)</h4>
<p>CenterNet은 특정 백본 구조에 얽매이지 않고, 다양한 종류의 완전 컨볼루션 네트워크(Fully Convolutional Network)를 백본으로 사용할 수 있다. 이는 사용자가 자신의 응용 분야에서 요구하는 속도와 정확도 수준에 맞춰 유연하게 아키텍처를 선택할 수 있게 해준다.24 논문에서 주로 실험되고 사용된 백본은 다음과 같다.</p>
<ul>
<li><strong>Hourglass-104</strong>: 이 네트워크는 대칭적인 인코더-디코더 구조가 여러 개 중첩된 형태로, ‘모래시계’ 모양을 닮았다.24 각 단계에서 다운샘플링(down-sampling)을 통해 특징을 압축하고, 다시 업샘플링(up-sampling)을 통해 해상도를 복원하면서 스킵 연결(skip connection)로 저수준의 공간 정보와 고수준의 의미 정보를 효과적으로 결합한다. 이러한 구조는 다중 스케일(multi-scale) 특징을 정교하게 포착하는 데 매우 뛰어나, 특히 인간 자세 추정과 같은 핵심점 추정 작업에서 최고의 정확도를 보인다. CenterNet에서도 Hourglass-104를 백본으로 사용했을 때 MS COCO 데이터셋에서 45.1% AP라는 가장 높은 정확도를 달성했지만, 모델의 크기가 매우 크고 연산량이 많아 추론 속도는 1.4 FPS로 매우 느리다.26</li>
<li><strong>DLA-34 (Deep Layer Aggregation)</strong>: DLA는 트리(tree) 형태의 구조를 통해 네트워크의 여러 계층에서 추출된 특징들을 계층적이고 반복적으로 집계(aggregate)한다. 이를 통해 얕은 층의 세밀한 공간 정보와 깊은 층의 추상적인 의미 정보를 효과적으로 융합할 수 있다.28 CenterNet에서 DLA-34 백본은 52 FPS의 빠른 속도와 37.4% AP라는 높은 정확도를 동시에 달성하여, 속도와 정확도 간의 가장 이상적인 균형을 제공하는 것으로 평가받는다.14</li>
<li><strong>ResNet-18/101</strong>: ResNet은 잔차 연결(residual connection)을 도입하여 매우 깊은 네트워크도 안정적으로 학습할 수 있도록 한 표준적인 CNN 아키텍처다.24 CenterNet에서는 사전 학습된 ResNet의 마지막 풀링 및 완전 연결 계층을 제거하고, 여러 개의 디컨볼루션(deconvolution) 또는 업샘플링 레이어를 추가하여 다운샘플링된 특징 맵의 해상도를 복원한다. 특히 경량 모델인 ResNet-18을 사용했을 경우, 142 FPS라는 실시간에 가까운 매우 빠른 추론 속도를 보여주며(28.1% AP), 실시간 탐지가 필수적인 응용 분야에 적합하다.3</li>
</ul>
<h4>3.2.2 예측 헤드 (Prediction Heads)</h4>
<p>백본 네트워크를 통과하여 생성된 최종 특징 맵은 세 개의 독립적인 예측 헤드로 전달된다. 각 헤드는 <span class="math math-inline">3x3</span> 컨볼루션 레이어로 시작하여 특징을 정제한 후, 최종 예측을 위한 <span class="math math-inline">1x1</span> 컨볼루션 레이어로 이어진다. 이 세 헤드는 병렬적으로 작동하며, 각각 다른 목적의 예측을 수행한다.5</p>
<ul>
<li><strong>히트맵 헤드 (Heatmap Head)</strong>: 이 헤드는 객체 중심점의 위치를 예측하는 역할을 한다. 출력 텐서의 크기는 <span class="math math-inline">(W/R, H/R, C)</span>이며, 여기서 <span class="math math-inline">W</span>, <span class="math math-inline">H</span>는 입력 이미지의 너비와 높이, <span class="math math-inline">R</span>은 출력 스트라이드(output stride, 일반적으로 4), <span class="math math-inline">C</span>는 탐지할 객체 클래스의 개수다. 각 <span class="math math-inline">C</span>개의 채널은 특정 클래스에 대한 히트맵을 나타낸다. 히트맵의 특정 위치 <span class="math math-inline">(x, y)</span>의 값이 1에 가까울수록 해당 위치에 그 클래스의 객체 중심이 존재할 확률이 높다는 것을 의미한다.16</li>
<li><strong>크기 헤드 (Size Head)</strong>: 이 헤드는 각 객체의 물리적 크기, 즉 경계 상자의 너비(width)와 높이(height)를 예측한다. 출력 텐서의 크기는 <span class="math math-inline">(W/R, H/R, 2)</span>이며, 2개의 채널은 각각 너비와 높이에 해당한다. 이 헤드는 모든 클래스에 대해 공유(class-agnostic)되며, 히트맵에서 탐지된 중심점 위치 <span class="math math-inline">(x, y)</span>에서 이 헤드의 값을 읽어 해당 객체의 크기 정보를 얻는다.5</li>
<li><strong>오프셋 헤드 (Offset Head)</strong>: 백본 네트워크를 통과하면서 입력 이미지는 <span class="math math-inline">R</span>배만큼 다운샘플링된다. 예를 들어, 원본 이미지에서 <span class="math math-inline">(13, 13)</span>에 있던 중심점은 <span class="math math-inline">R=4</span>일 때 특징 맵에서 <span class="math math-inline">(3.25, 3.25)</span>에 해당하지만, 이는 정수 좌표계에서 <span class="math math-inline">(3, 3)</span>으로 양자화(quantization)된다. 이 과정에서 <span class="math math-inline">(0.25, 0.25)</span>만큼의 오차가 발생한다. 오프셋 헤드는 바로 이 양자화 오차를 보정하기 위한 지역 오프셋(local offset)을 예측한다. 출력 텐서의 크기는 <span class="math math-inline">(W/R, H/R, 2)</span>이며, 2개의 채널은 각각 x축과 y축 방향의 오프셋 값을 나타낸다. 이 헤드 역시 모든 클래스에 대해 공유된다.15</li>
</ul>
<p>이처럼 CenterNet의 아키텍처는 ‘무엇이 있는가’(히트맵), ‘얼마나 큰가’(크기), 그리고 ‘정확히 어디에 있는가’(오프셋)라는 세 가지 질문을 독립적인 헤드로 분리하여 처리한다. 이러한 모듈식 설계는 단순할 뿐만 아니라 놀라운 확장성의 기반이 된다. 예를 들어, 3D 객체 탐지를 수행하려면 깊이(depth), 3D 차원, 방향(orientation)을 예측하는 새로운 헤드를 추가하기만 하면 된다. 인간 자세 추정을 위해서는 각 신체 관절의 위치를 중심점으로부터의 오프셋으로 예측하는 헤드를 추가하면 된다.13 이처럼 핵심적인 중심점 탐지 메커니즘을 그대로 유지한 채, 새로운 속성을 예측하는 헤드만 추가하면 되므로, CenterNet은 2D 탐지를 넘어 다양한 컴퓨터 비전 과업을 해결할 수 있는 통합 프레임워크로서의 잠재력을 지닌다.</p>
<p>다음 표는 주요 백본 네트워크에 따른 CenterNet의 성능을 MS COCO 데이터셋 기준으로 정리한 것이다. 이를 통해 사용자는 자신의 요구사항에 맞는 최적의 모델을 선택할 수 있다.</p>
<table><thead><tr><th>백본 네트워크 (Backbone)</th><th>COCO AP (%)</th><th>FPS</th><th>주요 특징 (Key Characteristics)</th></tr></thead><tbody>
<tr><td>Hourglass-104</td><td>45.1</td><td>1.4</td><td>최고 정확도, 매우 느림, 핵심점 추정에 특화</td></tr>
<tr><td>DLA-34</td><td>37.4</td><td>52</td><td>속도와 정확도의 최적 균형</td></tr>
<tr><td>ResNet-18</td><td>28.1</td><td>142</td><td>최고 속도, 실시간 애플리케이션에 적합</td></tr>
</tbody></table>
<h2>4.  학습 과정 상세 분석: 손실 함수와 Ground Truth 생성</h2>
<p>CenterNet의 우수한 성능은 독창적인 Ground Truth 생성 방식과 이를 효과적으로 학습하기 위해 설계된 손실 함수에 기인한다. 모델이 ‘객체를 점으로’ 인식하도록 유도하는 학습 과정의 핵심 메커니즘을 상세히 분석한다.</p>
<h3>4.1  히트맵 Ground Truth 생성 메커니즘</h3>
<p>모델의 히트맵 헤드가 예측해야 할 목표값, 즉 Ground Truth 히트맵은 실제 객체의 중심점 위치를 명확히 나타내야 한다. CenterNet은 이를 위해 단순한 이진 마스크(binary mask) 대신 부드러운 가우시안 분포를 활용한다.5</p>
<ol>
<li><strong>중심점 계산</strong>: 먼저, 학습 데이터셋의 각 객체에 대해 주어진 경계 상자 <span class="math math-inline">(x_min, y_min, x_max, y_max)</span>로부터 그 중심점 좌표 <span class="math math-inline">p = ((x_min+x_max)/2, (y_min+y_max)/2)</span>를 계산한다.3</li>
<li><strong>좌표 매핑</strong>: 이 중심점 <span class="math math-inline">p</span>를 네트워크의 출력 스트라이드 <span class="math math-inline">R</span> (예: 4)을 고려하여 저해상도 특징 맵 상의 대응점 <span class="math math-inline">p̃ = ⌊p/R⌋</span>로 매핑한다. 이 과정에서 소수점 이하의 정보는 버려지며, 이것이 바로 오프셋 헤드가 보정해야 할 양자화 오차의 원인이 된다.5</li>
<li><strong>가우시안 스플래팅(Gaussian Splatting)</strong>: 매핑된 중심점 <span class="math math-inline">p̃</span>를 중심으로 하는 2차원 가우시안 커널을 생성하여, 해당 객체의 클래스에 해당하는 Ground Truth 히트맵 <span class="math math-inline">Y</span> 채널에 렌더링한다.30 만약 두 개의 다른 가우시안 분포가 특정 위치에서 겹칠 경우, 요소별 최댓값(element-wise maximum)을 취한다. 이 과정의 수학적 표현은 다음과 같다 5:</li>
</ol>
<p><span class="math math-display">
Y_{xyc} = \exp\left(-\frac{(x-\tilde{p}_x)^2 + (y-\tilde{p}_y)^2}{2\sigma_p^2}\right)
</span></p>
<p>여기서 <span class="math math-inline">Y_xyc</span>는 클래스 <span class="math math-inline">c</span>의 히트맵에서 위치 <span class="math math-inline">(x, y)</span>의 값이며, <span class="math math-inline">σ_p</span>는 객체의 크기에 따라 적응적으로 결정되는 표준편차다. 객체의 크기가 클수록 더 넓은 반경의 가우시안 분포가 생성되고, 작을수록 더 좁고 뾰족한 분포가 생성된다.30 이는 모델이 중심점 주변의 예측에 대해 어느 정도의 유연성을 갖도록 하며, 학습을 안정화시키는 중요한 역할을 한다.</p>
<h3>4.2  전체 손실 함수: 구성 요소와 수학적 정의</h3>
<p>CenterNet의 전체 손실 함수 <span class="math math-inline">L_det</span>는 히트맵 손실(<span class="math math-inline">L_k</span>), 크기 손실(<span class="math math-inline">L_size</span>), 그리고 오프셋 손실(<span class="math math-inline">L_off</span>)이라는 세 가지 구성 요소의 가중 합으로 정의된다. 각 손실 항은 특정 예측 헤드의 출력을 감독(supervise)한다.5</p>
<p><span class="math math-display">
L_{det} = L_k + \lambda_{size} L_{size} + \lambda_{off} L_{off}
</span><br />
논문에서는 실험적으로 <span class="math math-inline">λ_size</span> = 0.1, <span class="math math-inline">λ_off</span> = 1을 사용하였다.</p>
<h4>4.2.1 히트맵 손실 (Heatmap Loss, <span class="math math-inline">L_k</span>)</h4>
<p>히트맵 손실 <span class="math math-inline">L_k</span>는 픽셀 단위 로지스틱 회귀(pixel-wise logistic regression)에 Focal Loss를 변형하여 적용한 형태다.5 Focal Loss는 객체 중심점(긍정 표본)의 수가 압도적으로 적고 대부분이 배경(부정 표본)인 극심한 클래스 불균형 문제를 다루기 위해 도입되었다.10 CenterNet은 여기에 추가적인 수정을 가하여 성능을 더욱 향상시켰다.</p>
<p><span class="math math-display">
L_k = -\frac{1}{N} \sum_{xyc}
\begin{cases}
(1-\hat{Y}_{xyc})^\alpha \log(\hat{Y}_{xyc}) &amp; \text{if } Y_{xyc}=1 \\
(1-Y_{xyc})^\beta (\hat{Y}_{xyc})^\alpha \log(1-\hat{Y}_{xyc}) &amp; \text{otherwise}
\end{cases}
</span></p>
<ul>
<li><span class="math math-inline">N</span>은 이미지 내에 존재하는 객체의 총 개수다.</li>
<li><span class="math math-inline">Ŷ_xyc</span>는 예측된 히트맵의 값, <span class="math math-inline">Y_xyc</span>는 Ground Truth 히트맵의 값이다.</li>
<li><span class="math math-inline">α</span>와 <span class="math math-inline">β</span>는 Focal Loss의 하이퍼파라미터로, 각각 쉬운 예제와 부정 표본의 가중치를 조절한다.</li>
<li><span class="math math-inline">Y_xyc=1</span>인 경우는 Ground Truth 중심점 위치에 해당하며, 표준적인 Focal Loss가 적용된다.</li>
<li><code>otherwise</code> 경우는 배경에 해당한다. 여기서 주목할 점은 <span class="math math-inline">(1-Y_xyc)^β</span> 항이다. Ground Truth 히트맵 <span class="math math-inline">Y</span>는 가우시안 분포를 따르므로, 중심점에서 멀어질수록 <span class="math math-inline">Y_xyc</span> 값은 1에서 0으로 점차 감소한다. 따라서 <span class="math math-inline">(1-Y_xyc)</span> 항은 중심점과 가까운 위치의 부정 표본에 대해서는 작은 가중치를, 먼 위치의 부정 표본에 대해서는 큰 가중치를 부여한다. 이는 모델이 중심점 바로 근처의 예측에 대해 덜 엄격한 페널티를 받도록 하여, 약간의 위치 부정확성을 허용하고 결과적으로 더 높은 품질의 히트맵을 생성하도록 유도한다.3</li>
</ul>
<h4>4.2.2 크기 손실 (Size Loss, <span class="math math-inline">L_{size}</span>)</h4>
<p>크기 손실 <span class="math math-inline">L_size</span>는 객체의 너비와 높이를 예측하는 크기 헤드를 학습시키기 위해 사용된다. 이 손실은 오직 Ground Truth 중심점 위치 <span class="math math-inline">p̃_k</span>에서만 계산된다. 다른 모든 위치의 손실은 무시된다. 손실 함수로는 L1 손실이 사용된다.5</p>
<p><span class="math math-display">
L_{size} = \frac{1}{N} \sum_{k=1}^{N} \vert \hat{S}_{\tilde{p}_k} - s_k \vert
</span></p>
<ul>
<li><span class="math math-inline">Ŝ_p̃k</span>는 위치 <span class="math math-inline">p̃_k</span>에서 예측된 크기 벡터 <span class="math math-inline">(ŵ_k, ĥ_k)</span>다.</li>
<li><span class="math math-inline">s_k</span>는 객체 <span class="math math-inline">k</span>의 실제 크기 벡터 <span class="math math-inline">(w_k, h_k)</span>다.</li>
</ul>
<p>이 손실 함수는 모델이 특정 위치의 특징 벡터로부터 해당 위치에 중심을 둔 객체의 실제 크기를 직접 회귀하도록 학습시킨다.29</p>
<h4>4.2.3 오프셋 손실 (Offset Loss, <span class="math math-inline">L_{off}</span>)</h4>
<p>오프셋 손실 <span class="math math-inline">L_off</span>는 다운샘플링으로 인해 발생하는 양자화 오차를 보정하기 위한 오프셋 예측을 학습시킨다. 크기 손실과 마찬가지로, 이 손실 역시 L1 손실을 사용하며 Ground Truth 중심점 위치에서만 계산된다.5</p>
<p><span class="math math-display">
L_{off} = \frac{1}{N} \sum_{k=1}^{N} \vert \hat{O}_{\tilde{p}_k} - \left(\frac{p_k}{R} - \tilde{p}_k\right) \vert
</span></p>
<ul>
<li><span class="math math-inline">Ô_p̃k</span>는 위치 <span class="math math-inline">p̃_k</span>에서 예측된 오프셋 벡터다.</li>
<li><span class="math math-inline">(p_k/R - p̃_k)</span>는 실제 양자화 오차 값으로, Ground Truth 오프셋에 해당한다.</li>
</ul>
<p>이 세 가지 손실 함수를 결합함으로써, CenterNet은 단일 네트워크를 통해 객체의 존재 확률, 정확한 위치, 그리고 크기를 동시에 학습할 수 있는 강력하고 통합된 학습 프레임워크를 구축한다.</p>
<h2>5.  추론 과정: 히트맵에서 최종 경계 상자까지</h2>
<p>CenterNet의 추론 과정은 학습 과정만큼이나 단순하고 효율적이다. NMS와 같은 복잡한 후처리 과정 없이, 네트워크의 순방향 전파(forward pass) 결과물로부터 직접 최종 경계 상자를 생성한다. 이 과정은 크게 세 단계로 나눌 수 있다: 피크 추출, 신뢰도 기반 필터링, 그리고 경계 상자 생성.</p>
<h3>5.1  피크 추출 (Peak Extraction)</h3>
<p>추론의 첫 단계는 네트워크가 출력한 각 클래스별 히트맵 <span class="math math-inline">Ŷ_c</span>에서 잠재적인 객체 중심점을 찾는 것이다. CenterNet은 이를 위해 NMS 대신 매우 효율적인 방법을 사용한다.</p>
<ol>
<li><strong>3x3 Max-Pooling 적용</strong>: 먼저, 예측된 히트맵 <span class="math math-inline">Ŷ</span>에 대해 커널 크기 <span class="math math-inline">3x3</span>, 스트라이드 1의 Max-Pooling 연산을 수행한다.16 이 연산의 결과로 생성된 새로운 히트맵</li>
</ol>
<p><span class="math math-inline">Ŷ_pooled</span>의 각 픽셀 값은 원본 히트맵의 해당 위치와 그 주변 8개 이웃(<span class="math math-inline">3x3</span> 영역) 중 최댓값을 갖게 된다.</p>
<ol start="2">
<li>
<p><strong>지역 최댓값 식별</strong>: 그 다음, 원본 히트맵 <span class="math math-inline">Ŷ</span>과 풀링된 히트맵 <span class="math math-inline">Ŷ_pooled</span>를 요소별로 비교한다. 만약 특정 위치 <span class="math math-inline">(x, y)</span>에서 <span class="math math-inline">Ŷ_xyc == Ŷ_pooled_xyc</span>가 성립한다면, 이는 해당 위치의 값이 주변 8개 이웃보다 크거나 같다는 것을 의미하며, 따라서 이 위치는 지역 최댓값(local peak)으로 간주된다.16</p>
</li>
<li>
<p><strong>피크 위치 저장</strong>: 이 조건을 만족하는 모든 위치 <span class="math math-inline">(x, y)</span>와 그 때의 히트맵 값(신뢰도 점수)을 해당 클래스의 잠재적 객체 중심점으로 저장한다.</p>
</li>
</ol>
<p>이 과정은 전통적인 NMS가 경계 상자 간의 IoU(Intersection over Union)를 반복적으로 계산하는 것과 비교하여 훨씬 계산적으로 효율적이다. NMS가 후보 상자들의 기하학적 관계에 기반한 후처리 알고리즘인 반면, CenterNet의 피크 추출은 특징 맵의 위상(topology)에 기반한 내재적 필터링 메커니즘이다. 이는 “객체의 유일성은 공간적 중첩이 아닌, 특징 맵 상에서의 응답 최고점으로 정의된다“는 철학적 전환을 보여준다. 이로 인해 전체 추론 과정이 단일 네트워크 연산의 흐름 안에서 끊김 없이 자연스럽게 이루어지며, 이는 CenterNet의 빠른 속도에 결정적으로 기여한다.</p>
<h3>5.2  신뢰도 기반 필터링 (Confidence-based Filtering)</h3>
<p>피크 추출 단계를 통해 수많은 잠재적 중심점들이 식별된다. 하지만 이들 중 다수는 신뢰도가 낮은 노이즈일 수 있다. 따라서, 이 후보들을 필터링하는 과정이 필요하다.</p>
<ul>
<li>
<p>추출된 모든 피크(중심점)들 중에서, 그 신뢰도 점수(히트맵 값)가 사전에 설정된 임계값(예: 0.1)보다 높은 것들만 남긴다.</p>
</li>
<li>
<p>그 후, 신뢰도 점수가 높은 순서대로 정렬하여 상위 <code>K</code>개(예: 100개)의 피크만 최종 후보로 선택한다.5 이</p>
</li>
</ul>
<p><code>K</code>값은 이미지 당 최대 탐지 객체 수를 제한하는 역할을 한다.</p>
<h3>5.3  경계 상자 생성 (Bounding Box Generation)</h3>
<p>마지막으로, 필터링을 통과한 각 최종 중심점 후보로부터 완전한 경계 상자 정보를 복원한다.</p>
<ol>
<li><strong>속성 정보 추출</strong>: 선택된 각 중심점의 저해상도 좌표 <span class="math math-inline">(x_i, y_i)</span>에 대해, 해당 위치에서 예측된 오프셋 벡터 <span class="math math-inline">(δx_i, δy_i) = Ô(x_i, y_i)</span>와 크기 벡터 <span class="math math-inline">(w_i, h_i) = Ŝ(x_i, y_i)</span>를 각각의 예측 헤드로부터 추출한다.15</li>
<li><strong>최종 좌표 계산</strong>: 이 정보들을 결합하여 원본 이미지 스케일의 최종 경계 상자 좌표를 계산한다. <span class="math math-inline">R</span>은 출력 스트라이드다.5</li>
</ol>
<ul>
<li><strong>정확한 중심점 X 좌표</strong>: <span class="math math-inline">(x_i + δx_i) * R</span></li>
<li><strong>정확한 중심점 Y 좌표</strong>: <span class="math math-inline">(y_i + δy_i) * R</span></li>
<li><strong>경계 상자 너비</strong>: <span class="math math-inline">w_i</span></li>
<li><strong>경계 상자 높이</strong>: <span class="math math-inline">h_i</span></li>
</ul>
<ol>
<li><strong>경계 상자 변환</strong>: 계산된 중심점 좌표와 너비, 높이 정보를 사용하여 최종적인 경계 상자 표현 <span class="math math-inline">(x_min, y_min, x_max, y_max)</span>으로 변환한다.</li>
</ol>
<ul>
<li><span class="math math-inline">x_min = (x_i + δx_i) * R - w_i / 2</span></li>
<li><span class="math math-inline">y_min = (y_i + δy_i) * R - h_i / 2</span></li>
<li><span class="math math-inline">x_max = (x_i + δx_i) * R + w_i / 2</span></li>
<li><span class="math math-inline">y_max = (y_i + δy_i) * R + h_i / 2</span></li>
</ul>
<p>이러한 일련의 과정을 통해 CenterNet은 히트맵, 오프셋, 크기라는 세 가지 분리된 예측을 하나의 완전한 객체 탐지 결과로 통합한다. 전체 과정이 단순한 행렬 연산과 조회(lookup)로 구성되어 있어 매우 빠르고 효율적으로 수행될 수 있다.</p>
<h2>6.  성능 분석 및 비교: Anchor 기반 탐지기와의 대결</h2>
<p>CenterNet의 혁신성을 평가하기 위해서는 기존의 지배적인 패러다임이었던 Anchor 기반 탐지기와의 직접적인 비교가 필수적이다. 본 장에서는 CenterNet을 대표적인 Anchor 기반 탐지기인 YOLO 및 Faster R-CNN과 비교 분석하고, Anchor-Free 접근법이 갖는 본질적인 장점과 한계를 고찰한다.</p>
<h3>6.1  CenterNet vs. YOLO &amp; Faster R-CNN</h3>
<p>CenterNet, YOLO, Faster R-CNN은 각각 객체 탐지 분야에서 서로 다른 철학과 구조를 대표하는 모델이다.</p>
<ul>
<li><strong>철학적 차이</strong>: 가장 근본적인 차이는 문제를 정의하는 방식에 있다. CenterNet은 객체 탐지를 ’핵심점 추정’이라는 회귀(regression) 문제로 접근한다.17 반면, YOLO와 Faster R-CNN은 수많은 Anchor Box에 대해 ’객체인가, 배경인가’를 판단하는 분류(classification) 문제로 접근한다. 이 차이가 구조와 성능의 모든 측면에 영향을 미친다.</li>
<li><strong>구조적 차이</strong>:</li>
<li><strong>CenterNet</strong>: 단일 스테이지(one-stage), Anchor-Free 구조로, NMS가 필요 없다. 파이프라인이 가장 단순하다.1</li>
<li><strong>YOLO</strong>: 단일 스테이지, Anchor 기반 구조다. 이미지를 그리드로 나누고 각 그리드 셀이 Anchor Box를 기반으로 예측을 수행한다. NMS가 필수적이다.1</li>
<li><strong>Faster R-CNN</strong>: 2-단계(two-stage), Anchor 기반 구조다. 첫 번째 단계(Region Proposal Network, RPN)에서 Anchor를 이용해 객체가 있을 법한 후보 영역(region proposal)을 생성하고, 두 번째 단계에서 이 후보 영역에 대해 정밀한 분류와 위치 회귀를 수행한다. 구조가 가장 복잡하지만, 높은 정확도를 달성하는 데 유리하다.2</li>
<li><strong>성능 비교</strong>:</li>
<li><strong>속도</strong>: 추론 속도 면에서는 파이프라인의 단순함이 큰 이점으로 작용한다. 경량 백본(ResNet-18)을 사용한 CenterNet은 142 FPS를 기록하여 매우 빠른 속도를 자랑한다. 일반적인 CPU 환경에서의 비교 실험에서도 CenterNet은 평균 370ms의 처리 시간을 보인 반면, YOLOv3는 1050ms가 소요되어 CenterNet이 약 3배 가까이 빠른 성능을 보였다.1 2-단계 구조인 Faster R-CNN은 이들보다 훨씬 느리다.</li>
<li><strong>정확도</strong>: 정확도 측면에서는 사용되는 백본 네트워크에 따라 결과가 달라진다. 최고 성능의 백본(Hourglass-104)을 장착한 CenterNet은 45.1% AP(MS COCO)를 달성하여, 당시의 모든 1-단계 탐지기를 능가하고 정교한 2-단계 탐지기인 Faster R-CNN과 필적하는 높은 정확도를 보였다.10 다만, 특정 데이터셋이나 조건에서는 YOLOv3가 CenterNet보다 근소하게 높은 mAP를 기록한 연구도 존재한다 (사람 탐지 실험에서 YOLOv3 98.42% vs CenterNet 97%).1 이는 특정 과업에 대한 모델의 적합성이 다를 수 있음을 시사한다.</li>
</ul>
<h3>6.2  Anchor-Free 방식의 장점과 한계</h3>
<p>CenterNet으로 대표되는 Anchor-Free 접근법은 객체 탐지 분야에 명확한 장점을 제시했지만, 동시에 새로운 형태의 한계점도 드러냈다.</p>
<h4>6.2.1 장점</h4>
<ul>
<li><strong>단순성과 효율성</strong>: 가장 큰 장점은 Anchor와 관련된 복잡한 하이퍼파라미터 튜닝(크기, 종횡비, 개수 등) 과정이 완전히 사라진다는 것이다.5 또한, NMS 후처리를 제거함으로써 전체 추론 파이프라인이 극도로 단순해지고 빨라진다. 이는 모델 개발 및 배포 과정을 크게 간소화한다.14</li>
<li><strong>일반성과 확장성</strong>: 객체를 ’점’이라는 보편적인 기하학적 요소로 표현하는 아이디어는 매우 일반적이어서 다른 컴퓨터 비전 과업으로의 확장이 용이하다. 실제로 2D 객체 탐지를 위해 개발된 동일한 프레임워크가 최소한의 수정만으로 3D 객체 탐지, 인간 자세 추정 등에도 성공적으로 적용되었다.5</li>
<li><strong>형태적 유연성</strong>: 사전 정의된 사각형 형태의 Anchor Box에 의존하지 않기 때문에, 길고 가느다란 객체나 비정형적인 형태를 가진 객체를 탐지하는 데 이론적으로 더 유리할 수 있다.4</li>
</ul>
<h4>6.2.2 한계</h4>
<ul>
<li><strong>중첩 객체 문제 (Overlapping Objects)</strong>: Anchor-Free 방식의 가장 잘 알려진 약점이다. 만약 두 개 이상의 서로 다른 객체의 중심점이 다운샘플링된 특징 맵 상에서 동일한 픽셀 위치나 매우 인접한 위치에 매핑될 경우, 네트워크는 하나의 피크(peak)만을 생성하게 되어 둘 중 하나의 객체를 놓치게 된다.34 이는 특히 작고 빽빽하게 밀집된 객체들(예: 군중 속의 사람들)을 탐지할 때 재현율(recall) 저하의 주요 원인이 된다.9</li>
<li><strong>사전 정보(Prior)의 부재</strong>: Anchor Box는 일종의 사전 정보 역할을 하여 모델이 객체의 일반적인 크기와 형태에 대해 더 쉽게 학습하도록 돕는다. Anchor-Free 모델은 이러한 사전 정보 없이 순전히 데이터로부터 모든 것을 학습해야 한다. 이로 인해 학습이 더 어려워지거나, 최고 수준의 정확도(SOTA, State-Of-The-Art)를 달성하는 데 있어 Anchor 기반 모델보다 다소 불리할 수 있다는 주장이 있다.15</li>
</ul>
<p>다음 표는 CenterNet과 주요 Anchor 기반 탐지기의 핵심 특징을 요약하여 비교한 것이다.</p>
<table><thead><tr><th>특징 (Feature)</th><th>CenterNet (Objects as Points)</th><th>YOLO (v3/v4)</th><th>Faster R-CNN</th></tr></thead><tbody>
<tr><td><strong>패러다임</strong></td><td>1-단계, Anchor-Free</td><td>1-단계, Anchor-기반</td><td>2-단계, Anchor-기반</td></tr>
<tr><td><strong>객체 표현</strong></td><td>중심점 + 속성</td><td>그리드 셀 + Anchor 오프셋</td><td>Region Proposals (from Anchors)</td></tr>
<tr><td><strong>NMS 필요 여부</strong></td><td>불필요 (피크 추출로 대체)</td><td>필수</td><td>필수</td></tr>
<tr><td><strong>속도</strong></td><td>매우 빠름 (백본에 따라)</td><td>매우 빠름</td><td>상대적으로 느림</td></tr>
<tr><td><strong>정확도</strong></td><td>높음 (2-단계와 경쟁 가능)</td><td>높음</td><td>매우 높음</td></tr>
<tr><td><strong>주요 장점</strong></td><td>단순성, 속도, 확장성</td><td>속도와 정확도의 균형</td><td>높은 정확도, 유연성</td></tr>
</tbody></table>
<h2>7.  CenterNet의 확장성: 2D 탐지를 넘어서</h2>
<p>CenterNet의 ‘Objects as Points’ 철학이 가진 진정한 힘은 2D 객체 탐지를 넘어 다른 복잡한 컴퓨터 비전 과업으로 손쉽게 확장될 수 있다는 점에서 드러난다. 객체를 ’점’이라는 근본적인 단위로 환원함으로써, 다양한 속성들을 이 점에 연관시키는 일관된 프레임워크를 제공한다.</p>
<h3>7.1  3D 객체 탐지로의 확장: CenterPoint</h3>
<p>자율 주행과 같은 분야에서 필수적인 3D 객체 탐지는 2D 탐지보다 훨씬 더 복잡한 과제다. 3D 공간에서의 객체는 위치와 크기뿐만 아니라, 임의의 방향(orientation)을 가지기 때문이다. 기존의 Anchor 기반 3D 탐지기들은 3D 공간에서 가능한 모든 방향과 크기의 Anchor Box를 생성해야 하므로, 탐색 공간이 기하급수적으로 증가하는 문제를 안고 있었다.35</p>
<p>CenterPoint는 CenterNet의 아이디어를 3D 공간으로 확장하여 이 문제를 우아하게 해결한다.35</p>
<ul>
<li><strong>방법론</strong>:</li>
</ul>
<ol>
<li><strong>입력 데이터 처리</strong>: LiDAR 센서로부터 얻은 3D 포인트 클라우드(point cloud) 데이터를 VoxelNet이나 PointPillars와 같은 3D 백본 네트워크에 입력한다. 이 네트워크는 희소하고 비정형적인 포인트 클라우드 데이터를 처리하여, 조감도(Bird’s-Eye-View, BEV) 시점의 밀집된 2D 특징 맵으로 변환한다.37</li>
<li><strong>중심점 탐지</strong>: 생성된 BEV 특징 맵을 2D 이미지처럼 간주하고, 여기에 표준 CenterNet 탐지 헤드를 적용한다. 이를 통해 BEV 평면상에서 각 객체의 2D 중심점을 나타내는 히트맵을 예측한다.37</li>
<li><strong>3D 속성 회귀</strong>: 히트맵에서 탐지된 각 중심점 위치의 특징 벡터로부터, 해당 객체의 3D 속성들을 직접 회귀하는 추가적인 예측 헤드를 사용한다. 예측되는 속성은 다음과 같다:</li>
</ol>
<ul>
<li>3D 크기 (너비, 길이, 높이)</li>
<li>지면으로부터의 높이</li>
<li>방향 (예: yaw 각도를 <span class="math math-inline">sin(yaw)</span>와 <span class="math math-inline">cos(yaw)</span>로 표현)</li>
<li>속도 (추적을 위한 정보).35</li>
<li><strong>CenterPoint의 장점</strong>: 이 접근법의 가장 큰 장점은 객체를 방향성에 무관한(rotation-invariant) ’점’으로 표현한다는 것이다. 점 자체에는 방향이라는 개념이 없기 때문에, 네트워크는 복잡한 방향성을 가진 Anchor Box를 분류하는 대신, 방향에 무관한 중심점을 찾고 방향을 다른 속성들과 마찬가지로 회귀하기만 하면 된다. 이는 탐색 공간을 극적으로 줄여 효율성을 높인다.35 또한, 3D 객체 추적(tracking) 문제 역시 연속된 프레임 간에 탐지된 점들을 가장 가까운 거리 기준으로 연결하는 단순한 매칭 문제로 치환되어, 전체 시스템의 복잡도를 크게 낮춘다.35 3D 공간에서 ’상자’가 가지는 회전이라는 추가적인 자유도는 탐색 공간의 ’차원의 저주’를 야기하는데, ’점’으로의 표현 전환은 이 문제를 근본적으로 회피하는 탁월한 해결책이다. 따라서 ‘Objects as Points’ 철학은 2D보다 3D 공간에서 더욱 강력한 영향력을 발휘한다.</li>
</ul>
<h3>7.2  인간 자세 추정으로의 응용</h3>
<p>다수의 사람이 등장하는 이미지에서 각 개인의 신체 관절 위치를 정확히 찾아내는 다중 인간 자세 추정(multi-person pose estimation) 역시 CenterNet 프레임워크를 통해 효과적으로 해결할 수 있다.13</p>
<p>전통적인 접근법은 주로 2-단계로 이루어진다: 먼저 사람을 탐지하는 객체 탐지기를 실행하고(top-down), 그 다음 각 탐지된 사람의 경계 상자 내에서 단일 인간 자세 추정 모델을 적용한다. 이 방식은 정확하지만, 두 개의 독립적인 모델을 순차적으로 실행해야 하므로 비효율적이다.</p>
<p>CenterNet은 이를 단일 네트워크에서 한 번에 처리하는 end-to-end 솔루션을 제공한다.</p>
<ul>
<li><strong>방법론</strong>:</li>
</ul>
<ol>
<li><strong>사람 중심점 탐지</strong>: 먼저, ‘사람’ 클래스에 대한 히트맵을 예측하여 이미지 내 모든 사람의 중심점을 탐지한다. 이는 표준 객체 탐지 과정과 동일하다.40</li>
<li><strong>관절 위치를 오프셋으로 정의</strong>: 각 신체 관절(예: 어깨, 팔꿈치, 무릎 등 <span class="math math-inline">k</span>개의 관절)의 위치를 해당 사람의 중심점으로부터의 2D 오프셋(offset) 벡터로 간주한다.3</li>
<li><strong>관절 위치 헤드 추가</strong>: 기존의 예측 헤드(히트맵, 크기, 오프셋)에 더해, ’관절 위치 헤드(Joint Location Head)’라는 새로운 헤드를 추가한다. 이 헤드는 <span class="math math-inline">2k</span>개의 채널을 가지며, 각 사람의 중심점 위치에서 <span class="math math-inline">k</span>개 관절에 대한 <span class="math math-inline">(Δx, Δy)</span> 오프셋 값들을 모두 회귀한다.15</li>
<li><strong>자세 재구성</strong>: 추론 시, 탐지된 각 사람의 중심점 좌표에 해당 위치에서 회귀된 <span class="math math-inline">k</span>개의 관절 오프셋 벡터들을 더하여, 각 개인의 완전한 2D 자세(skeleton)를 최종적으로 재구성한다.41</li>
</ol>
<p>이 방식은 별도의 ‘그룹핑(grouping)’ 과정 없이도 각 관절이 어떤 사람에게 속하는지를 자연스럽게 해결한다. 모든 관절은 그것이 회귀된 ’중심점’에 귀속되기 때문이다. 이처럼 CenterNet은 복잡한 다중 인간 자세 추정 문제를 객체(사람) 중심점 탐지와 관련 속성(관절 오프셋) 회귀라는 단순하고 통합된 프레임워크로 변환하여, 높은 효율성과 정확도를 동시에 달성한다.40</p>
<h2>8.  CenterNet (Keypoint Triplets): CornerNet의 진화</h2>
<p>’Objects as Points’가 객체 탐지에 대한 혁명적인 재정의를 시도했다면, Duan 등이 제안한 ‘Keypoint Triplets’ CenterNet은 기존 keypoint 기반 탐지기의 한계를 극복하려는 진화적인 접근법을 대표한다. 이 모델의 핵심은 CornerNet의 장점을 계승하면서, 그 치명적인 약점을 보완하는 데 있다.</p>
<h3>8.1  CornerNet의 한계와 동기</h3>
<p>CornerNet은 객체를 좌상단과 우하단 모서리점의 쌍으로 탐지함으로써 Anchor Box를 성공적으로 제거했다. 하지만 이 방식은 객체의 경계에만 집중하고 객체 내부의 시각적 정보를 전혀 활용하지 않는다는 근본적인 한계를 가지고 있었다.20 그 결과, CornerNet은 실제 객체가 존재하지 않음에도 불구하고 그럴듯한 모서리 쌍을 찾아내어 부정확한 경계 상자를 대량으로 생성하는 경향이 있었다.23 이러한 오탐지(False Discovery)는 특히 객체의 특징이 뚜렷하지 않은 작은 객체에서 두드러지게 나타났으며, 모델의 정밀도를 심각하게 저해하는 주요 원인이었다.23 ‘Keypoint Triplets’ CenterNet은 바로 이 문제를 해결하고자 하는 명확한 동기에서 출발했다.</p>
<h3>8.2  Triplet 표현과 검증 메커니즘</h3>
<p>이 모델의 핵심 아이디어는 모서리 쌍만으로는 불충분한 객체 존재의 증거를 ’중심점’이라는 추가 정보를 통해 보강하는 것이다.20</p>
<ul>
<li><strong>Triplet 표현</strong>: 객체는 더 이상 모서리 ’쌍(pair)’이 아닌, 좌상단 모서리, 우하단 모서리, 그리고 중심점이라는 ’삼중주(triplet)’로 표현된다. 이를 위해 CornerNet의 아키텍처에 중심점을 예측하기 위한 별도의 히트맵 브랜치(branch)가 추가된다.</li>
<li><strong>검증 메커니즘</strong>: 추론 과정은 2단계의 논리적 흐름을 따른다.</li>
</ul>
<ol>
<li><strong>후보 생성</strong>: 먼저, CornerNet과 동일한 방식으로 탐지된 좌상단 및 우하단 모서리들을 페어링하여 다수의 후보 경계 상자를 생성한다.</li>
<li><strong>중심점 검증</strong>: 그 다음, 생성된 각 후보 경계 상자에 대해 그 중앙 영역(central region)을 정의하고, 이 영역 내에 해당 후보와 동일한 클래스로 예측된 ’중심점’이 존재하는지를 확인한다.21 만약 중심점이 발견되면, 해당 후보 상자는 실제 객체일 가능성이 높다고 판단하여 최종 탐지 결과로 채택한다. 반면, 중앙 영역에 중심점이 없다면 이는 오탐지일 가능성이 높으므로 해당 후보 상자를 제거한다.42</li>
</ol>
<p>이러한 검증 메커니즘은 마치 2-단계 탐지기에서 RPN이 제안한 후보 영역(RoI)에 대해 RoI Pooling/Align을 통해 내부 특징을 다시 한번 살펴보는 것과 유사한 역할을 한다. 하지만 이를 별도의 스테이지 분리 없이, 단일 네트워크 내에서 효율적으로 수행한다는 점에서 1-단계 탐지기의 장점을 유지한다.23</p>
<h3>8.3  특화된 풀링 모듈</h3>
<p>객체의 중심과 모서리 정보를 더 효과적으로 포착하기 위해, 이 모델은 두 가지 독창적인 풀링 모듈을 제안했다.</p>
<ul>
<li><strong>Center Pooling</strong>: 객체의 기하학적 중심이 반드시 뚜렷한 시각적 특징을 갖는 것은 아니다. 예를 들어, 사람의 중심은 보통 특징이 모호한 몸통에 위치하지만, 정작 중요한 시각적 정보는 머리나 어깨에 있다. Center Pooling은 이러한 문제를 해결하기 위해 고안되었다. 특정 위치의 특징을 계산할 때, 그 위치 자체의 값뿐만 아니라 해당 위치를 지나는 수평 및 수직 방향 전체에서 특징의 최댓값을 각각 찾아 더해준다.20 이를 통해 중심점은 객체 전체에 걸친 더 넓은 문맥 정보를 포착할 수 있게 되어, 더 풍부하고 식별 가능한 특징 표현을 갖게 된다.21</li>
<li><strong>Cascade Corner Pooling</strong>: CornerNet의 Corner Pooling은 객체의 경계를 따라 특징의 최댓값을 찾는 방식으로 작동한다. 이는 모서리가 객체의 경계 정보를 잘 포착하게 하지만, 객체 내부의 모습에 대해서는 ’장님’이 되는 문제를 낳는다. Cascade Corner Pooling은 이 점을 개선한다. 이 모듈은 먼저 객체의 경계 방향으로 최댓값을 찾고(Corner Pooling과 동일), 그 다음 그 최댓값이 발견된 위치에서 객체 내부를 향하는 방향으로 다시 한번 최댓값을 찾는다. 최종적으로 이 두 최댓값을 더하여 모서리의 특징으로 사용한다.23 이 과정을 통해 모서리점은 객체의 경계 정보와 내부의 시각적 패턴을 모두 참고할 수 있게 되어, 더 정확한 위치 예측이 가능해진다.20</li>
</ul>
<p>이러한 개선점들을 통해 ‘Keypoint Triplets’ CenterNet은 MS-COCO 데이터셋에서 47.0% AP를 달성하여, 당시 모든 1-단계 탐지기들을 큰 차이로 능가하는 성능을 보였다. 이는 CornerNet 대비 약 4.9% AP의 성능 향상에 해당하며, 중심점 정보의 추가가 오탐지를 줄이고 정확도를 높이는 데 매우 효과적이었음을 입증한다.18</p>
<h2>9.  결론: CenterNet이 남긴 유산과 미래 전망</h2>
<p>2019년에 등장한 두 ’CenterNet’은 객체 탐지 분야의 연구 지형을 바꾸는 중요한 변곡점이 되었다. 이들은 Anchor 기반 패러다임의 한계를 명확히 지적하고, keypoint 기반의 Anchor-Free 접근법이 나아갈 새로운 방향을 제시했다. 특히 ’Objects as Points’가 제안한 철학은 그 이후의 수많은 연구에 깊은 영향을 미쳤다.</p>
<h3>9.1  CenterNet의 영향과 유산</h3>
<ul>
<li><strong>’Objects as Points’의 유산</strong>: 이 모델의 가장 큰 유산은 객체 탐지를 ’점 추정’이라는 극도로 단순한 문제로 재정의함으로써, Anchor-Free 패러다임을 학계와 산업계의 주류로 끌어올렸다는 점이다.15 NMS 후처리를 제거할 수 있다는 가능성을 실증적으로 보여줌으로써, 완전한 end-to-end 학습 및 추론 파이프라인이라는 이상에 한 걸음 더 다가섰다. 또한, 그 놀라운 구조적 유연성과 확장성은 3D 탐지(CenterPoint), 자세 추정 등 다양한 비전 과업을 하나의 통합된 프레임워크로 해결할 수 있는 길을 열어주었다. 이는 후속 연구들이 복잡한 문제를 더 단순하고 근본적인 방식으로 접근하도록 영감을 주었다.</li>
<li><strong>’Keypoint Triplets’의 유산</strong>: 이 모델은 keypoint 기반 탐지기가 직면했던 구체적인 문제, 즉 오탐지율을 어떻게 효과적으로 줄일 수 있는지에 대한 실용적인 해법을 제시했다. 객체 내부의 시각적 정보를 활용하여 후보를 ’검증’한다는 아이디어와 이를 구현하기 위한 Center Pooling, Cascade Corner Pooling과 같은 특화된 모듈은 keypoint 기반 탐지기의 정확도를 한 단계 끌어올리는 중요한 기술적 진보를 이루었다. 이는 CornerNet 계열 연구의 발전에 직접적으로 기여했으며, keypoint 표현을 풍부하게 만드는 방법에 대한 중요한 통찰을 제공했다.</li>
</ul>
<h3>9.2  미래 연구 방향 및 전망</h3>
<p>CenterNet이 제시한 패러다임은 여전히 활발한 연구 주제이며, 그 한계를 극복하고 새로운 영역으로 확장하려는 노력이 계속되고 있다.</p>
<ul>
<li><strong>한계 극복</strong>: CenterNet의 가장 큰 약점인 ‘중첩 객체’ 문제를 해결하기 위한 연구가 활발히 진행 중이다. 초기 특징 맵에 객체 중심점의 중첩을 완화하는 Differentiation 모듈을 도입하거나 9, 다중 스케일 특징을 효과적으로 융합하기 위해 Feature Pyramid Network(FPN) 구조를 통합하는 등의 개선 방안이 제시되었다.34 이러한 접근법들은 CenterNet의 재현율을 높이고, 더 빽빽하고 복잡한 장면에 강건한 탐지기를 만드는 데 중요한 역할을 할 것으로 기대된다.</li>
<li><strong>새로운 응용 분야로의 확장</strong>: CenterNet의 유연한 프레임워크는 계속해서 새로운 도메인에 적용되고 있다. 기존의 RGB 이미지를 넘어, 이벤트 카메라 데이터의 희소하고 비동기적인 특성에 맞춰 설계된 Spiking CenterNet 43이나, 문서 이미지에서 테이블의 복잡한 구조를 인식하기 위한 Cycle-CenterNet 44과 같은 연구들은 CenterNet의 철학이 다양한 데이터 형태와 과업에 적용될 수 있는 강력한 잠재력을 가지고 있음을 보여준다.</li>
<li><strong>탐지 신뢰도에 대한 이해</strong>: 미래의 객체 탐지는 단순히 ’무엇이 어디에 있는지’를 넘어, ’그 예측을 얼마나 신뢰할 수 있는지’를 함께 제공하는 방향으로 나아갈 것이다. Evidential Deep Learning을 CenterNet에 결합하여 분류 및 회귀에 대한 불확실성(uncertainty)을 직접 추정하는 EvCenterNet 45과 같은 연구는 이러한 방향의 초기 시도다. 이는 자율 주행이나 의료 진단과 같이 안전이 중요한(safety-critical) 분야에서 모델의 예측을 해석하고 신뢰하는 데 필수적인 정보를 제공할 수 있다.</li>
</ul>
<p>결론적으로, CenterNet은 객체 탐지 분야에 ’단순함의 힘’을 증명한 기념비적인 모델이다. 복잡한 장치들을 걷어내고 문제의 본질에 집중함으로써, 더 빠르고, 더 효율적이며, 더 확장 가능한 새로운 세대의 탐지기들을 위한 길을 열었다. 앞으로도 CenterNet의 핵심 철학은 다양한 형태로 변주되며 컴퓨터 비전 분야의 발전을 이끌어 나갈 것으로 전망된다.</p>
<h2>10. 참고 자료</h2>
<ol>
<li>(PDF) Speed and Accuracy Comparison of Person Detection Using Pretrained CenterNet and Yolov3 - ResearchGate, https://www.researchgate.net/publication/377906801_Speed_and_Accuracy_Comparison_of_Person_Detection_Using_Pretrained_CenterNet_and_Yolov3</li>
<li>Criteria to choose a object detection Method - Stack Overflow, https://stackoverflow.com/questions/64839039/criteria-to-choose-a-object-detection-method</li>
<li>[1904.07850] Objects as Points - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/1904.07850</li>
<li>State of Deep Learning for Object Detection - You Should Consider CenterNets!, https://victordibia.com/blog/state-of-object-detection/</li>
<li>CenterNet: Objects as Points – Anchor Free Object Detection Explained - LearnOpenCV, https://learnopencv.com/centernet-anchor-free-object-detection-explained/</li>
<li>Using Anchor-Free Object Detectors to Detect Surface Defects - MDPI, https://www.mdpi.com/2227-9717/12/12/2817</li>
<li>Disadvantages of anchor-based detectors. The blue rectangle represents… - ResearchGate, https://www.researchgate.net/figure/Disadvantages-of-anchor-based-detectors-The-blue-rectangle-represents-the-ground-truth_fig1_359008704</li>
<li>ANPR using NanoDet, an Anchor-free Object Detection model | ignitarium.com - Medium, https://medium.com/@Ignitarium/anpr-using-nanodet-an-anchor-free-object-detection-model-ignitarium-com-ebd383b05e5d</li>
<li>Object Detection Based on Center Point Proposals - ResearchGate, https://www.researchgate.net/publication/347401213_Object_Detection_Based_on_Center_Point_Proposals</li>
<li>[PDF] Objects as Points - Semantic Scholar, https://www.semanticscholar.org/paper/Objects-as-Points-Zhou-Wang/6a2e2fd1b5bb11224daef98b3fb6d029f68a73f2</li>
<li>Gradient Corner Pooling for Keypoint-Based Object Detection - AAAI Publications, https://ojs.aaai.org/index.php/AAAI/article/view/25231/25003</li>
<li>[D] Two different methods with the same name: CenterNet : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/eolkee/d_two_different_methods_with_the_same_name/</li>
<li>[1904.07850] Objects as Points - arXiv, https://arxiv.org/abs/1904.07850</li>
<li>dreamway/CenterNet-objects-as-points - GitHub, https://github.com/dreamway/CenterNet-objects-as-points</li>
<li>CenterNet and Its Variants. Author: Guanghan Ning - Medium, https://medium.com/analytics-vidhya/centernet-and-its-variants-9c2461ad6486</li>
<li>CenterNet Explained | Towards Data Science, https://towardsdatascience.com/centernet-explained-a7386f368962/</li>
<li>What are the differences between Yolo v1 and CenterNet? - AI Stack Exchange, https://ai.stackexchange.com/questions/16061/what-are-the-differences-between-yolo-v1-and-centernet</li>
<li>Codes for our paper “CenterNet: Keypoint Triplets for Object Detection” - GitHub, https://github.com/Duankaiwen/CenterNet</li>
<li>[1904.08189] CenterNet: Keypoint Triplets for Object Detection - arXiv, https://arxiv.org/abs/1904.08189</li>
<li>CenterNet: Keypoint Triplets for Object Detection - CVF Open Access, https://openaccess.thecvf.com/content_ICCV_2019/papers/Duan_CenterNet_Keypoint_Triplets_for_Object_Detection_ICCV_2019_paper.pdf</li>
<li>arXiv:1904.08189v3 [cs.CV] 19 Apr 2019, http://arxiv.org/pdf/1904.08189</li>
<li>CenterNet: Object Detection with Keypoint Triplets | Request PDF - ResearchGate, https://www.researchgate.net/publication/332494090_CenterNet_Object_Detection_with_Keypoint_Triplets</li>
<li>CenterNet Keypoint Triplets for Object Detection Review | by Tony Shin - Medium, https://medium.com/data-science/centernet-keypoint-triplets-for-object-detection-review-a314a8e4d4b0</li>
<li>CenterNet | docs.binaexperts.com - Introduction, https://docs.binaexperts.com/train/models/centernet</li>
<li>Overall structure of CenterNet based on Resnet50 - ResearchGate, https://www.researchgate.net/figure/Overall-structure-of-CenterNet-based-on-Resnet50_fig1_388883351</li>
<li>Objects as points | Learning-Deep-Learning, https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/centernet.html</li>
<li>CMC | Free Full-Text | Vehicle Detection in Challenging Scenes Using CenterNet Based Approach - Tech Science Press, https://www.techscience.com/cmc/v74n2/50185/html</li>
<li>A Benchmark for Maritime Object Detection with Centernet on an Improved Dataset, ABOships-PLUS - MDPI, https://www.mdpi.com/2077-1312/11/9/1638</li>
<li>CenterNet Heatmap Propagation for Real-time Video Object Detection - European Computer Vision Association, https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700222.pdf</li>
<li>Understanding the losses in CenterNet Architecture - GitHub Pages, <a href="https://akashprakas.github.io/akashBlog/posts/2022-08-24-understanding%20the%20losses%20in%20centernet%20architecture.html">https://akashprakas.github.io/akashBlog/posts/2022-08-24-understanding%20the%20losses%20in%20centernet%20architecture.html</a></li>
<li>CenterNet, Explained. CenterNet is an anchorless object… | by Uri Almog | TDS Archive, https://medium.com/data-science/centernet-explained-a7386f368962</li>
<li>A comparison of the Faster R-CNN, SSD, CenterNet, and YOLOv3 network frameworks., https://www.researchgate.net/figure/A-comparison-of-the-Faster-R-CNN-SSD-CenterNet-and-YOLOv3-network-frameworks_fig2_352279048</li>
<li>YOLO11 Anchor-Free Detection: Benefits - Ultralytics, https://www.ultralytics.com/blog/benefits-ultralytics-yolo11-being-anchor-free-detector</li>
<li>Separable CenterNet Detection Network Based on MobileNetV3 …, https://www.mdpi.com/2227-7390/12/16/2524</li>
<li>Center-Based 3D Object Detection and Tracking - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2021/papers/Yin_Center-Based_3D_Object_Detection_and_Tracking_CVPR_2021_paper.pdf</li>
<li>[2006.11275] Center-based 3D Object Detection and Tracking - arXiv, https://arxiv.org/abs/2006.11275</li>
<li>Center-based 3D Object Detection and Tracking | papers_we_read - VLG, https://vlgiitr.github.io/papers_we_read/summaries/3d_object_detection.html</li>
<li>CenterPoint: Center-based 3D Object Detection and Tracking Paper Review - Medium, https://medium.com/@parkie0517/centerpoint-center-based-3d-object-detection-and-tracking-paper-review-c50a0e187167</li>
<li>aiimageyaohui/Emergency_homework_human_pose_detection: human pose detection by centernet(anchor free) - GitHub, https://github.com/aiimageyaohui/Emergency_homework_human_pose_detection</li>
<li>Human Pose Estimation: Approaches, Use Cases, Implementation Tips - ITRex Group, https://itrexgroup.com/blog/human-pose-estimation-use-cases-implementation-tips/</li>
<li>CenterNet Pose Estimation Nets - Wolfram Neural Net Repository, https://resources.wolframcloud.com/NeuralNetRepository/resources/ad9e7395-15c4-4423-b9b5-f3f82e31be30/</li>
<li>Review — CenterNet: Keypoint Triplets for Object Detection (Object …, https://medium.com/nerd-for-tech/review-centernet-keypoint-triplets-for-object-detection-object-detection-26feee780efc</li>
<li>Spiking CenterNet: A Distillation-boosted Spiking Neural Network for Object Detection - arXiv, https://arxiv.org/html/2402.01287v2</li>
<li>[2109.02199] Parsing Table Structures in the Wild - arXiv, https://arxiv.org/abs/2109.02199</li>
<li>EvCenterNet: Uncertainty Estimation for Object Detection using Evidential Learning - arXiv, https://arxiv.org/pdf/2303.03037</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>