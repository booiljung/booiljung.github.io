<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:RetinaNet 조밀한 객체 탐지 (2017)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>RetinaNet 조밀한 객체 탐지 (2017)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>RetinaNet 조밀한 객체 탐지 (2017)</span></nav>
                </div>
            </header>
            <article>
                <h1>RetinaNet 조밀한 객체 탐지 (2017)</h1>
<h2>1.  1단계 객체 탐지기의 딜레마와 RetinaNet의 등장</h2>
<p>딥러닝 기반 객체 탐지(Object Detection) 분야는 역사적으로 2단계(Two-stage)와 1단계(One-stage) 탐지기라는 두 가지 주요 패러다임으로 양분되어 발전해왔다. 이 두 접근 방식은 정확도(accuracy)와 속도(speed) 사이의 근본적인 상충 관계(trade-off)를 형성하며 기술의 발전을 이끌었다.1</p>
<p>2단계 탐지기는 Faster R-CNN과 같은 R-CNN 계열 모델로 대표된다.3 이 방식은 먼저 후보 영역 제안 네트워크(Region Proposal Network, RPN)를 통해 객체가 존재할 가능성이 높은 영역을 선별하고, 이후 단계에서 해당 영역에 대한 정밀한 분류(classification)와 경계 상자 회귀(bounding box regression)를 수행한다. 이러한 다단계 구조는 높은 정확도를 달성하는 데 유리했지만, 복잡한 파이프라인으로 인해 추론 속도가 느려 실시간 적용에는 명백한 한계가 존재했다.1</p>
<p>반면, 1단계 탐지기는 YOLO, SSD와 같은 모델들이 대표적이다.3 이 접근법은 후보 영역 제안 단계를 생략하고, 입력 이미지 전체에 대해 미리 정의된 조밀한 앵커 박스(anchor box) 그리드를 사용하여 객체의 위치와 클래스를 한 번의 네트워크 통과로 동시에 예측한다. 이는 구조적 간결함과 빠른 속도를 보장했지만, 2단계 탐지기에 비해 정확도가 현저히 떨어지는 경향을 보였다.1</p>
<p>RetinaNet 논문의 저자들은 1단계 탐지기의 낮은 정확도가 아키텍처 자체의 본질적인 표현력 부족 때문이 아니라, 학습 과정에서 발생하는 ’극심한 전경-배경 클래스 불균형(extreme foreground-background class imbalance)’이라는 근본적인 문제에서 기인한다고 진단했다.7 1단계 탐지기는 이미지 한 장당 수만에서 수십만 개에 달하는 방대한 수의 후보 앵커를 평가하는데, 이 중 극소수만이 실제 객체(전경, foreground)를 포함하고 나머지 압도적 다수는 의미 없는 배경(배경, background)이다.5 이로 인해 학습 과정에서 모델은 소수의 중요한 객체 샘플보다 다수의 쉬운 배경 샘플에 의해 압도되어, 효과적인 학습이 저해된다.</p>
<p>이러한 문제의 본질을 꿰뚫어 본 연구진은, 당시 지배적이었던 더 깊고 복잡한 네트워크 아키텍처를 통해 성능을 개선하려는 연구 동향에서 벗어나, 학습의 핵심인 손실 함수(loss function)를 재설계하는 혁신적인 접근법을 제시했다. ’Focal Loss’라는 새로운 손실 함수를 도입하여, 학습 과정에서 압도적 다수를 차지하는 ‘쉬운 배경(easy negatives)’ 샘플이 전체 손실에 미치는 기여도를 동적으로 대폭 감소시키고, 학습이 어려운 소수의 ’어려운 샘플(hard examples)’에 모델이 집중하도록 유도한 것이다.8 이는 단순히 새로운 모델을 제시한 것을 넘어, 문제의 근본 원인을 파악하고 ‘학습 동역학(learning dynamics)’ 자체를 제어하는 것이 성능 향상의 핵심일 수 있다는 패러다임의 전환을 의미했다. 연구진은 의도적으로 ResNet-FPN이라는 강력하지만 표준적인 아키텍처를 채택함으로써, 성능 향상이 오롯이 Focal Loss 덕분임을 명확히 입증하는 과학적이고 전략적인 선택을 했다.8</p>
<p>본 안내서는 RetinaNet의 핵심 기여를 심층적으로 분석하고자 한다. 먼저 1단계 탐지기가 직면한 클래스 불균형 문제의 본질을 파헤치고, 이를 해결하기 위한 Focal Loss의 수학적 원리와 하이퍼파라미터의 역할을 상세히 설명한다. 이후, Focal Loss의 효과를 입증하기 위해 설계된 RetinaNet의 아키텍처를 해부하고, 마지막으로 COCO 데이터셋에서의 압도적인 성능 평가 결과를 통해 RetinaNet이 객체 탐지 분야에 미친 영향과 그 유산을 고찰한다. RetinaNet은 Focal Loss를 통해 1단계 탐지기가 2단계 탐지기의 정확도를 능가하면서도 빠른 속도를 유지할 수 있음을 최초로 증명한 기념비적인 모델이다.8</p>
<h2>2.  핵심 문제: 조밀한 탐지기에서의 클래스 불균형</h2>
<h3>2.1 조밀한 샘플링의 필연적 결과</h3>
<p>1단계 탐지기의 핵심 철학은 객체의 위치와 크기를 놓치지 않기 위해 입력 이미지 전체에 걸쳐 다양한 크기와 종횡비의 앵커 박스를 조밀하게(densely) 배치하는 것이다. 이로 인해 이미지 한 장당 평가해야 할 후보 영역의 수는 약 10만 개(<code>~100k</code>)에 달한다.5 반면, 2단계 탐지기는 RPN을 통해 후보 영역을 약 2천 개(<code>~2k</code>) 수준으로 미리 효과적으로 걸러내므로, 후속 분류기에서 다루어야 할 클래스 불균형 문제가 상대적으로 덜 심각하다.5 이 구조적 차이가 두 패러다임 간의 성능 격차를 유발하는 핵심 원인이다.</p>
<h3>2.2 표준 교차 엔트로피 손실의 한계</h3>
<p>분류 문제에서 널리 사용되는 표준 교차 엔트로피(Cross-Entropy, CE) 손실 함수는 모든 학습 샘플에 동일한 가중치를 부여한다. 이진 분류의 경우 CE 손실은 다음과 같이 정의된다.<br />
<span class="math math-display">
\text{CE}(p, y) = \begin{cases} -\log(p) &amp; \text{if } y=1 \\ -\log(1-p) &amp; \text{otherwise} \end{cases}
</span><br />
표기상의 편의를 위해, 정답 클래스에 대한 예측 확률을 <span class="math math-inline">p_t</span>로 정의하면(<span class="math math-inline">p_t = p</span> if <span class="math math-inline">y=1</span>, <span class="math math-inline">p_t = 1-p</span> if <span class="math math-inline">y \neq 1</span>) CE 손실은 <span class="math math-inline">CE(p_t) = -\log(p_t)</span>로 간결하게 표현할 수 있다.</p>
<p>문제는 10만 개의 앵커 중 실제 객체(전경, positive sample)는 불과 수십 개에 불과하고, 나머지 99.9% 이상이 모두 배경(배경, negative sample)이라는 점이다. 이 배경 샘플의 대다수는 모델이 매우 높은 확률로 ’배경’이라고 쉽게 판단할 수 있는 ’쉬운 배경(easy negatives)’이다.16 개별 ‘쉬운 배경’ 샘플이 생성하는 손실 값(<span class="math math-inline">p_t</span>가 1에 가까워 <span class="math math-inline">-\log(p_t)</span>는 0에 가깝지만 0은 아님)은 매우 작다. 하지만 이들의 수가 압도적으로 많기 때문에, 이 작은 손실 값들이 모두 합쳐지면 전체 손실(total loss)의 대부분을 차지하게 된다. 이는 소수의 ‘어려운’ 전경 샘플에서 발생하는 큰 손실 값을 압도(overwhelm)하는 ’손실의 지배 현상’을 초래한다.10</p>
<p>이 현상은 단순히 손실 값의 문제를 넘어, 모델 학습의 방향을 결정하는 그래디언트(gradient)의 문제로 이어진다. 딥러닝 학습은 손실 함수를 최소화하는 방향으로 가중치를 업데이트하는 과정이며, 이 업데이트 방향은 총 손실에 대한 그래디언트에 의해 결정된다.18 ‘쉬운 배경’ 샘플들의 총 손실이 전체 손실을 지배한다는 것은, 이들로부터 발생하는 그래디언트의 총합이 전체 그래디언트의 방향을 지배하게 됨을 의미한다. 결과적으로, 모델은 이미 잘하고 있는 ’쉬운 배경’을 더 잘 맞추는 방향으로만 미세하게 업데이트될 뿐, 정작 배워야 할 ‘어려운’ 전경 객체를 탐지하는 방향으로는 거의 학습이 이루어지지 않는다. 모델은 중요한 객체를 탐지하는 법을 배우기보다, 그저 모든 것을 배경으로 예측하도록 학습되어 성능이 저하되는 것이다.14</p>
<h3>2.3 기존 해결책들의 시도와 그 한계</h3>
<p>이러한 클래스 불균형 문제를 해결하기 위해 기존에도 여러 방법이 시도되었다.</p>
<ul>
<li><strong>하드 네거티브 마이닝 (Hard Negative Mining, OHEM):</strong> 학습 과정에서 손실 값이 큰, 즉 모델이 분류하기 어려워하는 배경 샘플(hard negatives)만을 선별하여 학습에 사용하는 방식이다. SSD와 같은 모델에서 사용되었으나, 이 방식은 대부분의 ‘쉬운’ 배경 샘플 정보를 버리게 되어 학습이 비효율적이고, 어떤 샘플을 선택할지 결정하는 과정이 복잡하며 종종 학습을 불안정하게 만들 수 있다.5</li>
<li><strong>가중 교차 엔트로피 (Balanced Cross-Entropy):</strong> 클래스 불균형을 완화하기 위해 소수 클래스(전경)에 가중치 <span class="math math-inline">α</span>를, 다수 클래스(배경)에 <span class="math math-inline">1-α</span>를 부여하는 간단하고 일반적인 방법이다.10 수식은 다음과 같다.</li>
</ul>
<p><span class="math math-display">
\text{CE}(p_t) = -\alpha_t \log(p_t)
</span></p>
<p>여기서 <span class="math math-inline">α_t</span>는 <span class="math math-inline">y=1</span>일 때 <span class="math math-inline">α</span>, 그 외에는 <span class="math math-inline">1-α</span> 값을 갖는다. 이 방식은 전경과 배경이라는 클래스 레벨의 불균형은 일부 보정할 수 있다. 하지만, 배경 클래스 내에 존재하는 압도적 다수의 ’쉬운 샘플’과 소수의 ’어려운 샘플’을 구분하지 못한다. 따라서 여전히 수많은 ’쉬운 배경’이 생성하는 손실의 총합이 전체 손실을 지배하는 근본적인 그래디언트 문제를 해결하지 못하는 한계를 지닌다.17</p>
<p>아래 표는 1단계와 2단계 탐지기의 핵심적인 차이점을 요약하여 RetinaNet의 탄생 배경을 명확히 한다.</p>
<p><strong>표 1: 1단계 vs. 2단계 객체 탐지기 비교</strong></p>
<table><thead><tr><th>특징 (Feature)</th><th>2단계 탐지기 (Two-Stage Detectors)</th><th>1단계 탐지기 (One-Stage Detectors)</th></tr></thead><tbody>
<tr><td><strong>방법론</strong></td><td>후보 영역 제안 후 분류 (Sequential)</td><td>위치 및 클래스 동시 예측 (Simultaneous)</td></tr>
<tr><td><strong>속도</strong></td><td>느림 2</td><td>빠름 2</td></tr>
<tr><td><strong>정확도 (전통적)</strong></td><td>높음 2</td><td>낮음 1</td></tr>
<tr><td><strong>후보 영역 수</strong></td><td>희소 (Sparse, ~2k) 5</td><td>조밀 (Dense, ~100k) 5</td></tr>
<tr><td><strong>클래스 불균형</strong></td><td>상대적으로 완화됨 7</td><td>극심함 5</td></tr>
<tr><td><strong>대표 모델</strong></td><td>R-CNN, Fast R-CNN, Faster R-CNN 3</td><td>YOLO, SSD, <strong>RetinaNet</strong> 3</td></tr>
</tbody></table>
<h2>3.  해결책: Focal Loss 심층 분석</h2>
<p>클래스 불균형 문제, 특히 ‘쉬운 배경’ 샘플에 의한 손실 지배 현상을 해결하기 위해 RetinaNet은 ’Focal Loss’라는 새로운 손실 함수를 제안했다. Focal Loss는 표준 교차 엔트로피 손실에 동적인 스케일링 계수를 추가하여, 학습 과정에서 어려운 샘플에 집중하도록 설계되었다.</p>
<h3>3.1 Focal Loss의 수학적 정의와 직관</h3>
<p>Focal Loss는 표준 교차 엔트로피 손실에 ‘변조 계수(modulating factor)’ <span class="math math-inline">(1 - p_t)^\gamma</span>를 곱하는 간단한 형태로 정의된다.7 이 계수는 모델이 정답을 얼마나 확신하는지(<span class="math math-inline">p_t</span>)에 따라 동적으로 변하며, 잘 분류된 샘플(well-classified examples, <span class="math math-inline">p_t</span> → 1)의 손실 기여도를 대폭 감소시키는 역할을 한다.</p>
<ul>
<li>
<p><strong>표준 교차 엔트로피 (CE):</strong><br />
<span class="math math-display">
\text{CE}(p_t) = -\log(p_t)
</span></p>
</li>
<li>
<p><strong>Focal Loss (FL):</strong><br />
<span class="math math-display">
\text{FL}(p_t) = -(1-p_t)^\gamma \log(p_t)
</span></p>
</li>
</ul>
<p>여기에 클래스 간의 정적인 불균형을 보정하기 위한 가중치 <span class="math math-inline">α_t</span>를 추가하면 최종 형태는 다음과 같다.10</p>
<ul>
<li><strong>α-가중 Focal Loss:</strong><br />
<span class="math math-display">
\text{FL}(p_t) = -\alpha_t (1-p_t)^\gamma \log(p_t)
</span></li>
</ul>
<h3>3.2 주요 하이퍼파라미터 상세 분석</h3>
<p>Focal Loss의 동작은 두 개의 핵심 하이퍼파라미터, <span class="math math-inline">\gamma</span>(gamma)와 <span class="math math-inline">\alpha</span>(alpha)에 의해 제어된다.</p>
<h4>3.2.1 집중 파라미터 (Focusing Parameter) <span class="math math-inline">\gamma</span></h4>
<p><span class="math math-inline">\gamma</span>는 쉬운 샘플의 손실을 감소시키는 정도를 조절하는 가장 중요한 파라미터다. <span class="math math-inline">\gamma &gt; 0</span>일 때 효과가 나타나며, <span class="math math-inline">\gamma=0</span>이면 Focal Loss는 표준 교차 엔트로피와 동일해진다.7</p>
<p><span class="math math-inline">\gamma</span>의 동작 원리는 다음과 같다. 모델이 어떤 샘플을 높은 확률(<span class="math math-inline">p_t</span> → 1)로 정확하게 예측하면, <span class="math math-inline">(1-p_t)</span> 항은 0에 가까워진다. <span class="math math-inline">\gamma</span>가 클수록 이 작은 값은 기하급수적으로 더 작아져(<span class="math math-inline">(0.1)^2=0.01</span>, <span class="math math-inline">(0.1)^5=0.00001</span> 등) 해당 샘플의 손실 기여도를 거의 0으로 만든다. 이는 해당 샘플로부터 발생하는 그래디언트를 사실상 소멸시켜 가중치 업데이트에 영향을 주지 않도록 한다. 반면, 모델이 예측을 틀리면(<span class="math math-inline">p_t</span> → 0), <span class="math math-inline">(1-p_t)</span> 항은 1에 가까워져 변조 계수의 영향이 거의 없게 되고, 원래의 CE 손실 값이 거의 그대로 유지된다.15</p>
<p>이러한 메커니즘은 손실 함수의 역할을 재정의하는 중요한 의미를 갖는다. 기존의 CE 손실 함수는 ’오답에 대한 페널티’만 존재했다. 그러나 Focal Loss는 ’정답에 대한 보상’의 개념을 도입한다. 즉, 더 확실하게 맞추는(더 쉬운) 샘플에 대해 ’훨씬 더 큰 손실 감소’라는 차등적 보상을 부여함으로써, 모델이 이미 잘하고 있는 것에 대해서는 신경 쓰지 않고 아직 미흡한 부분에만 학습 자원을 집중하도록 강력하게 유도한다.19</p>
<p><span class="math math-inline">\gamma</span> 값을 높일수록(논문에서는 실험을 통해 <span class="math math-inline">\gamma=2</span>를 최적으로 제시) 이러한 효과는 더욱 강해지며, 모델은 수많은 쉬운 샘플들을 효과적으로 무시하고 소수의 어려운 샘플(hard positives/negatives) 학습에 집중하게 된다.7</p>
<h4>3.2.2 균형 파라미터 (Balancing Parameter) <span class="math math-inline">\alpha</span></h4>
<p><span class="math math-inline">\alpha</span>는 <span class="math math-inline">\gamma</span>와 독립적으로, 전경과 배경 클래스 간의 정적인(static) 불균형을 직접적으로 다루는 가중치다. 이는 Balanced Cross-Entropy에서 사용된 것과 동일한 역할을 하며, 보통 소수 클래스인 전경(positive) 샘플에 더 높은 가중치를 부여하는 방식으로 설정된다.10</p>
<p>논문의 실험 결과에 따르면, <span class="math math-inline">\gamma</span>를 통한 동적 스케일링과 <span class="math math-inline">\alpha</span>를 통한 정적 가중치 부여를 함께 사용했을 때 성능이 가장 좋았다. <span class="math math-inline">\alpha</span>가 클래스 간의 중요도 불균형을 조절하고, <span class="math math-inline">\gamma</span>가 샘플의 난이도에 따른 학습 집중도를 조절하는 상호 보완적인 역할을 수행하기 때문이다.17 논문에서는 <span class="math math-inline">\alpha=0.25</span>, <span class="math math-inline">\gamma=2</span>를 최적의 하이퍼파라미터 조합으로 제시했다.15</p>
<h2>4.  RetinaNet 아키텍처 상세 해부</h2>
<p>RetinaNet은 Focal Loss의 효과를 명확하게 입증하기 위해 설계된 간결하고 효율적인 1단계 탐지기다. 그 구조는 크게 (1) 특징 추출을 위한 백본 네트워크, (2) 다중 스케일 특징을 위한 특징 피라미드 네트워크(Feature Pyramid Network, FPN), 그리고 (3) 실제 예측을 수행하는 두 개의 태스크 특화 서브넷으로 구성된다.12</p>
<h3>4.1  백본 네트워크 (Backbone Network: ResNet)</h3>
<p>RetinaNet은 특징 추출기로서 ImageNet 데이터셋으로 사전 학습된 ResNet(주로 ResNet-50 또는 ResNet-101)을 사용한다.23 입력 이미지에 대해 깊은 컨볼루션 연산을 수행하여 계층적인 특징 맵(feature map)을 생성한다. ResNet의 중간 레이어들에서 생성된 서로 다른 해상도의 특징 맵(논문에서는 C3, C4, C5 출력)들이 다음 단계인 FPN으로 전달되어 다중 스케일 분석의 기반이 된다.23</p>
<h3>4.2  특징 피라미드 네트워크 (Feature Pyramid Network, FPN)</h3>
<p>FPN의 목적은 단일 해상도의 입력 이미지로부터 다양한 스케일의 객체를 효과적으로 탐지하기 위해, 의미론적으로 풍부한 다중 스케일 특징 피라미드를 구축하는 것이다.24 FPN은 상향식 경로, 하향식 경로, 그리고 측면 연결의 세 가지 핵심 요소로 구성된다.</p>
<ul>
<li><strong>상향식 경로 (Bottom-up Pathway):</strong> 이는 ResNet 백본의 정방향(feed-forward) 경로와 동일하다. 입력 이미지부터 시작하여 컨볼루션 레이어를 거치면서 특징 맵의 공간 해상도는 점차 줄어들고(e.g., 2배씩), 채널 수는 늘어나며, 의미론적 정보는 더욱 풍부해진다.23</li>
<li><strong>하향식 경로 (Top-down Pathway):</strong> 가장 상위 레벨(가장 작고 의미론적으로 강한)의 특징 맵부터 시작하여, 업샘플링(upsampling, 보통 최근접 이웃 보간법 사용)을 통해 공간 해상도를 점차 2배씩 높여간다.23 이 과정은 고수준의 의미 정보를 저수준의 특징 맵으로 전파하는 역할을 한다.</li>
<li><strong>측면 연결 (Lateral Connections):</strong> FPN의 핵심적인 혁신으로, 하향식 경로에서 업샘플링된 특징 맵을 상향식 경로의 동일한 공간 해상도를 가진 특징 맵과 결합한다. 이때, 상향식 경로의 특징 맵은 1x1 컨볼루션을 거쳐 채널 수를 맞춘 후, 하향식 특징 맵과 요소별 덧셈(element-wise addition)으로 융합된다. 이 측면 연결은 고수준의 강한 의미 정보와 저수준의 정교한 위치 정보를 효과적으로 결합하여 모든 스케일에서 풍부한 표현력을 가진 특징 맵을 생성하게 한다.23</li>
</ul>
<p>이 과정을 통해 최종적으로 P3, P4, P5, P6, P7의 5개 레벨로 구성된 특징 피라미드가 생성된다. 각 레벨은 서로 다른 스케일의 객체를 탐지하는 데 특화되어 있어, 모델이 크기가 다양한 객체들을 효과적으로 처리할 수 있게 한다.23</p>
<h3>4.3  태스크 특화 서브넷 (Task-Specific Subnets)</h3>
<p>FPN의 각 피라미드 레벨(P3부터 P7까지)에는 동일한 구조와 파라미터를 공유하는 두 개의 작은 FCN(Fully Convolutional Network) 서브넷이 독립적으로 연결된다.22</p>
<ul>
<li><strong>분류 서브넷 (Classification Subnet):</strong> 이 서브넷은 각 앵커 박스가 <span class="math math-inline">K</span>개의 객체 클래스 중 하나에 속할 확률 또는 배경일 확률을 예측한다.12 여러 개의 3x3 컨볼루션 레이어와 ReLU 활성화 함수로 구성되며, 최종적으로 <span class="math math-inline">K \times A</span>개의 채널을 가진 특징 맵을 출력한다. 여기서 <span class="math math-inline">K</span>는 클래스 수, <span class="math math-inline">A</span>는 각 위치에서의 앵커 수(보통 3가지 크기 × 3가지 종횡비 = 9)이다.</li>
<li><strong>박스 회귀 서브넷 (Box Regression Subnet):</strong> 이 서브넷은 각 앵커 박스로부터 가장 가까운 실제 객체(ground-truth) 박스까지의 위치 오프셋(중심점 <span class="math math-inline">x, y</span>, 너비 <span class="math math-inline">w</span>, 높이 <span class="math math-inline">h</span>의 4개 값)을 예측(회귀)한다.12 분류 서브넷과 유사한 FCN 구조를 가지며, 최종적으로 <span class="math math-inline">4 \times A</span>개의 채널을 가진 특징 맵을 출력한다.</li>
</ul>
<p>RetinaNet 아키텍처는 ’분리(Disentanglement)’와 ’공유(Sharing)’라는 두 가지 설계 원칙을 효과적으로 활용한다. 분류와 회귀라는 서로 다른 성격의 태스크를 위한 서브넷을 완전히 분리하여 각 태스크가 독립적으로 최적의 특징을 학습하도록 한다.12 동시에, 각 서브넷의 파라미터는 FPN의 모든 스케일 레벨에 걸쳐 공유되어 모델의 파라미터 수를 크게 줄이고 효율성을 높이며 과적합을 방지한다.25 이러한 ’태스크 간 분리’와 ’스케일 간 공유’의 조합은 RetinaNet이 복잡한 다중 스케일 객체 탐지 문제를 우아하고 효율적으로 해결할 수 있게 만든 핵심적인 설계 철학이다.</p>
<h2>5.  성능 평가 및 영향력 분석</h2>
<h3>5.1 실험 결과 및 성능 비교</h3>
<p>RetinaNet의 성능은 객체 탐지 분야의 표준 벤치마크인 MS COCO 데이터셋에서 엄격하게 평가되었다. 성능 지표는 다양한 IoU(Intersection over Union) 임계값에 대한 평균 정밀도(Average Precision, AP)를 사용했다.</p>
<p>실험 결과, ResNet-101-FPN을 백본으로 사용한 RetinaNet 모델은 COCO <code>test-dev</code> 스플릿에서 <strong>39.1 AP</strong>라는 놀라운 성능을 달성했다. 이는 당시 발표된 모든 1단계 및 2단계 단일 모델의 성능을 능가하는 수치였다. 그러면서도 초당 5 프레임(5 FPS)의 비교적 빠른 추론 속도를 유지하여, ’속도와 정확도는 반비례한다’는 객체 탐지 분야의 오랜 통념을 깨뜨린 혁신적인 결과로 평가받았다.8</p>
<p>아래 표 2는 RetinaNet을 당시의 주요 1단계 및 2단계 모델들과 비교한 결과이다. RetinaNet이 기존 SOTA 모델이었던 Faster R-CNN+++를 포함한 모든 모델을 능가했음을 명확히 보여준다.</p>
<p><strong>표 2: COCO <code>test-dev</code> 데이터셋 성능 비교</strong></p>
<table><thead><tr><th>모델 (Detector)</th><th>백본 (Backbone)</th><th>AP</th><th>AP<span class="math math-inline">_{50}</span></th><th>AP<span class="math math-inline">_{75}</span></th><th>AP<span class="math math-inline">_{S}</span></th><th>AP<span class="math math-inline">_{M}</span></th><th>AP<span class="math math-inline">_{L}</span></th><th>속도 (ms)</th></tr></thead><tbody>
<tr><td><strong>Two-Stage</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Faster R-CNN+++</td><td>ResNet-101-FPN</td><td>36.2</td><td>59.1</td><td>39.0</td><td>18.2</td><td>39.0</td><td>48.2</td><td>-</td></tr>
<tr><td><strong>One-Stage</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>YOLOv2</td><td>DarkNet-19</td><td>21.6</td><td>44.0</td><td>19.2</td><td>5.0</td><td>22.4</td><td>35.5</td><td>25</td></tr>
<tr><td>SSD513</td><td>ResNet-101-FPN</td><td>31.2</td><td>50.4</td><td>33.3</td><td>10.2</td><td>34.5</td><td>49.8</td><td>80</td></tr>
<tr><td><strong>RetinaNet (Ours)</strong></td><td><strong>ResNet-50-FPN</strong></td><td><strong>35.7</strong></td><td><strong>55.3</strong></td><td><strong>38.5</strong></td><td><strong>18.7</strong></td><td><strong>39.4</strong></td><td><strong>47.4</strong></td><td><strong>122</strong></td></tr>
<tr><td><strong>RetinaNet (Ours)</strong></td><td><strong>ResNet-101-FPN</strong></td><td><strong>39.1</strong></td><td><strong>59.1</strong></td><td><strong>42.3</strong></td><td><strong>21.8</strong></td><td><strong>42.7</strong></td><td><strong>50.2</strong></td><td><strong>198</strong></td></tr>
</tbody></table>
<p>이러한 결과는 단순히 새로운 SOTA를 달성했다는 점을 넘어, 객체 탐지기 연구의 ’파레토 최적 전선(Pareto frontier)’을 새롭게 정의했다는 데 더 큰 의미가 있다. RetinaNet 이전의 모델들은 속도-정확도 그래프 위에서 2단계 모델은 ‘고정확도-저속도’ 영역에, 1단계 모델은 ‘저정확도-고속도’ 영역에 흩어져 있었다.8 RetinaNet은 이 그래프의 우측 상단, 즉 ‘고정확도-고속도’ 영역에 새로운 기준점을 제시하며 기존의 모든 탐지기들을 아래에 두는 ’상위 포락선(upper envelope)’을 형성했다.8 이는 특정 속도에서는 RetinaNet이 가장 높은 정확도를, 특정 정확도에서는 RetinaNet이 가장 빠른 속도를 제공함을 의미하며, 향후 모델들이 지향해야 할 새로운 성능 기준을 제시한 것이다.</p>
<h3>5.2 Focal Loss 효과 검증 (Ablation Study)</h3>
<p>논문에서는 성능 향상이 전적으로 Focal Loss 덕분임을 과학적으로 입증하기 위해 체계적인 분해 연구(Ablation Study)를 수행했다. 동일한 RetinaNet 아키텍처에서 손실 함수만 교체하며 성능을 비교한 결과는 아래 표 3과 같다.</p>
<p><strong>표 3: Focal Loss 분해 연구 (ResNet-50-FPN 백본 기준)</strong></p>
<table><thead><tr><th>손실 함수 (Loss Function)</th><th><span class="math math-inline">\alpha</span></th><th><span class="math math-inline">\gamma</span></th><th>AP</th></tr></thead><tbody>
<tr><td>Cross Entropy</td><td>-</td><td>0</td><td>30.2</td></tr>
<tr><td><span class="math math-inline">\alpha</span>-Balanced CE</td><td>0.25</td><td>0</td><td>31.1</td></tr>
<tr><td>Focal Loss</td><td>0.25</td><td>0.5</td><td>34.0</td></tr>
<tr><td>Focal Loss</td><td>0.25</td><td>1.0</td><td>34.6</td></tr>
<tr><td><strong>Focal Loss</strong></td><td><strong>0.25</strong></td><td><strong>2.0</strong></td><td><strong>35.7</strong></td></tr>
<tr><td>Focal Loss</td><td>0.25</td><td>5.0</td><td>34.3</td></tr>
</tbody></table>
<p>표준 교차 엔트로피(CE)를 사용했을 때 AP는 30.2로 매우 낮았다. <span class="math math-inline">\alpha</span>-가중 CE를 사용하면 성능이 31.1 AP로 소폭 향상되었다. 그러나 Focal Loss를 도입하고 <span class="math math-inline">\gamma</span> 값을 0.5에서 5까지 변화시키며 실험한 결과, <span class="math math-inline">\gamma=2</span>일 때 AP가 35.7로 극적으로 향상되며 최적의 성능을 보였다. 이는 <span class="math math-inline">\gamma</span> 파라미터가 쉬운 샘플의 손실을 효과적으로 억제하여 학습을 안정시키고 성능을 비약적으로 끌어올렸음을 명확히 보여주는 증거이다.</p>
<h2>6.  결론: 객체 탐지 분야에 남긴 유산</h2>
<p>RetinaNet은 ’Focal Loss’라는 혁신적인 손실 함수를 통해 1단계 객체 탐지기의 고질적인 문제였던 극심한 클래스 불균형을 효과적으로 해결했다. 이를 통해 1단계 탐지기가 2단계 탐지기의 정확도를 능가할 수 있음을 최초로 증명했으며, 속도와 정확도 사이의 오랜 상충 관계를 극복하는 새로운 가능성을 열었다.</p>
<p>RetinaNet의 성공은 객체 탐지 연구 커뮤니티에 두 가지 중요한 패러다임의 전환을 가져왔다.</p>
<p>첫째, <strong>1단계 탐지기의 재평가</strong>를 이끌었다. 1단계 방식이 본질적으로 정확도가 낮은 것이 아니라, 적절한 학습 전략, 특히 손실 함수의 부재가 성능의 발목을 잡고 있었음을 밝혔다. 이는 이후 YOLOv3, EfficientDet 등 고성능 1단계 탐지기 연구가 활발히 이루어지는 기폭제가 되었다.</p>
<p>둘째, <strong>손실 함수 설계의 중요성</strong>을 각인시켰다. 복잡한 네트워크 아키텍처 설계에 매몰되지 않고, 학습 과정의 근본적인 문제를 해결하는 손실 함수 설계가 SOTA(State-of-the-art) 성능을 달성하는 데 얼마나 중요한지를 명백히 보여주었다.</p>
<p>Focal Loss의 영향력은 객체 탐지 분야에만 머무르지 않는다. 이는 의료 영상 분석에서의 병변 탐지, 금융 분야의 사기 탐지, 제조 공정의 불량 검출 등 클래스 불균형이 심각한 다양한 딥러닝 응용 분야에서 표준적인 해결책 중 하나로 널리 채택되었다. 이는 RetinaNet이 단순히 하나의 뛰어난 모델을 제시한 것을 넘어, 딥러닝 분야 전반에 유용한 핵심 도구를 제공했음을 의미한다.</p>
<p>결론적으로, RetinaNet과 Focal Loss는 딥러닝 기반 객체 탐지의 역사에서 중요한 이정표로 기록된다. 이는 문제의 본질을 꿰뚫고, 가장 근본적인 지점에서 해결책을 찾으려는 과학적 접근법의 승리이며, 그 유산은 오늘날에도 여전히 수많은 연구와 응용에 강력한 영향력을 미치고 있다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>RetinaNet 구조와 학습 방식, <a href="https://small0753.tistory.com/m/entry/RetinaNet-%EA%B5%AC%EC%A1%B0%EC%99%80-%ED%95%99%EC%8A%B5-%EB%B0%A9%EC%8B%9D-%EC%A0%95%EB%A6%AC">https://small0753.tistory.com/m/entry/RetinaNet-%EA%B5%AC%EC%A1%B0%EC%99%80-%ED%95%99%EC%8A%B5-%EB%B0%A9%EC%8B%9D-%EC%A0%95%EB%A6%AC</a></li>
<li>[CV] One-Stage와 Two-Stage 차이 - 기록을 습관처럼, https://atonlee.tistory.com/98</li>
<li>[Object Detection] Architecture - 1 or 2 stage detector 차이 - velog, <a href="https://velog.io/@qtly_u/Object-Detection-Architecture-1-or-2-stage-detector-%EC%B0%A8%EC%9D%B4">https://velog.io/@qtly_u/Object-Detection-Architecture-1-or-2-stage-detector-%EC%B0%A8%EC%9D%B4</a></li>
<li>객체 감지란 무엇인가요? - IBM, https://www.ibm.com/kr-ko/think/topics/object-detection</li>
<li>Notes on Focal Loss and RetinaNet | by Hao Gao | Medium, https://medium.com/@smallfishbigsea/notes-on-focal-loss-and-retinanet-9c614a2367c6</li>
<li>RetinaNet: Focal Loss for Dense Object Detection - The VITALab website, https://vitalab.github.io/article/2018/03/22/retinanet.html</li>
<li>RetinaNet 원리 - GitHub Pages, https://inspaceai.github.io/2019/03/14/RetinaNet_Description/</li>
<li>Focal Loss for Dense Object Detection, https://arxiv.org/pdf/1708.02002</li>
<li>[1708.02002] Focal Loss for Dense Object Detection - arXiv, https://arxiv.org/abs/1708.02002</li>
<li>[1708.02002] Focal Loss for Dense Object Detection - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/1708.02002</li>
<li>[PDF] Focal Loss for Dense Object Detection - Semantic Scholar, https://www.semanticscholar.org/paper/Focal-Loss-for-Dense-Object-Detection-Lin-Goyal/79cfb51a51fc093f66aac8e858afe2e14d4a1f20</li>
<li>[CV] Feature Pyramid Networks for Object Detection(RetinaNet) review - velog, https://velog.io/@imfromk/CV-Feature-Pyramid-Networks-for-Object-DetectionRetinaNet-review</li>
<li>RetinaNet: Focal Loss for Dense Object Detection 논문 리뷰 | by Heejun Park | Medium, <a href="https://medium.com/@parkie0517/retinanet-focal-loss-for-dense-object-detection-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-70ae6d3a41c1">https://medium.com/@parkie0517/retinanet-focal-loss-for-dense-object-detection-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-70ae6d3a41c1</a></li>
<li>[X:AI] RetinaNet 논문 리뷰 - hyeon827 - 티스토리, https://hyeon827.tistory.com/79</li>
<li>[ML] Retina Net Object Detection 모델 - 앎의 공간 - 티스토리, https://techblog-history-younghunjo1.tistory.com/191</li>
<li>[Model] RetinaNet - MJ’s 우당탕탕 성장기 - 티스토리, https://mj-thump-thump-story.tistory.com/entry/Model-RetinaNet</li>
<li>Focal Loss 설명 - Object Detection 논문 리뷰 - FFighting, https://ffighting.net/deep-learning-paper-review/object-detection/focal-loss/</li>
<li>손실함수 (Loss Function) | 블로그 - 모두의연구소, https://modulabs.co.kr/blog/loss-function-machinelearning</li>
<li>Focal loss. 이것 또한 논문을 읽다가 배운 새로운 개념. | by ChengKang Tan | Medium, https://medium.com/@hichengkang/focal-loss-97ec5b44a2da</li>
<li>Focal loss 설명, <a href="https://velog.io/@heaseo/Focalloss-%EC%84%A4%EB%AA%85">https://velog.io/@heaseo/Focalloss-%EC%84%A4%EB%AA%85</a></li>
<li>[Image Segmentation] Custom Cost (Loss) Function - GOATLAB - 티스토리, https://goatlab.tistory.com/entry/Image-Segmentation-Custom-Cost-Loss-Function</li>
<li>The RetinaNet network architecture consists of a ResNet, a FPN, and two FCN subnets. - ResearchGate, https://www.researchgate.net/figure/The-RetinaNet-network-architecture-consists-of-a-ResNet-a-FPN-and-two-FCN-subnets_fig4_343384131</li>
<li>[RetinaNet] 2. Explaining overall RetinaNet model and Code analysis, https://yhu0409.tistory.com/3</li>
<li>How RetinaNet works? | ArcGIS API for Python - Esri Developer, https://developers.arcgis.com/python/latest/guide/how-retinanet-works/</li>
<li>RetinaNet: Single-Stage Object Detector with Accuracy Focus - Viso Suite, https://viso.ai/deep-learning/retinanet/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>