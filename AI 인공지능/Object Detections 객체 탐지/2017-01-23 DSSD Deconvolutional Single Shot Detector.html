<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:DSSD (Deconvolutional Single Shot Detectorm 2017-01-23)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>DSSD (Deconvolutional Single Shot Detectorm 2017-01-23)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>DSSD (Deconvolutional Single Shot Detectorm 2017-01-23)</span></nav>
                </div>
            </header>
            <article>
                <h1>DSSD (Deconvolutional Single Shot Detectorm 2017-01-23)</h1>
<h2>1.  소개</h2>
<h3>1.1  객체 탐지 패러다임의 진화</h3>
<p>현대 컴퓨터 비전 분야에서 객체 탐지(object detection)는 이미지 내 객체의 종류를 분류하고 위치를 특정하는 핵심 기술이다. 초기 딥러닝 기반 객체 탐지 연구는 크게 두 가지 패러다임으로 양분되어 발전했다. 첫 번째는 R-CNN 계열로 대표되는 2단계(Two-stage) 탐지기로, 후보 영역 생성(region proposal)과 클래스 분류 및 위치 회귀를 순차적으로 수행한다. 이 방식은 높은 정확도를 보장하지만, 다단계 처리 과정으로 인해 추론 속도가 느리다는 한계를 가진다.1 두 번째는 YOLO와 SSD로 대표되는 1단계(One-stage) 탐지기로, 후보 영역 생성 없이 단일 신경망을 통해 분류와 위치 회귀를 동시에 수행한다. 이 방식은 실시간 처리가 가능할 정도로 빠른 속도를 자랑하지만, 2단계 탐지기에 비해 정확도가 다소 떨어지는 경향이 있었다.1</p>
<h3>1.2  SSD (Single Shot MultiBox Detector)의 등장과 의의</h3>
<p>이러한 배경 속에서 등장한 SSD(Single Shot MultiBox Detector)는 1단계 탐지기의 성능을 한 단계 끌어올린 혁신적인 모델이었다. SSD는 입력 이미지를 고정된 크기의 그리드 셀(grid cell)로 나누고, 각 셀마다 다양한 크기와 종횡비(aspect ratio)를 가진 사전 정의된 기본 상자(Default Box)를 할당한다.4 이후 단일 컨볼루션 신경망(CNN)을 통과시켜 각 기본 상자에 대한 클래스 신뢰도와 위치 오프셋을 직접 예측한다.6 특히, SSD의 핵심 기여는 백본 네트워크의 여러 깊이에서 특징 맵을 추출하는 다중 스케일 특징 맵(Multi-scale feature maps) 구조를 도입한 것이다. 이를 통해 서로 다른 해상도의 특징 맵이 각각 다른 크기의 객체를 탐지하도록 하여, 단일 네트워크 내에서 다양한 스케일의 객체에 효과적으로 대응할 수 있게 되었다.8</p>
<h3>1.3  SSD의 근본적 한계와 DSSD의 필요성</h3>
<p>SSD는 빠른 속도와 준수한 정확도를 양립시키며 1단계 탐지기의 가능성을 입증했지만, 명백한 한계를 안고 있었다. 가장 치명적인 약점은 작은 객체(small objects)에 대한 탐지 성능이 현저히 저조하다는 점이었다.1 이 문제의 근원은 SSD의 구조적 한계에서 기인한다. 작은 객체 탐지를 담당하는 얕은 계층의 특징 맵(shallow feature maps)은 해상도가 높아 공간 정보는 풍부하지만, 소수의 컨볼루션 계층만을 통과했기 때문에 객체를 구별할 수 있는 의미론적 컨텍스트(semantic context)가 부족하다.2 반면, 깊은 계층으로 갈수록 특징 맵은 풍부한 의미 정보를 갖게 되지만, 반복적인 다운샘플링 과정에서 작은 객체의 형태와 위치 정보는 거의 소실된다.11 이러한 한계는 자율 주행에서의 신호등 탐지나 고해상도 항공 이미지 분석과 같이 작은 객체 탐지가 필수적인 실제 응용 분야에서 SSD의 적용을 어렵게 만들었다.1</p>
<h3>1.4  DSSD의 제안과 본 안내서의 목적</h3>
<p>DSSD(Deconvolutional Single Shot Detector)는 바로 이 SSD의 작은 객체 탐지 성능 문제를 해결하기 위해 제안된 핵심적인 개선 모델이다.12 DSSD의 핵심 아이디어는 디컨볼루션(Deconvolution) 계층을 도입하여 깊은 계층의 풍부한 의미 정보를 업샘플링하고, 이를 얕은 계층의 정밀한 공간 정보와 융합하는 것이다.10 이러한 접근은 1단계 탐지기 연구의 패러다임 전환을 예고하는 중요한 시도였다. 초기 1단계 탐지기들이 ’속도’를 위해 ’정확도’를 일부 희생하는 방향으로 발전했다면, DSSD는 1단계 프레임워크 내에서 2단계 탐지기 수준의 ’정확도’를 달성하기 위해 계산 복잡성 증가를 감수하는 첫 주요 시도였다. 이는 “1단계=속도, 2단계=정확도“라는 이분법적 구도를 허물고, 두 패러다임의 장점을 융합하려는 후속 연구들의 방향성을 제시했다. 본 안내서는 DSSD의 아키텍처, 핵심 구성 요소, 성능을 심층적으로 분석하고, 객체 탐지 기술 발전사에서 DSSD가 갖는 기술적 의의를 고찰하는 것을 목표로 한다.</p>
<h2>2.  기반 모델: SSD의 구조와 한계</h2>
<h3>2.1  SSD 아키텍처 상세 분석</h3>
<h4>2.1.1  백본 네트워크 (Backbone Network)</h4>
<p>SSD의 기본 구조는 이미지 분류 과제를 위해 ImageNet 데이터셋으로 사전 학습된 VGG-16 네트워크를 특징 추출기로 활용한다.4 원본 VGG-16의 마지막 완전 연결 계층(fully connected layers)들은 컨볼루션 계층으로 변환되어, 고정된 크기의 입력 대신 다양한 크기의 이미지를 처리할 수 있는 완전 컨볼루션 네트워크(Fully Convolutional Network) 형태로 구성된다. 이 백본 네트워크는 입력 이미지로부터 계층적인 특징을 추출하는 역할을 담당한다.</p>
<h4>2.1.2  다중 스케일 특징 맵 (Multi-scale Feature Maps)</h4>
<p>SSD의 가장 큰 특징은 백본 네트워크의 단일 최종 특징 맵만 사용하는 대신, 네트워크의 여러 깊이에서 특징 맵을 추출하여 예측에 활용하는 것이다. 구체적으로 VGG-16의 중간 계층인 <code>Conv4_3</code>과, 백본 뒤에 추가된 여러 보조 컨볼루션 계층들(<code>Conv7</code>, <code>Conv8_2</code>, <code>Conv9_2</code> 등)에서 특징 맵을 얻는다.2 이 특징 맵들은 점진적으로 해상도가 감소하며, 각기 다른 크기의 객체를 탐지하는 데 특화된다. 상대적으로 해상도가 높은 <code>Conv4_3</code>과 같은 얕은 계층은 작은 객체를, 해상도가 낮은 깊은 계층들은 큰 객체를 탐지하는 역할을 분담한다.4</p>
<h4>2.1.3  기본 상자 (Default Boxes)와 예측</h4>
<p>다중 스케일 특징 맵의 각 위치(cell)에는 사전에 정의된 여러 스케일과 종횡비를 가진 기본 상자(Default Box, 또는 앵커 박스)들이 할당된다.5 예를 들어, 특정 위치에 정사각형, 가로로 긴 직사각형, 세로로 긴 직사각형 등 다양한 형태의 기본 상자들이 존재한다. 네트워크는 각 기본 상자에 대해 두 가지 예측을 수행한다: (1) 해당 상자 내에 객체가 존재할 각 클래스별 신뢰도 점수(class confidence scores), (2) 기본 상자의 위치를 실제 객체에 더 잘 맞추기 위한 4개의 경계 상자 위치 오프셋(bounding box location offsets).4</p>
<h3>2.2  SSD의 내재적 한계 심층 분석</h3>
<p>SSD의 다중 스케일 전략은 각 특징 맵에 역할을 분담시키는 ’분업’의 개념으로 볼 수 있지만, 각 ‘부서’(특징 맵) 간의 효율적인 ’소통’이 부재한 구조적 문제를 안고 있다.</p>
<h4>2.2.1  얕은 특징 맵의 의미 정보 부족</h4>
<p>작은 객체 탐지를 주로 담당하는 <code>Conv4_3</code>과 같은 얕은 계층의 특징 맵은 상대적으로 높은 해상도를 가져 객체의 세밀한 위치 정보를 잘 보존한다. 하지만 이 특징 맵들은 소수의 컨볼루션 계층만을 통과했기 때문에, 객체의 클래스를 명확히 구분하는 데 필요한 고차원의 의미론적, 문맥적 정보를 충분히 담지 못한다.2 이는 마치 현장의 세부 정보는 잘 알지만 전체적인 전략(이 객체가 고양이인지 개인지)을 모르는 실무자와 같다. 이로 인해 작은 객체에 대한 분류 정확도가 저하되는 근본적인 원인이 된다.</p>
<h4>2.2.2  작은 객체의 공간 정보 손실</h4>
<p>입력 이미지는 VGG-16과 같은 깊은 네트워크를 통과하면서 반복적인 풀링(pooling)이나 스트라이드 컨볼루션(strided convolution)으로 인해 점차 다운샘플링된다.10 이 과정에서 32x32 픽셀 미만의 작은 객체들은 그 고유한 형태와 특징 정보가 점차 희미해지거나 완전히 사라져 버릴 수 있다.9 따라서 깊은 계층의 특징 맵에서는 해당 객체들을 탐지하는 것이 원천적으로 불가능해진다.</p>
<h4>2.2.3  정보 흐름의 단방향성</h4>
<p>SSD의 정보 흐름은 입력에서 출력으로 향하는 엄격한 단방향(feed-forward) 구조이다. 즉, 깊은 계층에서 추출된 풍부한 의미 정보가 다시 얕은 계층으로 전달되어 특징을 보강해주는 피드백 메커니즘이 존재하지 않는다.2 이로 인해 각 스케일의 특징 맵은 오직 자신과 그 이전 계층의 정보만을 바탕으로 독립적인 예측을 수행하게 되어, 이미지 전체의 컨텍스트를 종합적으로 활용하는 데 명백한 한계를 보인다. 이러한 구조적 결함은 각 계층이 가진 정보의 비대칭성 문제를 심화시키며, 특히 작은 객체 탐지에서 SSD의 성능을 저하시키는 근본적인 원인으로 작용한다.1</p>
<h2>3.  DSSD (Deconvolutional Single Shot Detector)의 제안</h2>
<h3>3.1  핵심 아이디어: 디컨볼루션을 통한 컨텍스트 강화</h3>
<p>DSSD는 SSD의 구조적 한계를 극복하기 위해 디컨볼루션을 통한 컨텍스트 강화라는 핵심 아이디어를 제시한다. 기존 SSD의 순방향 컨볼루션 구조를 이미지 정보를 압축하고 분석하는 인코더(encoder)로 간주하고, 여기에 역방향 디컨볼루션 구조의 디코더(decoder)를 추가하는 것이다.10 이 디코더는 깊은 계층의 작고 의미 정보가 풍부한 특징 맵을 입력받아 점진적으로 업샘플링(upsampling)하여 해상도를 복원한다.14 이 과정에서 생성된 고해상도 특징 맵을 인코더 부분의 동일한 해상도를 가진 특징 맵과 융합함으로써, 고해상도의 정밀한 공간 정보와 고차원의 풍부한 의미 정보를 모두 갖춘 새로운 특징 맵을 생성한다.14 이는 작은 객체 탐지에 결정적으로 필요한 풍부한 컨텍스트를 제공하는 역할을 한다.19</p>
<p>이러한 DSSD의 아키텍처는 단순히 SSD에 디컨볼루션을 추가한 것을 넘어, ’분석(Analysis)’과 ’합성(Synthesis)’이라는 두 가지 과정을 객체 탐지에 도입한 것으로 해석할 수 있다. 컨볼루션 경로(인코더)가 이미지를 저차원 의미 공간으로 ’분석’하는 과정이라면, 디컨볼루션 경로(디코더)는 이 의미 정보를 다시 고차원 공간 정보와 결합하여 정밀한 예측을 ’합성’하는 과정이다. 이러한 분석-합성 구조는 의미 분할(semantic segmentation) 분야에서 U-Net과 같은 모델들이 사용하던 성공적인 패러다임을 객체 탐지 분야로 이식한 중요한 시도이다. 이는 객체 탐지를 단순한 ‘분류+회귀’ 문제에서 ’의미적 이해에 기반한 공간적 위치 특정’이라는 더 복합적인 문제로 재정의하는 철학적 전환을 의미한다.</p>
<h3>3.2  DSSD의 주요 기여</h3>
<p>DSSD는 핵심 아이디어를 구현하기 위해 다음과 같은 세 가지 주요 기술적 기여를 포함한다.</p>
<ul>
<li>
<p><strong>고성능 백본 네트워크 도입:</strong> 기존의 VGG-16 대신, 당시 이미지 분류에서 최고 수준의 성능을 보이던 ResNet-101을 백본 네트워크로 채택하였다.10 ResNet의 잔차 연결(residual connection) 구조는 훨씬 깊은 네트워크의 안정적인 학습을 가능하게 하여 더 강력하고 풍부한 특징을 추출할 수 있게 해준다.10</p>
</li>
<li>
<p><strong>디컨볼루션 모듈 (Deconvolution Module):</strong> 업샘플링된 특징과 기존 특징을 효과적으로 융합하기 위한 새로운 모듈을 제안했다. 이 모듈은 두 특징 맵을 단순히 더하는 것을 넘어, 요소별 곱(element-wise product)과 같은 더 정교한 연산을 통해 두 정보 스트림을 비선형적으로 결합하여 특징 표현력을 극대화한다.13</p>
</li>
<li>
<p><strong>예측 모듈 (Prediction Module):</strong> 최종 예측을 수행하기 직전에, 융합된 각 특징 맵에 잔차 블록(residual block) 형태의 예측 모듈을 추가하여 특징을 한 번 더 정제한다.10 이는 특징 맵을 그대로 사용하던 SSD와 달리, 예측에 최적화된 변환을 추가적으로 학습할 기회를 제공하여 정확도를 높이는 역할을 한다.</p>
</li>
</ul>
<h2>4.  DSSD의 핵심 구성 요소 및 아키텍처</h2>
<h3>4.1  전체 아키텍처: 비대칭적 모래시계 구조</h3>
<p>DSSD는 전체적으로 인코더-디코더 구조를 따르지만, 인코더(ResNet-101 기반)에 비해 디코더(디컨볼루션 경로)가 훨씬 얕은 비대칭적(asymmetric) 모래시계(hourglass) 구조를 가진다.19 이러한 설계는 두 가지 현실적인 이유에 근거한다. 첫째, 대칭 구조는 추론 시간을 거의 두 배로 늘려 1단계 탐지기의 장점인 속도를 크게 해친다. 둘째, ImageNet과 같은 대규모 분류 데이터셋으로 사전 학습된 강력한 디코더 모델이 부재하기 때문에, 디코더를 무작정 깊게 쌓는 것이 오히려 학습을 불안정하게 만들 수 있다.19 이는 정확도 향상과 실용적인 추론 속도 사이의 균형을 맞추려는 의도적인 설계 결정이다.</p>
<h3>4.2  백본 네트워크: ResNet-101</h3>
<p>DSSD는 VGG-16 대신 101개 계층으로 구성된 ResNet-101을 백본으로 사용한다.10 ResNet의 핵심인 잔차 연결(skip-connection)은 기울기 소실(vanishing gradient) 문제를 완화하여 훨씬 깊은 네트워크의 안정적인 학습을 지원한다.10 이를 통해 VGG-16보다 더 풍부하고 강력한 계층적 특징을 추출할 수 있으며, 이는 객체 분류와 위치 특정의 정확도를 모두 높이는 데 기여한다. 다만, ResNet-101은 매우 깊기 때문에 작은 크기(e.g., 300x300)의 입력 이미지에서는 깊은 계층을 통과하며 공간 정보가 과도하게 손실될 수 있다. 따라서 DSSD는 513x513과 같이 더 큰 입력 이미지에서 최적의 성능을 보인다.10</p>
<h3>4.3  디컨볼루션 모듈 (Deconvolution Module)</h3>
<p>디컨볼루션 모듈은 DSSD의 핵심으로, 두 개의 정보 스트림을 융합하는 역할을 한다. 하나는 디컨볼루션 경로에서 온 업샘플링된 특징 맵이고, 다른 하나는 컨볼루션 경로에서 온 동일한 해상도의 특징 맵이다.18 두 특징 맵은 각각 별도의 컨볼루션 블록(Conv-BN-ReLU 등)을 통과하여 정제된 후, ‘요소별 곱(Element-wise Product)’ 연산을 통해 융합된다.13 논문에서는 요소별 합(element-wise sum)보다 곱 연산이 더 나은 성능을 보였다고 보고한다.19</p>
<p>이 ’요소별 곱’의 채택은 단순한 성능 비교를 넘어 중요한 의미를 갖는다. 이는 일종의 ‘주의 집중(Attention)’ 또는 ‘게이팅(Gating)’ 메커니즘으로 작용할 수 있다. 예를 들어, 디컨볼루션 경로의 특징 맵이 ‘사람’ 클래스에 해당하는 영역에서 높은 활성화 값을 가진다면, 컨볼루션 경로에서 추출된 ’사람’의 윤곽선이나 질감과 같은 저수준 특징들이 해당 영역에서만 선택적으로 증폭될 수 있다. 이는 두 정보 스트림을 단순히 더하는 것보다 훨씬 동적이고 문맥에 맞는 특징 융합을 가능하게 한다.</p>
<p>융합될 두 특징 맵을 각각 <span class="math math-inline">F_{conv} \in \mathbb{R}^{H \times W \times C}</span>와 <span class="math math-inline">F_{deconv} \in \mathbb{R}^{H \times W \times C}</span>라고 할 때, 요소별 곱으로 융합된 새로운 특징 맵 <span class="math math-inline">F_{fused}</span>는 다음과 같이 정의된다. 이 연산은 아다마르 곱(Hadamard Product)으로도 알려져 있다.21</p>
<p><span class="math math-display">
(F_{fused})_{i,j,k} = (F_{conv})_{i,j,k} \cdot (F_{deconv})_{i,j,k}
</span><br />
행렬 표기법으로는 <span class="math math-inline">F_{fused} = F_{conv} \odot F_{deconv}</span> 로 표현할 수 있다.22</p>
<h3>4.4  예측 모듈 (Prediction Module, PM)</h3>
<p>SSD에서는 특징 맵을 거의 그대로 예측에 사용하지만, DSSD는 각 스케일의 융합된 특징 맵 뒤에 예측 모듈을 추가한다.10 이 모듈의 목적은 특징 추출기(백본)가 순수하게 이미지 정보를 표현하는 데 집중하게 하고, 예측에 필요한 추가적인 변환 및 정제 작업은 예측 모듈이 담당하도록 역할을 분리하는 것이다.10 여러 구조에 대한 실험을 통해, 최종적으로 하나의 잔차 블록(residual block)을 사용하는 구조가 가장 효과적인 것으로 나타났다.10 이 잔차 블록은 융합된 특징 맵을 더욱 정제하여 최종적인 클래스 및 위치 예측의 정확도를 높인다.</p>
<h2>5.  손실 함수 및 학습 전략</h2>
<h3>5.1  손실 함수 (Loss Function)</h3>
<p>DSSD의 혁신은 아키텍처 설계, 즉 ’표현 학습(Representation Learning)’에 집중되어 있으며, 학습 방법론은 기존 SSD의 프레임워크를 거의 그대로 계승한다. 이는 DSSD의 저자들이 더 나은 특징 표현력을 가진 네트워크를 만들면, 기존의 학습 방식으로도 더 높은 성능을 달성할 수 있다고 판단했음을 시사한다. 손실 함수는 위치 손실(<span class="math math-inline">L_{loc}</span>)과 신뢰도 손실(<span class="math math-inline">L_{conf}</span>)의 가중 합으로 구성된다.4</p>
<p>전체 손실 함수는 다음과 같이 정의된다:</p>
<p><span class="math math-display">
L(x, c, l, g) = \frac{1}{N} (L_{conf}(x, c) + \alpha L_{loc}(x, l, g))
</span><br />
여기서 <span class="math math-inline">N</span>은 매칭된 기본 상자의 수, <span class="math math-inline">x</span>는 매칭 지표(<span class="math math-inline">x_{ij}^p = 1</span>이면 <span class="math math-inline">i</span>번째 기본 상자가 <span class="math math-inline">p</span> 클래스의 <span class="math math-inline">j</span>번째 실제 객체(ground truth)와 매칭됨), <span class="math math-inline">c</span>는 예측된 클래스 신뢰도, <span class="math math-inline">l</span>은 예측된 위치, <span class="math math-inline">g</span>는 실제 객체 위치를 의미한다. <span class="math math-inline">\alpha</span>는 두 손실 간의 균형을 맞추는 가중치로, 일반적으로 1로 설정된다.15</p>
<ul>
<li>
<p><strong>위치 손실 (<span class="math math-inline">L_{loc}</span>):</strong> 예측된 경계 상자와 실제 경계 상자 간의 차이를 측정하며, 이상치(outlier)에 덜 민감한 Smooth L1 손실 함수가 사용된다.4 이 손실은 실제 객체와 매칭된 포지티브 샘플(positive samples)에 대해서만 계산된다.3</p>
</li>
<li>
<p><strong>신뢰도 손실 (<span class="math math-inline">L_{conf}</span>):</strong> 다중 클래스 분류를 위해 소프트맥스 손실(Softmax Loss) 함수가 사용된다.4 이 손실은 포지티브 샘플과 네거티브 샘플(배경) 모두에 대해 계산된다.</p>
</li>
</ul>
<h3>5.2  학습 전략</h3>
<h4>5.2.1  매칭 전략 (Matching Strategy)</h4>
<p>훈련 과정에서 각 실제 객체(ground truth) 상자는 가장 높은 IoU(Intersection over Union) 값을 갖는 기본 상자와 매칭된다. 추가적으로, IoU가 특정 임계값(예: 0.5) 이상인 모든 기본 상자도 해당 실제 객체와 포지티브 샘플로 매칭된다. 이를 통해 네트워크는 하나의 객체에 대해 다양한 형태의 예측을 학습하게 된다.3</p>
<h4>5.2.2  어려운 네거티브 마이닝 (Hard Negative Mining)</h4>
<p>매칭되지 않은 대부분의 기본 상자는 배경, 즉 네거티브 샘플에 해당한다. 모든 네거티브 샘플을 학습에 사용하면 포지티브 샘플과의 심각한 클래스 불균형을 초래하여 학습이 불안정해진다.15 이를 해결하기 위해, 신뢰도 손실이 높은 네거티브 샘플, 즉 모델이 배경을 객체로 잘못 예측할 가능성이 높은 ‘어려운’ 샘플만을 선별하여 학습에 사용한다. 일반적으로 포지티브 샘플과 네거티브 샘플의 비율을 1:3으로 유지하여 학습 효율과 안정성을 높인다.3</p>
<h4>5.2.3  초기 2단계 학습 전략</h4>
<p>DSSD 논문에서는 초기에 2단계 학습 전략을 실험했다. 1단계에서는 사전 학습된 SSD 모델을 고정한 채 디컨볼루션 부분만 학습하고, 2단계에서 전체 네트워크를 미세 조정(fine-tuning)하는 방식이었다. 그러나 실험 결과, 이 방식이 성능 향상에 큰 도움이 되지 않거나 오히려 성능을 저하시키는 것으로 나타나 최종 모델에서는 단일 단계로 전체 네트워크를 학습하는 방식을 채택했다.18</p>
<h2>6.  성능 평가 및 분석</h2>
<h3>6.1  정확도 (Accuracy)</h3>
<p>DSSD는 여러 주요 객체 탐지 벤치마크 데이터셋에서 기존 SSD와 당시 최고 수준의 모델들을 능가하는 성능을 입증했다.</p>
<ul>
<li><strong>PASCAL VOC 데이터셋:</strong> PASCAL VOC 2007 test 데이터셋에서 DSSD513 모델은 81.5% mAP를 달성하여, 당시 SOTA 모델이었던 R-FCN(80.5%)과 SSD512*(79.8%)를 모두 능가했다.13 VOC 2012 test에서도 DSSD513은 80.0% mAP를 기록하며 R-FCN(77.6%) 및 SSD512*(76.8%) 대비 큰 폭의 성능 향상을 보였다.13</li>
</ul>
<table><thead><tr><th>모델 (Model)</th><th>백본 (Backbone)</th><th>입력 크기 (Input Size)</th><th>VOC 2007 test mAP (%)</th><th>VOC 2012 test mAP (%)</th></tr></thead><tbody>
<tr><td>SSD300*</td><td>VGG-16</td><td>300x300</td><td>77.2</td><td>75.8</td></tr>
<tr><td>SSD512*</td><td>VGG-16</td><td>512x512</td><td>79.8</td><td>76.8</td></tr>
<tr><td>R-FCN</td><td>ResNet-101</td><td>~</td><td>80.5</td><td>77.6</td></tr>
<tr><td><strong>DSSD321</strong></td><td><strong>ResNet-101</strong></td><td><strong>321x321</strong></td><td><strong>78.6</strong></td><td><strong>-</strong></td></tr>
<tr><td><strong>DSSD513</strong></td><td><strong>ResNet-101</strong></td><td><strong>513x513</strong></td><td><strong>81.5</strong></td><td><strong>80.0</strong></td></tr>
</tbody></table>
<ul>
<li><strong>MS COCO 데이터셋:</strong> 작은 객체가 많아 더 도전적인 MS COCO 데이터셋에서 DSSD의 진가가 드러났다. DSSD513 모델은 33.2% mAP를 달성하여 R-FCN(29.9%)과 SSD512(28.8%)를 모두 능가했다.13 특히, 작은 객체에 대한 성능 지표인 <span class="math math-inline">AP_S</span>에서 DSSD321은 7.4%를 기록하여 SSD321(6.2%) 대비 뚜렷한 개선을 보였다.18 이는 DSSD의 설계 목표가 성공적으로 달성되었음을 입증하는 핵심적인 결과이다.</li>
</ul>
<table><thead><tr><th>모델 (Model)</th><th>백본 (Backbone)</th><th>AP (IoU=.5:.95)</th><th><span class="math math-inline">AP_{.5}</span></th><th><span class="math math-inline">AP_S</span></th><th><span class="math math-inline">AP_M</span></th><th><span class="math math-inline">AP_L</span></th></tr></thead><tbody>
<tr><td>SSD321</td><td>ResNet-101</td><td>28.0</td><td>45.4</td><td>6.2</td><td>28.5</td><td>49.6</td></tr>
<tr><td>SSD512</td><td>ResNet-101</td><td>31.2</td><td>49.8</td><td>10.2</td><td>34.5</td><td>49.8</td></tr>
<tr><td>R-FCN</td><td>ResNet-101</td><td>29.9</td><td>51.5</td><td>10.8</td><td>32.8</td><td>45.0</td></tr>
<tr><td><strong>DSSD321</strong></td><td><strong>ResNet-101</strong></td><td><strong>28.0</strong></td><td><strong>46.1</strong></td><td><strong>7.4</strong></td><td><strong>29.5</strong></td><td><strong>47.6</strong></td></tr>
<tr><td><strong>DSSD513</strong></td><td><strong>ResNet-101</strong></td><td><strong>33.2</strong></td><td><strong>52.3</strong></td><td><strong>13.0</strong></td><td><strong>35.4</strong></td><td><strong>51.1</strong></td></tr>
</tbody></table>
<h3>6.2  추론 속도 (Inference Speed)</h3>
<p>DSSD는 정확도 향상을 위해 속도를 희생하는 명백한 상충 관계(trade-off)를 보인다. Titan X GPU 환경에서 SSD300(VGG 백본)이 46 FPS를 기록한 반면, DSSD321은 11.8 FPS에 그쳤다.23 이러한 속도 저하의 주요 원인은 다음과 같다: (1) VGG-16보다 훨씬 깊고 복잡한 ResNet-101 백본 사용, (2) 디컨볼루션 모듈과 예측 모듈의 추가적인 계산 오버헤드, (3) 더 많은 예측 계층으로 인한 기본 상자 수의 증가와 이에 따른 비최대 억제(NMS) 시간 증가.13</p>
<p>이러한 결과는 DSSD가 ’mAP’라는 단일 지표만으로는 온전히 평가될 수 없음을 보여준다. DSSD는 ‘정확도-속도 상충 관계 곡선’ 상에서 기존 SSD와는 다른 새로운 운영 지점(operating point)을 제시한 모델이다. 이는 사용자가 자신의 응용 시나리오(예: 실시간 처리가 중요한 엣지 디바이스 vs. 정확도가 중요한 서버)에 맞춰 모델의 복잡성과 성능 목표를 선택할 수 있는 새로운 가능성을 열었다.</p>
<table><thead><tr><th>모델 (Model)</th><th>백본 (Backbone)</th><th>입력 크기 (Input Size)</th><th>mAP (%) (VOC07)</th><th>속도 (FPS) (Titan X)</th></tr></thead><tbody>
<tr><td>SSD300</td><td>VGG-16</td><td>300x300</td><td>77.5</td><td>46</td></tr>
<tr><td>SSD321</td><td>ResNet-101</td><td>321x321</td><td>76.4</td><td>16.4</td></tr>
<tr><td>R-FCN</td><td>ResNet-101</td><td>~</td><td>80.5</td><td>9</td></tr>
<tr><td><strong>DSSD321</strong></td><td><strong>ResNet-101</strong></td><td><strong>321x321</strong></td><td><strong>78.6</strong></td><td><strong>11.8</strong></td></tr>
<tr><td><strong>DSSD513</strong></td><td><strong>ResNet-101</strong></td><td><strong>513x513</strong></td><td><strong>81.5</strong></td><td><strong>6.4</strong></td></tr>
</tbody></table>
<h3>6.3  종합 분석</h3>
<p>DSSD는 1단계 탐지기의 프레임워크를 유지하면서도, 디컨볼루션을 통한 컨텍스트 강화와 강력한 백본 네트워크 도입을 통해 2단계 탐지기 수준의 정확도를 달성했다. 특히 SSD의 핵심 약점이었던 작은 객체 탐지 성능을 유의미하게 개선함으로써 그 설계 목표를 성공적으로 달성했다. 하지만 이러한 성능 향상은 상당한 계산 비용 증가와 추론 속도 저하를 대가로 한다. 따라서 DSSD는 실시간 처리가 절대적으로 요구되는 응용 분야보다는, 정확도가 더 중요한 고성능 컴퓨팅 환경에 적합한 모델로 평가할 수 있다.</p>
<h2>7.  결론 및 의의</h2>
<h3>7.1  DSSD의 핵심 기여 요약</h3>
<p>DSSD는 1단계 탐지기인 SSD의 한계, 특히 작은 객체 탐지 성능 저하 문제를 해결하기 위해 제안된 선구적인 모델이다. DSSD는 다음 두 가지 핵심 전략을 통해 이 문제를 해결했다. 첫째, 강력한 ResNet-101 백본 네트워크를 도입하여 특징 추출 능력을 극대화했다. 둘째, 컨볼루션 경로(인코더)와 디컨볼루션 경로(디코더)를 결합하는 비대칭적 모래시계 구조를 통해 깊은 계층의 풍부한 의미 정보와 얕은 계층의 정밀한 공간 정보를 효과적으로 융합했다. 이를 통해 당시 최고 수준의 2단계 탐지기를 능가하는 정확도를 달성하며 1단계 탐지기의 새로운 가능성을 제시했다.</p>
<h3>7.2  기술적 의의와 후속 연구에 미친 영향</h3>
<p>DSSD의 가장 중요한 기술적 의의는 <strong>특징 피라미드 네트워크(Feature Pyramid Network, FPN)</strong> 24의 핵심 개념을 실질적으로 구현하고 그 효과를 입증했다는 점이다. DSSD가 제안한 상향식(bottom-up) 경로와 하향식(top-down) 경로를 통한 특징 융합 아이디어는 거의 동시대에 발표된 FPN에서 더욱 정교하게 다듬어졌다. FPN은 이후 수많은 객체 탐지 모델의 표준 아키텍처로 자리 잡았으며, DSSD는 이러한 흐름을 이끈 선구적인 연구로 평가받을 수 있다. 또한, DSSD는 FSSD (Feature Fusion SSD) 25, RSSD (Rainbow SSD) 17 등 다양한 SSD 개선 모델에 직접적인 영감을 주었으며, 1단계 탐지기의 정확도를 높이기 위한 특징 융합 연구를 촉발시키는 중요한 계기가 되었다.26</p>
<h3>7.3  1단계 탐지기의 발전 방향에 대한 통찰</h3>
<p>결론적으로 DSSD는 1단계 탐지기가 단순히 속도만 빠른 대안이 아니라, 정교한 아키텍처 설계를 통해 2단계 탐지기와 대등하거나 그 이상의 정확도를 가질 수 있음을 증명한 이정표적인 연구이다. DSSD가 제시한 정확도와 속도 사이의 상충 관계에 대한 깊은 고찰은 이후 모델 경량화, 효율적인 아키텍처 탐색(NAS), 그리고 다양한 응용 환경에 최적화된 모델 패밀리 개발 연구로 이어지며 객체 탐지 기술의 발전을 견인하고 있다. DSSD는 속도와 정확도라는 두 마리 토끼를 모두 잡기 위한 현대 객체 탐지 연구의 치열한 경쟁을 예고한 중요한 신호탄이었다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Aligned Matching: Improving Small Object Detection in SSD - MDPI, https://www.mdpi.com/1424-8220/23/5/2589</li>
<li>Single-Shot Object Detection With Enriched Semantics - CVF Open Access, https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Single-Shot_Object_Detection_CVPR_2018_paper.pdf</li>
<li>SSD object detection: Single Shot MultiBox Detector for real-time processing - Jonathan Hui, https://jonathan-hui.medium.com/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06</li>
<li>How single-shot detector (SSD) works? - GeeksforGeeks, https://www.geeksforgeeks.org/computer-vision/how-single-shot-detector-ssd-works/</li>
<li>How single-shot detector (SSD) works? | ArcGIS API for Python | Esri Developer, https://developers.arcgis.com/python/latest/guide/how-ssd-works/</li>
<li>Real-Time Object Detection with SSDs: Single Shot MultiBox Detectors - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2023/11/real-time-object-detection-with-ssds-single-shot-multibox-detectors/</li>
<li>SSD: Single Shot MultiBox Detector - UNC Computer Science, https://www.cs.unc.edu/~wliu/papers/ssd.pdf</li>
<li>14.7. Single Shot Multibox Detection — Dive into Deep Learning 1.0.3 documentation, http://d2l.ai/chapter_computer-vision/ssd.html</li>
<li>Single Shot MultiBox Detector (SSD) Explained by A.J | by Jesse Annan | Medium, https://medium.com/@jesse419419/single-shot-multibox-detector-ssd-explained-by-a-j-dda10ba42a29</li>
<li>Variations of SSD — Understanding Deconvolutional Single-Shot …, https://medium.com/@amadeusw6/variations-of-ssd-understanding-deconvolutional-single-shot-detectors-c0afb8686d03</li>
<li>Enhanced Single Shot Small Object Detector for Aerial Imagery Using Super-Resolution, Feature Fusion and Deconvolution - MDPI, https://www.mdpi.com/1424-8220/22/12/4339</li>
<li>[PDF] DSSD : Deconvolutional Single Shot Detector - Semantic Scholar, https://www.semanticscholar.org/paper/DSSD-%3A-Deconvolutional-Single-Shot-Detector-Fu-Liu/e94183191183a368bf07eb544654bae4b3cbf407</li>
<li>[1701.06659] DSSD : Deconvolutional Single Shot Detector - arXiv, https://arxiv.org/abs/1701.06659</li>
<li>MT-DSSD: Deconvolutional Single Shot Detector Using Multi Task Learning for Object Detection, Segmentation, and Grasping Detecti - MPRG, http://mprg.jp/data/MPRG/C_group/C20200603_araki.pdf</li>
<li>SSD : Understanding single shot object detection | Manal El Aidouni, <a href="https://manalelaidouni.github.io/Single%20shot%20object%20detection.html">https://manalelaidouni.github.io/Single%20shot%20object%20detection.html</a></li>
<li>Supplementary File . MDSSD: Multi-scale Deconvolutional Single Shot Detector for Small Objects, http://scis.scichina.com/en/2020/120113-supplementary.pdf</li>
<li>Single Shot Multibox Detector With Deconvolutional Region Magnification Procedure - Electrical &amp; Computer Engineering, https://www.ece.mcmaster.ca/~junchen/SSD_Access2021.pdf</li>
<li>Review: DSSD — Deconvolutional Single Shot Detector (Object Detection) | by Sik-Ho Tsang | TDS Archive | Medium, https://medium.com/data-science/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5</li>
<li>(PDF) DSSD : Deconvolutional Single Shot Detector - ResearchGate, https://www.researchgate.net/publication/312759848_DSSD_Deconvolutional_Single_Shot_Detector</li>
<li>DSSD: Deconvolutional Single Shot Detector - YouTube, https://www.youtube.com/watch?v=ZgomZgM_k-A</li>
<li>Hadamard product (matrices) - Wikipedia, https://en.wikipedia.org/wiki/Hadamard_product_(matrices)</li>
<li>hadamard product - Element-wise (or pointwise) operations notation?, https://math.stackexchange.com/questions/20412/element-wise-or-pointwise-operations-notation</li>
<li>[1801.05918] Extend the shallow part of Single Shot MultiBox Detector via Convolutional Neural Network - arXiv, https://arxiv.org/abs/1801.05918</li>
<li>DSSD : Deconvolutional Single Shot Detector - Connected Papers, https://www.connectedpapers.com/main/e94183191183a368bf07eb544654bae4b3cbf407/DSSD-%3A-Deconvolutional-Single-Shot-Detector/graph</li>
<li>arXiv:1712.00960v4 [cs.CV] 23 Feb 2024, https://arxiv.org/pdf/1712.00960</li>
<li>DF-SSD: An Improved SSD Object Detection Algorithm Based on DenseNet and Feature Fusion - ResearchGate, https://www.researchgate.net/publication/339025176_DF-SSD_An_Improved_SSD_Object_Detection_Algorithm_Based_on_DenseNet_and_Feature_Fusion</li>
<li>Layers of feature fusion modules - ResearchGate, https://www.researchgate.net/figure/Layers-of-feature-fusion-modules-a-the-fusion-module-for-element-wise-sum-and-product_fig2_345398483</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>