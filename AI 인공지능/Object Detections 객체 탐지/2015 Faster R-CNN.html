<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Faster R-CNN (Region Proposal Network를 통한 실시간 객체 탐지, 2015)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Faster R-CNN (Region Proposal Network를 통한 실시간 객체 탐지, 2015)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>Faster R-CNN (Region Proposal Network를 통한 실시간 객체 탐지, 2015)</span></nav>
                </div>
            </header>
            <article>
                <h1>Faster R-CNN (Region Proposal Network를 통한 실시간 객체 탐지, 2015)</h1>
<h2>1.  2-Stage 객체 탐지의 이정표</h2>
<h3>1.1  객체 탐지 기술의 발전 개요</h3>
<p>전통적인 컴퓨터 비전 시대의 객체 탐지는 이미지 전체를 다양한 크기의 윈도우로 훑는 슬라이딩 윈도우(Sliding Window) 방식과, 각 윈도우에서 HOG(Histogram of Oriented Gradients)와 같은 수작업 특징(Hand-crafted features)을 추출하여 객체를 분류하는 방식에 의존했다.1 이 접근법은 계산적으로 매우 비효율적이었으며, 다양한 객체의 형태와 변형에 대한 일반화 성능이 낮다는 근본적인 한계를 내포하고 있었다.</p>
<p>2012년 AlexNet의 성공 이후, 딥러닝, 특히 합성곱 신경망(CNN)의 등장은 특징 추출 과정을 데이터로부터 자동으로 학습하도록 만들어 객체 탐지 분야에 혁명적 변화를 가져왔다. CNN의 강력한 특징 표현 능력을 객체 탐지에 적용하려는 시도는 R-CNN(Regions with CNN features) 계열 모델의 탄생으로 이어졌으며, 이는 현대 객체 탐지 기술의 서막을 열었다.2</p>
<h3>1.2  R-CNN 계열과 2-Stage Detector의 개념 정립</h3>
<p>현대 딥러닝 기반 객체 탐지 모델은 크게 1-Stage Detector와 2-Stage Detector로 분류된다.3 2-Stage Detector는 이름에서 알 수 있듯이 두 단계에 걸쳐 탐지를 수행한다. 첫 번째 단계에서는 객체가 존재할 가능성이 높은 후보 영역(Region Proposal)을 제안하고, 두 번째 단계에서는 각 후보 영역에 대해 정밀한 객체 분류(Classification)와 위치 보정(Bounding Box Regression)을 수행한다. 이 방식은 정확도가 높은 경향이 있으나, 구조가 상대적으로 복잡하고 속도가 느릴 수 있다. R-CNN, Fast R-CNN, 그리고 본 안내서의 주제인 Faster R-CNN이 이 계열의 대표적인 모델이다.6 반면, 1-Stage Detector는 영역 제안 단계를 생략하고 이미지 전체에서 직접 객체의 위치와 클래스를 한 번에 예측한다. 이는 빠른 속도를 장점으로 가지지만, 초기 모델들은 2-Stage 방식에 비해 정확도가 다소 낮은 경향을 보였다. YOLO와 SSD가 1-Stage Detector의 대표적인 예이다.6</p>
<h3>1.3  Faster R-CNN의 연구사적 의의</h3>
<p>2015년에 발표된 Faster R-CNN 9은 이전 모델인 R-CNN과 Fast R-CNN이 해결하지 못한 핵심적인 병목 현상, 즉 CPU에 의존하는 느린 후보 영역 제안 알고리즘(주로 Selective Search) 문제를 정면으로 돌파했다.1 Faster R-CNN의 핵심 혁신은 **영역 제안 네트워크(Region Proposal Network, RPN)**의 도입에 있다. RPN은 후보 영역 제안 과정을 딥러닝 파이프라인에 완벽하게 통합시킨 최초의 시도였다. 이로써 영역 제안 단계 또한 GPU를 활용한 병렬 연산이 가능해져 속도가 비약적으로 향상되었을 뿐만 아니라, 전체 시스템이 하나의 통합된 네트워크로서 <strong>End-to-End 학습</strong>이 가능해지는 패러다임의 전환을 이끌었다.1</p>
<p>Faster R-CNN의 진정한 혁신은 단순히 속도를 개선한 것에 그치지 않는다. 이는 객체 탐지라는 복잡한 문제를 ’분리된 여러 단계의 조합’에서 ’단일 통합 네트워크’로 재정의했다는 데 더 큰 의미가 있다. R-CNN은 영역 제안, 특징 추출, 분류라는 세 개의 독립된 모듈로 구성되어 통합 학습이 원천적으로 불가능했다.1 Fast R-CNN은 특징 추출과 분류/회귀를 통합했지만, 여전히 영역 제안은 외부 알고리즘에 의존하는 ’절반의 성공’이었다.1 Faster R-CNN은 RPN을 도입하여 이 마지막 외부 의존성을 제거하고, 파이프라인의 모든 구성요소를 미분 가능한(differentiable) 신경망으로 구성함으로써 전체 시스템을 하나의 손실 함수 아래에서 공동으로 최적화할 수 있게 만들었다. 이는 딥러닝의 핵심 철학인 ‘End-to-End’ 학습을 2-Stage Detector에 성공적으로 구현한 최초의 사례로, 이후 수많은 객체 탐지 및 인스턴스 분할(Instance Segmentation) 모델의 기반이 되는 설계 사상의 초석을 다졌다.16</p>
<h2>2.  R-CNN에서 Fast R-CNN으로: 병목 현상 해결을 위한 여정</h2>
<h3>2.1  R-CNN (Regions with CNN features)</h3>
<p>R-CNN은 딥러닝을 객체 탐지에 성공적으로 적용한 첫 모델로 평가받지만, 그 구조는 여러 독립적인 모듈의 복잡한 조합이었다.</p>
<h4>2.1.1 동작 방식</h4>
<ol>
<li><strong>후보 영역 생성</strong>: 입력 이미지에 대해 Selective Search 알고리즘을 적용하여 객체가 있을 법한 후보 영역(Region Proposal)을 약 2,000개 생성한다.2</li>
<li><strong>영역 변환</strong>: 생성된 각 후보 영역을 CNN의 입력 크기에 맞추기 위해 고정된 크기(예: 227x227)로 강제 변환(Warping)한다. 이는 CNN의 마지막에 위치한 완전 연결 계층(Fully Connected Layer)이 고정된 크기의 입력을 요구하기 때문이다.2</li>
<li><strong>특징 추출</strong>: 변환된 2,000개의 영역 각각을 독립적으로 사전 학습된 CNN에 통과시켜 4096차원의 특징 벡터를 추출한다.1</li>
<li><strong>분류 및 위치 보정</strong>: 추출된 특징 벡터를 사용하여 각 객체 클래스에 대한 선형 SVM(Support Vector Machine) 분류기를 학습시킨다. 동시에, 별도의 선형 회귀 모델을 학습하여 Selective Search로 제안된 경계 상자(Bounding Box)의 위치를 보다 정확하게 보정한다.2</li>
</ol>
<h4>2.1.2 명백한 한계</h4>
<ul>
<li><strong>속도 병목</strong>: 약 2,000개의 후보 영역 각각에 대해 독립적으로 CNN Forward Pass를 수행해야 하므로 엄청난 계산량이 발생한다. 이로 인해 이미지 한 장을 처리하는 데 수십 초가 소요되어 실시간 적용이 불가능했다.1</li>
<li><strong>복잡한 다단계 파이프라인</strong>: 영역 제안, CNN 특징 추출, SVM 분류, BBox 회귀가 모두 분리된 모듈로 구성되어 있어 End-to-End 학습이 불가능했다. 이는 학습 과정을 복잡하고 비효율적으로 만들었다.1</li>
<li><strong>저장 공간 문제</strong>: 2,000개 영역에서 추출된 특징 벡터들을 SVM 학습을 위해 디스크에 캐싱해야 했는데, 이는 수백 기가바이트에 달하는 막대한 저장 공간을 요구했다.1</li>
</ul>
<h3>2.2  Fast R-CNN</h3>
<p>Fast R-CNN은 R-CNN의 치명적인 속도 문제를 해결하기 위해 제안되었다. 핵심 아이디어는 CNN 연산과 영역 추출의 순서를 바꿔 중복 계산을 원천적으로 제거하는 것이었다.13</p>
<h4>2.2.1 동작 방식</h4>
<ol>
<li><strong>통합 특징 추출</strong>: 입력 이미지 전체를 CNN에 <strong>단 한 번만</strong> 통과시켜 전체 이미지에 대한 특징 맵(Feature Map)을 생성한다.4</li>
<li><strong>RoI 투영</strong>: Selective Search로 찾은 후보 영역(RoI, Region of Interest)의 좌표를 1단계에서 생성된 특징 맵 위로 투영(Projection)한다.18</li>
<li><strong>RoI Pooling</strong>: <strong>RoI Pooling Layer</strong>라는 새로운 계층을 도입하여, 각기 다른 크기를 가진 RoI 특징 맵 영역들을 고정된 크기의 특징 벡터(예: 7x7)로 변환한다. 이는 각 RoI 영역을 고정된 수의 그리드로 나눈 뒤, 각 그리드 셀에 대해 Max Pooling을 적용하여 이루어진다.4</li>
<li><strong>통합된 최종 예측</strong>: 고정 크기 특징 벡터를 FC Layer에 통과시킨 후, Softmax 분류기와 BBox 회귀기를 통해 최종 결과를 동시에 출력한다. 이로써 SVM과 별도의 회귀 모델이 신경망 구조 안으로 통합되었다.13</li>
</ol>
<h4>2.2.2 개선점</h4>
<ul>
<li><strong>획기적인 속도 향상</strong>: 이미지 전체에 대한 컨볼루션 연산을 공유함으로써 R-CNN의 가장 큰 문제였던 중복 계산을 제거했다. 이로 인해 R-CNN 대비 학습 속도는 약 9배, 추론 속도는 약 188배 향상되었다.1</li>
<li><strong>End-to-End 학습의 진일보</strong>: SVM과 회귀 모델을 신경망으로 통합하고 RoI Pooling을 통해 미분 가능한 파이프라인을 구축함으로써, 특징 추출기(CNN)와 최종 탐지기(Classifier, Regressor)를 함께 학습시키는 것이 가능해졌다.13</li>
</ul>
<p>이러한 발전은 단순히 속도를 개선한 것을 넘어, 객체 탐지의 계산 패러다임을 근본적으로 바꾸었다. R-CNN이 ’객체 후보’를 먼저 찾고 각 후보를 개별적으로 처리하는 ‘객체 중심(Object-centric)’ 접근법이었다면 1, Fast R-CNN은 먼저 ’이미지 전체’의 특징을 한 번에 계산하고 그 위에서 후보 영역을 처리하는 ‘이미지 중심(Image-centric)’ 접근법을 채택했다.13 이는 ’연산 공유(Computation Sharing)’라는 개념을 객체 탐지에 본격적으로 도입한 것으로, 이후 등장하는 모든 효율적인 탐지 모델의 기본 원칙이 되었다.</p>
<h3>2.3  남겨진 과제: 영역 제안의 병목 현상</h3>
<p>Fast R-CNN은 탐지 네트워크 자체의 속도를 획기적으로 개선했지만, 여전히 후보 영역 제안은 CPU에서 동작하는 느린 Selective Search 알고리즘에 의존했다. 이로 인해 영역 제안 단계가 전체 파이프라인의 새로운 속도 병목으로 남게 되었다.1 또한, Selective Search는 학습이 불가능한 알고리즘이므로 특정 데이터셋에 맞춰 최적화하기 어렵다는 근본적인 문제도 여전히 존재했다.1 이 마지막 남은 과제를 해결하는 것이 바로 Faster R-CNN의 목표였다.</p>
<h2>3.  Faster R-CNN 아키텍처 심층 분석</h2>
<p>Faster R-CNN은 Fast R-CNN의 구조를 계승하면서, 병목이었던 Selective Search를 학습 가능한 신경망인 RPN으로 대체하여 완전한 통합 네트워크를 구현했다.</p>
<h3>3.1  전체 구조 및 데이터 흐름</h3>
<p>Faster R-CNN은 크게 두 개의 핵심 모듈로 구성된다: (1) 객체 후보 영역을 생성하는 <strong>RPN(Region Proposal Network)</strong>, (2) 제안된 영역을 기반으로 최종 객체를 탐지하는 <strong>Fast R-CNN Detector</strong>.1 이 두 모듈은 <strong>Backbone Network</strong>(예: VGG-16, ResNet)라 불리는 공통의 컨볼루션 계층들을 공유하며, 이를 통해 연산 효율성을 극대화한다.1</p>
<p>전체 데이터 흐름은 다음과 같다:</p>
<ol>
<li>입력 이미지가 Backbone Network를 통과하여 고수준의 의미 정보를 담은 특징 맵(Feature Map)을 생성한다.</li>
<li>이 특징 맵은 RPN과 Fast R-CNN Detector 양쪽에 입력으로 동시에 제공된다.</li>
<li>RPN은 특징 맵을 입력받아 객체가 있을 법한 위치와 크기를 담은 후보 영역(Region Proposals) 리스트를 출력한다.</li>
<li>원본 특징 맵과 RPN이 생성한 후보 영역들은 RoI Pooling Layer를 거쳐 고정된 크기의 특징 벡터로 변환된다.</li>
<li>이 특징 벡터는 최종 분류기(Classifier)와 경계 상자 회귀기(Bounding Box Regressor)에 전달되어, 각 후보 영역이 어떤 클래스의 객체인지, 그리고 그 정확한 위치는 어디인지를 최종적으로 예측한다.1</li>
</ol>
<h3>3.2  핵심 혁신: 영역 제안 네트워크 (Region Proposal Network, RPN)</h3>
<p>RPN은 Faster R-CNN의 가장 중요한 혁신으로, CPU 기반의 Selective Search를 대체하여 GPU 상에서 빠르고 효율적으로 고품질의 후보 영역을 생성하는 완전 컨볼루션 네트워크(Fully Convolutional Network, FCN)이다.14</p>
<h4>3.2.1 작동 원리</h4>
<ol>
<li>Backbone Network에서 출력된 특징 맵 위를 작은 <code>n x n</code>(논문에서는 3x3) 크기의 슬라이딩 윈도우가 순회한다.1</li>
<li>각 슬라이딩 윈도우의 중심 위치는 ’앵커(Anchor)’라고 불리며, 원본 이미지의 특정 영역에 대응된다.</li>
<li>각 앵커 위치에서, 미리 정의된 다양한 크기(scale)와 종횡비(aspect ratio)를 가진 <code>k</code>개의 **앵커 박스(Anchor Box)**가 생성된다.</li>
<li>슬라이딩 윈도우를 통해 추출된 특징 벡터(예: VGG-16의 경우 512-d)는 두 개의 자매(sibling) <code>1x1</code> 컨볼루션 계층으로 나란히 전달된다.1</li>
</ol>
<ul>
<li><strong>분류 계층 (cls layer)</strong>: 각 <code>k</code>개의 앵커 박스에 대해 ’객체인지 아닌지(배경)’를 판단하는 2개의 점수(Objectness Score)를 출력한다. 따라서 이 계층의 출력 채널 수는 <code>2k</code>가 된다.1</li>
<li><strong>회귀 계층 (reg layer)</strong>: 각 <code>k</code>개의 앵커 박스의 위치를 미세 조정하기 위한 4개의 좌표 오프셋 값(<code>dx, dy, dw, dh</code>)을 출력한다. 따라서 이 계층의 출력 채널 수는 <code>4k</code>가 된다.1</li>
</ul>
<ol start="5">
<li>이렇게 생성된 수많은 후보 영역들은 Objectness Score를 기준으로 상위 N개를 필터링하고, 비최대 억제(Non-Maximum Suppression, NMS) 알고리즘을 적용하여 심하게 겹치는 영역들을 제거한 후 최종 후보 영역으로 선택된다.14</li>
</ol>
<h3>3.3  앵커 박스(Anchor Box)의 개념과 활용</h3>
<p>앵커 박스는 RPN의 핵심 개념으로, 객체 탐지에서 ’어디를 볼 것인가’에 대한 효율적인 프레임워크를 제공한다.</p>
<ul>
<li><strong>정의</strong>: 특징 맵의 각 위치(앵커)를 중심으로 사전에 정의된, 다양한 크기와 종횡비를 가진 참조 상자(reference box)들의 집합이다.17</li>
<li><strong>목적</strong>: 전통적인 방식처럼 이미지 크기를 바꾸거나(Image Pyramid) 필터 크기를 바꾸는(Filter Pyramid) 비효율적인 방법 대신, 단일 스케일의 특징 맵만으로 다양한 크기와 형태의 객체를 효율적으로 탐지하기 위함이다.1 예를 들어, 원본 논문에서는 3가지 크기(<code>128^2</code>, <code>256^2</code>, <code>512^2</code>)와 3가지 종횡비(1:1, 1:2, 2:1)를 조합하여 각 앵커 위치당 <code>k=9</code>개의 앵커 박스를 사용한다.1</li>
</ul>
<p>앵커 박스의 도입은 객체 위치 탐색 문제를 근본적으로 재구성했다. 기존의 슬라이딩 윈도우 방식이 이미지의 모든 가능한 위치와 크기를 무차별적으로 탐색하는 비효율적인 방식이었다면 5, 앵커 박스는 탐색 공간을 특징 맵의 각 위치를 중심으로 한 <code>k</code>개의 사전 정의된 박스로 제한한다. 이는 ’어디서 찾을 것인가’라는 무한한 공간의 문제를 유한한 ’참조점’의 집합으로 이산화(discretize)하는 과정이다.19 이제 네트워크는 각 참조점(앵커 박스)에 대해 두 가지 간단한 질문, 즉 (1) “이 안에 객체가 있는가?” (Objectness Score)와 (2) “있다면, 이 박스를 얼마나 조정해야 실제 객체에 딱 맞는가?” (BBox Regression)에만 답하면 된다.22 이처럼 복잡한 ‘위치 탐색’ 문제를 잘 정의된 ’분류’와 ‘회귀’ 문제로 변환한 이 프레임워크는 매우 효과적이어서, 이후 YOLOv2, SSD 등 수많은 1-Stage 및 2-Stage 모델에서 핵심 구성 요소로 채택되었다.32</p>
<h3>3.4  RoI Pooling 및 최종 탐지 네트워크</h3>
<p>RPN이 후보 영역을 제안한 이후의 과정은 Fast R-CNN의 구조와 거의 동일하다.18</p>
<ul>
<li>RPN이 제안한 가변 크기의 후보 영역(RoI)들은 RoI Pooling Layer를 통해 Backbone 특징 맵으로부터 고정된 크기(예: 7x7)의 특징 벡터를 추출한다.1</li>
<li>추출된 고정 크기 벡터는 여러 개의 FC Layer를 거친다.</li>
<li>최종적으로 다중 클래스 분류기(예: Softmax)와 클래스별 BBox 회귀기에 전달되어, 각 RoI가 어떤 구체적인 객체인지(예: ‘사람’, ‘자동차’, ‘고양이’)와 그 객체를 더 정확하게 감싸는 최종 경계 상자의 위치를 예측한다.1</li>
</ul>
<h2>4.  학습 과정 및 손실 함수</h2>
<p>Faster R-CNN은 RPN과 최종 탐지 네트워크라는 두 개의 서브 네트워크를 학습시켜야 하며, 이들은 컨볼루션 계층을 공유한다. 이를 위해 독특한 손실 함수와 학습 전략이 사용된다.</p>
<h3>4.1  다중 작업 손실 함수 (Multi-task Loss Function)</h3>
<p>Faster R-CNN은 RPN과 최종 탐지 네트워크 모두에서 분류(Classification)와 위치 회귀(Regression)라는 두 가지 작업을 동시에 수행하므로, 다중 작업 손실(Multi-task Loss) 함수를 사용한다.11</p>
<h4>4.1.1 RPN의 손실 함수</h4>
<p>RPN을 학습시키기 위한 손실 함수는 다음과 같이 정의된다.11<br />
<span class="math math-display">
L({p_i}, {t_i}) = \frac{1}{N_{cls}} \sum_i L_{cls}(p_i, p_i^*) + \lambda \frac{1}{N_{reg}} \sum_i p_i^* L_{reg}(t_i, t_i^*)
</span></p>
<ul>
<li>첫 번째 항 <span class="math math-inline">\frac{1}{N_{cls}} \sum_i L_{cls}(p_i, p_i^*)</span>은 <strong>분류 손실</strong>이다.</li>
<li><span class="math math-inline">i</span>는 미니배치 내 앵커의 인덱스이다.</li>
<li><span class="math math-inline">p_i</span>는 앵커 <span class="math math-inline">i</span>가 객체일 것으로 예측한 확률이다.</li>
<li><span class="math math-inline">p_i^*</span>는 실제 레이블로, 객체를 포함하는 Positive 앵커일 경우 1, 배경인 Negative 앵커일 경우 0이다.</li>
<li><span class="math math-inline">L_{cls}</span>는 두 클래스(객체 vs. 배경)에 대한 로그 손실(Log Loss) 함수이다.</li>
<li>두 번째 항 <span class="math math-inline">\lambda \frac{1}{N_{reg}} \sum_i p_i^* L_{reg}(t_i, t_i^*)</span>는 <strong>회귀 손실</strong>이다.</li>
<li><span class="math math-inline">t_i</span>는 예측된 경계 상자의 4개 파라미터화된 좌표 벡터(<span class="math math-inline">t_x, t_y, t_w, t_h</span>)이다.</li>
<li><span class="math math-inline">t_i^*</span>는 Positive 앵커에 대응하는 실제 경계 상자(Ground-Truth Box)의 좌표 벡터이다.</li>
<li><span class="math math-inline">L_{reg}</span>는 Smooth L1 손실 함수로, 예측값과 실제값의 차이가 클 때 L1 손실처럼, 작을 때 L2 손실처럼 동작하여 이상치(outlier)에 덜 민감하다.</li>
<li><span class="math math-inline">p_i^*</span>가 곱해져 있으므로, 회귀 손실은 Positive 앵커(<span class="math math-inline">p_i^*=1</span>)에 대해서만 활성화되고 Negative 앵커에 대해서는 무시된다. 배경에 대해 위치를 예측하는 것은 의미가 없기 때문이다.</li>
<li><span class="math math-inline">N_{cls}</span>와 <span class="math math-inline">N_{reg}</span>는 각각 미니배치 크기와 앵커 위치 수로 정규화하기 위한 항이며, <span class="math math-inline">\lambda</span>는 두 손실 간의 가중치를 조절하는 균형 파라미터이다.</li>
</ul>
<h4>4.1.2 최종 탐지 네트워크의 손실 함수</h4>
<p>Fast R-CNN 부분의 손실 함수 역시 유사한 Multi-task Loss 형태를 가진다. 다만, RPN의 손실이 ’객체 유무(Objectness)’라는 이진 분류에 대한 것이었다면, 최종 탐지 네트워크의 손실은 RoI별로 ’(클래스 수 + 배경)’개의 클래스에 대한 다중 클래스 분류(Softmax Loss)와 클래스별 BBox 회귀(Smooth L1 Loss)를 수행한다는 차이가 있다.4</p>
<h3>4.2  4단계 교대 학습 (4-Step Alternating Training)</h3>
<p>RPN과 Fast R-CNN Detector가 컨볼루션 계층을 공유하면서도 각자의 고유한 가중치를 안정적으로 학습시키기 위해, 논문에서는 4단계 교대 학습(4-Step Alternating Training)이라는 다소 복잡한 방식을 제안했다.20</p>
<ol>
<li><strong>1단계 (RPN 학습)</strong>: ImageNet으로 사전 학습된 모델을 불러와 RPN을 End-to-End로 학습시킨다.</li>
<li><strong>2단계 (탐지기 학습)</strong>: 1단계에서 학습된 RPN을 사용하여 후보 영역을 생성한다. 이 영역들을 이용해 별도의 Fast R-CNN 탐지 네트워크를 학습시킨다. 이 탐지 네트워크 역시 ImageNet 사전 학습 모델로 초기화된다. 이 단계까지 두 네트워크는 컨볼루션 계층을 공유하지 않는다.</li>
<li><strong>3단계 (RPN 미세 조정)</strong>: 2단계에서 학습된 탐지 네트워크의 가중치로 RPN 학습을 다시 초기화한다. 이때, 두 네트워크가 공유할 컨볼루션 계층들의 가중치는 고정(freeze)시키고, RPN에만 속하는 고유한 계층들의 가중치만 미세 조정(fine-tuning)한다. 이 단계부터 두 네트워크는 컨볼루션 계층을 공유하게 된다.</li>
<li><strong>4단계 (탐지기 미세 조정)</strong>: 3단계에서 미세 조정된 RPN과 공유 컨볼루션 계층을 고정한 채, Fast R-CNN의 FC 계층들만 다시 미세 조정한다.</li>
</ol>
<p>이러한 4단계 교대 학습 방식은 공유 가중치를 가진 두 상호 의존적인 네트워크를 안정적으로 학습시키기 위한 실용적인 해결책이었다. 이상적으로는 전체 네트워크가 한 번에 공동으로 학습되어야 하지만, RPN의 가중치가 변함에 따라 생성되는 후보 영역의 분포 또한 계속 변하기 때문에, 이 불안정한 입력을 받는 Fast R-CNN을 동시에 학습시키는 것은 수렴을 어렵게 할 수 있다. 4단계 학습은 ’고정’과 ’학습’을 번갈아 수행함으로써 이 문제를 해결한다. 즉, 한쪽을 고정시켜 안정된 학습 환경을 만든 뒤 다른 쪽을 학습시키는 과정을 반복하는 것이다. 비록 우아한 방법은 아니지만, 당시 기술 수준에서 두 네트워크의 협력을 성공적으로 이끌어낸 실용적인 접근법이었으며, 이후 연구들이 더 정교한 통합 학습(Joint Training) 방법을 모색하게 된 직접적인 동기가 되었다.1</p>
<h2>5.  성능 분석 및 비교</h2>
<p>Faster R-CNN의 성능은 동시대의 다른 주요 객체 탐지 모델들과의 비교를 통해 가장 명확하게 이해할 수 있다. 성능 평가는 주로 정확도를 나타내는 평균 정밀도(mean Average Precision, mAP)와 속도를 나타내는 초당 프레임 수(Frames Per Second, FPS)로 이루어진다.</p>
<h3>5.1  주요 객체 탐지 모델과의 성능 비교</h3>
<p>Faster R-CNN은 2-Stage 방식의 특성상, 후보 영역을 먼저 제안하고 각 영역을 정밀하게 분석하므로 일반적으로 높은 정확도(mAP)를 달성한다. 반면, 동시대의 1-Stage 모델인 YOLO(You Only Look Once)나 SSD(Single Shot MultiBox Detector)는 이미지 전체를 한 번에 처리하여 빠른 속도(FPS)를 확보하는 대신 정확도에서 약간의 손해를 보는 경향이 있었다.8 이러한 성능 차이는 애플리케이션의 요구사항에 따라 모델 선택의 중요한 기준이 된다. 예를 들어, 실시간 비디오 감시 시스템과 같이 속도가 최우선인 경우 1-Stage 모델이 선호될 수 있으며, 의료 영상 분석과 같이 오탐을 최소화하고 최고의 정확도가 요구되는 경우 2-Stage 모델이 더 적합할 수 있다.33</p>
<h3>5.2  성능 비교표 (PASCAL VOC &amp; COCO 데이터셋)</h3>
<p>객체 탐지 분야의 발전을 이해하기 위해서는 주요 모델들의 정량적 성능을 직접 비교하는 것이 필수적이다. 아래 표는 Faster R-CNN을 동시대의 핵심 경쟁 모델들(SSD, YOLO)과 표준 데이터셋(PASCAL VOC, MS COCO)에서 비교하여, 각 모델의 아키텍처적 선택이 정확도(mAP)와 속도(FPS)라는 상충 관계에 어떤 영향을 미쳤는지 명확하게 보여준다.</p>
<table><thead><tr><th>모델 (Model)</th><th>Backbone</th><th>데이터셋 (Dataset)</th><th>mAP (%)</th><th>FPS</th><th>출처 (Source)</th></tr></thead><tbody>
<tr><td>Faster R-CNN</td><td>VGG-16</td><td>PASCAL VOC 2007</td><td>73.2</td><td>7</td><td>33</td></tr>
<tr><td>Faster R-CNN</td><td>ResNet-101</td><td>COCO</td><td>34.9</td><td>5</td><td>33</td></tr>
<tr><td>SSD300</td><td>VGG-16</td><td>PASCAL VOC 2007</td><td>77.2</td><td>46</td><td>33</td></tr>
<tr><td>SSD512</td><td>VGG-16</td><td>PASCAL VOC 2007</td><td>79.8</td><td>19</td><td>33</td></tr>
<tr><td>SSD300</td><td>VGG-16</td><td>COCO</td><td>25.1</td><td>-</td><td>33</td></tr>
<tr><td>YOLOv1</td><td>-</td><td>PASCAL VOC 2007</td><td>63.4</td><td>45</td><td>33</td></tr>
<tr><td>YOLOv2 (416x416)</td><td>Darknet-19</td><td>PASCAL VOC 2007</td><td>76.8</td><td>67</td><td>33</td></tr>
<tr><td>YOLOv3</td><td>Darknet-53</td><td>COCO</td><td>33.0 (AP)</td><td>20+</td><td>34</td></tr>
</tbody></table>
<p><em>참고: 위 표의 성능 수치는 사용된 하드웨어, 라이브러리 버전, 세부 구현에 따라 달라질 수 있으며, 논문 발표 당시의 대표적인 값이다.</em></p>
<h3>5.3  정확도와 속도의 상충 관계(Trade-off) 분석</h3>
<p>위 표는 2-Stage 모델과 1-Stage 모델 간의 전형적인 정확도-속도 상충 관계를 잘 보여준다. Faster R-CNN은 VGG-16 Backbone 사용 시 PASCAL VOC 2007 데이터셋에서 7 FPS의 속도를 기록한 반면, SSD300과 YOLOv2는 각각 46 FPS, 67 FPS로 훨씬 빠른 처리 속도를 보였다.33</p>
<p>하지만 이러한 상충 관계는 고정된 것이 아니다. Google Research의 한 연구에 따르면, Faster R-CNN은 RPN에서 생성하는 후보 영역(proposal)의 수를 조절함으로써 정확도와 속도 사이의 균형점을 유연하게 선택할 수 있다. 예를 들어, 후보 영역 수를 300개에서 50개로 줄이면 정확도는 4% 정도 소폭 하락하지만, 추론 속도는 3배까지 향상될 수 있다.33 이는 Faster R-CNN이 애플리케이션의 요구에 따라 성능을 튜닝할 수 있는 여지를 가지고 있음을 시사한다.</p>
<p>또한, 객체의 크기에 따른 성능 차이도 중요한 분석 지점이다. SSD와 같은 1-Stage 모델은 일반적으로 작은 객체 탐지에 약점을 보이는 반면, Faster R-CNN과 같은 Region-based detector는 이 부분에서 상대적인 강점을 보인다.33 이는 RPN이 후보 영역을 먼저 찾고 해당 영역에 집중하여 특징을 분석하는 2-Stage 방식의 본질적인 장점에서 기인한다.</p>
<h2>6.  한계점 및 후속 연구 동향</h2>
<p>Faster R-CNN은 객체 탐지 분야에 큰 획을 그었지만, 몇 가지 내재적 한계를 가지고 있었으며, 이는 후속 연구들의 중요한 출발점이 되었다.</p>
<h3>6.1  Faster R-CNN의 내재적 한계</h3>
<ul>
<li><strong>속도 문제</strong>: RPN을 통해 큰 폭의 속도 개선을 이루었음에도 불구하고, 약 5-7 FPS의 처리 속도는 1-Stage Detector에 비해 여전히 느려 진정한 의미의 실시간 비디오 처리에 한계가 있었다.33</li>
<li><strong>다단계 구조의 복잡성</strong>: RPN과 Fast R-CNN Detector로 구성된 2-stage 구조는 1-stage 모델에 비해 구조가 복잡하고 학습 과정이 까다롭다는 단점이 있었다.35</li>
<li><strong>작은 객체 탐지 성능</strong>: Backbone Network의 저수준 계층(고해상도, 저수준 특징)을 최종 탐지에 효과적으로 활용하는 메커니즘이 부재했다. 컨볼루션이 깊어질수록 공간 해상도가 낮아져 작은 객체의 정보가 소실될 수 있었기 때문이다.35</li>
<li><strong>RoI Pooling의 정보 손실</strong>: RoI Pooling 과정에서 후보 영역의 좌표를 특징 맵의 그리드에 맞추기 위해 소수점 좌표를 정수로 변환하는 양자화(quantization)가 두 번 발생한다. 이 과정에서 미세한 위치 정보의 손실이 발생하는데, 이는 경계 상자 예측에는 큰 영향을 미치지 않을 수 있지만, 픽셀 단위의 정밀한 마스크 예측이 필요한 인스턴스 분할(Instance Segmentation)과 같은 작업에는 치명적인 오정렬(misalignment)을 유발할 수 있다.38</li>
</ul>
<h3>6.2  주요 개선 모델 1: Feature Pyramid Network (FPN)</h3>
<p>FPN은 Faster R-CNN의 작은 객체 탐지 성능 한계를 극복하기 위해 제안되었다. 이는 CNN의 계층적 특징을 효과적으로 결합하여 모든 스케일에서 강력한 특징 표현을 생성하는 구조이다.37</p>
<ul>
<li><strong>구조</strong>: FPN은 세 가지 핵심 요소로 구성된다.</li>
</ul>
<ol>
<li><strong>Bottom-up pathway</strong>: 일반적인 Backbone CNN(예: ResNet)을 통해 각기 다른 해상도와 의미 수준을 가진 특징 맵 계층(<span class="math math-inline">C_2, C_3, C_4, C_5</span>)을 생성한다.</li>
<li><strong>Top-down pathway</strong>: 가장 상위 계층의 의미론적으로 풍부하지만 해상도가 낮은 특징 맵(<span class="math math-inline">C_5</span>)을 점진적으로 업샘플링(upsampling)하여 공간 해상도를 복원한다.</li>
<li><strong>Lateral connections</strong>: Top-down 경로의 업샘플링된 맵과 Bottom-up 경로의 동일한 해상도를 가진 맵을 <code>1x1</code> 컨볼루션을 통해 채널 수를 맞춘 후, 요소별 덧셈(element-wise addition)으로 결합한다.</li>
</ol>
<ul>
<li><strong>효과</strong>: 이 과정을 통해 생성된 최종 특징 맵 피라미드(<span class="math math-inline">P_2, P_3, P_4, P_5</span>)는 모든 스케일의 특징 맵이 고수준의 의미 정보(무엇인지)와 저수준의 정확한 공간 정보(어디인지)를 모두 갖게 된다. RPN과 최종 탐지기는 단일 스케일의 특징 맵이 아닌 이 다중 스케일 특징 맵 피라미드 위에서 작동함으로써, 특히 <strong>작은 객체에 대한 탐지 성능이 비약적으로 향상</strong>된다.37</li>
</ul>
<h3>6.3  주요 개선 모델 2: Mask R-CNN</h3>
<p>Mask R-CNN은 Faster R-CNN을 객체 탐지에서 인스턴스 분할(Instance Segmentation) 작업으로 자연스럽게 확장한 모델이다. 이는 객체의 경계 상자뿐만 아니라, 해당 객체에 속하는 모든 픽셀을 정확히 구분하는 픽셀 단위의 마스크(mask)까지 예측한다.16</p>
<ul>
<li><strong>구조적 개선</strong>:</li>
</ul>
<ol>
<li>Faster R-CNN의 최종 헤드(분류기, 회귀기)에 병렬적으로 **마스크 예측 브랜치(Mask Prediction Branch)**를 추가했다. 이 브랜치는 각 RoI에 대해 이진 마스크를 생성하는 작은 FCN(Fully Convolutional Network)이다.39</li>
<li>RoI Pooling의 정보 손실 문제를 해결하기 위해 <strong>RoIAlign</strong>으로 대체했다.</li>
</ol>
<ul>
<li><strong>RoIAlign의 원리</strong>: RoIAlign은 RoI Pooling의 양자화 문제를 해결하기 위해 부동 소수점 좌표를 그대로 사용한다. RoI 영역을 고정된 수의 그리드로 나누는 것은 동일하지만, 각 그리드 셀의 값을 결정할 때 정수 좌표의 픽셀 값만 사용하는 대신, 그리드 셀 내에 균일하게 샘플링 포인트를 찍고 각 포인트의 값을 **쌍선형 보간법(Bilinear Interpolation)**을 통해 주변 4개 픽셀 값으로부터 정확하게 계산한다. 이를 통해 특징 맵과 RoI 사이의 미세한 오정렬 문제를 해결하여 마스크 예측의 정확도를 크게 향상시켰다.38</li>
</ul>
<h3>6.4  주요 개선 모델 3: Cascade R-CNN</h3>
<p>Cascade R-CNN은 탐지기의 품질이 학습에 사용된 IoU(Intersection over Union) 임계값에 크게 의존한다는 관찰에서 출발한다. 낮은 IoU 임계값(예: 0.5)으로 학습된 탐지기는 노이즈가 많은 예측을 생성하는 경향이 있고, 높은 IoU 임계값으로 학습시키면 Positive 샘플이 급격히 줄어 과적합 문제가 발생한다. Cascade R-CNN은 이 문제를 해결하기 위해 점진적으로 더 높은 IoU 임계값으로 학습된 탐지기들을 연쇄적으로(cascade) 연결한 다단계 구조를 제안했다.45</p>
<ul>
<li><strong>작동 원리</strong>:</li>
</ul>
<ol>
<li>첫 번째 단계의 탐지기는 상대적으로 낮은 IoU 임계값(예: 0.5)으로 학습되어 초기 경계 상자 예측을 수행한다.</li>
<li>이 예측 결과는 다음 단계 탐지기의 입력으로 전달된다. 두 번째 단계의 탐지기는 더 높은 IoU 임계값(예: 0.6)으로 학습되었기 때문에, 첫 단계에서 생성된 비교적 품질이 좋은 후보 영역들을 입력받아 더 정밀하게 위치를 보정한다.</li>
<li>이 과정을 여러 단계(보통 3단계) 반복하면서, 각 단계의 분류기와 회귀기는 점차 더 높은 품질의 후보 영역(high-quality proposals)에 특화되도록 학습된다.</li>
</ol>
<ul>
<li><strong>효과</strong>: 이 방식은 학습 시 높은 IoU 임계값에서 발생하는 Positive 샘플 부족 문제를 완화하고, 추론 시 탐지기가 처리하는 후보 영역의 품질과 탐지기 자체의 품질 간의 불일치를 줄여준다. 결과적으로, IoU 임계값이 높은 까다로운 기준에서도 성능 저하 없이 <strong>고품질(High-quality)의 탐지</strong>를 안정적으로 달성할 수 있다.46</li>
</ul>
<p>이러한 후속 연구들의 흐름은 Faster R-CNN이 정의한 프레임워크가 얼마나 견고하고 확장 가능한 기반을 제공했는지를 명확히 보여준다. FPN, Mask R-CNN, Cascade R-CNN 등은 완전히 새로운 모델을 제안하기보다는, Faster R-CNN의 특정 ‘모듈’(특징 추출부, RoI 처리부, 최종 탐지 헤드)을 집중적으로 공략하고 개선하거나 교체하는 ’모듈식 혁신(Modular Innovation)’의 양상을 띤다. FPN은 ’Backbone’을, Mask R-CNN은 ’RoI Pooling’과 ’Detection Head’를, Cascade R-CNN은 ’Detection Head’의 작동 방식 자체를 혁신했다. 이는 Faster R-CNN이 제안한라는 파이프라인이 하나의 표준 ’플랫폼’처럼 작용했으며, 후속 연구자들이 이 플랫폼의 각 부품을 더 성능 좋은 부품으로 교체하거나 새로운 기능을 추가하는 방식으로 전체 시스템을 발전시켰음을 의미한다.</p>
<h2>7.  결론: Faster R-CNN의 학술적 의의와 산업적 영향</h2>
<h3>7.1  RPN의 혁신성 요약</h3>
<p>Faster R-CNN의 가장 크고 지속적인 공헌은 후보 영역 제안 과정을 학습 가능한 신경망인 RPN으로 대체하여, 2-Stage 객체 탐지 파이프라인을 최초로 완전한 End-to-End 통합 네트워크로 구현했다는 점이다. 이는 단순히 이전 모델들의 계산 병목을 해결한 기술적 성과를 넘어, 영역 제안이라는 과정을 데이터 기반으로 최적화하는 새로운 패러다임을 열었다. RPN은 사실상 네트워크가 스스로 이미지의 어떤 부분에 ’집중해야 할지(attention)’를 학습하는 메커니즘으로 작용했으며, 이는 객체 탐지 분야의 발전에 지대한 영향을 미쳤다.17</p>
<h3>7.2  현대 객체 탐지 모델에 미친 영향과 유산</h3>
<p>Faster R-CNN이 제시한 RPN, 앵커 박스, 특징 공유, 다중 작업 손실 함수 등의 핵심 개념들은 이후 등장한 수많은 1-Stage 및 2-Stage 탐지 모델들의 기초가 되었다.17 FPN, Mask R-CNN, Cascade R-CNN과 같은 직접적인 후속 연구들은 Faster R-CNN의 견고한 프레임워크 위에서 특정 문제를 해결하며 객체 탐지 기술의 지평을 넓혔다.</p>
<p>산업적으로 Faster R-CNN과 그 파생 모델들은 자율 주행에서의 차량 및 보행자 인식, 의료 영상 분석에서의 종양 탐지, 소매점에서의 재고 관리, 지능형 감시 시스템 등 다양한 분야에서 고정밀 객체 탐지 기술의 상용화를 앞당기는 데 결정적인 역할을 했다.4</p>
<p>비록 최신 모델들에 비해 속도는 다소 느릴 수 있지만, Faster R-CNN은 높은 정확도와 견고한 구조 덕분에 오늘날에도 여전히 많은 연구에서 강력한 비교 기준(baseline)으로 사용되고 있다. 이는 Faster R-CNN이 단순히 한 시대의 뛰어난 모델을 넘어, 객체 탐지 기술의 발전을 논할 때 결코 빼놓을 수 없는 중요한 학술적, 기술적 이정표로 남아있음을 증명한다.18</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Faster R-CNN Explained for Object Detection Tasks - DigitalOcean, https://www.digitalocean.com/community/tutorials/faster-r-cnn-explained-object-detection</li>
<li>
<ol start="2">
<li>faster R-CNN - 공부하려고 만든 블로그 - 티스토리, https://welcome-to-dewy-world.tistory.com/110</li>
</ol>
</li>
<li>R-CNN &amp; Fast R-CNN 비교 정리, https://candyz.tistory.com/20</li>
<li>Faster R-CNN: Breakthrough in Object Detection Tech - Viso Suite, https://viso.ai/deep-learning/faster-r-cnn-2/</li>
<li>[논문리뷰]R-CNN, Fast R-CNN, Faster R-CNN - velog, <a href="https://velog.io/@yunseo4401/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0R-CNN-Fast-R-CNN-Faster-R-CNN">https://velog.io/@yunseo4401/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0R-CNN-Fast-R-CNN-Faster-R-CNN</a></li>
<li>R-CNN, Fast R-CNN, Faster R-CNN - Deeper Learning - 티스토리, https://dlaiml.tistory.com/entry/R-CNN-Fast-R-CNN-Faster-R-CNN</li>
<li>Understanding and Implementing Faster R-CNN | by Rishabh Singh | Medium, https://medium.com/@RobuRishabh/understanding-and-implementing-faster-r-cnn-248f7b25ff96</li>
<li>Real-Time Object Detection: Comparing YOLO and SSD Architectures in Surveillance Systems - Science Excel, https://www.sciencexcel.com/articles/WctZlQ6tKQTUDzPNXlUdq1MfOKtAAT10ZNaSZXrZ.pdf</li>
<li>[X:AI] Faster-RCNN 논문 리뷰 - hyeon827 - 티스토리, https://hyeon827.tistory.com/76</li>
<li>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks - arXiv, https://arxiv.org/abs/1506.01497</li>
<li>Faster R-CNN: Towards Real-Time Object Detection with … - arXiv, https://arxiv.org/pdf/1506.01497</li>
<li>R-CNN &amp; Fast R-CNN &amp; Faster R-CNN 간단 비교(feat. RPN (region proposal network)) - 초보 개발자의 일기장 - 티스토리, https://iambeginnerdeveloper.tistory.com/204</li>
<li>[딥러닝] Object detection (2)(R-CNN / Fast R-CNN / Faster R-CNN 총정리) - 비전공자 데이터분석 노트 - 티스토리, https://bigdaheta.tistory.com/60</li>
<li>(논문리뷰&amp;재구현) Faster R-CNN 설명 및 정리 - 프라이데이, https://ganghee-lee.tistory.com/37</li>
<li>컴퓨터 비전 - 10. R-CNN vs. SPP-net vs. Fast R-CNN vs. Faster R-CNN 개요 - 귀퉁이 서재, <a href="https://bkshin.tistory.com/entry/%EC%BB%B4%ED%93%A8%ED%84%B0-%EB%B9%84%EC%A0%84-10-R-CNN-vs-SPP-net-vs-Fast-R-CNN-vs-Faster-R-CNN-%EA%B0%9C%EC%9A%94">https://bkshin.tistory.com/entry/%EC%BB%B4%ED%93%A8%ED%84%B0-%EB%B9%84%EC%A0%84-10-R-CNN-vs-SPP-net-vs-Fast-R-CNN-vs-Faster-R-CNN-%EA%B0%9C%EC%9A%94</a></li>
<li>Faster R-CNN | ML - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/faster-r-cnn-ml/</li>
<li>[논문 리뷰] Faster R-CNN - Deep Paper - 티스토리, https://deep-math.tistory.com/26</li>
<li>Faster RCNN in 2025: How it works and why it’s still the benchmark for Object Detection, https://www.thinkautonomous.ai/blog/faster-rcnn/</li>
<li>Faster R-CNN Object Detector | ArcGIS API for Python - Esri Developer, https://developers.arcgis.com/python/latest/guide/faster-rcnn-object-detector/</li>
<li>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Network, https://hugrypiggykim.com/2018/02/09/faster-r-cnn-towards-real-time-object-detection-with-region-proposal-network/</li>
<li>Faster R-CNN 논문 리뷰 &amp; R-CNN 계열의 발전 동향 | by Heejun Park …, <a href="https://medium.com/@parkie0517/r-cnn-%EA%B3%84%EC%97%B4%EC%9D%98-%EB%B0%9C%EC%A0%84-%EB%8F%99%ED%96%A5%EA%B3%BC-faster-r-cnn-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-155f46cca6ad">https://medium.com/@parkie0517/r-cnn-%EA%B3%84%EC%97%B4%EC%9D%98-%EB%B0%9C%EC%A0%84-%EB%8F%99%ED%96%A5%EA%B3%BC-faster-r-cnn-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-155f46cca6ad</a></li>
<li>Faster R-CNN - AI-BLACK-TIGER - 티스토리, https://ai-bt.tistory.com/entry/03-Faster-R-CNN</li>
<li>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks - velog, <a href="https://velog.io/@hsj1915/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks">https://velog.io/@hsj1915/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks</a></li>
<li>[논문리뷰/CV] Faster R-CNN 모델 (R-CNN, Fast R-CNN과 비교), https://shashacode.tistory.com/95</li>
<li>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (번역) - 프린이씨롯메 - 티스토리, https://hsejun07.tistory.com/66</li>
<li>객체 검출에서 후보 영역을 생성하는 네트워크 | Region Proposal Network 설명 - CV DOODLE, https://mvje.tistory.com/194</li>
<li>Faster R-CNN 논문(Faster R-CNN: Towards Real-Time ObjectDetection with Region Proposal Networks) 리뷰 - 약초의 숲으로 놀러오세요, https://herbwood.tistory.com/10</li>
<li>Faster R-CNN, https://ericwiener.github.io/ai-notes/AI-Notes/Object-Detection/Faster-R-CNN</li>
<li>Object Detection — Anchor Box VS Bounding Box | by Nikita Malviya | Medium, https://medium.com/@nikitamalviya/object-detection-anchor-box-vs-bounding-box-bf1261f98f12</li>
<li>Finally understand Anchor Boxes in Object Detection (2D and 3D), https://www.thinkautonomous.ai/blog/anchor-boxes/</li>
<li>Region Proposal Network (RPN) : A Complete Guide, https://www.listendata.com/2022/06/region-proposal-network.html</li>
<li>anchor box or bounding boxes in Yolo or Faster RCNN - Stack Overflow, https://stackoverflow.com/questions/50450998/anchor-box-or-bounding-boxes-in-yolo-or-faster-rcnn</li>
<li>Object detection: speed and accuracy comparison (Faster R-CNN, R …, https://jonathan-hui.medium.com/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359</li>
<li>A Comprehensive Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS - MDPI, https://www.mdpi.com/2504-4990/5/4/83</li>
<li>Faster RCNN : r/deeplearning - Reddit, https://www.reddit.com/r/deeplearning/comments/jqvye0/faster_rcnn/</li>
<li>Object Detection Based on Faster R-CNN Algorithm with Skip Pooling and Fusion of Contextual Information - MDPI, https://www.mdpi.com/1424-8220/20/19/5490</li>
<li>Understanding Feature Pyramid Networks for object detection (FPN …, https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c</li>
<li>Mask R-CNN Explained: Guide, Uses &amp; YOLO | Ultralytics, https://www.ultralytics.com/blog/what-is-mask-r-cnn-and-how-does-it-work</li>
<li>Improved Mask R-CNN Multi-Target Detection and Segmentation for Autonomous Driving in Complex Scenes - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC10146362/</li>
<li>Feature Pyramid Network (FPN) - GeeksforGeeks, https://www.geeksforgeeks.org/computer-vision/feature-pyramid-network-fpn/</li>
<li>HA-FPN: Hierarchical Attention Feature Pyramid Network for Object Detection - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC10181737/</li>
<li>ssFPN: Scale Sequence (S 2 ) Feature-Based Feature Pyramid Network for Object Detection, https://www.mdpi.com/1424-8220/23/9/4432</li>
<li>Improvement of Mask R-CNN Algorithm for Ore Segmentation - MDPI, https://www.mdpi.com/2079-9292/14/10/2025</li>
<li>R-CNN, Fast R-CNN, Faster R-CNN, and Mask R-CNN | by Okan Yenigün | Towards Dev - Medium, https://medium.com/towardsdev/r-cnn-fast-r-cnn-faster-r-cnn-and-mask-r-cnn-e7cd2e6f0a82</li>
<li>www.geeksforgeeks.org, <a href="https://www.geeksforgeeks.org/deep-learning/cascade-r-cnn-explained/#:~:text=Cascade%20R-CNN%20proposed%20as,over%20Union%20(IoU)%20thresholds.">https://www.geeksforgeeks.org/deep-learning/cascade-r-cnn-explained/#:~:text=Cascade%20R%2DCNN%20proposed%20as,over%20Union%20(IoU)%20thresholds.</a></li>
<li>Cascade R-CNN- Explained - GeeksforGeeks, https://www.geeksforgeeks.org/deep-learning/cascade-r-cnn-explained/</li>
<li>Cascade R-CNN, http://www.svcl.ucsd.edu/projects/cascade-rcnn/</li>
<li>Cascade R-CNN | CloudFactory Computer Vision Wiki, https://wiki.cloudfactory.com/docs/mp-wiki/model-architectures/cascade-rcnn</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>