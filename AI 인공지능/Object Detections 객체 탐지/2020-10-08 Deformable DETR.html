<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Deformable DETR (2020-10-08)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Deformable DETR (2020-10-08)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>Deformable DETR (2020-10-08)</span></nav>
                </div>
            </header>
            <article>
                <h1>Deformable DETR (2020-10-08)</h1>
<h2>1.  DETR의 혁신과 내재적 한계</h2>
<h3>1.1  End-to-End 객체 탐지 패러다임의 등장</h3>
<p>객체 탐지(Object Detection) 분야는 컴퓨터 비전의 핵심 과제 중 하나로, 이미지 내에 존재하는 객체의 종류를 분류하고 그 위치를 경계 상자(bounding box)로 정확히 표시하는 것을 목표로 한다. 딥러닝 시대 이전의 전통적인 방법론부터 R-CNN 계열로 대표되는 현대적인 딥러닝 기반 탐지기에 이르기까지, 이 분야는 괄목할 만한 발전을 이루었다.1 그러나 이러한 발전의 이면에는 복잡한 파이프라인과 수많은 수작업 설계(hand-designed) 구성 요소라는 고질적인 문제가 존재했다.</p>
<p>Faster R-CNN과 같은 2단계(two-stage) 탐지기는 Region Proposal Network (RPN)를 통해 객체가 존재할 만한 후보 영역을 먼저 제안하고, 이후 각 후보 영역에 대해 분류와 경계 상자 회귀를 수행하는 복잡한 구조를 가졌다.3 YOLO나 SSD와 같은 1단계(one-stage) 탐지기는 파이프라인을 단순화했지만, 여전히 사전 정의된 수천 개의 앵커 박스(anchor box)에 크게 의존했다.1 이러한 앵커 박스는 객체의 크기, 종횡비 등에 대한 강력한 사전 정보(prior)를 제공하여 학습을 용이하게 했지만, 동시에 하이퍼파라미터에 민감하고 다양한 형태의 객체에 유연하게 대처하기 어렵다는 단점을 내포했다.3 더욱이, 대부분의 탐지기는 최종 예측 단계에서 중복된 경계 상자를 제거하기 위해 비최대 억제(Non-Maximum Suppression, NMS)라는 후처리 과정에 의존해야만 했다.3 NMS는 별도의 단계로 작동하여 전체 파이프라인의 End-to-End 학습을 저해하고, 모델의 공동 최적화를 방해하는 요인이었다.3</p>
<p>이러한 배경 속에서 2020년, Carion 등이 발표한 DETR (DEtection TRansformer)은 객체 탐지 분야에 새로운 패러다임을 제시했다.6 DETR은 자연어 처리 분야에서 성공을 거둔 Transformer 아키텍처를 객체 탐지에 도입하여, 앵커 박스나 NMS와 같은 기존의 복잡한 구성 요소들을 완전히 제거했다.3 대신, 객체 탐지를 ‘집합 예측(set prediction)’ 문제로 재정의하고, 고정된 수의 학습 가능한 객체 쿼리(object queries)를 사용하여 이미지 특징과 상호작용하도록 설계했다. 최종적으로 이분 매칭(bipartite matching)을 통해 예측값과 실제 정답(ground truth) 간의 일대일 대응을 찾아 손실을 계산함으로써, 최초의 완전한 End-to-End 객체 탐지 모델을 구현했다.1 이 혁신적인 접근법은 기존의 복잡한 파이프라인을 극적으로 단순화하며, 객체 탐지 연구의 새로운 방향을 열었다.4</p>
<h3>1.2  DETR의 핵심 문제점: 느린 수렴과 높은 연산 복잡도</h3>
<p>DETR의 혁신적인 설계는 큰 주목을 받았으나, 실용적인 관점에서 두 가지 치명적인 한계를 드러냈다. 첫 번째는 극도로 느린 수렴 속도였다.3 COCO 벤치마크에서 경쟁력 있는 성능을 달성하기 위해 DETR은 500 에폭(epoch)에 달하는 방대한 학습 시간을 요구했다.7 이는 12에서 36 에폭 내외로 수렴하는 Faster R-CNN과 비교했을 때 약 10배에서 20배 이상 긴 시간으로, 막대한 계산 자원과 시간을 소모하는 비효율성을 야기했다.7</p>
<p>이러한 느린 수렴의 근본 원인은 DETR의 핵심인 Transformer의 글로벌 어텐션(global attention) 메커니즘에 있었다.7 Transformer 인코더는 이미지 전체 픽셀 간의 관계를 모델링하기 위해 모든 픽셀 위치에 대해 어텐션을 계산한다. 학습 초기 단계에서, 모델은 어떤 픽셀이 중요한지에 대한 정보가 없기 때문에 어텐션 가중치는 이미지 전체에 걸쳐 거의 균일하게(uniformly) 분포된다.7 모델이 수많은 학습 반복을 거치면서 객체가 존재하는 희소한(sparse) 위치에 어텐션을 집중하도록 학습하는 과정은 매우 더디고 비효율적이었다.9 이는 마치 넓은 모래사장에서 바늘을 찾기 위해 매번 모래 전체를 훑어보는 것과 같았다.</p>
<p>두 번째 한계는 글로벌 어텐션의 높은 연산 복잡도였다. Transformer의 셀프 어텐션(self-attention) 연산은 입력 시퀀스의 길이에 대해 이차적(quadratic)인 계산 복잡도를 가진다.11 이미지 특징 맵을 시퀀스로 간주할 때, 그 길이는 특징 맵의 높이(H)와 너비(W)의 곱, 즉 픽셀의 수(<code>HW</code>)에 비례한다. 따라서 어텐션 가중치를 계산하는 데 필요한 연산량은 <code>O((HW)^2 * C)</code>로, 특징 맵의 해상도가 증가함에 따라 기하급수적으로 폭증한다.7 이러한 높은 연산 및 메모리 복잡도는 DETR이 고해상도 특징 맵을 처리하는 것을 사실상 불가능하게 만드는 심각한 병목 현상으로 작용했다.7</p>
<h3>1.3  작은 객체 탐지 성능의 한계와 그 원인</h3>
<p>느린 수렴과 높은 연산 복잡도는 또 다른 문제, 즉 작은 객체(small objects) 탐지 성능 저하로 이어졌다.3 현대의 객체 탐지기들은 다양한 크기의 객체를 효과적으로 탐지하기 위해 FPN(Feature Pyramid Network)과 같은 다중 스케일(multi-scale) 특징 맵 구조를 적극적으로 활용한다.13 특히, 픽셀 수가 적은 작은 객체를 정밀하게 탐지하기 위해서는 고해상도 특징 맵에 담긴 세밀한 공간 정보가 필수적이다.7</p>
<p>그러나 DETR은 바로 이 지점에서 구조적인 딜레마에 직면했다. DETR의 핵심 장점인 글로벌 어텐션 메커니즘은 이미지 전체의 문맥 정보를 파악하여 객체 간의 복잡한 관계를 이해하는 데 탁월한 능력을 보였다.3 하지만 이 메커니즘의 이차적인 연산 복잡도는 고해상도 특징 맵의 사용을 원천적으로 차단했다.7 고해상도 맵을 입력으로 사용하면 연산량이 감당할 수 없을 정도로 폭증하기 때문에, DETR은 어쩔 수 없이 CNN 백본(backbone)의 마지막 레이어에서 추출된 저해상도 특징 맵에 의존해야만 했다.14 이 저해상도 맵에서는 작은 객체들의 세부 정보가 이미 소실된 상태이므로, 모델이 이를 정확히 탐지하는 것은 매우 어려운 과제가 되었다.</p>
<p>결론적으로, DETR은 자신의 가장 큰 장점인 ’글로벌 문맥 이해 능력’을 구현하는 글로벌 어텐션 때문에, 객체 탐지의 기본적인 요구사항인 ’고해상도 특징 활용’을 충족시킬 수 없는 ’구조적 자기모순’에 빠져 있었다. 이는 단순히 성능이 조금 낮은 문제가 아니라, 모델의 설계 철학 자체에 내재된 근본적인 한계였다. Deformable DETR은 바로 이 모순을 해결하고 Transformer 기반 객체 탐지기의 실용성을 확보하기 위한 결정적인 돌파구로서 제안되었다.</p>
<h2>2.  Deformable DETR의 제안: 희소 어텐션을 통한 효율성 극대화</h2>
<h3>2.1  Deformable Convolution과 Transformer의 개념적 결합</h3>
<p>DETR이 직면한 근본적인 문제를 해결하기 위해, 연구자들은 Transformer의 어텐션 메커니즘 자체를 재고하기 시작했다. 그들은 이미지의 모든 픽셀을 동등하게 고려하는 글로벌 어텐션의 비효율성에 주목하고, 보다 적응적이고 효율적인 정보 수집 방식의 필요성을 인식했다. 이러한 고민의 해답은 Deformable Convolution Networks (DCN)에서 발견되었다.7</p>
<p>Deformable Convolution은 기존의 고정된 사각형 격자 형태의 컨볼루션 필터에서 벗어나, 데이터에 따라 샘플링 위치를 동적으로 학습하는 혁신적인 아이디어였다.15 컨볼루션 필터의 각 샘플링 위치에 학습 가능한 오프셋(offset)을 더함으로써, 필터는 객체의 실제 형태와 크기에 맞춰 변형될 수 있었다. 이는 모델이 기하학적 변형에 강인한 특징을 학습하도록 도왔다.</p>
<p>Deformable DETR의 핵심 철학은 바로 이 Deformable Convolution의 ‘희소 공간 샘플링(sparse spatial sampling)’ 능력과 Transformer의 ‘관계 모델링(relation modeling)’ 능력을 결합하는 것이었다.7 이미지 전체를 무차별적으로 보는 대신, 객체와 관련성이 높을 것으로 예상되는 소수의 중요한 지점들만 선택적으로 샘플링하여 어텐션을 계산하자는 아이디어였다. 이는 Deformable Convolution이 고정된 필터 영역을 넘어 중요한 특징을 찾아 샘플링하는 방식과 개념적으로 일치했다. 이 개념적 결합을 통해, Deformable DETR은 Transformer의 강력한 문맥 이해 능력을 유지하면서도, Deformable Convolution의 효율성과 유연성을 어텐션 메커니즘에 이식하고자 했다.</p>
<h3>2.2  핵심 아이디어: 참조점 기반의 희소 공간 샘플링</h3>
<p>Deformable DETR의 구체적인 구현 방식은 ’참조점(reference point) 기반의 희소 공간 샘플링’이라는 아이디어로 구체화되었다. 이는 기존 DETR의 어텐션 방식과는 근본적으로 다른 접근법이다.</p>
<p>기존 어텐션이 이미지의 모든 픽셀을 잠재적인 ’키(key)’로 간주하고, 각 ’쿼리(query)’가 모든 키와 상호작용하여 어텐션 가중치를 계산했다면, Deformable DETR은 이 과정을 대폭 간소화했다. 새로운 어텐션 모듈은 각 쿼리에 대해 하나의 2차원 참조점을 먼저 설정한다. 이 참조점은 어텐션의 중심점 역할을 한다. 그런 다음, 모델은 이 참조점 주변의 소수의(a small set of) 키 샘플링 포인트(key sampling points)에만 집중하여 어텐션을 수행한다.7</p>
<p>가장 중요한 점은 이 샘플링 포인트들의 위치가 고정되어 있지 않다는 것이다. 모델은 쿼리 자체의 특징을 바탕으로 참조점으로부터의 상대적인 변위, 즉 오프셋을 학습한다. 다시 말해, 모델은 ’어디를 봐야 할지’를 스스로 학습하여 동적으로 어텐션의 초점을 맞춘다. 이 방식은 전체 특징 맵 픽셀 중에서 가장 두드러진 정보를 걸러내는 일종의 사전 필터(pre-filter) 역할을 수행하며, 불필요한 계산을 원천적으로 차단한다.7 이를 통해 Deformable DETR은 DETR의 고질적인 문제였던 높은 연산 복잡도와 느린 수렴 속도를 동시에 해결할 수 있는 이론적 기반을 마련했다.</p>
<h2>3.  핵심 메커니즘: Deformable Attention Module 심층 분석</h2>
<p>Deformable DETR의 혁신은 ’Deformable Attention Module’이라는 새로운 메커니즘에 집약되어 있다. 이 모듈은 기존 Transformer의 Multi-Head Attention을 대체하여 효율성과 성능을 극대화하는 핵심적인 역할을 수행한다.</p>
<h3>3.1  Multi-Head Attention과의 근본적 차이점</h3>
<p>표준 Multi-Head Attention과 Deformable Attention의 가장 근본적인 차이는 어텐션 가중치와 샘플링 위치를 결정하는 방식에 있다. 표준 어텐션은 ‘매칭(matching)’ 기반으로 작동한다. 즉, 주어진 쿼리(<code>Q</code>) 벡터를 모든 키(<code>K</code>) 벡터와 내적(dot-product)하여 유사도를 계산하고, 이를 소프트맥스(softmax) 함수에 통과시켜 각 키에 대한 어텐션 가중치를 얻는다.21 이 과정은 모든 키-쿼리 쌍에 대한 상호작용을 전제로 하므로, 키의 개수가 많아질수록 연산량이 폭발적으로 증가한다.</p>
<p>반면, Deformable Attention은 이러한 매칭 과정을 완전히 생략하고 ‘능동적 정보 수집(directed polling)’ 방식으로 작동한다. 이 모듈에서는 키(Key) 벡터를 계산하는 과정 자체가 존재하지 않는다.7 대신, 쿼리 특징(<code>z_q</code>) 하나가 모든 것을 결정한다. 쿼리 특징은 간단한 선형 변환(linear projection)을 통해 두 가지 정보를 동시에 예측한다: 첫째, 참조점(<code>p_q</code>)으로부터 얼마나 떨어진 위치를 샘플링할 것인지를 나타내는 ‘샘플링 오프셋(<code>Δp</code>)’, 둘째, 샘플링된 각 위치의 특징을 얼마나 중요하게 반영할 것인지를 나타내는 ’어텐션 가중치(<code>A</code>)’이다.7</p>
<p>이는 어텐션의 패러다임을 근본적으로 전환한 것이다. 수동적으로 모든 키와 자신을 비교하여 관계의 강도를 결정하는 방식에서, 쿼리 스스로가 능동적으로 정보를 수집할 위치와 그 중요도를 직접 결정하는 방식으로 바뀐 것이다. 이러한 패러다임의 전환은 연산 효율성을 극대화하는 동시에, 모델이 보다 유연하고 데이터 의존적인(data-dependent) 어텐션 패턴을 학습할 수 있도록 하는 결정적인 계기가 되었다.</p>
<h3>3.2  Deformable Attention의 수학적 원리 (핵심 수식 포함)</h3>
<p>Deformable Attention Module의 작동 원리는 다음의 핵심 수식으로 명확하게 표현된다. 입력 특징 맵 <span class="math math-inline">x \in \mathbb{R}^{C \times H \times W}</span>, 쿼리 특징 <span class="math math-inline">z_q \in \mathbb{R}^{C}</span>, 그리고 2차원 참조점 <span class="math math-inline">p_q</span>가 주어졌을 때, Deformable Attention의 출력은 다음과 같이 계산된다 7:<br />
<span class="math math-display">
\text{DeformAttn}(z_q, p_q, x) = \sum_{m=1}^{M} W_m \left( \sum_{k=1}^{K} A_{mqk} \cdot W&#39;_m x(p_q + \Delta p_{mqk}) \right)
</span><br />
이 수식은 Deformable Attention의 두 가지 핵심 과정을 보여준다.</p>
<ol>
<li><strong>희소 샘플링 (Sparse Sampling)</strong>: 안쪽 시그마(<span class="math math-inline">\sum_{k=1}^{K}</span>)는 샘플링 과정을 나타낸다.</li>
</ol>
<ul>
<li><span class="math math-inline">m</span>: <span class="math math-inline">M</span>개의 어텐션 헤드(head) 중 현재 헤드의 인덱스를 의미한다.</li>
<li><span class="math math-inline">k</span>: 헤드당 샘플링하는 <span class="math math-inline">K</span>개의 포인트(예: <span class="math math-inline">K=4</span>) 중 현재 포인트의 인덱스를 의미한다.23</li>
<li><span class="math math-inline">\Delta p_{mqk}</span>: <span class="math math-inline">m</span>번째 헤드에서 <span class="math math-inline">k</span>번째 샘플링 포인트를 위해 학습된 2차원 오프셋(offset) 벡터이다.</li>
<li><span class="math math-inline">x(p_q + \Delta p_{mqk})</span>: 참조점 <span class="math math-inline">p_q</span>에 오프셋 <span class="math math-inline">\Delta p_{mqk}</span>를 더하여 계산된 최종 샘플링 좌표에서 특징 맵 <span class="math math-inline">x</span>의 값을 가져오는 연산이다. 이 좌표는 실수(fractional) 값을 가질 수 있으므로, 주변 픽셀 값을 이용하여 보간(interpolation)하는 과정이 필요하다. 일반적으로 쌍선형 보간(bilinear interpolation)이 사용된다.12</li>
</ul>
<ol start="2">
<li><strong>가중 합 (Weighted Sum)</strong>: 바깥쪽 시그마(<span class="math math-inline">\sum_{m=1}^{M}</span>)는 샘플링된 특징들을 종합하는 과정을 나타낸다.</li>
</ol>
<ul>
<li><span class="math math-inline">W&#39;_m</span>: 샘플링된 특징 값을 각 헤드에 맞게 변환하는 선형 변환 행렬이다.</li>
<li><span class="math math-inline">A_{mqk}</span>: <span class="math math-inline">m</span>번째 헤드에서 <span class="math math-inline">k</span>번째 샘플링 포인트에 할당된 어텐션 가중치(스칼라 값)이다. 이 가중치는 <span class="math math-inline">\sum_{k=1}^{K} A_{mqk} = 1</span>로 정규화된다.</li>
<li><span class="math math-inline">W_m</span>: 각 헤드에서 계산된 결과들을 최종 출력으로 통합하기 위한 선형 변환 행렬이다.</li>
</ul>
<p>결론적으로, 이 수식은 각 쿼리가 <span class="math math-inline">M</span>개의 헤드를 통해 각각 <span class="math math-inline">K</span>개의 중요한 위치를 동적으로 찾아내고(<span class="math math-inline">\Delta p_{mqk}</span>), 그 위치들의 특징 값을 중요도(<span class="math math-inline">A_{mqk}</span>)에 따라 가중 평균하여 최종 특징을 생성하는 과정을 수학적으로 모델링한 것이다.</p>
<h3>3.3  샘플링 오프셋 및 어텐션 가중치의 학습 과정</h3>
<p>Deformable Attention의 유연성은 샘플링 오프셋(<span class="math math-inline">\Delta p_{mqk}</span>)과 어텐션 가중치(<span class="math math-inline">A_{mqk}</span>)가 모두 데이터로부터 직접 학습된다는 점에서 비롯된다. 이 학습 과정은 매우 간단하고 효율적인 방식으로 구현된다.</p>
<p>주어진 쿼리 특징 <span class="math math-inline">z_q</span>는 먼저 단일 선형 변환 레이어(a single linear projection layer)를 통과한다.7 이 레이어의 출력 채널 수는 <span class="math math-inline">3MK</span>로 설정된다. 여기서 <span class="math math-inline">M</span>은 헤드의 수, <span class="math math-inline">K</span>는 헤드당 샘플링 포인트의 수이다. 이 <span class="math math-inline">3MK</span> 차원의 출력 벡터는 세 부분으로 나뉘어 각각 오프셋과 가중치를 생성하는 데 사용된다.7</p>
<ol>
<li><strong>샘플링 오프셋(<span class="math math-inline">\Delta p_{mqk}</span>) 생성</strong>: 출력 벡터의 첫 <span class="math math-inline">2MK</span>개 채널은 샘플링 오프셋을 인코딩한다. 이 <span class="math math-inline">2MK</span>개의 값은 <span class="math math-inline">MK</span>개의 2차원 벡터로 재구성되어, 각 헤드(<span class="math math-inline">m</span>)와 각 샘플링 포인트(<span class="math math-inline">k</span>)에 대한 2D 오프셋 <span class="math math-inline">\Delta p_{mqk} \in \mathbb{R}^2</span>가 된다. 이 오프셋 값에는 별도의 활성화 함수가 적용되지 않아, 참조점으로부터 어떤 방향으로든 자유롭게 이동할 수 있는 유연성을 가진다.</li>
<li><strong>어텐션 가중치(<span class="math math-inline">A_{mqk}</span>) 생성</strong>: 나머지 <span class="math math-inline">MK</span>개 채널은 어텐션 가중치를 생성하는 데 사용된다. 이 <span class="math math-inline">MK</span>개의 값은 각 헤드별로 그룹화된 후, 소프트맥스(softmax) 함수를 통과한다. 소프트맥스 함수를 통해 각 헤드 내에서 <span class="math math-inline">K</span>개 샘플링 포인트의 가중치 합이 1이 되도록 정규화된 어텐션 가중치 <span class="math math-inline">A_{mqk}</span>가 계산된다 (<span class="math math-inline">\sum_{k=1}^{K} A_{mqk} = 1</span>).</li>
</ol>
<p>이러한 방식은 전체 어텐션 메커니즘이 미분 가능한(differentiable) 연산으로 구성되도록 보장한다. 따라서 샘플링 위치와 가중치를 결정하는 과정 전체가 손실 함수로부터의 그래디언트(gradient)를 받아 역전파(backpropagation)를 통해 End-to-End로 학습될 수 있다. 이는 모델이 최적의 샘플링 전략을 스스로 터득하게 하는 핵심적인 원리이다.</p>
<h3>3.4  연산 복잡도 분석: 선형 복잡도로의 전환</h3>
<p>Deformable Attention Module의 가장 큰 기여 중 하나는 Transformer의 고질적인 문제였던 이차적 연산 복잡도를 해결한 것이다. 이 모듈은 특징 맵의 전체 공간 크기(<span class="math math-inline">HW</span>)와 무관하게, 각 쿼리에 대해 항상 고정된 수(<span class="math math-inline">K</span>)의 포인트만을 샘플링한다. 이로 인해 연산 복잡도가 획기적으로 감소한다.</p>
<ul>
<li><strong>인코더(Encoder)의 Self-Attention</strong>:</li>
<li>기존 DETR: 모든 픽셀(<span class="math math-inline">HW</span>)이 쿼리이자 키가 되므로, 복잡도는 <span class="math math-inline">O((HW)^2 C)</span>였다.</li>
<li>Deformable DETR: 각 픽셀 쿼리가 <span class="math math-inline">K</span>개의 키만 샘플링한다. 총 쿼리 수는 <span class="math math-inline">HW</span>개이므로, 전체 복잡도는 <span class="math math-inline">O(HW \cdot K \cdot C^2)</span> 혹은 더 정확히는 <span class="math math-inline">O(HW \cdot C^2)</span>에 가깝게 분석된다. 이는 공간 크기 <span class="math math-inline">HW</span>에 대해 선형적(linear)이다.7</li>
<li><strong>디코더(Decoder)의 Cross-Attention</strong>:</li>
<li>기존 DETR: <span class="math math-inline">N_q</span>개의 객체 쿼리가 <span class="math math-inline">HW</span>개의 픽셀 키와 상호작용하므로, 복잡도는 <span class="math math-inline">O(N_q \cdot HW \cdot C)</span>였다.</li>
<li>Deformable DETR: <span class="math math-inline">N_q</span>개의 객체 쿼리가 각각 <span class="math math-inline">K</span>개의 키만 샘플링한다. 따라서 복잡도는 <span class="math math-inline">O(N_q \cdot K \cdot C^2)</span>로, 특징 맵의 공간 크기 <span class="math math-inline">HW</span>와는 무관(irrelevant)해진다.7</li>
</ul>
<p>이러한 선형 복잡도로의 전환은 Deformable DETR이 고해상도 특징 맵을 효율적으로 처리할 수 있게 만든 결정적인 요인이다. 이는 곧 DETR의 근본적인 한계였던 작은 객체 탐지 성능 저하 문제를 해결할 수 있는 이론적 토대가 되었다.21</p>
<h2>4.  아키텍처 상세: 다중 스케일 특징 처리와 인코더-디코더 구조</h2>
<p>Deformable DETR은 Deformable Attention Module을 핵심으로 하여 전체 아키텍처를 재구성했다. 특히 작은 객체 탐지 성능을 극대화하기 위해 다중 스케일 특징 맵을 효율적으로 처리하는 데 중점을 두었다.</p>
<h3>4.1  다중 스케일 특징 맵 생성 및 통합 전략</h3>
<p>작은 객체부터 큰 객체까지 다양한 크기의 객체를 효과적으로 탐지하기 위해서는 여러 해상도의 정보를 동시에 활용하는 것이 중요하다. Deformable DETR은 이를 위해 CNN 백본(예: ResNet-50)으로부터 다중 스케일 특징 맵을 추출한다.12</p>
<p>구체적으로, ResNet의 중간 스테이지인 C3, C4, C5의 출력 특징 맵을 사용한다. 이 특징 맵들은 입력 이미지 대비 각각 8배, 16배, 32배 축소된 해상도를 가진다. 각 특징 맵은 1x1 컨볼루션을 거쳐 채널 수가 256으로 통일된다. 여기에 더해, 가장 낮은 해상도의 특징 맵을 생성하기 위해 C5의 출력에 3x3 스트라이드(stride) 2 컨볼루션을 추가로 적용하여 64배 축소된 해상도를 가진 C6 특징 맵을 만든다.7 이렇게 생성된 총 4개의 스케일(<span class="math math-inline">L=4</span>)을 가진 특징 맵들이 Transformer 인코더의 입력으로 사용된다.</p>
<p>주목할 점은 Deformable DETR이 FPN(Feature Pyramid Network)과 같은 별도의 특징 융합 모듈을 사용하지 않는다는 것이다.7 FPN은 하향식 경로(top-down pathway)와 측면 연결(lateral connection)을 통해 고수준의 의미 정보와 저수준의 공간 정보를 결합하는 정교한 구조를 가진다. 그러나 Deformable DETR은 이러한 명시적인 융합 구조 대신, 뒤이어 설명할 Multi-scale Deformable Attention Module 자체가 서로 다른 스케일의 특징을 동적으로 샘플링하고 융합하는 역할을 수행하도록 설계했다.</p>
<h3>4.2  Multi-scale Deformable Attention Module의 작동 방식</h3>
<p>Multi-scale Deformable Attention Module은 단일 스케일 버전의 자연스러운 확장이다. 이 모듈은 쿼리가 단 하나의 특징 맵이 아닌, L개의 모든 특징 맵 풀(pool)에서 자유롭게 정보를 샘플링할 수 있도록 한다.</p>
<p>단일 스케일 버전에서 각 쿼리가 <span class="math math-inline">K</span>개의 포인트를 샘플링했다면, 다중 스케일 버전에서는 각 쿼리가 <span class="math math-inline">L</span>개의 모든 스케일에서 총 <span class="math math-inline">LK</span>개의 포인트를 샘플링한다.12 쿼리 특징 <span class="math math-inline">z_q</span>로부터 예측된 <span class="math math-inline">2LK</span>개의 오프셋과 <span class="math math-inline">LK</span>개의 어텐션 가중치는 <span class="math math-inline">L</span>개의 스케일 레벨과 <span class="math math-inline">K</span>개의 샘플링 포인트에 걸쳐 분배된다.</p>
<p>이때 모델이 각 픽셀 쿼리가 어떤 스케일 레벨에서 왔는지 구분할 수 있도록, 기존의 위치 임베딩(positional embedding)에 더해 학습 가능한 ‘스케일 레벨 임베딩(scale-level embedding)’ <span class="math math-inline">e_l</span>을 특징 표현에 추가한다.7 이를 통해 어텐션 모듈은 “이 쿼리는 고해상도 맵에서 왔으니, 주로 저해상도 맵의 넓은 문맥 정보를 참조해야겠다“와 같은 스케일 간의 관계를 학습할 수 있다.</p>
<p>이러한 설계는 고정된 규칙에 따라 특징을 융합하는 FPN과 근본적인 차이를 보인다. FPN이 정해진 파이프라인을 따라 정보를 혼합한다면, Multi-scale Deformable Attention은 각 쿼리의 필요에 따라 데이터 의존적으로 최적의 융합 전략을 실시간으로 학습하고 실행한다. 예를 들어, 이미지의 한 영역에 작은 객체가 의심될 경우, 해당 영역을 처리하는 쿼리는 고해상도 맵(예: C3)에서 더 많은 포인트를 샘플링하여 세부 정보를 확인하고, 동시에 저해상도 맵(예: C5, C6)에서 주변 문맥 정보를 가져와 최종 판단을 내릴 수 있다. 이러한 동적이고 유연한 특징 융합 능력은 Deformable DETR의 다중 스케일 객체 탐지 성능을 극대화하는 핵심 원리이다.</p>
<h3>4.3  Deformable Transformer 인코더의 구조와 역할</h3>
<p>Deformable DETR의 인코더는 기존 DETR의 모든 셀프 어텐션 모듈을 Multi-scale Deformable Attention 모듈로 대체한 구조를 가진다.11 인코더의 주된 역할은 입력된 다중 스케일 특징 맵을 정제하고 문맥 정보를 풍부하게 만들어 디코더가 객체를 쉽게 찾을 수 있도록 돕는 것이다.</p>
<p>인코더 내부에서, 특징 맵의 모든 픽셀은 하나의 쿼리(query) 역할을 수행한다. 각 픽셀 쿼리는 Multi-scale Deformable Attention을 통해 자기 자신을 참조점 삼아, 모든 스케일 레벨에 걸쳐 있는 다른 중요한 픽셀들의 정보를 샘플링하고 종합한다.7 이 과정을 통해 각 픽셀은 자신의 지역적 특징뿐만 아니라, 이미지 전체의 장거리 의존성 및 다른 스케일에 존재하는 관련 정보까지 통합된 풍부한 특징 표현을 갖게 된다.</p>
<p>Deformable Attention 덕분에 이 과정은 매우 효율적으로 이루어진다. 예를 들어, 객체의 경계에 위치한 픽셀은 같은 객체에 속하는 다른 경계 픽셀이나 객체 중심부의 픽셀 정보에 집중적으로 어텐션할 수 있으며, 배경에 위치한 픽셀은 연관성이 낮은 다른 영역에 대한 불필요한 계산을 수행하지 않는다. 이러한 방식으로 인코더는 객체의 특징을 강화하고 배경의 노이즈를 억제하여, 후속 디코더 단계의 부담을 크게 줄여준다.</p>
<h3>4.4  Deformable Transformer 디코더의 구조와 역할 (Cross-Attention 중심)</h3>
<p>디코더는 인코더가 정제한 특징 맵으로부터 실제 객체의 위치와 클래스를 예측하는 최종적인 역할을 수행한다. Deformable DETR의 디코더는 기존 DETR과 마찬가지로 객체 쿼리(object queries), 셀프 어텐션(self-attention), 크로스 어텐션(cross-attention), 그리고 FFN(Feed-Forward Network)으로 구성된다.</p>
<p>여기서 중요한 설계적 선택은, 디코더의 모든 어텐션 모듈을 교체하지 않고, ‘크로스 어텐션’ 모듈만 Multi-scale Deformable Attention으로 대체했다는 점이다.7</p>
<ul>
<li><strong>크로스 어텐션 (Cross-Attention)</strong>: 이 모듈은 객체 쿼리가 인코더 출력 특징 맵으로부터 객체에 대한 정보를 ’추출’하는 역할을 한다. 즉, 쿼리는 이미지 특징을 키(key)로 삼아 상호작용한다. 이 과정은 DETR의 연산 병목 현상이 가장 심하게 발생하는 부분 중 하나였기 때문에, Deformable Attention으로 대체하여 효율성을 극대화했다. 각 객체 쿼리는 자신의 임베딩으로부터 2D 참조점 <span class="math math-inline">p_q</span>를 예측하고, 이 참조점을 중심으로 인코더가 생성한 다중 스케일 특징 맵에서 희소하게 정보를 샘플링한다.7</li>
<li><strong>셀프 어텐션 (Self-Attention)</strong>: 이 모듈은 객체 쿼리들끼리 서로 상호작용하는 역할을 한다. 이를 통해 모델은 예측된 객체들 간의 관계를 학습하고, 예를 들어 중복된 예측을 억제하는 등의 역할을 수행한다. 쿼리 간의 상호작용은 이미지 특징 맵의 크기와 무관하므로, 기존의 표준 Multi-Head Attention을 그대로 사용해도 연산 부담이 크지 않다.7</li>
</ul>
<p>이러한 역할 분담은 매우 합리적인 설계이다. 연산량이 많은 ‘이미지 특징 추출’ 부분은 Deformable Attention으로 효율화하고, 연산 부담이 적고 객체 간 관계 모델링에 중요한 ‘쿼리 간 상호작용’ 부분은 기존의 강력한 글로벌 어텐션을 유지한 것이다. 이 구조를 통해 Deformable DETR은 효율성과 성능의 균형을 맞출 수 있었다.</p>
<h2>5.  성능 평가 및 분석: COCO 벤치마크를 중심으로</h2>
<p>Deformable DETR의 효과를 검증하기 위해, 논문에서는 대규모 객체 탐지 벤치마크인 COCO 2017 데이터셋을 사용하여 광범위한 실험을 수행했다. 실험 결과는 Deformable DETR이 기존 DETR의 핵심적인 문제점들을 성공적으로 해결했음을 정량적으로 입증한다.</p>
<h3>5.1  주요 모델과의 성능 비교 (DETR, Faster R-CNN)</h3>
<p>Deformable DETR의 전반적인 성능은 기존의 대표적인 모델인 Faster R-CNN 및 원본 DETR과 비교하여 평가되었다. 아래 표는 COCO 2017 검증 데이터셋(validation set)에서의 주요 성능 지표를 요약한 것이다.</p>
<table><thead><tr><th>Method</th><th>Epochs</th><th>AP</th><th>AP50</th><th>AP75</th><th>APS</th><th>APM</th><th>APL</th><th>Training GPU hours</th><th>Inference FPS</th></tr></thead><tbody>
<tr><td>Faster R-CNN + FPN</td><td>109</td><td>42.0</td><td>62.1</td><td>45.5</td><td>26.6</td><td>45.4</td><td>53.4</td><td>380</td><td>26</td></tr>
<tr><td>DETR</td><td>500</td><td>42.0</td><td>62.4</td><td>44.2</td><td>20.5</td><td>45.8</td><td>61.1</td><td>2000</td><td>28</td></tr>
<tr><td>DETR-DC5</td><td>500</td><td>43.3</td><td>63.1</td><td>45.9</td><td>22.5</td><td>47.3</td><td>61.1</td><td>7000</td><td>12</td></tr>
<tr><td>DETR-DC5+</td><td>50</td><td>36.2</td><td>57.0</td><td>37.4</td><td>16.3</td><td>39.2</td><td>53.9</td><td>700</td><td>12</td></tr>
<tr><td><strong>Deformable DETR</strong></td><td><strong>50</strong></td><td><strong>43.8</strong></td><td><strong>62.6</strong></td><td><strong>47.7</strong></td><td><strong>26.4</strong></td><td><strong>47.1</strong></td><td><strong>58.0</strong></td><td><strong>325</strong></td><td><strong>19</strong></td></tr>
<tr><td>+ iterative refinement</td><td>50</td><td>45.4</td><td>64.7</td><td>49.0</td><td>26.8</td><td>48.3</td><td>61.7</td><td>325</td><td>19</td></tr>
<tr><td>++ two-stage</td><td>50</td><td>46.2</td><td>65.2</td><td>50.0</td><td>28.8</td><td>49.2</td><td>61.7</td><td>340</td><td>19</td></tr>
</tbody></table>
<p>표 1: COCO 2017 val set에서 Deformable DETR과 다른 모델들의 성능 비교. AP는 Average Precision, AP50/AP75는 IoU 임계값이 각각 0.5, 0.75일 때의 AP, APS/APM/APL은 작은/중간/큰 객체에 대한 AP를 의미한다. 데이터는 7에서 종합되었다.</p>
<p>표 1에서 가장 주목할 만한 점은 Deformable DETR이 단 50 에폭의 학습만으로도 500 에폭을 학습한 DETR(42.0 AP)과 109 에폭을 학습한 Faster R-CNN(42.0 AP)을 뛰어넘는 43.8 AP를 달성했다는 것이다.7 이는 Deformable DETR이 훨씬 적은 학습 자원으로 더 높은 정확도를 달성할 수 있음을 의미한다. 추가적인 개선 기법인 반복적 바운딩 박스 개선(iterative bounding box refinement)과 2단계(two-stage) 방식을 적용했을 때는 AP가 각각 45.4, 46.2까지 상승하여, 당시 최고 수준의 성능을 기록했다.7</p>
<h3>5.2  학습 효율성 분석: 10배 빠른 수렴 속도 검증</h3>
<p>Deformable DETR의 가장 큰 성과는 학습 효율성의 극적인 향상이다. 표 1의 ’Epochs’와 ‘Training GPU hours’ 열은 이를 명확하게 보여준다.</p>
<p>DETR이 경쟁력 있는 성능(42.0 AP)에 도달하기 위해 500 에폭과 2000 GPU 시간을 필요로 했던 반면, Deformable DETR은 그보다 더 높은 성능(43.8 AP)을 단 50 에폭과 325 GPU 시간 만에 달성했다.7 이는 학습 에폭 기준으로는 10배, 총 학습 시간 기준으로는 약 6배 빠른 수렴 속도이다.7 이러한 결과는 Deformable Attention이 어텐션 가중치를 희소한 위치에 집중하도록 학습하는 과정을 얼마나 효율적으로 만들었는지를 입증한다. 이는 Transformer 기반 객체 탐지 모델을 실제 연구 및 산업 현장에서 활용할 수 있게 만든 실용적인 돌파구였다.</p>
<h3>5.3  작은 객체 탐지 성능(AP_S)의 획기적 개선 분석</h3>
<p>Deformable DETR은 DETR의 고질적인 약점이었던 작은 객체 탐지 성능을 성공적으로 개선했다. 표 1의 <code>APS</code> (AP for small objects) 열을 비교해 보면, DETR의 <code>APS</code>는 20.5로, 26.6을 기록한 Faster R-CNN에 비해 현저히 낮았다.7 이는 저해상도 특징 맵에 의존해야 했던 DETR의 구조적 한계를 반영한다.</p>
<p>반면, Deformable DETR은 <code>APS</code> 26.4를 기록하여 DETR 대비 약 6%p에 가까운 큰 폭의 성능 향상을 이루었으며, Faster R-CNN과 대등한 수준에 도달했다.7 2단계 방식을 적용했을 때는  <code>APS</code>가 28.8까지 상승하여 오히려 Faster R-CNN을 능가하는 성능을 보였다. 이 결과는 Deformable Attention이 이차적인 연산 복잡도 문제를 해결함으로써 고해상도 특징 맵을 효과적으로 활용할 수 있게 되었고, 이것이 작은 객체 탐지 능력의 직접적인 향상으로 이어졌음을 명확히 보여주는 증거이다.</p>
<h3>5.4  주요 구성 요소에 대한 Ablation Study</h3>
<p>Deformable DETR의 성능 향상이 어떤 설계 요소에서 비롯되었는지 과학적으로 분석하기 위해, 논문에서는 주요 구성 요소에 대한 제거 실험(Ablation Study)을 수행했다.</p>
<table><thead><tr><th>MS inputs</th><th>MS attention</th><th>K</th><th>FPNs</th><th>AP</th><th>AP50</th><th>AP75</th><th>APS</th><th>APM</th><th>APL</th></tr></thead><tbody>
<tr><td></td><td></td><td>1</td><td>w/o</td><td>39.7</td><td>60.1</td><td>42.4</td><td>21.2</td><td>44.3</td><td>56.0</td></tr>
<tr><td>X</td><td></td><td>1</td><td></td><td>41.4</td><td>60.9</td><td>44.9</td><td>24.1</td><td>44.6</td><td>56.1</td></tr>
<tr><td>X</td><td></td><td>4</td><td></td><td>42.3</td><td>61.4</td><td>46.0</td><td>24.8</td><td>45.1</td><td>56.3</td></tr>
<tr><td><strong>X</strong></td><td><strong>X</strong></td><td><strong>4</strong></td><td></td><td><strong>43.8</strong></td><td><strong>62.6</strong></td><td><strong>47.7</strong></td><td><strong>26.4</strong></td><td><strong>47.1</strong></td><td><strong>58.0</strong></td></tr>
<tr><td>X</td><td>X</td><td>4</td><td>FPN</td><td>43.8</td><td>62.6</td><td>47.8</td><td>26.5</td><td>47.3</td><td>58.1</td></tr>
<tr><td>X</td><td>X</td><td>4</td><td>BiFPN</td><td>43.9</td><td>62.5</td><td>47.7</td><td>25.6</td><td>47.4</td><td>57.7</td></tr>
</tbody></table>
<p>표 2: COCO 2017 val set에서 Deformable Attention에 대한 Ablation 연구. MS inputs는 다중 스케일 입력 사용 여부, MS attention은 다중 스케일 어텐션 사용 여부, K는 샘플링 포인트 수, FPNs는 FPN 추가 여부를 의미한다. 데이터는 7에서 종합되었다.</p>
<p>표 2의 결과는 다음과 같은 중요한 사실들을 알려준다.</p>
<ul>
<li><strong>다중 스케일 입력의 효과</strong>: 단일 스케일 입력(첫 번째 행) 대신 다중 스케일 입력(두 번째 행)을 사용하자 AP가 39.7에서 41.4로 1.7%p 상승했으며, 특히 <code>APS</code>는 21.2에서 24.1로 2.9%p나 크게 향상되었다.7 이는 작은 객체 탐지에 다중 스케일 정보가 얼마나 중요한지를 보여준다.</li>
<li><strong>샘플링 포인트 수의 효과</strong>: 헤드당 샘플링 포인트 <code>K</code>의 수를 1에서 4로 늘리자(세 번째 행), AP가 41.4에서 42.3으로 0.9%p 추가 상승했다.7 이는 더 많은 샘플링 포인트가 더 풍부한 문맥 정보를 포착하는 데 도움이 됨을 시사한다.</li>
<li><strong>다중 스케일 어텐션의 효과</strong>: 다중 스케일 어텐션, 즉 스케일 간 정보 교환을 허용하자(네 번째 행), AP가 42.3에서 43.8로 1.5%p나 크게 향상되었다.7 이는 Deformable Attention이 단순한 희소 샘플링을 넘어, 스케일 간 특징을 동적으로 융합하는 ’암시적 FPN’으로 기능한다는 분석을 강력하게 뒷받침한다.</li>
<li><strong>FPN 추가의 무효과성</strong>: 이미 다중 스케일 어텐션을 사용하는 모델에 FPN이나 BiFPN을 추가해도(다섯 번째, 여섯 번째 행) 성능 향상은 거의 없었다.7 이는 Deformable Attention이 이미 FPN의 역할을 충분히, 혹은 더 유연하게 수행하고 있음을 의미한다.</li>
</ul>
<p>이러한 체계적인 분석을 통해, Deformable DETR의 뛰어난 성능은 어느 하나의 기법이 아닌, 다중 스케일 입력, 다수의 샘플링 포인트, 그리고 스케일 간 정보 교환을 가능하게 하는 다중 스케일 어텐션이라는 여러 설계 요소들의 유기적인 결합의 결과임이 입증되었다.</p>
<h2>6.  추가 개선 기법 및 파생 모델</h2>
<p>Deformable DETR은 그 자체로도 뛰어난 성능을 보였지만, 유연한 아키텍처 덕분에 추가적인 개선 기법들과 쉽게 결합하여 성능을 더욱 향상시킬 수 있었다. 또한, 이 모델이 제시한 효율적인 어텐션 방식은 이후 수많은 후속 연구의 기술적 토대가 되었다.</p>
<h3>6.1  반복적 바운딩 박스 개선 (Iterative Bounding Box Refinement)</h3>
<p>이 기법은 디코더의 각 레이어가 독립적으로 예측을 수행하는 대신, 이전 레이어의 예측 결과를 입력받아 점진적으로 정교화하는 방식이다.7 디코더의 첫 번째 레이어가 객체의 대략적인 위치에 대한 초기 추정치를 생성하면, 두 번째 레이어는 이 초기 추정치를 참조하여 더 정확한 위치로 수정한다. 이 과정이 마지막 레이어까지 반복된다.</p>
<p>구체적으로, <code>d-1</code>번째 디코더 레이어에서 예측된 바운딩 박스 <span class="math math-inline">\hat{b}*{q}^{(d-1)}</span>가 주어지면, <code>d</code>번째 레이어는 이 박스의 중심을 새로운 참조점으로 삼고, 박스의 크기에 비례하여 샘플링 범위를 조절한다. 그리고 이전 박스로부터의 상대적인 오프셋(<span class="math math-inline">\Delta b_q^d</span>)을 예측하여 최종 박스 <span class="math math-inline">\hat{b}*{q}^{(d)}</span>를 업데이트한다.7 이 방식은 모델이 거친 수준에서 세밀한 수준으로(coarse-to-fine) 예측을 정제하도록 유도하여 최적화 과정을 안정시키고 최종 정확도를 높이는 효과를 가져온다. 실제로 이 기법을 적용했을 때, COCO 데이터셋에서 AP가 43.8에서 45.4로 1.6%p 상승하는 효과를 보였다.7</p>
<h3>6.2  2단계 Deformable DETR (Two-Stage Deformable DETR)</h3>
<p>이 방식은 전통적인 2단계 탐지기인 Faster R-CNN의 아이디어에서 영감을 얻었다. 전체 탐지 과정을 두 단계로 나누어 효율성과 정확도를 모두 높이고자 했다.7</p>
<ul>
<li><strong>1단계 (Region Proposal Generation)</strong>: 인코더만으로 구성된 경량화된 Deformable DETR을 사용하여 높은 점수를 받은 소수의 객체 제안(region proposals)을 생성한다. 이 단계에서는 각 픽셀이 객체 쿼리 역할을 하여 대략적인 바운딩 박스를 예측하고, 그중 가장 신뢰도 높은 상위 <code>K</code>개의 박스를 다음 단계로 전달한다.12</li>
<li><strong>2단계 (Proposal Refinement)</strong>: 1단계에서 생성된 객체 제안들을 디코더의 객체 쿼리로 사용한다. 즉, 학습 가능한 고정된 쿼리 임베딩 대신, 실제 이미지 내용에 기반한 구체적인 위치 정보를 가진 제안들을 쿼리로 활용하여 최종 예측을 수행한다.7</li>
</ul>
<p>이 접근법은 디코더가 탐색해야 할 공간을 유망한 영역으로 미리 좁혀주기 때문에, 학습의 어려움을 줄이고 수렴을 가속화하는 효과가 있다. 또한, 더 정확한 초기 위치 정보를 바탕으로 예측을 시작하므로 최종 탐지 성능, 특히 작은 객체에 대한 성능을 향상시키는 데 기여했다. 2단계 방식을 적용한 모델은 46.2 AP를 기록하며 Deformable DETR의 기본 모델보다 높은 성능을 달성했다.7</p>
<h3>6.3  후속 연구에 미친 영향: DAB-DETR과 DINO로의 발전</h3>
<p>Deformable DETR의 가장 큰 의의는 Transformer 기반 객체 탐지의 실용성을 입증하고, 후속 연구들이 딛고 올라설 수 있는 견고한 기술적 토대를 마련했다는 점에 있다. Deformable Attention은 이후 등장한 거의 모든 SOTA(State-of-the-Art) DETR 계열 모델의 핵심 구성 요소(fundamental component)로 자리 잡았다.11</p>
<p>Deformable DETR의 성공 이후, DETR 계열 모델의 발전은 크게 두 가지 방향으로 나아갔다. 첫 번째는 Deformable DETR이 해결한 ‘효율적인 특징 추출’ 문제이고, 두 번째는 여전히 과제로 남아있던 ‘안정적인 매칭 및 쿼리 설계’ 문제였다.</p>
<ul>
<li><strong>DAB-DETR (Dynamic Anchor Boxes are Better Queries for DETR)</strong>: 이 연구는 ‘쿼리 설계’ 문제에 집중했다. 기존의 객체 쿼리가 추상적인 학습 가능 임베딩이었던 것과 달리, DAB-DETR은 바운딩 박스의 4차원 좌표(중심점 x, y, 너비 w, 높이 h)를 직접 쿼리로 사용하는 파격적인 아이디어를 제안했다.28 이 4D 앵커 박스 쿼리는 디코더의 각 레이어를 거치면서 동적으로 업데이트된다.31 이는 쿼리에 명시적인 공간적 사전 정보를 부여하여, 모델이 어디에 집중해야 할지를 더 쉽게 학습하도록 돕고, 이분 매칭의 불안정성을 완화하여 수렴 속도를 더욱 가속화했다.29</li>
<li><strong>DINO (DETR with Improved DeNoising Anchor Boxes)</strong>: DINO는 DAB-DETR의 동적 앵커 박스 개념을 계승하고, Deformable DETR의 효율적인 어텐션을 기반으로 하여 성능을 극한으로 끌어올린 모델이다.27 DINO는 ‘안정적인 매칭’ 문제를 해결하기 위해 DN-DETR에서 제안된 디노이징(denoising) 훈련 방식을 도입하고, 이를 ’대조적 디노이징 훈련(Contrastive Denoising Training)’으로 발전시켰다.26 또한, 쿼리 초기화를 위한 ’혼합 쿼리 선택(Mixed Query Selection)’과 박스 예측을 정교화하는 ‘Look Forward Twice’ 기법 등을 추가하여, COCO 벤치마크에서 새로운 SOTA를 달성했다.32</li>
</ul>
<p>이처럼 Deformable DETR은 ‘입력 특징 처리’ 단계를 혁신하여 안정적인 기반을 마련했고, DAB-DETR과 DINO와 같은 후속 연구들은 이 기반 위에서 ‘쿼리-예측’ 단계를 혁신하며 DETR 계열 모델의 발전을 이끌었다. 이는 Deformable DETR이 단일 모델의 성공을 넘어, 하나의 연구 패러다임을 구축했음을 보여준다.</p>
<h2>7.  결론: Deformable DETR의 기여와 향후 과제</h2>
<h3>7.1  객체 탐지 분야에 대한 핵심 기여 요약</h3>
<p>Deformable DETR은 Transformer 기반 객체 탐지 모델의 역사에서 중요한 전환점을 마련한 기념비적인 모델이다. 이 모델의 기여는 크게 세 가지로 요약할 수 있다.</p>
<p>첫째, <strong>Transformer 기반 탐지기의 실용성을 입증했다.</strong> 원본 DETR이 End-to-End 객체 탐지라는 혁신적인 가능성을 제시했지만, 극도로 느린 수렴 속도와 엄청난 연산량이라는 현실적인 장벽에 부딪혔다. Deformable DETR은 Deformable Attention이라는 독창적인 메커니즘을 통해 이 두 가지 치명적인 단점을 동시에 해결함으로써, DETR 계열 모델이 학문적 아이디어를 넘어 실제 문제 해결에 사용될 수 있는 실용적인 도구로 자리 잡는 결정적인 계기를 마련했다.10</p>
<p>둘째, <strong>작은 객체 탐지 성능의 한계를 극복했다.</strong> 고해상도 특징 맵을 효율적으로 처리할 수 있게 됨에 따라, Deformable DETR은 DETR의 가장 큰 약점 중 하나였던 작은 객체 탐지 성능을 획기적으로 개선했다.7 이는 모델의 적용 범위를 크게 확장시켰으며, 특히 원격 탐사(remote sensing)나 자율 주행과 같이 작은 객체 탐지가 중요한 분야에서 Transformer 기반 모델의 활용 가능성을 열었다.38</p>
<p>셋째, <strong>후속 연구의 기술적 토대를 제공했다.</strong> Deformable Attention은 그 효율성과 유연성을 인정받아, 이후 등장하는 거의 모든 고성능 DETR 변종 모델(DAB-DETR, DINO, Co-DETR 등)의 표준 구성 요소로 채택되었다.11 이는 Deformable DETR이 단일 모델의 성공에 그치지 않고, 객체 탐지 분야의 기술 발전을 이끄는 핵심적인 플랫폼 역할을 수행했음을 의미한다.</p>
<h3>7.2  모델의 한계점 및 후속 연구 방향</h3>
<p>Deformable DETR은 수많은 기여에도 불구하고 몇 가지 한계점을 내포하고 있으며, 이는 후속 연구의 중요한 동기가 되었다.</p>
<p>첫째, <strong>CNN 백본에 대한 의존성이다.</strong> Deformable DETR은 특징 추출을 위해 여전히 ResNet과 같은 CNN 백본에 의존한다. 이는 CNN이 가진 고유의 한계, 예를 들어 제한된 수용장(receptive field)으로 인한 글로벌 문맥 이해의 부족이나, 고정된 필터 구조로 인한 유연성 부족 등에서 완전히 자유롭지 못함을 의미한다.37 이 문제는 이후 Vision Transformer(ViT)를 백본으로 사용하는 연구들로 이어지며, 완전한 Transformer 기반의 객체 탐지기 개발을 촉진했다.</p>
<p>둘째, <strong>희소 어텐션의 잠재적 트레이드오프이다.</strong> Deformable Attention은 효율성을 위해 전체 픽셀 중 일부만을 샘플링한다. 이는 대부분의 경우 효과적이지만, 이론적으로는 글로벌 어텐션이 포착할 수 있는 예기치 않은 장거리 문맥 정보를 놓칠 가능성을 내포한다. 특히 매우 크거나 비정형적인 객체를 탐지할 때, 제한된 수의 샘플링 포인트가 객체의 전체적인 정보를 충분히 포착하지 못할 수 있다는 우려가 제기되었다.13</p>
<p>셋째, <strong>특정 도메인에서의 적용성 문제이다.</strong> Deformable DETR의 복잡한 다중 스케일 처리 및 어텐션 메커니즘은 일반적인 자연 이미지(natural images)의 특성에 맞춰 최적화되어 있다. 그러나 의료 영상과 같이 이미지의 특성이 매우 다른 특정 도메인에서는 이러한 복잡한 설계가 오히려 성능을 저해할 수 있다는 연구 결과도 존재한다.40 이는 특정 응용 분야에 맞는 맞춤형 아키텍처 설계의 필요성을 시사한다.</p>
<p>이러한 한계점들은 Deformable DETR이 모든 문제를 해결한 최종적인 모델이 아님을 보여준다. 오히려, 이 모델은 Transformer 기반 객체 탐지라는 새로운 연구 분야의 문을 활짝 열었고, 그가 남긴 과제들은 오늘날까지도 더 효율적이고, 더 정확하며, 더 범용적인 객체 탐지 모델을 향한 연구를 이끄는 원동력이 되고 있다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>DETR: End-to-End Object Detection with Transformers | by DZ - Medium, https://dzdata.medium.com/detr-end-to-end-object-detection-with-transformers-f40ce77bfe44</li>
<li>Research of Improved DETR Models and Transformer Applications in Computer Vision - Atlantis Press, https://www.atlantis-press.com/article/126004120.pdf</li>
<li>A Review of DEtection TRansformer: From Basic Architecture to Advanced Developments and Visual Perception Applications - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC12252279/</li>
<li>Hybrid Proposal Refiner: Revisiting DETR Series from the Faster R-CNN Perspective - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_Hybrid_Proposal_Refiner_Revisiting_DETR_Series_from_the_Faster_R-CNN_CVPR_2024_paper.pdf</li>
<li>Deformable DETR: Deformable Transformers for End-to-End Object Detection | Request PDF - ResearchGate, https://www.researchgate.net/publication/344551949_Deformable_DETR_Deformable_Transformers_for_End-to-End_Object_Detection</li>
<li>Object Detection with Transformers: A Review - arXiv, https://arxiv.org/html/2306.04670</li>
<li>Deformable DETR: Deformable Transformers for End-to-End Object …, https://arxiv.org/pdf/2010.04159</li>
<li>Introduction to DETR (Detection Transformers): Everything You Need to Know - Lightly AI, https://www.lightly.ai/blog/detr</li>
<li>Accelerating DETR Convergence via Semantic-Aligned Matching - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Accelerating_DETR_Convergence_via_Semantic-Aligned_Matching_CVPR_2022_paper.pdf</li>
<li>Deformable DETR: Deformable Transformers for End-to-End Object Detection | OpenReview, https://openreview.net/forum?id=gZ9hCDWe6ke</li>
<li>Paper Review: Deformable Transformers for End-to-End Object Detection., https://cenk-bircanoglu.medium.com/paper-review-deformable-transformers-for-end-to-end-object-detection-ed0a452f775f</li>
<li>Review — Deformable Transformers for End-to-End Object Detection | by Sik-Ho Tsang, https://sh-tsang.medium.com/review-deformable-transformers-for-end-to-end-object-detection-e29786ef2b4c</li>
<li>AugDETR: Improving Multi-scale Learning for Detection Transformer - European Computer Vision Association, https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03484.pdf</li>
<li>Deformable attention — tackling DETRs runtime complexity | by Tilo Flasche - Medium, https://medium.com/@tnodecode/multi-scale-deformable-attention-tackling-detrs-runtime-complexity-and-problems-with-small-9c35bd969f48</li>
<li>Introducing Deformable Attention Transformer | by Joe El Khoury - GenAI Engineer | Medium, https://medium.com/@jelkhoury880/introducing-deformable-attention-transformer-ddb8b5363c5c</li>
<li>Deformable DETR - YouTube, https://www.youtube.com/watch?v=9UG4amweIjk</li>
<li>Deformable DETR Explained - Papers With Code, https://paperswithcode.com/method/deformable-detr</li>
<li>Deformable DETR: Deformable Transformers for End-to-End Object Detection - arXiv, https://arxiv.org/abs/2010.04159</li>
<li>ICLR 2021 Deformable DETR: Deformable Transformers for End-to-End Object Detection Oral - ICLR 2026, https://iclr.cc/virtual/2021/oral/3448</li>
<li>Deformable DETR: Deformable Transformers for End-to-End Object Detection. - GitHub, https://github.com/fundamentalvision/Deformable-DETR</li>
<li>Deformable Attention - Lei Mao’s Log Book, https://leimao.github.io/blog/Deformable-Attention/</li>
<li>Vision Transformer With Deformable Attention - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Xia_Vision_Transformer_With_Deformable_Attention_CVPR_2022_paper.pdf</li>
<li>[20.10] Deformable DETR - DOCSAID, https://docsaid.org/en/papers/object-detection/deformable-detr/</li>
<li>Attention Deficit is Ordered! Fooling Deformable Vision Transformers with Collaborative Adversarial Patches - arXiv, https://arxiv.org/html/2311.12914v2</li>
<li>Evaluating Transformer Based Object Detection Architecture - Medium, https://medium.com/@satya15july_11937/object-detection-with-transformer-based-architecture-cbcb6b9fa5fe</li>
<li>DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection, https://www.researchgate.net/publication/359079872_DINO_DETR_with_Improved_DeNoising_Anchor_Boxes_for_End-to-End_Object_Detection</li>
<li>DINO: DETR WITH IMPROVED DENOISING ANCHOR - OpenReview, https://openreview.net/references/pdf?id=7g0ZrHj5fO</li>
<li>[ICLR 2022] Official implementation of the paper “DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR” - GitHub, https://github.com/IDEA-Research/DAB-DETR</li>
<li>DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR | Request PDF - ResearchGate, https://www.researchgate.net/publication/358232679_DAB-DETR_Dynamic_Anchor_Boxes_are_Better_Queries_for_DETR</li>
<li>DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR, https://arxiv.org/pdf/2201.12329</li>
<li>DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR | OpenReview, https://openreview.net/forum?id=oMI9PjOb9Jl</li>
<li>DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection, https://arxiv.org/abs/2203.03605</li>
<li>IDEA-Research/DINO: [ICLR 2023] Official implementation of the paper “DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection” - GitHub, https://github.com/IDEA-Research/DINO</li>
<li>Dino: Detr With Improved Denoising Anchor Boxes For End-To-End Object Detection | PDF | Data Compression - Scribd, https://www.scribd.com/document/564176852/55</li>
<li>DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection, https://medium.com/@gagatsis94/dino-detr-with-improved-denoising-anchor-boxes-for-end-to-end-object-detection-4f32b9389baa</li>
<li>DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection, https://openreview.net/forum?id=3mRwyG5one</li>
<li>Context-Aware Enhanced Feature Refinement for small object detection with Deformable DETR - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC12185399/</li>
<li>[2505.24489] Deformable Attention Mechanisms Applied to Object Detection, case of Remote Sensing - arXiv, https://arxiv.org/abs/2505.24489</li>
<li>Context-Aware Enhanced Feature Refinement for small object detection with Deformable DETR - PubMed, https://pubmed.ncbi.nlm.nih.gov/40557308/</li>
<li>[2405.17677] Understanding differences in applying DETR to natural and medical images, https://arxiv.org/abs/2405.17677</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>