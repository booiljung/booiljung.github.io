<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Real-Time Detection Transformer (RT-DETR, 2023-04)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Real-Time Detection Transformer (RT-DETR, 2023-04)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>Real-Time Detection Transformer (RT-DETR, 2023-04)</span></nav>
                </div>
            </header>
            <article>
                <h1>Real-Time Detection Transformer (RT-DETR, 2023-04)</h1>
<h2>1.  실시간 객체 탐지의 패러다임 전환</h2>
<h3>1.1 실시간 객체 탐지의 현황과 YOLO의 헤게모니</h3>
<p>실시간 객체 탐지(Real-time object detection)는 자율 주행, 지능형 영상 감시, 실시간 객체 추적 등 현대 컴퓨터 비전의 수많은 응용 분야에서 핵심적인 기술로 자리 잡았다.1 이러한 응용 분야들은 높은 정확도와 더불어 초당 수십 프레임을 처리할 수 있는 빠른 추론 속도를 동시에 요구한다. 지난 수년간, YOLO(You Only Look Once) 계열의 모델들은 속도와 정확도 사이의 탁월한 균형점을 제시하며 실시간 탐지 분야의 사실상 표준(de facto standard)으로 군림해왔다.2 YOLO는 단일 단계(one-stage) 검출기로서의 효율성을 바탕으로 지속적인 아키텍처 개선을 통해 발전을 거듭했다.</p>
<p>하지만 YOLO를 포함한 대다수의 고성능 실시간 검출기들은 본질적인 한계를 내포하고 있었다. 바로 Non-Maximum Suppression (NMS)이라는 후처리 과정에 대한 의존성이다.2 NMS는 모델이 생성한 수많은 중복된 경계 상자(bounding box)들 중에서 가장 신뢰도 높은 상자만을 남기고 나머지를 제거하는 필수적인 단계이다. 그러나 이 과정은 추론 파이프라인에 병목 현상을 일으켜 속도를 저하시킬 뿐만 아니라, IoU 임계값(threshold)과 같은 수동으로 조정해야 하는 하이퍼파라미터를 도입하여 모델의 성능을 불안정하게 만드는 요인이 되었다.2</p>
<h3>1.2 DETR (DEtection TRansformer)의 등장과 한계</h3>
<p>이러한 배경 속에서 2020년 Facebook AI에 의해 제안된 DETR(DEtection TRansformer)은 객체 탐지 분야에 새로운 가능성을 제시했다.5 DETR은 자연어 처리 분야에서 성공을 거둔 트랜스포머(Transformer) 아키텍처를 비전 분야에 도입하여, 객체 탐지를 ‘직접적인 집합 예측(direct set prediction)’ 문제로 재정의했다.3 이 혁신적인 접근법은 고정된 수의 객체 쿼리(object query)를 사용하여 이미지 전체의 문맥을 이해하고, 최종적으로 중복 없는 예측 결과물을 생성한다. 그 결과, NMS와 같은 복잡한 후처리 과정이 전혀 필요 없는 최초의 완전한 종단간(End-to-End) 객체 탐지 파이프라인을 구축할 수 있었다.1</p>
<p>그러나 DETR의 등장은 실시간 탐지 분야의 즉각적인 해답이 되지는 못했다. 초기 DETR 모델들은 트랜스포머 인코더-디코더 구조가 가지는 막대한 연산량과 느린 학습 수렴 속도라는 두 가지 큰 장벽에 부딪혔다.3 특히, 이미지의 모든 픽셀 쌍 간의 관계를 계산하는 셀프 어텐션(self-attention) 메커니즘은 실시간 요구사항을 충족시키기에는 계산 비용이 너무 높았다. 결국 NMS를 제거하여 얻을 수 있는 속도적 이점은 높은 계산 비용이라는 더 큰 문제에 의해 상쇄되었고, 이로 인해 실용적인 실시간 응용에는 부적합하다는 평가를 받았다.2</p>
<h3>1.3 RT-DETR의 탄생 배경 및 핵심 목표</h3>
<p>이러한 상황은 컴퓨터 비전 연구자들에게 중요한 질문을 던졌다: “DETR의 NMS-free라는 구조적 우아함을 유지하면서, 실시간 시나리오에 적용할 수 있을 만큼 효율적인 모델을 만들 수 있는가? 나아가, 속도와 정확도 모두에서 NMS에 의존하는 YOLO를 능가할 수 있는가?”.1 이 질문에 대한 해답으로 바이두(Baidu) 연구진은 Real-Time Detection Transformer, 즉 RT-DETR을 제안했다. RT-DETR은 높은 정확도를 유지하면서 실시간 성능을 제공하는 것을 목표로 설계된 최초의 종단간 객체 탐지기이다.1</p>
<p>RT-DETR의 개발 과정은 체계적인 2단계 접근법을 따랐다. 첫 번째 단계는 기존 DETR의 정확도를 최대한 유지하면서 추론 속도를 극적으로 개선하는 데 초점을 맞췄고, 두 번째 단계는 이렇게 확보된 속도를 저하시키지 않으면서 다시 정확도를 끌어올리는 방향으로 진행되었다.1 이러한 개발 철학은 단순히 기존 모델을 경량화하는 것을 넘어, 실시간 탐지를 위한 근본적인 아키텍처 재설계를 추구했음을 보여준다. 이는 객체 탐지 분야의 연구 방향이 기존의 CNN 아키텍처를 미세 조정하고 NMS와 같은 후처리 기법을 최적화하는 것에서 벗어나, NMS 자체를 필요 없게 만드는 구조적 혁신을 통해 파이프라인의 근본적인 병목 현상을 해결하려는 패러다임의 전환을 시사한다.</p>
<h3>1.4 핵심 기여 (Contributions) 요약</h3>
<p>RT-DETR이 객체 탐지 분야에 기여한 핵심적인 내용은 다음과 같이 요약할 수 있다.</p>
<ol>
<li><strong>최초의 실시간 종단간 탐지기:</strong> 속도와 정확도라는 두 가지 척도 모두에서 동시대의 YOLO 모델들을 능가하는 성능을 달성하며, 실시간 종단간 객체 탐지기의 새로운 기준을 제시했다.1</li>
<li><strong>효율적 하이브리드 인코더:</strong> 기존 DETR의 가장 큰 병목이었던 트랜스포머 인코더를 재설계했다. 다중 스케일 특징(multi-scale features)을 효율적으로 처리하기 위해, 스케일 내(intra-scale) 특징 상호작용과 스케일 간(cross-scale) 특징 융합을 분리(decouple)하는 독창적인 하이브리드 구조를 도입하여 연산량을 획기적으로 줄였다.1</li>
<li><strong>향상된 쿼리 선택 메커니즘:</strong> 디코더의 초기 객체 쿼리 품질을 높이기 위해 IoU-aware 또는 Uncertainty-minimal 쿼리 선택 기법을 제안했다. 이를 통해 모델이 잠재적으로 객체가 존재할 가능성이 높은 영역의 특징에 집중하도록 유도하여 최종 탐지 정확도를 향상시켰다.2</li>
<li><strong>유연한 추론 속도 조절:</strong> 모델을 재학습할 필요 없이, 추론 시에 사용하는 디코더 레이어의 수를 조절하는 것만으로 속도와 정확도의 트레이드오프(trade-off)를 유연하게 조정할 수 있는 기능을 제공한다.2 이 특징은 MLOps 관점에서 매우 중요한 실용적 가치를 지닌다. 기존에는 저사양 엣지 디바이스용 모델과 고성능 서버용 모델을 각각 별도로 학습하고 관리해야 했지만, RT-DETR은 단일 학습된 모델 아티팩트(artifact)만으로 다양한 하드웨어 및 배포 요구사항에 동적으로 대응할 수 있는 ‘단일 모델, 다중 배포 타겟(Single Model, Multiple Deployment Targets)’ 전략을 가능하게 한다. 이는 모델 관리의 복잡성을 크게 줄이고 운영 효율성을 극대화하는 혁신적인 기능이다.</li>
</ol>
<h2>2.  RT-DETR 아키텍처 해부</h2>
<h3>2.1 전체 구조 개요</h3>
<p>RT-DETR의 아키텍처는 실시간 종단간 객체 탐지라는 명확한 목표를 달성하기 위해 정교하게 설계되었다. 전체적인 데이터 흐름은 다음과 같은 단계로 이루어진다. 먼저, 입력 이미지는 표준 합성곱 신경망(CNN) 기반의 백본 네트워크를 통과하여 여러 해상도를 가진 다중 스케일 특징 맵(feature map)을 추출한다. 이 중에서도 비교적 고수준의 의미 정보를 담고 있는 마지막 세 개의 스테이지, 예를 들어 {S3, S4, S5}가 다음 단계인 효율적 하이브리드 인코더의 입력으로 사용된다. 하이브리드 인코더는 이 다중 스케일 특징들을 효율적으로 처리하여 하나의 평탄화된 이미지 특징 시퀀스(sequence)로 변환한다. 이후, IoU-aware 쿼리 선택 모듈이 이 시퀀스에서 가장 유망한 K개의 특징을 선별하여 디코더를 위한 초기 객체 쿼리로 사용한다. 마지막으로, 트랜스포머 디코더는 이 초기 쿼리들을 입력받아 여러 레이어를 거치며 반복적으로 정제하고, 각 디코더 레이어는 보조 예측 헤드(auxiliary prediction heads)를 통해 중간 예측값을 출력한다. 최종 디코더 레이어의 출력이 모델의 최종 예측 결과, 즉 객체의 경계 상자와 클래스 신뢰도 점수가 된다.8</p>
<h3>2.2 백본 네트워크 (Backbone Network)</h3>
<p>RT-DETR은 백본 네트워크로 특정 구조에 국한되지 않고, ResNet이나 HGNetv2와 같은 검증된 CNN 아키텍처를 유연하게 채택할 수 있다.10 백본의 주된 역할은 입력 이미지로부터 다양한 수준의 시각적 특징을 계층적으로 추출하는 것이다. 저수준 특징은 엣지나 질감과 같은 세부 정보를 담고 있으며, 고수준 특징은 객체의 형태나 부분과 같은 더 복잡하고 추상적인 정보를 포함한다. RT-DETR은 특히 백본의 마지막 세 스테이지(예: ResNet의 C3, C4, C5)에서 생성된 다중 스케일 특징 맵 {S3, S4, S5}를 인코더의 입력으로 활용한다.8 이 특징 맵들은 각각 다른 해상도와 채널 수를 가지며, 다양한 크기의 객체를 효과적으로 탐지하는 데 필수적인 정보를 제공한다. 예를 들어, 저해상도 특징 맵(S5)은 큰 객체를 탐지하고 전역적인 문맥을 파악하는 데 유리하며, 고해상도 특징 맵(S3)은 작은 객체를 탐지하고 정확한 위치를 특정하는 데 중요하다.</p>
<h3>2.3 핵심 혁신: 효율적 하이브리드 인코더</h3>
<p>RT-DETR의 가장 핵심적인 혁신은 바로 효율적 하이브리드 인코더(Efficient Hybrid Encoder)에 있다. 기존 DETR 모델의 가장 큰 연산 병목은 트랜스포머 인코더에서 발생했는데, 이는 다중 스케일 특징 맵을 단순히 연결하여 매우 긴 시퀀스를 만든 뒤, 모든 토큰(픽셀) 간의 셀프 어텐션을 계산하기 때문이었다.1</p>
<p>RT-DETR은 이 문제를 해결하기 위해 ’전역적 어텐션의 선택적 적용’이라는 중요한 설계 원칙을 도입했다. 모든 특징에 대해 비싼 전역 어텐션을 무차별적으로 적용하는 대신, 연산의 범위를 제한하고 저렴한 연산으로 대체하는 하이브리드 접근법을 취했다. 이는 ’모든 픽셀이 다른 모든 픽셀과 상호작용할 필요는 없다’는 통찰에 기반한다. 이 인코더의 설계 철학은 연산 효율성을 극대화하기 위해 두 가지 핵심적인 연산을 분리(decouple)하는 것이다: 스케일 내(intra-scale) 특징 상호작용과 스케일 간(cross-scale) 특징 융합이다.4</p>
<h4>2.3.1 AIFI (Attention-based Intra-scale Feature Interaction)</h4>
<p>AIFI 모듈은 스케일 내 특징 상호작용을 담당한다. 즉, {S3, S4, S5} 각각의 특징 맵에 대해 독립적으로 연산을 수행한다. 각 스케일의 특징 맵 내에서만 셀프 어텐션을 적용하여, 동일한 해상도 내에서 공간적인 관계와 문맥 정보를 정제한다. 예를 들어, S5 특징 맵 내의 픽셀들은 S5 내의 다른 픽셀들과만 상호작용하고, S3나 S4의 픽셀과는 상호작용하지 않는다. 이렇게 연산 범위를 각 스케일로 국소화함으로써, 전체 특징을 하나의 긴 시퀀스로 처리할 때 발생하는 이차적인 계산 복잡도(<span class="math math-inline">O(N^2)</span>)를 크게 줄일 수 있다. 이 단계는 트랜스포머의 장점인 문맥 이해 능력은 유지하되, 가장 비용이 많이 드는 부분을 제한적으로 사용하여 실시간성을 확보하는 핵심적인 역할을 한다.</p>
<h4>2.3.2 CCFM (Cross-scale Feature-Fusion Module)</h4>
<p>AIFI를 통해 각 스케일별로 정제된 특징 맵들은 CCFM 모듈로 전달되어 하나로 융합된다. CCFM은 이름에서 알 수 있듯이 스케일 간 특징 융합을 담당한다. 기존 DETR이 어텐션을 통해 이 융합을 수행했던 것과 달리, RT-DETR의 CCFM은 훨씬 가볍고 효율적인 컨볼루션 기반의 연산을 사용한다. 이 모듈은 저해상도 특징 맵(S5, 풍부한 문맥 정보)을 점진적으로 업샘플링(upsampling)하면서 고해상도 특징 맵(S4, S3, 풍부한 세부 정보)과 결합하는 방식으로 작동한다. 이를 통해 각기 다른 스케일이 가진 장점들을 효과적으로 통합하여, 다양한 크기의 객체를 탐지하는 데 필요한 풍부한 다중 스케일 표현을 생성한다. 이처럼 어텐션과 컨볼루션의 장점을 결합한 하이브리드 방식은 RT-DETR이 성능과 효율의 최적점을 찾는 데 결정적인 기여를 했다.</p>
<h3>2.4 IoU-aware / Uncertainty-minimal 쿼리 선택</h3>
<p>하이브리드 인코더를 통과한 특징 시퀀스는 디코더에 전달될 초기 객체 쿼리를 선택하는 데 사용된다. 디코더에 입력되는 초기 쿼리의 품질은 모델의 최종 탐지 성능과 수렴 속도에 지대한 영향을 미친다. 만약 객체가 없을 가능성이 높거나 위치 정확도가 낮은 특징이 쿼리로 선택된다면, 디코더는 불필요한 연산을 수행하게 되고 최종 성능도 저하될 수 있다.2</p>
<p>이 문제를 해결하기 위해 RT-DETR은 향상된 쿼리 선택 메커니즘을 도입했다. 초기 버전에서는 IoU-aware query selection으로, CVPR 2024 논문에서는 이를 더욱 발전시킨 ’Uncertainty-minimal Query Selection’으로 명명했다.2 이 메커니즘은 객체 탐지에서 ’무엇(what, classification)’과 ’어디(where, localization)’의 불일치 문제를 직접적으로 모델링하려는 시도이다. 기존 방식들이 단순히 분류 점수가 높은 특징을 선택하는 데 그쳤다면, 이 방식은 분류와 위치 예측이 모두 일관성 있게 높은 신뢰도를 보이는 특징을 찾음으로써 디코더의 부담을 덜어주고 학습을 안정화시킨다.</p>
<p>작동 원리는 다음과 같다. 먼저, 분류 예측 분포(<span class="math math-inline">C(\hat{X})</span>)와 위치(경계 상자) 예측 분포(<span class="math math-inline">P(\hat{X})</span>) 사이의 불일치(discrepancy)를 ’인식론적 불확실성(epistemic uncertainty)’으로 정의한다. 이 불확실성은 두 분포 벡터 간의 거리로 측정될 수 있다.<br />
<span class="math math-display">
U(\hat{X}) = \|P(\hat{X}) - C(\hat{X})\|
</span><br />
여기서 <span class="math math-inline">\hat{X}</span>는 인코더 특징을 나타낸다. 이 불확실성 <span class="math math-inline">U(\hat{X})</span>를 최소화하기 위해, 이를 손실 함수에 직접 통합하여 경사 하강법 기반으로 최적화한다. 구체적으로는 분류 손실(<span class="math math-inline">L_{cls}</span>) 계산 시 이 불확실성 항을 함께 고려한다.2<br />
<span class="math math-display">
L(Xˆ, Yˆ, Y) = L_{box}(\hat{b}, b) + L_{cls}(U(Xˆ), \hat{c}, c)
</span><br />
이러한 학습 과정을 통해 모델은 분류 신뢰도와 위치 정확도가 모두 높은, 즉 불확실성이 낮은 특징을 생성하도록 유도된다. 추론 시에는 이렇게 계산된 신뢰도를 바탕으로 상위 K개의 가장 확실한 특징을 선택하여 디코더의 초기 객체 쿼리로 전달한다.8 결과적으로, 디코더는 처음부터 ’좋은 후보’에 집중하게 되어 수렴이 빨라지고, 더 정확한 최종 탐지 성능을 달성할 수 있다.</p>
<h3>2.5 유연한 트랜스포머 디코더</h3>
<p>RT-DETR의 디코더는 표준 트랜스포머 디코더 구조를 따른다. 이 디코더는 쿼리 선택 모듈로부터 전달받은 K개의 객체 쿼리들을 입력받아, 셀프 어텐션과 크로스 어텐션(cross-attention)을 통해 이를 반복적으로 정제한다. 셀프 어텐션은 쿼리들 간의 관계를 모델링하여 중복 예측을 억제하는 역할을 하고, 크로스 어텐션은 각 쿼리가 인코더로부터 나온 이미지 특징 시퀀스의 어떤 부분에 집중해야 할지를 학습한다. 이 과정을 여러 디코더 레이어에 걸쳐 반복하면서 쿼리는 점차 특정 객체에 대한 정밀한 위치와 클래스 정보로 수렴하게 된다.5</p>
<p>RT-DETR 디코더의 가장 큰 특징은 바로 <strong>유연성</strong>이다. 모델은 여러 개의 디코더 레이어(예: 6개)로 구성되지만, 추론 시에는 이 레이어들 중 일부만 선택적으로 사용할 수 있다.2 예를 들어, 최대의 정확도가 필요할 때는 6개의 레이어를 모두 사용하고, 더 빠른 추론 속도가 필요할 때는 1개 또는 3개의 레이어만 사용할 수 있다. 이 조절은 모델을 다시 학습시킬 필요 없이 추론 시점에 간단한 설정 변경만으로 가능하기 때문에, 하나의 학습된 모델로 다양한 하드웨어 환경과 실시간 요구사항에 대응할 수 있는 강력한 장점을 제공한다.8</p>
<h2>3.  학습 방법론: 손실 함수와 최적화</h2>
<h3>3.1 DETR의 손실 함수 철학 계승</h3>
<p>RT-DETR의 학습 방법론은 근본적으로 원본 DETR의 철학을 계승한다. 이는 객체 탐지를 ‘직접적인 집합 예측(direct set prediction)’ 문제로 간주하는 것이다.5 즉, 모델은 입력 이미지에 대해 고정된 크기 N(예: 300)개의 예측값 집합을 한 번에 출력한다. 이 집합의 각 요소는 <span class="math math-inline">(클래스, 경계 상자)</span> 쌍으로 구성된다. 이미지에 실제 객체 수가 N보다 적을 경우, 나머지 예측값들은 ‘객체 없음’(<span class="math math-inline">\emptyset</span>, no object) 클래스와 매칭되도록 학습된다. 이 접근법의 가장 큰 장점은 NMS나 앵커 생성(anchor generation)과 같이 수작업으로 설계된 복잡한 구성 요소들을 완전히 제거하고, 학습 가능한 신경망만으로 전체 파이프라인을 구성할 수 있다는 점이다.7</p>
<h3>3.2 이분 매칭 (Bipartite Matching)</h3>
<p>모델이 출력한 N개의 예측 집합과 실제 정답(ground truth) 객체 집합 사이의 손실을 계산하기 위해서는, 어떤 예측값이 어떤 정답 객체에 해당하는지를 결정하는 ‘매칭(matching)’ 과정이 선행되어야 한다. DETR 계열 모델들은 이 문제를 해결하기 위해 이분 매칭(bipartite matching) 방식을 사용한다.11</p>
<p>구체적으로, N개의 예측과 M개의 정답 객체(N &gt; M) 사이에 최적의 일대일(one-to-one) 매칭을 찾는다. 이 최적의 매칭은 헝가리안 알고리즘(Hungarian Algorithm)을 사용하여 전체 매칭 비용(matching cost)을 최소화하는 순열 <span class="math math-inline">\hat{\sigma}</span>를 찾는 방식으로 효율적으로 계산된다.3 매칭 비용 <span class="math math-inline">\mathcal{L}_{match}</span>는 각 <span class="math math-inline">(예측_i, 정답_j)</span> 쌍에 대해 정의되며, 클래스 예측의 정확도와 경계 상자의 유사도를 모두 고려하여 계산된다.<br />
<span class="math math-display">
\mathcal{L}_{match}(y_i, \hat{y}_{\sigma(i)}) = -\mathbb{1}_{\{c_i \neq \emptyset\}} \hat{p}_{\sigma(i)}(c_i) + \mathbb{1}_{\{c_i \neq \emptyset\}} \mathcal{L}_{box}(b_i, \hat{b}_{\sigma(i)})
</span><br />
여기서 <span class="math math-inline">y_i = (c_i, b_i)</span>는 정답 객체, <span class="math math-inline">\hat{y}_{\sigma(i)} = (\hat{p}_{\sigma(i)}, \hat{b}_{\sigma(i)})</span>는 순열 <span class="math math-inline">\sigma(i)</span>에 해당하는 예측값이다. <span class="math math-inline">\hat{p}_{\sigma(i)}(c_i)</span>는 예측 <span class="math math-inline">\sigma(i)</span>가 정답 클래스 <span class="math math-inline">c_i</span>일 확률을 의미하며, <span class="math math-inline">\mathcal{L}_{box}</span>는 경계 상자 손실을 나타낸다.11</p>
<h3>3.3 전체 손실 함수 분석</h3>
<p>최적의 매칭 순열 <span class="math math-inline">\hat{\sigma}</span>이 결정되고 나면, 매칭된 모든 쌍에 대해 최종적인 손실 <span class="math math-inline">\mathcal{L}_{Hungarian}</span>을 계산하여 모델의 가중치를 업데이트한다. 이 전체 손실 함수는 주로 세 가지 구성 요소의 가중 합으로 이루어진다.5<br />
<span class="math math-display">
\mathcal{L}_{Hungarian}(y, \hat{y}) = \sum_{i=1}^{N} \left[ -\log \hat{p}_{\hat{\sigma}(i)}(c_i) + \mathbb{1}_{\{c_i \neq \emptyset\}} \mathcal{L}_{box}(b_i, \hat{b}_{\hat{\sigma}(i)}) \right]
</span></p>
<h4>3.3.1 분류 손실 (Classification Loss)</h4>
<p>분류 손실은 매칭된 예측이 올바른 클래스를 예측하도록 학습시키는 역할을 한다. 일반적으로 로그 확률(log-probabilities)을 사용한 Negative Log-Likelihood 손실(Cross-Entropy 손실과 유사)이 사용된다.5 정답 클래스가 ‘객체 없음’(<span class="math math-inline">\emptyset</span>)인 경우, 클래스 불균형 문제를 완화하기 위해 손실의 가중치를 낮추는 기법이 적용된다.11</p>
<h4>3.3.2 바운딩 박스 손실 (Bounding Box Loss)</h4>
<p>경계 상자 손실 <span class="math math-inline">\mathcal{L}_{box}</span>는 예측된 상자의 위치와 크기가 실제 정답 상자와 최대한 일치하도록 학습시킨다. 이 손실은 L1 손실과 GIoU(Generalized Intersection over Union) 손실의 선형 결합으로 구성된다.5<br />
<span class="math math-display">
\mathcal{L}_{box}(b_i, \hat{b}_{\sigma(i)}) = \lambda_{iou} \mathcal{L}_{iou}(b_i, \hat{b}_{\sigma(i)}) + \lambda_{L1} \|b_i - \hat{b}_{\sigma(i)}\|_1
</span></p>
<ul>
<li><strong>L1 손실:</strong> 예측된 상자의 중심 좌표, 너비, 높이와 실제 상자의 값들 간의 절대적인 차이를 측정한다. 이는 직관적이고 간단한 회귀 손실이다.</li>
<li><strong>GIoU 손실:</strong> IoU는 두 상자가 겹치는 영역의 비율을 나타내는 척도이다. GIoU는 이를 일반화한 것으로, 두 상자가 전혀 겹치지 않는 경우에도 유의미한 그래디언트를 제공하여 학습을 더 안정적으로 만든다. 또한, 스케일에 불변(scale-invariant)하므로 다양한 크기의 객체에 대해 강건하게 작동한다.</li>
</ul>
<h3>3.4 RT-DETR의 추가적 학습 전략</h3>
<p>RT-DETR은 위에서 설명한 DETR의 기본 손실 함수 구조를 따르면서, 수렴을 가속화하고 성능을 향상시키기 위한 추가적인 학습 전략들을 채택한다.</p>
<ul>
<li><strong>Denoising Training:</strong> DN-DETR과 DINO에서 제안된 이 기법은 DETR 계열 모델의 느린 수렴 문제를 해결하는 데 매우 효과적이다.3 학습 과정에서 실제 정답 경계 상자에 약간의 노이즈(위치 이동, 크기 변화)를 추가한 뒤, 이를 디코더의 입력으로 함께 넣어준다. 그리고 모델이 이 노이즈가 섞인 쿼리로부터 원래의 깨끗한 정답 상자를 복원하도록 학습시킨다. Ultralytics 구현에 따르면 RT-DETR의 손실 함수는 이 Denoising 손실을 포함하여 계산된다.14 이 과정은 DETR의 학습에서 발생하는 ’매칭’과 ‘최적화’ 간의 ‘닭과 달걀’ 문제를 해결하는 데 중요한 역할을 한다. 학습 초기에는 모델의 예측이 부정확하여 헝가리안 매칭이 불안정하게 이루어지고, 이는 다시 부정확한 학습 신호로 이어진다. Denoising training은 ’노이즈 섞인 정답’이라는 명확한 학습 목표를 제공함으로써, 모델이 매칭 단계에 덜 의존하고도 경계 상자를 예측하는 법을 빠르게 배우도록 돕는다. 일단 모델이 기본적인 예측 능력을 갖추면, 헝가리안 매칭이 안정화되고, 이는 다시 모델의 정교한 최적화를 이끄는 선순환 구조를 형성한다.</li>
<li><strong>Uncertainty-minimal Query Selection Loss:</strong> 앞서 아키텍처 부분에서 설명한 바와 같이, 분류와 위치 예측의 불일치로 정의된 ’불확실성’을 최소화하는 항이 분류 손실에 통합된다.2 이는 모델이 더 품질 높은 초기 쿼리를 선택하도록 유도하여 전반적인 학습 효율과 최종 성능을 개선한다.</li>
</ul>
<h2>4.  성능 분석 및 벤치마크</h2>
<h3>4.1 주요 경쟁 모델과의 비교 (YOLO 계열)</h3>
<p>RT-DETR의 성능은 실시간 객체 탐지 분야의 지배적인 모델인 YOLO 계열과의 비교를 통해 가장 명확하게 평가될 수 있다. RT-DETR 논문이 발표될 당시, 동일한 스케일의 YOLO 모델들과 비교했을 때 COCO val2017 데이터셋에서 속도와 정확도(AP) 모두를 능가하는 인상적인 결과를 보여주었다.1 구체적으로, RT-DETR-R50 모델은 NVIDIA T4 GPU 환경에서 108 FPS의 추론 속도와 53.1%의 AP를 기록했으며, 더 큰 모델인 RT-DETR-R101은 74 FPS와 54.3% AP를 달성했다. 이는 당시의 YOLOv7-L/X, YOLOv8-L/X 모델들을 상회하는 성능이었다.1</p>
<p>그러나 기술 발전의 속도가 매우 빠른 이 분야에서 “RT-DETR이 YOLO를 이겼다“는 주장은 특정 시점과 조건 하에서 유효한 것이며, 보다 다각적인 분석이 필요하다. Ultralytics에서 제공하는 후속 벤치마크 자료는 최신 YOLOv8 모델과의 비교를 통해 더 복합적인 성능 양상을 보여준다.16</p>
<ul>
<li><strong>정확도(Accuracy):</strong> 가장 큰 모델인 RT-DETR-X가 YOLOv8-X에 비해 mAP에서 근소한 우위를 보인다. 하지만 전반적으로 YOLOv8 모델들은 모델 크기 대비 매우 경쟁력 있는 정확도를 제공한다.</li>
<li><strong>GPU 속도(Speed on GPU):</strong> YOLOv8이 전반적으로 더 빠른 추론 속도를 보인다. 특히, 가장 작은 모델인 YOLOv8n은 RT-DETR의 가장 작은 모델(RT-DETR-R18)보다 3배 이상 빠른 속도를 보여, 프레임률이 매우 중요한 응용 분야에 더 적합할 수 있다.</li>
<li><strong>CPU 속도(Speed on CPU):</strong> CPU 추론 환경에서는 YOLOv8이 압도적인 우위를 점한다. 이는 고도로 최적화된 CNN 기반 아키텍처가 트랜스포머 기반 아키텍처보다 CPU 환경에서 더 효율적으로 작동하기 때문이며, GPU가 없는 엣지 디바이스나 일반 서버 환경에서는 중요한 고려사항이다.</li>
<li><strong>효율성(Efficiency):</strong> YOLOv8 모델들은 파라미터 수와 연산량(FLOPs) 측면에서 RT-DETR보다 더 효율적이다. 예를 들어, YOLOv8x는 RT-DETR-x와 거의 비슷한 정확도를 더 적은 파라미터와 FLOPs로 달성한다.</li>
</ul>
<p>결론적으로, 모델 선택은 배포 환경(GPU vs CPU), 하드웨어 자원 제약(메모리, 연산량), 그리고 요구되는 속도와 정확도의 균형점에 따라 달라지는 복잡한 결정이다. RT-DETR은 ’최고 정확도를 지향하는 실시간 종단간 탐지’라는 특정 목표에서 중요한 성과를 거두었으며, YOLO는 ’범용적인 효율성과 접근성’이라는 측면에서 여전히 강력한 경쟁력을 유지하고 있다.</p>
<p>다음 표는 주요 RT-DETR 모델과 YOLOv8 모델의 성능을 요약한 것이다.</p>
<table><thead><tr><th>모델</th><th>백본</th><th>입력 크기</th><th>AP (val2017)</th><th>AP50</th><th>파라미터(M)</th><th>FLOPs(G)</th><th>T4 GPU FPS</th></tr></thead><tbody>
<tr><td><strong>RT-DETR-R18</strong></td><td>ResNet-18</td><td>640</td><td>46.5</td><td>63.8</td><td>20</td><td>60</td><td>217</td></tr>
<tr><td><strong>RT-DETR-R50</strong></td><td>ResNet-50</td><td>640</td><td>53.1</td><td>71.3</td><td>42</td><td>136</td><td>108</td></tr>
<tr><td><strong>RT-DETR-R101</strong></td><td>ResNet-101</td><td>640</td><td>54.3</td><td>72.7</td><td>76</td><td>259</td><td>74</td></tr>
<tr><td><strong>RT-DETR-L</strong></td><td>HGNetv2-L</td><td>640</td><td>53.0</td><td>71.6</td><td>32</td><td>110</td><td>114</td></tr>
<tr><td><strong>RT-DETR-X</strong></td><td>HGNetv2-X</td><td>640</td><td>54.8</td><td>73.1</td><td>67</td><td>234</td><td>74</td></tr>
<tr><td><strong>YOLOv8n</strong></td><td>-</td><td>640</td><td>37.3</td><td>53.9</td><td>3.2</td><td>8.7</td><td>485</td></tr>
<tr><td><strong>YOLOv8s</strong></td><td>-</td><td>640</td><td>44.9</td><td>62.5</td><td>11.2</td><td>28.6</td><td>370</td></tr>
<tr><td><strong>YOLOv8m</strong></td><td>-</td><td>640</td><td>50.2</td><td>67.2</td><td>25.9</td><td>78.9</td><td>256</td></tr>
<tr><td><strong>YOLOv8l</strong></td><td>-</td><td>640</td><td>52.9</td><td>69.4</td><td>43.7</td><td>165.2</td><td>185</td></tr>
<tr><td><strong>YOLOv8x</strong></td><td>-</td><td>640</td><td>53.9</td><td>70.4</td><td>68.2</td><td>257.8</td><td>125</td></tr>
</tbody></table>
<p>주: FPS는 T4 GPU, TensorRT FP16 환경에서 측정된 값이며, 구현 및 환경에 따라 달라질 수 있다. YOLOv8 데이터는 Ultralytics 문서 기준이며, RT-DETR 데이터는 공식 GitHub 저장소 기준이다.10</p>
<h3>4.2 기존 DETR 모델과의 비교</h3>
<p>RT-DETR의 효율성은 기존의 고성능 DETR 모델과 비교했을 때 더욱 두드러진다. 예를 들어, RT-DETR-R50은 당시 SOTA(State-of-the-art) 모델 중 하나였던 DINO-Deformable-DETR-R50과 비교하여, AP는 50.9%에서 53.1%로 2.2% 더 높으면서도, 추론 속도는 약 5 FPS에서 108 FPS로 무려 21배나 빠르다.1 이는 RT-DETR의 하이브리드 인코더와 최적화된 아키텍처가 DETR의 정확도는 유지하거나 능가하면서도, 실시간성을 저해하는 연산 병목을 얼마나 효과적으로 해결했는지를 극명하게 보여주는 결과이다.</p>
<h3>4.3 사전 학습의 효과</h3>
<p>대규모 데이터셋을 사용한 사전 학습(pre-training)은 RT-DETR의 성능을 한 단계 더 끌어올리는 중요한 요소이다. Objects365와 같은 대규모 데이터셋으로 모델을 먼저 학습시킨 후, 목표 데이터셋인 COCO에서 미세 조정(fine-tuning)하는 전략을 사용했을 때, 모든 모델에서 일관된 성능 향상이 관찰되었다.10 예를 들어, RT-DETR-R50은 COCO 데이터셋만으로 학습했을 때 53.1% AP를 기록했지만, Objects365로 사전 학습한 후에는 55.3% AP로 성능이 향상되었다. 마찬가지로 RT-DETR-R101은 54.3% AP에서 56.2% AP로 향상되었다.1</p>
<p>이러한 결과는 RT-DETR의 트랜스포머 기반 아키텍처가 대규모 데이터로부터 일반화된 시각적 표현(visual representation)을 학습하는 데 매우 효과적임을 시사한다. 트랜스포머는 자연어 처리 분야에서 대규모 텍스트 코퍼스를 통해 사전 학습될 때 그 잠재력이 폭발적으로 발현되었던 것과 유사하게, 비전 분야에서도 더 많은 데이터를 통해 전역적인 문맥 이해 능력을 효과적으로 강화할 수 있음을 보여준다. 따라서 RT-DETR의 진정한 잠재력은 향후 더 큰 규모의 데이터셋이나 자기지도학습(self-supervised learning)과 같은 기법과 결합될 때 더욱 발현될 수 있으며, 이는 미래 객체 탐지 모델의 중요한 발전 방향이 될 것이다.</p>
<h2>5.  RT-DETR의 진화: RT-DETRv2와 RT-DETRv3</h2>
<p>RT-DETR의 개발은 단발적인 성과에 그치지 않고, 지속적인 개선을 통해 v2와 v3로 진화했다. 이 진화 과정은 AI 모델이 성숙해가는 전형적인 단계를 보여준다. 초기 버전이 혁신적인 아키텍처를 통해 돌파구를 마련했다면, 후속 버전들은 그 아키텍처의 잠재력을 최대한 끌어내기 위한 정교한 학습 방법론 최적화에 집중한다.</p>
<h3>5.1 RT-DETRv2: Bag-of-Freebies를 통한 개선</h3>
<p>RT-DETRv2는 기존 RT-DETR의 아키텍처는 그대로 유지하면서, 추론 속도에 영향을 주지 않는 다양한 학습 기법들을 적용하여 성능을 향상시키는 데 중점을 둔다.3 이러한 기법들을 ’Bag-of-Freebies’라고 부르는데, 이는 추론 시 추가 비용 없이 ‘공짜로’ 성능을 얻는다는 의미를 담고 있다.10</p>
<p>RT-DETRv2의 주요 개선 사항 중 하나는 ’선택적 다중 스케일 샘플링(Selective Multi-Scale Sampling)’이다.17 이는 학습 과정에서 이미지 내의 큰 객체와 작은 객체를 다르게 처리하는 데이터 증강 및 샘플링 전략이다. 예를 들어, 작은 객체가 포함된 이미지를 더 자주 샘플링하거나, 이미지 패치를 생성할 때 작은 객체가 잘리지 않도록 하는 등의 기법을 통해, 모델이 특히 탐지하기 어려운 작은 객체에 대한 성능을 집중적으로 개선하도록 유도한다.</p>
<p>이러한 학습 전략 최적화를 통해 RT-DETRv2는 원본 RT-DETR 대비 유의미한 정확도 향상을 이루었다. 예를 들어, RT-DETRv2-S(ResNet-18 기반)는 기존 RT-DETR-R18에 비해 COCO 데이터셋에서 1.6% AP가 향상된 48.1% AP를 달성했으며, 다른 스케일의 모델들에서도 일관된 성능 개선이 확인되었다.3</p>
<h3>5.2 RT-DETRv3: 수렴 가속화를 위한 밀집 감독</h3>
<p>RT-DETRv3는 DETR 계열 모델들이 가진 고질적인 문제인 ’희소 감독(sparse supervision)’을 해결하기 위한 시도에서 출발한다.3 DETR의 일대일 매칭 방식은 NMS를 제거하는 우아함을 제공하지만, 각 이미지에서 실제 객체 수만큼의 예측(보통 수십 개)만이 긍정 샘플(positive sample)로 학습되고 나머지 수백 개의 예측은 부정 샘플(negative sample)로 취급된다. 이러한 극심한 불균형과 부족한 학습 신호는 모델의 수렴 속도를 늦추고 최종 성능을 제한하는 원인이 될 수 있다.3</p>
<p>RT-DETRv3는 이 문제를 해결하기 위해 ’계층적 밀집 긍정 감독(Hierarchical Dense Positive Supervision)’이라는 독창적인 하이브리드 학습 방식을 제안한다.3 이는 DETR과 YOLO 패러다임의 융합으로 볼 수 있다. 두 접근법이 상호 배타적인 것이 아니라, 서로의 단점을 보완하며 공존할 수 있음을 증명한 것이다.</p>
<ul>
<li><strong>핵심 아이디어:</strong> 학습 과정에만 보조적인 구조를 추가하여 인코더와 디코더에 더 풍부하고 밀집된 감독 신호를 제공하고, 추론 시에는 이 보조 구조를 모두 제거하여 기존 RT-DETR과 동일한 속도를 유지한다.3</li>
<li><strong>작동 방식:</strong></li>
</ul>
<ol>
<li><strong>CNN 기반 보조 분기:</strong> 하이브리드 인코더의 출력에 CNN 기반의 보조 헤드(auxiliary head)를 병렬로 연결한다. 이 보조 헤드는 PP-YOLOE와 같이 YOLO 계열에서 사용하는 일대다(one-to-many) 레이블 할당 방식, 즉 밀집 감독(dense supervision)을 사용하여 인코더를 직접 학습시킨다. 이는 인코더가 더 강력하고 풍부한 특징 표현을 학습하도록 ‘교사(teacher)’ 역할을 한다.3</li>
<li><strong>디코더 내 밀집 감독:</strong> 기존의 일대일 매칭을 사용하는 주 디코더 분기 내에서도 추가적인 감독 신호를 제공한다. 예를 들어, 디코더의 셀프 어텐션에 마스크를 적용하여 섭동(perturbation)을 줌으로써, 여러 쿼리 그룹이 동일한 정답 객체에 대해 다양한 방식으로 매칭되도록 유도한다. 또한, 공유 가중치를 사용하는 보조 디코더 분기를 추가하여 더 많은 긍정 쿼리가 학습에 기여하도록 한다.3</li>
</ol>
<p>이러한 하이브리드 학습 방식 덕분에 RT-DETRv3는 모델의 수렴 속도를 크게 가속화하고 최종 성능을 한 단계 더 끌어올렸다. 실험 결과, RT-DETRv3-R18은 RT-DETR-R18 대비 1.6% AP 향상된 48.1% AP를 달성했으며, RT-DETRv3-R101은 54.6% AP를 기록하여 YOLOv10-X와 같은 최신 모델을 능가하는 성능을 보였다.3 이는 미래의 객체 탐지기가 특정 목적에 따라 희소 감독과 밀집 감독을 결합하는 하이브리드 형태로 발전할 가능성을 시사한다.</p>
<h2>6.  실제 적용: 구현, 학습, 및 배포</h2>
<h3>6.1 공식 코드 및 생태계</h3>
<p>RT-DETR의 성공은 모델 자체의 뛰어난 성능뿐만 아니라, 개발자들이 쉽게 접근하고 활용할 수 있도록 잘 구축된 오픈소스 생태계 덕분이기도 하다. 이는 현대 AI 모델의 성공이 기술적 우수성과 더불어 개발자 접근성 및 커뮤니티 지원에 크게 의존함을 보여주는 좋은 사례이다.</p>
<ul>
<li><strong>공식 구현:</strong> RT-DETR의 최초 공식 구현은 바이두(Baidu)의 딥러닝 프레임워크인 PaddlePaddle을 기반으로 공개되었다.19</li>
<li><strong>PyTorch 지원:</strong> 이후 연구 및 개발 커뮤니티의 높은 수요에 부응하여 공식적인 PyTorch 포팅 버전이 제공되었다. 이로 인해 전 세계 대다수의 딥러닝 개발자들이 익숙한 환경에서 RT-DETR을 활용할 수 있게 되었다.10</li>
<li><strong>주요 라이브러리 통합:</strong> RT-DETR의 대중화에 결정적인 역할을 한 것은 Ultralytics, Hugging Face Transformers와 같은 주요 딥러닝 라이브러리로의 신속한 통합이다.10 특히, YOLO 시리즈로 유명한 Ultralytics 라이브러리에 RT-DETR이 포함되면서, 사용자들은 기존 YOLO 모델을 사용하던 것과 거의 동일한 간결한 코드로 RT-DETR 모델을 로드하고, 학습시키고, 추론할 수 있게 되었다.22 이러한 높은 접근성은 모델의 채택과 확산을 가속화하는 중요한 동력이 되었다.</li>
</ul>
<h3>6.2 학습 가이드</h3>
<p>RT-DETR을 실제 문제에 적용하기 위해 사용자 정의 데이터셋으로 모델을 학습하거나 미세 조정하는 것이 가능하다. 공식 저장소는 이를 위한 가이드를 제공하며, <code>remap_mscoco_category</code>와 같은 유틸리티 함수를 통해 사용자 정의 클래스를 기존 COCO 가중치와 매핑하는 과정을 용이하게 만들었다.10 하지만 RT-DETR 학습 시에는 트랜스포머 아키텍처의 특성상 몇 가지 주의할 점이 있다.</p>
<ul>
<li><strong>NaN 값 발생 문제:</strong> RT-DETR은 학습 과정이 CNN 기반 모델보다 민감하여, 학습 중에 손실(loss) 값이 비정상적으로 발산하여 NaN(Not a Number)이 되는 현상이 발생할 수 있다. 이는 특히 자동 혼합 정밀도(AMP, <code>amp=True</code>) 학습 시에 더 민감하게 나타날 수 있다.24</li>
<li><strong>해결 방안:</strong> 이러한 문제를 겪을 경우, 다음과 같은 방법들을 시도해 볼 수 있다.24</li>
<li><strong>낮은 학습률(Learning Rate):</strong> 학습률을 낮추어 모델 파라미터 업데이트의 보폭을 줄이면 학습이 안정화될 수 있다.</li>
<li><strong>사전 학습된 가중치 사용:</strong> 무작위 초기값에서 학습을 시작하는 것(<code>from scratch</code>)보다, COCO나 Objects365로 사전 학습된 가중치에서 미세 조정을 시작하는 것이 훨씬 안정적이고 빠른 수렴을 유도한다.</li>
<li><strong>데이터 정규화(Normalization):</strong> 입력 이미지 데이터가 올바르게 정규화되었는지 확인한다. 트랜스포머 모델은 입력 데이터의 스케일에 민감할 수 있다.</li>
<li><strong>데이터 증강(Augmentation):</strong> 너무 과도한 데이터 증강 기법은 오히려 학습을 불안정하게 만들 수 있으므로, 증강의 강도를 조절해 볼 필요가 있다.</li>
<li><strong>그래디언트 클리핑(Gradient Clipping):</strong> 그래디언트가 폭발적으로 증가하는 것을 방지하기 위해 일정 임계값을 넘지 않도록 잘라내는 기법을 적용한다.</li>
</ul>
<h3>6.3 추론 및 배포</h3>
<p>학습된 RT-DETR 모델은 다양한 환경에 배포하여 실제 추론에 사용할 수 있다. 특히 실시간 성능을 극대화하기 위해 여러 가속화 백엔드를 지원한다.</p>
<ul>
<li><strong>가속화 백엔드 지원:</strong> 모델은 표준 ONNX(Open Neural Network Exchange) 형식으로 내보낼 수 있으며, 이를 통해 NVIDIA의 TensorRT, Intel의 OpenVINO와 같은 다양한 하드웨어 가속 추론 엔진에서 실행할 수 있다.10</li>
<li><strong>성능 최적화:</strong> 특히 NVIDIA GPU 환경에서는 TensorRT를 사용하여 모델을 최적화했을 때 상당한 속도 향상을 기대할 수 있다. 앞서 제시된 성능표의 FPS 수치는 대부분 TensorRT FP16(16비트 부동소수점) 정밀도 환경에서 측정된 값으로, 이는 실제 배포 시 달성 가능한 최고 수준의 성능을 나타낸다.10</li>
<li><strong>배포 시 고려사항:</strong> 모델을 ONNX로 변환하고 TensorRT 엔진으로 빌드하는 과정은 때때로 복잡할 수 있다. 예를 들어, 특정 연산자(operation)가 TensorRT에서 지원되지 않거나, 가중치의 데이터 타입(INT64 vs INT32)이 호환되지 않는 문제가 발생할 수 있다. 이러한 문제를 해결하기 위해서는 PyTorch, ONNX, TensorRT 라이브러리 간의 버전 호환성을 신중하게 확인하고, ONNX 모델을 내보낼 때 불필요한 연산을 제거하는 단순화(simplification) 옵션을 사용하는 것이 도움이 될 수 있다.24</li>
</ul>
<h2>7.  결론: 강점, 한계, 그리고 미래</h2>
<h3>7.1 RT-DETR의 핵심 강점 요약</h3>
<p>RT-DETR은 실시간 객체 탐지 분야에 중요한 이정표를 세운 모델로, 그 핵심 강점은 다음과 같이 요약할 수 있다.</p>
<ul>
<li><strong>종단간 파이프라인의 간결성:</strong> NMS와 같은 복잡하고 휴리스틱한 후처리 과정을 완전히 제거하여, 학습부터 추론까지 이어지는 파이프라인을 매우 간결하고 우아하게 만들었다.</li>
<li><strong>최고 수준의 성능:</strong> 제안 당시, 속도와 정확도라는 두 가지 핵심 지표 모두에서 기존의 강력한 실시간 탐지기들의 기준을 한 단계 끌어올렸으며, 종단간 모델이 실시간 응용 분야에서 충분히 경쟁력이 있음을 입증했다.</li>
<li><strong>탁월한 유연성:</strong> 단일 학습 모델을 가지고 재학습 없이 추론 속도를 동적으로 조절할 수 있는 독보적인 기능을 제공한다. 이는 다양한 하드웨어와 지연 시간(latency) 요구사항을 가진 배포 시나리오에 매우 유연하게 대응할 수 있게 하는 실용적인 장점이다.</li>
</ul>
<h3>7.2 명확한 한계점</h3>
<p>모든 모델과 마찬가지로 RT-DETR 역시 명확한 한계점을 가지고 있다. 이는 주로 트랜스포머 기반 아키텍처의 내재적인 특성에서 기인한다.</p>
<ul>
<li><strong>작은 객체 탐지 성능:</strong> 논문의 저자들이 직접 인정한 바와 같이, 다른 DETR 계열 모델들과 마찬가지로 작은 객체(small object)에 대한 탐지 성능이 여전히 최신 CNN 기반 탐지기(예: YOLO)에 비해 다소 떨어지는 경향이 있다.2 이는 트랜스포머가 저해상도 특징 맵에서 전역적인 문맥을 파악하는 데 강점이 있는 반면, 고해상도 특징 맵의 미세한 디테일을 포착하는 데는 상대적으로 약할 수 있기 때문이다.</li>
<li><strong>계산 자원 요구사항:</strong> 트랜스포머 기반 모델은 일반적으로 동급의 CNN 기반 모델보다 더 많은 계산 자원을 필요로 한다. 특히 학습 시에는 더 많은 GPU 메모리와 시간이 소요되며, CPU 환경에서의 추론 속도는 고도로 최적화된 CNN 모델에 비해 현저히 느리다.16</li>
</ul>
<h3>7.3 향후 연구 방향</h3>
<p>RT-DETR이 제시한 가능성과 남겨진 과제들은 향후 객체 탐지 연구에 중요한 방향을 제시한다.</p>
<ul>
<li><strong>작은 객체 탐지 성능 개선:</strong> 앞서 언급된 한계점을 극복하기 위한 연구가 활발히 진행될 것이다. 고해상도 특징 맵을 더 효과적으로 활용하는 아키텍처 수정(예: RT-DETR-SEA 26)이나, 작은 객체에 특화된 데이터 증강 및 손실 함수 설계 등이 주요 연구 주제가 될 것이다.</li>
<li><strong>지식 증류 (Knowledge Distillation):</strong> 논문 저자들이 직접 유망한 방향으로 제안한 바와 같이, DINO와 같은 거대한 사전 학습 DETR 모델이 가진 풍부한 지식을 RT-DETR과 같은 경량 모델에 ’증류(distill)’하여 성능을 추가로 향상시키는 연구가 기대된다.2 이는 RT-DETR이 다른 실시간 탐지기들과 달리 거대한 상위 모델과 동일한 아키텍처 철학을 공유하기 때문에 가질 수 있는 독특한 장점이다.</li>
</ul>
<p>종합적으로, RT-DETR은 객체 탐지 분야에서 트랜스포머 아키텍처가 더 이상 학문적 호기심의 대상이 아니라, 실시간 응용이라는 가장 실용적인 영역에서도 CNN의 강력한 대안이 될 수 있음을 명확히 입증한 ‘변곡점(inflection point)’ 모델이다. RT-DETR이 실시간성의 벽을 허물면서, 연구 커뮤니티에는 ’트랜스포머를 어떻게 더 빠르고 효율적으로 만들 것인가’라는 새로운 연구의 장이 열렸다. 나아가 RT-DETRv3에서 볼 수 있듯이, 이는 기존의 CNN 기반 패러다임과의 창의적인 융합을 촉진하고 있다. 따라서 RT-DETR의 가장 큰 기여는 특정 성능 수치를 달성한 것을 넘어, 실시간 컴퓨터 비전 분야에서 아키텍처 설계의 다양성을 폭발적으로 증가시킨 촉매 역할을 했다는 점에 있다. 이는 향후 CNN과 트랜스포머의 장점을 결합한 더욱 강력하고 효율적인 하이브리드 아키텍처의 시대를 여는 중요한 이정표가 될 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>DETRs Beat YOLOs on Real-time Object Detection - arXiv, https://arxiv.org/html/2304.08069v3</li>
<li>DETRs Beat YOLOs on Real-time Object … - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_DETRs_Beat_YOLOs_on_Real-time_Object_Detection_CVPR_2024_paper.pdf</li>
<li>RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision - arXiv, https://arxiv.org/html/2409.08475v1</li>
<li>RT-DETR: The Next Evolution in Real-Time Object Detection | by Nandini Lokesh Reddy, https://medium.com/@nandinilreddy/rt-detr-the-next-evolution-in-real-time-object-detection-aa1c7880f368</li>
<li>Introduction to DETR (Detection Transformers): Everything You Need to Know - Lightly AI, https://www.lightly.ai/blog/detr</li>
<li>[2005.12872] End-to-End Object Detection with Transformers - arXiv, https://arxiv.org/abs/2005.12872</li>
<li>Align-DETR: Enhancing End-to-end Object Detection with Aligned Loss - arXiv, https://arxiv.org/html/2304.07527v2</li>
<li>Baidu’s RT-DETR: A Vision Transformer-Based Real-Time Object …, https://docs.ultralytics.com/models/rtdetr/</li>
<li>RT-DETR, https://sam99dave.github.io/supreme-goggles/Research-Paper-Overview/RT-DETR</li>
<li>lyuwenyu/RT-DETR: [CVPR 2024] Official RT-DETR … - GitHub, https://github.com/lyuwenyu/RT-DETR</li>
<li>Papers Explained 79: DETR. DEtection TRansformer or DETR… | by Ritvik Rastogi, https://ritvik19.medium.com/papers-explained-79-detr-bcdd53355d9f</li>
<li>DETR - Hugging Face, https://huggingface.co/docs/transformers/v4.33.2/model_doc/detr</li>
<li>DETR: Object Detection with Transformers (2020) - Naoki Shibuya, https://naokishibuya.github.io/blog/2022-10-30-detr-object-detection-with-transformers-2020/</li>
<li>Reference for ultralytics/models/utils/loss.py - Ultralytics YOLO Docs, https://docs.ultralytics.com/reference/models/utils/loss/</li>
<li>Detection Transformer with Stable Matching - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Detection_Transformer_with_Stable_Matching_ICCV_2023_paper.pdf</li>
<li>RTDETRv2 vs YOLOv8: A Technical Comparison - Ultralytics YOLO Docs, https://docs.ultralytics.com/compare/rtdetr-vs-yolov8/</li>
<li>RT-DETRv2 Beats YOLO? Full Comparison + Tutorial - Labellerr, https://www.labellerr.com/blog/rt-detrv2-beats-yolo-full-comparison-tutorial/</li>
<li>RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision - arXiv, https://arxiv.org/html/2409.08475v3</li>
<li>PaddlePaddle/PaddleDetection: Object Detection toolkit based on PaddlePaddle. It supports object detection, instance segmentation, multiple object tracking and real-time multi-person keypoint detection. - GitHub, https://github.com/PaddlePaddle/PaddleDetection</li>
<li>Any plan support RT-DETR? · Issue #10186 · open-mmlab/mmdetection - GitHub, https://github.com/open-mmlab/mmdetection/issues/10186</li>
<li>pranavdurai10/rtdetr-pytorch: This repository provides a PyTorch implementation of RT-DeTR, a state-of-the-art Realtime Detection Transformer for object detection tasks. - GitHub, https://github.com/pranavdurai10/rtdetr-pytorch</li>
<li>Ultralytics RT-DETR (Realtime Detection Transformer) - GitHub, https://github.com/bharath5673/RT-DETR</li>
<li>xunull/read-RT-DETR - GitHub, https://github.com/xunull/read-RT-DETR</li>
<li>RT-DETR (Realtime Detection Transformer) - Ultralytics YOLOv8 Docs #2545 - GitHub, https://github.com/orgs/ultralytics/discussions/2545</li>
<li>YOLOv7 vs RT-DETRv2: A Detailed Technical Comparison - Ultralytics Docs, https://docs.ultralytics.com/compare/yolov7-vs-rtdetr/</li>
<li>Nathancgy/RT-DETR-SEA: A Small Object Enhanced Architecture (SEA) to the RT-DETR hybrid encoder to improve small object detection performance in beach environments - GitHub, https://github.com/Nathancgy/RT-DETR-SEA</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>