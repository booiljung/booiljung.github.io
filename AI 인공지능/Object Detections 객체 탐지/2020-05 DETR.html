<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:DETR (DEtection TRansformer, 2020-05)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>DETR (DEtection TRansformer, 2020-05)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>DETR (DEtection TRansformer, 2020-05)</span></nav>
                </div>
            </header>
            <article>
                <h1>DETR (DEtection TRansformer, 2020-05)</h1>
<h2>1.  객체 탐지의 패러다임 전환</h2>
<h3>1.1  기존 객체 탐지 방법론의 현황과 한계</h3>
<p>현대 컴퓨터 비전 분야에서 객체 탐지(object detection)는 이미지나 비디오 내에서 특정 객체의 위치를 찾고 클래스를 분류하는 핵심적인 과업이다. DETR(DEtection TRansformer)의 등장 이전, 객체 탐지 분야는 크게 2단계(2-stage) 검출기와 1단계(1-stage) 검출기라는 두 가지 주류 방법론에 의해 지배되어 왔다.1</p>
<p>2단계 검출기의 대표적인 예는 Faster R-CNN 계열로, 이는 먼저 Region Proposal Network (RPN)를 통해 객체가 존재할 가능성이 높은 후보 영역(region proposals)을 생성하고, 이후 각 후보 영역에 대해 분류(classification)와 경계 상자 회귀(bounding box regression)를 수행하는 다단계 파이프라인 구조를 가진다.3 이러한 접근법은 높은 정확도를 달성했지만, 여러 단계로 구성된 복잡한 구조로 인해 전체적인 최적화가 어렵고 추론 속도가 느리다는 단점이 있었다.5</p>
<p>반면, 1단계 검출기인 YOLO, SSD 등은 RPN과 같은 별도의 후보 영역 생성 단계를 생략하고, 이미지 전체에 대해 한 번에 분류와 위치 예측을 수행하여 실시간 탐지를 가능하게 했다.2 이들은 사전 정의된 다양한 크기와 종횡비의 기준 상자인 앵커 박스(anchor boxes)나 그리드 셀(grid cells)을 기반으로 예측을 수행한다.1 그러나 이러한 접근법은 앵커 박스의 설계와 같은 수작업(hand-crafted) 요소에 크게 의존하며, 이는 하이퍼파라미터 튜닝에 민감하고 다양한 형태와 크기의 객체에 대한 일반화 성능을 저해하는 요인이 되었다.4</p>
<p>더욱이, 이 두 가지 방법론은 공통적으로 중복된 예측 결과를 제거하기 위해 비최대 억제(Non-Maximum Suppression, NMS)라는 후처리 과정에 필수적으로 의존했다.5 NMS는 동일한 객체에 대해 여러 개의 경계 상자가 예측되었을 때, 가장 신뢰도가 높은 상자 하나만을 남기고 나머지를 제거하는 휴리스틱한 알고리즘이다. 이 과정은 미분 불가능하여 end-to-end 학습을 방해하고, IoU 임계값과 같은 하이퍼파라미터에 민감하여 모델의 성능과 속도에 불안정성을 야기하는 근본적인 한계로 지적되어 왔다.6</p>
<h3>1.2  DETR의 등장: 직접 집합 예측(Direct Set Prediction)으로의 재정의</h3>
<p>2020년 Facebook AI Research (현 Meta AI)에서 발표한 DETR은 이러한 기존 객체 탐지 방법론의 복잡성과 한계를 정면으로 돌파하는 새로운 패러다임을 제시했다.9 DETR은 객체 탐지를 고정된 크기의 예측 집합(a fixed-size set of predictions)을 직접적으로 추론하는 ‘직접 집합 예측(direct set prediction)’ 문제로 재정의했다.1</p>
<p>이러한 혁신적인 접근법의 핵심은 자연어 처리(NLP) 분야에서 성공을 거둔 트랜스포머(Transformer) 아키텍처를 객체 탐지에 도입한 것이다.7 DETR은 트랜스포머의 인코더-디코더 구조와 셀프 어텐션 메커니즘을 활용하여 이미지의 전역적 맥락(global context)과 객체 간의 상호 관계를 추론한다.1 또한, 예측 집합과 실제 정답 집합 간의 유일한 대응 관계를 찾기 위해 이분 매칭(bipartite matching)에 기반한 집합 기반 손실 함수(set-based loss function)를 도입했다.1</p>
<p>이 두 가지 핵심 요소를 통해 DETR은 기존 파이프라인의 복잡성을 야기했던 RPN, 앵커 박스, NMS와 같은 수작업으로 설계된 구성 요소들을 완전히 제거할 수 있었다.5 이는 객체 탐지 문제에 대한 사전 지식(prior knowledge)을 하드코딩하는 대신, 모델이 데이터로부터 직접 문제의 본질을 학습하도록 유도하는 근본적인 철학의 변화를 의미한다. DETR의 가장 큰 기여는 새로운 구성 요소를 ’추가’한 것이 아니라, 기존의 복잡성을 야기했던 ’족쇄’를 ’제거’함으로써 더 단순하고 일반적인 해결책을 제시한 데 있다.</p>
<h3>1.3  End-to-End 철학의 구현과 그 의의</h3>
<p>DETR의 가장 중요한 의의는 객체 탐지 분야에서 최초로 완전한 end-to-end 프레임워크를 구현했다는 점이다.3 입력 이미지로부터 별도의 후처리 과정 없이 최종 예측 집합을 단일 패스(single pass)로 병렬적으로 출력함으로써, 학습부터 추론에 이르는 전체 파이프라인을 극도로 단순화했다.5</p>
<p>이러한 end-to-end 철학은 모델이 데이터로부터 직접 객체 간의 관계와 이미지의 전역적 맥락을 학습하도록 유도한다.1 기존 방법론들이 지역적인 특징에 의존하거나 휴리스틱한 규칙에 기반하여 예측을 결합했던 것과 달리, DETR의 트랜스포머는 이미지 전체를 한 번에 고려하여 각 예측을 생성한다. 이는 특히 객체들이 서로 겹쳐 있거나 복잡한 관계를 맺고 있는 장면에서 더 강인한 성능을 발휘하는 기반이 된다.14</p>
<p>결론적으로, DETR의 등장은 NLP 분야에서 트랜스포머가 그랬던 것처럼 컴퓨터 비전 분야, 특히 객체 탐지에서 새로운 연구 방향을 제시한 패러다임의 전환이었다.2 이는 복잡한 수작업 파이프라인에서 벗어나, 데이터 기반의 학습 가능한 단일 모델로 문제를 해결하려는 딥러닝의 근본 철학을 객체 탐지 분야에서 온전히 구현한 첫 사례로 평가받는다.</p>
<h2>2.  DETR 아키텍처 심층 분석</h2>
<h3>2.1  전체 구조 개요</h3>
<p>DETR 아키텍처는 전통적인 컴퓨터 비전의 강자인 CNN(Convolutional Neural Network)과 시퀀스 데이터 처리의 혁신을 이끈 트랜스포머를 결합한 독창적인 하이브리드 구조를 채택하고 있다. 이 구조는 크게 네 가지 핵심 구성 요소로 나눌 수 있다: (1) 이미지의 지역적 특징(local features)을 추출하는 CNN 백본, (2) 추출된 특징들 간의 전역적 관계(global relations)를 모델링하는 트랜스포머 인코더, (3) 객체별 정보를 추출하고 정제하는 트랜스포머 디코더, 그리고 (4) 최종 클래스와 경계 상자를 예측하는 FFN(Feed-Forward Network) 예측 헤드이다.13 이 설계는 이미지의 공간적, 계층적 특징 추출과 객체 간의 복잡한 상호작용 및 맥락 추론을 효과적으로 분리하고 통합한다.14</p>
<h3>2.2  데이터 흐름과 각 구성 요소의 역할</h3>
<h4>2.2.1  CNN 백본 (Backbone)</h4>
<p>데이터 처리의 첫 단계는 CNN 백본을 통해 입력 이미지로부터 고수준의 시각적 특징 표현을 추출하는 것이다.13 일반적으로 ImageNet으로 사전 학습된 ResNet-50 또는 ResNet-101 모델이 사용되며, 이 백본은 입력 이미지 <span class="math math-inline">x \in \mathbb{R}^{3 \times H_0 \times W_0}</span>를 받아 저해상도의 특징 맵 <span class="math math-inline">f \in \mathbb{R}^{C \times H \times W}</span>를 생성한다.15 이때 특징 맵의 해상도는 보통 입력 이미지의 1/32 수준(<span class="math math-inline">H = H_0/32, W = W_0/32</span>)이며, 채널 수 C는 2048이다. 이 고차원 특징 맵은 후속 트랜스포머가 처리하기 용이하도록 1x1 컨볼루션 연산을 통해 채널 차원을 더 작은 d (예: 256)로 축소한다. 이 축소된 특징 맵은 <span class="math math-inline">d \times H \times W</span> 크기를 가지며, 공간 차원을 펼쳐 <span class="math math-inline">HW \times d</span> 크기의 시퀀스로 변환되어 트랜스포머 인코더의 입력으로 전달된다.15</p>
<h4>2.2.2  위치 인코딩 (Positional Encoding)</h4>
<p>트랜스포머는 본질적으로 입력 순서에 무관한 순열 불변(permutation-invariant) 특성을 가지므로, 이미지 특징의 공간적 위치 정보를 명시적으로 주입해주어야 한다.15 이를 위해 각 특징 벡터의 위치에 해당하는 고정된 사인/코사인 함수 기반의 위치 인코딩 또는 학습 가능한 위치 인코딩 벡터가 생성되어 특징 맵에 더해진다.16 이 과정을 통해 트랜스포머는 각 특징이 이미지의 어느 위치에서 왔는지 인지할 수 있게 되며, 이는 객체의 절대적 및 상대적 위치를 이해하는 데 필수적이다.</p>
<h4>2.2.3  트랜스포머 인코더 (Transformer Encoder)</h4>
<p>위치 정보가 주입된 특징 시퀀스는 트랜스포머 인코더로 입력된다. 인코더는 여러 개의 동일한 레이어로 구성되며, 각 레이어는 멀티헤드 셀프 어텐션(Multi-Head Self-Attention) 모듈과 FFN으로 이루어져 있다.1 셀프 어텐션 메커니즘은 특징 맵 내의 모든 위치 쌍 간의 관계를 계산하여, 각 위치의 특징 벡터를 이미지 전체의 전역적인 맥락을 반영하여 업데이트한다.13 이를 통해 인코더는 객체와 배경, 그리고 객체들 간의 복잡한 상호작용을 포착하고, 개별 객체들을 분리해내는 능력을 학습한다.19 인코더의 최종 출력은 이미지의 풍부한 컨텍스트 정보를 압축한 ’메모리’로서 디코더에 전달된다.11</p>
<h4>2.2.4  트랜스포머 디코더 (Transformer Decoder)</h4>
<p>트랜스포머 디코더는 DETR 아키텍처의 핵심적인 추론이 이루어지는 부분으로, 두 종류의 입력을 받는다: (1) 인코더에서 생성된 전역적 컨텍스트 메모리와 (2) 고정된 수 N개(일반적으로 100)의 학습 가능한 임베딩인 ’Object Queries’이다.3 디코더 역시 여러 레이어로 구성되며, 각 레이어는 셀프 어텐션, 인코더-디코더 어텐션(크로스 어텐션), FFN 모듈을 포함한다.15</p>
<ul>
<li><strong>셀프 어텐션:</strong> Object Query들 간의 상호작용을 모델링한다. 이를 통해 쿼리들은 서로의 예측을 인지하게 되며, 이는 모델이 중복된 예측을 생성하는 것을 억제하는 데 중요한 역할을 한다.</li>
<li><strong>인코더-디코더 어텐션 (크로스 어텐션):</strong> 각 Object Query가 인코더의 출력 메모리에 ’질문’을 던져 관련 정보를 추출하는 과정이다. 각 쿼리는 이미지의 특정 영역이나 객체에 집중(attend)하여 해당 객체의 클래스 및 위치 정보를 담은 임베딩으로 정제된다.16</li>
</ul>
<p>이 과정은 디코더의 모든 레이어에 걸쳐 반복적으로 수행되며, 각 레이어를 통과할수록 쿼리 임베딩은 점차 더 정확한 객체 표현으로 발전한다.</p>
<h4>2.2.5  FFN 예측 헤드 (Prediction Heads)</h4>
<p>디코더의 각 레이어에서 출력된 N개의 정제된 쿼리 임베딩은 두 개의 공유된 FFN 예측 헤드로 전달된다.3</p>
<ul>
<li><strong>클래스 예측 헤드:</strong> 하나의 선형 레이어와 소프트맥스 함수로 구성된다. 각 쿼리 임베딩을 입력받아 <span class="math math-inline">C+1</span>개의 클래스(데이터셋의 C개 객체 클래스 + ’객체 없음’을 의미하는 <span class="math math-inline">\emptyset</span> 클래스)에 대한 확률 분포를 예측한다.15</li>
<li><strong>바운딩 박스 예측 헤드:</strong> 3개의 레이어로 구성된 MLP(Multi-Layer Perceptron)로, ReLU 활성화 함수를 사용한다. 각 쿼리에 대해 바운딩 박스의 정규화된 중심 좌표, 높이, 너비 <span class="math math-inline">(c_x, c_y, h, w)</span>를 나타내는 4개의 값을 직접 예측한다.15</li>
</ul>
<p>추론 시에는 마지막 디코더 레이어의 예측만을 사용하지만, 학습 시에는 모든 디코더 레이어의 출력에 예측 헤드를 연결하여 보조 손실(auxiliary losses)을 계산한다. 이는 모델이 초반 레이어에서도 의미 있는 예측을 하도록 유도하여 학습을 안정화하고 수렴을 돕는다.18</p>
<h3>2.3  핵심 개념 ‘Object Queries’</h3>
<p>’Object Queries’는 DETR의 가장 독창적인 개념 중 하나이다. 이는 사전 정의된 공간적 위치나 크기 정보가 없는, 학습 가능한 d차원 벡터(임베딩)의 집합이다.3 이 쿼리들은 앵커 박스처럼 ’공간적 사전 지식’을 하드코딩한 것이 아니라, 아무런 사전 정보 없이 시작하여 “이 이미지에서 내가 찾아야 할 것은 무엇인가?“를 스스로 학습하는 능동적인 ‘어텐션 프로브(attention probe)’ 또는 ’탐침’으로 기능한다.</p>
<p>학습 과정에서 각 쿼리는 디코더 내에서 이미지 전체의 컨텍스트와 상호작용하며 특정 객체를 찾아내도록 점차 특화된다.13 예를 들어, 어떤 쿼리는 수직으로 긴 객체(사람 등)를 전문적으로 탐지하도록, 다른 쿼리는 넓은 영역을 차지하는 큰 객체(버스 등)를 탐지하도록 역할 분담이 일어난다.19 이는 고정된 템플릿에 객체를 맞추는 기존의 방식과 달리, 학습된 ’개념’이 이미지 속에서 자신의 ’실체’를 찾아가는 Top-down 방식의 추론 과정과 유사하다.</p>
<p>쿼리의 수 N은 이미지 내 실제 객체 수와 무관한 고정된 하이퍼파라미터(보통 100)로, 이는 모델이 항상 고정된 크기의 예측 집합을 출력하게 만든다.13 이 고정된 출력 집합은 후술할 이분 매칭 기반의 손실 함수와 결합하여 DETR의 end-to-end 학습을 가능하게 하는 핵심적인 설계 요소이다.</p>
<h2>3.  학습 메커니즘: 집합 기반 손실과 이분 매칭</h2>
<h3>3.1  집합 예측의 과제: 순서 불변성과 유일성</h3>
<p>DETR은 고정된 수 N개의 예측을 병렬적으로 한 번에 출력한다. 이러한 ‘직접 집합 예측’ 방식은 기존의 순차적이거나 다단계적인 접근법과 근본적으로 달라 두 가지 주요 과제를 야기한다. 첫째, 모델이 출력하는 N개 예측의 순서는 매 추론마다 달라질 수 있다. 즉, 예측 집합은 순서에 무관(permutation-invariant)해야 한다.1 둘째, 여러 개의 예측 슬롯(쿼리)이 동일한 실제 객체를 가리키는 중복 예측 문제가 발생할 수 있다.12 따라서 손실을 계산하기 위해서는, 순서가 없는 예측 집합과 정답 객체 집합 사이에 안정적이고 유일한 대응 관계를 설정하는 메커니즘이 필수적이다.</p>
<h3>3.2  이분 매칭(Bipartite Matching)과 헝가리안 알고리즘</h3>
<p>DETR은 이 문제를 해결하기 위해 그래프 이론의 이분 매칭(bipartite matching) 개념을 도입했다. 이는 두 개의 분리된 정점 집합 사이에서 최적의 일대일(one-to-one) 대응을 찾는 과정이다.13 DETR의 학습 과정에서는 N개의 예측 집합과 N개의 정답 집합(ground truth set) 사이에 최적의 매칭을 찾는다. 이미지 내 실제 객체의 수가 M개 (<span class="math math-inline">M &lt; N</span>)일 경우, 정답 집합은 <span class="math math-inline">N-M</span>개의 ‘객체 없음(no object, <span class="math math-inline">\emptyset</span>)’ 클래스로 채워져(padding) 크기를 N으로 맞춘다.18</p>
<p>최적의 할당 <span class="math math-inline">\hat{\sigma}</span>는 N개 예측의 모든 가능한 순열(permutation) <span class="math math-inline">\sigma \in \mathfrak{S}_N</span> 중에서 전체 매칭 비용(matching cost)을 최소화하는 순열을 찾는 것으로 정의된다.1 이 최적 할당 문제는 조합 최적화 문제의 일종으로, 헝가리안 알고리즘(Hungarian Algorithm)을 사용하여 다항 시간 내에 효율적으로 해결할 수 있다.15 이 과정을 통해 각 예측은 최대 하나의 정답 객체에만 할당되며, 이는 중복 예측을 방지하는 학습 신호를 자연스럽게 제공한다. 즉, ’예측 후 필터링’이 아닌 ‘할당 후 최적화’ 과정을 통해 NMS의 역할을 학습 과정에 내재화하는 것이다.</p>
<h3>3.3  매칭 비용 및 전체 손실 함수 상세 분석</h3>
<h4>3.3.1  쌍별 매칭 비용 (Pair-wise Matching Cost)</h4>
<p>헝가리안 알고리즘이 최적의 매칭을 찾기 위해 사용하는 비용 함수 <span class="math math-inline">\mathcal{L}_{\text{match}}</span>는 각 예측과 정답 쌍에 대해 클래스 예측의 정확도와 바운딩 박스의 유사도를 종합적으로 평가한다.15 정답 객체 <span class="math math-inline">y_i = (c_i, b_i)</span>와 순열 <span class="math math-inline">\sigma(i)</span>에 해당하는 예측 <span class="math math-inline">\hat{y}_{\sigma(i)} = (\hat{p}_{\sigma(i)}(c_i), \hat{b}_{\sigma(i)})</span> 사이의 쌍별 매칭 비용은 다음과 같이 정의된다.1<br />
<span class="math math-display">
\mathcal{L}_{\text{match}}(y_i, \hat{y}_{\sigma(i)}) = -\mathbb{1}_{\{c_i \neq \emptyset\}} \hat{p}_{\sigma(i)}(c_i) + \mathbb{1}_{\{c_i \neq \emptyset\}} \mathcal{L}_{\text{box}}(b_i, \hat{b}_{\sigma(i)})
</span><br />
여기서 <span class="math math-inline">c_i</span>는 정답 클래스 레이블, <span class="math math-inline">\hat{p}_{\sigma(i)}(c_i)</span>는 예측된 클래스 확률, <span class="math math-inline">\mathcal{L}_{\text{box}}</span>는 바운딩 박스 손실을 의미한다. <span class="math math-inline">\mathbb{1}_{\{c_i \neq \emptyset\}}</span>는 지시 함수(indicator function)로, 정답 클래스가 ’객체 없음’이 아닐 경우에만 비용을 계산하도록 한다. 이 비용 함수는 클래스 예측 확률이 높고 바운딩 박스 손실이 작을수록 낮은 값을 가지도록 설계되었다.</p>
<h4>3.3.2  헝가리안 손실 (Hungarian Loss - 전체 손실 함수)</h4>
<p>헝가리안 알고리즘을 통해 최적의 매칭 <span class="math math-inline">\hat{\sigma}</span>가 결정되면, 이 매칭된 쌍들에 대해 최종적인 손실, 즉 헝가리안 손실(Hungarian Loss)을 계산한다. 이것이 DETR의 전체 손실 함수가 된다.1<br />
<span class="math math-display">
\mathcal{L}_{\text{Hungarian}}(y, \hat{y}) = \sum_{i=1}^{N} \left[ -\log \hat{p}_{\hat{\sigma}(i)}(c_i) + \mathbb{1}_{\{c_i \neq \emptyset\}} \mathcal{L}_{\text{box}}(b_i, \hat{b}_{\hat{\sigma}(i)}) \right]
</span><br />
클래스 예측에 대해서는 로그 확률(log-probabilities)을 사용한 Negative Log-Likelihood 손실이 적용된다. 이는 매칭 비용에서 확률 자체를 사용한 것과 차이가 있으며, 경험적으로 더 나은 성능을 보였다고 보고된다.15 또한, 실제 구현에서는 배경 클래스(‘객체 없음’)의 예측이 훨씬 많아 발생하는 클래스 불균형 문제를 완화하기 위해, <span class="math math-inline">c_i = \emptyset</span>인 경우의 로그 확률 항에 10배 낮은 가중치를 부여한다.1</p>
<h4>3.3.3  바운딩 박스 손실 (Bounding Box Loss)</h4>
<p>바운딩 박스 손실 <span class="math math-inline">\mathcal{L}_{\text{box}}</span>는 두 가지 손실 함수의 선형 결합으로 구성된다: L1 손실과 일반화된 IoU (Generalized Intersection over Union, GIoU) 손실이다.1<br />
<span class="math math-display">
\mathcal{L}_{\text{box}}(b_i, \hat{b}_{\sigma(i)}) = \lambda_{\text{iou}}\mathcal{L}_{\text{GIoU}}(b_i, \hat{b}_{\sigma(i)}) + \lambda_{L1} \|b_i - \hat{b}_{\sigma(i)}\|_1
</span><br />
L1 손실(<span class="math math-inline">\| \cdot \|_1</span>)은 예측된 박스 좌표와 정답 박스 좌표 간의 절대적인 차이를 측정한다. 그러나 L1 손실은 박스의 크기에 따라 손실 값의 스케일이 달라지는 문제가 있어, 큰 객체와 작은 객체에 대해 불균형한 학습을 유발할 수 있다.23 이러한 문제를 보완하기 위해, 박스의 크기와 위치에 상대적으로 불변하는(scale-invariant) GIoU 손실을 함께 사용한다. GIoU는 두 박스 간의 겹치는 영역뿐만 아니라, 두 박스를 모두 포함하는 가장 작은 박스와의 관계까지 고려하여 더 안정적인 학습을 가능하게 한다.1</p>
<p><span class="math math-inline">\lambda_{\text{iou}}</span>와 <span class="math math-inline">\lambda_{L1}</span>은 이 두 손실의 상대적 중요도를 조절하는 하이퍼파라미터이다.1</p>
<h2>4.  핵심 기여: NMS와 앵커의 제거</h2>
<p>DETR의 가장 혁신적인 기여는 객체 탐지 파이프라인을 수십 년간 지배해 온 두 가지 핵심 구성 요소, 즉 비최대 억제(NMS)와 앵커 박스(anchor boxes)를 완전히 제거한 것이다. 이는 단순히 파이프라인을 간소화하는 것을 넘어, 사람이 설계한 휴리스틱에서 벗어나 모델이 데이터 자체로부터 문제의 본질을 학습하게 만드는 근본적인 철학의 변화를 의미한다.</p>
<h3>4.1  NMS-Free Detector의 원리</h3>
<p>기존 검출기들은 하나의 객체에 대해 다수의 중복된 예측을 생성한 후, NMS라는 후처리 과정을 통해 가장 좋은 예측 하나만을 남기는 ‘과잉생성 후 제거’ 전략을 사용했다. DETR은 이러한 접근법을 근본적으로 뒤집었다.</p>
<p>DETR이 NMS 없이 작동할 수 있는 원리는 두 가지 핵심 메커니즘에 기반한다. 첫째, 앞서 설명한 <strong>집합 기반 손실과 일대일 매칭</strong>이다.1 헝가리안 알고리즘을 통한 이분 매칭은 학습 과정에서 각 정답 객체에 대해 단 하나의 예측만이 긍정적인 학습 신호(positive signal)를 받도록 강제한다.12 만약 두 개의 쿼리가 동일한 객체를 예측하더라도, 매칭 단계에서 하나는 해당 객체와, 다른 하나는 ’객체 없음(<span class="math math-inline">\emptyset</span>)’과 매칭되어 페널티를 받게 된다. 이러한 과정이 수많은 에포크에 걸쳐 반복되면서, 모델은 자연스럽게 각 쿼리가 서로 다른 객체를 담당하도록 학습하여 중복 예측을 원천적으로 억제한다.</p>
<p>둘째, <strong>트랜스포머 디코더의 셀프 어텐션</strong> 메커니즘이다.26 디코더 내에서 쿼리들은 셀프 어텐션을 통해 서로의 정보를 교환한다. 이는 각 쿼리가 다른 쿼리들이 어떤 객체를 예측하고 있는지를 인지하게 하여, 서로 동일한 객체에 집중하는 것을 피하도록 학습하는 효과를 낳는다.27 즉, 쿼리들 간의 상호작용을 통해 중복을 줄이는 일종의 ‘내부적 NMS’ 역할을 수행하는 것이다.</p>
<p>이 두 메커니즘의 결합으로 DETR은 추론 시점에 NMS가 전혀 필요 없는, 진정한 NMS-Free 검출기가 되었다. 이는 추론 속도 저하, 하이퍼파라미터 민감성, 그리고 end-to-end 학습의 제약이라는 오랜 문제를 해결하는 중요한 돌파구였다.8</p>
<h3>4.2  Anchor-Free Detector의 구현</h3>
<p>앵커 박스는 1단계 검출기의 성공에 핵심적인 역할을 했지만, 동시에 한계도 명확했다. 앵커는 객체의 크기와 종횡비에 대한 강력한 사전 가정(prior)으로, 이 가정에 잘 맞지 않는 객체에 대해서는 성능이 저하될 수 있으며, 최적의 앵커 설정을 찾는 것은 데이터셋에 따라 매우 번거로운 작업이었다.4</p>
<p>DETR은 이러한 앵커 기반 접근법을 완전히 배제했다. 기존 모델들이 사전 정의된 앵커 박스를 기준으로 상대적인 오프셋(offset)과 크기 조정을 예측했던 것과 달리, DETR의 예측 헤드는 <strong>바운딩 박스의 절대 좌표를 직접 예측</strong>한다.1 구체적으로, 각 쿼리에 대해 이미지 전체 크기에 대해 정규화된 중심 좌표 <span class="math math-inline">(c_x, c_y)</span>와 높이(<span class="math math-inline">h</span>), 너비(<span class="math math-inline">w</span>)를 직접 회귀한다.15</p>
<p>이는 트랜스포머 인코더가 이미지 전체의 전역적 맥락을 이해하고, 디코더가 이 정보를 바탕으로 각 쿼리에 대한 객체의 절대적인 위치와 크기를 추론할 수 있기 때문에 가능하다. 이 방식은 객체의 형태나 크기에 대한 어떠한 사전 가정도 필요로 하지 않으므로 모델의 유연성과 일반화 성능을 크게 향상시킨다.6 또한, 앵커 박스 설계와 관련된 복잡한 엔지니어링 과정이 완전히 사라져 모델 개발 과정을 단순화했다.5</p>
<h3>4.3  완전한 End-to-End 파이프라인의 가치</h3>
<p>NMS와 앵커의 성공적인 제거는 DETR을 최초의 완전한 end-to-end 객체 탐지 프레임워크로 만들었다.3 이러한 파이프라인의 가치는 단순한 ’간결함’을 넘어선다.</p>
<p>첫째, <strong>견고함(robustness)과 일반화 능력</strong>이 향상된다. NMS 임계값이나 앵커 박스 스케일과 같이 데이터셋이나 문제에 따라 민감하게 변하는 ‘깨지기 쉬운(brittle)’ 휴리스틱 구성 요소들을 제거함으로써, 모델은 새로운 도메인이나 데이터셋에 대해 더 안정적인 성능을 보인다. 이는 복잡한 하이퍼파라미터 재설계 없이도 모델의 이식성(portability)을 높이는 중요한 특성이다.</p>
<p>둘째, **재현성(reproducibility)과 확장성(scalability)**이 증대된다. DETR은 표준적인 CNN과 트랜스포머 블록만으로 구성되어 있어 특별한 라이브러리 없이도 쉽게 구현하고 재현할 수 있다.1 또한, 이 단순하고 명료한 구조는 객체 탐지를 넘어 Panoptic Segmentation과 같은 더 복잡한 비전 태스크로 쉽게 확장될 수 있는 강력한 기반이 되었다.1 전체 시스템이 미분 가능한 단일 모듈로 구성되어 있어, 새로운 태스크에 맞는 헤드(head)를 추가하고 손실 함수를 정의하는 것만으로도 end-to-end 공동 최적화가 가능하다.</p>
<h2>5.  DETR의 한계와 진화의 시작</h2>
<p>DETR은 객체 탐지에 혁신적인 패러다임을 제시했지만, 초기 모델은 실용적인 적용에 있어 몇 가지 명백한 한계를 드러냈다. 이러한 한계들은 이후 DETR 계열 모델들의 진화 방향을 결정하는 중요한 계기가 되었다. 오리지널 DETR의 한계는 ’자유’에 대한 대가로 볼 수 있다. 앵커와 같은 강력한 공간적 사전 지식(prior)을 제거함으로써 높은 일반성을 얻었지만, 그 대가로 모델이 아무런 가이드 없이 방대한 탐색 공간에서 객체의 위치를 학습해야 했다. 이 ’탐색 공간의 비대함’이 바로 느린 수렴과 작은 객체 탐지 실패의 근본 원인이었다.</p>
<h3>5.1  주요 문제점: 느린 학습 수렴 속도</h3>
<p>오리지널 DETR의 가장 큰 단점은 극도로 느린 학습 수렴 속도였다. COCO 데이터셋에서 경쟁력 있는 성능에 도달하기 위해 500 에포크라는 매우 긴 학습 시간을 필요로 했다. 이는 Faster R-CNN과 같은 기존 모델들이 12~36 에포크 만에 수렴하는 것과 비교해 10배에서 20배 이상 느린 것이다.1 이 문제의 원인은 다음과 같이 다각적으로 분석될 수 있다.</p>
<ul>
<li><strong>원인 분석 1 (어텐션의 비효율성):</strong> 트랜스포머 인코더의 셀프 어텐션은 특징 맵의 모든 픽셀 쌍에 대해 연산을 수행한다. 이는 특징 맵의 공간적 크기(<span class="math math-inline">H \times W</span>)에 대해 제곱에 비례하는(<span class="math math-inline">O(H^2W^2C)</span>) 막대한 연산량과 메모리를 요구한다.30 이로 인해 DETR은 고해상도 특징 맵을 처리하는 데 한계가 있었고, 이는 학습 효율성과 작은 객체 탐지 성능에 직접적인 악영향을 미쳤다.31</li>
<li><strong>원인 분석 2 (Cross-Attention의 매칭 어려움):</strong> 학습 초기 단계에서, 랜덤하게 초기화된 Object Query는 이미지의 어떤 부분에 집중해야 할지에 대한 사전 정보가 전혀 없다. 이로 인해 디코더의 크로스 어텐션 가중치는 이미지 전체에 거의 균일하게 퍼지게 된다.29 모델이 수많은 학습 반복을 통해 점진적으로 특정 객체에 집중하도록 어텐션 가중치를 학습하는 과정은 매우 비효율적이며, 이것이 느린 수렴의 핵심 원인 중 하나로 지목된다. 이는 Object Query와 인코더의 이미지 특징 간의 ‘의미론적 불일치(semantic misalignment)’ 문제로 설명될 수 있다.32</li>
<li><strong>원인 분석 3 (이분 매칭의 불안정성):</strong> 학습 과정에서 각 쿼리에 할당되는 정답 객체가 에포크마다 계속해서 바뀌는 불안정한 매칭 문제가 발생한다. 이는 최적화 목표를 계속해서 변경하여 학습을 어렵게 만들고 수렴을 방해하는 요인이 된다.7</li>
</ul>
<h3>5.2  주요 문제점: 작은 객체 탐지 성능 저하</h3>
<p>DETR은 큰 객체(AP_L)에 대해서는 기존 모델보다 뛰어난 성능을 보였지만, 작은 객체(AP_S)에 대한 탐지 성능은 현저히 낮았다.1</p>
<ul>
<li><strong>원인 분석:</strong> 이 문제의 가장 큰 원인은 앞서 언급한 어텐션의 연산량 문제로 인해 저해상도 특징 맵(백본의 마지막 레이어, 예: ResNet의 C5 스테이지, 해상도 <span class="math math-inline">H/32</span>)만을 사용했기 때문이다.10 작은 객체는 이러한 저해상도 맵에서 몇 개의 픽셀로만 표현되거나 아예 소실되어 공간적, 의미적 정보를 충분히 담지 못한다. 기존 검출기들은 FPN(Feature Pyramid Network)과 같은 다중 스케일 특징 맵을 활용하여 이 문제를 해결했지만, 오리지널 DETR의 트랜스포머 구조는 FPN을 적용할 경우 연산 복잡도가 감당할 수 없을 정도로 증가하여 이를 채택하기 어려웠다.10 작은 객체는 신호가 약해 어텐션 학습 과정에서 무시되기 쉬웠고, 저해상도 맵에서는 그 신호 자체가 소실되어 탐지가 더욱 어려워졌다.</li>
</ul>
<p>이러한 초기 한계들은 DETR의 잠재력을 완전히 발현시키기 위해 해결해야 할 명확한 과제들을 제시했다. 이후 등장하는 수많은 DETR 변형 모델들은 바로 이 ’느린 수렴’과 ’작은 객체 탐지 성능 저하’라는 두 가지 문제를 해결하는 데 집중하며 발전하게 된다.</p>
<h2>6.  주요 DETR 변형 모델 분석</h2>
<p>오리지널 DETR이 제시한 문제점들을 해결하기 위해 수많은 후속 연구가 진행되었으며, 이 과정에서 DETR의 성능과 효율성을 획기적으로 개선한 핵심 변형 모델들이 등장했다. 이 모델들의 발전 과정은 오리지널 DETR의 ’완전한 자유’가 야기한 비효율을 해결하기 위해, 점진적으로 더 정교하고 효율적인 ’가이드’를 다시 도입하는 역사로 볼 수 있다. 이는 사전 지식을 완전히 배제하는 것보다, ‘학습 가능한(learnable)’ 형태의 유연한 가이드를 제공하는 것이 더 효과적임을 시사한다.</p>
<h3>6.1  Deformable DETR: 효율성과 다중 스케일의 도입</h3>
<p>Deformable DETR은 오리지널 DETR의 핵심 한계인 느린 수렴과 높은 연산량을 해결하기 위해 제안된 첫 번째 주요 변형 모델이다.34</p>
<ul>
<li><strong>핵심 아이디어:</strong> 트랜스포머 어텐션이 이미지 특징 맵의 모든 픽셀을 계산하는 대신, Deformable Convolution에서 영감을 받아 각 쿼리에 대해 소수의 핵심적인 샘플링 지점(key sampling points)에만 집중(attend)하도록 만든다.31 이는 어텐션의 탐색 범위에 ’참조점(reference point)’이라는 약한 공간적 가이드를 도입하여 제한하는 방식이다.</li>
<li><strong>기술적 원리 (Deformable Attention Module):</strong> 각 쿼리와 그에 해당하는 2D 참조점에 대해, 모델은 선형 레이어를 통해 샘플링할 위치의 오프셋(offset)과 각 샘플의 중요도를 나타내는 어텐션 가중치를 동적으로 학습한다. 실제 어텐션 연산은 이 소수의 샘플링된 특징들에 대해서만 수행된다.30</li>
<li><strong>기여 및 효과:</strong></li>
<li><strong>연산량 감소:</strong> 어텐션 연산량이 특징 맵의 공간적 크기에 대해 제곱(<span class="math math-inline">O(H^2W^2)</span>)에서 선형(<span class="math math-inline">O(HW)</span>)으로 감소했다. 이로 인해 고해상도의 다중 스케일 특징 맵(multi-scale features) 사용이 가능해졌다.30</li>
<li><strong>수렴 속도 향상:</strong> 학습 초기에 어텐션이 전역에 분산되는 대신, 참조점 주변의 희소한 지점에 집중하게 되어 학습이 훨씬 빠르게 수렴한다. 이로 인해 학습 에포크를 10배가량 단축할 수 있었다.34</li>
<li><strong>작은 객체 성능 개선:</strong> 다중 스케일 특징 맵을 효율적으로 처리할 수 있게 되면서, 오리지널 DETR의 가장 큰 약점이었던 작은 객체 탐지 성능이 크게 향상되었다.34</li>
</ul>
<h3>6.2  Conditional DETR: 수렴 가속화를 위한 공간적 조건 부여</h3>
<p>Conditional DETR은 디코더의 크로스 어텐션 학습이 어려운 문제를 해결하여 수렴 속도를 더욱 가속화하는 데 초점을 맞춘 모델이다.38</p>
<ul>
<li><strong>핵심 아이디어:</strong> Deformable DETR보다 한발 더 나아가, 데이터에 따라 동적으로 변하는 ’조건부 공간 쿼리(conditional spatial query)’라는 가이드를 제공한다.</li>
<li><strong>기술적 원리:</strong> 디코더의 내용(content) 임베딩으로부터 2D 좌표 정보를 예측하고, 이를 각 쿼리에 대한 조건부 공간적 임베딩으로 변환하여 크로스 어텐션에 사용한다. 이를 통해 각 어텐션 헤드는 객체의 특정 부분(예: 상, 하, 좌, 우 모서리)을 포함하는 좁은 대역(band)에 집중하도록 유도된다.38</li>
<li><strong>기여 및 효과:</strong> 어텐션이 집중해야 할 공간적 범위를 효과적으로 좁혀줌으로써, 모델이 객체의 위치를 찾는 학습 난이도를 크게 낮추고 수렴 속도를 획기적으로 가속화했다(약 6.7배 ~ 10배).39</li>
</ul>
<h3>6.3  DINO: SOTA 달성을 위한 종합적 개선</h3>
<p>DINO(DETR with Improved deNoising anchOr boxes)는 이전 연구들(DAB-DETR, DN-DETR 등)의 장점을 집대성하고 새로운 아이디어를 추가하여 DETR 계열 모델의 성능을 SOTA(State-of-the-Art) 수준으로 끌어올린 모델이다.41</p>
<ul>
<li><strong>핵심 아이디어:</strong> DN-DETR(Denoising DETR)의 학습 안정화 기법을 기반으로, 쿼리 초기화, 노이즈 제거 학습, 박스 예측 방식을 종합적으로 개선하여 성능을 극대화한다.</li>
<li><strong>기술적 원리:</strong></li>
</ul>
<ol>
<li><strong>Contrastive Denoising Training:</strong> 기존의 노이즈 제거 학습을 확장하여, 정답 박스 주변에 어려운 음성 샘플(hard negative samples)을 생성하고 모델이 이를 ’객체 없음’으로 분류하도록 학습시킨다. 이를 통해 중복 예측 억제 능력을 강화한다.41</li>
<li><strong>Mixed Query Selection:</strong> 인코더 특징 맵에서 유망한 위치를 앵커(위치 쿼리)로 선택하되, 내용 쿼리(content query)는 학습 가능한 임베딩을 그대로 사용하여 쿼리 초기화 품질을 높인다. 이는 고품질 앵커 초기화를 통해 특히 작은 객체 탐지 성능을 향상시킨다.41</li>
<li><strong>Look Forward Twice Scheme:</strong> 각 디코더 레이어에서 박스를 업데이트할 때, 현재 레이어의 손실뿐만 아니라 다음 레이어의 손실로부터 오는 그래디언트까지 함께 고려하여 더 전역적으로 최적화된 박스 예측을 유도한다.41</li>
</ol>
<ul>
<li><strong>기여 및 효과:</strong> 이러한 기술들의 시너지를 통해 DINO는 수렴 속도와 최종 성능을 모두 크게 향상시켰다. ResNet-50 백본으로 12 에포크만 학습해도 49.4 AP를 달성했으며, 이는 COCO 벤치마크에서 SOTA 성능을 기록하며 DETR 계열 모델의 성능적 우수성을 입증했다.41</li>
</ul>
<h3>6.4  RT-DETR &amp; RT-DETRv2: 실시간 탐지를 향한 진화</h3>
<p>RT-DETR(Real-Time DETR)은 DETR의 높은 정확도를 유지하면서 YOLO 수준의 실시간 추론 속도를 달성하는 것을 목표로 개발되었다.8</p>
<ul>
<li>
<p><strong>핵심 아이디어:</strong> 연산량의 병목이었던 트랜스포머 인코더를 효율적인 하이브리드 구조로 대체하고, 쿼리 선택 과정을 개선하여 속도와 정확도의 균형을 맞춘다.</p>
</li>
<li>
<p><strong>기술적 원리 (RT-DETR):</strong></p>
</li>
<li>
<p><strong>Efficient Hybrid Encoder:</strong> 연산량이 많은 트랜스포머 인코더를, CNN 기반 블록으로 스케일 내 상호작용(intra-scale interaction)을 처리하고 소수의 트랜스포머 블록으로 스케일 간 융합(cross-scale fusion)을 처리하는 하이브리드 구조로 대체하여 효율성을 극대화한다.28 이는 컨볼루션의 ’지역성(locality)’이라는 강력한 귀납적 편향(inductive bias)을 가이드로 다시 활용한 것이다.</p>
</li>
<li>
<p><strong>IoU-aware Query Selection:</strong> 인코더 특징으로부터 초기 객체 쿼리를 선택할 때, 분류 점수뿐만 아니라 IoU 예측 품질까지 고려하여 더 높은 품질의 쿼리를 디코더에 제공한다.46</p>
</li>
<li>
<p><strong>기술적 원리 (RT-DETRv2):</strong> RT-DETR을 기반으로, 성능 저하 없이 유연성과 실용성을 높이는 ‘Bag-of-Freebies’ 최적화 기법들을 도입했다.47</p>
</li>
</ul>
<p><strong>Selective Multi-scale Sampling</strong>, 배포 제약을 없애는 <strong>Discrete Sampling Operator</strong>, <strong>Dynamic Data Augmentation</strong>, <strong>Scale-adaptive Hyperparameters</strong> 등의 기법을 통해 배포 용이성을 높이고 학습 전략을 최적화했다.47</p>
<ul>
<li><strong>기여 및 효과:</strong> RT-DETR 계열은 NMS가 없는 end-to-end 구조의 장점을 유지하면서, YOLO 계열 모델들을 속도와 정확도 모든 면에서 능가하는 최초의 실시간 DETR 모델이 되었다.8</li>
</ul>
<p>다음 표는 주요 DETR 변형 모델들의 발전 과정을 요약한 것이다.</p>
<table><thead><tr><th>모델 (Model)</th><th>발표 연도</th><th>해결하고자 한 문제</th><th>핵심 기술 (Key Innovation)</th><th>주요 기여</th></tr></thead><tbody>
<tr><td><strong>DETR</strong></td><td>2020</td><td>복잡한 파이프라인 (NMS, 앵커)</td><td>트랜스포머, 이분 매칭 손실</td><td>End-to-End 객체 탐지 패러다임 제시</td></tr>
<tr><td><strong>Deformable DETR</strong></td><td>2021</td><td>느린 수렴, 높은 연산량, 작은 객체 성능 저하</td><td>Deformable Attention Module</td><td>연산 효율화, 다중 스케일 특징 처리, 수렴 가속</td></tr>
<tr><td><strong>Conditional DETR</strong></td><td>2021</td><td>느린 수렴</td><td>Conditional Spatial Query</td><td>크로스 어텐션 학습 난이도 완화, 수렴 가속</td></tr>
<tr><td><strong>DINO</strong></td><td>2022</td><td>수렴 불안정성, 성능 한계</td><td>Contrastive Denoising, Mixed Query Selection</td><td>학습 안정화, SOTA 정확도 달성</td></tr>
<tr><td><strong>RT-DETR</strong></td><td>2023</td><td>실시간성 부재</td><td>Efficient Hybrid Encoder, IoU-aware Query Selection</td><td>실시간 성능 달성, YOLO 성능 능가</td></tr>
<tr><td><strong>RT-DETRv2</strong></td><td>2024</td><td>배포 제약, 학습 최적화</td><td>Bag-of-Freebies (e.g., Discrete Sampling)</td><td>실용성 및 유연성 증대, 학습 전략 최적화</td></tr>
</tbody></table>
<h2>7.  타 분야로의 확장</h2>
<p>DETR의 ‘쿼리-어텐션-예측’ 패러다임은 객체 탐지를 넘어 다양한 컴퓨터 비전 과제에 적용될 수 있는 강력한 일반성을 지닌다. 이는 DETR이 단순한 ’객체 탐지기’가 아니라, 다양한 시각적 구조를 예측할 수 있는 일반화된 ’인식 엔진(Perception Engine)’으로서의 가능성을 보여준다. ’쿼리’의 의미를 재정의하고 예측 헤드와 손실 함수를 교체하는 것만으로도 다양한 고차원 구조적 예측 문제에 일관되게 적용될 수 있다.</p>
<h3>7.1  Panoptic Segmentation: Mask DINO</h3>
<p>파노라마 분할(Panoptic Segmentation)은 이미지의 모든 픽셀에 대해 클래스 레이블(semantic segmentation)과 인스턴스 ID(instance segmentation)를 동시에 할당하는 복합적인 과업이다.22</p>
<ul>
<li><strong>아키텍처 확장:</strong> Mask DINO는 SOTA 객체 탐지 모델인 DINO 아키텍처를 기반으로, 기존의 바운딩 박스 예측 헤드와 병렬적으로 **마스크 예측 브랜치(mask prediction branch)**를 추가하여 분할 기능을 통합했다.50</li>
<li><strong>작동 원리:</strong> DINO의 디코더에서 정제된 N개의 쿼리 임베딩을 그대로 활용한다. 이 쿼리 임베딩을 고해상도 픽셀 임베딩 맵(인코더 특징으로부터 생성)과 내적(dot-product)하여 각 쿼리에 해당하는 N개의 이진 마스크(binary mask)를 예측한다.50 즉, ’객체’를 찾기 위한 질문이 ’픽셀 영역’을 찾기 위한 질문으로 자연스럽게 확장된 것이다.</li>
<li><strong>기여:</strong> Mask DINO는 탐지와 분할 태스크가 공유된 아키텍처와 공동 학습을 통해 상호 보완적으로 성능을 향상시킬 수 있음을 입증했다. DINO의 강력한 객체 탐지 능력이 마스크 예측의 품질을 높이는 가이드 역할을 했으며, 그 결과 인스턴스, 파노라마, 시맨틱 분할 모든 분야에서 기존의 전문화된 모델들을 능가하는 SOTA 성능을 달성했다.50</li>
</ul>
<h3>7.2  3D Object Detection: DETR3D</h3>
<p>자율주행과 같은 분야에서 필수적인 3D 객체 탐지는 여러 대의 카메라로부터 얻은 2D 이미지를 통해 3D 공간 내 객체의 위치, 크기, 방향을 예측하는 과업이다.53</p>
<ul>
<li><strong>아키텍처 확장:</strong> 기존 방법들이 2D 이미지에서 2D 예측을 수행한 후 3D로 변환하거나, 부정확할 수 있는 깊이(depth) 예측에 의존했던 것과 달리, DETR3D는 <strong>3D 공간에서 직접 예측을 수행</strong>한다.53</li>
<li><strong>작동 원리 (3D-to-2D Query):</strong></li>
</ul>
<ol>
<li>3D 공간에 정의된 희소한(sparse) 3D 객체 쿼리로부터 3D 참조점(reference point)을 예측한다.</li>
<li>이 3D 참조점을 카메라의 내/외부 파라미터를 담은 변환 행렬을 이용해 각 2D 이미지 평면에 **역투영(back-project)**한다.</li>
<li>역투영된 2D 좌표를 이용해 해당 위치의 2D 특징 맵에서 특징을 샘플링한다.</li>
<li>여러 뷰에서 샘플링된 특징들을 취합하여 3D 쿼리를 정제하고, 이 과정을 반복하여 최종 3D 바운딩 박스를 예측한다.53</li>
</ol>
<ul>
<li><strong>기여:</strong> DETR3D는 깊이를 명시적으로 예측하는 중간 단계를 제거함으로써, 깊이 예측 오류로 인한 성능 저하 문제를 근본적으로 해결했다. 또한, 3D 쿼리라는 일관된 표현을 통해 여러 카메라 뷰의 정보를 자연스럽게 융합하고, NMS 없이 end-to-end 학습을 가능하게 했다.55</li>
</ul>
<h3>7.3  Pose Estimation: PETR</h3>
<p>다중 인물 자세 추정(Multi-person Pose Estimation)은 이미지 내 모든 사람의 주요 신체 관절(keypoints)의 위치를 찾는 과업이다.</p>
<ul>
<li><strong>아키텍처 확장:</strong> PETR(Pose Estimation with TRansformers)은 이 과업을 <strong>계층적 집합 예측(hierarchical set prediction)</strong> 문제로 재정의하여 DETR 프레임워크를 적용했다.56</li>
<li><strong>작동 원리 (Pose Query):</strong></li>
</ul>
<ol>
<li>학습 가능한 ’포즈 쿼리(pose queries)’를 사용하여 이미지 전체의 컨텍스트 속에서 직접 인스턴스별 전신 포즈(full-body poses) 집합을 추론한다.</li>
<li>계층적 디코더 구조를 채택하여, **포즈 디코더(pose decoder)**가 먼저 인스턴스 수준의 전체적인 포즈를 거칠게 예측한다.</li>
<li>이후 **관절 디코더(joint decoder)**가 각 포즈의 관절들을 개별적으로 입력받아, 관절 간의 구조적 관계를 고려하며 위치를 더욱 정교하게 조정한다.56</li>
</ol>
<ul>
<li><strong>기여:</strong> PETR은 기존의 top-down(사람 먼저 찾고 포즈 추정) 또는 bottom-up(관절 먼저 찾고 사람으로 그룹핑) 방식의 복잡한 파이프라인과 RoI 크롭핑, 그룹핑, NMS와 같은 후처리 과정들을 모두 제거하여 최초의 완전한 end-to-end 자세 추정 프레임워크를 구현했다.56</li>
</ul>
<h2>8.  성능 비교 분석</h2>
<p>DETR 계열 모델의 성능을 객관적으로 평가하기 위해, 객체 탐지 분야의 표준 벤치마크인 COCO 데이터셋을 기준으로 주요 경쟁 모델인 Faster R-CNN, YOLO 계열과 비교 분석한다. 평가는 정확도를 나타내는 AP(Average Precision)와 추론 속도를 나타내는 FPS(Frames per Second)를 중심으로 이루어진다. DETR의 발전사는 초기 ‘정확도 우선’ 모델에서 시작하여, 점차 실용성을 개선하며 최종적으로는 정확도와 속도 모든 면에서 SOTA를 달성하는 ‘균형 잡힌’ 모델로 진화하는 과정으로 요약될 수 있다.</p>
<h3>8.1  DETR vs. Faster R-CNN</h3>
<p>오리지널 DETR은 발표 당시, 고도로 최적화된 2단계 검출기의 대표주자인 Faster R-CNN FPN과 직접적으로 비교되었다. 비슷한 파라미터 수를 가진 ResNet-50 백본 기준, DETR은 42.0 AP를 기록하며 Faster R-CNN과 동등한 수준의 정확도를 달성했다.1</p>
<p>성능의 세부적인 측면에서는 뚜렷한 차이를 보였다. DETR은 트랜스포머의 전역적 어텐션 능력 덕분에 이미지 전체의 맥락을 이해하는 데 강점을 보여, 큰 객체(AP_L) 탐지 성능에서 Faster R-CNN을 7.8 AP점이나 크게 능가했다.1 그러나 연산량 문제로 인해 저해상도 특징 맵만을 사용한 탓에, 작은 객체(AP_S) 탐지 성능은 5.5 AP점이나 뒤처지는 명백한 한계를 보였다.1 추론 속도 면에서는 ResNet-50 기반 DETR이 약 28 FPS로, Faster R-CNN-FPN(약 7-10 FPS)보다는 빨랐지만, 실시간 탐지 기준에는 미치지 못했다.5</p>
<h3>8.2  DETR vs. YOLO</h3>
<p>전통적으로 YOLO 계열은 1단계 구조와 지속적인 최적화를 통해 실시간 추론 속도에서 압도적인 우위를 점해왔다.2 오리지널 DETR이나 Deformable DETR은 정확도에서는 경쟁력이 있었지만, 속도 면에서는 YOLO를 따라잡지 못했다.</p>
<p>이러한 구도는 RT-DETR의 등장으로 완전히 역전되었다. RT-DETR은 NMS가 없는 end-to-end 구조의 장점을 유지하면서, 효율적인 하이브리드 인코더 설계를 통해 추론 속도를 획기적으로 개선했다. Baidu에서 발표한 결과에 따르면, RT-DETR-R50 모델은 T4 GPU에서 108 FPS의 속도로 53.1% AP를 달성했으며, 더 큰 모델인 RT-DETR-L은 114 FPS에서 53.0% AP를 기록했다.8 이는 당시 SOTA 실시간 모델이었던 YOLOv8의 L, X 모델들을 속도와 정확도 모든 면에서 능가하는 결과였다.58 후속 모델인 RT-DETRv2는 학습 전략 최적화를 통해 성능을 55.3% AP까지 끌어올리며, 실시간 DETR 모델의 우수성을 다시 한번 입증했다.59</p>
<h3>8.3  종합 성능 비교표</h3>
<p>다음 표는 주요 객체 탐지 모델들의 COCO val2017 데이터셋에 대한 성능을 요약한 것이다. 이를 통해 DETR 계열 모델이 초기 모델의 한계를 극복하고 어떻게 최상위 성능을 달성했는지 명확히 확인할 수 있다.</p>
<table><thead><tr><th>모델 (Model)</th><th>백본 (Backbone)</th><th>AP</th><th>AP<span class="math math-inline">_{50}</span></th><th>AP<span class="math math-inline">_{S}</span></th><th>AP<span class="math math-inline">_{M}</span></th><th>AP<span class="math math-inline">_{L}</span></th><th>FPS (T4 GPU)</th></tr></thead><tbody>
<tr><td>Faster R-CNN 1</td><td>ResNet-50-FPN</td><td>42.0</td><td>62.1</td><td>25.5</td><td>45.8</td><td>53.4</td><td>~10</td></tr>
<tr><td>YOLOv8-L 60</td><td>-</td><td>52.9</td><td>69.4</td><td>36.3</td><td>57.1</td><td>67.2</td><td>~100</td></tr>
<tr><td>YOLOv9-C 61</td><td>-</td><td>53.0</td><td>70.2</td><td>36.2</td><td>57.4</td><td>67.4</td><td>~125</td></tr>
<tr><td>DETR 1</td><td>ResNet-50</td><td>42.0</td><td>62.4</td><td>20.5</td><td>45.8</td><td>61.1</td><td>28</td></tr>
<tr><td>Deformable DETR 36</td><td>ResNet-50</td><td>43.8</td><td>62.6</td><td>26.4</td><td>47.1</td><td>58.0</td><td>~25</td></tr>
<tr><td>DINO 42</td><td>ResNet-50 (12ep)</td><td>49.4</td><td>66.8</td><td>31.8</td><td>52.9</td><td>63.8</td><td>~24</td></tr>
<tr><td>RT-DETR 8</td><td>ResNet-50</td><td>53.1</td><td>-</td><td>-</td><td>-</td><td>-</td><td>108</td></tr>
<tr><td>RT-DETRv2 59</td><td>ResNet-50</td><td>55.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>~108</td></tr>
</tbody></table>
<p><em>주: FPS는 모델, 하드웨어, 배치 크기 등 다양한 조건에 따라 달라질 수 있으며, 위 표는 보고된 수치를 기준으로 한 상대적인 비교를 위한 것이다.</em></p>
<p>이 표는 DETR의 진화 과정을 명확하게 보여준다. 오리지널 DETR은 Faster R-CNN과 비슷한 AP를 기록했지만 작은 객체(AP_S)에 약했다. Deformable DETR은 AP_S를 개선했으며, DINO는 전반적인 AP를 크게 끌어올렸다. 최종적으로 RT-DETR 계열은 YOLO를 능가하는 속도와 DINO를 상회하는 정확도를 동시에 달성하며, DETR 패러다임이 학술적 가능성을 넘어 산업 현장에서 주력으로 사용될 수 있는 강력하고 실용적인 기술로 성숙했음을 증명했다.</p>
<h2>9.  결론: 현재의 한계와 미래 연구 방향</h2>
<p>DETR은 객체 탐지 분야에 전례 없는 패러다임 전환을 가져왔으며, end-to-end 철학을 성공적으로 구현하여 NMS와 앵커 박스라는 오랜 제약에서 벗어났다. Deformable DETR, DINO, RT-DETR 등 수많은 후속 연구를 통해 초기 모델의 한계를 극복하고 현재는 정확도와 속도 모든 면에서 최상위 성능을 자랑하는 주류 아키텍처로 자리매김했다. 그러나 DETR 기반 모델들은 여전히 해결해야 할 과제를 안고 있으며, 이는 미래 연구의 중요한 방향을 제시한다. DETR의 미래는 ’더 똑똑한 쿼리’와 ’더 풍부한 컨텍스트’를 어떻게 구현하느냐에 달려있다. 초기 성공이 아키텍처 혁신에 있었다면, 미래의 발전은 (1) 쿼리가 더 적은 데이터로 더 빠르고 정확하게 정보를 학습하는 방법과 (2) 이미지뿐만 아니라 텍스트, 3D, 시간적 정보 등 더 넓은 컨텍스트를 쿼리가 활용하는 방법에 의해 주도될 것이다.</p>
<h3>9.1  DETR 기반 모델의 현재 당면 과제</h3>
<ul>
<li><strong>여전한 계산 복잡성:</strong> RT-DETR이 실시간 성능을 달성했지만, 트랜스포머 기반 아키텍처는 본질적으로 CNN 기반 모델보다 파라미터 수가 많고 연산량이 많다.14 특히 디코더의 어텐션 연산은 여전히 계산 비용이 높아, 모바일이나 엣지 디바이스와 같은 자원 제약 환경에 모델을 배포하는 데 있어 큰 장벽으로 작용한다.64</li>
<li><strong>학습 데이터 효율성:</strong> DETR의 학습 방식인 이분 매칭은 본질적으로 희소한 감독(sparse supervision)을 제공한다. 즉, 수백 개의 쿼리 중 오직 실제 객체 수만큼의 쿼리만이 긍정적인 학습 신호를 받는다.26 이로 인해 모델이 수렴하기까지 많은 양의 데이터와 긴 학습 시간을 요구하며, 이는 데이터가 부족한 특정 도메인에 적용하기 어렵게 만든다.66</li>
<li><strong>해석 가능성 부족:</strong> 어텐션 맵 시각화를 통해 모델이 이미지의 어느 부분에 집중하는지 어느 정도 파악할 수 있지만 3, 수백만 개의 파라미터가 복잡하게 상호작용하는 트랜스포머의 내부 동작을 완전히 이해하고 디버깅하는 것은 여전히 어려운 과제로 남아있다. 이는 모델의 예측 실패 원인을 분석하거나 신뢰성이 중요한 애플리케이션(예: 자율주행, 의료 영상)에 적용하는 데 걸림돌이 될 수 있다.65</li>
</ul>
<h3>9.2  미래 연구 방향</h3>
<p>이러한 한계들을 극복하기 위해 다음과 같은 연구 방향이 유망하게 탐색되고 있다.</p>
<ul>
<li><strong>모델 경량화 및 하드웨어 가속:</strong> 양자화(Quantization), 가지치기(Pruning), 지식 증류(Knowledge Distillation)와 같은 모델 압축 기술을 DETR 아키텍처에 효과적으로 적용하는 연구가 활발히 진행될 것이다.67 또한, Deformable Attention과 같은 특화된 연산을 효율적으로 처리할 수 있는 전용 하드웨어 가속기(hardware accelerator) 설계는 엣지 컴퓨팅 환경에서의 DETR 배포를 가속화할 것이다.65</li>
<li><strong>학습 효율성 개선:</strong> 희소한 감독 문제를 해결하기 위해, 기존의 일대일(one-to-one) 매칭을 유지하면서 보조적으로 일대다(one-to-many) 매칭을 활용하여 학습 신호를 풍부하게 하는 연구가 유망하다.26 또한, 더 나은 쿼리 초기화 전략이나 매칭 알고리즘을 개발하여 학습 초기의 불안정성을 줄이고 수렴 속도를 높이는 연구(예: DEIM)도 중요한 방향이다.66</li>
<li><strong>개방형 어휘 및 다중모달 탐지:</strong> 현재의 DETR은 사전 정의된 클래스 집합 내에서만 객체를 탐지한다. 미래에는 자연어 설명을 통해 이전에 보지 못한 객체를 탐지하는 개방형 어휘 탐지(Open-vocabulary detection)로의 확장이 중요해질 것이다. 또한, 이미지뿐만 아니라 텍스트, LiDAR, 오디오 등 다양한 모달리티의 정보를 융합하여 더 정확하고 강인한 인식을 수행하는 다중모달(Multi-modal) DETR 연구가 활발히 진행될 것이다.65</li>
<li><strong>기초 모델(Foundation Model)과의 결합:</strong> 대규모 데이터로 사전 학습된 Vision Foundation Model(VFM), 예를 들어 DINOv2, CLIP 등은 강력한 일반화 성능과 풍부한 시각적 표현력을 가지고 있다. 이러한 기초 모델을 DETR의 백본으로 활용하거나, 이들의 특징 표현력을 DETR의 쿼리나 어텐션 메커니즘에 융합하여 탐지 성능을 극대화하는 연구가 새로운 SOTA를 이끌 것으로 기대된다.68 이는 쿼리가 더 풍부하고 일반화된 컨텍스트 위에서 작동하게 하여 학습의 효율성과 최종 성능을 모두 향상시킬 수 있다.</li>
</ul>
<p>결론적으로, DETR은 객체 탐지 분야에 근본적인 혁신을 가져왔으며, 그 발전은 현재진행형이다. 앞으로의 연구는 계산 효율성, 데이터 효율성, 그리고 더 넓은 범위의 인식 능력으로의 확장에 초점을 맞출 것이며, 이는 DETR 패러다임이 컴퓨터 비전의 다양한 영역에서 더욱 핵심적인 역할을 수행하게 만들 것이다.</p>
<h2>10. 참고 자료</h2>
<ol>
<li>End-to-End Object Detection with Transformers, https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf</li>
<li>DEtection TRansformer (DETR) vs. YOLO for object detection. | by Fahim Rustamy, PhD, https://medium.com/@faheemrustamy/detection-transformer-detr-vs-yolo-for-object-detection-baeb3c50bc3</li>
<li>Introduction to DETR - Part 1 | DigitalOcean, https://www.digitalocean.com/community/tutorials/introduction-detr-hungarian-algorithm-1</li>
<li>A Review of DEtection TRansformer: From Basic Architecture to Advanced Developments and Visual Perception Applications - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC12252279/</li>
<li>Introduction to DETR (Detection Transformers): Everything You Need to Know - Lightly AI, https://www.lightly.ai/blog/detr</li>
<li>NAN-DETR: noising multi-anchor makes DETR better for object detection - Frontiers, https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2024.1484088/full</li>
<li>Object Detection with Transformers: A Review - arXiv, https://arxiv.org/html/2306.04670</li>
<li>DETRs Beat YOLOs on Real-time Object Detection - arXiv, https://arxiv.org/html/2304.08069v3</li>
<li>[2005.12872] End-to-End Object Detection with Transformers - arXiv, https://arxiv.org/abs/2005.12872</li>
<li>[R] End-to-End Object Detection with Transformers : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/grbipg/r_endtoend_object_detection_with_transformers/</li>
<li>Transform Object Detection with DETR Simplified - Viso Suite, https://viso.ai/deep-learning/detr-end-to-end-object-detection-with-transformers/</li>
<li>DETR Breakdown Part 1: Introduction to DEtection TRansformers - PyImageSearch, https://pyimagesearch.com/2023/05/22/detr-breakdown-part-1-introduction-to-detection-transformers/</li>
<li>What is DETR (Detection Transformers)? - Roboflow Blog, https://blog.roboflow.com/what-is-detr/</li>
<li>What is Detection Transformers (DETR)? - Zilliz Learn, https://zilliz.com/learn/detection-transformers-detr-end-to-end-object-detection-with-transformers</li>
<li>Papers Explained 79: DETR. DEtection TRansformer or DETR… | by …, https://ritvik19.medium.com/papers-explained-79-detr-bcdd53355d9f</li>
<li>DETR: Overview and Inference - LearnOpenCV, https://learnopencv.com/detr-overview-and-inference/</li>
<li>Review — DETR: End-to-End Object Detection with Transformers | by Sik-Ho Tsang, https://sh-tsang.medium.com/review-detr-end-to-end-object-detection-with-transformers-c64977be4b8e</li>
<li>DETR - Hugging Face, https://huggingface.co/docs/transformers/main/model_doc/detr</li>
<li>DETR:End-to-End Object Detection with Transformers | Learning-Deep-Learning, https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/detr.html</li>
<li>VachanVY/DETR: Simplifying Object detection. DEtection … - GitHub, https://github.com/VachanVY/DETR</li>
<li>DETR Explained | End-to-End Object Detection with Transformers | DETR Tutorial Part 1, https://www.youtube.com/watch?v=v900ZFKkWxA</li>
<li>Detr Resnet 50 Panoptic · Models - Dataloop, https://dataloop.ai/library/model/facebook_detr-resnet-50-panoptic/</li>
<li>DETR Breakdown Part 2: Methodologies and Algorithms - PyImageSearch, https://pyimagesearch.com/2023/06/12/detr-breakdown-part-2-methodologies-and-algorithms/</li>
<li>Introduction to DETR - Part 2: The Crucial Role of the Hungarian Algorithm | DigitalOcean, https://www.digitalocean.com/community/tutorials/introduction-detr-hungarian-algorithm-2</li>
<li>facebookresearch/detr: End-to-End Object Detection with Transformers - GitHub, https://github.com/facebookresearch/detr</li>
<li>Mr. DETR: Instructive Multi-Route Training for Detection Transformers - arXiv, https://arxiv.org/html/2412.10028v2</li>
<li>DAC-DETR: Divide the Attention Layers and Conquer, https://proceedings.neurips.cc/paper_files/paper/2023/file/edd0d433f8a1a51aa11237a6543fc280-Paper-Conference.pdf</li>
<li>Baidu’s RT-DETR: A Vision Transformer-Based Real-Time Object Detector - Ultralytics Docs, https://docs.ultralytics.com/models/rtdetr/</li>
<li>Accelerating DETR Convergence via Semantic-Aligned Matching - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Accelerating_DETR_Convergence_via_Semantic-Aligned_Matching_CVPR_2022_paper.pdf</li>
<li>Deformable DETR: Deformable Transformers for End-to-End Object …, https://arxiv.org/pdf/2010.04159</li>
<li>Deformable DETR: Deformable Transformers for End-to-End Object Detection | OpenReview, https://openreview.net/forum?id=gZ9hCDWe6ke</li>
<li>The analysis for the root of DETR’s slow convergence. Left - ResearchGate, https://www.researchgate.net/figure/The-analysis-for-the-root-of-DETRs-slow-convergence-Left-The-cross-attention-module-in_fig1_362323378</li>
<li>Align-DETR: Enhancing End-to-end Object Detection with Aligned Loss - arXiv, https://arxiv.org/html/2304.07527v2</li>
<li>Deformable DETR: Deformable Transformers for End-to-End Object Detection - arXiv, https://arxiv.org/abs/2010.04159</li>
<li>[2111.14330] Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity, https://arxiv.org/abs/2111.14330</li>
<li>Deformable DETR: Deformable Transformers for End-to-End Object Detection | Request PDF - ResearchGate, https://www.researchgate.net/publication/344551949_Deformable_DETR_Deformable_Transformers_for_End-to-End_Object_Detection</li>
<li>[R] Deformable DETR: Deformable Transformers for End-to-End Object Detection - Reddit, https://www.reddit.com/r/MachineLearning/comments/j7u4t4/r_deformable_detr_deformable_transformers_for/</li>
<li>Conditional DETR for Fast Training Convergence - arXiv, https://arxiv.org/html/2108.06152</li>
<li>[2108.06152] Conditional DETR for Fast Training Convergence - arXiv, https://arxiv.org/abs/2108.06152</li>
<li>arXiv:2108.06152v1 [cs.CV] 13 Aug 2021, https://arxiv.org/pdf/2108.06152</li>
<li>DINO: DETR WITH IMPROVED DENOISING ANCHOR - OpenReview, https://openreview.net/pdf?id=3mRwyG5one</li>
<li>DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection, https://arxiv.org/abs/2203.03605</li>
<li>Semi-DETR: Semi-Supervised Object Detection With Detection Transformers - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Semi-DETR_Semi-Supervised_Object_Detection_With_Detection_Transformers_CVPR_2023_paper.pdf</li>
<li>IDEA-Research/DINO: [ICLR 2023] Official implementation … - GitHub, https://github.com/IDEA-Research/DINO</li>
<li>RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer, https://arxiv.org/html/2407.17140v1</li>
<li>Implementing Baidu’s RT-DETR for Real-Time Object Detection | DigitalOcean, https://www.digitalocean.com/community/tutorials/rt-detr-realtime-detection-transformer</li>
<li>RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time …, https://arxiv.org/pdf/2407.17140</li>
<li>RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer, https://www.researchgate.net/publication/382526773_RT-DETRv2_Improved_Baseline_with_Bag-of-Freebies_for_Real-Time_Detection_Transformer</li>
<li>DETR for panoptic segmentation - algorithm and code reading - YouTube, https://www.youtube.com/watch?v=zc9aaTINDk4</li>
<li>Mask DINO: Towards a Unified Transformer-Based Framework for Object Detection and Segmentation - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Mask_DINO_Towards_a_Unified_Transformer-Based_Framework_for_Object_Detection_CVPR_2023_paper.pdf</li>
<li>Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation | Request PDF - ResearchGate, https://www.researchgate.net/publication/361135007_Mask_DINO_Towards_A_Unified_Transformer-based_Framework_for_Object_Detection_and_Segmentation</li>
<li>IDEA-Research/MaskDINO: [CVPR 2023] Official implementation of the paper “Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation” - GitHub, https://github.com/IDEA-Research/MaskDINO</li>
<li>[2110.06922] DETR3D: 3D Object Detection from Multi-view Images …, https://ar5iv.labs.arxiv.org/html/2110.06922</li>
<li>Enhancing 3D Object Detection with 2D Detection-Guided Query Anchors - arXiv, https://arxiv.org/html/2403.06093v1</li>
<li>DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries, https://proceedings.mlr.press/v164/wang22b/wang22b.pdf</li>
<li>End-to-End Multi-Person Pose Estimation With … - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Shi_End-to-End_Multi-Person_Pose_Estimation_With_Transformers_CVPR_2022_paper.pdf</li>
<li>DETRPose: Real-time end-to-end transformer model for multi-person pose estimation, https://www.researchgate.net/publication/392736546_DETRPose_Real-time_end-to-end_transformer_model_for_multi-person_pose_estimation</li>
<li>DETRs Beat YOLOs on Real-time Object Detection - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_DETRs_Beat_YOLOs_on_Real-time_Object_Detection_CVPR_2024_paper.pdf</li>
<li>RT-DETRv2 Beats YOLO? Full Comparison + Tutorial - Labellerr, https://www.labellerr.com/blog/rt-detrv2-beats-yolo-full-comparison-tutorial/</li>
<li>Performance Benchmark of YOLO v5, v7 and v8 - Stereolabs, https://www.stereolabs.com/blog/performance-of-yolo-v5-v7-and-v8</li>
<li>WongKinYiu/yolov9: Implementation of paper - YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information - GitHub, https://github.com/WongKinYiu/yolov9</li>
<li>YOLOv9 vs. RTDETRv2: A Technical Comparison for Object Detection - Ultralytics Docs, https://docs.ultralytics.com/compare/yolov9-vs-rtdetr/</li>
<li>RTDETRv2 vs YOLOv8: A Technical Comparison - Ultralytics YOLO Docs, https://docs.ultralytics.com/compare/rtdetr-vs-yolov8/</li>
<li>What is DEtection TRansformer (DETR)? - Educative.io, https://www.educative.io/answers/what-is-detection-transformer-detr</li>
<li>A Survey of Vision Transformers in Autonomous Driving: Current Trends and Future Directions - arXiv, https://arxiv.org/html/2403.07542v1</li>
<li>DEIM: DETR with Improved Matching for Fast Convergence - arXiv, https://arxiv.org/html/2412.04234v1</li>
<li>Fisher-aware Quantization for DETR Detectors with Critical-category Objectives - arXiv, https://arxiv.org/html/2407.03442v1</li>
<li>Enhancing DETR with Image Understanding from Frozen Foundation Models - arXiv, https://arxiv.org/html/2410.19635v1</li>
<li>RF-DETR Object Detection vs YOLOv12 : A Study of Transformer-based and CNN-based Architectures for Single-Class and Multi-Class Greenfruit Detection in Complex Orchard Environments Under Label Ambiguity - arXiv, https://arxiv.org/html/2504.13099v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>