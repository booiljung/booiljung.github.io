<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:MM-Grounding-DINO (2024-01) 통합 객체 그라운딩 및 탐지를 위한 개방형 파이프라인</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>MM-Grounding-DINO (2024-01) 통합 객체 그라운딩 및 탐지를 위한 개방형 파이프라인</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>MM-Grounding-DINO (2024-01) 통합 객체 그라운딩 및 탐지를 위한 개방형 파이프라인</span></nav>
                </div>
            </header>
            <article>
                <h1>MM-Grounding-DINO (2024-01) 통합 객체 그라운딩 및 탐지를 위한 개방형 파이프라인</h1>
<h2>1.  Open-Set 객체 탐지의 패러다임과 MM-Grounding-DINO의 등장 배경</h2>
<h3>1.1  폐쇄형(Closed-Set) 분류를 넘어서: Open-Vocabulary Detection의 정의와 과제</h3>
<p>전통적인 컴퓨터 비전의 객체 탐지 모델들은 사전 정의된 고정된 클래스 목록에 대해서만 학습하고 추론할 수 있는 ‘폐쇄형(closed-set)’ 시스템의 본질적 한계를 내포한다.1 이러한 모델들은 학습 데이터에 존재하는 범주 외의 새로운 객체를 인식하지 못하며, 이는 실제 세계의 무한하고 동적인 시각적 개념을 처리하는 데 있어 결정적인 제약으로 작용한다. 이러한 한계를 극복하기 위해, 텍스트 입력을 통해 한 번도 본 적 없는(unseen) 객체를 탐지하는 ‘개방형(open-set)’ 패러다임이 대두되었다.</p>
<p>이 패러다임은 세 가지 핵심 과제로 세분화된다: Open-Vocabulary Detection (OVD), Phrase Grounding (PG), 그리고 Referring Expression Comprehension (REC).4 OVD는 임의의 단어나 어휘로 표현된 객체를 탐지하는 과제이며, PG는 이미지 내 특정 영역을 해당 영역을 설명하는 구문(phrase)과 연결하는 것을 목표로 한다. REC는 더 나아가, “빨간 셔츠를 입은 사람“과 같이 속성이나 관계를 포함하는 복잡한 설명문을 이해하고 해당하는 객체를 정확히 찾아내는 고차원적인 능력을 요구한다.3 이 과제들은 모델이 단순한 시각적 패턴 인식을 넘어, 유연한 자연어 표현을 깊이 있게 이해하고 이를 시각적 세계의 구체적인 객체와 연결(grounding)하는 고도의 다중양식(multi-modal) 이해 능력을 갖추어야 함을 시사한다.4</p>
<h3>1.2  비전-언어 융합의 초석: Grounding DINO의 기술적 성취</h3>
<p>이러한 개방형 객체 탐지의 난제를 해결하기 위한 연구 흐름 속에서, Grounding DINO는 기념비적인 성취를 이룬 핵심 모델로 평가받는다.1 이 모델은 Transformer 기반의 최첨단 폐쇄형 탐지기인 DINO (DETR with Improved DeNoising Anchor Boxes)의 구조적 강점과, 대규모 이미지-텍스트 쌍 데이터를 활용하는 Grounded Pre-Training 방법론을 성공적으로 결합했다.6</p>
<p>Grounding DINO의 핵심적인 기술적 혁신은 시각과 언어 양식을 여러 단계에 걸쳐 긴밀하게 융합(tight fusion)하는 아키텍처에 있다.6 이 아키텍처는 (1) 이미지와 텍스트 특징을 상호 보강하는 Feature Enhancer, (2) 텍스트 정보를 기반으로 탐지할 객체 후보 쿼리를 동적으로 선택하는 Language-Guided Query Selection, (3) 두 양식의 정보를 최종적으로 결합하여 예측을 정제하는 Cross-Modality Decoder로 구성된다.4 이 다단계 융합 전략을 통해 Grounding DINO는 단순한 카테고리 이름부터 복잡한 설명구에 이르기까지, 임의의 텍스트 입력을 받아 해당 객체를 정확하게 탐지하는 능력을 입증했다. 특히, COCO 학습 데이터를 전혀 사용하지 않은 Zero-shot transfer 평가 환경에서 52.5 mAP라는 경이로운 성능을 달성하며 개방형 객체 탐지 분야의 새로운 SOTA(State-of-the-Art) 모델로 자리매김했다.4</p>
<h3>1.3  오픈소스의 필요성: MM-Grounding-DINO 개발의 핵심 동기</h3>
<p>Grounding DINO의 탁월한 성능과 학계 및 산업계에 미친 막대한 영향력에도 불구하고, 이 모델은 치명적인 한계를 가지고 있었다. 바로 학습 코드와 전체 파이프라인이 공개되지 않았다는 점이다.4 공개된 것은 추론 및 데모 코드에 불과하여, 연구 커뮤니티는 이 모델의 성능을 재현하거나, 다른 데이터셋에 적용하여 미세조정하거나, 아키텍처를 개선하는 등의 후속 연구를 수행하는 것이 사실상 불가능했다.5</p>
<p>이러한 상황은 Grounding DINO의 성공이 역설적으로 커뮤니티의 발전을 저해하는 병목 현상을 초래하는 ’재현성의 위기’를 야기했다. 연구의 흐름이 모델을 ’사용’하는 것에만 국한되고, 그 원리를 깊이 탐구하고 ’발전’시키는 방향으로 나아가지 못하게 된 것이다. 이러한 학문적 정체를 타개하고, Grounding DINO 아키텍처를 단순한 ’제품’이 아닌 커뮤니티가 함께 발전시킬 수 있는 ’플랫폼’으로 전환시키기 위한 목적으로 MM-Grounding-DINO가 개발되었다. OpenMMLab 프로젝트의 MMDetection 툴박스를 기반으로 구축된 MM-Grounding-DINO는 단순한 성능 복제를 넘어, 완전한 오픈소스 학습, 미세조정, 평가 파이프라인을 제공함으로써 해당 기술의 생명력을 연장하고 더 큰 학문적, 산업적 파급 효과를 창출하는 것을 목표로 한다.4 이는 현대 AI 연구에서 SOTA 모델의 발표가 논문과 추론 코드를 넘어, 학습 파이프라인 전체를 공개하는 것이 어떻게 학문적 진보를 가속화하는지에 대한 구체적인 증거를 제시한다.</p>
<h2>2.  아키텍처 심층 분석</h2>
<h3>2.1  핵심 프레임워크: 이중 인코더와 교차 양식 디코더 구조</h3>
<p>MM-Grounding-DINO는 원본 Grounding DINO의 검증된 핵심 아키텍처를 충실히 계승한다. 이 구조는 크게 세 부분으로 구성된다: (1) 이미지로부터 시각적 특징을 추출하는 이미지 백본(Image Backbone), (2) 텍스트 프롬프트로부터 언어적 의미를 추출하는 텍스트 백본(Text Backbone)의 이중 인코더 구조, 그리고 (3) 두 양식에서 추출된 정보를 통합하여 최종적인 객체 예측을 생성하는 단일 교차 양식 디코더(Cross-Modality Decoder)이다.10</p>
<p>이 모델은 입력된 이미지와 텍스트로부터 각각 고유한 특징 벡터를 생성한 후, 여러 단계에 걸친 정교한 융합 모듈을 통과시킨다. 이 과정을 통해 최종적으로 이미지 내 특정 객체의 위치(bounding box)와 그 객체가 입력된 텍스트와 얼마나 강하게 연관되어 있는지를 예측한다. 특히, DETR(DEtection TRansformer) 기반 아키텍처를 채택함으로써, Non-Maximum Suppression (NMS)과 같이 경험적으로 설계된 후처리 모듈 없이도 End-to-End 학습 및 추론이 가능하다는 장점을 갖는다.3</p>
<h3>2.2  시각 및 언어 특징 추출: Swin Transformer와 BERT 백본</h3>
<p>MM-Grounding-DINO의 성능은 강력한 특징 추출 능력에 크게 의존하며, 이를 위해 각 양식에서 SOTA 수준의 백본 모델을 사용한다.</p>
<ul>
<li><strong>Image Backbone:</strong> 시각 특징 추출기로는 주로 Swin Transformer가 사용된다. Swin Transformer는 이동된 윈도우(shifted windows) 기반의 자기 어텐션 메커니즘을 통해 이미지로부터 효율적으로 계층적인(hierarchical) 특징 맵을 생성한다. 모델의 복잡도와 성능 요구사항에 따라 Swin-Tiny (Swin-T), Swin-Base (Swin-B), Swin-Large (Swin-L) 등 다양한 크기의 변형이 사용된다.10</li>
<li><strong>Text Backbone:</strong> 텍스트 프롬프트로부터 풍부한 언어적, 의미적 특징을 추출하기 위해 BERT(Bidirectional Encoder Representations from Transformers)와 같은 검증된 Transformer 기반 언어 모델이 텍스트 백본으로 활용된다.10 이를 통해 모델은 단어의 의미뿐만 아니라 문장 전체의 문맥적 뉘앙스까지 포착할 수 있다.</li>
</ul>
<h3>2.3  융합 엔진: Feature Enhancer와 Language-Guided Query Selection</h3>
<p>추출된 시각 및 언어 특징을 효과적으로 융합하는 것은 모델의 핵심 성능을 좌우한다. MM-Grounding-DINO는 이를 위해 다단계 융합 엔진을 채택한다.</p>
<ul>
<li><strong>Feature Enhancer:</strong> 이 모듈은 이미지 특징과 텍스트 특징 간의 초기 융합을 담당한다. 양방향 교차 어텐션(bidirectional cross-attention)을 통해 이미지 특징 토큰과 텍스트 특징 토큰이 서로의 정보를 교환하고 보강하도록 설계되었다. 구체적으로, 이미지-텍스트 교차 어텐션은 시각적 특징에 언어적 맥락을 부여하고, 텍스트-이미지 교차 어텐션은 언어적 특징에 시각적 근거를 제공한다. 이 과정을 거치면서 각 양식의 특징은 서로에게 ’접지(grounded)’된다.2</li>
<li><strong>Language-Guided Query Selection:</strong> 이 모듈은 Grounding DINO 아키텍처의 가장 혁신적인 부분 중 하나이다. 기존 DETR 계열 모델들이 학습 가능한 고정된 객체 쿼리(object query)를 사용하는 것과 달리, 이 모듈은 텍스트 프롬프트와 가장 의미적으로 유사한 이미지 특징들을 동적으로 선택하여 디코더의 초기 쿼리로 활용한다.2 이는 탐지 프로세스 전체를 사용자의 언어적 의도에 맞게 능동적으로 ’조종’하는 역할을 하여, 불필요한 영역에 대한 계산을 줄이고 목표 객체에 집중함으로써 탐지의 효율성과 정확성을 극대화한다.2</li>
<li><strong>Cross-Modality Decoder:</strong> Language-Guided Query Selection을 통해 선택된 쿼리들은 교차 양식 디코더에 입력된다. 디코더의 각 레이어는 자기 어텐션(self-attention), 이미지 교차 어텐션(image cross-attention), 그리고 텍스트 교차 어텐션(text cross-attention) 블록으로 구성된다. 쿼리들은 이 레이어들을 반복적으로 통과하면서 객체의 위치를 더욱 정밀하게 예측하고, 텍스트와의 의미적 일치도를 높이는 방향으로 정제된다.3</li>
</ul>
<h3>2.4  MM-Grounding-DINO의 개선점: 성능 향상을 위한 아키텍처 미세 조정</h3>
<p>MM-Grounding-DINO는 원본 아키텍처를 단순히 복제하는 데 그치지 않고, 경험적 분석을 통해 성능을 한 단계 끌어올리기 위한 두 가지 핵심적인 구조적 개선을 도입했다.17</p>
<ol>
<li><strong>개선된 대조적 클래스 헤드 (Improved Contrastive Class Head):</strong> 최종적으로 각 객체 쿼리가 어떤 텍스트 구문과 일치하는지 판별하는 분류 헤드의 구조를 개선했다. 이를 통해 시각적 특징과 텍스트 임베딩 간의 대조 학습(contrastive learning)이 더 효과적으로 이루어지도록 하여, 미묘한 의미 차이를 더 잘 구분할 수 있게 되었다.</li>
<li><strong>디코더 내 파라미터 공유 제거 (Removing Parameter Sharing in the Decoder):</strong> 원본 DINO 모델은 효율성을 위해 디코더의 여러 레이어가 파라미터를 공유하는 구조를 가졌다. MM-Grounding-DINO는 이 파라미터 공유를 제거했다. 파라미터 공유는 모델 크기를 줄이는 장점이 있지만, 각 디코더 레이어가 독립적이고 전문화된 기능을 학습하는 것을 방해할 수 있다. 파라미터 공유를 제거함으로써 모델의 전체적인 표현 용량(capacity)이 증가하고, 각 레이어가 예측을 점진적으로 정제하는 과정에서 더 세분화되고 복잡한 변환을 학습할 수 있게 되었다.</li>
</ol>
<p>이러한 아키텍처의 미세 조정은 단순히 더 많은 데이터를 사용한 결과 이상의 성능 향상을 가져왔다. 특히, LVIS와 같이 수많은 클래스와 희귀 객체가 포함된 까다로운 long-tail 벤치마크에서 관찰된 큰 폭의 성능 향상은, 디코더의 표현력 증가가 복잡한 시각적-언어적 관계를 학습하는 데 결정적인 역할을 했음을 시사한다. 이는 데이터 전략뿐만 아니라, 아키텍처 설계의 전략적 선택이 SOTA 성능 달성에 얼마나 중요한지를 보여주는 사례이다.17</p>
<h2>3.  데이터 큐레이션 및 학습 방법론</h2>
<h3>3.1  견고성 확보를 위한 다중 데이터셋 사전학습 전략</h3>
<p>MM-Grounding-DINO의 뛰어난 일반화 성능은 다양한 유형의 대규모 데이터셋을 체계적으로 조합하여 사전학습을 진행한 전략에 기인한다.4 이 전략은 모델이 특정 데이터셋의 편향에 과적합되는 것을 방지하고, 다양한 시나리오와 주석(annotation) 형태에 대해 견고한 성능을 발휘하도록 설계되었다.</p>
<p>주요 사전학습 데이터셋은 다음과 같은 세 가지 유형으로 분류할 수 있다 4:</p>
<ul>
<li><strong>객체 탐지 데이터셋 (Object Detection Datasets):</strong></li>
<li><strong>Objects365:</strong> 365개의 카테고리와 60만 개 이상의 이미지를 포함하는 대규모 객체 탐지 데이터셋으로, 모델에 기본적인 객체 인식 능력을 부여한다.4</li>
<li><strong>V3Det:</strong> 13,000개 이상의 방대한 어휘(vast vocabulary)를 가진 데이터셋으로, 모델이 희귀하거나 생소한 객체에 대한 개념을 학습하는 데 결정적인 역할을 한다.4</li>
<li><strong>그라운딩 데이터셋 (Grounding Datasets):</strong></li>
<li><strong>GoldG:</strong> GQA와 Flickr30k Entities 데이터셋의 조합으로, 이미지 내 특정 영역과 이를 설명하는 자연어 구문을 연결하는 주석을 제공한다. 이는 모델의 Phrase Grounding 및 REC 능력을 직접적으로 향상시킨다.4</li>
<li><strong>RefCOCO/+/g:</strong> REC 작업을 위해 특별히 구축된 데이터셋으로, 모델이 복잡한 지시문을 이해하고 해당 객체를 찾는 능력을 미세하게 조정하는 데 사용된다.4</li>
<li><strong>이미지-텍스트 쌍 데이터셋 (Image-Text Pair Datasets):</strong></li>
<li><strong>GRIT:</strong> 대규모 이미지와 그에 대한 캡션 쌍으로 구성된 데이터셋으로, 모델이 시각적 내용과 언어적 설명 간의 전반적인 의미적 연관성을 학습하도록 돕는다.4</li>
</ul>
<p>특히, 원본 Grounding DINO가 사용했던 비공개 데이터셋인 Cap4M을 대체하기 위해, 공개적으로 사용 가능한 GRIT과 V3Det을 전략적으로 채택한 것은 MM-Grounding-DINO의 완전한 재현성과 개방성을 확보하는 데 중요한 결정이었다.4 MMDetection 프레임워크는 이러한 이질적인 데이터셋들을 ODVG (Open-Vocabulary Detection in a Generic format)라는 통합된 형식으로 변환하고 관리하는 효율적인 파이프라인을 제공하여, 복잡한 다중 데이터셋 학습 과정을 단순화했다.8 이처럼 ‘더 많은’ 데이터가 아닌 ‘더 다양한 유형의’ 데이터를 다각적으로 학습시킨 것이 모델의 견고성과 일반화 성능을 극대화하는 핵심 기제이다.</p>
<h3>3.2  손실 함수 삼중주: 위치 측정과 대조적 그라운딩의 결합</h3>
<p>MM-Grounding-DINO의 학습 목표는 객체의 위치를 정확히 예측하는 것과, 예측된 객체를 올바른 텍스트 설명과 연결하는 두 가지 과제를 동시에 최적화하는 것이다. 이를 위해 DETR의 표준 손실 함수와 대조적 손실 함수를 결합한 복합 손실 함수를 사용한다.2</p>
<ul>
<li>
<p><strong>위치 손실 (Localization Loss):</strong> 예측된 바운딩 박스와 실제(ground-truth) 바운딩 박스 간의 기하학적 차이를 최소화하기 위한 손실이다. 이는 두 가지 요소로 구성된다.</p>
</li>
<li>
<p><strong>L1 Loss:</strong> 박스의 중심 좌표와 너비, 높이 간의 절대적인 차이를 계산한다.</p>
</li>
<li>
<p><strong>GIoU (Generalized Intersection over Union) Loss:</strong> 두 박스 간의 IoU뿐만 아니라, 박스들의 상대적인 위치 관계까지 고려하여 더 안정적인 학습을 유도한다.</p>
</li>
<li>
<p>최종 위치 손실은 이 두 손실의 가중합으로 계산된다:</p>
<p><span class="math math-display">
\mathcal{L}_{\text{loc}} = \lambda_{\text{L1}} \mathcal{L}_{\text{L1}} + \lambda_{\text{GIoU}} \mathcal{L}_{\text{GIoU}}
</span></p>
</li>
</ul>
<p>여기서 <span class="math math-inline">\lambda_{\text{L1}}</span>과 <span class="math math-inline">\lambda_{\text{GIoU}}</span>는 각 손실의 중요도를 조절하는 하이퍼파라미터이다.2</p>
<ul>
<li><strong>대조적 그라운딩 손실 (Contrastive Grounding Loss):</strong> 이 손실은 각 객체 쿼리(시각적 특징)가 매칭되는 올바른 텍스트 구(sub-sentence) 임베딩과는 유사도를 높이고(positive pair), 매칭되지 않는 다른 텍스트 구 임베딩과는 유사도를 낮추도록(negative pairs) 학습하는 대조적 학습(contrastive learning) 방식으로 설계되었다. 학습의 안정성과 효율성을 높이기 위해 초점 손실(Focal Loss)의 변형이 적용된다.2</li>
</ul>
<p>이 두 가지 손실 함수의 조합을 통해, 모델은 ’무엇(what)’을 탐지해야 하는지와 ‘어디에(where)’ 위치시켜야 하는지를 통합적으로 학습하게 된다.</p>
<h3>3.3  MMDetection 프레임워크 내에서의 미세조정 전략</h3>
<p>MM-Grounding-DINO의 가장 큰 장점 중 하나는 MMDetection 프레임워크를 통해 체계적이고 유연한 미세조정(fine-tuning)을 지원한다는 것이다. MMDetection은 Grounding DINO의 미세조정을 공식적으로 지원하는 유일한 라이브러리로 알려져 있다.13 사용자는 다운스트림 작업의 특성에 따라 다음과 같은 세 가지 주요 미세조정 모드 중 하나를 선택할 수 있다 21:</p>
<ol>
<li><strong>폐쇄형 미세조정 (Closed-set fine-tuning):</strong> COCO와 같은 특정 데이터셋에 대해 모델의 성능을 극대화하는 방식이다. 이 경우, 텍스트 프롬프트는 학습 데이터셋의 클래스 이름으로 고정되며, 모델은 해당 도메인에 특화된 고성능 탐지기로 변환된다. 일반화 능력은 일부 상실될 수 있다.</li>
<li><strong>개방형 지속 사전학습 (Open-set continued pretraining):</strong> 사전학습과 동일한 학습 방식을 유지하면서 목표 데이터셋(예: COCO)을 추가로 학습하는 방식이다. 기존 사전학습 데이터와 목표 데이터를 혼합하거나, 낮은 학습률로 목표 데이터에 대해서만 추가 학습을 진행할 수 있다. 이는 모델의 일반화 성능 저하를 최소화하면서 목표 데이터셋에 대한 성능을 향상시키는 것을 목표로 한다.</li>
<li><strong>개방형 어휘 미세조정 (Open-vocabulary fine-tuning):</strong> OVD 분야의 표준적인 평가 프로토콜을 따르는 방식이다. 데이터셋의 클래스를 base 클래스와 novel 클래스로 분리한 후, 오직 base 클래스 데이터로만 미세조정을 진행한다. 평가는 base와 novel 클래스 모두에 대해 수행하여, 모델이 학습 중에 보지 못한 novel 클래스를 얼마나 잘 일반화하여 탐지하는지를 측정한다.</li>
</ol>
<p>이러한 체계적인 미세조정 옵션들은 거대한 Foundation Model을 다양한 실제 응용 분야에 효과적으로 적용하는 방법에 대한 구체적인 청사진을 제시하며, 연구와 산업 적용 간의 간극을 줄이는 데 결정적인 역할을 한다.</p>
<h2>4.  실증적 성능 분석 및 벤치마킹</h2>
<p>MM-Grounding-DINO의 성능은 다수의 표준 벤치마크에서 원본 Grounding DINO 및 다른 SOTA 모델들과 비교하여 실증적으로 검증되었다. 특히, COCO와 LVIS 데이터셋에서의 Zero-shot transfer 성능은 이 모델의 핵심적인 우수성을 입증한다.</p>
<h3>4.1  COCO 벤치마크에서의 Zero-Shot Transfer 성능</h3>
<p>COCO는 객체 탐지 분야에서 가장 널리 사용되는 표준 벤치마크이다. 이 데이터셋에 대해 전혀 학습하지 않은 상태에서 모델의 성능을 평가하는 Zero-shot transfer 실험은 모델의 일반화 능력을 측정하는 중요한 척도이다. MM-Grounding-DINO는 다양한 데이터 조합과 모델 크기에서 일관되게 원본 모델을 능가하는 성능을 보였다.4</p>
<p>아래 표 1은 COCO 벤치마크에서의 Zero-shot 성능을 요약한 것이다. O365, GoldG, V3Det 데이터로 사전학습된 MM-Grounding-DINO-Tiny 모델은 50.6 mAP를 달성하여, 비공개 데이터셋(Cap4M)을 제외한 원본 모델의 성능을 2.2 mAP 포인트 상회했다.17 더 큰 Swin-B 및 Swin-L 백본을 사용하고 모든 가용 데이터를 활용한 Base 및 Large 모델은 각각 59.5 mAP와 60.3 mAP라는 더욱 인상적인 성능을 기록했다. 이는 모델의 크기와 사전학습 데이터의 양과 질이 성능에 직접적인 긍정적 영향을 미침을 명확히 보여준다.17</p>
<table><thead><tr><th>모델</th><th>백본</th><th>사전학습 데이터</th><th>COCO mAP (Zero-shot)</th></tr></thead><tbody>
<tr><td>Grounding-DINO-Tiny (Official)</td><td>Swin-T</td><td>O365, GoldG, Cap4M</td><td>48.4</td></tr>
<tr><td>MM-Grounding-DINO-Tiny</td><td>Swin-T</td><td>O365, GoldG</td><td>50.4 (+2.0 vs GDINO-T w/o Cap4M)</td></tr>
<tr><td>MM-Grounding-DINO-Tiny</td><td>Swin-T</td><td>O365, GoldG, V3Det</td><td>50.6 (+2.2 vs GDINO-T w/o Cap4M)</td></tr>
<tr><td>MM-Grounding-DINO-Base</td><td>Swin-B</td><td>O365, GoldG, V3Det</td><td>52.5</td></tr>
<tr><td>MM-Grounding-DINO-Base</td><td>Swin-B</td><td>ALL</td><td>59.5</td></tr>
<tr><td>MM-Grounding-DINO-Large</td><td>Swin-L</td><td>O365V2, OpenImageV6, GoldG</td><td>53.0</td></tr>
<tr><td>MM-Grounding-DINO-Large</td><td>Swin-L</td><td>ALL</td><td>60.3</td></tr>
</tbody></table>
<h3>4.2  LVIS 벤치마크에서의 Long-Tail 인식 성능 평가</h3>
<p>LVIS 벤치마크는 1,000개 이상의 클래스가 희귀(rare), 보통(common), 빈번(frequent)하게 나타나는 극심한 불균형 분포, 즉 long-tail 분포를 가지고 있다. 이는 실제 세계의 데이터 분포와 유사하여, 모델의 실질적인 일반화 성능과 희귀 객체 인식 능력을 평가하는 데 더욱 적합한 벤치마크로 간주된다.</p>
<p>MM-Grounding-DINO는 LVIS 벤치마크에서 특히 압도적인 성능 향상을 보였다. 이는 COCO에서의 소폭 성능 향상과 뚜렷한 대조를 이룬다. 표 2에서 볼 수 있듯이, O365, GoldG, GRIT, V3Det 데이터로 학습한 MM-Grounding-DINO-Tiny 모델은 LVIS MiniVal에서 41.4 AP, Val1.0에서 31.9 AP를 달성했다. 이는 원본 모델 대비 각각 +12.6 AP, +11.8 AP라는 경이적인 개선 수치이다.17</p>
<p>이러한 극적인 성능 향상의 원인은 MM-Grounding-DINO의 데이터 전략에서 찾을 수 있다. COCO와 달리 LVIS의 핵심 과제는 ’희귀 객체 인식’이며, MM-Grounding-DINO가 추가로 사용한 V3Det과 같은 방대한 어휘 데이터셋은 모델이 희귀하거나 처음 보는 개념에 대한 일반화 능력을 키우는 데 직접적으로 기여했다. 특히 희귀 클래스에 대한 성능 지표인 APr(Average Precision for rare categories)이 18.8에서 34.2 (MiniVal 기준)로 대폭 상승한 것은, 다양한 데이터셋을 통한 사전학습이 long-tail 문제 해결에 결정적인 역할을 했음을 시사한다. 따라서 MM-Grounding-DINO의 성공은 단순한 성능 향상을 넘어, 현대 컴퓨터 비전의 난제인 long-tail 문제에 대한 효과적인 해결책을 데이터 전략을 통해 제시했다는 점에서 더 큰 의미를 갖는다.</p>
<table><thead><tr><th>모델 (Tiny Variant)</th><th>사전학습 데이터</th><th>MiniVal AP</th><th>MiniVal APr</th><th>Val1.0 AP</th><th>Val1.0 APr</th></tr></thead><tbody>
<tr><td>Grounding-DINO-Tiny(c)</td><td>O365, GoldG, Cap4M</td><td>28.8</td><td>18.8</td><td>20.1</td><td>10.1</td></tr>
<tr><td>MM-Grounding-DINO-Tiny(b)</td><td>O365, GoldG</td><td>35.7 (+6.9)</td><td>28.1</td><td>27.0 (+6.9)</td><td>17.1</td></tr>
<tr><td>MM-Grounding-DINO-Tiny(c3)</td><td>O365, GoldG, GRIT, V3Det</td><td><strong>41.4 (+12.6)</strong></td><td><strong>34.2</strong></td><td><strong>31.9 (+11.8)</strong></td><td><strong>23.6</strong></td></tr>
</tbody></table>
<h3>4.3  Ablation Studies: 다양한 데이터 조합의 영향 분석</h3>
<p>MM-Grounding-DINO 연구는 다양한 데이터 조합에 따른 성능 변화를 체계적으로 보고함으로써, 각 데이터셋의 기여도를 분석할 수 있는 상세한 소거법 연구(ablation study) 결과를 제공한다. 성능 표를 분석해 보면, 데이터셋을 점진적으로 추가함에 따라 성능이 꾸준히 향상되는 것을 확인할 수 있다.17</p>
<p>예를 들어, Objects365와 GoldG를 기본 조합으로 사용했을 때보다, 대규모 이미지-텍스트 쌍 데이터셋인 GRIT이나 방대한 어휘 데이터셋인 V3Det을 추가했을 때 COCO와 LVIS 모두에서 성능이 더욱 향상되었다. 이는 서로 다른 유형의 데이터(객체 탐지, 그라운딩, 이미지-텍스트 쌍)가 모델의 성능에 상보적으로 기여하며, 특히 다양한 데이터 소스를 통합하는 것이 모델의 일반화 능력을 강화하는 데 매우 효과적임을 실험적으로 증명한다.</p>
<h2>5.  비교 연구: MM-Grounding-DINO vs. Grounding DINO</h2>
<p>MM-Grounding-DINO와 원본 Grounding DINO의 비교는 단순히 성능 수치를 나열하는 것을 넘어, 현대 AI 연구에서 오픈소스와 재현성이 갖는 전략적 가치를 조명한다.</p>
<h3>5.1  정량적 성능 우위 분석</h3>
<p>앞선 4절의 벤치마크 분석에서 명확히 드러났듯이, MM-Grounding-DINO는 정량적 성능 면에서 원본 Grounding DINO를 일관되게, 그리고 LVIS와 같은 특정 벤치마크에서는 압도적으로 능가한다.17 이러한 성능 향상은 단순히 더 많은 데이터를 사용했기 때문만이 아니라, (1) 디코더 파라미터 공유 제거와 같은 아키텍처 미세 조정, (2) Cap4M을 GRIT과 V3Det과 같은 공개 데이터셋으로 대체하면서 얻게 된 어휘력 확장 및 데이터 다양성 확보라는 두 가지 핵심 요소의 시너지 효과로 분석된다.8</p>
<h3>5.2  개방형 학습 파이프라인의 전략적 이점</h3>
<p>수치적 성능 향상을 넘어, MM-Grounding-DINO가 갖는 가장 근본적인 기여는 ’완전한 개방성’에 있다.4 원본 모델의 학습 코드가 비공개였던 것과 달리, MM-Grounding-DINO는 전체 학습 및 평가 파이프라인을 MMDetection 프레임워크 위에서 투명하게 공개한다.15</p>
<p>이러한 개방성은 다음과 같은 전략적 이점을 제공한다:</p>
<ul>
<li><strong>재현성 및 신뢰성 확보:</strong> 모든 연구자는 동일한 조건에서 실험 결과를 재현할 수 있으며, 이는 발표된 성능 수치에 대한 신뢰도를 높인다.</li>
<li><strong>연구의 가속화:</strong> 연구자들은 베이스라인 모델을 기반으로 새로운 데이터셋을 적용하거나, 하이퍼파라미터를 조정하거나, 모델의 특정 모듈을 수정하는 등 다양한 실험을 자유롭게 수행할 수 있다. 이는 새로운 아이디어의 신속한 검증과 모델의 지속적인 개선을 촉진한다.</li>
<li><strong>생태계 확장:</strong> MMDetection이라는 강력하고 모듈화된 프레임워크에 통합됨으로써, 다른 탐지 모델과의 손쉬운 결합이나 공정한 비교 실험이 가능해졌다. 이는 더 넓은 연구 생태계와의 시너지를 창출한다.15</li>
</ul>
<p>결론적으로, MM-Grounding-DINO는 Grounding DINO 아키텍처를 소수의 연구 그룹이 소유한 ’블랙박스’에서 전 세계 연구 커뮤니티가 공유하고 발전시키는 ’공공재’로 전환시켰다.</p>
<h3>5.3  아키텍처 및 데이터 활용의 주요 차이점</h3>
<p>두 모델 간의 핵심적인 차이점은 아래 표 3에 요약되어 있다. 이 표는 두 모델의 기술적, 전략적 차이를 한눈에 파악할 수 있도록 돕는다.</p>
<table><thead><tr><th>특징</th><th>Grounding DINO</th><th>MM-Grounding-DINO</th></tr></thead><tbody>
<tr><td><strong>학습 코드 공개 여부</strong></td><td>비공개</td><td>완전한 오픈소스</td></tr>
<tr><td><strong>개발 프레임워크</strong></td><td>비공개 / 자체 프레임워크</td><td>OpenMMLab MMDetection</td></tr>
<tr><td><strong>미세조정 지원</strong></td><td>공식 지원 없음</td><td>공식 지원 (다중 모드)</td></tr>
<tr><td><strong>주요 아키텍처 변경점</strong></td><td>베이스라인</td><td>개선된 대조적 헤드, 디코더 파라미터 공유 제거</td></tr>
<tr><td><strong>주요 데이터셋</strong></td><td>O365, GoldG, Cap4M (비공개)</td><td>O365, GoldG, GRIT, V3Det (모두 공개)</td></tr>
<tr><td><strong>베이스라인 성능 (Tiny)</strong></td><td>COCO: 48.4 AP, LVIS: 28.8 AP</td><td>COCO: 50.6 AP, LVIS: 41.4 AP</td></tr>
</tbody></table>
<h2>6.  응용 및 생태계 통합</h2>
<p>MM-Grounding-DINO의 강력한 개방형 어휘 탐지 능력은 다양한 실제 응용 분야에 적용될 수 있으며, 다른 Foundation Model과의 결합을 통해 그 가치를 더욱 확장한다.</p>
<h3>6.1  핵심 역량의 실제 적용: OVD, PG, REC</h3>
<p>MM-Grounding-DINO의 핵심 역량은 텍스트를 통해 시각 세계를 이해하고 상호작용하는 능력에 있다. 이는 여러 첨단 기술 분야에서 혁신적인 솔루션을 제공할 잠재력을 가진다.</p>
<ul>
<li><strong>자율 주행 및 로보틱스:</strong> “횡단보도 위에서 자전거를 타는 아이“와 같이 복잡하고 구체적인 시나리오를 텍스트로 입력하여 실시간으로 위험 요소를 감지하고 대응하는 데 활용될 수 있다.24</li>
<li><strong>지능형 이미지 검색:</strong> “파란 하늘 아래 해변에서 뛰노는 강아지“와 같은 자연어 쿼리를 통해 방대한 이미지 데이터베이스에서 원하는 이미지를 정확하게 검색할 수 있다.</li>
<li><strong>의료 영상 분석:</strong> “간의 우엽에 위치한 저에코성 종양“과 같은 전문적인 의학 용어를 포함한 텍스트 프롬프트로 의료 영상에서 특정 병변을 자동으로 탐지하여 진단을 보조할 수 있다.25</li>
</ul>
<h3>6.2  시너지 파이프라인: Segment Anything Model (SAM)과의 결합</h3>
<p>MM-Grounding-DINO의 진정한 파급력은 다른 강력한 Foundation Model과 결합될 때 극대화된다. 특히, Segment Anything Model (SAM)과의 결합은 ’Grounded-SAM’이라는 혁신적인 파이프라인을 탄생시켰다.25</p>
<p>이 파이프라인의 작동 원리는 다음과 같다:</p>
<ol>
<li>사용자는 “고양이“와 같은 텍스트 프롬프트를 입력한다.</li>
<li>Grounding DINO가 이 프롬프트를 받아 이미지 내에서 고양이의 위치를 바운딩 박스로 탐지한다.</li>
<li>이 바운딩 박스는 SAM에 프롬프트로 전달된다.</li>
<li>SAM은 전달받은 바운딩 박스를 기반으로 해당 객체의 정확한 픽셀 단위 마스크를 생성한다.</li>
</ol>
<p>이 ’텍스트 → 박스 → 마스크’로 이어지는 자동화된 워크플로우는 텍스트 입력만으로 원하는 모든 객체를 탐지하고 정밀하게 분할하는 ’Detect and Segment Anything’을 가능하게 한다.29 이는 대화형 이미지 편집, 자동 데이터 주석, 비디오 객체 추적 등 다양한 응용 분야에서 새로운 가능성을 열어준다.10 이 결합은 현대 AI 개발 패러다임이 단일 만능 모델을 추구하는 것에서, 각기 다른 강점을 가진 전문가 모델들이 협력하는 ’모듈화’와 ’구성(Composition)’의 중요성을 명확히 보여준다.</p>
<h3>6.3  연구 가속화: 자동 데이터 주석 도구로서의 활용</h3>
<p>고품질의 대규모 데이터셋을 구축하는 것은 딥러닝 연구 및 개발에서 가장 시간과 비용이 많이 소요되는 과정 중 하나이다. MM-Grounding-DINO는 이 문제를 해결하는 강력한 도구로 활용될 수 있다. 텍스트 프롬프트를 이용해 자동으로 이미지에 바운딩 박스 레이블을 생성하는 ‘자동 주석(Auto-Labeling)’ 기능은 수동 레이블링에 드는 막대한 비용을 획기적으로 절감할 수 있다.1</p>
<p>예를 들어, 실시간 탐지가 중요한 응용 분야에서는 YOLO와 같이 추론 속도가 빠른 모델이 필요하다. 하지만 YOLO 모델을 학습시키기 위해서는 대량의 레이블링된 데이터가 필요하다. 이때, 추론 속도는 느리지만 정확도가 높은 Grounding DINO를 ’교사 모델(teacher model)’로 사용하여 대량의 데이터에 자동으로 레이블을 생성하고, 이 데이터를 ’학생 모델(student model)’인 YOLO를 학습시키는 데 활용할 수 있다.32 이 교사-학생 학습 패러다임은 AI 연구 개발의 전반적인 사이클을 가속화하는 데 크게 기여할 수 있다.</p>
<h2>7.  비판적 평가와 미래 연구 방향</h2>
<p>MM-Grounding-DINO는 개방형 객체 탐지 분야에서 중요한 이정표를 세웠지만, 여전히 해결해야 할 한계점과 미래 연구 과제를 안고 있다.</p>
<h3>7.1  현재의 한계: 의미적 복잡성과 추론 속도의 문제</h3>
<ul>
<li><strong>의미 이해의 한계 (Semantic Comprehension):</strong> Grounding DINO 계열 모델은 텍스트 인코더로 BERT를 사용하는데, 이는 최신 대규모 언어 모델(LLM)에 비해 언어 이해 능력에 한계가 있다. 이로 인해, 복잡한 문장 구조, 다중 객체 간의 미묘한 관계, 부정문, 혹은 추상적인 개념을 완벽하게 이해하고 시각적으로 그라운딩하는 데 어려움을 겪는다. 예를 들어, “개가 아닌 고양이“와 같은 부정문을 처리하거나, “테이블 위의 컵 왼쪽의 포크“처럼 정밀한 공간 관계를 추론하는 데 실패하는 경우가 발생할 수 있다.34</li>
<li><strong>추론 속도 (Inference Speed):</strong> Transformer 기반의 복잡한 아키텍처로 인해, YOLO와 같은 경량화된 CNN 기반 모델에 비해 추론 속도가 현저히 느리다. 이는 실시간 비디오 분석이나 자율 주행과 같이 즉각적인 반응이 요구되는 응용 분야에 직접적으로 적용하기 어렵게 만드는 주요 제약 조건이다.3</li>
<li><strong>특수 도메인에서의 성능 저하 (Domain-Specific Challenges):</strong> 의료 영상이나 위성 이미지와 같은 고도로 전문화된 도메인에서는 일반적인 데이터로 사전학습된 모델의 Zero-shot 성능이 기대에 미치지 못할 수 있다. 해당 도메인의 고유한 시각적 특성과 용어에 대한 데이터가 부족하기 때문에, 모델이 새로운 도메인에 효과적으로 일반화되지 못하는 문제가 발생한다.26</li>
</ul>
<h3>7.2  나아갈 길: LMMs와의 통합 및 실시간 탐지 기술 발전</h3>
<p>이러한 한계점들은 역설적으로 차세대 비전-언어 모델이 나아가야 할 방향을 명확히 제시한다. 현재의 한계는 단순한 ’시각 지각(visual perception)’의 문제를 넘어, ’언어 이해(language understanding)’와 ’추론(reasoning)’의 문제에 가깝다.</p>
<ul>
<li><strong>대규모 다중양식 모델(LMMs)과의 통합:</strong> BERT의 언어 이해 능력 한계를 극복하기 위해, 훨씬 더 강력한 언어 이해 및 추론 능력을 갖춘 LMMs(Large Multimodal Models)를 텍스트 인코더로 대체하거나, LMMs에 Grounding DINO의 정밀한 시각 그라운딩 능력을 주입하는 방식의 연구가 유망하다. 이는 모델이 복잡한 지시를 더 깊이 이해하고 수행할 수 있게 할 것이다.34</li>
<li><strong>실시간 성능 확보:</strong> 모델 경량화 기술(예: 양자화, 가지치기), 지식 증류(knowledge distillation), 그리고 하드웨어 가속 기법을 적용하여 추론 속도를 개선하는 연구가 필수적이다. 이를 통해 실시간 탐지가 가능한 ‘Edge’ 버전의 모델을 개발하여 응용 범위를 넓힐 수 있다.37</li>
<li><strong>구성적 이해 능력 향상 (Enhancing Compositionality):</strong> 여러 객체와 그들 간의 복잡한 속성, 관계를 정확하게 분해하고 이해하여 탐지하는 ‘구성적 이해(compositional understanding)’ 능력을 강화하는 것이 향후 연구의 핵심 과제이다. 이는 단순히 객체를 찾는 것을 넘어, 장면 전체의 의미론적 구조를 파악하는 고차원적인 능력으로의 발전을 의미한다.7</li>
</ul>
<p>궁극적으로, Grounding DINO와 MM-Grounding-DINO가 연 길은 단순히 ‘보는(seeing)’ AI를 넘어, 인간의 언어를 통해 세상을 ‘보고, 이해하며, 추론하는(seeing, understanding, and reasoning)’ AI로 진화하는 과정의 중요한 단계라고 할 수 있다. MM-Grounding-DINO가 제공하는 개방형 연구 플랫폼은 이러한 미래를 향한 커뮤니티의 집단적 노력을 가속화하는 핵심적인 동력이 될 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>GroundingDINO Object Detection Model: What is, How to Use - Roboflow, https://roboflow.com/model/groundingdino</li>
<li>Fine-Tuning Grounding DINO: Open-Vocabulary Object Detection - LearnOpenCV, https://learnopencv.com/fine-tuning-grounding-dino/</li>
<li>Grounding DINO : SOTA Zero-Shot Object Detection - Roboflow Blog, https://blog.roboflow.com/grounding-dino-zero-shot-object-detection/</li>
<li>An Open and Comprehensive Pipeline for Unified Object Grounding and Detection - arXiv, https://arxiv.org/html/2401.02361v2</li>
<li>[2401.02361] An Open and Comprehensive Pipeline for Unified Object Grounding and Detection - arXiv, https://arxiv.org/abs/2401.02361</li>
<li>Marrying DINO with Grounded Pre-Training for Open-Set Object Detection - arXiv, https://arxiv.org/abs/2303.05499</li>
<li>Exploring the DINO Family Part 2: The Debut of Grounding DINO for Open-Set Object Detection | by Ideacvr | Medium, https://medium.com/@ideacvr2024/exploring-the-dino-family-part-2-the-debut-of-grounding-dino-for-open-set-object-detection-4d5928b9216b</li>
<li>MM-Grounding-DINO: Unified Object Grounding - Emergent Mind, https://www.emergentmind.com/papers/2401.02361</li>
<li>Grounding DINO: Revolutionizing AI with Zero-Shot Detection - Ikomia, https://www.ikomia.ai/blog/grounding-dino-zero-shot-detection-explained</li>
<li>Exploring DINO Family Part2: Grounding DINO for Open-Set Object Detection Debuts, https://deepdataspace.com/en/blog/14/</li>
<li>Grounding DINO - Hugging Face, https://huggingface.co/docs/transformers/v4.50.0/model_doc/grounding-dino</li>
<li>Grounding DINO - Wow AI, https://wow-ai.com/grounding-dino.html</li>
<li>Changelog of v3.x - MMDetection’s documentation! - Read the Docs, https://mmdetection.readthedocs.io/en/v3.3.0/notes/changelog.html</li>
<li>Changelog of v3.x - MMDetection’s documentation! - Read the Docs, https://mmdetection.readthedocs.io/en/dev-3.x/notes/changelog.html</li>
<li>open-mmlab/mmdetection: OpenMMLab Detection Toolbox and Benchmark - GitHub, https://github.com/open-mmlab/mmdetection</li>
<li>Releases · open-mmlab/mmdetection - GitHub, https://github.com/open-mmlab/mmdetection/releases</li>
<li>openmmlab-community/mm_grounding_dino_large_all - Hugging Face, https://huggingface.co/openmmlab-community/mm_grounding_dino_large_all</li>
<li>MM Grounding DINO - Hugging Face, https://huggingface.co/docs/transformers/main/model_doc/mm-grounding-dino</li>
<li>FDS/mmdetection - Gitee, https://gitee.com/qq771304328_admin/mmdetection/blob/main/configs/mm_grounding_dino/README.md</li>
<li>Grounding DINO: Detect Any Object from Text | by David Cochard | ailia-ai | Medium, https://medium.com/axinc-ai/grounding-dino-detect-any-object-from-text-29808580cb32</li>
<li>mmdetection/configs/mm_grounding_dino/dataset_prepare.md at main - GitHub, https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md</li>
<li>MM Grounding dino · Issue #11443 · open-mmlab/mmdetection - GitHub, https://github.com/open-mmlab/mmdetection/issues/11443</li>
<li>Grounding DINO — Tao Toolkit - NVIDIA Documentation, https://docs.nvidia.com/tao/tao-toolkit/text/cv_finetuning/pytorch/object_detection/grounding_dino.html</li>
<li>Zero-shot Object Detection Using Grounding DINO Base - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2024/10/grounding-dino-base/</li>
<li>Understanding the Differences Between CLIP, Grounding DINO, and SAM: A Deep Dive into Vision-Language Models and Segmentation | by Shawn | Jul, 2025 | Medium, https://medium.com/@hexiangnan/understanding-the-differences-between-clip-grounding-dino-and-sam-a-deep-dive-into-b0724d15d92c</li>
<li>Illustrative examples of the performance limitations of the Grounding DINO model, https://www.researchgate.net/figure/Illustrative-examples-of-the-performance-limitations-of-the-Grounding-DINO-model-a-a_fig1_372370608</li>
<li>Using Text Prompts for Image Annotation with Grounding DINO and Label Studio, https://labelstud.io/blog/using-text-prompts-for-image-annotation-with-grounding-dino-and-label-studio/</li>
<li>IDEA-Research/Grounded-Segment-Anything: Grounded SAM: Marrying Grounding DINO with Segment Anything &amp; Stable Diffusion &amp; Recognize Anything - Automatically Detect , Segment and Generate Anything - GitHub, https://github.com/IDEA-Research/Grounded-Segment-Anything</li>
<li>Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator - arXiv, https://arxiv.org/html/2411.15466v1</li>
<li>IDEA-Research/Grounded-SAM-2: Grounded SAM 2: Ground and Track Anything in Videos with Grounding DINO, Florence-2 and SAM 2 - GitHub, https://github.com/IDEA-Research/Grounded-SAM-2</li>
<li>Detect Anything You Want with Grounding DINO | Zero Shot Object Detection SOTA, https://www.youtube.com/watch?v=cMa77r3YrDk</li>
<li>Teacher–Student Model Using Grounding DINO and You Only Look Once for Multi-Sensor-Based Object Detection - MDPI, https://www.mdpi.com/2076-3417/14/6/2232</li>
<li>Teacher–Student Model Using Grounding DINO and You Only Look Once for Multi-Sensor-Based Object Detection - MDPI, https://www.mdpi.com/2076-3417/14/6/2232/review_report</li>
<li>LLM-Optic: Unveiling the Capabilities of Large Language Models for Universal Visual Grounding - arXiv, https://arxiv.org/html/2405.17104v2</li>
<li>GroundingDINO for Open-Set Lesion Detection in Medical Imaging | OpenReview, https://openreview.net/forum?id=Rvdet5Tm9n</li>
<li>Here are several bad detection results of Grounding DINO model when the… | Download Scientific Diagram - ResearchGate, https://www.researchgate.net/figure/Here-are-several-bad-detection-results-of-Grounding-DINO-model-when-the-input-prompts_fig2_370982134</li>
<li>IDEA-Research/Grounding-DINO-1.5-API - GitHub, https://github.com/IDEA-Research/Grounding-DINO-1.5-API</li>
<li>DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding - arXiv, https://arxiv.org/html/2411.14347v1</li>
<li>Grounding DINO 1.5 Pro - DeepDataSpace, https://deepdataspace.com/blog/5/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>