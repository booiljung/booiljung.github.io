<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:FSSD (Feature Fusion Single Shot Detector, 2017-12-04)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>FSSD (Feature Fusion Single Shot Detector, 2017-12-04)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>FSSD (Feature Fusion Single Shot Detector, 2017-12-04)</span></nav>
                </div>
            </header>
            <article>
                <h1>FSSD (Feature Fusion Single Shot Detector, 2017-12-04)</h1>
<h2>1. 서론: 단일 단계 객체 검출기와 SSD의 근본적 한계</h2>
<p>객체 검출(Object Detection)은 컴퓨터 비전 분야의 핵심 과제로, 이미지 내에 존재하는 객체의 종류를 분류하고 그 위치를 경계 상자(Bounding Box)로 특정하는 작업을 포함한다. 현대 딥러닝 기반 객체 검출기는 크게 두 가지 패러다임으로 나뉜다.</p>
<h3>1.1 객체 검출의 두 가지 패러다임</h3>
<h4>1.1.1 -단계 검출기 (Two-Stage Detectors)</h4>
<p>R-CNN, Fast R-CNN, Faster R-CNN 등으로 대표되는 2-단계 검출기는 높은 정확도를 목표로 설계되었다. 이 접근법은 먼저 이미지 내에서 객체가 존재할 가능성이 높은 후보 영역(Region Proposal)을 선별한 후, 각 후보 영역에 대해 정밀한 분류(Classification) 및 위치 보정(Regression)을 수행하는 다단계 파이프라인을 채택한다.1 이러한 구조는 정확도 측면에서 뛰어난 성능을 보장하지만, 여러 단계를 순차적으로 거쳐야 하므로 추론 속도가 느려 실시간 응용 분야에 적용하기에는 한계가 명확하다.3</p>
<h4>1.1.2 -단계 검출기 (One-Stage Detectors)</h4>
<p>YOLO(You Only Look Once)와 SSD(Single Shot MultiBox Detector)와 같은 1-단계 검출기는 속도와 정확도 간의 균형을 맞추는 데 중점을 둔다. 이들은 후보 영역 생성과 분류/회귀 과정을 단일 신경망 내에서 통합하여 한 번의 순전파(forward pass)만으로 모든 예측을 완료한다.1 이 덕분에 매우 빠른 추론 속도를 달성하여 실시간 객체 검출에 널리 사용된다. 본 안내서에서 심층적으로 다룰 FSSD는 바로 이 1-단계 검출기 계보에 속하며, 특히 SSD의 한계를 극복하기 위해 제안된 모델이다.</p>
<h3>1.2 SSD (Single Shot MultiBox Detector) 아키텍처 심층 분석</h3>
<p>FSSD를 이해하기 위해서는 그 기반이 되는 SSD의 구조와 본질적인 문제점을 먼저 파악해야 한다. SSD는 다음과 같은 핵심 요소로 구성된다.</p>
<ul>
<li>
<p><strong>기반 네트워크 (Base Network):</strong> VGG-16과 같은 사전 학습된 이미지 분류 네트워크를 특징 추출기(feature extractor)로 활용한다. 원본 네트워크의 완전 연결 계층(Fully Connected Layer)은 컨볼루션 계층(Convolutional Layer)으로 변환되어 다양한 크기의 입력 이미지에 대응할 수 있도록 설계되었다.2</p>
</li>
<li>
<p><strong>피처 피라미드 계층 (Feature Pyramid Hierarchy):</strong> SSD의 가장 큰 특징 중 하나는 기반 네트워크의 뒤에 점진적으로 공간 해상도가 감소하는 여러 개의 추가적인 컨볼루션 계층을 배치하여 다중 스케일 피처 맵(Multi-scale Feature Maps)의 계층 구조를 형성하는 것이다.6 각기 다른 해상도를 가진 피처 맵은 서로 다른 크기의 객체를 탐지하는 데 특화되어 있다. 예를 들어, 해상도가 높은 초기 피처 맵은 작은 객체를, 해상도가 낮은 후기 피처 맵은 큰 객체를 탐지하는 역할을 담당한다.2</p>
</li>
<li>
<p><strong>예측 메커니즘 (Prediction Mechanism):</strong> 각 피처 맵의 모든 위치(cell)에서는 사전에 정의된 다양한 종횡비(aspect ratio)와 크기(scale)를 갖는 기본 상자(Default Box 또는 Anchor Box) 집합이 사용된다. 이후, 각 기본 상자에 대해 객체의 클래스 신뢰도(Confidence Score)와 경계 상자의 위치 오프셋(Location Offset)을 예측하기 위해 작은 <span class="math math-inline">3 \times 3</span> 컨볼루션 필터가 적용된다.3</p>
</li>
</ul>
<h3>1.3 SSD의 핵심 문제점: 의미 정보와 위치 정보의 분리</h3>
<p>SSD는 다중 스케일 접근법을 통해 혁신적인 속도와 준수한 정확도를 달성했지만, 아키텍처 자체에 내재된 근본적인 한계를 가지고 있다. 이는 특히 작은 객체 검출 성능을 저하시키는 주요 원인이 된다.</p>
<ul>
<li>
<p><strong>얕은 계층의 한계:</strong> <code>conv4_3</code>과 같은 네트워크의 초기 계층에서 추출된 피처 맵은 높은 공간 해상도를 유지하고 있어 작은 객체의 정밀한 위치 정보를 잘 보존한다. 그러나 이 피처들은 소수의 컨볼루션 연산만을 거쳤기 때문에, 객체의 클래스를 구분하는 데 필요한 추상적이고 고차원적인 의미론적 정보(Semantic Information)가 매우 부족하다.10 결과적으로, 이 계층의 예측기는 객체와 배경을 효과적으로 구별하는 데 어려움을 겪는다.</p>
</li>
<li>
<p><strong>깊은 계층의 한계:</strong> 반대로 <code>fc7</code>, <code>conv7_2</code>와 같이 네트워크 후반부의 깊은 계층에서 생성된 피처 맵은 풍부한 의미 정보를 포함하고 있어 객체 분류에 매우 유리하다. 하지만 반복적인 풀링(pooling) 또는 스트라이드(stride) 컨볼루션을 통해 공간 해상도가 크게 손실된 상태이다. 이 과정에서 작은 객체와 관련된 세밀한 특징 정보는 거의 소실되어 버린다.6</p>
</li>
</ul>
<p>이 문제의 본질은 SSD의 피처 피라미드 구조가 <strong>위치 정보가 풍부한 피처</strong>와 <strong>의미 정보가 풍부한 피처</strong>를 효과적으로 결합하는 메커니즘을 제공하지 않는다는 점에 있다. 각 피처 맵은 독립적으로 예측을 수행하므로, 얕은 계층은 객체가 ’무엇’인지 충분히 이해하지 못한 채 위치를 예측하고, 깊은 계층은 객체가 ’어디’에 있는지에 대한 정확한 정보 없이 분류를 시도하게 된다.1</p>
<p>이러한 구조적 한계는 피할 수 없는 트레이드오프를 야기한다. 컨볼루션 신경망의 기본 원리상, 의미적 복잡성을 높이는 과정은 필연적으로 공간 해상도의 감소를 동반한다. SSD는 이 계층적 특징을 그대로 예측에 사용함으로써, 특정 계층에서는 좋은 위치 정보와 나쁜 분류 정보, 또는 나쁜 위치 정보와 좋은 분류 정보 중 하나를 강제로 선택해야 하는 딜레마에 빠진다. FSSD는 바로 이 패러다임에 도전하여, 예측 단계 이전에 위치와 의미 정보를 모두 갖춘 새로운 피처를 생성함으로써 이 문제를 해결하고자 한다.</p>
<h2>2. FSSD (Feature Fusion Single Shot Detector)의 구조적 혁신</h2>
<p>FSSD는 SSD의 근본적인 문제, 즉 의미 정보와 위치 정보의 분리를 해결하기 위해 ’피처 퓨전(Feature Fusion)’이라는 개념을 도입했다. 이는 기존의 피처 피라미드를 그대로 사용하는 대신, 여러 계층의 특징을 지능적으로 결합하여 보다 강력하고 균형 잡힌 새로운 피처 피라미드를 생성하는 것을 목표로 한다.</p>
<h3>2.1 FSSD 전체 네트워크 아키텍처</h3>
<p>FSSD의 전체적인 구조는 SSD를 기반으로 하되, 핵심적인 차이점을 가진다. 기반 네트워크로는 SSD와 동일하게 VGG-16을 사용하지만, 예측을 수행하는 방식이 근본적으로 다르다. SSD가 VGG-16과 추가 컨볼루션 계층에서 나온 피처 맵들을 직접 예측에 사용하는 반면, FSSD는 이들 중 일부를 **피처 퓨전 모듈(Feature Fusion Module, FFM)**의 입력으로 전달한다.6</p>
<p>피처 퓨전 모듈은 서로 다른 스케일과 의미 수준을 가진 피처 맵들을 하나로 융합하여 고차원의 단일 융합 피처 맵(Fused Feature Map)을 생성한다. 그 후, 이 융합된 피처 맵을 시작점으로 하여 SSD와 유사한 방식의 다운샘플링 블록들을 순차적으로 적용함으로써 <strong>완전히 새로운 피처 피라미드</strong>를 구축한다. 최종적인 객체 검출과 위치 예측은 바로 이 새롭게 생성된 피라미드의 각 계층에서 수행된다.1 이 방식은 모든 예측 계층이 얕은 계층의 정밀한 위치 정보와 깊은 계층의 풍부한 의미 정보를 모두 공유하게 만들어, SSD의 근본적인 한계를 극복한다.</p>
<h3>2.2 핵심 구성 요소: 피처 퓨전 모듈(Feature Fusion Module) 상세 해부</h3>
<p>FSSD의 혁신은 피처 퓨전 모듈(FFM)에 집약되어 있다. 이 모듈의 설계는 신중한 실험과 분석을 통해 결정되었으며, 그 과정은 다음과 같은 네 단계로 나눌 수 있다.</p>
<h4>2.2.1  입력 피처 맵 선정 (Selection of Input Feature Maps)</h4>
<p>모든 피처 맵을 융합하는 것이 최선은 아니다. FSSD의 저자들은 실험을 통해 VGG-16 기반 SSD의 여러 피처 맵 중에서 어떤 조합이 가장 효과적인지를 탐색했다. 그 결과, 비교적 높은 해상도와 중간 수준의 의미 정보를 가진 <code>conv4_3</code>, 그리고 풍부한 의미 정보를 담고 있는 <code>fc_7</code>과 <code>conv7_2</code>를 융합하는 것이 최적의 성능을 낸다는 것을 발견했다.6 이 선택은 위치 정보와 의미 정보 간의 균형을 맞추려는 의도를 반영한다.</p>
<h4>2.2.2  피처 맵의 전처리: 크기 및 채널 조절 (Preprocessing of Feature Maps: Adjusting Scale and Channels)</h4>
<p>선택된 피처 맵들은 서로 다른 공간 해상도와 채널 수를 가지고 있어 직접 융합할 수 없다. 따라서 다음과 같은 전처리 과정을 거친다.</p>
<ul>
<li>
<p><strong>크기 통일:</strong> 융합의 기준이 될 기본 해상도를 정한다. FSSD에서는 선택된 피처 맵 중 가장 해상도가 높은 <span class="math math-inline">conv4_3</span>의 크기(예: 300x300 입력 기준 38x38)를 기준으로 삼는다.11</p>
</li>
<li>
<p><strong>업샘플링:</strong> <code>fc_7</code>(19x19)과 <code>conv7_2</code>(19x19)처럼 기준보다 작은 피처 맵은 **쌍선형 보간법(Bilinear Interpolation)**을 사용하여 기준 크기(38x38)로 확대한다. 이 방법은 역컨볼루션(Deconvolution)과 같은 학습 가능한 파라미터를 사용하는 방식보다 계산적으로 훨씬 가볍고 효율적이다.2</p>
</li>
<li>
<p><strong>채널 조절:</strong> 각 피처 맵에 <strong><code>1 \times 1</code> 컨볼루션</strong> 연산을 적용하여 채널 수를 동일하게 맞춘다(예: 256 채널). <span class="math math-inline">1 \times 1</span> 컨볼루션은 채널 수를 조절하는 동시에 각 채널의 정보를 선형 결합하여 특징을 정제하고, 계산 복잡도를 줄이는 효과적인 수단이다.11</p>
</li>
</ul>
<h4>2.2.3  융합 방식: 연결 (Fusion Method: Concatenation)</h4>
<p>전처리를 마친 피처 맵들은 채널 축(channel axis)을 따라 **연결(Concatenate)**된다. 예를 들어, 3개의 38x38x256 피처 맵이 연결되면 38x38x768 크기를 갖는 하나의 거대한 융합 피처 맵이 생성된다.6 이 방식은 각 피처 맵이 가진 고유한 정보를 손실 없이 그대로 보존하면서 결합할 수 있다는 장점이 있다. 연결 직후에는</p>
<p><strong>배치 정규화(Batch Normalization)</strong> 층이 적용되는데, 이는 서로 다른 스케일과 분포를 가진 피처들을 안정화시켜 후속 학습 과정을 원활하게 하고 모델의 성능을 향상시키는 역할을 한다.11</p>
<h4>2.2.4  새로운 피처 피라미드 생성 (Generation of a New Feature Pyramid)</h4>
<p>생성된 융합 피처 맵은 새로운 피처 피라미드의 첫 번째 계층(가장 높은 해상도)이 된다. 이후, 이 융합 피처 맵에 연속적인 다운샘플링 블록(예: 스트라이드가 2인 <span class="math math-inline">3 \times 3</span> 컨볼루션)을 적용하여 점진적으로 크기가 작아지는 피처 맵들을 생성한다. 이 과정을 통해 구축된 피라미드의 모든 계층은 초기 융합 단계에서부터 위치 정보와 의미 정보를 모두 물려받게 된다.1</p>
<p>FSSD의 이러한 ‘경량’ 설계 철학은 DSSD(Deconvolutional Single Shot Detector)와 같은 동시대의 다른 개선 모델과 뚜렷한 대조를 이룬다. DSSD는 강력한 ResNet-101을 백본으로 사용하고 복잡한 역컨볼루션 경로를 추가하여 정확도를 높였지만, 그 대가로 속도가 크게 저하되었다.6 반면 FSSD는 기존 네트워크가 이미 추출한 풍부한 특징들을 쌍선형 보간,</p>
<p><span class="math math-inline">1 \times 1</span> 컨볼루션, 연결 등 계산적으로 매우 효율적인 연산자들을 통해 재조합하는 방식을 택했다. 이는 모델의 복잡성이나 크기를 늘리기보다, 기존 자원을 더 지능적으로 활용하는 것만으로도 상당한 성능 향상을 이끌어낼 수 있음을 보여주는 중요한 사례이다.</p>
<h2>3. FSSD의 학습 방법론 및 손실 함수</h2>
<p>FSSD의 핵심 혁신은 아키텍처, 특히 피처 퓨전 모듈에 집중되어 있다. 따라서 모델을 학습시키는 데 사용되는 손실 함수(Loss Function)는 기존 SSD의 검증된 방식을 그대로 계승한다. 이는 FSSD가 생성한 새로운 피처 피라미드가 SSD의 예측 헤드 및 손실 함수와 완벽하게 호환되도록 설계되었음을 의미한다.</p>
<h3>3.1 SSD로부터 계승된 다중 작업 손실 함수</h3>
<p>SSD의 손실 함수는 하나의 객체를 정확히 검출하기 위해 필요한 두 가지 핵심 작업, 즉 ’무엇’을 찾았는가(분류)와 ’어디’에 있는가(위치)를 동시에 최적화하는 다중 작업 손실(Multi-Task Loss)이다. 이 손실 함수는 **위치 손실(Localization Loss, <span class="math math-inline">L_loc</span>)**과 **신뢰도 손실(Confidence Loss, <span class="math math-inline">L_conf</span>)**이라는 두 가지 구성 요소의 가중합(weighted sum)으로 정의된다.15</p>
<h3>3.2 전체 손실 함수의 수학적 공식</h3>
<p>FSSD에서 사용되는 전체 손실 함수 <span class="math math-inline">L</span>은 다음과 같은 수학적 공식으로 표현된다.4</p>
<p><span class="math math-display">
L(x, c, l, g) = \frac{1}{N}(L_{conf}(x, c) + \alpha L_{loc}(x, l, g))
</span><br />
각 항의 의미는 다음과 같다.</p>
<ul>
<li>
<p><span class="math math-inline">N</span>: 실제 객체(Ground Truth)와 매칭된 기본 상자의 총 개수. 이를 통해 손실 값을 정규화한다. 만약 매칭된 상자가 없다면(<span class="math math-inline">N=0</span>), 손실은 0으로 설정된다.</p>
</li>
<li>
<p><span class="math math-inline">\alpha</span>: 위치 손실과 신뢰도 손실 간의 상대적 중요도를 조절하는 가중치 파라미터이다. 교차 검증을 통해 일반적으로 1로 설정된다.</p>
</li>
<li>
<p><span class="math math-inline">x</span>: <span class="math math-inline">i</span>번째 기본 상자와 <span class="math math-inline">j</span>번째 실제 객체가 <span class="math math-inline">p</span> 클래스에 대해 매칭되었는지 여부를 나타내는 지표. 매칭되면 1, 아니면 0이다.</p>
</li>
<li>
<p><span class="math math-inline">c</span>: 예측된 클래스 신뢰도 점수 벡터.</p>
</li>
<li>
<p><span class="math math-inline">l</span>: 예측된 경계 상자의 위치 오프셋 벡터.</p>
</li>
<li>
<p><span class="math math-inline">g</span>: 실제 객체의 경계 상자 좌표 벡터.</p>
</li>
</ul>
<h3>3.3 손실 함수 구성 요소 상세 분석</h3>
<h4>3.3.1 위치 손실 (<span class="math math-inline">L_loc</span>)</h4>
<p>위치 손실은 모델이 예측한 경계 상자(<span class="math math-inline">l</span>)가 실제 경계 상자(<span class="math math-inline">g</span>)와 얼마나 정확하게 일치하는지를 측정한다.</p>
<ul>
<li>
<p><strong>함수:</strong> L1 손실에 비해 이상치(outlier)에 덜 민감하여 안정적인 학습을 가능하게 하는 <strong>Smooth L1 Loss</strong>가 사용된다.8</p>
</li>
<li>
<p><strong>적용 대상:</strong> 이 손실은 실제 객체와 성공적으로 매칭된 양성 샘플(positive samples)에 대해서만 계산된다. 배경으로 판정된 상자의 위치는 최적화할 필요가 없기 때문이다.3</p>
</li>
<li>
<p><strong>수식:</strong><br />
<span class="math math-display">
L_{loc}(x, l, g) = \sum_{i \in Pos}^{N} \sum_{m \in \{cx, cy, w, h\}} x_{ij}^k \text{smooth}_{L1}(l_i^m - \hat{g}_j^m)
</span><br />
여기서 <span class="math math-inline">l_i^m</span>은 <span class="math math-inline">i</span>번째 예측 상자의 좌표(중심점 <span class="math math-inline">cx</span>, <span class="math math-inline">cy</span>, 너비 <span class="math math-inline">w</span>, 높이 <span class="math math-inline">h</span>)를, <span class="math math-inline">\hat{g}_j^m</span>는 <span class="math math-inline">i</span>번째 기본 상자(<span class="math math-inline">d</span>)에 대한 <span class="math math-inline">j</span>번째 실제 객체 상자의 인코딩된 오프셋을 의미한다.</p>
</li>
</ul>
<h4>3.3.2 신뢰도 손실 (<span class="math math-inline">L_conf</span>)</h4>
<p>신뢰도 손실은 각 기본 상자에 대해 모델이 예측한 클래스가 실제 클래스와 얼마나 일치하는지를 측정한다.</p>
<ul>
<li>
<p><strong>함수:</strong> 다중 클래스 분류 문제에 표준적으로 사용되는 <strong>Softmax Loss</strong> (Cross-Entropy Loss의 일종)가 적용된다.15</p>
</li>
<li>
<p><strong>수식:</strong> 신뢰도 손실은 양성 샘플에 대한 손실과 음성 샘플(배경)에 대한 손실의 합으로 구성된다.</p>
<p><span class="math math-display">
L_{conf}(x, c) = - \sum_{i \in Pos}^{N} x_{ij}^p \log(\hat{c}_i^p) - \sum_{i \in Neg} \log(\hat{c}_i^0)
</span><br />
여기서 <span class="math math-inline">\hat{c}_i^p</span>는 <span class="math math-inline">i</span>번째 상자가 클래스 <span class="math math-inline">p</span>에 속할 것으로 예측된 Softmax 확률값이며, <span class="math math-inline">\hat{c}_i^0</span>는 배경 클래스(<span class="math math-inline">0</span>)에 속할 것으로 예측된 확률값이다.</p>
</li>
</ul>
<h3>3.4 하드 네거티브 마이닝 (Hard Negative Mining)</h3>
<p>객체 검출 시 이미지 내 대부분의 영역은 배경에 해당하므로, 수많은 기본 상자 역시 배경(음성 샘플)으로 분류된다. 이로 인해 양성 샘플에 비해 음성 샘플의 수가 압도적으로 많아지는 심각한 클래스 불균형 문제가 발생한다. 만약 모든 음성 샘플을 학습에 사용한다면, 모델은 손실을 최소화하기 위해 모든 것을 배경으로 예측하는 방향으로 편향될 수 있다.3</p>
<p>이 문제를 해결하기 위해 SSD와 FSSD는 <strong>하드 네거티브 마이닝</strong> 기법을 사용한다. 이 기법은 모든 음성 샘플을 사용하는 대신, 신뢰도 손실 값이 가장 높은, 즉 모델이 가장 헷갈려하는 ‘어려운’ 음성 샘플들만 선별하여 학습에 참여시킨다. 이때, 학습의 안정성을 위해 선별된 음성 샘플과 양성 샘플의 비율을 최대 3:1로 제한한다. 이를 통해 모델은 쉬운 배경을 무시하고 객체와 유사한 어려운 배경을 구별하는 능력을 집중적으로 학습하게 되어, 더 빠르고 안정적인 수렴과 높은 성능을 달성할 수 있다.3</p>
<p>FSSD가 기존 SSD의 손실 함수를 그대로 재사용한다는 사실은 FFM의 효과를 역설적으로 증명한다. 성능 향상이 새로운 학습 목표나 손실 함수 설계에서 비롯된 것이 아니라, 순수하게 아키텍처 개선을 통해 예측기에 더 양질의 정보를 제공한 결과이기 때문이다. FFM은 손실 함수가 최적화해야 할 피처 맵의 품질 자체를 근본적으로 향상시키는 ‘피처 전처리기’ 역할을 수행한다. 신뢰도 손실(<span class="math math-inline">L_conf</span>)은 의미 정보가 풍부해진 피처 덕분에 객체와 배경을 더 쉽게 구분할 수 있게 되고, 위치 손실(<span class="math math-inline">L_loc</span>)은 공간 정보와 컨텍스트가 결합된 피처 덕분에 객체의 경계를 더 정밀하게 회귀할 수 있게 된다. 즉, FSSD의 성공은 더 나은 목표가 아닌, 기존 목표를 달성하기 위한 더 나은 재료를 제공한 데 있다.</p>
<h2>4. 실험 결과 및 성능 분석</h2>
<p>FSSD의 아키텍처적 우수성은 표준 벤치마크 데이터셋인 PASCAL VOC와 MS COCO에서의 실험 결과를 통해 정량적으로 입증되었다. 분석의 핵심은 정확도(mAP)와 속도(FPS) 지표를 통해 기존 SSD 및 다른 주요 모델들과의 성능을 비교하는 것이다.</p>
<h3>4.1 PASCAL VOC 데이터셋 성능 비교</h3>
<p>PASCAL VOC 2007 test 데이터셋은 객체 검출 모델의 성능을 평가하는 전통적인 표준 벤치마크이다. FSSD는 이 데이터셋에서 기존 SSD의 성능을 크게 상회하면서도 속도 저하를 최소화하여 그 효율성을 입증했다.</p>
<p><strong>Table 1: PASCAL VOC 2007 test 데이터셋 성능 비교</strong></p>
<table><thead><tr><th>모델 (Model)</th><th>기반 네트워크 (Backbone)</th><th>입력 크기 (Input)</th><th>mAP (%)</th><th>FPS</th><th>출처 (Source)</th></tr></thead><tbody>
<tr><td>Faster R-CNN</td><td>VGG-16</td><td>~600x1000</td><td>73.2</td><td>7</td><td>3</td></tr>
<tr><td>SSD300</td><td>VGG-16</td><td>300x300</td><td>77.2</td><td>46</td><td>17</td></tr>
<tr><td>DSSD321</td><td>ResNet-101</td><td>321x321</td><td>78.6</td><td>9.5</td><td>14</td></tr>
<tr><td><strong>FSSD300</strong></td><td>VGG-16</td><td>300x300</td><td><strong>78.8</strong></td><td><strong>65.8</strong></td><td>6</td></tr>
<tr><td><strong>FSSD300</strong>*</td><td>VGG-16</td><td>300x300</td><td><strong>82.7</strong></td><td><strong>65.8</strong></td><td>6</td></tr>
</tbody></table>
<p><em>주: FSSD300</em>는 추가적인 데이터 증강 및 최적화가 적용된 결과일 수 있음. FPS는 Nvidia 1080Ti GPU 환경에서 측정된 값임.6*</p>
<p>분석:</p>
<p>위 표는 FSSD의 우수한 성능 균형을 명확히 보여준다. FSSD300은 동일한 VGG-16 기반의 SSD300에 비해 mAP(mean Average Precision)가 1.6%p 더 높다. FPS 수치는 사용된 하드웨어의 차이를 감안해야 하지만, 최소한 속도 저하가 크지 않거나 오히려 더 빠를 수 있음을 시사한다. 더욱 인상적인 점은 훨씬 무겁고 복잡한 ResNet-101 기반의 DSSD321과 비교했을 때이다. FSSD300은 DSSD321보다도 높은 mAP를 기록하면서, 속도는 6배 이상 빠르다. 이는 FSSD가 정확도와 속도 간의 트레이드오프(trade-off) 관계를 획기적으로 개선했음을 증명하는 강력한 증거이다.</p>
<h3>4.2 MS COCO 데이터셋 성능 비교</h3>
<p>MS COCO 데이터셋은 PASCAL VOC에 비해 이미지 수가 훨씬 많고, 이미지 당 객체 수도 많으며, 특히 작은 객체(small objects)의 비율이 높아 모델의 강건함과 일반화 성능을 평가하기에 더 까다로운 벤치마크이다. FSSD는 이 데이터셋에서 피처 퓨전의 진정한 가치를 입증했다.</p>
<p><strong>Table 2: MS COCO test-dev 데이터셋 성능 비교</strong></p>
<table><thead><tr><th>모델 (Model)</th><th>입력 크기 (Input)</th><th>AP@[.5:.95]</th><th>AP@.5</th><th>AP_S</th><th>AP_M</th><th>AP_L</th><th>출처 (Source)</th></tr></thead><tbody>
<tr><td>SSD300</td><td>300x300</td><td>25.1</td><td>43.1</td><td>6.6</td><td>25.9</td><td>41.4</td><td>6</td></tr>
<tr><td><strong>FSSD300</strong></td><td>300x300</td><td><strong>29.4</strong></td><td><strong>48.3</strong></td><td><strong>11.2</strong></td><td><strong>31.3</strong></td><td><strong>43.7</strong></td><td>6</td></tr>
</tbody></table>
<p><em>주: AP@[.5:.95]는 IoU 임계값을 0.5부터 0.95까지 0.05 간격으로 변화시키며 측정한 mAP의 평균으로, 가장 엄격한 평가지표이다. AP_S, AP_M, AP_L은 각각 작은, 중간, 큰 객체에 대한 AP를 의미한다.</em></p>
<p>분석:</p>
<p>MS COCO 데이터셋에서의 결과는 더욱 극적이다. FSSD는 모든 평가지표에서 SSD를 큰 차이로 능가했다. 전체적인 성능을 나타내는 AP@[.5:.95]는 4.3%p 상승했으며, IoU 0.5 기준의 AP는 5.2%p 상승했다.</p>
<p>그러나 가장 주목해야 할 부분은 **작은 객체에 대한 성능(AP_S)**이다. SSD300이 6.6%의 AP_S를 기록한 반면, FSSD300은 **11.2%**를 달성했다. 이는 <strong>절대 수치로 4.6%p, 상대적 비율로는 약 70%에 달하는 경이로운 성능 향상</strong>이다. 이 결과는 FSSD의 피처 퓨전 모듈이 얕은 계층의 위치 정보와 깊은 계층의 의미 정보를 성공적으로 결합하여, 기존 SSD에서는 감지하기 어려웠던 작은 객체들을 안정적으로 검출할 수 있게 만들었음을 명백히 보여준다.6 이는 FSSD가 해결하고자 했던 바로 그 문제를 정확하게 해결했음을 정량적으로 입증하는 것이다.</p>
<h2>5. 설계 결정에 대한 고찰: 어블레이션 연구</h2>
<p>FSSD의 최종 아키텍처는 임의로 결정된 것이 아니라, 다양한 설계 선택지에 대한 엄격한 실험과 검증을 통해 도출된 결과물이다. FSSD의 저자들은 피처 퓨전 모듈(FFM)의 성능에 영향을 미칠 수 있는 핵심 요소들을 분리하여 각각의 효과를 체계적으로 분석하는 어블레이션 연구(Ablation Study)를 수행했다. 이를 통해 각 설계 결정의 타당성을 입증하고 최적의 구조를 찾아냈다.6</p>
<h3>5.1 피처 퓨전 모듈의 최적 구조 탐색</h3>
<p>어블레이션 연구는 PASCAL VOC 2007 test 데이터셋에서 진행되었으며, 주요 탐색 변수는 융합할 계층의 범위, 융합 방식, 그리고 정규화 적용 여부였다.</p>
<p><strong>Table 3: FFM 설계에 대한 어블레이션 연구 (PASCAL VOC 2007 test)</strong></p>
<table><thead><tr><th>융합 계층 (Fusion Layers)</th><th>융합 방식 (Fusion Method)</th><th>정규화 (Normalization)</th><th>mAP (%)</th><th>분석 (Analysis)</th></tr></thead><tbody>
<tr><td><code>conv3_3</code>, <code>conv4_3</code>, <code>fc_7</code>, <code>conv7_2</code></td><td>Concatenation</td><td>Yes</td><td>78.6</td><td>모든 계층을 융합한 기본 모델 6</td></tr>
<tr><td><strong><code>conv4_3</code>, <code>fc_7</code>, <code>conv7_2</code></strong></td><td><strong>Concatenation</strong></td><td><strong>Yes</strong></td><td><strong>78.8</strong></td><td><strong>최종 모델: <span class="math math-inline">conv3_3</span> 제거 시 성능 향상</strong> 6</td></tr>
<tr><td><code>conv3_3</code>, <code>conv4_3</code>, <code>fc_7</code></td><td>Concatenation</td><td>Yes</td><td>78.3</td><td><span class="math math-inline">conv7_2</span> 제거 시 성능 하락 6</td></tr>
<tr><td><code>conv4_3</code>, <code>fc_7</code>, <code>conv7_2</code></td><td>Element-wise Sum</td><td>Yes</td><td>76.3</td><td>연결 방식이 요소별 합보다 2.5%p 우수 6</td></tr>
<tr><td><code>conv4_3</code>, <code>fc_7</code>, <code>conv7_2</code></td><td>Concatenation</td><td>No</td><td>78.1</td><td>배치 정규화 사용 시 성능 향상 11</td></tr>
</tbody></table>
<h3>5.2 핵심 발견 및 통찰</h3>
<p>위 실험 결과는 FSSD의 최종 설계가 데이터에 기반한 최적화의 산물임을 보여준다. 각 실험에서 얻은 핵심적인 통찰은 다음과 같다.</p>
<ul>
<li><strong>융합 계층의 범위 (Range of Layers to Fuse):</strong> 가장 흥미로운 발견 중 하나는 가장 얕은 계층인 <code>conv3_3</code>을 융합 대상에서 제외했을 때, 오히려 mAP가 78.6%에서 78.8%로 0.2%p 상승했다는 점이다.6 이는 ’더 많은 피처를 융합할수록 좋다’는 단순한 가정이 항상 옳지는 않음을 시사한다.</li>
</ul>
<p><code>conv3_3</code> 계층은 공간 해상도는 가장 높지만, 의미론적 정보가 너무 부족하여 유용한 ’신호(signal)’보다 분류에 방해가 되는 ’잡음(noise)’을 더 많이 제공할 수 있다. 즉, 피처 퓨전에는 의미론적 ’신호 대 잡음비’가 최적화되는 특정 지점이 존재하며, <code>conv3_3</code>은 이 임계점보다 낮은 수준의 피처를 제공하여 융합 피처의 전체적인 품질을 저하시킬 수 있다. 반면, 깊은 계층인 <code>conv7_2</code>를 제거했을 때는 성능이 하락하여, 풍부한 의미 정보의 중요성을 재확인시켜 주었다.</p>
<ul>
<li>
<p><strong>융합 방식 (Fusion Method):</strong> <strong>연결(Concatenation)</strong> 방식이 요소별 합(Element-wise Sum) 방식에 비해 mAP에서 2.5%p라는 압도적인 우위를 보였다.6 요소별 합은 두 피처 맵의 정보를 평균화하거나 혼합하는 경향이 있어 각 피처 맵의 고유한 특성이 일부 손실될 수 있다. 반면, 연결 방식은 각 피처 맵의 정보를 채널 차원에서 그대로 보존한 채 결합하므로, 후속 레이어가 어떤 정보를 더 중요하게 사용할지 스스로 학습할 수 있는 유연성을 제공한다. 이 결과는 특징 표현력 측면에서 연결 방식이 훨씬 효과적임을 명확히 보여준다.</p>
</li>
<li>
<p><strong>피처 정규화 (Feature Normalization):</strong> 융합된 피처 맵에 **배치 정규화(Batch Normalization)**를 적용했을 때 mAP가 78.1%에서 78.8%로 0.7%p 상승했다.11 이는 서로 다른 깊이와 스케일에서 추출된 피처 맵들이 상이한 통계적 분포를 가질 수 있기 때문이다. 배치 정규화는 이러한 분포 차이를 완화하고 피처 스케일을 정규화함으로써, 융합 과정 및 후속 피라미드 생성 단계에서의 학습을 안정시키고 최적화를 용이하게 하는 중요한 역할을 수행한다.</p>
</li>
</ul>
<p>결론적으로, FSSD의 피처 퓨전 모듈은 <code>conv4_3</code>, <code>fc_7</code>, <code>conv7_2</code> 계층을 선택하여, 쌍선형 보간과 <code>1 \times 1</code> 컨볼루션으로 전처리한 후, 연결 방식으로 융합하고 배치 정규화를 적용하는 일련의 최적화된 과정을 거친다. 이는 철저한 실험적 검증을 통해 확립된, 효율성과 성능을 모두 고려한 설계의 결과물이다.</p>
<h2>6. 결론: FSSD의 기여와 의의</h2>
<p>FSSD(Feature Fusion Single Shot Detector)는 객체 검출 기술의 발전사에서 중요한 이정표를 제시한 모델이다. 이는 기존의 강력한 아키텍처인 SSD의 본질적인 한계를 명확히 파악하고, 최소한의 계산 비용으로 이를 효과적으로 해결하는 외과수술적 접근법을 통해 실시간 객체 검출 분야의 실용적인 기준을 한 단계 끌어올렸다.</p>
<h3>6.1 FSSD의 핵심 기여 요약</h3>
<p>FSSD의 기여는 다음 세 가지로 요약할 수 있다.</p>
<ol>
<li>
<p><strong>경량 피처 퓨전 모듈의 제안:</strong> FSSD는 SSD의 피처 피라미드가 가진 고질적인 문제, 즉 얕은 계층의 의미 정보 부족과 깊은 계층의 위치 정보 손실을 해결하기 위해, 간단하면서도 매우 효과적인 피처 퓨전 모듈(FFM)을 도입했다.6 이 모듈은 서로 다른 수준의 피처를 지능적으로 결합하여 모든 예측 계층이 풍부한 의미 정보와 정밀한 위치 정보를 동시에 활용할 수 있는 새로운 피처 피라미드를 생성한다.</p>
</li>
<li>
<p><strong>정확도-속도 트레이드오프의 재정의:</strong> DSSD와 같이 무거운 백본 네트워크나 복잡한 디코더 구조에 의존하는 대신, FSSD는 경량 모듈 추가만으로 성능을 개선했다. 그 결과, 최소한의 속도 저하만으로 SSD의 정확도를, 특히 작은 객체 검출 성능을 대폭 향상시켰다.6 이는 실시간성을 유지하면서도 높은 정확도를 요구하는 수많은 응용 분야에 새로운 가능성을 제시했다.</p>
</li>
<li>
<p><strong>효율적인 피처 재활용의 가치 입증:</strong> FSSD는 새로운 특징을 학습하기 위해 막대한 파라미터를 추가하는 대신, 기존 네트워크가 이미 추출한 특징들을 지능적으로 재조합하는 것만으로도 큰 성능 향상을 이끌어낼 수 있음을 증명했다. 이는 모델 설계에 있어 무조건적인 복잡성 증가보다 효율적인 정보 흐름과 재활용이 더 중요할 수 있다는 중요한 교훈을 남겼다.</p>
</li>
</ol>
<h3>6.2 객체 검출 분야에 미친 영향과 후속 연구</h3>
<p>FSSD의 성공은 이후 객체 검출 연구에서 피처 퓨전의 중요성을 크게 부각시키는 계기가 되었다. 비록 FPN(Feature Pyramid Network)과 같은 탑다운(top-down) 방식의 피처 퓨전이 주류를 이루었지만, FSSD는 바텀업(bottom-up) 방식으로 먼저 특징을 융합한 후 피라미드를 생성하는 독자적인 접근법의 유효성을 보여주었다.18</p>
<p>FSSD가 제시한 ’효율적인 퓨전’이라는 아이디어는 많은 후속 연구에 영감을 주었다. 공간 어텐션(Spatial Attention)을 결합한 FASSD, 채널 어텐션(Channel Attention)을 추가한 FFASSD, 팽창 및 역컨볼루션을 활용한 DDSSD 등 FSSD의 개념을 확장하거나 개선하려는 다양한 변형 모델들이 제안되었다.2 이는 FSSD가 제시한 방향성이 학계에서 중요한 연구 주제로 자리 잡았음을 방증한다.</p>
<h3>6.3 최종 평가</h3>
<p>FSSD는 객체 검출 기술의 역사에서 ’혁명’보다는 ’현명한 진화’를 대표하는 모델로 평가할 수 있다. 이는 기존 기술의 강점을 계승하면서도 그 약점을 정확히 진단하고, 가장 효율적인 방법으로 보완하는 공학적 우수성을 보여준다. FSSD는 단순한 성능 향상을 넘어, 어떻게 아키텍처의 정보 흐름을 재설계하여 근본적인 한계를 극복할 수 있는지에 대한 깊은 통찰을 제공했으며, 오늘날에도 여전히 효율적인 객체 검출기 설계에 중요한 참고 자료가 되고 있다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>(PDF) FSSD: Feature Fusion Single Shot Multibox Detector - ResearchGate, https://www.researchgate.net/publication/321511662_FSSD_Feature_Fusion_Single_Shot_Multibox_Detector</li>
<li>AnimprovedSSDbasedonfeature fusion and attention - IJERA, https://www.ijera.com/papers/vol11no4/Series-1/B1104010517.pdf</li>
<li>SSD object detection: Single Shot MultiBox Detector for real-time processing - Jonathan Hui, https://jonathan-hui.medium.com/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06</li>
<li>SSD: Single Shot MultiBox Detector - UNC Computer Science, https://www.cs.unc.edu/~wliu/papers/ssd.pdf</li>
<li>DETECTING SMALL OBJECTS IN THERMAL IMAGES USING SINGLE-SHOT DETECTOR Hao Zhang, Xiang-gong Hong*, Li Zhu, 1. INTRODUCTION - arXiv, https://arxiv.org/pdf/2108.11101</li>
<li>FSSD: Feature Fusion Single Shot Multibox Detector - arXiv, https://arxiv.org/html/1712.00960v4</li>
<li>FSSD: Feature Fusion Single Shot Multibox Detector, https://arxiv.org/pdf/1712.00960</li>
<li>Object Detection Part 4: Fast Detection Models | Lil’Log, https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/</li>
<li>[1512.02325] SSD: Single Shot MultiBox Detector - arXiv, https://arxiv.org/abs/1512.02325</li>
<li>Single Shot Multibox Detector With Deconvolutional Region Magnification Procedure - Electrical &amp; Computer Engineering, https://www.ece.mcmaster.ca/~junchen/SSD_Access2021.pdf</li>
<li>Variations of SSD: Feature-Fusion SSD (FSSD) and Rainbow SSD (RSSD) - Medium, https://medium.com/@amadeusw6/variations-of-ssd-feature-fusion-ssd-fssd-and-rainbow-ssd-rssd-beeb993f2778</li>
<li>e Model structure of (a) SSD and (b) FSSD. Results on PASCAL VOC2007… - ResearchGate, https://www.researchgate.net/figure/e-Model-structure-of-a-SSD-and-b-FSSD-Results-on-PASCAL-VOC2007-dataset-SSD-772_fig2_331444970</li>
<li>FSSD: Feature Fusion Single Shot Multibox Detector (1712.00960v4) - Emergent Mind, https://www.emergentmind.com/papers/1712.00960</li>
<li>[1701.06659] DSSD : Deconvolutional Single Shot Detector - arXiv, https://arxiv.org/abs/1701.06659</li>
<li>How single-shot detector (SSD) works? - GeeksforGeeks, https://www.geeksforgeeks.org/computer-vision/how-single-shot-detector-ssd-works/</li>
<li>www.geeksforgeeks.org, <a href="https://www.geeksforgeeks.org/computer-vision/how-single-shot-detector-ssd-works/#:~:text=The%20SSD%20loss%20function%20combines,class%20scores%20using%20softmax%20loss.">https://www.geeksforgeeks.org/computer-vision/how-single-shot-detector-ssd-works/#:~:text=The%20SSD%20loss%20function%20combines,class%20scores%20using%20softmax%20loss.</a></li>
<li>An Approach to Improve SSD through Skip Connection of Multiscale Feature Maps - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC7142387/</li>
<li>Application of Feature Pyramid Network and Feature Fusion Single Shot Multibox Detector for Real-Time Prostate Capsule Detection - MDPI, https://www.mdpi.com/2079-9292/12/4/1060</li>
<li>FASSD: A Feature Fusion and Spatial Attention-Based Single Shot Detector for Small Object Detection - Semantic Scholar, https://pdfs.semanticscholar.org/049e/6054bcc2dcdcd1433249d9dda2a85fdf72ae.pdf</li>
<li>FASSD: A Feature Fusion and Spatial Attention-Based Single Shot Detector for Small Object Detection - MDPI, https://www.mdpi.com/2079-9292/9/9/1536</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>