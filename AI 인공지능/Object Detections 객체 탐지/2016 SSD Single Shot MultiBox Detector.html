<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:SSD (Single Shot MultiBox Detector, 2016) 단일 샷 객체 탐지기</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>SSD (Single Shot MultiBox Detector, 2016) 단일 샷 객체 탐지기</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>SSD (Single Shot MultiBox Detector, 2016) 단일 샷 객체 탐지기</span></nav>
                </div>
            </header>
            <article>
                <h1>SSD (Single Shot MultiBox Detector, 2016) 단일 샷 객체 탐지기</h1>
<h2>1. 서론</h2>
<p>객체 탐지(Object Detection)는 컴퓨터 비전 분야의 핵심적인 과제로, 이미지나 비디오 내에 존재하는 특정 객체의 종류를 분류하고 그 위치를 경계 박스(Bounding Box)로 정확히 표시하는 것을 목표로 한다. 현대 딥러닝 기반 객체 탐지 알고리즘은 크게 두 가지 패러다임, 즉 2단계 검출기(Two-Stage Detector)와 1단계 검출기(One-Stage Detector)로 나뉜다.</p>
<h3>1.1 객체 탐지 패러다임: 1단계 및 2단계 검출기</h3>
<p>2단계 검출기는 이름에서 알 수 있듯이 탐지 과정을 두 개의 주요 단계로 분리하여 수행한다. 첫 번째 단계에서는 객체가 존재할 가능성이 높은 후보 영역(Region Proposal)을 생성한다. R-CNN 계열의 알고리즘에서 초기에는 Selective Search와 같은 기법을 사용했으나, Faster R-CNN에서는 이 과정을 Region Proposal Network (RPN)라는 별도의 신경망을 통해 학습함으로써 효율성을 높였다.1 두 번째 단계에서는 생성된 각 후보 영역에 대해 특징을 추출하고, 이를 기반으로 객체의 클래스를 분류하며 경계 박스의 위치를 보다 정밀하게 조정한다.2 이러한 2단계 접근법은 각 단계를 전문화하여 높은 정확도를 달성할 수 있다는 장점이 있지만, 다단계 파이프라인으로 인해 추론 속도가 상대적으로 느리다는 본질적인 한계를 가진다.3</p>
<p>반면, 1단계 검출기는 이러한 다단계 과정을 단일 신경망 내에서 통합적으로 처리한다. 후보 영역 생성 단계를 생략하고, 입력 이미지를 한 번만 네트워크에 통과시켜(single pass) 객체의 위치와 클래스를 동시에 예측한다.3 이 방식은 구조가 단순하고 모든 계산이 단일 네트워크에서 이루어지기 때문에 2단계 검출기에 비해 월등히 빠른 추론 속도를 자랑한다.1 YOLO(You Only Look Once)와 SSD(Single Shot MultiBox Detector)가 이 패러다임의 대표적인 알고리즘이다. 초기 1단계 검출기들은 속도에서 큰 이점을 보였지만, 2단계 검출기에 비해 정확도가 다소 떨어지는 경향이 있었다.2</p>
<p>이러한 배경 속에서, 정확도 우선주의에서 실시간 처리 가능성으로 패러다임의 전환을 이끈 중요한 모델이 바로 SSD이다. 기존의 2단계 검출기들이 7 FPS(Frames Per Second) 수준의 속도에 머물러 실시간 응용에 한계가 있었던 반면 6, SSD는 객체 탐지 문제를 단일 회귀 문제로 재정의하여 실시간에 가까운 성능을 구현했다. 이는 단순히 점진적인 개선이 아닌, 후보 영역 제안 단계의 필요성에 대한 근본적인 재고를 촉발시킨 개념적 도약이었다. SSD의 등장은 자율 주행, 실시간 감시 등 새로운 응용 분야의 문을 열었으며 3, 이후 연구 커뮤니티가 속도 이점을 유지하면서 정확도 격차를 줄이는 방향으로 나아가게 하는 중요한 계기가 되었다.</p>
<h3>1.2 SSD의 개념과 핵심 기여</h3>
<p>SSD(Single Shot MultiBox Detector)는 2015년 Wei Liu 등에 의해 제안된 1단계 객체 탐지 알고리즘으로, 단일 심층 신경망을 사용하여 이미지 내 객체를 탐지한다.7 SSD의 가장 핵심적인 기여는 2단계 검출기의 병목이었던 후보 영역 생성 및 후속 특징 재샘플링(feature resampling) 단계를 완전히 제거하고, 모든 연산을 단일 네트워크에 통합한 것이다.9 이를 통해 훈련과 추론을 위한 통합된 프레임워크를 제공하며, 이전의 1단계 검출기인 YOLO보다 훨씬 높은 정확도를 달성했고, 동시에 Faster R-CNN과 같은 2단계 검출기와 필적하는 정확도를 훨씬 빠른 속도로 구현했다.7</p>
<p>SSD의 성공은 단순히 단일 샷(single shot) 방식에만 기인하지 않는다. 그 이름에 담겨 있듯이 ’MultiBox’와 ’Multi-scale’이라는 두 가지 핵심 혁신의 시너지를 통해 정확도 격차를 극복했다. 첫째, 각 특징 맵의 위치마다 여러 종횡비(aspect ratio)를 가진 사전 정의된 ’기본 박스(Default Box)’를 사용하는 ‘MultiBox’ 개념을 도입했다.7 이는 “처음부터 박스를 예측하는” 어려운 문제를 “제시된 좋은 사전 형태를 미세 조정하는” 더 쉬운 문제로 전환시켰다. 둘째, 이러한 예측기들을 단일 특징 맵이 아닌, 서로 다른 해상도를 가진</p>
<p><em>여러</em> 특징 맵(Multi-scale feature maps)에 적용했다.11 깊고 해상도가 낮은 특징 맵은 큰 객체를, 얕고 해상도가 높은 특징 맵은 작은 객체를 담당하게 함으로써 다양한 크기의 객체에 효과적으로 대응할 수 있었다. 이 두 아이디어의 결합은 SSD가 이전 1단계 검출기들의 주요 약점을 극복하고 2단계 방식과 경쟁할 수 있는 정확도를 확보하게 한 원동력이었다. 이 다중 스케일, 다중 사전 형태(multi-prior) 접근법은 이후 RetinaNet, 후기 버전의 YOLO 등 거의 모든 고성능 검출기의 기본 설계 패턴으로 자리 잡게 되었다.</p>
<h2>2. SSD 아키텍처 상세</h2>
<p>SSD의 아키텍처는 특징 추출을 위한 ’기반 네트워크(Base Network)’와 다중 스케일 탐지를 위한 ’추가 특징 레이어(Extra Feature Layers)’라는 두 가지 주요 구성 요소로 이루어진다.</p>
<h3>2.1  기반 네트워크: VGG-16</h3>
<p>SSD는 사전 훈련된 이미지 분류 네트워크를 특징 추출기로 사용하며, 원본 논문에서는 VGG-16 아키텍처를 기반으로 했다.7 당시 VGG-16은 강력한 특징 추출 능력과 범용성으로 널리 사용되던 모델이었으며, 이는 연구자들이 새로운 탐지 헤드 구조에 집중할 수 있게 한 실용적인 선택이었다.12</p>
<p>SSD는 표준 VGG-16 구조를 그대로 사용하지 않고, 객체 탐지에 적합하도록 몇 가지 수정을 가했다. 먼저, VGG-16의 마지막 분류 단계에 해당하는 완전 연결 레이어(<code>fc6</code>, <code>fc7</code>)를 컨볼루션 레이어로 변환했다. 이 과정에서 파라미터는 서브샘플링(subsampling)을 통해 가져온다. 또한, <code>pool5</code> 레이어의 커널 크기는 <code>2x2</code>, 스트라이드(stride)는 2에서 커널 크기 <code>3x3</code>, 스트라이드 1로 변경하여 특징 맵의 해상도 감소를 완화했다. 이로 인해 발생하는 “구멍“을 메우기 위해 à trous 알고리즘이 사용된다. 마지막으로, 과적합 방지를 위한 드롭아웃(dropout) 레이어와 최종 분류 레이어인 <code>fc8</code>은 제거된다.7 이렇게 수정된 VGG-16 네트워크는 입력 이미지로부터 풍부한 의미론적 특징을 추출하는 역할을 수행한다.13</p>
<p>그러나 VGG-16을 기반으로 한 이 아키텍처 선택은 SSD의 강점이자 동시에 치명적인 약점의 원인이 되었다. VGG-16은 구조적으로 공격적인 다운샘플링을 수행하기 때문에, 작은 객체 탐지에 주로 사용되는 네트워크의 얕은 층(예: <code>conv4_3</code>)은 상대적으로 큰 수용장(receptive field)을 가지지만, 고수준의 의미론적 정보가 부족하다는 문제를 안고 있다.6 이 층에서 추출된 특징은 객체의 클래스를 판별하기에는 너무 저수준(모서리, 색상 등)의 정보에 머물러 있었다. 이 구조적 결정은 SSD가 작은 객체에 대해 낮은 성능을 보이는 근본적인 원인이 되었으며, 이는 원본 논문에서도 인정하고 후속 연구들에서 집중적으로 다루어진 한계점이다.15 이는 의미론적 풍부함과 공간적 해상도 사이의 근본적인 상충 관계를 보여주며, 이 문제를 해결하려는 노력이 FPN, DSSD와 같은 상향식 경로(top-down pathway)와 특징 융합(feature fusion)을 갖춘 더 복잡한 아키텍처의 개발을 직접적으로 촉진했다.</p>
<h3>2.2  추가 특징 레이어</h3>
<p>SSD는 수정된 VGG-16 기반 네트워크의 끝단에 점진적으로 크기가 작아지는 일련의 보조 컨볼루션 레이어(auxiliary convolutional layers)를 추가한다.11 이 추가 레이어들은 다양한 스케일에서 객체를 탐지할 수 있도록 설계되었으며, 각 레이어는 서로 다른 해상도의 특징 맵을 생성한다.6 예를 들어, 300x300 입력 이미지를 사용하는 SSD300 모델의 경우, VGG-16의 <code>conv7</code>(변환된 <code>fc7</code>) 이후에 <code>conv8_2</code>, <code>conv9_2</code>, <code>conv10_2</code>, <code>conv11_2</code>와 같은 레이어들이 추가되어 더 낮은 해상도의 특징 맵을 생성한다.7</p>
<h3>2.3  다중 스케일 특징 맵을 이용한 예측</h3>
<p>SSD의 핵심적인 특징 중 하나는 단일 스케일이 아닌 다중 스케일의 특징 맵에서 예측을 수행한다는 점이다. 예측은 기반 네트워크의 일부 중간 레이어(예: <span class="math math-inline">conv4_3</span>)와 추가된 보조 레이어들에서 각각 독립적으로 이루어진다.7 이 접근법은 단일 네트워크 내에서 이미지 피라미드(image pyramid)와 유사한 효과를 구현하여, 추가적인 연산 없이 다양한 크기의 객체를 효과적으로 처리할 수 있게 한다.</p>
<p>구체적으로, 네트워크의 얕은 층에서 생성된 고해상도 특징 맵은 상대적으로 작은 수용장을 가지므로 작은 객체를 탐지하는 데 사용된다. 반대로, 네트워크의 깊은 층과 추가된 보조 레이어에서 생성된 저해상도 특징 맵은 큰 수용장을 가지므로 큰 객체를 탐지하는 데 적합하다.11 이처럼 각기 다른 스케일의 특징 맵이 특정 크기의 객체를 전담하도록 함으로써, SSD는 단일 샷 구조의 속도를 유지하면서도 스케일 변화에 강건한 탐지 성능을 확보한다. SSD300 아키텍처에서 예측에 사용되는 주요 레이어와 각 레이어에서 생성되는 기본 박스의 수는 다음 표와 같다.</p>
<p><strong>표 1: SSD300 아키텍처의 예측 레이어 요약</strong></p>
<table><thead><tr><th>예측 레이어 (Prediction Layer)</th><th>특징 맵 크기 (Feature Map Size)</th><th>셀 당 기본 박스 수 (Default Boxes per Cell, k)</th><th>총 기본 박스 수 (Total Default Boxes)</th></tr></thead><tbody>
<tr><td><span class="math math-inline">conv4_3</span></td><td><span class="math math-inline">38 x 38</span></td><td>4</td><td><span class="math math-inline">38 * 38 * 4 = 5776</span></td></tr>
<tr><td><span class="math math-inline">conv7</span> (fc7)</td><td><span class="math math-inline">19 x 19</span></td><td>6</td><td><span class="math math-inline">19 * 19 * 6 = 2166</span></td></tr>
<tr><td><span class="math math-inline">conv8_2</span></td><td><span class="math math-inline">10 x 10</span></td><td>6</td><td><span class="math math-inline">10 * 10 * 6 = 600</span></td></tr>
<tr><td><span class="math math-inline">conv9_2</span></td><td><span class="math math-inline">5 x 5</span></td><td>6</td><td><span class="math math-inline">5 * 5 * 6 = 150</span></td></tr>
<tr><td><span class="math math-inline">conv10_2</span></td><td><span class="math math-inline">3 x 3</span></td><td>4</td><td><span class="math math-inline">3 * 3 * 4 = 36</span></td></tr>
<tr><td><span class="math math-inline">conv11_2</span></td><td><span class="math math-inline">1 x 1</span></td><td>4</td><td><span class="math math-inline">1 * 1 * 4 = 4</span></td></tr>
<tr><td><strong>총계 (Total)</strong></td><td>-</td><td>-</td><td><strong>8732</strong></td></tr>
</tbody></table>
<h2>3. 핵심 메커니즘</h2>
<p>SSD의 성공적인 성능은 ’기본 박스(Default Boxes)’와 ’컨볼루션 예측기(Convolutional Predictors)’라는 두 가지 핵심 메커니즘에 기반한다. 이들은 함께 작동하여 단일 네트워크 내에서 효율적이고 정확한 객체 탐지를 가능하게 한다.</p>
<h3>3.1  기본 박스 (Default Boxes)</h3>
<p>기본 박스는 Faster R-CNN의 앵커 박스(Anchor Box)와 유사한 개념으로, 각 예측 특징 맵의 모든 셀(cell) 위치에 대해 사전에 정의된 경계 박스 집합이다.7 이 박스들은 다양한 크기(scale)와 종횡비(aspect ratio)를 가지며, 객체의 가능한 형태와 크기에 대한 사전 가정(prior) 역할을 한다.11 네트워크는 이 기본 박스를 기반으로 실제 객체의 위치에 맞게 미세 조정(offset)하는 방식으로 예측을 수행한다.</p>
<h4>3.1.1 스케일 및 종횡비 계산</h4>
<p>기본 박스의 설계는 학습되는 것이 아니라, 사전에 정의된 규칙에 따라 결정되는 휴리스틱(heuristic) 방식이다. 이는 훈련 과정을 단순화하고 계산 효율성을 높이는 장점이 있지만, 동시에 데이터셋의 객체 분포와 사전 정의된 박스의 형태가 잘 맞지 않을 경우 성능 저하의 원인이 될 수 있는 한계도 내포한다.6</p>
<p><span class="math math-inline">m</span>개의 예측 특징 맵이 있을 때, <span class="math math-inline">k</span>번째 특징 맵(<span class="math math-inline">k \in [1, m]</span>)에 대한 기본 박스의 스케일 <span class="math math-inline">s_k</span>는 다음의 선형 증가 공식으로 계산된다 7:<br />
<span class="math math-display">
s_k = s_{min} + \frac{s_{max} - s_{min}}{m-1}(k-1)
</span><br />
SSD300 모델의 경우, 가장 낮은 층의 스케일인 <span class="math math-inline">s_{\min}</span>은 0.2, 가장 높은 층의 스케일인 <span class="math math-inline">s_{\max}</span>는 0.9로 설정된다. 이는 가장 얕은 예측 층의 기본 박스가 이미지 크기의 20%를, 가장 깊은 층의 기본 박스가 90%를 차지하도록 설계되었음을 의미한다.</p>
<p>각 스케일에 대해, 다양한 형태의 객체를 포착하기 위해 여러 종횡비 <span class="math math-inline">a_r</span>이 적용된다. 일반적으로 <span class="math math-inline">a_r \in \{1, 2, 3, 1/2, 1/3\}</span>의 집합이 사용된다.6 이 스케일과 종횡비를 이용하여 각 기본 박스의 너비(<span class="math math-inline">w</span>)와 높이(<span class="math math-inline">h</span>)를 다음과 같이 계산한다 7:<br />
<span class="math math-display">
w_k^a = s_k \sqrt{a_r} \quad \text{and} \quad h_k^a = s_k / \sqrt{a_r}
</span><br />
추가적으로, 종횡비가 1인 경우(<span class="math math-inline">a_r = 1</span>), 현재 층의 스케일(<span class="math math-inline">s_k</span>)과 다음 층의 스케일(<span class="math math-inline">s_{k+1}</span>)의 기하 평균에 해당하는 스케일 <span class="math math-inline">s&#39;_k = \sqrt{s_k s_{k+1}}</span>을 갖는 정사각형 박스를 하나 더 추가한다. 이로 인해 대부분의 특징 맵 위치에서는 총 6개의 기본 박스가 생성된다.7 이러한 수동적이고 신중하게 설계된 휴리스틱은 SSD가 다양한 객체 형태에 효과적으로 대응하면서도, 학습해야 할 파라미터 수를 줄여 훈련을 단순화하는 데 기여했다. 그러나 이는 동시에 SSD의 성능이 사전 정의된 앵커들의 분포에 강하게 의존하게 만들어, 이후 ‘앵커 프리(anchor-free)’ 검출기 연구를 촉발하는 계기가 되었다.</p>
<h3>3.2  컨볼루션 예측기</h3>
<p>SSD는 각 예측 레이어에서 작은 크기의 컨볼루션 필터(주로 <span class="math math-inline">3x3</span>)를 사용하여 각 기본 박스에 대한 예측을 생성한다.7 이는 완전 연결 레이어를 사용하는 대신 컨볼루션 연산을 통해 위치와 클래스를 예측함으로써 파라미터 수를 줄이고 계산 효율성을 높이는 방식이다.</p>
<p>각 특징 맵의 모든 위치에서, <span class="math math-inline">k</span>개의 기본 박스 각각에 대해 두 종류의 예측이 이루어진다:</p>
<ol>
<li><strong>클래스 신뢰도(Class Confidence):</strong> 각 클래스(배경 포함)에 대한 신뢰도 점수. 총 <span class="math math-inline">c</span>개의 클래스가 있다면, <span class="math math-inline">c</span>개의 점수를 예측한다.</li>
<li><strong>위치 오프셋(Location Offset):</strong> 기본 박스의 중심 좌표(<span class="math math-inline">c_x, c_y</span>)와 너비(<span class="math math-inline">w</span>), 높이(<span class="math math-inline">h</span>)를 실제 객체 박스에 맞추기 위한 4개의 조정 값.</li>
</ol>
<p>따라서 각 위치에서는 총 <span class="math math-inline">k * (c + 4)</span>개의 값이 예측된다. 예를 들어, <span class="math math-inline">19x19</span> 크기의 특징 맵에서 셀당 6개의 기본 박스(<span class="math math-inline">k=6</span>)를 사용하고, PASCAL VOC 데이터셋(20개 클래스 + 배경 1개 = 21개)을 가정하면(<span class="math math-inline">c=21</span>), 해당 특징 맵에서만 <span class="math math-inline">19 * 19 * 6 * (21 + 4) = 54,150</span>개의 출력이 생성된다. 이 모든 예측은 단일 순방향 패스(forward pass)에서 병렬적으로 계산된다.</p>
<h2>4. 훈련 과정</h2>
<p>SSD 모델을 효과적으로 훈련시키기 위해서는 예측된 수많은 박스들을 실제 객체(Ground Truth)와 연결하고, 그 차이를 줄여나가는 정교한 과정이 필요하다. 이 과정은 크게 ‘매칭 전략’, ‘MultiBox 손실 함수’, ‘어려운 네거티브 마이닝’, 그리고 ’데이터 증강’의 네 단계로 구성된다.</p>
<h3>4.1  매칭 전략: 자카드 중첩</h3>
<p>SSD는 훈련 과정에서 8732개에 달하는 방대한 수의 기본 박스를 생성한다. 이 예측들을 지도하기 위해, 어떤 기본 박스가 어떤 실제 객체를 담당해야 하는지를 결정하는 ’매칭 전략’이 필요하다. SSD는 자카드 중첩(Jaccard Overlap), 즉 IoU(Intersection over Union)를 기준으로 이 매칭을 수행한다.18</p>
<p>매칭 과정은 두 단계로 이루어진다:</p>
<ol>
<li>먼저, 이미지 내의 각 실제 객체 박스에 대해, 모든 기본 박스와의 IoU를 계산하여 가장 높은 IoU 값을 갖는 기본 박스를 찾아 매칭시킨다. 이 규칙은 모든 실제 객체가 최소한 하나의 ‘포지티브(positive)’ 기본 박스와 연결되도록 보장한다.20</li>
<li>그 다음, 남은 기본 박스들에 대해, 특정 임계값(일반적으로 0.5) 이상의 IoU를 갖는 실제 객체 박스가 있다면 해당 기본 박스 역시 포지티브로 매칭시킨다.17</li>
</ol>
<p>이 2단계 전략은 의도적인 설계의 결과이다. 이미지당 객체 수는 적고 기본 박스는 매우 많기 때문에, 단순히 가장 좋은 박스 하나만 매칭할 경우 포지티브 샘플의 수가 극도로 부족해진다. 두 번째 규칙을 통해 “최고는 아니지만 충분히 좋은” 예측들을 포지티브 샘플 풀에 추가함으로써, 모델이 위치 회귀를 학습하는 데 필요한 훈련 신호를 풍부하게 제공한다.20 이처럼 하나의 실제 객체는 여러 개의 포지티브 기본 박스와 매칭될 수 있다. 이 두 규칙에 따라 매칭되지 않은 모든 기본 박스는 ‘네거티브(negative)’ 샘플, 즉 배경으로 간주된다.11</p>
<h3>4.2  MultiBox 손실 함수</h3>
<p>SSD의 손실 함수는 위치 손실(<span class="math math-inline">L_{loc}</span>)과 신뢰도 손실(<span class="math math-inline">L_{conf}</span>)의 가중 합으로 구성된다.12 전체 손실 함수는 다음과 같이 정의된다:<br />
<span class="math math-display">
L(x, c, l, g) = \frac{1}{N} (L_{conf}(x, c) + \alpha L_{loc}(x, l, g))
</span><br />
여기서 각 기호는 다음을 의미한다 7:</p>
<ul>
<li><span class="math math-inline">N</span>: 포지티브로 매칭된 기본 박스의 총 개수.</li>
<li><span class="math math-inline">x_{ij}^p</span>: <span class="math math-inline">i</span>번째 기본 박스가 <span class="math math-inline">p</span> 클래스의 <span class="math math-inline">j</span>번째 실제 객체와 매칭되면 1, 아니면 0인 표시자(indicator).</li>
<li><span class="math math-inline">c</span>: 예측된 클래스 신뢰도.</li>
<li><span class="math math-inline">l</span>: 예측된 위치 오프셋.</li>
<li><span class="math math-inline">g</span>: 실제 객체 박스 좌표.</li>
<li><span class="math math-inline">\alpha</span>: 두 손실 간의 가중치를 조절하는 하이퍼파라미터로, 교차 검증을 통해 일반적으로 1로 설정된다.</li>
</ul>
<h4>4.2.1 위치 손실 (<span class="math math-inline">L_{loc}</span>): Smooth L1 손실</h4>
<p>위치 손실은 포지티브로 매칭된 기본 박스에 대해서만 계산된다. 이 손실은 예측된 경계 박스(<span class="math math-inline">l</span>)와 실제 객체 박스(<span class="math math-inline">g</span>) 간의 차이를 측정하며, Faster R-CNN과 마찬가지로 Smooth L1 손실 함수를 사용한다.12 Smooth L1 손실은 L1 손실과 L2 손실의 장점을 결합한 형태로, 오차가 작을 때는 L2처럼 부드럽게, 클 때는 L1처럼 이상치(outlier)에 덜 민감하게 반응하여 안정적인 학습을 돕는다. 네트워크는 실제 좌표를 직접 예측하는 대신, 기본 박스(<span class="math math-inline">d</span>)의 중심(<span class="math math-inline">c_x, c_y</span>), 너비(<span class="math math-inline">w</span>), 높이(<span class="math math-inline">h</span>)에 대한 오프셋(offset)을 회귀하도록 학습한다. 위치 손실의 구체적인 공식은 다음과 같다 7:<br />
<span class="math math-display">
L_{loc}(x, l, g) = \sum_{i \in Pos}^{N} \sum_{m \in \{cx, cy, w, h\}} x_{ij}^{k} \text{smooth}_{L1}(l_i^m - \hat{g}_j^m)
</span><br />
여기서 <span class="math math-inline">\hat{g}_j^m</span>는 실제 객체 박스 좌표 <span class="math math-inline">g_j</span>를 <span class="math math-inline">i</span>번째 기본 박스 <span class="math math-inline">d_i</span>를 기준으로 인코딩한 목표 값이다.</p>
<h4>4.2.2 신뢰도 손실 (<span class="math math-inline">L_{conf}</span>): Softmax 손실</h4>
<p>신뢰도 손실은 각 기본 박스가 올바른 클래스를 예측하도록 학습시키는 역할을 한다. 이 손실은 모든 포지티브 샘플과, 후술할 ’어려운 네거티브 마이닝’을 통해 선택된 네거티브 샘플에 대해 계산된다. 다중 클래스 분류를 위해 Softmax 손실(Cross-Entropy Loss)이 사용된다.12 신뢰도 손실의 공식은 다음과 같다 7:<br />
<span class="math math-display">
L_{conf}(x, c) = - \sum_{i \in Pos}^{N} x_{ij}^{p} \log(\hat{c}_i^p) - \sum_{i \in Neg} \log(\hat{c}_i^0)
</span><br />
여기서 <span class="math math-inline">\hat{c}_i^p</span>는 <span class="math math-inline">i</span>번째 기본 박스가 클래스 <span class="math math-inline">p</span>에 속할 예측 확률을 나타내는 Softmax 출력이다. 첫 번째 항은 포지티브 샘플에 대한 손실로, 올바른 객체 클래스의 확률을 높이도록 유도한다. 두 번째 항은 선택된 네거티브 샘플에 대한 손실로, 배경 클래스(인덱스 0)의 확률을 높이도록, 즉 객체가 아니라고 예측하도록 유도한다.</p>
<h3>4.3  어려운 네거티브 마이닝 (Hard Negative Mining)</h3>
<p>매칭 전략의 결과로, 8732개의 기본 박스 중 극소수만이 포지티브 샘플이 되고 압도적인 다수가 네거티브 샘플이 된다. 이러한 극심한 클래스 불균형 상태에서 모든 네거티브 샘플을 훈련에 사용하면, 손실 함수의 대부분이 배경 예측에서 발생하게 되어 모델이 객체 탐지보다 배경 분류에 치중하게 된다.6</p>
<p>이 문제를 해결하기 위해 SSD는 ‘어려운 네거티브 마이닝(Hard Negative Mining, HNM)’ 기법을 사용한다. 이 기법은 모든 네거티브 샘플을 사용하는 대신, 모델이 가장 헷갈려하는, 즉 배경임에도 불구하고 높은 객체 신뢰도를 예측하여 큰 손실 값을 발생시키는 ‘어려운’ 네거티브 샘플만을 선별하여 훈련에 사용한다.6 구체적으로, 모든 네거티브 샘플에 대해 신뢰도 손실을 계산한 뒤, 이 손실 값을 기준으로 내림차순 정렬한다. 그리고 상위의 샘플들만 선택하여 포지티브 샘플과 네거티브 샘플의 비율이 대략 1:3을 넘지 않도록 유지한다.11 이 과정을 통해 훈련이 안정화되고 최적화 속도가 빨라진다.</p>
<p>HNM은 당시 조밀한 예측을 수행하는 검출기에서 클래스 불균형 문제를 해결하는 효과적인 방법이었으나, 계산적으로 비효율적인 측면이 있다. 훈련의 각 단계마다 모든 네거티브 샘플에 대한 순방향 전파(forward pass)와 손실 계산, 그리고 정렬 과정이 필요하기 때문이다. 이러한 비효율성은 이후 RetinaNet에서 제안된 Focal Loss와 같은 더 정교한 손실 함수 설계의 동기가 되었다.23 Focal Loss는 손실 함수 자체를 변형하여, 잘 분류되는 쉬운 샘플(대부분의 네거티브)의 손실 기여도를 동적으로 감소시킴으로써 HNM과 동일한 목표를 더 효율적이고 우아하게 달성했다.</p>
<h3>4.4  데이터 증강 (Data Augmentation)</h3>
<p>SSD는 모델의 강건성(robustness)과 일반화 성능을 높이기 위해 강력한 데이터 증강 전략을 채택했다. 단순한 기하학적 변환을 넘어, 객체의 스케일 변화에 대한 대응 능력을 키우는 데 초점을 맞춘다. 각 훈련 이미지는 다음 옵션 중 하나를 통해 무작위로 샘플링된다 7:</p>
<ul>
<li>전체 원본 이미지를 그대로 사용한다.</li>
<li>이미지 내 객체들과의 최소 자카드 중첩(IoU)이 0.1, 0.3, 0.5, 0.7, 0.9가 되도록 패치(patch)를 샘플링한다. 이는 모델이 다양한 크기와 가려짐(occlusion) 상태의 객체를 학습하도록 강제한다.</li>
<li>무작위로 패치를 샘플링한다.</li>
</ul>
<p>이러한 샘플링 전략은 마치 객체를 ’확대(zoom in)’하거나 ’축소(zoom out)’하는 효과를 주어, 모델이 다양한 스케일의 객체에 강건해지도록 돕는다. 이 외에도 수평 뒤집기(horizontal flipping)와 같은 일반적인 기하학적 변환과, 밝기, 대비, 채도 등을 무작위로 변경하는 광도 왜곡(photometric distortions)과 같은 색상 관련 증강 기법도 함께 적용하여 모델의 일반화 성능을 극대화한다.24</p>
<h2>5. 추론 및 후처리</h2>
<p>훈련된 SSD 네트워크는 추론(inference) 시 입력 이미지 한 장에 대해 수천 개(SSD300의 경우 8732개)의 경계 박스 예측을 생성한다. 이 예측들은 각 기본 박스에 대한 클래스 신뢰도와 위치 오프셋을 포함한다. 그러나 이 원시(raw) 출력에는 동일한 객체에 대한 수많은 중복 예측과 신뢰도가 낮은 배경 예측이 포함되어 있으므로, 최종 탐지 결과를 얻기 위해서는 후처리(post-processing) 과정이 필수적이다. 이 과정의 핵심은 ‘비-최대 억제(Non-Maximum Suppression, NMS)’ 알고리즘이다.</p>
<h3>5.1  비-최대 억제 (Non-Maximum Suppression, NMS)</h3>
<p>NMS는 동일한 객체를 가리키는 여러 개의 중복된 경계 박스 중에서 가장 정확한 박스 하나만을 남기고 나머지를 제거하는 알고리즘이다.26 SSD의 후처리 파이프라인은 다음과 같은 단계로 진행된다:</p>
<ol>
<li>
<p><strong>신뢰도 임계값 적용 (Confidence Thresholding):</strong> 먼저, 모든 예측 박스 중에서 클래스 신뢰도 점수가 매우 낮은(예: 0.01) 박스들을 제거한다. 이는 대부분 명백한 배경 영역에 해당하므로, 이후 NMS 연산의 계산 부담을 줄여준다.19</p>
</li>
<li>
<p><strong>클래스별 NMS 수행:</strong> 남은 박스들에 대해 각 객체 클래스별로 독립적으로 NMS를 수행한다. 특정 클래스에 대한 NMS 알고리즘은 다음과 같다 26:</p>
</li>
</ol>
<p>a. 해당 클래스에 대한 모든 예측 박스를 신뢰도 점수 기준으로 내림차순 정렬한다.</p>
<p>b. 정렬된 목록에서 가장 높은 신뢰도를 가진 박스를 선택하여 최종 탐지 결과 목록에 추가하고, 원래 목록에서는 제거한다.</p>
<p>c. 선택된 박스와 원래 목록에 남아있는 모든 박스 간의 IoU를 계산한다.</p>
<p>d. 계산된 IoU가 미리 설정된 임계값(예: 0.45)을 초과하는 박스들을 목록에서 제거(억제)한다. 이들은 가장 신뢰도 높은 박스와 동일한 객체를 탐지한 중복 예측으로 간주된다.</p>
<p>e. 목록에 더 이상 박스가 남아있지 않을 때까지 b~d 단계를 반복한다.</p>
<ol start="3">
<li><strong>최종 결과 생성:</strong> 모든 클래스에 대해 NMS를 수행한 후, 최종 탐지 결과 목록에 포함된 박스들이 최종 출력으로 반환된다.</li>
</ol>
<p>NMS는 SSD의 출력에서 필수적인 정제 과정이지만, 이는 네트워크 자체가 중복 탐지를 억제하도록 학습하지 못한다는 한계를 드러낸다. NMS는 학습 불가능한(non-learnable) 휴리스틱 알고리즘이며, 그 성능은 IoU 임계값이라는 하이퍼파라미터에 민감하다.29 임계값이 너무 낮으면 서로 겹쳐있는 다른 객체들을 잘못 제거할 수 있고, 너무 높으면 중복된 탐지를 남길 수 있다. 이러한 의존성은 NMS를 네트워크 아키텍처나 훈련 과정에 통합하여 파이프라인을 단순화하고 진정한 의미의 종단간(end-to-end) 학습을 구현하려는 후속 연구들을 촉발시켰다.26</p>
<h2>6. 성능 분석 및 비교</h2>
<p>SSD의 가치는 절대적인 성능 수치뿐만 아니라, 당시의 다른 주요 모델들과의 비교를 통해 가장 잘 이해될 수 있다. 본 장에서는 주요 벤치마크 데이터셋에서의 성능을 분석하고, 모델의 본질적인 장단점, 특히 작은 객체 탐지의 한계를 심층적으로 탐구한다.</p>
<h3>6.1  성능 평가: PASCAL VOC 및 COCO 데이터셋</h3>
<p>SSD는 PASCAL VOC, MS COCO, ILSVRC와 같은 표준 객체 탐지 벤치마크 데이터셋에서 그 성능을 입증했다.7 원본 논문에 따르면, 300x300 크기의 이미지를 입력으로 사용하는 SSD300 모델은 PASCAL VOC2007 test 데이터셋에서 74.3%의 mAP(mean Average Precision)를 기록하며, Nvidia Titan X GPU 기준으로 59 FPS의 빠른 추론 속도를 보였다. 입력 이미지 크기를 512x512로 늘린 SSD512 모델은 mAP를 76.9%까지 향상시켰으며, 이는 당시 최첨단 2단계 검출기였던 Faster R-CNN의 성능(73.2% mAP)을 능가하는 수치였다.7 이 결과는 SSD가 2단계 검출기의 정확도에 근접하면서도 실시간 처리가 가능한 속도를 제공할 수 있음을 명확히 보여주었다.</p>
<h3>6.2  장단점 분석: 작은 객체 탐지의 한계</h3>
<p>SSD의 가장 두드러진 약점은 작은 객체에 대한 낮은 탐지 정확도이다.6 이 문제는 단일 요인이 아닌, SSD의 아키텍처, 특징 표현 방식, 그리고 훈련 전략이 복합적으로 작용한 결과로 발생하는 시스템적인 한계이다.</p>
<ol>
<li><strong>아키텍처적 원인:</strong> SSD는 VGG-16과 같은 순방향(feed-forward) 네트워크를 기반으로 한다. 이러한 구조에서는 네트워크의 깊이가 깊어질수록 특징 맵의 공간 해상도는 감소하고 의미론적 정보는 풍부해진다. 작은 객체는 주로 공간 해상도가 높은 네트워크의 얕은 층(예: <span class="math math-inline">conv4_3</span>)에서 탐지되어야 하는데, 이 층들은 충분한 컨볼루션 연산을 거치지 않아 고수준의 의미론적 정보를 거의 담고 있지 않다.30 즉, 모델은 ’무엇’인지 판단하기에 불충분한 저수준 특징(모서리, 색상 등)만을 가지고 작은 객체를 분류해야 하는 어려움에 직면한다.6</li>
<li><strong>특징 표현의 한계:</strong> 얕은 층의 특징은 깊은 층의 풍부한 의미론적 정보와 융합되지 않고 독립적으로 예측에 사용된다.15 이로 인해 작은 객체 주변의 문맥(context) 정보를 활용하기 어렵고, 배경과 작은 객체를 구분하는 능력이 저하된다.</li>
<li><strong>훈련 전략의 문제:</strong> 고정된 IoU 임계값을 사용하는 매칭 전략은 작은 객체에 대해 불안정하게 작용할 수 있다. 작은 객체의 경우, 경계 박스에서 단 몇 픽셀만 이동해도 IoU 값이 크게 변동하기 때문에, 훈련 과정에서 포지티브/네거티브 샘플 할당이 일관되지 않을 수 있다. 이는 작은 객체에 대한 학습 효율을 감소시키는 원인이 된다.16</li>
</ol>
<p>이러한 복합적인 요인들은 작은 객체 탐지라는 특정 실패 모드(failure mode)를 야기한다. 이 문제를 해결하기 위해서는 단순히 파라미터를 조정하는 수준을 넘어, 네트워크 내 정보의 흐름을 근본적으로 바꾸는 구조적 개선이 필요하다. 이는 DSSD와 FSSD 같은 후속 모델들이 디컨볼루션이나 특징 융합과 같은 새로운 모듈을 도입하여 얕은 특징 맵을 의미론적으로 강화하려는 시도로 이어졌다.</p>
<h3>6.3  타 모델과의 비교: Faster R-CNN 및 YOLO</h3>
<p>SSD의 혁신성은 당시의 주요 경쟁 모델인 Faster R-CNN(2단계 대표) 및 YOLO(1단계 대표)와의 비교를 통해 명확히 드러난다. 아래 표는 PASCAL VOC2007 test 데이터셋에서의 성능을 요약한 것이다.</p>
<p><strong>표 2: PASCAL VOC2007 test 성능 비교</strong></p>
<table><thead><tr><th>모델 (Model)</th><th>입력 크기 (Input Size)</th><th>mAP (%)</th><th>FPS</th></tr></thead><tbody>
<tr><td>Faster R-CNN (VGG-16)</td><td>~1000x600</td><td>73.2</td><td>7</td></tr>
<tr><td>YOLO (GoogLeNet)</td><td>448x448</td><td>63.4</td><td>45</td></tr>
<tr><td><strong>SSD300 (VGG-16)</strong></td><td><strong>300x300</strong></td><td><strong>74.3</strong></td><td><strong>59</strong></td></tr>
<tr><td><strong>SSD512 (VGG-16)</strong></td><td><strong>512x512</strong></td><td><strong>76.9</strong></td><td><strong>22</strong></td></tr>
</tbody></table>
<p>이 표는 당시 객체 탐지 분야의 속도-정확도 상충 관계(trade-off)를 명확하게 보여준다. Faster R-CNN은 가장 높은 정확도를 보였지만, 7 FPS라는 속도는 실시간 응용에 부적합했다. YOLO는 빠른 속도를 자랑했지만, 정확도에서 상당한 손실을 보였다. 반면, SSD는 이 두 모델 사이의 ’최적점(sweet spot)’에 위치한다. Faster R-CNN보다 빠르면서도 더 높은 정확도를 달성했고, YOLO보다는 다소 느리지만 월등히 높은 정확도를 기록했다. 이처럼 SSD는 실시간에 가까운 속도와 높은 정확도 사이의 균형을 성공적으로 달성함으로써, 1단계 검출기의 새로운 가능성을 제시했다.</p>
<h2>7. SSD 개선 모델</h2>
<p>SSD가 제시한 프레임워크는 매우 성공적이었지만, 작은 객체 탐지 성능이 낮다는 명확한 한계를 가지고 있었다. 이 문제를 해결하기 위해 수많은 후속 연구가 진행되었으며, 그중에서도 DSSD와 FSSD는 SSD의 구조적 한계를 극복하려는 대표적인 시도이다. 이 두 모델은 ’특징 융합(feature fusion)’이라는 공통된 아이디어를 각기 다른 철학으로 구현했다.</p>
<h3>7.1  DSSD: Deconvolutional Single Shot Detector</h3>
<p>DSSD(Deconvolutional Single Shot Detector)는 SSD의 작은 객체 탐지 성능을 극대화하기 위해 제안된 모델이다.31 DSSD는 성능 우선주의 접근법을 채택하여, 두 가지 주요한 구조적 변경을 도입했다.</p>
<p>첫째, 기반 네트워크를 기존의 VGG-16에서 당시 최첨단 분류 모델이었던 ResNet-101로 교체하여 특징 추출 능력을 강화했다.31 둘째, SSD의 단순한 순방향 보조 레이어 구조 대신, 디컨볼루션(deconvolution) 레이어를 포함하는 인코더-디코더와 유사한 구조를 도입했다.31 이 구조는 의미 분할(semantic segmentation) 분야에서 널리 사용되는 U-Net과 같은 모델에서 영감을 받은 것으로, 네트워크의 깊은 층에서 추출된 풍부한 의미론적 정보를 상향식 경로(top-down pathway)를 통해 얕은 층으로 전달한다. 스킵 연결(skip connection)을 통해 전달된 고수준의 컨텍스트 정보는 얕은 층의 고해상도 특징 맵과 융합되어, 작은 객체를 탐지하는 데 필요한 풍부하고 정제된 특징을 생성한다.14</p>
<p>DSSD는 의미 분할과 같은 조밀한 예측(dense prediction) 작업의 원리가 객체 탐지 성능 향상에 성공적으로 적용될 수 있음을 보여주었다. 이 아이디어는 이후 Feature Pyramid Network(FPN)와 같은 더욱 발전된 형태로 이어져, 현대 객체 검출기의 표준적인 구성 요소로 자리 잡는 데 큰 영향을 미쳤다.</p>
<h3>7.2  FSSD: Feature Fusion Single Shot Detector</h3>
<p>FSSD(Feature Fusion Single Shot Detector)는 DSSD와 동일하게 SSD의 특징 맵들이 서로 독립적으로 사용되어 정보 융합이 부족하다는 문제를 해결하고자 했으나, 보다 실용적이고 효율적인 접근법을 취했다.14 FSSD는 DSSD의 복잡하고 연산량이 많은 디컨볼루션 구조 대신, ’가볍고 효율적인 특징 융합 모듈’을 제안했다.14</p>
<p>FSSD의 특징 융합 모듈은 여러 스케일의 특징 맵들을 별도의 업샘플링 과정 없이 바로 연결(concatenate)한다. 그 후, 소수의 컨볼루션 레이어로 구성된 다운샘플링 블록을 통해 새로운 특징 피라미드를 생성하여 예측에 사용한다.14 이 방식은 DSSD에 비해 훨씬 적은 계산 비용으로 특징 융합을 구현하면서도, SSD의 성능을 크게 향상시켰다. 특히 작은 객체 탐지 성능에서 유의미한 개선을 보였으며, mAP를 크게 높이면서도 FPS 저하는 최소화했다.14</p>
<p>FSSD의 성공은 무겁고 복잡한 구조 없이도, 영리하고 효율적인 특징 융합 설계만으로 상당한 성능 향상을 이끌어낼 수 있음을 증명했다. 이는 단순히 특징을 융합하는 것을 넘어, 목표 응용 프로그램의 계산 예산을 고려하여 효율적으로 융합하는 것이 중요하다는 귀중한 설계 원칙을 제시했다. 성능과 효율성 사이의 이러한 균형점 탐색은 오늘날 신경망 아키텍처 설계의 핵심적인 주제로 남아 있다.</p>
<h2>8. 결론</h2>
<h3>8.1 SSD의 영향력과 유산</h3>
<p>SSD는 객체 탐지 분야의 역사에서 중요한 이정표를 세운 모델이다. 2단계 검출기의 높은 정확도와 1단계 검출기의 빠른 속도 사이의 간극을 성공적으로 메움으로써, 실시간 객체 탐지 패러다임의 실용적 가능성을 입증했다. 영역 제안 단계를 제거하고, 다중 스케일 특징 맵과 사전 정의된 기본 박스(앵커)를 결합하는 SSD의 핵심 설계 철학은 이후 등장하는 수많은 객체 검출기들의 청사진이 되었다. RetinaNet, YOLOv3 이후 버전 등 현대의 고성능 검출기들은 정도의 차이는 있지만 대부분 SSD가 제시한 기본 프레임워크 위에서 발전했다고 볼 수 있다. 이처럼 SSD는 단순히 하나의 성공적인 모델을 넘어, 객체 탐지 분야의 연구 방향을 정의하고 기술적 발전을 촉진한 유산을 남겼다.</p>
<h3>8.2 주요 한계점 및 후속 연구 방향 요약</h3>
<p>그럼에도 불구하고 SSD는 명확한 한계를 가지고 있었다. 가장 핵심적인 한계는 순방향 아키텍처와 독립적인 특징 맵 사용으로 인해 발생하는 작은 객체에 대한 낮은 탐지 성능이었다. 이 문제를 해결하려는 노력은 DSSD와 FSSD를 거쳐, 상향식 경로와 측면 연결을 통해 특징을 효과적으로 융합하는 FPN과 같은 더욱 정교한 아키텍처의 탄생으로 이어졌다.</p>
<p>또한, 훈련 과정에서 발생하는 극심한 클래스 불균형 문제를 해결하기 위해 사용된 어려운 네거티브 마이닝(HNM)은 효과적이었지만 계산적으로 비효율적이었다. 이는 손실 함수 자체를 변형하여 문제를 더 우아하게 해결하는 Focal Loss의 등장을 촉발했다. 마지막으로, 추론 파이프라인의 마지막 단계를 차지하는 비-최대 억제(NMS)에 대한 의존성은, 후처리 과정까지 학습 가능한 완전한 종단간(end-to-end) 모델에 대한 연구 방향을 제시하는 계기가 되었다. 결국 SSD의 한계점들은 그 자체로 후속 연구들이 해결해야 할 명확한 과제를 제시했으며, 이는 오늘날 객체 탐지 기술을 더욱 발전시키는 원동력이 되고 있다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>One stage vs two stage object detection - Stack Overflow, https://stackoverflow.com/questions/65942471/one-stage-vs-two-stage-object-detection</li>
<li>Single Stage Detector vs 2 Stage Detector | by Abhishek Jain | Medium, https://medium.com/@abhishekjainindore24/single-stage-detector-vs-2-stage-detector-3e540ea81213</li>
<li>One-Stage Object Detectors Explained - Ultralytics, https://www.ultralytics.com/glossary/one-stage-object-detectors</li>
<li>One-stage vs. two-stage detection - which one to choose for a project? : r/computervision, https://www.reddit.com/r/computervision/comments/1901faz/onestage_vs_twostage_detection_which_one_to/</li>
<li>An overview of object detection: one-stage methods. - Jeremy Jordan, https://www.jeremyjordan.me/object-detection-one-stage/</li>
<li>SSD object detection: Single Shot MultiBox Detector for real-time processing - Jonathan Hui, https://jonathan-hui.medium.com/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06</li>
<li>[1512.02325] SSD: Single Shot MultiBox Detector - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/1512.02325</li>
<li>[1512.02325] SSD: Single Shot MultiBox Detector - arXiv, https://arxiv.org/abs/1512.02325</li>
<li>(PDF) SSD: Single Shot MultiBox Detector - ResearchGate, https://www.researchgate.net/publication/308278279_SSD_Single_Shot_MultiBox_Detector</li>
<li>SSD: Single Shot MultiBox Detector - UNC Computer Science, https://www.cs.unc.edu/~wliu/papers/ssd.pdf</li>
<li>SSD for Object Detection | MindSpore 2.3.0-rc1 Tutorials, https://www.mindspore.cn/tutorials/application/en/r2.3.0rc1/cv/ssd.html</li>
<li>How single-shot detector (SSD) works? - GeeksforGeeks, https://www.geeksforgeeks.org/computer-vision/how-single-shot-detector-ssd-works/</li>
<li>How single-shot detector (SSD) works? | ArcGIS API for Python | Esri Developer, https://developers.arcgis.com/python/latest/guide/how-ssd-works/</li>
<li>FSSD: Feature Fusion Single Shot Multibox Detector - arXiv, https://arxiv.org/html/1712.00960v4</li>
<li>A Novel SSD-Based Detection Algorithm Suitable for Small Object - ResearchGate, https://www.researchgate.net/publication/370440415_A_Novel_SSD-Based_Detection_Algorithm_Suitable_for_Small_Object</li>
<li>Aligned Matching: Improving Small Object Detection in SSD - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC10007149/</li>
<li>SSD. Accurate real-time object detection | by Sanchit Tanwar | Analytics Vidhya | Medium, https://medium.com/analytics-vidhya/ssd-fec074ce57c</li>
<li>sgrvinod/a-PyTorch-Tutorial-to-Object-Detection: SSD: Single Shot MultiBox Detector, https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection</li>
<li>SSD : Understanding single shot object detection | Manal El Aidouni, <a href="https://manalelaidouni.github.io/Single%20shot%20object%20detection.html">https://manalelaidouni.github.io/Single%20shot%20object%20detection.html</a></li>
<li>Unravelling Single Shot Multi-box Detector (SSD) | by Akshay Shah | Medium, https://medium.com/@akshayhitendrashah/unravelling-single-shot-multi-box-detector-ssd-7def1834e925</li>
<li>Can someone give me an explanation for Multibox loss function? - Stack Overflow, https://stackoverflow.com/questions/57407108/can-someone-give-me-an-explanation-for-multibox-loss-function</li>
<li>Getting Started with SSD Multibox Detection - MATLAB &amp; Simulink - MathWorks, https://www.mathworks.com/help/vision/ug/getting-started-with-ssd.html</li>
<li>[1708.02002] Focal Loss for Dense Object Detection - arXiv, https://arxiv.org/abs/1708.02002</li>
<li>Data Augmentation Example with TensorFlow Lite (in iDrone), http://borg.csueastbay.edu/~grewe/CS663/Mat/DeepLearning/Training/DataAugmentationExample.html</li>
<li>Everything You Need To Know About Torchvision’s SSD Implementation - PyTorch, https://pytorch.org/blog/torchvision-ssd-implementation/</li>
<li>Non-Maximum Suppression (NMS) Explained - Ultralytics, https://www.ultralytics.com/glossary/non-maximum-suppression-nms</li>
<li>What is Non-Maximum Suppression? - GeeksforGeeks, https://www.geeksforgeeks.org/computer-vision/what-is-non-maximum-suppression/</li>
<li>Non Max Suppression (NMS) - Medium, https://medium.com/analytics-vidhya/non-max-suppression-nms-6623e6572536</li>
<li>A Beginner’s Guide to Non-Maximum Suppression in Object Detection - Akridata, https://akridata.ai/blog/non-maximum-suppression-object-detection-guide/</li>
<li>Small object detection algorithm based on improved SSD - SPIE Digital Library, https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13181/131812L/Small-object-detection-algorithm-based-on-improved-SSD/10.1117/12.3031256.full</li>
<li>(PDF) DSSD : Deconvolutional Single Shot Detector - ResearchGate, https://www.researchgate.net/publication/312759848_DSSD_Deconvolutional_Single_Shot_Detector</li>
<li>Variations of SSD — Understanding Deconvolutional Single-Shot Detectors - Medium, https://medium.com/@amadeusw6/variations-of-ssd-understanding-deconvolutional-single-shot-detectors-c0afb8686d03</li>
<li>[1701.06659] DSSD : Deconvolutional Single Shot Detector - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/1701.06659</li>
<li>[1712.00960] FSSD: Feature Fusion Single Shot Multibox Detector - arXiv, https://arxiv.org/abs/1712.00960</li>
<li>FSSD: Feature Fusion Single Shot Multibox Detector (1712.00960v4) - Emergent Mind, https://www.emergentmind.com/papers/1712.00960</li>
<li>(PDF) FSSD: Feature Fusion Single Shot Multibox Detector - ResearchGate, https://www.researchgate.net/publication/321511662_FSSD_Feature_Fusion_Single_Shot_Multibox_Detector</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>