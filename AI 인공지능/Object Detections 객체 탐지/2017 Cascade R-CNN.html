<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Cascade R-CNN (2017) 고품질 객체 탐지를 위한 다단계 아키텍처</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Cascade R-CNN (2017) 고품질 객체 탐지를 위한 다단계 아키텍처</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>Cascade R-CNN (2017) 고품질 객체 탐지를 위한 다단계 아키텍처</span></nav>
                </div>
            </header>
            <article>
                <h1>Cascade R-CNN (2017) 고품질 객체 탐지를 위한 다단계 아키텍처</h1>
<h2>1.  고품질 객체 탐지의 역설과 IoU 임계값 문제</h2>
<h3>1.1  객체 탐지에서의 ’품질’과 IoU의 역할</h3>
<p>객체 탐지(Object Detection)는 컴퓨터 비전의 핵심 분야로, 이미지 내에서 객체의 클래스를 판별하는 분류(classification) 문제와 객체의 정확한 위치를 경계 상자(bounding box)로 특정하는 위치 특정(localization) 문제를 동시에 해결해야 한다.1 이 중 탐지 결과의 품질, 특히 위치 특정의 정확도는 일반적으로 예측된 경계 상자와 실제 정답(ground-truth) 상자 간의 <strong><code>IoU</code>(Intersection over Union)</strong> 지표로 평가된다.2</p>
<p><code>IoU</code>는 두 영역의 교집합을 합집합으로 나눈 값으로, 0과 1 사이의 값을 가진다. 이 값은 모델이 얼마나 정확하게 객체의 위치를 예측했는지를 나타내는 직관적인 척도이며, 객체 탐지 모델의 학습 과정에서 Positive(객체)와 Negative(배경) 훈련 샘플을 정의하는 <strong>임계값(<code>u</code>)</strong> 설정의 기준으로 사용된다. 통상적으로 예측 상자와 실제 상자 간의 <code>IoU</code>가 설정된 임계값 <code>u</code> 이상일 경우 (<span class="math math-inline">IoU \geq u</span>) 해당 예측은 Positive로 간주된다.2</p>
<p>따라서 탐지기의 ’품질’은 해당 탐지기를 훈련시키는 데 사용된 IoU 임계값 <code>u</code>에 의해 정의된다고 볼 수 있다. 더 높은 <code>u</code> 값으로 훈련된 탐지기는 더 정밀한 위치 예측 능력을 갖도록, 즉 더 높은 품질의 탐지를 수행하도록 설계된다.1</p>
<h3>1.2  기존 2단계 탐지기의 한계: IoU 임계값의 딜레마</h3>
<p>Faster R-CNN과 같은 표준적인 2단계(two-stage) 탐지기는 일반적으로 <span class="math math-inline">u=0.5</span>라는 비교적 낮은 IoU 임계값을 사용한다.1 이러한 설정은 Positive로 간주되는 훈련 샘플을 충분히 확보하여 모델을 안정적으로 학습시킬 수 있다는 장점이 있다. 하지만 이처럼 느슨한 기준은 실제 객체의 위치와 약간 어긋난 예측, 즉 ’가까운 오탐지(close false positives)’를 효과적으로 제거하지 못하게 만든다. 결과적으로 <span class="math math-inline">u=0.5</span>로 훈련된 탐지기는 노이즈가 많고(noisy) 정밀하지 못한 탐지 결과를 생성하는 경향이 있다.1</p>
<p>그렇다면 탐지 품질을 높이기 위해 훈련 시 IoU 임계값 <code>u</code>를 높이면(예: <span class="math math-inline">u=0.7</span>) 성능이 향상될까? 직관과는 반대로, 임계값을 높이면 오히려 전체적인 탐지 성능(mAP, mean Average Precision)이 저하되는 <strong>‘고품질 탐지의 역설(paradox of high-quality detection)’</strong> 현상이 발생한다.1 이 역설의 근본적인 원인은 다음 두 가지로 분석된다.3</p>
<ol>
<li><strong>훈련 중 과적합 (Overfitting during Training)</strong>: IoU 임계값을 높이면 Positive로 분류될 수 있는 훈련 샘플의 수가 기하급수적으로 감소한다. 예를 들어, <span class="math math-inline">u=0.5</span> 기준으로는 Positive였던 많은 샘플들이 <span class="math math-inline">u=0.7</span> 기준에서는 Negative로 분류된다. 이는 대규모 데이터를 필요로 하는 심층 신경망의 특성상 심각한 데이터 부족 문제를 야기하며, 결국 모델이 소수의 고품질 샘플에만 과적합되는 결과로 이어진다.4</li>
<li><strong>추론 시 불일치 (Inference-time Mismatch)</strong>: 탐지기는 자신이 훈련된 IoU 수준(<code>u</code>)의 제안(hypotheses) 분포에 대해 최적의 성능을 발휘하도록 학습된다.3 그러나 RPN(Region Proposal Network)과 같은 제안 메커니즘이 생성하는 후보 영역들은 대부분 품질이 낮은, 즉 IoU가 낮은 제안들이다. 따라서 높은 <code>u</code> 값으로 훈련된 고품질 탐지기는 추론 시에 입력되는 저품질 제안의 분포와 심각한 불일치를 겪게 되어 제 성능을 발휘하지 못한다.3</li>
</ol>
<h3>1.3  Cascade R-CNN의 제안: 문제 해결을 위한 새로운 패러다임</h3>
<p>이러한 근본적인 문제들을 해결하기 위해, Zhaowei Cai와 Nuno Vasconcelos는 2018년 <strong>Cascade R-CNN</strong>이라는 새로운 아키텍처를 제안했다.1 이는 단일 탐지기를 사용하는 대신, 점진적으로 증가하는 IoU 임계값으로 순차적으로 훈련된 **탐지기들의 시퀀스(sequence of detectors)**로 구성된 다단계(multi-stage) 아키텍처이다.1</p>
<p>Cascade R-CNN의 핵심 아이디어는 ’분업’과 ’점진적 전문화’에 있다. 단일 탐지기에게 모든 품질 수준의 제안을 처리하라는 어려운 과제를 부여하는 대신, 각기 다른 전문성(IoU 임계값)을 가진 여러 전문가(탐지기 스테이지)를 순서대로 배치하여 문제를 해결한다. 각 스테이지는 이전 단계의 출력을 입력으로 받아 경계 상자를 점진적으로 정제하고, 이 과정을 통해 다음 단계를 위한 더 높은 품질의 훈련 샘플 분포를 효과적으로 생성(재샘플링)한다. 이 구조는 훈련 시 과적합을 방지하는 동시에, 추론 시에는 각 단계의 탐지기 품질과 입력 제안의 품질을 자연스럽게 일치시켜 전체적인 탐지 정확도를 극대화한다.1</p>
<h2>2.  Cascade R-CNN 아키텍처 상세</h2>
<h3>2.1  전체 구조 개요</h3>
<p>Cascade R-CNN은 Faster R-CNN과 같은 표준 2단계 탐지기 프레임워크를 기반으로 한 다단계 확장 구조를 가진다.1 전체적인 파이프라인은 이미지로부터 특징을 추출하는 <strong>백본 네트워크(Backbone Network)</strong>, 초기 후보 영역을 생성하는 <strong>RPN(Region Proposal Network)</strong>, 그리고 순차적으로 연결된 여러 개의 **탐지 헤드(Detection Head)**로 구성된다.10</p>
<p>아래 그림은 기존의 Faster R-CNN, 반복적 경계 상자 회귀(Iterative BBox), 그리고 Cascade R-CNN의 구조를 비교하여 보여준다. Cascade R-CNN은 각기 다른 가중치를 가진 헤드(<span class="math math-inline">H^1</span>, <span class="math math-inline">H^2</span>, <span class="math math-inline">H^3</span>)가 순차적으로 연결되어, 이전 단계의 경계 상자 출력(<span class="math math-inline">B^1</span>, <span class="math math-inline">B^2</span>)을 다음 단계의 입력으로 사용하는 독특한 캐스케이드(cascade) 구조를 형성한다.10</p>
<h3>2.2  구성 요소별 분석</h3>
<ul>
<li><strong>백본 네트워크 (Backbone Network)</strong>: 입력 이미지에서 의미론적 정보를 담은 계층적 특징 맵(feature map)을 추출하는 역할을 한다. ResNet, ResNeXt 등 다양한 심층 컨볼루션 신경망(CNN) 아키텍처가 사용될 수 있다.4 Cascade R-CNN의 강점 중 하나는 특정 백본에 국한되지 않고 다양한 아키텍처와 유연하게 결합하여 일관된 성능 향상을 보인다는 점이다.1</li>
<li><strong>영역 제안 네트워크 (Region Proposal Network, RPN)</strong>: 백본 네트워크가 추출한 특징 맵을 기반으로 객체가 존재할 가능성이 높은 수천 개의 후보 영역, 즉 RoI(Region of Interest) 또는 제안(proposals, <span class="math math-inline">B^0</span>)을 신속하게 생성한다.12 RPN이 생성한 제안들은 품질 분포가 매우 넓으며, 대부분 IoU가 낮은 저품질 제안들로 구성된다.3</li>
<li><strong>다단계 탐지 헤드 (Cascade of Detection Heads)</strong>: Cascade R-CNN 아키텍처의 핵심적인 부분이다. 일반적으로 3개의 스테이지로 구성되며, 각 스테이지는 구조적으로는 동일하지만 서로 다른 가중치를 가지며 독립적으로 훈련된 헤드(<span class="math math-inline">H^1</span>, <span class="math math-inline">H^2</span>, <span class="math math-inline">H^3</span>)로 이루어져 있다.10</li>
<li>각 헤드는 입력된 RoI에 해당하는 특징을 고정된 크기로 추출하는 RoI 풀링(또는 RoIAlign) 레이어와, 객체 클래스를 예측하는 분류기(<span class="math math-inline">C_t</span>), 그리고 경계 상자의 위치를 정제하는 회귀기(<span class="math math-inline">B_t</span>)로 구성된 서브 네트워크를 포함한다.12</li>
<li><strong>Stage 1 (<span class="math math-inline">H^1</span>)</strong>: RPN이 생성한 초기 제안(<span class="math math-inline">B^0</span>)을 입력받아 첫 번째 분류 점수(<span class="math math-inline">C^1</span>)와 한 단계 정제된 경계 상자(<span class="math math-inline">B^1</span>)를 출력한다.</li>
<li><strong>Stage 2 (<span class="math math-inline">H^2</span>)</strong>: <span class="math math-inline">H^1</span>이 출력한 <span class="math math-inline">B^1</span>을 입력으로 받아 두 번째 분류(<span class="math math-inline">C^2</span>)와 회귀(<span class="math math-inline">B^2</span>)를 수행하여 경계 상자를 더욱 정밀하게 조정한다.</li>
<li><strong>Stage 3 (<span class="math math-inline">H^3</span>)</strong>: <span class="math math-inline">H^2</span>가 출력한 <span class="math math-inline">B^2</span>를 입력으로 받아 최종 분류 점수(<span class="math math-inline">C^3</span>)와 가장 정제된 경계 상자(<span class="math math-inline">B^3</span>)를 예측한다.</li>
</ul>
<p>이 구조는 RPN이 생성한 ’원석’과 같은 제안들을 각 스테이지라는 ’가공 단계’를 거치면서 점차 ’보석’으로 다듬어가는 데이터 분포 정제 파이프라인으로 해석할 수 있다. 각 스테이지는 단순한 예측기가 아니라, 다음 스테이지를 위한 더 나은 품질의 데이터셋을 생성하는 ’필터’이자 ‘정제기’ 역할을 수행하는 것이다.</p>
<h3>2.3  반복적 경계 상자 회귀와의 차이점</h3>
<p>얼핏 보면 Cascade R-CNN은 단순히 경계 상자 회귀를 여러 번 반복하는 것처럼 보일 수 있다. 하지만 이는 추론 시에 **동일한 헤드(<span class="math math-inline">H^1</span>)**를 여러 번 반복 적용하여 경계 상자를 정제하는 ‘반복적 경계 상자 회귀(Iterative BBox Regression)’ 방식과는 근본적인 차이가 있다.10</p>
<p>반복적 BBox 회귀는 훈련 과정과 추론 과정 간의 불일치(mismatch) 문제를 야기한다. 훈련 시에는 헤드가 한 번만 사용되지만, 추론 시에는 여러 번 사용되기 때문이다. 또한, 동일한 회귀기는 동일한 입력 분포(RPN 제안 분포)에 최적화되어 있으므로, 이미 한 번 정제되어 분포가 달라진 경계 상자에 반복적으로 적용할 경우 성능 향상에 한계가 있다. 실험적으로도 보통 2회 이상 적용 시 개선 효과가 미미한 것으로 알려져 있다.10</p>
<p>반면, Cascade R-CNN은 각기 **다른 헤드(<span class="math math-inline">H^1</span>, <span class="math math-inline">H^2</span>, <span class="math math-inline">H^3</span>)**를 사용하며, 각 헤드는 이전 단계의 출력 분포에 맞게 <strong>개별적으로 훈련된다</strong>. 이는 훈련과 추론 과정이 완벽하게 동일하며, 추론 시에만 적용되는 후처리(post-processing) 기법이 아니라 아키텍처 자체에 체계적으로 통합된 재샘플링(resampling) 과정이라는 점에서 본질적인 차이를 가진다.10</p>
<h2>3.  단계별 학습 및 추론 메커니즘</h2>
<h3>3.1  점진적 임계값을 이용한 순차적 학습</h3>
<p>Cascade R-CNN의 각 스테이지는 독립적이면서도 순차적으로 학습된다. <span class="math math-inline">t</span>번째 스테이지의 탐지기(<span class="math math-inline">H_t</span>)는 <span class="math math-inline">t-1</span>번째 스테이지의 회귀기(<span class="math math-inline">B_{t-1}</span>)가 출력한 경계 상자 집합을 새로운 학습 데이터로 사용하여 훈련된다.1</p>
<p>이 학습 과정의 핵심은 각 스테이지별로 적용되는 IoU 임계값 <span class="math math-inline">u_t</span>가 점진적으로 증가한다는 점이다 (<span class="math math-inline">u^1 &lt; u^2 &lt; u^3</span>). 일반적으로 <code>{0.5, 0.6, 0.7}</code>과 같은 값이 사용된다.10</p>
<ul>
<li><strong>Stage 1 (<span class="math math-inline">H^1</span>)</strong>: RPN이 제안한 RoI들을 가장 낮은 임계값 <span class="math math-inline">u^1=0.5</span>로 Positive/Negative 샘플링하여 학습한다. 이는 일반적인 Faster R-CNN의 헤드 학습 방식과 동일하며, 상대적으로 거친 초기 제안들 중에서 객체의 대략적인 위치를 잡아내는 것을 목표로 한다.</li>
<li><strong>Stage 2 (<span class="math math-inline">H^2</span>)</strong>: Stage 1의 회귀기 <span class="math math-inline">B^1</span>이 출력한, 한 단계 정제된 경계 상자들을 새로운 RoI로 사용한다. 이 RoI들을 더 엄격한 임계값 <span class="math math-inline">u^2=0.6</span>으로 샘플링하여 <span class="math math-inline">H^2</span>를 학습한다. 이 단계는 ‘꽤 괜찮은’ 품질의 제안들을 ‘더 정확한’ 제안으로 만드는 데 특화된 전문가를 훈련시키는 과정이다.</li>
<li><strong>Stage 3 (<span class="math math-inline">H^3</span>)</strong>: Stage 2의 회귀기 <span class="math math-inline">B^2</span>가 출력한, 두 단계 정제된 경계 상자들을 RoI로 사용하고, 가장 엄격한 임계값 <span class="math math-inline">u^3=0.7</span>로 샘플링하여 <span class="math math-inline">H^3</span>를 학습한다. 이 단계는 거의 정확한 위치의 제안들을 최종적으로 미세 조정하여 최고 품질의 경계 상자를 예측하는 고도로 전문화된 탐지기를 훈련시킨다.</li>
</ul>
<h3>3.2  재샘플링(Resampling)을 통한 과적합 방지</h3>
<p>앞서 언급했듯이, 단일 탐지기에서 높은 IoU 임계값을 사용하면 Positive 샘플이 급격히 줄어 과적합이 발생한다. Cascade R-CNN은 이 문제를 다단계 구조를 통해 효과적으로 해결한다.1</p>
<p>각 스테이지의 회귀기는 입력된 제안들의 IoU를 평균적으로 향상시키는 경향이 있다.3 즉, Stage 1을 통과한 제안(<span class="math math-inline">B^1</span>)들의 IoU 분포는 RPN 제안(<span class="math math-inline">B^0</span>)보다 높고, Stage 2를 통과한 제안(<span class="math math-inline">B^2</span>)들의 IoU 분포는 <span class="math math-inline">B^1</span>보다 높다. 이러한 <strong>분포의 점진적 향상</strong>은 일종의 <strong>재샘플링(resampling)</strong> 효과를 가져온다. 더 높은 IoU 임계값(<span class="math math-inline">u_t</span>)을 사용하는 <span class="math math-inline">t</span>번째 스테이지는 이전 스테이지 덕분에 더 높은 품질의 제안(<span class="math math-inline">B^{t-1}</span>)을 입력받게 되므로, 엄격한 기준에도 불구하고 충분한 수의 Positive 훈련 샘플을 확보할 수 있는 것이다.1</p>
<p>이러한 재샘플링 과정은 모델 스스로가 다음 학습 단계를 위한 더 어렵고(더 높은 IoU를 요구하는) 동시에 더 적절한(분포가 이동된) 데이터를 생성해내는 ’적응형 데이터 증강(Adaptive Data Augmentation)’의 한 형태로 볼 수 있다. 이는 고정된 데이터셋에만 의존하는 것이 아니라, 학습 과정 중에 동적으로 더 유용한 데이터 분포를 만들어내는 정교한 학습 전략이다.</p>
<p>참고로, 이러한 재샘플링은 어려운 배경 샘플을 찾는 데 집중하는 하드 네거티브 마이닝(hard negative mining)과는 목적이 다르다. Cascade R-CNN의 재샘플링은 다음 단계를 효과적으로 훈련시키기 위한 **‘가까운 오탐지(close false positives)’**의 좋은 집합을 찾는 것을 목표로 한다.1</p>
<h3>3.3  추론 메커니즘</h3>
<p>추론(inference) 과정은 학습 과정과 완전히 동일한 캐스케이드 구조를 따른다.1 이는 훈련과 추론 간의 불일치를 원천적으로 제거하는 Cascade R-CNN의 중요한 설계 특징이다.10</p>
<p>입력 이미지에 대해 RPN이 초기 제안(<span class="math math-inline">B^0</span>)을 생성하면, 이 제안들은 <span class="math math-inline">H^1</span>, <span class="math math-inline">H^2</span>, <span class="math math-inline">H^3</span>를 순차적으로 통과한다. 각 스테이지를 거치면서 경계 상자는 계속해서 정제되고, 분류 점수 또한 업데이트된다. 최종 예측 결과는 마지막 스테이지(<span class="math math-inline">H^3</span>)의 분류 점수와 가장 정제된 경계 상자 위치를 기반으로 결정된다. 이러한 방식은 추론 시에도 제안의 품질과 각 스테이지 탐지기의 전문성(품질)이 점진적으로 일치하게 만들어, 최종적으로 매우 정밀하고 신뢰도 높은 탐지 결과를 도출하게 한다.1</p>
<h2>4.  손실 함수 심층 분석</h2>
<h3>4.1  각 스테이지의 손실 함수</h3>
<p>Cascade R-CNN의 각 스테이지 <span class="math math-inline">t</span>는 자체적인 분류기(<span class="math math-inline">h_t</span>)와 회귀기(<span class="math math-inline">f_t</span>)를 가지며, 이들은 해당 스테이지의 IoU 임계값 <span class="math math-inline">u_t</span>에 최적화되도록 학습된다.12 스테이지 <span class="math math-inline">t</span>에서의 손실 함수 <span class="math math-inline">L</span>은 분류 손실(<span class="math math-inline">L_{cls}</span>)과 위치 회귀 손실(<span class="math math-inline">L_{loc}</span>)의 가중합으로 구성되며, 이는 Faster R-CNN의 헤드 손실 함수와 형태가 유사하다.3</p>
<p>전체 아키텍처의 총 손실은 모든 스테이지의 손실을 합산하여 계산된다. 3-스테이지 캐스케이드의 경우, 총 손실은 각 스테이지 손실의 합으로 정의된다. 스테이지 <span class="math math-inline">t</span>에서의 손실 함수는 다음과 같이 표현된다.3<br />
<span class="math math-display">
L(x^t, g) = L_{cls}(h_t(x^t), y_t) + \lambda [y_t \geq 1] L_{loc}(f_t(x^t, b^t), g)
</span></p>
<h3>4.2  손실 함수 구성 요소 및 변수 해설</h3>
<p>위 수식의 각 구성 요소는 다음과 같은 의미를 가진다 3:</p>
<ul>
<li><span class="math math-inline">x^t</span>: 스테이지 <span class="math math-inline">t</span>에 입력되는 RoI의 특징 벡터이다.</li>
<li><span class="math math-inline">g</span>: 실제 객체의 클래스 레이블과 경계 상자 좌표를 포함하는 ground-truth 정보이다.</li>
<li><span class="math math-inline">b^t</span>: 스테이지 <span class="math math-inline">t</span>에 입력되는 제안 경계 상자이다. <span class="math math-inline">t&gt;1</span>일 때, <span class="math math-inline">b^t = f_{t-1}(x^{t-1}, b^{t-1})</span>로, 이전 스테이지 회귀기의 출력이 된다.</li>
<li><span class="math math-inline">u_t</span>: 스테이지 <span class="math math-inline">t</span>에서 Positive와 Negative 샘플을 구분하는 IoU 임계값이다. 점진적으로 증가하여 <span class="math math-inline">u_t &gt; u_{t-1}</span> 관계를 만족한다.</li>
<li><span class="math math-inline">y_t</span>: 임계값 <span class="math math-inline">u_t</span>에 따라 결정되는 <span class="math math-inline">x^t</span>의 레이블이다. 제안 <span class="math math-inline">b^t</span>와 ground-truth <span class="math math-inline">g</span> 간의 IoU가 <span class="math math-inline">u_t</span> 이상이면 객체의 실제 클래스 레이블이 할당되고, 그렇지 않으면 배경(클래스 0)으로 할당된다.</li>
<li><span class="math math-inline">h_t(x^t)</span>: 스테이지 <span class="math math-inline">t</span>의 분류기(classifier)이다. 입력 특징 <span class="math math-inline">x^t</span>에 대해 클래스별 확률을 예측한다.</li>
<li><span class="math math-inline">f_t(x^t, b^t)</span>: 스테이지 <span class="math math-inline">t</span>의 회귀기(regressor)이다. 입력 특징 <span class="math math-inline">x^t</span>와 제안 <span class="math math-inline">b^t</span>를 바탕으로 정제된 경계 상자를 예측한다.</li>
<li><span class="math math-inline">L_{cls}</span>: 분류 손실(classification loss)이다. 일반적으로 교차 엔트로피(Cross-Entropy) 손실 함수가 사용된다.</li>
<li><span class="math math-inline">L_{loc}</span>: 위치 회귀 손실(localization loss)이다. 일반적으로 Smooth L1 손실 함수가 사용되며 3, 예측된 경계 상자와 실제 경계 상자 간의 좌표 차이를 최소화하는 역할을 한다.</li>
<li><code>$\lambda$</code>: 두 손실 항의 가중치를 조절하는 하이퍼파라미터로, 일반적으로 1로 설정된다.3</li>
<li><code>$[y_t \geq 1]$</code>: 아이버슨 괄호(Iverson bracket) 표기법으로, 조건이 참이면 1, 거짓이면 0을 반환하는 지시 함수이다. 즉, <span class="math math-inline">y_t</span>가 배경이 아닌 Positive 샘플일 경우에만 <span class="math math-inline">L_{loc}</span>을 계산하도록 하여 배경 샘플에 대해서는 위치 회귀를 수행하지 않음을 의미한다.3</li>
</ul>
<p>이 손실 함수의 구조는 모든 스테이지에서 동일하지만, 각 스테이지에서 최적화하는 목표는 근본적으로 다르다. 이는 Positive 샘플의 정의, 즉 <span class="math math-inline">y_t</span>가 임계값 <span class="math math-inline">u_t</span>에 따라 계속 변하기 때문이다. Stage 1의 손실 함수는 <code>u=0.5</code> 기준으로 ‘대충 겹치는’ 제안들을 Positive로 보고 학습하는 반면, Stage 3의 손실 함수는 <code>u=0.7</code> 기준으로 ‘매우 정확하게 겹치는’ 제안들만을 Positive로 보고 학습한다. 결과적으로 동일한 형태의 손실 함수가 각 스테이지의 네트워크가 점진적으로 더 정교하고 어려운 분류 및 회귀 문제를 풀도록 유도하는 정교한 학습 전략의 핵심이 된다.</p>
<h2>5.  성능 평가 및 비교 분석</h2>
<h3>5.1  COCO 데이터셋에서의 성능</h3>
<p>Cascade R-CNN은 제안 당시, 특히 어렵기로 알려진 COCO 데이터셋에서 모든 단일 모델(single-model) 객체 탐지기를 능가하는 SOTA(State-of-the-Art) 성능을 달성하며 그 효과를 입증했다.1</p>
<p>이 모델의 진정한 강점은 전체 평균 mAP뿐만 아니라, 더 엄격한 IoU 임계값을 사용하는 평가 지표, 예를 들어 <span class="math math-inline">AP_{75}</span>(IoU=0.75에서 평가)와 <span class="math math-inline">AP_{90}</span>에서 더욱 두드러지게 나타난다. 이는 Cascade R-CNN이 단순히 객체를 찾는 것을 넘어, 매우 정확한 위치를 예측하는 고품질 탐지에 탁월함을 보여주는 증거이다.1 실험 결과에 따르면, 3개의 스테이지를 사용하는 것이 성능과 계산 효율성 간의 가장 좋은 균형을 보였으며, 스테이지를 4개로 늘릴 경우 성능이 정체되거나 오히려 약간 하락하는 경향이 관찰되었다.10</p>
<h3>5.2  주요 객체 탐지 모델과의 성능 비교</h3>
<p>Cascade R-CNN은 특정 기술에 종속되지 않는 범용적인 프레임워크 개선 방안으로, 다양한 백본 네트워크(ResNet-50, ResNet-101, ResNeXt-101 등)와 결합될 수 있다. 어떤 백본을 사용하든 기존의 단일 스테이지 탐지기(Faster R-CNN, FPN 등)에 비해 일관되게 2~4%p의 mAP 성능 향상을 보였다.1</p>
<p>아래 표는 COCO test-dev 데이터셋에서 주요 객체 탐지 모델들의 성능(mAP)과 속도(FPS)를 비교한 것이다. 이를 통해 정확도와 속도 간의 트레이드오프 관계 속에서 Cascade R-CNN의 위치를 명확히 파악할 수 있다.</p>
<p><strong>표 1: COCO test-dev 데이터셋 기반 주요 객체 탐지 모델 성능 비교 (V100 GPU 기준)</strong></p>
<table><thead><tr><th>모델 (Model)</th><th>백본 / 변형 (Backbone/Variant)</th><th>mAP (<code>AP@[.5:.95]</code>)</th><th><span class="math math-inline">AP_{75}</span></th><th>FPS (V100)</th><th>출처</th></tr></thead><tbody>
<tr><td><strong>Cascade R-CNN</strong></td><td>ResNet-50</td><td>44.3%</td><td>48.2%</td><td>8-12</td><td>19</td></tr>
<tr><td></td><td>ResNet-101</td><td>46.3%</td><td>50.6%</td><td>6-8</td><td>19</td></tr>
<tr><td></td><td>ResNeXt-101</td><td>48.1%</td><td>52.9%</td><td>4-6</td><td>19</td></tr>
<tr><td><strong>Faster R-CNN</strong></td><td>ResNet-101-FPN</td><td>약 42.0%</td><td>-</td><td>약 9</td><td>(일반적 Baseline)</td></tr>
<tr><td><strong>DetectoRS</strong></td><td>ResNeXt-101-64x4d</td><td>55.7%</td><td>-</td><td>-</td><td>20</td></tr>
<tr><td><strong>Swin Transformer</strong></td><td>Swin-L (HTC++)</td><td>58.7%</td><td>-</td><td>-</td><td>20</td></tr>
<tr><td><strong>YOLOv7</strong></td><td>YOLOv7-W6</td><td>54.9%</td><td>-</td><td>약 84</td><td>20</td></tr>
<tr><td><strong>EfficientDet</strong></td><td>EfficientDet-D7x</td><td>55.1%</td><td>-</td><td>3.8 미만</td><td>19</td></tr>
</tbody></table>
<p>이 표는 Cascade R-CNN이 차지하는 독특한 위치를 보여준다. YOLO와 같은 1단계 탐지기의 실시간 속도를 희생하는 대신, 위치 특정 정확도에서 상당한 향상을 이룬다. 특히 <span class="math math-inline">AP_{75}</span> 지표에서 높은 성능을 보이는 것은 이 모델의 다단계 정제 메커니즘이 위치 정확도를 높이는 데 직접적으로 기여하기 때문이다. 따라서 자율 주행이나 의료 영상 분석처럼 픽셀 단위의 정밀도가 중요한 애플리케이션에서는 전체 mAP가 조금 낮더라도 <span class="math math-inline">AP_{75}</span>가 높은 Cascade R-CNN 계열의 모델이 더 선호될 수 있다. 이 모델은 객체 탐지 분야의 ’품질’에 대한 패러다임을 한 단계 발전시켰다고 평가할 수 있다.</p>
<h2>6.  확장 모델 및 응용 분야</h2>
<h3>6.1  주요 확장 모델</h3>
<p>Cascade R-CNN이 제시한 ’점진적 정제’라는 강력한 설계 원칙은 객체 탐지를 넘어 다른 컴퓨터 비전 태스크로 성공적으로 확장되었다.</p>
<ul>
<li><strong>Cascade Mask R-CNN</strong>: Cascade R-CNN의 아키텍처를 인스턴스 분할(Instance Segmentation) 태스크로 확장한 모델이다. 각 캐스케이드 헤드에 경계 상자 예측 브랜치와 병렬로 마스크 예측 브랜치(<span class="math math-inline">S</span>)를 추가한 구조이다.14 이를 통해 경계 상자뿐만 아니라 픽셀 단위의 객체 마스크 또한 점진적으로 정제하여 분할 정확도를 획기적으로 높였다.</li>
<li><strong>Hybrid Task Cascade (HTC)</strong>: Cascade Mask R-CNN을 한 단계 더 발전시킨 모델로, 탐지와 분할 태스크 간의 상호 보완적인 관계를 적극적으로 활용한다. 기존 캐스케이드 모델이 두 태스크를 각 스테이지에서 독립적으로 정제했다면, HTC는 두 태스크를 상호 결합(interweave)하여 정보를 교환하도록 설계했다.25 예를 들어, 이전 스테이지의 마스크 예측 정보를 현재 스테이지의 경계 상자 예측에 활용하는 식이다. 또한, 배경과 전경을 구분하는 데 도움을 주는 의미론적 분할(semantic segmentation) 브랜치를 추가하여 공간적 맥락 정보를 활용함으로써 성능을 더욱 끌어올렸다.26</li>
</ul>
<h3>6.2  주요 응용 분야</h3>
<p>Cascade R-CNN의 고정밀 위치 특정 능력은 정확성이 핵심적인 다양한 실제 문제 해결에 효과적으로 사용된다.12</p>
<ul>
<li><strong>의료 영상 분석 (Medical Imaging Analysis)</strong>: 미세한 종양, 병변, 혹은 골절을 정확하게 탐지하는 데 필수적인 높은 정밀도를 제공한다. X-ray 이미지에서 발견하기 어려운 작은 흉골 골절을 탐지하거나 29, 내시경 이미지에서 초기 단계의 용종이나 비정상 조직을 식별하는 데 성공적으로 적용되었다.30</li>
<li><strong>자율 주행 (Autonomous Driving)</strong>: 도로 위의 차량, 보행자, 교통 표지판 등의 위치를 단 몇 픽셀의 오차 없이 정확하게 파악하는 것은 시스템의 안전과 직결된다. Cascade R-CNN은 특히 작거나 일부가 가려진 객체에 대한 탐지 성능을 개선하여 자율 주행 시스템의 신뢰도를 높이는 데 기여한다.31</li>
<li><strong>산업 품질 관리 (Industrial Quality Control)</strong>: 반도체 웨이퍼나 디스플레이 패널의 미세한 결함을 자동으로 검출하는 등 제조 공정의 품질 관리에 사용된다. 높은 정확도는 불량품을 최소화하고 생산 효율을 극대화하는 데 기여한다.12</li>
<li><strong>위성 및 항공 이미지 분석 (Satellite and Aerial Image Analysis)</strong>: 넓은 지역을 촬영한 고해상도 위성 이미지에서 건물, 선박, 차량 등 작은 객체를 정밀하게 탐지하고 분석하는 데 활용된다.</li>
</ul>
<h2>7.  결론: Cascade R-CNN의 의의와 한계</h2>
<h3>7.1  핵심 기여 및 의의</h3>
<p>Cascade R-CNN은 ’고품질 탐지의 역설’이라는 객체 탐지 분야의 오랜 난제를 해결하기 위한 체계적이고 효과적인 프레임워크를 제시했다는 점에서 큰 의의를 가진다.6 다단계 구조와 점진적으로 증가하는 IoU 임계값 설정을 통해, 훈련 데이터의 과적합 문제와 추론 시 발생하는 품질 불일치 문제를 동시에 해결하며 객체 탐지기의 정밀도를 한 차원 높은 수준으로 끌어올렸다.1</p>
<p>무엇보다 이 모델은 특정 백본이나 탐지기 구조에 얽매이지 않는 범용적인 아키텍처 개선 방안으로, 다양한 2단계 탐지기에 손쉽게 적용하여 일관된 성능 향상을 가져올 수 있음을 입증했다.1 이는 Cascade R-CNN이 단일 모델을 넘어, 고품질 탐지를 위한 강력한 설계 패턴임을 시사한다.</p>
<h3>7.2  장점과 단점</h3>
<ul>
<li><strong>장점 (Pros)</strong>:</li>
<li><strong>높은 정확도</strong>: 특히 IoU 기준이 엄격한 고정밀도 평가에서 다른 모델 대비 월등한 성능을 보이며, 매우 정밀한 위치 예측이 가능하다.12</li>
<li><strong>과적합 방지</strong>: 구조적인 재샘플링 메커니즘을 통해 고품질 샘플에 대한 과적합 없이 안정적인 학습이 가능하다.1</li>
<li><strong>범용성 및 유연성</strong>: 다양한 백본 네트워크 및 2단계 탐지기 프레임워크에 쉽게 통합하여 성능을 개선할 수 있다.1</li>
<li><strong>단점 (Cons)</strong>:</li>
<li><strong>추론 속도 및 계산 비용</strong>: 다단계 구조로 인해 순차적인 연산이 필수적이므로, 단일 헤드를 사용하는 Faster R-CNN이나 1단계 탐지기(YOLO, SSD 등)에 비해 추론 속도가 상대적으로 느리고 계산 비용이 높다.8 이로 인해 실시간 처리가 매우 중요한 애플리케이션에는 부적합할 수 있다.</li>
<li><strong>구조적 복잡성</strong>: 모델 구조와 학습 파이프라인이 단일 스테이지 모델보다 복잡하여 구현 및 디버깅의 난이도가 높다.</li>
</ul>
<h3>7.3  객체 탐지 분야에 미친 영향 및 향후 전망</h3>
<p>Cascade R-CNN은 객체 탐지 연구의 초점을 ‘속도’ 중심의 경쟁에서 ’품질’과 ’정밀도’의 중요성을 함께 고려하는 방향으로 전환하는 데 중요한 역할을 했다. 이 모델의 등장은 mAP라는 단일 지표뿐만 아니라, <span class="math math-inline">AP_{75}</span>, <span class="math math-inline">AP_{90}</span> 등 더 세분화된 ‘품질’ 지표의 중요성을 부각시키는 계기가 되었다.</p>
<p>’점진적 정제’라는 개념은 고품질 탐지를 위한 표준적인 접근법 중 하나로 자리매김했으며, 이후 등장한 Hybrid Task Cascade(HTC)와 같은 수많은 고성능 탐지기에 직접적인 영감을 주었다. 이 설계 원칙은 객체 탐지를 넘어 다른 복잡한 컴퓨터 비전 태스크를 해결하기 위한 유용한 패턴으로 활용될 잠재력을 지니고 있다. 향후 연구는 캐스케이드 구조의 효율성을 높여 속도 저하를 최소화하는 경량화 연구, 각 스테이지의 전문성을 더욱 극대화하는 학습 전략 연구, 또는 Transformer와 같은 새로운 아키텍처와 결합하여 성능의 한계를 뛰어넘는 방향으로 진행될 것으로 전망된다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Cascade R-CNN: Delving into High Quality Object Detection - Statistical Visual Computing Lab, http://www.svcl.ucsd.edu/publications/conference/2018/cvpr/cascade-rcnn.pdf</li>
<li>Faster R-CNNs - PyImageSearch, https://pyimagesearch.com/2023/11/13/faster-r-cnns/</li>
<li>Cascade R-CNN: High Quality Object Detection and Instance … - arXiv, https://arxiv.org/pdf/1906.09756</li>
<li>Cascade R-CNN | CloudFactory Computer Vision Wiki, https://wiki.cloudfactory.com/docs/mp-wiki/model-architectures/cascade-rcnn</li>
<li>Cascade R-CNN: Delving Into High Quality Object Detection - CVF Open Access, https://openaccess.thecvf.com/content_cvpr_2018/papers/Cai_Cascade_R-CNN_Delving_CVPR_2018_paper.pdf</li>
<li>Cascade R-CNN: High Quality Object Detection and Instance Segmentation - Statistical Visual Computing Lab, http://svcl.ucsd.edu/publications/journal/2019/cascadercnn_pami.pdf</li>
<li>Cascade R-CNN: Delving Into High Quality Object Detection - CVPR 2018 Open Access Repository - The Computer Vision Foundation, https://openaccess.thecvf.com/content_cvpr_2018/html/Cai_Cascade_R-CNN_Delving_CVPR_2018_paper.html</li>
<li>Cascade R-CNN: Delving into High Quality Object Detection | Request PDF - ResearchGate, https://www.researchgate.net/publication/321512547_Cascade_R-CNN_Delving_into_High_Quality_Object_Detection</li>
<li>[1712.00726] Cascade R-CNN: Delving into High Quality Object Detection - arXiv, https://arxiv.org/abs/1712.00726</li>
<li>Reading: Cascade R-CNN — Delving into High Quality Object …, https://sh-tsang.medium.com/reading-cascade-r-cnn-delving-into-high-quality-object-detection-object-detection-8c7901cc7864</li>
<li>Cascade R-CNN in Detectron - GitHub, https://github.com/zhaoweicai/Detectron-Cascade-RCNN</li>
<li>Cascade R-CNN- Explained - GeeksforGeeks, https://www.geeksforgeeks.org/deep-learning/cascade-r-cnn-explained/</li>
<li>Cascade R-CNN: High Quality Object Detection and Instance Segmentation - ResearchGate, https://www.researchgate.net/publication/337617828_Cascade_R-CNN_High_Quality_Object_Detection_and_Instance_Segmentation</li>
<li>R-CNN - Region-Based Convolutional Neural Networks - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/r-cnn-region-based-cnns/</li>
<li>Papers Explained 77: Cascade RCNN | by Ritvik Rastogi | Medium, https://ritvik19.medium.com/papers-explained-77-cascade-rcnn-720b161d86e4</li>
<li>Faster R-CNN Explained for Object Detection Tasks - DigitalOcean, https://www.digitalocean.com/community/tutorials/faster-r-cnn-explained-object-detection</li>
<li>Cascade R-CNN: High Quality Object Detection and Instance Segmentation | Request PDF - ResearchGate, https://www.researchgate.net/publication/333993251_Cascade_R-CNN_High_Quality_Object_Detection_and_Instance_Segmentation</li>
<li>Cascade R-CNN: Enhancing Object Detection - Emergent Mind, https://www.emergentmind.com/articles/1712.00726</li>
<li>Object Detection: State-of-the-Art Models in 2025 - HiringNet, https://hiringnet.com/object-detection-state-of-the-art-models-in-2025</li>
<li>Papers with code · GitHub, <a href="https://paperswithcode.com/sota/object-detection-on-coco?tag_filter=12,10,6,13,14">https://paperswithcode.com/sota/object-detection-on-coco?tag_filter=12%2C10%2C6%2C13%2C14</a></li>
<li>9 Best Object Detection Models of 2025: Reviewed &amp; Compared - Hitech BPO, https://www.hitechbpo.com/blog/top-object-detection-models.php</li>
<li>Cascade Mask R-CNN and Keypoint Detection used in Floorplan Parsing - DiVA portal, https://www.diva-portal.org/smash/get/diva2:1450823/FULLTEXT01.pdf</li>
<li>CascadeMask R-CNN - Computer Vision Wiki - CloudFactory, https://wiki.cloudfactory.com/docs/mp-wiki/model-architectures/cascademask-r-cnn</li>
<li>anonymoussss/Cascade-Mask-RCNN - GitHub, https://github.com/anonymoussss/Cascade-Mask-RCNN</li>
<li>[PDF] Hybrid Task Cascade for Instance Segmentation - Semantic Scholar, https://www.semanticscholar.org/paper/Hybrid-Task-Cascade-for-Instance-Segmentation-Chen-Pang/21248bcc81539e7cd1ef83b3b184768603f6f247</li>
<li>Hybrid Task Cascade for Instance Segmentation - ResearchGate, https://www.researchgate.net/publication/338513481_Hybrid_Task_Cascade_for_Instance_Segmentation</li>
<li>Hybrid Task Cascade for Instance Segmentation - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Hybrid_Task_Cascade_for_Instance_Segmentation_CVPR_2019_paper.pdf</li>
<li>HybridTask Cascade - Computer Vision Wiki - CloudFactory, https://wiki.cloudfactory.com/docs/mp-wiki/model-architectures/hybridtask-cascade</li>
<li>An attention‐based cascade R‐CNN model for sternum fracture detection in X‐ray images, https://www.researchgate.net/publication/357711761_An_attention-based_cascade_R-CNN_model_for_sternum_fracture_detection_in_X-ray_images</li>
<li>Artefact Detection and Segmentation Using Cascade R-CNN and U-Net - CEUR-WS, https://ceur-ws.org/Vol-2595/endoCV2020_paper_id_31.pdf</li>
<li>Object Detection in Autonomous Driving Scenarios Based on an Improved Faster-RCNN, https://www.mdpi.com/2076-3417/11/24/11630</li>
<li>Enhancing Object Detection in Autonomous Cars: A Fusion of YOLO and Cascade R-CNN - NORMA@NCI Library, https://norma.ncirl.ie/7507/1/kalyanilaxmikantdeshpande.pdf</li>
<li>Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution - NIPS, http://papers.neurips.cc/paper/8423-cascade-rpn-delving-into-high-quality-region-proposal-network-with-adaptive-convolution.pdf</li>
<li>Cascade R-CNN: High Quality Object Detection and Instance Segmentation - PubMed, https://pubmed.ncbi.nlm.nih.gov/31794388/</li>
<li>What is R-CNN? - Roboflow Blog, https://blog.roboflow.com/what-is-r-cnn/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>