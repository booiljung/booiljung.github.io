<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:FCOS (Fully Convolutional One-Stage Object Detection, 2019)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>FCOS (Fully Convolutional One-Stage Object Detection, 2019)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>FCOS (Fully Convolutional One-Stage Object Detection, 2019)</span></nav>
                </div>
            </header>
            <article>
                <h1>FCOS (Fully Convolutional One-Stage Object Detection, 2019)</h1>
<h2>1. 서론</h2>
<h3>1.1 FCOS 개요</h3>
<p>FCOS(Fully Convolutional One-Stage Object Detection)는 객체 탐지(object detection) 분야의 기존 패러다임을 근본적으로 전환시킨 앵커 프리(anchor-free) 및 제안 프리(proposal-free) 단일 단계(one-stage) 모델이다. FCOS의 핵심 철학은 객체 탐지라는 과업을 의미론적 분할(semantic segmentation)과 같은 기존의 밀집 예측(dense prediction) 과업과 유사한 픽셀 단위(per-pixel) 예측 문제로 재정의하는 데 있다. 이 접근법은 특징 맵(feature map)의 각 위치(location)가 사전에 정의된 앵커 박스(anchor box)에 의존하는 대신, 직접적으로 객체의 경계 상자(bounding box)를 예측하도록 한다.</p>
<p>이러한 혁신을 통해 FCOS는 Faster R-CNN, SSD, YOLOv3 등 당대의 주류를 이루던 앵커 기반 탐지기들이 필연적으로 가졌던 복잡성을 대폭 제거했다. 앵커 박스와 관련된 복잡한 IoU(Intersection over Union) 계산, 하이퍼파라미터 튜닝 과정 등을 완전히 배제함으로써 모델의 설계 파라미터를 최소화하고 훈련 및 추론 속도를 획기적으로 향상시켰다.</p>
<p>FCOS의 등장은 단순히 ’앵커를 제거했다’는 기술적 개선을 넘어, 객체 탐지라는 과업을 컴퓨터 비전의 다른 핵심 과업들과 동일한 ‘밀집 예측’ 프레임워크로 통합하려는 더 큰 학문적 흐름을 대변한다. 이전까지 앵커 박스라는 특수한 구조물에 의존했던 객체 탐지는 픽셀 단위 예측을 수행하는 다른 과업들과 개념적으로 분리되어 있었다. FCOS는 “객체 탐지를 의미론적 분할처럼 해결할 수 없을까?“라는 근본적인 질문에 대한 긍정적인 답을 제시함으로써, 과업별로 파편화되었던 해결책 대신 더 일반적이고 통일된 프레임워크의 가능성을 열었다. FCOS의 성공은 객체 탐지 역시 완전 합성곱 신경망(FCN) 기반의 픽셀 단위 예측으로 해결될 수 있음을 증명했으며, 이는 분할이나 키포인트 예측과 같은 다른 분야의 아이디어와 기술을 객체 탐지에 더 용이하게 적용할 수 있는 길을 열었다는 점에서 중요한 의의를 가진다.</p>
<h3>1.2 안내서의 구조</h3>
<p>본 안내서는 FCOS의 제안 배경이 된 기존 앵커 기반 모델의 한계를 시작으로, FCOS의 핵심 아키텍처, 방법론적 혁신, 독창적인 손실 함수 설계, 그리고 주요 모델들과의 성능 비교 벤치마킹 결과를 심층적으로 분석한다. 나아가 FCOS가 후속 연구에 미친 영향과 그 유산을 조명하여, 현대 객체 탐지 기술의 발전에 FCOS가 어떤 기여를 했는지 종합적으로 고찰하고자 한다.</p>
<h2>2. 앵커 프리 탐지의 당위성: 기존 앵커 기반 모델의 한계</h2>
<p>FCOS가 제안된 배경에는 기존 앵커 기반 탐지기들이 가진 명백한 한계점들이 존재했다. 이러한 한계들은 독립적인 문제가 아니라, ’사전에 정의된 이산적인 공간 샘플링’이라는 앵커의 근본 철학에서 파생된 연쇄적인 문제들이었다.</p>
<h3>2.1 하이퍼파라미터에 대한 민감성</h3>
<p>앵커 기반 탐지기의 성능은 앵커 박스의 크기(size), 종횡비(aspect ratio), 개수 등 사용자가 사전에 직접 정의해야 하는 하이퍼파라미터에 매우 민감하게 반응한다. 예를 들어, RetinaNet의 경우 이러한 하이퍼파라미터를 어떻게 설정하는지에 따라 COCO 데이터셋에서의 평균 정밀도(AP)가 최대 4%까지 변동될 수 있다. 이는 새로운 데이터셋이나 객체 크기 분포가 다른 환경에 모델을 적용할 때마다 많은 비용과 시간이 소요되는 재설계 및 튜닝 과정을 거쳐야 함을 의미하며, 모델의 일반화 능력을 저해하는 주요 요인으로 작용한다.</p>
<h3>2.2 계산 복잡성 및 메모리 비효율성</h3>
<p>훈련 과정에서 모델은 이미지 위에 촘촘하게 배치된 수많은 앵커 박스와 실제 객체(ground-truth) 간의 IoU를 일일이 계산하고, 이를 기반으로 각 앵커를 양성 또는 음성 샘플로 할당해야 한다. 특징 피라미드 네트워크(FPN)를 사용하는 현대적인 탐지기에서는 이미지 한 장당 18만 개가 넘는 앵커 박스가 생성될 수 있다. 이처럼 방대한 양의 앵커에 대한 계산은 상당한 연산 부하를 유발하고 훈련 메모리 사용량을 증가시켜 비효율을 초래한다.</p>
<h3>2.3 양성/음성 샘플의 극심한 불균형</h3>
<p>높은 재현율(recall)을 확보하기 위해 이미지 전체에 앵커 박스를 촘촘하게 배치하는 전략은 필연적으로 객체를 포함하지 않는 배경 영역에 대한 앵커, 즉 음성 샘플(negative sample)의 수가 압도적으로 많아지는 결과를 낳는다. 이처럼 과도한 음성 샘플은 훈련 과정에서 양성 샘플과 음성 샘플 간의 극심한 데이터 불균형을 야기한다. 이 불균형은 모델이 대부분의 학습 시간을 쉬운 배경을 판별하는 데만 사용하게 만들어 학습을 비효율적으로 만들고, 결국 모델의 성능 저하로 이어진다.</p>
<h3>2.4 일반화 능력의 한계</h3>
<p>앵커 박스는 고정된 형태와 크기를 가지므로, 예측하고자 하는 객체의 형태가 매우 다양하거나 특히 작은 객체를 탐지해야 할 때 어려움을 겪는다. 사전 정의된 앵커의 형태와 실제 객체의 형태 간 불일치가 클수록 정확한 경계 상자를 회귀하기 어려워지며, 이는 모델의 전반적인 일반화 성능을 제한하는 요인이 된다.</p>
<p>이러한 문제들은 앵커 박스가 ’객체가 존재할 만한 위치와 형태’를 미리 가정하고 공간을 이산적인 박스들로 샘플링하는 방식 자체에서 비롯된다. 높은 재현율을 위해 샘플링을 촘촘하게 할수록(2.2), 계산 부하와 메모리 비효율성이 증가하고(2.3), 이는 극심한 전경-배경 불균형 문제를 야기한다. 결국 이 모든 문제는 ’사전 가정’이라는 하이퍼파라미터에 대한 민감성(2.1)으로 귀결된다. 따라서 FCOS가 채택한 ‘사전 샘플링’ 자체를 폐기하고 ’모든 위치’를 잠재적 객체 중심으로 보는 연속적인 관점의 전환은 이러한 연쇄적인 문제들을 근본적으로 해결하기 위한 필연적인 선택이었다.</p>
<h2>3. FCOS 아키텍처 심층 분석: 백본부터 예측까지</h2>
<p>FCOS는 백본 네트워크, 특징 피라미드 네트워크, 그리고 다중 레벨 예측 헤드의 세 가지 주요 구성 요소로 이루어진다. 이 구조는 앵커를 제거함으로써 발생하는 스케일 불변성 부재와 예측 품질 저하라는 두 가지 핵심 문제를 해결하기 위해 유기적으로 설계되었다.</p>
<h3>3.1 백본 네트워크 (Backbone Network)</h3>
<p>백본 네트워크는 입력 이미지로부터 고수준의 의미론적 특징 맵을 추출하는 역할을 담당한다. FCOS는 특정 백본에 국한되지 않고 다양한 표준 CNN 아키텍처를 유연하게 채택할 수 있다. 원 논문에서는 주로 ResNet과 ResNeXt가 백본으로 사용되었으며, 이는 성능과 효율성 측면에서 검증된 선택이다.1 PyTorch와 같은 주요 딥러닝 프레임워크에서는 ImageNet 등으로 사전 훈련된 다양한 백본 모델을 제공하므로, 사용자는 이를 통해 FCOS를 쉽게 구현하고 활용할 수 있다.</p>
<h3>3.2 특징 피라미드 네트워크 (Feature Pyramid Network, FPN)</h3>
<p>앵커를 제거하면 각 위치는 더 이상 사전 정의된 크기 정보를 갖지 않게 되므로, 다양한 크기의 객체를 처리하는 데 어려움이 발생한다. FCOS는 이 문제를 해결하기 위해 FPN을 도입했다. FPN은 백본 네트워크의 여러 계층에서 추출된 특징 맵들을 결합하여 다중 스케일 특징 피라미드를 구축한다. 이 피라미드의 각 레벨은 서로 다른 해상도와 수용 영역(receptive field)을 가지므로, 자연스럽게 다양한 크기의 객체를 탐지하는 데 특화된다.1 예를 들어, 고해상도의 저수준 특징 맵은 작은 객체를, 저해상도의 고수준 특징 맵은 큰 객체를 탐지하는 데 사용된다. 이처럼 FPN은 앵커의 ‘다중 스케일 샘플링’ 역할을 ’다중 레벨 특징 표현’으로 대체하여, 앵커 없이도 스케일 불변성을 확보하는 핵심적인 역할을 수행한다.</p>
<h3>3.3 다중 레벨 예측 헤드 (Multi-Level Prediction Heads)</h3>
<p>FPN의 각 피라미드 레벨(일반적으로 P3, P4, P5, P6, P7)에서 생성된 특징 맵에는 파라미터를 공유하는(shared) 단일 예측 헤드가 적용된다. 이 헤드는 세 개의 병렬적인 브랜치로 구성되어, 각 위치에서 객체 탐지에 필요한 모든 정보를 동시에 예측한다.1</p>
<h4>3.3.1 분류 헤드 (Classification Head)</h4>
<p>분류 헤드는 특징 맵의 각 위치가 C개의 객체 클래스 중 하나에 속할 확률을 예측한다. 그 결과는 각 픽셀 위치마다 C차원의 벡터를 출력하는 형태이며, 이 벡터의 각 요소는 특정 클래스에 대한 신뢰도 점수(confidence score)를 나타낸다.1</p>
<h4>3.3.2 회귀 헤드 (Regression Head)</h4>
<p>회귀 헤드는 각 위치에서 해당 위치를 포함하는 객체의 경계 상자까지의 거리를 예측한다. 구체적으로, 현재 위치에서 경계 상자의 왼쪽(<code>l</code>), 위쪽(<code>t</code>), 오른쪽(<code>r</code>), 아래쪽(<code>b</code>) 경계까지의 거리를 나타내는 4차원 실수 벡터 <code>(l, t, r, b)</code>를 직접 회귀한다.1 이 방식은 앵커 박스를 기준으로 한 오프셋(offset) 회귀가 아닌, 위치 자체를 기준으로 한 직접적인 거리 예측이라는 점에서 차별화된다.</p>
<h4>3.3.3 중심성 헤드 (Center-ness Head)</h4>
<p>앵커 없이 실제 경계 상자 내의 모든 픽셀을 양성 샘플로 간주하면, 객체의 경계에 가까운 픽셀에서도 경계 상자 예측이 이루어진다. 이러한 예측들은 중심부에서 생성된 예측에 비해 품질이 낮을 가능성이 높다. ‘중심성(center-ness)’ 헤드는 바로 이 문제를 해결하기 위해 도입되었다. 이 헤드는 각 위치가 자신이 속한 경계 상자의 중심에 얼마나 가까운지를 0과 1 사이의 스칼라 값으로 예측한다. 이 중심성 점수는 후처리 과정에서 분류 점수에 곱해져, 객체 중심에서 멀리 떨어진 위치에서 생성된 저품질 탐지를 효과적으로 억제하는 필터 역할을 한다.1</p>
<p>결론적으로 FPN과 중심성 헤드는 단순히 추가된 모듈이 아니라, 앵커 프리라는 핵심 설계 철학을 성공시키기 위한 필수적이고 상호 보완적인 구성 요소이다. FPN이 스케일 문제를 해결하고, 중심성 헤드가 위치에 따른 예측 품질 문제를 해결함으로써 FCOS의 완전한 아키텍처가 완성된다.</p>
<h2>4. 핵심 메커니즘 및 방법론적 혁신</h2>
<p>FCOS의 성공은 앵커라는 명시적인 사전 제약(explicit prior)을 ’크기 제약’과 ’위치 제약’이라는 두 가지 암시적인 제약(implicit prior)으로 대체한 정교한 설계 덕분이다.</p>
<h3>4.1 픽셀 단위 경계 상자 회귀 (Per-Pixel Bounding Box Regression)</h3>
<p>FCOS는 특징 맵의 모든 위치 <code>(x, y)</code>를 잠재적인 훈련 샘플로 직접 간주한다.2 만약 특정 위치 <code>(x, y)</code>가 실제 경계 상자 <span class="math math-inline">B_i = (x_0^{(i)}, y_0^{(i)}, x_1^{(i)}, y_1^{(i)}, c^{(i)})</span> 내부에 속한다면, 해당 위치는 양성 샘플로 분류된다. 이 경우, 모델은 해당 위치에서 경계 상자의 네 변까지의 거리인 4차원 벡터 <code>t* = (l*, t*, r*, b*)</code>를 직접 회귀하도록 학습된다.2 회귀 목표(regression target)는 다음 수식으로 정의된다.<br />
<span class="math math-display">
\begin{aligned}
l^* &amp;= x - x_0^{(i)}, &amp; t^* &amp;= y - y_0^{(i)} \\
r^* &amp;= x_1^{(i)} - x, &amp; b^* &amp;= y_1^{(i)} - y
\end{aligned}
</span><br />
여기서 <code>(x, y)</code>는 특징 맵 상의 위치를 원본 이미지에 매핑한 좌표이며, <span class="math math-inline">(x_0^{(i)}, y_0^{(i)})`와 `(x_1^{(i)}, y_1^{(i)})</span>는 각각 실제 경계 상자의 좌측 상단 및 우측 하단 좌표이다.2 이 4D 벡터 예측값은 항상 양수여야 하므로, 회귀 브랜치 마지막 단에 <code>exp(sᵢx)</code>와 같은 지수 함수를 적용한다. 여기서 <code>sᵢ</code>는 각 FPN 레벨에 대한 학습 가능한 스케일 파라미터로, 예측 범위를 유연하게 조절하는 역할을 한다.</p>
<h3>4.2 다중 레벨 예측을 통한 모호성 해결 (Resolving Ambiguity with Multi-Level Prediction)</h3>
<p>픽셀 단위 예측 방식에서는 하나의 픽셀이 여러 개의 겹치는(overlapping) 실제 경계 상자에 동시에 포함될 수 있다. 이 경우, 해당 픽셀이 어떤 경계 상자를 기준으로 회귀해야 할지 모호해지는 문제가 발생한다. FCOS는 FPN의 다중 레벨 구조를 활용하여 이 문제를 효과적으로 해결한다.3</p>
<p>이는 ’크기에 대한 암시적 제약’을 통해 이루어진다. 각 실제 경계 상자는 그 크기에 따라 특정 FPN 레벨에만 할당된다. 구체적으로, 회귀 목표 <code>max(l*, t*, r*, b*)</code> 값이 사전에 정의된 범위 <code>[mᵢ₋₁, mᵢ]</code> 내에 속하는 객체는 FPN 레벨 <code>Pᵢ</code>에서만 양성 샘플로 간주된다. 논문에서는 <code>P₃</code>부터 <code>P₇</code> 레벨에 대해 각각 <code>, </code>, <code>, </code>, <code>[512, ∞]</code>의 크기 범위를 할당했다. 이 전략 덕분에 크기가 서로 다른 두 객체가 공간적으로 겹치더라도, 각 객체는 서로 다른 FPN 레벨에 할당될 가능성이 높아져 대부분의 모호성이 자연스럽게 해소된다. 만약 동일한 레벨에 할당될 만큼 크기가 비슷한 객체들이 겹치는 드문 경우에는, 더 작은 면적을 가진 객체를 우선적으로 할당하여 모호성을 최종적으로 해결한다.</p>
<h3>4.3 중심성 점수 (The “Center-ness” Score)</h3>
<p>’중심성’은 ’위치에 대한 암시적 제약’을 모델에 부여하는 핵심 메커니즘이다. 이는 “객체를 가장 잘 표현하는 예측은 객체의 중심 근처에서 나온다“는 직관에 기반하며, 객체 중심에서 멀리 떨어진 위치에서 생성되는 저품질의 경계 상자를 억제하기 위해 도입되었다. 중심성 점수는 해당 위치가 객체 중심에 가까울수록 1에 가깝고, 경계에 가까울수록 0에 가까운 값을 갖도록 설계되었다.4</p>
<p>중심성 목표(target)는 회귀 목표 <code>(l*, t*, r*, b*)</code>를 사용하여 다음과 같이 계산된다.<br />
<span class="math math-display">
\text{centerness}^* = \sqrt{\frac{\min(l^*, r^*)}{\max(l^*, r^*)} \times \frac{\min(t^*, b^*)}{\max(t^*, b^*)}}
</span><br />
수식에 제곱근을 사용하는 이유는 중심성 값의 감쇠를 완만하게 만들기 위함이다.2 훈련 시에는 별도의 헤드가 이 목표값을 예측하도록 학습되며, 추론 시에는 예측된 중심성 점수를 최종 분류 점수에 곱하여 순위를 재조정한다. 이 과정을 통해 중심에서 먼 위치에서 나온 예측들의 점수가 자연스럽게 낮아지므로, 후처리 단계인 NMS(Non-Maximum Suppression) 과정에서 쉽게 제거될 수 있다. 이는 YOLOv1이 객체 중심으로 예측을 한정했던 아이디어를, 모든 지점을 사용하되 중심성에 따라 가중치를 부여하는 더 유연하고 일반화된 방식으로 발전시킨 것이다.</p>
<h2>5. FCOS 훈련 방법론: 3중 손실 함수</h2>
<p>FCOS의 손실 함수는 앵커 프리 설계에서 발생하는 특정 문제들을 해결하기 위해 전략적으로 선택된 세 가지 구성 요소의 조합, 즉 ’솔루션 패키지’로 볼 수 있다. 각 손실 함수는 아키텍처 설계와 긴밀하게 연결되어 있다.</p>
<h3>5.1 전체 손실 함수 구조</h3>
<p>FCOS의 전체 손실 함수 <span class="math math-inline">L</span>은 분류 손실(<span class="math math-inline">L_{cls}</span>), 회귀 손실(<span class="math math-inline">L_{reg}</span>), 그리고 중심성 손실(<span class="math math-inline">L_{ctr}</span>)의 가중 합으로 정의된다.2</p>
<p><span class="math math-display">
L(\{p_{x,y}\}, \{t_{x,y}\}, \{ctr_{x,y}\}) = \frac{1}{N_{pos}} \sum_{x,y} L_{cls}(p_{x,y}, c^*_{x,y}) + \frac{\lambda}{N_{pos}} \sum_{x,y} \mathbb{I}_{\{c^*_{x,y}&gt;0\}} \left( L_{reg}(t_{x,y}, t^*_{x,y}) + L_{ctr}(ctr_{x,y}, ctr^*_{x,y}) \right)
</span><br />
여기서 <span class="math math-inline">N_{pos}</span>는 양성 샘플의 개수, <code>λ</code>는 회귀 및 중심성 손실에 대한 가중치(논문에서는 1로 설정), 그리고 <span class="math math-inline">\mathbb{I}_{\{c^*_{x,y}&gt;0\}}</span>는 해당 위치가 전경(foreground) 객체일 때만 회귀 및 중심성 손실을 계산하도록 하는 지시 함수(indicator function)이다.2</p>
<h3>5.2 분류 손실 (Classification Loss): Focal Loss</h3>
<p>앵커 없이 모든 픽셀을 후보로 삼는 FCOS의 설계는 배경에 해당하는 ’쉬운 음성 샘플’이 앵커 기반 모델보다 훨씬 더 압도적으로 많아지는 결과를 초래한다. 이러한 극심한 클래스 불균형 문제를 해결하기 위해, FCOS는 분류 손실로 Focal Loss를 채택했다.<br />
<span class="math math-display">
L_{cls} = FL(p_t) = -\alpha_t (1-p_t)^\gamma \log(p_t)
</span><br />
Focal Loss는 조절 인자 <span class="math math-inline">(1-p_t)^\gamma</span>를 통해 모델이 이미 잘 분류하고 있는 샘플(예: 대부분의 배경, <span class="math math-inline">p_t \to 1</span>)의 손실 기여도를 동적으로 감소시킨다. 이를 통해 모델은 소수이지만 중요한, 어려운 샘플(예: 작거나 가려진 객체)의 학습에 더 집중할 수 있게 된다.</p>
<h3>5.3 회귀 손실 (Regression Loss): IoU/GIoU Loss</h3>
<p>경계 상자의 위치를 직접 회귀하는 FCOS의 방식에는 스케일에 불변하고 평가 지표(IoU)와 직접적으로 연관된 손실 함수가 유리하다. 초기 FCOS는 IoU Loss를 사용했으나, 후속 개선을 통해 GIoU(Generalized IoU) Loss를 도입하여 성능을 더욱 향상시켰다.<br />
<span class="math math-display">
L_{reg} = L_{GIoU} = 1 - GIoU = 1 - \left( IoU - \frac{\vert C - (B \cup B_{gt}) \vert}{\vert C \vert} \right)
</span><br />
여기서 <code>B</code>와 <span class="math math-inline">B_{gt}</span>는 각각 예측된 박스와 실제 박스이며, <code>C</code>는 두 박스를 모두 포함하는 가장 작은 볼록 집합(convex hull)이다. GIoU Loss는 두 박스가 전혀 겹치지 않는 경우에도 <code>IoU</code>가 0이 되어 기울기가 소실되는 <code>IoU Loss</code>의 문제를 해결한다. <code>C</code>와의 관계를 통해 페널티를 부여함으로써, 모델이 예측 박스를 실제 박스 방향으로 점진적으로 이동시키도록 유도하여 더 안정적인 학습을 가능하게 한다.</p>
<h3>5.4 중심성 손실 (Center-ness Loss): Binary Cross-Entropy (BCE) Loss</h3>
<p>중심성 점수는 0과 1 사이의 연속적인 값을 예측하는 회귀 문제이지만, 그 값이 특정 위치가 ’중심에 속할 확률’과 유사한 성격을 띤다. 따라서 FCOS는 이 점수를 학습시키기 위해 이진 교차 엔트로피(BCE) 손실 함수를 사용한다.<br />
<span class="math math-display">
L_{ctr} = BCE(ctr, ctr^*) = - (ctr^* \log(ctr) + (1-ctr^*) \log(1-ctr))
</span><br />
이 손실 함수는 중심성 브랜치의 출력이 목표값에 가깝도록 최적화하며, 전체 손실 함수에 포함되어 다른 두 손실과 함께 종단간(end-to-end)으로 학습된다.</p>
<h2>6. 성능 평가 및 비교 벤치마킹</h2>
<p>FCOS의 효과를 검증하기 위해, COCO <code>test-dev</code> 데이터셋을 기준으로 동시대의 주요 앵커 기반 모델들과의 성능을 비교 분석한다. 비교의 공정성을 위해 가능한 한 동일한 백본 네트워크를 사용한 결과를 중심으로 평가한다.</p>
<h3>6.1 실험 설정</h3>
<p>모든 실험은 대규모 객체 탐지 벤치마크인 MS COCO 데이터셋에서 수행되었다. 성능 지표로는 다양한 IoU 임계값에서의 평균 정밀도(AP), IoU가 0.5일 때의 정밀도(<span class="math math-inline">AP_{50}</span>), 0.75일 때의 정밀도(<span class="math math-inline">AP_{75}</span>), 그리고 객체 크기별 정밀도(<span class="math math-inline">AP_S</span>, <span class="math math-inline">AP_M</span>, <span class="math math-inline">AP_L</span>)가 사용된다.</p>
<h3>6.2 주요 모델과의 성능 비교</h3>
<p>아래 표는 FCOS를 대표적인 1단계 앵커 기반 모델인 RetinaNet, YOLOv3 및 2단계 모델인 Faster R-CNN과 비교한 결과이다.</p>
<table><thead><tr><th>모델 (Detector)</th><th>백본 (Backbone)</th><th>AP</th><th><span class="math math-inline">AP_{50}</span></th><th><span class="math math-inline">AP_{75}</span></th><th><span class="math math-inline">AP_S</span></th><th><span class="math math-inline">AP_M</span></th><th><span class="math math-inline">AP_L</span></th></tr></thead><tbody>
<tr><td>Faster R-CNN</td><td>ResNet-50-FPN</td><td>36.8</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr>
<tr><td>YOLOv3</td><td>DarkNet-53</td><td>33.7</td><td>57.9</td><td>34.4</td><td>18.3</td><td>35.4</td><td>41.9</td></tr>
<tr><td>RetinaNet</td><td>ResNeXt-101-FPN</td><td>41.6</td><td>62.5</td><td>44.7</td><td>24.5</td><td>45.4</td><td>52.8</td></tr>
<tr><td><strong>FCOS</strong></td><td>ResNet-50-FPN</td><td>38.7</td><td>57.4</td><td>41.4</td><td>22.3</td><td>42.5</td><td>49.8</td></tr>
<tr><td><strong>FCOS</strong></td><td>ResNeXt-101-FPN</td><td>42.6</td><td>62.2</td><td>45.7</td><td>25.0</td><td>46.2</td><td>54.4</td></tr>
<tr><td><strong>FCOS (w/ imprv.)</strong></td><td>ResNeXt-64x4d-101-DCN</td><td><strong>49.0</strong></td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr>
</tbody></table>
<p><em>주: 위 표의 수치는 여러 소스에서 집계되었으며, 일부 모델의 세부 AP 메트릭은 해당 소스에서 제공되지 않을 수 있다. “w/ imprv.“는 GIoU, 중심성 샘플링 등 추가 개선 사항이 적용된 모델을 의미한다.</em></p>
<h3>6.3 성능 분석</h3>
<h4>6.3.1 정확도</h4>
<p>표에서 볼 수 있듯이, FCOS는 동일한 ResNet-50-FPN 백본을 사용했을 때 2단계 탐지기의 표준인 Faster R-CNN의 36.8 AP를 상회하는 38.7 AP를 달성했다. 이는 더 간단한 1단계 앵커 프리 구조로도 더 복잡한 2단계 모델보다 높은 정확도를 달성할 수 있음을 입증한 중요한 결과이다. 또한, 더 강력한 ResNeXt-101-FPN 백본을 사용했을 때 FCOS는 42.6 AP를 기록하여, 동일한 백본을 사용한 RetinaNet(41.6 AP)과 비교해도 경쟁력 있는 성능을 보였다. 나아가 Deformable Convolution과 같은 최신 기술을 결합하고 추가적인 개선 사항을 적용한 최상위 FCOS 모델은 COCO <code>test-dev</code>에서 49.0% AP라는 당시 SOTA(State-of-the-Art) 수준의 성능을 달성했다.</p>
<h4>6.3.2 속도 및 효율성</h4>
<p>FCOS는 앵커와 관련된 복잡한 계산(IoU 매칭 등)과 하이퍼파라미터를 제거함으로써 상당한 효율성 향상을 이루었다. 동일한 하드웨어 및 ResNet-50-FPN 백본 조건에서 FCOS는 Faster R-CNN에 비해 훈련 시간이 8.8시간에서 6.5시간으로 단축되었고, 추론 시간 역시 이미지당 56ms에서 44ms로 12ms 더 빨라졌다. 또한, 앵커 기반인 RetinaNet과 비교했을 때도 약간 더 빠른 속도와 낮은 메모리 사용량을 보이는 경향이 있다. 이는 FCOS가 정확도뿐만 아니라 실용적인 측면에서도 우수함을 보여준다.</p>
<h4>6.3.3 종합 평가</h4>
<p>결론적으로 FCOS는 앵커 기반 모델들의 고질적인 문제였던 설계 복잡성과 하이퍼파라미터 의존성을 성공적으로 제거하면서도, 정확도와 속도 측면에서 기존의 주류 모델들을 능가하는 성능을 달성했다. 이를 통해 FCOS는 더 간단하면서도 강력한 객체 탐지기의 새로운 표준을 제시했음을 입증했다.</p>
<h2>7. FCOS의 유산과 발전</h2>
<p>FCOS의 가장 큰 유산은 단순히 하나의 뛰어난 모델을 제시한 것에 그치지 않고, 객체 탐지 연구 커뮤니티에 ’새로운 질문’을 던져 패러다임의 진화를 촉진한 촉매제 역할을 했다는 점에 있다.</p>
<h3>7.1 앵커 프리 탐지의 대중화</h3>
<p>FCOS의 성공은 ’고성능 객체 탐지를 위해서는 앵커 박스가 필수적’이라는 당시의 지배적인 통념에 정면으로 도전하여, 앵커 프리 방식이 주류가 될 수 있음을 명백히 증명했다. 이 연구를 기점으로 수많은 후속 앵커 프리 탐지기들이 등장했으며, FCOS의 핵심 아이디어는 3D 객체 탐지(FCOS3D), 회전된 객체 탐지(FCOSR) 등 다양한 하위 분야로 성공적으로 확장되었다. FCOS는 객체 탐지 연구의 지형을 바꾸고, 더 단순하고 직관적인 접근법에 대한 탐구를 가속화했다.</p>
<h3>7.2 샘플 선택 전략에 대한 새로운 관점: ATSS의 등장</h3>
<p>FCOS는 “앵커가 꼭 필요한가?“라는 질문을 던져 앵커 프리라는 흐름을 만들었고, 그 결과로 등장한 ATSS(Adaptive Training Sample Selection)는 여기서 한 걸음 더 나아가 “앵커 프리와 앵커 기반 모델의 본질적인 차이는 무엇인가?“라는 더 근본적인 질문을 던졌다. ATSS는 FCOS와 RetinaNet을 엄밀하게 비교 분석하여, 두 패러다임 간의 성능 차이를 만드는 핵심 요인이 ’앵커의 유무’라는 구조적 차이가 아니라 ’양성/음성 훈련 샘플을 정의하는 방식’에 있음을 밝혔다.5</p>
<p>ATSS가 발견한 FCOS의 잠재적 문제점은, 실제 경계 상자 내의 모든 점을 후보 양성 샘플로 간주하는 전략이 객체 경계 근처의 ’저품질 양성 샘플’을 다수 포함하게 만들어 최적의 학습을 방해할 수 있다는 것이었다.5 이를 해결하기 위해 ATSS는 각 실제 객체의 통계적 특성(후보 샘플들의 IoU 평균 및 표준편차)을 이용해 양성 샘플을 선택하는 IoU 임계값을 동적으로 설정하는 ‘적응형 샘플 선택’ 방식을 제안했다.5 이 방식은 FCOS의 고정된 스케일 범위 할당 방식보다 더 정교하고 유연하게 고품질의 양성 샘플을 선택할 수 있게 해주었으며, FCOS의 성능을 유의미하게 향상시켰다. 이처럼 FCOS는 그 자체로도 뛰어난 모델이었지만, 동시에 후속 연구를 통해 더 깊은 통찰을 이끌어내는 과학적 질문의 연쇄 반응을 일으킨 중요한 이정표였다.</p>
<h3>7.3 객체 탐지 프레임워크의 통합</h3>
<p>FCOS는 객체 탐지를 의미론적 분할과 같은 다른 밀집 예측 과업들과 동일한 FCN 기반 프레임워크로 통합할 수 있는 가능성을 제시했다. 이로써 객체 탐지는 더 이상 고립된 특별한 과업이 아니라, 컴퓨터 비전의 여러 기본 과업들을 아우르는 더 일반적이고 통일된 프레임워크의 일부로 자리매김하게 되었다. 이는 향후 다양한 비전 과업을 하나의 모델로 처리하는 범용 비전 모델(general-purpose vision model) 개발에 중요한 이론적 기반을 제공했다.</p>
<h2>8. 결론</h2>
<h3>8.1 FCOS의 핵심 기여 요약</h3>
<p>FCOS는 앵커 박스라는 인공적인 구조물에 의존하던 기존 객체 탐지 패러다임에서 벗어나, 이를 의미론적 분할과 같은 직관적인 픽셀 단위 예측 프레임워크로 성공적으로 재정의했다. 이 혁신적인 접근법은 앵커 박스와 필연적으로 연관되었던 복잡한 계산 과정, 까다로운 하이퍼파라미터 튜닝, 그리고 훈련의 비효율성을 근본적으로 제거하여 객체 탐지 모델을 훨씬 더 간단하고 강력하게 만들었다.</p>
<h3>8.2 성능과 효율성의 양립</h3>
<p>FCOS의 가장 중요한 성과는 이러한 구조적 단순화가 성능 저하를 동반하지 않는다는 것을 입증한 데 있다. 오히려 FCOS는 동일 조건에서 기존의 더 복잡한 앵커 기반 2단계 모델인 Faster R-CNN을 능가하는 정확도를 달성했으며, 1단계 모델들과 비교해서도 속도와 정확도 모든 면에서 뛰어난 경쟁력을 보였다. 이는 ’단순함이 곧 강력함’이 될 수 있다는 것을 보여준 사례로, 객체 탐지 연구의 방향성에 큰 영향을 미쳤다.</p>
<h3>8.3 미래 전망</h3>
<p>FCOS가 제시한 앵커 프리 패러다임은 이후 객체 탐지 연구의 거대한 흐름을 형성했으며, 수많은 후속 연구에 영감을 주었다. 또한, FCOS를 통해 촉발된 ’효과적인 훈련 샘플 선택’에 대한 논의는 오늘날 객체 탐지 모델의 성능을 한계까지 끌어올리는 핵심 연구 주제로 자리 잡았다. 결론적으로 FCOS는 단순히 하나의 성공적인 모델을 넘어, 객체 탐지 분야의 복잡성을 걷어내고 문제의 본질에 더 집중하도록 이끈, 현대 객체 탐지 기술 발전의 초석이 된 기념비적인 연구라 할 수 있다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>FCOS: Fully Convolutional One-Stage Object Detection - Viso Suite, https://viso.ai/deep-learning/fcos-object-detection/</li>
<li>FCOS: Fully Convolutional One-Stage Object Detection - CVF Open Access, https://openaccess.thecvf.com/content_ICCV_2019/papers/Tian_FCOS_Fully_Convolutional_One-Stage_Object_Detection_ICCV_2019_paper.pdf</li>
<li>1월 1, 1970에 액세스, https://openaccess.thecvf.com/content_ICCV_20_19/papers/Tian_FCOS_Fully_Convolutional_One-Stage_Object_Detection_ICCV_2019_paper.pdf</li>
<li>FCOS- Anchor Free Object Detection Explained | LearnOpenCV #, https://learnopencv.com/fcos-anchor-free-object-detection-explained/</li>
<li>Bridging the Gap Between Anchor-based and Anchor-free Detection …, https://arxiv.org/abs/1912.02424</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>