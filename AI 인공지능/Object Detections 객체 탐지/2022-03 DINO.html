<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:DINO (2022-03) 대조적 노이즈 제거를 통한 End-to-End 객체 탐지</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>DINO (2022-03) 대조적 노이즈 제거를 통한 End-to-End 객체 탐지</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>DINO (2022-03) 대조적 노이즈 제거를 통한 End-to-End 객체 탐지</span></nav>
                </div>
            </header>
            <article>
                <h1>DINO (2022-03) 대조적 노이즈 제거를 통한 End-to-End 객체 탐지</h1>
<h2>1. 서론</h2>
<p>객체 탐지(Object Detection) 분야는 2020년 DEtection TRansformer (DETR)의 등장으로 근본적인 패러다임 전환을 맞이했다.1 기존의 객체 탐지 모델들은 Faster R-CNN이나 YOLO와 같이 앵커 생성(anchor generation), 비최대 억제(Non-Maximum Suppression, NMS) 등 복잡하고 수동으로 설계된 여러 단계를 포함하는 파이프라인에 의존했다.3 반면 DETR은 Transformer 아키처를 전면적으로 도입하여 객체 탐지를 하나의 집합 예측(set prediction) 문제로 재정의했고, 이를 통해 최초로 완전한 End-to-End 탐지 모델의 가능성을 제시했다.5 이 접근법은 파이프라인을 극적으로 단순화하며 학계에 큰 반향을 일으켰다.</p>
<p>그러나 DETR의 개념적 우아함에도 불구하고, 초기 모델들은 실용적인 측면에서 명확한 한계를 보였다. 가장 큰 문제점은 학습 수렴 속도가 기존 CNN 기반 탐지기에 비해 현저히 느리다는 것이었다.1 또한, Transformer 디코더에 입력되는 학습 가능한 쿼리(query) 임베딩의 물리적 의미가 불분명하여 모델의 작동 방식을 직관적으로 이해하고 개선하기 어려웠으며, 최종 성능 역시 고도로 최적화된 CNN 기반 모델들을 넘어서지 못했다.4</p>
<p>이러한 한계들을 극복하기 위해 DETR의 계보를 잇는 일련의 연구들이 연속적으로 등장하며 기술적 진화를 이끌었다. 이 과정은 단순히 개별적인 개선의 나열이 아니라, 명확한 문제 해결의 서사를 따른다. 먼저, <strong>Deformable DETR</strong>은 Transformer의 전체 어텐션(global attention)으로 인한 막대한 연산량을 해결하기 위해 희소한 샘플링 포인트에만 집중하는 Deformable Attention을 도입하여 수렴 속도를 획기적으로 개선했다.9 다음으로, <strong>DAB-DETR</strong>은 불분명했던 쿼리를 명시적인 4차원 앵커 박스 좌표 <span class="math math-inline">(x, y, w, h)</span>로 정의하여, 각 디코더 레이어가 박스를 점진적으로 정제해나가는 과정을 투명하게 만들었다.8 마지막으로, <strong>DN-DETR</strong>은 학습 초기에 이분 매칭(bipartite matching) 결과가 불안정하여 최적화 목표가 계속 흔들리는 문제를 해결하고자, 노이즈가 추가된 Ground Truth(GT) 박스를 매칭 과정 없이 직접 디코더에 주입하여 안정적인 보조 학습 신호를 제공하는 노이즈 제거(denoising) 기법을 제안했다.5</p>
<p>본 안내서는 이러한 기술적 진화의 정점에 있는 <strong>DINO (DETR with Improved deNoising anchOr boxes)</strong> 모델을 심층적으로 분석한다. DINO는 Deformable DETR의 효율성, DAB-DETR의 명시성, DN-DETR의 안정성을 모두 계승하면서, 여기에 세 가지 핵심적인 기법을 추가하여 DETR 계열 모델 최초로 COCO 벤치마크에서 SOTA(State-of-the-Art) 성능을 달성하는 위업을 이루었다. 그 핵심 기여는 (1) 정답뿐만 아니라 오답까지 학습하여 정밀도를 극대화하는 <strong>대조적 노이즈 제거(Contrastive Denoising)</strong>, (2) 이미지 정보에 기반한 강력한 초기 앵커를 제공하는 <strong>혼합 쿼리 선택(Mixed Query Selection)</strong>, 그리고 (3) 박스 정제 과정을 더욱 효율적으로 만드는 <strong>Look Forward Twice</strong>이다.3 본 안내서는 이 기법들이 어떻게 상호작용하여 DINO를 객체 탐지 분야의 새로운 지평으로 이끌었는지 상세히 기술하고자 한다.</p>
<h2>2.  DINO의 기반 기술: DETR 계열 모델의 발전 과정</h2>
<p>DINO의 혁신을 온전히 이해하기 위해서는 그 토대가 된 선행 연구들의 핵심 아이디어를 먼저 살펴볼 필요가 있다. DINO는 이전 모델들의 장점을 유기적으로 통합한 결과물이며, 각 기반 기술은 DETR의 특정 한계를 해결하는 데 중점을 두었다.</p>
<h3>2.1  Deformable DETR: 효율성과 수렴 속도의 혁신</h3>
<p>초기 DETR의 가장 큰 약점은 Transformer 인코더와 디코더의 어텐션 메커니즘이 이미지의 모든 픽셀 쌍 간의 관계를 계산해야 하므로 발생하는 막대한 연산량과 느린 수렴 속도였다.9 Deformable DETR은 이 문제를 해결하기 위해 Deformable Convolution에서 영감을 받은 Deformable Attention 메커니즘을 제안했다.9</p>
<p>Deformable Attention은 각 쿼리가 전체 이미지 특징 맵을 모두 참고하는 대신, 소수의 핵심적인 샘플링 포인트에만 ’집중(attend)’하도록 설계되었다. 모델은 각 쿼리에 대해 기준점(reference point) 주변의 오프셋을 학습하여 동적으로 샘플링 위치를 결정한다. 이 방식은 어텐션의 계산 복잡도를 이미지 크기에 비례하는 <span class="math math-inline">O(H^2 W^2)</span>에서 샘플링 포인트 수에 비례하는 <span class="math math-inline">O(HWK^2)</span>로 크게 줄여, 학습 수렴을 극적으로 가속하고 메모리 사용량을 절감하는 효과를 가져왔다.9</p>
<p>또한, Deformable DETR은 Multi-scale Feature를 효과적으로 융합하는 방식을 도입했다. 백본 네트워크에서 추출된 다양한 해상도의 특징 맵들로부터 샘플링 포인트를 가져옴으로써, 모델은 큰 객체와 작은 객체를 동시에 효과적으로 탐지할 수 있게 되었다. 특히 작은 객체는 저해상도 특징 맵에서는 정보가 소실되기 쉬운데, 고해상도 특징 맵에서 직접 정보를 샘플링할 수 있게 되면서 작은 객체에 대한 탐지 성능(APs)이 크게 향상되었다.7 DINO 역시 이 Deformable Attention과 Multi-scale 구조를 채택하여 높은 연산 효율성과 강력한 객체 탐지 성능의 기반을 마련했다.</p>
<h3>2.2  DAB-DETR: 쿼리의 명시적 재정의</h3>
<p>Deformable DETR이 효율성 문제를 해결했다면, DAB-DETR은 쿼리의 ‘의미’ 문제를 다루었다. 기존 DETR 모델에서 쿼리는 학습 가능한 고차원 임베딩 벡터로, 이것이 이미지의 어떤 공간적 위치나 크기와 연관되는지 명확하지 않았다.1 이로 인해 디코더가 박스를 정제하는 과정이 마치 블랙박스처럼 작동했다.</p>
<p>DAB-DETR은 이 문제를 해결하기 위해 쿼리를 명시적인 4차원 앵커 박스 좌표, 즉 중심점과 너비, 높이를 나타내는 <span class="math math-inline">(x, y, w, h)</span>로 직접 공식화했다.8 이는 쿼리에 명확한 물리적 의미를 부여한 혁신적인 접근이었다. 이 동적 앵커 박스(Dynamic Anchor Box)는 각 디코더 레이어를 통과하며 점진적으로 정제된다. 구체적으로, <span class="math math-inline">l</span>번째 디코더 레이어는 <span class="math math-inline">l-1</span>번째 레이어에서 출력된 앵커 박스를 입력받아, 박스의 위치와 크기를 미세 조정하기 위한 오프셋 <span class="math math-inline">(\Delta x, \Delta y, \Delta w, \Delta h)</span>를 예측한다.8 이 예측된 오프셋을 이전 박스에 더하여 <span class="math math-inline">l</span>번째 레이어의 정제된 앵커 박스를 생성한다.</p>
<p>이러한 점진적 박스 정제(Iterative Box Refinement) 방식은 모델의 예측 과정을 훨씬 더 투명하고 해석 가능하게 만들었으며, 고전적인 앵커 기반 탐지기와 DETR 계열 모델 사이의 개념적 간극을 좁혔다.1 DINO는 이 DAB-DETR의 프레임워크를 그대로 계승하여, 쿼리를 동적 앵커 박스로 다루고 이를 계층적으로 정제하는 방식을 기본 구조로 채택했다. 이는 DINO의 ‘Look Forward Twice’ 기법이 적용될 수 있는 토대가 되었다.</p>
<h3>2.3  DN-DETR: 이분 매칭 안정화를 통한 학습 가속</h3>
<p>DETR 계열 모델의 또 다른 고질적인 문제는 학습 초기의 불안정성이었다. DETR은 예측된 박스 집합과 실제 GT 박스 집합 간의 최적의 쌍을 찾기 위해 헝가리안 알고리즘을 사용한 이분 매칭을 수행한다.5 그러나 학습 초기에는 모델의 예측이 매우 부정확하기 때문에, 이 매칭 결과가 에폭마다 심하게 요동치는 현상이 발생한다. 이는 각 쿼리에 할당되는 최적화 목표가 계속 바뀌게 만들어 학습을 불안정하게 하고 수렴을 더디게 하는 주된 원인이었다.5</p>
<p>DN-DETR은 이 문제를 해결하기 위해 독창적인 노이즈 제거(Denoising) 보조 과업을 도입했다.13 이 방법의 핵심은 학습 데이터의 GT 박스에 의도적으로 약간의 노이즈(예: 위치 이동, 크기 변화)를 추가한 뒤, 이 ‘오염된’ 박스들을 원래의 학습 쿼리들과 함께 디코더에 직접 입력하는 것이다.5 이 노이즈가 추가된 쿼리들은 이미 자신의 GT 박스 파트너를 알고 있으므로, 불안정한 이분 매칭 과정을 거칠 필요가 없다. 모델은 이 쿼리들에 대해 원래의 깨끗한 GT 박스를 정확히 복원하도록 학습된다.13</p>
<p>이 노이즈 제거 과업은 모델에게 매우 안정적이고 상대적으로 쉬운 학습 신호를 제공한다. 이분 매칭이라는 어려운 과업을 우회함으로써, 디코더는 박스 좌표를 예측하는 방법을 더 빠르고 안정적으로 학습할 수 있게 된다.5 이 보조 과업은 전체 학습 과정을 안정화하고 수렴 속도를 크게 향상시키는 효과를 가져왔다. DINO의 가장 핵심적인 혁신인 ’대조적 노이즈 제거’는 바로 이 DN-DETR의 아이디어를 더욱 발전시키고 확장한 것이다.</p>
<h2>3.  DINO의 핵심 방법론 분석</h2>
<p>DINO는 앞서 설명한 기반 기술들 위에 세 가지 독창적인 방법론을 추가하여 성능을 극대화했다. 이 방법론들은 각각 박스 예측의 정밀도, 쿼리 초기화의 효율성, 그리고 점진적 정제 과정의 최적화라는 DETR의 남은 과제들을 해결하는 데 초점을 맞추고 있다.</p>
<h3>3.1  대조적 노이즈 제거 학습 (Contrastive Denoising Training, CDN)</h3>
<p>DINO의 가장 핵심적인 기여는 DN-DETR의 노이즈 제거 학습을 대조적 학습(Contrastive Learning)의 관점에서 재해석하고 확장한 것이다. 이는 단순한 개선을 넘어, 학습 목표 자체에 근본적인 변화를 가져온 혁신적인 아이디어였다.</p>
<p>DN-DETR의 노이즈 제거는 본질적으로 ‘재구성(reconstruction)’ 과업이다. 즉, 노이즈가 낀 입력으로부터 원본을 복원하는 ‘긍정적(positive)’ 학습만을 수행한다.8 이 방식은 학습을 안정화하는 데는 효과적이었지만, 모델이 정답과 매우 유사하지만 미세하게 다른 ’오답’을 명확하게 구분하도록 가르치지는 못했다. 예를 들어, 하나의 객체 주위에 여러 쿼리가 비슷한 품질의 박스를 예측할 경우, 모델은 이들을 모두 정답에 가깝다고 판단하여 중복된 예측을 억제하지 못하는 경향이 있었다.3 DINO의 개발자들은 모델이 단순히 GT 박스를 잘 맞추는 것을 넘어, GT 박스가 아닌 영역을 ’아니다’라고 명확하게 거절하는 능력, 즉 더 날카로운 결정 경계(decision boundary)를 학습해야 한다고 보았다.</p>
<p>이를 위해 DINO는 대조적 노이즈 제거(CDN)를 제안했다. CDN의 핵심은 하나의 GT 박스에 대해 두 종류의 노이즈 샘플, 즉 <strong>Positive 샘플</strong>과 <strong>Negative 샘플</strong>을 동시에 생성하여 학습에 사용하는 것이다.3 구체적인 과정은 다음과 같다.</p>
<ol>
<li>하나의 GT 박스에 대해 서로 다른 강도의 노이즈를 두 번 적용한다.</li>
<li>상대적으로 GT 박스와의 차이가 적은(노이즈가 약한) 박스를 <strong>Positive 샘플</strong>로 지정한다.</li>
<li>상대적으로 GT 박스와의 차이가 큰(노이즈가 강한) 박스를 <strong>Negative 샘플</strong>로 지정한다.3</li>
</ol>
<p>학습 과정에서 이 두 샘플은 서로 다른 목표를 부여받는다. 모델은 Positive 샘플에 대해서는 DN-DETR과 마찬가지로 원래의 GT 박스를 정확히 복원하도록 학습한다(재구성 손실). 반면, Negative 샘플에 대해서는 특정 객체 클래스가 아닌 ‘객체 없음(no object)’ 클래스를 예측하도록 학습한다(분류 손실).8 이 대조적인 학습 방식은 모델에 다음과 같은 강력한 효과를 가져온다. 모델은 GT 박스 주변 공간에 대한 이해도를 높여, 정답 박스가 존재해야 할 영역과 그렇지 않은 영역을 명확히 구분하는 능력을 기르게 된다. 이는 마치 자기주도학습 분야의 대조적 학습이 유사한 샘플은 가깝게, 다른 샘플은 멀게 밀어내어 표현 공간을 구조화하는 것과 유사한 원리다. 결과적으로, 이 방식은 박스 예측의 정밀도를 크게 향상시키고, 하나의 객체에 대한 중복 출력을 효과적으로 억제하여 전반적인 탐지 성능을 높이는 결정적인 역할을 수행했다.3</p>
<h3>3.2  혼합 쿼리 선택 (Mixed Query Selection)</h3>
<p>디코더의 초기 쿼리 품질은 모델의 최종 성능과 수렴 속도에 지대한 영향을 미친다. 초기 쿼리가 객체가 있을 법한 위치에 대한 정보를 전혀 담고 있지 않다면, 모델은 맨땅에서부터 정답을 찾아야 하므로 학습이 비효율적이다. Deformable DETR은 인코더의 예측 결과를 필터링하여 쿼리를 초기화하는 방식을 제안했지만, 인코더의 초기 예측은 상대적으로 조악하여 최적의 방식은 아니었다.8</p>
<p>DINO는 이 문제를 해결하기 위해 쿼리를 **위치 쿼리(positional query)**와 **콘텐츠 쿼리(content query)**로 분리하고, 각각을 다른 방식으로 초기화하는 ‘혼합 쿼리 선택(Mixed Query Selection)’ 방식을 제안했다.3</p>
<ul>
<li><strong>위치 쿼리:</strong> DINO는 Transformer 인코더가 출력한 Multi-scale 특징 맵에서 객체가 존재할 확률이 높은 상위 K개의 위치를 선택한다. 그리고 이 위치들을 초기 앵커 박스, 즉 위치 쿼리로 사용한다. 이 방식은 이미지의 실제 내용에 기반한 강력한 공간적 사전 정보(spatial prior)를 디코더에 제공하여, 학습 초반부터 유망한 영역에 집중할 수 있도록 돕는다.8</li>
<li><strong>콘텐츠 쿼리:</strong> 위치 정보와 달리, 객체의 의미론적 특징을 담는 콘텐츠 쿼리는 특정 위치에 얽매이지 않도록 학습 가능한 파라미터(learnable parameters)로 남겨둔다. 이는 모델이 위치에 구애받지 않고 더 유연하고 풍부한 객체 표현을 학습할 수 있도록 자유도를 부여한다.8</li>
</ul>
<p>이처럼 위치 정보는 이미지로부터 직접 추출하고, 콘텐츠 정보는 유연하게 학습하도록 분리하는 혼합 방식은 디코더가 더 효율적으로 학습을 시작할 수 있게 만들었다. 이는 특히 학습 초기 단계에서 안정적인 수렴을 유도하고 최종 성능을 향상시키는 데 기여했다.</p>
<h3>3.3  Look Forward Twice</h3>
<p>DAB-DETR에서 도입된 점진적 박스 정제 방식은 각 디코더 레이어가 독립적으로 자신의 파라미터를 업데이트했다. 이는 각 레이어가 오직 자신의 예측 결과와 GT 간의 손실만을 보고 학습한다는 것을 의미한다. DINO는 이 과정을 최적화하기 위해 ’Look Forward Twice’라는 기법을 도입했다.3</p>
<p>이 기법의 핵심 아이디어는 현재 레이어의 학습에 미래의 더 정제된 정보를 활용하는 것이다. 구체적으로, <span class="math math-inline">i</span>번째 디코더 레이어의 파라미터를 업데이트하기 위한 그래디언트를 계산할 때, <span class="math math-inline">i</span>번째 레이어에서 계산된 손실뿐만 아니라, 한 단계 더 나아가 <span class="math math-inline">i+1</span>번째 레이어에서 계산된 손실까지 함께 역전파하여 사용한다.14 즉, <span class="math math-inline">i</span>번째 레이어는 자신의 예측이 다음 단계에서 어떻게 더 개선되는지를 ‘미리 보고’ 학습하는 셈이다.</p>
<p>이 방식은 각 레이어가 더 나은 방향으로 박스를 정제하도록 유도하는 추가적인 감독 신호를 제공한다. 더 정제된 미래의 정보를 현재 학습에 반영함으로써, 전체적인 박스 정제 과정이 더욱 안정적이고 효율적으로 이루어지며, 이는 최종 예측의 정확도를 높이는 데 긍정적인 영향을 미쳤다.</p>
<h2>4.  성능 평가 및 분석</h2>
<p>DINO의 우수성은 COCO(Common Objects in Context) 벤치마크에서의 압도적인 성능으로 입증되었다. DINO는 DETR 계열 모델의 성능을 한 차원 끌어올렸을 뿐만 아니라, 당시 SOTA 모델들이 의존하던 ’규모의 경제’에 대한 통념을 깨뜨렸다.</p>
<h3>4.1  COCO 벤치마크 SOTA 성능 분석</h3>
<p>DINO는 표준 ResNet-50 백본과 Multi-scale 특징을 사용하는 설정에서 놀라운 학습 효율성과 성능을 보여주었다. 단 12 에폭의 학습만으로 49.4 AP를 달성했으며, 이는 당시 최고의 DETR 계열 모델이었던 DN-DETR의 50 에폭 성능에 근접하는 수치였다. 24 에폭 학습 시에는 51.3 AP를 기록하며, 동일 에폭의 DN-DETR 대비 무려 +2.7 AP의 성능 향상을 이루었다.3 이는 DINO가 제안한 방법론들이 수렴 속도와 최종 정확도 모두를 크게 개선했음을 의미한다.</p>
<p>DINO의 진정한 저력은 대규모 백본과 데이터셋을 사용했을 때 드러났다. Swin-L Transformer를 백본으로 사용하고 Objects365 데이터셋으로 사전 학습한 후 COCO 데이터셋에 미세 조정했을 때, DINO는 COCO <code>val2017</code>에서 63.2 AP, <code>test-dev</code>에서 <strong>63.3 AP</strong>라는 경이적인 성능을 달성하며 당시 COCO 리더보드 1위를 차지했다.8</p>
<p>이 성과의 가장 중요한 함의는 DINO가 달성한 ’방식’에 있다. 당시 리더보드 상위권은 SwinV2-G와 같이 30억 개에 달하는 파라미터를 가진 거대 모델들이 차지하고 있었다.3 이 모델들은 Transformer를 특징 추출을 위한 백본으로 사용했지만, 최종 예측을 위해서는 HTC++와 같이 복잡하게 설계된 CNN 기반 탐지 헤드에 의존했다.1 이는 SOTA 성능이 주로 모델과 데이터의 막대한 규모에서 비롯된다는 인식을 낳았다.</p>
<p>그러나 DINO는 이러한 통념에 정면으로 도전했다. 아래 표에서 볼 수 있듯이, DINO는 SwinV2-G 모델의 1/15에 불과한 2억 1800만 개의 파라미터와 훨씬 적은 사전 학습 데이터를 사용하고도 더 높은 AP 점수를 기록했다.3 이는 단순히 더 큰 모델을 만드는 것보다, 더 지능적인 아키텍처와 학습 전략(완전한 End-to-End Transformer 파이프라인과 대조적 노이즈 제거)이 파라미터 효율성 측면에서 훨씬 우월할 수 있음을 증명한 것이다. 이 결과는 DETR 계열 모델을 학문적 대안에서 벗어나 실질적인 SOTA 프레임워크로 격상시키는 결정적인 계기가 되었다.</p>
<p><strong>Table 1: DINO와 주요 SOTA 모델의 COCO 벤치마크 성능 비교</strong></p>
<table><thead><tr><th>모델 (Model)</th><th>백본 (Backbone)</th><th>사전학습 데이터 (Pre-train Data)</th><th>파라미터 (Params)</th><th>AP (val)</th><th>AP (test-dev)</th></tr></thead><tbody>
<tr><td>DN-DETR 3</td><td>ResNet-50</td><td>-</td><td>47M</td><td>48.6 (50 epochs)</td><td>-</td></tr>
<tr><td><strong>DINO (ours)</strong> 3</td><td><strong>ResNet-50</strong></td><td>-</td><td><strong>47M</strong></td><td><strong>51.3 (24 epochs)</strong></td><td>-</td></tr>
<tr><td>DyHead + SwinL 3</td><td>Swin-L</td><td>IN-22K</td><td>≥284M</td><td>-</td><td>60.6</td></tr>
<tr><td>SwinV2-G + HTC++ 3</td><td>SwinV2-G</td><td>IN-22K-300M</td><td>3.0B</td><td>62.5</td><td>63.1</td></tr>
<tr><td><strong>DINO (ours)</strong> 3</td><td><strong>Swin-L</strong></td><td><strong>IN-22K + O365</strong></td><td><strong>218M</strong></td><td><strong>63.2</strong></td><td><strong>63.3</strong></td></tr>
</tbody></table>
<h3>4.2  Ablation Study: 핵심 요소별 기여도 정량 분석</h3>
<p>DINO의 성공은 단 하나의 혁신이 아닌, 세 가지 핵심 방법론이 시너지를 일으킨 결과이다. 논문에서 제시된 Ablation Study는 각 구성 요소가 최종 성능에 얼마나 기여했는지를 정량적으로 보여준다.3 아래 표는 DN-Deformable DETR의 최적화된 버전을 베이스라인으로 하여, DINO의 각 구성 요소를 순차적으로 추가했을 때의 성능 변화를 요약한 것이다.</p>
<p><strong>Table 2: DINO 구성 요소별 성능 기여도 분석 (Ablation Study on COCO val, 12 epochs)</strong></p>
<table><thead><tr><th>설정 (Configuration)</th><th>AP</th><th>ΔAP</th></tr></thead><tbody>
<tr><td>1. Baseline (Optimized DN-Deformable DETR)</td><td>47.0</td><td>-</td></tr>
<tr><td>2. + Mixed Query Selection (MQS)</td><td>47.9</td><td>+0.9</td></tr>
<tr><td>3. + Look Forward Twice (LFT)</td><td>48.3</td><td>+0.4</td></tr>
<tr><td>4. <strong>+ Contrastive Denoising (CDN)</strong> (DINO 최종 모델)</td><td><strong>49.4</strong></td><td><strong>+1.1</strong></td></tr>
</tbody></table>
<p>분석 결과, 세 가지 방법론 모두 성능 향상에 긍정적인 기여를 했음을 명확히 확인할 수 있다. 혼합 쿼리 선택(MQS)은 베이스라인 대비 +0.9 AP를, Look Forward Twice(LFT)는 여기에 추가로 +0.4 AP를 향상시켰다. 가장 주목할 점은 대조적 노이즈 제거(CDN)가 마지막에 추가되었을 때 +1.1 AP라는 가장 큰 폭의 성능 향상을 이끌어냈다는 것이다. 이는 CDN이 박스 예측의 정밀도를 높이고 중복 탐지를 억제하는 데 결정적인 역할을 했으며, DINO의 성능을 SOTA 수준으로 끌어올린 가장 핵심적인 혁신임을 정량적으로 뒷받침한다.</p>
<h2>5.  DINO의 영향력과 미래 전망</h2>
<p>DINO의 등장은 객체 탐지 분야의 연구 지형에 상당한 영향을 미쳤으며, 그 아이디어들은 후속 연구들을 통해 지속적으로 확장되고 있다.</p>
<h3>5.1  객체 탐지 분야에 미친 영향</h3>
<p>DINO가 남긴 가장 큰 유산은 Transformer 기반 End-to-End 탐지 모델을 명실상부한 ’주류(mainstream)’로 만들었다는 점이다.8 이전까지 DETR 계열 모델들은 개념적 혁신에도 불구하고 성능 면에서는 항상 최적화된 CNN 기반 모델들의 뒤를 쫓는 입장이었다. DINO는 이 구도를 완전히 뒤집어, DETR 구조가 SOTA 성능을 달성할 수 있는 가장 강력한 프레임워크 중 하나임을 증명했다.4</p>
<p>또한, DINO는 ’효율적인 SOTA 달성’이라는 새로운 방향성을 제시했다. 단순히 파라미터와 데이터의 규모를 키우는 방식이 아니라, 아키텍처와 학습 방법론의 근본적인 개선을 통해 더 적은 자원으로 더 높은 성능을 달성할 수 있음을 보여주었다.3 이는 자원 제약이 있는 현실 세계의 애플리케이션에 고성능 모델을 적용하는 데 있어 중요한 시사점을 남겼다.</p>
<h3>5.2  후속 연구 및 개념의 확장</h3>
<p>DINO의 성공은 그 자체로 끝이 아니라, 수많은 후속 연구의 강력한 기반이 되었다.</p>
<ul>
<li><strong>Co-DETR:</strong> DINO의 아키텍처를 기반으로 한 Co-DETR은 학습 과정에서 전통적인 one-to-many 방식의 보조 헤드(auxiliary head)를 여러 개 추가하여, Transformer 인코더에 더 풍부하고 밀도 높은 감독 신호를 제공하는 협력적 하이브리드 할당(Collaborative Hybrid Assignments) 훈련 방식을 제안했다.17 이 접근법은 DINO의 성능을 한 단계 더 끌어올리며 새로운 SOTA를 달성했고, DINO가 후속 연구의 중요한 디딤돌 역할을 했음을 보여준다.20</li>
<li><strong>CDN 개념의 확장:</strong> DINO의 핵심 아이디어인 대조적 노이즈 제거는 객체 탐지라는 특정 과업을 넘어 더 넓은 영역으로 확장될 잠재력을 보여주었다. 예를 들어, <strong>CCDN-DETR</strong>은 합성 구경 레이더(SAR) 이미지라는 다른 데이터 도메인에서 객체를 탐지하기 위해 DINO의 CDN 개념을 ‘제약된(constrained)’ 형태로 변형하여 성공적으로 적용했다.21 이는 DINO가 제안한 방법론이 특정 데이터셋에 과적합된 것이 아니라, 일반성과 확장성을 갖춘 강력한 원리임을 시사한다.</li>
</ul>
<h3>5.3  잠재적 한계 및 향후 연구 방향</h3>
<p>모든 혁신적인 모델과 마찬가지로 DINO 역시 한계를 가지며, 이는 새로운 연구 방향을 제시한다.</p>
<ul>
<li><strong>Closed-Set의 한계:</strong> DINO는 COCO 데이터셋처럼 사전에 정의된 클래스 집합 내에서는 최고의 성능을 보이지만, 학습 데이터에 존재하지 않았던 새로운 범주의 객체를 탐지하는 데는 근본적인 한계를 가진다. 이러한 ‘Open-Vocabulary’ 또는 ‘Zero-shot’ 탐지 문제는 DINO 이후의 중요한 연구 주제가 되었다. <strong>Grounding DINO</strong>와 같은 후속 모델은 자연어 텍스트 프롬프트를 활용하여 DINO의 강력한 탐지 능력과 개방형 어휘 능력을 결합함으로써 이 문제를 해결하고자 시도했다.24</li>
<li><strong>실시간성 문제:</strong> DINO는 이전 DETR 모델들에 비해 학습과 추론이 모두 효율적이지만, YOLO 계열과 같이 실시간 처리를 목표로 설계된 경량 모델들에 비해서는 여전히 추론 속도가 느리다.25 따라서 자율 주행이나 로보틱스와 같이 실시간성이 극도로 중요한 응용 분야에 DINO를 직접 적용하기 위해서는 모델 경량화, 양자화, 증류 등 추가적인 추론 최적화 연구가 필요하다.</li>
<li><strong>데이터 품질 의존성:</strong> DINO와 같은 고성능 모델의 성능은 대규모의 고품질 어노테이션 데이터에 크게 의존한다. 최근에는 COCO 데이터셋 자체에도 미세한 어노테이션 오류가 존재하며, 이것이 모델 성능 평가에 미치는 영향을 분석하는 <strong>COCO-ReM</strong>과 같은 연구가 등장했다.26 이는 향후 모델의 성능을 극한까지 끌어올리기 위해서는 모델 아키텍처 개선뿐만 아니라, 데이터의 품질을 높이는 데이터 중심(Data-Centric) AI 접근법이 더욱 중요해질 것임을 시사한다.</li>
</ul>
<h2>6. 결론</h2>
<p>DINO는 Deformable DETR, DAB-DETR, DN-DETR 등 선행 연구들이 쌓아 올린 기술적 성과를 성공적으로 계승하고 통합한 모델이다. 여기에 더해, DINO는 세 가지 독창적인 기여를 통해 객체 탐지 성능을 새로운 차원으로 끌어올렸다. 첫째, Positive 샘플과 Negative 샘플을 동시에 활용하여 모델이 정답과 오답의 경계를 명확히 학습하도록 유도한 **대조적 노이즈 제거(CDN)**는 박스 예측의 정밀도를 극대화했다. 둘째, 이미지 내용에 기반한 위치 쿼리와 학습 가능한 콘텐츠 쿼리를 결합한 **혼합 쿼리 선택(MQS)**은 강력한 사전 정보를 제공하여 학습 효율성을 높였다. 셋째, 미래의 정제된 정보를 현재 학습에 반영하는 <strong>Look Forward Twice</strong>는 점진적 박스 정제 과정을 최적화했다.</p>
<p>기술적 관점에서 DINO의 가장 큰 의의는 Transformer 기반의 완전한 End-to-End 모델이 복잡하게 설계된 기존의 CNN 기반 SOTA 모델들을 성능과 효율성 모든 측면에서 능가할 수 있음을 최초로 증명했다는 점이다. 이는 객체 탐지 분야의 SOTA 패러다임을 전환시키는 중요한 변곡점이 되었으며, 이후의 많은 연구들이 DINO가 제시한 방향성을 따르게 되는 계기를 마련했다.</p>
<p>결론적으로, DINO는 DETR 계열 모델의 오랜 과제였던 학습 불안정성과 성능 한계를 극복하고, 객체 탐지 분야에 새로운 표준을 제시한 기념비적인 모델이다. DINO가 제시한 안정적이고 효율적인 End-to-End 학습 방법론은 향후 객체 탐지뿐만 아니라, 분할(segmentation), 추적(tracking) 등 다양한 컴퓨터 비전 분야의 발전에 지속적으로 기여할 것으로 전망된다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection - SciSpace, https://scispace.com/pdf/dino-detr-with-improved-denoising-anchor-boxes-for-end-to-84dstzi8.pdf</li>
<li>[2005.12872] End-to-End Object Detection with Transformers - arXiv, https://arxiv.org/abs/2005.12872</li>
<li>DINO: DETR WITH IMPROVED DENOISING ANCHOR - OpenReview, https://openreview.net/pdf?id=3mRwyG5one</li>
<li>Exploring the DINO Family Part 1: DINO, The Pioneering Object Detection Model | by Ideacvr, https://medium.com/@ideacvr2024/exploring-the-dino-family-part-1-dino-the-pioneering-object-detection-model-8e453eb5cd34</li>
<li>DN-DETR: Accelerate DETR Training by Introducing Query DeNoising - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Li_DN-DETR_Accelerate_DETR_Training_by_Introducing_Query_DeNoising_CVPR_2022_paper.pdf</li>
<li>facebookresearch/detr: End-to-End Object Detection with Transformers - GitHub, https://github.com/facebookresearch/detr</li>
<li>DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection, https://openreview.net/forum?id=3mRwyG5one</li>
<li>DINO: Advancing Object Detection with Transformers …, https://deepdataspace.com/blog/1/</li>
<li>DETRs with Dynamic Contrastive Denoising Training for Smartphone Assembly Parts, https://www.csroc.org.tw/journal/JOC35-3/JOC3503-13.pdf</li>
<li>NAN-DETR: noising multi-anchor makes DETR better for object detection - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC11513373/</li>
<li>Adding Training Noise To Improve Detections — The Denoising Mechanism - Medium, https://medium.com/data-science-collective/adding-training-noise-to-improve-detections-the-denoising-mechanism-2060b52f0d23</li>
<li>DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection, https://arxiv.org/abs/2203.03605</li>
<li>DN-DETR: Accelerate DETR Training by Introducing Query DeNoising - arXiv, https://arxiv.org/pdf/2203.01305</li>
<li>DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End object Detection | by Poorna Ravuri | Medium, https://medium.com/@poorna.ravuri/dino-detr-with-improved-denoising-anchor-boxes-for-end-to-end-object-detection-1ef246b070cf</li>
<li>CSPCL: Category Semantic Prior Contrastive Learning for Deformable DETR-Based Prohibited Item Detectors - arXiv, https://arxiv.org/html/2501.16665v1</li>
<li>DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection, https://www.researchgate.net/publication/359079872_DINO_DETR_with_Improved_DeNoising_Anchor_Boxes_for_End-to-End_Object_Detection</li>
<li>DETRs with Collaborative Hybrid Assignments Training - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2023/papers/Zong_DETRs_with_Collaborative_Hybrid_Assignments_Training_ICCV_2023_paper.pdf</li>
<li>Paper page - DETRs with Collaborative Hybrid Assignments Training - Hugging Face, https://huggingface.co/papers/2211.12860</li>
<li>Co-DETR. Tackling the instability of DETR’s… | by Tilo Flasche - Medium, https://medium.com/@tnodecode/co-detr-66d2f82f06c3</li>
<li>COCO test-dev Benchmark (Object Detection) - Papers With Code, https://paperswithcode.com/sota/object-detection-on-coco?p=dino-detr-with-improved-denoising-anchor-1</li>
<li>CCDN-DETR: A Detection Transformer Based on Constrained Contrast Denoising for Multi-Class Synthetic Aperture Radar Object Detection - MDPI, https://www.mdpi.com/1424-8220/24/6/1793</li>
<li>CCDN-DETR: A Detection Transformer Based on Constrained Contrast Denoising for Multi-Class Synthetic Aperture Radar Object Detection - PubMed, https://pubmed.ncbi.nlm.nih.gov/38544056/</li>
<li>(PDF) CCDN-DETR: A Detection Transformer Based on Constrained Contrast Denoising for Multi-Class Synthetic Aperture Radar Object Detection - ResearchGate, https://www.researchgate.net/publication/378889334_CCDN-DETR_A_Detection_Transformer_Based_on_Constrained_Contrast_Denoising_for_Multi-Class_Synthetic_Aperture_Radar_Object_Detection</li>
<li>Illustrative examples of the performance limitations of the Grounding DINO model, https://www.researchgate.net/figure/Illustrative-examples-of-the-performance-limitations-of-the-Grounding-DINO-model-a-a_fig1_372370608</li>
<li>Teacher–Student Model Using Grounding DINO and You Only Look Once for Multi-Sensor-Based Object Detection - MDPI, https://www.mdpi.com/2076-3417/14/6/2232</li>
<li>ECCV Poster Benchmarking Object Detectors with COCO: A New Path Forward, https://eccv.ecva.net/virtual/2024/poster/1898</li>
<li>Benchmarking Object Detectors with COCO: A New Path Forward - arXiv, https://arxiv.org/html/2403.18819v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>