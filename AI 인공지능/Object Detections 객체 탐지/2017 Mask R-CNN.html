<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Mask R-CNN (2017)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Mask R-CNN (2017)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>Mask R-CNN (2017)</span></nav>
                </div>
            </header>
            <article>
                <h1>Mask R-CNN (2017)</h1>
<h2>1. 서론</h2>
<h3>1.1 컴퓨터 비전의 계층적 과제: 분류, 탐지, 분할</h3>
<p>컴퓨터 비전의 궁극적인 목표는 인간의 시각 시스템처럼 이미지와 비디오로부터 의미 있는 정보를 추출하고 이해하는 것이다. 이 목표를 달성하기 위한 연구는 정보의 정밀도와 세분화 수준에 따라 계층적인 과제들로 발전해왔다. 이 과제들은 이미지 분류(Classification), 객체 탐지(Object Detection), 그리고 이미지 분할(Image Segmentation)로 크게 나눌 수 있으며, 이미지 분할은 다시 의미론적 분할(Semantic Segmentation)과 인스턴스 분할(Instance Segmentation)로 세분화된다.</p>
<p>**객체 탐지 (Object Detection)**는 이미지 내에 어떤 객체들이 존재하는지 식별하고, 각 객체의 위치를 경계 상자(Bounding Box)라는 직사각형 형태로 근사하여 표시하는 과제이다.1 이는 “이미지 안에 무엇이(What) 어디에(Where) 있는가?“라는 질문에 대한 답을 제공한다. 예를 들어, 거리 사진에서 ’자동차’와 ’사람’을 인식하고 각각을 둘러싸는 사각형을 그리는 것이 객체 탐지의 결과물이다. 그러나 경계 상자는 객체의 대략적인 위치만을 나타낼 뿐, 객체의 정확한 형태나 픽셀 단위의 경계는 제공하지 못하는 본질적인 한계를 가진다.3</p>
<p>**의미론적 분할 (Semantic Segmentation)**은 한 단계 더 나아가 이미지의 모든 픽셀을 특정 의미론적 클래스(semantic class)로 분류하는 것을 목표로 한다.5 예를 들어, 거리 사진의 모든 픽셀에 ‘도로’, ‘건물’, ‘하늘’, ‘자동차’, ’사람’과 같은 레이블을 할당하여, 이미지 전체를 의미 있는 영역들로 나누는 것이다. 이 방식은 장면의 구성 요소를 픽셀 수준에서 이해하게 해주지만, 동일한 클래스에 속하는 개별 객체 인스턴스(instance)를 구분하지는 않는다.1 만약 여러 대의 자동차가 나란히 주차되어 있다면, 의미론적 분할은 이들을 모두 ’자동차’라는 단일 영역으로 취급할 뿐, ‘자동차 1’, ’자동차 2’와 같이 개별적으로 인식하지 못한다.</p>
<p>**인스턴스 분할 (Instance Segmentation)**은 이러한 컴퓨터 비전 과제 계층의 정점에 위치하며, 가장 정교하고 포괄적인 장면 이해를 요구하는 과제이다.6 이는 객체 탐지와 의미론적 분할의 목표를 결합한 것으로, 이미지 내에 존재하는 모든 객체를 탐지할 뿐만 아니라, 각 개별 인스턴스를 픽셀 수준에서 정확하게 분할해내는 것을 목표로 한다.2 즉, 동일한 클래스에 속하는 여러 객체(예: 여러 명의 사람들, 여러 마리의 양)가 있더라도 각각을 고유한 개체로 인식하고, 그 형태를 정밀한 마스크(mask)로 그려내는 것이다.3 이처럼 인스턴스 분할은 객체의 종류, 위치, 그리고 정확한 형태까지 파악함으로써 가장 풍부하고 상세한 시각 정보를 제공한다.</p>
<p>이러한 과제들의 발전 과정은 컴퓨터 비전이 이미지로부터 점점 더 조밀하고(dense) 정밀한(precise) 정보를 추출하는 방향으로 진화해왔음을 보여준다. Mask R-CNN은 이러한 진화의 흐름 속에서 등장하여, 특히 인스턴스 분할 분야에 기념비적인 족적을 남긴 모델이다.</p>
<h3>1.2 Mask R-CNN의 등장 배경 및 학술적 의의</h3>
<p>Mask R-CNN이 2017년에 발표되기 전, 인스턴스 분할 분야는 주로 분할 우선(segmentation-first) 전략에 기반한 복잡한 파이프라인에 의존하는 경향이 있었다.7 이러한 접근법들은 먼저 픽셀 단위의 의미론적 분할을 수행한 후, 복잡한 후처리 과정을 통해 개별 인스턴스를 분리하려 시도했다. 그러나 이 방식은 성능이 제한적이고, 서로 다른 과제를 순차적으로 해결해야 하는 비효율성을 내포하고 있었다.</p>
<p>이러한 배경 속에서 Kaiming He 연구팀은 Faster R-CNN이라는 매우 성공적인 객체 탐지 프레임워크에 주목했다.7 Faster R-CNN은 객체의 위치를 먼저 파악하는 데 탁월한 성능을 보였고, 연구팀은 이 강력한 위치 파악 능력을 기반으로 각 인스턴스의 마스크를 예측하는 것이 더 효율적이고 정확할 것이라는 직관적인 아이디어를 제시했다. 이것이 바로 ‘인스턴스 우선(instance-first)’ 전략이며, Mask R-CNN의 핵심 철학이다.7 즉, 객체를 먼저 탐지하고(instance), 그 다음에 각 탐지된 객체에 대해 분할을 수행하는(segmentation) 방식이다.</p>
<p>Mask R-CNN은 Faster R-CNN의 검증된 2단계 구조를 거의 그대로 유지하면서, 기존의 분류 및 경계 상자 회귀 브랜치와 병렬적으로 작동하는 간단한 마스크 예측 브랜치를 추가하는 최소한의 변경만으로 인스턴스 분할을 구현했다.7 이 구조적 단순함과 유연성은 Mask R-CNN의 가장 큰 강점 중 하나가 되었다. 이 모델은 발표와 동시에 COCO(Common Objects in Context) 챌린지의 세 가지 주요 트랙인 인스턴스 분할, 객체 탐지, 그리고 인간 키포인트 탐지 모두에서 기존의 모든 단일 모델을 능가하는 최고 성능을 달성하며 학계에 큰 충격을 주었다.7 특별한 기교나 복잡한 앙상블 없이도 2016년 챌린지 우승 모델들을 압도하는 성능을 보여줌으로써, Mask R-CNN은 인스턴스 분할 분야의 새로운 표준(de facto standard)이자 후속 연구들이 비교 대상으로 삼는 견고한 베이스라인(solid baseline)으로 확고히 자리매김했다.</p>
<p>Mask R-CNN의 성공은 단순히 성능 수치를 경신한 것을 넘어, 두 개의 주요 연구 흐름이었던 객체 탐지(Faster R-CNN)와 의미론적 분할(FCN)의 장점을 하나의 통합된 프레임워크 안에서 우아하게 결합했다는 점에서 더 큰 의의를 가진다. 강력한 객체 탐지 능력이 정교한 인스턴스 분할을 위한 효과적인 전제 조건이 될 수 있음을 증명한 것이다. 본 안내서는 이처럼 컴퓨터 비전 분야에 큰 획을 그은 Mask R-CNN 모델의 아키텍처, 핵심적인 기술 혁신, 수학적 원리, 그리고 성능 벤치마크를 심층적으로 분석하고자 한다. 또한, 모델의 한계점과 이것이 어떻게 후속 연구들의 방향성에 영향을 미쳤는지를 고찰함으로써, 이 기념비적인 모델에 대한 포괄적이고 깊이 있는 이해를 제공하는 것을 목표로 한다.</p>
<h2>2.  Mask R-CNN의 구조적 분석</h2>
<p>Mask R-CNN의 구조를 이해하기 위해서는 그 개념적 토대가 된 R-CNN 계열 모델들의 발전 과정을 먼저 살펴보는 것이 필수적이다. Mask R-CNN은 어느 날 갑자기 등장한 것이 아니라, 객체 탐지 분야에서 수년간 축적된 아이디어와 점진적인 개선의 정점에 서 있는 모델이기 때문이다.</p>
<h3>2.1  R-CNN 계열 모델의 진화: 개념적 토대</h3>
<p>R-CNN 계열의 진화는 객체 탐지의 정확도를 유지하면서 계산 효율성을 극대화하는 방향으로 이루어졌다.</p>
<ul>
<li><strong>R-CNN (Regions with CNN features):</strong> 2014년에 제안된 R-CNN은 딥러닝을 객체 탐지에 성공적으로 적용한 선구적인 모델이다.11 R-CNN의 파이프라인은 여러 단계로 구성된다. 첫째, 선택적 탐색(Selective Search)과 같은 전통적인 컴퓨터 비전 알고리즘을 사용하여 이미지에서 객체가 있을 법한 약 2000개의 후보 영역(Region Proposal)을 생성한다.12 둘째, 각 후보 영역을 고정된 크기로 변환한 후, 사전 훈련된 CNN(Convolutional Neural Network)에 개별적으로 입력하여 특징 벡터를 추출한다. 마지막으로, 이 특징 벡터를 SVM(Support Vector Machine) 분류기와 경계 상자 회귀(Bounding Box Regressor)에 전달하여 최종 클래스와 정제된 경계 상자를 예측한다.11 R-CNN은 높은 정확도를 달성했지만, 수천 개의 후보 영역에 대해 각각 CNN 연산을 수행해야 했기 때문에 학습과 추론 속도가 매우 느리다는 치명적인 단점을 가졌다.11</li>
<li><strong>Fast R-CNN:</strong> R-CNN의 속도 문제를 해결하기 위해 2015년에 등장한 Fast R-CNN은 계산 공유(computation sharing)라는 혁신적인 아이디어를 도입했다.11 후보 영역별로 CNN을 통과시키는 대신, 전체 이미지에 대해 단 한 번만 CNN 연산을 수행하여 고수준의 특징 맵(feature map)을 생성한다.16 그 후, 각 후보 영역(RoI, Region of Interest)을 이 공유된 특징 맵에 투영하고, RoIPool(Region of Interest Pooling)이라는 새로운 레이어를 통해 각 RoI에 해당하는 특징들을 고정된 크기의 벡터로 추출한다.12 이 고정 크기 벡터는 후속 FC(Fully Connected) 레이어를 거쳐 분류와 경계 상자 회귀를 동시에 수행한다. 이 구조 덕분에 Fast R-CNN은 R-CNN보다 학습은 9배, 추론은 213배나 빨라지는 획기적인 속도 향상을 이루었다.15</li>
<li><strong>Faster R-CNN:</strong> Fast R-CNN은 속도를 크게 개선했지만, 여전히 선택적 탐색과 같은 외부 알고리즘에 의존하여 후보 영역을 생성하는 병목 현상이 남아있었다.18 2015년 말에 제안된 Faster R-CNN은 이 마지막 병목마저 제거하며 완전한 종단 간(end-to-end) 딥러닝 객체 탐지 시스템을 완성했다.7 Faster R-CNN의 핵심은 **영역 제안 네트워크(Region Proposal Network, RPN)**의 도입이다.16 RPN은 Fast R-CNN과 컨볼루션 특징을 공유하는 작은 신경망으로, 외부 알고리즘 대신 데이터로부터 직접 후보 영역을 학습하여 생성한다. 이로써 후보 영역 제안 과정이 전체 네트워크에 통합되어 GPU 상에서 효율적으로 계산될 수 있게 되었고, 거의 비용 없이(nearly cost-free) 영역 제안이 가능해졌다.19 Faster R-CNN은 RPN이 후보 영역을 제안하는 1단계와 Fast R-CNN이 해당 영역을 분류하고 위치를 정제하는 2단계로 구성된, 오늘날 2단계(two-stage) 검출기의 표준 아키텍처를 정립했다. Mask R-CNN은 바로 이 강력하고 효율적인 Faster R-CNN의 2단계 프레임워크를 그 기본 골격으로 채택하고 확장한 모델이다.9</li>
</ul>
<h3>2.2  백본 네트워크와 특징 피라미드 네트워크 (Backbone and Feature Pyramid Network - FPN)</h3>
<p>Mask R-CNN의 아키텍처는 입력 이미지로부터 계층적인 특징을 추출하는 백본 네트워크에서 시작된다.</p>
<ul>
<li>
<p><strong>백본 네트워크 (Backbone Network):</strong> 백본 네트워크는 이미지의 픽셀 정보를 의미론적 정보(semantic information)가 풍부한 고차원 특징 맵으로 변환하는 역할을 하는 깊은 컨볼루션 신경망이다.9 일반적으로 ImageNet과 같은 대규모 데이터셋으로 사전 훈련된 모델을 사용하여, 이미지의 기본적인 시각적 패턴(가장자리, 질감, 색상 등)에 대한 지식을 활용한다. Mask R-CNN에서는 주로 ResNet(Residual Network)이나 ResNeXt와 같은 고성능 아키텍처가 백본으로 사용된다.9 백본 네트워크는 여러 개의 컨볼루션 블록을 통과하면서 점차 공간적 해상도는 낮아지고(downsampling), 채널의 깊이는 깊어지는 특징 맵 계층(feature map hierarchy)을 생성한다. 낮은 수준의 계층은 공간적으로 정밀한 정보를 담고 있고, 높은 수준의 계층은 더 추상적이고 의미론적인 정보를 담고 있다.</p>
</li>
<li>
<p><strong>특징 피라미드 네트워크 (Feature Pyramid Network - FPN):</strong> 이미지 속 객체들은 다양한 크기를 가질 수 있다. 전통적인 CNN은 마지막 계층의 특징 맵만을 사용하여 예측을 수행하므로, 작은 객체에 대한 정보가 소실되거나 큰 객체의 세부 정보를 놓치기 쉽다. FPN은 이러한 다중 스케일(multi-scale) 문제를 해결하기 위해 고안된 독창적인 구조이다.9 FPN은 백본 네트워크의 여러 해상도 계층에서 특징 맵을 추출한 후, 이를 효과적으로 결합하여 다중 스케일 특징 피라미드를 구축한다.23</p>
</li>
</ul>
<p>FPN의 작동 방식은 두 가지 주요 경로로 구성된다.</p>
<ol>
<li>
<p><strong>하향식 경로 (Bottom-up pathway):</strong> 이는 일반적인 백본 네트워크의 순전파 과정으로, 입력 이미지로부터 시작하여 점차 공간 해상도가 줄어들고 의미 정보가 풍부해지는 특징 맵 계층을 생성한다.</p>
</li>
<li>
<p><strong>상향식 경로와 측면 연결 (Top-down pathway and lateral connections):</strong> FPN의 핵심은 상향식 경로에 있다. 가장 높은 수준(가장 의미론적이지만 해상도가 낮은)의 특징 맵에서 시작하여, 업샘플링(upsampling)을 통해 점차 해상도를 높여나간다. 이 과정에서, 하향식 경로의 동일한 공간 해상도를 가진 특징 맵을 측면 연결(lateral connection)을 통해 가져와 합산(element-wise addition)한다. 이 연결을 통해 저해상도의 강한 의미 정보와 고해상도의 정밀한 위치 정보가 융합된다.9</p>
</li>
</ol>
<p>결과적으로 FPN은 모든 스케일 수준에서 풍부한 의미 정보를 담고 있는 새로운 특징 맵 피라미드를 생성한다. 이 피라미드의 각 층은 서로 다른 크기의 객체를 탐지하는 데 특화되어 있어, Mask R-CNN이 이미지 내의 작은 객체부터 큰 객체까지 모두 효과적으로 탐지하고 분할할 수 있도록 하는 결정적인 역할을 수행한다.21</p>
<h3>2.3  1단계: 영역 제안 네트워크 (Stage 1: Region Proposal Network - RPN)</h3>
<p>RPN은 Faster R-CNN에서 도입된 핵심 모듈로, 이미지 내에서 객체가 존재할 가능성이 높은 영역, 즉 RoI를 신속하게 제안하는 역할을 한다.24 Mask R-CNN은 이 RPN을 1단계에서 그대로 사용한다.</p>
<p>RPN은 FPN이 생성한 다중 스케일 특징 맵 위에서 작동하는 작은 완전 컨볼루션 네트워크(FCN)이다.9 각 특징 맵의 모든 위치에서 슬라이딩 윈도우(sliding window) 방식으로 작동하며, 미리 정의된 다양한 크기(scale)와 종횡비(aspect ratio)를 가진 <strong>앵커 박스(anchor boxes)</strong> 집합을 기준으로 삼는다.21</p>
<p>각 앵커 위치에서 RPN은 두 가지 예측을 동시에 수행한다:</p>
<ol>
<li><strong>객체성 점수 (Objectness Score):</strong> 해당 앵커가 객체를 포함하고 있는지(전경, foreground) 혹은 배경(background)인지에 대한 이진 분류 확률을 예측한다.</li>
<li><strong>경계 상자 회귀 (Bounding Box Regression):</strong> 앵커 박스의 위치와 크기를 실제 객체에 더 잘 맞도록 미세 조정하기 위한 4개의 좌표 보정값(<code>dx, dy, dw, dh</code>)을 예측한다.</li>
</ol>
<p>이 과정을 통해 RPN은 수많은 앵커 박스 중에서 객체를 포함할 가능성이 높은 소수의 후보 영역(RoI)들을 점수와 함께 생성한다. 이후 비최대 억제(Non-Maximum Suppression, NMS)를 적용하여 심하게 겹치는 제안들을 제거하고, 점수가 높은 상위 N개의 RoI를 선별하여 2단계로 전달한다.4 RPN 덕분에 Mask R-CNN은 이미지 전체를 무차별적으로 탐색할 필요 없이, 유망한 영역에만 계산 자원을 집중할 수 있어 높은 효율성을 달성한다.</p>
<h3>2.4  2단계: 헤드 아키텍처 (Stage 2: Head Architecture)</h3>
<p>RPN이 제안한 RoI들은 2단계인 헤드 네트워크로 전달되어 최종 예측을 수행한다. 이 헤드 아키텍처는 Mask R-CNN의 핵심적인 확장이 이루어지는 부분이다. 먼저, 각 RoI는 서로 다른 크기와 종횡비를 가지므로, 후속 처리를 위해 고정된 크기의 특징 맵으로 변환되어야 한다. 이 과정은 RoIAlign이라는 혁신적인 기법(3장에서 상세히 설명)을 통해 수행된다.</p>
<p>RoIAlign을 통해 추출된 고정 크기의 특징 벡터는 세 개의 독립적인 브랜치(head)로 병렬적으로 입력된다.7</p>
<ol>
<li><strong>분류 헤드 (Classification Head):</strong> 이 브랜치는 RoI 내에 있는 객체의 구체적인 클래스(예: ‘사람’, ‘자동차’, ‘고양이’ 등 <code>K</code>개의 클래스 + ‘배경’)를 예측한다. 일반적으로 여러 개의 FC 레이어와 최종적으로 소프트맥스(softmax) 활성화 함수로 구성되어, 각 클래스에 대한 확률 분포를 출력한다.9</li>
<li><strong>경계 상자 회귀 헤드 (Bounding Box Regression Head):</strong> 이 브랜치는 RPN이 제안한 RoI의 위치를 더욱 정밀하게 조정하는 역할을 한다. 분류 헤드와 마찬가지로 FC 레이어로 구성되며, 각 클래스별로 최종 경계 상자의 위치와 크기를 미세 조정하기 위한 4개의 좌표 보정값을 출력한다.9</li>
<li><strong>마스크 예측 헤드 (Mask Prediction Head):</strong> 이것이 바로 Mask R-CNN을 Faster R-CNN과 구별하는 가장 중요한 추가 요소이다.7 이 브랜치는 각 RoI에 대해 픽셀 단위의 이진 분할 마스크(binary segmentation mask)를 생성하는 역할을 한다. 마스크 예측은 픽셀 간의 공간적 구조를 보존해야 하므로, FC 레이어 대신 여러 개의 컨볼루션 레이어로 구성된 작은 FCN(Fully Convolutional Network)으로 구현된다.11 이 FCN은 RoIAlign으로부터 받은 특징 맵을 입력으로 받아, 저해상도의 마스크(예: 28x28 픽셀)를 출력한다. 이 마스크는 추론 시에 최종 탐지된 경계 상자 크기에 맞게 확대(upscale)되어 객체의 정밀한 윤곽을 나타낸다.24</li>
</ol>
<p>이러한 모듈식 구조는 Mask R-CNN의 강력한 유연성과 확장성의 기반이 된다. 각기 다른 역할을 수행하는 모듈들(백본, RPN, 다중 헤드)이 명확하게 분리되어 있으면서도 종단 간 학습을 통해 유기적으로 연결된다. 이 설계 덕분에 연구자들은 전체 프레임워크를 재설계할 필요 없이 특정 모듈(예: 백본 네트워크)만 교체하여 성능을 개선하거나 새로운 작업에 적용하기가 용이하다. 예를 들어, ResNet-50 백본을 더 강력한 ResNeXt-101로 교체하거나, 미래에 등장할 새로운 백본 아키텍처를 적용하는 것이 간단하다.7 이러한 적응성은 Mask R-CNN이 발표 이후 오랫동안 다양한 연구와 응용 분야에서 표준 베이스라인으로 활용될 수 있었던 핵심적인 이유 중 하나이다.</p>
<h2>3.  핵심 혁신: RoIAlign</h2>
<p>Mask R-CNN의 성공을 가능하게 한 가장 결정적인 기술적 혁신은 RoIAlign 레이어의 도입이다. 겉보기에는 사소한 개선처럼 보일 수 있지만, RoIAlign은 2단계 검출기 프레임워크에서 픽셀 단위의 정밀한 예측을 가능하게 만든 핵심 열쇠였다. 이를 이해하기 위해서는 먼저 기존의 RoIPool 방식이 가진 근본적인 문제점을 파악해야 한다.</p>
<h3>3.1  RoIPool의 근본적인 문제: 공간 양자화</h3>
<p>Fast R-CNN과 Faster R-CNN에서 사용된 RoIPool의 목적은 다양한 크기의 RoI로부터 고정된 크기(예: 7x7)의 특징 맵을 추출하는 것이다. 이는 후속 FC 레이어에 일관된 크기의 입력을 제공하기 위해 필수적인 과정이다. 그러나 RoIPool은 이 과정에서 두 번의 <strong>공간 양자화(spatial quantization)</strong>, 즉 좌표를 정수로 반올림하는 연산을 수행한다.27</p>
<ol>
<li><strong>첫 번째 양자화:</strong> RPN이 제안한 RoI의 좌표는 원본 이미지 스케일에서의 부동 소수점 값이다. 이를 컨볼루션 특징 맵 스케일로 변환할 때, 특징 맵의 픽셀 그리드에 맞추기 위해 좌표를 정수로 반올림한다. 예를 들어, 특징 맵의 스트라이드(stride)가 16일 때, 원본 이미지의 x좌표 250.5는 특징 맵에서 15.656…이 되는데, 이를 16으로 반올림하는 식이다.29 이 과정에서 RoI의 경계가 원래 위치에서 약간 벗어나는 첫 번째 불일치가 발생한다.</li>
<li><strong>두 번째 양자화:</strong> 정수 좌표로 변환된 RoI 영역을 목표 크기(예: 7x7)의 빈(bin)으로 나눌 때, 각 빈의 경계 또한 정수 좌표로 다시 한번 반올림된다.28 예를 들어, 20x20 크기의 RoI를 7x7로 나누면 각 빈의 크기는 약 2.85x2.85가 되는데, 이를 2x2 또는 3x3과 같은 정수 크기의 영역으로 근사하는 것이다.</li>
</ol>
<p>이러한 두 번의 거친 양자화 과정은 RoI의 실제 위치와 그로부터 추출된 특징 간의 미세한 공간적 불일치(misalignment)를 누적시킨다.21 객체의 클래스를 분류하거나 경계 상자의 위치를 대략적으로 예측하는 작업에서는 이러한 작은 오차가 비교적 큰 영향을 미치지 않을 수 있다. 그러나 픽셀 하나하나의 소속을 정확히 판단해야 하는 인스턴스 분할 작업에서는 이 불일치가 치명적인 정보 손실로 이어진다.21 이는 마치 흐릿하고 왜곡된 돋보기로 정밀한 그림을 그리려는 것과 같아서, 결과적으로 마스크의 경계가 부정확하고 뭉개지는 현상을 야기한다. Mask R-CNN의 저자들은 Faster R-CNN이 애초에 “네트워크 입력과 출력 간의 픽셀 대 픽셀 정렬을 위해 설계되지 않았다“고 명시하며 이 문제를 정확히 지적했다.30</p>
<h3>3.2  RoIAlign의 작동 원리: 보간법을 통한 정합성 보존</h3>
<p>RoIAlign은 RoIPool의 양자화 문제를 해결하기 위해 고안된, 양자화가 없는(quantization-free) 레이어이다.30 RoIAlign의 핵심 아이디어는 모든 좌표 계산에서 부동 소수점의 정밀도를 그대로 유지하고, 이산적인 그리드 위치가 아닌 곳의 특징 값은 **이중 선형 보간법(bilinear interpolation)**을 사용하여 추정하는 것이다.21</p>
<p>RoIAlign의 작동 과정은 다음과 같이 단계별로 이루어진다 27:</p>
<ol>
<li><strong>정확한 영역 추출 (Exact Region Extraction):</strong> RPN으로부터 전달받은 RoI의 부동 소수점 좌표를 특징 맵 스케일로 변환할 때 반올림하지 않는다. 예를 들어, <code>(x, y)</code> 좌표는 스트라이드로 나눈 부동 소수점 값 <code>(x/stride, y/stride)</code>를 그대로 사용한다.27</li>
<li><strong>영역 분할 (Dividing the Region):</strong> 변환된 RoI 영역을 목표 크기(예: 7x7)의 빈으로 나눌 때도 경계를 반올림하지 않는다. 각 빈의 너비와 높이는 <code>(RoI_width / 7, RoI_height / 7)</code>과 같이 부동 소수점 값으로 정확하게 계산된다.28</li>
<li><strong>샘플링 및 이중 선형 보간법 (Sampling and Bilinear Interpolation):</strong> 각 빈 내부에서 고정된 개수(예: 4개)의 샘플링 포인트(sampling points)를 규칙적인 간격으로 정의한다. 이 샘플링 포인트들의 좌표 역시 부동 소수점이다. 각 샘플링 포인트의 특징 값은 직접적으로 존재하지 않으므로, 특징 맵 상에서 해당 포인트를 둘러싸고 있는 가장 가까운 4개의 그리드 포인트(픽셀) 값을 이용하여 이중 선형 보간법으로 계산한다.23 이중 선형 보간법은 4개 포인트까지의 거리에 따라 가중 평균을 내어 값을 추정하는 방식으로, 픽셀 사이의 연속적인 값을 근사할 수 있게 해준다.27</li>
<li><strong>집계 (Aggregation):</strong> 각 빈에 대해 보간법으로 계산된 4개의 샘플링 포인트 값들을 집계(aggregation)한다. 일반적으로 최대 풀링(max pooling)이나 평균 풀링(average pooling)이 사용된다. 이 집계된 값이 해당 빈의 최종 출력 값이 된다.27</li>
</ol>
<p>이러한 과정을 통해 RoIAlign은 입력 특징과 출력 특징 간의 공간적 정합성을 픽셀 이하(sub-pixel) 수준까지 엄밀하게 보존한다. 양자화로 인한 정보 손실이 없기 때문에, 마스크 예측 헤드는 객체의 경계에 대한 훨씬 더 정확하고 충실한(faithful) 특징 정보를 전달받게 된다.30</p>
<h3>3.3  성능에 미친 영향</h3>
<p>RoIAlign의 도입이 Mask R-CNN의 성능에 미친 영향은 지대했다. Mask R-CNN 논문에 따르면, RoIPool을 RoIAlign으로 교체하는 이 “사소해 보이는 변경“만으로도 마스크 AP(Average Precision)가 10%에서 50%까지 상대적으로 향상되었다.30 특히, IoU(Intersection over Union) 임계값이 높은, 즉 더 정밀한 분할을 요구하는 엄격한 평가 지표(예: AP75)에서 성능 향상이 더욱 두드러졌다.25</p>
<p>이는 RoIAlign이 Mask R-CNN의 핵심 과제인 고품질 인스턴스 마스크 생성을 실질적으로 가능하게 만든 결정적인 기술적 돌파구였음을 의미한다. 마스크 예측 브랜치를 추가한다는 개념적 아이디어는 직관적이지만, 그 아이디어가 실제로 높은 성능으로 구현될 수 있었던 것은 RoIAlign이 제공하는 충실한 공간 정보 덕분이었다. 즉, 마스크 브랜치라는 새로운 ’임무’가 픽셀 정렬이라는 ’필요’를 낳았고, 이 필요가 RoIAlign이라는 ’해결책’의 발명을 촉발했으며, 이 해결책이 다시 모델의 전반적인 성능 향상을 이끌어낸 명확한 인과 관계가 성립하는 것이다.</p>
<h2>4.  다중 작업 손실 함수</h2>
<p>Mask R-CNN은 단일 네트워크 내에서 객체 분류, 경계 상자 위치 조정, 그리고 픽셀 단위 마스크 생성이라는 세 가지 서로 다른 작업을 동시에 수행한다. 이러한 다중 작업 학습(multi-task learning)을 효과적으로 지도하기 위해, 각 작업에 대한 손실 함수를 결합한 통합된 형태의 다중 작업 손실 함수(multi-task loss function)를 사용한다.</p>
<h3>4.1  통합 손실 함수</h3>
<p>학습 과정에서 각 후보 RoI에 대해 계산되는 전체 손실 함수 <code>L</code>은 세 가지 개별 손실 함수의 단순 합으로 정의된다.23<br />
<span class="math math-display">
L = L_{cls} + L_{box} + L_{mask}
</span><br />
여기서 <span class="math math-inline">L_{cls}</span>는 분류 손실(classification loss), <span class="math math-inline">L_{box}</span>는 경계 상자 회귀 손실(bounding box regression loss), 그리고 <span class="math math-inline">L_{mask}</span>는 마스크 분할 손실(mask segmentation loss)을 나타낸다. 이 세 가지 손실은 각기 다른 헤드 브랜치의 출력을 기반으로 계산되며, 이들의 합이 역전파(backpropagation)를 통해 전체 네트워크의 가중치를 업데이트하는 데 사용된다.</p>
<h3>4.2  분류 및 경계 상자 손실 (<code>L_cls</code>, <code>L_box</code>)</h3>
<p>Mask R-CNN의 분류 및 경계 상자 회귀 헤드는 Faster R-CNN의 구조와 역할을 그대로 계승하므로, 이 두 손실 함수 역시 Faster R-CNN에서 사용된 것과 동일하다.32</p>
<ul>
<li><strong>분류 손실 (<span class="math math-inline">L_{cls}</span>):</strong> 분류 헤드는 각 RoI가 <code>K</code>개의 객체 클래스와 1개의 배경 클래스 중 어디에 속하는지를 예측한다. 따라서 <span class="math math-inline">L_{cls}</span>는 다중 클래스 분류 문제에 표준적으로 사용되는 <strong>로그 손실(log loss)</strong>, 즉 교차 엔트로피 손실(cross-entropy loss)로 계산된다.</li>
<li><strong>경계 상자 회귀 손실 (<span class="math math-inline">L_{box}</span>):</strong> 경계 상자 회귀 헤드는 예측된 경계 상자와 실제 경계 상자(ground-truth box) 간의 기하학적 차이를 최소화하도록 학습된다. 이를 위해 <strong>Smooth L1 Loss</strong>가 사용된다.35 Smooth L1 Loss는 예측값과 실제값의 차이가 작을 때는 L2 손실(제곱 오차)처럼 작동하여 부드러운 그래디언트를 제공하고, 차이가 클 때는 L1 손실(절대값 오차)처럼 작동하여 이상치(outlier)에 덜 민감하게 반응하는 특성을 가진다.18 이 덕분에 학습 과정이 더 안정적이고 견고해진다.</li>
</ul>
<p>Faster R-CNN의 손실 함수는 다음과 같이 정의된다. 각 RoI에 대해,<br />
<span class="math math-display">
L({p_i}, {t_i}) = \frac{1}{N_{cls}} \sum_i L_{cls}(p_i, p_i^*) + \lambda \frac{1}{N_{reg}} \sum_i p_i^* L_{reg}(t_i, t_i^*)
</span><br />
여기서 <span class="math math-inline">i</span>는 미니배치 내 앵커의 인덱스, <span class="math math-inline">p_i</span>는 앵커 <span class="math math-inline">i</span>가 객체일 확률 예측값, <span class="math math-inline">p_i^*</span>는 실제 레이블(객체이면 1, 배경이면 0)이다. <span class="math math-inline">t_i</span>는 예측된 경계 상자 좌표 벡터, <span class="math math-inline">t_i^*</span>는 실제 경계 상자 좌표 벡터이다. <span class="math math-inline">L_{cls}</span>는 로그 손실, <span class="math math-inline">L_{reg}</span>는 Smooth L1 손실이다. <span class="math math-inline">p_i^* L_{reg}</span> 항은 <span class="math math-inline">p_i^*=1</span>일 때, 즉 앵커가 객체일 때만 회귀 손실이 활성화됨을 의미한다. Mask R-CNN의 <span class="math math-inline">L_{cls}</span>와 <span class="math math-inline">L_{box}</span>는 이와 동일한 원리로 계산된다.</p>
<h3>4.3  마스크 손실 (<code>L_mask</code>) 및 FCN과의 비교</h3>
<p>Mask R-CNN의 가장 독창적인 부분은 마스크 손실 <span class="math math-inline">L_{mask}</span>의 설계에 있다. 이는 전통적인 의미론적 분할 모델인 FCN(Fully Convolutional Network)의 접근 방식과 중요한 차이를 보인다.</p>
<p>마스크 예측 헤드는 각 RoI에 대해, 총 <code>K</code>개의 클래스 각각에 해당하는 <code>m x m</code> 크기(예: 28x28)의 이진 마스크를 예측한다. 따라서 헤드의 총 출력 차원은 <code>K * m^2</code>가 된다.36</p>
<ul>
<li>
<p><strong>FCN과의 핵심 차이점:</strong> FCN과 같은 일반적인 분할 모델은 각 픽셀에 대해 <code>K</code>개의 클래스 중 하나를 할당하는 다중 클래스 분류 문제로 접근한다. 이를 위해 마지막 레이어에서 <strong>픽셀 단위 소프트맥스(per-pixel softmax)</strong> 활성화 함수를 사용하고, **다항 교차 엔트로피 손실(multinomial cross-entropy loss)**을 최소화한다.36 이 방식은 각 픽셀이 단 하나의 클래스에만 속할 수 있다는 상호 배타적인 가정을 내포하며, 클래스 간 경쟁을 유발한다.</p>
</li>
<li>
<p><strong>Mask R-CNN의 접근 방식:</strong> 반면, Mask R-CNN은 마스크 생성과 클래스 분류를 분리(decouple)한다.36 마스크 헤드는 “이 픽셀이 어떤 클래스인가?“를 묻는 대신, “이 픽셀이 클래스 1에 속하는가?”, “이 픽셀이 클래스 2에 속하는가?” 와 같이</p>
</li>
</ul>
<p><code>K</code>개의 독립적인 이진 분류 문제를 푼다. 이를 위해 <strong>픽셀 단위 시그모이드(per-pixel sigmoid)</strong> 활성화 함수를 사용하고, **평균 이진 교차 엔트로피 손실(average binary cross-entropy loss)**을 <span class="math math-inline">L_{mask}</span>로 정의한다.36</p>
<ul>
<li>
<p><strong>마스크 손실 수식:</strong> 특정 RoI가 실제 클래스(ground-truth class) <code>k</code>와 연관되어 있다고 가정할 때, <span class="math math-inline">L_{mask}</span>는 오직 <code>k</code>번째로 예측된 마스크 채널에 대해서만 계산된다. 다른 <code>K-1</code>개의 마스크 출력은 해당 RoI의 손실 계산에 기여하지 않는다.37 수식은 다음과 같다 36:<br />
<span class="math math-display">
L_{mask} = -\frac{1}{m^2} \sum_{i=1}^{m} \sum_{j=1}^{m} [y_{ij} \log(\hat{y}_{ij}^k) + (1-y_{ij}) \log(1-\hat{y}_{ij}^k)]
</span><br />
여기서 <span class="math math-inline">y_{ij}</span>는 <code>m x m</code> 크기의 실제 마스크에서 픽셀 <code>(i, j)</code>의 값(객체에 속하면 1, 아니면 0)이고, <span class="math math-inline">\hat{y}_{ij}^k</span>는 실제 클래스 <code>k</code>에 대해 예측된 마스크에서 해당 픽셀이 객체에 속할 확률(시그모이드 출력값)이다.</p>
</li>
<li>
<p><strong>접근 방식의 장점:</strong> 이러한 설계는 매우 중요한 장점을 가진다. 클래스 간 경쟁을 제거함으로써, 네트워크는 각 클래스의 형태적 특징(shape prior)을 학습하는 데 더 집중할 수 있다.32 예를 들어, 한 사람이 테니스 라켓을 들고 있는 장면에서 두 객체가 겹치는 경우, 소프트맥스 기반 모델은 각 픽셀을 ‘사람’ 또는 ‘라켓’ 중 하나로 강제로 할당해야 하므로 경계가 모호해질 수 있다. 하지만 Mask R-CNN은 ‘사람’ 마스크와 ‘라켓’ 마스크를 독립적으로 생성하므로 이러한 간섭 없이 각 객체의 형태를 더 명확하게 학습할 수 있다. 이처럼 손실 함수의 설계는 모델의 ‘인스턴스 우선’ 철학을 수학적으로 구현한 것이며, 복잡한 장면에서 고품질의 마스크를 생성할 수 있게 하는 핵심적인 원동력이다.</p>
</li>
</ul>
<h2>5.  학습 및 추론 파이프라인</h2>
<p>Mask R-CNN은 학습(training) 단계와 추론(inference) 단계에서 서로 다른 목표를 최적화하기 위해 비대칭적인 파이프라인을 채택한다. 이러한 설계는 모델의 성능과 효율성 사이의 균형을 맞추기 위한 매우 실용적이고 영리한 선택이다.</p>
<h3>5.1  학습 파이프라인</h3>
<p>학습 단계의 주된 목표는 네트워크의 모든 가중치를 최적화하여 주어진 데이터에 대한 전체 손실을 최소화하는 것이다. 이를 위해 Mask R-CNN은 모든 구성 요소가 동시에 학습에 참여하는 <strong>병렬적(parallel)</strong> 구조를 사용한다.38</p>
<p>학습 파이프라인의 흐름은 다음과 같다:</p>
<ol>
<li>입력 이미지가 백본 네트워크와 FPN을 통과하여 다중 스케일 특징 맵을 생성한다.</li>
<li>RPN은 이 특징 맵을 기반으로 다수의 RoI를 제안한다.</li>
<li>제안된 RoI 중에서 실제 객체(ground-truth)와 일정 수준 이상(예: IoU &gt; 0.5) 겹치는 RoI는 긍정(positive) 샘플로, 그렇지 않은 RoI는 부정(negative) 샘플로 레이블링된다.</li>
<li>샘플링된 각 RoI에 대해 RoIAlign을 통해 고정 크기 특징 맵이 추출된다.</li>
<li>이 특징 맵은 2단계의 세 헤드(분류, 경계 상자 회귀, 마스크)로 <strong>동시에</strong> 전달된다.</li>
<li>각 헤드는 예측값을 출력하고, 이 예측값과 실제 레이블을 비교하여 <span class="math math-inline">L_{cls}</span>, <span class="math math-inline">L_{box}</span>, <span class="math math-inline">L_{mask}</span>가 각각 계산된다. (긍정 샘플에 대해서만 <span class="math math-inline">L_{box}</span>와 <span class="math math-inline">L_{mask}</span>가 계산된다.)</li>
<li>최종적으로 통합 손실 <span class="math math-inline">L = L_{cls} + L_{box} + L_{mask}</span>이 계산되고, 이 단일 손실 값이 역전파되어 네트워크의 모든 가중치(백본, FPN, RPN, 3개의 헤드)를 종단 간(end-to-end) 방식으로 업데이트한다.23</li>
</ol>
<p>이 병렬 구조는 매우 중요하다. 모든 작업(분류, 위치 조정, 분할)이 동일한 공유 특징으로부터 동시에 학습되기 때문에, 백본 네트워크는 세 가지 작업 모두에 유용한 풍부하고 일반화된 특징 표현을 학습하도록 강제된다. 만약 학습 과정이 순차적으로 진행된다면, 그래디언트 흐름이 복잡해지고 특정 작업에 편향된 특징이 학습될 수 있다. 따라서 병렬 학습은 전체적인 모델 성능을 최적화하기 위한 필수적인 전략이다.</p>
<h3>5.2  추론 파이프라인</h3>
<p>추론 단계의 목표는 학습된 모델을 사용하여 새로운 이미지에 대해 가능한 한 빠르고 정확하게 예측 결과를 생성하는 것이다. 이 단계에서는 그래디언트 계산이 필요 없으며, 가중치는 고정되어 있다. Mask R-CNN은 효율성을 극대화하기 위해 학습 때와는 다른 <strong>순차적(sequential)</strong> 필터링 방식을 사용한다.38</p>
<p>추론 파이프라인의 흐름은 다음과 같다:</p>
<ol>
<li>학습 때와 마찬가지로, 입력 이미지가 백본과 FPN을 통과하고, RPN이 다수의 RoI(예: FPN 사용 시 1000개)를 제안한다.</li>
<li>제안된 모든 RoI에 대해, 계산 비용이 비교적 저렴한 <strong>분류 헤드와 경계 상자 회귀 헤드</strong>가 먼저 실행된다. 이를 통해 각 RoI에 대한 클래스 점수와 정제된 경계 상자 좌표를 얻는다.</li>
<li><strong>비최대 억제(Non-Maximum Suppression, NMS)</strong> 알고리즘이 적용된다. NMS는 동일한 객체를 가리키는 중복된 경계 상자들을 제거하고, 가장 높은 신뢰도 점수를 가진 상자만 남기는 역할을 한다.</li>
<li>NMS를 통과한 경계 상자들 중에서, 클래스 점수가 가장 높은 상위 <code>N</code>개(원본 논문에서는 100개)의 최종 탐지 결과(detection boxes)만을 선별한다.38</li>
<li>마지막으로, 계산 비용이 가장 높은 <strong>마스크 예측 헤드</strong>는 이 선별된 <code>N</code>개의 고신뢰도 탐지 결과에 대해서만 실행된다. 이를 통해 최종 인스턴스 마스크가 생성된다.</li>
</ol>
<h3>5.3  비대칭적 파이프라인의 효과</h3>
<p>학습과 추론 파이프라인의 이러한 비대칭성은 두 가지 중요한 효과를 가져온다.38</p>
<ul>
<li><strong>추론 속도 향상:</strong> 마스크 예측 헤드는 여러 컨볼루션 레이어로 구성된 FCN이기 때문에 계산 비용이 상당히 높다. RPN이 제안하는 수백, 수천 개의 모든 RoI에 대해 이 마스크 헤드를 실행하는 것은 매우 비효율적이다. 추론 시에는 먼저 저비용의 헤드를 사용하여 유망한 후보들을 걸러내고, 가장 가능성이 높은 소수의 후보에만 고비용의 마스크 예측을 집중함으로써 전체 추론 시간을 대폭 단축할 수 있다.</li>
<li><strong>정확도 향상:</strong> 이 순차적 필터링 방식은 부수적으로 정확도 향상에도 기여할 수 있다. RPN이 제안한 RoI 중에는 객체를 제대로 포함하지 않거나 배경인 경우가 많다. 이러한 낮은 품질의 RoI에 대해 마스크를 생성하면, 종종 노이즈가 많거나 의미 없는 마스크가 출력될 수 있다. 신뢰도 점수가 높은 상위 RoI에 대해서만 마스크를 생성함으로써, 이러한 저품질의 예측 결과를 사전에 차단하고 최종 출력의 전반적인 품질을 높일 수 있다.</li>
</ul>
<p>결론적으로, Mask R-CNN의 비대칭적 파이프라인은 학습과 추론이라는 서로 다른 목표에 맞춰 최적화된 설계의 좋은 예시이다. 학습 시에는 통합된 그래디언트 신호를 통한 전역적 최적화를 위해 병렬 구조를, 추론 시에는 계산 자원의 효율적 배분을 통한 속도 및 정확도 향상을 위해 순차적 필터링 구조를 채택한 것이다. 이는 복잡한 딥러닝 시스템을 실용적으로 구현하기 위한 깊이 있는 공학적 고려가 반영된 결과라 할 수 있다.</p>
<h2>6.  성능 평가 및 벤치마크 분석</h2>
<p>어떤 딥러닝 모델의 우수성을 입증하기 위해서는 객관적인 데이터셋과 표준화된 평가 지표를 통한 정량적 평가가 필수적이다. Mask R-CNN은 당시 가장 도전적인 벤치마크였던 COCO 데이터셋에서 압도적인 성능을 기록하며 그 가치를 증명했다.</p>
<h3>6.1  COCO 데이터셋과 평가 지표</h3>
<ul>
<li><strong>COCO (Common Objects in Context) 데이터셋:</strong> COCO는 객체 탐지 및 인스턴스 분할 분야의 표준 벤치마크로 널리 사용되는 대규모 데이터셋이다.7 80개의 객체 카테고리와 33만 개 이상의 이미지를 포함하며, 특히 하나의 이미지에 다수의 객체 인스턴스가 복잡하게 얽혀있고, 작은 객체들이 많이 포함되어 있어 모델의 실제적인 성능과 일반화 능력을 평가하기에 매우 적합하다.22</li>
<li><strong>주요 평가 지표 - AP (Average Precision):</strong> 인스턴스 분할 성능은 주로 AP(Average Precision)를 통해 평가된다. AP는 정밀도-재현율 곡선(Precision-Recall curve)의 아래 면적(Area Under Curve)으로 계산되며, 모델이 얼마나 정확하게 객체를 탐지하고 분할하는지를 종합적으로 나타내는 지표이다.40 COCO 챌린지에서는 다음과 같은 다양한 AP 지표를 사용한다.</li>
<li><strong>AP (Primary Challenge Metric):</strong> 가장 대표적인 종합 성능 지표. IoU(Intersection over Union) 임계값을 0.5에서 0.95까지 0.05 간격으로 변화시키면서 계산한 10개의 AP 값을 평균 낸 것이다. 이 지표는 모델이 다양한 수준의 정밀도 요구사항을 얼마나 잘 만족시키는지를 평가한다.40</li>
<li><strong>AP50:</strong> IoU 임계값을 0.5로 고정했을 때의 AP. 이는 PASCAL VOC 챌린지에서 사용된 전통적인 지표로, 예측 마스크와 실제 마스크가 50% 이상 겹치면 올바른 예측으로 간주하는 비교적 관대한 기준이다.40</li>
<li><strong>AP75:</strong> IoU 임계값을 0.75로 고정했을 때의 AP. 예측 마스크가 실제 마스크와 75% 이상 겹쳐야 하므로, 훨씬 더 정확하고 정밀한 분할을 요구하는 엄격한 기준이다.40</li>
<li><span class="math math-inline">AP_S</span>, <span class="math math-inline">AP_M</span>, <span class="math math-inline">AP_L</span>:** 각각 작은(Small), 중간(Medium), 큰(Large) 크기의 객체에 대한 AP를 나타낸다. 이 지표들은 모델이 객체의 크기에 따라 어떤 성능 특성을 보이는지 분석하는 데 유용하다.</li>
</ul>
<h3>6.2  백본 네트워크별 성능 비교</h3>
<p>Mask R-CNN의 성능은 특징 추출을 담당하는 백본 네트워크의 아키텍처와 깊이에 따라 크게 달라진다. 원본 논문에서는 COCO <code>test-dev</code> 데이터셋에 대한 평가 결과를 제시하며, 이를 통해 이전의 최첨단(state-of-the-art) 모델들과의 성능을 직접적으로 비교했다. 아래 표는 해당 논문에서 발췌한 핵심 성능 데이터를 정리한 것이다.7</p>
<table><thead><tr><th>모델 (백본 네트워크)</th><th>AP</th><th>AP50</th><th>AP75</th><th><span class="math math-inline">AP_S</span></th><th><span class="math math-inline">AP_M</span></th><th><span class="math math-inline">AP_L</span></th></tr></thead><tbody>
<tr><td>MNC (ResNet-101-C4)</td><td>24.6</td><td>44.3</td><td>24.8</td><td>4.7</td><td>25.9</td><td>43.6</td></tr>
<tr><td>FCIS+++ (ResNet-101)</td><td>33.6</td><td>54.5</td><td>-</td><td>-</td><td>-</td><td>-</td></tr>
<tr><td><strong>Mask R-CNN (ResNet-101-C4)</strong></td><td>33.1</td><td>54.9</td><td>34.8</td><td>12.1</td><td>35.6</td><td>51.1</td></tr>
<tr><td><strong>Mask R-CNN (ResNet-101-FPN)</strong></td><td>35.7</td><td>58.0</td><td>37.8</td><td>15.5</td><td>38.1</td><td>52.4</td></tr>
<tr><td><strong>Mask R-CNN (ResNeXt-101-FPN)</strong></td><td><strong>37.1</strong></td><td><strong>60.0</strong></td><td><strong>39.4</strong></td><td><strong>16.9</strong></td><td><strong>39.9</strong></td><td><strong>53.5</strong></td></tr>
</tbody></table>
<p><em>표 1: COCO test-dev 데이터셋에서의 인스턴스 분할 마스크 AP 성능 비교. MNC는 COCO 2015, FCIS는 COCO 2016 챌린지 우승 모델이다. FCIS+++는 다중 스케일 학습/테스트, 수평 뒤집기 테스트 등 복잡한 기법을 적용한 버전이다.</em></p>
<p>이 표는 Mask R-CNN의 우수성과 그 구조적 선택의 타당성을 명확하게 보여주는 정량적 증거이다. 몇 가지 핵심적인 분석 포인트를 짚어볼 수 있다.</p>
<ol>
<li><strong>FPN의 효과 입증:</strong> 동일한 ResNet-101 백본을 사용하더라도, FPN이 없는 C4 백본(전통적인 Faster R-CNN 방식)을 사용했을 때의 AP는 33.1인 반면, FPN을 적용했을 때는 35.7로 약 2.6 AP 포인트 상승했다. 이는 FPN의 다중 스케일 특징 융합이 다양한 크기의 객체를 분할하는 데 매우 효과적임을 명백히 보여준다.</li>
<li><strong>백본 네트워크의 확장성:</strong> ResNet-101-FPN 백본을 더 강력한 아키텍처인 ResNeXt-101-FPN으로 교체했을 때, AP는 35.7에서 37.1로 1.4 AP 포인트 추가 상승했다. 이는 Mask R-CNN 프레임워크가 더 강력한 백본 네트워크의 성능 향상을 온전히 수용할 수 있는 확장 가능한 구조임을 의미하며, 이는 2.4절에서 논의된 모듈식 설계의 장점을 뒷받침한다.</li>
<li><strong>최첨단 성능 달성:</strong> 가장 강력한 버전인 Mask R-CNN (ResNeXt-101-FPN)은 37.1 AP를 기록하며, 복잡한 학습 기법을 총동원한 이전 챌린지 우승 모델 FCIS+++(33.6 AP)를 특별한 기교 없이도 큰 차이로 능가했다.7 이는 Mask R-CNN 아키텍처 자체가 근본적으로 우수함을 시사한다.</li>
</ol>
<p>더 깊이 분석해보면, 모든 모델에서 AP50 점수에 비해 AP75 점수가 현저히 낮다는 점을 발견할 수 있다. 예를 들어, ResNeXt-101-FPN 모델의 AP50은 60.0이지만 AP75는 39.4로 20포인트 이상 급락한다. 이는 객체의 대략적인 영역을 맞추는 것(IoU 0.5)은 비교적 쉽지만, 객체의 경계를 매우 정밀하게 분할하는 것(IoU 0.75)은 훨씬 더 어려운 과제임을 보여준다. Mask R-CNN이 이전 모델들에 비해 AP75에서 특히 큰 성능 향상을 보인다는 점은, 3장에서 설명한 RoIAlign이 픽셀 단위 정렬을 통해 바로 이 어려운 고정밀 분할 문제 해결에 결정적으로 기여했음을 시사하는 강력한 증거이다.</p>
<h2>7.  한계점 및 후속 연구 동향</h2>
<p>Mask R-CNN은 인스턴스 분할 분야에 혁신을 가져왔지만, 동시에 몇 가지 내재적인 한계점을 가지고 있었다. 역설적으로, 이 모델의 엄청난 성공은 그 한계점들을 명확하게 드러내는 계기가 되었고, 이는 후속 연구들이 나아갈 방향을 제시하는 중요한 촉매 역할을 했다. Mask R-CNN 이후의 인스턴스 분할 연구는 크게 두 가지 방향, 즉 ’마스크 품질 개선’과 ’속도 개선’으로 나뉘어 발전했다.</p>
<h3>7.1  Mask R-CNN의 내재적 한계</h3>
<p>Mask R-CNN의 주요 한계점은 다음과 같이 요약할 수 있다.</p>
<ul>
<li><strong>계산 복잡성 및 속도:</strong> RPN과 헤드 네트워크로 구성된 2단계(two-stage) 구조는 본질적으로 1단계(one-stage) 모델보다 복잡하고 느리다.21 논문에서 보고된 5 fps(초당 프레임 수)의 속도는 많은 실시간 애플리케이션에 적용하기에는 부족했다.7</li>
<li><strong>높은 자원 요구량:</strong> 깊은 백본 네트워크와 다중 헤드 구조는 학습과 추론 과정 모두에서 상당한 양의 GPU 메모리와 연산 능력을 필요로 한다.14 이로 인해 제한된 하드웨어 환경에서는 사용이 어려웠다.</li>
<li><strong>작은 객체에 대한 성능 한계:</strong> FPN을 통해 다중 스케일 문제를 크게 완화했지만, 여전히 해상도가 매우 낮은 이미지나 극도로 작은 객체에 대한 탐지 및 분할 성능은 상대적으로 떨어지는 경향이 있었다.9</li>
<li><strong>거친 마스크 경계:</strong> 마스크 예측 헤드는 계산 효율성을 위해 비교적 저해상도(예: 28x28)의 특징 맵에서 마스크를 생성한 후, 이를 다시 원본 해상도로 확대(upscale)한다. 이 과정에서 보간(interpolation)이 이루어지기 때문에, 최종 마스크의 경계가 뭉개지거나 세밀한 부분이 표현되지 않는 ’blobby’한 결과가 나타날 수 있다.44</li>
<li><strong>고비용의 데이터 어노테이션:</strong> Mask R-CNN을 학습시키기 위해서는 픽셀 단위로 정교하게 레이블링된 대규모 데이터셋이 필요하다. 이러한 마스크 어노테이션 작업은 단순한 경계 상자 레이블링에 비해 훨씬 더 많은 시간과 노력이 소요되는 노동 집약적인 과정이다.14</li>
</ul>
<h3>7.2  후속 연구 동향 1: 마스크 품질 개선</h3>
<p>Mask R-CNN의 거친 경계 문제를 해결하려는 노력은 마스크 품질을 향상시키는 방향의 연구를 촉진했다.</p>
<ul>
<li><strong>PointRend:</strong> 이 문제를 해결한 가장 대표적인 후속 연구는 PointRend이다.44 PointRend는 고전 컴퓨터 그래픽스의 렌더링(rendering) 개념에서 영감을 얻었다. 이미지 분할 결과는 대부분의 영역(객체 내부)이 부드럽고, 경계 영역에서만 급격한 변화가 일어난다는 점에 착안했다. 따라서 모든 픽셀에 대해 동일한 계산을 수행하는 대신, 불확실성이 높은 경계 영역의 픽셀들을 **적응적으로 샘플링(adaptively sample)**하여 해당 포인트들에 대해서만 정밀한 예측을 수행한다.46 이 포인트들은 저해상도 예측 결과에서 가장 불확실한 지점(예: 예측 확률이 0.5에 가까운 지점)을 반복적으로 세분화하는 방식으로 선택된다. 선택된 각 포인트에 대해, 백본 네트워크의 고해상도 특징 맵에서 직접 특징을 보간하여 작은 MLP(Multi-Layer Perceptron)를 통해 최종 예측을 수행한다.44 이 방식은 계산 효율성을 유지하면서도 매우 선명하고(crisp) 정교한 마스크 경계를 생성할 수 있다. PointRend는 독립적인 모듈로 설계되어, Mask R-CNN의 기존 마스크 헤드를 간단히 교체하는 것만으로 쉽게 통합될 수 있다는 장점이 있다.48</li>
<li><strong>기타 개선 연구:</strong> 이 외에도 객체의 경계 정보를 명시적으로 학습하여 마스크가 경계선에 더 잘 정렬되도록 하는 <strong>Boundary-preserving Mask R-CNN (BMask R-CNN)</strong> 51이나, 마스크의 품질 자체를 평가하는 별도의 헤드를 추가하여 분류 점수와 마스크 품질 간의 불일치를 보정하는 <strong>Mask Scoring R-CNN</strong> 41과 같은 연구들이 마스크 품질을 개선하기 위해 제안되었다.</li>
</ul>
<h3>7.3  후속 연구 동향 2: 속도 개선 및 1단계 접근법</h3>
<p>Mask R-CNN의 속도 한계는 실시간 처리가 중요한 산업 현장에서의 적용을 어렵게 만들었고, 이는 1단계 인스턴스 분할 모델의 등장을 촉진했다. 1단계 모델들은 영역 제안과 최종 예측(분류, 경계 상자, 마스크)을 별도의 단계로 나누지 않고, 단일 네트워크에서 병렬적으로 동시에 수행하여 속도를 획기적으로 개선하는 것을 목표로 한다.</p>
<ul>
<li><strong>YOLACT (You Only Look At CoefficienTs):</strong> 실시간 인스턴스 분할을 목표로 한 대표적인 1단계 모델이다.2 YOLACT는 두 개의 병렬 브랜치를 사용한다. 첫 번째 브랜치는 이미지 전체에 적용될 수 있는 소수의 고품질 <strong>프로토타입 마스크(prototype masks)</strong> 집합을 생성한다. 두 번째 브랜치는 각 객체 인스턴스를 탐지하고, 해당 인스턴스에 대한 마스크를 생성하기 위해 프로토타입 마스크들을 어떻게 선형 결합(linear combination)할지를 나타내는 **마스크 계수(mask coefficients)**를 예측한다. 최종 마스크는 프로토타입 마스크들과 예측된 계수들의 행렬 곱셈 및 후처리(cropping)를 통해 매우 빠르게 생성된다. 이 독창적인 접근법은 2단계 모델의 복잡성을 제거하고 실시간에 가까운 성능을 달성했다.52</li>
<li><strong>TensorMask:</strong> 또 다른 혁신적인 1단계 접근법으로, 전통적인 슬라이딩 윈도우(sliding-window) 방식을 인스턴스 분할에 적용했다.44 TensorMask는 이미지의 각 위치에서, 해당 위치를 중심으로 하는 국소적인 마스크를 4차원 텐서(4D tensor) 형태로 직접 예측한다. 이는 영역 제안 단계를 완전히 제거하고 조밀한(dense) 예측을 수행하는 새로운 패러다임을 제시했다는 점에서 의의가 있다.52</li>
</ul>
<p>이처럼 Mask R-CNN의 성공은 그 자체로도 큰 성과였지만, 동시에 그 한계점들이 후속 연구자들에게 명확한 문제의식을 제공하며 전체 분야의 발전을 이끄는 원동력이 되었다. Mask R-CNN이 정립한 높은 정확도의 기준점은 ’속도’와 ’품질’이라는 두 가지 핵심적인 연구 방향을 파생시켰고, 오늘날 인스턴스 분할 분야는 이 두 축을 중심으로 끊임없이 발전하고 있다.</p>
<h2>8.  결론</h2>
<p>Mask R-CNN은 컴퓨터 비전, 특히 인스턴스 분할 분야의 역사에서 하나의 분수령을 이룬 기념비적인 모델이다. 이 모델은 단순히 기존의 성능을 뛰어넘는 것을 넘어, 문제에 접근하는 방식 자체에 대한 패러다임 전환을 이끌었으며, 이후 수많은 연구와 응용의 기틀을 마련했다.</p>
<h3>8.1  Mask R-CNN의 핵심 기여 요약</h3>
<p>Mask R-CNN의 핵심적인 학술적, 기술적 기여는 다음 세 가지로 요약할 수 있다.</p>
<ol>
<li><strong>직관적이고 유연한 통합 프레임워크 제시:</strong> Mask R-CNN은 매우 성공적인 객체 탐지 모델인 Faster R-CNN의 2단계 구조를 최소한으로 수정하여, 분류, 경계 상자 회귀, 마스크 예측이라는 세 가지 작업을 하나의 우아한 프레임워크로 통합했다. 이 ‘인스턴스 우선’ 접근법은 복잡했던 기존의 인스턴스 분할 파이프라인을 대체하며, 개념적으로 단순하면서도 강력한 새로운 표준을 제시했다.</li>
<li><strong>RoIAlign을 통한 픽셀 단위 정합성 확보:</strong> 2단계 검출기 프레임워크에서 픽셀 단위의 정밀한 예측을 수행할 때 발생하는 공간적 불일치 문제를 RoIAlign이라는 혁신적인 레이어를 통해 해결했다. 양자화를 배제하고 이중 선형 보간법을 사용한 이 기법은 마스크 예측의 정확도를 획기적으로 향상시켰으며, 이후 픽셀 수준의 예측을 요구하는 많은 모델에 필수적인 구성 요소가 되었다.</li>
<li><strong>작업 분리를 통한 고품질 마스크 생성:</strong> 마스크 예측을 클래스 예측과 분리하고, 픽셀 단위 시그모이드와 이진 교차 엔트로피 손실을 사용한 독창적인 헤드 및 손실 함수 설계는 클래스 간 경쟁을 없애고 네트워크가 각 객체의 형태적 특징 학습에 집중할 수 있도록 만들었다. 이 설계는 특히 객체들이 겹쳐 있는 복잡한 장면에서 고품질의 마스크를 생성하는 데 결정적인 역할을 했다.</li>
</ol>
<h3>8.2  인스턴스 분할 분야의 패러다임 전환</h3>
<p>Mask R-CNN의 등장은 인스턴스 분할 연구의 지형을 완전히 바꾸어 놓았다. 이 모델은 압도적인 성능으로 COCO 벤치마크를 평정하며, 이후 발표되는 거의 모든 인스턴스 분할 논문에서 비교 대상으로 삼는 ’공통 언어’이자 표준 베이스라인이 되었다. 의료 영상 분석에서의 종양 및 장기 분할 54, 자율 주행에서의 차량 및 보행자 인식 2, 위성 영상 분석을 통한 건물 및 지형지물 추출 16 등 수많은 실제 산업 응용 분야에서 Mask R-CNN은 강력한 기반 기술로 채택되었다.11</p>
<p>동시에, Mask R-CNN의 성공은 그 자체의 한계점, 즉 상대적으로 느린 속도와 완벽하지 않은 경계 품질을 명확하게 부각시키는 역할도 했다. 이는 학계에 새로운 연구 과제를 던져주었고, PointRend와 같이 마스크 품질을 극대화하는 연구와 YOLACT처럼 실시간 성능을 목표로 하는 1단계 모델 연구라는 두 개의 주요한 후속 연구 흐름을 촉발시키는 직접적인 계기가 되었다. 이처럼 Mask R-CNN은 단순히 하나의 정답을 제시한 것이 아니라, 더 나은 해결책을 찾기 위한 활발한 논의와 경쟁의 장을 연 것이다.</p>
<h3>8.3  향후 전망</h3>
<p>오늘날 인스턴스 분할 분야는 Transformer 기반의 ViTDet 39이나 쿼리 기반(query-based)의 Mask2Former 39와 같은 새로운 아키텍처들이 등장하며 또 다른 패러다임의 변화를 맞이하고 있다. 이들 모델은 2단계 접근법의 복잡성을 줄이면서도 더 높은 성능을 달성하며 새로운 가능성을 열어가고 있다.</p>
<p>그럼에도 불구하고 Mask R-CNN이 남긴 기술적 유산은 여전히 깊은 영향을 미치고 있다. ‘강력한 탐지를 기반으로 분할을 수행하는 인스턴스 우선’ 철학, ’정확한 예측을 위한 충실한 특징 정렬’의 중요성, 그리고 ’복잡한 문제를 해결하기 위한 작업 분리’라는 핵심 아이디어들은 오늘날의 최신 모델 설계에도 다양한 형태로 변주되며 녹아들어 있다. 따라서 Mask R-CNN은 단순히 과거의 뛰어난 모델로 기억되는 것을 넘어, 현대 컴퓨터 비전의 발전을 이끈 핵심적인 아이디어들의 집약체이자, 앞으로도 오랫동안 연구자와 개발자들에게 영감을 주는 중요한 기술적 이정표로 남을 것이다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>What is the difference between semantic segmentation, object detection and instance segmentation? - Data Science Stack Exchange, https://datascience.stackexchange.com/questions/52015/what-is-the-difference-between-semantic-segmentation-object-detection-and-insta</li>
<li>What Is Instance Segmentation? | IBM, https://www.ibm.com/think/topics/instance-segmentation</li>
<li>Instance Segmentation - Ultralytics YOLO Docs, https://docs.ultralytics.com/tasks/segment/</li>
<li>Semantic Segmentation vs Object Detection: Understanding the Differences - Keymakr, https://keymakr.com/blog/semantic-segmentation-vs-object-detection-understanding-the-differences/</li>
<li>Instance vs Semantic Segmentation: Understanding the Difference - Keylabs, https://keylabs.ai/blog/instance-vs-semantic-segmentation-understanding-the-difference/</li>
<li>What is Instance Segmentation? A Guide. [2025] - Roboflow Blog, https://blog.roboflow.com/instance-segmentation/</li>
<li>Mask R-CNN, https://arxiv.org/pdf/1703.06870</li>
<li>Mask R-CNN - CVF Open Access, https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf</li>
<li>Ultimate Guide to Mask R-CNN: Architecture and Applications - Ikomia, https://www.ikomia.ai/blog/ultimate-guide-mask-rcnn-object-detection-segmentation</li>
<li>[1703.06870] Mask R-CNN - arXiv, https://arxiv.org/abs/1703.06870</li>
<li>Mask R-CNN: An Overview - by James Fahey - Medium, https://medium.com/@fahey_james/mask-r-cnn-an-overview-ca682955a1a1</li>
<li>R-CNN, Fast R-CNN, Faster R-CNN, and Mask R-CNN - Medium, https://medium.com/towardsdev/r-cnn-fast-r-cnn-faster-r-cnn-and-mask-r-cnn-e7cd2e6f0a82</li>
<li>Getting Started with R-CNN, Fast R-CNN, and Faster R-CNN - MATLAB &amp; Simulink - MathWorks, https://www.mathworks.com/help/vision/ug/getting-started-with-r-cnn-fast-r-cnn-and-faster-r-cnn.html</li>
<li>Mask R-CNN | ML - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/mask-r-cnn-ml/</li>
<li>arXiv:1504.08083v2 [cs.CV] 27 Sep 2015, https://arxiv.org/pdf/1504.08083</li>
<li>Explore Mask R-CNN for Superior Image Segmentation - Viso Suite, https://viso.ai/deep-learning/mask-r-cnn/</li>
<li>[1504.08083] Fast R-CNN - arXiv, https://arxiv.org/abs/1504.08083</li>
<li>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks - NIPS, https://proceedings.neurips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf</li>
<li>Faster R-CNN: Towards Real-Time Object Detection with … - arXiv, https://arxiv.org/pdf/1506.01497</li>
<li>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks - arXiv, https://arxiv.org/abs/1506.01497</li>
<li>Mask R-CNN Explained: Guide, Uses &amp; YOLO | Ultralytics, https://www.ultralytics.com/blog/what-is-mask-r-cnn-and-how-does-it-work</li>
<li>matterport/Mask_RCNN: Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow - GitHub, https://github.com/matterport/Mask_RCNN</li>
<li>What is Mask R-CNN? The Ultimate Guide. - Roboflow Blog, https://blog.roboflow.com/mask-rcnn/</li>
<li>Image Segmentation with Mask R-CNN, GrabCut, and OpenCV - GeeksforGeeks, https://www.geeksforgeeks.org/computer-vision/image-segmentation-with-mask-r-cnn-grabcut-and-opencv/</li>
<li>Mask-Refined R-CNN: A Network for Refining Object Details in Instance Segmentation, https://www.mdpi.com/1424-8220/20/4/1010</li>
<li>[1703.06870] Mask R-CNN - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/1703.06870</li>
<li>Region of Interest Pooling and Region of Interest Align explained …, https://deepsense.ai/blog/region-of-interest-pooling-explained/</li>
<li>Understanding Region of Interest - Part 2 (RoI Align) - Blog by Kemal Erdem, https://erdem.pl/2020/02/understanding-region-of-interest-part-2-ro-i-align/</li>
<li>ROI pooling vs. ROI align - Firiuza - Medium, https://firiuza.medium.com/roi-pooling-vs-roi-align-65293ab741db</li>
<li>arXiv:1703.06870v3 [cs.CV] 24 Jan 2018, http://arxiv.org/pdf/1703.06870</li>
<li>RoIAlign: A Deep Dive | SERP AI, https://serp.ai/posts/roialign/</li>
<li>stackoverflow.com, <a href="https://stackoverflow.com/questions/46272841/what-is-the-loss-function-of-the-mask-rcnn/46274246#:~:text=The%20multi-task%20loss%20function,as%20in%20Faster%20R-CNN.&amp;text=Because%20the%20model%20is%20trying,among%20classes%20for%20generating%20masks.">https://stackoverflow.com/questions/46272841/what-is-the-loss-function-of-the-mask-rcnn/46274246#:~:text=The%20multi%2Dtask%20loss%20function,as%20in%20Faster%20R%2DCNN.&amp;text=Because%20the%20model%20is%20trying,among%20classes%20for%20generating%20masks.</a></li>
<li>stackoverflow.com, <a href="https://stackoverflow.com/questions/46272841/what-is-the-loss-function-of-the-mask-rcnn#:~:text=The%20multi-task%20loss%20function,as%20in%20Faster%20R-CNN.&amp;text=Because%20the%20model%20is%20trying,among%20classes%20for%20generating%20masks.">https://stackoverflow.com/questions/46272841/what-is-the-loss-function-of-the-mask-rcnn#:~:text=The%20multi%2Dtask%20loss%20function,as%20in%20Faster%20R%2DCNN.&amp;text=Because%20the%20model%20is%20trying,among%20classes%20for%20generating%20masks.</a></li>
<li>Object Detection Based on Faster R-CNN Algorithm with Skip Pooling and Fusion of Contextual Information, https://pmc.ncbi.nlm.nih.gov/articles/PMC7582940/</li>
<li>What exactly are the losses in Matterport Mask-R-CNN? - Stack Overflow, https://stackoverflow.com/questions/55360262/what-exactly-are-the-losses-in-matterport-mask-r-cnn</li>
<li>deep learning - What is the loss function of the Mask RCNN? - Stack …, https://stackoverflow.com/questions/46272841/what-is-the-loss-function-of-the-mask-rcnn</li>
<li>Mask R-CNN | Pixels &amp; Predictions, https://www.sfu.ca/~kabhishe/posts/posts/summary_iccv_maskrcnn_2017/</li>
<li>deep learning - Mask R-CNN: how is the inference done? - Artificial …, https://ai.stackexchange.com/questions/34548/mask-r-cnn-how-is-the-inference-done</li>
<li>Benchmarking Object Detectors with COCO: A New Path Forward - arXiv, https://arxiv.org/html/2403.18819v1</li>
<li>Mean Average Precision (mAP) in Object Detection - LearnOpenCV, https://learnopencv.com/mean-average-precision-map-object-detection-model-evaluation-metric/</li>
<li>Mask Scoring R-CNN - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2019/papers/Huang_Mask_Scoring_R-CNN_CVPR_2019_paper.pdf</li>
<li>Pipeline of Faster R-CNN and Mask R-CNN: RPN is trained separately. In… - ResearchGate, https://www.researchgate.net/figure/Pipeline-of-Faster-R-CNN-and-Mask-R-CNN-RPN-is-trained-separately-In-addition-the-mask_fig1_358410491</li>
<li>What is Mask R-CNN? | SKY ENGINE AI, https://www.skyengine.ai/blog/what-is-mask-r-cnn</li>
<li>PointRend: Image Segmentation As Rendering - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2020/papers/Kirillov_PointRend_Image_Segmentation_As_Rendering_CVPR_2020_paper.pdf</li>
<li>Challenges and limitations of applying Faster R-CNN and Mask R-CNN to real-world scenarios | by Pias Das | Medium, https://medium.com/@aparnadaspias98/challenges-and-limitations-of-applying-faster-r-cnn-and-mask-r-cnn-to-real-world-scenarios-88474c275ead</li>
<li>(PDF) PointRend: Image Segmentation As Rendering (2020) | Alexander Kirillov - SciSpace, https://scispace.com/papers/pointrend-image-segmentation-as-rendering-nsw2r3do7l</li>
<li>Towards Fine-grained Large Object Segmentation 1st Place Solution to 3D AI Challenge 2020 - arXiv, https://arxiv.org/html/2009.04650v2</li>
<li>How Mask R-CNN Works? | ArcGIS API for Python - Esri Developer, https://developers.arcgis.com/python/latest/guide/how-maskrcnn-works/</li>
<li>[R] Best Neural Network for Instance Segmentation? : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/kwuk3g/r_best_neural_network_for_instance_segmentation/</li>
<li>PointRend Segmentation for a Densely Occluded Moving Object in a Video - ResearchGate, https://www.researchgate.net/publication/354110025_PointRend_Segmentation_for_a_Densely_Occluded_Moving_Object_in_a_Video</li>
<li>[2007.08921] Boundary-preserving Mask R-CNN - arXiv, https://arxiv.org/abs/2007.08921</li>
<li>Benchmarking the Robustness of Instance Segmentation Models - arXiv, https://arxiv.org/html/2109.01123v3</li>
<li>TensorMask: A Foundation for Dense Object Segmentation - CVF Open Access, https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_TensorMask_A_Foundation_for_Dense_Object_Segmentation_ICCV_2019_paper.pdf</li>
<li>[2107.12889] Improved-Mask R-CNN: Towards an Accurate Generic MSK MRI instance segmentation platform (Data from the Osteoarthritis Initiative) - arXiv, https://arxiv.org/abs/2107.12889</li>
<li>(PDF) Review: Mask R-CNN Models - ResearchGate, https://www.researchgate.net/publication/367191571_Review_Mask_R-CNN_Models</li>
<li>An Enhanced Mask R-CNN Approach for Pulmonary Embolism Detection and Segmentation, https://pmc.ncbi.nlm.nih.gov/articles/PMC11171979/</li>
<li>Building Extraction From Satellite Images Using Mask R-CNN With Building Boundary Regularization - CVF Open Access, https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w4/Zhao_Building_Extraction_From_CVPR_2018_paper.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>