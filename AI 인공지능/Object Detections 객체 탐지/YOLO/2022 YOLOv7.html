<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:YOLOv7 (2022)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>YOLOv7 (2022)</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">객체 탐지 (Object Detection)</a> / <a href="index.html">YOLO 객체 탐지 모델</a> / <span>YOLOv7 (2022)</span></nav>
                </div>
            </header>
            <article>
                <h1>YOLOv7 (2022)</h1>
<h2>1.  실시간 객체 탐지의 새로운 지평</h2>
<p>객체 탐지(Object Detection) 기술은 컴퓨터 비전 분야의 핵심 과제로서, 이미지나 비디오 내에서 특정 객체의 위치와 종류를 식별하는 것을 목표로 한다. 초기 객체 탐지 알고리즘, 특히 R-CNN 계열로 대표되는 2-stage detector는 먼저 객체가 존재할 만한 후보 영역(region proposal)을 생성하고, 이후 각 영역에 대해 분류(classification)를 수행하는 다단계 파이프라인 구조를 가졌다.1 이러한 접근 방식은 높은 정확도를 달성했지만, 복잡한 과정으로 인해 실시간 처리가 거의 불가능했다.3</p>
<p>이러한 한계를 극복하기 위해 2016년 Joseph Redmon이 발표한 YOLO(You Only Look Once)는 객체 탐지 분야에 혁명적인 변화를 가져왔다. YOLO는 객체 탐지 문제를 이미지 전체를 단 한 번만 보고 경계 상자(bounding box)와 클래스 확률(class probability)을 직접 예측하는 단일 회귀 문제(single regression problem)로 재정의했다.5 이 1-stage detector 방식은 2-stage 방식의 복잡성을 제거하여 압도적으로 빠른 처리 속도를 구현하면서도 높은 정확도를 유지했고, 이는 자율주행, 지능형 감시, 로보틱스 등 실시간성이 필수적인 응용 분야의 폭발적인 성장을 견인했다.4</p>
<p>YOLO의 등장은 하나의 거대한 기술적 계보를 형성했다. 초기 YOLOv1부터 v3까지는 원저자인 Joseph Redmon이 Darknet 프레임워크를 기반으로 개발을 주도하며 속도와 정확도를 꾸준히 개선했다.1 이후 YOLOv4부터는 Alexey Bochkovskiy 등 새로운 연구진이 계보를 이어받았으며, YOLOv5, YOLOv6 등 다양한 연구 그룹에 의해 독립적으로 발전하며 기술적 다각화가 이루어졌다.9 2022년 7월, YOLOv4의 저자들이 다시 발표한 YOLOv7은 이러한 기술적 흐름 속에서 중요한 분기점을 제시했다.10</p>
<p>YOLOv7의 핵심 주장은 명확하고 강력하다. 5 FPS에서 160 FPS에 이르는 광범위한 속도 구간에서 기존에 알려진 모든 객체 탐지기를 속도와 정확도 측면에서 모두 능가했으며, 특히 NVIDIA V100 GPU 환경에서 30 FPS 이상의 실시간 탐지가 가능한 모델 중 56.8% AP(Average Precision)라는 가장 높은 정확도를 달성했다고 발표했다.12 더욱 주목할 점은 이 성과가 외부 데이터셋이나 사전 훈련된 가중치(pre-trained weights) 없이 오직 MS COCO 데이터셋만으로 처음부터(from scratch) 훈련하여 이루어졌다는 것이다.14 이는 모델의 학습 능력과 아키텍처의 효율성이 매우 뛰어남을 방증한다.</p>
<p>그러나 YOLOv7의 진정한 의의는 단순히 성능 수치를 경신한 것에 그치지 않는다. 그것은 객체 탐지 모델의 성능을 향상시키는 연구의 무게 중심을 ’아키텍처 최적화’에서 ’훈련 과정 최적화’로 이동시키는 패러다임의 전환을 제안했다는 데 있다.12 이전까지의 연구는 대부분 ResNet, CSPNet과 같은 새로운 네트워크 블록을 설계하거나 FPN, PANet과 같은 특징 융합 구조를 개선하는 등 모델의 ’설계도’를 바꾸는 데 집중했다.8 YOLOv4에서 추론 시간에는 영향을 주지 않으면서 훈련 기법만으로 성능을 높이는 ‘Bag of Freebies’(BoF) 개념이 소개되었으나 10, YOLOv7은 이를 ’Trainable Bag-of-Freebies’라는 개념으로 한 단계 심화시켰다.15 이는 훈련 과정 자체에 최적화된 모듈이나 전략을 도입하여, 훈련 중에는 추가적인 비용과 복잡성을 감수하더라도 추론 시에는 이를 완전히 제거하거나 통합함으로써 최종 모델의 효율성은 전혀 손상시키지 않고 정확도만 극대화하는 적극적인 최적화 철학이다.18</p>
<p>이러한 접근 방식의 전환은 실시간 객체 탐지 연구가 하드웨어적 설계(아키텍처)의 점진적 개선만으로는 한계에 부딪힐 수 있음을 인지하고, 소프트웨어적 혁신(훈련 방법론)을 통해 그 한계를 돌파하려는 거대한 흐름을 상징한다. 즉, ’더 나은 모델을 만드는 법’에서 ’모델을 더 잘 훈련시키는 법’으로 연구의 초점이 이동하고 있음을 시사하는 것이다. 본 안내서는 YOLOv7이 제시한 이러한 새로운 패러다임을 중심으로, 그 기술적 토대가 되는 아키텍처의 혁신, 훈련 최적화 기법의 정수, 그리고 정량적 성능 분석과 실제적 고찰을 통해 YOLOv7이 실시간 객체 탐지 분야에 남긴 유산과 미래적 가치를 심층적으로 고찰하고자 한다.</p>
<h2>2.  YOLOv7의 기술적 토대: 아키텍처 심층 분석</h2>
<p>YOLOv7의 압도적인 성능은 단순히 훈련 기법의 혁신만으로 이루어진 것이 아니다. 그 기저에는 깊은 네트워크를 안정적으로 학습시키고, 다양한 크기로 효율적으로 확장할 수 있도록 설계된 정교한 아키텍처가 자리 잡고 있다. 본 장에서는 YOLOv7의 구조적 근간을 이루는 두 가지 핵심 기술, E-ELAN(Extended Efficient Layer Aggregation Network)과 연결 기반 모델을 위한 복합 스케일링(Compound Scaling)에 대해 심층적으로 분석한다.</p>
<h3>2.1  E-ELAN (Extended Efficient Layer Aggregation Network)</h3>
<p>딥러닝 네트워크가 깊어질수록 더 풍부한 특징을 학습할 수 있지만, 동시에 그래디언트 소실(vanishing gradient) 또는 폭주(exploding gradient) 문제로 인해 학습이 불안정해지는 고질적인 문제가 발생한다.20 YOLOv7은 이 문제를 해결하고 깊은 네트워크의 학습 효율을 극대화하기 위해 백본(Backbone)의 핵심 계산 블록으로 E-ELAN을 채택했다.3</p>
<p>ELAN(Efficient Layer Aggregation Network)의 기본 설계 철학은 네트워크 내에서 그래디언트가 전파되는 가장 짧은 경로와 가장 긴 경로를 효과적으로 제어하여, 네트워크의 깊이가 증가하더라도 안정적인 학습이 가능하도록 하는 것이다.18 이는 CSPNet이나 VoVNet과 같은 이전 연구에서 영감을 받은 것으로, 효율적인 그래디언트 경로를 보장함으로써 각 레이어가 더 다양한 특징을 학습하도록 유도한다.15</p>
<p>YOLOv7은 이 ELAN 구조를 한 단계 발전시킨 E-ELAN을 제안했다.11 E-ELAN의 핵심 목표는 기존 ELAN이 가진 안정적인 그래디언트 전파 경로라는 장점은 그대로 유지하면서, 네트워크의 학습 능력 자체를 더욱 강화하는 것이다. 이를 위해 ’expand, shuffle, merge cardinality’라는 독창적인 메커니즘을 도입했다.24</p>
<ul>
<li><strong>Expand (확장)</strong>: 계산 블록 내에서 그룹 컨볼루션(group convolution)을 사용하여 특징 맵의 채널(channel)과 카디널리티(cardinality, 독립적인 특징 그룹의 수)를 확장한다. 이는 더 적은 파라미터로 더 풍부한 특징 표현을 가능하게 한다.22</li>
<li><strong>Shuffle (셔플)</strong>: 확장된 카디널리티 그룹에서 나온 특징 맵들을 서로 섞어준다. 이 과정을 통해 각 특징 그룹 간의 정보 교류가 일어나며, 특정 그룹이 특정 특징에만 과도하게 전문화되는 것을 방지한다.11</li>
<li><strong>Merge Cardinality (카디널리티 병합)</strong>: 셔플된 특징 맵 그룹들을 최종적으로 연결(concatenate)하고 병합한다. 이 구조를 통해 네트워크는 다양한 그룹의 계산 블록들이 학습한 다채로운 특징들을 종합하여 최종적인 특징 표현을 형성하게 된다.22</li>
</ul>
<p>이러한 E-ELAN의 설계는 ’학습 다양성’과 ’구조적 안정성’이라는 두 마리 토끼를 동시에 잡으려는 정교한 전략으로 해석될 수 있다. 딥러닝 모델의 성능을 높이기 위해서는 각 레이어가 최대한 다양하고 풍부한 특징을 학습해야 하지만(학습 다양성), 네트워크가 깊어질수록 학습 과정 자체가 불안정해지는 문제가 발생한다(구조적 안정성). E-ELAN은 카디널리티를 확장하고 셔플하는 과정을 통해 각 특징 그룹이 독립적이면서도 다채로운 정보를 학습하도록 강제하여 ’학습 다양성’을 극대화한다. 동시에, 이 모든 과정은 기존 ELAN의 안정적인 그래디언트 흐름을 보장하는 큰 틀 안에서 이루어지므로 ’구조적 안정성’을 해치지 않는다. 이는 단순히 레이어를 깊게 쌓는 것을 넘어, 안정적인 학습을 보장하는 구조적 제약 하에서 특징 표현의 잠재력을 최대한 끌어내는 고도의 설계 철학을 보여준다. 비록 논문에 제시된 E-ELAN 다이어그램과 실제 코드 구현(yolov7-e6e.yaml)이 외형적으로 다소 차이를 보일 수 있으나, 이는 기능적으로 동일한 연산을 수행하는 등가(equivalence) 아키텍처로 이해해야 한다.26</p>
<h3>2.2  연결 기반 모델을 위한 복합 스케일링 (Compound Scaling)</h3>
<p>객체 탐지 모델은 다양한 하드웨어 환경과 요구 성능에 맞춰 여러 크기의 버전(e.g., tiny, small, large)으로 제공되는 것이 일반적이다. 이를 위해 모델 스케일링(model scaling) 기법이 사용되는데, 이는 주로 네트워크의 깊이(depth), 너비(width), 입력 이미지 해상도(resolution)를 조정하는 방식으로 이루어진다.15</p>
<p>하지만 EfficientNet 등에서 사용된 기존의 스케일링 방식은 모든 아키텍처에 효과적이지 않다. 특히 DenseNet이나 VoVNet과 같이 이전 레이어의 출력을 다음 레이어의 입력에 연결(concatenation)하는 구조를 가진 모델에서는 심각한 문제가 발생한다.15 이러한 연결 기반 아키텍처에서 깊이(레이어 수)만 단독으로 늘리면, 특정 계산 블록의 출력 채널 수가 변하게 된다. 이는 곧바로 다음 전이 레이어(transition layer)의 입력 채널 수에 영향을 미쳐, 초기 설계 시 의도했던 최적의 정보 흐름과 채널 비율이 깨지게 된다.11 이는 마치 정교하게 설계된 건물의 한 기둥만 임의로 두껍게 만들어 전체 구조의 균형을 무너뜨리는 것과 같다.</p>
<p>YOLOv7은 이 문제를 해결하기 위해 ’복합 스케일링(compound scaling)’이라는 새로운 방법을 제안했다.11 이 방법의 핵심은 깊이와 너비를 독립적으로 보지 않고, 서로 연동하여 스케일링하는 것이다. 구체적인 과정은 다음과 같다.</p>
<ol>
<li>연결 기반 계산 블록의 <strong>깊이</strong>를 원하는 스케일링 계수만큼 조정한다.</li>
<li>깊이 조정으로 인해 해당 블록의 <strong>출력 채널 수</strong>가 얼마나 변했는지 계산한다.</li>
<li>계산된 채널 변화량과 동일한 비율로, 해당 블록에 이어지는 전이 레이어의 **너비(채널 수)**를 함께 조정한다.11</li>
</ol>
<p>이러한 복합 스케일링 접근법을 통해, 모델의 크기를 조절하더라도 초기 설계 시 가졌던 아키텍처의 고유한 속성과 최적의 구조를 그대로 보존할 수 있다.20 YOLOv7-tiny, YOLOv7, YOLOv7-X, YOLOv7-W6, E6, D6 등 다양한 버전의 모델들은 모두 이 복합 스케일링 기법을 통해 효율적으로 생성되었다.12</p>
<p>이러한 접근은 단순한 크기 조절을 넘어, 아키텍처의 ’위상적 무결성(Topological Integrity)’을 유지하려는 시도로 볼 수 있다. ‘연결’ 기반 모델의 핵심은 레이어 간의 강한 의존성과 정교하게 설계된 정보 흐름 비율에 있다. 복합 스케일링은 한 부분의 변경이 다른 부분에 미치는 영향을 체계적으로 고려하여 연동함으로써, 모델의 근본적인 설계 원칙과 효율성을 모든 스케일에서 일관되게 보존한다. 이는 향후 복잡한 연결 구조를 가진 차세대 딥러닝 모델들의 스케일링 방식에 중요한 표준을 제시한 것으로 평가할 수 있다.</p>
<h2>3.  훈련 최적화의 정수: Trainable Bag-of-Freebies</h2>
<p>YOLOv7이 기존 객체 탐지 모델들과 가장 차별화되는 지점은 바로 ’Trainable Bag-of-Freebies’라는 개념을 통해 훈련 과정을 혁신적으로 최적화했다는 점이다. 이는 추론(inference) 단계에서는 추가적인 연산 비용을 전혀 발생시키지 않으면서도, 훈련(training) 단계에서만 일시적으로 모델의 복잡성을 높여 최종 정확도를 극대화하는 기법들의 집합을 의미한다.12 본 장에서는 이 개념을 구현하는 두 가지 핵심 기술, ’계획된 재매개변수화 컨볼루션’과 ’심층 지도 기반 Coarse-to-Fine 레이블 할당’의 원리를 심층적으로 분석한다.</p>
<h3>3.1  계획된 재매개변수화 컨볼루션 (Planned Re-parameterized Convolution)</h3>
<p>모델 재매개변수화(Model Re-parameterization)는 훈련 시에는 풍부한 특징 학습을 위해 다중 브랜치(multi-branch) 구조와 같이 복잡한 모듈을 사용하고, 훈련이 완료된 후 추론 시점에는 이를 수학적으로 등가인 단일 연산 모듈로 통합(merge)하는 기법이다.17 이는 일종의 모듈 수준 앙상블(module-level ensemble) 효과를 통해, 추론 속도의 손실 없이 모델의 표현력을 강화하는 매우 효율적인 최적화 전략이다.15</p>
<p>이 분야의 대표적인 연구인 RepVGG는 훈련 시에 3x3 컨볼루션, 1x1 컨볼루션, 그리고 항등 연결(identity connection)의 세 브랜치를 사용하고, 추론 시에는 이를 단일 3x3 컨볼루션으로 병합하는 RepConv 모듈을 제안했다.11 그러나 이 RepConv 모듈을 ResNet의 잔여 연결(residual connection)이나 DenseNet의 연결(concatenation) 구조를 가진 네트워크에 직접 적용할 경우, 오히려 성능이 크게 저하되는 심각한 문제가 발견되었다.12</p>
<p>YOLOv7 연구팀은 이 문제의 근본 원인이 RepConv의 ‘항등 연결’ 브랜치가 ResNet이나 DenseNet이 가진 고유의 그래디언트 흐름 다양성을 파괴하기 때문임을 그래디언트 흐름 경로 분석(gradient flow propagation path analysis)을 통해 규명했다.12 잔여 연결과 연결 구조는 서로 다른 레이어로부터 온 특징들을 보존하고 조합함으로써 다양한 그래디언트 경로를 만드는데, 항등 연결이 이 과정에 개입하여 정보의 병목 현상을 유발하고 학습을 방해한다는 것이다.</p>
<p>이러한 분석을 바탕으로 YOLOv7은 ’계획된 재매개변수화(Planned Re-parameterization)’라는 새로운 접근법을 제시했다. 이는 모든 곳에 동일한 재매개변수화 기법을 적용하는 대신, 네트워크 아키텍처의 ’맥락’을 고려하여 최적의 전략을 ‘계획적으로’ 선택하는 방식이다.20 구체적으로, 잔여 연결이나 연결 구조와 함께 사용되는 컨볼루션 레이어에는 항등 연결이 없는 RepConvN(RepConv without identity connection)을 적용하고, 일반적인 PlainNet 구조의 레이어에는 기존의 RepConv를 적용하는 등, 각 레이어의 위치와 역할에 따라 재매개변수화 방식을 다르게 설계했다.11</p>
<p>이러한 ‘맥락-인지적(Context-Aware)’ 최적화는 매우 중요한 시사점을 가진다. 이는 특정 최적화 기법이 ’만능 열쇠’가 될 수 없으며, 그 효과는 전체 시스템, 즉 네트워크 아키텍처와의 상호작용 속에서 결정된다는 것을 보여준다. YOLOv7은 Conv-BN 통합, YOLOR의 암묵적 지식(implicit knowledge) 재해석 등 다양한 부분에 이러한 계획된 재매개변수화 철학을 일관되게 적용함으로써 15, 모델 최적화 연구가 더욱 정교하고 시스템 수준의 접근을 취해야 함을 증명했다.</p>
<h3>3.2  심층 지도와 Coarse-to-Fine 레이블 할당</h3>
<p>심층 지도(Deep Supervision)는 깊은 신경망의 중간 레이어들에 보조적인 출력 헤드(Auxiliary Head)를 추가하여 추가적인 손실(loss)을 계산함으로써, 네트워크 전체에 걸쳐 그래디언트가 잘 전파되도록 돕는 고전적인 훈련 기법이다.21 YOLOv7은 이 기법을 현대적인 객체 탐지기의 훈련 방식과 결합하여 한 단계 발전시켰다. YOLOv7에서는 최종 출력을 담당하는 주된 헤드를 <strong>리드 헤드(Lead Head)</strong>, 그리고 훈련 과정에서 중간 레이어의 학습을 돕는 헤드를 **보조 헤드(Auxiliary Head)**로 명명했다.18</p>
<p>그러나 여기에 최신 객체 탐지기에서 널리 사용되는 동적 레이블 할당(Dynamic Label Assignment) 기법을 적용하면서 새로운 문제가 발생했다. 동적 레이블 할당은 모델의 예측 결과를 고려하여 각 객체에 가장 적합한 앵커 박스(anchor box)를 동적으로 할당하는 방식인데, 리드 헤드와 보조 헤드가 있는 다중 헤드 구조에서 각 헤드에 어떻게 일관성 있는 레이블을 할당할 것인가라는 난제가 생긴 것이다.17 기존 방식처럼 각 헤드가 독립적으로 레이블을 할당받을 경우, 두 헤드가 서로 다른 최적화 목표를 향해 학습하게 되어 전체적인 수렴을 방해할 수 있다.34</p>
<p>YOLOv7은 이 문제를 해결하기 위해 ’리드 헤드 가이드 레이블 할당(Lead Head Guided Label Assigner)’이라는 독창적인 방법을 제안했다. 이 방법의 핵심 아이디어는, 일반적으로 더 깊은 곳에 위치하여 학습 능력이 더 강한 리드 헤드의 예측을 일종의 ‘가이드’ 또는 ’교사’로 활용하는 것이다.11 구체적인 과정은 다음과 같다.</p>
<ol>
<li>리드 헤드의 예측 결과와 실제 정답(Ground Truth)을 함께 최적화하여 가장 이상적인 할당 결과인 ’소프트 레이블(soft label)’을 생성한다.</li>
<li>이렇게 생성된 <strong>동일한 소프트 레이블</strong>을 리드 헤드와 보조 헤드 <strong>모두</strong>의 학습 목표로 사용한다.21</li>
</ol>
<p>이 방식을 통해, 상대적으로 학습 능력이 약한 보조 헤드는 이미 리드 헤드가 학습한 정제된 정보를 직접 전달받아 효율적으로 학습할 수 있다. 그 결과, 리드 헤드는 이미 학습된 정보에 대한 부담을 덜고, 아직 학습하지 못한 더 어려운 잔여 정보(residual information)에 집중할 수 있게 되어 전체 네트워크의 학습 효율이 극대화된다.21</p>
<p>나아가 YOLOv7은 여기서 한 걸음 더 나아가 ‘Coarse-to-Fine(개략적인 것에서 세밀한 것으로)’ 전략을 도입했다.24 이는 리드 헤드와 보조 헤드의 학습 능력 차이를 고려하여 서로 다른 난이도의 학습 목표를 부여하는 방식이다.</p>
<ul>
<li><strong>Fine Label (세밀한 레이블)</strong>: 리드 헤드에는 위에서 설명한 방식대로 생성된 정교하고 엄격한 소프트 레이블을 할당한다.</li>
<li><strong>Coarse Label (개략적인 레이블)</strong>: 보조 헤드에는 positive sample로 간주되는 그리드의 조건을 완화하여 더 많은 후보를 정답으로 취급하는, 즉 더 느슨하고 쉬운 레이블을 할당한다.11</li>
</ul>
<p>이 Coarse-to-Fine 전략은 두 가지 중요한 학습 원리를 내포하고 있다. 첫째, 리드 헤드(교사)의 지식을 보조 헤드(학생)에게 전달하는 것은 ’자기 지식 증류(Self-Knowledge Distillation)’의 형태를 띤다. 둘째, 각 헤드의 학습 능력에 맞춰 쉬운 문제(Coarse)와 어려운 문제(Fine)를 차등적으로 부여하는 것은 ’커리큘럼 학습(Curriculum Learning)’의 철학과 일치한다. 이처럼 YOLOv7은 기존의 심층 지도 기법을 동적 레이블 할당 프레임워크에 창의적으로 융합함으로써, 복잡한 다중 출력 모델의 훈련을 안정화하고 가속화하는 매우 효과적인 방법론을 제시했다.</p>
<h2>4.  성능 벤치마크 정량 분석</h2>
<p>YOLOv7의 기술적 혁신은 결국 MS COCO와 같은 표준 벤치마크 데이터셋에서의 정량적 성능으로 입증된다. 본 장에서는 YOLOv7의 공식 논문과 관련 자료에서 제공하는 실험 결과를 바탕으로, 주요 경쟁 모델들과의 성능을 비교 분석하고, YOLOv7 내부 버전 간의 특성을 파악하여 속도와 정확도 사이의 트레이드오프 관계를 명확히 제시한다. 모든 실험 결과는 별도의 언급이 없는 한 MS COCO 데이터셋과 NVIDIA V100 GPU 환경을 기준으로 한다.12</p>
<p>아래 표는 주요 실시간 객체 탐지기들의 성능을 종합적으로 비교한 것이다. 이 표는 YOLOv7의 핵심 주장인 ’최고의 속도-정확도 균형’을 직접적으로 증명하는 가장 중요한 근거 자료이다. 이를 통해 특정 속도 구간에서 YOLOv7이 가장 효율적인 선택지임을 한눈에 파악할 수 있다.</p>
<table><thead><tr><th>모델</th><th>파라미터 (M)</th><th>FLOPs (G)</th><th>입력 크기 (pixels)</th><th>FPS (V100)</th><th>APtest / APval 50-95</th><th>AP50test</th><th>AP75test</th></tr></thead><tbody>
<tr><td>YOLOv5-N (r6.1)</td><td>1.9</td><td>4.5</td><td>640</td><td>159</td><td>- / 28.0%</td><td>-</td><td>-</td></tr>
<tr><td>YOLOv5-S (r6.1)</td><td>7.2</td><td>16.5</td><td>640</td><td>156</td><td>- / 37.4%</td><td>-</td><td>-</td></tr>
<tr><td>YOLOv5-L (r6.1)</td><td>46.5</td><td>109.1</td><td>640</td><td>99</td><td>- / 49.0%</td><td>-</td><td>-</td></tr>
<tr><td>YOLOv5-X (r6.1)</td><td>86.7</td><td>205.7</td><td>640</td><td>83</td><td>- / 50.7%</td><td>-</td><td>-</td></tr>
<tr><td>YOLOR-CSP</td><td>52.9</td><td>120.4</td><td>640</td><td>106</td><td>51.1% / 50.8%</td><td>69.6%</td><td>55.7%</td></tr>
<tr><td>YOLOX-L</td><td>54.2</td><td>155.6</td><td>640</td><td>69</td><td>50.1% / 49.7%</td><td>-</td><td>-</td></tr>
<tr><td>PPYOLOE-L</td><td>52.2</td><td>110.1</td><td>640</td><td>78</td><td>51.4% / 50.9%</td><td>68.9%</td><td>55.6%</td></tr>
<tr><td><strong>YOLOv7-tiny-SiLU</strong></td><td><strong>6.2</strong></td><td><strong>13.8</strong></td><td><strong>640</strong></td><td><strong>286</strong></td><td><strong>38.7% / 38.7%</strong></td><td><strong>56.7%</strong></td><td><strong>41.7%</strong></td></tr>
<tr><td><strong>YOLOv7</strong></td><td><strong>36.9</strong></td><td><strong>104.7</strong></td><td><strong>640</strong></td><td><strong>161</strong></td><td><strong>51.4% / 51.2%</strong></td><td><strong>69.7%</strong></td><td><strong>55.9%</strong></td></tr>
<tr><td><strong>YOLOv7-X</strong></td><td><strong>71.3</strong></td><td><strong>189.9</strong></td><td><strong>640</strong></td><td><strong>114</strong></td><td><strong>53.1% / 52.9%</strong></td><td><strong>71.2%</strong></td><td><strong>57.8%</strong></td></tr>
<tr><td><strong>YOLOv7-W6</strong></td><td><strong>70.4</strong></td><td><strong>360.0</strong></td><td><strong>1280</strong></td><td><strong>84</strong></td><td><strong>54.9% / 54.6%</strong></td><td><strong>72.6%</strong></td><td><strong>60.1%</strong></td></tr>
<tr><td><strong>YOLOv7-E6</strong></td><td><strong>97.2</strong></td><td><strong>515.2</strong></td><td><strong>1280</strong></td><td><strong>56</strong></td><td><strong>56.0% / 55.9%</strong></td><td><strong>73.5%</strong></td><td><strong>61.2%</strong></td></tr>
<tr><td><strong>YOLOv7-D6</strong></td><td><strong>154.7</strong></td><td><strong>806.8</strong></td><td><strong>1280</strong></td><td><strong>44</strong></td><td><strong>56.6% / 56.3%</strong></td><td><strong>74.0%</strong></td><td><strong>61.8%</strong></td></tr>
<tr><td><strong>YOLOv7-E6E</strong></td><td><strong>151.7</strong></td><td><strong>843.2</strong></td><td><strong>1280</strong></td><td><strong>36</strong></td><td><strong>56.8% / 56.8%</strong></td><td><strong>74.4%</strong></td><td><strong>62.1%</strong></td></tr>
</tbody></table>
<p>표 데이터 출처: 13</p>
<h3>4.1  주요 모델과의 성능 비교 분석</h3>
<p>표의 데이터를 통해 YOLOv7이 주요 경쟁 모델 대비 어떤 우위를 보이는지 명확하게 확인할 수 있다.</p>
<ul>
<li><strong>vs. YOLOv5</strong>: YOLOv7은 모든 체급에서 YOLOv5를 압도하는 성능을 보여준다. 가장 작은 모델인 YOLOv7-tiny-SiLU는 YOLOv5-N보다 127 FPS나 더 빠르면서도 AP는 10.7%p 더 높다.13 대형 모델인 YOLOv7-X는 YOLOv5-X와 유사한 파라미터 수를 가지지만, 추론 속도는 31 FPS 더 빠르고(114 vs 83 FPS), 파라미터는 22%, 연산량(FLOPs)은 8% 더 적음에도 불구하고 AP는 2.2%p 더 높다.13 이는 YOLOv7 아키텍처와 훈련 방식이 근본적으로 더 효율적임을 의미한다.</li>
<li><strong>vs. YOLOR</strong>: YOLOv7의 기본 모델은 YOLOR-CSP와 비교했을 때, 파라미터 수를 43%, 연산량을 15%나 줄이면서도 0.4%p 더 높은 AP를 달성했다.11 이는 더 적은 자원으로 더 나은 성능을 내는 YOLOv7의 효율성을 잘 보여주는 사례이다.</li>
<li><strong>vs. PPYOLOE</strong>: YOLOv7 기본 모델은 PPYOLOE-L과 동일한 51.4%의 AP를 기록했지만, 추론 속도는 161 FPS로 PPYOLOE-L의 78 FPS보다 두 배 이상 빠르다. 동시에 파라미터 수는 41%나 더 적다.11 이는 동일한 정확도를 달성하는 데 필요한 연산 비용이 훨씬 적다는 것을 의미한다.</li>
<li><strong>vs. Transformer/ConvNeXt 기반 모델</strong>: YOLOv7-E6(56.0% AP, 56 FPS)는 당시 SOTA였던 Transformer 기반의 SWIN-L Cascade-Mask R-CNN(53.9% AP, 9.2 FPS)이나 ConvNeXt 기반의 ConvNeXt-XL Cascade-Mask R-CNN(55.2% AP, 8.6 FPS)과 비교했을 때, 정확도는 더 높으면서 속도는 5배 이상 빠르다.12 이는 복잡한 어텐션 메커니즘이나 대규모 아키텍처에 의존하지 않고도, 잘 설계된 CNN 기반 1-stage detector가 여전히 실시간 객체 탐지 분야에서 최고의 자리를 차지할 수 있음을 강력하게 시사한다.</li>
</ul>
<h3>4.2  YOLOv7 버전별 속도-정확도 트레이드오프</h3>
<p>YOLOv7은 복합 스케일링 기법을 통해 다양한 하드웨어와 요구사항에 맞는 모델 라인업을 구축했다. 각 버전은 속도와 정확도 사이에서 명확한 트레이드오프 관계를 보인다.</p>
<ul>
<li><strong>목적별 설계</strong>: YOLOv7은 특정 컴퓨팅 환경을 목표로 설계되었다. YOLOv7-tiny는 리소스가 제한된 엣지 GPU, YOLOv7 기본 모델은 일반적인 소비자용 GPU, 그리고 YOLOv7-W6 이상은 고성능 클라우드 GPU 환경에 최적화되었다.12</li>
<li><strong>스케일링에 따른 성능 변화</strong>:</li>
<li><strong>YOLOv7-tiny-SiLU</strong>: 6.2M의 매우 적은 파라미터로 286 FPS라는 압도적인 속도를 자랑하지만, AP는 38.7%로 가장 낮다. 이는 속도가 정확도보다 훨씬 중요한 경량화 애플리케이션이나 임베디드 시스템에 가장 적합한 선택이다.13</li>
<li><strong>YOLOv7 (base)</strong>: 36.9M 파라미터로 161 FPS의 빠른 속도와 51.4% AP라는 높은 정확도를 달성하여, 속도와 정확도의 가장 이상적인 균형을 보여주는 모델이다. 대부분의 범용 실시간 탐지 작업에 표준으로 사용될 수 있다.13</li>
<li><strong>YOLOv7-X</strong>: 71.3M 파라미터로 크기가 커졌지만, 53.1% AP라는 더 높은 정확도를 제공하면서도 114 FPS라는 빠른 속도를 유지한다. 기본 모델보다 더 높은 정밀도가 요구되는 시나리오에 적합하다.13</li>
<li><strong>YOLOv7-W6, E6, D6, E6E</strong>: 이 모델들은 1280x1280의 고해상도 입력을 사용하여 54.9%에서 최고 56.8%에 이르는 SOTA급 정확도를 달성한다. 하지만 파라미터 수가 70M에서 155M까지 크게 증가하고, FLOPs 또한 급증하여 추론 속도는 36~84 FPS 수준으로 감소한다. 이는 최고의 정확도가 필수적이며, 강력한 클라우드 GPU 자원을 활용할 수 있는 연구나 특정 고성능 애플리케이션을 위한 모델들이다.12</li>
</ul>
<p>이러한 벤치마크 분석을 통해 드러나는 YOLOv7의 진정한 강점은 단순히 절대적인 최고 성능 수치가 아니라, 주어진 컴퓨팅 자원(파라미터, FLOPs) 내에서 최고의 성능을 이끌어내는 ’효율성’에 있다. 성능 비교표의 데이터를 속도-정확도 그래프로 시각화하면, YOLOv7 계열 모델들이 다른 경쟁 모델들보다 우위에 있는 파레토 최적 경계(Pareto frontier)를 형성하는 것을 볼 수 있다. 이는 주어진 속도에서는 최고의 정확도를, 주어진 정확도에서는 최고의 속도를 제공함을 의미한다. 이러한 ’자원 효율성’은 E-ELAN, 복합 스케일링, Trainable Bag-of-Freebies 등 앞서 분석한 기술적 혁신들이 유기적으로 결합하여 만들어낸 결과물이며, 컴퓨팅 예산의 제약이 큰 실제 산업 환경에서 YOLOv7이 가지는 강력한 경쟁력의 원천이다.</p>
<h2>5.  실제적 고찰: 응용 분야와 기술적 한계</h2>
<p>YOLOv7의 기술적 우수성과 뛰어난 성능 벤치마크는 실제 산업 현장에서의 광범위한 활용 가능성을 시사한다. 그러나 동시에, 모든 기술이 그러하듯 YOLOv7 역시 특정 조건에서는 한계를 드러낸다. 본 장에서는 YOLOv7이 자율주행, 보안, 로보틱스 분야에서 어떻게 구체적으로 활용되는지 살펴보고, 현재 기술 수준에서 직면한 난제, 특히 소형 객체 및 밀집 환경에서의 탐지 문제와 이를 극복하기 위한 연구 동향을 조망한다.</p>
<h3>5.1  산업적 활용 (자율주행, 보안, 로보틱스)</h3>
<p>자율주행, 로보틱스, 지능형 감시 시스템과 같은 현대 산업의 핵심 분야들은 공통적으로 동적인 환경에서 신속하고 정확한 상황 인식을 요구한다. YOLOv7의 빠른 추론 속도와 높은 정확도는 이러한 실시간 객체 탐지 요구사항을 충족시키는 데 이상적이다.8</p>
<ul>
<li><strong>자율주행</strong>: 자율주행 시스템의 안전은 주변 환경의 차량, 보행자, 자전거, 교통 표지판 등을 실시간으로 정확하게 인지하는 능력에 달려있다.8 YOLOv7의 빠른 추론 속도는 밀리초 단위의 판단이 요구되는 주행 상황에서 차량의 즉각적인 반응을 가능하게 한다.8 특히 안개, 비, 눈과 같은 악천후나 급격한 조명 변화 속에서도 강건한 성능을 유지하는 것이 중요한데, YOLOv7을 기반으로 한 다양한 개선 연구들이 이러한 문제에 집중하고 있다.8 실제 자율주행 벤치마크 데이터셋인 KITTI나 가상 시뮬레이션 환경인 CARLA를 이용한 연구에서 YOLOv7 기반 모델이 높은 mAP와 실시간 추론 속도를 달성하며 그 실용성을 입증했다.37</li>
<li><strong>보안 및 감시</strong>: 지능형 보안 시스템은 YOLOv7을 활용하여 다양한 시나리오에 적용될 수 있다. 공장이나 제한 구역 내 침입자를 실시간으로 탐지하거나, 건설 현장에서 허가된 차량의 움직임을 모니터링하는 데 사용될 수 있다.4 또한, 작업자의 안전모나 안전 조끼와 같은 개인 보호 장비(PPE) 착용 여부를 자동으로 확인하여 산업 재해를 예방하거나, 광활한 산림 지역에서 산불의 초기 징후인 연기를 조기에 감지하는 시스템에도 활용될 수 있다.4 더 나아가 칼과 같은 위험물을 소지한 사람을 식별하거나 1, 노약자의 낙상을 감지하고, 특정 공간 내 사람들의 행동 패턴을 분석하는 등 18 고도화된 애플리케이션으로 확장되고 있다.</li>
<li><strong>로보틱스</strong>: 로봇이 주변 환경과 효과적으로 상호작용하기 위해서는 객체를 정확히 인식하고 위치를 파악하는 능력이 필수적이다. YOLOv7, 특히 경량화 버전인 YOLOv7-tiny는 연산 능력이 제한된 엣지 디바이스나 모바일 로봇에 탑재하기에 적합하다.12 이를 통해 로봇은 실시간으로 장애물을 회피하고, 특정 물체를 집어 옮기거나, 사람과 상호작용하는 등 복잡한 작업을 수행할 수 있다.</li>
</ul>
<h3>5.2  기술적 난제와 극복 노력</h3>
<p>MS COCO와 같은 범용 데이터셋에서 SOTA 성능을 달성했음에도 불구하고, YOLOv7을 포함한 YOLO 계열 모델들은 특정 유형의 문제에서 공통적인 한계를 보인다. 가장 대표적인 난제는 바로 ‘소형 객체(small objects)’ 탐지와 ’밀집 환경(crowded/dense scenes)’에서의 성능 저하이다.40</p>
<ul>
<li><strong>문제의 원인</strong>:</li>
<li><strong>특징 정보 소실</strong>: 소형 객체는 이미지에서 차지하는 픽셀 수가 절대적으로 적기 때문에, 네트워크의 백본을 통과하며 반복되는 다운샘플링(down-sampling) 과정에서 고유의 미세한 특징 정보가 소실되기 쉽다.43</li>
<li><strong>배경과의 혼동 및 가려짐</strong>: 밀집 환경에서는 여러 객체가 서로 겹쳐 있거나(occlusion), 복잡한 배경과 뒤섞여 객체의 경계가 불분명해진다. 이로 인해 모델이 개별 객체를 정확히 분리하고 인식하는 데 어려움을 겪는다.41</li>
<li><strong>IoU 지표의 민감성</strong>: 전통적인 성능 지표인 IoU(Intersection over Union)는 소형 객체의 경우, 단 몇 픽셀의 위치 오차만으로도 그 값이 급격하게 변동한다. 이는 훈련 과정에서 손실 함수를 불안정하게 만들고, 추론 시 NMS(Non-Maximum Suppression) 과정에서 올바른 경계 상자를 억제하는 원인이 될 수 있다.44</li>
<li><strong>탐지 수량의 한계</strong>: YOLO는 이미지를 그리드 셀로 나누어 예측을 수행하므로, 하나의 셀에 여러 개의 작은 객체가 몰려 있을 경우 일부를 놓칠 수 있다. 또한, ONNX와 같은 형식으로 모델을 변환할 때 <code>topk</code>와 같은 파라미터에 의해 탐지 가능한 객체의 최대 수가 제한될 수도 있다.45</li>
</ul>
<p>이러한 한계를 극복하기 위해 YOLOv7을 기반(baseline)으로 한 수많은 후속 연구가 활발히 진행되고 있다.</p>
<ul>
<li><strong>개선 연구 동향</strong>:</li>
<li><strong>다중 스케일 탐지 강화</strong>: 기존의 3개 예측 헤드에 더해, 더 얕은 레이어의 고해상도 특징 맵을 활용하는 네 번째 예측 헤드를 추가하여 소형 객체에 대한 민감도를 높이는 연구가 다수 제안되었다.43</li>
<li><strong>어텐션 메커니즘 도입</strong>: CBAM, Coordinate Attention(CA), SimAM과 같은 어텐션 모듈을 네트워크에 통합하여, 모델이 소형 객체나 중요한 특징 영역에 더 집중하도록 유도한다.42</li>
<li><strong>새로운 유사도 지표 및 손실 함수</strong>: IoU의 단점을 보완하기 위해 경계 상자를 가우시안 분포로 모델링하는 NWD(Normalized Gaussian Wasserstein Distance)와 같은 새로운 지표를 손실 함수와 NMS에 적용하여 소형 객체 탐지 성능을 향상시킨다.44</li>
<li><strong>하이브리드 아키텍처</strong>: CNN의 지역적 특징 추출 능력과 Swin Transformer와 같은 비전 트랜스포머의 전역적 문맥 이해 능력을 결합하여 특징 표현력을 강화하려는 시도도 이루어지고 있다.47</li>
</ul>
<p>이러한 현상은 ‘No Free Lunch’ 정리를 상기시킨다. 즉, 어떠한 단일 모델도 모든 종류의 문제를 완벽하게 해결할 수는 없다. YOLOv7의 성공은 범용 데이터셋에서의 뛰어난 일반화 성능을 입증했지만, 동시에 이는 특정 도메인의 특화된 문제(specificity)와의 트레이드오프 관계를 명확히 보여준다. 결국 YOLOv7의 진정한 역할은 그 자체로 모든 문제의 해결책이 되기보다는, 다양한 특화 문제 해결을 위한 강력하고 효율적인 ’범용 플랫폼’을 제공하는 데 있다. 실제로 수많은 후속 연구들이 YOLOv7을 기반으로 특정 문제에 맞게 아키텍처를 수정하고 최적화하는 방식으로 진행되고 있으며, 이는 YOLOv7의 성공이 오히려 이러한 ‘특화’ 연구를 더욱 활성화시키는 중요한 촉매제가 되었음을 의미한다.</p>
<h2>6.  결론: YOLOv7의 유산과 미래 전망</h2>
<p>YOLOv7은 실시간 객체 탐지 기술의 역사에서 중요한 이정표를 세운 모델이다. 단순히 이전 모델들의 성능을 뛰어넘는 것을 넘어, 연구 개발의 패러다임을 전환하고 후속 기술 발전에 깊은 영향을 미쳤다. 본 장에서는 YOLOv7이 남긴 핵심적인 기여를 종합적으로 요약하고, 이후 모델들에 미친 영향과 미래 객체 탐지 기술의 발전 방향을 전망하며 안내서를 마무리한다.</p>
<p>YOLOv7의 핵심 기여는 세 가지로 요약할 수 있다.</p>
<p>첫째, <strong>훈련 최적화라는 새로운 패러다임을 제시했다.</strong> ’Trainable Bag-of-Freebies’라는 혁신적인 개념을 통해, 추론 성능에 부담을 주지 않으면서 훈련 과정의 복잡성을 적극적으로 활용하여 정확도를 극대화하는 새로운 연구 방향을 개척했다.12 이는 아키텍처 설계에 집중되었던 기존 연구의 관점을 확장시키는 중요한 계기가 되었다.</p>
<p>둘째, <strong>효율적인 아키텍처 설계의 정점을 보여주었다.</strong> E-ELAN과 복합 스케일링 기법은 깊고 복잡한 네트워크의 고질적인 문제였던 학습 불안정성과 비효율적인 확장성 문제를 해결하는 정교한 해법을 제시했다.11</p>
<p>셋째, <strong>CNN 기반 1-stage detector의 잠재력을 재입증했다.</strong> Transformer와 같은 새로운 아키텍처가 부상하는 가운데, 잘 설계되고 최적화된 CNN 기반 모델이 여전히 속도와 정확도 모든 면에서 최고 수준의 성능을 달성할 수 있음을 증명했다.12</p>
<p>YOLOv7의 성공은 그 이후에 등장한 YOLOv8, v9, v10과 같은 후속 모델들의 개발 철학에도 지대한 영향을 미쳤다. YOLOv7이 성능 최적화의 정점을 보여주었다면, 후속 모델들은 이러한 고성능을 기반으로 ’플랫폼화’와 ’생태계 구축’이라는 새로운 과제에 집중하는 경향을 보인다. 예를 들어, Ultralytics의 YOLOv8은 단순히 성능 수치를 높이는 것뿐만 아니라, 사용 편의성, 코드의 모듈성, 그리고 객체 탐지를 넘어 인스턴스 분할(instance segmentation), 자세 추정(pose estimation) 등 다양한 downstream task로의 손쉬운 확장을 지원하는 데 큰 중점을 둔다.2 이는 객체 탐지 기술이 소수의 전문가를 위한 연구 단계를 넘어, 다양한 분야의 개발자들이 쉽게 활용하고 응용할 수 있는 범용 ’플랫폼’으로 진화하고 있음을 의미한다.</p>
<p>이러한 흐름 속에서 미래의 실시간 객체 탐지 기술은 다음과 같은 방향으로 나아갈 것으로 전망된다.</p>
<ul>
<li><strong>향상된 설명가능성(Explainability)</strong>: 모델이 특정 객체를 탐지하거나 실패하는 이유를 인간이 이해할 수 있도록 만드는 기술이 중요해질 것이다. 이는 모델의 신뢰성을 높이고, 특히 의료나 자율주행과 같이 결정의 근거가 중요한 분야에서 필수적인 요구사항이다.9</li>
<li><strong>강화된 강건성(Robustness)</strong>: 악천후, 예상치 못한 조명 변화, 카메라 왜곡 등 현실 세계의 비정형적이고 어려운 조건 속에서도 안정적인 성능을 유지하는 능력이 더욱 중요해질 것이다. 이는 단순히 데이터 증강을 넘어, 도메인 일반화(domain generalization)와 같은 근본적인 해결책을 요구한다.8</li>
<li><strong>효율적인 배포(Efficient Deployment)</strong>: 고성능 클라우드 GPU부터 저전력 엣지 디바이스에 이르기까지, 다양한 하드웨어 플랫폼에 모델을 최적화하고 효율적으로 배포하는 기술이 핵심 경쟁력이 될 것이다. 모델 양자화(quantization), 가지치기(pruning), 그리고 하드웨어-인지적 신경망 아키텍처 탐색(NAS) 등이 더욱 활발히 연구될 것이다.9</li>
</ul>
<p>결론적으로, YOLOv7은 YOLO 계보에서 ’최적화의 정점’을 찍은 전환적인 모델로 평가할 수 있다. 이 모델은 성능 경쟁 시대의 한 막을 내리는 상징적인 성과를 이루었으며, 그 기술적 유산은 후속 모델들이 성능을 넘어 ‘사용성’, ‘확장성’, 그리고 ’생태계’라는 새로운 가치를 추구하게 만든 중요한 원동력이 되었다. YOLOv7은 객체 탐지 기술의 성숙을 증명하는 동시에, 연구의 초점이 ‘최고의 단일 모델’ 개발에서 ‘모델을 둘러싼 강력한 플랫폼’ 구축으로 이동하는 변곡점의 역할을 수행했다고 할 수 있다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Performance of YOLOv7 in Kitchen Safety While Handling Knife - arXiv, https://arxiv.org/html/2501.05399v1</li>
<li>YOLO Evolution: A Comprehensive Benchmark and Architectural Review of YOLOv12, YOLO11, and Their Previous Versions - arXiv, https://arxiv.org/html/2411.00201v2</li>
<li>YOLOv7-RAR for Urban Vehicle Detection - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC9964850/</li>
<li>What is YOLO? The Ultimate Guide [2025] - Roboflow Blog, https://blog.roboflow.com/guide-to-yolo-models/</li>
<li>Ultralytics YOLO Docs: Home, https://docs.ultralytics.com/</li>
<li>Evolution of YOLO Object Detection Model From V5 to V8 [Updated] - Labellerr, https://www.labellerr.com/blog/evolution-of-yolo-object-detection-model-from-v5-to-v8/</li>
<li>YOLO Object Detection Explained: A Beginner’s Guide - DataCamp, https://www.datacamp.com/blog/yolo-object-detection-explained</li>
<li>The YOLO Framework: A Comprehensive Review of Evolution …, https://www.mdpi.com/2073-431X/13/12/336</li>
<li>YOLO Explained: From v1 to Present - Viso Suite, https://viso.ai/computer-vision/yolo-explained/</li>
<li>Evolution of YOLO: A Timeline of Versions and Advancements in Object Detection, https://yolovx.com/evolution-of-yolo-a-timeline-of-versions-and-advancements-in-object-detection/</li>
<li>Review — YOLOv7: Trainable Bag-of-Freebies Sets New State-of …, https://sh-tsang.medium.com/review-yolov7-trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors-b29f33c041a8</li>
<li>[2207.02696] YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - ar5iv, https://ar5iv.labs.arxiv.org/html/2207.02696</li>
<li>YOLOv7: Trainable Bag-of-Freebies - Ultralytics YOLO Docs, https://docs.ultralytics.com/models/yolov7/</li>
<li>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors, https://www.researchgate.net/publication/361807900_YOLOv7_Trainable_bag-of-freebies_sets_new_state-of-the-art_for_real-time_object_detectors</li>
<li>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real …, https://homepage.iis.sinica.edu.tw/papers/liao/new-7175-F.pdf</li>
<li>WongKinYiu/yolov7: Implementation of paper - YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - GitHub, https://github.com/WongKinYiu/yolov7</li>
<li>YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors, https://www.computer.org/csdl/proceedings-article/cvpr/2023/012900h464/1POSiroxBZe</li>
<li>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - CVPR, https://cvpr.thecvf.com/media/cvpr-2023/Slides/22602.pdf</li>
<li>YOLOv7 explanation and implementation from scratch - Kaggle, https://www.kaggle.com/code/vishakkbhat/yolov7-explanation-and-implementation-from-scratch</li>
<li>What is YOLOv7? A Complete Guide. - Roboflow Blog, https://blog.roboflow.com/yolov7-breakdown/</li>
<li>YOLOv7 Object Detection Paper Explanation &amp; Inference - LearnOpenCV, https://learnopencv.com/yolov7-object-detection-paper-explanation-and-inference/</li>
<li>YOLOv7 Architecture Explanation - Plugger - AI, https://www.plugger.ai/blog/yolov7-architecture-explanation</li>
<li>Underwater target detection based on improved YOLOv7 - arXiv, https://arxiv.org/pdf/2302.06939</li>
<li>YOLOv7- Real-time Object Detection at its Best - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2022/08/yolov7-real-time-object-detection-at-its-best/</li>
<li>Animal detection model with YOLOv7 - Daniel Sierra, http://www.sierradaniel.com/mcd_yolo/</li>
<li>question about E-ELAN structure and Re-parameterized · Issue #339 · WongKinYiu/yolov7, https://github.com/WongKinYiu/yolov7/issues/339</li>
<li>Model scaling for concatenation-based models. From (a) to (b), we… - ResearchGate, https://www.researchgate.net/figure/Model-scaling-for-concatenation-based-models-From-a-to-b-we-observe-that-when-depth_fig3_361807900</li>
<li>Model Scaling of YOLOv7 (Wang et al., 2022) - ResearchGate, https://www.researchgate.net/figure/Model-Scaling-of-YOLOv7-Wang-et-al-2022_fig3_368377088</li>
<li>YOLOv7: A Powerful Object Detection Algorithm- viso.ai, https://viso.ai/deep-learning/yolov7-guide/</li>
<li>YOLOv7: The Most Advanced Object Detection Algorithm? - Unite.AI, https://www.unite.ai/yolov7/</li>
<li>Advances in trash detection project : presentation of YOLOv7 and methods for addressing class imbalance - GreenAI UPPA, https://greenai-uppa.github.io/seminars/sem25.pdf</li>
<li>Planned re-parameterized model. In the proposed planned… - ResearchGate, https://www.researchgate.net/figure/Planned-re-parameterized-model-In-the-proposed-planned-re-parameterized-model-we-found_fig4_361807900</li>
<li>YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_YOLOv7_Trainable_Bag-of-Freebies_Sets_New_State-of-the-Art_for_Real-Time_Object_Detectors_CVPR_2023_paper.pdf</li>
<li>Coarse for auxiliary and fine for lead head label assigner. Compare… | Download Scientific Diagram - ResearchGate, https://www.researchgate.net/figure/Coarse-for-auxiliary-and-fine-for-lead-head-label-assigner-Compare-with-normal-model_fig5_361807900</li>
<li>A Decade of You Only Look Once (YOLO) for Object Detection - arXiv, https://arxiv.org/html/2504.18586v1</li>
<li>Enhancing Object Detection in Self-Driving Cars Using a Hybrid Approach - MDPI, https://www.mdpi.com/2079-9292/12/13/2768</li>
<li>YOLOv7-Based Autonomous Driving Object Detection Algorithm - ResearchGate, https://www.researchgate.net/publication/382811253_YOLOv7-Based_Autonomous_Driving_Object_Detection_Algorithm</li>
<li>Autonomous driving target detection based on improved YOLOv7 - ResearchGate, https://www.researchgate.net/publication/384481781_Autonomous_driving_target_detection_based_on_improved_YOLOv7</li>
<li>Navigating the Future Advancing Autonomous Vehicles through Robust Target Recognition and Real-Time Avoidance - AUC Knowledge Fountain - The American University in Cairo, https://fount.aucegypt.edu/etds/2422/</li>
<li>YOLO Algorithm for Object Detection Explained [+Examples] - V7 Labs, https://www.v7labs.com/blog/yolo-object-detection</li>
<li>(PDF) Evaluating the Effectiveness of YOLO Models in Different Sized Object Detection and Feature-Based Classification of Small Objects - ResearchGate, https://www.researchgate.net/publication/373899688_Evaluating_the_Effectiveness_of_YOLO_Models_in_Different_Sized_Object_Detection_and_Feature-Based_Classification_of_Small_Objects</li>
<li>YOLOv7 Optimization Model Based on Attention Mechanism Applied …, https://www.mdpi.com/2076-3417/13/16/9173</li>
<li>Dense Small Object Detection Based on an Improved YOLOv7 Model - MDPI, https://www.mdpi.com/2076-3417/14/17/7665</li>
<li>A Small Object Detection Algorithm for Traffic Signs Based on …, https://pmc.ncbi.nlm.nih.gov/articles/PMC10459082/</li>
<li>Is there any limit to detected objects in YOLOv7? - Stack Overflow, https://stackoverflow.com/questions/75507636/is-there-any-limit-to-detected-objects-in-yolov7</li>
<li>Enhanced YOLOv7 integrated with small target enhancement for rapid detection of objects on water surfaces - Frontiers, https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2023.1315251/full</li>
<li>MS-YOLOv7:YOLOv7 Based on Multi-Scale for Object Detection on UAV Aerial Photography, https://www.mdpi.com/2504-446X/7/3/188</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>