<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:YOLOv5 (2020)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>YOLOv5 (2020)</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">객체 탐지 (Object Detection)</a> / <a href="index.html">YOLO 객체 탐지 모델</a> / <span>YOLOv5 (2020)</span></nav>
                </div>
            </header>
            <article>
                <h1>YOLOv5 (2020)</h1>
<h2>1. YOLOv5의 등장과 맥락</h2>
<h3>1.1  YOLO 계보와 단일 단계 객체 탐지 패러다임</h3>
<p>YOLOv5를 이해하기 위한 기술적 배경으로서, 먼저 단일 단계(Single-Stage) 객체 탐지 패러다임에 대한 이해가 선행되어야 한다. 객체 탐지 알고리즘은 크게 2단계(Two-Stage)와 단일 단계 방식으로 나뉜다. Faster R-CNN과 같은 2단계 탐지기는 먼저 객체가 존재할 만한 후보 영역(Region Proposal)을 생성하고, 이후 각 후보 영역에 대해 분류(Classification)와 경계 상자 회귀(Bounding Box Regression)를 수행하는 분리된 파이프라인을 가진다.1 이 방식은 높은 정확도를 보장하지만, 다단계 처리로 인해 속도가 느리다는 본질적인 한계를 지닌다.</p>
<p>반면, YOLO(You Only Look Once) 계열로 대표되는 단일 단계 탐지기는 객체 탐지 전체 과정을 하나의 통합된 회귀(Regression) 문제로 재정의한다.2 입력 이미지를 한 번만 신경망에 통과시켜 경계 상자의 위치와 클래스 확률을 동시에 예측함으로써, 구조적 단순성과 압도적인 추론 속도를 확보한다.4 이러한 패러다임의 전환은 실시간 객체 탐지 분야의 비약적인 발전을 이끌었다.</p>
<p>YOLO의 기술적 진화 과정은 다음과 같이 요약할 수 있다. 2015년 Joseph Redmon 등에 의해 처음 제안된 YOLOv1은 입력 이미지를 그리드 셀(Grid Cell)로 나누고 각 셀이 객체 탐지를 책임지는 혁신적인 개념을 도입했다.3 YOLOv2(YOLO9000)는 사전 정의된 다양한 형태의 기준 상자인 앵커 박스(Anchor Box) 개념과 배치 정규화(Batch Normalization)를 도입하여 탐지 정확도를 크게 향상시켰다.3 YOLOv3는 다크넷-53(Darknet-53)이라는 더 깊은 백본 네트워크와 FPN(Feature Pyramid Network) 구조를 차용하여, 서로 다른 크기의 특징 맵에서 예측을 수행함으로써 다중 스케일(Multi-Scale) 객체 탐지 능력을 획기적으로 개선했다.7 2020년 4월에 발표된 YOLOv4는 CSP(Cross Stage Partial) 구조를 백본에 적용하고, 데이터 증강 및 훈련 기법 모음인 ’Bag of Freebies’와 ’Bag of Specials’와 같은 다양한 최적화 기법을 집대성하여 속도와 정확도의 균형을 한 단계 끌어올렸다.8</p>
<p>이러한 기술적 토대 위에서, YOLOv4가 발표된 지 불과 두 달 만인 2020년 6월, Ultralytics의 Glenn Jocher에 의해 YOLOv5가 공개되었다.9 이처럼 짧은 간격의 등장은 객체 탐지 분야의 연구 개발 주기가 극도로 단축되고 있음을 보여주는 상징적인 사건이었다.</p>
<h3>1.2  YOLOv5의 탄생 배경: Darknet에서 PyTorch로의 전환</h3>
<p>YOLOv5의 가장 본질적인 차별점은 이전 버전들이 사용했던 C언어 기반의 Darknet 프레임워크에서 완전히 벗어나, PyTorch 프레임워크를 기반으로 새롭게 구현되었다는 점이다.9 이는 원저자인 Joseph Redmon의 코드를 단순히 수정하거나 확장(fork)한 것이 아니라, Glenn Jocher가 이전에 개발하여 이미 커뮤니티에서 높은 인기를 얻고 있던 YOLOv3 PyTorch 구현체를 계승하고 발전시킨 결과물이다.2</p>
<p>이러한 프레임워크의 전환은 단순한 구현 언어의 변경을 넘어서는 전략적 선택이었다. Darknet은 특정 연구 목적으로는 강력했지만, 상대적으로 사용자층이 좁고 다른 딥러닝 생태계와의 연동이 어려워 폭넓은 채택에 장벽이 존재했다. 반면, PyTorch는 방대한 사용자 커뮤니티, 유연한 동적 계산 그래프, 풍부한 라이브러리 생태계를 바탕으로 당시 머신러닝 연구 및 개발의 표준으로 자리 잡고 있었다. YOLOv5가 PyTorch를 채택함으로써, 개발자들은 익숙한 도구와 환경에서 손쉽게 모델을 실험, 수정, 배포할 수 있게 되었다. 이는 YOLOv5의 접근성과 사용 편의성을 극적으로 향상시키는 결정적 계기가 되었고, 고성능 객체 탐지 기술의 대중화를 이끄는 촉매제로 작용했다.2 결과적으로 YOLOv5의 가치는 아키텍처의 혁신성 자체보다, 최첨단 기술을 누구나 쉽게 활용할 수 있도록 만든 탁월한 엔지니어링과 사용자 경험에 더 큰 비중을 두게 되었다.</p>
<h3>1.3  안내서의 목적과 구성</h3>
<p>본 안내서는 YOLOv5의 아키텍처, 핵심 기술, 성능 벤치마크, 개발 배경과 관련된 논쟁, 그리고 객체 탐지 분야 전반에 미친 영향을 종합적이고 심층적으로 고찰하는 것을 목표로 한다. 이를 위해 먼저 YOLOv5의 구조를 Backbone, Neck, Head로 나누어 각 구성 요소의 역할과 기술적 특징을 상세히 분석한다. 다음으로 모자이크 데이터 증강, AutoAnchor, 손실 함수 등 모델의 성능을 뒷받침하는 핵심 기술과 훈련 전략을 탐구한다. 이후 공인된 데이터셋에서의 성능을 정량적으로 평가하고, YOLOv4를 비롯한 주요 경쟁 모델들과 비교 분석한다. 또한, 모델의 이름과 공개 방식을 둘러싼 논쟁을 조명하여 기술 외적인 맥락을 제공한다. 마지막으로, 이러한 분석을 종합하여 YOLOv5의 기술적 성취와 역사적 의의를 다각적으로 평가하고 결론을 도출한다.</p>
<h2>2. YOLOv5 아키텍처 심층 분석</h2>
<h3>2.1  전체 구조 개요: Backbone-Neck-Head</h3>
<p>YOLOv5는 현대 객체 탐지 모델의 표준적인 3단 구조를 충실히 따른다. 이 모듈식 설계는 YOLOv4에서 정립된 구조를 계승한 것으로, 각 부분의 독립적인 개선과 최적화를 용이하게 한다.4 전체적인 데이터 흐름은 다음과 같다.</p>
<ol>
<li><strong>Backbone (백본)</strong>: 입력 이미지를 받아 다양한 해상도의 계층적인 특징 맵(feature map)을 추출하는 역할을 한다. 이미지의 저수준 특징(모서리, 질감)부터 고수준의 의미론적 특징(객체의 일부)까지 점진적으로 학습한다.12</li>
<li><strong>Neck (넥)</strong>: 백본에서 생성된 여러 수준의 특징 맵들을 입력받아 이를 융합하고 재가공한다. 이 과정을 통해 서로 다른 스케일의 특징 정보를 결합하여, 모델이 다양한 크기의 객체를 효과적으로 탐지할 수 있도록 풍부한 컨텍스트 정보를 생성한다.12</li>
<li><strong>Head (헤드)</strong>: 넥으로부터 처리된 특징 맵을 전달받아, 최종적으로 객체의 경계 상자(bounding box) 좌표, 객체 존재 확률(objectness score), 그리고 클래스 분류 확률(class probabilities)을 예측하는 역할을 수행한다.4</li>
</ol>
<p>이 세 부분의 유기적인 상호작용을 통해 YOLOv5는 단일 순전파(forward pass)만으로 빠르고 정확한 객체 탐지를 실현한다.</p>
<h3>2.2  Backbone: CSPDarknet53 - 효율적 특징 추출의 핵심</h3>
<p>YOLOv5 백본의 핵심 아키텍처는 CSPDarknet53으로, 이는 YOLOv4에서 도입된 CSP(Cross Stage Partial) 개념을 계승하고 발전시킨 구조다.4 CSP 구조의 도입은 연산 효율성과 성능이라는 두 가지 목표를 동시에 달성하기 위한 전략적 선택이었다.</p>
<h4>2.2.1 Cross Stage Partial (CSP) 구조</h4>
<p>CSPNet은 깊은 컨볼루션 신경망(CNN)에서 흔히 발생하는 중복된 경사도 정보(redundant gradient information) 문제를 해결하기 위해 제안되었다.2 이는 특히 DenseNet과 같은 고밀도 연결 구조에서 영감을 얻었으나, DenseNet의 연산 병목 현상을 완화하는 데 초점을 맞췄다.2 CSP의 핵심 아이디어는 각 연산 블록(stage)의 입력 특징 맵을 두 개의 경로로 분할하는 것이다. 한 경로는 기존의 연산 블록(예: Dense Block 또는 ResNet Block)을 통과하여 특징을 변환하고, 다른 경로는 별도의 변환 없이 그대로 다음 단계로 전달된다. 마지막에 두 경로의 출력을 결합(concatenate)함으로써, 경사도 흐름의 일부를 분리하여 불필요한 연산을 줄인다. 이 구조는 경사도 소실 문제를 완화하고 특징 전파를 강화하면서도, 전체 연산량(FLOPS)을 크게 감소시켜 모델의 효율성을 극대화한다.2</p>
<h4>2.2.2 C3 모듈</h4>
<p>YOLOv5에서는 이러한 CSP 구조를 C3 모듈이라는 형태로 구체화했다. C3 모듈은 이전 YOLO 버전의 BottleneckCSP 모듈을 개선한 것으로, 3개의 컨볼루션 레이어와 다수의 Bottleneck 블록으로 구성된다.18 이 모듈은 CSP 구조의 원리를 따르면서도 더 가볍고 빠른 구조를 지향하며, 백본과 넥 전반에 걸쳐 반복적으로 사용되어 파라미터 효율성을 높이고 풍부한 특징 표현을 학습하는 데 중추적인 역할을 한다.</p>
<h4>2.2.3 초기 버전의 Focus 레이어와 후속 버전의 6x6 Conv2d</h4>
<p>초기 YOLOv5 버전(v6.0 이전)의 독특한 구성 요소 중 하나는 입력단에 위치한 ‘Focus’ 레이어였다.14 이 레이어는 고해상도 이미지(예: 640x640x3)를 공간적으로 재배열하는 슬라이싱(slicing) 연산을 수행한다. 이미지를 2x2 픽셀 단위로 샘플링하여 4개의 독립적인 이미지로 분리한 후, 이들을 채널 차원에서 결합(concatenate)한다. 그 결과, 공간 해상도는 절반(320x320)으로 줄어드는 대신 채널 수는 4배(12)로 늘어난다. 이 방식은 정보 손실을 최소화하면서도 후속 컨볼루션 레이어의 연산량을 효과적으로 줄이는 역할을 했다.</p>
<p>그러나 후속 버전(v6.0 이상)에서는 이 독창적인 Focus 레이어가 하드웨어 가속(예: TensorRT)에 더 유리하고 기능적으로 동일한 역할을 수행하는 6x6 커널 크기의 단일 컨볼루션 레이어(stride=2)로 대체되었다.15 이는 YOLOv5의 개발 철학이 아키텍처의 이론적 독창성보다는 실제 배포 환경에서의 실용적인 성능과 효율성을 우선시함을 명확히 보여주는 사례다. 즉, 더 표준적이고 하드웨어 친화적인 연산으로 동일한 목표를 달성할 수 있다면 과감히 기존 구조를 개선하는 엔지니어링적 실용주의가 반영된 것이다.</p>
<h3>2.3  Neck: PANet과 SPPF - 다중 스케일 특징의 융합</h3>
<p>백본에서 추출된 다양한 스케일의 특징 맵들은 객체의 크기와 위치에 따라 서로 다른 정보를 담고 있다. 넥(Neck)의 역할은 이러한 다중 스케일 특징들을 효과적으로 융합하여, 모든 크기의 객체를 강건하게 탐지할 수 있는 통합된 특징 표현을 생성하는 것이다. YOLOv5는 이를 위해 PANet과 SPPF라는 두 가지 핵심 요소를 사용한다.</p>
<h4>2.3.1 PANet (Path Aggregation Network)</h4>
<p>YOLOv5는 특징 융합을 위해 PANet 구조를 채택했다.4 PANet은 기존의 FPN(Feature Pyramid Network)을 확장한 개념이다. FPN은 백본의 깊은 층에서 추출된 고수준의 의미론적(semantic) 특징을 업샘플링(upsampling)을 통해 얕은 층으로 전달하는 하향식(top-down) 경로를 구축한다. 이를 통해 저해상도 특징 맵에 고수준의 컨텍스트 정보를 보강한다.</p>
<p>PANet은 여기에 더해, 얕은 층의 정밀한 위치 정보(localization information)를 다시 깊은 층으로 전달하는 상향식(bottom-up) 경로를 추가한다.5 이 양방향 정보 흐름은 서로 다른 수준의 특징들이 효과적으로 융합되도록 하여, 특히 작은 객체의 위치를 정확하게 파악하고 큰 객체의 클래스를 명확하게 분류하는 능력을 동시에 향상시킨다.</p>
<h4>2.3.2 SPPF (Spatial Pyramid Pooling - Fast)</h4>
<p>백본의 가장 깊은 층에서 나온 특징 맵은 가장 넓은 수용 영역(receptive field)을 가지지만, 공간 해상도가 낮다. 다양한 크기의 객체를 인식하기 위해서는 이 단계에서 여러 스케일의 컨텍스트 정보를 통합하는 것이 중요하다. 이를 위해 YOLOv5는 SPP(Spatial Pyramid Pooling) 계열의 모듈을 사용한다. YOLOv4의 SPP 모듈은 입력 특징 맵에 대해 여러 다른 커널 크기(예: 5x5, 9x9, 13x13)의 Max-Pooling을 병렬로 적용한 후 그 결과를 모두 결합하는 방식이었다.</p>
<p>YOLOv5는 이를 SPPF(Spatial Pyramid Pooling - Fast) 모듈로 최적화했다.15 SPPF는 5x5 커널의 Max-Pooling을 병렬이 아닌 순차적으로 3번 적용한다. 놀랍게도, 이 간단한 변경만으로도 병렬 SPP와 거의 동일한 수용 영역 확장 효과를 얻으면서 연산량은 현저히 줄일 수 있다. 이 최적화는 YOLOv5의 추론 속도를 두 배 이상 향상시키는 데 결정적인 기여를 했다.15 SPPF 역시 SPP와 마찬가지로 입력 특징 맵의 크기에 관계없이 고정된 길이의 특징 벡터를 출력하여 후속 레이어와의 연결을 용이하게 한다.</p>
<h3>2.4  Head: YOLOv3 기반 탐지 헤드 - 최종 예측 생성</h3>
<p>YOLOv5의 헤드(Head)는 최종 예측을 생성하는 부분으로, YOLOv3와 YOLOv4에서 그 성능이 검증된 구조를 거의 그대로 계승한다.4 이는 새로운 구조를 도입하기보다 안정적이고 효율적인 기존 설계를 활용하려는 YOLOv5의 실용주의적 접근을 다시 한번 보여준다.</p>
<p>헤드는 넥으로부터 3개의 서로 다른 스케일을 갖는 특징 맵(P3, P4, P5)을 입력받는다. 각 특징 맵은 해상도에 따라 특정 크기의 객체를 탐지하는 데 특화되어 있다.14</p>
<ul>
<li><strong>P3 (고해상도, 예: 80x80)</strong>: 가장 해상도가 높아 작은 객체(small objects) 탐지에 사용된다.</li>
<li><strong>P4 (중간 해상도, 예: 40x40)</strong>: 중간 크기 객체(medium objects) 탐지에 사용된다.</li>
<li><strong>P5 (저해상도, 예: 20x20)</strong>: 해상도가 가장 낮지만 수용 영역이 넓어 큰 객체(large objects) 탐지에 사용된다.</li>
</ul>
<p>각 특징 맵의 모든 그리드 셀(grid cell)은 미리 정의된 3개의 앵커 박스(anchor box)를 기준으로 예측을 수행한다. 각 앵커 박스에 대해 모델은 다음 5 + C개의 값을 예측한다.14</p>
<ol>
<li><strong>경계 상자 좌표 (4개 값)</strong>: <code>x, y, w, h</code> (중심점 x, 중심점 y, 너비, 높이)</li>
<li><strong>객체성 점수 (1개 값)</strong>: 해당 앵커 박스 내에 객체가 존재할 확률(objectness score).</li>
<li><strong>클래스 확률 (C개 값)</strong>: 객체가 각 클래스에 속할 확률(class probabilities). C는 데이터셋의 총 클래스 수이다 (예: COCO 데이터셋의 경우 80).</li>
</ol>
<p>따라서 최종적으로 생성되는 예측 텐서의 차원은 <code>(배치 크기, 앵커 수, 그리드 높이, 그리드 너비, 5 + 클래스 수)</code> 형태를 가진다.14 이 예측값들은 후처리 과정(Post-processing)에서 비최대 억제(Non-Maximum Suppression, NMS) 알고리즘을 거쳐 중복된 경계 상자들이 제거되고 최종 탐지 결과가 생성된다.</p>
<h2>3. 핵심 기술 및 훈련 전략</h2>
<p>YOLOv5의 뛰어난 성능은 단순히 아키텍처의 우수성만으로 설명될 수 없다. 모델의 학습 능력을 극대화하고 일반화 성능을 향상시키기 위해 고안된 정교한 훈련 전략과 핵심 기술들이 그 기반을 이루고 있다. 이러한 기술들은 사용자가 복잡한 설정 없이도 높은 성능을 달성할 수 있도록 훈련 파이프라인에 내장되어 있다.</p>
<h3>3.1  데이터 증강: 모자이크 증강의 효과</h3>
<p>YOLOv5는 훈련 과정에서 실시간으로 데이터를 변형하고 확장하는 다양한 증강(augmentation) 기법을 사용하며, 그중 가장 핵심적인 것은 모자이크 증강(Mosaic Augmentation)이다.12 이는 YOLOv4에서 처음 도입된 기법으로, 4개의 훈련 이미지를 무작위로 잘라내고 크기를 조절하여 하나의 큰 이미지로 합성하는 방식이다.12</p>
<p>이 기법이 가져오는 주요 효과는 다음과 같다. 첫째, 작은 객체 탐지 능력을 크게 향상시킨다. 합성 과정에서 원본 이미지의 객체들이 상대적으로 작아지거나 일부가 잘린 형태로 나타나기 때문에, 모델은 작은 크기의 객체나 가려진 객체를 인식하는 데 더 강건해진다.12 이는 특히 COCO와 같이 작은 객체가 많은 데이터셋에서 매우 효과적이다. 둘째, 모델의 컨텍스트 학습 능력을 강화한다. 일반적인 이미지에서는 볼 수 없는, 다양한 객체와 배경이 하나의 이미지 내에 공존하는 비정상적인 장면을 학습하게 함으로써, 모델이 특정 배경에 과적합(overfitting)되는 것을 방지하고 일반화 성능을 높인다.24</p>
<p>또한, YOLOv5는 훈련의 마지막 몇 에포크(epoch) 동안에는 모자이크 증강을 비활성화하는 <code>close_mosaic</code> 옵션을 제공한다.26 이는 훈련 후반부에 모델이 일반적인 단일 이미지(합성되지 않은)에 대한 탐지 성능을 미세 조정하고 안정화할 수 있도록 돕는 전략이다.</p>
<h3>3.2  앵커 박스 자동 학습: AutoAnchor</h3>
<p>YOLO 계열 모델은 사전 정의된 앵커 박스를 기준으로 경계 상자의 위치와 크기를 예측한다. 따라서 앵커 박스의 크기와 비율이 훈련 데이터셋에 있는 객체들의 분포와 잘 맞을수록 모델의 성능이 향상된다. 이전 버전에서는 사용자가 K-means 클러스터링과 같은 방법을 사용하여 데이터셋에 최적화된 앵커 박스를 수동으로 계산하고 설정해야 했다.</p>
<p>YOLOv5는 이 과정을 완전히 자동화하는 AutoAnchor 기능을 도입하여 사용자 편의성을 획기적으로 개선했다.12 훈련이 시작되기 전, AutoAnchor 기능은 데이터셋의 모든 실제 경계 상자(Ground Truth)의 형태(너비와 높이)를 분석한다. 먼저 K-means 클러스터링을 사용하여 데이터 분포에 적합한 초기 앵커 박스 후보군을 찾는다.28 이후, 유전 알고리즘(Genetic Algorithm)을 통해 이 후보군을 반복적으로 변형하고 평가하여, 주어진 데이터셋에 대해 가장 높은 적합도(fitness)를 보이는 최적의 앵커 박스 세트를 자동으로 생성하고 적용한다.15 이 기능 덕분에 사용자는 데이터셋의 특성에 대한 깊은 사전 지식 없이도 최적에 가까운 성능을 손쉽게 얻을 수 있다. 이는 전문 지식을 코드에 내장하여 기술의 접근성을 높인 대표적인 사례다.</p>
<h3>3.3  손실 함수 심층 분석</h3>
<p>YOLOv5의 전체 손실 함수(Loss Function)는 모델이 예측한 값과 실제 값의 차이를 측정하고, 이를 최소화하는 방향으로 모델의 가중치를 업데이트하는 역할을 한다. 이 손실 함수는 세 가지 주요 구성 요소의 가중 합으로 이루어진다: 경계 상자 손실(<span class="math math-inline">L_{box}</span>), 객체성 손실(<span class="math math-inline">L_{obj}</span>), 그리고 분류 손실(`Lcls)이다.15</p>
<h4>3.3.1 경계 상자 손실 (Lbox)</h4>
<p>경계 상자의 위치와 크기를 얼마나 정확하게 예측했는지를 평가하는 손실이다. YOLOv5는 이 손실 계산을 위해 CIoU(Complete Intersection over Union) Loss를 기본으로 사용한다.4 CIoU는 기존의 IoU(Intersection over Union)를 개선한 것으로, 두 경계 상자 간의 세 가지 핵심 요소를 모두 고려한다: 1) 겹치는 영역(Overlap area), 2) 중심점 거리(Center point distance), 3) 가로세로 비율(Aspect ratio)의 일관성. 이를 통해 예측 상자가 실제 상자와 전혀 겹치지 않는 경우에도 유의미한 경사도를 제공하여, 더 빠르고 안정적인 수렴을 가능하게 한다.31 CIoU Loss의 수식은 다음과 같다.<br />
<span class="math math-display">
L_{CIoU} = 1 - IoU + \frac{\rho^2(b, b^{gt})}{c^2} + \alpha v
</span><br />
여기서 <span class="math math-inline">IoU</span>는 예측 상자(<span class="math math-inline">b</span>)와 실제 상자(<span class="math math-inline">b^{gt}</span>)의 IoU 값, <span class="math math-inline">\rho^2(b, b^{gt})</span>는 두 상자 중심점 간의 유클리드 거리 제곱, <span class="math math-inline">c</span>는 두 상자를 모두 포함하는 가장 작은 볼록 상자의 대각선 길이, 그리고 <span class="math math-inline">\alpha v</span>는 가로세로 비율의 일관성을 측정하는 페널티 항이다.23</p>
<h4>3.3.2 객체성 및 분류 손실 (<span class="math math-inline">L_{obj}</span>, Lcls)</h4>
<p>객체성 손실은 특정 그리드 셀과 앵커 박스가 객체를 포함하고 있는지 여부를 예측하는 것에 대한 손실이며, 분류 손실은 해당 객체가 어떤 클래스에 속하는지를 예측하는 것에 대한 손실이다. YOLOv5는 이 두 가지 손실을 계산하기 위해 모두 BCEWithLogitsLoss(Binary Cross-Entropy with Logits Loss)를 사용한다.4 이 손실 함수는 각 클래스에 대해 독립적인 이진 분류를 수행하는 것과 같기 때문에, 하나의 객체가 여러 클래스에 속하는 다중 레이블(multi-label) 분류 문제를 자연스럽게 지원할 수 있다.</p>
<h4>3.3.3 손실 균형 조정</h4>
<p>객체 탐지에서는 대부분의 예측이 객체가 없는 배경(negative samples)에 해당하므로, 객체가 있는 경우(positive samples)와 배경 간의 심각한 불균형이 발생한다. 이를 해결하기 위해 YOLOv5는 객체성 손실에 대해 각 예측 레이어(P3, P4, P5)별로 차등적인 가중치를 적용한다. 예를 들어, <code>[4.0, 1.0, 0.4]</code>와 같은 가중치를 사용하여, 작은 객체를 탐지하는 고해상도 특징 맵(P3)에서 발생하는 수많은 배경 예측에 대한 손실의 영향력을 줄이고, 상대적으로 중요한 객체 예측에 더 집중하도록 유도한다.15</p>
<h3>3.4  그리드 민감도 문제 해결</h3>
<p>이전 YOLO 버전들은 객체의 중심이 그리드 셀의 경계(예: x=0 또는 x=1)에 정확히 위치할 경우, 시그모이드(sigmoid) 함수의 특성상 해당 값을 예측하기 위해 네트워크 출력이 무한대에 가까워져야 하는 ‘그리드 민감도(grid sensitivity)’ 문제를 가지고 있었다.4 이는 훈련을 불안정하게 만드는 요인이었다.</p>
<p>YOLOv5는 경계 상자 중심점 (<span class="math math-inline">(b_x, b_y)</span>)와 크기 (<span class="math math-inline">(b_w, b_h)</span>)를 예측하는 수식을 수정하여 이 문제를 해결했다.</p>
<p>이전 버전의 수식:<br />
<span class="math math-display">
b_x = \sigma(t_x) + c_x
</span></p>
<p><span class="math math-display">
b_w = p_w e^{t_w}
</span></p>
<p>YOLOv5의 수정된 수식:<br />
<span class="math math-display">
b_x = (2 \cdot \sigma(t_x) - 0.5) + c_x
</span></p>
<p><span class="math math-display">
b_w = p_w \cdot (2 \cdot \sigma(t_w))^2
</span></p>
<p>여기서 <span class="math math-inline">t</span>는 네트워크의 원시 출력, <span class="math math-inline">c</span>는 그리드 셀의 좌상단 좌표, <span class="math math-inline">p</span>는 앵커 박스의 크기이다. 수정된 중심점 수식은 예측 범위를 기존의 <span class="math math-inline">(0, 1)</span>에서 <span class="math math-inline">(-0.5, 1.5)</span>로 확장하여, 모델이 그리드 셀 경계를 넘어서 중심점을 쉽게 예측할 수 있도록 한다.15 또한, 크기 예측에 사용되던 지수 함수(<span class="math math-inline">e^t</span>)는 예측값이 무한대로 발산하여 경사도 폭주(runaway gradients)나 NaN 손실을 유발할 위험이 있었다. YOLOv5는 이를 제곱 형태의 스케일링된 시그모이드 함수로 대체하여 예측값의 범위를 제한하고 훈련 과정을 안정화시켰다.15</p>
<h2>4. 성능 평가 및 벤치마크</h2>
<p>YOLOv5의 가치는 다양한 환경에서의 정량적 성능 평가를 통해 입증된다. 본 장에서는 표준 벤치마크인 COCO 데이터셋에서의 공식 성능을 분석하고, 주요 경쟁 모델들과의 비교를 통해 YOLOv5의 상대적 위치와 강점을 평가한다.</p>
<h3>4.1  COCO 데이터셋 공식 성능</h3>
<p>YOLOv5는 모델의 깊이와 너비를 조절하는 <code>depth_multiple</code>과 <code>width_multiple</code>이라는 두 가지 스케일링 파라미터를 통해 다양한 크기의 모델을 제공한다.4 이를 통해 사용자는 자신의 애플리케이션이 요구하는 속도, 정확도, 그리고 하드웨어 리소스 제약에 맞춰 최적의 모델을 선택할 수 있는 유연성을 확보한다. 주요 모델은 나노(n), 스몰(s), 미디엄(m), 라지(l), 엑스트라 라지(x)로 구분된다.</p>
<p>아래 표는 COCO val2017 데이터셋에서 평가된 YOLOv5 P5 모델(입력 해상도 640x640)의 공식 성능 지표를 요약한 것이다. 이 표는 각 모델 변형 간의 정확도(mAP), 속도(ms), 그리고 모델 복잡도(파라미터 수, 연산량)의 상충 관계(trade-off)를 명확하게 보여준다. 이는 실무자가 특정 작업에 가장 적합한 모델을 선택하는 데 있어 핵심적인 정량적 근거를 제공한다.</p>
<p><strong>표 1: YOLOv5 모델별 COCO val2017 성능 비교</strong> 34</p>
<table><thead><tr><th>Model</th><th>size (pixels)</th><th>mAPval 50-95</th><th>mAPval 50</th><th>Speed V100 b1 (ms)</th><th>Params (M)</th><th>FLOPs @640 (B)</th></tr></thead><tbody>
<tr><td>YOLOv5n</td><td>640</td><td>28.0</td><td>45.7</td><td>6.3</td><td>1.9</td><td>4.5</td></tr>
<tr><td>YOLOv5s</td><td>640</td><td>37.4</td><td>56.8</td><td>6.4</td><td>7.2</td><td>16.5</td></tr>
<tr><td>YOLOv5m</td><td>640</td><td>45.4</td><td>64.1</td><td>8.2</td><td>21.2</td><td>49.0</td></tr>
<tr><td>YOLOv5l</td><td>640</td><td>49.0</td><td>67.3</td><td>10.1</td><td>46.5</td><td>109.1</td></tr>
<tr><td>YOLOv5x</td><td>640</td><td>50.7</td><td>68.9</td><td>12.1</td><td>86.7</td><td>205.7</td></tr>
</tbody></table>
<p><em>주: mAPval 50-95는 IoU 임계값을 0.5부터 0.95까지 0.05 간격으로 변화시키며 측정한 평균 AP이며, mAPval 50은 IoU 임계값 0.5에서의 AP를 의미한다. 속도는 NVIDIA V100 GPU, 배치 크기 1 환경에서 측정되었다.</em></p>
<p>표에서 볼 수 있듯이, 모델 크기가 커질수록 파라미터 수와 연산량이 증가하며 mAP 정확도 또한 꾸준히 향상된다. 반면, 추론 속도는 점차 느려진다. 특히 YOLOv5n과 YOLOv5s는 매우 적은 파라미터와 연산량으로 실시간 처리가 가능한 속도를 보여주어, 임베디드 시스템이나 엣지 디바이스에 매우 적합하다. 반면 YOLOv5l과 YOLOv5x는 높은 정확도가 요구되는 서버 기반 애플리케이션에 더 적합하다.</p>
<h3>4.2  YOLOv4와의 비교 분석</h3>
<p>YOLOv5는 직접적인 선행 모델인 YOLOv4와 자주 비교된다. 두 모델은 CSPDarknet53 백본과 PANet 넥이라는 유사한 아키텍처 골격을 공유하지만, 구현 프레임워크와 최적화 수준에서 중요한 차이를 보인다.</p>
<ul>
<li><strong>정확도 및 속도</strong>: 다수의 독립적인 비교 연구에서 YOLOv5는 YOLOv4와 동등하거나 더 높은 mAP를 달성하면서도, 추론 속도는 훨씬 빠른 것으로 일관되게 나타났다.4 Ultralytics가 PyTorch 프레임워크에 맞춰 수행한 심층적인 최적화 덕분에, Tesla V100 GPU 환경에서 YOLOv5는 140 FPS에 달하는 놀라운 속도를 기록한 반면, 동일한 PyTorch 환경으로 이식된 YOLOv4는 50 FPS 수준에 머물렀다.9 이러한 속도 차이는 YOLOv5의 실용적 가치를 크게 높이는 핵심 요소다.</li>
<li><strong>모델 크기</strong>: YOLOv5는 모델 크기 면에서 YOLOv4에 비해 압도적인 이점을 가진다. YOLOv5s 모델의 가중치 파일 크기는 약 27MB에 불과한 반면, Darknet 기반의 YOLOv4는 244MB에 달한다.9 이는 약 90%에 달하는 크기 감소로, 모델을 저장하고 전송하며, 특히 메모리 제약이 심한 임베디드 시스템이나 엣지 디바이스에 배포할 때 매우 중요한 장점으로 작용한다.35</li>
<li><strong>프레임워크 및 사용성</strong>: YOLOv4는 C언어 기반의 Darknet 프레임워크를 사용하는 반면, YOLOv5는 PyTorch를 기반으로 한다.6 이 차이는 단순한 구현의 차이를 넘어, 개발 생태계 전반에 영향을 미친다. PyTorch의 거대한 커뮤니티, 풍부한 문서, 쉬운 디버깅 환경, 그리고 다른 라이브러리와의 원활한 통합은 YOLOv5의 훈련, 미세 조정, 배포 과정을 훨씬 더 용이하게 만들었다.</li>
</ul>
<h3>4.3  타 주요 모델과의 비교</h3>
<h4>4.3.1 vs. EfficientDet</h4>
<p>Google Brain이 개발한 EfficientDet은 EfficientNet 백본, BiFPN(Bi-directional Feature Pyramid Network) 넥, 그리고 모델의 깊이, 너비, 해상도를 동시에 조절하는 복합 스케일링(compound scaling) 기법을 통해 높은 정확도와 파라미터 효율성을 달성한 모델이다.38 순수 정확도 측면에서는, 가장 큰 EfficientDet 모델이 YOLOv5x를 능가하는 성능을 보이기도 한다. 그러나 이러한 정확도는 상당한 추론 시간을 대가로 한다. 예를 들어, 비슷한 mAP를 보이는 YOLOv5l 모델(49.0 mAP)은 EfficientDet-D4보다 5배 이상 빠른 추론 속도를 보인다.39 따라서 실시간성이 중요한 대부분의 산업 및 상업용 애플리케이션에서는 속도와 정확도의 균형이 뛰어난 YOLOv5가 더 실용적이고 합리적인 선택으로 평가된다.12</p>
<h4>4.3.2 vs. Faster R-CNN</h4>
<p>Faster R-CNN은 RPN(Region Proposal Network)을 사용하는 대표적인 2단계(Two-Stage) 탐지기이다. 이 방식은 후보 영역을 먼저 생성하고 각 영역을 정밀하게 분석하므로, 특히 작거나 심하게 가려진 객체에 대해 높은 정밀도를 제공하는 경향이 있다.1 하지만 구조적으로 복잡하고 다단계 처리로 인해 추론 속도가 매우 느리다는 명백한 단점을 가진다. 반면, 단일 단계 탐지기인 YOLOv5는 속도 면에서 압도적인 우위를 점하며, 정확도와 속도 간의 균형(trade-off)에서 훨씬 효율적인 모습을 보인다.1 따라서 실시간 비디오 분석, 자율 주행, 로보틱스 등 즉각적인 반응이 필수적인 애플리케이션에서는 YOLOv5가 훨씬 더 적합한 솔루션이다.</p>
<h2>5. 개발 배경과 논쟁</h2>
<p>YOLOv5의 등장은 기술적 성취만큼이나 그 개발 및 공개 과정을 둘러싼 논쟁으로도 많은 주목을 받았다. 이는 YOLOv5의 정체성과 신뢰성에 대한 커뮤니티의 폭넓은 논의를 촉발시켰다.</p>
<h3>5.1  Ultralytics의 역할과 오픈소스 전략</h3>
<p>YOLOv5는 전통적인 학술 연구의 결과물과는 다른 경로로 개발되었다. 이는 특정 학술 논문의 발표를 통해 공개된 것이 아니라, Ultralytics라는 회사가 주도하는 지속적인 오픈소스 소프트웨어 프로젝트로서의 성격이 매우 강하다.10 이러한 배경은 YOLOv5의 개발 방식에 큰 영향을 미쳤다. 개발은 GitHub 저장소를 중심으로 이루어졌으며, 정기적인 버전 업데이트, 이슈 트래커를 통한 활발한 커뮤니티와의 소통, 그리고 사용자 편의성 개선에 집중하는 실용적인 접근 방식을 취했다. 이는 학문적 독창성보다는 실제 사용자의 요구와 산업 현장에서의 적용 가능성을 우선시하는 개발 철학을 반영한다.</p>
<h3>5.2  ‘YOLOv5’ 네이밍 논란</h3>
<p>YOLOv5가 공개되었을 때 가장 큰 논란 중 하나는 ’YOLOv5’라는 이름 자체였다. YOLO의 원저자인 Joseph Redmon이나 YOLOv4를 개발한 Alexey Bochkovskiy 팀과 직접적인 계승 관계가 없음에도 불구하고 ’v5’라는 명칭을 사용한 것은 커뮤니티 내에서 정통성에 대한 의문을 불러일으켰다.42 특히 Joseph Redmon이 YOLOv4의 등장을 공식적으로 인정하고 지지했던 것과는 대조적으로 43, YOLOv5는 그러한 승인 절차 없이 명명되었다. 이는 YOLO라는 강력한 브랜드의 연속성을 임의로 주장하는 것으로 비춰져 일부 개발자들로부터 비판을 받았다.</p>
<p>이에 대해 Ultralytics 측은 자신들이 개발한 YOLOv3-PyTorch 저장소가 이미 사실상의 표준으로 널리 사용되고 있었고, YOLOv4 또한 자신들의 작업을 상당 부분 참조했기 때문에 YOLOv5로의 발전은 자연스러운 기술적 흐름이라는 입장을 보였다.43 이 논쟁은 YOLOv5의 기술적 가치와는 별개로, 오픈소스 커뮤니티 내에서의 브랜딩과 정통성에 대한 중요한 논의를 촉발시켰다.</p>
<h3>5.3  논문 부재와 벤치마크 공정성 문제</h3>
<p>YOLOv5는 동료 심사(peer-review)를 거친 공식 학술 논문 없이 GitHub를 통해 모델과 코드를 공개했다. 이는 학계의 전통적인 검증 절차를 따르지 않았다는 점에서 모델의 신뢰성에 대한 비판을 받았다.9 논문의 부재는 모델의 아키텍처와 훈련 방법에 대한 상세하고 체계적인 설명이 부족하다는 것을 의미했고, 이는 초기 사용자들에게 혼란을 주기도 했다.</p>
<p>더욱이, 출시 초기 Roboflow 블로그를 통해 발표된 벤치마크 결과는 YOLOv4의 저자인 Alexey Bochkovskiy로부터 공정성에 대한 강한 비판을 받았다.10 비판의 핵심은 두 가지였다. 첫째, 추론 시간(latency)을 실시간성을 평가하는 표준적인 방식인 배치 크기(batch size) 1이 아닌, 처리량을 높이는 데 유리한 배치 크기 32로 측정한 후 32로 나누어 FPS를 부풀렸다는 점이다. 둘째, 성능이 상대적으로 낮은 경량 모델인 YOLOv5s를 성능이 높은 대형 모델인 YOLOv4와 직접 비교하여, YOLOv5의 속도와 크기 이점을 과장했다는 점이다.42</p>
<p>이러한 비판은 매우 타당한 지적이었으며, 이후 Roboflow 측은 커뮤니티의 피드백을 수용하여 벤치마크 방법론을 수정하고, 재현 가능한 코드를 공개함으로써 논쟁을 일부 해소했다.43 이 사건은 새로운 모델의 성능을 주장할 때 투명하고 공정한 비교의 중요성을 다시 한번 상기시키는 계기가 되었다.</p>
<p>이러한 논란에도 불구하고 YOLOv5가 폭넓게 채택된 현상은 주목할 만하다. 이는 최종 사용자인 개발자와 엔지니어들에게 모델의 ‘혈통’(이름의 정통성, 논문 발표 여부)보다 ‘실용성’(설치의 용이성, 훈련 속도, 실제 데이터에서의 성능, 우수한 문서, 활발한 커뮤니티 지원)이 더 중요한 가치로 작용했음을 시사한다. 즉, YOLOv5는 전통적인 학술적 검증 절차를 거치지 않았음에도 불구하고, 압도적인 사용 편의성과 강력한 실용적 가치를 통해 스스로의 정당성을 입증하며 시장의 선택을 받았다. 이는 응용 AI 분야에서 사용자 경험과 엔지니어링의 우수성이 학문적 권위를 넘어설 수 있음을 보여주는 중요한 사례로 남았다.</p>
<p>종합 고찰 및 의의</p>
<h3>5.4  객체 탐지 기술의 대중화에 대한 기여</h3>
<p>YOLOv5의 가장 큰 역사적 의의는 고성능 객체 탐지 기술의 문턱을 극적으로 낮추고 대중화를 이끌었다는 점에 있다. 이전의 연구 중심 모델들이 특정 프레임워크에 종속되거나 복잡한 설정 과정을 요구했던 것과 달리, YOLOv5는 PyTorch 기반의 직관적인 사용법, 포괄적인 공식 문서, 그리고 풍부한 튜토리얼을 제공함으로써 딥러닝 비전문가도 단 몇 줄의 코드로 모델을 훈련하고 배포할 수 있는 길을 열었다.34</p>
<p>특히, 다양한 크기의 모델(n/s/m/l/x)을 제공한 전략은 객체 탐지 기술의 응용 범위를 폭발적으로 확장시켰다. YOLOv5n과 같은 초경량 모델은 수백만 개의 파라미터만으로도 준수한 성능을 보여, 컴퓨팅 자원이 극도로 제한된 엣지 디바이스나 모바일 환경에서도 실시간 탐지를 가능하게 했다.4 반면, YOLOv5x와 같은 대형 모델은 높은 정확도가 요구되는 클라우드 서버 환경에서 복잡한 문제를 해결하는 데 사용되었다. 이러한 유연성은 학술 연구실에서부터 산업 현장, 개인 프로젝트에 이르기까지 다양한 스펙트럼의 사용자들이 각자의 필요에 맞는 솔루션을 찾을 수 있게 해주었다.</p>
<h3>5.5  기술적 한계와 후속 모델에의 영향</h3>
<p>모든 기술이 그러하듯 YOLOv5 역시 명확한 기술적 한계를 가지고 있으며, 이러한 한계는 후속 모델 개발의 중요한 동력이 되었다.</p>
<ul>
<li><strong>기술적 한계</strong>:</li>
</ul>
<ol>
<li><strong>앵커 박스 의존성</strong>: YOLOv5는 앵커 박스 기반의 탐지 방식을 사용한다. AutoAnchor 기능으로 최적화 과정을 자동화했지만, 이 방식은 본질적으로 사전 정의된 박스 형태에 의존한다. 따라서 데이터셋에 매우 특이한 형태나 극단적인 가로세로 비율을 가진 객체가 많을 경우, 성능이 저하될 수 있는 잠재적 한계를 가진다.38</li>
<li><strong>정확도 포화</strong>: YOLOv5는 속도와 정확도의 탁월한 균형에 초점을 맞추었기 때문에, 시간이 지나고 더 진보된 아키텍처(예: Transformer 기반 모델)가 등장하면서 순수 정확도(mAP) 경쟁에서는 최고 수준의 자리를 유지하지 못하게 되었다.49</li>
<li><strong>탐지 실패 사례</strong>: 다른 모든 탐지기와 마찬가지로, YOLOv5 역시 특정 시나리오에서 탐지에 어려움을 겪는다. 극도로 작은 객체, 수많은 객체가 밀집된 환경, 객체의 상당 부분이 다른 객체에 의해 가려진 심한 가림(occlusion) 상황, 그리고 객체와 배경의 구분이 모호한 복잡한 배경 등에서는 여전히 오탐지(False Positive) 및 미탐지(False Negative) 문제가 발생한다.52</li>
</ol>
<ul>
<li><strong>후속 모델(YOLOv8)에 미친 영향</strong>: YOLOv5의 엄청난 성공은 Ultralytics가 후속 모델을 개발하는 데 있어 결정적인 발판이 되었다. YOLOv5가 구축한 사용자 친화적인 생태계, 잘 정립된 훈련 파이프라인, 그리고 방대한 사용자 커뮤니티는 후속 모델인 YOLOv8에 그대로 계승되었다.50 동시에, YOLOv5의 기술적 한계는 YOLOv8이 나아가야 할 명확한 방향을 제시했다. 예를 들어, YOLOv8은 앵커 박스 의존성을 없앤 Anchor-Free 헤드를 도입하여 유연성을 높였고, 분류와 회귀 헤드를 분리(decoupled head)하여 각 태스크의 성능을 최적화했으며, YOLOv5의 C3 모듈을 연산 효율이 더 높은 C2f 모듈로 개선하는 등 아키텍처를 현대화했다. 이는 YOLOv5가 단순히 성공적인 모델에 그치지 않고, 후속 연구의 성능을 측정하는 ’기준점(baseline)’이자 극복해야 할 ’개선 대상’으로서 중요한 역할을 했음을 의미한다.7 즉, YOLOv5는 그 자체로 하나의 완성된 제품이었을 뿐만 아니라, 지속적인 반복 혁신을 위한 견고한 플랫폼의 역할을 수행한 것이다.</li>
</ul>
<h3>5.6  결론: ’엔지니어링의 승리’로서의 YOLOv5</h3>
<p>YOLOv5를 종합적으로 평가할 때, 이는 완전히 새로운 이론을 제시한 ’과학적 돌파구(Scientific Breakthrough)’라기보다는, 기존의 최첨단 기술들을 정교하게 조합하고, 최상의 소프트웨어 엔지니어링을 통해 사용성, 속도, 안정성을 극대화한 ’엔지니어링의 승리(Triumph of Engineering)’로 규정하는 것이 가장 타당하다.</p>
<p>YOLOv5의 아키텍처는 YOLOv4에서 많은 부분을 계승했지만, PyTorch로의 전환, SPPF와 같은 세밀한 최적화, 그리고 AutoAnchor와 같은 자동화된 훈련 전략을 통해 실용적인 가치를 극대화했다. 개발 과정을 둘러싼 논쟁에도 불구하고, YOLOv5가 전 세계적으로 가장 널리 사용되는 객체 탐지 모델 중 하나가 될 수 있었던 이유는 바로 이 지점에 있다.</p>
<p>따라서 YOLOv5의 진정한 유산은 COCO 데이터셋에서의 mAP 점수 그 자체가 아니다. 그것은 전 세계 수많은 개발자, 연구자, 학생들이 복잡하고 다루기 어려웠던 객체 탐지 기술을 자신의 문제 해결에 손쉽게 적용할 수 있도록 만든, 강력하고 접근성 높은 통합 플랫폼을 제공했다는 점에 있다. YOLOv5는 객체 탐지 기술의 응용 범위를 연구실의 울타리를 넘어 산업 현장과 일상으로 폭발적으로 확장시킨 기폭제로서 그 역사적 의의를 가진다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Comparative Analysis of YOLO and Faster R-CNN Models for Detecting Traffic Object - The Science and Information (SAI) Organization, https://thesai.org/Downloads/Volume16No3/Paper_42-Comparative_Analysis_of_YOLO_and_Faster_R_CNN_Models.pdf</li>
<li>What is YOLOv5: A deep look into the internal features of the popular object detector - arXiv, https://arxiv.org/html/2407.20892v1</li>
<li>YOLO Algorithm for Object Detection Explained [+Examples] - V7 Labs, https://www.v7labs.com/blog/yolo-object-detection</li>
<li>YOLO v5 model architecture [Explained] - OpenGenus IQ, https://iq.opengenus.org/yolov5/</li>
<li>My Experiments with Yolov5:Almost everything you want to know about Yolov5-Series -Part 2 | by Manjusha Sithik | Medium, https://medium.com/@manjusha.bs/my-experiments-with-yolov5-almost-everything-you-want-to-know-about-yolov5-series-part-2-6549abdb5b63</li>
<li>Comparative Analysis of YOLOv5, YOLOv4, and SSD for Aircraft Detection in Airport Environments Using Remote Sensing Data - SETSCI Conference Proceedings, https://www.set-science.com/pdf_view.php?cacryn=ITTSCONF2024&amp;cid=98&amp;pid=4&amp;fv=SETSCI_ITTSCONF2024_0098_004</li>
<li>YOLO Evolution: A Comprehensive Benchmark and Architectural Review of YOLOv12, YOLO11, and Their Previous Versions - arXiv, https://arxiv.org/html/2411.00201v2</li>
<li>Comparing YOLOv3, YOLOv4 and YOLOv5 for Autonomous Landing Spot Detection in Faulty UAVs - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC8778480/</li>
<li>YOLO v4 or YOLO v5 or PP-YOLO? - Towards Data Science, https://towardsdatascience.com/yolo-v4-or-yolo-v5-or-pp-yolo-dad8e40f7109/</li>
<li>YOLOv5 Controversy — Is YOLOv5 Real? | by Ritesh Kanjee | Augmented AI | Medium, https://medium.com/augmented-startups/yolov5-controversy-is-yolov5-real-20e048bebb08</li>
<li>YOLOv4 vs. YOLOv5: Compared and Contrasted - Roboflow, https://roboflow.com/compare/yolov4-vs-yolov5</li>
<li>What is YOLOv5? A Guide for Beginners. - Roboflow Blog, https://blog.roboflow.com/yolov5-improvements-and-evaluation/</li>
<li>The architecture of the YOLOv5 model, which consists of three parts - ResearchGate, https://www.researchgate.net/figure/The-architecture-of-the-YOLOv5-model-which-consists-of-three-parts-i-Backbone_fig1_363777336</li>
<li>YOLOv5s Model Architecture. YOLO detectors are popular in the …, https://medium.com/@hussainirfan510/yolov5s-model-architecture-344415f3fc83</li>
<li>Ultralytics YOLOv5 Architecture - Ultralytics YOLO Docs, https://docs.ultralytics.com/yolov5/tutorials/architecture_description/</li>
<li>YOLOv5 architecture. Backbone: CSPD; neck: PANet; and head: YOLO layer… | Download Scientific Diagram - ResearchGate, https://www.researchgate.net/figure/YOLOv5-architecture-Backbone-CSPD-neck-PANet-and-head-YOLO-layer-detection-results_fig4_364270124</li>
<li>Enhance Object Detection Performance Through Anchor Box Optimization - Medium, https://medium.com/@beam_villa/enhance-object-detection-performance-through-anchor-box-optimization-761b68a1a4a4</li>
<li>Exploring the Advanced Architecture of YOLOv5 for Object Detection | by Nagvekar, https://nagvekar.medium.com/exploring-the-advanced-architecture-of-yolov5-for-object-detection-ee6ad40d0fec</li>
<li>Brief Review: YOLOv5 for Object Detection | by Sik-Ho Tsang - Medium, https://sh-tsang.medium.com/brief-review-yolov5-for-object-detection-84cc6c6a0e3a</li>
<li>YOLO V5 — Explained and Demystified - Towards AI, <a href="https://towardsai.net/p/computer-vision/yolo-v5%E2%80%8A-%E2%80%8Aexplained-and-demystified">https://towardsai.net/p/computer-vision/yolo-v5%E2%80%8A-%E2%80%8Aexplained-and-demystified</a></li>
<li>Concept of Head in YoloV5 : r/computervision - Reddit, https://www.reddit.com/r/computervision/comments/tkfyv4/concept_of_head_in_yolov5/</li>
<li>Understanding and how to use YOLOv5: My Personal Guide to Object Detection kinda | by Abdul Rauf | Medium, https://medium.com/@raufpokemon00/understanding-and-how-to-use-yolov5-my-personal-guide-to-object-detection-kinda-9b42d3b99ce2</li>
<li>Understanding YOLOv5 Loss: A Comprehensive Analysis, https://pgmesa.medium.com/understanding-yolov5-loss-a-comprehensive-analysis-ffe35e6203e0</li>
<li>Understanding Mosaic Data Augmentation - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2023/12/mosaic-data-augmentation/</li>
<li>YOLOX Explanation — Mosaic and Mixup For Data Augmentation | by Gabriel Mongaras, https://gmongaras.medium.com/yolox-explanation-mosaic-and-mixup-for-data-augmentation-3839465a3adf</li>
<li>Data Augmentation using Ultralytics YOLO, https://docs.ultralytics.com/guides/yolo-data-augmentation/</li>
<li>Tips for Best YOLOv5 Training Results - Ultralytics YOLO Docs, https://docs.ultralytics.com/yolov5/tutorials/tips_for_best_training_results/</li>
<li>Training YOLO? Select Anchor Boxes Like This - Towards Data Science, https://towardsdatascience.com/training-yolo-select-anchor-boxes-like-this-3226cb8d7f0b/</li>
<li>Yolov5 : Bounding Boxes and Anchor Boxes with numerical example | by Nitin Kushwaha, https://medium.com/@Nitin_Indian/yolov5-bounding-boxes-and-anchor-boxes-with-numerical-example-67d6f92e4ba6</li>
<li>Loss function during the YOLOv5 and YOLOv7 as the training progress…. - ResearchGate, https://www.researchgate.net/figure/Loss-function-during-the-YOLOv5-and-YOLOv7-as-the-training-progress-See-Ref-22-for-a_fig1_370606909</li>
<li>Enhancing YOLOv5 Performance for Small-Scale Corrosion Detection in Coastal Environments Using IoU-Based Loss Functions - MDPI, https://www.mdpi.com/2077-1312/12/12/2295</li>
<li>mAP values of GIoU-YOLOv5 and CIoU-YOLOv5 under different IoU NMS. - ResearchGate, https://www.researchgate.net/figure/mAP-values-of-GIoU-YOLOv5-and-CIoU-YOLOv5-under-different-IoU-NMS_tbl1_360041457</li>
<li>YOLOv4 vs YOLOv5. Where is the truth? | by Ildar Idrisov, PhD | Deelvin Machine Learning, https://medium.com/deelvin-machine-learning/yolov4-vs-yolov5-db1e0ac7962b</li>
<li>ultralytics/yolov5: YOLOv5 in PyTorch &gt; ONNX &gt; CoreML … - GitHub, https://github.com/ultralytics/yolov5</li>
<li>Evaluating YOLOv4 and YOLOv5 for Enhanced Object Detection in UAV-Based Surveillance, https://www.mdpi.com/2227-9717/13/1/254</li>
<li>YOLOv5: State-of-the-art object detection at 140 FPS | Hacker News, https://news.ycombinator.com/item?id=23478151</li>
<li>Comparative Performance Analysis of YOLOv4 and YOLOv5 Algorithms on Dangers Objects, https://www.researchgate.net/publication/372589174_Comparative_Performance_Analysis_of_YOLOv4_and_YOLOv5_Algorithms_on_Dangers_Objects</li>
<li>YOLOv5 vs. EfficientDet: A Detailed Technical Comparison - Ultralytics YOLO Docs, https://docs.ultralytics.com/compare/yolov5-vs-efficientdet/</li>
<li>EfficientDet vs YOLOv5: A Detailed Technical Comparison - Ultralytics YOLO Docs, https://docs.ultralytics.com/compare/efficientdet-vs-yolov5/</li>
<li>Faster R-CNN vs. YOLOv5: Compared and Contrasted - Roboflow, https://roboflow.com/compare/faster-r-cnn-vs-yolov5</li>
<li>YOLO-V5: Architecture deep-dive || YOLO OBJECT DETECTION SERIES - YouTube, https://www.youtube.com/watch?v=XRZmynM4nB4</li>
<li>YOLOv5: A 2025 Guide to the Model &amp; its Controversy - Viso Suite, https://viso.ai/computer-vision/yolov5-controversy/</li>
<li>[Discussion] Responding to the Controversy about YOLOv5 : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/h7kxpd/discussion_responding_to_the_controversy_about/</li>
<li>Responding to the Controversy about YOLOv5 - Roboflow Blog, https://blog.roboflow.com/yolov4-versus-yolov5/</li>
<li>[News] YOLOv5 is Here: State-of-the-Art Object Detection at 140 FPS : r/MachineLearning, https://www.reddit.com/r/MachineLearning/comments/h0ddia/news_yolov5_is_here_stateoftheart_object/</li>
<li>Enhanced YOLOv5: An Efficient Road Object Detection Method - MDPI, https://www.mdpi.com/1424-8220/23/20/8355</li>
<li>A Comprehensive Review of YOLOv5: Advances in Real-Time Object Detection - ijircst.org, https://www.ijircst.org/DOC/12-a-comprehensive-review-of-yolov5-advances-in-real-time-object-detection.pdf</li>
<li>A comparative study of YOLOv5 models performance for image localization and classification, https://archive.ceciis.foi.hr/public/conferences/2022/Proceedings/IS/IS4.pdf</li>
<li>YOLOv5 vs YOLO11: A Technical Comparison - Ultralytics YOLO Docs, https://docs.ultralytics.com/compare/yolov5-vs-yolo11/</li>
<li>YOLOv8 vs YOLOv5: A Detailed Comparison - Ultralytics YOLO Docs, https://docs.ultralytics.com/compare/yolov8-vs-yolov5/</li>
<li>YOLOv10 vs. YOLOv5: A Detailed Technical Comparison - Ultralytics YOLO Docs, https://docs.ultralytics.com/compare/yolov10-vs-yolov5/</li>
<li>(PDF) A Comparative Study of YOLOv5 and YOLOv7 Object Detection Algorithms, https://www.researchgate.net/publication/368377088_A_Comparative_Study_of_YOLOv5_and_YOLOv7_Object_Detection_Algorithms</li>
<li>Improved YOLOv5 Based on Transformer Prediction Head for Object Detection on Drone-Captured Scenarios - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2021W/VisDrone/papers/Zhu_TPH-YOLOv5_Improved_YOLOv5_Based_on_Transformer_Prediction_Head_for_Object_ICCVW_2021_paper.pdf</li>
<li>Error Analysis Summary of Case 3 Test Video - ResearchGate, https://www.researchgate.net/figure/Error-Analysis-Summary-of-Case-3-Test-Video_tbl1_355107980</li>
<li>YOLOv5 vs. YOLOv8: A Detailed Comparison - Ultralytics YOLO Docs, https://docs.ultralytics.com/compare/yolov5-vs-yolov8/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>