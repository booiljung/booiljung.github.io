<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:YOLOv4 (2020)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>YOLOv4 (2020)</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">객체 탐지 (Object Detection)</a> / <a href="index.html">YOLO 객체 탐지 모델</a> / <span>YOLOv4 (2020)</span></nav>
                </div>
            </header>
            <article>
                <h1>YOLOv4 (2020)</h1>
<h2>1. 서론</h2>
<p>2020년 이전의 객체 탐지(Object Detection) 분야는 2-stage detector와 1-stage detector라는 두 가지 주요 패러다임의 경쟁과 발전을 통해 성숙기에 접어들고 있었다. Faster R-CNN과 같은 2-stage detector는 높은 정확도를 자랑했지만, 실시간 처리가 어려운 속도의 한계를 지니고 있었다. 반면, YOLO(You Only Look Once)와 SSD(Single Shot MultiBox Detector)로 대표되는 1-stage detector는 추론 속도에서 압도적인 우위를 점하며 실시간 애플리케이션의 가능성을 열었다.1 특히 YOLO 시리즈는 v1부터 v3에 이르기까지 빠른 속도와 준수한 정확도를 양립시키며 큰 성공을 거두었다.3 그러나 YOLOv3 역시 다양한 크기의 객체를 탐지하는 데 있어 성능의 변동성이 있었고, C언어 기반의 Darknet 프레임워크는 연구 커뮤니티가 최신 기술을 신속하게 통합하고 실험하기에는 유연성이 부족하다는 평가를 받았다.3</p>
<p>이러한 기술적 배경 속에서 YOLOv4는 독특한 철학을 가지고 등장했다. YOLO의 원저자인 조셉 레드몬(Joseph Redmon)이 컴퓨터 비전 연구의 윤리적 문제에 대한 우려로 프로젝트를 중단한 이후 5, 알렉세이 보흐코프스키(Alexey Bochkovskiy), 치엔야오 왕(Chien-Yao Wang), 홍위안 마크 랴오(Hong-Yuan Mark Liao)가 그 명맥을 이어받았다.6 이 새로운 연구팀의 목표는 완전히 새로운 아키텍처를 창조하는 것이 아니었다. 대신, 그들은 당시에 존재하던 수많은 최첨단 기법들을 체계적으로 실험하고, 최적의 조합을 찾아내어 가장 효율적인 객체 탐지기를 ’설계’하는 공학적 접근법을 택했다.8 이는 순수한 학술적 발명보다는 실용적이고 체계적인 엔지니어링을 통해 최첨단 기술을 달성할 수 있음을 보여주는 중요한 전환점이었다.</p>
<p>YOLOv4 논문의 핵심 명제는 명확했다. 단일의 보편적인 GPU(예: Tesla V100, 1080 Ti, 2080 Ti) 환경에서 실시간 처리가 가능하면서도 최고의 정확도를 달성하는, 즉 속도와 정확도 간의 파레토 최적성(Pareto optimality)을 만족하는 객체 탐지기를 만드는 것이었다.8 이는 고성능 객체 탐지 기술의 진입 장벽을 낮추어 더 넓은 연구 및 개발자 커뮤니티가 최첨단 기술을 활용할 수 있도록 하겠다는 ’민주화’의 철학을 내포하고 있었다. 본 안내서는 이러한 YOLOv4의 아키텍처, 핵심 혁신, 성능 및 기술사적 의의를 심층적으로 분석하고자 한다.</p>
<h2>2.  YOLOv4 아키텍처 심층 분석</h2>
<p>YOLOv4의 아키텍처는 Backbone, Neck, Head라는 세 부분으로 명확하게 구분되는 구조를 채택했다.4 이는 각 구성 요소의 역할을 분리하여 체계적인 최적화를 가능하게 한 설계이다. 연구팀은 각 부분에 대해 당시 가장 효과적이라고 입증된 기술들을 실험하고 조합하여 전체적인 성능을 극대화했다. 이러한 구조는 YOLOv4가 단순한 업그레이드가 아니라, 수많은 실험 결과를 바탕으로 세심하게 조율된 엔지니어링의 산물임을 보여준다.</p>
<h3>2.1  Backbone: CSPDarknet53</h3>
<p>Backbone은 입력 이미지로부터 다양한 수준의 특징(feature)을 추출하는 역할을 담당하는 네트워크의 근간이다. YOLOv4는 YOLOv3의 Backbone이었던 Darknet53을 기반으로, 여기에 CSPNet(Cross-Stage Partial Network)의 개념을 통합하여 CSPDarknet53을 탄생시켰다.14</p>
<p>YOLOv3의 Darknet53은 Darknet-19 아키텍처에 ResNet의 잔차 연결(residual connection) 개념을 결합한 53개 층의 깊은 네트워크였다.1 이는 깊은 네트워크에서도 안정적인 학습을 가능하게 했지만, 연산량이 많다는 단점이 있었다. YOLOv4 연구팀은 이 문제를 해결하기 위해 CSP 연결을 도입했다. CSPNet의 핵심 아이디어는 각 단계(stage)의 입력 특징 맵(feature map)을 두 부분으로 분할하는 것이다. 한 부분은 기존의 연산 블록(예: Dense Block)을 그대로 통과하고, 다른 한 부분은 연산을 건너뛰어 다음 단계의 입력으로 직접 전달된다. 이 두 경로의 결과는 마지막에 다시 하나로 합쳐진다.15 이 구조는 정보의 흐름을 다변화하여 중복되는 그래디언트 계산을 줄이고, 전체 연산량을 약 20% 감소시키면서도 정확도는 유지하거나 오히려 향상시키는 효과를 가져온다.6 YOLOv4 연구팀은 철저한 실험을 통해 객체 분류(classification) 작업에서는 CSPResNext50이 우수했지만, 객체 탐지(detection) 작업에서는 CSPDarknet53이 더 높은 성능을 보인다는 사실을 확인하고 이를 Backbone으로 최종 채택했다.11 이는 작업의 특성에 맞춰 최적의 구성 요소를 선택하는 YOLOv4의 실용주의적 개발 철학을 잘 보여주는 대목이다.</p>
<p>또한, Backbone의 활성화 함수(activation function)로 기존의 Leaky ReLU 대신 Mish를 채택했다.13 ReLU 계열 함수들은 음수 입력에 대해 그래디언트가 0이 되어 뉴런이 비활성화되는 ‘Dying ReLU’ 문제가 발생할 수 있다. Mish는 이를 해결하기 위해 고안된 함수로, 다음과 같은 수식으로 정의된다.18<br />
<span class="math math-display">
f(x) = x \cdot \tanh(\text{softplus}(x))
</span><br />
여기서 <span class="math math-inline">\text{softplus}(x) = \ln(1 + e^x)</span>이다. Mish 함수는 음수 영역에서도 작은 값을 유지하는 비단조성(non-monotonic)을 가지며, 모든 지점에서 미분 가능한 부드러운(smooth) 특성을 지닌다.17 이러한 특성 덕분에 그래디언트 흐름이 안정화되고, 네트워크가 더 풍부하고 표현력 있는 특징을 학습할 수 있게 되어 최종적인 탐지 정확도 향상에 기여한다.17</p>
<h3>2.2  Neck: SPP와 PANet의 결합</h3>
<p>Neck은 Backbone에서 추출된 여러 수준의 특징 맵을 재가공하고 융합하여 Head가 최종 예측을 더 효과적으로 수행할 수 있도록 돕는 중간 다리 역할을 한다.15 YOLOv4는 이 부분에서 SPP(Spatial Pyramid Pooling)와 PANet(Path Aggregation Network)이라는 두 가지 강력한 모듈을 결합했다.</p>
<p>SPP는 고정된 크기의 입력 이미지를 요구하지 않으면서도 다양한 스케일의 정보를 포착하기 위해 고안된 기법이다.13 YOLOv4에서는 이를 변형하여 Backbone의 마지막 특징 맵에 적용했다. 구체적으로, 서로 다른 커널 크기(예: 1x1, 5x5, 9x9, 13x13)를 가진 최대 풀링(max-pooling)을 병렬적으로 수행한 후, 그 결과들을 모두 채널 방향으로 연결(concatenate)한다.10 이 과정을 통해 네트워크의 유효 수용 영역(effective receptive field)이 크게 확장되어, 이미지 내의 작은 객체와 큰 객체를 모두 효과적으로 인식할 수 있는 능력이 향상된다. 중요한 점은 이 과정이 추론 속도에 거의 영향을 미치지 않으면서도 성능을 높인다는 것이다.10</p>
<p>PANet은 YOLOv3에서 사용된 FPN(Feature Pyramid Network)을 개선한 구조이다.3 FPN은 고수준의 의미론적 정보(semantic information)를 담고 있는 상위 계층의 특징 맵을 저수준의 정밀한 위치 정보(localization information)를 담고 있는 하위 계층으로 전달하는 하향식(top-down) 경로를 사용한다. PANet은 여기에 더해, 하위 계층의 강한 지역적 특징을 다시 상위 계층으로 전달하는 상향식(bottom-up) 경로를 추가한다.13 이 양방향 정보 흐름을 통해 각기 다른 수준의 특징 맵들이 서로의 정보를 보강하여 더욱 정교한 특징 피라미드를 형성한다. YOLOv4는 표준 PANet에서 특징 맵을 합치는 방식을 덧셈(addition)이 아닌 연결(concatenation)로 변경하여 정보 손실을 최소화했다.17</p>
<h3>2.3  Head: YOLOv3의 유산</h3>
<p>Head는 Neck으로부터 전달받은 최종 특징 맵을 이용하여 실제 객체의 경계 상자(bounding box), 신뢰도 점수(confidence score), 그리고 클래스 확률(class probability)을 예측하는 부분이다.13 흥미롭게도 YOLOv4는 Backbone과 Neck에서 대대적인 혁신을 이루었음에도 불구하고, Head는 YOLOv3의 것을 그대로 유지했다.12</p>
<p>YOLOv3의 Head는 앵커 박스(anchor box) 기반의 예측 방식을 사용하며, FPN 구조를 통해 생성된 세 가지 다른 스케일의 특징 맵(예: 13x13, 26x26, 52x52) 각각에서 독립적으로 예측을 수행한다.4 이 다중 스케일 예측 방식은 다양한 크기의 객체를 탐지하는 데 매우 효과적이다. YOLOv4 연구팀이 YOLOv3의 Head를 변경하지 않고 유지하기로 한 결정은 그들의 개발 철학을 명확히 보여준다. 즉, 실험을 통해 명백한 성능 향상이 입증되지 않은 부분은 굳이 바꾸지 않고, 검증된 최선의 구성 요소를 유지함으로써 안정성과 효율성을 추구한 것이다. 이는 ’변화를 위한 변화’가 아닌, 철저히 데이터에 기반한 실용적이고 합리적인 선택이었다.</p>
<h2>3.  핵심 혁신: Bag of Freebies (BoF)와 Bag of Specials (BoS)</h2>
<p>YOLOv4의 가장 중요한 방법론적 기여는 수많은 최적화 기법들을 ’Bag of Freebies(BoF)’와 ’Bag of Specials(BoS)’라는 두 가지 개념적 틀로 체계화한 것이다. 이 프레임워크는 딥러닝 모델 최적화에 대한 강력하고 실용적인 사고방식을 제공한다.</p>
<h3>3.1  개념 정의</h3>
<ul>
<li><strong>Bag of Freebies (BoF):</strong> 모델의 추론 비용(inference cost)은 증가시키지 않으면서, 오직 학습 전략이나 비용을 변경하여 모델의 정확도를 향상시키는 기법들의 모음을 의미한다.7 대부분의 모델 학습은 오프라인에서 이루어지므로, 이러한 기법들은 배포 시점에서는 말 그대로 ’공짜’로 성능 향상을 얻는 효과를 가져온다.</li>
<li><strong>Bag of Specials (BoS):</strong> 추론 비용을 약간 증가시키지만, 그 대가로 정확도를 크게 향상시키는 플러그인 모듈 및 후처리 기법들의 모음을 지칭한다.7 이는 성능과 비용 간의 의도적인 트레이드오프를 통해 최적의 균형점을 찾는 전략이다.</li>
</ul>
<h3>3.2  Bag of Freebies (BoF) 심층 분석</h3>
<p>YOLOv4는 다양한 BoF 기법들을 적극적으로 활용하여 학습 과정 자체를 최적화했다.</p>
<h4>3.2.1 데이터 증강 (Data Augmentation)</h4>
<ul>
<li><strong>Mosaic Data Augmentation:</strong> YOLOv4에서 새롭게 제안된 가장 독창적인 데이터 증강 기법이다. 이는 4개의 다른 학습 이미지를 무작위로 잘라 하나의 이미지처럼 이어 붙이는 방식이다.7 이 기법은 모델이 일반적이지 않은 맥락 속에서도 객체를 인식하도록 강제하여 일반화 성능을 높인다. 특히 작은 객체 탐지 능력을 향상시키는 데 효과적이다. 또한, 배치 정규화(Batch Normalization)가 한 번에 4개의 다른 이미지로부터 통계치를 계산하게 되므로, 매우 큰 미니배치(mini-batch) 크기를 사용하지 않고도 안정적인 학습이 가능해지는 부수적인 이점도 있다.10</li>
<li><strong>Self-Adversarial Training (SAT):</strong> 모델의 강건함(robustness)을 높이기 위한 새로운 데이터 증강 기법이다. 이는 두 단계로 진행된다. 첫 번째 단계에서는 신경망이 순전파(forward pass)를 통해 원본 이미지를 미세하게 변형하여, 이미지 내에 원하는 객체가 없는 것처럼 ‘속이는’ 적대적 이미지를 생성한다. 두 번째 단계에서는 이렇게 변형된 이미지를 가지고 객체를 탐지하도록 네트워크를 학습시킨다.7 이 과정을 통해 모델은 미세한 변화나 노이즈에 더 강인해진다.</li>
</ul>
<h4>3.2.2 정규화 기법 (Regularization Techniques)</h4>
<ul>
<li><strong>DropBlock:</strong> 전통적인 Dropout이 개별 뉴런을 무작위로 비활성화하는 반면, DropBlock은 특징 맵의 연속적인 사각형 영역 전체를 비활성화한다.8 이는 모델이 특정 지역의 특징에 과도하게 의존하는 것을 방지하고, 더 넓은 영역에서 분산된 특징 표현을 학습하도록 유도하여 과적합을 효과적으로 억제한다.7</li>
</ul>
<h4>3.2.3 경계 상자 회귀를 위한 손실 함수 (Loss Function for Bounding Box Regression)</h4>
<ul>
<li><strong>Complete IoU (CIoU) Loss:</strong> 경계 상자의 위치를 예측하는 회귀(regression) 문제에서 손실 함수는 매우 중요하다. 전통적인 MSE(Mean Squared Error) 손실은 객체 크기에 따라 손실 값의 스케일이 변하는 문제가 있고, 표준 IoU(Intersection over Union) 손실은 예측 상자와 실제 상자가 겹치지 않을 때 그래디언트가 0이 되어 학습이 진행되지 않는 치명적인 단점이 있다.23 이러한 문제를 해결하기 위해 GIoU(겹치지 않는 경우를 위한 페널티 추가), DIoU(중심점 거리 페널티 추가) 등이 제안되었다.24 YOLOv4는 여기서 한 걸음 더 나아가 CIoU 손실을 채택했다. CIoU는 겹치는 영역(IoU), 중심점 간의 거리, 그리고 경계 상자의 가로세로 비율(aspect ratio)까지 고려하는 가장 포괄적인 손실 함수이다.7 CIoU 손실 함수는 다음과 같이 정의된다.<br />
<span class="math math-display">
\mathcal{L}_{CIoU} = 1 - IoU + \frac{\rho^2(b, b^{gt})}{c^2} + \alpha v
</span><br />
여기서 <span class="math math-inline">b</span>와 <span class="math math-inline">b^{gt}</span>는 각각 예측 상자와 실제 상자를 나타낸다. <span class="math math-inline">\rho^2(b, b^{gt})</span>는 두 상자 중심점 간의 유클리드 거리의 제곱이며, <span class="math math-inline">c</span>는 두 상자를 모두 포함하는 가장 작은 볼록 상자의 대각선 길이이다. 마지막 항인 <span class="math math-inline">\alpha v</span>는 가로세로 비율의 일관성을 측정하는 페널티 항으로, <span class="math math-inline">v = \frac{4}{\pi^2}(\arctan\frac{w^{gt}}{h^{gt}} - \arctan\frac{w}{h})^2</span>와 $ \alpha = \frac{v}{(1-IoU)+v} $로 계산된다.27 이처럼 다각적인 페널티를 통해 CIoU는 더 빠르고 안정적인 수렴과 높은 정확도의 경계 상자 예측을 가능하게 한다.</li>
</ul>
<h3>3.3  Bag of Specials (BoS) 심층 분석</h3>
<p>BoS는 약간의 추론 비용을 감수하고 정확도를 극대화하기 위한 선택적 모듈들이다. YOLOv4의 아키텍처를 구성하는 핵심 요소들 다수가 여기에 해당한다.</p>
<ul>
<li><strong>활성화 함수 (Activation Function):</strong> Backbone에 적용된 <strong>Mish 활성화 함수</strong>는 Leaky ReLU에 비해 연산 비용이 약간 더 높기 때문에 BoS로 분류된다.17</li>
<li><strong>수용 영역 확장 (Enhanced Receptive Field):</strong> Neck에 추가된 <strong>SPP 블록</strong>은 추가적인 풀링 및 연결 연산을 수행하므로 추론 시 약간의 비용을 발생시키는 BoS 모듈이다.13</li>
<li><strong>어텐션 메커니즘 (Attention Mechanism):</strong> YOLOv4는 공간적 어텐션 모듈(SAM)을 변형하여 사용한다. 표준 SAM 대신 점별(point-wise) 어텐션 방식으로 수정하여, 연산 비용 증가를 0.1% 수준으로 최소화하면서도 네트워크가 중요한 특징에 집중하도록 돕는다.13</li>
<li><strong>특징 집계 (Feature Aggregation):</strong> <strong>PANet</strong> 역시 다중 경로의 특징 맵을 융합하는 복잡한 연산을 포함하므로 BoS의 일부로 간주된다.13</li>
<li><strong>후처리 (Post-processing):</strong> <strong>DIoU-NMS</strong>는 기존의 NMS(Non-Maximum Suppression)를 개선한 후처리 기법이다. 전통적인 NMS는 IoU 값만을 기준으로 중복된 경계 상자를 제거하기 때문에, 서로 다른 객체가 가깝게 겹쳐 있을 경우 올바른 상자까지 제거하는 실수를 범할 수 있다. DIoU-NMS는 IoU와 더불어 두 상자의 중심점 간 거리까지 고려한다. 이를 통해 겹침이 크더라도 중심점이 멀리 떨어져 있다면 서로 다른 객체로 판단하여, 겹친 객체들을 더 정확하게 분리해낼 수 있다.17</li>
</ul>
<p>이러한 BoF와 BoS의 체계적인 분류와 적용은 YOLOv4 개발의 핵심 철학을 보여준다. 개발자는 먼저 모든 ‘공짜’ 기법(BoF)을 적용하여 모델의 기본 성능을 최대한 끌어올린 후, 주어진 연산 예산 내에서 목표 정확도를 달성하기 위해 ‘특별’ 모듈(BoS)을 선택적으로 추가하는 실용적인 최적화 워크플로우를 따를 수 있다.</p>
<h2>4.  성능 평가 및 비교 분석</h2>
<p>YOLOv4의 성공은 수많은 실험을 통해 입증된 정량적 성능에 기반한다. 특히 객체 탐지 분야의 표준 벤치마크인 MS COCO 데이터셋에서의 결과는 YOLOv4가 당시 실시간 객체 탐지기의 새로운 기준을 세웠음을 명확히 보여준다.</p>
<h3>4.1  MS COCO 벤치마크 성능</h3>
<p>YOLOv4는 발표 당시 MS COCO 데이터셋에서 SOTA(State-of-the-Art) 성능을 기록했다. Tesla V100 GPU 환경에서 약 65 FPS의 실시간 처리 속도를 유지하면서, <strong>43.5%의 AP(Average Precision)와 65.7%의 AP50</strong>를 달성했다.6 이 수치는 속도와 정확도라는 두 마리 토끼를 모두 잡은 결과로, 실시간 애플리케이션에 고정확도 모델을 적용하고자 하는 많은 연구자와 개발자들에게 큰 반향을 일으켰다.</p>
<h3>4.2  YOLOv3와의 비교: 세대적 도약</h3>
<p>YOLOv4는 이전 세대인 YOLOv3와 비교했을 때 괄목할 만한 성능 향상을 이루었다. 동일한 조건에서 YOLOv3 대비 <strong>AP는 10%, FPS는 12% 향상</strong>되었다.10 이는 단순한 수치 개선을 넘어선 세대적 도약으로 평가받으며, 앞서 논의한 CSPDarknet53 Backbone, SPP와 PANet을 결합한 Neck, 그리고 다양한 BoF/BoS 전략들의 유효성을 강력하게 입증했다.</p>
<table><thead><tr><th>모델</th><th>AP</th><th>AP50</th><th>AP75</th><th>FPS (Tesla V100)</th></tr></thead><tbody>
<tr><td>YOLOv3</td><td>33.0%</td><td>57.9%</td><td>34.4%</td><td>~58</td></tr>
<tr><td><strong>YOLOv4</strong></td><td><strong>43.5%</strong></td><td><strong>65.7%</strong></td><td><strong>47.3%</strong></td><td><strong>~65</strong></td></tr>
</tbody></table>
<h3>4.3  EfficientDet과의 비교: 파레토 최적성</h3>
<p>YOLOv4의 진정한 강점은 단일 지표에서의 최고 성능이 아니라, 속도-정확도 스펙트럼 전반에 걸친 우위, 즉 파레토 최적성에 있다. 당시 강력한 경쟁자였던 구글의 EfficientDet과 비교했을 때 이러한 특징이 두드러진다. YOLOv4는 “비교 가능한 성능의 EfficientDet보다 두 배 빠르다“고 평가되었다.10 예를 들어, Scaled-YOLOv4-P6 모델은 54.3% AP를 30 FPS로 처리하는 반면, 비슷한 정확도를 가진 EfficientDet-D7은 53.7% AP를 8.2 FPS로 처리하는 데 그쳤다.30 이는 어떤 정확도 수준을 선택하든 YOLOv4가 더 빠른 속도를 제공하며, 어떤 속도를 기준으로 하든 더 높은 정확도를 제공함을 의미한다. 이는 YOLOv4의 아키텍처와 최적화 전략이 근본적으로 더 효율적임을 시사한다.</p>
<table><thead><tr><th>모델</th><th>입력 해상도</th><th>AP (COCO test-dev)</th><th>FPS (Tesla V100)</th></tr></thead><tbody>
<tr><td>EfficientDet-D3</td><td>896x896</td><td>47.5%</td><td>36</td></tr>
<tr><td>EfficientDet-D7</td><td>1536x1536</td><td>53.7%</td><td>8.2</td></tr>
<tr><td>YOLOv4-CSP</td><td>608x608</td><td>47.5%</td><td>75</td></tr>
<tr><td><strong>Scaled-YOLOv4-P6</strong></td><td><strong>1280x1280</strong></td><td><strong>54.3%</strong></td><td><strong>30</strong></td></tr>
</tbody></table>
<h3>4.4  YOLOv5와의 비교: 철학과 프레임워크의 분기</h3>
<p>YOLOv4와 YOLOv5의 관계는 종종 혼동을 일으키는데, 이는 선형적인 발전 관계가 아니기 때문이다. YOLOv5는 YOLOv4 발표 직후 Ultralytics라는 다른 팀에 의해 독립적으로 개발되었다.14 가장 큰 차이점은 개발 프레임워크의 전환이다. YOLOv4가 Darknet(C언어 기반)의 정점이었다면, YOLOv5는 현대적인 딥러닝 프레임워크인 PyTorch를 기반으로 개발되어 사용자 편의성과 개발 속도를 크게 향상시켰다.3</p>
<p>아키텍처 측면에서도 미묘한 차이가 존재한다. 두 모델 모두 CSP 기반의 Backbone을 사용하지만, Neck 구성이나 활성화 함수(YOLOv4는 Mish, YOLOv5는 Leaky ReLU/Sigmoid) 등에서 차이를 보인다.32 성능 면에서는 어떤 모델이 절대적으로 우월하다고 단정하기 어렵다. 특정 데이터셋이나 작업에서는 YOLOv5가 더 빠르고 정확한 결과를 보이는 반면 14, 다른 연구에서는 YOLOv4가 학습 데이터에서 보지 못한 새로운 데이터에 대해 더 강건한 성능을 보인다는 결과도 있다.32 결국 YOLOv4와 YOLOv5는 YOLO 계보가 두 개의 다른 생태계(Darknet vs. PyTorch)로 분기되는 중요한 분기점을 나타낸다고 볼 수 있다. YOLOv4가 Darknet 프레임워크의 기술적 최적화를 극한까지 끌어올렸다면, YOLOv5는 그 성능을 더 접근하기 쉬운 형태로 재해석하여 커뮤니티의 저변을 넓혔다.</p>
<table><thead><tr><th>특징</th><th>YOLOv4</th><th>YOLOv5</th></tr></thead><tbody>
<tr><td><strong>개발 프레임워크</strong></td><td>Darknet (C 기반)</td><td>PyTorch (Python 기반)</td></tr>
<tr><td><strong>Backbone</strong></td><td>CSPDarknet53</td><td>CSPDarknet53</td></tr>
<tr><td><strong>Neck</strong></td><td>SPP + PANet</td><td>PANet</td></tr>
<tr><td><strong>활성화 함수</strong></td><td>Mish</td><td>Leaky ReLU &amp; Sigmoid</td></tr>
<tr><td><strong>핵심 설계 철학</strong></td><td>Darknet 프레임워크 내에서의 성능 극한 최적화</td><td>사용자 편의성, 빠른 학습 및 배포 용이성</td></tr>
</tbody></table>
<h2>5.  YOLOv4의 영향력과 유산</h2>
<p>YOLOv4는 객체 탐지 기술의 역사에서 단순한 성능 향상을 넘어선 중요한 족적을 남겼다. 그 영향력과 유산은 기술적 성취와 방법론적 혁신, 그리고 생태계에 미친 파급 효과라는 세 가지 측면에서 조명할 수 있다.</p>
<p>첫째, YOLOv4는 최첨단 객체 탐지 기술의 **‘민주화’**를 이끌었다. 연구팀의 핵심 목표 중 하나는 고가의 다중 GPU 서버가 아닌, 널리 보급된 단일의 고성능 GPU에서도 학습하고 배포할 수 있는 모델을 만드는 것이었다.10 이 목표를 성공적으로 달성함으로써 YOLOv4는 소규모 연구 그룹, 스타트업, 개인 개발자들도 SOTA급 객체 탐지 기술에 접근할 수 있는 길을 열었다. 이는 기술의 진입 장벽을 낮추고, 더 다양한 분야에서 혁신적인 애플리케이션이 탄생할 수 있는 토양을 마련했다.</p>
<p>둘째, YOLOv4는 모델 개발에 대한 새로운 **‘청사진’**을 제시했다. 앞서 살펴본 ’Bag of Freebies’와 ’Bag of Specials’라는 개념적 프레임워크는 YOLOv4의 가장 중요한 방법론적 유산이다. 이는 딥러닝 연구가 반드시 완전히 새로운 구성 요소를 발명해야만 진보하는 것이 아님을 보여주었다. 대신, 기존에 제안된 수많은 기법들을 체계적으로 실험하고, 그 효과를 정량적으로 검증하며, 목표에 맞게 최적으로 조합하는 ’엔지니어-과학자’의 역할이 얼마나 중요한지를 역설했다. 이 방법론은 이후 수많은 응용 딥러닝 연구에 영감을 주며, 실용적이고 효율적인 모델 개발의 표준적인 접근법 중 하나로 자리 잡았다.</p>
<p>셋째, YOLOv4는 뛰어난 실시간 성능과 정확도를 바탕으로 다양한 <strong>실제 산업 현장</strong>에 빠르게 도입되었다.</p>
<ul>
<li><strong>무인 항공기(UAV):</strong> 속도와 정확도의 균형, 그리고 특히 원거리의 작은 객체를 탐지하는 데 강점을 보여 항공 감시 및 정찰 시스템에 효과적으로 적용되었다.33</li>
<li><strong>자율 주행:</strong> 차량, 보행자, 교통 표지판 등을 실시간으로 인식하는 자율 주행 시스템의 핵심 인식 모듈로 활용되어 안전성 향상에 기여했다.34</li>
<li><strong>산업 및 안전:</strong> 건설 현장의 안전모 착용 여부 감지 35, 공공장소의 보안 감시 36 등 정밀하고 빠른 판단이 요구되는 다양한 모니터링 작업에 널리 채택되었다.</li>
</ul>
<p>마지막으로, YOLOv4는 역설적으로 **‘Darknet 타이탄의 마지막’**으로서 YOLO 생태계의 패러다임 전환을 촉진하는 기폭제가 되었다. YOLOv4가 Darknet 프레임워크로 달성할 수 있는 성능의 정점을 보여주자, 시장과 커뮤니티에서는 이 정도의 성능을 더 사용하기 쉬운 프레임워크로 구현하고자 하는 강력한 요구가 발생했다. 이러한 요구에 부응하여 등장한 것이 바로 PyTorch 기반의 YOLOv5와 그 이후 버전들이다.3 즉, YOLOv4의 엄청난 성공이 오히려 Darknet 시대의 종말을 고하고, YOLO 생태계가 PyTorch라는 새로운 중심으로 이동하며 더욱 폭발적으로 성장하고 다각화되는 계기를 마련한 것이다. 이처럼 YOLOv4는 기술적 정점이자 변화의 촉매제로서 이중적인 역사적 의미를 지닌다.</p>
<h2>6.  결론</h2>
<p>YOLOv4는 객체 탐지 분야에서 단순한 점진적 개선을 넘어선 하나의 이정표를 세운 모델이다. 본 안내서에서 심층적으로 분석한 바와 같이, YOLOv4의 성공은 고도로 최적화된 <strong>CSPDarknet53-PANet-SPP 아키텍처</strong>와, 수많은 최신 기법들을 체계적으로 분류하고 적용한 **‘Bag of Freebies’ 및 ‘Bag of Specials’**라는 혁신적인 최적화 방법론에 기인한다.</p>
<p>이 모델의 가장 큰 성취는 속도와 정확도라는 상충하는 두 목표 사이에서 새로운 **파레토 최적 경계(Pareto frontier)**를 확립한 것이다. YOLOv4는 동시대의 어떤 실시간 탐지기보다도 우월한 속도-정확도 트레이드오프를 제공함으로써, 이후 등장하는 모든 실시간 객체 탐지기들이 따라야 할 새로운 벤치마크를 설정했다.</p>
<p>결론적으로 YOLOv4의 유산은 수많은 기술적 선택지 속에서 최적의 균형점을 찾아낸 ’장인적 엔지니어링(masterful engineering)’의 승리라 할 수 있다. 이는 강력한 성능, 실용적인 접근성, 그리고 체계적인 개발 방법론을 컴퓨터 비전 커뮤니티에 선물했으며, 객체 탐지 기술의 발전 방향에 깊고 지속적인 영향을 미쳤다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>History of YOLO: From YOLOv1 to YOLOv10 - Labelvisor, https://www.labelvisor.com/history-of-yolo-from-yolov1-to-yolov10/</li>
<li>YOLO Object Detection Explained: Evolution, Algorithm, and Applications - Encord, https://encord.com/blog/yolo-object-detection-guide/</li>
<li>YOLO Evolution: A Comprehensive Benchmark and Architectural Review of YOLOv12, YOLO11, and Their Previous Versions - arXiv, https://arxiv.org/html/2411.00201v2</li>
<li>YOLO Explained: From v1 to Present - Viso Suite, https://viso.ai/computer-vision/yolo-explained/</li>
<li>YOLOv4: Optimal Speed and Accuracy of Object Detection | Hacker News, https://news.ycombinator.com/item?id=22995427</li>
<li>[PDF] YOLOv4: Optimal Speed and Accuracy of Object Detection | Semantic Scholar, https://www.semanticscholar.org/paper/YOLOv4%3A-Optimal-Speed-and-Accuracy-of-Object-Bochkovskiy-Wang/2a6f7f0d659c5f7dcd665064b71e7b751592c80e</li>
<li>arXiv:2004.10934v1 [cs.CV] 23 Apr 2020, https://arxiv.org/abs/2004.10934</li>
<li>(Open Access) YOLOv4: Optimal Speed and Accuracy of Object Detection (2020) | Alexey Bochkovskiy | 12689 Citations - SciSpace, https://scispace.com/papers/yolov4-optimal-speed-and-accuracy-of-object-detection-mzs6seakj0</li>
<li>[2004.10934] YOLOv4: Optimal Speed and Accuracy of Object Detection - arXiv, https://arxiv.org/abs/2004.10934?sid=K1CjCk</li>
<li>(PDF) YOLOv4: Optimal Speed and Accuracy of Object Detection - ResearchGate, https://www.researchgate.net/publication/340883401_YOLOv4_Optimal_Speed_and_Accuracy_of_Object_Detection</li>
<li>Review — YOLOv4: Optimal Speed and Accuracy of Object Detection | by Sik-Ho Tsang, https://sh-tsang.medium.com/review-yolov4-optimal-speed-and-accuracy-of-object-detection-8198e5b37883</li>
<li>YOLOv4: High-Speed and Precise Object Detection - Ultralytics YOLO Docs, https://docs.ultralytics.com/models/yolov4/</li>
<li>Review of YOLOv4 Architecture. Paper, Original Code, PyTorch Code - Cenk Bircanoglu, https://cenk-bircanoglu.medium.com/review-of-yolov4-architecture-f488ec32c1c4</li>
<li>Comparing YOLOv3, YOLOv4 and YOLOv5 for Autonomous Landing Spot Detection in Faulty UAVs - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC8778480/</li>
<li>What is YOLOv4? A Detailed Breakdown. - Roboflow Blog, https://blog.roboflow.com/a-thorough-breakdown-of-yolov4/</li>
<li>YOLOv4 — Version 3: Proposed Workflow - Medium, https://medium.com/visionwizard/yolov4-version-3-proposed-workflow-e4fa175b902</li>
<li>YOLOv4 - Part 3: Bag of Specials | VisionWizard - Medium, https://medium.com/visionwizard/yolov4-version-2-bag-of-specials-fab1032b7fa0</li>
<li>Mish: A Self Regularized Non-Monotonic Neural Activation … - arXiv, https://arxiv.org/vc/arxiv/papers/1908/1908.08681v1.pdf</li>
<li>Mish As Neural Networks Activation Function - Sefik Ilkin Serengil, https://sefiks.com/2019/10/28/mish-as-neural-networks-activation-function/</li>
<li>Mish: A Self Regularized Activation Function [TF] - Kaggle, https://www.kaggle.com/code/samuelcortinhas/mish-a-self-regularized-activation-function-tf</li>
<li>YOLO-V4: CSPDARKNET, SPP, FPN, PANET, SAM || YOLO OBJECT DETECTION SERIES, https://www.youtube.com/watch?v=wMQEfzsARrw</li>
<li>A Comprehensive Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS - MDPI, https://www.mdpi.com/2504-4990/5/4/83</li>
<li>YOLO 1 through 5: A complete and detailed overview - Kaggle, https://www.kaggle.com/code/vikramsandu/yolo-1-through-5-a-complete-and-detailed-overview/notebook</li>
<li>YOLOv4 — Version 1: Bag of Freebies - Object Detection - Medium, https://medium.com/visionwizard/yolov4-bag-of-freebies-dc126623fc2d</li>
<li>Introduction to YOLOv4: Research review - Fritz ai, https://fritz.ai/introduction-to-yolov4-research-review/</li>
<li>Explanation of YOLO V4 a one stage detector | by Pierrick RUGERY | Becoming Human, https://becominghuman.ai/explaining-yolov4-a-one-stage-detector-cdac0826cbd7</li>
<li>GIoU, CIoU and DIoU: Variants of IoU and how they are better compared to IoU | by Abhishek Jain | Medium, https://medium.com/@abhishekjainindore24/giou-ciou-and-diou-variants-of-iou-and-how-they-are-better-compared-to-iou-4610a015643a</li>
<li>IoU Loss Functions for Faster &amp; More Accurate Object Detection, https://learnopencv.com/iou-loss-functions-object-detection/</li>
<li>YOLOv4: A Breakthrough in Real-Time Object Detection - arXiv, https://arxiv.org/html/2502.04161v1</li>
<li>Scaled YOLO v4 is the best neural network for object detection on …, https://alexeyab84.medium.com/scaled-yolo-v4-is-the-best-neural-network-for-object-detection-on-ms-coco-dataset-39dfa22fa982</li>
<li>What is YOLOv5? A Guide for Beginners. - Roboflow Blog, https://blog.roboflow.com/yolov5-improvements-and-evaluation/</li>
<li>(PDF) Comparative Performance Analysis of YOLOv4 and YOLOv5 …, https://www.researchgate.net/publication/372589174_Comparative_Performance_Analysis_of_YOLOv4_and_YOLOv5_Algorithms_on_Dangers_Objects</li>
<li>Evaluating YOLOv4 and YOLOv5 for Enhanced Object Detection in …, https://www.mdpi.com/2227-9717/13/1/254</li>
<li>The YOLO Framework: A Comprehensive Review of Evolution, Applications, and Benchmarks in Object Detection - MDPI, https://www.mdpi.com/2073-431X/13/12/336</li>
<li>Y.M. (2020) Yolov4 Optimal Speed and Accuracy of Object Detection. arXiv preprint arXiv 2004.10934. - References - Scientific Research Publishing, https://www.scirp.org/reference/referencespapers?referenceid=3308955</li>
<li>Enhancing Real-Time Object Detection: Implementing YOLOv4-Tiny with OpenCV, https://arounddatascience.com/blog/case-studies/enhancing-real-time-object-detection-implementing-yolov4-tiny-with-opencv/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>