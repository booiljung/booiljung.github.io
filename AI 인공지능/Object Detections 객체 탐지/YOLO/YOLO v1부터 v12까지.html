<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:YOLO v1부터 v12까지</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>YOLO v1부터 v12까지</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">객체 탐지 (Object Detection)</a> / <a href="index.html">YOLO 객체 탐지 모델</a> / <span>YOLO v1부터 v12까지</span></nav>
                </div>
            </header>
            <article>
                <h1>YOLO v1부터 v12까지</h1>
<h2>1. 서론</h2>
<p>컴퓨터 비전 분야에서 객체 탐지(Object Detection)는 이미지나 비디오 프레임 내에서 객체의 종류를 식별하고 그 위치를 찾아내는 핵심적인 과제이다.1 2010년대 중반까지 이 분야는 R-CNN(Regions with Convolutional Neural Networks) 계열로 대표되는 2-stage detector들이 주도하고 있었다.1 이들은 ’후보 영역 제안(Region Proposal)’과 ’분류(Classification)’라는 두 단계를 거쳐 높은 정확도를 달성했지만, 복잡한 파이프라인으로 인해 실시간 처리가 거의 불가능하다는 명백한 한계를 가지고 있었다.1 자율 주행, 실시간 감시, 로보틱스 등 즉각적인 반응이 필수적인 응용 분야가 부상하면서, 속도와 정확도를 동시에 만족시키는 새로운 패러다임의 필요성이 절실해졌다.</p>
<p>2015년, Joseph Redmon과 그의 동료들이 발표한 “You Only Look Once: Unified, Real-Time Object Detection” 논문은 이러한 시대적 요구에 대한 혁명적인 해답이었다.3 YOLOv1은 객체 탐지를 다단계 분류 문제가 아닌, 단일 회귀 문제(regression problem)로 재정의했다.3 이름 그대로, 단 한 번(Once)의 네트워크 연산만으로 이미지 전체를 보고(Look) 객체의 위치와 클래스를 동시에 예측하는 통합된 파이프라인을 제시한 것이다.1 이 혁신적인 접근법은 당시로서는 상상하기 어려웠던 45 FPS(Frames Per Second)라는 실시간 처리 속도를 가능하게 하며 객체 탐지 분야의 패러다임을 완전히 바꾸어 놓았다.3</p>
<p>YOLO의 등장은 단순한 성능 개선을 넘어, 실시간 객체 탐지 기술의 대중화를 이끌었다. 이후 10년에 가까운 시간 동안 YOLO는 수많은 연구자와 개발자들에 의해 계승되고 발전하며 거대한 기술 생태계를 형성했다. 본 안내서는 2015년 등장한 YOLOv1부터 시작하여 Redmon 시대의 완성작인 v2와 v3, 포스트-Redmon 시대의 혁신을 이끈 v4와 v5, 다양한 방향성을 모색한 v6와 v7, 그리고 현재의 기술 표준을 정립한 v8부터 최신 버전인 v11, v12에 이르기까지, 지난 10년간의 기술적 진화 과정을 연대기적으로 추적한다. 각 버전의 핵심 철학, 아키텍처의 변화, 최적화 전략, 그리고 그 이면에 담긴 기술적 통찰을 심층적으로 분석함으로써 YOLO라는 거대한 계보에 대한 통찰력 있는 고찰을 제공하는 것을 목표로 한다.1</p>
<h2>2.  창세기: YOLOv1 - 통일된 패러다임의 제시</h2>
<h3>2.1 핵심 철학: 객체 탐지를 회귀 문제로 재정의</h3>
<p>YOLOv1의 가장 근본적인 혁신은 객체 탐지에 대한 관점의 전환에 있다. 이전의 지배적인 방법론들은 분류기(classifier)를 탐지에 재활용(repurpose)하는 방식이었다.3 예를 들어, 슬라이딩 윈도우(sliding window) 방식은 이미지의 모든 위치와 스케일에서 분류기를 실행하여 객체를 찾았고, R-CNN 계열은 후보 영역을 먼저 제안한 뒤 각 영역에 대해 분류기를 실행했다.4 이러한 다단계 접근법은 복잡하고 비효율적이었다.</p>
<p>YOLO는 이 패러다임을 완전히 뒤집었다. 객체 탐지를 이미지 픽셀에서 시작하여 바운딩 박스 좌표(bounding box coordinates)와 클래스 확률(class probabilities)로 직접 이어지는 단일 회귀 문제로 프레임워크를 재구성한 것이다.3 이 철학은 “You Only Look Once“라는 이름에 그대로 담겨 있다. 시스템은 이미지를 단 한 번만 바라보고, 이미지 내에 어떤 객체들이 어디에 있는지를 한 번에 예측한다.1 이 통합된 접근 방식 덕분에 전체 탐지 파이프라인이 단일 신경망으로 구성될 수 있었고, 이는 종단간(end-to-end) 최적화를 가능하게 하여 탐지 성능에 대해 직접적으로 모델을 훈련시킬 수 있게 했다.3</p>
<p>이러한 구조적 통합과 종단간 최적화는 필연적으로 극적인 속도 향상이라는 결과를 낳았다. 다단계 파이프라인의 각 부분을 개별적으로 실행하고 결과를 조합하는 과정에서 발생하는 병목 현상이 원천적으로 제거되었기 때문이다. 이처럼 YOLOv1은 단순히 빠른 모델을 만든 것이 아니라, 객체 탐지를 바라보는 새로운 철학적 관점을 제시하며 1-stage detector라는 새로운 기술 계보의 문을 열었다.</p>
<h3>2.2 아키텍처: 통합 파이프라인과 그리드 셀 시스템</h3>
<p>YOLOv1의 통합 파이프라인은 ’그리드 셀(grid cell)’이라는 독창적인 시스템을 기반으로 작동한다. 전체 아키텍처는 다음과 같이 구성된다 8:</p>
<ol>
<li><strong>입력 이미지 분할</strong>: 입력 이미지를 S×S 크기의 그리드로 나눈다. 예를 들어 S=7이라면 이미지는 49개의 셀로 분할된다.</li>
<li><strong>그리드 셀의 역할</strong>: 어떤 객체의 중심점이 특정 그리드 셀 안에 위치하게 되면, 그 그리드 셀이 해당 객체를 탐지하는 역할을 맡게 된다.</li>
<li><strong>예측 요소</strong>: 각 그리드 셀은 세 가지 종류의 정보를 예측한다:</li>
</ol>
<ul>
<li><strong>바운딩 박스(Bounding Boxes)</strong>: 각 셀은 B개의 바운딩 박스를 예측한다. 각 박스는 5개의 값, 즉 박스의 중심 좌표 (x,y), 너비 (w), 높이 (h), 그리고 신뢰도 점수(confidence score)로 구성된다. 여기서 <span class="math math-inline">(x, y)</span>는 해당 그리드 셀 경계에 대한 상대적 위치이며, <span class="math math-inline">(w, h)</span>는 전체 이미지에 대한 상대적 크기이다.5</li>
<li><strong>신뢰도 점수(Confidence Score)</strong>: 이 점수는 해당 박스가 객체를 포함하고 있는지에 대한 확신과, 예측된 박스가 실제 객체와 얼마나 정확하게 일치하는지를 모두 반영한다. 수식으로는 <span class="math math-inline">Pr(\text{Object}) * IOU_{pred}^{truth}</span>로 정의된다. 만약 셀에 객체가 없다면 이 점수는 0이 되어야 한다.</li>
<li><strong>클래스 확률(Class Probabilities)</strong>: 각 그리드 셀은 C개의 조건부 클래스 확률, 즉 <span class="math math-inline">P(Class_i | \text{Object})</span>를 예측한다. 이는 해당 셀에 객체가 존재한다는 가정 하에, 그 객체가 각 클래스(Classi)에 속할 확률을 의미한다. 바운딩 박스의 수와 관계없이 셀당 하나의 클래스 확률 세트만 예측한다.5</li>
</ul>
<p>결과적으로, 이 모든 정보는 <span class="math math-inline">S×S×(B×5+C)</span> 크기의 단일 텐서(tensor)로 인코딩되어 네트워크의 최종 출력물이 된다. 예를 들어 PASCAL VOC 데이터셋(<span class="math math-inline">C=20</span>)에 대해 <span class="math math-inline">S=7</span>,<span class="math math-inline">B=2</span>로 설정하면, 최종 예측 텐서의 크기는 <span class="math math-inline">7×7×(2×5+20)=7×7×30</span>이 된다.</p>
<p>이 구조의 또 다른 중요한 특징은 전체 이미지를 한 번에 처리하여 각 예측이 전역적인 문맥 정보(global context)를 활용한다는 점이다.3 R-CNN 계열이 이미지의 일부 영역만을 보고 예측을 수행하는 것과 대조적으로, YOLO는 이미지 전체의 정보를 바탕으로 예측하기 때문에 배경을 객체로 잘못 탐지하는 배경 오류(background errors)에 훨씬 강한 모습을 보인다.3 그러나 이 그리드 셀 시스템은 YOLOv1의 속도와 단순성의 원천인 동시에, 내재적인 한계의 원인이기도 했다. 각 셀이 제한된 수의 바운딩 박스와 단 하나의 클래스만 예측할 수 있다는 제약은, 여러 작은 객체가 한 셀에 밀집해 있을 때 필연적으로 일부 객체를 놓치는 문제를 야기했다.6 이 필연적 한계는 후속 버전들이 앵커 박스나 다중 스케일 예측과 같은 새로운 기술을 도입해야만 했던 근본적인 동력이 되었다.</p>
<h3>2.3 기반이 되는 손실 함수 분석</h3>
<p>YOLOv1의 손실 함수는 모델이 세 가지 목표, 즉 위치 정확도(localization), 객체 존재 신뢰도(confidence), 그리고 분류 정확도(classification)를 동시에 학습하도록 설계된 다중 작업 손실(multi-part loss)이다. 모든 오차는 계산이 간단한 제곱합 오차(sum-squared error, SSE)를 기반으로 하지만, 각 작업의 중요도와 특성을 고려하여 몇 가지 가중치와 조정 장치를 포함한다.9</p>
<p>전체 손실 함수는 다음과 같이 표현될 수 있다:<br />
$$<br />
\lambda_{coord}\sum_{i=0}<sup>{S^2}\sum_{j=0}</sup>{B}\mathbb{I}_{ij}^{obj}\left [ (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2 \right ] \</p>
<ul>
<li>\lambda_{coord}\sum_{i=0}<sup>{S^2}\sum_{j=0}</sup>{B}\mathbb{I}_{ij}^{obj}\left [ (\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2 \right ] \</li>
<li>\sum_{i=0}<sup>{S^2}\sum_{j=0}</sup>{B}\mathbb{I}_{ij}^{obj}(C_i - \hat{C}_i)^2 \</li>
<li>\lambda_{noobj}\sum_{i=0}<sup>{S^2}\sum_{j=0}</sup>{B}\mathbb{I}_{ij}^{noobj}(C_i - \hat{C}_i)^2 \</li>
<li>\sum_{i=0}<sup>{S^2}\mathbb{I}_{i}</sup>{obj}\sum_{c\in classes}(p_i(c) - \hat{p}_i(c))^2<br />
$$<br />
이 복잡한 수식은 다음과 같은 다섯 부분으로 구성된다:</li>
</ul>
<ol>
<li><strong>중심점 위치 손실 (Localization Loss - Coordinates)</strong>: 첫 번째 줄은 예측된 바운딩 박스의 중심점 <span class="math math-inline">(x, y)</span>과 실제 값 <span class="math math-inline">(\hat{x},\hat{y})</span> 사이의 오차를 계산한다.</li>
<li><strong>크기 위치 손실 (Localization Loss - Dimensions)</strong>: 두 번째 줄은 박스의 너비 <span class="math math-inline">(w)</span>와 높이 <span class="math math-inline">(h)</span>의 오차를 계산한다. 여기서 중요한 점은 너비와 높이에 직접 오차를 적용하는 대신, 제곱근을 취한 값 <span class="math math-inline">(\sqrt{w}, \sqrt{h})</span>에 오차를 적용한다는 것이다. 이는 큰 박스에서의 작은 오차와 작은 박스에서의 작은 오차가 손실에 미치는 영향을 동등하게 만들려는 시도이다. 제곱근을 사용함으로써, 큰 박스의 편차보다 작은 박스의 편차에 더 큰 페널티를 부여하게 된다.</li>
<li><strong>객체 존재 신뢰도 손실 (Confidence Loss - Object)</strong>: 세 번째 줄은 객체가 존재한다고 판단되는(“responsible”) 예측 박스의 신뢰도 점수 <span class="math math-inline">C_i</span>에 대한 손실이다.</li>
<li><strong>객체 부재 신뢰도 손실 (Confidence Loss - No Object)</strong>: 네 번째 줄은 객체가 존재하지 않는 대부분의 박스에 대한 신뢰도 손실이다. 이미지 내 대부분의 그리드 셀은 객체를 포함하지 않으므로, 이들의 신뢰도 점수는 <span class="math math-inline">0</span>에 가깝게 훈련되어야 한다. 이 손실 항이 전체 손실을 지배하지 않도록 가중치 <span class="math math-inline">λ_{noobj}</span> (논문에서는 0.5)를 곱하여 그 영향을 줄인다.</li>
<li><strong>분류 손실 (Classification Loss)</strong>: 마지막 줄은 객체가 존재하는 그리드 셀에 대해서만 클래스 예측 확률 <span class="math math-inline">p_i(c)</span>의 오차를 계산한다.</li>
</ol>
<p>여기서 사용된 기호들의 의미는 다음과 같다:</p>
<ul>
<li><span class="math math-inline">\mathbb I_i^{obj}</span>: 그리드 셀 i에 객체가 존재하면 1, 아니면 0인 표시 함수.</li>
<li><span class="math math-inline">\mathbb I _{ij}^{obj}</span>: 그리드 셀 i의 j번째 바운딩 박스 예측기가 해당 객체 예측에 “책임(responsible)“이 있으면 1, 아니면 0. “책임“은 해당 셀의 예측 박스들 중 실제 박스와 가장 높은 IOU(Intersection over Union)를 갖는 박스에게 주어진다.</li>
<li><span class="math math-inline">\mathbb I _{ij}^{noobj}</span>: <span class="math math-inline">\mathbb{I}_{ij}^{obj}</span>가 0일 때 1인 함수.</li>
<li><span class="math math-inline">\lambda_{coord}</span>와 <span class="math math-inline">λ_{noobj}</span>: 각각 위치 손실과 객체 부재 신뢰도 손실의 가중치를 조절하는 하이퍼파라미터. 논문에서는 <span class="math math-inline">λ_{coord}=5</span>, <span class="math math-inline">λ_{noobj}=0.5</span>를 사용했다. 이는 위치 정확도를 신뢰도보다 더 중요하게 여기고, 배경 예측의 영향을 줄이려는 의도를 반영한다.</li>
</ul>
<h3>2.4 초기 모델의 성능과 명확한 한계점</h3>
<p>YOLOv1은 발표 당시 성능 면에서 매우 인상적인 결과를 보여주었다. 베이스 모델은 45 FPS, 그리고 네트워크 크기를 줄인 Fast YOLO는 무려 155 FPS의 처리 속도를 기록하며 실시간 객체 탐지의 새로운 기준을 제시했다.3 이는 당시 다른 실시간 탐지기보다 두 배 이상 높은 mAP(mean Average Precision)를 달성한 결과였다.4</p>
<p>하지만 이러한 전례 없는 속도는 정확도와의 트레이드오프 관계에 있었다. YOLOv1은 다음과 같은 명확한 한계점을 가지고 있었다:</p>
<ol>
<li><strong>정확도 문제</strong>: Fast R-CNN과 같은 최신 2-stage detector에 비해 전반적인 정확도가 낮았으며, 특히 객체의 위치를 정확하게 특정하는 위치 정확도(localization accuracy)에서 더 많은 오류를 보였다.3</li>
<li><strong>작은 객체 탐지의 어려움</strong>: 그리드 시스템의 구조적 한계로 인해, 특히 여러 개가 모여 있는 작은 객체들을 탐지하는 데 어려움을 겪었다. 예를 들어, 새 떼와 같이 작은 객체들이 하나의 그리드 셀에 몰려 있을 경우, 셀당 하나의 클래스만 예측할 수 있는 제약 때문에 일부 객체를 놓치게 된다.6</li>
<li><strong>일반화 성능의 한계</strong>: 훈련 데이터에서 보지 못했던 새로운 종횡비(aspect ratio)나 형태를 가진 객체에 대한 일반화 성능이 상대적으로 낮았다. 이는 모델이 바운딩 박스를 데이터로부터 학습하기보다 특정 패턴에 의존하는 경향이 있었기 때문이다.</li>
</ol>
<p>이러한 한계점들은 YOLOv1이 해결해야 할 명확한 과제를 제시했으며, 이는 후속 버전인 YOLOv2와 YOLOv3에서 앵커 박스 도입, 다중 스케일 예측 등 핵심적인 개선 사항들이 등장하는 직접적인 배경이 되었다.</p>
<h2>3.  Redmon 시대의 완성: 점진적 개선 (YOLOv2 &amp; YOLOv3)</h2>
<p>YOLOv1이 제시한 혁신적인 패러다임은 Joseph Redmon 자신에 의해 v2와 v3로 이어지며 완성도를 높여갔다. 이 시기는 YOLOv1의 명확한 한계들을 극복하고, 당시 최신 기술들을 실용적으로 융합하여 속도와 정확도의 균형을 극한으로 끌어올리는 데 집중한 ’점진적 완성’의 시대였다.</p>
<h3>3.1 YOLOv2 (YOLO9000): 더 좋게, 더 빠르게, 더 강하게</h3>
<p>YOLOv2는 “Better, Faster, Stronger“라는 부제에 걸맞게 YOLOv1의 거의 모든 측면을 개선했다.10 주요 개선 사항은 정확도(Better), 속도(Faster), 그리고 탐지 가능한 클래스 수(Stronger)의 확장에 초점을 맞추었다.</p>
<h4>3.1.1 아키텍처의 도약: Darknet-19</h4>
<p>YOLOv2는 특징 추출을 위해 새로운 백본 네트워크인 <strong>Darknet-19</strong>를 도입했다.12 당시 많은 탐지 모델들이 VGG-16과 같이 무겁고 복잡한 네트워크를 사용했던 것과 달리, Darknet-19는 효율성에 중점을 두고 설계되었다.</p>
<ul>
<li><strong>구조</strong>: 19개의 컨볼루션 레이어와 5개의 맥스풀링 레이어로 구성된다.12 VGG 모델과 유사하게 주로 3x3 필터를 사용하고 풀링 단계마다 채널 수를 두 배로 늘리는 방식을 따르면서도, Network in Network (NIN)에서 영감을 받아 1x1 컨볼루션을 사용하여 특징 표현을 압축하고 파라미터 수를 줄였다.12</li>
<li><strong>배치 정규화 (Batch Normalization)</strong>: 모든 컨볼루션 레이어 뒤에 배치 정규화를 적용했다. 이는 훈련을 안정화시키고 수렴 속도를 높이며, 드롭아웃(dropout) 없이도 모델을 정규화하는 효과를 가져왔다.12 이 간단한 기법만으로도 mAP가 2% 이상 향상되었다.</li>
</ul>
<h4>3.1.2 핵심 혁신: 앵커 박스와 차원 클러스터링</h4>
<p>YOLOv1의 가장 큰 약점 중 하나는 부정확한 위치 예측이었다. YOLOv2는 이 문제를 해결하기 위해 당시 2-stage detector인 Faster R-CNN의 핵심 아이디어인 **앵커 박스(Anchor Boxes)**를 도입했다.11</p>
<ul>
<li><strong>앵커 박스 도입</strong>: YOLOv1이 바운딩 박스의 좌표를 직접 예측하던 방식에서 벗어나, 사전에 정의된 다양한 형태의 앵커 박스로부터의 오프셋(offset)을 예측하는 방식으로 변경했다. 이를 통해 모델은 위치 예측이라는 어려운 문제를 더 쉬운 문제로 변환할 수 있었고, 리콜(recall)을 크게 향상시켰다.</li>
<li><strong>차원 클러스터링 (Dimension Clusters)</strong>: 하지만 Redmon은 단순히 앵커 박스를 차용하는 데 그치지 않았다. 앵커 박스의 크기와 종횡비를 수동으로 설정하는 대신, 훈련 데이터셋에 있는 실제 바운딩 박스들의 형태를 분석하여 최적의 앵커 박스를 자동으로 찾아내는 <strong>k-평균 클러스터링(k-means clustering)</strong> 기법을 제안했다.12 이때, 유클리드 거리 대신 IOU를 거리 측정 기준으로 사용(<span class="math math-inline">d(box,centroid)=1−IOU(box,centroid)</span>)하여 박스의 크기와 무관하게 클러스터링이 잘 되도록 했다. 이 데이터 기반 접근법은 모델이 더 쉽게 학습을 시작할 수 있도록 도와주었다.</li>
</ul>
<h4>3.1.3 훈련 기법 고도화: 직접 위치 예측 및 다중 스케일 훈련</h4>
<ul>
<li><strong>직접 위치 예측 (Direct Location Prediction)</strong>: 앵커 박스를 사용하면 훈련 초기에 모델이 불안정해지는 문제가 발생할 수 있다. 이를 완화하기 위해 YOLOv2는 예측된 오프셋의 범위를 제한했다. 바운딩 박스의 중심점 좌표는 시그모이드(sigmoid) 함수를 통과시켜 해당 그리드 셀 내부(0~1)로 값이 제한되도록 했다. 이는 모델의 안정성을 높이고 학습을 용이하게 만들었다.12</li>
<li><strong>다중 스케일 훈련 (Multi-Scale Training)</strong>: YOLOv2는 완전 연결 계층(fully connected layer)이 없기 때문에 다양한 크기의 입력 이미지에 대응할 수 있다. 이 점을 활용하여, 훈련 중에 10 배치마다 입력 이미지의 해상도를 {320x320, 352x352,…, 608x608} 사이에서 무작위로 변경했다.12 이 기법은 모델이 다양한 스케일의 객체를 잘 탐지하도록 강제하여 모델의 강건성(robustness)을 크게 향상시켰다.</li>
</ul>
<h4>3.1.4 바운딩 박스 예측의 수학적 공식</h4>
<p>YOLOv2의 바운딩 박스 예측은 다음 수식으로 표현된다. 여기서 tx,ty,tw,th,to는 네트워크의 직접적인 출력값이고, <span class="math math-inline">(c_x, c_y)</span>는 그리드 셀의 좌상단 좌표, <span class="math math-inline">(p_w, p_h)</span>는 앵커 박스의 너비와 높이를 나타낸다.12<br />
<span class="math math-display">
b_x = \sigma(t_x) + c_x \\
b_y = \sigma(t_y) + c_y \\
b_w = p_w e^{t_w} \\
b_h = p_h e^{t_h} \\
Pr(\text{object}) * IOU(b, \text{object}) = \sigma(t_o)
</span></p>
<h4>3.1.5 YOLO9000</h4>
<p>YOLOv2 논문은 <strong>YOLO9000</strong>이라는 확장된 모델도 함께 제시했다. 이는 COCO와 같은 탐지 데이터셋과 ImageNet과 같은 대규모 분류 데이터셋을 동시에 활용하여 훈련하는 혁신적인 방법을 제안했다. WordNet을 기반으로 계층적인 트리 구조(WordTree)를 만들어, 탐지 데이터에 레이블이 없는 클래스에 대해서도 일반화하여 예측할 수 있게 했다. 그 결과, YOLO9000은 9000개가 넘는 방대한 수의 객체 클래스를 실시간으로 탐지할 수 있었다.10</p>
<h3>3.2 YOLOv3: 의미 있는 점진적 향상</h3>
<p>YOLOv3는 Redmon이 자신의 블로그 포스트에서 “점진적 개선(An Incremental Improvement)“이라고 겸손하게 표현했듯이, 혁명적인 변화보다는 v2의 성공을 바탕으로 당시의 최신 기술들을 통합하여 완성도를 높이는 데 집중했다.14 하지만 이 ’점진적 개선’들은 YOLO를 SOTA(State-of-the-Art) 수준의 정확도를 갖춘 실시간 탐지기로 올려놓았고, 이후 등장하는 모든 YOLO 모델의 원형(archetype)을 정립했다는 점에서 매우 중요한 의미를 갖는다.</p>
<h4>3.2.1 백본의 진화: Darknet-53</h4>
<p>YOLOv3는 특징 추출을 위해 더 깊고 강력한 <strong>Darknet-53</strong> 백본을 도입했다.16</p>
<ul>
<li><strong>구조</strong>: Darknet-19에 ResNet의 핵심 아이디어인 <strong>잔차 연결(residual connections)</strong> 또는 숏컷 연결(shortcut connections)을 결합하여 총 53개의 컨볼루션 레이어로 구성했다. 이를 통해 그래디언트 소실 문제없이 훨씬 깊은 네트워크를 훈련할 수 있게 되었고, 특징 추출 능력이 크게 향상되었다.</li>
<li><strong>성능</strong>: Darknet-53은 당대 최고의 백본이었던 ResNet-101보다 1.5배 빠르면서도 동등한 분류 성능을 보였고, ResNet-152와는 비슷한 성능을 보이면서도 2배나 효율적이었다. 이는 ResNet의 과도한 레이어 수에 비해 Darknet-53의 구조가 GPU 활용에 더 최적화되었기 때문이다.16</li>
</ul>
<h4>3.2.2 탐지 능력 강화: 다중 스케일 예측</h4>
<p>YOLOv1과 v2의 가장 큰 약점 중 하나는 작은 객체 탐지 성능이었다. YOLOv3는 이 문제를 해결하기 위해 <strong>다중 스케일 예측(Predictions Across Scales)</strong> 개념을 도입했다.16</p>
<ul>
<li><strong>FPN 스타일의 구조</strong>: Feature Pyramid Networks(FPN)와 유사한 방식으로, 네트워크의 서로 다른 깊이에서 3개의 다른 스케일(scale)의 특징 맵을 추출하여 예측을 수행했다. 깊은 레이어의 특징 맵(예: 13x13)은 큰 객체를, 얕은 레이어의 특징 맵(예: 52x52)은 작은 객체를 탐지하는 데 사용되었다.</li>
<li><strong>특징 융합</strong>: 얕은 레이어의 세밀한 정보(fine-grained features)와 깊은 레이어의 의미론적 정보(semantic features)를 업샘플링과 연결(concatenation)을 통해 융합함으로써, 각 스케일의 예측이 더 풍부한 정보를 활용할 수 있도록 했다.16 이 구조는 작은 객체 탐지 성능을 극적으로 향상시키는 결정적인 역할을 했다.</li>
<li><strong>앵커 박스 할당</strong>: k-평균 클러스터링으로 찾은 총 9개의 앵커 박스를 3개의 스케일에 각각 3개씩 할당하여 사용했다.</li>
</ul>
<h4>3.2.3 분류 방식의 개선: 독립적인 로지스틱 분류기</h4>
<p>YOLOv3는 클래스 예측 방식도 개선했다. 기존의 소프트맥스(softmax) 함수는 각 박스가 단 하나의 클래스에만 속한다고 가정하기 때문에, ’사람(Person)’과 ’여성(Woman)’처럼 중첩되는 레이블을 가진 데이터셋(예: Open Images Dataset)에 적용하기 어렵다. YOLOv3는 이 문제를 해결하기 위해 소프트맥스 대신 **독립적인 로지스틱 분류기(independent logistic classifiers)**를 사용했다.16</p>
<ul>
<li><strong>다중 레이블 분류</strong>: 각 클래스에 대해 독립적인 이진 분류를 수행함으로써 하나의 객체가 여러 개의 레이블을 가질 수 있도록 했다. 예를 들어, 한 객체에 대해 ’사람’과 ‘여성’ 클래스 모두에 대해 높은 확률을 예측할 수 있다.</li>
<li><strong>손실 함수</strong>: 훈련 시에는 클래스 예측에 대해 **이진 교차 엔트로피 손실(binary cross-entropy loss)**을 사용했다. 이 변화는 모델이 더 복잡하고 현실적인 데이터 분포를 잘 모델링할 수 있게 해주었다.</li>
</ul>
<p>이러한 개선들을 통해 YOLOv3는 속도를 거의 희생하지 않으면서도 RetinaNet과 같은 SOTA 모델들과 견줄 만한 정확도를 달성했다.14 더 중요한 것은, Darknet-53 백본, FPN 스타일의 넥, 다중 스케일 예측 헤드로 구성된 이 3단 구조가 이후 등장하는 YOLOv4, v5, v7, v8 등 거의 모든 후속 YOLO 모델들의 기본 골격, 즉 **현대 YOLO의 원형(Archetype)**을 정립했다는 점이다. Redmon의 시대는 YOLOv3를 통해 기술적 정점에 도달하며 막을 내렸고, 이후의 YOLO 생태계는 이 견고한 토대 위에서 꽃피우기 시작했다.</p>
<h2>4.  포스트-Redmon 시대: 혁신의 캄브리아기 (YOLOv4 &amp; YOLOv5)</h2>
<p>Joseph Redmon이 YOLOv3를 끝으로 연구를 중단한 이후, YOLO의 개발은 새로운 국면을 맞이했다. 2020년 4월, YOLOv4와 YOLOv5가 거의 동시에 등장하며 포스트-Redmon 시대의 서막을 열었다. 이 두 버전은 YOLO의 철학을 계승하면서도, 혁신에 대한 서로 다른 접근 방식을 보여주며 YOLO 생태계에 폭발적인 다양성을 가져왔다. 이 시기는 마치 생명체의 종이 폭발적으로 증가한 캄브리아기처럼, 수많은 아이디어와 기술들이 경쟁하고 융합하며 YOLO를 한 단계 더 높은 수준으로 끌어올렸다.</p>
<h3>4.1 YOLOv4: 속도와 정확도의 최적 균형</h3>
<p>YOLOv4는 YOLOv3의 원저자와는 다른 연구팀(Alexey Bochkovskiy 등)에 의해 개발되었으며, Redmon의 공식 Darknet 프레임워크를 계승했다.17 YOLOv4의 핵심 철학은 새로운 아키텍처를 발명하기보다는, 당시 존재하던 수많은 딥러닝 기술들을 체계적으로 실험하고 조합하여 “실시간 객체 탐지기의 속도와 정확도를 최적화“하는 것이었다.18</p>
<h4>4.1.1 체계적인 최적화: Bag of Freebies와 Bag of Specials</h4>
<p>YOLOv4의 가장 큰 공헌은 수많은 기술들을 **“Bag of Freebies (BoF)”**와 **“Bag of Specials (BoS)”**라는 두 가지 범주로 나누어 체계적으로 분석하고 최적의 조합을 찾아낸 것이다.18</p>
<ul>
<li><strong>Bag of Freebies (BoF)</strong>: 훈련 전략이나 데이터 증강 기법과 같이, <strong>추론 비용(inference cost)을 증가시키지 않으면서</strong> 모델의 정확도를 향상시키는 방법들의 모음이다. YOLOv4는 다음과 같은 BoF 기법들을 적극적으로 활용했다 19:</li>
<li><strong>데이터 증강 (Data Augmentation)</strong>:</li>
<li><strong>Mosaic</strong>: 4개의 훈련 이미지를 하나로 합쳐서 모델을 훈련시키는 기법. 이는 모델이 일반적인 맥락을 벗어난 객체도 탐지하도록 강제하고, 작은 객체 탐지 성능을 향상시킨다. 또한, 큰 배치 크기 없이도 배치 정규화가 효과적으로 작동하도록 돕는다.</li>
<li><strong>CutMix</strong>, <strong>Random Erase</strong>, <strong>Hide-and-Seek</strong> 등 다양한 증강 기법.</li>
<li><strong>Self-Adversarial Training (SAT)</strong>: 훈련 과정에서 모델 자신에게 적대적인(adversarial) 예시를 생성하여 강건성을 높이는 기법.</li>
<li><strong>DropBlock Regularization</strong>: 특징 맵의 연속적인 영역을 제거하여 과적합을 방지하는 정규화 기법.</li>
<li><strong>Bag of Specials (BoS)</strong>: <strong>추론 비용을 약간 증가시키지만 정확도를 크게 향상시키는</strong> 플러그인 모듈이나 후처리 방법들의 모음이다.19</li>
<li><strong>Mish Activation</strong>: ReLU보다 부드러운 활성화 함수로, 약간의 계산 비용 증가는 있지만 더 나은 정보 흐름을 통해 정확도를 개선한다.</li>
<li><strong>SPP (Spatial Pyramid Pooling) 블록</strong>: 다양한 크기의 풀링을 통해 수용 필드(receptive field)를 크게 확장하고, 중요한 컨텍스트 특징을 분리한다.</li>
<li><strong>SAM (Spatial Attention Module)</strong>: 공간적 어텐션 메커니즘을 도입하여 중요한 특징에 집중하도록 돕는다.</li>
<li><strong>PANet (Path Aggregation Network)</strong>: 특징 융합을 위한 경로를 추가하여 저수준 특징과 고수준 특징 간의 정보 흐름을 강화한다.</li>
</ul>
<p>이러한 체계적인 접근은 마치 최고의 부품들을 모아 최적의 성능을 내는 F1 머신을 조립하는 과정과 같았다. 이는 YOLOv4가 단일 혁신에 의존하기보다, 수많은 검증된 기술들의 시너지를 통해 성능을 극대화하는 ’체계적 공학(Systematic Engineering)’의 정수를 보여준다.</p>
<h4>4.1.2 아키텍처 조합: CSPDarknet53, SPP, PANet</h4>
<p>YOLOv4는 YOLOv3의 기본 구조(백본-넥-헤드)를 유지하면서 각 부분을 BoS 기법들로 강화했다.18</p>
<ul>
<li><strong>Backbone</strong>: <strong>CSPDarknet53</strong>. Darknet-53에 Cross Stage Partial (CSP) 연결을 적용하여, 그래디언트 정보 흐름을 개선하고 연산 병목 현상을 줄여 정확도를 높이면서도 계산량을 감소시켰다.</li>
<li><strong>Neck</strong>: <strong>SPP</strong> 블록과 <strong>PANet</strong>을 결합했다. SPP는 백본의 마지막 특징 맵 위에 위치하여 수용 필드를 확장하고, PANet은 YOLOv3의 FPN 구조를 개선하여 상향식(bottom-up) 경로를 추가함으로써 특징 융합 능력을 극대화했다.</li>
<li><strong>Head</strong>: YOLOv3와 동일한 앵커 기반(anchor-based) 헤드를 사용했다.</li>
</ul>
<h4>4.1.3 손실 함수의 발전: CIoU Loss의 도입</h4>
<p>바운딩 박스 회귀의 정확도를 높이기 위해, YOLOv4는 <strong>Complete-IoU (CIoU) Loss</strong>를 채택했다.18 기존의 IoU Loss는 박스가 겹치지 않으면 그래디언트가 0이 되는 문제가 있었고, GIoU Loss는 수렴이 느린 문제가 있었다. CIoU Loss는 이 문제들을 해결하기 위해 세 가지 중요한 기하학적 요소를 모두 고려한다 20:</p>
<ol>
<li><strong>겹침 영역 (Overlap Area)</strong>: IoU</li>
<li><strong>중심점 거리 (Central Point Distance)</strong>: 예측 박스와 실제 박스 중심점 간의 거리</li>
<li><strong>종횡비 (Aspect Ratio)</strong>: 두 박스의 종횡비의 일관성</li>
</ol>
<p>CIoU Loss의 수식은 다음과 같다 20:<br />
<span class="math math-display">
L_{CIoU} = 1 - IoU + \frac{\rho^2(b, b^{gt})}{c^2} + \alpha v
</span></p>
<ul>
<li><span class="math math-inline">IoU</span>: 예측 박스(<span class="math math-inline">b</span>)와 실제 박스(<span class="math math-inline">b^{gt}</span>)의 Intersection over Union.</li>
<li><span class="math math-inline">ρ^2(b,b^{gt})</span>: 두 박스 중심점 간의 유클리드 거리 제곱.</li>
<li><span class="math math-inline">c</span>: 두 박스를 모두 포함하는 가장 작은 볼록 박스(convex hull)의 대각선 길이 제곱.</li>
<li>α: 가중치 파라미터.</li>
<li><span class="math math-inline">v</span>: 종횡비의 일관성을 측정하는 항으로, <span class="math math-inline">v=π24(\arctan h^{gt} w^{gt} − \arctan hw)^2</span>로 정의된다.</li>
</ul>
<p>이 손실 함수는 더 빠르고 안정적인 수렴을 가능하게 하여 바운딩 박스 회귀 성능을 크게 향상시켰다.</p>
<h3>4.2 YOLOv5: 개발자 친화적 생태계의 구축</h3>
<p>YOLOv4가 발표된 지 불과 몇 주 후, Ultralytics의 Glenn Jocher는 완전히 새로운 YOLOv5를 GitHub에 공개했다.21 YOLOv5는 공식 논문 없이 배포되었으며, YOLOv4와는 전혀 다른 철학을 기반으로 했다. YOLOv5의 혁신은 mAP 수치 자체보다, 기술의 **‘접근성’**과 **‘사용성’**에 초점을 맞춘 **‘개발자 생태계 구축’**에 있었다.</p>
<h4>4.2.1 프레임워크 전환: Ultralytics의 PyTorch 기반 구현</h4>
<p>YOLOv5의 가장 큰 변화는 C로 작성된 Darknet 프레임워크에서 벗어나, 당시 연구와 산업계에서 가장 널리 사용되던 <strong>PyTorch</strong> 프레임워크로 완전히 재구현되었다는 점이다.21</p>
<ul>
<li><strong>접근성 및 편의성</strong>: PyTorch로의 전환은 수많은 개발자들이 YOLO를 더 쉽게 설치하고, 수정하고, 실험할 수 있게 만들었다. 복잡한 C 코드 대신 파이썬 기반의 직관적인 코드는 진입 장벽을 크게 낮췄다.24</li>
<li><strong>빠른 개발 주기와 커뮤니티</strong>: GitHub를 중심으로 한 오픈소스 프로젝트로 운영되면서, 버그 수정, 새로운 기능 추가, 사용자 피드백 반영이 매우 신속하게 이루어졌다. 이는 활발한 커뮤니티를 형성하는 기반이 되었고, YOLOv5를 살아있는 프로젝트로 만들었다.22</li>
</ul>
<h4>4.2.2 핵심 특징: 모델 스케일링과 자동 앵커</h4>
<p>YOLOv5는 사용자의 편의성을 극대화하는 실용적인 기능들을 도입했다.</p>
<ul>
<li><strong>모델 스케일링 (Model Scaling)</strong>: **Nano(n), Small(s), Medium(m), Large(l), Extra-large(x)**와 같이 미리 정의된 다양한 크기의 모델을 제공했다.21 사용자는 자신의 하드웨어 사양이나 애플리케이션의 요구사항(예: 속도 우선 vs. 정확도 우선)에 따라 최적의 모델을 간단히 선택할 수 있었다. 이 스케일링은 네트워크의 깊이(depth_multiple)와 너비(width_multiple)를 조절하여 이루어진다.</li>
<li><strong>자동 앵커 생성 (Auto-Anchor)</strong>: YOLOv2에서 도입된 k-평균 클러스터링을 한 단계 더 발전시켰다. YOLOv5는 훈련을 시작하기 전에 사용자의 커스텀 데이터셋에 있는 라벨들을 분석하여, <strong>k-평균 클러스터링과 유전 알고리즘(Genetic Algorithm)을 결합</strong>하여 최적의 앵커 박스를 자동으로 계산하고 조정한다.26 이 ‘Auto-Anchor’ 기능은 사용자가 앵커 박스에 대해 전혀 신경 쓰지 않아도 되게 만들어, 모델 훈련 과정을 극도로 단순화시켰다.</li>
</ul>
<h4>4.2.3 손실 함수의 구성 요소: BCE Loss와 CIoU Loss의 결합</h4>
<p>YOLOv5의 손실 함수는 세 가지 주요 구성 요소의 가중 합으로 이루어진다.28</p>
<ul>
<li><strong>위치 손실 (Location/Box Loss)</strong>: 바운딩 박스 회귀를 위해 YOLOv4와 마찬가지로 기본적으로 <strong>CIoU Loss</strong>를 사용한다.</li>
<li><strong>분류 손실 (Classification Loss)</strong>: 클래스 예측을 위해 **BCEWithLogitsLoss (Binary Cross-Entropy with Logits Loss)**를 사용한다. 이는 YOLOv3처럼 다중 레이블 분류를 지원한다.</li>
<li><strong>객체성 손실 (Objectness Loss)</strong>: 특정 위치에 객체가 있는지 없는지를 예측하는 데에도 <strong>BCEWithLogitsLoss</strong>를 사용한다.</li>
</ul>
<p>특히, 객체성 손실은 3개의 다른 스케일에서 예측을 수행하는 각 예측 레이어(P3, P4, P5)에 대해 서로 다른 가중치(기본값: <code>[4.0, 1.0, 0.4]</code>)를 적용하여 균형을 맞춘다. 이는 작은 객체를 탐지하는 고해상도 레이어의 손실에 더 큰 가중치를 부여하여, 모델이 모든 스케일에서 안정적으로 학습하도록 돕는다.28</p>
<p>YOLOv4와 v5의 등장은 ’YOLO’라는 브랜드가 더 이상 단일 저자의 창작물이 아니라, 그 철학을 계승하는 여러 갈래의 프로젝트를 포괄하는 ‘브랜드’ 또는 ’운동’이 되었음을 알리는 신호탄이었다.5 이로 인해 커뮤니티에서는 어떤 것이 ‘진정한’ 후속작인지에 대한 논쟁이 벌어지기도 했으며, 특히 YOLOv5가 채택한 AGPL-3.0 라이선스는 상업적 사용에 대한 고려사항을 낳는 등 기술 외적인 요소들이 중요하게 부상하기 시작했다.25</p>
<h2>5.  분화와 전문화: 다양한 방향성 모색 (YOLOv6 &amp; YOLOv7)</h2>
<p>YOLOv4와 v5가 열어젖힌 포스트-Redmon 시대는 곧바로 ‘다극화(multipolarization)’ 현상으로 이어졌다. Ultralytics가 주도하는 YOLOv5 생태계가 빠르게 확장되는 동안, 다른 연구 그룹과 기업들도 각자의 목표와 철학을 담아 YOLO의 새로운 버전을 발표하기 시작했다. 2022년에 등장한 YOLOv6와 YOLOv7은 이러한 분화와 전문화의 흐름을 명확하게 보여주는 대표적인 사례다. 이 시기는 마치 리눅스 커널에서 다양한 목적의 배포판이 파생되듯, YOLO라는 공통의 철학 위에서 다양한 기술적 시도들이 꽃피운 시기였다.</p>
<h3>5.1 YOLOv6: 산업용 애플리케이션을 위한 프레임워크</h3>
<p>YOLOv6는 중국의 거대 기술 기업인 메이투안(Meituan)의 연구팀에 의해 개발되었다.31 그들의 목표는 학술적인 SOTA 달성을 넘어, 실제 산업 현장에서의 **효율적인 배포(deployment)**에 최적화된 탐지기를 만드는 것이었다. 이 목표는 YOLOv6의 아키텍처 설계 전반에 깊이 반영되었다.</p>
<h4>5.1.1 하드웨어 친화적 설계: 재매개변수화와 EfficientRep 백본</h4>
<p>YOLOv6의 핵심 철학은 <strong>재매개변수화(Reparameterization)</strong> 기법의 적극적인 도입이다. 이는 RepVGG 논문에서 제안된 아이디어로, 훈련 시에는 복잡한 다중 브랜치(multi-branch) 구조를 사용하여 모델의 표현력을 극대화하고, 추론 시에는 이 구조를 수학적으로 등가인 단순한 단일 경로(single-path) 구조로 변환하여 하드웨어에서의 추론 속도를 극대화하는 기법이다.32</p>
<ul>
<li><strong>EfficientRep Backbone &amp; Rep-PAN Neck</strong>: YOLOv6는 이 재매개변수화 개념을 기반으로 새로운 <strong>EfficientRep 백본</strong>과 <strong>Rep-PAN 넥</strong>을 설계했다. 작은 모델(YOLOv6-N)의 경우, 훈련 시 RepBlock을 사용하고 추론 시에는 3x3 컨볼루션 스택(RepConv)으로 변환하여 GPU에서 높은 연산 밀도를 달성했다. 더 큰 모델의 경우, CSP 구조를 결합한 CSPStackRep 블록을 사용하여 성능과 효율성의 균형을 맞췄다.32 이 설계는 모델이 훈련 중에는 풍부한 특징을 학습하고, 실제 배포 환경에서는 최소한의 지연 시간으로 작동하도록 만들었다.</li>
</ul>
<h4>5.1.2 탐지 헤드 혁신: 앵커 프리와 분리된 헤드</h4>
<p>YOLOv6는 탐지 헤드에서도 과감한 혁신을 선택했다.</p>
<ul>
<li><strong>앵커 프리 (Anchor-Free)</strong>: YOLOv5까지 주류였던 앵커 기반 방식을 버리고, 사전 정의된 앵커 박스 없이 객체를 탐지하는 <strong>앵커 프리</strong> 방식을 전면적으로 도입했다.32 이는 앵커 박스 설계의 복잡성을 제거하고, 다양한 형태의 객체에 대한 일반화 성능을 높이며, NMS(Non-Maximum Suppression)와 같은 후처리 과정의 계산 비용을 줄여 전체적인 파이프라인 속도를 향상시키는 효과를 가져왔다.</li>
<li><strong>분리된 헤드 (Decoupled Head)</strong>: 분류(classification) 작업과 위치 회귀(regression) 작업을 별개의 네트워크 브랜치에서 처리하는 **효율적인 분리형 헤드(Efficient Decoupled Head)**를 사용했다.33 이 구조는 두 작업 간의 충돌을 줄여 각각의 성능을 최적화하는 데 도움을 준다.</li>
</ul>
<h4>5.1.3 손실 함수 혁신: Varifocal Loss와 Distribution Focal Loss</h4>
<p>정확도 향상을 위해 손실 함수도 개선되었다. 분류 손실로는 고품질 앵커에 더 집중하는 **Varifocal Loss (VFL)**를, 바운딩 박스 회귀 손실로는 박스 경계의 분포를 직접 학습하는 **Distribution Focal Loss (DFL)**를 채택하여 탐지 정확도를 한 단계 끌어올렸다.33</p>
<h3>5.2 YOLOv7: 훈련 가능한 Bag-of-Freebies</h3>
<p>YOLOv7은 YOLOv4를 개발했던 바로 그 저자 팀(Chien-Yao Wang 등)이 발표한 후속작이다.34 이들의 목표는 YOLOv4의 철학을 계승하여, 학술적인 SOTA 성능을 다시 한번 경신하는 것이었다. YOLOv7은 ’훈련 가능한 Bag-of-Freebies’라는 새로운 개념을 중심으로 아키텍처와 훈련 전략을 정교하게 설계했다.</p>
<h4>5.2.1 아키텍처의 진보: 확장된 ELAN (E-ELAN)</h4>
<p>YOLOv7의 백본은 **E-ELAN (Extended-ELAN)**이라는 새로운 네트워크 아키텍처를 기반으로 한다.35 ELAN(Efficient Layer Aggregation Network)은 네트워크의 깊이가 깊어져도 안정적인 학습이 가능하도록 그래디언트 경로를 효율적으로 관리하는 구조이다.</p>
<ul>
<li><strong>E-ELAN</strong>: YOLOv7은 기존 ELAN의 구조를 유지하면서도, 그룹 컨볼루션(group convolution)을 사용하여 계산 블록의 채널과 카디널리티(cardinality)를 확장했다. 확장된 특징 맵들을 셔플하고 다시 병합하는 과정을 통해, 네트워크가 원래의 효율적인 구조를 손상시키지 않으면서도 더 다양한 특징을 학습할 수 있도록 했다.35</li>
</ul>
<h4>5.2.2 모델 스케일링 철학: 확장 및 복합 스케일링</h4>
<p>YOLOv7은 연결 기반(concatenation-based) 아키텍처의 특성을 고려한 새로운 <strong>확장 및 복합 스케일링(Extended and Compound Scaling)</strong> 방법을 제안했다.35 일반적인 모델 스케일링은 네트워크의 깊이(depth), 너비(width), 해상도(resolution)를 독립적으로 조절하지만, ELAN과 같이 특징 맵을 연결하는 구조에서는 깊이를 스케일링하면 후속 레이어의 입력 채널 수가 변하게 된다. YOLOv7의 복합 스케일링은 이러한 상호 의존성을 고려하여 깊이와 너비를 함께 조절함으로써, 모델이 스케일링된 후에도 최적의 구조를 유지하도록 했다.</p>
<h4>5.2.3 Trainable Bag-of-Freebies</h4>
<p>YOLOv7의 핵심 개념은 **‘훈련 가능한 Bag-of-Freebies’**이다.35 이는 YOLOv4의 BoF 개념을 확장한 것으로, 훈련 비용(시간, 자원)은 증가시키지만 추론 비용은 전혀 증가시키지 않는 최적화 기법들을 의미한다. 여기에는 모델 재매개변수화, 동적 레이블 할당(dynamic label assignment), 보조 헤드(auxiliary head)를 사용한 심층 감독(deep supervision) 등 다양한 기법들이 포함된다. 이러한 기법들은 훈련 과정에서 모델이 더 풍부한 감독 신호를 받도록 하여, 최종적으로 추론 시에는 추가 비용 없이 더 높은 성능을 내는 모델을 만들어낸다.</p>
<p>YOLOv6와 v7의 등장은 YOLO 생태계가 더 이상 단일 주체에 의해 주도되지 않는 ‘다극화’ 시대에 접어들었음을 명확히 보여준다. 특히 두 버전 모두 ‘재매개변수화’ 기술을 핵심적으로 활용했다는 점은 주목할 만하다.32 이는 YOLO 커뮤니티가 단순히 mAP 수치를 넘어, 실제 하드웨어에서의 ’추론 속도’를 얼마나 중요하게 여기는지를 보여주는 증거다. 이 트렌드는 모델을 설계하는 단계부터 최종 배포 환경을 염두에 두는 MLOps 철학이 YOLO 개발 패러다임에 깊숙이 통합되고 있음을 시사한다.</p>
<h2>6.  현대 Ultralytics 시대: SOTA의 재정의 (YOLOv8, v9, v10, v11, v12)</h2>
<p>2023년 이후, YOLO 생태계는 Ultralytics를 중심으로 다시 한번 재편되며 새로운 SOTA(State-of-the-Art)의 기준을 정립하는 시기를 맞이했다. 이 시기의 모델들은 이전 버전들에서 분화되었던 다양한 성공적인 기술들을 집대성하고, 더 나아가 객체 탐지의 근본적인 문제에 도전하며 미래 방향성을 제시했다.</p>
<h3>6.1 YOLOv8: 새로운 기준점</h3>
<p>2023년 1월, Ultralytics는 YOLOv8을 출시하며 다시 한번 YOLO 생태계의 주도권을 잡았다.25 YOLOv8은 특정 논문을 기반으로 한 혁신이라기보다는, YOLOv5, v6, v7, 그리고 YOLOX 등에서 검증된 성공적인 기술들을 하나의 사용자 친화적인 프레임워크로 통합한 ’결정판’의 성격이 강하다.</p>
<h4>6.1.1 아키텍처 재설계: C2f 모듈</h4>
<p>YOLOv8의 백본과 넥은 YOLOv5의 C3 모듈을 <strong>C2f (Cross Stage Partial with 2 bottlenecks)</strong> 모듈로 대체했다.38 이 모듈은 YOLOv7의 ELAN 개념에서 영감을 받아 설계되었으며, 더 많은 숏컷 연결을 통해 특징 맵을 분할하고 병합한다.41 이를 통해 더 풍부한 그래디언트 흐름을 제공하여 성능을 향상시키면서도 효율적인 구조를 유지한다.</p>
<h4>6.1.2 헤드의 진화: 앵커 프리 분리형 헤드</h4>
<p>YOLOv8은 헤드 구조에서 가장 큰 변화를 보였다. YOLOv6와 YOLOX의 성공적인 접근법을 채택하여, YOLOv5의 앵커 기반 결합형 헤드(anchor-based coupled head)에서 완전히 벗어났다.25</p>
<ul>
<li><strong>앵커 프리 (Anchor-Free)</strong>: 객체의 중심점을 직접 예측하고, 중심점에서 경계까지의 거리를 예측하는 방식으로 변경되었다. 이는 앵커 박스에 대한 의존성을 제거하여 모델의 일반화 성능을 높이고 후처리 속도를 개선한다.25</li>
<li><strong>분리형 헤드 (Decoupled Head)</strong>: 분류(classification)와 바운딩 박스 회귀(regression)를 위한 네트워크 헤드를 분리했다. 이는 두 작업 간의 최적화 충돌을 줄여 전체적인 정확도를 향상시키는 효과를 가져온다.39</li>
</ul>
<h4>6.1.3 손실 함수의 통합: DFL과 CIoU Loss의 결합</h4>
<p>YOLOv8은 손실 함수 구성도 현대적인 방식으로 변경했다.38</p>
<ul>
<li><strong>회귀 손실 (Regression Loss)</strong>: 바운딩 박스 경계의 분포를 직접 학습하는 <strong>Distribution Focal Loss (DFL) v2</strong>와 기존의 <strong>CIoU Loss</strong>를 함께 사용한다.</li>
<li><strong>분류 손실 (Classification Loss)</strong>: **BCE Loss (Binary Cross-Entropy Loss)**를 사용한다.</li>
<li><strong>객체성 손실 제거</strong>: YOLOv5에 존재했던 객체성 손실(Objectness Loss) 항이 제거되었다. 이는 분리형 헤드 구조와 연관이 있으며, 모델 구조를 더욱 간소화하는 데 기여했다.</li>
</ul>
<p>이러한 기술들의 통합을 통해 YOLOv8은 사용 편의성, 속도, 정확도 모든 면에서 새로운 기준점을 제시했으며, 객체 탐지뿐만 아니라 인스턴스 분할(instance segmentation), 포즈 추정(pose estimation), 분류 등 다양한 작업으로 확장 가능한 통합 프레임워크로 자리 잡았다.37</p>
<h3>6.2 YOLOv9: 정보 손실 문제에 대한 도전</h3>
<p>YOLOv9은 YOLOv4와 v7을 개발했던 연구팀이 2024년 2월에 발표한 모델로, 성능을 몇 퍼센트 올리는 것을 넘어 딥러닝의 근본적인 문제에 도전하는 이론적 깊이를 보여준다.46</p>
<h4>6.2.1 핵심 개념: 프로그래밍 가능한 그래디언트 정보 (PGI)</h4>
<p>YOLOv9의 핵심은 **PGI (Programmable Gradient Information)**라는 새로운 개념이다.46 심층 신경망에서는 데이터가 여러 레이어를 통과하면서 정보가 손실되는</p>
<p><strong>정보 병목(information bottleneck)</strong> 현상이 발생한다.46 이로 인해 네트워크가 불완전한 정보를 바탕으로 학습하게 되어 그래디언트가 신뢰성을 잃고 성능 저하로 이어진다.</p>
<ul>
<li><strong>PGI의 작동 원리</strong>: PGI는 이 문제를 해결하기 위해 주 네트워크 경로 외에 보조적인 <strong>역전 가능한(reversible)</strong> 브랜치를 둔다. 이 보조 브랜치는 정보 손실 없이 입력 정보를 깊은 레이어까지 전달하는 역할을 한다. 따라서 손실 함수를 계산할 때, 모델은 이 보조 경로를 통해 완전한 입력 정보에 접근할 수 있고, 이를 통해 생성된 신뢰할 수 있는 그래디언트를 사용하여 네트워크 가중치를 업데이트한다.47</li>
</ul>
<h4>6.2.2 혁신적 아키텍처: 일반화된 ELAN (GELAN)</h4>
<p>PGI 개념을 효과적으로 구현하기 위해, YOLOv9은 **GELAN (Generalized ELAN)**이라는 새로운 경량 네트워크 아키텍처를 설계했다.46 GELAN은 기존의 ELAN 구조를 기반으로 하면서도, 다양한 컨볼루션 연산자들을 유연하게 결합할 수 있도록 일반화된 구조를 갖는다. 이 아키텍처는 PGI와 결합되었을 때 뛰어난 파라미터 효율성과 성능을 보여주었다.47</p>
<h3>6.3 YOLOv10: 진정한 End-to-End 탐지를 향하여</h3>
<p>2024년 5월, 칭화대학교 연구팀이 발표한 YOLOv10은 객체 탐지 파이프라인의 오랜 숙원이었던 **‘진정한 종단간(End-to-End) 탐지’**를 목표로 한다.49</p>
<h4>6.3.1 NMS 병목 현상과 그 해결책</h4>
<p>기존의 모든 YOLO 모델들은 추론(inference) 과정에서 수많은 중복된 바운딩 박스를 생성하고, 이를 제거하기 위해 **NMS (Non-Maximum Suppression)**라는 후처리 과정에 의존했다.49 NMS는 효율적이긴 하지만 추론 지연을 유발하는 명백한 병목 지점이며, 이 때문에 기존 모델들은 엄밀한 의미에서 완전한 종단간 시스템이라고 보기 어려웠다.</p>
<h4>6.3.2 솔루션: NMS-Free 훈련을 위한 일관된 이중 할당</h4>
<p>YOLOv10은 NMS를 제거하기 위해 **일관된 이중 할당(Consistent Dual Assignments)**이라는 독창적인 훈련 전략을 제안했다.49</p>
<ul>
<li><strong>이중 할당 (Dual Assignments)</strong>: 훈련 중에 두 개의 예측 헤드를 동시에 사용한다. 하나는 기존처럼 하나의 실제 객체에 여러 예측을 할당하는 <strong>1-to-many 할당</strong>을 사용하는 헤드이고, 다른 하나는 하나의 실제 객체에 단 하나의 예측만을 할당하는 <strong>1-to-1 할당</strong>을 사용하는 헤드이다.</li>
<li><strong>훈련과 추론</strong>: 훈련 시에는 두 헤드를 함께 최적화하여 모델이 1-to-many 할당으로부터 풍부한 감독 신호를 학습하도록 한다. 하지만 추론 시에는 1-to-many 헤드를 버리고, 중복 예측을 생성하지 않는 <strong>1-to-1 헤드만을 사용</strong>한다. 이를 통해 NMS 후처리 과정 자체가 필요 없게 된다.</li>
<li><strong>일관된 매칭 (Consistent Matching)</strong>: 두 헤드 간의 성능 저하를 막기 위해, 1-to-many 헤드에서 가장 좋은 예측이 1-to-1 헤드에서도 가장 좋은 예측이 되도록 매칭 지표를 일관되게 만들어 감독 신호를 조화시켰다.49</li>
</ul>
<p>이러한 접근은 YOLO를 NMS의 제약에서 해방시켜, 지연 시간을 크게 줄이고 진정한 의미의 종단간 실시간 탐지를 가능하게 했다. YOLOv9과 v10의 등장은 YOLO의 진화가 단순한 성능 개선을 넘어, ’구조적 우아함’과 ’이론적 완결성’을 추구하는 새로운 단계로 접어들었음을 보여주는 이정표이다.</p>
<h3>6.4 YOLOv11 &amp; YOLOv12: 최전선과 미래</h3>
<ul>
<li><strong>YOLOv11</strong>: 2024년 후반, Ultralytics는 YOLOv8의 공식 후속 버전인 YOLOv11을 공개했다.7 YOLOv11은 YOLOv8의 성공적인 아키텍처를 기반으로 몇 가지 핵심 모듈을 개선하는 데 집중했다. C2f 블록을 커널 크기를 조정한 **C3k2(Cross Stage Partial with kernel size 2)**로 대체하고, 병렬 공간 어텐션 메커니즘을 적용한</li>
</ul>
<p><strong>C2PSA(Convolutional block with Parallel Spatial Attention)</strong> 모듈을 도입하여 특징 추출 능력을 더욱 강화했다.7 이는 YOLOv8의 성공적인 프레임워크를 유지하면서 성능과 효율성을 점진적으로 개선하는 Ultralytics의 개발 철학을 보여준다.53</p>
<ul>
<li><strong>YOLOv12</strong>: 2025년 현재, YOLOv12는 아직 단일화된 공식 릴리즈나 논문이 존재하지 않는다.5 대신, 커뮤니티에서는 최신 기술들을 통합한 고성능 프로젝트에 YOLOv12라는 이름을 붙여 사용하는 경향을 보이고 있다.56 이는 YOLO의 개발 방식이 중앙화된 단일 릴리즈에서 벗어나, 여러 개발자들이 최신 기술을 빠르게 적용하고 공유하는 <strong>탈중앙화된 커뮤니티 기반의 진화</strong>로 변화하고 있음을 시사한다. 현재 YOLOv12로 지칭되는 프로젝트들은 실시간 추적(live tracking) 기능과의 결합에 중점을 두는 경우가 많다.56</li>
</ul>
<h2>7.  종합 비교 분석 및 고찰</h2>
<p>지난 10년간 YOLO 시리즈는 실시간 객체 탐지 분야에서 끊임없는 혁신을 거듭하며 발전해왔다. 각 버전은 이전 버전의 한계를 극복하고 당시의 최신 기술을 흡수하며 독자적인 특성을 구축했다. 이 섹션에서는 YOLOv1부터 v12까지의 핵심적인 기술적 특징들을 한눈에 비교하고, 그 진화의 흐름 속에 담긴 거시적인 트렌드를 분석한다.</p>
<h3>7.1 핵심 지표 비교 분석표</h3>
<p>아래 표는 YOLO 시리즈의 주요 버전별 핵심 특징을 요약한 것이다. 이 표는 각 버전의 기술적 스펙을 신속하게 비교하고, 버전 간의 진화적 관계를 직관적으로 파악하는 데 도움을 준다.</p>
<table><thead><tr><th><strong>Version</strong></th><th><strong>Year</strong></th><th><strong>Primary Author(s)/Org.</strong></th><th><strong>Backbone</strong></th><th><strong>Neck</strong></th><th><strong>Head</strong></th><th><strong>Loss Function</strong></th><th><strong>Key Innovations</strong></th></tr></thead><tbody>
<tr><td>YOLOv1</td><td>2015</td><td>Redmon et al.</td><td>GoogLeNet-based</td><td>-</td><td>Unified, Coupled, Anchor-Based</td><td>SSE</td><td>Unified Pipeline, Grid Cell 3</td></tr>
<tr><td>YOLOv2</td><td>2016</td><td>Redmon &amp; Farhadi</td><td>Darknet-19</td><td>Passthrough</td><td>Coupled, Anchor-Based</td><td>SSE</td><td>Anchor Boxes, k-means, Multi-Scale Training 10</td></tr>
<tr><td>YOLOv3</td><td>2018</td><td>Redmon &amp; Farhadi</td><td>Darknet-53</td><td>FPN</td><td>Coupled, Anchor-Based</td><td>BCE</td><td>Multi-Scale Prediction, Logistic Classifier 14</td></tr>
<tr><td>YOLOv4</td><td>2020</td><td>Bochkovskiy et al.</td><td>CSPDarknet53</td><td>SPP, PANet</td><td>Coupled, Anchor-Based</td><td>CIoU, BCE</td><td>Bag of Freebies/Specials, Mosaic Aug. 18</td></tr>
<tr><td>YOLOv5</td><td>2020</td><td>Ultralytics</td><td>CSPDarknet53</td><td>PANet</td><td>Coupled, Anchor-Based</td><td>CIoU, BCE</td><td>PyTorch-native, Auto-Anchor, Model Scaling 21</td></tr>
<tr><td>YOLOv6</td><td>2022</td><td>Meituan</td><td>EfficientRep</td><td>Rep-PAN</td><td>Decoupled, Anchor-Free</td><td>VFL, DFL</td><td>Reparameterization, Industrial Focus 31</td></tr>
<tr><td>YOLOv7</td><td>2022</td><td>Wang et al.</td><td>E-ELAN</td><td>E-ELAN</td><td>Coupled, Anchor-Based</td><td>BCE, CIoU</td><td>Trainable BoF, Compound Scaling 35</td></tr>
<tr><td>YOLOv8</td><td>2023</td><td>Ultralytics</td><td>CSPDarknet (C2f)</td><td>PANet</td><td>Decoupled, Anchor-Free</td><td>DFL, CIoU, BCE</td><td>C2f module, Anchor-Free Decoupled Head 37</td></tr>
<tr><td>YOLOv9</td><td>2024</td><td>Wang et al.</td><td>GELAN</td><td>GELAN</td><td>Decoupled, Anchor-Free</td><td>CIoU, BCE</td><td>PGI, GELAN 46</td></tr>
<tr><td>YOLOv10</td><td>2024</td><td>Wang et al. (THU)</td><td>CSPDarknet</td><td>PANet</td><td>Decoupled, Anchor-Free</td><td>(Dual Assignment)</td><td>NMS-Free (Consistent Dual Assignments) 49</td></tr>
<tr><td>YOLOv11</td><td>2024</td><td>Ultralytics</td><td>CSPDarknet (C3k2)</td><td>SPPF, C2PSA</td><td>Decoupled, Anchor-Free</td><td>DFL, CIoU, BCE</td><td>C3k2, C2PSA modules 7</td></tr>
<tr><td>YOLOv12</td><td>2025</td><td>Community/Various</td><td>(Evolving)</td><td>(Evolving)</td><td>(Evolving)</td><td>(Various)</td><td>Community-driven, Real-time tracking focus 5</td></tr>
</tbody></table>
<h3>7.2 진화의 흐름 분석</h3>
<p>YOLO 시리즈의 10년 역사는 몇 가지 뚜렷한 기술적 진화의 흐름을 보여준다.</p>
<ul>
<li><strong>백본의 진화 (Backbone Evolution)</strong>: 초기 GoogLeNet 기반의 구조에서 시작하여 <strong>Darknet (v2, v3)</strong> 시리즈를 통해 자체적인 효율적인 백본을 구축했다. 이후 <strong>CSP (v4, v5, v8)</strong> 구조를 도입하여 연산 효율성과 정보 흐름을 개선했으며, 최근에는 **ELAN (v7) 및 GELAN (v9)**과 같이 더 정교한 그래디언트 제어와 특징 집계를 목표로 하는 구조로 발전했다. 이는 단순히 깊고 복잡한 네트워크가 아닌, 효율성과 특징 추출 능력 간의 최적 균형을 맞추려는 지속적인 노력을 보여준다.</li>
<li><strong>헤드의 진화 (Head Evolution)</strong>: 헤드 구조는 가장 극적인 변화를 겪은 부분 중 하나이다. 초기 <strong>앵커 기반(Anchor-Based) 결합형(Coupled) 헤드</strong>에서 시작하여, 점차 <strong>앵커 프리(Anchor-Free)</strong> 방식과 <strong>분리형(Decoupled) 헤드</strong>로 수렴하는 명확한 흐름을 보인다. 앵커 프리 방식은 앵커 설계의 복잡성을 제거하고 일반화 성능을 높이며, 분리형 헤드는 분류와 회귀 작업 간의 충돌을 완화하여 성능을 향상시킨다. 이 두 가지 혁신은 이제 현대 YOLO 모델의 표준으로 자리 잡았다.</li>
<li><strong>개발 방식의 진화 (Development Methodology Evolution)</strong>: YOLO의 개발 주체와 방식 또한 크게 변화했다. 초기에는 **단일 저자(Joseph Redmon)**의 주도 하에 개발이 이루어졌다. 포스트-Redmon 시대에 들어서면서 **다수의 학술 연구 그룹(v4, v7, v9)**과 **기업(v5, v6, v8, v11)**이 경쟁적으로 개발을 주도하는 다극화 시대를 맞이했다. 그리고 최근에는 특정 주체 없이 **커뮤니티 기반(v12)**으로 최신 기술이 융합되는 탈중앙화된 형태로 진화하고 있다. 이는 YOLO가 더 이상 개인이나 특정 그룹의 소유물이 아님을 명확히 보여준다.</li>
</ul>
<h3>7.3 ‘YOLO’ 브랜드에 대한 고찰</h3>
<p>결론적으로, ’YOLO’는 더 이상 특정 논문이나 코드를 지칭하는 단일한 개념이 아니다. 오늘날 YOLO는 **‘실시간 처리, 단일 단계(single-stage) 구조, 효율성’**이라는 핵심 철학을 공유하는 거대한 기술 생태계이자 브랜드가 되었다.1 이 생태계 안에서 다양한 버전들은 각기 다른 목표와 철학을 가지고 발전하고 있으며, 이는 각 버전이 채택한 소프트웨어 라이선스에서도 드러난다.</p>
<p>예를 들어, YOLOv4와 v7의 저자들은 학술적 기여에 중점을 두어 비교적 자유로운 라이선스를 채택한 반면, Ultralytics의 YOLOv5와 v8은 AGPL-3.0 라이선스를 채택하여 오픈소스 생태계를 유지하면서도 상업적 활용에 대한 명확한 규칙을 제시했다.25 Meituan의 YOLOv6는 Apache-2.0 라이선스를 통해 산업계에서의 폭넓은 활용을 장려했다.24 이처럼 라이선스의 다양성은 각 버전의 개발 철학과 목표(학술적 공개 vs. 상업적 생태계 구축)를 반영하는 중요한 지표이며, 사용자는 기술적 특징뿐만 아니라 이러한 라이선스 정책까지 고려하여 자신의 목적에 맞는 YOLO를 선택해야 한다.</p>
<h2>8.  결론 및 미래 전망</h2>
<h3>8.1 YOLO의 10년이 컴퓨터 비전 분야에 미친 영향 요약</h3>
<p>지난 10년간 YOLO는 컴퓨터 비전, 특히 객체 탐지 분야에 지대한 영향을 미쳤다. YOLOv1의 등장은 실시간 객체 탐지를 불가능의 영역에서 현실의 영역으로 끌어내렸고, 이는 학문적 연구를 넘어 산업 전반에 걸쳐 수많은 실제 애플리케이션의 탄생을 촉발했다.1 자율 주행 차량의 눈, 스마트 시티의 감시 시스템, 공장 자동화 라인의 검사기, 농업 분야의 작물 모니터링 등 YOLO의 기술은 우리 삶의 다양한 측면에 스며들었다.2</p>
<p>YOLO 시리즈의 진화 과정은 속도와 정확도라는 상충하는 목표 사이에서 최적의 균형점을 찾기 위한 끊임없는 탐구의 역사였다. 이 과정에서 앵커 박스, 특징 피라미드, 재매개변수화, 앵커 프리, NMS-Free 등 객체 탐지 분야의 수많은 핵심 기술들이 제안되고 검증되었다. 그 결과, YOLO는 단순한 모델 시리즈를 넘어, 실시간 객체 탐지 분야의 사실상 표준(de facto standard) 도구로 자리매김했다.</p>
<h3>8.2 실무자를 위한 상황별 최적 YOLO 버전 선택 가이드라인</h3>
<p>다양한 YOLO 버전이 공존하는 현재, 특정 작업에 가장 적합한 모델을 선택하는 것은 중요한 결정이다. 다음은 상황별 최적 YOLO 버전을 선택하기 위한 가이드라인이다.</p>
<ul>
<li><strong>최고의 사용 편의성과 커뮤니티 지원이 필요할 때</strong>: <strong>YOLOv5, YOLOv8, YOLOv11</strong> (Ultralytics 계열)</li>
<li>이 버전들은 PyTorch 기반으로 매우 사용자 친화적이며, 방대한 문서, 튜토리얼, 그리고 활발한 커뮤니티를 자랑한다. 처음 객체 탐지를 시작하거나 빠른 프로토타이핑이 필요할 때 가장 이상적인 선택이다.21</li>
<li><strong>특정 하드웨어/산업용 배포 최적화가 목표일 때</strong>: <strong>YOLOv6</strong></li>
<li>재매개변수화 기법을 통해 추론 속도를 극대화하도록 설계되었으며, 산업용 애플리케이션 배포에 특화된 기능들을 제공한다. 특정 임베디드 장치나 서버 환경에서 최고의 성능을 끌어내야 할 때 고려할 수 있다.31</li>
<li><strong>최신 학술적 아이디어를 탐구하고 싶을 때</strong>: <strong>YOLOv9, YOLOv10</strong></li>
<li>YOLOv9의 PGI나 YOLOv10의 NMS-Free 훈련 방식은 객체 탐지의 근본적인 문제에 대한 최신 연구 동향을 담고 있다. 연구 목적으로 새로운 개념을 실험하고 이해하고자 할 때 적합하다.46</li>
<li><strong>안정성과 검증된 성능을 선호할 때</strong>: <strong>YOLOv4, YOLOv7</strong></li>
<li>이 버전들은 수많은 연구와 애플리케이션을 통해 성능이 충분히 검증되었으며, 특히 YOLOv4는 체계적인 실험을 통해 최적의 조합을 찾아낸 모델로서 안정적인 성능을 기대할 수 있다.</li>
</ul>
<h3>8.3 향후 발전 방향: YOLO의 다음 10년</h3>
<p>YOLO의 진화는 멈추지 않을 것이며, 앞으로의 10년은 더욱 흥미로운 방향으로 전개될 것으로 예상된다.</p>
<ul>
<li><strong>기술적 전망</strong>: YOLOv10이 제시한 <strong>NMS-Free</strong>와 같은 완전한 종단간 아키텍처가 더욱 보편화될 것이다.49 또한, 현재 AI 분야를 주도하고 있는</li>
</ul>
<p><strong>Transformer</strong>, **Mamba(State Space Model)**와 같은 새로운 아키텍처와의 융합이 가속화될 것이다.3 이를 통해 YOLO는 더 넓은 문맥을 이해하고, 더 정교한 특징을 추출하는 능력을 갖추게 될 것이다.</p>
<ul>
<li><strong>응용 분야 확장</strong>: YOLO는 단순한 객체 탐지기를 넘어, 더 큰 AI 시스템의 핵심적인 ’눈’으로서의 역할을 수행하게 될 것이다. 이미 <strong>YOLO-World</strong>와 같은 모델은 비전-언어 모델링(vision-language modeling)을 통해 사전 정의되지 않은 어떠한 객체도 텍스트 입력만으로 탐지하는 능력을 보여주었다.58 앞으로 YOLO는 **멀티모달리티(multimodality)**를 적극적으로 수용하고, 궁극적으로는</li>
</ul>
<p><strong>인공 일반 지능(AGI)</strong> 시스템이 현실 세계와 상호작용하는 데 필수적인 시각적 인식 모듈로 진화할 것이다.7</p>
<p>결론적으로, YOLO는 지난 10년간 ’효율성’과 ’실용성’이라는 강력한 무기를 바탕으로 객체 탐지 기술의 발전을 이끌어왔다. 앞으로의 10년 동안에도 YOLO는 그 핵심 철학을 유지하면서, 더욱 복잡하고 지능적인 AI 시스템의 핵심 구성 요소로서 그 영향력을 계속해서 확장해 나갈 것이다.</p>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>