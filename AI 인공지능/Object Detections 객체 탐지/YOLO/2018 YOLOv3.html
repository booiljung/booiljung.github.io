<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:YOLOv3 (2018)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>YOLOv3 (2018)</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">객체 탐지 (Object Detection)</a> / <a href="index.html">YOLO 객체 탐지 모델</a> / <span>YOLOv3 (2018)</span></nav>
                </div>
            </header>
            <article>
                <h1>YOLOv3 (2018)</h1>
<h2>1. 서론: YOLOv3의 등장과 철학</h2>
<h3>1.1 YOLO 계보 속 YOLOv3의 위치</h3>
<p>YOLOv3는 Joseph Redmon과 Ali Farhadi가 2018년 4월 arXiv를 통해 발표한 객체 탐지 모델로, 그 부제인 “An Incremental Improvement“는 모델의 정체성을 명확히 규정한다.1 이는 기존 객체 탐지 패러다임을 완전히 뒤엎는 혁명적 변화가 아닌, 이전 버전인 YOLOv2의 강점을 계승하면서 당대의 성공적인 아이디어들을 실용적으로 통합한 점진적 개선판임을 시사한다.4 YOLO(You Only Look Once) 계보를 살펴보면, YOLOv1은 객체 탐지를 경계 상자(bounding box) 예측과 클래스 확률 예측을 결합한 단일 회귀 문제로 재정의하여 실시간 탐지의 가능성을 열었다.6 YOLOv2(YOLO9000)는 앵커 박스(anchor box) 개념을 도입하고 더 효율적인 백본 네트워크인 Darknet-19를 사용하여 정확도와 속도를 개선했다.2 YOLOv3는 이러한 흐름을 이어받아, 다중 스케일 예측(multi-scale prediction) 기법과 훨씬 더 깊어진 백본 네트워크인 Darknet-53을 도입함으로써 전반적인 정확도, 특히 탐지가 어려웠던 작은 객체에 대한 성능을 획기적으로 끌어올리는 데 집중했다.7</p>
<h3>1.2 Redmon의 독창적 접근: 기술 안내서의 파격과 그 이면</h3>
<p>YOLOv3 원본 논문은 학계의 전통적인 서술 방식에서 크게 벗어난 독특한 스타일로 작성되었다. “We present some updates to YOLO! We made a bunch of little design changes to make it better“와 같은 구어체적이고 솔직한 표현은 논문의 전반적인 특징을 이룬다.1 이러한 파격적인 문체는 연구 커뮤니티 내에서 다양한 해석을 낳았다. 다수의 연구자들은 이를 신선하고 읽기 즐거운 접근 방식으로 평가하며, 복잡한 개념을 의도적으로 난해하게 포장하는 학계의 관행에 대한 비판적 메시지로 받아들였다.11 특히, 수많은 GPU를 동원한 하이퍼파라미터 튜닝의 고된 과정이나 실패한 실험(“we don’t know why / this was a total guess”)에 대해 솔직하게 인정하는 투명성은 큰 호응을 얻었다.11</p>
<p>이러한 논문의 스타일은 단순히 문체적 선택을 넘어 YOLO 모델의 핵심적인 공학 철학을 반영한다. 논문은 학술적 엄밀함이나 이론적 독창성을 내세우기보다, 무엇을 시도했고, 무엇이 효과가 있었으며, 무엇이 그렇지 않았는지를 실용적으로 기술하는 데 집중한다. 모델의 설계 역시 이러한 철학을 그대로 따른다. YOLOv3는 단일한 이론적 돌파구에 기반한 것이 아니라, ResNet의 잔차 연결(residual connection)이나 FPN(Feature Pyramid Network)의 다중 스케일 예측과 같이 “다른 사람들로부터 온 좋은 아이디어들(good ideas from other people)“을 적극적으로 차용하고 조합한 결과물이다.4 이는 저자들의 주된 목표가 이론적으로 새로운 논문을 발표하는 것이 아니라, 더 빠르고 정확하며 실용적인 탐지기를 만드는 것이었음을 시사한다. 따라서 YOLOv3 논문의 독특한 스타일은 YOLO 개발을 관통하는 핵심 철학, 즉 이론적 순수성이나 학술적 형식주의보다 실용적인 성능과 검증된 결과를 우선시하는 태도의 발현으로 해석할 수 있다. 이러한 신속하고 실용적인 반복 개선의 철학은 이후 YOLO 시리즈의 상징이 되었다.</p>
<h3>1.3 객체 탐지 패러다임에 대한 기여</h3>
<p>YOLOv3의 가장 핵심적인 기여는 2-stage detector(예: Faster R-CNN)의 영역으로 간주되었던 높은 정확도와 1-stage detector(예: SSD, YOLO)의 고유한 장점인 실시간 처리 속도를 전례 없는 수준으로 융합했다는 점에 있다.15 논문에서 제시된 성능 지표는 이러한 성과를 명확하게 보여준다. 320x320의 낮은 해상도 입력에서도 YOLOv3는 SSD와 동등한 수준인 28.2 mAP(mean Average Precision)를 달성하면서 처리 시간은 22ms로 3배 이상 빨랐다.3 또한, 당시 최고 수준의 정확도를 자랑하던 RetinaNet과 비교했을 때, 전통적인 평가 기준인 AP50(IoU 0.5 기준 AP)에서 57.9로 유사한 성능을 보이면서도 처리 시간은 51ms로 3.8배 빠른 속도를 기록했다.3 이러한 압도적인 속도-정확도 균형은 YOLOv3를 단순한 학술 연구용 모델을 넘어, 자율주행, CCTV 감시, 공장 자동화 등 실제 산업 현장에서 실시간 객체 탐지를 구현할 수 있는 강력하고 신뢰성 있는 표준으로 자리매김하게 했다.19</p>
<h2>2. 핵심 아키텍처 분석: Darknet-53</h2>
<h3>2.1 네트워크의 구조적 특징</h3>
<p>YOLOv3의 성능 향상의 중심에는 새롭게 설계된 백본 네트워크, Darknet-53이 있다.8 이는 YOLOv2에서 사용된 Darknet-19를 대폭 확장한 것으로, 이름에서 알 수 있듯이 53개의 컨볼루션 레이어로 구성되어 훨씬 더 깊고 강력한 특징 추출 능력을 제공한다.21 전체 아키텍처는 주로 3x3 커널과 1x1 커널을 사용하는 컨볼루션 레이어가 교차 반복되는 구조를 가진다.12 여기서 1x1 컨볼루션은 특징 맵의 채널 수를 효율적으로 조절하여 연산량을 줄이는 병목(bottleneck) 역할을 수행하고, 3x3 컨볼루션은 실질적인 공간적 특징 추출을 담당한다. 이 53개의 레이어는 순수하게 특징 추출기(feature extractor)로서의 역할만 수행하며, 실제 객체 탐지를 위해서는 추가적으로 53개의 레이어가 그 위에 적층되어 총 106개의 레이어로 구성된 완전 컨볼루션 네트워크(Fully Convolutional Network, FCN)를 형성한다.13</p>
<h3>2.2 ResNet의 영향: 잔차 연결의 도입</h3>
<p>Darknet-53의 가장 중요한 구조적 혁신은 당대 최고의 이미지 분류 모델이었던 ResNet에서 영감을 받은 잔차 연결(Residual Connection 또는 Skip Connection)의 도입이다.8 심층 신경망(Deep Neural Network)은 레이어가 깊어질수록 역전파 과정에서 기울기가 점차 작어져 사라지는 기울기 소실(vanishing gradient) 문제에 직면하게 되는데, 이는 깊은 네트워크의 학습을 매우 어렵게 만든다. 잔차 연결은 특정 레이어 블록의 입력을 여러 컨볼루션 연산을 건너뛰어(skip) 해당 블록의 출력에 직접 더해주는 구조를 가진다.21 이를 통해 기울기가 하위 레이어까지 손실 없이 효과적으로 전파될 수 있는 ’고속도로’를 만들어주어 깊은 네트워크의 학습을 안정화하고, 더 낮은 수준의 원본 특징 정보가 상위 레이어로 잘 전달되도록 돕는다. Darknet-53은 이러한 잔차 블록을 1개, 2개, 8개, 8개, 4개씩 묶은 그룹들을 순차적으로 배치하여 네트워크의 깊이를 효과적으로 늘렸다.13</p>
<h3>2.3 다운샘플링 전략과 기본 구성 요소</h3>
<p>Darknet-53은 기존의 많은 CNN 아키텍처에서 흔히 사용되던 최대 풀링(Max Pooling) 계층을 사용하지 않는다는 특징을 가진다.22 대신, 스트라이드(stride) 값이 2인 컨볼루션 레이어를 사용하여 특징 맵의 공간적 해상도를 절반으로 줄이는 다운샘플링을 수행한다.13</p>
<p>이러한 설계는 작은 객체 탐지 성능을 향상시키려는 YOLOv3의 핵심 목표와 직결된 의도적인 선택이다. 최대 풀링은 특정 영역에서 가장 큰 값 하나만을 남기고 나머지 정보를 버리는 비모수적(non-parametric) 연산으로, 계산 효율은 높지만 상당한 양의 공간적, 세부적 정보를 손실시킨다. 반면, 스트라이드 컨볼루션은 학습 가능한 가중치(커널)를 가진 연산으로, 네트워크가 주어진 태스크에 최적화된 방식으로 다운샘플링을 수행하도록 학습할 수 있다. 작은 객체는 이미지 내에서 불과 몇 픽셀만을 차지하기 때문에, 공격적인 다운샘플링 과정에서 그 특징이 쉽게 소실될 수 있다. 스트라이드 컨볼루션을 사용함으로써 Darknet-53은 최대 풀링으로 인한 정보 손실을 방지하고, 작은 객체를 정확히 탐지하고 위치를 특정하는 데 필수적인 저수준 특징과 위치 정보를 더 효과적으로 보존할 수 있다.13</p>
<p>Darknet-53의 각 컨볼루션 레이어는 배치 정규화(Batch Normalization)와 Leaky ReLU 활성화 함수가 순차적으로 적용되는 구조를 따른다.21 배치 정규화는 각 레이어의 입력 분포를 정규화하여 학습 과정을 안정시키고 수렴 속도를 가속화하는 역할을 한다. Leaky ReLU 활성화 함수는 입력값이 음수일 때 작은 기울기(e.g., 0.1)를 부여하여, 표준 ReLU 함수에서 발생하는 ‘죽은 뉴런(dying ReLU)’ 문제를 완화한다.21 이 세 가지 요소(컨볼루션, 배치 정규화, Leaky ReLU)의 조합은 Darknet 아키텍처를 구성하는 기본 빌딩 블록(building block)으로 기능한다.</p>
<p>다음 표는 Darknet-53 특징 추출기의 상세한 구조를 요약한 것이다.</p>
<p><strong>Table 1: Darknet-53 아키텍처 상세 구조</strong></p>
<table><thead><tr><th>Layer Type</th><th>Filters</th><th>Size/Stride</th><th>Output Size (416x416 Input)</th></tr></thead><tbody>
<tr><td>Convolutional</td><td>32</td><td>3x3 / 1</td><td>416 x 416</td></tr>
<tr><td>Convolutional</td><td>64</td><td>3x3 / 2</td><td>208 x 208</td></tr>
<tr><td>Residual Block x1</td><td>32, 64</td><td>1x1, 3x3</td><td>208 x 208</td></tr>
<tr><td>Convolutional</td><td>128</td><td>3x3 / 2</td><td>104 x 104</td></tr>
<tr><td>Residual Block x2</td><td>64, 128</td><td>1x1, 3x3</td><td>104 x 104</td></tr>
<tr><td>Convolutional</td><td>256</td><td>3x3 / 2</td><td>52 x 52</td></tr>
<tr><td>Residual Block x8</td><td>128, 256</td><td>1x1, 3x3</td><td>52 x 52</td></tr>
<tr><td>Convolutional</td><td>512</td><td>3x3 / 2</td><td>26 x 26</td></tr>
<tr><td>Residual Block x8</td><td>256, 512</td><td>1x1, 3x3</td><td>26 x 26</td></tr>
<tr><td>Convolutional</td><td>1024</td><td>3x3 / 2</td><td>13 x 13</td></tr>
<tr><td>Residual Block x4</td><td>512, 1024</td><td>1x1, 3x3</td><td>13 x 13</td></tr>
<tr><td><strong>(Feature Extractor Output)</strong></td><td></td><td></td><td></td></tr>
</tbody></table>
<h2>3. 주요 메커니즘 심층 탐구</h2>
<h3>3.1 다중 스케일 예측: 피처 피라미드 네트워크(FPN)의 원리</h3>
<p>YOLOv3의 가장 중요한 개선점 중 하나는 작은 객체 탐지 성능을 획기적으로 향상시키기 위해 피처 피라미드 네트워크(FPN)와 유사한 개념을 도입한 것이다.7 이전 버전들이 최종 특징 맵이라는 단일 스케일에서만 예측을 수행했던 것과 달리, YOLOv3는 서로 다른 3개의 스케일에서 예측을 수행한다. 이를 위해 Darknet-53 백본의 서로 다른 깊이에서 특징 맵을 추출하는데, 이들은 각각 32, 16, 8의 스트라이드로 다운샘플링된 크기를 가진다. 예를 들어, 416x416 크기의 이미지가 입력될 경우, 각각 13x13, 26x26, 52x52 크기를 갖는 3개의 특징 맵이 예측에 사용된다.22</p>
<p>탐지 과정은 다음과 같은 계층적 구조를 따른다.</p>
<ol>
<li><strong>첫 번째 예측 (큰 객체용):</strong> 가장 깊은 층에서 추출된 13x13 크기의 특징 맵에서 첫 번째 예측이 이루어진다. 이 특징 맵은 가장 넓은 수용장(receptive field)을 가지므로, 이미지 전체의 맥락을 파악하고 큰 객체를 탐지하는 데 가장 적합하다.26</li>
<li><strong>두 번째 예측 (중간 크기 객체용):</strong> 13x13 특징 맵을 2배 업샘플링(upsample)하여 26x26 크기로 만든다. 그 후, 이 업샘플링된 특징 맵을 Darknet-53의 중간 깊이에서 가져온 26x26 특징 맵과 채널 축을 따라 결합(concatenate)한다. 이 결합된 특징 맵에서 두 번째 예측이 수행되며, 이는 중간 크기의 객체를 탐지하는 데 사용된다.13</li>
<li><strong>세 번째 예측 (작은 객체용):</strong> 이 과정을 한 번 더 반복한다. 26x26 크기의 결합된 특징 맵을 다시 2배 업샘플링하여 52x52 크기로 만들고, 이를 Darknet-53의 더 얕은 층에서 가져온 52x52 특징 맵과 결합한다. 이 최종 결합 특징 맵에서 세 번째 예측이 이루어지며, 이는 가장 높은 해상도를 가지므로 작은 객체를 탐지하는 데 최적화되어 있다.13</li>
</ol>
<p>이러한 다중 스케일 접근 방식은 2-stage detector의 핵심 강점 중 하나를 실시간 프레임워크에 실용적으로 이식한 것이다. Darknet-53의 표준적인 순방향 경로(bottom-up pathway)는 의미론적으로는 풍부하지만 공간적으로는 거친 특징 맵(예: 13x13)을 생성한다. 업샘플링 과정은 이러한 거친 의미론적 맵을 다시 고해상도로 변환하는 역방향 경로(top-down pathway)를 만든다. 그리고 결합(concatenation) 연산은 이 역방향 경로의 맵과 순방향 경로의 초기 층에서 나온 공간적으로는 정밀하지만 의미론적으로는 빈약한 맵을 잇는 측면 연결(lateral connection)의 역할을 한다.13 이 구조 덕분에, 최종적으로 작은 객체를 탐지하는 52x52 스케일의 예측 계층은 얕은 층의 고해상도 위치 정보와 깊은 층의 풍부한 의미론적 문맥 정보를 동시에 활용할 수 있게 된다. 과거에 이러한 고성능 다중 스케일 탐지는 FPN을 장착한 Faster R-CNN과 같이 복잡하고 느린 2-stage 모델의 전유물이었으나 27, YOLOv3는 이 강력한 개념의 간소화된 버전을 1-stage 프레임워크에 성공적으로 통합함으로써 핵심적인 속도 우위를 유지하면서도 작은 객체에 대한 mAP를 크게 향상시켰다.</p>
<h3>3.2 바운딩 박스 예측의 수학적 원리</h3>
<p>네트워크는 각 그리드 셀의 각 앵커 박스에 대해 4개의 원시 값(<span class="math math-inline">t_x, t_y, t_w, t_h</span>)을 직접 예측한다. 이 값들은 학습을 안정시키기 위한 변환된 값으로, 실제 이미지 좌표계의 최종 바운딩 박스 좌표(<span class="math math-inline">b_x, b_y, b_w, b_h</span>)로 변환되어야 한다.4 이 변환 과정은 다음의 수식을 통해 이루어진다.4<br />
<span class="math math-display">
b_x = \sigma(t_x) + c_x
</span></p>
<p><span class="math math-display">
b_y = \sigma(t_y) + c_y
</span></p>
<p><span class="math math-display">
b_w = p_w e^{t_w}
</span></p>
<p><span class="math math-display">
b_h = p_h e^{t_h}
</span></p>
<p>각 변수의 의미는 다음과 같다.</p>
<ul>
<li><span class="math math-inline">(c_x, c_y)</span>: 예측이 이루어지는 그리드 셀의 좌상단 좌표(오프셋). 예를 들어, 13x13 그리드에서 (0,0)부터 (12,12)까지의 정수 값을 가진다.</li>
<li><span class="math math-inline">\sigma</span>: 로지스틱 함수(시그모이드). 네트워크 출력 <span class="math math-inline">t_x</span>와 <span class="math math-inline">t_y</span>에 적용되어 예측값을 0과 1 사이로 제한한다. 이는 예측된 객체의 중심점이 항상 해당 그리드 셀 내부에 위치하도록 강제하여 학습 초기의 불안정성을 줄이고 수렴을 돕는다.</li>
<li><span class="math math-inline">(p_w, p_h)</span>: 앵커 박스(Anchor Box)의 사전 정의된 너비와 높이. 이는 일종의 기준점(prior) 역할을 한다.</li>
<li><span class="math math-inline">e^{(...)}</span>: 지수 함수. 네트워크가 예측한 <span class="math math-inline">t_w</span>와 <span class="math math-inline">t_h</span>를 앵커 박스의 크기에 대한 스케일링 팩터로 사용한다. 이를 통해 예측된 바운딩 박스의 너비와 높이가 항상 양수 값을 갖도록 보장하며, 앵커 박스 대비 큰 폭의 크기 변화도 안정적으로 예측할 수 있게 한다.</li>
</ul>
<p>앵커 박스는 YOLOv2에서 도입된 개념으로, YOLOv3에서도 핵심적인 역할을 한다. YOLOv3는 훈련 데이터셋(주로 MS COCO)에 포함된 모든 실제 객체(ground truth) 박스들의 형태를 분석하여 가장 대표적인 형태를 미리 찾아낸다. 이를 위해 k-평균 군집화(k-means clustering) 알고리즘을 사용하여 최적의 앵커 박스 크기 9개를 사전 계산한다.23 이 9개의 앵커는 3개의 예측 스케일에 각각 3개씩 할당된다. 예를 들어, 13x13 특징 맵에는 가장 큰 앵커 3개가, 52x52 특징 맵에는 가장 작은 앵커 3개가 할당된다.23 이 방식을 통해 네트워크는 임의의 크기와 비율을 처음부터 학습하는 대신, 사전 정의된 대표 형태로부터의 미세한 조정값(offset)만을 학습하도록 유도된다. 이는 학습 과정을 훨씬 안정적이고 효율적으로 만든다.31</p>
<h3>3.3 클래스 예측: 독립 로지스틱 분류기와 다중 레이블 처리</h3>
<p>YOLOv3는 객체의 클래스를 예측하는 방식에도 중요한 변화를 도입했다. 이전 버전들에서 널리 사용되던 소프트맥스(softmax) 함수를 폐기하고, 각 클래스에 대해 독립적인 로지스틱 분류기(independent logistic classifiers)를 사용하는 방식을 채택했다.4</p>
<p>소프트맥스 함수는 모든 클래스에 대한 예측 확률의 총합이 1이 되도록 강제하는 특징이 있다. 이는 하나의 바운딩 박스는 오직 하나의 클래스에만 속할 수 있다는 상호 배제적인(mutually exclusive) 가정을 내포한다. 그러나 Open Images와 같은 복잡한 데이터셋에서는 ’사람(Person)’과 ’여성(Woman)’처럼 개념적으로 중첩되는 레이블이 하나의 객체에 동시에 적용되는 경우가 흔하다.4 소프트맥스는 이러한 다중 레이블(multi-label) 시나리오를 효과적으로 처리할 수 없다.</p>
<p>반면, 독립적인 로지스틱 분류기는 각 클래스에 대한 예측을 개별적으로 수행한다. 각 클래스마다 시그모이드 함수를 통해 ’해당 클래스일 확률’을 0과 1 사이의 값으로 예측하며, 이 예측들은 서로에게 영향을 주지 않는다. 따라서 하나의 객체에 대해 여러 클래스의 확률이 동시에 높게 나타날 수 있어, 다중 레이블 분류 문제를 자연스럽게 해결할 수 있다.10 학습 과정에서는 각 클래스 예측에 대해 이진 교차 엔트로피(Binary Cross-Entropy, BCE) 손실 함수를 사용하여 모델을 최적화한다.4</p>
<h2>4. 손실 함수 해부</h2>
<h3>4.1 통합 공식의 부재와 구성 요소별 분석</h3>
<p>YOLOv3 원본 논문은 전체 손실 함수를 하나의 통합된 수학 공식으로 명시적으로 제시하지 않는다는 중요한 특징이 있다.4 이는 커뮤니티에서 많은 혼란과 논의를 야기한 지점이다.36 대신, 논문은 전체 손실을 구성하는 각 요소에 어떤 종류의 손실 함수가 사용되는지를 개별적으로 설명하는 방식을 취한다.4 이는 YOLOv3가 객체 탐지라는 복합적인 문제를 지역화(localization), 객체성(objectness), 분류(classification)라는 세 개의 하위 문제로 나누고, 각 문제의 성격에 가장 적합하다고 판단되는 검증된 손실 함수들을 조합한 실용적인 접근 방식을 취했음을 보여준다. 따라서 전체 손실 <span class="math math-inline">L_{total}</span>은 이 세 가지 구성 요소, 즉 지역화 손실(<span class="math math-inline">L_{loc}</span>), 객체성 손실(<span class="math math-inline">L_{obj}</span>), 분류 손실(<span class="math math-inline">L_{cls}</span>)의 가중 합으로 이해할 수 있다.</p>
<h3>4.2 지역화 손실(Localization Loss)</h3>
<p>지역화 손실은 예측된 바운딩 박스의 위치(x, y)와 크기(w, h)가 실제 객체의 위치 및 크기와 얼마나 일치하는지를 측정한다. 논문에서는 좌표 예측에 대해 “제곱합 오차 손실(sum of squared error loss)“을 사용한다고 명시했다.4 이는 L2 손실(L2 loss)과 동일하며, 예측된 원시 값 <span class="math math-inline">t_x, t_y, t_w, t_h</span>와 그라운드 트루 박스로부터 역산된 목표 값 <span class="math math-inline">\hat{t}_x, \hat{t}_y, \hat{t}_w, \hat{t}_h</span> 간의 유클리드 거리 제곱을 최소화하는 것을 목표로 한다. 이 손실은 객체가 존재한다고 판단되는 그리드 셀의 특정 앵커 박스 예측에 대해서만 계산된다. 수식에서 <span class="math math-inline">1_{ij}^{obj}</span>는 그리드 셀 <code>i</code>의 <code>j</code>번째 앵커 박스가 실제 객체 예측을 담당할 경우 1이 되고, 그렇지 않으면 0이 되는 indicator 함수이다.<br />
<span class="math math-display">
L_{loc} = \sum_{i=0}^{S^2} \sum_{j=0}^{B} 1_{ij}^{obj} \left[ (t_x - \hat{t}_x)^2 + (t_y - \hat{t}_y)^2 + (t_w - \hat{t}_w)^2 + (t_h - \hat{t}_h)^2 \right]
</span><br />
원본 다크넷 구현에서는 작은 크기의 바운딩 박스에서 발생하는 오차에 더 큰 가중치를 부여하기 위해 <span class="math math-inline">2 - w \times h</span>와 같은 스케일링 팩터를 손실에 곱해주기도 한다. 이는 작은 객체의 위치 정확도가 전체 성능에 미치는 영향을 증폭시켜 모델이 작은 객체 탐지에 더 집중하도록 유도하는 역할을 한다.35</p>
<h3>4.3 객체성 및 분류 손실(Objectness and Classification Loss)</h3>
<p>객체성 손실은 해당 바운딩 박스 내에 객체가 존재할 확률(confidence 또는 objectness)을 학습하는 것을 목표로 하며, 분류 손실은 그 객체가 어떤 클래스에 속하는지를 학습하는 것을 목표로 한다. YOLOv3는 이 두 가지 분류 성격의 문제에 모두 이진 교차 엔트로피(Binary Cross-Entropy, BCE) 손실을 일관되게 적용한다.10</p>
<ul>
<li><strong>객체성 손실 (<span class="math math-inline">L_{obj}</span>):</strong> 이 손실은 객체가 존재하는 박스(positive sample)와 존재하지 않는 박스(negative sample) 모두에 대해 계산된다.</li>
<li><strong>Positive/Negative 할당:</strong> 각 그라운드 트루 박스에 대해, 모든 앵커 박스와의 IoU(Intersection over Union)를 계산하여 가장 높은 IoU를 가진 앵커 박스가 해당 객체를 예측할 책임(positive sample, 레이블=1)을 진다. 만약 다른 앵커 박스가 특정 임계값(논문에서는 0.5) 이상의 IoU를 가지지만 최고는 아닐 경우, 이 예측은 손실 계산에서 무시된다. 이는 학습 초기에 혼란을 줄 수 있는 애매한 예측들을 배제하기 위함이다.4 나머지 모든 앵커 박스들(IoU가 임계값 미만)은 배경(negative sample, 레이블=0)으로 간주된다.</li>
<li><strong>클래스 불균형:</strong> 이미지 내 대부분의 앵커 박스는 배경에 해당하므로, negative sample의 수가 positive sample보다 압도적으로 많아 클래스 불균형 문제가 발생한다. 이를 완화하기 위해 YOLOv1과 마찬가지로 <span class="math math-inline">λ_{noobj}</span>와 같은 하이퍼파라미터를 사용하여 배경 예측(negative)의 손실에 대한 가중치를 낮춘다.33</li>
<li><strong>분류 손실 (<span class="math math-inline">L_{cls}</span>):</strong> 이 손실은 객체가 존재한다고 할당된 positive sample에 대해서만(<span class="math math-inline">1_{ij}^{obj} = 1</span>) 계산된다. 각 클래스 <code>c</code>에 대해 독립적인 BCE 손실을 계산하고 이를 모두 합산한다.</li>
</ul>
<p>이진 교차 엔트로피 손실의 일반적인 형태는 다음과 같다. 여기서 <span class="math math-inline">\hat{p}</span>는 실제 레이블(0 또는 1), <span class="math math-inline">p</span>는 모델의 예측 확률을 의미한다.39<br />
<span class="math math-display">
L_{BCE}(p, \hat{p}) = -[\hat{p}\log(p) + (1-\hat{p})\log(1-p)]
</span><br />
이처럼 손실 함수를 회귀 문제(지역화)와 분류 문제(객체성, 분류)로 명확히 분리하고, 각 문제에 적합한 표준적인 손실 함수(SSE와 BCE)를 사용하는 ‘분할 정복(divide and conquer)’ 전략은 YOLOv3의 학습 안정성을 크게 향상시켰다. 바운딩 박스 좌표 예측은 본질적으로 연속적인 값을 예측하는 회귀 작업이며, SSE(L2 손실)는 이러한 작업에 가장 표준적이고 안정적인 손실 함수이다. 반면, “객체가 있는가?” 또는 “이것이 고양이인가?“와 같은 질문에 답하는 객체성 및 클래스 예측은 시그모이드 함수를 통해 0과 1 사이의 확률을 출력하는 분류 작업이므로, BCE가 가장 자연스러운 선택이다. 이렇게 각 하위 작업을 분리함으로써 지역화에 대한 그래디언트와 분류에 대한 그래디언트가 복잡하게 얽히지 않고 독립적으로 계산되어 최적화 과정을 단순화한다. 이는 나중에 YOLOv4/v5에서 도입된 GIoU 손실과 같은 통합적인 접근 방식과 대조된다.9 GIoU 손실은 더 많은 정보를 담고 있어 강력하지만 때로는 학습이 불안정할 수 있다. YOLOv3의 더 단순하고 분리된 접근 방식은 구현과 디버깅이 용이하며, 이는 실용성을 중시하는 모델의 철학과도 일치한다.</p>
<p>다음 표는 YOLOv3의 복합적인 손실 함수를 구성 요소별로 명확하게 요약한 것이다.</p>
<p><strong>Table 2: YOLOv3 손실 함수 구성 요소 요약</strong></p>
<table><thead><tr><th>구성 요소</th><th>손실 함수 유형</th><th>수식 (개념적)</th><th>목표</th></tr></thead><tbody>
<tr><td>지역화 (Localization)</td><td>제곱합 오차 (SSE)</td><td><span class="math math-inline">L_{loc} = \sum 1^{obj} (\lVert t - \hat{t} \rVert_2^2)</span></td><td>예측 박스와 실제 박스 좌표 간의 유클리드 거리 최소화</td></tr>
<tr><td>객체성 (Objectness)</td><td>이진 교차 엔트로피 (BCE)</td><td><span class="math math-inline">L_{obj} = \sum 1^{obj} BCE(p_o, 1) + \lambda_{noobj} \sum 1^{noobj} BCE(p_o, 0)</span></td><td>전경(객체)과 배경을 구별하고, 박스 내 객체 존재 신뢰도 예측</td></tr>
<tr><td>분류 (Classification)</td><td>이진 교차 엔트로피 (BCE)</td><td><span class="math math-inline">L_{cls} = \sum 1^{obj} \sum_{c \in classes} BCE(p_c, \hat{p}_c)</span></td><td>객체가 포함된 박스에 대해 올바른 클래스(들)를 예측</td></tr>
</tbody></table>
<h2>5. 성능 평가 및 비교 분석</h2>
<h3>5.1 COCO 데이터셋 벤치마크 결과</h3>
<p>YOLOv3의 성능은 80개의 클래스와 다양한 크기의 객체를 포함하여 객체 탐지 분야의 표준 벤치마크로 자리 잡은 MS COCO 데이터셋을 통해 공식적으로 평가되었다.20 원본 논문에 따르면, YOLOv3는 입력 이미지의 해상도에 따라 속도와 정확도 사이에서 유연한 트레이드오프를 보여주었다.</p>
<ul>
<li><strong>320x320 입력:</strong> 22ms의 빠른 처리 속도로 28.2 mAP 달성</li>
<li><strong>416x416 입력:</strong> 29ms의 처리 속도로 31.0 mAP 달성</li>
<li><strong>608x608 입력:</strong> 51ms의 처리 속도로 33.0 mAP 달성</li>
</ul>
<p>특히 주목할 점은 전통적인 평가 지표인 IoU@0.5 기준의 AP, 즉 AP50에서는 57.9 mAP라는 매우 높은 성능을 기록했다는 것이다.3 이는 COCO의 표준 mAP(IoU 0.5에서 0.95까지 0.05 간격으로 평균)와 비교했을 때 상대적으로 높은 수치이다. 이 차이는 YOLOv3가 객체의 존재를 탐지하는 능력은 매우 뛰어나지만, 2-stage detector에 비해 예측된 바운딩 박스의 위치 정밀도(더 높은 IoU 임계값을 만족시키는 능력)는 다소 부족할 수 있음을 시사한다. 즉, 객체를 ‘찾는’ 데는 탁월하지만, 그 경계를 ‘매우 정밀하게’ 그리는 데는 약간의 한계가 있었던 것이다.17</p>
<h3>5.2 동시대 모델과의 비교</h3>
<p>YOLOv3는 출시 당시 주요 객체 탐지 모델들과의 비교에서 뚜렷한 강점을 보였다.</p>
<ul>
<li><strong>vs. SSD (Single Shot MultiBox Detector):</strong> YOLOv3는 다양한 입력 해상도에서 SSD보다 월등히 빠르면서도 동등하거나 더 나은 mAP를 제공했다. 예를 들어, 320x320 입력에서 SSD321과 정확도는 비슷했지만 속도는 3배 더 빨랐다.10 이는 더 깊고 효율적인 Darknet-53 백본과 다중 스케일 예측의 도입이 성공적이었음을 입증한다.15</li>
<li><strong>vs. Faster R-CNN:</strong> 2-stage detector의 대표주자인 Faster R-CNN은 일반적으로 YOLOv3보다 높은 mAP를 달성했다. 하지만 이는 훨씬 더 복잡한 파이프라인과 높은 연산 비용의 대가였다. Faster R-CNN의 처리 속도는 실시간 적용이 거의 불가능한 수준이었던 반면, YOLOv3는 mAP에서 약간의 손해를 감수하는 대신 8배 이상 빠른 속도를 제공하여 실시간 탐지라는 새로운 시장에서 압도적인 우위를 점했다.15</li>
<li><strong>vs. RetinaNet:</strong> RetinaNet은 Focal Loss라는 혁신적인 손실 함수를 도입하여 1-stage detector의 정확도를 2-stage detector 수준으로 끌어올린 강력한 경쟁자였다. 하지만 YOLOv3는 AP50 기준에서 RetinaNet과 거의 동등한 성능을 보이면서도 처리 속도는 3.8배 더 빨랐다.10 이는 극도의 정확도가 요구되지 않는 대부분의 실시간 애플리케이션에서 YOLOv3가 훨씬 더 매력적인 선택지임을 의미했다.34</li>
</ul>
<h3>5.3 후속 모델과의 비교: YOLOv4 및 YOLOv5</h3>
<p>YOLOv3의 등장은 끝이 아니라 새로운 시작이었다. 그 견고한 아키텍처는 이후 수많은 후속 모델의 기반이 되었다.</p>
<ul>
<li><strong>YOLOv4:</strong> YOLOv3를 직접적인 베이스라인으로 삼아, 당시 연구되던 다양한 최신 기술들을 집대성했다. “Bag of Freebies”(Mosaic 데이터 증강, CIoU 손실 함수, Self-Adversarial Training 등 학습 기법)와 “Bag of Specials”(CSP 백본 구조, PANet 넥 구조, Mish 활성화 함수 등 아키텍처 개선)라는 개념을 도입하여, 추가적인 추론 비용 없이 정확도를 대폭 향상시켰다.19 YOLOv4의 성공은 YOLOv3의 아키텍처가 얼마나 견고하고 확장 가능한 기반이었는지를 역설적으로 증명했다.</li>
<li><strong>YOLOv5:</strong> C++ 기반의 Darknet 프레임워크에서 완전히 벗어나, 파이썬 기반의 PyTorch 네이티브로 구현되었다. 이는 모델의 사용성, 실험, 배포 편의성을 극적으로 향상시켜 YOLO의 대중화를 이끌었다.46 아키텍처적으로는 YOLOv4와 유사하게 CSPDarknet53 백본과 PANet 넥을 채택했지만, Focus layer(초기 버전)와 같은 구조적 개선, 자동 앵커 생성, 최적화된 학습 전략 등을 통해 더 나은 성능과 사용 편의성을 제공했다.46</li>
</ul>
<p>YOLOv3는 현대 실시간 객체 탐지기의 구조적 청사진을 확립했다. YOLOv4와 YOLOv5의 핵심 구조는 여전히 깊은 백본(CSPDarknet), 특징을 융합하는 넥(PANet/FPN), 그리고 다중 스케일 예측 헤드(YOLOv3 Head)로 구성된다.46 이는 YOLOv3가 정립한 패러다임과 정확히 일치한다. 후속 모델들의 성능 향상은 이 근본적인 패러다임을 대체함으로써가 아니라, 그 구성 요소를 더 나은 부품으로 교체하고(예: Darknet-53 -&gt; CSPDarknet53), 모델을 훈련시키는 방법론을 고도화함으로써(예: 데이터 증강, 손실 함수 개선) 달성되었다.42 결국 YOLOv3의 유산은 단순히 그 자체의 성능을 넘어, 이후 수많은 모델들이 더 빠른 엔진과 더 정교한 제어 시스템을 장착할 수 있었던 견고하고 모듈화된 ’섀시(chassis)’를 제공했다는 데 있다.</p>
<p><strong>Table 3: 주요 객체 탐지 모델 성능 비교 (MS COCO 기준)</strong></p>
<table><thead><tr><th>Model</th><th>Backbone</th><th>Input Size</th><th>mAP@[.5:.95] (%)</th><th>AP@.5 (%)</th><th>FPS (Titan X / V100)</th></tr></thead><tbody>
<tr><td>Faster R-CNN</td><td>ResNet-101-FPN</td><td>600 x 1000</td><td>36.2</td><td>59.1</td><td>~7</td></tr>
<tr><td>SSD321</td><td>ResNet-101-SSD</td><td>321 x 321</td><td>28.0</td><td>45.4</td><td>~16</td></tr>
<tr><td>RetinaNet</td><td>ResNet-101-FPN</td><td>500 x 800</td><td>39.1</td><td>59.1</td><td>~9</td></tr>
<tr><td><strong>YOLOv3</strong></td><td><strong>Darknet-53</strong></td><td><strong>608 x 608</strong></td><td><strong>33.0</strong></td><td><strong>57.9</strong></td><td><strong>~20 (51ms)</strong></td></tr>
<tr><td>YOLOv4</td><td>CSPDarknet53</td><td>608 x 608</td><td>43.5</td><td>65.7</td><td>~30</td></tr>
<tr><td>YOLOv5l</td><td>CSPDarknet53</td><td>640 x 640</td><td>49.0</td><td>67.3</td><td>~45</td></tr>
</tbody></table>
<p><em>주: FPS는 하드웨어 및 구현에 따라 달라질 수 있으며, 상대적인 비교를 위한 값임.</em></p>
<h2>6. 결론: YOLOv3의 유산과 영향</h2>
<h3>6.1 YOLOv3의 기술적 성과와 한계</h3>
<p>YOLOv3는 객체 탐지 분야에 중요한 기술적 이정표를 남겼다. 가장 큰 성과는 실시간 객체 탐지의 새로운 기준을 제시했다는 점이다. Darknet-53이라는 효율적이면서도 깊은 백본, FPN-유사 구조를 통한 다중 스케일 예측, 다중 레이블 분류를 위한 독립 로지스틱 분류기 등 당대의 성공적인 아이디어들을 실용적으로 통합하여 속도와 정확도의 균형을 전례 없는 수준으로 끌어올렸다. 이는 학계를 넘어 산업계 전반에 딥러닝 기반 객체 탐지 기술의 적용을 가속화하는 기폭제가 되었다.</p>
<p>그러나 명확한 한계 또한 존재했다. 동시대의 2-stage detector에 비해 바운딩 박스의 위치 정밀도가 상대적으로 낮아, 높은 IoU 임계값에서 mAP가 저하되는 경향을 보였다.17 또한, 원본 논문에서 손실 함수나 구체적인 학습 전략에 대한 설명이 불분명하여, 모델을 완벽하게 재현하고 개선하는 데 커뮤니티의 집단적인 리버스 엔지니어링과 분석 노력이 필요했다.11</p>
<h3>6.2 현대 객체 탐지 모델에 미친 영향</h3>
<p>이러한 한계에도 불구하고 YOLOv3가 현대 객체 탐지 모델에 미친 영향은 지대하다. YOLOv3가 정립한 ’백본-넥-헤드’의 모듈식 아키텍처와 다중 스케일 예측 패러다임은 이후 등장한 거의 모든 실시간 객체 탐지 모델의 표준적인 설계로 자리 잡았다.46 YOLOv4, YOLOv5, YOLOX, PP-YOLO 등 수많은 후속 연구들은 YOLOv3를 강력한 베이스라인으로 삼아, 더 나은 백본(CSPNet), 넥(PANet, BiFPN), 손실 함수(CIoU, Focal Loss), 레이블 할당 전략(SimOTA) 등을 추가하는 방식으로 발전했다.9 이는 YOLOv3의 아키텍처가 근본적으로 우수하고 뛰어난 확장성을 지녔음을 증명하는 것이다.</p>
<h3>6.3 향후 발전 방향에 대한 고찰</h3>
<p>YOLOv3의 성공은 ’점진적이고 실용적인 개선’의 힘을 명확히 보여준다. 이는 향후 연구가 반드시 완전히 새로운 패러다임을 제시해야만 가치가 있는 것이 아니라, 기존의 견고한 아키텍처 위에서 학습 방법론, 데이터 처리, 손실 함수 설계 등 다양한 측면의 최적화를 통해 최고 수준의 성능(State-Of-The-Art)을 달성할 수 있음을 시사한다. YOLOv3로부터 시작된 실시간 탐지 모델의 발전은 이제 단순히 mAP 수치를 높이는 것을 넘어, 모델 경량화, 추론 효율성, 그리고 특정 하드웨어(예: 엣지 디바이스, 모바일 기기)에 대한 최적화로 그 초점을 옮겨가고 있다. YOLOv3가 객체 탐지 분야에 깊이 각인시킨 ’실용성’의 철학은 앞으로도 이 분야의 발전을 이끄는 핵심 동력으로 작용할 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>[PDF] YOLOv3: An Incremental Improvement - Semantic Scholar, https://www.semanticscholar.org/paper/YOLOv3%3A-An-Incremental-Improvement-Redmon-Farhadi/ebc96892b9bcbf007be9a1d7844e4b09fde9d961</li>
<li>‪Joseph Redmon‬ - ‪Google Scholar‬, https://scholar.google.com/citations?user=TDk_NfkAAAAJ&amp;hl=en</li>
<li>[1804.02767] YOLOv3: An Incremental Improvement - arXiv, https://arxiv.org/abs/1804.02767</li>
<li>YOLOv3: An Incremental Improvement, https://ask.qcloudimg.com/draft/2661027/xvkfa6hvyg.pdf</li>
<li>yolov4 - arXiv, <a href="https://arxiv.org/pdf/2502.04161">https://arxiv.org/pdf/2502.04161?</a></li>
<li>[1506.02640] You Only Look Once: Unified, Real-Time Object Detection - arXiv, https://arxiv.org/abs/1506.02640</li>
<li>Improved YOLO-V3 with DenseNet for Multi-Scale Remote Sensing Target Detection - MDPI, https://www.mdpi.com/1424-8220/20/15/4276</li>
<li>What is YOLOv3? An Introductory Guide. - Roboflow Blog, https://blog.roboflow.com/what-is-yolov3/</li>
<li>(PDF) A Review on YOLOv8 and Its Advancements - ResearchGate, https://www.researchgate.net/publication/377216968_A_Review_on_YOLOv8_and_Its_Advancements</li>
<li>YOLOv3 - Tethys, https://tethys.pnnl.gov/sites/default/files/publications/Redmonetal.pdf</li>
<li>[R] YOLOv3: An Incremental Improvement : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/877ahu/r_yolov3_an_incremental_improvement/</li>
<li>Darknet-53 architecture adopted by YOLOv3 (from [13]). - ResearchGate, https://www.researchgate.net/figure/Darknet-53-architecture-adopted-by-YOLOv3-from-13_fig4_336646881</li>
<li>An Incremental Improvement with Darknet-53 and Multi-Scale Predictions (YOLOv3), https://pyimagesearch.com/2022/05/09/an-incremental-improvement-with-darknet-53-and-multi-scale-predictions-yolov3/</li>
<li>The structure of YOLOv3. YOLOv3 predicts objects at three different scales and finally combines the results to get the final detection. YOLO - ResearchGate, https://www.researchgate.net/figure/The-structure-of-YOLOv3-YOLOv3-predicts-objects-at-three-different-scales-and-finally_fig1_343170773</li>
<li>Comparison of YOLO v3, Faster R-CNN, and SSD for Real-Time Pill Identification, https://www.researchgate.net/publication/353590069_Comparison_of_YOLO_v3_Faster_R-CNN_and_SSD_for_Real-Time_Pill_Identification</li>
<li>A Comparative Analysis of Modern Object Detection Algorithms: YOLO vs. SSD vs. Faster R-CNN, https://syekhnurjati.ac.id/journal/index.php/itej/article/download/123/84/</li>
<li>Object detection: speed and accuracy comparison (Faster R-CNN, R-FCN, SSD, FPN, RetinaNet and YOLOv3) - Jonathan Hui, https://jonathan-hui.medium.com/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359</li>
<li>YOLOv3: An Incremental Improvement | Request PDF - ResearchGate, https://www.researchgate.net/publication/324387691_YOLOv3_An_Incremental_Improvement</li>
<li>arXiv:2007.12099v3 [cs.CV] 3 Aug 2020, https://arxiv.org/pdf/2007.12099</li>
<li>A Study on Object Detection Performance of YOLOv4 for Autonomous Driving of Tram, https://www.mdpi.com/1424-8220/22/22/9026</li>
<li>Darknet 53 - GeeksforGeeks, https://www.geeksforgeeks.org/computer-vision/darknet-53/</li>
<li>A Comprehensive Guide To YOLOv3!!! | by Atharva Musale - Medium, https://atharvamusale.medium.com/a-comprehensive-guide-to-yolov3-74029810ca81</li>
<li>YOLOv3 Object Detection - Medium, https://medium.com/@Shahidul1004/yolov3-object-detection-f3090a24efcd</li>
<li>yolov3/docs/darknet.md at master - GitHub, https://github.com/conan7882/yolov3/blob/master/docs/darknet.md</li>
<li>YOLOv3 with Spatial Pyramid Pooling for Object Detection with Unmanned Aerial Vehicles - arXiv, https://arxiv.org/pdf/2305.12344</li>
<li>An improved helmet detection method for YOLOv3 on an unbalanced dataset - arXiv, https://arxiv.org/pdf/2011.04214</li>
<li>Full article: An improved YOLOv3 model based on skipping connections and spatial pyramid pooling - Taylor &amp; Francis Online, https://www.tandfonline.com/doi/full/10.1080/21642583.2020.1824132</li>
<li>YOLOv3 Predicting bounding boxes for grid containing multiple centers, https://datascience.stackexchange.com/questions/92307/yolov3-predicting-bounding-boxes-for-grid-containing-multiple-centers</li>
<li>YOLO v3 Explained. How YOLOv3 works, from capturing the… | by Hassan Rady | Medium, https://medium.com/@HassanRady/yolo-v3-explained-39881163a27f</li>
<li>Evaluation of Deep Learning YOLOv3 Algorithm for Object Detection and Classification - Menoufia Journal of Electronic Engineering Research, https://mjeer.journals.ekb.eg/article_146237_c087103bc03c214cadab46549e8a8bdc.pdf</li>
<li>Anchor Boxes in YOLO : How are they decided - Stack Overflow, https://stackoverflow.com/questions/52710248/anchor-boxes-in-yolo-how-are-they-decided</li>
<li>Anchor Boxes Generation using K-Means clustering | by Yerdaulet Zhumabay | Medium, https://medium.com/@yerdaulet.zhumabay/generating-anchor-boxes-by-k-means-82f11c690b82</li>
<li>Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3 …, https://jonathan-hui.medium.com/real-time-object-detection-with-yolo-yolov2-28b1b93e2088</li>
<li>YOLOv3: Real-Time Object Detection Algorithm (Guide) - Viso Suite, https://viso.ai/deep-learning/yolov3-overview/</li>
<li>TensorFlow 2 YOLOv3 Mnist detection training tutorial - PyLessons.com, https://pylessons.com/YOLOv3-TF2-mnist</li>
<li>YOLOv3 loss function - Cross Validated - Stack Exchange, https://stats.stackexchange.com/questions/380012/yolov3-loss-function</li>
<li>Yolo v3 loss function - neural networks - Stats Stackexchange, https://stats.stackexchange.com/questions/373266/yolo-v3-loss-function</li>
<li>What is the loss function of YOLOv3 - Stack Overflow, https://stackoverflow.com/questions/55395205/what-is-the-loss-function-of-yolov3</li>
<li>Cross Entropy for YOLOv3 · Issue #1354 · pjreddie/darknet - GitHub, https://github.com/pjreddie/darknet/issues/1354</li>
<li>BCELoss — PyTorch 2.8 documentation, https://docs.pytorch.org/docs/stable/generated/torch.nn.BCELoss.html</li>
<li>Binary Cross Entropy: Where To Use Log Loss In Model Monitoring - Arize AI, https://arize.com/blog-course/binary-cross-entropy-log-loss/</li>
<li>YOLOv4: A Breakthrough in Real-Time Object Detection - arXiv, https://arxiv.org/html/2502.04161v1</li>
<li>[PDF] Comparison of YOLO v3, Faster R-CNN, and SSD for Real-Time Pill Identification, https://www.semanticscholar.org/paper/6395a83fce36e8aea818989f75a160367cb32144</li>
<li>Real-Time Object Detection: Comparing YOLO and SSD Architectures in Surveillance Systems - Science Excel, https://www.sciencexcel.com/articles/WctZlQ6tKQTUDzPNXlUdq1MfOKtAAT10ZNaSZXrZ.pdf</li>
<li>Improved YOLOv4-tiny based on attention mechanism for skin detection - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC10280476/</li>
<li>Comparing YOLOv3, YOLOv4 and YOLOv5 for Autonomous Landing Spot Detection in Faulty UAVs - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC8778480/</li>
<li>What is YOLOv5: A deep look into the internal features of the popular object detector - arXiv, https://arxiv.org/html/2407.20892v1</li>
<li>Ultralytics YOLOv5 Architecture, https://docs.ultralytics.com/yolov5/tutorials/architecture_description/</li>
<li>Comparison between structures of YOLOv3, YOLOv4 and YOLOv5. - ResearchGate, https://www.researchgate.net/figure/Comparison-between-structures-of-YOLOv3-YOLOv4-and-YOLOv5_tbl1_357684232</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>