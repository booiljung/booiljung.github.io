<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:YOLOv2 (2016)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>YOLOv2 (2016)</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">객체 탐지 (Object Detection)</a> / <a href="index.html">YOLO 객체 탐지 모델</a> / <span>YOLOv2 (2016)</span></nav>
                </div>
            </header>
            <article>
                <h1>YOLOv2 (2016)</h1>
<h2>1.  실시간 객체 탐지의 새로운 지평</h2>
<p>2016년 컴퓨터 비전 분야, 특히 객체 탐지(Object Detection) 영역은 정확도와 속도라는 두 가지 상충하는 목표 사이에서 중요한 기로에 서 있었다. 당시 객체 탐지 모델들은 크게 두 가지 패러다임으로 양분되어 있었다. 첫 번째는 R-CNN(Regions with CNN) 계열로 대표되는 2단계 탐지기(Two-Stage Detector)이며, 두 번째는 YOLO(You Only Look Once)와 SSD(Single Shot MultiBox Detector)가 주도하는 1단계 탐지기(One-Stage Detector)이다.1</p>
<p>2단계 탐지기는 ‘제안 후 분류(propose-then-classify)’ 방식으로 동작한다. Faster R-CNN과 같은 모델은 먼저 Region Proposal Network (RPN)를 사용하여 객체가 존재할 가능성이 높은 후보 영역(region proposals)을 생성한 후, 각 후보 영역에 대해 분류(classification)와 경계 상자 회귀(bounding box regression)를 수행한다.4 이 접근법은 각 영역을 면밀히 분석하므로 매우 높은 정확도를 달성할 수 있었으나, 다단계 파이프라인으로 인해 추론 속도가 GPU 환경에서도 초당 5-7 프레임(FPS)에 불과하여 실시간 응용에는 부적합했다.1</p>
<p>반면, 1단계 탐지기는 객체 탐지를 하나의 통합된 회귀 문제로 재정의했다. 입력 이미지를 한 번만 네트워크에 통과시켜 모든 경계 상자와 클래스 확률을 직접 예측하는 단일 파이프라인을 채택함으로써, 실시간 처리가 가능한 혁신적인 속도를 구현했다.1 그러나 이러한 속도는 정확도와의 트레이드오프 관계에 있었다. 특히 YOLO의 첫 번째 버전인 YOLOv1은 속도 면에서는 혁명적이었지만, 몇 가지 명백한 한계를 가지고 있었다.</p>
<p>YOLOv1의 가장 큰 문제점은 2단계 탐지기에 비해 현저히 높은 위치 결정 오류(localization error)였다.10 손실 함수가 큰 경계 상자의 작은 오차와 작은 경계 상자의 작은 오차를 동일하게 취급하여, 특히 작은 객체의 위치를 정밀하게 예측하는 데 어려움을 겪었다.7 또한, 각 그리드 셀(grid cell)이 단 하나의 클래스와 두 개의 경계 상자만을 예측하는 공간적 제약으로 인해, 객체 재현율(recall)이 낮았으며 새 떼와 같이 작은 객체들이 밀집한 경우를 제대로 탐지하지 못했다.11 더불어, 경계 상자 좌표 예측에 제약이 없어 훈련 초기 단계가 불안정해지는 경향도 있었다.11</p>
<p>이러한 배경 속에서 2017년, Joseph Redmon과 Ali Farhadi는 “YOLO9000: Better, Faster, Stronger“라는 논문을 통해 YOLOv2를 발표했다.17 이 연구의 목표는 단순히 YOLOv1을 개선하는 것을 넘어, 속도와 정확도 사이의 가파른 상충 관계 곡선을 완만하게 만들어 실시간 객체 탐지의 실용성을 극대화하는 것이었다. YOLOv2는 속도를 유지하면서 정확도를 대폭 향상시키고(‘Better’), 더 효율적인 네트워크 아키텍처를 통해 속도를 더욱 개선하며(‘Faster’), 데이터셋 결합과 계층적 분류를 통해 탐지 가능한 객체의 범주를 9000개 이상으로 확장(‘Stronger’)하는 세 가지 핵심 목표를 제시했다.</p>
<p>YOLOv2의 등장은 단순한 점진적 개선 이상의 의미를 지녔다. 이는 1단계 탐지기가 복잡한 2단계 시스템의 정확도에 근접하면서도 훨씬 빠른 속도를 유지할 수 있음을 입증함으로써, 실시간 고정밀 객체 탐지가 더 이상 일부 특수 목적을 위한 기술이 아닌, 자율 주행, 로보틱스, 임베디드 시스템 등 광범위한 분야에 적용 가능한 보편적 기술이 될 수 있다는 가능성을 제시했다.8 이로 인해 학계와 산업계는 1단계 탐지기의 잠재력을 재평가하게 되었고, 이후 RetinaNet과 같은 후속 연구들이 정확도 격차를 완전히 해소하려는 노력으로 이어지는 계기가 되었다. 본 안내서는 YOLOv2를 구성하는 핵심 기술들을 “Better”, “Faster”, “Stronger“라는 세 가지 축을 중심으로 심층적으로 고찰하고, 그 성능과 한계, 그리고 객체 탐지 분야에 미친 영향을 종합적으로 분석하고자 한다.</p>
<h2>2.  YOLOv2의 핵심 개선점: “더 정확하게 (Better)”</h2>
<p>YOLOv2는 YOLOv1의 주요 약점이었던 낮은 재현율과 부정확한 위치 결정을 해결하기 위해 기존 연구에서 입증된 기법들과 독창적인 아이디어를 체계적으로 결합했다. 이러한 개선 사항들은 모델의 정확도를 비약적으로 향상시켰으며, 각각의 기법은 상호 보완적으로 작용하여 시너지를 창출했다.</p>
<h3>2.1  배치 정규화 (Batch Normalization)</h3>
<p>가장 먼저 도입된 변화는 모든 컨볼루션 계층(convolutional layer) 뒤에 배치 정규화(Batch Normalization, BN)를 추가한 것이다. BN은 각 계층에 입력되는 데이터의 분포를 평균 0, 분산 1로 정규화하여 학습 과정을 안정시킨다. 이는 내부 공변량 변화(internal covariate shift) 문제를 완화하여 그래디언트 소실 또는 폭주를 방지하고, 더 높은 학습률(learning rate)을 사용할 수 있게 하여 수렴 속도를 높인다.23 또한, BN은 미미한 규제(regularization) 효과를 제공하여 과적합(overfitting)을 억제하는 데 도움을 주므로, 드롭아웃(dropout)과 같은 다른 규제 기법의 필요성을 줄일 수 있다.16 이 단순하면서도 강력한 기법의 도입만으로 YOLOv2는 평균 정밀도(mean Average Precision, mAP)에서 약 2%의 성능 향상을 달성했다.16</p>
<h3>2.2  고해상도 분류기 (High-Resolution Classifier)</h3>
<p>YOLOv1은 224x224 해상도의 ImageNet 이미지로 분류기(classifier)를 사전 훈련(pre-training)한 후, 곧바로 448x448 해상도의 이미지로 탐지기(detector)를 훈련했다. 이 갑작스러운 해상도 변화는 네트워크가 새로운 고해상도 특징에 적응하는 동시에 객체 탐지라는 복잡한 과업을 학습해야 하는 부담을 주었다.7 YOLOv2는 이 문제를 해결하기 위해 단계적인 접근법을 취했다. 먼저 Darknet-19 분류기 모델을 224x224 해상도로 사전 훈련한 뒤, 탐지기 훈련에 앞서 448x448 해상도의 ImageNet 이미지로 10 에폭(epoch) 동안 추가적으로 미세 조정(fine-tuning)했다.11 이 중간 단계를 통해 네트워크의 컨볼루션 필터들이 고해상도 입력에 미리 적응할 수 있었고, 이는 탐지기 훈련의 안정성과 성능을 크게 향상시켰다. 이 해상도 보정(resolution calibration) 과정은 mAP를 약 4%나 끌어올리는 중요한 성과를 거두었다.16</p>
<h3>2.3  앵커 박스를 이용한 완전 컨볼루션 네트워크 (Fully Convolutional Network with Anchor Boxes)</h3>
<p>YOLOv2는 구조적으로 큰 변화를 겪었다. YOLOv1의 완전 연결 계층(fully connected layer)을 모두 제거하여 완전 컨볼루션 네트워크(fully convolutional network)로 전환했다.20 이 변화는 모델이 다양한 크기의 입력 이미지를 처리할 수 있게 하는 유연성을 부여했다.</p>
<p>또한, YOLOv1이 그리드 셀의 특징으로부터 직접 경계 상자의 좌표를 예측하던 방식에서 벗어나, Faster R-CNN에서 도입되어 큰 성공을 거둔 앵커 박스(anchor box) 메커니즘을 채택했다.11 앵커 박스는 미리 정의된 다양한 크기와 종횡비(aspect ratio)를 가진 기준 상자(prior box)들이다. 네트워크는 이제 경계 상자의 절대 좌표를 예측하는 대신, 각 그리드 셀에 위치한 여러 앵커 박스로부터의 상대적인 오프셋(offset)과 신뢰도 점수(confidence score)를 예측한다.16 이 방식은 각 예측기가 특정 형태의 객체를 전담하도록 유도하여 학습을 용이하게 하고, 그리드 셀당 예측 가능한 객체의 수를 늘려 재현율을 높이는 데 결정적인 역할을 한다. 이 변화의 결과는 매우 의미 있었다. mAP는 69.5%에서 69.2%로 소폭 감소했지만, 재현율은 81%에서 88%로 대폭 상승했다. 이는 모델이 이미지 내의 객체들을 놓치지 않고 찾아내는 능력이 크게 향상되었음을 의미하며, 정확도를 약간 희생하더라도 재현율을 높이는 것이 더 중요한 개선 방향이라고 판단한 결과이다.16</p>
<h3>2.4  차원 클러스터링 (Dimension Clusters)</h3>
<p>앵커 박스를 도입했지만, YOLOv2는 기존 방식에 머무르지 않았다. Faster R-CNN 등에서는 앵커 박스의 크기와 종횡비를 경험적으로 수동 선택했다. 이는 데이터셋에 존재하는 실제 객체들의 형태 분포와 일치하지 않을 수 있어 비효율을 야기할 수 있었다.16 YOLOv2는 이 문제를 해결하기 위해 데이터 기반의 독창적인 접근법인 차원 클러스터링(Dimension Clusters)을 제안했다.</p>
<p>훈련 데이터셋에 있는 모든 실제 경계 상자(ground truth box)의 너비와 높이 차원을 대상으로 k-평균 군집화(k-means clustering) 알고리즘을 실행하여 최적의 앵커 박스 형태(prior)를 자동으로 찾아냈다.16 여기서 핵심은 거리 측정 방식이다. 일반적인 유클리드 거리(Euclidean distance)를 사용하면 큰 상자의 오차가 작은 상자의 오차보다 더 크게 계산되는 문제가 발생한다. 이를 방지하기 위해 상자 크기와 무관하게 오차를 측정할 수 있는 새로운 거리 함수를 정의했다. 이 함수는 두 상자 간의 Intersection over Union (IOU) 값을 사용한다.16<br />
<span class="math math-display">
d(\text{box}, \text{centroid}) = 1 - \text{IOU}(\text{box}, \text{centroid})
</span><br />
분석 결과, 클러스터의 수 <span class="math math-inline">k=5</span> 때 모델 복잡도와 재현율 간의 균형이 가장 좋았다. 이렇게 데이터로부터 학습된 5개의 앵커 박스는 평균 IOU 61.0%를 기록했는데, 이는 Faster R-CNN에서 수동으로 선택한 9개의 앵커 박스가 기록한 60.9%와 유사한 수준이었다. 즉, 더 적은 수의 앵커 박스로 더 효율적인 예측이 가능해진 것이다.16</p>
<h3>2.5  직접 위치 예측 (Direct Location Prediction)</h3>
<p>앵커 박스를 사용한 오프셋 예측 방식은 훈련 초기에 불안정성을 유발할 수 있다. 제약 조건이 없는 오프셋은 예측된 경계 상자의 중심이 해당 예측을 담당하는 그리드 셀의 범위를 크게 벗어나는 것을 허용하기 때문이다.15 YOLOv2는 이 문제를 해결하기 위해 예측 값에 제약을 가하는 직접 위치 예측(Direct Location Prediction) 메커니즘을 도입했다.</p>
<p>네트워크는 각 앵커 박스에 대해 5개의 값, 즉 <span class="math math-inline">t_x, t_y, t_w, t_h, t_o</span>를 예측한다. 예측된 경계 상자의 중심 좌표 <span class="math math-inline">(b_x, b_y)</span>는 해당 그리드 셀의 좌상단 좌표 <span class="math math-inline">(c_x, c_y)</span>에 상대적인 위치로 계산된다. 이때, <span class="math math-inline">t_x</span>와 <span class="math math-inline">t_y</span>에 시그모이드(sigmoid) 함수 <span class="math math-inline">σ</span>를 적용하여 그 값을 0과 1 사이로 제한한다. 이로써 예측된 중심점은 항상 해당 그리드 셀 내부에 위치하게 된다. 경계 상자의 너비와 높이 <span class="math math-inline">(b_w, b_h)</span>는 앵커 박스의 크기 <span class="math math-inline">(p_w, p_h)</span>에 지수 함수를 적용하여 계산된다. 최종적인 수식은 다음과 같다.16<br />
<span class="math math-display">
b_x = \sigma(t_x) + c_x \\
b_y = \sigma(t_y) + c_y \\
b_w = p_w e^{t_w} \\
b_h = p_h e^{t_h} \\
\text{Pr}(\text{object}) \times \text{IOU}(b, \text{object}) = \sigma(t_o)
</span><br />
이러한 제약은 모델의 학습을 안정시키고 예측의 일관성을 높여주었다. 차원 클러스터링과 직접 위치 예측 기법을 함께 적용한 결과, 앵커 박스만 사용했을 때보다 mAP가 약 5% 향상되는 효과를 보였다.16</p>
<h3>2.6  세분화된 특징 (Fine-Grained Features)</h3>
<p>YOLOv1의 주요 약점 중 하나는 작은 객체 탐지 성능이 떨어진다는 것이었다. 이는 최종적으로 사용되는 13x13 크기의 저해상도 특징 맵(feature map)이 큰 객체를 탐지하는 데는 충분하지만, 작은 객체를 구별하는 데 필요한 세밀한 공간 정보를 상당 부분 잃어버리기 때문이다.16 YOLOv2는 이 문제를 해결하기 위해 패스스루 계층(passthrough layer)이라는 독창적인 방법을 도입했다.</p>
<p>이 계층은 네트워크의 더 이전 단계에 있는 고해상도 특징 맵(예: 26x26x512)을 가져와 최종 특징 맵과 결합한다. 단순히 두 특징 맵을 연결하는 것이 아니라, 고해상도 특징 맵을 재구성하는 방식을 사용한다. 26x26x512 크기의 특징 맵을 공간적으로 인접한 특징들을 서로 다른 채널로 쌓는 방식으로 재배열하여 13x13x2048 크기의 특징 맵으로 변환한다. 이렇게 변환된 고해상도 특징 맵을 기존의 저해상도 특징 맵(13x13x1024)과 채널 차원에서 연결(concatenate)하여 최종적으로 13x13x3072 크기의 풍부한 특징 맵을 생성한다.16 이 기법은 작은 객체 탐지에 필요한 세분화된 정보를 보존하고 활용할 수 있게 하여 mAP를 1% 추가로 향상시켰다.16</p>
<h3>2.7  다중 스케일 훈련 (Multi-Scale Training)</h3>
<p>완전 컨볼루션 구조로 변경되면서 YOLOv2는 다양한 크기의 입력 이미지를 처리할 수 있는 능력을 갖추게 되었다. 연구진은 이를 모델의 강인함(robustness)을 높이는 데 적극적으로 활용했다. 다중 스케일 훈련(Multi-Scale Training)은 훈련 과정에서 주기적으로 입력 이미지의 해상도를 변경하는 기법이다. 10 배치(batch)마다 320x320부터 608x608까지 32 픽셀 간격으로 새로운 이미지 차원을 무작위로 선택하여 네트워크를 훈련시킨다.11</p>
<p>이 방식은 네트워크가 다양한 스케일의 객체에 대해 강인하게 학습하도록 만드는 효과적인 데이터 증강(data augmentation) 역할을 한다. 더 중요한 것은, 이렇게 훈련된 단일 모델이 추론 시에도 다양한 해상도에서 실행될 수 있다는 점이다.29 사용자는 별도의 모델을 재훈련할 필요 없이, 단순히 입력 이미지의 크기를 조절하는 것만으로 속도와 정확도 사이의 균형을 손쉽게 선택할 수 있다. 예를 들어, 저해상도 입력(예: 288x288)에서는 90 FPS 이상의 매우 빠른 속도를 달성하면서도 Fast R-CNN에 필적하는 정확도를 보였고, 고해상도 입력에서는 최고의 정확도를 얻을 수 있었다.16</p>
<p>YOLOv2의 “Better” 측면은 단편적인 개선들의 단순한 집합이 아니다. 이는 문제 진단, 기존 해결책 탐색, 그리고 필요에 따른 독창적 발명을 체계적으로 결합한 실용적 엔지니어링의 정수를 보여준다. 재현율 부족이라는 문제를 해결하기 위해 앵커 박스를 채택했지만, 수동 선택의 비효율성을 인지하고 데이터 기반의 차원 클러스터링을 발명했다. 앵커 박스 도입으로 인한 훈련 불안정성은 직접 위치 예측이라는 제약 메커니즘으로 해결했다. 동시에 배치 정규화와 같은 업계 표준 기술을 도입하여 학습의 기반을 다지고, 고해상도 분류기 미세 조정으로 태스크 전환의 충격을 완화했다. 마지막으로, 고질적인 작은 객체 문제를 해결하기 위해 패스스루 계층이라는 맞춤형 특징 융합 기법을 고안했다. 이러한 개선 사항들이 서로 맞물려 돌아가면서, 각 부분의 합을 훨씬 뛰어넘는 강력한 시너지 효과를 창출했고, 이는 YOLOv2를 당대 최고의 실시간 객체 탐지기로 만드는 원동력이 되었다.</p>
<h2>3.  YOLOv2의 속도 혁신: “더 빠르게 (Faster)” - Darknet-19 아키텍처 분석</h2>
<p>YOLOv2의 경이로운 속도는 단순히 알고리즘 개선만으로 이룰 수 있는 것이 아니었다. 그 중심에는 “더 빠르게(Faster)“라는 목표를 달성하기 위해 처음부터 효율성을 극대화하도록 설계된 맞춤형 백본 네트워크(backbone network), Darknet-19가 있었다.</p>
<h3>3.1  설계 철학 및 구조</h3>
<p>당시 대부분의 최첨단 탐지기들은 VGG-16과 같이 정확도는 높지만 매우 무겁고 복잡한 네트워크를 특징 추출기(feature extractor)로 사용했다.31 VGG-16은 224x224 크기의 단일 이미지를 처리하는 데 약 306.9억 번의 부동소수점 연산(Floating Point Operations, FLOPS)을 필요로 했다.11 이는 실시간 처리에 큰 병목으로 작용했다.</p>
<p>YOLOv2의 저자들은 이러한 의존성에서 벗어나, 정확도의 큰 손실 없이 연산 효율을 극대화하는 것을 목표로 Darknet-19를 설계했다. Darknet-19는 기존의 성공적인 네트워크 디자인 원칙들을 차용했다. VGG 모델처럼 주로 3x3 크기의 컨볼루션 필터를 사용하고, 각 풀링 계층(pooling layer)을 지날 때마다 채널 수를 두 배로 늘리는 방식을 따랐다.23 또한, Network in Network (NIN)의 아이디어를 따라 전역 평균 풀링(global average pooling)을 사용하여 최종 예측을 수행하고, 1x1 컨볼루션 필터를 적극적으로 활용하여 특징 맵의 채널 수를 압축함으로써 파라미터 수를 줄이고 계산 효율을 높였다.23</p>
<h3>3.2  아키텍처 세부 구성 및 연산 효율성</h3>
<p>Darknet-19는 이름에서 알 수 있듯이 19개의 컨볼루션 계층과 5개의 최대 풀링(max-pooling) 계층으로 구성된다. 아래 표는 Darknet-19의 분류 모델 아키텍처를 상세히 보여준다.</p>
<p><strong>표 1: Darknet-19 분류 네트워크 아키텍처</strong></p>
<table><thead><tr><th>계층 유형</th><th>필터 수</th><th>커널 크기 / 스트라이드</th><th>출력 해상도</th></tr></thead><tbody>
<tr><td>Convolutional</td><td>32</td><td>3x3 / 1</td><td>416x416</td></tr>
<tr><td>Maxpool</td><td></td><td>2x2 / 2</td><td>208x208</td></tr>
<tr><td>Convolutional</td><td>64</td><td>3x3 / 1</td><td>208x208</td></tr>
<tr><td>Maxpool</td><td></td><td>2x2 / 2</td><td>104x104</td></tr>
<tr><td>Convolutional</td><td>128</td><td>3x3 / 1</td><td>104x104</td></tr>
<tr><td>Convolutional</td><td>64</td><td>1x1 / 1</td><td>104x104</td></tr>
<tr><td>Convolutional</td><td>128</td><td>3x3 / 1</td><td>104x104</td></tr>
<tr><td>Maxpool</td><td></td><td>2x2 / 2</td><td>52x52</td></tr>
<tr><td>Convolutional</td><td>256</td><td>3x3 / 1</td><td>52x52</td></tr>
<tr><td>Convolutional</td><td>128</td><td>1x1 / 1</td><td>52x52</td></tr>
<tr><td>Convolutional</td><td>256</td><td>3x3 / 1</td><td>52x52</td></tr>
<tr><td>Maxpool</td><td></td><td>2x2 / 2</td><td>26x26</td></tr>
<tr><td>Convolutional</td><td>512</td><td>3x3 / 1</td><td>26x26</td></tr>
<tr><td>Convolutional</td><td>256</td><td>1x1 / 1</td><td>26x26</td></tr>
<tr><td>Convolutional</td><td>512</td><td>3x3 / 1</td><td>26x26</td></tr>
<tr><td>Convolutional</td><td>256</td><td>1x1 / 1</td><td>26x26</td></tr>
<tr><td>Convolutional</td><td>512</td><td>3x3 / 1</td><td>26x26</td></tr>
<tr><td>Maxpool</td><td></td><td>2x2 / 2</td><td>13x13</td></tr>
<tr><td>Convolutional</td><td>1024</td><td>3x3 / 1</td><td>13x13</td></tr>
<tr><td>Convolutional</td><td>512</td><td>1x1 / 1</td><td>13x13</td></tr>
<tr><td>Convolutional</td><td>1024</td><td>3x3 / 1</td><td>13x13</td></tr>
<tr><td>Convolutional</td><td>512</td><td>1x1 / 1</td><td>13x13</td></tr>
<tr><td>Convolutional</td><td>1024</td><td>3x3 / 1</td><td>13x13</td></tr>
<tr><td>—</td><td>—</td><td>—</td><td>—</td></tr>
<tr><td>Convolutional (for classification)</td><td>1000</td><td>1x1 / 1</td><td>13x13</td></tr>
<tr><td>Global Avgpool</td><td></td><td></td><td>1x1</td></tr>
<tr><td>Softmax</td><td></td><td></td><td>1000</td></tr>
</tbody></table>
<p>객체 탐지를 위해서는 위 아키텍처의 마지막 컨볼루션 계층(분류용)을 제거하고, 대신 탐지에 필요한 출력을 생성하기 위한 새로운 계층들을 추가한다. 구체적으로, 1024개의 필터를 가진 3x3 컨볼루션 계층 3개와, 최종 예측 텐서를 생성하기 위한 1x1 컨볼루션 계층 1개를 추가한다.11 예를 들어 PASCAL VOC 데이터셋(20개 클래스)에 대해 그리드 셀당 5개의 앵커 박스를 사용한다면, 각 앵커 박스는 5개의 좌표값(<span class="math math-inline">t_x, t_y, t_w, t_h, t_o</span>)과 20개의 클래스 확률을 예측해야 하므로, 최종 1x1 컨볼루션 계층은 <span class="math math-inline">(5+20) \times 5 = 125</span>개의 필터를 갖게 된다.23</p>
<p>Darknet-19의 가장 두드러지는 장점은 압도적인 연산 효율성이다. 224x224 해상도 이미지에 대한 순방향 전파(forward pass)에 필요한 연산량을 비교하면 다음과 같다.</p>
<ul>
<li><strong>Darknet-19:</strong> 55.8억 FLOPS (5.58 BFLOPS) 11</li>
<li><strong>VGG-16:</strong> 306.9억 FLOPS (30.69 BFLOPS) 11</li>
</ul>
<p>이 수치는 Darknet-19가 VGG-16보다 5.5배 이상 계산적으로 효율적임을 명확히 보여준다. 이러한 경량 구조는 YOLOv2가 경쟁 모델들보다 월등히 빠른 추론 속도를 달성할 수 있었던 근본적인 이유이며, 정확도와 속도라는 두 마리 토끼를 모두 잡으려는 YOLOv2의 설계 철학을 완벽하게 구현한 결과물이라 할 수 있다.</p>
<h2>4.  YOLO9000의 확장성: “더 강력하게 (Stronger)”</h2>
<p>YOLOv2의 혁신은 정확도와 속도 개선에만 그치지 않았다. “YOLO9000“이라는 이름으로 제시된 세 번째 목표, “더 강력하게(Stronger)“는 객체 탐지 기술의 근본적인 한계, 즉 제한된 규모의 탐지 데이터셋 문제를 정면으로 다루었다.</p>
<h3>4.1  문제 제기: 탐지 데이터셋의 규모 한계</h3>
<p>객체 탐지 연구의 발전 속도는 양질의 대규모 데이터셋 확보에 크게 의존한다. 그러나 탐지용 데이터셋을 구축하는 작업은 이미지에 단순히 클래스 태그를 붙이는 분류(classification) 작업과 비교할 수 없을 정도로 많은 비용과 시간을 요구한다. 각 이미지에 포함된 모든 객체에 대해 정확한 경계 상자를 그려야 하기 때문이다. 이로 인해 COCO 데이터셋이 약 80개의 클래스를 포함하는 반면, ImageNet과 같은 분류 데이터셋은 수만 개의 클래스를 포함하는 등, 두 데이터셋 간에는 엄청난 규모의 차이가 존재한다.33 YOLO9000의 목표는 이러한 대규모 분류 데이터셋을 활용하여 탐지기의 어휘력, 즉 탐지 가능한 객체의 범주를 비약적으로 확장하는 새로운 방법을 제시하는 것이었다.</p>
<h3>4.2  해결 방안 1: WordTree를 이용한 계층적 분류</h3>
<p>서로 다른 레이블 체계를 가진 COCO(탐지)와 ImageNet(분류) 데이터셋을 결합하기 위해서는 일관된 구조가 필요했다. 단순히 두 데이터셋의 클래스를 합쳐 평평한(flat) 구조로 만들고 소프트맥스(softmax)를 적용하는 것은 불가능하다. 왜냐하면 클래스들이 상호 배타적이지 않기 때문이다. 예를 들어, ImageNet의 ’노퍽 테리어(Norfolk terrier)’는 COCO의 ‘개(dog)’ 클래스에 포함되는 하위 개념이다.</p>
<p>이 문제를 해결하기 위해 YOLO9000은 어휘 개념들을 계층적 관계로 구성한 언어 데이터베이스인 WordNet을 활용했다.33 ImageNet과 COCO의 모든 클래스 레이블을 WordNet의 노드(synset)에 매핑하고, 이들 사이의 상하위 관계를 따라가며 <strong>WordTree</strong>라는 계층적 트리 구조를 구축했다.</p>
<p>이 WordTree 구조 하에서 분류는 각 노드에서 조건부 확률(conditional probability)을 예측하는 방식으로 수행된다. 예를 들어, ‘개’ 노드에서는 <span class="math math-inline">Pr(\text{테리어} \rvert \text{개})</span>, <span class="math math-inline">Pr(\text{리트리버} \rvert \text{개})</span> 등을 예측하고, ‘테리어’ 노드에서는 <span class="math math-inline">Pr(\text{노퍽 테리어} \rvert \text{테리어})</span>, <span class="math math-inline">Pr(\text{요크셔 테리어} \rvert \text{테리어})</span> 등을 예측한다. 특정 하위 클래스(예: ‘노퍽 테리어’)의 최종 확률은 루트 노드(‘물리적 객체’)로부터 해당 노드까지의 경로에 있는 모든 조건부 확률을 곱하여 계산된다.33<br />
<span class="math math-display">
Pr(\text{노퍽 테리어}) = Pr(\text{노퍽 테리어} \rvert \text{테리어}) \times Pr(\text{테리어} \rvert \text{사냥개}) \times \dots \times Pr(\text{개} \rvert \text{포유류})
</span><br />
이 계층적 접근법은 데이터셋을 논리적으로 통합할 수 있게 해줄 뿐만 아니라, 새로운 클래스에 대해서도 유연하게 대처할 수 있게 한다. 예를 들어, 네트워크가 어떤 개의 품종을 정확히 식별하지 못하더라도 ’개’라는 상위 클래스에 대해서는 높은 신뢰도를 예측할 수 있다.</p>
<h3>4.3  해결 방안 2: 검출 및 분류 데이터의 합동 훈련</h3>
<p>WordTree로 데이터셋을 통합한 후, YOLO9000은 검출 데이터와 분류 데이터를 함께 사용하는 합동 훈련(Joint Training) 메커니즘을 통해 학습을 진행한다.23 훈련 중에는 COCO의 탐지 이미지와 ImageNet의 분류 이미지가 혼합되어 네트워크에 입력된다.</p>
<ul>
<li><strong>탐지 이미지(COCO)가 입력될 경우:</strong> 네트워크는 YOLOv2의 전체 손실 함수(위치, 신뢰도, 분류 손실)를 기반으로 역전파(backpropagation)를 수행한다. 이를 통해 모델은 객체의 위치를 정확히 찾고 경계 상자를 그리는 방법을 학습한다.33</li>
<li><strong>분류 이미지(ImageNet)가 입력될 경우:</strong> 네트워크는 오직 분류 손실에 대해서만 역전파를 수행한다. 경계 상자 레이블이 없으므로, 해당 이미지의 클래스에 대해 가장 높은 확률을 예측한 앵커 박스를 찾아 그 박스의 예측된 WordTree에 대해서만 손실을 계산한다. 예를 들어, ’개’라는 레이블이 주어지면, ‘개’ 수준 이상의 상위 계층에 대해서만 손실을 전파하고, 그 하위 품종에 대한 예측에는 오류를 할당하지 않는다. 이 과정을 통해 모델은 방대한 종류의 객체에 대한 시각적 개념을 학습하게 된다.33</li>
</ul>
<h3>4.4  성능 및 의의</h3>
<p>이러한 합동 훈련의 결과로 탄생한 YOLO9000은 9000개 이상의 클래스를 실시간으로 탐지할 수 있는 강력한 모델이 되었다. 그 성능을 검증하기 위해 ImageNet 탐지 데이터셋으로 평가를 진행했다. 이 데이터셋의 200개 클래스 중 44개만이 COCO와 겹쳤기 때문에, 나머지 156개 클래스에 대해서는 YOLO9000이 단 한 번도 경계 상자 레이블을 본 적이 없었다.</p>
<p>결과는 놀라웠다. YOLO9000은 전체적으로 <strong>19.7 mAP</strong>를 달성했으며, 특히 탐지 데이터가 전혀 없었던 156개의 새로운 클래스에 대해서도 <strong>16.0 mAP</strong>라는 준수한 성능을 기록했다.17 이는 분류 데이터만으로도 객체의 일반적인 형태와 특징을 학습하여 위치를 추정하는 능력을 갖추게 되었음을 의미한다.</p>
<p>YOLO9000의 시도는 대규모 약지도 학습(weakly-supervised learning) 기반 객체 탐지의 성공적인 초기 사례로 평가받는다. 이는 구하기 쉬운 대량의 이미지 레벨 레이블(분류 데이터)을 활용하여, 확보하기 어려운 소량의 바운딩 박스 레벨 레이블(탐지 데이터)로 훈련된 모델의 능력을 확장하는 효과적인 방법론을 제시했다. 이 아이디어는 이후 컴퓨터 비전 분야에서 서로 다른 종류와 규모의 데이터를 결합하여 모델의 성능과 일반화 능력을 향상시키려는 연구에 큰 영감을 주었다.</p>
<h2>5.  성능 평가 및 비교 분석</h2>
<p>YOLOv2의 우수성은 다양한 표준 벤치마크 데이터셋에서의 정량적 평가를 통해 입증되었다. 본 장에서는 PASCAL VOC와 MS COCO 데이터셋에서 YOLOv2를 당대의 주요 경쟁 모델인 Faster R-CNN, SSD와 비교한 결과를 분석한다. 모든 실험 결과는 Geforce GTX Titan X GPU 환경에서 측정되었다.31</p>
<h3>5.1  PASCAL VOC 2007 벤치마크</h3>
<p>PASCAL VOC 2007 데이터셋은 객체 탐지 모델의 기본적인 성능을 평가하는 데 널리 사용되는 벤치마크이다. 아래 표 2는 각 모델의 mAP와 FPS를 비교한 결과이다.</p>
<p><strong>표 2: PASCAL VOC 2007 테스트셋 성능 비교</strong></p>
<table><thead><tr><th>탐지 프레임워크</th><th>훈련 데이터</th><th>mAP (%)</th><th>FPS</th></tr></thead><tbody>
<tr><td>Fast R-CNN</td><td>07+12</td><td>70.0</td><td>0.5</td></tr>
<tr><td>Faster R-CNN VGG-16</td><td>07+12</td><td>73.2</td><td>7</td></tr>
<tr><td>Faster R-CNN ResNet</td><td>07+12</td><td>76.4</td><td>5</td></tr>
<tr><td>YOLOv1</td><td>07+12</td><td>63.4</td><td>45</td></tr>
<tr><td>SSD300</td><td>07+12</td><td>74.3</td><td>46</td></tr>
<tr><td>SSD512</td><td>07+12</td><td>76.8</td><td>19</td></tr>
<tr><td><strong>YOLOv2 288x288</strong></td><td><strong>07+12</strong></td><td><strong>69.0</strong></td><td><strong>91</strong></td></tr>
<tr><td><strong>YOLOv2 352x352</strong></td><td><strong>07+12</strong></td><td><strong>73.7</strong></td><td><strong>81</strong></td></tr>
<tr><td><strong>YOLOv2 416x416</strong></td><td><strong>07+12</strong></td><td><strong>76.8</strong></td><td><strong>67</strong></td></tr>
<tr><td><strong>YOLOv2 480x480</strong></td><td><strong>07+12</strong></td><td><strong>77.8</strong></td><td><strong>59</strong></td></tr>
<tr><td><strong>YOLOv2 544x544</strong></td><td><strong>07+12</strong></td><td><strong>78.6</strong></td><td><strong>40</strong></td></tr>
</tbody></table>
<p>출처: 31</p>
<p>분석 결과, YOLOv2는 속도와 정확도 모든 면에서 압도적인 성능을 보여준다. 544x544 고해상도 입력에서 YOLOv2는 <strong>78.6% mAP</strong>를 기록하며, 당시 최고 수준의 정확도를 자랑하던 Faster R-CNN with ResNet(76.4%)과 SSD512(76.8%)를 능가했다. 더욱 놀라운 점은 이러한 정확도를 40 FPS라는 실시간 속도로 달성했다는 것이다. 이는 각각 5 FPS와 19 FPS에 불과했던 경쟁 모델들과 비교할 때 2배에서 8배 빠른 속도이다.</p>
<p>또한, 다중 스케일 훈련의 유연성이 이 표에서 명확하게 드러난다. 동일하게 훈련된 단일 모델이 입력 해상도를 288x288로 낮추자 <strong>91 FPS</strong>라는 극도로 빠른 속도를 기록했으며, 이때의 mAP(69.0%)조차도 2단계 탐지기인 Fast R-CNN(70.0%)에 근접했다. 이 결과는 YOLOv2가 속도-정확도 스펙트럼에서 새로운 최첨단(state-of-the-art) 기준을 세웠음을 명백히 증명한다.17</p>
<h3>5.2  PASCAL VOC 2012 벤치마크</h3>
<p>PASCAL VOC 2012는 2007보다 더 도전적인 데이터셋으로, 모델의 일반화 성능을 더 깊이 있게 평가할 수 있다. 표 3은 각 모델의 클래스별 mAP를 포함한 상세 비교 결과이다.</p>
<p><strong>표 3: PASCAL VOC 2012 테스트셋 성능 비교</strong></p>
<table><thead><tr><th>모델</th><th>데이터</th><th>mAP</th><th>aero</th><th>bike</th><th>bird</th><th>boat</th><th>bottle</th><th>bus</th><th>car</th><th>cat</th><th>chair</th><th>cow</th><th>table</th><th>dog</th><th>horse</th><th>mbike</th><th>person</th><th>plant</th><th>sheep</th><th>sofa</th><th>train</th><th>tv</th></tr></thead><tbody>
<tr><td>Fast R-CNN</td><td>07++12</td><td>68.4</td><td>82.3</td><td>78.4</td><td>70.8</td><td>52.3</td><td>38.7</td><td>77.8</td><td>71.6</td><td>89.3</td><td>44.2</td><td>73.0</td><td>55.0</td><td>87.5</td><td>80.5</td><td>80.8</td><td>72.0</td><td>35.1</td><td>68.3</td><td>65.7</td><td>80.4</td><td>64.2</td></tr>
<tr><td>Faster R-CNN</td><td>07++12</td><td>70.4</td><td>84.9</td><td>79.8</td><td>74.3</td><td>53.9</td><td>49.8</td><td>77.5</td><td>75.9</td><td>88.5</td><td>45.6</td><td>77.1</td><td>55.3</td><td>86.9</td><td>81.7</td><td>80.9</td><td>79.6</td><td>40.1</td><td>72.6</td><td>60.9</td><td>81.2</td><td>61.5</td></tr>
<tr><td>YOLOv1</td><td>07++12</td><td>57.9</td><td>77.0</td><td>67.2</td><td>57.7</td><td>38.3</td><td>22.7</td><td>68.3</td><td>55.9</td><td>81.4</td><td>36.2</td><td>60.8</td><td>48.5</td><td>77.2</td><td>72.3</td><td>71.3</td><td>63.5</td><td>28.9</td><td>52.2</td><td>54.8</td><td>73.9</td><td>50.8</td></tr>
<tr><td>SSD300</td><td>07++12</td><td>72.4</td><td>85.6</td><td>80.1</td><td>70.5</td><td>57.6</td><td>46.2</td><td>79.4</td><td>76.1</td><td>89.2</td><td>53.0</td><td>77.0</td><td>60.8</td><td>87.0</td><td>83.1</td><td>82.3</td><td>79.4</td><td>45.9</td><td>75.9</td><td>69.5</td><td>81.9</td><td>67.5</td></tr>
<tr><td>SSD512</td><td>07++12</td><td>74.9</td><td>87.4</td><td>82.3</td><td>75.8</td><td>59.0</td><td>52.6</td><td>81.7</td><td>81.5</td><td>90.0</td><td>55.4</td><td>79.0</td><td>59.8</td><td>88.4</td><td>84.3</td><td>84.7</td><td>83.3</td><td>50.2</td><td>78.0</td><td>66.3</td><td>86.3</td><td>72.0</td></tr>
<tr><td>ResNet (Faster R-CNN)</td><td>07++12</td><td>73.8</td><td>86.5</td><td>81.6</td><td>77.2</td><td>58.0</td><td>51.0</td><td>78.6</td><td>76.6</td><td>93.2</td><td>48.6</td><td>80.4</td><td>59.0</td><td>92.1</td><td>85.3</td><td>84.8</td><td>80.7</td><td>48.1</td><td>77.3</td><td>66.5</td><td>84.7</td><td>65.6</td></tr>
<tr><td>YOLOv2 544</td><td>07++12</td><td>73.4</td><td>86.3</td><td>82.0</td><td>74.8</td><td>59.2</td><td>51.8</td><td>79.8</td><td>76.5</td><td>90.6</td><td>52.1</td><td>78.2</td><td>58.5</td><td>89.3</td><td>82.5</td><td>83.4</td><td>81.3</td><td>49.1</td><td>77.2</td><td>62.4</td><td>83.8</td><td>68.7</td></tr>
</tbody></table>
<p>출처: 31</p>
<p>VOC 2012 데이터셋에서 YOLOv2는 <strong>73.4% mAP</strong>를 기록했다. 이는 ResNet 기반 Faster R-CNN(73.8%)과 거의 대등한 수준이며, SSD512(74.9%)에는 다소 뒤처지는 결과이다. 이 결과는 YOLOv2가 모든 면에서 경쟁 모델을 압도하지는 않았음을 보여준다. 하지만 여전히 2~10배 빠른 속도를 고려하면, YOLOv2가 최상위권의 정확도를 유지하면서도 비교 불가능한 속도를 제공한다는 사실은 변함이 없다. 이는 YOLOv2가 정확도와 속도 간의 실용적인 균형점을 매우 성공적으로 찾아냈음을 시사한다.</p>
<h3>5.3  MS COCO 벤치마크</h3>
<p>MS COCO 데이터셋은 더 많은 클래스, 더 작은 객체, 더 높은 객체 밀도를 특징으로 하며, IOU 임계값을 0.5부터 0.95까지 0.05 간격으로 변화시키며 mAP를 평균내는 등 훨씬 더 엄격한 평가 지표를 사용한다. 이 벤치마크는 특히 모델의 위치 결정 정밀도(localization precision)를 엄격하게 평가한다.</p>
<p><strong>표 4: MS COCO test-dev2015 성능 비교</strong></p>
<table><thead><tr><th>모델</th><th>데이터</th><th>mAP @[.5:.95]</th><th>mAP @.5</th><th>mAP @.75</th><th>AP Small</th><th>AP Medium</th><th>AP Large</th></tr></thead><tbody>
<tr><td>Faster R-CNN VGG-16</td><td>trainval</td><td>21.9</td><td>42.7</td><td>-</td><td>-</td><td>-</td><td>-</td></tr>
<tr><td>Faster R-CNN ResNet</td><td>trainval</td><td>24.2</td><td>45.3</td><td>23.5</td><td>7.7</td><td>26.4</td><td>37.1</td></tr>
<tr><td>SSD300</td><td>trainval35k</td><td>23.2</td><td>41.2</td><td>23.4</td><td>5.3</td><td>23.2</td><td>39.6</td></tr>
<tr><td>SSD512</td><td>trainval35k</td><td>26.8</td><td>46.5</td><td>27.8</td><td>9.0</td><td>28.9</td><td>41.9</td></tr>
<tr><td><strong>YOLOv2</strong></td><td><strong>trainval35k</strong></td><td><strong>21.6</strong></td><td><strong>44.0</strong></td><td><strong>19.2</strong></td><td><strong>5.0</strong></td><td><strong>22.4</strong></td><td><strong>35.5</strong></td></tr>
</tbody></table>
<p>출처: 31</p>
<p>COCO 벤치마크에서 YOLOv2의 한계가 드러난다. 주요 지표인 mAP@[.5:.95]에서 YOLOv2는 **21.6%**를 기록하여 SSD512(26.8%)와 Faster R-CNN(24.2%)에 비해 낮은 성능을 보였다. 특히 작은 객체에 대한 정밀도(AP Small)는 **5.0%**로, 경쟁 모델들(각각 9.0%, 7.7%)에 비해 현저히 낮았다.</p>
<p>이 결과는 YOLOv2가 앵커 박스와 패스스루 계층 등을 통해 재현율을 높이고 객체를 ‘찾아내는’ 능력은 향상시켰지만, 그 경계 상자의 위치를 2단계 탐지기만큼 ‘정밀하게’ 그리는 데에는 여전히 약점이 있음을 시사한다. COCO의 엄격한 IOU 기준은 이러한 미세한 위치 결정 오류를 용납하지 않으며, 이것이 mAP 격차의 주된 원인으로 분석된다. 이 약점은 YOLOv2의 후속 버전인 YOLOv3에서 해결해야 할 가장 중요한 과제가 되었다.</p>
<h2>6.  YOLOv2의 한계와 후속 연구 동향</h2>
<p>YOLOv2는 실시간 객체 탐지 분야에 큰 획을 그었지만, 완벽한 모델은 아니었다. 그 한계점들은 명확했으며, 이는 곧바로 후속 연구, 특히 YOLOv3의 개발 방향을 결정하는 중요한 이정표가 되었다.</p>
<h3>6.1  작은 객체 탐지의 어려움</h3>
<p>YOLOv2의 가장 지속적인 약점은 작은 객체를 탐지하는 능력이었다. ’세분화된 특징(Fine-Grained Features)’을 위한 패스스루 계층을 도입했음에도 불구하고, 이 문제는 근본적으로 해결되지 않았다.12 그 원인은 아키텍처의 구조적 한계에 있었다. YOLOv2는 최종적으로 13x13이라는 상대적으로 거친(coarse) 해상도의 단일 특징 맵에서 모든 예측을 수행했다. 이러한 단일 스케일 예측 방식은 고해상도의 세밀한 공간 정보를 충분히 활용하기 어려워, 이미지에서 작은 픽셀 영역을 차지하는 객체들을 안정적으로 탐지하는 데 본질적인 한계를 가졌다. MS COCO 벤치마크에서 작은 객체에 대한 AP(Average Precision)가 경쟁 모델들에 비해 현저히 낮게 나타난 것이 이를 뒷받침하는 명백한 증거이다.31</p>
<h3>6.2  2단계 탐지기 대비 낮은 위치 결정 정확도</h3>
<p>COCO 벤치마크 결과가 보여주듯이, YOLOv2는 2단계 탐지기에 비해 경계 상자의 위치 결정 정밀도가 떨어졌다.6 YOLOv2는 재현율을 높이는 데 성공하여 객체를 놓치지 않고 찾아내는 능력은 크게 향상되었지만, 그 위치를 픽셀 단위로 정확하게 맞추는 데는 여전히 약점을 보였다. 이는 1단계 탐지기의 본질적인 특성과 관련이 있다. 2단계 탐지기는 RPN을 통해 생성된 후보 영역을 두 번째 단계에서 다시 한번 정제(refinement)하는 과정을 거치기 때문에 더 정밀한 위치 예측이 가능하다.5 반면, YOLOv2는 단일 패스에서 예측을 완료하므로 이러한 정제 과정이 없다. PASCAL VOC와 같이 상대적으로 너그러운 IOU 기준(0.5)에서는 이러한 차이가 크게 부각되지 않았지만, COCO의 엄격한 IOU 기준에서는 이 약점이 성능 저하의 주요 원인이 되었다.</p>
<h3>6.3  YOLOv3로의 발전: 한계 극복을 위한 진화</h3>
<p>YOLOv2의 이러한 명확한 한계점들은 YOLOv3 개발의 직접적인 동기가 되었다. YOLOv3는 YOLOv2의 약점을 정면으로 겨냥한 핵심적인 개선 사항들을 도입했다.</p>
<ul>
<li><strong>다중 스케일 예측 (Multi-Scale Predictions):</strong> 작은 객체 탐지 문제를 해결하기 위해, YOLOv3는 Feature Pyramid Network(FPN)와 유사한 개념을 도입했다. 단일 스케일에서 예측하는 대신, 세 가지 다른 해상도(예: 13x13, 26x26, 52x52)의 특징 맵에서 독립적으로 예측을 수행한다.21 거친 해상도의 특징 맵은 큰 객체를, 중간 해상도는 중간 크기 객체를, 그리고 미세한 해상도의 특징 맵은 작은 객체를 탐지하는 데 사용된다. 이로써 모델은 다양한 스케일의 객체를 효과적으로 처리할 수 있게 되었고, 특히 작은 객체에 대한 탐지 성능이 비약적으로 향상되었다. YOLOv3는 이 개선을 통해 COCO 데이터셋에서 작은 객체 AP를 5.0에서 18 이상으로 끌어올리는 극적인 성과를 거두었다.39</li>
<li><strong>더 깊고 강력한 백본 네트워크 (Darknet-53):</strong> 전반적인 특징 표현 능력을 강화하여 정확도를 높이기 위해, YOLOv3는 Darknet-19를 Darknet-53으로 교체했다.40 Darknet-53은 이름에서 알 수 있듯이 53개의 컨볼루션 계층으로 구성된 훨씬 더 깊은 네트워크이다. 특히 ResNet에서 영감을 받은 잔차 연결(residual connection)을 도입하여, 깊은 네트워크에서 발생할 수 있는 그래디언트 소실 문제를 완화하고 효과적인 학습을 가능하게 했다. Darknet-53은 ResNet-101보다 1.5배 빠르면서도 ResNet-152와 동등한 수준의 정확도를 달성하여, 효율성과 성능을 모두 잡은 뛰어난 백본 네트워크로 평가받는다.40</li>
</ul>
<p>이처럼 YOLOv3는 YOLOv2의 한계를 명확히 인식하고, 이를 극복하기 위한 구체적이고 효과적인 해결책을 제시함으로써 YOLO 시리즈의 진화를 이끌었다. YOLOv2가 닦아놓은 ’실시간 고정밀 탐지’라는 길 위에서, YOLOv3는 그 완성도를 한 차원 더 높은 수준으로 끌어올렸다.</p>
<h2>7.  결론: YOLOv2가 객체 탐지 분야에 미친 영향</h2>
<p>YOLOv2는 객체 탐지의 역사에서 단순한 성능 개선 모델을 넘어, 하나의 패러다임을 정립한 중요한 이정표로 기록된다. “Better, Faster, Stronger“라는 명확한 슬로건 아래 제시된 일련의 혁신들은 속도와 정확도 사이의 기존 통념을 깨고, 실시간 객체 탐지 기술의 실용성과 접근성을 전례 없는 수준으로 끌어올렸다.</p>
<p>YOLOv2의 가장 큰 성과는 1단계 탐지기가 2단계 탐지기의 전유물로 여겨졌던 높은 정확도에 근접할 수 있음을 실증적으로 보여준 것이다. 배치 정규화, 고해상도 분류기, 앵커 박스, 차원 클러스터링, 직접 위치 예측, 패스스루 계층, 다중 스케일 훈련 등 체계적으로 고안된 개선점들은 상호 유기적으로 작용하여 YOLOv2를 PASCAL VOC 벤치마크에서 당대 최고의 모델로 만들었다. 특히, 맞춤형으로 설계된 Darknet-19 아키텍처는 압도적인 연산 효율성을 바탕으로, 높은 정확도를 유지하면서도 경쟁 모델들을 수 배 이상 능가하는 추론 속도를 가능하게 했다.</p>
<p>이러한 성취는 실시간 객체 탐지를 학술적 연구의 영역에서 벗어나, 자율 주행 자동차, 드론, 지능형 감시 시스템, 로보틱스, 모바일 기기 등 속도가 핵심적인 수많은 실제 산업 응용 분야로 확산시키는 기폭제가 되었다.8 YOLOv2의 간결한 엔드-투-엔드(end-to-end) 구조와 뛰어난 성능은 수많은 개발자와 연구자들에게 실시간 컴퓨터 비전 시스템을 구축할 수 있는 강력하고 신뢰할 수 있는 도구를 제공했다.</p>
<p>더 나아가, YOLO9000 프로젝트는 대규모 분류 데이터셋을 활용하여 탐지기의 어휘력을 확장하는 약지도 학습의 가능성을 제시하며 새로운 연구의 지평을 열었다. 이는 데이터 확보의 어려움이라는 객체 탐지 분야의 고질적인 문제를 해결하기 위한 창의적인 접근법으로, 오늘날까지도 활발히 연구되는 주제의 초석을 다졌다.</p>
<p>물론 YOLOv2는 작은 객체 탐지와 위치 결정 정밀도 측면에서 명백한 한계를 가지고 있었다. 그러나 이러한 한계점들은 실패가 아닌, 다음 세대의 발전을 위한 명확한 과제를 제시했다는 점에서 오히려 긍정적으로 평가할 수 있다. YOLOv3는 YOLOv2가 남긴 과제들을 해결하기 위해 다중 스케일 예측과 더 깊은 백본 네트워크를 도입했고, 이는 YOLO 시리즈가 지속적으로 실시간 객체 탐지 분야의 선두를 유지하는 원동력이 되었다.</p>
<p>결론적으로, YOLOv2는 속도와 정확도의 균형을 재정의하고, 실용적인 실시간 객체 탐지의 시대를 연 선구자적 모델이다. YOLOv2가 제시한 수많은 실용적인 기법들과 설계 철학은 이후 등장한 모든 YOLO 시리즈의 근간을 이루었으며, 1단계 탐지기 패러다임을 현대 컴퓨터 비전의 핵심 기둥 중 하나로 굳건히 세웠다. YOLOv2는 그 자체로 완성된 모델이자, 더 위대한 발전을 위한 견고한 토대였다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Mastering Object Detection: A Comprehensive Guide to YOLO Series, Faster R-CNN, SSD MultiBox, and Mask R-CNN - Awareye, https://www.awareye.ai/post/mastering-object-detection-a-comprehensive-guide-to-yolo-series-faster-r-cnn-ssd-multibox-and-mask-r-cnn</li>
<li>YOLOv1 to YOLOv11: A Comprehensive Survey of Real-Time Object Detection Innovations and Challenges - arXiv, https://arxiv.org/html/2508.02067v1</li>
<li>YOLO Object Detection Explained: Models, Tools, Use Cases - Lightly, https://www.lightly.ai/blog/yolo</li>
<li>Practical comparation of the accuracy and speed of YOLO, SSD and Faster RCNN for drone detection - ResearchGate, https://www.researchgate.net/publication/354771273_Practical_comparation_of_the_accuracy_and_speed_of_YOLO_SSD_and_Faster_RCNN_for_drone_detection</li>
<li>Single Stage Detector vs 2 Stage Detector | by Abhishek Jain | Medium, https://medium.com/@abhishekjainindore24/single-stage-detector-vs-2-stage-detector-3e540ea81213</li>
<li>One-Stage Object Detectors Explained - Ultralytics, https://www.ultralytics.com/glossary/one-stage-object-detectors</li>
<li>Understanding YOLO and YOLOv2 - Manal El Aidouni, <a href="https://manalelaidouni.github.io/Understanding%20YOLO%20and%20YOLOv2.html">https://manalelaidouni.github.io/Understanding%20YOLO%20and%20YOLOv2.html</a></li>
<li>A Decade of You Only Look Once (YOLO) for Object Detection: A Review - arXiv, https://arxiv.org/html/2504.18586v2</li>
<li>A Review of the Single-Stage vs. Two-Stage Detectors Algorithm: Comprehensive Insights into Object Detection, https://theaspd.com/index.php/ijes/article/download/337/308</li>
<li>[PDF] YOLO9000: Better, Faster, Stronger - Semantic Scholar, https://www.semanticscholar.org/paper/YOLO9000%3A-Better%2C-Faster%2C-Stronger-Redmon-Farhadi/7d39d69b23424446f0400ef603b2e3e22d0309d6</li>
<li>YOLO v2 - Object Detection - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/yolo-v2-object-detection/</li>
<li>YOLO Explained: From v1 to Present - Viso Suite, https://viso.ai/computer-vision/yolo-explained/</li>
<li>YOLO Object Detection Explained: A Beginner’s Guide - DataCamp, https://www.datacamp.com/blog/yolo-object-detection-explained</li>
<li>Detecting very small objects using YOLO v2 in high resolution images - Reddit, https://www.reddit.com/r/computervision/comments/dhh1qp/detecting_very_small_objects_using_yolo_v2_in/</li>
<li>Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3 - Jonathan Hui - Medium, https://jonathan-hui.medium.com/real-time-object-detection-with-yolo-yolov2-28b1b93e2088</li>
<li>Review: YOLOv2 &amp; YOLO9000 — You Only Look Once (Object Detection) | by Sik-Ho Tsang, https://medium.com/data-science/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65</li>
<li>YOLO9000: Better, Faster, Stronger | Request PDF - ResearchGate, https://www.researchgate.net/publication/311925600_YOLO9000_Better_Faster_Stronger</li>
<li>YOLO9000: Better, Faster, Stronger - Department of Computer Science, University of Toronto, https://www.cs.utoronto.ca/~fidler/teaching/2018/slides/CSC2548/HarisKhan_YOLO9000review_v2.pdf</li>
<li>‪Joseph Redmon‬ - ‪Google Scholar‬, https://scholar.google.com/citations?user=TDk_NfkAAAAJ&amp;hl=en</li>
<li>YOLOv1 to YOLOv10: A comprehensive review of YOLO variants and their application in the agricultural domain - arXiv, https://arxiv.org/html/2406.10139v1</li>
<li>What is YOLO? The Ultimate Guide [2025] - Roboflow Blog, https://blog.roboflow.com/guide-to-yolo-models/</li>
<li>YOLOv1 to YOLOv10: The fastest and most accurate real-time object detection systems - arXiv, https://arxiv.org/pdf/2408.09332</li>
<li>A Comprehensive Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS - MDPI, https://www.mdpi.com/2504-4990/5/4/83</li>
<li>YOLO, YOLOv2, and YOLOv3: All You want to know | by Amro Kamal - Medium, https://amrokamal-47691.medium.com/yolo-yolov2-and-yolov3-all-you-want-to-know-7e3e92dc4899</li>
<li>Real-time object detection with YOLO - Machine, Think!, https://machinethink.net/blog/object-detection-with-yolo/</li>
<li>YOLO-V2 (You Only Look Once), https://www.ijarsct.co.in/Paper17515.pdf</li>
<li>YOLO v2 Comprehensive Tutorial: Building on YOLO v1 Mistakes | by Sachinsoni - Medium, https://medium.com/@sachinsoni600517/yolo-v2-comprehensive-tutorial-building-on-yolo-v1-mistakes-aa7912292c1a</li>
<li>An Improved YOLOv2 for Vehicle Detection - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC6308705/</li>
<li>Review On YOLOv2 - by Arun Mohan - DataDrivenInvestor, https://medium.datadriveninvestor.com/review-on-yolov2-11e93c5ea3f1</li>
<li>YOLO9000: Better, Faster, Stronger - CVPR 2017 Open Access Repository, https://openaccess.thecvf.com/content_cvpr_2017/html/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.html</li>
<li>YOLO9000: Better, Faster, Stronger, http://arxiv.org/pdf/1612.08242</li>
<li>YOLOv2’s Darknet-19 Backbone: A Comprehensive Analysis | SERP AI, https://serp.ai/posts/darknet-19/</li>
<li>[1612.08242] YOLO9000: Better, Faster, Stronger - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/1612.08242</li>
<li>Paper page - YOLO9000: Better, Faster, Stronger - Hugging Face, https://huggingface.co/papers/1612.08242</li>
<li>YOLOv2 Object Detection Model - Sipeed Wiki, https://wiki.sipeed.com/ai/en/nn_models/yolov2.html</li>
<li>Comparative Analysis of Two-Stage and One-Stage Object Detection Models - SciTePress, https://www.scitepress.org/Papers/2024/135159/135159.pdf</li>
<li>On the Performance of One-Stage and Two-Stage Object Detectors in Autonomous Vehicles Using Camera Data - MDPI, https://www.mdpi.com/2072-4292/13/1/89</li>
<li>One-Stage vs. Two-Stage Detectors in Object Detection: Unveiling the Performance Trade-Offs - LakshmiShreeA1’s blog, https://lakshmishreea1.hashnode.dev/one-stage-vs-two-stage-detectors-in-object-detection-unveiling-the-performance-trade-offs</li>
<li>YOLOv2 (YOLO9000) and YOLOv3 Explained - YouTube, https://www.youtube.com/watch?v=wYjkiI-Lm-8</li>
<li>YOLOv3: Real-Time Object Detection Algorithm (Guide) - Viso Suite, https://viso.ai/deep-learning/yolov3-overview/</li>
<li>(PDF) Enhancing Real-time Object Detection with YOLO Algorithm - ResearchGate, https://www.researchgate.net/publication/376252973_Enhancing_Real-time_Object_Detection_with_YOLO_Algorithm</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>