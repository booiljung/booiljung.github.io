<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:YOLO의 비판적 연대기</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>YOLO의 비판적 연대기</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">객체 탐지 (Object Detection)</a> / <a href="index.html">YOLO 객체 탐지 모델</a> / <span>YOLO의 비판적 연대기</span></nav>
                </div>
            </header>
            <article>
                <h1>YOLO의 비판적 연대기</h1>
<h2>1.  YOLOv1의 근본적인 결함</h2>
<p>최초의 YOLO 모델을 해부하는 이 장에서는 모델의 가장 중대한 문제점들이 단순한 성능 격차를 넘어, 그 혁신적인 설계 철학에서 비롯된 근본적이고 내재적인 결과물이었음을 규명한다. 전례 없는 속도를 위한 트레이드오프는 일련의 깊이 내재된 구조적 한계들을 낳았다.</p>
<h3>1.1  통합 회귀 패러다임과 그 내재적 제약</h3>
<p>YOLOv1은 객체 탐지를 단일 회귀 문제(single regression problem)로 재구성하여, 전체 이미지로부터 경계 상자(bounding box)와 클래스 확률을 한 번에 직접 예측한다.1 이 통합된 종단간(end-to-end) 아키텍처는 타이탄 X GPU에서 초당 45프레임(FPS)이라는 놀라운 속도의 원천이다.1</p>
<p>이러한 혁신적인 접근 방식은 네트워크가 전체 이미지에 대해 한 번에 전역적으로 추론(reason globally)하도록 강제한다.3 그러나 이 전역적 추론은 지역적 정밀도(local precision)를 희생하는 대가로 이루어진다. 먼저 관심 영역(regions of interest)을 식별한 후 이를 분류하는 2단계 탐지기(two-stage detectors, 예: R-CNN)와는 달리, YOLOv1의 단일 네트워크는 두 가지 과업을 동시에 수행해야 하므로 위치 정확도(localization accuracy)에서 타협이 발생한다.2 이 문제는 개념적인 트레이드오프 관계에 있다. 즉, 영역 제안 네트워크(region proposal networks)가 제공하는 집중적이고 고해상도 분석을 포기함으로써 속도를 얻는 것이다.</p>
<h3>1.2  그리드 셀 병목 현상: 위치 특정 및 소형 객체 탐지의 구조적 장애물</h3>
<p>YOLOv1의 핵심 메커니즘은 입력 이미지 위에 거친 <span class="math math-inline">S \times S</span> 그리드(예: 7x7)를 씌우는 것이다. 특정 그리드 셀은 객체의 중심이 해당 셀 내에 위치할 경우에만 그 객체를 탐지할 책임을 단독으로 진다.5 각 그리드 셀은 고정된 소수의 경계 상자(예: B=2)와 하나의 클래스 확률 집합만을 예측한다.2</p>
<p>이 그리드 시스템은 YOLOv1의 가장 빈번하게 지적되는 실패의 주요 원인이다.</p>
<p>첫째, 그리드는 문제 공간에 대해 경직되고 거친 공간적 양자화(spatial quantization) 한계를 부과한다. 이는 공간적으로 인접한 다수의 객체를 탐지하는 모델의 능력을 근본적으로 제한한다. 모델은 객체의 중심점을 기준으로 단일 그리드 셀에 객체 탐지의 책임을 할당한다.5 각 셀은 고정된 수(예: 2개)의 객체만 예측할 수 있다.2 따라서 새 떼나 군중과 같이 작은 객체 그룹의 중심점들이 동일한 그리드 셀 내에 위치하게 되면, 모델은 구조적으로 고정된 예측 한계(이 경우 2개) 이상을 탐지할 수 없게 된다. 이는 학습의 실패가 아니라 하드코딩된 아키텍처적 제약이다. 이는 공간 그리드 해상도에 직접적으로 연결된 용량 병목 현상으로, ’밀집된 객체(densely packed objects)’에 대한 낮은 성능은 피할 수 없는 결과가 된다.2</p>
<p>둘째, 거친 그리드는 낮은 위치 정확도의 직접적인 원인이기도 하다. 모델은 경계 상자 좌표를 더 정밀한 앵커(anchor)가 아닌 그리드 셀의 위치로부터의 오프셋(offset)으로 예측한다. 이는 모델의 공간 해상도가 근본적으로 그리드 크기에 의해 제한됨을 의미한다. 모델은 먼저 거친 영역(그리드 셀)을 식별한 후 그 안에서 위치를 미세 조정하려고 시도한다. 이는 객체와 유사한 특징을 기반으로 영역을 제안하는 방법(예: Faster R-CNN)보다 본질적으로 덜 정확하다. 결과적으로 YOLOv1은 그리드 구조와 잘 정렬되지 않거나 그리드 셀 크기에 비해 상대적으로 작은 객체들을 다루는 데 어려움을 겪으며, 이는 심각한 위치 오류로 이어진다.2</p>
<h3>1.3  제곱합 오차(SSE) 손실 함수 분석과 성능 저하에 미치는 영향</h3>
<p>YOLOv1은 위치(x, y, w, h), 신뢰도(confidence), 분류 오류를 동시에 최적화하기 위해 단순한 제곱합 오차(Sum-Squared Error, SSE) 손실 함수를 사용한다.6 SSE의 선택은 아키텍처의 결함을 더욱 악화시키는 몇 가지 치명적인 문제들을 야기한다.</p>
<p>SSE는 모든 오류를 동일하게 취급하는데, 이는 객체 탐지에서 문제가 된다. SSE는 예측값과 실제값 사이의 제곱 차이를 계산한다. 이는 큰 경계 상자에서의 10픽셀 오류가 작은 경계 상자에서의 10픽셀 오류와 동일한 페널티를 받는다는 것을 의미한다. 직관적으로 후자가 훨씬 더 심각한 위치 특정 실패이다. 이 문제를 완화하기 위해 저자들은 너비와 높이 값 자체 대신 그 제곱근(<span class="math math-inline">\sqrt{w}</span>, <span class="math math-inline">\sqrt{h}</span>)을 예측하도록 했는데, 이는 큰 상자에서의 편차에 더 적은 페널티를 부여한다.7 그러나 이는 근본적인 해결책이 아닌 임시방편에 불과하다. 핵심 문제는 SSE가 위치 특정의 주요 평가 지표인 IoU(Intersection over Union)와 직접적으로 연계되지 않는다는 점이다. 더욱이, 손실 함수는 객체가 없는 방대한 수의 그리드 셀에 대한 신뢰도 오류와 위치 오류의 균형을 맞추기 위해 가중치 파라미터(<span class="math math-inline">\lambda_{coord}</span> 및 <span class="math math-inline">\lambda_{noobj}</span>)를 수동으로 사용한다.7 이러한 수동적 균형 조정은 손실 함수가 본질적으로 해당 과업에 적합하지 않다는 것을 시사하는 경험적 방법에 해당한다.</p>
<p>YOLOv1의 전체 손실 함수는 다중 파트 SSE로, 다음과 같이 표현될 수 있다:<br />
<span class="math math-display">
\begin{aligned}
L = &amp; \lambda_{coord} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{I}_{ij}^{obj} \left[ (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2 \right] \\
&amp; + \lambda_{coord} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{I}_{ij}^{obj} \left[ (\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2 \right] \\
&amp; + \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{I}_{ij}^{obj} (C_i - \hat{C}_i)^2 \\
&amp; + \lambda_{noobj} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{I}_{ij}^{noobj} (C_i - \hat{C}_i)^2 \\
&amp; + \sum_{i=0}^{S^2} \mathbb{I}_{i}^{obj} \sum_{c \in \text{classes}} (p_i(c) - \hat{p}_i(c))^2
\end{aligned}
</span><br />
여기서 <span class="math math-inline">\mathbb{I}_{ij}^{obj}</span>는 지시 함수(indicator function)로, <span class="math math-inline">i</span>번째 셀의 <span class="math math-inline">j</span>번째 경계 상자 예측기가 실제 상자에 대해 “책임“이 있을 경우 1이 된다. 이 공식은 수동 가중치(<span class="math math-inline">\lambda</span>)와 문제가 있는 SSE 항들을 명확히 보여준다.</p>
<p>이러한 근본적인 한계들을 요약하고 후속 버전에서의 개선 과정을 조망하기 위해, 아래 표는 YOLO 계열의 진화 과정을 개괄적으로 보여준다.</p>
<table><thead><tr><th>버전</th><th>이전 버전의 주요 한계점</th><th>핵심 아키텍처 대응</th><th>새롭게 발생한 문제점 / 한계</th></tr></thead><tbody>
<tr><td><strong>YOLOv1</strong></td><td>(기준 모델)</td><td>통합 회귀, 그리드 셀 시스템</td><td>낮은 위치 정확도, 소형 객체 탐지 성능 저하, 낮은 재현율, 셀당 고정된 객체 수.</td></tr>
<tr><td><strong>YOLOv2</strong></td><td>낮은 재현율, 다양한 형태에 대한 위치 정확도 저하.</td><td>앵커 박스, Darknet-19, 배치 정규화, 다중 스케일 훈련.</td><td>하이퍼파라미터 민감도 (앵커 선택), 2단계 탐지기와의 지속적인 정확도 격차.</td></tr>
<tr><td><strong>YOLOv3</strong></td><td>소형 객체 탐지 성능 저하, 정확도 격차.</td><td>Darknet-53, 다중 스케일 예측 (FPN 유사).</td><td>높은 계산/메모리 비용, 높은 IoU 임계값에서의 정밀도 저하.</td></tr>
<tr><td><strong>YOLOv4</strong></td><td>최적화된 속도/정확도 균형 필요.</td><td>“Bag of Freebies” &amp; “Bag of Specials” (CSP, PAN, Mish, CIoU 손실).</td><td>극심한 훈련 복잡성, 분석이 어려운 혼합 아키텍처.</td></tr>
<tr><td><strong>YOLOv5</strong></td><td>Darknet 프레임워크의 사용성/접근성.</td><td>네이티브 PyTorch 구현, YAML 구성.</td><td>명명/벤치마킹 논란, 동료 심사 논문 부재, 출처에 대한 의문.</td></tr>
<tr><td><strong>YOLOv6</strong></td><td>하드웨어에서의 극대화된 추론 속도 필요.</td><td>재매개변수화 백본 (EfficientRep), 분리된 헤드.</td><td>훈련-추론 아키텍처의 이중성, 탐지 외 작업에 대한 제한된 다용성.</td></tr>
<tr><td><strong>YOLOv7</strong></td><td>기존 패러다임 내에서 정확도 한계 돌파.</td><td>E-ELAN, 훈련 가능한 BoF, 고급 모델 스케일링.</td><td>극심한 개념적 복잡성, 접근/해석이 어려운 아키텍처.</td></tr>
<tr><td><strong>YOLOv8</strong></td><td>통합되고 사용하기 쉬운 프레임워크 필요.</td><td>앵커 프리 분리된 헤드, C2f 모듈, 다중 작업 지원.</td><td>대형 모델의 지속적인 계산 요구량, 소형 객체 탐지의 지속적인 과제.</td></tr>
</tbody></table>
<h2>2.  YOLOv2 - 새로운 복잡성으로 재현율(Recall) 문제를 다루다</h2>
<p>이 장에서는 YOLOv2를 YOLOv1의 가장 명백한 약점, 특히 낮은 재현율에 대한 직접적이고 실용적인 대응으로 분석한다. 그러나 이 모델이 채택한 해결책들—주로 2단계 탐지기에서 차용한 앵커 박스—이 새로운 차원의 복잡성과 하이퍼파라미터 민감도를 도입하여, 한 세트의 문제를 해결하는 동시에 다른 문제를 야기했음을 주장한다.</p>
<h3>2.1  앵커 박스: 사전 형태(Prior Shapes)에 대한 해결책이자 하이퍼파라미터 민감도의 시작</h3>
<p>YOLOv2는 마지막 완전 연결 계층(fully connected layers)을 제거하고 경계 상자를 예측하기 위해 앵커 박스(anchor boxes)를 도입한다.9 이를 통해 모델은 미리 정의된 다양한 형태와 크기의 상자를 예측할 수 있게 되어, 모든 객체를 찾는 능력인 재현율을 크게 향상시킨다.</p>
<p>앵커 박스의 채택은 주요한 패러다임 전환이었으며, YOLOv2를 Faster R-CNN 및 SSD와 같은 아키텍처에 더 가깝게 만들었다. 앵커 박스는 YOLOv1의 주요 결함이었던 객체 형태에 대한 더 나은 “사전 정보(priors)” 또는 초기 추측을 모델에 제공한다. 그러나 이는 새로운 치명적인 문제를 야기한다: 모델의 성능이 이러한 앵커 박스의 선택에 매우 의존하게 된다는 점이다. YOLOv1은 상자 차원을 처음부터 예측했기 때문에 비표준적인 종횡비를 가진 객체를 다루는 데 어려움을 겪었다. 반면 YOLOv2는 앵커 박스를 미리 정의된 템플릿으로 사용한다.12 이는 일반적인 형태의 객체를 탐지하는 능력을 즉시 향상시킨다. 그러나 이러한 앵커 박스는 하이퍼파라미터이다. 만약 선택된 앵커가 데이터셋의 객체 형태 분포와 일치하지 않으면 성능이 저하될 것이다. 이 문제를 해결하기 위해 저자들은 훈련 세트의 경계 상자에 k-평균 군집화(k-means clustering)를 사용하여 좋은 앵커 사전 정보를 자동으로 찾는 방법을 제안했다.12 이는 지능적인 해결책이지만, 데이터에 의존적인 전처리 단계를 추가하며, 특히 다양한 객체 형태를 가진 데이터셋에 대해 최적성을 보장하지는 않는다. 위치 특정 문제는 부분적으로 네트워크에서 하이퍼파라미터 최적화 문제로 이전되었다.</p>
<h3>2.2  Darknet-19와 아키텍처 개선: 불안정성 완화</h3>
<p>YOLOv2는 Darknet-19라는 새롭고 더 단순한 백본(backbone)을 도입한다.9 또한 모든 컨볼루션 계층에 배치 정규화(Batch Normalization)를 통합하여 훈련을 안정화하고, 수렴을 개선하며, 정규화 효과를 제공한다.10 다중 스케일 훈련(multi-scale training)도 도입되어 네트워크가 다양한 크기의 이미지로 훈련됨으로써 더 견고해진다.10</p>
<p>이러한 변화들은 더 복잡한 앵커 기반 예측을 안정적으로 만드는 데 결정적이었다. 특히 배치 정규화는 핵심적인 조력자 역할을 했으며, 단독으로도 2%의 mAP 이득을 가져왔고 드롭아웃(dropout)의 필요성을 제거했다.13 다중 스케일 훈련은 YOLOv1의 고정 해상도 입력에 대한 직접적인 대응이었지만, 이는 또한 약점을 드러낸다. 즉, 모델의 성능이 입력 크기 및 속도와의 트레이드오프 관계에 놓이게 된다는 점이다.13 사용자는 이제 이 속도/정확도 곡선상의 한 지점을 선택해야 하며, 이는 배포 복잡성을 한층 더 가중시킨다.</p>
<h3>2.3  경계 상자 예측의 미해결 문제와 YOLOv2 손실 함수</h3>
<p>YOLOv2는 제약 없는 위치 예측으로 인해 초기 훈련 반복 동안 여전히 불안정성을 겪는다.13 이를 해결하기 위해, 그리드 셀에 상대적인 위치 좌표를 예측하고 로지스틱 활성화 함수(<span class="math math-inline">\sigma</span>)를 사용하여 오프셋을 셀 내로 제한한다.13 또한, 소형 객체 탐지를 돕기 위해 이전 계층의 미세한 특징(fine-grained features)을 가져오는 “통과 계층(passthrough layer)“을 사용한다.14</p>
<p>이러한 개선 사항들은 점진적인 수정에 해당한다. 매우 작은 객체를 탐지하는 핵심 문제는 완화되었지만 여전히 남아있다. 손실 함수는 제공된 자료에서 전체 공식이 명시적으로 상세히 설명되지는 않았지만, 여전히 좌표, 신뢰도, 분류 오류의 균형을 맞추는 복합 함수이다.7</p>
<p>손실 함수는 완전히 원칙에 입각한 목표보다는 경험적 구성 요소에 계속 의존한다. 손실 함수는 좌표 손실, 신뢰도(객체 존재 여부) 손실, 분류 손실의 합으로 구성된다.15 좌표 손실은 여전히 <span class="math math-inline">(x, y, w, h)</span> 차이에 대한 SSE와 유사한 항에 기반하지만, 이제는 앵커 사전 정보로부터의 오프셋에 적용된다.15 이는 여전히 IoU를 직접적으로 최적화하지 않는다. 신뢰도 손실은 복잡하며, 객체(<code>obj</code>)와 비객체(<code>noobj</code>)에 대한 페널티를 구분하고, 음성 샘플(negative samples)을 정의하기 위해 임계값(예: IoU &lt; 0.6)을 사용한다.15 이 임계값은 또 다른 민감한 하이퍼파라미터이다. 전체 손실은 이러한 구성 요소들의 가중 합이며, 그 기여도를 조절하기 위해 <span class="math math-inline">\lambda</span> 값을 사용한다.7 이러한 구조는 기능적이기는 하지만, 후대의 IoU 기반 손실 함수들보다 덜 정교하며 이러한 <span class="math math-inline">\lambda</span> 가중치들의 신중한 튜닝을 요구한다. 따라서 모델의 성능은 이질적인 손실 항들의 섬세한 균형에 민감하다.</p>
<p>15에 기술된 구성 요소들을 바탕으로, YOLOv2 손실 함수는 개념적으로 다음과 같이 작성될 수 있다:<br />
<span class="math math-display">
L = \lambda_{coord} L_{coord} + \lambda_{obj} L_{conf\_obj} + \lambda_{noobj} L_{conf\_noobj} + \lambda_{class} L_{class}
</span><br />
여기서 <span class="math math-inline">L_{coord}</span>는 양성 앵커(positive anchors)에 대한 <span class="math math-inline">(x, y, w, h)</span> 예측 오류에 페널티를 부과하고, <span class="math math-inline">L_{conf\_obj}</span>는 양성 앵커에 대한 낮은 신뢰도에 페널티를, <span class="math math-inline">L_{conf\_noobj}</span>는 음성 앵커에 대한 높은 신뢰도에 페널티를 부과하며, <span class="math math-inline">L_{class}</span>는 양성 앵커에 대한 분류 손실이다. 각 구성 요소는 자체적인 복잡한 공식과 하이퍼파라미터 의존성을 가진다.</p>
<h2>3.  YOLOv3 - 세분화의 추구와 그 계산 비용</h2>
<p>이 장에서는 YOLOv3를 2단계 탐지기와의 정확도 격차, 특히 소형 객체에 대한 격차를 줄이기 위한 중요한 단계로 평가한다. 그러나 이는 훨씬 더 깊은 아키텍처와 다중 스케일 예측 메커니즘을 채택함으로써 달성되었으며, 이는 새롭고 중대한 문제를 야기했다. 즉, 계산 및 메모리 오버헤드의 상당한 증가로 인해 자원이 제한된 장치에서의 배포에 어려움을 초래한 것이다.</p>
<h3>3.1  Darknet-53과 특징 피라미드 네트워크: 다중 스케일 능력의 도약</h3>
<p>YOLOv3는 Darknet-19를 훨씬 더 깊은 53개 계층의 네트워크인 Darknet-53으로 대체한다.9 이 네트워크는 ResNet-101/152와 같은 대안보다 더 강력하고 효율적이다.16 결정적으로, YOLOv3는 다양한 크기의 객체 탐지를 개선하기 위해 세 가지 다른 스케일에서 예측을 수행하는데, 이는 특징 피라미드 네트워크(Feature Pyramid Network, FPN)와 유사하다.9</p>
<p>이 다중 스케일 예측 전략은 YOLOv3에서 가장 중요한 단일 아키텍처 변화이며, 지속적인 약점이었던 소형 객체 탐지를 직접적으로 겨냥한다.16 서로 다른 해상도의 특징 맵에서 예측을 수행함으로써, 모델은 소형 객체를 위해 고해상도 맵을 사용하고, 대형 객체를 위해 의미적으로 풍부한 저해상도 맵을 사용할 수 있다. 이는 엄청난 개선이었으며, 소형 객체에 대한 AP(Average Precision)를 크게 향상시켰다.16</p>
<h3>3.2  높은 IoU 정밀도 문제: 미세한 위치 특정의 지속적인 약점</h3>
<p>이러한 개선에도 불구하고, YOLOv3의 성능은 mAP@0.75와 같은 더 엄격한 위치 특정 지표에서는 여전히 저하되었다.2 이는 모델이 객체를 <em>찾는</em> 능력(더 높은 재현율)은 향상되었지만, 2단계 탐지기에 비해 매우 정밀한 경계 상자 정렬을 달성하는 데는 여전히 어려움을 겪었음을 나타낸다.</p>
<p>이 한계의 핵심 원인은 다시 한번 손실 함수에 있다. YOLOv3는 경계 상자 회귀를 위해 여전히 전통적인 손실(변환된 좌표에 대한 제곱합 오차)을 사용하고, 객체 존재 여부 및 클래스 예측을 위해 이진 교차 엔트로피(Binary Cross-Entropy)를 사용한다.16 경계 상자 회귀 손실은 예측된 좌표(<span class="math math-inline">t_x, t_y, t_w, t_h</span>)와 목표값 사이의 제곱 오차를 최소화한다.21 이 손실 지표는 IoU와 직접적인 상관관계가 없다. 좌표에 대한 낮은 SSE를 가지면서도 낮은 IoU를 가질 수 있으며, 그 반대도 가능하다. 따라서 모델은 IoU를 최대화하도록 명시적으로 최적화되지 않는다. 일반적으로 좋은 상자를 생성하도록 학습되지만, 높은 IoU 임계값(예: 0.75 또는 0.9)에 필요한 픽셀 수준의 완벽한 정렬을 달성하도록 압박받지는 않는다. 이는 mAP@0.75에서의 성능 저하를 설명한다.2 모델은 mAP@0.5에는 “충분히 좋지만” 더 높은 정밀도에서 탁월한 성능을 발휘하기 위한 미세 조정 압력이 부족하며, 이 문제는 후속 버전에서 IoU 기반 손실 함수가 도입되면서 비로소 제대로 해결될 것이다.</p>
<h3>3.3  효율성 트레이드오프: 모델 깊이 증가와 배포에 미치는 영향</h3>
<p>더 깊은 Darknet-53 백본으로의 전환은 정확도를 높이는 동시에, 이전 YOLO 버전에 비해 더 높은 계산 및 메모리 요구 사항을 초래했다.2 이는 YOLO 철학에서 중요한 전환점을 의미한다. 2단계 탐지기에 비해 여전히 빠르지만 22, 원래 YOLO의 “경량” 특성은 정확도를 위해 희생되고 있었다. 이는 새로운 문제, 즉 배포 오버헤드(deployment overhead)를 야기했다.2 더 커진 모델 크기와 증가된 계산 요구량은 YOLO가 이전에 탁월했던 분야인 제한된 자원을 가진 엣지 장치에서 YOLOv3를 실행하는 데 어려움을 초래했다. 이로 인해 YOLOv3-tiny와 같은 더 작은 변형에 대한 수요가 발생했으며 19, 이는 주력 모델이 특정 응용 분야에는 너무 무거워지고 있음을 인정한 것이다.</p>
<h2>4.  YOLOv4 - 기술의 통합과 복잡성 문제</h2>
<p>이 장에서는 YOLOv4를 단일 아키텍처 혁신이 아닌, 수십 가지 기존 기술을 최적화된 전체로 결합한 엔지니어링 및 경험적 검증의 대가로 묘사한다. 따라서 YOLOv4의 주된 “문제“는 특정 성능 결함이 아니라, 그 엄청난 복잡성과 “모든 것을 다 넣는(kitchen sink)” 접근 방식으로, 이는 아키텍처를 덜 간결하게 만들고 분석을 더 어렵게 만든다.</p>
<h3>4.1  “Bag of Freebies” (BoF): “무료” 정확도 향상의 숨겨진 훈련 비용</h3>
<p>BoF는 추론 비용을 증가시키지 않으면서 정확도를 향상시키는 훈련 시점의 방법들을 의미한다. YOLOv4는 CutMix 및 Mosaic 데이터 증강, DropBlock 정규화, 클래스 레이블 스무딩, 자기-적대적 훈련(Self-Adversarial Training, SAT), 단일 실제값에 여러 앵커 사용 등 방대한 기술들을 채택했다.6</p>
<p>“Freebies(공짜)“라는 용어는 오해의 소지가 있다. 이 기술들은 <em>추론</em> 비용을 추가하지는 않지만, <em>훈련</em>의 복잡성, 시간, 그리고 계산 자원 요구량을 현저히 증가시킨다. Mosaic 증강(네 개의 이미지를 하나로 합치는 것)이나 SAT(2단계 적대적 프로세스)와 같은 기술들은 훈련 루프 동안 계산 비용이 많이 든다.23 이들은 더 정교한 데이터 로더와 훈련 파이프라인을 요구한다. 상호작용하는 수많은 BoF 기술들 때문에 하이퍼파라미터 튜닝은 기하급수적으로 더 어려워진다. 저자들 스스로가 최적의 하이퍼파라미터를 찾기 위해 유전 알고리즘을 사용했는데, 이는 대부분의 실무자들에게는 불가능한 과정이다.23 따라서 “비용“은 추론에서 훈련 및 개발 단계로 이전된다. 제한된 GPU 자원을 가진 연구자나 기업에게 YOLOv4의 훈련 과정을 재현하는 것은 엄청난 도전이다. 문제는 접근성과 재현성에 있다.</p>
<h3>4.2  “Bag of Specials” (BoS): 고급 모듈의 추론 오버헤드 정량화</h3>
<p>BoS는 추론 비용을 약간 증가시키지만 정확도를 크게 향상시키는 플러그인 모듈 및 후처리 방법을 의미한다. YOLOv4는 Mish 활성화 함수, 백본의 교차 단계 부분 연결(Cross-stage partial connections, CSP) (CSPDarknet53), 공간 피라미드 풀링(Spatial Pyramid Pooling, SPP), 그리고 경로 집계 네트워크(Path Aggregation Network, PAN) 넥(neck)을 사용한다.9</p>
<p>이러한 각 모듈은 아키텍처 복잡성을 한층 더하고 정량화 가능한 추론 비용을 추가한다. 예를 들어, SPP 블록은 AP50을 2.7% 증가시키는 대신 계산 비용을 0.5% 증가시키는 것으로 나타났다.23 각각의 개별 비용은 작지만, 그 누적 효과는 YOLOv3보다 더 크고 계산 집약적인 모델에 기여하며, 원시 속도를 더 높은 정확도와 맞바꾸는 추세를 이어간다.</p>
<h3>4.3  CIoU 손실 함수에 대한 비판적 분석</h3>
<p>YOLOv4는 경계 상자 회귀를 위해 완전한 IoU(Complete Intersection over Union, CIoU) 손실을 채택한다.6 이는 이전 버전들의 SSE 기반 손실로부터의 주요한 진전이었다.</p>
<p>CIoU 손실은 IoU와 밀접하게 관련된 지표들을 직접적으로 최적화한다. 이는 세 가지 핵심 기하학적 요소를 고려한다: 겹침 영역(IoU), 중심점 거리, 그리고 종횡비 일관성.6 이는 YOLOv3의 높은 IoU 정밀도 문제를 직접적으로 해결한다.</p>
<p>CIoU 손실 함수는 다음과 같다:<br />
<span class="math math-display">
L_{CIoU} = 1 - IoU + \frac{\rho^2(b, b^{gt})}{c^2} + \alpha v
</span><br />
여기서 <span class="math math-inline">\rho^2(b, b^{gt})</span>는 예측 상자 <span class="math math-inline">b</span>와 실제 상자 <span class="math math-inline">b^{gt}</span>의 중심점 사이의 유클리드 거리의 제곱이고, <span class="math math-inline">c</span>는 두 상자를 포함하는 가장 작은 볼록 상자의 대각선 길이며, <span class="math math-inline">\alpha v</span>는 종횡비 차이에 페널티를 부과하는 항이다.</p>
<p>CIoU는 상당한 개선이지만, SSE보다 계산이 더 복잡하다. 더 중요한 것은, 종횡비에 대한 페널티 항(<span class="math math-inline">v</span>)이 때때로 훈련 중 불안정한 그래디언트를 유발할 수 있다는 점이며, 이는 후속 손실 함수(예: SIoU)가 해결하고자 했던 문제이다. 이는 경계 상자 회귀 손실에 대한 거의 최적이지만 아직 완벽하지는 않은 해결책을 나타낸다.</p>
<h2>5.  YOLOv5 - PyTorch 시대와 출처의 문제</h2>
<p>이 장에서는 YOLOv5를 주로 기술적 장점보다는 출시를 둘러싼 논란의 관점에서 분석한다. YOLOv5와 관련된 핵심 “문제“는 과학적 출판 및 벤치마킹의 전통적인 규범에 도전하여 커뮤니티에 혼란과 회의론을 야기했다는 점이다.</p>
<h3>5.1  아키텍처 전환 대 프레임워크 이식</h3>
<p>YOLOv5는 Darknet 대신 PyTorch로 네이티브하게 출시된 최초의 주요 버전이다.25 아키텍처는 YOLOv4의 개념을 사용하지만 YAML 구성 파일로 공식화되어 있으며 CSP 기반 백본과 PANet 넥을 활용한다.27 이는 매우 빠르고 경량이다 (YOLOv5s는 27MB에 불과하다).25</p>
<p>PyTorch로의 전환은 사용성과 접근성 측면에서 엄청난 기여를 했다.25 그러나 아키텍처 변경 자체는 YOLOv4에서 통합된 개념을 기반으로 한 진화적인 것이었지 혁명적인 것은 아니었다. 주요 혁신은 엔지니어링과 프레임워크 구현에 있었으며, 이는 YOLO 계열을 훨씬 더 넓은 사용자층에게 접근 가능하게 만들었다.</p>
<h3>5.2  명명 논란과 동료 심사 논문의 부재</h3>
<p>YOLOv5는 YOLOv4 직후 Ultralytics의 Glenn Jocher에 의해 학술 논문 없이 출시되었다.25 이는 원저자나 YOLOv4 팀에서 나온 것이 아니었고 논문의 과학적 검증이 부족했기 때문에 “YOLOv5“라고 불려야 하는지에 대한 상당한 논쟁을 불러일으켰다.26</p>
<p>YOLOv5 논란은 빠르게 진행되는 반복적인 오픈소스 소프트웨어 개발 문화와 더 느리고 엄격한 학술 연구 표준이 충돌한 중대한 순간을 나타낸다. 학문적 진보는 전통적으로 새로운 기여를 상세히 기술하고 재현 가능한 벤치마크를 제공하는 동료 심사 논문을 통해 검증된다(예: YOLOv1-v4). Ultralytics의 YOLOv5는 공식 논문보다 빠른 개발, 엔지니어링 우수성, 사용자 접근성을 우선시하여 GitHub 저장소로 출시되었다.26 커뮤니티의 반발은 이러한 전통의 파괴에서 비롯되었다. “YOLO“라는 이름은 상당한 학문적 비중을 가지며, 논문 없이 이를 사용하는 것은 일부에게는 기대되는 과학적 기여나 정당성 제공 없이 브랜드를 활용하는 것으로 비춰졌다.26 이는 <em>출처</em>와 <em>정당성</em>의 문제를 야기했다. 논문이 없으면 연구자들이 구체적인 새로운 기여를 확인하고 초기 성능 주장을 신뢰하기 어려웠다. 문제는 반드시 모델의 품질이 아니라, 그것을 검증할 표준적인 과학적 결과물이 없다는 점이었다.</p>
<h3>5.3  벤치마킹 불일치와 커뮤니티 인식에 미친 영향</h3>
<p>제3자(Roboflow)의 초기 블로그 게시물들은 YOLOv5의 성능에 대해 대담한 주장(예: 140 FPS)을 했으나, 이는 나중에 결함 있는 방법론에 기반한 것으로 밝혀졌다.25 YOLOv4의 저자인 Alexey Bochkovskiy는 해당 비교가 지연 시간 측정에 큰 배치 크기를 사용했으며, 작고 덜 정확한 YOLOv5 변형을 크고 정확한 YOLOv4 변형과 비교했다고 지적했다.26</p>
<p>이 사건은 YOLOv5의 초기 인식을 심각하게 손상시켰다. 표준화된 벤치마크를 담은 공식 논문의 부재는 정보의 공백을 만들었고, 이 공백은 오해의 소지가 있는 마케팅 주장으로 채워졌다. 이는 시스템적인 문제를 드러낸다. 즉, 개발이 공식적인 출판을 앞지를 때, 커뮤니티는 일관성 없고 재현 불가능한 성능 지표에 취약해져 혼란과 불신을 낳게 된다는 것이다.26 “문제“는 성능 평가를 위한 신뢰할 수 있는 중앙 집중식 정보원의 상실이었다.</p>
<h2>6.  대분열 시대: YOLOv6, v7, v8의 비교 분석</h2>
<p>이 장에서는 YOLOv5 이후 시대를 다루며, 이는 여러 연구 그룹에서 다수의 YOLO 버전이 병렬적으로 개발되는 특징을 보인다. 가장 큰 문제는 단일하고 선형적인 발전 과정이 부재하여, 실무자들이 서로 다른 설계 철학과 트레이드오프를 가진 경쟁 아키텍처들 사이에서 선택해야 하는 혼란스러운 환경이 조성되었다는 점이다.</p>
<h3>6.1  YOLOv6: 재매개변수화와 산업적 초점의 이중성</h3>
<p>Meituan에서 개발한 YOLOv6는 재매개변수화된(reparameterized) 하드웨어 효율적인 백본(EfficientRep)과 분리된 헤드(decoupled head)를 도입한다.32 재매개변수화는 모델이 훈련 중에는 더 복잡한 구조를 가지지만, 추론 시에는 더 단순하고 빠른 구조로 융합될 수 있음을 의미한다. 또한 앵커 프리(anchor-free) 접근 방식을 사용한다.33</p>
<p>재매개변수화는 새로운 종류의 문제를 야기한다: 훈련 아키텍처와 추론 아키텍처 간의 상당한 괴리이다. 훈련 중 네트워크는 특징 표현과 그래디언트 흐름을 개선하기 위해 다중 분기 구조(예: RepVGG 블록)를 사용한다.18 배포 전, 이 분기들은 수학적으로 단일의 단순한 컨볼루션 계층으로 융합되어 추론 시 매우 빠르다. 문제는 이것이 개발 및 훈련 파이프라인을 복잡하게 만든다는 점이다. 코드는 두 가지 다른 모델 상태를 관리해야 하며, 훈련 시 모델의 동작이 추론 시 모델과 직접적으로 매핑되지 않기 때문에 훈련 문제 디버깅이 더 어려워진다. 이는 인상적인 속도를 달성하기 위해 새로운 차원의 엔지니어링 복잡성을 도입한다. 또한, 고효율적이기는 하지만 YOLOv6는 주로 객체 탐지에 초점을 맞추고 있어, 나중에 YOLOv8과 같은 모델이 제공할 분할이나 자세 추정 같은 다른 작업에 대한 내장된 다용성이 부족하다.34</p>
<h3>6.2  YOLOv7: “훈련 가능한 Bag-of-Freebies“와 고급 모델 스케일링의 복잡성</h3>
<p>YOLOv4 팀이 개발한 YOLOv7은 확장된 효율적 계층 집계 네트워크(Extended Efficient Layer Aggregation Network, E-ELAN)와 같은 고급 아키텍처 개념과, 재매개변수화 컨볼루션 및 보조 헤드를 이용한 심층 감독(deep supervision)과 같은 기술들이 훈련 과정 자체에 통합된 “훈련 가능한 bag-of-freebies“를 도입한다.9</p>
<p>YOLOv7은 YOLOv4 팀의 “복잡하지만 강력한” 설계 철학의 정점을 나타낸다. 문제는 아키텍처 구성 요소들이 매우 정교하고 이해하거나 구현하기가 쉽지 않다는 점이다. 연결 기반 모델을 위한 “복합 모델 스케일링“이나 “coarse-to-fine 선도 헤드 유도 레이블 할당“과 같은 개념들은 강력하지만 개념적 복잡성을 크게 증가시킨다.35 이는 모델을 많은 실무자들에게 “블랙박스“로 만들고, 맞춤형 수정이나 확장에 대한 장벽을 높인다. 주된 문제는 핵심 설계의 해석 가능성과 접근성에 있다.</p>
<h3>6.3  YOLOv8: 앵커 프리 패러다임, C2f 모듈, 그리고 새로운 배포 고려사항</h3>
<p>Ultralytics(YOLOv5 개발사)가 개발한 YOLOv8은 C2f 모듈(C3 모듈 대체)을 사용하는 새로운 CSP 기반 백본, 앵커 프리 분리된 헤드, 그리고 새로운 손실 함수를 통해 YOLOv5의 성공을 기반으로 구축되었다.36 이는 탐지, 분할, 분류를 위한 통합 프레임워크로 제시된다.36</p>
<p>앵커 프리 헤드의 채택은 주요한 진전으로, 출력 계층을 단순화하고 데이터에 의존적인 앵커 생성 단계(YOLOv2 이후 지속된 문제)의 필요성을 제거한다.36 분리된 헤드는 분류와 회귀를 분리하여 성능을 향상시킬 수 있지만, 두 작업 간의 불일치 위험도 초래한다.37</p>
<p>성능은 뛰어나지만, YOLOv8은 대형 변형(예: YOLOv8x)에서 계산 요구량이 증가하는 추세를 이어간다.39 더 미묘하게는, 다른 단일 단계 탐지기들과 마찬가지로, 특화된 2단계 탐지기에 비해 극도로 작거나 밀집된 객체를 탐지하는 데 여전히 어려움을 겪을 수 있으며, 이는 YOLO 패러다임의 지속적이지만 줄어든 한계를 나타낸다.34 더욱이, YOLOv5와 마찬가지로 전통적인 학술 논문 없이 출시되어 문서와 코드에 의존하며, 이는 공식적인 동료 심사보다 오픈소스 출시를 우선시하는 경향을 이어간다.36</p>
<h2>7.  종합 분석 및 향후 전망</h2>
<p>이 마지막 장에서는 연대기적 분석에서 얻은 결과들을 종합하여, YOLO 계열의 진화를 정의하는 포괄적인 주제와 지속적인 과제들을 식별한다.</p>
<h3>7.1  YOLO 계열 전반에 걸친 지속적인 과제에 대한 종합적 시각</h3>
<p>초기의 단순한 속도와 정확도 간의 트레이드오프는 더 복잡한 세 가지 문제, 즉 속도-정확도-복잡성 삼각형으로 진화했다. 후기 버전들은 높은 정확도와 속도를 달성하지만, 이는 엄청난 아키텍처 및 훈련 복잡성을 대가로 한다.</p>
<p>소형 객체 탐지는 통과 계층(v2)에서 FPN(v3), 고급 넥 디자인(v4 이상)으로 이어지는 해결책의 진화를 통해 혁신의 지속적인 원동력이었음을 알 수 있다. 그럼에도 불구하고, 이는 특화된 탐지기에 비해 여전히 상대적인 약점으로 남아있다.</p>
<p>v3 이후의 분기는 시스템적인 문제이다. 이는 버전 번호가 더 이상 직접적이고 선형적인 개선을 의미하지 않는 혼란스러운 생태계를 조성했으며, 사용자들은 서로 다른 철학을 가진 여러 저자의 경쟁 모델들 사이에서 복잡한 환경을 탐색해야만 한다.</p>
<h3>7.2  손실 함수의 궤적: 비교 요약</h3>
<p>이 섹션은 손실 함수에 대한 요약 및 비판적 비교를 제공하며, 경험적 방법에 의존하는 SSE에서 더 원칙적이고 기하학을 고려한 IoU 기반 손실로의 명확한 경향을 강조한다. 이 진화는 전체 YOLO 이야기의 축소판이다: 단순하지만 결함이 있는 초기 개념에서 시작하여, 일련의 더 복잡하지만 효과적인 해결책으로 나아가는 과정이다.</p>
<table><thead><tr><th>버전</th><th>손실 함수 유형</th><th>LaTeX 공식 (개념적)</th><th>비판 / 한계점</th></tr></thead><tbody>
<tr><td><strong>YOLOv1</strong></td><td>제곱합 오차 (SSE)</td><td><span class="math math-inline">\sum (x-\hat{x})^2 + (y-\hat{y})^2 + (\sqrt{w}-\sqrt{\hat{w}})^2 + (\sqrt{h}-\sqrt{\hat{h}})^2</span></td><td>IoU 지표와 연계되지 않음. 소형 및 대형 상자 오류를 동일하게 취급 ( <span class="math math-inline">\sqrt{\cdot}</span> 로 부분 완화). 수동 <span class="math math-inline">\lambda</span> 균형 조정 필요.</td></tr>
<tr><td><strong>YOLOv2/v3</strong></td><td>변환된 좌표에 대한 SSE</td><td><span class="math math-inline">\sum (t_x-\hat{t}_x)^2 + (t_y-\hat{t}_y)^2 +...</span></td><td>여전히 IoU를 직접 최적화하지 않음. 높은 IoU 임계값에서 낮은 성능으로 이어짐.</td></tr>
<tr><td><strong>YOLOv4</strong></td><td>CIoU 손실</td><td><span class="math math-inline">1 - IoU + \frac{\rho^2(b, b^{gt})}{c^2} + \alpha v</span></td><td>겹침, 중심 거리, 종횡비를 직접 최적화. 주요 개선. 그러나 종횡비 항(<span class="math math-inline">v</span>)이 불안정한 그래디언트를 유발할 수 있음.</td></tr>
<tr><td><strong>YOLOv5</strong></td><td>CIoU/GIoU 손실</td><td>(YOLOv4와 유사)</td><td>CIoU의 강점과 사소한 약점을 계승. 특정 변형은 구성 가능.</td></tr>
<tr><td><strong>YOLOv8</strong></td><td>CIoU 손실 + 분포 초점 손실 (DFL)</td><td><span class="math math-inline">L_{box} = \alpha L_{CIoU} + \beta L_{DFL}</span></td><td>기하학적 손실(CIoU)과 상자 경계 분포 학습(DFL)을 결합하여 모호한 경우의 정확도 향상. 손실 함수 복잡성 증가.</td></tr>
</tbody></table>
<p>7.3. 결론: 단일 비전에서 분열된 생태계로</p>
<p>이 안내서는 YOLO의 여정을 되돌아보며 마무리한다. 그것은 단일 저자의 독창적이고 우아하지만 결함이 있는 비전에서 시작되었다. 이후 여러 상업 및 학술 팀에 의해 주도되는 분열되었지만 강력한 모델 생태계로 진화했다. YOLO의 “문제“는 더 이상 단일 모델의 결함에 관한 것이 아니라, 이 풍부하고 다양한 혈통의 복잡성을 탐색하는 것에 관한 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>[1506.02640] You Only Look Once: Unified, Real-Time Object Detection - arXiv, https://arxiv.org/abs/1506.02640</li>
<li>YOLOv1 to YOLOv11: A Comprehensive Survey of Real-Time Object Detection Innovations and Challenges - arXiv, https://arxiv.org/html/2508.02067v1</li>
<li>arXiv:1506.02640v5 [cs.CV] 9 May 2016, https://arxiv.org/pdf/1506.02640</li>
<li>YOLOv1 to YOLOv10: The fastest and most accurate real-time object detection systems - arXiv, https://arxiv.org/html/2408.09332v1</li>
<li>YOLO-v1 to YOLO-v8, the Rise of YOLO and Its Complementary Nature toward Digital Manufacturing and Industrial Defect Detection - MDPI, https://www.mdpi.com/2075-1702/11/7/677</li>
<li>YOLOv4: A Breakthrough in Real-Time Object Detection - arXiv, https://arxiv.org/html/2502.04161v1</li>
<li>Yolo Loss function explanation - Cross Validated - Stack Exchange, https://stats.stackexchange.com/questions/287486/yolo-loss-function-explanation</li>
<li>YOLO loss function width and height component explanation - Cross Validated, https://stats.stackexchange.com/questions/332907/yolo-loss-function-width-and-height-component-explanation</li>
<li>ODverse33: Is the New YOLO Version Always Better? A Multi Domain benchmark from YOLO v5 to v11 - arXiv, https://arxiv.org/html/2502.14314v2</li>
<li>A Review of YOLOv12: Attention-Based Enhancements vs. Previous Versions - arXiv, https://arxiv.org/html/2504.11995v1</li>
<li>arXiv:2504.11995v1 [cs.CV] 16 Apr 2025, <a href="https://arxiv.org/pdf/2504.11995">https://arxiv.org/pdf/2504.11995?</a></li>
<li>An Improved YOLOv2 for Vehicle Detection - MDPI, https://www.mdpi.com/1424-8220/18/12/4272</li>
<li>YOLO v2 - Object Detection - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/yolo-v2-object-detection/</li>
<li>【ML Paper】YOLOv2 Summary - Zenn, https://zenn.dev/yuto_mo/articles/308f63fb92d7c7</li>
<li>YOLO v2 loss function - Cross Validated - Stack Exchange, https://stats.stackexchange.com/questions/575783/yolo-v2-loss-function</li>
<li>YOLOv3: Real-Time Object Detection Algorithm (Guide) - Viso Suite, https://viso.ai/deep-learning/yolov3-overview/</li>
<li>Object Detection Using YOLO V3 - IJNRD, https://www.ijnrd.org/papers/IJNRD2302040.pdf</li>
<li>YOLO Evolution: A Comprehensive Benchmark and Architectural Review of YOLOv12, YOLO11, and Their Previous Versions - arXiv, https://arxiv.org/html/2411.00201v2</li>
<li>YOLO v3-Tiny: Object Detection and Recognition using one stage improved model, https://www.researchgate.net/publication/340893960_YOLO_v3-Tiny_Object_Detection_and_Recognition_using_one_stage_improved_model</li>
<li>What is the loss function of YOLOv3 - Stack Overflow, https://stackoverflow.com/questions/55395205/what-is-the-loss-function-of-yolov3</li>
<li>Yolo v3 loss function - Cross Validated - Stack Exchange, https://stats.stackexchange.com/questions/373266/yolo-v3-loss-function</li>
<li>[1804.02767] YOLOv3: An Incremental Improvement - arXiv, https://arxiv.org/abs/1804.02767</li>
<li>(PDF) YOLOv4: Optimal Speed and Accuracy of Object Detection, https://www.researchgate.net/publication/340883401_YOLOv4_Optimal_Speed_and_Accuracy_of_Object_Detection</li>
<li>YOLOv4 — Version 1: Bag of Freebies - Object Detection - Medium, https://medium.com/visionwizard/yolov4-bag-of-freebies-dc126623fc2d</li>
<li>YOLOv5 is Here: State-of-the-Art Object Detection at 140 FPS - Roboflow Blog, https://blog.roboflow.com/yolov5-is-here/</li>
<li>YOLOv5: Advancements &amp; Controversies in Computer Vision - Viso Suite, https://viso.ai/computer-vision/yolov5-controversy/</li>
<li>What is YOLOv5? A Guide for Beginners. - Roboflow Blog, https://blog.roboflow.com/yolov5-improvements-and-evaluation/</li>
<li>ultralytics/yolov5: YOLOv5 in PyTorch &gt; ONNX &gt; CoreML … - GitHub, https://github.com/ultralytics/yolov5</li>
<li>Responding to the Controversy about YOLOv5 - Roboflow Blog, https://blog.roboflow.com/yolov4-versus-yolov5/</li>
<li>[Discussion] Responding to the Controversy about YOLOv5 : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/h7kxpd/discussion_responding_to_the_controversy_about/</li>
<li>I’m just going to call this out as bullshit. This isn’t YOLOv5. I doubt they eve… | Hacker News, https://news.ycombinator.com/item?id=23480884</li>
<li>Meituan YOLOv6 - Ultralytics YOLO Docs, https://docs.ultralytics.com/models/yolov6/</li>
<li>What is YOLOv6? A Deep Insight into the Object Detection Model - arXiv, https://arxiv.org/html/2412.13006v1</li>
<li>YOLOv6-3.0 vs YOLOv8: A Detailed Technical Comparison - Ultralytics Docs, https://docs.ultralytics.com/compare/yolov6-vs-yolov8/</li>
<li>[2207.02696] YOLOv7: Trainable bag-of-freebies sets new state-of …, https://ar5iv.labs.arxiv.org/html/2207.02696</li>
<li>Explore Ultralytics YOLOv8 - Ultralytics YOLO Docs, https://docs.ultralytics.com/models/yolov8/</li>
<li>Explore YOLOv8: Latest in Object Detection Tech - Viso Suite, https://viso.ai/deep-learning/yolov8-guide/</li>
<li>Algorithm principles and implementation with YOLOv8 - MMYOLO’s documentation!, https://mmyolo.readthedocs.io/en/latest/recommended_topics/algorithm_descriptions/yolov8_description.html</li>
<li>YOLOv5 vs. YOLOv8: A Detailed Comparison - Ultralytics YOLO Docs, https://docs.ultralytics.com/compare/yolov5-vs-yolov8/</li>
<li>Object Detection with YOLOv8 Advanced Capabilities - DigitalOcean, https://www.digitalocean.com/community/tutorials/yolov8-a-revolutionary-advancement-in-object-detection-2</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>