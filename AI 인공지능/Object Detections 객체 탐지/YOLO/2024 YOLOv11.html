<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:YOLOv11 (2024)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>YOLOv11 (2024)</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">객체 탐지 (Object Detection)</a> / <a href="index.html">YOLO 객체 탐지 모델</a> / <span>YOLOv11 (2024)</span></nav>
                </div>
            </header>
            <article>
                <h1>YOLOv11 (2024)</h1>
<h2>1. 서론</h2>
<h3>1.1 YOLO 계보의 진화와 실시간 객체 탐지의 패러다임</h3>
<p>컴퓨터 비전 분야에서 객체 탐지(Object Detection)는 기계가 시각적 세계를 이해하는 핵심 기술로 자리매김해왔다. 2015년, Joseph Redmon 등이 발표한 YOLO(You Only Look Once)는 이 분야에 혁명적인 변화를 가져왔다.1 기존의 R-CNN 계열과 같은 2-stage detector들이 후보 영역을 먼저 제안하고 각 영역에 대해 분류를 수행하는 다단계 접근법을 취했던 것과 달리, YOLO는 객체 탐지를 이미지 전체를 한 번만 바라보는 단일 회귀 문제(regression problem)로 재정의했다.1 이 혁신적인 발상은 경계 상자(bounding box) 예측과 클래스 분류를 하나의 통합된 신경망에서 동시에 처리함으로써, 정확도를 일부 희생하는 대신 추론 속도를 전례 없는 수준으로 끌어올렸고, 이는 실시간 객체 탐지라는 새로운 패러다임을 열었다.3</p>
<p>YOLOv3 이후, ’YOLO’라는 명칭은 더 이상 단일 저자나 연구 그룹에 귀속되지 않고, 실시간 고성능 객체 탐지기를 지칭하는 일종의 브랜드이자 기술적 대명사로 자리 잡았다.2 수많은 연구자들이 YOLO의 기본 철학을 계승하며 독자적인 개선 버전을 발표했고, 이 과정에서 YOLOv4, YOLOv5, YOLOv9, YOLOv10 등 주요 모델들이 등장하며 기술적 진보를 이끌었다.3 각 버전은 저마다의 방식으로 속도와 정확도의 균형점을 탐색하며 새로운 아키텍처, 학습 기법, 그리고 후처리 최적화 방안을 제시해왔다.</p>
<p>이러한 기술적 흐름 속에서 Ultralytics는 YOLOv5와 YOLOv8을 연달아 성공시키며 YOLO 생태계의 가장 강력한 관리자이자 핵심 동력으로 부상했다.7 그들은 단순히 고성능 모델을 개발하는 데 그치지 않고, 사용하기 쉬운 파이썬 라이브러리, 포괄적인 기술 문서, 그리고 활발한 커뮤니티 지원을 통해 개발자들의 진입 장벽을 극적으로 낮췄다.8 이는 AI 모델의 성공이 더 이상 학술적 SOTA(State-of-the-Art) 달성에만 국한되지 않음을 시사한다. 사용 편의성, 강력한 생태계, 그리고 명확한 상업화 경로가 결합될 때, 기술은 비로소 시장의 표준으로 자리 잡을 수 있다. YOLOv11의 등장은 단순한 모델 업데이트를 넘어, Ultralytics가 ‘YOLO’ 브랜드를 통해 AI 개발 생태계의 패권을 강화하려는 전략적 행보로 해석될 수 있다.</p>
<h3>1.2 Ultralytics의 YOLOv11 발표: SOTA(State-of-the-Art) 성능의 재정의</h3>
<p>2024년 9월 말, Ultralytics는 연례 AI 컨퍼런스인 YOLO Vision 2024 (YV24)에서 차세대 모델인 YOLOv11을 공식 발표했다.1 이는 YOLOv9과 YOLOv10이 발표된 지 채 1년이 되지 않은 시점이었으며, 시장의 기대를 한 몸에 받았다. YOLOv11은 이전 버전들의 성공적인 발전을 계승하면서, 최신 딥러닝 및 컴퓨터 비전 연구 성과를 집약하여 속도와 정확도 측면에서 타의 추종을 불허하는 성능을 제공하는 것을 목표로 했다.8</p>
<p>YOLOv11은 단순히 특정 벤치마크 점수를 높이는 것을 넘어, 다중 작업(multi-task) 지원, 다양한 하드웨어 플랫폼에 대한 적응성, 그리고 개발자 생태계와의 유기적 결합을 통해 통합적인 비전 AI 프레임워크로의 도약을 꾀했다.8 객체 탐지뿐만 아니라 인스턴스 분할, 자세 추정, 분류 등 다양한 작업을 하나의 모델 아키텍처 내에서 지원함으로써, 개발자들이 복잡한 AI 애플리케이션을 보다 효율적으로 구축할 수 있는 기반을 마련한 것이다.12</p>
<h3>1.3  본 안내서의 목적 및 구성</h3>
<p>본 안내서는 YOLOv11의 기술적 실체를 다각도로 심층 분석하는 것을 목적으로 한다. 아키텍처의 혁신성, 정량적 성능 벤치마크, 학습 전략의 핵심 원리, 그리고 AI 생태계 내에서의 전략적 위치를 종합적으로 고찰할 것이다.</p>
<p>특히 YOLOv11은 Ultralytics의 공식적인 학술 논문 없이 출시되었다는 특징을 가진다.5 이는 연구자들이 모델의 작동 원리와 아키텍처 설계의 이론적 배경을 파악하는 데 상당한 어려움을 야기한다.14 이러한 상황은 AI 연구 개발의 속도전이 낳은 새로운 패러다임을 보여준다. ’빠른 출시와 시장 선점’이 ’엄격한 학술적 검증’보다 우선시될 수 있음을 의미하며, 이는 재현성(reproducibility)과 기술적 투명성에 대한 중요한 논쟁을 제기한다. 따라서 본 안내서는 공개된 소스 코드, 공식 기술 문서, 그리고 여러 연구자들이 발표한 분석 논문들을 면밀히 교차 검증하여, 흩어져 있는 정보들을 종합하고 아키텍처와 성능 사이의 인과관계를 규명하는 데 중점을 둔다. 이를 통해 YOLOv11의 기술적 가치를 객관적으로 평가하고, 실시간 비전 AI의 미래 방향성을 전망하고자 한다.</p>
<h2>2.  YOLOv11 아키텍처 심층 분석</h2>
<p>YOLOv11의 아키텍처는 이전 버전들의 성공적인 설계를 계승하면서도, 효율성과 정확도를 극대화하기 위한 몇 가지 핵심적인 혁신을 도입했다. 전반적인 구조는 특징 추출을 담당하는 백본(Backbone), 특징을 융합하고 정제하는 넥(Neck), 그리고 최종 예측을 수행하는 헤드(Head)의 3단 구조를 유지한다.15 YOLOv11의 설계 철학은 YOLOv10이 시도했던 NMS-Free와 같은 혁명적인 구조 변경보다는, YOLOv8의 검증된 아키텍처를 기반으로 약점을 보완하고 효율성을 최적화하는 데 있다. 이는 Ultralytics가 급진적 변화로 인한 잠재적 불안정성보다 기존 생태계와의 호환성을 유지하며 점진적으로 성능을 개선하는 안정적인 경로를 선호함을 보여준다. 이는 실용주의적 공학 접근법의 승리로, 검증된 아키텍처를 정교하게 튜닝하고 최신 연구 성과를 실용적으로 통합하여 SOTA를 달성할 수 있음을 증명한다.</p>
<h3>2.1  통합 구조: 백본(Backbone), 넥(Neck), 헤드(Head)</h3>
<h4>2.1.1  백본 (Backbone)</h4>
<p>백본은 입력 이미지로부터 의미론적 특징을 추출하는 네트워크의 중추이다. YOLOv11의 백본은 다수의 표준 컨볼루션(Conv) 블록과 새롭게 도입된 C3k2 블록으로 구성된다.15 입력 이미지는 일련의 다운샘플링 과정을 거치며 공간적 해상도는 점차 감소하고 채널의 수는 증가한다. 이 과정을 통해 다양한 스케일의 특징 맵(feature map) P1(가장 큰 해상도)부터 P5(가장 작은 해상도)까지 생성된다.15 백본의 마지막 단에는 SPPF(Spatial Pyramid Pooling - Fast) 블록과 C2PSA(Convolutional block with Parallel Spatial Attention) 블록이 위치하여, 다중 스케일 컨텍스트 정보를 집약하고 공간적 중요도에 따라 특징을 가중하여 고수준의 특징 표현을 완성한다.15</p>
<h4>2.1.2  넥 (Neck)</h4>
<p>넥은 백본에서 추출된 서로 다른 수준의 특징 맵들을 융합하여 객체 탐지에 더 유용한 특징 피라미드를 구축하는 역할을 한다. YOLOv11은 PAN(Path Aggregation Network) 구조를 채택하여 상향식(bottom-up) 경로와 하향식(top-down) 경로를 통해 저수준의 세밀한 특징(예: P3)과 고수준의 의미론적 특징(예: P5)을 효과적으로 결합한다.15 이 과정에서 업샘플링(Upsample), 특징 맵 결합(Concatenation), 그리고 C3k2 및 C2PSA 블록이 반복적으로 사용되어 각기 다른 스케일의 특징들을 정제하고 풍부하게 만든다.15 이를 통해 모델은 다양한 크기의 객체를 안정적으로 탐지할 수 있는 능력을 갖추게 된다.</p>
<h4>2.1.3  헤드 (Head)</h4>
<p>헤드는 넥으로부터 전달받은 최종 특징 맵을 기반으로 실제 예측을 수행하는 부분이다. YOLOv11은 앵커 박스를 미리 정의하지 않는 앵커 프리(anchor-free) 방식을 채택하여 설계 복잡성을 줄이고 일반화 성능을 높였다.19 또한, 분류(classification)와 위치 회귀(regression) 작업을 별도의 브랜치에서 처리하는 분리된 헤드(decoupled head) 구조를 사용하여 각 작업의 성능을 최적화한다.20 헤드는 총 3개의 탐지 블록으로 구성되며, 각각 넥에서 전달된 서로 다른 스케일의 특징 맵(P3, P4, P5에 해당)을 입력받아 각각 작은 객체, 중간 크기 객체, 큰 객체를 탐지하는 데 특화되어 있다.15</p>
<h3>2.2  핵심 구성 요소 상세 분석</h3>
<h4>2.2.1  C3k2 (Cross Stage Partial with kernel size 2) 블록</h4>
<p>C3k2 블록은 YOLOv11 아키텍처의 가장 핵심적인 변경 사항 중 하나로, 이전 버전인 YOLOv8의 C2f 블록을 대체한다.15 이 블록은 CSP(Cross Stage Partial) 아키텍처의 원리를 계승하여, 입력 특징 맵을 두 개의 경로로 분할한 뒤 일부만 복잡한 연산(bottleneck 레이어)을 거치고 나머지는 그대로 전달하여 나중에 합치는 구조를 가진다.22 이를 통해 그래디언트 정보의 흐름을 원활하게 하고 계산량을 줄이면서도 풍부한 특징 표현을 학습할 수 있다. C3k2는 3x3보다 작은 커널을 사용하거나(이름의 ’k2’가 이를 암시할 수 있으나, 실제 구현은 3x3 커널을 사용하는 것으로 분석된다) 병목 레이어의 구조를 최적화하여 C2f 블록보다 더 높은 계산 효율성을 달성한다.1 이 블록은 백본, 넥, 헤드 전반에 걸쳐 사용되어 모델의 전반적인 속도 향상과 경량화에 결정적인 기여를 한다.1</p>
<h4>2.2.2  SPPF (Spatial Pyramid Pooling - Fast)</h4>
<p>SPPF는 이전 YOLO 버전에서 그 효율성이 입증되어 YOLOv11에서도 계승된 중요한 모듈이다.15 백본의 마지막 부분에 위치하여 고수준 특징 맵을 입력받아, 서로 다른 커널 크기(예: 5x5, 9x9, 13x13)를 가진 다수의 맥스 풀링(max-pooling) 레이어를 병렬로 적용한다.15 각 풀링 레이어를 통과한 특징 맵들과 원본 특징 맵을 모두 결합함으로써, 모델은 다양한 크기의 수용장(receptive field)을 동시에 확보하게 된다. 이는 모델이 이미지 내의 다양한 크기와 비율을 가진 객체들을 효과적으로 인식하고, 특히 작은 객체에 대한 탐지 성능을 강인하게 유지하는 데 핵심적인 역할을 한다.22</p>
<h4>2.2.3  C2PSA (Convolutional block with Parallel Spatial Attention)</h4>
<p>C2PSA 블록은 YOLOv11의 정확도 향상을 이끈 가장 중요한 아키텍처 혁신이다.15 이 블록은 공간적 어텐션(Spatial Attention) 메커니즘을 CSP 구조에 효율적으로 통합한 것이다. C2f와 유사하게 입력 특징 맵을 두 경로로 나누어 처리하지만, 한쪽 경로에 PSA(Parallel Spatial Attention) 모듈을 적용하여 특징 맵 내에서 객체가 존재할 가능성이 높은 중요 영역에 모델이 집중하도록 유도한다.1 어텐션 메커니즘은 각 픽셀 위치의 중요도를 학습하여 특징을 재가중함으로써, 배경과 같이 불필요한 정보는 억제하고 객체와 관련된 유용한 정보는 강조하는 효과를 낳는다. 이는 특히 객체가 다른 물체에 의해 가려져 있거나(occlusion), 크기가 매우 작은 경우에 탐지 정확도를 크게 향상시킨다.22 C2PSA는 YOLOv8에는 없었던 기능으로, YOLOv11이 더 높은 mAP를 달성하는 데 결정적인 역할을 한다.1</p>
<p>아래 표는 YOLOv11의 아키텍처를 이전 세대 모델들과 비교하여 주요 변경점을 요약한 것이다.</p>
<p><strong>표 1: YOLOv8, YOLOv10, YOLOv11 아키텍처 주요 변경점 비교</strong></p>
<table><thead><tr><th>특징</th><th>YOLOv8</th><th>YOLOv10</th><th>YOLOv11</th></tr></thead><tbody>
<tr><td><strong>백본/넥 핵심 블록</strong></td><td>C2f (Cross Stage Partial Fast)</td><td>C2f 기반 Rank-guided 블록</td><td><strong>C3k2 (Cross Stage Partial with kernel size 2)</strong></td></tr>
<tr><td><strong>어텐션 모듈</strong></td><td>없음</td><td>부분적 Self-Attention</td><td><strong>C2PSA (Parallel Spatial Attention)</strong></td></tr>
<tr><td><strong>헤드 디자인</strong></td><td>Decoupled Head, Anchor-Free</td><td>Decoupled Head, Anchor-Free</td><td>Decoupled Head, Anchor-Free</td></tr>
<tr><td><strong>후처리</strong></td><td>NMS (Non-Maximum Suppression)</td><td><strong>NMS-Free (Dual label assignments)</strong></td><td>NMS (Non-Maximum Suppression)</td></tr>
</tbody></table>
<p>이 표는 YOLOv11이 YOLOv8의 C2f를 더 효율적인 C3k2로 대체하고, C2PSA라는 강력한 어텐션 메커니즘을 새롭게 도입했음을 명확히 보여준다. 동시에, YOLOv10이 NMS-Free라는 독자적인 노선을 추구한 것과 달리, YOLOv11은 검증된 NMS 기반의 후처리 방식을 유지하며 아키텍처 내부의 효율성과 표현력 강화에 집중했음을 알 수 있다.</p>
<h2>3.  성능 벤치마크 및 비교 분석</h2>
<p>모델의 아키텍처 혁신은 궁극적으로 정량적인 성능 지표를 통해 그 가치를 입증받아야 한다. YOLOv11은 속도, 정확도, 그리고 모델 효율성이라는 세 가지 축에서 이전 세대 모델들을 능가하는 것을 목표로 설계되었다. 본 장에서는 주요 성능 지표를 정의하고, 표준 벤치마크인 COCO 데이터셋을 기준으로 YOLOv11의 성능을 YOLOv8, YOLOv10과 심층 비교 분석한다.</p>
<h3>3.1  주요 성능 지표의 이해</h3>
<ul>
<li><strong>mAP (mean Average Precision)</strong>: 객체 탐지 모델의 정확도를 종합적으로 평가하는 핵심 지표이다. 정밀도(Precision)와 재현율(Recall)을 바탕으로 계산된 AP(Average Precision)를 모든 클래스에 대해 평균 낸 값이다. <span class="math math-inline">mAP50</span>은 IoU(Intersection over Union) 임계값을 0.5로 고정했을 때의 성능을, <span class="math math-inline">mAP50-95</span>는 IoU 임계값을 0.5부터 0.95까지 0.05 간격으로 변화시키며 측정한 mAP의 평균을 의미한다. 후자가 더 엄격한 기준으로, 객체의 위치를 얼마나 정밀하게 예측하는지까지 평가한다.25</li>
<li><strong>Latency / FPS (Frames Per Second)</strong>: 모델이 단일 이미지를 처리하는 데 걸리는 시간(ms)과 초당 처리할 수 있는 이미지의 수(FPS)를 나타내는 속도 지표이다. Latency가 낮을수록, FPS가 높을수록 모델의 추론 속도가 빠르다는 의미이며, 이는 실시간 영상 분석과 같은 애플리케이션에서 절대적으로 중요한 요소이다.24 이 지표는 CPU, GPU 등 어떤 하드웨어에서 측정되었는지에 따라 크게 달라진다.</li>
<li><strong>Parameters &amp; FLOPs</strong>: 모델의 크기와 계산 복잡도를 나타내는 지표이다. 파라미터(Parameters)는 모델이 학습한 가중치의 총 개수로, 모델의 메모리 점유량과 직결된다. FLOPs(Floating Point Operations per Second)는 모델이 추론을 위해 수행하는 부동소수점 연산의 총량으로, 계산 복잡도를 나타낸다. 두 지표가 낮을수록 모델이 더 가볍고 효율적이며, 이는 리소스가 제한된 엣지 디바이스에 배포할 때 중요한 고려사항이 된다.5</li>
</ul>
<h3>3.2  COCO 데이터셋 기반 성능 평가</h3>
<p>아래 표 2는 COCO 2017 <span class="math math-inline">val</span> 데이터셋을 기준으로 YOLOv11, YOLOv10, YOLOv8의 객체 탐지 성능을 모델 크기별로 종합 비교한 것이다. 모든 모델은 640x640 해상도의 이미지를 입력으로 사용하였다.</p>
<p><strong>표 2: COCO 데이터셋 기반 객체 탐지 성능 종합 비교 (YOLOv11 vs. YOLOv10 vs. YOLOv8)</strong></p>
<table><thead><tr><th>모델</th><th>mAPval 50-95</th><th>속도 CPU ONNX (ms)</th><th>속도 T4 TensorRT (ms)</th><th>파라미터 (M)</th><th>FLOPs (B)</th><th></th></tr></thead><tbody>
<tr><td>YOLOv8n</td><td>37.3</td><td>-</td><td>1.21</td><td>3.2</td><td>8.7</td><td></td></tr>
<tr><td>YOLOv10n</td><td>39.5</td><td>1.84</td><td>-</td><td>2.6</td><td>6.5</td><td></td></tr>
<tr><td><strong>YOLOv11n</strong></td><td><strong>39.5</strong></td><td><strong>1.55</strong></td><td><strong>1.05</strong></td><td><strong>2.6</strong></td><td><strong>6.5</strong></td><td></td></tr>
<tr><td>YOLOv8s</td><td>44.9</td><td>-</td><td>1.79</td><td>11.2</td><td>28.6</td><td></td></tr>
<tr><td>YOLOv10s</td><td>46.8</td><td>2.49</td><td>-</td><td>9.4</td><td>21.5</td><td></td></tr>
<tr><td><strong>YOLOv11s</strong></td><td><strong>47.0</strong></td><td><strong>2.46</strong></td><td><strong>1.35</strong></td><td><strong>9.4</strong></td><td><strong>21.5</strong></td><td></td></tr>
<tr><td>YOLOv8m</td><td>50.2</td><td>-</td><td>3.16</td><td>25.9</td><td>78.9</td><td></td></tr>
<tr><td>YOLOv10m</td><td>51.3</td><td>4.74</td><td>-</td><td>20.1</td><td>68.0</td><td></td></tr>
<tr><td><strong>YOLOv11m</strong></td><td><strong>51.5</strong></td><td><strong>4.70</strong></td><td><strong>2.21</strong></td><td><strong>20.1</strong></td><td><strong>68.0</strong></td><td></td></tr>
<tr><td>YOLOv8l</td><td>52.9</td><td>-</td><td>4.79</td><td>43.7</td><td>165.2</td><td></td></tr>
<tr><td>YOLOv10l</td><td>53.4</td><td>7.28</td><td>-</td><td>25.3</td><td>86.9</td><td></td></tr>
<tr><td><strong>YOLOv11l</strong></td><td><strong>53.4</strong></td><td><strong>6.16</strong></td><td><strong>2.97</strong></td><td><strong>25.3</strong></td><td><strong>86.9</strong></td><td></td></tr>
<tr><td>YOLOv8x</td><td>53.9</td><td>-</td><td>7.40</td><td>68.2</td><td>257.8</td><td></td></tr>
<tr><td>YOLOv10x</td><td>54.4</td><td>10.70</td><td>-</td><td>56.9</td><td>194.9</td><td></td></tr>
<tr><td><strong>YOLOv11x</strong></td><td><strong>54.7</strong></td><td><strong>11.31</strong></td><td><strong>4.45</strong></td><td><strong>56.9</strong></td><td><strong>194.9</strong></td><td></td></tr>
</tbody></table>
<p>주: YOLOv8, v10의 일부 속도 데이터는 공개된 자료 부족으로 기재하지 않음. 데이터는 5 등을 종합하여 재구성함.</p>
<p>표에서 드러나듯, YOLOv11은 이전 세대 모델들과 비교하여 모든 크기에서 mAP 성능이 동등하거나 소폭 우위에 있다. 더 주목할 점은 모델의 효율성이다. 예를 들어, YOLOv11m은 YOLOv8m보다 22%나 적은 파라미터를 사용함에도 불구하고 1.3 포인트 더 높은 mAP를 달성했다.5 이는 C3k2와 같은 경량화된 블록 설계가 성공적으로 작동했음을 보여주는 명백한 증거이다.</p>
<h3>3.3  성능 분석: 정확도와 속도의 절충점</h3>
<p>YOLOv11의 진정한 가치는 속도-정확도 절충점(Accuracy-Speed Trade-off) 곡선을 분석할 때 명확히 드러난다. 벤치마크 데이터 전반에서 YOLOv11의 가장 두드러진 개선점은 mAP의 소폭 증가가 아닌, 추론 속도, 특히 GPU 환경에서의 latency의 대폭적인 감소이다.27 예를 들어, YOLOv11l은 YOLOv10l과 동일한 mAP를 기록했지만, T4 GPU 환경에서 TensorRT로 최적화했을 때 추론 시간이 YOLOv8l(4.79ms)보다 훨씬 빠른 2.97ms를 기록했다. 이는 아키텍처 설계 단계부터 TensorRT와 같은 GPU 가속 라이브러리와의 호환성 및 최적화가 깊이 고려되었음을 시사한다. Ultralytics가 ONNX, TensorRT, OpenVINO 등 다양한 포맷으로의 내보내기 기능을 강조하며 이를 통해 최대 5배의 GPU 속도 향상이 가능하다고 주장하는 것도 이러한 맥락이다.31 YOLOv11의 SOTA 주장은 순수 알고리즘의 우월성뿐만 아니라, ’소프트웨어-하드웨어 통합 최적화’의 결과물로 해석해야 한다. 이는 미래의 AI 모델 경쟁이 단순히 더 나은 아키텍처를 설계하는 것을 넘어, 특정 하드웨어 스택에서 최고의 ’실사용 성능’을 끌어내는 능력에서 판가름 날 수 있음을 보여준다.</p>
<p>다만, 가장 큰 모델인 YOLOv11x의 경우 CPU ONNX latency가 11.31ms로 YOLOv10x의 10.70ms보다 오히려 소폭 증가하는 현상이 관찰되었다.28 이는 모델의 크기가 일정 수준 이상으로 커질 때 발생하는 복잡성과 잠재적인 병목 현상을 시사하며, 스케일업 시에는 아키텍처의 효율성이 항상 선형적으로 유지되지는 않음을 보여주는 사례이다. 그럼에도 불구하고, GPU 환경에서는 YOLOv11x가 4.45ms라는 압도적인 속도를 보여주며, 대규모 모델일수록 GPU 가속의 중요성이 더욱 커짐을 알 수 있다.</p>
<h2>4.  학습 전략 및 손실 함수</h2>
<p>YOLOv11의 뛰어난 성능은 효율적인 아키텍처뿐만 아니라, 정교하게 설계된 학습 전략과 손실 함수(Loss Function)에 기인한다. 모델은 학습 과정에서 예측값과 실제 정답(ground truth)의 차이를 손실 함수를 통해 정량화하고, 이 손실을 최소화하는 방향으로 내부 파라미터를 업데이트한다.</p>
<h3>4.1  손실 함수의 삼위일체: Box, Classification, DFL</h3>
<p>YOLOv11의 전체 손실 함수는 경계 상자의 위치를 교정하는 Box Loss, 객체의 클래스를 맞추는 Classification Loss, 그리고 경계 상자 좌표의 분포를 학습하는 DFL(Distribution Focal Loss)의 세 가지 주요 구성 요소의 가중 합으로 이루어진다.32<br />
<span class="math math-display">
L_{total} = w_{box} \cdot L_{box} + w_{cls} \cdot L_{cls} + w_{dfl} \cdot L_{dfl}
</span><br />
여기서 <span class="math math-display">w_{box}</span>, <span class="math math-display">w_{cls}</span>, <span class="math math-display">w_{dfl}</span>은 각 손실의 중요도를 조절하는 가중치로, Ultralytics 프레임워크에서는 각각 7.5, 0.5, 1.5의 기본값이 사용된다.33</p>
<h4>4.1.1. Box Loss (CIoU - Complete IoU)</h4>
<p>경계 상자 회귀(bounding box regression)를 위해 YOLOv11은 CIoU(Complete IoU) Loss를 사용한다.33 이는 단순 IoU Loss가 겹치지 않는 두 상자 사이의 거리를 측정하지 못하는 한계를 극복하기 위해 제안된 GIoU, DIoU를 더욱 발전시킨 형태이다. CIoU는 세 가지 핵심적인 기하학적 요소를 동시에 고려하여 보다 빠르고 안정적인 수렴을 유도한다 34:</p>
<ol>
<li><strong>겹침 영역(Overlap Area)</strong>: 예측 상자와 실제 상자 간의 IoU.</li>
<li><strong>중심점 거리(Center Point Distance)</strong>: 두 상자의 중심점 사이의 정규화된 유클리드 거리.</li>
<li><strong>종횡비(Aspect Ratio)</strong>: 두 상자의 너비와 높이 비율의 일관성.</li>
</ol>
<p>CIoU Loss는 다음과 같은 수식으로 정의된다:<br />
<span class="math math-display">
L_{CIoU} = 1 - IoU + \frac{\rho^2(b, b^{gt})}{c^2} + \alpha v
</span></p>
<ul>
<li><span class="math math-display">IoU</span>: 예측 상자와 실제 상자의 Intersection over Union.</li>
<li><span class="math math-display">\rho^2(b, b^{gt})</span>: 예측 상자의 중심점 <span class="math math-display">b</span>와 실제 상자의 중심점 <span class="math math-display">b^{gt}</span> 사이의 유클리드 거리 제곱.</li>
<li><span class="math math-display">c</span>: 두 상자를 모두 포함하는 가장 작은 볼록 상자(convex hull)의 대각선 길이.</li>
<li><span class="math math-display">\alpha v</span>: 종횡비의 일관성을 측정하는 페널티 항. 여기서 <span class="math math-display">v = \frac{4}{\pi^2}(\arctan\frac{w^{gt}}{h^{gt}} - \arctan\frac{w}{h})^2</span>는 종횡비 차이를, <span class="math math-display">\alpha = \frac{v}{(1-IoU)+v}</span>는 트레이드오프를 조절하는 가중치이다.</li>
</ul>
<h4>4.1.2. Classification Loss (BCE - Binary Cross-Entropy)</h4>
<p>객체의 클래스를 예측하기 위해, YOLOv11은 각 클래스에 대해 독립적으로 확률을 계산하는 이진 교차 엔트로피(Binary Cross-Entropy) Loss를 사용한다.33 이는 하나의 객체가 여러 클래스에 속할 수 있는 다중 레이블(multi-label) 분류 문제에 효과적이며, Sigmoid 활성화 함수와 함께 사용된다. 일반적으로 클래스 불균형 문제를 완화하기 위해 Focal Loss와 같은 변형 기법이 함께 적용되기도 한다.</p>
<h4>4.1.3. Distribution Focal Loss (DFL)</h4>
<p>DFL은 YOLOv11 손실 함수의 가장 혁신적인 부분 중 하나로, 경계 상자 좌표를 단일 연속 값으로 직접 회귀하는 대신, 이산적인(discrete) 확률 분포로 모델링하는 접근법이다.33 이는 좌표 예측을 일종의 분류 문제로 변환하는 것으로, 실제 좌표 값 주변의 확률 분포를 학습하도록 유도한다. 이 방식은 객체 경계의 모호함이나 불확실성을 자연스럽게 모델링할 수 있으며, 최종적으로는 분포의 기댓값을 계산하여 서브픽셀(sub-pixel) 수준의 정밀도를 달성할 수 있다.38</p>
<p>DFL은 네트워크가 실제 좌표 값 <span class="math math-display">y</span>에 가장 가까운 두 이산 값 <span class="math math-display">y_i</span>와 <span class="math math-display">y_{i+1}</span> (단, <span class="math math-display">y_i \le y \le y_{i+1}</span>)의 예측 확률 <span class="math math-display">S_i</span>와 <span class="math math-display">S_{i+1}</span>을 높이도록 강제한다. 손실 함수는 다음과 같이 정의된다 40:<br />
<span class="math math-display">
DFL(S_i, S_{i+1}) = -((y_{i+1} - y)\log(S_i) + (y - y_i)\log(S_{i+1}))
</span><br />
이 손실 함수는 <span class="math math-display">y</span>와의 거리에 비례하여 <span class="math math-display">S_i</span>와 <span class="math math-display">S_{i+1}</span>에 대한 로그 손실을 가중하는 형태이다. 이 손실이 최소화될 때, 예측 분포의 기댓값은 실제 값 <span class="math math-display">y</span>와 정확히 일치하게 된다. DFL의 도입은 객체 탐지가 결정론적(deterministic) 문제에서 확률론적(probabilistic) 문제로 진화하고 있음을 보여준다. 이는 특히 경계가 모호하거나 가려진 객체에 대해 더 강인하고 현실적인 예측을 가능하게 하며, 자율주행이나 의료 영상처럼 예측의 불확실성을 정량화하는 것이 중요한 분야에서 모델의 신뢰도를 높이는 핵심 기술이 될 수 있다.</p>
<h3>4.2. 효율적인 학습을 위한 전략</h3>
<p>YOLOv11은 정교한 손실 함수 외에도 다양한 학습 전략을 활용하여 최적의 성능을 달성한다.</p>
<ul>
<li><strong>전이 학습 (Transfer Learning)</strong>: COCO와 같은 대규모 공개 데이터셋으로 사전 학습된 모델의 가중치를 초기값으로 사용하여 학습을 시작하는 것이 일반적이다. 이는 모델이 기본적인 특징 추출 능력을 미리 갖추게 하므로, 적은 양의 커스텀 데이터로도 빠르고 안정적으로 높은 성능에 도달하게 해준다.32</li>
<li><strong>데이터 증강 및 정규화 (Data Augmentation &amp; Regularization)</strong>: 모델이 다양한 환경 변화에 강인하게 대응하도록 학습 데이터에 인위적인 변화를 주는 데이터 증강 기법(예: 모자이크, 믹스업, 색상 변환 등)이 적극적으로 사용된다. 또한, 과적합(overfitting)을 방지하기 위해 드롭아웃(dropout), 가중치 감쇠(weight decay)와 같은 정규화 기법이 적용된다.32</li>
<li><strong>하이퍼파라미터 튜닝 (Hyperparameter Tuning)</strong>: Ultralytics 프레임워크는 배치 크기(batch size), 학습률(learning rate), 모멘텀(momentum) 등 학습 과정에 영향을 미치는 주요 하이퍼파라미터들을 사용자가 쉽게 조정할 수 있도록 지원한다.41 특히, 검증 세트(validation set)의 성능이 일정 에포크(epoch) 동안 개선되지 않으면 학습을 조기 종료하는 <code>patience</code> 옵션은 불필요한 학습을 막고 과적합을 방지하는 데 효과적이다.41</li>
</ul>
<h2>V. 다중 작업 역량 및 응용 분야</h2>
<p>YOLOv11의 가장 큰 특징 중 하나는 단순한 객체 탐지기를 넘어, 다양한 컴퓨터 비전 작업을 수행할 수 있는 다목적 프레임워크라는 점이다. 이는 AI 모델의 ‘플랫폼화’ 경향을 반영하는 것으로, YOLOv11이 특정 문제 해결을 위한 도구를 넘어 다양한 비전 AI 애플리케이션 개발을 위한 기반 플랫폼 역할을 지향하고 있음을 보여준다.</p>
<h3>5.1. 객체 탐지를 넘어선 확장성</h3>
<p>YOLOv11은 단일 통합 프레임워크 내에서 다음과 같은 5가지 핵심 비전 AI 작업을 모두 지원한다 1:</p>
<ol>
<li><strong>객체 탐지 (Object Detection)</strong>: 이미지나 영상에서 객체의 위치를 경계 상자로 표시하고 클래스를 분류하는 가장 기본적인 작업이다.</li>
<li><strong>인스턴스 분할 (Instance Segmentation)</strong>: 객체를 경계 상자로 탐지하는 것을 넘어, 픽셀 단위로 객체의 정확한 윤곽(마스크)을 분리해낸다.</li>
<li><strong>이미지 분류 (Image Classification)</strong>: 이미지 전체에 대해 단일 클래스 레이블을 할당한다.</li>
<li><strong>자세 추정 (Pose Estimation)</strong>: 사람의 관절과 같은 주요 키포인트(keypoint)를 탐지하여 자세나 움직임을 분석한다.</li>
<li><strong>회전된 객체 탐지 (Oriented Object Detection, OBB)</strong>: 일반적인 수평/수직 경계 상자가 아닌, 회전 각도를 포함한 경계 상자로 객체를 탐지한다.</li>
</ol>
<p>Ultralytics는 각 작업에 특화된 사전 학습 모델(예: <span class="math math-inline">yolov11n-seg.pt</span> for segmentation, <span class="math math-inline">yolov11n-pose.pt</span> for pose estimation)을 제공하며, 개발자들은 일관된 API를 통해 간단히 모델을 교체하는 것만으로 원하는 작업을 수행할 수 있다.5 이러한 플랫폼화 전략은 개발자들의 진입 장벽을 낮추고 Ultralytics 생태계에 대한 의존도를 높이는 효과를 가져온다. 개별 SOTA 모델을 찾아 헤매는 대신, ‘YOLOv11 플랫폼’ 위에서 대부분의 비전 AI 문제를 해결하려는 개발자들이 늘어날 것이며, 이는 AI 기술의 보급을 가속화하는 동시에 특정 기업의 기술적 영향력을 강화하는 결과를 낳을 수 있다.</p>
<h3>5.2. 산업별 적용 사례 및 잠재력</h3>
<p>YOLOv11의 다중 작업 역량과 뛰어난 실시간 성능은 다양한 산업 분야에서 혁신적인 애플리케이션을 가능하게 한다.</p>
<ul>
<li><strong>자율 주행 및 지능형 교통 시스템 (ITS)</strong>: 실시간으로 차량, 보행자, 자전거, 교통 신호를 정확하게 탐지하여 충돌을 방지하고 안전한 주행을 지원한다. 또한, 교통 흐름을 분석하여 신호 체계를 최적화하는 데 활용될 수 있다.12</li>
<li><strong>스마트 제조 및 품질 관리</strong>: 고속으로 움직이는 생산 라인에서 제품의 미세한 결함이나 조립 불량을 실시간으로 검출(인스턴스 분할 활용)하고, 작업자의 안전 장비(PPE) 착용 여부를 모니터링하여 산업 재해를 예방한다.1</li>
<li><strong>의료 영상 분석</strong>: CT나 MRI 영상에서 종양이나 특정 장기의 영역을 픽셀 단위로 정밀하게 분할하여 의사의 진단을 보조하고, 수술 계획 수립에 기여한다.1</li>
<li><strong>소매 및 재고 관리</strong>: 매장 내 카메라를 통해 고객의 동선을 추적(자세 추정 활용)하거나, 선반의 상품을 인식(객체 탐지 활용)하여 자동으로 재고를 파악하고 관리 효율을 높인다.1</li>
<li><strong>농업 및 환경 모니터링</strong>: 드론이나 위성에서 촬영한 항공 이미지에서 특정 작물의 재배 면적을 계산하거나 병충해를 조기에 발견한다. OBB 기능은 기울어져 촬영된 이미지에서도 객체를 정확하게 탐지할 수 있어 항공 이미지 분석에 특히 유용하다.7 또한, 야생 동물의 개체 수를 파악하고 생태계를 모니터링하는 데에도 활용될 수 있다.1</li>
</ul>
<h2>VI. 한계 및 향후 전망</h2>
<p>YOLOv11은 실시간 객체 탐지 기술의 정점에 서 있는 모델이지만, 동시에 몇 가지 내재적 한계와 비판에 직면해 있다. 이는 YOLO 계보의 미래와 컴퓨터 비전의 다음 단계를 조망하는 중요한 단서를 제공한다.</p>
<h3>6.1. 비판적 고찰 및 현재의 한계</h3>
<ul>
<li><strong>공식 논문 부재</strong>: 가장 큰 비판점은 모델의 설계 철학과 실험 결과에 대한 공식적인 학술 논문이 없다는 것이다. 이는 아키텍처 변경(예: C3k2의 정확한 구조)이나 학습 방법의 이론적 배경, 설계 결정의 근거를 불분명하게 만들어 학술적 검증과 심층 연구에 장벽으로 작용한다.14 연구자들은 제3자의 리버스 엔지니어링이나 추측에 의존할 수밖에 없는 상황이다.</li>
<li><strong>점근적 성능 향상</strong>: YOLOv8, YOLOv10과 비교했을 때 mAP의 향상 폭이 점차 미미해지고 있다는 점은 실시간 객체 탐지 기술이 성능 포화 상태에 가까워지고 있다는 신호일 수 있다.28 많은 리뷰에서 YOLOv11의 mAP 향상은 ’미미(minute)’하지만 속도 향상은 ’극적(dramatic)’이라고 평가하는데, 이는 시장의 요구가 ’최고의 정확도’에서 ’충분히 좋은 정확도와 압도적인 속도’로 이동하고 있음을 반영한다. Ultralytics는 이러한 시장의 실질적인 요구를 파악하고, 연구실 수준의 mAP 경쟁보다는 현장에서 즉시 가치를 창출할 수 있는 ’속도’와 ’개발 편의성’에 자원을 집중 투자했을 가능성이 높다. 이는 AI 연구의 평가 기준이 학술적 벤치마크를 넘어 ‘배포 용이성’, ’총 소유 비용(TCO)’과 같은 실용적 지표로 확장되고 있음을 시사한다.</li>
<li><strong>데이터셋 의존성</strong>: 모든 딥러닝 모델과 마찬가지로, YOLOv11의 성능은 학습에 사용된 데이터의 품질과 양에 크게 의존한다. 특정 도메인, 예를 들어 의료 영상의 희귀 질병이나 제조 공정의 비정형 결함 탐지에서는 고품질의 대규모 데이터셋을 구축하는 것이 여전히 가장 큰 병목으로 남아있다.45</li>
<li><strong>계산 자원 요구</strong>: 가장 큰 모델인 YOLOv11x는 SOTA 수준의 정확도를 달성하지만, 이를 학습하고 배포하기 위해서는 상당한 계산 자원(고성능 GPU 등)이 필요하다. 이는 일반적인 개발 환경이나 리소스가 제한된 엣지 디바이스에서는 사용하기 어렵다는 제약을 가진다.27</li>
</ul>
<h3>6.2. YOLO 계보의 미래와 컴퓨터 비전의 다음 단계</h3>
<p>YOLOv11이 제시한 방향성과 현재의 한계를 바탕으로, YOLO 계보와 컴퓨터 비전 기술의 미래를 다음과 같이 전망할 수 있다.</p>
<ul>
<li><strong>하드웨어 특화 및 경량화</strong>: 정확도 향상이 점차 어려워짐에 따라, 향후 YOLO 모델들은 latency를 더욱 줄이고, 모바일 AP나 NPU(Neural Processing Unit)와 같은 특정 하드웨어에서의 연산 효율성을 극대화하는 방향으로 더욱 발전할 것이다.30</li>
<li><strong>멀티모달(Multi-modal) 통합</strong>: 시각 정보(RGB 이미지)뿐만 아니라, 텍스트 프롬프트(예: YOLO-World), 열화상 이미지 46, 깊이(depth) 정보, LiDAR 데이터 등 다른 양식의 정보를 함께 활용하여 복잡하고 어려운 환경에서의 탐지 성능을 획기적으로 높이는 방향으로 진화할 것이다.</li>
<li><strong>End-to-End 아키텍처의 확산</strong>: YOLOv10이 시도했던 NMS-Free와 같은 End-to-End 접근법이 더욱 정교화될 것이다. 복잡한 후처리 과정을 신경망 내부에 통합하여 전체 추론 파이프라인을 단순화하고, 잠재적인 성능 병목을 제거하려는 연구가 계속될 것이다.</li>
<li><strong>생성형 AI와의 융합</strong>: 객체를 탐지하고 분류하는 것을 넘어, 탐지된 객체와의 상호작용을 통해 장면을 이해하고, 사용자의 요구에 따라 이미지를 편집하거나 새로운 시나리오를 생성하는 등 분석적 AI와 생성형 AI의 경계가 허물어지는 방향으로 기술이 확장될 것이다.</li>
</ul>
<h2>VII. 결론</h2>
<h3>7.1. YOLOv11의 핵심 기여 요약</h3>
<p>YOLOv11은 실시간 객체 탐지 기술의 역사에서 중요한 이정표를 제시한다. 본 안내서에서 심층 분석한 바와 같이, YOLOv11의 핵심 기여는 다음과 같이 요약할 수 있다.</p>
<p>첫째, YOLOv11은 혁명적인 아키텍처 변화보다는 YOLOv8의 검증된 기반 위에 C3k2와 같은 효율적인 연산 블록과 C2PSA라는 강력한 어텐션 메커니즘을 정교하게 통합하여 아키텍처를 ’최적화’하는 데 성공했다. 이는 공학적 완성도를 통해 실질적인 성능 개선을 이끌어낸 대표적인 사례이다.</p>
<p>둘째, 이러한 최적화는 정확도의 점진적 향상과 더불어, 특히 GPU 환경에서 극적인 추론 속도 향상을 이끌어냈다. 이를 통해 속도-정확도 trade-off 곡선을 다시 한번 개선하며, 실시간 성능이 요구되는 다양한 산업 현장에서의 적용 가능성을 크게 확장했다.</p>
<p>셋째, 단일 프레임워크 내에서 객체 탐지, 인스턴스 분할, 자세 추정 등 5가지 이상의 핵심 비전 AI 작업을 포괄적으로 지원함으로써, ’만능 비전 AI 플랫폼’으로서의 가능성을 제시했다. 이는 개발자 생태계를 확장하고 AI 기술의 대중화를 가속하는 중요한 전략적 행보이다.</p>
<h3>7.2. 실시간 비전 AI 분야에서의 의의 및 최종 평가</h3>
<p>결론적으로, YOLOv11은 학술적 새로움이나 이론적 혁신보다는 공학적 완성도와 실용성에 초점을 맞춘 모델이다. 이는 AI 기술이 순수 연구 단계를 넘어, 실제 산업 현장의 문제를 해결하는 성숙한 기술로 자리 잡고 있음을 보여주는 상징적인 사례라 할 수 있다.</p>
<p>공식 논문의 부재라는 학술적 한계에도 불구하고, YOLOv11은 벤치마크를 통해 입증된 강력한 성능, <span class="math math-inline">pip</span> 한 줄로 설치 가능한 탁월한 사용 편의성, 그리고 Ultralytics의 활발한 커뮤니티 지원을 바탕으로 실시간 객체 탐지 분야의 새로운 산업 표준(de facto standard)이 될 잠재력을 충분히 갖추고 있다. YOLOv11은 더 이상 단순한 알고리즘이 아니라, 시장의 요구에 맞춰 정교하게 설계된 ’제품’이자, 수많은 개발자와 애플리케이션을 아우르는 강력한 ’생태계’이다. 앞으로 YOLOv11이 열어갈 비전 AI의 새로운 지평을 기대해 본다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>YOLOv11: An Overview of the Key Architectural Enhancements - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/html/2410.17725v1</li>
<li>What is YOLO? The Ultimate Guide [2025] - Roboflow Blog, 8월 24, 2025에 액세스, https://blog.roboflow.com/guide-to-yolo-models/</li>
<li>YOLOv1 to YOLOv10: The fastest and most accurate real-time object detection systems - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/html/2408.09332v1</li>
<li>New Yolo model - YOLOv12 : r/LocalLLaMA - Reddit, 8월 24, 2025에 액세스, https://www.reddit.com/r/LocalLLaMA/comments/1itbszy/new_yolo_model_yolov12/</li>
<li>YOLO11: A New Iteration of “You Only Look Once” - Viso Suite, 8월 24, 2025에 액세스, https://viso.ai/computer-vision/yolov11/</li>
<li>History of YOLO: From YOLOv1 to YOLOv10 - Labelvisor, 8월 24, 2025에 액세스, https://www.labelvisor.com/history-of-yolo-from-yolov1-to-yolov10/</li>
<li>YOLOVision Dropped YOLOv11: All That We Know So Far | by Zain Shariff | Medium, 8월 24, 2025에 액세스, https://medium.com/@zainshariff6506/yolovision-dropped-yolov11-all-that-we-know-so-far-eacdf181d442</li>
<li>Ultralytics YOLO Docs: Home, 8월 24, 2025에 액세스, https://docs.ultralytics.com/</li>
<li>YOLO11: Future of AI Vision - Ultralytics, 8월 24, 2025에 액세스, https://www.ultralytics.com/blog/ultralytics-yolo11-has-arrived-redefine-whats-possible-in-ai</li>
<li>YOLOv11 | Launch and Overview | Object Detection | DesignSpark - RS Online, 8월 24, 2025에 액세스, https://www.rs-online.com/designspark/yolov11-launch-and-overview-of-object-detection</li>
<li>Models Supported by Ultralytics, 8월 24, 2025에 액세스, https://docs.ultralytics.com/models/</li>
<li>Computer Vision Tasks supported by Ultralytics YOLO11, 8월 24, 2025에 액세스, https://docs.ultralytics.com/tasks/</li>
<li>YOLOv11 Official Paper - Research - Ultralytics, 8월 24, 2025에 액세스, https://community.ultralytics.com/t/yolov11-official-paper/991</li>
<li>YOLOv8 to YOLO11: A Comprehensive Architecture In-depth Comparative Review - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/abs/2501.13400</li>
<li>YOLOv11 Optimization for Efficient Resource Utilization - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/html/2412.14790v2</li>
<li>Guide on YOLOv11 Model Building from Scratch using PyTorch - Analytics Vidhya, 8월 24, 2025에 액세스, https://www.analyticsvidhya.com/blog/2025/01/yolov11-model-building/</li>
<li>(PDF) YOLOv11 Optimization for Efficient Resource Utilization - ResearchGate, 8월 24, 2025에 액세스, https://www.researchgate.net/publication/387264763_YOLOv11_Optimization_for_Efficient_Resource_Utilization</li>
<li>YOLO11 Architecture - Detailed Explanation - YouTube, 8월 24, 2025에 액세스, https://www.youtube.com/watch?v=L9Va7Y9UT8E</li>
<li>YOLO11 performance results on benchmark datasets - ResearchGate, 8월 24, 2025에 액세스, https://www.researchgate.net/figure/YOLO11-performance-results-on-benchmark-datasets-Performance-comparison-of-YOLO-models_fig1_385317546</li>
<li>Comparing Ultralytics YOLO11 vs previous YOLO models, 8월 24, 2025에 액세스, https://www.ultralytics.com/blog/comparing-ultralytics-yolo11-vs-previous-yolo-models</li>
<li>YOLO Evolution: A Comprehensive Benchmark and Architectural Review of YOLOv12, YOLO11, and Their Previous Versions - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/html/2411.00201v2</li>
<li>A Comprehensive Guide to YOLOv11 Object Detection - Analytics Vidhya, 8월 24, 2025에 액세스, https://www.analyticsvidhya.com/blog/2024/10/yolov11-object-detection/</li>
<li>YOLOv11: An Overview of the Key Architectural Enhancements - ResearchGate, 8월 24, 2025에 액세스, https://www.researchgate.net/publication/385177106_YOLOv11_An_Overview_of_the_Key_Architectural_Enhancements</li>
<li>YOLOv11 Architecture Explained: Next-Level Object Detection with Enhanced Speed and Accuracy | by S Nikhileswara Rao | Medium, 8월 24, 2025에 액세스, https://medium.com/@nikhil-rao-20/yolov11-explained-next-level-object-detection-with-enhanced-speed-and-accuracy-2dbe2d376f71</li>
<li>Benchmarking YOLOv11 Against Previous Versions [23] - ResearchGate, 8월 24, 2025에 액세스, https://www.researchgate.net/figure/Benchmarking-YOLOv11-Against-Previous-Versions-23_fig1_385177106</li>
<li>Performance Metrics Deep Dive - Ultralytics YOLO Docs, 8월 24, 2025에 액세스, https://docs.ultralytics.com/guides/yolo-performance-metrics/</li>
<li>YOLO11 vs YOLOv10: A Detailed Technical Comparison - Ultralytics YOLO Docs, 8월 24, 2025에 액세스, https://docs.ultralytics.com/compare/yolo11-vs-yolov10/</li>
<li>YOLOv11 Is Officially Out! What You Need To Know! | by Zain Shariff …, 8월 24, 2025에 액세스, https://medium.com/@zainshariff6506/yolov11-is-officially-out-what-you-need-to-know-6738c5d25be1</li>
<li>Ultralytics YOLO11, 8월 24, 2025에 액세스, https://docs.ultralytics.com/models/yolo11/</li>
<li>A Hands-On Review on YOLOv11 - Medium, 8월 24, 2025에 액세스, https://medium.com/@zainshariff6506/a-hands-on-review-on-yolov11-c484d78ef7db</li>
<li>Model Benchmarking with Ultralytics YOLO, 8월 24, 2025에 액세스, https://docs.ultralytics.com/modes/benchmark/</li>
<li>What is YOLOv11? - GeeksforGeeks, 8월 24, 2025에 액세스, https://www.geeksforgeeks.org/blogs/what-is-yolov11/</li>
<li>YOLO11 detection loss · Issue #20387 · ultralytics/ultralytics - GitHub, 8월 24, 2025에 액세스, https://github.com/ultralytics/ultralytics/issues/20387</li>
<li>Definition of regression loss function in YOLOv8 · Issue #8781 - GitHub, 8월 24, 2025에 액세스, https://github.com/ultralytics/ultralytics/issues/8781</li>
<li>Complete IoU - Karthik, 8월 24, 2025에 액세스, https://karthikziffer.github.io/journal/Complete-IoU.html</li>
<li>GIoU, CIoU and DIoU: Variants of IoU and how they are better …, 8월 24, 2025에 액세스, https://medium.com/@abhishekjainindore24/giou-ciou-and-diou-variants-of-iou-and-how-they-are-better-compared-to-iou-4610a015643a</li>
<li>loss - YOLOv8 dfl_loss metric - Stack Overflow, 8월 24, 2025에 액세스, https://stackoverflow.com/questions/75950283/yolov8-dfl-loss-metric/76910093</li>
<li>Is the explanation of DFL Loss correct? · Issue #21048 - GitHub, 8월 24, 2025에 액세스, https://github.com/ultralytics/ultralytics/issues/21048</li>
<li>YOLO Loss Function Part 2: GFL and VFL Loss - LearnOpenCV, 8월 24, 2025에 액세스, https://learnopencv.com/yolo-loss-function-gfl-vfl-loss/</li>
<li>Generalized Focal Loss: Learning Qualified and Distributed …, 8월 24, 2025에 액세스, https://proceedings.neurips.cc/paper/2020/file/f0bda020d2470f2e74990a07a607ebd9-Paper.pdf</li>
<li>Model Training with Ultralytics YOLO, 8월 24, 2025에 액세스, https://docs.ultralytics.com/modes/train/</li>
<li>Yolo11 training with custom dataset - Support - Ultralytics, 8월 24, 2025에 액세스, https://community.ultralytics.com/t/yolo11-training-with-custom-dataset/697</li>
<li>What is YOLOv11? An Introduction - Roboflow Blog, 8월 24, 2025에 액세스, https://blog.roboflow.com/what-is-yolo11/</li>
<li>[2410.22898] YOLOv11 for Vehicle Detection: Advancements, Performance, and Applications in Intelligent Transportation Systems - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/abs/2410.22898</li>
<li>YOLO Object Detection for Real-Time Fabric Defect Inspection in the Textile Industry: A Review of YOLOv1 to YOLOv11 - ResearchGate, 8월 24, 2025에 액세스, https://www.researchgate.net/publication/390503572_YOLO_Object_Detection_for_Real-Time_Fabric_Defect_Inspection_in_the_Textile_Industry_A_Review_of_YOLOv1_to_YOLOv11</li>
<li>YOLOv11-RGBT: Towards a Comprehensive Single-Stage Multispectral Object Detection Framework - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/html/2506.14696v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>