<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Drone-YOLO (2023-08-11) 항공 감시를 위한 딥러닝 객체 탐지</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Drone-YOLO (2023-08-11) 항공 감시를 위한 딥러닝 객체 탐지</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">객체 탐지 (Object Detection)</a> / <a href="index.html">YOLO 객체 탐지 모델</a> / <span>Drone-YOLO (2023-08-11) 항공 감시를 위한 딥러닝 객체 탐지</span></nav>
                </div>
            </header>
            <article>
                <h1>Drone-YOLO (2023-08-11) 항공 감시를 위한 딥러닝 객체 탐지</h1>
<h2>1.  Drone-YOLO의 등장과 의의</h2>
<h3>1.1  드론 기반 객체 탐지의 필요성과 패러다임 변화</h3>
<p>최근 몇 년간 무인 항공기(Unmanned Aerial Vehicle, UAV), 즉 드론 기술은 군사적 목적을 넘어 산업, 보안, 재난 대응, 학술 연구 등 사회 전반에 걸쳐 급격하게 보급되었다.1 드론은 기존의 고정형 감시 시스템이나 지상 기반의 인력 순찰이 제공할 수 없는 광범위한 시야와 신속한 배포 능력을 통해 데이터 수집 및 분석의 패러다임을 근본적으로 변화시키고 있다.3 이러한 기술적 확산은 새로운 기회를 창출하는 동시에, 불법 드론 활동과 같은 새로운 위협을 야기하며 이를 효과적으로 탐지하고 대응하는 시스템의 필요성을 증대시켰다.1</p>
<p>이러한 시대적 요구에 부응하여, 드론을 활용한 객체 탐지 기술은 항공 감시(aerial surveillance) 분야의 핵심 기술로 부상했다. 특히 수색 및 구조(Search and Rescue) 임무에서 드론은 접근이 불가능한 광활한 지역을 신속하게 탐색할 수 있으며, 정밀 농업(Precision Agriculture) 분야에서는 작물의 상태를 정밀하게 모니터링하고, 지능형 교통 시스템(Intelligent Transportation Systems)에서는 실시간 교통 흐름을 분석하는 등 그 응용 범위가 무한히 확장되고 있다.5 이처럼 드론 기술의 발전은 새로운 사회·산업적 수요를 창출했고, 이 과정에서 드론이 취득한 방대한 양의 영상 데이터를 자동으로 분석할 수 있는 고성능 컴퓨터 비전 기술의 필요성이 폭발적으로 증가했다. 이러한 배경 속에서 기존의 강력한 실시간 객체 탐지 모델인 YOLO(You Only Look Once)를 드론 환경에 적용하려는 시도가 자연스럽게 이루어졌으나, 이는 예상치 못한 기술적 한계에 부딪히게 되었다. 드론 영상의 고유한 특성으로 인해 표준 데이터셋으로 학습된 일반 YOLO 모델의 성능이 현저히 저하되는 ’성능 격차(performance gap)’가 발생한 것이다. 이 문제를 해결하기 위한 특화된 연구 분야가 바로 ’Drone-YOLO’의 시작점이다.</p>
<h3>1.2  ‘Drone-YOLO’: 개념 정의 및 기술적 접근</h3>
<p>’Drone-YOLO’는 특정 단일 모델의 공식 명칭이 아니다. 이는 ‘You Only Look Once’ 계열의 실시간 객체 탐지 알고리즘을 드론 운용 환경의 특수성에 맞게 최적화하고 수정한 광범위한 연구 분야 전체를 지칭하는 포괄적인 용어이다.1 Drone-YOLO의 핵심 기술적 접근법은 YOLO가 가진 빠른 추론 속도와 높은 정확도 간의 탁월한 균형을 유지하면서, 드론 영상에서 빈번하게 발생하는 고유한 문제들을 해결하기 위해 신경망 아키텍처를 수정하거나 학습 전략을 고도화하는 데에 초점을 맞춘다.8</p>
<p>이러한 접근법은 단순히 일반적인 객체 탐지를 넘어, 배경과 구분이 어려운 위장 객체(camouflaged object)를 탐지하는 문제로까지 확장된다. 예를 들어, YOLO-FEDER FusionNet 아키텍처는 일반 객체 탐지기와 위장 객체 탐지기의 특징(feature)을 네트워크 중간에서 융합하는 혁신적인 방식을 제안한다.11 이 모델은 복잡하고 질감이 높은 환경(highly textured environments)에서 드론이 배경과 거의 구별되지 않는 상황에서도 탐지 성능을 극대화하여, 특히 안티드론(anti-drone) 시스템과 같은 고난도 응용 분야에서 그 가치를 입증한다.10 이처럼 Drone-YOLO는 단순한 모델의 적용을 넘어, 특정 도메인의 문제를 해결하기 위한 근본적인 아키텍처 재설계와 최적화 과정을 포괄하는 개념이다.</p>
<h3>1.3  핵심 과제 개괄: 일반 YOLO와의 차별점</h3>
<p>Drone-YOLO 연구가 해결하고자 하는 핵심 과제들은 표준적인 객체 탐지 환경과는 뚜렷한 차이를 보인다. 이러한 차별점들이 바로 일반 YOLO 모델이 드론 영상에서 성능 저하를 겪는 근본적인 원인이다.</p>
<p>첫째, <strong>작은 객체(small object) 문제</strong>가 가장 두드러진다. 드론은 높은 고도에서 지상을 촬영하기 때문에, 탐지 대상 객체(사람, 차량 등)가 전체 이미지에서 차지하는 픽셀 영역이 극히 작아진다.14 이러한 작은 객체들은 심층 컨볼루션 신경망(Deep Convolutional Neural Network)의 특징 추출 과정에서 정보가 쉽게 소실되어 탐지가 매우 어렵다.16</p>
<p>둘째, <strong>역동적인 환경 변화</strong>에 대응해야 한다. 드론의 빠른 움직임과 카메라 진동으로 인한 <strong>모션 블러(motion blur)</strong>, 시시각각 변하는 태양광으로 인한 급격한 <strong>조명 변화</strong>, 그리고 도시나 숲과 같은 **복잡한 배경(cluttered backgrounds)**은 이미지의 품질을 저하시키고 객체와 배경의 구분을 어렵게 만든다.16</p>
<p>셋째, <strong>객체의 크기(scale)와 방향(orientation)의 가변성</strong>이 매우 크다. 드론의 고도와 촬영 각도가 계속 변하기 때문에, 동일한 객체라도 이미지 상에서의 크기와 보이는 형태가 역동적으로 변화한다.16 이는 모델이 다양한 스케일과 시점에 대해 강건한(robust) 탐지 능력을 갖추어야 함을 의미한다.</p>
<p>이러한 문제들은 MS COCO와 같은 표준 데이터셋으로 학습된 일반 YOLO 모델의 성능을 저하시키는 주요 원인이며, Drone-YOLO 연구가 집중적으로 해결하고자 하는 기술적 난제들이다.7 더 나아가, 드론은 제한된 배터리와 탑재 중량으로 인해 고성능 컴퓨팅 자원을 탑재하기 어렵다는 현실적인 제약을 안고 있다.15 이는 모델의 정확도뿐만 아니라, ’모델 크기’와 ’연산 속도’까지 고려한 최적화가 필수적임을 시사한다. 이러한 제약 조건 하에서 성능을 극대화하려는 Drone-YOLO의 연구 노력은, 드론 분야를 넘어 모바일 기기, 자율주행차, 스마트 팩토리 등 모든 ‘엣지 컴퓨팅(Edge Computing)’ 환경에서의 실시간 AI 기술 발전에 기여하는 선행 연구로서의 중요한 의의를 가진다.</p>
<h2>2.  YOLO 아키텍처의 해부: YOLOv8을 중심으로</h2>
<p>Drone-YOLO의 기술적 기반을 이해하기 위해서는 그 근간이 되는 YOLO 아키텍처, 특히 현재 가장 널리 활용되는 YOLOv8에 대한 깊이 있는 분석이 선행되어야 한다. YOLOv8의 구조적 특징과 설계 철학은 Drone-YOLO의 다양한 변형 모델들이 왜 그러한 방식으로 개선되었는지를 이해하는 열쇠가 된다.</p>
<h3>2.1  YOLO의 기본 철학: 단일 단계 검출기</h3>
<p>YOLO(You Only Look Once)의 이름이 암시하듯, 이 알고리즘 계열의 핵심 철학은 ’속도’와 ’효율성’에 있다. 기존의 2단계(two-stage) 검출기(예: Faster R-CNN)가 ’객체가 있을 만한 후보 영역을 먼저 제안하고, 각 영역에 대해 분류를 수행’하는 복잡한 파이프라인을 가졌던 것과 달리, YOLO는 입력 이미지를 단 한 번만 신경망에 통과시켜 객체의 위치(bounding box)와 클래스(class)를 동시에 예측하는 단일 단계(one-stage) 방식을 채택했다.8 이러한 혁신적인 접근법은 구조적 단순성을 통해 압도적인 추론 속도를 달성했으며, 실시간 객체 탐지가 필수적인 수많은 응용 분야에서 표준 기술로 자리매김하게 만들었다.3 2023년 1월에 출시된 YOLOv8은 이러한 YOLO의 기본 철학을 계승하면서도, 이전 버전들의 장점을 집대성하고 새로운 구조적 개선을 도입하여 정확도, 속도, 그리고 사용 유연성을 한 차원 높은 수준으로 끌어올린 최신 모델이다.20</p>
<h3>2.2  백본(Backbone): CSPDarknet 기반 특징 추출</h3>
<p>모든 컨볼루션 신경망 기반 객체 탐지기의 심장은 입력 이미지로부터 유의미한 시각적 특징을 추출하는 백본 네트워크에 있다. YOLOv8의 백본은 이전 버전에서 그 성능이 입증된 CSPDarknet 아키텍처를 기반으로 설계되었으며, 저수준의 질감, 모서리, 색상 정보부터 고수준의 의미론적(semantic) 정보에 이르기까지 계층적인 특징(hierarchical features)을 효과적으로 추출하는 역할을 수행한다.12</p>
<p>YOLOv8 백본의 핵심 구성 요소는 <strong>C2f (Cross Stage Partial bottleneck with 2 convolutions)</strong> 모듈이다.23 이는 YOLOv5에서 사용되었던 C3 모듈을 대체하는 것으로, 더 많은 분기(split)와 교차 단계 연결(cross-stage connection)을 통해 특징 맵을 처리한다. 이러한 설계는 네트워크의 깊은 곳까지 그래디언트(gradient)가 원활하게 전파되도록 돕는 ’풍부한 그래디언트 흐름(richer gradient flow)’을 제공하여, 모델의 학습 안정성과 최종 성능을 향상시킨다.25 동시에, CSP(Cross-Stage Partial) 개념을 적용하여 연산량의 일부를 건너뛰게 함으로써 파라미터 수를 효율적으로 관리하고 계산 복잡도를 낮춘다. C2f 블록 내부에 포함되는 Bottleneck 블록의 개수는 모델의 전체적인 깊이와 표현력을 결정하는</p>
<p><code>depth_multiple</code> 하이퍼파라미터에 의해 조절되며, 이는 모델의 크기와 성능 간의 트레이드오프를 조절하는 중요한 요소이다.25</p>
<h3>2.3  넥(Neck): 다중 스케일 특징 융합</h3>
<p>백본에서 추출된 특징 맵들은 서로 다른 해상도(resolution)를 가진다. 네트워크의 초기 레이어에서 나온 고해상도 특징 맵은 객체의 정밀한 위치 정보를 담고 있는 반면, 후기 레이어에서 나온 저해상도 특징 맵은 객체의 종류를 구분하는 데 유리한 풍부한 의미 정보를 담고 있다. 넥(Neck) 아키텍처는 이렇게 서로 다른 수준의 특징 맵들을 효과적으로 융합하여, 이미지 내에 존재하는 다양한 크기의 객체들(작은 객체부터 큰 객체까지)을 모두 강건하게 탐지할 수 있도록 특징을 정제하고 재구성하는 중요한 역할을 한다.25</p>
<p>YOLOv8은 이 역할을 수행하기 위해 <strong>PANet(Path Aggregation Network)</strong> 구조를 채택한다.4 PANet은 전통적인 FPN(Feature Pyramid Network)이 가진 상향식(top-down) 경로, 즉 의미 정보가 풍부한 저해상도 특징을 점차 업샘플링(upsampling)하여 고해상도 특징과 결합하는 방식에, 추가적으로 하향식(bottom-up) 경로를 도입한다.28 이 하향식 경로는 저수준의 정밀한 위치 정보를 다시 상위 레이어로 전달하는 역할을 하여, 정보의 흐름을 양방향으로 만들어 줌으로써 각기 다른 수준의 특징들이 더욱 효과적으로 융합되도록 돕는다.23</p>
<p>또한, 백본의 마지막 단과 넥의 시작점 사이에는 <strong>SPPF(Spatial Pyramid Pooling Fast)</strong> 모듈이 위치한다.12 SPPF는 입력된 특징 맵에 대해 서로 다른 크기의 커널(kernel)로 최대 풀링(max-pooling)을 병렬적으로 수행한 후, 그 결과들을 모두 연결(concatenate)하는 방식으로 동작한다. 이는 모델이 입력 이미지의 크기에 상관없이 고정된 크기의 특징 벡터를 생성하게 하여 다양한 크기의 객체에 대한 강건성을 확보하고, 기존의 SPP 모듈보다 연산 속도를 개선하여 효율성을 높인 버전이다.27</p>
<h3>2.4  헤드(Head): 최종 예측 생성</h3>
<p>넥에서 정제된 최종 특징 맵을 입력받아 실제 객체의 클래스 확률(class probabilities)과 경계 상자 좌표(bounding box coordinates)를 예측하는 부분이 헤드(Head)이다. YOLOv8의 헤드는 이전 버전들과 비교하여 두 가지 핵심적인 혁신을 담고 있다.</p>
<p>첫째, <strong>앵커 프리(Anchor-Free)</strong> 방식을 전면적으로 채택했다.20 과거의 앵커 기반(anchor-based) 모델들은 사전에 정의된 여러 형태와 크기의 ’앵커 박스’들을 기준으로 삼고, 이들로부터 얼마나 이동하고 크기를 조절해야 하는지(offset)를 예측하는 간접적인 방식을 사용했다. 이는 데이터셋에 따라 최적의 앵커 박스를 설계해야 하는 번거로움이 있었고, 불필요한 예측을 많이 생성했다. 반면, YOLOv8의 앵커 프리 방식은 객체의 중심점을 직접 예측하고, 중심점으로부터 각 경계까지의 거리를 예측하는 훨씬 직관적이고 효율적인 방식을 사용한다.25 이로 인해 예측해야 할 박스의 수가 크게 줄어들어, 후처리 과정인 NMS(Non-Maximum Suppression)의 계산 부담을 경감시키고 전체적인 추론 속도를 향상시키는 효과를 가져온다.20</p>
<p>둘째, <strong>분리형 헤드(Decoupled Head)</strong> 구조를 사용한다.24 이는 객체의 위치를 예측하는 회귀(regression) 작업과 객체의 종류를 예측하는 분류(classification) 작업을 별도의 컨볼루션 레이어 브랜치에서 독립적으로 수행하도록 설계한 것이다. 위치 예측은 공간적 정밀성을 요구하는 반면, 분류 예측은 의미론적 구별력을 요구하는 등 두 작업의 특성이 다르기 때문에, 이를 분리하여 각각 최적화하는 것이 전체 모델의 정확도를 높이는 데 더 유리하다는 아이디어에 기반한다.30</p>
<p>이러한 YOLOv8의 아키텍처 진화(C3→C2f, Anchor-based→Anchor-free, Coupled→Decoupled Head)는 단순히 성능 수치를 높이는 것을 넘어, ’효율성’과 ’정확도’라는 두 가지 상충될 수 있는 목표를 동시에 달성하기 위한 정교한 엔지니어링의 산물이다. C2f 모듈은 더 나은 특징 표현력을 제공하며 정확도를 높이고, 앵커 프리 방식은 추론 속도를 직접적으로 향상시킨다. 분리형 헤드는 두 예측 작업 간의 간섭을 줄여 다시 한번 정확도를 끌어올린다. 이 세 가지 핵심 변화의 시너지는 YOLOv8을 드론과 같이 자원이 제한된 엣지 환경에서 객체 탐지를 수행하기 위한 강력하고 신뢰성 있는 베이스라인 모델로 만드는 근본적인 이유가 된다.</p>
<h3>2.5  모델 변종 및 성능 트레이드오프</h3>
<p>YOLOv8은 단일 모델이 아닌, 다양한 규모와 성능을 가진 모델 제품군(family of models)으로 제공된다. 이는 사용자가 자신의 하드웨어 제약 조건과 목표 성능에 맞춰 최적의 모델을 선택할 수 있도록 하는 유연성을 제공한다. 모델 이름 뒤에 붙는 접미사(n, s, m, l, x)는 모델의 크기를 나타내며, ‘n’(nano)이 가장 작고 빠르며 ‘x’(extra-large)가 가장 크고 정확하다. 이러한 모델 변종들은 주로 백본의 깊이(<code>depth_multiple</code>)와 각 레이어의 채널 수(<code>width_multiple</code>)를 조절하여 구현된다.25 아래 표는 COCO 데이터셋에서 평가된 YOLOv8의 주요 변종 모델들의 성능 지표를 요약한 것이다.</p>
<table><thead><tr><th>모델 (Model)</th><th>입력 크기 (Size)</th><th>mAP (val, 50-95)</th><th>속도 CPU ONNX (ms)</th><th>속도 A100 TensorRT (ms)</th><th>파라미터 (M)</th><th>FLOPs (B)</th></tr></thead><tbody>
<tr><td>YOLOv8n</td><td>640</td><td>37.3</td><td>80.4</td><td>0.99</td><td>3.2</td><td>8.7</td></tr>
<tr><td>YOLOv8s</td><td>640</td><td>44.9</td><td>128.4</td><td>1.20</td><td>11.2</td><td>28.6</td></tr>
<tr><td>YOLOv8m</td><td>640</td><td>50.2</td><td>234.7</td><td>1.83</td><td>25.9</td><td>78.9</td></tr>
<tr><td>YOLOv8l</td><td>640</td><td>52.9</td><td>375.2</td><td>2.39</td><td>43.7</td><td>165.2</td></tr>
<tr><td>YOLOv8x</td><td>640</td><td>53.9</td><td>479.1</td><td>3.53</td><td>68.2</td><td>257.8</td></tr>
</tbody></table>
<p>표 1: YOLOv8 모델 변종별 성능 지표 (COCO 데이터셋 기준).21</p>
<p>이 표는 ‘정확도-속도-모델 크기’ 간의 명확한 트레이드오프 관계를 보여준다. 예를 들어, 드론의 온보드 컴퓨터(예: NVIDIA Jetson Nano)와 같이 연산 능력이 매우 제한적인 환경에서는 YOLOv8n이나 YOLOv8s 모델이 실시간 처리를 위한 현실적인 선택지가 될 수 있다. 반면, 더 높은 탐지 정확도가 요구되고 비교적 고성능의 컴퓨팅 자원을 사용할 수 있는 경우에는 YOLOv8m이나 YOLOv8l을 베이스라인으로 선택하고 추가적인 최적화나 경량화 기법을 적용하는 전략을 고려할 수 있다. 이처럼, Drone-YOLO 개발의 첫 단계는 주어진 임무와 하드웨어 제약 조건에 가장 적합한 베이스라인 모델을 이 트레이드오프 곡선 상에서 전략적으로 선택하는 것이다.</p>
<h2>3.  드론의 시선: 항공 영상 객체 탐지의 근본적 난제</h2>
<p>드론의 카메라는 인간의 눈이나 지상의 CCTV와는 전혀 다른 시점에서 세상을 바라본다. 이러한 독특한 ’드론의 시선’은 객체 탐지 알고리즘에 기존에는 경험하지 못했던 복합적이고 근본적인 난제들을 제기한다. 이 문제들은 상호 독립적이지 않고 서로의 영향을 증폭시키는 연쇄적인 특성을 가지며, 이로 인해 일반적인 객체 탐지 모델의 성능을 급격히 저하시킨다.</p>
<h3>3.1  작은 객체 문제 (Small Object Detection, SOD)</h3>
<p>드론 기반 객체 탐지에서 가장 핵심적이고 어려운 문제는 단연 ’작은 객체 탐지(Small Object Detection, SOD)’이다.14 드론이 수십, 수백 미터의 높은 고도에서 지상을 촬영할 때, 사람이나 차량과 같은 객체들은 이미지 상에서 불과 수십 픽셀, 심한 경우 32x32 픽셀 미만의 매우 작은 영역만을 차지하게 된다.15</p>
<p>이러한 작은 객체들은 컨볼루션 신경망(CNN)의 구조적 특성 때문에 탐지가 극도로 어려워진다. CNN 백본은 특징 추출을 위해 컨볼루션과 풀링(pooling) 또는 스트라이드(stride)가 있는 컨볼루션 연산을 반복적으로 수행하며 특징 맵의 공간적 해상도를 점진적으로 줄여나간다(down-sampling). 이 과정에서 작은 객체들이 가지고 있던 희소한 공간 정보와 세밀한 질감 특징(fine-grained details)은 여러 레이어를 거치면서 쉽게 희석되거나 완전히 소실되어 버린다.16 결국 네트워크의 깊은 레이어에서는 작은 객체의 존재 자체가 사라져 버려, 모델이 이를 탐지할 수 없게 되는 것이다. 이는 제한된 해상도와 부족한 컨텍스트 정보로 인해 발생하는 근본적인 문제이다.14</p>
<h3>3.2  스케일 및 시점의 역동성</h3>
<p>드론은 고정된 위치에서 촬영하는 CCTV와 달리, 3차원 공간을 자유롭게 이동하며 촬영한다. 이러한 역동성은 객체의 외관에 두 가지 큰 변화를 야기한다.</p>
<p>첫째, <strong>스케일 변화(scale variation)</strong> 문제이다. 드론이 상승하거나 하강함에 따라, 또는 카메라 줌 기능을 사용함에 따라 동일한 객체라도 이미지 내에서 차지하는 크기가 수시로, 그리고 급격하게 변한다.16 모델이 특정 스케일의 객체에만 과적합(overfitting)되지 않고, 이렇게 광범위한 스케일 변화에 강건하게 대응할 수 있도록 설계되어야 한다.</p>
<p>둘째, <strong>시점 및 방향 변화(viewpoint and orientation variation)</strong> 문제이다. 드론은 객체를 정면, 측면, 후면뿐만 아니라, 지상을 수직으로 내려다보는 부감(bird’s-eye view) 등 매우 다양한 각도에서 촬영한다. 이로 인해 객체의 보이는 형태와 방향(orientation)이 계속해서 바뀐다. 기존의 객체 탐지 모델들이 사용하는 일반적인 수평 경계 상자(Horizontal Bounding Box, HBB)는 길고 비스듬한 객체(예: 주차된 트럭)를 감쌀 때 불필요한 배경 영역을 많이 포함하게 되어 탐지 정확도를 떨어뜨린다. 이러한 문제를 해결하기 위해, 객체의 회전 각도까지 고려하여 더 정밀하게 객체를 감싸는 방향성 경계 상자(Oriented Bounding Box, OBB)의 필요성이 대두되고 있다.18</p>
<h3>3.3  환경적 제약: 블러, 조명, 배경, 가려짐</h3>
<p>드론은 통제되지 않은 야외 환경에서 운용되므로, 예측 불가능한 여러 환경적 요인들이 이미지 품질을 저하시키고 탐지를 방해한다.</p>
<ul>
<li>
<p><strong>모션 블러 (Motion Blur):</strong> 드론의 빠른 기동이나 프로펠러로 인한 기체 진동은 이미지에 흐림 효과(blur)를 발생시킨다. 모션 블러는 객체의 경계선을 뭉개고 내부 질감 정보를 파괴하여, 모델이 객체의 특징을 명확하게 인식하는 것을 방해한다.16</p>
</li>
<li>
<p><strong>조명 변화 (Lighting Variation):</strong> 하루 중 시간대(아침, 정오, 저녁), 날씨(맑음, 흐림, 비), 그리고 건물이나 나무로 인해 생기는 짙은 그림자 등은 조명 조건을 급격하게 변화시킨다. 이는 객체의 본래 색상과 대비(contrast)를 왜곡시켜 모델의 탐지 성능에 직접적인 영향을 미친다.16</p>
</li>
<li>
<p><strong>복잡한 배경 (Cluttered Background):</strong> 도시의 빌딩 숲, 숲속의 우거진 나뭇잎, 주차장의 수많은 차량 등 복잡하고 질감이 다양한 배경은 탐지 대상 객체와 배경을 분리하기 어렵게 만든다. 특히, 객체가 배경과 유사한 색상이나 패턴을 가질 경우, 마치 보호색처럼 배경에 녹아드는 위장(camouflage) 효과가 발생하여 탐지를 더욱 어렵게 만든다.10</p>
</li>
<li>
<p><strong>객체 가려짐 (Occlusion):</strong> 드론의 부감 시점에서는 객체들이 서로 겹쳐 보이는 경우가 빈번하다. 예를 들어, 군중 속의 사람들, 빽빽하게 주차된 차들, 또는 나무나 건물과 같은 다른 구조물에 의해 객체의 일부가 가려지는 상황이 자주 발생한다.14 모델은 이렇게 불완전한 정보만을 가지고도 객체의 전체 형태와 위치를 추론할 수 있어야 한다.</p>
</li>
</ul>
<p>이러한 난제들은 독립적으로 작용하기보다 서로의 효과를 증폭시키는 연쇄적인 특성을 가진다. 예를 들어, ’높은 고도’로 인해 이미 ’작은 객체’가 된 대상에 ’모션 블러’가 더해지면 형체를 알아보기 거의 불가능해지고, 이것이 ‘복잡한 배경’ 속에 파묻히면 신호 대 잡음비(Signal-to-Noise Ratio)가 극도로 낮아진다. 여기에 ’객체 가려짐’까지 발생하면 가시적인 픽셀 수가 더욱 줄어들어 탐지는 사실상 불가능에 가까워진다. 이처럼 복합적인 문제 상황을 해결하기 위해서는 모델이 극도로 제한된 정보 속에서도 강건한 특징을 추출하고 추론할 수 있어야 한다. 이는 단순히 기존 모델의 마지막 레이어를 파인튜닝하는 수준으로는 해결이 불가능하며, 특징 추출 초기 단계부터 정보 손실을 최소화하고, 넥 단계에서 소실된 정보를 보강하며, 특정 특징을 증폭시키는 등, 네트워크의 전반적인 정보 흐름을 근본적으로 재설계하는 접근법이 필수적임을 시사한다. 이것이 바로 Drone-YOLO 연구의 핵심 동력이다.</p>
<h3>3.4  온보드 시스템의 한계</h3>
<p>기술적인 난제 외에도, 드론이라는 플랫폼 자체가 가진 물리적, 하드웨어적 한계 역시 중요한 제약 조건으로 작용한다. 드론에 탑재되는 임베디드 컴퓨팅 보드(예: NVIDIA Jetson 시리즈, Raspberry Pi)는 데이터센터의 서버급 GPU에 비해 연산 능력, 메모리 용량, 전력 소모량 등 모든 면에서 현저히 제한된다.5</p>
<p>이러한 하드웨어의 제약은 아무리 정확도가 높은 모델이라도 그 크기가 너무 크거나 계산 복잡도가 높으면 드론에 탑재하여 실시간으로 구동하는 것이 불가능함을 의미한다. 따라서 Drone-YOLO 모델 개발은 단순히 정확도를 높이는 것을 넘어, 모델의 경량화(lightweighting)와 연산 효율성 극대화라는 과제를 반드시 함께 해결해야 한다.15 이는 정확도, 속도, 모델 크기라는 세 가지 요소 간의 최적의 균형점을 찾는 정교한 엔지니어링 문제를 제기하며, Drone-YOLO 연구의 또 다른 중요한 축을 형성한다.</p>
<h2>4.  성능 한계 돌파를 위한 아키텍처 혁신</h2>
<p>드론 영상이 제기하는 복합적인 난제들을 극복하기 위해, 연구자들은 YOLO의 기본 구조를 창의적으로 수정하고 확장하는 다양한 아키텍처 혁신을 제안해왔다. 이러한 혁신들은 크게 ’보완(Compensation)’과 ’융합(Fusion)’이라는 두 가지 핵심 전략으로 요약될 수 있다. ‘보완’ 전략은 CNN의 구조적 한계로 인해 소실되거나 약화된 정보를 어텐션 메커니즘 등을 통해 강화하고 보충하는 접근법이다. ‘융합’ 전략은 서로 다른 종류의 정보(예: 다중 스케일, 다중 모달리티, 다중 모델)를 지능적으로 결합하여 단일 정보원만으로는 얻을 수 없는 새로운 시너지를 창출하는 접근법이다.</p>
<h3>4.1  어텐션 메커니즘의 적용: 핵심에 집중하기</h3>
<p>어텐션 메커니즘(Attention Mechanism)은 인간의 시각 시스템이 중요한 정보에 집중하고 나머지는 흘려보는 방식에서 영감을 얻은 기술이다. 신경망에 이를 적용하면, 특징 맵의 모든 픽셀이나 채널을 동등하게 취급하는 대신, 객체 탐지에 더 중요하고 유의미한 영역이나 채널에 더 높은 가중치를 부여하여 정보의 표현력을 선택적으로 강화할 수 있다.11 이는 제한된 정보 자원을 효율적으로 사용하는 ‘선택과 집중’ 전략으로, 다운샘플링으로 인한 정보 소실의 부정적 영향을 효과적으로 보완한다.</p>
<ul>
<li>
<p><strong>채널 및 공간 어텐션 (Channel and Spatial Attention):</strong> 가장 널리 사용되는 어텐션 방식 중 하나는 CBAM(Convolutional Block Attention Module)이다. CBAM은 두 개의 하위 모듈로 구성된다. 먼저 **채널 어텐션(Channel Attention)**은 특징 맵의 여러 채널들 중에서 어떤 채널이 객체 인식에 더 중요한지(“무엇을” 볼 것인가) 학습하여 채널별 가중치를 부여한다. 그 다음 **공간 어텐션(Spatial Attention)**은 어떤 위치의 픽셀들이 더 중요한지(“어디를” 볼 것인가) 학습하여 공간적 가중치 맵을 생성한다. 이 두 어텐션을 순차적으로 적용함으로써 특징 맵을 효과적으로 정제하고 객체 관련 특징을 부각시킨다.11</p>
</li>
<li>
<p><strong>최신 어텐션 기법:</strong> Drone-YOLO 연구에서는 더 가볍고 효율적이거나, 특정 문제에 더 특화된 새로운 어텐션 기법들이 활발히 도입되고 있다.</p>
</li>
<li>
<p><strong>Triplet Attention:</strong> 원격 탐사 이미지의 작은 객체 탐지를 위해 제안된 경량 어텐션 모듈로, 세 개의 브랜치를 사용하여 채널과 공간 차원 간의 상호 의존성을 포착한다.32</p>
</li>
<li>
<p><strong>EMA (Efficient Multi-scale Attention):</strong> 다중 스케일 특징 맵에 걸쳐 장거리 의존성(long-range dependency)을 효율적으로 모델링한다. 이를 통해 지역적인 세부 특징과 전역적인 컨텍스트 특징을 모두 강화하여 보다 포괄적인 특징 표현을 가능하게 한다.33</p>
</li>
<li>
<p><strong>하이브리드 어텐션 (HAM - Hybrid Attention Mechanism):</strong> EMSA(Enhanced Multi-head Self-Attention)와 CA(Coordinate Attention)를 결합한 구조이다. EMSA가 넓은 범위의 컨텍스트 정보를 인식하는 능력을 향상시키고, CA가 채널 간의 관계를 강화하면서 위치 정보의 손실을 줄여, 장거리 위치 정보 인식과 지역적 특징 학습 능력을 동시에 극대화한다.28</p>
</li>
</ul>
<h3>4.2  고도화된 특징 융합 전략</h3>
<p>단일 시점, 단일 스케일, 단일 센서에서 얻는 정보는 본질적으로 불완전하다. 작은 객체는 저해상도 특징 맵에서 잘 보이지 않고, 큰 객체는 고해상도 특징 맵의 제한된 수용장(receptive field) 안에서 전체적인 형태를 파악하기 어렵다. 고도화된 특징 융합 전략은 이러한 서로 다른 정보들을 지능적으로 결합하여 한계를 극복하고 시너지를 창출하는 것을 목표로 한다.</p>
<ul>
<li>
<p><strong>FPN, PANet에서 BiFPN으로의 진화:</strong> 특징 융합 네트워크는 FPN(Feature Pyramid Network)에서 시작하여 PANet으로 발전했고, 최근에는 **BiFPN(Bidirectional FPN)**이 주목받고 있다.34 BiFPN은 PANet의 양방향 정보 흐름 구조를 유지하면서, 각 특징 맵이 융합에 기여하는 중요도에 따라 학습 가능한 가중치를 부여한다. 이를 통해 불필요한 특징의 영향을 줄이고 중요한 특징을 강조하여, 더 적은 연산량으로 더 높은 융합 효율과 성능을 달성한다.34</p>
</li>
<li>
<p><strong>YOLO-FEDER FusionNet의 모델 수준 융합:</strong> 이 아키텍처는 특징 융합을 한 차원 높은 수준으로 끌어올린다. 일반적인 객체 탐지기(YOLOv8 백본)와 특정 목적에 특화된 탐지기(위장 객체 탐지기 FEDER)를 두 개의 독립적인 브랜치로 구성하고, 각 브랜치에서 추출된 고수준의 특징 맵을 넥(neck) 단계에서 융합한다.10 이 접근법은 드론이 복잡한 숲이나 도시 배경 속에 숨어있는 다른 드론을 탐지해야 하는 안티드론 시나리오와 같이, 특정하고 어려운 문제에 대해 기존의 단일 모델로는 달성하기 어려운 압도적인 성능 향상을 보여준다.13</p>
</li>
<li>
<p><strong>DGE-YOLO의 멀티모달 융합:</strong> 드론에 가시광선 카메라와 적외선(thermal) 카메라를 함께 장착하는 멀티모달(multi-modal) 시스템은 야간이나 악천후와 같은 열악한 조건에서의 탐지 강건성을 크게 향상시킬 수 있다. DGE-YOLO는 이러한 멀티모달 입력을 효과적으로 처리하기 위해, 각 센서(적외선, 가시광선) 데이터를 독립적으로 처리하는 <strong>듀얼 브랜치(dual-branch) 백본</strong>을 설계했다. 각 브랜치에서 추출된 특징들은 네트워크의 여러 의미론적 스케일(semantic scales)에서 점진적으로 융합되어, 두 모달리티의 상호 보완적인 정보를 최대한 활용한다.33</p>
</li>
</ul>
<h3>4.3  경량화를 위한 네트워크 설계</h3>
<p>드론의 제한된 온보드 컴퓨팅 자원 위에서 실시간으로 동작하기 위해서는 모델의 경량화가 필수적이다. 이는 단순히 큰 모델을 작게 만드는 것을 넘어, 성능 저하를 최소화하면서 모델의 크기와 연산량을 줄이는 정교한 네트워크 설계를 요구한다.</p>
<p>LACF-YOLO 모델은 이러한 경량화 설계의 성공적인 사례를 보여준다.32 이 모델은 YOLOv8을 기반으로 하되, 상대적으로 무거운 C2f 모듈을 더 적은 파라미터로 유사한 성능을 내는 Dilated Inverted Convolution Layer로 대체했다. 또한, Triplet Attention과 같은 경량 어텐션 모듈을 도입하여 중요한 특징에 집중하도록 유도했다. 그 결과, 베이스라인 모델보다 mAP(mean Average Precision)를 향상시키면서도 파라미터 수를 34.9%, FLOPs(Floating Point Operations)를 26.2%나 감소시키는 놀라운 효율성을 달성했다. 이는 드론 탑재를 위한 매우 현실적이고 효과적인 접근법이다.32</p>
<p>또 다른 연구에서는 GhostNet에서 제안된 Ghost 모듈을 YOLOv5의 병목(bottleneck) 구조에 통합하는 방법을 사용했다.10 Ghost 모듈은 표준 컨볼루션 연산을 더 적은 비용의 선형 변환(linear transformation)으로 대체하여, 유사한 특징 맵을 더 적은 파라미터와 연산량으로 생성할 수 있게 한다. 이를 통해 모델을 경량화하면서도 드론 표적 특징 추출 능력을 높이고 배경 억제 능력을 개선하는 효과를 거두었다.10</p>
<h2>5.  모델 학습의 정교화: 손실 함수와 데이터셋</h2>
<p>최첨단 아키텍처를 설계하는 것만큼이나 중요한 것은 모델이 주어진 데이터로부터 효과적으로 학습할 수 있도록 유도하는 정교한 학습 전략을 수립하는 것이다. 여기에는 모델의 예측이 얼마나 틀렸는지를 측정하고 가중치 업데이트 방향을 결정하는 ’손실 함수(Loss Function)’의 설계와, 실제 드론 환경의 복잡성을 잘 반영하는 고품질의 ’데이터셋(Dataset)’을 활용하는 것이 포함된다.</p>
<h3>5.1  Bounding Box Regression 손실 함수의 진화</h3>
<p>YOLO 모델의 전체 손실 함수는 크게 세 가지 요소로 구성된다: 예측된 경계 상자가 실제 경계 상자와 얼마나 정확히 일치하는지를 측정하는 <strong>위치 손실(Localization Loss)</strong>, 해당 상자 안에 객체가 존재할 확률을 측정하는 <strong>신뢰도 손실(Confidence/Objectness Loss)</strong>, 그리고 예측된 객체의 클래스가 실제 클래스와 얼마나 일치하는지를 측정하는 <strong>분류 손실(Classification Loss)</strong>.22 이 중에서 모델의 최종적인 위치 정확도에 가장 지대한 영향을 미치는 것은 위치 손실이며, 이 손실 함수는 지난 몇 년간 눈부신 발전을 거듭해왔다.</p>
<p>이러한 발전의 역사는 경계 상자 회귀(Bounding Box Regression) 문제를 점점 더 ’기하학적(Geometric)’으로 정교하게 모델링하려는 노력의 과정으로 볼 수 있다. 이는 단순히 두 박스의 좌표 값 차이를 줄이는 것을 넘어, 수렴 과정 자체를 더 안정적이고 효율적으로 만들려는 시도이다.</p>
<ul>
<li>
<p><strong>IoU 기반 손실 함수의 등장과 발전:</strong></p>
</li>
<li>
<p>초기 YOLO 버전들은 예측된 박스의 중심 좌표(x, y)와 너비/높이(w, h)를 실제 값과 직접 비교하는 L2 손실(Squared Error)을 사용했다. 그러나 이 방식은 박스의 크기에 따라 손실의 스케일이 달라지고, 각 변수가 독립적으로 취급되어 박스의 기하학적 형태를 종합적으로 고려하지 못하는 단점이 있었다.</p>
</li>
<li>
<p>이러한 문제를 해결하기 위해 YOLOv4 이후부터는 예측 박스와 실제 박스 간의 중첩 영역을 직접적으로 측정하는 <strong>IoU(Intersection over Union)</strong> 기반 손실 함수가 주류가 되었다.36 IoU는 두 박스의 교집합 영역을 합집합 영역으로 나눈 값으로, 0과 1 사이의 정규화된 값을 가지며 박스의 크기와 위치에 무관하게 일관된 척도를 제공한다.</p>
</li>
<li>
<p>그러나 순수 IoU 손실(<code>$1 - IoU$</code>)은 두 박스가 전혀 겹치지 않을 때 손실 값이 1로 일정하고 그래디언트가 0이 되어, 모델이 어느 방향으로 박스를 움직여야 할지 학습할 수 없는 치명적인 문제가 있었다.</p>
</li>
<li>
<p>이 문제를 해결하기 위해 **GIoU (Generalized IoU)**가 제안되었다. GIoU는 IoU 항에 추가적으로, 두 박스를 모두 포함하는 가장 작은 볼록 상자(convex hull)와 두 박스의 합집합 간의 면적 차이를 페널티로 부여한다. 이를 통해 겹치지 않는 경우에도 두 박스가 얼마나 멀리 떨어져 있는지 측정하여 그래디언트를 제공할 수 있게 되었다.37</p>
</li>
<li>
<p>**DIoU (Distance IoU)**는 한 걸음 더 나아가, 두 박스의 중심점 간의 거리를 직접적으로 최소화하는 페널티 항을 추가했다. 이는 GIoU보다 더 빠르고 안정적인 수렴을 가능하게 했다.37</p>
</li>
<li>
<p>**CIoU (Complete IoU)**는 DIoU에 박스의 가로세로 비율(aspect ratio)의 일관성을 고려하는 항을 추가하여, 중심점 거리, 중첩 영역, 그리고 박스의 형태까지 종합적으로 고려하는 가장 완성된 형태의 IoU 기반 손실 함수로 자리 잡았다.37 CIoU 손실의 공식은 다음과 같다.</p>
</li>
</ul>
<p>코드 스니펫</p>
<pre><code>L_{CIoU} = 1 - IoU + \frac{\rho^2(b, b^{gt})}{c^2} + \alpha v
$$```
여기서 `$b$`와 `$b^{gt}$`는 각각 예측 박스와 실제 박스를, `$\rho(\cdot)$`는 두 박스 중심점 간의 유클리드 거리를, `$c$`는 두 박스를 포함하는 가장 작은 볼록 상자의 대각선 길이를 나타낸다. `$v$`는 가로세로 비율의 일관성을 측정하는 항이며, `$\alpha$`는 이를 조절하는 가중치 파라미터이다.
</code></pre>
<ul>
<li>
<p><strong>최신 손실 함수: SIoU와 WIoU</strong></p>
<ul>
<li><strong>SIoU (Scylla IoU):</strong> 드론 영상처럼 객체의 방향과 시점이 동적으로 변하는 환경에서는 예측 박스가 목표를 향해 가는 ‘경로’ 자체를 최적화하는 것이 중요하다. SIoU는 이러한 문제에 착안하여, 예측 박스와 실제 박스의 중심점을 잇는 벡터의 **‘각도(Angle Cost)’**를 고려하는 새로운 페널티 항을 도입했다.36 이 각도 페널티는 예측 박스가 불필요하게 “헤매는(wandering)” 현상을 줄이고, 가장 가까운 축 방향으로 먼저 이동한 후 거리를 좁히도록 유도하여 학습 속도와 최종 정확도를 모두 향상시킨다.38 SIoU 손실은 각도, 거리, 형태, IoU 비용의 네 가지 요소로 구성된다.</li>
</ul>
<p>코드 스니펫</p>
</li>
</ul>
<pre><code>L_{SIoU} = 1 - IoU + \frac{\Delta + \Omega}{2}
$$```
여기서 `$\Delta$`는 각도를 고려한 거리 비용, `$\Omega$`는 형태 비용을 나타낸다.

*   **WIoU (Wise IoU):** 학습 데이터셋에는 종종 품질이 낮은 라벨(예: 부정확한 경계 상자)이 포함될 수 있다. 이러한 이상치(outlier)들은 손실 계산에 과도한 영향을 미쳐 모델의 학습을 방해할 수 있다. WIoU는 이러한 문제를 해결하기 위해, 앵커 박스의 품질을 동적으로 평가하고, 품질이 낮은 박스(이상치)에는 낮은 가중치를, 품질이 좋은 박스에는 높은 가중치를 부여하는 동적 가중치 메커니즘을 도입했다. 이를 통해 모델이 품질 좋은 예시에 더 집중하여 학습하도록 유도함으로써 전반적인 강건성과 성능을 향상시킨다.[8, 34]
</code></pre>
<p>아래 표는 주요 IoU 기반 손실 함수들의 핵심 아이디어와 특징을 요약한 것이다.</p>
<table><thead><tr><th>손실 함수 (Loss Function)</th><th>핵심 아이디어 (Core Idea)</th><th>장점 (Pros)</th><th>해결하려는 문제 (Problem Addressed)</th></tr></thead><tbody>
<tr><td><strong>IoU</strong></td><td>중첩 영역 최대화</td><td>스케일 불변성, 정규화된 메트릭</td><td>겹치지 않을 때 그래디언트 소실</td></tr>
<tr><td><strong>GIoU</strong></td><td>IoU + 볼록 상자 페널티</td><td>겹치지 않아도 학습 가능</td><td>수렴 속도가 느리고, 정렬 방식 미반영</td></tr>
<tr><td><strong>DIoU</strong></td><td>GIoU + 중심점 거리 최소화</td><td>빠른 수렴 속도, 더 나은 위치 정확도</td><td>가로세로 비율(형태) 불일치 문제</td></tr>
<tr><td><strong>CIoU</strong></td><td>DIoU + 가로세로 비율 일관성</td><td>거리, 중첩, 형태를 모두 고려</td><td>복잡한 회귀 문제에서 안정성 확보</td></tr>
<tr><td><strong>SIoU</strong></td><td>CIoU + 예측 방향(각도) 고려</td><td>학습 경로 최적화, 빠른 수렴</td><td>예측 박스가 “헤매는” 현상, 동적 환경</td></tr>
<tr><td><strong>WIoU</strong></td><td>IoU + 동적 가중치 부여</td><td>이상치(outlier)에 대한 강건성</td><td>품질 낮은 데이터 라벨로 인한 학습 불안정</td></tr>
</tbody></table>
<p>표 2: 주요 IoU 기반 손실 함수 비교.34</p>
<h3>5.2  벤치마크 데이터셋: VisDrone</h3>
<p>고성능 Drone-YOLO 모델을 개발하고 그 성능을 객관적으로 평가하기 위해서는 실제 드론 운용 환경의 복잡성과 다양성을 잘 반영하는 대규모 벤치마크 데이터셋이 필수적이다. 이 분야에서 가장 널리 사용되는 표준 벤치마크는 <strong>VisDrone 데이터셋</strong>이다.39</p>
<p>VisDrone 데이터셋은 중국 톈진 대학교의 AISKYEYE 팀에 의해 구축되었으며, 드론 기반 컴퓨터 비전 연구를 위해 특별히 설계되었다.39 이 데이터셋은 중국의 14개 다른 도시와 농촌 지역에서, 다양한 날씨(맑음, 흐림, 비)와 조명 조건(낮, 밤, 황혼) 하에 촬영된 방대한 양의 고해상도 이미지(최대 2000x1500 픽셀)와 비디오 시퀀스를 포함한다.40</p>
<p>데이터셋은 보행자(pedestrian), 사람(people), 자전거(bicycle), 자동차(car), 밴(van), 트럭(truck), 세발자전거(tricycle), 차양 달린 세발자전거(awning-tricycle), 버스(bus), 모터(motor) 등 10개의 객체 클래스에 대해 총 260만 개 이상의 경계 상자 주석이 수동으로 정교하게 달려 있다.8 특히, 이 데이터셋은 다음과 같은 드론 영상의 현실적인 난제들을 충실히 반영하고 있어 Drone-YOLO 모델의 성능을 종합적으로 평가하는 데 매우 적합하다.</p>
<ul>
<li>
<p><strong>다양한 스케일의 객체:</strong> 드론의 고도 변화로 인해 매우 작은 객체부터 큰 객체까지 다양한 크기의 객체가 혼재한다.</p>
</li>
<li>
<p><strong>높은 객체 밀도와 가려짐:</strong> 도심 장면에서는 수많은 차량과 보행자들이 빽빽하게 밀집해 있고, 서로 겹쳐 가려지는 경우가 빈번하다.</p>
</li>
<li>
<p><strong>다양한 시점:</strong> 정면, 측면, 부감 등 다양한 카메라 각도에서 촬영된 이미지를 포함한다.</p>
</li>
</ul>
<p>VisDrone 데이터셋은 구체적으로 5개의 주요 태스크(Task)로 구성되어 있다: (1) 이미지 내 객체 탐지, (2) 비디오 내 객체 탐지, (3) 단일 객체 추적, (4) 다중 객체 추적, (5) 군중 계수(crowd counting).40 이 중 Task 1과 2는 Drone-YOLO 모델의 탐지 성능을 평가하는 데 주로 사용되며, 수많은 연구 논문에서 이 데이터셋을 이용한 성능 비교 결과를 제시하고 있다.8</p>
<h2>6.  정량적 성능 분석 및 비교</h2>
<p>Drone-YOLO 분야의 다양한 아키텍처 혁신과 학습 전략들이 실제로 얼마나 효과적인지를 평가하기 위해서는 객관적이고 표준화된 지표를 사용한 정량적 분석이 필수적이다. 본 장에서는 모델 성능 평가에 사용되는 주요 지표들을 해설하고, 표준 벤치마크인 VisDrone 데이터셋을 중심으로 여러 Drone-YOLO 모델들의 성능을 비교 분석한다.</p>
<h3>6.1  평가 지표 해설</h3>
<p>객체 탐지 모델의 성능은 정확도, 속도, 효율성 등 여러 측면에서 다각적으로 평가된다.</p>
<ul>
<li>
<p><strong>mAP (mean Average Precision):</strong> 객체 탐지 모델의 정확도를 평가하는 가장 표준적인 지표이다. AP(Average Precision)는 단일 클래스에 대한 모델의 성능을 나타내는 값으로, 재현율(Recall) 변화에 따른 정밀도(Precision) 곡선의 아래 면적을 의미한다. mAP는 데이터셋에 존재하는 모든 클래스에 대한 AP 값의 평균을 계산한 것이다.</p>
</li>
<li>
<p><strong><code>mAP@0.5</code> (또는 <code>mAP50</code>):</strong> 예측된 경계 상자와 실제 경계 상자 간의 IoU(Intersection over Union) 임계값을 0.5로 설정했을 때의 mAP를 의미한다. 즉, IoU가 0.5 이상이면 ’올바른 탐지(True Positive)’로 간주한다. 이는 객체의 존재 유무를 탐지하는 능력을 주로 평가한다.8</p>
</li>
<li>
<p><strong><code>mAP@0.5:0.95</code>:</strong> IoU 임계값을 0.5부터 0.95까지 0.05 간격으로 변화시키면서 각각의 mAP를 측정한 후, 그 값들을 모두 평균낸 것이다. 이는 더 높은 IoU 임계값에서의 성능까지 종합적으로 고려하므로, 객체의 위치를 얼마나 정밀하게 예측하는지(localization accuracy)를 더 엄격하게 평가하는 지표이다.21</p>
</li>
<li>
<p><strong>mAPS (mAP for Small objects):</strong> 전체 객체가 아닌, ‘작은 객체’(일반적으로 32x32 픽셀 미만)에 대해서만 계산한 mAP이다. 드론 영상에서는 작은 객체 탐지가 핵심적인 난제이므로, mAPS는 Drone-YOLO 모델의 성능을 평가하는 데 특히 중요한 지표로 활용된다.32</p>
</li>
<li>
<p><strong>Parameters (M):</strong> 모델이 학습을 통해 최적화해야 하는 가중치(weight)와 편향(bias)의 총 개수를 의미하며, 보통 백만(Million) 단위로 표기한다. 이는 모델의 크기와 직결되며, 메모리 요구량을 가늠하는 척도가 된다.21</p>
</li>
<li>
<p><strong>FLOPs / GFLOPs:</strong> 모델이 한 번의 추론(inference)을 위해 수행해야 하는 부동소수점 연산(Floating Point Operations)의 총 수를 의미한다. 보통 십억(Giga) 단위로 표기한다. 이는 모델의 계산 복잡도를 나타내며, 특정 하드웨어에서의 추론 속도를 예측하는 데 사용된다.21</p>
</li>
<li>
<p><strong>FPS (Frames Per Second):</strong> 1초 동안 처리할 수 있는 이미지 프레임의 수를 의미한다. 이는 모델의 실시간 처리 능력을 직접적으로 보여주는 속도 지표이다. FPS가 높을수록 더 원활한 실시간 탐지가 가능하다.8</p>
</li>
</ul>
<h3>6.2  성능 비교 분석: VisDrone 데이터셋을 중심으로</h3>
<p>아래 표는 표준 벤치마크인 VisDrone2019 데이터셋에서 평가된 여러 객체 탐지 모델들의 성능을 비교한 것이다. 이 표는 Drone-YOLO를 위해 제안된 모델(LACF-YOLO)이 기존의 범용 모델들(Faster R-CNN, YOLOv5, YOLOv8 등)과 비교하여 어떤 이점을 가지는지를 명확하게 보여준다.</p>
<table><thead><tr><th>모델 (Model)</th><th>파라미터 (M)</th><th>GFLOPs</th><th>mAP (%)</th><th>mAPS (%)</th></tr></thead><tbody>
<tr><td>Faster R-CNN</td><td>41.15</td><td>63.25</td><td>50.1</td><td>40.5</td></tr>
<tr><td>YOLOv5</td><td>7.01</td><td>8.20</td><td>54.2</td><td>43.5</td></tr>
<tr><td>YOLOv7</td><td>6.22</td><td>6.88</td><td>69.5</td><td>55.2</td></tr>
<tr><td>YOLOv8s</td><td>11.13</td><td>14.27</td><td>73.3</td><td>55.1</td></tr>
<tr><td>YOLOv8m</td><td>26.21</td><td>39.61</td><td>75.6</td><td>57.8</td></tr>
<tr><td>YOLOv8l</td><td>43.92</td><td>83.17</td><td>77.4</td><td>59.1</td></tr>
<tr><td><strong>LACF-YOLO (Ours)</strong></td><td><strong>7.25</strong></td><td><strong>10.53</strong></td><td><strong>76.8</strong></td><td><strong>58.9</strong></td></tr>
</tbody></table>
<p>표 3: VisDrone2019 데이터셋 기반 모델 성능 비교.32</p>
<p>이 표는 Drone-YOLO 연구의 핵심적인 성과와 방향성을 명확히 보여준다.</p>
<ul>
<li>
<p><strong>정확도와 효율성의 동시 달성:</strong> <strong>LACF-YOLO</strong> 모델은 7.25M의 파라미터와 10.53 GFLOPs의 연산량만으로 76.8%의 mAP와 58.9%의 mAPS를 달성했다. 이는 훨씬 더 큰 모델인 YOLOv8m(26.21M, 39.61 GFLOPs)과 거의 대등한 mAP를 보이면서, 작은 객체 탐지 성능(mAPS)은 오히려 능가하는 결과이다. 또한, 베이스라인으로 볼 수 있는 YOLOv8s(11.13M, 14.27 GFLOPs)와 비교하면, 더 적은 파라미터와 연산량으로 mAP를 3.5%p, mAPS를 3.8%p나 향상시켰다. 이는 IV장에서 논의된 경량화된 어텐션 메커니즘과 효율적인 컨볼루션 레이어 대체 전략이 드론 환경에서 매우 효과적임을 수치적으로 증명하는 것이다.</p>
</li>
<li>
<p><strong>작은 객체 탐지 성능의 중요성:</strong> YOLOv7부터 최신 YOLO 모델들은 mAPS에서 큰 성능 향상을 보였다. 특히 LACF-YOLO는 mAPS에서 58.9%를 기록하며, 이는 자신보다 훨씬 큰 YOLOv8m을 능가하는 수치이다. 이는 Drone-YOLO 연구가 단순히 전체적인 mAP를 높이는 것을 넘어, 드론 환경의 가장 핵심적인 난제인 ‘작은 객체 탐지’ 성능을 집중적으로 개선하고 있음을 보여준다.</p>
</li>
<li>
<p><strong>특화 모델의 압도적 성능:</strong> 또 다른 연구인 <strong>YOLO-DroneMS</strong>는 YOLOv8n을 기반으로 다중 스케일 탐지 능력을 강화하여, VisDrone2019 데이터셋에서 mAP@50을 3.6% 향상시키고, FPS도 78.7에서 83.3으로 끌어올렸다.8 이는 대규모 커널 어텐션(LSKA)과 동적 업샘플링(DySample) 같은 특화 모듈이 드론 영상의 다중 스케일 문제 해결에 효과적임을 보여준다. 한편, 안티드론 시나리오에 특화된</p>
</li>
</ul>
<p><strong>YOLO-FEDER FusionNet</strong>은 일반 YOLOv5l 모델과 비교했을 때, 드론이 배경에 위장된 어려운 상황에서 미탐지율(False Negative Rate, FNR)을 최대 39.1%p 감소시키고, mAP를 최대 62.8%p 증가시키는 등 특정 문제에 대한 압도적인 성능 향상을 보였다.12</p>
<p>결론적으로, 정량적 성능 분석은 Drone-YOLO 연구가 막연한 아이디어의 제시에 그치는 것이 아니라, 객관적인 데이터를 통해 그 효과를 입증하고 있음을 보여준다. 특히, 범용 모델을 그대로 사용하는 것보다 드론 환경의 특성을 고려하여 아키텍처를 정교하게 수정하고 최적화하는 것이 정확도와 효율성 측면에서 월등히 우수한 결과를 낳는다는 사실을 명확히 한다.</p>
<h2>7.  실제 세계에서의 활용: 응용 분야 및 사례 연구</h2>
<p>Drone-YOLO 기술의 발전은 단순히 학술적 성과에 머무르지 않고, 실제 세계의 다양한 산업 현장에서 구체적인 가치를 창출하고 있다. Drone-YOLO의 핵심 기술적 특징인 ’광범위한 공간에서 작고 특정적인 대상을 신속하게 탐지하는 능력’은 여러 응용 분야가 공통적으로 요구하는 핵심 요구사항과 정확히 일치한다. 이는 Drone-YOLO 기술이 특정 분야에 국한되지 않고, ’항공 감시를 통한 정밀 탐지’가 필요한 모든 산업으로 쉽게 확장될 수 있는 높은 범용성을 가지고 있음을 시사한다.</p>
<h3>7.1  수색 및 구조 (Search and Rescue, SAR)</h3>
<p>재난 상황이나 실종자 발생 시, 골든타임 내에 생존자를 찾는 것은 무엇보다 중요하다. 드론은 헬리콥터보다 저렴하고 신속하게 배치될 수 있으며, 구조대원의 접근이 어려운 산악, 해안, 붕괴 현장 등 광범위한 지역을 신속하게 수색하는 데 매우 효과적인 도구이다.5</p>
<p>이러한 SAR 시나리오에서 Drone-YOLO는 핵심적인 역할을 수행한다. 드넓은 수색 영역을 촬영한 고해상도 항공 이미지 속에서, 점처럼 작게 보이는 실종자를 사람의 눈으로 일일이 판독하는 것은 엄청난 시간과 피로를 유발하며 오류 발생 가능성도 높다. Drone-YOLO는 이 과정을 자동화하여, 이미지 속에서 사람으로 추정되는 객체를 실시간으로 탐지하고 그 위치를 구조팀에 즉시 알릴 수 있다.42 특히, VisDrone과 같은 일반 항공 영상 데이터셋으로 사전 학습한 후, Heridal과 같이 SAR 시나리오에 특화된 데이터셋으로 추가 미세 조정(fine-tuning)하는 2단계 전이 학습(transfer learning) 전략을 사용하면, 숲 속에 쓰러져 있거나 잔해에 일부 가려진 사람을 탐지하는 성능을 극대화할 수 있다.42</p>
<p>실제 구축 사례를 보면, Holybro X500과 같은 상용 드론에 NVIDIA Jetson Nano와 같은 소형 온보드 컴퓨터를 탑재하고, 여기에 최적화된 Drone-YOLO 모델을 배포한다. 드론은 자율 비행으로 수색 영역을 촬영하고, 온보드 컴퓨터는 실시간으로 영상 분석을 수행한다. 탐지 결과(경계 상자, GPS 위치 등)는 무선 통신을 통해 지상 통제소(Ground Control Station)의 노트북으로 전송되며, GUI(Graphical User Interface)를 통해 시각적으로 표시되어 구조팀의 신속하고 정확한 의사결정을 돕는다.5</p>
<h3>7.2  정밀 농업 (Precision Agriculture)</h3>
<p>전통적인 농업은 농경지 전체에 동일한 양의 물, 비료, 농약을 살포하는 방식으로 이루어져 자원 낭비와 환경 오염의 원인이 되기도 했다. 정밀 농업은 데이터에 기반하여 작물의 상태를 정밀하게 진단하고, 필요한 곳에 필요한 만큼만 자원을 투입하는 것을 목표로 한다. 드론은 이러한 정밀 농업을 구현하는 데 있어 가장 중요한 데이터 수집 도구이다.7</p>
<p>드론에 장착된 카메라와 Drone-YOLO 기술은 넓은 농경지를 비행하며 촬영한 영상에서 특정 잡초, 병충해에 감염된 작물, 또는 물 부족으로 스트레스를 받는 영역을 정확하게 식별해낸다.6 예를 들어, 콩밭에서 특정 종류의 잡초만을 탐지하도록 학습된 모델은 잡초가 발생한 위치의 GPS 좌표를 맵핑하여, 해당 위치에만 정밀하게 제초제를 살포하는 자동화 시스템과 연동될 수 있다.44 이는 제초제 사용량을 획기적으로 줄여 생산 비용을 절감하고, 환경을 보호하며, 농산물의 품질을 높이는 데 직접적으로 기여한다.43</p>
<p>더 나아가, 강화학습(Reinforcement Learning)과 Drone-YOLO를 결합하는 연구도 활발히 진행되고 있다. 이는 드론이 단순히 정해진 경로를 비행하는 것을 넘어, 실시간 탐지 결과를 바탕으로 스스로 비행 경로를 수정하는 ‘적응형 경로 계획(adaptive path planning)’ 기술이다. 예를 들어, 특정 지역에서 병충해가 발견되면, 드론은 그 주변 지역을 더 낮은 고도에서 더 꼼꼼하게 비행하여 감염 확산 범위를 정밀하게 파악하는 등, 보다 지능적이고 효율적인 데이터 수집을 수행할 수 있게 된다.6</p>
<h3>7.3  지능형 교통 시스템 및 감시</h3>
<p>도시의 교통 혼잡을 완화하고 안전을 증진시키기 위해, 실시간 교통 데이터를 수집하고 분석하는 지능형 교통 시스템(ITS)의 중요성이 날로 커지고 있다. 드론은 고정된 위치의 교통 카메라보다 훨씬 유연하고 넓은 시야를 제공하여, 교차로나 고속도로의 전체적인 교통 흐름을 입체적으로 파악하는 데 매우 유용하다.2</p>
<p>Drone-YOLO는 항공 영상에서 개별 차량들을 탐지하고, 이들을 자동차, 버스, 트럭 등 종류별로 분류한다.45 여기에 다중 객체 추적(Multi-Object Tracking, MOT) 알고리즘을 결합하면, 각 차량의 이동 경로, 속도, 차선 변경 패턴 등을 지속적으로 추적할 수 있다.2 이러한 데이터는 실시간 교통량 분석, 신호 체계 최적화, 사고 발생 감지, 교통 법규 위반(예: 갓길 주행, 불법 유턴) 단속 등에 활용될 수 있다.</p>
<p>교통 분야 외에도, Drone-YOLO는 주요 산업 시설, 국경, 항만 등 넓은 지역에 대한 보안 감시 및 침입 탐지 시스템의 핵심 기술로 활용된다.1 24시간 인력으로 순찰하기 어려운 광범위한 지역을 드론이 주기적으로 자율 비행하며 감시하고, 침입자나 이상 활동을 탐지하면 즉시 보안 센터에 경보를 보내는 방식으로 운용된다. 이는 보안 인력의 한계를 보완하고 감시 시스템의 효율성과 대응 능력을 크게 향상시킨다.</p>
<h2>8.  결론: Drone-YOLO의 미래 전망과 과제</h2>
<h3>8.1  현재 기술 수준 요약 및 종합 평가</h3>
<p>Drone-YOLO는 지난 몇 년간 YOLOv8과 같은 강력한 실시간 객체 탐지 모델을 기반으로, 드론 운용 환경의 특수성을 극복하기 위한 다양한 아키텍처 혁신과 학습 전략을 접목하며 비약적인 발전을 이루었다. 어텐션 메커니즘, 고도화된 특징 융합 네트워크, 그리고 정교한 손실 함수의 도입을 통해, 특히 드론 탐지의 가장 큰 난제였던 작은 객체에 대한 탐지 성능(mAPS)을 크게 향상시켰다.</p>
<p>더욱 중요한 것은, 이러한 성능 향상이 단순히 모델의 크기와 복잡도를 키우는 방식이 아니라, 제한된 온보드 컴퓨팅 자원 내에서의 배포를 전제로 한 경량화 및 효율화 설계와 병행되었다는 점이다. LACF-YOLO와 같은 모델들은 베이스라인 모델보다 더 적은 파라미터와 연산량으로 더 높은 정확도를 달성함으로써, Drone-YOLO 기술이 학술적 연구 단계를 넘어 실제 산업 현장에 적용될 수 있는 성숙도에 이르렀음을 입증했다. 현재 Drone-YOLO는 수색 및 구조, 정밀 농업, 지능형 감시 등 다양한 분야에서 실질적인 가치를 창출하는 핵심 기술로 자리매김하고 있다.</p>
<h3>8.2  향후 연구 방향 및 기술적 과제</h3>
<p>현재의 성과에도 불구하고, Drone-YOLO 기술이 한 단계 더 도약하기 위해서는 여전히 해결해야 할 여러 기술적 과제와 도전적인 연구 방향이 남아있다.</p>
<ul>
<li>
<p><strong>멀티모달 센서 퓨전 (Multi-modal Sensor Fusion):</strong> 현재 대부분의 Drone-YOLO 연구는 가시광선(RGB) 카메라 영상에 집중되어 있다. 그러나 야간, 안개, 폭우와 같은 악천후 조건에서는 가시광선 카메라의 성능이 급격히 저하된다. 이러한 한계를 극복하기 위해, 빛 조건에 관계없이 온도 차이를 감지하는 열화상(Thermal) 카메라, 3차원 공간 정보를 정밀하게 측정하는 라이다(LiDAR), 또는 물질의 고유한 분광 특성을 분석하는 고분광(Hyperspectral) 센서 등 다양한 센서로부터 얻은 데이터를 융합하는 연구가 필수적이다.16 서로 다른 모달리티의 정보를 효과적으로 융합하는 정교한 퓨전 아키텍처 개발은 모든 환경 조건에서 강건하게 작동하는 전천후 드론 탐지 시스템의 핵심이 될 것이다.</p>
</li>
<li>
<p><strong>자율비행 및 능동적 인식 (Autonomous Flight and Active Perception):</strong> 현재의 드론 탐지 시스템은 대부분 사전에 설정된 경로를 비행하며 수동적으로 데이터를 수집하고 분석하는 방식에 머물러 있다. 미래의 Drone-YOLO는 여기서 더 나아가, 탐지된 객체의 정보(위치, 종류, 수 등)를 실시간으로 피드백 받아 드론의 비행 경로와 카메라 각도를 스스로 최적화하는 <strong>능동적 인식(active perception)</strong> 기술과 결합되어야 한다.6 예를 들어, 실종자로 추정되는 객체를 멀리서 희미하게 탐지했다면, 드론이 자율적으로 해당 지점으로 더 가까이 접근하여 더 높은 해상도의 이미지로 재확인하는 식이다. 이는 데이터 수집의 효율성과 탐지의 신뢰도를 극대화하는 차세대 지능형 드론의 핵심 기능이 될 것이다.</p>
</li>
<li>
<p><strong>온디바이스 학습 및 적응 (On-device Learning and Adaptation):</strong> 현재 모델들은 데이터센터에서 대규모 데이터셋으로 학습된 후, 드론에는 고정된 가중치 상태로 배포된다. 그러나 실제 운용 환경은 예측 불가능한 새로운 객체나 배경이 계속해서 나타난다. 이러한 변화에 적응하기 위해, 드론 자체에서 새로운 데이터를 바탕으로 모델을 실시간으로 미세 조정(fine-tuning)하거나 적응시키는 <strong>온디바이스 학습(on-device learning)</strong> 또는 <strong>지속적인 학습(continual learning)</strong> 기술이 요구된다. 이는 클라우드 서버와의 통신 없이도 모델의 성능을 현장에서 지속적으로 개선하고, 특정 임무 환경에 대한 전문성을 높이는 데 기여할 것이다.</p>
</li>
<li>
<p><strong>데이터셋의 한계 극복 및 합성 데이터 활용:</strong> 실제 드론 운용 환경에서 발생할 수 있는 무한에 가까운 시나리오(다양한 기종의 드론, 날씨, 조명, 지역, 객체 등)를 모두 포함하는 실제 데이터셋을 구축하는 것은 물리적으로나 비용적으로 불가능에 가깝다. 이러한 데이터 부족 문제를 해결하기 위해, 사실적인 3D 렌더링 기술을 이용해 고품질의 **합성 데이터(synthetic data)**를 대량으로 생성하고 이를 학습에 활용하는 연구가 더욱 중요해질 것이다.12 또한, 라벨링이 없는 방대한 양의 드론 영상 데이터를 활용하여 모델이 스스로 유용한 특징을 학습하도록 하는</p>
</li>
</ul>
<p><strong>자기 지도 학습(self-supervised learning)</strong> 기법은 라벨링 비용을 획기적으로 줄이면서도 모델의 일반화 성능을 높이는 유망한 대안이 될 수 있다.</p>
<p>이러한 미래 연구 방향들은 Drone-YOLO가 단순한 ‘탐지’ 도구를 넘어, 스스로 ’판단’하고 ’행동’하는 지능형 자율 시스템의 핵심적인 시각 인지 모듈로 진화해 나갈 것임을 예고한다. 이러한 도전 과제들을 성공적으로 해결해 나갈 때, Drone-YOLO 기술은 우리 사회의 안전, 생산성, 효율성을 한 차원 높은 수준으로 끌어올리는 데 핵심적인 역할을 수행하게 될 것이다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>[논문 리뷰] Drone Detection and Tracking with YOLO and a Rule-based Method - Moonlight, https://www.themoonlight.io/ko/review/drone-detection-and-tracking-with-yolo-and-a-rule-based-method</li>
<li>드론 데이터를 활용한 딥러닝 기반 다중 객체 추적, https://public.thinkonweb.com/sites/2024w/media?key=site/2024w/abs/0344-XZZPM.pdf</li>
<li>Real-Time Object Detection Using YOLO-8 Model: A Drone Based Approach, https://www.researchgate.net/publication/390476011_Real-Time_Object_Detection_Using_YOLO-8_Model_A_Drone_Based_Approach</li>
<li>Real-Time Object Detection Using YOLO-8 Model: A Drone- Based Approach, https://jowua.com/wp-content/uploads/2025/04/2025.I1.011.pdf</li>
<li>Real-Time Search and Rescue with Drones: A Deep Learning Approach for Small-Object Detection Based on YOLO - MDPI, https://www.mdpi.com/2504-446X/9/8/514</li>
<li>A drone that learns to efficiently find objects in agricultural fields: from simulation to the real world This research is part of the research program Synergia, funding was obtained from the Dutch Research Council (NWO grant 17626), IMEC-One Planet, and other private parties. The authors have declared that no competing interests exist in the - arXiv, https://arxiv.org/html/2505.09278v1</li>
<li>Real-time object detection and video monitoring in Drone System - IRJET, https://www.irjet.net/archives/V10/i8/IRJET-V10I873.pdf</li>
<li>YOLO-DroneMS: Multi-Scale Object Detection Network for Unmanned Aerial Vehicle (UAV) Images - MDPI, https://www.mdpi.com/2504-446X/8/11/609</li>
<li>Drone Surveillance using YOLOv8 Object Detection - IJNRD, https://ijnrd.org/papers/IJNRD2305829.pdf</li>
<li>arXiv:2406.11641v1 [cs.CV] 17 Jun 2024, https://arxiv.org/pdf/2406.11641</li>
<li>[논문 리뷰] YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection - Moonlight, https://www.themoonlight.io/ko/review/yolo-feder-fusionnet-a-novel-deep-learning-architecture-for-drone-detection</li>
<li>Performance Optimization of YOLO-FEDER FusionNet for Robust Drone Detection in Visually Complex Environments - arXiv, https://arxiv.org/html/2509.14012v1</li>
<li>[2406.11641] YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection - arXiv, https://arxiv.org/abs/2406.11641</li>
<li>Small Object Detection: A Comprehensive Survey on Challenges, Techniques and Real-World Applications - ResearchGate, https://www.researchgate.net/publication/390213601_Small_Object_Detection_A_Comprehensive_Survey_on_Challenges_Techniques_and_Real-World_Applications</li>
<li>Small-Object Detection for UAV-Based Images Using a Distance Metric Method - MDPI, https://www.mdpi.com/2504-446X/6/10/308</li>
<li>Enhancing Small Object Detection in UAVs: Challenges, Methods, and Future Directions - iarjset, https://iarjset.com/wp-content/uploads/2025/04/IARJSET-AITCON-43.pdf</li>
<li>Research on Object Detection and Recognition Method for UAV Aerial Images Based on Improved YOLOv5 - MDPI, https://www.mdpi.com/2504-446X/7/6/402</li>
<li>Improving the Detection of Small Oriented Objects in Aerial Images - arXiv, https://arxiv.org/html/2401.06503v1</li>
<li>AI BASED DRONE DETECTION USING YOLOv8 - RJPN, https://rjpn.org/ijcspub/papers/IJCSP24B1215.pdf</li>
<li>YOLOv8 정리 - RHEE RHEE - 티스토리, https://develop-rhee.tistory.com/1</li>
<li>Ultralytics YOLOv8 살펴보기 - Ultralytics YOLO Docs, https://docs.ultralytics.com/ko/models/yolov8/</li>
<li>YOLO로 이해하는 이미지 객체 감지(2) - YOLO의 역사, https://eair.tistory.com/41</li>
<li>YOLOv8 Architecture: A Detailed Overview | by Vindya Lenawala | Medium, https://medium.com/@vindyalenawala/yolov8-architecture-a-detailed-overview-5e2c371cf82a</li>
<li>Brief summary of YOLOv8 model structure · Issue #189 - GitHub, https://github.com/ultralytics/ultralytics/issues/189</li>
<li>YOLOv8 Architecture Explained! - Abin Timilsina - Medium, https://abintimilsina.medium.com/yolov8-architecture-explained-a5e90a560ce5</li>
<li>YOLOv8 model The backbone of YOLOv8 primarily comprises the C2f module… | Download Scientific Diagram - ResearchGate, https://www.researchgate.net/figure/YOLOv8-model-The-backbone-of-YOLOv8-primarily-comprises-the-C2f-module-inspired-by-the_fig1_371845645</li>
<li>[5] Understanding the YOLOv8 Architecture.pptx, https://www.slideshare.net/slideshow/5-understanding-the-yolov8-architecture-pptx/270472844</li>
<li>YOLO algorithm with hybrid attention feature pyramid network for solder joint defect detection - arXiv, https://arxiv.org/html/2401.01214v1</li>
<li>YOLOv8 vs YOLO11: 상세 기술 비교 - Ultralytics Docs, https://docs.ultralytics.com/ko/compare/yolov8-vs-yolo11/</li>
<li>dongle94.github.io, <a href="https://dongle94.github.io/paper/yolov5-8/#:~:text=YOLOv8%20Architecture,-%EA%B7%B8%EB%A6%BC%2017%EC%9D%80&amp;text=YOLOv8%EC%9D%80%20%EC%95%B5%EC%BB%A4%20%ED%94%84%EB%A6%AC%20%EB%AA%A8%EB%8D%B8,%EC%A0%84%EB%B0%98%EC%A0%81%EC%9D%B8%20%EC%A0%95%ED%99%95%EB%8F%84%EB%A5%BC%20%ED%96%A5%EC%83%81%EC%8B%9C%ED%82%B5%EB%8B%88%EB%8B%A4.">https://dongle94.github.io/paper/yolov5-8/#:~:text=YOLOv8%20Architecture,-%EA%B7%B8%EB%A6%BC%2017%EC%9D%80&amp;text=YOLOv8%EC%9D%80%20%EC%95%B5%EC%BB%A4%20%ED%94%84%EB%A6%AC%20%EB%AA%A8%EB%8D%B8,%EC%A0%84%EB%B0%98%EC%A0%81%EC%9D%B8%20%EC%A0%95%ED%99%95%EB%8F%84%EB%A5%BC%20%ED%96%A5%EC%83%81%EC%8B%9C%ED%82%B5%EB%8B%88%EB%8B%A4.</a></li>
<li>YOLOv8 Architecture - Advanced real time Object Detection, https://yolov8architecture.com/</li>
<li>An Improved YOLOv8-Based Lightweight Attention Mechanism for …, https://www.mdpi.com/2072-4292/17/6/1044</li>
<li>arxiv.org, https://arxiv.org/html/2506.23252v1</li>
<li>Use IoU, GIoU, DIoU, CIoU, SIoU, WIoU, Diag-IoU, MIoU, Alpha-CIoU, Dice… - ResearchGate, https://www.researchgate.net/figure/Use-IoU-GIoU-DIoU-CIoU-SIoU-WIoU-Diag-IoU-MIoU-Alpha-CIoU-Dice-loss-N-CIoU-with_fig3_375996062</li>
<li>Understanding YOLO - Deep Neural Network/ full explanation - YouTube, https://www.youtube.com/watch?v=sH9KLUc76xo</li>
<li>YOLO Loss Function Part 1: SIoU and Focal Loss - LearnOpenCV, https://learnopencv.com/yolo-loss-function-siou-focal-loss/</li>
<li>SIoU Loss: More Powerful Learning for Bounding Box Regression - Semantic Scholar, https://www.semanticscholar.org/paper/SIoU-Loss%3A-More-Powerful-Learning-for-Bounding-Box-Gevorgyan/406b457cf2dc9a62417b05b755ef5434df0a4d33</li>
<li>1 SIoU Loss: More Powerful Learning for Bounding Box Regression Zhora Gevorgyan Abstract The effectiveness of Object Detection, - arXiv, https://arxiv.org/pdf/2205.12740</li>
<li>VisDrone Dataset - Kaggle, https://www.kaggle.com/datasets/banuprasadb/visdrone-dataset</li>
<li>VisDrone 데이터 세트 - Ultralytics YOLO 문서, https://docs.ultralytics.com/ko/datasets/detect/visdrone/</li>
<li>VisDrone-VDT2018: The Vision Meets Drone Video Detection and Tracking Challenge Results - CVF Open Access, https://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Zhu_VisDrone-VDT2018_The_Vision_Meets_Drone_Video_Detection_and_Tracking_Challenge_ECCVW_2018_paper.pdf</li>
<li>(PDF) Real-Time Search and Rescue with Drones: A Deep Learning Approach for Small-Object Detection Based on YOLO - ResearchGate, https://www.researchgate.net/publication/393913821_Real-Time_Search_and_Rescue_with_Drones_A_Deep_Learning_Approach_for_Small-Object_Detection_Based_on_YOLO</li>
<li>Drones in Agriculture: Real-World Applications and Impactful Case Studies, https://kujnsr.com/JNSR/article/view/164</li>
<li>UAV- Based Object Detection and Tracking: Real-World Applications - ijrpr, https://ijrpr.com/uploads/V4ISSUE12/IJRPR20772.pdf</li>
<li>드론 기반 딥러닝을 활용한 교차로 교통 데이터 분석 Crossroads Traffic Data Analysis Using Drone, https://www.j-kosham.or.kr/upload/pdf/KOSHAM-2024-24-1-73.pdf</li>
<li>[보고서]드론 영상에서의 다중 객체 추적을 위한 딥러닝 기반 영상 처리 기술 개발, https://scienceon.kisti.re.kr/srch/selectPORSrchReport.do?cn=TRKO202100009096</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>