<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:YOLOv6 (2022)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>YOLOv6 (2022)</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">객체 탐지 (Object Detection)</a> / <a href="index.html">YOLO 객체 탐지 모델</a> / <span>YOLOv6 (2022)</span></nav>
                </div>
            </header>
            <article>
                <h1>YOLOv6 (2022)</h1>
<h2>1.  YOLOv6의 등장</h2>
<h3>1.1  YOLO 시리즈의 진화와 실시간 객체 탐지의 중요성</h3>
<p>객체 탐지(Object Detection) 기술은 컴퓨터 비전 분야의 핵심 과제 중 하나로, 이미지나 영상 속에서 특정 객체의 위치와 종류를 식별하는 기술이다. 2015년 Joseph Redmon 등에 의해 발표된 YOLO(You Only Look Once)는 이 분야의 패러다임을 근본적으로 바꾸었다.1 이전의 2단계(2-stage) 검출기들이 객체 후보 영역을 먼저 제안하고 각 영역에 대해 분류를 수행하는 복잡한 파이프라인을 가졌던 것과 달리, YOLO는 이미지 전체를 한 번만 보는 단일 단계(1-stage) 접근법을 제시했다.3 이 혁신적인 구조는 객체 탐지를 하나의 회귀 문제로 재정의함으로써, 정확도를 일정 수준 유지하면서도 추론 속도를 획기적으로 향상시켰고, 이는 자율주행, 영상 감시, 로보틱스 등 실시간 처리가 필수적인 응용 분야의 문을 활짝 열었다.</p>
<p>YOLO의 등장은 폭발적인 후속 연구를 촉발했다. YOLOv2는 앵커 박스(anchor box)와 배치 정규화(batch normalization)를 도입하여 성능을 개선했고, YOLOv3는 다중 스케일(multi-scale) 예측을 통해 작은 객체 탐지 능력을 강화했다.5 이후 AlexeyAB가 YOLOv4를 통해 학습 기법의 집합체인 ’Bag of Freebies’와 ‘Bag of Specials’ 개념을 도입했고, Ultralytics는 PyTorch 기반의 YOLOv5를 통해 사용자 친화적인 프레임워크와 지속적인 개선을 제공하며 커뮤니티를 확장했다.2 YOLOX는 앵커-프리(anchor-free) 방식과 개선된 레이블 할당(label assignment) 기법을 도입했으며, YOLOv7은 모델 재매개변수화(re-parameterization)와 효율적인 네트워크 구조 설계를 통해 다시 한번 속도-정확도 균형을 최고 수준으로 끌어올렸다.6 이처럼 YOLO 시리즈는 실시간 객체 탐지 기술의 최전선에서 끊임없이 진화하며 새로운 표준을 제시해왔다.</p>
<h3>1.2  Meituan의 YOLOv6 등장 배경 및 ‘산업용 애플리케이션’ 목표 제시</h3>
<p>이러한 기술적 흐름 속에서 2022년, 중국의 거대 기술 기업 Meituan의 연구팀은 YOLOv6를 공개했다.2 YOLOv6는 Joseph Redmon의 공식 시리즈나 Ultralytics의 YOLOv5/v8과는 독립적으로 개발된 프레임워크로, YOLO 커뮤니티의 발전에 기여하고자 원저자들의 허락 하에 ’YOLO’라는 이름을 사용했다고 밝히고 있다.9 이는 모델의 라이선스(GPL-3.0), 생태계, 커뮤니티 지원 측면에서 다른 YOLO 계열 모델들과 차별화되는 지점이다.11</p>
<p>YOLOv6가 내세운 가장 중요한 목표는 ’산업용 애플리케이션(Industrial Applications)’에 최적화된 객체 탐지기를 만드는 것이었다.6 이는 단순히 COCO 데이터셋과 같은 학술적 벤치마크에서 높은 점수를 기록하는 것을 넘어, 실제 산업 현장에서 마주하는 실용적인 문제들을 해결하는 데 초점을 맞추고 있음을 의미한다. 구체적으로 이는 하드웨어 친화적인 네트워크 설계, NVIDIA TensorRT와 같은 추론 엔진에서의 극대화된 속도, 그리고 리소스가 제한된 엣지 디바이스 배포를 위한 모델 양자화(Quantization) 지원 등을 핵심 고려사항으로 삼았음을 시사한다.4</p>
<p>YOLOv6의 등장은 ’YOLO’라는 명칭이 더 이상 특정 연구 그룹의 직계 후속 모델을 의미하는 것을 넘어, ’고성능 실시간 단일 단계 객체 탐지기’라는 <strong>사실상의 표준(de facto standard)</strong> 아키텍처를 지칭하는 용어로 확장되었음을 보여준다. 여러 기술 주체들이 ’YOLO’라는 브랜드 아래 각자의 철학과 강점을 담은 모델을 출시하며 경쟁하는 구도는 기술 발전의 촉매제 역할을 하는 동시에, 사용자에게는 라이선스, 기술 지원, 생태계 측면에서 더 복잡하고 다각적인 선택지를 제시하고 있다. ’산업용’이라는 목표 역시, 특정 응용 분야에 국한되기보다는 ’배포 준비성(deployment-readiness)’을 최우선으로 하는 설계 철학을 의미하는 동시에, 모델의 실용성을 강조하는 마케팅적 포지셔닝이라는 이중적 의미를 내포한다. 이러한 배경을 이해하는 것은 YOLOv6의 기술적 특징과 그 가치를 정확히 평가하는 데 있어 필수적인 전제 조건이 된다.</p>
<h2>2.  YOLOv6 아키텍처 심층 분석</h2>
<p>YOLOv6의 아키텍처는 ’하드웨어 친화적 설계(hardware-friendly design)’라는 명확한 철학 아래 구축되었다.4 이는 추론 시 하드웨어의 계산 능력을 최대한 효율적으로 활용하여 속도를 극대화하는 것을 목표로 하며, 이를 위해 ’재매개변수화(Reparameterization)’라는 핵심 기술을 아키텍처 전반에 적용했다.</p>
<h3>2.1  설계 철학: 하드웨어 친화적 설계를 위한 재매개변수화(Reparameterization)</h3>
<p>재매개변수화는 네트워크의 훈련 단계와 추론 단계에서 서로 다른 구조를 사용하는 기법이다. 훈련 시에는 스킵 연결(skip connection)이나 1x1 컨볼루션 등 다중 분기(multi-branch) 구조를 활용하여 네트워크의 표현력을 높이고 경사(gradient) 흐름을 원활하게 함으로써 높은 정확도를 달성한다.17 훈련이 완료된 후, 이 복잡한 다중 분기 구조는 수학적으로 등가인 단일 3x3 컨볼루션 레이어와 같은 단순한 구조로 융합(fuse)된다. 이 융합된 구조는 추론 시 메모리 접근 비용을 줄이고 GPU나 CPU의 병렬 연산 유닛을 효율적으로 활용할 수 있어 속도를 크게 향상시킨다. 이 기법은 VGG와 같은 선형 네트워크의 빠른 속도와 ResNet과 같은 복잡한 네트워크의 높은 정확도라는 두 마리 토끼를 모두 잡으려는 시도라고 할 수 있다.17</p>
<p>이러한 설계 철학은 모델 아키텍처가 고정된 것이 아니라, ’훈련’과 ’추론’이라는 서로 다른 요구사항에 맞춰 동적으로 최적화되는 패러다임의 전환을 보여준다. 훈련 시에는 경사 하강법에 유리한 복잡한 구조를, 추론 시에는 하드웨어 가속에 유리한 단순한 구조를 사용하는 이 접근법은 소프트웨어의 컴파일 최적화와 유사한 개념을 신경망 아키텍처에 적용한 것으로, YOLOv6의 핵심적인 혁신이라 할 수 있다.</p>
<h3>2.2  백본(Backbone): EfficientRep 구조와 스케일별 블록 설계</h3>
<p>YOLOv6의 백본은 특징 추출을 담당하는 부분으로, 전체적으로 <strong>EfficientRep 백본</strong>이라 불린다.17 EfficientRep 백본의 가장 큰 특징은 모델의 규모(scale)에 따라 서로 다른 구성 블록을 사용하는 이원화된 설계 전략을 채택했다는 점이다.</p>
<ul>
<li><strong>소형 모델 (YOLOv6-N/S):</strong> RepVGG 스타일의 <strong>RepBlock</strong>을 핵심 구성 요소로 사용한다.16 RepBlock은 훈련 시에는 3x3 컨볼루션과 1x1 컨볼루션, 그리고 신원(identity) 브랜치를 가지지만, 추론 시에는 이들이 모두 단일 3x3 컨볼루션으로 융합된다. 이러한 단일 경로(single-path) 구조는 메모리 사용량이 적고 병렬 처리도가 높아, 상대적으로 모델 크기가 작은 Nano나 Small 버전에서 속도와 정확도 간의 더 나은 균형점을 제공한다.16</li>
<li><strong>대형 모델 (YOLOv6-M/L):</strong> CSPNet(Cross Stage Partial Network) 구조를 재매개변수화 기법과 결합한 <strong>CSPStackRep Block</strong>을 사용한다.17 모델 용량이 커질수록 단일 경로 구조는 파라미터와 연산량이 기하급수적으로 증가하는 문제가 있다. CSPStackRep 블록은 특징 맵을 두 부분으로 나누어 일부만 연산을 수행하고 나중에 합치는 CSP 구조를 통해 이러한 문제를 완화하며, 다중 분기(multi-branch) 구조를 통해 더 효율적으로 모델의 표현력을 확장한다.16</li>
</ul>
<p>이처럼 모델의 크기에 따라 최적의 블록을 다르게 선택하는 것은 단순히 파라미터를 스케일링하는 것을 넘어, 특정 연산량과 메모리 대역폭 조건 하에서 최적의 속도-정확도 균형점을 찾는 ’조건부 아키텍처 설계’에 해당하며, YOLOv6의 하드웨어 친화적 설계 철학을 잘 보여준다.</p>
<h3>2.3  넥(Neck): Rep-PAN, Bi-directional Concatenation (BiC), SimCSPSPPF 블록의 역할</h3>
<p>넥(Neck)은 백본에서 추출된 다양한 스케일의 특징 맵을 집계하고 융합하여 각기 다른 크기의 객체를 탐지할 수 있도록 준비하는 역할을 한다. YOLOv6의 넥은 YOLOv4와 YOLOv5에서 효과가 입증된 PAN(Path Aggregation Network) 토폴로지를 기반으로 하되, 재매개변수화된 블록을 적용하여 <strong>Rep-PAN</strong>으로 명명되었다.2 특히 YOLOv6 v3.0에서는 넥의 성능을 극대화하기 위한 두 가지 중요한 개선이 이루어졌다.</p>
<ul>
<li><strong>Bi-directional Concatenation (BiC) 모듈:</strong> 이는 넥 구조의 핵심적인 개선 사항으로, 더 정확한 지역화 신호(localization signal)를 제공하기 위해 도입되었다.14 BiC 모듈은 기존 PAN 구조에서 한 단계 더 나아가, 인접한 세 개의 레이어(하위, 현재, 상위)로부터 특징 맵을 동시에 연결(concatenate)한다. 이를 통해 저수준(low-level)의 세밀한 공간 정보를 고수준(high-level)의 의미 정보와 효과적으로 융합하며, 특히 작은 객체 탐지 성능을 1.8% 향상시키는 데 크게 기여했다.19 이 모든 과정은 미미한 속도 저하만으로 달성되었다.</li>
<li><strong>SimCSPSPPF 블록:</strong> 기존의 SPPF(Spatial Pyramid Pooling-Fast) 블록을 단순화하고 최적화한 버전이다.6 SPPF는 다양한 크기의 수용장(receptive field)을 효율적으로 확보하기 위해 여러 개의 MaxPool 레이어를 직렬로 연결하는 구조이다. SimCSPSPPF는 이를 CSP 구조와 결합하고 내부 채널 수를 줄임으로써, 표현력은 강화하면서도 연산 효율성을 높였다. 이 블록은 YOLOv6-N/S 모델에 적용되어 성능 향상을 이끌었다.20</li>
</ul>
<p>이러한 개선들은 YOLOv6가 전체 아키텍처를 전면적으로 재설계하기보다, 성능의 병목이 되는 특정 부분(예: 작은 객체 지역화, 특징 융합 효율)을 정밀하게 식별하고 이를 집중적으로 개선하는 실용적인 최적화 전략을 따르고 있음을 보여준다.</p>
<h3>2.4  헤드(Head): Efficient Decoupled Head의 구조와 장점</h3>
<p>헤드(Head)는 넥에서 전달받은 특징 맵을 최종적으로 분석하여 객체의 클래스, 신뢰도, 그리고 경계 상자 좌표를 예측하는 부분이다. YOLOv5와 같은 이전 모델들이 분류와 회귀 작업을 위한 파라미터를 공유하는 결합된 헤드(Coupled Head)를 사용한 반면, YOLOv6는 YOLOX에서 영감을 받은 **분리형 헤드(Decoupled Head)**를 채택했다.2</p>
<p>분리형 헤드는 분류(classification)와 경계 상자 회귀(box regression)를 위한 네트워크 브랜치를 분리하는 구조이다. 이 두 작업은 서로 다른 특성을 가지므로(분류는 특징의 질감과 패턴에, 회귀는 정확한 위치 정보에 집중), 브랜치를 분리하면 각 작업이 독립적으로 최적화될 수 있다. 이는 두 작업 간의 잠재적인 충돌을 완화하고, 결과적으로 연산량을 줄이면서도 정확도를 향상시키는 효과를 가져온다.17</p>
<p>YOLOv6는 여기서 한 걸음 더 나아가, 중간 컨볼루션 레이어 수를 줄이는 등 구조를 더욱 단순화하여 <strong>Efficient Decoupled Head</strong>를 구현했다.16 이를 통해 추론 지연 시간을 최소화하면서 분리형 헤드의 장점을 유지하여, 속도와 정확도 모두를 최적화했다.</p>
<h2>3.  핵심 훈련 전략 및 혁신</h2>
<p>YOLOv6의 높은 성능은 단순히 아키텍처 설계에만 기인하지 않는다. 모델의 잠재력을 최대한 끌어내기 위해 고안된 정교한 훈련 전략들 또한 핵심적인 역할을 한다. 특히 ’앵커 보조 훈련(AAT)’과 ’자기 증류(Self-Distillation)’는 YOLOv6의 철학을 가장 잘 보여주는 혁신적인 기법이다.</p>
<h3>3.1  앵커-프리 패러다임과 앵커 보조 훈련(Anchor-Aided Training, AAT)</h3>
<p>YOLOv6는 기본적으로 추론 속도를 극대화하기 위해 <strong>앵커-프리(anchor-free)</strong> 방식을 채택했다.17 앵커-프리 방식은 사전에 정의된 여러 형태의 앵커 박스를 사용하지 않고, 특징 맵의 각 위치에서 직접 객체의 중심점과 크기를 예측한다. 이는 하이퍼파라미터 튜닝의 복잡성을 줄이고, NMS(Non-Maximum Suppression)와 같은 후처리 과정을 단순화하여 추론 속도를 51%까지 향상시키는 장점이 있다.17</p>
<p>그러나 앵커 박스는 객체의 일반적인 형태와 크기에 대한 강력한 사전 정보(prior)를 제공하기 때문에, 앵커-프리 방식은 훈련 초기 단계가 불안정하거나 작은 객체 탐지에 약점을 보일 수 있다. 실제로 YOLOv6 개발팀은 실험을 통해 앵커-기반 패러다임이 YOLOv6-N 모델에서 추가적인 성능 향상을 가져온다는 사실을 확인했다.20</p>
<p>이러한 딜레마를 해결하기 위해 YOLOv6 v3.0에서는 **앵커 보조 훈련(Anchor-Aided Training, AAT)**이라는 독창적인 전략을 도입했다.19</p>
<ul>
<li><strong>원리:</strong> AAT는 훈련 과정에만 앵커-기반 예측을 수행하는 **보조 브랜치(auxiliary branch)**를 추가한다. 이 보조 브랜치는 앵커를 통해 안정적인 학습 신호를 생성하고, 이 신호는 역전파(backpropagation) 과정에서 주된 앵커-프리 헤드의 학습을 돕는 가이드 역할을 한다.20</li>
<li><strong>추론:</strong> 훈련이 완료된 후, 이 보조 브랜치는 모델에서 완전히 제거된다. 따라서 최종 추론 모델은 순수한 앵커-프리 구조를 가지므로 빠른 속도의 이점을 그대로 누릴 수 있다.19</li>
</ul>
<p>결과적으로 AAT는 추론 비용을 전혀 증가시키지 않으면서 앵커-기반 방식의 학습 안정성과 앵커-프리 방식의 추론 속도라는 두 가지 장점을 모두 취하는 매우 효율적인 전략이다.</p>
<h3>3.2  자기 증류(Self-Distillation) 전략과 분리된 지역화 증류(DLD)</h3>
<p>지식 증류(Knowledge Distillation)는 일반적으로 크고 성능이 좋은 교사(teacher) 모델의 ’지식’을 작고 가벼운 학생(student) 모델에게 전달하여 학생 모델의 성능을 끌어올리는 기법이다. YOLOv6는 별도의 교사 모델 없이, 자기 자신(또는 더 무거운 버전의 자신)을 교사로 사용하는 <strong>자기 증류(self-distillation)</strong> 방식을 채택했다.11</p>
<p>초기 버전에서는 대형 모델(M/L)에만 제한적으로 적용되었으나, YOLOv6 v3.0에서는 소형 모델의 성능을 극대화하기 위해 더욱 정교한 자기 증류 전략이 도입되었다.6</p>
<ul>
<li><strong>새로운 자기 증류 전략:</strong> 분류 손실뿐만 아니라 회귀 손실에 대해서도 증류를 수행한다. 특히, 훈련 과정에 따라 교사의 예측(소프트 레이블)과 실제 정답(하드 레이블)의 중요도를 동적으로 조절한다. 훈련 초기에는 학습하기 쉬운 교사의 소프트 레이블에 더 큰 가중치를 두고, 훈련이 진행되어 학생 모델의 성능이 향상되면 더 정확한 정보인 하드 레이블에 집중하도록 **코사인 가중치 감쇠(cosine weight decay)**를 적용한다.6</li>
<li><strong>분리된 지역화 증류 (Decoupled Localization Distillation, DLD):</strong> DFL(Distribution Focal Loss)은 회귀 정확도를 높이는 데 효과적이지만, 회귀 브랜치에 추가 파라미터를 요구하여 소형 모델의 추론 속도를 저하시키는 부작용이 있다.6 DLD는 이 문제를 해결하기 위해 고안된 기법으로, AAT와 유사한 철학을 공유한다. 훈련 시에만 DFL을 사용하는 무거운(heavy) 보조 회귀 브랜치를 추가하여 증류에 활용하고, 추론 시에는 이 보조 브랜치를 제거하여 경량의 기본 회귀 브랜치만 남긴다. 이를 통해 속도 저하 없이 DFL이 주는 성능 향상의 이점을 얻을 수 있다.20</li>
</ul>
<p>AAT와 DLD에서 공통적으로 나타나는 이러한 접근 방식은 YOLOv6의 핵심 철학인 **‘훈련-추론 비대칭성(Training-Inference Asymmetry)’**을 명확히 보여준다. 이는 ’추론 성능’이라는 최종 목표를 달성하기 위해 훈련 과정을 의도적으로 비대칭적이고 복잡하게 설계하는 전략이다. 훈련 단계에만 존재하는 ’가상의 비계(scaffolding)’를 활용하여 모델의 학습 잠재력을 최대한 끌어올린 뒤, 추론 시에는 이를 모두 제거하여 오직 효율성에만 집중하는 것이다. 이는 향후 모델 설계에서 훈련 과정 자체를 더욱 정교하게 만들어 추론 모델의 효율성과 성능을 동시에 최적화하는 새로운 연구 방향을 제시한다.</p>
<h2>4.  손실 함수 설계 및 수학적 분석</h2>
<p>YOLOv6의 학습 과정은 정교하게 설계된 손실 함수(loss function)에 의해 유도된다. 손실 함수는 모델의 예측이 실제 정답과 얼마나 다른지를 측정하는 척도로, 이 값을 최소화하는 방향으로 모델의 가중치가 업데이트된다. YOLOv6의 전체 손실 함수는 주로 분류 손실과 경계 상자 회귀 손실의 가중 합으로 구성된다.</p>
<h3>4.1  전체 손실 함수 구조</h3>
<p>YOLOv6의 전체 탐지 손실 <span class="math math-inline">L_{det}</span>는 분류 손실 <span class="math math-inline">L_{cls}</span>와 회귀 손실 <span class="math math-inline">L_{reg}</span>의 합으로 표현할 수 있다. 초기 버전에서는 객체의 존재 여부를 판단하는 객체성 손실(objectness loss)도 고려되었으나, 실험 결과 레이블 할당 전략과의 충돌로 인해 성능에 부정적인 영향을 미쳐 최종적으로는 제외되었다.10<br />
<span class="math math-display">
L_{det} = L_{cls} + \lambda L_{reg}
</span><br />
여기서 <span class="math math-inline">\lambda</span>는 회귀 손실의 중요도를 조절하는 하이퍼파라미터이다. 자기 증류 전략이 적용될 경우, 여기에 지식 증류 손실 <span class="math math-inline">L_{KD}</span>가 추가된다.20<br />
<span class="math math-display">
L_{total} = L_{det} + \alpha L_{KD}
</span><br />
여기서 <span class="math math-inline">\alpha</span>는 훈련 과정에 따라 동적으로 변하는 가중치이다.</p>
<h3>4.2  분류 손실: VariFocal Loss (VFL)</h3>
<p>객체 탐지에서는 이미지의 대부분이 배경(background)에 해당하므로, 긍정 샘플(객체)과 부정 샘플(배경) 간에 극심한 클래스 불균형 문제가 발생한다. 이를 해결하기 위해 YOLOv6는 Focal Loss에서 파생된 **VariFocal Loss (VFL)**를 분류 손실로 채택했다.16</p>
<p>VFL의 핵심 아이디어는 긍정 샘플과 부정 샘플을 비대칭적으로 처리하는 것이다.</p>
<ul>
<li><strong>긍정 샘플:</strong> 단순히 객체인지 아닌지만 보는 것이 아니라, 예측된 경계 상자가 실제 정답과 얼마나 잘 겹치는지를 나타내는 IoU(Intersection over Union) 값을 학습 타겟으로 삼는다. IoU가 높은, 즉 ’고품질’의 긍정 샘플에 대해서는 손실을 크게 부여하여 모델이 더 정확한 예측에 집중하도록 유도한다.</li>
<li><strong>부정 샘플:</strong> Focal Loss와 유사하게, 쉽게 분류되는(예측 확률이 0에 가까운) 대다수의 부정 샘플에 대해서는 손실 기여도를 크게 낮춘다. 이를 통해 어렵고 혼동되는 부정 샘플에 대한 학습에 집중할 수 있다.</li>
</ul>
<p>VFL의 수식은 다음과 같이 표현된다.26<br />
<span class="math math-display">
VFL(p, q) = 
\begin{cases}
    -q(q \log(p) + (1-q) \log(1-p)) &amp; \text{if } q &gt; 0 \\
    -\alpha p^{\gamma} \log(1-p) &amp; \text{if } q = 0
\end{cases}
</span><br />
여기서 <span class="math math-inline">p</span>는 모델의 예측 확률이고, <span class="math math-inline">q</span>는 타겟 점수이다. 긍정 샘플의 경우 <span class="math math-inline">q</span>는 실제 IoU 값이며, 부정 샘플의 경우 <span class="math math-inline">q=0</span>이다. <span class="math math-inline">\alpha</span>와 <span class="math math-inline">\gamma</span>는 Focal Loss와 동일한 하이퍼파라미터이다.</p>
<h3>4.3  경계 상자 회귀 손실: SIoU, GIoU, Distribution Focal Loss (DFL)</h3>
<p>경계 상자 회귀 손실은 예측된 박스의 위치와 크기를 실제 정답과 최대한 일치시키는 역할을 한다. YOLOv6는 모델 스케일과 훈련 전략에 따라 여러 회귀 손실 함수를 조합하여 사용한다.</p>
<ul>
<li>
<p><strong>SIoU (SCYLLA-IoU) 및 GIoU (Generalized IoU) Loss:</strong></p>
</li>
<li>
<p>YOLOv6는 기본적으로 IoU 기반 손실 함수를 사용한다. GIoU는 두 박스가 겹치지 않을 때도 손실을 계산할 수 있게 하여 기존 IoU Loss의 한계를 개선했다.16</p>
</li>
<li>
<p>특히 소형 모델(YOLOv6-N/T)에서는 <strong>SIoU Loss</strong>가 주로 사용되었다.16 SIoU는 기존 IoU 기반 손실들이 고려하지 않았던 예측 박스와 실제 박스 중심점 간의</p>
</li>
</ul>
<p><strong>벡터 방향</strong>, 즉 **각도(Angle)**를 손실 계산에 포함시킨 것이 가장 큰 특징이다. 이는 예측 박스가 불필요하게 “방황하는(wandering)” 현상을 줄여주어 수렴을 가속화하고 지역화 정확도를 높이는 데 도움을 준다.27 SIoU 손실은 각도 비용, 거리 비용, 형태 비용, IoU 비용의 네 가지 요소로 구성된다.</p>
<ul>
<li>
<p><strong>Distribution Focal Loss (DFL):</strong></p>
</li>
<li>
<p>대형 모델(YOLOv6-M/L)과 자기 증류 과정에서는 정확도를 더욱 높이기 위해 DFL이 추가적으로 사용되었다.6</p>
</li>
<li>
<p>기존의 회귀 방식이 경계 상자의 좌표(예: x, y, w, h)를 하나의 연속적인 값으로 예측하는 것과 달리, DFL은 각 좌표가 존재할 위치를 **이산적인 확률 분포(discrete probability distribution)**로 모델링한다.26 예를 들어, 객체의 왼쪽 경계 x좌표가 10.4라고 예측하는 대신, 10일 확률이 0.6이고 11일 확률이 0.4라는 식으로 확률 분포를 학습한다. 최종 좌표는 이 분포의 기댓값(expected value)으로 계산된다.</p>
</li>
<li>
<p>이 방식은 실제 경계가 모호하거나 불확실한 객체에 대해 더 유연하고 강건한 예측을 가능하게 하여, 특히 박스 지역화 정확도를 향상시키는 데 효과적이다.10</p>
</li>
</ul>
<h2>5.  성능 평가 및 비교 분석</h2>
<p>YOLOv6의 성능을 객관적으로 평가하기 위해, 표준 벤치마크인 COCO 데이터셋에서의 결과를 분석하고 동시대의 주요 경쟁 모델들과 비교한다. 평가는 주로 정확도 지표인 mAP(mean Average Precision)와 속도 지표인 FPS(Frames Per Second)를 중심으로 이루어지며, 모델의 효율성을 나타내는 파라미터 수와 연산량(FLOPs)도 함께 고려한다.</p>
<h3>5.1  COCO 데이터셋 벤치마크: 모델 스케일별 성능 분석</h3>
<p>YOLOv6는 다양한 하드웨어와 요구사항에 대응하기 위해 N(Nano), S(Small), M(Medium), L(Large) 등 여러 스케일의 모델을 제공한다.18 또한, v3.0에서는 1280x1280의 고해상도 입력을 처리하는 P6 모델(N6, S6, M6, L6)을 추가하여 탐지 성능을 한층 더 끌어올렸다.20 아래 표는 각 모델의 성능 지표를 요약한 것이다.</p>
<p>표에서 명확히 드러나듯이, 모델 스케일이 커짐에 따라 파라미터 수와 FLOPs가 증가하고, 이는 mAP로 대표되는 정확도의 향상으로 이어진다. 반면, 모델의 복잡도 증가는 추론 시간을 늘려 FPS를 감소시킨다. 예를 들어, YOLOv6-N은 1187 FPS라는 경이로운 속도를 보이지만 mAP는 37.5%인 반면, YOLOv6-L6는 57.2%라는 높은 mAP를 달성하지만 속도는 29 FPS로 감소한다. 이처럼 YOLOv6는 명확한 <strong>속도-정확도 트레이드오프(Speed-Accuracy Trade-off)</strong> 관계를 보여주며, 사용자는 이를 바탕으로 자신의 응용 분야에 가장 적합한 모델을 선택할 수 있다.</p>
<p><strong>Table 1: YOLOv6 버전별 COCO 데이터셋 성능 지표 (Tesla T4, TensorRT FP16 기준)</strong></p>
<table><thead><tr><th>모델 (Model)</th><th>입력 크기 (Size)</th><th>mAPval</th><th>FPS (bs=1)</th><th>FPS (bs=32)</th><th>파라미터 (M)</th><th>FLOPs (G)</th></tr></thead><tbody>
<tr><td>YOLOv6-N</td><td>640</td><td>37.5%</td><td>779</td><td>1187</td><td>4.7</td><td>11.4</td></tr>
<tr><td>YOLOv6-S</td><td>640</td><td>45.0%</td><td>339</td><td>484</td><td>18.5</td><td>45.3</td></tr>
<tr><td>YOLOv6-M</td><td>640</td><td>50.0%</td><td>175</td><td>226</td><td>34.9</td><td>85.8</td></tr>
<tr><td>YOLOv6-L</td><td>640</td><td>52.8%</td><td>98</td><td>116</td><td>59.6</td><td>150.7</td></tr>
<tr><td>YOLOv6-N6</td><td>1280</td><td>44.9%</td><td>228</td><td>281</td><td>10.4</td><td>49.8</td></tr>
<tr><td>YOLOv6-S6</td><td>1280</td><td>50.3%</td><td>98</td><td>108</td><td>41.4</td><td>198.0</td></tr>
<tr><td>YOLOv6-M6</td><td>1280</td><td>55.2%</td><td>47</td><td>55</td><td>79.6</td><td>379.5</td></tr>
<tr><td>YOLOv6-L6</td><td>1280</td><td>57.2%</td><td>26</td><td>29</td><td>140.4</td><td>673.4</td></tr>
</tbody></table>
<p>Data sourced from.16</p>
<h3>5.2  주요 YOLO 모델과의 성능 비교</h3>
<p>YOLOv6의 기술적 위치를 파악하기 위해서는 동시대의 주요 경쟁 모델들과의 직접적인 비교가 필수적이다. 아래 표는 동일한 조건(COCO val2017, 640x640 입력, Tesla T4 GPU, TensorRT FP16)에서 YOLOv6 v3.0을 YOLOv5, YOLOX, YOLOv7, YOLOv8과 비교한 결과이다.</p>
<p><strong>Table 2: YOLOv6와 주요 경쟁 모델 성능 비교 분석</strong></p>
<table><thead><tr><th>모델 계열</th><th>모델명</th><th>mAPval</th><th>FPS (bs=32)</th><th>파라미터 (M)</th></tr></thead><tbody>
<tr><td>YOLOv5</td><td>YOLOv5-S</td><td>37.4%</td><td>444</td><td>7.2</td></tr>
<tr><td></td><td>YOLOv5-M</td><td>45.4%</td><td>209</td><td>21.2</td></tr>
<tr><td></td><td>YOLOv5-L</td><td>49.0%</td><td>126</td><td>46.5</td></tr>
<tr><td>YOLOX</td><td>YOLOX-S</td><td>40.5%</td><td>396</td><td>9.0</td></tr>
<tr><td></td><td>YOLOX-M</td><td>46.9%</td><td>179</td><td>25.3</td></tr>
<tr><td></td><td>YOLOX-L</td><td>49.7%</td><td>103</td><td>54.2</td></tr>
<tr><td>YOLOv7</td><td>YOLOv7-tiny</td><td>37.4%</td><td>519</td><td>6.2</td></tr>
<tr><td></td><td>YOLOv7</td><td>51.2%</td><td>122</td><td>36.9</td></tr>
<tr><td>YOLOv8</td><td>YOLOv8-S</td><td>44.9%</td><td>-</td><td>11.2</td></tr>
<tr><td></td><td>YOLOv8-M</td><td>50.2%</td><td>-</td><td>25.9</td></tr>
<tr><td></td><td>YOLOv8-L</td><td>52.9%</td><td>-</td><td>43.7</td></tr>
<tr><td><strong>YOLOv6</strong></td><td><strong>YOLOv6-S</strong></td><td><strong>45.0%</strong></td><td><strong>484</strong></td><td><strong>18.5</strong></td></tr>
<tr><td></td><td><strong>YOLOv6-M</strong></td><td><strong>50.0%</strong></td><td><strong>226</strong></td><td><strong>34.9</strong></td></tr>
<tr><td></td><td><strong>YOLOv6-L</strong></td><td><strong>52.8%</strong></td><td><strong>116</strong></td><td><strong>59.6</strong></td></tr>
</tbody></table>
<p>Data compiled from.10 YOLOv8 FPS data is omitted due to different official testing environments, making direct comparison difficult.</p>
<p>비교 분석 결과, YOLOv6는 여러 스케일에서 강력한 경쟁력을 보여준다.</p>
<ul>
<li><strong>YOLOv6-S</strong>는 45.0%의 mAP와 484 FPS를 기록하여, YOLOv5-S, YOLOX-S, YOLOv8-S 등 동일 체급의 모든 경쟁 모델을 정확도와 속도(처리량) 면에서 모두 능가한다.20</li>
<li><strong>YOLOv6-M</strong>은 50.0%의 mAP로 YOLOv5-M과 YOLOX-M보다 높은 정확도를 보이면서, YOLOv8-M과 유사한 수준의 정확도를 더 빠른 속도로 달성한다.</li>
<li><strong>YOLOv6-L</strong>은 52.8%의 mAP를 기록하여, YOLOv7을 제외한 대부분의 L 스케일 모델보다 높은 정확도를 보인다. YOLOv8-L이 약간 더 높은 52.9%의 mAP를 보이지만, YOLOv6-L은 더 많은 파라미터를 사용한다.</li>
</ul>
<p>종합적으로, YOLOv6는 특히 배치(batch) 처리 시의 처리량(throughput)에서 강점을 보이며, 이는 재매개변수화 아키텍처가 NVIDIA GPU와 TensorRT 환경에 매우 잘 최적화되어 있음을 시사한다. 이는 ’산업용’이라는 목표에 부합하는 중요한 특성이다. 다만, YOLOv8과 비교했을 때 파라미터 효율성 측면에서는 다소 열세를 보이는 경우도 있어, 모델 선택 시 배포 환경과 최우선 순위(최고 속도 vs. 모델 경량화)를 고려해야 한다.</p>
<h2>6.  실용적 고찰 및 평가</h2>
<p>YOLOv6는 ’산업용 프레임워크’라는 명확한 정체성을 가지고 있다. 이는 단순히 학술적인 성능 지표를 넘어, 실제 현장에서의 배포 용이성, 안정성, 효율성 등을 중요한 가치로 삼고 있음을 의미한다. 이러한 관점에서 YOLOv6의 실용적인 장점과 한계를 비판적으로 고찰할 필요가 있다.</p>
<h3>6.1  ‘산업용’ 프레임워크로서의 장점과 한계</h3>
<h4>6.1.1 장점</h4>
<ul>
<li><strong>높은 GPU 추론 속도:</strong> 벤치마크 결과에서 확인된 바와 같이, YOLOv6는 재매개변수화 아키텍처와 TensorRT 최적화를 통해 특히 NVIDIA GPU 환경에서 매우 뛰어난 처리량(throughput)을 보인다.12 이는 다수의 카메라 스트림을 동시에 처리해야 하는 영상 감시 시스템이나, 생산 라인에서 빠른 속도로 제품을 검사해야 하는 스마트 팩토리 등 실시간 처리가 필수적인 산업 현장에 직접적인 이점을 제공한다.</li>
<li><strong>우수한 양자화(Quantization) 지원:</strong> YOLOv6는 모델의 가중치를 8비트 정수(INT8) 등으로 변환하여 모델 크기를 줄이고 추론 속도를 높이는 양자화를 적극적으로 지원한다. PTQ(Post-Training Quantization) 시 발생할 수 있는 정확도 저하 문제를 해결하기 위해 RepOptimizer와 같은 양자화 친화적 학습 기법과 QAT(Quantization-Aware Training)를 도입했다.14 이를 통해 양자화된 YOLOv6-S 모델은 43.3% AP를 유지하면서 869 FPS라는 놀라운 속도를 달성했다.9 이는 리소스가 제한된 엣지 디바이스나 임베디드 시스템에 모델을 배포할 때 결정적인 장점이 된다.</li>
<li><strong>다양한 배포 옵션:</strong> YOLOv6는 ONNX, OpenVINO, TensorRT, MNN 등 다양한 추론 엔진과 백엔드를 지원하여 높은 배포 유연성을 제공한다.11 이를 통해 개발자는 특정 하드웨어(NVIDIA GPU, Intel CPU, ARM 기반 모바일 기기 등)에 맞춰 모델을 최적화하고 손쉽게 배포할 수 있다.</li>
</ul>
<h4>6.1.2 한계 및 비판</h4>
<ul>
<li><strong>제한된 다기능성:</strong> YOLOv6는 객체 탐지에 고도로 특화되어 있다. 반면, 경쟁 모델인 Ultralytics의 YOLOv8은 단일 프레임워크 내에서 객체 탐지, 인스턴스 분할, 자세 추정, 이미지 분류 등 다양한 컴퓨터 비전 작업을 포괄적으로 지원한다.12 YOLOv6도 이후 버전에서 세그멘테이션 모델을 출시했지만 31, YOLOv8만큼 통합된 다기능 생태계를 갖추지는 못했다. 이는 다양한 기능이 필요한 복합적인 애플리케이션을 개발할 때 단점으로 작용할 수 있다.</li>
<li><strong>상대적으로 높은 리소스 사용량:</strong> 벤치마크 분석에서 나타났듯이, YOLOv8과 비교 시 유사한 mAP를 달성하기 위해 YOLOv6가 더 많은 파라미터와 FLOPs를 요구하는 경향이 있다.3 이는 모델의 학습에 더 많은 시간과 컴퓨팅 자원이 필요할 수 있으며, 모델의 경량화 측면에서 약점이 될 수 있다.</li>
<li><strong>작은 생태계와 커뮤니티:</strong> Ultralytics의 YOLOv5/v8은 방대한 사용자 커뮤니티와 풍부한 문서, 튜토리얼, 서드파티 통합 라이브러리를 자랑한다. 이에 비해 YOLOv6는 상대적으로 커뮤니티 규모가 작아 문제 발생 시 해결책을 찾거나 개발 지원을 받기가 더 어려울 수 있다.12</li>
<li><strong>‘산업용’ 목표의 모호성:</strong> YOLOv6의 논문과 공식 자료들은 ’산업용’이라는 목표를 지속적으로 강조하지만, 스마트 팩토리, 자율주행, 품질 검사 등 특정 산업 도메인에서 마주하는 구체적인 문제점이나 요구사항에 대한 심층적인 분석은 부족하다는 비판이 학계 리뷰에서 제기되었다.29 이는 ’산업용’이라는 용어가 기술적 지향점이라기보다는 모델의 실용성을 강조하기 위한 마케팅적 수사에 가깝다는 인상을 줄 수 있다.</li>
</ul>
<h3>6.2  YOLOv6 개발 이력 및 버전별 주요 변경점</h3>
<p>YOLOv6는 정적인 모델이 아니라, 지속적인 연구 개발을 통해 빠르게 진화해 온 능동적인 프로젝트이다. GitHub 릴리즈 노트를 통해 그 발전 과정을 추적하면 모델의 기술적 성숙도와 개발 방향을 명확히 파악할 수 있다.</p>
<p><strong>Table 3: YOLOv6 주요 릴리즈별 핵심 변경 사항 요약</strong></p>
<table><thead><tr><th>릴리즈 버전</th><th>날짜</th><th>주요 변경 사항 및 개선점</th></tr></thead><tbody>
<tr><td><strong>v1.0 (초기 릴리즈)</strong></td><td>2022년 6월</td><td>- EfficientRep 백본, Rep-PAN 넥, Efficient Decoupled Head 아키텍처 최초 공개- 앵커-프리, SimOTA 레이블 할당, SIoU 손실 함수 등 기본 훈련 파이프라인 적용 2</td></tr>
<tr><td><strong>v2.0 / v2.1</strong></td><td>2022년 9월</td><td>- M/L 모델 추가 및 기존 N/T/S 모델 성능 대폭 향상- 자기 증류(Self-distillation), QAT 등 산업 친화적 개선 사항 도입- 훈련 시간 단축 및 PTQ(Post-Training Quantization) 성능 개선 11</td></tr>
<tr><td><strong>v3.0</strong></td><td>2023년 1월</td><td>- BiC 모듈 및 SimCSPSPPF 블록 도입으로 넥 구조 혁신- 앵커 보조 훈련 (AAT) 전략 제안- 소형 모델을 위한 새로운 자기 증류 전략 (DLD 포함) 도입- P6 모델(고해상도 입력) 추가로 SOTA 성능 달성 20</td></tr>
<tr><td><strong>YOLOv6-Lite / MBLA</strong></td><td>2023년 5월</td><td>- 모바일/CPU 환경을 위한 경량 모델 ‘YOLOv6Lite’ 릴리즈- MBLABlock 구조 업데이트를 통해 모바일 성능 최적화 31</td></tr>
<tr><td><strong>YOLOv6-Segmentation</strong></td><td>2023년 9월</td><td>- 인스턴스 분할(Instance Segmentation) 기능 공식 지원- N, S, M, L, X 등 전체 스케일의 세그멘테이션 모델 릴리즈 31</td></tr>
</tbody></table>
<p>Data sourced from official GitHub releases and technical reports.17</p>
<p>이러한 발전 과정은 YOLOv6가 초기에는 속도와 정확도의 균형에 초점을 맞추다가, 점차 훈련 기법 고도화(v3.0), 경량화 및 모바일 지원(Lite), 그리고 기능 확장(Segmentation)으로 나아가고 있음을 보여준다.</p>
<h2>7.  결론</h2>
<h3>7.1  YOLOv6의 기술적 기여와 객체 탐지 분야에서의 위치 요약</h3>
<p>YOLOv6는 객체 탐지 분야에 완전히 새로운 개념을 발명하기보다는, 재매개변수화, 앵커-프리 설계, 지식 증류, 진보된 손실 함수 등 당대의 최신 기술들을 ’산업용’이라는 명확한 목표 아래 성공적으로 융합하고 최적화한 모델로 평가할 수 있다. 그 기술적 기여는 단순히 여러 기법의 조합을 넘어, 실제 배포 환경에서의 효율성을 극대화하는 실용적인 지혜에 있다.</p>
<p>특히 ’훈련-추론 비대칭성’이라는 철학을 극대화한 AAT(앵커 보조 훈련)와 DLD(분리된 지역화 증류) 전략은 주목할 만하다. 이 기법들은 추론 성능에 어떠한 부담도 주지 않으면서 훈련 과정의 복잡성을 높여 모델의 잠재력을 최대한 끌어내는 독창적인 접근법으로, 향후 고효율 모델 설계에 중요한 영감을 제공한다.</p>
<p>성능 측면에서 YOLOv6는 NVIDIA GPU와 TensorRT로 가속되는 환경에서 매우 강력한 처리량(throughput)을 보여주며, 이는 실시간 고성능 컴퓨팅이 요구되는 산업 현장에서의 명확한 강점이다. 비록 파라미터 효율성이나 다기능성 측면에서는 YOLOv8과 같은 경쟁 모델에 일부 우위를 내주기도 하지만, 속도와 정확도의 균형 잡힌 트레이드오프를 제공하며 YOLOv5, YOLOX, YOLOv7 등과 함께 실시간 객체 탐지 분야의 최상위권(State-of-the-Art) 모델 그룹을 확고히 형성하고 있다.</p>
<h3>7.2  향후 발전 방향 및 전망</h3>
<p>YOLOv6는 Meituan이라는 강력한 산업적 배경을 바탕으로 지속적으로 발전할 잠재력을 가지고 있다. 향후 YOLOv6가 나아갈 방향은 몇 가지로 예측해볼 수 있다. 첫째, YOLOv8과 같이 객체 탐지를 넘어 인스턴스 분할, 자세 추정 등 다양한 작업을 지원하는 다기능(multi-task) 프레임워크로의 확장을 가속화할 것이다. 둘째, 더 가볍고 효율적인 모델 아키텍처에 대한 탐구는 계속될 것이며, 특히 모바일 및 엣지 디바이스에서의 성능 최적화가 중요한 과제가 될 것이다. 마지막으로, ’산업용’이라는 정체성을 강화하기 위해 특정 산업 도메인(예: 물류, 소매, 제조)의 데이터와 요구사항에 특화된 모델을 개발하여 실질적인 적용 사례를 축적해 나갈 것으로 예상된다.</p>
<p>한편, 객체 탐지 기술은 YOLO-NAS, YOLOv10 등에서 보이는 바와 같이 NMS-free 설계, 동적 헤드, 실시간 End-to-End 탐지 등 새로운 패러다임으로 빠르게 이동하고 있다.5 YOLOv6가 이러한 최신 연구 동향을 어떻게 흡수하고 자신의 아키텍처에 녹여내어 경쟁력을 유지하고 발전시켜 나갈지 귀추가 주목된다. YOLOv6는 실시간 객체 탐지의 치열한 경쟁 속에서 ’실용성’과 ’효율성’이라는 가치를 중심으로 자신만의 독자적인 영역을 구축해 나가고 있다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>YOLO Object Detection Explained: A Beginner’s Guide - DataCamp, https://www.datacamp.com/blog/yolo-object-detection-explained</li>
<li>What is YOLOv6? The Ultimate Guide. - Roboflow Blog, https://blog.roboflow.com/yolov6/</li>
<li>YOLOv6: next generation object detection - review and comparison - DagsHub, https://dagshub.com/blog/yolov6/</li>
<li>YOLOv6: next-generation object detection - review and comparison | Towards Data Science, https://towardsdatascience.com/yolov6-next-generation-object-detection-review-and-comparison-c02e515dc45f/</li>
<li>YOLO Object Detection Explained: Models, Tools, Use Cases - Lightly, https://www.lightly.ai/blog/yolo</li>
<li>YOLOV6: A SINGLE-STAGE OBJECT DETECTION FRAMEWORK FOR INDUSTRIAL APPLICATIONS - OpenReview, https://openreview.net/pdf?id=7c3ZOKGQ6s</li>
<li>(PDF) Yolov5, Yolo-x, Yolo-r, Yolov7 Performance Comparison: A Survey - ResearchGate, https://www.researchgate.net/publication/363848609_Yolov5_Yolo-x_Yolo-r_Yolov7_Performance_Comparison_A_Survey</li>
<li>MT-YOLOv6: A YOLO-Inspired Object Detection Model Released | ml-news - Wandb, https://wandb.ai/telidavies/ml-news/reports/MT-YOLOv6-A-YOLO-Inspired-Object-Detection-Model-Released–VmlldzoyMjMzMzI5</li>
<li>YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications, https://www.researchgate.net/publication/363363400_YOLOv6_A_Single-Stage_Object_Detection_Framework_for_Industrial_Applications</li>
<li>YOLOv6: A Single-Stage Object Detection Framework for Industrial …, https://arxiv.org/abs/2209.02976</li>
<li>YOLOv6: a single-stage object detection framework dedicated to industrial applications. - GitHub, https://github.com/DeGirum/YOLOv6-dg</li>
<li>YOLOv6-3.0 vs YOLOv8: A Detailed Technical Comparison, https://docs.ultralytics.com/compare/yolov6-vs-yolov8/</li>
<li>YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications | Latest Papers | HyperAI, https://hyper.ai/en/papers/2209.02976</li>
<li>YOLOv6: Single-Stage Object Detection - Viso Suite, https://viso.ai/deep-learning/yolov6/</li>
<li>arXiv:2209.02976v1 [cs.CV] 7 Sep 2022, https://arxiv.org/pdf/2209.02976</li>
<li>[2209.02976] YOLOv6: A Single-Stage Object Detection Framework …, https://ar5iv.labs.arxiv.org/html/2209.02976</li>
<li>YOLOv6 Object Detection – Paper Explanation and Inference - LearnOpenCV, https://learnopencv.com/yolov6-object-detection/</li>
<li>What is YOLOv6? A Deep Insight into the Object Detection Model - arXiv, https://arxiv.org/html/2412.13006v1</li>
<li>Meituan YOLOv6 - Ultralytics YOLO Docs, https://docs.ultralytics.com/models/yolov6/</li>
<li>YOLOv6 v3. 0: A Full-Scale Reloading, https://arxiv.org/abs/2301.05586</li>
<li>[Literature Review] What is YOLOv6? A Deep Insight into the Object Detection Model - Moonlight, https://www.themoonlight.io/en/review/what-is-yolov6-a-deep-insight-into-the-object-detection-model</li>
<li>YOLOv6 : Explanation, Features and Implementation | by Revca - Medium, https://revca-technologies.medium.com/yolov6-explained-in-simple-terms-c46a0248bddc</li>
<li>YOLO11 Anchor-Free Detection: Benefits - Ultralytics, https://www.ultralytics.com/blog/benefits-ultralytics-yolo11-being-anchor-free-detector</li>
<li>YOLOv6 v3. 0: A Full-Scale Reloading, https://arxiv.org/pdf/2301.05586</li>
<li>Brief Review — YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications | by Sik-Ho Tsang, https://sh-tsang.medium.com/brief-review-yolov6-a-single-stage-object-detection-framework-for-industrial-applications-4e2991da53f2</li>
<li>YOLO Loss Function Part 2: GFL and VFL Loss - LearnOpenCV, https://learnopencv.com/yolo-loss-function-gfl-vfl-loss/</li>
<li>YOLO Loss Function Part 1: SIoU and Focal Loss - LearnOpenCV, https://learnopencv.com/yolo-loss-function-siou-focal-loss/</li>
<li>SIoU Loss: More Powerful Learning for Bounding Box Regression - Scylla, https://www.scylla.ai/siou-loss-more-powerful-learning-for-bounding-box-regression/</li>
<li>YOLOV6: A SINGLE-STAGE OBJECT DETECTION FRAMEWORK …, https://openreview.net/forum?id=7c3ZOKGQ6s</li>
<li>Is the explanation of DFL Loss correct? · Issue #21048 - GitHub, https://github.com/ultralytics/ultralytics/issues/21048</li>
<li>Releases · meituan/YOLOv6 - GitHub, https://github.com/meituan/YOLOv6/releases</li>
<li>YOLOv7: Trainable Bag-of-Freebies - Ultralytics YOLO Docs, https://docs.ultralytics.com/models/yolov7/</li>
<li>YOLOv9 vs. YOLOv6-3.0: A Detailed Technical Comparison - Ultralytics YOLO, https://docs.ultralytics.com/compare/yolov9-vs-yolov6/</li>
<li>What is YOLO? The Ultimate Guide [2025] - Roboflow Blog, https://blog.roboflow.com/guide-to-yolo-models/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>