<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:YOLO 객체 탐지 프레임워크에 대한 비판적 고찰</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>YOLO 객체 탐지 프레임워크에 대한 비판적 고찰</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">객체 탐지 (Object Detection)</a> / <a href="index.html">YOLO 객체 탐지 모델</a> / <span>YOLO 객체 탐지 프레임워크에 대한 비판적 고찰</span></nav>
                </div>
            </header>
            <article>
                <h1>YOLO 객체 탐지 프레임워크에 대한 비판적 고찰</h1>
<h2>1.  YOLO에 대하여</h2>
<h3>1.1  객체 탐지의 이분법: 2-단계와 1-단계 탐지기</h3>
<p>객체 탐지(Object Detection) 기술의 발전은 크게 두 가지 패러다임의 경쟁과 상호 보완을 통해 이루어져 왔다. 첫 번째는 2-단계(Two-Stage) 탐지기로, R-CNN(Region-based Convolutional Neural Networks) 계열의 모델들이 대표적이다.1 이 접근법은 먼저 이미지 내에서 객체가 존재할 가능성이 있는 영역(Region Proposal)을 생성하고, 이후 각 후보 영역에 대해 분류(Classification)와 경계 상자 회귀(Bounding Box Regression)를 수행하는 2단계 과정을 거친다.3 이러한 구조적 특성 덕분에 2-단계 탐지기는 높은 정확도를 달성하는 데 유리하지만, 다단계 파이프라인으로 인해 상당한 추론 지연(Latency)이 발생하는 본질적인 한계를 가진다.1</p>
<p>이와 대조적으로 1-단계(Single-Stage) 탐지기는 객체 탐지를 하나의 통합된 회귀 문제(Regression Problem)로 재정의한다.1 YOLO(You Only Look Once)와 SSD(Single Shot MultiBox Detector)가 이 패러다임을 대표하며, 입력 이미지로부터 경계 상자 좌표와 클래스 확률을 단 한 번의 네트워크 통과(Single Pass)로 직접 예측한다.5 이 방식은 2-단계 탐지기에 비해 훨씬 빠른 속도를 제공하지만, 초기 모델들은 정확도 측면에서 열세를 보였다.1 이처럼 속도와 정확도 사이의 상충 관계(Trade-off)는 객체 탐지 분야의 핵심적인 도전 과제이며, YOLO 시리즈의 전체 발전 과정은 이 상충 관계의 최적점을 찾기 위한 끊임없는 노력의 역사라고 할 수 있다.7</p>
<h3>1.2  YOLO의 패러다임 전환: 회귀 문제로서의 객체 탐지</h3>
<p>2015년 YOLO의 등장은 객체 탐지 분야에 혁명적인 변화를 가져왔다. 기존의 연구들이 분류기(Classifier)를 탐지 작업에 맞게 변용하는 방식을 취했던 것과 달리, YOLO는 이미지 픽셀에서 경계 상자 좌표와 클래스 확률로 직접 매핑되는 단일 회귀 문제로 탐지 작업을 재구성했다.9 이 통합된 아키텍처는 전체 탐지 파이프라인이 단일 네트워크로 구성되어 종단간(End-to-End) 최적화가 가능하게 했으며, 이는 전례 없는 수준의 처리 속도를 실현하는 기반이 되었다.9</p>
<p>YOLO의 혁신성은 단순히 속도를 높인 것을 넘어, 실시간 영상 스트림을 처리할 수 있는 현실적인 가능성을 열었다는 데 있다.10 슬라이딩 윈도우(Sliding Window)나 후보 영역 기반 기법과 달리, YOLO는 훈련과 테스트 과정에서 이미지 전체를 한 번에 보기 때문에 객체의 클래스와 외형뿐만 아니라 맥락적 정보(Contextual Information)까지 암묵적으로 인코딩할 수 있었다.10 이러한 접근 방식은 YOLO를 단순한 기술적 개선이 아닌, 객체 탐지에 대한 근본적인 관점의 전환으로 만들었다.</p>
<h3>1.3  안내서의 목적과 구조</h3>
<p>본 안내서는 YOLO 프레임워크의 진화 과정을 각 버전별로 내재된 문제점과 한계를 중심으로 비판적으로 분석하는 것을 목표로 한다. 단순히 성능 향상을 나열하는 대신, 특정 버전의 한계가 어떻게 다음 버전의 혁신을 촉발했는지를 추적하는 데 중점을 둘 것이다. 분석은 최초의 YOLOv1부터 시작하여 YOLOv5, v6, v7, v8을 포함하는 현대의 분화된 개발 환경에 이르기까지 연대순으로 진행된다.8</p>
<p>안내서의 서술 구조는 ’문제-해결’의 순환적 관계로 구성된다. 각 섹션에서는 특정 버전의 아키텍처, 손실 함수, 훈련 전략을 심층적으로 분석하여 그 근본적인 한계를 명확히 규명한다. 이어서 다음 버전에서 이러한 문제들을 해결하기 위해 도입된 핵심적인 기술적 변화들을 상세히 설명한다. 이러한 접근을 통해, YOLO의 발전이 단순한 성능 개선의 연속이 아니라, 이전 세대의 결함을 극복하기 위한 논리적이고 필연적인 과정이었음을 입증하고자 한다.</p>
<h2>2.  YOLOv1 - 기초 아키텍처와 내재적 결함</h2>
<h3>2.1  핵심 개념: 통합된 그리드 기반 회귀 모델</h3>
<p>YOLOv1의 핵심 아이디어는 입력 이미지를 <span class="math math-inline">S \times S</span> 크기의 그리드(일반적으로 <span class="math math-inline">7 \times 7</span>)로 분할하는 것에서 시작한다.3 만약 어떤 객체의 중심점이 특정 그리드 셀 내에 위치한다면, 해당 그리드 셀이 그 객체를 탐지할 책임을 진다.15 각 그리드 셀은 <span class="math math-inline">B</span>개의 경계 상자(일반적으로 2개)와 하나의 클래스 확률 집합을 예측하도록 설계되었다. 결과적으로, 모델의 최종 출력은 <span class="math math-inline">S \times S \times (B \times 5 + C)</span> 차원의 텐서가 된다. 여기서 <span class="math math-inline">5</span>는 경계 상자의 좌표(<span class="math math-inline">x, y, w, h</span>)와 신뢰도 점수(Confidence Score)를, <span class="math math-inline">C</span>는 클래스의 수를 의미한다.3 이처럼 모든 예측을 단일 텐서로 통합하는 방식은 YOLOv1의 경이로운 속도의 원천인 동시에, 이후에 드러날 구조적 한계의 근원이기도 했다. 이 설계는 본질적으로 모델의 출력에 강력한 공간적 제약(Spatial Constraint)을 부과하기 때문이다.</p>
<h3>2.2  아키텍처 분석: GoogLeNet 기반 설계</h3>
<p>YOLOv1의 네트워크 아키텍처는 이미지 분류 모델인 GoogLeNet에서 영감을 받아 설계되었으며, 24개의 합성곱 레이어(Convolutional Layer)와 2개의 완전 연결 레이어(Fully Connected Layer)로 구성된다.3 활성화 함수로는 마지막 레이어를 제외한 대부분의 레이어에서 Leaky ReLU가 사용되었다.19 특히, 최종 예측을 위해 완전 연결 레이어에 의존하는 구조는 몇 가지 중요한 특성을 낳았다. 첫째, 이 구조는 네트워크가 고정된 크기(448x448)의 입력을 요구하게 만들었다.17 둘째, 완전 연결 레이어는 이미지 전체의 특징을 종합하여 전역적인 맥락(Global Context)을 포착하는 역할을 수행했다.10 이 덕분에 YOLOv1은 슬라이딩 윈도우 방식에 비해 배경을 객체로 잘못 탐지하는 오류(Background False Positives)가 현저히 적었지만, 동시에 모델의 공간 추론 능력에 경직성을 부여하는 원인이 되었다. 이 아키텍처적 선택은 후속 버전에서 폐기되는 중요한 결정이었다.</p>
<h3>2.3  합-제곱 오차(SSE) 손실 함수: 결함이 있는 토대</h3>
<p>YOLOv1은 탐지를 회귀 문제로 정의했기 때문에, 손실 함수로 다중 부분으로 구성된 합-제곱 오차(Sum-Squared Error, SSE)를 사용했다.22 이 손실 함수는 크게 위치 손실(Localization Loss), 신뢰도 손실(Confidence Loss), 그리고 분류 손실(Classification Loss)의 세 부분으로 나뉜다.17<br />
$$<br />
\lambda_{coord} \sum_{i=0}<sup>{S^2} \sum_{j=0}</sup>{B} \mathbb{I}_{ij}^{obj} \left[ (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2 \right] \</p>
<ul>
<li>\lambda_{coord} \sum_{i=0}<sup>{S^2} \sum_{j=0}</sup>{B} \mathbb{I}_{ij}^{obj} \left[ (\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2 \right] \</li>
<li>\sum_{i=0}<sup>{S^2} \sum_{j=0}</sup>{B} \mathbb{I}_{ij}^{obj} (C_i - \hat{C}_i)^2 \</li>
<li>\lambda_{noobj} \sum_{i=0}<sup>{S^2} \sum_{j=0}</sup>{B} \mathbb{I}_{ij}^{noobj} (C_i - \hat{C}_i)^2 \</li>
<li>\sum_{i=0}<sup>{S^2} \mathbb{I}_{i}</sup>{obj} \sum_{c \in classes} (p_i(c) - \hat{p}_i(c))^2<br />
$$<br />
여기서 <span class="math math-inline">\mathbb{I}_{ij}^{obj}</span>는 그리드 셀 <span class="math math-inline">i</span>의 <span class="math math-inline">j</span>번째 경계 상자 예측기가 객체를 탐지할 책임이 있을 때 1, 그렇지 않으면 0인 지시 함수이다. 이 손실 함수는 개념적으로는 직관적이지만, 몇 가지 근본적인 문제점을 내포하고 있었다.</li>
</ul>
<ol>
<li><strong>경계 상자 크기에 대한 동일한 가중치 부여:</strong> SSE는 큰 상자의 오차와 작은 상자의 오차를 동일하게 취급한다. 하지만 작은 상자에서의 약간의 픽셀 오차는 IoU(Intersection over Union)에 훨씬 더 큰 영향을 미친다. 너비(<span class="math math-inline">w</span>)와 높이(<span class="math math-inline">h</span>)에 제곱근을 취하는 것은 이 문제를 부분적으로 완화하려는 시도였으나, 근본적인 해결책이 되지는 못했다.17</li>
<li><strong>위치 손실과 분류 손실의 불균형:</strong> 손실 함수는 위치 예측 오차와 분류 오차를 동등하게 고려한다. 그러나 위치 예측은 일반적으로 더 어려운 문제이다. 이 문제를 해결하기 위해, 위치 손실의 중요도를 높이는 가중치 <span class="math math-inline">\lambda_{coord}=5</span>가 도입되었다.23</li>
<li><strong>객체 유무에 따른 불균형:</strong> 대부분의 그리드 셀에는 객체가 포함되어 있지 않다. 이로 인해 ’객체 없음(no-object)’을 예측하는 셀에서 발생하는 손실이 ’객체 있음(object)’을 예측하는 셀의 손실을 압도하여 학습을 불안정하게 만들 수 있다. 이를 해결하기 위해 ‘객체 없음’ 셀의 신뢰도 손실에 <span class="math math-inline">\lambda_{noobj}=0.5</span>라는 낮은 가중치를 부여했다.22</li>
</ol>
<h3>2.4  주요 문제점 분석: 단순함의 대가</h3>
<p>YOLOv1의 문제점들은 버그가 아니라, 속도를 최우선으로 하는 핵심 설계 철학에서 비롯된 필연적인 결과였다. 단일 통합 회귀 네트워크라는 목표는 그리드 구조를 필요로 했고, 이는 다시 공간적 제약과 위치 부정확성 문제를 야기했다. 즉, YOLOv1의 한계는 그 장점과 불가분의 관계에 있었다.</p>
<ul>
<li><strong>심각한 공간적 제약:</strong> 가장 치명적인 결함으로, 각 그리드 셀은 오직 하나의 클래스와 고정된 수(<span class="math math-inline">B=2</span>)의 경계 상자만을 예측할 수 있다. 이로 인해 동일한 그리드 셀에 중심점이 위치하는 여러 객체를 탐지하는 것이 원천적으로 불가능했다. 새 떼와 같이 작은 객체들이 밀집한 경우 탐지 성능이 급격히 저하되는 주된 원인이었다.8 이는 훈련의 실패가 아닌, 아키텍처 자체의 하드웨어적 제약이었다.</li>
<li><strong>낮은 위치 정확도:</strong> 2-단계 탐지기에 비해 현저한 위치 오차를 보였다.9 이는 상대적으로 성긴 <span class="math math-inline">7 \times 7</span> 그리드와, 사전 정보(prior)나 정제 단계 없이 좌표를 직접 회귀하는 방식의 직접적인 결과였다. 이는 덜 제약된(under-constrained) 문제로, 예측의 분산을 높이고 정확도를 떨어뜨렸다.</li>
<li><strong>새로운 종횡비에 대한 취약성:</strong> 모델은 훈련 데이터에 나타난 객체들의 형태를 학습하여 경계 상자 모양을 예측한다. 따라서 훈련 과정에서 보지 못했던 새롭거나 특이한 종횡비(aspect ratio)를 가진 객체에 대해서는 일반화 성능이 떨어졌다.8 이는 형태 예측을 안내할 강력한 사전 정보, 즉 앵커 박스(anchor box)의 부재로 인한 문제였다.</li>
<li><strong>성긴 특징 표현:</strong> 네트워크의 여러 다운샘플링 레이어는 최종적으로 성긴(coarse) 특징 맵을 생성한다. 이로 인해 작은 객체를 정밀하게 위치시키는 데 필요한 세밀한 정보가 손실되었다.8</li>
</ul>
<p>결론적으로, YOLOv1의 한계는 통합 회귀 접근법을 통해 속도를 얻기 위해 지불해야 했던 대가였다. 이는 모든 후속 YOLO 버전의 개발 방향을 설정하는 계기가 되었다. 이후의 버전들은 속도라는 핵심 장점을 희생하지 않으면서 이러한 근본적인 상충 관계를 완화하려는 시도들이었다.</p>
<h2>3.  YOLOv2 &amp; YOLO9000 - “더 좋고, 더 빠르고, 더 강하게” 핵심 결함 해결</h2>
<h3>3.1  개발 동기: YOLOv1의 결함에 대한 체계적 개선</h3>
<p>YOLOv2는 YOLOv1의 명백한 단점들을 체계적으로 해결하는 것을 목표로 개발되었다. 논문에서는 YOLOv1이 후보 영역 기반의 2-단계 탐지기들에 비해 높은 위치 오차와 낮은 재현율(recall)을 보인다는 점을 명시적으로 인정했다.27 따라서 개발의 핵심 목표는 속도를 유지하면서 정확도, 특히 재현율을 대폭 향상시키는 것이었다.30 이 버전은 완전히 새로운 개념을 창조하기보다는, 당시 연구 커뮤니티에서 효과가 입증된 여러 기법들을 실용적으로 조합하고 적용하는 엔지니어링 접근법을 취했다.</p>
<h3>3.2  아키텍처 및 훈련 방식 개선</h3>
<p>YOLOv2는 “더 좋고(Better), 더 빠르고(Faster), 더 강하게(Stronger)“라는 슬로건 아래 다방면에 걸친 개선을 이루었다.</p>
<ul>
<li><strong>Darknet-19 백본:</strong> GoogLeNet 스타일의 기존 아키텍처를 대체하기 위해 19개의 합성곱 레이어로 구성된 새로운 백본 네트워크인 Darknet-19를 도입했다. 이 네트워크는 VGG-16보다 빠르면서도 높은 정확도를 유지하도록 설계되었다.28</li>
<li><strong>배치 정규화(Batch Normalization):</strong> 모든 합성곱 레이어에 배치 정규화를 추가했다. 이는 훈련을 안정화하고 수렴 속도를 높였으며, 과적합을 방지하는 정규화 효과를 제공하여 드롭아웃(dropout)을 제거할 수 있게 했다. 이 개선만으로도 mAP(mean Average Precision)가 2% 향상되었다.25</li>
<li><strong>고해상도 분류기:</strong> 훈련 파이프라인을 수정하여, 탐지 훈련에 앞서 고해상도(448x448) 이미지로 분류기 부분을 먼저 미세 조정(fine-tuning)했다. 이를 통해 네트워크의 필터가 탐지에 사용될 고해상도 입력에 미리 적응하게 하여 mAP를 약 4% 향상시켰다.25</li>
<li><strong>앵커 박스(Anchor Boxes) 도입:</strong> YOLOv1의 낮은 위치 정확도와 재현율 문제를 해결하기 위한 가장 결정적인 변화였다. 경계 상자 좌표를 직접 예측하는 대신, 미리 정의된 다양한 형태와 크기의 사전 상자인 ’앵커 박스’로부터의 오프셋(offset)을 예측하는 방식으로 전환했다.28 이 방식은 위치 예측과 크기/형태 예측을 분리하여 학습 문제를 단순화했고, 그 결과 재현율이 81%에서 88%로 극적으로 향상되었다. 약간의 mAP 하락이 있었지만, 모델의 개선 잠재력을 크게 높였다.29</li>
<li><strong>차원 클러스터링(Dimension Clusters):</strong> 앵커 박스의 형태를 수동으로 지정하는 대신, 훈련 데이터셋의 실제 경계 상자들을 대상으로 k-평균 클러스터링(k-means clustering)을 수행하여 최적의 앵커 형태를 자동으로 찾아냈다. 이때 거리 측정 기준으로 유클리드 거리 대신 <code>d(box, centroid) = 1 - IOU(box, centroid)</code>를 사용하여, 상자 크기와 무관하게 높은 IoU를 달성하는 데 유리한 앵커를 선택할 수 있었다.28</li>
<li><strong>직접 위치 예측(Direct Location Prediction):</strong> 앵커 박스의 위치 예측이 초기 훈련 단계에서 불안정해지는 문제를 해결하기 위해, 예측된 좌표에 제약을 가했다. 로지스틱 활성화 함수(<span class="math math-inline">\sigma</span>)를 사용하여 예측값이 그리드 셀 경계 내(0과 1 사이)에 위치하도록 제한함으로써 훈련 안정성을 높이고 mAP를 약 5% 개선했다.25</li>
<li><strong>세분화된 특징(Fine-Grained Features)을 위한 Passthrough 레이어:</strong> 작은 객체 탐지 성능을 보완하기 위해 ‘Passthrough’ 레이어를 추가했다. 이 레이어는 네트워크의 초기 단계에 있는 더 높은 해상도의 특징 맵(26x26)을 가져와, 깊은 단계의 저해상도 특징 맵(13x13)과 채널 차원에서 연결(concatenate)한다. 이를 통해 최종 탐지 레이어가 더 세밀한 정보를 활용할 수 있게 되어 성능이 약 1% 향상되었다.25</li>
<li><strong>다중-스케일 훈련(Multi-Scale Training):</strong> 완전 연결 레이어를 제거하여 네트워크가 다양한 크기의 이미지를 입력으로 받을 수 있게 되었다. 이를 활용해 훈련 중에 입력 이미지 크기를 주기적으로(예: 320x320에서 608x608까지) 변경했다. 이 방식은 모델이 다양한 스케일의 객체에 대해 강건하게 학습하도록 만들었고, 추론 시에는 속도와 정확도 사이에서 유연한 선택을 가능하게 했다.25</li>
</ul>
<h3>3.3  남겨진 문제점 분석: 여전히 어려운 작은 객체 탐지</h3>
<p>YOLOv2는 앵커 박스 패러다임을 채택하여 위치 정확도와 재현율 측면에서 비약적인 발전을 이루었다. 그러나 작은 객체 탐지 문제는 부분적으로만 해결되었다. Passthrough 레이어는 다중 스케일 특징의 중요성을 입증한 영리한 해결책이었지만, 그 구현은 다소 임시방편적이었다.11</p>
<p>핵심 탐지 메커니즘은 여전히 상대적으로 성긴 13x13 특징 맵에 의존하고 있었기 때문에, 이 해상도는 큰 객체를 탐지하기에는 충분했지만 밀집되어 있거나 아주 작은 객체들을 구분하기에는 공간적 정밀도가 부족했다.20 또한, 객체들이 많이 겹쳐 있는 복잡한 장면에서의 탐지 성능 역시 여전히 한계가 있었다.11 이는 YOLOv2가 해결해야 할 다음 과제가 다중 스케일 특징을 보다 정교하고 체계적으로 통합하는 것임을 시사했다.</p>
<h2>4.  YOLOv3 - 다중-스케일 예측과 정교해진 분류</h2>
<h3>4.1  아키텍처의 도약: 더 깊어진 백본과 특징 피라미드</h3>
<p>YOLOv3는 아키텍처 측면에서 큰 도약을 이루었다. 가장 큰 변화는 훨씬 더 깊고 강력한 새로운 백본 네트워크인 <strong>Darknet-53</strong>의 도입이었다. 이 53개 레이어로 구성된 네트워크는 ResNet과 유사하게 잔차 연결(Residual Connection)을 통합하여, 성능 저하 없이 깊은 네트워크를 구성할 수 있게 했다. 그 결과 Darknet-19보다 훨씬 강력하면서도 ResNet-101이나 ResNet-152보다는 효율적인 백본이 탄생했다.41</p>
<p>YOLOv3를 정의하는 가장 핵심적인 특징은 **다중-스케일 예측(Multi-Scale Prediction)**이다. 이는 특징 피라미드 네트워크(Feature Pyramid Networks, FPN)에서 영감을 받은 개념으로, 네트워크의 서로 다른 깊이에서 <strong>세 가지 다른 스케일</strong>의 특징 맵(각각 13x13, 26x26, 52x52 크기)을 사용하여 예측을 수행한다.6 이 구조는 다양한 크기의 객체를 자연스럽게 처리하도록 설계되었다. 가장 성긴 13x13 그리드는 큰 객체를 탐지하고, 중간 크기의 26x26 그리드는 중간 크기 객체를, 그리고 가장 세밀한 52x52 그리드는 작은 객체를 탐지하는 데 특화되었다.42 이 접근 방식은 YOLOv2의 Passthrough 레이어가 시도했던 작은 객체 탐지 문제를 보다 근본적이고 체계적인 방식으로 해결했다.</p>
<h3>4.2  손실 함수의 진화: 단일 레이블에서 다중 레이블 분류로</h3>
<p>YOLOv3는 분류 방식에도 중요한 변화를 주었다. 기존의 소프트맥스(Softmax) 활성화 함수를 사용하는 대신, 각 클래스에 대해 독립적인 로지스틱 분류기(Logistic Classifier)를 사용하도록 변경했다.20 이에 따라 분류 손실 함수는 각 클래스 예측에 대해</p>
<p><strong>이진 교차 엔트로피(Binary Cross-Entropy, BCE) 손실</strong>을 계산하는 방식으로 바뀌었다.41</p>
<p>이 변화는 매우 중요한 의미를 가진다. 소프트맥스는 각 경계 상자가 단 하나의 클래스에만 속할 수 있다는 상호 배타적인 가정을 전제한다. 하지만 COCO와 같은 복잡한 데이터셋에서는 한 객체가 여러 레이블을 동시에 가질 수 있다(예: ’사람’과 ‘여성’). BCE를 사용한 다중 레이블(Multi-Label) 접근 방식은 이러한 현실 세계의 데이터 분포를 더 유연하고 정확하게 모델링할 수 있게 해 주었다.41</p>
<h3>4.3  문제점 분석과 성능의 상충 관계</h3>
<p>YOLOv3는 작은 객체 탐지 성능을 크게 향상시키고 높은 mAP@0.5 점수를 달성했지만, 새로운 상충 관계와 한계점을 드러냈다.</p>
<ul>
<li><strong>높은 IoU 임계값에서의 정밀도 저하:</strong> YOLOv3는 객체를 ‘찾는’ 능력은 뛰어났지만, 더 엄격한 IoU 임계값(예: mAP@[.5:.95] 또는 mAP@.75)에서는 2-단계 탐지기보다 성능 하락 폭이 더 컸다.11 이는 YOLOv3의 경계 상자 예측이 2-단계 방법만큼 정밀하게 객체에 정렬되지 않았음을 의미한다. 즉, ‘어디에’ 있는지 대략적으로 찾는 것은 잘했지만, ‘얼마나 정확하게’ 경계를 설정하는지에서는 여전히 약점이 있었다. 이 문제는 이후 IoU 기반 손실 함수의 도입을 촉발하는 직접적인 계기가 된다.</li>
<li><strong>계산 복잡도 증가:</strong> Darknet-53이라는 훨씬 깊은 백본을 사용하고 세 가지 다른 스케일에서 예측을 수행함에 따라, 모델의 복잡성, 파라미터 수, 그리고 메모리 요구량이 크게 증가했다.11 이로 인해 이전 버전에 비해 리소스가 제한된 장치에 배포하기가 더 어려워졌다. 이는 정확도를 크게 향상시키기 위해 어느 정도의 속도와 효율성을 희생한 명백한 상충 관계였다.</li>
</ul>
<p>결론적으로, YOLOv3는 YOLO 계열이 단순히 속도에만 초점을 맞춘 틈새 탐지기에서 벗어나, 범용적인 최첨단 탐지기로 성숙했음을 보여주는 이정표였다. FPN과 유사한 다중-스케일 예측의 채택은 단일 특징 맵 접근법이 현실 세계의 다양한 객체 스케일을 처리하기에 근본적으로 불충분하다는 것을 인정한 결과였다. 이로써 YOLO는 ‘빠르기만 한’ 모델에서 ‘빠르면서도 매우 정확한’ 모델로 진화했으며, 수년간 객체 탐지 분야의 지배적인 아키텍처로 자리매김했다.</p>
<h2>5.  YOLOv4 - “Bag of Freebies“와 “Bag of Specials“를 통한 최적화의 체계화</h2>
<h3>5.1  새로운 철학: 종합을 통한 최적의 속도와 정확도</h3>
<p>YOLOv4는 단일의 혁신적인 발명보다는, 기존의 최첨단 기술들을 체계적으로 조합하여 최적의 성능을 달성하는 데 초점을 맞춘 모델이다. 논문의 주된 목표는 단일 GPU 환경에서도 훈련이 가능한 빠르고 정확한 탐지기를 설계하는 것이었다.46 이를 위해 저자들은 수많은 기존 기법들을 실험하고 그 효과를 검증했으며, 이를 두 가지 범주로 나누어 체계화했다: **“Bag of Freebies (BoF)”**와 <strong>“Bag of Specials (BoS)”</strong>.46</p>
<ul>
<li><strong>Bag of Freebies (BoF):</strong> 추론 비용을 증가시키지 않으면서 훈련 과정에만 변화를 주어 모델의 정확도를 향상시키는 기법들을 의미한다. 데이터 증강(Data Augmentation)이 대표적인 예이다.48</li>
<li><strong>Bag of Specials (BoS):</strong> 추론 비용을 약간 증가시키지만 그 대가로 정확도를 크게 향상시키는 플러그인 모듈이나 후처리 기법들을 의미한다.48</li>
</ul>
<p>YOLOv4의 핵심 기여는 이러한 기법들을 최적으로 조합하여 시너지를 극대화하는 방법을 찾아낸 방대한 엔지니어링 및 실험적 노력 그 자체에 있다.46</p>
<h3>5.2  핵심 아키텍처 구성 요소 및 훈련 기법</h3>
<p>YOLOv4는 YOLOv3의 구조를 기반으로 각 구성 요소를 최신 기술로 대폭 업그레이드했다.</p>
<ul>
<li><strong>백본: CSPDarknet53:</strong> 기존의 Darknet-53 백본에 <strong>CSP(Cross Stage Partial) 연결</strong>을 적용했다. CSPNet은 특징 맵을 두 부분으로 나누어 한 부분만 밀집 블록(Dense Block)을 통과시킨 후 다시 합치는 방식으로, 정확도를 유지하면서 계산량을 줄이는 효과적인 구조이다.51</li>
<li><strong>넥(Neck): SPP와 PANet:</strong> 넥 부분은 두 가지 주요 모듈로 강화되었다. 먼저, <strong>SPP(Spatial Pyramid Pooling) 블록</strong>을 추가하여 다양한 크기의 풀링을 통해 수용 영역(Receptive Field)을 크게 늘리고 중요한 맥락적 특징을 분리해냈다. 다음으로, 특징 집계를 위해 **PANet(Path Aggregation Network)**을 사용하여 서로 다른 백본 레벨의 특징들을 효과적으로 융합했다.51</li>
<li><strong>Bag of Freebies (BoF) 주요 기법:</strong></li>
<li><strong>Mosaic 데이터 증강:</strong> YOLOv4의 독창적인 기여 중 하나로, 4개의 훈련 이미지를 하나로 합쳐 새로운 이미지를 만든다. 이를 통해 모델은 다양한 맥락 속에서 객체를 학습하고, 특히 작은 객체 탐지 능력이 향상된다.19</li>
<li><strong>Self-Adversarial Training (SAT):</strong> 모델을 속이는 방향으로 이미지를 교란시켜 훈련 데이터에 추가하는 2단계 증강 기법으로, 모델의 강건성(Robustness)을 높인다.19</li>
<li><strong>CIoU Loss:</strong> 손실 함수에 대한 주요 업그레이드이다. 기존의 IoU 손실을 넘어, 경계 상자의 중심점 거리와 종횡비 차이까지 고려하여 페널티를 부과한다. 이는 YOLOv3에서 문제되었던 높은 IoU 임계값에서의 정밀도 문제를 직접적으로 해결하며, 더 빠른 수렴과 정확한 위치 예측을 가능하게 했다.46</li>
<li><strong>Bag of Specials (BoS) 주요 기법:</strong></li>
<li><strong>Mish 활성화 함수:</strong> Leaky ReLU보다 깊은 네트워크에서 더 나은 성능을 보이는 부드럽고 비단조적인(non-monotonic) 활성화 함수이다.19</li>
<li><strong>DropBlock 정규화:</strong> 드롭아웃의 구조화된 형태로, 특징 맵의 연속적인 영역을 제거하여 모델이 특정 특징에 과도하게 의존하는 것을 방지하고 과적합을 억제한다.46</li>
</ul>
<h3>5.3  문제점 분석: 복잡성의 대가와 프레임워크의 정체</h3>
<p>YOLOv4는 당대 최고의 기술들을 집대성하여 뛰어난 성능을 달성했지만, 이러한 접근 방식은 새로운 종류의 문제점을 야기했다.</p>
<ul>
<li><strong>“키친 싱크” 문제:</strong> 너무나 많은 이질적인 기법들을 결합한 결과, YOLOv4 아키텍처는 매우 복잡하고 거대해졌다. 이는 연구자들이 특정 구성 요소의 영향을 분석하거나, 수정하거나, 확장하기 어렵게 만들었다. 즉, 고도로 최적화된 최종 결과물이기는 했지만, 유연한 연구 프레임워크로서는 한계가 있었다.51</li>
<li><strong>Darknet 프레임워크의 한계:</strong> YOLOv4는 여전히 원저자의 C언어 기반 Darknet 프레임워크 위에서 구현되었다. 이 프레임워크는 성능이 우수했지만, 당시 딥러닝 커뮤니티는 이미 PyTorch나 TensorFlow와 같이 유연하고 사용자 친화적인 프레임워크로 빠르게 통합되고 있었다. Darknet은 이러한 주류 생태계와의 괴리로 인해 새로운 연구자들의 진입 장벽이 높았고, 이는 모델의 채택과 실험 확산을 더디게 만드는 요인이 되었다.53</li>
<li><strong>배포 및 양자화의 어려움:</strong> 모델의 복잡성과 커스텀 프레임워크 의존성은 특정 하드웨어(예: 엣지 디바이스)에 대한 최적화나 양자화(Quantization)와 같은 배포 기술을 적용하는 것을 더 어렵게 만들 수 있었다. 주류 프레임워크가 제공하는 광범위한 도구와 지원 생태계의 부재가 큰 단점이이었다.54</li>
</ul>
<p>결론적으로, YOLOv4는 ‘Darknet 시대’ YOLO의 정점을 상징하는 모델이었다. 급진적인 아키텍처 발명보다는 체계적이고 경험적인 최적화 접근법의 유효성을 입증했다. 그러나, 틈새 프레임워크 내에서 복잡하고 긴밀하게 통합된 시스템을 만들어내는 데 성공함으로써, 역설적으로 사용성, 모듈성, 그리고 주류 딥러닝 생태계와의 통합이라는 새로운 패러다임의 필요성을 부각시켰다. YOLOv4가 해결하지 못한 핵심 문제는 성능이 아니라, 바로 <em>접근성과 미래 확장성</em>이었다. 이는 PyTorch 기반의 YOLOv5가 등장하여 YOLO 계보의 분화를 이끌어낼 수 있는 완벽한 기회를 제공했다.</p>
<h2>6.  거대한 분기: Post-Darknet 시대</h2>
<p>YOLOv4 이후, YOLO의 발전은 단일한 경로를 따르지 않고 여러 갈래로 분화되었다. 이는 특정 생태계와 최종 목표에 최적화된 전문 아키텍처의 등장을 의미하며, 객체 탐지 분야가 성숙기에 접어들었음을 보여주는 현상이다. ‘모두를 위한 하나의 모델’ 접근법은 이제 사용 편의성, 산업적 배포, 최첨단 성능 연구 등 각기 다른 문제에 집중하는 전문화된 모델들로 대체되었다.</p>
<h3>6.1  YOLOv5 &amp; YOLOv8 (Ultralytics): PyTorch로의 전환과 앵커-프리 설계</h3>
<h4>6.1.1  YOLOv5의 엔지니어링 중심 접근</h4>
<p>YOLOv4가 발표된 직후, 원저자가 아닌 Ultralytics사에 의해 공개된 YOLOv5는 처음부터 PyTorch 프레임워크로 구현되었다.53 이 모델은 연구적 혁신보다는 개발자 경험과 실용성에 초점을 맞췄다. 쉬운 훈련 과정, 간편한 배포, 그리고 <strong>AutoAnchor</strong>(훈련 시작 시 데이터셋에 맞는 앵커를 자동으로 재생성하는 기능)와 같은 실용적인 기능들을 제공했다.63 또한, 모델 크기를 n(nano), s(small), m(medium), l(large), x(extra-large)로 세분화하여 사용자가 자신의 요구사항에 맞는 속도-정확도 균형점을 쉽게 선택할 수 있도록 했다.63</p>
<p>하지만 YOLOv5의 출시는 논란을 동반했다. 동료 심사를 거친 논문 없이 공개되었고, YOLOv4와의 초기 성능 비교가 공정하지 않다는 비판을 받았다.11 따라서 YOLOv5의 주된 ’문제’는 기술적 결함이라기보다는, 근본적인 연구적 도약보다는 엔지니어링적 개선에 가까웠음에도 ’YOLOv5’라는 명칭을 사용한 것에 대한 학술적 검증과 명명의 정당성 문제였다.11</p>
<h4>6.1.2  YOLOv8의 앵커-프리 혁명</h4>
<p>Ultralytics가 YOLOv5의 후속으로 출시한 YOLOv8은 더 큰 아키텍처적 변화를 보여준다. 가장 핵심적인 특징은 <strong>앵커-프리(Anchor-Free)</strong> 설계를 채택했다는 점이다.67 앵커-프리 탐지기는 미리 정의된 앵커 박스에 의존하는 대신, 객체의 중심점과 크기를 직접 예측한다. 이를 통해 앵커 박스 설정과 관련된 복잡한 하이퍼파라미터 튜닝 과정을 제거하고, 특이한 형태의 객체에 대한 일반화 성능을 향상시킬 수 있다.71 또한, YOLOv5의 C3 블록을 대체하는 새로운 백본 블록인 C2f를 도입하여 특징 추출 능력을 개선했다.74</p>
<p>그러나 앵커-프리 설계는 새로운 도전 과제를 제시한다. 가장 큰 문제는 <strong>훈련 안정성 확보</strong>와 효과적인 <strong>레이블 할당(Label Assignment) 전략</strong>의 설계이다. 앵커라는 초기 추측값이 없어지면서 모델이 학습해야 할 문제가 더 어려워졌고, 어떤 그리드 셀(또는 특징점)이 어떤 객체를 예측하도록 할당할 것인지 결정하는 과정이 훨씬 더 복잡해졌다. 이는 현대 객체 탐지기 연구의 주요 주제 중 하나이다.</p>
<h3>6.2  YOLOv6 (Meituan): 산업용 배포와 재매개변수화에 집중</h3>
<h4>6.2.1  하드웨어 인식 설계</h4>
<p>중국의 기술 기업 Meituan이 산업 현장 적용을 목표로 개발한 YOLOv6는 **하드웨어 인식 재매개변수화(Hardware-Aware Re-parameterization)**라는 독특한 개념을 도입했다.7 훈련 시에는 풍부한 특징 표현을 학습하기 위해 다중 분기 구조(RepBlock)를 사용하지만, 추론 시에는 이 분기들을 수학적으로 단일 경로의 효율적인 합성곱 레이어로 통합한다. 이 방식은 배포될 하드웨어의 계산 밀도를 극대화하여 추론 속도를 높이는 데 초점을 맞춘다.7 또한, 손실 함수로는 VFL(Varifocal Loss)과 DFL(Distribution Focal Loss)을 사용하여 분류 및 위치 예측 정확도를 개선했다.76</p>
<h4>6.2.2  문제점 분석: 양자화 병목 현상</h4>
<p>재매개변수화 기법은 FP32나 FP16과 같은 고정밀도 연산에서는 매우 빠른 속도를 보이지만, 심각한 문제점을 안고 있다. 바로 <strong>양자화(Quantization)</strong> 이후 성능이 저하된다는 점이다.78 훈련 시의 다중 분기 구조를 추론 시의 단일 구조로 통합하는 과정에서 발생하는 불일치가 양자화 인식 훈련(Quantization-Aware Training, QAT) 과정에서 증폭된다. 이는 최대 처리량이 요구되는 엣지 디바이스 배포 환경에서 INT8 추론이 필수적인 경우가 많다는 점을 고려할 때, 산업용 적용에 있어 치명적인 한계로 작용한다.78</p>
<h3>6.3  YOLOv7 (원저자 그룹): “훈련 가능한 Bag-of-Freebies”</h3>
<h4>6.3.1  아키텍처 및 훈련 혁신</h4>
<p>YOLOv4의 저자 일부가 참여하여 개발한 YOLOv7은 추론 과정이 아닌 훈련 과정 자체를 최적화하는 데 집중한다. 이를 위해 **“훈련 가능한 Bag-of-Freebies (Trainable Bag-of-Freebies)”**라는 새로운 개념을 제시했다.80 주요 혁신은 다음과 같다.</p>
<ol>
<li><strong>E-ELAN (Extended Efficient Layer Aggregation Network):</strong> 기존의 그래디언트 경로를 파괴하지 않으면서 네트워크의 학습 능력을 향상시키도록 설계된 진보된 백본 아키텍처이다.81</li>
<li><strong>연결 기반 모델을 위한 모델 스케일링:</strong> 네트워크의 깊이와 너비를 조화롭게 확장하는 새로운 모델 스케일링 방법을 제안했다.81</li>
<li><strong>보조 헤드를 이용한 깊은 감독(Deep Supervision):</strong> 네트워크 중간층에 보조 예측 헤드(Auxiliary Head)를 두어 추가적인 훈련 신호를 제공한다. 이때 ‘Coarse-to-Fine’ 레이블 할당 전략을 사용하여 주 예측 헤드(Lead Head)와 보조 헤드의 학습을 차별화하고 효율을 높인다.85</li>
</ol>
<h4>6.3.2  문제점 분석: 복잡성의 부담</h4>
<p>YOLOv7의 주된 한계는 추론 성능이 아니라, <strong>훈련 방법론의 극심한 복잡성</strong>에 있다. 계획된 재매개변수화, 복합 스케일링, 그리고 주 헤드와 보조 헤드 간의 복잡한 레이블 할당 로직 등은 훈련 파이프라인을 이해하고, 재현하고, 튜닝하는 것을 매우 어렵게 만든다.82 이러한 복잡성은 해당 기술에 대한 깊은 전문 지식이 없는 팀에게는 모델 채택의 큰 장벽으로 작용할 수 있다.</p>
<h2>7.  종합 및 결론 분석</h2>
<h3>7.1  YOLO 버전별 한계점 비교 분석</h3>
<p>지난 10년간 YOLO 프레임워크는 객체 탐지 분야의 발전을 선도하며 끊임없이 진화해 왔다. 각 버전은 이전 세대의 명백한 문제점을 해결하기 위한 뚜렷한 목표를 가지고 개발되었으며, 그 과정에서 새로운 기술적 과제를 남겼다. 아래 표는 YOLO의 진화 과정에서 나타난 주요 문제점과 이를 해결하기 위한 핵심적인 변화, 그리고 그 결과로 남거나 새로 발생한 한계점을 요약한 것이다.</p>
<p><strong>표 1: YOLO 프레임워크의 문제점과 해결책에 대한 진화적 요약</strong></p>
<table><thead><tr><th>버전</th><th>핵심 아키텍처/방법론적 변화</th><th>이전 버전에서 해결된 주요 문제점</th><th>주요 잔존 또는 신규 한계점</th></tr></thead><tbody>
<tr><td><strong>YOLOv1</strong></td><td>통합 그리드 회귀 (Unified Grid Regression)</td><td>-</td><td>• 심각한 공간적 제약 (밀집/소형 객체 탐지 불가)  • 낮은 재현율 및 위치 정확도  • 새로운 종횡비에 대한 취약성</td></tr>
<tr><td><strong>YOLOv2</strong></td><td>앵커 박스 및 Passthrough 레이어 도입</td><td>• 낮은 재현율 및 위치 정확도 문제 개선</td><td>• 여전히 미흡한 소형 객체 탐지 성능  • 겹치는 객체가 많은 복잡한 장면에 대한 한계</td></tr>
<tr><td><strong>YOLOv3</strong></td><td>FPN 기반 다중-스케일 예측, Darknet-53 백본</td><td>• 소형 객체 탐지 성능 대폭 향상</td><td>• 높은 IoU 임계값에서의 정밀도 저하  • 계산 복잡도 및 메모리 요구량 증가</td></tr>
<tr><td><strong>YOLOv4</strong></td><td>BoF/BoS (CSP, SPP, PAN, CIoU Loss 등)</td><td>• 훈련 방식 최적화 및 높은 IoU 정밀도 개선</td><td>• 아키텍처의 과도한 복잡성  • Darknet 프레임워크 의존성 및 생태계 고립</td></tr>
<tr><td><strong>YOLOv5</strong></td><td>PyTorch 기반 재설계, 모델 스케일링</td><td>• 프레임워크 사용성 및 개발자 경험 개선</td><td>• 학술적 검증 부족 및 명명 논란  • 근본적인 연구적 도약보다는 엔지니어링적 개선</td></tr>
<tr><td><strong>YOLOv6</strong></td><td>하드웨어 인식 재매개변수화</td><td>• 산업용 배포를 위한 추론 속도 극대화</td><td>• 양자화(INT8) 시 성능 저하 문제 발생  • 제한된 범용성 (탐지 외 작업 지원 부족)</td></tr>
<tr><td><strong>YOLOv7</strong></td><td>훈련 가능한 BoF (E-ELAN, 보조 헤드)</td><td>• 훈련 과정 최적화를 통한 SOTA 성능 달성</td><td>• 훈련 파이프라인의 극심한 복잡성  • 재현 및 튜닝의 어려움</td></tr>
<tr><td><strong>YOLOv8</strong></td><td>앵커-프리 설계, C2f 백본</td><td>• 앵커 박스 관련 하이퍼파라미터 제거 및 설계 단순화</td><td>• 훈련 안정성 확보의 어려움  • 효과적인 레이블 할당 전략의 필요성</td></tr>
</tbody></table>
<p>이 표는 YOLO의 발전이 단순히 선형적인 성능 향상이 아니라, 특정 문제를 해결하는 과정에서 새로운 상충 관계가 발생하고, 이를 다시 극복해 나가는 역동적인 순환 과정이었음을 명확히 보여준다.</p>
<h3>7.2  지속적인 도전 과제와 미래 전망</h3>
<p>YOLO 프레임워크는 지난 10년간의 발전을 통해 실시간 객체 탐지 분야에서 괄목할 만한 성과를 이루었지만, 여전히 해결해야 할 근본적인 도전 과제들이 남아있다. 첫째, <strong>속도와 정확도 사이의 근본적인 상충 관계</strong>는 여전히 존재하며, 특정 애플리케이션의 요구사항에 맞는 최적의 균형점을 찾는 것은 계속해서 중요한 연구 주제가 될 것이다.1 둘째, 훈련 데이터와 다른 환경에 대한</p>
<p><strong>도메인 적응 및 강건성</strong> 확보는 자율 주행이나 로보틱스와 같은 실제 환경 적용에 있어 매우 중요하다.11 특히 악천후나 저조도 환경에서의 성능 저하는 여전히 큰 문제이다.</p>
<p>셋째, 극도로 작거나, 가려지거나, 밀집된 객체에 대한 탐지 성능은 모든 버전에서 꾸준히 개선되었음에도 불구하고 여전히 완벽하게 해결되지 않은 난제이다.11 넷째, 앵커-프리나 재매개변수화와 같은 최신 기술들은 새로운 가능성을 열었지만, 동시에</p>
<p><strong>훈련 안정성, 양자화 호환성, 그리고 모델의 해석 가능성</strong>과 같은 새로운 공학적 과제를 제시하고 있다.11</p>
<p>미래의 YOLO는 단일한 발전 경로를 따르기보다는 현재의 분화된 경향을 더욱 가속화할 것으로 예상된다. 엣지 AI, 클라우드 컴퓨팅, 학술 연구 등 각기 다른 목표와 제약 조건을 가진 분야에 특화된 아키텍처들이 계속해서 등장할 것이다. 따라서 ’YOLO’는 더 이상 단일 모델의 이름이 아니라, 속도와 효율성을 중시하는 객체 탐지 설계 철학을 공유하는 하나의 거대한 패밀리를 지칭하는 용어로 자리매김할 것이다. 앞으로의 발전은 단순히 mAP 점수를 높이는 것을 넘어, 특정 응용 분야의 고유한 문제들을 얼마나 효과적으로 해결하는가에 의해 평가될 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Optimizing the Trade-off between Single-Stage and Two-Stage Deep Object Detectors using Image Difficulty Prediction - arXiv, https://arxiv.org/pdf/1803.08707</li>
<li>On the Performance of One-Stage and Two-Stage Object Detectors in Autonomous Vehicles Using Camera Data - MDPI, https://www.mdpi.com/2072-4292/13/1/89</li>
<li>Review on One-Stage Object Detection Based on Deep Learning - EUDL, https://eudl.eu/pdf/10.4108/eai.9-6-2022.174181</li>
<li>MimicDet: Bridging the Gap Between One-Stage and Two-Stage Object Detection - European Computer Vision Association, https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590528.pdf</li>
<li>[1803.08707] Optimizing the Trade-off between Single-Stage and Two-Stage Object Detectors using Image Difficulty Prediction - arXiv, https://arxiv.org/abs/1803.08707</li>
<li>YOLO Algorithm for Object Detection Explained [+Examples] - V7 Labs, https://www.v7labs.com/blog/yolo-object-detection</li>
<li>YOLOV6: A SINGLE-STAGE OBJECT DETECTION FRAMEWORK FOR INDUSTRIAL APPLICATIONS - OpenReview, https://openreview.net/pdf?id=7c3ZOKGQ6s</li>
<li>A Comprehensive Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS - arXiv, https://arxiv.org/html/2304.00501v6</li>
<li>[1506.02640] You Only Look Once: Unified, Real-Time Object Detection - arXiv, https://arxiv.org/abs/1506.02640</li>
<li>You Only Look Once: Unified, Real-Time Object Detection - The Computer Vision Foundation, https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf</li>
<li>YOLOv1 to YOLOv11: A Comprehensive Survey of Real-Time Object Detection Innovations and Challenges - arXiv, https://arxiv.org/html/2508.02067v1</li>
<li>A Decade of You Only Look Once (YOLO) for Object Detection - arXiv, https://arxiv.org/html/2504.18586v1</li>
<li>[2508.02067] YOLOv1 to YOLOv11: A Comprehensive Survey of Real-Time Object Detection Innovations and Challenges - arXiv, https://www.arxiv.org/abs/2508.02067</li>
<li>[2408.09332] YOLOv1 to YOLOv10: The fastest and most accurate real-time object detection systems - arXiv, https://arxiv.org/abs/2408.09332</li>
<li>YOLO Object Detection Explained: A Beginner’s Guide - DataCamp, https://www.datacamp.com/blog/yolo-object-detection-explained</li>
<li>Concept of YOLOv1:The Evolution of Real-Time Object Detection | by Sachinsoni | Medium, https://medium.com/@sachinsoni600517/concept-of-yolov1-the-evolution-of-real-time-object-detection-d773770ef773</li>
<li>YOLO 1 through 5: A complete and detailed overview - Kaggle, https://www.kaggle.com/code/vikramsandu/yolo-1-through-5-a-complete-and-detailed-overview/notebook</li>
<li>A simple explanation to the YOLO Algorithm - Shaunak Halbe, https://shaunak27.github.io/data/yoloblog.pdf</li>
<li>YOLO Evolution: Transforming Object Detection 2015-2024 - Viso Suite, https://viso.ai/computer-vision/yolo-explained/</li>
<li>Review of YOLO: drawback and improvement from v1 to v3 - Tech Hive, https://sheng-fang.github.io/2020-04-25-review_yolo/</li>
<li>YOLOv1 to YOLOv10: The fastest and most accurate real-time object detection systems - arXiv, https://arxiv.org/html/2408.09332v1</li>
<li>The history of YOLO: The origin of the YOLOv1 algorithm | SuperAnnotate, https://www.superannotate.com/blog/yolov1-algorithm</li>
<li>YOLO v1: Part3. Introduction | by Divakar Kapil | Escapades in Machine Learning | Medium, https://medium.com/adventures-with-deep-learning/yolo-v1-part3-78f22bd97de4</li>
<li>Yolo Loss function explanation - Cross Validated - Stack Exchange, https://stats.stackexchange.com/questions/287486/yolo-loss-function-explanation</li>
<li>A Comprehensive Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS - MDPI, https://www.mdpi.com/2504-4990/5/4/83</li>
<li>Deep dive into YOLOv1 - by Abhishek Jain - Medium, https://medium.com/@abhishekjainindore24/deep-dive-into-yolov1-c70111debe60</li>
<li>(PDF) YOLO9000: Better, Faster, Stronger (2017) | Joseph Redmon | 16278 Citations, https://scispace.com/papers/yolo9000-better-faster-stronger-c9oe1ojxvr?citations_page=48</li>
<li>YOLO v2 - Object Detection - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/yolo-v2-object-detection/</li>
<li>A Better, Faster, and Stronger Object Detector (YOLOv2) - PyImageSearch, https://pyimagesearch.com/2022/04/18/a-better-faster-and-stronger-object-detector-yolov2/</li>
<li>[1612.08242] YOLO9000: Better, Faster, Stronger - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/1612.08242</li>
<li>Paper page - YOLO9000: Better, Faster, Stronger - Hugging Face, https://huggingface.co/papers/1612.08242</li>
<li>YOLO9000: Better, Faster, Stronger | Request PDF - ResearchGate, https://www.researchgate.net/publication/311925600_YOLO9000_Better_Faster_Stronger</li>
<li>YOLO-V2 (You Only Look Once), https://www.ijarsct.co.in/Paper17515.pdf</li>
<li>Understanding YOLO and YOLOv2 - Manal El Aidouni, <a href="https://manalelaidouni.github.io/Understanding%20YOLO%20and%20YOLOv2.html">https://manalelaidouni.github.io/Understanding%20YOLO%20and%20YOLOv2.html</a></li>
<li>YOLO Algorithm. YOLO (You Only Look Once) is a… | by Riwaj Neupane | Medium, https://medium.com/@RiwajNeupane/yolo-algorithm-c4f4bb1cdcd8</li>
<li>【ML Paper】YOLOv2 Summary - Zenn, https://zenn.dev/yuto_mo/articles/308f63fb92d7c7</li>
<li>Training YOLO? Select Anchor Boxes Like This - Towards Data Science, https://towardsdatascience.com/training-yolo-select-anchor-boxes-like-this-3226cb8d7f0b/</li>
<li>YOLOv1 to YOLOv10: A comprehensive review of YOLO variants and their application in the agricultural domain - arXiv, https://arxiv.org/html/2406.10139v1</li>
<li>YOLOv2 Object Detection Model - Sipeed Wiki, https://wiki.sipeed.com/ai/en/nn_models/yolov2.html</li>
<li>Detecting very small objects using YOLO v2 in high resolution images - Reddit, https://www.reddit.com/r/computervision/comments/dhh1qp/detecting_very_small_objects_using_yolo_v2_in/</li>
<li>YOLOv3 - Tethys, https://tethys.pnnl.gov/sites/default/files/publications/Redmonetal.pdf</li>
<li>The Ultimate Guide to YOLO3 Architecture - ProjectPro, https://www.projectpro.io/article/yolov3-architecture/836</li>
<li>YOLO - object detection — OpenCV tutorial 2019 documentation - Read the Docs, https://opencv-tutorial.readthedocs.io/en/latest/yolo/yolo.html</li>
<li>Loss Functions in YOLOv3 | Evan’s Personal page, https://evanchien.github.io/Loss-Functions.html</li>
<li>[1804.02767] YOLOv3: An Incremental Improvement - arXiv, https://arxiv.org/abs/1804.02767</li>
<li>[2004.10934] YOLOv4: Optimal Speed and Accuracy of Object Detection - arXiv, https://arxiv.org/abs/2004.10934</li>
<li>(PDF) YOLOv4: Optimal Speed and Accuracy of Object Detection - ResearchGate, https://www.researchgate.net/publication/340883401_YOLOv4_Optimal_Speed_and_Accuracy_of_Object_Detection</li>
<li>Introduction to YOLOv4: Research review - Fritz ai, https://fritz.ai/introduction-to-yolov4-research-review/</li>
<li>Achieving Optimal Speed and Accuracy in Object Detection (YOLOv4) - PyImageSearch, https://pyimagesearch.com/2022/05/16/achieving-optimal-speed-and-accuracy-in-object-detection-yolov4/</li>
<li>YOLOv4 — Version 1: Bag of Freebies - Object Detection - Medium, https://medium.com/visionwizard/yolov4-bag-of-freebies-dc126623fc2d</li>
<li>What is YOLOv4? A Detailed Breakdown. - Roboflow Blog, https://blog.roboflow.com/a-thorough-breakdown-of-yolov4/</li>
<li>Comparing YOLOv3, YOLOv4 and YOLOv5 for Autonomous Landing Spot Detection in Faulty UAVs - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC8778480/</li>
<li>Comparative Performance Analysis of YOLOv4 and YOLOv5 Algorithms on Dangers Objects, https://www.researchgate.net/publication/372589174_Comparative_Performance_Analysis_of_YOLOv4_and_YOLOv5_Algorithms_on_Dangers_Objects</li>
<li>Lightweight Yolov4 Target Detection Algorithm Fused with ECA Mechanism - MDPI, https://www.mdpi.com/2227-9717/10/7/1285</li>
<li>YOLOv4 — Version 2: Bag of Specials - Medium, https://medium.com/visionwizard/yolov4-version-2-bag-of-specials-fab1032b7fa0</li>
<li>From YOLO to YOLOv4. YOLO Object detection explained in a… | by Pedro Azevedo | Medium, https://medium.com/@pedroazevedo6/from-yolo-to-yolov4-3dcba691d96a</li>
<li>YOLOv4: A Breakthrough in Real-Time Object Detection - arXiv, https://arxiv.org/html/2502.04161v1</li>
<li>Review of YOLOv4 Architecture. Paper, Original Code, PyTorch Code - Cenk Bircanoglu, https://cenk-bircanoglu.medium.com/review-of-yolov4-architecture-f488ec32c1c4</li>
<li>YOLOv5: Advancements &amp; Controversies in Computer Vision, https://viso.ai/computer-vision/yolov5-controversy/</li>
<li>YOLOv4 vs. YOLOv5: Compared and Contrasted - Roboflow, https://roboflow.com/compare/yolov4-vs-yolov5</li>
<li>Comparison between structures of YOLOv3, YOLOv4 and YOLOv5. - ResearchGate, https://www.researchgate.net/figure/Comparison-between-structures-of-YOLOv3-YOLOv4-and-YOLOv5_tbl1_357684232</li>
<li>What are the advantages and disadvantages of using YOLOv4 over SSD in object detection tasks? - Massed Compute, <a href="https://massedcompute.com/faq-answers/?question=What+are+the+advantages+and+disadvantages+of+using+YOLOv4+over+SSD+in+object+detection+tasks?">https://massedcompute.com/faq-answers/?question=What%20are%20the%20advantages%20and%20disadvantages%20of%20using%20YOLOv4%20over%20SSD%20in%20object%20detection%20tasks?</a></li>
<li>Ultralytics YOLOv5 Architecture - Ultralytics YOLO Docs, https://docs.ultralytics.com/yolov5/tutorials/architecture_description/</li>
<li>ultralytics/yolov5: YOLOv5 in PyTorch &gt; ONNX &gt; CoreML &gt; TFLite - GitHub, https://github.com/ultralytics/yolov5</li>
<li>Comprehensive Guide to Ultralytics YOLOv5, https://docs.ultralytics.com/yolov5/</li>
<li>Brief Review: YOLOv5 for Object Detection | by Sik-Ho Tsang - Medium, https://sh-tsang.medium.com/brief-review-yolov5-for-object-detection-84cc6c6a0e3a</li>
<li>What is YOLOv8: An In-Depth Exploration of the Internal Features of the Next-Generation Object Detector - arXiv, https://arxiv.org/html/2408.15857v1</li>
<li>Ultralytics YOLOv5, https://docs.ultralytics.com/models/yolov5/</li>
<li>YOLOv8: State-of-the-Art Computer Vision Model, https://yolov8.com/</li>
<li>Explore Ultralytics YOLOv8, https://docs.ultralytics.com/models/yolov8/</li>
<li>CenterNet: Objects as Points – Anchor Free Object Detection Explained - LearnOpenCV, https://learnopencv.com/centernet-anchor-free-object-detection-explained/</li>
<li>Anchor-Free Object Detection - Ultralytics, https://www.ultralytics.com/glossary/anchor-free-detectors</li>
<li>What are anchor-free Object Detectors? - LearnOpenCV - Quora, https://learnopencv.quora.com/What-are-anchor-free-Object-Detectors</li>
<li>YOLOv8 Architecture &amp; C2F Backbone | Complete Breakdown - YouTube, https://www.youtube.com/watch?v=DeW9BQGB7SM</li>
<li>Meituan YOLOv6 - Ultralytics YOLO Docs, https://docs.ultralytics.com/models/yolov6/</li>
<li>What is YOLOv6? A Deep Insight into the Object Detection Model - arXiv, https://arxiv.org/html/2412.13006v1</li>
<li>YOLOv6-3.0 vs YOLOv8: A Detailed Technical Comparison - Ultralytics Docs, https://docs.ultralytics.com/compare/yolov6-vs-yolov8/</li>
<li>[22.09] YOLOv6 - DOCSAID, https://docsaid.org/en/papers/object-detection/yolov6/</li>
<li>YOLOv6/tools/qat/README.md at main - GitHub, https://github.com/meituan/YOLOv6/blob/main/tools/qat/README.md</li>
<li>WongKinYiu/yolov7: Implementation of paper - YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - GitHub, https://github.com/WongKinYiu/yolov7</li>
<li>YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_YOLOv7_Trainable_Bag-of-Freebies_Sets_New_State-of-the-Art_for_Real-Time_Object_Detectors_CVPR_2023_paper.pdf</li>
<li>YOLOv7 explanation and implementation from scratch - Kaggle, https://www.kaggle.com/code/vishakkbhat/yolov7-explanation-and-implementation-from-scratch</li>
<li>YOLOv7: A Powerful Object Detection Algorithm - Viso Suite, https://viso.ai/deep-learning/yolov7-guide/</li>
<li>What is YOLOv7? A Complete Guide. - Roboflow Blog, https://blog.roboflow.com/yolov7-breakdown/</li>
<li>YOLOv7 Paper Explanation: Object Detection and YOLOv7 Pose, https://learnopencv.com/yolov7-object-detection-paper-explanation-and-inference/</li>
<li>The YOLO Framework: A Comprehensive Review of Evolution, Applications, and Benchmarks in Object Detection - MDPI, https://www.mdpi.com/2073-431X/13/12/336</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>