<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:YOLOv1 (2015)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>YOLOv1 (2015)</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">객체 탐지 (Object Detection)</a> / <a href="index.html">YOLO 객체 탐지 모델</a> / <span>YOLOv1 (2015)</span></nav>
                </div>
            </header>
            <article>
                <h1>YOLOv1 (2015)</h1>
<h2>1.  객체 탐지의 패러다임 전환</h2>
<h3>1.1  YOLOv1 이전의 시대: 2-Stage Detectors의 지배</h3>
<p>2015년 YOLOv1이 등장하기 이전, 객체 탐지 분야는 R-CNN(Regions with CNN features) 계열의 2-stage detector들이 지배하고 있었다.1 R-CNN, Fast R-CNN, Faster R-CNN으로 이어지는 이 계보는 탐지 과정을 두 개의 명확한 단계로 분리하는 접근법을 취했다. 첫 번째는 ‘후보 영역 제안(Region Proposal)’ 단계이며, 두 번째는 제안된 영역을 ’분류(Classification)’하는 단계이다.3</p>
<p>초기 R-CNN과 Fast R-CNN은 Selective Search와 같은 알고리즘을 사용하여 이미지 한 장당 약 2000개의 객체가 있을 법한 후보 영역을 생성했다.4 이 방식은 계산적으로 매우 비효율적이었으며, 각 후보 영역에 대해 독립적으로 컨볼루션 신경망(CNN) 특징을 추출하고 분류를 수행해야 했다. 이로 인해 R-CNN은 이미지 한 장을 처리하는 데 47초라는 막대한 시간이 소요되었고, Fast R-CNN은 이를 2초로 단축했지만 여전히 실시간 응용에는 한계가 명확했다.4 Faster R-CNN은 후보 영역 제안 과정을 학습 가능한 Region Proposal Network(RPN)으로 대체하여 속도를 5-7 FPS(Frames Per Second)까지 끌어올렸으나, 복잡한 다단계 파이프라인 구조는 종단간(end-to-end) 최적화를 어렵게 만들었고 실시간 탐지라는 목표와는 여전히 거리가 있었다.6</p>
<h3>1.2  YOLOv1의 혁신: 통합된 회귀 문제로의 재정의</h3>
<p>이러한 배경 속에서 Joseph Redmon 등이 발표한 YOLOv1은 객체 탐지에 대한 근본적인 관점을 바꾸는 혁신을 제시했다.10 YOLO(You Only Look Once)라는 이름이 암시하듯, 이 모델은 분리된 파이프라인을 단일 신경망으로 통합하고 객체 탐지를 “한 번만 보는” 단일 회귀 문제(single regression problem)로 재정의했다.8</p>
<p>YOLOv1의 접근법은 이미지 전체를 단 한 번의 순전파(single forward pass) 과정으로 처리하여 바운딩 박스(bounding box)의 좌표와 해당 박스의 클래스 확률을 동시에 직접 예측하는 것이다.4 이 통합된 아키텍처는 이전 모델들의 병목 현상을 원천적으로 제거함으로써 속도를 수십 배 향상시켰다. 기본 YOLOv1 모델은 45 FPS, 더 가벼운 Fast YOLO 모델은 155 FPS라는 경이적인 처리 속도를 달성하며 진정한 의미의 실시간 객체 탐지 시대의 서막을 열었다.10</p>
<p>이러한 변화는 단순한 속도 개선을 넘어, 객체 탐지 분야의 연구 방향에 철학적인 분기점을 제시했다. R-CNN 계열이 정확도를 위해 복잡성과 계산량을 감수하는 ’정밀 탐지’의 길을 대표했다면, YOLOv1은 속도와 효율성을 최우선으로 하여 ’실용적 실시간 탐지’라는 새로운 길을 개척했다. 이로 인해 이전에는 상상하기 어려웠던 자율 주행, 실시간 영상 감시, 로보틱스 등 다양한 응용 분야의 가능성이 폭발적으로 증가하게 되었다.6</p>
<h2>2.  YOLOv1의 통합 아키텍처</h2>
<h3>2.1  그리드 셀 시스템: 공간 분할 기반 예측</h3>
<p>YOLOv1의 핵심 아이디어는 입력 이미지를 일정한 크기의 그리드(grid)로 분할하여 문제를 단순화하는 것이다. 원본 논문에서는 이미지를 <span class="math math-inline">S \times S</span> 크기, 구체적으로 <span class="math math-inline">7 \times 7</span> 그리드로 나눈다.8 탐지 과정에서 특정 객체의 중심점(center)이 어떤 그리드 셀(grid cell) 내에 위치하게 되면, 바로 그 그리드 셀이 해당 객체를 탐지할 “책임(responsible)“을 맡게 된다.8 이 규칙은 하나의 객체를 여러 그리드 셀이 중복으로 탐지하는 것을 방지하는 강력한 공간적 제약(spatial constraint)으로 작용한다.18</p>
<p>각 그리드 셀은 두 가지 주요 정보를 예측한다. 첫째, <span class="math math-inline">B</span>개(논문에서는 <span class="math math-inline">B=2</span>)의 바운딩 박스와 각 박스에 대한 신뢰도 점수(confidence score). 둘째, <span class="math math-inline">C</span>개(PASCAL VOC 데이터셋 기준 20개)의 클래스에 대한 조건부 클래스 확률(conditional class probabilities)이다.14</p>
<p>이 그리드 셀 시스템은 YOLOv1의 놀라운 속도와 구조적 단순성의 원천이다. 후보 영역 제안이라는 복잡한 단계를 완전히 제거했기 때문이다. 하지만 이는 동시에 모델의 가장 큰 한계점을 낳는 양날의 검이기도 하다. 거친 <span class="math math-inline">7 \times 7</span> 그리드는 공간을 이산적으로 양자화(spatial quantization)하는데, 이로 인해 여러 개의 작은 객체들이 하나의 그리드 셀에 포함될 가능성이 높아진다. 각 셀은 단 하나의 클래스 집합만 예측할 수 있으므로, 이 경우 네트워크는 가장 두드러지는 객체 하나만을 학습하거나 혹은 여러 객체의 평균적인 특징을 학습하게 되어 나머지 객체들은 놓치게 된다. 이는 단순히 탐지를 못하는 수준을 넘어, 양자화된 공간 내에서 여러 객체를 동시에 표현할 수 없는 구조적 한계를 의미한다.9</p>
<h3>2.2  네트워크 구조: GoogLeNet으로부터의 영감</h3>
<p>YOLOv1의 네트워크 아키텍처는 이미지 분류에서 뛰어난 성능을 보인 GoogLeNet에서 영감을 받았다. 전체 구조는 24개의 컨볼루션 레이어와 2개의 완전 연결(fully connected) 레이어로 구성된다.17</p>
<p>하지만 GoogLeNet의 복잡한 인셉션 모듈(Inception module)을 그대로 사용하지 않고, 이를 단순화하여 계산 효율성을 높였다. 구체적으로 <span class="math math-inline">1 \times 1</span> reduction layer를 먼저 적용해 채널(channel) 차원을 줄인 뒤, <span class="math math-inline">3 \times 3</span> 컨볼루션 레이어를 통해 공간적 특징을 추출하는 방식을 채택했다.14 네트워크의 전반적인 흐름은 초기 20개의 컨볼루션 레이어가 이미지의 특징을 추출하는 백본(backbone) 역할을 하고, 마지막 4개의 컨볼루션 레이어와 2개의 완전 연결 레이어가 바운딩 박스 좌표와 클래스 확률을 예측하는 헤드(head) 역할을 수행하는 구조이다.5</p>
<p>활성화 함수로는 마지막 레이어의 선형(Linear) 함수를 제외한 모든 컨볼루션 및 완전 연결 레이어에 Leaky ReLU를 사용하여, 그래디언트가 0이 되어 학습이 멈추는 ‘dying ReLU’ 문제를 방지했다.17 한편, 더 빠른 속도를 위해 제작된 Fast YOLO 버전은 컨볼루션 레이어 수를 24개에서 9개로 줄여 연산량을 크게 감소시켰다.18</p>
<h3>2.3  최종 출력 텐서의 해부</h3>
<p>YOLOv1 네트워크의 최종 출력은 <span class="math math-inline">S \times S \times (B \times 5 + C)</span> 형태의 3차원 텐서(tensor)이다.18 PASCAL VOC 데이터셋(<span class="math math-inline">C=20</span>)을 기준으로 할 때, 이 텐서의 크기는 <span class="math math-inline">7 \times 7 \times (2 \times 5 + 20)</span>, 즉 <span class="math math-inline">7 \times 7 \times 30</span>이 된다.14 이 텐서의 각 차원은 다음과 같은 구체적인 의미를 가진다.</p>
<ul>
<li><span class="math math-inline">S \times S</span> (<span class="math math-inline">7 \times 7</span>): 이미지의 공간적 그리드를 나타낸다. 텐서의 첫 두 차원 <span class="math math-inline">(row, col)</span>은 49개의 그리드 셀 각각에 해당한다.</li>
<li><span class="math math-inline">B \times 5</span>: 각 그리드 셀이 예측하는 <span class="math math-inline">B</span>개(2개)의 바운딩 박스 정보를 담고 있다. 각 바운딩 박스는 5개의 값으로 표현된다.</li>
<li><span class="math math-inline">x, y</span>: 바운딩 박스 중심의 좌표이다. 이 값들은 <strong>자신이 속한 그리드 셀의 좌상단 모서리를 원점 (0,0), 우하단 모서리를 (1,1)로 하는 상대 좌표</strong>로 정규화된다.8</li>
<li><span class="math math-inline">w, h</span>: 바운딩 박스의 너비와 높이이다. 이 값들은 <strong>전체 이미지의 너비와 높이에 대한 상대적인 비율</strong>로 정규화된다.8</li>
<li><code>Confidence Score</code>: 이 바운딩 박스 안에 객체가 존재할 신뢰도와 예측된 박스가 실제 객체와 얼마나 잘 맞는지를 종합한 점수이다. 수식적으로는 <span class="math math-inline">Pr(Object) \times IOU_{pred}^{truth}</span>로 정의된다. 여기서 <span class="math math-inline">Pr(Object)</span>는 셀 안에 객체가 있을 확률, <span class="math math-inline">IOU</span>는 예측된 박스와 실제(ground truth) 박스 간의 Intersection over Union을 의미한다.7</li>
<li><span class="math math-inline">C</span>: 각 그리드 셀이 예측하는 조건부 클래스 확률 <span class="math math-inline">Pr(Class_i | Object)</span>를 나타낸다. 이는 그리드 셀 안에 객체가 존재한다고 가정했을 때, 그 객체가 <span class="math math-inline">i</span>번째 클래스일 확률을 의미한다.14</li>
</ul>
<h2>3.  학습 파이프라인: 사전 학습과 미세 조정</h2>
<p>YOLOv1의 학습 과정은 두 단계로 나뉜다. 첫 번째는 대규모 이미지 분류 데이터셋을 이용한 특징 추출기 사전 학습(pre-training)이고, 두 번째는 객체 탐지 데이터셋을 이용한 전체 모델 미세 조정(fine-tuning)이다.</p>
<h3>3.1  1단계: ImageNet을 이용한 특징 추출기 사전 학습</h3>
<p>본격적인 탐지 모델 학습에 앞서, YOLOv1 네트워크의 초기 20개 컨볼루션 레이어를 ImageNet 1000-class 분류 데이터셋으로 사전 학습시킨다.5 이 단계에서는 <span class="math math-inline">224 \times 224</span> 해상도의 이미지를 입력으로 사용하며, 네트워크의 끝에 average-pooling 레이어와 완전 연결 레이어를 추가하여 1000개의 클래스를 분류하는 작업을 수행한다.5 이 사전 학습의 목적은 객체 탐지 작업에 유용하게 사용될 수 있는 일반적이고 강건한 시각적 특징(예: 모서리, 질감, 색상 패턴 등)을 네트워크가 미리 학습하도록 하는 것이다.5</p>
<h3>3.2  2단계: PASCAL VOC를 이용한 탐지 모델 미세 조정</h3>
<p>사전 학습이 완료되면, 학습된 20개의 컨볼루션 레이어 위에 탐지를 위한 4개의 컨볼루션 레이어와 2개의 완전 연결 레이어를 추가하여 전체 탐지 네트워크를 구성한다.5 이후 PASCAL VOC 2007 및 2012 데이터셋을 사용하여 모델 전체를 미세 조정한다.16</p>
<p>이 단계에서 중요한 변화는 입력 이미지의 해상도를 분류 사전 학습 시의 <span class="math math-inline">224 \times 224</span>에서 <span class="math math-inline">448 \times 448</span>로 높이는 것이다.18 이 방식은 네트워크가 더 세밀한 특징을 학습할 수 있게 하지만, 동시에 모델에게 이중의 부담을 안겨준다. 즉, 네트워크는 갑자기 바뀐 고해상도 입력에 적응해야 하는 동시에, 분류라는 기존 과제보다 훨씬 복잡한 객체 위치 예측(localization)이라는 새로운 과제를 학습해야만 한다. 이러한 급작스러운 변화는 학습의 효율성을 저해하는 요인으로 작용했으며, YOLOv1의 성능을 제한하는 원인 중 하나가 되었다. 후속 모델인 YOLOv2는 이 문제를 해결하기 위해, 탐지 학습에 들어가기 전에 고해상도 이미지로 분류 모델을 추가로 미세 조정하는 단계를 도입했다. 이는 YOLO 시리즈가 어떻게 이전 버전의 약점을 분석하고 점진적으로 개선되었는지를 보여주는 명확한 사례이다.18</p>
<h2>4.  다중 파트 손실 함수 심층 분석</h2>
<h3>4.1  전체 수식과 구성 요소</h3>
<p>YOLOv1은 객체 탐지를 하나의 거대한 회귀 문제로 정의했기 때문에, 손실 함수(loss function)는 전체적으로 Sum-Squared Error (SSE)를 기본 골격으로 사용한다.18 하지만 단순한 SSE가 아닌, 객체 탐지라는 과제의 특수성을 반영하여 위치(localization), 신뢰도(confidence), 분류(classification)의 세 가지 요소를 모두 고려하는 다중 파트 손실 함수(multi-part loss function)로 설계되었다.</p>
<p>전체 손실 함수의 수식은 다음과 같다.<br />
$$<br />
\lambda_{coord}\sum_{i=0}<sup>{S^2}\sum_{j=0}</sup>{B}\mathbb{I}_{ij}^{obj}[(x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2] \</p>
<ul>
<li>\lambda_{coord}\sum_{i=0}<sup>{S^2}\sum_{j=0}</sup>{B}\mathbb{I}_{ij}^{obj}[(\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2] \</li>
<li>\sum_{i=0}<sup>{S^2}\sum_{j=0}</sup>{B}\mathbb{I}_{ij}^{obj}(C_i - \hat{C}_i)^2 \</li>
<li>\lambda_{noobj}\sum_{i=0}<sup>{S^2}\sum_{j=0}</sup>{B}\mathbb{I}_{ij}^{noobj}(C_i - \hat{C}_i)^2 \</li>
<li>\sum_{i=0}<sup>{S^2}\mathbb{I}_{i}</sup>{obj}\sum_{c \in classes}(p_i(c) - \hat{p}_i(c))^2<br />
$$<br />
여기서 <span class="math math-inline">\mathbb{I}_{ij}^{obj}</span>는 <span class="math math-inline">i</span>번째 그리드 셀의 <span class="math math-inline">j</span>번째 바운딩 박스 예측기가 객체 탐지에 “책임“이 있으면 1, 아니면 0인 indicator 함수를 의미한다.19</li>
</ul>
<h3>4.2  각 항의 상세 해설</h3>
<ul>
<li><strong>Localization Loss (1, 2번째 항):</strong> 이 손실은 객체가 존재하는 그리드 셀(<span class="math math-inline">\mathbb{I}_{ij}^{obj}=1</span>)의 “책임 있는” 바운딩 박스 예측기에 대해서만 계산된다.29</li>
<li><strong>중심 좌표 손실 (x, y):</strong> 예측된 중심 좌표 <span class="math math-inline">(x_i, y_i)</span>와 실제 값 <span class="math math-inline">(\hat{x}_i, \hat{y}_i)</span> 간의 SSE를 계산한다.</li>
<li><strong>크기 손실 (w, h):</strong> 예측된 너비와 높이 <span class="math math-inline">(w_i, h_i)</span>와 실제 값 <span class="math math-inline">(\hat{w}_i, \hat{h}_i)</span> 간의 SSE를 계산한다. 여기서 주목할 점은 너비와 높이에 직접 SSE를 적용하는 대신 제곱근(square root)을 취한 값에 적용한다는 것이다. 이는 작은 바운딩 박스에서 발생하는 오차에 더 큰 페널티를 부여하기 위함이다. SSE는 절대 오차에 기반하기 때문에, 큰 박스에서 5픽셀의 오차와 작은 박스에서 5픽셀의 오차를 동일하게 취급한다. 하지만 IOU 관점에서는 작은 박스에서의 오차가 훨씬 더 치명적이다. 제곱근을 사용하면 값이 작을수록 기울기가 가팔라지므로, 작은 박스에서의 오차가 손실 값에 더 큰 영향을 미치게 되어 이 문제를 부분적으로 완화할 수 있다.16</li>
<li><strong>Confidence Loss (3, 4번째 항):</strong> 신뢰도 점수에 대한 손실은 객체가 있는 경우와 없는 경우로 나누어 계산된다.</li>
<li><strong>객체가 있는 경우:</strong> 객체를 포함하는 셀의 바운딩 박스 신뢰도 점수 <span class="math math-inline">C_i</span>가 실제 IOU 값인 <span class="math math-inline">\hat{C}_i</span>(일반적으로 1)에 가까워지도록 학습한다.30</li>
<li><strong>객체가 없는 경우:</strong> 이미지의 대부분을 차지하는, 객체가 없는 셀들의 신뢰도 점수 <span class="math math-inline">C_i</span>를 0에 가깝게 만들도록 학습한다.</li>
<li><strong>Classification Loss (5번째 항):</strong> 이 손실은 객체가 존재하는 그리드 셀(<span class="math math-inline">\mathbb{I}_{i}^{obj}=1</span>)에 대해서만 계산된다. 해당 셀의 예측된 클래스 확률 분포 <span class="math math-inline">p_i(c)</span>가 실제 클래스(one-hot vector 형태의 <span class="math math-inline">\hat{p}_i(c)</span>)와 유사해지도록 SSE를 통해 학습한다.29</li>
</ul>
<h3>4.3  가중치 파라미터의 역할 (<span class="math math-inline">\lambda_{coord}</span>와 <span class="math math-inline">\lambda_{noobj}</span>)</h3>
<p>이 손실 함수에는 두 개의 중요한 가중치 파라미터가 존재한다. 이는 단순히 SSE를 적용한 것이 아니라, 객체 탐지 문제의 본질적인 특성을 고려한 정교한 설계의 결과물이다.</p>
<ul>
<li><span class="math math-inline">\lambda_{coord} = 5</span>: 위치 예측 손실(localization loss)에 5라는 높은 가중치를 부여한다. 이는 객체 탐지에서 정확한 바운딩 박스를 예측하는 것이 클래스를 맞추는 것보다 더 어렵고 중요하다고 판단했기 때문이다.30</li>
<li><span class="math math-inline">\lambda_{noobj} = 0.5</span>: 객체가 없는(no object) 셀의 신뢰도 손실에 0.5라는 낮은 가중치를 부여한다. 이미지 내 49개의 그리드 셀 대부분은 배경, 즉 객체를 포함하지 않는다. 만약 이들의 손실을 동등하게 반영한다면, 소수의 객체를 포함한 셀에서 발생하는 그래디언트를 압도(overpowering)하여 모델이 단순히 “객체 없음“이라고만 예측하도록 학습될 위험이 있다. 이 가중치는 이러한 데이터 불균형 문제를 완화하고 학습을 안정시키는 역할을 한다.14</li>
</ul>
<p>결론적으로, YOLOv1의 손실 함수는 가중치 시스템과 제곱근 변환 등을 통해 아키텍처의 한계를 수학적으로 보완하고, 문제의 본질(데이터 불균형, 위치 예측의 중요성)을 해결하려는 실용적이고 공학적인 접근법의 산물이라 할 수 있다.</p>
<h2>5.  후처리: 비최대 억제(NMS)의 역할</h2>
<h3>5.1  NMS의 필요성</h3>
<p>YOLOv1은 단 한 번의 순전파로 이미지 당 98개(<span class="math math-inline">7 \times 7 \times 2</span>)의 바운딩 박스를 예측한다.16 이 과정에서 하나의 객체에 대해 여러 그리드 셀이 중복된 예측을 내놓을 수 있다. 특히 크기가 큰 객체나 여러 그리드 셀의 경계에 걸쳐 있는 객체는 다수의 후보 박스를 생성할 가능성이 높다.22 비최대 억제(Non-Maximal Suppression, NMS)는 이렇게 생성된 수많은 예측들 중에서 중복되고 불필요한 것들을 제거하고, 각 객체에 대해 가장 정확하다고 판단되는 단 하나의 바운딩 박스만을 최종 결과로 남기기 위한 필수적인 후처리(post-processing) 단계이다.34</p>
<h3>5.2  NMS 알고리즘 단계별 설명</h3>
<p>NMS는 각 바운딩 박스의 신뢰도 점수(confidence score)와 박스 간의 겹치는 정도를 나타내는 IoU(Intersection over Union)를 핵심 기준으로 사용하여 작동한다.35 특정 클래스에 대한 NMS 알고리즘의 작동 과정은 다음과 같은 의사 코드로 표현할 수 있다.36</p>
<ol>
<li>
<p>탐지된 모든 바운딩 박스 리스트 <code>B</code>와 각 박스의 신뢰도 점수 리스트 <code>S</code>를 입력으로 받는다. 최종 결과를 담을 리스트 <code>F</code>를 초기화한다.</p>
</li>
<li>
<p>신뢰도 점수가 특정 임계값(예: 0.2)보다 낮은 박스들을 <code>B</code>에서 모두 제거한다. 이는 가능성이 매우 낮은 예측을 사전에 걸러내는 과정이다.</p>
</li>
<li>
<p><code>B</code>에 남아있는 박스들을 신뢰도 점수 <code>S</code>를 기준으로 내림차순 정렬한다.</p>
</li>
<li>
<p>B가 비어있지 않은 동안 다음 과정을 반복한다:</p>
</li>
</ol>
<p>a. B에서 가장 높은 신뢰도 점수를 가진 박스 H를 꺼내 F에 추가한다.</p>
<p>b. B에 남아있는 나머지 모든 박스 b에 대해, H와의 IoU를 계산한다.</p>
<p>c. IoU(H, b)가 미리 설정된 NMS 임계값(예: 0.5)보다 높은 모든 박스 b를 B에서 제거한다. 이는 H와 동일한 객체를 탐지한 중복 박스로 간주하기 때문이다.</p>
<ol start="5">
<li>반복이 끝나면, 리스트 <code>F</code>에는 각 객체에 대한 최적의 바운딩 박스만이 남게 되며, 이를 최종 탐지 결과로 반환한다.</li>
</ol>
<p>이 과정을 통해 YOLOv1은 수많은 초기 예측을 정제하여 깔끔하고 정확한 최종 탐지 결과를 생성할 수 있다.</p>
<h2>6.  성능 평가 및 비교 분석</h2>
<h3>6.1  PASCAL VOC 벤치마크 성능</h3>
<p>YOLOv1의 성능은 PASCAL VOC 벤치마크 데이터셋을 통해 검증되었다. 주요 결과는 다음과 같다.</p>
<ul>
<li><strong>YOLOv1 (기본 모델):</strong> PASCAL VOC 2007 데이터셋에서 **63.4% mAP(mean Average Precision)**와 <strong>45 FPS</strong>의 성능을 기록했다.16 이는 당시 DPM(Deformable Part Models)과 같은 실시간 탐지기들의 성능을 크게 상회하는 수치였다.26</li>
<li><strong>Fast YOLO:</strong> 9개의 컨볼루션 레이어만을 사용한 경량화 버전으로, 정확도는 <strong>52.7% mAP</strong>로 다소 하락했지만 <strong>155 FPS</strong>라는 압도적인 속도를 달성하여 초고속 실시간 탐지의 가능성을 입증했다.16</li>
<li><strong>YOLO VGG-16:</strong> 백본 네트워크를 VGG-16으로 교체한 버전은 <strong>66.4% mAP</strong>로 더 높은 정확도를 보였으나, <span class="math math-inline">1 \times 1</span> 컨볼루션을 통한 모델 압축이 없어 속도는 <strong>21 FPS</strong>로 크게 감소했다.16</li>
</ul>
<h3>6.2  2-Stage Detectors와의 비교</h3>
<p>YOLOv1은 당시 최고 수준의 정확도를 자랑하던 2-stage detector들과 비교했을 때 명확한 장단점을 보였다.</p>
<ul>
<li><strong>속도 vs. 정확도:</strong> 가장 큰 차이점은 속도와 정확도 간의 트레이드오프였다. YOLOv1은 Fast R-CNN이나 Faster R-CNN에 비해 속도는 월등히 빨랐지만, mAP 수치는 다소 낮았다. 이는 1-stage 방식이 속도를 위해 위치 예측의 정밀도를 일부 희생한 결과였다.6</li>
<li><strong>오류 분석:</strong> 흥미로운 점은 오류의 유형이었다. YOLOv1은 Fast R-CNN에 비해 객체의 위치를 정확하게 잡지 못하는 위치 예측 오류(localization error)가 더 많았다. 반면, 배경을 객체로 잘못 탐지하는 배경 오류(background error)는 Fast R-CNN의 절반 이하로 현저히 적었다.1 이는 YOLOv1이 이미지 전체의 맥락(global context)을 한 번에 보기 때문에, 객체와 배경을 구분하는 능력이 더 뛰어났기 때문으로 분석된다.22</li>
<li><strong>모델 앙상블:</strong> 두 모델의 오류 특성이 상호 보완적이라는 점에 착안하여 YOLOv1과 Fast R-CNN의 예측을 결합한 결과, PASCAL VOC 2007 테스트셋에서 75.0% mAP, VOC 2012에서는 70.7% mAP라는 당시 최고 수준의 성능을 달성했다.16 이는 1-stage와 2-stage 접근법이 서로 다른 강점을 가지고 있음을 명확히 보여주는 결과이다.</li>
</ul>
<p>아래 표는 PASCAL VOC 2007 벤치마크를 기준으로 당시 주요 실시간 객체 탐지 모델들의 성능을 비교한 것이다.</p>
<table><thead><tr><th>모델 (Model)</th><th>아키텍처 유형 (Architectural Stage)</th><th>mAP (%)</th><th>속도 (FPS)</th><th>주요 특징 (Key Characteristics)</th></tr></thead><tbody>
<tr><td>Fast R-CNN</td><td>2-Stage</td><td>70.0</td><td>0.5</td><td>높은 정확도, 느린 속도, 외부 후보 영역 생성기에 의존</td></tr>
<tr><td>Faster R-CNN (VGG-16)</td><td>2-Stage</td><td>73.2</td><td>7</td><td>높은 정확도, RPN으로 종단간 학습 가능, 실시간 미흡</td></tr>
<tr><td><strong>YOLOv1</strong></td><td><strong>1-Stage</strong></td><td><strong>63.4</strong></td><td><strong>45</strong></td><td><strong>실시간 성능, 통합된 회귀 문제, 배경 오류 낮음</strong></td></tr>
<tr><td><strong>Fast YOLO</strong></td><td><strong>1-Stage</strong></td><td><strong>52.7</strong></td><td><strong>155</strong></td><td><strong>초고속 실시간 성능, 정확도 희생</strong></td></tr>
</tbody></table>
<p>이 표는 YOLOv1이 가져온 패러다임의 전환을 명확하게 보여준다. 이전 모델들이 10 FPS의 벽을 넘지 못하며 정확도에 집중할 때, YOLOv1은 실시간의 기준을 새롭게 정의하며 속도와 정확도 사이의 새로운 균형점을 제시했다.</p>
<p>VII. YOLOv1의 한계와 그 유산</p>
<h3>6.3  구조적 한계점 심층 탐구</h3>
<p>혁신적인 아이디어에도 불구하고, YOLOv1은 몇 가지 명백한 구조적 한계점을 가지고 있었다.</p>
<ul>
<li><strong>작은 객체 및 군집 객체 탐지의 어려움:</strong> YOLOv1의 가장 치명적인 약점은 작은 객체 탐지 성능이었다. 이는 근본적으로 거친 <span class="math math-inline">7 \times 7</span> 그리드 구조에서 기인한다. 새 떼나 군중 속의 사람들처럼 여러 개의 작은 객체들이 밀집해 있을 경우, 이들의 중심점이 하나의 그리드 셀에 포함될 확률이 매우 높다.5 하지만 각 그리드 셀은 단 하나의 클래스 집합만을 예측할 수 있으므로, 결국 하나의 객체만 탐지하고 나머지는 모두 놓치게 되는 구조적 문제를 안고 있었다.11</li>
<li><strong>부정확한 위치 예측:</strong> YOLOv1은 여러 개의 다운샘플링(down-sampling) 레이어를 거쳐 생성된 상대적으로 거친 특징 맵(coarse feature map) 위에서 바운딩 박스를 예측한다. 이로 인해 2-stage 모델들이 후보 영역에 대해 고해상도 특징을 추출하여 정밀하게 위치를 보정하는 것에 비해, 바운딩 박스의 위치 정밀도가 떨어지는 경향을 보였다.9</li>
<li><strong>새로운 형태나 비율의 객체에 대한 일반화 능력 부족:</strong> 바운딩 박스 예측이 전적으로 데이터로부터 학습되기 때문에, 훈련 데이터셋에서 보지 못했던 새로운 종횡비(aspect ratio)나 특이한 형태를 가진 객체에 대해서는 일반화 성능이 떨어지는 문제를 보였다.11</li>
</ul>
<h3>6.4  후속 모델에서의 발전: YOLOv2와 그 너머</h3>
<p>역설적으로, YOLOv1의 이러한 명백한 한계점들은 후속 YOLO 시리즈가 해결해야 할 명확한 과제를 제시하며 기술 발전의 원동력이 되었다.</p>
<ul>
<li><strong>YOLOv2 (YOLO9000):</strong> YOLOv1의 주요 약점들을 체계적으로 개선했다.</li>
<li><strong>앵커 박스(Anchor Boxes) 도입:</strong> 위치 예측 정확도를 높이고 다양한 형태의 객체를 탐지하기 위해, 사전 정의된 다양한 크기와 비율의 앵커 박스를 도입했다. 이는 바운딩 박스의 좌표를 직접 예측하는 대신 앵커 박스로부터의 오프셋(offset)을 예측하는 방식으로, 학습을 안정화시키고 재현율(recall)을 81%에서 88%로 크게 향상시켰다.17</li>
<li><strong>배치 정규화(Batch Normalization) 적용:</strong> 모든 컨볼루션 레이어에 배치 정규화를 추가하여 학습 과정을 안정시키고 과적합을 억제했으며, 이를 통해 mAP를 약 2% 향상시켰다.5</li>
<li><strong>YOLOv3와 그 이후:</strong></li>
<li><strong>다중 스케일 예측(Multi-Scale Prediction):</strong> YOLOv3는 작은 객체 탐지 성능을 개선하기 위해 서로 다른 크기의 특징 맵 3개에서 예측을 수행하는 Feature Pyramid Network(FPN) 개념을 도입했다.17</li>
<li><strong>Anchor-free로의 회귀:</strong> 흥미롭게도 최신 YOLO 버전들(예: YOLOv8)은 앵커 박스의 복잡성을 줄이고자 다시 앵커를 사용하지 않는 anchor-free 방식으로 회귀하면서도 높은 성능을 달성하며, 아키텍처가 끊임없이 진화하고 있음을 보여준다.17</li>
</ul>
<h3>6.5  결론: 실시간 객체 탐지의 초석</h3>
<p>YOLOv1은 여러 한계점을 지니고 있었지만, 객체 탐지를 복잡한 다단계 파이프라인에서 벗어나 빠르고 통합된 단일 회귀 문제로 풀어내는 혁신적인 접근법을 제시했다. 이를 통해 ’실시간 객체 탐지’라는 개념을 현실로 만들었으며, 이후 등장하는 모든 1-stage detector들의 기본 철학을 정립했다.9</p>
<p>YOLOv1이 남긴 유산은 단순히 빠른 모델 하나에 그치지 않는다. 그것이 드러낸 한계점들은 후속 연구자들이 해결해야 할 명확한 로드맵이 되었고, 앵커 박스, 다중 스케일 예측, 향상된 백본 네트워크 등 객체 탐지 기술 전반의 발전을 촉진하는 기폭제가 되었다.9 따라서 YOLOv1은 하나의 모델을 넘어, 현대 실시간 컴퓨터 비전의 초석을 다진 기념비적인 연구로 평가받아야 마땅하다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>YOLO VS FASTER R-CNN - OBJECT DETECTION - IRJMETS, https://www.irjmets.com/uploadedfiles/paper//issue_9_september_2022/30226/final/fin_irjmets1664212182.pdf</li>
<li>YOLOv1 to YOLOv10: The fastest and most accurate real-time object detection systems - arXiv, https://arxiv.org/html/2408.09332v1</li>
<li>YOLO and R-CNN differences - Convolutional Neural Networks - DeepLearning.AI, https://community.deeplearning.ai/t/yolo-and-r-cnn-differences/378376</li>
<li>Object Detection Algorithms: R-CNN, Fast R-CNN, Faster R-CNN, and YOLO - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2024/07/object-detection-algorithms/</li>
<li>YOLO Algorithm for Object Detection Explained [+Examples] - V7 Labs, https://www.v7labs.com/blog/yolo-object-detection</li>
<li>Object Detection with YOLO and Faster R-CNN | by Hey Amit | Biased-Algorithms - Medium, https://medium.com/biased-algorithms/object-detection-with-yolo-and-faster-r-cnn-c2c1eda63dad</li>
<li>Comparison of Object Detection Algorithms CNN, YOLO and SSD, https://www.ijsrtjournal.com/article/Comparison+of+Object+Detection+Algorithms+CNN+YOLO+and+SSD</li>
<li>Understanding a Real-Time Object Detection Network: You Only Look Once (YOLOv1), https://pyimagesearch.com/2022/04/11/understanding-a-real-time-object-detection-network-you-only-look-once-yolov1/</li>
<li>YOLOv1 to YOLOv11: A Comprehensive Survey of Real-Time Object Detection Innovations and Challenges - arXiv, https://arxiv.org/html/2508.02067v1</li>
<li>arXiv:1506.02640v5 [cs.CV] 9 May 2016, https://arxiv.org/abs/1506.02640</li>
<li>YOLO Object Detection Explained: A Beginner’s Guide - DataCamp, https://www.datacamp.com/blog/yolo-object-detection-explained</li>
<li>(PDF) Enhancing Real-time Object Detection with YOLO Algorithm - ResearchGate, https://www.researchgate.net/publication/376252973_Enhancing_Real-time_Object_Detection_with_YOLO_Algorithm</li>
<li>You Only Look Once: Unified, Real-Time Object Detection | Request PDF - ResearchGate, https://www.researchgate.net/publication/311609522_You_Only_Look_Once_Unified_Real-Time_Object_Detection</li>
<li>Review On YOLOv1. YOLO stands for You Only Look Once. As… | by Arun Mohan | DataDrivenInvestor, https://medium.datadriveninvestor.com/review-on-yolov1-3c85304b617d</li>
<li>[PDF] You Only Look Once: Unified, Real-Time Object Detection - Semantic Scholar, https://www.semanticscholar.org/paper/You-Only-Look-Once%3A-Unified%2C-Real-Time-Object-Redmon-Divvala/f8e79ac0ea341056ef20f2616628b3e964764cfd</li>
<li>Review: YOLOv1 — You Only Look Once (Object Detection) | by Sik …, https://medium.com/data-science/yolov1-you-only-look-once-object-detection-e1f3ffec8a89</li>
<li>YOLO Explained: From v1 to Present - Viso Suite, https://viso.ai/computer-vision/yolo-explained/</li>
<li>YOLO 1 through 5: A complete and detailed overview - Kaggle, https://www.kaggle.com/code/vikramsandu/yolo-1-through-5-a-complete-and-detailed-overview/notebook</li>
<li>YOLOv1: You Only Look Once | Ara Intelligence Blog, https://araintelligence.com/blogs/deep-learning/object-detection/yolo_v1/</li>
<li>Evolution of YOLO Object Detection Model From V5 to V8 [Updated] - Labellerr, https://www.labellerr.com/blog/evolution-of-yolo-object-detection-model-from-v5-to-v8/</li>
<li>Deep dive into YOLOv1 - Medium, https://medium.com/@abhishekjainindore24/deep-dive-into-yolov1-c70111debe60</li>
<li>YOLOv1 Explained by A.J. Paper: You Only Look Once: Unified… | by Jesse Annan, https://medium.com/@jesse419419/yolov1-explained-by-a-j-7fe408764c55</li>
<li>YOLO Object Detection | YoloV1 Explanation and Implementation Tutorial - YouTube, https://www.youtube.com/watch?v=TPD9AfY7AHo</li>
<li>The history of YOLO: The origin of the YOLOv1 algorithm …, https://www.superannotate.com/blog/yolov1-algorithm</li>
<li>A Comprehensive Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS - arXiv, https://arxiv.org/html/2304.00501v6</li>
<li>YOLO-v1 to YOLO-v8, the Rise of YOLO and Its Complementary Nature toward Digital Manufacturing and Industrial Defect Detection - MDPI, https://www.mdpi.com/2075-1702/11/7/677</li>
<li>A Better, Faster, and Stronger Object Detector (YOLOv2 …, https://pyimagesearch.com/2022/04/18/a-better-faster-and-stronger-object-detector-yolov2/</li>
<li>YOLO Loss Function Part 1: SIoU and Focal Loss - LearnOpenCV, https://learnopencv.com/yolo-loss-function-siou-focal-loss/</li>
<li>YOLO v1: Part3. Introduction | by Divakar Kapil | Escapades in Machine Learning | Medium, https://medium.com/adventures-with-deep-learning/yolo-v1-part3-78f22bd97de4</li>
<li>neural networks - Yolo Loss function explanation - Cross Validated, https://stats.stackexchange.com/questions/287486/yolo-loss-function-explanation</li>
<li>For the YOLO loss function, what is the calculation performed in order to obtain the value of the term 1objij? - Stack Overflow, https://stackoverflow.com/questions/57295132/for-the-yolo-loss-function-what-is-the-calculation-performed-in-order-to-obtain</li>
<li>YOLO Algorithm, https://dkharazi.github.io/notes/ml/cnn/yolo/</li>
<li>Questions about the YOLO loss function : r/computervision - Reddit, https://www.reddit.com/r/computervision/comments/rvzgs1/questions_about_the_yolo_loss_function/</li>
<li>Non-Maximum Suppression (NMS) Explained - Ultralytics, https://www.ultralytics.com/glossary/non-maximum-suppression-nms</li>
<li>A Deep Dive Into Non-Maximum Suppression (NMS) | Built In, https://builtin.com/machine-learning/non-maximum-suppression</li>
<li>Selecting the Right Bounding Box Using Non-Max Suppression, https://www.analyticsvidhya.com/blog/2020/08/selecting-the-right-bounding-box-using-non-max-suppression-with-implementation/</li>
<li>A Decade of You Only Look Once (YOLO) for Object Detection - arXiv, https://arxiv.org/html/2504.18586v1</li>
<li>Tutorial: Understanding Intersection over Union and Non-Maximum Suppression | by Jesse Annan | Medium, https://medium.com/@jesse419419/understanding-iou-and-nms-by-a-j-dcebaad60652</li>
<li>YOLO Object Detection Explained: Models, Tools, Use Cases - Lightly, https://www.lightly.ai/blog/yolo</li>
<li>YOLO vs. Faster R-CNN: Which Object Detection Framework Should You Use for Real-Time Tasks? : r/deeplearning - Reddit, https://www.reddit.com/r/deeplearning/comments/1luqx6z/yolo_vs_faster_rcnn_which_object_detection/</li>
<li>YOLO Object Detection: Key Features and Benefits, https://labelyourdata.com/articles/yolo-object-detection</li>
<li>Can someone clarify the anchor box concept used in Yolo? · Issue #568 · pjreddie/darknet - GitHub, https://github.com/pjreddie/darknet/issues/568</li>
<li>Need some clear explainations about the importance of anchor box in yolov3, https://stackoverflow.com/questions/58627473/need-some-clear-explainations-about-the-importance-of-anchor-box-in-yolov3</li>
<li>ODverse33: Is the New YOLO Version Always Better? A Multi Domain benchmark from YOLO v5 to v11 - arXiv, https://arxiv.org/html/2502.14314v2</li>
<li>Review of YOLO: drawback and improvement from v1 to v3 - Tech Hive, https://sheng-fang.github.io/2020-04-25-review_yolo/</li>
<li>IYOLO-NL: An improved you only look once and none left object detector for real-time face mask detection - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC10457516/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>