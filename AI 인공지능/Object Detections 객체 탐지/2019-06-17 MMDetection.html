<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:MMDetection (2019-06-17)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>MMDetection (2019-06-17)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>MMDetection (2019-06-17)</span></nav>
                </div>
            </header>
            <article>
                <h1>MMDetection (2019-06-17)</h1>
<h2>1. 서론</h2>
<h3>1.1 MMDetection의 정의와 비전</h3>
<p>MMDetection은 PyTorch를 기반으로 구축된 오픈 소스 객체 탐지(Object Detection) 툴박스이자, 현대 컴퓨터 비전 연구 및 개발 생태계의 핵심 구성 요소이다.1 이는 단순히 다양한 탐지 알고리즘을 구현한 코드의 집합을 넘어, 연구와 개발 과정을 가속화하고, 알고리즘 재구현의 복잡성을 줄이며, 공정하고 신뢰할 수 있는 벤치마크를 제공하기 위한 통합 플랫폼으로서 설계되었다.3 MMDetection은 OpenMMLab 프로젝트의 철학을 계승하여, 고품질의 모듈화된 구성 요소를 제공함으로써 연구자들이 기존 방법론을 손쉽게 재구현하고, 나아가 자신만의 새로운 탐지기를 개발할 수 있는 유연한 환경을 제공하는 것을 궁극적인 비전으로 삼는다.5 이러한 비전은 MMDetection을 단순한 도구에서 컴퓨터 비전 커뮤니티의 성장을 촉진하는 연구 플랫폼으로 격상시키는 원동력이 된다.</p>
<h3>1.2 역사적 배경: COCO Challenge 2018의 유산</h3>
<p>MMDetection의 기원은 2018년 개최된 컴퓨터 비전 분야의 가장 권위 있는 대회 중 하나인 COCO Detection Challenge로 거슬러 올라간다. 당시 이 대회의 탐지 트랙에서 우승을 차지한 MMDet 팀의 코드베이스가 바로 MMDetection의 모태가 되었다.1 이 역사적 배경은 MMDetection의 근본적인 특성을 이해하는 데 매우 중요한 단서를 제공한다. MMDetection은 순수한 이론적 탐구나 학술적 실험을 위해 처음부터 설계된 프레임워크가 아니다. 그보다는, 세계 최고 수준의 경쟁 환경에서 최고의 성능을 달성하기 위해 실전적으로 최적화되고 검증된 코드에서 파생되었다.</p>
<p>이러한 ‘실전 지향적’ 기원은 프레임워크의 초기 설계 단계부터 높은 효율성과 정확성이 최우선적인 고려사항이었음을 시사한다. 모듈성, 유연성, 확장성과 같은 설계 원칙들은 추상적인 개념으로 존재했던 것이 아니라, ’최고 성능 달성’이라는 구체적이고 실용적인 목표를 달성하기 위한 수단으로서 구현되었다. 예를 들어, 다양한 모델 아키텍처와 학습 전략을 신속하게 실험하고 조합하여 최적의 성능을 찾아내는 과정에서 자연스럽게 모듈화의 필요성이 대두되었고, 이는 MMDetection의 핵심 철학으로 자리 잡게 되었다. 이처럼 치열한 경쟁 환경에서 단련된 견고함과 신뢰성은 MMDetection이 학계와 산업계 양쪽에서 빠르게 표준적인 도구로 채택될 수 있었던 근본적인 이유 중 하나로 분석된다.</p>
<h3>1.3  안내서의 구조와 목적</h3>
<p>본 안내서는 MMDetection에 대한 포괄적이고 심층적인 분석을 제공하는 것을 목적으로 한다. 이를 위해, 프레임워크의 근간을 이루는 핵심 설계 철학부터 시작하여, OpenMMLab 생태계 내에서 MMEngine 및 MMCV와 같은 기반 라이브러리와의 상호작용을 분석한다. 이후, 객체 탐지기의 핵심 구성 요소인 Backbone, Neck, Head로 이루어진 아키텍처를 해부하고, 이들 사이의 데이터 흐름을 상세히 추적한다. 또한, 설치 및 환경 설정, 설정 파일 시스템의 작동 원리, 그리고 기본적인 학습 및 추론 워크플로우를 포함한 실용적인 활용법을 단계별로 안내한다. 나아가, MMDetection이 제공하는 방대한 모델 라이브러리를 체계적으로 정리하고, COCO 데이터셋을 기준으로 한 성능 벤치마크를 제시하며, 주요 경쟁 프레임워크인 Detectron2와의 비교 분석을 수행한다. 마지막으로, 사용자 정의 데이터셋 활용 및 새로운 컴포넌트 개발과 같은 고급 사용자 정의 기법을 탐구하고, MMDetection의 현재 강점과 약점을 종합하여 미래 발전 방향성을 전망하며 안내서를 마무리한다.</p>
<h2>2.  핵심 설계 철학 및 원칙</h2>
<p>MMDetection의 성공은 네 가지 핵심 설계 철학에 기반한다: 모듈성(Modular Design), 설정 파일 기반 워크플로우(Config-Driven Workflow), 고효율성(High Efficiency), 그리고 다중 작업 지원(Multi-Task Support). 이 원칙들은 상호 유기적으로 작용하며 MMDetection을 강력하고 유연한 연구 개발 플랫폼으로 만든다.</p>
<h3>2.1  모듈성</h3>
<p>MMDetection 설계의 가장 근본적인 원칙은 모듈성이다.1 복잡한 객체 탐지 프레임워크를 기능적으로 독립적인 여러 구성 요소(component)로 분해하는 것을 의미한다. 주요 구성 요소는 다음과 같다.</p>
<ul>
<li><strong>Backbone:</strong> 입력 이미지로부터 특징 맵(feature map)을 추출하는 역할을 한다. (예: ResNet, Swin Transformer)</li>
<li><strong>Neck:</strong> Backbone에서 추출된 특징 맵을 받아 융합하고 정제하여 후속 단계로 전달한다. (예: FPN)</li>
<li><strong>Head:</strong> Neck으로부터 받은 특징 맵을 사용하여 최종적으로 객체의 클래스와 위치를 예측한다. (예: RPN Head, Retina Head)</li>
</ul>
<p>이러한 모듈식 접근법을 통해 사용자는 마치 레고 블록을 조립하듯, 다양한 구성 요소를 자유롭게 조합하여 자신만의 맞춤형 객체 탐지기를 손쉽게 구축할 수 있다.1 예를 들어, ResNet-50 Backbone에 FPN Neck, 그리고 Faster R-CNN Head를 결합한 모델을 만들거나, 동일한 Backbone과 Neck에 RetinaNet Head를 결합하여 1-단계 탐지기로 전환하는 것이 코드의 최소한의 수정만으로 가능하다. 이 설계는 새로운 아이디어나 논문에서 제안된 모듈을 기존 프레임워크에 신속하게 통합하고 그 성능을 검증하는 과정을 극적으로 단순화시켜, 연구 개발의 속도를 크게 향상시킨다.3</p>
<h3>2.2  설정 파일 기반 워크플로우</h3>
<p>MMDetection의 모듈성을 실제적으로 구현하고 제어하는 수단이 바로 설정 파일 기반 워크플로우이다. 모델의 아키텍처, 데이터셋의 종류와 경로, 데이터 증강 파이프라인, 옵티마이저, 학습률 스케줄러 등 모델 학습과 평가에 관련된 거의 모든 요소가 Python 코드 파일(<code>.py</code>) 형식의 설정 파일(config file)을 통해 정의되고 제어된다.2</p>
<p>이 방식은 연구의 재현성과 이식성을 극대화하는 데 결정적인 역할을 한다. 연구자는 실험 조건을 변경하기 위해 프레임워크의 핵심 코드를 수정할 필요 없이, 단지 설정 파일의 해당 부분만 변경하면 된다.2 예를 들어, 학습 에포크 수를 12에서 24로 늘리거나, 옵티마이저를 SGD에서 AdamW로 변경하는 작업은 설정 파일의 몇 줄을 수정하는 것으로 완료된다.</p>
<p>여기서 MMDetection의 모듈성과 설정 파일 시스템이 단순한 두 개의 개별적인 특징이 아니라, ’실험의 추상화’라는 하나의 통합된 철학을 구현하기 위한 핵심적인 두 축임을 이해하는 것이 중요하다. 모듈화가 실험에서 ‘무엇을’ 변경할 수 있는지(예: Backbone, Neck, Head)를 정의한다면, 설정 파일은 그것을 ‘어떻게’ 변경할지(예: <code>type='ResNet'</code>, <code>depth=50</code>)를 구체적으로 명시하는 ‘설계도’ 역할을 한다. 이 두 시스템의 긴밀한 결합은 연구자가 저수준의 코드 구현 세부사항에 얽매이지 않고, “ResNet50 백본을 Swin Transformer로 교체하고 학습률 스케줄을 코사인 어닐링으로 변경한다“와 같은 고수준의 아이디어 자체에만 집중할 수 있는 환경을 제공한다. 이는 MMDetection이 제공하는 가장 강력한 기능 중 하나이지만, 동시에 이 복잡하고 추상화된 시스템을 이해해야 하는 초심자에게는 상당한 학습 곡선을 요구하는 진입 장벽으로 작용하기도 한다.13</p>
<h3>2.3  고효율성</h3>
<p>MMDetection은 연구의 유연성뿐만 아니라 실제적인 성능 또한 중요하게 고려한다. 이를 위해 프레임워크의 여러 부분이 고도로 최적화되었다. 특히, 경계 상자(bounding box)와 마스크(mask) 연산과 같이 계산 집약적인 기본 작업들은 모두 GPU 상에서 효율적으로 실행되도록 구현되었다.1 이러한 최적화 덕분에 MMDetection의 학습 속도는 Detectron2, maskrcnn-benchmark, SimpleDet과 같은 다른 주요 객체 탐지 코드베이스와 비교했을 때 동등하거나 더 빠른 수준을 보인다.1 이는 대규모 데이터셋을 이용한 반복적인 실험이 필수적인 딥러닝 연구에서 상당한 시간과 자원을 절약해주는 중요한 장점이다.</p>
<h3>2.4  다중 작업 지원</h3>
<p>MMDetection은 단일 프레임워크 내에서 다양한 컴퓨터 비전 작업을 포괄적으로 지원하도록 설계되었다. 핵심적인 객체 탐지(Object Detection) 외에도, 픽셀 단위로 객체의 영역을 분할하는 인스턴스 분할(Instance Segmentation), 배경과 객체를 함께 분할하는 파노라마 분할(Panoptic Segmentation), 그리고 레이블이 없는 데이터를 활용하는 준지도 객체 탐지(Semi-supervised Object Detection)와 같은 여러 관련 작업을 별도의 큰 수정 없이 즉시 지원한다.1 이러한 다중 작업 지원은 MMDetection을 특정 작업에 국한된 라이브러리가 아닌, 객체 수준의 시각 이해(object-level visual understanding)를 위한 범용적인 플랫폼으로 만들어준다.</p>
<h2>3.  OpenMMLab 생태계와 MMDetection의 상호작용</h2>
<p>MMDetection은 독립적으로 존재하는 툴박스가 아니라, OpenMMLab이라는 거대한 컴퓨터 비전 알고리즘 생태계의 일부이다. 이 생태계 내에서 MMDetection은 기반 라이브러리인 MMEngine과 MMCV에 깊이 의존하며, MMPretrain, MMDeploy 등 다른 툴박스와 유기적으로 상호작용하여 연구부터 배포까지의 전체 파이프라인을 지원한다.</p>
<h3>3.1  기반 라이브러리: MMEngine과 MMCV</h3>
<p>MMDetection의 안정성과 효율성은 두 개의 핵심 기반 라이브러리, MMEngine과 MMCV에 의해 뒷받침된다.</p>
<ul>
<li><strong>MMEngine (OpenMMLab Foundational Library for Training Deep Learning Models):</strong> OpenMMLab 2.0 아키텍처의 심장부로서, 딥러닝 모델 학습을 위한 범용적이고 강력한 실행기(Runner)를 제공한다.6 MMEngine은 학습 루프(training loop), 평가(evaluation), 훅(Hook) 시스템, 로깅, 분산 학습 관리 등 다양한 툴박스에서 공통적으로 필요한 기능들을 추상화하여 통합 관리한다. MMDetection은 MMEngine의 <code>Runner</code>를 상속받아 사용함으로써, 학습 프로세스의 세부 구현에 대한 부담을 덜고 탐지 모델 자체의 로직에만 집중할 수 있다. 이로 인해 MMDetection의 코드는 더욱 간결해지고, MMSegmentation이나 MMPose와 같은 다른 OpenMMLab 툴박스와의 일관성이 유지된다.16</li>
<li><strong>MMCV (OpenMMLab Computer Vision Foundation):</strong> 컴퓨터 비전 연구를 위한 필수적인 기본 도구들을 모아놓은 라이브러리이다.1 여기에는 고도로 최적화된 커스텀 CUDA 연산자(예: RoIAlign, Deformable Convolution), 다양한 이미지 및 비디오 처리 함수, 그리고 데이터 증강에 사용되는 데이터 변환(transform) 파이프라인 등이 포함되어 있다.17 MMDetection은 이러한 MMCV의 기능들을 적극적으로 활용하여 높은 연산 효율성과 성능을 달성한다. MMCV 2.0 버전부터는 기존에 포함되어 있던 학습 관련 기능들이 MMEngine으로 완전히 이전되었고, MMCV는 순수한 컴퓨터 비전 기본 연산 및 데이터 처리 라이브러리로서의 역할에 더욱 집중하게 되었다. 또한, 사용자의 필요에 따라 CUDA 연산자를 포함하는 완전판(<code>mmcv</code>)과 포함하지 않는 경량판(<code>mmcv-lite</code>)으로 패키지가 분리되어 제공된다.17</li>
</ul>
<h3>3.2  OpenMMLab 2.0 아키텍처의 영향</h3>
<p>OpenMMLab 2.0의 핵심인 MMEngine으로의 전환은 MMDetection에 단순한 코드 리팩토링 이상의 구조적인 변화를 가져왔다. MMDetection 3.x 버전부터는 훈련, 테스트, 데이터셋, 모델, 평가, 시각화 등 모든 핵심 모듈의 인터페이스와 내부 로직이 MMEngine의 표준에 맞춰 통일되고 재구성되었다.19</p>
<p>이러한 변화는 OpenMMLab의 장기적인 전략 방향성을 명확히 보여준다. 이전의 개별 툴박스들이 각자의 작업 영역에 특화된 ‘수직적 사일로(vertical silos)’ 구조였다면, MMEngine의 도입은 이들을 ’수평적 플랫폼(horizontal platform)’으로 전환하는 계기가 되었다. 이제 모든 툴박스는 MMEngine이라는 공통의 실행기 위에서 동일한 인터페이스와 데이터 구조(예: <code>DataSample</code>)를 공유한다. 이는 연구자들이 MMDetection의 객체 탐지기 모듈과 MMSegmentation의 시맨틱 분할 헤드 모듈을 가져와 MMEngine 위에서 손쉽게 결합하여 새로운 다중 작업(multi-task) 모델을 구축하는 것을 기술적으로 가능하게 한다. 이 아키텍처 통합은 향후 파운데이션 모델(foundation model)과 같이 더욱 복잡하고 융합된 비전 모델 연구를 위한 견고한 기반을 마련한 것으로, OpenMMLab 생태계 전체의 장기적인 확장성과 경쟁력을 확보하기 위한 핵심적인 진화라고 평가할 수 있다.</p>
<h3>3.3  타 툴박스와의 연동성</h3>
<p>MMDetection은 OpenMMLab 생태계 내 다른 툴박스들과의 긴밀한 연동을 통해 강력한 시너지를 창출한다.</p>
<ul>
<li><strong>MMPretrain:</strong> MMDetection에서 사용되는 대부분의 Backbone 모델(예: ResNet, Swin Transformer)은 ImageNet과 같은 대규모 데이터셋에서 사전 학습된 가중치를 사용하여 초기화된다. MMPretrain은 이러한 사전 학습된 모델들을 제공하는 툴박스로, MMDetection은 설정 파일에서 간단한 지정만으로 MMPretrain의 모델들을 Backbone으로 손쉽게 가져와 사용할 수 있다.20</li>
<li><strong>MMDeploy:</strong> 연구 단계에서 학습된 모델을 실제 서비스에 배포하는 것은 또 다른 차원의 과제이다. MMDeploy는 MMDetection에서 학습된 모델을 ONNX, TensorRT, TorchServe, CoreML 등 다양한 상용 추론 엔진 및 디바이스에 맞게 변환하고 최적화하여 배포하는 과정을 자동화해주는 툴박스이다.6 이를 통해 사용자는 연구(MMDetection)부터 검증, 최적화, 그리고 최종 배포(MMDeploy)에 이르는 전체 머신러닝 파이프라인을 OpenMMLab 생태계 안에서 완결적으로 수행할 수 있다.</li>
</ul>
<p>이러한 상호 연동성은 OpenMMLab이 단순한 알고리즘 라이브러리들의 집합이 아니라, 아이디어 구상부터 실제 제품화까지 전 과정을 지원하는 통합적인 ’AI 개발 플랫폼’을 지향하고 있음을 보여준다.</p>
<h2>4.  아키텍처 심층 분석: 객체 탐지기 해부</h2>
<p>MMDetection의 강력함은 현대 딥러닝 기반 객체 탐지기의 표준 아키텍처를 체계적으로 모듈화하여 구현한 데 있다. 이 섹션에서는 탐지기의 개념적 구조를 해부하고, 각 구성 요소의 역할과 그 사이를 흐르는 데이터의 여정을 심층적으로 분석한다.</p>
<h3>4.1  표준 객체 탐지 파이프라인</h3>
<p>현대의 객체 탐지 모델은 대부분 개념적으로 <code>Input -&gt; Backbone -&gt; Neck -&gt; Head -&gt; Output</code>의 파이프라인 구조를 따른다.22 MMDetection은 이 추상적인 파이프라인을 실제 코드의 모듈로 명확하게 구현하여, 각 단계의 역할을 분리하고 교체 가능하도록 만들었다.</p>
<ol>
<li><strong>Input:</strong> 전처리 및 데이터 증강을 거친 이미지 텐서.</li>
<li><strong>Backbone:</strong> 이미지에서 다양한 수준의 특징을 추출한다.</li>
<li><strong>Neck:</strong> Backbone이 추출한 특징들을 융합하고 강화한다.</li>
<li><strong>Head:</strong> 강화된 특징을 바탕으로 최종 예측을 수행한다.</li>
<li><strong>Output:</strong> 예측된 객체의 클래스, 경계 상자, 그리고 경우에 따라 마스크 정보.</li>
</ol>
<h3>4.2  Backbone (특징 추출기)</h3>
<p>Backbone은 객체 탐지 모델의 ’눈’과 같은 역할을 한다. 입력 이미지로부터 시각적 특징을 추출하는 것이 주된 임무이다.</p>
<ul>
<li><strong>역할:</strong> 일반적으로 ImageNet과 같은 대규모 데이터셋에서 사전 학습된 FCN(Fully Convolutional Network)을 사용하여, 입력 이미지로부터 계층적인 특징 맵(hierarchical feature maps)을 생성한다.2 네트워크의 초기 레이어에서는 엣지, 코너, 색상과 같은 저수준(low-level) 특징을 추출하고, 더 깊은 레이어로 갈수록 객체의 일부나 질감과 같은 고수준(high-level)의 의미론적(semantic) 특징을 인코딩한다.</li>
<li><strong>대표 예시:</strong> MMDetection은 ResNet, ResNeXt와 같은 전통적인 CNN 기반 백본부터, HRNet, 그리고 최신 Vision Transformer 계열인 Swin Transformer에 이르기까지 매우 광범위한 Backbone을 지원한다.8 이를 통해 사용자는 자신의 문제와 자원 상황에 가장 적합한 특징 추출기를 선택할 수 있다.</li>
</ul>
<h3>4.3  Neck (특징 융합기)</h3>
<p>Backbone이 단순히 특징을 ’추출’하는 데 집중한다면, Neck은 추출된 특징들을 ’정제’하고 ’융합’하여 탐지 성능을 극대화하는 역할을 한다.</p>
<ul>
<li><strong>역할:</strong> Backbone의 여러 레이어에서 나온 다양한 스케일의 특징 맵들을 입력으로 받는다. 깊은 레이어의 특징 맵은 의미 정보가 풍부하지만 해상도가 낮아 객체의 정확한 위치를 파악하기 어렵고, 얕은 레이어의 특징 맵은 해상도가 높아 위치 정보가 정확하지만 의미 정보가 부족하다. Neck은 이 두 종류의 특징 맵을 효과적으로 결합하여, 의미 정보와 위치 정보를 모두 풍부하게 담고 있는 새로운 특징 맵 피라미드를 생성한다.2 이를 통해 모델은 이미지 내의 다양한 크기의 객체들을 효과적으로 탐지할 수 있게 된다.</li>
<li><strong>대표 예시:</strong> 가장 대표적인 Neck 구조는 FPN(Feature Pyramid Network)이며, 이를 개선한 PAFPN(Path Aggregation FPN), BiFPN, NAS-FPN 등 다양한 변형 모델들이 MMDetection에서 지원된다.25</li>
</ul>
<h3>4.4  Head (예측 헤드)</h3>
<p>Head는 파이프라인의 최종 단계로, Neck으로부터 전달받은 잘 정제된 특징 맵을 사용하여 실제 예측을 수행하는 부분이다.</p>
<ul>
<li><strong>역할:</strong> Neck이 생성한 특징 맵의 각 위치에서 객체의 존재 여부, 클래스(예: 사람, 자동차), 그리고 정확한 위치(경계 상자의 좌표)를 예측한다.2 인스턴스 분할 모델의 경우, 픽셀 단위의 마스크를 예측하는 역할도 추가로 수행한다.</li>
<li><strong>분류:</strong> MMDetection은 객체 탐지 방법론의 큰 두 갈래에 따라 Head를 크게 두 종류로 구분한다.</li>
<li><strong>DenseHead (1-단계 탐지기):</strong> 특징 맵의 모든 픽셀 위치 또는 미리 정의된 앵커 박스 위치에서 조밀하게(densely) 예측을 한 번에 수행한다. 속도가 빠르다는 장점이 있다. 대표적으로 RetinaNet의 <code>RetinaHead</code>, FCOS의 <code>FCOSHead</code>, YOLO의 <code>YOLOv3Head</code> 등이 여기에 속한다.28</li>
<li><strong>RoIHead (2-단계 탐지기):</strong> 먼저 RPN(Region Proposal Network)과 같은 모듈을 통해 객체가 있을 법한 후보 영역, 즉 RoI(Region of Interest)를 제안받는다. 그 후, 제안된 RoI 영역에 대해서만 정밀한 클래스 분류와 위치 보정을 수행한다. 일반적으로 DenseHead보다 정확도가 높지만 속도가 느리다. Faster R-CNN, Mask R-CNN, Cascade R-CNN 등이 이 구조를 사용하며, 내부적으로 경계 상자를 예측하는 <code>BBoxHead</code>와 마스크를 예측하는 <code>MaskHead</code>로 다시 나뉠 수 있다.27</li>
</ul>
<h3>4.5  데이터 흐름</h3>
<p>MMDetection 내에서 데이터가 각 모듈을 거쳐 처리되는 과정은 학습과 추론 단계에서 약간의 차이를 보인다.</p>
<ul>
<li><strong>학습 과정 (Training Flow):</strong></li>
</ul>
<ol>
<li><strong>데이터 로딩 및 전처리 (<code>DataLoader</code> &amp; <code>transforms</code>):</strong> <code>DataLoader</code>가 디스크에서 이미지와 어노테이션(정답 경계 상자, 클래스 등)을 읽어온다. 이 데이터는 설정 파일에 정의된 <code>transforms</code> 파이프라인을 통과하며 <code>Resize</code>, <code>RandomFlip</code>과 같은 데이터 증강(augmentation) 및 정규화(normalization) 과정을 거친다. 파이프라인의 마지막 단계인 <code>PackDetInputs</code>는 이 정보들을 <code>DetDataSample</code>이라는 표준화된 데이터 구조로 패키징한다.28</li>
<li><strong>배치 구성 (<code>Data Preprocessor</code>):</strong> <code>Data Preprocessor</code>는 <code>DataLoader</code>로부터 <code>DetDataSample</code> 리스트를 받아, 이미지들을 하나의 배치 텐서로 묶고 GPU로 전송하는 역할을 한다. 정답 정보는 <code>DetDataSample</code> 객체 리스트 형태로 모델에 함께 전달된다.31</li>
<li><strong>순전파 (<code>Model Forward</code>):</strong> 배치 데이터는 모델의 <code>forward</code> 함수로 입력된다. 데이터는 먼저 Backbone을 통과하여 여러 수준의 특징 맵을 생성하고, 이 특징 맵들은 Neck에서 융합 및 정제된다. 마지막으로 Head는 이 정제된 특징 맵을 기반으로 예측값(logits)을 계산한다.</li>
<li><strong>손실 계산 (<code>Loss Calculation</code>):</strong> Head 내부에 정의된 <code>loss</code> 모듈이 모델의 예측값과 <code>DetDataSample</code>에 담겨 있는 정답(ground truth) 정보를 비교하여 손실(loss) 값을 계산한다. 예를 들어, 분류 손실에는 <code>FocalLoss</code>나 <code>CrossEntropyLoss</code>가, 위치 회귀 손실에는 <code>L1Loss</code>나 <code>GIoULoss</code>가 사용될 수 있다.25</li>
<li><strong>가중치 업데이트 (<code>Optimization</code>):</strong> 계산된 총 손실 값은 <code>OptimizerWrapper</code>로 전달된다. <code>OptimizerWrapper</code>는 이 손실 값을 기반으로 역전파(backpropagation)를 수행하여 모델의 모든 학습 가능한 파라미터(가중치)를 업데이트한다.16 이 과정이 수많은 반복(iteration)을 통해 진행되며 모델이 학습된다.</li>
</ol>
<ul>
<li><strong>추론 과정 (Inference Flow):</strong></li>
</ul>
<ol>
<li><strong>데이터 로딩 및 전처리:</strong> 학습 과정의 1, 2단계와 유사하게 진행되지만, 정답 어노테이션 정보는 사용되지 않는다.</li>
<li><strong>순전파:</strong> 학습 과정의 3단계와 동일하게, 입력 이미지가 Backbone, Neck, Head를 순서대로 통과하여 최종 예측값을 생성한다.</li>
<li><strong>후처리 (<code>Post-processing</code>):</strong> Head에서 나온 원시 예측값(raw predictions)은 그대로 사용하기 어렵다. 따라서 후처리 과정이 필요하다. 이 단계에서는 수많은 예측 상자들 중에서 신뢰도가 낮은 것들을 제거하고, NMS(Non-Maximum Suppression) 알고리즘을 적용하여 동일한 객체에 대한 중복된 예측 상자들을 하나로 통합한다. 이 과정을 거쳐 최종적인 탐지 결과(경계 상자, 클래스, 신뢰도 점수)가 생성되며, 이 역시 <code>DetDataSample</code> 형태로 반환된다.2</li>
</ol>
<p>이처럼 MMDetection은 복잡한 객체 탐지 과정을 체계적인 모듈과 명확한 데이터 흐름으로 구조화하여, 사용자가 각 단계를 직관적으로 이해하고 필요에 따라 수정 및 확장할 수 있도록 지원한다.</p>
<table><thead><tr><th>구성 요소 (Component)</th><th>주요 역할 (Primary Role)</th><th>핵심 기능 (Key Functions)</th><th>대표 예시 (Representative Examples)</th></tr></thead><tbody>
<tr><td><strong>Backbone</strong></td><td>특징 추출 (Feature Extraction)</td><td>입력 이미지로부터 계층적 특징 맵 생성</td><td>ResNet, ResNeXt, HRNet, Swin Transformer</td></tr>
<tr><td><strong>Neck</strong></td><td>특징 융합 (Feature Fusion)</td><td>다양한 스케일의 특징 맵을 결합하여 정보 강화</td><td>FPN, PAFPN, NAS-FPN, BiFPN</td></tr>
<tr><td><strong>Head</strong></td><td>예측 생성 (Prediction Generation)</td><td>강화된 특징 맵으로부터 최종 결과 예측</td><td><strong>DenseHead:</strong> RetinaHead, FCOSHead, YOLOV3Head <strong>RoIHead:</strong> StandardRoIHead (BBoxHead, MaskHead)</td></tr>
</tbody></table>
<h2>5.  실용적 구현 및 활용</h2>
<p>이론적 구조를 이해했다면, 다음 단계는 MMDetection을 실제로 설치하고 활용하는 방법을 익히는 것이다. 이 섹션에서는 환경 설정부터 기본적인 학습 및 추론 워크플로우까지의 과정을 단계별로 안내한다.</p>
<h3>5.1  설치 및 환경 설정</h3>
<p>MMDetection을 사용하기 위한 가장 안정적이고 권장되는 방법은 Conda 가상 환경을 구성하고 <code>mim</code> 도구를 사용하는 것이다.</p>
<ul>
<li><strong>표준 설치 절차:</strong></li>
</ul>
<ol>
<li>
<p><strong>Conda 가상 환경 생성 및 활성화:</strong> 먼저, 프로젝트를 위한 격리된 Python 환경을 생성한다.32</p>
<pre><code class="language-Bash">conda create -n openmmlab python=3.8 -y
conda activate openmmlab
</code></pre>
</li>
</ol>
<pre><code>
2. **PyTorch 설치:** 자신의 CUDA 버전에 맞는 PyTorch를 공식 홈페이지의 지침에 따라 설치한다.

     ```Bash
     # 예시: CUDA 11.8 버전용 PyTorch 설치
     conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
</code></pre>
<ol start="3">
<li>
<p><strong>MIM 및 핵심 라이브러리 설치:</strong> OpenMMLab 패키지 관리 도구인 <code>mim</code>을 설치하고, 이를 이용해 MMEngine과 MMCV를 설치한다.32</p>
<pre><code class="language-Bash">pip install -U openmim
mim install mmengine
mim install "mmcv&gt;=2.0.0"
</code></pre>
</li>
</ol>
<pre><code>
4. **MMDetection 설치:** 마지막으로 MMDetection을 설치한다. 설치 방식은 사용 목적에 따라 두 가지로 나뉜다.32

- **소스 코드 개발용 (Editable mode):** MMDetection 코드를 직접 수정하거나 내부 구조를 분석하며 개발할 경우, 소스 코드를 클론하여 편집 가능한 모드로 설치한다. 이 방식은 코드 변경 사항이 즉시 적용되는 장점이 있다.

       ```Bash
       git clone https://github.com/open-mmlab/mmdetection.git
       cd mmdetection
       pip install -v -e.
</code></pre>
<ul>
<li>
<p><strong>라이브러리 사용용 (Package mode):</strong> MMDetection을 하나의 라이브러리처럼 단순히 가져와서 사용할 경우, <code>mim</code>을 통해 간편하게 설치할 수 있다.</p>
<pre><code> ```Bash
 mim install mmdet
</code></pre>
</li>
</ul>
<pre><code>
- **Docker를 활용한 환경 구축:** 로컬 환경의 의존성 충돌을 피하고 싶거나, 재현 가능한 실행 환경을 보장하고 싶을 경우 Docker를 사용하는 것이 좋은 대안이다. MMDetection은 공식 Dockerfile을 제공하여, 필요한 모든 환경이 구성된 이미지를 손쉽게 빌드하고 실행할 수 있다.34

  ```Bash
  # Docker 이미지 빌드
  docker build -t mmdetection docker/
  # Docker 컨테이너 실행 (GPU 사용 및 데이터 볼륨 마운트)
  docker run --gpus all --shm-size=8g -it -v /path/to/your/data:/mmdetection/data mmdetection
</code></pre>
<h3>5.2  설정 파일 시스템 심층 탐구</h3>
<p>MMDetection의 모든 실험은 설정 파일(<code>.py</code>)을 통해 제어된다. 이 시스템의 핵심은 ‘상속(inheritance)’ 메커니즘으로, 코드의 중복을 최소화하고 실험 간의 차이점을 명확하게 관리할 수 있게 해준다.</p>
<ul>
<li>
<p><strong><code>_base_</code>를 이용한 상속:</strong> 대부분의 설정 파일은 <code>_base_</code> 라는 특별한 필드를 가지고 있다. 이 필드에 다른 설정 파일의 경로를 리스트 형태로 지정하면, 해당 파일들의 모든 설정을 그대로 가져와 기본값으로 사용한다. 그 후, 현재 파일에서 필요한 부분만 새로 정의하거나 덮어쓰면 된다.10</p>
</li>
<li>
<p><strong>예시:</strong> <code>configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py</code> 파일은 일반적으로 다음과 같은 기본 설정 파일들을 상속받는다.</p>
<pre><code class="language-Python">_base_ = [
    '../_base_/models/faster_rcnn_r50_fpn.py',  # 모델 아키텍처
    '../_base_/datasets/coco_detection.py',      # 데이터셋 설정
    '../_base_/schedules/schedule_1x.py',         # 학습 스케줄 (12 에포크)
    '../_base_/default_runtime.py'              # 로깅, 체크포인트 등 기본 런타임 설정
]
</code></pre>
</li>
</ul>
<pre><code>
- 만약 여기서 학습 에포크만 24로 늘리고 싶다면, 새로운 설정 파일을 만들고 위 파일을 `_base_`로 지정한 뒤, 학습 스케줄 관련 부분만 덮어쓰면 된다. 이 방식은 매우 효율적이며, 어떤 부분이 변경되었는지 직관적으로 파악할 수 있게 해준다.

- **커맨드 라인을 통한 동적 수정:** 때로는 설정 파일을 직접 수정하지 않고, 터미널에서 명령어를 실행하는 시점에 특정 파라미터만 임시로 변경하고 싶을 때가 있다. 이때 `--cfg-options` 인자를 사용하면 된다.21

  ```Bash
  # 데이터셋의 worker 수를 4로 변경하여 학습 실행
  python tools/train.py configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \
      --cfg-options data.workers_per_gpu=4
  
  # 모델 백본의 깊이를 101로 변경 (사전 학습 가중치도 맞춰야 함)
  python tools/train.py configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \
      --cfg-options model.backbone.depth=101
</code></pre>
<p>이 기능은 하이퍼파라미터 튜닝과 같이 동일한 기본 설정에서 일부 값만 바꿔가며 반복적인 실험을 수행할 때 매우 유용하다.</p>
<h3>5.3  표준 워크플로우: 학습 및 추론</h3>
<p>MMDetection은 학습, 테스트, 추론을 위한 표준화된 스크립트를 제공한다.</p>
<ul>
<li>
<p><strong>학습 (Training):</strong> <code>tools/train.py</code> 스크립트는 지정된 설정 파일을 읽어와 모델 학습을 시작한다. 단일 GPU 또는 다중 GPU 환경을 모두 지원한다.</p>
<pre><code class="language-Bash"># 단일 GPU 학습
python tools/train.py configs/rtmdet/rtmdet_tiny_8xb32-300e_coco.py

# 4개의 GPU를 사용한 분산 학습
./tools/dist_train.sh configs/rtmdet/rtmdet_tiny_8xb32-300e_coco.py 4
</code></pre>
</li>
</ul>
<pre><code>
학습 과정에서 생성된 로그 파일과 모델 체크포인트는 work_dirs/ 디렉토리 아래에 설정 파일 이름과 동일한 폴더에 저장된다.21

- **테스트 (Testing):** 학습이 완료된 모델의 성능을 평가하기 위해 `tools/test.py` 스크립트를 사용한다. 이 스크립트는 지정된 체크포인트 파일을 로드하여 테스트 데이터셋에 대한 추론을 수행하고, 설정 파일에 정의된 평가 지표(예: COCO mAP)를 계산하여 출력한다.

  ```Bash
  # 4개의 GPU를 사용하여 모델 성능 평가
  ./tools/dist_test.sh configs/rtmdet/rtmdet_tiny_8xb32-300e_coco.py 
  work_dirs/rtmdet_tiny_8xb32-300e_coco/epoch_300.pth 4 --out results.pkl
</code></pre>
<p>–out 옵션을 사용하면 예측 결과를 파일로 저장하여 추후 상세 분석에 활용할 수 있다.21</p>
<ul>
<li>
<p><strong>추론 (Inference):</strong> 학습된 모델을 사용하여 새로운 이미지나 비디오에 대한 예측을 수행하는 과정이다.</p>
</li>
<li>
<p><strong>데모 스크립트 사용:</strong> MMDetection은 이미지, 비디오, 웹캠 입력을 위한 간단한 데모 스크립트를 제공한다. 이는 모델의 작동을 빠르게 확인하는 데 유용하다.3</p>
<pre><code class="language-Bash"># 단일 이미지에 대한 추론 및 시각화
python demo/image_demo.py demo/demo.jpg \
    configs/rtmdet/rtmdet_tiny_8xb32-300e_coco.py \
    --weights /path/to/your/checkpoint.pth --device cpu
</code></pre>
</li>
</ul>
<pre><code>
- **Python API 사용:** 실제 애플리케이션에 통합하기 위해서는 Python 코드 내에서 직접 추론을 수행해야 한다. `mmdet.apis` 모듈의 `init_detector`와 `inference_detector` 함수를 사용하면 몇 줄의 코드로 간편하게 추론을 실행할 수 있다.

    ```Python
    from mmdet.apis import init_detector, inference_detector
    
    config_file = 'configs/rtmdet/rtmdet_tiny_8xb32-300e_coco.py'
    checkpoint_file = '/path/to/your/checkpoint.pth'
    
    # 모델 초기화
    model = init_detector(config_file, checkpoint_file, device='cuda:0')
    
    # 추론 수행
    result = inference_detector(model, 'demo/demo.jpg')
    
    # 결과 시각화 (옵션)
    model.show_result(
        'demo/demo.jpg',
        result,
        out_file='result.jpg'
    )
</code></pre>
<p>이 API 기반 접근 방식은 웹 서비스, 로봇 공학 등 다양한 응용 프로그램에 MMDetection 모델을 통합할 때 핵심적인 역할을 한다.</p>
<h2>6.  모델 라이브러리(Model Zoo) 및 성능 벤치마크</h2>
<p>MMDetection의 가장 큰 강점 중 하나는 최신 SOTA(State-of-the-Art) 모델부터 고전적인 모델에 이르기까지, 방대하고 체계적으로 관리되는 모델 라이브러리(Model Zoo)를 제공한다는 점이다.2 이 라이브러리는 250개가 넘는 사전 학습된 모델 가중치를 포함하고 있어, 사용자가 별도의 학습 없이도 즉시 고성능 모델을 활용할 수 있게 해준다.10</p>
<h3>6.1  주요 지원 모델 분석</h3>
<p>MMDetection이 지원하는 모델들은 객체 탐지 기술의 발전사를 보여주는 축소판과 같다. 주요 모델들을 계열별로 분류하고 그 특징을 분석하면 다음과 같다.</p>
<ul>
<li><strong>2-단계(Two-stage) 탐지기:</strong> 높은 정확도를 특징으로 하며, ’후보 영역 제안’과 ’분류 및 위치 보정’의 두 단계로 구성된다.</li>
<li><strong>Faster R-CNN:</strong> RPN(Region Proposal Network)을 도입하여 2-단계 탐지기 파이프라인을 최초로 종단간(end-to-end) 학습 가능하게 만든 선구적인 모델이다.8</li>
<li><strong>Mask R-CNN:</strong> Faster R-CNN을 확장하여 경계 상자 예측과 동시에 픽셀 단위의 인스턴스 분할 마스크를 예측하는 기능을 추가했다.8</li>
<li><strong>Cascade R-CNN:</strong> 탐지 헤드를 여러 단계로 쌓아(cascade) 점진적으로 더 정확한 예측을 수행하도록 설계된 모델로, 특히 높은 IoU(Intersection over Union) 기준에서 뛰어난 성능을 보인다.8</li>
<li><strong>Hybrid Task Cascade (HTC):</strong> Cascade R-CNN에 인스턴스 분할과 시맨틱 분할 작업을 유기적으로 결합하여 상호 보완적인 정보를 활용, 전반적인 성능을 향상시킨 모델이다.</li>
<li><strong>1-단계(Single-stage) 탐지기:</strong> 후보 영역 제안 없이 특징 맵에서 직접 예측을 수행하여 빠른 속도를 자랑한다.</li>
<li><strong>SSD (Single Shot MultiBox Detector):</strong> 다양한 스케일의 특징 맵을 사용하여 여러 크기의 객체를 한 번에 탐지하는 고전적인 1-단계 모델이다.8</li>
<li><strong>RetinaNet:</strong> 극심한 전경-배경 클래스 불균형 문제를 해결하기 위해 Focal Loss를 제안하여 1-단계 탐지기의 정확도를 2-단계 탐지기 수준으로 끌어올렸다.8</li>
<li><strong>YOLOv3:</strong> 속도와 정확도의 균형을 맞춘 실시간 탐지 모델의 대명사로, MMDetection에서도 지원된다.</li>
<li><strong>Anchor-Free 탐지기:</strong> 사전 정의된 앵커 박스(anchor box)에 의존하지 않고 객체를 직접 예측하는 방식이다. 하이퍼파라미터 튜닝이 비교적 간단하고 유연성이 높다.</li>
<li><strong>FCOS (Fully Convolutional One-Stage Object Detection):</strong> 픽셀 단위로 객체의 중심점과 경계까지의 거리를 예측하는 방식을 사용한다.8</li>
<li><strong>CenterNet:</strong> 객체를 중심점(keypoint)으로 간주하고, 히트맵(heatmap)을 통해 중심점을 예측한 후 객체의 크기 등 다른 속성들을 회귀하는 방식을 취한다.</li>
<li><strong>Transformer 기반 탐지기:</strong> 자연어 처리 분야에서 성공을 거둔 Transformer 아키텍처를 비전에 도입한 모델들로, NMS와 같은 복잡한 후처리 과정이 필요 없는 종단간 학습이 가능하다.</li>
<li><strong>DETR (DEtection TRansformer):</strong> CNN 백본과 Transformer 인코더-디코더 구조를 결합하여 객체 탐지를 집합 예측(set prediction) 문제로 재정의했다.</li>
<li><strong>Deformable DETR:</strong> DETR의 느린 수렴 속도와 작은 객체 탐지 성능 문제를 해결하기 위해 Deformable Attention 메커니즘을 도입했다.</li>
<li><strong>DINO:</strong> Deformable DETR을 기반으로 대조적인 노이즈 제거(contrastive denoising) 학습 방식을 추가하여 SOTA 성능을 달성한 모델이다.36</li>
<li><strong>최신 고성능 실시간 탐지기:</strong></li>
<li><strong>RTMDet (Real-Time Models for object Detection):</strong> OpenMMLab이 자체 개발한 고성능 실시간 탐지기로, 백본과 넥의 용량을 균형 있게 설계하고 대형 커널 컨볼루션을 활용하여 YOLO 시리즈를 능가하는 속도-정확도 균형을 달성했다.1</li>
<li><strong>오픈 보캐뷸러리 / 그라운딩 탐지기:</strong> 사전 정의된 클래스 목록을 넘어, 자연어 텍스트 입력을 기반으로 객체를 탐지하거나 위치를 찾는(grounding) 모델이다.</li>
<li><strong>GLIP, Grounding DINO:</strong> 비전-언어 사전 학습을 통해 뛰어난 제로샷(zero-shot) 및 오픈 보캐뷸러리 탐지 성능을 보여준다.36</li>
<li><strong>MM-Grounding-DINO:</strong> MMDetection은 공식 Grounding DINO의 학습 코드가 공개되지 않은 상황에서, 이를 자체적으로 재구현하고 데이터 및 학습 전략을 개선하여 원본을 능가하는 성능을 달성한 MM-Grounding-DINO를 공개했다. 이는 MMDetection이 단순한 모델 구현체를 넘어, SOTA 연구를 주도하는 플랫폼임을 보여주는 사례이다.1</li>
</ul>
<table><thead><tr><th>계열 (Category)</th><th>모델명 (Model Name)</th><th>핵심 특징 (Key Feature)</th><th>설정 파일 예시 (Config Example Path)</th></tr></thead><tbody>
<tr><td><strong>2-단계 (Two-Stage)</strong></td><td>Faster R-CNN</td><td>RPN을 통한 종단간 학습 가능한 2-단계 탐지기의 표준</td><td><code>configs/faster_rcnn/</code></td></tr>
<tr><td></td><td>Mask R-CNN</td><td>Faster R-CNN에 마스크 예측 브랜치를 추가하여 인스턴스 분할 지원</td><td><code>configs/mask_rcnn/</code></td></tr>
<tr><td></td><td>Cascade R-CNN</td><td>다단계 헤드 구조를 통해 점진적으로 예측 정확도 향상</td><td><code>configs/cascade_rcnn/</code></td></tr>
<tr><td><strong>1-단계 (Single-Stage)</strong></td><td>RetinaNet</td><td>Focal Loss를 도입하여 1-단계 탐지기의 클래스 불균형 문제 해결</td><td><code>configs/retinanet/</code></td></tr>
<tr><td></td><td>YOLOv3</td><td>속도와 정확도의 뛰어난 균형을 갖춘 실시간 탐지 모델</td><td><code>configs/yolo/</code></td></tr>
<tr><td><strong>Anchor-Free</strong></td><td>FCOS</td><td>앵커 박스 없이 픽셀 단위로 객체를 예측하는 완전 컨볼루션 방식</td><td><code>configs/fcos/</code></td></tr>
<tr><td></td><td>CenterNet</td><td>객체를 중심점으로 간주하고 히트맵을 통해 예측</td><td><code>configs/centernet/</code></td></tr>
<tr><td><strong>Transformer 기반</strong></td><td>DETR</td><td>Transformer를 활용한 최초의 종단간 객체 탐지 모델</td><td><code>configs/detr/</code></td></tr>
<tr><td></td><td>DINO</td><td>Deformable DETR 기반에 노이즈 제거 학습을 추가하여 SOTA 달성</td><td><code>configs/dino/</code></td></tr>
<tr><td><strong>실시간 (Real-Time)</strong></td><td>RTMDet</td><td>YOLO를 능가하는 속도-정확도 균형을 목표로 OpenMMLab이 개발</td><td><code>configs/rtmdet/</code></td></tr>
<tr><td><strong>오픈 보캐뷸러리</strong></td><td>Grounding DINO</td><td>자연어 구문(phrase)을 이해하여 해당 객체의 위치를 찾는 그라운딩 모델</td><td><code>configs/mm_grounding_dino/</code></td></tr>
</tbody></table>
<h3>6.2  성능 벤치마크</h3>
<p>MMDetection은 제공하는 모든 모델에 대해 MS COCO 2017 데이터셋을 기준으로 엄격한 벤치마크 결과를 함께 제공한다. 이는 연구자들이 모델을 선택하고 성능을 비교할 때 객관적인 기준을 제공한다.8</p>
<ul>
<li><strong>주요 성능 지표:</strong></li>
<li><strong>정확도 (mAP):</strong> COCO 데이터셋의 표준 평가 지표인 mAP(mean Average Precision)를 사용한다. IoU 임계값을 0.5부터 0.95까지 0.05 간격으로 변화시키며 계산한 AP의 평균값이다.</li>
<li><strong>추론 속도 (FPS):</strong> 단일 GPU에서 초당 처리할 수 있는 이미지의 수(Frames Per Second)로, 모델의 실시간성을 평가하는 중요한 척도이다.</li>
<li><strong>학습 시간 (s/iter):</strong> 학습 시 하나의 배치(iteration)를 처리하는 데 걸리는 시간으로, 모델의 학습 효율성을 나타낸다.</li>
<li><strong>GPU 메모리 사용량 (GB):</strong> 학습 시 필요한 최대 GPU 메모리 양으로, 모델을 학습시키기 위한 하드웨어 요구사항을 결정한다.</li>
<li><strong>RTMDet 성능 예시:</strong> 최신 모델인 RTMDet의 경우, 가장 큰 모델인 RTMDet-X는 COCO 데이터셋에서 52.8%의 높은 AP를 달성하면서도 NVIDIA 3090 GPU에서 300 FPS 이상의 빠른 추론 속도를 보여준다. 이는 기존의 산업 표준이었던 YOLO 계열 모델들을 능가하는 뛰어난 속도-정확도 트레이드오프를 달성했음을 의미한다.1</li>
</ul>
<h3>6.3  비교 분석: MMDetection vs. Detectron2</h3>
<p>Detectron2는 Facebook AI Research(FAIR)에서 개발한 또 다른 주요 객체 탐지 프레임워크로, MMDetection의 가장 직접적인 비교 대상이다. 두 프레임워크의 성능을 비교할 때는 단순한 수치뿐만 아니라 그 배경에 있는 실험 설정의 차이를 이해하는 것이 중요하다.</p>
<p>MMDetection 공식 문서에서 제공하는 벤치마크에 따르면, 두 프레임워크의 성능은 유사한 수준이다. 예를 들어, Faster R-CNN R-50-FPN 1x 스케줄 모델의 경우, Detectron2는 37.9 AP, MMDetection은 38.0 AP를 기록했다. Mask R-CNN의 경우에도 Detectron2는 38.6(bbox) / 35.2(mask) AP, MMDetection은 38.8 / 35.4 AP로 대동소이한 결과를 보였다.</p>
<p>이러한 성능 수치를 해석할 때, 프레임워크 자체의 본질적인 우열보다는 ’실험 설정의 투명성’과 ’재현성’의 중요성을 인식해야 한다. 두 프레임워크 간의 미세한 성능 차이는 종종 코드의 근본적인 효율성 차이가 아니라, 기본적으로 적용되는 학습 하이퍼파라미터의 차이에서 비롯되는 경우가 많다. 예를 들어, Detectron2는 기본적으로 다중 스케일 학습(multi-scale training)을 사용하는 경향이 있는 반면, MMDetection의 초기 벤치마크는 단일 스케일 학습을 기준으로 하는 경우가 많았다.10 실제로 동일한 학습 설정(학습률 스케줄, 데이터 증강 방식 등)을 적용하면 두 프레임워크의 성능은 거의 동일한 수준으로 수렴하는 것으로 알려져 있다.10</p>
<p>이는 MMDetection의 설정 파일 시스템이 갖는 중요한 가치를 시사한다. MMDetection의 Model Zoo는 단순히 사전 학습된 가중치 파일의 모음이 아니라, 해당 성능을 정확하게 재현할 수 있는 모든 실험 설정이 명시된 ’레시피(설정 파일)’의 저장소 역할을 한다. 이러한 접근 방식은 연구의 재현성을 보장하고, 다른 연구자들이 결과를 공정하게 비교할 수 있는 기반을 제공한다는 점에서 매우 큰 의미를 갖는다.</p>
<table><thead><tr><th>모델 타입 (Model Type)</th><th>학습 스케줄 (Lr schd)</th><th>Detectron2 (AP)</th><th>mmdetection (AP)</th><th>학습 속도 (s/iter, mmdet)</th><th>추론 속도 (fps, mmdet)</th><th>학습 메모리 (GB, mmdet)</th></tr></thead><tbody>
<tr><td>Faster R-CNN</td><td>1x</td><td>37.9</td><td>38.0</td><td>0.216</td><td>22.2</td><td>3.8</td></tr>
<tr><td>Mask R-CNN</td><td>1x</td><td>38.6 (bbox) 35.2 (mask)</td><td>38.8 (bbox) 35.4 (mask)</td><td>0.265</td><td>19.6</td><td>3.9</td></tr>
<tr><td>RetinaNet</td><td>1x</td><td>36.5</td><td>37.0</td><td>0.205</td><td>20.6</td><td>3.4</td></tr>
</tbody></table>
<p><em>주: 위 표의 성능은 R-50-FPN 백본을 사용한 결과이며, MMDetection v2.0 벤치마크 기준임.</em></p>
<h2>7.  고급 사용자 정의 기법</h2>
<p>MMDetection의 진정한 힘은 제공된 모델을 사용하는 것을 넘어, 사용자가 자신의 필요에 맞게 프레임워크를 확장하고 수정할 수 있는 능력에서 나온다. 이 섹션에서는 사용자 정의 데이터셋을 활용하는 방법과 새로운 컴포넌트를 직접 개발하여 MMDetection에 통합하는 고급 기법을 다룬다.</p>
<h3>7.1  사용자 정의 데이터셋 활용</h3>
<p>대부분의 실제 프로젝트에서는 COCO나 Pascal VOC와 같은 표준 데이터셋이 아닌, 자체적으로 구축한 데이터셋을 사용해야 한다. MMDetection은 이를 위해 세 가지 주요 방법을 제공한다.21</p>
<ol>
<li><strong>COCO 포맷으로 변환 (Reorganize to COCO format):</strong> 가장 권장되는 방법이다. 자신의 데이터셋을 COCO와 동일한 JSON 어노테이션 형식과 디렉토리 구조로 변환하는 것이다. 일단 변환이 완료되면, <code>configs/_base_/datasets/coco_detection.py</code>와 같은 기존 설정 파일을 거의 수정 없이 재사용할 수 있어 매우 편리하다. 클래스 이름과 데이터 경로만 수정해주면 된다.</li>
<li><strong>중간 포맷으로 변환 (Reorganize to a middle format):</strong> MMDetection은 모든 어노테이션 정보를 로드한 후, 내부적으로 <code>dict</code> 객체들의 리스트 형태로 변환하여 사용한다. 따라서, 자신의 데이터셋을 이 중간 포맷으로 변환하는 스크립트를 작성하고, <code>CustomDataset</code> 클래스를 사용하여 이를 로드할 수 있다. 이는 COCO 포맷으로의 완전한 변환이 번거로울 때 유용한 대안이다.</li>
<li><strong>새로운 Dataset 클래스 구현 (Implement a new dataset):</strong> 데이터셋의 형식이 매우 독특하여 기존 클래스로는 처리하기 어려운 경우, <code>BaseDataset</code>을 상속받아 자신만의 새로운 Dataset 클래스를 직접 구현할 수 있다. 이 방법은 가장 유연하지만, <code>load_data_list</code>와 같은 핵심 메소드를 직접 구현해야 하므로 가장 많은 노력이 필요하다.</li>
</ol>
<p><strong>설정 파일 수정 예시:</strong> 사용자 정의 데이터셋을 사용하기 위해서는 설정 파일에서 <code>data</code> 딕셔너리 부분을 수정해야 한다.</p>
<pre><code class="language-Python"># 데이터셋 타입 및 경로 설정
dataset_type = 'CocoDataset'
data_root = 'data/my_custom_dataset/'

# 클래스 정보 정의
metainfo = {
    'classes': ('class_a', 'class_b', 'class_c', ),
}

# 학습, 검증, 테스트 데이터 로더 설정
train_dataloader = dict(
    batch_size=2,
    num_workers=2,
    dataset=dict(
        type=dataset_type,
        data_root=data_root,
        metainfo=metainfo,
        ann_file='annotations/train.json',
        data_prefix=dict(img='train/'),
      ...
    ))
# val_dataloader, test_dataloader도 유사하게 설정
...
</code></pre>
<p>이처럼 설정 파일에서 데이터셋 관련 경로, 파일 이름, 클래스 정보를 명확하게 지정함으로써 MMDetection이 사용자 정의 데이터를 인식하고 학습에 사용할 수 있도록 한다.21</p>
<h3>7.2  새로운 컴포넌트 개발</h3>
<p>MMDetection의 모듈식 설계는 사용자가 Backbone, Neck, Head, 심지어 Loss 함수까지 자신만의 새로운 컴포넌트를 개발하고 손쉽게 통합할 수 있도록 지원한다. 이는 MMDetection을 단순한 라이브러리가 아닌, 새로운 아이디어를 실험하는 ’프레임워크’로 만들어주는 핵심 기능이다.21</p>
<p>이러한 확장은 MMDetection의 ‘레지스트리(Registry)’ 메커니즘을 통해 이루어진다. 레지스트리는 특정 유형의 모듈(예: <code>BACKBONES</code>, <code>NECKS</code>, <code>HEADS</code>)들을 이름(문자열)과 실제 클래스를 매핑하여 관리하는 전역적인 등록소 역할을 한다.</p>
<p><strong>새로운 컴포넌트 추가 과정 (예: 새로운 Backbone):</strong></p>
<ol>
<li>
<p><strong>클래스 정의 및 등록:</strong> 먼저, 새로운 컴포넌트에 대한 Python 클래스를 정의한다. 예를 들어, <code>MyBackbone</code>이라는 새로운 백본을 만들고 싶다면, <code>torch.nn.Module</code>을 상속받아 클래스를 구현한다. 그리고 이 클래스 위에 <code>@MODELS.register_module()</code> (또는 이전 버전의 경우 <code>@BACKBONES.register_module()</code>)이라는 데코레이터를 추가한다. 이 데코레이터가 <code>MyBackbone</code> 클래스를 <code>MODELS</code>라는 전역 레지스트리에 ’MyBackbone’이라는 이름으로 자동 등록하는 역할을 한다.25</p>
<pre><code class="language-Python"># mmdet/models/backbones/my_backbone.py
import torch.nn as nn
from mmdet.registry import MODELS

@MODELS.register_module()
class MyBackbone(nn.Module):
    def __init__(self, arg1, arg2):
        super().__init__()
        #... 네트워크 레이어 초기화...
        pass

    def forward(self, x):
        #... 순전파 로직 구현...
        # 특징 맵의 튜플을 반환해야 함
        return (feat1, feat2, feat3)
</code></pre>
</li>
</ol>
<pre><code>
2. **모듈 임포트:** MMDetection이 새로 작성한 파일을 인식할 수 있도록 해야 한다. 가장 간단한 방법은 설정 파일에 `custom_imports` 필드를 추가하는 것이다.

   ```Python
   # 설정 파일 내
   custom_imports = dict(
       imports=['mmdet.models.backbones.my_backbone'],
       allow_failed_imports=False)
</code></pre>
<p>이 설정을 통해 MMDetection은 실행 시 지정된 경로의 Python 파일을 임포트하고, 데코레이터에 의해 <code>MyBackbone</code> 클래스가 레지스트리에 등록된다.</p>
<ol start="3">
<li>
<p><strong>설정 파일에서 사용:</strong> 이제 설정 파일의 <code>model</code> 딕셔너리에서 <code>backbone</code>의 <code>type</code> 필드에 방금 등록한 클래스 이름(‘MyBackbone’)을 문자열로 지정하기만 하면 된다. MMDetection은 레지스트리에서 ’MyBackbone’이라는 이름을 찾아 해당 클래스를 인스턴스화하고, 함께 제공된 다른 인자들(<code>arg1</code>, <code>arg2</code>)을 생성자에 전달한다.</p>
<pre><code class="language-Python"># 설정 파일 내
model = dict(
  ...
    backbone=dict(
        type='MyBackbone',  # 레지스트리에 등록된 이름 사용
        arg1=123,
        arg2='abc'),
  ...
)
</code></pre>
</li>
</ol>
<pre><code>
이러한 레지스트리 기반의 확장 메커니즘은 Neck, Head, Loss 등 다른 모든 컴포넌트에도 동일하게 적용된다. 이를 통해 사용자는 MMDetection의 핵심 코드를 전혀 수정하지 않고도 프레임워크의 기능을 무한히 확장할 수 있으며, 이는 MMDetection이 학술 연구 커뮤니티에서 높은 평가를 받는 핵심적인 이유이다.

## 8.  결론 및 전망


MMDetection은 지난 몇 년간 객체 탐지 분야에서 가장 영향력 있는 오픈 소스 프로젝트 중 하나로 확고히 자리매김했다. 본 안내서는 MMDetection의 설계 철학, 핵심 아키텍처, 실용적 활용법, 그리고 OpenMMLab 생태계 내에서의 역할을 종합적으로 분석하였다. 이를 바탕으로 MMDetection의 강점과 약점을 요약하고, 프로젝트의 미래 발전 방향을 전망하며 최종적인 평가를 내린다.

### 8.1  MMDetection의 강점과 약점 요약


- **강점:**
- **압도적인 모듈성과 유연성:** Backbone, Neck, Head로 구성된 모듈식 설계는 연구자들이 새로운 아이디어를 신속하게 프로토타이핑하고 검증할 수 있는 독보적인 유연성을 제공한다.1
- **방대한 모델 라이브러리:** 고전적인 모델부터 최신 SOTA 모델까지 아우르는 포괄적인 모델 라이브러리와 사전 학습된 가중치를 제공하여, 강력한 베이스라인을 손쉽게 구축할 수 있다.2
- **연구 재현성 보장:** 설정 파일 기반의 워크플로우는 모든 실험 조건을 명시적으로 문서화하여, 연구 결과의 재현성을 높이는 데 크게 기여한다.2
- **OpenMMLab 생태계와의 시너지:** MMPretrain, MMDeploy 등 다른 툴박스와의 긴밀한 연동을 통해 사전 학습부터 모델 배포까지의 엔드투엔드 파이프라인을 지원한다.6
- **약점:**
- **높은 초기 학습 곡선:** 설정 파일 시스템과 내부 아키텍처의 복잡성 및 추상화 수준이 높아, 초심자가 프레임워크의 작동 방식을 완전히 이해하고 자유자재로 활용하기까지 상당한 시간과 노력이 필요하다.3
- **패키지 호환성 문제:** MMDetection은 MMEngine, MMCV 등 여러 라이브러리에 의존하고 있으며, 이들 간의 버전 호환성이 맞지 않을 경우 설치나 실행 과정에서 예기치 않은 오류가 발생할 수 있다.14

### 8.2  프로젝트의 발전 방향성


MMDetection은 현재에 안주하지 않고 지속적으로 발전하며 컴퓨터 비전 기술의 최전선을 이끌고 있다. 최근의 업데이트들은 프로젝트의 미래 방향성을 명확하게 보여준다.

- **최신 SOTA 모델의 지속적인 통합:** RTMDet과 같은 고효율 실시간 탐지 모델을 자체적으로 개발하고, Grounding DINO와 같은 최신 비전-언어 모델을 빠르게 재구현하고 개선(MM-Grounding-DINO)하는 등, 학계와 산업계의 최신 기술 트렌드를 적극적으로 수용하고 선도하고 있다.1 이는 MMDetection이 앞으로도 가장 최신의, 가장 강력한 모델들을 가장 먼저 접할 수 있는 플랫폼이 될 것임을 시사한다.
- **다중 모달리티 및 다중 작업을 향한 확장:** MMEngine을 기반으로 한 아키텍처 통합은 MMDetection의 미래가 단순한 객체 탐지를 넘어, 텍스트, 오디오 등 다른 모달리티와 결합하고, 탐지, 분할, 캡셔닝 등 여러 작업을 동시에 수행하는 더 크고 일반화된 '파운데이션 모델' 연구를 지원하는 방향으로 나아갈 것임을 암시한다.

### 8.3  최종 평가


결론적으로, MMDetection은 단순한 객체 탐지 라이브러리를 넘어, 컴퓨터 비전 연구와 개발을 위한 포괄적이고 강력하며 확장 가능한 **'플랫폼'**으로 평가할 수 있다. 가파른 학습 곡선이라는 진입 장벽이 존재함에도 불구하고, MMDetection이 제공하는 탁월한 유연성, 방대한 모델 지원, 그리고 엄격한 재현성은 복잡하고 도전적인 연구 과제를 수행하는 연구자들과 고성능 모델을 필요로 하는 개발자들에게 대체 불가능한 가치를 제공한다. OpenMMLab 2.0 아키텍처를 통해 미래 비전 연구를 위한 기반을 다진 MMDetection은, 앞으로도 컴퓨터 비전 커뮤니티의 발전을 이끄는 핵심적인 동력으로 기능할 것으로 전망된다.

## 9. 참고 자료


1. open-mmlab/mmdetection: OpenMMLab Detection Toolbox and Benchmark - GitHub, https://github.com/open-mmlab/mmdetection
2. MMDetection in Computer Vision - GeeksforGeeks, https://www.geeksforgeeks.org/computer-vision/mmdetection-in-computer-vision/
3. Comprehensive Guide to Mastering MMDetection (MMDet) for Object Detection - Ikomia, https://www.ikomia.ai/blog/ultimate-guide-mmdetection-mmdet-object-detection
4. [1906.07155] MMDetection: Open MMLab Detection Toolbox and Benchmark - arXiv, https://arxiv.org/abs/1906.07155
5. OpenMMLab - GitHub, https://github.com/open-mmlab
6. OpenMMLab 2.0: new architecture, algorithm, and ecology - Medium, https://openmmlab.medium.com/openmmlab-2-0-new-architecture-algorithm-and-ecology-cf568fa0fc12
7. ma-xu/detic-mmdet: OpenMMLab Detection Toolbox and Benchmark - GitHub, https://github.com/ma-xu/detic-mmdet
8. [1906.07155] MMDetection: Open MMLab Detection Toolbox and Benchmark - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/1906.07155
9. MMDetection: Open MMLab Detection Toolbox and Benchmark - ResearchGate, https://www.researchgate.net/publication/333841608_MMDetection_Open_MMLab_Detection_Toolbox_and_Benchmark
10. MMDetection V2.0：A Faster and Stronger General Detection Framework | by OpenMMLab, https://openmmlab.medium.com/mmdetection-v2-0-a-faster-and-stronger-general-detection-framework-94943c83ab91
11. r/computervision on Reddit: Object Detection using PyTorch: Would you recommend a Framework (Detectron2, MMDetection, ...) or a project from scratch, https://www.reddit.com/r/computervision/comments/12m2cdf/object_detection_using_pytorch_would_you/
12. Comprehensive Guide to CV Pipelines with Detectron2 and MMDetection - Rapid Innovation, https://www.rapidinnovation.io/post/building-a-computer-vision-pipeline-with-detectron2-and-mmdetection
13. Dos actually many people use MMDetection as their framework to work on object detection?, https://www.reddit.com/r/computervision/comments/1b4k11s/dos_actually_many_people_use_mmdetection_as_their/
14. MMDetection vs. Detectron2 for Instance Segmentation — Which Framework Would You Recommend? - Reddit, https://www.reddit.com/r/computervision/comments/1jxplnp/mmdetection_vs_detectron2_for_instance/
15. MMEngine | Weights &amp; Biases Documentation - Wandb, https://docs.wandb.ai/guides/integrations/mmengine/
16. Introduction — mmengine 0.10.0 documentation, https://mmengine.readthedocs.io/en/v0.10.0/get_started/introduction.html
17. OpenMMLab/mmcv - GitLink, https://www.gitlink.org.cn/OpenMMLab/mmcv
18. open-mmlab/mmcv: OpenMMLab Computer Vision Foundation - GitHub, https://github.com/open-mmlab/mmcv
19. Changelog of v3.x — MMDetection 3.3.0 documentation, https://mmdetection.readthedocs.io/en/main/notes/changelog.html
20. Welcome to MMDetection's documentation! — MMDetection 3.3.0 documentation, https://mmdetection.readthedocs.io/
21. Welcome to MMDetection's documentation! — MMDetection 2.28.2 ..., https://mmdetection.readthedocs.io/en/dev/
22. Deep Learning based Neck Models for Object Detection: A Review and a Benchmarking Study - The Science and Information (SAI) Organization, https://thesai.org/Downloads/Volume12No11/Paper_19-Deep_Learning_based_Neck_Models_for_Object_Detection.pdf
23. A detection model contains a backbone, neck, head module. The backbone... | Download Scientific Diagram - ResearchGate, https://www.researchgate.net/figure/A-detection-model-contains-a-backbone-neck-head-module-The-backbone-module-exploits_fig1_356638292
24. What is an object detection "head"? - Stack Overflow, https://stackoverflow.com/questions/53012856/what-is-an-object-detection-head
25. Tutorial 4: Customize Models - MMDetection's documentation! - Read the Docs, https://mmdetection.readthedocs.io/en/dev/tutorials/customize_models.html
26. Customize Models — MMDetection 3.3.0 documentation, https://mmdetection.readthedocs.io/en/stable/advanced_guides/customize_models.html
27. mmdetection/demo/MMDet_InstanceSeg_Tutorial.ipynb at main - GitHub, https://github.com/open-mmlab/mmdetection/blob/main/demo/MMDet_InstanceSeg_Tutorial.ipynb
28. OVERVIEW — MMDetection 3.3.0 documentation, https://mmdetection.readthedocs.io/en/latest/overview.html
29. [arXiv 2019/Paper Notes] Step-by-Step Guide: Installing and Configuring MMDetection for Object Detection Projects | by Alifya Febriana | Medium, https://medium.com/@alifyafebriana/cvpr-2019-paper-notes-step-by-step-guide-installing-and-configuring-mmdetection-for-object-bfc5aea2c457
30. Use MMDetection with arcgis.learn | ArcGIS API for Python - Esri Developer, https://developers.arcgis.com/python/latest/guide/use-mmdetection-with-arcgis-learn/
31. Dataflow — MMSegmentation 1.2.2 documentation, https://mmsegmentation.readthedocs.io/en/main/advanced_guides/data_flow.html
32. GET STARTED — MMDetection 3.3.0 documentation, https://mmdetection.readthedocs.io/en/latest/get_started.html
33. GET STARTED — MMDetection 3.3.0 documentation, https://mmdetection.readthedocs.io/en/stable/get_started.html
34. A Practical Guide to Object Detection using MMDetection with Docker - Medium, https://medium.com/@rezaie.j/a-practical-guide-to-object-detection-using-mmdetection-with-docker-c7c480d997f4
35. MMDetection/docs/INSTALL.md at master - GitHub, https://github.com/davidhalladay/MMDetection/blob/master/docs/INSTALL.md
36. V3Det/mmdetection-V3Det: OpenMMLab Detection Toolbox and Benchmark for V3Det - GitHub, https://github.com/V3Det/mmdetection-V3Det
37. arXiv:2212.07784v2 [cs.CV] 16 Dec 2022, https://arxiv.org/pdf/2212.07784
38. Lower performances than Detectron2 · Issue #4019 · open-mmlab/mmdetection - GitHub, https://github.com/open-mmlab/mmdetection/issues/4019
</code></pre>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>