<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:RTMDet (Real-Time Models for Object Detection, 2022-12-14)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>RTMDet (Real-Time Models for Object Detection, 2022-12-14)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>RTMDet (Real-Time Models for Object Detection, 2022-12-14)</span></nav>
                </div>
            </header>
            <article>
                <h1>RTMDet (Real-Time Models for Object Detection, 2022-12-14)</h1>
<h2>1. 서론: 실시간 객체 탐지의 새로운 패러다임, RTMDet</h2>
<h3>1.1 RTMDet의 등장 배경과 목표: YOLO 시리즈를 넘어서</h3>
<p>RTMDet(Real-Time Models for Object Detection)은 YOLO(You Only Look Once) 시리즈를 능가하는 효율적인 실시간 객체 탐지기 설계를 목표로 개발되었다.1 2022년 발표 당시, 실시간 객체 탐지 분야는 YOLO 계열 모델들이 지배하고 있었으나, 이들은 복잡한 모델 구조나 특정 라이선스 정책으로 인해 실제 산업 현장에서의 활용에 일부 제약을 가지고 있었다.4 이러한 배경 속에서 RTMDet은 단순히 객체 탐지 성능을 넘어 인스턴스 분할(instance segmentation), 회전 객체 탐지(rotated object detection) 등 다양한 인식 작업으로의 손쉬운 확장을 핵심 목표로 설정했다.3</p>
<p>RTMDet의 개발 목표는 단순히 기술적 우위를 점하는 것을 넘어, 시장에서의 전략적 위치 선정을 깊이 고려한 결과물로 분석된다. 당시 유행하던 재매개변수화(re-parameterization) 기법을 의도적으로 배제하고 3, 상업적 활용에 제약이 없는 아파치 2.0(Apache-2.0) 라이선스를 채택한 것은 4 기술적 선택인 동시에 명확한 비즈니스적 결정이다. 이는 연구 커뮤니티뿐만 아니라, 라이선스에 민감한 기업 사용자를 명확한 대상으로 설정했음을 시사한다. 즉, RTMDet는 ‘기술적으로 우수하고, 사용하기 편하며, 상업적으로 자유로운’ 실시간 탐지기의 새로운 표준을 지향한다.</p>
<p>더 나아가 RTMDet의 성공은 모델 자체의 성능을 넘어, OpenMMLab이라는 강력한 생태계에 기반한다.6 MMDetection, MMYOLO, MMRotate, MMDeploy와 같은 포괄적인 툴체인은 RTMDet의 연구, 개발, 배포 전 과정을 유기적으로 지원한다.8 사용자는 단일 모델이 아닌, 잘 갖춰진 개발 환경 전체를 활용할 수 있게 되므로, 이는 다른 독립적인 모델들에 비해 강력한 진입장벽이자 매력적인 요소로 작용한다. 따라서 RTMDet의 기여는 모델 아키텍처의 혁신뿐만 아니라, 강력한 오픈소스 생태계와 결합하여 시너지를 창출한 성공 사례라는 점에서 더 큰 의의를 찾을 수 있다.</p>
<h2>2. RTMDet 아키텍처 해부</h2>
<p>RTMDet은 백본(Backbone), 넥(Neck), 헤드(Head)로 구성된 전형적인 단일-단계(one-stage) 탐지기 구조를 따른다. 그러나 각 구성요소의 설계에는 효율성과 성능을 극대화하기 위한 독창적인 철학이 담겨 있다.</p>
<h3>2.1 백본 (Backbone): CSPNeXt의 설계 철학</h3>
<p>RTMDet의 백본은 CSPNeXt라 명명되었으며, 이는 CSPDarkNet을 기반으로 설계되었다.3 이 백본의 핵심은 기본 빌딩 블록에 5x5 크기의 대형 커널(large-kernel) 깊이별 컨볼루션(depth-wise convolution)을 도입하여 유효 수용 필드(effective receptive field)를 효과적으로 확장한 데 있다.3 이를 통해 모델은 더 넓은 영역의 문맥 정보를 포착하여 정확도를 향상시킨다. 동시에, 깊이 증가로 인한 추론 속도 저하를 방지하기 위해 각 스테이지의 블록 수를 줄이는 대신 채널의 너비를 확장하여, 연산 효율성과 모델 용량 간의 정교한 균형을 맞추었다.3</p>
<p>이러한 설계는 중요한 철학적 선택을 내포한다. RTMDet은 재매개변수화 기법을 의도적으로 사용하지 않았다.3 재매개변수화는 훈련 시에는 복잡한 구조를, 추론 시에는 단순한 구조를 사용하여 속도를 높이는 기법이지만, 훈련과 추론 간의 불일치를 유발하고 모델 양자화(quantization) 과정에서 성능 저하의 원인이 될 수 있다.3 RTMDet은 구조적으로 더 단순한 대형 커널 깊이별 컨볼루션을 채택함으로써 훈련-추론 간의 일관성을 확보하고 배포 용이성을 높였다. 이는 학술적 성능 지표뿐만 아니라, 실제 산업 현장에서의 안정성과 예측 가능성을 더 중요하게 고려한 실용주의적 접근법이라 할 수 있다.</p>
<h3>2.2 넥 (Neck): CSP-PAFPN을 통한 특징 융합 최적화</h3>
<p>넥은 백본에서 추출된 다단계 특징 맵을 융합하여 다양한 크기의 객체를 효과적으로 탐지할 수 있도록 지원한다. RTMDet은 CSP 블록이 적용된 PAFPN(Path Aggregation Feature Pyramid Network) 구조를 채택했다.3</p>
<p>RTMDet 넥 설계의 가장 큰 특징은 ‘백본-넥 용량 균형’ 전략에 있다. 전통적인 탐지기들이 백본에 대부분의 연산량을 집중시키는 것과 달리, RTMDet은 백본에서 넥으로 더 많은 파라미터와 연산을 의도적으로 할당한다.3 이를 통해 넥이 백본과 유사한 수준의 용량(compatible capacity)을 갖도록 설계했다.3 이는 모델 전체를 하나의 유기적인 시스템으로 보고 연산 예산을 최적으로 재분배하는 접근법이다. 백본이 추출한 저수준 특징을 충분한 용량을 가진 넥에서 정교하게 융합하고 정제하는 것이 최종 성능에 더 유리하다는 실험적 결론에 기반한 것이다. 이는 단순히 각 컴포넌트를 개별적으로 강화하는 것을 넘어, 컴포넌트 간의 상호작용과 전체적인 정보 흐름을 최적화하는 시스템 수준의 설계 혁신으로 평가된다.</p>
<h3>2.3 헤드 (Head): 분리형 정규화를 통한 효율 극대화</h3>
<p>탐지 헤드는 넥에서 전달받은 특징 맵을 기반으로 객체의 경계 상자(bounding box)와 클래스를 예측하는 역할을 한다. RTMDet은 여러 스케일의 특징 맵에 걸쳐 컨볼루션 레이어의 가중치를 공유(shared weights)하되, 배치 정규화(Batch Normalization, BN) 레이어는 분리하여 사용하는 독특한 구조를 채택했다.3</p>
<p>이 설계는 특징 피라미드 아키텍처의 본질적 특성을 깊이 이해하고 활용한 결과이다. 특징 피라미드의 각 레벨은 서로 다른 스케일의 객체를 주로 담당하므로, 각 레벨의 특징 맵은 통계적 분포가 상이하다. 만약 BN 레이어까지 공유한다면 모든 스케일의 통계량을 평균 내어 정규화하게 되므로 최적의 성능을 기대하기 어렵다. RTMDet은 객체의 일반적인 패턴을 학습하는 컨볼루션 필터는 공유하여 파라미터 효율성을 높이는 동시에, 각 스케일의 고유한 통계적 특성은 분리된 BN 레이어가 독립적으로 학습하도록 설계했다. 이는 파라미터 효율성과 성능 사이의 매우 영리한 절충안이라 할 수 있다.</p>
<h2>3. 핵심 기술 혁신: 정확도와 효율성의 동시 달성</h2>
<p>RTMDet의 우수한 성능은 아키텍처뿐만 아니라, 훈련 과정을 최적화하는 핵심 기술 혁신에 기인한다.</p>
<h3>3.1 동적 소프트 레이블 할당 (Dynamic Soft Label Assignment)</h3>
<p>RTMDet은 훈련 과정에서 예측과 실제 정답(Ground Truth, GT) 간의 할당 방식을 개선하기 위해 동적 소프트 레이블 할당 기법을 제안했다.2 이는 기존의 동적 레이블 할당 기법인 SimOTA 등의 한계를 극복하기 위해 매칭 비용(matching cost) 계산 시 하드 레이블(hard label) 대신 소프트 레이블(soft label)을 도입한 것이다.3</p>
<p>전체 비용 함수 <span class="math math-inline">C</span>는 분류 비용 <span class="math math-inline">C_{cls}</span>, 회귀 비용 <span class="math math-inline">C_{reg}</span>, 중심 사전 비용 <span class="math math-inline">C_{center}</span>의 가중합으로 정의된다.3</p>
<p><span class="math math-display">
C = \lambda_1 C_{cls} + \lambda_2 C_{reg} + \lambda_3 C_{center}
</span><br />
여기서 각 비용은 다음과 같이 개선되었다.</p>
<ul>
<li>
<p><strong>분류 비용 (<span class="math math-inline">C_{cls}</span>)</strong>: 예측과 GT 박스 간의 IoU(Intersection over Union) 값을 소프트 레이블 <span class="math math-inline">Y_{soft}</span>로 사용하여 분류 비용을 계산한다. 이를 통해 분류 점수와 위치 정확도 간의 불일치 문제를 해결한다.3</p>
<p><span class="math math-display">
C_{cls} = CE(P, Y_{soft}) \times (Y_{soft} - P)^2
</span></p>
</li>
<li>
<p><strong>회귀 비용 (<span class="math math-inline">C_{reg}</span>)</strong>: 기존의 GIoU 대신 <span class="math math-inline">-\log(IoU)</span>를 회귀 비용으로 사용하여, IoU가 낮은 예측에 더 큰 페널티를 부여함으로써 고품질 예측과 저품질 예측 간의 변별력을 높였다.3</p>
<p><span class="math math-display">
C_{reg} = -\log(IoU)
</span></p>
</li>
</ul>
<p>이러한 소프트 레이블링 기법은 단일-단계 탐지기 훈련의 고질적인 문제인 ’불안정성’을 완화하는 데 핵심적인 역할을 한다. 기존 방식은 분류 점수는 높지만 위치가 부정확한 예측이 낮은 비용을 가질 수 있는 ’모호성’이 존재했다. RTMDet은 위치 정확도(IoU)를 분류 비용에 직접 통합함으로써 이러한 모호성을 제거한다. 위치가 부정확한 예측은 분류 점수가 아무리 높아도 높은 비용을 갖게 되므로, 훈련 초기에 모델이 ‘어디에’ 집중해야 하는지를 더 명확하고 안정적으로 안내한다. 즉, 이 기법은 단순한 정확도 향상을 넘어, 훈련 과정 자체를 안정화시켜 더 빠르고 확실하게 고성능 모델을 생성하게 하는 핵심 메커니즘이다.</p>
<h3>3.2 최적화된 훈련 전략</h3>
<p>RTMDet은 데이터 증강 및 최적화 과정에서도 독창적인 전략을 사용한다. 훈련 과정은 크게 두 단계로 나뉜다. 초기 280 에포크 동안은 캐시된(cached) Mosaic 및 MixUp 데이터 증강을 사용하여 모델이 다양한 객체 문맥과 형태를 폭넓게 학습하도록 유도한다. 이후 마지막 20 에포크에서는 이러한 강력한 증강을 끄고, 대신 LSJ(Large Scale Jittering)를 사용하여 학습된 특징을 실제 데이터 분포에 맞게 미세 조정한다.3</p>
<p>이 2단계 증강 전략은 일종의 ’커리큘럼 학습(Curriculum Learning)’으로 해석될 수 있다. 1단계에서 강력하지만 노이즈가 많은 증강 기법을 통해 모델이 과적합을 피하고 강건한 특징 표현을 학습하도록 돕는다. 2단계에서는 상대적으로 ‘깨끗한’ 증강을 통해 모델을 정교하게 다듬는다. 어블레이션 연구에서 이 전략이 1.5%에서 2.0%에 달하는 AP(Average Precision) 향상을 가져왔다는 사실은 3, 훈련 단계에 따라 최적의 데이터 증강 방식이 다르다는 것을 명확히 보여준다. 최적화기로는 AdamW를, 학습률 스케줄러로는 Flat-Cosine 방식을 채택하여 훈련 안정성을 더욱 높였다.3</p>
<h2>4. 성능 분석 및 비교</h2>
<h3>4.1 RTMDet 모델군 성능 평가 (COCO val2017)</h3>
<p>RTMDet은 tiny부터 extra-large(x)까지 총 5가지 크기의 모델을 제공하여, 사용자가 자신의 하드웨어 및 애플리케이션 요구사항에 맞춰 최적의 모델을 선택할 수 있는 유연성을 제공한다.2 각 모델의 성능은 COCO val2017 데이터셋에서 측정되었으며, 그 결과는 아래 표와 같다.</p>
<p><strong>표 1: RTMDet 모델 버전별 성능 지표 (COCO val2017)</strong></p>
<table><thead><tr><th>모델</th><th>파라미터 (M)</th><th>FLOPs (G)</th><th>지연 시간 (ms)</th><th>FPS</th><th>AP (%)</th></tr></thead><tbody>
<tr><td>RTMDet-tiny</td><td>4.8</td><td>8.1</td><td>0.98</td><td>~1020</td><td>41.1</td></tr>
<tr><td>RTMDet-s</td><td>8.99</td><td>14.8</td><td>1.22</td><td>~819</td><td>44.6</td></tr>
<tr><td>RTMDet-m</td><td>24.7</td><td>39.3</td><td>1.62</td><td>~617</td><td>49.4</td></tr>
<tr><td>RTMDet-l</td><td>52.3</td><td>80.2</td><td>2.40</td><td>~417</td><td>51.5</td></tr>
<tr><td>RTMDet-x</td><td>94.9</td><td>141.7</td><td>3.10</td><td>~323</td><td>52.8</td></tr>
</tbody></table>
<p>주: 지연 시간 및 FPS는 NVIDIA 3090 GPU, TensorRT 8.4.3, FP16, 배치 크기=1 환경에서 NMS를 제외하고 측정됨.3</p>
<p>분석 결과, 모델 크기(파라미터, FLOPs)가 증가함에 따라 정확도(AP)는 선형적으로 향상되는 반면, 속도(FPS)는 점차 감소하는 명확한 트레이드오프 관계를 보인다. 이는 다양한 시나리오에 대응할 수 있는 잘 설계된 모델군임을 입증한다.</p>
<h3>4.2 주요 실시간 탐지기와의 성능 비교</h3>
<p>RTMDet은 동시대의 다른 최신 실시간 객체 탐지 모델들과 비교했을 때 모든 스케일에서 우수한 파라미터-정확도 효율성을 보여준다.</p>
<p><strong>표 2: RTMDet과 주요 실시간 탐지기 성능 비교 (COCO val2017)</strong></p>
<table><thead><tr><th>모델</th><th>파라미터 (M)</th><th>FLOPs (G)</th><th>지연 시간 (ms)</th><th>AP (%)</th></tr></thead><tbody>
<tr><td>YOLOX-s</td><td>9.0</td><td>13.4</td><td>1.20</td><td>40.5</td></tr>
<tr><td>PPYOLOE-s</td><td>7.9</td><td>8.7</td><td>1.34</td><td>43.0</td></tr>
<tr><td>YOLOv6-s</td><td>17.2</td><td>22.1</td><td>0.92</td><td>43.5</td></tr>
<tr><td><strong>RTMDet-s</strong></td><td><strong>8.99</strong></td><td><strong>14.8</strong></td><td><strong>1.22</strong></td><td><strong>44.6</strong></td></tr>
<tr><td>YOLOX-l</td><td>54.2</td><td>77.8</td><td>2.19</td><td>49.7</td></tr>
<tr><td>PPYOLOE-l</td><td>52.2</td><td>55.0</td><td>2.57</td><td>51.4</td></tr>
<tr><td>YOLOv7</td><td>36.9</td><td>52.4</td><td>2.63</td><td>51.2</td></tr>
<tr><td><strong>RTMDet-l</strong></td><td><strong>52.3</strong></td><td><strong>80.2</strong></td><td><strong>2.40</strong></td><td><strong>51.5</strong></td></tr>
<tr><td>YOLOX-x</td><td>99.1</td><td>141.0</td><td>2.98</td><td>51.1</td></tr>
<tr><td>PPYOLOE-x</td><td>98.4</td><td>103.3</td><td>3.07</td><td>52.3</td></tr>
<tr><td><strong>RTMDet-x</strong></td><td><strong>94.9</strong></td><td><strong>141.7</strong></td><td><strong>3.10</strong></td><td><strong>52.8</strong></td></tr>
</tbody></table>
<p>주: 지연 시간 측정 조건은 표 1과 동일함.3</p>
<p>표에서 볼 수 있듯이, RTMDet은 모든 모델 스케일에서 유사한 크기의 다른 모델들보다 더 적은 파라미터와 연산량으로 더 높거나 동등한 수준의 정확도를 달성한다. 특히 경량 모델인 RTMDet-s는 YOLOv6-s의 절반 수준의 파라미터로 더 높은 정확도를 기록하는 등 뛰어난 효율성을 입증했다.3</p>
<p>다만, 벤치마크 결과를 해석할 때 숨겨진 비용을 고려해야 한다. 논문에 제시된 300+ FPS와 같은 수치는 1 TensorRT FP16과 같이 고도로 최적화된 배포 환경에서 측정된 ’최고 속도’이다.10 일반적인 PyTorch 환경에서의 추론 속도는 이보다 현저히 느릴 수 있다.13 이는 RTMDet 아키텍처가 배포 최적화 도구와 매우 잘 통합된다는 강점을 보여주는 동시에, 사용자는 자신의 배포 환경에 맞는 현실적인 성능 기대를 가져야 함을 시사한다. 따라서 RTMDet의 성능은 단일 숫자가 아닌, ’PyTorch 기본 성능’부터 ’TensorRT 최적화 성능’까지의 스펙트럼으로 이해해야 하며, 이는 모델의 잠재력을 최대한 활용하기 위해 배포 단계의 최적화가 필수적임을 의미한다.</p>
<h3>4.3 확장 태스크 성능 평가</h3>
<p>RTMDet의 아키텍처는 범용성이 뛰어나 최소한의 구조 변경만으로 다양한 다운스트림 태스크에서 최고 수준의 성능을 달성한다. 이는 RTMDet의 백본과 넥이 매우 강력하고 일반화된 특징 추출기임을 입증하는 증거이다.</p>
<p><strong>표 3: RTMDet 확장 태스크 성능 요약</strong></p>
<table><thead><tr><th>태스크</th><th>데이터셋</th><th>모델</th><th>성능 지표</th><th>FPS (3090)</th></tr></thead><tbody>
<tr><td>인스턴스 분할</td><td>COCO</td><td>RTMDet-Ins-l</td><td>44.6% (Mask AP)</td><td>188</td></tr>
<tr><td>회전 객체 탐지</td><td>DOTA 1.0</td><td>RTMDet-R-l</td><td>81.3% (mAP)</td><td>121</td></tr>
</tbody></table>
<p>주: FPS는 TensorRT FP16, 배치 크기=1 환경에서 측정됨.3</p>
<h2>5. RTMDet 실제 활용 가이드 (MMDetection 기반)</h2>
<h3>5.1 환경 설정 및 사전 훈련 모델 활용</h3>
<p>RTMDet은 OpenMMLab의 MMDetection과 MMYOLO 툴박스를 통해 공식적으로 배포된다.6 사용자는</p>
<p><code>pip</code>과 <code>mim</code> 명령어를 통해 MMEngine, MMCV, MMDetection과 같은 필수 라이브러리를 손쉽게 설치할 수 있다.12</p>
<p>MMDetection은 다양한 크기와 태스크에 맞춰 사전 훈련된 RTMDet 모델과 설정 파일을 제공한다. 이를 통해 사용자는 처음부터 모델을 훈련할 필요 없이, 검증된 모델을 즉시 사용하거나 파인튜닝의 기반으로 활용할 수 있다.</p>
<p><strong>표 4: 사전 훈련된 RTMDet 모델 및 설정 파일</strong></p>
<table><thead><tr><th>모델</th><th>태스크</th><th>설정 파일 이름</th></tr></thead><tbody>
<tr><td>RTMDet-tiny</td><td>객체 탐지</td><td><code>rtmdet_tiny_8xb32-300e_coco.py</code></td></tr>
<tr><td>RTMDet-s</td><td>객체 탐지</td><td><code>rtmdet_s_8xb32-300e_coco.py</code></td></tr>
<tr><td>RTMDet-m</td><td>객체 탐지</td><td><code>rtmdet_m_8xb32-300e_coco.py</code></td></tr>
<tr><td>RTMDet-l</td><td>객체 탐지</td><td><code>rtmdet_l_8xb32-300e_coco.py</code></td></tr>
<tr><td>RTMDet-x</td><td>객체 탐지</td><td><code>rtmdet_x_8xb32-300e_coco.py</code></td></tr>
<tr><td>RTMDet-Ins-tiny</td><td>인스턴스 분할</td><td><code>rtmdet-ins_tiny_8xb32-300e_coco.py</code></td></tr>
<tr><td>RTMDet-Ins-l</td><td>인스턴스 분할</td><td><code>rtmdet-ins_l_8xb32-300e_coco.py</code></td></tr>
</tbody></table>
<p>주: 전체 모델 리스트는 MMDetection 공식 저장소에서 확인 가능함.10</p>
<h3>5.2 추론(Inference) 실행</h3>
<p>MMDetection 3.x 버전부터 도입된 <code>DetInferencer</code> API를 사용하면 단 몇 줄의 코드로 간편하게 추론을 실행할 수 있다.15 이 API는 이미지 경로, NumPy 배열, URL, 디렉토리 등 다양한 형태의 입력을 지원한다.15</p>
<p>아래는 <code>DetInferencer</code>를 사용하여 사전 훈련된 RTMDet-tiny 모델로 이미지 추론을 수행하는 완전한 Python 코드 예시이다.</p>
<pre><code class="language-Python">from mmdet.apis import DetInferencer

# 1. 'rtmdet_tiny_8xb32-300e_coco' 모델 이름으로 DetInferencer 초기화
# MMDetection이 자동으로 사전 훈련된 가중치를 다운로드함
inferencer = DetInferencer(model='rtmdet_tiny_8xb32-300e_coco')

# 2. 추론할 이미지 경로 지정 (예: 'demo/demo.jpg')
image_path = 'demo/demo.jpg'

# 3. 추론 실행
# show=True 옵션은 결과를 팝업 창으로 보여줌 (GUI 환경 필요)
# out_dir='outputs/' 옵션으로 결과를 파일로 저장할 수 있음
results = inferencer(image_path, show=True)

# 4. 결과 확인
# results 딕셔너리에는 'predictions'와 'visualization' 키가 포함됨
print("추론 결과:")
# 'predictions' 키는 탐지된 객체의 레이블, 점수, 경계 상자 정보를 포함
for pred in results['predictions']:
    print(f"  레이블: {pred['labels']}")
    print(f"  점수: {pred['scores']}")
    print(f"  경계 상자: {pred['bboxes']}")
</code></pre>
<h3>5.3 커스텀 데이터셋을 이용한 파인튜닝(Fine-tuning)</h3>
<p>사전 훈련된 RTMDet 모델은 커스텀 데이터셋에 대한 파인튜닝을 통해 특정 도메인에 최적화될 수 있다.14 MMDetection의 유연성은 강력한 장점이지만, 초보 사용자에게는 복잡한 설정 파일 시스템이 가장 큰 진입 장벽으로 작용할 수 있다.6 성공적인 파인튜닝의 핵심은 기존 설정 파일을 자신의 데이터셋에 맞게 정확하게 수정하는 것이다.</p>
<p>파인튜닝 과정은 다음과 같다.</p>
<ol>
<li>
<p><strong>데이터셋 준비</strong>: 커스텀 데이터셋을 COCO 형식으로 변환한다.6</p>
</li>
<li>
<p><strong>설정 파일 수정</strong>: 사전 훈련된 모델의 설정 파일(<code>.py</code>)을 복사하여 수정한다. <code>_base_</code>를 통한 상속 구조를 활용하면 변경이 용이하다.17 필수적으로 수정해야 할 주요 항목은 다음과 같다.</p>
</li>
</ol>
<ul>
<li>
<p><code>data_root</code>: 데이터셋 경로</p>
</li>
<li>
<p><code>metainfo</code>: 클래스 이름(<code>classes</code>)</p>
</li>
<li>
<p><code>num_classes</code>: 헤드의 클래스 수 (기존 클래스 수와 동일하게 설정)</p>
</li>
<li>
<p><code>load_from</code>: 사용할 사전 훈련 가중치 파일 경로</p>
</li>
<li>
<p><code>max_epochs</code>: 총 훈련 에포크 수</p>
</li>
</ul>
<ol start="3">
<li><strong>훈련 실행</strong>: <code>tools/train.py</code> 스크립트를 사용하여 훈련을 시작한다.14</li>
</ol>
<h3>5.4 모델 배포 (Deployment)</h3>
<p>MMDetection은 MMDeploy 툴킷을 통해 훈련된 모델을 ONNX, TensorRT 등 다양한 추론 백엔드로 변환하는 기능을 지원한다.10 이를 통해 RTMDet 모델을 서버, 엣지 디바이스 등 다양한 환경에 효율적으로 배포할 수 있다. 최상의 추론 성능을 위해서는 입력 크기를 고정하는 정적(static) 형태로 모델을 변환하는 것이 권장된다.10 이는 RTMDet이 연구 단계를 넘어 실제 제품에 탑재될 수 있도록 엔드-투-엔드 파이프라인을 갖추고 있음을 보여준다.</p>
<h2>6. 결론: RTMDet의 종합 평가 및 전망</h2>
<h3>6.1 RTMDet의 핵심 강점 요약</h3>
<p>RTMDet은 실시간 객체 탐지 분야에서 괄목할 만한 성과를 이룬 모델로, 다음과 같은 핵심 강점을 가진다.</p>
<ul>
<li>
<p><strong>최상의 성능 효율성</strong>: 모든 모델 스케일에서 속도, 정확도, 파라미터 간의 최상의 트레이드오프를 달성하여 다양한 하드웨어 제약 조건 하에서 최적의 선택지를 제공한다.</p>
</li>
<li>
<p><strong>뛰어난 확장성</strong>: 객체 탐지를 넘어 인스턴스 분할, 회전 객체 탐지 등 여러 컴퓨터 비전 태스크로 손쉽게 확장 가능한 유연한 아키텍처를 갖추고 있다.</p>
</li>
<li>
<p><strong>높은 사용성 및 허용적 라이선스</strong>: OpenMMLab이라는 강력한 생태계를 통해 개발, 훈련, 배포가 용이하며, 아파치 2.0 라이선스를 채택하여 상업적 활용에 대한 장벽이 없다.</p>
</li>
</ul>
<h3>6.2 실시간 객체 탐지 분야에 미친 영향 및 향후 전망</h3>
<p>RTMDet은 YOLO의 아성을 위협하며 실시간 객체 탐지기의 새로운 기준점을 제시했다. RTMDet의 성공은 향후 개발될 실시간 탐지기들이 단순히 속도와 정확도라는 2차원적 지표를 넘어 다차원적인 요소를 고려해야 함을 시사한다. 여기에는 아키텍처의 단순성과 배포 용이성, 다양한 다운스트림 태스크로의 확장성, 허용적인 라이선스 정책, 그리고 강력한 오픈소스 생태계의 지원이 포함된다.</p>
<p>미래의 연구는 RTMDet의 효율적인 아키텍처를 기반으로 Transformer 19나 새로운 메커니즘을 결합하여 원격 탐사, 의료 영상 등 특정 도메인에 더욱 최적화된 형태로 발전할 가능성이 높다. RTMDet은 그 자체로도 뛰어난 모델이지만, 차세대 실시간 탐지기들의 개발 방향성에 중요한 영향을 미치는 ’게임 체인저’로서의 역할을 수행했다고 평가할 수 있다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>[2212.07784] RTMDet: An Empirical Study of Designing Real-Time Object Detectors - ar5iv, https://ar5iv.labs.arxiv.org/html/2212.07784</li>
<li>RTMDet: An Empirical Study of Designing Real-Time Object Detectors - SciSpace, https://scispace.com/papers/rtmdet-an-empirical-study-of-designing-real-time-object-3dqveh0b</li>
<li>RTMDet: An Empirical Study of Designing Real-Time Object Detectors, https://arxiv.org/pdf/2212.07784</li>
<li>IS YOLO V8 the fastest and the most accurate algorithm for real time ? : r/computervision, https://www.reddit.com/r/computervision/comments/19diuab/is_yolo_v8_the_fastest_and_the_most_accurate/</li>
<li>RTMDet: An Empirical Study of Designing Real-Time Object Detectors - Semantic Scholar, https://www.semanticscholar.org/paper/RTMDet%3A-An-Empirical-Study-of-Designing-Real-Time-Lyu-Zhang/99ccc15094b9438448668d136d90a12e0c9443e1</li>
<li>How to Train RTMDet on a Custom Dataset - Roboflow Blog, https://blog.roboflow.com/how-to-train-rtmdet-on-a-custom-dataset/</li>
<li>ma-xu/detic-mmdet: OpenMMLab Detection Toolbox and Benchmark - GitHub, https://github.com/ma-xu/detic-mmdet</li>
<li>open-mmlab/mmdetection: OpenMMLab Detection Toolbox and Benchmark - GitHub, https://github.com/open-mmlab/mmdetection</li>
<li>rtmdet · GitHub Topics, https://github.com/topics/rtmdet</li>
<li>external/det/configs/rtmdet/README.md · facebook/sapiens-pose at 2d4ff18f0b4690b2c70b8bbe10889bc846ebcaba - Hugging Face, https://huggingface.co/spaces/facebook/sapiens-pose/blob/2d4ff18f0b4690b2c70b8bbe10889bc846ebcaba/external/det/configs/rtmdet/README.md</li>
<li>Algorithm principles and implementation with RTMDet - MMYOLO’s documentation!, https://mmyolo.readthedocs.io/en/latest/recommended_topics/algorithm_descriptions/rtmdet_description.html</li>
<li>train-rtmdet-object-detection-on-custom-data.ipynb - Colab, https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-rtmdet-object-detection-on-custom-data.ipynb</li>
<li>Implement the RTMDET target detection algorithm, and test its FPS. It is found that only 7-8, not the official 300+ #10037 - GitHub, https://github.com/open-mmlab/mmdetection/issues/10037</li>
<li>makeabilitylab/mmdet-fine-tuning: Fine-tuning RTMDet for Instance Segmentation - GitHub, https://github.com/makeabilitylab/mmdet-fine-tuning</li>
<li>Inference with existing models — MMDetection 3.3.0 documentation, https://mmdetection.readthedocs.io/en/latest/user_guides/inference.html</li>
<li>inference_demo.ipynb - Colab, https://colab.research.google.com/github/open-mmlab/mmdetection/blob/dev-3.x/demo/inference_demo.ipynb</li>
<li>A variety of object detection models How to use MMdetection | by MLBoy - Medium, https://rockyshikoku.medium.com/a-variety-of-object-detection-models-how-to-use-mmdetection-d35f75a01fd6</li>
<li>RTMDet: An Empirical Study of Designing Real-Time Object Detectors - Gitea, https://git.chiebot.com:10000/Chiebot-Mirror/mmdetection/src/commit/8a5b70f54dbff1dc4702454aaf1a222156a99ed8/configs/rtmdet/README.md</li>
<li>Baidu’s RT-DETR: A Vision Transformer-Based Real-Time Object Detector - Ultralytics Docs, https://docs.ultralytics.com/models/rtdetr/</li>
<li>PP-YOLOE+ vs RTDETRv2: A Technical Comparison - Ultralytics YOLO Docs, https://docs.ultralytics.com/compare/pp-yoloe-vs-rtdetr/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>