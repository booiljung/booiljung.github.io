<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Detectron2Go</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Detectron2Go</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>Detectron2Go</span></nav>
                </div>
            </header>
            <article>
                <h1>Detectron2Go</h1>
<h2>1. 서론: 최첨단 연구와 온디바이스 배포의 간극 해소</h2>
<h3>1.1 Detectron에서 Detectron2로의 진화: 모듈성 및 PyTorch로의 전환</h3>
<p>컴퓨터 비전 분야에서 Meta AI Research(FAIR)가 개발한 Detectron 라이브러리는 객체 탐지 및 분할 알고리즘의 표준으로 자리매김했다. 그러나 Caffe2 프레임워크를 기반으로 한 초기 Detectron은 연구의 빠른 반복과 새로운 아이디어의 유연한 적용에 있어 구조적 한계를 보였다.1 이러한 한계를 극복하고 차세대 연구 플랫폼을 제공하기 위해, FAIR는 PyTorch를 기반으로 Detectron2를 완전히 재작성했다.2</p>
<p>이러한 전환은 단순한 프레임워크 변경을 넘어선 전략적 결정이었다. Detectron2는 처음부터 모듈성(modularity), 확장성(extensibility), 그리고 효율성(efficiency)을 핵심 설계 원칙으로 삼았다.2 PyTorch의 직관적인 개발 방식과 동적 계산 그래프는 연구자들이 모델의 어느 부분이든 손쉽게 수정하고 실험할 수 있는 환경을 제공했다.1 또한, 전체 훈련 파이프라인을 GPU로 이전함으로써 훈련 속도를 획기적으로 개선했다.1 Detectron에서 Detectron2로의 진화는 단순한 업데이트가 아닌, 근본적인 아키텍처 재설계를 통한 패러다임의 전환이었다. 이 과정에서 확보된 모듈성과 확장성은 이후 Detectron2Go와 같은 프로덕션 지향 확장 기능 개발의 필수적인 기술적 토대가 되었다. 즉, 연구 중심의 라이브러리를 프로덕션 시스템으로 전환할 수 있는 잠재력은 Detectron2의 설계 철학에 이미 내재되어 있었던 것이다.</p>
<h3>1.2 엣지 AI의 필요성: Detectron2Go의 등장 배경</h3>
<p>인공지능 모델의 복잡성이 증가함에 따라, 서버-클라우드 기반의 중앙 집중식 처리 방식은 여러 가지 본질적인 문제에 직면했다. 첫째, 데이터가 디바이스에서 클라우드로 전송되고 처리된 후 다시 디바이스로 돌아오는 과정에서 발생하는 지연 시간(latency)은 자율 주행이나 증강 현실(AR)과 같은 실시간 상호작용이 필수적인 애플리케이션에서는 수용 불가능한 수준이다.6 둘째, 사용자의 개인적인 이미지를 외부 서버로 전송하는 것에 대한 개인정보 보호(privacy) 우려가 점차 커지고 있었다.6</p>
<p>Detectron2Go(D2Go)는 이러한 문제들에 대한 Meta의 직접적인 해답으로 등장했다. D2Go의 핵심 철학은 모델을 중앙 서버가 아닌 사용자의 디바이스, 즉 ’엣지(edge)’에서 직접 실행하는 것이다.6 이를 통해 데이터 전송에 소요되는 시간을 제거하여 지연 시간을 극적으로 줄이고, 모든 데이터를 디바이스 내에서 처리함으로써 사용자의 개인정보를 원천적으로 보호한다.7 D2Go의 개발은 단순히 기술적 최적화를 넘어, 실시간 상호작용이라는 비즈니스 요구와 데이터 주권이라는 윤리적 고려에 부응하는, 엣지 AI로의 산업적 전환을 반영하는 중요한 이정표라 할 수 있다.</p>
<h3>1.3 시스템 개요: 모바일 비전을 위한 엔드투엔드 툴킷</h3>
<p>Detectron2Go는 “프로덕션 준비가 완료된 소프트웨어 시스템(production-ready software system)“으로 정의되며, 모델 훈련부터 모바일 배포에 이르는 전 과정을 지원하는 포괄적인 엔드투엔드(end-to-end) 파이프라인을 제공한다.6 이 시스템은 개발자에게 모델 훈련, 양자화(quantization), 그리고 배포를 위한 TorchScript 형식으로의 간편한 내보내기(export) 등 핵심 기능들을 통합적으로 제공한다.9</p>
<p>D2Go는 Meta의 컴퓨터 비전 분야 “연구-프로덕션” 수직 통합 파이프라인에서 마지막 핵심 연결고리 역할을 수행한다. 이는 FAIR의 최첨단 연구 성과(예: FBNet과 같은 새로운 아키텍처)가 Portal, Instagram과 같은 대중적인 제품에 원활하게 배포될 수 있도록 하는 기술적 통로이다.2 이처럼 D2Go는 혁신적인 연구 아이디어가 실제 제품 가치로 빠르게 전환되는 선순환 생태계를 구축함으로써, Meta의 기술 혁신과 제품 개발 속도를 가속화하는 핵심 동력으로 기능한다.</p>
<h2>2. 코어 아키텍처 및 개발자 워크플로우</h2>
<h3>2.1 시스템 구성 요소: Detectron2, PyTorch Mobile, TorchVision의 통합</h3>
<p>Detectron2Go의 아키텍처는 기존의 강력하고 안정적인 오픈소스 프로젝트들을 전략적으로 통합하여 구축되었다. 그 기반은 세 가지 핵심 라이브러리로 구성된다. 첫째, Detectron2는 Faster R-CNN, Mask R-CNN 등 최첨단 비전 알고리즘의 구현체를 제공하는 핵심 엔진 역할을 한다. 둘째, PyTorch Mobile은 훈련된 모델이 모바일 환경에서 효율적으로 실행될 수 있도록 하는 온디바이스 추론 프레임워크를 제공한다. 마지막으로, TorchVision은 모바일 환경에 최적화된 다양한 비전 연산(operation)과 유틸리티를 지원한다.6 D2Go는 이 세 가지 구성 요소 위에 정교한 추상화 계층을 추가하여, 복잡한 모바일 배포 과정을 단순화하고 일관된 개발 경험을 제공한다.</p>
<h3>2.2 개발 파이프라인: 설치부터 추론까지</h3>
<h4>2.2.1 환경 설정 및 의존성</h4>
<p>D2Go를 사용하기 위한 초기 환경 설정은 몇 가지 특정 의존성 설치를 요구한다. 안정적인 최신 기능을 활용하기 위해 PyTorch의 Nightly 버전을 설치해야 하며, Detectron2는 사전 컴파일된 패키지가 아닌 소스 코드로부터 직접 빌드하여 설치해야 한다. 마지막으로, 모바일 비전 관련 특화 기능을 담고 있는 <code>mobile_cv</code> 라이브러리를 설치해야 전체 개발 환경이 완성된다.9 이러한 과정은 D2Go가 최신 연구 성과와 모바일 최적화 기술을 긴밀하게 통합하고 있음을 보여준다.</p>
<h4>2.2.2 데이터 준비: COCO와 같은 표준 형식의 역할</h4>
<p>머신러닝 프로젝트에서 데이터 준비는 종종 가장 많은 시간과 노력을 요구하는 단계이지만, D2Go는 데이터 형식의 표준화를 통해 이 과정을 크게 간소화한다. D2Go는 Detectron2의 “표준 데이터셋 사전(standard dataset dict)” 구조를 그대로 계승하며, 특히 COCO JSON 형식을 기본 데이터 포맷으로 채택하고 있다.1 개발자는 <code>register_coco_instances</code>와 같은 유틸리티 함수를 사용하여 자신의 커스텀 데이터셋을 시스템에 손쉽게 등록할 수 있다.9</p>
<p>이처럼 COCO와 같은 표준화된 데이터 형식에 대한 엄격한 준수는 D2Go가 약속하는 “엔드투엔드” 파이프라인의 기술적 근간을 이룬다. 이는 프로젝트마다 달라지는 데이터 엔지니어링의 복잡성을 제거하고, 데이터 주석부터 모델 평가에 이르는 전체 워크플로우를 일관되게 만들어 시스템 구성 요소 간의 상호 운용성을 보장한다.</p>
<h4>2.2.3 훈련 패러다임: CLI 및 API 활용</h4>
<p>D2Go는 개발자의 다양한 요구사항에 맞춰 두 가지 형태의 훈련 인터페이스를 제공한다. 첫째, 명령줄 인터페이스(CLI)인 <code>d2go.train_net</code>은 COCO와 같은 내장 데이터셋을 사용하여 표준적인 모델을 훈련할 때 빠르고 간편한 방법을 제공한다.9 둘째, Python API는 커스텀 데이터셋을 사용하거나 훈련 로직의 세밀한 제어가 필요할 때 유연성을 제공한다.9</p>
<p>특히 D2Go의 API는 기존 Detectron2의 API와 거의 동일한 구문과 구조를 갖도록 의도적으로 설계되었다.9 이는 Detectron2에 익숙한 방대한 연구 및 개발 커뮤니티의 학습 곡선을 최소화하여 D2Go의 채택을 가속화하기 위한 전략적 결정이다. 이를 통해 연구자들은 모바일 배포를 훈련 이후의 복잡한 후처리 단계가 아닌, 개발 생애 주기의 자연스러운 일부로 고려하게 된다.</p>
<h4>2.2.4 모델 내보내기: 프로덕션용 TorchScript 아티팩트 생성</h4>
<p>훈련 및 최적화가 완료된 모델은 실제 프로덕션 환경에 배포되기 위해 이식성이 높고 효율적인 형식으로 변환되어야 한다. D2Go는 이 과정을 위해 TorchScript로의 내보내기 기능을 제공한다.9 TorchScript는 PyTorch 모델을 정적 그래프 형태로 직렬화하여 Python 의존성 없이 C++와 같은 환경에서 실행할 수 있게 해주는 포맷이다.</p>
<p><code>d2go.exporter</code> 도구를 사용하면 단일 명령으로 훈련된 모델 가중치와 구성 파일을 TorchScript 모델(<code>.pt</code> 또는 <code>.pth</code> 파일)로 변환할 수 있으며, 이 아티팩트는 PyTorch Mobile을 통해 안드로이드 및 iOS 애플리케이션에 직접 통합될 수 있다.9</p>
<h2>3. 모바일 효율성을 위한 고급 최적화</h2>
<h3>3.1 신경망 아키텍처 탐색(NAS): 하드웨어 인식 설계 자동화</h3>
<h4>3.1.1 미분 가능한 NAS(DNAS) 프레임워크</h4>
<p>전통적인 신경망 설계는 전문가의 직관과 수많은 시행착오에 의존했지만, 신경망 아키텍처 탐색(Neural Architecture Search, NAS)은 이러한 과정을 자동화하는 기술이다.18 특히 FBNet에서 사용된 미분 가능한 NAS(Differentiable NAS, DNAS)는 탐색 공간 자체를 연속적으로 만들어 경사 하강법(gradient-based method)을 통해 최적의 아키텍처를 효율적으로 탐색한다. 이는 가능한 모든 후보 아키텍처를 개별적으로 훈련하고 평가해야 했던 기존의 강화학습 기반 NAS 방식에 비해 탐색 비용을 획기적으로 줄이는 혁신을 가져왔다.20</p>
<h4>3.1.2 FBNet 아키텍처 제품군: 공동 설계의 사례 연구</h4>
<p>DNAS 프로세스의 대표적인 산출물은 모바일 환경에 최적화된 FBNet 모델 제품군이다.6 FBNet의 가장 핵심적인 혁신은 ‘하드웨어 인식(hardware-aware)’ 손실 함수에 있다. 이 손실 함수는 모델의 정확도(일반적으로 교차 엔트로피 손실)뿐만 아니라, 특정 타겟 디바이스에서의 지연 시간(latency)과 에너지 소비량(energy consumption)을 직접적인 최적화 대상으로 포함한다.20</p>
<p>이 손실 함수의 수학적 표현은 다음과 같다.<br />
<span class="math math-display">
L(a, w_a) = CE(a, w_a) + \alpha \cdot LAT(a) + \gamma \cdot ENER(a)
</span><br />
여기서 <span class="math math-inline">a</span>는 아키텍처, <span class="math math-inline">w_a</span>는 가중치, <span class="math math-inline">CE</span>는 교차 엔트로피 손실, <span class="math math-inline">LAT</span>는 지연 시간, <span class="math math-inline">ENER</span>는 에너지 소비를 나타낸다. <span class="math math-inline">\alpha</span>와 <span class="math math-inline">\gamma</span>는 정확도와 하드웨어 비용 간의 균형을 조절하는 하이퍼파라미터다.22</p>
<p>FBNet의 DNAS 방법론은 기존의 *사후 모델 최적화(post-hoc model optimization)*에서 *사전 모델 공동 설계(ab-initio model co-design)*로의 근본적인 패러다임 전환을 의미한다. 이는 단순히 거대한 사전 정의된 아키텍처를 압축하는 것이 아니라, 설계 초기 단계부터 지연 시간이나 전력 소비와 같은 하드웨어 제약 조건을 최우선 목표로 고려하여 아키텍처를 “발견“하는 방식이다. 이러한 공동 설계 접근법은 정확도와 효율성 사이의 파레토 최적 전선(Pareto frontier)을 밀어내는 데 본질적으로 더 효과적이다.</p>
<h3>3.2 양자화를 통한 모델 압축</h3>
<h4>3.2.1 <code>int8</code> 양자화의 원리</h4>
<p>양자화는 모델의 가중치(weights)와 활성화(activations) 값을 일반적인 32비트 부동소수점(<code>float32</code>)에서 저정밀도 8비트 정수(<code>int8</code>)로 표현하여 모델 크기와 연산 비용을 줄이는 핵심적인 압축 기술이다.27</p>
<p><code>float32</code> 실수 <span class="math math-inline">x</span>를 <code>int8</code> 정수 <span class="math math-inline">x_q</span>로 변환하는 핵심적인 선형 양자화 공식은 다음과 같다.<br />
<span class="math math-display">
x_q = \text{round}(x/S + Z)
</span><br />
여기서 <span class="math math-inline">S</span>는 스케일(scale)이라 불리는 양의 실수이며, <span class="math math-inline">Z</span>는 제로 포인트(zero-point)로, 실수 0에 해당하는 정수 값을 나타낸다.27 이 변환을 통해 메모리 사용량은 4분의 1로 줄어들고, 정수 연산을 지원하는 하드웨어에서는 추론 속도가 크게 향상될 수 있다.</p>
<h4>3.2.2 훈련 후 양자화(PTQ) 대 양자화 인식 훈련(QAT)</h4>
<p>Detectron2Go는 두 가지 주요 양자화 전략을 제공하여 다양한 프로덕션 요구에 대응한다.9</p>
<ul>
<li><strong>훈련 후 양자화(Post-Training Quantization, PTQ):</strong> PTQ는 이미 훈련된 <code>float32</code> 모델을 <code>int8</code>로 변환하는 가장 간단하고 빠른 방법이다. 별도의 재훈련이 필요 없어 적용이 용이하지만, 정밀도 손실로 인해 모델의 정확도가 저하될 수 있다.29 D2Go에서는 CLI 명령에 <code>--predictor-type torchscript_int8</code> 옵션을 추가하는 것만으로 PTQ를 적용할 수 있다.9</li>
<li><strong>양자화 인식 훈련(Quantization-Aware Training, QAT):</strong> QAT는 훈련 과정 자체에 양자화로 인한 오차를 시뮬레이션하는 ‘가짜’ 양자화 노드(fake quantization node)를 삽입하는 고급 기법이다. 이를 통해 모델은 훈련 중에 양자화 오차에 강건해지도록 학습하므로, 최종적으로 변환된 <code>int8</code> 모델의 정확도 저하를 최소화할 수 있다.29 D2Go에서 QAT를 적용하려면 <code>qat_faster_rcnn_fbnetv3a_C4.yaml</code>과 같이 <code>QUANTIZATION</code> 섹션이 포함된 별도의 구성 파일을 사용해야 한다.9</li>
</ul>
<p>D2Go가 PTQ와 QAT를 모두 제공하는 것은 의도적인 설계 선택이다. PTQ는 빠른 프로토타이핑이나 정확도가 덜 중요한 애플리케이션을 위한 “빠른 시작” 옵션 역할을 하는 반면, QAT는 정확도 유지가 타협 불가능한 핵심 기능을 위한 “고정밀” 도구를 제공한다. 이러한 이원성은 D2Go 프레임워크를 접근성이 높으면서도 강력하게 만든다.</p>
<h3>3.3 네트워크 가지치기를 통한 모델 희소성 확보</h3>
<h4>3.3.1 가지치기 기법</h4>
<p>모델 가지치기(pruning)는 모델의 성능에 거의 영향을 주지 않는 중복되거나 중요하지 않은 파라미터를 제거하여 모델을 더 작고 효율적으로 만드는 기술이다.31 가지치기는 크게 두 가지로 나뉜다. 비구조적 가지치기(unstructured pruning)는 개별 가중치를 0으로 만들어 희소 행렬(sparse matrix)을 생성하는 반면, 구조적 가지치기(structured pruning)는 필터나 채널과 같은 더 큰 단위의 구조 전체를 제거하여 모델의 아키텍처 자체를 변경한다.31</p>
<h4>3.3.2 PyTorch와의 통합</h4>
<p>D2Go에서의 가지치기는 PyTorch의 내장 라이브러리인 <code>torch.nn.utils.prune</code>을 통해 구현된다.33 이 라이브러리는 다양한 가지치기 기법을 모듈화하여 제공하며, 개발자는 이를 통해 특정 레이어의 특정 파라미터에 가지치기를 손쉽게 적용하고, 결과적으로 더 작고 잠재적으로 더 빠른 모델을 생성할 수 있다.</p>
<p>그러나 가지치기의 효과는 직관과 다를 수 있다. 한 연구에서는 비구조적 가지치기를 적용했음에도 불구하고 추론 시간이 오히려 증가하는 역설적인 결과가 보고되었다.37 이는 이론적인 연산량(FLOPs) 감소가 실제 하드웨어에서의 속도 향상으로 반드시 이어지지는 않음을 시사한다. 비구조적 희소성은 현대 CPU/GPU가 고도로 최적화한 밀집 행렬 연산(dense matrix operation) 및 메모리 접근 패턴을 방해하여, 연산량 감소의 이점을 상쇄하는 오버헤드를 유발할 수 있다. 이는 가지치기의 실질적인 효과가 알고리즘 자체뿐만 아니라, 결과적인 모델의 희소성 패턴과 하드웨어/소프트웨어 실행 환경 간의 복잡한 상호작용에 의해 결정된다는 중요한 사실을 보여준다.</p>
<h2>4. 경험적 성능 평가 및 벤치마킹</h2>
<h3>4.1 온디바이스 성능 지표: 지연 시간, 정확도, 모델 크기</h3>
<p>모바일 환경에서 딥러닝 모델의 성능을 평가할 때는 서버 환경과는 다른 다차원적인 지표가 요구된다. 가장 중요한 세 가지 지표는 다음과 같다. 첫째, **추론 지연 시간(inference latency)**은 모델이 입력 데이터를 처리하여 결과를 출력하는 데 걸리는 시간으로, 실시간 애플리케이션의 반응성을 결정한다. 둘째, **정확도(accuracy)**는 모델이 얼마나 올바르게 예측하는지를 나타내며, 객체 탐지에서는 주로 COCO와 같은 표준 데이터셋에 대한 평균 정밀도(mean Average Precision, mAP)로 측정된다. 마지막으로, **모델 크기(model size)**는 모델 파일이 차지하는 저장 공간으로, 모바일 디바이스의 제한된 스토리지와 배포 용이성에 직접적인 영향을 미친다.8</p>
<h3>4.2 비교 분석: D2Go 대 경량 아키텍처 대안</h3>
<p>D2Go의 성능을 객관적으로 평가하기 위해, FBNet 기반 모델을 다른 주류 경량 객체 탐지 아키텍처와 비교 분석하였다. 각 모델은 모바일 및 엣지 환경에서 널리 사용되며, 속도와 정확도 사이에서 각기 다른 균형점을 제시한다.</p>
<h4>4.2.1 MobileNet-SSD, YOLO, EfficientDet-Lite와의 벤치마크</h4>
<ul>
<li><strong>MobileNet-SSD:</strong> MobileNet-SSD 계열은 효율성과 균형 잡힌 성능으로 모바일 비전의 대표적인 기준으로 자리 잡았다.38</li>
<li><strong>YOLO:</strong> YOLO 계열의 경량 버전(예: YOLOv3-tiny, YOLOv8n)은 종종 정확도를 일부 희생하는 대신 압도적인 추론 속도를 자랑한다.43</li>
<li><strong>EfficientDet-Lite:</strong> EfficientDet-Lite 제품군은 모델 크기, 지연 시간, 정확도 간의 명확하고 확장 가능한 트레이드오프를 제공하는 최신 아키텍처이다.41</li>
</ul>
<p>아래 표 1은 이들 주요 경량 객체 탐지 모델들의 성능을 종합적으로 비교한 것이다. 이 표는 다양한 벤치마크 데이터를 통합하여 D2Go의 경쟁력을 한눈에 파악할 수 있도록 한다.</p>
<p><strong>표 1: 주요 경량 객체 탐지 모델 COCO 데이터셋 성능 비교</strong></p>
<table><thead><tr><th>모델</th><th>COCO mAP (%)</th><th>지연 시간 (ms, RPi 4 기준 추정)</th><th>모델 크기 (MB)</th></tr></thead><tbody>
<tr><td><strong>D2Go (FBNet-B)</strong></td><td>38.5 (추정치)</td><td>~23 (S8 기준)</td><td>4.5</td></tr>
<tr><td><strong>MobileNetV2-SSD</strong></td><td>22.0 - 31.0</td><td>209 (Pi4)</td><td>7.0</td></tr>
<tr><td><strong>YOLOv8n</strong></td><td>37.3</td><td>383 (AGX Orin)</td><td>6.3</td></tr>
<tr><td><strong>EfficientDet-Lite0</strong></td><td>25.7</td><td>146 (Pi4)</td><td>4.4</td></tr>
</tbody></table>
<p>주: 각 모델의 mAP, 지연 시간, 크기는 다양한 소스21에서 보고된 값을 기반으로 종합 및 추정한 것으로, 측정 환경에 따라 달라질 수 있다. FBNet-B의 mAP는 ImageNet 분류 정확도를 기반으로 한 추정치이다.</p>
<h3>4.3 FBNet 성능 심층 분석: 모바일 하드웨어</h3>
<p>FBNet 모델의 진정한 가치는 하드웨어 인식 설계를 통해 달성한 구체적인 성능 향상에 있다. 원본 논문에서 제시된 벤치마크 결과는 DNAS 접근법의 성공을 정량적으로 입증한다.20</p>
<p>아래 표 2는 FBNet 모델과 수동으로 설계된 강력한 기준선인 MobileNetV2를 ImageNet 데이터셋에서 상세히 비교한 결과이다.</p>
<p><strong>표 2: FBNet 대 MobileNetV2 ImageNet 성능 비교</strong></p>
<table><thead><tr><th>모델</th><th>Top-1 정확도 (%)</th><th>FLOPs (M)</th><th>파라미터 수 (M)</th><th>Latency (Samsung S8, ms)</th><th>Latency (iPhone X, ms)</th></tr></thead><tbody>
<tr><td><strong>FBNet-A</strong></td><td>73.0</td><td>249</td><td>4.3</td><td>19.8</td><td>-</td></tr>
<tr><td>1.0-MobileNetV2</td><td>72.0</td><td>300</td><td>3.4</td><td>21.7</td><td>-</td></tr>
<tr><td><strong>FBNet-B</strong></td><td>74.1</td><td>295</td><td>4.5</td><td>23.1</td><td>-</td></tr>
<tr><td>1.3-MobileNetV2</td><td>74.4</td><td>509</td><td>5.3</td><td>33.8</td><td>-</td></tr>
<tr><td><strong>FBNet-C</strong></td><td>74.9</td><td>375</td><td>5.5</td><td>28.1</td><td>-</td></tr>
<tr><td>1.4-MobileNetV2</td><td>74.7</td><td>585</td><td>6.9</td><td>37.4</td><td>-</td></tr>
<tr><td><strong>FBNet-iPhoneX</strong></td><td>73.2</td><td>322</td><td>4.47</td><td>23.33</td><td><strong>19.84</strong></td></tr>
<tr><td><strong>FBNet-S8</strong></td><td>73.27</td><td>293</td><td>4.43</td><td><strong>22.12</strong></td><td>27.53</td></tr>
</tbody></table>
<p>데이터 출처: 21</p>
<p>표 2의 결과는 명확하다. FBNet-B는 MobileNetV2-1.3과 유사한 정확도를 보이면서도 FLOPs는 1.7배, Samsung S8에서의 지연 시간은 1.5배 더 우수하다. 이는 DNAS가 파레토 최적 전선을 성공적으로 확장했음을 보여준다.</p>
<p>특히 주목할 점은 Samsung S8에 최적화된 FBNet-S8 모델과 iPhone X에 최적화된 FBNet-iPhoneX 모델 간의 성능 교차이다.21 FBNet-S8은 S8에서는 22.12ms의 빠른 속도를 보이지만 iPhone X에서는 27.53ms로 느려진다. 반대로 FBNet-iPhoneX는 iPhone X에서는 19.84ms의 속도를 보이지만 S8에서는 23.33ms로 느려진다. 이 데이터는 ‘최적의’ 신경망 아키텍처가 보편적으로 존재하지 않으며, 타겟 하드웨어의 특정 연산 특성에 따라 달라진다는 ‘하드웨어 인식’ 철학을 강력하게 실증한다. DNAS 프로세스는 이러한 디바이스별 최적점을 성공적으로 발견해내며, 이는 새로운 하드웨어 타겟마다 아키텍처 탐색을 수행하는 비용을 정당화한다.</p>
<h2>5. 프로덕션 사용 사례 및 실제 영향</h2>
<h3>5.1 Meta 제품의 지능형 기능 활성화</h3>
<p>Detectron2Go의 기술적 성과는 Meta의 주력 제품들에서 사용자가 직접 체감할 수 있는 혁신적인 기능으로 구현되었다.</p>
<h4>5.1.1 Portal 스마트 카메라: 실시간 객체 추적</h4>
<p>Meta Portal의 핵심 기능인 ’스마트 카메라’는 화상 통화 중 인물의 움직임을 자동으로 추적하여 항상 최적의 구도로 프레임 안에 위치시키는 기능이다.54 이 기능의 이면에는 D2Go의 효율적인 키포인트 추정(keypoint estimation) 기술이 있다.8 지능적인 프레이밍에 필요한 실시간 인체 자세 추적은 D2Go 툴체인을 통해 생성된 저지연 모델 덕분에 디바이스 상에서 직접 실행될 수 있다. 이는 추상적인 기술(“키포인트 추정”)이 구체적이고 “마법 같은” 사용자 경험으로 연결되는 대표적인 사례이며, 모든 연산이 디바이스 내에서 이루어지므로 사용자의 사생활 또한 보호된다.59 D2Go는 Portal 제품군에 직접적으로 사용되는 것으로 명시되어 있다.2</p>
<h4>5.1.2 Facebook 3D 사진: 온디바이스 깊이 추정</h4>
<p>Facebook의 ‘3D 사진’ 기능, 특히 일반 2D 이미지를 입체적인 3D 이미지로 변환하는 기능은 모바일 비전 기술의 정점을 보여준다.61 이 기능의 핵심은 단일 이미지로부터 깊이 정보(depth map)를 추정하는 온디바이스 단안 깊이 추정(monocular depth estimation) 기술이다.</p>
<p>이 기능에 대한 기술 문서들은 객체 탐지 작업은 아니지만, FBNet의 구성 블록, ChamNet NAS 알고리즘, 그리고 양자화 인식 훈련(QAT)과 같은 D2Go 생태계의 핵심 기술들이 사용되었음을 명시적으로 언급하고 있다.65 이는 D2Go <em>툴체인</em>이 단순히 객체 탐지 모델뿐만 아니라, 깊이 추정 모델과 같은 다양한 종류의 효율적인 모바일 비전 CNN을 구축하고 배포하는 범용 프레임워크로 활용되고 있음을 강력하게 시사한다. 이 애플리케이션은 Meta의 더 큰 AR/VR 비전을 향한 중요한 기술적 발판이기도 하다.</p>
<h3>5.2 AI 생태계에서의 광범위한 응용</h3>
<p>D2Go와 같은 기술의 영향력은 Meta의 제품군을 넘어선다. 실시간 온디바이스 인식이 필수적인 자율 시스템, 로보틱스, 증강 현실과 같은 분야에서 D2Go가 제시한 원칙과 방법론은 산업 전반의 표준으로 자리 잡고 있다.8 낮은 지연 시간과 높은 에너지 효율성을 갖춘 지능형 엣지 디바이스의 구현은 이들 분야의 상용화를 앞당기는 핵심 요소이다.</p>
<h2>6. 결론 및 향후 전망</h2>
<h3>6.1 모바일 AI에 대한 D2Go의 기여 요약</h3>
<p>Detectron2Go는 고성능 컴퓨터 비전 모델의 모바일 배포를 대중화하는 데 결정적인 기여를 했다. 하드웨어 인식 아키텍처 탐색, 모델 압축, 그리고 간소화된 배포 파이프라인을 하나의 통합된 솔루션으로 제공함으로써, D2Go는 최첨단 연구와 실제 제품 간의 간극을 효과적으로 메웠다. 이는 개발자들이 더 이상 모바일 환경의 제약에 얽매이지 않고, 더 창의적이고 지능적인 애플리케이션을 구현할 수 있는 길을 열어주었다.</p>
<h3>6.2 최신 동향과 효율적인 컴퓨터 비전의 미래</h3>
<p>Detectron2Go는 빠르게 진화하는 분야의 기초 구성 요소이다. 2024년과 2025년의 최신 연구 동향을 살펴보면, 모바일 비전의 미래는 몇 가지 명확한 방향으로 나아가고 있다.67</p>
<p>첫째, AI 모델은 스마트폰을 넘어 마이크로컨트롤러와 같은 훨씬 더 자원이 제한된 환경으로 확장될 것이다.75 둘째, 클라우드에서의 재훈련 없이 새로운 환경에 적응하는 온디바이스 지속 학습(continual learning) 기술이 중요해질 것이다.72 셋째, 이러한 효율적인 원칙들은 3D 객체 탐지와 같은 더 복잡하고 다중 모드(multi-modal) 작업으로 확장 적용될 것이다.69 Detectron2Go는 이러한 차세대 도전 과제에 대한 청사진과 툴킷을 제공하며, 미래 엣지 AI 기술의 발전에 지속적으로 기여할 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Detectron2 Object Detection Model: What is, How to Use - Roboflow, https://roboflow.com/model/detectron2</li>
<li>Detectron Q&amp;A: The origins, evolution, and future of our pioneering computer vision library, https://ai.meta.com/blog/detectron-everingham-prize/</li>
<li>Detectron2 by Facebook AI Research is Here! | by Susant Achary | Analytics Vidhya, https://medium.com/analytics-vidhya/detectron2-by-facebook-ai-research-is-here-b666987ad8b8</li>
<li>Object Detection and Segmentation using Meta’s Detectron2 - Tech-with-JHA, https://techspace.hashnode.dev/object-detection-and-segmentation-using-metas-detectron2</li>
<li>Detectron2 | Papers With Code, https://paperswithcode.com/lib/detectron2</li>
<li>The Mobile Vision Team at Facebook Introduces Detectron2Go (D2Go): A Detectron2 Extension For Training and Deploying Efficient Deep Learning Object Detection Models on Mobile Devices - MarkTechPost, https://www.marktechpost.com/2021/03/07/the-mobile-vision-team-at-facebook-introduces-detectron2go-d2go-a-detectron2-extension-for-training-and-deploying-efficient-deep-learning-object-detection-models-on-mobile-devices/</li>
<li>The Mobile Vision Team at Facebook Introduces Detectron2Go (D2Go) | Kaggle, https://www.kaggle.com/general/224825</li>
<li>D2Go brings Detectron2 to mobile - AI at Meta, https://ai.meta.com/blog/d2go-brings-detectron2-to-mobile/</li>
<li>D2Go - Use Detectron2 on mobile devices - Gilbert Tanner, https://gilberttanner.com/blog/d2go-use-detectron2-on-mobile-devices/</li>
<li>D2Go is a toolkit for efficient deep learning - GitHub, https://github.com/facebookresearch/d2go</li>
<li>Detectron2 is a platform for object detection, segmentation and other visual recognition tasks. - GitHub, https://github.com/facebookresearch/detectron2</li>
<li>Facebook Reality Labs: Detectron2Go (D2Go), a new, state-of-the-art extension for training and deploying efficient deep learning object detection models on mobile devices : r/computervision - Reddit, https://www.reddit.com/r/computervision/comments/lyfz4p/facebook_reality_labs_detectron2go_d2go_a_new/</li>
<li>Comprehensive Guide to CV Pipelines with Detectron2 and MMDetection - Rapid Innovation, https://www.rapidinnovation.io/post/building-a-computer-vision-pipeline-with-detectron2-and-mmdetection</li>
<li>detectron2/docs/tutorials/datasets.md at main - GitHub, https://github.com/facebookresearch/detectron2/blob/main/docs/tutorials/datasets.md</li>
<li>Your Guide to Object Detection with Detectron2 in PyTorch - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2021/08/your-guide-to-object-detection-with-detectron2-in-pytorch/</li>
<li>Object detection with Detectron2 on Amazon SageMaker | Artificial Intelligence - AWS, https://aws.amazon.com/blogs/machine-learning/object-detection-with-detectron2-on-amazon-sagemaker/</li>
<li>Getting Started with Detectron2, https://detectron2.readthedocs.io/tutorials/getting_started.html</li>
<li>Neural architecture search - Wikipedia, https://en.wikipedia.org/wiki/Neural_architecture_search</li>
<li>Neural Architecture Search (NAS): Automating Model Design - Roboflow Blog, https://blog.roboflow.com/neural-architecture-search/</li>
<li>[1812.03443] FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search - arXiv, https://arxiv.org/abs/1812.03443</li>
<li>FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable …, https://arxiv.org/pdf/1812.03443</li>
<li>Hardware Aware Neural Network Architectures (using FBNet) - arXiv, https://arxiv.org/abs/1906.07214</li>
<li>FBNet - Pytorch Image Models, https://pprp.github.io/timm/models/fbnet/</li>
<li>FBNet: Efficient Convolutional Neural Architecture Design | SERP AI, https://serp.ai/posts/fbnet/</li>
<li>FBNetV3: Joint Architecture-Recipe Search using Predictor Pretraining | Request PDF - ResearchGate, https://www.researchgate.net/publication/355881630_FBNetV3_Joint_Architecture-Recipe_Search_using_Predictor_Pretraining</li>
<li>Hardware Aware Neural Network Architectures (using FBNet) - arXiv, https://arxiv.org/pdf/1906.07214</li>
<li>Quantization - Hugging Face, https://huggingface.co/docs/optimum/concept_guides/quantization</li>
<li>A Comprehensive Study on Quantization Techniques for Large Language Models - arXiv, https://arxiv.org/html/2411.02530v1</li>
<li>d2go/demo/README.md at main · facebookresearch/d2go · GitHub, https://github.com/facebookresearch/d2go/blob/main/demo/README.md</li>
<li>[2304.09785] Improving Post-Training Quantization on Object Detection with Task Loss-Guided Lp Metric - arXiv, https://arxiv.org/abs/2304.09785</li>
<li>A Comprehensive Guide to Neural Network Model Pruning - Datature, https://www.datature.io/blog/a-comprehensive-guide-to-neural-network-model-pruning</li>
<li>Post-Training Model Pruning - Introduction - Datature, https://developers.datature.io/docs/post-training-model-pruning</li>
<li>PyTorch Pruning | How it’s Made by Michela Paganini - YouTube, https://www.youtube.com/watch?v=TaOwEa3m5dw</li>
<li>Pruning Tutorial - PyTorch documentation, https://docs.pytorch.org/tutorials/intermediate/pruning_tutorial.html</li>
<li>Dean/mlops - f8ef3a82f5e5aeda298932d817fe1dd03cf15ce4 | DagsHub, https://dagshub.com/Dean/mlops/src/f8ef3a82f5e5aeda298932d817fe1dd03cf15ce4/datasets/projects.json</li>
<li>PyTorch Pocket Reference: Building and Deploying Deep Learning Models [1 ed.] 9781492090007 - DOKUMEN.PUB, https://dokumen.pub/pytorch-pocket-reference-building-and-deploying-deep-learning-models-1nbsped-9781492090007.html</li>
<li>Pruning increases inference time for detectron2 model - vision - PyTorch Forums, https://discuss.pytorch.org/t/pruning-increases-inference-time-for-detectron2-model/164959</li>
<li>Comparative analysis of neural network models performance on low-power devices for a real-time object detection task, https://computeroptics.ru/KO/PDF/KO48-2/480211.pdf</li>
<li>Comparison of Object Detection Algorithms for Street-level Objects - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/2208.11315</li>
<li>Benchmarking Machine Learning on the New Raspberry Pi 4, Model B | by Alasdair Allan, https://aallan.medium.com/benchmarking-machine-learning-on-the-new-raspberry-pi-4-model-b-88db9304ce4</li>
<li>Benchmarking Deep Learning Models for Object Detection on Edge Computing Devices, https://arxiv.org/html/2409.16808v1</li>
<li>Road Object Detection using SSD-MobileNet Algorithm: Case Study for Real-Time ADAS Applications - Journal UMY, https://journal.umy.ac.id/index.php/jrc/article/download/21145/9060</li>
<li>(PDF) Comparative Performance of Yolov8 and Ssd-mobilenet Algorithms for Road Damage Detection in Mobile Applications - ResearchGate, https://www.researchgate.net/publication/394048272_Comparative_Performance_of_Yolov8_and_Ssd-mobilenet_Algorithms_for_Road_Damage_Detection_in_Mobile_Applications</li>
<li>YOLOv8 vs SSD: Choosing the Right Object Detection Model - Keylabs, https://keylabs.ai/blog/yolov8-vs-ssd-choosing-the-right-object-detection-model/</li>
<li>What is YOLO? The Ultimate Guide [2025] - Roboflow Blog, https://blog.roboflow.com/guide-to-yolo-models/</li>
<li>Performance of tiny YOLOv3 and YOLOv3 on the COCO Dataset (Redmon, 2020), https://www.researchgate.net/figure/Performance-of-tiny-YOLOv3-and-YOLOv3-on-the-COCO-Dataset-Redmon-2020_fig4_342179672</li>
<li>YOLOv4 Tiny Object Detection Model: What is, How to Use - Roboflow, https://roboflow.com/model/yolov4-tiny</li>
<li>Performance Benchmark of YOLO v5, v7 and v8 - Stereolabs, https://www.stereolabs.com/blog/performance-of-yolo-v5-v7-and-v8</li>
<li>Tests - YOLOvX by Wiserli!, https://yolovx.com/benchmark/tests/</li>
<li>TannerGilbert/TFLite-Object-Detection-with-TFLite-Model … - GitHub, https://github.com/TannerGilbert/TFLite-Object-Detection-with-TFLite-Model-Maker</li>
<li>tensorflow-lite-custom-object/Model_Maker_Object_Detection.ipynb at main - GitHub, https://github.com/freedomwebtech/tensorflow-lite-custom-object/blob/main/Model_Maker_Object_Detection.ipynb</li>
<li>Model Maker Object Detection for Android Figurine - Colab, https://colab.research.google.com/github/khanhlvg/tflite_raspberry_pi/blob/main/object_detection/Train_custom_model_tutorial.ipynb</li>
<li>YOLOv8 vs. EfficientDet: A Technical Comparison - Ultralytics YOLO Docs, https://docs.ultralytics.com/compare/yolov8-vs-efficientdet/</li>
<li>Portal Go: Wireless Video Calling Device - Meta Store, https://www.meta.com/portal/products/portal-go/</li>
<li>Facebook Portal Mini (2nd Gen) Review - MediaTech Ventures, https://mediatech.ventures/facebook-portal-mini-2nd-gen-review/</li>
<li>Meta Portal - Wikipedia, https://en.wikipedia.org/wiki/Meta_Portal</li>
<li>Smart Camera | Move &amp; Talk Freely on Video | Meta Portal, https://www.meta.com/portal/features/smart-camera/</li>
<li>Portal: Video Calling Device for Your Home - Meta Store, https://www.meta.com/portal/products/portal/</li>
<li>Facebook’s new video chat camera, Portal, can follow you - YouTube, https://www.youtube.com/watch?v=-WYNsiXrCTA</li>
<li>Detectron2: Meta’s Next-generation Platform | by xis.ai | Medium, https://medium.com/@xis.ai/detectron2-metas-next-generation-platform-876c17acaf30</li>
<li>3D photos: How they work and how anyone can take them - Tech at Meta - Facebook, https://tech.facebook.com/reality-labs/2019/10/3d-photos-how-they-work-and-how-anyone-can-take-them/</li>
<li>How To Use 3D Photos On Facebook For Your Business - Neil Patel, https://neilpatel.com/blog/3d-photos-facebook/</li>
<li>Facebook Adds New Option to Create 3D Photos From Any 2D Image | Social Media Today, https://www.socialmediatoday.com/news/facebook-adds-new-option-to-create-3d-photos-from-any-2d-image/573235/</li>
<li>3D Photography using a 2D Image and AI! | by Sally Robotics - Medium, https://medium.com/@sallyrobotics.blog/3d-photography-using-a-2d-image-and-ai-704317060a42</li>
<li>Facebook is experimenting with new technologies to turn flat photos into 3D photos, https://www.photofast.tw/facebook-is-experimenting-with-new-technologies-to-turn-flat-photos-into-3d-photos/</li>
<li>Detectron2: Leading Object Detection Innovation - Viso Suite, https://viso.ai/deep-learning/detectron2/</li>
<li>9 Best Object Detection Models of 2025: Reviewed &amp; Compared - Hitech BPO, https://www.hitechbpo.com/blog/top-object-detection-models.php</li>
<li>Object Detection: State-of-the-Art Models in 2025 - HiringNet, https://hiringnet.com/object-detection-state-of-the-art-models-in-2025</li>
<li>PromptDet: A Lightweight 3D Object Detection Framework with LiDAR Prompts - arXiv, https://arxiv.org/abs/2412.12460</li>
<li>[2503.20516] Small Object Detection: A Comprehensive Survey on Challenges, Techniques and Real-World Applications - arXiv, https://arxiv.org/abs/2503.20516</li>
<li>Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models, https://arxiv.org/html/2505.20612v1</li>
<li>[2409.16215] Tiny Robotics Dataset and Benchmark for Continual Object Detection - arXiv, https://arxiv.org/abs/2409.16215</li>
<li>YOLOv4: A Breakthrough in Real-Time Object Detection - arXiv, https://arxiv.org/html/2502.04161v1</li>
<li>LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation - arXiv, https://arxiv.org/html/2503.13794v1</li>
<li>Design and Implementation of a Lightweight Object Detection System for Resource-Constrained Edge Environments - arXiv, https://arxiv.org/html/2507.16155v1</li>
<li>Small Object Detection in Traffic Scenes for Mobile Robots: Challenges, Strategies, and Future Directions - MDPI, https://www.mdpi.com/2079-9292/14/13/2614</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>