<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:RT-DETRv2 (2024-07-24) 실시간 객체 탐지를 위한 Bag-of-Freebies 기반 Transformer</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>RT-DETRv2 (2024-07-24) 실시간 객체 탐지를 위한 Bag-of-Freebies 기반 Transformer</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>RT-DETRv2 (2024-07-24) 실시간 객체 탐지를 위한 Bag-of-Freebies 기반 Transformer</span></nav>
                </div>
            </header>
            <article>
                <h1>RT-DETRv2 (2024-07-24) 실시간 객체 탐지를 위한 Bag-of-Freebies 기반 Transformer</h1>
<h2>1.  소개</h2>
<h3>1.1 실시간 객체 탐지 분야의 패러다임 경쟁</h3>
<p>실시간 객체 탐지(Real-time Object Detection) 기술은 컴퓨터 비전 분야의 핵심 과제로서, 자율 주행, 지능형 감시, 로보틱스 등 다양한 산업 분야의 발전을 견인하고 있다. 이 분야는 오랜 기간 동안 두 가지 주요한 기술적 패러다임의 경쟁과 발전을 통해 진화해왔다. 첫 번째 패러다임은 YOLO(You Only Look Once) 계열로 대표되는 합성곱 신경망(CNN) 기반의 접근법이다.1 YOLO 모델들은 속도와 정확도 사이의 탁월한 균형을 바탕으로 실시간 탐지 분야의 표준으로 자리 잡았으며, Anchor 기반 또는 Anchor-Free 설계를 채택하고 Non-Maximum Suppression(NMS)이라는 후처리 과정에 의존하여 최종 탐지 결과를 도출하는 특징을 가진다.3 NMS는 중복된 경계 상자(Bounding Box)를 제거하는 데 효과적이지만, 하이퍼파라미터에 민감하고 객체가 밀집한 환경에서 성능 저하를 유발할 수 있으며, 추론 속도를 불안정하게 만드는 요인으로 작용하기도 한다.4</p>
<p>이러한 배경 속에서 2020년 등장한 DETR(DEtection TRansformer)은 두 번째 패러다임의 시작을 알렸다.5 DETR은 자연어 처리 분야에서 성공을 거둔 트랜스포머 아키텍처를 객체 탐지에 도입하여, NMS와 같은 수작업으로 설계된 후처리 과정을 완전히 제거한 최초의 완전한 End-to-End(E2E) 프레임워크를 제시했다. DETR은 이분 매칭(Bipartite Matching)을 위한 Hungarian 알고리즘과 집합 기반 손실 함수(Set-based Loss)를 통해 모델 스스로 유일한 예측값을 학습하도록 설계하여 객체 탐지 파이프라인을 획기적으로 단순화했다.6 그러나 DETR의 트랜스포머 인코더-디코더 구조는 입력 이미지의 모든 픽셀 쌍 간의 관계를 계산하는 과정에서 막대한 연산량(<span class="math math-inline">O(N^2)</span>)을 요구했고, 이로 인해 훈련 수렴 속도가 매우 느리고 실시간 추론이 불가능하다는 명백한 한계를 보였다.3</p>
<h3>1.2 RT-DETR의 탄생 - 실시간 트랜스포머의 서막</h3>
<p>DETR이 제시한 End-to-End의 우아함과 잠재력에도 불구하고, 높은 연산 복잡도는 실용적인 적용을 가로막는 가장 큰 장벽이었다. 이 문제를 해결하기 위해 Baidu 연구팀은 RT-DETR(Real-Time DEtection TRansformer)을 개발하며 실시간 트랜스포머 탐지기 시대를 열었다.1 RT-DETR의 핵심은 DETR의 성능 저하를 유발했던 바닐라 트랜스포머 인코더를 ’효율적 하이브리드 인코더(Efficient Hybrid Encoder)’로 대체한 것이다.8 이 하이브리드 인코더는 연산 비용이 높은 스케일 내 상호작용(intra-scale interaction)과 상대적으로 저렴한 스케일 간 융합(cross-scale fusion)을 분리하여 처리함으로써, 다중 스케일 특징을 효율적으로 다루면서도 추론 속도를 극적으로 향상시켰다.4 더 나아가, ‘IoU-Aware Query Selection’ 메커니즘을 도입하여 디코더에 전달될 초기 객체 쿼리의 품질을 높임으로써 모델의 수렴 속도와 최종 정확도를 개선했다.5 이러한 혁신들을 통해 RT-DETR은 YOLO 계열 모델들과 대등한 속도와 정확도를 달성하며, 트랜스포머 기반 탐지기가 실시간 영역에서 충분한 경쟁력을 가질 수 있음을 최초로 입증했다.1</p>
<h3>1.3 RT-DETRv2의 철학 - “Bag-of-Freebies”</h3>
<p>RT-DETR이 성공적인 실시간 탐지기 아키텍처의 기준을 제시했다면, 그 후속 모델인 RT-DETRv2는 여기서 한 걸음 더 나아가 성숙한 엔지니어링적 접근을 통해 모델의 완성도를 높이는 데 집중한다.1 본 안내서의 핵심 주제인 RT-DETRv2는 “공짜 개선책 모음(Bag-of-Freebies)“이라는 독특한 철학을 기반으로 설계되었다.11 이는 추론에 사용되는 모델의 아키텍처나 연산량을 변경하지 않아 추론 비용(inference cost)을 전혀 증가시키지 않으면서, 오직 훈련 전략의 최적화와 배포 유연성 확보를 통해 성능과 실용성을 극대화하는 것을 목표로 한다.1</p>
<p>이러한 접근 방식은 실시간 객체 탐지 분야의 경쟁 구도에 중요한 시사점을 던진다. 단순히 더 크고 복잡한 모델을 만들어 소폭의 성능 향상을 꾀하는 대신, RT-DETRv2는 개발자 경험, 배포 편의성, 훈련 효율성과 같은 실용적인 가치를 경쟁의 핵심 요소로 삼는다. RT-DETR을 통해 속도 경쟁력을 확보한 후, RT-DETRv2는 주어진 속도 제약 내에서 성능과 사용성을 최대화하는 방향으로 진화했다. 이는 연구실의 프로토타입을 넘어, 다양한 하드웨어와 실제 산업 현장에서 안정적으로 구동될 수 있는 신뢰성 높은 도구로 거듭나기 위한 전략적 선택이라 할 수 있다. 본 안내서는 RT-DETRv2를 구성하는 핵심 기술들을 심층적으로 분석하고, 그 성능을 객관적인 데이터에 기반하여 평가하며, 실시간 객체 탐지 분야에서 갖는 기술적 의의를 고찰하고자 한다.</p>
<h2>2.  RT-DETRv2 아키텍처 심층 분석</h2>
<h3>2.1  전체 구조 개요</h3>
<p>RT-DETRv2는 선행 모델인 RT-DETR의 검증된 아키텍처 골격을 그대로 계승한다.14 이는 RT-DETRv2의 개선이 추론 시의 구조적 변경이 아닌, 훈련 방식과 배포 유연성에 초점을 맞추고 있음을 의미한다. 전체 구조는 크게 세 가지 핵심 구성 요소로 나뉜다: (1) 입력 이미지로부터 계층적 특징을 추출하는 CNN 백본, (2) 다중 스케일 특징을 효율적으로 처리하는 효율적 하이브리드 인코더, (3) 객체 쿼리를 정제하여 최종 예측을 생성하는 트랜스포머 디코더이다.8 RT-DETRv2에서 이루어진 핵심적인 기술적 수정은 마지막 단계인 트랜스포머 디코더, 특히 Deformable Attention 메커니즘 내부에 집중되어 있다.</p>
<h3>2.2  백본 네트워크 (Backbone Network)</h3>
<p>백본 네트워크는 RT-DETRv2 아키텍처의 가장 첫 단계로, 입력된 이미지로부터 객체 탐지에 필요한 의미론적, 공간적 정보를 담은 다중 스케일 특징 맵(multi-scale feature maps)을 추출하는 역할을 담당한다. 이는 후속 모듈인 인코더와 디코더가 효과적으로 작동하기 위한 기반을 마련하는 중요한 과정이다.</p>
<p>RT-DETRv2는 유연성을 확보하기 위해 다양한 종류의 표준 CNN 아키텍처를 백본으로 활용할 수 있다. 주로 ImageNet 데이터셋으로 사전 훈련된 ResNet 계열 모델(예: ResNet-18, ResNet-50, ResNet-101)이 사용되며, 이는 풍부한 시각적 패턴을 학습한 가중치를 초기값으로 사용하여 훈련 효율성과 최종 성능을 높이는 데 기여한다.14 또한, 경량화와 속도에 중점을 둔 HGNetv2와 같은 최신 아키텍처도 백본으로 사용될 수 있다.9 백본은 입력 이미지를 여러 계층의 합성곱 연산을 통해 처리하며, 점진적으로 공간 해상도는 줄이고 채널 차원은 늘려나간다. 이 과정에서 생성되는 여러 단계의 특징 맵들 중에서, RT-DETRv2는 일반적으로 마지막 세 단계, 즉 S3, S4, S5에 해당하는 특징 맵을 선택하여 하이브리드 인코더의 입력으로 전달한다.4 이 세 가지 스케일의 특징 맵은 각각 다른 수준의 정보를 담고 있다. S3는 상대적으로 고해상도로서 객체의 세밀한 경계와 위치 정보를, S5는 저해상도이지만 넓은 수용장(receptive field)을 통해 객체의 클래스를 구분하는 데 필요한 고수준의 의미 정보를 풍부하게 포함하고 있다.</p>
<h3>2.3  효율적 하이브리드 인코더 (Efficient Hybrid Encoder)</h3>
<p>효율적 하이브리드 인코더는 RT-DETR의 가장 핵심적인 혁신으로, 트랜스포머 기반 탐지기의 실시간화를 가능하게 한 주역이다. RT-DETRv2는 이 강력하고 효율적인 모듈을 구조적 변경 없이 그대로 채택하여 안정적인 실시간 성능을 보장한다.14 이 인코더의 핵심 전략은 기존 DETR에서 병목 현상을 일으켰던 다중 스케일 특징 처리 방식을 근본적으로 재설계하는 데 있다. 즉, 연산량이 폭증하는 원인이었던 ’모든 스케일의 특징을 하나로 묶어 처리하는 방식’을 버리고, ’스케일 내 상호작용(intra-scale interaction)’과 ’스케일 간 융합(cross-scale fusion)’이라는 두 가지 연산을 효율적으로 분리하여 처리한다.1</p>
<ul>
<li>
<p><strong>AIFI (Attention-based Intra-scale Feature Interaction):</strong> 이 모듈은 각 스케일(S3, S4, S5)의 특징 맵에 대해 독립적으로 연산을 수행한다. 각 특징 맵 내에서 표준 트랜스포머의 self-attention 메커니즘을 적용하여, 동일한 해상도 내의 픽셀(또는 패치)들 간의 공간적 관계와 문맥 정보를 모델링한다.3 이를 통해 각 스케일별로 풍부한 표현력을 갖춘 특징을 생성한다.</p>
</li>
<li>
<p><strong>CCFM (CNN-based Cross-scale Feature Fusion Module):</strong> AIFI를 통해 정제된 각 스케일의 특징 맵들을 융합하는 역할을 담당한다. 여기서 핵심은 연산 비용이 높은 어텐션 대신, 매우 가볍고 효율적인 CNN 기반의 융합 블록을 사용한다는 점이다.3 CCFM은 인접한 두 스케일의 특징 맵을 입력받아 합성곱 연산을 통해 정보를 통합하고, 새로운 융합 특징 맵을 생성한다. 이 과정이 계층적으로 반복되면서, 고해상도의 공간 정보와 저해상도의 의미 정보가 효율적으로 결합된다.</p>
</li>
</ul>
<p>이처럼 하이브리드 인코더는 연산량이 많은 어텐션은 각 스케일 내부로 제한하고, 스케일 간의 정보 교환은 가벼운 CNN으로 처리하는 영리한 분업 구조를 통해 전체적인 연산 복잡도를 크게 낮추면서도 다중 스케일 정보의 이점을 효과적으로 활용한다.</p>
<h3>2.4  IoU-Aware Query Selection</h3>
<p>인코더를 통과하여 정제된 다중 스케일 특징 맵은 디코더가 객체를 탐지하는 데 사용할 정보의 원천이 된다. 하지만 이 모든 특징을 쿼리로 사용하는 것은 비효율적이다. IoU-Aware Query Selection 모듈은 이 특징 맵으로부터 가장 ‘유망한’ 후보, 즉 객체일 가능성이 높은 위치의 특징들을 선별하여 디코더에 전달할 초기 객체 쿼리(initial object queries)로 만드는 역할을 한다.5</p>
<p>이 모듈은 각 위치의 특징이 객체의 중심에 해당할 확률과 그 위치에서 예측될 바운딩 박스의 IoU(Intersection over Union) 점수를 예측하도록 훈련된다. 이를 통해 불확실성이 낮고 신뢰도가 높은 상위 K개의 특징을 선택할 수 있다. 이렇게 고품질의 초기 쿼리를 디코더에 제공함으로써, 디코더는 무작위적인 위치에서 탐지를 시작하는 대신 이미 객체가 존재할 가능성이 높은 위치에서부터 예측을 정제해 나갈 수 있다.1 이는 모델의 수렴을 가속화하고, 최종적인 탐지 정확도를 높이는 데 결정적인 기여를 한다. 이 모듈 역시 RT-DETR로부터 변경 없이 계승된 핵심 구성 요소 중 하나이다.1</p>
<h3>2.5  트랜스포머 디코더 (Transformer Decoder)</h3>
<p>트랜스포머 디코더는 RT-DETRv2 아키텍처의 최종 단계로서, IoU-Aware Query Selection 모듈로부터 전달받은 고품질의 객체 쿼리들을 입력받아 이를 반복적으로 정제하고, 최종적으로 각 객체의 클래스 레이블과 정확한 바운딩 박스 좌표를 예측하는 역할을 수행한다. 디코더는 여러 개의 동일한 구조를 가진 디코더 레이어를 쌓아서 구성되며, 각 레이어는 세 가지 주요 하위 모듈로 이루어져 있다: self-attention, cross-attention, 그리고 FFN(Feed-Forward Network)이다.5</p>
<ul>
<li>
<p><strong>Self-Attention:</strong> 이 모듈은 입력된 모든 객체 쿼리들 간의 상호 관계를 학습한다. 이를 통해 모델은 이미지 내에 여러 객체가 존재할 때, 객체들 간의 관계(예: ’사람’과 ’자전거’는 함께 나타날 가능성이 높다)를 이해하고, 중복된 예측을 억제하는 능력을 학습하게 된다. 이는 NMS 없이도 고유한 예측을 생성하는 DETR 계열 모델의 핵심적인 기능이다.</p>
</li>
<li>
<p><strong>Cross-Attention:</strong> 이 모듈은 self-attention을 통과한 객체 쿼리와 하이브리드 인코더에서 생성된 이미지 특징 맵을 연결하는 다리 역할을 한다. 각 객체 쿼리는 이 모듈을 통해 이미지의 전역적인 특징 정보 중에서 자신과 가장 관련이 높은 영역에 ’집중(attend)’하여, 자신의 위치와 클래스를 특정하는 데 필요한 정보를 얻는다. RT-DETRv2의 핵심적인 구조적 혁신인 ’선택적 다중 스케일 특징 추출’이 바로 이 cross-attention 단계, 특히 연산 효율성을 위해 도입된 Deformable Attention 메커니즘 내에서 이루어진다.1</p>
</li>
<li>
<p><strong>FFN (Feed-Forward Network):</strong> 어텐션 모듈을 통해 얻은 정보를 바탕으로 각 쿼리의 표현을 더욱 풍부하게 만들고 비선형성을 추가하는 역할을 하는 간단한 다층 퍼셉트론(MLP)이다.</p>
</li>
</ul>
<p>이러한 과정을 여러 디코더 레이어에 걸쳐 반복하면서 객체 쿼리는 점차 정교하게 다듬어지고, 최종적으로 각 쿼리는 하나의 객체에 대한 예측(클래스 확률 및 박스 좌표)을 출력하게 된다.</p>
<h2>3.  RT-DETRv2의 핵심 기술 혁신: “Bag-of-Freebies”</h2>
<p>RT-DETRv2의 가장 큰 특징은 추론 성능에 영향을 주지 않으면서 모델의 정확도, 유연성, 실용성을 향상시키는 일련의 개선책, 즉 “Bag-of-Freebies“를 도입한 것이다.1 이는 모델의 아키텍처 자체를 복잡하게 만들기보다, 기존 구조의 잠재력을 최대한 이끌어내는 훈련 기법과 배포 편의성을 높이는 영리한 수정에 초점을 맞춘다. 이러한 접근 방식은 연구실 수준의 모델을 실제 산업 현장에 적용할 때 발생하는 현실적인 문제들을 해결하기 위한 성숙한 엔지니어링 전략을 보여준다.</p>
<h3>3.1  유연성 향상: 선택적 다중 스케일 특징 추출 (Improving Flexibility: Selective Multi-scale Feature Extraction)</h3>
<h4>3.1.1 문제 제기</h4>
<p>기존의 Deformable DETR 및 RT-DETR 아키텍처는 연산 효율성을 위해 디코더의 cross-attention에 Deformable Attention 메커니즘을 사용한다. 이 메커니즘은 각 객체 쿼리가 전체 특징 맵을 보는 대신, 소수의 ’샘플링 포인트’를 참조하여 정보를 추출한다. 그러나 기존 방식에서는 백본에서 추출된 모든 다중 스케일 특징 맵(예: S3, S4, S5)에 대해 동일한 개수의 샘플링 포인트를 할당했다.1 이는 각 스케일이 가진 정보의 특성을 고려하지 않은 경직된 접근법이다. 예를 들어, S5와 같은 저해상도 특징 맵은 넓은 영역의 의미론적, 문맥적 정보를 담고 있어 더 많은 포인트를 통해 넓게 탐색하는 것이 유리할 수 있다. 반면, S3와 같은 고해상도 특징 맵은 객체의 정밀한 경계와 위치 정보를 담고 있으므로, 적은 수의 포인트를 사용하더라도 특정 영역에 집중하는 것이 더 효율적일 수 있다. 이처럼 모든 스케일에 동일한 규칙을 적용하는 것은 특징 추출 능력의 잠재력을 완전히 활용하지 못하게 하는 제약으로 작용했다.14</p>
<h4>3.1.2 RT-DETRv2의 해결책 및 기대 효과</h4>
<p>이러한 비효율성을 해결하기 위해, RT-DETRv2는 Deformable Attention 모듈 내에서 각 특징 스케일별로 <strong>서로 다른 수의 샘플링 포인트를 설정</strong>하는 혁신적인 아이디어를 제안한다.11 이를 통해 모델은 각 스케일의 고유한 특성에 맞춰 정보 수집 전략을 최적화할 수 있다. 예를 들어, 저해상도 특징 맵(S5)에는 4개의 샘플링 포인트를, 중간 해상도(S4)에는 2개, 고해상도(S3)에는 1개의 포인트를 할당하는 식으로 차등을 둘 수 있다.</p>
<p>이러한 ‘선택적 다중 스케일 특징 추출’ 방식은 디코더가 보다 유연하고 지능적으로 특징을 활용하도록 유도한다.10 모델은 더 이상 획일적인 방식으로 정보를 수집하는 것이 아니라, 문맥이 중요할 때는 넓게, 정밀함이 중요할 때는 좁고 깊게 정보를 샘플링할 수 있게 된다. 이는 모델의 전반적인 표현력을 강화하여, 특히 다양한 크기의 객체가 혼재하거나 복잡한 배경을 가진 장면에서 탐지 정확도를 향상시키는 데 직접적으로 기여한다.1 이 모든 개선은 추론 시 추가적인 연산 없이, 단지 훈련 가능한 파라미터의 재분배와 학습을 통해 이루어지므로 “공짜” 개선책의 철학에 완벽하게 부합한다.</p>
<h3>3.2  실용성 증대: 이산 샘플링 연산자 (Enhancing Practicality: The Discrete Sampling Operator)</h3>
<h4>3.2.1 문제 제기</h4>
<p>RT-DETR이 실시간 성능을 달성했음에도 불구하고, 실제 산업 현장에 모델을 배포하는 과정에서는 예상치 못한 장벽에 부딪히곤 했다. 그 원인은 Deformable Attention의 핵심 연산인 <code>grid_sample</code>에 있었다.1</p>
<p><code>grid_sample</code> 연산자는 예측된 오프셋 위치의 픽셀 값을 추정하기 위해 주변 픽셀 값을 참조하여 선형 보간(bilinear interpolation)을 수행한다. 이 연산은 PyTorch와 같은 주요 딥러닝 프레임워크에서는 기본적으로 지원되지만, 특정 하드웨어 가속기(예: NPU)나 일부 딥러닝 추론 엔진, 특히 구형 버전의 NVIDIA TensorRT(8.5 미만)에서는 지원되지 않는 경우가 많았다.17 이는 RT-DETR 모델을 특정 환경에 배포하는 것을 불가능하게 만들거나, 지원되지 않는 연산을 CPU에서 처리하도록 하여 심각한 성능 저하를 유발하는 등 실용성을 크게 저해하는 요인이었다.</p>
<h4>3.2.2 RT-DETRv2의 해결책 및 기대 효과</h4>
<p>RT-DETRv2는 이러한 배포 제약을 극복하고 YOLO처럼 “어디서든(everywhere)” 사용 가능한 모델을 만들기 위해, <code>grid_sample</code>을 대체할 수 있는 선택적 <strong><code>discrete_sample</code>(이산 샘플링) 연산자</strong>를 제안한다.1 이 연산자의 아이디어는 매우 간단하면서도 효과적이다. 복잡한 보간법을 사용하는 대신, 예측된 샘플링 오프셋 좌표를 <strong>단순히 반올림(rounding)하여 가장 가까운 정수 좌표의 픽셀 값을 그대로 사용</strong>하는 것이다.3 이를 통해 시간이 많이 소요되고 하드웨어 지원이 제한적인 보간 과정을 완전히 생략할 수 있다.</p>
<p>물론, 반올림 연산은 미분 불가능(non-differentiable)하기 때문에 역전파 과정에서 그래디언트를 계산할 수 없다는 문제가 있다. RT-DETRv2는 이를 해결하기 위해 영리한 2단계 훈련 전략을 사용한다.</p>
<ol>
<li>
<p><strong>1단계 (사전 훈련):</strong> 먼저 표준 <code>grid_sample</code> 연산자를 사용하여 모델을 충분히 훈련시킨다. 이 단계에서 모델은 객체의 특징을 샘플링하는 방법을 학습한다.</p>
</li>
<li>
<p><strong>2단계 (미세 조정):</strong> 훈련이 완료된 모델의 <code>grid_sample</code> 연산자를 <code>discrete_sample</code> 연산자로 교체한 후, 짧은 기간 동안 추가로 미세 조정(fine-tuning)을 진행한다. 이 때, 미분 불가능한 반올림 연산과 관련된 파라미터의 그래디언트 업데이트는 비활성화한다.1</p>
</li>
</ol>
<p>이러한 전략을 통해, <code>discrete_sample</code> 연산자는 최종 성능, 특히 <span class="math math-inline">AP_{50}</span>(IoU 임계값 0.5에서의 평균 정밀도)에서 눈에 띄는 손실 없이 <code>grid_sample</code>을 대체할 수 있음이 실험적으로 확인되었다.18 결과적으로 RT-DETRv2는 특정 하드웨어나 소프트웨어 스택에 대한 의존성을 제거함으로써, 개발자들이 더 넓은 범위의 환경에 손쉽게 모델을 배포할 수 있는 뛰어난 실용성과 범용성을 확보하게 되었다. 이는 연구 성과를 실제 제품으로 전환하는 과정에서 발생하는 중요한 간극을 메우는, 매우 실용적인 혁신이라 할 수 있다.</p>
<h3>3.3  최적화된 훈련 전략 (Optimized Training Strategy)</h3>
<p>RT-DETRv2의 성능 향상은 아키텍처 수정뿐만 아니라, 모델을 훈련시키는 과정 자체를 최적화함으로써 이루어진다. 이는 추가적인 추론 비용 없이 모델의 잠재력을 최대한 끌어내는 “공짜” 개선의 핵심이다.</p>
<h4>3.3.1 동적 데이터 증강 (Dynamic Data Augmentation)</h4>
<p>데이터 증강은 모델이 다양한 환경에 대해 강건함(robustness)을 갖도록 돕는 필수적인 훈련 기법이다. 하지만 훈련의 모든 단계에서 동일한 강도의 증강을 적용하는 것은 최적이 아닐 수 있다. RT-DETRv2는 훈련 과정을 단계별로 나누어 데이터 증강의 강도를 동적으로 조절하는 전략을 채택했다.1</p>
<ul>
<li>
<p><strong>훈련 초기 단계:</strong> 모델이 아직 데이터의 다양한 변형에 충분히 학습되지 않았으므로, 일반화 성능을 높이기 위해 <code>RandomPhotometricDistort</code>(색상 왜곡), <code>RandomZoomOut</code>(이미지 축소), <code>RandomIoUCrop</code>(IoU 기반 자르기) 등과 같은 강력한 데이터 증강 기법을 적극적으로 사용한다.14 이를 통해 모델은 조명, 크기, 가려짐 등의 변화에 강건한 특징을 학습하게 된다.</p>
</li>
<li>
<p><strong>훈련 후기 단계:</strong> 모델이 어느 정도 수렴하고 나면, 강력한 데이터 증강은 오히려 실제 평가 데이터의 분포와 차이를 만들어 성능에 방해가 될 수 있다. 따라서 훈련 마지막 몇 에포크(epoch) 동안에는 이러한 강력한 증강 기법들을 비활성화한다.14 이 단계에서는 모델이 목표 도메인(target domain)의 미세한 특성에 더 잘 정렬(align)되도록 하여 최종 성능을 미세 조정한다.</p>
</li>
</ul>
<p>이러한 동적 접근법은 훈련의 각 단계에 맞는 최적의 학습 환경을 제공함으로써, 최종 모델이 더 높은 일반화 성능과 정확도를 갖도록 돕는다.</p>
<h4>3.3.2 규모 적응형 하이퍼파라미터 (Scale-Adaptive Hyperparameters)</h4>
<p>RT-DETR 계열 모델은 다양한 연산량과 성능을 갖는 여러 스케일(예: S, M, L, X)로 제공된다. 기존에는 이 모든 스케일의 모델에 동일한 하이퍼파라미터(특히 학습률)를 적용하는 경우가 많았다. 그러나 RT-DETRv2 연구팀은 이것이 각 모델의 잠재력을 최대한 발휘하는 데 방해가 된다는 점을 발견했다.14 모델의 크기, 즉 백본의 용량에 따라 최적의 학습률이 다르기 때문이다.</p>
<ul>
<li>
<p><strong>원리:</strong> ResNet-18과 같이 상대적으로 작은 백본을 사용하는 모델은 특징 표현력이 약하기 때문에, 더 높은 학습률을 적용하여 파라미터 공간을 더 적극적으로 탐색하고 충분히 학습할 기회를 주어야 한다. 반면, ResNet-101과 같이 매우 큰 백본을 사용하는 모델은 이미 ImageNet 사전 훈련을 통해 강력하고 안정적인 특징 표현력을 갖추고 있으므로, 낮은 학습률을 적용하여 기존의 유용한 지식을 보존하면서 안정적으로 미세 조정하는 것이 더 효과적이다.1</p>
</li>
<li>
<p><strong>효과:</strong> RT-DETRv2는 이 원리에 기반하여 각 모델 스케일에 맞는 학습률을 개별적으로 설정하는 ‘규모 적응형 하이퍼파라미터’ 전략을 도입했다. 이를 통해 작은 모델은 잠재 성능을 최대한 끌어올리고, 큰 모델은 안정적인 수렴을 보장함으로써, 전체 모델 라인업의 성능-속도 트레이드오프 곡선(trade-off curve)을 전반적으로 개선하는 효과를 얻었다.</p>
</li>
</ul>
<h2>4.  연산 메커니즘 및 손실 함수</h2>
<h3>4.1  어텐션 메커니즘 (Attention Mechanism)</h3>
<p>RT-DETRv2의 디코더는 연산 효율성과 성능을 모두 잡기 위해 Deformable DETR에서 제안된 <strong>Deformable Attention</strong> 메커니즘을 핵심적인 연산 단위로 사용한다.1 이는 바닐라 트랜스포머의 어텐션이 가진 이차적인 연산 복잡도(<span class="math math-inline">O(N^2)</span>) 문제를 해결하기 위해 고안된 경량화된 어텐션 방식이다.</p>
<p>기존 어텐션이 쿼리(query) 하나가 이미지 특징 맵의 모든 키(key)와 상호작용하는 것과 달리, Deformable Attention은 각 쿼리가 소수의 고정된 개수(K)의 샘플링 포인트에만 집중(attend)한다. 이 샘플링 포인트의 위치는 쿼리 자체로부터 학습된 오프셋(offset)에 의해 동적으로 결정되므로, 모델은 객체의 형태나 크기에 맞춰 유연하게 정보를 수집할 수 있다. 이로 인해 연산 복잡도는 시퀀스 길이에 선형적인 <span class="math math-inline">O(N)</span> 수준으로 크게 감소한다.</p>
<p>Deformable Attention의 연산 과정은 다음 수식으로 표현될 수 있다. 여기서 <span class="math math-inline">z_q</span>는 쿼리 특징, <span class="math math-inline">p_q</span>는 쿼리의 2D 참조 포인트(reference point), <span class="math math-inline">x</span>는 입력 다중 스케일 특징 맵을 나타낸다.</p>
<p><span class="math math-display">
\text{DeformAttn}(z_q, p_q, x) = \sum_{m=1}^{M} W_m \left[ \sum_{k=1}^{K} A_{mqk} \cdot W&#39;_m \phi (x(p_q + \Delta p_{mqk})) \right]
</span></p>
<ul>
<li>
<p><span class="math math-inline">m</span>은 어텐션 헤드(attention head)의 인덱스, <span class="math math-inline">l</span>은 특징 레벨(스케일)의 인덱스, <span class="math math-inline">k</span>는 해당 레벨의 샘플링 포인트 인덱스를 의미한다.</p>
</li>
<li>
<p><span class="math math-inline">\Delta p_{mlqk}</span>는 쿼리 <span class="math math-inline">z_q</span>로부터 예측된 샘플링 오프셋이며, <span class="math math-inline">\phi_l(p_q)</span>는 참조 포인트를 <span class="math math-inline">l</span>번째 스케일의 좌표계로 변환한 것이다.</p>
</li>
<li>
<p><span class="math math-inline">A_{mlqk}</span>는 쿼리 <span class="math math-inline">z_q</span>로부터 예측된 어텐션 가중치(attention weight)로, <span class="math math-inline">\sum_{l=1}^{L} \sum_{k=1}^{K_l} A_{mlqk} = 1</span>을 만족한다.</p>
</li>
<li>
<p><span class="math math-inline">x(\cdot)</span>는 특징 맵 <span class="math math-inline">x</span>에서 특정 좌표의 특징 벡터를 추출하는 함수이며, <code>grid_sample</code> 또는 RT-DETRv2에서 제안된 <span class="math math-inline">discrete_sample</span> 연산자를 통해 구현된다.</p>
</li>
</ul>
<p>RT-DETRv2의 핵심적인 구조적 혁신은 바로 이 수식에서 나타난다. 기존 모델에서는 모든 특징 레벨 <span class="math math-inline">l</span>에 대해 샘플링 포인트의 개수 <span class="math math-inline">K</span>가 동일했지만, RT-DETRv2에서는 각 레벨별로 샘플링 포인트의 총 개수인 <span class="math math-inline">K_l</span>을 가변적으로 설정할 수 있도록 개선했다. 이를 통해 모델은 각 스케일의 정보적 특성에 맞춰 최적의 샘플링 전략을 학습할 수 있게 된다.</p>
<h3>4.2  손실 함수 (Loss Function)</h3>
<p>RT-DETRv2는 손실 함수 및 최적화 과정에서 DETR 계열의 표준적인 프레임워크를 그대로 따른다.5 이는 개별적인 예측을 독립적으로 평가하는 것이 아니라, 예측된 객체 ’집합’과 실제 객체 ‘집합’ 간의 전역적인 관계를 고려하는 집합 기반 손실(set-based loss)을 사용한다.5 이 과정은 크게 ’할당(assignment)’과 ’손실 계산(loss calculation)’의 두 단계로 나뉜다.</p>
<h4>4.2.1 할당 (Assignment)</h4>
<p>모델이 출력한 N개의 예측값 집합과 이미지에 존재하는 M개의 실제 객체(ground-truth) 집합 사이의 최적의 일대일 매칭을 찾는 과정이다. N은 보통 M보다 훨씬 큰 값(예: 300)으로 설정된다. 이 매칭 문제를 해결하기 위해, RT-DETRv2는 <strong>Hungarian 알고리즘</strong>을 사용한다.5 각 예측값과 실제 객체 쌍에 대해 클래스 예측의 정확도와 바운딩 박스의 유사도(L1 거리 및 GIoU)를 고려한 비용(cost)을 계산하고, 전체 비용의 합이 최소가 되는 최적의 매칭을 찾는다. 이를 통해 각 실제 객체는 가장 잘 맞는 하나의 예측값과 짝을 이루게 되며, 매칭되지 않은 나머지 예측값들은 ’배경(no object)’으로 간주된다.</p>
<h4>4.2.2 손실 계산 (Loss Calculation)</h4>
<p>Hungarian 알고리즘을 통해 매칭된 쌍들에 대해서만 손실을 계산한다. 전체 손실 함수 <span class="math math-inline">\mathcal{L}_{\text{total}}</span>는 분류 손실과 박스 회귀 손실의 가중합으로 구성된다.</p>
<p><span class="math math-display">
\mathcal{L}_{\text{total}} = \lambda_{cls} \mathcal{L}_{cls} + \lambda_{L1} \mathcal{L}_{L1} + \lambda_{GIoU} \mathcal{L}_{GIoU}
</span></p>
<ul>
<li>
<p><strong>분류 손실 (<span class="math math-inline">\mathcal{L}_{cls}</span>):</strong> 매칭된 예측값의 클래스 레이블을 정확하게 예측하도록 학습한다. RT-DETR 계열에서는 클래스 불균형 문제에 강건한 VFL(Variable Focal Loss)이 주로 사용되지만 5, 표준적인 Cross-Entropy Loss도 사용될 수 있다.19</p>
</li>
<li>
<p><strong>박스 회귀 손실:</strong> 바운딩 박스의 위치를 정교하게 학습하기 위해 두 가지 손실 함수를 조합하여 사용한다.</p>
</li>
<li>
<p><strong>L1 Loss (<span class="math math-inline">\mathcal{L}_{L1}</span>):</strong> 예측된 박스와 실제 박스 좌표 간의 L1 거리를 최소화한다. 이는 박스의 크기와 무관하게 절대적인 거리 차이를 줄이는 데 효과적이다.</p>
</li>
<li>
<p><strong>GIoU Loss (<span class="math math-inline">\mathcal{L}_{GIoU}</span>):</strong> Generalized Intersection over Union 손실은 두 박스 간의 겹치는 영역뿐만 아니라, 두 박스를 모두 포함하는 가장 작은 박스와의 관계까지 고려한다. 이는 두 박스가 전혀 겹치지 않을 때도 유의미한 그래디언트를 제공하여 학습을 안정화하고, 박스의 형태와 정렬을 더 잘 학습하도록 돕는다.5</p>
</li>
</ul>
<p><span class="math math-inline">\lambda_{cls}</span>, <span class="math math-inline">\lambda_{L1}</span>, <span class="math math-inline">\lambda_{GIoU}</span>는 각 손실 항의 중요도를 조절하는 가중치 하이퍼파라미터이다. 이처럼 집합 기반의 손실 함수는 모델이 NMS 없이도 중복을 피하고 정확한 예측을 생성하도록 유도하는 핵심적인 메커니즘으로 작동한다.</p>
<h2>5.  성능 평가 및 비교 분석</h2>
<h3>5.1  COCO 데이터셋 벤치마크</h3>
<p>RT-DETRv2의 객관적인 성능을 평가하기 위해, 가장 널리 사용되는 객체 탐지 벤치마크인 COCO val2017 데이터셋에서의 성능을 분석한다. 아래 표는 RT-DETRv2를 이전 버전인 RT-DETR, 후속 버전인 RT-DETRv3, 그리고 주요 경쟁 모델인 YOLO 계열과 비교하여 정확도(AP), 추론 속도(Latency), 모델 복잡도(파라미터 수, GFLOPs)를 종합적으로 보여준다. 모든 속도는 NVIDIA T4 GPU에서 TensorRT FP16을 사용하여 측정된 값이다.</p>
<p><strong>Table 1: Performance Comparison on COCO val2017</strong></p>
<table><thead><tr><th>모델 (Model)</th><th>백본 (Backbone)</th><th>파라미터 (M)</th><th>GFLOPs</th><th>Latency (ms)</th><th><span class="math math-inline">AP^{val}</span> (%)</th></tr></thead><tbody>
<tr><td>RT-DETR</td><td>R18</td><td>20</td><td>60</td><td>4.6</td><td>46.5</td></tr>
<tr><td><strong>RT-DETRv2</strong></td><td><strong>R18 (S)</strong></td><td><strong>20</strong></td><td><strong>60</strong></td><td><strong>4.6</strong></td><td><strong>48.1</strong></td></tr>
<tr><td>RT-DETRv3</td><td>R18</td><td>20</td><td>60</td><td>4.6</td><td>48.1 / 48.7†</td></tr>
<tr><td>—</td><td>—</td><td>—</td><td>—</td><td>—</td><td>—</td></tr>
<tr><td>RT-DETR</td><td>R50</td><td>42</td><td>136</td><td>9.2</td><td>53.1</td></tr>
<tr><td><strong>RT-DETRv2</strong></td><td><strong>R50 (L)</strong></td><td><strong>42</strong></td><td><strong>136</strong></td><td><strong>9.2 (108 FPS)</strong></td><td><strong>53.4</strong></td></tr>
<tr><td>RT-DETRv3</td><td>R50</td><td>42</td><td>136</td><td>9.2</td><td>53.4</td></tr>
<tr><td><strong>RT-DETRv2</strong></td><td><strong>- (X)</strong></td><td><strong>76</strong></td><td><strong>259</strong></td><td><strong>13.5 (74 FPS)</strong></td><td><strong>54.3</strong></td></tr>
<tr><td>—</td><td>—</td><td>—</td><td>—</td><td>—</td><td>—</td></tr>
<tr><td>YOLOv8-S</td><td>-</td><td>11.2</td><td>28.6</td><td>7.1</td><td>46.2</td></tr>
<tr><td>YOLOv9-S</td><td>-</td><td>7.1</td><td>26.4</td><td>-</td><td>46.7</td></tr>
<tr><td>YOLOv10-S</td><td>-</td><td>7.2</td><td>21.6</td><td>2.5</td><td>46.3</td></tr>
<tr><td>—</td><td>—</td><td>—</td><td>—</td><td>—</td><td>—</td></tr>
<tr><td>YOLOv8-L</td><td>-</td><td>43.0</td><td>165.0</td><td>14.1</td><td>52.9</td></tr>
<tr><td>YOLOv10-L</td><td>-</td><td>24.4</td><td>120.3</td><td>7.28</td><td>53.2</td></tr>
<tr><td>YOLOv8-X</td><td>-</td><td>68.2</td><td>257.8</td><td>16.9</td><td>53.9</td></tr>
<tr><td>YOLOv10-X</td><td>-</td><td>29.5</td><td>160.4</td><td>10.7</td><td>54.4</td></tr>
</tbody></table>
<p>†: 120 에포크 훈련 결과. 데이터 출처: 5</p>
<h4>5.1.1 분석</h4>
<p>표의 결과는 RT-DETRv2의 “Bag-of-Freebies” 전략이 매우 효과적이었음을 명확하게 보여준다. 가장 주목할 점은 RT-DETRv2가 파라미터 수, GFLOPs, 그리고 추론 지연 시간(Latency)을 RT-DETR과 동일하게 유지하면서도 일관된 정확도(<span class="math math-inline">AP</span>) 향상을 달성했다는 것이다. 예를 들어, R18 백본을 사용하는 Small 스케일 모델의 경우, RT-DETRv2-S는 RT-DETR-R18 대비 무려 1.6% <span class="math math-inline">AP</span>라는 상당한 성능 향상을 이뤄냈다.13 이는 추론 비용의 증가 없이 오직 훈련 전략의 최적화만으로 모델의 잠재력을 끌어올렸음을 입증하는 강력한 증거이다.</p>
<p>YOLO 계열 모델과의 비교에서는 더욱 흥미로운 양상이 나타난다. 작은 모델 스케일에서는 YOLOv10-S와 같은 최신 모델이 극도로 낮은 파라미터와 GFLOPs로 매우 빠른 속도를 보여주며 효율성 측면에서 강점을 보인다. 그러나 모델 스케일이 커질수록(L, X), RT-DETRv2는 YOLO 계열 모델들을 능가하는 정확도를 달성한다. 특히 RT-DETRv2-X 모델은 54.3% <span class="math math-inline">AP</span>를 기록하며, YOLOv8-X(53.9% <span class="math math-inline">AP</span>)보다 높은 정확도를 보여준다. 이는 트랜스포머 아키텍처가 가진 전역적 컨텍스트 이해 능력이 복잡한 장면이나 객체 간의 관계를 파악하는 데 더 유리하게 작용할 수 있음을 시사한다.2 다만, YOLOv10-X가 훨씬 적은 파라미터와 GFLOPs로 더 높은 54.4% <span class="math math-inline">AP</span>를 달성한 것은 CNN 기반 아키텍처의 효율성 또한 끊임없이 발전하고 있음을 보여주며, 두 패러다임 간의 치열한 경쟁이 계속되고 있음을 알 수 있다.5</p>
<p>결론적으로, RT-DETRv2는 실시간 트랜스포머 탐지기의 성능 기준을 한 단계 끌어올렸으며, 특히 정확도가 최우선시되는 고성능 실시간 애플리케이션에서 강력한 대안으로 자리매김했다.</p>
<h3>5.2  Ablation Study 분석</h3>
<p>RT-DETRv2의 성능 향상이 어떤 기술 요소로부터 비롯되었는지 정량적으로 파악하기 위해 각 “Bag-of-Freebies” 요소의 기여도를 분석하는 Ablation Study(제거 연구) 결과를 살펴본다. 비록 명시적인 Ablation Study 표가 제공되지는 않았으나, 관련 논문 및 기술 문서의 서술을 바탕으로 그 효과를 재구성하면 다음과 같다.</p>
<p><strong>Table 2: Ablation Study of RT-DETRv2’s “Bag-of-Freebies”</strong></p>
<table><thead><tr><th>설정 (Configuration)</th><th><span class="math math-inline">AP^{val}</span> (%)</th><th><span class="math math-inline">AP_{50}</span> (%)</th><th>비고 (Notes)</th></tr></thead><tbody>
<tr><td>Baseline (RT-DETR-R18)</td><td>46.5</td><td>63.8</td><td>기준 모델 성능</td></tr>
<tr><td>+ Dynamic Data Augmentation</td><td>+</td><td>+</td><td>훈련 후반부 증강을 줄여 성능 향상</td></tr>
<tr><td>+ Scale-Adaptive Hyperparameters</td><td>+</td><td>+</td><td>모델 스케일에 맞는 학습률 적용으로 성능 최적화</td></tr>
<tr><td><strong>Full RT-DETRv2-R18 (S)</strong></td><td><strong>47.9 / 48.1</strong></td><td><strong>64.9</strong></td><td>모든 개선책 적용 시 상당한 성능 향상 13</td></tr>
<tr><td>RT-DETRv2-R18 with <span class="math math-inline">discrete_sample</span></td><td>~48.1</td><td>~64.9</td><td><span class="math math-inline">AP_{50}</span>에서 미미한 성능 차이로 배포 용이성 확보 14</td></tr>
</tbody></table>
<h4>5.2.1 분석</h4>
<p>Ablation Study 결과는 RT-DETRv2에 적용된 각 훈련 전략이 개별적으로도, 그리고 종합적으로도 성능 향상에 긍정적인 영향을 미쳤음을 시사한다. ’동적 데이터 증강’은 훈련 단계에 맞는 최적의 학습 환경을 제공함으로써 모델의 일반화 능력을 높이는 데 기여한다. ’규모 적응형 하이퍼파라미터’는 각 모델 스케일의 고유한 특성을 고려하여 학습을 최적화함으로써, 작은 모델부터 큰 모델까지 전반적인 성능을 끌어올리는 역할을 한다.</p>
<p>이러한 개선책들이 모두 적용된 완전한 RT-DETRv2-S 모델은 베이스라인인 RT-DETR-R18 대비 1.4~1.6% <span class="math math-inline">AP</span>라는 괄목할 만한 성능 향상을 달성했다. 이는 개별 기술 요소들이 서로 시너지를 일으켜 더 큰 폭의 개선을 이끌어냈음을 의미한다.</p>
<p>또한, 배포 실용성을 위해 제안된 <span class="math math-inline">discrete_sample</span> 연산자의 성능을 주목할 필요가 있다. 이 연산자를 적용했을 때, <span class="math math-inline">AP_{50}</span>(객체를 탐지했는지 여부를 판단하는 데 더 관대한 기준) 성능 저하가 거의 무시할 수 있는 수준으로 나타났다.14 이는 개발자들이 성능을 거의 희생하지 않으면서도 하드웨어 호환성 문제를 해결하고 모델을 더 넓은 환경에 배포할 수 있게 되었음을 의미한다. 따라서 <code>discrete_sample</code> 연산자는 정확도와 실용성 사이의 매우 성공적인 절충안이라 평가할 수 있다. 이처럼 RT-DETRv2의 각 구성 요소는 명확한 목적을 가지고 설계되었으며, 실험적으로 그 효과가 입증되었다.</p>
<h2>6.  구현 및 실제 적용</h2>
<h3>6.1  프레임워크 지원 및 생태계</h3>
<p>RT-DETRv2의 가장 큰 강점 중 하나는 뛰어난 접근성과 잘 구축된 개발 생태계이다. 모델의 성능이 아무리 뛰어나더라도 사용하기 어렵거나 특정 환경에 종속되어 있다면 널리 확산되기 어렵다. RT-DETRv2는 이러한 점을 고려하여 주요 딥러닝 프레임워크와 커뮤니티를 통해 폭넓게 지원된다.</p>
<ul>
<li>
<p><strong>공식 저장소:</strong> RT-DETRv2의 공식 코드는 <code>lyuwenyu/RT-DETR</code> GitHub 저장소를 통해 공개되어 있다.13 이곳에서는 연구 및 개발에 가장 널리 사용되는 두 프레임워크인 PyTorch와 PaddlePaddle 구현 코드를 모두 제공하여, 개발자들이 자신의 환경에 맞춰 선택적으로 사용할 수 있도록 지원한다.23</p>
</li>
<li>
<p><strong>Ultralytics 통합:</strong> RT-DETRv2는 전 세계적으로 가장 인기 있는 객체 탐지 라이브러리 중 하나인 <strong>Ultralytics</strong>에 공식적으로 통합되었다.8 이는 사용자들이 YOLO 모델을 다루는 것과 거의 동일한 방식으로 RT-DETRv2 모델을 손쉽게 로드하고, 커스텀 데이터셋에 대해 훈련, 검증, 추론 및 다양한 포맷으로의 내보내기(export)를 수행할 수 있음을 의미한다. 이러한 통합은 RT-DETRv2의 사용 장벽을 극적으로 낮추고, 방대한 YOLO 사용자 커뮤니티가 트랜스포머 기반 모델을 쉽게 접할 수 있는 기회를 제공한다.</p>
</li>
<li>
<p><strong>Hugging Face Transformers 지원:</strong> 또한, RT-DETRv2는 AI 모델의 허브 역할을 하는 <strong>Hugging Face Transformers</strong> 라이브러리에서도 지원된다.11 이를 통해 수많은 AI 개발자 및 연구자들이 표준화된 API를 통해 모델을 다운로드하고 다른 모델들과 연계하여 활용할 수 있게 되었으며, 이는 RT-DETRv2가 학계를 넘어 산업계 전반으로 확산되는 데 중요한 발판이 된다.</p>
</li>
</ul>
<p>이처럼 강력한 생태계 지원은 RT-DETRv2를 단순한 연구 결과물이 아닌, 누구나 쉽게 활용할 수 있는 실용적인 도구로 만들어주는 핵심적인 요소이다.</p>
<h3>6.2  사전 훈련 모델 (Pre-trained Models)</h3>
<p>RT-DETRv2는 다양한 애플리케이션의 요구사항에 대응하기 위해, 여러 규모와 성능을 가진 사전 훈련된 모델(pre-trained models) 라인업을 제공한다. 사용자는 자신의 프로젝트가 요구하는 정확도 수준과 사용 가능한 하드웨어의 연산 능력 사이의 트레이드오프를 고려하여 최적의 모델을 선택할 수 있다.</p>
<p>제공되는 모델은 일반적으로 백본 네트워크의 크기에 따라 구분되며, 다음과 같은 스케일로 제공된다 13:</p>
<ul>
<li>
<p><strong>RT-DETRv2-S (Small):</strong> ResNet-18과 같은 경량 백본을 사용하여 가장 빠른 추론 속도를 제공한다. 엣지 디바이스나 실시간성이 극도로 중요한 애플리케이션에 적합하다.</p>
</li>
<li>
<p><strong>RT-DETRv2-M (Medium):</strong> ResNet-34 또는 ResNet-50-m과 같은 중간 크기의 백본을 사용하여 속도와 정확도 사이의 균형을 맞춘 모델이다.</p>
</li>
<li>
<p><strong>RT-DETRv2-L (Large):</strong> ResNet-50을 백본으로 사용하여 높은 정확도를 제공하며, 대부분의 GPU 기반 실시간 탐지 시스템에 적합한 표준 모델이다.</p>
</li>
<li>
<p><strong>RT-DETRv2-X (Extra-Large):</strong> ResNet-101과 같은 대형 백본을 사용하여 최고의 정확도를 목표로 한다. 최고의 성능이 필요하고 충분한 연산 자원을 확보할 수 있는 경우에 선택된다.</p>
</li>
</ul>
<p>이러한 사전 훈련 모델들은 대규모 데이터셋(예: COCO, Objects365)으로 미리 학습되어 있어, 사용자들은 처음부터 모델을 훈련시킬 필요 없이 자신의 커스텀 데이터셋에 대해 전이 학습(transfer learning) 또는 미세 조정(fine-tuning)을 수행하여 빠르고 효과적으로 원하는 탐지기를 구축할 수 있다.</p>
<h3>6.3  주요 응용 분야 및 전망</h3>
<p>RT-DETRv2가 가진 높은 정확도, 실시간 추론 성능, 그리고 NMS-free End-to-End 파이프라인이라는 고유한 특성은 다양한 실제 산업 분야에서 그 잠재력을 발휘할 수 있게 한다.</p>
<ul>
<li>
<p><strong>자율 주행 (Autonomous Driving):</strong> 자율 주행 시스템은 주변 환경을 완벽하게 인식하기 위해 차량, 보행자, 자전거, 교통 신호 등 수많은 객체를 실시간으로, 그리고 매우 높은 신뢰도로 탐지해야 한다.25 RT-DETRv2의 트랜스포머 아키텍처는 이미지 전체의 전역적인 컨텍스트를 이해하는 데 강점을 가지므로, 다른 객체에 의해 일부가 가려진(occlusion) 객체를 탐지하거나 복잡한 교차로 상황을 해석하는 데 유리하다.2 또한, 실시간 성능은 차량의 빠른 의사결정에 필수적이다.8</p>
</li>
<li>
<p><strong>지능형 영상 감시 (Intelligent Video Surveillance):</strong> 공항, 기차역, 도심 등 넓은 지역을 감시하는 수많은 CCTV 영상에서 실시간으로 이상 행동, 침입, 혹은 버려진 물체 등을 자동으로 탐지하는 데 활용될 수 있다.30 특히 사람이 밀집한 군중 속에서 특정 인물을 추적해야 하는 경우, 기존 NMS 기반 탐지기는 중첩된 바운딩 박스를 잘못 제거하여 탐지에 실패할 수 있다. 반면, NMS가 없는 RT-DETRv2는 이러한 밀집 상황에서도 안정적인 추론 속도와 성능을 유지할 수 있어 감시 시스템의 신뢰도를 높일 수 있다.6</p>
</li>
<li>
<p><strong>로보틱스 및 산업 자동화 (Robotics and Industrial Automation):</strong> 공장 자동화 라인에서 부품을 집거나 조립하는 로봇, 혹은 물류 창고에서 상품을 운반하는 로봇은 주변 환경과 동적으로 상호작용하기 위해 객체를 빠르고 정확하게 인식해야 한다.2 RT-DETRv2의 높은 정확도는 로봇이 작업을 정밀하게 수행하도록 돕고, 실시간 성능은 로봇이 지연 없이 반응하도록 보장한다. 이를 통해 생산 효율성과 안전성을 크게 향상시킬 수 있다.8</p>
</li>
</ul>
<p>이 외에도 의료 영상 분석, 위성 이미지 분석, 스마트 리테일 등 정확성과 실시간성이 동시에 요구되는 거의 모든 컴퓨터 비전 분야에서 RT-DETRv2는 강력한 솔루션이 될 수 있다.</p>
<h2>7.  결론</h2>
<h3>7.1 RT-DETRv2의 핵심 기여 요약</h3>
<p>RT-DETRv2는 실시간 객체 탐지 분야에서 중요한 이정표를 세운 모델이다. 이는 단순히 기존 모델보다 더 빠르거나 정확한 모델을 만드는 것을 넘어, 어떻게 하면 강력한 연구 프로토타입을 실제 산업 현장에서 유용하게 사용될 수 있는 성숙한 기술로 발전시킬 수 있는지에 대한 모범적인 사례를 제시했다. “Bag-of-Freebies“라는 철학 아래, RT-DETRv2는 이미 강력한 실시간 탐지기였던 RT-DETR을 기반으로 추론 비용을 전혀 증가시키지 않으면서도 성능, 유연성, 그리고 배포 실용성을 한 단계 끌어올렸다.</p>
<ul>
<li>
<p><strong>성능:</strong> 동적 데이터 증강과 규모 적응형 하이퍼파라미터와 같은 최적화된 훈련 전략을 통해 동일한 아키텍처에서 더 높은 정확도를 이끌어냈다.</p>
</li>
<li>
<p><strong>유연성:</strong> 디코더의 Deformable Attention 메커니즘에 ‘선택적 다중 스케일 특징 추출’ 개념을 도입하여, 모델이 각 특징 스케일의 특성에 맞춰 보다 지능적으로 정보를 활용할 수 있도록 개선했다.</p>
</li>
<li>
<p><strong>실용성:</strong> 특정 하드웨어에서 배포 제약을 야기했던 <code>grid_sample</code> 연산자를 성능 저하 거의 없이 대체할 수 있는 ’이산 샘플링 연산자’를 제안함으로써, 모델의 범용성과 배포 편의성을 극대화했다.</p>
</li>
</ul>
<p>이러한 기여들은 학계 연구자들이 새로운 아이디어를 검증하는 것을 넘어, 그 기술이 실제 세계에서 마주할 현실적인 문제들을 어떻게 해결해야 하는지에 대한 중요한 시사점을 제공한다.</p>
<h3>7.2 실시간 객체 탐지의 미래</h3>
<p>RT-DETRv2의 성공은 실시간 객체 탐지 분야의 미래에 대한 몇 가지 중요한 방향성을 제시한다. 첫째, 트랜스포머 기반 모델이 더 이상 높은 연산량을 요구하는 무겁고 느린 아키텍처가 아니며, 고도로 최적화된 CNN 기반 모델(YOLO 계열)과의 실시간 성능 경쟁에서 동등하거나 그 이상의 잠재력을 가질 수 있음을 명확히 보여주었다. 이는 향후 CNN의 공간적 특징 추출 능력과 트랜스포머의 전역적 관계 모델링 능력을 결합한 하이브리드 아키텍처에 대한 연구를 더욱 가속화할 것이다.</p>
<p>둘째, NMS를 제거한 완전한 End-to-End 파이프라인의 실용성을 입증했다. 이는 복잡한 후처리 과정과 그에 따른 하이퍼파라미터 튜닝의 필요성을 없애 전체 개발 과정을 단순화하고, 특히 객체가 밀집한 어려운 시나리오에서 더 강건하고 안정적인 성능을 제공할 수 있음을 의미한다.</p>
<p>결론적으로, RT-DETRv2는 단순한 성능 개선을 넘어 실시간 객체 탐지 기술의 패러다임을 한 단계 진화시켰다. 앞으로의 실시간 객체 탐지 기술은 단순히 속도와 정확도라는 두 축을 넘어, RT-DETRv2가 보여준 것처럼 배포 용이성, 훈련 효율성, 그리고 아키텍처의 유연성과 같은 실용적인 가치를 중요한 경쟁력으로 삼는 방향으로 발전해 나갈 것으로 전망된다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>RT-DETRv2 Object Detector | ArcGIS API for Python | Esri Developer, https://developers.arcgis.com/python/latest/guide/rt-detrv2-object-detector/</li>
<li>RT-DETR: Hybrid Object Detection Model Combining Convolutions and Transformers | by David Cochard | ailia-ai | Medium, https://medium.com/axinc-ai/rt-detr-hybrid-object-detection-model-combining-convolution-and-transformer-26809f9fd742</li>
<li>DETRs Beat YOLOs on Real-time Object Detection - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_DETRs_Beat_YOLOs_on_Real-time_Object_Detection_CVPR_2024_paper.pdf</li>
<li>arXiv:2409.08475v3 [cs.CV] 19 Dec 2024, https://arxiv.org/pdf/2409.08475</li>
<li>RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision - arXiv, https://arxiv.org/html/2409.08475v2</li>
<li>anasch07/DETR-Object-Detection: An end-to-end object detection model using Transformers - GitHub, https://github.com/anasch07/DETR-Object-Detection</li>
<li>Baidu’s RT-DETR: A Vision Transformer-Based Real-Time Object Detector - Ultralytics Docs, https://docs.ultralytics.com/models/rtdetr/</li>
<li>DETRs Beat YOLOs on Real-time Object Detection - arXiv, https://arxiv.org/html/2304.08069v3</li>
<li>RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer, https://www.semanticscholar.org/paper/1e030d91607e38b7b7fdd002123ca8baafbedc8f</li>
<li>lyuwenyu/RT-DETR: [CVPR 2024] Official RT-DETR … - GitHub, https://github.com/lyuwenyu/RT-DETR</li>
<li>RT-DETRv2 - Hugging Face, https://huggingface.co/docs/transformers/model_doc/rt_detr_v2</li>
<li>RT-DETRv2 Beats YOLO? Full Comparison + Tutorial - Labellerr, https://www.labellerr.com/blog/rt-detrv2-beats-yolo-full-comparison-tutorial/</li>
<li>DETR Breakdown Part 2: Methodologies and Algorithms - PyImageSearch, https://pyimagesearch.com/2023/06/12/detr-breakdown-part-2-methodologies-and-algorithms/</li>
<li>Introduction to DETR - Part 2: The Crucial Role of the Hungarian Algorithm | DigitalOcean, https://www.digitalocean.com/community/tutorials/introduction-detr-hungarian-algorithm-2</li>
<li>DETR (Transformers for Object Detection) - Towards Data Science, https://towardsdatascience.com/detr-transformers-for-object-detection-a8b3327b737a/</li>
<li>DETR - Hugging Face, https://huggingface.co/docs/transformers/v4.17.0/model_doc/detr</li>
<li>DAMO-YOLO vs. RTDETRv2: A Technical Comparison - Ultralytics …, https://docs.ultralytics.com/compare/damo-yolo-vs-rtdetr/</li>
<li>YOLOv8 vs RTDETRv2: A Technical Comparison - Ultralytics YOLO …, https://docs.ultralytics.com/compare/yolov8-vs-rtdetr/</li>
<li>YOLOv7 vs RT-DETRv2: A Detailed Technical Comparison, https://docs.ultralytics.com/compare/yolov7-vs-rtdetr/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>