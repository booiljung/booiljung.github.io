<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:GLIP과 Grounding DINO (2021-12) 개방형 어휘 객체 탐지</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>GLIP과 Grounding DINO (2021-12) 개방형 어휘 객체 탐지</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>GLIP과 Grounding DINO (2021-12) 개방형 어휘 객체 탐지</span></nav>
                </div>
            </header>
            <article>
                <h1>GLIP과 Grounding DINO (2021-12) 개방형 어휘 객체 탐지</h1>
<h2>1. 서론: 언어 기반 개방형 어휘 객체 탐지의 패러다임</h2>
<p>전통적인 객체 탐지(Object Detection) 모델은 사전 정의된 고정된 클래스 목록에 대해서만 객체를 인식하고 분류할 수 있었다. 예를 들어, COCO 데이터셋으로 학습된 모델은 80개의 정해진 카테고리 외의 새로운 객체를 탐지할 수 없었다. 이러한 ‘폐쇄형(Closed-Set)’ 접근 방식은 실제 세계에 존재하는 무한하고 다양한 객체 개념을 처리하는 데 근본적인 한계를 지닌다.1 이러한 한계를 극복하기 위해 등장한 패러다임이 바로 ‘개방형 어휘(Open-Vocabulary)’ 탐지이다. 개방형 어휘 탐지는 훈련 데이터에 존재하지 않았던 새로운 카테고리의 객체를 “빨간 옷을 입은 사람“과 같은 자연어 텍스트 프롬프트를 통해 탐지하는 것을 목표로 한다.2 이는 인공 일반 지능(AGI) 시스템이 예측 불가능한 개방된 세계의 시나리오를 효과적으로 처리하기 위한 핵심 능력으로 간주된다.3</p>
<p>이러한 패러다임 전환의 핵심 해결책은 비전-언어 사전 학습(Vision-Language Pre-training, VLP)의 부상에서 찾을 수 있다. OpenAI의 CLIP과 같은 모델은 수억 개의 이미지-텍스트 쌍으로 구성된 대규모 데이터셋을 활용하여, 이미지와 텍스트를 동일한 의미론적 임베딩 공간에 정렬시키는 데 성공했다.4 이는 시각적 개념을 언어적으로 일반화할 수 있는 강력한 토대를 마련했다. GLIP과 Grounding DINO는 이러한 VLP 패러다임을 한 단계 발전시켜, 이미지 전체 수준의 의미 이해를 넘어 객체 수준의 위치 정보(localization)와 결합한 대표적인 모델이다. 이들은 단순 분류를 넘어 특정 객체의 위치를 정확히 찾아내는 탐지(detection) 및 그라운딩(grounding) 태스크로 비전-언어 모델의 적용 범위를 확장했다.</p>
<p>CLIP과 같은 모델에서 GLIP, Grounding DINO로 이어지는 발전 과정은 단순히 성능 개선을 넘어, 비전-언어 이해의 ’단위(granularity)’가 근본적으로 변화했음을 시사한다. CLIP이 이미지 전체에 대한 의미론적 정렬, 즉 “이 이미지에 무엇이 있는가?“라는 질문에 답했다면, GLIP과 Grounding DINO는 이 정렬 단위를 객체 또는 영역 수준으로 끌어내려 “이 이미지에서 <em>이것</em>은 어디에 있는가?“라는 더 세분된 질문에 답할 수 있게 만들었다. GLIP은 객체 탐지를 ‘구문 그라운딩’ 문제로 재정의함으로써 이미지의 구성 요소와 텍스트 구문을 명시적으로 연결하는 첫걸음을 내디뎠다.6 Grounding DINO는 이 개념을 더욱 정교하고 통합된 종단간(end-to-end) 탐지 아키텍처에 내재화하여 훨씬 더 정밀한 영역 수준의 정렬을 달성했다.2 이러한 세분성의 심화는 CLIP만으로는 불가능했던 개방형 어휘 인스턴스 분할(Grounded-SAM)이나 정밀한 로봇 상호작용과 같은 고차원적인 응용을 가능하게 하는 결정적인 기술적 도약이었다.7</p>
<h2>2. GLIP: 객체 탐지를 구문 그라운딩으로 재정의하다</h2>
<h3>2.1  핵심 개념: 탐지와 그라운딩의 통합</h3>
<p>GLIP(Grounded Language-Image Pre-training)의 가장 핵심적인 기여는 객체 탐지를 구문 그라운딩 문제로 완전히 재정의한 데 있다.6 기존 탐지 모델이 이미지 내 객체를 사전 정의된 클래스 ID로 분류하는 방식에서 벗어나, GLIP은 이미지의 특정 영역(region)과 입력된 텍스트 프롬프트 내의 단어 또는 구문(phrase) 간의 정렬(alignment) 점수를 계산하여 가장 일치하는 쌍을 찾는 방식으로 작동한다.9</p>
<p>이러한 접근 방식의 통합은 두 가지 중요한 이점을 가져왔다. 첫째, 서로 다른 성격의 데이터셋인 객체 탐지 데이터(바운딩 박스와 클래스 레이블)와 구문 그라운딩 데이터(바운딩 박스와 텍스트 구문)를 통합된 프레임워크에서 함께 사용할 수 있게 되었다. 이는 두 태스크가 서로의 학습을 보완하며 성능을 상호 향상시키는 시너지 효과를 창출했다.6 둘째, 이 통합을 통해 웹에서 대규모로 수집 가능한 이미지-캡션 쌍 데이터를 그라운딩 데이터로 활용할 길을 열었다. 이는 모델이 방대한 양의 시각적 개념을 학습하여 매우 풍부한 의미론적(semantic-rich) 시각 표현을 구축하도록 도왔다.10</p>
<h3>2.2  아키텍처 분석: 듀얼 인코더와 심층 융합</h3>
<p>GLIP은 이미지 인코더와 언어 인코더를 공동으로 학습시키는 듀얼 인코더(dual-encoder) 구조를 기반으로 한다.6</p>
<ul>
<li>
<p><strong>이미지 인코더</strong>: 당시 최고 수준의 객체 탐지기 헤드였던 DyHead를 이미지 인코더로 채택하여, 이미지로부터 강력하고 다층적인 시각 특징을 추출한다.9</p>
</li>
<li>
<p><strong>언어 인코더</strong>: BERT를 언어 인코더로 사용하여 입력된 텍스트 프롬프트로부터 문맥을 이해하는 단어 및 구문 특징을 추출한다.9</p>
</li>
<li>
<p><strong>심층 융합 (Deep Fusion)</strong>: GLIP은 두 양식의 정보를 단순히 최종 단계에서 결합하는 ‘후기 융합(late fusion)’ 방식 대신, 인코더의 마지막 몇 개 레이어에서 정보를 조기에 융합하는 ‘심층 융합’ 방식을 도입했다. 이 융합은 교차 양식 다중 헤드 어텐션(Cross-Modality Multi-Head Attention, X-MHA) 모듈을 통해 이루어진다. 시각 특징이 언어 특징에 어텐션하고, 언어 특징이 시각 특징에 어텐션하는 양방향 정보 교환을 통해, 모델은 언어의 의미를 이해하는 ‘language-aware’ 시각 표현을 학습하게 된다.6</p>
</li>
<li>
<p><strong>손실 함수 (Loss Function)</strong>: GLIP의 학습 목표는 그라운딩 손실과 지역화 손실의 합으로 정의된다. 그라운딩 손실은 이미지 영역 특징 벡터 행렬 <span class="math math-inline">O</span>와 텍스트 토큰 특징 벡터 행렬 <span class="math math-inline">P</span> 간의 내적(dot product)을 통해 정렬 점수 <span class="math math-inline">S_{ground}</span>를 계산하고, 이를 최적화하는 방식으로 기존 탐지 모델의 분류 손실을 대체한다.9<br />
<span class="math math-display">
S_{ground} = O \cdot P^T
</span><br />
이 점수 행렬을 기반으로 대조 손실(contrastive loss)을 계산하여 올바른 영역-단어 쌍의 점수는 높이고, 틀린 쌍의 점수는 낮추도록 학습한다. 전체 손실 함수는 이 그라운딩 손실과 바운딩 박스의 위치를 정확하게 예측하기 위한 표준 지역화 손실(e.g., L1 loss, GIoU loss)의 합으로 구성된다.</p>
</li>
</ul>
<h3>2.3  학습 방법론: 대규모 그라운딩 데이터와 자기 학습</h3>
<p>GLIP의 강력한 제로샷 및 퓨샷 성능은 대규모 데이터 기반의 사전 학습 전략에 기인한다. 모델은 총 2,700만 개의 그라운딩 데이터로 사전 학습되었으며, 이는 300만 개의 사람이 직접 주석을 단 고품질 데이터와 2,400만 개의 웹에서 수집한 이미지-텍스트 쌍으로 구성된다.6</p>
<p>이처럼 방대한 데이터를 효율적으로 활용하기 위해 GLIP은 교사-학생(teacher-student) 패러다임에 기반한 자기 학습(self-training) 기법을 도입했다. 먼저, 고품질의 주석 데이터(gold data)로 학습된 ’교사 GLIP 모델’을 구축한다. 그 후, 이 교사 모델을 사용하여 2,400만 개의 웹 이미지-텍스트 쌍에 대해 의사(pseudo) 바운딩 박스 레이블을 자동으로 생성한다. 이 과정에서 7,810만 개 이상의 높은 신뢰도를 가진 의사 주석이 생성되었다.6 마지막으로, ’학생 GLIP 모델’은 사람이 만든 주석 데이터와 교사 모델이 생성한 방대한 양의 의사 레이블 데이터를 모두 사용하여 학습된다. 이 전략을 통해 GLIP은 데이터의 양과 질을 모두 확보하여 강력한 일반화 성능을 달성할 수 있었다.6</p>
<p>GLIP의 기여는 새로운 아키텍처의 발명보다는 기존의 강력한 구성 요소(DyHead, BERT)를 실용적으로 재구성한 데 있다. 진정한 혁신은 ’탐지를 그라운딩으로’라는 문제의 재정의와 ’교사-학생 모델을 통한 데이터 확장’이라는 학습 전략에 있었다. 이 접근 방식은 개방형 어휘 탐지라는 개념의 실현 가능성을 입증하는 중요한 교량 역할을 했으며, Grounding DINO와 같은 보다 근본적인 아키텍처 혁신을 이끌어내는 발판이 되었다.</p>
<h2>3. Grounding DINO: 트랜스포머 디텍터와 그라운딩의 결합</h2>
<h3>3.1  핵심 개념: DINO 기반의 개방형 어휘 탐지</h3>
<p>Grounding DINO는 Transformer 기반의 최첨단 객체 탐지기인 DINO와 GLIP이 증명한 그라운딩 사전 학습의 개념을 결합(marrying)한 모델이다.2 DINO는 DETR 계열 모델의 한계를 개선한 모델로, NMS(Non-Maximum Suppression)와 같은 복잡한 후처리 과정이 필요 없는 완전한 종단간(end-to-end) 아키텍처를 제공한다. 이러한 깔끔한 구조는 언어 정보와의 통합을 훨씬 더 자연스럽고 효율적으로 만들었다.2</p>
<p>Grounding DINO는 GLIP의 성공에서 영감을 받아 그라운딩 사전 학습의 철학을 계승했지만, 이를 기존 탐지기 파이프라인을 수정하는 방식이 아닌, DINO라는 더 강력하고 통합된 Transformer 아키텍처 위에 처음부터 구축했다는 점에서 근본적인 차이가 있다.2</p>
<h3>3.2  아키텍처 분석: 다단계 긴밀한 융합 전략</h3>
<p>Grounding DINO의 핵심적인 아키텍처 혁신은 탐지 파이프라인을 특징 추출(neck), 쿼리 초기화(query initialization), 예측 정제(head)의 세 단계로 개념적으로 나누고, 각 단계에서 시각과 언어 정보의 ‘긴밀한(tight)’ 융합을 수행한다는 점이다.2</p>
<ol>
<li><strong>특징 강화기 (Feature Enhancer)</strong>: 이미지 백본(Swin Transformer)과 텍스트 백본(BERT)에서 추출된 초기 특징을 융합하고 강화하는 첫 단계이다. 이 모듈은 Deformable Self-Attention (이미지 특징 내), Text-to-Image Cross-Attention (이미지 특징이 텍스트 정보에 주목), Image-to-Text Cross-Attention (텍스트 특징이 이미지 정보에 주목)을 순차적으로 적용하여, 두 양식의 특징이 서로의 정보를 반영하여 상호 보완적으로 풍부해지도록 만든다.1</li>
<li><strong>언어 유도 쿼리 선택 (Language-Guided Query Selection)</strong>: 기존 DETR 계열 모델들이 학습 가능한 고정된 객체 쿼리를 사용하는 것과 달리, Grounding DINO는 텍스트 프롬프트와 가장 관련성이 높은 이미지 특징들을 동적으로 선택하여 디코더의 초기 쿼리로 사용한다. 이는 탐지기가 탐색을 시작하는 단계부터 언어 정보에 의해 유도된, 가장 유망한 영역에 집중하도록 만들어 수렴을 가속화하고 제로샷 성능을 크게 향상시킨다.1</li>
<li><strong>교차 양식 디코더 (Cross-Modality Decoder)</strong>: DINO의 디코더 구조를 확장하여, 각 디코더 레이어에 텍스트 교차 어텐션(text cross-attention)을 추가했다. 이로 인해 객체 쿼리가 여러 디코더 레이어를 거치며 반복적으로 정제되는 과정에서, 이미지 특징뿐만 아니라 텍스트 특징과도 지속적으로 상호작용하며 정렬된다. 이를 통해 최종적으로 예측되는 바운딩 박스는 시각적 증거와 언어적 의미를 모두 충실히 반영하게 된다.1</li>
</ol>
<h3>3.3  학습 방법론: 대조 손실과 하위 문장 수준 특징</h3>
<p>Grounding DINO의 학습 목표는 크게 두 가지 손실 함수의 결합으로 이루어진다. 하나는 DINO와 마찬가지로 예측된 바운딩 박스의 위치 정확도를 높이기 위한 표준 지역화 손실(L1 loss와 GIoU loss)이다. 다른 하나는 예측된 객체(쿼리)가 올바른 텍스트 구문과 연결되도록 하는 대조 손실(contrastive loss)이다. 이 대조 손실은 초점 손실(Focal Loss)을 사용하여 계산되며, 각 쿼리와 텍스트 토큰 간의 유사도를 높이거나 낮추는 방식으로 학습이 진행된다.1</p>
<p>또한, Grounding DINO는 텍스트 프롬프트를 처리하는 방식에서도 GLIP을 개선했다. GLIP은 모든 카테고리 이름을 하나의 문장으로 단순히 이어 붙여 사용했는데, 이는 “개“와 “고양이“를 탐지할 때 서로 관련 없는 두 개념이 텍스트 인코더 내에서 불필요한 의미적 간섭을 일으킬 수 있는 문제를 야기했다.16 Grounding DINO는 이 문제를 해결하기 위해 ’하위 문장 수준 특징(sub-sentence level features)’이라는 기법을 제안했다. 이는 입력 프롬프트를 “개”, “고양이“와 같이 의미적으로 독립적인 구문 단위로 분할하고, 각 단위를 독립적으로 인코딩하는 방식이다. 또한, 어텐션 마스크를 적용하여 각 구문 내의 토큰들만 서로에게 어텐션을 적용하도록 제한함으로써, 명확하고 모호하지 않은 그라운딩을 가능하게 하여 복잡한 프롬프트에 대한 강건성을 크게 높였다.1</p>
<p>Grounding DINO의 우수성은 시각(Transformer)과 언어(Transformer) 구성 요소 간의 아키텍처적 동질성에서 비롯된다. GLIP이 이종(CNN 계열 + Transformer) 아키텍처를 결합한 반면, Grounding DINO는 동일한 Transformer 기반 구조를 공유한다. 이러한 동질성은 이미지 패치와 텍스트 토큰을 근본적으로 동일한 방식으로 처리할 수 있게 하여, 특징 강화기, 쿼리 선택, 디코더 등 파이프라인의 모든 단계에서 훨씬 더 자연스럽고 깊은 정보 융합을 가능하게 했다. 이 깊은 융합은 단순한 성능 향상을 넘어, 복잡한 속성 기반의 쿼리를 처리하고 SAM과 같은 다른 Transformer 기반 모델과 원활하게 결합되는 등 새로운 차원의 능력을 발현시키는 핵심적인 동력이 되었다.</p>
<h2>4. 주요 모델 비교 분석: GLIP 대 Grounding DINO</h2>
<h3>4.1  철학적 접근 방식의 차이</h3>
<p>GLIP과 Grounding DINO는 개방형 어휘 탐지라는 동일한 목표를 추구하지만, 그 접근 방식에는 철학적인 차이가 존재한다.</p>
<ul>
<li>GLIP: 개념적 재정의 (Conceptual Reformulation)</li>
</ul>
<p>GLIP은 기존의 강력한 객체 탐지기 아키텍처(DyHead)를 거의 그대로 유지하면서, 최종 분류 헤드 부분만을 그라운딩 문제로 대체하는 실용적인 접근 방식을 취했다.9 이는 아키텍처의 근본적인 변경을 최소화하면서도, 문제 정의를 바꿈으로써 개방형 어휘 능력을 효과적으로 구현한 사례이다.</p>
<ul>
<li>Grounding DINO: 구조적 통합 (Architectural Integration)</li>
</ul>
<p>Grounding DINO는 언어 정보를 모델의 핵심 구성 요소 전반에 걸쳐 깊숙이 통합하는 보다 근본적인 접근 방식을 채택했다. 이는 DINO라는 현대적인 Transformer 기반 탐지기 위에서 가능했으며, 특징 추출, 쿼리 생성, 예측 정제 등 모든 과정에 언어가 관여하는 구조를 설계했다.2</p>
<h3>4.2  아키텍처의 진화와 혁신</h3>
<p>두 모델의 아키텍처를 비교하면 비전-언어 융합 방식의 뚜렷한 진화를 확인할 수 있다.</p>
<ul>
<li><strong>기반 탐지기</strong>: GLIP은 CNN 기반의 헤드 구조를 가진 DyHead를 사용한 반면, Grounding DINO는 완전한 Transformer 기반의 종단간(end-to-end) 구조인 DINO를 채택했다. 이로 인해 Grounding DINO는 NMS와 같은 수작업 후처리 과정이 불필요하여 파이프라인이 더 간결하고 효율적이다.9</li>
<li><strong>융합 방식</strong>: GLIP은 인코더 후반부에서 ’심층 융합’을 수행하지만, 본질적으로는 분리된 두 인코더의 후기 결합에 가깝다.6 반면, Grounding DINO는 특징 강화기, 쿼리 선택, 디코더에 걸친 ‘다단계 긴밀한 융합’ 전략을 통해 훨씬 더 깊고 지속적인 상호작용을 유도한다. 이러한 구조적 차이가 Grounding DINO가 더 우수한 성능을 보이는 핵심적인 이유 중 하나로 분석된다.2</li>
<li><strong>텍스트 처리</strong>: GLIP은 카테고리 이름을 단순 연결하여 의미 간섭의 여지를 남겼으나 3, Grounding DINO는 ‘하위 문장 수준’ 처리 기법으로 이를 개선하여 복잡한 프롬프트에 대한 이해도와 강건성을 높였다.1</li>
</ul>
<h3>4.3  성능 벤치마크 종합</h3>
<p>제로샷(Zero-Shot) 성능은 모델이 훈련 데이터에서 보지 못한 데이터셋에 대해 얼마나 잘 일반화하는지를 보여주는 핵심 지표이다.</p>
<p>GLIP-L 모델은 CVPR 2022 발표 당시, COCO val2017 데이터셋에 대해 49.8 AP, LVIS val 데이터셋에 대해 26.9 AP를 달성하며 당시의 많은 지도 학습 기반 모델을 능가하는 강력한 제로샷 성능을 입증했다.6</p>
<p>Grounding DINO는 여기서 한 걸음 더 나아가, COCO 제로샷 벤치마크에서 52.5 AP를 달성했으며, 13개의 다양한 실제 환경 데이터셋으로 구성된 ODinW(Object Detection in the Wild) 벤치마크에서도 26.1 mAP로 새로운 최고 기록을 세웠다.2</p>
<p>이후 연구는 계속해서 발전을 거듭하여, 더 크고 정제된 데이터셋(Grounding-30M)으로 학습한 최신 버전인 Grounding DINO 1.6 Pro는 COCO에서 55.4 AP, LVIS-minival에서 57.7 AP라는 훨씬 더 높은 성능을 달성하며 개방형 어휘 탐지 기술의 수준을 한 단계 끌어올렸다.20</p>
<p>다음 표는 주요 벤치마크에서 두 모델 계열의 제로샷 성능을 종합적으로 비교한 것이다.</p>
<table><thead><tr><th>모델 (Model)</th><th>사전 학습 데이터 (Pre-train Data)</th><th>COCO val2017 Zero-Shot AP</th><th>LVIS val Zero-Shot AP</th><th>ODinW Zero-Shot mAP</th></tr></thead><tbody>
<tr><td>GLIP-T (Swin-T)</td><td>O365, GoldG, CC3M, SBU</td><td>46.6</td><td>20.1 (APr)</td><td>42.7</td></tr>
<tr><td>GLIP-L (Swin-L)</td><td>O365, GoldG, CC12M, SBU</td><td><strong>49.8</strong></td><td><strong>26.9</strong></td><td>-</td></tr>
<tr><td>Grounding DINO-T (Swin-T)</td><td>O365, GoldG, CC3M, SBU</td><td>50.6 (MM-GD)</td><td>41.4 (LVIS-mini)</td><td>-</td></tr>
<tr><td>Grounding DINO-L (Swin-L)</td><td>O365, GoldG, Cap4M</td><td><strong>52.5</strong></td><td>-</td><td><strong>26.1</strong></td></tr>
<tr><td>Grounding DINO 1.6 Pro</td><td>Grounding-30M</td><td><strong>55.4</strong></td><td><strong>51.1</strong></td><td>-</td></tr>
</tbody></table>
<h2>5. 응용 분야 및 생태계 확장</h2>
<h3>5.1 GLIP의 응용</h3>
<p>GLIP은 특히 특정 전문 분야(domain-specific)에서 강력한 제로샷 및 퓨샷 성능을 기반으로 활발히 응용되고 있다.</p>
<ul>
<li><strong>특수 도메인 적용</strong>: 의료 영상 분야에서는 Med-GLIP이라는 파생 모델이 개발되어, 언어 구문과 의료 영상 내 특정 병변 영역을 정렬하는 데 사용된다.21 또한 3D 포인트 클라우드 데이터에서 특정 부품을 분할하거나 23, 수도관 내부의 결함을 탐지하는 등 24, 데이터 수집이 어려운 전문 분야에서 그 가치를 입증하고 있다. 이는 복잡한 도메인 지식을 프롬프트 튜닝을 통해 모델에 효율적으로 주입할 수 있는 유연성 덕분이다.</li>
<li><strong>로보틱스 및 자율 시스템</strong>: 로봇이 목표 객체를 탐색하고, 그 탐지 결과를 검증하는 프레임워크에 GLIP이 활용되어, 사전에 라벨링된 데이터에 대한 의존도를 크게 줄이고 자율성을 높이는 데 기여한다.7</li>
</ul>
<h3>5.2 Grounding DINO의 응용 및 생태계</h3>
<p>Grounding DINO는 특정 분야를 넘어 범용적인 탐지기로서 더 넓은 생태계를 구축하고 있다.</p>
<ul>
<li><strong>범용 탐지기로서의 역할</strong>: 자율 주행, 감시 및 보안, 소매업 재고 관리, 로보틱스, 생물 다양성 모니터링, 제조업 품질 관리 등 매우 광범위한 산업 분야에서 범용 제로샷 탐지기로 사용될 잠재력을 가지고 있다.13</li>
<li><strong>자동 라벨링 (Auto-Labeling)</strong>: Grounding DINO의 가장 실용적인 응용 중 하나는 자동 라벨링이다. 수동으로 데이터를 라벨링하는 데 드는 막대한 시간과 비용을 획기적으로 줄일 수 있어, 많은 기업과 연구자들이 데이터셋 구축의 병목 현상을 해결하기 위해 이를 활용하고 있다. Grounding DINO로 데이터셋의 초벌 라벨링을 신속하게 수행한 후, YOLO와 같은 경량 모델을 학습시키는 파이프라인이 널리 제안되고 있다.26</li>
<li><strong>파운데이션 모델 생태계의 핵심 구성 요소</strong>: Grounding DINO의 가장 큰 영향력은 다른 파운데이션 모델과의 결합에서 나타난다. 특히 <strong>Grounded-SAM</strong>은 Grounding DINO의 “무엇을(what)” 탐지하는 능력과 SAM(Segment Anything Model)의 “어떻게(how)” 분할하는 능력을 결합한 것이다.8 이 결합을 통해 텍스트 프롬프트만으로 이미지 내 특정 객체를 정확히 분할해내는 **개방형 어휘 인스턴스 분할(Open-Vocabulary Instance Segmentation)**이라는 새로운 태스크가 가능해졌다. 이는 비전 AI 생태계에서 매우 중요한 기술적 진전으로 평가받는다.30</li>
</ul>
<p>두 모델의 응용 사례를 분석해 보면, GLIP은 특정 도메인 문제를 해결하기 위한 강력한 ’응용 특화 도구’로 활용되는 경향이 있는 반면, Grounding DINO는 Grounded-SAM과 같이 더 큰 AI 시스템을 구성하는 범용적인 ’시스템 수준의 구성 요소’로 자리매김하고 있음을 알 수 있다. 이는 Grounding DINO의 더 일반적이고 강건하며 구조적으로 깔끔한 설계가 다른 모델과의 결합(composability)에 더 유리하기 때문으로 분석된다. 이러한 경향은 미래 AI 개발이 강력한 파운데이션 모델들을 조립하여 더 복잡한 시스템을 구축하는 방향으로 나아갈 것임을 시사하며, 이 패러다임에서 Grounding DINO의 역할은 단순한 탐지기 성능을 넘어선 장기적인 영향력을 가질 것임을 보여준다.</p>
<h2>6. 한계점 및 향후 연구 방향</h2>
<h3>6.1 GLIP의 한계점</h3>
<ul>
<li><strong>도메인 격차 (Domain Gap)</strong>: 자연 이미지로 사전 학습된 GLIP은 의료 영상과 같이 분포가 크게 다른 특정 도메인에 직접 적용될 때 제로샷 성능이 현저히 저하되는 경향이 있다. 따라서 효과적인 활용을 위해서는 해당 도메인에 특화된 파인튜닝이 필수적이다.22</li>
<li><strong>확장성 및 과적합</strong>: GLIP과 같은 대규모 비전-언어 모델은 매우 큰 데이터셋으로 학습할 때 확장성 문제나 특정 데이터 분포에 과적합될 위험이 있다.32</li>
<li><strong>해석 가능성 부족</strong>: 모델이 특정 프롬프트를 기반으로 어떻게 객체를 탐지하는지에 대한 내부 작동 방식이 불투명하다. 이는 모델의 결정을 신뢰하기 어렵게 만들고, 실패 사례가 발생했을 때 원인을 분석하고 디버깅하는 것을 어렵게 한다.31</li>
</ul>
<h3>6.2 Grounding DINO의 한계점</h3>
<ul>
<li><strong>사전 학습 데이터 의존성</strong>: 제로샷 성능이 사전 학습 데이터의 구성에 크게 의존한다. 예를 들어, LVIS 데이터셋의 희귀 클래스(rare classes)에 대한 성능이 일반 클래스보다 상대적으로 떨어지는 경향을 보이는데, 이는 해당 희귀 객체들이 사전 학습 데이터에 충분히 포함되지 않았기 때문으로 분석된다.33</li>
<li><strong>정확도와 속도의 상충 관계 (Trade-off)</strong>: YOLO와 같은 실시간 탐지기에 비해 추론 속도가 현저히 느려, 실시간 처리가 필수적인 시나리오(e.g., 실시간 비디오 분석)에 직접 적용하기에는 한계가 있다.14 이를 해결하기 위해 Grounding DINO Edge 버전과 같은 경량화 모델이 개발되고 있다.13</li>
<li><strong>Grounded-SAM의 한계</strong>: Grounding DINO를 사용하는 Grounded-SAM 파이프라인은 복잡한 형태의 경계, 배경과 대비가 낮은 영역, 그리고 매우 작은 객체에 대한 분할 마스크의 정확도가 떨어질 수 있다. 또한, 항공 이미지에서 지붕과 태양광 패널을 구분하는 것과 같이 미세한 분류가 필요한 특정 도메인에서는 부정확한 분류가 발생할 수 있다.29</li>
</ul>
<h3>6.3 향후 연구 방향</h3>
<ul>
<li><strong>효율적인 도메인 적응</strong>: 소량의 데이터만으로 특정 도메인에 빠르고 효율적으로 모델을 적응시키는 방법론에 대한 연구가 필요하다. 프롬프트 튜닝, LoRA와 같은 파라미터 효율적 파인튜닝(PEFT) 기법들이 유망한 방향으로 연구되고 있다.20</li>
<li><strong>데이터 의존성 완화</strong>: 사전 학습 데이터에 덜 의존하고, 진정한 의미의 개방형 시나리오에서 처음 보는 개념에 대해서도 강건하게 작동하는 모델을 개발하는 것이 중요한 과제이다.</li>
<li><strong>실시간 성능 확보</strong>: 모델 압축, 지식 증류(knowledge distillation), 하드웨어 최적화(e.g., TensorRT) 등을 통해 실시간 추론이 가능한 경량화된 개방형 어휘 탐지 모델에 대한 연구가 활발히 진행될 것이다.</li>
<li><strong>통합 모델 (Unified Models)</strong>: 탐지, 분할, 캡셔닝, VQA 등 다양한 시각 태스크를 하나의 통합된 모델에서 동시에 수행하는 차세대 파운데이션 모델로의 발전이 기대된다. Grounded-SAM은 이러한 방향성의 초기 단계로 볼 수 있으며, 앞으로 더 많은 기능이 통합될 것이다.8</li>
</ul>
<h2>7. 결론: 차세대 비전-언어 모델을 향하여</h2>
<p>본 안내서는 개방형 어휘 객체 탐지 분야의 두 핵심 모델인 GLIP과 Grounding DINO를 심층적으로 분석했다. GLIP은 객체 탐지를 구문 그라운딩 문제로 재정의하고 대규모 데이터 기반 사전 학습의 효용성을 입증함으로써 개방형 어휘 탐지의 가능성을 열었다. Grounding DINO는 이러한 철학을 계승하여, 더 우수하고 통합적인 Transformer 아키텍처 위에서 다단계 융합 전략을 통해 성능과 유연성을 한 차원 높은 수준으로 끌어올렸다.</p>
<p>GLIP에서 Grounding DINO로의 발전은 단순히 성능 향상을 넘어, 비전-언어 모델이 어떻게 설계되어야 하는지에 대한 패러다임의 전환을 명확히 보여준다. 이는 이종(heterogeneous) 아키텍처의 실용적 결합에서 동종(homogeneous) Transformer 아키텍처의 깊은 통합으로의 이동을 의미하며, 후자가 더 높은 성능과 확장성을 제공함을 입증했다.</p>
<p>현재 Grounding DINO와 같은 모델은 독립적인 애플리케이션을 넘어, 더 큰 AI 시스템을 구성하는 핵심적인 ’지각 모듈(perception module)’로 기능하고 있다. 특히 Grounded-SAM과의 결합은 이러한 경향을 명확히 보여준다. 향후 연구는 이러한 모듈의 성능, 효율성, 그리고 대규모 언어 모델(LLM)이나 생성 모델과의 상호작용 능력을 극대화하는 방향으로 나아갈 것이다. 이는 궁극적으로 인간의 언어로 소통하며 시각 세계를 정밀하게 이해하고 조작할 수 있는, 진정한 의미의 다중 양식(multi-modal) 지능 시스템의 기반이 될 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Fine-Tuning Grounding DINO: Open-Vocabulary Object Detection, https://learnopencv.com/fine-tuning-grounding-dino/</li>
<li>MARRYING DINO WITH GROUNDED PRE … - OpenReview, https://openreview.net/pdf/4ea82438d972660ee0a54720c0e82663ef6cbe59.pdf</li>
<li>Marrying DINO with Grounded Pre-Training for Open-Set Object Detection - European Computer Vision Association, https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06319.pdf</li>
<li>Understanding the Differences Between CLIP, Grounding DINO, and SAM: A Deep Dive into Vision-Language Models and Segmentation | by Shawn | Jul, 2025 | Medium, https://medium.com/@hexiangnan/understanding-the-differences-between-clip-grounding-dino-and-sam-a-deep-dive-into-b0724d15d92c</li>
<li>CLIP: Connecting text and images - OpenAI, https://openai.com/index/clip/</li>
<li>Grounded Language-Image Pre-training - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Grounded_Language-Image_Pre-Training_CVPR_2022_paper.pdf</li>
<li>Reliable Semantic Understanding for Real World Zero-shot Object Goal Navigation - arXiv, https://arxiv.org/html/2410.21926v1</li>
<li>Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks - arXiv, https://arxiv.org/html/2401.14159v1</li>
<li>GLIP: Grounded Language-Image Pre-training | by Sik-Ho Tsang - Medium, https://sh-tsang.medium.com/glip-grounded-language-image-pre-training-2be2483295b3</li>
<li>(PDF) Grounded Language-Image Pre-training - ResearchGate, https://www.researchgate.net/publication/356841575_Grounded_Language-Image_Pre-training</li>
<li>[2112.03857] Grounded Language-Image Pre-training - arXiv, https://arxiv.org/abs/2112.03857</li>
<li>Cvpr2022 Glip Grounded Language Image Pre Training | PDF | Computer Vision - Scribd, https://www.scribd.com/document/617774192/Cvpr2022-Glip-Grounded-Language-Image-Pre-Training</li>
<li>Grounding DINO 1.5: Pushing the Boundaries of Open-Set Object Detection | DigitalOcean, https://www.digitalocean.com/community/tutorials/grounding-dino-1-5-open-set-object-detection</li>
<li>Grounding DINO : SOTA Zero-Shot Object Detection - Roboflow Blog, https://blog.roboflow.com/grounding-dino-zero-shot-object-detection/</li>
<li>Exploring DINO Family Part2: Grounding DINO for Open-Set Object …, https://deepdataspace.com/en/blog/14/</li>
<li>Marrying DINO with Grounded Pre-Training for Open-Set Object Detection - arXiv, https://arxiv.org/html/2303.05499v5</li>
<li>Enhanced Zero-shot Labels using DINO &amp; Grounded Pre-training, https://www.labellerr.com/blog/grounded-dino-combining-dino-with-grounded-pre-training-for-open-set-object-annotations/</li>
<li>Marrying DINO with Grounded Pre-Training for Open-Set Object Detection - arXiv, https://arxiv.org/html/2303.05499</li>
<li>Grounded Language-Image Pre-Training - CVPR 2022 Open Access Repository, https://openaccess.thecvf.com/content/CVPR2022/html/Li_Grounded_Language-Image_Pre-Training_CVPR_2022_paper.html</li>
<li>Grounding DINO 1.6 - DeepDataSpace, https://deepdataspace.com/blog/6/</li>
<li>[2508.10528] Med-GLIP: Advancing Medical Language-Image Pre-training with Large-scale Grounded Dataset - arXiv, https://www.arxiv.org/abs/2508.10528</li>
<li>Med-GLIP: Advancing Medical Language-Image Pre-training with Large-scale Grounded Dataset - arXiv, https://arxiv.org/html/2508.10528v1</li>
<li>Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models, https://arxiv.org/html/2212.01558v2</li>
<li>Enhancing urban infrastructure: water supply and drainage pipeline defect detection with the GLIP pre-trained model and the YOLO, https://iwaponline.com/jwcc/article-pdf/doi/10.2166/wcc.2024.673/1504104/jwc2024673.pdf</li>
<li>Zero-shot Object Detection Using Grounding DINO Base - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2024/10/grounding-dino-base/</li>
<li>Teacher–Student Model Using Grounding DINO and You Only Look Once for Multi-Sensor-Based Object Detection - MDPI, https://www.mdpi.com/2076-3417/14/6/2232</li>
<li>GroundingDINO Object Detection Model: What is, How to Use - Roboflow, https://roboflow.com/model/groundingdino</li>
<li>Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection | Request PDF - ResearchGate, https://www.researchgate.net/publication/369116353_Grounding_DINO_Marrying_DINO_with_Grounded_Pre-Training_for_Open-Set_Object_Detection</li>
<li>Grounding Dino + SAM - Labelbox, https://labelbox.com/product/model/foundry-models/grounding-dino-sam/</li>
<li>Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks - arXiv, https://arxiv.org/abs/2401.14159</li>
<li>Prompt as Knowledge Bank: Boost Vision-language model via Structural Representation for zero-shot medical detection | OpenReview, https://openreview.net/forum?id=l0t2rumAvR</li>
<li>GLIP zero-shot transfers to various detection tasks, by writing the categories of interest into a text prompt. - ResearchGate, https://www.researchgate.net/figure/GLIP-zero-shot-transfers-to-various-detection-tasks-by-writing-the-categories-of_fig1_356841575</li>
<li>Marrying DINO with Grounded Pre-Training for Open-Set Object Detection | OpenReview, https://openreview.net/forum?id=DS5qRs0tQz</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>