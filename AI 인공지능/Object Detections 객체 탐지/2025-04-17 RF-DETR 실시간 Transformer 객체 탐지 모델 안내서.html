<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:RF-DETR 실시간 Transformer 객체 탐지 모델 안내서</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>RF-DETR 실시간 Transformer 객체 탐지 모델 안내서</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>RF-DETR 실시간 Transformer 객체 탐지 모델 안내서</span></nav>
                </div>
            </header>
            <article>
                <h1>RF-DETR 실시간 Transformer 객체 탐지 모델 안내서</h1>
<h2>1.  서론</h2>
<h3>1.1 RF-DETR의 등장: 실시간 객체 탐지 패러다임의 전환</h3>
<p>객체 탐지(Object Detection) 분야는 오랫동안 ’정확도(Accuracy)’와 ’추론 속도(Speed)’라는 두 가지 상충하는 목표 사이의 균형을 맞추는 과제를 안고 있었다. 특히, 실시간 애플리케이션에서는 높은 정확도를 유지하면서도 초당 수십 프레임을 처리할 수 있는 속도가 필수적이다. 이러한 요구사항 속에서, 컨볼루션 신경망(CNN)에 기반한 YOLO(You Only Look Once) 계열 모델들은 국소적 특징 추출(local feature extraction)에 최적화된 구조와 효율적인 파이프라인을 통해 실시간 객체 탐지 분야의 지배적인 패러다임으로 자리 잡았다.1</p>
<p>그러나 CNN 기반 모델들은 대규모 데이터셋을 통한 사전 훈련(pre-training)의 이점을 온전히 활용하기 어렵고, 이미지 전체의 전역적 맥락(global context)을 이해하는 데 구조적인 한계를 보였다. 이러한 배경 속에서 Roboflow가 개발한 RF-DETR(Roboflow Detection Transformer)은 Transformer 아키텍처가 실시간 성능과 최첨단(SOTA, State-of-the-Art) 정확도를 동시에 달성할 수 있음을 입증하며 객체 탐지 분야의 새로운 이정표를 제시했다.2 RF-DETR은 단일 기술의 혁신이 아닌, 검증된 여러 최신 기술들을 전략적으로 융합한 결과물이다. 이는 현대 딥러닝 연구에서 완전히 새로운 구조를 발명하는 것만큼이나 기존의 강력한 구성 요소들을 최적으로 조합하는 ’아키텍처 엔지니어링’의 중요성이 부각되고 있음을 보여준다.</p>
<h3>1.2 핵심 기여: 정확도, 속도, 그리고 도메인 적응성의 통합</h3>
<p>RF-DETR의 핵심적인 기여는 다음 세 가지로 요약할 수 있다.</p>
<ol>
<li><strong>최첨단(SOTA) 정확도 달성</strong>: RF-DETR은 실시간 모델로는 최초로 컴퓨터 비전 분야의 표준 벤치마크인 MS COCO 데이터셋에서 평균 정밀도(mean Average Precision, mAP) 60점을 돌파하는 성과를 거두었다.2 이는 Transformer 기반 모델이 더 이상 정확도를 위해 속도를 희생하지 않아도 됨을 증명한 중요한 성과이다.</li>
<li><strong>실시간 성능 확보</strong>: RF-DETR은 YOLO 계열 모델들과 비견될 만한 빠른 추론 속도를 자랑한다. 특히, DETR(DEtection TRansformer) 계열의 고유한 특성 덕분에 예측된 경계 상자(bounding box)들 중 중복을 제거하는 후처리 과정인 비최대 억제(Non-Maximum Suppression, NMS)가 필요 없다.3 이로 인해 전체 추론 파이프라인이 간소화되고, 실제 배포 환경에서 더욱 효율적으로 작동할 수 있다.</li>
<li><strong>뛰어난 도메인 적응성(Domain Adaptability)</strong>: RF-DETR의 가장 주목할 만한 특징 중 하나는 다양한 실제 환경에 대한 뛰어난 일반화 성능이다. Roboflow가 구축한 RF100-VL 벤치마크는 항공 이미지, 산업 검사, 자연 생태계 등 100개의 이질적인 데이터셋으로 구성되어 모델의 실세계 적응 능력을 측정한다.2 RF-DETR은 이 벤치마크에서 기존 모델들을 압도하는 성능을 보이며, 특정 도메인에 대한 전이 학습(transfer learning) 능력이 매우 뛰어남을 입증했다.4</li>
</ol>
<p>이러한 도메인 적응성의 강조는 COCO 데이터셋 중심의 벤치마크 평가 방식에 대한 중요한 문제 제기이기도 하다. COCO와 같은 대규모 일반 데이터셋에서의 높은 점수가 실제 산업 현장의 특수한 환경(예: 위성 이미지에서의 작은 객체 탐지, 농업 환경에서의 가려진 객체 탐지)에서의 성능을 보장하지는 않는다는 인식이 확산되고 있다. RF-DETR의 성공은 향후 모델 평가의 기준이 단일 벤치마크 점수를 넘어, 얼마나 다양한 도메인에 강건하게 일반화될 수 있는지를 측정하는 방향으로 나아가야 함을 시사하는 중요한 전환점이라 할 수 있다.</p>
<h3>1.3 보고서의 구조 및 개요</h3>
<p>본 보고서는 RF-DETR에 대한 심층적이고 체계적인 분석을 제공하는 것을 목표로 한다. 먼저 <strong>II장</strong>에서는 RF-DETR의 이론적 기반이 되는 DETR 패러다임의 진화 과정과 RF-DETR을 구성하는 핵심 기술 요소들(Deformable DETR, DINOv2, LW-DETR)을 상세히 분석하고, 이들이 어떻게 하나의 통합된 아키텍처로 결합되었는지 설명한다. <strong>III장</strong>에서는 MS COCO와 RF100-VL 벤치마크 데이터를 통해 RF-DETR의 성능을 다각도로 분석하고, 주요 경쟁 모델들과의 비교 우위를 정량적으로 제시한다. 또한, 특정 응용 분야에서의 연구 사례를 통해 모델의 실제적 강점을 탐구한다. <strong>IV장</strong>에서는 <span class="math math-inline">rfdetr</span> 패키지를 활용한 설치, 추론, 커스텀 데이터셋 학습 및 배포에 이르는 실용적인 가이드를 제공한다. 마지막으로 <strong>V장</strong>에서는 RF-DETR의 기술적 성과와 의의를 요약하고, 객체 탐지 분야의 향후 연구 방향에 대한 전망을 제시하며 보고서를 마무리한다.</p>
<h2>2.  RF-DETR의 이론적 배경 및 구조</h2>
<p>RF-DETR의 아키텍처를 이해하기 위해서는 그 근간을 이루는 DETR 패러다임의 발전 과정과 이를 구성하는 핵심 기술 요소들에 대한 깊이 있는 분석이 선행되어야 한다.</p>
<h3>2.1 DETR 패러다임의 진화와 한계</h3>
<h4>2.1.1 원본 DETR (DEtection TRansformer)의 개념과 기여</h4>
<p>2020년 Facebook AI Research(현 Meta AI)가 발표한 DETR은 객체 탐지 문제를 완전히 새로운 관점에서 접근했다.7 기존의 객체 탐지 모델들이 수많은 후보 영역(region proposal)이나 앵커 박스(anchor box)를 생성하고 이를 분류 및 회귀하는 복잡한 다단계 파이프라인을 가졌던 반면, DETR은 객체 탐지를 ‘직접적인 집합 예측(direct set prediction)’ 문제로 재정의했다.5</p>
<p>DETR은 CNN 백본을 통해 추출된 이미지 피처를 Transformer의 인코더-디코더 구조에 입력한다. 여기서 핵심적인 역할을 하는 것은 ’객체 쿼리(object queries)’라는 고정된 수의 학습 가능한 임베딩이다. 이 쿼리들은 디코더에서 이미지 피처와 상호작용하며 각각 하나의 객체(클래스 및 경계 상자)를 예측하도록 학습된다. 이 방식은 최종 예측값 집합과 실제 정답 객체 집합 간의 이분 매칭(bipartite matching)을 통해 손실을 계산함으로써, 복잡한 후처리 과정인 NMS를 완전히 제거한 최초의 완전한 End-to-End 객체 탐지 파이프라인을 구현했다.5</p>
<h4>2.1.2 DETR의 문제점</h4>
<p>혁신적인 설계에도 불구하고, 초기 DETR 모델은 몇 가지 심각한 한계를 가지고 있었다. 가장 큰 문제는 Transformer의 표준 어텐션 메커니즘이 이미지 피처 맵의 모든 픽셀 쌍 간의 관계를 계산하는 전역 어텐션(global attention)에 의존한다는 점이었다.10 이미지의 높이를  <span class="math math-inline">H</span>, 너비를 <span class="math math-inline">W</span>라 할 때, 어텐션의 연산 복잡도는 <span class="math math-inline">O(H^2W^2)</span>에 달했다. 이로 인해 다음과 같은 문제들이 발생했다.</p>
<ol>
<li><strong>느린 학습 수렴 속도</strong>: 전역 어텐션은 학습 초기 단계에서 이미지 전체에 걸쳐 의미 있는 관계를 학습하는 데 어려움을 겪어, 수렴에 수백 에포크(epoch)에 달하는 매우 긴 학습 시간을 필요로 했다.11</li>
<li><strong>고해상도 피처 처리의 어려움</strong>: 연산 복잡도가 이미지 크기에 제곱으로 비례하기 때문에, 작은 객체 탐지에 필수적인 고해상도 피처 맵을 처리하는 것이 계산적으로 거의 불가능했다. 이로 인해 DETR은 작은 객체에 대한 탐지 성능이 현저히 저하되는 약점을 보였다.10</li>
</ol>
<h3>2.2 RF-DETR의 핵심 구성 요소 분석</h3>
<p>RF-DETR은 원본 DETR의 한계를 극복하기 위해 제안된 여러 후속 연구들의 성과를 전략적으로 통합한 모델이다. 핵심 구성 요소는 Deformable DETR, DINOv2, 그리고 LW-DETR이다.</p>
<h4>2.2.1 Deformable DETR과 Deformable Attention 메커니즘</h4>
<p>Deformable DETR은 DETR의 높은 연산 복잡도와 느린 수렴 문제를 해결하기 위해 제안되었다.12 핵심 아이디어는 Deformable Convolution에서 영감을 받은  <strong>Deformable Attention</strong> 메커니즘이다.11</p>
<ul>
<li>
<p><strong>작동 원리</strong>: Deformable Attention은 이미지 피처 맵 전체에 대해 어텐션을 계산하는 대신, 각 쿼리(query)에 대한 참조점(reference point)을 중심으로 주변의 소수(예: 4개 또는 8개)의 핵심 샘플링 포인트(key sampling points)에만 어텐션을 집중한다.11 이때 샘플링 포인트의 위치(오프셋)는 쿼리 자체로부터 학습되어 데이터에 따라 동적으로 결정된다. 이는 마치 모델이 이미지의 어느 부분을 ’집중해서 봐야 할지’를 스스로 학습하는 것과 같다.</p>
</li>
<li>
<p><strong>효율성 개선</strong>: 이 희소 샘플링(sparse sampling) 방식은 어텐션의 연산 복잡도를 <span class="math math-inline">O(H^2W^2)</span>에서 <span class="math math-inline">O(HW)</span> 수준으로 획기적으로 감소시켰다. 이 덕분에 고해상도의 다중 스케일 피처 맵을 효율적으로 처리할 수 있게 되었고, 결과적으로 작은 객체 탐지 성능이 크게 향상되었으며 학습 수렴 속도는 기존 DETR 대비 10배 이상 빨라졌다.10</p>
</li>
<li>
<p><strong>Multi-scale Deformable Attention의 수학적 원리</strong>: Deformable Attention은 다중 스케일 피처 맵을 처리하도록 자연스럽게 확장될 수 있다. <span class="math math-inline">L</span>개의 피처 맵 레벨 <span class="math math-inline">{x_l}_{l=1}^L</span>이 주어졌을 때, 쿼리 피처 <span class="math math-inline">z_q</span>와 정규화된 2D 참조점 <span class="math math-inline">\hat{p}_q</span>에 대한 Multi-scale Deformable Attention은 다음과 같이 정의된다.13</p>
<p><span class="math math-display">
\text{MSDeformAttn}(z_q, \hat{p}_q, \{x_l\}_{l=1}^L) = \sum_{m=1}^{M} W_m \sum_{l=1}^{L} \sum_{k=1}^{K} A_{mlqk} \cdot W_m^{\prime} x_l(\phi_l(\hat{p}_q) + \Delta p_{mlqk})
</span><br />
여기서 각 항의 의미는 다음과 같다.</p>
</li>
<li>
<p><span class="math math-inline">m</span>: 어텐션 헤드(head)의 인덱스 (<span class="math math-inline">M</span>은 총 헤드 수).</p>
</li>
<li>
<p><span class="math math-inline">l</span>: 피처 레벨의 인덱스 (<span class="math math-inline">L</span>은 총 레벨 수).</p>
</li>
<li>
<p><span class="math math-inline">k</span>: 샘플링 포인트의 인덱스 (<span class="math math-inline">K</span>는 레벨당 샘플링 포인트 수).</p>
</li>
<li>
<p><span class="math math-inline">W_m, W_m^{\prime}</span>: 학습 가능한 가중치 행렬.</p>
</li>
<li>
<p><span class="math math-inline">A_{mlqk}</span>: <span class="math math-inline">m</span>번째 헤드, <span class="math math-inline">l</span>번째 레벨, <span class="math math-inline">k</span>번째 샘플링 포인트에 대한 어텐션 가중치. <span class="math math-inline">0</span>과 <span class="math math-inline">1</span> 사이의 값을 가지며, 총합은 <span class="math math-inline">1</span>이다.</p>
</li>
<li>
<p><span class="math math-inline">\phi_l(\hat{p}_q)</span>: 정규화된 좌표 <span class="math math-inline">\hat{p}_q</span>를 <span class="math math-inline">l</span>번째 피처 맵의 실제 좌표로 변환하는 함수.</p>
</li>
<li>
<p><span class="math math-inline">\Delta p_{mlqk}</span>: 쿼리 피처 <span class="math math-inline">z_q</span>로부터 예측된 샘플링 오프셋.</p>
</li>
<li>
<p><span class="math math-inline">x_l(\cdot)</span>: <span class="math math-inline">l</span>번째 피처 맵에서 주어진 좌표의 피처 값을 추출하는 함수 (좌표가 정수가 아닐 경우, 쌍선형 보간법(bilinear interpolation) 사용).</p>
</li>
</ul>
<h4>2.2.2 DINOv2: 강력한 사전 훈련 비전 백본</h4>
<p>RF-DETR의 뛰어난 도메인 적응성과 높은 정확도의 근간에는 <strong>DINOv2</strong>라는 강력한 비전 백본이 있다.3</p>
<ul>
<li><strong>역할과 특징</strong>: DINOv2는 대규모 이미지 데이터셋을 사용하여 자기 지도 학습(self-supervised learning) 방식으로 사전 훈련된 비전 트랜스포머(ViT)이다.7 레이블이 없는 데이터로부터 이미지의 본질적인 시각적 표현을 학습했기 때문에, 특정 도메인에 치우치지 않는 매우 풍부하고 일반화된 의미론적(semantic), 구조적 특징을 추출하는 능력이 탁월하다.9</li>
<li><strong>전이 학습 성능에 미치는 영향</strong>: 강력한 사전 훈련된 DINOv2 백본을 사용함으로써, RF-DETR은 비교적 적은 양의 레이블된 데이터만으로도 특정 응용 분야에 매우 빠르고 효과적으로 미세조정(fine-tuning)될 수 있다. 이는 RF100-VL 벤치마크에서 SOTA 성능을 달성한 핵심적인 이유이며, RF-DETR을 다양한 실세계 문제에 즉시 적용할 수 있게 만드는 원동력이다.3</li>
</ul>
<h4>2.2.3 LW-DETR: 경량화 구조의 기반</h4>
<p>RF-DETR의 ‘실시간’ 성능은 **LW-DETR(Light-Weight DETR)**의 효율적인 구조 설계를 계승한 덕분이다.3</p>
<ul>
<li><strong>아키텍처</strong>: LW-DETR은 실시간 탐지를 목표로 제안된 경량 DETR 모델로, ViT 인코더, 피처 맵 차원을 조정하는 컨볼루션 프로젝터(projector), 그리고 6개보다 적은 수의 레이어로 구성된 얕은(shallow) DETR 디코더라는 단순한 구조를 가진다.16</li>
<li><strong>실시간 성능 기여</strong>: LW-DETR의 경량화된 구조, 특히 얕은 디코더와 효율적인 어텐션 메커니즘(interleaved window and global attention)의 사용은 모델의 전체 파라미터 수와 연산량을 줄여 실시간 추론을 가능하게 한다.16 RF-DETR은 이러한 구조적 효율성을 채택하여 정확도와 속도 사이의 균형을 맞추었다.</li>
</ul>
<h3>2.3 RF-DETR 통합 아키텍처</h3>
<p>RF-DETR은 앞서 설명한 세 가지 핵심 요소들을 다음과 같이 전략적으로 결합한다.</p>
<ol>
<li>
<p><strong>백본 (Backbone)</strong>: DINOv2를 사용하여 입력 이미지로부터 강력하고 일반화된 피처 표현을 추출한다.</p>
</li>
<li>
<p><strong>인코더-디코더 (Encoder-Decoder)</strong>: LW-DETR의 경량화된 구조를 기반으로 하되, 디코더의 어텐션 메커니즘으로 Deformable Attention을 채택하여 빠른 수렴과 효율적인 연산을 달성한다.</p>
</li>
<li>
<p><strong>피처 추출 방식</strong>: 주목할 점은 RF-DETR의 설계적 선택이다. Deformable DETR 원본이나 FPN 기반 모델들이 작은 객체 탐지를 위해 다중 스케일 피처 맵을 사용하는 것이 표준적인 접근법이었으나 10, RF-DETR은 DINOv2 백본으로부터</p>
</li>
</ol>
<p><strong>단일 스케일</strong>의 고수준 피처 맵을 추출하여 이를 디코더에 전달한다.3</p>
<p>이러한 단일 스케일 접근법은 언뜻 보기에 성능 저하를 감수하는 것처럼 보일 수 있다. 그러나 이는 DINOv2 백본이 추출하는 단일 고해상도 피처 맵 내에 이미 다양한 스케일의 객체를 탐지하기에 충분한 정보가 풍부하게 인코딩되어 있다는 강력한 신뢰에 기반한 과감한 설계적 선택이다. 이 설계는 후속 인코더-디코더 구조를 단순화하여 속도를 향상시키는 동시에, DINOv2의 탁월한 피처 표현력 덕분에 성능 저하를 최소화한다. 이는 강력한 백본의 성능이 후속 넥(neck) 및 헤드(head) 구조의 복잡성을 줄일 수 있음을 보여주는 중요한 사례이다.</p>
<p>아래 표는 현재 공개된 RF-DETR 모델 제품군의 주요 사양을 요약한 것이다.</p>
<table><thead><tr><th>모델명 (Model)</th><th>파라미터 수 (Parameters)</th><th>COCO 사전 훈련 여부 (Pre-trained on COCO)</th></tr></thead><tbody>
<tr><td>RF-DETR-Nano</td><td>-</td><td>Yes</td></tr>
<tr><td>RF-DETR-Small</td><td>-</td><td>Yes</td></tr>
<tr><td>RF-DETR-Medium</td><td>-</td><td>Yes</td></tr>
<tr><td>RF-DETR-Base</td><td>29M</td><td>Yes</td></tr>
<tr><td>RF-DETR-Large</td><td>129M</td><td>Yes</td></tr>
</tbody></table>
<h3>2.4 손실 함수 (Loss Function)</h3>
<p>RF-DETR은 DETR 계열의 모델들과 마찬가지로 집합 기반 손실(set-based loss)을 사용하여 End-to-End 학습을 수행한다.</p>
<ul>
<li>
<p><strong>이분 매칭과 헝가리안 알고리즘</strong>: 모델이 <span class="math math-inline">N</span>개의 예측값 집합 <span class="math math-inline">\{\hat{y}_i\}_{i=1}^N</span>을 출력하고, 실제 정답 객체 집합 <span class="math math-inline">y = \{y_i\}_{i=1}^M</span> (<span class="math math-inline">M \le N</span>)이 주어졌을 때, 손실을 계산하기 위해 먼저 두 집합 간의 최적의 일대일 매칭을 찾아야 한다. 이 과정은 헝가리안 알고리즘(Hungarian Algorithm)을 사용하여 총 매칭 비용 <span class="math math-inline">\mathcal{L}_{\text{match}}</span>를 최소화하는 순열 <span class="math math-inline">\hat{\sigma}</span>를 찾는 방식으로 수행된다.19</p>
<p><span class="math math-display">
\hat{\sigma} = \arg\min_{\sigma \in \mathfrak{S}_N} \sum_{i=1}^{N} \mathcal{L}_{\text{match}}(y_i, \hat{y}_{\sigma(i)})
</span><br />
여기서 매칭 비용 <span class="math math-inline">\mathcal{L}_{\text{match}}</span>는 클래스 예측 확률과 경계 상자의 유사도를 결합하여 계산된다.</p>
</li>
<li>
<p><strong>손실 함수 구성</strong>: 최적의 매칭 <span class="math math-inline">\hat{\sigma}</span>가 결정되면, 최종 손실 함수는 매칭된 모든 쌍에 대한 손실의 합으로 계산된다. 이 손실은 분류 손실(classification loss)과 경계 상자 회귀 손실(bounding box loss)의 가중합으로 구성된다.</p>
<p><span class="math math-display">
\mathcal{L}_{\text{Hungarian}}(y, \hat{y}) = \sum_{i=1}^{N} \left[ -\log \hat{p}_{\hat{\sigma}(i)}(c_i) + \mathbb{I}_{\{c_i \neq \emptyset\}} \mathcal{L}_{\text{box}}(b_i, \hat{b}_{\hat{\sigma}(i)}) \right]
</span></p>
</li>
<li>
<p><span class="math math-inline">\hat{p}_{\hat{\sigma}(i)}(c_i)</span>: 매칭된 예측이 정답 클래스 <span class="math math-inline">c_i</span>일 확률. 분류 손실로는 주로 Focal Loss나 Cross-Entropy가 사용된다.</p>
</li>
<li>
<p><span class="math math-inline">\mathbb{I}_{\{c_i \neq \emptyset\}}</span>: 정답이 ’객체 없음(no object)’이 아닌 경우에만 경계 상자 손실을 계산하기 위한 표시 함수.</p>
</li>
<li>
<p><span class="math math-inline">\mathcal{L}_{\text{box}}</span>: 경계 상자 손실로, 일반적으로 L1 손실과 일반화된 IoU(Generalized Intersection over Union, GIoU) 손실의 선형 결합으로 구성된다.20</p>
<p><span class="math math-display">
  \mathcal{L}_{\text{box}}(b_i, \hat{b}_{\sigma(i)}) = \lambda_{\text{iou}}\mathcal{L}_{\text{iou}}(b_i, \hat{b}_{\sigma(i)}) + \lambda_{\text{L1}}\mathcal{L}_{\text{L1}}(b_i, \hat{b}_{\sigma(i)})
</span></p>
</li>
</ul>
<p>이러한 집합 기반 손실 함수는 모델이 중복된 예측을 스스로 억제하도록 유도하여 NMS 후처리 과정을 불필요하게 만든다.</p>
<h2>3.  성능 분석 및 벤치마크</h2>
<p>RF-DETR의 성능은 표준 벤치마크와 특정 응용 분야에서의 비교 연구를 통해 다각적으로 검증되었다. 본 장에서는 COCO 데이터셋과 RF100-VL 벤치마크에서의 성능을 분석하고, 실제 문제 해결 능력을 구체적인 사례를 통해 살펴본다.</p>
<h3>3.1 표준 벤치마크 성능: COCO 데이터셋</h3>
<p>MS COCO 데이터셋은 객체 탐지 모델의 일반적인 성능을 평가하는 데 가장 널리 사용되는 표준 벤치마크이다. RF-DETR은 이 벤치마크에서 기존 실시간 모델들의 성능을 뛰어넘는 결과를 보여주었다.</p>
<ul>
<li><strong>주요 실시간 모델과의 성능 비교</strong>: 아래 표는 RF-DETR과 주요 경쟁 모델들의 COCO 검증 데이터셋(val) 성능을 비교한 것이다. RF-DETR-Large 모델은 728x728 입력 해상도에서 60.5 mAP를 달성하여, Papers with Code에서 정의하는 실시간 모델(NVIDIA T4 GPU에서 25 FPS 이상) 중 최초로 60 mAP의 벽을 넘었다.3</li>
</ul>
<table><thead><tr><th>모델 (Model)</th><th>파라미터 (M)</th><th>입력 크기 (Input Size)</th><th>mAP (val)</th><th>Latency (ms, T4)</th><th>FPS (T4)</th></tr></thead><tbody>
<tr><td><strong>RF-DETR-Large</strong></td><td>129</td><td>728x728</td><td><strong>60.5</strong></td><td>16.5</td><td>60.6</td></tr>
<tr><td><strong>RF-DETR-Base</strong></td><td>29</td><td>640x640</td><td>53.3</td><td>8.7</td><td>114.9</td></tr>
<tr><td><strong>RF-DETR-Medium</strong></td><td>-</td><td>640x640</td><td>54.2</td><td>10.1</td><td>99.0</td></tr>
<tr><td><strong>RF-DETR-Small</strong></td><td>-</td><td>640x640</td><td>51.0</td><td>7.9</td><td>126.6</td></tr>
<tr><td><strong>RF-DETR-Nano</strong></td><td>-</td><td>640x640</td><td>48.1</td><td>5.9</td><td>169.5</td></tr>
<tr><td>YOLOv8-X</td><td>68.2</td><td>640x640</td><td>53.9</td><td>12.0</td><td>83.3</td></tr>
<tr><td>YOLOv11-L</td><td>52.2</td><td>640x640</td><td>52.6</td><td>-</td><td>-</td></tr>
<tr><td>RT-DETR-L</td><td>33</td><td>640x640</td><td>53.1</td><td>13.9</td><td>71.9</td></tr>
<tr><td>D-FINE-L</td><td>61.6</td><td>640x640</td><td>54.1</td><td>13.2</td><td>75.8</td></tr>
</tbody></table>
<ul>
<li><strong>파레토 최적(Pareto Optimal) 관점 분석</strong>: 정확도-속도 그래프에서 파레토 최적이란, 동일한 속도에서 더 높은 정확도를 보이거나 동일한 정확도에서 더 빠른 속도를 보이는 모델들의 집합을 의미한다. RF-DETR 계열 모델들은 YOLO 계열 모델들 대비 엄격하게 파레토 최적 경계선에 위치한다.3 예를 들어, RF-DETR-Nano는 YOLOv11-n보다 11 mAP(mAP@50:95 기준) 더 높으면서도 추론 속도는 0.17ms 더 빠르다.21 이는 사용자가 자신의 애플리케이션 요구사항(정확도 우선 vs. 속도 우선)에 맞춰 최적의 모델을 선택할 수 있는 유연성을 제공함을 의미한다.</li>
</ul>
<h3>3.2 도메인 적응성 평가: RF100-VL 벤치마크</h3>
<ul>
<li><strong>RF100-VL의 중요성</strong>: RF100-VL은 100개의 다양한 실제 데이터셋으로 구성되어, 모델이 학술적 벤치마크를 넘어 얼마나 다양한 실제 환경에 잘 적응하는지를 측정하는 데 특화되어 있다.2 이는 모델의 실용성을 평가하는 중요한 척도이다.</li>
<li><strong>SOTA 성능</strong>: RF-DETR은 이 벤치마크에서 기존의 모든 모델을 능가하는 SOTA 성능을 달성했다. 특히 DINOv2 백본의 강력한 일반화 성능 덕분에, 특정 도메인에 대한 미세조정 시 매우 높은 성능을 보인다. 예를 들어 드론, 위성, 레이더 등 항공 이미지 데이터셋에서 RF-DETR-Medium 모델은 YOLOv11-Medium 모델을 평균 5 mAP 차이로 크게 앞섰다.6 이는 RF-DETR이 단순한 COCO 벤치마크용 모델이 아니라, 실제 산업 현장의 다양한 문제에 효과적으로 적용될 수 있는 범용 솔루션임을 시사한다.</li>
</ul>
<h3>3.3 특정 응용 분야에서의 비교 연구</h3>
<p>단순한 벤치마크 점수를 넘어, 특정 문제 상황에서 모델이 어떻게 작동하는지를 살펴보는 것은 모델의 질적인 특성을 이해하는 데 중요하다.</p>
<h4>3.3.1 농업 분야 (Greenfruit 탐지)</h4>
<p>arXiv:2504.13099 논문은 가려짐(occlusion), 배경과의 융합, 레이블 모호성 등 시각적으로 매우 복잡한 과수원 환경에서 녹색 과일(greenfruit)을 탐지하는 태스크를 통해 RF-DETR과 YOLOv12의 성능을 심층 비교했다.14</p>
<table><thead><tr><th>모델 (Model)</th><th>탐지 유형 (Detection Type)</th><th>mAP@50</th><th>mAP@50:95</th></tr></thead><tbody>
<tr><td><strong>RF-DETR Base</strong></td><td>단일 클래스 (Single-class)</td><td><strong>0.9464</strong></td><td>0.7510</td></tr>
<tr><td>YOLOv12N</td><td>단일 클래스 (Single-class)</td><td>0.9320</td><td><strong>0.7620</strong></td></tr>
<tr><td><strong>RF-DETR Base</strong></td><td>다중 클래스 (Multi-class)</td><td><strong>0.8298</strong></td><td>0.6531</td></tr>
<tr><td>YOLOv12L</td><td>다중 클래스 (Multi-class)</td><td>0.8170</td><td><strong>0.6622</strong></td></tr>
</tbody></table>
<p>이 연구 결과는 mAP 지표 이면에 숨겨진 모델의 특성을 잘 보여준다.</p>
<ul>
<li><strong>복잡한 상황에서의 강점</strong>: RF-DETR은 mAP@50(IoU 임계값 0.5, 객체의 존재 유무와 대략적인 위치를 찾는 능력)에서 일관되게 우수한 성능을 보였다. 특히 가려진 객체를 포함하는 다중 클래스 탐지에서 높은 성능을 기록했는데, 이는 Transformer 아키텍처가 이미지의 전역적인 맥락과 객체 간의 관계를 모델링하여 부분적인 정보만으로도 객체를 효과적으로 추론할 수 있음을 시사한다. CNN의 로컬 수용장(receptive field)이 어려움을 겪는 복잡한 공간적, 맥락적 추론이 필요한 상황에서 RF-DETR의 강점이 드러난다.</li>
<li><strong>정밀도 vs. 위치 특정 능력</strong>: 반면, YOLOv12는 mAP@50:95(더 엄격한 IoU 기준, 정확한 경계 상자를 예측하는 능력)에서 근소하게 앞서는 경향을 보였다. 이는 RF-DETR이 복잡한 상황에서 객체를 ’찾아내는 능력’이 뛰어난 반면, YOLO는 특정 조건에서 객체를 ’정밀하게 분류하고 위치를 잡는 능력’이 좋을 수 있음을 의미한다. 따라서 모델 선택 시, 해결하고자 하는 문제의 특성(예: 객체들이 자주 가려지는가?)을 고려하는 것이 중요하다.</li>
</ul>
<h4>3.3.2 산업 현장 (컨테이너 손상 탐지)</h4>
<p>또 다른 연구에서는 손상된 운송 컨테이너를 탐지하는 태스크에서 RF-DETR, YOLOv11, YOLOv12를 비교했다.7 전체 mAP@50 점수는 YOLO 모델들이 81.9%로 RF-DETR의 77.7%보다 다소 높게 나타났다. 그러나, 데이터셋에 드물게 나타나는 **‘흔하지 않은 유형의 손상’**을 탐지하는 테스트에서는 RF-DETR이 다른 모델들을 압도하며 더 높은 신뢰도로 정확하게 손상을 탐지했다. 이는 RF-DETR이 학습 데이터에 자주 나타나지 않는 예외적인 사례에 대해서도 더 나은 일반화 성능을 가질 수 있음을 보여준다.</p>
<h4>3.3.3 학습 역학 및 효율성</h4>
<p>Greenfruit 탐지 연구에서 RF-DETR은 단일 클래스 설정에서 <strong>10 에포크 이내에 성능이 수렴</strong>하는 매우 빠른 학습 속도를 보였다.1 이는 DETR 계열의 고질적인 문제였던 긴 학습 시간을 성공적으로 해결했음을 의미한다.12 이러한 빠른 수렴 속도는 단순히 개발 시간을 단축하는 것을 넘어, 대규모 GPU 클러스터에 접근하기 어려운 중소기업이나 연구실에서도 최첨단 Transformer 모델을 효율적으로 미세조정하여 활용할 수 있게 만든다. 즉, ’학습 효율성’이 모델의 ’접근성’과 ’민주화’에 직접적인 영향을 미치는 중요한 요소가 됨을 보여준다.</p>
<h2>4.  RF-DETR의 실제적 활용</h2>
<p>RF-DETR은 오픈소스 라이선스(Apache 2.0)와 잘 구축된 Python 패키지를 통해 사용자들이 쉽게 접근하고 활용할 수 있도록 지원된다. 본 장에서는 설치부터 추론, 학습, 배포에 이르는 전 과정을 안내한다.</p>
<h3>4.1 설치 및 환경 설정</h3>
<p>RF-DETR을 사용하기 위한 첫 단계는 <span class="math math-inline">rfdetr</span> Python 패키지를 설치하는 것이다. Python 3.9 이상의 환경이 필요하다.6</p>
<ul>
<li>pip을 통한 설치 (권장):</li>
</ul>
<p>가장 간단한 설치 방법은 pip을 사용하는 것이다.</p>
<pre><code class="language-Bash">pip install rfdetr
</code></pre>
<ul>
<li>소스 코드로부터 직접 설치:</li>
</ul>
<p>아직 공식 릴리스에 포함되지 않은 최신 기능이나 개선 사항을 사용하고 싶다면, GitHub 저장소에서 직접 소스 코드를 클론하여 설치할 수 있다.6</p>
<pre><code class="language-Bash">pip install git+https://github.com/roboflow/rf-detr.git
</code></pre>
<h3>4.2 추론 실행 (Running Inference)</h3>
<p>사전 훈련된 RF-DETR 모델을 사용하여 이미지나 영상에서 객체를 탐지하는 추론 과정은 매우 간단하다.</p>
<ul>
<li>.predict() 메소드 사용:</li>
</ul>
<p>rfdetr 패키지는 다양한 입력을 처리할 수 있는 .predict() 메소드를 제공한다. 입력 형식으로는 파일 경로, PIL(Pillow) 이미지 객체, NumPy 배열, 그리고 PyTorch 텐서 등이 지원된다. 입력 이미지는 RGB 채널 순서여야 한다.6</p>
<p>다음은 <span class="math math-inline">RFDETRBase</span> 모델을 로드하여 웹 상의 이미지에 대해 추론을 실행하는 예제 코드이다.</p>
<pre><code class="language-Python">import io
import requests
import supervision as sv
from PIL import Image
from rfdetr import RFDETRBase

# 모델 로드 (COCO 사전 훈련 가중치 자동 다운로드)
model = RFDETRBase()

# 이미지 다운로드 및 열기
url = "https://media.roboflow.com/notebooks/examples/dog-2.jpeg"
image = Image.open(io.BytesIO(requests.get(url).content))

# 추론 실행
detections = model.predict(image, threshold=0.5)

# 결과 시각화 (supervision 라이브러리 사용)
box_annotator = sv.BoxAnnotator()
label_annotator = sv.LabelAnnotator()
annotated_image = box_annotator.annotate(scene=image.copy(), detections=detections)
annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)

sv.plot_image(annotated_image)
</code></pre>
<ul>
<li>추론 속도 최적화:</li>
</ul>
<p>만약 추론 시 이미지 크기나 배치 크기가 동적으로 변하지 않는 고정된 환경이라면, .optimize_for_inference() 메소드를 사용하여 추론 속도를 최대 2배까지 향상시킬 수 있다. 이 메소드는 모델을 현재 플랫폼에 맞게 최적화한다.6</p>
<pre><code class="language-Python">model.optimize_for_inference()
# 이후 model.predict() 호출 시 더 빠른 속도로 실행됨
</code></pre>
<h3>4.3 커스텀 데이터셋을 이용한 모델 학습 및 미세조정</h3>
<p>RF-DETR의 가장 큰 장점 중 하나는 커스텀 데이터셋에 대한 미세조정이 용이하다는 점이다.</p>
<ul>
<li>데이터셋 준비:</li>
</ul>
<p>RF-DETR 학습 파이프라인은 COCO JSON 주석 형식을 사용한다. 데이터셋은 다음과 같은 디렉토리 구조를 가져야 한다.21</p>
<pre><code>dataset/
├── train/
│   ├── _annotations.coco.json
│   ├── image1.jpg
│   └──...
├── valid/
│   ├── _annotations.coco.json
│   ├── image3.jpg
│   └──...
└── test/
    ├── _annotations.coco.json
    ├── image5.jpg
    └──...
</code></pre>
<p>Roboflow 플랫폼을 사용하면 다양한 형식(예: YOLO TXT)의 데이터셋을 COCO JSON 형식으로 쉽게 변환하고 위와 같은 구조로 내보낼 수 있다.21</p>
<ul>
<li>학습 스크립트 예제:</li>
</ul>
<p>rfdetr 패키지의 train() 메소드를 사용하여 간단하게 모델 학습을 시작할 수 있다. 다음은 RFDETRMedium 모델을 커스텀 데이터셋으로 10 에포크 동안 학습하는 예제이다.21</p>
<pre><code class="language-Python">from rfdetr import RFDETRMedium

# 학습할 모델 인스턴스 생성
model = RFDETRMedium()

# 모델 학습 시작
# dataset_dir은 위에서 설명한 구조를 가진 데이터셋의 루트 경로
model.train(
    dataset_dir='path/to/your/dataset',
    epochs=10,
    batch_size=8,
    grad_accum_steps=2
)
</code></pre>
<ul>
<li>학습 파라미터 설정:</li>
</ul>
<p>GPU의 VRAM(비디오 메모리) 크기는 한 번에 처리할 수 있는 데이터의 양, 즉 배치 크기를 제한한다. rfdetr은 이를 유연하게 처리하기 위해 batch_size와 grad_accum_steps(그래디언트 누적 단계) 두 가지 파라미터를 제공한다. 권장되는 총 배치 크기는 16이므로, 두 파라미터의 곱이 16이 되도록 설정하는 것이 좋다. 예를 들어, VRAM이 큰 A100 GPU에서는 batch_size=16, grad_accum_steps=1로 설정하고, VRAM이 작은 T4 GPU에서는 batch_size=4, grad_accum_steps=4로 설정하여 동일한 학습 효과를 얻을 수 있다.21</p>
<ul>
<li>고급 학습 기능:</li>
</ul>
<p>rfdetr은 효율적인 학습을 위해 조기 종료(early stopping), 그래디언트 체크포인팅(gradient checkpointing), 학습 재개(training resume), 그리고 TensorBoard 및 Weights &amp; Biases(W&amp;B)를 통한 로깅 등 다양한 유틸리티를 지원한다.6</p>
<h3>4.4 배포 전략</h3>
<p>학습이 완료된 RF-DETR 모델은 다양한 환경에 배포할 수 있다.</p>
<ul>
<li><strong>Roboflow 플랫폼</strong>: Roboflow Train을 통해 학습된 모델은 클릭 몇 번으로 Roboflow Inference 서버나 Roboflow Workflows를 통해 클라우드 또는 엣지 디바이스에 쉽게 배포할 수 있다.3</li>
<li><strong>ONNX 변환</strong>: PyTorch 모델을 표준화된 ONNX(Open Neural Network Exchange) 형식으로 변환하면, 특정 프레임워크에 대한 의존성을 줄이고 다양한 하드웨어 및 플랫폼(예: C++, Java 환경)에서 고성능 추론을 실행할 수 있다. 관련 커뮤니티 저장소에서 ONNX 변환 및 추론 스크립트를 찾을 수 있다.24</li>
<li><strong>서버리스 배포</strong>: Replicate와 같은 MLaaS(Machine Learning as a Service) 플랫폼을 사용하면, 모델 가중치와 간단한 설정 파일만으로 RF-DETR을 확장 가능한 서버리스 API 엔드포인트로 배포할 수 있어 인프라 관리에 대한 부담을 줄일 수 있다.25</li>
</ul>
<h2>5.  결론 및 전망</h2>
<h3>5.1 RF-DETR의 기술적 성과 요약</h3>
<p>RF-DETR은 현대 컴퓨터 비전 연구의 중요한 흐름을 반영하는 모델이다. 이는 완전히 새로운 아키텍처의 발명이 아닌, DINOv2의 강력한 표현력, LW-DETR의 구조적 효율성, 그리고 Deformable DETR의 빠른 수렴 및 연산 효율성이라는, 각기 다른 영역에서 최첨단을 달리던 기술들을 전략적으로 융합하여 시너지를 극대화한 결과물이다.</p>
<p>이러한 통합을 통해 RF-DETR은 실시간 객체 탐지 분야에서 세 가지 중요한 성과를 동시에 달성했다. 첫째, 실시간 모델 최초로 COCO 벤치마크에서 60 mAP를 돌파하며 정확도의 새로운 기준을 제시했다. 둘째, NMS가 필요 없는 End-to-End 파이프라인과 효율적인 아키텍처를 통해 YOLO 계열과 경쟁할 수 있는 빠른 추론 속도를 확보했다. 셋째, RF100-VL 벤치마크에서의 압도적인 성능을 통해 학술적 데이터셋을 넘어 다양한 실세계 도메인에 대한 뛰어난 적응성을 입증했다. 특히 가려짐이나 배경 융합과 같은 복잡하고 어려운 사례에서 보여준 강건함은 Transformer 아키텍처의 근본적인 잠재력을 명확히 보여준다.</p>
<h3>5.2 객체 탐지 분야에서의 RF-DETR의 위상과 의의</h3>
<p>RF-DETR의 등장은 객체 탐지 분야에 두 가지 중요한 의의를 남긴다.</p>
<p>첫째, Transformer 기반 모델이 더 이상 학술적 연구나 대규모 컴퓨팅 환경의 전유물이 아니며, CNN 기반 모델이 지배해 온 실시간 탐지 분야에서도 강력하고 실용적인 대안이 될 수 있음을 명확히 입증했다. 이는 향후 실시간 애플리케이션 개발에 있어 아키텍처 선택의 폭을 넓히고, Transformer 기반 모델의 채택을 가속화하는 계기가 될 것이다.</p>
<p>둘째, RF-DETR과 RF100-VL 벤치마크의 성공은 모델 평가의 패러다임 전환을 촉구한다. 단일 표준 벤치마크의 점수만으로는 모델의 진정한 가치를 평가하기 어렵다는 인식이 확산되는 가운데, ’실세계 적응성’이라는 새로운 평가 기준의 중요성을 부각시켰다. 이는 향후 모델 개발이 특정 벤치마크에 대한 과적합을 넘어, 다양한 환경에 강건하게 일반화될 수 있는 능력을 키우는 방향으로 나아가야 함을 시사한다.</p>
<h3>5.3 향후 발전 가능성</h3>
<p>RF-DETR은 현재의 성과에 머무르지 않고 지속적으로 발전할 잠재력을 가지고 있다.</p>
<ul>
<li><strong>모델 확장</strong>: Roboflow는 현재 성능의 한계를 더욱 확장하기 위해 동일한 기술을 적용한 Large 및 X-Large 모델을 적극적으로 개발하고 있다.6 이러한 더 큰 모델들은 정확도가 최우선시되는 고성능 애플리케이션에서 새로운 가능성을 열어줄 것이다.</li>
<li><strong>기능 추가</strong>: 현재 RF-DETR은 객체 탐지에 초점을 맞추고 있지만, 아키텍처의 유연성을 고려할 때 향후 인스턴스 분할(Instance Segmentation)이나 자세 추정(Pose Estimation)과 같은 더 복잡한 비전 태스크로 기능이 확장될 가능성이 있다.4</li>
<li><strong>오픈소스 생태계의 힘</strong>: RF-DETR의 코드와 사전 훈련된 가중치는 허용적인 Apache 2.0 라이선스 하에 완전히 공개되어 있다.2 이는 전 세계의 연구자들과 개발자들이 자유롭게 모델을 사용하고, 개선하며, 새로운 아이디어를 기여할 수 있는 활발한 오픈소스 생태계를 조성한다. 커뮤니티의 집단 지성을 통해 RF-DETR은 앞으로도 지속적으로 발전하며 객체 탐지 기술의 경계를 넓혀나갈 것으로 기대된다.</li>
</ul>
<h2>6. 참고 자료</h2>
<ol>
<li>(PDF) RF-DETR Object Detection vs YOLOv12 : A Study of Transformer-based and CNN-based Architectures for Single-Class and Multi-Class Greenfruit Detection in Complex Orchard Environments Under Label Ambiguity - ResearchGate, https://www.researchgate.net/publication/390893074_RF-DETR_Object_Detection_vs_YOLOv12_A_Study_of_Transformer-based_and_CNN-based_Architectures_for_Single-Class_and_Multi-Class_Greenfruit_Detection_in_Complex_Orchard_Environments_Under_Label_Ambiguity</li>
<li>RF-DETR Object Detection Model: What is, How to Use - Roboflow, https://roboflow.com/model/rf-detr</li>
<li>RF-DETR: A SOTA Real-Time Object Detection Model - Roboflow Blog, https://blog.roboflow.com/rf-detr/</li>
<li>Roboflow’s RF-DETR: Bridging Speed and Accuracy in Object Detection - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2025/03/roboflows-rf-detr/</li>
<li>RF-DETR: Real-Time Object Detection with Transformers | DigitalOcean, https://www.digitalocean.com/community/tutorials/rf-detr-real-time-object-detection</li>
<li>RF-DETR is a real-time object detection model architecture developed by Roboflow, SOTA on COCO and designed for fine-tuning. - GitHub, https://github.com/roboflow/rf-detr</li>
<li>Container damage detection using advanced computer vision model – Yolov12 vs Yolov11 vs RF-DETR – A comparative analysis - arXiv, https://arxiv.org/pdf/2506.22517</li>
<li>HSF-DETR: Hyper Scale Fusion Detection Transformer for Multi-Perspective UAV Object Detection - MDPI, https://www.mdpi.com/2072-4292/17/12/1997</li>
<li>RF-DETR Object Detection vs YOLOv12 : A Study of … - arXiv, <a href="https://arxiv.org/pdf/2504.13099">https://arxiv.org/pdf/2504.13099?</a></li>
<li>Paper Review: Deformable Transformers for End-to-End Object Detection., https://cenk-bircanoglu.medium.com/paper-review-deformable-transformers-for-end-to-end-object-detection-ed0a452f775f</li>
<li>Deformable DETR: Deformable Transformers for End-to-End Object Detection. - GitHub, https://github.com/fundamentalvision/Deformable-DETR</li>
<li>Deformable DETR: Deformable Transformers for End-to-End Object …, https://arxiv.org/pdf/2010.04159</li>
<li>[20.10] Deformable DETR - DOCSAID, https://docsaid.org/en/papers/object-detection/deformable-detr/</li>
<li>RF-DETR Object Detection vs YOLOv12 : A Study of Transformer-based and CNN-based Architectures for Single-Class and Multi-Class Greenfruit Detection in Complex Orchard Environments Under Label Ambiguity - Hugging Face, https://huggingface.co/papers/2504.13099</li>
<li>RF-DETR Object Detection vs YOLOv12 : A Study of Transformer-based and CNN-based Architectures for Single-Class and Multi-Class Greenfruit Detection in Complex Orchard Environments Under Label Ambiguity - Consensus, https://consensus.app/papers/details/9d9a980608d35c75aee2389593041014/</li>
<li>[Literature Review] LW-DETR: A Transformer Replacement to YOLO for Real-Time Detection, https://www.themoonlight.io/en/review/lw-detr-a-transformer-replacement-to-yolo-for-real-time-detection</li>
<li>LW-DETR: A Transformer Replacement to YOLO for Real-Time Detection - arXiv, https://arxiv.org/html/2406.03459v1</li>
<li>LW-DETR: A Transformer Replacement to YOLO for Real-Time Detection - ResearchGate, https://www.researchgate.net/publication/381190259_LW-DETR_A_Transformer_Replacement_to_YOLO_for_Real-Time_Detection</li>
<li>DETR - Hugging Face, https://huggingface.co/docs/transformers/model_doc/detr</li>
<li>Metal Character Detection Based on Improved Deformable Detr - CEUR-WS.org, https://ceur-ws.org/Vol-3344/paper20.pdf</li>
<li>How to Train RF-DETR Object Detection on a Custom Dataset - Colab - Google, https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-rf-detr-on-detection-dataset.ipynb</li>
<li>[2504.13099] RF-DETR Object Detection vs YOLOv12 : A Study of Transformer-based and CNN-based Architectures for Single-Class and Multi-Class Greenfruit Detection in Complex Orchard Environments Under Label Ambiguity - arXiv, https://arxiv.org/abs/2504.13099</li>
<li>rfdetr · PyPI, https://pypi.org/project/rfdetr/</li>
<li>PierreMarieCurie/rf-detr-onnx: ONNX model with inference and visualization scripts for RF-DETR - GitHub, https://github.com/PierreMarieCurie/rf-detr-onnx</li>
<li>hardikdava/rf-detr-cog - GitHub, https://github.com/hardikdava/rf-detr-cog</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>