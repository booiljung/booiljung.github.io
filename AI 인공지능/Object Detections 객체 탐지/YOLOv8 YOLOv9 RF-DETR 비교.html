<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:YOLOv8, YOLOv9, RF-DETR 비교</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>YOLOv8, YOLOv9, RF-DETR 비교</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>YOLOv8, YOLOv9, RF-DETR 비교</span></nav>
                </div>
            </header>
            <article>
                <h1>YOLOv8, YOLOv9, RF-DETR 비교</h1>
<h2>1. 서론: 실시간 객체 탐지 모델의 진화와 새로운 패러다임</h2>
<h3>1.1  문제 제기: 정확도, 속도, 그리고 효율성의 삼각관계</h3>
<p>실시간 객체 탐지(Real-time Object Detection) 기술은 자율주행, 지능형 영상 감시, 공장 자동화, 증강 현실 등 현대 산업의 핵심적인 시각 인지 기능을 담당하는 중추 기술로 자리 잡았다.1 이 기술의 핵심 과제는 주어진 컴퓨팅 자원 제약 하에서 ‘정확도(Accuracy)’, ‘속도(Speed)’, 그리고 ’효율성(Efficiency)’이라는 세 가지 상충하는 목표 사이에서 최적의 균형점을 찾는 것이다. 높은 정확도를 위해서는 복잡하고 큰 모델이 필요하지만, 이는 추론 속도를 저하시키고 더 많은 연산 자원을 소모하게 만든다. 반대로, 빠른 속도를 위해 모델을 경량화하면 정확도가 희생될 수 있다. 이 근본적인 삼각관계(Trilemma)를 해결하기 위해 지난 몇 년간 객체 탐지 분야는 두 가지 주요한 기술적 흐름을 중심으로 발전해왔다.</p>
<p>첫 번째 흐름은 합성곱 신경망(Convolutional Neural Network, CNN)에 기반한 1-단계(One-Stage) 탐지기이다. 이 패러다임의 선두주자인 YOLO(You Only Look Once) 계열은 입력 이미지를 한 번만 처리하여 객체의 위치와 클래스를 동시에 예측하는 방식으로, 속도와 정확도 간의 탁월한 균형을 제시하며 실시간 탐지의 표준으로 자리매김했다.3 YOLO는 지속적인 아키텍처 개선과 학습 전략 고도화를 통해 발전을 거듭해왔다.</p>
<p>두 번째 흐름은 Transformer 아키텍처에 기반한 End-to-End 탐지기이다. 2020년 등장한 DETR(DEtection TRansformer)은 객체 탐지 문제를 ‘집합 예측(Set Prediction)’ 문제로 재정의하고, 복잡한 후처리 과정인 NMS(Non-Maximum Suppression)를 제거한 혁신적인 파이프라인을 제안했다.2 초기 DETR은 높은 연산량과 느린 학습 수렴 속도라는 한계를 가졌으나, Deformable DETR, RT-DETR 등의 후속 연구를 통해 실시간성을 확보하며 CNN 기반 모델의 강력한 대안으로 부상했다.</p>
<p>흥미로운 점은 이 두 패러다임이 독립적으로 발전하는 것을 넘어, 서로의 장점을 흡수하며 수렴하는 경향을 보인다는 것이다. DETR 계열은 실시간 성능을 확보하기 위해 CNN 백본과 하이브리드 인코더를 적극적으로 도입하고 있으며 5, YOLO 계열은 후처리 파이프라인의 복잡성을 줄이기 위해 점차 NMS-free 설계를 지향하고 있다.8 이는 미래의 객체 탐지기가 CNN의 효율적인 지역적 특징 추출 능력과 Transformer의 전역적 컨텍스트 이해 능력을 결합한 하이브리드 형태로 진화할 것임을 강력히 시사한다.</p>
<h3>1.2  분석 대상 모델 소개: YOLOv8, YOLOv9, RF-DETR</h3>
<p>본 보고서는 이러한 기술적 진화의 최전선에 있는 세 가지 대표적인 모델, YOLOv8, YOLOv9, 그리고 RF-DETR을 심층적으로 비교 분석한다. 이 세 모델은 각각 객체 탐지 기술의 현주소와 미래 방향성을 상징하는 중요한 이정표이다.</p>
<ul>
<li><strong>YOLOv8:</strong> 2023년 1월 Ultralytics가 공개한 모델로, 이전 버전인 YOLOv5의 성공을 계승하여 아키텍처, 성능, 그리고 개발자 경험을 전반적으로 개선했다.9 C2f 모듈, Anchor-free 감지 헤드 등 검증된 최신 기술들을 성공적으로 융합하여, 현재 산업계에서 가장 널리 사용되는 고성능 실시간 탐지기 중 하나로 평가받는다. YOLOv8은 ’최적화’와 ’안정성’을 대표한다.</li>
<li><strong>YOLOv9:</strong> 2024년 2월 Chien-Yao Wang 등에 의해 발표된 모델로, 심층 신경망의 근본적인 문제인 ‘정보 병목(Information Bottleneck)’ 현상을 해결하기 위한 이론적 혁신을 담고 있다.12 프로그래머블 그래디언트 정보(PGI)와 일반화된 효율적 계층 집계 네트워크(GELAN)라는 독창적인 개념을 통해, 동일한 연산 비용으로 더 높은 정확도를 달성하며 효율성의 한계를 한 단계 끌어올렸다. YOLOv9은 ’이론적 혁신’과 ’효율성’을 상징한다.</li>
<li><strong>RF-DETR:</strong> 2025년 3월 Roboflow가 공개한 Transformer 기반 실시간 탐지기로, 실시간 모델 최초로 MS COCO 데이터셋에서 60 mAP를 돌파하는 기염을 토했다.15 Deformable DETR과 RT-DETR의 아이디어를 계승하고 NMS-free 파이프라인을 구현하여, 정확도와 속도, 그리고 파이프라인의 단순성까지 모두 갖춘 End-to-End 모델이다. RF-DETR은 ’차세대 성능’과 ’단순성’을 대표한다.</li>
</ul>
<p>본 보고서는 이 세 가지 모델의 아키텍처, 핵심 메커니즘, 손실 함수를 상세히 해부하고, 파라미터 수, 연산량(GFLOPs), 정확도(mAP) 등 정량적 지표를 통해 성능을 다각적으로 비교 분석한다. 이를 통해 각 모델의 장단점과 기술적 트레이드오프를 명확히 규명하고, 최종적으로 사용자의 구체적인 요구사항과 응용 분야에 가장 적합한 모델을 선택할 수 있는 전문적인 가이드라인을 제공하고자 한다.</p>
<h2>2. YOLOv8 아키텍처 심층 분석: 검증된 성능의 최적화</h2>
<p>YOLOv8은 특정 기술의 혁신적인 발명보다는, 지난 몇 년간 객체 탐지 커뮤니티에서 그 효과가 입증된 다양한 최신 기술들을 성공적으로 통합하고 최적화한 실용적인 모델이다. 이는 학술적 독창성보다 산업 현장에서의 안정성과 즉각적인 성능 향상을 우선시하는 개발 철학을 반영한다. YOLOv8의 성공은 개별 기술의 새로움이 아닌, 검증된 요소들을 효율적으로 융합하여 정확도와 속도 간의 새로운 최적점을 찾아낸 ’엔지니어링의 승리’로 해석할 수 있다.</p>
<h3>2.1  아키텍처 개관: Backbone-Neck-Head 구조</h3>
<p>YOLOv8은 전통적인 1-단계 탐지기의 표준 구조인 Backbone-Neck-Head 프레임워크를 따른다. 각 구성 요소는 다음과 같은 역할을 수행한다.</p>
<ul>
<li><strong>Backbone:</strong> 입력 이미지로부터 다양한 스케일의 특징 맵(feature map)을 추출하는 역할을 한다. YOLOv5의 CSPDarknet 구조를 계승 및 발전시켜, C2f 모듈을 핵심 구성 요소로 사용한다. 이를 통해 이미지의 저수준 특징(모서리, 질감)부터 고수준 특징(객체의 일부)까지 계층적으로 학습한다.</li>
<li><strong>Neck:</strong> Backbone에서 추출된 여러 스케일의 특징 맵을 융합하여 의미론적 정보와 공간적 정보를 모두 풍부하게 만드는 역할을 한다. YOLOv8은 이전 버전들과 마찬가지로 PANet(Path Aggregation Network) 구조를 채택하여 상향식(bottom-up) 경로와 하향식(top-down) 경로를 통해 특징 정보를 효과적으로 집계한다.16</li>
<li><strong>Head:</strong> Neck에서 전달받은 최종 특징 맵을 사용하여 객체의 클래스(classification)와 위치(bounding box regression)를 예측한다. YOLOv8은 Anchor-free 방식을 채택하고, 분류와 회귀 작업을 분리하는 Decoupled Head 구조를 사용하여 예측 정확도를 높였다.9</li>
</ul>
<h3>2.2  핵심 구성 요소 분석</h3>
<h4>2.2.1  C2f (Cross Stage Partial bottleneck with 2 convolutions) 모듈</h4>
<p>C2f 모듈은 YOLOv8 아키텍처의 가장 중요한 개선 사항 중 하나로, YOLOv5에서 사용되던 C3 모듈을 대체한다.9 C2f는 CSP(Cross Stage Partial) 개념을 기반으로 설계되었으며, 더 효율적인 정보 흐름과 풍부한 그래디언트 전파를 목표로 한다.</p>
<p>C3 모듈은 내부의 Bottleneck 블록들 중 마지막 블록의 출력만을 사용하여 다음 단계로 정보를 전달했다. 반면, C2f 모듈은 내부의 모든 Bottleneck 블록들의 출력을 채널 차원(channel dimension)에서 모두 연결(concatenate)한 후 다음 레이어로 전달한다.9 이러한 설계는 각 Bottleneck 블록에서 추출된 다양한 수준의 특징 정보를 손실 없이 활용할 수 있게 하여, 모델의 특징 표현 능력을 극대화한다. 더 많은 교차 계층 연결(cross-layer connections)은 학습 과정에서 그래디언트가 네트워크 깊숙이 원활하게 전파되도록 도와 모델의 수렴을 돕고 전반적인 성능을 향상시키는 효과를 가져온다.16</p>
<h4>2.2.2  Anchor-Free Detection</h4>
<p>YOLOv8은 이전 버전들과 달리 사전 정의된 앵커 박스(Anchor Box)를 사용하지 않는 Anchor-free 방식을 채택했다.9 앵커 기반 방식은 다양한 크기와 종횡비의 앵커 박스를 미리 정의하고, 각 앵커 박스로부터의 오프셋(offset)을 예측하는 방식이었다. 이 방식은 모델의 설계와 학습 과정을 복잡하게 만드는 단점이 있었다.</p>
<p>Anchor-free 방식은 객체의 중심점을 직접 예측하고, 중심점으로부터 각 경계까지의 거리를 회귀하는 방식으로 작동한다. 이 접근법은 다음과 같은 장점을 가진다.</p>
<ol>
<li><strong>후처리 속도 향상:</strong> 예측해야 할 후보 박스의 수가 앵커 기반 방식에 비해 크게 줄어들어, 후처리 단계인 NMS(Non-Maximum Suppression)의 연산 부담을 줄이고 전체 추론 속도를 높인다.9</li>
<li><strong>유연성 증대:</strong> 사전 정의된 앵커 박스의 형태에 제약을 받지 않으므로, 길고 얇거나 불규칙한 형태의 객체를 탐지하는 데 더 유리하다.16</li>
<li><strong>설계 단순화:</strong> 앵커 박스 관련 하이퍼파라미터(개수, 크기, 종횡비 등)를 튜닝할 필요가 없어 모델 설계와 학습 파이프라인이 단순해진다.</li>
</ol>
<h4>2.2.3  Decoupled Head</h4>
<p>YOLOv8의 예측 헤드는 분류(Classification)와 회귀(Bounding Box Regression)를 위한 네트워크를 분리하는 Decoupled Head 구조를 사용한다.16 객체를 ‘무엇인지’ 맞추는 분류 작업과 ‘어디에 있는지’ 맞추는 회귀 작업은 서로 다른 특성을 가진다. 이 두 작업을 단일 네트워크 헤드에서 동시에 처리하려고 하면, 두 작업 간의 목표 충돌(task conflict)이 발생하여 성능이 저하될 수 있다.</p>
<p>Decoupled Head는 분류를 위한 브랜치와 회귀를 위한 브랜치를 각각 독립적으로 구성하여 이러한 문제를 해결한다. 각 브랜치는 자신의 작업에만 집중하여 최적화될 수 있으므로, 모델의 수렴 속도가 빨라지고 최종적인 탐지 정확도가 향상되는 효과를 가져온다.16</p>
<h3>2.3  손실 함수 (Loss Function)</h3>
<p>YOLOv8의 전체 손실 함수 <span class="math math-inline">\mathcal{L}_{total}</span>는 Bbox 회귀 손실(<span class="math math-inline">\mathcal{L}_{box}</span>), 분류 손실(<span class="math math-inline">\mathcal{L}_{cls}</span>), 그리고 분포 초점 손실(<span class="math math-inline">\mathcal{L}_{dfl}</span>)의 가중합으로 구성된다. 각 손실 항의 가중치는 실험적으로 결정된 하이퍼파라미터(<span class="math math-inline">\lambda_{box}</span>, <span class="math math-inline">\lambda_{cls}</span>, <span class="math math-inline">\lambda_{dfl}</span>)에 의해 조절된다.19</p>
<p><span class="math math-display">
\mathcal{L}_{total} = \lambda_{box} \mathcal{L}_{box} + \lambda_{cls} \mathcal{L}_{cls} + \lambda_{dfl} \mathcal{L}_{dfl}
</span></p>
<h4>2.3.1  Bbox Loss (Lbox): CIoU Loss</h4>
<p>Bbox 회귀를 위해서는 CIoU(Complete Intersection over Union) Loss가 사용된다.20 CIoU는 전통적인 IoU(Intersection over Union)를 개선한 지표로, 두 Bbox 간의 관계를 세 가지 측면에서 종합적으로 평가한다.</p>
<ol>
<li><strong>중첩 영역(Overlap Area):</strong> IoU를 통해 두 박스가 얼마나 겹치는지를 측정한다.</li>
<li><strong>중심점 거리(Center Point Distance):</strong> 두 박스의 중심점 사이의 거리를 정규화하여 패널티를 부여한다.</li>
<li><strong>종횡비 일관성(Aspect Ratio Consistency):</strong> 두 박스의 너비와 높이 비율의 차이를 고려하여 패널티를 부여한다.</li>
</ol>
<p>CIoU Loss의 수식은 다음과 같다.21</p>
<p><span class="math math-display">
\mathcal{L}_{CIoU} = 1 - IoU + \frac{\rho^2(b, b^{gt})}{c^2} + \alpha v
</span><br />
여기서 각 항의 의미는 다음과 같다.</p>
<ul>
<li><span class="math math-inline">IoU</span>: 예측 박스(b)와 실제 박스(bgt)의 Intersection over Union.</li>
<li><span class="math math-inline">\rho^2(b, b^{gt})</span>: 두 박스 중심점 사이의 유클리드 거리 제곱.</li>
<li><span class="math math-inline">c</span>: 두 박스를 모두 포함하는 가장 작은 볼록 박스(convex box)의 대각선 길이.</li>
<li><span class="math math-inline">v</span>: 종횡비의 일관성을 측정하는 항으로, <span class="math math-inline">v = \frac{4}{\pi^2}(\arctan\frac{w^{gt}}{h^{gt}} - \arctan\frac{w}{h})^2</span>로 정의된다.</li>
<li><span class="math math-inline">\alpha</span>: <span class="math math-inline">v</span>의 가중치를 조절하는 양의 트레이드오프 파라미터로, <span class="math math-inline">\alpha = \frac{v}{(1-IoU)+v}</span>로 정의된다.</li>
</ul>
<h4>2.3.2  Classification Loss (Lcls): BCE Loss</h4>
<p>분류 손실에는 일반적으로 이진 교차 엔트로피(Binary Cross-Entropy, BCE) Loss가 사용된다.16 이는 각 클래스의 존재 여부를 독립적인 이진 분류 문제로 간주하여 손실을 계산한다. 때로는 클래스 불균형 문제를 완화하기 위해 VFL(Varifocal Loss)과 같은 변형된 형태의 손실 함수가 사용되기도 한다.16</p>
<h4>2.3.3  Distribution Focal Loss (Ldfl)</h4>
<p>YOLOv8은 Bbox 회귀의 정확도를 더욱 높이기 위해 DFL(Distribution Focal Loss)을 추가로 도입했다.19 전통적인 회귀 방식은 Bbox의 좌표를 단일 실수 값으로 예측한다. 하지만 실제 객체의 경계는 모호하거나 불확실한 경우가 많다. DFL은 이러한 불확실성을 모델링하기 위해 Bbox의 각 좌표(top, bottom, left, right)를 하나의 값이 아닌, 이산적인 확률 분포로 학습한다.20</p>
<p>예를 들어, 실제 경계값 <span class="math math-inline">y</span>가 <span class="math math-inline">y_i</span>와 <span class="math math-inline">y_{i+1}</span> 사이에 존재할 때, DFL은 모델이 예측한 분포 <span class="math math-inline">S</span>가 <span class="math math-inline">y_i</span>와 <span class="math math-inline">y_{i+1}</span>에 높은 확률을 할당하도록 유도한다. 이는 Cross-Entropy Loss를 회귀 문제에 적용한 것과 유사하며, 수식은 다음과 같다.23</p>
<p><span class="math math-display">
\mathcal{L}_{DFL}(S_i, S_{i+1}) = -( (y_{i+1} - y)\log(S_i) + (y - y_i)\log(S_{i+1}) )
</span><br />
여기서 <span class="math math-inline">S_i</span>와 <span class="math math-inline">S_{i+1}</span>은 각각 위치 <span class="math math-inline">y_i</span>와 <span class="math math-inline">y_{i+1}</span>에 대한 예측 확률이다. 이 방식을 통해 모델은 경계 위치에 대한 ’확신’의 정도를 학습하게 되어, 더 유연하고 강건한 위치 예측이 가능해진다.</p>
<h2>3. YOLOv9: 정보 손실 극복을 위한 이론적 혁신</h2>
<p>YOLOv9은 이전 버전들이 네트워크 ’구조(Architecture)’를 개선하는 데 집중했던 것과 달리, ’어떻게 학습할 것인가’라는 근본적인 질문에 대한 새로운 해답을 제시한다. 모델의 성능 한계가 아키텍처 자체뿐만 아니라, 깊은 네트워크에서의 정보 손실로 인한 ’불완전한 학습’에도 기인한다는 깊은 통찰을 바탕으로, 네트워크의 ‘학습 과정(Training Process)’ 자체를 혁신하려 시도한다. 이는 객체 탐지 모델 개발의 패러다임을 아키텍처 설계 중심에서 정보 흐름 제어 중심으로 전환하려는 중요한 시도이다.</p>
<h3>3.1  이론적 배경: 정보 병목 현상 (Information Bottleneck Principle)</h3>
<p>심층 신경망은 입력 데이터로부터 유용한 특징을 추출하기 위해 여러 개의 비선형 변환 계층을 순차적으로 통과시킨다. 이 과정에서 필연적으로 정보 손실이 발생하는데, 이를 ’정보 병목 현상’이라고 한다.12 데이터가 네트워크의 깊은 계층으로 전달될수록 원본 입력이 가졌던 풍부한 정보 중 일부는 소실되고, 이는 최종적인 예측 성능을 저해하는 주요 원인이 된다.</p>
<p>이 현상은 수학적으로 상호 정보량(Mutual Information)을 사용하여 표현할 수 있다. 입력 데이터 <span class="math math-inline">X</span>가 함수 <span class="math math-inline">f_\theta</span>와 <span class="math math-inline">g_\phi</span>를 차례로 통과할 때, 상호 정보량의 관계는 다음과 같은 부등식을 만족한다.27</p>
<p><span class="math math-display">
I(X, X) \ge I(X, f_\theta(X)) \ge I(X, g_\phi(f_\theta(X)))
</span><br />
이 부등식은 데이터가 네트워크 계층을 통과할수록 원본 <span class="math math-inline">X</span>와의 상호 정보량이 단조 감소함을 의미한다. 즉, 정보는 보존되거나 손실될 뿐, 새로 생성될 수 없다. 정보 손실이 심각해지면, 네트워크는 손실 함수를 계산하고 파라미터를 업데이트하는 데 필요한 신뢰성 있는 그래디언트를 생성하기 어려워진다.26 이러한 문제는 특히 파라미터 수가 적어 표현력이 제한적인 경량 모델에서 더욱 치명적으로 작용한다.12</p>
<h3>3.2  핵심 기술 1: 프로그래머블 그래디언트 정보 (PGI - Programmable Gradient Information)</h3>
<p>YOLOv9은 정보 병목 문제를 해결하기 위해 PGI라는 혁신적인 학습 메커니즘을 제안했다.12 PGI는 네트워크 구조 자체를 변경하여 추론 비용을 증가시키는 대신, 학습 과정에만 개입하여 정보 흐름을 제어하고 그래디언트의 품질을 높이는 것을 목표로 한다. PGI는 세 가지 주요 구성 요소로 이루어져 있다.12</p>
<ol>
<li><strong>Main Branch:</strong> 실제 추론(inference)에 사용되는 주 네트워크이다. 효율적인 추론을 위해 최적화된 구조를 가진다.</li>
<li><strong>Auxiliary Reversible Branch:</strong> 학습(training) 시에만 활성화되는 보조 분기이다. 이 브랜치는 ’가역 함수(Reversible Function)’의 개념을 차용하여 설계되었다. 가역 함수는 정보 손실 없이 입력을 복원할 수 있는 함수로, 이 브랜치를 통해 입력 데이터의 완전한 정보가 네트워크의 깊은 계층까지 손실 없이 전달될 수 있다.</li>
<li><strong>Multi-level Auxiliary Information:</strong> 보조 분기에서 보존된 완전한 정보를 주 네트워크의 여러 중간 계층에 주입하는 역할을 한다. 이를 통해 주 네트워크의 깊은 계층에서도 손실 함수가 신뢰성 있는 그래디언트를 계산하는 데 필요한 충분한 정보를 얻게 된다.</li>
</ol>
<p>PGI의 작동 원리는 다음과 같다. 학습 과정에서 입력 데이터는 Main Branch와 Auxiliary Reversible Branch로 동시에 입력된다. Main Branch는 일반적인 순방향 전파를 수행하는 반면, Auxiliary Reversible Branch는 정보 손실을 최소화하며 특징을 전달한다. 이후 Multi-level Auxiliary Information 메커니즘을 통해 보조 분기의 풍부한 정보가 Main Branch의 여러 지점에 연결되어, Main Branch가 겪는 정보 병목 현상을 완화한다. 이렇게 보강된 정보를 바탕으로 손실 함수는 더 정확하고 신뢰성 있는 그래디언트를 계산하고, 이를 통해 모델 파라미터가 효과적으로 업데이트된다.12</p>
<p>학습이 완료된 후, 추론 시에는 Auxiliary Reversible Branch와 관련된 모든 구성 요소가 제거된다. 따라서 PGI는 추론 속도에 전혀 영향을 주지 않으면서(no additional inference cost) 모델의 최종 정확도와 학습 효율만을 향상시키는 매우 효율적인 학습 전략이다.12</p>
<h3>3.3  핵심 기술 2: 일반화된 효율적 계층 집계 네트워크 (GELAN - Generalized Efficient Layer Aggregation Network)</h3>
<p>GELAN은 PGI의 효과를 극대화하기 위해 함께 설계된 새로운 경량 네트워크 아키텍처이다.3 이는 기존의 고효율 네트워크 설계 기법인 ELAN(Efficient Layer Aggregation Network)을 일반화하고 확장한 것이다. GELAN의 설계 철학은 파라미터 효율성, 연산 복잡도, 학습 능력, 그리고 추론 속도라는 네 가지 요소를 동시에 고려하는 데 있다.12</p>
<p>GELAN의 가장 큰 특징 중 하나는 Depth-wise convolution과 같은 특수하고 복잡한 연산에 의존하지 않고, 전통적인 표준 합성곱(conventional convolution) 연산만을 사용하여 높은 효율성을 달성한다는 점이다.12 이는 특정 하드웨어에 대한 의존성을 줄여 범용성을 높이고, 더 나은 파라미터 활용도를 가능하게 한다. 또한, GELAN은 사용자가 다양한 추론 장치(예: 고성능 GPU, 저전력 엣지 디바이스)의 특성에 맞게 내부의 계산 블록을 자유롭게 선택하고 조합할 수 있는 유연한 구조를 제공한다.12</p>
<h3>3.4  손실 함수와의 관계</h3>
<p>YOLOv9은 새로운 손실 함수를 제안하지 않는다. 대신, PGI라는 프레임워크를 통해 기존의 어떤 손실 함수를 사용하더라도 그 성능을 극대화할 수 있는 기반을 제공한다. PGI의 핵심은 손실 함수 자체를 개선하는 것이 아니라, 손실 함수로 전달되는 ’정보의 질’을 높이는 데 있다.12</p>
<p>기존 심층 네트워크에서는 정보 병목으로 인해 손실 함수가 불완전하고 왜곡된 정보(deep features)를 기반으로 손실 값을 계산했다. 이는 결국 부정확한 그래디언트를 생성하여 모델 학습을 방해하는 원인이 되었다. PGI는 Auxiliary Reversible Branch를 통해 원본 입력의 완전한 정보를 보존하고 이를 손실 함수 계산에 필요한 특징들과 연결함으로써, 손실 함수가 ’완전한 정보’에 기반하여 훨씬 더 신뢰성 있는 그래디언트를 생성하도록 돕는다.12 이러한 접근 방식은 특정 손실 함수에 구애받지 않는 범용성을 가지며, YOLOv9이 다양한 객체 탐지 관련 손실 함수들과 유연하게 결합될 수 있도록 한다.12</p>
<h2>4. RF-DETR: Transformer 기반 실시간 탐지의 새로운 지평</h2>
<p>RF-DETR은 독자적인 완전한 혁신이라기보다는, DETR, Deformable DETR, RT-DETR, DINOv2 등 선행 연구들의 핵심 아이디어를 성공적으로 융합하고 실시간 성능에 맞게 최적화한 ’통합과 최적화’의 산물이다. RF-DETR의 경쟁력은 하나의 획기적인 아이디어에서 비롯된 것이 아니라, Transformer 기반 객체 탐지 분야에서 지난 몇 년간 축적된 핵심 연구 성과(효율적인 어텐션, 실시간 인코더, 고품질 Query, 강력한 백본)를 영리하게 조합하여 실용적인 SOTA 모델로 완성시킨 데 있다.</p>
<h3>4.1  아키텍처의 계보: DETR의 진화</h3>
<p>RF-DETR의 아키텍처를 이해하기 위해서는 그 기술적 뿌리가 되는 DETR 계열 모델들의 진화 과정을 먼저 살펴볼 필요가 있다.</p>
<ul>
<li><strong>DETR (DEtection TRansformer):</strong> 2020년 Facebook AI에 의해 제안된 DETR은 객체 탐지 분야에 Transformer를 처음 도입한 모델이다. 객체 탐지를 이미지 특징과 고정된 수의 학습 가능한 ‘객체 쿼리(Object Queries)’ 사이의 상호작용을 통해 예측하는 ‘집합 예측(Set Prediction)’ 문제로 재정의했다.4 이 방식은 헝가리안 알고리즘을 이용한 이분 매칭 손실을 통해 End-to-End 학습이 가능하게 하여, 복잡한 후처리 과정인 NMS를 완전히 제거하는 데 성공했다. 하지만 Transformer의 어텐션 메커니즘이 이미지 전체 픽셀에 대해 연산을 수행하기 때문에 계산 복잡도가 매우 높고 학습 수렴 속도가 느리다는 치명적인 단점을 안고 있었다.30</li>
<li><strong>Deformable DETR:</strong> DETR의 높은 연산량과 느린 수렴 문제를 해결하기 위해 제안되었다. Deformable DETR의 핵심은 ‘Deformable Attention’ 메커니즘이다.30 이는 각 쿼리가 이미지 특징 맵 전체를 보는 대신, 객체의 위치와 스케일을 고려하여 주변의 소수(a small set)의 핵심적인 샘플링 포인트에만 집중(attend)하도록 하는 방식이다.30 이를 통해 연산량을 획기적으로 줄이면서도, 특히 작은 객체에 대한 탐지 성능을 크게 향상시켰다. RF-DETR은 바로 이 Deformable DETR의 효율적인 어텐션 메커니즘을 기반으로 구축되었다.15</li>
<li><strong>RT-DETR (Real-Time DETR):</strong> Deformable DETR을 더욱 발전시켜 최초로 실시간 성능을 달성한 End-to-End 탐지기이다.1 RT-DETR은 두 가지 핵심적인 개선을 통해 실시간성을 확보했다. 첫째, 기존의 무거운 Transformer 인코더를 CNN과 Transformer의 장점을 결합한 ’효율적인 하이브리드 인코더(Efficient Hybrid Encoder)’로 대체하여 다중 스케일 특징을 빠르게 처리했다. 둘째, 디코더의 성능을 높이기 위해 ‘IoU-aware Query Selection’ (또는 Uncertainty-minimal Query Selection) 메커니즘을 도입하여 더 품질 좋은 초기 객체 쿼리를 선택하도록 했다.5 RF-DETR은 RT-DETR의 이러한 실시간 최적화 기법들로부터 깊은 영감을 받았다.32</li>
</ul>
<h3>4.2  End-to-End 파이프라인: NMS 제거의 의미</h3>
<p>RF-DETR을 포함한 모든 DETR 계열 모델의 가장 큰 구조적 특징은 NMS(Non-Maximum Suppression)가 필요 없는 완전한 End-to-End 파이프라인을 구현했다는 점이다.</p>
<p>기존의 YOLO와 같은 탐지기들은 일반적으로 실제 객체 수보다 훨씬 많은 수의 후보 바운딩 박스를 예측한 후, NMS라는 후처리 단계를 통해 중복된 예측들을 제거하고 최종 결과를 얻는다.2 NMS는 효과적이지만 몇 가지 본질적인 문제점을 가진다. 첫째, IoU 임계값이나 신뢰도 점수 임계값과 같은 하이퍼파라미터에 민감하여 튜닝이 필요하고, 이는 모델의 성능 불안정성을 야기한다.7 둘째, NMS 알고리즘 자체의 연산 비용으로 인해 전체 추론 파이프라인의 병목이 될 수 있다.</p>
<p>반면, RF-DETR은 학습 과정에서 헝가리안 알고리즘을 이용한 이분 매칭을 통해, 각 예측 쿼리가 단 하나의 실제 객체에만 할당되도록 강제한다.5 즉, 모델 자체가 중복 없는 예측을 생성하도록 학습되기 때문에 NMS가 원천적으로 불필요하다.32 이는 다음과 같은 중요한 이점을 제공한다.</p>
<ul>
<li><strong>파이프라인 단순화:</strong> 복잡한 후처리 로직이 사라져 전체 시스템의 설계와 배포가 매우 간결해진다.</li>
<li><strong>성능 안정성:</strong> NMS 관련 하이퍼파라미터 튜닝이 필요 없어, 다양한 데이터셋과 환경에서 더 안정적이고 예측 가능한 성능을 보인다.</li>
<li><strong>잠재적 속도 향상:</strong> NMS 연산에 소요되던 시간을 절약할 수 있다.</li>
</ul>
<h3>4.3  핵심 메커니즘</h3>
<h4>4.3.1  Query Selection</h4>
<p>DETR 계열 모델의 성능은 디코더에 입력되는 초기 객체 쿼리(initial object queries)의 품질에 크게 좌우된다.34 초기 쿼리가 객체가 있을 법한 위치의 유용한 특징 정보를 담고 있어야 디코더가 빠르고 정확하게 예측을 정제할 수 있기 때문이다.</p>
<p>RT-DETR은 이를 위해 IoU-aware(또는 논문에 따라 Uncertainty-minimal) Query Selection이라는 진보된 방식을 제안했다.5 이 방식은 단순히 분류 점수가 높은 특징을 선택하는 것을 넘어, 위치 정확도(IoU로 대변됨)까지 고려하여 분류와 위치 양쪽에서 모두 신뢰도가 높은 특징을 쿼리로 선택한다.6 RF-DETR 역시 이러한 고품질 쿼리 선택 전략을 채택하여, 디코더가 더 적은 반복만으로도 정확한 예측에 도달할 수 있도록 하여 효율성을 높인다.</p>
<h4>4.3.2  Iterative Box Refinement</h4>
<p>DETR의 디코더는 일반적으로 여러 개의 동일한 구조의 레이어가 쌓인 형태를 가진다. 이 구조는 자연스럽게 ‘반복적 정제(Iterative Refinement)’ 메커니즘으로 작동한다.1</p>
<p>첫 번째 디코더 레이어는 초기 쿼리를 입력받아 객체의 위치와 클래스에 대한 초기 예측을 생성한다. 이 예측 결과(특히 예측된 Bbox 좌표)는 다음 디코더 레이어의 입력 쿼리로 다시 사용된다. 두 번째 레이어는 이 정제된 쿼리를 바탕으로 더 정확한 예측을 수행하고, 이 과정이 마지막 디코더 레이어까지 반복된다.38 각 디코더 레이어는 이전 레이어의 예측 오류를 보정하는 역할을 수행하며, 이러한 계층적이고 반복적인 정제 과정은 최종 예측의 정확도를 점진적으로 높이는 데 매우 중요한 역할을 한다.30</p>
<h3>4.4  손실 함수: 이분 매칭과 헝가리안 손실 (Hungarian Loss)</h3>
<p>RF-DETR의 학습을 가능하게 하는 핵심은 ’헝가리안 손실’로 알려진 독특한 손실 함수이다. 이 손실 함수는 두 단계로 구성된다.40</p>
<h4>4.4.1  1단계: 최적 이분 매칭 (Optimal Bipartite Matching)</h4>
<p>모델은 고정된 개수 <span class="math math-inline">N</span>(예: 100개)의 예측 결과 집합 <span class="math math-inline">\hat{y} = \{\hat{y}_i\}_{i=1}^N</span>을 출력한다. 이미지 내 실제 객체(Ground Truth)의 수는 가변적이므로, 실제 객체 집합 <span class="math math-inline">y</span>에 “객체 없음(∅)” 클래스를 추가하여 크기를 <span class="math math-inline">N</span>으로 맞춘다.</p>
<p>첫 번째 단계는 이 두 집합 간의 최적의 일대일 매칭(one-to-one matching)을 찾는 것이다. 즉, 어떤 예측 결과(<span class="math math-inline">\hat{y}_{\sigma(i)}</span>)가 어떤 실제 객체(<span class="math math-inline">y_i</span>)에 대응되는지를 결정하는 과정이다. 이 최적의 매칭 <span class="math math-inline">\hat{\sigma}</span>는 전체 매칭 비용을 최소화하는 순열(permutation)로 정의되며, 이 문제를 효율적으로 풀기 위해 헝가리안 알고리즘이 사용된다.29</p>
<p><span class="math math-display">
\hat{\sigma} = \arg\min_{\sigma \in S_N} \sum_{i}^{N} \mathcal{L}_{\text{match}}(y_i, \hat{y}_{\sigma(i)})
</span><br />
이때, 개별 쌍에 대한 매칭 비용 <span class="math math-inline">\mathcal{L}_{\text{match}}</span>는 클래스 예측의 정확성과 Bbox 예측의 정확성을 모두 고려하여 계산된다. 실제 객체가 존재하는 경우(<span class="math math-inline">c_i \neq \emptyset</span>), 매칭 비용은 해당 클래스에 대한 예측 확률(<span class="math math-inline">\hat{p}_{\sigma(i)}(c_i)</span>에 음수를 취한 값)과 Bbox 손실(<span class="math math-inline">\mathcal{L}_{\text{box}}</span>)의 합으로 구성된다.41</p>
<p><span class="math math-display">
\mathcal{L}_{\text{match}}(y_i, \hat{y}_{\sigma(i)}) = - \mathbf{1}_{\{c_i \neq \emptyset\}} \hat{p}_{\sigma(i)}(c_i) + \mathbf{1}_{\{c_i \neq \emptyset\}} \mathcal{L}_{\text{box}}(b_i, \hat{b}_{\sigma(i)})
</span></p>
<h4>4.4.2  2단계: 헝가리안 손실 계산</h4>
<p>최적의 매칭 <span class="math math-inline">\hat{\sigma}</span>이 결정되면, 이 매칭된 쌍들에 대해서만 최종 손실을 계산한다. 이를 헝가리안 손실이라고 하며, 분류 손실과 Bbox 손실의 합으로 구성된다.41</p>
<p><span class="math math-display">
\mathcal{L}_{\text{Hungarian}}(y, \hat{y}) = \sum_{i=1}^{N} \left[ - \log \hat{p}_{\hat{\sigma}(i)}(c_i) + \mathbf{1}_{\{c_i \neq \emptyset\}} \mathcal{L}_{\text{box}}(b_i, \hat{b}_{\hat{\sigma}(i)}) \right]
</span></p>
<ul>
<li><strong>분류 손실:</strong> 매칭된 실제 클래스 <span class="math math-inline">c_i</span>에 대한 예측 확률의 음의 로그 우도(Negative Log-Likelihood)로 계산된다. 이는 일반적인 Cross-Entropy Loss와 동일하다.</li>
<li><strong>Bbox 손실 (<span class="math math-inline">\mathcal{L}_{box}</span>):</strong> Bbox 회귀를 위한 손실로, L1 Loss와 GIoU(Generalized IoU) Loss의 선형 결합으로 정의된다. L1 Loss는 Bbox 좌표의 절대적인 차이를, GIoU Loss는 Bbox의 크기와 위치 관계를 종합적으로 고려한다.40</li>
</ul>
<p><span class="math math-display">
\mathcal{L}_{\text{box}}(b_i, \hat{b}_{\sigma(i)}) = \lambda_{\text{iou}} \mathcal{L}_{\text{GIoU}}(b_i, \hat{b}_{\sigma(i)}) + \lambda_{L1} ||b_i - \hat{b}_{\sigma(i)}||_1
</span></p>
<p>이러한 두 단계의 손실 계산 방식을 통해 RF-DETR은 각 객체에 대해 단 하나의 고유한 예측을 생성하도록 학습되며, 이는 NMS를 불필요하게 만드는 핵심 원리이다.</p>
<h2>5. 정량적 성능 비교 분석: 정확도 대 연산량</h2>
<p>모델의 성능을 객관적으로 평가하기 위해서는 정확도와 함께 모델의 크기 및 연산량을 종합적으로 고려해야 한다. 본 장에서는 세 모델군(YOLOv8, YOLOv9, RF-DETR)의 세부 모델별 성능을 정량적으로 비교하고, 이를 통해 각 아키텍처의 효율성과 트레이드오프를 분석한다.</p>
<h3>5.1  성능 평가 지표 소개</h3>
<ul>
<li><strong>mAP (mean Average Precision):</strong> 모델의 탐지 정확도를 나타내는 핵심 지표이다. 본 보고서에서는 MS COCO val2017 데이터셋을 기준으로, IoU(Intersection over Union) 임계값을 0.5에서 0.95까지 0.05 간격으로 변화시키며 측정한 mAP의 평균값(mAPval 50-95)을 사용한다. 이 값은 모델의 전반적인 위치 및 분류 정확도를 종합적으로 평가한다.9</li>
<li><strong>Parameters (M):</strong> 모델의 학습 가능한 파라미터 개수를 백만(Million) 단위로 나타낸다. 모델의 크기와 메모리 요구량을 가늠하는 척도이다.</li>
<li><strong>FLOPs (Floating Point Operations per Second):</strong> 640x640 크기의 단일 이미지를 순방향으로 처리하는 데 필요한 부동소수점 연산의 총량을 Giga(109) 단위로 나타낸다. 이는 모델의 이론적인 계산 복잡도를 의미하며, 하드웨어에 독립적인 지표이다.10</li>
<li><strong>Latency/FPS:</strong> 특정 하드웨어(예: NVIDIA T4, A100 GPU)에서 단일 이미지를 처리하는 데 걸리는 시간(ms) 또는 초당 처리할 수 있는 프레임 수(FPS)이다. 이는 모델의 실제 추론 속도를 나타내며, 하드웨어 및 소프트웨어 최적화 수준에 따라 달라질 수 있다.44</li>
</ul>
<h3>5.2  세부 모델별 성능 스펙트럼 종합 비교</h3>
<p>아래 표는 YOLOv8, YOLOv9, RF-DETR의 주요 세부 모델들에 대한 성능 지표를 종합적으로 정리한 것이다. 이 표는 각 모델군이 제공하는 다양한 스케일의 모델들이 정확도와 연산량 측면에서 어떤 스펙트럼을 형성하는지 한눈에 파악할 수 있도록 돕는다.</p>
<p><strong>Table 1: YOLOv8, YOLOv9, RF-DETR 세부 모델 종합 성능 비교 (MS COCO val2017 기준)</strong></p>
<table><thead><tr><th>Model Family</th><th>Model Variant</th><th>mAPval 50-95 (%)</th><th>Parameters (M)</th><th>FLOPs (B) @640px</th><th></th></tr></thead><tbody>
<tr><td><strong>YOLOv8</strong></td><td>YOLOv8n</td><td>37.3</td><td>3.2</td><td>8.7</td><td></td></tr>
<tr><td></td><td>YOLOv8s</td><td>44.9</td><td>11.2</td><td>28.6</td><td></td></tr>
<tr><td></td><td>YOLOv8m</td><td>50.2</td><td>25.9</td><td>78.9</td><td></td></tr>
<tr><td></td><td>YOLOv8l</td><td>52.9</td><td>43.7</td><td>165.2</td><td></td></tr>
<tr><td></td><td>YOLOv8x</td><td>53.9</td><td>68.2</td><td>257.8</td><td></td></tr>
<tr><td><strong>YOLOv9</strong></td><td>YOLOv9-T</td><td>38.3</td><td>2.0</td><td>7.7</td><td></td></tr>
<tr><td></td><td>YOLOv9-S</td><td>46.8</td><td>7.1</td><td>26.4</td><td></td></tr>
<tr><td></td><td>YOLOv9-M</td><td>51.4</td><td>20.0</td><td>76.3</td><td></td></tr>
<tr><td></td><td>YOLOv9-C</td><td>53.0</td><td>25.3</td><td>102.1</td><td></td></tr>
<tr><td></td><td>YOLOv9-E</td><td>55.6</td><td>57.3</td><td>189.0</td><td></td></tr>
<tr><td><strong>RF-DETR</strong></td><td>RF-DETR-N</td><td>48.4</td><td>30.5</td><td>N/A</td><td></td></tr>
<tr><td></td><td>RF-DETR-S</td><td>53.0</td><td>32.1</td><td>N/A</td><td></td></tr>
<tr><td></td><td>RF-DETR-M</td><td>54.7</td><td>33.7</td><td>N/A</td><td></td></tr>
<tr><td></td><td>RF-DETR-B</td><td>53.3</td><td>29.0</td><td>N/A</td><td></td></tr>
<tr><td></td><td>RF-DETR-L</td><td>60.5</td><td>128.0</td><td>N/A</td><td></td></tr>
<tr><td>Data Sources: 10</td><td></td><td></td><td></td><td></td><td></td></tr>
</tbody></table>
<p><em>(주: RF-DETR의 GFLOPs 데이터는 공개된 자료에서 확인되지 않았다.)</em></p>
<h3>5.3  Pareto 최적 곡선 및 연산 효율성 분석</h3>
<p>위 표의 데이터를 바탕으로 각 모델군의 정확도-연산량 트레이드오프를 분석하면 다음과 같은 중요한 경향성을 발견할 수 있다.</p>
<ul>
<li>YOLOv9 vs. YOLOv8: 효율성의 경계를 밀어 올리다.</li>
</ul>
<p>YOLOv9은 모든 체급에서 YOLOv8을 압도하며 새로운 파레토 최적 곡선(Pareto Frontier)을 형성한다. 이는 PGI와 GELAN이 단순한 이론적 개념을 넘어, 실제 연산 효율성을 극적으로 향상시키는 실질적인 기술임을 입증한다. 예를 들어, YOLOv9-C 모델은 25.3M의 파라미터와 102.1 GFLOPs의 연산량으로 53.0%의 mAP를 달성한다. 이는 비슷한 크기의 YOLOv8-m(25.9M, 78.9 GFLOPs, 50.2% mAP)보다 훨씬 높은 정확도이다. 특히, 가장 큰 모델인 YOLOv9-E는 57.3M의 파라미터와 189.0 GFLOPs로 55.6%의 mAP를 기록했는데, 이는 더 많은 파라미터(68.2M)와 연산량(257.8 GFLOPs)을 사용하는 YOLOv8x의 53.9% mAP를 가볍게 뛰어넘는 수치이다. 이는 YOLOv9의 아키텍처(GELAN)와 학습 방식(PGI)이 기존 YOLOv8보다 근본적으로 더 효율적이라는 강력한 증거이며, 정보 손실을 줄이는 것이 곧 연산 자원의 효율적 사용으로 이어진다는 이론을 실증적으로 보여준다.</p>
<ul>
<li>RF-DETR vs. YOLO: 새로운 성능 체급의 등장.</li>
</ul>
<p>RF-DETR은 YOLO 계열과는 다른 트레이드오프 곡선을 보여준다. 특히 주목할 점은, YOLO 계열 모델들이 55% mAP 근방에서 성능이 포화되는 경향을 보이는 반면, RF-DETR-L 모델은 128M의 파라미터를 사용하여 60.5%라는 압도적인 mAP를 달성한다는 것이다.15 이는 실시간 탐지기(NVIDIA T4 GPU 기준 25+ FPS를 만족하는 모델) 중에서는 전례 없는 성능으로, 기존에는 불가능하다고 여겨졌던 ’실시간 초고정확도’라는 새로운 시장을 Transformer 기반 모델이 개척할 수 있음을 보여주는 신호탄이다. 이는 고성능 GPU 자원을 활용할 수 있는 클라우드 기반 AI 서비스나 고성능 자율주행 시스템과 같은 응용 분야에서, YOLO 계열 대신 RF-DETR이 새로운 표준이 될 수 있는 가능성을 열어준다.</p>
<ul>
<li>파라미터 효율성에 대한 고찰.</li>
</ul>
<p>Transformer 기반인 RF-DETR과 CNN 기반인 YOLO 계열을 단순 파라미터 수로 비교하는 것은 오해의 소지가 있다. 예를 들어, RF-DETR-S는 32.1M의 파라미터로 53.0%의 mAP를 달성하여, 이보다 훨씬 많은 43.7M의 파라미터를 사용하는 YOLOv8l(52.9% mAP)과 동등하거나 더 나은 성능을 보인다.47 이는 Transformer의 Self-attention 메커니즘이 이미지의 전역적 컨텍스트를 모델링하는 데 더 효율적이어서, CNN보다 파라미터 당 더 높은 표현력을 가질 수 있음을 시사한다. 따라서 두 아키텍처 계열 간의 진정한 효율성을 평가하기 위해서는 단순 파라미터 수뿐만 아니라, 연산량(GFLOPs)과 실제 추론 속도(Latency)를 함께 고려하는 다각적인 접근이 필수적이다.</p>
<h2>6. 구조적 특성에 따른 질적 고찰 및 종합</h2>
<p>정량적인 성능 지표를 넘어, 각 모델의 근본적인 아키텍처 차이가 어떤 질적인 특성으로 이어지는지 고찰하는 것은 모델 선택에 있어 매우 중요하다. CNN과 Transformer라는 두 가지 다른 철학은 모델의 작동 방식, 파이프라인의 복잡도, 그리고 특정 상황에서의 강건성에 뚜렷한 차이를 만들어낸다.</p>
<h3>6.1  CNN vs. Transformer: 국소적 특징과 전역적 컨텍스트</h3>
<ul>
<li><strong>YOLO (CNN 기반):</strong> YOLOv8과 YOLOv9의 근간을 이루는 CNN은 본질적으로 국소적인 연산(local operation)이다. 합성곱 필터는 이미지의 작은 영역(receptive field) 내에서 특징을 추출하고, 이러한 국소적 특징들이 여러 계층을 거치며 점차 조합되어 더 복잡하고 추상적인 특징으로 발전한다. 이 방식은 객체의 질감, 모서리, 형태 등 지역적인 시각적 패턴을 인식하는 데 매우 효율적이고 효과적이다. 하지만 이로 인해 이미지 전체의 맥락이나 객체들 간의 원거리 관계를 파악하는 데에는 구조적인 한계를 가질 수 있다.</li>
<li><strong>RF-DETR (Transformer 기반):</strong> RF-DETR의 핵심인 Transformer의 Self-attention 메커니즘은 이미지의 모든 픽셀(또는 특징 벡터) 쌍 간의 관계를 직접적으로 계산한다.32 이는 모델이 이미지의 특정 부분에만 국한되지 않고, 전체적인 장면의 전역적 컨텍스트(global context)를 이해할 수 있게 한다. 예를 들어, ’테니스 라켓’을 든 ’사람’을 탐지할 때, 두 객체 간의 관계를 이해하는 것이 탐지 정확도를 높일 수 있다. 이러한 능력은 여러 객체가 복잡하게 얽혀 있거나, 배경과 객체의 구분이 모호한 장면에서 특히 강력한 장점으로 작용할 수 있다.</li>
</ul>
<p>이러한 근본적인 작동 방식의 차이는 모델이 범하는 오류의 유형에도 영향을 미친다. CNN 기반의 YOLO는 국소적 특징에 의존하기 때문에, 객체의 일부가 가려지거나 비정형적인 모습을 하고 있을 때 객체 자체를 놓치거나(False Negative) 다른 객체로 오인(Classification Error)할 가능성이 상대적으로 높을 수 있다. 반면, Transformer 기반의 RF-DETR은 전역적 컨텍스트를 과도하게 고려하여, 객체와 유사한 패턴을 가진 복잡한 배경을 객체로 오인하거나(False Positive) Bbox의 경계를 약간 부정확하게 예측(Localization Error)할 가능성이 있다. 따라서 단순히 mAP 점수만으로 모델을 평가하기보다는, 특정 애플리케이션에서 어떤 종류의 오류가 더 치명적인지를 분석하고, 해당 오류를 덜 범하는 경향이 있는 아키텍처를 선택하는 전략적 접근이 필요하다.</p>
<h3>6.2  파이프라인 복잡도: NMS의 유무가 미치는 영향</h3>
<ul>
<li><strong>YOLO:</strong> YOLOv8과 YOLOv9은 NMS(Non-Maximum Suppression) 후처리 과정이 필수적이다. 모델이 예측한 수많은 후보 Bbox 중에서 가장 신뢰도 높은 Bbox를 남기고 중복된 것들을 제거하는 이 과정은, 전체 추론 파이프라인에 또 다른 로직과 하이퍼파라미터(IoU 임계값, 신뢰도 임계값)를 추가한다.7 이는 배포 및 최적화 과정을 복잡하게 만들고, 하이퍼파라미터 값에 따라 최종 성능이 민감하게 변동될 수 있는 불안정성을 내포한다.</li>
<li><strong>RF-DETR:</strong> End-to-End 학습 방식을 통해 NMS가 원천적으로 불필요하다.5 모델의 출력이 곧바로 최종 결과물이 되므로 전체 파이프라인이 매우 단순하고 명료하다. 이는 개발 및 배포의 편의성을 크게 향상시키며, NMS 관련 튜닝 없이도 일관되고 안정적인 예측 결과를 보장하는 강력한 장점을 가진다.</li>
</ul>
<h3>6.3  일반화 및 도메인 적응성</h3>
<p>모델이 학습 데이터셋뿐만 아니라, 한 번도 보지 못한 새로운 환경이나 도메인에서도 얼마나 잘 작동하는지는 실제 적용에 있어 매우 중요한 요소이다.</p>
<p>RF-DETR은 MS COCO와 같은 표준 벤치마크뿐만 아니라, 항공 이미지, 산업 검사, 자연 환경 등 100개의 다양한 실제 데이터셋으로 구성된 RF100-VL 벤치마크에서도 SOTA 수준의 성능을 입증했다.45 이러한 뛰어난 일반화 성능은 두 가지 요인에 기인하는 것으로 분석된다. 첫째, Transformer 아키텍처의 강력한 표현력과 전역적 컨텍스트 이해 능력이 다양한 시각적 패턴에 더 잘 적응하도록 돕는다. 둘째, RF-DETR은 DINOv2와 같이 방대한 양의 데이터로 사전 학습된 강력한 Vision Transformer를 백본으로 사용하여, 특정 도메인에 국한되지 않는 범용적인 특징 추출 능력을 갖추었기 때문이다.32 반면, 상대적으로 특정 데이터셋(주로 COCO)에 맞춰 처음부터 학습(train-from-scratch)되는 경우가 많은 YOLO 계열은 새로운 도메인에 적용될 때 RF-DETR에 비해 성능 저하가 더 클 수 있다.</p>
<h2>7. 결론: 사용 사례 기반 모델 선정 가이드라인 및 전망</h2>
<h3>7.1  최종 요약 및 모델별 강점 정리</h3>
<p>본 보고서에서 심층 분석한 YOLOv8, YOLOv9, RF-DETR은 각각 뚜렷한 강점과 특징을 가지며, 실시간 객체 탐지 기술의 서로 다른 발전 방향을 대표한다. 각 모델의 핵심적인 강점을 요약하면 다음과 같다.</p>
<ul>
<li><strong>YOLOv8: 균형과 성숙의 표준 모델.</strong> YOLOv8은 현재 산업계에서 가장 널리 검증되고 사용되는 모델 중 하나이다. 정확도와 속도 간의 뛰어난 균형을 제공하며, 방대한 커뮤니티, 풍부한 문서, 그리고 안정적인 개발 생태계를 갖추고 있다. 새로운 기술의 도입보다는 검증된 기술들의 최적화에 초점을 맞추었기 때문에, 예측 가능하고 안정적인 개발 및 배포가 최우선 순위인 프로젝트에 가장 적합한 선택지이다.</li>
<li><strong>YOLOv9: 압도적인 연산 효율성.</strong> YOLOv9은 정보 병목이라는 근본적인 문제를 해결하기 위한 PGI와 GELAN을 통해, 동일한 연산 자원 대비 최고의 정확도를 달성한다. 이는 특히 컴퓨팅 자원이 제한적인 엣지 디바이스나 임베디드 시스템 환경에서 성능을 극한으로 끌어올려야 할 때 가장 강력한 무기가 된다.12 YOLOv9은 ’효율성’이 핵심 요구사항인 모든 시나리오에서 가장 먼저 고려되어야 할 모델이다.</li>
<li><strong>RF-DETR: 최고의 절대 성능과 파이프라인 단순성.</strong> RF-DETR은 실시간 모델 최초로 60 mAP의 벽을 넘어서며 절대적인 정확도의 새로운 기준을 제시했다. 동시에 NMS-free End-to-End 파이프라인을 통해 개발 및 배포 과정을 획기적으로 단순화했다. 최고의 정확도가 필요하고, 고성능 GPU 자원을 활용할 수 있으며, 복잡한 후처리 로직을 피하고 싶은 환경에서 최적의 솔루션이다.</li>
</ul>
<h3>7.2  사용 사례별 추천 모델</h3>
<p>이러한 특성을 바탕으로, 구체적인 사용 사례에 따른 모델 선정 가이드라인을 다음과 같이 제시한다.</p>
<ul>
<li><strong>엣지 컴퓨팅 / 모바일 디바이스:</strong></li>
<li><strong>추천 모델:</strong> <strong>YOLOv9-S/T</strong> 또는 <strong>YOLOv8n</strong></li>
<li><strong>이유:</strong> 연산량(FLOPs)과 모델 크기(Parameters)가 가장 중요한 제약 조건인 환경이다. YOLOv9은 동일 연산량 대비 최고의 정확도를 제공하므로, YOLOv9의 경량 모델(S, T)이 가장 효율적인 선택이다. YOLOv8n 역시 매우 가볍고 검증된 성능을 제공하는 좋은 대안이다.</li>
<li><strong>클라우드 기반 실시간 분석 서비스 (예: 지능형 CCTV, AI 비디오 분석):</strong></li>
<li><strong>추천 모델:</strong> <strong>RF-DETR-M/L</strong></li>
<li><strong>이유:</strong> 높은 처리량(Throughput)과 함께 최고의 정확도가 동시에 요구되는 환경이다. 고성능 GPU 자원을 활용할 수 있으므로, RF-DETR의 대형 모델이 제공하는 압도적인 정확도(60+ mAP)와 NMS-free의 안정적인 파이프라인이 가장 큰 가치를 제공한다.</li>
<li><strong>범용 고성능 데스크톱 애플리케이션:</strong></li>
<li><strong>추천 모델:</strong> <strong>YOLOv9-C</strong> 또는 <strong>YOLOv8l</strong></li>
<li><strong>이유:</strong> 대부분의 일반적인 시나리오에서 적절한 속도와 높은 정확도 사이의 균형이 필요하다. YOLOv9-C는 YOLOv8l과 유사하거나 더 적은 연산량으로 더 높은 정확도를 제공하므로, 효율성 측면에서 더 나은 선택이 될 수 있다.</li>
<li><strong>새로운 도메인으로의 빠른 전이학습 (예: 의료 영상, 위성 이미지 분석):</strong></li>
<li><strong>추천 모델:</strong> <strong>RF-DETR-B/S</strong></li>
<li><strong>이유:</strong> 학습 데이터가 부족하고 도메인 특수성이 강한 경우, 모델의 일반화 성능이 매우 중요하다. 강력한 사전 학습 백본(DINOv2)을 사용하고 전역적 컨텍스트 이해 능력이 뛰어난 RF-DETR이 새로운 도메인에 더 빠르고 효과적으로 적응할 가능성이 높다.</li>
</ul>
<h3>7.3  향후 전망</h3>
<p>YOLOv8, YOLOv9, RF-DETR의 등장은 실시간 객체 탐지 분야의 미래가 나아갈 방향에 대한 몇 가지 중요한 시사점을 던진다.</p>
<p>첫째, <strong>CNN과 Transformer의 융합은 가속화될 것이다.</strong> RF-DETR이 CNN 백본을 사용하고, 차세대 YOLO 모델들이 어텐션 메커니즘을 도입하는 것처럼, 두 아키텍처의 장점을 결합한 하이브리드 구조가 대세가 될 것이다.</p>
<p>둘째, <strong>정보 이론 기반의 학습 방법론이 부상할 것이다.</strong> YOLOv9의 PGI가 보여주었듯이, 단순히 네트워크 구조를 복잡하게 만드는 것을 넘어, 정보 흐름을 제어하고 학습 과정을 최적화하는 연구가 모델 성능 향상의 새로운 돌파구가 될 것이다.</p>
<p>셋째, <strong>NMS-free가 실시간 탐지의 표준으로 자리 잡을 것이다.</strong> RF-DETR이 증명한 End-to-End 파이프라인의 단순성과 안정성은 매우 매력적이다. YOLO 계열 역시 이 방향으로 진화하고 있으며, 머지않아 NMS는 레거시 기술이 될 가능성이 높다.</p>
<p>마지막으로, 모델 평가의 기준은 다변화될 것이다. MS COCO 데이터셋에서의 mAP 경쟁은 점차 포화 상태에 이르고 있다. 앞으로는 RF100-VL 벤치마크처럼 다양한 실제 환경에서의 <strong>도메인 일반화 능력</strong>과 **강건성(Robustness)**을 측정하는 것이 모델의 진정한 가치를 평가하는 더 중요한 척도가 될 것이다. 이러한 변화 속에서 차세대 객체 탐지 모델들은 더 빠르고, 더 정확하며, 더 똑똑한 방향으로 끊임없이 진화할 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision - arXiv, https://arxiv.org/html/2409.08475v1</li>
<li>RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision - arXiv, https://arxiv.org/html/2409.08475v3</li>
<li>YOLOv9: SOTA Object Detection Model Explained - Encord, https://encord.com/blog/yolov9-sota-machine-learning-object-dection-model/</li>
<li>Introduction to DETR (Detection Transformers): Everything You Need to Know - Lightly AI, https://www.lightly.ai/blog/detr</li>
<li>arXiv:2409.08475v3 [cs.CV] 19 Dec 2024, https://arxiv.org/pdf/2409.08475</li>
<li>Baidu’s RT-DETR: A Vision Transformer-Based Real-Time Object Detector - Ultralytics Docs, https://docs.ultralytics.com/models/rtdetr/</li>
<li>DETRs Beat YOLOs on Real-time Object Detection - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_DETRs_Beat_YOLOs_on_Real-time_Object_Detection_CVPR_2024_paper.pdf</li>
<li>D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution Refinement, https://arxiv.org/html/2410.13842v1</li>
<li>What is YOLOv8? A Complete Guide - Roboflow Blog, https://blog.roboflow.com/what-is-yolov8/</li>
<li>YOLOv8 Object Detection Model: What is, How to Use - Roboflow, https://roboflow.com/model/yolov8</li>
<li>Explore Ultralytics YOLOv8, https://docs.ultralytics.com/models/yolov8/</li>
<li>YOLOv9: Learning What You Want to Learn Using Programmable …, https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04462.pdf</li>
<li>YOLOv9: Advancing the YOLO Legacy - LearnOpenCV, https://learnopencv.com/yolov9-advancing-the-yolo-legacy/</li>
<li>[2402.13616] YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information - arXiv, https://arxiv.org/abs/2402.13616</li>
<li>RF-DETR: A SOTA Real-Time Object Detection Model - Roboflow Blog, https://blog.roboflow.com/rf-detr/</li>
<li>BGF-YOLO: Enhanced YOLOv8 with Multiscale Attentional … - arXiv, https://arxiv.org/pdf/2309.12585</li>
<li>YOLOv5, YOLOv8 and YOLOv10: The Go-To Detectors for Real-time Vision - arXiv, https://arxiv.org/html/2407.02988v1</li>
<li>Accuracy–Efficiency Trade-Off: Optimizing YOLOv8 for Structural Crack Detection - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC12252445/</li>
<li>Loss function explanation · Issue #10465 - GitHub, https://github.com/ultralytics/ultralytics/issues/10465</li>
<li>Explore YOLOv8: Latest in Object Detection Tech - Viso Suite, https://viso.ai/deep-learning/yolov8-guide/</li>
<li>GIoU, CIoU and DIoU: Variants of IoU and how they are better compared to IoU | by Abhishek Jain | Medium, https://medium.com/@abhishekjainindore24/giou-ciou-and-diou-variants-of-iou-and-how-they-are-better-compared-to-iou-4610a015643a</li>
<li>YOLO Loss Function Part 1: SIoU and Focal Loss - LearnOpenCV, https://learnopencv.com/yolo-loss-function-siou-focal-loss/</li>
<li>YOLO Loss Function Part 2: GFL and VFL Loss - LearnOpenCV, https://learnopencv.com/yolo-loss-function-gfl-vfl-loss/</li>
<li>Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection, https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/gfocal.html</li>
<li>loss - YOLOv8 dfl_loss metric - Stack Overflow, https://stackoverflow.com/questions/75950283/yolov8-dfl-loss-metric</li>
<li>YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information, https://arxiv.org/html/2402.13616v2</li>
<li>YOLOv9: A Leap Forward in Object Detection Technology - Ultralytics Docs, https://docs.ultralytics.com/models/yolov9/</li>
<li>YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information, https://arxiv.org/html/2402.13616v1</li>
<li>DETR - Hugging Face, https://huggingface.co/docs/transformers/model_doc/detr</li>
<li>Deformable DETR - arXiv, https://arxiv.org/pdf/2010.04159</li>
<li>Paper Review: Deformable Transformers for End-to-End Object Detection., https://cenk-bircanoglu.medium.com/paper-review-deformable-transformers-for-end-to-end-object-detection-ed0a452f775f</li>
<li>RF-DETR Object Detection vs YOLOv12 : A Study of Transformer-based and CNN-based Architectures for Single-Class and Multi-Class Greenfruit Detection in Complex Orchard Environments Under Label Ambiguity - arXiv, https://arxiv.org/html/2504.13099v1</li>
<li>RF-DETR: Real-Time Object Detection with Transformers | DigitalOcean, https://www.digitalocean.com/community/tutorials/rf-detr-real-time-object-detection</li>
<li>[2409.06443] Knowledge Distillation via Query Selection for Detection Transformer - arXiv, https://arxiv.org/abs/2409.06443</li>
<li>Real-Time Beach Litter Detection and Counting: A Comparative Analysis of RT-DETR Model Variants - arXiv, https://arxiv.org/html/2508.13101v1</li>
<li>RT-DETR Object Detection Model: What is, How to Use - Roboflow, https://roboflow.com/model/rt-detr</li>
<li>DETRs Beat YOLOs on Real-time Object Detection, https://arxiv.org/abs/2304.08069</li>
<li>Context-Aware Enhanced Feature Refinement for small object detection with Deformable DETR - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC12185399/</li>
<li>DAB-DETR: DYNAMIC ANCHOR BOXES - OpenReview, https://openreview.net/pdf?id=oMI9PjOb9Jl</li>
<li>anasch07/DETR-Object-Detection: An end-to-end object detection model using Transformers - GitHub, https://github.com/anasch07/DETR-Object-Detection</li>
<li>End-to-End Object Detection with Transformers, https://arxiv.org/abs/2005.12872</li>
<li>A Novel Compression Framework for YOLOv8: Achiev- ing Real-Time Aerial Object Detection on Edge Devices via Structured Pruning a - arXiv, https://arxiv.org/pdf/2509.12918</li>
<li>WongKinYiu/yolov9: Implementation of paper - YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information - GitHub, https://github.com/WongKinYiu/yolov9</li>
<li>Performance Benchmark of YOLO v5, v7 and v8 - Stereolabs, https://www.stereolabs.com/blog/performance-of-yolo-v5-v7-and-v8</li>
<li>RF-DETR Github - Kaggle, https://www.kaggle.com/datasets/dschettler8845/rf-detr-github</li>
<li>Real-World Inference Battle: How RF-DETR Stacks Up Against YOLOv11 | by Erfan Akbarnezhad | Aug, 2025 | Medium, https://medium.com/@akbarnezhad1380/real-world-inference-battle-how-rf-detr-stacks-up-against-yolov11-5500d4357417</li>
<li>Benchmarks - RF-DETR - Roboflow, https://rfdetr.roboflow.com/learn/benchmarks/</li>
<li>Roboflow’s RF-DETR: Bridging Speed and Accuracy in Object Detection - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2025/03/roboflows-rf-detr/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>