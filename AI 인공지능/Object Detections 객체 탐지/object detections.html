<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:객체 탐지(Object Detection)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>객체 탐지(Object Detection)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>객체 탐지(Object Detection)</span></nav>
                </div>
            </header>
            <article>
                <h1>객체 탐지(Object Detection)</h1>
<h2>1.  객체 탐지(Object Detection)의 정의와 핵심 원리</h2>
<h3>1.1  객체 탐지의 개념: ’무엇’과 ’어디’를 찾는 기술</h3>
<p>객체 탐지(Object Detection)는 컴퓨터 비전(Computer Vision) 분야의 핵심적인 기술 중 하나로, 디지털 이미지나 비디오 스트림 내에서 특정 의미론적 범주(semantic class)에 속하는 객체 인스턴스(instance)의 존재를 식별하고, 그 위치를 정확하게 찾아내는 것을 목표로 한다.1 이 기술은 인간의 시각 시스템이 복잡한 환경 속에서도 수많은 객체를 거의 즉각적으로 인식하고 그 위치를 파악하는 고도의 지능적 능력을 기계적으로 복제하려는 시도에서 출발한다.1</p>
<p>객체 탐지는 단순히 이미지 전체를 하나의 레이블로 분류하는 과업을 넘어서, 이미지 내에 ’무엇’이 존재하는지(분류, Classification)와 그것이 ’어디’에 위치하는지(위치 측정, Localization)라는 두 가지 근본적인 질문에 동시에 답해야 하는 복합적인 과업이다.4 예를 들어, 도로를 촬영한 이미지에서 객체 탐지 시스템은 ’자동차’가 있다는 사실뿐만 아니라, 이미지의 어느 좌표에 ’자동차’가 있는지, 또 다른 좌표에는 ’보행자’가 있고, 또 다른 곳에는 ’신호등’이 있다는 식으로 다수의 객체에 대한 종류와 위치 정보를 모두 제공해야 한다. 이처럼 객체 탐지는 이미지에 대한 깊이 있는 이해를 제공함으로써, 기계가 주변 환경과 상호작용할 수 있는 기반을 마련하는 중추적인 역할을 수행한다.</p>
<h3>1.2  핵심 구성 요소: 경계 상자(Bounding Box)와 신뢰도 점수(Confidence Score)</h3>
<p>객체 탐지 모델의 최종 출력은 탐지된 각 객체에 대한 구체적인 정보를 담고 있으며, 이는 일반적으로 세 가지 핵심 요소로 구성된다: 경계 상자(Bounding Box), 클래스 레이블(Class Label), 그리고 신뢰도 점수(Confidence Score)이다.7</p>
<ul>
<li><strong>경계 상자 (Bounding Box):</strong> 탐지된 객체의 공간적 위치와 크기를 정의하는 가장 보편적인 방법으로, 객체를 온전히 감싸는 직사각형으로 표현된다. 이 직사각형의 좌표는 주로 두 가지 형식으로 나타낸다. 첫 번째는 경계 상자의 중심점 좌표와 너비, 높이를 나타내는 <code>(x, y, w, h)</code> 형식이며, 두 번째는 좌측 상단 모서리 좌표와 우측 하단 모서리 좌표를 나타내는 <code>(x_min, y_min, x_max, y_max)</code> 형식이다.6 모델은 객체의 실제 경계와 가장 근접한 경계 상자를 예측하도록 학습된다.</li>
<li><strong>클래스 레이블 (Class Label):</strong> 해당 경계 상자 내에 존재하는 객체가 어떤 사전 정의된 범주에 속하는지를 명시하는 식별자이다.7 예를 들어, ‘사람’, ‘자동차’, ‘고양이’, ‘정지 표지판’ 등이 클래스 레이블이 될 수 있다. 다중 클래스 탐지 모델은 수십에서 수백 개에 이르는 다양한 클래스를 구분할 수 있다.</li>
<li><strong>신뢰도 점수 (Confidence Score):</strong> 모델이 해당 경계 상자 내에 특정 클래스의 객체가 존재한다고 얼마나 확신하는지를 나타내는 0과 1 사이의 확률 값이다.7 이 점수는 모델 예측의 불확실성을 정량화한 것으로, 일반적으로 신뢰도 점수가 특정 임계값(threshold) 이상인 예측 결과만을 최종 탐지 결과로 채택하는 후처리 과정에 사용된다. 이를 통해 신뢰도가 낮은 불필요한 탐지(false positives)를 효과적으로 제거할 수 있다.</li>
</ul>
<h3>1.3  컴퓨터 비전 내 다른 과업과의 비교 분석: 분류, 위치 측정, 분할</h3>
<p>객체 탐지의 고유한 특성과 기술적 난이도를 명확히 이해하기 위해서는, 이와 밀접하게 관련된 다른 컴퓨터 비전 과업들과의 관계를 체계적으로 비교 분석하는 것이 필수적이다. 이미지 분류, 객체 위치 측정, 이미지 분할 등은 시각적 인식의 복잡도와 정밀도 측면에서 계층적인 관계를 형성하며, 객체 탐지는 이 계층 구조의 중요한 한 축을 담당한다.</p>
<ul>
<li><strong>이미지 분류 (Image Classification):</strong> 컴퓨터 비전의 가장 기본적인 과업으로, 입력된 이미지 전체에 대해 단 하나의 클래스 레이블을 할당하는 것을 목표로 한다.4 예를 들어, 고양이 한 마리가 있는 사진을 입력하면 모델은 ’고양이’라는 단일 레이블을 출력한다. 이 과업은 이미지에 ’무엇’이 있는지는 알려주지만, 그 객체의 위치나 이미지 내에 존재하는 객체의 개수에 대한 정보는 전혀 제공하지 않는다.6</li>
<li><strong>객체 위치 측정 (Object Localization):</strong> 이미지 분류에서 한 단계 더 나아간 과업으로, 이미지 내에 <em>단일</em> 객체가 존재한다고 가정하고 그 객체의 클래스를 분류함과 동시에, 해당 객체의 위치를 경계 상자로 표시하는 것을 목표로 한다.4 즉, ’무엇’과 ’어디’에 대한 질문에 답하지만, 그 대상이 이미지 내의 주된 단일 객체로 한정된다.</li>
<li><strong>객체 탐지 (Object Detection):</strong> 객체 위치 측정의 일반화된 형태로, 이미지 내에 존재할 수 있는 <em>다수의</em> 객체들을 각각 찾아내고, 각각의 객체에 대해 클래스 분류와 경계 상자 위치 측정을 모두 수행한다.3 이는 이미지에 여러 종류의 객체가 혼재하는 현실 세계의 복잡한 시나리오를 다룰 수 있게 해주는 핵심적인 차이점이다.</li>
<li><strong>이미지 분할 (Image Segmentation):</strong> 객체의 위치를 직사각형의 경계 상자로 근사하는 것을 넘어, 픽셀 단위로 객체의 정확한 모양과 영역을 식별하는 가장 정밀한 수준의 인식 과업이다.4 이미지 분할은 다시 목표에 따라 두 가지 주요 유형으로 나뉜다.</li>
<li><strong>시맨틱 분할 (Semantic Segmentation):</strong> 이미지 내의 모든 픽셀을 특정 클래스(예: ‘하늘’, ‘도로’, ‘건물’, ‘사람’)로 분류한다. 이 방식의 핵심 특징은 같은 클래스에 속하는 개별 객체 인스턴스들을 구분하지 않는다는 점이다.5 예를 들어, 이미지에 세 명의 사람이 있다면, 시맨틱 분할은 모든 사람에 해당하는 픽셀을 동일하게 ’사람’이라는 레이블로 칠할 뿐, 세 명을 별개의 개체로 인식하지 않는다.</li>
<li><strong>인스턴스 분할 (Instance Segmentation):</strong> 시맨틱 분할에서 한 걸음 더 나아가, 같은 클래스에 속하더라도 물리적으로 구분되는 개별 객체 인스턴스를 각각 식별하여 분할한다.1 즉, 이미지 내의 세 명의 사람을 각각 ‘사람1’, ‘사람2’, ’사람3’으로 구분하여 픽셀 단위 마스크(mask)를 생성한다. 이는 객체 탐지(개별 객체 식별)와 시맨틱 분할(픽셀 단위 영역화)이 결합된 가장 복잡하고 정교한 형태의 시각 인식 과업으로 간주된다.17</li>
</ul>
<p>이러한 과업들의 관계는 시각 정보 처리의 ’정밀도(granularity)’가 점진적으로 증가하는 과정으로 이해할 수 있다. 이미지 분류는 가장 거시적인 수준에서 “이미지에 무엇이 있는가?“에 답한다. 객체 탐지는 여기서 더 나아가 “여러 객체들이 각각 무엇이며 어디에 있는가?“라는 질문에 답하며 공간적 정보를 추가한다. 마지막으로 인스턴스 분할은 “각 객체의 정확한 형태는 무엇인가?“라는 질문에 답하며 가장 미시적이고 정밀한 수준의 이해를 제공한다. 이러한 정밀도의 발전은 단순히 학문적 호기심의 결과가 아니라, 자율 주행, 의료 영상 분석 등 점점 더 복잡하고 정교한 수준의 시각적 이해를 요구하는 현실 세계의 응용 분야 수요에 의해 추동된 필연적인 기술 진화의 과정이다. 예를 들어, 단순한 사진 태깅 시스템은 이미지 분류만으로 충분할 수 있지만, 자율 주행 차량은 전방의 보행자를 단순히 상자로 감지하는 것을 넘어, 그 정확한 윤곽과 자세를 파악(분할)해야만 더욱 안전한 예측과 판단을 내릴 수 있다. 이처럼 컴퓨터 비전 과업의 정밀도 수준은 해결하고자 하는 문제의 복잡도와 직접적으로 연관된다.</p>
<p>아래 표는 이러한 주요 컴퓨터 비전 과업들의 목표와 출력을 체계적으로 비교하여 정리한 것이다.</p>
<table><thead><tr><th>과업 (Task)</th><th>목표 (Goal)</th><th>출력 (Output)</th></tr></thead><tbody>
<tr><td>이미지 분류 (Image Classification)</td><td>이미지 전체에 대한 단일 클래스 예측</td><td>클래스 레이블 1개 (e.g., ‘고양이’)</td></tr>
<tr><td>객체 위치 측정 (Object Localization)</td><td>이미지 내 단일 객체의 클래스와 위치 예측</td><td>클래스 레이블 1개 + 경계 상자 1개</td></tr>
<tr><td><strong>객체 탐지 (Object Detection)</strong></td><td>이미지 내 다수 객체의 클래스와 위치 예측</td><td>다수의 [클래스 레이블 + 경계 상자] 쌍</td></tr>
<tr><td>시맨틱 분할 (Semantic Segmentation)</td><td>이미지의 모든 픽셀을 클래스로 분류</td><td>픽셀 단위의 클래스 맵 (인스턴스 구분 없음)</td></tr>
<tr><td>인스턴스 분할 (Instance Segmentation)</td><td>이미지의 모든 픽셀을 개별 객체 인스턴스로 분류 및 분할</td><td>픽셀 단위의 인스턴스 맵 (개별 객체 구분)</td></tr>
</tbody></table>
<h2>2.  객체 탐지 알고리즘의 발전 과정</h2>
<p>객체 탐지 기술은 지난 수십 년간 괄목할 만한 발전을 이루어왔다. 특히 딥러닝의 등장은 이 분야의 패러다임을 완전히 바꾸어 놓았다. 이 장에서는 딥러닝 이전의 전통적인 접근법부터 시작하여, 현대 객체 탐지 기술의 양대 산맥을 이루는 2단계 및 1단계 탐지기, 그리고 최근 새로운 가능성을 제시하고 있는 트랜스포머 기반 탐지기에 이르기까지의 기술적 진화 과정을 체계적으로 고찰한다.</p>
<h3>2.1  전통적 머신러닝 기법: 딥러닝 이전의 접근법</h3>
<p>딥러닝이 컴퓨터 비전 분야를 지배하기 이전, 객체 탐지는 주로 ’특징 추출(feature extraction)’과 ’분류(classification)’라는 두 단계의 파이프라인으로 구성되었다. 이 접근법의 핵심은 연구자가 직접 특정 객체를 잘 표현할 수 있는 시각적 특징(hand-crafted features)을 고안하고, 이를 머신러닝 분류기에 입력하여 객체 여부를 판별하는 방식이었다.1</p>
<ul>
<li><strong>Viola-Jones 알고리즘:</strong> 2001년에 제안된 이 알고리즘은 실시간 얼굴 탐지를 가능하게 하여 객체 탐지 분야에 큰 파장을 일으켰다. 이는 간단한 흑백 사각형 패턴의 픽셀 값 차이를 이용하는 ’하르 유사 특징(Haar-like features)’과, 이들 중 효과적인 특징들만을 빠르게 선택하고 결합하는 ‘에이다부스트(AdaBoost)’ 학습 알고리즘, 그리고 탐지 속도를 획기적으로 높인 ‘관심 영역 계단식 분류기(Cascade of Classifiers)’ 구조를 통해 높은 효율성을 달성했다.1</li>
<li><strong>HOG (Histogram of Oriented Gradients)와 SVM (Support Vector Machine):</strong> HOG는 이미지의 지역적 영역에서 픽셀의 그래디언트(gradient) 방향과 크기 분포를 히스토그램으로 만들어 특징 벡터를 생성하는 기법이다. 이는 객체의 외곽선과 형태 정보를 효과적으로 포착할 수 있어, 특히 사람과 같은 비정형 객체 탐지에 강점을 보였다. 이렇게 추출된 HOG 특징은 주로 선형 SVM(Support Vector Machine) 분류기와 결합되어 보행자 탐지 분야에서 당시 최고의 성능을 기록했다.1</li>
</ul>
<p>이러한 전통적인 방법들은 특정 도메인에서는 상당한 성공을 거두었으나, 근본적인 한계를 내포하고 있었다. 가장 큰 문제는 특징 추출 과정이 전적으로 인간의 직관과 전문 지식에 의존한다는 점이었다. 이는 새로운 종류의 객체를 탐지하고자 할 때마다 해당 객체에 맞는 새로운 특징을 설계해야 하는 번거로움을 야기했으며, 조명 변화, 자세 변화, 가려짐 등 다양한 현실 세계의 변수에 대해 강건한 특징을 설계하는 것이 매우 어려웠다. 딥러닝 기반 모델들은 바로 이 특징 추출 과정을 데이터로부터 자동으로 학습(feature learning)함으로써, 이러한 한계를 극복하고 객체 탐지 기술의 비약적인 발전을 이끌었다.1</p>
<h3>2.2  2단계 탐지기(Two-Stage Detectors): R-CNN 계열의 진화</h3>
<p>딥러닝 시대의 객체 탐지는 ’2단계 탐지기(Two-Stage Detector)’의 등장과 함께 본격적으로 시작되었다. 이 접근법은 이름에서 알 수 있듯이 탐지 과정을 두 개의 주요 단계로 나눈다. 첫 번째 단계에서는 객체가 존재할 가능성이 높은 후보 영역(region proposal)을 이미지에서 찾아내고, 두 번째 단계에서는 각 후보 영역에 대해 정밀한 분류와 위치 보정을 수행한다. 이러한 구조는 높은 정확도를 달성하는 데 유리하지만, 구조적 복잡성으로 인해 속도가 상대적으로 느린 경향이 있다.11 이 계열의 선구자이자 대표적인 모델 군이 바로 R-CNN 계열이다.</p>
<h4>2.2.1  R-CNN (Regions with CNN features): 딥러닝 기반 탐지의 서막</h4>
<p>R-CNN은 2014년에 제안되어 딥러닝을 객체 탐지 문제에 성공적으로 적용한 최초의 모델로 평가받는다. 이는 전통적인 컴퓨터 비전의 영역 제안 방식과 딥러닝의 강력한 특징 표현 능력을 결합한 하이브리드 접근법이었다.18</p>
<ul>
<li><strong>작동 방식:</strong> R-CNN의 파이프라인은 다음과 같은 4개의 독립적인 모듈로 구성된다.</li>
</ul>
<ol>
<li><strong>영역 제안 (Region Proposal):</strong> 입력 이미지에 대해 ’선택적 탐색(Selective Search)’과 같은 전통적인 컴퓨터 비전 알고리즘을 사용하여 약 2,000개의 객체 후보 영역(Region of Interest, RoI)을 생성한다.19</li>
<li><strong>특징 추출 (Feature Extraction):</strong> 생성된 2,000개의 후보 영역 각각을 CNN(당시에는 AlexNet)의 입력 크기(예: 227x227 픽셀)에 맞게 강제로 변형(warp)하거나 잘라낸다. 이후, 사전 훈련된 CNN에 각 영역을 개별적으로 입력하여 4096차원의 고정 길이 특징 벡터를 추출한다.19</li>
<li><strong>분류 (Classification):</strong> 추출된 특징 벡터를 각 클래스별로 독립적으로 훈련된 선형 SVM 분류기에 입력하여 해당 영역이 특정 객체 클래스에 속하는지, 아니면 배경인지를 판별한다.19</li>
<li><strong>경계 상자 회귀 (Bounding Box Regression):</strong> 객체로 분류된 영역에 한해, 별도로 훈련된 선형 회귀 모델을 사용하여 선택적 탐색이 제안한 경계 상자의 위치와 크기를 더욱 정밀하게 조정한다.19</li>
</ol>
<ul>
<li><strong>의의와 한계:</strong> R-CNN은 PASCAL VOC와 같은 표준 벤치마크 데이터셋에서 기존의 모든 기록을 경신하며 딥러닝의 압도적인 성능을 입증했다. 하지만 이 모델은 치명적인 단점을 안고 있었다. 약 2,000개의 후보 영역 각각에 대해 독립적으로 CNN 특징 추출 연산을 수행해야 했기 때문에, 이미지 한 장을 처리하는 데 수십 초가 걸리는 등 추론 속도가 극도로 느렸다.19 또한, 영역 제안, 특징 추출, 분류, 경계 상자 회귀가 모두 별개의 모듈로 분리되어 있어 전체 시스템을 한 번에 최적화하는 종단간(End-to-End) 학습이 불가능했다.</li>
</ul>
<h4>2.2.2  Fast R-CNN: 특징 공유를 통한 속도 혁신</h4>
<p>Fast R-CNN은 R-CNN의 근본적인 문제인 속도 저하를 해결하기 위해 제안되었다. 이 모델의 핵심 아이디어는 후보 영역별로 중복되는 CNN 연산을 제거하고, 이미지 전체에 대해 단 한 번만 CNN 연산을 수행하여 생성된 특징 맵(feature map)을 모든 후보 영역이 공유하도록 하는 것이다.19</p>
<ul>
<li><strong>작동 방식:</strong></li>
</ul>
<ol>
<li>입력 이미지 전체를 CNN에 한 번 통과시켜 고해상도의 컨볼루션 특징 맵을 생성한다.</li>
<li>R-CNN과 마찬가지로 선택적 탐색 알고리즘을 사용하여 RoI를 생성하고, 이 RoI의 좌표를 앞서 계산된 특징 맵 상의 위치로 투영(projection)한다.</li>
<li><strong>RoI 풀링 계층 (RoI Pooling Layer):</strong> Fast R-CNN의 핵심적인 혁신으로, 투영된 각 RoI 영역으로부터 고정된 크기(예: 7x7)의 특징 벡터를 추출하는 새로운 계층이다. 이 계층은 다양한 크기와 종횡비를 가진 RoI들을 후속 완전 연결 계층(Fully Connected Layer)이 처리할 수 있는 표준화된 형태로 변환해준다.19</li>
<li>RoI 풀링을 거친 특징 벡터는 FC 레이어를 통과한 후, 두 개의 분기된 출력 계층으로 전달된다. 하나는 소프트맥스 분류기(softmax classifier)로 객체의 클래스를 예측하고, 다른 하나는 경계 상자 회귀기(bounding box regressor)로 위치를 미세 조정한다. 이 두 과업은 다중 과업 손실(multi-task loss)을 통해 단일 네트워크 내에서 동시에 학습된다.6</li>
</ol>
<ul>
<li><strong>개선점:</strong> 특징 맵 공유라는 단순하지만 강력한 아이디어를 통해 Fast R-CNN은 R-CNN 대비 훈련 속도를 약 9배, 추론 속도를 약 213배 향상시키는 극적인 성능 개선을 이루었다.24 또한, SVM을 소프트맥스 분류기로 대체하고 전체 네트워크(영역 제안 제외)를 종단간으로 학습시킴으로써 정확도(mAP) 또한 소폭 향상되었다. 그러나, 여전히 CPU 기반의 느린 선택적 탐색 알고리즘에 의존하여 후보 영역을 생성했기 때문에, 이 영역 제안 단계가 전체 파이프라인의 새로운 속도 병목으로 작용하게 되었다.19</li>
</ul>
<h4>2.2.3  Faster R-CNN: 영역 제안 네트워크(RPN)의 도입</h4>
<p>Faster R-CNN은 Fast R-CNN이 남긴 마지막 숙제, 즉 외부 알고리즘에 의존하는 느린 영역 제안 문제를 해결하기 위해 등장했다. 이 모델의 혁신은 영역 제안 과정 자체를 데이터로부터 학습 가능한 심층 신경망으로 대체한 데 있으며, 이를 **영역 제안 네트워크(Region Proposal Network, RPN)**라고 명명했다.19</p>
<ul>
<li><strong>작동 방식:</strong></li>
</ul>
<ol>
<li>이전 모델들과 마찬가지로, 입력 이미지 전체에 대한 공유 컨볼루션 특징 맵을 생성한다.</li>
<li><strong>RPN:</strong> 이 특징 맵 위에 RPN이 적용된다. RPN은 작은 크기의 신경망으로, 특징 맵 위를 슬라이딩 윈도우(sliding window) 방식으로 훑고 지나가며 각 위치에서 객체가 있을 확률(objectness score)과 경계 상자 좌표를 직접 예측한다.19</li>
<li><strong>앵커 박스 (Anchor Boxes):</strong> RPN은 다양한 형태의 객체를 효율적으로 제안하기 위해 ’앵커 박스’라는 개념을 도입했다. 앵커 박스는 슬라이딩 윈도우의 중심에 위치하는, 사전에 정의된 다양한 크기(scale)와 종횡비(aspect ratio)를 가진 기준 상자들이다. 예를 들어, 3개의 크기와 3개의 종횡비를 조합하여 각 위치마다 총 9개의 앵커 박스를 사용한다. RPN은 이 9개의 앵커 박스 각각에 대해 객체일 확률과, 앵커 박스로부터의 상대적인 좌표 조정값(offset)을 예측한다.26</li>
<li>RPN이 생성한 높은 객체 확률을 가진 후보 영역(proposals)들은 비최대 억제(Non-Maximum Suppression, NMS)를 거쳐 일부가 선택된 후, Fast R-CNN의 탐지 네트워크(detection network)로 전달된다. 탐지 네트워크는 RoI 풀링을 통해 각 후보 영역의 특징을 추출하고, 최종적인 클래스 분류와 경계 상자 위치 정밀 조정을 수행한다.</li>
</ol>
<ul>
<li><strong>의의:</strong> Faster R-CNN의 가장 큰 의의는 RPN이 탐지 네트워크와 컨볼루션 특징을 공유한다는 점이다. 이로 인해 영역 제안에 소요되는 추가적인 계산 비용이 거의 없어졌으며(nearly cost-free), 객체 탐지 파이프라인 전체가 GPU 상에서 실행되는 단일 통합 네트워크로 구성될 수 있었다.19 이는 진정한 의미의 종단간 학습을 가능하게 했고, VGG-16 모델 기준 5 FPS의 실시간에 가까운 속도와 당시 최고 수준의 정확도를 동시에 달성하는 쾌거를 이루었다.31 Faster R-CNN은 현대 2단계 탐지기의 사실상 표준 아키텍처로 자리매김하며 이후 수많은 연구의 기반이 되었다.</li>
</ul>
<p>R-CNN에서 Faster R-CNN에 이르는 발전 과정은 딥러닝 시스템의 진화에 대한 중요한 통찰을 제공한다. 이 과정은 본질적으로 분리되고, 학습 불가능하며, 전통적인 방식에 의존했던 구성 요소들을 점진적으로 단일화된 종단간 딥러닝 프레임워크 안으로 통합해 온 역사라 할 수 있다. R-CNN은 선택적 탐색(비학습 알고리즘), CNN(특징 추출), SVM(별도의 머신러닝 모델)이라는 세 개의 이질적인 조각으로 이루어진 하이브리드 시스템이었다.19 Fast R-CNN은 SVM을 내부의 소프트맥스 분류기로 대체하고 분류와 회귀를 통합함으로써 첫 번째 주요 통합을 이루었지만, 영역 제안은 여전히 외부의 전처리 단계로 남아 새로운 병목을 만들었다.19 Faster R-CNN은 마지막 남은 외부 요소인 선택적 탐색을 학습 가능한 RPN으로 대체하고, 심지어 탐지 네트워크와 특징까지 공유시킴으로써 전체 파이프라인을 하나의 유기적인 시스템으로 완성했다.19 이러한 통합의 흐름은 딥러닝 모델 개발의 핵심 원칙, 즉 다단계 하이브리드 파이프라인에서 단일 종단간 미분 가능 모델로의 전환이 성능과 효율성을 극대화하는 길임을 명확히 보여준다. 통합은 시스템의 복잡도를 낮추고, 최적화되지 않은 구성 요소로 인한 병목을 제거하며, 전체 시스템이 최종 목표에 대해 전역적으로 최적화될 수 있도록 한다.</p>
<h3>2.3  1단계 탐지기(One-Stage Detectors): 실시간 탐지를 향하여</h3>
<p>2단계 탐지기가 정확도에서 강점을 보이는 동안, 실시간 처리가 필수적인 응용 분야(예: 비디오 감시, 자율 주행)에서는 속도가 더 중요한 요소로 부상했다. 이러한 요구에 부응하여 등장한 것이 ’1단계 탐지기(One-Stage Detector)’이다. 1단계 탐지기는 2단계 탐지기의 특징인 별도의 후보 영역 제안 단계를 생략하고, 이미지 전체를 한 번의 네트워크 순전파(single pass) 과정에서 객체의 위치와 클래스를 동시에 예측한다. 이러한 구조적 단순함 덕분에 1단계 탐지기는 매우 빠른 속도를 자랑하지만, 초기 모델들은 2단계 탐지기에 비해 정확도가 다소 떨어지는 경향을 보였다.8</p>
<h4>2.3.1  YOLO (You Only Look Once): 탐지를 회귀 문제로 재정의</h4>
<p>2016년에 등장한 YOLO는 객체 탐지 문제를 완전히 새로운 관점에서 접근하여 1단계 탐지기의 시대를 열었다. YOLO의 핵심 철학은 객체 탐지를 복잡한 다단계 파이프라인이 아닌, 이미지 픽셀로부터 경계 상자 좌표와 클래스 확률을 직접 예측하는 단일 회귀 문제(single regression problem)로 재구성한 것이다.33</p>
<ul>
<li><strong>작동 방식:</strong></li>
</ul>
<ol>
<li>먼저 입력 이미지를 일정한 크기의 <code>S x S</code> 그리드(grid)로 분할한다. 예를 들어 7x7 그리드를 사용할 수 있다.</li>
<li>어떤 객체의 실제 중심점(ground-truth center)이 특정 그리드 셀 내에 위치하면, 해당 그리드 셀이 그 객체를 탐지할 책임을 맡게 된다.33</li>
<li>각 그리드 셀은 두 가지 정보를 예측한다: (1) <code>B</code>개의 경계 상자와 각 상자에 대한 신뢰도 점수(confidence score). 각 경계 상자는 위치 및 크기 정보(<code>x, y, w, h</code>) 4개와 신뢰도 점수 1개를 포함한다. (2) <code>C</code>개의 클래스에 대한 조건부 확률(conditional class probabilities). 이는 해당 셀에 객체가 존재한다는 가정 하에, 그 객체가 각 클래스에 속할 확률을 의미한다.33</li>
<li>추론 시, 각 그리드 셀이 예측한 클래스 확률과 각 경계 상자의 신뢰도 점수를 곱하여, 각 경계 상자별 최종 클래스 신뢰도 점수를 계산한다. 이후 비최대 억제(NMS)를 적용하여 중복된 탐지를 제거하고 최종 결과를 도출한다.</li>
</ol>
<ul>
<li><strong>장단점:</strong> YOLO의 가장 큰 장점은 이름 그대로 ‘한 번만 보기’ 때문에 달성할 수 있는 압도적인 처리 속도이다. 기본 모델은 초당 45 프레임(FPS), 경량화된 Fast YOLO 모델은 155 FPS라는 경이로운 속도를 기록하여, 실시간 비디오 처리를 현실화했다.33 또한, 이미지 전체를 한 번에 처리하기 때문에 객체와 그 주변의 전역적인 문맥(global context) 정보를 예측에 활용할 수 있다. 이는 후보 영역만 보는 2단계 탐지기에 비해 배경을 객체로 잘못 탐지하는 오류(background errors)를 줄이는 데 기여했다.33</li>
</ul>
<p>하지만 초기 YOLO 모델은 명확한 한계점을 가지고 있었다. 그리드 셀 단위로 예측하는 구조적 제약 때문에, 하나의 셀에 여러 개의 작은 객체가 밀집해 있거나(예: 새 떼), 비정상적인 종횡비를 가진 객체를 탐지하는 데 어려움을 겪었다. 결과적으로 2단계 탐지기에 비해 위치 정확도(localization accuracy)가 상대적으로 낮았다.33</p>
<h4>2.3.2  SSD (Single Shot MultiBox Detector): 다중 스케일 특징 맵과 기본 박스</h4>
<p>SSD는 YOLO의 빠른 속도를 유지하면서도 R-CNN 계열의 높은 정확도를 달성하고자 하는 목표로 개발되었다. 이를 위해 SSD는 두 가지 핵심적인 아이디어를 도입했다: 다양한 크기의 객체에 대응하기 위한 ’다중 스케일 특징 맵(multi-scale feature maps)’과, 다양한 형태의 객체를 예측하기 위한 ’기본 박스(default boxes)’이다.37</p>
<ul>
<li><strong>작동 방식:</strong></li>
</ul>
<ol>
<li>SSD는 VGG-16과 같은 잘 알려진 이미지 분류 네트워크를 기본 네트워크(base network)로 사용한다. 이 기본 네트워크는 이미지의 저수준 특징을 추출하는 역할을 한다.</li>
<li>기본 네트워크의 뒷부분에 여러 개의 컨볼루션 레이어를 추가로 쌓는다. 이 추가 레이어들은 점진적으로 특징 맵의 공간적 해상도를 줄여나간다.</li>
<li><strong>다중 스케일 예측 (Multi-scale Prediction):</strong> SSD의 가장 큰 특징은 단일 최종 특징 맵에서만 예측을 수행하는 YOLO와 달리, 네트워크의 여러 다른 깊이에 위치한 특징 맵들에서 동시에 객체 탐지를 수행한다는 점이다. 상대적으로 해상도가 높은 네트워크 초반부의 특징 맵에서는 작은 객체를 탐지하고, 해상도가 낮아지는 후반부의 특징 맵에서는 큰 객체를 탐지한다. 이로써 다양한 크기의 객체에 효과적으로 대응할 수 있다.37</li>
<li><strong>기본 박스 (Default Boxes):</strong> SSD는 Faster R-CNN의 앵커 박스와 유사한 개념인 ’기본 박스’를 사용한다. 각기 다른 스케일의 특징 맵에 있는 모든 셀(위치)마다, 사전에 정의된 다양한 종횡비와 크기를 가진 기본 박스들을 할당한다. 네트워크는 각 기본 박스에 대해, 실제 객체와의 위치 차이를 나타내는 오프셋(offset) 4개와, 각 클래스에 대한 신뢰도 점수를 예측한다.37</li>
</ol>
<ul>
<li><strong>의의:</strong> SSD는 1단계 탐지기 프레임워크 내에서 속도와 정확도 간의 균형을 성공적으로 맞춘 모델로 평가받는다. 영역 제안 단계를 제거하여 YOLO처럼 빠른 속도를 유지하면서도, 다중 스케일 특징 맵이라는 계층적 접근법을 통해 YOLO의 주요 약점이었던 작은 객체 탐지 성능을 크게 개선했다.40 SSD의 등장은 1단계 탐지기가 단순히 빠른 것을 넘어, 2단계 탐지기와 견줄 만한 정확도를 가질 수 있다는 가능성을 보여주었으며, 이후 많은 1단계 탐지기 연구에 큰 영향을 미쳤다.</li>
</ul>
<h3>2.4  트랜스포머 기반 탐지기(Transformer-based Detectors): 새로운 패러다임</h3>
<p>자연어 처리(NLP) 분야에서 혁명을 일으킨 트랜스포머(Transformer) 아키텍처는 그 성공에 힘입어 컴퓨터 비전 분야에도 적용되기 시작했다. 객체 탐지 분야에서는 2020년에 발표된 DETR(DEtection TRansformer)이 기존의 CNN 기반 접근법과는 완전히 다른 새로운 패러다임을 제시하며 큰 주목을 받았다.</p>
<h4>2.4.1  DETR (DEtection TRansformer): 집합 예측을 통한 End-to-End 접근법</h4>
<p>DETR의 핵심 철학은 객체 탐지 문제를 ‘직접적인 집합 예측(direct set prediction)’ 문제로 재정의하고, 이를 통해 앵커 박스 생성, 비최대 억제(NMS)와 같이 수작업으로 설계된 복잡한 구성 요소들을 완전히 제거하는 것이다.42</p>
<ul>
<li><strong>작동 방식:</strong> DETR의 아키텍처는 세 가지 주요 부분으로 구성된다.</li>
</ul>
<ol>
<li><strong>CNN 백본 (CNN Backbone):</strong> 기존의 객체 탐지기와 마찬가지로, ResNet과 같은 표준 CNN을 사용하여 입력 이미지로부터 공간적 정보를 담은 2D 특징 맵을 추출한다.</li>
<li><strong>트랜스포머 인코더-디코더 (Transformer Encoder-Decoder):</strong></li>
</ol>
<ul>
<li><strong>인코더 (Encoder):</strong> CNN이 추출한 특징 맵을 1D 시퀀스 형태로 변환하여 입력으로 받는다. 인코더 내의 셀프 어텐션(self-attention) 메커니즘은 이미지의 모든 픽셀(또는 특징) 간의 관계를 계산하여, 이미지의 전역적인 문맥 정보를 풍부하게 담은 특징 표현을 학습한다.42</li>
<li><strong>디코더 (Decoder):</strong> 디코더는 두 종류의 입력을 받는다: 인코더의 출력과 ’객체 쿼리(object queries)’이다. 객체 쿼리는 학습을 통해 특정 객체를 찾아내는 방법을 배우는, 고정된 개수(N)의 학습 가능한 위치 임베딩(positional embedding)이다. 디코더는 이 객체 쿼리들이 인코더의 전역적 이미지 특징과 상호작용(cross-attention)하도록 하여, 각 쿼리가 최종적으로 이미지 내 특정 객체의 존재 여부, 클래스, 그리고 경계 상자 위치를 예측하도록 만든다.42</li>
</ul>
<ol start="3">
<li><strong>예측 헤드와 이분 매칭 손실 (Prediction Heads &amp; Bipartite Matching Loss):</strong> 디코더의 각 출력은 피드포워드 신경망(FFN)을 통과하여 최종 예측(클래스 확률 + 경계 상자 좌표)을 생성한다. DETR은 고정된 N개의 예측 결과 집합을 출력하는데, 이를 실제 정답(ground-truth) 객체 집합과 비교하기 위해 ’이분 매칭 손실(bipartite matching loss)’을 사용한다. 이 손실 함수는 헝가리안 알고리즘(Hungarian algorithm)을 이용하여 예측 결과와 실제 정답 간에 가장 비용이 적은 최적의 일대일 매칭(one-to-one assignment)을 찾는다. 이 매칭을 기반으로 손실을 계산함으로써, 모델은 중복된 예측을 생성하지 않고 각 실제 객체에 대해 단 하나의 유일한 예측을 하도록 강제된다.42</li>
</ol>
<ul>
<li><strong>의의:</strong> DETR의 가장 큰 기여는 NMS나 앵커 박스와 같은, 지난 수년간 객체 탐지 모델 설계의 필수 요소로 여겨졌던 사전 지식 기반의 수동 설계 구성 요소들을 완전히 제거하고, 진정한 의미의 종단간(End-to-End) 학습 모델을 구현했다는 점이다.42 이는 객체 탐지 파이프라인을 극도로 단순화시켰으며, 트랜스포머 아키텍처가 복잡한 비전 문제를 해결할 수 있는 새로운 가능성을 열었다. 하지만 초기 DETR 모델은 상대적으로 수렴 속도가 매우 느리고, 특히 작은 객체에 대한 탐지 성능이 기존의 잘 최적화된 탐지기들보다 낮다는 한계점을 보였다.42</li>
</ul>
<p>객체 탐지 아키텍처의 발전사는 ’사전 지식(priors)’의 활용과 순수한 ‘종단간 학습’ 사이를 오가는 진자 운동과 같다. Faster R-CNN, YOLO, SSD와 같은 모델들은 ’앵커 박스’라는 강력한 사전 지식에 크게 의존한다.29 앵커 박스는 “객체는 일반적으로 특정 크기와 종횡비를 가질 것이다“라는 우리의 지식을 명시적으로 인코딩한 것이다. 마찬가지로, NMS는 “크게 겹치는 여러 개의 탐지 상자는 아마도 동일한 객체를 가리킬 것이다“라는 사전 가정을 기반으로 하는 후처리 단계이다.8 이러한 사전 지식들은 모델이 풀어야 할 문제를 더 쉽게 만들어 학습을 안정시키고 가속화하는 역할을 한다.</p>
<p>그러나 이러한 수동 설계 요소들은 특정 데이터셋에 맞게 세심하게 조정해야 하는 번거로움이 있고, 파이프라인의 복잡도를 높이며, 때로는 최적이 아닐 수 있다. 예를 들어, NMS는 객체가 밀집한 장면에서 올바른 탐지를 실수로 제거할 수 있다. DETR은 이러한 사전 지식으로부터의 과감한 탈피를 시도한다.42 앵커와 NMS를 모두 버리고, 사전 정의된 상자로부터의 상대적 위치를 예측하는 대신 절대적인 상자 좌표를 직접 예측한다. 중복 탐지를 NMS로 제거하는 대신, 이분 매칭 손실을 통해 애초에 중복 없는 예측을 생성하도록 강제한다. 즉, 모델이 앵커나 NMS와 같은 명시적인 가이드 없이 데이터로부터 직접 객체를 찾고 중복을 피하는 방법을 배우도록 하는 것이다.</p>
<p>이러한 접근 방식의 변화는 AI 연구의 더 깊은 철학적 질문, 즉 “모델에 얼마나 많은 인간의 지식을 주입해야 하는가, 그리고 얼마나 많은 것을 데이터로부터 배우게 해야 하는가?“를 반영한다. 사전 지식은 유용한 귀납적 편향(inductive bias)을 제공하여 학습을 도울 수 있지만, 모델의 유연성과 일반화 능력을 제한할 수도 있다. DETR의 접근법은 초기에는 느린 수렴과 작은 객체 탐지 성능 저하라는 어려움을 겪었지만, 인간이 만든 발견법(heuristics)에 덜 의존하고 문제의 근본적인 구조를 직접 학습할 수 있는 미래의 탐지 모델 방향을 제시했다. Deformable DETR 46이나 Sparse DETR 47과 같은 후속 연구들은 DETR의 ‘사전 지식 없는’ 철학을 유지하면서 초기 문제들을 해결하려는 노력의 일환이다.</p>
<h2>3.  객체 탐지 모델의 성능 평가 지표</h2>
<p>객체 탐지 모델의 성능을 정량적으로 평가하는 것은 모델을 비교하고 개선 방향을 설정하는 데 있어 매우 중요하다. 객체 탐지는 단순히 객체를 ’분류’하는 것을 넘어 ’위치’까지 정확하게 맞춰야 하므로, 평가 지표 역시 이 두 가지 측면을 종합적으로 고려해야 한다. 이 장에서는 객체 탐지 모델의 성능을 평가하는 데 사용되는 핵심 지표인 IoU, 정밀도, 재현율, 그리고 이들을 종합한 mAP에 대해 심도 있게 다룬다.</p>
<h3>3.1  IoU (Intersection over Union): 중첩 영역 측정의 기준</h3>
<p>IoU(Intersection over Union)는 예측된 경계 상자(predicted bounding box)와 실제 정답 경계 상자(ground-truth bounding box)가 얼마나 정확하게 겹치는지를 측정하는 가장 기본적인 지표이다.10 이는 자카드 지수(Jaccard Index)라고도 불린다.48</p>
<ul>
<li>
<p><strong>계산 공식:</strong> IoU는 두 경계 상자의 교집합(intersection) 영역의 넓이를 합집합(union) 영역의 넓이로 나눈 값으로 정의된다. 이 값은 항상 0과 1 사이의 범위를 가지며, 1에 가까울수록 두 상자가 정확하게 일치함을 의미하고, 0은 전혀 겹치지 않음을 의미한다.50 예측된 경계 상자를 <span class="math math-inline">B_p</span>, 실제 정답 경계 상자를 <span class="math math-inline">B_{gt}</span>라고 할 때, IoU는 다음과 같이 계산된다.<br />
<span class="math math-display">
IoU(B_p, B_{gt}) = \frac{\text{area}(B_p \cap B_{gt})}{\text{area}(B_p \cup B_{gt})}
</span></p>
</li>
<li>
<p><strong>역할:</strong> IoU의 주된 역할은 모델의 개별 예측이 ’성공’적인지 아닌지를 판단하는 기준, 즉 임계값(threshold)으로 사용되는 것이다. 예를 들어, IoU 임계값을 0.5로 설정했다고 가정하자. 모델이 예측한 경계 상자와 실제 정답 상자 간의 IoU를 계산했을 때, 그 값이 0.5 이상이면 해당 예측은 위치를 올바르게 찾았다고 간주하여 ‘참 긍정(True Positive)’ 후보로 분류한다. 만약 IoU가 0.5 미만이면, 위치를 제대로 찾지 못한 것으로 간주하여 ’거짓 긍정(False Positive)’으로 처리된다.2 이처럼 IoU 임계값은 모델의 위치 정확도에 대한 최소 요구 수준을 설정하는 역할을 한다.</p>
</li>
</ul>
<h3>3.2  정밀도(Precision)와 재현율(Recall): 탐지 정확도와 완전성의 상충 관계</h3>
<p>IoU 임계값을 기준으로 모델의 모든 예측을 평가하고 나면, 각 예측은 다음 세 가지 유형 중 하나로 분류될 수 있다. 이를 기반으로 모델의 전체적인 성능을 나타내는 정밀도와 재현율을 계산할 수 있다.53</p>
<ul>
<li><strong>TP (True Positive, 참 긍정):</strong> 모델이 올바른 클래스의 객체를 올바른 위치(IoU &gt; threshold)에 성공적으로 탐지한 경우.</li>
<li><strong>FP (False Positive, 거짓 긍정):</strong> 모델의 예측이 다음 중 하나에 해당하는 경우:</li>
</ul>
<ol>
<li>실제 객체가 없는 배경을 객체로 잘못 탐지한 경우.</li>
<li>객체의 위치는 올바르게 찾았으나(IoU &gt; threshold), 클래스를 잘못 예측한 경우.</li>
<li>객체의 위치를 제대로 찾지 못한 경우(IoU &lt; threshold).</li>
</ol>
<ul>
<li><strong>FN (False Negative, 거짓 부정):</strong> 모델이 이미지에 실제로 존재하는 객체를 탐지하지 못하고 놓친 경우.</li>
</ul>
<p>이 세 가지 통계를 바탕으로 정밀도와 재현율은 다음과 같이 정의된다.</p>
<ul>
<li>
<p><strong>정밀도 (Precision):</strong> 모델이 ’긍정(Positive)’이라고 예측한 모든 결과 중에서, 실제로 ’참 긍정(True Positive)’인 것의 비율을 의미한다. 이는 “모델이 객체라고 예측한 것들이 얼마나 신뢰할 만한가?” 또는 “모델의 예측이 얼마나 정확한가?“라는 질문에 답한다.52 정밀도가 높다는 것은 거짓 긍정(FP)이 적다는 것을 의미한다.<br />
<span class="math math-display">
\text{Precision} = \frac{TP}{TP + FP}
</span></p>
</li>
<li>
<p><strong>재현율 (Recall):</strong> 이미지에 실제로 존재하는 모든 ‘긍정(Positive)’ 객체들 중에서, 모델이 성공적으로 ’참 긍정(True Positive)’으로 탐지해낸 것의 비율을 의미한다. 이는 “모델이 찾아야 할 객체들을 얼마나 빠짐없이 찾아냈는가?“라는 질문에 답한다.52 재현율이 높다는 것은 거짓 부정(FN), 즉 놓친 객체가 적다는 것을 의미한다.<br />
<span class="math math-display">
\text{Recall} = \frac{TP}{TP + FN}
</span></p>
</li>
<li>
<p><strong>상충 관계 (Trade-off):</strong> 정밀도와 재현율은 일반적으로 상충 관계(trade-off)에 있다. 객체 탐지 모델은 보통 각 예측에 대한 신뢰도 점수를 출력하는데, 이 점수에 대한 임계값을 조절함으로써 정밀도와 재현율을 조절할 수 있다. 신뢰도 임계값을 매우 높게 설정하면, 모델은 아주 확실한 예측만 결과로 내놓게 된다. 이 경우, 거짓 긍정(FP)이 줄어들어 정밀도는 높아지지만, 일부 애매한 실제 객체들을 놓치게 되어(FN 증가) 재현율은 낮아진다. 반대로, 신뢰도 임계값을 낮추면 더 많은 예측을 결과로 내놓아 실제 객체들을 더 많이 찾아낼 수 있으므로(FN 감소) 재현율은 높아지지만, 동시에 배경을 객체로 오인하는 등의 거짓 긍정(FP)도 늘어나 정밀도는 낮아지게 된다.53 따라서 모델의 성능을 하나의 숫자로만 평가하기는 어렵다.</p>
</li>
</ul>
<h3>3.3  AP (Average Precision)와 mAP (mean Average Precision): 표준화된 성능 벤치마크</h3>
<p>정밀도와 재현율의 상충 관계를 고려하여 모델의 성능을 종합적으로 평가하기 위해 AP(Average Precision)와 mAP(mean Average Precision)라는 지표가 사용된다.</p>
<ul>
<li><strong>정밀도-재현율 곡선 (Precision-Recall Curve):</strong> 이 곡선은 정밀도와 재현율의 관계를 시각적으로 보여주는 그래프이다. 모델이 출력한 모든 예측을 신뢰도 점수가 높은 순서대로 정렬한 뒤, 맨 위부터 하나씩 예측을 채택해 나가면서 각 지점에서의 정밀도와 재현율을 계산하여 그린다. 재현율을 x축, 정밀도를 y축으로 하여 그래프를 그리면, 일반적으로 우하향하는 형태의 곡선이 나타난다. 이상적인 모델은 모든 재현율 수준에서 높은 정밀도를 유지하므로, 곡선이 그래프의 우측 상단에 가깝게 그려질수록 성능이 우수함을 의미한다.53</li>
<li><strong>AP (Average Precision):</strong> AP는 단일 객체 클래스에 대한 모델의 성능을 하나의 숫자로 요약한 지표이다. 이는 정밀도-재현율 곡선 아래의 면적(Area Under the Curve, AUC)을 계산함으로써 얻어진다.52 직관적으로, AP는 0부터 1까지 모든 재현율 수준에 걸친 정밀도의 평균값이라고 해석할 수 있다. AP 값이 높을수록 해당 클래스에 대한 탐지 성능이 전반적으로 우수함을 나타낸다.</li>
<li><strong>mAP (mean Average Precision):</strong> mAP는 데이터셋에 존재하는 모든 객체 클래스에 대한 AP 값들의 평균을 계산한 것이다.2 예를 들어, 데이터셋에 ‘사람’, ‘자동차’, ‘자전거’ 세 개의 클래스가 있다면, 각 클래스에 대해 개별적으로 AP를 계산한 뒤 이 세 AP 값의 산술 평균을 낸 것이 mAP가 된다. mAP는 여러 클래스를 다루는 객체 탐지 모델의 전체적인 성능을 평가하는 가장 표준적이고 널리 사용되는 최종 지표이다.</li>
<li><strong>표기법과 표준:</strong> mAP를 보고할 때는 어떤 IoU 임계값을 사용했는지를 명시하는 것이 중요하다. 예를 들어, <code>mAP@0.5</code> 또는 <code>mAP50</code>은 IoU 임계값을 0.5로 고정했을 때 계산된 mAP를 의미하며, 이는 PASCAL VOC 데이터셋에서 전통적으로 사용되던 방식이다.55 한편, MS COCO 데이터셋에서는 더 엄격한 평가를 위해 IoU 임계값을 0.5에서 0.95까지 0.05 간격으로 변화시키면서 각각의 mAP를 계산한 뒤, 이들을 다시 평균 낸 값을 최종 성능 지표로 사용한다. 이는 <code>mAP@[.5:.05:.95]</code>로 표기된다.52</li>
</ul>
<p>아래 표는 객체 탐지 모델의 성능을 평가하는 데 사용되는 핵심 지표들을 수식과 함께 요약한 것이다.</p>
<table><thead><tr><th>지표 (Metric)</th><th>수식 (Formula)</th><th>설명 (Description)</th></tr></thead><tbody>
<tr><td>IoU</td><td><span class="math math-inline">\frac{\vert B_p \cap B_{gt} \vert}{\vert B_p \cup B_{gt} \vert}</span></td><td>예측 상자와 실제 상자 간의 중첩 정도를 측정</td></tr>
<tr><td>정밀도 (Precision)</td><td><span class="math math-inline">\frac{TP}{TP + FP}</span></td><td>예측된 결과 중 실제 객체인 비율</td></tr>
<tr><td>재현율 (Recall)</td><td><span class="math math-inline">\frac{TP}{TP + FN}</span></td><td>실제 객체 중 모델이 탐지해낸 비율</td></tr>
<tr><td>AP</td><td><span class="math math-inline">\int_{0}^{1} p(r) dr</span></td><td>단일 클래스에 대한 정밀도-재현율 곡선 아래 면적</td></tr>
<tr><td>mAP</td><td><span class="math math-inline">\frac{1}{N} \sum_{i=1}^{N} AP_i</span></td><td>모든 N개 클래스에 대한 AP의 평균</td></tr>
</tbody></table>
<h2>4.  객체 탐지의 주요 응용 분야 및 미래 전망</h2>
<p>객체 탐지 기술은 이론적 발전을 넘어 다양한 산업 분야에 깊숙이 침투하여 실질적인 가치를 창출하고 있다. 자율 주행에서부터 의료, 보안, 소매업에 이르기까지, 기계가 시각적 세계를 이해하고 상호작용할 수 있도록 하는 이 기술은 현대 사회의 수많은 혁신을 이끄는 핵심 동력으로 자리 잡았다.3 이 장에서는 객체 탐지의 주요 응용 분야를 살펴보고, 앞으로의 기술적 과제와 미래 전망을 탐색한다.</p>
<h3>4.1  자율 주행 (Autonomous Driving)</h3>
<p>객체 탐지는 자율 주행 시스템의 ’눈’이라 할 수 있는 인지(perception) 시스템의 가장 핵심적인 구성 요소이다. 자율 주행 차량은 주변 환경을 정확하게 인식해야만 안전한 판단과 제어를 수행할 수 있으며, 객체 탐지는 이를 위한 필수적인 정보를 제공한다.</p>
<ul>
<li><strong>핵심 역할:</strong> 자율 주행 시스템은 카메라, LiDAR, 레이더 등 다양한 센서를 통해 수집된 데이터를 바탕으로 도로 위의 다른 차량, 보행자, 자전거, 오토바이와 같은 동적 객체뿐만 아니라, 교통 표지판, 신호등, 차선, 연석 등 정적 객체들을 실시간으로 탐지하고 분류한다.1 각 객체의 위치, 크기, 종류를 파악하는 것은 충돌 회피, 경로 계획, 교통 법규 준수 등 안전한 주행을 위한 모든 의사 결정 과정의 출발점이다.60 특히, 보행자와 같이 예측 불가능한 움직임을 보이는 취약한 도로 사용자를 정확하게 탐지하는 것은 치명적인 사고를 예방하는 데 있어 최우선 과제이다.58 현대의 자율 주행 시스템은 카메라 이미지 기반의 2D 객체 탐지를 넘어, LiDAR 포인트 클라우드나 레이더 데이터를 융합하여 객체의 3차원 위치와 속도까지 정확하게 추정하는 3D 객체 탐지 기술로 발전하고 있다.62</li>
</ul>
<h3>4.2  의료 영상 분석 (Medical Image Analysis)</h3>
<p>의료 분야에서 객체 탐지 기술은 방대한 양의 의료 영상 데이터를 분석하여 의사의 진단을 보조하고, 질병의 조기 발견율을 높이는 데 크게 기여하고 있다. 이는 ‘컴퓨터 보조 진단(Computer-Aided Diagnosis, CADx)’ 시스템의 핵심 기술로 활용된다.</p>
<ul>
<li><strong>핵심 역할:</strong> X-ray, 컴퓨터 단층 촬영(CT), 자기 공명 영상(MRI), 초음파 등 다양한 의료 영상에서 종양, 결절, 병변, 미세 출혈과 같은 비정상적인 조직이나 질병의 징후를 자동으로 탐지하고 그 위치를 표시한다.11 예를 들어, CT 스캔 영상에서 폐 결절을 탐지하거나, 유방 촬영술 이미지에서 악성 종양의 의심 영역을 찾아내고, 망막 사진에서 당뇨병성 망막증의 징후인 미세 동맥류를 식별하는 등의 작업이 가능하다.65 이를 통해 의사는 판독 과정에서 발생할 수 있는 피로도를 줄이고, 육안으로 놓치기 쉬운 미세한 이상 징후를 발견할 가능성을 높여 진단의 정확성과 일관성을 향상시킬 수 있다.63</li>
</ul>
<h3>4.3  보안 및 감시 (Security and Surveillance)</h3>
<p>객체 탐지는 전통적인 CCTV 기반의 수동적 감시 시스템을 지능형 능동 감시 시스템으로 탈바꿈시키는 데 결정적인 역할을 한다. 24시간 내내 수많은 카메라 피드를 인간이 모니터링하는 것의 한계를 극복하고, 자동화된 분석을 통해 보안 효율성을 극대화한다.</p>
<ul>
<li><strong>핵심 역할:</strong> 감시 카메라 영상에서 실시간으로 사람, 차량, 특정 물체 등을 탐지하고 추적하여 다양한 보안 시나리오에 대응한다.2 예를 들어, 특정 구역에 허가되지 않은 사람이 침입했을 때 자동으로 경보를 울리는 ‘침입 감지’, 공항이나 역사에 오랜 시간 방치된 가방과 같은 ‘유기물 탐지’, 특정 장소에 비정상적으로 많은 사람이 모이는 것을 감지하는 ‘군중 행동 분석’ 등이 가능하다. 또한, 탐지된 객체의 움직임을 추적하여 동선을 파악하거나, 특정 행동 패턴(예: 폭력 행위, 기물 파손)을 인식하는 등 더욱 고도화된 분석으로 확장될 수 있다. 이를 통해 보안 인력은 실제 위협 상황에 신속하고 집중적으로 대응할 수 있게 되며, 사고 예방 및 사후 증거 확보에 효과적으로 활용할 수 있다.70</li>
</ul>
<h3>4.4  소매 및 유통 (Retail and Distribution)</h3>
<p>소매 및 유통 산업에서 객체 탐지 기술은 매장 운영 효율화, 고객 경험 향상, 손실 방지 등 다방면에 걸쳐 혁신을 주도하고 있다. 매장 내에 설치된 카메라를 단순한 보안용이 아닌, 데이터 수집 및 분석 도구로 활용하는 것이다.</p>
<ul>
<li><strong>핵심 역할:</strong></li>
<li><strong>자동 재고 관리:</strong> 매장 선반을 촬영하는 카메라 영상을 분석하여 진열된 상품을 실시간으로 인식하고 개수를 파악한다. 특정 상품의 재고가 소진되거나 부족해지면 직원에게 자동으로 알림을 보내 품절로 인한 판매 기회 손실을 막고, 수동으로 재고를 확인하는 데 드는 노동력을 절감한다.73</li>
<li><strong>고객 행동 분석:</strong> 매장 내 고객들의 움직임을 추적하여 동선을 분석하고, 어떤 상품 앞에서 얼마나 오래 머무르는지, 어떤 상품을 집어 드는지 등의 데이터를 수집한다. 이 데이터를 시각화한 히트맵(heatmap) 등을 통해 고객의 관심도와 쇼핑 패턴을 파악하고, 이를 바탕으로 매장 레이아웃, 상품 진열, 마케팅 전략을 최적화할 수 있다.76</li>
<li><strong>무인 매장 및 자동 결제:</strong> Amazon Go와 같은 계산대 없는 매장을 구현하는 핵심 기술이다. 천장에 설치된 수많은 카메라와 센서가 고객이 선반에서 어떤 상품을 집는지 정확하게 탐지하고, 고객이 매장을 나갈 때 가상 장바구니에 담긴 상품 목록을 기반으로 자동으로 결제가 이루어지게 한다. 이는 고객에게 대기 시간 없는 편리한 쇼핑 경험을 제공한다.74</li>
</ul>
<h3>4.5  미래 전망 및 기술적 과제</h3>
<p>객체 탐지 기술은 이미 많은 성과를 이루었지만, 여전히 해결해야 할 기술적 과제들이 남아 있으며, 이는 곧 미래 연구의 방향을 제시한다.</p>
<ul>
<li><strong>미래 전망:</strong></li>
<li><strong>효율성 증대와 엣지 컴퓨팅:</strong> 모델 압축, 양자화, 지식 증류 등 모델 경량화 기술과 신경망 처리 장치(NPU)와 같은 전용 하드웨어 가속 기술의 발전이 결합되면서, 스마트폰, 드론, IoT 기기와 같은 엣지 디바이스에서 직접 고성능 객체 탐지를 실시간으로 수행하는 것이 더욱 보편화될 것이다.</li>
<li><strong>데이터 효율적 학습:</strong> 대규모의 정교하게 라벨링된 데이터셋을 구축하는 데는 막대한 비용과 시간이 소요된다. 따라서 적은 수의 라벨링된 데이터만으로도 모델을 효과적으로 학습시킬 수 있는 소수샷 학습(Few-shot learning), 제로샷 학습(Zero-shot learning), 그리고 라벨 없이 데이터 자체의 구조를 학습하는 자기 지도 학습(Self-supervised learning) 기반의 객체 탐지 연구가 더욱 중요해질 것이다.</li>
<li><strong>다중 모달리티 융합:</strong> 시각 정보(이미지, 비디오)뿐만 아니라 텍스트 설명, 소리, 3D 포인트 클라우드, 열화상 등 다양한 종류의 데이터를 함께 활용하는 다중 모달(multi-modal) 객체 탐지가 더욱 발전할 것이다. 이는 단일 센서의 한계를 보완하고, 더욱 강건하고 풍부한 상황 인지를 가능하게 할 것이다.</li>
<li><strong>기술적 과제:</strong></li>
<li><strong>작고 가려진 객체 탐지:</strong> 이미지에서 차지하는 픽셀 수가 매우 적은 작은 객체나, 다른 객체에 의해 상당 부분이 가려진(occluded) 객체를 정확하게 탐지하는 것은 여전히 가장 어려운 문제 중 하나로 남아있다.42</li>
<li><strong>도메인 적응 (Domain Adaptation):</strong> 특정 환경(예: 맑은 날의 주간 도로)에서 수집된 데이터로 학습된 모델은, 학습 데이터와 분포가 다른 새로운 환경(예: 야간, 비 오는 날, 안개 낀 날)에서는 성능이 급격히 저하되는 경향이 있다. 이러한 도메인 격차(domain gap)를 극복하고 다양한 환경에 강건한 모델을 개발하는 것이 중요한 과제이다.2</li>
<li><strong>설명 가능성 (Explainability, XAI):</strong> 특히 자율 주행이나 의료 진단과 같이 안전과 신뢰성이 극도로 중요한 분야에서는, 딥러닝 모델이 왜 특정 객체를 탐지했는지(또는 탐지하지 못했는지) 그 판단의 근거를 인간이 이해할 수 있는 방식으로 설명하는 기술이 요구된다.</li>
<li><strong>윤리 및 프라이버시 문제:</strong> 얼굴 인식, 개인의 행동 추적 등 객체 탐지 기술의 응용은 심각한 프라이버시 침해 문제를 야기할 수 있다. 또한, 학습 데이터에 내재된 편향(bias)이 모델에 그대로 반영되어 특정 인종이나 성별에 대해 차별적인 성능을 보이는 문제가 발생할 수 있다. 이러한 윤리적 문제를 해결하기 위한 기술적, 제도적, 사회적 논의와 노력이 병행되어야 한다.73</li>
</ul>
<p>다양한 응용 분야를 관통하는 중요한 사실은, 객체 탐지가 단순히 최종 결과물로서의 가치만을 갖는 것이 아니라는 점이다. 오히려 객체 탐지는 더 복잡한 추론과 행동 시스템을 구축하기 위한 근본적인 ’인지 기본 단위(perception primitive)’로서 기능한다. 자율 주행에서 ’보행자’를 탐지하는 것은 그 자체로 끝이 아니라, 그 보행자의 향후 궤적을 예측하고, 충돌 회피 경로를 계획하며, 차량의 조향 및 제동을 제어하는 일련의 후속 의사 결정 과정의 첫 단추이다.58 소매업에서 선반의 ’빈 공간’을 탐지하는 것은 재고 보충 알림을 생성하는 행동을 촉발하는 원시 입력값이다.75 보안 시스템에서 ’허가되지 않은 사람’을 탐지하는 것은 경보를 울리고 보안 요원에게 통지하는 행동으로 이어진다.68 의료 분야에서 ’종양’을 탐지하는 것은 그 크기를 측정하고, 시간 경과에 따른 성장을 추적하며, 치료 계획을 수립하는 고차원적 작업의 시작점이다.64</p>
<p>이러한 관점은 객체 탐지 기술의 중요성을 재정의한다. 그 가치는 독립적인 기능뿐만 아니라, 수많은 다운스트림 AI 응용 생태계를 가능하게 하는 핵심적인 조력자로서의 역할에 있다. 따라서 객체 탐지 기술의 정확도, 속도, 강건성 향상은 연쇄적으로 수많은 다른 분야에 긍정적인 파급 효과를 미친다. 앞으로의 객체 탐지 연구는 단순히 벤치마크 데이터셋의 mAP를 높이는 것을 넘어, 이러한 후속 추론 작업에 더욱 유용한 형태의 출력(예: 불확실성 추정치 제공, 객체 간의 관계 탐지 등)을 생성하는 방향으로 발전해 나갈 것이다.</p>
<h2>5.  결론</h2>
<p>본 안내서는 객체 탐지 기술의 근본적인 정의와 원리에서부터 시작하여, 그 기술적 발전의 궤적, 성능을 측정하는 표준화된 지표, 그리고 사회 다방면에 미치는 심대한 영향력과 미래의 도전 과제에 이르기까지 포괄적인 분석을 제시했다.</p>
<p>객체 탐지는 이미지 속에서 ’무엇’이 ’어디’에 있는지를 식별하는 컴퓨터 비전의 핵심 과업으로, 단순한 이미지 분류를 넘어 기계에 공간적, 의미론적 이해 능력을 부여하는 중추적 역할을 한다. 그 발전 과정은 뚜렷한 기술적 변곡점들을 거쳐왔다. 초기 Viola-Jones, HOG와 같은 전통적 기법들은 수동으로 설계된 특징에 의존하는 한계를 보였으나, 딥러닝의 등장은 이 패러다임을 완전히 바꾸었다. R-CNN 계열은 딥러닝의 강력한 특징 학습 능력을 탐지 문제에 성공적으로 도입하며 정확도의 새로운 기준을 세웠고, Fast R-CNN과 Faster R-CNN으로 이어지는 진화는 외부 의존적이고 비효율적인 구성 요소들을 점진적으로 학습 가능한 단일 네트워크 안으로 통합하며 ’종단간 학습’이라는 이상에 다가섰다.</p>
<p>한편, YOLO와 SSD로 대표되는 1단계 탐지기는 실시간 처리라는 시대적 요구에 부응하여 속도와 정확도 사이의 균형점을 찾아냈으며, 특히 다중 스케일 특징 맵과 같은 혁신을 통해 그 실용성을 입증했다. 가장 최근에는 트랜스포머 아키텍처를 도입한 DETR이 앵커 박스나 NMS와 같은 기존의 사전 지식 기반 설계에서 벗어나, 집합 예측이라는 새로운 접근법을 통해 진정한 의미의 종단간 탐지 모델의 가능성을 제시하며 또 다른 패러다임의 전환을 예고하고 있다. 이러한 발전사는 사전 지식의 유용성과 순수한 데이터 기반 학습의 잠재력 사이의 끊임없는 탐색 과정으로 요약될 수 있다.</p>
<p>기술의 발전과 더불어, IoU, 정밀도, 재현율, 그리고 이들을 종합한 mAP와 같은 표준화된 평가 지표는 모델의 성능을 객관적으로 측정하고 건전한 학술적, 산업적 경쟁을 촉진하는 기반이 되었다. 이러한 정량적 평가를 바탕으로 객체 탐지 기술은 자율 주행, 의료 영상 분석, 지능형 감시, 자동화된 소매업 등 다양한 분야에서 혁신적인 응용을 창출하며, 인간의 능력을 보조하고 확장하는 ’인지 기본 단위’로서의 역할을 공고히 하고 있다.</p>
<p>그러나 객체 탐지 기술의 미래가 장밋빛만은 아니다. 작은 객체 및 가려진 객체에 대한 탐지 성능, 다양한 환경 변화에 대한 강건성, 모델의 판단 근거를 설명하는 능력, 그리고 기술의 오남용으로 인한 프라이버시 및 윤리적 문제 등은 여전히 해결해야 할 중요한 과제로 남아있다. 따라서 미래의 연구는 단순히 정확도를 높이는 것을 넘어, 효율성, 데이터 효율성, 신뢰성, 그리고 사회적 책임성을 모두 고려하는 방향으로 나아가야 할 것이다. 객체 탐지 기술은 이미 세상을 바꾸고 있으며, 앞으로 마주할 도전들을 슬기롭게 극복해 나간다면 그 잠재력은 무한히 확장될 것이다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>What Is Object Detection? - MATLAB &amp; Simulink - MathWorks, https://www.mathworks.com/discovery/object-detection.html</li>
<li>Object detection - Wikipedia, https://en.wikipedia.org/wiki/Object_detection</li>
<li>What Is Computer Vision Object Detection Vs. Object Recognition - Chooch AI, https://www.chooch.com/blog/what-is-object-detection/</li>
<li>What is Object Detection? - IBM, https://www.ibm.com/think/topics/object-detection</li>
<li>Object Detection vs Object Recognition vs Image Segmentation - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/object-detection-vs-object-recognition-vs-image-segmentation/</li>
<li>A Gentle Introduction to Object Recognition With Deep Learning - MachineLearningMastery.com, https://machinelearningmastery.com/object-recognition-with-deep-learning/</li>
<li>Object Detection - Ultralytics YOLO Docs, https://docs.ultralytics.com/tasks/detect/</li>
<li>A Decade of You Only Look Once (YOLO) for Object Detection - arXiv, https://arxiv.org/html/2504.18586v1</li>
<li>Object Classification, Detection, Localization and Segmentation - Slajd 1 - AGH, <a href="https://home.agh.edu.pl/~horzyk/lectures/ci/CI%20Object%20Detection%20and%20Localization.pdf">https://home.agh.edu.pl/~horzyk/lectures/ci/CI%20Object%20Detection%20and%20Localization.pdf</a></li>
<li>Intersection Over Union (IoU): From Theory to Practice - Lightly AI, https://www.lightly.ai/blog/intersection-over-union</li>
<li>Object Detection: Models, Use Cases, Examples - Encord, https://encord.com/blog/object-detection/</li>
<li>Image Classification vs. Object Detection vs. Image Segmentation | by Pulkit Sharma | Analytics Vidhya | Medium, https://medium.com/analytics-vidhya/image-classification-vs-object-detection-vs-image-segmentation-f36db85fe81</li>
<li>Object Classification vs Object Localization vs Object Detection | by Abhishek Jain | Medium, https://medium.com/@abhishekjainindore24/object-classification-vs-object-localization-vs-object-detection-caf74860f473</li>
<li>Semantic Segmentation vs Object Detection: Understanding the Differences - Keymakr, https://keymakr.com/blog/semantic-segmentation-vs-object-detection-understanding-the-differences/</li>
<li>What is the difference between object detection, semantic segmentation and localization?, https://cs.stackexchange.com/questions/51387/what-is-the-difference-between-object-detection-semantic-segmentation-and-local</li>
<li>Image classification vs Object detection vs Image Segmentation | Deep Learning Tutorial 28, https://m.youtube.com/watch?v=taC5pMCm70U</li>
<li>[1703.06870] Mask R-CNN - arXiv, https://arxiv.org/abs/1703.06870</li>
<li>Rich feature hierarchies for accurate object detection and semantic segmentation - arXiv, https://arxiv.org/abs/1311.2524</li>
<li>Faster R-CNN: Towards Real-Time Object Detection with … - arXiv, https://arxiv.org/pdf/1506.01497</li>
<li>Getting Started with R-CNN, Fast R-CNN, and Faster R-CNN - MATLAB &amp; Simulink - MathWorks, https://www.mathworks.com/help/vision/ug/getting-started-with-r-cnn-fast-r-cnn-and-faster-r-cnn.html</li>
<li>Fast R-CNN | ML - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/fast-r-cnn-ml/</li>
<li>Faster R-CNN | ML - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/faster-r-cnn-ml/</li>
<li>Faster R-CNN: Breakthrough in Object Detection Tech - Viso Suite, https://viso.ai/deep-learning/faster-r-cnn-2/</li>
<li>[1504.08083] Fast R-CNN - arXiv, https://arxiv.org/abs/1504.08083</li>
<li>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks - arXiv, https://arxiv.org/abs/1506.01497</li>
<li>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks - NIPS, https://proceedings.neurips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf</li>
<li>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks - NIPS, https://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks</li>
<li>Faster R-CNN - - NeuralCeption -, https://www.neuralception.com/objectdetection-fasterrcnn/</li>
<li>Understanding and Implementing Faster R-CNN | by Rishabh Singh | Medium, https://medium.com/@RobuRishabh/understanding-and-implementing-faster-r-cnn-248f7b25ff96</li>
<li>Region Proposal Network (RPN) — Backbone of Faster R-CNN | by Tanay Karmarkar, https://medium.com/@codeplumber/region-proposal-network-rpn-backbone-of-faster-r-cnn-4a744a38d7f9</li>
<li>Faster R-CNN Explained for Object Detection Tasks - DigitalOcean, https://www.digitalocean.com/community/tutorials/faster-r-cnn-explained-object-detection</li>
<li>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, https://www.computer.org/csdl/journal/tp/2017/06/07485869/13rRUx0gera</li>
<li>You Only Look Once: Unified, Real-Time Object Detection, https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf</li>
<li>[1506.02640] You Only Look Once: Unified, Real-Time Object Detection - arXiv, https://arxiv.org/abs/1506.02640</li>
<li>You-Only-Look-Once: Object Detection with YOLO | by Zia Babar | Medium, https://medium.com/@zbabar/you-only-look-once-object-detection-with-yolo-de9fd5455306</li>
<li>YOLO Object Detection Explained: A Beginner’s Guide - DataCamp, https://www.datacamp.com/blog/yolo-object-detection-explained</li>
<li>SSD: Single Shot MultiBox Detector - UNC Computer Science, https://www.cs.unc.edu/~wliu/papers/ssd.pdf</li>
<li>How single-shot detector (SSD) works? - GeeksforGeeks, https://www.geeksforgeeks.org/computer-vision/how-single-shot-detector-ssd-works/</li>
<li>Single Shot MultiBox Detector (SSD) Explained by A.J | by Jesse Annan | Medium, https://medium.com/@jesse419419/single-shot-multibox-detector-ssd-explained-by-a-j-dda10ba42a29</li>
<li>SSD object detection: Single Shot MultiBox Detector for real-time processing - Jonathan Hui, https://jonathan-hui.medium.com/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06</li>
<li>Real-Time Object Detection with SSDs: Single Shot MultiBox Detectors - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2023/11/real-time-object-detection-with-ssds-single-shot-multibox-detectors/</li>
<li>End-to-End Object Detection with Transformers, https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf</li>
<li>Object Detection with Transformers: A Review - arXiv, https://arxiv.org/html/2306.04670</li>
<li>[2005.12872] End-to-End Object Detection with Transformers - arXiv, https://arxiv.org/abs/2005.12872</li>
<li>[R] End-to-End Object Detection with Transformers : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/grbipg/r_endtoend_object_detection_with_transformers/</li>
<li>[2010.04159] Deformable DETR: Deformable Transformers for End-to-End Object Detection, https://arxiv.org/abs/2010.04159</li>
<li>[2111.14330] Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity, https://arxiv.org/abs/2111.14330</li>
<li>Generalized Intersection over Union, https://giou.stanford.edu/</li>
<li>Understanding Intersection over Union for Model Accuracy - Viso Suite, https://viso.ai/computer-vision/intersection-over-union-iou/</li>
<li>Intersection over Union (IoU) for object detection - SuperAnnotate, https://www.superannotate.com/blog/intersection-over-union-for-object-detection</li>
<li>A Gentle Guide to Intersection over Union (IoU) - learnml.io, https://www.learnml.io/posts/a-gentle-guide-to-intersection-over-union/</li>
<li>How Compute Accuracy For Object Detection works—ArcGIS Pro | Documentation, https://pro.arcgis.com/en/pro-app/latest/tool-reference/image-analyst/how-compute-accuracy-for-object-detection-works.htm</li>
<li>Mean average precision (mAP) in object detection | SuperAnnotate, https://www.superannotate.com/blog/mean-average-precision-and-its-uses-in-object-detection</li>
<li>What is Mean Average Precision (mAP) in Object Detection? - Roboflow Blog, https://blog.roboflow.com/mean-average-precision/</li>
<li>Performance Metrics Deep Dive - Ultralytics YOLO Docs, https://docs.ultralytics.com/guides/yolo-performance-metrics/</li>
<li>mAP (mean Average Precision) for Object Detection | by Jonathan Hui - Medium, https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173</li>
<li>Understanding IoU, Precision, Recall, and mAP for Object Detection Models - Medium, https://medium.com/@Spritan/understanding-iou-precision-recall-and-map-for-object-detection-models-4f06511d289c</li>
<li>Real-time Traffic Object Detection for Autonomous Driving - arXiv, https://arxiv.org/html/2402.00128v2</li>
<li>Detailed Overview of Object Detection in Autonomous Vehicles - Sapien, https://www.sapien.io/blog/object-detection-in-autonomous-vehicles</li>
<li>How Object Detection coded on Autonomous vehicles — Real-world Example - Charles Lo, https://charlesarea.medium.com/how-object-detection-coded-on-autonomous-vehicles-real-world-example-809a42752de2?source=author_recirc—–cffe035d148f––1––––––––––––––</li>
<li>A survey on 3D object detection in real time for autonomous driving - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC10950960/</li>
<li>Recent Advances in 3D Object Detection for Self-Driving Vehicles: A Survey - MDPI, https://www.mdpi.com/2673-2688/5/3/61</li>
<li>Deep Learning Based Object Detection in Medical Image with YOLOv4-CSP with U-Net Algorithms | IIETA, https://www.iieta.org/journals/ts/paper/10.18280/ts.420115</li>
<li>AI-Powered Object Detection in Radiology: Current Models, Challenges, and Future Direction - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC12112695/</li>
<li>Object Detection for Medical Image Analysis: Insights from the RT-DETR Model - arXiv, https://arxiv.org/abs/2501.16469</li>
<li>Application of Object Detection in Medical Image Diagnosis using Deep Learning, https://www.researchgate.net/publication/388968266_Application_of_Object_Detection_in_Medical_Image_Diagnosis_using_Deep_Learning</li>
<li>Medical Object Detection - Papers With Code, https://paperswithcode.com/task/medical-object-detection</li>
<li>Intelligent Video Surveillance using Object Detection - FastPix, https://www.fastpix.io/blog/intelligent-video-surveillance-using-object-detection</li>
<li>The Role of Object Detection in Video Surveillance - Kastle Systems, https://www.kastle.com/resource/the-role-of-object-detection-in-video-surveillance/</li>
<li>www.kastle.com, <a href="https://www.kastle.com/resource/the-role-of-object-detection-in-video-surveillance/#:~:text=By%20continuously%20scanning%20for%20people,abandoned%20objects%20in%20public%20areas.">https://www.kastle.com/resource/the-role-of-object-detection-in-video-surveillance/#:~:text=By%20continuously%20scanning%20for%20people,abandoned%20objects%20in%20public%20areas.</a></li>
<li>The Future of AI Security Monitoring for Businesses, https://eyeqmonitoring.com/2025/02/what-is-object-detection-how-does-it-work-a-guide-to-ai-powered-surveillance/</li>
<li>Object Detection: Applications in Video Surveillance and Image Retrieval Systems, https://www.augmentedstartups.com/blog/object-detection-applications-in-video-surveillance-and-image-retrieval-systems</li>
<li>AI-Powered Object Detection: Transforming Retail Inventory Management | The AI Journal, https://aijourn.com/ai-powered-object-detection-transforming-retail-inventory-management/</li>
<li>Transforming Retail: How Object Detection Enhances the Shopping Experience with Image Recognition - Analyticsmart, https://analyticsmart.com/transforming-retail-how-object-detection-enhances-the-shopping-experience-with-image-recognition/</li>
<li>How AI Object Detection is Shaping the Future of Retail - AI Time Journal - Artificial Intelligence, Automation, Work and Business, https://www.aitimejournal.com/how-ai-object-detection-is-shaping-the-future-of-retail/51685/</li>
<li>How AI-Driven Object Detection Enhances Security in Retail Environments | Medium, https://medium.com/@API4AI/how-ai-driven-object-detection-enhances-security-in-retail-environments-6738a1dbf18f</li>
<li>Advanced Customer Behavior Tracking and Heatmap Analysis with YOLOv5 and DeepSORT in Retail Environment - MDPI, https://www.mdpi.com/2079-9292/13/23/4730</li>
<li>Revolutionizing Retail with Object Recognition Technology - Keylabs, https://keylabs.ai/blog/revolutionizing-retail-with-object-recognition-technology/</li>
<li>AI in retail: Enhancing customer experience using Computer Vision - Ultralytics, https://www.ultralytics.com/blog/ai-in-retail-enhancing-customer-experience-using-computer-vision</li>
<li>Enhancing Object Detection in Smart Video Surveillance: A Survey of Occlusion-Handling Approaches - MDPI, https://www.mdpi.com/2079-9292/13/3/541</li>
<li>How Autonomous Vehicle Object Detection Works - Roboflow Blog, https://blog.roboflow.com/autonomous-vehicle-object-detection/</li>
<li>How to boost retail profitability with AI-powered shelf object detection - Labelbox, https://labelbox.com/guides/how-to-boost-retail-profitability-with-ai-powered-shelf-object-detection/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>