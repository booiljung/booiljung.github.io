<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Fast R-CNN 통합된 고속 객체 탐지 프레임워크</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Fast R-CNN 통합된 고속 객체 탐지 프레임워크</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>Fast R-CNN 통합된 고속 객체 탐지 프레임워크</span></nav>
                </div>
            </header>
            <article>
                <h1>Fast R-CNN 통합된 고속 객체 탐지 프레임워크</h1>
<h2>1. 서론</h2>
<p>객체 탐지(Object Detection)는 컴퓨터 비전 분야의 핵심 과제로, 이미지 내에 존재하는 모든 객체의 위치를 경계 상자(bounding box)로 특정하고 각 객체의 클래스를 분류하는 작업을 포함한다. 딥러닝 시대의 개막과 함께, R-CNN(Regions with Convolutional Neural Networks)은 합성곱 신경망(Convolutional Neural Network, CNN)의 강력한 특징 추출 능력을 객체 탐지에 성공적으로 적용하며 해당 분야에 패러다임 전환을 가져왔다.1 그러나 R-CNN은 다단계로 구성된 복잡한 파이프라인과 각 후보 영역에 대한 독립적인 CNN 연산으로 인해 극심한 계산 비효율성을 보였다.2</p>
<p>이러한 R-CNN의 속도 문제를 해결하기 위해, SPP-net(Spatial Pyramid Pooling network)은 전체 이미지에 대해 단 한 번만 CNN 연산을 수행하고 특징 맵(feature map)을 공유하는 방식을 제안하여 연산 효율성을 일부 개선했다.2 하지만 SPP-net 역시 다단계 학습 파이프라인을 유지했으며, 특히 CNN 계층의 종단간(end-to-end) 미세조정(fine-tuning)이 비효율적이라는 근본적인 한계를 극복하지 못했다.5</p>
<p>이러한 배경 속에서 2015년 Ross Girshick에 의해 제안된 Fast R-CNN은 R-CNN과 SPP-net의 단점을 해결하고 객체 탐지 모델의 학습 및 추론 방식을 근본적으로 혁신한 프레임워크다.2 Fast R-CNN은 객체 분류와 경계 상자 회귀를 단일 네트워크에서 동시에 학습하는 다중 작업 손실 함수(multi-task loss)와, 다양한 크기의 후보 영역 특징을 고정된 크기로 변환하는 RoI 풀링(Region of Interest Pooling) 계층을 도입했다. 이를 통해 기존의 분절된 다단계 파이프라인을 하나의 통합된 종단간 학습 가능 모델로 전환하는 데 성공했다.</p>
<p>그 결과, Fast R-CNN은 VGG16 네트워크 기준, R-CNN 대비 학습 속도를 9배, SPP-net 대비 3배 향상시켰다. 더욱 극적인 것은 추론 속도로, R-CNN 대비 213배, SPP-net 대비 10배 빠른 성능을 달성했으며, PASCAL VOC 2012 데이터셋에서 더 높은 평균 정밀도(mAP)를 기록하며 정확도까지 개선했다.2 본 안내서는 Fast R-CNN의 핵심적인 구조와 혁신적인 아이디어를 심층적으로 분석하고, 선행 연구인 R-CNN 및 SPP-net과의 비교를 통해 그 기술적 의의를 명확히 규명하고자 한다. 이는 단순히 속도 개선을 넘어, 객체 탐지 프레임워크가 어떻게 통합되고 효율화되었는지를 보여주는 중요한 이정표가 될 것이다.</p>
<h2>2.  선행 연구의 구조와 한계</h2>
<p>Fast R-CNN의 혁신을 이해하기 위해서는 그 선행 모델들인 R-CNN과 SPP-net이 가졌던 구조적 특징과 명백한 한계를 먼저 살펴볼 필요가 있다. 이 모델들은 딥러닝 기반 객체 탐지의 가능성을 열었지만, 동시에 심각한 효율성 문제를 내포하고 있었다.</p>
<h3>2.1  R-CNN: 다단계 파이프라인의 비효율성</h3>
<p>R-CNN은 딥러닝을 객체 탐지에 성공적으로 도입한 첫 모델이었으나, 그 구조는 여러 독립적인 모듈이 순차적으로 연결된 복잡하고 비효율적인 다단계 파이프라인이었다.1 R-CNN의 작업 흐름은 다음과 같은 네 단계로 구성된다.</p>
<ol>
<li><strong>영역 제안 (Region Proposal):</strong> 입력 이미지에서 객체가 존재할 가능성이 있는 약 2000개의 후보 영역(Region of Interest, RoI)을 선택적 탐색(Selective Search)과 같은 외부 알고리즘을 사용하여 생성한다.1</li>
<li><strong>특징 추출 (Feature Extraction):</strong> 생성된 2000개의 각 후보 영역을 CNN의 입력 크기(예: 224x224)에 맞게 강제로 변형(warp)하거나 잘라낸다. 이후, 사전 학습된 CNN 모델(예: AlexNet, VGG)을 사용하여 각 영역으로부터 고정된 길이의 특징 벡터를 독립적으로 추출한다.</li>
<li><strong>클래스 분류 (Classification):</strong> 추출된 특징 벡터들을 각 객체 클래스별로 독립적으로 학습된 선형 SVM(Support Vector Machine) 분류기에 입력하여 해당 영역이 어떤 객체에 속하는지, 혹은 배경인지를 판별한다.</li>
<li><strong>경계 상자 회귀 (Bounding Box Regression):</strong> SVM을 통해 객체로 분류된 영역들의 위치 정확도를 높이기 위해, 별도로 학습된 선형 회귀 모델을 사용하여 경계 상자의 좌표를 미세 조정한다.</li>
</ol>
<p>이러한 구조는 몇 가지 치명적인 한계를 야기했다. 가장 큰 문제는 <strong>반복적인 CNN 연산</strong>이었다.2 한 이미지 내의 후보 영역들은 대부분 서로 크게 겹치지만, R-CNN은 이러한 중복을 고려하지 않고 약 2000개의 모든 영역에 대해 독립적으로 CNN 순전파(forward pass)를 수행했다. 이로 인해 엄청난 양의 중복 계산이 발생하여 학습과 추론 과정 모두에서 극심한 속도 저하를 유발했다.7</p>
<p>또한, <strong>다단계 학습 과정</strong>은 전체 시스템을 비효율적으로 만들었다. CNN 미세조정, SVM 학습, 경계 상자 회귀 모델 학습이 모두 개별적으로 이루어져야 했기 때문에, 전체 파이프라인을 한 번에 최적화하는 종단간 학습이 불가능했다.3 이 과정에서 CNN으로 추출된 특징을 SVM과 회귀 모델 학습에 사용하기 위해 디스크에 캐싱해야 했는데, 이는 수백 기가바이트에 달하는 막대한 저장 공간을 요구하는 부가적인 문제도 낳았다.3</p>
<h3>2.2  SPP-net: 특징 공유의 도입과 남겨진 과제</h3>
<p>R-CNN의 핵심 병목 현상인 반복적인 CNN 연산 문제를 해결하기 위해 SPP-net이 제안되었다. SPP-net의 핵심 아이디어는 후보 영역별로 CNN을 통과시키는 대신, <strong>전체 이미지에 대해 단 한 번만 CNN 연산을 수행</strong>하여 컨볼루션 특징 맵을 생성하고, 이 특징 맵을 모든 후보 영역이 공유하는 것이다.2</p>
<p>이를 가능하게 한 핵심 기술은 <strong>공간 피라미드 풀링(Spatial Pyramid Pooling, SPP) 계층</strong>이다. 기존 CNN은 완전 연결 계층(Fully Connected Layer)에 입력하기 위해 고정된 크기의 입력 이미지를 요구했다. 하지만 SPP 계층은 다양한 크기의 입력 특징 영역을 받아 항상 고정된 길이의 벡터를 출력할 수 있다.4 SPP 계층은 입력된 후보 영역 특징 맵을 여러 수준의 그리드(예: 4x4, 2x2, 1x1)로 분할하고, 각 그리드 셀에서 최대 풀링(max pooling)을 수행한 후, 그 결과들을 모두 연결하여 최종적인 고정 길이 특징 벡터를 생성한다.</p>
<p>이러한 접근 방식 덕분에 SPP-net은 R-CNN의 특징 추출 시간을 획기적으로 단축시켰다.8 하지만 SPP-net 역시 근본적인 한계를 가지고 있었다. 가장 큰 문제는</p>
<p><strong>컨볼루션 계층의 비효율적인 미세조정</strong>이었다. SPP-net의 학습 방식은 여전히 R-CNN과 마찬가지로 여러 이미지에서 추출된 후보 영역들을 하나의 미니배치로 구성했다. 이 경우, 각 후보 영역의 오차(loss)를 역전파(backpropagation)하려면 해당 영역이 속한 원본 이미지 전체의 특징 맵에 대한 그래디언트를 계산해야 한다. 미니배치에 포함된 후보 영역들이 모두 다른 이미지에서 왔다면, 이는 사실상 미니배치 크기만큼의 이미지 전체에 대한 역전파를 수행하는 것과 같아져 계산이 매우 비효율적이 된다.5 이 문제로 인해 SPP-net은 실질적으로 SPP 계층 이전의 컨볼루션 계층들을 동결(freeze)하고 완전 연결 계층만 학습시키는 방식을 택했으며, 이는 모델의 정확도 향상을 제한하는 요인이 되었다.4 결국 SPP-net은 특징 추출의 병목은 해결했지만, 여전히 다단계 학습 파이프라인과 종단간 최적화의 부재라는 과제를 남겼다.</p>
<h2>3.  Fast R-CNN: 통합된 고속 객체 탐지 프레임워크</h2>
<p>Fast R-CNN은 R-CNN의 다단계 파이프라인과 SPP-net의 제한적인 학습 구조를 모두 극복하기 위해 설계되었다. 이 모델의 핵심은 전체 객체 탐지 과정을 하나의 단일 네트워크로 통합하여 학습과 추론을 모두 효율적으로 수행하는 것이다. 이는 두 가지 핵심적인 혁신, 즉 RoI 풀링 계층과 다중 작업 손실 함수를 통해 가능해졌다.</p>
<h3>3.1  아키텍처 개요: 단일 단계 학습 및 추론</h3>
<p>Fast R-CNN의 아키텍처는 입력부터 최종 출력까지 모든 과정이 매끄럽게 연결된 단일 네트워크다.2 그 구조는 다음과 같다.</p>
<ol>
<li><strong>입력:</strong> 네트워크는 전체 이미지 한 장과 해당 이미지에 대한 객체 후보 영역(RoI)들의 목록을 입력으로 받는다. 후보 영역은 여전히 선택적 탐색과 같은 외부 알고리즘을 통해 생성된다.</li>
<li><strong>컨볼루션 특징 추출:</strong> 이미지는 VGG16과 같은 깊은 CNN 백본 네트워크를 단 한 번 통과하여 전체 이미지에 대한 고수준의 특징 맵을 생성한다. 이 과정은 SPP-net과 유사하게 연산 공유를 통해 효율성을 극대화한다.</li>
<li><strong>RoI 풀링:</strong> 각 후보 영역의 좌표는 원본 이미지 스케일에서 특징 맵 스케일로 투영된다. 이후 <strong>RoI 풀링 계층</strong>은 이 투영된 영역에 해당하는 특징 맵 부분을 입력으로 받아, 항상 고정된 크기(예: 7x7)의 특징 벡터를 추출한다.</li>
<li><strong>분류 및 회귀:</strong> RoI 풀링을 거쳐 생성된 고정 길이 특징 벡터들은 일련의 완전 연결(FC) 계층을 통과한 후, 최종적으로 두 개의 형제(sibling) 출력 계층으로 분기된다.2</li>
</ol>
<ul>
<li><strong>분류기(Classifier):</strong> <span class="math math-inline">K</span>개의 객체 클래스와 1개의 배경 클래스를 포함하여 총 <span class="math math-inline">K+1</span>개의 클래스에 대한 확률 분포를 출력하는 소프트맥스(softmax) 계층.</li>
<li><strong>경계 상자 회귀기(Bounding Box Regressor):</strong> 각 <span class="math math-inline">K</span>개의 객체 클래스에 대해 예측된 경계 상자의 위치를 미세 조정하기 위한 4개의 좌표값(오프셋)을 출력하는 계층.</li>
</ul>
<p>이러한 통합된 구조 덕분에 Fast R-CNN은 R-CNN의 3단계 프로세스(CNN, SVM, 회귀기)를 하나의 단일 단계(single-stage) 학습 및 추론 파이프라인으로 압축할 수 있었다.3 특징을 디스크에 캐싱할 필요가 없어졌으며, 모든 네트워크 가중치는 종단간 학습을 통해 한 번에 최적화될 수 있다.2</p>
<h3>3.2  핵심 혁신 1: RoI 풀링 계층</h3>
<p>RoI(Region of Interest) 풀링 계층은 Fast R-CNN의 핵심 구성 요소로, SPP-net의 공간 피라미드 풀링을 단순화한 특수한 형태라고 볼 수 있다.3 SPP가 여러 스케일의 그리드를 사용하는 반면, RoI 풀링은 단 하나의 고정된 그리드 스케일(예:</p>
<p><span class="math math-inline">H \times W</span>)만을 사용한다. RoI 풀링의 작동 방식은 다음과 같다.7</p>
<ol>
<li>주어진 컨볼루션 특징 맵과 원본 이미지 좌표계 기준의 후보 영역(RoI)이 있다.</li>
<li>후보 영역의 좌표를 특징 맵의 스케일에 맞게 비례적으로 축소하여 특징 맵 상에 투영한다.</li>
<li>투영된 영역을 목표 출력 크기인 <span class="math math-inline">H \times W</span> (예: 7x7)개의 하위 윈도우(sub-window)로 균등하게 분할한다. 분할 시 입력 영역의 크기가 목표 크기로 나누어 떨어지지 않더라도 강제로 분할한다.</li>
<li>각각의 <span class="math math-inline">H \times W</span> 하위 윈도우 내에서 최대 풀링(max-pooling)을 독립적으로 수행하여 각 윈도우를 대표하는 단일 값을 추출한다.</li>
<li>이 과정을 통해 최종적으로 크기가 다른 어떤 입력 후보 영역이라도 항상 고정된 크기(<span class="math math-inline">H \times W</span>)의 특징 맵을 출력하게 된다.</li>
</ol>
<p>이 고정된 크기의 출력은 후속 완전 연결 계층의 입력 요구사항을 충족시키면서, 역전파 과정에서 그래디언트가 컨볼루션 계층까지 효율적으로 전달될 수 있도록 하는 중요한 역할을 한다.</p>
<h3>3.3  핵심 혁신 2: 다중 작업 손실 함수</h3>
<p>Fast R-CNN은 분류(classification)와 위치 측정(localization)이라는 두 가지 다른 작업을 단일 프레임워크 내에서 동시에 학습시키기 위해 **다중 작업 손실 함수(multi-task loss function)**를 도입했다.2 각 학습 후보 영역(RoI)에 대한 최종 손실</p>
<p><span class="math math-inline">L</span>은 분류 손실 <span class="math math-inline">L_{cls}</span>와 경계 상자 회귀 손실 <span class="math math-inline">L_{loc}</span>의 조합으로 정의된다.<br />
<span class="math math-display">
L(p, u, t^u, v) = L_{cls}(p, u) + \lambda [u \ge 1] L_{loc}(t^u, v)
</span><br />
이 수식의 각 구성 요소는 다음과 같은 의미를 가진다.1</p>
<ul>
<li>
<p><span class="math math-inline">L_{cls}(p, u) = -\log p_u</span>: 분류 손실은 실제 클래스(ground-truth class) <span class="math math-inline">u</span>에 대한 로그 손실(log loss)이다. 여기서 <span class="math math-inline">p = (p_0, \dots, p_K)</span>는 소프트맥스 출력층이 계산한 <span class="math math-inline">K+1</span>개 클래스에 대한 확률 분포다.</p>
</li>
<li>
<p><span class="math math-inline">L_{loc}(t^u, v)</span>: 위치 측정 손실은 실제 경계 상자 좌표 <span class="math math-inline">v</span>와 예측된 좌표 <span class="math math-inline">t^u</span> 간의 차이를 측정한다. Fast R-CNN은 이상치(outlier)에 덜 민감하고 안정적인 학습을 위해 L2 손실보다 강건한 <strong>Smooth L1 손실</strong>을 사용한다.<br />
<span class="math math-display">
\text{smooth}_{L_1}(x) = \begin{cases} 0.5x^2 &amp; \text{if } |x| &lt; 1 \\ |x| - 0.5 &amp; \text{otherwise} \end{cases}
</span></p>
</li>
<li>
<p><span class="math math-inline">[u \ge 1]</span>: 이는 <strong>아이버슨 괄호(Iverson bracket)</strong> 표기법으로, 조건이 참이면 1, 거짓이면 0의 값을 갖는다. 여기서 <span class="math math-inline">u</span>는 실제 클래스를 의미하며, 관례적으로 배경 클래스는 <span class="math math-inline">u=0</span>으로 지정된다. 따라서 이 항은 후보 영역이 배경이 아닐 경우(<span class="math math-inline">u \ge 1</span>)에만 위치 측정 손실 <span class="math math-inline">L_{loc}</span>를 전체 손실에 포함시키고, 배경일 경우에는 무시하도록 하는 역할을 한다. 배경 영역에 대해서는 정확한 위치를 정의할 필요가 없기 때문이다.</p>
</li>
<li>
<p><span class="math math-inline">\lambda</span>: 두 손실 항의 중요도를 조절하는 하이퍼파라미터로, 논문에서는 별도의 조정 없이 <span class="math math-inline">\lambda=1</span>로 설정하여 두 작업을 동등하게 취급했다.</p>
</li>
</ul>
<p>이 다중 작업 손실 함수를 통해 분류와 회귀 작업의 그래디언트가 RoI 풀링 계층과 컨볼루션 계층을 거쳐 네트워크 전체로 역전파될 수 있게 되었고, 이는 진정한 의미의 종단간 학습을 가능하게 한 핵심 요소다.</p>
<h3>3.4  학습 효율성 극대화: 계층적 샘플링</h3>
<p>SPP-net이 겪었던 비효율적인 역전파 문제를 해결하기 위해, Fast R-CNN은 **계층적 샘플링(hierarchical sampling)**이라는 새로운 미니배치 구성 전략을 도입했다.2 기존의 R-CNN과 SPP-net 방식은 미니배치를 구성할 때 전체 데이터셋에서</p>
<p><span class="math math-inline">R</span>개의 후보 영역을 무작위로 샘플링했다. 이 경우, <span class="math math-inline">R</span>개의 후보 영역은 대부분 서로 다른 이미지에서 오게 된다.</p>
<p>반면, Fast R-CNN은 다음과 같은 계층적 방식으로 미니배치를 구성한다.</p>
<ol>
<li>먼저, 전체 이미지 데이터셋에서 <span class="math math-inline">N</span>개의 이미지를 무작위로 샘플링한다.</li>
<li>다음으로, 샘플링된 각 <span class="math math-inline">N</span>개의 이미지에서 각각 <span class="math math-inline">R/N</span>개의 후보 영역을 샘플링한다.</li>
<li>이렇게 하여 총 <span class="math math-inline">N \times (R/N) = R</span>개의 후보 영역으로 하나의 미니배치를 구성한다.</li>
</ol>
<p>예를 들어, <span class="math math-inline">N=2</span>, <span class="math math-inline">R=128</span>로 설정하면, 단 두 개의 이미지에서 각각 64개의 후보 영역을 샘플링하여 128개의 미니배치를 만든다. 이 방식의 가장 큰 장점은 순전파 및 역전파 과정에서 <strong>연산 공유의 효율성을 극대화</strong>한다는 점이다. 미니배치 내의 모든 후보 영역이 단 <span class="math math-inline">N</span>개의 이미지에서 나왔기 때문에, 컨볼루션 연산은 <span class="math math-inline">N</span>개의 이미지에 대해서만 수행하면 된다. 이는 <span class="math math-inline">R</span>개의 다른 이미지에 대해 연산을 수행해야 했던 기존 방식에 비해 훨씬 효율적이다. 논문에 따르면, <span class="math math-inline">N=2, R=128</span>일 때 이 방식은 기존 방식보다 약 64배 더 빠르다.2 동일 이미지 내 후보 영역 간의 상관관계로 인해 학습 수렴이 느려질 수 있다는 우려가 있었지만, 실제 실험에서는 이러한 문제가 발생하지 않았으며 적은 SGD 반복으로도 좋은 성능을 달성했다.2</p>
<h2>4.  성능 분석 및 실험 결과</h2>
<p>Fast R-CNN의 아키텍처적 혁신은 실제 실험 결과에서도 그 우수성을 명확히 입증했다. PASCAL VOC 데이터셋을 기준으로 수행된 실험들은 정확도(mAP)와 속도 측면에서 기존 모델들을 압도하는 성능을 보여주었다.</p>
<h3>4.1  정확도 및 속도 비교 분석</h3>
<p>Fast R-CNN의 성능을 가장 직관적으로 보여주는 것은 선행 모델인 R-CNN, SPP-net과의 직접적인 비교다. VGG16을 백본 네트워크로 사용했을 때, PASCAL VOC 2007 테스트셋에서의 성능은 아래 표와 같이 요약될 수 있다.2</p>
<table><thead><tr><th>모델 (백본: VGG16)</th><th>학습 시간 (R-CNN 대비)</th><th>추론 시간 (이미지 당)</th><th>mAP (%)</th></tr></thead><tbody>
<tr><td>R-CNN</td><td>1× (84시간)</td><td>47.0초</td><td>66.0</td></tr>
<tr><td>SPP-net</td><td>≈0.33× (25시간)</td><td>2.3초</td><td>63.1</td></tr>
<tr><td><strong>Fast R-CNN</strong></td><td>0.11× <strong>(9.5시간)</strong></td><td><strong>0.22초</strong></td><td><strong>66.9</strong></td></tr>
</tbody></table>
<p><em>표의 값은 논문에 보고된 수치를 기반으로 재구성되었으며, 추론 시간은 경계 상자 회귀를 포함하나 후보 영역 생성 시간은 제외한 값이다.</em></p>
<p>위 표는 Fast R-CNN의 압도적인 성능 향상을 명확하게 보여준다.</p>
<ul>
<li><strong>학습 속도:</strong> Fast R-CNN은 R-CNN보다 9배, SPP-net보다 3배 빠르게 학습을 완료했다. 이는 다중 작업 손실과 계층적 샘플링을 통한 효율적인 종단간 학습 덕분이다.</li>
<li><strong>추론 속도:</strong> 추론 속도 개선은 더욱 극적이다. R-CNN이 이미지 한 장을 처리하는 데 47초가 걸린 반면, Fast R-CNN은 단 0.22초 만에 처리를 완료하여 213배의 속도 향상을 이루었다. 이는 컨볼루션 연산 공유와 통합된 네트워크 구조가 가져온 결과다. SPP-net과 비교해도 10배 이상 빠르다.</li>
<li><strong>정확도(mAP):</strong> 속도뿐만 아니라 정확도 측면에서도 Fast R-CNN은 우위를 점했다. R-CNN보다 높은 66.9%의 mAP를 달성했으며, 이는 SPP-net보다도 높은 수치다. 이는 모든 네트워크 계층을 종단간 미세조정하여 탐지 작업에 더 특화된 특징을 학습할 수 있었기 때문이다.</li>
</ul>
<h3>4.2  컨볼루션 계층 미세조정의 효과</h3>
<p>Fast R-CNN의 정확도 향상에 있어 핵심적인 요인 중 하나는 SPP-net에서는 불가능했던 <strong>컨볼루션 계층의 종단간 미세조정</strong>이 가능하다는 점이다. 논문에서는 이를 증명하기 위한 소거법 연구(ablation study)를 수행했다.</p>
<p>실험 결과, VGG16 네트워크에서 RoI 풀링 계층 이후의 완전 연결 계층만 미세조정했을 때 mAP는 61.4%에 그쳤다. 하지만 <code>conv3_1</code> 계층부터 그 이후의 모든 컨볼루션 및 완전 연결 계층을 함께 미세조정하자 mAP가 66.9%로 크게 상승했다.5 이는 객체 탐지라는 특정 작업을 위해 컨볼루션 계층의 특징 추출기 자체를 최적화하는 것이 성능에 지대한 영향을 미친다는 것을 실험적으로 보여준다. 즉, 이미지 분류를 위해 사전 학습된 특징이 탐지 작업에도 유용하지만, 탐지 데이터셋에 맞춰 특징 추출기 자체를 미세조정할 때 비로소 최적의 성능을 발휘할 수 있다는 것이다. Fast R-CNN의 통합된 아키텍처와 효율적인 학습 전략이 바로 이러한 전 계층 미세조정을 실용적으로 가능하게 만든 핵심 열쇠였다.</p>
<h2>5.  결론: Fast R-CNN의 의의와 한계</h2>
<p>Fast R-CNN은 객체 탐지 분야의 역사에서 중요한 전환점을 마련한 모델이다. 단순히 기존 모델보다 빠르고 정확한 것을 넘어, 이후 등장하는 수많은 2-stage detector의 기본 패러다임을 정립했다는 점에서 그 의의가 매우 크다.</p>
<h3>5.1  주요 기여 요약</h3>
<p>Fast R-CNN이 객체 탐지 분야에 남긴 핵심적인 기여는 다음과 같이 네 가지로 요약할 수 있다.2</p>
<ol>
<li><strong>더 높은 탐지 정확도 (Higher mAP):</strong> R-CNN, SPP-net과 같은 이전의 최첨단 모델들보다 높은 mAP를 달성하며 성능의 새로운 기준을 제시했다.</li>
<li><strong>단일 단계 다중 작업 학습 (Single-stage, Multi-task Training):</strong> 분류기와 경계 상자 회귀기를 하나의 손실 함수 아래 통합하여, 복잡했던 다단계 학습 파이프라인을 단순하고 효율적인 단일 단계 학습으로 전환했다.</li>
<li><strong>모든 네트워크 계층의 학습 가능성 (All Layers are Trainable):</strong> RoI 풀링 계층과 효율적인 샘플링 전략을 통해, SPP-net의 한계였던 컨볼루션 계층의 미세조정을 가능하게 했다. 이를 통해 네트워크 전체가 탐지 작업에 최적화된 특징을 학습할 수 있게 되었다.</li>
<li><strong>특징 캐싱을 위한 디스크 저장 공간 불필요 (No Disk Storage for Features):</strong> 종단간 학습 파이프라인은 중간 단계의 특징을 디스크에 저장하고 다시 불러오는 과정을 완전히 제거하여, 학습 워크플로우를 간소화하고 막대한 양의 저장 공간 요구사항을 없앴다.</li>
</ol>
<p>이러한 기여들은 R-CNN 계열 모델들의 진화 과정에서 가장 중요한 철학적 변화, 즉 <strong>분절된 여러 기술의 조합에서 통합된 단일 딥러닝 프레임워크로의 전환</strong>을 완성시켰다.</p>
<h3>5.2  남겨진 병목 현상과 후속 연구</h3>
<p>모든 혁신이 그렇듯, Fast R-CNN 또한 새로운 문제를 드러냈다. Fast R-CNN은 탐지 네트워크 자체의 속도를 극적으로 향상시켰지만, 전체 시스템의 속도는 여전히 <strong>후보 영역 생성 알고리즘</strong>에 의해 발목 잡혔다.1 R-CNN 시절에는 CNN 특징 추출에 47초, 선택적 탐색에 약 2초가 소요되어 후보 영역 생성 시간의 비중은 약 4%에 불과했다. 하지만 Fast R-CNN이 탐지 시간을 0.22초로 단축시키자, 변함없이 2초가 걸리는 선택적 탐색이 전체 추론 시간의 약 90%를 차지하는 새로운 병목으로 부상했다.</p>
<p>Fast R-CNN의 성공이 역설적으로 드러낸 이 문제는 후속 연구인 <strong>Faster R-CNN</strong>의 직접적인 동기가 되었다.11 Faster R-CNN은 선택적 탐색과 같은 CPU 기반의 느린 외부 알고리즘을, 딥러닝 네트워크 내부에 통합된 가볍고 빠른 **영역 제안 네트워크(Region Proposal Network, RPN)**로 대체했다.3 RPN은 GPU 가속을 통해 거의 비용 없이 후보 영역을 생성함으로써, 객체 탐지 파이프라인 전체를 진정한 의미의 종단간 딥러닝 모델로 완성시켰고, 실시간에 가까운 탐지를 가능하게 했다.</p>
<p>결론적으로, Fast R-CNN은 탐지 네트워크를 완성함으로써 후보 영역 생성의 중요성을 부각시킨 결정적인 교량 역할을 했다. 이는 이후 Mask R-CNN(인스턴스 분할) 13, Cascade R-CNN(고품질 탐지) 14, Oriented R-CNN(회전된 객체 탐지) 15 등 수많은 R-CNN 계열의 발전 모델들이 등장할 수 있는 견고한 토대를 마련해주었다. Fast R-CNN이 정립한 통합된 2-stage 탐지 프레임워크는 오늘날까지도 정확도와 속도 사이의 균형을 맞추는 가장 강력한 패러다임 중 하나로 남아있다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Object Detection for Dummies Part 3: R-CNN Family | Lil’Log, https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/</li>
<li>Fast R-CNN, https://arxiv.org/pdf/1504.08083</li>
<li>Faster R-CNN Explained for Object Detection Tasks - DigitalOcean, https://www.digitalocean.com/community/tutorials/faster-r-cnn-explained-object-detection</li>
<li>Spatial Pyramid Pooling in Deep Convolutional Networks for Visual …, https://arxiv.org/abs/1406.4729</li>
<li>Understanding Fast-RCNN for Object Detection - Towards Data Science, https://towardsdatascience.com/understanding-fast-rcnn-for-object-detection-7a1c3f63fc66/</li>
<li>[1504.08083] Fast R-CNN - arXiv, https://arxiv.org/abs/1504.08083</li>
<li>Fast R-CNN | ML - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/fast-r-cnn-ml/</li>
<li>Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition - Department of Computer Science, University of Toronto, https://www.cs.toronto.edu/~bonner/courses/2022s/csc2547/papers/discriminative/object-detection/spp-net,-he,-tpami-2015.pdf</li>
<li>Faster R-CNN: Breakthrough in Object Detection Tech - Viso Suite, https://viso.ai/deep-learning/faster-r-cnn-2/</li>
<li>Object Detection Explained: Fast R-CNN | by Ching (Chingis) - Medium, https://medium.com/mlearning-ai/object-detection-explained-fast-r-cnn-bc11e607411f</li>
<li>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks - NIPS, https://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks</li>
<li>Faster RCNN in 2025: How it works and why it’s still the benchmark for Object Detection, https://www.thinkautonomous.ai/blog/faster-rcnn/</li>
<li>[1703.06870] Mask R-CNN - arXiv, https://arxiv.org/abs/1703.06870</li>
<li>[1712.00726] Cascade R-CNN: Delving into High Quality Object Detection - arXiv, https://arxiv.org/abs/1712.00726</li>
<li>[2108.05699] Oriented R-CNN for Object Detection - arXiv, https://arxiv.org/abs/2108.05699</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>