<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Co-DETR (2022-11-19) 협력적 하이브리드 할당을 통한 DETR 기반 객체 탐지기의 훈련 최적화</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Co-DETR (2022-11-19) 협력적 하이브리드 할당을 통한 DETR 기반 객체 탐지기의 훈련 최적화</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">객체 탐지 (Object Detection)</a> / <span>Co-DETR (2022-11-19) 협력적 하이브리드 할당을 통한 DETR 기반 객체 탐지기의 훈련 최적화</span></nav>
                </div>
            </header>
            <article>
                <h1>Co-DETR (2022-11-19) 협력적 하이브리드 할당을 통한 DETR 기반 객체 탐지기의 훈련 최적화</h1>
<h2>1.  서론 (Introduction)</h2>
<p>2020년 Facebook AI Research(FAIR)에서 발표한 DETR(DEtection TRansformer)은 객체 탐지 분야에 혁신적인 패러다임 전환을 가져왔다.1 기존의 Faster R-CNN이나 YOLO와 같은 모델들이 Non-Maximum Suppression(NMS)과 같은 복잡한 후처리 과정이나 수동으로 설계해야 하는 앵커 박스(Anchor Boxes)에 의존했던 것과 달리, DETR은 Transformer의 인코더-디코더 구조와 집합 예측(set prediction) 문제를 통해 완전한 End-to-End 파이프라인을 구현했다.2 이는 객체 탐지 프로세스를 대폭 단순화하고 모델의 전체적인 공동 최적화를 가능하게 했다는 점에서 중요한 학술적, 실용적 의의를 가진다.4</p>
<p>그러나 이러한 혁신에도 불구하고, 초기 DETR 모델은 두 가지 명확한 한계점을 드러냈다. 첫째, 모델의 수렴 속도가 매우 느려 Faster R-CNN과 같은 기존 강자들과 경쟁력 있는 성능에 도달하기까지 수백 에포크에 달하는 방대한 학습 시간이 필요했다.5 이는 Transformer 어텐션 모듈이 이미지 특징 맵 전체를 조밀하게 처리하는 과정에서 발생하는 비효율성 때문이었다.7 둘째, 작은 객체에 대한 탐지 성능이 상대적으로 저조했다.4 전역적 어텐션 메커니즘이 이미지 전체에 주의(attention)를 분산시키는 과정에서, 몇 픽셀에 불과한 작은 객체의 미세한 특징 정보가 거대한 배경이나 다른 큰 객체에 의해 희석되기 쉬웠기 때문이다.5</p>
<p>Co-DETR은 이러한 문제들의 근본 원인이 DETR의 핵심 설계 철학인 ’One-to-One 집합 매칭’에 있다고 진단한다. 이 방식은 각 Ground-Truth 객체에 단 하나의 예측 쿼리만을 할당함으로써 인코더와 디코더에 극도로 희소한 감독 신호(sparse supervision)를 제공하게 되고, 이는 결국 학습의 비효율성과 불안정성으로 이어진다.8 Co-DETR, 즉 “DETRs with</p>
<p><strong>Co</strong>llaborative Hybrid Assignments Training“은 이 문제를 해결하기 위해 제안된 새로운 <em>훈련 패러다임</em>이다.2 이는 새로운 모델 아키텍처를 제안하는 것이 아니라, 기존 DETR 계열 모델의 훈련 과정에 ‘협력적 하이브리드 할당’ 방식을 도입하여 학습 과정을 최적화하는 방법론이다. 훈련 중에만 여러 보조 헤드(auxiliary heads)와 추가적인 긍정 쿼리(positive queries)를 활용하여 인코더와 디코더에 조밀한 감독 신호를 제공하고, 추론 시에는 이를 모두 제거하여 어떠한 추가적인 계산 비용 없이 모델의 성능을 극대화한다.8</p>
<p>이러한 접근 방식은 컴퓨터 비전 분야의 발전 방향이 단순히 더 깊고 복잡한 네트워크를 설계하는 것을 넘어, 주어진 아키텍처의 잠재력을 최대한 이끌어내는 정교한 학습 전략을 개발하는 방향으로 나아가고 있음을 시사한다. Co-DETR은 모델의 최종 성능이 아키텍처뿐만 아니라, 최적의 가중치를 찾아가는 ’훈련 경로’에도 크게 의존함을 보여준다. 따라서 Co-DETR은 특정 모델에 대한 개선책을 넘어, 희소하거나 불안정한 감독 신호를 갖는 다른 딥러닝 문제에도 적용될 수 있는 일반적인 ’훈련 가속 및 최적화 프레임워크’로 해석될 수 있다.</p>
<h2>2.  DETR 계열 모델의 발전과 한계 (Evolution and Limitations of DETR-based Models)</h2>
<h3>2.1  DETR의 구조와 핵심 원리 (Architecture and Core Principles of DETR)</h3>
<p>DETR의 구조는 이미지 특징 추출을 위한 CNN 백본, 전역적 문맥 이해를 위한 Transformer 인코더-디코더, 그리고 최종 예측을 위한 피드포워드 네트워크(FFN)로 구성된다. 먼저 ResNet과 같은 CNN 백본을 통해 입력 이미지에서 특징 맵을 추출하고, 이를 1차원 벡터로 펼친 후 위치 인코딩(positional encoding)을 더하여 Transformer 인코더의 입력 시퀀스로 사용한다.1 인코더는 셀프 어텐션을 통해 이미지의 전역적인 관계와 문맥을 학습한다. 디코더는 학습 가능한 N개의 임베딩, 즉 객체 쿼리(object queries)를 입력으로 받아 인코더의 출력과 교차 어텐션을 수행한다. 각 쿼리는 이미지 내 특정 객체의 존재 유무, 클래스, 그리고 위치를 예측하는 전문가 역할을 하도록 학습된다.3</p>
<p>모델이 예측한 N개의 결과와 실제 이미지에 존재하는 M개의 Ground-Truth 객체 집합 간의 최적의 일대일 매칭을 찾기 위해, DETR은 헝가리안 알고리즘(Hungarian algorithm)을 사용한 양자 매칭(Bipartite Matching)을 수행한다.1 이 매칭 결과를 기반으로 할당된 예측-정답 쌍에 대해 분류 손실(classification loss)과 경계 상자 손실(bounding box loss, L1 loss와 GIoU loss의 조합)을 계산하여 모델을 최적화한다. 이 구조의 가장 큰 의의는 NMS나 앵커 박스 같은 수동 설계 요소를 완전히 제거하여 완전한 End-to-End 학습을 구현했다는 점이다.2 NMS는 중복된 예측을 제거하는 후처리 과정으로, 종종 하이퍼파라미터에 민감하고 End-to-End 학습을 방해하는 요소로 작용했다.4 DETR은 집합 예측을 통해 자연스럽게 중복을 회피하므로 NMS가 불필요하다.</p>
<h3>2.2  DETR의 주요 한계점 분석 (Analysis of Key Limitations of DETR)</h3>
<p>DETR의 혁신적인 설계는 동시에 몇 가지 근본적인 한계점을 야기했다. 가장 큰 문제는 One-to-One 매칭에서 비롯된다.8 일반적으로 수백 개의 객체 쿼리가 사용되지만, 실제 이미지 내 객체 수는 수십 개에 불과하다. 따라서 극소수의 쿼리만이 긍정 샘플로 할당되고 나머지 대다수는 ‘배경(no object)’ 클래스로 취급된다. 이는 인코더가 출력하는 특징 맵의 대부분 영역이 유의미한 그래디언트를 받지 못하는 ‘희소 감독(sparse supervision)’ 문제를 야기한다.9</p>
<p>이러한 희소 감독은 모델의 수렴 속도를 현저히 저하시키는 주된 원인이 된다. 어텐션 모듈이 이미지의 중요한 영역에 집중하는 방법을 배우는 데 매우 오랜 시간이 걸리기 때문이다.5 실제로 Faster R-CNN과 같은 기존 탐지기들이 수십 에포크 만에 수렴하는 반면, DETR은 경쟁력 있는 성능을 위해 500 에포크에 달하는 긴 학습을 요구했다.4 또한, 작은 객체 탐지 성능 저하 문제도 심각했다. Transformer의 전역 어텐션은 계산 복잡도가 이미지 픽셀 수의 제곱에 비례(<span class="math math-inline">O(H^2W^2)</span>)하므로, 작은 객체 탐지에 필수적인 고해상도 특징 맵을 사용하기 어렵다.7 저해상도 특징 맵에서는 작은 객체의 정보가 소실되기 쉬우며 5, 전역 어텐션 메커니즘 자체가 몇 픽셀에 불과한 작은 객체보다 크고 지배적인 객체에 편향되는 경향이 있어 작은 객체를 놓치기 쉽다.5</p>
<h3>2.3  주요 변형 모델 분석: Deformable DETR과 DINO (Analysis of Key Variants: Deformable DETR and DINO)</h3>
<p>DETR의 한계를 극복하기 위해 다양한 변형 모델이 제안되었다. 그중 가장 대표적인 모델은 Deformable DETR과 DINO이다.</p>
<p><strong>Deformable DETR</strong>은 DETR의 높은 계산 복잡도와 느린 수렴 문제를 해결하기 위해 ‘Deformable Attention’ 메커니즘을 제안했다.6 이는 전체 특징 맵의 모든 픽셀과 어텐션을 계산하는 대신, 각 쿼리에 대해 소수의 핵심 샘플링 지점(key sampling points)에만 주의를 집중하는 방식이다.13 이를 통해 계산 복잡도를 선형 시간으로 크게 줄이고, 다중 스케일 특징 맵을 효율적으로 활용하여 작은 객체 탐지 성능을 개선했다. 결과적으로 Deformable DETR은 원본 DETR보다 10배 이상 단축된 학습 시간으로 더 높은 성능을 달성했다.6</p>
<p>**DINO (DETR with Improved DeNoising Anchor Boxes)**는 헝가리안 매칭의 불안정성을 해결하고 학습을 가속화하기 위해 ‘De-Noising(DN) 훈련’ 개념을 도입하고 발전시켰다.14 이 방식은 훈련 시 Ground-Truth 경계 상자에 의도적으로 노이즈를 추가한 뒤, 이를 디코더에 직접 입력하여 원래의 깨끗한 상자를 복원하도록 하는 보조 과제를 추가한 것이다.15 이 과정은 모델이 직접적으로 경계 상자를 예측하고 정제하는 법을 배우게 하여 매칭 과정을 안정화시키고 수렴을 가속하는 효과를 낳는다.16 DINO는 여기에 Contrastive Denoising Training, Mixed Query Selection 등 추가적인 개선 사항을 결합하여 당시 최고 수준(SOTA)의 성능을 달성했다.14</p>
<p>Deformable DETR과 DINO는 각각 계산 효율성과 매칭 안정성 측면에서 큰 발전을 이루었지만, DETR의 근본적인 One-to-One 매칭으로 인한 ‘희소 감독’ 문제를 완전히 해결하지는 못했다. 여전히 인코더는 제한된 수의 긍정 샘플로부터만 학습 신호를 받는다는 한계가 남아있었다. 이처럼 DETR의 발전사는 ’문제-해결’의 연쇄 반응으로 볼 수 있다. DETR이 NMS와 앵커 문제를 해결했지만 계산량과 수렴 속도 문제를 낳았고, Deformable DETR이 이를 해결했지만 매칭 불안정성과 희소 감독 문제가 남았다. DINO가 매칭 불안정성을 완화했지만, 희소 감독 문제는 여전히 근본적인 과제였다. Co-DETR은 바로 이 연쇄의 마지막 단계인 ‘희소 감독’ 문제를 정면으로 공략하는, 논리적 귀결점에 위치한 기술이라고 할 수 있다. 따라서 Co-DETR은 기존의 성공적인 변형 모델들(Deformable DETR, DINO 등)과 경쟁하는 관계가 아니라, 그들의 기반 위에서 시너지를 창출하는 보완적 관계에 있다.8</p>
<h2>3.  Co-DETR: 협력적 하이브리드 할당 훈련 패러다임 (Co-DETR: The Collaborative Hybrid Assignments Training Paradigm)</h2>
<h3>3.1  핵심 개념: One-to-One 매칭의 한계 극복 (Core Concept: Overcoming the Limitations of One-to-One Matching)</h3>
<p>Co-DETR의 핵심 철학은 DETR의 End-to-End 설계가 가지는 장점을 그대로 유지하면서, Faster R-CNN과 같은 전통적인 객체 탐지기에서 사용되던 One-to-Many 레이블 할당 방식의 장점, 즉 ’조밀한 감독’을 훈련 과정에 통합하는 것이다.9 이는 서로 다른 두 가지 접근법을 ’협력’시켜 양쪽의 이점을 모두 취하려는 하이브리드 전략이다. One-to-One 매칭은 NMS가 필요 없는 깔끔한 파이프라인을 보장하고, One-to-Many 할당은 모델이 더 빠르고 효과적으로 학습할 수 있도록 풍부한 정보를 제공한다.</p>
<h3>3.2  아키텍처 상세 분석 (Detailed Architectural Analysis)</h3>
<p>Co-DETR의 훈련 아키텍처는 기존 DETR 모델(백본 + 인코더 + 디코더)을 기반으로 확장된다. 인코더가 추출한 다중 스케일 특징 맵은 두 갈래로 나뉜다. 한 갈래는 원래의 DETR 디코더로 전달되어 기존과 동일하게 One-to-One 매칭을 수행한다. 다른 한 갈래는 여러 개의 병렬적인 ’보조 헤드(Auxiliary Heads)’로 전달된다.8</p>
<p>데이터 흐름을 상세히 살펴보면, 입력 이미지가 백본과 인코더를 통과해 특징 맵 F를 생성한다. F는 멀티스케일 어댑터를 거쳐 특징 피라미드 <span class="math math-inline">{F_1,..., F_J}</span>로 변환된다. 이 특징 피라미드는 K개의 보조 헤드와 메인 디코더 모두에 입력으로 사용된다. 각 보조 헤드는 독립적으로 One-to-Many 방식에 따라 예측을 수행하고 손실을 계산하여 인코더를 학습시킨다. 또한, 이 보조 헤드들에서 식별된 긍정 샘플들은 ’사용자 정의 긍정 쿼리’를 생성하는 데 사용되어 메인 디코더의 학습을 추가적으로 돕는다.9</p>
<p>Co-DETR의 가장 큰 실용적 장점 중 하나는 이러한 복잡한 구조가 오직 훈련 시에만 사용된다는 점이다. 훈련에 사용된 모든 보조 헤드와 사용자 정의 쿼리 생성 메커니즘은 추론 시에는 완전히 제거된다.8 따라서 추론 시간, 메모리 사용량, 모델 파라미터 수는 원래의 DETR 모델과 완벽하게 동일하다. 이는 성능 향상을 위해 어떠한 추론 비용도 추가되지 않는 ‘Overhead-Free’ 설계를 의미한다.</p>
<h3>3.3  협력적 학습의 원리: 보조 헤드와 One-to-Many 할당 (Principle of Collaborative Learning: Auxiliary Heads and One-to-Many Assignment)</h3>
<p>각 보조 헤드는 Faster R-CNN, ATSS, FCOS 등과 같이 과거에 검증된 효율적인 One-to-Many 레이블 할당 전략을 사용하도록 감독된다.8 이 방식들은 하나의 Ground-Truth 객체에 대해 여러 개의 긍정 샘플(예: 높은 IoU 값을 가지는 앵커 박스)을 할당한다.</p>
<p>이렇게 다양한 기준에 따라 할당된 긍정 샘플들은 공유된 인코더에 대해 풍부하고 조밀한 감독 신호를 제공한다. 결과적으로 인코더는 원래의 희소한 One-to-One 매칭 손실뿐만 아니라, 여러 개의 조밀한 One-to-Many 손실까지 동시에 최소화해야 하는 다중 과제 학습(multi-task learning)과 유사한 환경에 놓이게 된다. 이 과정은 인코더가 특정 할당 방식에 과적합되지 않고, 더 일반적이고 판별력 있는 강건한 특징 표현을 학습하도록 강제한다.9 이는 마치 여러 명의 각기 다른 전문가(보조 헤드)가 한 명의 학생(인코더)을 동시에 가르쳐 더 종합적이고 균형 잡힌 지식을 갖추게 하는 것과 같다. 이 다각적인 감독 방식은 단일한 One-to-One 매칭이 가질 수 있는 불안정성과 편향을 효과적으로 상쇄시킨다.</p>
<h3>3.4  사용자 정의 긍정 쿼리 생성을 통한 디코더 학습 강화 (Enhancing Decoder Training via Customized Positive Query Generation)</h3>
<p>Co-DETR은 인코더뿐만 아니라 디코더의 학습도 최적화한다. 각 보조 헤드는 One-to-Many 할당을 통해 다수의 긍정 샘플 좌표를 식별하는데, Co-DETR은 이 긍정 좌표들을 추출하여 ’사용자 정의 긍정 쿼리(Customized Positive Queries)’를 동적으로 생성한다.8 이 쿼리들은 해당 좌표의 위치 인코딩과 특징 맵 상의 특징 벡터를 결합하여 만들어진다.</p>
<p>생성된 긍정 쿼리들은 원래의 학습 가능한 객체 쿼리들과 함께 디코더에 입력된다. 이는 디코더가 학습 초반부터 무작위 탐색을 하는 대신, 확실한 긍정 샘플에 대해 집중적으로 학습할 기회를 제공한다. 결과적으로 디코더의 교차 어텐션 학습이 안정화되고, 헝가리안 매칭의 불안정성으로 인해 발생할 수 있는 비효율적인 학습 과정을 보완하여 전체 모델의 수렴을 가속화한다.8 이는 전문가(보조 헤드)가 찾아낸 ’확실한 정답’을 학생(디코더)에게 직접 알려주어 학습을 돕는 효율적인 과외와 같다.</p>
<h2>4.  수학적 공식화 및 손실 함수 (Mathematical Formulation and Loss Function)</h2>
<h3>4.1  전체 손실 함수 분석 (Analysis of the Global Loss Function)</h3>
<p>Co-DETR의 전체 학습 목표 <span class="math math-inline">\mathcal{L}^{global}</span>는 세 가지 주요 구성 요소의 가중 합으로 이루어진다. 수식은 다음과 같다 9:</p>
<p><span class="math math-display">
\mathcal{L}^{global} = \sum_{l=1}^{L}(\widetilde{\mathcal{L}}^{dec}_{l} + \lambda_{1}\sum_{i=1}^{K}\mathcal{L}^{dec}_{i, l} + \lambda_{2}\mathcal{L}^{enc})
</span><br />
각 항의 의미는 다음과 같다.</p>
<ul>
<li>
<p><strong>원본 디코더 손실 (<span class="math math-inline">\widetilde{\mathcal{L}}^{dec}_{l}</span>)</strong>: 이 항은 원래의 DETR 파이프라인에 해당하는 손실이다. l번째 디코더 레이어에서 나온 예측과 헝가리안 매칭을 통해 할당된 Ground-Truth 간의 손실(분류 손실 + 경계 상자 손실)을 의미한다. 이는 모델이 스스로 쿼리를 통해 객체를 찾는 능력을 유지하고 전체 End-to-End 파이프라인의 정합성을 보장하는 역할을 한다.</p>
</li>
<li>
<p><strong>보조 디코더 손실 (<span class="math math-inline">\mathcal{L}^{dec}_{i, l}</span>)</strong>: 이 항은 i번째 보조 헤드에서 생성된 사용자 정의 긍정 쿼리에 대한 디코더 손실이다. 이 쿼리들은 이미 긍정 샘플임이 명확하므로, 별도의 매칭 과정 없이 해당 Ground-Truth를 직접 예측하도록 학습된다. 이 손실은 디코더의 학습을 직접적으로 가이드하여 수렴을 돕는 역할을 한다.9</p>
</li>
<li>
<p><strong>인코더 손실 (<span class="math math-inline">\mathcal{L}^{enc}</span>)</strong>: 이 항은 K개의 보조 헤드 자체의 손실을 모두 합한 것이다. 각 보조 헤드는 One-to-Many 방식으로 할당된 긍정 및 부정 샘플에 대해 예측을 수행하고, 이에 대한 손실(<span class="math math-inline">\mathcal{L}_{i}^{enc}</span>)이 계산된다. 이 손실은 인코더가 다양한 관점에서 객체를 탐지할 수 있는 풍부한 특징을 학습하도록 직접적으로 감독하는 핵심적인 역할을 한다.9</p>
</li>
<li>
<p><strong>가중치 (<span class="math math-inline">\lambda_1, \lambda_2</span>)</strong>: 이들은 각 손실 항의 중요도를 조절하는 하이퍼파라미터로, 세 가지 손실 요소 간의 균형을 맞추는 역할을 한다.9</p>
</li>
</ul>
<p>이 손실 함수의 구조는 Co-DETR이 인코더와 디코더의 학습을 어떻게 분리하고 또 연결하는지를 명확히 보여준다. <span class="math math-inline">\mathcal{L}^{enc}</span>는 주로 인코더의 특징 표현력을 강화하는 데 초점을 맞추고, <span class="math math-inline">\mathcal{L}^{dec}_{i, l}</span>는 디코더의 수렴을 돕는 데 집중하며, <span class="math math-inline">\widetilde{\mathcal{L}}^{dec}_{l}</span>는 전체 파이프라인의 일관성을 유지한다. 이는 각 모듈의 역할을 명확히 분담시켜 학습의 안정성과 효율성을 동시에 달성하는 정교한 설계이다. 인코더에는 ’다양한 문제’를 풀어보게 하고, 디코더에는 ’쉬운 문제’부터 풀게 함으로써 전체 시스템의 학습을 최적화하는, 효율적인 학습 과정과 유사하다.</p>
<h2>5.  실험 결과 및 성능 분석 (Experimental Results and Performance Analysis)</h2>
<h3>5.1  COCO 데이터셋 성능 비교 분석 (Comparative Performance Analysis on the COCO Dataset)</h3>
<p>Co-DETR은 기존 SOTA 모델인 Deformable-DETR 및 DINO-Deformable-DETR에 적용되었을 때, 일관되고 상당한 성능 향상을 보였다.9 이는 Co-DETR이 특정 아키텍처에 국한되지 않고 DETR 계열 모델 전반에 적용될 수 있는 일반적인 훈련 패러다임임을 입증한다.</p>
<p>주요 결과는 다음과 같다:</p>
<ul>
<li>
<p>ResNet-50 백본을 사용하고 12 에포크만 학습한 조건에서, Co-Deformable-DETR은 49.5 AP를 달성하여 기존 Deformable-DETR을 크게 상회하는 성능을 보였다.9</p>
</li>
<li>
<p>Swin-L 백본을 사용한 Co-DINO-Deformable-DETR은 36 에포크 학습 후 60.0 AP를 기록했으며, 이는 기존 DINO 모델을 능가하는 수치다.9</p>
</li>
<li>
<p>ViT-L 백본을 사용한 Co-DETR은 COCO test-dev에서 66.0 AP라는 새로운 SOTA 기록을 달성하며, Co-DETR의 잠재력을 최대로 보여주었다.9</p>
</li>
</ul>
<p><strong>Table 1: COCO val SOTA 모델 성능 비교</strong> 9</p>
<table><thead><tr><th>Method</th><th>Backbone</th><th>#epochs</th><th>AP</th><th>AP50</th><th>AP75</th><th>APS</th><th>APM</th><th>APL</th></tr></thead><tbody>
<tr><td>Deformable-DETR</td><td>R50</td><td>50</td><td>46.9</td><td>65.6</td><td>51.0</td><td>29.6</td><td>50.1</td><td>61.6</td></tr>
<tr><td>DINO-Deformable-DETR†</td><td>R50</td><td>12</td><td>49.4</td><td>66.9</td><td>53.8</td><td>32.3</td><td>52.5</td><td>63.9</td></tr>
<tr><td><strong>Co-Deformable-DETR</strong></td><td><strong>R50</strong></td><td><strong>12</strong></td><td><strong>49.5</strong></td><td><strong>67.6</strong></td><td><strong>54.3</strong></td><td><strong>32.4</strong></td><td><strong>52.7</strong></td><td><strong>63.7</strong></td></tr>
<tr><td>DINO-Deformable-DETR†</td><td>Swin-L</td><td>36</td><td>58.5</td><td>77.0</td><td>64.1</td><td>41.5</td><td>62.3</td><td>74.0</td></tr>
<tr><td><strong>Co-DINO-Deformable-DETR†</strong></td><td><strong>Swin-L</strong></td><td><strong>36</strong></td><td><strong>60.0</strong></td><td><strong>77.7</strong></td><td><strong>66.1</strong></td><td><strong>44.6</strong></td><td><strong>63.9</strong></td><td><strong>75.7</strong></td></tr>
</tbody></table>
<p><strong>Table 2: COCO test-dev SOTA 모델 성능 비교</strong> 9</p>
<table><thead><tr><th>Method</th><th>Backbone</th><th>#params</th><th>APbox (test-dev)</th></tr></thead><tbody>
<tr><td>DINO</td><td>Swin-L</td><td>218M</td><td>63.3</td></tr>
<tr><td>Group DETRv2</td><td>ViT-H</td><td>629M</td><td>64.5</td></tr>
<tr><td>DINO</td><td>InternImage-G</td><td>3.0B</td><td>65.5</td></tr>
<tr><td><strong>Co-DETR</strong></td><td><strong>ViT-L</strong></td><td><strong>304M</strong></td><td><strong>66.0</strong></td></tr>
</tbody></table>
<h3>5.2  Ablation 연구: 각 구성 요소의 기여도 평가 (Ablation Studies: Evaluating the Contribution of Each Component)</h3>
<p>Co-DETR의 각 설계 요소가 성능에 미치는 영향을 분석하기 위해 상세한 Ablation 연구가 수행되었다.</p>
<p><strong>보조 헤드의 종류와 개수</strong>: 다양한 One-to-Many 헤드(RetinaNet, Faster-RCNN, ATSS 등)를 실험한 결과, 어떤 종류의 보조 헤드를 사용하더라도 베이스라인 성능을 일관되게 향상시켰으며, 그중 ATSS가 가장 좋은 성능을 보였다.9 보조 헤드의 개수(K)는 2개 또는 3개일 때 최적의 성능을 보였으며, 6개로 늘렸을 때는 오히려 성능이 소폭 하락했다. 이는 너무 많은 보조 헤드가 제공하는 그래디언트 신호들이 서로 상충하여 학습에 방해가 될 수 있음을 시사한다. 즉, ’감독의 양’뿐만 아니라 ’감독의 질과 다양성’이 중요하며, 과도한 감독은 오히려 해가 될 수 있다는 중요한 교훈을 준다.9</p>
<p><strong>Table 3: 보조 헤드 종류별 성능 비교 (Deformable-DETR, 36 epochs)</strong> 9</p>
<table><thead><tr><th>Auxiliary head</th><th>AP</th><th>AP50</th><th>AP75</th></tr></thead><tbody>
<tr><td>Baseline</td><td>43.3</td><td>62.3</td><td>47.1</td></tr>
<tr><td>Faster-RCNN</td><td>46.3</td><td>64.7</td><td>50.5</td></tr>
<tr><td>FCOS</td><td>46.5</td><td>64.8</td><td>50.7</td></tr>
<tr><td><strong>ATSS</strong></td><td><strong>46.8</strong></td><td><strong>65.1</strong></td><td><strong>51.5</strong></td></tr>
</tbody></table>
<p><strong>각 구성 요소의 효과</strong>: ’보조 헤드(aux head)’와 ’사용자 정의 긍정 쿼리(pos queries)’의 효과를 분리하여 실험한 결과, 두 구성 요소 모두 개별적으로 성능 향상에 크게 기여했으며, 함께 사용했을 때 시너지 효과가 가장 컸다.9 이는 두 메커니즘이 각각 인코더의 특징 학습과 디코더의 수렴 안정화라는 서로 다른 문제를 효과적으로 해결하고 있음을 실험적으로 입증한다.</p>
<p><strong>Table 4: 구성 요소별 기여도 분석 (Deformable-DETR, 36 epochs)</strong> 9</p>
<table><thead><tr><th>aux head</th><th>pos queries</th><th>AP</th><th>AP Gain</th></tr></thead><tbody>
<tr><td>✗</td><td>✗</td><td>43.3</td><td>-</td></tr>
<tr><td>✓</td><td>✗</td><td>46.2</td><td>+2.9</td></tr>
<tr><td>✗</td><td>✓</td><td>45.3</td><td>+2.0</td></tr>
<tr><td>✓</td><td>✓</td><td><strong>46.8</strong></td><td><strong>+3.5</strong></td></tr>
</tbody></table>
<h3>5.3  수렴 속도 분석 (Convergence Speed Analysis)</h3>
<p>Co-DETR은 최종 성능뿐만 아니라 학습 효율성 측면에서도 큰 개선을 보였다. 제공되는 조밀한 감독 신호 덕분에 모델의 수렴이 극적으로 가속화되었다. 예를 들어, Co-Deformable-DETR은 단 36 에포크 학습만으로, 100 에포크 또는 150 에포크를 학습한 베이스라인 Deformable-DETR보다도 높은 성능을 달성했다.9 이는 Co-DETR이 제한된 컴퓨팅 자원 환경에서도 더 빠르고 효율적으로 고성능 모델을 훈련시킬 수 있음을 의미한다.</p>
<h2>6.  응용 분야 및 향후 연구 방향 (Applications and Future Research Directions)</h2>
<h3>6.1  주요 응용 사례 (Key Application Cases)</h3>
<p>Co-DETR 훈련 패러다임은 특히 강건하고 정확한 객체 탐지가 요구되는 분야에서 큰 잠재력을 가진다. 대표적인 예로 자율 주행 차량(AVD)의 주변 환경 인식 시스템을 들 수 있다.2 Co-DETR을 통해 학습된 모델은 더 판별력 있는 특징을 학습하므로, 저조도, 악천후, 객체 가림 등 어려운 주행 환경에서 탐지 성능을 크게 향상시킬 수 있다.18 이는 자율 주행의 안전성과 신뢰성을 높이는 데 직접적으로 기여할 수 있다.</p>
<h3>6.2  향후 연구 방향 (Future Research Directions)</h3>
<p><strong>실시간 객체 탐지 모델로의 확장</strong>: Co-DETR은 훈련 비용이 다소 높지만 추론 비용은 전혀 없다는 특징을 가진다. 이는 RT-DETR과 같은 실시간 탐지 모델의 정확도를 훈련 단계에서 극대화하는 데 이상적인 방법론이 될 수 있다.21 흥미롭게도, 최근 제안된 RT-DETRv3의 ‘계층적 조밀 긍정 감독(hierarchical dense positive supervision)’ 방식은 Co-DETR의 핵심 아이디어(보조 브랜치를 통한 조밀한 감독)와 매우 유사한 접근법을 취한다.22 이는 Co-DETR이 제시한 방향이 DETR 연구의 자연스러운 다음 단계임을 강력하게 뒷받침한다. 이처럼 서로 다른 연구 그룹에서 유사한 해결책이 독립적으로 도출되었다는 사실은, 해당 아이디어가 특정 논문에 국한된 기법이 아니라 DETR 계열 모델의 훈련 비효율성을 해결하기 위한 ’수렴 진화(convergent evolution)’의 결과임을 보여준다.</p>
<p><strong>다른 비전 태스크로의 적용</strong>: Co-DETR의 ‘협력적 하이브리드 훈련’ 패러다임은 객체 탐지를 넘어 인스턴스 세분화(instance segmentation), 자세 추정(pose estimation) 등 다른 밀집 예측(dense prediction) 태스크에도 성공적으로 적용될 수 있다. 희소한 감독 신호로 인해 학습에 어려움을 겪는 모든 분야에서 모델의 성능과 수렴 속도를 개선할 잠재력을 가지고 있다.</p>
<p><strong>오픈 월드 및 제로샷 탐지로의 확장</strong>: Grounding DINO와 같은 최신 모델들은 텍스트 프롬프트를 사용하여 미리 정의되지 않은 객체를 탐지하는 개방형 어휘 탐지(open-vocabulary detection)를 수행한다.23 이러한 멀티모달 모델의 성능은 강력한 시각적 특징 인코더에 크게 의존한다. Co-DETR의 훈련 방식을 통해 학습된 더 강건하고 일반화된 시각적 특징 인코더는 이러한 모델들의 시각적 기반(visual grounding) 능력을 향상시켜, 더 정확하고 신뢰성 있는 오픈 월드 탐지를 가능하게 할 것이다.</p>
<h2>7.  결론 (Conclusion)</h2>
<p>Co-DETR은 새로운 모델 아키텍처를 제안하는 대신, 기존 DETR 계열 모델의 훈련 방식을 근본적으로 개선하는 혁신적인 ’훈련 패러다임’을 제시했다. ’협력적 하이브리드 할당’이라는 핵심 아이디어를 통해 One-to-One 매칭 방식의 고질적인 문제였던 희소 감독 문제를 효과적으로 해결했으며, 이를 통해 훈련 효율성과 최종 성능을 동시에 극대화했다.</p>
<p>연구 분야에서 Co-DETR은 Deformable DETR, DINO와 같은 아키텍처 개선의 흐름과 상호 보완적으로 작용하며, Transformer 기반 객체 탐지기의 성능을 새로운 차원으로 끌어올렸다. 특히 추론 비용의 증가 없이 오직 훈련 전략의 개선만으로 성능을 향상시키는 접근 방식은 실제 산업 현장에서의 적용 가능성 측면에서 매우 큰 가치를 지닌다.</p>
<p>궁극적으로 Co-DETR은 End-to-End 모델의 잠재력을 최대한 이끌어내기 위해서는 아키텍처 설계만큼이나 정교한 훈련 전략이 중요하다는 것을 명확히 보여주었다. 이 방법론은 향후 객체 탐지뿐만 아니라 다양한 컴퓨터 비전 분야에서 모델의 성능을 한 단계 끌어올리는 중요한 표준 도구 중 하나로 자리 잡을 가능성이 크다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Object detection Using Detection Transformer (Detr) on custom dataset - YouTube, https://www.youtube.com/watch?v=xuh37qziXnw</li>
<li>arXiv:2502.17843v1 [cs.CV] 25 Feb 2025, https://arxiv.org/pdf/2502.17843</li>
<li>End-to-End Detection Transformer (DETR) - - NeuralCeption -, https://www.neuralception.com/objectdetection-detr/</li>
<li>A Review of DEtection TRansformer: From Basic Architecture to Advanced Developments and Visual Perception Applications - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC12252279/</li>
<li>Deformable attention — tackling DETRs runtime complexity | by Tilo Flasche - Medium, https://medium.com/@tnodecode/multi-scale-deformable-attention-tackling-detrs-runtime-complexity-and-problems-with-small-9c35bd969f48</li>
<li>Deformable DETR: Deformable Transformers for End-to-End Object Detection. - GitHub, https://github.com/fundamentalvision/Deformable-DETR</li>
<li>Deformable DETR - arXiv, https://arxiv.org/pdf/2010.04159</li>
<li>Co-DETR. Tackling the instability of DETR’s… | by Tilo Flasche - Medium, https://medium.com/@tnodecode/co-detr-66d2f82f06c3</li>
<li>DETRs with Collaborative Hybrid Assignments … - CVF Open Access, https://openaccess.thecvf.com/content/ICCV2023/papers/Zong_DETRs_with_Collaborative_Hybrid_Assignments_Training_ICCV_2023_paper.pdf</li>
<li>DETR - Hugging Face, https://huggingface.co/docs/transformers/model_doc/detr</li>
<li>Review — Deformable Transformers for End-to-End Object Detection | by Sik-Ho Tsang, https://sh-tsang.medium.com/review-deformable-transformers-for-end-to-end-object-detection-e29786ef2b4c</li>
<li>[2010.04159] Deformable DETR: Deformable Transformers for End-to-End Object Detection - arXiv, https://arxiv.org/abs/2010.04159</li>
<li>Deformable DETR - Hugging Face, https://huggingface.co/docs/transformers/v4.26.1/model_doc/deformable_detr</li>
<li>Exploring the DINO Family Part 1: DINO, The Pioneering Object Detection Model | by Ideacvr, https://medium.com/@ideacvr2024/exploring-the-dino-family-part-1-dino-the-pioneering-object-detection-model-8e453eb5cd34</li>
<li>DN-DETR: Accelerate DETR Training by Introducing Query DeNoising - arXiv, https://arxiv.org/pdf/2203.01305</li>
<li>DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection, https://openreview.net/forum?id=3mRwyG5one</li>
<li>DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection, https://arxiv.org/abs/2203.03605</li>
<li>arXiv:2405.03519v1 [cs.CV] 6 May 2024, https://arxiv.org/pdf/2405.03519</li>
<li>Automatic Vehicle Detection using DETR: A Transformer-Based Approach for Navigating Treacherous Roads - arXiv, https://arxiv.org/html/2502.17843v1</li>
<li>arXiv:2405.03519v1 [cs.CV] 6 May 2024, <a href="https://arxiv.org/pdf/2405.03519">https://arxiv.org/pdf/2405.03519?</a></li>
<li>Baidu’s RT-DETR: A Vision Transformer-Based Real-Time Object Detector - Ultralytics Docs, https://docs.ultralytics.com/models/rtdetr/</li>
<li>RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision - arXiv, https://arxiv.org/html/2409.08475v3</li>
<li>Grounding DINO : SOTA Zero-Shot Object Detection - Roboflow Blog, https://blog.roboflow.com/grounding-dino-zero-shot-object-detection/</li>
<li>Zero-shot Object Detection Using Grounding DINO Base - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2024/10/grounding-dino-base/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>