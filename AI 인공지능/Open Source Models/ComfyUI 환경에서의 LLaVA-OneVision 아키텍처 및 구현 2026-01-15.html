<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:ComfyUI 환경에서의 LLaVA-OneVision 아키텍처 및 구현</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>ComfyUI 환경에서의 LLaVA-OneVision 아키텍처 및 구현</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">오픈 소스 AI 모델</a> / <span>ComfyUI 환경에서의 LLaVA-OneVision 아키텍처 및 구현</span></nav>
                </div>
            </header>
            <article>
                <h1>ComfyUI 환경에서의 LLaVA-OneVision 아키텍처 및 구현</h1>
<p>2026-01-15, G30DR</p>
<h2>1.  서론: 로컬 멀티모달 인텔리전스의 새로운 지평</h2>
<p>인공지능 기술의 민주화는 단순히 모델 소스의 공개를 넘어, 이를 구동할 수 있는 인터페이스의 접근성에 달려 있다. 최근 생성형 AI 생태계, 특히 <strong>ComfyUI</strong>로 대변되는 노드 기반의 워크플로우 환경은 단순한 이미지 생성을 넘어 복합적인 추론과 분석이 가능한 ‘에이전트(Agent)’ 시스템으로 진화하고 있다. 이러한 진화의 최전선에 등장한 모델이 바로 <strong>LLaVA-OneVision</strong>이며, 이를 일반 소비자용 하드웨어(GPU)에서 구동할 수 있도록 통합한 개발자 <strong>kijai</strong>의 커스텀 노드(<code>ComfyUI-LLaVA-OneVision</code>)는 로컬 AI 연구 및 활용에 있어 중대한 전환점을 시사한다.</p>
<p>본 보고서는 사용자의 GPU만으로 클라우드 API에 의존하지 않고 이미지와 비디오를 완벽하게 분석할 수 있는 LLaVA-OneVision 모델의 기술적 아키텍처와 ComfyUI 내에서의 구현 방법론, 그리고 실제 워크플로우 적용 전략을 심층적으로 분석한다. 특히 단일 이미지(Single-Image), 다중 이미지(Multi-Image), 비디오(Video) 시나리오를 하나의 모델로 통합하려는 ‘OneVision’ 전략의 기술적 함의와, 720억(72B) 파라미터 규모의 거대 모델을 로컬 환경에서 운용하기 위한 최적화 기법(양자화, Flash Attention 등)을 상세히 기술한다.</p>
<h2>2.  LLaVA-OneVision의 아키텍처적 기반과 기술적 특이점</h2>
<p>LLaVA-OneVision은 기존 오픈소스 멀티모달 대형 언어 모델(MLLM)들이 겪었던 고질적인 문제, 즉 고해상도 이미지 처리 능력의 부재와 비디오 이해 능력의 파편화를 해결하기 위해 설계된 <strong>LLaVA-NeXT</strong> 라인업의 정점이다. 이 모델은 시각적 입력을 처리하는 비전 인코더와 이를 언어적으로 해석하는 LLM 백본 간의 결합 방식에서 혁신적인 설계를 도입했다.</p>
<h3>2.1  하이브리드 인코더 전략: SigLIP과 Qwen2의 결합</h3>
<p>LLaVA-OneVision의 강력한 성능은 두 가지 핵심 컴포넌트의 유기적인 결합에서 비롯된다.</p>
<h4>2.1.1  비전 인코더: SigLIP (Sigmoid Loss for Language Image Pre-training)</h4>
<p>기존의 LLaVA 모델들이 주로 OpenAI의 CLIP-ViT-L/14를 사용했던 것과 달리, LLaVA-OneVision은 <strong>SigLIP</strong> 인코더를 채택했다. CLIP이 사용하는 Softmax 기반의 대조 학습(Contrastive Learning)은 배치(Batch) 크기에 민감하고 계산 효율성이 떨어지는 단점이 있었다. 반면, SigLIP은 Sigmoid 손실 함수를 사용하여 이미지-텍스트 쌍을 독립적으로 학습시킬 수 있어, 더 효율적인 학습과 정교한 시각적 특징 추출이 가능하다. 이는 모델이 이미지 내의 미세한 텍스트(OCR)나 복잡한 도표, 사물의 질감 등을 인식하는 데 있어 결정적인 성능 향상을 가져온다.</p>
<h4>2.1.2  언어 백본: Qwen2 (Alibaba Cloud)</h4>
<p>시각적 정보를 언어로 변환하고 추론하는 두뇌 역할은 <strong>Qwen2</strong> 모델이 담당한다. LLaVA-OneVision은 Qwen2-0.5B, 7B, 72B의 세 가지 변형을 제공한다. Qwen2를 선택한 기술적 배경에는 다음과 같은 이유가 있다:</p>
<ol>
<li><strong>다국어 처리 능력:</strong> Qwen2는 영어와 중국어뿐만 아니라 한국어를 포함한 다국어 데이터에 대해 탁월한 성능을 보인다. 이는 기존 Llama-3 기반 모델들이 한국어 처리에서 보여주었던 한계를 극복하고, 국내 사용자들이 별도의 번역 과정 없이 자연스러운 한국어로 시각적 질의응답을 수행할 수 있게 한다.</li>
<li><strong>확장된 컨텍스트 윈도우:</strong> 최대 32K 토큰(Token)에 달하는 컨텍스트 윈도우는 긴 비디오를 분석하거나 다수의 이미지를 동시에 비교 분석할 때 필수적이다. 비디오 분석 시 수천 개의 시각 토큰이 생성되는데, Qwen2의 긴 문맥 처리 능력은 이러한 대용량 정보를 손실 없이 유지하는 데 기여한다.</li>
</ol>
<h3>2.2  Anyres-9: 고해상도 이미지 처리의 핵심 기술</h3>
<p>LLaVA-OneVision의 가장 큰 특징 중 하나는 <strong>Anyres-9</strong>라 불리는 동적 해상도 처리 기술이다. 기존 MLLM들은 입력 이미지를 224x224 또는 336x336과 같은 고정된 저해상도로 리사이징하여 처리했기 때문에, 고해상도 이미지의 디테일이 소실되는 문제가 발생했다.</p>
<p>Anyres-9 전략은 다음과 같은 매커니즘으로 작동한다:</p>
<ol>
<li><strong>그리드 분할(Grid Splitting):</strong> 입력 이미지를 <span class="math math-inline">3 \times 3</span> 그리드, 즉 9개의 패치(Patch)로 분할한다.</li>
<li><strong>전역 컨텍스트(Global Context):</strong> 분할된 9개의 패치 외에, 원본 이미지를 리사이징한 전체 뷰(Global View) 1개를 추가한다.</li>
<li><strong>토큰화:</strong> 총 10개의 이미지(9개 패치 + 1개 전체 뷰)가 각각 비전 인코더를 통과한다. 각 패치는 약 729개의 토큰으로 변환되므로, 단 한 장의 고해상도 이미지를 처리하는 데 약 <strong>7,290개</strong>(<span class="math math-inline">729 \times 10</span>)의 시각 토큰이 생성된다.</li>
</ol>
<p>이러한 방식은 이미지를 단순히 압축하는 것이 아니라, 원본의 해상도를 최대한 보존하면서 LLM이 이해할 수 있는 형태로 변환한다. 이는 ComfyUI 워크플로우에서 사용자가 4K 해상도의 이미지를 입력했을 때, 모델이 이미지 구석에 있는 작은 글씨나 배경의 미세한 디테일까지 정확하게 캡션할 수 있는 이유이다. 하지만 이는 동시에 VRAM 사용량을 급격히 증가시키는 원인이 되기도 하므로, 하드웨어 사양에 따른 적절한 설정이 요구된다.</p>
<h3>2.3  비디오 풀링(Video Pooling)과 시간적 이해</h3>
<p>이미지 처리에서는 Anyres-9으로 디테일을 극대화하지만, 비디오 처리에서는 정반대의 전략인 **풀링(Pooling)**을 사용한다. 비디오는 시간의 흐름에 따른 연속된 프레임의 집합이므로, 모든 프레임을 Anyres-9으로 처리할 경우 토큰 수가 기하급수적으로 늘어나(예: 30프레임 <span class="math math-inline">\times</span> 7,290토큰 = 218,700토큰) 현존하는 어떤 GPU에서도 처리할 수 없게 된다.</p>
<p>이를 해결하기 위해 LLaVA-OneVision은 비디오 프레임에 대해 다음과 같은 압축 전략을 취한다:</p>
<ul>
<li><strong>프레임당 토큰 압축:</strong> 비디오의 각 프레임은 양선형 보간(Bilinear Interpolation) 및 풀링 과정을 거쳐 단 <strong>196개</strong>의 토큰으로 압축된다.</li>
<li><strong>프레임 샘플링:</strong> 긴 비디오에서 대표성을 띠는 프레임(주로 16~32프레임)을 추출하여 시퀀스로 구성한다.</li>
<li><strong>총 토큰량:</strong> 32프레임 비디오를 분석할 경우, 약 6,000개(<span class="math math-inline">32 \times 196</span>)의 시각 토큰이 생성된다. 이는 Qwen2의 32K 컨텍스트 윈도우 내에 충분히 안착하며, 텍스트 프롬프트와의 상호작용을 위한 여유 공간을 남겨둔다.</li>
</ul>
<p>이러한 이원화된 처리 방식(이미지는 고해상도 패치, 비디오는 압축 토큰) 덕분에 LLaVA-OneVision은 ’OneVision’이라는 이름처럼 단일 모델로 정적 이미지와 동적 영상을 모두 효율적으로 처리할 수 있다.</p>
<h2>3.  ComfyUI 통합 구현: kijai의 커스텀 노드 분석</h2>
<p>개발자 kijai가 공개한 <code>ComfyUI-LLaVA-OneVision</code> 노드는 복잡한 Python 추론 코드를 ComfyUI의 그래픽 인터페이스로 완벽하게 이식했다. 이 노드 팩은 단순한 래퍼(Wrapper)를 넘어, 메모리 관리, 데이터 전처리, 모델 로딩 최적화 등 실사용을 위한 다양한 기능을 내장하고 있다.</p>
<h3>3.1  <code>(Down)Load LLaVA-OneVision Model</code> 노드 상세 분석</h3>
<p>이 노드는 모델의 초기화를 담당하며, 사용자의 하드웨어 환경에 맞춰 모델을 최적화하여 로드하는 핵심 기능을 수행한다.</p>
<h4>3.1.1  모델 변형(Variant) 선택: <code>si</code> vs <code>ov</code></h4>
<p>사용자는 <code>model_id</code> 파라미터를 통해 다양한 체크포인트를 선택할 수 있다. 여기서 접미사 <code>si</code>와 <code>ov</code>의 차이를 이해하는 것이 중요하다.</p>
<ul>
<li><strong><code>si</code> (Single-Image):</strong> 단일 이미지 분석에 특화되어 훈련된 모델이다. 고해상도 이미지의 OCR이나 정밀한 객체 탐지가 주 목적일 때 유리하다. 비디오 처리 능력은 상대적으로 제한적일 수 있다.</li>
<li><strong><code>ov</code> (OneVision):</strong> 단일 이미지, 다중 이미지, 비디오 데이터를 모두 포함하여 학습된 범용 모델이다. 시간적 맥락을 이해하거나 여러 장의 이미지를 비교하는 작업에 필수적이다. ComfyUI에서 비디오 분석 워크플로우를 구축할 때는 반드시 <code>ov</code> 모델을 선택해야 한다.</li>
</ul>
<h4>3.1.2  정밀도(Precision) 및 양자화 설정</h4>
<p>이 노드는 <code>transformers</code> 라이브러리와 <code>bitsandbytes</code>를 활용하여 다양한 정밀도 설정을 지원한다. 이는 VRAM 용량이 제한적인 소비자용 GPU에서 72B와 같은 거대 모델을 구동하는 데 결정적인 역할을 한다.</p>
<ul>
<li><strong>FP16 / BF16:</strong> 원본 가중치 정밀도. 7B 모델 구동 시 약 16GB 이상의 VRAM이 필요하다.</li>
<li><strong>4-bit (NF4):</strong> <code>load_in_4bit</code> 옵션을 활성화하면 모델 가중치를 4비트로 양자화하여 로드한다. 이 경우 7B 모델은 약 6~8GB VRAM, 72B 모델은 약 40~48GB VRAM(추론 오버헤드 포함) 수준에서 구동이 가능해진다. kijai의 노드는 이러한 양자화 과정을 자동화하여 사용자가 복잡한 설정 없이 체크박스 하나로 메모리 절약을 할 수 있게 돕는다.</li>
</ul>
<h3>3.2  <code>LLaVA-OneVision Run</code> 노드 메커니즘</h3>
<p>실제 추론이 이루어지는 이 노드는 이미지/비디오 입력, 텍스트 프롬프트, 그리고 생성 제어 변수들을 처리한다.</p>
<ul>
<li><strong>동적 입력 처리:</strong> <code>image</code> 입력 단자는 단일 이미지 텐서뿐만 아니라 비디오 프레임 리스트(Batch)를 모두 수용한다. 내부적으로 입력 데이터의 형태를 분석하여 Anyres-9 처리를 할지, 비디오 풀링 처리를 할지 결정한다.</li>
<li><strong>프롬프트 템플릿 자동화:</strong> Qwen2 모델은 특정 채팅 템플릿(Chat Template)을 준수해야 한다(예: <code>&lt;|im_start|&gt;user...</code>). 이 노드는 사용자가 입력한 자연어 프롬프트를 모델이 요구하는 포맷으로 자동 변환하고, 시각 토큰(<code>&lt;image&gt;</code>)을 적절한 위치에 삽입한다.</li>
<li><strong>생성 제어 파라미터:</strong></li>
<li><code>max_tokens</code>: 생성할 텍스트의 최대 길이를 제한한다. 비디오 분석과 같이 상세한 설명이 필요한 경우 512 이상으로 설정하는 것이 좋다.</li>
<li><code>temperature</code>: 텍스트 생성의 창의성을 조절한다. 팩트 기반의 분석(OCR, 객체 카운팅)에서는 0.1~0.3의 낮은 값을, 창의적인 캡션 생성에서는 0.7 정도의 값을 권장한다.</li>
</ul>
<h3>3.3  <code>OneVision Caption Folder</code> 노드</h3>
<p>대량의 데이터셋 구축을 위해 설계된 이 노드는 지정된 폴더 내의 모든 이미지를 순차적으로 로드하여 캡션을 생성한다.</p>
<ul>
<li><strong>배치 처리 효율성:</strong> 모델을 한 번만 로드한 상태에서 연속적으로 추론을 수행하므로, 매번 모델을 로드/언로드하는 방식보다 훨씬 빠른 속도로 데이터셋 라벨링이 가능하다.</li>
<li><strong>Flux/SD 학습 연동:</strong> 여기서 생성된 고품질의 텍스트 캡션은 <code>.txt</code> 파일로 저장되어, Flux.1이나 Stable Diffusion 모델의 LoRA(Low-Rank Adaptation) 학습 데이터로 즉시 활용될 수 있다.</li>
</ul>
<h2>4.  하드웨어 요구 사항 및 성능 최적화 가이드</h2>
<p>LLaVA-OneVision은 강력한 성능만큼이나 높은 하드웨어 사양을 요구한다. 특히 “로컬” 구동을 위해서는 VRAM 관리가 필수적이다. 다음은 각 모델 사이즈별 VRAM 요구 사항과 최적화 전략에 대한 상세 분석이다.</p>
<h3>4.1  모델별 VRAM 점유율 분석</h3>
<p>아래 표는 ComfyUI 환경에서 <code>transformers</code> 백엔드를 기준으로 측정한 대략적인 VRAM 소모량이다. (시스템 오버헤드 및 컨텍스트 길이에 따라 변동 가능)</p>
<table><thead><tr><th><strong>모델 크기 (파라미터)</strong></th><th><strong>정밀도 설정</strong></th><th><strong>모델 가중치 VRAM</strong></th><th><strong>KV 캐시 및 활성화 VRAM (추정)</strong></th><th><strong>권장 GPU VRAM</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>0.5B (Qwen2)</strong></td><td>FP16</td><td>~1 GB</td><td>~1-2 GB</td><td><strong>4 GB</strong></td><td>구형 노트북이나 저사양 GPU에서도 구동 가능.</td></tr>
<tr><td><strong>7B (Qwen2)</strong></td><td>FP16</td><td>~14 GB</td><td>~2-4 GB</td><td><strong>24 GB</strong> (RTX 3090/4090)</td><td>16GB VRAM 카드에서는 OOM 발생 가능성 높음.</td></tr>
<tr><td><strong>7B (Qwen2)</strong></td><td><strong>4-bit (NF4)</strong></td><td><strong>~5 GB</strong></td><td>~2-4 GB</td><td><strong>8-12 GB</strong> (RTX 3060/4070)</td><td><strong>가장 대중적인 설정.</strong> 12GB VRAM으로 쾌적하게 구동 가능.</td></tr>
<tr><td><strong>72B (Qwen2)</strong></td><td>FP16</td><td>~144 GB</td><td>~10+ GB</td><td>A100 80GB x 2</td><td>소비자용 하드웨어 구동 불가.</td></tr>
<tr><td><strong>72B (Qwen2)</strong></td><td><strong>4-bit (NF4)</strong></td><td><strong>~40 GB</strong></td><td>~5-10 GB</td><td><strong>48 GB</strong> (RTX 6000 Ada / Dual 3090)</td><td>전문가용 워크스테이션 또는 듀얼 GPU 환경 필요.</td></tr>
<tr><td><strong>72B (Qwen2)</strong></td><td>GGUF (Q4_K_M)</td><td>시스템 RAM 의존</td><td>GPU VRAM</td><td><strong>24 GB + 64GB RAM</strong></td><td><code>ComfyUI-GGUF</code> 노드 활용 시 속도는 느리지만 구동 가능.</td></tr>
</tbody></table>
<h3>4.2  필수 최적화 기술: Flash Attention 2</h3>
<p>Qwen2 기반 모델의 성능을 극대화하기 위해서는 <strong>Flash Attention 2</strong>의 적용이 필수적이다. 이는 어텐션 연산의 메모리 접근 비용을 획기적으로 줄여주는 기술로, 특히 긴 시퀀스(비디오 처리)에서 큰 효과를 발휘한다.</p>
<ul>
<li><strong>효과:</strong> 추론 속도 2~3배 향상 및 피크 메모리 사용량 감소.</li>
<li><strong>설치 난이도:</strong> Windows 환경에서는 사전 컴파일된 <code>flash-attn</code> 휠(whl) 파일을 구해서 수동으로 설치해야 하는 경우가 많다. kijai의 노드는 이를 지원하지만, 환경 설정이 올바르지 않으면 자동으로 비활성화되거나 오류를 발생시킬 수 있다.</li>
</ul>
<h3>4.3  시스템 RAM 오프로딩 (Low VRAM Mode)</h3>
<p>ComfyUI는 VRAM이 부족할 경우 모델의 일부 레이어를 시스템 RAM(DDR4/DDR5)으로 내리는 기능을 기본 제공한다. LLaVA-OneVision 7B FP16 모델을 12GB GPU에서 구동할 경우, 초과되는 용량만큼 시스템 RAM을 사용하게 되어 속도는 느려지지만 OOM(Out of Memory) 오류 없이 실행은 가능하다. 하지만 72B 모델의 경우 CPU 오프로딩 시 추론 속도가 현저히 저하되므로 실시간 애플리케이션에는 부적합하다.</p>
<h2>5.  실전 워크플로우 시나리오</h2>
<p>kijai의 LLaVA-OneVision 노드를 활용하여 구축할 수 있는 구체적인 워크플로우 사례를 통해 이 기술의 실용성을 탐구한다.</p>
<h3>5.1  시나리오 A: 합성 데이터 생성을 위한 고밀도 이미지 캡셔닝</h3>
<p>최신 이미지 생성 모델(Flux.1, SD3)은 자연어 프롬프트를 깊이 있게 이해한다. 따라서 파인튜닝(Fine-tuning)을 위한 데이터셋 준비 과정에서 이미지의 내용을 아주 상세하게 묘사하는 “Dense Captioning“이 필수적이다.</p>
<ol>
<li><strong>입력:</strong> <code>Load Image Directory</code> 노드를 통해 수천 장의 학습용 이미지 폴더를 연결한다.</li>
<li><strong>처리:</strong> <code>OneVision Caption Folder</code> 노드를 연결하고, 프롬프트로 *“Describe this image in extreme detail, focusing on lighting, texture, camera angle, and artistic style.”*을 입력한다.</li>
<li><strong>모델 설정:</strong> 7B-si (Single Image) 모델을 4-bit로 로드하여 처리 속도를 높인다.</li>
<li><strong>출력:</strong> 각 이미지 파일명과 동일한 <code>.txt</code> 파일이 생성된다.</li>
<li><strong>결과:</strong> 기존의 간단한 태그(예: <code>1girl, running, outdoors</code>) 대신, *“A cinematic shot of a young woman sprinting through a sun-dappled forest, dynamic motion blur, high contrast lighting…”*와 같은 풍부한 묘사를 얻을 수 있어, 학습된 모델의 프롬프트 반응성을 극대화한다.</li>
</ol>
<h3>5.2  시나리오 B: 비디오 내용 기반 검색 및 요약 (Semantic Video Search)</h3>
<p>개인 소장 비디오나 CCTV 영상에서 특정 사건을 찾아내는 시스템을 로컬에서 구축할 수 있다.</p>
<ol>
<li><strong>비디오 로드:</strong> <code>VHS_LoadVideo</code> 노드를 사용하여 비디오 파일을 로드한다.</li>
<li><strong>프레임 샘플링:</strong> <code>GetFrames</code> 노드(별도 커스텀 노드 필요 가능)를 통해 비디오 전체에서 32개의 주요 프레임을 균등 추출한다.</li>
<li><strong>추론:</strong> <code>LLaVA-OneVision Run</code> 노드(7B-ov 모델 사용)에 비디오 프레임 배치를 연결한다.</li>
<li><strong>프롬프트:</strong> <em>“이 비디오의 주요 사건을 시간 순서대로 요약해줘. 그리고 만약 ’빨간색 자동차’가 등장한다면 그 시점과 상황을 정확히 명시해.”</em></li>
<li><strong>출력:</strong> 모델은 비디오의 흐름을 이해하고, *“00:15초경 빨간색 스포츠카가 좌측에서 우측으로 지나감.”*과 같은 분석 결과를 텍스트로 반환한다. 이는 클라우드로 영상을 업로드하지 않고도 개인정보 보호가 보장된 상태에서 영상 분석을 수행할 수 있음을 의미한다.</li>
</ol>
<h3>5.3  시나리오 C: 생성형 에이전트 루프 (Self-Correcting Generation)</h3>
<p>LLaVA-OneVision을 ’감시자(Critic)’로 활용하여 이미지 생성의 품질을 스스로 검증하고 수정하는 워크플로우다.</p>
<ol>
<li><strong>이미지 생성:</strong> Flux 또는 SDXL 모델로 이미지를 생성한다.</li>
<li><strong>검증(Evaluation):</strong> 생성된 이미지를 <code>LLaVA-OneVision Run</code> 노드에 입력한다.</li>
<li><strong>프롬프트:</strong> <em>“이 이미지에서 손가락의 개수와 형태가 해부학적으로 정확한지 평가해. 만약 기형적이거나 오류가 있다면 ‘FAIL’, 완벽하다면 ’PASS’라고만 대답해.”</em></li>
<li><strong>조건부 분기:</strong> LLaVA의 텍스트 출력을 <code>String Match</code> 노드(Logic 노드류)로 판별한다. ’FAIL’이 나오면 시드(Seed)를 변경하여 다시 이미지를 생성하도록 루프를 돌린다.</li>
<li><strong>의의:</strong> 사용자가 일일이 결과물을 확인하지 않아도, AI가 스스로 품질 관리를 수행하는 자율 에이전트 시스템의 기초가 된다.</li>
</ol>
<h2>6.  LLaVA-OneVision 성능 벤치마크 및 비교</h2>
<p>객관적인 성능 지표를 통해 LLaVA-OneVision이 현재 오픈소스 생태계에서 갖는 위치를 확인해본다.</p>
<table><thead><tr><th><strong>벤치마크 (항목)</strong></th><th><strong>LLaVA-OneVision 7B</strong></th><th><strong>LLaVA-OneVision 72B</strong></th><th><strong>GPT-4o (유사 체급 추정)</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>DocVQA (문서 이해)</strong></td><td><strong>73.7%</strong></td><td>-</td><td>80%대 상회</td><td>오픈소스 7B 모델 중 최상위권 OCR 성능.</td></tr>
<tr><td><strong>MMMU (멀티모달 이해)</strong></td><td>50% 중반</td><td><strong>60%대</strong></td><td>69%</td><td>복잡한 대학 수준의 문제를 푸는 데 있어 72B 모델은 상용 모델에 근접함.</td></tr>
<tr><td><strong>Video-MME (비디오)</strong></td><td>우수함</td><td><strong>SOTA급</strong></td><td>-</td><td>긴 비디오의 맥락 이해에서 ‘ov’ 모델의 강점이 드러남.</td></tr>
<tr><td><strong>MathVista (수학 시각화)</strong></td><td>경쟁력 있음</td><td>매우 우수</td><td>-</td><td>도표 및 기하학적 문제 해결 능력 탁월.</td></tr>
</tbody></table>
<p>특히 <strong>한국어 성능</strong>에 있어서 Qwen2 기반인 LLaVA-OneVision은 Llama-3 기반의 LLaVA 모델보다 월등히 자연스러운 문장 생성 능력을 보여준다. 이는 국내 사용자들에게 있어 번역기를 거치지 않는 직관적인 워크플로우를 가능하게 하는 중요한 요소다.</p>
<h2>7.  설치 가이드 및 트러블슈팅</h2>
<p>ComfyUI에서 LLaVA-OneVision을 성공적으로 구동하기 위한 설치 절차와 흔히 발생하는 문제들에 대한 해결책이다.</p>
<h3>7.1  설치 필수 의존성 (Dependencies)</h3>
<p>ComfyUI의 Python 환경(주로 <code>python_embeded</code>)에 다음 패키지들이 최신 버전으로 설치되어 있어야 한다.</p>
<ul>
<li><code>transformers</code>: 4.37.0 이상 권장. 최신 아키텍처 지원을 위해 필수.</li>
<li><code>accelerate</code>: 대용량 모델 로딩 및 오프로딩 관리.</li>
<li><code>bitsandbytes</code>: 4-bit 양자화를 위한 핵심 라이브러리. Windows 지원 버전 설치 필요.</li>
<li><code>flash-attn</code>: (선택 사항이나 권장) 설치 시 CUDA 버전과 정확히 일치하는 whl 파일을 찾아 <code>pip install</code> 해야 함.</li>
</ul>
<h3>7.2  트러블슈팅 (FAQ)</h3>
<p><strong>Q1: 모델 로딩 중 <code>ImportError: cannot import name 'LlavaOnevisionForConditionalGeneration'</code> 오류가 발생합니다.</strong></p>
<ul>
<li><strong>원인:</strong> <code>transformers</code> 라이브러리의 버전이 너무 낮아 해당 모델 아키텍처를 인식하지 못하는 경우입니다.</li>
<li><strong>해결:</strong> ComfyUI 매니저를 통해 <code>transformers</code>를 업데이트하거나, 터미널에서 <code>pip install -U transformers</code>를 실행하여 최신 버전으로 업데이트해야 합니다.</li>
</ul>
<p><strong>Q2: 4-bit 로딩을 시도했는데 <code>CUDA Setup failed</code> 오류가 뜹니다.</strong></p>
<ul>
<li><strong>원인:</strong> Windows 환경에서 <code>bitsandbytes</code> 라이브러리가 호환되지 않거나 DLL 파일이 누락된 경우입니다.</li>
<li><strong>해결:</strong> <code>bitsandbytes-windows</code>와 같은 호환 패키지를 설치하거나, kijai가 제공하는 워크플로우 가이드를 참조하여 환경 변수를 설정해야 합니다.</li>
</ul>
<p><strong>Q3: 비디오 처리 시 VRAM이 폭발적으로 증가합니다.</strong></p>
<ul>
<li><strong>원인:</strong> <code>max_frames</code> 설정이 너무 높거나, 입력 해상도가 너무 큰 경우일 수 있습니다.</li>
<li><strong>해결:</strong> <code>max_frames</code>를 16 또는 24로 줄이거나, 비디오 로드 노드에서 <code>force_size</code>를 사용하여 입력 해상도를 720p 이하로 낮추는 것이 좋습니다. 또한 <code>ov</code> 모델을 사용 중인지 확인하십시오.</li>
</ul>
<h2>8.  결론 및 향후 전망</h2>
<p>kijai의 <strong>ComfyUI-LLaVA-OneVision</strong> 노드 공개는 로컬 AI 사용자들에게 ’눈’을 달아준 것과 다름없다. 텍스트를 입력하면 이미지를 그려주는 단방향적 창작에서, 이제는 AI가 사용자가 제공한 시각 정보를 이해하고, 분석하고, 이를 바탕으로 새로운 창작을 제안하는 양방향적 상호작용이 가능해졌다.</p>
<p><strong>핵심 요약:</strong></p>
<ol>
<li><strong>통합된 시각 지능:</strong> 단일 이미지, 다중 이미지, 비디오를 아우르는 아키텍처는 워크플로우의 파편화를 막고 일관된 성능을 제공한다.</li>
<li><strong>로컬 구동의 현실화:</strong> 4-bit 양자화 기술 덕분에 RTX 3060/4070 등 보급형 GPU 사용자도 최신 SOTA(State-of-the-Art) VLM을 활용할 수 있게 되었다.</li>
<li><strong>한국어 친화성:</strong> Qwen2 백본의 강력한 다국어 능력은 국내 사용자들에게 큰 이점이다.</li>
</ol>
<p>향후 이 기술은 <strong>‘비디오-투-비디오(Video-to-Video)’</strong> 생성 기술과 결합하여, AI가 영화의 콘티를 분석하고 자동으로 편집하거나, 스타일을 변환하는 등 영상 제작 프로세스 전반을 혁신할 것으로 전망된다. 또한, GGUF 포맷 지원의 안정화와 함께, 더 적은 VRAM으로도 72B급 모델을 구동할 수 있는 최적화 기술들이 지속적으로 등장할 것이다. 이제 ComfyUI는 단순한 이미지 생성 도구를 넘어, 시각적 추론과 창작이 결합된 종합 AI 워크스테이션으로 거듭나고 있다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>kijai/ComfyUI-LLaVA-OneVision - GitHub, https://github.com/kijai/ComfyUI-LLaVA-OneVision</li>
<li>LLaVA-OneVision: Easy Visual Task Transfer, https://llava-vl.github.io/blog/2024-08-05-llava-onevision/</li>
<li>LLaVA-OneVision - Hugging Face, https://huggingface.co/docs/transformers/model_doc/llava_onevision</li>
<li>#211 LLaVA-OneVision: Easy Visual Task Transfer - YouTube, https://www.youtube.com/watch?v=VVcVpZxpgdU</li>
<li>lmms-lab/llava-onevision-qwen2-7b-ov - Hugging Face, https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-ov</li>
<li>Expanding Frontiers in Korean Vision-Language Models - arXiv, https://arxiv.org/html/2411.19103v1</li>
<li>Qwen2-VL-7B-Instruct — PaddleNLP 文档, https://paddlenlp.readthedocs.io/zh/latest/website/Qwen/Qwen2-VL-7B-Instruct/</li>
<li>LLaVA-OneVision - Hugging Face, https://huggingface.co/docs/transformers/v4.52.3/model_doc/llava_onevision</li>
<li>LLaVA-NeXT/docs/LLaVA_OneVision.md at main - GitHub, https://github.com/LLaVA-VL/LLaVA-NeXT/blob/main/docs/LLaVA_OneVision.md</li>
<li>ComfyUI Llava-OneVision detailed guide - RunComfy, https://www.runcomfy.com/comfyui-nodes/ComfyUI-LLaVA-OneVision</li>
<li>(Down)Load LLaVA-OneVision Model - RunComfy, https://www.runcomfy.com/comfyui-nodes/ComfyUI-LLaVA-OneVision/DownloadAndLoadLLaVAOneVisionModel</li>
<li>1월 15, 2026에 액세스, [https://dataloop.ai/library/model/lmms-lab_llava-onevision-qwen2-72b-si/#:<sub>:text=The%20LLaVA%2DOneVision%20model%20is,and%20respond%20to%20visual%20information.](https://dataloop.ai/library/model/lmms-lab_llava-onevision-qwen2-72b-si/#:</sub>:text=The LLaVA-OneVision model is, <a href="https://dataloop.ai/library/model/lmms-lab_llava-onevision-qwen2-72b-si/#:~:text=The%20LLaVA-OneVision%20model%20is,and%20respond%20to%20visual%20information.">https://dataloop.ai/library/model/lmms-lab_llava-onevision-qwen2-72b-si/#:~:text=The%20LLaVA%2DOneVision%20model%20is,and%20respond%20to%20visual%20information.</a></li>
<li>Llava Onevision Qwen2 72b Si · Models - Dataloop, https://dataloop.ai/library/model/lmms-lab_llava-onevision-qwen2-72b-si/</li>
<li>llava-hf/llava-onevision-qwen2-7b-si-hf · Hugging Face, https://huggingface.co/llava-hf/llava-onevision-qwen2-7b-si-hf</li>
<li>LLaVA-OneVision Run - RunComfy, https://www.runcomfy.com/comfyui-nodes/ComfyUI-LLaVA-OneVision/LLaVA_OneVision_Run</li>
<li>OneVision Caption Folder - ComfyUI Cloud - Comfy.ICU, https://comfy.icu/node/OneVisionCaptionFolder</li>
<li>GPU Buying Guide for AI Art - ComfyUI Wiki, https://comfyui-wiki.com/en/install/install-comfyui/gpu-buying-guide</li>
<li>What’s the minimum system config to be able to create videos?, https://www.reddit.com/r/comfyui/comments/1jkvh50/whats_the_minimum_system_config_to_be_able_to/</li>
<li>lmms-lab/llava-onevision-qwen2-7b-ov - Hugging Face, https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-ov/discussions/6</li>
<li>ComfyUI only using 50% of my VRAM? #1043 - GitHub, https://github.com/Comfy-Org/ComfyUI/discussions/1043</li>
<li>gokayfem/ComfyUI_VLM_nodes: Custom ComfyUI nodes … - GitHub, https://github.com/gokayfem/ComfyUI_VLM_nodes</li>
<li>Sage Attention &amp; Flash Attention for latest Comfyui v0.3.75 windows, https://www.reddit.com/r/comfyui/comments/1p8xibn/sage_attention_flash_attention_for_latest_comfyui/</li>
<li>perhaps missing flash attention in Comfy-Portable-windows? #1, https://github.com/kijai/ComfyUI-MochiWrapper/issues/1</li>
<li>Using ComfyUI low VRAM mode, Stable Cascade and NVIDIA RTX …, https://www.youtube.com/watch?v=cqBwhpcNGF0</li>
<li>Llava-Onevision-Qwen2-0.5b-Ov Free Chat Online – skywork.ai, https://skywork.ai/blog/models/llava-onevision-qwen2-0-5b-ov-free-chat-online-skywork-ai/</li>
<li>LLaVA-VL/LLaVA-NeXT - GitHub, https://github.com/LLaVA-VL/LLaVA-NeXT</li>
<li>Expanding Frontiers in Korean Vision-Language Models - Liner, https://liner.com/review/varcovision-expanding-frontiers-in-korean-visionlanguage-models</li>
<li>Llava-llama is huge. OOM. · Issue #114 · kijai/ComfyUI … - GitHub, https://github.com/kijai/ComfyUI-HunyuanVideoWrapper/issues/114</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>