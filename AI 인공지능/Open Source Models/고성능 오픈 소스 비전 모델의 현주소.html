<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:고성능 오픈 소스 비전 모델의 현주소</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>고성능 오픈 소스 비전 모델의 현주소</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">오픈 소스 AI 모델</a> / <span>고성능 오픈 소스 비전 모델의 현주소</span></nav>
                </div>
            </header>
            <article>
                <h1>고성능 오픈 소스 비전 모델의 현주소</h1>
<h2>1. 현대 비전 AI 생태계의 이해</h2>
<h3>1.1 패러다임의 전환: 컴퓨터 비전의 현재</h3>
<p>컴퓨터 비전 분야는 지난 십여 년간 괄목할 만한 발전을 거듭해왔습니다. 초기 AlexNet, VGG, ResNet과 같은 합성곱 신경망(Convolutional Neural Networks, CNN)이 지배하던 시대를 지나, 현재는 비전 트랜스포머(Vision Transformers, ViT)와 대규모 멀티모달 모델(Large Multimodal Models, LMM)이 최첨단(State-of-the-Art, SOTA) 성능의 기준을 새롭게 정립하고 있습니다.1 현대 비전 AI의 혁신은 여러 전선에서 동시에 일어나고 있습니다. 한편에서는 ConvNeXt와 같이 순수 CNN 아키텍처를 현대적으로 재해석하여 성능을 극한으로 끌어올리고 있으며, 다른 한편에서는 Swin Transformer V2처럼 트랜스포머 아키텍처의 확장성과 효율성을 최적화하고 있습니다. 더 나아가, LLaVA나 InternVL과 같은 모델들은 시각 정보 처리 능력에 언어 이해 능력을 결합하여 ’비전 모델’의 정의 자체를 확장하고 있습니다.4</p>
<h3>1.2 핵심 비전 과제의 분류</h3>
<p>고성능 비전 모델들의 성능을 가늠하는 주요 시험대는 다음과 같은 핵심 과제들입니다.1 이 안내서 전반에 걸쳐 모델들은 이 과제들을 기준으로 평가될 것입니다.</p>
<ul>
<li><strong>이미지 분류 (Image Classification):</strong> 입력 이미지 전체에 대해 단일 레이블(예: ‘고양이’, ‘개’)을 할당하는 가장 기본적인 과제입니다. ImageNet과 같은 벤치마크가 주로 사용됩니다.</li>
<li><strong>객체 탐지 (Object Detection):</strong> 이미지 내 다수 객체의 위치를 경계 상자(bounding box)로 특정하고, 각 객체를 분류하는 과제입니다. 이미지 분류와 위치 추정(localization)이 결합된 형태이며, MS COCO 벤치마크가 표준으로 사용됩니다.1</li>
<li><strong>이미지 분할 (Image Segmentation):</strong> 이미지를 픽셀 수준에서 이해하는 가장 정교한 과제로, 다음과 같은 세부 유형으로 나뉩니다.</li>
<li><strong>시맨틱 분할 (Semantic Segmentation):</strong> 이미지의 모든 픽셀을 특정 클래스(예: ‘도로’, ‘하늘’, ‘나무’)로 분류합니다. 동일 클래스의 개별 인스턴스는 구분하지 않습니다.</li>
<li><strong>인스턴스 분할 (Instance Segmentation):</strong> 시맨틱 분할에서 더 나아가 동일 클래스 내의 개별 객체 인스턴스까지 구분합니다(예: ‘고양이_1’, ‘고양이_2’).</li>
<li><strong>파놉틱 분할 (Panoptic Segmentation):</strong> 시맨틱 분할과 인스턴스 분할을 통합한 과제로, 모든 픽셀에 대해 클래스 레이블과 인스턴스 ID를 모두 할당합니다.</li>
</ul>
<h3>1.3 벤치마크와 평가 지표의 역할과 한계</h3>
<p>모델의 성능을 공정하게 비교하기 위해서는 표준화된 벤치마크와 평가 지표가 필수적입니다. ImageNet 7, MS COCO 9, ADE20K 11 등은 학계와 산업계에서 널리 인정받는 표준 벤치마크입니다. 본 안내서에서는 다음과 같은 핵심 지표를 사용하여 모델들을 분석할 것입니다.</p>
<ul>
<li><strong>Top-1 정확도 (Top-1 Accuracy):</strong> 이미지 분류에서 모델이 예측한 최상위 클래스가 정답과 일치할 확률입니다.12</li>
<li><strong>평균 정밀도 (mean Average Precision, mAP):</strong> 객체 탐지 성능을 종합적으로 평가하는 핵심 지표로, 다양한 IoU(Intersection over Union) 임계값에 대한 정밀도-재현율 곡선의 아래 면적을 평균 내어 계산합니다.13</li>
<li><strong>연산량 (FLOPs / Mult-Adds):</strong> 모델의 순방향 패스에 필요한 부동소수점 연산의 총량으로, 모델의 계산 복잡성을 나타냅니다.12</li>
<li><strong>파라미터 수 (Parameter Count):</strong> 모델이 가진 학습 가능한 가중치의 총개수로, 모델의 크기와 메모리 요구량을 가늠하는 척도입니다.14</li>
<li><strong>지연 시간 (Latency):</strong> 단일 입력을 처리하는 데 걸리는 시간으로, 모델의 실제 추론 속도를 나타냅니다.6</li>
</ul>
<p>그러나 이러한 표준 벤치마크에는 명백한 한계가 존재합니다. 연구에 따르면, 많은 모델들이 COCO와 같은 특정 벤치마크에 ’과적합’되는 경향을 보이며, 이는 실험실 환경의 이상적인 이미지를 넘어서는 현실 세계의 복잡하고 ‘지저분한(messy)’ 데이터에 대해서는 낮은 성능으로 이어질 수 있습니다.9 실제로 ImageNet 데이터셋에 대해 높은 정확도를 보이는 모델들이 ImageNet-D와 같이 실제 왜곡을 모방한 데이터셋에서는 성능이 급격히 하락하는 현상이 관찰되었습니다.15 이는 벤치마크 점수가 모델의 우수성을 보장하는 필요조건일 뿐, 충분조건은 아님을 시사합니다. 진정한 고성능 모델은 벤치마크 점수뿐만 아니라, 실제 환경에서의 강건성(robustness)과 일반화 능력까지 갖추어야 합니다.</p>
<h3>1.4 오픈 소스의 중요성: 라이선스와 접근성</h3>
<p>고성능 모델을 논할 때 기술적 측면만큼이나 중요한 것이 바로 ’오픈 소스’의 법적, 실용적 의미입니다. 모델의 소스 코드 공개 여부를 넘어, 어떤 라이선스 하에 배포되는지가 모델의 활용 범위를 결정짓기 때문입니다. 오픈 소스 라이선스는 크게 두 가지로 나뉩니다.16</p>
<ul>
<li><strong>허용적 라이선스 (Permissive Licenses):</strong> MIT, Apache 2.0 등이 대표적입니다. 원 저작자 표시 등 최소한의 의무만 지키면 파생 저작물을 독점 소프트웨어로 만들거나 상업적으로 판매하는 등 거의 모든 행위를 허용합니다. 이는 기술의 광범위한 채택과 상업적 활용을 장려합니다.18</li>
<li><strong>카피레프트 라이선스 (Copyleft Licenses):</strong> GNU GPL(General Public License)이 대표적입니다. 파생 저작물 역시 원 저작물과 동일한 라이선스(즉, GPL)로 공개해야 한다는 ‘감염성’ 조항을 가집니다. 이는 소프트웨어의 자유를 보장하고 모든 기여가 커뮤니티에 환원되도록 하는 데 목적이 있습니다.16</li>
</ul>
<p>최근에는 AI 모델, 데이터, 가중치 등 복합적인 구성 요소를 포괄하는 OpenMDW와 같은 AI 특화 라이선스도 등장하며 생태계의 복잡성을 더하고 있습니다.19 따라서 모델을 선택할 때, 그 모델의 라이선스가 프로젝트의 목적(학술 연구, 상업용 제품 개발 등)과 부합하는지 면밀히 검토하는 것은 기술적 성능을 분석하는 것만큼이나 중요한 전략적 결정입니다.</p>
<h2>2. 합성곱 아키텍처의 진화: ConvNeXt V2</h2>
<h3>2.1 년대를 위한 ConvNet</h3>
<p>ConvNeXt는 비전 트랜스포머의 부상에 대한 직접적인 응답으로 탄생했습니다. 이 모델은 점진적으로 ViT의 성공적인 설계 원칙들을 도입하여 고전적인 ConvNet을 현대화했으며, 이를 통해 순수 합성곱 아키텍처가 여전히 SOTA 수준의 성능을 달성할 수 있음을 증명했습니다.2 ConvNeXt V1이 지도 학습(supervised learning) 환경에서 아키텍처의 우수성을 입증했다면, ConvNeXt V2는 한 걸음 더 나아가 마스크드 오토인코더(Masked Autoencoders, MAE)와 같은 자기 지도 학습(self-supervised learning) 프레임워크와의 ’공동 설계(co-design)’에 초점을 맞추어 개발되었습니다.21</p>
<h3>2.2 아키텍처 심층 분석</h3>
<p>ConvNeXt의 핵심은 깊이별 합성곱(depthwise convolution), 역병목 구조(inverted bottleneck), 그리고 Swin Transformer에서 영감을 받은 큰 커널 크기(large kernel size)를 사용하는 ConvNeXt 블록에 있습니다.2 ConvNeXt V2의 가장 중요한 혁신은 바로 <strong>전역 응답 정규화(Global Response Normalization, GRN)</strong> 레이어의 도입입니다.</p>
<p>연구진들은 ConvNeXt 아키텍처와 MAE 사전 학습을 단순 결합했을 때 기대 이하의 성능을 보이는 문제를 발견했습니다.21 그 원인은 MAE 방식이 특징 다양성(feature diversity)으로부터 큰 이점을 얻지만, 표준 ConvNeXt 구조는 이를 충분히 촉진하지 못했기 때문입니다. GRN은 바로 이 문제를 해결하기 위해, 즉 채널 간 특징 경쟁을 강화하고 특징의 다양성을 높이기 위해 고안되었습니다.21</p>
<p>GRN의 작동 메커니즘은 다음과 같은 3단계로 구성됩니다 21:</p>
<ol>
<li>
<p>전역 특징 집계 (Global Feature Aggregation): 각 채널의 공간적 특징들을 하나의 값으로 집계합니다. 논문에서는 L2-norm을 사용하는 것이 가장 효과적임을 보였습니다. 입력 특징 맵 <span class="math math-inline">X \in \mathbb{R}^{H \times W \times C}</span>의 각 채널 Xi에 대해, 전역 특징 벡터 <span class="math math-inline">gx</span>의 <span class="math math-inline">i</span>번째 요소는 다음과 같이 계산됩니다.<br />
<span class="math math-display">
gx_i= \| X_i \| _2
</span></p>
</li>
<li>
<p>특징 정규화 (Feature Normalization): 집계된 값들을 모든 채널에 걸쳐 정규화하여 각 채널의 상대적 중요도를 계산합니다.<br />
<span class="math math-display">
nx_i=\frac{\|Xi\|_2}{\sum_{j=1}^C\|X_j\|_2}
</span><br />
특징 보정 (Feature Calibration): 계산된 정규화 점수를 원래의 특징 맵에 곱하여 채널별 응답을 보정합니다.<br />
<span class="math math-display">
X_i&#39;=X_i⋅nx_i
</span><br />
안정적인 학습을 위해 학습 가능한 파라미터 <span class="math math-inline">γ</span>, <span class="math math-inline">β</span>와 잔차 연결(residual connection)이 추가된 최종 GRN 블록의 공식은 다음과 같습니다.21</p>
</li>
</ol>
<p><span class="math math-display">
X_i \leftarrow \gamma \cdot \left(X_i \cdot \frac{\|X_i\|_2}{\sum_{j=1}^{C} \|X_j\|_2 + \epsilon}\right) + \beta + X_i
</span></p>
<p>여기서 <span class="math math-inline">ϵ</span>은 <span class="math math-inline">0</span>으로 나누는 것을 방지하기 위한 작은 상수입니다. 이처럼 GRN은 핵심적인 부분에 학습 가능한 파라미터가 없어 매우 단순하면서도 강력한 성능 향상을 이끌어냅니다.</p>
<h3>2.3 성능 분석 및 벤치마킹</h3>
<p>ConvNeXt V2는 순수 ConvNet으로서 SOTA 수준의 성능을 달성했습니다. 특히 공개된 데이터셋만으로 학습한 Huge 모델은 ImageNet-1K 데이터셋에서 **Top-1 정확도 88.9%**라는 놀라운 성과를 기록했습니다.21 또한 COCO 객체 탐지, ADE20K 시맨틱 분할과 같은 다운스트림 과제에서도 뛰어난 성능을 보여, 범용적인 백본(backbone) 아키텍처로서의 가치를 입증했습니다.21 ConvNeXt V2는 Atto(3.7M 파라미터)부터 Huge(650M 파라미터)까지 다양한 크기의 모델을 제공하여, 사용 환경에 따른 유연한 선택이 가능합니다.21</p>
<h3>2.4 강점, 약점 및 라이선스</h3>
<ul>
<li><strong>강점:</strong> 표준 ConvNet의 단순성과 효율성을 유지하면서도 트랜스포머와 대등하거나 그 이상의 성능과 확장성을 보여줍니다.20 GRN 레이어는 간단한 구조로 의미 있는 성능 향상을 이끌어내는 효율적인 설계의 좋은 예입니다.21</li>
<li><strong>약점:</strong> 가장 큰 규모의 트랜스포머 기반 멀티모달 모델들이 제공하는 제로샷 추론(zero-shot reasoning)과 같은 능력은 순수 ConvNet 아키텍처로는 구현하기 어렵습니다. 또한, 초기 ConvNeXt와 MAE의 결합 시도가 실패했던 것처럼, 아키텍처가 특정 학습 방법에 민감하게 반응할 수 있습니다.21</li>
<li><strong>라이선스:</strong> 공식 코드는 Facebook Research(현 Meta AI)에서 공개했으며, 일반적으로 MIT와 같은 허용적 라이선스를 따릅니다. 이는 학술 및 상업적 용도로 자유롭게 활용할 수 있음을 의미합니다. <code>timm</code>, <code>Hugging Face Transformers</code>와 같은 주요 라이브러리에서도 널리 지원되어 접근성이 매우 높습니다.22</li>
</ul>
<p>이러한 ConvNeXt V2의 성공은 아키텍처와 학습 방법론이 분리될 수 없으며, 반드시 ’공동 설계’되어야 한다는 중요한 교훈을 남깁니다. SOTA 아키텍처와 SOTA 사전 학습 방법을 단순히 결합하는 것만으로는 최적의 결과를 보장할 수 없으며, 두 요소의 상호작용을 깊이 이해하고 그에 맞춰 아키텍처를 수정(GRN 도입)하는 과정이 SOTA 달성의 핵심이었던 것입니다. 이는 또한 비전 AI 분야가 트랜스포머라는 단일 아키텍처로 수렴되는 것이 아니라, ConvNet과 같은 검증된 패러다임 내에서도 여전히 혁신과 성능 향상의 여지가 충분함을 보여주는 증거이기도 합니다.</p>
<h2>3. 비전 트랜스포머의 지배</h2>
<h3>3.1 Swin Transformer V2: 용량과 해상도의 확장</h3>
<h4>3.1.1 서론</h4>
<p>Swin Transformer는 계층적 구조(hierarchical structure)와 이동된 윈도우(shifted windows) 기반의 셀프 어텐션을 도입하여, ViT를 범용 컴퓨터 비전 백본으로 실용화한 선구적인 아키텍처입니다.3 기존 ViT의 고정된 해상도와 입력 크기에 대한 제곱의 복잡도 문제를 해결했기 때문입니다. Swin Transformer V2는 여기서 한 걸음 더 나아가, 모델을 수십억 개의 파라미터 규모로 확장할 때 발생하는 근본적인 문제들을 해결하는 데 초점을 맞춥니다.25</p>
<h4>3.1.2 아키텍처 심층 분석 및 핵심 혁신</h4>
<p>대규모 비전 모델을 훈련할 때 두 가지 주요 문제가 발생합니다: 훈련 불안정성, 그리고 사전 학습과 미세 조정 간의 해상도 불일치입니다. Swin V2는 이 문제들을 해결하기 위해 세 가지 핵심 기술을 제안합니다.25</p>
<ol>
<li><strong>훈련 불안정성 문제와 해결책 (Res-Post-Norm &amp; Scaled Cosine Attention):</strong></li>
</ol>
<ul>
<li>
<p><strong>문제:</strong> Pre-LN(Layer Normalization) 구조를 사용하는 기존 ViT는 모델이 깊어질수록 특정 레이어의 활성화 값이 폭발적으로 증가하여 훈련이 불안정해지는 현상을 보였습니다.25</p>
</li>
<li>
<p><strong>해결책 1 (Res-Post-Norm):</strong> LN 레이어를 각 잔차 블록의 시작이 아닌 끝으로 이동시키는 Post-LN 방식을 적용했습니다. 이는 메인 브랜치에 병합되기 전에 출력을 정규화하여, 네트워크가 깊어져도 활성화 값의 진폭이 안정적으로 유지되도록 합니다.25</p>
</li>
<li>
<p><strong>해결책 2 (Scaled Cosine Attention):</strong> 표준 내적(dot-product) 어텐션을 스케일링된 코사인(scaled cosine) 함수로 대체했습니다. 코사인 어텐션은 본질적으로 정규화되어 있어 입력 특징의 크기에 덜 민감하며, 일부 픽셀 쌍이 어텐션 맵을 지배하는 현상을 완화하여 훈련을 더욱 안정화시킵니다. 어텐션 계산은 다음과 같이 수정됩니다.<br />
<span class="math math-display">
   \text{Attention}(Q, K, V) = \text{SoftMax}\left(\frac{\cos(Q, K)}{\tau} + B\right)V
</span><br />
여기서 \tau는 학습 가능한 온도 파라미터이고, B는 위치 편향입니다.</p>
</li>
</ul>
<ol start="2">
<li><strong>해상도 불일치 문제와 해결책 (Log-Spaced Continuous Position Bias):</strong></li>
</ol>
<ul>
<li><strong>문제:</strong> 저해상도 이미지로 사전 학습된 모델을 고해상도 이미지로 미세 조정할 때, 위치 편향(position bias)을 보간(interpolation)하는 과정에서 성능 저하가 발생합니다.</li>
<li><strong>해결책 (Log-CPB):</strong> 상대 좌표를 로그 공간(log-space)으로 변환한 후, 작은 메타 네트워크를 통해 연속적인 위치 편향 값을 동적으로 생성합니다. 이 방식은 해상도 변경 시 필요한 외삽(extrapolation) 비율을 크게 줄여, 모델이 다양한 해상도에 걸쳐 부드럽게 적응하고 성능을 유지할 수 있도록 합니다.25</li>
</ul>
<h4>3.1.3 성능, 데이터 및 라이선스</h4>
<p>Swin Transformer V2는 이러한 혁신을 바탕으로 엄청난 규모까지 성공적으로 확장되었습니다. SwinV2-G(Giant) 모델은 ImageNet-1K에서 **Top-1 정확도 90.17%**를, COCO test-dev 데이터셋에서는 <strong>63.1 box mAP</strong>를 달성하며 SOTA 성능을 기록했습니다.26 이러한 성능을 달성하기 위해 JFT-3B의 7천만 개 이미지 부분집합이나 비공개 ImageNet-22K 확장 데이터셋과 같은 초거대 데이터셋을 사전 학습에 활용했습니다.25</p>
<ul>
<li><strong>라이선스:</strong> 공식 코드는 Microsoft에서 공개했으며, 일반적으로 MIT와 같은 허용적 라이선스를 채택하여 학술 및 상업적 활용에 용이합니다.26</li>
</ul>
<h3>3.2 EVA-02: 네온 제네시스를 위한 시각적 표현</h3>
<h4>3.2.1 서론</h4>
<p>EVA-02는 ’더 크고 더 많은 데이터’라는 일반적인 스케일업 경쟁에서 벗어나, 훈련 효율성과 접근성에 초점을 맞춘 모델입니다. 이 모델의 목표는 중간 크기의 모델과 공개적으로 접근 가능한 데이터만을 사용하여 SOTA 성능을 달성함으로써, 강력한 비전 모델에 대한 접근을 민주화하는 것입니다.29</p>
<h4>3.2.2 아키텍처 심층 분석 및 핵심 혁신</h4>
<p>EVA-02의 핵심 아이디어는 마스크드 이미지 모델링(MIM)을 사용한 사전 학습에 있지만, 그 목표 설정 방식에 독창성이 있습니다.29</p>
<ul>
<li><strong>핵심 아이디어:</strong> 표준 ViT를 MIM 방식으로 사전 학습합니다.</li>
<li><strong>핵심 혁신 (Teacher-Student 학습):</strong> 모델이 마스킹된 이미지 패치를 보고 원본 픽셀이나 이산적인 토큰을 복원하도록 하는 대신, EVA-02는 <strong>강력하고, 사전 학습되었으며, 고정된(frozen) 교사 모델(teacher model)의 특징 표현을 재구성</strong>하도록 학습됩니다. 이 연구에서는 10억 파라미터 규모의 EVA-CLIP 비전 인코더가 교사 모델로 사용되었습니다.</li>
<li><strong>작동 메커니즘:</strong></li>
</ul>
<ol>
<li>입력 이미지에 블록 단위로 마스크를 적용합니다(예: 40% 마스킹).</li>
<li>마스킹되지 않은 패치들을 학생 모델인 EVA-02에 입력합니다.</li>
<li>EVA-02는 마스킹되었던 영역에 대해, <strong>고정된 EVA-CLIP 교사 모델이 생성했을 특징 벡터를 예측</strong>하도록 훈련됩니다.</li>
<li>손실 함수는 학생 모델의 예측과 교사 모델의 실제 특징 벡터 간의 음의 코사인 유사도(negative cosine similarity)로 계산됩니다.</li>
</ol>
<h4>3.2.3 성능, 데이터 및 라이선스</h4>
<p>EVA-02는 파라미터 대비 성능 효율성에서 놀라운 결과를 보여주었습니다. 304M(3억 4백만)개의 파라미터를 가진 EVA-02-L 모델은 ImageNet-1K에서 **Top-1 정확도 90.0%**라는 경이로운 성능을 달성했습니다.29 제로샷 성능 또한 뛰어나, CLIP 인코더로 사용될 때 ImageNet에서 80.4%의 정확도를 기록하여 훨씬 더 큰 모델들을 능가했습니다.30</p>
<p>중요한 점은 이 모든 성과가 ImageNet-21K와 같은 공개 데이터셋과, 가장 큰 모델의 경우에도 3,800만 장 규모의 공개 데이터셋 병합본을 사용하여 이루어졌다는 것입니다.29 이는 수십억 장 규모의 비공개 데이터셋에 의존하는 다른 모델들과의 핵심적인 차별점입니다.</p>
<ul>
<li><strong>라이선스:</strong> BAAI(베이징 인공지능 연구원)에서 “개방형 접근과 개방형 연구를 촉진하기 위해“라는 명시적인 목표 하에 허용적 라이선스로 코드와 모델을 공개했습니다.29</li>
</ul>
<p>Swin V2와 EVA-02는 SOTA 성능 달성에 대한 두 가지 상반된 철학을 보여줍니다. Swin V2가 막대한 컴퓨팅과 데이터를 동원하여 성능의 한계를 밀어붙이는 ‘더 크게(bigger is better)’ 접근 방식을 따른다면, EVA-02는 우월한 교사 모델로부터 지식을 효율적으로 증류(distillation)하여 훨씬 적은 자원으로 비슷한 수준의 성능을 달성하는 ‘더 똑똑하게(smarter, not harder)’ 접근 방식을 따릅니다. 이는 자원이 한정된 대부분의 연구 기관이나 기업에게 EVA-02의 지식 전달 철학이 훨씬 더 실용적이고 비용 효율적인 SOTA 달성 경로임을 시사합니다. 동시에, EVA-02의 성공은 그 자체로 완결된 것이 아니라, 그 배경에 EVA-CLIP과 같은 더 강력한 교사 모델의 존재가 필수적이었음을 보여줍니다. 이는 AI 생태계가 거대 자본을 투입해 ’교사 모델’을 만드는 소수의 연구 그룹과, 그 지식을 증류하여 더 작고 효율적인 ’학생 모델’을 만드는 광범위한 커뮤니티로 계층화되고 있음을 암시합니다.</p>
<h2>4. 객체 탐지의 최전선</h2>
<h3>4.1 YOLO 시리즈: 실시간 탐지의 재정의</h3>
<h4>4.1.1 서론</h4>
<p>YOLO(You Only Look Once) 계열은 속도(지연 시간, FPS)와 정확도(mAP) 사이의 효과적인 균형을 통해 <em>실시간</em> 객체 탐지 분야의 지배적인 패러다임으로 자리 잡았습니다.33 최신 버전인 YOLOv9과 YOLOv10은 정보 흐름의 근본적인 문제와 후처리 과정을 개선하여 이 성능-효율성 경계를 더욱 확장합니다.</p>
<h4>4.1.2 YOLOv9: 원하는 것을 학습하는 방법</h4>
<p>YOLOv9은 딥 네트워크의 정보 병목 현상, 즉 데이터가 깊은 층을 통과하면서 예측에 필요한 핵심 정보가 손실되는 문제를 해결하는 데 중점을 둡니다.35</p>
<ul>
<li><strong>핵심 혁신 1: 프로그래머블 그래디언트 정보 (Programmable Gradient Information, PGI):</strong> 훈련 과정 중에 ’보조 가역 브랜치(auxiliary reversible branch)’를 추가합니다. 이 브랜치는 손실 함수가 기울기를 계산할 때 필요한 ‘완전한’ 정보를 제공하여, 메인 네트워크가 손실되거나 왜곡되지 않은 신뢰할 수 있는 그래디언트를 통해 가중치를 업데이트하도록 돕습니다. 이 보조 브랜치는 추론 시에는 제거되므로 추가적인 계산 비용이 발생하지 않습니다.</li>
<li><strong>핵심 혁신 2: 일반화된 효율적 레이어 집계 네트워크 (Generalized Efficient Layer Aggregation Network, GELAN):</strong> 기울기 경로 계획(gradient path planning)에 기반하여 설계된 새롭고 가벼우며 유연한 네트워크 아키텍처입니다. GELAN은 PGI와 결합되었을 때 매우 높은 효율성과 성능을 보여줍니다.</li>
</ul>
<h4>4.1.3 YOLOv10: 실시간 종단간 객체 탐지</h4>
<p>YOLOv10은 YOLO 계열의 오랜 숙제였던 후처리 의존성 문제를 해결하여 진정한 종단간(end-to-end) 탐지를 목표로 합니다.34</p>
<ul>
<li><strong>핵심 혁신 1: NMS-Free 훈련을 위한 일관된 이중 할당 (Consistent Dual Assignments):</strong> 비최대 억제(Non-Maximum Suppression, NMS)는 중복된 경계 상자를 제거하는 필수 후처리 과정이지만, 추론 속도를 저해하는 병목 구간이었습니다. YOLOv10은 훈련 중에 두 개의 예측 헤드를 사용하는 이중 할당 전략을 도입합니다. 하나는 기존 YOLO처럼 최적화에 유리한 일대다(one-to-many) 할당을, 다른 하나는 NMS가 필요 없는 일대일(one-to-one) 할당을 수행합니다. 두 헤드 간의 예측이 일관되도록 손실 함수를 설계하여, 풍부한 감독 하에 훈련하면서도 추론 시에는 NMS 없이 빠른 속도를 달성합니다.</li>
<li><strong>핵심 혁신 2: 총체적 효율성-정확도 기반 모델 설계 (Holistic Efficiency-Accuracy Driven Model Design):</strong> 경량화된 분류 헤드, 공간-채널 분리 다운샘플링, 순위 기반 블록 설계 등 아키텍처의 다양한 구성 요소를 효율성과 정확도 관점에서 전면적으로 최적화하여 계산 중복성을 크게 줄였습니다.</li>
</ul>
<h4>4.1.4 비교 성능 및 라이선스</h4>
<p>YOLOv9과 YOLOv10은 다양한 모델 크기(T, S, M, C, B, L, X, E)에서 SOTA 수준의 실시간 성능을 보여줍니다. 예를 들어, YOLOv10-S는 비슷한 AP 성능을 가진 RT-DETR-R18보다 1.8배 빠르며, YOLOv10-B는 동일한 성능의 YOLOv9-C에 비해 지연 시간은 46%, 파라미터는 25% 더 적습니다.34 아래 표는 각 모델의 상세 성능 지표입니다.34</p>
<table><thead><tr><th>모델</th><th>파라미터 (M)</th><th>FLOPs (G)</th><th>Latency (ms)</th><th>COCO APval (%)</th></tr></thead><tbody>
<tr><td>YOLOv9-T</td><td>2.0</td><td>7.7</td><td>-</td><td>38.3</td></tr>
<tr><td>YOLOv9-S</td><td>7.1</td><td>26.4</td><td>-</td><td>46.8</td></tr>
<tr><td>YOLOv9-M</td><td>20.0</td><td>76.3</td><td>-</td><td>51.4</td></tr>
<tr><td>YOLOv9-C</td><td>25.3</td><td>102.1</td><td>-</td><td>53.0</td></tr>
<tr><td>YOLOv9-E</td><td>57.3</td><td>189.0</td><td>-</td><td>55.6</td></tr>
<tr><td>YOLOv10-N</td><td>2.3</td><td>6.7</td><td>1.84</td><td>38.5</td></tr>
<tr><td>YOLOv10-S</td><td>7.2</td><td>21.6</td><td>2.49</td><td>46.3</td></tr>
<tr><td>YOLOv10-M</td><td>15.4</td><td>59.1</td><td>4.74</td><td>51.1</td></tr>
<tr><td>YOLOv10-B</td><td>19.1</td><td>92.0</td><td>5.74</td><td>52.5</td></tr>
<tr><td>YOLOv10-L</td><td>24.4</td><td>120.3</td><td>7.28</td><td>53.2</td></tr>
<tr><td>YOLOv10-X</td><td>29.5</td><td>160.4</td><td>10.70</td><td>54.4</td></tr>
</tbody></table>
<ul>
<li><strong>라이선스:</strong> 여기서 매우 중요한 점은 라이선스입니다. YOLOv9의 공식 저장소는 강력한 카피레프트 라이선스인 <strong>GPL-3.0</strong> 하에 배포됩니다.41 이는 YOLOv9을 사용하여 개발한 파생 소프트웨어도 GPL-3.0으로 공개해야 함을 의미하여, 독점 상용 소프트웨어에 사용하는 데 법적 제약이 큽니다. Ultralytics에서 배포하는 YOLOv10은 AGPL-3.0 라이선스를 따르며, 별도의 상업용 라이선스 구매 옵션을 제공합니다. 이는 개발자에게 중요한 전략적 고려사항입니다.</li>
</ul>
<h3>4.2 DINO: 트랜스포머를 이용한 종단간 탐지</h3>
<h4>4.2.1 서론</h4>
<p>DINO(DETR with Improved deNoising anchOr boxes)는 DETR을 기반으로 한 종단간 트랜스포머 객체 탐지기의 SOTA 모델입니다. DINO는 기존 DETR의 느린 수렴 속도와 훈련의 어려움을 해결하여, 고도로 최적화된 고전적인 탐지기들을 능가하는 성능을 달성했습니다.42</p>
<h4>4.2.2 아키텍처 심층 분석 및 핵심 혁신</h4>
<p>DINO는 DETR의 훈련 과정을 안정화하고 가속화하기 위한 세 가지 핵심 기술을 도입했습니다.42</p>
<ul>
<li><strong>핵심 혁신 1: 대조적 노이즈 제거 훈련 (Contrastive De-Noising Training):</strong> 훈련을 가속화하기 위해, 실제 정답(ground-truth) 바운딩 박스에 노이즈를 추가한 버전을 디코더에 입력하고 원본 박스를 복원하도록 학습시킵니다. DINO는 여기서 더 나아가, 하나의 정답 박스에 대해 작은 노이즈가 추가된 ’긍정 샘플(positive sample)’과 큰 노이즈가 추가된 ’부정 샘플(negative sample)’을 동시에 생성합니다. 모델은 긍정 샘플로부터는 원본 박스를 정확히 복원해야 하고, 부정 샘플은 거부해야 합니다. 이 대조적인 학습 방식은 모델이 동일 객체에 대해 중복된 예측을 생성하는 것을 방지하고, 일대일 매칭 능력을 향상시킵니다.</li>
<li><strong>핵심 혁신 2: 혼합 쿼리 선택 (Mixed Query Selection):</strong> 디코더의 객체 쿼리(동적 앵커 박스)를 더 효과적으로 초기화하기 위해 혼합 전략을 사용합니다. 일부 쿼리는 인코더의 출력에서 내용 기반(content-based)으로 선택되고, 다른 일부 쿼리는 학습 가능한 정적 위치 정보(positional prior)로부터 초기화됩니다. 이는 데이터 기반 방식과 사전 지식 기반 방식의 장점을 결합합니다.</li>
<li><strong>핵심 혁신 3: 두 번 앞서 보기 (Look Forward Twice):</strong> 박스 좌표를 점진적으로 정제하는 과정의 성능을 높이기 위해, 특정 디코더 레이어에서의 예측은 현재 레이어의 정보뿐만 아니라 <em>이전 레이어에서 이미 정제된 박스 정보</em>까지 함께 참조하여 업데이트됩니다. 이는 정제 과정에 더 넓은 정보 ’수용장(receptive field)’을 제공하는 효과를 낳습니다.</li>
</ul>
<h4>4.2.3 성능 및 라이선스</h4>
<p>DINO는 Swin-L 백본과 결합되었을 때 COCO test-dev에서 <strong>63.3 AP</strong>라는 SOTA 성능을 기록했습니다.44 또한 ResNet-50 백본으로 12 에포크만 훈련했을 때 이전 SOTA DETR 모델보다 +6.0 AP 높은 성능을 달성하며 훈련 효율성도 크게 개선했음을 입증했습니다.43</p>
<ul>
<li><strong>라이선스:</strong> DINO의 공식 구현체(예: <code>IDEACVR/DINO</code>)는 일반적으로 Apache 2.0과 같은 허용적 라이선스로 배포되어, 학술 연구는 물론 상업적 활용에도 매우 유리합니다.44</li>
</ul>
<p>객체 탐지 분야의 두 거두인 YOLO와 DINO는 서로 다른 경로를 통해 발전해왔지만, NMS가 없는 진정한 종단간 예측이라는 동일한 목표를 향해 수렴하고 있습니다. YOLOv10이 NMS-free를 구현함으로써 두 패밀리 간의 구조적 격차는 줄어들고 있으며, 경쟁의 초점은 이제 ‘어떻게’ 종단간 탐지를 달성하면서 정확도-지연 시간의 균형을 최적화할 것인가로 옮겨가고 있습니다. 이 과정에서 라이선스 정책은 기술적 성능만큼이나 중요한 전략적 분기점을 형성합니다. GPL/AGPL 계열의 YOLO는 커뮤니티 기여를 유도하고 상업용 라이선스 판매를 통한 수익 모델을 구축하는 반면, Apache/MIT 계열의 DINO는 학계와 산업계 전반에 걸친 광범위한 채택을 통해 표준으로 자리매김하려는 전략을 취하고 있습니다.</p>
<h2>5. 프론티어: 대규모 멀티모달 모델 (LMMs)</h2>
<h3>5.1 LLaVA-NeXT (LLaVA-1.6): 시각적 지시 튜닝의 발전</h3>
<h4>5.1.1 서론</h4>
<p>LLaVA(Large Language-and-Vision Assistant)는 이미지에 대해 ’대화’하고 지시를 따를 수 있는 오픈 소스 범용 시각 보조 모델의 선구자입니다.49 LLaVA-NeXT(v1.6)는 추론, 광학 문자 인식(OCR), 그리고 세상 지식(world knowledge) 능력을 대폭 향상시킨 주요 업그레이드 버전입니다.50</p>
<h4>5.1.2 아키텍처 심층 분석</h4>
<p>LLaVA의 성공은 단순하면서도 효과적인 아키텍처와 독창적인 훈련 방법론에 기인합니다.49</p>
<ul>
<li><strong>핵심 구조:</strong> Vision Encoder (CLIP ViT-L/14) –&gt;&gt; Projector (MLP) –&gt;&gt; LLM (Vicuna, Llama-3 등).</li>
<li><strong>프로젝터 (Projector):</strong> 시각적 특징을 LLM의 단어 임베딩 공간으로 변환하는 ‘시각적 토크나이저(visual tokenizer)’ 역할을 합니다. LLaVA는 간단한 2계층 MLP를 사용하여 이 연결을 구현했으며, LLaVA-NeXT 역시 이 효율적인 설계를 계승합니다.</li>
<li><strong>2단계 훈련 (Two-Stage Training):</strong></li>
</ul>
<ol>
<li><strong>특징 정렬 사전 학습 (Feature Alignment Pre-training):</strong> Vision Encoder와 LLM은 고정(frozen)시킨 채, <strong>프로젝터 MLP만</strong> 이미지-텍스트 쌍 데이터로 학습시킵니다. 이는 시각 도메인과 언어 도메인을 정렬하는 과정입니다.</li>
<li><strong>종단간 미세 조정 (End-to-End Fine-tuning):</strong> 프로젝터와 LLM(선택적으로 Vision Encoder 포함)을 모두 고품질의 다양한 시각적 지시 데이터셋으로 미세 조정하여 모델의 대화 및 추론 능력을 극대화합니다.</li>
</ol>
<ul>
<li><strong>LLaVA-NeXT의 핵심 혁신 (동적 고해상도):</strong> LLaVA-1.5보다 4배 더 많은 픽셀을 처리할 수 있으며, 이미지를 왜곡하지 않고 다양한 종횡비(예: 672x672, 336x1344)를 처리하기 위해 패치 기반 전략을 사용합니다. 이는 OCR과 같이 미세한 디테일이 중요한 작업에서 결정적인 성능 향상을 가져옵니다.51</li>
</ul>
<h4>5.1.3 성능 및 라이선스</h4>
<p>LLaVA-NeXT는 MMMU, MathVista, SEED-Bench와 같은 주요 멀티모달 벤치마크에서 다른 오픈 소스 LMM들을 능가하고, 상용 모델인 Gemini Pro와 대등한 성능을 보입니다.51 7B, 13B, 34B 등 다양한 크기의 모델이 제공되어 사용자의 컴퓨팅 환경에 맞춰 선택할 수 있습니다.51</p>
<ul>
<li><strong>라이선스:</strong> 모델과 코드는 <strong>Apache-2.0 라이선스</strong>로 공개되어, 학술 및 상업적 목적 모두에 매우 유리한 조건을 제공합니다.53</li>
</ul>
<h3>5.2 InternVL 2: 가장 강력한 오픈 소스 MLLM</h3>
<h4>5.2.1 서론</h4>
<p>InternVL 2는 현재 오픈 소스 MLLM 중 가장 강력한 성능을 목표로 개발되었으며, 여러 도전적인 벤치마크에서 GPT-4o와 같은 선두적인 상용 모델의 성능에 필적하거나 이를 능가합니다.54</p>
<h4>5.2.2 아키텍처 심층 분석 및 핵심 혁신</h4>
<p>InternVL 2의 SOTA 성능은 거대한 모델 크기뿐만 아니라, 정교한 훈련 전략과 효율적인 아키텍처 설계에 기반합니다.55</p>
<ul>
<li><strong>핵심 아이디어:</strong> 엣지 디바이스용 1B 모델부터 100B가 넘는 초거대 모델까지 포괄하는 모델 패밀리를 구축하여, 성능의 한계를 탐구합니다.</li>
<li><strong>핵심 혁신 1: 점진적 정렬 훈련 전략 (Progressive Alignment Training Strategy):</strong> 모델의 크기를 작은 것에서 큰 것으로 점진적으로 확장하면서, 동시에 훈련 데이터를 거친(coarse-grained) 것에서 정제된(fine-grained) 것으로 점차 고도화하는 다단계 훈련 방식을 사용합니다. 이는 비교적 적은 비용으로 거대 모델을 효율적으로 훈련할 수 있게 합니다.</li>
<li><strong>핵심 혁신 2: 설계 기반의 멀티태스크 및 멀티모달 (Multitask &amp; Multimodal by Design):</strong> 아키텍처 자체가 이미지, 텍스트, 비디오 등 다양한 입력을 처리하고, 텍스트, 경계 상자, 마스크 등 다양한 출력을 생성하도록 설계된 통합 기반 모델입니다.</li>
<li><strong>핵심 혁신 3 (InternVL-X): 시각 토큰 압축 (Visual Token Compression):</strong> 후속 연구인 InternVL-X에서는 PVTC(Projector Visual Token Compressor), LVTC(Layer-wise Visual Token Compressor)와 같은 고급 기술을 사용하여 LLM에 입력되는 시각 토큰의 수를 획기적으로 줄여, 성능 저하 없이 추론 효율성을 크게 향상시킵니다.56</li>
</ul>
<h4>5.2.3 성능 및 라이선스</h4>
<p>InternVL 2는 깊은 추론 능력을 요구하는 벤치마크에서 특히 압도적인 성능을 자랑합니다. <strong>MathVista에서 66.3%</strong>, <strong>MMMU에서 62.0%</strong>, **DocVQA에서 95.1%**의 정확도를 달성하며 오픈 소스 모델의 새로운 기준을 제시했습니다.55</p>
<ul>
<li><strong>라이선스:</strong> InternVL 모델들은 일반적으로 학술 연구와 광범위한 채택을 장려하기 위해 허용적 라이선스로 배포됩니다.54</li>
</ul>
<p>LMM 분야의 발전은 아키텍처 자체만큼이나 훈련 데이터의 질과 양에 의해 결정되고 있습니다. LLaVA가 GPT-4를 이용해 지시 데이터를 생성한 것이나 49, InternVL 2가 시험 문제, OCR 데이터 등을 대규모로 수집하고 정제한 것에서 볼 수 있듯이 55, 이제는 ’데이터 해자(Data Moat)’가 SOTA 성능 복제의 가장 큰 장벽이 되고 있습니다. 동일한 오픈 소스 아키텍처를 사용하더라도, 고품질의 대규모 미세 조정 데이터셋 없이는 최고 성능을 재현하기 어렵습니다.57 이와 동시에, InternVL 2-Pro와 같은 초거대 모델의 등장은 실용성을 위한 효율화 연구를 촉진하고 있습니다. LLaVA-NeXT의 SGLang을 통한 효율적 배포나 51, InternVL-X의 토큰 압축 기술은 56 LMM 분야가 극한의 성능을 추구하는 흐름과, 이를 현실 세계에서 사용 가능하도록 만드는 효율화 연구의 흐름으로 양분되고 있음을 보여줍니다.</p>
<h2>6. 종합 분석 및 전략적 권장 사항</h2>
<h3>6.1 종합 비교 프레임워크</h3>
<p>지금까지 심층 분석한 모델들은 각기 다른 철학과 강점을 가지고 있습니다. 이들을 효과적으로 비교하기 위해 다음과 같은 다차원적 프레임워크를 적용할 수 있습니다.</p>
<ul>
<li><strong>아키텍처 철학:</strong> 순수 CNN (ConvNeXt), 계층적 ViT (Swin), 일반 ViT (EVA), 종단간 탐지기 (DINO, YOLOv10), LMM (LLaVA, InternVL) 간의 근본적인 설계 차이.</li>
<li><strong>성능 대 효율성:</strong> 정확도(mAP/Top-1)를 계산 비용(FLOPs/Latency) 대비로 시각화하여 각 모델의 효율성-성능 곡선 상 위치를 파악.</li>
<li><strong>과제별 적합성:</strong> 이미지 분류, 실시간 탐지, 고정확도 탐지, 멀티모달 추론 등 특정 과제에 어떤 모델이 가장 뛰어난가.</li>
<li><strong>데이터 및 훈련 복잡성:</strong> 공개 데이터로 훈련된 모델과 거대 비공개 데이터로 훈련된 모델, 그리고 단순 미세 조정과 복잡한 다단계 훈련의 차이.</li>
</ul>
<h3>6.2 고성능 오픈 소스 비전 모델 요약표</h3>
<p>아래 표는 본 안내서에서 분석한 핵심 모델들의 주요 특성을 한눈에 비교할 수 있도록 정리한 것입니다. 이는 사용자가 자신의 요구사항(예: 특정 작업, 라이선스 제약, 하드웨어 예산)에 맞는 최적의 모델을 신속하게 찾는 데 도움을 줄 것입니다.</p>
<table><thead><tr><th>모델 계열</th><th>대표 모델/변형</th><th>주요 과제</th><th>아키텍처 유형</th><th>핵심 혁신</th><th>ImageNet Top-1 (%)</th><th>COCO box AP (%)</th><th>파라미터(M)</th><th>FLOPs(G)/Latency(ms)</th><th>라이선스</th></tr></thead><tbody>
<tr><td><strong>ConvNeXt V2</strong></td><td>Huge</td><td>분류/탐지/분할</td><td>CNN</td><td>Global Response Normalization (GRN)</td><td>88.9 21</td><td>59.1 (Mask R-CNN)</td><td>650 21</td><td>-</td><td>MIT (추정)</td></tr>
<tr><td><strong>Swin V2</strong></td><td>SwinV2-G</td><td>분류/탐지/분할</td><td>Hierarchical ViT</td><td>Res-Post-Norm, Scaled Cosine Attention</td><td>90.17 26</td><td>63.1 (HTC++) 26</td><td>3000 26</td><td>-</td><td>MIT (추정)</td></tr>
<tr><td><strong>EVA-02</strong></td><td>EVA-02-L</td><td>분류/특징추출</td><td>Plain ViT</td><td>MIM with Teacher Model Features</td><td>90.0 31</td><td>-</td><td>304 31</td><td>-</td><td>Permissive</td></tr>
<tr><td><strong>DINO</strong></td><td>Swin-L Backbone</td><td>고정확도 탐지</td><td>Transformer Detector</td><td>Contrastive De-Noising, Mixed Query</td><td>-</td><td>63.3 44</td><td>-</td><td>-</td><td>Apache 2.0 (추정)</td></tr>
<tr><td><strong>YOLOv9</strong></td><td>YOLOv9-E</td><td>실시간 탐지</td><td>CNN (GELAN)</td><td>Programmable Gradient Info (PGI)</td><td>-</td><td>55.6 41</td><td>57.3 41</td><td>189.0 G 41</td><td><strong>GPL-3.0</strong> 41</td></tr>
<tr><td><strong>YOLOv10</strong></td><td>YOLOv10-X</td><td>실시간 탐지</td><td>CNN</td><td>NMS-Free (Consistent Dual Assign.)</td><td>-</td><td>54.4 34</td><td>29.5 34</td><td>10.70 ms 34</td><td><strong>AGPL-3.0</strong></td></tr>
<tr><td><strong>LLaVA-NeXT</strong></td><td>34B</td><td>멀티모달 채팅</td><td>LMM</td><td>Dynamic High Resolution</td><td>-</td><td>-</td><td>34,750 51</td><td>-</td><td><strong>Apache-2.0</strong> 53</td></tr>
<tr><td><strong>InternVL 2</strong></td><td>Pro</td><td>멀티모달 추론</td><td>LMM</td><td>Progressive Alignment Training</td><td>-</td><td>-</td><td>108,700 55</td><td>-</td><td>Permissive (추정)</td></tr>
</tbody></table>
<h3>6.3 사용 사례별 전략적 권장 사항</h3>
<ul>
<li><strong>학술 연구:</strong> DINO나 ConvNeXt V2처럼 이해하기 쉬운 새로운 아키텍처나, 제한된 하드웨어에서 빠른 실험이 가능한 EVA-02와 같은 고효율 모델을 추천합니다.</li>
<li><strong>실시간 상업용 애플리케이션 (예: 영상 감시, 로보틱스):</strong> NMS-free로 효율성이 극대화된 YOLOv10을 강력히 추천하지만, <strong>AGPL-3.0 라이선스에 대한 법적 검토 및 상업용 라이선스 구매</strong>를 반드시 고려해야 합니다. 허용적 라이선스가 필수적인 경우, RT-DETR과 같은 대안을 검토해야 합니다.</li>
<li><strong>고정확도 오프라인 처리 (예: 의료 영상 분석, 위성 영상):</strong> 탐지 및 분할 작업에서 최고의 정확도를 보이는 DINO(Swin V2 백본)나 ConvNeXt V2-Huge 모델을 추천합니다.</li>
<li><strong>고급 인간-AI 상호작용 및 추론:</strong> 강력한 대화 능력이 필요하다면 LLaVA-NeXT를, 복잡한 추론 작업에서 SOTA 성능이 필요하다면 InternVL 2를 추천하며, 사용 가능한 컴퓨팅 예산에 맞는 모델 크기를 선택해야 합니다.</li>
<li><strong>자원 제약적인 엣지 배포:</strong> YOLOv10-N, ConvNeXtV2-Atto와 같은 경량 모델이나, InternVL-1B와 같이 엣지용으로 설계된 LMM 변형 모델을 추천합니다.</li>
</ul>
<h2>7. 결론 및 미래 전망</h2>
<h3>7.1 주요 발견 요약</h3>
<p>본 안내서는 현대 고성능 오픈 소스 비전 모델의 지형을 심층적으로 분석했으며, 다음과 같은 핵심적인 흐름을 확인했습니다. 첫째, 아키텍처와 훈련 방법론의 ’공동 설계’가 SOTA 성능의 필수 조건이 되었습니다. 둘째, 객체 탐지 분야는 NMS가 없는 완전한 종단간 예측으로 수렴하고 있습니다. 셋째, 최첨단 모델들은 시각과 언어를 통합한 멀티모달 모델로 진화하고 있습니다. 넷째, 고품질 훈련 데이터의 확보가 ’데이터 해자’를 형성하며 모델 성능의 핵심 변수로 부상했습니다. 다섯째, 연구의 방향이 극한의 성능 추구와 실용성을 위한 효율화로 양분되고 있습니다. 마지막으로, 오픈 소스 라이선스는 기술 선택에 있어 법적, 전략적으로 매우 중요한 고려사항이 되었습니다.</p>
<h3>7.2 미래는 통합되고 멀티모달하다</h3>
<p>분야의 궤적은 명확합니다. InternVL 2와 같이 단일 아키텍처 내에서 여러 모달리티(이미지, 비디오, 텍스트)와 여러 과제(탐지, 분할, VQA)를 동시에 처리할 수 있는 통합 기반 모델(unified foundation models)로 나아가고 있습니다.4 미래의 SOTA는 순수한 ‘비전’ 모델이 아니라, 세상을 종합적으로 인식하고 추론하는 범용 지능 엔진의 형태를 띨 것입니다.</p>
<h3>7.3 미해결 과제와 연구 방향</h3>
<p>이러한 눈부신 발전에도 불구하고, 여전히 중요한 과제들이 남아있습니다.</p>
<ul>
<li><strong>강건성과 일반화:</strong> 벤치마크 점수를 넘어 실제 세계의 예측 불가능하고 지저분한 데이터에 강건한 모델을 만드는 것은 여전히 핵심 과제입니다.11</li>
<li><strong>데이터 부족과 큐레이션:</strong> ’데이터 해자’는 개방형 연구에 큰 장벽입니다. 더 적은 데이터, 혹은 더 쉽게 얻을 수 있는 데이터로 강력한 모델을 훈련하는 기술 개발이 시급합니다.</li>
<li><strong>LMM의 환각(Hallucination):</strong> LMM은 강력하지만, 여전히 사실과 다르거나 비논리적인 내용을 생성하는 환각 현상을 보입니다.5 모델의 답변을 현실에 더 단단히 기반하게(grounding) 만드는 연구가 활발히 진행 중입니다.</li>
<li><strong>온디바이스 AI:</strong> 거대한 클라우드 모델과 스마트폰이나 자동차에서 구동 가능한 모델 간의 성능 격차는 여전히 큽니다. 효율적인 지식 증류, 양자화(quantization) 58, 그리고 하드웨어 친화적인 새로운 아키텍처 설계는 SOTA AI를 엣지 환경으로 가져오기 위한 필수적인 연구 분야로 남을 것입니다.</li>
</ul>
<h2>8. 참고 자료</h2>
<ol>
<li>Top Computer Vision Models: Comparing the Best CV Models - Encord, accessed July 20, 2025, https://encord.com/blog/top-computer-vision-models/</li>
<li>arXiv:2311.15599v2 [cs.CV] 18 Mar 2024, accessed July 20, 2025, https://arxiv.org/pdf/2311.15599</li>
<li>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows - arXiv, accessed July 20, 2025, https://arxiv.org/abs/2103.14030</li>
<li>Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions - arXiv, accessed July 20, 2025, https://arxiv.org/html/2404.07214v1</li>
<li>JackYFL/awesome-VLLMs: This repository collects papers on VLLM applications. We will update new papers irregularly. - GitHub, accessed July 20, 2025, https://github.com/JackYFL/awesome-VLLMs</li>
<li>12 Benchmarking AI - Machine Learning Systems, accessed July 20, 2025, https://mlsysbook.ai/contents/core/benchmarking/benchmarking.html</li>
<li>ImageNet, accessed July 20, 2025, https://www.image-net.org/</li>
<li>ImageNet Challenge Leaderboard from 2011 to 2020. - ResearchGate, accessed July 20, 2025, https://www.researchgate.net/figure/ImageNet-Challenge-Leaderboard-from-2011-to-2020_fig5_366336973</li>
<li>The Why and What of our Computer Vision Benchmark Tool | by Jérémy Keusters | ML6team, accessed July 20, 2025, https://blog.ml6.eu/why-ml6-is-working-on-a-computer-vision-benchmark-tool-e8d4644ba10f</li>
<li>COCO - Common Objects in Context, accessed July 20, 2025, https://cocodataset.org/index.htm</li>
<li>Lacking Good Computer Vision Benchmark Datasets Is a Problem-Let’s Fix That!, accessed July 20, 2025, https://www.activeloop.ai/resources/lacking-good-computer-vision-benchmark-datasets-is-a-problem-lets-fix-that/</li>
<li>ImageNet Classification Leaderboard, accessed July 20, 2025, https://kobiso.github.io/Computer-Vision-Leaderboard/imagenet.html</li>
<li>COCO test-dev Benchmark (Object Detection) - Papers With Code, accessed July 20, 2025, https://paperswithcode.com/sota/object-detection-on-coco</li>
<li>ImageNet Benchmark (Image Classification) - Papers With Code, accessed July 20, 2025, https://paperswithcode.com/sota/image-classification-on-imagenet</li>
<li>CVPR 2024 Datasets and Benchmarks - Part 2 - Voxel51, accessed July 20, 2025, https://voxel51.com/blog/cvpr-2024-datasets-and-benchmarks-part-2-benchmarks</li>
<li>What are the most common open-source licenses? - Milvus, accessed July 20, 2025, https://milvus.io/ai-quick-reference/what-are-the-most-common-opensource-licenses</li>
<li>The Landscape of Open Source Licensing in AI: A Primer on LLMs and Vector Databases, accessed July 20, 2025, https://medium.com/@zilliz_learn/the-landscape-of-open-source-licensing-in-ai-a-primer-on-llms-and-vector-databases-5effbccbccd5</li>
<li>AI Licenses: What You Should Know for Your Applications - Viso Suite, accessed July 20, 2025, https://viso.ai/deep-learning/ai-licenses/</li>
<li>The Open Source Legacy and AI’s Licensing Challenge - Linux Foundation, accessed July 20, 2025, https://www.linuxfoundation.org/blog/the-open-source-legacy-and-ais-licensing-challenge</li>
<li>IMvision12/ConvNeXt-tf: A Tensorflow Implementation of “A ConvNet for the 2020s” and “ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders” (ConvNeXt and ConvNeXtV2) - GitHub, accessed July 20, 2025, https://github.com/IMvision12/ConvNeXt-tf</li>
<li>ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders - arXiv, accessed July 20, 2025, https://arxiv.org/abs/2301.00808</li>
<li>ConvNeXt V2: Co-designing and Scaling ConvNets with Masked …, accessed July 20, 2025, https://paperswithcode.com/paper/convnext-v2-co-designing-and-scaling-convnets</li>
<li>convnext / GitHub Topics, accessed July 20, 2025, https://github.com/topics/convnext</li>
<li>ConvNeXT - Hugging Face, accessed July 20, 2025, https://huggingface.co/docs/transformers/model_doc/convnext</li>
<li>[2111.09883] Swin Transformer V2: Scaling Up Capacity and …, accessed July 20, 2025, https://ar5iv.labs.arxiv.org/html/2111.09883</li>
<li>Swin Transformer V2: Scaling Up Capacity and Resolution | Papers …, accessed July 20, 2025, https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and</li>
<li>Papers Explained 215: Swin Transformer V2 | by Ritvik Rastogi - Medium, accessed July 20, 2025, https://ritvik19.medium.com/papers-explained-215-swin-transformer-v2-53bee16ab668</li>
<li>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | Papers With Code, accessed July 20, 2025, https://paperswithcode.com/paper/swin-transformer-hierarchical-vision</li>
<li>[2303.11331] EVA-02: A Visual Representation for Neon Genesis - arXiv, accessed July 20, 2025, https://arxiv.org/abs/2303.11331</li>
<li>arXiv:2303.11331v2 [cs.CV] 22 Mar 2023, accessed July 20, 2025, https://arxiv.org/pdf/2303.11331</li>
<li>EVA-02: A Visual Representation for Neon Genesis | Papers With …, accessed July 20, 2025, https://paperswithcode.com/paper/eva-02-a-visual-representation-for-neon</li>
<li>EVA/README.md at master / baaivision/EVA - GitHub, accessed July 20, 2025, https://github.com/baaivision/EVA/blob/master/README.md</li>
<li>Top 5 Open-Source Computer Vision Models - Unitlab Blogs, accessed July 20, 2025, https://blog.unitlab.ai/top-5-open-source-computer-vision-models/</li>
<li>YOLOv10: Real-Time End-to-End Object Detection, accessed July 20, 2025, https://arxiv.org/pdf/2405.14458</li>
<li>arXiv:2402.13616v2 [cs.CV] 29 Feb 2024, accessed July 20, 2025, https://arxiv.org/abs/2402.13616</li>
<li>YOLOv9: Learning What You Want to Learn Using Programmable …, accessed July 20, 2025, https://paperswithcode.com/paper/yolov9-learning-what-you-want-to-learn-using</li>
<li>Paper Review: YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information | by Andrew Lukyanenko, accessed July 20, 2025, https://artgor.medium.com/paper-review-yolov9-learning-what-you-want-to-learn-using-programmable-gradient-information-8ec2e6e13551</li>
<li>YOLOv10: Real-Time End-to-End Object Detection [NeurIPS 2024] - GitHub, accessed July 20, 2025, https://github.com/THU-MIG/yolov10</li>
<li>models/yolov10/ / ultralytics / Discussion #13117 - GitHub, accessed July 20, 2025, https://github.com/orgs/ultralytics/discussions/13117</li>
<li>Learn What Is Introduced in YOLOv10 | YOLOv10 Paper Explained - YouTube, accessed July 20, 2025, https://www.youtube.com/watch?v=2ZFJbeJXXDM</li>
<li>WongKinYiu/yolov9: Implementation of paper - YOLOv9 … - GitHub, accessed July 20, 2025, https://github.com/WongKinYiu/yolov9</li>
<li>[2203.03605] DINO: DETR with Improved DeNoising Anchor Boxes …, accessed July 20, 2025, https://ar5iv.labs.arxiv.org/html/2203.03605</li>
<li>DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection, accessed July 20, 2025, https://arxiv.org/abs/2203.03605</li>
<li>DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End …, accessed July 20, 2025, https://paperswithcode.com/paper/dino-detr-with-improved-denoising-anchor-1</li>
<li>wzk1015/COCO-leaderboard: Notes of COCO leaderboard (based on https://paperswithcode.com/sota/object-detection-on-coco) - GitHub, accessed July 20, 2025, https://github.com/wzk1015/COCO-leaderboard</li>
<li>DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection, accessed July 20, 2025, https://www.researchgate.net/publication/359079872_DINO_DETR_with_Improved_DeNoising_Anchor_Boxes_for_End-to-End_Object_Detection</li>
<li>DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection, accessed July 20, 2025, https://openreview.net/forum?id=3mRwyG5one</li>
<li>This repo contains the code and configuration files for reproducing object detection results of FocalNets with DINO - GitHub, accessed July 20, 2025, https://github.com/FocalNet/FocalNet-DINO</li>
<li>LLaVA, accessed July 20, 2025, https://llava-vl.github.io/</li>
<li>haotian-liu/LLaVA: [NeurIPS’23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond. - GitHub, accessed July 20, 2025, https://github.com/haotian-liu/LLaVA</li>
<li>LLaVA-NeXT: Improved reasoning, OCR, and world knowledge …, accessed July 20, 2025, https://llava-vl.github.io/blog/2024-01-30-llava-next/</li>
<li>arXiv:2304.08485v2 [cs.CV] 11 Dec 2023, accessed July 20, 2025, https://arxiv.org/abs/2304.08485</li>
<li>LLaVA-VL/LLaVA-NeXT - GitHub, accessed July 20, 2025, https://github.com/LLaVA-VL/LLaVA-NeXT</li>
<li>OpenGVLab/InternVL: [CVPR 2024 Oral] InternVL Family: A Pioneering Open-Source Alternative to GPT-4o. 接近GPT-4o表现的开源多模态对话模型 - GitHub, accessed July 20, 2025, https://github.com/OpenGVLab/InternVL</li>
<li>InternVL2, accessed July 20, 2025, https://internvl.github.io/blog/2024-07-02-InternVL-2.0/</li>
<li>Advancing and Accelerating InternVL Series with Efficient Visual Token Compression - arXiv, accessed July 20, 2025, https://arxiv.org/html/2503.21307v1</li>
<li>Open-LLaVA-NeXT/docs/Data.md at master - GitHub, accessed July 20, 2025, https://github.com/xiaoachen98/Open-LLaVA-NeXT/blob/master/docs/Data.md</li>
<li>llava-next-multimodal-chatbot.ipynb - GitHub, accessed July 20, 2025, https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/llava-next-multimodal-chatbot/llava-next-multimodal-chatbot.ipynb</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>