<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Meta의 개방형 AI 생태계</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Meta의 개방형 AI 생태계</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">오픈 소스 AI 모델</a> / <span>Meta의 개방형 AI 생태계</span></nav>
                </div>
            </header>
            <article>
                <h1>Meta의 개방형 AI 생태계</h1>
<h2>1.  개방형 AI를 향한 Meta의 비전과 전략</h2>
<p>Meta는 인공지능(AI) 기술의 민주화를 통해 모든 개발자와 연구자가 그 혜택을 누릴 수 있도록 한다는 명확한 비전 아래, 오픈소스 전략을 일관되게 추진하고 있다.1 이는 단순히 사전 학습된 모델을 공개하는 차원을 넘어, AI 개발 생태계 전반의 혁신을 가속하고 기술 발전을 공동으로 이끌어 가려는 거시적 목표를 내포한다.2 Meta의 이러한 접근 방식은 AI 기술의 투명성을 높이고, 전 세계 커뮤니티의 집단 지성을 활용하여 모델의 안전성과 성능을 개선하며, 궁극적으로는 자사의 기술 스택, 특히 PyTorch를 산업 표준으로 공고히 하려는 장기적 포석으로 분석된다.</p>
<p>그러나 Meta의 개방형 AI 전략은 순탄한 길만을 걷고 있는 것은 아니다. 2025년 상반기 미국 VC 기업 Menlo Ventures의 데이터에 따르면, 전 세계 기업의 생성형 AI 지출 84억 달러 중 OpenAI와 Anthropic 같은 폐쇄형, 독점 모델이 57%를 차지하며 시장을 주도하고 있다.4 반면, 오픈소스 진영의 대표 주자인 Meta의 Llama 모델은 기업 지출 점유율이 16%에서 9%로 하락하는 등, 초기 기대와 달리 시장 지배력을 즉각적으로 확보하지는 못하는 양상을 보인다.4 이러한 현상은 많은 기업이 AI 도입 초기 단계에서 모델의 유지보수, 미세 조정, 보안 관리에 요구되는 높은 수준의 내부 전문성과 비용 부담을 회피하고자 하기 때문이다. 대신 이들은 기술 지원이 보장되고 사용이 간편한 상용 API를 선호하는 경향이 뚜렷하다.4 이는 오픈소스 모델의 ’총 소유 비용(Total Cost of Ownership, TCO)’이 단순 API 사용료보다 높을 수 있다는 시장의 인식을 반영한다.</p>
<p>이러한 시장 환경 속에서 Meta는 전략적 딜레마에 직면해 있다. 한편으로는 고성능 오픈소스 모델을 무료로 제공하여 방대한 개발자 커뮤니티를 확보하고 생태계를 장악한 후, 이를 기반으로 더 강력한 독점 모델이나 부가 서비스를 통해 수익을 창출하는 전략을 고려할 수 있다.4 하지만 Meta가 가장 큰 규모의 모델로 알려진 ’Llama Behemoth’의 출시를 공식적으로 지연시키고, ‘개인 초지능(personal superintelligence)’ 구축으로의 방향 전환을 시사한 점은 이러한 단순한 이원화 전략에 대한 재고 가능성을 암시한다.4 이는 오픈소스 모델의 직접적인 수익화 문제와 막대한 연구 개발 비용 사이의 근본적인 긴장 관계를 보여주는 대목이다.</p>
<p>결론적으로 Meta의 전략은 ’최고 성능’의 모델을 개발하여 프론티어 모델 경쟁에서 정면으로 맞서는 것에서, ’최적의 효율’을 갖춘 접근성 높은 모델을 제공하여 광범위한 개발자 생태계를 구축하고 자사 플랫폼의 가치를 높이는 방향으로 무게 중심을 이동하고 있다. 시장 데이터는 순수 성능 경쟁에서 오픈소스가 상업적으로 밀리는 듯 보이지만, 동시에 Stanford AI Index 안내서는 오픈소스와 폐쇄형 모델 간의 성능 격차가 1.7%까지 좁혀졌다고 분석한다.4 이는 기술적 격차는 거의 해소되었으나, 상업적 채택의 장벽인 유지보수, 보안, 라이선스 복잡성 등이 여전히 높다는 것을 의미한다. Meta가 Llama Behemoth의 출시를 늦추고 ’개인 초지능’을 강조하는 것은, 이러한 거대 모델 경쟁에서 한발 물러나 Meta AI 어시스턴트, Ray-Ban Meta 안경과 같은 실제 자사 제품에 통합될 수 있는 개인화되고 효율적인 AI에 집중하겠다는 신호로 해석된다.5 이는 Llama 3.1 405B와 같은 고성능 모델은 오픈소스로 공개하여 커뮤니티가 프론티어의 한계를 탐구하도록 유도하고, Llama 3.2와 같은 경량 모델은 엣지 디바이스에서의 사용성을 극대화하는 이중 전략을 통해, Meta가 생태계 전체를 성장시켜 자사 플랫폼의 영향력을 강화하려는 더 큰 그림을 그리고 있음을 시사한다.</p>
<h2>2.  대규모 언어 모델의 진화: Llama 연대기</h2>
<p>Meta의 Llama(Large Language Model Meta AI) 시리즈는 오픈소스 대규모 언어 모델(LLM)의 발전에 있어 중요한 이정표를 제시해왔다. 각 세대는 기술적 한계를 돌파하고, AI 커뮤니티에 새로운 가능성을 열어주며 진화했다.</p>
<h3>2.1  Llama 1 &amp; 2: 파운데이션 모델의 서막</h3>
<p><strong>Llama 1</strong>은 2023년 2월 24일에 처음 공개되었으며, 70억(7B)에서 650억(65.2B)에 이르는 다양한 파라미터 규모로 출시되었다.7 당시 Llama 1은 상업적 이용이 불가능한 연구용 라이선스로 배포되었으나, CommonCrawl, GitHub, Wikipedia, ArXiv 등 전적으로 공개된 데이터셋만을 사용하여 최첨단(SOTA) 성능에 도달할 수 있음을 입증하며 학계와 연구 커뮤니티에 큰 충격을 주었다.7</p>
<p>이후 2023년 7월 18일, Meta는 Microsoft와의 전략적 파트너십을 통해 <strong>Llama 2</strong>를 발표했다.7 Llama 2는 70억(6.7B)부터 700억(69B) 파라미터 규모로 제공되었으며, 상업적 이용이 가능한 커뮤니티 라이선스를 도입함으로써 오픈소스 AI 생태계의 폭발적인 성장을 견인했다.7 Llama 1 대비 학습 데이터는 2조(2T) 토큰으로 크게 확장되었고, 처리 가능한 컨텍스트 길이는 4096 토큰으로 두 배 증가하여 모델의 활용성을 높였다.7</p>
<p>Llama 2의 성공을 바탕으로 Meta는 특정 도메인에 특화된 모델의 가능성을 탐색하기 시작했다. 그 첫 결과물이 2023년 8월 24일에 공개된 <strong>Code Llama</strong>이다.7 Code Llama는 Llama 2를 기반으로 5,000억(500B) 개의 코드 관련 토큰을 추가로 학습시켜 코드 생성 및 이해 능력에 특화된 모델이다.7 이는 Llama가 범용 언어 모델을 넘어, 특정 전문 분야에 최적화될 수 있는 강력한 잠재력을 지녔음을 보여준 중요한 사례였다.12</p>
<h3>2.2  Llama 3: 성능과 효율성의 비약적 발전</h3>
<p>2024년 4월 18일에 출시된 <strong>Llama 3</strong>는 당시 최고의 독점 모델들과 필적하는 성능을 갖춘 개방형 모델을 구축하겠다는 야심 찬 목표 아래 개발되었다.3 이를 위해 Meta는 모델 아키텍처, 학습 데이터, 훈련 규모, 그리고 지시 미세 조정(instruction fine-tuning) 등 네 가지 핵심 요소에 집중했다.3</p>
<p>아키텍처 측면에서 Llama 3는 표준적인 디코더-온리 트랜스포머(decoder-only transformer) 구조를 유지하면서도, 추론 효율성을 극대화하기 위한 핵심적인 개선을 도입했다. 바로 **Grouped-Query Attention (GQA)**의 적용이다.3 기존의 Multi-Head Attention (MHA) 방식은 모든 쿼리 헤드(Query Head)가 각각 별도의 키(Key) 및 값(Value) 헤드를 가지는 구조로, 높은 성능을 보장하지만 추론 시 메모리 대역폭에 큰 부담을 주었다.13 GQA는 여러 쿼리 헤드가 하나의 키/값 헤드 쌍을 공유하도록 설계하여, MHA의 높은 품질과 Multi-Query Attention (MQA)의 빠른 속도 사이에서 최적의 균형점을 찾았다.13 이로 인해 추론 과정에서 메모리에 로드해야 하는 KV 캐시의 크기가 줄어들어, 성능 저하를 최소화하면서도 상당한 속도 향상을 이룰 수 있었다.</p>
<p>학습 데이터의 규모와 품질 또한 비약적으로 향상되었다. Llama 3는 Llama 2 대비 7배 이상 증가한 <strong>15조(15T) 개 이상의 토큰</strong>으로 구성된 방대한 데이터셋으로 사전 학습되었다.3 이 데이터셋에는 4배 더 많은 코드가 포함되었으며, 다국어 성능 강화를 위해 30개 이상의 언어로 구성된 고품질 비영어 데이터가 전체의 5% 이상을 차지했다.3 특히, 데이터의 질을 높이기 위해 이전 세대 모델인 Llama 2를 활용하여 고품질 데이터를 식별하는 텍스트 품질 분류기를 학습시키는 등, 데이터 정제 파이프라인을 한층 더 고도화했다.3</p>
<p>이러한 발전을 기반으로 Llama 3는 다양한 파생 모델을 통해 생태계를 확장했다.</p>
<ul>
<li><strong>Llama 3.1 (2024년 7월)</strong>: <strong>4,050억(405B) 파라미터</strong>라는 프론티어급 모델을 최초로 오픈소스로 공개하며 AI 커뮤니티에 큰 충격을 주었다.2 컨텍스트 길이를 128,000(</li>
</ul>
<p>128K) 토큰으로 대폭 확장하고, 8개 언어를 공식 지원하며 GPT-4, Claude 3.5 Sonnet과 같은 최상위 독점 모델과 경쟁 가능한 수준의 성능을 입증했다.2</p>
<ul>
<li>
<p><strong>Llama 3.2 (2024년 9월)</strong>: 10억(1B), 30억(3B), 110억(11B), 900억(90B) 등 더 작고 가벼운 모델들을 출시하여, 모바일 및 엣지 디바이스에서의 온디바이스 AI 활용 가능성을 열었다.6</p>
</li>
<li>
<p><strong>Llama 3.3 (2024년 12월)</strong>: 기존 700억(70B) 모델을 지속적으로 개선하여 안정성과 성능을 더욱 높였다.7</p>
</li>
</ul>
<h3>2.3  Llama 4: 멀티모달과 Mixture-of-Experts (MoE) 시대의 개막</h3>
<p>2025년 4월 5일에 공개된 <strong>Llama 4</strong>는 아키텍처 측면에서 두 가지 혁신적인 변화를 통해 Llama 시리즈를 새로운 차원으로 끌어올렸다: **Mixture-of-Experts (MoE)**와 **네이티브 멀티모달리티(Native Multimodality)**이다.7</p>
<p>Llama 4의 핵심 아키텍처인 MoE는 모델의 총 파라미터 수를 대폭 늘리면서도, 추론 시에는 입력된 정보에 따라 가장 관련성 높은 일부 ‘전문가(expert)’ 하위 네트워크만을 선택적으로 활성화하는 방식이다.18 각 입력 토큰은 ‘라우터(router)’ 네트워크를 통해 최적의 전문가에게 전달되며, 이로 인해 모델은 방대한 지식을 저장(총 파라미터)하면서도 특정 작업에 대해서는 매우 빠르고 효율적으로 응답(활성 파라미터)할 수 있다.20 Llama 3의 GQA가 어텐션 계산을 최적화했다면, Llama 4의 MoE는 모델 전체의 연산량을 최적화하는 한 단계 더 진화한 접근법이다. 이 두 기술은 모두 ’추론 비용 대비 성능’이라는, 오픈소스 모델의 실질적 확산에 가장 중요한 지표를 극대화하려는 Meta의 일관된 전략을 보여준다.</p>
<p>또한 Llama 4는 텍스트와 이미지 토큰을 모델 학습 초기 단계부터 함께 훈련하는 <strong>조기 융합(Early Fusion)</strong> 방식을 채택하여 진정한 의미의 네이티브 멀티모달리티를 구현했다.17 이는 각 모달리티를 별도의 인코더로 처리한 후 나중에 결합하는 방식보다 훨씬 더 깊은 수준에서 텍스트와 이미지 간의 상호 이해를 가능하게 한다.</p>
<p>Llama 4의 주요 모델 라인업은 다음과 같다 7:</p>
<ul>
<li><strong>Llama 4 Scout</strong>: 170억(17B) 활성 파라미터, 16개 전문가, 총 1,090억(109B) 파라미터. 특히 <strong>1,000만(10M) 토큰</strong>이라는 업계 최대 수준의 컨텍스트 창을 지원하여, 방대한 양의 문서를 한 번에 분석하거나 장기 기억을 요구하는 개인화된 AI 어시스턴트 개발에 전례 없는 가능성을 제공한다.7</li>
<li><strong>Llama 4 Maverick</strong>: 170억(17B) 활성 파라미터, 128개의 전문가, 총 4,000억(400B) 파라미터. Scout보다 8배 많은 전문가를 보유하여, 복잡하고 미묘한 이미지 및 텍스트 이해 작업에서 더욱 뛰어난 성능을 발휘한다.7</li>
<li><strong>Llama 4 Behemoth (Preview)</strong>: 총 2조(2T) 파라미터에 달할 것으로 예상되는 거대 모델로, Scout과 Maverick을 훈련시키는 ’교사 모델(teacher model)’로 사용되었다.17 아직 훈련이 진행 중이지만, Meta의 프론티어 모델 연구 방향성을 가늠할 수 있는 중요한 지표이다.</li>
</ul>
<p>성능 측면에서 Llama 4 Maverick은 MMLU, GPQA 등 주요 언어 이해 벤치마크뿐만 아니라, 이미지 이해 관련 벤치마크에서도 GPT-4o, Gemini 2.0 Flash 등 최상위 경쟁 모델들과 대등하거나 일부 항목에서는 더 우수한 성능을 기록하며 오픈소스 모델의 기술적 위상을 한 단계 끌어올렸다.17</p>
<table><thead><tr><th>모델명</th><th>출시일</th><th>파라미터 (활성/총)</th><th>컨텍스트 길이 (토큰)</th><th>학습 데이터 (토큰 수)</th><th>핵심 아키텍처</th><th>라이선스</th></tr></thead><tbody>
<tr><td><strong>Llama 1</strong></td><td>2023-02-24</td><td>65.2B</td><td>2048</td><td>1.4T</td><td>표준 Transformer</td><td>연구용 (비상업)</td></tr>
<tr><td><strong>Llama 2</strong></td><td>2023-07-18</td><td>69B</td><td>4096</td><td>2T</td><td>표준 Transformer</td><td>Llama 2 Community License</td></tr>
<tr><td><strong>Code Llama</strong></td><td>2023-08-24</td><td>69B</td><td>4096</td><td>2T+</td><td>표준 Transformer</td><td>Llama 2 Community License</td></tr>
<tr><td><strong>Llama 3</strong></td><td>2024-04-18</td><td>70.6B</td><td>8192</td><td>15T</td><td>GQA</td><td>Llama 3 Community License</td></tr>
<tr><td><strong>Llama 3.1</strong></td><td>2024-07-23</td><td>405B</td><td>128,000</td><td>15T</td><td>GQA</td><td>Llama 3 Community License</td></tr>
<tr><td><strong>Llama 3.2</strong></td><td>2024-09-25</td><td>1B - 90B</td><td>128,000</td><td>9T</td><td>GQA</td><td>Llama 3.2 Community License</td></tr>
<tr><td><strong>Llama 4 Scout</strong></td><td>2025-04-05</td><td>17B / 109B</td><td>10,000,000</td><td>40T</td><td>MoE, Native Multimodal</td><td>Llama 4 Community License</td></tr>
<tr><td><strong>Llama 4 Maverick</strong></td><td>2025-04-05</td><td>17B / 400B</td><td>1,000,000</td><td>22T</td><td>MoE, Native Multimodal</td><td>Llama 4 Community License</td></tr>
<tr><td><strong>Llama 4 Behemoth</strong></td><td>Preview</td><td>~288B / ~2T</td><td>?</td><td>?</td><td>MoE, Native Multimodal</td><td>?</td></tr>
</tbody></table>
<table><thead><tr><th>모델명</th><th>GPQA (acc)</th><th>MMLU (5-shot)</th><th>MATH (CoT)</th><th>HumanEval (pass@1)</th><th>VBench</th><th>멀티모달 지원</th><th>컨텍스트 창</th></tr></thead><tbody>
<tr><td><strong>Llama 4 Behemoth</strong></td><td>49.4</td><td>95.0</td><td>82.2</td><td>73.7</td><td>85.8</td><td>예</td><td>?</td></tr>
<tr><td><strong>Llama 4 Maverick</strong></td><td>43.4</td><td>90.0</td><td>69.8</td><td>84.6</td><td>80.5</td><td>예</td><td>1M</td></tr>
<tr><td><strong>Llama 4 Scout</strong></td><td>32.8</td><td>88.8</td><td>57.2</td><td>81.5 (추정치)</td><td>74.3</td><td>예</td><td>10M</td></tr>
<tr><td><strong>GPT-4.5</strong></td><td>-</td><td>-</td><td>-</td><td>71.4</td><td>85.1</td><td>예</td><td>?</td></tr>
<tr><td><strong>GPT-4o</strong></td><td>32.3</td><td>85.7</td><td>53.6</td><td>81.5</td><td>-</td><td>예</td><td>128K</td></tr>
<tr><td><strong>Gemini 2.0 Pro</strong></td><td>36.0</td><td>91.8</td><td>79.1</td><td>64.7</td><td>-</td><td>예</td><td>?</td></tr>
<tr><td><strong>Claude Sonnet 3.7</strong></td><td>-</td><td>82.2</td><td>-</td><td>68.0</td><td>83.2</td><td>예</td><td>?</td></tr>
<tr><td><strong>DeepSeek v3.1</strong></td><td>-</td><td>81.2</td><td>68.4</td><td>-</td><td>-</td><td>아니요</td><td>128K</td></tr>
</tbody></table>
<h2>3.  시각 지능의 혁신: 컴퓨터 비전 파운데이션 모델</h2>
<p>Meta는 대규모 언어 모델뿐만 아니라, 컴퓨터 비전 분야에서도 파운데이션 모델의 새로운 지평을 열었다. 특히 Segment Anything Model (SAM)과 Detectron2는 각각 이미지 분할과 객체 탐지 분야에서 패러다임의 전환을 이끌고 있다.</p>
<h3>3.1  Segment Anything Model (SAM): 제로샷 분할의 패러다임 전환</h3>
<p>**Segment Anything Model (SAM)**은 특정 데이터셋에 대한 추가적인 미세 조정 없이, 어떠한 이미지 속의 어떠한 객체라도 분할(segmentation)할 수 있는 범용 파운데이션 모델을 목표로 개발되었다.22 이는 기존의 분할 모델들이 특정 클래스(예: 사람, 자동차)나 특정 도메인(예: 의료 영상)에 대해서만 작동했던 한계를 근본적으로 극복하려는 시도이다.</p>
<p>SAM의 핵심 기능은 **‘프롬프트 기반 분할(Promptable Segmentation)’**이다.22 사용자가 분할하고자 하는 객체에 대해 점(point)을 찍거나, 경계 상자(bounding box)를 그리거나, 대략적인 마스크를 제공하는 등 다양한 형태의 프롬프트를 입력하면, SAM은 이를 실시간으로 해석하여 해당 객체의 정교한 마스크를 생성한다.22 만약 프롬프트가 여러 객체를 가리킬 수 있는 모호한 상황이라면, SAM은 여러 개의 유효한 마스크를 동시에 생성하여 사용자가 가장 적합한 결과를 선택할 수 있도록 함으로써 상호작용성과 정확성을 높였다.24</p>
<p>이러한 혁신적인 기능은 세 부분으로 구성된 독창적인 아키텍처에 의해 구현된다 22:</p>
<ul>
<li><strong>a) 이미지 인코더 (Image Encoder)</strong>: 무거운 Vision Transformer (ViT-H) 아키텍처를 기반으로 하며, 6억 3200만(632M) 개의 파라미터를 가진다.22 이 인코더는 입력 이미지를 단 한 번만 처리하여 고차원의 이미지 임베딩(embedding)을 생성한다. 이 과정은 NVIDIA A100 GPU 기준으로 약 0.15초가 소요되며, 한 번 계산된 임베딩은 여러 다른 프롬프트에 대해 재사용될 수 있어 전체적인 시스템의 효율성을 극대화한다.22</li>
<li><strong>b) 프롬프트 인코더 (Prompt Encoder)</strong>: 점, 상자, 마스크, 텍스트 등 다양한 유형의 프롬프트를 실시간으로 임베딩 벡터로 변환한다. 희소한 프롬프트(점, 상자)는 위치 인코딩(positional encoding)과 학습된 임베딩을 통해 표현되고, 조밀한 프롬프트(마스크)는 컨볼루션 연산을 통해 임베딩된 후 이미지 임베딩과 합산된다.</li>
<li><strong>c) 마스크 디코더 (Mask Decoder)</strong>: 400만(4M) 개의 파라미터를 가진 경량 트랜스포머 디코더로, 이미지 임베딩과 프롬프트 임베딩을 입력받아 최종 객체 마스크를 예측한다.22 두 임베딩 간의 상호작용을 모델링하기 위해 셀프 어텐션과 크로스 어텐션을 활용하며, CPU 환경에서도 수십 밀리초 내에 실행될 만큼 매우 빠르고 효율적으로 설계되었다.22</li>
</ul>
<p>SAM 프로젝트의 또 다른 핵심적인 기여는 <strong>SA-1B 데이터셋</strong>이다. 이 데이터셋은 1,100만 개의 고해상도 이미지에 대해 <strong>11억 개 이상의 고품질 마스크</strong>를 포함하는, 현존하는 가장 큰 규모의 분할 데이터셋이다.22 이 방대한 데이터셋은 ’모델-인-더-루프(model-in-the-loop)’라는 독특한 방식으로 구축되었다.22 초기에는 불완전한 SAM 모델을 이용해 주석 작업을 보조하고, 전문 주석가들이 모델의 예측을 수정하면 그 데이터를 다시 모델 학습에 사용하여 모델과 데이터의 품질을 점진적으로 함께 향상시키는 선순환 구조를 만든 것이다. 이 방법론은 고품질 데이터셋 구축에 필요한 비용과 시간을 극적으로 단축시키는 새로운 패러다임을 제시했으며, SAM 모델 자체의 성능만큼이나 중요한 혁신으로 평가받는다. SAM을 만들어낸 이 ‘데이터 엔진’ 프로세스는 의료, 위성, 자율주행 등 데이터 레이블링이 어렵고 비용이 많이 드는 모든 전문 분야에 적용될 수 있는 강력한 청사진을 제공한다.</p>
<p>최근에는 SAM의 후속 버전인 <strong>SAM 2</strong>가 공개되었으며, 이는 기존의 이미지 분할 기능을 비디오로 확장하고 스트리밍 메모리 아키텍처를 도입하여 실시간 비디오 객체 분할을 지원하는 방향으로 발전하고 있다.12</p>
<h3>3.2  Detectron2: 객체 탐지 및 분할 연구 플랫폼</h3>
<p><strong>Detectron2</strong>는 Meta의 Facebook AI Research (FAIR) 팀이 개발한 차세대 객체 탐지(object detection) 및 분할(segmentation) 라이브러리이자 연구 플랫폼이다.29 Detectron2의 핵심 목표는 전 세계 컴퓨터 비전 연구자들이 최신 알고리즘을 신속하게 구현하고, 실험하며, 평가할 수 있는 유연하고 강력한 기반을 제공하는 것이다.30</p>
<p>Detectron2는 이전 버전인 Detectron에 비해 몇 가지 중요한 개선점을 가진다 30:</p>
<ul>
<li><strong>PyTorch 기반 전환</strong>: 기존의 Detectron이 Caffe2 프레임워크를 기반으로 했던 것과 달리, Detectron2는 PyTorch로 완전히 재작성되었다. 이는 연구 커뮤니티에서 압도적인 지지를 받는 PyTorch의 유연성과 직관적인 디버깅 환경을 활용하여 개발 편의성과 생산성을 크게 향상시켰다.</li>
<li><strong>모듈식 설계</strong>: 모델의 백본(backbone), 헤드(head), 손실 함수(loss function) 등 각 구성 요소를 독립적인 모듈로 설계하여, 연구자들이 특정 부분만을 쉽게 교체하거나 확장하여 새로운 아이디어를 실험할 수 있도록 유연성을 극대화했다.</li>
<li><strong>성능 및 속도 향상</strong>: 전체 훈련 파이프라인을 GPU에서 효율적으로 실행되도록 최적화하여, 이전 버전에 비해 훈련 속도가 훨씬 빨라졌다.</li>
</ul>
<p>Detectron2는 단일 라이브러리가 아니라, 최신 컴퓨터 비전 모델들을 집대성한 일종의 ‘모델 동물원(Model Zoo)’ 역할을 한다.29 여기에는 Mask R-CNN, Faster R-CNN, RetinaNet과 같은 고전적인 모델부터 DensePose, Panoptic FPN, PointRend, Cascade R-CNN 등 다양한 최신 알고리즘들이 구현되어 있어, 연구자들이 광범위한 문제에 대해 강력한 베이스라인 모델을 즉시 활용할 수 있도록 지원한다.29</p>
<h2>4.  언어의 장벽을 넘어서: 음성 및 번역 모델</h2>
<p>Meta는 텍스트와 이미지를 넘어, 인간 소통의 근간이 되는 음성 분야에서도 혁신적인 파운데이션 모델을 통해 언어의 장벽을 허물고 있다. wav2vec 2.0과 Seamless Communication 프로젝트는 각각 음성 인식과 다국어 번역 분야에서 새로운 표준을 제시했다.</p>
<h3>4.1  wav2vec 2.0: 자기 지도 학습 기반 음성 표현의 혁신</h3>
<p><strong>wav2vec 2.0</strong>은 레이블이 지정되지 않은 대량의 원시 음성 오디오 데이터만으로 강력한 음성 표현(representation)을 학습하는 자기 지도 학습(self-supervised learning) 프레임워크이다.33 이 모델의 핵심 아이디어는, 방대한 양의 전사(transcription) 데이터가 필요한 기존의 지도 학습 방식의 한계를 극복하고, 소량의 레이블링된 데이터만으로도 높은 성능의 음성 인식 모델을 구축하는 것이다.</p>
<p>wav2vec 2.0의 학습 방식은 세 가지 핵심 요소로 구성된다 33:</p>
<ol>
<li><strong>잠재 공간 마스킹 (Latent Space Masking)</strong>: 먼저, 원시 오디오 파형은 다층 컨볼루션 신경망(CNN) 인코더를 통해 연속적인 잠재 표현(latent representation)으로 변환된다. 그 후, BERT의 마스크드 언어 모델링(Masked Language Modeling)과 유사하게, 이 잠재 표현 시퀀스의 일부 구간을 무작위로 마스킹한다.</li>
<li><strong>대조적 과제 (Contrastive Task)</strong>: 모델은 마스킹된 위치의 ‘진짜’ 잠재 표현을, 동일 발화 내 다른 위치에서 추출된 여러 개의 ‘가짜’ 잠재 표현(distractors) 중에서 구별해내는 대조적 과제를 해결하도록 훈련된다. 이 과정을 통해 모델은 주변 문맥 정보를 활용하여 마스킹된 부분의 음향적, 언어적 특징을 예측하는 능력을 학습하게 된다.</li>
<li><strong>양자화 (Quantization)</strong>: wav2vec 2.0은 연속적인 잠재 표현을 이산적인 ’음성 단위(speech units)’로 양자화하는 과정을 학습에 도입했다. Gumbel-Softmax 기법을 사용하여 이 과정을 미분 가능하게 만들고, 양자화된 이산 단위를 대조적 과제의 목표(target)로 사용한다. 이는 모델이 노이즈나 화자 특성과 같은 비본질적인 정보 대신, 음소와 같은 핵심적인 음성 단위를 학습하도록 유도하여 표현의 강건성과 일반화 성능을 높이는 데 결정적인 역할을 한다.</li>
</ol>
<p>wav2vec 2.0의 가장 큰 의의는 음성 인식 기술의 접근성을 획기적으로 개선했다는 점이다. 수천 시간의 전사 데이터가 없으면 불가능했던 고성능 음성 인식을, 단 10분의 레이블링된 데이터만으로도 실용적인 수준까지 구현할 수 있음을 보여주었다.33 이는 전 세계 약 7,000여 개 언어 중 데이터가 부족한 대다수의 저자원 언어(low-resource languages)에 대한 음성 기술 개발의 새로운 가능성을 열었다.</p>
<h3>4.2  Seamless Communication: 통합 다국어·멀티모달 번역</h3>
<p><strong>Seamless Communication</strong>은 공상과학 소설에 등장하는 ’바벨 피쉬’처럼, 서로 다른 언어를 사용하는 사람들 간의 원활하고 자연스러운 의사소통을 가능하게 하는 단일 통합 모델 구축을 목표로 하는 야심 찬 프로젝트이다.34</p>
<p>프로젝트의 핵심인 <strong>SeamlessM4T</strong> 모델은 하나의 통합된 아키텍처 내에서 5가지 핵심적인 번역 및 인식 작업을 모두 수행할 수 있다 36:</p>
<ul>
<li>음성-음성 번역 (Speech-to-Speech Translation, S2ST)</li>
<li>음성-텍스트 번역 (Speech-to-Text Translation, S2TT)</li>
<li>텍스트-음성 번역 (Text-to-Speech Translation, T2ST)</li>
<li>텍스트-텍스트 번역 (Text-to-Text Translation, T2TT)</li>
<li>자동 음성 인식 (Automatic Speech Recognition, ASR)</li>
</ul>
<p>약 100개의 언어를 지원하는 SeamlessM4T는 음성 인식(ASR), 기계 번역(MT), 음성 합성(TTS) 모델을 개별적으로 연결하는 기존의 캐스케이드(cascaded) 방식과 비교할 때, 각 단계에서 발생하는 오류의 누적을 방지하고 지연 시간을 줄여 훨씬 더 높은 품질과 효율성을 제공한다.34</p>
<p>이러한 통합 모델은 Meta가 수년간 축적해 온 개별 분야의 최첨단 연구 성과들을 유기적으로 결합한 결과물이다. 음성 표현 학습을 위한 <strong>w2v-BERT 2.0</strong> (초기 100만 시간에서 v2에서는 450만 시간의 음성 데이터로 확장 학습)을 강력한 음성 인코더로 사용하고, 200개 언어를 지원하는 NLLB(No Language Left Behind) 프로젝트의 성과를 텍스트 인코더 및 디코더에 통합했다.34 또한, 모델 학습을 위해 자동으로 정렬된 대규모 다중 모달 번역 코퍼스인 <strong>SeamlessAlign</strong>을 자체적으로 구축하여 활용했다.34 이는 Meta의 연구 개발이 단순히 ’최고의 부품’을 만드는 단계를 넘어, 이들을 융합하여 ’최고의 완성 시스템’을 조립하는 단계로 진입했음을 보여준다.</p>
<p>SeamlessM4T를 기반으로, Meta는 특정 기능에 특화된 다양한 파생 모델들을 지속적으로 선보이고 있다:</p>
<ul>
<li><strong>SeamlessM4T v2</strong>: UnitY2라는 새로운 비자기회귀적(non-autoregressive) 디코더 아키텍처를 도입하여, 음성 생성 품질과 추론 속도를 한층 더 개선했다.37</li>
<li><strong>SeamlessExpressive</strong>: 단순한 내용 전달을 넘어, 번역된 음성에 원본 화자의 말하는 속도, 억양, 감정 등 표현적인 요소(prosody)를 보존하는 데 중점을 둔 모델이다. 이는 통합 모델이기에 가능한, 캐스케이드 방식으로는 구현하기 어려운 고차원적인 기능이다.35</li>
<li><strong>SeamlessStreaming</strong>: 약 2초의 매우 낮은 지연 시간(latency)으로 실시간 스트리밍 번역을 지원하여, 동시통역과 같은 실시간 소통 시나리오에 적용될 수 있는 가능성을 열었다.35</li>
</ul>
<h2>5.  Meta AI 생태계를 구성하는 핵심 도구와 라이브러리</h2>
<p>Meta의 오픈소스 전략은 단순히 개별 모델을 공개하는 데 그치지 않고, 이 모델들을 활용하고 발전시킬 수 있는 강력한 도구와 라이브러리 생태계를 함께 구축하는 것을 포함한다. 이 생태계의 중심에는 연구와 개발의 전 과정을 지원하는 핵심적인 프레임워크들이 자리 잡고 있다.</p>
<ul>
<li><strong>PyTorch</strong>: Meta가 주도적으로 개발하고 있는 딥러닝 프레임워크로, 직관적인 인터페이스와 ‘Define-by-Run’ 철학을 바탕으로 한 유연성 덕분에 전 세계 AI 연구 커뮤니티에서 사실상의 표준으로 자리 잡았다.41 Llama, SAM, Detectron2 등 Meta가 공개한 대부분의 AI 모델은 PyTorch를 기반으로 구축되어, 연구자들이 쉽게 코드를 이해하고 수정하며 새로운 아이디어를 실험할 수 있는 기반을 제공한다.30</li>
<li><strong>Fairseq</strong>: 기계 번역, 텍스트 요약, 언어 모델링 등과 같은 서열-투-서열(sequence-to-sequence) 작업을 위한 모델링 툴킷이다.36 Meta의 다국어 번역 모델인 NLLB와 SeamlessM4T 등 수많은 언어 모델 연구의 핵심적인 역할을 수행했으며, 연구자들이 복잡한 트랜스포머 기반 모델을 효율적으로 훈련하고 평가할 수 있도록 지원한다.</li>
<li><strong>Faiss (Facebook AI Similarity Search)</strong>: 고차원 벡터 공간에서 유사도 검색을 초고속으로 수행할 수 있도록 최적화된 라이브러리이다.36 수십억 개에 달하는 대규모 임베딩 벡터에 대해서도 효율적인 근접 이웃 탐색(nearest neighbor search)을 지원하여, 검색 증강 생성(Retrieval-Augmented Generation, RAG) 시스템이나 추천 시스템 등에서 핵심적인 구성 요소로 활용된다.</li>
</ul>
<p>이 외에도 Meta는 다양한 목적을 위한 전문화된 도구들을 오픈소스로 제공하며 생태계를 풍부하게 만들고 있다:</p>
<ul>
<li><strong>FastText</strong>: 단어 임베딩 학습과 텍스트 분류를 위한 경량 라이브러리로, 특히 대규모 데이터셋과 저자원 환경에서도 빠르고 효율적으로 작동하도록 설계되었다.36</li>
<li><strong>ReAgent</strong>: 강화 학습(Reinforcement Learning) 및 컨텍스추얼 밴딧(Contextual Bandits)과 같은 대규모 상호작용형 추론 시스템을 설계, 배포, 평가하기 위한 엔드투엔드 플랫폼이다.36</li>
<li><strong>Detectron</strong>: Detectron2의 전신으로, Caffe2 기반으로 구축된 객체 탐지 연구 코드베이스이다. Detectron2의 개발에 중요한 기반이 되었다.41</li>
</ul>
<table><thead><tr><th>프로젝트명</th><th>분류</th><th>핵심 기능</th><th>라이선스</th></tr></thead><tbody>
<tr><td><strong>Llama 4</strong></td><td>대규모 언어 모델 (LLM)</td><td>MoE 아키텍처, 네이티브 멀티모달, 10M 토큰 컨텍스트</td><td>Llama 4 Community License</td></tr>
<tr><td><strong>SAM 2</strong></td><td>컴퓨터 비전</td><td>제로샷 비디오/이미지 분할, 프롬프트 기반 인터페이스</td><td>Apache 2.0</td></tr>
<tr><td><strong>SeamlessM4T</strong></td><td>음성/번역</td><td>통합 5-in-1 번역 (S2ST, S2TT 등), 100개 언어 지원</td><td>CC-BY-NC 4.0</td></tr>
<tr><td><strong>wav2vec 2.0</strong></td><td>음성</td><td>자기 지도 학습 기반 음성 표현 학습</td><td>MIT</td></tr>
<tr><td><strong>Detectron2</strong></td><td>컴퓨터 비전</td><td>객체 탐지 및 분할 연구 플랫폼, 모델 동물원</td><td>Apache 2.0</td></tr>
<tr><td><strong>PyTorch</strong></td><td>딥러닝 프레임워크</td><td>유연한 딥러닝 모델 구축, 훈련, 배포</td><td>BSD-style</td></tr>
<tr><td><strong>Faiss</strong></td><td>라이브러리</td><td>대규모 벡터 유사도 고속 검색</td><td>MIT</td></tr>
</tbody></table>
<h2>6.  종합 평가 및 미래 전망</h2>
<p>Meta의 오픈소스 AI 전략은 기술적 성취와 생태계 기여라는 두 가지 측면에서 괄목할 만한 성과를 거두었다. Llama 시리즈를 통해 오픈소스 대규모 언어 모델의 성능을 최상위 독점 모델에 근접한 수준까지 끌어올렸으며, SAM과 wav2vec 2.0을 통해 각각 컴퓨터 비전과 음성 처리 분야에서 파운데이션 모델의 새로운 가능성을 제시했다. 이러한 기술적 성과는 전 세계 개발자와 연구자들에게 전례 없는 수준의 AI 기술 접근성을 제공했다. Llama의 등장은 Vicuna, Alpaca와 같은 수많은 고품질 파생 모델의 탄생을 촉발했으며 9, 한국의 Upstage나 일본의 Elyza와 같은 스타트업들이 이를 기반으로 자체 상용 모델을 개발하는 등 글로벌 AI 혁신을 가속하는 기폭제가 되었다.1 또한, Meta는 Llama Guard, Prompt Guard와 같은 안전 도구를 함께 배포하며, 커뮤니티가 책임감 있는 AI를 구축할 수 있도록 지원하는 노력도 병행하고 있다.2</p>
<p>하지만 이러한 성과 이면에는 여전히 해결해야 할 과제들이 남아있다. 오픈소스 모델의 직접적인 수익화 모델 부재, 복잡한 라이선스 정책, 그리고 기업 사용자들이 우려하는 보안 및 거버넌스 문제 등은 오픈소스 AI가 주류로 자리 잡기 위해 반드시 넘어야 할 산이다.4</p>
<p>Meta의 미래 로드맵은 ‘개인 초지능(Personal Superintelligence)’ 구축이라는 새로운 비전을 향하고 있다.4 이는 인류 전체를 위한 범용 인공지능(AGI)보다는, 각 개인 사용자에게 고도로 맞춤화되고 일상에 깊숙이 통합된 효율적인 AI 어시스턴트에 집중하겠다는 전략적 방향 전환을 시사한다. 이러한 비전은 이미 Meta AI 앱, Ray-Ban Meta 안경 등 자사의 하드웨어 및 소프트웨어 플랫폼과의 긴밀한 통합을 통해 구체화되고 있다.5</p>
<p>결론적으로, Meta의 오픈소스 전략은 앞으로도 이러한 ’개인 초지능’을 실현하는 과정에서 핵심적인 역할을 수행할 것이다. 전 세계 개발자 커뮤니티의 집단 지성을 활용하여 기술적 난제를 해결하고, 방대한 생태계를 통해 혁신적인 사용 사례를 발굴하며, 자사 플랫폼의 경쟁력을 강화하는 선순환 구조를 구축하는 데 있어 오픈소스는 대체 불가능한 수단으로 계속 유지될 전망이다. Meta는 AI 기술의 미래가 소수의 거대 기업에 의해 독점되는 것이 아니라, 개방과 협력을 통해 모두에게 열려 있어야 한다는 철학을 가장 강력하게 실천하고 있는 기업으로 평가받을 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Open Source AI | Meta, https://ai.meta.com/temp/open-source-ai/</li>
<li>Introducing Llama 3.1: Our most capable models to date - AI at Meta, https://ai.meta.com/blog/meta-llama-3-1/</li>
<li>Introducing Meta Llama 3: The most capable openly available LLM …, https://ai.meta.com/blog/meta-llama-3/</li>
<li>Open source AI: Mission aborted?, https://economictimes.indiatimes.com/tech/artificial-intelligence/open-source-ai-mission-aborted/articleshow/123509924.cms</li>
<li>Personal AI that understands you - AI at Meta, https://ai.meta.com/meta-ai/</li>
<li>AI - Technology Teams | Meta Careers, https://www.metacareers.com/teams/technology?tab=AI</li>
<li>Llama (language model) - Wikipedia, https://en.wikipedia.org/wiki/Llama_(language_model)</li>
<li>[2302.13971] LLaMA: Open and Efficient Foundation Language Models - arXiv, https://arxiv.org/abs/2302.13971</li>
<li>Introduction to Meta AI’s LLaMA: Empowering AI Innovation | DataCamp, https://www.datacamp.com/blog/introduction-to-meta-ai-llama</li>
<li>Open-Source Large Language Models and Their Market Impact | by ByteBridge - Medium, https://bytebridge.medium.com/open-source-large-language-models-and-their-market-impact-447db9fd2194</li>
<li>Meta Llama - Hugging Face, https://huggingface.co/meta-llama</li>
<li>Research - AI at Meta, https://ai.meta.com/research/</li>
<li>Grouped Query Attention (GQA) vs. Multi Head Attention (MHA): LLM Inference Serving Acceleration - FriendliAI, https://friendli.ai/blog/gqa-vs-mha</li>
<li>What is Grouped Query Attention (GQA)? - Klu.ai, https://klu.ai/glossary/grouped-query-attention</li>
<li>arXiv:2407.21783v3 [cs.AI] 23 Nov 2024, https://arxiv.org/pdf/2407.21783</li>
<li>meta-llama/Llama-3.2-1B - Hugging Face, https://huggingface.co/meta-llama/Llama-3.2-1B</li>
<li>Llama: Industry Leading, Open-Source AI, https://www.llama.com/</li>
<li>Llama 4 Runs on Arm, https://newsroom.arm.com/blog/llama-4-runs-on-arm</li>
<li>Meta AI: What is Llama 4 and why does it matter? - Zapier, https://zapier.com/blog/llama-meta/</li>
<li>Meta’s Llama 4 is now available on Workers AI - The Cloudflare Blog, https://blog.cloudflare.com/meta-llama-4-is-now-available-on-workers-ai/</li>
<li>unsloth/Llama-4-Scout-17B-16E-Instruct - Hugging Face, https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct</li>
<li>Segment Anything | Meta AI, https://segment-anything.com/</li>
<li>[2304.02643] Segment Anything - arXiv, https://arxiv.org/abs/2304.02643</li>
<li>Meta AI’s Segment Anything Model (SAM) Explained: The Ultimate Guide - Encord, https://encord.com/blog/segment-anything-model-explained/</li>
<li>What is Segment Anything Model (SAM) &amp; its Uses - Sama, https://www.sama.com/blog/what-is-segment-anything-model-sam-its-uses</li>
<li>1월 1, 1970에 액세스, https://arxiv.org/pdf/2304.02643.pdf</li>
<li>Segment Anything - Meta Research - Facebook, https://research.facebook.com/publications/segment-anything/</li>
<li>facebookresearch/segment-anything: The repository provides code for running inference with the SegmentAnything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model. - GitHub, https://github.com/facebookresearch/segment-anything</li>
<li>Detectron2 is a platform for object detection, segmentation and other visual recognition tasks. - GitHub, https://github.com/facebookresearch/detectron2</li>
<li>Detectron2 - AI at Meta, https://ai.meta.com/tools/detectron2/</li>
<li>Detectron2: A Rundown of Meta’s Computer Vision Framework - Viso Suite, https://viso.ai/deep-learning/detectron2/</li>
<li>Detectron2 Object Detection Model: What is, How to Use - Roboflow, https://roboflow.com/model/detectron2</li>
<li>wav2vec 2.0: A Framework for Self-Supervised Learning of Speech …, https://arxiv.org/pdf/2006.11477</li>
<li>SeamlessM4T—Massively Multilingual &amp; Multimodal Machine Translation - AI at Meta, https://ai.meta.com/research/publications/seamlessm4t-massively-multilingual-multimodal-machine-translation/</li>
<li>Seamless Communication - AI at Meta, https://ai.meta.com/research/seamless-communication/</li>
<li>Models and libraries - Meta AI, https://ai.meta.com/resources/models-and-libraries/</li>
<li>facebook/seamless-m4t-v2-large - Hugging Face, https://huggingface.co/facebook/seamless-m4t-v2-large</li>
<li>Paper page - SeamlessM4T-Massively Multilingual &amp; Multimodal …, https://huggingface.co/papers/2308.11596</li>
<li>Seamless Communication Models - AI at Meta, https://ai.meta.com/resources/models-and-libraries/seamless-communication-models/</li>
<li>Seamless Translation | Meta FAIR, https://seamless.metademolab.com/</li>
<li>FAIR at 5: Facebook Artificial Intelligence Research accomplishments - Engineering at Meta, https://engineering.fb.com/2018/12/05/ai-research/fair-fifth-anniversary/</li>
<li>The Open Source AI Revolution: How Llama and Other Models Are Democratizing Artificial Intelligence | by Ishana sabrish | Medium, https://medium.com/@ishanasabrish78/the-open-source-ai-revolution-how-llama-and-other-models-are-democratizing-artificial-intelligence-030a6e599915</li>
<li>The Llama Ecosystem: Past, Present, and Future - AI at Meta, https://ai.meta.com/blog/llama-2-updates-connect-2023/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>