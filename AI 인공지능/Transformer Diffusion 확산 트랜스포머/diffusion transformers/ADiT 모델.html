<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:ADiT 모델</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>ADiT 모델</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">트랜스포머 기반 확산 모델 (Diffusion Models)</a> / <a href="index.html">DiT (Diffusion Transformer)</a> / <span>ADiT 모델</span></nav>
                </div>
            </header>
            <article>
                <h1>ADiT 모델</h1>
<p>확산 모델, 트랜스포머, 그리고 CLIP의 융합</p>
<h2>1. 생성형 AI의 새로운 종합, 컨볼루션에서 트랜스포머로</h2>
<p>인공지능(AI)의 하위 분야인 생성 모델링은 최근 몇 년간 괄목할 만한 발전을 이루었으며, 특히 이미지 생성 및 변환 기술은 학계와 산업계 모두에서 지대한 관심을 받고 있다. 이 안내서는 “주의 기반 확산 트랜스포머(Attention-based Diffusion Transformer)”, 약칭 ADiT 모델에 대한 심층적인 기술적 고찰을 제공하는 것을 목표로 한다. ADiT는 단일한 발명품이라기보다는, 기계 학습 분야에서 동시에 발생한 여러 핵심적인 돌파구를 정교하게 종합하여 이전 세대 모델들의 근본적인 한계를 극복하고자 설계된 아키텍처이다. 따라서 ADiT를 온전히 이해하기 위해서는 이 모델이 탄생한 지적, 기술적 배경을 먼저 탐색하는 것이 필수적이다. 본 서론에서는 생성적 적대 신경망(GAN)의 시대부터 확산 모델의 부상, 그리고 컴퓨터 비전 분야에 트랜스포머가 가져온 파괴적 혁신에 이르기까지의 과정을 추적하며, ADiT가 이러한 기술적 흐름의 정점에서 어떻게 탄생했는지를 조망할 것이다.</p>
<h3>1.1  생성적 적대 신경망(GAN)의 시대와 그 불만</h3>
<p>2014년 이안 굿펠로우(Ian Goodfellow)와 동료들에 의해 제안된 생성적 적대 신경망(Generative Adversarial Networks, GANs)은 생성 모델 분야에 혁명을 일으켰다.1 GAN은 생성자(Generator)와 판별자(Discriminator)라는 두 개의 신경망이 서로 경쟁하는 독특한 적대적 학습(adversarial training) 구조를 통해 실제와 구별하기 어려운 고품질의 이미지를 생성하는 데 성공했다.1 생성자는 무작위 노이즈로부터 가짜 데이터를 생성하고, 판별자는 실제 데이터와 생성된 가짜 데이터를 구별하도록 학습한다. 이 과정은 제로섬 게임(zero-sum game)과 같아서, 생성자는 판별자를 속이는 방향으로, 판별자는 더 정확하게 구별하는 방향으로 발전하며, 결국 생성자는 매우 사실적인 데이터를 만들어내게 된다.3</p>
<p>이러한 능력 덕분에 GAN은 이미지 생성, 스타일 변환, 초해상도 등 다양한 분야에서 지배적인 패러다임으로 자리 잡았다. 특히, 조건부 GAN(conditional GANs, cGANs)의 등장은 특정 조건(예: 클래스 레이블, 텍스트, 다른 이미지)에 맞는 이미지를 생성할 수 있게 함으로써 그 활용 범위를 크게 넓혔다. 이 중 Pix2Pix와 같은 모델은 한 이미지 도메인에서 다른 도메인으로 변환하는 ‘이미지-투-이미지(Image-to-Image) 변환’ 작업을 위해 설계되었으며, 이는 ADiT 모델이 직접적으로 성능을 비교하는 중요한 기준점이 되었다.4</p>
<p>그러나 GAN의 강력한 성능 이면에는 근본적인 한계점들이 존재했다. 가장 큰 문제는 ’학습의 불안정성’이었다. 생성자와 판별자 간의 균형을 맞추는 것이 매우 어려워, 학습 과정이 종종 발산하거나 붕괴되는 현상이 발생했다.2 또한, 생성자가 판별자를 속이기 쉬운 소수의 특정 결과물만을 반복적으로 생성하는 ‘모드 붕괴(mode collapse)’ 현상은 생성된 이미지의 다양성을 심각하게 저해하는 고질적인 문제였다.5 이러한 GAN의 내재적 불안정성은 더 안정적이고 예측 가능한 학습 과정을 제공하는 새로운 생성 모델링 프레임워크에 대한 탐색을 촉발하는 결정적인 계기가 되었다.</p>
<h3>1.2  확산 모델의 부상: 안정성과 품질의 새로운 패러다임</h3>
<p>GAN의 한계에 대한 대안으로 부상한 것이 바로 확산 모델(Diffusion Models)이다. 2020년경부터 본격적으로 주목받기 시작한 확산 모델은 열역학에서 영감을 받은 독특한 접근법을 취한다.6 이 모델은 두 단계의 프로세스로 구성된다. 첫 번째는 ’순방향 확산 과정(forward diffusion process)’으로, 원본 이미지에 점진적으로 가우시안 노이즈(Gaussian noise)를 추가하여 이미지를 순수한 노이즈 상태로 만드는 과정이다. 두 번째는 ’역방향 제거 과정(reverse denoising process)’으로, 신경망이 순방향 과정의 역을 학습하여 노이즈 상태에서 시작해 점진적으로 노이즈를 제거함으로써 원본 이미지를 복원하는 과정이다.1</p>
<p>확산 모델은 GAN에 비해 몇 가지 결정적인 장점을 가지고 있다. 첫째, 학습 과정이 매우 안정적이다. 적대적 학습에서 발생하는 민감한 균형 맞추기 문제가 없기 때문에, 모델 붕괴 없이 안정적으로 수렴하는 경향이 있다.5 둘째, 복잡한 데이터 분포를 매우 정밀하게 포착하여 높은 품질과 뛰어난 다양성을 갖춘 샘플을 생성할 수 있다.2</p>
<p>초기 확산 모델은 픽셀 공간에서 직접 연산을 수행하여 계산 비용이 매우 높다는 단점이 있었다. 이 문제를 해결한 것이 ’잠재 확산 모델(Latent Diffusion Models, LDM)’이다. Stable Diffusion과 같은 LDM은 고차원의 픽셀 이미지를 저차원의 ’잠재 공간(latent space)’으로 압축하는 사전 학습된 변이형 오토인코더(Variational Autoencoder, VAE)를 사용한다.8 확산 과정 전체가 이 압축된 잠재 공간에서 이루어지므로, 계산 효율성이 극적으로 향상되어 고해상도 이미지 생성도 가능해졌다.8 이 잠재 공간에서의 확산 패러다임은 ADiT 아키텍처가 직접적으로 계승하는 핵심적인 기반 기술이 된다.</p>
<h3>1.3  컴퓨터 비전 분야를 뒤흔든 트랜스포머의 혁신</h3>
<p>자연어 처리(NLP) 분야에서 혁명을 일으켰던 트랜스포머(Transformer) 아키텍처는 2020년 ’비전 트랜스포머(Vision Transformer, ViT)’의 등장과 함께 컴퓨터 비전 분야에도 지대한 영향을 미치기 시작했다. 기존의 컨볼루션 신경망(Convolutional Neural Networks, CNNs)이 지역적인 특징을 포착하는 데 강점을 가졌던 것과 달리, 트랜스포머의 핵심인 ‘셀프 어텐션(self-attention)’ 메커니즘은 이미지 내 모든 픽셀(또는 패치) 간의 관계를 동시에 고려하여 장거리 의존성(long-range dependencies)과 전역적 맥락(global context)을 효과적으로 모델링할 수 있었다.10</p>
<p>이러한 트랜스포머의 강점은 확산 모델과의 결합으로 이어졌다. 2023년 피블스(Peebles)와 시에(Xie)는 ’확산 트랜스포머(Diffusion Transformer, DiT)’라는 논문에서 기존 확산 모델의 표준적인 백본(backbone)이었던 U-Net 아키텍처를 트랜스포머로 대체하는 획기적인 시도를 선보였다.4 DiT는 뛰어난 확장성(scalability)을 보여주며, 모델의 크기와 데이터셋이 커짐에 따라 생성 품질이 지속적으로 향상되는 것을 입증했다.10 이 연구는 트랜스포머가 확산 모델의 노이즈 제거 엔진으로서 U-Net을 능가할 수 있는 잠재력을 보여주었으며, ADiT 모델의 직접적인 아키텍처적 부모 역할을 하게 된다.</p>
<h3>1.4  논지: 세 가지 혁신의 삼위일체로서의 ADiT</h3>
<p>이 안내서의 핵심 논지는 다음과 같다: 주창(Zhu) 등이 발표한 논문 “확산 트랜스포머와 CLIP 기반 이미지 조건화를 이용한 이미지-투-이미지 변환“에서 제안된 ADiT 모델 12은, 특정하고 도전적인 과제인 ’쌍을 이룬 이미지-투-이미지 변환(paired image-to-image translation)’을 해결하기 위해 당대의 가장 강력한 세 가지 기술을 최초로 성공적으로 융합했다는 점에서 그 중요성을 갖는다. 그 세 가지 기술은 다음과 같다.</p>
<ol>
<li><strong>확산 모델</strong>의 안정적인 생성 프레임워크</li>
<li>**트랜스포머(DiT)**의 강력한 전역적 모델링 능력</li>
<li><strong>CLIP 기반 이미지 조건화</strong>의 풍부한 의미론적 이해</li>
</ol>
<p>ADiT의 설계는 임의적인 선택의 결과가 아니라, 이전 모델들의 실패에 대한 체계적인 해결책의 연쇄 과정으로 이해할 수 있다. 첫째, GAN의 학습 불안정성 2은 더 안정적인 확산 모델 프레임워크를 채택하게 된 동기가 되었다.7 둘째, Stable Diffusion 등에서 표준으로 사용되는 U-Net은 컨볼루션 기반으로, 지역적 특징에는 강하지만 이미지 전체의 구조적 일관성을 유지하는 데에는 한계가 있었다. 특히 원본 이미지의 전역적 구조를 보존하는 것이 무엇보다 중요한 이미지 변환 작업에서, 트랜스포머의 전역적 맥락 이해 능력은 논리적인 대안이었다.11 셋째, 이미지 변환은 조건화(conditioning)를 필요로 한다. 기존의 텍스트-투-이미지 모델들은 텍스트 프롬프트를 사용했지만 9, 텍스트는 특정 인물의 얼굴과 같은 원본 이미지의 고유한 정체성과 구조를 포착하기에는 너무 추상적이거나 불충분했다. ADiT 논문은 명시적으로 텍스트나 클래스 레이블에서 벗어나고자 했으며 4, 그 대안으로 이미지 자체를 의미론적 벡터로 인코딩하는 CLIP 13을 채택했다. 이 이미지 임베딩은 텍스트보다 훨씬 풍부하고 정밀한 조건화 신호를 제공하여 원본의 정체성을 보존하는 데 결정적인 역할을 했다.</p>
<p>결론적으로 ADiT는 생성형 AI 분야의 더 넓은 추세, 즉 ‘다중 패러다임(multi-paradigm)’ 모델로의 전환을 예고하는 중요한 지표이다. 단일한 거대 아키텍처에 의존하는 대신, 미래의 최첨단 모델들은 특정 문제를 해결하기 위해 각기 다른 기반 기술(예: 확산 과정, 어텐션 메커니즘, 대조적 언어-비전 모델)의 장점만을 결합하는 복잡한 종합체가 될 가능성이 높다. ADiT는 이러한 모듈식, 문제 해결 중심의 모델 구축 방식에 대한 초기이면서도 매우 성공적인 사례로 평가받을 수 있다.</p>
<h2>2. ADiT 모델의 아키텍처 해부</h2>
<p>ADiT 모델의 혁신성을 이해하기 위해서는 그 내부 구조를 정밀하게 분해하고 각 구성 요소의 기능과 상호작용을 분석하는 것이 필수적이다. 본 섹션에서는 주창(Zhu) 등의 원 논문 4에 기술된 내용을 바탕으로 ADiT 모델의 기술적 구성 요소를 심층적으로 해부한다. 모델의 데이터 흐름과 각 아키텍처 선택이 어떤 목적을 가지고 이루어졌는지, 그리고 이들이 어떻게 유기적으로 결합하여 고품질의 이미지 변환을 달성하는지를 살펴볼 것이다.</p>
<h3>2.1  잠재 확산 기반</h3>
<p>ADiT 모델은 고차원의 픽셀 공간에서 직접 작동하지 않고, 계산적으로 효율적인 저차원의 ’잠재 공간(latent space)’에서 작동한다. 이는 잠재 확산 모델(LDM) 패러다임을 직접적으로 계승한 것으로 15, 사전 학습된 변이형 오토인코더(VAE)를 활용한다.8</p>
<p>프로세스는 다음과 같이 진행된다. 먼저, VAE의 인코더(E)가 입력 이미지 x를 저차원의 잠재 표현(latent representation) <span class="math math-inline">z = E(x)</span>로 압축한다. 이후의 모든 확산 및 노이즈 제거 과정은 이 잠재 표현 z에 대해 수행된다. 수십 번의 노이즈 제거 단계가 완료되어 최종적인 잠재 표현 z0가 생성되면, VAE의 디코더(D)가 이를 다시 픽셀 공간으로 변환하여 최종 출력 이미지 <span class="math math-inline">\hat{x} = D(z_0)</span>를 만들어낸다.8</p>
<p>이러한 잠재 공간 접근법의 핵심적인 이점은 계산 복잡성의 극적인 감소에 있다. 예를 들어, 512×512 크기의 이미지를 8배 다운샘플링하는 VAE를 사용하면, 모델이 처리해야 할 데이터의 차원은 64×64로 줄어든다. 이는 픽셀 공간에서 직접 작업하는 것에 비해 계산량을 64배나 줄여주는 효과를 가져온다. 이 덕분에 계산적으로 매우 무거운 트랜스포머 아키텍처를 확산 모델의 백본으로 사용하는 것이 현실적으로 가능해졌다.4 즉, 잠재 공간 접근법은 ADiT가 트랜스포머의 강력한 모델링 능력을 활용하기 위한 필수적인 전제 조건이라고 할 수 있다.</p>
<h3>2.2  노이즈 제거 백본: 확산 트랜스포머 (DiT)</h3>
<p>ADiT의 핵심적인 노이즈 제거 엔진은 기존 확산 모델에서 널리 사용되던 U-Net 아키텍처가 아닌, ’확산 트랜스포머(DiT)’로 대체되었다.4 이는 ADiT의 가장 큰 구조적 특징 중 하나이다.</p>
<h4>2.2.1  패치 기반 처리</h4>
<p>트랜스포머는 본래 순차적인 데이터(sequence data)를 처리하도록 설계되었기 때문에, 2차원 그리드 형태의 이미지 데이터를 처리하기 위해서는 변환 과정이 필요하다. ADiT는 ViT와 유사한 방식을 채택하여, VAE로부터 얻은 잠재 표현 z를 겹치지 않는 여러 개의 작은 ’패치(patch)’로 분할한다.8 이렇게 분할된 각 패치는 선형 투영(linear projection)을 통해 벡터화되어, 트랜스포머가 처리할 수 있는 토큰 시퀀스(token sequence)를 형성한다. 이 과정에서 각 패치의 원래 공간적 위치 정보를 보존하기 위해, 각 토큰에 ’위치 임베딩(positional embedding)’이 더해진다.8 이로써 2차원 이미지는 트랜스포머가 이해할 수 있는 1차원 토큰 시퀀스로 변환된다.</p>
<h4>2.2.2  DiT 블록</h4>
<p>변환된 토큰 시퀀스는 여러 개의 ’DiT 블록(DiT block)’을 순차적으로 통과하며 처리된다. 각 DiT 블록은 자연어 처리용 트랜스포머의 기본 구조와 유사하게, ‘다중 헤드 셀프 어텐션(multi-head self-attention, MSA)’ 레이어와 ’피드포워드 신경망(feed-forward network, FFN)’으로 구성된다.8</p>
<p>셀프 어텐션 메커니즘은 ADiT가 U-Net 기반 모델과 차별화되는 핵심적인 이유이다. 특정 패치 토큰을 처리할 때, 셀프 어텐션은 시퀀스 내의 다른 모든 패치 토큰과의 관계를 계산하고 그 중요도를 가중치로 부여한다. 이 과정을 통해 모델은 이미지의 한 부분에서 다른 멀리 떨어진 부분까지의 관계, 즉 전역적 맥락과 장거리 의존성을 효과적으로 포착할 수 있다.10 이는 이미지 변환 시 원본 이미지의 전체적인 구조와 형태를 일관성 있게 유지하는 데 매우 중요한 역할을 한다. 반면, 컨볼루션 기반의 U-Net은 주로 인접한 픽셀들 간의 지역적 관계를 모델링하는 데 강점을 가지므로, 전역적 일관성을 확보하는 데에는 상대적으로 한계가 있다.</p>
<h3>2.3  다중 목표 손실 함수</h3>
<p>ADiT의 성공적인 학습은 서로 다른 목표를 균형 있게 달성하도록 설계된 정교한 ’다중 목표 손실 함수(multi-objective loss function)’에 크게 의존한다. 이 손실 함수는 세 가지 개별적인 손실 항의 가중치 합으로 구성되어, 모델이 서로 상충될 수 있는 요구사항들을 동시에 최적화하도록 유도한다.4</p>
<p>전체 손실 함수 <span class="math math-inline">L_\text{total}</span>는 다음과 같이 정의된다:<br />
<span class="math math-display">
L_\text{total}=λ_1 L_1 + λ_\text{lpips} L _\text{lpips} + λ _\text{clip} L_\text{clip}
</span><br />
여기서 <span class="math math-inline">\lambda_1, \lambda_\text{lpips}, \lambda_\text{clip}</span>은 각 손실 항의 중요도를 조절하는 하이퍼파라미터이다.</p>
<ol>
<li>L1 재구성 손실 (<span class="math math-inline">L_1</span>):</li>
</ol>
<p>이 손실은 생성된 이미지(<span class="math math-inline">\hat{x}</span>)와 실제 목표 이미지(<span class="math math-inline">x_\text{target}</span>) 간의 픽셀 단위 차이를 최소화하는 것을 목표로 한다. 이는 가장 기본적인 재구성 손실로, 생성된 이미지가 목표 이미지와 전반적으로 유사한 색상과 구조를 갖도록 강제한다.</p>
<ol start="2">
<li>LPIPS 지각 손실 (<span class="math math-inline">L_\text{lpips}</span>):</li>
</ol>
<p>LPIPS(Learned Perceptual Image Patch Similarity) 손실은 단순한 픽셀 값 비교를 넘어, 인간의 시각 시스템이 인지하는 방식과 유사하게 이미지 간의 유사성을 측정하는 고급 지표이다.16 사전 학습된 심층 신경망의 중간 레이어에서 추출한 특징 맵을 비교함으로써, LPIPS는 픽셀 값은 약간 다르더라도 인간이 보기에 유사한 질감이나 구조를 가진 이미지를 더 가깝다고 판단한다. 이는 생성된 이미지의 시각적 충실도(visual fidelity)를 향상시켜, 결과물이 더 자연스럽고 사실적으로 보이게 만드는 데 기여한다.4</p>
<ol start="3">
<li>CLIP 유사도 손실 (<span class="math math-inline">L_\text{clip}</span>):</li>
</ol>
<p>이것은 ADiT의 손실 함수에서 가장 독창적이고 중요한 부분이다. CLIP 유사도 손실은 생성된 이미지(\hat{x})와 **원본 소스 이미지(xsource)**의 CLIP 임베딩 간의 코사인 유사도(cosine similarity)를 최대화하도록 모델을 학습시킨다.4 즉, 이 손실은 생성된 이미지의 ’스타일’이 목표 이미지와 유사해지더라도, 그 ’의미’나 ’정체성’은 원본 소스 이미지와 동일하게 유지되도록 강제하는 역할을 한다. 예를 들어, 특정 인물의 얼굴 사진을 만화 스타일로 변환할 때, 이 손실은 변환된 만화 캐릭터가 여전히 원래 인물의 정체성을 유지하도록 보장한다.</p>
<p>이러한 아키텍처 설계는 이미지 변환이라는 과제가 가진 근본적인 긴장 관계, 즉 ’콘텐츠 보존(content preservation)’과 ‘스타일 변환(stylistic transformation)’ 사이의 균형을 어떻게 맞출 것인가에 대한 직접적인 해답을 제시한다. DiT 백본과 셀프 어텐션은 원본의 전역적 구조(콘텐츠)를 보존하는 데 유리한 구조를 제공한다. 그리고 손실 함수는 이 긴장 관계를 명시적으로 모델링한다. L1과 Llpips 손실은 생성된 결과물이 <em>목표</em> 스타일을 따르도록 유도하는 반면, Lclip 손실은 결과물이 <em>소스</em> 이미지의 의미를 잊지 않도록 강제한다.4 이 두 상반된 방향으로의 힘을 정교하게 조율하는 것이 ADiT 모델 성공의 핵심이다.</p>
<p>이는 복잡한 생성 과제에 있어 단일한 목표를 가진 ’단일 손실 함수(monolithic loss function)’가 더 이상 충분하지 않음을 시사한다. 미래의 정교한 생성 모델들은 ADiT의 사례처럼, 각기 다른, 때로는 서로 경쟁하는 결과물의 측면(예: 의미론, 지각적 사실성, 구조적 정확성)을 책임지는 여러 구성 요소로 이루어진 ‘다중 목표’ 또는 ‘다중 헤드’ 손실 함수를 채택하는 방향으로 발전할 가능성이 높다. ADiT의 손실 함수는 이러한 모듈식, 목표 지향적 최적화 접근법의 선구적인 사례로 볼 수 있다.</p>
<h2>3. 작동의 핵심: CLIP 기반 의미론적 조건화</h2>
<p>ADiT 모델의 가장 혁신적인 기여는 생성 과정을 안내하는 ‘조건화(conditioning)’ 방식에 있다. 기존 모델들이 주로 텍스트나 단순한 클래스 레이블에 의존했던 것과 달리, ADiT는 이미지 자체에 내재된 풍부한 의미 정보를 활용하여 변환 과정을 정밀하게 제어한다. 이 섹션에서는 ADiT가 어떻게 CLIP(Contrastive Language-Image Pre-training) 모델을 활용하여 기존의 한계를 뛰어넘는 미세하고 구조적으로 일관된 이미지 변환을 달성하는지 심층적으로 분석한다.</p>
<h3>3.1  텍스트와 클래스 레이블을 넘어서</h3>
<p>이미지-투-이미지 변환의 목표는 소스 도메인의 이미지를 타겟 도메인의 스타일로 바꾸면서도 원본의 핵심 콘텐츠를 보존하는 것이다. 기존의 조건부 생성 모델들은 주로 텍스트 프롬프트나 클래스 레이블을 조건으로 사용했다. 그러나 이러한 방식은 세밀한 변환 작업에 있어 명백한 한계를 가진다. 예를 들어, “한 남자의 만화 스타일 초상화“라는 텍스트 프롬프트는 매우 일반적이어서 수많은 결과물을 낳을 수 있지만, 특정 인물 ’존 스미스’의 얼굴 특징을 정확히 반영하는 만화 초상화를 생성하지는 못한다. 클래스 레이블 역시 너무 포괄적이어서 개별 객체의 고유한 정체성을 담아내지 못한다.</p>
<p>ADiT 논문은 이러한 한계를 명확히 인지하고, 텍스트나 클래스 레이블에 의존하지 않는 새로운 조건화 방법을 제안한다.4 모델의 목표는 소스와 타겟 도메인 간의 미묘한 시각적 관계를 파악하여 “세밀하고(fine-grained) 구조적으로 일관된(structurally consistent) 변환“을 달성하는 것이다.4 이를 위해 ADiT는 소스 이미지 자체를 조건으로 사용한다.</p>
<h3>3.2  CLIP의 비전 인코더 활용</h3>
<p>ADiT는 이 목표를 달성하기 위해 OpenAI가 개발한 강력한 대조적 언어-이미지 사전학습 모델인 CLIP을 활용한다. CLIP은 방대한 양의 이미지와 텍스트 쌍을 학습하여, 이미지와 텍스트를 동일한 다차원 임베딩 공간에 매핑할 수 있다. 이 과정에서 CLIP의 이미지 인코더는 이미지의 스타일, 색상 등 표면적인 특징보다는 추상적이고 의미론적인 콘텐츠, 즉 이미지의 ’무엇임(what-ness)’을 포착하는 능력을 학습하게 된다.13</p>
<p>ADiT는 사전 학습된 CLIP의 이미지 인코더(구체적으로 ViT-L/14)를 사용하여 소스 이미지로부터 고차원의 특징 벡터, 즉 ’이미지 임베딩(image embedding)’을 추출한다.4 이 임베딩은 소스 이미지의 핵심적인 의미론적 정수를 담고 있으며, 스타일 변화에 강건(robust)하다. 이 임베딩 벡터가 바로 ADiT의 노이즈 제거 과정을 안내하는 핵심적인 ‘지도’ 역할을 하게 된다.</p>
<h3>3.3  DiT에 의미론적 안내 주입</h3>
<p>추출된 CLIP 이미지 임베딩은 확산 트랜스포머(DiT)의 노이즈 제거 과정에 정교하게 주입되어야 한다. ADiT는 이를 위해 두 가지 주요 메커니즘을 사용한다.8</p>
<ol>
<li>다중 헤드 교차 어텐션 (Multi-Head Cross-Attention):</li>
</ol>
<p>DiT 블록 내부에 표준적인 셀프 어텐션 레이어 외에 ‘교차 어텐션(cross-attention)’ 레이어가 추가된다. 셀프 어텐션이 생성 중인 이미지의 패치들 사이의 관계를 모델링하는 반면, 교차 어텐션은 생성 중인 이미지의 패치들과 조건 신호(여기서는 CLIP 임베딩) 사이의 관계를 모델링한다.</p>
<p>구체적으로, 노이즈가 낀 잠재 표현의 각 패치 임베딩이 ‘쿼리(Query)’ 역할을 하고, 소스 이미지로부터 추출된 CLIP 임베딩이 ’키(Key)’와 ‘밸류(Value)’ 역할을 한다. 이를 통해 생성 중인 이미지의 각 부분은 노이즈 제거의 매 단계에서 소스 이미지의 전체적인 의미론적 콘텐츠에 “질문“을 던지고 관련 정보를 가져올 수 있다. 이 메커니즘은 생성 과정이 지속적으로 소스 이미지의 정체성에 의해 안내되도록 보장한다.</p>
<ol start="2">
<li>적응형 레이어 정규화 (AdaLN-Zero):</li>
</ol>
<p>적응형 레이어 정규화(Adaptive Layer Normalization)는 DiT 블록 내부의 특징 활성화(feature activation)를 조건 신호에 따라 동적으로 조절하는 강력한 기술이다. DiT 블록 내의 레이어 정규화(Layer Normalization) 층은 단순히 특징을 정규화하는 것을 넘어, 조건 신호로부터 예측된 스케일링(<span class="math math-inline">γ</span>) 및 시프팅(<span class="math math-inline">β</span>) 파라미터를 사용하여 변조된다. ADiT에서는 현재 타임스텝 임베딩과 CLIP 이미지 임베딩을 함께 사용하여 이 <span class="math math-inline">γ</span>와 <span class="math math-inline">β</span>를 예측한다. 이는 소스 이미지의 콘텐츠에 기반하여 노이즈 제거의 각 단계에서 생성될 이미지의 전반적인 스타일과 구조에 깊은 영향을 미칠 수 있게 한다. 특히 ‘AdaLN-Zero’ 방식은 초기에는 조건화의 영향을 거의 주지 않다가 학습이 진행됨에 따라 점차 그 영향력을 키워나가도록 설계되어 학습 안정성을 높인다.</p>
<p>이러한 CLIP 기반 이미지 조건화 방식은 생성 모델의 제어 패러다임에 중요한 전환을 의미한다. 이는 ‘서술적(descriptive)’ 안내 방식(예: 텍스트 프롬프트)에서 ‘유추적(analogical)’ 안내 방식으로의 전환이다. “이러한 설명을 만족하는 이미지를 생성하라“는 지시에서 “이 이미지와 다른 도메인에서 유추 관계에 있는 이미지를 생성하라“는 지시로 바뀐 것이다. 텍스트 프롬프트는 보편적이고 여러 결과물에 적용될 수 있는 ’설명’이지만, 특정 사진에서 추출한 이미지 임베딩은 단일한 정체성을 가리키는 고유하고 고차원적인 ’지시자(pointer)’이다.</p>
<p>교차 어텐션 메커니즘 8은 모델이 이 유추적 지시자를 지속적으로 참조하게 함으로써, 스타일이 변환되는 과정에서도 소스의 정체성을 정의하는 핵심 특징들이 보존되도록 보장한다. 이것이 ADiT가 <code>face2comics</code>와 같은 정체성 보존이 필수적인 작업에서 특히 효과적인 이유이다.4 모델은 단순히 <em>어떤</em> 만화 얼굴을 만드는 것이 아니라, <em>바로 그 특정 인물</em>의 만화 얼굴을 만들어낸다.</p>
<p>이러한 이미지 기반 조건화 방법의 성공은 ‘예제 기반(example-based)’ 생성 모델이라는 새로운 종류의 모델에 대한 가능성을 열어준다. 언어에만 의존하는 대신, 미래의 사용자 인터페이스와 모델들은 하나 이상의 ’시각적 예제’를 제공하여 생성을 안내하는 방식으로 구축될 수 있다. 이는 스타일 전송, 캐릭터 디자인, 제품 변형과 같이 텍스트 설명보다 시각적 참조가 더 강력한 힘을 발휘하는 창의적인 작업에서 훨씬 더 직관적이고 강력한 도구로 이어질 수 있다. ADiT는 이러한 ’시각적 프롬프팅(visual-prompting)’이라는 새로운 방향을 개척한 선구적인 연구라 할 수 있다.</p>
<h2>4. 실증적 검증 및 성능 벤치마크</h2>
<p>모든 새로운 모델의 가치는 이론적 우아함뿐만 아니라 실증적 성능을 통해 입증되어야 한다. 이 섹션에서는 ADiT 모델이 주요 벤치마크 데이터셋에서 어떤 성능을 보였는지, 특히 기존의 강력한 기준 모델들과 비교하여 어떤 강점과 약점을 드러냈는지 비판적으로 평가한다. 이 분석은 모델의 실제적 유용성과 한계를 명확히 이해하는 데 필수적이다.</p>
<h3>4.1  벤치마크 데이터셋</h3>
<p>ADiT의 성능 검증을 위해 두 개의 표준적인 쌍을 이룬(paired) 이미지-투-이미지 변환 데이터셋이 사용되었다.12</p>
<ul>
<li><strong><code>face2comics</code>:</strong> 실제 사람의 얼굴 사진을 만화 스타일의 일러스트레이션으로 변환하는 작업을 위한 데이터셋이다.17 이 데이터셋은 10,000개의 쌍으로 구성되어 있으며, 각 쌍은 원본 얼굴 사진과 그에 상응하는 만화 스타일 이미지로 이루어져 있다. 이 작업은 모델이 스타일을 성공적으로 변환하는 동시에, 원본 인물의 정체성(identity)과 같은 핵심적인 의미론적 정보를 얼마나 잘 보존하는지를 평가하는 데 이상적이다.</li>
<li><strong><code>edges2shoes</code>:</strong> 스케치와 같은 윤곽선 맵(edge map)을 사실적인 신발 이미지로 변환하는 고전적인 데이터셋이다. 이 작업은 모델이 희소한 구조적 정보(윤곽선)로부터 풍부한 질감(texture), 색상, 음영 등 세부적인 디테일을 합성해내는 능력을 시험한다. 이는 구조적 일관성과 사실적 텍스처 생성 능력을 평가하는 데 중점을 둔다.</li>
</ul>
<h3>4.2  기준 모델과의 질적 및 양적 비교</h3>
<p>ADiT는 이미지-투-이미지 변환 분야에서 널리 알려진 GAN 기반의 기준 모델인 Pix2Pix 및 그 고해상도 버전인 Pix2PixHD와 비교되었다.4</p>
<p>논문에 제시된 시각적 비교 결과에 따르면, ADiT는 이들 GAN 기반 모델보다 “눈에 띄게 더 높은 품질, 더 선명한 디테일, 그리고 더 적은 아티팩트(artifact, 인공적인 결함)“를 가진 이미지를 생성했다.4 특히 Pix2Pix가 종종 흐릿한 영역이나 불일치하는 부분을 생성하는 경향이 있는 반면, ADiT의 결과물은 전반적으로 더 선명하고 일관성이 있었다.</p>
<p><code>face2comics</code> 데이터셋에서는 머리 스타일, 안경, 얼굴의 미세한 특징들을 더 잘 보존했으며, <code>edges2shoes</code> 데이터셋에서는 신발의 질감과 하이라이트를 더 자연스럽고 사실적으로 생성했다.4</p>
<p>흥미로운 점은 모델의 성능이 데이터셋에 따라 다소 다르게 나타났다는 것이다. ADiT는 <code>face2comics</code>보다 <code>edges2shoes</code> 데이터셋에서 더 나은 결과를 보였는데, 이는 명확한 구조적 가이드(윤곽선)로부터 텍스처를 합성하는 작업에 특히 강점을 보인다는 것을 시사한다.4</p>
<h3>4.3  아킬레스건: 소규모 데이터의 저주</h3>
<p>ADiT 모델의 성능을 평가하는 데 있어 가장 중요하고 통찰력 있는 발견은 소규모 데이터셋에서의 성능이다. 연구진은 모델의 강건성(robustness)을 평가하기 위해 단 400개의 훈련 이미지만을 포함하는 CMP Facade Database에서 추가 실험을 진행했다.4</p>
<p>이 실험에서 결과는 극적으로 반전되었다. 소규모 데이터셋에서는 GAN 기반의 <strong>Pix2Pix가 ADiT 모델보다 더 우수한 성능을 보였다</strong>.4 이 발견은 ADiT와 같은 트랜스포머 기반 모델의 근본적인 특성과 한계를 명확하게 드러낸다. 논문의 저자들은 이 결과를 바탕으로 “확산 트랜스포머는 더 큰 훈련 데이터셋으로부터 상당한 이점을 얻는다“고 결론지었으며, 이는 트랜스포머 아키텍처가 일반적으로 데이터에 비례하여 성능이 향상되는 ’확장 법칙(scaling properties)’을 따른다는 기존의 관찰과도 일치한다.4</p>
<p>이러한 성능의 이중성은 아키텍처 선택이 보편적으로 우월한 것이 아니라, 주어진 문제의 맥락, 특히 사용 가능한 데이터의 규모에 따라 달라진다는 중요한 사실을 보여준다. CNN 기반 아키텍처(Pix2Pix의 U-Net과 같은)는 ’귀납적 편향(inductive bias)’을 가지고 있다. 즉, 이미지는 지역성(locality)과 이동 불변성(translation equivariance)과 같은 특정 속성을 가진다는 사전 가정을 내장하고 있다. 이 편향은 모델이 더 적은 데이터로부터 합리적인 패턴을 학습할 수 있도록 ’지름길’을 제공하여, 데이터 효율성을 높인다.</p>
<p>반면, 트랜스포머는 이러한 내장된 편향이 거의 없다. 이는 더 큰 유연성을 제공하지만, 이미지의 근본적인 속성을 처음부터 학습하기 위해 방대한 양의 데이터를 필요로 한다. 즉, 트랜스포머는 ‘데이터에 굶주려(data-hungry)’ 있다. 하지만 일단 충분한 데이터를 통해 학습이 이루어지면, 전역적 관계를 모델링하는 탁월한 능력 덕분에 CNN 기반 모델보다 더 높은 성능의 상한선에 도달할 수 있다. 따라서 이미지 변환을 위해 트랜스포머 기반 생성기와 CNN 기반 생성기 중 하나를 선택하는 것은 단순히 최고 성능의 문제가 아니라, 사용 가능한 훈련 데이터의 규모에 기반한 실용적인 결정이 된다.</p>
<p>이러한 발견은 AI 기술의 ’민주화’라는 더 넓은 맥락에서도 중요한 함의를 가진다. ADiT와 같은 트랜스포머 기반 모델이 새로운 SOTA(State-of-the-Art) 벤치마크를 설정할 수는 있지만, 대규모 데이터셋에 대한 강한 의존성은 최첨단 AI 기술 개발을 그러한 데이터를 수집하고 모델을 훈련할 자원을 가진 소수의 거대 기업이나 연구 기관에 집중시킬 수 있다. 반면, Pix2Pix와 같이 데이터 효율성이 더 높은 아키텍처는 제한된 데이터를 가진 소규모 연구실, 스타트업, 또는 특정 틈새 응용 분야에서 여전히 더 실용적이고 접근 가능한 선택지로 남을 것이다.</p>
<hr />
<p><strong>표 1: ADiT와 GAN 기반 모델의 성능 비교</strong></p>
<table><thead><tr><th>모델</th><th>데이터셋</th><th>질적 성능 (선명도, 아티팩트, 디테일 보존)</th><th>핵심 발견</th></tr></thead><tbody>
<tr><td><strong>ADiT</strong></td><td><code>edges2shoes</code> (대규모)</td><td>Pix2Pix/Pix2PixHD 대비 높은 선명도, 적은 아티팩트, 우수한 질감 및 하이라이트 보존 4</td><td>대규모 데이터셋에서 GAN 기반 모델을 능가하는 고품질 텍스처 합성 능력 입증</td></tr>
<tr><td><strong>ADiT</strong></td><td><code>face2comics</code> (대규모)</td><td>Pix2Pix/Pix2PixHD 대비 더 나은 디테일(머리 스타일, 안경 등) 보존, 더 자연스러운 결과물 4</td><td>정체성 보존이 중요한 의미론적 변환 작업에서 우수한 성능 발휘</td></tr>
<tr><td><strong>ADiT</strong></td><td><code>CMP Facade</code> (소규모)</td><td>Pix2Pix 대비 낮은 성능, 흐릿하거나 불일치하는 결과 생성 가능성 4</td><td>소규모 데이터셋에서는 GAN 기반 모델에 비해 성능이 저하됨</td></tr>
<tr><td><strong>Pix2Pix / Pix2PixHD</strong></td><td><code>edges2shoes</code> / <code>face2comics</code></td><td>ADiT 대비 상대적으로 흐릿하고 아티팩트가 발생할 수 있음 4</td><td>대규모 데이터셋에서는 ADiT에 비해 시각적 품질이 떨어짐</td></tr>
<tr><td><strong>Pix2Pix / Pix2PixHD</strong></td><td><code>CMP Facade</code> (소규모)</td><td>ADiT를 능가하는 성능 4</td><td>CNN의 귀납적 편향 덕분에 데이터 효율성이 높아 소규모 데이터셋에서 강점을 보임</td></tr>
</tbody></table>
<hr />
<h2>5. 생성형 변환 모델의 비교 분류 체계</h2>
<p>ADiT 모델의 독창성과 위치를 명확히 파악하기 위해서는, 이 모델을 생성형 AI라는 더 넓은 생태계 안에 놓고 다른 주요 모델들과 비교 분석하는 것이 필수적이다. 본 섹션에서는 ADiT를 GAN, CycleGAN, Stable Diffusion, ControlNet과 같은 대표적인 모델들과 다각적으로 비교하여, 각 모델의 제어 방식과 아키텍처 철학에 기반한 ’비교 분류 체계(comparative taxonomy)’를 제시하고자 한다. 이를 통해 ADiT가 생성형 변환 모델의 진화 과정에서 어떤 독자적인 위치를 차지하는지 이해할 수 있다.</p>
<h3>5.1  ADiT 대 GANs (Pix2Pix)</h3>
<ul>
<li>핵심 충돌: 노이즈 제거 대 적대적 게임</li>
</ul>
<p>ADiT와 Pix2Pix의 가장 근본적인 차이는 생성 과정의 철학에 있다. ADiT는 확산 모델로서, 노이즈가 가득한 상태에서 점진적으로 노이즈를 제거해 나가는 반복적인 정제 과정을 통해 이미지를 생성한다.4 이 과정은 안정적이고 예측 가능하다. 반면, Pix2Pix와 같은 GAN 모델은 생성자가 단 한 번의 시도(single-shot)로 이미지를 생성하고, 판별자가 그 결과를 평가하여 피드백을 주는 적대적 게임을 통해 학습한다.1</p>
<ul>
<li>트레이드오프: 안정성 대 추론 속도</li>
</ul>
<p>이러한 철학의 차이는 명확한 트레이드오프로 이어진다. ADiT는 학습 안정성과 높은 시각적 충실도를 얻는 대신, 수십 번의 노이즈 제거 단계를 거쳐야 하므로 추론 속도가 느리다는 단점이 있다.3 또한 트랜스포머 백본으로 인해 훈련 비용도 높다. 반면, GAN은 일단 성공적으로 학습되기만 하면 단 한 번의 네트워크 통과로 이미지를 생성할 수 있어 추론 속도가 매우 빠르지만, 학습 과정 자체가 불안정하여 성공을 보장하기 어렵다.2</p>
<h3>5.2  ADiT 대 CycleGAN</h3>
<ul>
<li>핵심 충돌: 쌍을 이룬 데이터 대 쌍을 이루지 않은 데이터</li>
</ul>
<p>ADiT와 CycleGAN의 결정적인 차이는 학습에 필요한 데이터의 종류에 있다. ADiT는 ‘쌍을 이룬(paired)’ 데이터셋을 요구한다. 즉, 모든 소스 이미지 A에 대해 그에 상응하는 정답 타겟 이미지 B가 존재해야 한다.4 이는 윤곽선-사진, 흑백-컬러 사진처럼 쌍을 만들기 용이한 작업에 적합하다.</p>
<p>반면, CycleGAN의 핵심 혁신은 ‘쌍을 이루지 않은(unpaired)’ 데이터셋을 사용할 수 있다는 점이다.18 CycleGAN은 ’순환 일관성 손실(cycle-consistency loss)’이라는 개념을 도입하여, 도메인 A에서 B로 변환한 이미지를 다시 A로 변환했을 때 원래 이미지와 유사해야 한다는 제약을 통해 학습한다.20</p>
<ul>
<li>함의: 적용 범위의 차이</li>
</ul>
<p>이 차이는 두 모델의 적용 범위를 극명하게 가른다. ADiT는 높은 정밀도가 요구되고 쌍을 이룬 데이터 구축이 가능한 전문적인 작업에 더 적합하다. 반면, CycleGAN은 말-얼룩말, 모네 그림-사진과 같이 현실적으로 쌍을 이룬 데이터를 구하는 것이 불가능한 광범위한 창의적 변환 작업을 가능하게 했다.19</p>
<h3>5.3  ADiT 대 Stable Diffusion (U-Net 기반)</h3>
<ul>
<li>핵심 충돌: 트랜스포머 대 U-Net 백본</li>
</ul>
<p>이는 확산 모델이라는 동일한 프레임워크 내에서의 순수한 아키텍처 비교이다. ADiT의 DiT 백본은 셀프 어텐션을 통해 이미지의 전역적 맥락을 포착하는 데 탁월한 능력을 보인다.11 이는 원본의 전체적인 구조를 보존하는 데 유리하다.</p>
<p>반면, Stable Diffusion의 U-Net 백본은 고도로 최적화된 CNN 아키텍처이다. 인코더-디코더 구조와 ’스킵 커넥션(skip connection)’을 통해 저수준의 지역적 특징과 고수준의 의미론적 특징을 효과적으로 융합한다.7 U-Net은 수많은 비전 과제에서 그 효율성과 성능이 입증되어 확산 모델의 사실상 산업 표준(de facto standard)으로 자리 잡았다.</p>
<ul>
<li>함의: 이론적 순수성 대 실용적 표준</li>
</ul>
<p>ADiT의 설계는 트랜스포머 패러다임을 이미지 생성에 적용한 ‘이론적으로 순수한’ 접근 방식에 가깝다고 볼 수 있다. 반면, Stable Diffusion의 U-Net은 오랜 기간 검증되고 계산적으로 균형 잡힌 ‘실용적이고 강력한’ 아키텍처이다. ADiT가 트랜스포머의 잠재력을 입증했다면, Stable Diffusion은 U-Net의 견고함을 바탕으로 대중화를 이끌었다.</p>
<h3>5.4  ADiT 대 ControlNet</h3>
<ul>
<li>핵심 충돌: 의미론적 제어 대 공간적/기하학적 제어</li>
</ul>
<p>이 비교는 모델이 어떤 ’종류’의 제어를 수행하는지에 대한 가장 통찰력 있는 분석을 제공한다. ADiT는 CLIP 이미지 임베딩을 사용하여 고수준의 ‘의미론적(semantic)’ 안내를 제공한다. 즉, “이 얼굴의 정체성을 보존하라“와 같은 추상적인 지시를 수행한다.8</p>
<p>반면, ControlNet은 표준 확산 모델(주로 U-Net 기반)에 병렬적인 인코더를 추가하여, 캐니 엣지(Canny edges), 깊이 맵(depth maps), 인간 포즈 골격과 같은 입력으로부터 명시적인 ‘공간적/기하학적(spatial/geometric)’ 안내를 제공한다.21 즉, “이 포즈를 정확히 따르라” 또는 “이 윤곽선을 유지하라“와 같은 구체적인 지시를 수행한다.</p>
<ul>
<li>함의: 경쟁자가 아닌 보완재</li>
</ul>
<p>ADiT와 ControlNet은 제어 문제의 서로 다른 측면을 해결하기 때문에 경쟁 관계가 아니라 상호 보완적인 관계에 있다. ADiT가 ‘무엇을’ 그릴지에 대한 의미를 제어한다면, ControlNet은 ‘어떻게’ 그릴지에 대한 구조를 제어한다. 이는 미래 모델의 발전 방향을 시사한다. 즉, ControlNet 방식의 입력으로 정확한 포즈와 레이아웃을 정의하고, 동시에 ADiT 방식의 CLIP 임베딩으로 피사체의 의미론적 정체성과 스타일을 정의하는 하이브리드 모델의 등장을 예견할 수 있다.</p>
<hr />
<p><strong>표 2: 생성형 이미지 변환 모델의 제어 방식에 따른 분류 체계</strong></p>
<table><thead><tr><th>모델</th><th>백본 아키텍처</th><th>데이터 요구사항</th><th>조건화 유형</th><th>주요 사용 사례</th><th>핵심 강점</th><th>핵심 약점</th></tr></thead><tbody>
<tr><td><strong>ADiT</strong></td><td>트랜스포머 (DiT)</td><td>쌍을 이룸 (Paired)</td><td>의미론적 이미지 임베딩 (CLIP) 8</td><td>정체성 보존이 중요한 고품질 변환 (예: <code>face2comics</code>)</td><td>높은 시각적 충실도, 전역적 일관성, 안정적인 학습</td><td>느린 추론 속도, 대규모 데이터 의존성 4</td></tr>
<tr><td><strong>CycleGAN</strong></td><td>CNN (ResNet 기반)</td><td>쌍을 이루지 않음 (Unpaired)</td><td>암시적 (순환 일관성 손실) 20</td><td>도메인 적응, 예술적 스타일 전송 (예: 말↔얼룩말)</td><td>데이터 유연성 (쌍 필요 없음), 광범위한 적용 가능성</td><td>생성 품질의 불안정성, 아티팩트 발생 가능성</td></tr>
<tr><td><strong>Stable Diffusion</strong></td><td>CNN (U-Net)</td><td>쌍을 이루지 않음 (Unpaired)</td><td>텍스트 임베딩 (CLIP) 9</td><td>텍스트-투-이미지 생성</td><td>빠른 생태계 확장, 높은 접근성, 계산 효율성 (LDM)</td><td>텍스트만으로는 세밀한 구조 제어의 한계</td></tr>
<tr><td><strong>ControlNet</strong></td><td>CNN (U-Net) + 제어 인코더</td><td>쌍을 이룸 (Paired)</td><td>공간적/기하학적 맵 (포즈, 엣지, 깊이 등) 21</td><td>포즈/레이아웃 제어가 필요한 정밀 생성</td><td>이미지 구조에 대한 강력하고 명시적인 제어 능력</td><td>제어 유형이 사전 정의된 맵에 한정됨</td></tr>
</tbody></table>
<hr />
<h2>6. DiT의 유산: 한계, 진화, 그리고 미래의 지평</h2>
<p>ADiT 모델은 확산 모델과 트랜스포머의 결합이 이미지 변환 분야에서 가질 수 있는 강력한 잠재력을 입증한 이정표적인 연구이다. 그러나 모든 선구적인 기술과 마찬가지로, ADiT와 그 기반이 되는 1세대 DiT 아키텍처는 명확한 한계를 가지고 있었다. 이 섹션에서는 이러한 내재적 한계를 분석하고, 후속 연구들이 이 문제들을 어떻게 해결하며 진화해왔는지 추적하며, 트랜스포머 기반 생성 모델의 미래 발전 방향을 조망하고자 한다.</p>
<h3>6.1  1세대 DiT 아키텍처의 내재적 한계</h3>
<p>ADiT가 보여준 성공에도 불구하고, 그 기반이 되는 DiT 아키텍처는 몇 가지 근본적인 도전에 직면했다.</p>
<ul>
<li><strong>계산 비용 문제:</strong> 트랜스포머의 셀프 어텐션 메커니즘은 입력 시퀀스 길이에 대해 제곱에 비례하는(O(N2)) 계산 복잡도를 가진다. 이는 이미지 패치의 수가 증가할수록, 즉 고해상도 이미지를 처리할수록 계산 비용과 메모리 요구량이 기하급수적으로 증가함을 의미한다.16 이로 인해 1세대 DiT는 훈련과 추론이 매우 느리고 자원 집약적이었다.</li>
<li><strong>고정된 공간 압축의 비효율성:</strong> 표준 DiT 아키텍처는 이미지 전체에 걸쳐 고정된 비율로 공간 압축(패치화)을 적용한다. 그러나 이미지의 정보 밀도는 영역마다 다르다. 예를 들어, 푸른 하늘과 같이 단순한 배경 영역은 적은 수의 토큰으로도 충분히 표현될 수 있지만, 사람의 얼굴이나 복잡한 질감과 같이 세부 정보가 많은 영역은 더 많은 토큰이 필요하다. 고정된 압축 방식은 복잡한 영역의 정보를 과도하게 압축하여 지역적 디테일을 손상시키거나(높은 압축률), 단순한 영역을 불필요하게 많은 토큰으로 표현하여 전역적 일관성 모델링을 어렵게 하고 계산 비효율을 초래하는(낮은 압축률) 문제를 야기한다.23</li>
<li><strong>데이터 의존성:</strong> 4장에서 논의된 바와 같이, DiT는 CNN과 같은 강력한 귀납적 편향이 없기 때문에, 그 잠재력을 최대한 발휘하기 위해서는 방대한 양의 훈련 데이터를 필요로 한다.4 이는 DiT 기반 모델의 개발과 활용을 대규모 데이터셋에 접근 가능한 소수의 기관으로 제한하는 요인이 된다.</li>
</ul>
<h3>6.2  차세대 물결: 확산 트랜스포머의 진화</h3>
<p>ADiT의 성공은 DiT 아키텍처의 가능성을 입증함과 동시에 그 한계를 명확히 드러냈다. 이러한 한계들은 자연스럽게 후속 연구들의 핵심적인 해결 과제가 되었다.</p>
<ul>
<li><strong>효율성 개선 (EDiT):</strong> 연구자들은 DiT의 제곱 복잡도 병목 현상을 해결하기 위해 더 효율적인 어텐션 메커니즘을 개발하거나 아키텍처를 수정하는 연구에 집중했다. ’효율적인 확산 트랜스포머(Efficient Diffusion Transformer, EDiT)’와 같은 연구들은 기존 DiT의 성능을 유지하면서도 계산 비용을 줄이는 것을 목표로 한다.16</li>
<li><strong>이미지 편집으로의 확장 (DiT4Edit):</strong> DiT의 원리는 이미지 생성 및 변환을 넘어, 더 복잡한 과제인 ’이미지 편집’으로 확장되었다. ’DiT4Edit’는 DiT 기반의 최초 이미지 편집 프레임워크로 제안되었다.10 이 모델은 트랜스포머가 장거리 의존성을 다루는 데 능숙하다는 점을 활용하여, 이미지의 큰 부분을 변경하는 대규모 변형 편집에서 U-Net 기반 방법들을 능가하는 성능을 보였다.10 또한, DDIM보다 더 적은 스텝으로 고품질의 역변환을 수행하는 DPM-Solver와 같은 새로운 기술을 도입하여 추론 속도를 개선했다.15</li>
<li><strong>프로그램적 편집 (IEAP):</strong> 더 나아가, ’프로그램으로서의 이미지 편집(Image Editing As Programs, IEAP)’과 같은 프레임워크는 DiT 백본 위에 구축되지만, 복잡한 편집 지시를 ’원자적 연산(atomic operations)’의 시퀀스로 분해한다.24 비전-언어 모델(VLM) 에이전트가 이 프로그램을 작성하고, 각 연산은 특정 유형의 편집에 특화된 경량 어댑터를 통해 수행된다. 이는 기존 모델들이 어려움을 겪었던 구조적으로 불일치하는 복잡한 변형을 가능하게 한다.25</li>
</ul>
<p>이러한 진화 과정은 ADiT가 일종의 ‘길잡이(pathfinder)’ 역할을 했음을 보여준다. ADiT는 트랜스포머가 생성적 변환에 매우 효과적일 수 있음을 증명함으로써 그 접근법의 타당성을 입증했다. 그러나 바로 그 성공이 속도, 비용, 데이터 요구량과 같은 실용적인 병목 현상을 수면 위로 드러냈고, 이는 곧바로 해당 분야의 다음 세대 연구들의 주요 의제가 되었다. 즉, ADiT의 성공이 역설적으로 그 한계를 조명했고, 그 한계를 극복하는 과정에서 DiT4Edit, IEAP와 같은 더 정교하고 실용적인 모델들이 탄생한 것이다.</p>
<h3>6.3  미래 궤적과 미해결 과제</h3>
<p>ADiT로부터 시작된 DiT 기반 생성 모델의 여정은 여전히 많은 가능성과 도전 과제를 남겨두고 있다.</p>
<ul>
<li><strong>쌍을 이루지 않은 변환과 DiT:</strong> 원본 ADiT 논문은 쌍을 이룬 변환에 초점을 맞췄다. DiT 아키텍처의 강력한 생성 능력을 CycleGAN의 순환 일관성 원칙과 결합하여, 쌍을 이루지 않은 데이터셋에서도 고품질의 이미지 변환을 수행하는 강건한 방법을 개발하는 것은 중요한 연구 분야로 남아있다.26</li>
<li><strong>적응형 아키텍처:</strong> 고정된 공간 압축의 한계 23는 이미지 영역의 정보 밀도에 따라 계산 자원을 동적으로 할당하는 미래의 ‘적응형(adaptive)’ DiT 아키텍처의 필요성을 시사한다. 이는 모델이 복잡한 영역에는 더 많은 주의를 기울이고 단순한 영역은 효율적으로 처리하여, 전반적인 생성 품질과 효율성을 동시에 향상시킬 수 있을 것이다.</li>
<li><strong>다중 모드 제어:</strong> ADiT의 의미론적 제어와 ControlNet의 공간적 제어가 상호 보완적이라는 사실은, 미래 모델이 텍스트, 참조 이미지, 기하학적 맵, 심지어는 소리나 다른 감각 데이터까지 포함하는 풍부하고 다중적인 모드의 입력을 동시에 받아들여 생성을 안내하는 방향으로 발전할 것임을 암시한다.</li>
</ul>
<p>이러한 진화 궤적은 해당 분야의 성숙도를 보여준다. 초기의 관심사가 순수한 ‘생성 능력’(좋은 이미지를 만들 수 있는가?)에 있었다면, ADiT에서 DiT4Edit, IEAP로 이어지는 흐름은 ‘제어 가능성과 효율성’(우리가 원하는 <em>정확한</em> 이미지를 빠르고 저렴하게 만들 수 있는가?)으로 초점이 이동하고 있음을 보여준다. 이는 기술이 학문적 발견의 단계를 지나, 추론 속도, 자원 소비, 사용자 의도와의 일치와 같은 실용적인 고려사항이 혁신을 주도하는 엔지니어링 및 제품화 단계로 나아가고 있음을 시사하는 중요한 신호이다.</p>
<h2>7. 결론</h2>
<p>본 안내서는 ‘주의 기반 확산 트랜스포머(ADiT)’ 모델에 대한 심층적인 기술적 고찰을 제공했다. 분석을 통해 ADiT는 단순히 새로운 생성 모델 중 하나가 아니라, 생성형 AI의 발전 과정에서 중요한 변곡점을 나타내는 핵심적인 종합체임이 분명해졌다.</p>
<p>ADiT의 의의는 세 가지 동시대의 가장 강력한 기술, 즉 <strong>확산 모델의 안정성</strong>, <strong>트랜스포머의 전역적 모델링 능력</strong>, 그리고 <strong>CLIP의 의미론적 조건화</strong>를 ’쌍을 이룬 이미지-투-이미지 변환’이라는 특정 과제를 해결하기 위해 최초로 성공적으로 융합했다는 데 있다. 이 설계는 이전 세대 모델인 GAN의 불안정성, U-Net의 지역적 한계, 그리고 텍스트 조건화의 불충분함에 대한 체계적이고 논리적인 해답의 결과물이었다. 특히, 소스 이미지 자체를 CLIP 임베딩을 통해 조건으로 사용하는 방식은 ‘서술적’ 제어에서 ‘유추적’ 제어로의 패러다임 전환을 이끌었으며, 이는 정체성 보존과 같은 미세한 제어를 가능하게 하는 결정적인 혁신이었다.</p>
<p>그러나 ADiT의 성공은 동시에 그 한계를 명확히 드러냈다. 트랜스포머 아키텍처 고유의 높은 계산 비용과 대규모 데이터에 대한 강한 의존성은 실용적인 적용에 있어 중요한 병목 현상임이 실증적 검증을 통해 밝혀졌다. 소규모 데이터셋에서 기존의 CNN 기반 모델에 뒤처지는 성능을 보인 것은, 최첨단 아키텍처의 우수성이 데이터 규모라는 현실적인 맥락에 따라 상대적일 수 있음을 보여주는 중요한 교훈이다.</p>
<p>결론적으로, ADiT의 진정한 유산은 모델 자체의 성능을 넘어, 그것이 후속 연구에 미친 영향에 있다. ADiT는 트랜스포머가 생성적 변환의 새로운 지평을 열 수 있음을 증명한 ’길잡이’였다. 그리고 ADiT가 드러낸 계산 효율성과 데이터 의존성이라는 과제는 DiT4Edit, IEAP와 같은 차세대 모델들의 연구 의제를 정의했다. 이로써 AI 연구의 초점은 순수한 ’생성 능력’에서 정밀한 ’제어 가능성’과 ’효율성’으로 이동하게 되었으며, 이는 기술이 학문적 탐구를 넘어 실용적인 엔지니어링 단계로 성숙하고 있음을 보여주는 명백한 증거이다. 따라서 ADiT는 생성형 AI가 단순한 이미지 생성을 넘어, 인간의 의도를 정밀하게 반영하는 창의적 도구로 진화하는 과정에서 필수적인 디딤돌을 놓은 모델로 평가되어야 할 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>GANs vs. Diffusion Models: In-Depth Comparison and Analysis - Sapien, accessed July 20, 2025, https://www.sapien.io/blog/gans-vs-diffusion-models-a-comparative-analysis</li>
<li>Comparative Analysis of GANs and Diffusion Models in Image Generation - ResearchGate, accessed July 20, 2025, https://www.researchgate.net/publication/387444028_Comparative_Analysis_of_GANs_and_Diffusion_Models_in_Image_Generation</li>
<li>[D] What are the advantages of GANs over Diffusion Models in image generation? - Reddit, accessed July 20, 2025, https://www.reddit.com/r/MachineLearning/comments/184j8c3/d_what_are_the_advantages_of_gans_over_diffusion/</li>
<li>Image-to-Image Translation with Diffusion Transformers and CLIP-Based Image Conditioning - arXiv, accessed July 20, 2025, https://arxiv.org/html/2505.16001v1</li>
<li>www.sapien.io, accessed July 20, 2025, <a href="https://www.sapien.io/blog/gans-vs-diffusion-models-a-comparative-analysis#:~:text=GANs%20tend%20to%20require%20fewer,samples%20more%20quickly%20once%20trained.">https://www.sapien.io/blog/gans-vs-diffusion-models-a-comparative-analysis#:~:text=GANs%20tend%20to%20require%20fewer,samples%20more%20quickly%20once%20trained.</a></li>
<li>Aditi Rao Hydari - Wikipedia, accessed July 20, 2025, https://en.wikipedia.org/wiki/Aditi_Rao_Hydari</li>
<li>An Introduction to Diffusion Models and Stable Diffusion - Marvik - Blog, accessed July 20, 2025, https://blog.marvik.ai/2023/11/28/an-introduction-to-diffusion-models-and-stable-diffusion/</li>
<li>[Literature Review] Image-to-Image Translation with Diffusion Transformers and CLIP-Based Image Conditioning - Moonlight, accessed July 20, 2025, https://www.themoonlight.io/en/review/image-to-image-translation-with-diffusion-transformers-and-clip-based-image-conditioning</li>
<li>Understanding Stable Diffusion Architecture and UNet | by Yash Jain | Medium, accessed July 20, 2025, https://medium.com/@ydhupiya1710/understanding-stable-diffusion-architecture-and-unet-4aad410929c4</li>
<li>[2411.03286] DiT4Edit: Diffusion Transformer for Image Editing - arXiv, accessed July 20, 2025, https://arxiv.org/abs/2411.03286</li>
<li>DiT4Edit: Diffusion Transformer for Image Editing - arXiv, accessed July 20, 2025, https://arxiv.org/html/2411.03286v2</li>
<li>[2505.16001] Image-to-Image Translation with Diffusion Transformers and CLIP-Based Image Conditioning - arXiv, accessed July 20, 2025, https://arxiv.org/abs/2505.16001</li>
<li>Paper page - Image-to-Image Translation with Diffusion Transformers and CLIP-Based Image Conditioning - Hugging Face, accessed July 20, 2025, https://huggingface.co/papers/2505.16001</li>
<li>CLIP: Connecting text and images - OpenAI, accessed July 20, 2025, https://openai.com/index/clip/</li>
<li>DiT4Edit: Diffusion Transformer for Image Editing - hkust (gz), accessed July 20, 2025, https://cislab.hkust-gz.edu.cn/media/documents/_AAAI_2025__DiT4Edit_Camera_Ready.pdf</li>
<li>[2503.16726] EDiT: Efficient Diffusion Transformers with Linear Compressed Attention, accessed July 20, 2025, https://arxiv.org/abs/2503.16726</li>
<li>Sxela/face2comics: face2comics datasets - GitHub, accessed July 20, 2025, https://github.com/Sxela/face2comics</li>
<li>What is the difference between CycleGAN and DiscoGAN? They both seem to be the same thing. - Quora, accessed July 20, 2025, https://www.quora.com/What-is-the-difference-between-CycleGAN-and-DiscoGAN-They-both-seem-to-be-the-same-thing</li>
<li>How to Get Started with Image-to-Image Translation with Cyclegan - Analytics Vidhya, accessed July 20, 2025, https://www.analyticsvidhya.com/blog/2023/08/image-to-image-translation-with-cyclegan/</li>
<li>Generative Adversarial Networks variants: DCGAN, Pix2pix, CycleGAN - Harshit Kumar, accessed July 20, 2025, https://kharshit.github.io/blog/2019/04/05/generative-adversarial-networks-variants-dcgan-pix2pix-cyclegan</li>
<li>A Complete Guide on Stable Diffusion ControlNet - Aiarty, accessed July 20, 2025, https://www.aiarty.com/stable-diffusion-guide/stable-diffusion-controlnet.htm</li>
<li>Conditioning image generation 🖌️: implementation with Stable Diffusion, ControlNet and IP-Adapter | by Isabella | Medium, accessed July 20, 2025, <a href="https://medium.com/@isa.dario.isa/conditioning-image-generation-%EF%B8%8F-implementation-with-stable-diffusion-controlnet-and-ipadapter-b502bfe9315d">https://medium.com/@isa.dario.isa/conditioning-image-generation-%EF%B8%8F-implementation-with-stable-diffusion-controlnet-and-ipadapter-b502bfe9315d</a></li>
<li>D2iT: Dynamic Diffusion Transformer for Accurate Image Generation - arXiv, accessed July 20, 2025, https://arxiv.org/html/2504.09454v1</li>
<li>[2506.04158] Image Editing As Programs with Diffusion Models - arXiv, accessed July 20, 2025, https://arxiv.org/abs/2506.04158</li>
<li>Image Editing As Programs with Diffusion Models - arXiv, accessed July 20, 2025, https://arxiv.org/html/2506.04158v1</li>
<li>Unpaired Image-to-Image Translation with Diffusion Adversarial Network - MDPI, accessed July 20, 2025, https://www.mdpi.com/2227-7390/12/20/3178</li>
<li>(PDF) Unpaired image-to-image translation with diffusion adversarial network, accessed July 20, 2025, https://www.researchgate.net/publication/381411130_Unpaired_image-to-image_translation_with_diffusion_adversarial_network</li>
<li>BBDM: Image-to-Image Translation With Brownian Bridge Diffusion Models - CVF Open Access, accessed July 20, 2025, https://openaccess.thecvf.com/content/CVPR2023/papers/Li_BBDM_Image-to-Image_Translation_With_Brownian_Bridge_Diffusion_Models_CVPR_2023_paper.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>