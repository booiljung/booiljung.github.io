<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Google DeepMind의 D4RT: 단일 피드포워드 아키텍처 기반 동적 4D 장면 재구성 및 추적 기술</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Google DeepMind의 D4RT: 단일 피드포워드 아키텍처 기반 동적 4D 장면 재구성 및 추적 기술</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">피지컬 AI (Physical AI)</a> / <span>Google DeepMind의 D4RT: 단일 피드포워드 아키텍처 기반 동적 4D 장면 재구성 및 추적 기술</span></nav>
                </div>
            </header>
            <article>
                <h1>Google DeepMind의 D4RT: 단일 피드포워드 아키텍처 기반 동적 4D 장면 재구성 및 추적 기술</h1>
<p>2026-02-18, G30DR</p>
<h2>1.  서론: 4D 컴퓨터 비전의 역문제와 인지 병목 현상의 극복</h2>
<p>인류가 세상을 인식하는 방식은 본질적으로 3차원적이며, 시간의 흐름에 따라 객체와 환경이 어떻게 이동하고 상호작용하는지를 직관적이고 연속적으로 이해하는 4차원적 시공간 인지 능력에 기반을 두고 있다. 그러나 인공지능(AI) 및 컴퓨터 비전 시스템에게 이러한 수준의 동적 시공간 인식 능력은 오랫동안 극복하기 매우 까다로운 컴퓨팅 병목 현상으로 작용해 왔다. 로봇이나 자율 주행 차량, 증강 현실(AR) 디바이스에 카메라를 장착하여 시각적 입력값을 제공하는 것은 문제의 절반을 해결하는 것에 불과하다. 진정한 시각적 지능을 기계에 구현하기 위해서는 평면적인 2차원(2D) 투영의 연속체인 비디오 데이터로부터 본래의 풍부하고 입체적인 3차원(3D) 볼륨 세계를 시간의 흐름(motion) 속에서 온전히 복원해 내는 고도의 ’역문제(inverse problem)’를 해결해야만 한다.</p>
<p>2026년 1월, Google DeepMind의 연구진은 이러한 4D 장면 재구성의 난제를 근본적으로 해결하기 위해 D4RT(Dynamic 4D Reconstruction and Tracking)라는 혁신적인 AI 비전 모델을 발표했다. D4RT는 단일 비디오 입력으로부터 깊이(depth), 시공간적 서신(spatio-temporal correspondence), 그리고 카메라 파라미터(camera parameters)를 동시에 추론하는 통합된 피드포워드(feedforward) 트랜스포머 아키텍처이다. 이 모델은 기존 기술들이 동적 환경을 이해하기 위해 수많은 전문화된 모듈을 기형적으로 덧붙여 사용했던 파편화된 접근 방식을 탈피하고, 모든 4D 재구성 프로세스를 단일 프레임워크 내로 우아하게 통합했다.</p>
<p>그 결과, D4RT는 기존의 최첨단(State-of-the-Art) 방법론 대비 최소 18배에서 최대 300배에 달하는 경이로운 연산 속도 향상을 이루어냈으며, 동시에 추적 및 재구성의 정확도 측면에서도 새로운 글로벌 표준을 확립했다. 본 보고서는 D4RT 모델이 채택한 핵심 아키텍처의 혁신성, 데이터 처리 메커니즘, 벤치마크 상의 정량적 성능 지표, 그리고 이 기술이 차세대 로보틱스와 일반인공지능(AGI)을 위한 ‘세계 모델(World Models)’ 구축에 미치는 2차, 3차적 파급 효과를 심층적으로 분석한다.</p>
<h2>2.  기존 3D/4D 재구성 기술의 한계와 패러다임의 전환</h2>
<p>과거 컴퓨터 비전 분야에서 3D 재구성 기술이 던진 근본적인 질문은 “모든 곳에 있는 모든 것의 기하학적 구조를 한 번에 파악할 수 있는가?“라는 매우 포괄적이고 정적인 명제였다. 그러나 이러한 전방위적이고 경직된 접근 방식은 끊임없이 변화하고 객체가 이동하는 동적인(dynamic) 현실 세계를 모델링하는 데에는 본질적인 한계를 드러냈다. 동적 장면을 처리하기 위한 명확한 통합 프레임워크가 부재했던 과거에는, 이 거대한 역문제를 개별적이고 지엽적인 태스크로 분할하여 접근하는 것이 일반적이었다.</p>
<p>예를 들어, 널리 사용되던 MegaSaM과 같은 파이프라인은 단안 깊이(mono-depth) 추정, 메트릭 깊이(metric depth) 추론, 그리고 모션 분할(motion segmentation) 등 각기 다른 목적을 위해 사전 훈련된 기성(off-the-shelf) 모델들을 모자이크처럼 기형적으로 이어 붙여 사용했다. 이러한 이질적인 신호들을 하나의 장면으로 융합하기 위해서는 기하학적 일관성을 강제로 맞추기 위한 막대한 비용의 테스트 타임 최적화(test-time optimization) 과정이 필수적으로 동반되어야만 했다. 또한 VGGT와 같은 초기 피드포워드 모델들조차 특정 프레임에 종속된 촘촘한(dense) 디코딩을 수행하거나, 각각의 태스크를 처리하기 위해 여러 개의 특화된 디코더를 병렬로 관리해야 하는 복잡한 구조적 약점을 지니고 있었다.</p>
<p>프레임 단위의 밀집 디코딩은 시공간적 연속성을 확보하기 위해 프레임 간의 픽셀 매칭 및 정렬 알고리즘을 추가로 요구하며, 이는 필연적으로 극심한 연산 지연(latency)을 유발한다. 결과적으로 기존 AI의 재구성 방식은 극도로 느리고 파편화되어 있었으며, 엣지 디바이스나 실시간 처리가 생명인 자율형 로보틱스 및 AR 환경에서의 즉각적인 배포를 불가능하게 만들었다. D4RT는 이러한 전통적인 ’무차별 대입식 프레임 렌더링’의 연산적 무거움과 다중 디코더 아키텍처의 복잡성을 완전히 우회(sidestep)하는 근본적인 패러다임 전환을 시도했다.</p>
<h2>3.  D4RT의 통합 트랜스포머 아키텍처 및 핵심 메커니즘</h2>
<p>D4RT의 설계 철학은 객체의 자체적인 움직임(object motion), 관찰자인 카메라의 움직임(camera motion), 그리고 정적인 배경의 기하학(static geometry)을 완벽하게 분리(disentangle)하여 추론하는 데 있다. 이를 달성하기 위해 연구진은 Scene Representation Transformer (SRT)의 개념에서 영감을 받아, 고도로 단순화된 인코더-디코더 기반의 통합 트랜스포머 아키텍처를 구축했다.</p>
<h3>3.1  전역 셀프-어텐션 인코더 (Global Self-Attention Encoder)</h3>
<p>D4RT 파이프라인의 첫 번째 단계는 입력된 비디오 시퀀스를 압축적이고 다차원적인 잠재 공간(latent space)으로 변환하는 것이다. 비디오 입력값은 비전 트랜스포머(Vision Transformer) 기반의 글로벌 인코더를 거치게 되며, 이 인코더 내부에는 로컬 프레임 단위의 어텐션 층(local frame-wise attention layers)과 전역 셀프-어텐션 층(global self-attention layers)이 교차로 촘촘하게 배치되어 있다.</p>
<p>이러한 교차 어텐션 구조는 두 가지 핵심적인 시각적 통찰을 모델에 부여한다. 첫째, 로컬 어텐션은 개별 프레임 내의 미세한 질감과 기하학적 특징을 포착한다. 둘째, 전역 어텐션은 전체 비디오 프레임을 가로지르는 조밀한 대응 관계(dense correspondence)를 식별하여 시간의 흐름(flow of time)이 대상 장면과 객체에 미치는 물리적 변형 및 궤적의 변화를 학습한다. 결과적으로 인코더는 단순한 픽셀의 집합을 장면의 기하학과 모션 정보가 응축된 ’전역 장면 표현(Global Scene Representation, <span class="math math-inline">F</span>)’이라는 고차원적 잠재 벡터로 성공적으로 변환해 낸다.</p>
<h3>3.2  독립적 프로빙이 가능한 경량 디코더 (Lightweight Decoder)</h3>
<p>인코더가 형성한 전역 장면 표현 <span class="math math-inline">F</span>는 후속 단계에서 극도로 경량화된 교차-어텐션 트랜스포머 디코더로 전달된다. 기존 시스템들이 깊이 추정용, 카메라 포즈 추정용, 모션 추적용 디코더를 각각 별도로 두었던 것과 달리, D4RT는 단 하나의 단일 디코더만을 사용하여 모든 태스크를 병행 처리한다. 이 디코더의 가장 큰 특징은 순차적이거나 밀집된 렌더링을 강제하지 않고, 모델로 하여금 시공간의 임의의 지점을 독립적이고 유연하게 탐색(probe)할 수 있도록 허용한다는 점이다.</p>
<h3>3.3  혁신적인 시공간 포인트 쿼리 메커니즘 (Novel Spatio-Temporal Querying Mechanism)</h3>
<p>D4RT의 진정한 혁신은 디코더와 상호작용하는 유연한 ‘질의(Query)’ 메커니즘 자체에 존재한다. 전체 3D 볼륨을 맹목적으로 디코딩하는 대신, D4RT의 디코더는 단 하나의 근본적이고 철학적인 질문에만 답하도록 고안되었다. “비디오 내에 존재하는 특정 픽셀이, 사용자가 선택한 특정 카메라 시점에서 보았을 때, 임의의 특정 시간에 3차원 공간의 정확히 어느 좌표에 위치하는가?”.</p>
<p>수학적으로 이 질의 메커니즘은 독립적인 시공간 포인트 쿼리 벡터 <span class="math math-inline">q = (u, v, t_{src}, t_{tgt}, t_{cam})</span>로 정의된다.</p>
<ul>
<li><span class="math math-inline">(u, v)</span>: 사용자가 추적하거나 재구성하고자 하는 소스 프레임에서의 초기 2D 픽셀 좌표.</li>
<li><span class="math math-inline">t_{src}</span>: 해당 픽셀이 처음 관측된 원본 타임스텝 (Source Timestep).</li>
<li><span class="math math-inline">t_{tgt}</span>: 해당 픽셀의 3D 위치를 파악하고자 하는 목표 타임스텝 (Target Timestep).</li>
<li><span class="math math-inline">t_{cam}</span>: 3D 위치를 투영하여 바라볼 기준 카메라 좌표계 (Camera Coordinate).</li>
</ul>
<p>디코더는 이 쿼리 <span class="math math-inline">q</span>를 입력받아 전역 장면 표현 <span class="math math-inline">F</span>에 교차 어텐션을 수행하고, 최종적으로 대상 픽셀의 3차원 위치 <span class="math math-inline">P</span>를 예측해 낸다. 특히 연구진은 모델의 성능을 극대화하기 위한 아키텍처적 강화 기법으로서, 단순한 좌표값뿐만 아니라 대상 포인트 <span class="math math-inline">(u, v)</span>를 중심으로 한 로컬 <span class="math math-inline">9 \times 9</span> 픽셀 크기의 RGB 패치 임베딩을 쿼리에 결합했다. 이 로컬 RGB 패치는 점 하나가 가질 수 없는 주변부의 텍스처와 미세한 구조적 맥락(spatial context)을 쿼리에 주입함으로써, 모델이 모호한 영역이나 질감이 부족한 평면에서도 대상 객체의 3D 위치를 극도로 정밀하게 디코딩할 수 있는 마스터키 역할을 수행한다.</p>
<h3>3.4  현대 AI 하드웨어에 최적화된 극단적 병렬 처리 (Parallel Processing and Scalability)</h3>
<p>이러한 쿼리 기반 설계의 파급력은 ’연산의 독립성’에서 기인한다. 디코더가 처리하는 수만 개의 쿼리는 서로 어떠한 순차적 의존성도 갖지 않기 때문에, 현대의 TPU나 GPU와 같은 텐서 연산 하드웨어 위에서 완벽한 수준의 병렬 처리가 가능하다. 모델은 단지 소수의 픽셀 궤적만을 추적하는 가벼운 작업부터, 전체 4D 장면을 수백만 개의 쿼리로 동시에 스캔하여 재구성하는 무거운 작업까지 어떠한 병목 없이 확장성(scalability)을 유지한다.</p>
<h2>4.  단일 유연 인터페이스를 통한 다목적 4D 태스크 수행 능력</h2>
<p>전통적인 딥러닝 비전 모델은 새로운 태스크가 주어지면 네트워크 아키텍처 자체를 수정하거나 새로운 손실 함수로 파인튜닝(fine-tuning)을 거쳐야 했다. 반면, D4RT는 네트워크 구조의 변경 없이 오직 쿼리 파라미터(<span class="math math-inline">t_{src}, t_{tgt}, t_{cam}</span>)의 조합을 조작하는 것만으로 복잡한 4D 이해 태스크들을 단일 인터페이스에서 즉각적으로 해결한다.</p>
<h3>4.1  3D 포인트 추적 (3D Point Tracking)</h3>
<p>비디오 프레임에 걸쳐 특정 객체나 표면 픽셀의 3D 궤적을 맵핑하는 태스크이다. 소스 픽셀 <span class="math math-inline">(u, v)</span>와 소스 타임스텝 <span class="math math-inline">t_{src}</span>를 상수로 고정하고, 타겟 타임스텝 <span class="math math-inline">t_{tgt}</span>와 로컬 카메라 좌표계 <span class="math math-inline">t_{cam}</span> 변수를 연속적으로 변화시키면서 쿼리를 던지면, 디코더는 시간 축에 따른 픽셀의 3D 이동 경로(Point Tracks)를 출력한다. D4RT의 추적 능력이 지닌 가장 강력한 이점은, 대상 객체가 다른 물체에 의해 가려지거나(occlusion) 프레임 바깥으로 일시적으로 벗어나 시각적으로 관측되지 않는 순간에도 전역 장면 표현을 기반으로 보이지 않는 위치를 논리적으로 추론하여 강건하게 예측을 이어간다는 것이다.</p>
<h3>4.2  포인트 클라우드 재구성 (Point Cloud Reconstruction)</h3>
<p>장면 전체의 3D 구조를 온전히 캡처하기 위해, 시간과 시점을 고정하는 방식(<span class="math math-inline">t_{src} = t_{tgt} = t_{cam}</span>)을 취한다. 비디오 내의 광범위한 픽셀들에 대해 해당 쿼리를 병렬로 실행하면, 공유된 단일 기준 프레임 내에 모든 픽셀의 전체 3D 위치가 예측된다. 기존 모델들은 여러 시점의 2D 이미지를 3D로 병합할 때 필연적으로 시점 변환 과정에서 발생하는 노이즈(noisy coordinate transformations)로 인해 객체가 왜곡되는 현상을 겪었으나, D4RT는 이를 원천적으로 방지한다. 나아가 별도의 명시적인 서신(explicit correspondences)이나 반복적인 모션 세그먼테이션 없이도, 카메라 포즈만을 활용하여 깊이 값을 투영함으로써 장면에 존재하는 동적 객체들을 중복 없이(deduplicated) 깔끔하게 재구성한다.</p>
<h3>4.3  깊이 맵 복원 (Depth Maps)</h3>
<p>단안 카메라 영상으로부터 픽셀별 물리적 거리를 산출하는 단안 깊이 맵(Monocular Depth Map) 추정 또한 단일 인터페이스의 단순한 변형이다. 타겟 시간과 카메라 좌표를 소스와 동일하게 설정(<span class="math math-inline">t_{src} = t_{tgt} = t_{cam}</span>)한 후 픽셀을 쿼리하여 출력된 3차원 위치 벡터 <span class="math math-inline">P</span>에서 Z축(Z-dimension) 값만을 추출해 유지하면, 그것이 곧 해당 픽셀의 정밀한 거리 정보인 깊이 맵이 된다.</p>
<h3>4.4  카메라 파라미터 예측 (Camera Pose Estimation)</h3>
<p>관찰자인 카메라 자체의 이동 궤적(trajectory)과 렌즈 특성을 복원하는 태스크이다. D4RT는 다양한 관점(viewpoints)에서 동일한 순간에 대한 3D 스냅샷을 생성하고 이를 상호 정렬하는 방식으로 카메라 파라미터를 역산한다.</p>
<ul>
<li><strong>외부 파라미터 (Extrinsics / Relative Camera Poses):</strong> 서로 다른 참조 카메라 프레임(reference frames) 하에서 그리드(grid) 단위로 일정하게 샘플링된 소스 포인트들의 3D 좌표를 추출한 뒤, 점군 정렬 기법인 우메야마 알고리즘(Umeyama’s algorithm)을 적용하여 카메라 간의 상대적인 이동 및 회전 변환 행렬을 도출한다.</li>
<li><strong>내부 파라미터 (Intrinsics):</strong> 카메라 렌즈의 초점 거리(<span class="math math-inline">f_x, f_y</span>)와 같은 고유 파라미터는, 디코더가 예측한 수많은 3D 위치 데이터로부터 파생된 개별 추정치들의 중앙값(median)을 취하는 통계적 방식을 사용하여 노이즈에 강건한 최종 파라미터를 획득한다.</li>
</ul>
<h3>4.5  모든 픽셀의 밀집 추적 (Tracking All Pixels)</h3>
<p>D4RT 연구진은 모델의 병렬성을 극대화하여 비디오 내에 존재하는 모든 픽셀의 3D 궤적을 전역 세계 좌표계(world coordinates) 상에서 예측해 내는 총체적(holistic) 장면 재구성 알고리즘(Algorithm 1)을 함께 제안했다. 모든 픽셀을 무차별적으로 쿼리하는 것은 연산의 낭비를 초래하므로, 2D 이미지 공간에 점유 그리드(occupancy grid, <span class="math math-inline">G</span>)를 초기화하여 관리한다. 모델은 아직 궤적이 추론되지 않은(unvisited) 소스 포인트를 배치(batch) 단위로 스마트하게 샘플링하고, 디코더가 병렬로 궤적 쿼리를 처리함과 동시에 현재 프레임에 노출된(visible) 픽셀 영역을 그리드 상에서 ‘방문 완료(visited)’ 상태로 신속하게 마스킹한다. 이 과정은 점유 그리드가 모두 채워질 때까지 반복되며, 결과적으로 극도로 효율적인 전체 장면 추적을 완성한다.</p>
<h2>5.  방대한 데이터셋 및 종단간 훈련 파이프라인 최적화</h2>
<p>D4RT가 단일 모델임에도 불구하고 실내외, 정적/동적 환경을 가리지 않고 범용적인 공간 지각력을 발휘할 수 있는 배경에는, 방대한 양의 고품질 혼합 데이터셋(Training Mixture)과 세밀하게 조정된 복합 손실 함수 기반의 훈련 파이프라인이 자리 잡고 있다.</p>
<h3>5.1  다양성을 고려한 대규모 훈련 데이터셋 구성</h3>
<p>모델의 학습에는 물리 법칙이 적용된 정교한 합성(Synthetic) 데이터 세트와 노이즈 및 가변적 조명을 포함하는 리얼 월드(Real-world) 데이터 세트가 총망라되어 사용되었다. 연구에 사용된 주요 공개 및 내부 데이터셋은 다음과 같다 :</p>
<ul>
<li>BlendedMVS, Co3Dv2, Dynamic Replica, Kubric, MVS-Synth, PointOdyssey, ScanNet, ScanNet++, Tartanair, VirtualKitti, Waymo Open.</li>
</ul>
<p>특히 심층적인 ‘밀도 인식(Density-aware)’ 모델링 능력을 부여하기 위해 DeepMind 연구진은 DDS (Density-aware Dynamic Scenes)라는 대규모 데이터셋을 독자적으로 큐레이션하여 학습에 투입했다. DDS 데이터셋은 실내 공간, 야외 도시 환경, 그리고 불규칙한 자연 경관 등 다양한 시나리오에서 촬영된 100시간 이상의 고품질 다중 카메라 RGB-D 비디오로 구성되어 있다. 이 비디오들은 극단적인 자아-모션(ego-motion), 고속으로 이동하는 객체, 급격한 조명 변화의 상황을 담고 있으며, 특히 3D 공간 상에서 물질이 채워진 공간(occupied space), 텅 빈 공간(empty voids), 그리고 유리나 안개처럼 속성을 확신하기 어려운 투명/반투명 불확실 영역(uncertain regions)을 모델이 스스로 구분하도록 유도한다. 이러한 밀도 학습은 기존 시스템들을 번번이 혼란에 빠뜨렸던 다층 구조의 객체나 심각한 가림(occlusion) 환경을 극복하는 데 핵심적인 기여를 했다.</p>
<h3>5.2  훈련 세부 사양 및 압도적인 연산 효율성</h3>
<p>D4RT는 Kauldron 신경망 훈련 프레임워크 상에서 구현되었으며, 훈련 과정 자체도 극도로 최적화되어 있다. 입력 데이터로는 <span class="math math-inline">256 \times 256</span> 해상도로 리사이징된 48프레임 길이의 비디오 클립이 사용된다. 모델은 한 번의 학습 스텝마다 2,048개의 무작위 쿼리를 디코딩하도록 설계되었으나, 정보량이 부족한 배경 픽셀보다는 움직임이 복잡하거나 객체의 경계선이 교차하는 핵심 영역에 쿼리를 집중시키는 오버샘플링(oversampling) 기법을 동원하여 훈련 효율을 비약적으로 끌어올렸다. AdamW 옵티마이저를 채택하여 총 500,000(500k) 스텝 동안 모델 파라미터를 업데이트했으며, 로컬 배치 사이즈는 1로 설정하여 메모리 한계를 우회하면서도 64개의 TPU 칩을 병렬로 가동하여 대규모 분산 학습을 수행했다. 그 결과, 일반적인 3D/4D 비디오 파운데이션 모델들이 수주에서 수개월의 학습 시간을 요구하는 것과 대조적으로, D4RT는 단 이틀 남짓(just over 2 days)이라는 매우 짧은 시간 안에 바닥부터(from scratch) 전체 학습을 완료하는 놀라운 연산 효율성을 증명했다.</p>
<h3>5.3  3D 재구성을 위한 복합 손실 함수 (Composite Loss Function) 설계</h3>
<p>D4RT는 다양한 4D 속성 간의 역학적 일관성을 유지하기 위해 종단간(end-to-end)으로 다수의 손실 함수를 동시에 최소화하는 전략을 사용한다.</p>
<ul>
<li><strong>기본 손실 (Primary Loss):</strong> 모델이 예측한 정규화된 3D 포인트 위치 <span class="math math-inline">P</span>에 대한 L1 손실(L1 loss)을 기본으로 적용한다. 이때 매우 중요한 기하학적 보정 기법이 동원되는데, 3D 포인트 위치를 단순히 L1 오차로만 계산할 경우, 하늘이나 먼 배경에 위치한 포인트에서 극단적인 오차값이 발생하여 모델의 기울기 폭발(gradient explosion)을 초래할 수 있다. 이를 억제하기 위해 평균 깊이 값으로 좌표를 정규화한 뒤 로그 변환(log transform)을 거친 값에 대해 손실을 계산함으로써, 원거리 포인트가 전체 학습에 미치는 악영향을 부드럽게 감쇠(dampen)시켰다.</li>
<li><strong>보조 손실 (Auxiliary Losses):</strong> 3D 구조의 미세한 디테일을 살리기 위해 다각도의 보조 손실이 결합된다. 이미지 공간으로 투영된 2D 좌표에 대한 L1 손실을 통해 픽셀 간 시각적 오차를 줄이고, 3D 표면 법선(surface normals) 간의 코사인 유사도(cosine similarity) 손실을 적용하여 렌더링된 객체의 표면이 실제처럼 매끄럽고 물리적 연속성을 띠도록 강제한다. 또한 쿼리된 타겟 포인트가 카메라 시야 내에 존재하는지(visibility) 여부를 예측하는 이진 교차 엔트로피(Binary Cross-Entropy) 손실과, 포인트의 순간적 움직임 및 모델 예측의 신뢰도(confidence) 점수에 대한 L1 손실을 병합하여 시공간적 일관성을 완성했다.</li>
</ul>
<h2>6.  정량적 성능 지표 및 벤치마크 평가 결과</h2>
<p>광범위한 벤치마크 실험 결과, D4RT는 4D 재구성 및 추적의 모든 하위 태스크에 걸쳐 이전의 상태(State-of-the-Art, SOTA) 기술들인 MegaSaM, <span class="math math-inline">\pi^3</span>, SpatialTrackerV2, TAPIR 등을 일제히 압도하며 모델 아키텍처의 절대적인 우위성을 증명했다.</p>
<h3>6.1  비약적인 추론 속도 및 실시간 처리 능력</h3>
<p>효율성 측면에서 D4RT가 거둔 성과는 가히 파괴적이다. 기존의 SOTA 모델들과 비교할 때, D4RT는 최소 <strong>18배에서 최대 300배 빠른 속도</strong>로 동작한다. 구체적인 사례로, 기존 방식들이 1분 분량의 고해상도 비디오 궤적을 재구성하는 데 최대 10분이 소요되었던 반면, D4RT는 단일 TPU 칩 환경에서 동일한 영상을 불과 <strong>약 5초</strong> 만에 처리해낸다 (기존 대비 120배 이상의 연산 시간 단축). 단일 태스크인 카메라 포즈 추정에 국한하여 평가할 경우 속도 격차는 더욱 벌어진다. D4RT는 200 FPS(초당 프레임 처리 수) 이상을 가볍게 상회하며, 이는 유사 모델인 VGGT보다 9배, 최적화가 무거운 MegaSaM 모델보다 무려 100배 빠른 처리 속도이다. 놀라운 점은 연산 속도를 극단적으로 끌어올렸음에도 불구하고 정확도(accuracy) 지표에서 단 한 번의 양보 없이 기존 모델들을 꺾었다는 사실이다.</p>
<h3>6.2  3D 포인트 및 세계 좌표계 기반 추적 성능</h3>
<p>동적 서신(dynamic correspondences) 예측 능력을 평가하기 위해, 모델은 비제한적이고 복잡한 실제 환경 비디오로 구성된 TAPVid-3D 벤치마크를 통해 테스트되었다. D4RT는 지상 관측치 고유 파라미터(ground-truth intrinsics)가 주어지지 않은 열악한 조건에서도 SpatialTracker, CoTracker, TAPIR 등 포인트 추적 전용 모델들을 뛰어넘는 정확도를 기록했다.</p>
<table><thead><tr><th><strong>평가 벤치마크</strong></th><th><strong>주요 지표명</strong></th><th><strong>D4RT 달성 성능</strong></th><th><strong>성능의 시사점</strong></th></tr></thead><tbody>
<tr><td><strong>TAPVid-3D</strong></td><td><strong>AJ (3D Average Jaccard)</strong></td><td><strong>0.257</strong></td><td>예측된 3D 궤적과 실제 궤적 간의 전반적인 유사도 및 정확도가 SOTA를 경신함.</td></tr>
<tr><td><strong>TAPVid-3D</strong></td><td><strong>APD3D</strong></td><td><strong>0.345</strong></td><td>3D 공간 상의 절대적 위치 오차율이 경쟁 모델 대비 가장 낮음.</td></tr>
<tr><td><strong>TAPVid-3D</strong></td><td><strong>OA (Occlusion Accuracy)</strong></td><td><strong>0.875</strong></td><td>물체가 다른 사물에 가려진 상황(Occlusion)에서도 87.5%의 높은 정확도로 보이지 않는 궤적을 연속 추론함.</td></tr>
<tr><td><strong>DriveTrack</strong></td><td><strong>L1 Error (World Coord)</strong></td><td><strong>0.020</strong></td><td>자율주행 데이터셋을 활용한 세계 좌표계 매핑 시 에러가 0.020으로 극도로 낮아, 거시적 맵핑에 매우 안정적임.</td></tr>
<tr><td><strong>DriveTrack</strong></td><td><strong>APD3D (World Coord)</strong></td><td><strong>0.319</strong></td><td>실외 주행 환경에서의 3D 포인트 정확도 역시 최상위권 달성.</td></tr>
</tbody></table>
<h3>6.3  포인트 클라우드 및 깊이 추정 성능 비교</h3>
<p>비디오 픽셀을 동일한 세계 좌표계 상에 예측하는 가장 총체적인 3D 재구성 태스크에서도 탁월한 결과를 보였다. 동적인 모션 블러와 비강체 변형(non-rigid deformation, 예: 사람의 근육이나 옷깃의 펄럭임)이 빈번하게 발생하는 합성 데이터셋 <strong>MPI Sintel</strong>과 실내 정적 환경 위주의 <strong>ScanNet</strong> 벤치마크 모두에서 우위를 점했다. 연구진이 예측된 포인트 클라우드와 정답(ground-truth)을 평균 이동(mean-shifting) 방식으로 정렬한 후 계산한 평균 L1 거리 지표에서 D4RT는 기존 SOTA 모델의 오차율을 크게 하회했다. 깊이 추정 성능의 경우, Scale-only (S) 정렬과 3D 병진 이동 항이 추가된 Scale-and-Shift (SS) 정렬 두 가지 환경에서 Absolute Relative Error (AbsRel)를 계산한 결과 최상위 성능을 유지했다.</p>
<h3>6.4  카메라 포즈 추정 성능 우위</h3>
<p>다양한 실내 정적 장면(ScanNet, RE10k) 및 동적 야외 장면(Sintel)을 아우르는 데이터셋에서 카메라 궤적을 복원하는 능력을 비교했다. D4RT는 <span class="math math-inline">\pi^3</span> 및 MegaSaM 모델들을 상대로 우월성을 입증했다. RE10k 데이터셋에서는 카메라 포즈 추정에 있어 가장 높은 AUC(Area Under the Curve) 점수를 획득했으며, MPI Sintel 벤치마크 결과에서는 경쟁 모델 <span class="math math-inline">\pi^3</span>가 특정 지표에서 0.086의 오차를 기록할 때 D4RT는 0.065로 오차를 억제하는 등 다수의 세부 지표에서 일제히 1위를 차지했다.</p>
<p>나아가 정성적 성능 평가(Qualitative Performance)에서도 움직이는 기차나 비행하는 백조의 비디오와 같은 고난도 동적 시나리오에서 3D 궤적을 붕괴 없이 견고하게 생성했으며, AR 스마트 글래스 카메라로 촬영되어 극심한 사용자 자아-모션(ego-motion)이 포함된 <strong>Aria Digital Twin</strong> 현실 가정 환경 데이터셋에서도 최상위 성과를 도출하여 탁월한 강건성을 시각적으로 입증했다.</p>
<h2>7.  차세대 융합 산업에 미치는 파급 효과 (Downstream Applications)</h2>
<p>D4RT가 증명한 초고속 추론 능력, 유연한 단일 인터페이스, 그리고 단안(monocular) 카메라만으로 밀도 높은 4D 재구성이 가능한 특성은, 그간 오프라인 컴퓨팅 환경에 머물러 있던 비전 모델들을 실시간(real-time) 상호작용이 필수적인 공간 컴퓨팅(Spatial Computing) 영역으로 끌어올리는 거대한 마중물이 된다.</p>
<h3>7.1  로보틱스 및 체화된 인공지능 (Robotics &amp; Embodied AI)</h3>
<p>미래의 로봇은 고정된 환경의 공장 라인이 아니라, 예측 불가능하게 움직이는 사람들과 물체들로 붐비는 매우 동적인 환경(dynamic environments)을 안전하게 탐색하고 개입해야 한다. 이를 위해서는 로봇이 카메라 렌즈를 통해 들어오는 2D 이미지를 즉각적으로 3D 공간 지각(spatial awareness)으로 변환해 내는 능력이 절대적이다. D4RT는 단안 카메라만으로도 투명한 유리나 복잡한 다층 객체를 밀도 기반으로 분리해 내며, 초당 30프레임 이상의 실시간 처리를 지원하므로 자원이 극도로 제한된 모바일 로봇 플랫폼에서도 완벽하게 구동된다.</p>
<p>이러한 즉각적 4D 지각력은 로봇이 돌발적으로 튀어나오는 장애물이나 보행자의 궤적을 선제적으로 예측하여 충돌을 회피하는 ‘안전한 내비게이션(safe navigation)’ 기술을 비약적으로 발전시킨다. 또한 시시각각 변하는 테이블 위의 물체들을 파지하고 섬세하게 다루는 ‘유연한 매니퓰레이션(dextrous manipulation)’ 수행의 핵심 인지 기저망으로 작용한다. 특히 구글 딥마인드가 발표한 최신 시각-언어-행동(VLA) 모델인 <em>Gemini Robotics 1.5</em>와 D4RT가 결합될 경우 그 시너지는 폭발적이다. D4RT가 실시간으로 물리 공간의 4D 구조를 파악해 넘겨주면, 고수준 사고(High-level brain)를 담당하는 Gemini 1.5 시스템이 해당 공간 데이터를 기반으로 ’재활용품 분류’와 같은 복잡한 논리적 계획을 세우고, ALOHA 2나 Apollo와 같은 휴머노이드 로봇의 정밀한 모터 제어 명령으로 번역하는 완벽한 인지-행동 파이프라인이 완성될 수 있다.</p>
<h3>7.2  저지연 증강 현실 (Augmented Reality, AR) 장치</h3>
<p>증강 현실 스마트 글래스와 같은 웨어러블 디바이스가 디지털 홀로그램이나 객체를 실제 물리 세계의 책상 위나 벽면에 자연스럽게 덧입히기(overlay) 위해서는, 기기가 착용자의 시선에 맞춰 주변 장면의 3D 기하학적 구조를 즉각적이고 지연 시간(low-latency) 없이 이해해야 한다. 기존의 AR 글래스들은 뎁스 구조를 파악하기 위해 라이다(LiDAR) 센서를 부착하거나 무거운 연산을 모바일 AP에 전가하여 발열과 배터리 고갈 문제를 겪었다. 그러나 D4RT 아키텍처는 단일 RGB 카메라 센서만으로도 빛의 반사나 가림 현상(occlusion)을 정확히 인지하는 밀도 모델링 능력을 갖추고 있어, 향후 출시될 경량화된 AR 하드웨어의 시각 처리 엔진으로 곧바로 채택될 수 있는 막대한 잠재력을 지니고 있다.</p>
<h3>7.3  세계 모델 구축 및 일반인공지능(AGI)으로의 도약</h3>
<p>인공지능 연구의 궁극적 목표인 인공일반지능(AGI)에 도달하기 위해, AI 시스템은 단순히 텍스트를 생성하거나 이미지를 렌더링하는 것을 넘어 물리적 현실의 인과 법칙을 시뮬레이션하고 자신의 행동이 환경에 미칠 영향을 예측하는 이른바 ’세계 모델(World Models)’을 내재화해야 한다. 진정한 세계 모델은 장면 속에서 ’나의 움직임(카메라 자아-모션)’과 ‘타자의 움직임(객체 모션)’, 그리고 ’변하지 않는 환경(정적 기하학)’을 완벽하게 분리(disentangling)하여 이해하는 능력을 전제로 한다.</p>
<p>D4RT는 이 세 가지 복잡한 물리적 역학을 하나의 트랜스포머 공간 내에서 명확히 분리하고 추적해 내는 데 성공했다. 이는 D4RT가 단순한 픽셀 분석 도구를 넘어서, AGI로 향하는 필수적인 인지적 디딤돌(necessary step on the path to AGI) 역할을 수행함을 시사한다. 최근 구글 딥마인드가 공개한 대화형 인터랙티브 가상 세계 생성 모델 <em>Genie 3</em>나 초고품질 비디오 생성 모델 <em>Veo 3.1</em> 등은 텍스트 프롬프트를 4D 물리 시뮬레이션으로 변환하는 ‘생성(Generation)’ 기능을 담당한다. 이와 대칭되는 지점에서, 2D 시각 정보로부터 4D 물리 법칙과 3D 공간을 정교하게 역설계(Reverse-engineering)하여 추출해 내는 D4RT의 ‘지각(Perception)’ 능력이 상호 결합할 때, 기계는 마침내 인간과 동일한 수준으로 세상을 입체적으로 상상하고 이해하는 인지 체계를 갖추게 될 것이다.</p>
<h2>8.  개방형 생태계 확산 및 미래 연구 방향</h2>
<p>DeepMind 연구진은 D4RT가 단순히 벤치마크 점수 경신에 그치지 않고 학계와 산업계 전반의 응용 연구를 가속할 수 있도록, 개방형 과학(Open Science)의 철학을 적극적으로 실천하고 있다. 이들은 체화된 AI(Embodied AI) 커뮤니티의 발전을 도모하기 위해 D4RT의 소스 코드, 사전 학습된 가중치(weights) 파라미터, 그리고 막대한 시간과 자원을 투입해 자체 구축한 DDS(Density-aware Dynamic Scenes) 데이터셋 일체를 오픈소스로 공개할 계획임을 시사했다. 이러한 정책적 결정은 수많은 서드파티 연구자들이 모델의 근원적 아키텍처에 접근하여 로보틱스 내비게이션 코드 최적화, AR 디바이스 포팅 등 다채로운 파생 연구를 동시다발적으로 진행할 수 있는 강력한 인프라를 제공한다.</p>
<p>물론 D4RT 아키텍처가 4D 컴퓨터 비전의 모든 한계를 완벽히 소거한 것은 아니다. 현재 모델은 극도로 어두운 저조도 환경이나 섬광이 내리쬐는 극단적인 조명 변화 조건, 그리고 학습 데이터셋(훈련 혼합물)에 단 한 번도 등장하지 않았던 완전히 새로운 범주의 미등록(novel) 객체를 마주했을 때 궤적 추정의 일반화(generalization) 성능이 일시적으로 저하될 수 있다는 기술적 과제를 안고 있다. 또한 일부 합성 벤치마크 데이터셋들이 물리 법칙이 복잡하게 얽힌 리얼 월드의 시각적 동적 복잡성(visual and motion complexity)을 온전히 대변하지 못한다는 한계점도 후속 연구를 통해 보완되어야 할 주요한 숙제로 지적된다.</p>
<p>그럼에도 불구하고, D4RT는 기존 모델들이 공유하던 구조적 병목 현상을 본질적으로 타파함으로써 향후 비전 파운데이션 모델(Vision Foundation Models)이 나아가야 할 아키텍처적 방향성을 명확히 제시했다. 향후 연구는 트랜스포머의 교차 어텐션 디코더 효율을 한 단계 더 끌어올려 초경량화를 달성하거나, D4RT를 언어 모델(LLM) 및 오디오 인식 모델과 결합하여 다중 모달리티(Multimodality) 기반의 전방위적 4D 세계 인식 모델을 구축하는 방향으로 뻗어나갈 것이다.</p>
<h2>9.  결론</h2>
<p>Google DeepMind가 발표한 D4RT(Dynamic 4D Reconstruction and Tracking)는 그간 컴퓨터 비전 분야를 괴롭혀온 ’동적 4D 장면의 즉각적 이해 및 역설계’라는 고질적 난제를 전례 없이 우아하고 효율적인 방식으로 해결해 낸 기술적 마일스톤이다. 여러 개의 특화 모델을 억지로 이어 붙이던 기존의 파편화된 다단계 파이프라인과 결별하고, 단일한 인코더-디코더 트랜스포머 구조를 채택한 D4RT의 결단은 컴퓨팅 자원의 낭비를 막고 극단적 병렬 연산의 길을 열었다. 특히 “특정 픽셀이 특정 시간과 특정 카메라 관점에서 어느 좌표에 존재하는가?“라는 독립적 시공간 포인트 질의(Querying Mechanism)를 통해 모델을 프로빙하는 방식은, 밀집 디코딩의 연산 과부하를 회피함과 동시에 모든 4D 맵핑 태스크를 유연한 단일 인터페이스 하나로 완벽하게 통합했다.</p>
<p>이러한 설계의 단순성은 벤치마크 상에서 기존 상태(SOTA) 기술들을 최소 18배에서 300배 이상의 압도적 처리 속도로 따돌리는 충격적인 효율성으로 직결되었으며 , TAPVid-3D와 DriveTrack, ScanNet 등 저명한 데이터셋에서의 3D 추적, 카메라 포즈 추정, 포인트 클라우드 재구성 지표에서 일제히 경쟁 모델들을 압도하는 무결점의 성능을 과시했다. 더욱이 DDS 데이터셋 학습을 통해 획득한 고도의 밀도 인식(density-aware) 기반 4D 추론 능력은 로보틱스의 정밀 주행 및 조작 능력을 실질적으로 향상하고, 하드웨어 제약이 큰 AR 글래스 환경에 저지연 공간 매핑 솔루션을 제공하는 등 산업 전반의 기술 지형을 재편할 핵심 기저 기술로 평가받고 있다.</p>
<p>인공지능이 진정한 자율성과 인간 수준의 인지 능력을 확보하기 위해서는 물리 세계가 움직이는 시공간의 법칙을 내재화해야만 한다. 기계가 카메라 렌즈를 통해 세상을 평면으로 단순하게 ‘보는(See)’ 것을 넘어, 인간처럼 시간과 3차원 공간이 교차하는 4차원의 흐름 속에서 사물을 입체적으로 파악하고 ’이해(Understand)’할 수 있도록 길을 튼 D4RT는, 결국 일반인공지능(AGI)과 진정한 세계 모델(World Model)의 완성을 향한 여정에서 가장 눈부시고 결정적인 도약 중 하나로 컴퓨터 비전 역사에 기록될 것이다.</p>
<h2>10. 참고 자료</h2>
<ol>
<li>D4RT: Unified, Fast 4D Scene Reconstruction &amp; Tracking — Google …, https://deepmind.google/blog/d4rt-teaching-ai-to-see-the-world-in-four-dimensions/</li>
<li>Google Deepmind’s D4RT model aims to give robots and AR devices more human-like spatial awareness - The Decoder, https://the-decoder.com/google-deepminds-d4rt-model-aims-to-give-robots-and-ar-devices-more-human-like-spatial-awareness/</li>
<li>Google DeepMind just Introduced D4RT: A New AI Model for 4D Scene Reconstruction and Tracking - DataGlobal Hub, https://dataglobalhub.org/resource/articles/google-deep-mind-just-introduced-d-4-rt-a-new-ai</li>
<li>Google DeepMind, https://deepmind.google/</li>
<li>Efficiently Reconstructing Dynamic Scenes One D4RT at a Time - arXiv, https://arxiv.org/html/2512.08924v1</li>
<li>[2512.08924] Efficiently Reconstructing Dynamic Scenes One D4RT at a Time - arXiv.org, https://arxiv.org/abs/2512.08924</li>
<li>D4RT, https://d4rt-paper.github.io/</li>
<li>Efficiently Reconstructing Dynamic Scenes One D4RT at a Time - arXiv, https://arxiv.org/html/2512.08924v2</li>
<li>Efficiently Reconstructing Dynamic Scenes One D4RT at a Time - Googleapis.com, https://storage.googleapis.com/d4rt_assets/D4RT_paper.pdf</li>
<li>Google Deepmind’s D4RT model aims to give robots and AR devices more human-like spatial awareness - Gnoppix Forum, https://forum.gnoppix.org/t/google-deepminds-d4rt-model-aims-to-give-robots-and-ar-devices-more-human-like-spatial-awareness/4169</li>
<li>Google Deepmind - D4RT: Unified, Fast 4D Scene Reconstruction &amp; Tracking : r/singularity, https://www.reddit.com/r/singularity/comments/1qmjdsh/google_deepmind_d4rt_unified_fast_4d_scene/</li>
<li>TAPVid-3D: A Benchmark for Tracking Any Point in 3D | Request PDF - ResearchGate, https://www.researchgate.net/publication/397202042_TAPVid-3D_A_Benchmark_for_Tracking_Any_Point_in_3D</li>
<li>SpatialTracker: Tracking Any 2D Pixels in 3D Space | Request PDF - ResearchGate, https://www.researchgate.net/publication/384209970_SpatialTracker_Tracking_Any_2D_Pixels_in_3D_Space</li>
<li>google-deepmind/deepmind-research: This repository contains implementations and illustrative code to accompany DeepMind publications - GitHub, https://github.com/google-deepmind/deepmind-research</li>
<li>PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking | Request PDF, https://www.researchgate.net/publication/377430927_PointOdyssey_A_Large-Scale_Synthetic_Dataset_for_Long-Term_Point_Tracking</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>