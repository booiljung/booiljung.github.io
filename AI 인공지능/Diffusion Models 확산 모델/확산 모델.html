<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:확산 모델</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>확산 모델</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">확산 모델 (Diffusion Model)</a> / <span>확산 모델</span></nav>
                </div>
            </header>
            <article>
                <h1>확산 모델</h1>
<h2>1. 서론</h2>
<p>생성 모델(Generative Model) 분야는 지난 십수 년간 괄목할 만한 발전을 거듭하며 인공지능 연구의 최전선을 이끌어왔다. 생성적 적대 신경망(Generative Adversarial Networks, GANs), 변분 오토인코더(Variational Autoencoders, VAEs), 그리고 자기회귀 모델(Autoregressive Models)과 같은 초기 패러다임들은 각기 다른 접근법을 통해 데이터의 잠재적 분포를 학습하고 새로운 샘플을 생성하는 능력을 선보였다.1 이들은 이미지, 텍스트, 음성 등 다양한 도메인에서 인상적인 결과를 만들어냈으나, 학습 과정의 불안정성, 생성된 샘플의 다양성 부족, 또는 낮은 품질 등의 한계를 종종 드러내었다. 이러한 상황 속에서, 비평형 통계물리학에서 영감을 받은 확산 모델(Diffusion Models)이 새로운 대안으로 부상하며 생성 모델링의 패러다임을 근본적으로 바꾸어 놓았다.2</p>
<p>확산 모델은 데이터를 점진적으로 노이즈로 변환하는 순방향 과정(forward process)과, 그 과정을 역으로 거슬러 올라가 노이즈로부터 데이터를 복원하는 역방향 과정(reverse process)을 학습하는 독특한 구조를 가진다.5 이 접근법은 특히 이미지 생성 분야에서 전례 없는 수준의 품질과 사실성을 달성하며 학계와 산업계의 폭발적인 주목을 받았다. CIFAR-10, ImageNet과 같은 표준 벤치마크에서 기존 모델들을 압도하는 FID(Fréchet Inception Distance) 및 Inception 점수를 기록하며, 확산 모델은 고품질 생성의 새로운 표준으로 자리매김했다.5 예를 들어, 초기 연구에서부터 CIFAR-10 데이터셋에 대해 2.20의 FID 점수를 달성하는 등 기록적인 성능을 보였다.5</p>
<p>그러나 이러한 뛰어난 성능의 이면에는 치명적인 단점이 존재했다. 바로 생성 속도의 문제였다. 초기 확산 모델들은 고품질의 샘플 하나를 생성하기 위해 수백에서 수천 번에 이르는 반복적인 신경망 평가를 요구했다.2 이로 인해 실시간 응용이나 대규모 생성 작업에는 부적합하다는 평가를 받았으며, 이는 확산 모델의 대중화를 가로막는 가장 큰 기술적 장벽으로 작용했다.11 이 문제를 해결하기 위한 노력은 이후 확산 모델 연구의 핵심적인 동력이 되었으며, 이는 본 안내서의 주요 논의 주제 중 하나이다.</p>
<p>본 안내서는 확산 모델의 이론적 근간부터 최신 기술 동향, 그리고 미래 전망에 이르기까지 포괄적이고 심층적인 고찰을 제공하는 것을 목표로 한다. 이를 위해 안내서는 다음과 같은 논리적 흐름에 따라 전개된다. 제1장에서는 확산 모델의 수학적 기초가 되는 확률적 미분 방정식(SDE) 프레임워크를 탐구하고, 이것이 어떻게 기존 모델들을 통합하며 새로운 연구의 지평을 열었는지 분석한다. 제2장에서는 확산 모델의 가장 큰 약점이었던 샘플링 속도를 개선하기 위한 핵심 기술들, 즉 점진적 증류와 컨시스턴시 모델의 원리와 발전을 상세히 다룬다. 제3장에서는 아키텍처의 진화를 조명하며, Sora와 Lumiere로 대표되는 비디오 생성 모델들이 어떻게 시공간 데이터를 처리하는지에 대한 상이한 철학을 비교 분석한다. 제4장에서는 확산 모델이 과학적 발견(3D 분자 구조 생성)과 예술적 창작(텍스트-음악 생성) 등 다양한 전문 분야로 확장되면서 마주하는 도메인 특화 과제들을 살펴본다. 마지막으로 제5장에서는 지금까지의 논의를 종합하고, 현재 패러다임의 근본적인 한계에 대한 비판적 고찰을 통해 차세대 생성 모델, 즉 ’월드 모델’을 향한 미래 전망을 제시하며 안내서를 마무리한다.</p>
<h2>2.  확산 모델의 수학적 기초: 확률적 미분 방정식</h2>
<p>확산 모델의 현대적 이해는 확률적 미분 방정식(Stochastic Differential Equation, SDE)이라는 강력한 수학적 도구를 통해 완성되었다. SDE 프레임워크는 기존의 이산적인 시간 단계에 기반한 접근법들을 연속 시간의 관점에서 통합하고 일반화함으로써, 모델 설계의 유연성을 극대화하고 새로운 이론적 발전을 위한 토대를 마련했다. 이 장에서는 데이터가 노이즈로 변환되는 순방향 과정과 노이즈로부터 데이터가 생성되는 역방향 과정이 어떻게 SDE로 공식화되는지, 그리고 이 프레임워크가 어떻게 확산 모델 연구의 패러다임을 바꾸었는지 심도 있게 탐구한다.</p>
<h3>2.1  데이터에서 노이즈로: 순방향 SDE</h3>
<p>확산 모델의 근본적인 아이디어는 복잡하고 구조화된 데이터 분포 <span class="math math-inline">p_0(\mathbf{x})</span>를 점진적으로 파괴하여, 다루기 쉬운 단순한 사전 분포(prior distribution) <span class="math math-inline">p_T(\mathbf{x})</span>(예: 표준 정규분포)로 변환하는 것이다.5 이 과정을 무한히 작은 시간 단계의 연속으로 간주할 때, 데이터 포인트 <span class="math math-inline">\mathbf{x}(t)</span>의 시간적 변화는 다음과 같은 일반적인 형태의 Itô SDE로 모델링될 수 있다.13<br />
<span class="math math-display">
d\mathbf{x} = \mathbf{f}(\mathbf{x}, t)dt + g(t)d\mathbf{w}
</span><br />
여기서 <span class="math math-inline">t</span>는 <span class="math math-inline">0</span>부터 <span class="math math-inline">T</span>까지 흐르는 연속적인 시간 변수이며, <span class="math math-inline">\mathbf{x}(0)</span>은 원본 데이터 분포 <span class="math math-inline">p_0</span>에서 샘플링된 데이터 포인트를 의미한다. <span class="math math-inline">\mathbf{w}</span>는 표준 위너 과정(Wiener process) 또는 브라운 운동(Brownian motion)으로, 시간에 따른 무작위적인 변동을 나타낸다.2</p>
<p><span class="math math-inline">\mathbf{f}(\mathbf{x}, t)</span>는 드리프트 계수(drift coefficient)로서 시간에 따른 <span class="math math-inline">\mathbf{x}</span>의 결정론적 변화 경향을 제어하고, <span class="math math-inline">g(t)</span>는 확산 계수(diffusion coefficient)로서 주입되는 노이즈의 강도를 조절한다.13 이 순방향 SDE의 중요한 특징은 데이터 분포 자체에 의존하지 않으며, 학습 가능한 파라미터 없이 사전에 정의된다는 점이다.13 즉, 데이터가 노이즈로 변환되는 ’경로’는 미리 정해져 있다.</p>
<h3>2.2  노이즈에서 데이터로: 역방향 SDE와 스코어 함수</h3>
<p>생성 모델링의 목표는 순방향 과정의 역, 즉 사전 분포 <span class="math math-inline">p_T</span>에서 샘플링한 노이즈 <span class="math math-inline">\mathbf{x}(T)</span>로부터 원본 데이터 <span class="math math-inline">\mathbf{x}(0)</span>를 생성하는 것이다. Anderson (1982)의 선구적인 연구에 따르면, 특정 조건 하에서 모든 확산 과정 SDE는 시간의 흐름을 거꾸로 뒤집은 역방향 SDE(reverse-time SDE)를 갖는다.5 이 역방향 SDE는 다음과 같이 표현된다.6<br />
<span class="math math-display">
d\mathbf{x} = [\mathbf{f}(\mathbf{x}, t) - g(t)^2 \nabla_{\mathbf{x}} \log p_t(\mathbf{x})]dt + g(t)d\bar{\mathbf{w}}
</span><br />
여기서 <span class="math math-inline">d\bar{\mathbf{w}}</span>는 시간이 역으로 흐를 때의 위너 과정이며, <span class="math math-inline">dt</span>는 음의 무한소 시간 변화량을 의미한다. 이 방정식의 가장 중요하고 핵심적인 부분은 역방향 과정이 오직 <strong>스코어 함수(score function)</strong>, 즉 각 시간 <span class="math math-inline">t</span>에서의 주변 데이터 분포 <span class="math math-inline">p_t(\mathbf{x})</span>의 로그 확률 밀도에 대한 그래디언트인 <span class="math math-inline">\nabla_{\mathbf{x}} \log p_t(\mathbf{x})</span>에만 의존한다는 사실이다.5 스코어 함수는 데이터 밀도가 높은 방향을 가리키는 벡터 필드로 해석될 수 있으며, 생성 과정에서 노이즈 샘플이 데이터 매니폴드(data manifold)를 향해 이동하도록 안내하는 역할을 한다.</p>
<p>문제는 <span class="math math-inline">p_t(\mathbf{x})</span>를 직접 계산하는 것이 불가능하므로 스코어 함수 역시 알 수 없다는 점이다. 확산 모델은 이 미지의 스코어 함수를 파라미터 <span class="math math-inline">\theta</span>를 갖는 신경망, 즉 스코어 모델 <span class="math math-inline">\mathbf{s}_\theta(\mathbf{x}, t)</span>로 근사한다.5 이 스코어 모델은 스코어 매칭(score matching)이라는 목적 함수를 최소화하도록 학습된다.14 학습이 완료되면, 근사된 스코어 모델 <span class="math math-inline">\mathbf{s}_\theta(\mathbf{x}, t)</span>를 역방향 SDE에 대입하고 수치적 SDE 솔버(numerical SDE solver)를 사용하여 노이즈로부터 고품질의 데이터 샘플을 생성할 수 있다.8</p>
<h3>2.3  통합 프레임워크로서의 SDE</h3>
<p>SDE 프레임워크의 진정한 가치는 그것이 기존의 대표적인 확산 모델 계열들을 하나의 통일된 관점에서 설명하고 일반화한다는 데 있다.5 이전까지 서로 다른 접근법으로 여겨졌던 Denoising Diffusion Probabilistic Models (DDPM) 6와 Score Matching with Langevin Dynamics (SMLD) 13는 사실상 SDE 프레임워크의 특정 이산화(discretization)에 해당한다.5</p>
<ul>
<li><strong>DDPM과 분산 보존(VP) SDE:</strong> DDPM에서 사용되는 이산적인 노이즈 주입 과정은, 무한히 많은 단계로 극한을 취할 때 분산 보존(Variance Preserving, VP) SDE로 수렴한다. 이 SDE는 시간이 지나도 데이터의 분산이 일정하게 유지되는 경로를 모델링한다.13</li>
<li><strong>SMLD와 분산 폭발(VE) SDE:</strong> SMLD에서 사용되는, 점차 강해지는 노이즈 스케줄은 분산 폭발(Variance Exploding, VE) SDE의 이산화로 볼 수 있다. 이 SDE는 시간이 흐름에 따라 데이터의 분산이 기하급수적으로 증가하는 경로를 나타낸다.13</li>
</ul>
<p>이러한 통합은 단순히 두 모델을 수학적으로 연결하는 것을 넘어, 확산 모델 연구를 위한 근본적인 추상화 계층을 제공했다. 연구자들은 더 이상 특정 이산적 노이즈 스케줄이나 구현 방식에 얽매일 필요가 없어졌다. 대신, 드리프트 함수 <span class="math math-inline">\mathbf{f}</span>와 확산 함수 <span class="math math-inline">g</span>라는 두 가지 핵심 요소를 설계함으로써, 데이터와 노이즈 사이를 잇는 무한한 종류의 ‘경로’ 자체를 탐색할 수 있는 디자인 공간을 확보하게 된 것이다.</p>
<p>이러한 개념적 전환의 가장 중요한 산물 중 하나는 **확률 흐름 상미분 방정식(Probability Flow ODE)**의 발견이다.5 역방향 SDE에서 확률적 항(<span class="math math-inline">g(t)d\bar{\mathbf{w}}</span>)을 제거하면 다음과 같은 결정론적(deterministic) 상미분 방정식(ODE)을 얻을 수 있다.<br />
<span class="math math-display">
\frac{d\mathbf{x}}{dt} = \mathbf{f}(\mathbf{x}, t) - \frac{1}{2} g(t)^2 \nabla_{\mathbf{x}} \log p_t(\mathbf{x})
</span><br />
이 PF-ODE의 해(solution)는 놀랍게도 원래의 SDE와 동일한 주변 분포 <span class="math math-inline">p_t(\mathbf{x})</span>의 궤적을 따른다.14 즉, 확률적인 변동 없이도 노이즈에서 데이터로의 변환이 가능하다. 이 결정론적 경로는 샘플링 과정에서 발생하는 오차를 줄여 더 빠른 샘플링을 가능하게 하고, 가역성(reversibility)을 보장하여 정확한 로그 확률 계산의 길을 열었다.5 더 나아가, “하나의 고유한 경로가 존재한다“는 이 아이디어는 이후 등장할 컨시스턴시 모델의 이론적 초석이 되었다. PF-ODE가 제시한 ’결정론적 궤적’이라는 개념이 없었다면, “궤적 위의 모든 점은 동일한 시작점으로 돌아가야 한다“는 자기 일관성(self-consistency) 원리, 즉 컨시스턴시 모델의 핵심 아이디어는 개념적으로 정립되기 어려웠을 것이다.2 이처럼 SDE 프레임워크, 특히 PF-ODE의 발견은 확산 모델을 단순한 생성 알고리즘에서 풍부한 수학적 구조를 지닌 대상으로 격상시켰으며, 이후의 혁신적인 발전을 위한 이론적 발판을 마련한 결정적인 전환점이었다.</p>
<h2>3.  샘플링 가속화: 실시간 생성을 향한 도약</h2>
<p>확산 모델이 제시한 압도적인 생성 품질에도 불구하고, 초기 모델들은 수백에서 수천 번의 반복적인 함수 평가를 요구하는 느린 샘플링 속도라는 치명적인 약점을 안고 있었다.2 이는 모델의 실용성을 크게 저해하는 요소였으며, 이 문제를 해결하기 위한 연구는 확산 모델 발전의 가장 중요한 축을 형성했다. 이 장에서는 샘플링 가속화를 위한 두 가지 핵심적인 접근법, 즉 ’점진적 증류’와 ’컨시스턴시 모델’을 중심으로 그 원리와 발전을 상세히 분석하고, 이들이 어떻게 실시간 생성의 가능성을 열었는지 탐구한다.</p>
<h3>3.1  점진적 증류</h3>
<p>샘플링 속도 문제를 해결하기 위한 초기 접근법 중 가장 성공적인 사례는 Salimans와 Ho가 제안한 ‘점진적 증류(Progressive Distillation)’ 기법이다.12 이 방법은 이미 학습된 고품질의 느린 ‘교사(teacher)’ 모델의 지식을, 더 적은 샘플링 스텝으로 유사한 결과를 내는 ‘학생(student)’ 모델에게 전달하는 증류(distillation) 과정을 기반으로 한다.10</p>
<p>점진적 증류의 핵심 아이디어는 교사 모델이 수행하는 두 번의 샘플링 스텝을 학생 모델이 단 한 번의 스텝으로 모방하도록 학습시키는 것이다.10 구체적인 과정은 다음과 같다. 먼저, 수천 스텝(예: <span class="math math-inline">N</span> 스텝)을 사용하는 결정론적 DDIM 샘플러 기반의 교사 모델이 있다.12 학생 모델은 교사 모델의 파라미터로 초기화된다. 학습 과정에서, 특정 시점 <span class="math math-inline">t</span>의 노이즈 데이터 <span class="math math-inline">\mathbf{x}_t</span>가 주어지면, 교사 모델은 두 번의 DDIM 스텝을 수행하여 <span class="math math-inline">\mathbf{x}_{t-2}</span>를 계산한다 (<span class="math math-inline">\mathbf{x}_t \to \mathbf{x}_{t-1} \to \mathbf{x}_{t-2}</span>). 학생 모델의 목표는 <span class="math math-inline">\mathbf{x}_t</span>에서 단 한 번의 스텝으로 이 <span class="math math-inline">\mathbf{x}_{t-2}</span>와 동일한 결과를 예측하도록 학습하는 것이다. 이를 위해, 한 스텝으로 <span class="math math-inline">\mathbf{x}_t</span>에서 <span class="math math-inline">\mathbf{x}_{t-2}</span>로 이동시키는 데 필요한 이상적인 예측값 <span class="math math-inline">\tilde{\mathbf{x}}</span>를 계산하고, 학생 모델의 출력이 이 <span class="math math-inline">\tilde{\mathbf{x}}</span>에 가까워지도록 손실 함수를 구성하여 학습한다.12</p>
<p>이 증류 과정을 한 번 마치면, 학생 모델은 교사 모델의 절반인 <span class="math math-inline">N/2</span> 스텝만으로 유사한 품질의 샘플을 생성할 수 있게 된다. ’점진적’이라는 이름이 붙은 이유는 이 과정을 반복적으로 적용하기 때문이다. 첫 번째 증류를 마친 학생 모델이 새로운 교사 모델이 되어, 다시 그 절반인 <span class="math math-inline">N/4</span> 스텝을 사용하는 새로운 학생 모델을 학습시킨다. 이 과정을 계속 반복하면(<span class="math math-inline">N \to N/2 \to N/4 \dots</span>), 샘플링 스텝 수를 기하급수적으로 줄일 수 있다.10 연구 결과에 따르면, 이 기법을 통해 초기 1024 또는 8192 스텝을 사용하던 모델을 단 4~8 스텝만으로도 높은 지각적 품질을 유지하는 모델로 증류할 수 있었으며, CIFAR-10 데이터셋에서 4스텝만으로 3.0의 FID 점수를 달성하는 등 인상적인 성과를 보였다.10 점진적 증류는 기존 확산 모델의 프레임워크를 크게 벗어나지 않으면서도 샘플링 속도를 획기적으로 개선한 실용적이고 강력한 공학적 해결책이었다.</p>
<h3>3.2  컨시스턴시 모델</h3>
<p>점진적 증류가 기존의 샘플링 경로를 ’압축’하는 공학적 최적화에 가까웠다면, Song 등이 제안한 ’컨시스턴시 모델(Consistency Models)’은 샘플링 문제에 대한 근본적인 패러다임 전환을 제시했다.2 이 모델의 목표는 제1장에서 소개된 PF-ODE의 결정론적 궤적 위 어떤 지점(<span class="math math-inline">\mathbf{x}_t</span>)에서든, 그 궤적의 시작점인 원본 데이터(<span class="math math-inline">\mathbf{x}_0</span>)로 직접 매핑하는 함수 <span class="math math-inline">f(\mathbf{x}_t, t) = \mathbf{x}_0</span>를 학습하는 것이다.2</p>
<p>이러한 직접 매핑을 가능하게 하는 핵심 원리는 **자기 일관성(self-consistency)**이다.2 이는 “하나의 동일한 PF-ODE 궤적에 속하는 모든 점들(<span class="math math-inline">\mathbf{x}_{t_1}, \mathbf{x}_{t_2}, \dots</span>)은 반드시 동일한 시작점 <span class="math math-inline">\mathbf{x}_0</span>에 매핑되어야 한다“는 제약 조건이다. 즉, <span class="math math-inline">f(\mathbf{x}_{t_i}, t_i) = f(\mathbf{x}_{t_j}, t_j)</span> for all <span class="math math-inline">i, j</span>가 성립해야 한다. 이 자기 일관성 속성을 모델에 강제함으로써, 모델은 어떤 시점의 노이즈가 주어지더라도 단 한 번의 함수 평가만으로 최종 이미지를 생성할 수 있게 된다. 이는 반복적인 샘플링 과정 자체를 생략하는 혁신적인 접근법이다.7</p>
<p>컨시스턴시 모델의 학습 방법은 크게 두 가지로 나뉜다 7:</p>
<ol>
<li><strong>증류 기반 학습 (Consistency Distillation):</strong> 이 방법은 사전 학습된 확산 모델(스코어 모델)을 활용한다.2 먼저, 스코어 모델과 ODE 솔버를 사용하여 PF-ODE 궤적 상의 인접한 두 점,</li>
</ol>
<p><span class="math math-inline">\mathbf{x}_{t_{n+1}}</span>과 <span class="math math-inline">\mathbf{x}_{t_n}</span>을 생성한다. 그 후, 컨시스턴시 모델 <span class="math math-inline">f_\theta</span>의 출력이 이 두 점에 대해 일관성을 갖도록, 즉 두 출력값 사이의 거리(예: LPIPS)가 최소화되도록 학습한다. 이 과정은 교사 모델(스코어 모델)의 지식을 학생 모델(컨시스턴시 모델)에게 증류하는 것과 유사하다. 이 방법은 CIFAR-10에서 단일 스텝 생성으로 3.55의 FID를, ImageNet 64x64에서 6.20의 FID를 달성하며 기존 증류 기법들을 능가하는 성능을 보였다.7</p>
<ol start="2">
<li><strong>독립 학습 (Consistency Training):</strong> 이 방법은 사전 학습된 모델 없이 데이터로부터 직접 컨시스턴시 모델을 학습한다.7 스코어 함수를 알 수 없기 때문에, ODE 솔버의 단일 스텝 근사를 사용하여 인접한 점을 추정하고, 이 추정된 점들에 대해 자기 일관성 손실을 적용한다.24 이 접근법은 사전 학습 모델의 성능에 제약받지 않는다는 장점이 있다. 초기에는 증류 기반 모델보다 성능이 낮았으나, 이후 연구에서 EMA(Exponential Moving Average)를 제거하고 Pseudo-Huber와 같은 더 견고한 손실 함수를 도입하는 등 학습 기법을 개선함으로써 증류 모델의 성능을 뛰어넘는 결과를 달성했다.25 개선된 독립 학습 기법은 CIFAR-10에서 2.51, ImageNet 64x64에서 3.25의 FID를 단일 스텝으로 달성하며 그 잠재력을 입증했다.25</li>
</ol>
<p>컨시스턴시 모델은 단일 스텝 생성뿐만 아니라, 몇 번의 추가적인 정제 스텝을 통해 샘플 품질을 더욱 향상시킬 수 있는 유연성도 제공한다.2 또한, 명시적인 학습 없이도 이미지 인페인팅, 색상화, 초해상도와 같은 제로샷(zero-shot) 데이터 편집 작업을 수행할 수 있는 능력까지 갖추고 있어 2, 단순한 가속화 기술을 넘어 새로운 생성 모델 패러다임으로서의 가능성을 보여주었다.</p>
<h3>3.3  다중 스텝 컨시스턴시 모델</h3>
<p>단일 스텝 생성이 가능한 컨시스턴시 모델은 속도 면에서 혁신적이었지만, 여전히 수십~수백 스텝을 사용하는 전통적인 확산 모델에 비해 샘플 품질이 다소 떨어지는 한계가 있었다. 반면, 확산 모델은 품질은 높지만 매우 느렸다. 이 두 극단 사이의 실용적인 절충안을 제시한 것이 바로 ’다중 스텝 컨시스턴시 모델(Multistep Consistency Models)’이다.26</p>
<p>이 모델은 컨시스턴시 모델(1-step)과 확산 모델(<span class="math math-inline">\infty</span>-step) 사이를 매끄럽게 보간(interpolate)하는 통합 프레임워크를 제안한다. 사용자는 샘플링 시 사용할 스텝의 수(예: 2, 4, 8 스텝)를 자유롭게 선택할 수 있으며, 이를 통해 계산량(속도)과 샘플 품질 간의 트레이드오프를 직접 제어할 수 있다.26 예를 들어, 8스텝 샘플링을 사용하면 단일 스텝 컨시스턴시 모델보다 훨씬 높은 품질의 이미지를 생성하면서도, 수백 스텝을 요구하는 확산 모델보다는 훨씬 빠른 속도를 유지할 수 있다. 이 접근법은 ImageNet 64x64 데이터셋에서 8스텝만으로 1.4 FID라는 매우 높은 성능을 달성하며, 속도와 품질이라는 두 마리 토끼를 모두 잡는 실용적인 해결책임을 입증했다.26</p>
<p>이러한 발전의 궤적은 샘플링 속도 문제에 대한 접근 방식이 어떻게 진화했는지를 명확히 보여준다. 점진적 증류는 기존 프레임워크 내에서 “어떻게 하면 기존의 여러 스텝을 더 적은 스텝으로 압축할 수 있을까?“라는 공학적 질문에 대한 답이었다. 이는 기존 경로를 인정하고 이를 효율화하려는 시도였다. 반면, 컨시스턴시 모델은 “왜 굳이 경로를 따라가야 하는가? 경로 위의 어떤 지점에서든 목적지로 바로 점프할 수는 없는가?“라는 더 근본적인 질문을 던졌다. 이는 PF-ODE라는 이론적 토대 위에서 생성 방식 자체를 재정의하려는 이론적 도약이었다. 그리고 다중 스텝 컨시스턴시 모델의 등장은, 이 두 접근법의 장점을 결합하려는 변증법적 종합의 결과로 볼 수 있다. 즉, 컨시스턴시 모델의 ’단일 스텝’이라는 이론적 이상과, 확산 모델의 ’다중 스텝’이 주는 품질 안정성이라는 공학적 현실 사이에서 최적의 균형점을 찾으려는 시도인 것이다. 이는 기술 발전이 단순히 선형적으로 이루어지는 것이 아니라, 때로는 이전 패러다임의 장점을 다시 통합하며 더 성숙한 형태로 나아감을 보여주는 대표적인 사례이다.</p>
<p>아래 표는 본 장에서 논의된 주요 샘플링 가속화 기법들의 핵심적인 특징을 비교하여 요약한 것이다.</p>
<h3>3.4 표 1: 주요 샘플링 가속화 기법 비교</h3>
<table><thead><tr><th><strong>특성 (Feature)</strong></th><th><strong>SDE/ODE 샘플러 (SDE/ODE Sampler)</strong></th><th><strong>점진적 증류 (Progressive Distillation)</strong></th><th><strong>컨시스턴시 모델 (Consistency Models)</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 아이디어</strong></td><td>역방향 SDE/ODE를 수치적으로 풀어 노이즈를 데이터로 변환</td><td>교사 모델의 2스텝을 학생 모델의 1스텝으로 압축하는 과정 반복</td><td>PF-ODE 궤적 상의 모든 점을 시작점으로 직접 매핑 (자기 일관성)</td></tr>
<tr><td><strong>샘플링 방식</strong></td><td>반복적, 순차적 (Iterative, Sequential)</td><td>반복적, 순차적 (단, 스텝 수 감소)</td><td>단일 스텝 (One-Step) 또는 소수 스텝</td></tr>
<tr><td><strong>일반적 스텝 수</strong></td><td>100 ~ 8000+</td><td>4 ~ 64</td><td>1 ~ 8</td></tr>
<tr><td><strong>사전 학습 모델</strong></td><td>필요 없음 (자체 학습)</td><td><strong>필수</strong> (증류할 교사 모델)</td><td>선택적 (증류 기반 또는 독립 학습)</td></tr>
<tr><td><strong>품질 대 속도</strong></td><td>최고 품질, 가장 느림</td><td>스텝 수에 따라 품질과 속도 조절</td><td>최고 속도, 스텝 수 증가 시 품질 향상</td></tr>
<tr><td><strong>주요 논문</strong></td><td>Song et al., 2021 5</td><td>Salimans &amp; Ho, 2022 12</td><td>Song et al., 2023 7</td></tr>
</tbody></table>
<h2>4.  아키텍처의 확장: 시공간 데이터 생성을 향하여</h2>
<p>확산 모델이 이미지 생성에서 거둔 성공은 자연스럽게 더 복잡하고 고차원적인 데이터, 특히 비디오와 같은 시공간 데이터로의 확장에 대한 기대로 이어졌다. 비디오 생성은 단순히 고품질의 프레임을 나열하는 것을 넘어, 시간의 흐름에 따른 객체의 움직임, 상호작용, 그리고 장면의 변화를 일관성 있게 모델링해야 하는 근본적인 도전을 안고 있다. 이 장에서는 비디오 생성을 위해 제안된 두 가지 대표적인 아키텍처, 즉 OpenAI의 Sora가 채택한 ’확산 트랜스포머’와 Google의 Lumiere가 제시한 ’시공간 U-Net’을 중심으로, 이들이 시공간 데이터를 모델링하는 상이한 철학과 접근법을 심층적으로 비교 분석한다.</p>
<h3>4.1  확산 트랜스포머와 Sora</h3>
<p>전통적인 확산 모델은 주로 U-Net 아키텍처를 노이즈 제거 네트워크의 백본으로 사용해왔다. 그러나 Peebles와 Xie는 ’확산 트랜스포머(Diffusion Transformer, DiT)’라는 연구를 통해 트랜스포머가 확산 모델의 백본으로서 뛰어난 성능과 확장성(scalability)을 가질 수 있음을 입증했다.27 DiT는 모델의 파라미터 수나 계산량(Gflops)을 늘릴수록 FID 점수가 예측 가능하게 향상되는 강력한 확장성을 보여주었으며, 이는 더 큰 모델과 데이터셋을 통해 성능을 지속적으로 개선할 수 있음을 시사했다.27</p>
<p>OpenAI의 텍스트-비디오 모델인 Sora는 바로 이 DiT 아키텍처의 잠재력을 비디오 생성에 극대화한 대표적인 사례이다.28 Sora는 비디오 데이터를 처리하기 위해 다음과 같은 혁신적인 구성 요소들을 도입했다 29:</p>
<ul>
<li><strong>비디오 압축 네트워크 (Video Compression Network):</strong> Sora의 처리 파이프라인은 먼저 원본 비디오를 시공간적으로 압축된 저차원 잠재 공간(latent space)으로 인코딩하는 것에서 시작한다.29 이 압축 네트워크는 고차원의 픽셀 데이터를 효율적으로 처리 가능한 형태로 변환하는 역할을 한다. 모든 후속 생성 과정은 이 압축된 잠재 공간 내에서 이루어지며, 최종적으로 디코더가 잠재 표현을 다시 픽셀 공간으로 복원한다.29 이는 Stable Diffusion과 같은 잠재 공간 확산 모델의 접근법과 유사하다.28</li>
<li><strong>시공간 잠재 패치 (Spacetime Latent Patches):</strong> Sora 아키텍처의 가장 핵심적인 아이디어는 압축된 잠재 비디오를 ’시공간 잠재 패치(spacetime latent patches)’라는 단위로 분할하는 것이다.29 이 패치들은 공간(프레임 내의 영역)과 시간(연속된 프레임들)을 모두 포함하는 4차원 덩어리다. 이렇게 생성된 패치 시퀀스는 거대 언어 모델(LLM)의 ’텍스트 토큰’과 동일한 역할을 수행하며, DiT의 입력으로 제공된다.28 이 패치 기반 표현 방식은 Sora에 놀라운 유연성을 부여한다. 모델은 가변적인 해상도, 길이, 그리고 종횡비를 가진 비디오와 이미지를 별도의 처리 없이 동일한 프레임워크 내에서 통합적으로 학습할 수 있다. 이미지는 단순히 단일 프레임으로 구성된 비디오로 간주된다.29</li>
</ul>
<p>Sora의 접근법은 본질적으로 비디오를 ’시각적 단어들의 시퀀스’로 간주하고, 트랜스포머를 이용해 이 단어들 간의 복잡한 문법(시공간적 관계)을 학습하는 언어 모델적 패러다임을 시각 데이터에 적용한 것이다.</p>
<h3>4.2  시공간 U-Net과 Lumiere</h3>
<p>반면, Google Research에서 개발한 Lumiere는 비디오의 시간적 일관성(temporal consistency)이라는 문제를 해결하기 위해 근본적으로 다른 아키텍처 철학을 제시한다.30 기존 비디오 생성 모델들이 주로 키프레임을 먼저 생성하고 그 사이를 보간(temporal super-resolution)하는 계단식(cascaded) 접근법을 사용했던 것과 달리, Lumiere는 비디오의 <strong>전체 시간 길이(entire temporal duration)를 단일 패스(single pass)로 직접 생성</strong>하는 것을 목표로 한다.30</p>
<p>이러한 전체론적(holistic) 생성을 가능하게 하는 핵심 기술이 바로 ‘시공간 U-Net(Space-Time U-Net, STUNet)’ 아키텍처이다.30 STUNet은 기존의 2D U-Net을 시공간 차원으로 확장한 구조로, 다음과 같은 특징을 가진다:</p>
<ul>
<li><strong>시공간 동시 처리:</strong> STUNet은 공간(프레임의 높이와 너비)과 시간(프레임의 수)을 분리하지 않고 하나의 통합된 4D 텐서로 처리한다.</li>
<li><strong>시간적 다운/업샘플링:</strong> U-Net의 인코더-디코더 구조에서 공간적 해상도를 줄였다가 다시 복원하는 다운/업샘플링이 이루어지는 것과 마찬가지로, STUNet은 시간적 차원에서도 다운샘플링과 업샘플링을 수행한다.30 즉, 모델의 깊은 층으로 갈수록 공간적 해상도뿐만 아니라 시간적 길이도 줄어든다. 이를 통해 모델은 매우 압축된 시공간 표현(compact space-time representation)에서 대부분의 연산을 수행하게 되며, 이는 계산 효율성을 높이는 동시에 비디오 전체의 장기적인 움직임 패턴을 학습하는 데 유리하다.30</li>
<li><strong>사전 학습된 T2I 모델 활용:</strong> STUNet은 사전 학습된 텍스트-이미지(T2I) 모델의 가중치를 고정한 채, 그 사이에 시간적 연산을 수행하는 모듈(예: 1D 시간적 컨볼루션, 시간적 어텐션)을 삽입하여 학습하는 방식을 취한다.30 이를 통해 강력한 이미지 생성 능력을 기반으로 시간적 일관성을 효율적으로 학습할 수 있다.</li>
</ul>
<p>Lumiere의 접근법은 비디오를 개별 프레임이나 패치의 집합이 아닌, 하나의 온전한 ’시공간 신호(spatiotemporal signal)’로 간주한다. 이러한 관점은 전통적인 신호 처리 및 컴퓨터 비전 연구의 철학과 맥을 같이하며, 구조적으로 부드럽고 일관된 움직임을 생성하는 데 강점을 가진다.</p>
<h3>4.3  아키텍처 철학 비교 분석</h3>
<p>Sora와 Lumiere가 제시하는 두 아키텍처는 단순한 기술적 차이를 넘어, 복잡한 시공간 데이터를 모델링하는 방법에 대한 근본적인 철학적 분기를 드러낸다. 이는 ’세상을 이산적인 구성 요소의 조합으로 볼 것인가, 아니면 연속적인 신호의 전체로 볼 것인가’라는 오랜 질문과 맞닿아 있다.</p>
<ul>
<li><strong>Sora (조합적/토큰 기반 접근법):</strong> Sora의 철학은 LLM의 성공 방정식에 깊이 뿌리내리고 있다. LLM이 텍스트를 이산적인 토큰으로 분해하고 그들 사이의 통계적 관계를 학습하여 복잡한 언어 현상을 모델링했듯이, Sora는 시각 세계를 ’시공간 패치’라는 이산적인 토큰으로 분해한다.29 이 접근법의 성공은 세상의 많은 부분이 조합적(compositional)이고 모듈화될 수 있다는 가정을 전제로 한다. 이는 엄청난 유연성과 확장성을 제공하지만, 개별 패치들을 조합하여 장기적으로 일관된 전체를 만들어내는 것이 핵심적인 도전 과제로 남는다.29</li>
<li><strong>Lumiere (전체론적/신호 기반 접근법):</strong> Lumiere는 반대 가설을 제시한다. 특히 ’움직임(motion)’과 같은 시간적 현상은 개별 프레임이나 패치로 나눌 수 없는 전체론적(holistic) 속성을 본질적으로 가진다는 것이다.30 따라서 비디오를 처음부터 끝까지 하나의 덩어리로 처리해야만 진정으로 자연스럽고 일관된 움직임을 포착할 수 있다고 주장한다. 이는 구조적으로 일관성을 보장하는 데 유리하지만, 가변적인 길이의 비디오를 처리하거나 부분적인 편집을 수행하는 데 있어서는 토큰 기반 접근법보다 유연성이 떨어질 수 있다.</li>
</ul>
<p>이 두 철학적 대립은 생성 모델의 미래에 중요한 함의를 가진다. 만약 Sora와 같은 토큰 기반 접근법이 궁극적으로 더 우월하고 확장 가능한 것으로 판명된다면, 미래의 AI는 모든 종류의 데이터(이미지, 비디오, 음성, 3D 모델 등)를 결국 ’토큰화’하여 단일한 거대 트랜스포머 아키텍처로 처리하는 방향으로 수렴할 것이다. Open-Sora와 같은 오픈소스 프로젝트의 등장은 이러한 모듈식 접근법이 커뮤니티에서 더 빠르고 광범위하게 채택될 수 있음을 시사하며, 이는 기술적 우월성과는 별개로 생태계의 승자를 결정하는 중요한 변수가 될 수 있다.27 반면, Lumiere의 전체론적 접근법이 더 효과적이라면, 각 데이터 모달리티가 가진 고유한 구조(예: 시공간 연속성, 기하학적 대칭성)를 존중하는 특화된 아키텍처의 중요성은 계속해서 유지될 것이다. 결국 이 경쟁의 결과는 미래 AI가 세상을 어떻게 인식하고, 이해하며, 생성하는지에 대한 근본적인 방향을 결정하게 될 것이다.</p>
<h2>5.  주요 응용 분야와 도메인 특화 과제</h2>
<p>확산 모델의 강력한 생성 능력은 이미지와 비디오를 넘어, 고도의 전문 지식과 물리적 제약이 요구되는 다양한 도메인으로 빠르게 확장되고 있다. 이 장에서는 확산 모델이 과학적 발견과 예술적 창작이라는 두 가지 대표적인 응용 분야에서 어떻게 활용되고 있으며, 각 도메인의 고유한 특성으로 인해 발생하는 특화된 과제들은 무엇인지 탐구한다. 특히, 성공적인 도메인 적응을 위해서는 알고리즘 자체의 개선만큼이나 해당 분야의 본질을 담아내는 ‘표현(representation)’ 방식의 혁신이 얼마나 중요한지를 중점적으로 논의한다.</p>
<h3>5.1  과학적 발견: 3D 분자 구조 생성</h3>
<p>확산 모델의 가장 유망한 응용 분야 중 하나는 신약 개발(de novo drug design) 및 단백질 공학(protein engineering)이다.3 이 분야의 목표는 단순히 시각적으로 그럴듯한 결과물을 만드는 것을 넘어, 특정 생물학적 기능을 수행하거나 원하는 화학적 특성을 갖는, 물리적으로 타당한 3D 분자 구조를 생성하는 것이다.3</p>
<p>이러한 과학적 생성 과제에서 확산 모델이 직면하는 가장 핵심적인 도전은 **<span class="math math-inline">E(3)</span> 등변성(E(3) Equivariance)**이다.4 3차원 분자의 물리적, 화학적 속성은 3차원 유클리드 공간(Euclidean space)에서의 회전(rotation), 반사(reflection), 병진(translation) 변환에 대해 변하지 않아야 한다. 예를 들어, 분자를 공간상에서 회전시킨다고 해서 그 분자의 약효나 안정성이 변해서는 안 된다. 따라서 생성 모델은 이러한 기하학적 변환에 대해 일관된 출력을 내놓아야 하는데, 이를</p>
<p><span class="math math-inline">E(3)</span> 등변성이라 한다. 이 제약은 선택이 아닌 필수이며, 이를 만족시키기 위해서는 모델 아키텍처 자체에 기하학적 대칭성을 내장해야 한다. 많은 연구들이 기하학적 그래프 신경망(Geometric GNNs) 등을 활용하여 이 등변성 속성을 모델에 부여하고 있다.4</p>
<p>또 다른 중요한 혁신은 <strong>데이터 표현 방식</strong>에서 일어나고 있다. 전통적인 단백질 설계는 20종류의 표준 아미노산 서열을 기반으로 이루어졌다. 그러나 실제 생체 내에서는 비표준 아미노산(non-canonical amino acids)이나 번역 후 변형(Post-Translational Modifications, PTMs)이 단백질의 기능 다양성에 결정적인 역할을 한다.38 최근 연구들은 이러한 한계를 극복하기 위해 SELFIES(Self-Referencing Embedded Strings)와 같은 ‘모든 원자(all-atom)’ 표현법을 도입하고 있다.38 SELFIES는 각 아미노산의 원자 구성을 직접 문자열로 표현함으로써, 표준 아미노산의 경계를 넘어 설계 가능한 분자의 공간을 획기적으로 확장한다. 이처럼 모든 원자 수준에서 분자를 표현하고 이산 확산 모델(discrete diffusion models)을 적용하는 접근법은, 기존에 불가능했던 새로운 기능성 단백질이나 약물 후보 물질을 설계할 수 있는 새로운 가능성을 열고 있다.38</p>
<h3>5.2  창작 도구: 텍스트-음악 및 기타 모달리티</h3>
<p>확산 모델은 과학뿐만 아니라 예술 창작 분야에서도 새로운 도구로 각광받고 있다. 특히 텍스트 설명으로부터 고품질의 음악 오디오를 생성하는 ‘텍스트-음악(Text-to-Music)’ 생성은 활발히 연구되는 분야 중 하나다.1 이 분야는 이미지 생성과는 다른 독특한 과제들을 안고 있다.</p>
<ul>
<li><strong>제어 가능성 (Controllability):</strong> 사용자는 단순히 “슬픈 분위기의 재즈“와 같은 전체적인 묘사뿐만 아니라, “드럼 비트를 더 강하게” 또는 “피아노 멜로디를 바이올린으로 교체“와 같이 음악의 특정 구성 요소(stem)를 세밀하게 편집하기를 원한다.39 이는 모델이 음악의 계층적 구조를 이해하고, 각 요소를 독립적으로 제어할 수 있는 능력을 갖추어야 함을 의미한다.</li>
<li><strong>장기적 일관성 (Long-term Coherence):</strong> 음악은 시간에 따라 전개되는 구조물이다. 수 분에 달하는 긴 음악을 생성할 때, 도입-전개-절정-결말로 이어지는 구조적 통일성을 유지하고, 멜로디나 리듬의 모티프가 일관되게 반복 및 변주되도록 하는 것은 매우 어려운 과제다.41</li>
<li><strong>데이터 및 표현 문제:</strong> 고품질의 음악 오디오와 그에 상응하는 상세한 텍스트 설명이 쌍으로 이루어진 대규모 데이터셋은 여전히 부족하다.41 또한, 음악의 복잡한 구조(화성, 리듬, 템포, 음색 등)를 손실 없이 효과적으로 담아낼 수 있는 디지털 표현 방식을 찾는 것 역시 중요한 연구 주제다.</li>
</ul>
<p>Meta의 MusicGen과 같은 모델들은 이러한 문제에 대응하기 위해 EnCodec과 같은 효율적인 오디오 압축기와 트랜스포머 아키텍처를 결합하는 접근법을 사용한다.39 EnCodec으로 음악을 이산적인 토큰 시퀀스로 변환한 뒤, 트랜스포머가 텍스트 조건에 따라 이 토큰 시퀀스를 생성하도록 학습하는 방식이다.</p>
<p>이 두 응용 분야의 사례는 확산 모델을 새로운 도메인에 성공적으로 적용하는 데 있어 핵심적인 통찰을 제공한다. 그것은 바로 알고리즘 자체의 정교함만큼이나, 혹은 그 이상으로 해당 <strong>도메인의 내재적 구조와 제약을 포착하는 데이터 표현 방식의 중요성</strong>이다. 3D 분자 생성에서 <span class="math math-inline">E(3)</span> 등변성이라는 ‘딱딱한(hard)’ 물리적 제약은 모델의 생존을 결정하는 필수 조건이다. 이를 만족시키지 못하는 모델은 과학적으로 무의미한 결과를 내놓을 뿐이다. 음악 생성에서의 제어 가능성은 물리 법칙처럼 절대적인 제약은 아니지만, 창작 도구로서의 유용성을 결정하는 핵심적인 ‘부드러운(soft)’ 제약이다. 두 경우 모두, 문제 해결의 실마리는 표현 방식의 혁신에서 나왔다. 단백질 설계에서 SELFIES 표현을 사용하자 비표준 아미노산이라는 새로운 창작의 세계가 열렸듯이, 음악에서 스템(stem)별로 분리된 표현이나 계층적 표현을 사용하면 사용자의 제어 가능성이 극대화될 수 있다. 이는 생성 모델의 미래가 단순히 더 크고 강력한 단일 모델을 만드는 데만 있는 것이 아니라, 각 도메인의 고유한 ’언어’를 학습하고 그 ‘문법’(제약 조건)을 존중하는 정교한 표현 체계를 개발하는 데 있음을 강력하게 시사한다. 결국, 어떤 표현을 선택하느냐가 모델이 세상을 인식하고 상호작용하는 방식을 결정하는 것이다.</p>
<h2>6.  종합 고찰 및 미래 전망</h2>
<p>지금까지 본 안내서는 확산 모델의 수학적 기초에서부터 샘플링 가속화, 아키텍처 확장, 그리고 다양한 응용 분야에 이르기까지 그 발전 과정을 다각도로 조명했다. 이 마지막 장에서는 이러한 논의들을 종합하여 확산 모델의 진화 동력을 재정리하고, 현재 패러다임이 가진 근본적인 한계에 대한 비판적 고찰을 통해 차세대 생성 모델이 나아가야 할 방향을 전망하고자 한다.</p>
<h3>6.1  확산 모델의 진화 동력 종합</h3>
<p>확산 모델의 발전사는 세 가지 핵심적인 축-<strong>①생성 속도(Speed)</strong>, <strong>(2)제어 가능성(Controllability)</strong>, <strong>(3)확장성(Scalability)</strong>-을 중심으로 이해할 수 있다. 이 세 가지 요소는 서로 맞물리며 기술의 진화를 이끌어왔다.</p>
<ul>
<li><strong>속도:</strong> SDE/ODE 프레임워크가 제공하는 이론적 기반 위에서, 점진적 증류와 컨시스턴시 모델은 확산 모델의 가장 큰 약점이었던 느린 생성 속도를 극복하고 실시간 생성을 가시권에 두었다. 이는 확산 모델의 실용성을 극대화하고 응용 범위를 넓히는 데 결정적인 기여를 했다.</li>
<li><strong>제어 가능성:</strong> 초기 조건부 생성 모델에서부터 텍스트-음악 편집, 비디오 인페인팅과 같은 정교한 제어 기술에 이르기까지, 사용자의 의도를 생성 과정에 반영하려는 노력은 확산 모델을 단순한 ’샘플 생성기’에서 강력한 ’창작 도구’로 변모시켰다.</li>
<li><strong>확장성:</strong> DiT와 STUNet과 같은 새로운 아키텍처의 등장은 확산 모델이 이미지라는 2D 데이터를 넘어 비디오와 같은 복잡한 시공간 데이터까지 처리할 수 있도록 확장성을 부여했다. 이는 모델이 더 복잡하고 동적인 세계를 모델링할 수 있는 가능성을 열었다.</li>
</ul>
<p>결론적으로, SDE 이론이 단단한 기초를 놓고, 컨시스턴시 모델이 속도의 날개를 달았으며, 다양한 조건부 생성 및 편집 기술이 제어의 키를 쥐어주었고, 혁신적인 아키텍처들이 더 넓은 세계로 나아갈 수 있는 발판을 마련했다. 이 모든 발전이 유기적으로 결합하여 오늘날 우리가 목도하는 강력한 확산 모델 생태계를 구축한 것이다.</p>
<h3>6.2  현재 패러다임의 한계와 비판적 고찰</h3>
<p>이처럼 눈부신 성공에도 불구하고, 현 세대 생성 모델은 근본적인 한계를 안고 있다는 비판에 직면해 있다. Meta의 수석 AI 과학자이자 AI 분야의 거두인 Yann LeCun은 이러한 비판의 중심에 서 있다.42 그의 주장에 따르면, 현재의 생성 모델, 특히 LLM을 포함한 자기회귀적 모델들은 몇 년 안에 구식이 될 것이며, 이는 그들이 가진 본질적인 한계 때문이다.42</p>
<p>LeCun이 지적하는 핵심적인 비판점들은 다음과 같다:</p>
<ul>
<li><strong>물리 세계에 대한 이해 부재:</strong> 현재 모델들은 방대한 데이터로부터 픽셀이나 단어의 통계적 패턴을 학습할 뿐, 그 이면에 있는 물리 법칙, 인과관계, 공간적 제약, 상식 등 실제 세계가 작동하는 방식에 대한 진정한 이해(understanding)가 없다.42 Sora가 보여주는 놀라운 ‘세계 시뮬레이션’ 능력조차, LeCun의 관점에서는 매우 정교하고 고차원적인 패턴 모방(pattern matching)과 보간(interpolation)일 뿐, 근본적인 세계 모델링과는 거리가 멀 수 있다.29 예를 들어, 모델은 유리가 깨지는 수많은 영상을 보고 그럴듯한 파편을 생성할 수는 있지만, ’깨지기 쉬움’이라는 속성이나 중력의 법칙을 내재적으로 이해하고 있지는 않다.</li>
<li><strong>추론 및 계획 능력의 한계:</strong> 현재 모델들은 주어진 프롬프트에 따라 그럴듯한 결과물을 ‘한 번에’ 생성하는 데는 능하지만, 복잡한 목표를 달성하기 위해 여러 단계를 거쳐 논리적으로 추론하고 행동을 계획하는 능력은 현저히 부족하다.42</li>
<li><strong>지속적인 기억의 부재:</strong> 모델들은 훈련 데이터에 기반한 정적인 지식을 가지고 있을 뿐, 세상과 지속적으로 상호작용하며 새로운 경험을 통해 자신의 지식을 업데이트하는 동적인 기억 메커니즘을 갖추고 있지 않다.42</li>
</ul>
<p>이러한 비판은 현재 생성 모델의 성공을 폄하하려는 것이 아니라, 연구의 최종 목표를 ’더 그럴듯한 생성’에서 ’더 깊은 세계 이해’로 재설정해야 한다는 근본적인 문제 제기이다.</p>
<h3>6.3  차세대 생성 모델을 향하여: 월드 모델</h3>
<p>LeCun과 같은 비판가들이 제시하는 미래 AI의 비전은 바로 **‘월드 모델(World Models)’**이다.42 월드 모델은 단순히 관찰된 데이터를 모방하여 생성하는 것을 넘어, 관찰과 상호작용을 통해 세상이 어떻게 작동하는지에 대한 내재적인 모델(internal model)을 스스로 구축하는 시스템을 의미한다. 이러한 내재적 모델을 갖춘 AI는 다음과 같은 능력을 가질 수 있다.</p>
<ol>
<li><strong>예측 (Prediction):</strong> 현재 상태와 특정 행동이 주어졌을 때, 미래에 어떤 일이 일어날지 시뮬레이션하고 예측한다.</li>
<li><strong>추론 (Reasoning):</strong> 관찰되지 않은 변수나 인과관계를 추론한다.</li>
<li><strong>계획 (Planning):</strong> 주어진 목표를 달성하기 위해 자신의 행동이 가져올 결과를 예측하고, 최적의 행동 순서를 계획한다.</li>
</ol>
<p>이러한 월드 모델의 관점에서 보면, 본 안내서에서 논의된 확산 모델의 모든 발전은 그 자체로 최종 목표가 아니라, 진정한 월드 모델을 구축하기 위한 필수적인 부품(component)들을 개발하는 과정으로 재해석될 수 있다.</p>
<ul>
<li><strong>SDE 기반 확산 모델</strong>은 월드 모델이 상상하고 예측한 미래를 인간이 이해할 수 있도록 시각화해주는 고품질 **‘렌더링 엔진’**의 역할을 할 수 있다.</li>
<li><strong>컨시스턴시 모델</strong>은 월드 모델이 실시간으로 세상과 상호작용하고 빠르게 예측을 수행하기 위한 **‘고속 연산 모듈’**을 제공한다.</li>
<li><strong>Sora와 Lumiere</strong>와 같은 시공간 아키텍처는 월드 모델이 동적인 세계를 인식하고(입력) 시뮬레이션 결과(출력)를 생성하기 위한 **‘시각 인터페이스’**의 기반이 된다.</li>
<li><strong><span class="math math-inline">E(3)</span> 등변성 모델</strong>은 월드 모델이 물리 법칙과 같은 세상의 **‘기본 규칙’**을 존중하도록 강제하는 중요한 제약 조건 모듈이다.</li>
</ul>
<p>이러한 재해석은 신약 개발, 비디오 생성, 샘플링 가속화 등 개별적으로 보이던 연구 분야들을 ’지능의 본질에 다가간다’는 하나의 거대한 청사진 아래 통합한다. 이들은 더 이상 별개의 문제가 아니라, 세상을 이해하고 예측하며 행동하는 지능적인 에이전트를 구축하기 위한 상호 연결된 퍼즐 조각이 된다.</p>
<p>따라서 확산 모델의 미래는 단순히 더 사실적인 이미지나 더 긴 비디오를 생성하는 것을 넘어, 지금까지 개발된 강력한 생성 능력과 속도, 확장성, 그리고 제어 가능성을 어떻게 유기적으로 통합하여, 세상을 내재적으로 이해하고 상호작용할 수 있는 차세대 AI, 즉 월드 모델을 구축할 것인가라는 더 원대한 질문으로 귀결될 것이다. 현재의 확산 모델은 그 위대한 여정의 중요한 첫걸음이자, 앞으로 만들어질 지능의 핵심 구성 요소로 자리매김할 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>How AI Text-to-Music is Rewiring the Industry’s Creative DNA | by Myk Eff - Medium, https://medium.com/ai-music/how-ai-text-to-music-is-rewiring-the-industrys-creative-dna-3c43b9dc1f86</li>
<li>Consistency Models - arXiv, http://arxiv.org/pdf/2303.01469</li>
<li>Diffusion Models in De Novo Drug Design - PubMed, https://pubmed.ncbi.nlm.nih.gov/39322943/</li>
<li>Diffusion Models in De Novo Drug Design - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC11481093/</li>
<li>Score based generative modeling through stochastic … - arXiv, https://arxiv.org/abs/2011.13456</li>
<li>Score based generative modeling through stochastic differential equations - arXiv, https://arxiv.org/pdf/2011.13456</li>
<li>Consistency Models - arXiv, https://arxiv.org/abs/2303.01469</li>
<li>Track: Outstanding Paper Session 1 - ICLR 2026, https://iclr.cc/virtual/2021/session/4366</li>
<li>Consistency Models - ResearchGate, https://www.researchgate.net/publication/368935569_Consistency_Models</li>
<li>PROGRESSIVE DISTILLATION FOR FAST SAMPLING OF DIFFUSION MODELS - OpenReview, https://openreview.net/references/pdf?id=thGUcAtQmU</li>
<li>Progressive Distillation for Fast Sampling of Diffusion Models - OpenReview, https://openreview.net/forum?id=TIdIXIpzhoI</li>
<li>Progressive Distillation for Fast Sampling of Diffusion Models, https://arxiv.org/abs/2202.00512</li>
<li>SCORE-BASED GENERATIVE MODELING … - OpenReview, https://openreview.net/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf?ref=news-tutorials-ai-research</li>
<li>Score-based Generative Models based on SDEs/ODEs - Jakub M. Tomczak, https://jmtomczak.github.io/blog/17/17_sbgms.html</li>
<li>Revision History for Score-Based Generative Modeling… - OpenReview, https://openreview.net/revisions?id=PxTIG12RRHS</li>
<li>Score-based Generative Modeling Through Backward Stochastic Differential Equations - arXiv, https://arxiv.org/pdf/2304.13224</li>
<li>Score-Based Generative Modeling through Stochastic Differential Equations - OpenReview, https://openreview.net/forum?id=PxTIG12RRHS</li>
<li>Simple and Fast Distillation of Diffusion Models, https://neurips.cc/media/neurips-2024/Slides/96231.pdf</li>
<li>Progressive Distillation for Fast Sampling of Diffusion Models - ResearchGate, https://www.researchgate.net/publication/358291518_Progressive_Distillation_for_Fast_Sampling_of_Diffusion_Models</li>
<li>Consistency Models - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v202/song23a/song23a.pdf</li>
<li>Consistency Models: One-Step Image Generation - Paperspace Blog, https://blog.paperspace.com/consistency-models/</li>
<li>Yang Song, Prafulla Dhariwal, Mark Chen, Ilya Sutskever OpenAI Generation is SLOW!, https://yang-song.net/assets/pdf/ICML2023/consistency.pdf</li>
<li>publications | Yang Song, https://yang-song.net/publications/</li>
<li>Improving Consistency Models with Generator-Augmented Flows - ICML 2025, https://icml.cc/virtual/2025/poster/45833</li>
<li>[2310.14189] Improved Techniques for Training Consistency Models - arXiv, https://arxiv.org/abs/2310.14189</li>
<li>[2403.06807] Multistep Consistency Models - arXiv, https://arxiv.org/abs/2403.06807</li>
<li>hpcaitech/Open-Sora: Open-Sora: Democratizing Efficient Video Production for All - GitHub, https://github.com/hpcaitech/Open-Sora</li>
<li>OpenAI Sora’s Technical Review - Jianing Qi, https://j-qi.medium.com/openai-soras-technical-review-a8f85b44cb7f</li>
<li>Video generation models as world simulators | OpenAI, https://openai.com/index/video-generation-models-as-world-simulators/</li>
<li>Lumiere: A Space-Time Diffusion Model for Video Generation - arXiv, https://arxiv.org/html/2401.12945v1</li>
<li>[2401.12945] Lumiere: A Space-Time Diffusion Model for Video Generation - arXiv, https://arxiv.org/abs/2401.12945</li>
<li>How to Use Lumiere - Google AI Video Generator - Fliki, https://fliki.ai/blog/how-to-use-lumiere</li>
<li>Google’s Lumiere Brings AI Video Closer to Real than Unreal, https://www.analyticsvidhya.com/blog/2024/01/googles-introduces-video-generation-model-lumiere/</li>
<li>Introducing LUMIERE: The Future of AI-Powered Video Generation - Admazes, https://admazes.com/blog/introducing-lumiere-the-future-of-ai-powered-video-generation/</li>
<li>Diffusion models for protein design - HITS - Heidelberg Institute for Theoretical Studies, https://www.h-its.org/projects/diffusion_models/</li>
<li>[2501.02680] From thermodynamics to protein design: Diffusion models for biomolecule generation towards autonomous protein engineering - arXiv, https://arxiv.org/abs/2501.02680</li>
<li>Diffusion Models in De Novo Drug Design - PMC, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11481093/</li>
<li>All-Atom Protein Sequence Design using Discrete Diffusion Models …, https://www.biorxiv.org/content/10.1101/2025.06.13.659451v1.full-text</li>
<li>Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning - arXiv, https://arxiv.org/html/2405.18386v3</li>
<li>Text-to-Music Generation | Papers With Code, <a href="https://paperswithcode.com/task/text-to-music-generation?page=3&amp;q">https://paperswithcode.com/task/text-to-music-generation?page=3&amp;q=</a></li>
<li>AI-Enabled Text-to-Music Generation: A Comprehensive Review of Methods, Frameworks, and Future Directions - MDPI, https://www.mdpi.com/2079-9292/14/6/1197</li>
<li>Meta’s Chief AI Scientist Yann LeCun Questions the Longevity of …, https://www.hpcwire.com/2025/02/11/metas-chief-ai-scientist-yann-lecun-questions-the-longevity-of-current-genai-and-llms/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>