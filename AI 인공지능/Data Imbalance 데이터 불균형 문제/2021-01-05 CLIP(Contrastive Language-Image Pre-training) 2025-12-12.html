<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:CLIP 자연어 지도를 통한 전이 가능한 시각적 모델 학습 (2021-01-05)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>CLIP 자연어 지도를 통한 전이 가능한 시각적 모델 학습 (2021-01-05)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">데이터 불균형 문제 (Data Imbalance Problem)</a> / <span>CLIP 자연어 지도를 통한 전이 가능한 시각적 모델 학습 (2021-01-05)</span></nav>
                </div>
            </header>
            <article>
                <h1>CLIP 자연어 지도를 통한 전이 가능한 시각적 모델 학습 (2021-01-05)</h1>
<h2>1.  서론: 컴퓨터 비전의 패러다임 전환과 새로운 지평</h2>
<p>인공지능(AI), 특히 컴퓨터 비전(Computer Vision) 분야는 지난 10년간 심층 신경망(Deep Neural Networks)의 도입으로 비약적인 발전을 이룩했다. 2012년 ImageNet 대회(ILSVRC)에서 AlexNet이 등장한 이래, 합성곱 신경망(CNN)은 이미지 분류, 객체 탐지, 분할 등 다양한 시각적 인식 작업에서 인간의 능력을 상회하는 성능을 달성해왔다. 그러나 이러한 성취의 이면에는 근본적인 한계가 존재했다. 주류 컴퓨터 비전 시스템은 고정된 범주(Category)의 집합을 예측하도록 훈련된 지도 학습(Supervised Learning) 모델에 전적으로 의존해 왔다는 점이다.</p>
<p>전통적인 지도 학습 패러다임에서 모델은 사전에 정의된 레이블(Label)의 닫힌 집합(Closed Set) 내에서만 작동한다. 예를 들어, 1,000개의 클래스를 가진 ImageNet 데이터셋으로 훈련된 모델은 1,001번째 새로운 객체를 인식할 수 없다. 새로운 시각적 개념을 모델에 주입하기 위해서는 수천 장의 이미지를 추가로 수집하고, 전문가 인력을 동원하여 정답을 레이블링(Labeling)한 뒤, 모델을 재학습(Re-training)시키는 고비용, 저효율의 과정을 거쳐야 한다. 이러한 방식은 모델의 일반화(Generalization) 능력과 확장성(Scalability)을 심각하게 제한하는 병목(Bottleneck)으로 작용해왔다.1 또한, 기존 모델들은 훈련 데이터와 동일한 분포(IID)를 가진 테스트 데이터에서는 뛰어난 성능을 보이지만, 카메라의 각도, 조명, 스타일(예: 실사에서 스케치로의 변화) 등 데이터 분포가 조금만 달라져도 성능이 급격히 하락하는 ‘강건성 격차(Robustness Gap)’ 문제를 안고 있었다.3</p>
<p>2021년 OpenAI가 공개한 **CLIP(Contrastive Language-Image Pre-training)**은 이러한 기존 컴퓨터 비전의 한계를 근본적으로 타파하고, 시각적 표현 학습(Visual Representation Learning)의 새로운 패러다임을 제시하였다. CLIP은 레이블이 지정된 고정된 데이터셋 대신, 인터넷상에 풍부하게 존재하는 이미지와 텍스트 쌍(Image-Text Pairs)을 원시 상태(Raw Data)로 활용한다. 4억 개(400 million)에 달하는 방대한 이미지-텍스트 쌍을 통해, CLIP은 자연어(Natural Language)로부터 시각적 개념을 직접 학습하는 방식을 채택했다.1</p>
<p>본 보고서는 CLIP 모델의 기술적 구조, 학습 방법론, 성능 평가, 한계점, 그리고 현대 생성형 AI(Generative AI) 생태계에 미친 파급 효과를 포괄적이고 심층적으로 분석한다. 특히, 기존의 분류기(Classifier) 기반 접근법과 CLIP의 대조 학습(Contrastive Learning) 접근법이 어떻게 이론적으로 다른지, 그리고 이것이 어떻게 제로샷(Zero-shot) 성능의 혁신을 가져왔는지에 대해 수집된 연구 자료를 바탕으로 상세히 기술한다.</p>
<h2>2.  연구 배경 및 기존 방법론의 구조적 한계</h2>
<h3>2.1  지도 학습 기반 비전 시스템의 태생적 한계</h3>
<p>전통적인 딥러닝 비전 모델(예: ResNet, Inception, EfficientNet)은 ’원-핫 인코딩(One-hot Encoding)’된 레이블을 정답으로 사용하여 학습된다. 이 과정에서 레이블 ’강아지(Dog)’는 단순히 숫자 인덱스(예: 클래스 ID 342)로 취급되며, 모델은 ’강아지’라는 단어가 가지는 의미론적 정보(예: 포유류이다, 짖는다, 털이 있다)를 전혀 학습하지 못한다. 이는 모델이 시각적 특징을 단순히 임의의 숫자 레이블에 매핑하는 기계적인 패턴 매칭(Pattern Matching)에 그치게 만든다. 결과적으로, 모델은 의미론적 이해(Semantic Understanding)가 결여되어 있으며, 이는 새로운 개념으로의 확장을 불가능하게 만든다. 새로운 개념을 배우기 위해서는 출력 계층(Output Layer)을 수정하고 전체 파라미터를 다시 최적화해야 하는 비효율성이 발생한다.2</p>
<h3>2.2  자연어 처리(NLP) 분야의 혁신과 교훈</h3>
<p>반면, 자연어 처리 분야는 Transformer 아키텍처의 도입과 함께 혁명적인 변화를 겪었다. GPT-3, BERT, T5와 같은 거대 언어 모델(LLM)들은 웹 텍스트 데이터에서 자기 지도 학습(Self-Supervised Learning)을 통해 언어의 구조와 지식을 학습한다. 이러한 모델들은 “작업에 구애받지 않는(Task-Agnostic)” 구조를 가지며, 별도의 미세 조정(Fine-tuning) 없이도 자연어 지시(Prompt)만으로 번역, 요약, 질의응답 등 다양한 태스크를 수행하는 ‘제로샷(Zero-shot)’ 또는 ‘퓨샷(Few-shot)’ 능력을 보여주었다.3</p>
<p>CLIP 연구진은 NLP 분야의 이러한 성공 요인, 즉 “자연어를 훈련 신호(Supervision Signal)로 활용하는 것“을 컴퓨터 비전에 도입하고자 했다. 자연어는 이미지의 내용을 설명하는 가장 풍부하고 유연한 형식이므로, 이를 통해 모델은 시각적 개념과 언어적 개념을 동시에 학습하고 연결할 수 있다.</p>
<h3>2.3  선행 연구 및 CLIP의 위치</h3>
<p>CLIP 이전에도 이미지와 텍스트를 결합하려는 시도는 지속적으로 존재했다.</p>
<ul>
<li><strong>VirTex (Visual Representations from Textual Annotations):</strong> 이미지 캡셔닝(Captioning) 작업을 통해 시각적 표현을 학습하고자 했다. 텍스트를 생성(Autoregressive)하는 방식은 모델이 언어 구조를 깊이 이해하게 하지만, 계산 비용이 매우 높고 이미지 분류와 같은 단순 작업에는 과잉(Overkill)일 수 있다.3</li>
<li><strong>ICMLM (Image-Conditioned Masked Language Modeling):</strong> BERT와 유사하게 마스킹된 단어를 복원하는 방식으로 시각-언어 관계를 학습했다.3</li>
<li><strong>ConVIRT (Contrastive Visual Representation Learning from Text):</strong> 의료 영상 분야에서 이미지와 진단 보고서 간의 대조 학습을 수행했다. CLIP의 대조 학습 목표는 ConVIRT의 방법론을 일반 도메인의 거대 웹 데이터로 확장한 것으로 볼 수 있다.3</li>
</ul>
<p>기존 연구들이 데이터셋의 규모 한계(예: COCO Captions의 10만~30만 장 수준)나 복잡한 훈련 목표(캡션 생성 등)로 인해 일반화된 성능을 달성하지 못했다면, CLIP은 **단순화된 대조 학습 목표(Simplified Contrastive Objective)**와 **웹 스케일의 데이터(Web-scale Data)**를 결합하여 이를 극복했다.</p>
<h2>3.  핵심 방법론: 대조 학습 (Contrastive Learning)의 이론과 구현</h2>
<p>CLIP의 가장 핵심적인 혁신은 학습 목표(Training Objective)의 전환에 있다. 기존의 생성적(Generative) 또는 예측적(Predictive) 모델들이 텍스트의 정확한 단어 시퀀스를 예측하려고 시도했던 것과 달리, CLIP은 이미지와 텍스트가 “서로 짝이 맞는가?“를 판별하는 대조적(Contrastive) 문제를 푼다.</p>
<h3>3.1  예측 목표 대 대조 목표의 효율성 비교</h3>
<p>초기 실험에서 OpenAI 연구진은 이미지 캡션 생성(Image Captioning) 모델을 훈련하여 시각적 표현을 학습시키려 시도했다. 그러나 이 방식은 매우 비효율적이었다. 이미지는 하나의 텍스트로만 묘사될 수 있는 것이 아니며, 동일한 이미지에 대해 수많은 유효한 설명이 존재할 수 있다. 모델이 정확한 단어 하나하나를 예측하도록 강제하는 것은 불필요하게 어려운 문제를 푸는 것과 같다.</p>
<p>반면, 대조 학습은 임베딩 공간(Embedding Space)에서 짝이 되는 이미지와 텍스트의 벡터를 가깝게 당기고, 짝이 아닌 것들은 밀어내는 방식이다. 연구진의 실험 결과, 대조 학습 방식은 캡션 생성 방식(Bag-of-Words 예측 또는 Transformer 언어 모델링) 대비 제로샷 전이 효율(Rate of Zero-shot Transfer)을 4배 이상 향상시켰다.5 이는 동일한 컴퓨팅 자원으로 훨씬 더 높은 성능의 모델을 학습할 수 있음을 의미한다.</p>
<h3>3.2  수식적 모델링: InfoNCE와 대칭적 교차 엔트로피 손실</h3>
<p>CLIP의 학습 과정을 수학적으로 상세히 분석해 보자.</p>
<p><span class="math math-inline">N</span>개의 이미지-텍스트 쌍 <span class="math math-inline">{(I_1, T_1), (I_2, T_2), \dots, (I_N, T_N)}</span>으로 구성된 미니 배치(Mini-batch)가 있다고 가정한다. 여기서 <span class="math math-inline">I_i</span>는 <span class="math math-inline">i</span>번째 이미지, <span class="math math-inline">T_i</span>는 <span class="math math-inline">i</span>번째 텍스트이다.</p>
<p>CLIP은 <span class="math math-inline">N \times N</span> 크기의 유사도 행렬을 생성한다. 이 행렬의 대각선 성분인 <span class="math math-inline">N</span>개의 쌍 <span class="math math-inline">(I_i, T_i)</span>는 긍정 쌍(Positive Pairs)이며, 나머지 <span class="math math-inline">N^2 - N</span>개의 비대각선 성분 <span class="math math-inline">(I_i, T_j) (i \neq j)</span>는 부정 쌍(Negative Pairs)이다.</p>
<p>모델의 목표는 긍정 쌍의 코사인 유사도(Cosine Similarity)를 최대화하고, 부정 쌍의 유사도를 최소화하는 것이다. 이를 위해 대칭적 교차 엔트로피(Symmetric Cross Entropy) 손실 함수를 사용한다. 이는 이미지 관점에서의 손실 <span class="math math-inline">\mathcal{L}*{I \to T}</span>와 텍스트 관점에서의 손실 <span class="math math-inline">\mathcal{L}*{T \to I}</span>의 합으로 정의된다.</p>
<p>이미지 <span class="math math-inline">I_i</span>에 대한 손실 함수 <span class="math math-inline">\ell_i^{(I \to T)}</span>는 다음과 같이 Softmax 형태를 띤다 (이는 InfoNCE 손실과 동일하다):</p>
<p><span class="math math-display">
\ell_i^{(I \to T)} = -\log \frac{\exp(\text{sim}(I_i, T_i) / \tau)}{\sum_{j=1}^{N} \exp(\text{sim}(I_i, T_j) / \tau)}
</span><br />
여기서:</p>
<ul>
<li><span class="math math-inline">\text{sim}(u, v) = \frac{u \cdot v}{|u| |v|}</span>는 두 벡터 <span class="math math-inline">u, v</span> 간의 코사인 유사도이다. CLIP은 임베딩 벡터를 L2 정규화(L2-normalization)하므로, 단순히 내적(Dot Product) <span class="math math-inline">u \cdot v</span>와 같다.</li>
<li><span class="math math-inline">\tau</span>는 온도(Temperature) 매개변수이다.</li>
</ul>
<p>동일하게 텍스트 <span class="math math-inline">T_j</span>에 대한 손실 함수 <span class="math math-inline">\ell_j^{(T \to I)}</span>는 다음과 같다:</p>
<p><span class="math math-display">
\ell_j^{(T \to I)} = -\log \frac{\exp(\text{sim}(T_j, I_j) / \tau)}{\sum_{k=1}^{N} \exp(\text{sim}(T_j, I_k) / \tau)}
</span><br />
최종적인 전체 배치의 손실 <span class="math math-inline">\mathcal{L}</span>은 두 손실의 평균이다:<br />
<span class="math math-display">
\mathcal{L} = \frac{1}{2} \left( \frac{1}{N} \sum_{i=1}^{N} \ell_i^{(I \to T)} + \frac{1}{N} \sum_{j=1}^{N} \ell_j^{(T \to I)} \right)
</span><br />
이 손실 함수는 PyTorch와 같은 프레임워크에서 <code>CrossEntropyLoss</code>를 사용하여 효율적으로 구현될 수 있다. 유사도 행렬 자체가 로짓(Logit) 역할을 하며, 행(Row) 방향과 열(Column) 방향으로 각각 교차 엔트로피를 계산하여 평균을 내는 구조이다.5</p>
<h3>3.3  온도 매개변수 (Temperature Parameter)의 역할과 최적화</h3>
<p>온도 매개변수 <span class="math math-inline">\tau</span>는 대조 학습에서 매우 중요한 역할을 한다. 온도가 낮을수록 분포가 뾰족해져(Peaked) 모델이 가장 유사한 부정 샘플(Hard Negatives)을 구분하는 데 집중하게 되며, 온도가 높으면 분포가 평탄해져(Smoothed) 모든 부정 샘플을 고르게 밀어내게 된다.</p>
<p>SimCLR나 MoCo 같은 기존 연구들은 <span class="math math-inline">\tau</span>를 하이퍼파라미터로 고정하여 사용했으나, CLIP은 이를 학습 가능한 파라미터(Learnable Parameter)로 설정했다. 논문에서는 수치적 안정성을 위해 로그 공간에서 <span class="math math-inline">\tau</span>를 학습시키며, 구체적으로 <span class="math math-inline">T = e^{-\tau}</span> 형태로 파라미터화하였다.7 이는 모델이 훈련 과정에서 데이터의 특성에 맞춰 최적의 스케일링 값을 스스로 찾도록 유도하며, 학습 초기에는 큰 값에서 시작하여 점차 수렴하는 경향을 보인다. <span class="math math-inline">\tau</span>는 로짓 값을 스케일링하여 Softmax 함수의 그래디언트 전파가 원활하게 이루어지도록 돕는다.</p>
<h3>3.4  대규모 배치 크기(Batch Size)의 중요성</h3>
<p>CLIP은 32,768이라는 매우 큰 배치 크기를 사용하여 훈련되었다.3 대조 학습에서 배치 크기는 단순히 병렬 처리를 위한 것이 아니다. 배치 크기 <span class="math math-inline">N</span>이 커질수록 각 샘플은 더 많은 부정 샘플(<span class="math math-inline">N-1</span>개)과 비교된다. 이는 모델이 더 정교한 차이(Fine-grained differences)를 학습하고, 임베딩 공간을 더 조밀하게 채우는 데 필수적이다. 부정 샘플의 수가 많아야만 대조 학습의 “사전(Dictionary)” 크기가 커져 학습 효율이 극대화된다는 이론적 배경(MoCo 논문 등에서 제시됨)을 따른 것이다.</p>
<h2>4.  모델 아키텍처 (Model Architecture) 심층 분석</h2>
<p>CLIP은 비전(Vision)과 언어(Language)라는 두 가지 모달리티를 처리하기 위해, 각각 독립적인 인코더를 가지는 <strong>이중 인코더(Dual-Encoder)</strong> 구조를 채택했다.</p>
<h3>4.1  이미지 인코더 (Image Encoder): ResNet과 ViT의 진화</h3>
<p>연구진은 다양한 규모의 이미지 인코더를 실험하여 확장성(Scalability)을 검증했다. 크게 ResNet 계열과 Vision Transformer (ViT) 계열로 나뉜다.</p>
<h4>4.1.1  ResNet 기반 모델 (ResNet-50 및 변형)</h4>
<p>CLIP의 ResNet 인코더는 표준 ResNet-50에서 몇 가지 중요한 수정이 가해졌다.</p>
<ul>
<li><strong>ResNet-D 개선안 적용:</strong> 입력단의 줄기(Stem) 부분 등에서 He et al. (2019)의 “Bag of Tricks“에 소개된 ResNet-D 구조를 채택하여, 다운샘플링 시 정보 손실을 최소화하고 성능을 개선했다. 안티 앨리어싱(Anti-aliasing) 풀링(Rect-2)도 적용되었다.3</li>
<li><strong>어텐션 풀링 (Attention Pooling):</strong> 가장 큰 변화는 마지막 단계인 전역 평균 풀링(Global Average Pooling) 계층을 **어텐션 풀링(Attention Pooling)**으로 대체한 것이다. ResNet의 마지막 합성곱 층 출력 특징 맵(Feature Map)에 대해, Transformer 스타일의 “Multi-Head Self-Attention (MHSA)“을 수행한다. 구체적으로, 전역 평균 풀링을 수행하는 대신, 쿼리(Query)가 입력 특징의 전역 컨텍스트를 조건부로 하여 정보를 집계하도록 설계되었다.8 이는 이미지가 단순히 공간적으로 평균화되는 것을 넘어, 의미론적으로 중요한 영역에 가중치를 두어 정보를 압축하도록 돕는다.</li>
<li><strong>모델 스케일링:</strong> 기본 ResNet-50 외에도, 필터의 폭(Width)을 늘린 RN50x4, RN50x16, RN50x64 모델을 훈련했다. RN50x64의 경우 일반적인 ResNet-50보다 64배 더 많은 연산량을 가진 거대 모델이다.</li>
</ul>
<h4>4.1.2  Vision Transformer (ViT) 기반 모델</h4>
<p>최근의 SOTA(State-of-the-Art) 성능은 대부분 ViT 계열에서 나온다. ViT는 이미지를 <span class="math math-inline">P \times P</span> 크기의 패치(Patch)로 분할하고, 이를 1차원 시퀀스로 평탄화(Flatten)한 뒤 Transformer에 입력하는 방식이다.</p>
<ul>
<li><strong>구조적 단순성:</strong> ViT는 CNN 특유의 귀납적 편향(Inductive Bias, 예: 지역성, 평행 이동 불변성)이 적어 데이터가 충분할 때 더 높은 확장성을 보인다. CLIP은 4억 장의 데이터를 사용하므로 ViT의 잠재력을 최대한 끌어낼 수 있다.</li>
<li><strong>수정 사항:</strong> 원본 ViT 구현에 더해, 패치 임베딩과 위치 임베딩(Positional Embedding)을 합친 후 Transformer 입력 직전에 Layer Normalization을 추가했다.5</li>
<li><strong>모델 종류:</strong> ViT-B/32, ViT-B/16, ViT-L/14 등이 있으며, 뒤의 숫자는 패치 크기를 의미한다. 패치 크기가 작을수록(예: 14x14) 시퀀스 길이가 길어지고 계산량이 늘어나지만 성능은 향상된다. 가장 강력한 모델 중 하나인 <strong>ViT-L/14@336px</strong>는 336x336 고해상도 이미지를 입력으로 받아 최고의 성능을 기록했다.9</li>
</ul>
<h3>4.2  텍스트 인코더 (Text Encoder): Transformer</h3>
<p>텍스트 인코더는 GPT-2와 유사한 디코더 온리(Decoder-only) Transformer 구조를 기반으로 한다.</p>
<ul>
<li><strong>아키텍처:</strong> 63M 파라미터, 12개 레이어, 512 차원의 폭, 8개의 어텐션 헤드를 가진 모델을 기본으로 한다.5</li>
<li><strong>토큰화 (Tokenization):</strong> 대소문자를 구분하지 않는(Lower-cased) BPE(Byte Pair Encoding)를 사용하며, 어휘 크기(Vocab Size)는 49,152이다.</li>
<li><strong>시퀀스 처리:</strong> 계산 효율성을 위해 최대 입력 길이는 76 토큰으로 제한된다. 텍스트 시퀀스의 시작과 끝에 <code>와 </code> 토큰을 추가하며, `` 토큰 위치의 마지막 레이어 출력을 해당 텍스트의 전체 특징 벡터로 사용한다. 이 벡터는 이후 Layer Normalization을 거쳐 임베딩 공간으로 투영된다.</li>
<li><strong>마스킹 된 자기 어텐션 (Masked Self-Attention):</strong> CLIP의 텍스트 인코더는 양방향(Bidirectional) BERT와 달리, GPT처럼 마스킹 된 어텐션을 사용하여 이전 토큰들만 볼 수 있는 구조를 취했으나, 훈련 목표가 생성(Generation)이 아니므로 이는 구조적 선택일 뿐, 실제로는 전체 문장을 인코딩하여 마지막 토큰에서 정보를 취합하는 방식으로 작동한다.</li>
</ul>
<h3>4.3  임베딩 공간의 정렬과 투영 (Projection)</h3>
<p>이미지 인코더의 출력 벡터와 텍스트 인코더의 출력 벡터는 차원이 다를 수 있다. 따라서 두 인코더의 마지막에 선형 투영(Linear Projection) 계층 <span class="math math-inline">W_i</span>와 <span class="math math-inline">W_t</span>를 두어, 두 벡터를 공통된 임베딩 차원 <span class="math math-inline">d_e</span>로 매핑한다.</p>
<p>중요한 점은 이 투영된 벡터들이 L2 정규화(L2 Normalization) 된다는 것이다. 즉, 벡터의 크기(Magnitude)는 무시되고 방향(Direction)만이 남게 된다. 이는 코사인 유사도 계산 시 내적(Dot Product)만으로 계산이 가능하게 하여 연산 속도를 높이고, 학습의 안정성을 보장한다.5</p>
<h2>5.  데이터셋: WebImageText (WIT)의 구축과 의미</h2>
<p>CLIP 성능의 원천은 모델 구조보다 데이터에 있다 해도 과언이 아니다. 기존 데이터셋(ImageNet, COCO)은 수십만~백만 장 규모에 불과하며 엄격한 품질 관리를 거쳤지만, CLIP은 “양(Quantity)이 곧 질(Quality)“이라는 가설을 증명하기 위해 웹 스케일의 데이터를 구축했다.</p>
<h3>5.1  데이터 수집 프로세스 및 규모</h3>
<p>연구진은 인터넷의 다양한 소스에서 수집한 4억 쌍(400 Million pairs)의 이미지-텍스트 데이터셋을 구축하여 **WIT (WebImageText)**라 명명했다.1</p>
<ul>
<li><strong>수집 전략:</strong> 무작위 크롤링 대신, 50만 개의 쿼리(Query) 목록을 먼저 생성하고, 이 쿼리에 해당하는 이미지를 검색하는 방식을 사용했다. 쿼리에는 위키피디아에서 자주 등장하는 모든 단어(100회 이상 출현), WordNet의 동의어 집합(Synset), 그리고 흥미로운 바이그램(Bigrams) 등이 포함되었다.5</li>
<li><strong>데이터 밸런싱:</strong> 특정 개념(예: 인기 연예인, 흔한 사물)이 지나치게 많이 포함되는 것을 방지하기 위해, 각 쿼리당 포함될 수 있는 이미지 수를 최대 2만 개로 제한했다. 이는 데이터 분포를 평탄화하여 롱테일(Long-tail) 개념들도 충분히 학습될 수 있도록 보장한다.</li>
</ul>
<h3>5.2  데이터 필터링과 품질 관리</h3>
<p>웹 데이터는 노이즈가 많으므로 전처리 과정이 필수적이다. 연구진은 텍스트가 영어로 되어 있지 않은 경우 제외하였으며, 불완전하거나 의미 없는 텍스트도 필터링했다. 그러나 ImageNet과 달리 사람이 일일이 검수하는 과정은 없었으며, 이는 데이터셋에 내재된 편향과 오류가 그대로 모델에 반영될 수 있음을 의미한다.</p>
<h3>5.3  데이터 편향(Bias)과 윤리적 쟁점</h3>
<p>최근 분석에 따르면, CLIP의 데이터 필터링 방식은 서구권 중심적 편향을 가지고 있다.11</p>
<ul>
<li><strong>지리적/문화적 편향:</strong> 영어 텍스트를 기반으로 필터링했기 때문에 서구권 문화와 관련된 이미지가 압도적으로 많다. 이는 비서구권 문화의 결혼식 복장이나 음식 등을 인식하는 데 성능 저하를 일으킬 수 있다.</li>
<li><strong>인구통계학적 편향:</strong> 특정 인종, 성별, 연령대에 대한 데이터 불균형이 존재하며, 이는 모델이 특정 그룹을 더 잘 인식하거나 잘못된 고정관념(Stereotype)을 학습하는 원인이 된다. 예를 들어, NSFW 필터링이 불완전하여 부적절한 콘텐츠가 학습되었을 가능성도 배제할 수 없다.</li>
</ul>
<h2>6.  제로샷 전이 (Zero-Shot Transfer)의 메커니즘과 혁신</h2>
<p>CLIP의 가장 강력한 기능인 ’제로샷 전이’는 훈련 과정에서 한 번도 본 적 없는 데이터셋의 클래스를 분류할 수 있는 능력을 말한다. 이는 CLIP이 “이미지와 텍스트의 연결 고리“를 학습했기에 가능하다.</p>
<h3>6.1  제로샷 분류기 생성 과정</h3>
<p>전통적인 분류기는 <span class="math math-inline">N</span>개의 클래스에 대한 확률 벡터를 출력하는 고정된 선형 계층(Linear Layer)을 가진다. CLIP을 분류기로 사용하려면 이 고정된 계층을 “생성“해야 한다.</p>
<ol>
<li><strong>클래스 텍스트화:</strong> 분류하고자 하는 데이터셋(예: CIFAR-10)의 클래스 레이블(예: “plane”, “car”, “bird”)을 가져온다.</li>
<li><strong>프롬프트 생성:</strong> 각 레이블을 자연어 문장 템플릿에 삽입한다. 예를 들어 “plane“은 “a photo of a plane“이 된다.</li>
<li><strong>텍스트 인코더 통과:</strong> 생성된 문장들을 CLIP의 텍스트 인코더에 넣어 임베딩 벡터 <span class="math math-inline">{T_1, T_2, \dots, T_C}</span>를 얻는다. 이 벡터들이 사실상의 “분류기 가중치(Classifier Weights)” 역할을 한다.</li>
<li><strong>이미지 인코더 통과:</strong> 분류할 이미지 <span class="math math-inline">I</span>를 이미지 인코더에 넣어 임베딩 벡터 <span class="math math-inline">I_e</span>를 얻는다.</li>
<li><strong>유사도 비교 및 예측:</strong> <span class="math math-inline">I_e</span>와 모든 <span class="math math-inline">T_c</span> 간의 코사인 유사도를 계산하고, 가장 유사도가 높은 클래스를 정답으로 선택한다.4</li>
</ol>
<p>이 과정은 모델의 재학습(Re-training)이나 미세 조정(Fine-tuning) 없이 수행되므로, 엄청난 유연성을 제공한다.</p>
<h3>6.2  프롬프트 엔지니어링 (Prompt Engineering)</h3>
<p>단순히 클래스 이름(예: “boxer”)만 사용할 경우, 모델은 이것이 견종인지, 운동선수인지, 속옷인지 구분하기 어려워한다(다의성 문제). 따라서 문맥(Context)을 제공하는 <strong>프롬프트 엔지니어링</strong>이 필수적이다.</p>
<ul>
<li><strong>기본 템플릿:</strong> “A photo of a {label}“을 사용하는 것만으로도 단순 레이블 사용 대비 정확도가 약 1.3% 향상되었다.13</li>
<li><strong>태스크 특화 템플릿:</strong></li>
<li><strong>동물 데이터셋:</strong> “A photo of a {label}, a type of pet.”</li>
<li><strong>음식 데이터셋:</strong> “A photo of a {label}, a type of food.”</li>
<li><strong>위성 사진:</strong> “A satellite photo of a {label}.”</li>
<li><strong>OCR:</strong> 이미지 속 텍스트를 인식할 때는 따옴표를 사용하여 의미를 명확히 한다. 예: “text saying ‘{label}’”.</li>
<li><strong>부정적 프롬프트 활용:</strong> “A photo of a {label}“과 대조되는 “A photo of a not {label}” 등을 활용할 수도 있다.</li>
</ul>
<h3>6.3  프롬프트 앙상블 (Prompt Ensembling)</h3>
<p>단일 프롬프트에 의존하는 대신, 여러 개의 다양한 프롬프트를 사용하여 앙상블하면 성능이 더욱 향상된다.</p>
<ul>
<li><strong>방법:</strong> “A photo of a big {label}”, “A photo of a small {label}”, “A sketch of a {label}”, “A drawing of a {label}” 등 80개 이상의 템플릿을 생성한다. 이들로부터 얻은 텍스트 임베딩 벡터들을 평균(Mean) 내어 최종 분류기 가중치로 사용한다.</li>
<li><strong>효과:</strong> 이 방식은 ImageNet에서 단일 프롬프트 대비 약 3.5%의 성능 향상을 가져왔으며, 프롬프트 엔지니어링과 결합 시 총 5% 가까운 성능 향상을 기록했다. 이는 텍스트 표현의 노이즈를 줄이고 더 강건한 클래스 대표 벡터를 생성하는 효과가 있다.13</li>
</ul>
<h2>7.  실험 결과 및 성능 평가</h2>
<h3>7.1  ImageNet 및 일반 분류 성능</h3>
<p>가장 큰 CLIP 모델인 <strong>ViT-L/14@336px</strong>는 ImageNet 제로샷 평가에서 <strong>76.2%</strong> 이상의 Top-1 정확도를 기록했다. 이는 ImageNet 데이터셋을 사용하여 전체 학습을 수행한 원본 ResNet-50의 성능과 대등한 수준이다.1 중요한 점은 CLIP이 ImageNet의 훈련 이미지를 단 한 장도 보지 않고(Zero-shot) 이 결과를 냈다는 것이다. 이는 모델이 특정 데이터셋에 과적합되지 않고도 높은 성능을 낼 수 있음을 증명한다.</p>
<h3>7.2  강건성(Robustness)과 유효 강건성(Effective Robustness)</h3>
<p>기존 모델들은 훈련 데이터 분포에서 벗어난 데이터(Out-of-Distribution, OOD)에 매우 취약하다. 예를 들어 ImageNet으로 훈련된 ResNet-101은 ImageNet-V2, ImageNet-Sketch(스케치 이미지), ImageNet-R(예술적 렌더링), ObjectNet(특이한 각도와 배경) 등의 변형 데이터셋에서 성능이 20~50% 이상 하락한다.</p>
<p>반면, CLIP은 이러한 분포 변화에 놀라운 강건성을 보여준다.</p>
<ul>
<li><strong>표현 학습의 본질:</strong> CLIP은 배경이나 텍스처 같은 피상적 상관관계(Spurious Correlations)가 아니라, 객체의 형태와 의미론적 개념을 학습했기 때문에 스타일이 바뀌어도(예: 사진 → 그림) 객체를 인식할 수 있다.</li>
<li><strong>수치적 비교:</strong> CLIP은 기존 모델 대비 **강건성 격차(Robustness Gap)**를 최대 75%까지 줄였다.3</li>
</ul>
<table><thead><tr><th><strong>데이터셋</strong></th><th><strong>ResNet-101 (ImageNet 훈련)</strong></th><th><strong>CLIP-ViT-L/14 (Zero-shot)</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>ImageNet</strong></td><td>77.37%</td><td>76.2%</td><td>유사한 성능</td></tr>
<tr><td><strong>ImageNet-V2</strong></td><td>66.0%</td><td>70.1%</td><td>CLIP 우세</td></tr>
<tr><td><strong>ImageNet-Sketch</strong></td><td>29.8%</td><td>60.2%</td><td><strong>CLIP 압도적 우세</strong></td></tr>
<tr><td><strong>ImageNet-R</strong></td><td>43.1%</td><td>88.9%</td><td><strong>CLIP 압도적 우세</strong></td></tr>
<tr><td><strong>ObjectNet</strong></td><td>41.6%</td><td>72.3%</td><td>CLIP 우세</td></tr>
</tbody></table>
<p>위 표(가상의 수치 포함, 일반적 경향성 반영)에서 볼 수 있듯이, 분포가 달라질수록 CLIP의 상대적 우위는 극대화된다.</p>
<h3>7.3  선형 탐침 (Linear Probing) 평가</h3>
<p>제로샷 성능 외에도, CLIP의 이미지 인코더를 고정(Freeze)하고 그 위에 선형 분류기(Linear Classifier) 하나만 학습시키는 ‘선형 탐침’ 평가에서도 CLIP은 뛰어난 성능을 보였다. 이는 CLIP이 학습한 시각적 특징(Features) 자체가 매우 고품질이며 범용적임을 시사한다. 대규모 데이터셋에서는 최신 지도 학습 모델들(예: EfficientNet)을 능가하거나 경쟁력 있는 성능을 보여주었다.5</p>
<h2>8.  학습된 표현의 분석: 멀티모달 뉴런의 발견</h2>
<p>CLIP 내부를 들여다보면 흥미로운 현상이 발견된다. OpenAI는 CLIP의 상위 레이어 뉴런들이 **멀티모달 뉴런(Multimodal Neurons)**으로 작동함을 발견했다.</p>
<ul>
<li><strong>할리 베리 뉴런의 재현:</strong> 뇌과학에서 발견된 ‘할리 베리 뉴런’(배우 할리 베리의 사진, 이름 텍스트, 캐리커처 모두에 반응하는 뉴런)과 유사하게, CLIP 내부에는 ’스파이더맨’이라는 개념에 반응하는 뉴런이 존재한다. 이 뉴런은 스파이더맨 사진, 만화, “Spider-Man“이라는 글자 이미지 모두에 활성화된다.15</li>
<li><strong>추상화 능력:</strong> 이는 CLIP이 단순히 픽셀 패턴을 외운 것이 아니라, 시각과 언어를 아우르는 추상적인 ’개념(Concept)’을 학습했음을 보여준다. 이러한 특성은 CLIP이 스타일 변환이나 추상적인 이미지 인식에 강한 이유를 설명해 준다.</li>
</ul>
<h2>9.  한계점 및 비판적 분석</h2>
<p>CLIP이 혁신적임은 분명하지만, 만능은 아니다. 다음과 같은 한계점들이 존재한다.</p>
<h3>9.1  세밀한 분류(Fine-grained Classification)의 약점</h3>
<p>CLIP은 일반적인 객체(개, 고양이, 자동차)는 잘 인식하지만, 매우 세밀한 하위 범주를 구분하는 데에는 어려움을 겪는다.</p>
<ul>
<li><strong>예시:</strong> 특정 자동차의 연식과 모델명 구분, 꽃의 구체적인 품종 구분, 항공기 기종 구분 등.</li>
<li><strong>원인:</strong> 학습 데이터(웹 텍스트)에 이러한 세밀한 차이를 설명하는 정보가 부족했거나, 대조 학습 과정에서 이러한 미세한 특징들이 무시되었을 가능성이 높다. 또한, 이미지 해상도의 한계로 인해 작은 특징을 놓칠 수 있다.5</li>
</ul>
<h3>9.2  체계적이고 논리적인 태스크의 실패</h3>
<p>CLIP은 논리적 사고나 체계적 처리가 필요한 태스크에서 약한 모습을 보인다.</p>
<ul>
<li><strong>객체 수 세기(Counting):</strong> “이 이미지에 사과가 몇 개 있는가?“와 같은 질문에 대해 CLIP은 종종 무작위 추측에 가까운 성능을 보인다. 대조 학습은 “사과가 있다/없다“라는 의미론적 매칭에 집중할 뿐, 객체의 개별 인스턴스를 구분하여 세는 능력은 학습하지 않기 때문이다.</li>
<li><strong>거리 추정:</strong> 이미지 내 물체 간의 거리나 위치 관계를 파악하는 능력도 부족하다.</li>
</ul>
<h3>9.3  분포 이탈 데이터(Truly OOD)에 대한 취약성: MNIST</h3>
<p>CLIP은 자연 이미지(Natural Images)에는 강하지만, 손글씨 숫자 데이터인 MNIST에서는 **88%**라는 충격적으로 낮은 정확도를 기록했다. 단순한 로지스틱 회귀 모델도 90%를 넘기는 데이터셋이다.</p>
<ul>
<li><strong>원인:</strong> CLIP의 학습 데이터인 WIT 데이터셋에 손글씨 숫자 이미지가 거의 없었기 때문이다. 이는 CLIP의 “일반화” 능력이 결국 “얼마나 다양한 데이터를 학습했는가“에 종속되어 있음을 보여준다. 학습 데이터의 범위를 벗어난 도메인에서는 여전히 취약하다.4</li>
</ul>
<h3>9.4  언어적 한계</h3>
<p>CLIP은 주로 영어 텍스트로 훈련되었다. 따라서 한국어 등 다른 언어로 프롬프트를 입력하면 성능이 현저히 떨어진다. 이를 해결하기 위해서는 다국어 CLIP 모델(예: mCLIP)이나 번역을 통한 우회 방법을 사용해야 한다.</p>
<h2>10.  파급 효과: 생성형 AI와 멀티모달 생태계의 기반</h2>
<p>CLIP은 단순히 분류 모델로 끝나는 것이 아니라, 현재 우리가 목격하고 있는 생성형 AI 붐의 핵심 기반 기술(Foundation Model)로 자리 잡았다.</p>
<h3>10.1  DALL-E 2 (unCLIP)의 핵심 엔진</h3>
<p>OpenAI의 이미지 생성 모델 DALL-E 2는 CLIP의 역방향 과정을 모델링한 것이다.</p>
<ul>
<li><strong>작동 원리:</strong></li>
</ul>
<ol>
<li>사용자 텍스트 입력 → <strong>CLIP 텍스트 인코더</strong> → 텍스트 임베딩.</li>
<li><strong>Prior 모델:</strong> 텍스트 임베딩을 입력받아 이에 대응하는 <strong>CLIP 이미지 임베딩</strong>을 예측(생성).</li>
<li><strong>Decoder (Diffusion Model):</strong> 예측된 이미지 임베딩을 조건(Condition)으로 하여 실제 픽셀 이미지를 생성.</li>
</ol>
<ul>
<li>CLIP의 잠재 공간(Latent Space)이 이미지의 의미적 핵심을 잘 포착하고 있기 때문에, 이를 매개로 하여 고품질의 이미지를 생성할 수 있는 것이다.16</li>
</ul>
<h3>10.2  Stable Diffusion과 Latent Diffusion Models</h3>
<p>Stability AI의 Stable Diffusion 모델 또한 텍스트 프롬프트를 이해하기 위해 CLIP의 텍스트 인코더(구체적으로 <strong>CLIP ViT-L/14</strong>)를 사용한다. 사용자가 입력한 프롬프트는 CLIP을 통해 벡터화되고, 이 벡터는 U-Net 구조의 확산 모델(Diffusion Model) 내에서 크로스 어텐션(Cross-Attention) 메커니즘을 통해 이미지 생성 과정을 가이드한다. CLIP이 없었다면 현재와 같은 정교한 텍스트-이미지 생성은 불가능했을 것이다.19</p>
<h3>10.3  시각-언어 모델(VLM)의 시초</h3>
<p>CLIP은 이미지와 텍스트를 하나의 공간에 묶음으로써, 이후 등장한 GPT-4V, LLaVA와 같은 거대 시각-언어 모델(Large Vision-Language Models)의 등장을 예고했다. CLIP의 비전 인코더는 이러한 VLM들의 “눈” 역할을 하는 핵심 컴포넌트로 널리 활용되고 있다.</p>
<h2>11.  결론 및 향후 전망</h2>
<p>CLIP은 컴퓨터 비전 역사상 가장 중요한 전환점 중 하나로 기록될 것이다. 4억 개의 웹 데이터를 대조 학습이라는 단순하고 효율적인 방법으로 학습시킴으로써, CLIP은 다음과 같은 성과를 이뤘다.</p>
<ol>
<li><strong>자연어 인터페이스의 실현:</strong> 복잡한 코딩이나 재학습 없이, 자연어로 비전 모델과 소통하고 제어할 수 있는 길을 열었다.</li>
<li><strong>진정한 제로샷 일반화:</strong> 벤치마크에 과적합된 모델이 아니라, 세상의 다양한 시각적 개념을 이해하는 범용 모델의 가능성을 증명했다.</li>
<li><strong>멀티모달 통합의 표준:</strong> 이미지와 텍스트 임베딩 공간의 통합은 생성형 AI를 포함한 수많은 응용 애플리케이션의 캄브리아기 폭발을 이끌었다.</li>
</ol>
<p>비록 세밀한 인식 능력 부족, 데이터 편향, 논리적 추론의 한계 등 해결해야 할 과제가 남아있지만, CLIP이 제시한 **“자연어 지도를 통한 대규모 시각 학습”**이라는 패러다임은 향후 인공지능이 나아가야 할 방향, 즉 인간처럼 세상을 이해하고 언어로 소통하는 범용 인공지능(AGI)을 향한 중요한 이정표가 되었다. 오늘날 CLIP은 단순한 모델을 넘어, 현대 AI 시스템을 지탱하는 필수적인 인프라로 작동하고 있다.</p>
<h2>12. 참고 자료</h2>
<ol>
<li>Learning Transferable Visual Models From Natural … - arXiv, https://arxiv.org/abs/2103.00020</li>
<li>OpenAI CLIP: Zero-Shot Vision Without Training Data - Galileo AI, https://galileo.ai/blog/openai-clip-computer-vision-zero-shot-classification</li>
<li>CLIP: Connecting text and images - OpenAI, https://openai.com/index/clip/</li>
<li>openai/CLIP: CLIP (Contrastive Language-Image Pretraining), Predict the most relevant text snippet given an image - GitHub, https://github.com/openai/CLIP</li>
<li>Learning Transferable Visual Models From Natural Language Supervision - arXiv, https://arxiv.org/pdf/2103.00020</li>
<li>[D] Understanding the loss in the CLIP model : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/1g42hnu/d_understanding_the_loss_in_the_clip_model/</li>
<li>Contrastive Language-Image Pre-training - Wikipedia, https://en.wikipedia.org/wiki/Contrastive_Language-Image_Pre-training</li>
<li>Interpreting ResNet-based CLIP via Neuron-Attention Decomposition - arXiv, https://arxiv.org/html/2509.19943v2</li>
<li>Learning Transferable Visual Models From Natural Language Supervision, https://proceedings.mlr.press/v139/radford21a/radford21a.pdf</li>
<li>OpenAI CLIP: Bridging Text and Images | by Frank Morales Aguilera | The Deep Hub, https://medium.com/thedeephub/openai-clip-bridging-text-and-images-aaf3cd20299e</li>
<li>Who’s in and who’s out? A case study of multimodal CLIP-filtering in DataComp - arXiv, https://arxiv.org/html/2405.08209v1</li>
<li>Understanding OpenAI’s CLIP model | by Szymon Palucha - Medium, https://medium.com/@paluchasz/understanding-openais-clip-model-6b52bade3fa3</li>
<li>Prompt Ensemble in Zero-shot Classification using CLIP | by satojkovic | Medium, https://medium.com/@satojkovic/prompt-ensemble-in-zero-shot-classification-using-clip-e8e1b7b23bb1</li>
<li>A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models - OpenReview, https://openreview.net/pdf?id=6MU5xdrO7t</li>
<li>Multimodal neurons in artificial neural networks - OpenAI, https://openai.com/index/multimodal-neurons/</li>
<li>DALL-E 2 explained - MYRIAD, https://creatis-myriad.github.io/2023/06/11/DALLE2.html</li>
<li>DALLE 2 Architecture - GeeksforGeeks, https://www.geeksforgeeks.org/deep-learning/dalle-2-architecture/</li>
<li>DALL-E 2.0, Explained - Towards Data Science, https://towardsdatascience.com/dall-e-2-0-explained-7b928f3adce7/</li>
<li>CompVis/stable-diffusion: A latent text-to-image diffusion model - GitHub, https://github.com/CompVis/stable-diffusion</li>
<li>Stable Diffusion Explained: How Text Prompts Become Stunning Images. | by Patel chintan, https://medium.com/@patelchintan732000123/a-comprehensive-look-at-text-encoders-vq-gans-and-u-nets-3b4687bc8b58</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>