<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:M3ID (상호 정보량 최대화 디코딩, 지루한 응답 문제)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>M3ID (상호 정보량 최대화 디코딩, 지루한 응답 문제)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">데이터 불균형 문제 (Data Imbalance Problem)</a> / <span>M3ID (상호 정보량 최대화 디코딩, 지루한 응답 문제)</span></nav>
                </div>
            </header>
            <article>
                <h1>M3ID (상호 정보량 최대화 디코딩, 지루한 응답 문제)</h1>
<p>2025-12-09, G30DR</p>
<h2>1.  서론: 신경망 언어 생성의 패러다임과 ’지루한 응답’의 역설</h2>
<p>지난 10년간 자연어 처리(NLP) 분야는 통계적 모델에서 신경망 모델로의 급격한 패러다임 전환을 겪었다. 특히 시퀀스-투-시퀀스(Sequence-to-Sequence, Seq2Seq) 프레임워크의 등장은 기계 번역, 텍스트 요약, 그리고 대화 생성(Dialogue Generation) 분야에서 비약적인 성능 향상을 이끌어냈다. 그러나 이러한 기술적 성취의 이면에는 ’생성된 텍스트의 다양성 상실’이라는 고질적인 문제가 자리 잡고 있다. 신경망 모델이 인간의 언어 패턴을 학습하는 과정에서, 문법적으로는 완벽하지만 의미적으로는 공허한, 이른바 “지루한 응답(Dull Response)“을 양산하는 경향이 발견된 것이다.</p>
<p>본 보고서는 이러한 문제를 해결하기 위해 제안된 <strong>상호 정보량 최대화 디코딩(Maximum Mutual Information Decoding, M3ID)</strong> 방법론을 심층적으로 분석한다. 기존의 최대 우도 추정(MLE) 방식이 갖는 구조적 한계를 규명하고, 정보 이론(Information Theory)에 기반한 MMI가 어떻게 대화의 구체성과 다양성을 회복시킬 수 있는지에 대한 수학적, 경험적 증거를 제시한다. 나아가 2016년 Li et al.에 의해 제안된 초기 MMI 모델부터, 이를 현대적인 거대 언어 모델(LLM) 환경에 맞게 재해석한 대조적 디코딩(Contrastive Decoding) 및 프롬프트 엔지니어링 기법에 이르기까지, MMI의 진화 과정을 포괄적으로 다룬다.</p>
<h3>1.1  최대 우도 추정(MLE)의 구조적 한계와 안전한 응답 편향</h3>
<p>전통적인 신경망 언어 모델은 훈련 데이터셋에 존재하는 입력 시퀀스 <span class="math math-inline">S</span>와 출력 시퀀스 <span class="math math-inline">T</span>의 쌍에 대해, 조건부 확률 <span class="math math-inline">p(T|S)</span>를 최대화하는 방향으로 학습된다. 이를 디코딩 단계에 적용하면, 모델은 주어진 입력에 대해 가장 확률이 높은 응답을 선택하게 된다.1</p>
<p><span class="math math-display">\hat{T} = \arg\max_T \log p(T|S)</span></p>
<p>이러한 목적 함수는 기계 번역과 같이 정답이 비교적 명확한 과제에서는 효과적이다. 그러나 대화 생성과 같이 “일대다(One-to-Many)” 매핑이 가능한 개방형 과제에서는 심각한 부작용을 초래한다. 대화 데이터셋에서는 “I don’t know(모르겠습니다)”, “I think so(그런 것 같아요)”, “Yes(네)“와 같은 범용적이고 모호한 응답들이 압도적으로 높은 빈도로 등장한다. MLE 기반 모델은 이러한 고빈도 패턴을 학습하여, 입력 문맥이 무엇이든 간에 가장 ‘안전한’ 선택지인 이들 문장을 생성하려는 강한 편향(Bias)을 보인다.3</p>
<h3>1.2 다양성(Diversity)과 정보성(Informativeness)의 상실</h3>
<p>연구 결과에 따르면, Seq2Seq 모델이 생성하는 응답의 상당수는 문맥과 무관하게 통용될 수 있는 일반적인 문장들로 수렴한다. 이는 모델이 구체적인 정보(Specific Information)를 전달하기보다는, 예측 오류를 최소화하려는 방어적인 전략을 취하기 때문이다.1 “지루한 응답 문제“는 단순한 성능 저하가 아니라, 사용자의 참여(Engagement)를 저해하고 대화 시스템의 실용성을 떨어뜨리는 치명적인 결함이다. 사용자가 구체적인 질문을 던졌음에도 챗봇이 “모르겠습니다“라고만 반복한다면, 그 시스템은 지능형 에이전트로서의 가치를 상실한다.6 따라서 모델이 입력 메시지의 고유한 의미를 포착하고, 그에 상응하는 구체적인(Specific) 응답을 생성하도록 유도하는 새로운 목적 함수가 요구되었다.</p>
<h2>2. 정보 이론적 기초: 상호 정보량(Mutual Information)의 재해석</h2>
<p>MMI 디코딩의 이론적 배경은 섀넌(Shannon)의 정보 이론에 뿌리를 두고 있다. 상호 정보량은 두 확률 변수 간의 의존성을 정량화하는 척도로, 대화 모델에서는 입력 메시지와 출력 응답 사이의 ’공유된 정보량’을 의미한다.</p>
<h3>2.1 상호 정보량의 수학적 정의 및 직관</h3>
<p>두 확률 변수 <span class="math math-inline">X</span>와 <span class="math math-inline">Y</span> 사이의 상호 정보량 <span class="math math-inline">I(X;Y)</span>는 다음과 같이 정의된다7:</p>
<p><span class="math math-display">I(X;Y) = \sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x)p(y)} = H(X) - H(X|Y)</span></p>
<p>여기서 <span class="math math-inline">H(X)</span>는 엔트로피를 나타낸다. 이 수식은 <span class="math math-inline">Y</span>를 알았을 때 <span class="math math-inline">X</span>에 대한 불확실성이 얼마나 감소하는지를 측정한다. 이를 NLP의 맥락으로 가져와 입력 <span class="math math-inline">S</span>와 응답 <span class="math math-inline">T</span>에 적용하면 다음과 같다:</p>
<p><span class="math math-display">I(S;T) = \log \frac{p(S,T)}{p(S)p(T)}</span></p>
<p>대화 모델링에서 우리는 입력 <span class="math math-inline">S</span>가 주어졌을 때 최적의 <span class="math math-inline">T</span>를 찾고자 하므로, <span class="math math-inline">p(S)</span>는 상수 취급할 수 있다. 따라서 MMI 목적 함수는 다음과 같이 유도된다1:</p>
<p><span class="math math-display">\hat{T}_{MMI} = \arg\max_T \log \frac{p(T|S)}{p(T)} = \arg\max_T \{ \log p(T|S) - \log p(T) \}</span></p>
<h3>1.2  MLE 대 MMI: 목적 함수의 본질적 차이</h3>
<p>위의 유도된 식은 MMI와 MLE의 결정적인 차이를 보여준다. MLE는 단순히 <span class="math math-inline">\log p(T|S)</span>만을 최대화한다. 반면 MMI는 여기서 <span class="math math-inline">\log p(T)</span>를 차감한다.</p>
<ul>
<li><strong><span class="math math-inline">\log p(T|S)</span> (조건부 우도):</strong> 입력 <span class="math math-inline">S</span>에 대해 응답 <span class="math math-inline">T</span>가 얼마나 문법적이고 맥락에 맞는지를 평가한다.</li>
<li><strong><span class="math math-inline">-\log p(T)</span> (패널티 항):</strong> 응답 <span class="math math-inline">T</span> 자체가 얼마나 일반적인지(Commonplace)를 평가하여 패널티를 부여한다. <span class="math math-inline">p(T)</span>가 높다는 것은 “I don’t know“처럼 어디서나 볼 수 있는 흔한 문장임을 의미한다.</li>
</ul>
<p>따라서 MMI는 <strong>입력 <span class="math math-inline">S</span>에는 매우 적합하지만, 다른 상황에서는 잘 쓰이지 않는 희소하고 구체적인 응답</strong>을 찾도록 모델을 유도한다. 이는 “이 응답 <span class="math math-inline">T</span>는 오직 입력 <span class="math math-inline">S</span>를 위해서만 존재한다“는 배타적 연관성을 강화하는 과정으로 해석할 수 있다.3</p>
<h3>1.3  채널 코딩(Channel Coding) 관점에서의 해석</h3>
<p>통신 이론의 관점에서 대화는 화자의 의도(<span class="math math-inline">S</span>)가 통신 채널(모델)을 통해 전달되어 응답(<span class="math math-inline">T</span>)으로 변환되는 과정이다. 이상적인 채널은 입력의 정보를 손실 없이 출력으로 전달해야 한다. MMI를 최대화한다는 것은 채널 용량(Channel Capacity)을 최대로 활용한다는 의미와 상통한다.8 즉, 청자가 응답 <span class="math math-inline">T</span>만 듣고도 화자의 원래 발화 <span class="math math-inline">S</span>가 무엇이었는지 역추적할 수 있어야 한다는 것이다. 만약 모델이 모든 입력에 대해 “모르겠습니다“라고 답한다면, 응답 <span class="math math-inline">T</span>를 통해 입력 <span class="math math-inline">S</span>를 추측하는 것은 불가능하다(상호 정보량 <span class="math math-inline">\approx 0</span>). 반면, 입력의 핵심 키워드를 포함한 구체적인 응답은 입력 <span class="math math-inline">S</span>에 대한 강력한 단서를 제공하므로 높은 상호 정보량을 갖는다.</p>
<h2>2.  Li et al. (2016)의 MMI 디코딩 모델 상세 분석</h2>
<p>2016년 발표된 Li et al.의 연구 “A Diversity-Promoting Objective Function for Neural Conversation Models“는 MMI를 신경망 대화 모델에 실질적으로 구현한 선구적인 시도이다. 이 연구는 MMI 목적 함수를 실제 디코딩에 적용하기 위해 두 가지 변형된 전략, 즉 <strong>MMI-antiLM</strong>과 <strong>MMI-bidi</strong>를 제안했다.1</p>
<h3>2.1  MMI-antiLM: 반(Anti) 언어 모델 전략</h3>
<p>MMI-antiLM 모델은 앞서 유도한 수식 <span class="math math-inline">\log p(T|S) - \lambda \log p(T)</span>를 직접적으로 최적화하는 방식이다. 여기서 <span class="math math-inline">\lambda</span>는 다양성에 부여할 가중치를 조절하는 하이퍼파라미터이다.</p>
<h4>2.1.1  비문(Ungrammaticality) 문제의 발생 메커니즘</h4>
<p>이론적으로는 완벽해 보이는 이 수식은 실제 적용 시 심각한 문제를 야기한다. <span class="math math-inline">p(T)</span>는 일반적인 언어 모델(Language Model)로서 문장의 유창성(Fluency)을 나타낸다. 이를 목적 함수에서 뺀다는 것은, 유창하고 문법적으로 완벽한 문장에 패널티를 주는 것과 동일한 효과를 낳는다. 그 결과, 모델은 문법이 파괴되거나 비정상적인 단어 조합을 선택함으로써 <span class="math math-inline">p(T)</span>를 낮추려는 시도를 하게 된다.5</p>
<p>예를 들어, “I am a student“는 문법적으로 완벽하여 <span class="math math-inline">p(T)</span>가 높다. MMI-antiLM은 이를 피하기 위해 “I student am“과 같이 문법적으로 틀린 문장을 선택할 가능성이 있다. 이는 문법적 오류가 있는 문장은 훈련 데이터에서 거의 등장하지 않아 <span class="math math-inline">p(T)</span>가 매우 낮기 때문이다.10</p>
<h4>2.1.2  해결책: 가중치 조절 언어 모델 <span class="math math-inline">U(T)</span> 도입</h4>
<p>Li et al.은 이러한 비문 문제를 완화하기 위해 <span class="math math-inline">p(T)</span> 대신 <span class="math math-inline">U(T)</span>라는 수정된 확률 분포를 사용했다. 이는 문장의 길이에 따라 가중치를 달리 적용하는 방식이다.</p>
<p><span class="math math-display">p(T) \approx U(T) = \prod_{i=1}^{L_T} p(t_i | t_1, \dots, t_{i-1}) \cdot g(i)</span></p>
<p>여기서 <span class="math math-inline">g(i)</span>는 현재 생성 중인 토큰의 위치 <span class="math math-inline">i</span>에 따른 가중치 함수이다. 연구진은 문장의 초반부에 패널티를 집중시키고, 일정 길이 이후에는 패널티를 제거하는 전략을 채택했다.</p>
<p><span class="math math-display">g(k) = \begin{cases} 1 &amp; \text{if } k \le \gamma \\ 0 &amp; \text{if } k &gt; \gamma \end{cases}</span></p>
<p>이 방식은 대화의 ’지루함’이 주로 문장의 시작 부분(예: “Well,”, “I think,”, “It is”)에 집중된다는 점에 착안한 것이다. 초기 <span class="math math-inline">\gamma</span>개의 토큰에 대해서만 언어 모델의 확률을 차감하여 상투적인 시작을 억제하고, 그 이후에는 표준 Seq2Seq 모델의 확률(<span class="math math-inline">p(T|S)</span>)이 지배하도록 하여 문법적 완결성을 보장하려 했다.10 그러나 실험 결과, 이러한 보정에도 불구하고 MMI-antiLM은 여전히 문법적으로 불안정한 문장을 생성하는 경향이 있음이 확인되었다.10</p>
<h3>2.2  MMI-bidi: 양방향(Bidirectional) 재순위화 전략</h3>
<p>문법성 저하 문제를 근본적으로 해결하고, MMI의 이점을 극대화하기 위해 제안된 것이 바로 MMI-bidi 모델이다. 이 모델은 베이즈 정리를 활용하여 목적 함수를 재구성함으로써, 문법적으로 안전하면서도 정보량이 높은 응답을 찾아낸다.</p>
<h4>2.2.1  베이즈 정리를 통한 수식의 재구성</h4>
<p>베이즈 정리에 따르면, <span class="math math-inline">\log p(T|S) - \log p(T)</span>는 <span class="math math-inline">\log p(S|T) - \log p(S)</span>와 동치이다. 최적화 과정에서 입력 <span class="math math-inline">S</span>의 확률 <span class="math math-inline">p(S)</span>는 상수이므로 무시할 수 있다. 따라서 MMI 목적 함수는 다음과 같이 변형된다5:</p>
<p><span class="math math-display">\hat{T} = \arg\max_T \{ (1-\lambda)\log p(T|S) + \lambda \log p(S|T) \}</span></p>
<p>이 수식은 두 가지 모델의 가중합(Weighted Sum)으로 해석된다:</p>
<ol>
<li><strong>순방향 모델 <span class="math math-inline">p(T|S)</span>:</strong> 입력 <span class="math math-inline">S</span>가 주어졌을 때 응답 <span class="math math-inline">T</span>의 유창성과 적합성을 평가한다. 이는 문법적으로 올바른 문장을 보장하는 역할을 한다.</li>
<li><strong>역방향 모델 <span class="math math-inline">p(S|T)</span>:</strong> 응답 <span class="math math-inline">T</span>가 주어졌을 때 원본 입력 <span class="math math-inline">S</span>를 예측할 확률이다. 이는 응답 <span class="math math-inline">T</span>가 <span class="math math-inline">S</span>에 대해 얼마나 ’고유한가’를 평가한다.</li>
</ol>
<h4>3.2.2 역방향 모델의 의미: 정보적 고유성</h4>
<p><span class="math math-inline">p(S|T)</span> 항의 역할은 결정적이다. “잘 지내?“라는 질문(<span class="math math-inline">S</span>)에 대해 “응”(<span class="math math-inline">T_1</span>)이라고 답하는 경우, “응“이라는 단어만으로는 원래 질문이 “밥 먹었어?”, “숙제 했어?”, “잘 지내?” 중 무엇이었는지 알기 어렵다. 따라서 <span class="math math-inline">p(S|T_1)</span>은 낮다. 반면 “나는 아주 잘 지내고 있어”(<span class="math math-inline">T_2</span>)라고 답한다면, 이를 통해 원래 질문이 안부 인사였음을 높은 확률로 추론할 수 있다. 따라서 <span class="math math-inline">p(S|T_2)</span>는 높다. MMI-bidi는 이처럼 역추적이 용이한, 즉 정보 보존율이 높은 응답을 선호한다.10</p>
<h2>4. 디코딩 알고리즘 구현 및 최적화 전략</h2>
<p>MMI-bidi 모델은 이론적으로 우수하지만, 실제 구현에는 난관이 따른다. <span class="math math-inline">p(S|T)</span>를 계산하기 위해서는 응답 <span class="math math-inline">T</span>가 완전히 생성되어야 하므로, 토큰을 하나씩 생성하는 실시간 디코딩 과정에 직접 적용하기 어렵다(Intractable). 이를 해결하기 위해 N-best 리스트 재순위화(Reranking) 기법이 사용된다.</p>
<h3>4.1 N-best 리스트 생성과 빔 탐색(Beam Search)</h3>
<p>첫 번째 단계는 표준 Seq2Seq 모델(<span class="math math-inline">p(T|S)</span>)을 사용하여 후보 응답군을 생성하는 것이다. 이때 빔 탐색(Beam Search) 알고리즘이 핵심적인 역할을 수행한다.</p>
<h4>4.1.1 빔 사이즈(Beam Size)의 딜레마와 MMI의 해법</h4>
<p>일반적인 기계 번역(NMT)에서는 빔 사이즈를 4~5 정도로 작게 설정한다. 연구에 따르면 빔 사이즈가 너무 크면 오히려 번역 품질(BLEU 점수)이 하락하는 현상이 발생하는데, 이는 모델이 훈련 데이터의 잡음이나 과적합된 패턴을 탐색하게 되기 때문이다.14</p>
<p>그러나 MMI 디코딩에서는 빔 사이즈를 <strong>200</strong> 수준으로 매우 크게 설정한다.4 이는 목적이 다르기 때문이다. 작은 빔 사이즈에서는 <span class="math math-inline">p(T|S)</span>가 가장 높은 “안전한 응답“들만 살아남는다. 빔 폭을 넓혀야만 순위가 다소 낮더라도 문법적으로 허용 가능하며 내용이 풍부한 다양한 후보들이 리스트에 포함될 수 있다. MMI-bidi의 목표는 이 ’숨겨진 보석’들을 찾아내는 것이다.</p>
<h4>4.1.2 N-best 리스트 추출 프로세스</h4>
<ol>
<li><strong>초기 생성:</strong> 빔 사이즈 200으로 디코딩을 수행한다.</li>
<li><strong>후보 유지:</strong> 생성 과정에서 EOS(End of Sentence) 토큰을 만나 완료된 문장들을 N-best 리스트에 저장한다.</li>
<li><strong>다양성 확보:</strong> 완료된 가설이 빔에서 빠져나가면, 그 자리를 덜 유력하지만 잠재력 있는 다른 가설들로 채워 빔 사이즈를 일정하게 유지한다. 이를 통해 최종적으로 빔 사이즈보다 훨씬 많은 수의 후보군을 확보할 수 있다.12</li>
</ol>
<h3>4.2 재순위화(Reranking) 및 파라미터 튜닝</h3>
<p>생성된 <span class="math math-inline">N</span>개의 후보 문장들에 대해 역방향 모델을 사용하여 <span class="math math-inline">p(S|T)</span> 점수를 계산한다. 최종 점수는 다음과 같이 계산된다:</p>
<p><span class="math math-display">Score(T) = (1-\lambda)\log p(T|S) + \lambda \log p(S|T) + \gamma N_t</span></p>
<p>여기서 <span class="math math-inline">N_t</span>는 응답 문장의 길이(단어 수)이며, <span class="math math-inline">\gamma</span>는 길이에 대한 보상 가중치이다.</p>
<h4>2.2.2  길이 보정의 필요성</h4>
<p>역방향 모델 <span class="math math-inline">p(S|T)</span>는 종종 매우 짧은 문장을 선호하는 경향이 있다. 예를 들어 길이가 긴 문장은 확률의 곱셈으로 인해 전체 확률값이 작아질 수 있기 때문이다. 이를 방지하기 위해 단어 수에 비례하는 보너스 점수(<span class="math math-inline">\gamma N_t</span>)를 추가하여 적절한 길이의 응답을 장려한다.10</p>
<h4>2.2.3  MERT를 이용한 하이퍼파라미터 튜닝</h4>
<p><span class="math math-inline">\lambda</span>와 <span class="math math-inline">\gamma</span> 값은 시스템의 성능을 좌우하는 중요한 요소이다. Li et al.은 이를 최적화하기 위해 기계 번역에서 사용되는 <strong>최소 오류율 훈련(Minimum Error Rate Training, MERT)</strong> 알고리즘을 사용했다. 개발 데이터셋(Dev set)에 대해 BLEU 점수를 최대화하는 방향으로 이들 파라미터를 튜닝함으로써, 유창성, 정보성, 길이 간의 최적 균형점을 찾아내었다.9</p>
<h3>2.3  계산 복잡도(Computational Complexity)와 하드웨어적 함의</h3>
<p>MMI-bidi 디코딩은 표준 빔 탐색에 비해 현저히 높은 계산 비용을 요구한다.</p>
<table><thead><tr><th><strong>알고리즘</strong></th><th><strong>시간 복잡도</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>Greedy Decoding</strong></td><td><span class="math math-inline">O(L)</span></td><td>가장 빠름, 품질 낮음</td></tr>
<tr><td><strong>Standard Beam Search</strong></td><td><span class="math math-inline">O(B \cdot L)</span></td><td><span class="math math-inline">B=5</span>일 때 효율적</td></tr>
<tr><td><strong>MMI-bidi Reranking</strong></td><td><span class="math math-inline">O(B_{large} \cdot L) + N \cdot O(L_{inverse})</span></td><td><strong>매우 높음</strong></td></tr>
</tbody></table>
<ul>
<li><strong>1단계 부하:</strong> 빔 사이즈를 5에서 200으로 늘리는 것만으로도 연산량은 40배 증가한다.</li>
<li><strong>2단계 부하:</strong> 생성된 수백 개의 후보 문장 각각에 대해 별도의 역방향 모델 인퍼런스를 수행해야 한다.</li>
</ul>
<p>이러한 특성은 실시간 응답이 필수적인 상용 챗봇 시스템에서 지연 시간(Latency) 문제를 야기할 수 있다.15 따라서 실제 구현 시에는 병렬 처리(Parallel Processing)가 가능한 GPU 인프라가 필수적이며, 경우에 따라 빔 사이즈를 타협하거나 경량화된 역방향 모델을 사용하는 등의 최적화가 필요하다.</p>
<h2>3.  실증적 평가 및 결과 분석: 다양성의 정량화</h2>
<p>MMI 모델의 성능을 검증하기 위해서는 단순한 정확도가 아닌 ’다양성’을 측정할 수 있는 새로운 지표가 필요했다. Li et al.은 이를 위해 Distinct-N 지표를 도입하고, 광범위한 실험을 통해 MMI의 우수성을 입증했다.</p>
<h3>3.1  실험 데이터셋 및 환경</h3>
<p>실험은 두 가지 상이한 성격의 데이터셋에서 수행되었다.</p>
<ol>
<li><strong>Twitter Dataset:</strong> 약 2,300만 개의 대화 쌍. 짧고, 구어체이며, 문법적 파괴가 많고 잡음이 심하다.</li>
<li><strong>OpenSubtitles Dataset:</strong> 영화 자막 데이터. 문맥이 길고, 서사적이며, 상대적으로 문법이 잘 지켜진다.5</li>
</ol>
<p>모델 아키텍처로는 4-layer LSTM Seq2Seq 모델이 사용되었으며, 빔 사이즈 200으로 MMI-bidi 디코딩을 수행했다.</p>
<h3>3.2  평가 지표: Distinct-1/2 및 BLEU</h3>
<p>기존의 BLEU 점수는 정답 문장과의 n-gram 일치도만을 측정하므로, “안전한 응답“을 생성하는 모델이 오히려 높은 점수를 받는 역설적인 상황이 발생할 수 있다.17 이를 보완하기 위해 Li et al.은 <strong>Distinct-1</strong>과 <strong>Distinct-2</strong> 지표를 제안했다.</p>
<p><span class="math math-display">\text{Distinct-}n = \frac{\text{Count}(\text{unique } n\text{-grams})}{\text{Count}(\text{total } n\text{-grams})}</span></p>
<p>이 지표는 생성된 전체 응답 집합에서 고유한(Unique) 유니그램과 바이그램이 차지하는 비율을 나타낸다. 값이 높을수록 모델이 매번 똑같은 표현을 반복하지 않고 다양한 어휘를 구사함을 의미한다.6</p>
<h3>5.3 정량적 실험 결과</h3>
<p>다음 표는 Twitter 데이터셋에 대한 Li et al. (2016)의 실험 결과를 요약한 것이다.10</p>
<table><thead><tr><th><strong>모델 (Model)</strong></th><th><strong>BLEU</strong></th><th><strong>Distinct-1</strong></th><th><strong>Distinct-2</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>Seq2Seq (Baseline)</strong></td><td>4.31</td><td>0.023</td><td>0.107</td><td>기준 모델</td></tr>
<tr><td><strong>Seq2Seq (Greedy)</strong></td><td>4.51</td><td>0.032</td><td>0.148</td><td>탐욕적 탐색</td></tr>
<tr><td><strong>MMI-antiLM</strong></td><td>4.86</td><td>0.033</td><td>0.175</td><td>반 언어 모델</td></tr>
<tr><td><strong>MMI-bidi</strong></td><td><strong>5.22</strong></td><td><strong>0.051</strong></td><td><strong>0.270</strong></td><td><strong>양방향 MMI</strong></td></tr>
</tbody></table>
<ul>
<li><strong>다양성의 비약적 증가:</strong> MMI-bidi 모델은 기본 Seq2Seq 모델 대비 Distinct-1 점수에서 2배 이상(0.023 <span class="math math-inline">\rightarrow</span> 0.051), Distinct-2 점수에서 2.5배 이상(0.107 <span class="math math-inline">\rightarrow</span> 0.270)의 향상을 보였다. 이는 반복적인 표현이 획기적으로 줄어들었음을 시사한다.</li>
<li><strong>품질의 동반 상승:</strong> 일반적으로 다양성을 추구하면 정확도(BLEU)가 떨어질 것이라는 우려와 달리, BLEU 점수 또한 4.31에서 5.22로 상승했다. 이는 MMI가 무작위로 단어를 섞는 것이 아니라, 문맥에 더 적합한 단어를 선택함으로써 정답과의 유사도까지 높였음을 의미한다.5</li>
</ul>
<p>OpenSubtitles 데이터셋에서도 유사한 경향이 관찰되었다. MMI-bidi는 Distinct-2 점수를 0.0136(Baseline)에서 0.0303으로 약 122% 향상시켰다.10 다만 영화 자막 데이터의 특성상 전반적인 점수대는 트위터 데이터보다 낮게 형성되었다.</p>
<h3>5.4 정성적 분석 (Qualitative Analysis)</h3>
<p>실제 생성된 예시를 통해 MMI의 효과를 직관적으로 확인할 수 있다.20</p>
<ul>
<li><strong>Case 1:</strong>
<ul>
<li><strong>Input:</strong> “I got to get them in.” (그들을 들여보내야 해.)</li>
<li><strong>Seq2Seq:</strong> “I don’t have time for this.” (난 그럴 시간 없어. - 회피성 응답)</li>
<li><strong>MMI-bidi:</strong> “I gotta talk to them all.” (내가 걔네랑 다 얘기해봐야겠어. - ’them’을 반영한 능동적 응답)</li>
</ul>
</li>
<li><strong>Case 2:</strong>
<ul>
<li><strong>Input:</strong> “What is up with those two?” (저 둘은 대체 왜 저러는 거야?)</li>
<li><strong>Seq2Seq:</strong> “I don’t know what you are talking about.” (무슨 말인지 모르겠어.)</li>
<li><strong>MMI-bidi:</strong> “I don’t know what the hell is going on here!” (대체 무슨 일이 벌어지고 있는 건지 모르겠네! - 감정과 문맥이 풍부함)</li>
</ul>
</li>
</ul>
<p>MMI-bidi는 입력 문장의 대명사(them)나 상황적 긴박함을 반영하여, 훨씬 더 구체적이고 인간다운 응답을 생성해냈다. 특히 “I don’t know“라는 표현을 쓰더라도, 단순한 회피가 아니라 상황에 대한 구체적인 반응으로서 사용하는 모습을 보였다.</p>
<h2>6. 다른 디코딩 전략과의 비교 및 한계</h2>
<p>MMI 디코딩의 위치를 정확히 파악하기 위해서는, 이후 등장한 다양한 디코딩 전략들과의 비교가 필수적이다. 특히 현대 LLM에서 주로 사용되는 샘플링(Sampling) 기법과의 차이점은 MMI의 고유한 가치를 드러낸다.</p>
<h3>6.1 결정론적 탐색 vs 확률론적 샘플링</h3>
<ul>
<li><strong>탐욕적 탐색(Greedy) &amp; 빔 탐색(Beam Search):</strong> 이들은 결정론적(Deterministic)이다. 즉, 같은 입력에 대해 항상 같은 출력을 내놓는다. MMI는 기본적으로 이 범주에 속하지만, 목적 함수를 변경하여 다양성을 확보한다.</li>
<li><strong>Top-k &amp; 뉴클리어스 샘플링(Nucleus/Top-p Sampling):</strong> 이들은 확률론적(Stochastic)이다. 확률 분포에서 무작위로 토큰을 선택하되, 꼬리(Tail) 부분을 잘라내어 엉뚱한 단어가 선택될 확률을 줄인다. 최근 GPT 시리즈 등에서 표준으로 사용된다.21</li>
</ul>
<h3>6.2 MMI와 샘플링의 트레이드오프</h3>
<p>샘플링 방식은 “운“에 맡겨 다양성을 확보한다. 매번 다른 난수가 적용되므로 다양한 문장이 나오지만, 때로는 문맥과 전혀 상관없는 환각(Hallucination)이나 논리적 비약이 발생할 위험이 있다.24</p>
<p>반면, MMI는 **‘정보적 필연성’**에 의해 다양성을 확보한다. MMI가 선택한 응답은 무작위로 뽑힌 것이 아니라, 수학적으로 입력과 가장 강력하게 결속된 응답이다. 따라서 MMI는 샘플링 방식보다 **문맥 적합성(Relevance)**과 <strong>일관성(Coherence)</strong> 면에서 우월하다. 그러나 앞서 언급한 계산 비용 문제로 인해, 실시간성이 중요한 대규모 서비스에서는 샘플링 방식에 자리를 내어주게 되었다.25</p>
<h2>7. MMI의 현대적 진화: LLM과 프롬프트 엔지니어링</h2>
<p>MMI 디코딩 알고리즘 자체는 계산 비용 문제로 주류에서 밀려났을지 모르나, 그 핵심 사상인 “정보량 최대화“는 현대 NLP의 최전선에서 다양한 형태로 부활하고 있다.</p>
<h3>7.1 대조적 디코딩(Contrastive Decoding, CD)</h3>
<p>2023년 Li et al.이 제안한 **Contrastive Decoding (CD)**은 MMI의 직계 후손이라 할 수 있다. CD는 거대 모델(Expert)과 소형 모델(Amateur) 간의 확률 차이를 이용한다.27</p>
<p><span class="math math-display">\text{Score}(x) = \log p_{Expert}(x) - \log p_{Amateur}(x)</span></p>
<p>이 수식은 MMI의 <span class="math math-inline">\log p(T|S) - \log p(T)</span>와 놀라울 정도로 유사하다. 여기서 ’아마추어 모델’은 문맥을 깊이 이해하지 못하고 표면적인 통계(빈도)에만 의존하므로, MMI 식의 <span class="math math-inline">p(T)</span>(일반 언어 모델)와 동일한 역할을 수행한다. 즉, CD는 “전문가 모델만이 알고 있는 통찰“을 극대화하고 “아마추어 모델도 알 법한 뻔한 소리“를 억제함으로써, MMI의 철학을 현대적인 트랜스포머 아키텍처 위에서 구현해냈다. 실험 결과, CD는 뉴클리어스 샘플링보다 훨씬 더 일관성 있고 품질 높은 텍스트를 생성하는 것으로 밝혀졌다.26</p>
<h3>3.3  프롬프트 엔지니어링에서의 MMI (Maximum Mutual Information Method)</h3>
<p>프롬프트 엔지니어링 분야에서도 MMI는 최적의 프롬프트를 선별하는 기준(Metric)으로 활용된다.3</p>
<ol>
<li><strong>템플릿 생성:</strong> LLM을 사용하여 다양한 프롬프트 후보군을 생성한다.</li>
<li><strong>MMI 점수 계산:</strong> 각 프롬프트(<span class="math math-inline">P</span>)와 모델의 출력(<span class="math math-inline">O</span>) 간의 상호 정보량을 측정한다.</li>
<li><strong>최적 선택:</strong> <span class="math math-inline">I(P;O)</span>가 가장 높은 프롬프트를 선택한다.</li>
</ol>
<p>이는 정답(Gold Label)이 없는 상황에서도, 모델이 가장 확신을 가지고 구체적인 답변을 생성할 수 있는 질문 형태를 수학적으로 찾아내는 방법론이다. 즉, 프롬프트가 모호할수록 출력과의 상호 정보량은 낮아지고, 프롬프트가 명확하고 모델의 지식과 잘 정렬될수록 상호 정보량은 높아진다는 원리를 이용한다.</p>
<h3>3.4  검색 증강 생성(RAG)에서의 응용</h3>
<p>RAG 시스템에서 검색된 문서(Document)와 질문(Query) 사이의 관련성을 평가하는 리랭커(Reranker) 모델들 역시 MMI의 원리를 차용하고 있다. 단순한 벡터 유사도(Cosine Similarity)는 표면적인 단어 일치에 치중할 수 있으나, 질문 <span class="math math-inline">Q</span>가 주어졌을 때 문서 <span class="math math-inline">D</span>의 우도 <span class="math math-inline">p(D|Q)</span> 혹은 그 반대인 <span class="math math-inline">p(Q|D)</span>를 계산하는 방식은 의미적 함의(Implication)를 포착하는 데 훨씬 유리하다.29 이는 검색의 정확도를 높여 환각 현상을 줄이는 데 기여한다.</p>
<h2>4.  결론</h2>
<p>상호 정보량 최대화(MMI) 디코딩은 신경망 언어 모델이 직면했던 ’다양성 상실’과 ’지루한 응답’이라는 난제에 대해 정보 이론적 해법을 제시한 기념비적인 연구이다. Li et al. (2016)은 단순히 확률이 높은 문장이 아닌, 입력과 정보적으로 긴밀하게 연결된 문장을 찾음으로써 대화의 질을 획기적으로 개선할 수 있음을 입증했다.</p>
<p>본 연구를 통해 도출된 핵심 시사점은 다음과 같다:</p>
<ol>
<li><strong>목적 함수의 중요성:</strong> 최대 우도(Likelihood)는 모델 학습의 도구일 뿐, 인간다운 대화를 위한 최적의 목표는 아니다. 상호 정보량(Mutual Information)은 단순한 정답 맞추기를 넘어 의미의 교환을 최적화하는 더 높은 차원의 기준을 제공한다.</li>
<li><strong>구현의 난제와 극복:</strong> MMI-antiLM의 비문 생성 문제와 MMI-bidi의 계산 복잡도 문제는 이상적인 이론을 현실에 적용할 때 발생하는 트레이드오프를 보여준다. 그러나 N-best 재순위화와 파라미터 튜닝을 통해 이러한 한계를 현실 가능한 수준으로 극복해냈다.</li>
<li><strong>지속적인 영향력:</strong> 비록 원형 그대로의 MMI 디코딩은 계산 비용 문제로 인해 실시간 챗봇에서 널리 쓰이지 않게 되었으나, 그 핵심 철학은 대조적 디코딩(Contrastive Decoding), 프롬프트 최적화, RAG 리랭킹 등 현대 NLP 기술의 기저에 깊이 스며들어 있다.</li>
</ol>
<p>결론적으로, M3ID는 단순한 과거의 알고리즘이 아니라, 인공지능이 인간의 언어를 더 깊이 이해하고 더 풍부하게 구사하도록 만들기 위한 근본적인 접근법으로서 여전히 강력한 유효성을 지닌다. 향후 연구는 이러한 MMI의 이점을 유지하면서도 계산 비용을 획기적으로 줄일 수 있는 경량화된 구조나, 역방향 모델 없이도 상호 정보량을 근사할 수 있는 새로운 학습 방법론 개발에 집중되어야 할 것이다.</p>
<h2>5. 참고 자료</h2>
<ol>
<li>A Diversity-Promoting Objective Function for Neural Conversation Models - Microsoft, https://www.microsoft.com/en-us/research/publication/a-diversity-promoting-objective-function-for-neural-conversation-models/?locale=ko-kr</li>
<li>Machine Learning for Neural Decoding - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC7470933/</li>
<li>Max Mutual Information (MMI) Method - Learn Prompting, https://learnprompting.org/docs/advanced/ensembling/max_mutual_information_method</li>
<li>Generating More Interesting Responses in Neural Conversation Models with Distributional Constraints - ACL Anthology, https://aclanthology.org/D18-1431.pdf</li>
<li>A Diversity-Promoting Objective Function for Neural Conversation Models - Stanford NLP Group, https://nlp.stanford.edu/pubs/jiwei2016diversity.pdf</li>
<li>A Diversity-Promoting Objective Function for Neural Conversation Models - ResearchGate, https://www.researchgate.net/publication/282843943_A_Diversity-Promoting_Objective_Function_for_Neural_Conversation_Models</li>
<li>Decoding Mutual Information: A Guide for Machine Learning Practitioners - Medium, https://medium.com/@suvendulearns/decoding-mutual-information-mi-a-guide-for-machine-learning-practitioners-b0f0ca0b30c9</li>
<li>Mutual information - Scholarpedia, http://www.scholarpedia.org/article/Mutual_information</li>
<li>A Diversity-Promoting Objective Function for Neural Conversation …, https://arxiv.org/abs/1510.03055</li>
<li>arXiv:1510.03055v3 [cs.CL] 10 Jun 2016, https://arxiv.org/pdf/1510.03055</li>
<li>Improving Neural Response Diversity with Frequency-Aware Cross-Entropy Loss - UvA-DARE (Digital Academic Repository), https://pure.uva.nl/ws/files/42480648/p2879_jiang.pdf</li>
<li>A Diversity-Promoting Objective Function for Neural Conversation Models - ACL Anthology, https://aclanthology.org/N/N16/N16-1014.pdf</li>
<li>The MSR-NLP System at Dialog System Technology Challenges 6 - Microsoft, https://www.microsoft.com/en-us/research/wp-content/uploads/2017/12/DSTC6_Task_2_MSR_System_Description.pdf</li>
<li>Breaking the Beam Search Curse: A Study of (Re-)Scoring Methods and Stopping Criteria for Neural Machine Translation - Baidu Research, https://research.baidu.com/Public/uploads/5b8c971b1778a.pdf</li>
<li>DAL: Dual Adversarial Learning for Dialogue Generation - ACL Anthology, https://aclanthology.org/W19-2302.pdf</li>
<li>A new approach for neural decoding by inspiring of hyperdimensional computing for implantable intra-cortical BMIs - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC11458893/</li>
<li>BLEU is Not Suitable for the Evaluation of Text Simplification - UPenn CIS, https://www.cis.upenn.edu/~eliors/papers/bleu_simplification.pdf</li>
<li>A Diversity-Promoting Objective Function for Neural Conversation Models - ResearchGate, https://www.researchgate.net/publication/305334287_A_Diversity-Promoting_Objective_Function_for_Neural_Conversation_Models</li>
<li>[Quick Review] A Diversity-Promoting Objective Function for Neural Conversation Models, https://liner.com/review/diversitypromoting-objective-function-for-neural-conversation-models</li>
<li>arXiv:1701.06549v2 [cs.CL] 3 Feb 2017, https://arxiv.org/pdf/1701.06549</li>
<li>Two minutes NLP — Most used Decoding Methods for Language Models | by Fabio Chiusano | Generative AI | Medium, https://medium.com/nlplanet/two-minutes-nlp-most-used-decoding-methods-for-language-models-9d44b2375612</li>
<li>Foundation model parameters: decoding and stopping criteria - IBM, https://www.ibm.com/docs/en/watsonx/saas?topic=prompts-model-parameters-prompting</li>
<li>How to generate text: using different decoding methods for language generation with Transformers - Hugging Face, https://huggingface.co/blog/how-to-generate</li>
<li>The Unreasonable Ineffectiveness of Nucleus Sampling on Mitigating Text Memorization, https://arxiv.org/html/2408.16345v1</li>
<li>[D] What happened to “creative” decoding strategy? : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/1e42das/d_what_happened_to_creative_decoding_strategy/</li>
<li>(PDF) Contrastive Decoding: Open-ended Text Generation as Optimization - ResearchGate, https://www.researchgate.net/publication/364813961_Contrastive_Decoding_Open-ended_Text_Generation_as_Optimization</li>
<li>Contrastive Decoding: Open-ended Text Generation as Optimization - ACL Anthology, https://aclanthology.org/2023.acl-long.687/</li>
<li>Contrastive Decoding: Open-ended Text Generation as Optimization - arXiv, https://arxiv.org/html/2210.15097</li>
<li>(PDF) An Information-Theoretic Framework for Retrieval-Augmented Generation Systems, https://www.researchgate.net/publication/393933114_An_Information-Theoretic_Framework_for_Retrieval-Augmented_Generation_Systems</li>
<li>A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions - arXiv, https://arxiv.org/html/2507.18910v1</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>