<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:변이형 오토인코더</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>변이형 오토인코더</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">인코더 (Encoders)</a> / <span>변이형 오토인코더</span></nav>
                </div>
            </header>
            <article>
                <h1>변이형 오토인코더</h1>
<h2>1. 생성 모델의 패러다임과 변이형 오토인코더의 등장</h2>
<h3>1.1  생성 모델의 정의와 목표</h3>
<p>생성 모델(Generative Model)은 기계 학습의 한 분야로서, 훈련 데이터의 기저에 있는 패턴과 확률 분포를 학습하여 기존 데이터와 유사하지만 완전히 새로운 데이터를 생성하는 것을 목표로 하는 모델을 총칭한다.1 이는 입력 데이터가 주어졌을 때 특정 레이블이나 값을 예측하는 판별 모델(Discriminative Model)과는 근본적으로 다른 접근 방식을 취한다.3</p>
<p>판별 모델은 주어진 입력 <span class="math math-inline">X</span>에 대한 레이블 <span class="math math-inline">Y</span>의 조건부 확률 <span class="math math-inline">p(Y|X)</span>를 모델링하는 데 집중한다.5 예를 들어, 이미지 분류기는 고양이 사진(<span class="math math-inline">X</span>)이 주어졌을 때 ’고양이’라는 레이블(<span class="math math-inline">Y</span>)이 나올 확률을 최대화하도록 학습된다. 반면, 생성 모델은 입력 데이터 <span class="math math-inline">X</span>와 레이블 <span class="math math-inline">Y</span>의 결합 확률 분포 <span class="math math-inline">p(X, Y)</span>를 학습하거나, 레이블이 없는 비지도 학습 환경에서는 데이터 자체의 분포 <span class="math math-inline">p(X)</span>를 직접 학습한다.3 이 능력 덕분에 생성 모델은 단순히 데이터를 분류하는 것을 넘어, 데이터가 ‘어떻게’ 생성되었는지 그 과정을 이해하고, 이 이해를 바탕으로 해당 분포로부터 새로운 샘플을 ’생성(generate)’할 수 있다.2</p>
<p>생성 모델의 핵심 목표는 훈련 데이터셋의 복잡하고 고차원적인 확률 분포를 근사하는 것이다. 일단 이 분포를 성공적으로 학습하면, 모델은 마치 훈련 데이터의 스타일과 특징을 모두 이해한 예술가처럼, 세상에 존재하지 않았던 새로운 이미지, 텍스트, 음악 등을 창조해낼 수 있다.2 이러한 능력은 데이터가 부족한 분야에서 데이터를 증강(data augmentation)하거나, 예술 및 콘텐츠 창작, 신약 개발과 같은 혁신적인 분야에서 무한한 가능성을 열어주고 있다.2</p>
<h3>1.2  표준 오토인코더(AE)의 한계와 생성 모델로서의 부적합성</h3>
<p>표준 오토인코더(Autoencoder, AE)는 인코더(encoder)와 디코더(decoder)라는 두 개의 신경망으로 구성된 비지도 학습 모델이다.8 인코더는 고차원의 입력 데이터를 저차원의 잠재 공간(latent space)으로 압축하고, 디코더는 이 압축된 잠재 표현(latent representation)으로부터 원본 데이터를 다시 복원하는 역할을 한다.10 AE의 주된 목적은 재구성 오차(reconstruction error)를 최소화하는 과정에서 데이터의 중요한 특징을 추출하는 것으로, 주로 데이터 압축, 노이즈 제거, 차원 축소 등에 활용된다.9</p>
<p>그러나 AE는 진정한 의미의 생성 모델로 기능하기에는 결정적인 한계를 지닌다. 그 한계는 바로 잠재 공간의 불규칙성(irregularity)과 비연속성에 있다. AE의 학습 목표는 오직 ’재구성’에만 초점이 맞춰져 있어, 잠재 공간의 구조에 대해서는 어떠한 제약도 가하지 않는다.8 그 결과, 인코더는 각 입력 데이터를 잠재 공간 상의 특정 ‘점(point)’ 또는 ’좌표(coordinate)’로 매핑하도록 학습될 뿐, 이 점들이 잠재 공간 내에서 의미 있는 분포나 구조를 형성하도록 강제되지 않는다.10</p>
<p>이러한 불규칙한 잠재 공간은 생성 능력에 치명적인 약점으로 작용한다. 훈련된 AE의 잠재 공간에서, 학습된 데이터 포인트들 사이에 존재하는 ’빈 공간(empty space)’에서 임의의 벡터를 샘플링하여 디코더에 입력하면, 디코더는 이전에 본 적 없는 생소한 입력을 받았기 때문에 의미 있는 출력을 만들어내지 못한다.11 대부분의 경우, 결과물은 알아볼 수 없는 노이즈나 왜곡된 형태일 뿐, 새롭고 그럴듯한 데이터라고 보기 어렵다.11 따라서 AE는 뛰어난 ‘재구성’ 모델일 수는 있으나, 잠재 공간으로부터 새로운 데이터를 창조하는 ‘생성’ 모델로서는 부적합하다.13</p>
<h3>1.3  변이형 오토인코더(VAE)의 개념적 도약: 확률적 잠재 공간의 도입</h3>
<p>표준 오토인코더의 근본적인 한계를 극복하기 위해, 2013년 Diederik P. Kingma와 Max Welling은 변이형 오토인코더(Variational Autoencoder, VAE)를 제안하였다.15 VAE는 AE의 기본 구조를 차용하면서도, 생성 모델로서 기능하기 위해 베이즈 추론(Bayesian inference)의 원리를 접목한 심층 생성 모델이다.17</p>
<p>VAE의 가장 핵심적인 혁신은 잠재 공간을 다루는 방식에 있다. AE가 입력을 잠재 공간의 결정론적인(deterministic) 단일 벡터로 인코딩하는 것과 달리, VAE는 입력을 연속적인 확률 분포(probabilistic distribution)로 인코딩한다.9 구체적으로, VAE의 인코더는 입력 데이터 <span class="math math-inline">x</span>가 주어졌을 때, 잠재 공간 상의 특정 점 <span class="math math-inline">z</span>를 직접 출력하는 것이 아니라, 해당 점이 따를 확률 분포(일반적으로 정규분포)의 파라미터, 즉 평균(<span class="math math-inline">\mu</span>)과 분산(<span class="math math-inline">\sigma^2</span>)을 출력한다.11</p>
<p>이러한 확률적 인코딩 방식은 잠재 공간의 구조에 근본적인 변화를 가져온다. VAE는 손실 함수를 통해 모든 데이터 포인트의 잠재 분포가 특정 사전 분포(prior distribution, 보통 표준 정규분포)에 가깝도록 강제한다. 이 규제(regularization) 효과 덕분에 VAE의 잠재 공간은 불규칙하고 비어있는 공간 없이, 부드럽고(smooth) 연속적인(continuous) 구조를 갖게 된다.20 결과적으로 잠재 공간 내의 어떤 지점에서 샘플링을 하더라도 디코더가 이를 의미 있는 데이터로 해석하여 생성할 수 있게 된다. 이는 VAE가 AE의 데이터 압축 및 재구성 기능을 수행하면서도, 동시에 강력한 생성 능력을 갖추게 되는 근본적인 이유이다.9</p>
<p>이러한 전환은 단순한 구조적 개선을 넘어선 패러다임의 변화를 의미한다. AE의 목표가 ’재구성 오차의 최소화’라는 비교적 단순한 문제였다면, VAE는 ’데이터의 로그 가능도 최대화’라는 훨씬 더 근본적이고 어려운 문제를 풀고자 한다.12 VAE는 이 문제를 해결하기 위한 도구로 변분 추론이라는 통계적 기법을 사용하며, 오토인코더와 유사한 구조는 이 기법을 신경망으로 구현하는 과정에서 자연스럽게 나타난 형태일 뿐이다. 실제로 VAE의 수학적 기반은 희소 오토인코더나 노이즈 제거 오토인코더와 같은 고전적인 오토인코더와는 거의 관련이 없으며, 변분 베이즈 방법론에 깊이 뿌리내리고 있다.15 따라서 VAE는 ’생성을 위해 개선된 AE’가 아니라, ’AE의 구조를 활용하여 확률적 생성 모델링 문제를 푸는 새로운 종류의 모델’로 이해하는 것이 더 정확하다. 이 확률적 기반 덕분에 VAE는 단순 생성을 넘어 불확실성에 대한 추론이 중요한 이상 탐지나 준지도 학습과 같은 고차원적인 문제에도 성공적으로 적용될 수 있다.</p>
<h2>2. 변이형 오토인코더의 구조와 원리</h2>
<h3>2.1  인코더(Encoder): 추론 네트워크(Inference Network)로서의 역할</h3>
<p>변이형 오토인코더의 인코더는 종종 ‘추론 네트워크(inference network)’ 또는 ’인식 모델(recognition model)’로 불린다.12 그 주된 역할은 관찰된 데이터 <span class="math math-inline">x</span>로부터 눈에 보이지 않는 잠재 변수 <span class="math math-inline">z</span>의 분포를 추론하는 것이다. 통계학적 용어로 이는 데이터 <span class="math math-inline">x</span>가 주어졌을 때의 잠재 변수 <span class="math math-inline">z</span>의 사후 확률(posterior probability), 즉 <span class="math math-inline">p(z|x)</span>를 근사하는 과정에 해당한다.25 VAE는 이 실제 사후 확률을 직접 계산하기 어렵기 때문에, 이를 신경망으로 구현된 함수 <span class="math math-inline">q_\phi(z|x)</span>를 통해 근사한다. 여기서 <span class="math math-inline">\phi</span>는 인코더 신경망의 가중치와 편향 같은 학습 가능한 파라미터를 의미한다.26</p>
<p>표준 오토인코더의 인코더가 입력 <span class="math math-inline">x</span>에 대해 잠재 공간의 단일 벡터 <span class="math math-inline">z</span>를 출력하는 결정론적(deterministic) 함수인 것과 대조적으로, VAE의 인코더는 확률적(probabilistic)이다. 즉, 인코더는 <span class="math math-inline">q_\phi(z|x)</span>가 특정 확률 분포를 따르도록 해당 분포의 파라미터를 출력한다. 일반적으로 이 분포는 다변량 정규분포(multivariate Gaussian distribution) <span class="math math-inline">\mathcal{N}(\mu(x), \Sigma(x))</span>로 가정된다.24 따라서 인코더의 최종 출력은 잠재 벡터 자체가 아니라, 이 정규분포를 정의하는 평균 벡터 <span class="math math-inline">\mu(x)</span>와 공분산 행렬 <span class="math math-inline">\Sigma(x)</span>이다.11</p>
<p>실제 구현에서는 계산의 복잡성을 줄이고 효율성을 높이기 위해 몇 가지 가정이 추가된다. 가장 일반적인 가정은 잠재 변수의 각 차원이 서로 독립적이라고 보는 것이다. 이 경우 공분산 행렬 <span class="math math-inline">\Sigma(x)</span>은 대각 성분 외에는 모두 0인 대각 행렬(diagonal matrix)이 된다. 결과적으로 인코더는 각 잠재 차원 <span class="math math-inline">i</span>에 대한 평균 <span class="math math-inline">\mu_i</span>와 분산 <span class="math math-inline">\sigma_i^2</span>만을 계산하면 되므로, 최종적으로 두 개의 벡터, 즉 평균 벡터 <span class="math math-inline">\mu</span>와 분산 벡터 <span class="math math-inline">\sigma^2</span>을 출력하게 된다.21 더 나아가, 분산 값은 항상 양수여야 한다는 제약을 만족시키기 위해, 네트워크가 직접 분산을 출력하는 대신 분산의 로그 값(<span class="math math-inline">\log \sigma^2</span>)을 출력하도록 설계하는 것이 일반적이다. 이 로그-분산 값에 지수 함수를 취하면 항상 양수인 분산 값을 얻을 수 있기 때문이다.11</p>
<h3>2.2  잠재 공간(Latent Space): 연속적이고 구조화된 확률적 표현</h3>
<p>VAE의 가장 핵심적인 특징이자 표준 AE와의 근본적인 차이점은 바로 잠재 공간의 구조에 있다. 표준 AE의 잠재 공간은 학습 데이터 포인트들이 흩어져 있는 불규칙한 공간으로, 데이터가 존재하지 않는 ’구멍’이 많아 생성 모델로 활용하기 어렵다.12 반면, VAE는 잠재 공간이 매우 잘 조직된, 즉 연속적이고(continuous), 부드러우며(smooth), 밀집된(dense) 구조를 갖도록 학습 과정에서 명시적으로 강제한다.20</p>
<p>이러한 구조화는 VAE의 손실 함수에 포함된 규제 항, 즉 쿨백-라이블러 발산(Kullback-Leibler Divergence, KL Divergence)에 의해 달성된다. KL 발산은 두 확률 분포 사이의 차이를 측정하는 지표로, VAE에서는 인코더가 출력하는 각 데이터 포인트의 잠재 분포 <span class="math math-inline">q_\phi(z|x)</span>와 우리가 사전에 정의한 간단한 사전 분포(prior distribution) <span class="math math-inline">p(z)</span> 사이의 거리를 측정한다.8 이 사전 분포 <span class="math math-inline">p(z)</span>는 보통 평균이 0이고 공분산이 단위 행렬인 표준 정규분포 <span class="math math-inline">\mathcal{N}(0, I)</span>로 설정된다.20</p>
<p>훈련 과정에서 VAE는 이 KL 발산 값을 최소화하도록 학습된다. 이는 인코더가 생성하는 모든 잠재 분포 <span class="math math-inline">q_\phi(z|x)</span>를 원점 중심의 표준 정규분포 <span class="math math-inline">p(z)</span>에 가깝게 만들도록 강제하는 강력한 규제로 작용한다. 이 규제 덕분에 두 가지 중요한 효과가 발생한다. 첫째, 비슷한 입력 데이터들은 잠재 공간에서 서로 가까운 위치에 있는 유사한 분포로 인코딩된다. 둘째, 잠재 공간 전체가 데이터 포인트들의 분포로 조밀하게 채워져 ’빈 공간’이 거의 사라진다.</p>
<p>이렇게 잘 구조화된 잠재 공간은 ’보간(interpolation)’을 의미 있게 만든다. 예를 들어, 잠재 공간에서 숫자 ’1’에 해당하는 점과 숫자 ’7’에 해당하는 점을 찾은 뒤, 두 점을 잇는 직선 상의 중간 지점들을 샘플링하여 디코더에 넣으면, ’1’이 점차 ’7’로 변해가는 자연스러운 중간 형태의 이미지들을 생성할 수 있다. 더 나아가, 안경을 쓴 사람의 이미지에서 추출한 잠재 벡터에서 ’안경’이라는 속성에 해당하는 잠재 벡터를 빼면, 안경을 벗은 얼굴 이미지를 생성하는 것과 같은 의미론적 벡터 연산(semantic arithmetic)도 가능해진다.28 이는 VAE가 데이터의 단순한 픽셀 패턴을 넘어, 데이터의 기저에 있는 추상적인 생성 요인들을 학습하고 있음을 시사한다.</p>
<h3>2.3  디코더(Decoder): 생성 네트워크(Generative Network)로서의 역할</h3>
<p>VAE의 디코더는 ’생성 네트워크(generative network)’로 불리며, 인코더와는 반대의 역할을 수행한다.12 즉, 저차원의 잠재 공간에 있는 한 점 <span class="math math-inline">z</span>를 입력으로 받아, 이를 고차원의 원본 데이터 공간으로 다시 매핑하는 것이다. 구체적으로 디코더는 잠재 변수 <span class="math math-inline">z</span>가 주어졌을 때의 데이터 <span class="math math-inline">x</span>의 조건부 확률 분포 <span class="math math-inline">p_\theta(x|z)</span>를 모델링한다. 여기서 <span class="math math-inline">\theta</span>는 디코더 신경망의 학습 가능한 파라미터를 나타낸다.25</p>
<p>훈련 단계에서 VAE의 전체 과정은 다음과 같다. 먼저 입력 데이터 <span class="math math-inline">x</span>가 인코더를 통과하여 잠재 분포 <span class="math math-inline">q_\phi(z|x)</span>의 파라미터(<span class="math math-inline">\mu, \sigma^2</span>)를 얻는다. 그 다음, 이 분포로부터 잠재 벡터 <span class="math math-inline">z</span>를 샘플링한다. 마지막으로, 이 샘플링된 <span class="math math-inline">z</span>가 디코더에 입력되어 원본 입력 <span class="math math-inline">x</span>와 최대한 유사한 출력 <span class="math math-inline">\hat{x}</span>를 생성하도록 학습된다.22 이 과정에서 디코더는 잠재 공간의 각 점들이 어떤 데이터에 해당하는지를 학습하게 된다.</p>
<p>VAE가 완전히 훈련되고 나면, 우리는 새로운 데이터를 생성하기 위해 디코더만을 독립적으로 사용할 수 있다. 이 때 인코더는 더 이상 필요하지 않다.9 생성 과정은 매우 간단하다. 먼저 잠재 공간의 사전 분포 <span class="math math-inline">p(z)</span>(즉, <span class="math math-inline">\mathcal{N}(0, I)</span>)에서 임의의 벡터 <span class="math math-inline">z_{new}</span>를 샘플링한다. 그리고 이 <span class="math math-inline">z_{new}</span>를 훈련된 디코더에 입력한다. 그러면 디코더는 이 잠재 벡터를 해석하여, 훈련 데이터에서는 본 적이 없지만 통계적 특성이 유사한 새롭고 그럴듯한 데이터 <span class="math math-inline">\hat{x}_{new}</span>를 생성해낸다.14 이 과정이 VAE를 강력한 생성 모델로 만드는 핵심 원리이다.</p>
<p>다음 표는 표준 오토인코더와 변이형 오토인코더의 핵심적인 차이점을 요약하여 보여준다. 이 비교를 통해 두 모델의 구조적 유사성에도 불구하고 그 철학과 목적이 근본적으로 어떻게 다른지 명확히 이해할 수 있다.</p>
<p><strong>Table 1: 표준 오토인코더(AE)와 변이형 오토인코더(VAE) 비교</strong></p>
<table><thead><tr><th>특징 (Feature)</th><th>표준 오토인코더 (Standard Autoencoder, AE)</th><th>변이형 오토인코더 (Variational Autoencoder, VAE)</th></tr></thead><tbody>
<tr><td><strong>주요 목적</strong></td><td>데이터 압축, 특징 추출, 노이즈 제거 8</td><td>새로운 데이터 생성, 데이터 분포 학습 8</td></tr>
<tr><td><strong>인코더 출력</strong></td><td>잠재 공간의 단일 벡터 (결정론적) 9</td><td>잠재 분포의 파라미터 (평균 <span class="math math-inline">\mu</span>, 분산 <span class="math math-inline">\sigma^2</span>) (확률적) 20</td></tr>
<tr><td><strong>잠재 공간</strong></td><td>불규칙하고 비연속적일 수 있음. 구조에 대한 제약 없음 8</td><td>연속적이고 부드러운 분포를 따르도록 규제됨 (보통 정규분포) 20</td></tr>
<tr><td><strong>손실 함수</strong></td><td>재구성 손실 (Reconstruction Loss) (e.g., MSE) 10</td><td>재구성 손실 + KL 발산 (ELBO 최대화) 8</td></tr>
<tr><td><strong>데이터 생성 능력</strong></td><td>약함. 잠재 공간의 임의 샘플링이 의미 있는 출력을 보장하지 않음 11</td><td>강함. 잠재 공간의 사전 분포에서 샘플링하여 새로운 데이터 생성 가능 9</td></tr>
<tr><td><strong>이론적 기반</strong></td><td>표현 학습, 차원 축소 13</td><td>변분 추론, 확률적 그래픽 모델 15</td></tr>
</tbody></table>
<h2>3. 변이형 오토인코더의 수학적 탐구</h2>
<h3>3.1  문제 정의: 다루기 힘든 사후 확률</h3>
<p>모든 확률적 생성 모델의 근본적인 목표는 훈련 데이터가 나타날 확률, 즉 데이터의 증거(evidence) 또는 가능도(likelihood) <span class="math math-inline">p(x)</span>를 최대화하는 것이다. 잠재 변수 <span class="math math-inline">z</span>를 사용하는 모델에서 이 가능도는 잠재 변수에 대한 주변화(marginalization)를 통해 계산된다.24<br />
<span class="math math-display">
p(x) = \int p(x, z) dz = \int p(x|z)p(z)dz
</span><br />
여기서 <span class="math math-inline">p(z)</span>는 잠재 변수의 사전 분포(prior distribution)이고, <span class="math math-inline">p(x|z)</span>는 잠재 변수 <span class="math math-inline">z</span>가 주어졌을 때 데이터 <span class="math math-inline">x</span>가 생성될 조건부 확률, 즉 디코더에 해당한다. 이 적분은 잠재 공간 전체에 대해 수행되어야 한다. 잠재 변수 <span class="math math-inline">z</span>의 차원이 조금만 높아져도 이 적분은 해석적으로 풀 수 없으며, 수치적으로 계산하는 것 또한 거의 불가능하다. 이러한 문제를 ’다루기 힘들다(intractable)’고 표현한다.17</p>
<p>이 문제와 직결된 또 다른 난관은 잠재 변수의 사후 확률(posterior probability) <span class="math math-inline">p(z|x)</span>를 계산하는 것이다. 베이즈 정리(Bayes’ rule)에 따르면 사후 확률은 다음과 같이 표현된다.17<br />
<span class="math math-display">
p(z|x) = \frac{p(x|z)p(z)}{p(x)}
</span><br />
분모에 intractable한 <span class="math math-inline">p(x)</span>가 포함되어 있기 때문에, 사후 확률 <span class="math math-inline">p(z|x)</span> 역시 직접 계산하는 것이 불가능하다. 사후 확률은 주어진 데이터 <span class="math math-inline">x</span>를 가장 잘 설명하는 잠재 변수 <span class="math math-inline">z</span>가 무엇인지에 대한 정보를 담고 있으므로, 이를 계산할 수 없다는 것은 모델 학습에 있어 심각한 장애물이다. VAE는 이 두 가지 intractability 문제를 해결하기 위해 변분 추론이라는 강력한 근사 기법을 도입한다.</p>
<h3>3.2  변분 추론과 증거 하한(ELBO)의 유도</h3>
<p>변분 추론(Variational Inference)은 다루기 힘든 실제 사후 확률 분포 <span class="math math-inline">p(z|x)</span>를, 다루기 쉬운 간단한 확률 분포 <span class="math math-inline">q_\phi(z|x)</span>(예: 정규분포)로 근사하는 방법론이다.15 여기서 <span class="math math-inline">q_\phi(z|x)</span>는 인코더 네트워크에 해당하며, <span class="math math-inline">\phi</span>는 인코더의 파라미터이다. 변분 추론의 목표는 근사 분포 <span class="math math-inline">q_\phi(z|x)</span>를 실제 사후 분포 <span class="math math-inline">p(z|x)</span>와 최대한 유사하게 만드는 것이다. 두 분포 사이의 유사성은 쿨백-라이블러 발산(Kullback-Leibler Divergence)으로 측정하며, 따라서 우리의 목표는 이 KL 발산을 최소화하는 것이다.30<br />
<span class="math math-display">
\min_\phi D_{KL}(q_\phi(z|x) \parallel p(z|x))
</span><br />
KL 발산의 정의(<span class="math math-inline">D_{KL}(q \parallel p) = \mathbb{E}_{z \sim q}[\log q(z) - \log p(z)]</span>)를 이용하여 위 식을 전개하면, 데이터의 로그 가능도 <span class="math math-inline">\log p(x)</span>에 대한 매우 중요한 관계식을 유도할 수 있다.24<br />
<span class="math math-display">
\begin{align*}
D_{KL}(q_\phi(z|x) \parallel p(z|x)) &amp;= \mathbb{E}_{z \sim q_\phi(z|x)}[\log q_\phi(z|x) - \log p(z|x)] \\
&amp;= \mathbb{E}_{z \sim q_\phi(z|x)}[\log q_\phi(z|x) - \log \frac{p(x|z)p(z)}{p(x)}] \\
&amp;= \mathbb{E}_{z \sim q_\phi(z|x)}[\log q_\phi(z|x) - \log p(x|z) - \log p(z) + \log p(x)] \\
&amp;= \mathbb{E}_{z \sim q_\phi(z|x)}[\log q_\phi(z|x) - \log p(z)] - \mathbb{E}_{z \sim q_\phi(z|x)}[\log p(x|z)] + \log p(x) \\
&amp;= D_{KL}(q_\phi(z|x) \parallel p(z)) - \mathbb{E}_{z \sim q_\phi(z|x)}[\log p(x|z)] + \log p(x)
\end{align*}
</span><br />
이 식을 <span class="math math-inline">\log p(x)</span>에 대해 정리하면 다음과 같다.<br />
<span class="math math-display">
\log p(x) = D_{KL}(q_\phi(z|x) \parallel p(z|x)) - D_{KL}(q_\phi(z|x) \parallel p(z)) + \mathbb{E}_{z \sim q_\phi(z|x)}[\log p_\theta(x|z)]
</span><br />
여기서 마지막 두 항을 묶어 <span class="math math-inline">\mathcal{L}(\phi, \theta; x)</span>라고 정의하면,<br />
<span class="math math-display">
\log p(x) = D_{KL}(q_\phi(z|x) \parallel p(z|x)) + \mathcal{L}(\phi, \theta; x)
</span><br />
이때 <span class="math math-inline">\mathcal{L}(\phi, \theta; x)</span>를 <strong>증거 하한(Evidence Lower Bound, ELBO)</strong> 이라고 부른다. KL 발산 값은 항상 0보다 크거나 같으므로(<span class="math math-inline">D_{KL} \ge 0</span>), <span class="math math-inline">\log p(x) \ge \mathcal{L}(\phi, \theta; x)</span> 라는 부등식이 항상 성립한다.30 즉, ELBO는 우리가 최대화하고자 하는 로그 가능도 <span class="math math-inline">\log p(x)</span>의 하한선(lower bound)이 된다. 위 식에서 <span class="math math-inline">\log p(x)</span>는 주어진 데이터에 대해 상수이므로, ELBO(<span class="math math-inline">\mathcal{L}</span>)를 최대화하는 것은 다루기 힘든 KL 발산 <span class="math math-inline">D_{KL}(q_\phi(z|x) \parallel p(z|x))</span>을 최소화하는 것과 동일한 효과를 가진다. 따라서 VAE는 직접 최적화하기 어려운 <span class="math math-inline">\log p(x)</span>를 최대화하는 대신, 다루기 쉬운 대리 목표(surrogate objective)인 ELBO를 최대화하는 방향으로 학습을 진행한다.26</p>
<p>ELBO는 다음과 같이 두 가지 의미 있는 항으로 분해될 수 있다.8<br />
<span class="math math-display">
\mathcal{L}(\phi, \theta; x) = \underbrace{\mathbb{E}_{z \sim q_\phi(z|x)}[\log p_\theta(x|z)]}_{\text{Reconstruction Term}} - \underbrace{D_{KL}(q_\phi(z|x) \parallel p(z))}_{\text{Regularization Term}}
</span></p>
<ol>
<li><strong>재구성 항 (Reconstruction Term):</strong> 첫 번째 항 <span class="math math-inline">\mathbb{E}_{z \sim q_\phi(z|x)}[\log p_\theta(x|z)]</span>는 인코더가 만든 잠재 분포 <span class="math math-inline">q_\phi(z|x)</span>에서 샘플링한 잠재 변수 <span class="math math-inline">z</span>를 디코더 <span class="math math-inline">p_\theta(x|z)</span>에 입력했을 때, 원본 데이터 <span class="math math-inline">x</span>가 나타날 로그 확률의 기댓값이다. 이는 디코더가 원본 입력을 얼마나 잘 복원하는지를 측정하는 항으로, 이 값을 최대화하는 것은 재구성 오차를 최소화하는 것과 같다. 이 때문에 ’재구성 손실(reconstruction loss)’의 음수 값으로 해석되기도 한다.20</li>
<li><strong>규제 항 (Regularization Term):</strong> 두 번째 항 <span class="math math-inline">- D_{KL}(q_\phi(z|x) \parallel p(z))</span>는 인코더가 만든 근사 사후 분포 <span class="math math-inline">q_\phi(z|x)</span>가 사전 분포 <span class="math math-inline">p(z)</span>(보통 표준 정규분포 <span class="math math-inline">\mathcal{N}(0, I)</span>)와 얼마나 다른지를 측정하는 KL 발산에 음수를 취한 것이다. ELBO를 최대화하려면 이 KL 발산 값을 최소화해야 한다. 이는 인코더가 출력하는 잠재 분포가 사전 분포의 형태를 따르도록 강제하는 규제 역할을 하여, 잠재 공간이 부드럽고 잘 구조화되도록 만든다.20</li>
</ol>
<p>결국 VAE의 학습은 이 두 항 사이의 균형을 찾는 과정이다. 재구성 항은 모델이 데이터의 세부 사항을 잘 포착하도록(data fidelity) 유도하고, 규제 항은 모델이 과적합되는 것을 방지하고 잠재 공간의 일반화 성능을 높이도록(model regularity) 유도한다. 이 두 가지 상충하는 목표 사이의 절묘한 균형이 VAE의 강력한 생성 능력의 원천이 된다.</p>
<h3>3.3  재매개변수화 기법(Reparameterization Trick): 미분 가능한 샘플링</h3>
<p>VAE의 목적 함수인 ELBO를 경사 하강법으로 최적화하기 위해서는 목적 함수를 모델 파라미터(<span class="math math-inline">\phi, \theta</span>)에 대해 미분할 수 있어야 한다. 그러나 ELBO의 재구성 항 <span class="math math-inline">\mathbb{E}_{z \sim q_\phi(z|x)}[\log p_\theta(x|z)]</span>에는 심각한 문제가 있다. 바로 기댓값을 계산하기 위한 샘플링 과정 <span class="math math-inline">z \sim q_\phi(z|x)</span> 때문이다. 샘플링은 본질적으로 확률적인 연산으로, 특정 지점에서 미분 계수를 정의할 수 없는 미분 불가능(non-differentiable)한 과정이다.33 이로 인해 디코더에서 계산된 손실의 그래디언트가 샘플링 노드를 거슬러 올라가 인코더의 파라미터</p>
<p><span class="math math-inline">\phi</span>까지 전달될 수 없다. 즉, 역전파(backpropagation)가 불가능하다.24</p>
<p>이 문제를 해결하기 위해 VAE는 <strong>재매개변수화 기법(reparameterization trick)</strong> 이라는 독창적인 방법을 사용한다.33 이 기법의 핵심 아이디어는 확률 변수 <span class="math math-inline">z</span>의 샘플링 과정을 두 부분으로 분리하는 것이다: (1) 모델 파라미터와 무관한 고정된 분포에서 노이즈를 샘플링하는 확률적 부분과, (2) 이 노이즈를 모델 파라미터를 이용해 변환하는 결정론적 부분.35</p>
<p>예를 들어, 우리가 <span class="math math-inline">z</span>를 평균이 <span class="math math-inline">\mu</span>이고 표준편차가 <span class="math math-inline">\sigma</span>인 정규분포 <span class="math math-inline">\mathcal{N}(\mu, \sigma^2)</span>에서 샘플링하고자 할 때, 재매개변수화 기법은 다음과 같이 <span class="math math-inline">z</span>를 계산한다.</p>
<ol>
<li>먼저, 파라미터와 무관한 표준 정규분포에서 노이즈 변수 <span class="math math-inline">\epsilon</span>을 샘플링한다: <span class="math math-inline">\epsilon \sim \mathcal{N}(0, I)</span>.</li>
<li>그 다음, 이 <span class="math math-inline">\epsilon</span>을 인코더가 출력한 <span class="math math-inline">\mu</span>와 <span class="math math-inline">\sigma</span>를 이용해 결정론적인 함수로 변환한다: <span class="math math-inline">z = \mu + \sigma \cdot \epsilon</span>.11</li>
</ol>
<p>이 두 과정은 수학적으로 <span class="math math-inline">z \sim \mathcal{N}(\mu, \sigma^2)</span>에서 직접 샘플링하는 것과 동일한 결과를 낳는다. 하지만 결정적인 차이가 있다. 이제 확률적인 샘플링 과정은 모델의 계산 그래프 외부에서 입력으로 주어지는 <span class="math math-inline">\epsilon</span>으로 분리되었고, 잠재 변수 <span class="math math-inline">z</span>는 파라미터 <span class="math math-inline">\mu</span>와 <span class="math math-inline">\sigma</span>에 대해 미분 가능한 결정론적 함수가 되었다. 따라서 디코더에서 계산된 손실의 그래디언트는 이제 <span class="math math-inline">z</span>를 거쳐 <span class="math math-inline">\mu</span>와 <span class="math math-inline">\sigma</span>로, 그리고 최종적으로 인코더의 파라미터 <span class="math math-inline">\phi</span>까지 아무런 문제 없이 역전파될 수 있다.24</p>
<p>이처럼 재매개변수화 기법은 VAE의 이론적 토대인 변분 추론을 현대 딥러닝의 강력한 최적화 도구인 경사 하강법 및 역전파와 결합시켜주는 결정적인 다리 역할을 한다. 이 기법이 없었다면 VAE를 종단간(end-to-end) 방식으로 효율적으로 훈련하는 것은 불가능했을 것이다. VAE의 성공은 변분 추론이라는 우아한 이론적 프레임워크와 재매개변수화 기법이라는 실용적인 공학적 해결책의 완벽한 결합에 기인한다고 할 수 있다.</p>
<h2>4. 심층 분석: VAE의 주요 특징과 한계</h2>
<h3>4.1  생성 이미지의 흐릿함(Blurriness) 문제와 그 원인</h3>
<p>변이형 오토인코더는 강력한 생성 모델이지만, 특히 생성적 적대 신경망(GAN)과 비교했을 때 생성된 이미지가 종종 흐릿하거나(blurry) 뭉개져 보이는(smudged) 경향이 있다는 한계점을 지닌다.28 이러한 현상은 단일 원인이 아닌, VAE의 구조와 학습 방식에 내재된 여러 요인이 복합적으로 작용한 결과이다.</p>
<p>첫 번째 주요 원인은 **픽셀 단위 손실 함수(pixel-wise loss function)**의 사용이다. VAE의 재구성 항은 일반적으로 각 픽셀의 차이를 측정하는 평균 제곱 오차(Mean Squared Error, MSE)나 이진 교차 엔트로피(Binary Cross-Entropy, BCE)를 손실 함수로 사용한다.10 이러한 손실 함수는 이미지의 전체적인 구조, 텍스처, 또는 인간의 시각적 인식과 관련된 지각적 품질(perceptual quality)을 고려하지 않는다. 대신, 픽셀 값의 평균적인 차이를 최소화하는 데에만 집중한다. 예를 들어, 실제 이미지가 여러 가능한 형태(mode)를 가질 때(가령, 약간씩 다르게 쓰인 손글씨 숫자 ‘7’), MSE를 최소화하는 가장 쉬운 방법은 이 모든 형태들의 ’평균’에 해당하는 이미지를 생성하는 것이다. 이미지 공간에서 평균을 취하는 연산은 일반적으로 날카로운 경계나 세밀한 디테일을 뭉개는 효과를 낳기 때문에, 결과적으로 흐릿한 이미지가 생성된다.38</p>
<p>두 번째 원인은 <strong>잠재 공간의 연속성과 정규분포 가정</strong>에 있다. VAE는 KL 발산 규제를 통해 잠재 공간이 연속적인 정규분포를 따르도록 강제한다. 이는 데이터 생성의 다양성과 안정성을 확보하는 데에는 유리하지만, 디코더가 이 연속적인 공간의 평균적인 특성을 학습하게 만든다.40 즉, 디코더는 잠재 공간의 특정 지점 하나하나에 대해 매우 날카로운 이미지를 생성하기보다는, 넓은 영역에 걸쳐 전반적으로 그럴듯한(plausible) 이미지를 생성하는 ‘안전한’ 전략을 택하게 된다. 이는 모델이 데이터 분포 전체를 부드럽게 덮으려는 시도가 오히려 개별 샘플의 선명도를 희생시키는 결과로 이어진다.41</p>
<p>세 번째 원인은 **정보 병목 현상(information bottleneck)**이다. VAE의 인코더는 고차원의 원본 이미지 데이터를 저차원의 잠재 공간으로 압축한다. 이 과정에서 필연적으로 정보의 손실이 발생한다. 특히, 이미지의 미세한 텍스처나 작은 글씨와 같은 고주파(high-frequency) 성분의 정보는 압축 과정에서 쉽게 사라지는 경향이 있다.41 디코더는 이 정보가 부족한 잠재 벡터로부터 이미지를 복원해야 하므로, 저주파(low-frequency) 성분의 전반적인 구조 위주로 복원하게 되고, 이는 결과적으로 흐릿한 이미지로 나타난다.37 잠재 공간의 차원을 늘리면 재구성 품질이 향상될 수는 있지만, 이는 모델의 복잡도를 높이고 과적합의 위험을 증가시킬 수 있다.10</p>
<h3>4.2  후방 붕괴(Posterior Collapse) 문제</h3>
<p>후방 붕괴(Posterior Collapse)는 VAE, 특히 텍스트나 시계열 데이터와 같이 순차적인 정보를 다루는 모델에서 자주 발생하는 심각한 학습 실패 현상이다.43 이 현상은 인코더가 출력하는 근사 사후 분포 <span class="math math-inline">q_\phi(z|x)</span>가 입력 데이터 <span class="math math-inline">x</span>와 무관하게 사전 분포 <span class="math math-inline">p(z)</span>와 거의 동일해져 버리는 것을 의미한다.45</p>
<p>후방 붕괴가 발생하면, VAE의 손실 함수 중 KL 발산 항 <span class="math math-inline">D_{KL}(q_\phi(z|x) \parallel p(z))</span>은 0에 가깝게 수렴하여 최적화된 것처럼 보이지만, 실제로는 모델이 잠재 변수를 전혀 활용하지 못하는 상태에 빠진 것이다.47 잠재 변수 <span class="math math-inline">z</span>가 입력 <span class="math math-inline">x</span>에 대한 어떤 정보도 담지 못하기 때문에, 디코더는 <span class="math math-inline">z</span>를 완전히 무시하고 오직 이전에 생성된 출력에만 의존하여 다음 출력을 예측하게 된다. 결과적으로 모델은 잠재 공간의 의미 있는 표현을 학습하지 못하고, 단순한 언어 모델처럼 작동하게 된다.43</p>
<p>이 문제의 주된 원인은 <strong>강력한 자기회귀(autoregressive) 디코더</strong>의 사용에 있다. LSTM이나 Transformer와 같은 자기회귀 모델은 이전 타임스텝의 출력을 다음 타임스텝의 입력으로 사용하여 순차적인 데이터를 매우 효과적으로 모델링할 수 있다. 만약 디코더의 성능이 너무 강력하여 잠재 변수 <span class="math math-inline">z</span>의 도움 없이도 훈련 데이터를 거의 완벽하게 재구성할 수 있다면, 모델은 굳이 <span class="math math-inline">x</span>에 대한 정보를 <span class="math math-inline">z</span>에 인코딩하려는 노력을 할 필요가 없어진다. 대신, 최적화 과정에서 더 다루기 쉬운 KL 발산 항을 0으로 만드는, 즉 <span class="math math-inline">q_\phi(z|x)</span>를 <span class="math math-inline">p(z)</span>로 수렴시키는 ’쉬운 길’을 택하게 된다.44</p>
<p>후방 붕괴 문제를 완화하기 위해 여러 가지 기법들이 제안되었다. 대표적인 방법으로는 **KL 비용 어닐링(KL cost annealing)**이 있다. 이 방법은 훈련 초반에는 KL 발산 항의 가중치를 0에 가깝게 두어 모델이 재구성에 집중하게 하고, 훈련이 진행됨에 따라 가중치를 점진적으로 1까지 증가시켜 잠재 변수를 활용하도록 유도하는 기법이다.49 또 다른 방법으로는 **단어 드롭아웃(word dropout)**이 있다. 이는 자기회귀 디코더의 입력으로 들어가는 이전 타임스텝의 정답 단어를 일정 확률로 제거하여, 디코더가 부족한 정보를 채우기 위해 잠재 변수 <span class="math math-inline">z</span>에 더 의존하도록 강제하는 방식이다.49 이 외에도 디코더의 구조를 약화시키거나, 손실 함수를 변형하는 등의 다양한 연구가 진행되고 있다.</p>
<h3>4.3  생성적 적대 신경망(GAN)과의 비교 분석</h3>
<p>VAE와 함께 현대 생성 모델의 양대 산맥을 이루는 생성적 적대 신경망(Generative Adversarial Network, GAN)과의 비교는 VAE의 특징과 한계를 더욱 명확하게 이해하는 데 도움을 준다.50 두 모델은 새로운 데이터를 생성한다는 공통된 목표를 가지지만, 그 철학과 접근 방식은 극명하게 대조된다.</p>
<p>VAE가 확률 분포를 명시적으로 모델링하고 최대 가능도 추정(maximum likelihood estimation)을 통해 학습하는 추론 기반(inference-based) 모델이라면, GAN은 생성자(generator)와 판별자(discriminator)라는 두 네트워크가 서로 경쟁하는 게임 이론 기반(game-theoretic)의 모델이다.51 생성자는 실제 데이터와 유사한 가짜 데이터를 만들고, 판별자는 주어진 데이터가 실제인지 가짜인지를 구별하도록 학습한다. 이 둘의 적대적 경쟁을 통해 생성자는 점차 실제와 구분하기 어려운 고품질의 데이터를 생성하게 된다.51</p>
<p>이러한 근본적인 차이는 두 모델의 장단점으로 이어진다. VAE는 학습 과정이 안정적이고, 데이터의 전체적인 분포를 학습하므로 생성된 샘플의 다양성이 높다. 또한, 잘 구조화된 잠재 공간을 가지고 있어 의미론적 보간이나 특징 조작이 용이하다.28 하지만 앞서 언급했듯이 생성된 이미지가 흐릿하다는 단점이 있다. 반면, GAN은 판별자라는 ’심판’이 지각적 현실성(perceptual realism)을 기준으로 생성자를 훈련시키기 때문에 매우 선명하고 사실적인 이미지를 생성할 수 있다.36 그러나 생성자와 판별자 사이의 균형을 맞추기가 매우 어려워 훈련이 불안정하며, 생성자가 판별자를 속이기 쉬운 몇 가지 데이터 형태만 반복적으로 생성하는 ‘모드 붕괴(mode collapse)’ 현상이 발생하기 쉽다.40</p>
<p>이러한 특성은 두 모델이 서로 다른 방식으로 ’어려움 보존의 법칙’을 따르고 있음을 시사한다. 즉, 생성 모델링이라는 근본적으로 어려운 문제를 푸는 데 있어 쉬운 길은 없다는 것이다. VAE는 명시적인 확률 모델링을 통해 학습의 안정성과 다양성을 얻는 대신, 픽셀 단위 손실 함수의 한계로 인해 생성물의 선명도를 희생한다. GAN은 적대적 학습을 통해 선명도를 얻는 대신, 학습의 안정성과 생성물의 다양성을 희생한다. 이처럼 두 모델의 장단점은 독립적인 버그가 아니라, 각자의 기본 철학에서 비롯된 필연적인 결과이다. 이 본질적인 트레이드오프 관계를 이해하는 것은 두 모델을 적재적소에 활용하고, 나아가 두 모델의 장점을 결합하려는 VAE-GAN 28이나 확산 모델(Diffusion Models) 40과 같은 차세대 생성 모델의 등장 배경을 이해하는 데 매우 중요하다.</p>
<p><strong>Table 2: 변이형 오토인코더(VAE)와 생성적 적대 신경망(GAN) 비교</strong></p>
<table><thead><tr><th>특징 (Feature)</th><th>변이형 오토인코더 (Variational Autoencoder, VAE)</th><th>생성적 적대 신경망 (Generative Adversarial Network, GAN)</th></tr></thead><tbody>
<tr><td><strong>아키텍처</strong></td><td>인코더(Encoder)와 디코더(Decoder) 51</td><td>생성자(Generator)와 판별자(Discriminator) 51</td></tr>
<tr><td><strong>학습 목표</strong></td><td>증거 하한(ELBO) 최대화 (확률적 재구성) 51</td><td>Minimax 게임: 생성자는 판별자를 속이고, 판별자는 진짜와 가짜를 구별 (적대적 학습) 51</td></tr>
<tr><td><strong>손실 함수</strong></td><td>재구성 손실 + KL 발산 52</td><td>생성자 손실 + 판별자 손실 (보통 Binary Cross-Entropy) 52</td></tr>
<tr><td><strong>생성 이미지 품질</strong></td><td>흐릿하지만 다양성이 높음 (Blurry but diverse) 36</td><td>선명하지만 다양성이 낮을 수 있음 (Sharp but prone to mode collapse) 36</td></tr>
<tr><td><strong>학습 안정성</strong></td><td>상대적으로 안정적이고 수렴이 쉬움 52</td><td>불안정하고, 훈련이 어려움. 모드 붕괴(Mode Collapse) 문제 발생 가능 40</td></tr>
<tr><td><strong>잠재 공간</strong></td><td>잘 구조화되어 보간, 특징 조작에 용이함 28</td><td>명시적인 인코더가 없어 구조화되어 있지 않음. 잠재 공간 탐색이 어려움 39</td></tr>
<tr><td><strong>평가</strong></td><td>로그 가능도(Log-likelihood)로 평가 가능</td><td>평가가 어려움. 주로 시각적 품질이나 FID 같은 간접적 지표 사용 53</td></tr>
</tbody></table>
<h2>5. 변이형 오토인코더의 확장과 응용</h2>
<p>VAE의 기본 프레임워크는 그 유연성과 이론적 명확성 덕분에 다양한 방향으로 확장되어 왔다. 이러한 확장 모델들은 VAE의 고유한 한계를 극복하거나 특정 목적에 맞게 기능을 강화하는 것을 목표로 한다. VAE의 진화는 VAE 프레임워크의 각 구성 요소(입력, 손실 함수, 잠재 공간 구조)를 체계적으로 수정하여 이루어졌다. 이는 VAE가 모듈식 설계를 가지고 있어 특정 문제를 해결하기 위한 ’외과적 수술’과 같은 정밀한 개선이 가능함을 보여준다.</p>
<h3>5.1  조건부 VAE (Conditional VAE, CVAE): 제어 가능한 생성</h3>
<p>기본적인 VAE는 비조건부(unconditional) 생성 모델로, 어떤 데이터가 생성될지 사용자가 제어할 수 없다. **조건부 VAE(Conditional VAE, CVAE)**는 이러한 한계를 극복하기 위해 제안된 모델이다.54 CVAE는 생성 과정에 조건(condition) 변수 <span class="math math-inline">c</span>를 추가하여 사용자가 원하는 특성을 가진 데이터를 생성할 수 있도록 한다. 이 조건 변수는 클래스 레이블, 텍스트 설명, 또는 다른 이미지 등 다양한 형태의 정보가 될 수 있다.54</p>
<p>CVAE의 구조는 기본 VAE와 유사하지만, 조건 변수 <span class="math math-inline">c</span>가 인코더와 디코더 양쪽에 추가적인 입력으로 제공된다는 점이 다르다. 인코더는 입력 데이터 <span class="math math-inline">x</span>와 조건 <span class="math math-inline">c</span>를 함께 받아 조건부 잠재 분포 <span class="math math-inline">q_\phi(z|x, c)</span>를 학습하고, 디코더는 잠재 변수 <span class="math math-inline">z</span>와 조건 <span class="math math-inline">c</span>를 함께 받아 조건부 데이터 분포 <span class="math math-inline">p_\theta(x|z, c)</span>를 학습한다.56</p>
<p>이러한 구조 덕분에 CVAE는 목표 지향적인(targeted) 생성이 가능해진다. 예를 들어, MNIST 손글씨 데이터셋을 학습할 때 각 숫자의 레이블을 조건으로 부여하면, 사용자는 ’숫자 7을 생성하라’는 명령을 통해 명확하게 숫자 7 이미지만을 생성할 수 있다.58 또한, 이미지 캡션을 조건으로 주어 특정 설명에 부합하는 이미지를 생성하거나, 스케치 이미지를 조건으로 주어 사실적인 채색 이미지로 변환하는 등의 복잡한 이미지 대 이미지 변환(image-to-image translation) 작업도 수행할 수 있다.55 이처럼 CVAE는 VAE의 생성 능력에 ’제어’라는 중요한 차원을 더한 모델이다.</p>
<h3>5.2  β-VAE: 분리된 표현 학습 (Disentangled Representation Learning)</h3>
<p>VAE의 잠재 공간은 연속적이고 부드럽지만, 각 잠재 차원이 데이터의 어떤 의미론적 특징(semantic feature)에 해당하는지는 명확하지 않다. 예를 들어, 얼굴 이미지를 학습했을 때 잠재 변수의 첫 번째 차원이 ’미소의 정도’를, 두 번째 차원이 ’머리카락의 길이’를 나타내도록 학습된다는 보장이 없다. 이러한 표현을 ‘얽힌(entangled)’ 표현이라고 한다.</p>
<p><strong>β-VAE</strong>는 잠재 공간에서 데이터의 생성 요인(generative factors)을 서로 독립적인 차원으로 분리하여 학습하는, 즉 <strong>분리된 표현(disentangled representation)</strong> 학습을 목표로 하는 모델이다.60 이를 위해 β-VAE는 VAE의 ELBO 목적 함수를 다음과 같이 수정한다.61<br />
<span class="math math-display">
\mathcal{L} = \mathbb{E}_{z \sim q_\phi(z|x)}[\log p_\theta(x|z)] - \beta D_{KL}(q_\phi(z|x) \parallel p(z))
</span><br />
기존 VAE의 목적 함수에서 KL 발산 항에 하이퍼파라미터 <span class="math math-inline">\beta</span>가 곱해진 형태이다. 여기서 <span class="math math-inline">\beta=1</span>이면 원래의 VAE와 동일하다. 만약 <span class="math math-inline">\beta &gt; 1</span>로 설정하면, 모델은 KL 발산 항을 최소화하는 데 더 큰 가중치를 두게 된다. 이는 인코더가 출력하는 잠재 분포 <span class="math math-inline">q_\phi(z|x)</span>를 사전 분포 <span class="math math-inline">p(z)</span>(독립적인 다변량 정규분포)에 더욱 강력하게 맞추도록 강제하는 효과를 낳는다. 이 강력한 규제가 잠재 변수들이 서로 통계적으로 독립적인 방향으로 학습되도록 유도하여, 각 차원이 데이터의 독립적인 생성 요인(예: 색상, 크기, 회전 등)을 포착하게 만든다.60</p>
<p>그러나 이러한 분리된 표현 학습에는 대가가 따른다. <span class="math math-inline">\beta</span> 값을 높여 잠재 공간에 대한 규제를 강화할수록, 잠재 변수가 담을 수 있는 정보의 양은 줄어든다(정보 병목 현상 심화). 이로 인해 디코더가 원본 이미지를 재구성하는 데 필요한 세부 정보를 잃게 되어, 재구성 품질이 저하되는 트레이드오프(trade-off) 관계가 존재한다.62 따라서 실제 적용 시에는 분리 성능과 재구성 품질 사이의 적절한 균형점을 찾는 것이 중요하다.</p>
<h3>5.3  벡터 양자화 VAE (Vector Quantized VAE, VQ-VAE): 이산적 잠재 공간과 선명한 이미지</h3>
<p>VAE의 고질적인 문제인 흐릿한 이미지 생성을 해결하기 위한 혁신적인 접근법 중 하나가 **벡터 양자화 VAE(Vector Quantized VAE, VQ-VAE)**이다.63 VQ-VAE의 핵심 아이디어는 VAE의 연속적인(continuous) 잠재 공간을 이산적인(discrete) 잠재 공간으로 대체하는 것이다.64</p>
<p>VQ-VAE의 인코더는 일반적인 CNN 구조를 통해 입력 이미지 <span class="math math-inline">x</span>로부터 연속적인 특징 맵 <span class="math math-inline">z_e(x)</span>를 출력한다. 그 다음, VQ-VAE의 핵심인 <strong>벡터 양자화(Vector Quantization)</strong> 단계가 수행된다. 이 단계에서는 미리 정의된 학습 가능한 임베딩 벡터 집합, 즉 <strong>코드북(codebook)</strong> <span class="math math-inline">E = \{e_1, e_2,..., e_K\}</span>을 사용한다. 인코더 출력 <span class="math math-inline">z_e(x)</span>의 각 벡터는 코드북에 있는 모든 임베딩 벡터 <span class="math math-inline">e_k</span>와의 거리를 계산하여, 가장 가까운 임베딩 벡터로 대체(양자화)된다.65 이 양자화된 벡터들의 집합 <span class="math math-inline">z_q(x)</span>가 최종적으로 디코더의 입력으로 사용된다.67</p>
<p>이 과정에는 ’가장 가까운 벡터 찾기’라는 미분 불가능한 연산(argmin)이 포함되어 있어, 일반적인 역전파 방식으로는 인코더를 학습시킬 수 없다. VQ-VAE는 이 문제를 해결하기 위해 <strong>Straight-Through Estimator</strong>라는 기법을 사용한다. 이는 순전파 시에는 양자화를 수행하지만, 역전파 시에는 양자화 과정을 건너뛰고 디코더로부터 온 그래디언트를 양자화된 벡터 <span class="math math-inline">z_q(x)</span>에서 인코더의 출력 <span class="math math-inline">z_e(x)</span>로 그대로 복사하는 방식이다.67</p>
<p>VQ-VAE의 손실 함수는 세 가지 항으로 구성된다.64</p>
<ol>
<li><strong>재구성 손실(Reconstruction Loss):</strong> 디코더가 양자화된 잠재 벡터로부터 원본 이미지를 얼마나 잘 복원하는지를 측정한다.</li>
<li><strong>코드북 손실(Codebook Loss):</strong> 코드북의 임베딩 벡터 <span class="math math-inline">e_k</span>들이 인코더의 출력 <span class="math math-inline">z_e(x)</span>에 가까워지도록 업데이트한다. 이는 코드북이 데이터의 유용한 특징들을 학습하도록 한다.</li>
<li><strong>전념 손실(Commitment Loss):</strong> 인코더의 출력 <span class="math math-inline">z_e(x)</span>가 선택된 코드북 벡터에 ’전념(commit)’하도록, 즉 너무 멀리 벗어나지 않도록 규제한다. 이는 인코더와 코드북의 학습 속도를 조절하고 안정성을 높인다.</li>
</ol>
<p>이산적인 잠재 표현을 사용함으로써 VQ-VAE는 연속적인 공간에서 발생하는 평균화 효과를 근본적으로 방지하고, GAN에 필적할 만큼 선명하고 고품질의 이미지를 생성할 수 있다.28 또한, KL 발산 항이 없어 후방 붕괴 문제로부터 비교적 자유롭다는 장점도 있다.63</p>
<h3>5.4  주요 응용 분야</h3>
<p>VAE의 독특한 확률적 잠재 공간 모델링 능력은 단순한 이미지 생성을 넘어 다양한 분야에서 실용적인 응용을 가능하게 한다.</p>
<ul>
<li><strong>이상 탐지 (Anomaly Detection):</strong> VAE는 정상 데이터의 복잡한 분포를 학습하는 데 매우 효과적이다. 정상 데이터만으로 VAE를 훈련시킨 후, 새로운 데이터를 입력했을 때 발생하는 재구성 오차를 측정한다. 만약 입력 데이터가 훈련 시 보았던 정상 데이터의 분포에서 크게 벗어나는 비정상(anomalous) 데이터라면, 모델은 이를 제대로 재구성하지 못하고 큰 재구성 오차를 발생시킨다. 이 원리를 이용하여 특정 임계값을 넘는 재구성 오차를 가진 데이터를 이상치로 탐지할 수 있다.70 이 방법은 금융 거래에서의 사기 탐지, 산업 설비의 고장 예측, 의료 영상에서의 종양 발견 등 레이블이 없는 데이터에서 희귀한 이벤트를 찾아내는 데 널리 사용된다.70</li>
<li><strong>데이터 증강 및 압축 (Data Augmentation &amp; Compression):</strong> 머신러닝 모델의 성능은 훈련 데이터의 양과 질에 크게 좌우된다. 데이터가 부족한 경우, 잘 훈련된 VAE는 학습된 잠재 공간에서 새로운 샘플들을 생성하여 훈련 데이터셋을 인위적으로 늘리는 데이터 증강(data augmentation)에 활용될 수 있다.2 한편, VAE의 인코더는 고차원 데이터를 저차원의 의미 있는 잠재 벡터로 압축하는 효율적인 비선형 압축기로 기능할 수 있다. 이는 정보이론의 손실 압축(lossy compression)과도 깊은 이론적 연관성을 가지며, 실제로 VAE 아키텍처는 최신 이미지 및 비디오 압축 기술 개발에 영감을 주고 있다.16</li>
<li><strong>준지도 학습 (Semi-Supervised Learning):</strong> 많은 실제 문제에서는 소수의 레이블된 데이터와 대량의 레이블되지 않은 데이터가 함께 존재한다. 준지도 학습은 이 두 종류의 데이터를 모두 활용하여 모델 성능을 높이는 것을 목표로 한다. VAE는 이러한 시나리오에 매우 적합하다. 먼저 대량의 레이블되지 않은 데이터를 사용하여 VAE를 훈련시켜 데이터의 기저 구조를 반영하는 강력한 잠재 공간을 학습한다. 그 다음, 이 VAE의 인코더를 특징 추출기로 사용하여 모든 데이터를 저차원의 잠재 벡터로 변환하고, 이 잠재 벡터를 입력으로 하여 소수의 레이블된 데이터만으로 분류기를 학습시킨다.22 레이블되지 않은 데이터를 통해 학습된 풍부한 잠재 표현이 분류 작업에 매우 유용한 정보를 제공하기 때문에, 적은 레이블만으로도 높은 성능의 모델을 구축할 수 있다.77</li>
</ul>
<h2>6. 결론: 변이형 오토인코더의 현재와 미래</h2>
<h3>6.1  VAE의 기여와 의의 요약</h3>
<p>변이형 오토인코더(VAE)는 딥러닝 시대에 베이즈 변분 추론이라는 통계적 원리를 성공적으로 접목시킨 선구적인 생성 모델이다. VAE의 등장은 생성 모델링 분야에 중요한 패러다임 전환을 가져왔다. 이전의 모델들이 단순히 데이터의 표면적인 형태를 모방하는 데 그쳤다면, VAE는 데이터의 기저에 있는 확률 분포를 명시적으로 모델링하고 학습할 수 있는 길을 열었다.</p>
<p>가장 큰 기여는 ’확률적 잠재 공간’이라는 개념을 도입하고 이를 실용적으로 구현했다는 점이다. 결정론적 매핑에 의존했던 표준 오토인코더의 한계를 극복하고, 연속적이고 잘 구조화된 잠재 공간을 통해 의미 있는 데이터 생성과 조작을 가능하게 했다. 이는 VAE가 단순한 재구성 도구를 넘어 진정한 생성 모델로서 기능하게 된 근본적인 이유이다. 또한, ELBO라는 명확한 목적 함수와 재매개변수화 기법이라는 독창적인 최적화 해법을 제시함으로써, 복잡한 확률 모델을 표준적인 딥러닝 프레임워크 내에서 효율적으로 학습시킬 수 있음을 증명했다. 이러한 이론적 우아함과 실용적 유연성 덕분에 VAE는 이상 탐지, 데이터 증강, 준지도 학습 등 다양한 응용 분야의 발전을 촉진하는 핵심 기술로 자리매김했다.</p>
<h3>6.2  현재의 도전 과제와 연구 동향</h3>
<p>VAE는 많은 기여에도 불구하고 여전히 해결해야 할 도전 과제들을 안고 있다. 생성된 이미지의 ’흐릿함’과 강력한 디코더 사용 시 발생하는 ’후방 붕괴’는 VAE의 성능을 제약하는 고전적인 문제이다.36 이러한 문제들을 해결하기 위한 연구는 VAE의 진화를 이끄는 주요 동력으로 작용하고 있다. VQ-VAE는 이산적 잠재 공간을 도입하여 흐릿함 문제를 효과적으로 해결했으며 63, VAE-GAN과 같이 VAE와 GAN을 결합하여 각 모델의 장점을 취하려는 하이브리드 모델 연구도 활발히 진행되었다.28 최근에는 VAE를 확산 모델(Diffusion Models)과 결합하여 생성 품질과 샘플링 효율을 동시에 높이려는 시도들이 주목받고 있다.40</p>
<p>또 다른 중요한 연구 방향은 잠재 공간의 ’해석 가능성(interpretability)’과 ’분리(disentanglement)’를 향상시키는 것이다. β-VAE와 같은 모델들이 이 분야의 초기 연구를 이끌었지만, 여전히 감독 없이(unsupervised) 의미 있는 분리된 표현을 학습하는 것은 어려운 문제로 남아있다.60 AI가 데이터를 인간처럼 이해하고, 특정 속성만을 선택적으로 제어하기 위해서는 잠재 공간의 각 차원이 어떤 의미를 갖는지 명확히 파악하는 기술이 필수적이며, 이는 앞으로도 중요한 연구 주제가 될 것이다. 이와 더불어, 대규모 데이터셋과 거대 모델(예: Stable Diffusion에 사용되는 VAE) 환경에서 VAE의 학습 효율성과 성능을 최적화하는 연구 또한 지속적으로 이루어지고 있다.7</p>
<h3>6.3  미래 전망: 진화하는 생성 AI 생태계 속 VAE의 역할</h3>
<p>미래의 생성 AI 생태계에서 VAE는 단독 생성 모델로서의 역할뿐만 아니라, 더 크고 복잡한 시스템의 핵심 ’구성 요소(component)’로서 그 중요성이 더욱 커질 것으로 전망된다. 이러한 경향은 Stable Diffusion과 같은 최신 텍스트-이미지 생성 모델에서 이미 뚜렷하게 나타나고 있다.</p>
<p>Stable Diffusion은 직접적으로 고차원의 픽셀 공간에서 이미지를 생성하는 대신, VAE를 사용하여 이미지를 다루기 쉬운 저차원의 잠재 공간으로 압축한다.42 실제 이미지 생성 과정인 확산 프로세스는 바로 이 압축된 잠재 공간에서 수행되며, 마지막 단계에서 VAE의 디코더를 통해 잠재 표현을 다시 고해상도 이미지로 복원한다.7 여기서 VAE는 단순한 생성기가 아니라, 픽셀의 세계와 의미의 세계를 잇는 ’효율적인 번역기’이자 ’의미론적 압축기’로서 기능한다. 이 접근법은 계산 비용을 획기적으로 줄이면서도 고품질의 이미지를 생성할 수 있게 하여, 거대 생성 모델의 대중화를 이끈 핵심 요인 중 하나가 되었다.</p>
<p>이처럼 VAE의 강력한 표현 학습 능력과 차원 축소 능력은 다른 생성 패러다임과 결합될 때 엄청난 시너지를 발휘한다. 앞으로 VAE는 GAN, 확산 모델, 트랜스포머 등 다양한 모델 아키텍처와 결합하여, 생성 모델의 효율성, 품질, 제어 가능성을 높이는 데 필수적인 기반 기술로 계속해서 활용될 것이다. VAE는 생성 AI라는 거대한 건축물을 지탱하는 견고한 주춧돌 중 하나로서, 그 진화를 계속해 나갈 것으로 기대된다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>What is a Generative Model? | IBM, https://www.ibm.com/think/topics/generative-model</li>
<li>What is a Generative Model? - DataCamp, https://www.datacamp.com/blog/what-is-a-generative-model</li>
<li>Generative model - Wikipedia, https://en.wikipedia.org/wiki/Generative_model</li>
<li>Generative AI Models Explained - AltexSoft, https://www.altexsoft.com/blog/generative-ai/</li>
<li>Background: What is a Generative Model? | Machine Learning - Google for Developers, https://developers.google.com/machine-learning/gan/generative</li>
<li>VAEs for Image Generation - Number Analytics, https://www.numberanalytics.com/blog/vaes-for-image-generation</li>
<li>What Is VAE in Stable Diffusion? - Built In, https://builtin.com/artificial-intelligence/stable-diffusion-vae</li>
<li>Understanding the Differences Between AutoEncoder (AE) and Variational AutoEncoder (VAE) | by Etorezone | Medium, https://medium.com/@etorezone/understanding-the-differences-between-autoencoder-ae-and-variational-autoencoder-vae-1ccb52ebf76c</li>
<li>What is a Variational Autoencoder? - IBM, https://www.ibm.com/think/topics/variational-autoencoder</li>
<li>[Hands-On] 오토인코더의 이해와 구현 - Medium, <a href="https://medium.com/@hugmanskj/hands-on-%EC%98%A4%ED%86%A0%EC%9D%B8%EC%BD%94%EB%8D%94%EC%9D%98-%EC%9D%B4%ED%95%B4%EC%99%80-%EA%B5%AC%ED%98%84-f0d9e3b31819">https://medium.com/@hugmanskj/hands-on-%EC%98%A4%ED%86%A0%EC%9D%B8%EC%BD%94%EB%8D%94%EC%9D%98-%EC%9D%B4%ED%95%B4%EC%99%80-%EA%B5%AC%ED%98%84-f0d9e3b31819</a></li>
<li>[만들면서 배우는 생성 AI] 3장 - 변이형 오토인코더(VAE) - velog, <a href="https://velog.io/@running_learning/%EB%A7%8C%EB%93%A4%EB%A9%B4%EC%84%9C-%EB%B0%B0%EC%9A%B0%EB%8A%94-%EC%83%9D%EC%84%B1-AI-2%EC%9E%A5-%EB%94%A5%EB%9F%AC%EB%8B%9D">https://velog.io/@running_learning/%EB%A7%8C%EB%93%A4%EB%A9%B4%EC%84%9C-%EB%B0%B0%EC%9A%B0%EB%8A%94-%EC%83%9D%EC%84%B1-AI-2%EC%9E%A5-%EB%94%A5%EB%9F%AC%EB%8B%9D</a></li>
<li>Variational Autoencoders - Deep Learning, CMU, https://deeplearning.cs.cmu.edu/S20/document/recitation/recitation12.pdf</li>
<li>Autoencoder or Variational Auto Encoder? : r/learnmachinelearning - Reddit, https://www.reddit.com/r/learnmachinelearning/comments/1aiohuu/autoencoder_or_variational_auto_encoder/</li>
<li>Image Generation Series - Variational Autoencoders | by Mani - Medium, https://medium.com/@manikantan.srinivasan2001/image-generation-series-variational-autoencoders-f8da3e6e0559</li>
<li>Variational autoencoder - Wikipedia, https://en.wikipedia.org/wiki/Variational_autoencoder</li>
<li>Variational Autoencoders - Dremio, https://www.dremio.com/wiki/variational-autoencoders/</li>
<li>Tutorial: VAE as an inference paradigm for neuroimaging - arXiv, https://arxiv.org/pdf/2501.08009</li>
<li>Tutorial: VAE as an inference paradigm for neuroimaging - arXiv, https://arxiv.org/html/2501.08009v1</li>
<li>[생성AI] 생성AI 기초(6) : 변이 오토인코더 - 글래스 비드 게임, <a href="https://futures-studies.tistory.com/entry/%EC%83%9D%EC%84%B1AI-%EC%83%9D%EC%84%B1AI-%EA%B8%B0%EC%B4%886-%EB%B3%80%EC%9D%B4-%EC%98%A4%ED%86%A0%EC%9D%B8%EC%BD%94%EB%8D%94">https://futures-studies.tistory.com/entry/%EC%83%9D%EC%84%B1AI-%EC%83%9D%EC%84%B1AI-%EA%B8%B0%EC%B4%886-%EB%B3%80%EC%9D%B4-%EC%98%A4%ED%86%A0%EC%9D%B8%EC%BD%94%EB%8D%94</a></li>
<li>What is Variational Autoencoders ? - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2023/07/an-overview-of-variational-autoencoders/</li>
<li>Variational AutoEncoders - GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/variational-autoencoders/</li>
<li>Variational Autoencoders: How They Work and Why They Matter | DataCamp, https://www.datacamp.com/tutorial/variational-autoencoders</li>
<li>www.ibm.com, <a href="https://www.ibm.com/kr-ko/think/topics/autoencoder#:~:text=4-,%EB%B3%80%EC%9D%B4%ED%98%95%20%EC%98%A4%ED%86%A0%EC%9D%B8%EC%BD%94%EB%8D%94,%EC%83%9D%EC%84%B1%ED%95%98%EB%8A%94%20%EB%8D%B0%20%EC%82%AC%EC%9A%A9%EB%90%A9%EB%8B%88%EB%8B%A4.">https://www.ibm.com/kr-ko/think/topics/autoencoder#:~:text=4-,%EB%B3%80%EC%9D%B4%ED%98%95%20%EC%98%A4%ED%86%A0%EC%9D%B8%EC%BD%94%EB%8D%94,%EC%83%9D%EC%84%B1%ED%95%98%EB%8A%94%20%EB%8D%B0%20%EC%82%AC%EC%9A%A9%EB%90%A9%EB%8B%88%EB%8B%A4.</a></li>
<li>Tutorial on Variational Autoencoders, https://arxiv.org/abs/1606.05908</li>
<li>Tutorial - What is a variational autoencoder? – Jaan Lı 李, https://jaan.io/what-is-variational-autoencoder-vae-tutorial/</li>
<li>Variational Inference &amp; Derivation of the Variational Autoencoder (VAE) Loss Function: A True Story | by Dr Stephen Odaibo | The Blog of RETINA-AI Health, Inc. | Medium, https://medium.com/retina-ai-health-inc/variational-inference-derivation-of-the-variational-autoencoder-vae-loss-function-a-true-story-3543a3dc67ee</li>
<li>Variational autoencoders simply explained - AI Accelerator Institute, https://www.aiacceleratorinstitute.com/variational-autoencoders-simply-explained/</li>
<li>A Crash Course on VAEs, VQ-VAEs, and VAE-GANs - Medium, https://medium.com/mlearning-ai/a-crash-course-on-vaes-vq-vaes-and-vae-gans-3fdcc40b059e</li>
<li>[D] The popular theoretical explanation for VAE is inconsistent. Please change my mind., https://www.reddit.com/r/MachineLearning/comments/1h5f6co/d_the_popular_theoretical_explanation_for_vae_is/</li>
<li>Evidence lower bound - Wikipedia, https://en.wikipedia.org/wiki/Evidence_lower_bound</li>
<li>ELBO (Evidence of Lower BOund) - Lifetime behind every seconds, https://yonghyuc.wordpress.com/2019/09/26/elbo-evidence-of-lower-bound/</li>
<li>Derivation of ELBO in VAE. I read a blog on how to build VAE using …, https://fangdahan.medium.com/derivation-of-elbo-in-vae-25ad7991fdf7</li>
<li>The Reparameterization Trick - Gregory Gundersen, https://gregorygundersen.com/blog/2018/04/29/reparameterization/</li>
<li>7.3.2 재매개변수화 트릭 - 밑바닥부터 시작하는 딥러닝 5 [Book], https://www.oreilly.com/library/view/mitbadagbuteo-sijaghaneun-dibreoning/9791169212960/chapter-95.html</li>
<li>재매개변수화 트릭은 무엇이며 VAE(Variational Autoencoders) 교육에 중요한 이유는 무엇입니까?, <a href="https://ko.eitca.org/%EC%9D%B8%EA%B3%B5-%EC%A7%80%EB%8A%A5/eitc-ai-adl-%EA%B3%A0%EA%B8%89-%EB%94%A5-%EB%9F%AC%EB%8B%9D/%EA%B3%A0%EA%B8%89-%EC%83%9D%EC%84%B1-%EB%AA%A8%EB%8D%B8/%ED%98%84%EB%8C%80%EC%9D%98-%EC%9E%A0%EC%9E%AC-%EB%B3%80%EC%88%98-%EB%AA%A8%EB%8D%B8/%EC%8B%9C%ED%97%98-%EA%B2%80%ED%86%A0-%ED%98%84%EB%8C%80-%EC%9E%A0%EC%9E%AC-%EB%B3%80%EC%88%98-%EB%AA%A8%EB%8D%B8/%EC%9E%AC%EB%A7%A4%EA%B0%9C%EB%B3%80%EC%88%98%ED%99%94-%ED%8A%B8%EB%A6%AD%EC%9D%80-%EB%AC%B4%EC%97%87%EC%9D%B4%EB%A9%B0-%EC%9D%B4%EA%B2%83%EC%9D%B4-%EB%B3%80%ED%98%95-%EC%9E%90%EB%8F%99-%EC%9D%B8%EC%BD%94%EB%8D%94%EC%9D%98-%ED%9B%88%EB%A0%A8%EC%97%90-%EC%A4%91%EC%9A%94%ED%95%9C-%EC%9D%B4%EC%9C%A0%EB%8A%94-%EB%AC%B4%EC%97%87%EC%9E%85%EB%8B%88%EA%B9%8C%3F/">https://ko.eitca.org/%EC%9D%B8%EA%B3%B5-%EC%A7%80%EB%8A%A5/eitc-ai-adl-%EA%B3%A0%EA%B8%89-%EB%94%A5-%EB%9F%AC%EB%8B%9D/%EA%B3%A0%EA%B8%89-%EC%83%9D%EC%84%B1-%EB%AA%A8%EB%8D%B8/%ED%98%84%EB%8C%80%EC%9D%98-%EC%9E%A0%EC%9E%AC-%EB%B3%80%EC%88%98-%EB%AA%A8%EB%8D%B8/%EC%8B%9C%ED%97%98-%EA%B2%80%ED%86%A0-%ED%98%84%EB%8C%80-%EC%9E%A0%EC%9E%AC-%EB%B3%80%EC%88%98-%EB%AA%A8%EB%8D%B8/%EC%9E%AC%EB%A7%A4%EA%B0%9C%EB%B3%80%EC%88%98%ED%99%94-%ED%8A%B8%EB%A6%AD%EC%9D%80-%EB%AC%B4%EC%97%87%EC%9D%B4%EB%A9%B0-%EC%9D%B4%EA%B2%83%EC%9D%B4-%EB%B3%80%ED%98%95-%EC%9E%90%EB%8F%99-%EC%9D%B8%EC%BD%94%EB%8D%94%EC%9D%98-%ED%9B%88%EB%A0%A8%EC%97%90-%EC%A4%91%EC%9A%94%ED%95%9C-%EC%9D%B4%EC%9C%A0%EB%8A%94-%EB%AC%B4%EC%97%87%EC%9E%85%EB%8B%88%EA%B9%8C%3F/</a></li>
<li>변형 오토인코더란 무엇인가요? - IBM, https://www.ibm.com/kr-ko/think/topics/variational-autoencoder</li>
<li>Why is the variational auto-encoder’s output blurred, while GANs output is crisp and has sharp edges? - Artificial Intelligence Stack Exchange, https://ai.stackexchange.com/questions/8885/why-is-the-variational-auto-encoders-output-blurred-while-gans-output-is-crisp</li>
<li>[D]Why are images created by GAN sharper than images by VAE? : r …, https://www.reddit.com/r/MachineLearning/comments/9t712f/dwhy_are_images_created_by_gan_sharper_than/</li>
<li>[D] Quality of generated image from VAE vs GAN : r/MachineLearning, https://www.reddit.com/r/MachineLearning/comments/gum5iq/d_quality_of_generated_image_from_vae_vs_gan/</li>
<li>Comparing Diffusion, GAN, and VAE Techniques - Generative AI Lab, https://generativeailab.org/l/generative-ai/a-tale-of-three-generative-models-comparing-diffusion-gan-and-vae-techniques/569/</li>
<li>Explicitly Minimizing the Blur Error of Variational Autoencoders, https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/594280/1/Explicitly_Minimizing_the_Blur_Error_of_Variational_Autoencoders.pdf</li>
<li>VAE. The Latent Bottleneck: Why Image Generation Processes Lose Fine Details - Medium, https://medium.com/@efrat_37973/vae-the-latent-bottleneck-why-image-generation-processes-lose-fine-details-a056dcd6015e</li>
<li>Detecting Posterior Collapse in Conditional and Hierarchical Variational Autoencoders - arXiv, https://arxiv.org/html/2306.05023v3</li>
<li>Scale-VAE: Preventing Posterior Collapse in Variational Autoencoder - ACL Anthology, https://aclanthology.org/2024.lrec-main.1250.pdf</li>
<li>Controlling Posterior Collapse by an Inverse Lipschitz Constraint on the Decoder Network, https://arxiv.org/html/2304.12770v2</li>
<li>Posterior collapse - Just Do It - 티스토리, https://xogns7652.tistory.com/entry/Posterior-collapse</li>
<li>[D] Variational Autoencoders are not autoencoders : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/al0lvl/d_variational_autoencoders_are_not_autoencoders/</li>
<li>How does variational autoencoders actually work in comparison to GAN?, https://datascience.stackexchange.com/questions/121467/how-does-variational-autoencoders-actually-work-in-comparison-to-gan</li>
<li>Preventing Posterior Collapse with DVAE for Text Modeling - MDPI, https://www.mdpi.com/1099-4300/27/4/423</li>
<li>www.geeksforgeeks.org, <a href="https://www.geeksforgeeks.org/deep-learning/generative-models-in-ai-a-comprehensive-comparison-of-gans-and-vaes/#:~:text=Neural%20Network-Based%3A%20Both%20GANs,outputs%20from%20this%20latent%20space.">https://www.geeksforgeeks.org/deep-learning/generative-models-in-ai-a-comprehensive-comparison-of-gans-and-vaes/#:~:text=Neural%20Network%2DBased%3A%20Both%20GANs,outputs%20from%20this%20latent%20space.</a></li>
<li>Generative Models in AI: A Comprehensive Comparison of GANs …, https://www.geeksforgeeks.org/deep-learning/generative-models-in-ai-a-comprehensive-comparison-of-gans-and-vaes/</li>
<li>VAE Vs. GAN For Image Generation | Baeldung on Computer Science, https://www.baeldung.com/cs/vae-vs-gan-image-generation</li>
<li>VAE v/s GAN - A case study. Deep learning is a field that focuses …, https://medium.com/@parakatta/vae-v-s-gan-a-case-study-b09c7169ac02</li>
<li>Learn Conditional VAE | Introduction to Generative Networks - Codefinity, https://codefinity.com/courses/v2/0cb5bd44-cf54-4200-9e26-ac4232a374bb/3a629837-6e9d-43f9-b4d9-a66ddd8fb15b/59514068-ee4e-46a4-9caf-8ced6af5af25</li>
<li>Exploring Advanced Generative AI | Conditional VAEs - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2023/09/generative-ai-conditional-vaes/</li>
<li>Conditional Variational Auto-encoder - Pyro Tutorials 1.9.1 documentation, https://pyro.ai/examples/cvae.html</li>
<li>CVAE 설명 - 치킨고양이짱아 공부일지, https://chickencat-jjanga.tistory.com/4</li>
<li>Hands-on with Conditional Variational Autoencoders (CVAE) - YouTube, https://www.youtube.com/watch?v=e4izCK7l4Kc&amp;pp=0gcJCfwAo7VqN5tD</li>
<li>[논문 리뷰] CRepair: CVAE-based Automatic Vulnerability Repair Technology - Moonlight, https://www.themoonlight.io/ko/review/crepair-cvae-based-automatic-vulnerability-repair-technology</li>
<li>Disentangling Variational Autoencoders | beta_vae – Weights &amp; Biases - Wandb, https://wandb.ai/arpastrana/beta_vae/reports/Disentangling-Variational-Autoencoders–VmlldzozNDQ3MDk</li>
<li>A Closer Look at Disentangling in β-VAE - arXiv, https://arxiv.org/pdf/1912.05127</li>
<li>Denoising Multi-Beta VAE: Representation Learning for … - arXiv, https://arxiv.org/pdf/2507.06613</li>
<li>[1711.00937] Neural Discrete Representation Learning - arXiv, https://arxiv.org/abs/1711.00937</li>
<li>Understanding Vector Quantized Variational Autoencoders (VQ-VAE …, https://shashank7-iitd.medium.com/understanding-vector-quantized-variational-autoencoders-vq-vae-323d710a888a</li>
<li>VQ-VAE EXPLAINER: Learn Vector-Quantized Variational Autoencoders with Interactive Visualization, https://xnought.github.io/files/vq_vae_explainer.pdf</li>
<li>VQ-VAE(Neural Discrete Representation Learning) - velog, https://velog.io/@trwy25/VQ-VAENeural-Discrete-Representation-Learning</li>
<li>Understanding Vector Quantization in VQ-VAE - Hugging Face, https://huggingface.co/blog/ariG23498/understand-vq</li>
<li>Variational Autoencoders: VAE to VQ-VAE / dVAE - Rohit Bandaru, https://rohitbandaru.github.io/blog/VAEs/</li>
<li>Generating Diverse High-Fidelity Images with VQ-VAE2 - DeepStudy - 티스토리, https://deepseow.tistory.com/42</li>
<li>Variational Autoencoder Based Anomaly Detection in Large-Scale …, https://www.mdpi.com/1996-1073/18/11/2770</li>
<li>Training a Variational Autoencoder for Anomaly Detection Using TensorFlow, https://www.analyticsvidhya.com/blog/2023/09/variational-autoencode-for-anomaly-detection-using-tensorflow/</li>
<li>Data Augmentation &amp; Anomaly Detection with Variational … - Ksolves, https://www.ksolves.com/blog/artificial-intelligence/a-multifaceted-approach-exploring-advanced-vaes-for-data-augmentation-and-anomaly-detection</li>
<li>Multi-Channel Multi-Scale Convolution Attention Variational Autoencoder (MCA-VAE): An Interpretable Anomaly Detection Algorithm Based on Variational Autoencoder - MDPI, https://www.mdpi.com/1424-8220/24/16/5316</li>
<li>오토인코더와 변이형 오토인코더를 활용한 공유 킥보드 사용자 인증 시스템 강화 - Korea Science, https://koreascience.kr/article/CFKO202133648814920.pdf</li>
<li>CBN-VAE: A Data Compression Model with Efficient Convolutional Structure for Wireless Sensor Networks - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC6721250/</li>
<li>Lossy Image Compression With Quantized Hierarchical VAEs - CVF Open Access, https://openaccess.thecvf.com/content/WACV2023/papers/Duan_Lossy_Image_Compression_With_Quantized_Hierarchical_VAEs_WACV_2023_paper.pdf</li>
<li>Exploring Semi-supervised Variational Autoencoders for Biomedical …, https://pmc.ncbi.nlm.nih.gov/articles/PMC6708455/</li>
<li>Semi-supervised Learning with Variational Autoencoders | Bounded Rationality, https://bjlkeng.io/posts/semi-supervised-learning-with-variational-autoencoders/</li>
<li>The Semi-Supervised VAE - Pyro Tutorials 1.9.1 documentation, https://pyro.ai/examples/ss-vae.html</li>
<li>Infinite Variational Autoencoder for Semi-Supervised Learning - CVF Open Access, https://openaccess.thecvf.com/content_cvpr_2017/papers/Abbasnejad_Infinite_Variational_Autoencoder_CVPR_2017_paper.pdf</li>
<li>[2402.02346] Closed-Loop Unsupervised Representation Disentanglement with <span class="math math-inline">β</span>-VAE Distillation and Diffusion Probabilistic Feedback - arXiv, https://arxiv.org/abs/2402.02346</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>