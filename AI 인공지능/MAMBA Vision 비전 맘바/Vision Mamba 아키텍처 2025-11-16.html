<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Vision Mamba 아키텍처</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Vision Mamba 아키텍처</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">비전 맘바 (Vision Mamamba)</a> / <span>Vision Mamba 아키텍처</span></nav>
                </div>
            </header>
            <article>
                <h1>Vision Mamba 아키텍처</h1>
<p>2025-11-16, G25DR</p>
<h2>1.  서론: Vision Mamba의 부상</h2>
<h3>1.1  Vision Transformer(ViT)의 현주소와 본질적 한계</h3>
<p>지난 몇 년간 컴퓨터 비전(Computer Vision) 분야는 컨볼루션 신경망(CNN)의 국소적 수용장(receptive field) 한계를 극복한 Vision Transformer(ViT)의 등장으로 근본적인 패러다임 전환을 경험하였다.1 ViT의 핵심 작동 원리인 ‘셀프 어텐션(Self-Attention)’ 메커니즘은 이미지 전체를 패치(patch)의 집합으로 간주하여, 모든 패치 간의 관계를 동시에 모델링함으로써 전례 없는 수준의 전역적(global) 컨텍스트 이해 능력을 선보였다.4</p>
<p>그러나 ViT의 이러한 강력한 성능은 ’확장성(scalability)’이라는 명확한 한계와 맞닿아 있다. 셀프 어텐션 메커니즘은 본질적으로 시퀀스 길이 <span class="math math-inline">L</span>(이미지 패치의 수)에 대해 <span class="math math-inline">O(L^2)</span>의 2차 연산 복잡도(Quadratic Computational Complexity)를 요구한다.1 이는 입력 이미지의 해상도가 증가하거나 패치 크기가 작아질수록(즉, <span class="math math-inline">L</span>이 커질수록), 필요한 계산 비용과 GPU 메모리 요구량이 기하급수적으로 폭증하는 문제를 야기한다.6</p>
<h3>1.2  대안의 등장: State Space Models (SSM)과 Mamba</h3>
<p>ViT의 2차 복잡도 문제를 해결하기 위한 대안으로, 자연어 처리(NLP) 분야에서 순차적 데이터를 효율적으로 처리하기 위해 발전해 온 RNN(Recurrent Neural Network) 계열의 State Space Model(SSM)이 다시 주목받기 시작하였다.1</p>
<p>특히 2023년 말에 등장한 ‘Mamba’ 아키텍처는 기존 SSM의 한계(time-invariant parameterization 등) 1를 ‘선택적 스캔(Selective Scan)’ 메커니즘을 통해 입력 데이터에 따라 동적으로 파라미터를 조절하도록 개선하였다. 또한, 하드웨어 친화적(hardware-aware) 병렬 스캔 알고리즘을 구현하여 시퀀스 길이에 대해 <span class="math math-inline">O(L)</span>의 선형 시간 복잡도(Linear Time Complexity)를 달성하며 NLP 분야에서 큰 성공을 거두었다.4 Mamba는 ViT의 <span class="math math-inline">O(L^2)</span> 복잡성 문제에 대한 가장 유력한 해결책, 즉 <strong>선형 복잡성으로 전역적 컨텍스트를 모델링</strong>할 수 있는 잠재력을 제시하였다.2</p>
<h3>1.3  보고서의 목적과 구조</h3>
<p>본 보고서는 이 강력한 1D 시퀀스 모델인 Mamba를 2D 시각 데이터(이미지)에 적용하려는 초기 시도들, 즉 ‘Vision Mamba’ 아키텍처 제품군을 심층적으로 분석하는 것을 목적으로 한다.</p>
<p>이는 단순히 ’더 빠른 ViT’를 찾는 문제를 넘어, 2D 공간 정보를 1D 시퀀스로 변환하여 처리하는 과정에서 발생하는 근본적인 ‘차원 불일치’ 문제와, 이를 해결하기 위한 다양한 아키텍처적 진화 과정을 추적한다. Mamba는 본질적으로 1D 순차(recurrent) 모델이며 5, 이미지는 2D 비순차(non-sequential) 데이터이다. 따라서 Mamba를 2D 이미지에 적용하려면, “2D 데이터를 어떻게 1D 시퀀스로 변환(serialize)할 것인가“라는 근본적인 문제에 봉착하게 된다.10</p>
<p>이 ‘2D-to-1D’ 변환 문제에 대한 서로 다른 해답이 바로 <strong>Vim</strong>과 <strong>VMamba</strong>라는 두 개의 상이한 아키텍처 분기를 탄생시킨 핵심 원인이다. 본 보고서는 이 두 초기 핵심 모델의 아키텍처 비교(II장)를 시작으로, 정량적 성능 검증(III장), 의료 및 원격 탐사 등 도메인 특화 적용(IV장), 그리고 확장성 한계와 ’MambaOut’으로 대표되는 학술적 비판(V장), 마지막으로 Mamba-Transformer 하이브리드라는 최신 동향(VI장) 순으로 전개된다.</p>
<h2>2.  핵심 아키텍처 분석: Vim 대 VMamba</h2>
<p>2024년 1월, 거의 동시에 발표된 Vim과 VMamba는 2D-to-1D 변환 문제에 대해 상이한 접근법을 제시하며 Vision Mamba 분야의 초석을 다졌다.</p>
<h3>2.1  Vim (Vision Mamba): 1D 평탄화와 양방향 SSM (BSSM)</h3>
<p>Vim(Vision Mamba)은 “Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model” (ICML 2024) 논문을 통해 처음 소개되었다.14</p>
<p>Vim의 아키텍처는 ViT와 매우 유사한 ‘비계층적(non-hierarchical)’ 백본 구조를 채택하였다.6 ViT가 Transformer Encoder 블록을 쌓아 올린 것과 동일하게, Vim은 단순히 이 Transformer 블록을 ’Vim 블록’으로 대체하는 방식을 사용하였다.10</p>
<p>Vim의 핵심 작동 원리는 **‘Bidirectional SSM (BSSM)’**이며, 2D 이미지를 1D Mamba로 처리하기 위해 다음과 같은 ViT의 접근 방식을 그대로 수용한다:</p>
<ol>
<li><strong>패치화 및 평탄화 (Patchify &amp; Flatten):</strong> ViT와 동일하게 이미지를 <span class="math math-inline">16 \times 16</span> 등의 패치로 나누고, 이를 1D 시퀀스로 단순 평탄화(flattening)한다.10</li>
<li><strong>위치 임베딩 (Positional Embedding):</strong> 평탄화 과정에서 손실된 2D 공간 정보를 보충하기 위해 시퀀스에 위치 임베딩을 추가한다.10</li>
<li><strong>양방향 스캔 (Bidirectional Scan):</strong> 1D Mamba는 ‘인과적(causal)’ 모델링(즉, <span class="math math-inline">t</span> 시점은 <span class="math math-inline">t-1</span> 시점까지만 참조)을 수행한다. 하지만 이미지는 모든 픽셀이 동등하게 중요한 ‘비인과적(non-causal)’ 데이터이다. Vim은 이 문제를 해결하기 위해, 평탄화된 1D 시퀀스를 **정방향(forward pass)**으로 스캔하고, 다시 **역방향(backward pass)**으로 스캔하여 두 결과를 병합(merge)한다.10</li>
</ol>
<p>Vim의 접근은 1D Mamba 아키텍처의 변경을 최소화하면서 2D 이미지의 비인과적 특성을 반영하려는 ‘데이터 전처리’ 수준의 해결책이라 할 수 있다.</p>
<h3>2.2  VMamba (Visual State Space Model): 2D 평면 스캔 (SS2D)</h3>
<p>VMamba는 “VMamba: Visual State Space Model” (NeurIPS 2024) 논문을 통해 소개되었다.18</p>
<p>VMamba는 Vim과 달리, Swin Transformer와 유사한 ‘계층적(hierarchical)’ 백본 구조를 채택하였다.24 이는 다운스트림 작업(탐지, 분할)에 더 유리한 다중 스케일 특징 맵을 생성한다. VMamba의 핵심 빌딩 블록은 ’Visual State-Space (VSS) 블록’이다.22</p>
<p>VMamba의 핵심 작동 원리는 Vim의 1D 평탄화 방식이 2D 공간의 인접성을 파괴한다고 비판하며 13, **‘SS2D (2D Selective Scan)’**라는 아키텍처적 해결책을 제시한다. 이는 ’Cross-Scan Module (CSM)’이라고도 불린다.22</p>
<p>SS2D 메커니즘은 1D Mamba 모듈을 2D 평면에 직접 적용하기 위해 다음과 같은 3단계 프로세스를 수행한다 22:</p>
<ol>
<li><strong>(a) 스캔 확장 (Scan Expand):</strong> 2D 피처 맵을 1D로 평탄화하는 대신, 2D 맵을 ’대표’할 수 있는 4개의 서로 다른 1D 시퀀스로 ‘펼친다(unfold)’. 이는 2D 맵을 4개의 주요 방향(예: (1)좌상단 <span class="math math-inline">\to</span> 우하단, (2)우하단 <span class="math math-inline">\to</span> 좌상단, (3)우상단 <span class="math math-inline">\to</span> 좌하단, (4)좌하단 <span class="math math-inline">\to</span> 우상단)으로 스캔하여 4개의 독립적인 1D 시퀀스를 생성하는 것을 의미한다.4</li>
<li><strong>(b) S6 블록 처리:</strong> 이 4개의 1D 시퀀스를 각각 독립적인 S6(Mamba) 블록에 <strong>병렬적으로</strong> 통과시켜 장거리 의존성을 모델링한다.22</li>
<li><strong>(c) 스캔 병합 (Scan Merge):</strong> S6 블록을 통과한 4개의 1D 시퀀스 결과를 다시 병합하여 2D 피처 맵으로 재구성한다.22</li>
</ol>
<h3>2.3  비교 분석: ’스캔 전략’의 근본적 차이</h3>
<p>Vim과 VMamba는 2D-to-1D 문제에 대해 ’데이터 중심(Vim)’과 ’아키텍처 중심(VMamba)’이라는 상이한 철학을 보여준다.</p>
<ul>
<li><strong>Vim (BSSM):</strong> ViT의 패러다임을 수용한다. “Mamba는 1D 모델이니, 2D 이미지를 1D 시퀀스로 만들자 (ViT 방식). 단, 이미지는 인과적이지 않으니, 시퀀스를 뒤집어서 한 번 더 처리하자 (Bidirectional).” 이는 아키텍처 변경을 최소화하고 1D Mamba를 재활용하는 ‘데이터 중심’ 접근법이다.10</li>
<li><strong>VMamba (SS2D):</strong> 2D 공간 정보 보존에 집중한다. “Mamba는 1D 모델이지만, 2D 공간을 1D로 평탄화하면 정보가 손실된다. 그렇다면 2D 공간을 ’대표’할 수 있는 1D 경로를 여러 개(4개) 만들고, 각각을 Mamba로 처리한 뒤, 그 결과를 합치자.” 이는 2D 문제를 2D에 가깝게 풀기 위해 1D Mamba 모듈을 ’수정’하는 ‘아키텍처 중심’ 해결책이다.22</li>
</ul>
<p>VMamba의 SS2D 방식은 Vim의 단순 평탄화 방식보다 2D 공간 정보를 훨씬 정교하게 포착하며 25, 계층적 구조는 다양한 스케일의 특징을 요구하는 다운스트림 작업에 더 적합하다. 이러한 아키텍처의 우수성은 III장에서 다룰 성능 벤치마크 결과, 특히 시맨틱 분할(semantic segmentation) 작업에서 극명한 성능 차이로 입증된다.</p>
<h2>3.  정량적 성능 벤치마크 분석</h2>
<p>Vision Mamba 아키텍처의 유효성은 두 가지 핵심 지표, 즉 (A) ViT 대비 압도적인 연산 효율성, 그리고 (B) 기존 SOTA 모델 대비 동등하거나 우수한 정확도를 통해 입증된다.</p>
<h3>3.1  핵심 효율성 지표: ViT/DeiT와의 비교</h3>
<p>Vision Mamba의 가장 큰 존재 의의는 ViT의 2차 복잡도 문제를 선형 복잡도로 해결하는 데 있다. 이러한 이론적 이점은 실제 고해상도 이미지 처리에서 막대한 효율성 증대로 나타난다.</p>
<p>Vim(Vision Mamba)은 1248x1248 픽셀의 고해상도 이미지에 대한 배치 추론(batch inference) 작업에서, 유사한 크기의 ViT 변형 모델인 DeiT보다 <strong>2.8배 빠르고</strong>, GPU 메모리 사용량은 <strong>86.8% 절약</strong>하는 경이적인 효율성을 달성하였다.6</p>
<p>이러한 경향은 해상도가 더욱 증가할수록 극대화된다. 한 Mamba 기반 백본(Adventurer-Base)은 1280x1280 픽셀 입력 크기에서 ViT-Base 모델 대비 <strong>11.7배의 속도 향상</strong>과 <strong>14.0배의 메모리 절약</strong> 효과를 보고하였다.8 이는 ViT의 <span class="math math-inline">O(L^2)</span> 복잡성과 Mamba의 <span class="math math-inline">O(L)</span> 선형 복잡성의 이론적 차이가 실제 고해상도 환경에서 실용적인 병목 현상으로 직결됨을 명확히 보여준다.8</p>
<h3>3.2  표준 벤치마크: ImageNet, COCO, ADE20K</h3>
<p>Vim과 VMamba는 이러한 효율성을 확보하는 동시에, ImageNet(분류), COCO(객체 탐지), ADE20K(시맨틱 분할)와 같은 표준 비전 벤치마크에서도 강력한 성능을 입증하였다.</p>
<p>Vim (비계층적) 벤치마크</p>
<p>Vim은 ViT/DeiT와 유사한 비계층적 구조를 가지므로 DeiT와의 직접 비교가 적절하다.6 아래 표 1은 Vim이 효율성뿐만 아니라 정확도 측면에서도 DeiT를 능가함을 보여준다.</p>
<p>표 1. Vim (ICML 2024) 벤치마크 성능 (vs. DeiT)</p>
<p>(데이터 소스: 16)</p>
<table><thead><tr><th><strong>모델 (백본)</strong></th><th><strong>파라미터</strong></th><th><strong>ImageNet-1K (Top-1 Acc. %)</strong></th><th><strong>COCO (Detection mAP %)</strong></th><th><strong>ADE20K (Seg. mIoU %)</strong></th></tr></thead><tbody>
<tr><td>DeiT-Ti</td><td>6M</td><td>72.2</td><td>40.5 (Cascade Mask R-CNN)</td><td>(데이터 없음)</td></tr>
<tr><td><strong>Vim-Ti</strong></td><td>7M</td><td><strong>76.1</strong> / <strong>78.3</strong> (v2)</td><td><strong>41.7</strong> (Cascade Mask R-CNN)</td><td><strong>35.9</strong> (Segmenter)</td></tr>
<tr><td>DeiT-S</td><td>22M</td><td>79.8</td><td>(데이터 없음)</td><td>(Vim-S 대비 -0.9)</td></tr>
<tr><td><strong>Vim-S</strong></td><td>26M</td><td><strong>80.5</strong> / <strong>81.6</strong> (v2)</td><td>(데이터 없음)</td><td>(DeiT-S 대비 +0.9)</td></tr>
</tbody></table>
<p>VMamba (계층적) 벤치마크</p>
<p>VMamba는 Swin Transformer와 같은 계층적 구조를 채택했으므로 Swin과의 비교가 적절하다.23 아래 표 2는 VMamba의 아키텍처적 우수성, 특히 II장에서 분석한 SS2D의 효과가 성능으로 직결됨을 보여준다.</p>
<p>표 2. VMamba (NeurIPS 2024) 벤치마크 성능 (vs. Swin)</p>
<p>(데이터 소스: 23)</p>
<table><thead><tr><th><strong>모델 (백본)</strong></th><th><strong>파라미터</strong></th><th><strong>ImageNet-1K (Top-1 Acc. %)</strong></th><th><strong>COCO (MaskRCNN@1x, bboxAP)</strong></th><th><strong>ADE20K (UperNet@512, mIoU SS)</strong></th></tr></thead><tbody>
<tr><td>Swin-T</td><td>29M</td><td>81.3</td><td>43.8</td><td>44.4</td></tr>
<tr><td><strong>VMamba-T</strong></td><td>30M</td><td><strong>82.6</strong></td><td><strong>47.3</strong></td><td><strong>47.9</strong></td></tr>
<tr><td>Swin-S</td><td>50M</td><td>83.0</td><td>46.9</td><td>47.6</td></tr>
<tr><td><strong>VMamba-S</strong></td><td>50M</td><td><strong>83.6</strong></td><td><strong>48.7</strong></td><td><strong>50.6</strong></td></tr>
<tr><td>Swin-B</td><td>88M</td><td>83.5</td><td>48.0</td><td>48.1</td></tr>
<tr><td><strong>VMamba-B</strong></td><td>89M</td><td><strong>83.9</strong></td><td><strong>49.2</strong></td><td><strong>51.0</strong></td></tr>
</tbody></table>
<p>벤치마크 분석</p>
<p>표 1과 표 2의 비교 분석을 통해 VMamba가 Vim보다 성능 향상 폭이 훨씬 크다는 점이 명확히 드러나며, 특히 **시맨틱 분할(ADE20K)**에서 그 차이가 두드러진다.</p>
<p>Vim은 DeiT 대비 ImageNet 분류와 COCO 탐지에서 소폭의 성능 향상을 보였다.17 반면, VMamba는 Swin 대비 COCO 탐지에서 큰 폭의 향상(VMamba-T: +3.5 AP)을 보였고, ADE20K 분할에서는 <strong>압도적인 성능 향상</strong>(VMamba-T: +3.5 mIoU, VMamba-S: +3.0 mIoU)을 달성하였다.23</p>
<p>이러한 결과는 II장의 아키텍처 분석에서 제기된 가설을 강력하게 뒷받침한다. 픽셀 수준의 조밀한 예측(dense prediction)을 요구하는 시맨틱 분할 작업에서, 1D 평탄화(Vim)는 2D 공간 정보 보존에 한계가 있었던 반면, 2D 평면 스캔(VMamba의 SS2D)은 2D 공간 맥락과 글로벌 수용장을 훨씬 효과적으로 포착한다.22 즉, VMamba의 ‘SS2D’ 스캔 전략이 Mamba 아키텍처의 비전 태스크 잠재력을 최대로 이끌어낸 핵심 요인이라 결론지을 수 있다.</p>
<h2>4.  주요 적용 분야 및 특화 변형 모델</h2>
<p>Vim과 VMamba가 제시한 Mamba의 비전 태스크 적용 가능성은 2024년 한 해 동안 폭발적인 연구로 이어졌다. 특히 ViT의 <span class="math math-inline">O(L^2)</span> 복잡도 문제가 가장 심각하게 대두되었던 고해상도 도메인, 즉 의료 영상과 원격 탐사 분야에서 Mamba는 즉각적인 해결책으로 채택되었다.</p>
<h3>4.1  의료 영상 분석: MedMamba와 CNN-SSM 하이브리드</h3>
<p>의료 영상(예: 기가픽셀 단위의 병리 슬라이드(WSI), 고해상도 CT 및 MRI) 분석은 Vision Mamba의 핵심 적용 분야 중 하나이다.14 이 분야는 CNN의 국소적 수용장 한계와 ViT의 계산 비용 폭증 문제가 동시에 발생하는 대표적인 영역이다.40</p>
<p>이러한 문제를 해결하기 위해 “MedMamba: Vision Mamba for Medical Image Classification” (arXiv:2403.03849) 41이 제안되었다.</p>
<p>MedMamba의 핵심은 순수 Mamba가 아닌, CNN과 SSM(Mamba)을 결합한 <strong>‘SS-Conv-SSM’ 하이브리드 블록</strong>을 제안한 데 있다.41</p>
<ul>
<li><strong>하이브리드 결합 이유:</strong> MedMamba 연구진은 의료 영상 분류에 순수 CNN이나 순수 SSM만으로는 최적이 아니라고 판단하였다. CNN은 미세한 병변의 <strong>국소적 텍스처(local texture) 특징 추출</strong>에 강력하며 41, Mamba(SSM)는 장기(organ)의 전체적인 형태나 병변의 광범위한 분포 등 <strong>전역적 컨텍스트(global context) 및 장거리 의존성 포착</strong>에 강력하기 때문이다.28</li>
<li><strong>SS-Conv-SSM 작동 원리:</strong> 이 하이브리드 블록은 VMamba의 VSS 블록을 기반으로, 국소-전역 정보의 시너지를 극대화하도록 설계되었다.28</li>
</ul>
<ol>
<li><strong>Channel-Split:</strong> 입력 피처맵을 두 개의 그룹으로 분할한다.</li>
<li><strong>Conv-Branch:</strong> 한 그룹은 CNN(합성곱) 레이어를 통과시켜 국소적 특징을 추출한다.</li>
<li><strong>SSM-Branch:</strong> 다른 그룹은 VMamba의 VSS 블록(SS2D 스캔 사용)을 통과시켜 전역적 공간 관계를 모델링한다.</li>
<li><strong>Channel-Concatenation &amp; Shuffle:</strong> 두 브랜치에서 처리된 결과를 다시 결합하고 채널을 섞어(shuffle), 국소 정보와 전역 정보가 효율적으로 통합되도록 한다.</li>
</ol>
<p>MedMamba 외에도 Swin-UMamba 40, LoG-VMamba 48 등 Mamba 아키텍처를 의료 영상 분할(segmentation)에 적용하려는 연구가 활발히 진행되고 있다.</p>
<h3>4.2  원격 탐사 (Remote Sensing): 고해상도 위성 영상 분석</h3>
<p>원격 탐사(RS) 분야는 Mamba 아키텍처의 <strong>‘킬러 애플리케이션(Killer Application)’</strong> 영역으로 부상하였다. 위성 영상은 해상도가 극도로 높으며 49, CNN의 수용장 한계와 ViT의 <span class="math math-inline">O(L^2)</span> 복잡도 문제가 가장 심각하게 대두되는 분야이기 때문이다.50 Mamba의 선형 복잡성(<span class="math math-inline">O(L)</span>)은 이 영역에서 전역 모델링을 가능하게 하는 거의 유일한 대안으로 평가받는다.49</p>
<ol>
<li>분류 (Classification): ‘RSMamba’ (arXiv:2403.19654)</li>
</ol>
<p>RSMamba는 원격 탐사 이미지 ’분류’를 위해 제안되었다.54 흥미롭게도 이 모델은 Vim의 BSSM이나 VMamba의 SS2D와는 또 다른 제3의 스캔 전략을 사용한다. RSMamba는 바닐라 Mamba 블록(1D)을 유지하되, 2D 이미지 입력을 정방향(forward), 역방향(reverse), 무작위 셔플(random shuffle)의 **다중 경로(multi-path)**로 동시에 처리하여 그 결과를 합산한다.54 이 방식이 위성 이미지 데이터셋에서 Vim/VMamba보다 우수한 분류 성능을 달성하였다고 보고되었다.57</p>
<ol start="2">
<li>분할 (Segmentation): ‘Samba’ (arXiv:2404.01705)</li>
</ol>
<p>Samba는 Mamba 블록을 인코더로 사용하는 U-Net 기반 아키텍처(Samba-Encoder, UperNet-Decoder)를 통해 위성 영상의 ’의미론적 분할’에서 SOTA(State-of-the-Art) 성능을 달성하였다.59 이 외에도 VM-UNet 62, CVMH-UNet 50 등 Mamba 기반 분할 모델이 다수 제안되며 고해상도 분할의 표준으로 자리 잡고 있다.</p>
<ol start="3">
<li>변화 탐지 (Change Detection): ‘ChangeMamba’ (arXiv:2404.03425)</li>
</ol>
<p>변화 탐지는 두 시점(bi-temporal)의 이미지를 비교하여 변화 영역을 찾는 고유한 태스크이다. ChangeMamba(MambaBCD)는 Mamba 인코더로 각 시점의 전역적 공간 정보를 추출할 뿐만 아니라, Mamba의 시퀀스 모델링 능력을 활용하여 두 이미지 간의 ‘시공간적(spatio-temporal)’ 관계를 모델링하는 특화된 디코더를 제안하였다.53</p>
<h3>4.3  2024-2025년 Vision Mamba 주요 변형 모델 생태계</h3>
<p>Vim과 VMamba의 등장은 불과 1년 만에 컴퓨터 비전 전반에 걸쳐 방대한 Mamba 기반 변형 모델 생태계를 구축하였다. 아래 표 3은 2024년부터 2025년 초까지 발표된 주요 Vision Mamba 변형 모델과 그 핵심 적용 분야를 요약한 것이다.</p>
<p>표 3. 2024-2025년 Vision Mamba 주요 변형 모델 및 적용 분야</p>
<p>(데이터 소스: 2)</p>
<table><thead><tr><th><strong>모델명</strong></th><th><strong>발표 (연도/월)</strong></th><th><strong>핵심 적용 분야</strong></th><th><strong>주요 특징 (아키텍처)</strong></th></tr></thead><tbody>
<tr><td><strong>Vim</strong></td><td>2024.01</td><td>범용 비전 백본</td><td>Bidirectional SSM (BSSM) 14</td></tr>
<tr><td><strong>VMamba</strong></td><td>2024.01</td><td>범용 비전 백본</td><td>2D Selective Scan (SS2D) / VSS 블록 21</td></tr>
<tr><td><strong>Mamba-ND</strong></td><td>2024.02</td><td>다차원 데이터 (3D/4D)</td><td>N-Dimensional Scan 65</td></tr>
<tr><td><strong>MambaIR</strong></td><td>2024.02</td><td>이미지 복원 (Restoration)</td><td>RSS 블록 (VSS + Conv + Residual) 10</td></tr>
<tr><td><strong>MedMamba</strong></td><td>2024.03</td><td>의료 영상 분류</td><td><strong>SS-Conv-SSM (CNN+SSM 하이브리드)</strong> 41</td></tr>
<tr><td><strong>LocalMamba</strong></td><td>2024.03</td><td>범용 비전 백본</td><td>Windowed Selective Scan 65</td></tr>
<tr><td><strong>RSMamba</strong></td><td>2024.03</td><td>원격 탐사 분류</td><td>다중 경로 (Forward/Reverse/Shuffle) 스캔 54</td></tr>
<tr><td><strong>PlainMamba</strong></td><td>2024.03</td><td>범용 비전 백본</td><td>비계층적, Serpentine Scan 65</td></tr>
<tr><td><strong>Samba</strong></td><td>2024.04</td><td>원격 탐사 분할</td><td>Mamba Encoder + UperNet Decoder 59</td></tr>
<tr><td><strong>ChangeMamba</strong></td><td>2024.04</td><td>원격 탐사 변화 탐지</td><td>시공간 Mamba 디코더 53</td></tr>
<tr><td><strong>MambaOut</strong></td><td>2024.05</td><td>범용 비전 (비판)</td><td><strong>SSM 제거 (Gated CNN만 사용)</strong> 30</td></tr>
<tr><td><strong>Adventurer</strong></td><td>2024 (2025 pub)</td><td>효율적 비전 백본</td><td>Mamba-2 구조, One-way scan 최적화 8</td></tr>
<tr><td><strong>MambaVision</strong></td><td>2024.07 (CVPR 2025)</td><td>범용 비전 백본</td><td><strong>Mamba-Transformer 하이브리드</strong> 66</td></tr>
</tbody></table>
<h2>5.  기술적 한계, 비판 및 학술적 논쟁</h2>
<p>Vision Mamba는 폭발적인 관심과 적용에도 불구하고, 심각한 기술적 한계와 근본적인 아키텍처 비판에 직면해 있다.</p>
<h3>5.1  확장성, 훈련 불안정성 및 오버피팅 (Overfitting) 문제</h3>
<p>Mamba는 이론적으로 <span class="math math-inline">O(L)</span>의 효율적인 확장성을 가졌음에도 불구하고, 실제로는 모델 크기(파라미터 수)를 키울수록 ViT 대형 모델에 비해 성능이 저조한 ’확장성 한계’가 다수 보고되었다. 구체적으로, Vision Mamba 모델은 약 80M~90M 파라미터를 초과하면 심각한 **오버피팅(overfitting)**에 직면하여 훈련이 불안정해지고 성능이 저하되는 경향을 보였다.67</p>
<p>이 문제의 핵심 원인은 Mamba의 1D 순차 처리 방식 그 자체에 있다. Mamba는 RNN과 유사하게 이전 시퀀스 정보를 ’압축된 숨겨진 상태(hidden state)’로 전달하는 반면 70, ViT의 어텐션은 <span class="math math-inline">O(L^2)</span> 비용으로 모든 패치에 대한 ‘무손실(lossless)’ 접근을 유지한다.70 2D 이미지를 1D 시퀀스로 강제 변환하는 ‘고정된 스캔 경로(fixed scan path)’(예: raster scan)는 2D 공간의 인접성을 파괴할 뿐만 아니라 15, 모델이 일반화된 시각 특징이 아닌 훈련 데이터의 ‘특정 스캔 순서’ 자체를 암기하도록 유도하여 오버피팅을 유발할 수 있다.68</p>
<p>이러한 Mamba 고유의 오버피팅 문제를 해결하기 위해 <strong>‘Stochastic Layer-Wise Shuffle (SLWS)’</strong> (arXiv:2408.17081)이라는 정규화(regularization) 기법이 제안되었다.67</p>
<ul>
<li><strong>작동 원리:</strong> SLWS는 훈련(training) 시에만 적용되며 67, Mamba 블록에 입력되는 패치 시퀀스의 순서를 **확률적으로 무작위 셔플(shuffle)**한다.69</li>
<li><strong>핵심 아이디어:</strong> 이 셔플 확률은 고정된 것이 아니라, <strong>레이어가 깊어질수록(deeper layers) 높아진다</strong>.69</li>
<li><strong>이론적 근거:</strong> 얕은 레이어(shallow layers)는 위치에 민감한 저수준 특징(텍스처 등)을 학습해야 하므로 셔플하지 않는다. 반면, 깊은 레이어(deep layers)는 위치에 둔감한 고수준 의미(semantic) 특징을 학습해야 한다. 깊은 레이어에서 시퀀스 순서를 의도적으로 섞으면, 모델이 ’특정 스캔 순서’에 의존해 암기하는 것을 방지하고 67, 위치에 무관한 ‘의미론적’ 특징을 학습하도록 강제하여 오버피팅을 효과적으로 완화한다.68</li>
</ul>
<p>SLWS의 적용은 Vision Mamba 모델을 300M 파라미터 수준까지 성공적으로 확장할 수 있게 만들었다.67</p>
<h3>5.2  근본적 비판: “MambaOut” - Mamba는 비전 분류에 불필요한가?</h3>
<p>Vision Mamba의 성능에 대한 더 근본적인 비판은 “MambaOut: Do We Really Need Mamba for Vision?” (CVPR 2025) 논문에서 제기되었다.30</p>
<p>이 논문은 Mamba 블록의 아키텍처를 ’Gated CNN’과 ‘SSM’ 모듈의 결합체로 분해한다.30 그리고 Mamba(SSM)의 본질적 특성이 **(1) 장-시퀀스(long-sequence)**와 **(2) 자기회귀/인과성(autoregressive/causal)**에 최적화되어 있다고 주장한다.30</p>
<p>이를 바탕으로 두 가지 가설을 설정하고 실험으로 검증하였다:</p>
<ol>
<li><strong>가설 1: Mamba는 분류(Classification)에 불필요하다.</strong></li>
</ol>
<ul>
<li><em>근거:</em> 이미지 분류(ImageNet)는 (1) 장-시퀀스 태스크가 아니며 (2) 인과성을 요구하지도 않는다. 따라서 분류 성능은 SSM 모듈이 아닌 Gated CNN 모듈에서 나올 것이다.30</li>
<li><em>검증 (MambaOut 모델):</em> Mamba 블록에서 **SSM 모듈을 ‘제거’**하고 Gated CNN만 남긴 ‘MambaOut’ 모델을 구축하였다.30</li>
<li><em>결과:</em> MambaOut (SSM 없음)이 ImageNet 분류에서 모든 순수 Vision Mamba 모델(Vim, VMamba 등)의 성능을 <strong>능가하였다</strong>.30</li>
</ul>
<ol start="2">
<li><strong>가설 2: Mamba는 탐지/분할(Detection/Segmentation)에 잠재력이 있다.</strong></li>
</ol>
<ul>
<li><em>근거:</em> 탐지 및 분할 작업은 고해상도 이미지를 처리하므로 (1) 장-시퀀스 특성을 가진다. 따라서 SSM 모듈이 유용할 것이다.30</li>
<li><em>검증 (MambaOut 모델):</em> MambaOut (SSM 없음)을 탐지/분할 작업에 적용하였다.</li>
<li><em>결과:</em> MambaOut이 SOTA Vision Mamba 모델(예: VMamba-T)보다 COCO 탐지 등에서 성능이 <strong>더 낮았다</strong> (예: -1.4 AP).30</li>
</ul>
<p>MambaOut 논문의 결론은 Vision Mamba가 ImageNet에서 보인 성공이 ’Mamba(SSM)’의 효율성 때문이 아니라 ’Gated CNN’이라는 아키텍처 구성 요소 덕분일 수 있음을 시사한다. 동시에, Mamba(SSM)의 진정한 가치는 범용 백본이 아니라, <strong>고해상도의 조밀한 예측(탐지/분할) 작업</strong>에서 ’장-시퀀스’를 효율적으로 처리하는 특화된 모듈로서의 역할에 있음을 역설적으로 입증하였다.</p>
<h2>6.  결론: 하이브리드 아키텍처와 Vision Mamba의 미래</h2>
<h3>6.1  학술적 논쟁의 종합: Mamba는 만능인가, 특화 도구인가?</h3>
<p>Vision Mamba의 발전 과정을 종합하면, Mamba는 ViT를 대체하는 만능 해결책이 아니라, 고유한 장단점을 지닌 특화된 아키텍처 구성 요소로 귀결된다.</p>
<ol>
<li>Vision Mamba는 ViT의 <span class="math math-inline">O(L^2)</span> 문제를 <span class="math math-inline">O(L)</span>로 해결하며 등장하였다 (I장).</li>
<li>VMamba(SS2D)는 Vim(BSSM)보다 2D 공간 정보를 더 잘 보존하여, 특히 분할 작업에서 우수함을 입증하였다 (II, III장).</li>
<li>MedMamba(CNN-SSM)는 Mamba가 국소적 특징 추출에 약점을 보이며, 이를 CNN과 결합해야 할 필요성을 제시하였다 (IV장).</li>
<li>SLWS는 Mamba의 1D 스캔 방식이 대형 모델의 오버피팅을 유발하며, 이를 해결하기 위한 ‘순서 셔플’ 정규화가 필요함을 보였다 (V.A장).</li>
<li>MambaOut은 Mamba(SSM)가 분류 작업에는 불필요할 수 있으며, 오직 ’장-시퀀스’가 특징인 조밀한 예측(dense prediction) 작업에만 유용할 수 있다는 강력한 증거를 제시하였다 (V.B장).</li>
</ol>
<p>이러한 학술적 논쟁의 결과, Mamba는 CNN(국소 처리), Attention(전역 처리, 고비용)과 어깨를 나란히 하는 **‘효율적인 장-시퀀스 처리’**를 담당하는 제3의 핵심 아키텍처 구성 요소(building block)로 자리매김하고 있다.</p>
<h3>6.2  최신 동향 (2025): Mamba-Transformer 하이브리드</h3>
<p>Mamba가 만능이 아닌 특화된 도구라는 인식이 확산되면서, Mamba의 장점(효율성)과 기존 아키텍처(CNN, Transformer)의 장점을 결합하려는 하이브리드 모델이 2025년의 주류로 부상하고 있다.</p>
<p>Mamba(SSM)는 효율적(<span class="math math-inline">O(L)</span>)이지만, 인과적/순차적 처리로 인해 진정한 전역적 수용장(full receptive field)을 한 번에 포착하는 데 한계가 있다.66 반면, Transformer(Attention)는 <span class="math math-inline">O(L^2)</span>의 비용이 들지만 완벽한 전역적 컨텍스트를 제공한다.66</p>
<p>가장 논리적인 해결책은 이 둘을 결합하는 것이며, 이는 <strong>MambaVision</strong> (CVPR 2025) 66에 의해 구현되었다.</p>
<p>MambaVision의 하이브리드 아키텍처는 각 컴포넌트의 장점을 극대화하도록 계층적으로 설계되었다 66:</p>
<ol>
<li><strong>초기 스테이지 (Shallow):</strong> <strong>CNN 블록</strong>을 사용하여 로컬 특징을 빠르고 효율적으로 추출한다.</li>
<li><strong>중간 스테이지 (Middle):</strong> **Mamba 블록(SSM)**을 사용하여 ‘효율적으로(<span class="math math-inline">O(L)</span>)’ 시퀀스 정보를 압축하고 장거리 의존성을 모델링한다.</li>
<li><strong>최종 스테이지 (Deep):</strong> <strong>Transformer(Self-Attention) 블록</strong>을 사용하여, Mamba가 효율적으로 처리한 정보들을 기반으로 ‘완전한 전역적(<span class="math math-inline">O(L^2)</span>)’ 컨텍스트를 혼합(mix)하고 최종적인 장거리 공간 종속성을 포착한다.</li>
</ol>
<p>이 하이브리드 접근법은 MedMamba가 입증한 CNN의 국소성, MambaOut이 입증한 Mamba의 장-시퀀스 효율성, 그리고 ViT가 입증한 Attention의 전역적 성능을 모두 활용한다. 그 결과 MambaVision은 순수 Mamba(Vim/VMamba)와 순수 ViT 모델 모두를 능가하는 SOTA 성능을 달성하였다.66</p>
<h3>6.3  최종 결론</h3>
<p>Vision Mamba는 Vision Transformer를 ’대체’하는 것이 아니라, CNN, Attention과 함께 비전 아키텍처를 구성하는 <strong>제3의 핵심 빌딩 블록</strong>으로 진화하고 있다. ‘Mamba 대 Transformer’ 79라는 2024년 초의 담론은 ‘Mamba와 Transformer의 시너지’ 66라는 2025년의 현실로 빠르게 수렴하고 있다.</p>
<p>Mamba(SSM)의 미래는 그 자체로 SOTA가 되는 것이 아니라, MambaVision과 MedMamba의 사례처럼 CNN의 국소적 효율성과 Attention의 전역적 강력함 사이의 간극을 메우는 **‘효율적인 장-시퀀스 모델러’**로서 차세대 하이브리드 비전 백본의 핵심축을 담당하는 것이다.</p>
<p>특히 원격 탐사 51나 의료 영상 41과 같이 고해상도의 ‘장-시퀀스’ 처리가 필수적이며 ViT의 <span class="math math-inline">O(L^2)</span> 복잡도가 실용적 배포를 가로막는 도메인에서, Mamba의 영향력은 압도적인 효율성을 바탕으로 지속적으로 확대될 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>A Survey on Vision Mamba: Models, Applications and Challenges - arXiv, https://arxiv.org/html/2404.18861v1</li>
<li>Mamba in Vision: A Comprehensive Survey of Techniques and Applications - arXiv, https://arxiv.org/html/2410.03105v1</li>
<li>Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model - Hugging Face, https://huggingface.co/blog/mikelabs/vision-mamba-efficient-visual-representation-learn</li>
<li>MHS-VIT: Mamba hybrid self-attention vision transformers for traffic image detection, https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0325962</li>
<li>Evaluating the Necessity of Mamba Mechanisms in Visual Recognition Tasks-MambaOut | DigitalOcean, https://www.digitalocean.com/community/tutorials/mamba-mechanisms-visual-recognitions-mambaout</li>
<li>Vision Mamba: Like a Vision Transformer but Better | Towards Data …, https://towardsdatascience.com/vision-mamba-like-a-vision-transformer-but-better-3b2660c35848/</li>
<li>VMambaCC: A Visual State Space Model for Crowd Counting - arXiv, https://arxiv.org/html/2405.03978v1</li>
<li>Adventurer: Optimizing Vision Mamba Architecture Designs for Efficiency - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC12574601/</li>
<li>Vision Mamba for Permeability Prediction of Porous Media - arXiv, https://arxiv.org/html/2510.14516v1</li>
<li>A Survey on Visual Mamba - MDPI, https://www.mdpi.com/2076-3417/14/13/5683</li>
<li>Scaling Vision Mamba Across Resolutions via Fractal Traversal - arXiv, https://arxiv.org/html/2505.14062v1</li>
<li>[2405.04404] Vision Mamba: A Comprehensive Survey and Taxonomy - arXiv, https://arxiv.org/abs/2405.04404</li>
<li>2DMamba: Efficient State Space Model for Image Representation with Applications on Giga-Pixel Whole Slide Image Classification, https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_2DMamba_Efficient_State_Space_Model_for_Image_Representation_with_Applications_CVPR_2025_paper.pdf</li>
<li>Vision Mamba: The Next Leap in Visual Representation Learning | by azhar - Medium, https://medium.com/ai-insights-cobet/vision-mamba-the-next-leap-in-visual-representation-learning-24a10e5a9cde</li>
<li>[2505.14062] Scaling Vision Mamba Across Resolutions via Fractal Traversal - arXiv, https://arxiv.org/abs/2505.14062</li>
<li>Vision Mamba: Efficient Visual Representation Learning with … - arXiv, https://arxiv.org/abs/2401.09417</li>
<li>hustvl/Vim: [ICML 2024] Vision Mamba: Efficient Visual … - GitHub, https://github.com/hustvl/Vim</li>
<li>(PDF) Rethinking Scanning Strategies With Vision Mamba in Semantic Segmentation of Remote Sensing Imagery: An Experimental Study - ResearchGate, https://www.researchgate.net/publication/384580233_Rethinking_Scanning_Strategies_with_Vision_Mamba_in_Semantic_Segmentation_of_Remote_Sensing_Imagery_An_Experimental_Study</li>
<li>MambaCL: A Contrastive Learning Framework for Change Detection on Pseudotemporal Images Using Visual State Space Model - IEEE Xplore, https://ieeexplore.ieee.org/iel8/4609443/10766875/11157781.pdf</li>
<li>[PDF] VMamba: Visual State Space Model - Semantic Scholar, https://www.semanticscholar.org/paper/VMamba%3A-Visual-State-Space-Model-Liu-Tian/b24e899ec0f77eef2fc87a9b8e50516367aa1f97</li>
<li>[2401.10166] VMamba: Visual State Space Model - arXiv, https://arxiv.org/abs/2401.10166</li>
<li>VMamba: Visual State Space Model - arXiv, https://arxiv.org/html/2401.10166v4</li>
<li>MzeroMiko/VMamba - Visual State Space Models - GitHub, https://github.com/MzeroMiko/VMamba</li>
<li>Flatten Wisely: How Patch Order Shapes Mamba-Powered Vision for MRI Segmentation This work was supported by Qatar Research, Development, and Innovation (QRDI) Council under UREP Grant No. UREP31-077-2-025. ∗Equal contribution - arXiv, https://arxiv.org/html/2507.13384v1</li>
<li>[Literature Review] VMamba: Visual State Space Model - Moonlight, https://www.themoonlight.io/en/review/vmamba-visual-state-space-model</li>
<li>VMamba: Visual State Space Model - NIPS papers, https://proceedings.neurips.cc/paper_files/paper/2024/file/baa2da9ae4bfed26520bb61d259a3653-Paper-Conference.pdf</li>
<li>VMamba: Visual State Space Model - arXiv, https://arxiv.org/html/2401.10166v3</li>
<li>MedMamba Explained : The first Vision Mamba for Generalized …, https://ai.gopubby.com/medmamba-explained-the-first-vision-mamba-for-generalized-medical-image-classification-is-3aee20a0751a</li>
<li>VMamba: Visual State Space Model - arXiv, https://arxiv.org/html/2401.10166v2</li>
<li>MambaOut: Do We Really Need Mamba for Vision? - arXiv, https://arxiv.org/html/2405.07992v1</li>
<li>VMamba: Another transformer moment for vision tasks? - Mahendran Narayanan - Medium, https://mahendran-narayanan.medium.com/vmamba-another-transformer-moment-for-vision-tasks-8365a410421a?source=rss——artificial_intelligence-5</li>
<li>MambaReID: Exploiting Vision Mamba for Multi-Modal Object Re-Identification - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC11280729/</li>
<li>The workflow of 2D Selective Scan (SS2D). The input features are… - ResearchGate, https://www.researchgate.net/figure/The-workflow-of-2D-Selective-Scan-SS2D-The-input-features-are-scanned-in-four_fig2_381036064</li>
<li>VMamba for plant leaf disease identification: design and experiment - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC12000097/</li>
<li>Vision Mamba more accurate than transformers and is 2.8x faster and uses 86.8% less GPU memory, https://morris-lee.medium.com/vision-mamba-more-accurate-than-transformers-and-is-2-8x-faster-and-uses-86-8-less-gpu-memory-d447f638be3e</li>
<li>GlobalMamba: Global Image Serialization for Vision Mamba - OpenReview, https://openreview.net/forum?id=XKQ2qzajbU</li>
<li>Vision Mamba: A Comprehensive Survey and Taxonomy - arXiv, https://arxiv.org/html/2405.04404v1</li>
<li>VMamba: Visual State Space Model - arXiv, https://arxiv.org/html/2401.10166v1</li>
<li>A Comprehensive Survey of Mamba Architectures for Medical Image Analysis: Classification, Segmentation, Restoration and Beyond - arXiv, https://arxiv.org/html/2410.02362v3</li>
<li>Swin-UMamba†: Adapting Mamba-Based Vision Foundation Models for Medical Image Segmentation - IEEE Xplore, https://ieeexplore.ieee.org/document/10771659/</li>
<li>[2403.03849] MedMamba: Vision Mamba for Medical Image Classification - arXiv, https://arxiv.org/abs/2403.03849</li>
<li>YubiaoYue/MedMamba: This is the official code repository for “MedMamba: Vision Mamba for Medical Image Classification” - GitHub, https://github.com/YubiaoYue/MedMamba</li>
<li>MedMamba: Vision Mamba for Medical Image Classification - arXiv, https://arxiv.org/html/2403.03849v5</li>
<li>MedMamba architecture and illustration of SS-Conv-SSM block. From [104] - ResearchGate, https://www.researchgate.net/figure/MedMamba-architecture-and-illustration-of-SS-Conv-SSM-block-From-104_fig1_381190407</li>
<li>[PDF] MedMamba: Vision Mamba for Medical Image Classification - Semantic Scholar, https://www.semanticscholar.org/paper/MedMamba%3A-Vision-Mamba-for-Medical-Image-Yue-Li/9463ef5f893e4ade0363242894be081f7684350f</li>
<li>[Literature Review] MedMamba: Vision Mamba for Medical Image Classification - Moonlight, https://www.themoonlight.io/en/review/medmamba-vision-mamba-for-medical-image-classification</li>
<li>MedMamba: Vision Mamba for Medical Image Classification - arXiv, https://arxiv.org/html/2403.03849v4</li>
<li>LoG-VMamba: Local-Global Vision Mamba for Medical Image Segmentation - CVF Open Access, https://openaccess.thecvf.com/content/ACCV2024/papers/Dang_LoG-VMamba_Local-Global_Vision_Mamba_for_Medical_Image_Segmentation_ACCV_2024_paper.pdf</li>
<li>Vision Mamba in Remote Sensing: A Comprehensive Survey of Techniques, Applications and Outlook - arXiv, https://arxiv.org/html/2505.00630v1</li>
<li>[2410.05624] Remote Sensing Image Segmentation Using Vision Mamba and Multi-Scale Multi-Frequency Feature Fusion - arXiv, https://arxiv.org/abs/2410.05624</li>
<li>[2505.00630] Vision Mamba in Remote Sensing: A Comprehensive Survey of Techniques, Applications and Outlook - arXiv, https://arxiv.org/abs/2505.00630</li>
<li>Rethinking Scanning Strategies With Vision Mamba in Semantic Segmentation of Remote Sensing Imagery: An Experimental Study - IEEE Xplore, https://ieeexplore.ieee.org/document/10703181/</li>
<li>[2404.03425] ChangeMamba: Remote Sensing Change Detection With Spatiotemporal State Space Model - arXiv, https://arxiv.org/abs/2404.03425</li>
<li>RSMamba: Remote Sensing Image Classification with State Space Model - arXiv, https://arxiv.org/html/2403.19654v1</li>
<li>[2403.19654] RSMamba: Remote Sensing Image Classification with State Space Model - arXiv, https://arxiv.org/abs/2403.19654</li>
<li>KyanChen/RSMamba: This is the pytorch implement of the … - GitHub, https://github.com/KyanChen/RSMamba</li>
<li>RSMamba: Remote Sensing Image Classification with State Space Model - IEEE Xplore, https://ieeexplore.ieee.org/iel8/8859/4357975/10542538.pdf</li>
<li>RSMamba: Remote Sensing Image Classification With State Space Model - IEEE Xplore, https://ieeexplore.ieee.org/iel8/8859/10365397/10542538.pdf</li>
<li>Samba Semantic Segmentation of Remotely Sensed Images with State Space Model - The University of Liverpool Repository, https://livrepository.liverpool.ac.uk/3184827/1/2404.01705v2.pdf</li>
<li>[2404.01705] Samba: Semantic Segmentation of Remotely Sensed Images with State Space Model - arXiv, https://arxiv.org/abs/2404.01705</li>
<li>zhuqinfeng1999/Samba - GitHub, https://github.com/zhuqinfeng1999/Samba</li>
<li>Satellite Image Segmentation Using Vision Mamba UNet | by F. Kuzey Edes-Huyal | Medium, https://medium.com/@kuzeyuvercinka/satellite-image-segmentation-using-vision-mamba-unet-83983268f334</li>
<li>Remote Sensing Image Segmentation Using Vision Mamba and Multi-Scale Multi-Frequency Feature Fusion - MDPI, https://www.mdpi.com/2072-4292/17/8/1390</li>
<li>A Novel Remote Sensing Image Change Detection Approach Based on Multilevel State Space Model - IEEE Xplore, https://ieeexplore.ieee.org/document/10756674/</li>
<li>XiudingCai/Awesome-Mamba-Collection: A curated … - GitHub, https://github.com/XiudingCai/Awesome-Mamba-Collection</li>
<li>MambaVision: A Hybrid Mamba-Transformer Vision Backbone - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2025/papers/Hatamizadeh_MambaVision_A_Hybrid_Mamba-Transformer_Vision_Backbone_CVPR_2025_paper.pdf</li>
<li>Stochastic Layer-Wise Shuffle: A Good Practice to Improve Vision …, https://openreview.net/forum?id=EispKqtw5B</li>
<li>Stochastic Layer-Wise Shuffle for Improving Vision Mamba Training - arXiv, https://arxiv.org/html/2408.17081v2</li>
<li>[Literature Review] Stochastic Layer-Wise Shuffle: A Good Practice to Improve Vision Mamba Training - Moonlight, https://www.themoonlight.io/en/review/stochastic-layer-wise-shuffle-a-good-practice-to-improve-vision-mamba-training</li>
<li>[D] What Are the Fundamental Drawbacks of Mamba Compared to Transformers? - Reddit, https://www.reddit.com/r/MachineLearning/comments/1ayog60/d_what_are_the_fundamental_drawbacks_of_mamba/</li>
<li>MambaOut: Do We Really Need Mamba for Vision? - arXiv, https://arxiv.org/pdf/2405.07992</li>
<li>Stochastic Layer-Wise Shuffle: A Good Practice to Improve Vision Mamba Training - arXiv, https://arxiv.org/html/2408.17081v1</li>
<li>Stochastic Layer-Wise Shuffle for Improving Vision Mamba Training - OpenReview, https://openreview.net/forum?id=6vP20U55bA</li>
<li>MambaOut: Do We Really Need Mamba for Vision? (CVPR 2025) - GitHub, https://github.com/yuweihao/MambaOut</li>
<li>MambaOut:Rethinking State-Space Models in the Context of Image …, https://medium.com/@zergtant/mambaout-rethinking-state-space-models-in-the-context-of-image-classification-e03a31dfc7d4</li>
<li>MambaOut: Do We Really Need Mamba for Vision? - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2025/papers/Yu_MambaOut_Do_We_Really_Need_Mamba_for_Vision_CVPR_2025_paper.pdf</li>
<li>MambaVision: A Hybrid Mamba-Transformer Vision Backbone - Research at NVIDIA, https://research.nvidia.com/publication/2025-06_mambavision-hybrid-mamba-transformer-vision-backbone</li>
<li>[CVPR 2025] Official PyTorch Implementation of MambaVision: A Hybrid Mamba-Transformer Vision Backbone - GitHub, https://github.com/NVlabs/MambaVision</li>
<li>[D] Vision Mamba Strikes Again! Is the Transformer Throne Crumbling? - Reddit, https://www.reddit.com/r/MachineLearning/comments/19eemq2/d_vision_mamba_strikes_again_is_the_transformer/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>