<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Meta Llama 1 (2023-02-24)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Meta Llama 1 (2023-02-24)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">거대 언어 모델 (LLM, Large Language Models)</a> / <span>Meta Llama 1 (2023-02-24)</span></nav>
                </div>
            </header>
            <article>
                <h1>Meta Llama 1 (2023-02-24)</h1>
<h2>1.  효율적 파운데이션 모델의 등장, Llama 1</h2>
<h3>1.1  Llama 1의 발표 배경 및 의의</h3>
<p>2023년 2월 24일, Meta AI는 Llama(Large Language Model Meta AI)를 발표하며 대규모 언어 모델(LLM) 연구 지형에 중요한 전환점을 제시했다.1 당시 LLM 분야는 막대한 양의 인프라와 자본을 보유한 소수의 거대 기술 기업들이 주도하고 있었다. 이로 인해 최첨단 모델에 대한 접근성이 극도로 제한되었고, 이는 학계와 독립 연구 커뮤니티의 발전을 저해하는 요인으로 작용했다.2</p>
<p>Llama는 이러한 문제에 대한 직접적인 대응으로, ’더 작고, 더 성능 좋은 모델’이라는 명확한 목표를 가지고 설계되었다.2 이는 단순히 모델의 파라미터 수를 늘리는 경쟁에서 벗어나, 더 많은 양의 데이터로 상대적으로 작은 모델을 훈련시키는 것이 더 효율적이고 효과적일 수 있다는 DeepMind의 Chinchilla 스케일링 법칙에 이론적 기반을 둔다.1 Llama의 출시는 LLM의 작동 방식, 견고성, 그리고 편향 및 독성과 같은 내재적 문제 해결을 위한 광범위한 연구를 촉진하여, 궁극적으로 AI 연구 분야의 접근성을 민주화하려는 시도였다.2</p>
<p>이러한 접근 방식은 LLM 개발의 패러다임에 대한 근본적인 질문을 던졌다. 2022-2023년의 LLM 경쟁이 PaLM-540B와 같이 ’더 큰 모델’을 만드는 방향으로 치닫고 있을 때, Meta는 ’효율성’과 ’접근성’이라는 대안적 가치를 제시했다.2 이는 단순한 기술적 우위를 넘어, LLM 개발의 방향성에 대한 철학적 논쟁을 촉발시켰다. 즉, ’규모의 경쟁’에서 ’효율의 경쟁’으로의 전환 가능성을 제시한 것이다.</p>
<h3>1.2  핵심 철학: 공개 데이터셋을 통한 최첨단 성능 증명</h3>
<p>Llama의 가장 중요한 철학적 기여는 독점적이고 접근 불가능한 데이터셋에 의존하지 않고, 오직 공개적으로 사용 가능한 데이터 소스만으로 최첨단(State-of-the-art) 성능을 달성할 수 있음을 실증적으로 증명한 것이다.1 이는 모델 성능에 있어 데이터의 ’양’뿐만 아니라, 데이터의 ’질’과 ’구성’이 결정적인 역할을 한다는 점을 명확히 보여주었다.</p>
<p>이러한 접근은 데이터 큐레이션의 중요성을 부각시켰으며, AI 커뮤니티에 큰 영감을 주었다. Llama의 성공은 이후 RedPajama와 같은 Llama의 훈련 데이터셋을 재현하려는 오픈소스 프로젝트를 촉발시키는 직접적인 계기가 되었고, 데이터 중심의 LLM 개발 문화를 확산시켰다.9</p>
<h3>1.3  모델 제품군 개요: 7B, 13B, 33B, 65B</h3>
<p>Llama 1은 다양한 계산 리소스와 연구 목적에 대응하기 위해 7B, 13B, 33B, 65B의 네 가지 파라미터 크기로 구성된 모델 제품군으로 출시되었다.2 이러한 다각화 전략은 연구자들이 각자의 인프라 규모에 맞는 모델을 선택하여 실험할 수 있도록 함으로써 LLM 연구의 진입 장벽을 낮추는 데 크게 기여했다.</p>
<p>특히 Llama-13B 모델은 자신보다 10배 이상 큰 175B 파라미터의 GPT-3 모델을 대부분의 벤치마크에서 능가하는 성능을 보여주며 AI 커뮤니티에 큰 충격을 주었다.4 이 결과는 ’작은 모델의 효율성’이라는 Llama의 핵심 주장을 강력하게 입증했으며, Meta가 제시한 새로운 개발 패러다임에 대한 신뢰를 구축하는 결정적 계기가 되었다.</p>
<p>Meta는 Llama 1을 비상업적 연구용 라이선스로 배포하는 전략을 선택했다.2 이는 즉각적인 상업적 이익보다는 학계와 연구 커뮤니티 내에서의 영향력 확대를 우선시한 결정이었다. 연구자들이 Llama를 기반으로 자유롭게 실험하고 Stanford Alpaca, Vicuna와 같은 파생 모델을 만들도록 장려함으로써 14, Meta는 자연스럽게 Llama를 LLM 연구의 사실상 표준(de facto standard)으로 자리매김시켰다. 이 전략은 성공을 거두어 Llama 생태계를 폭발적으로 성장시키는 원동력이 되었고, 후속작인 Llama 2가 상업적 사용을 허용하며 더 넓은 생태계로 나아갈 수 있는 견고한 발판을 마련했다.</p>
<table><thead><tr><th>모델</th><th>파라미터 (B)</th><th>차원 (Dim)</th><th>어텐션 헤드 수</th><th>레이어 수</th><th>학습률 (LR)</th></tr></thead><tbody>
<tr><td>Llama-7B</td><td>7</td><td>4096</td><td>32</td><td>32</td><td>$3.0 \times 10^{-4}`</td></tr>
<tr><td>Llama-13B</td><td>13</td><td>5120</td><td>40</td><td>40</td><td>$3.0 \times 10^{-4}`</td></tr>
<tr><td>Llama-33B</td><td>33</td><td>6656</td><td>52</td><td>60</td><td>$1.5 \times 10^{-4}`</td></tr>
<tr><td>Llama-65B</td><td>65</td><td>8192</td><td>64</td><td>80</td><td>$1.5 \times 10^{-4}`</td></tr>
</tbody></table>
<h2>2.  Llama 1의 기술적 아키텍처 분석</h2>
<h3>2.1  표준 트랜스포머 아키텍처와의 차이점</h3>
<p>Llama 1은 Vaswani et al. (2017)이 제안한 표준 트랜스포머 아키텍처를 근간으로 한다. 그러나 독창적인 아키텍처를 새로 개발하기보다는, 훈련 안정성과 성능을 극대화하기 위해 GPT-3, PaLM, GPT-Neo 등 기존의 성공적인 모델들에서 그 효과가 검증된 세 가지 핵심 개선 사항을 선별하여 종합적으로 채택했다.17 이는 Llama 1의 아키텍처가 ’혁신’보다는 ’최적의 조합’을 통한 실용주의적 엔지니어링의 산물임을 보여준다. 즉, 연구개발의 리스크를 최소화하고 검증된 기술들을 통해 안정적으로 높은 성능을 달성하려는 전략적 선택이었다.</p>
<h3>2.2  Pre-normalization: RMSNorm의 도입</h3>
<p>훈련 안정성(training stability)을 향상시키기 위해, Llama 1은 각 트랜스포머 서브레이어의 출력(Post-normalization)이 아닌 입력(Pre-normalization)에 정규화를 적용했다.13 이는 GPT-3에서 영감을 받은 방식으로, 더 깊은 네트워크에서도 그래디언트가 안정적으로 전파되도록 돕는다.20</p>
<p>특히, Llama 1은 표준 Layer Normalization 대신 Biao Zhang과 Rico Sennrich (2019)가 제안한 RMSNorm(Root Mean Square Layer Normalization)을 사용했다.13 RMSNorm은 LayerNorm의 연산 과정에서 평균을 계산하는 부분(re-centering)을 제거하고, 제곱 평균 제곱근(RMS)을 이용한 재조정(re-scaling)에만 집중한다. 이를 통해 계산 복잡도를 줄여 효율성을 높이면서도 LayerNorm과 동등한 수준의 성능을 유지한다.20</p>
<p>입력 벡터 <span class="math math-inline">a \in \mathbb{R}^n</span>에 대한 RMSNorm은 다음과 같이 정의된다. LayerNorm의 평균(<span class="math math-inline">\mu</span>) 항이 제거되어 연산이 단순화된 것을 확인할 수 있다.20<br />
<span class="math math-display">
\text{RMS}(a) = \sqrt{\frac{1}{n} \sum_{i=1}^{n} a_i^2}
</span></p>
<p><span class="math math-display">
\bar{a}_i = \frac{a_i}{\text{RMS}(a)} g_i
</span></p>
<p>여기서 <span class="math math-inline">g_i</span>는 학습 가능한 gain 파라미터다.20</p>
<h3>2.3  활성화 함수: SwiGLU의 채택</h3>
<p>Llama 1은 기존 트랜스포머에서 널리 사용되던 ReLU(Rectified Linear Unit) 활성화 함수를 Noam Shazeer (2020)가 제안한 SwiGLU(Swish-Gated Linear Unit)로 대체하여 모델의 표현력과 성능을 향상시켰다.16</p>
<p>SwiGLU는 GLU(Gated Linear Unit)의 한 변형으로, Swish 활성화 함수를 게이트(gate) 메커니즘에 사용한다. 이 게이트는 입력 값에 따라 정보의 흐름을 동적으로 조절하는 역할을 한다. 이를 통해 모델은 더 유연하게 비선형성을 학습할 수 있으며, ReLU의 고질적인 문제인 ‘Dying ReLU’ 현상(뉴런이 비활성화되어 더 이상 학습에 기여하지 못하는 문제)을 완화할 수 있다.18</p>
<p>SwiGLU의 수학적 정의는 다음과 같다.24<br />
<span class="math math-display">
\text{SwiGLU}(x, W, V) = \text{Swish}_1(xW) \otimes (xV)
</span><br />
여기서 <span class="math math-inline">\text{Swish}_\beta(x) = x \cdot \sigma(\beta x)</span>이며, Llama에서는 <span class="math math-inline">\beta=1</span>을 사용한다. <span class="math math-inline">\otimes</span>는 두 행렬의 원소별 곱셈(element-wise product)을 의미한다.</p>
<h3>2.4  위치 임베딩: Rotary Positional Embeddings (RoPE)</h3>
<p>토큰의 순서 정보를 모델에 제공하기 위해, Llama 1은 기존의 절대 위치 임베딩(absolute positional embeddings)을 제거하고, Jianlin Su et al. (2021)이 제안한 RoPE(Rotary Positional Embeddings)를 네트워크의 각 레이어에 적용했다.16</p>
<p>RoPE는 토큰의 절대 위치를 하나의 회전 행렬로 인코딩한 후, 어텐션 메커니즘 내에서 쿼리(query)와 키(key) 벡터에 이 회전을 적용하는 독특한 방식이다. 이 방식의 핵심적인 장점은 두 토큰 벡터의 내적(dot product) 값이 그들의 임베딩 내용과 <strong>상대적인 위치</strong>에만 의존하도록 설계되었다는 점이다.18 이는 모델이 훈련 시 보았던 것보다 더 긴 시퀀스에 대해서도 위치 정보를 효과적으로 일반화하고, 토큰 간의 상대적 거리가 멀어질수록 어텐션 점수가 자연스럽게 감쇠하는 효과를 학습하도록 돕는다.27</p>
<p>수학적으로, 쿼리 <span class="math math-inline">q</span>와 키 <span class="math math-inline">k</span>에 위치 <span class="math math-inline">m</span>과 <span class="math math-inline">n</span>에 대한 RoPE를 적용한 내적은 상대 위치 <span class="math math-inline">m-n</span>에 대한 함수로 표현될 수 있다.26<br />
<span class="math math-display">
q_m^T k_n = (R_{\Theta,m}^d W_q x_m)^T (R_{\Theta,n}^d W_k x_n) = x_m^T W_q^T R_{\Theta, n-m}^d W_k x_n
</span><br />
위 식에서 <span class="math math-inline">R</span>은 위치에 따라 결정되는 회전 행렬이며, 내적 결과가 상대 위치 <span class="math math-inline">n-m</span>에 의존함을 보여준다.</p>
<p>이 세 가지 아키텍처 개선 사항은 개별적으로도 효과적이지만, 서로 상호 보완적으로 작용하여 ’안정성-성능-효율성’의 선순환 구조를 만들어낸다. 먼저, <strong>RMSNorm</strong>은 그래디언트 흐름을 안정시켜 더 깊은 네트워크의 안정적인 훈련을 가능하게 한다.13 이렇게 확보된 <strong>안정성</strong>은 <strong>SwiGLU</strong>와 같이 더 복잡하고 표현력 높은 활성화 함수를 효과적으로 사용할 수 있는 기반이 된다. SwiGLU는 ReLU보다 더 나은 <strong>성능</strong>을 제공한다.17 마지막으로, <strong>RoPE</strong>는 절대 위치 임베딩보다 더 긴 시퀀스에 대한 일반화 <strong>성능</strong>을 높이고, 상대적 위치 정보를 어텐션에 직접 주입하여 <strong>효율성</strong>을 개선한다.18 결과적으로, RMSNorm이 안정성을 잡고, SwiGLU와 RoPE가 성능을 끌어올리며, RMSNorm과 RoPE가 효율성을 뒷받침하는, 세 요소가 긴밀하게 연결된 최적화된 아키텍처가 완성된다.</p>
<h2>3.  훈련 방법론: 데이터와 최적화</h2>
<h3>3.1  사전 훈련 데이터셋</h3>
<p>Llama 모델들은 총 1.4조 개의 토큰으로 구성된 방대한 데이터셋으로 사전 훈련되었다.2 가장 작은 Llama-7B 모델은 1조 개의 토큰으로 훈련되었다.2 이 데이터셋의 가장 큰 특징은 모두 공개적으로 접근 가능한 소스로만 구성되었다는 점이다.3</p>
<p>주요 데이터 소스는 다음과 같다 3:</p>
<ul>
<li><strong>CommonCrawl:</strong> 웹에서 수집된 방대한 양의 텍스트 데이터</li>
<li><strong>C4:</strong> CommonCrawl을 정제한 고품질 데이터셋</li>
<li><strong>Github:</strong> 공개 소스 코드 저장소</li>
<li><strong>Wikipedia:</strong> 다국어 위키피디아 덤프</li>
<li><strong>Books:</strong> 공개 도서 자료 (Books3 등)</li>
<li><strong>ArXiv:</strong> 과학 및 학술 논문</li>
<li><strong>Stack Exchange:</strong> 다양한 주제에 대한 질의응답 데이터</li>
</ul>
<p>데이터는 중복 제거, 불완전한 문장 폐기, 공격적이거나 노이즈가 많은 콘텐츠 제거 등 엄격한 정제 과정을 거쳤다.20 또한, 전 세계적으로 널리 사용되는 20개 언어의 텍스트를 포함했으며, 특히 라틴 문자와 키릴 문자를 사용하는 언어에 중점을 두었다.2</p>
<p>Llama의 데이터셋 구성은 ’규모’와 ‘품질’ 사이의 전략적 균형을 보여준다. 데이터셋의 67%를 차지하는 CommonCrawl은 모델의 일반적인 언어 능력을 확보하는 데 필요한 방대한 양의 데이터를 제공한다.16 그러나 품질이 낮다는 단점을 보완하기 위해, Wikipedia(4.5%), Books(4.5%), ArXiv(2.5%)와 같이 고도로 정제되고 구조화된 고품질 데이터를 혼합했다.16 이는 저품질 대량 데이터의 ’일반성’과 고품질 소량 데이터의 ’정확성’을 결합하려는 의도적인 설계로, 모델이 광범위한 상식을 가지면서도 특정 전문 분야에서 신뢰도 높은 텍스트를 생성하는 능력의 기반이 되었다.</p>
<p>특히, 아래 표에서 볼 수 있듯이 대부분의 데이터셋은 약 1회의 에포크(epoch)로 훈련된 반면, Wikipedia와 Books 데이터셋은 2회 이상의 에포크로 훈련되었다.16 이 두 데이터 소스는 다른 웹 데이터에 비해 사실적 정보와 잘 구성된 문장의 밀도가 매우 높다. 특정 고품질 데이터셋을 반복적으로 학습시키는 것은 모델이 해당 데이터의 패턴과 지식을 더 깊이 내재화하도록 유도한다. 따라서 이는 모델의 사실 기반 응답 능력(factuality)과 문장의 일관성(coherence)을 강화하기 위한 의도적인 ‘과잉 학습(over-sampling)’ 전략으로 해석할 수 있다.</p>
<table><thead><tr><th>데이터 소스</th><th>샘플링 비율 (%)</th><th>1.4T 토큰 훈련 시 에포크 수</th><th>디스크 크기</th></tr></thead><tbody>
<tr><td>CommonCrawl</td><td>67.0</td><td>1.10</td><td>3.3 TB</td></tr>
<tr><td>C4</td><td>15.0</td><td>1.06</td><td>783 GB</td></tr>
<tr><td>Github</td><td>4.5</td><td>0.64</td><td>328 GB</td></tr>
<tr><td>Wikipedia</td><td>4.5</td><td>2.45</td><td>83 GB</td></tr>
<tr><td>Books</td><td>4.5</td><td>2.23</td><td>85 GB</td></tr>
<tr><td>ArXiv</td><td>2.5</td><td>1.06</td><td>92 GB</td></tr>
<tr><td>StackExchange</td><td>2.0</td><td>1.03</td><td>78 GB</td></tr>
</tbody></table>
<h3>3.2  최적화 기법</h3>
<p>모델 훈련에는 AdamW 옵티마이저가 사용되었다.16 하이퍼파라미터는 다음과 같이 설정되었다:</p>
<ul>
<li><span class="math math-inline">\beta_1 = 0.9</span>, <span class="math math-inline">\beta_2 = 0.95</span></li>
<li>가중치 감쇠(Weight decay): 0.1</li>
<li>그래디언트 클리핑(Gradient clipping): 1.0</li>
</ul>
<p>학습률 스케줄로는 코사인 스케줄(cosine schedule)을 채택했으며, 2,000번의 웜업(warmup) 단계를 거친 후 학습률을 최대치의 10%까지 점진적으로 감소시켰다.16 또한, 모델의 크기(7B, 13B, 33B, 65B)에 따라 최대 학습률과 배치 사이즈를 다르게 설정하여 각 모델에 최적화된 훈련을 진행했다.</p>
<h2>4.  성능 평가: 주요 벤치마크 결과 분석</h2>
<h3>4.1  벤치마크 개요</h3>
<p>Llama 1은 다양한 자연어 처리(NLP) 벤치마크에서 제로샷(zero-shot) 및 퓨샷(few-shot) 방식으로 평가되었다. 이는 모델이 특정 태스크에 대한 별도의 파인튜닝 없이, 사전 훈련된 일반적인 능력만으로 문제를 해결하는 능력을 측정하기 위함이다.16 주요 비교 대상은 당시 최고 성능 모델로 평가받던 GPT-3, Gopher, Chinchilla, PaLM 등이었다.12</p>
<h3>4.2  상식 추론 (Common Sense Reasoning)</h3>
<p>상식 추론 능력은 BoolQ, PIQA, SIQA, HellaSwag 등 8개의 벤치마크를 통해 평가되었다.16 결과적으로 Llama-65B는 대부분의 벤치마크에서 Chinchilla-70B와 PaLM-540B를 능가하는 강력한 성능을 보였다.5</p>
<p>가장 주목할 만한 결과는 Llama-13B가 자신보다 10배 이상 큰 GPT-3(175B)를 대부분의 벤치마크에서 앞서는 놀라운 성과를 거둔 것이다.4 이 결과는 ’파라미터 수’가 성능의 유일한 척도가 아님을 명확히 입증한 사례가 되었다. 이는 모델의 성능이 단순히 파라미터 수에 비례하는 것이 아니라, 훈련 데이터의 양과 질, 그리고 아키텍처의 효율성에 크게 좌우된다는 것을 실증적으로 보여주었다. 이 결과는 Chinchilla의 스케일링 법칙을 재확인하며, ’무조건 큰 모델’을 추구하던 당시의 트렌드에 제동을 걸고 ’컴퓨팅 최적화 모델(compute-optimal model)’이라는 개념을 대중화하는 데 결정적인 역할을 했다.</p>
<table><thead><tr><th>모델</th><th>BoolQ</th><th>PIQA</th><th>SIQA</th><th>HellaSwag</th><th>WinoGrande</th><th>ARC (e)</th><th>ARC (c)</th><th>OBQA</th></tr></thead><tbody>
<tr><td><strong>Llama-13B</strong></td><td>83.3</td><td>80.7</td><td>71.3</td><td>82.2</td><td>73.8</td><td>75.3</td><td>48.9</td><td>50.8</td></tr>
<tr><td>GPT-3 (175B)</td><td>76.5</td><td>81.0</td><td>68.9</td><td>78.9</td><td>70.2</td><td>71.2</td><td>41.6</td><td>48.0</td></tr>
<tr><td><strong>Llama-65B</strong></td><td><strong>85.2</strong></td><td><strong>82.8</strong></td><td><strong>75.1</strong></td><td><strong>85.4</strong></td><td>76.9</td><td><strong>82.3</strong></td><td><strong>57.8</strong></td><td><strong>56.8</strong></td></tr>
<tr><td>Chinchilla-70B</td><td>83.1</td><td>82.5</td><td>74.5</td><td>83.8</td><td>75.3</td><td>79.5</td><td>54.0</td><td>54.2</td></tr>
<tr><td>PaLM-540B</td><td>88.2</td><td>81.9</td><td>74.1</td><td>83.7</td><td>80.1</td><td>79.2</td><td>53.0</td><td>54.4</td></tr>
</tbody></table>
<h3>4.3  코드 생성 및 수학적 추론</h3>
<p>코드 생성 능력은 HumanEval과 MBPP 벤치마크에서, 수학적 추론 능력은 MATH와 GSM8k 벤치마크에서 평가되었다. Llama-65B는 코드 생성에서 PaLM-62B보다 더 나은 성능을 보였으며, 별도의 수학 데이터 파인튜닝 없이도 GSM8k 벤치마크에서 수학 전문 모델인 Minerva-62B를 능가하는 인상적인 결과를 기록했다.16 이는 Llama가 일반적인 언어 모델임에도 불구하고 특정 전문 분야에서 높은 잠재력을 가지고 있음을 시사한다.</p>
<table><thead><tr><th>모델</th><th>HumanEval</th><th>MBPP</th><th>MATH</th><th>GSM8k</th></tr></thead><tbody>
<tr><td><strong>Llama-13B</strong></td><td>18.3</td><td>29.8</td><td>6.2</td><td>14.6</td></tr>
<tr><td>LaMDA-137B</td><td>13.4</td><td>19.3</td><td>3.2</td><td>8.7</td></tr>
<tr><td><strong>Llama-65B</strong></td><td><strong>29.9</strong></td><td><strong>45.0</strong></td><td>18.8</td><td><strong>50.7</strong></td></tr>
<tr><td>PaLM-62B</td><td>26.2</td><td>36.8</td><td>8.8</td><td>17.9</td></tr>
<tr><td>Minerva-62B</td><td>-</td><td>-</td><td><strong>25.2</strong></td><td>49.3</td></tr>
</tbody></table>
<h3>4.4  종합 언어 이해 (MMLU)</h3>
<p>MMLU(Massive Multitask Language Understanding)는 인문학, STEM, 사회 과학 등 57개 분야의 전문 지식을 종합적으로 평가하는 까다로운 벤치마크다. 이 벤치마크에서 Llama-65B는 Chinchilla-70B와 PaLM-540B에 비해 다소 뒤처지는 결과를 보였다. 논문 저자들은 이를 사전 훈련 데이터에서 책(Books)과 학술 논문(ArXiv)의 비중이 상대적으로 적었기 때문일 수 있다고 분석했다.16</p>
<p>이러한 벤치마크별 성능 편차는 Llama 1의 훈련 데이터 ’레시피’의 강점과 약점을 투명하게 드러낸다. 상식 추론이나 코드 생성과 같은 광범위한 작업에서의 뛰어난 성능은 CommonCrawl, Github, StackExchange 등 다양한 소스를 혼합한 데이터셋의 힘을 보여준다.3 반면, MMLU에서의 상대적 약세는 훈련 데이터셋에서 ArXiv(2.5%)와 Books(4.5%)의 비중이 낮았던 것과 직접적인 관련이 있다.16 따라서, Llama 1의 벤치마크 결과는 그 자체로 하나의 ‘데이터셋 성능 분석 안내서’ 역할을 하며, 특정 분야의 성능을 높이려면 해당 분야의 고품질 데이터를 더 많이 포함해야 한다는 직관적인 교훈을 명확한 수치로 제공한다.</p>
<h2>5.  영향과 유산: AI 생태계의 변화를 이끌다</h2>
<h3>5.1  연구의 민주화와 생태계의 폭발적 성장</h3>
<p>Llama 1의 등장은 LLM 연구의 접근성을 획기적으로 개선하며 ’연구의 민주화’를 이끌었다.6 이전까지 거대 기업의 전유물로 여겨졌던 고성능 LLM을 학계 연구자나 개인 개발자도 사용할 수 있게 되면서, 관련 연구와 개발이 폭발적으로 증가했다.2 Llama 모델의 다운로드 횟수는 기하급수적으로 증가했으며 30, Hugging Face와 같은 플랫폼에는 수만 개의 Llama 파생 모델이 등록되는 등 활발한 커뮤니티가 형성되었다.30 이러한 현상은 Stable Diffusion이 이미지 생성 분야에서 일으켰던 변화와 비견될 만큼 강력한 파급력을 가졌다.3</p>
<h3>5.2  ‘오픈소스’ vs. ‘오픈 웨이트(Open Weight)’ 논쟁</h3>
<p>Meta는 Llama를 ’오픈소스’로 적극 홍보했지만 6, 오픈소스 이니셔티브(OSI)를 비롯한 커뮤니티 일각에서는 이에 대한 비판이 제기되었다.32 Llama 1의 라이선스는 상업적 이용을 금지하고 폭력, 범죄 등 특정 용도를 제한하는데, 이는 OSI가 정의하는 ‘사용 목적에 대한 차별 금지’ 원칙에 위배된다.32 또한, 모델을 완전히 재현하는 데 필요한 훈련 코드와 전체 데이터셋이 공개되지 않았기 때문에, ’소스 코드’의 공개를 핵심으로 하는 오픈소스의 정의를 충족하지 못한다는 지적이 있었다.34</p>
<p>이로 인해 Llama와 같이 가중치는 공개되었지만 소스 코드나 데이터가 완전히 공개되지 않은 모델을 ’오픈소스’가 아닌 ‘오픈 웨이트(Open Weight)’ 또는 ‘공개 가중치’ 모델로 불러야 한다는 주장이 힘을 얻었다.34 이 논쟁은 단순한 명칭 문제를 넘어, AI 기술의 통제권과 미래 방향성에 대한 중요한 대리전(proxy war)의 성격을 띠었다. Meta의 ‘오픈소스’ 브랜딩은 개발자 커뮤니티의 지지를 얻기 위한 전략이었던 반면 29, OSI의 반박은 용어의 가치가 희석되는 것을 막으려는 시도였다.32 이는 AI 기술의 미래가 소수 기업이 제공하는 플랫폼 위에서 발전할 것인지, 아니면 커뮤니티에 의해 완전히 재창조될 수 있는 형태로 나아갈 것인지에 대한 철학적, 전략적 갈등을 반영한다.</p>
<h3>5.3  파생 모델의 탄생: Alpaca와 Vicuna</h3>
<p>Llama 1은 그 자체로도 강력한 기반 모델이었지만, 진정한 파급력은 이를 기반으로 한 수많은 파인튜닝 모델들을 통해 나타났다.</p>
<ul>
<li><strong>Stanford Alpaca:</strong> 스탠포드 대학 연구팀은 Llama-7B 모델을 OpenAI의 <code>text-davinci-003</code>으로 생성한 52,000개의 지시-응답 데이터로 파인튜닝하여 Alpaca를 개발했다.14 단 600달러 미만의 비용으로 ChatGPT와 유사한 지시 수행 능력을 보여주며, 저비용 파인튜닝의 엄청난 잠재력을 증명했다.36</li>
<li><strong>Vicuna:</strong> UC 버클리 등 공동 연구팀은 Llama-13B 모델을 ShareGPT.com에서 수집한 약 7만 개의 실제 사용자 대화 데이터로 파인튜닝하여 Vicuna를 개발했다.15 GPT-4를 이용한 평가에서 ChatGPT 품질의 90% 이상을 달성했다고 주장하며, 대화형 AI의 성능을 한 단계 끌어올렸다.38</li>
</ul>
<p>이러한 모델들의 등장은 LLM 개발의 무게 중심을 ’사전 훈련(Pre-training)’에서 ’파인튜닝(Fine-tuning)’으로 이동시키는 계기가 되었다. Llama 1 이전에는 최첨단 LLM을 만드는 것이 곧 막대한 비용의 사전 훈련과 동의어였다. 그러나 Llama 1은 강력한 사전 훈련된 ’베이스캠프’를 제공했고, Alpaca와 Vicuna는 이 베이스캠프에서 출발하여 적은 비용과 노력으로도 특정 작업에 대한 고성능이라는 ’정상’에 도달할 수 있음을 보여주었다.14 이로 인해 수많은 연구자와 개발자들이 거대 자본의 사전 훈련 경쟁에서 벗어나, 창의적인 파인튜닝 데이터셋 구축과 효율적인 튜닝 기법 개발에 집중하게 되었다. 이는 LLM 생태계의 혁신이 더 이상 거대 기업의 독점물이 아니라, 아이디어와 실행력을 갖춘 소규모 팀에게도 열려있음을 의미하는 패러다임의 전환이었다.</p>
<table><thead><tr><th>모델</th><th>기반 Llama 모델</th><th>파인튜닝 데이터</th><th>핵심 특징</th></tr></thead><tbody>
<tr><td><strong>Alpaca</strong></td><td>Llama-7B</td><td>Self-instruct 데이터 52K (GPT-3.5 생성)</td><td>저비용으로 지시 수행 능력 확보 가능성 입증</td></tr>
<tr><td><strong>Vicuna</strong></td><td>Llama-13B</td><td>ShareGPT 실제 사용자 대화 70K</td><td>고품질 대화 데이터 기반으로 대화 능력 극대화</td></tr>
</tbody></table>
<h2>6.  결론: Llama 1의 재평가와 미래 전망</h2>
<h3>6.1  Llama 1의 종합적 재평가</h3>
<p>Llama 1은 기술적으로 기존의 성공적인 기법들을 영리하게 조합한 ’최적화된 모델’이었지만, 그 파급력은 LLM 역사상 가장 ‘혁신적인 모델’ 중 하나로 평가받기에 충분하다. Llama 1은 단순히 하나의 모델을 공개한 것을 넘어, LLM 개발의 방향성에 대한 새로운 화두를 던졌다.</p>
<p>’효율성’이라는 개념을 통해 LLM 경쟁의 판도를 바꾸었고, ’접근성’을 제공하여 AI 연구의 민주화를 이끌었으며, 강력한 기반 모델로서 수많은 파생 모델의 탄생을 가능하게 한 ’생태계의 초석’이 되었다. Llama 1의 가치는 모델 자체의 성능을 넘어, AI 커뮤니티 전체에 미친 영향력에 있다.</p>
<h3>6.2  Llama 시리즈의 미래와 시사점</h3>
<p>Llama 1의 성공은 Llama 2, Llama 3로 이어지는 Meta의 개방형 AI 전략의 견고한 기틀을 마련했다.11 Llama 2는 상업적 이용을 허용하며 생태계를 산업계로 확장했고, Llama 3.1 405B와 같은 최신 모델은 최고의 폐쇄형 모델과 직접 경쟁하는 수준에 이르렀다.40</p>
<p>Llama 1이 시작한 ’개방’의 흐름은 AI 기술의 발전이 더 이상 소수 기업에 의해 독점되지 않을 수 있다는 가능성을 보여주었다. 전 세계 개발자 커뮤니티와의 협력을 통해 더 빠르고, 더 다양하며, 잠재적으로 더 안전한 방향으로 AI가 발전할 수 있음을 증명한 것이다. Llama 1이 남긴 유산은 앞으로도 AI 생태계의 발전에 지속적인 영향을 미칠 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>[PDF] LLaMA: Open and Efficient Foundation Language Models - Semantic Scholar, https://www.semanticscholar.org/paper/LLaMA%3A-Open-and-Efficient-Foundation-Language-Touvron-Lavril/57e849d0de13ed5f91d086936296721d4ff75a75</li>
<li>Introducing LLaMA: A foundational, 65-billion-parameter language …, https://ai.meta.com/blog/large-language-model-llama-meta-ai/</li>
<li>Llama (language model) - Wikipedia, https://en.wikipedia.org/wiki/Llama_(language_model)</li>
<li>Paper page - LLaMA: Open and Efficient Foundation Language …, https://huggingface.co/papers/2302.13971</li>
<li>(PDF) LLaMA: Open and Efficient Foundation Language Models - ResearchGate, https://www.researchgate.net/publication/368842729_LLaMA_Open_and_Efficient_Foundation_Language_Models</li>
<li>Llama: shaping the future of AI as a community, https://hellofuture.orange.com/en/llama-shaping-the-future-of-ai-as-a-community/</li>
<li>LLaMA: Open and Efficient Foundation Language Models …, https://ai.meta.com/research/publications/llama-open-and-efficient-foundation-language-models/</li>
<li>(PDF) LLaMA: Open and Efficient Foundation Language Models (2023) | Hugo Touvron | 6015 Citations - SciSpace, https://scispace.com/papers/llama-open-and-efficient-foundation-language-models-1aeusmr9</li>
<li>RedPajama: an Open Dataset for Training Large Language Models - arXiv, https://arxiv.org/html/2411.12372v1</li>
<li>How Have Pre-Training Datasets for Large Language Models Evolved? - Medium, https://medium.com/@jelkhoury880/how-have-pre-training-datasets-for-large-language-models-evolved-13d74c01f8e8</li>
<li>Introduction to Meta AI’s LLaMA: Empowering AI Innovation | DataCamp, https://www.datacamp.com/blog/introduction-to-meta-ai-llama</li>
<li>[R] Meta AI open sources new SOTA LLM called LLaMA. 65B version (trained on 1.4T tokens) is competitive with Chinchilla and Palm-540B. 13B version outperforms OPT and GPT-3 175B on most benchmarks. : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/11awp4n/r_meta_ai_open_sources_new_sota_llm_called_llama/</li>
<li>Vinija’s Notes • Models • LLaMA, https://vinija.ai/models/LLaMA/</li>
<li>LLaMA vs Alpaca: Comparing the Animal-Inspired AI Models - ProjectPro, https://www.projectpro.io/article/llama-vs-alpaca-models/866</li>
<li>arXiv:2304.03277v1 [cs.CL] 6 Apr 2023, https://arxiv.org/pdf/2304.03277</li>
<li>arXiv:2302.13971v1 [cs.CL] 27 Feb 2023, https://arxiv.org/pdf/2302.13971</li>
<li>Llama - Hugging Face, https://huggingface.co/docs/transformers/model_doc/llama</li>
<li>The Evolution of Llama: From Llama 1 to Llama 3.1 - Towards Data Science, https://towardsdatascience.com/the-evolution-of-llama-from-llama-1-to-llama-3-1-13c4ebe96258/</li>
<li>The Evolution of Meta’s LLaMA Model | by Nathan Bailey | Medium, https://nathanbaileyw.medium.com/the-evolution-of-metas-llama-model-db82623da2d2</li>
<li>LLaMA: Concepts Explained (Summary) | by Anshu Kumar - Medium, https://akgeni.medium.com/llama-concepts-explained-summary-a87f0bd61964</li>
<li>[1910.07467] Root Mean Square Layer Normalization - arXiv, https://arxiv.org/abs/1910.07467</li>
<li>Root Mean Square Layer Normalization, https://arxiv.org/pdf/1910.07467</li>
<li>Aman’s AI Journal • Models • LLaMA, https://aman.ai/primers/ai/LLaMA/</li>
<li>GLU Variants Improve Transformer, https://arxiv.org/abs/2002.05202</li>
<li>MABViT - Modified Attention Block Enhances Vision Transformers - arXiv, https://arxiv.org/html/2312.01324v2</li>
<li>RoFormer: Enhanced Transformer with Rotary Position Embedding, https://arxiv.org/abs/2104.09864</li>
<li>[2104.09864] RoFormer: Enhanced Transformer with Rotary Position Embedding - ar5iv, https://ar5iv.labs.arxiv.org/html/2104.09864</li>
<li>Round and Round We Go! What makes Rotary Positional Encodings useful? - arXiv, https://arxiv.org/html/2410.06205v1</li>
<li>Is Open-Source the Future of AI? Unpacking Meta’s Llama 3.1 Release - Medium, https://medium.com/@muzammil.rawjani/is-open-source-the-future-of-ai-unpacking-metas-llama-3-1-release-3964057a136e</li>
<li>The future of AI: Built with Llama - AI at Meta, https://ai.meta.com/blog/future-of-ai-built-with-llama/</li>
<li>With 10x growth since 2023, Llama is the leading engine of AI innovation - AI at Meta, https://ai.meta.com/blog/llama-usage-doubled-may-through-july-2024/</li>
<li>Meta’s LLaMa license is still not Open Source, https://opensource.org/blog/metas-llama-license-is-still-not-open-source</li>
<li>What Is Llama 2? | IBM, https://www.ibm.com/think/topics/llama-2</li>
<li>OSI Calls Out Meta for its Misleading ‘Open Source’ AI Models : r/LocalLLaMA - Reddit, https://www.reddit.com/r/LocalLLaMA/comments/1g78uig/osi_calls_out_meta_for_its_misleading_open_source/</li>
<li>tatsu-lab/stanford_alpaca: Code and documentation to train Stanford’s Alpaca models, and generate the data. - GitHub, https://github.com/tatsu-lab/stanford_alpaca</li>
<li>Alpaca: A Strong, Replicable Instruction-Following Model - Stanford CRFM, https://crfm.stanford.edu/2023/03/13/alpaca.html</li>
<li>Stanford Alpaca, and the acceleration of on-device large language model development, https://simonwillison.net/2023/Mar/13/alpaca/</li>
<li>Vicuna: an Open-Source Large Language Model for Chatbots - NovitaAI, https://novita.hashnode.dev/vicuna-an-open-source-large-language-model-for-chatbots</li>
<li>The Significance of Vicuna, an Open-Source Large Language Model for Chatbots - Medium, https://medium.com/mlearning-ai/the-significance-of-vicuna-an-open-source-large-language-model-for-chatbots-23b4765711ff</li>
<li>Introducing Llama 3.1: Our most capable models to date - AI at Meta, https://ai.meta.com/blog/meta-llama-3-1/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>