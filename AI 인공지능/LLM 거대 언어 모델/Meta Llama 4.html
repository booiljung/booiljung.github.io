<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Meta Llama 4 (2025-04-05)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Meta Llama 4 (2025-04-05)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">거대 언어 모델 (LLM, Large Language Models)</a> / <span>Meta Llama 4 (2025-04-05)</span></nav>
                </div>
            </header>
            <article>
                <h1>Meta Llama 4 (2025-04-05)</h1>
<h2>1.  Llama 4와 희소 멀티모달 아키텍처의 서막</h2>
<h3>1.1  Llama 4 발표의 의의</h3>
<p>2025년 4월 5일, Meta는 인공지능(AI) 분야의 새로운 이정표를 제시하며 Llama 4 모델군을 공식 발표했다.1 이 발표는 단순한 모델 성능 향상을 넘어, Meta의 AI 전략 방향과 오픈-웨이트(open-weight) 생태계에 대한 근본적인 패러다임 전환을 선언하는 중대한 사건으로 평가된다. Llama 3까지 이어져 온 고밀도(dense) 트랜스포머 아키텍처의 전통에서 벗어나, 희소(sparse) 아키텍처로의 과감한 전환을 통해 모델의 확장성과 효율성이라는 두 마리 토끼를 동시에 잡으려는 시도를 본격화한 것이다.3 Llama 4는 AI 모델이 더 크고 유능해져야 한다는 압박과 동시에, 기하급수적으로 증가하는 계산 비용을 제어해야 하는 산업계의 딜레마에 대한 Meta의 구체적인 해답을 담고 있다.</p>
<h3>1.2  핵심 기술 혁신 개요</h3>
<p>Llama 4의 혁신은 세 가지 핵심 기술 축을 중심으로 전개된다. 본 안내서는 이 세 가지 기술적 도약을 심층적으로 분석하여 Llama 4가 AI 시장에 던지는 함의를 탐구하고자 한다.</p>
<p>첫째, <strong>전문가 혼합(Mixture-of-Experts, MoE) 아키텍처</strong>의 전면적인 도입이다. 이는 모델의 전체 파라미터 수를 폭발적으로 증가시키면서도, 추론 시에는 일부 파라미터만 선택적으로 활성화하여 계산 비용을 합리적인 수준으로 제어하는 희소 모델링 기법이다.1 이를 통해 Llama 4는 전례 없는 규모의 지식을 내장하면서도 효율적인 추론 성능을 달성하고자 했다.</p>
<p>둘째, **네이티브 멀티모달리티(Native Multimodality)**의 구현이다. 기존 모델들이 텍스트와 이미지를 별개의 모듈로 처리한 후 나중에 결과를 결합하는 ‘후기 융합(Late Fusion)’ 방식을 사용한 것과 달리, Llama 4는 ’초기 융합(Early Fusion)’이라는 새로운 접근법을 채택했다.1 이는 텍스트, 이미지, 심지어 비디오 데이터까지 초기 입력 단계에서부터 하나의 통합된 데이터 스트림으로 처리하여, 모달리티 간의 깊고 유기적인 관계를 학습하도록 설계되었다.</p>
<p>셋째, <strong>초장문 컨텍스트(Unparalleled Long Context)</strong> 처리 능력의 확보다. 특히 Llama 4 Scout 모델은 업계 최고 수준인 1,000만 토큰의 컨텍스트 창을 지원하며, 이는 기존 모델들의 한계를 크게 뛰어넘는 수치이다.5 이러한 혁신은 위치 임베딩을 사용하는 레이어와 사용하지 않는 레이어를 교차 배치하는 독창적인 ‘iRoPE’ 아키텍처를 통해 가능해졌다.1</p>
<p>이 세 가지 혁신은 Llama 4를 이전 세대와 차별화하는 핵심 요소이며, AI 모델의 설계, 학습, 그리고 활용 방식에 대한 근본적인 질문을 던진다.</p>
<h2>2.  Llama 4의 핵심 아키텍처 혁신</h2>
<h3>2.1  전문가 혼합(MoE) 패러다임: 희소성과 효율성의 조화</h3>
<h4>2.1.1 이론적 배경</h4>
<p>고밀도 트랜스포머 아키텍처는 모델의 파라미터 수가 증가함에 따라 성능이 향상되는 스케일링 법칙(scaling law)을 입증했지만, 동시에 모든 토큰을 처리할 때마다 모델의 모든 파라미터가 활성화되어야 한다는 근본적인 비효율성을 내포하고 있었다.4 이는 계산 비용의 기하급수적 증가로 이어져 모델 확장의 실질적인 한계로 작용했다. MoE는 이러한 한계를 극복하기 위한 대안으로, 모델의 총 파라미터 수(지식의 총량)와 추론 시 활성화되는 파라미터 수(실제 계산 비용)를 분리하는 아이디어에 기반한다.8 이를 통해 모델은 방대한 지식을 보유하면서도, 각 입력에 대해 가장 관련성 높은 지식(전문가)만을 선택적으로 활용하여 계산 효율성을 극대화할 수 있다.</p>
<h4>2.1.2 작동 메커니즘</h4>
<p>MoE 아키텍처는 크게 ’전문가(Experts)’와 이들을 조율하는 ‘게이팅 네트워크(Gating Network)’ 또는 ’라우터(Router)’로 구성된다.</p>
<ul>
<li><strong>전문가(Experts)</strong>: 각 전문가는 특정 영역의 데이터 처리(예: 프로그래밍 언어의 구문 분석, 과학적 추론, 창의적 작문 등)에 특화된 독립적인 신경망 서브네트워크다.5 Llama 4에서는 일반적으로 트랜스포머 블록의 피드포워드 네트워크(FFN) 부분을 다수의 전문가 네트워크로 대체하는 방식을 사용한다.11</li>
<li><strong>게이팅 네트워크(Router)</strong>: 라우터는 입력된 각 토큰의 특성을 분석하여, 해당 토큰을 처리하기에 가장 적합하다고 판단되는 전문가(들)에게 동적으로 전달하는 역할을 수행한다.3 Llama 4 Maverick 모델의 경우, 각 토큰은 모든 토큰이 공통으로 거치는 1개의 ’공유 전문가(shared expert)’와, 라우터에 의해 선택된 128개의 ‘라우팅된 전문가(routed experts)’ 중 단 1개, 즉 총 2개의 전문가에게만 보내진다.6</li>
<li><strong>희소 활성화(Sparse Activation)</strong>: MoE의 핵심은 희소 활성화에 있다. 예를 들어, Llama 4 Maverick은 총 4000억 개(400B)의 방대한 파라미터를 가지고 있지만, 특정 토큰을 처리하는 순간에는 오직 170억 개(17B)의 파라미터만이 활성화된다.3 이는 전체 전문가 중 극소수(top-k, Maverick의 경우 k=1)만이 계산에 참여하기 때문에 가능한 일이며, 이를 통해 계산 비용을 획기적으로 절감한다.</li>
</ul>
<h4>2.1.3 고밀도 모델과의 비교</h4>
<p>MoE 아키텍처는 고밀도 모델과 비교했을 때 뚜렷한 장단점을 가진다.</p>
<ul>
<li><strong>계산 효율성</strong>: 동일한 총 파라미터 수를 가진 고밀도 모델과 비교할 때, MoE 모델은 토큰당 필요한 부동소수점 연산(FLOPs)의 수를 현저히 줄일 수 있다.13 Llama 3 70B 모델이 토큰당 약 140 TFLOPs를 요구하는 반면, Llama 4 Maverick(17B 활성)은 약 42 TFLOPs만을 필요로 하여 거의 3배에 가까운 계산 효율성을 보인다.13</li>
<li><strong>메모리 요구사항</strong>: MoE의 계산 효율성은 추론 과정에 국한된 이야기다. 모델을 실행하기 위해서는 비활성화된 전문가를 포함한 모든 파라미터(Maverick의 경우 400B)를 GPU VRAM에 상주시켜야 한다.15 이는 MoE 모델이 계산 병목 현상을 상당 부분 해소하는 대신, 극심한 메모리 병목 현상을 야기함을 의미한다. 결과적으로 Llama 4와 같은 대규모 MoE 모델을 구동하기 위해서는 단일 소비자용 GPU로는 불가능하며, 수백 GB의 VRAM을 갖춘 NVIDIA H100과 같은 고가의 서버급 GPU 여러 개를 필요로 한다.16 이러한 구조는 ’성능 대비 비용’이라는 개념을 재정의하게 만든다. API를 통한 추론 비용은 저렴할 수 있으나, 모델을 직접 호스팅하거나 미세 조정하는 데 필요한 총소유비용(TCO)은 막대한 하드웨어 인프라 비용으로 인해 오히려 증가할 수 있다. 이는 Meta가 Llama 4를 ’효율적’이라고 홍보하는 것과, 개발자들이 ’실행하기 어렵다’고 느끼는 인식의 괴리를 설명하는 핵심적인 지점이다.18</li>
<li><strong>학습의 어려움</strong>: MoE 모델의 학습은 고밀도 모델보다 복잡하며 여러 기술적 과제를 동반한다. 특정 전문가에게만 계산이 편중되는 ‘부하 불균형(load balancing)’ 문제, 학습 과정의 불안정성, 그리고 최적의 라우팅 정책을 학습시키는 것의 어려움 등이 대표적이다.19 Llama 4는 이러한 문제를 완화하기 위해 MoE 레이어와 고밀도 트랜스포머 레이어를 번갈아 배치하는 교차(interleaved) 아키텍처를 채택하여, 네트워크 전체의 정보 흐름 안정성을 높이고 라우팅의 부담을 줄였다.21</li>
</ul>
<h3>2.2  네이티브 멀티모달리티: 초기 융합(Early Fusion) 접근법</h3>
<h4>2.2.1 개념 정의</h4>
<p>Llama 4의 또 다른 핵심 혁신은 ’네이티브 멀티모달리티’를 구현한 방식에 있다. 기존의 많은 멀티모달 모델들은 텍스트와 이미지를 처리하는 각각의 인코더를 독립적으로 학습시킨 후, 후반 단계에서 그 결과물을 결합하는 ‘후기 융합(Late Fusion)’ 방식을 사용했다.22 이와 대조적으로 Llama 4는 텍스트 토큰과 이미지(또는 비디오)에서 추출된 시각 토큰을 모델의 가장 초기 입력 단계에서부터 하나의 통합된 시퀀스로 결합하는 ‘초기 융합(Early Fusion)’ 아키텍처를 채택했다.1 이는 Llama 4가 단순히 여러 종류의 데이터를 처리하는 것을 넘어, 처음부터 세상을 통합된 시각으로 학습하도록 설계되었음을 의미한다.</p>
<h4>2.2.2 기술적 구현</h4>
<p>초기 융합 과정은 다음과 같이 이루어진다. 먼저, MetaCLIP과 같은 강력한 비전 인코더를 사용하여 이미지나 비디오 프레임을 텍스트와 동일한 고차원 임베딩 공간으로 투영한다.24 이렇게 생성된 시각 토큰들은 텍스트 토큰들과 나란히 연결(concatenate)되어 하나의 긴 시퀀스를 형성한다. 이 통합된 시퀀스는 트랜스포머의 첫 번째 레이어부터 입력되어, 모든 레이어에서 텍스트와 시각 정보가 함께 처리된다.13 이 구조 덕분에 모델 내의 셀프 어텐션 메커니즘은 모달리티 간의 경계를 넘어 상호작용할 수 있다. 예를 들어, “파란색 자동차“라는 텍스트 토큰이 이미지 내의 실제 파란색 자동차 픽셀 영역을 나타내는 시각 토큰에 직접 어텐션(cross-modal attention)을 수행하며 깊은 연관성을 학습하게 된다.13</p>
<h4>2.2.3 장점 및 시사점</h4>
<p>초기 융합 방식은 후기 융합 방식에 비해 여러 가지 중요한 장점을 제공한다.</p>
<ul>
<li><strong>깊은 상호작용 학습</strong>: 모달리티 간의 상호작용이 모델의 초기 레이어부터 발생하므로, 단순한 표면적 연관성을 넘어 미묘하고 복잡한 관계를 학습할 수 있다. 이는 이미지의 특정 영역에 대한 정밀한 텍스트 설명을 생성하거나(Expert Image Grounding), 다이어그램과 텍스트가 혼합된 기술 문서를 깊이 있게 이해하는 등 고차원적인 멀티모달 추론 능력의 기반이 된다.26</li>
<li><strong>대규모 비지도 학습의 효율성</strong>: 초기 융합 아키텍처는 레이블이 없는 방대한 양의 텍스트, 이미지, 비디오 데이터를 한 번에 공동으로 사전 학습(jointly pre-train)하는 것을 가능하게 한다.1 Llama 4는 30조 개가 넘는 멀티모달 토큰 데이터셋으로 학습되었으며, 이는 모델의 일반화 성능과 세상에 대한 상식적 이해 능력을 크게 향상시키는 데 기여했다.24</li>
<li><strong>통합 지능으로의 발전</strong>: 초기 융합 아키텍처의 채택은 단순한 기술적 선택을 넘어, Meta의 장기적인 AI 전략과 맞닿아 있다. 이는 Meta의 수석 AI 과학자 Yann LeCun이 주창하는 ‘월드 모델(World Models)’ 비전과 깊은 관련이 있다.29 월드 모델은 세상을 텍스트만으로 이해하는 것이 아니라, 시각 정보와 물리적 법칙을 포함한 통합된 내부 모델을 구축하여 예측하고 계획하는 능력을 갖춘 AI를 지향한다.31 Llama 4의 초기 융합 방식은 이처럼 분리된 정보들을 하나의 통합된 지능 체계로 묶어내려는 시도의 구체적인 아키텍처적 발현이라고 볼 수 있다. 이는 Llama 4를 단순한 챗봇 경쟁을 넘어, 인공 일반 지능(AGI)을 향한 경쟁의 주요 주자로 포지셔닝하려는 Meta의 전략적 의도를 보여준다.</li>
</ul>
<h3>2.3  컨텍스트의 한계 돌파: 1,000만 토큰과 iRoPE 아키텍처</h3>
<h4>2.3.1 문제 정의</h4>
<p>트랜스포머 아키텍처의 가장 큰 제약 중 하나는 컨텍스트 길이 처리 문제다. 셀프 어텐션 메커니즘은 시퀀스 내 모든 토큰 쌍 간의 상호작용을 계산해야 하므로, 시퀀스 길이(<span class="math math-inline">N</span>)에 대해 계산 복잡도가 제곱(<span class="math math-inline">O(N^2)</span>)으로 증가한다.32 또한, 이전 토큰들의 정보를 저장하는 Key-Value(KV) 캐시의 메모리 사용량은 시퀀스 길이에 비례하여 선형적으로 증가한다.33 이러한 문제들로 인해 수만 토큰을 넘어서는 초장문 컨텍스트를 효율적으로 처리하는 것은 기존 모델들에게 큰 도전 과제였다.</p>
<h4>2.3.2 iRoPE (interleaved Rotary Position Embedding) 아키텍처</h4>
<p>Llama 4 Scout는 이러한 한계를 극복하고 1,000만 토큰이라는 전례 없는 컨텍스트 창을 지원하기 위해 ’iRoPE’라는 혁신적인 아키텍처를 도입했다.1</p>
<ul>
<li><strong>RoPE의 원리</strong>: 회전 위치 임베딩(Rotary Position Embedding, RoPE)은 토큰의 절대 위치를 복소평면상의 회전으로 인코딩하는 방식이다. 이를 통해 셀프 어텐션 계산 과정에서 두 토큰 간의 상대적 위치 정보가 자연스럽게 반영되도록 한다.34 RoPE는 긴 시퀀스에 대한 일반화 성능이 우수하여 많은 최신 LLM에서 표준 기술로 자리 잡았다.</li>
<li><strong>’interleaved’의 개념</strong>: iRoPE의 핵심 아이디어는 모든 어텐션 레이어에 RoPE를 일관되게 적용하는 대신, RoPE를 사용하는 레이어와 위치 임베딩을 전혀 사용하지 않는(No Positional Encoding, NoPE) 레이어를 의도적으로 교차(interleave)하여 배치하는 것이다.1</li>
<li><strong>작동 방식</strong>: 이 하이브리드 구조는 두 메커니즘의 장점을 결합하여 시너지를 창출한다. RoPE가 적용된 레이어(특히 슬라이딩 윈도우 어텐션과 결합된 SWA-RoPE)는 제한된 국소적(local) 범위 내에서 토큰 간의 정밀한 순서와 상대적 위치 정보를 제공한다.37 반면, NoPE가 적용된 전역적(global) 어텐션 레이어는 위치 정보에 구애받지 않고 컨텍스트 전체에 걸쳐 의미적으로 중요한 정보들 간의 장거리 의존성을 포착한다. NoPE 레이어는 명시적인 위치 정보가 없더라도, 디코더의 인과적 어텐션 마스크(causal attention mask) 구조 덕분에 토큰의 순서를 암묵적으로 학습할 수 있는 능력이 있다.36 즉, RoPE 레이어가 ’나무’를 보고, NoPE 레이어가 ’숲’을 보게 함으로써, 모델은 국소적 정밀성과 전역적 이해력을 동시에 확보하여 훈련 시 경험하지 못한 매우 긴 시퀀스에 대해서도 일반화할 수 있는 능력을 갖추게 된다.</li>
</ul>
<h4>2.3.3 실용적 과제</h4>
<p>1,000만 토큰이라는 컨텍스트 길이는 이론적으로는 혁신적이지만, 현실적인 적용에는 여러 가지 심각한 과제가 따른다.</p>
<ul>
<li><strong>메모리 요구사항</strong>: 1,000만 토큰의 KV 캐시를 저장하기 위해서는 수백 기가바이트(GB)의 VRAM이 필요하다.33 예를 들어, INT4 양자화를 적용하더라도 Llama 4 Scout가 1,000만 토큰 컨텍스트를 처리하려면 약 75GB의 VRAM이 필요하며, 이는 단일 H100 GPU의 용량에 육박하는 수치다.38 이는 현재 상용화된 하드웨어의 한계를 시험하는 수준이다.</li>
<li><strong>성능 저하 문제</strong>: 긴 컨텍스트를 처리할 때 모델이 입력의 중간 부분에 있는 정보를 제대로 활용하지 못하는 ‘가운데에서 길을 잃는(Lost in the Middle)’ 현상은 여전히 해결되지 않은 문제다.32 실제로 Llama 4를 대상으로 한 ‘건초더미에서 바늘 찾기(Needle in a Haystack)’ 테스트 결과, 컨텍스트 길이가 증가함에 따라 정보 검색 정확도가 급격히 하락하는 모습을 보였다.40</li>
<li><strong>개발자 커뮤니티의 회의적 반응</strong>: 많은 개발자들이 실제 사용 환경에서 1,000만 토큰 컨텍스트가 광고된 만큼 효과적으로 작동하지 않는다고 보고했다. 여러 개의 긴 문서를 입력하고 요약을 요청하는 비교적 간단한 작업에서도 모델이 마지막 문서만 처리하고 나머지는 무시하는 등의 실패 사례가 공유되었다.42 이는 Llama 4의 초장문 컨텍스트 기능이 아직 연구 개발 단계에 가까우며, 안정적인 상용 애플리케이션에 적용하기에는 시기상조일 수 있음을 시사한다.</li>
</ul>
<h2>3.  Llama 4 모델군 “The Herd”: Scout, Maverick, Behemoth 비교 분석</h2>
<p>Meta는 다양한 사용 사례와 계산 자원 환경에 대응하기 위해 Llama 4를 ’The Herd(무리)’라는 이름 아래 세 가지 주요 모델로 구성하여 출시했다. 각 모델은 동일한 아키텍처 철학을 공유하면서도, 파라미터 규모, 전문가 수, 컨텍스트 길이 등에서 뚜렷한 차이를 보인다.</p>
<table><thead><tr><th>항목 (Category)</th><th>Llama 4 Scout</th><th>Llama 4 Maverick</th><th>Llama 4 Behemoth (Preview)</th></tr></thead><tbody>
<tr><td><strong>활성 파라미터 (Active Parameters)</strong></td><td>17B</td><td>17B</td><td>288B</td></tr>
<tr><td><strong>총 파라미터 (Total Parameters)</strong></td><td>109B</td><td>400B</td><td>~2T</td></tr>
<tr><td><strong>아키텍처 (Architecture)</strong></td><td>MoE (16 Experts)</td><td>MoE (128 Experts)</td><td>MoE (16 Experts)</td></tr>
<tr><td><strong>컨텍스트 길이 (Context Length)</strong></td><td>10,000,000 tokens</td><td>1,000,000 tokens</td><td>128,000 tokens (추정)</td></tr>
<tr><td><strong>핵심 특징 (Key Feature)</strong></td><td>초장문 컨텍스트 분석</td><td>고성능 멀티모달 추론</td><td>지식 증류용 교사 모델</td></tr>
<tr><td><strong>목표 하드웨어 (Target Hardware)</strong></td><td>Single H100 GPU (INT4)</td><td>Single H100 Host (FP8)</td><td>내부 인프라 (미공개)</td></tr>
<tr><td><strong>공개 여부 (Public Access)</strong></td><td>공개 (Available)</td><td>공개 (Available)</td><td>미공개 (Unreleased)</td></tr>
</tbody></table>
<h3>3.1  Llama 4 Scout: 장문 컨텍스트의 개척자</h3>
<p>Llama 4 Scout은 170억(17B) 개의 활성 파라미터와 1090억(109B) 개의 총 파라미터를 가지며, 16개의 전문가로 구성된 MoE 아키텍처를 기반으로 한다.1 Scout의 가장 큰 특징은 iRoPE 아키텍처를 통해 구현된 1,000만 토큰이라는 업계 최대 규모의 컨텍스트 창이다.2</p>
<p>이 모델은 효율성에 초점을 맞춰 설계되었다. INT4 정수 양자화를 적용할 경우, 단일 NVIDIA H100 GPU(80GB VRAM)에서 추론이 가능하도록 최적화되어, 상대적으로 접근성이 높다.1 그러나 미세조정(fine-tuning)을 위해서는 최소 4개의 H100 GPU가 필요하며, 컨텍스트 길이를 최대로 활용할 경우 VRAM 요구량은 100GB를 훌쩍 넘길 수 있어 여전히 상당한 하드웨어 자원을 요구한다.45</p>
<p>Scout의 주요 사용 사례는 방대한 양의 정보를 한 번에 처리해야 하는 작업에 집중된다. 예를 들어, 수백 페이지에 달하는 법률 문서나 연구 논문 전체를 요약하고 분석하거나, 기업의 전체 코드베이스를 컨텍스트로 제공하여 버그를 찾고 리팩토링을 제안하는 등의 작업에 특화되어 있다. 또한, 사용자와의 장기간 상호작용 기록을 모두 기억하여 고도로 개인화된 응답을 제공하는 AI 에이전트 개발에도 활용될 수 있다.6</p>
<h3>3.2  Llama 4 Maverick: 고성능 범용 모델</h3>
<p>Llama 4 Maverick은 Llama 4 제품군의 주력 모델로, Scout과 동일한 17B의 활성 파라미터를 가지지만, 전문가 수를 128개로 크게 늘려 총 4000억(400B) 개의 파라미터를 보유하고 있다.5 100만 토큰의 컨텍스트 창을 지원하며, 더 많은 전문가를 통해 더 넓고 깊은 지식과 미묘한 추론 능력을 제공하는 데 중점을 둔다.</p>
<p>Maverick은 고성능을 요구하는 만큼 더 많은 계산 자원을 필요로 한다. FP8 양자화 기준으로 단일 H100 DGX 호스트(8개의 H100 GPU로 구성) 환경에 최적화되어 있으며, 미세조정에는 최소 8개의 H100 GPU가 권장된다.1</p>
<p>이 모델은 OpenAI의 GPT-4o나 Google의 Gemini 2.0 Flash와 직접적으로 경쟁하는 범용 모델이다. 복잡한 이미지와 텍스트를 동시에 이해하고 추론하는 능력, 12개 언어를 지원하는 강력한 다국어 능력, 그리고 창의적인 글쓰기 능력 등 다방면에 걸쳐 뛰어난 성능을 보인다.1 따라서 대화형 AI 어시스턴트, 고객 지원 챗봇, 다국어 콘텐츠 생성 등 광범위한 상용 애플리케이션에 가장 적합한 모델로 포지셔닝된다.49</p>
<h3>3.3  Llama 4 Behemoth: 지식의 원천, 교사 모델</h3>
<p>Llama 4 Behemoth는 현재까지 공개되지 않은 연구용 모델로, 2880억(288B) 개의 활성 파라미터와 약 2조(2T) 개의 총 파라미터를 가진 것으로 추정되는 Meta의 가장 강력한 모델이다.6 Behemoth는 일반에 배포되기 위한 모델이 아니라, 더 작은 모델인 Scout와 Maverick을 ‘가르치는’ 교사 모델(teacher model)로서의 역할을 수행한다.1</p>
<p>Meta는 Behemoth를 사용하여 ’지식 증류(Knowledge Distillation)’라는 프로세스를 수행한다. 이는 강력한 교사 모델(Behemoth)이 생성한 풍부하고 정교한 출력(soft target)을 학생 모델(Scout, Maverick)이 모방하도록 학습시키는 기법이다.1 Meta는 이 과정에서 교사 모델의 출력과 실제 정답(hard target) 간의 균형을 동적으로 조절하는 새로운 손실 함수를 개발하여 증류 효율을 극대화했다고 밝혔다.1</p>
<p>Behemoth는 아직 훈련이 진행 중인 상태임에도 불구하고, MATH-500(수학 문제 해결), GPQA Diamond(전문가 수준 질의응답)와 같은 고난도 STEM 벤치마크에서 GPT-4.5, Claude Sonnet 3.7 등 현존하는 최상위 모델들을 능가하는 성능을 기록했다.1 이는 Behemoth가 Llama 4 제품군 전체의 지능 수준을 한 단계 끌어올리는 핵심적인 역할을 하고 있음을 보여준다.</p>
<h2>4.  성능 분석 및 경쟁 모델 벤치마킹</h2>
<h3>4.1  정량적 벤치마크 분석</h3>
<p>Llama 4 모델군의 성능은 다양한 표준 벤치마크를 통해 경쟁 모델들과 비교되었다. 특히 주력 모델인 Maverick과 교사 모델인 Behemoth는 여러 영역에서 인상적인 결과를 보여주었다. 아래 표는 주요 벤치마크 점수와 추론 비용을 종합적으로 비교한 것이다.</p>
<table><thead><tr><th>모델 (Model)</th><th>추론 비용 (Inference Cost)¹</th><th>MMMU</th><th>MathVista</th><th>MMLU Pro</th><th>GPQA Diamond</th><th>LiveCodeBench</th><th>ChartQA</th><th>DocVQA</th></tr></thead><tbody>
<tr><td><strong>Llama 4 Maverick</strong></td><td><strong>$0.19 - $0.49</strong></td><td><strong>73.4</strong></td><td><strong>73.7</strong></td><td><strong>80.5</strong></td><td><strong>69.8</strong></td><td><strong>43.4</strong></td><td><strong>90.0</strong></td><td><strong>94.4</strong></td></tr>
<tr><td><strong>Llama 4 Scout</strong></td><td>미제공 (N/A)</td><td>69.4</td><td>70.7</td><td>74.3</td><td>57.2</td><td>32.8</td><td>88.8</td><td>94.4</td></tr>
<tr><td>GPT-4o</td><td>$4.38</td><td>69.1</td><td>63.8</td><td>-</td><td>53.6</td><td>32.3</td><td>85.7</td><td>92.8</td></tr>
<tr><td>Gemini 2.0 Flash</td><td>$0.17</td><td>71.7</td><td>73.1</td><td>77.6</td><td>60.1</td><td>34.5</td><td>88.3</td><td>-</td></tr>
<tr><td>DeepSeek v3.1</td><td>$0.48</td><td>-</td><td>-</td><td>81.2</td><td>68.4</td><td>45.8</td><td>-</td><td>-</td></tr>
<tr><td><strong>Llama 4 Behemoth</strong></td><td>미공개 (N/A)</td><td><strong>76.1</strong></td><td>-</td><td><strong>82.2</strong></td><td><strong>73.7</strong></td><td><strong>49.4</strong></td><td>-</td><td>-</td></tr>
<tr><td>Claude Sonnet 3.7</td><td>미제공 (N/A)</td><td>71.8</td><td>-</td><td>-</td><td>68.0</td><td>-</td><td>-</td><td>-</td></tr>
<tr><td>Gemini 2.0 Pro</td><td>미제공 (N/A)</td><td>72.7</td><td>-</td><td>79.1</td><td>64.7</td><td>36.0</td><td>-</td><td>-</td></tr>
<tr><td>GPT-4.5</td><td>미제공 (N/A)</td><td>74.4</td><td>-</td><td>-</td><td>71.4</td><td>-</td><td>-</td><td>-</td></tr>
</tbody></table>
<p>¹ Cost per 1M input &amp; output tokens (3:1 blended). Llama 4, Gemini, DeepSeek, GPT-4o 비용은 llama.com 출처.26</p>
<p>분석 결과, Llama 4 Maverick은 멀티모달 추론(MMMU, MathVista)과 이미지 이해(ChartQA, DocVQA) 벤치마크에서 직접적인 경쟁 모델인 GPT-4o와 Gemini 2.0 Flash를 모두 능가하는 성능을 보였다.1 특히 주목할 점은 비용 효율성이다. Maverick의 추론 비용은 GPT-4o의 약 1/10 수준에 불과하여, 압도적인 가격 대비 성능 우위를 점하고 있다.50 이는 MoE 아키텍처의 효율성이 실제 비용 절감으로 이어졌음을 명확히 보여준다. 비공개 모델인 Behemoth는 모든 공개 모델을 뛰어넘는 최고 수준의 성능을 기록하며, Meta의 프론티어 모델 개발 역량을 입증했다.1</p>
<h3>4.2  실사용 성능 및 정성적 평가</h3>
<p>표준 벤치마크에서의 인상적인 성과에도 불구하고, Llama 4는 실제 사용 환경에서 몇 가지 중요한 한계와 논란에 직면했다. 이는 현대 LLM 평가 방식의 근본적인 문제, 즉 ’벤치마크 최적화’와 ‘실사용 일반화’ 사이의 괴리를 드러내는 사례가 되었다.</p>
<ul>
<li><strong>초기 개발자 피드백</strong>: 2025년 4월 출시 직후, 많은 개발자들이 코딩, 논리적 추론, 복잡한 지시 따르기와 같은 실제 작업에서 Llama 4의 성능이 기대에 미치지 못한다고 지적했다.52 이러한 비판에 대응하여 Meta는 내부적으로 버그를 수정하고 성능을 개선한 Llama 4.X (4.1, 4.2) 버전을 연말까지 출시할 계획이라고 밝혔다.52</li>
<li><strong>장문 컨텍스트 실효성 논란</strong>: Llama 4 Scout의 1,000만 토큰 컨텍스트 창은 발표 당시 큰 주목을 받았으나, 실제 성능은 기대에 미치지 못했다. ’건초더미에서 바늘 찾기(Needle in a Haystack)’와 같은 정보 검색 테스트에서 컨텍스트 길이가 길어질수록 정확도가 급격히 떨어지는 현상이 관찰되었다.40 특히, 복잡한 서사 구조와 시점 변화를 이해해야 하는 Fiction.live의 테스트에서는 Llama 3.3 모델보다도 낮은 점수를 기록하며, 단순 정보 검색을 넘어선 깊이 있는 장문 이해 능력에는 한계가 있음을 보여주었다.40</li>
<li><strong>벤치마크와 현실의 괴리</strong>: 이러한 성능 논란의 배경에는 Meta의 공격적인 벤치마크 최적화 전략이 자리 잡고 있다. LMArena 리더보드에서 Maverick이 높은 순위를 기록한 것은, 일반에 공개된 버전이 아닌 인간 평가자의 선호도에 맞게 특별히 미세 조정된 ’실험용 채팅 버전’을 사용했기 때문이라는 사실이 밝혀지면서 큰 논란이 되었다.55 이 실험용 버전은 일반 버전보다 더 길고, 이모티콘을 사용하는 등 감성적인 응답을 생성하는 경향이 있었다.18 또한, 일부 벤치마크(MMLU Pro, GPQA Diamond)에서는 모델이 특정 답변 형식(“The best answer is A”)에 과적합되어, 평가 방법론을 수정하자 점수가 크게 상승하는 현상이 나타나기도 했다.40</li>
</ul>
<p>이러한 일련의 사건들은 표준화된 벤치마크 점수가 모델의 실제 강건함(robustness)이나 일반화 능력을 완벽하게 대변하지 못한다는 사실을 명백히 보여준다. Meta의 전략은 단기적으로 경쟁 우위를 확보하는 데는 성공했을지 모르나, 장기적으로는 개발자 커뮤니티의 신뢰를 훼손할 수 있는 위험을 내포하고 있다.</p>
<h2>5.  Llama 4 생태계: 배포, 라이선스, 그리고 논란</h2>
<h3>5.1  배포 및 접근성</h3>
<p>Llama 4는 오픈-웨이트 모델로서 다양한 경로를 통해 개발자들에게 제공되며, 이는 Meta의 생태계 확장 전략의 핵심을 이룬다.</p>
<ul>
<li><strong>API 제공업체</strong>: 개발자들은 Hugging Face, Cloudflare Workers AI, Snowflake, Databricks 등 여러 클라우드 및 AI 플랫폼 파트너를 통해 Llama 4 모델에 접근할 수 있다.5 특히 Together.ai와 Groq 같은 전문 API 제공업체들은 경쟁력 있는 가격과 빠른 추론 속도를 제공하며 Llama 4의 채택을 가속화하고 있다.57</li>
</ul>
<table><thead><tr><th>모델 (Model)</th><th>제공업체 (Provider)</th><th>입력 (Input)</th><th>출력 (Output)</th><th>속도 (Tokens/Sec)</th></tr></thead><tbody>
<tr><td><strong>Llama 4 Maverick</strong></td><td>Together.ai</td><td>$0.27</td><td>$0.85</td><td>-</td></tr>
<tr><td></td><td>Groq</td><td>$0.20</td><td>$0.60</td><td>562</td></tr>
<tr><td><strong>Llama 4 Scout</strong></td><td>Together.ai</td><td>$0.18</td><td>$0.59</td><td>-</td></tr>
<tr><td></td><td>Groq</td><td>$0.11</td><td>$0.34</td><td>594</td></tr>
</tbody></table>
<ul>
<li><strong>로컬 배포</strong>: 로컬 환경에서의 모델 실행을 위해 여러 도구들이 지원된다.</li>
<li><strong>Ollama</strong>: 간단한 명령어 하나로 양자화된 모델을 다운로드하고 실행할 수 있는 사용자 친화적인 도구다.59</li>
<li><strong>Llama.cpp</strong>: C++로 작성된 고성능 추론 프레임워크로, CPU 환경에서도 최적화된 성능을 제공하며, GGUF와 같은 다양한 양자화 포맷을 지원하여 저사양 하드웨어에서도 모델을 구동할 수 있게 해준다.60</li>
<li><strong>미세조정(Fine-Tuning)</strong>: Llama 4는 특정 도메인이나 작업에 맞게 성능을 최적화하는 미세조정을 지원한다. 그러나 MoE 아키텍처의 막대한 메모리 요구량 때문에 전체 파라미터를 미세조정하는 것은 매우 어렵다. 이를 해결하기 위해 QLoRA와 같은 PEFT(Parameter-Efficient Fine-Tuning) 기법이 널리 사용된다.62 PEFT는 모델의 원본 가중치는 동결시킨 채, 소수의 추가 파라미터(어댑터)만을 학습시켜 메모리 사용량을 크게 줄인다. Unsloth와 같은 최적화 프레임워크는 이러한 과정을 더욱 효율적으로 만들어, 제한된 GPU 자원(예: 단일 H100)으로도 Llama 4 Scout와 같은 대규모 모델을 미세조정할 수 있도록 지원한다.45</li>
</ul>
<h3>5.2  “오픈-웨이트” 논쟁: Llama 4 라이선스 분석</h3>
<p>Meta는 Llama 모델군을 ’오픈 소스’라고 홍보하지만, Llama 4에 적용된 ’Llama 4 커뮤니티 라이선스’는 오픈소스 이니셔티브(OSI)가 정의하는 엄격한 오픈 소스 라이선스의 기준을 충족하지 못한다.65 이는 ‘소스 사용 가능(Source-available)’ 또는 ‘오픈-웨이트(Open-weight)’ 라이선스로 분류하는 것이 더 정확하다.</p>
<ul>
<li><strong>주요 제한 조항</strong>:</li>
<li><strong>7억 MAU(월간 활성 사용자) 조항</strong>: 이 라이선스의 가장 핵심적인 제한 사항으로, Llama 4를 활용한 제품이나 서비스의 월간 활성 사용자가 7억 명을 초과할 경우, Meta로부터 별도의 상업적 라이선스를 취득해야 한다.44 이는 Google, Apple, Microsoft와 같은 거대 기술 기업들이 Llama 4를 자사 핵심 서비스에 통합하여 Meta와 직접적으로 경쟁하는 것을 막기 위한 전략적 조항으로 해석된다.68</li>
<li><strong>허용 가능한 사용 정책(AUP)</strong>: 라이선스는 별도의 AUP 문서를 참조하며, 이 정책은 불법적이거나 유해한 콘텐츠 생성, 군사적 목적의 사용, 규제된 전문 분야(의료, 법률 등)에서의 무허가 활동 등을 금지한다.69 이 조항은 모델의 책임 있는 사용을 장려하기 위한 것이지만, 특정 ’사용 분야’를 제한한다는 점에서 OSI의 오픈 소스 정의(OSD)와 충돌한다.68</li>
<li><strong>EU 지역 제한</strong>: 특히 Llama 4의 멀티모달 모델에 대해서는, 유럽 연합(EU)에 거주하는 개인이나 EU에 본사를 둔 기업의 직접적인 사용을 제한하는 조항이 포함되어 있다.24 이는 EU의 AI Act와 같은 강력한 규제 환경에서 발생할 수 있는 법적 리스크를 사전에 차단하려는 Meta의 방어적 조치로 보인다.71</li>
<li><strong>OSI와의 비교</strong>: OSI는 Llama 4 라이선스가 ‘개인이나 그룹에 대한 차별 금지’(OSD 5)와 ‘사용 분야에 대한 차별 금지’(OSD 6) 원칙을 명백히 위반한다고 지적한다.66 7억 MAU 조항은 대규모 사용자를 차별하고, AUP는 특정 활동 분야를 제한하기 때문이다. 따라서 Llama 4는 진정한 의미의 ’오픈 소스’가 아니며, Meta의 이러한 용어 사용은 ’오픈 워싱(open washing)’에 해당한다는 비판이 제기된다.</li>
</ul>
<h3>5.3  벤치마크 무결성 논란: LMArena 사건</h3>
<p>Llama 4 출시 초기, 모델의 성능을 둘러싼 가장 큰 논란은 LMArena 벤치마크를 중심으로 발생했다.</p>
<ul>
<li><strong>사건의 발단</strong>: Llama 4 Maverick은 출시 직후, 인간의 선호도를 기반으로 모델 순위를 매기는 LMArena 리더보드에서 최상위권에 올랐다. Meta는 이를 주요 홍보 포인트로 활용했다. 그러나 곧 이 순위가 일반에 공개된 버전이 아닌, Meta 내부의 ’실험용 채팅 버전’으로 달성되었다는 사실이 밝혀지면서 논란이 불거졌다.55</li>
<li><strong>차이점 분석</strong>: 커뮤니티의 분석 결과, 실험용 버전은 일반 버전보다 더 길고, 정중하며, 이모티콘을 자주 사용하는 등 인간 평가자에게 긍정적인 인상을 주도록 미세 조정된 경향이 뚜렷하게 나타났다.18 이는 벤치마크 점수가 모델의 순수한 능력이 아닌, 특정 평가 환경에 대한 최적화의 결과일 수 있다는 의혹을 낳았다.</li>
<li><strong>영향 및 후속 조치</strong>: 이 사건은 AI 모델 평가의 투명성과 신뢰성에 대한 중요한 질문을 제기했다. Meta는 다양한 버전을 테스트하는 것은 일반적인 개발 과정의 일부라고 해명했지만 72, LMArena는 향후 제출되는 모델이 일반 대중이 접근 가능한 버전과 동일해야 한다는 점을 명확히 하는 등 정책을 강화했다.55 이 사건은 업계 전반에 걸쳐 벤치마크 결과의 맹신을 경계하고, 보다 다각적이고 현실적인 평가 방법론의 필요성을 환기시키는 계기가 되었다.</li>
</ul>
<h2>6.  전략적 함의와 미래 전망</h2>
<h3>6.1  Llama 4와 Meta의 초지능 로드맵</h3>
<p>Llama 4는 단기적인 제품 출시를 넘어, 인류 수준을 뛰어넘는 ’초지능(Superintelligence)’을 구축하려는 Meta의 장기적인 비전과 깊숙이 연결되어 있다.</p>
<ul>
<li><strong>Meta Superintelligence Labs (MSL)의 출범</strong>: Llama 4의 초기 성능에 대한 비판적 여론에 직면한 Meta는, AI 개발의 방향을 재설정하고 프론티어 연구에 집중하기 위해 2024년 6월 ’Meta Superintelligence Labs(MSL)’를 설립했다.52 Scale AI의 창업자 Alexandr Wang을 영입하여 연구를 총괄하게 하는 등, 외부의 최고 인재들을 공격적으로 영입하며 조직을 강화했다.73</li>
<li><strong>Mark Zuckerberg의 비전</strong>: Zuckerberg는 거대 조직보다는 소수의 핵심 인재로 구성된 밀집형 팀이 AI 분야의 돌파구를 만드는 데 더 효과적이라고 믿는다.75 그는 MSL을 통해 장기적으로 모든 사람을 위한 개인화된 초지능 비서를 개발하는 것을 최종 목표로 삼고 있으며, 이를 위해 연간 수십억 달러에 달하는 막대한 자본을 AI 인프라(특히 NVIDIA GPU 확보)에 투자하고 있다.76</li>
<li><strong>Yann LeCun의 비전</strong>: Meta의 수석 AI 과학자 Yann LeCun은 현재의 LLM이 가진 근본적인 한계(물리적 세계에 대한 이해 부족, 추론 능력의 부재 등)를 지적하며, 새로운 패러다임의 필요성을 역설한다.78 그가 제시하는 대안은 ‘월드 모델(World Models)’ 기반의 고급 기계 지능(Advanced Machine Intelligence, AMI)으로, 이는 AI가 관찰을 통해 세상이 작동하는 방식에 대한 내부 모델을 스스로 학습하고, 이를 바탕으로 예측하고 계획하는 능력을 갖추는 것을 목표로 한다.29 Meta가 개발 중인 V-JEPA와 같은 자기지도학습(self-supervised learning) 아키텍처가 이 비전의 핵심이며, Llama 4의 네이티브 멀티모달리티는 텍스트를 넘어 시각적 세계를 이해하려는 월드 모델로 나아가는 중요한 첫걸음으로 해석될 수 있다.</li>
<li><strong>내부 과제</strong>: 그러나 Meta의 야심찬 계획은 순탄하지만은 않다. MSL은 출범 직후부터 높은 연봉에도 불구하고 핵심 연구 인력들이 경쟁사인 OpenAI 등으로 이탈하는 등 내부적인 불안정성을 겪고 있다.52 잦은 조직 개편과 Zuckerberg의 직접적인 관여가 오히려 연구의 자율성을 해친다는 비판도 제기되며, 장기적인 비전 달성 여부에는 여전히 불확실성이 존재한다.80</li>
</ul>
<h3>6.2  AI 시장에 미치는 영향 및 결론</h3>
<p>Llama 4의 등장은 AI 시장의 경쟁 구도와 생태계 전반에 깊은 영향을 미치고 있다.</p>
<ul>
<li><strong>경쟁 구도 변화</strong>: Llama 4, 특히 Maverick 모델은 GPT-4o와 같은 최상위 폐쇄형 모델과 대등한 성능을 보이면서도 훨씬 저렴한 추론 비용을 제공한다.50 이는 OpenAI와 Google이 주도해 온 고가의 API 시장에 강력한 가격 압박을 가하며, 기업과 개발자들에게 더 많은 선택권과 협상력을 부여한다. 이러한 가격 경쟁의 심화는 AI 기술의 도입 비용을 낮춰 전반적인 시장 성장을 촉진하는 긍정적인 효과를 낳을 수 있다.81</li>
<li><strong>오픈-웨이트 생태계의 성장</strong>: Llama 4는 개발자들이 모델 가중치에 직접 접근하여 특정 도메인에 맞게 미세조정하고, 완전히 새로운 애플리케이션을 창조할 수 있는 자유를 제공한다.83 이는 폐쇄형 API가 제공하는 제한된 기능만으로는 만족할 수 없었던 수많은 개발자와 스타트업에게 혁신의 기회를 열어주며, Llama를 중심으로 한 강력한 오픈-웨이트 생태계의 성장을 가속화하고 있다.84</li>
</ul>
<p>결론적으로, Llama 4는 단순한 AI 모델을 넘어선 하나의 거대한 플랫폼 전략의 핵심이다. Meta는 Llama 4를 통해 직접적인 수익을 창출하기보다는, AI 개발의 표준을 자사의 기술로 정립하려는 더 큰 그림을 그리고 있다. 이는 과거 Google이 Android 운영체제를 무료로 배포하여 모바일 생태계를 장악한 전략과 유사하다. 핵심 모델(가중치)을 사실상 무료에 가까운 라이선스로 제공하여 광범위한 시장 채택을 유도하고, 그 위에 Llama Stack, Llama API, 클라우드 파트너십과 같은 개발 도구와 인프라 생태계를 구축한다.84 동시에 7억 MAU 조항과 같은 안전장치를 통해 자사의 핵심 비즈니스를 보호한다. 이 전략이 성공할 경우, Meta는 단기적인 API 수익 경쟁을 넘어, 미래 AI 시대의 기술 패권을 장악하는 ’게임 체인저’가 될 잠재력을 가지고 있다. Llama 4는 초기 성능에 대한 비판과 라이선스를 둘러싼 논란에도 불구하고, AI 기술의 민주화와 산업 전반의 혁신을 이끄는 가장 중요한 동력 중 하나로 기록될 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation, https://ai.meta.com/blog/llama-4-multimodal-intelligence/</li>
<li>Llama (language model) - Wikipedia, https://en.wikipedia.org/wiki/Llama_(language_model)</li>
<li>Meta AI: What is Llama 4 and why does it matter? - Zapier, https://zapier.com/blog/llama-meta/</li>
<li>Llama 4 by Meta: Everything You Need to Know – IT Exams Training - Pass4sure, https://www.pass4sure.com/blog/llama-4-by-meta-everything-you-need-to-know/</li>
<li>Meta’s Llama 4 is now available on Workers AI - The Cloudflare Blog, https://blog.cloudflare.com/meta-llama-4-is-now-available-on-workers-ai/</li>
<li>Llama 4 by Meta AI – Everything You Need to Know - Deploy.AI, https://www.deploy.ai/blog-post/llama-4-by-meta-ai-everything-you-need-to-know</li>
<li>MoE vs Dense vs Hybrid LLM architectures | hybridMoe – Weights &amp; Biases - Wandb, https://wandb.ai/zaiinn440/hybridMoe/reports/MoE-vs-Dense-vs-Hybrid-LLM-architectures–Vmlldzo3NzYwNzAw</li>
<li>Mixture of Experts in Large Language Models †: Corresponding author - arXiv, https://arxiv.org/html/2507.11181v1</li>
<li>What is mixture of experts? | IBM, https://www.ibm.com/think/topics/mixture-of-experts</li>
<li>What Is Mixture of Experts (MoE)? How It Works, Use Cases &amp; More | DataCamp, https://www.datacamp.com/blog/mixture-of-experts-moe</li>
<li>A Survey on Mixture of Experts - arXiv, https://arxiv.org/html/2407.06204v2</li>
<li>Llama 4 is here : r/LocalLLaMA - Reddit, https://www.reddit.com/r/LocalLLaMA/comments/1jsahy4/llama_4_is_here/</li>
<li>Llama 4 Technical Analysis: Decoding the Architecture Behind Meta’s Multimodal MoE Revolution | by Karan_bhutani | Medium, https://medium.com/@karanbhutani477/llama-4-technical-analysis-decoding-the-architecture-behind-metas-multimodal-moe-revolution-535b2775d07d</li>
<li>Applying Mixture of Experts in LLM Architectures | NVIDIA Technical Blog, https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/</li>
<li>Mixture of Experts (MoE) vs Dense LLMs, https://maximilian-schwarzmueller.com/articles/understanding-mixture-of-experts-moe-llms/</li>
<li>Llama 4 Scout and Maverick Are Here—How Do They Shape Up? | Runpod Blog, https://www.runpod.io/blog/llama4-scout-maverick</li>
<li>Running Llama 4 Models on Vast.ai, https://vast.ai/article/running-llama-4-models-on-vast</li>
<li>LM Arena confirm that the version of Llama-4 Maverick listed on the arena is a “customized model to optimize for human preference” : r/LocalLLaMA - Reddit, https://www.reddit.com/r/LocalLLaMA/comments/1ju0nd6/lm_arena_confirm_that_the_version_of_llama4/</li>
<li>Mixture of experts - Wikipedia, https://en.wikipedia.org/wiki/Mixture_of_experts</li>
<li>Improving Deep Learning Performance with Mixture of Experts and Sparse Activation, https://www.preprints.org/manuscript/202503.0611/v1</li>
<li>Deep Technical Analysis of Llama 4 Scout, Maverick and Behemoth - Collabnix, https://collabnix.com/deep-technical-analysis-of-llama-4-scout-maverick-and-behemoth/</li>
<li>(PDF) On Comparing Early and Late Fusion Methods - ResearchGate, https://www.researchgate.net/publication/374301002_On_Comparing_Early_and_Late_Fusion_Methods</li>
<li>Early Fusion vs. Late Fusion in Multimodal Data Processing - GeeksforGeeks, https://www.geeksforgeeks.org/deep-learning/early-fusion-vs-late-fusion-in-multimodal-data-processing/</li>
<li>LLaMA-4 Explained: Everything You Need to Know About Meta’s New AI Family | by Ashish Chadha | Medium, https://medium.com/@ashishchadha11944/metas-llama-4-ai-models-technical-specifications-benchmarks-and-strategic-implications-ddabfa3d0b52</li>
<li>Llama 4’s Architecture Deconstructed: MoE, iRoPE, and Early Fusion Explained - Medium, https://medium.com/@mandeep0405/llama-4s-architecture-deconstructed-moe-irope-and-early-fusion-explained-e58eb9403067</li>
<li>Unmatched Performance and Efficiency | Llama 4, https://www.llama.com/models/llama-4/</li>
<li>Llama: Industry Leading, Open-Source AI, https://www.llama.com/</li>
<li>Meta releases Llama 4 AI models to rival DeepSeek and OpenAI - The American Bazaar, https://americanbazaaronline.com/2025/04/08/meta-releases-llama-4-ai-models-to-rival-deepseek-and-openai/</li>
<li>Meta’s Le Cun Outlines Path to Artificial Superintelligence - EE Times Europe, https://www.eetimes.eu/meta-le-cun-outlines-path-to-artificial-super-intelligence/</li>
<li>Meta chief AI scientist claims AGI will be viable in 3-5 years - National Technology News, https://nationaltechnology.co.uk/Meta_Chief_AI_Scientist_Claims_AGI_Will_Be_Viable_In_3_5_Years.php</li>
<li>I-JEPA: The first AI model based on Yann LeCun’s vision for more human-like AI, https://ai.meta.com/blog/yann-lecun-ai-model-i-jepa/</li>
<li>Inside Llama 4: How Meta’s New Open-Source AI Crushes GPT-4o and Gemini - Devansh, https://machine-learning-made-simple.medium.com/inside-llama-4-how-metas-new-open-source-ai-crushes-gpt-4o-and-gemini-e3265f914599</li>
<li>Analysis of Llama 4’s 10 Million Token Context Window Claim | by Sander Ali Khowaja, https://sandar-ali.medium.com/analysis-of-llama-4s-10-million-token-context-window-claim-9e68ee5abcde</li>
<li>[2104.09864] RoFormer: Enhanced Transformer with Rotary Position Embedding - arXiv, https://arxiv.org/abs/2104.09864</li>
<li>RoFormer: Enhanced Transformer with Rotary Position Embedding, https://arxiv.org/pdf/2104.09864</li>
<li>Llama 4’s Approach to Positional Information | by Deeraj Manjaray - Medium, https://deerajmanjaray.medium.com/llama-4s-approach-to-positional-information-0eb736179e5f</li>
<li>SWAN-GPT: An Efficient and Scalable Approach for Long-Context Language Modeling, https://arxiv.org/html/2504.08719v1</li>
<li>Mark presenting four Llama 4 models, even a 2 trillion parameters model!!! - Reddit, https://www.reddit.com/r/LocalLLaMA/comments/1jsampe/mark_presenting_four_llama_4_models_even_a_2/</li>
<li>Lost in the Middle: How Language Models Use Long Contexts - Stanford Computer Science, https://cs.stanford.edu/~nfliu/papers/lost-in-the-middle.arxiv2023.pdf</li>
<li>Meta’s Llama 4 models show promise on standard tests, but struggle with long-context tasks, https://the-decoder.com/metas-llama-4-models-show-promise-on-standard-tests-but-struggle-with-long-context-tasks/</li>
<li>NoLiMa: Long-Context Evaluation Beyond Literal Matching - arXiv, https://arxiv.org/pdf/2502.05167</li>
<li>Meta is racing the clock to launch its newest Llama AI model this year : r/LocalLLaMA, https://www.reddit.com/r/LocalLLaMA/comments/1n2xc58/meta_is_racing_the_clock_to_launch_its_newest/</li>
<li>A timeline of LLM Context Windows, Over the past 5 years. (done right this time) - Reddit, https://www.reddit.com/r/LocalLLaMA/comments/1mymyfu/a_timeline_of_llm_context_windows_over_the_past_5/</li>
<li>Meta’s Llama 4: Features, Access, How It Works, and More - DataCamp, https://www.datacamp.com/blog/llama-4</li>
<li>Fine-Tuning Llama 4: A Guide With Demo Project - DataCamp, https://www.datacamp.com/tutorial/fine-tuning-llama-4</li>
<li>Realistic VRAM Requirements for Llama 4 Scout - Veltech IT Support Christchurch, https://veltech.nz/articles/llama-4-scout-vram-requirements</li>
<li>Introducing the Llama 4 herd in Azure AI Foundry and Azure Databricks | Microsoft Azure Blog, https://azure.microsoft.com/en-us/blog/introducing-the-llama-4-herd-in-azure-ai-foundry-and-azure-databricks/</li>
<li>Meta Llama - Hugging Face, https://huggingface.co/meta-llama</li>
<li>Llama 4 vs. GPT-4o: A Detailed Comparison - Ponder AI, https://ponder.ing/researches/llama-4-vs-gpt-4o-a-detailed-comparison</li>
<li>Llama 4 and Llama 3 Models - Together AI, https://www.together.ai/llama</li>
<li>Llama 4: Models, Architecture, Benchmarks &amp; More | by Jatin Garg | Medium, https://medium.com/@jatingargiitk/llama-4-models-architecture-benchmarks-more-4f297d6dc0fb</li>
<li>Meta is racing the clock to launch its newest Llama AI model this year, https://ca.news.yahoo.com/meta-racing-clock-launch-newest-182009600.html</li>
<li>Why Llama 4 is a Disaster - Codersera, https://codersera.com/blog/why-llama-4-is-a-disaster</li>
<li>Meta Accelerates Development of Llama 4.X AI Model - GuruFocus, https://www.gurufocus.com/news/3086576/meta-accelerates-development-of-llama-4x-ai-model</li>
<li>LMArena tightens rules after Llama 4 incident | Digital Watch Observatory, https://dig.watch/updates/lmarena-tightens-rules-after-llama-4-incident</li>
<li>Meta faces backlash over Llama 4 release | Digital Watch Observatory, https://dig.watch/updates/meta-faces-backlash-over-llama-4-release</li>
<li>Pricing - Together AI, https://www.together.ai/pricing</li>
<li>Pricing | Groq is fast inference for AI builders, https://groq.com/pricing</li>
<li>LLaMa 4: Running Locally in Under an Hour | by Anthony Maio | Digital Children | Medium, https://medium.com/digital-children/llama-4-running-locally-in-under-an-hour-aa0202a2dfb5</li>
<li>Running LLaMA Locally with Llama.cpp: A Complete Guide | by Mostafa Farrag - Medium, https://medium.com/hydroinformatics/running-llama-locally-with-llama-cpp-a-complete-guide-adb5f7a2e2ec</li>
<li>Running LLaMA 4 on Windows: Step by Step Installation Guide - Codersera, https://codersera.com/blog/running-llama-4-on-windows-step-by-step-installation-guide</li>
<li>The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities (Version 1.0) - arXiv, https://arxiv.org/html/2408.13296v1</li>
<li>fine-tune-code-llama.ipynb - Colab, https://colab.research.google.com/github/seanses/LLM_fine_tuning/blob/main/fine-tune-code-llama.ipynb</li>
<li>unslothai/unsloth: Fine-tuning &amp; Reinforcement Learning for LLMs. Train OpenAI gpt-oss, Qwen3, Llama 4, DeepSeek-R1, Gemma 3, TTS 2x faster with 70% less VRAM. - GitHub, https://github.com/unslothai/unsloth</li>
<li>Llama 4 is NOT Open Source! - YouTube, https://www.youtube.com/watch?v=0O8Vp1yHn6E</li>
<li>Meta’s LLaMa license is still not Open Source, https://opensource.org/blog/metas-llama-license-is-still-not-open-source</li>
<li>Llama 4 Community License Agreement, https://www.llama.com/llama4/license/</li>
<li>Why Is the Llama License Not Open Source?, https://shujisado.org/2025/01/27/why-is-the-llama-license-not-open-source/</li>
<li>Llama 4 Acceptable Use Policy, https://www.llama.com/llama4/use-policy/</li>
<li>Llama FAQs, https://www.llama.com/faq/</li>
<li>Breaking the Llama Community License | Hacker News, https://news.ycombinator.com/item?id=43676254</li>
<li>Meta’s New Llama 4 Models Stir Controversy - BankInfoSecurity, https://www.bankinfosecurity.com/metas-new-llama-4-models-stir-controversy-a-27949</li>
<li>Meta’s Strategic AI Integration and Competitive Positioning in the AI Ecosystem - AInvest, https://www.ainvest.com/news/meta-strategic-ai-integration-competitive-positioning-ai-ecosystem-2508/</li>
<li>Meta’s billion-dollar hiring spree: DeepMind boss says money can’t buy the frontier of AI. Sam Altman reacts, https://economictimes.indiatimes.com/news/new-updates/metas-billion-dollar-hiring-spree-deepmind-boss-says-money-cant-buy-the-frontier-of-ai-sam-altman-reacts/articleshow/123472940.cms</li>
<li>‘It’s a bit of a different setup, as …’, says Mark Zuckerberg on Meta AI’s superintelligence group ‘vs’ the company’s 70,000 employees, https://timesofindia.indiatimes.com/technology/tech-news/its-a-bit-of-a-different-setup-as-says-mark-zuckerberg-on-meta-ais-superintelligence-group-vs-the-companys-70000-employees/articleshow/123480468.cms</li>
<li>Meta’s Llama 4 AI Model: A Game-Changer in the AI Landscape, https://www.kavout.com/market-lens/metas-llama-4-ai-model-a-game-changer-in-the-ai-landscape</li>
<li>Meta’s AI Shows Self-Learning Breakthrough, Zuckerberg Calls It a Step Toward Superintelligence - AMW Group, https://www.amworldgroup.com/blog/meta-ai-takes-first-step-to-superintelligence</li>
<li>Meta’s Chief AI Scientist Yann LeCun Questions the Longevity of Current GenAI and LLMs, https://www.hpcwire.com/2025/02/11/metas-chief-ai-scientist-yann-lecun-questions-the-longevity-of-current-genai-and-llms/</li>
<li>AI researchers lured with high salaries are leaving Meta, quoting Mark Zuckerberg’s own advice on their way out, https://economictimes.indiatimes.com/magazines/panache/ai-researchers-lured-with-high-salaries-are-leaving-metas-superintelligence-lab-quoting-mark-zuckerbergs-own-words-on-their-way-out/articleshow/123562342.cms</li>
<li>Meta’s Superintelligence Lab hits turbulence: Why top AI researchers are walking away just months after launch, https://timesofindia.indiatimes.com/education/news/metas-superintelligence-lab-hits-turbulence-why-top-ai-researchers-are-walking-away-just-months-after-launch/articleshow/123535925.cms</li>
<li>DeepSeek implications: Generative AI value chain winners &amp; losers - IoT Analytics, https://iot-analytics.com/winners-losers-generative-ai-value-chain/</li>
<li>“Open-weights” AI models offer transparency and control. - Oracle, https://www.oracle.com/artificial-intelligence/ai-open-weights-models/</li>
<li>LLaMA 4 vs Gemini 2.5: Comparing AI Titans in 2025 - Redblink, https://redblink.com/llama-4-vs-gemini-2-5/</li>
<li>The future of AI: Built with Llama - AI at Meta, https://ai.meta.com/blog/future-of-ai-built-with-llama/</li>
<li>On the Societal Impact of Open Foundation Models - Stanford CRFM, https://crfm.stanford.edu/open-fms/</li>
<li>Build the Future | Llama API, https://www.llama.com/products/llama-api/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>