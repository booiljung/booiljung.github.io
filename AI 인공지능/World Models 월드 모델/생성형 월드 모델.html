<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:생성형 월드 모델</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>생성형 월드 모델</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">월드 모델 (World Models)</a> / <span>생성형 월드 모델</span></nav>
                </div>
            </header>
            <article>
                <h1>생성형 월드 모델</h1>
<h2>1. 요약:</h2>
<p>생성형 월드 모델(Generative World Models)은 단순히 통계적 패턴을 모방하는 것을 넘어, 세계의 인과적, 상호작용적, 예측적 시뮬레이션을 학습하는 인공지능(AI)의 근본적인 패러다임 전환을 의미한다. 모델 기반 강화학습(Model-Based Reinforcement Learning)의 초기 연구에서 시작된 이 흐름은 OpenAI의 Sora나 Google의 Genie와 같은 대규모 월드 시뮬레이터의 등장으로 정점에 달했다. 이러한 모델들은 명백한 한계에도 불구하고, 얀 르쿤(Yann LeCun)과 같은 저명한 연구자들을 포함한 많은 이들에게 인공일반지능(Artificial General Intelligence, AGI)으로 가는 길에 있어 비판적이고 필수적인 구성 요소로 여겨지고 있다. 본 안내서는 월드 모델의 핵심 아키텍처 이정표, 혁신적인 응용 분야, 그리고 이 분야의 최전선을 규정하는 기술적 및 윤리적 과제들을 포함하여 생성형 월드 모델 분야 전반을 종합적으로 고찰한다.</p>
<h2>2.  월드 모델의 개념적 및 역사적 기반</h2>
<p>본 섹션에서는 월드 모델의 지적, 역사적 맥락을 확립하고, 초기 AI 연구에서부터 이 개념을 주류로 이끈 2018년의 중추적인 논문에 이르기까지 그 기원을 추적한다.</p>
<h3>2.1  패러다임의 정의: 생성형 월드 모델이란 무엇인가?</h3>
<p>생성형 월드 모델은 환경의 근본적인 패턴, 분포, 동역학(dynamics)을 학습하여 해당 환경에 대한 예측적이고 상호작용적인 시뮬레이션을 생성하는 생성형 AI(Generative AI)의 한 유형이다.1 이는 데이터를 분류하는 판별 모델(discriminative model)이나 단순히 훈련 데이터와 유사한 새로운 데이터를 생성하는 표준 생성 모델과는 근본적으로 구별된다.4 월드 모델의 핵심 목표는 시간의 흐름에 따른 환경의 ’진화’를 이해하고 예측하는 것이며, 이는 종종 에이전트의 행동에 대한 반응으로 나타난다.6 즉, 상태, 행동, 그리고 미래 상태의 결합 확률 분포(<span class="math math-inline">P(\text{state}_{t+1}∣\text{state}_t,\text{action}_t)</span>)를 학습하는 것을 목표로 한다.5</p>
<p>이 아이디어의 핵심은 인간이 예측하고, 계획하고, 행동하기 위해 세상에 대한 ’정신 모델(mental model)’을 형성하는 인지 과정에서 영감을 받았다.9 제이 라이트 포레스터(Jay Wright Forrester)가 설명했듯이, 인간은 세상 전체를 머릿속에 담는 것이 아니라 선택된 개념과 그 관계를 사용하여 실제 시스템을 표현한다.10 이러한 내적 모델은 우리가 매번 실제 세계와 상호작용할 필요 없이 빠르고 반사적인 행동을 수행하고 미래를 계획할 수 있게 해준다. 예를 들어, 야구 타자가 시속 100마일의 강속구를 쳐내는 것은 시각 신호가 뇌에 도달하는 시간보다 짧은 순간에 공이 어디로 올지 본능적으로 예측하는 내적 모델 덕분이다.9</p>
<h3>2.2  초기 개념과 선구자들 (1990년대)</h3>
<p>현대 월드 모델의 개념적 뿌리는 1990년대 위르겐 슈미트후버(Jürgen Schmidhuber)의 연구로 거슬러 올라간다.12 이 선구적인 연구는 현재 월드 모델의 기반이 되는 여러 핵심 개념을 제시했다.</p>
<ul>
<li><strong>순환 신경망(RNN)을 이용한 계획:</strong> 순환 신경망(RNN)을 학습 가능한 ’월드 모델’로 사용하여 에이전트의 행동에 기반한 미래 환경 상태를 예측하는 아이디어가 처음 제시되었다.12 에이전트는 이 학습된 모델 내에서 소위 ’롤아웃(rollout)’을 수행함으로써 여러 시간 단계 앞을 내다보며 계획을 세울 수 있었다.12</li>
<li><strong>인공적 호기심과 내재적 동기:</strong> 에이전트가 자신의 월드 모델을 개선하기 위해 새로운 데이터를 수집하도록 내재적으로 동기 부여되는 개념이 도입되었다. 이는 월드 모델이 환경의 반응을 예측하려 하고, ‘컨트롤러’ 신경망은 월드 모델이 예측하기 어려워하거나 놀라워하는 데이터로 이어지는 행동 시퀀스를 생성함으로써 보상을 받는 시스템으로 구성되었다.12 이는 현대의 자기 지도 학습(self-supervised learning) 및 탐험 기반 학습의 명백한 전조였다.</li>
<li><strong>고차원 보상 신호:</strong> 단일 스칼라 값의 보상을 넘어, 여러 센서로부터 들어오는 벡터 값의 보상 신호를 예측하는 개념이 제안되었다.12 이는 보상을 보다 전체론적으로 모델링하는 방식으로, 에이전트가 환경의 결과를 더 풍부하게 이해하도록 돕는다.</li>
</ul>
<p>이러한 개념들은 시대를 앞서갔지만, 당시의 컴퓨팅 파워와 신경망 아키텍처의 한계로 인해 그 잠재력을 완전히 실현하지는 못했다.13</p>
<h3>2.3  2018년 “World Models” 논문: 분수령이 된 순간</h3>
<p>2018년 데이비드 하(David Ha)와 위르겐 슈미트후버가 발표한 “World Models” 논문은 이 개념을 현대화하고 다시 활력을 불어넣은 기념비적인 출판물로 널리 인정받고 있다.9 이 논문은 1990년대부터 2015년까지 이어진 핵심 아이디어들을 변분 오토인코더(Variational Autoencoders, VAE) 및 혼합 밀도 신경망(Mixture Density Networks, MDN)과 같은 현대적인 딥러닝 도구와 결합했다.10</p>
<p>이 논문이 제시한 가장 강력한 개념 증명은 에이전트가 환경의 압축된 시공간 모델을 학습한 후, 이 모델이 생성한 자신만의 ‘환각적인 꿈(hallucinated dream)’ 속에서 <strong>전적으로</strong> 훈련하여 복잡한 과제(예: Car Racing, VizDoom)를 해결할 수 있음을 보여준 것이었다.9 이는 실제 환경과의 지속적인 상호작용을 요구하는 전통적인 모델-프리(model-free) 방식에서 급진적으로 벗어난 접근법이었다.</p>
<p>이 논문이 ’분수령’이 된 이유는 단순히 아이디어를 발명했기 때문이 아니다. 오히려 딥러닝 혁명 이후 등장한 강력한 표현 학습 도구(VAE)와 대규모 RNN을 효과적으로 훈련시킬 수 있는 GPU 파워라는 새로운 기술과 오래된 아이디어를 성공적으로 결합했기 때문이다. ’꿈속에서의 학습’이라는 개념을 경험적으로 증명함으로써, 하와 슈미트후버는 이 오래된 아이디어가 이론적으로 타당할 뿐만 아니라, 복잡한 과제에 대해 실용적으로 달성 가능하며 매우 효율적임을 보여주었다. 이로 인해 월드 모델은 틈새 개념에서 주류 연구 방향으로 전환되었고, 모델 기반 강화학습의 새로운 물결과 오늘날 우리가 보는 대규모 시뮬레이터의 등장을 촉발하는 계기가 되었다.</p>
<h2>3.  기초 아키텍처 (V-M-C)와 그 의의</h2>
<p>이 섹션에서는 고전적인 V-M-C 아키텍처를 심층적으로 분석하여 각 구성 요소가 어떻게 함께 작동하는지, 그리고 그 설계가 왜 그렇게 영향력이 있었는지를 설명한다.</p>
<h3>3.1  V-M-C 아키텍처 해부</h3>
<p>이 아키텍처는 에이전트의 지능을 대규모 월드 모델과 소규모 컨트롤러로 분리하여 모듈화한다.6 이 설계는 지능의 서로 다른 측면을 분리하여 각 부분을 가장 효율적인 방식으로 훈련시키기 위한 전략적 선택이다.</p>
<ul>
<li><strong>비전 모델 (V) - “눈”</strong></li>
<li><strong>기능:</strong> 게임의 픽셀 프레임과 같은 고차원 감각 입력을 저차원의 잠재 벡터(latent vector) z로 인식하고 압축하는 역할을 한다.6</li>
<li><strong>구현:</strong> 일반적으로 변분 오토인코더(VAE)가 사용된다. VAE는 이미지를 잠재 공간의 확률 분포(평균 μ와 분산 σ)로 인코딩하고, 이 분포에서 z를 샘플링한 후, 다시 z를 원본 이미지로 디코딩하도록 훈련된다. 이 과정은 z가 이미지의 불필요한 노이즈를 걸러내고 가장 핵심적인 특징을 포착하도록 강제한다.6 이 훈련은 비지도 학습 방식으로, 환경의 이미지 데이터셋만 있으면 가능하다.10</li>
<li><strong>메모리 모델 (M) - “예측하는 뇌”</strong></li>
<li><strong>기능:</strong> 환경의 ‘시간적’ 동역학을 모델링한다. 현재 잠재 상태(zt), 취해진 행동(at), 그리고 자신의 내부 기억(은닉 상태 ht)을 기반으로 ‘다음’ 잠재 상태(zt+1)를 예측한다.9</li>
<li><strong>구현:</strong> 장기 의존성 처리에 능한 순환 신경망(RNN), 특히 LSTM(Long Short-Term Memory) 네트워크가 자주 사용된다.17 미래의 본질적인 불확실성을 다루기 위해, RNN의 출력은 종종 혼합 밀도 신경망(MDN)에 입력된다. MDN-RNN은 단일한 결정론적 미래를 예측하는 대신, 다음 잠재 상태 <span class="math math-inline">z_{t+1}</span>에 대한 ‘확률 분포’(가우시안 혼합)를 예측한다.17 이는 에이전트가 월드 모델의 결정론적 결함을 악용하는 것을 방지하는 데 매우 중요하다.10</li>
<li><strong>컨트롤러 (C) - “반사 신경”</strong></li>
<li><strong>기능:</strong> 결정을 내리고 행동을 선택한다. 현재 관찰의 잠재 코드 <span class="math math-inline">z_t</span>와 메모리 모델의 은닉 상태 <span class="math math-inline">h_t</span>를 입력으로 받아 행동 <span class="math math-inline">a_t</span>를 출력한다.9</li>
<li><strong>구현:</strong> 결정적으로, 컨트롤러는 매우 간단하고 작게 설계된다. 종종 단일 선형 레이어(<span class="math math-inline">a_t=W_c[z_th_t]+b_c</span>)로 구현된다.9 이 작은 크기는 핵심적인 설계 철학이다.</li>
<li><strong>훈련:</strong> 파라미터 공간이 작기 때문에, CMA-ES(Covariance Matrix Adaption Evolution Strategy)와 같은 진화 전략을 사용하여 효율적으로 훈련할 수 있다. 이 방법은 희소한 그래디언트를 가진 노이즈가 많은 문제에 적합하며, 전체 월드 모델을 통한 역전파의 복잡성을 피할 수 있다.17</li>
</ul>
<h3>3.2  “꿈“의 힘: 잠재 공간에서의 훈련</h3>
<p>이 아키텍처의 핵심 혁신 중 하나는 컨트롤러(C)가 월드 모델이 생성한 환경, 즉 ‘꿈’ 속에서 전적으로 훈련될 수 있음을 보여준 것이다.9 이 과정은 다음과 같다: 에이전트는 특정 상태에서 시작하는 것을 ’상상’하고, M 모델은 C의 행동에 기반하여 다음 상태를 예측하며, 이 루프가 반복되어 ’꿈의 궤적’을 생성한다. 컨트롤러의 적합도는 이 꿈속에서 달성한 누적 보상에 따라 평가된다.17</p>
<p>이 접근법은 두 가지 주요 이점을 제공한다. 첫째, <strong>효율성</strong>이다. 실제 환경에서 훈련하는 것보다 계산적으로 훨씬 효율적이다. 매 단계마다 고해상도 픽셀을 렌더링하거나 복잡한 게임 엔진을 실행할 필요 없이, 모든 계산이 압축된 잠재 공간에서 이루어진다.10 둘째, <strong>강건성</strong>이다. MDN-RNN의 샘플링 과정에서 ‘온도(temperature)’ 파라미터(\tau)를 조절함으로써 꿈속 세계의 무작위성을 제어할 수 있다.9 더 시끄럽고 불확실한 꿈(높은 온도) 속에서 컨트롤러를 훈련시키면, 월드 모델의 특정 결함을 악용할 가능성이 적은 더 강건한 정책을 학습하게 된다. 이렇게 학습된 정책은 실제 환경으로 더 잘 이전(transfer)된다.9</p>
<h3>3.3  월드 모델 대 모델-프리 강화학습(RL)</h3>
<p>V-M-C 아키텍처의 등장은 전통적인 모델-프리 강화학습과의 근본적인 차이를 부각시켰다. 강화학습에서 가장 어려운 문제 중 하나인 ’신용 할당 문제(credit assignment problem)’는 먼 미래의 결과에 대해 과거의 특정 행동에 신용이나 비난을 할당하기 어려운 문제를 말한다.10 이 문제는 대규모 모델-프리 에이전트의 훈련을 병목시키는 주요 원인이다.10</p>
<p>V-M-C 설계는 이 문제를 영리하게 우회한다. 세계를 이해하는 복잡한 작업(V와 M 모델)과 정책을 최적화하는 어려운 작업(C 모델)을 분리함으로써, 각기 다른 훈련 패러다임의 장점을 최대한 활용한다. V와 M 모델은 재구성 및 예측 손실과 같은 표준적인 지도/비지도 학습을 통해 훈련되며, 이는 대규모 네트워크와 GPU에서 잘 확장된다.10 반면, 신용 할당 문제가 집중되는 컨트롤러(C)는 의도적으로 작고 단순하게 유지되어, 진화 전략과 같은 강화학습 기법으로 관리 가능한 탐색 공간 내에서 정책을 찾을 수 있게 한다.9 따라서 이 아키텍처는 단순한 모듈식 설계를 넘어, 강화학습에서 이전에는 다루기 어려웠던 훨씬 크고 표현력 있는 모델의 사용을 가능하게 하는 전략적 분업 체계인 것이다.</p>
<p>이러한 패러다임적 차이는 다음 표에 요약되어 있다.</p>
<p><strong>표 1: 패러다임 비교: 월드 모델 대 모델-프리 RL</strong></p>
<table><thead><tr><th>구분</th><th>월드 모델 (모델 기반)</th><th>모델-프리 RL</th></tr></thead><tbody>
<tr><td><strong>학습 메커니즘</strong></td><td>1. 환경 모델 학습2. 학습된 모델을 사용해 정책 학습 22</td><td>상태-행동 매핑 정책을 직접 학습 (시행착오) 10</td></tr>
<tr><td><strong>복잡성 처리</strong></td><td>세계 이해(V, M)와 의사 결정(C)을 분리. 대규모 월드 모델과 소규모 컨트롤러 사용 10</td><td>신용 할당 문제로 인해 대규모 모델 훈련에 어려움. 주로 더 작은 네트워크 사용 10</td></tr>
<tr><td><strong>데이터 효율성</strong></td><td>초기 데이터 수집 후, 내부 시뮬레이션(“꿈”)을 통해 방대한 경험 데이터 생성 가능 10</td><td>실제 환경과의 지속적이고 광범위한 상호작용 필요 10</td></tr>
<tr><td><strong>표현력</strong></td><td>V와 M 모델이 미래 예측을 포함한 풍부하고 압축된 시공간 표현을 학습하여 컨트롤러에 제공 10</td><td>주로 원시 픽셀이나 수작업 특징에 의존 10</td></tr>
</tbody></table>
<p>또한, 월드 모델의 또 다른 핵심 요소는 확률적 예측의 도입이다. 학습된 월드 모델은 필연적으로 현실의 불완전한 근사치일 수밖에 없다.10 만약 모델이 결정론적이라면, 에이전트는 꿈속에서만 가능한 물리 법칙의 버그를 찾아내어 현실에서는 불가능한 방식으로 높은 점수를 얻는 법을 배울 수 있다.10 MDN-RNN을 사용하여 모델의 예측을 확률적으로 만들면, 환경은 무작위적이고 예측 불가능해진다.17 이는 에이전트가 단일한 결함을 악용하기 어렵게 만들고, 다양한 가능한 미래에 대해 강건한 정책을 학습하도록 강제한다. 모델의 불확실성에 의해 강제된 이 강건함이야말로, 학습된 정책이 실제 복잡한 환경으로 성공적으로 일반화되고 이전될 수 있게 하는 핵심 열쇠이다.20</p>
<h2>4.  대규모 월드 시뮬레이터로의 진화</h2>
<p>고전적인 V-M-C 모델이 패러다임의 가능성을 입증한 이후, AI 연구소들은 이 개념을 대규모로 확장하기 시작했다. 이 섹션에서는 아키텍처의 변화와 그로 인해 가능해진 새로운 능력에 초점을 맞춰, 월드 모델이 어떻게 대규모 파운데이션 모델 스타일의 시뮬레이터로 진화했는지 추적한다.</p>
<h3>4.1  아키텍처의 확장과 발전</h3>
<p>VAE-RNN 접근 방식은 기초를 다졌지만, 시퀀스 길이, 메모리, 병렬 처리 능력 면에서 한계를 보였다.13 이러한 한계를 극복하기 위해 AI 커뮤니티는 언어 모델링 분야에서 검증된 강력한 아키텍처들을 도입하기 시작했다.</p>
<ul>
<li><strong>트랜스포머의 등장:</strong> 셀프 어텐션(self-attention) 메커니즘을 갖춘 트랜스포머(Transformer) 아키텍처는 데이터의 장기 의존성을 포착하는 데 RNN보다 우월함이 입증되었다.1 이로 인해 트랜스포머는 월드 모델에 통합되어 RNN을 대체하거나 보강하게 되었다.14</li>
<li><strong>통합된 표현으로서의 토큰화:</strong> 핵심적인 변화는 연속적인 잠재 벡터(z)에서 이산적인 토큰(token)으로 전환한 것이었다. TokenWM 23, Sora 24와 같은 모델들은 시각 데이터를 패치(또는 비디오의 경우 “시공간 패치”)로 토큰화하여, 이를 문장 속 단어처럼 취급한다. 이 접근법 덕분에 월드 모델은 대규모 언어 모델(LLM)을 위해 개발된 강력하고 확장 가능한 기술들을 직접 활용할 수 있게 되었다.23 예를 들어, 비전 트랜스포머(ViT) 인코더를 사용하여 이러한 패치 기반 특징을 생성할 수 있다.23</li>
<li><strong>확산 모델의 부상:</strong> 확산 모델(Diffusion model)은 강력한 생성 기술로 부상했다.1 이 모델은 데이터에 점진적으로 노이즈를 추가한 다음, 그 과정을 역으로 되돌리는 모델을 훈련시키는 방식으로 작동한다. 이 접근법은 매우 높은 충실도의 결과를 생성하는 것으로 나타났으며, Sora와 같은 모델의 핵심 기술이 되었다.24 아키텍처는 종종 확산 트랜스포머(Diffusion Transformer, DiT)를 기반으로 한다.26</li>
</ul>
<p>이러한 아키텍처의 발전은 월드 모델의 개념적 목표가 변했다기보다는, 그 목표를 달성하기 위한 기본 ’기계 장치’가 바뀐 것을 의미한다. 트랜스포머, 토큰화, 대규모 데이터라는 ’LLM의 성공 공식’을 채택함으로써, 연구자들은 단순한 게임 환경을 넘어 복잡하고 충실도 높은 비디오를 시뮬레이션하는 데 필요한 확장성을 확보했다.</p>
<h3>4.2  사례 연구: OpenAI의 Sora - 월드 시뮬레이터</h3>
<p>OpenAI는 Sora를 명시적으로 “월드 시뮬레이터로서의 비디오 생성 모델“로 규정한다.24 이는 Sora의 목표가 단순히 아름다운 비디오를 만드는 것을 넘어, 물리적 세계의 동역학을 암묵적으로 학습하고 시뮬레이션하는 데 있음을 시사한다. Sora의 아키텍처는 세 가지 핵심 기둥으로 구성된다.</p>
<ol>
<li><strong>비디오 압축 네트워크:</strong> 진보된 VAE처럼 작동하며, 원본 비디오를 입력받아 시간적, 공간적으로 압축된 저차원 잠재 공간으로 변환한다.24</li>
<li><strong>시공간 잠재 패치:</strong> 압축된 잠재 비디오는 “시공간 패치(spacetime patches)” 시퀀스로 분해된다. 이 패치들은 LLM의 토큰과 동등한 역할을 한다. 이 통합된 표현 방식은 확장성이 매우 뛰어나며, Sora가 다양한 길이, 해상도, 종횡비의 비디오와 이미지로 훈련할 수 있게 해준다.24</li>
<li><strong>확산 트랜스포머 (DiT):</strong> Sora는 트랜스포머 아키텍처를 사용하는 확산 모델이다. 텍스트 프롬프트와 노이즈가 섞인 잠재 패치가 주어지면, DiT는 원래의 “깨끗한” 패치를 예측하도록 훈련된다.26 방대한 비디오 데이터셋에 대한 DiT의 확장이 바로 Sora의 고품질 출력을 가능하게 한 원동력이다.26</li>
</ol>
<p>방대한 양의 비디오 데이터를 학습함으로써, Sora는 물리적 세계에 대한 창발적(emergent) 이해를 보여준다.24 여러 캐릭터가 상호작용하는 복잡한 장면을 생성하고, 장기적인 시간적 일관성과 객체 영속성을 유지하며, 반사, 유체 역학, 인과 관계(예: 남자가 햄버거를 먹으면 베어 문 자국이 남는 것)와 같은 일부 물리 현상을 시뮬레이션할 수 있다.24 심지어 마인크래프트와 같은 디지털 세계도 시뮬레이션할 수 있다.24</p>
<p>하지만 시뮬레이터로서 Sora의 내적 월드 모델은 여전히 불완전하다. OpenAI의 기술 안내서는 유리가 깨지거나 물체가 서로 올바르게 상호작용하는 것과 같은 기본적인 상호작용의 물리학을 정확하게 모델링하는 데 한계가 있음을 명시적으로 언급한다.24 이는 ‘시각적으로 그럴듯한’ 미래를 생성하는 것과 ‘물리적으로 정확한’ 미래를 시뮬레이션하는 것 사이의 중대한 간극을 보여준다.</p>
<h3>4.3  사례 연구: Google의 Genie - 생성형 상호작용 환경</h3>
<p>Google DeepMind의 Genie는 “생성형 상호작용 환경(generative interactive environments)“을 만드는 “파운데이션 월드 모델(foundation world model)“로 제시된다.30 110억 개의 파라미터를 가진 이 모델은 시공간 트랜스포머를 기반으로 하며, 세 가지 핵심 요소로 구성된다.32</p>
<ol>
<li><strong>시공간 비디오 토크나이저:</strong> Sora와 유사하게, 비디오를 이산적인 시공간 토큰 시퀀스로 변환한다.7 이는 종종 ST-ViViT 오토인코더로 구현된다.35</li>
<li><strong>잠재 행동 모델 (LAM):</strong> 이것이 Genie의 가장 중요한 혁신이다. LAM은 두 개의 연속된 비디오 프레임 사이에서 발생했음에 틀림없는 이산적인 “잠재 행동“을 추론하도록 훈련된다. 이는 프레임들을 관찰하고 그 변화를 나타내는 행동 토큰을 예측함으로써 이루어진다. 결정적으로, 이 과정은 어떠한 실제 행동 레이블 없이 수행된다.30 모델은 유사한 전환(예: “캐릭터가 오른쪽으로 움직임”)을 동일한 잠재 행동 토큰으로 군집화하는 법을 스스로 학습한다.36</li>
<li><strong>자기회귀 동역학 모델:</strong> 토큰화된 프레임과 예측된 잠재 행동을 입력으로 받아, 다음 프레임을 자기회귀적으로(autoregressively) 토큰 단위로 예측한다.32 효율적인 생성을 위해 종종 MaskGIT 아키텍처를 사용한다.33</li>
</ol>
<p>Genie의 가장 큰 돌파구는 레이블이 없는 방대한 인터넷 비디오 데이터셋(예: 2D 플랫폼 게임 영상)으로부터 제어 가능한 프레임 단위의 행동 공간을 학습하는 능력이다.30 이는 월드 모델 구축의 주요 병목 현상이었던, 비용이 많이 드는 행동 주석 데이터를 필요로 하는 문제를 해결한다.34</p>
<p>Genie는 수동적인 비디오 생성을 넘어선다. 잠재 행동 공간을 사용자에게 노출함으로써, 사용자가 생성된 세계를 프레임 단위로 ’제어’할 수 있게 한다.30 사용자는 초기 이미지(손으로 그린 스케치 포함)를 제공한 다음, 잠재 행동 시퀀스(예: 0-7 사이의 정수)를 입력하여 생성된 환경을 “플레이“할 수 있다.30 이는 월드 모델을 단순한 시뮬레이터에서 플레이 가능한 게임 엔진으로 변모시킨다.</p>
<p>Sora와 Genie는 월드 모델의 두 가지 발산적이면서도 상호 보완적인 최전선을 대표한다. Sora는 높은 충실도의 <strong>수동적 시뮬레이션</strong>을 추구하는 반면, Genie는 높은 충실도의 <strong>능동적 상호작용</strong>을 추구한다. Sora의 성공은 프롬프트로부터 생성된 비디오의 시각적 사실성과 시간적 일관성으로 측정된다. 반면 Genie의 성공은 사용자의 행동이 일관되고 논리적인 환경 변화로 이어지는 제어 가능한 세계를 생성하는 능력으로 측정된다. Sora는 ’관찰’에 관한 것이고, Genie는 ’주체성(agency)’에 관한 것이다. 궁극적인 AGI는 아마도 이 두 가지의 합성을 요구할 것이다. 즉, 세계를 수동적으로 시뮬레이션하는 능력과 자신의 행동이 그 시뮬레이션을 어떻게 변화시킬지 이해하는 능력을 모두 갖추어야 할 것이다.</p>
<p><strong>표 2: 기초 월드 모델 아키텍처 비교 분석</strong></p>
<table><thead><tr><th>모델</th><th>주창자</th><th>연도</th><th>핵심 아키텍처</th><th>핵심 혁신</th><th>주요 기능</th><th>한계</th></tr></thead><tbody>
<tr><td><strong>World Model</strong></td><td>Ha &amp; Schmidhuber</td><td>2018</td><td>VAE + MDN-RNN</td><td>“꿈” 잠재 공간에서 RL 에이전트 훈련</td><td>효율적인 RL 에이전트</td><td>단순한 환경에 국한, 연속적 잠재 벡터에 의존 9</td></tr>
<tr><td><strong>Sora</strong></td><td>OpenAI</td><td>2024</td><td>비디오 압축망 + 시공간 패치 + 확산 트랜스포머</td><td>창발적 시뮬레이션을 달성하기 위한 비디오 생성의 확장</td><td>고충실도 월드 시뮬레이터</td><td>상호작용성 부재, 불완전한 물리 모델, 비공개 24</td></tr>
<tr><td><strong>Genie</strong></td><td>Google DeepMind</td><td>2024</td><td>시공간 토크나이저 + 잠재 행동 모델 + 자기회귀 동역학 모델</td><td>레이블 없는 비디오에서 제어 가능한 행동 공간의 비지도 학습</td><td>생성형 상호작용 환경</td><td>2D 플랫폼 게임에 집중, 잠재적/추론적 행동 공간, 복잡한 3D 물리 미적용 30</td></tr>
</tbody></table>
<h2>5.  핵심 응용 분야</h2>
<p>이 섹션에서는 월드 모델의 이론적 개념이 로보틱스와 자율주행이라는 구체적이고 가치 높은 현실 세계 문제 해결에 어떻게 적용되고 있는지 상세히 설명한다.</p>
<h3>5.1  로보틱스와 체화된 AI (Embodied AI)</h3>
<p>월드 모델은 사전 프로그래밍된 로봇을 넘어, 복잡하고 비정형적인 환경에서 학습하고, 적응하며, 계획할 수 있는 에이전트로 나아가는 데 있어 매우 중요하다.7 이 기술의 핵심 가치는 로봇 훈련의 가장 큰 병목인 실제 세계 데이터와 상호작용에 대한 의존성을 깨뜨리는 데 있다. 이는 현실 세계의 고충실도 ’디지털 트윈’을 학습하여 훈련과 계획에 활용함으로써 달성된다.</p>
<ul>
<li><strong>학습 및 데이터 효율성 향상:</strong></li>
<li><strong>데이터 생성:</strong> 생성 모델은 조작 정책을 훈련시키기 위해 방대한 양의 합성 훈련 데이터(이미지, 비디오, 객체 변형)를 생성할 수 있다. 이는 비용이 많이 들고 희소한 실제 데이터 수집의 병목 현상을 완화한다.39 예를 들어, NVIDIA의 DreamGen은 월드 파운데이션 모델을 사용하여 합성 로봇 궤적 데이터를 생성한다.40</li>
<li><strong>내부 시뮬레이션 (상상):</strong> 로봇은 위험하고 느린 실제 세계의 시행착오 대신, 자신의 월드 모델을 사용하여 잡기나 탐색과 같은 작업을 위한 최적의 계획을 찾기 위해 수천 개의 잠재적 행동 시퀀스를 내부적으로 시뮬레이션하고 평가할 수 있다.6 이는 학습 효율성과 안전성을 극적으로 향상시킨다.38</li>
<li><strong>복잡한 작업 수행:</strong></li>
<li><strong>장기 계획 (Long-Horizon Planning):</strong> “컵을 집어 싱크대에 놓아라“와 같은 다단계 작업의 경우, 월드 모델은 에이전트가 “앞을 내다보고” 작업을 하위 목표로 분해하고 일관된 행동 순서를 계획할 수 있게 한다.38</li>
<li><strong>새로움에 대한 대처 및 적응:</strong> 물리 및 상호작용에 대한 일반적인 모델을 학습함으로써 로봇은 보지 못했던 물체를 더 잘 다루고 환경의 동적 변화에 적응할 수 있어, 잡기 계획과 같은 작업의 강건성을 향상시킨다.22 심지어 전통적인 시뮬레이터로는 매우 어려운 변형 가능한 물체와의 상호작용도 시뮬레이션할 수 있다.30</li>
</ul>
<p>이러한 능력은 로보틱스 분야의 패러다임을 ’지금 무엇을 할 것인가’라는 반응적 정책 학습에서 ’다음에 무슨 일이 일어날 것인가’라는 예측 모델 학습으로 전환시키고 있다. 이는 인간 운전자가 바로 앞 차만 보는 것이 아니라, 몇 대 앞의 차가 브레이크를 밟을 것을 예상하거나 휴대폰을 보는 보행자가 도로로 뛰어들 수 있음을 예측하는 것과 같다. 이러한 예측 능력은 단순한 반사적 행동이 아닌 계획과 전략적 의사결정을 가능하게 하며, 이는 체화된 AI가 요구하는 높은 수준의 신뢰성을 달성하는 데 필수적이다.11</p>
<h3>5.2  자율주행 (AVs)</h3>
<p>월드 모델은 자율주행차의 기본 기술로, 매우 동적이고 안전이 중요한 교통 시나리오를 예측하고 추론할 수 있게 한다.6</p>
<ul>
<li><strong>학습 기반 고충실도 시뮬레이터:</strong></li>
<li>전통적인 자율주행 시뮬레이터는 수작업으로 코딩되어 실제 주행의 모든 복잡성과 ‘롱테일’ 이벤트를 포착하는 데 어려움을 겪는다. 반면, Wayve의 GAIA-1 25, DrivingWorld 44, Waabi의 Copilot4D 45와 같은 월드 모델은 방대한 양의 실제 주행 데이터로부터 직접 시뮬레이터를 학습한다.</li>
<li>이렇게 학습된 시뮬레이터는 주행 정책을 훈련하고 검증하기 위한 현실적이고 다양한, 제어 가능한 주행 시나리오를 생성할 수 있으며, 이는 수동적인 장면 생성보다 훨씬 확장성이 뛰어나다.25</li>
<li><strong>예측과 반사실적 추론:</strong></li>
<li>자율주행차에서 월드 모델의 핵심 기능은 미래를 예측하는 것이다.25 다른 차량, 보행자, 자전거 이용자의 궤적을 예측한다.</li>
<li>더 중요한 것은, 이들이 반사실적 추론(“만약 ~라면?”)을 가능하게 한다는 점이다. 자율주행차는 자신의 잠재적 행동(“만약 급브레이크를 밟으면?”, “지금 차선을 변경하면?”)의 결과를 월드 모델을 통해 시뮬레이션한 후, 최적의 행동을 선택할 수 있다.45 예를 들어, Copilot4D는 잠재적 행동을 프롬프트로 받아 다른 차량의 반응을 예측할 수 있다.45</li>
<li><strong>공변량 변화(Covariate Shift) 문제 해결:</strong> 자율주행을 위한 모방 학습의 주요 문제 중 하나는 ’공변량 변화’이다. 모델이 작은 실수를 저질러 훈련 데이터에서 본 적 없는 상태에 진입하면 치명적인 실패로 이어질 수 있다. 월드 모델은 모델의 시뮬레이션 내에서 생성된 오류 복구 시나리오에 대해 정책을 훈련시킴으로써 이 문제를 완화할 수 있다.48</li>
</ul>
<h2>6.  핵심 도전과제와 내재적 한계</h2>
<p>이 섹션에서는 월드 모델이 잠재력을 완전히 실현하기 위해 극복해야 할 중대한 장애물들을 기술적, 안전, 윤리적 차원에서 비판적으로 분석한다.</p>
<h3>6.1  충실도-안정성-일관성 삼중고</h3>
<p>월드 모델의 핵심 과제는 <strong>생성적 충실도</strong>(시뮬레이션을 실제처럼 보이게 만드는 것)와 <strong>인과적 정확성</strong>(시뮬레이션이 실제 세계의 규칙에 따라 작동하게 만드는 것) 사이의 근본적인 긴장에서 비롯된다. 현재 모델들은 전자에 훨씬 능숙하지만 후자에는 어려움을 겪고 있다. Sora와 같은 모델은 시각적 재구성 및 그럴듯함에 우선순위를 둔 목표(예: 픽셀 또는 잠재 수준 손실)로 훈련된다.26 이들은 다음 프레임이 ’어떻게 보여야 하는지’를 예측하는 데 탁월하다. 그러나 이러한 목표는 물리, 논리, 인과의 기본 법칙을 명시적으로 강제하지 않는다. Sora가 유리 깨짐을 제대로 모델링하지 못하는 것 28은 완벽한 예시다. 즉, 깨진 유리가 어떻게 생겼는지는 알지만, 깨지는 물리적 과정은 이해하지 못한다. 이 간극을 메우는 것이 핵심 연구 과제이다.</p>
<ul>
<li><strong>장기 예측 정확도:</strong> 근본적인 도전 과제는 예측 오류가 시간이 지남에 따라 복리처럼 쌓인다는 것이다. 모델이 단기 예측(몇 프레임 또는 몇 초)에서는 정확할 수 있지만, 장기적으로는 예측이 “폭발“하거나 비현실적인 상태로 발산하는 경향이 있다.22</li>
<li><strong>월드 안정성:</strong> 관련된 개념으로, 모델이 일관성을 유지하는 능력을 측정하는 “월드 안정성(World Stability)“이 있다. 예를 들어, 에이전트가 어떤 행동을 수행한 다음 그 반대 행동을 수행하면 초기 상태로 돌아와야 한다. 현재의 확산 기반 월드 모델들은 이를 달성하는 데 상당한 어려움을 보이며, 이는 객체 영속성과 인과적 가역성에 대한 진정한 이해가 부족함을 나타낸다.50</li>
<li><strong>취약성과 비일관성:</strong> 표준 진단 지표에서 좋은 성능을 보이는 모델조차도 내적으로는 비일관적인 월드 모델을 가질 수 있다. 이는 취약성을 야기하는데, 모델이 도메인의 근본적인 인과 논리가 아닌 통계적 상관관계를 학습했기 때문에 관련 있지만 미묘하게 다른 작업에서는 실패할 수 있다.51</li>
</ul>
<h3>6.2  환각과 안전이 중요한 시스템에서의 위험</h3>
<p>다른 생성 모델과 마찬가지로 월드 모델도 “환각(hallucination)”, 즉 비현실적이거나 물리적으로 불가능한 시나리오를 생성하는 경향이 있다.13 자율주행과 같은 안전이 중요한 응용 분야에서 이는 심각한 위험이다. 월드 모델이 비합리적인 교통 시나리오를 생성하거나 다른 에이전트의 안전하지 않은 행동을 예측하면, 자율주행차의 계획 결정에 치명적인 결과를 초래할 수 있다.13 또한 에이전트가 시뮬레이션에서 높은 보상을 얻기 위해 자신의 월드 모델의 불완전성을 악용하는 법을 배울 위험이 있으며, 이는 실제 세계에서는 재앙이 될 정책으로 이어질 수 있다.10</p>
<h3>6.3  일반화, 확장성, 그리고 데이터</h3>
<ul>
<li><strong>Sim-to-Real 격차:</strong> 학습된 시뮬레이터는 강력하지만, 시뮬레이션과 현실 사이의 격차는 여전히 주요 과제이다. 월드 모델에서만 훈련된 정책은 모델이 포착하지 못한 미묘한 차이 때문에 실제 세계로 완벽하게 이전되지 않을 수 있다.22</li>
<li><strong>열린 세계로의 일반화:</strong> 현재 모델들은 종종 제한된 도메인(예: Genie의 2D 플랫폼 게임, GAIA-1의 주행)에서 뛰어난 성능을 보인다. 현실 세계의 완전하고 개방적이며 동적인 복잡성으로 일반화하는 것은 엄청난 확장성 과제이다.13</li>
<li><strong>데이터 요구사항과 편향:</strong> 이러한 거대한 모델을 훈련시키려면 인터넷 규모의 데이터가 필요하다.24 이 데이터에는 인종적, 성별, 지리적 편향과 같은 내재된 편향이 포함될 수 있으며, 모델은 이를 학습하고 시뮬레이션에서 영속시킬 것이다.53</li>
</ul>
<h3>6.4  월드 시뮬레이터의 윤리적 함의</h3>
<p>월드 모델은 생성형 AI의 모든 윤리적 과제를 상속받을 뿐만 아니라, 상호작용적이고 시뮬레이션된 맥락에서 이를 증폭시킨다.55</p>
<ul>
<li><strong>허위 정보와 딥페이크:</strong> 사람과 사건의 현실적이고 제어 가능한 비디오를 생성하는 능력은 정교한 허위 정보와 딥페이크를 만드는 데 심각한 위협이 된다.56</li>
<li><strong>편향과 공정성:</strong> 편향된 훈련 데이터는 해로운 고정관념을 영속시키거나 특정 집단에 불공정하게 작동하는 시뮬레이션 세계로 이어질 수 있다.54</li>
<li><strong>개인정보 및 저작권:</strong> 인터넷 비디오로 훈련하는 것은 막대한 개인정보 및 저작권 침해 문제를 야기한다. 모델이 훈련 데이터의 일부를 “기억“하고 재구성할 수 있으며, 여기에는 사적인 개인이나 저작권이 있는 콘텐츠가 포함될 수 있다.56</li>
<li><strong>책임 소재:</strong> 월드 모델에서 훈련된 에이전트가 실제 세계에서 해를 끼쳤을 때 책임은 누구에게 있는가? 이러한 시뮬레이션 세계 내에서 생성된 콘텐츠에 대한 책임은 누가 지는가? 이는 해결되지 않은 중대한 법적, 윤리적 질문이다.55</li>
</ul>
<p>Genie와 같은 모델의 ‘비지도’ 학습 방식은 데이터 수집의 돌파구이지만, 새로운 도전을 제기한다. 학습된 잠재 행동 공간은 해석이 불가능하고 인간의 행동 개념과 일치하지 않을 수 있어 제어 및 안전 문제를 야기한다. 예를 들어, Genie는 세계의 전환을 소수의 잠재 행동(예: 8개)으로 그룹화하는 법을 배운다.33 이는 레이블이 필요 없기 때문에 강력하다. 그러나 “잠재 행동 3“이 실제로 무엇을 의미하는가? “오른쪽으로 점프“와 대략적으로 일치할 수도 있지만, 모델이 통계적으로 편리하게 함께 묶은 복잡한 움직임의 조합일 수도 있다. 이러한 해석 불가능성은 안전 문제이다. 에이전트가 무엇을 할 수 있는지 쉽게 검증하거나 행동을 예측할 수 없다. 이는 비지도 학습이 데이터 문제를 해결하는 동시에 새로운 해석 가능성 및 제어 문제를 생성함을 보여준다.</p>
<h2>7.  월드 모델의 미래와 AGI로 가는 길</h2>
<p>이 마지막 섹션에서는 앞선 분석을 종합하여 월드 모델의 미래 궤적, LLM과 같은 다른 AI 패러다임과의 관계, 그리고 인공일반지능(AGI) 탐구에서의 잠재적 역할을 논의한다.</p>
<h3>7.1  월드 모델 대 대규모 언어 모델(LLM): 지능에 대한 두 가지 패러다임</h3>
<p>월드 모델과 LLM은 근본적으로 다른 지능 개념을 구현한다.58 LLM은 ’언어적 추상화’로서의 지능을 대표한다. 그 힘은 방대한 텍스트 코퍼스의 통계적 패턴에서 나오며, 세계에 대한 ’이해’는 언어에서 개념들이 어떻게 관련되어 있는지에 기반한 추론적인 것이지 물리적 현실에 기반을 둔(grounded) 것이 아니다.58 반면, 월드 모델은 ’체화된 인지’와 ’인과적 시뮬레이션’으로서의 지능을 대표한다. 다중 모드 감각 데이터를 통해 환경의 표현을 학습하고, 학습된 시뮬레이션을 통해 인과 관계와 물리 법칙을 모방하도록 설계되었다.11 이는 AGI를 향한 두 가지 주요 접근법 사이의 철학적, 기술적 논쟁의 핵심을 형성한다.</p>
<p><strong>표 3: AGI로 가는 길에서의 월드 모델 대 대규모 언어 모델(LLM)</strong></p>
<table><thead><tr><th>패러다임</th><th>핵심 원리</th><th>현실 모델</th><th>훈련 목표</th><th>주요 양식</th><th>핵심 강점</th><th>근본적 한계</th><th>AGI에 대한 관점</th></tr></thead><tbody>
<tr><td><strong>LLMs</strong></td><td>언어적 추상화</td><td>토큰 동시 발생의 통계적 모델</td><td>다음 토큰 예측</td><td>텍스트</td><td>놀라운 유창함, 지식 검색, 언어 작업 일반화</td><td>물리적 현실에 기반하지 않음, 진정한 인과적 추론 부재 58</td><td>확장을 통해 창발적 속성으로 AGI가 등장할 것</td></tr>
<tr><td><strong>월드 모델</strong></td><td>체화된 인지</td><td>환경의 인과적, 예측적 시뮬레이션</td><td>미래 감각 상태 예측 (행동 조건부)</td><td>다중 모드 (시각, 물리, 행동)</td><td>기반 있는 이해, 계획, 인과 관계 추론</td><td>계산 비용이 높고, 장기적 일관성 및 충실도에 어려움 6</td><td>기반과 계획을 위한 월드 모델 없이는 AGI 불가능</td></tr>
</tbody></table>
<h3>7.2  르쿤의 관점: AGI의 전제 조건으로서의 월드 모델</h3>
<p>저명한 AI 과학자 얀 르쿤은 월드 모델의 강력한 지지자이며, LLM 단독 경로를 통한 AGI 달성에 회의적이다.18 그의 주장은 진정한 지능이 추론, 계획, 그리고 물리적 세계에 대한 상식적 이해를 요구하며, LLM은 현실에 기반을 두지 않았기 때문에 이러한 능력이 근본적으로 부족하다는 것이다.62 그는 동물과 유아가 언어보다 훨씬 먼저 감각적 상호작용을 통해 세상에 대해 방대한 양을 배우며, 이 감각 데이터가 텍스트보다 훨씬 풍부하다고 지적한다.61</p>
<p>르쿤이 제안하는 AGI 아키텍처는 월드 모델이 핵심 구성 요소인 모듈식 시스템이다. 이 시스템은 감각 입력을 받아, 월드 모델을 사용하여 잠재적 행동의 결과를 예측하고, 그 결과를 일련의 목표에 대해 평가하여 계획을 세운다.64 이는 근본적으로 모델 기반의 지능 접근법이다.</p>
<h3>7.3  패러다임의 통합: 언어 유도 월드 모델(LWMs)의 부상</h3>
<p>미래는 LLM과 월드 모델 사이의 선택이 아니라, 그들의 통합에 있을 수 있다.58 ’언어 유도 월드 모델(Language-Guided World Models, LWMs)’이라는 새로운 연구 방향은 자연어 지시를 통해 조종하고 적응할 수 있는 월드 모델을 구축하는 것을 목표로 한다.65</p>
<p>LWM은 언어적 설명과 환경 동역학을 연관 짓는 법을 배운다. 예를 들어, 인간이 에이전트에게 “이제 파란색 블록은 무겁다“고 말하면, 월드 모델은 새로운 시각적 훈련 없이 내부 물리 시뮬레이션을 업데이트하여 에이전트의 행동을 변경한다.65 DLLM과 같은 모델은 언어 기반 하위 목표를 월드 모델의 롤아웃에 통합하여 탐험을 유도한다.66 이 접근법은 월드 모델의 기반 능력과 LLM의 유연한 추상적 통신 인터페이스를 결합하여, 순전히 시각 기반 모델의 해석 가능성 및 제어 문제를 해결할 잠재력을 가진다.65</p>
<h3>7.4  맺음말: 꿈꾸는 에이전트에서 시뮬레이션된 현실로</h3>
<p>이 분야는 강화학습 효율화를 위한 단순한 RNN 기반 모델에서 시작하여 9, 고충실도 비디오 시뮬레이터(Sora) 24와 생성형 상호작용 환경(Genie) 30으로 발전해왔다. 이 궤적은 명확한 야망을 보여준다: 단순히 세상의 ’모델’을 만드는 것이 아니라, 완전히 상호작용 가능한 현실의 ’시뮬레이션된 사본’을 창조하는 것이다.</p>
<p>월드 모델은 통계적 언어 패턴만으로는 달성하기 어려운 기반 있는 추론, 계획, 상식과 같은 능력을 위한 메커니즘을 제공하기 때문에 AGI의 핵심적인 구성 요소로 간주된다.38 그러나 충실도, 안정성, 안전성 면에서 중대한 과제가 남아있다.6 AGI의 도래 시점은 수년에서 수십 년까지 다양하게 예측되지만 68, 점점 더 강력해지는 생성형 월드 모델의 개발은 그 진전을 가늠하는 핵심 지표가 될 것이다. 2018년의 ‘꿈꾸는’ 에이전트에서 오늘날의 월드 시뮬레이터에 이르는 여정은 AI 연구의 심오한 변화, 즉 수동적 학습에서 능동적이고 기반 있는 상상으로의 전환을 의미한다.6 궁극적인 목표는 일부 연구자들이 구상하는 것처럼, 모든 실행 가능한 미래를 시뮬레이션할 수 있는 계층적 월드 모델을 갖춘 물리적, 주체적, 중첩된(Physical, Agentic, and Nested, PAN) AGI 시스템일 수 있다.70</p>
<h2>8. 참고 자료</h2>
<ol>
<li>What is a Generative Model? | IBM, accessed July 17, 2025, https://www.ibm.com/think/topics/generative-model</li>
<li>생성형 AI란? - ServiceNow, accessed July 17, 2025, https://www.servicenow.com/kr/now-platform/what-is-generative-ai.html</li>
<li>생성형 인공지능 - 나무위키, accessed July 17, 2025, <a href="https://namu.wiki/w/%EC%83%9D%EC%84%B1%ED%98%95%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5">https://namu.wiki/w/%EC%83%9D%EC%84%B1%ED%98%95%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5</a></li>
<li>Generative AI Models Explained - AltexSoft, accessed July 17, 2025, https://www.altexsoft.com/blog/generative-ai/</li>
<li>Generative AI: A Guide To Generative Models - Viso Suite, accessed July 17, 2025, https://viso.ai/deep-learning/generative-ai/</li>
<li>World Modeling: The Future of AI - by Sandeep Chatterjee - Medium, accessed July 17, 2025, https://medium.com/@ML-today/world-modeling-the-future-of-ai-ff8703daa220</li>
<li>World Models | Rohit Bandaru, accessed July 17, 2025, https://rohitbandaru.github.io/blog/World-Models/</li>
<li>&lt;지식 사전&gt; 현실을 이해하는 AI – 월드 모델(World Model)의 다음 진화, accessed July 17, 2025, https://blog.kakaocloud.com/214</li>
<li>World Models - Department of Computer Science and Technology |, accessed July 17, 2025, https://www.cl.cam.ac.uk/~ey204/teaching/ACS/R244_2022_2023/papers/ha_arXiv_2018.pdf</li>
<li>World Models, accessed July 17, 2025, https://worldmodels.github.io/</li>
<li>World Models in Artificial Intelligence: Sensing, Learning, and Reasoning Like a Child - arXiv, accessed July 17, 2025, https://arxiv.org/pdf/2503.15168</li>
<li>1990: Planning &amp; Reinforcement Learning with Recurrent World Models and Artificial Curiosity, accessed July 17, 2025, https://people.idsia.ch/~juergen/world-models-planning-curiosity-fki-1990.html</li>
<li>arxiv.org, accessed July 17, 2025, https://arxiv.org/html/2411.07690v1</li>
<li>WORKSHOP ON WORLD MODELS: UNDERSTANDING, MODELLING AND SCALING - OpenReview, accessed July 17, 2025, https://openreview.net/pdf?id=5uXDDU0dOh</li>
<li>[1803.10122] World Models - arXiv, accessed July 17, 2025, https://arxiv.org/abs/1803.10122</li>
<li>Paper page - World Models - Hugging Face, accessed July 17, 2025, https://huggingface.co/papers/1803.10122</li>
<li>World models - a reinforcement learning story | by SmartLab AI - Medium, accessed July 17, 2025, https://smartlabai.medium.com/world-models-a-reinforcement-learning-story-cdcc86093c5</li>
<li>월드 모델? 물리 AI? LLM은 사라질거라고? 글쎄.. - AiDA Lab., accessed July 17, 2025, https://aidalab.tistory.com/274</li>
<li>Sora와 월드모델: 우리 손에 들어온 마법의 지팡이 - 브런치, accessed July 17, 2025, https://brunch.co.kr/@byoungchaneum/74</li>
<li>Reviews: Recurrent World Models Facilitate Policy Evolution - NIPS, accessed July 17, 2025, https://proceedings.neurips.cc/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Reviews.html</li>
<li>Revisiting “Recurrent World Models Facilitate Policy Evolution” | Request PDF, accessed July 17, 2025, https://www.researchgate.net/publication/354447140_Revisiting_Recurrent_World_Models_Facilitate_Policy_Evolution</li>
<li>Robot Learning via World Models - by Kashif Ansari - Medium, accessed July 17, 2025, https://medium.com/@kansari_61048/robot-learning-via-world-models-0b6c92fa76f2</li>
<li>RECURRENT WORLD MODEL WITH TOKENIZED LATENT STATES - OpenReview, accessed July 17, 2025, https://openreview.net/pdf?id=xmwcdUdcWz</li>
<li>Video generation models as world simulators | OpenAI, accessed July 17, 2025, https://openai.com/index/video-generation-models-as-world-simulators/</li>
<li>Scaling GAIA-1: 9-billion parameter generative world model for autonomous driving - Wayve, accessed July 17, 2025, https://wayve.ai/thinking/scaling-gaia-1/</li>
<li>Under The Hood: How OpenAI’s Sora Model Works - Factorial Funds, accessed July 17, 2025, https://www.factorialfunds.com/blog/under-the-hood-how-openai-s-sora-model-works</li>
<li>OpenAI Sora’s Technical Review - Jianing Qi, accessed July 17, 2025, https://j-qi.medium.com/openai-soras-technical-review-a8f85b44cb7f</li>
<li>영상 제작 AI 모델 Sora(소라) 기술 리포트 요약 정리 - 테크뷰 블로그, accessed July 17, 2025, <a href="https://reviewinsight.blog/2024/02/20/sora%EC%86%8C%EB%9D%BC-%EA%B8%B0%EC%88%A0technical-%EB%A6%AC%ED%8F%AC%ED%8A%B8-%EC%9A%94%EC%95%BD-%EC%A0%95%EB%A6%AC/">https://reviewinsight.blog/2024/02/20/sora%EC%86%8C%EB%9D%BC-%EA%B8%B0%EC%88%A0technical-%EB%A6%AC%ED%8F%AC%ED%8A%B8-%EC%9A%94%EC%95%BD-%EC%A0%95%EB%A6%AC/</a></li>
<li>Sora: Creating video from text - OpenAI, accessed July 17, 2025, https://openai.com/index/sora/</li>
<li>Genie: Generative Interactive Environments - Google Sites, accessed July 17, 2025, https://sites.google.com/view/genie-2024/home</li>
<li>Genie: Generative Interactive Environments - Google DeepMind, accessed July 17, 2025, https://deepmind.google/research/publications/60474/</li>
<li>Genie: Generative Interactive Environments - Hugging Face, accessed July 17, 2025, https://huggingface.co/blog/vladbogo/genie-generative-interactive-environments</li>
<li>Generative Interactive Environments - Genie - GitHub, accessed July 17, 2025, https://raw.githubusercontent.com/mlresearch/v235/main/assets/bruce24a/bruce24a.pdf</li>
<li>Exploration-Driven Generative Interactive Environments - CVF Open Access, accessed July 17, 2025, https://openaccess.thecvf.com/content/CVPR2025/papers/Savov_Exploration-Driven_Generative_Interactive_Environments_CVPR_2025_paper.pdf</li>
<li>Learning Generative Interactive Environments By Trained Agent Exploration - OpenReview, accessed July 17, 2025, https://openreview.net/pdf/dc140297e80cff3db770ca993536cc916fe80b3f.pdf</li>
<li>게임 개발자까지 위협하는 Genie AI (Google DeepMind 가상 게임 AI 엔진) - YouTube, accessed July 17, 2025, https://www.youtube.com/watch?v=03GNezizv3c</li>
<li>[AI넷] [구글의 AI는 냅킨 그림을 기반으로 비디오 게임을 만든다]. 구글 딥마인드는 최근 인터넷 동영상을 분석하여 2D 비디오 게임 제작 방법을 학습한 AI 모델을 발표했다. 일단 훈련되면 인간에게 필요한 유일한 자산은 단일 이미지이다. 냅킨 그림도 가능하다., accessed July 17, 2025, http://www.ainet.link/13720</li>
<li>World Models: The Blueprint for Intelligent Robotics and AGI | Towards AI, accessed July 17, 2025, https://towardsai.net/p/machine-learning/world-models-the-blueprint-for-intelligent-robotics-and-agi</li>
<li>arxiv.org, accessed July 17, 2025, https://arxiv.org/html/2503.03464v1</li>
<li>R²D²: Training Generalist Robots with NVIDIA Research Workflows and World Foundation Models, accessed July 17, 2025, https://developer.nvidia.com/blog/r2d2-training-generalist-robots-with-nvidia-research-workflows-and-world-foundation-models/</li>
<li>Innovative Applications of Generative AI in Robotics - BotPenguin, accessed July 17, 2025, https://botpenguin.com/blogs/innovative-applications-of-generative-ai-in-robotics</li>
<li>Collect some World Models for Autonomous Driving (and Robotic) papers. - GitHub, accessed July 17, 2025, https://github.com/LMD0311/Awesome-World-Model</li>
<li>AI’s Next Frontier: Advancing Large World Models for Robotics &amp; AVs | Cutter Consortium, accessed July 17, 2025, https://www.cutter.com/article/large-world-models</li>
<li>DRIVINGWORLD: CONSTRUCTING WORLD MODEL FOR AUTONOMOUS DRIVING VIA VIDEO GPT - OpenReview, accessed July 17, 2025, https://openreview.net/pdf/aad7fc36550d4db84152c09c15aae1687253abc5.pdf</li>
<li>Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion - Waabi, accessed July 17, 2025, https://waabi.ai/copilot-4d/</li>
<li>World Models and the Sparks of Little Robotics | Andreessen Horowitz, accessed July 17, 2025, https://a16z.com/world-models-and-the-sparks-of-little-robotics/</li>
<li>CVPR24 E2EAI | Gianluca Corrado: Learning Models of the World - YouTube, accessed July 17, 2025, https://www.youtube.com/watch?v=q9ZO1RO5-ys</li>
<li>Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles Using Latent Space Generative World Models - Research at NVIDIA, accessed July 17, 2025, https://research.nvidia.com/publication/2024-09_mitigating-covariate-shift-imitation-learning-autonomous-vehicles-using-latent</li>
<li>Long-Term Predictions, Hold the ‘Explosions’ | Illinois Institute of Technology, accessed July 17, 2025, https://www.iit.edu/news/long-term-predictions-hold-explosions</li>
<li>Toward Stable World Models: Measuring and Addressing World Instability in Generative Environments - arXiv, accessed July 17, 2025, https://arxiv.org/html/2503.08122v1</li>
<li>[2406.03689] Evaluating the World Model Implicit in a Generative Model - arXiv, accessed July 17, 2025, https://arxiv.org/abs/2406.03689</li>
<li>LLM) Large Language Model 기본 개념 알아보기, accessed July 17, 2025, https://data-newbie.tistory.com/953</li>
<li>On the Challenges and Opportunities in Generative AI - arXiv, accessed July 17, 2025, https://arxiv.org/html/2403.00025v3</li>
<li>Generative AI in Education: The Impact, Ethical Considerations, and Use Cases - Litslink, accessed July 17, 2025, https://litslink.com/blog/generative-ai-in-education-the-impact-ethical-considerations-and-use-cases</li>
<li>(PDF) A Conceptual Framework for Solving Ethical Issues in …, accessed July 17, 2025, https://www.researchgate.net/publication/377370419_A_Conceptual_Framework_for_Solving_Ethical_Issues_in_Generative_Artificial_Intelligence</li>
<li>A Conceptual Framework for Solving Ethical Issues in Generative Artificial Intelligence - IOS Press Ebooks, accessed July 17, 2025, https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA231182</li>
<li>On the Challenges and Opportunities in Generative AI - arXiv, accessed July 17, 2025, https://arxiv.org/html/2403.00025v1</li>
<li>World Models vs. Large Language Models and the Potential to …, accessed July 17, 2025, https://medium.com/@d.incecushman/world-models-vs-large-language-models-and-the-potential-to-co-f1db3d947122</li>
<li>ELI5: What is the difference between Large Language Models and Artificial Inteligence? : r/explainlikeimfive - Reddit, accessed July 17, 2025, https://www.reddit.com/r/explainlikeimfive/comments/1ik1v7j/eli5_what_is_the_difference_between_large/</li>
<li>[Discussion] What exactly are World Models in AI? What problems do they solve, and where are they going? : r/MachineLearning - Reddit, accessed July 17, 2025, https://www.reddit.com/r/MachineLearning/comments/1kf3pes/discussion_what_exactly_are_world_models_in_ai/</li>
<li>Tech leaders eye world models as link to smarter AI | IBM, accessed July 17, 2025, https://www.ibm.com/think/news/world-models-smarter-ai</li>
<li>Yann LeCun on Lex Fridman’s Podcast: The Road to AGI Runs Through Open Source AI, accessed July 17, 2025, https://nyudatascience.medium.com/yann-lecun-on-lex-fridmans-podcast-the-road-to-agi-runs-through-open-source-ai-e536bbd17317</li>
<li>Yann LeCun: We Won’t Reach AGI By Scaling Up LLMS - YouTube, accessed July 17, 2025, https://www.youtube.com/watch?v=4__gg83s_Do&amp;pp=0gcJCfwAo7VqN5tD</li>
<li>What does Yann LeCun think about AGI? A summary of his talk …, accessed July 17, 2025, https://adamjones.me/blog/yann-lecun-on-agi/</li>
<li>LANGUAGE-GUIDED WORLD MODELS A MODEL-BASED APPROACH TO AI CONTROL, accessed July 17, 2025, https://language-guided-world-model.github.io/static/pdfs/lgwm.pdf</li>
<li>World Models with Hints of Large Language Models for Goal Achieving - ACL Anthology, accessed July 17, 2025, https://aclanthology.org/2025.naacl-long.3.pdf</li>
<li>Artificial General Intelligence: Hype or Future Reality - Megatrends by HP, accessed July 17, 2025, https://hpmegatrends.com/artificial-general-intelligence-hype-or-future-reality-d8856551610f</li>
<li>When Will AGI/Singularity Happen? 8,590 Predictions Analyzed - Research AIMultiple, accessed July 17, 2025, https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/</li>
<li>Scenario Planning for an AGI Future-Anton Korinek - International Monetary Fund (IMF), accessed July 17, 2025, https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek</li>
<li>[2507.05169] Critiques of World Models - arXiv, accessed July 17, 2025, https://arxiv.org/abs/2507.05169</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>