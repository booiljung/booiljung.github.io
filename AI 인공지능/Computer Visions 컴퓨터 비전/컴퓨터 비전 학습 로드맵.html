<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:컴퓨터 비전 학습 로드맵</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>컴퓨터 비전 학습 로드맵</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">컴퓨터 비전 (Computer Visions)</a> / <span>컴퓨터 비전 학습 로드맵</span></nav>
                </div>
            </header>
            <article>
                <h1>컴퓨터 비전 학습 로드맵</h1>
<h2>1. 컴퓨터 비전의 세계로</h2>
<p>컴퓨터 비전은 인공지능(AI)의 한 분야로, 기계가 시각적 세계를 해석하고 이해하도록 훈련시키는 학문입니다.1 인간의 시각 시스템이 평생의 경험을 통해 세상을 보고 이해하는 것처럼, 컴퓨터 비전은 카메라, 데이터, 알고리즘을 통해 기계에 ’눈’을 부여하여 이미지와 동영상 속에서 객체를 식별하고, 움직임을 추적하며, 장면을 이해하는 것을 목표로 합니다.2</p>
<p>이 기술은 단순히 학문적 호기심을 넘어, 우리 생활과 산업 전반에 깊숙이 자리 잡고 있습니다. 자율주행차는 컴퓨터 비전을 통해 도로와 보행자를 인식하고 3, 의료 분야에서는 의사들이 질병을 더 빠르고 정확하게 진단하도록 돕습니다.1 제조 공장에서는 불량품을 자동으로 검수하고 4, 농업에서는 작물의 성장을 관리하며 3, 소매점에서는 고객 경험을 혁신합니다.6</p>
<p>본 안내서는 컴퓨터 비전을 배우고자 하는 학습자를 위해 기초부터 최신 기술까지 아우르는 체계적인 학습 로드맵을 제시합니다. 이 여정은 디지털 이미지를 구성하는 가장 작은 단위인 픽셀에서 시작하여, 전통적인 이미지 처리 기법을 거쳐, 딥러NING 혁명을 이끈 합성곱 신경망(CNN)과 그 발전 과정을 탐구합니다. 나아가, 현재 컴퓨터 비전의 최전선에 있는 비전 트랜스포머(ViT), 생성형 AI(GAN, 디퓨전 모델), 그리고 자기 지도 학습(SSL)과 같은 고급 주제들을 다룰 것입니다. 마지막으로, 이 강력한 기술을 책임감 있게 사용하기 위해 반드시 알아야 할 윤리적 문제와 연구 동향, 미래 전망까지 조망하며 컴퓨터 비전 전문가로 성장하기 위한 완전한 가이드를 제공하는 것을 목표로 합니다.</p>
<h2>2.  컴퓨터 비전의 기초</h2>
<p>이 파트에서는 컴퓨터 비전 분야 전체의 개념적, 실용적 토대를 마련합니다. 컴퓨터 비전의 정의와 역사부터 시작하여, 분석의 기본 단위인 디지털 이미지의 구조, 그리고 실습에 필수적인 도구와 데이터셋에 대해 학습합니다.</p>
<h3>2.1  컴퓨터 비전이란 무엇인가?: 역사와 응용 분야 개요</h3>
<h4>2.1.1  정의와 목표</h4>
<p>컴퓨터 비전(Computer Vision)은 기계(컴퓨터)가 인간의 시각 시스템이 수행하는 작업을 자동화하도록 하는 인공지능의 한 분야입니다.5 근본적인 목표는 디지털 이미지나 비디오와 같은 시각적 데이터로부터 의미 있는 정보를 추출하고 해석하여, 이를 바탕으로 특정 작업을 수행하거나 의사결정을 내리는 것입니다.5 이는 단순히 이미지를 ‘보는’ 것을 넘어, 그 안에 담긴 객체, 장면, 활동을 ’이해’하는 것을 포함합니다. 예를 들어, 시스템은 이미지를 보고 ’고양이’라는 레이블을 붙이는 것(분류)뿐만 아니라, 이미지 속 고양이의 위치를 찾아내고(탐지), 심지어는 ’소파 위에서 잠자고 있는 고양이’라는 장면 전체의 맥락을 파악하는(장면 이해) 수준에 도달하는 것을 지향합니다.</p>
<h4>2.1.2  역사적 흐름: 아이디어에서 혁명으로</h4>
<p>컴퓨터 비전의 역사는 하드웨어, 데이터, 알고리즘의 발전과 궤를 같이하는 흥미로운 여정입니다. 이 세 요소의 상호작용이 어떻게 오늘날의 혁신을 이끌었는지 이해하는 것은 매우 중요합니다.</p>
<ul>
<li>
<p><strong>태동기 (1950년대-1970년대):</strong> 컴퓨터 비전의 개념은 인공지능 연구의 초창기에 시작되었습니다.7 1957년 러셀 커쉬가 최초의 디지털 이미지 스캐너를 개발하며 시각 정보를 숫자로 변환하는 길이 열렸습니다.8 1966년 MIT의 ’여름 비전 프로젝트(Summer Vision Project)’는 “카메라를 컴퓨터에 연결하여 무엇을 보는지 설명하게 하라“는 야심 찬 목표를 세웠습니다.7 비록 이 목표가 당시에는 지나치게 낙관적이었음이 증명되었지만, 이는 컴퓨터 비전이라는 분야의 공식적인 시작을 알리는 신호탄이었습니다. 이 시기에는 이미지에서 선이나 원과 같은 기하학적 패턴을 찾는 허프 변환(Hough Transform)과 같은 기초적인 알고리즘들이 개발되었습니다.5 당시 연구의 핵심은 2D 이미지로부터 3D 구조를 추론하여 완전한 장면을 이해하려는 시도였습니다.7</p>
</li>
<li>
<p><strong>발전기 (1980년대-2000년대 초반):</strong> 1980년대에는 더 엄밀한 수학적 분석에 기반한 연구들이 등장했습니다.7 기계 학습 기법이 도입되기 시작했고, 얼굴 인식(Viola-Jones 알고리즘)과 같은 실용적인 응용 분야에서 중요한 돌파구가 마련되었습니다.10 그러나 이 시기까지도 컴퓨터 비전은 제한된 환경에서 특정 문제만을 해결할 수 있었으며, 이는 주로 데이터와 컴퓨팅 파워의 한계 때문이었습니다.</p>
</li>
<li>
<p><strong>혁명기 (2010년대-현재):</strong> 21세기에 들어 인터넷의 보급과 디지털 카메라의 대중화는 전례 없는 규모의 시각적 데이터를 만들어냈습니다. 이러한 배경 속에서 페이페이 리 교수가 주도한 이미지넷(ImageNet) 프로젝트는 1,000만 장이 넘는 레이블링된 이미지 데이터셋을 구축하며 패러다임의 전환을 이끌었습니다.12 이 거대한 데이터셋은 이전에는 불가능했던 데이터 기반 학습을 가능하게 했습니다. 동시에, 그래픽 처리를 위해 개발된 GPU(Graphics Processing Unit)가 대규모 병렬 연산에 탁월하다는 점이 발견되면서, 이론으로만 존재하던 깊은 신경망(Deep Neural Networks)의 학습이 현실화되었습니다.11</p>
</li>
</ul>
<p>이러한 하드웨어와 데이터의 결합은 2012년 이미지넷 이미지 인식 챌린지(ILSVRC)에서 결정적인 순간을 맞이했습니다. Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton이 개발한 ’알렉스넷(AlexNet)’이라는 깊은 합성곱 신경망(CNN)이 기존의 모든 경쟁자를 압도적인 성능 차이로 누르고 우승을 차지한 것입니다.9 이는 컴퓨터 비전 분야에서 딥러닝 시대의 개막을 알리는 역사적인 사건이었으며, 이후 연구와 산업의 방향을 완전히 바꾸어 놓았습니다.</p>
<h4>2.1.3  핵심 과제와 현대적 응용</h4>
<p>컴퓨터 비전은 다양한 하위 과제들로 구성되며, 이들은 종종 복합적으로 사용되어 복잡한 문제를 해결합니다.</p>
<ul>
<li><strong>핵심 과제:</strong></li>
<li><strong>이미지 분류 (Image Classification):</strong> 이미지 전체에 대해 하나의 레이블(예: ‘고양이’, ‘자동차’)을 할당하는 작업입니다.2</li>
<li><strong>객체 탐지 (Object Detection):</strong> 이미지 내에 있는 특정 객체들의 위치를 경계 상자(Bounding Box)로 표시하고, 각 객체가 무엇인지 분류하는 작업입니다.4</li>
<li><strong>이미지 분할 (Image Segmentation):</strong> 이미지의 모든 픽셀을 특정 클래스나 객체에 할당하는, 가장 정교한 인식 작업입니다. 예를 들어, 자율주행차는 도로, 보행자, 다른 차량, 하늘 등을 픽셀 단위로 구분해야 합니다.3</li>
<li><strong>자세 추정 (Pose Estimation):</strong> 이미지나 비디오에서 사람의 관절 위치와 같은 주요 지점(Keypoints)을 찾아내어 자세를 파악하는 기술입니다.4</li>
<li><strong>객체 추적 (Object Tracking):</strong> 비디오 시퀀스에서 특정 객체를 프레임 간에 지속적으로 따라가는 기술입니다.3</li>
<li><strong>현대적 응용 분야:</strong></li>
<li><strong>자율 시스템 (Autonomous Systems):</strong> 자율주행차는 컴퓨터 비전을 통해 차선, 신호등, 보행자, 장애물 등을 실시간으로 인식하여 주행 결정을 내립니다.3</li>
<li><strong>의료 및 헬스케어 (Healthcare):</strong> X-레이, CT, MRI와 같은 의료 영상을 분석하여 종양이나 병변을 탐지하고, 의사의 진단을 보조합니다.1 스마트폰 카메라로 피부 상처를 촬영하여 3D로 측정하는 기술도 개발되었습니다.3</li>
<li><strong>제조 및 소매 (Manufacturing &amp; Retail):</strong> 생산 라인에서 제품의 결함을 자동으로 검사하고(품질 관리), 로봇이 재고를 파악하며(재고 관리), 고객의 동선을 분석하여 매장 레이아웃을 최적화합니다.3</li>
<li><strong>농업 (Agriculture):</strong> 드론이나 로봇이 촬영한 이미지로 작물의 건강 상태를 모니터링하고, 잡초를 식별하여 정밀하게 농약을 살포함으로써 생산성을 높이고 비용을 절감합니다.3</li>
<li><strong>보안 및 감시 (Security &amp; Surveillance):</strong> 얼굴 인식을 통한 출입 통제, CCTV 영상에서 이상 행동을 감지하여 경고하는 시스템 등에 활용됩니다.5</li>
<li><strong>증강 및 가상현실 (AR/VR):</strong> 현실 세계를 인식하고 그 위에 가상 정보를 덧씌우거나, 완전한 가상 환경을 구축하는 데 핵심적인 역할을 합니다.5</li>
</ul>
<p>1.2. 디지털 이미지: 픽셀, 색 공간, 히스토그램</p>
<p>컴퓨터 비전의 가장 기본적인 분석 단위는 디지털 이미지입니다. 컴퓨터에게 이미지는 단순히 그림이 아니라, 숫자로 이루어진 거대한 행렬 또는 텐서(Tensor)입니다.19 이 숫자들의 의미와 구조를 이해하는 것이 모든 이미지 처리의 출발점입니다.</p>
<h4>2.1.4  픽셀: 이미지의 기본 구성 요소</h4>
<p>디지털 이미지는 픽셀(Pixel, Picture Element)이라는 작은 사각형 점들의 2차원 격자(Grid)로 구성됩니다.19 각 픽셀은 이미지의 특정 위치에서의 색상과 밝기 정보를 담고 있으며, 이 픽셀들이 모여 전체 이미지를 형성합니다. 컴퓨터는 이 픽셀 값들을 숫자 행렬로 인식하여 처리합니다.19</p>
<h4>2.1.5  색상 표현: 색 깊이와 색 공간</h4>
<p>픽셀이 어떻게 색을 표현하는지는 ’색 깊이(Color Depth)’와 ’색 공간(Color Space)’에 의해 결정됩니다.</p>
<ul>
<li><strong>색 깊이 (Bit Depth):</strong> 하나의 픽셀을 표현하는 데 사용되는 비트(bit)의 수를 의미합니다.21 색 깊이가 클수록 더 많은 색상을 표현할 수 있습니다.</li>
<li><strong>8비트:</strong> 픽셀당 8비트를 사용하여 28=256가지의 색상이나 명암 단계를 표현합니다. 흑백(Grayscale) 이미지가 대표적입니다.19</li>
<li><strong>24비트 트루 컬러 (True Color):</strong> 픽셀당 24비트를 사용하며, 일반적으로 빨강(R), 초록(G), 파랑(B) 각 채널에 8비트씩 할당됩니다. 이는 약 1,677만(224) 가지의 색상을 표현할 수 있어 인간의 눈으로 구별할 수 있는 대부분의 색을 나타냅니다.23</li>
<li><strong>색 공간 (Color Space):</strong> 색상을 수치적으로 표현하기 위한 다양한 모델(체계)입니다. 각 색 공간은 특정 목적에 맞게 설계되었으며, 컴퓨터 비전에서는 작업의 특성에 따라 적절한 색 공간으로 변환하여 사용하는 경우가 많습니다.23</li>
<li><strong>그레이스케일 (Grayscale):</strong> 색상 정보 없이 오직 밝기(명암) 정보만을 가지는 단일 채널 이미지입니다.23 각 픽셀은 0(검은색)부터 255(흰색)까지의 값으로 표현됩니다.19 많은 전통적인 컴퓨터 비전 알고리즘은 색상보다 밝기 변화에 더 민감하게 반응하도록 설계되었기 때문에, 컬러 이미지를 그레이스케일로 변환하여 처리하는 경우가 많습니다. 이는 연산량을 줄이는 효과도 있습니다.19</li>
<li><strong>RGB (Red, Green, Blue):</strong> 빛의 삼원색인 빨강, 초록, 파랑을 혼합하여 색을 표현하는 가산 혼합 모델입니다.22 디지털 카메라, 모니터 등 대부분의 디스플레이 장치에서 표준으로 사용되며, 3개의 채널을 가진 3차원 텐서로 표현됩니다.19</li>
<li><strong>HSV (Hue, Saturation, Value):</strong> 색상(Hue), 채도(Saturation), 명도(Value)의 세 가지 요소로 색을 표현합니다.23 이는 인간이 색을 인지하는 방식과 유사하여 직관적입니다. 특히, 조명 변화에 덜 민감한 색상(Hue) 정보만을 분리하여 사용할 수 있어 특정 색상의 객체를 검출하는 작업에 매우 유용합니다.19</li>
<li><strong>YCbCr:</strong> 밝기 정보인 루마(Luma, Y)와 색상 정보인 크로마(Chroma, Cb/Cr)를 분리하여 표현하는 색 공간입니다.25 인간의 시각 시스템은 밝기 변화에 비해 색상 변화에 덜 민감하다는 특성을 이용합니다. JPEG 이미지 압축이나 비디오 스트리밍에서 크로마 채널의 해상도를 낮추어(Chroma Subsampling) 데이터 크기를 효율적으로 줄이는 데 핵심적인 역할을 합니다.26</li>
</ul>
<h4>2.1.6  이미지 히스토그램: 이미지의 통계적 초상화</h4>
<p>이미지 히스토그램은 이미지에 포함된 픽셀들의 명암 값 분포를 그래프로 나타낸 것입니다.27</p>
<ul>
<li><strong>개념:</strong> 가로축은 픽셀의 밝기 값(예: 0~255), 세로축은 해당 밝기 값을 가진 픽셀의 개수를 나타내는 막대그래프입니다.27 이는 이미지의 전반적인 밝기와 대비(Contrast)를 한눈에 파악할 수 있는 통계적 요약 정보입니다.</li>
<li><strong>해석과 활용:</strong></li>
<li>히스토그램이 한쪽(어두운 쪽 또는 밝은 쪽)에 치우쳐져 있다면, 해당 이미지는 노출이 부족하거나 과다하다는 것을 의미합니다.</li>
<li>히스토그램이 좁은 영역에 집중되어 있다면, 이는 대비가 낮은, 즉 전반적으로 흐릿한 이미지임을 나타냅니다.29</li>
<li>이러한 히스토그램 정보를 활용하여 이미지의 품질을 개선할 수 있습니다. 대표적인 기법이 **히스토그램 평활화(Histogram Equalization)**로, 좁게 분포된 픽셀 값들을 전체 범위에 걸쳐 균일하게 재분배하여 이미지의 대비를 극대화하는 방법입니다.27</li>
</ul>
<h3>2.2  실무자의 도구 상자: OpenCV, PyTorch, TensorFlow 입문</h3>
<p>컴퓨터 비전을 실제로 구현하기 위해서는 적절한 도구를 선택하고 활용하는 능력이 필수적입니다. 현재 컴퓨터 비전 분야는 고전적인 알고리즘을 위한 라이브러리와 딥러닝 모델을 위한 프레임워크로 양분되어 있습니다.</p>
<h4>2.2.1  OpenCV: 전통 컴퓨터 비전의 강자</h4>
<p>OpenCV(Open Source Computer Vision Library)는 컴퓨터 비전과 머신러닝을 위한 가장 대표적인 오픈소스 라이브러리입니다.32 1999년 인텔에서 시작되어 C++로 개발되었으며, 현재는 Python, Java 등 다양한 언어를 지원합니다.32</p>
<ul>
<li><strong>주요 기능:</strong> 2,500개가 넘는 최적화된 알고리즘을 포함하고 있으며, 다음과 같은 광범위한 기능을 제공합니다.33</li>
<li><strong>이미지 처리 (<code>imgproc</code>):</strong> 필터링, 기하학적 변환, 색 공간 변환, 히스토그램 계산 등 기본적인 이미지 조작 기능.</li>
<li><strong>특징점 검출 및 기술 (<code>features2d</code>):</strong> SIFT, SURF, ORB와 같은 특징점 검출 및 기술자 생성 알고리즘.</li>
<li><strong>객체 탐지 (<code>objdetect</code>):</strong> Haar Cascade와 같은 전통적인 객체 탐지 기법.</li>
<li><strong>비디오 분석 (<code>video</code>):</strong> 객체 추적, 배경 제거, 모션 추정.</li>
<li><strong>DNN 추론 (<code>dnn</code>):</strong> 사전 훈련된 딥러닝 모델(TensorFlow, PyTorch 등에서 훈련된)을 불러와 추론(Inference)을 수행하는 기능.</li>
<li><strong>학습에서의 역할:</strong> OpenCV는 딥러닝 이전의 전통적인 컴퓨터 비전 알고리즘을 학습하고 실험하는 데 필수적입니다. 또한, 딥러닝 파이프라인에서도 이미지 로딩, 전처리, 데이터 증강 등 유틸리티 라이브러리로서 중요한 역할을 계속해서 수행하고 있습니다.</li>
</ul>
<h4>2.2.2  PyTorch vs. TensorFlow: 딥러닝 프레임워크 양대 산맥</h4>
<p>현대의 컴퓨터 비전은 딥러닝과 동의어처럼 사용될 정도로 딥러닝 모델, 특히 CNN에 크게 의존합니다. PyTorch와 TensorFlow는 이러한 딥러닝 모델을 구축, 훈련, 배포하기 위한 가장 강력하고 널리 사용되는 두 프레임워크입니다.35 학습자는 자신의 목표와 선호도에 따라 프레임워크를 선택해야 합니다.</p>
<table><thead><tr><th>특징</th><th>PyTorch</th><th>TensorFlow</th><th>학습자를 위한 핵심</th></tr></thead><tbody>
<tr><td><strong>계산 그래프</strong></td><td><strong>동적 (Define-by-Run)</strong> 36</td><td><strong>정적 (Define-and-Run)</strong> (TF 1.x) / 동적 (TF 2.x Eager Execution) 36</td><td>PyTorch는 코드가 실행되는 시점에 그래프를 생성하여 유연하고 디버깅이 직관적입니다. 연구와 프로토타이핑에 유리합니다. TensorFlow는 먼저 그래프를 정의하고 세션을 통해 실행하여 최적화와 배포에 강점이 있었습니다.</td></tr>
<tr><td><strong>API 스타일 및 사용 편의성</strong></td><td>Pythonic, 직관적 37</td><td>Keras를 통한 고수준 API 제공, 유연성은 다소 낮음 37</td><td>PyTorch는 Python 프로그래머에게 매우 친숙한 객체지향적 구조를 가집니다. TensorFlow는 Keras를 통해 사용이 간편해졌지만, 여전히 PyTorch가 더 유연하고 배우기 쉽다는 평이 많습니다.</td></tr>
<tr><td><strong>디버깅</strong></td><td>Python 표준 디버거(pdb 등) 사용 가능, 용이함 39</td><td>상대적으로 복잡, 전용 디버거 필요 (TF 1.x) 40</td><td>동적 그래프 덕분에 PyTorch는 중간 계산 값을 확인하는 등 디버깅이 훨씬 쉽습니다. 이는 학습 과정에서 매우 큰 장점입니다.</td></tr>
<tr><td><strong>배포 및 상용화</strong></td><td>TorchServe, PyTorch Live 등 발전 중이나 아직은 미성숙 35</td><td><strong>TensorFlow Serving, TFLite, TF.js 등 성숙하고 강력한 생태계 보유</strong> 41</td><td><strong>상용 제품이나 모바일/IoT/웹에 모델을 배포하는 것이 목표라면 TensorFlow가 여전히 강력한 우위를 점하고 있습니다.</strong></td></tr>
<tr><td><strong>커뮤니티 및 연구 동향</strong></td><td><strong>연구 커뮤니티에서 압도적 우위</strong>, 최신 논문 대부분 PyTorch로 구현 35</td><td>구글의 강력한 지원, 산업계에서 넓은 사용자 기반 보유 37</td><td>최신 연구 동향을 따라가고 새로운 모델을 빠르게 실험해보고 싶다면 PyTorch가 필수적입니다. HuggingFace와 같은 모델 허브에서도 PyTorch 모델이 대다수입니다.</td></tr>
<tr><td><strong>시각화 도구</strong></td><td>Visdom (기능 제한적), TensorBoard 통합 지원</td><td><strong>TensorBoard</strong> (매우 강력하고 표준적인 도구) 36</td><td>TensorBoard는 훈련 과정의 손실, 정확도, 가중치 분포 등을 시각화하는 데 매우 강력한 도구이며, 현재는 PyTorch에서도 쉽게 사용할 수 있습니다.</td></tr>
</tbody></table>
<p><strong>학습자를 위한 제언:</strong> 컴퓨터 비전의 원리와 딥러닝 모델링 자체를 깊이 있게 배우고 싶다면 <strong>PyTorch</strong>로 시작하는 것을 강력히 추천합니다.41 파이썬 친화적인 문법과 직관적인 디버깅 환경은 학습 곡선을 완만하게 만들어주며, 최신 연구 논문들을 직접 구현하고 실험해보기에 가장 좋은 환경을 제공합니다. 하지만 최종 목표가 산업 현장에서의 모델 배포와 서비스화에 있다면, TensorFlow의 강력한 배포 도구들(TFX, TFLite)에 대해서도 반드시 학습해두어야 합니다.41</p>
<h3>2.3  필수 데이터셋: ImageNet과 COCO의 역할 이해</h3>
<p>대규모의 고품질 데이터셋은 현대 딥러닝 기반 컴퓨터 비전의 발전을 이끈 핵심 동력입니다. 그중에서도 이미지넷(ImageNet)과 COCO는 가장 중요하고 상징적인 데이터셋입니다.</p>
<h4>2.3.1  이미지넷 (ImageNet): 딥러닝 혁명의 촉매제</h4>
<p>이미지넷은 WordNet의 계층 구조에 따라 정리된 대규모 이미지 데이터베이스입니다.44</p>
<ul>
<li><strong>규모와 영향:</strong> 1,400만 장 이상의 이미지와 2만 개 이상의 카테고리를 포함하는 방대한 규모를 자랑합니다.46 특히, 매년 열렸던 이미지넷 대규모 시각 인식 챌린지(ILSVRC)는 컴퓨터 비전 연구의 방향을 제시하는 중요한 역할을 했습니다.47 2012년 이 대회에서 AlexNet이 압도적인 성능으로 우승하면서 딥러닝의 시대가 열렸습니다.12</li>
<li><strong>주요 용도 (전이 학습):</strong> 이미지넷의 가장 중요한 역할은 **전이 학습(Transfer Learning)**을 위한 사전 훈련(Pre-training) 데이터셋으로서의 기능입니다.48 1,000개 클래스와 120만 장의 훈련 이미지를 가진 ILSVRC 데이터셋으로 딥러닝 모델(예: ResNet)을 미리 훈련시키면, 모델은 이미지의 기본적인 특징(선, 질감, 형태 등)부터 복잡한 특징(객체의 일부)까지 일반적인 시각적 표현을 학습하게 됩니다. 이렇게 사전 훈련된 모델의 가중치를 가져와, 자신이 풀고자 하는 특정 문제(예: 의료 영상 분석, 특정 제품 불량 검출)에 맞게 소규모의 데이터로 추가 학습(Fine-tuning)시키면, 처음부터 모델을 훈련시키는 것보다 훨씬 적은 데이터와 시간으로 높은 성능을 달성할 수 있습니다.46</li>
</ul>
<h4>2.3.2  COCO: 문맥 속의 객체들</h4>
<p>COCO(Common Objects in Context)는 이미지 분류를 넘어 더 복잡하고 현실적인 장면 이해를 목표로 하는 데이터셋입니다.49</p>
<ul>
<li><strong>특징:</strong> COCO는 약 33만 개의 이미지에 80개의 객체(things) 카테고리와 91개의 배경(stuff) 카테고리에 대한 정교한 주석(Annotation)을 제공합니다.49 이미지넷이 주로 단일 객체가 중앙에 위치한 ‘상징적인’ 이미지가 많은 반면, COCO는 일상적인 환경 속에서 여러 객체들이 다양한 크기와 위치로, 서로 겹쳐져 있는 복잡한 장면을 담고 있습니다.51</li>
<li><strong>풍부한 주석:</strong> COCO는 단순한 이미지 레이블을 넘어 다음과 같은 다양한 주석을 제공하여 여러 연구의 표준 벤치마크로 사용됩니다.50</li>
<li><strong>객체 탐지:</strong> 각 객체 인스턴스에 대한 경계 상자(Bounding Box).</li>
<li><strong>분할:</strong> 각 객체의 정확한 외곽선을 따르는 픽셀 단위 마스크(Pixel-level Mask).</li>
<li><strong>키포인트 탐지:</strong> 사람의 관절 위치 등 주요 지점.</li>
<li><strong>이미지 캡셔닝:</strong> 이미지를 설명하는 5개의 자연어 문장.</li>
</ul>
<p>이러한 특성 때문에 COCO는 객체 탐지, 인스턴스 분할(Instance Segmentation), 키포인트 탐지 모델의 성능을 평가하는 표준 벤치마크로 자리 잡았습니다.54</p>
<h2>3.  전통적인 컴퓨터 비전 기법</h2>
<p>딥러닝이 컴퓨터 비전의 주류가 되기 전, 이 분야는 수십 년간 정교하게 다듬어진 수학적, 통계적 알고리즘에 의해 발전해 왔습니다. 이러한 ‘수작업(hand-crafted)’ 기법들을 이해하는 것은 두 가지 중요한 의미를 가집니다. 첫째, 이미지에서 어떤 특징이 중요한지에 대한 근본적인 직관을 제공합니다. 둘째, 이 기법들은 여전히 현대 딥러닝 파이프라인에서 전처리, 후처리, 혹은 특정 모듈의 일부로 활발히 사용되고 있습니다.</p>
<h3>3.1  이미지 전처리: 필터링, 변환, 히스토그램 평활화</h3>
<p>이미지 전처리는 원본 이미지로부터 유의미한 정보를 추출하기 쉽도록 이미지를 변환하거나 개선하는 과정입니다. 노이즈를 제거하고, 특징을 강조하며, 분석에 적합한 형태로 데이터를 가공하는 것이 주된 목적입니다.</p>
<h4>3.1.1  이미지 필터링 (컨볼루션)</h4>
<p>이미지 필터링은 컴퓨터 비전에서 가장 기본적인 연산 중 하나로, 커널(Kernel) 또는 필터(Filter)라 불리는 작은 행렬을 이미지 전체에 걸쳐 슬라이딩시키며 적용하는 컨볼루션(Convolution) 연산을 통해 수행됩니다.55 커널의 값에 따라 이미지에 다양한 효과를 줄 수 있습니다.</p>
<ul>
<li><strong>저주파 통과 필터 (Low-Pass Filters) - 블러링:</strong> 이미지의 고주파 성분(급격한 밝기 변화, 노이즈)을 제거하고 부드럽게 만드는 효과가 있습니다.57 이는 특징 추출 전 노이즈를 줄이는 데 필수적입니다.</li>
<li><strong>평균 필터 (Mean/Box Filter):</strong> 커널 내의 모든 픽셀 값의 산술 평균으로 중심 픽셀 값을 대체합니다. 간단하지만 블러 효과가 부자연스러울 수 있습니다.58</li>
<li><strong>가우시안 블러 (Gaussian Blur):</strong> 정규분포(가우시안 분포)에 기반한 가중 평균을 사용합니다. 중심 픽셀에 가까울수록 높은 가중치를 부여하여 더 자연스러운 블러 효과를 만듭니다. Canny 엣지 검출이나 SIFT와 같은 여러 중요 알고리즘의 전처리 단계로 사용됩니다.58</li>
<li><strong>고주파 통과 필터 (High-Pass Filters) - 샤프닝:</strong> 이미지의 경계선이나 세부적인 디테일(고주파 성분)을 강조하여 이미지를 더 선명하게 만듭니다.57 대표적인 기법인 **언샤프 마스크(Unsharp Mask)**는 원본 이미지에서 블러링된 이미지를 빼서 경계선 정보만 추출한 뒤, 이를 다시 원본 이미지에 더하여 경계를 강조하는 방식입니다.57</li>
<li><strong>노이즈 제거 특화 필터:</strong></li>
<li><strong>미디언 필터 (Median Filter):</strong> 커널 내 픽셀 값들의 중앙값(Median)으로 중심 픽셀을 대체합니다. 이미지에 소금과 후추를 뿌린 듯한 형태의 ‘소금-후추 노이즈(Salt-and-Pepper Noise)’ 제거에 매우 효과적입니다.58</li>
<li><strong>양방향 필터 (Bilateral Filter):</strong> 노이즈를 제거하면서도 객체의 경계선은 선명하게 보존하는 고급 필터입니다.58 픽셀 간의 공간적 거리뿐만 아니라 픽셀 값(밝기)의 차이도 함께 고려합니다. 이 덕분에 비슷한 밝기를 가진 영역 내에서는 스무딩을 적용하고, 밝기 차이가 큰 경계선 부분은 스무딩을 약하게 적용하여 엣지를 보존할 수 있습니다.55</li>
</ul>
<h4>3.1.2  형태학적 변환 (Morphological Transformations)</h4>
<p>주로 이진(binary) 이미지에서 객체의 형태를 변형하거나 노이즈를 제거하기 위해 사용되는 기법들입니다.62</p>
<ul>
<li><strong>침식 (Erosion)과 팽창 (Dilation):</strong> 침식은 객체 경계의 밝은 픽셀을 제거하여 객체의 크기를 줄이는 효과가 있으며, 작은 노이즈를 제거하는 데 사용됩니다. 팽창은 반대로 객체 경계에 밝은 픽셀을 추가하여 객체의 크기를 키우고, 객체 내부의 작은 구멍을 메우는 데 사용됩니다.62</li>
<li><strong>열림 (Opening)과 닫힘 (Closing):</strong> 이 두 연산을 조합한 것입니다. 열림은 침식 후 팽창을 적용하는 것으로, 돌출된 작은 노이즈를 효과적으로 제거합니다. 닫힘은 팽창 후 침식을 적용하는 것으로, 객체 내부의 작은 구멍이나 끊어진 부분을 연결하는 데 효과적입니다.62</li>
</ul>
<h4>3.1.3  히스토그램 평활화 (Histogram Equalization)</h4>
<p>대비(Contrast)가 낮은 이미지의 품질을 향상시키는 강력한 기법입니다.27</p>
<ul>
<li><strong>원리:</strong> 이미지의 히스토그램을 분석하여 픽셀 값들이 특정 범위에 몰려 있는 경우, 이 분포를 전체 명암 범위(0~255)에 걸쳐 균일하게 펼쳐줍니다.31 수학적으로는 이미지의 누적 분포 함수(Cumulative Distribution Function, CDF)를 변환 함수로 사용하여 픽셀 값을 재매핑합니다.64</li>
<li><strong>효과:</strong> 너무 어둡거나 밝게 촬영되어 세부 사항이 잘 보이지 않는 이미지의 대비를 극적으로 향상시켜, 숨겨져 있던 특징들이 잘 드러나게 만듭니다.29 주로 그레이스케일 이미지에 적용됩니다.64</li>
</ul>
<h3>3.2  특징 검출 및 기술: Canny, Sobel부터 SIFT, SURF, ORB까지</h3>
<p>이미지에서 의미 있는 정보를 추출하는 핵심 단계는 ’특징(Feature)’을 찾아내는 것입니다. 특징은 이미지의 크기, 회전, 조명 변화에도 불구하고 안정적으로 검출될 수 있는 고유한 지점이나 영역을 의미합니다.</p>
<h4>3.2.1  엣지 검출 (Edge Detection)</h4>
<p>엣지는 이미지에서 픽셀의 밝기가 급격하게 변하는 지점으로, 보통 객체와 배경, 또는 객체와 객체 사이의 경계를 나타냅니다.61 엣지 정보는 이미지 분할, 객체 인식 등의 기초 자료로 활용됩니다.</p>
<ul>
<li><strong>소벨 연산자 (Sobel Operator):</strong> 3x3 크기의 두 커널을 사용하여 이미지의 가로(x) 및 세로(y) 방향의 미분 값(기울기)을 근사적으로 계산합니다.65 간단하고 빠르지만, 노이즈에 민감하고 검출된 엣지가 두껍게 나타나는 단점이 있습니다.67</li>
<li><strong>캐니 엣지 검출기 (Canny Edge Detector):</strong> 현재까지도 가장 널리 사용되는 표준적인 엣지 검출 알고리즘으로, 소벨의 단점을 크게 개선하여 우수한 성능을 보입니다.67 캐니는 다음의 다단계 프로세스를 통해 정교한 엣지를 검출합니다 70:</li>
</ul>
<ol>
<li><strong>노이즈 제거:</strong> 5x5 가우시안 필터를 적용하여 입력 이미지의 노이즈를 줄입니다.61</li>
<li><strong>기울기 계산:</strong> 소벨 필터를 사용하여 각 픽셀의 기울기 크기(Gradient Magnitude)와 방향(Direction)을 계산합니다.61</li>
<li><strong>비최대 억제 (Non-maximum Suppression):</strong> 기울기 방향을 따라 픽셀들을 검사하여, 국소적 최대값(Local Maximum)이 아닌 픽셀들을 제거합니다. 이 과정을 통해 소벨 연산자의 결과물이었던 두꺼운 엣지가 한 픽셀 두께의 얇은 선으로 바뀝니다.61</li>
<li><strong>이력 임계값 (Hysteresis Thresholding):</strong> 두 개의 임계값(높은 값, 낮은 값)을 사용합니다. 높은 임계값보다 큰 픽셀은 ’강한 엣지’로, 두 임계값 사이에 있는 픽셀은 ’약한 엣지’로 분류합니다. 최종적으로 강한 엣지와 연결된 약한 엣지만을 실제 엣지로 인정하고, 나머지 약한 엣지(노이즈일 가능성이 높은)는 제거합니다. 이 과정 덕분에 실제 엣지는 끊어지지 않고 잘 연결되면서 노이즈는 효과적으로 제거됩니다.69</li>
</ol>
<h4>3.2.2  특징점(Keypoint) 검출 및 기술</h4>
<p>서로 다른 이미지(예: 다른 각도에서 찍은 같은 건물 사진)에서 동일한 지점을 안정적으로 찾아내기 위한 기법입니다. 이렇게 찾은 점을 <strong>특징점(Feature Point)</strong> 또는 **키포인트(Keypoint)**라고 합니다.</p>
<ul>
<li><strong>SIFT (Scale-Invariant Feature Transform):</strong> 이미지의 크기 변화(Scale), 회전(Rotation), 조명 변화에 모두 강인한(Robust) 특징점을 검출하는 획기적인 알고리즘입니다.72 SIFT는 그 강력한 성능으로 인해 오랫동안 특징점 검출의 ’황금 표준’으로 여겨졌습니다.</li>
</ul>
<ol>
<li><strong>스케일-공간 극값 검출:</strong> 이미지 피라미드(다양한 크기의 이미지)를 만들고, 각 이미지에 대해 서로 다른 수준의 가우시안 블러를 적용합니다. 인접한 블러 이미지 간의 차이(Difference of Gaussians, DoG)를 계산하여, 스케일과 공간상에서 극값(최대/최소값)을 갖는 점들을 키포인트 후보로 찾습니다. 이 과정은 크기 변화에 불변하는 특징점을 찾기 위함입니다.72</li>
<li><strong>키포인트 위치 정밀화:</strong> 후보점들 중 대비(contrast)가 낮거나 엣지 위에 있는 불안정한 점들을 제거하여 신뢰도 높은 키포인트만 남깁니다.74</li>
<li><strong>방향 할당:</strong> 각 키포인트 주변 픽셀들의 기울기 방향에 대한 히스토그램을 만들어 가장 우세한 방향을 키포인트의 주 방향으로 할당합니다. 이로써 회전 불변성을 확보합니다.74</li>
<li><strong>키포인트 기술자(Descriptor) 생성:</strong> 키포인트의 주 방향을 기준으로 정렬된 16x16 픽셀 영역에서, 4x4로 나눈 16개 소영역 각각의 8방향 기울기 히스토그램을 계산합니다. 이를 연결하여 128차원의 벡터(기술자)를 생성합니다. 이 기술자는 해당 키포인트의 고유한 ‘지문’ 역할을 하여 다른 이미지의 키포인트와 비교하는 데 사용됩니다.75</li>
</ol>
<ul>
<li><strong>SURF (Speeded-Up Robust Features):</strong> SIFT의 성능은 유지하면서 속도를 크게 향상시킨 알고리즘입니다.78 SIFT의 DoG 연산을 적분 영상(Integral Image)을 이용한 박스 필터(Box Filter)로 근사하여 계산 속도를 획기적으로 줄였습니다.80 기술자도 64차원으로 더 간결합니다.</li>
<li><strong>ORB (Oriented FAST and Rotated BRIEF):</strong> 실시간 처리가 중요한 환경을 위해 개발된 매우 빠르고 효율적인 알고리즘입니다.79 빠른 코너 검출기인 FAST와 효율적인 이진(binary) 기술자인 BRIEF를 결합하고, 방향 정보를 추가하여 회전 불변성을 확보했습니다.82 SIFT나 SURF에 비해 성능은 다소 떨어질 수 있지만, 계산 비용이 매우 낮아 모바일 기기나 임베디드 시스템에서 널리 사용됩니다.79</li>
</ul>
<p>이러한 알고리즘들의 선택은 ’공짜 점심은 없다’는 원칙을 잘 보여줍니다. SIFT는 가장 강력한 성능과 안정성을 제공하지만 계산 비용이 매우 높습니다. 반면 ORB는 매우 빠르지만 성능의 안정성은 상대적으로 낮습니다. 따라서 응용 프로그램의 요구사항(실시간성, 정확도, 자원 제약 등)에 따라 적절한 알고리즘을 선택하는 엔지니어링적 판단이 필요합니다.</p>
<h3>3.3  전통적 이미지 분할: 임계값, 영역 기반, 클러스터링</h3>
<p>이미지 분할(Image Segmentation)은 이미지를 의미 있는 여러 영역으로 나누는 과정입니다. 각 픽셀에 특정 레이블을 할당하여, 예를 들어 ‘사람’, ‘자동차’, ‘도로’ 등으로 구분합니다.</p>
<ul>
<li><strong>임계값 기반 분할 (Thresholding):</strong> 가장 간단한 분할 방법입니다. 픽셀의 밝기 값을 기준으로 특정 임계값(Threshold)보다 밝으면 전경(Foreground), 어두우면 배경(Background)으로 나누는 방식입니다.83</li>
</ul>
<p><strong>오츄(Otsu)의 방법</strong>은 클래스 내 분산은 최소화하고 클래스 간 분산은 최대화하는 최적의 임계값을 자동으로 찾아주는 알고리즘입니다.83 조명이 균일하지 않은 이미지에서는 전체 이미지에 단일 임계값을 적용하기 어렵기 때문에, 이미지를 여러 작은 영역으로 나누어 각 영역에 맞는 임계값을 적용하는 **적응적 임계처리(Adaptive Thresholding)**가 사용됩니다.62</p>
<ul>
<li>
<p><strong>영역 기반 분할 (Region-Based Segmentation):</strong> 인접한 픽셀들이 비슷한 특성(색상, 질감 등)을 가질 것이라는 가정에서 출발합니다.</p>
</li>
<li>
<p><strong>영역 확장법 (Region Growing):</strong> 몇 개의 ‘씨앗(seed)’ 픽셀에서 시작하여, 주변에 있는 비슷한 특성을 가진 픽셀들을 점차적으로 병합해 나가며 영역을 키우는 방식입니다.83</p>
</li>
<li>
<p><strong>클러스터링 기반 분할 (Clustering-Based Segmentation):</strong> 이미지의 픽셀들을 특징 공간(예: RGB 색상 공간)의 데이터 포인트로 간주하고, 클러스터링 알고리즘을 적용하여 비슷한 픽셀들을 그룹으로 묶는 방법입니다.</p>
</li>
<li>
<p><strong>K-평균 군집화 (K-Means Clustering):</strong> 사용자가 지정한 K개의 클러스터로 픽셀들을 분할합니다. 각 픽셀은 가장 가까운 클러스터의 중심으로 할당되고, 클러스터의 중심은 소속된 픽셀들의 평균값으로 계속 업데이트됩니다. 이 과정을 클러스터 중심이 더 이상 변하지 않을 때까지 반복합니다.83 간단하고 빠르지만, K값을 미리 정해야 하고 초기 중심 위치에 따라 결과가 달라지는 단점이 있습니다.</p>
</li>
<li>
<p><strong>워터셰드 알고리즘 (Watershed Algorithm):</strong> 이미지의 기울기 크기(Gradient Magnitude)를 지형도로 간주하는 독특한 접근법입니다. 밝기 변화가 적은 영역은 ‘유역(Catchment Basin)’, 변화가 큰 경계선은 ’분수령(Watershed Ridge Line)’에 해당합니다. 물을 채워나갈 때 서로 다른 유역에서 시작된 물이 만나는 지점이 경계가 되는 원리를 이용해 이미지를 분할합니다.83</p>
</li>
</ul>
<h3>3.4  고전적 객체 추적: 칼만 필터와 상관 필터의 논리</h3>
<p>객체 추적(Object Tracking)은 비디오의 연속된 프레임에서 특정 객체의 위치를 지속적으로 따라가는 기술입니다. 객체 탐지(Object Detection)가 매 프레임마다 처음부터 객체를 찾는 반면, 추적은 이전 프레임의 정보를 활용하여 현재 프레임에서 객체의 위치를 ’예측’하기 때문에 훨씬 빠르고 효율적입니다.87</p>
<ul>
<li><strong>칼만 필터 (Kalman Filter):</strong> 선형적인 움직임과 가우시안 노이즈를 가정하는 시스템에서 상태를 추정하는 데 매우 강력한 재귀 필터입니다.88 객체 추적에서는 다음과 같은 예측-갱신 주기로 작동합니다:</li>
</ul>
<ol>
<li>
<p><strong>예측 (Predict):</strong> 이전 프레임의 상태(위치, 속도)와 움직임 모델을 기반으로 현재 프레임에서 객체가 어디에 있을지 예측합니다.</p>
</li>
<li>
<p>갱신 (Update): 현재 프레임에서 실제로 측정된 값(예: 객체 탐지기의 결과)을 사용하여 예측을 보정하고 상태를 갱신합니다.</p>
</li>
</ol>
<p>이 칼만 필터는 SORT(Simple Online and Realtime Tracking) 알고리즘의 핵심입니다. SORT는 칼만 필터로 객체의 움직임을 예측하고, 헝가리안 알고리즘(Hungarian Algorithm)을 사용하여 예측된 위치와 현재 프레임에서 탐지된 객체들을 IoU(Intersection over Union) 기반으로 가장 효율적으로 연결(매칭)합니다.88</p>
<ul>
<li>
<p><strong>상관 필터 (Correlation Filters) (예: KCF, CSRT):</strong> 이 추적기들은 객체 영역에 높은 응답 값을 출력하고 배경 영역에는 낮은 값을 출력하는 필터를 학습하는 방식입니다.</p>
</li>
<li>
<p><strong>KCF (Kernelized Correlation Filter):</strong> 매우 빠른 속도를 자랑하는 상관 필터 추적기입니다. 하지만 객체의 외형이 급격히 변하거나 빠르게 움직일 경우 추적을 놓치는 경향이 있습니다.87</p>
</li>
<li>
<p><strong>CSRT (Discriminative Correlation Filter with Channel and Spatial Reliability):</strong> KCF보다 정확도와 강인성이 뛰어난 추적기이지만 속도는 더 느립니다. 특히 객체가 가려지는(Occlusion) 상황에 더 잘 대처합니다. 이는 채널 및 공간 신뢰도 맵을 사용하여 필터가 객체의 더 신뢰할 수 있는 부분에 집중하도록 학습하기 때문입니다.87</p>
</li>
</ul>
<h3>3.5  3차원 재구성: 스테레오 비전과 깊이 추정의 원리</h3>
<p>컴퓨터 비전의 궁극적인 목표 중 하나는 2D 이미지로부터 3D 세계를 재구성하는 것입니다. 깊이 추정(Depth Estimation)은 이미지의 각 픽셀이 카메라로부터 얼마나 멀리 떨어져 있는지를 계산하는 기술입니다.90</p>
<ul>
<li><strong>스테레오 비전 (Stereo Vision):</strong> 인간이 두 눈을 사용하여 깊이감을 느끼는 원리(양안시)를 모방한 기술입니다.91 알려진 거리만큼 떨어져 있는 두 대 이상의 카메라로 같은 장면을 동시에 촬영합니다.</li>
<li><strong>삼각 측량 원리 (Triangulation):</strong> 왼쪽 이미지와 오른쪽 이미지에서 동일한 지점(대응점, Corresponding Point)을 찾으면, 두 카메라의 위치와 기하학적 관계를 이용하여 삼각 측량법으로 해당 지점의 3D 좌표를 계산할 수 있습니다.90</li>
<li><strong>시차 (Disparity):</strong> 두 이미지에서 대응점 간의 수평 위치 차이를 의미합니다. 시차는 깊이와 <strong>반비례</strong> 관계에 있습니다. 즉, 가까운 물체일수록 시차가 크고, 멀리 있는 물체일수록 시차가 작습니다.91</li>
<li><strong>스테레오 매칭 (Stereo Matching):</strong> 스테레오 비전의 가장 핵심적인 과제는 두 이미지 간의 대응점을 정확하게 찾는 것입니다. **블록 매칭(Block Matching)**은 왼쪽 이미지의 특정 영역(블록)을 오른쪽 이미지의 에피폴라 라인(Epipolar Line)을 따라 탐색하여 가장 유사한 블록을 찾는 고전적인 방법입니다.90</li>
<li><strong>장단점:</strong> 주변광만으로 작동 가능하고 외부 광원 간섭에 강하지만 91, 질감(Texture)이 없는 평평한 벽이나 하늘 같은 영역에서는 대응점을 찾기 어려워 깊이 추정에 실패할 수 있습니다.91</li>
<li><strong>능동적 방법 (Active Methods) vs. 수동적 방법 (Passive Methods):</strong></li>
<li><strong>수동적 방법:</strong> 일반적인 스테레오 비전처럼 주변의 자연광에만 의존하는 방식입니다.94</li>
<li><strong>능동적 방법:</strong> 깊이 정보를 더 쉽게 얻기 위해 장비가 직접 빛이나 패턴을 장면에 투사하는 방식입니다.94</li>
<li><strong>구조광 (Structured Light):</strong> 격자나 점과 같은 특정 패턴을 물체에 투사하고, 물체 표면의 굴곡에 따라 패턴이 왜곡되는 형태를 카메라로 촬영하여 깊이를 계산합니다. 높은 정밀도를 자랑하지만, 밝은 야외에서는 투사된 패턴이 보이지 않을 수 있습니다.91</li>
<li><strong>ToF (Time-of-Flight):</strong> 적외선 레이저 펄스를 발사하여 물체에 맞고 센서로 돌아오는 데 걸리는 시간을 측정하여 거리를 계산합니다. 실시간 반응이 중요하고 빠른 측정이 가능하지만, 구조광 방식에 비해 해상도가 낮을 수 있습니다.91</li>
</ul>
<p>이처럼 전통적인 컴퓨터 비전 기법들은 이미지의 기울기, 즉 밝기 값의 변화율이라는 근본적인 정보를 다양한 방식으로 활용하여 엣지, 특징점, 영역을 정의하고 이를 통해 더 높은 수준의 작업을 수행합니다. 이러한 접근 방식은 딥러닝 모델 내부에서 일어나는 일들을 이해하는 데 중요한 기초 지식을 제공하며, 딥러닝이 왜 그토록 강력한지를 역설적으로 보여주는 기준점이 됩니다.</p>
<h2>4.  딥러닝 혁명과 비전 모델</h2>
<p>2010년대 초, 딥러닝의 등장은 컴퓨터 비전 분야에 지각변동을 일으켰습니다. 수작업으로 특징을 설계하던 시대는 저물고, 데이터로부터 특징 계층을 자동으로 학습하는 ‘엔드-투-엔드(end-to-end)’ 학습 패러다임이 도래했습니다. 이 혁명의 중심에는 **합성곱 신경망(Convolutional Neural Network, CNN)**이 있었습니다. 이 파트에서는 CNN의 기본 구조부터 시작하여, 이미지 분류, 객체 탐지, 이미지 분할 분야에서 획기적인 발전을 이끈 대표적인 딥러닝 아키텍처들을 심층적으로 분석합니다.</p>
<h3>4.1  합성곱 신경망(CNN): 새로운 패러다임</h3>
<p>전통적인 완전 연결 신경망(Fully-Connected Neural Network)은 입력 데이터를 1차원 벡터로 처리해야 합니다. 이는 2차원(흑백) 또는 3차원(컬러) 구조를 가진 이미지 데이터를 입력으로 사용할 때, 픽셀들을 일렬로 펼치는 과정에서 중요한 공간적 정보(spatial information)를 손실시키는 근본적인 한계를 가집니다.20 CNN은 이러한 문제를 해결하기 위해 이미지의 공간 구조를 유지한 채로 학습할 수 있도록 설계된 특별한 신경망입니다.20 그 영감은 동물의 시각 피질에서 뉴런들이 시야의 특정 영역(수용장, Receptive Field)에만 반응하는 구조에서 비롯되었습니다.96</p>
<h4>4.1.1  합성곱 계층(Convolutional Layer): 커널을 통한 특징 추출</h4>
<p>합성곱 계층은 CNN의 핵심 구성 요소로, 이미지에서 특징을 추출하는 역할을 담당합니다.98</p>
<ul>
<li><strong>핵심 연산:</strong> 이 계층은 학습 가능한 여러 개의 <strong>필터(Filter)</strong> 또는 **커널(Kernel)**로 구성됩니다.95 필터는 작은 크기의 가중치 행렬(예: 3x3, 5x5)로, 입력 이미지 위를 일정한 간격(stride)으로 이동하면서 각 위치에서</li>
</ul>
<p><strong>컨볼루션(합성곱)</strong> 연산을 수행합니다. 이 연산은 필터와 이미지의 해당 영역 간의 원소별 곱셈(element-wise multiplication)의 합으로 계산되며, 그 결과로 <strong>특징 맵(Feature Map)</strong> 또는 **활성화 맵(Activation Map)**이라는 2차원 행렬이 생성됩니다.20</p>
<ul>
<li><strong>기능과 계층적 특징 학습:</strong> 각 필터는 특정 종류의 시각적 패턴을 감지하도록 학습됩니다. 예를 들어, 네트워크의 초기 계층에 있는 필터들은 수직선, 수평선, 특정 색상의 점과 같은 단순하고 기본적인 특징들을 감지합니다. 네트워크가 깊어질수록, 이전 계층에서 추출된 단순한 특징 맵들을 입력으로 받아 이들을 조합하여 눈, 코, 바퀴와 같은 더 복잡하고 추상적인 특징들을 학습하게 됩니다.95 이러한</li>
</ul>
<p><strong>계층적 특징 학습(Hierarchical Feature Learning)</strong> 능력은 CNN이 이미지의 복잡한 구조를 이해하는 핵심 원리입니다.96</p>
<ul>
<li>
<p><strong>주요 하이퍼파라미터:</strong></p>
</li>
<li>
<p><strong>필터 크기(Filter Size):</strong> 커널의 너비와 높이. 작은 필터(예: 3x3)는 국소적인 세부 특징을, 큰 필터는 더 넓은 영역의 특징을 포착합니다.103</p>
</li>
<li>
<p><strong>스트라이드(Stride):</strong> 필터가 한 번에 이동하는 픽셀의 거리입니다. 스트라이드가 1이면 픽셀 단위로 촘촘하게 이동하고, 2 이상이면 성큼성큼 이동하며 출력 특징 맵의 크기를 줄이는 효과가 있습니다.20</p>
</li>
<li>
<p><strong>패딩(Padding):</strong> 입력 이미지의 가장자리에 0과 같은 특정 값을 채워 넣는 것입니다. 패딩을 사용하면 컨볼루션 연산 후에도 특징 맵의 공간적 크기가 줄어들지 않도록 조절할 수 있으며, 이미지 가장자리의 정보 손실을 방지하는 중요한 역할을 합니다.20</p>
</li>
</ul>
<h4>4.1.2  풀링 계층(Pooling Layer): 다운샘플링과 불변성</h4>
<p>풀링 계층은 특징 맵의 공간적 크기(너비와 높이)를 점진적으로 줄여나가는 <strong>다운샘플링(Downsampling)</strong> 역할을 합니다.98</p>
<ul>
<li><strong>기능:</strong> 특징 맵의 크기를 줄임으로써 네트워크의 전체 파라미터 수와 계산량을 감소시켜 학습 효율을 높입니다.95 또한, 객체의 위치가 약간 변하더라도 풀링의 결과는 크게 달라지지 않기 때문에, 모델이 객체의 미세한 위치 변화에 덜 민감해지는 **이동 불변성(Translation Invariance)**을 어느 정도 확보하게 해줍니다.105</li>
<li><strong>종류:</strong></li>
<li><strong>최대 풀링(Max Pooling):</strong> 특징 맵의 특정 영역에서 가장 큰 값(가장 강하게 활성화된 특징)을 선택합니다. 가장 널리 사용되는 방식으로, 중요한 특징을 보존하는 데 효과적입니다.98</li>
<li><strong>평균 풀링(Average Pooling):</strong> 특정 영역의 모든 값의 평균을 계산합니다. 최대 풀링에 비해 덜 사용되지만, 일부 아키텍처에서는 유용하게 쓰입니다.104</li>
</ul>
<h4>4.1.3  활성화 함수와 분류: ReLU와 완전 연결 계층</h4>
<ul>
<li>
<p><strong>활성화 함수 (ReLU):</strong> 합성곱 연산 자체는 선형 변환입니다. 여기에 비선형(non-linear) 활성화 함수를 적용해야만 네트워크가 복잡한 패턴을 학습할 수 있습니다.108</p>
</li>
<li>
<p><strong>ReLU (Rectified Linear Unit):</strong> <code>f(x) = max(0, x)</code>로 정의되는 함수입니다. 즉, 입력이 양수이면 그대로 출력하고, 음수이면 0으로 만듭니다.109 ReLU는 기존에 사용되던 시그모이드(Sigmoid)나 하이퍼볼릭 탄젠트(tanh) 함수가 깊은 네트워크에서 겪던 **기울기 소실 문제(Vanishing Gradient Problem)**를 효과적으로 완화합니다. 또한 계산이 매우 간단하여 딥러닝 모델의 학습 속도를 크게 향상시켰습니다.110</p>
</li>
<li>
<p><strong>단점:</strong> 입력이 음수일 때 기울기가 0이 되어 해당 뉴런이 더 이상 학습되지 않는 <strong>‘죽은 ReLU(Dying ReLU)’</strong> 문제가 발생할 수 있습니다. 이를 해결하기 위해 Leaky ReLU와 같은 변형 함수들이 제안되었습니다.108</p>
</li>
<li>
<p><strong>완전 연결 계층 (Fully-Connected Layer, FC Layer):</strong> 여러 개의 합성곱과 풀링 계층을 거쳐 추출된 고수준의 특징 맵은 최종적으로 분류 작업을 위해 1차원 벡터로 펼쳐집니다(Flatten).20 이 벡터는 하나 이상의 완전 연결 계층에 입력됩니다. 완전 연결 계층은 전통적인 신경망의 계층과 동일하며, 이전 계층의 모든 뉴런이 다음 계층의 모든 뉴런과 연결된 구조입니다.107 이 계층들은 추출된 특징들을 종합하여 최종적인 분류를 수행하며, 마지막 출력 계층에서는 주로</p>
</li>
</ul>
<p><strong>소프트맥스(Softmax)</strong> 함수를 사용하여 각 클래스에 대한 확률을 출력합니다.98</p>
<h3>4.2  대표 아키텍처: 이미지 분류</h3>
<p>이미지 분류는 컴퓨터 비전의 가장 기본적인 작업이며, 딥러닝 모델의 성능을 겨루는 주요 무대였습니다. 이 과정에서 등장한 대표적인 아키텍처들은 이후 더 복잡한 작업들의 기반이 되었습니다.</p>
<table><thead><tr><th>모델</th><th>발표 연도</th><th>핵심 혁신</th><th>깊이</th><th>영향</th></tr></thead><tbody>
<tr><td><strong>AlexNet</strong></td><td>2012</td><td>딥 CNN의 가능성 입증- ReLU 활성화 함수 사용- Dropout 및 데이터 증강- 다중 GPU 병렬 학습</td><td>8 Layers</td><td>딥러닝 혁명의 시작. 이후 모든 비전 모델에 영감을 줌.</td></tr>
<tr><td><strong>VGGNet</strong></td><td>2014</td><td>깊이의 중요성 강조- 3x3의 작은 필터만 사용- 균일하고 단순한 구조</td><td>16-19 Layers</td><td>깊은 네트워크의 효용성을 증명하고, 많은 모델의 표준 ’백본’으로 사용됨.</td></tr>
<tr><td><strong>ResNet</strong></td><td>2015</td><td>극도로 깊은 네트워크 학습 가능- 잔차 학습(Residual Learning) 도입- 스킵 연결(Skip Connection) 구조</td><td>34-152+ Layers</td><td>’네트워크는 깊을수록 좋다’는 명제를 현실로 만듦. 현재 대부분의 SOTA 모델에서 사용되는 아키텍처.</td></tr>
</tbody></table>
<h4>4.2.1  AlexNet (2012): 돌파구의 순간</h4>
<p>AlexNet은 2012년 ILSVRC에서 기존의 컴퓨터 비전 접근법들을 압도적인 차이로 이기며 딥러닝의 실용성을 세상에 증명한 모델입니다.14</p>
<ul>
<li>
<p>구조적 혁신 13:</p>
</li>
<li>
<p><strong>ReLU 활성화 함수:</strong> 학습 속도를 6배까지 가속화하며 깊은 네트워크의 훈련을 가능하게 했습니다.113</p>
</li>
<li>
<p><strong>다중 GPU 활용:</strong> 당시 GPU 메모리의 한계를 극복하기 위해 모델을 두 개의 GPU에 나누어 병렬로 훈련시키는 독창적인 방법을 사용했습니다.114</p>
</li>
<li>
<p><strong>겹치는 풀링 (Overlapping Pooling):</strong> 풀링 윈도우의 보폭(stride)을 윈도우 크기보다 작게 설정하여 풀링 영역이 서로 겹치게 했습니다. 이는 정확도를 소폭 향상시키고 과적합(Overfitting)을 줄이는 효과가 있었습니다.13</p>
</li>
<li>
<p><strong>데이터 증강 및 Dropout:</strong> 제한된 훈련 데이터로 인한 과적합을 방지하기 위해 이미지 좌우 반전, 무작위 자르기, 색상 변화 등 광범위한 데이터 증강 기법을 사용했습니다.14 또한, 완전 연결 계층에</p>
</li>
</ul>
<p><strong>Dropout</strong>을 적용하여 훈련 중 무작위로 뉴런을 비활성화함으로써 모델의 일반화 성능을 높였습니다.113</p>
<h4>4.2.2  VGGNet (2014): 깊이와 단순함의 미학</h4>
<p>VGGNet은 네트워크의 ’깊이’가 성능 향상의 핵심 요소임을 명확히 보여준 모델입니다.101 그 구조의 단순함과 균일성 덕분에 이해하고 변형하기 쉬워 많은 후속 연구에서 ‘백본(backbone)’ 아키텍처로 널리 채택되었습니다.97</p>
<ul>
<li>구조적 혁신 118:</li>
<li><strong>깊이의 확장:</strong> AlexNet의 8개 층에서 16개 또는 19개 층으로 네트워크를 대폭 깊게 쌓았습니다.101</li>
<li><strong>3x3 필터의 균일한 사용:</strong> 네트워크 전체에 걸쳐 3x3이라는 매우 작은 크기의 컨볼루션 필터만을 사용했습니다. 이는 VGGNet의 핵심적인 통찰로, 예를 들어 3x3 필터를 두 번 연속으로 사용하는 것은 5x5 필터 한 번과 동일한 수용장(receptive field)을 가지면서도, 더 많은 비선형성(ReLU)을 도입하고 파라미터 수를 줄이는 이점이 있습니다.101 이로 인해 더 효율적이고 강력한 특징 학습이 가능해졌습니다.</li>
</ul>
<h4>4.2.3  ResNet (2015): 성능 저하 문제와 잔차 학습</h4>
<p>VGGNet이 깊이의 중요성을 보여줬지만, 무작정 층을 깊게 쌓자 ‘성능 저하(Degradation)’ 문제가 발생했습니다. 특정 깊이를 넘어서자 더 깊은 네트워크가 얕은 네트워크보다 훈련 에러가 더 높아지는 현상이 나타난 것입니다. 이는 과적합이 아니라, 깊은 네트워크의 최적화가 어렵기 때문에 발생하는 문제였습니다.121 ResNet(Residual Network)은 이 문제를 해결하며 152층이라는 전례 없는 깊이의 네트워크를 성공적으로 훈련시켰습니다.122</p>
<ul>
<li>구조적 혁신 (잔차 학습) 121:</li>
<li><strong>스킵 연결 (Skip/Shortcut Connection):</strong> ResNet의 핵심 아이디어는 하나 이상의 계층을 건너뛰는 ’지름길 연결’을 추가하는 것입니다. 네트워크가 입력 <code>x</code>를 받아 목표하는 출력 <code>H(x)</code>를 직접 학습하는 대신, **잔차(Residual)**인 <code>F(x) = H(x) - x</code>를 학습하도록 구조를 변경합니다. 그리고 계층의 최종 출력은 <code>F(x) + x</code>가 됩니다.126</li>
<li><strong>작동 원리:</strong> 만약 특정 계층에서 최적의 변환이 항등 함수(Identity Function, 즉 <code>H(x) = x</code>)라면, 기존 네트워크는 여러 비선형 계층을 통해 이 항등 함수를 근사해야 하는 어려운 문제를 풀어야 합니다. 하지만 ResNet 구조에서는 잔차 <code>F(x)</code>를 0으로 만드는 것만으로 쉽게 항등 변환을 구현할 수 있습니다. 이는 기울기 소실 문제를 완화하고, 네트워크가 불필요한 계층을 사실상 ‘건너뛸’ 수 있게 해주어 훨씬 더 깊은 구조의 학습을 가능하게 합니다.124</li>
<li><strong>병목 구조 (Bottleneck Architecture):</strong> 50층 이상의 매우 깊은 네트워크에서는 계산 효율성을 위해 ‘병목’ 블록이 사용됩니다. 이는 3x3 컨볼루션 앞뒤에 1x1 컨볼루션을 배치하여 채널 수를 줄였다가 다시 복원하는 구조로, 두 개의 3x3 컨볼루션을 사용하는 것보다 파라미터 수와 연산량을 크게 줄여줍니다.123</li>
</ul>
<h3>4.3  대표 아키텍처: 객체 탐지</h3>
<p>객체 탐지는 이미지 속 객체의 종류를 분류하는 것을 넘어, 그 위치까지 경계 상자로 정확히 찾아내야 하는 더 복잡한 작업입니다. 이 분야의 발전은 ’얼마나 정확하게’와 ’얼마나 빠르게’라는 두 가지 목표를 동시에 추구하는 방향으로 이루어졌습니다.</p>
<table><thead><tr><th>패러다임</th><th>대표 모델</th><th>핵심 방법론</th><th>속도</th><th>정확도</th><th>주요 장점</th></tr></thead><tbody>
<tr><td><strong>2단계 탐지기</strong></td><td>R-CNN, Fast R-CNN, <strong>Faster R-CNN</strong></td><td>1. 영역 제안 (Region Proposal)2. 제안된 영역 분류 (Classification)</td><td>상대적으로 느림</td><td>높음</td><td>작은 객체 탐지에 강하고, 위치 정확도가 높음.</td></tr>
<tr><td><strong>1단계 탐지기</strong></td><td><strong>YOLO</strong>, SSD</td><td>전체 이미지를 한 번에 처리하여 위치와 클래스를 동시에 회귀(Regression) 문제로 해결</td><td>매우 빠름 (실시간)</td><td>상대적으로 낮음 (초기 모델)</td><td>실시간 처리가 가능할 정도로 압도적인 속도.</td></tr>
</tbody></table>
<h4>4.3.1  R-CNN 계열: 영역에서 실시간으로</h4>
<p>R-CNN 계열은 “먼저 객체가 있을 만한 후보 영역을 찾고, 그 영역에 대해 분류를 수행하자“는 2단계(Two-stage) 접근법을 취합니다.</p>
<ul>
<li><strong>R-CNN (2014):</strong> 딥러닝을 객체 탐지에 성공적으로 적용한 첫 모델입니다. 하지만 ① 전통적인 <strong>선택적 탐색(Selective Search)</strong> 알고리즘으로 2000여 개의 후보 영역을 생성하고, (2) 각 영역을 CNN에 개별적으로 입력하여 특징을 추출한 뒤, (3) SVM으로 분류하는 다단계 파이프라인으로 인해 극도로 느렸습니다.128</li>
<li><strong>Fast R-CNN (2015):</strong> 속도를 크게 개선했습니다. 전체 이미지를 CNN에 단 한 번만 통과시켜 특징 맵을 얻고, 후보 영역을 이 특징 맵에 투영했습니다. <strong>RoI (Region of Interest) 풀링</strong>이라는 새로운 계층을 통해 각기 다른 크기의 후보 영역에서 고정된 크기의 특징 벡터를 추출하여, 분류와 경계 상자 회귀를 하나의 네트워크에서 동시에 처리했습니다.128 하지만 여전히 느린 선택적 탐색에 의존하는 병목 현상이 있었습니다.131</li>
<li><strong>Faster R-CNN (2015):</strong> 객체 탐지 분야의 기념비적인 모델로, 최초의 엔드-투-엔드 딥러닝 탐지기입니다. 가장 큰 혁신은 선택적 탐색을 **영역 제안 네트워크(Region Proposal Network, RPN)**라는 작은 신경망으로 대체한 것입니다.131 RPN은 주력 CNN이 추출한 특징 맵 위에서 작동하며, 다양한 크기와 종횡비를 가진 사전 정의된 **앵커 박스(Anchor Box)**들을 기반으로 객체가 있을 확률(objectness score)과 위치 보정 값을 빠르게 제안합니다.133 RPN과 Fast R-CNN 탐지기가 컨볼루션 계층을 공유함으로써, 전체 시스템이 하나의 통합된 네트워크로 훈련될 수 있게 되어 속도와 정확도를 모두 잡았습니다.132</li>
</ul>
<h4>4.3.2  YOLO (You Only Look Once): 새로운 철학</h4>
<p>YOLO는 2단계 접근법을 완전히 버리고, 객체 탐지를 “이미지 픽셀로부터 경계 상자 좌표와 클래스 확률을 직접 예측하는 단일 회귀(Regression) 문제“로 재정의했습니다.135 이 1단계(One-stage) 접근법은 압도적인 속도를 자랑하며 실시간 객체 탐지의 시대를 열었습니다.137</p>
<ul>
<li><strong>YOLOv1 핵심 아이디어:</strong></li>
</ul>
<ol>
<li>입력 이미지를 S x S 그리드로 나눕니다.135</li>
<li>객체의 중심이 특정 그리드 셀에 속하면, 그 셀이 해당 객체를 탐지할 책임이 있습니다.</li>
<li>각 그리드 셀은 B개의 경계 상자와 각 상자에 대한 신뢰도 점수(confidence score), 그리고 C개의 클래스 확률을 예측합니다.135</li>
<li>이 모든 과정이 단일 CNN의 한 번의 순전파(forward pass)로 이루어집니다.139</li>
</ol>
<ul>
<li><strong>YOLOv3의 발전:</strong> 초기 YOLO는 작은 객체 탐지에 약점이 있었으나, YOLOv3는 다음과 같은 개선을 통해 정확도를 크게 높였습니다.</li>
<li><strong>Darknet-53:</strong> ResNet과 유사하게 잔차 연결을 포함하는 더 깊고 강력한 53계층의 백본 네트워크를 도입했습니다.140</li>
<li><strong>다중 스케일 예측:</strong> 특징 피라미드 네트워크(FPN)와 유사하게, 3개의 다른 스케일(해상도)에서 특징 맵을 추출하여 예측을 수행합니다. 이를 통해 다양한 크기의 객체, 특히 작은 객체에 대한 탐지 성능이 향상되었습니다.139</li>
<li><strong>독립적인 로지스틱 분류기:</strong> 클래스 예측에 단일 소프트맥스를 사용하는 대신, 각 클래스에 대해 독립적인 로지스틱 분류기를 사용하여 하나의 객체가 여러 개의 레이블을 가질 수 있도록 했습니다(예: ’여성’과 ‘사람’).141</li>
</ul>
<h3>4.4  대표 아키텍처: 이미지 분할</h3>
<p>이미지 분할은 모든 픽셀에 레이블을 할당하는 가장 정교한 작업입니다. 이 분야의 발전은 “어떻게 고수준의 의미 정보(semantic information)와 저수준의 정밀한 위치 정보(spatial information)를 효과적으로 결합할 것인가?“라는 질문에 대한 답을 찾아가는 과정이었습니다.</p>
<h4>4.4.1  U-Net (2015): 생물의학 이미지를 위한 인코더-디코더와 스킵 연결</h4>
<p>U-Net은 특히 생물의학 이미지 분할 분야에서 적은 양의 데이터로도 매우 정밀한 결과를 보여주며 혁명을 일으켰습니다.142 그 이름은 아키텍처의 U자 형태에서 유래했습니다.</p>
<ul>
<li>아키텍처 144:</li>
<li><strong>수축 경로 (Contracting Path, 인코더):</strong> 일반적인 CNN처럼 컨볼루션과 최대 풀링을 반복하여 이미지의 특징을 추출하고 문맥(context) 정보를 포착합니다. 이 과정에서 특징 맵의 공간적 해상도는 점차 줄어들고 채널 수는 늘어납니다. 즉, ’무엇(what)’에 대한 정보는 강화되지만 ’어디(where)’에 대한 정보는 손실됩니다.</li>
<li><strong>확장 경로 (Expansive Path, 디코더):</strong> <strong>상향 컨볼루션(Up-convolution)</strong> 또는 **전치 컨볼루션(Transposed Convolution)**을 사용하여 특징 맵의 해상도를 점차 복원합니다.</li>
<li><strong>핵심 혁신 (스킵 연결, Skip Connections):</strong> U-Net의 가장 중요한 특징입니다. 수축 경로의 각 단계에서 생성된 특징 맵을 확장 경로의 동일한 해상도를 가진 층에 직접 **연결(concatenation)**합니다.144 이 지름길은 인코더에서 손실되었던 정밀한 위치 정보를 디코더에 직접 전달해줍니다. 덕분에 네트워크는 디코더의 깊은 층에서 얻은 고수준의 의미 정보와 인코더에서 온 저수준의 공간 정보를 결합하여 매우 정확한 경계선을 가진 분할 맵을 생성할 수 있습니다.145</li>
</ul>
<h4>4.4.2  DeepLab (v1-v3+): 아트러스 컨볼루션과 공간 피라미드 풀링</h4>
<p>DeepLab 계열 모델들은 의미론적 분할(Semantic Segmentation) 분야에서 SOTA(State-of-the-art) 성능을 이끌어 온 구글의 대표적인 아키텍처입니다.149 이 모델들은 다중 스케일 객체를 효과적으로 처리하고 높은 해상도를 유지하기 위한 새로운 기법들을 제시했습니다.</p>
<ul>
<li><strong>핵심 혁신:</strong></li>
<li><strong>아트러스 컨볼루션 (Atrous / Dilated Convolution):</strong> ‘구멍이 있는’ 컨볼루션으로, 필터 내부에 0을 삽입(dilation)하여 실제 필터 크기나 파라미터 수를 늘리지 않고도 수용장(receptive field)을 효과적으로 확장하는 기법입니다.150 딜레이션 비율(dilation rate)을 조절함으로써, 풀링으로 인한 해상도 손실 없이 다양한 스케일의 문맥 정보를 포착할 수 있습니다.153</li>
<li><strong>아트러스 공간 피라미드 풀링 (Atrous Spatial Pyramid Pooling, ASPP):</strong> 동일한 특징 맵에 대해 서로 다른 딜레이션 비율을 가진 여러 개의 아트러스 컨볼루션을 병렬로 적용하고, 추가로 전역 평균 풀링(Global Average Pooling)을 통해 이미지 전체의 문맥 정보를 추출합니다. 이렇게 다양한 스케일에서 얻은 특징들을 모두 결합하여, 크기가 다른 객체들을 강건하게 분할할 수 있는 풍부한 특징 표현을 만들어냅니다.149</li>
<li><strong>DeepLabv3+:</strong> DeepLabv3의 인코더 구조에 U-Net과 유사한 간단한 디코더 모듈을 추가하여, 분할 결과, 특히 객체 경계선의 품질을 더욱 향상시켰습니다.154</li>
</ul>
<p>이러한 딥러닝 아키텍처들의 발전 과정을 살펴보면, 컴퓨터 비전의 진화가 몇 가지 핵심적인 아이디어의 연속임을 알 수 있습니다. 첫째, <strong>‘백본과 헤드’ 패러다임</strong>입니다. ResNet과 같은 강력한 분류 모델이 범용적인 특징 추출기(백본) 역할을 하고, 그 위에 특정 작업을 위한 모듈(헤드, 예: RPN, ASPP)을 얹는 방식은 전이 학습의 효율성을 극대화했습니다. 둘째, <strong>공간 정보와의 싸움</strong>입니다. U-Net의 스킵 연결과 DeepLab의 아트러스 컨볼루션은 모두 다운샘플링으로 인한 공간 정보 손실 문제를 해결하기 위한 독창적인 해법입니다. 셋째, <strong>통합과 효율성</strong>입니다. 객체 탐지의 역사는 여러 개의 느린 파이프라인을 하나의 빠르고 통합된 네트워크로 만들어가는 과정 그 자체였습니다. 이러한 핵심 원리들을 이해하는 것은 새로운 아키텍처를 학습하고 자신만의 모델을 설계하는 데 중요한 기반이 될 것입니다.</p>
<h2>5.  고급 아키텍처와 생성형 AI의 부상</h2>
<p>CNN의 성공을 넘어, 컴퓨터 비전은 이제 새로운 아키텍처와 패러다임을 받아들이고 있습니다. 자연어 처리 분야를 평정한 트랜스포머가 비전 분야에 적용되기 시작했고, 단순히 이미지를 이해하는 것을 넘어 새로운 이미지를 창조하는 생성형 AI 분야가 폭발적으로 성장하고 있습니다. 이 파트에서는 컴퓨터 비전의 현재와 미래를 이끄는 두 가지 핵심 동력, 비전 트랜스포머와 생성 모델(GAN, 디퓨전)을 탐구합니다.</p>
<h3>5.1  컨볼루션을 넘어서: 비전 트랜스포머 (ViT)</h3>
<p>CNN은 이미지의 지역성(locality, 인접 픽셀 간의 관계가 중요함)과 이동 등변성(translation equivariance, 객체의 위치가 바뀌어도 동일하게 인식함)이라는 강력한 ’귀납적 편향(inductive bias)’을 내장하고 있어 이미지 처리에 매우 효율적입니다.155 반면, 자연어 처리(NLP)에서 혁명을 일으킨 트랜스포머(Transformer)는 이러한 편향 없이, 셀프 어텐션(Self-Attention) 메커니즘을 통해 데이터의 모든 부분 간의 전역적인 관계(global relationship)를 학습하는 데 탁월합니다. 비전 트랜스포머(Vision Transformer, ViT)는 “이미지 고유의 편향을 최소화하고 순수한 트랜스포머 구조를 비전에 적용할 수 있을까?“라는 질문에서 출발한 모델입니다.155</p>
<ul>
<li>아키텍처 156:</li>
</ul>
<ol>
<li><strong>이미지를 패치 시퀀스로 변환:</strong> ViT의 핵심 아이디어는 2D 이미지를 1D 시퀀스로 변환하는 것입니다. 이를 위해 이미지를 바둑판처럼 일정한 크기(예: 16x16 픽셀)의 **패치(patch)**들로 나눕니다.</li>
<li><strong>패치 임베딩:</strong> 각 패치를 1차원 벡터로 펼친(flatten) 후, 학습 가능한 선형 투영(linear projection)을 통해 고정된 차원의 벡터, 즉 **‘토큰(token)’**으로 변환합니다. 이것이 트랜스포머의 입력이 됩니다.</li>
<li><strong>위치 임베딩:</strong> 트랜스포머는 순서의 개념이 없기 때문에, 각 패치의 원래 위치 정보를 알려주기 위해 학습 가능한 **위치 임베딩(positional embedding)**을 각 패치 토큰에 더해줍니다.</li>
<li>** 토큰:** BERT 모델에서 영감을 받아, 패치 시퀀스의 맨 앞에 특별한 **<code>(분류) 토큰**을 추가합니다. 트랜스포머 인코더를 통과한 후, 이</code> 토큰에 해당하는 최종 출력 벡터가 전체 이미지를 대표하는 특징으로 사용되어 분류 작업에 활용됩니다.</li>
<li><strong>트랜스포머 인코더:</strong> 이렇게 준비된 토큰 시퀀스는 표준 트랜스포머 인코더로 입력됩니다. 인코더는 **다중 헤드 셀프 어텐션(Multi-Head Self-Attention, MSA)**과 MLP 블록이 번갈아 가며 쌓인 구조로, 셀프 어텐션을 통해 모든 패치가 다른 모든 패치와 상호작용하며 이미지 전체의 전역적인 문맥을 파악합니다.158</li>
</ol>
<ul>
<li><strong>성능과 트레이드오프:</strong></li>
<li><strong>데이터 요구량:</strong> ViT는 CNN의 내장된 편향이 없기 때문에, 상대적으로 적은 데이터로 학습할 경우 CNN보다 성능이 떨어지는 경향이 있습니다. 이미지의 기본적인 구조를 처음부터 데이터만으로 학습해야 하기 때문입니다. 하지만, 이미지넷-21k(1,400만 장)나 JFT-300M(3억 장)과 같은 초거대 데이터셋으로 사전 훈련했을 때, ViT는 기존의 최고 성능 CNN 모델들을 능가하는 결과를 보여주었습니다.156 이는 충분한 데이터가 주어진다면, 모델이 귀납적 편향을 데이터로부터 직접 학습하는 것이 더 효과적일 수 있음을 시사합니다.</li>
<li><strong>하이브리드 아키텍처:</strong> CNN과 트랜스포머의 장점을 결합하려는 시도도 많습니다. CNN을 특징 추출기(feature extractor)로 사용하여 저수준의 지역적 특징을 효율적으로 뽑아낸 뒤, 그 결과로 나온 특징 맵을 트랜스포머에 입력하여 전역적인 관계를 모델링하는 하이브리드 방식은 더 적은 데이터로도 좋은 성능을 낼 수 있습니다.158</li>
</ul>
<h3>5.2  생성적 적대 신경망 (GANs)</h3>
<p>생성적 적대 신경망(Generative Adversarial Networks, GANs)은 2014년 이안 굿펠로우(Ian Goodfellow)에 의해 제안된 혁신적인 생성 모델 프레임워크입니다. GAN은 단순히 데이터를 분류하거나 예측하는 것을 넘어, 훈련 데이터와 유사한 새로운 데이터를 ’창조’하는 것을 목표로 합니다.</p>
<h4>5.2.1  적대적 게임: 생성자와 판별자</h4>
<p>GAN의 핵심은 **생성자(Generator, G)**와 **판별자(Discriminator, D)**라는 두 개의 신경망이 서로 경쟁하며 학습하는 ’적대적 과정(adversarial process)’에 있습니다.159 이 관계는 종종 위조지폐범과 경찰에 비유됩니다.</p>
<ul>
<li><strong>생성자 (Generator):</strong> ’위조지폐범’의 역할을 합니다. 잠재 공간(latent space)에서 추출한 무작위 노이즈 벡터 <code>z</code>를 입력으로 받아, 실제 데이터(예: 실제 사람 얼굴 이미지)와 구별하기 어려운 가짜 데이터를 생성하는 것이 목표입니다.159 생성자는 판별자를 속이는 방향으로 학습합니다.</li>
<li><strong>판별자 (Discriminator):</strong> ’경찰’의 역할을 합니다. 입력된 데이터가 훈련 데이터셋에서 온 ’진짜’인지, 아니면 생성자가 만든 ’가짜’인지를 판별하는 이진 분류기입니다.159 판별자는 진짜와 가짜를 더 정확하게 구별하는 방향으로 학습합니다.</li>
<li><strong>학습 과정:</strong></li>
</ul>
<ol>
<li>판별자 훈련: 실제 이미지(레이블 1)와 생성자가 만든 가짜 이미지(레이블 0)를 판별자에 보여주고, 이를 잘 구별하도록 가중치를 업데이트합니다. 이 단계에서는 생성자의 가중치는 고정됩니다.160</li>
<li>생성자 훈련: 생성자는 노이즈를 입력받아 가짜 이미지를 생성하고, 이를 판별자에 통과시킵니다. 생성자는 판별자가 이 가짜 이미지를 ‘진짜’(레이블 1)라고 판단하도록, 즉 판별자를 속이도록 자신의 가중치를 업데이트합니다. 이 단계에서는 판별자의 가중치가 고정됩니다.160</li>
</ol>
<p>이 두 과정을 반복하면, 생성자는 점점 더 정교한 이미지를 만들고, 판별자는 점점 더 예리하게 가짜를 구별하게 됩니다. 이상적으로 이 게임은 **내시 균형(Nash Equilibrium)**에 도달하는데, 이때 생성된 이미지는 실제와 구별할 수 없게 되고, 판별자는 어떤 이미지가 들어와도 진짜일 확률을 0.5로 판단하게 됩니다.159</p>
<h4>5.2.2  학습의 어려움과 응용</h4>
<ul>
<li><strong>학습의 어려움:</strong></li>
<li><strong>모드 붕괴 (Mode Collapse):</strong> GAN 학습에서 가장 흔하게 발생하는 문제입니다. 생성자가 판별자를 속이기 가장 쉬운 몇 가지 소수의 결과물만 반복적으로 생성하고, 훈련 데이터의 전체적인 다양성을 학습하지 못하는 현상입니다.163 예를 들어, 다양한 얼굴을 생성해야 하는데 특정 각도의 특정 얼굴 형태만 계속 만들어내는 경우입니다.</li>
<li><strong>불안정한 학습:</strong> 생성자와 판별자 간의 균형을 맞추기가 매우 어려워 학습 과정이 불안정하고 수렴하지 않는 경우가 많습니다.166</li>
<li><strong>응용 분야:</strong></li>
<li><strong>이미지 생성:</strong> 세상에 존재하지 않는 고해상도의 사실적인 이미지를 생성합니다.167</li>
<li><strong>이미지-투-이미지 변환 및 스타일 전이:</strong> CycleGAN과 같은 모델을 사용하여 사진을 반 고흐 풍의 그림으로 바꾸거나, 여름 풍경을 겨울 풍경으로 변환하는 등 이미지의 스타일을 바꿉니다.168</li>
<li><strong>데이터 증강:</strong> 부족한 훈련 데이터를 보충하기 위해 실제와 유사한 합성 데이터를 생성합니다. 특히 의료 영상이나 불량품 데이터처럼 희귀한 데이터를 증강하는 데 유용합니다.170</li>
</ul>
<h3>5.3  디퓨전 모델: 이미지 생성의 새로운 시대</h3>
<p>최근 몇 년간, 디퓨전 모델(Diffusion Models)은 GAN을 능가하는 고품질 이미지 생성 능력으로 생성형 AI 분야의 새로운 강자로 떠올랐습니다.172 이 모델은 비평형 열역학에서 영감을 받은 독특한 방식으로 작동합니다.173</p>
<h4>5.3.1  순방향 및 역방향 프로세스: 노이즈를 더하고 제거하기</h4>
<p>디퓨전 모델은 두 가지 핵심 프로세스로 구성됩니다.173</p>
<ul>
<li>
<p><strong>순방향 프로세스 (Forward Process / Diffusion Process):</strong> 이 과정은 고정되어 있으며 학습이 필요 없습니다. 원본 이미지에서 시작하여, 정해진 타임스텝 <code>T</code> (예: 1000)에 걸쳐 점진적으로 가우시안 노이즈를 아주 조금씩 추가합니다. <code>T</code> 스텝이 지나면 이미지는 완전히 순수한 노이즈로 변환됩니다.175</p>
</li>
<li>
<p><strong>역방향 프로세스 (Reverse Process / Denoising Process):</strong> 이 과정이 바로 모델이 학습하는 부분입니다. 순수한 노이즈 이미지에서 시작하여, 순방향 프로세스를 거꾸로 거슬러 올라가며 각 스텝에서 추가되었던 노이즈를 예측하고 제거합니다.173 모델(주로 U-Net 아키텍처 사용)은 특정 타임스텝의 노이즈 낀 이미지를 입력받아, 그 이전 스텝의 조금 더 깨끗한 이미지를 예측하도록 훈련됩니다. 이 과정을</p>
</li>
</ul>
<p><code>T</code>번 반복하면 최종적으로 깨끗한 이미지가 생성됩니다.</p>
<h4>5.3.2  디퓨전 모델 vs. GANs: 비교 분석</h4>
<table><thead><tr><th>특징</th><th>생성적 적대 신경망 (GANs)</th><th>디퓨전 모델 (Diffusion Models)</th><th>승자 / 맥락</th></tr></thead><tbody>
<tr><td><strong>핵심 원리</strong></td><td>생성자와 판별자의 적대적 게임</td><td>점진적 노이즈 제거 (Denoising)</td><td>-</td></tr>
<tr><td><strong>학습 안정성</strong></td><td>불안정하며, 수렴이 어려움 166</td><td>매우 안정적임 166</td><td><strong>디퓨전 모델</strong></td></tr>
<tr><td><strong>샘플 품질 및 다양성</strong></td><td>모드 붕괴(Mode Collapse) 현상으로 다양성이 부족할 수 있음 166</td><td>일반적으로 더 높은 품질과 뛰어난 다양성을 보임 166</td><td><strong>디퓨전 모델</strong></td></tr>
<tr><td><strong>샘플링 속도 (추론)</strong></td><td><strong>매우 빠름</strong> (단일 순전파)</td><td><strong>매우 느림</strong> (수백~수천 번의 반복적인 Denoising 스텝 필요)</td><td><strong>GANs</strong> (실시간 생성이 필요할 경우)</td></tr>
<tr><td><strong>제어 가능성</strong></td><td>제어가 상대적으로 어려움 (cGAN 등으로 일부 가능)</td><td>텍스트 프롬프트 등을 통한 정교한 제어가 용이함</td><td><strong>디퓨전 모델</strong></td></tr>
</tbody></table>
<h4>5.3.3  대표 모델: DALL-E 2와 스테이블 디퓨전</h4>
<p>디퓨전 모델의 잠재력은 텍스트 프롬프트를 이미지로 변환하는 <strong>텍스트-투-이미지(Text-to-Image)</strong> 모델에서 폭발했습니다. 이 모델들은 강력한 언어-이미지 모델(예: CLIP)과 디퓨전 모델을 결합합니다.177</p>
<ul>
<li><strong>DALL-E 2 (OpenAI):</strong> 텍스트 프롬프트를 기반으로 CLIP 이미지 임베딩을 생성하는 ’사전 모델(prior)’과, 이 임베딩을 조건으로 이미지를 생성하는 ‘디코더(decoder)’ 디퓨전 모델로 구성된 2단계 구조를 가집니다.177 1024x1024의 고해상도 이미지를 생성하며, 창의적이고 예술적인 스타일의 결과물로 유명합니다.180</li>
<li><strong>스테이블 디퓨전 (Stability AI):</strong> 가장 큰 혁신은 **잠재 디퓨전 모델(Latent Diffusion Model, LDM)**이라는 점입니다.178 고차원의 픽셀 공간에서 직접 디퓨전 과정을 수행하는 대신, 먼저 오토인코더(Autoencoder)를 사용해 이미지를 훨씬 작은 크기의 **잠재 공간(latent space)**으로 압축합니다. 모든 노이즈 추가 및 제거 과정이 이 효율적인 잠재 공간에서 이루어지고, 마지막에만 디코더를 통해 완전한 이미지로 복원됩니다.178 이 방식은 계산 효율성을 크게 높였으며, 오픈소스로 공개되어 커뮤니티에 의해 빠르게 발전하고 있습니다. 특히 사실적이고 디테일이 풍부한 이미지 생성에 강점을 보입니다.180</li>
</ul>
<p>이러한 고급 아키텍처의 등장은 컴퓨터 비전의 패러다임이 다시 한번 변화하고 있음을 보여줍니다. ViT는 CNN의 ’귀납적 편향’이 절대적인 것이 아니라, 데이터의 양에 따라 학습될 수 있는 하나의 ’선택’임을 보여주었습니다. 생성 모델, 특히 텍스트-투-이미지 디퓨전 모델은 단순히 이미지를 인식하는 것을 넘어, 언어라는 추상적인 개념과 시각 세계를 연결하는 ’세계 모델(World Model)’을 구축하는 방향으로 나아가고 있습니다. 이는 컴퓨터 비전이 단순한 패턴 인식을 넘어, 진정한 의미의 ’이해’와 ’창조’로 나아가는 중요한 이정표입니다.</p>
<h2>6.  컴퓨터 비전의 최전선</h2>
<p>이 파트에서는 현재 컴퓨터 비전 연구의 가장 활발한 분야들을 탐구합니다. 이 주제들은 기존의 지도 학습과 2차원 이미지 이해의 한계를 넘어서, 데이터 효율성, 3차원 공간 인식, 그리고 프라이버시 문제와 같은 현실 세계의 복잡한 요구사항에 대응하기 위한 노력의 결과물입니다. 이 분야들을 이해하는 것은 미래의 컴퓨터 비전 기술이 나아갈 방향을 예측하는 데 중요합니다.</p>
<h3>6.1  레이블 없는 데이터로부터의 학습: 자기 지도 학습 (SSL)</h3>
<p>딥러닝 모델, 특히 대규모 모델의 성능은 방대한 양의 레이블링된 데이터에 크게 의존합니다. 그러나 고품질의 레이블을 만드는 작업은 시간과 비용이 많이 드는 병목 현상을 유발합니다.183 **자기 지도 학습(Self-Supervised Learning, SSL)**은 이러한 한계를 극복하기 위해, 레이블이 없는 데이터 자체의 구조를 활용하여 모델이 스스로 감독 신호(supervisory signal)를 생성하고 학습하는 패러다임입니다.185</p>
<ul>
<li><strong>핵심 아이디어:</strong> SSL은 “사전 텍스트 과제(pretext task)“라는 인위적인 문제를 설정합니다. 모델은 이 과제를 해결하는 과정에서 데이터의 본질적인 구조와 의미론적 표현(representation)을 학습하게 됩니다. 이렇게 사전 훈련된 모델은 이후 소량의 레이블링된 데이터만으로 특정 ‘다운스트림 과제(downstream task)’(예: 이미지 분류, 객체 탐지)에 미세 조정(fine-tuning)되어 높은 성능을 발휘할 수 있습니다.184</li>
<li><strong>주요 SSL 기법:</strong></li>
<li><strong>대조 학습 (Contrastive Learning):</strong> 이 방법은 ‘비슷한 것은 가깝게, 다른 것은 멀게’ 만드는 것을 목표로 합니다. 예를 들어, SimCLR나 MoCo와 같은 모델에서는 하나의 이미지에 대해 서로 다른 방식의 데이터 증강(augmentation, 예: 자르기, 색상 변경, 회전)을 적용하여 두 개의 ’긍정 쌍(positive pair)’을 만듭니다. 모델은 이 긍정 쌍의 표현(임베딩 벡터)은 임베딩 공간에서 서로 가깝게 만들고, 다른 이미지에서 온 ’부정 쌍(negative pair)’의 표현과는 멀어지도록 학습합니다.185 이 과정을 통해 모델은 이미지의 고유한 의미적 특징을 학습하게 됩니다.</li>
<li><strong>마스크 모델링 (Masked Modeling):</strong> NLP 분야의 BERT에서 영감을 받은 기법입니다. MAE(Masked Autoencoders)와 같은 모델은 입력 이미지의 일부 패치를 무작위로 가리고(masking), 모델이 가려진 부분을 나머지 보이는 부분의 맥락을 통해 복원하도록 학습시킵니다.184 이미지를 성공적으로 복원하기 위해서는 모델이 객체의 형태나 질감과 같은 이미지의 전체적인 구조를 이해해야만 합니다.</li>
<li><strong>비-대조 학습 (Non-Contrastive Learning):</strong> 대조 학습이 효과적이려면 많은 수의 부정 쌍이 필요하여 큰 배치 사이즈가 요구되는 단점이 있습니다. 비-대조 학습은 오직 긍정 쌍만을 사용하여 학습하면서도, 아키텍처에 특별한 장치(예: 예측기 헤드, 기울기 전파 차단)를 두어 모델이 모든 출력을 동일하게 만드는 ‘붕괴(collapse)’ 현상을 방지합니다.187</li>
<li><strong>영향:</strong> SSL은 인터넷에 존재하는 방대한 양의 레이블 없는 시각적 데이터를 활용할 수 있는 길을 열었습니다. 이는 데이터 레이블링에 대한 의존도를 크게 낮추고, 더 강건하고 일반화 성능이 뛰어난 ’파운데이션 모델(Foundation Models)’을 구축하는 핵심 기술로 자리 잡고 있습니다.183</li>
</ul>
<h3>6.2  새로운 뷰 합성: 뉴럴 래디언스 필드 (NeRF)</h3>
<p>**뉴럴 래디언스 필드(Neural Radiance Fields, NeRF)**는 여러 각도에서 촬영된 2D 이미지들로부터 해당 장면의 사실적인 3D 뷰를 새롭게 생성해내는 획기적인 기술입니다.188 이는 컴퓨터 비전과 컴퓨터 그래픽스의 경계를 허무는 기술로 평가받습니다.</p>
<ul>
<li><strong>핵심 개념:</strong> NeRF는 3D 장면 자체를 하나의 연속적인 함수, 즉 **뉴럴 네트워크(주로 MLP)**로 표현합니다. 이 네트워크는 ’래디언스 필드’라고 불리며, 5차원 입력(3D 공간 좌표 <code>(x, y, z)</code>와 2D 시선 방향 <code>(θ, φ)</code>)을 받아 해당 지점의 **색상(color)**과 **밀도(volume density)**를 출력합니다.189</li>
<li><strong>렌더링 과정:</strong> 새로운 시점에서 이미지를 생성하기 위해, 가상의 카메라 위치에서 장면을 향해 수많은 광선(ray)을 쏩니다. 각 광선을 따라 여러 지점을 샘플링하고, 이 지점들의 5D 좌표를 NeRF 네트워크에 입력하여 색상과 밀도 값을 얻습니다. 고전적인 <strong>볼륨 렌더링(volume rendering)</strong> 기법을 사용하여 이 값들을 광선을 따라 적분하면, 최종적으로 해당 픽셀의 색상이 결정됩니다.189</li>
<li><strong>훈련:</strong> 전체 렌더링 과정은 미분 가능(differentiable)하므로, 렌더링된 이미지와 실제 촬영된 입력 이미지 간의 차이(손실)를 최소화하는 방향으로 네트워크의 가중치를 최적화할 수 있습니다. 이를 통해 네트워크는 여러 뷰에 걸쳐 일관성 있는 3D 장면 표현을 학습하게 됩니다.189</li>
<li><strong>발전과 응용:</strong> 초기 NeRF는 렌더링과 훈련 속도가 느리고, 해상도가 다른 뷰에 대해 에일리어싱(aliasing) 문제가 있었습니다. <strong>Mip-NeRF</strong>는 광선 대신 원뿔(cone)을 추적하는 방식으로 이 문제를 해결하여 더 선명하고 안정적인 결과를 얻었습니다.191 NeRF 기술은 가상현실(VR), 증강현실(AR), 시각 효과(VFX), 3D 모델링, 로보틱스, 의료 영상 등에서 무한한 잠재력을 보여주고 있습니다.189</li>
</ul>
<h3>6.3  프라이버시 보존 컴퓨터 비전</h3>
<p>컴퓨터 비전 기술이 의료, 금융, 개인용 기기 등 민감한 데이터를 다루는 영역으로 확장되면서, 데이터 프라이버시 보호는 기술적 성능만큼이나 중요한 과제가 되었습니다. 중앙 서버로 모든 데이터를 전송하여 학습하는 전통적인 방식은 심각한 프라이버시 침해 위험을 내포하고 있습니다.192</p>
<ul>
<li><strong>연합 학습 (Federated Learning, FL):</strong> 데이터를 중앙 서버로 옮기지 않고, 데이터가 생성된 위치(예: 개인 스마트폰, 병원 서버)에서 모델을 훈련하는 분산 학습 패러다임입니다.192</li>
<li><strong>과정:</strong> 중앙 서버가 초기 모델을 각 클라이언트 기기로 전송합니다. 각 클라이언트는 자신의 로컬 데이터로 모델을 학습시킨 뒤, 원본 데이터가 아닌 학습된 모델의 업데이트 정보(가중치 또는 기울기)만을 암호화하여 서버로 다시 보냅니다. 서버는 여러 클라이언트로부터 받은 업데이트를 종합(예: <strong>Federated Averaging</strong>)하여 전역 모델(global model)을 개선하고, 이를 다시 클라이언트에 배포합니다. 이 과정이 반복됩니다.192</li>
<li><strong>장점:</strong> 원본 데이터가 기기 외부로 유출되지 않으므로 프라이버시를 근본적으로 강화할 수 있습니다.193</li>
<li><strong>차분 프라이버시 (Differential Privacy, DP):</strong> 데이터 분석 결과로부터 특정 개인의 정보가 노출되는 것을 수학적으로 방지하는 강력한 프라이버시 보장 프레임워크입니다.196</li>
<li><strong>핵심 아이디어:</strong> 데이터셋에 특정 개인의 데이터가 포함되든 안 되든, 알고리즘의 최종 출력 결과가 통계적으로 거의 동일하게 유지되도록 만듭니다. 이는 훈련 과정이나 결과물에 신중하게 보정된 **노이즈(noise)**를 주입함으로써 달성됩니다.196 딥러닝에서는 주로 각 데이터 샘플이 모델 업데이트에 미치는 영향을 제한(기울기 클리핑)하고, 종합된 기울기에 노이즈를 추가하는 방식으로 구현됩니다.197</li>
<li><strong>트레이드오프:</strong> 프라이버시 보호 수준(더 많은 노이즈)과 모델의 유용성/정확도(더 적은 노이즈) 사이에는 본질적인 상충 관계가 존재합니다.199</li>
<li><strong>동형 암호 (Homomorphic Encryption, HE):</strong> 데이터를 암호화된 상태 그대로 연산할 수 있게 하는 최첨단 암호 기술입니다.200</li>
<li><strong>응용:</strong> 사용자는 자신의 민감한 이미지(예: 의료 영상)를 암호화하여 클라우드 서버로 보냅니다. 서버는 암호화된 데이터를 복호화하지 않고 그대로 AI 모델에 입력하여 추론을 수행하고, 암호화된 결과값을 사용자에게 반환합니다. 최종 결과는 개인키를 가진 사용자만이 복호화할 수 있습니다.202</li>
<li><strong>한계:</strong> 이론적으로는 완벽한 프라이버시를 제공하지만, 현재 기술로는 연산 비용이 극도로 높아 실용성이 제한됩니다. 또한, ReLU와 같은 비선형 활성화 함수를 다항식으로 근사하는 등 모델 아키텍처의 수정이 필요합니다.202</li>
</ul>
<p>이러한 최전선 기술들은 컴퓨터 비전이 직면한 근본적인 도전 과제들을 해결하려는 노력의 산물입니다. SSL은 데이터 부족 문제를, NeRF는 2D에서 3D로의 도약을, 그리고 프라이버시 보존 기술들은 데이터가 자원이자 동시에 잠재적 책임(liability)이 된 시대의 요구를 반영합니다. 이러한 발전은 기술이 단순히 정확도를 높이는 것을 넘어, 더 효율적이고, 더 현실적이며, 더 책임감 있는 방향으로 진화하고 있음을 보여줍니다.</p>
<h2>7.  더 넓은 맥락: 연구, 윤리, 그리고 미래</h2>
<p>컴퓨터 비전 기술을 마스터하는 것은 단순히 알고리즘과 코드를 배우는 것을 넘어, 이 기술이 작동하는 더 넓은 학문적, 사회적, 윤리적 맥락을 이해하는 것을 포함합니다. 이 마지막 파트에서는 학습자가 전문가로서 성장하기 위해 필요한 지식, 즉 최신 연구 동향을 파악하는 방법, 기술에 내재된 윤리적 책임을 인식하는 방법, 그리고 미래의 기술 발전 방향을 예측하는 통찰력을 제공합니다.</p>
<h3>7.1  연구 동향 파악하기: CVPR, ICCV, ECCV 항해법</h3>
<p>컴퓨터 비전 분야는 매우 빠르게 발전하고 있으며, 최신 기술과 아이디어는 주로 세계적인 학술대회(Conference)를 통해 발표됩니다. 이 학회들의 동향을 따라가는 것은 현재 연구의 최전선이 어디인지를 파악하는 가장 좋은 방법입니다.</p>
<ul>
<li><strong>컴퓨터 비전 3대 학회:</strong> 이 세 학회는 컴퓨터 비전 분야에서 가장 권위 있고 영향력 있는 학회로 인정받습니다.203</li>
<li><strong>CVPR (Conference on Computer Vision and Pattern Recognition):</strong> 매년 개최되며, 일반적으로 가장 규모가 크고 경쟁이 치열한 최고의 학회로 꼽힙니다.203</li>
<li><strong>ICCV (International Conference on Computer Vision):</strong> 2년마다 개최되며, CVPR과 함께 최고 수준의 학회로 평가받습니다.203</li>
<li><strong>ECCV (European Conference on Computer Vision):</strong> 2년마다 ICCV와 번갈아 개최되는 유럽 최고의 컴퓨터 비전 학회입니다.203</li>
</ul>
<table><thead><tr><th>학회명</th><th>주기</th><th>위상 및 특징</th></tr></thead><tbody>
<tr><td><strong>CVPR</strong></td><td>매년</td><td>세계 최고 수준의 컴퓨터 비전 학회. 가장 많은 논문이 발표되며, 산업계와 학계의 참여가 가장 활발함.</td></tr>
<tr><td><strong>ICCV</strong></td><td>2년</td><td>CVPR과 동급의 최상위 학회. 이론과 실제 응용을 아우르는 깊이 있는 연구들이 발표됨.</td></tr>
<tr><td><strong>ECCV</strong></td><td>2년</td><td>유럽을 대표하는 최상위 학회. CVPR, ICCV와 함께 3대 학회로 불리며 높은 수준의 연구를 다룸.</td></tr>
</tbody></table>
<ul>
<li><strong>최신 연구 동향 (2024-2025년 학회 기준):</strong></li>
<li><strong>생성형 AI와 디퓨전 모델:</strong> 모델의 효율성을 높이고(Adversarial Diffusion Distillation 205), 제어 가능성을 향상시키며(ControlNet 206), 텍스트-투-비디오 206나 3D 생성 205과 같은 새로운 영역으로 확장하는 연구가 주를 이룹니다.</li>
<li><strong>3D 비전과 체화된 AI (Embodied AI):</strong> NeRF와 같은 3D 재구성 기술의 발전과 함께, 로봇이나 자율주행차와 같이 물리적 세계와 상호작용하는 에이전트를 위한 비전 기술 연구가 큰 주목을 받고 있습니다.207</li>
<li><strong>비전-언어 모델 (Vision-Language Models, VLMs):</strong> 대규모 언어 모델(LLM)을 비전과 통합하여, 이미지를 단순 분류하는 것을 넘어 텍스트로 설명하고, 이미지에 대한 질문에 답하며, 텍스트 지시로 이미지를 편집하는 등 고차원적인 이해를 추구하는 연구가 활발합니다.210</li>
<li><strong>효율성과 강건성 (Efficiency and Robustness):</strong> 모델의 크기가 기하급수적으로 커지면서, 이를 더 효율적으로 만들거나(모델 압축, 양자화 212), 훈련 데이터와 실제 배포 환경 간의 차이(분포 변화, distribution shift)에 강건한 모델을 만드는 연구의 중요성이 커지고 있습니다.205</li>
<li><strong>자기 지도 학습:</strong> 레이블링 비용 없이 대규모 데이터를 활용하려는 노력은 계속해서 중요한 연구 주제입니다.183</li>
</ul>
<h3>7.2  컴퓨터 비전의 윤리적 과제</h3>
<p>컴퓨터 비전 기술은 사회에 막대한 혜택을 주지만, 동시에 심각한 윤리적 문제를 야기할 수 있습니다. 책임감 있는 개발자는 이러한 문제를 깊이 이해하고 해결하기 위해 노력해야 합니다. 윤리적 AI 개발의 핵심 원칙은 **공정성(Fairness), 투명성(Transparency), 책무성(Accountability), 프라이버시(Privacy)**입니다.214</p>
<h4>7.2.1  알고리즘 편향과 공정성</h4>
<ul>
<li><strong>문제점:</strong> 컴퓨터 비전 모델은 훈련 데이터에 존재하는 사회적, 문화적 편견을 학습하고 증폭시킬 수 있습니다.216 이로 인해 특정 인구 집단에게 불공정한 결과를 초래하는 시스템이 만들어질 수 있습니다.</li>
<li><strong>사례 연구: 얼굴 인식 기술의 편향:</strong> 얼굴 인식은 편향 문제의 가장 대표적인 사례입니다. MIT와 미국 국립표준기술연구소(NIST)의 연구에 따르면, 다수의 상용 얼굴 인식 시스템이 유색인종, 특히 <strong>흑인 여성</strong>에 대해 백인 남성보다 훨씬 높은 오류율을 보였습니다.218 이러한 편향된 시스템이 사법 시스템에 사용될 경우, 무고한 사람을 범죄자로 오인하여 부당하게 체포하는 등 심각한 인권 침해로 이어질 수 있습니다.219</li>
<li><strong>편향의 원인:</strong></li>
<li><strong>데이터 편향:</strong> 훈련 데이터셋이 특정 인구 집단(예: 백인 남성)을 과도하게 대표하고 다른 집단을 과소 대표할 때 발생합니다. 모델은 자신이 자주 본 데이터에 대해서만 잘 작동하도록 학습됩니다.218</li>
<li><strong>알고리즘 편향:</strong> 알고리즘 설계 자체가 특정 집단에 유리한 특징에 과도하게 의존하도록 만들어질 수 있습니다.224</li>
<li><strong>완화 전략:</strong> 다양한 인구 집단을 공정하게 대표하는 데이터셋을 구축하고, 데이터 증강, 샘플 가중치 조절, 적대적 학습 등의 기법을 사용하며, 개발된 모델이 여러 인구 통계학적 그룹에 걸쳐 공정하게 작동하는지 지속적으로 감사(auditing)해야 합니다.225</li>
</ul>
<h4>7.2.2  프라이버시, 감시, 데이터 권리</h4>
<ul>
<li><strong>위협:</strong> CCTV, 스마트폰, 소셜 미디어 등에서 생성되는 방대한 시각적 데이터와 이를 자동으로 분석하는 컴퓨터 비전 기술의 결합은 전례 없는 규모의 <strong>대규모 감시 사회</strong>를 가능하게 합니다.228 이는 개인의 동의 없이 일상적인 활동을 추적하고 분석하여 프라이버시라는 기본권을 침해할 수 있습니다.</li>
<li><strong>데이터 수집과 동의:</strong> 많은 대규모 데이터셋은 인터넷에서 사용자의 명시적인 동의 없이 수집된 이미지로 만들어져, 데이터 소유권과 사용에 대한 심각한 윤리적 문제를 제기합니다.216</li>
<li><strong>기술적 해결책의 필요성:</strong> 이 지점에서 파트 5.3에서 다룬 <strong>연합 학습, 차분 프라이버시, 동형 암호</strong>와 같은 프라이버시 보존 기술은 단순한 기술적 발전을 넘어 윤리적 필수 요건이 됩니다.232</li>
</ul>
<h4>7.2.3  딥페이크 딜레마와 사회적 영향</h4>
<ul>
<li><strong>기술:</strong> GAN이나 디퓨전 모델과 같은 생성 모델은 실제와 구별이 거의 불가능한 가짜 이미지나 비디오, 즉 **딥페이크(Deepfake)**를 만들 수 있습니다.234</li>
<li><strong>악의적 사용:</strong> 딥페이크 기술은 불행히도 악의적인 목적으로 널리 사용되고 있습니다. 동의 없이 제작된 음란물(non-consensual pornography)이 가장 큰 비중을 차지하며, 유명인이나 기업 임원을 사칭한 금융 사기, 특정 정치인에 대한 가짜 뉴스를 퍼뜨려 선거에 개입하는 등 사회적 혼란을 야기하는 데 사용됩니다.234</li>
<li><strong>사회적 영향:</strong> 가장 큰 위험은 <strong>신뢰의 붕괴</strong>입니다. 어떤 시각적, 청각적 증거도 쉽게 조작될 수 있다는 인식이 퍼지면, 사람들은 미디어, 공적 기관, 심지어는 법정 증거까지도 믿지 못하게 됩니다. 이는 진실과 거짓의 경계를 무너뜨려 ‘탈진실(post-truth)’ 시대를 가속화할 수 있습니다.235</li>
</ul>
<h4>7.2.4  책임감 있는 AI를 위한 프레임워크</h4>
<p>기술의 발전 속도를 법과 제도가 따라가지 못하는 상황에서, 개발자, 기업, 연구자 커뮤니티의 자발적이고 선제적인 윤리 프레임워크 구축이 필수적입니다.236</p>
<ul>
<li><strong>핵심 원칙:</strong> 인간 감독, 투명성(AI 생성 콘텐츠임을 명시), 책무성, 피해 방지 원칙 등이 포함되어야 합니다.214</li>
<li><strong>EU AI Act:</strong> 유럽연합이 제정한 AI 규제법은 이러한 노력의 중요한 이정표입니다. 이 법안은 AI 시스템을 위험 수준에 따라 분류하여, 사회적 점수제나 공공장소에서의 실시간 생체 인식과 같은 ‘용납할 수 없는 위험’ 시스템은 금지하고, 의료, 고용, 사법 등 ‘고위험’ 시스템에 대해서는 데이터 품질, 문서화, 인간 감독 등 엄격한 의무를 부과합니다.240 이는 향후 글로벌 AI 규제의 모델이 될 가능성이 높습니다.</li>
</ul>
<h3>7.3  미래 전망: 새로운 트렌드와 미해결 과제</h3>
<p>컴퓨터 비전은 앞으로도 계속해서 빠르게 진화할 것입니다. 주목해야 할 몇 가지 주요 트렌드는 다음과 같습니다.</p>
<ul>
<li><strong>AI의 민주화와 전문화:</strong> AutoML과 같은 기술은 비전문가도 쉽게 AI 모델을 개발할 수 있게 하여 AI의 접근성을 높일 것입니다.212 동시에, 범용 모델보다는 의료, 농업, 제조 등 특정 도메인에 고도로 특화된 전문 모델에 대한 수요가 증가할 것입니다.</li>
<li><strong>멀티모달리티 (Multimodality):</strong> 미래의 AI는 시각 정보만 사용하는 것을 넘어, 언어(VLM), 음성, 촉각 등 다양한 종류의 데이터를 통합하여 세상을 더 풍부하고 맥락적으로 이해하게 될 것입니다.211</li>
<li><strong>엣지 AI와 실시간 처리:</strong> 모델이 더 효율화되고 엣지 디바이스(스마트폰, IoT 센서)의 AI 가속기 성능이 향상됨에 따라, 더 많은 연산이 클라우드가 아닌 기기 자체에서 이루어질 것입니다. 이는 자율주행차나 AR처럼 낮은 지연 시간과 높은 프라이버시가 요구되는 응용 분야에 필수적입니다.243</li>
<li><strong>3D 비전의 보편화:</strong> 2D 이미지를 넘어 3D 공간을 완벽하게 이해하고 재구성하는 기술(NeRF, 3D 센서 퓨전 등)이 비전 연구의 주류가 될 것입니다.243</li>
<li><strong>파운데이션 모델과 자기 지도 학습:</strong> 웹 스케일의 레이블 없는 데이터로 거대한 ’파운데이션 모델’을 자기 지도 방식으로 사전 훈련하고, 이를 다양한 하위 작업에 맞춰 미세 조정하는 패러다임이 더욱 지배적이 될 것입니다. 이는 특정 작업을 위한 데이터 레이블링의 필요성을 더욱 줄여줄 것입니다.183</li>
</ul>
<p>결론적으로, 21세기의 컴퓨터 비전 전문가가 된다는 것은 단순히 기술적 역량을 갖추는 것을 의미하지 않습니다. 그것은 데이터 거버넌스, 윤리적 감사, 규제 준수에 대한 깊은 이해를 바탕으로, 강력할 뿐만 아니라 공정하고, 투명하며, 신뢰할 수 있는 시스템을 구축할 수 있는 능력을 갖추는 것을 의미합니다. 기술의 힘이 커질수록 그것을 사용하는 사람의 책임 또한 무거워진다는 사실을 항상 명심해야 할 것입니다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Computer Vision: 그 개념과 중요성 - SAS, accessed July 4, 2025, https://www.sas.com/ko_kr/insights/analytics/computer-vision.html</li>
<li>컴퓨터 비전의 응용 분야 설명 - Ultralytics, accessed July 4, 2025, https://www.ultralytics.com/ko/blog/exploring-how-the-applications-of-computer-vision-work</li>
<li>컴퓨터 비전(Computer Vision)이란? 정의, 활용 사례, 기술, accessed July 4, 2025, https://kr.appen.com/blog/computer-vision/</li>
<li>2025년의 컴퓨터 비전: 트렌드 및 애플리케이션 | 울트라 애널리틱스 - Ultralytics, accessed July 4, 2025, https://www.ultralytics.com/ko/blog/everything-you-need-to-know-about-computer-vision-in-2025</li>
<li>컴퓨터 비전의 발전 - 기술 혁신과 윤리적 고민 - 티티, accessed July 4, 2025, <a href="https://titibbang795.tistory.com/entry/%EC%BB%B4%ED%93%A8%ED%84%B0-%EB%B9%84%EC%A0%84%EC%9D%98-%EB%B0%9C%EC%A0%84-%EA%B8%B0%EC%88%A0-%ED%98%81%EC%8B%A0%EA%B3%BC-%EC%9C%A4%EB%A6%AC%EC%A0%81-%EA%B3%A0%EB%AF%BC">https://titibbang795.tistory.com/entry/%EC%BB%B4%ED%93%A8%ED%84%B0-%EB%B9%84%EC%A0%84%EC%9D%98-%EB%B0%9C%EC%A0%84-%EA%B8%B0%EC%88%A0-%ED%98%81%EC%8B%A0%EA%B3%BC-%EC%9C%A4%EB%A6%AC%EC%A0%81-%EA%B3%A0%EB%AF%BC</a></li>
<li>10 Computer Vision Applications for 2025 | DigitalOcean, accessed July 4, 2025, https://www.digitalocean.com/resources/articles/computer-vision-applications</li>
<li>en.wikipedia.org, accessed July 4, 2025, https://en.wikipedia.org/wiki/Computer_vision</li>
<li>History Of Computer Vision - Let’s Data Science, accessed July 4, 2025, https://letsdatascience.com/learn/history/history-of-computer-vision/</li>
<li>Computer Vision Through the Ages | TELUS Digital, accessed July 4, 2025, https://www.telusdigital.com/insights/data-and-ai/article/computer-vision-through-the-ages</li>
<li>Computer Vision: History and How it Works - Sama, accessed July 4, 2025, https://www.sama.com/blog/computer-vision-history-how-it-works</li>
<li>What is Computer Vision? (History, Applications, Challenges) | by Ambika | Medium, accessed July 4, 2025, https://medium.com/@ambika199820/what-is-computer-vision-history-applications-challenges-13f5759b48a5</li>
<li>ImageNet: A Pioneering Vision for Computers - History of Data Science, accessed July 4, 2025, https://www.historyofdatascience.com/imagenet-a-pioneering-vision-for-computers/</li>
<li>AlexNet: ImageNet Classification with Deep Convolutional Neural Networks, accessed July 4, 2025, https://deepdaiv.oopy.io/paper/alexnet</li>
<li>AlexNet - Wikipedia, accessed July 4, 2025, https://en.wikipedia.org/wiki/AlexNet</li>
<li>What Is Image Segmentation? | IBM, accessed July 4, 2025, https://www.ibm.com/think/topics/image-segmentation</li>
<li>Applications of Computer Vision - GeeksforGeeks, accessed July 4, 2025, https://www.geeksforgeeks.org/applications-of-computer-vision/</li>
<li>21 Examples of Computer Vision Applications Across Industries - Coursera, accessed July 4, 2025, https://www.coursera.org/articles/computer-vision-applications</li>
<li>50 Computer Vision Examples and Real-World Applications - Roboflow Blog, accessed July 4, 2025, https://blog.roboflow.com/computer-vision-examples/</li>
<li>[OpenCV Practice 07] 색공간 추적하기 - 데이터 사이언스 사용 설명서 - 티스토리, accessed July 4, 2025, https://dsbook.tistory.com/166</li>
<li>[딥러닝 모델] CNN (Convolutional Neural Network) 설명 - 고양이 미로 - 티스토리, accessed July 4, 2025, https://rubber-tree.tistory.com/116</li>
<li>What is a Pixel in Digital Images? - Intelgic, accessed July 4, 2025, https://intelgic.com/what-is-pixel-in-digital-images</li>
<li>이미지 데이터 이론ㅣ색공간, 이미지 형식, OpenCV - velog, accessed July 4, 2025, <a href="https://velog.io/@sobit/%EC%9D%B4%EB%AF%B8%EC%A7%80-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%9D%B4%EB%A1%A0%E3%85%A3%EC%83%89%EA%B3%B5%EA%B0%84-%EC%9D%B4%EB%AF%B8%EC%A7%80-%ED%98%95%EC%8B%9D-OpenCV">https://velog.io/@sobit/%EC%9D%B4%EB%AF%B8%EC%A7%80-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%9D%B4%EB%A1%A0%E3%85%A3%EC%83%89%EA%B3%B5%EA%B0%84-%EC%9D%B4%EB%AF%B8%EC%A7%80-%ED%98%95%EC%8B%9D-OpenCV</a></li>
<li>이미지처리 기초 - velog, accessed July 4, 2025, <a href="https://velog.io/@samuelitis/%EC%9D%B4%EB%AF%B8%EC%A7%80%EC%B2%98%EB%A6%AC-%EA%B8%B0%EC%B4%88-1">https://velog.io/@samuelitis/%EC%9D%B4%EB%AF%B8%EC%A7%80%EC%B2%98%EB%A6%AC-%EA%B8%B0%EC%B4%88-1</a></li>
<li>From Pixels to Perception: A Technical Guide to Color Analysis in Digital Images - Medium, accessed July 4, 2025, https://medium.com/@moehamade/from-pixels-to-perception-a-technical-guide-to-color-analysis-in-digital-images-d8c47ee0b89e</li>
<li>Introduction to Color Spaces - Tutorials Point, accessed July 4, 2025, https://www.tutorialspoint.com/dip/introduction_to_color_spaces.htm</li>
<li>YCbCr Color Space: Understanding Its Role in Digital Photography - PRO EDU, accessed July 4, 2025, https://proedu.com/blogs/photography-fundamentals/ycbcr-color-space-understanding-its-role-in-digital-photography</li>
<li>이미지 처리의 히스토그램 이해 - 전자통신공학 알아보기 - 티스토리, accessed July 4, 2025, https://everytipoflife.tistory.com/26</li>
<li>이미지 히스토그램 - velog, accessed July 4, 2025, <a href="https://velog.io/@syl_vana/%EC%9D%B4%EB%AF%B8%EC%A7%80-%ED%9E%88%EC%8A%A4%ED%86%A0%EA%B7%B8%EB%9E%A8">https://velog.io/@syl_vana/%EC%9D%B4%EB%AF%B8%EC%A7%80-%ED%9E%88%EC%8A%A4%ED%86%A0%EA%B7%B8%EB%9E%A8</a></li>
<li>2장 영상처리 - 히스토그램, accessed July 4, 2025, https://heo-seongil.tistory.com/27</li>
<li>[OpenCV] 6. 히스토그램, 명암비, 스트레칭, 평활화, 역투영 - Computistics - 티스토리, accessed July 4, 2025, https://computistics.tistory.com/38</li>
<li>Histogram equalization - Wikipedia, accessed July 4, 2025, https://en.wikipedia.org/wiki/Histogram_equalization</li>
<li>What is OpenCV? A Guide for Beginners. - Roboflow Blog, accessed July 4, 2025, https://blog.roboflow.com/what-is-opencv/</li>
<li>About - OpenCV, accessed July 4, 2025, https://opencv.org/about/</li>
<li>Introduction - OpenCV Documentation, accessed July 4, 2025, https://docs.opencv.org/4.x/d1/dfb/intro.html</li>
<li>PyTorch vs TensorFlow in 2022 - velog, accessed July 4, 2025, https://velog.io/@freejack/PyTorch-vs-TensorFlow-in-2022</li>
<li>PyTorch vs. TensorFlow: Key Differences to Know for Deep Learning - Built In, accessed July 4, 2025, https://builtin.com/data-science/pytorch-vs-tensorflow</li>
<li>PyTorch vs TensorFlow: Comparative Guide of AI Frameworks 2025 - OpenCV, accessed July 4, 2025, https://opencv.org/blog/pytorch-vs-tensorflow/</li>
<li>[pytorch vs. Tensorflow] 딥러닝 프레임워크? 어떤 차이가 있을까? - Korean Bioinformatics, accessed July 4, 2025, https://mopipe.tistory.com/218</li>
<li>Pytorch vs Tensorflow: A Head-to-Head Comparison - viso.ai, accessed July 4, 2025, https://viso.ai/deep-learning/pytorch-vs-tensorflow/</li>
<li>Pytorch VS Tensorflow by 2022 - Hosik’s 성장의 기록 - 티스토리, accessed July 4, 2025, https://hyoseok-personality.tistory.com/entry/Week-2-1-Pytorch-VS-Tensorflow-by-202</li>
<li>2022년 PyTorch 와 TensorFlow 비교 - GeekNews, accessed July 4, 2025, https://news.hada.io/topic?id=5578</li>
<li>Tensorflow, Pytorch 비교 - velog, accessed July 4, 2025, <a href="https://velog.io/@ttogle918/Tensorflow-Pytorch-%EB%B9%84%EA%B5%90">https://velog.io/@ttogle918/Tensorflow-Pytorch-%EB%B9%84%EA%B5%90</a></li>
<li>How to Choose Between TensorFlow vs PyTorch in 2025 - YouTube, accessed July 4, 2025, https://www.youtube.com/watch?v=yOGi4vmtNaY</li>
<li>ImageNet, accessed July 4, 2025, https://www.image-net.org/</li>
<li>Understanding ImageNet: A Key Resource for Computer Vision and AI Research - Zilliz, accessed July 4, 2025, https://zilliz.com/glossary/imagenet</li>
<li>The Impact of ImageNet. A child learns how to speak their first… | by TheArtificialPenguin | Medium, accessed July 4, 2025, https://medium.com/@theartificialpenguin/where-should-one-learn-from-55a07d29fd1a</li>
<li>0031 Popular Image Dataset - Deepest Documentation - Read the Docs, accessed July 4, 2025, https://deepestdocs.readthedocs.io/en/latest/003_image_processing/0031/</li>
<li>An Introduction to ImageNet - Roboflow Blog, accessed July 4, 2025, https://blog.roboflow.com/introduction-to-imagenet/</li>
<li>[Computer Vision] COCO 데이터셋 - SITUDY - 티스토리, accessed July 4, 2025, https://situdy.tistory.com/m/87</li>
<li>COCO Dataset: All You Need to Know to Get Started - V7 Labs, accessed July 4, 2025, https://www.v7labs.com/blog/coco-dataset-guide</li>
<li>CS : COCO Dataset - value = able - 티스토리, accessed July 4, 2025, https://kxmjhwn.tistory.com/273</li>
<li>What is the COCO Dataset? What You Need to Know - viso.ai, accessed July 4, 2025, https://viso.ai/computer-vision/coco-dataset/</li>
<li>COCO Dataset: A Step-by-Step Guide to Loading and Visualizing with Custom Code, accessed July 4, 2025, https://machinelearningspace.com/coco-dataset-a-step-by-step-guide-to-loading-and-visualizing/</li>
<li>COCO Dataset Object Detection Dataset and Pre-Trained Model by Microsoft - Roboflow Universe, accessed July 4, 2025, https://universe.roboflow.com/microsoft/coco</li>
<li>Image Filtering using Convolution in OpenCV, accessed July 4, 2025, https://opencv.org/blog/image-filtering-using-convolution-in-opencv/</li>
<li>OpenCV Python - Image Filtering - Tutorialspoint, accessed July 4, 2025, https://www.tutorialspoint.com/opencv_python/opencv_python_image_filtering.htm</li>
<li>영상처리-필터 - velog, accessed July 4, 2025, <a href="https://velog.io/@pos1504/%EC%98%81%EC%83%81%EC%B2%98%EB%A6%AC-%ED%95%84%ED%84%B0">https://velog.io/@pos1504/%EC%98%81%EC%83%81%EC%B2%98%EB%A6%AC-%ED%95%84%ED%84%B0</a></li>
<li>OpenCV - 17. 필터(Filter)와 컨볼루션(Convolution) 연산, 평균 블러링, 가우시안 블러링, 미디언 블러링, 바이레터럴 필터 - 귀퉁이 서재, accessed July 4, 2025, <a href="https://bkshin.tistory.com/entry/OpenCV-17-%ED%95%84%ED%84%B0Filter%EC%99%80-%EC%BB%A8%EB%B3%BC%EB%A3%A8%EC%85%98Convolution-%EC%97%B0%EC%82%B0-%ED%8F%89%EA%B7%A0-%EB%B8%94%EB%9F%AC%EB%A7%81-%EA%B0%80%EC%9A%B0%EC%8B%9C%EC%95%88-%EB%B8%94%EB%9F%AC%EB%A7%81-%EB%AF%B8%EB%94%94%EC%96%B8-%EB%B8%94%EB%9F%AC%EB%A7%81-%EB%B0%94%EC%9D%B4%EB%A0%88%ED%84%B0%EB%9F%B4-%ED%95%84%ED%84%B0">https://bkshin.tistory.com/entry/OpenCV-17-%ED%95%84%ED%84%B0Filter%EC%99%80-%EC%BB%A8%EB%B3%BC%EB%A3%A8%EC%85%98Convolution-%EC%97%B0%EC%82%B0-%ED%8F%89%EA%B7%A0-%EB%B8%94%EB%9F%AC%EB%A7%81-%EA%B0%80%EC%9A%B0%EC%8B%9C%EC%95%88-%EB%B8%94%EB%9F%AC%EB%A7%81-%EB%AF%B8%EB%94%94%EC%96%B8-%EB%B8%94%EB%9F%AC%EB%A7%81-%EB%B0%94%EC%9D%B4%EB%A0%88%ED%84%B0%EB%9F%B4-%ED%95%84%ED%84%B0</a></li>
<li>Smoothing Images - OpenCV Documentation, accessed July 4, 2025, https://docs.opencv.org/4.x/d4/d13/tutorial_py_filtering.html</li>
<li>[OpenCV] 영상 필터링 - 별준 - 티스토리, accessed July 4, 2025, https://junstar92.tistory.com/405</li>
<li>영상의 edge detection (Sobel, Canny Edge) 원리와 사용법 - JINSOL KIM, accessed July 4, 2025, https://gaussian37.github.io/vision-concept-edge_detection/</li>
<li>이미지 필터링 - 데이터 사이언스 스쿨, accessed July 4, 2025, <a href="https://datascienceschool.net/03%20machine%20learning/03.02.02%20%EC%9D%B4%EB%AF%B8%EC%A7%80%20%ED%95%84%ED%84%B0%EB%A7%81.html">https://datascienceschool.net/03%20machine%20learning/03.02.02%20%EC%9D%B4%EB%AF%B8%EC%A7%80%20%ED%95%84%ED%84%B0%EB%A7%81.html</a></li>
<li>www.sci.utah.edu, accessed July 4, 2025, <a href="https://www.sci.utah.edu/~acoste/uou/Image/project1/Arthur_COSTE_Project_1_report.html#:~:text=Histogram%20equalization%20is%20a%20method,function%20associated%20to%20the%20image.">https://www.sci.utah.edu/<sub>acoste/uou/Image/project1/Arthur_COSTE_Project_1_report.html#:</sub>:text=Histogram%20equalization%20is%20a%20method,function%20associated%20to%20the%20image.</a></li>
<li>Histogram Equalization - OpenCV Documentation, accessed July 4, 2025, https://docs.opencv.org/4.x/d4/d1b/tutorial_histogram_equalization.html</li>
<li>영상처리-C++로 이미지 Sobel 경계 검출 및 영상 이진화 - 김효랑이, accessed July 4, 2025, https://kim-hoya.tistory.com/21</li>
<li>Edge Detection in Image Processing: An Introduction - Roboflow Blog, accessed July 4, 2025, https://blog.roboflow.com/edge-detection/</li>
<li>Canny Edge Detection - velog, accessed July 4, 2025, https://velog.io/@mykirk98/Canny-Edge-Detection</li>
<li>에지 검출(1) - 1차/2차 미분, Prewitt, Sobel :: 𝄢 - 티스토리, accessed July 4, 2025, https://deep-learning-basics.tistory.com/49</li>
<li>Edge Detection – The Canny Algorithm - FutureLearn, accessed July 4, 2025, https://www.futurelearn.com/info/courses/introduction-to-image-analysis-for-plant-phenotyping/0/steps/302632</li>
<li>OpenCV 에지 (경계선) 검출, accessed July 4, 2025, http://kocw-n.xcache.kinxcdn.com/data/document/2021/kumoh/leehaeyeoun0824/18.pdf</li>
<li>Canny edge detector - Wikipedia, accessed July 4, 2025, https://en.wikipedia.org/wiki/Canny_edge_detector</li>
<li>SIFT (Scale-Invariant-Feature TRansform)를 활용한 이미지 특징 추출 및 매칭 알고리즘, accessed July 4, 2025, <a href="https://do-my-best.tistory.com/entry/SIFT-Scale-Invariant-Feature-TRansform%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%EC%9D%B4%EB%AF%B8%EC%A7%80-%ED%8A%B9%EC%A7%95-%EC%B6%94%EC%B6%9C-%EB%B0%8F-%EB%A7%A4%EC%B9%AD-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98">https://do-my-best.tistory.com/entry/SIFT-Scale-Invariant-Feature-TRansform%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%EC%9D%B4%EB%AF%B8%EC%A7%80-%ED%8A%B9%EC%A7%95-%EC%B6%94%EC%B6%9C-%EB%B0%8F-%EB%A7%A4%EC%B9%AD-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98</a></li>
<li>What is SIFT(Scale Invariant Feature Transform) Algorithm? - Analytics Vidhya, accessed July 4, 2025, https://www.analyticsvidhya.com/blog/2019/10/detailed-guide-powerful-sift-technique-image-matching-python/</li>
<li>Scale-Invariant Feature Transform | Baeldung on Computer Science, accessed July 4, 2025, https://www.baeldung.com/cs/scale-invariant-feature-transform</li>
<li>[Computer Vision / Image Precessing] SIFT (Scale Invariant Feature Transform) - 주머니 속 메모장 - 티스토리, accessed July 4, 2025, https://alex-an0207.tistory.com/164</li>
<li>[CV] Scale-Invariant Feature Transform (SIFT) - velog, accessed July 4, 2025, https://velog.io/@u_jinju/CV-Scale-Invariant-Feature-Transform-SIFT</li>
<li>[OpenCV] 특징 디스크립터 검출기 (SIFT, SURF, ORB) - zeroeun - 티스토리, accessed July 4, 2025, https://zeroeun.tistory.com/41</li>
<li>[OpenCV] SIFT, SURF에 대한 간단 설명 - AI 연구하는 깨굴이 - 티스토리, accessed July 4, 2025, https://hsyaloe.tistory.com/77</li>
<li>[OpenCV] Feature Detection &amp; Matching | 특징 검출과 매칭 | 이미지에서 유사한 특징 찾아내기 | 이미지 대응점 - CV DOODLE, accessed July 4, 2025, https://mvje.tistory.com/133</li>
<li>[OpenCV] SURF 알고리즘, accessed July 4, 2025, <a href="https://velog.io/@checking_pks/SURF-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98">https://velog.io/@checking_pks/SURF-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98</a></li>
<li>Speeded up robust features - Wikipedia, accessed July 4, 2025, https://en.wikipedia.org/wiki/Speeded_up_robust_features</li>
<li>OpenCV 특징점 검출과 매칭 - velog, accessed July 4, 2025, <a href="https://velog.io/@brillianthhj/OpenCV-%ED%8A%B9%EC%A7%95%EC%A0%90-%EA%B2%80%EC%B6%9C%EA%B3%BC-%EB%A7%A4%EC%B9%AD">https://velog.io/@brillianthhj/OpenCV-%ED%8A%B9%EC%A7%95%EC%A0%90-%EA%B2%80%EC%B6%9C%EA%B3%BC-%EB%A7%A4%EC%B9%AD</a></li>
<li>이미지 분할이란 무엇인가요? - IBM, accessed July 4, 2025, https://www.ibm.com/kr-ko/think/topics/image-segmentation</li>
<li>Image Segmentation: Deep Learning vs Traditional [Guide] - V7 Labs, accessed July 4, 2025, https://www.v7labs.com/blog/image-segmentation-guide</li>
<li>Image Segmentation Mastery: Techniques &amp; Applications Guide - Rapid Innovation, accessed July 4, 2025, https://www.rapidinnovation.io/post/mastering-image-segmentation-a-comprehensive-guide-to-techniques-and-applications</li>
<li>[패턴인식] 영상분할(1): 영상 분할의 원리, 전통적 방법 - 코딩스뮤, accessed July 4, 2025, https://codingsmu.tistory.com/117</li>
<li>[이론/Imple] Object Tracking 객체 트래킹 - ZOELOG - 티스토리, accessed July 4, 2025, https://stroy-jy.tistory.com/106</li>
<li>[Overview] Multipe Object Tracking 이란 (1) - 공돌이 공룡의 서재 - 티스토리, accessed July 4, 2025, https://mr-waguwagu.tistory.com/48</li>
<li>YOLO_NAS_SORT 이용하여 people counting - 주뇽’s 저장소 - 티스토리, accessed July 4, 2025, https://jypark1111.tistory.com/82</li>
<li>깊이 추정(Depth Estimation)이란? - byeol3325, accessed July 4, 2025, https://byeol3325.github.io/_posts/studying/research/2024-07-29-depth/</li>
<li>3D 카메라 기술 이해: 스테레오 비전 (Stereo Vision) / 구조광 (Structured Light) / ToF (Time-of-Flight) | Namuga - 나무가, accessed July 4, 2025, https://namuga.com/kor/invest/news_view.php?v_seqno=245</li>
<li>docs.luxonis.com, accessed July 4, 2025, <a href="https://docs.luxonis.com/hardware/platform/depth/configuring-stereo-depth#:~:text=Stereo%20depth%20vision%20works%20by,what%20our%20right%20eye%20sees.">https://docs.luxonis.com/hardware/platform/depth/configuring-stereo-depth#:~:text=Stereo%20depth%20vision%20works%20by,what%20our%20right%20eye%20sees.</a></li>
<li>Configuring Stereo Depth - Luxonis Docs, accessed July 4, 2025, https://docs.luxonis.com/hardware/platform/depth/configuring-stereo-depth</li>
<li>11장 3차원 비전, accessed July 4, 2025, https://heo-seongil.tistory.com/84</li>
<li>[딥러닝] 합성곱 신경망, CNN(Convolutional Neural Network) - 이론편 - 공대생의 차고, accessed July 4, 2025, https://underflow101.tistory.com/25</li>
<li>합성곱 신경망 - 위키백과, 우리 모두의 백과사전, accessed July 4, 2025, <a href="https://ko.wikipedia.org/wiki/%ED%95%A9%EC%84%B1%EA%B3%B1_%EC%8B%A0%EA%B2%BD%EB%A7%9D">https://ko.wikipedia.org/wiki/%ED%95%A9%EC%84%B1%EA%B3%B1_%EC%8B%A0%EA%B2%BD%EB%A7%9D</a></li>
<li>CNN의 등장과 발전 과정 - 2 (VGGNet, ResNet, DenseNet, EfficientNet) - AI 하는 빌리의 반란, accessed July 4, 2025, https://dotiromoook.tistory.com/21</li>
<li>What are Convolutional Neural Networks? - IBM, accessed July 4, 2025, https://www.ibm.com/think/topics/convolutional-neural-networks</li>
<li>Introduction to Convolution Neural Network - GeeksforGeeks, accessed July 4, 2025, https://www.geeksforgeeks.org/machine-learning/introduction-convolution-neural-network/</li>
<li>Convolutional Neural Networks (CNN) Tutorial - Analytics Vidhya, accessed July 4, 2025, https://www.analyticsvidhya.com/blog/2018/12/guide-convolutional-neural-network-cnn/</li>
<li>VGGNet 아직도 정리 못했다면 빠르게 핵심만! - Day to_day, accessed July 4, 2025, https://day-to-day.tistory.com/64</li>
<li>AlexNet and ImageNet: The Birth of Deep Learning - Pinecone, accessed July 4, 2025, https://www.pinecone.io/learn/series/image-search/imagenet/</li>
<li>CNN Explainer - Polo Club of Data Science, accessed July 4, 2025, https://poloclub.github.io/cnn-explainer/</li>
<li>
<ol start="4">
<li>CNN(합성곱신경망) 1강. CNN 원리, accessed July 4, 2025, http://contents2.kocw.or.kr/KOCW/data/document/2020/edu1/bdu/hongseungwook1118/041.pdf</li>
</ol>
</li>
<li>Convolutional Neural Network (CNN) - Unsupervised Feature Learning and Deep Learning Tutorial, accessed July 4, 2025, http://deeplearning.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/</li>
<li>Convolutional Neural Networks (CNNs / ConvNets) - CS231n Deep Learning for Computer Vision, accessed July 4, 2025, https://cs231n.github.io/convolutional-networks/</li>
<li>Convolutional Neural Networks (CNNs) : A Complete Guide - Medium, accessed July 4, 2025, https://medium.com/@alejandro.itoaramendia/convolutional-neural-networks-cnns-a-complete-guide-a803534a1930</li>
<li>Convolutional Neural Network - Lesson 9: Activation Functions in CNNs - Medium, accessed July 4, 2025, https://medium.com/@nerdjock/convolutional-neural-network-lesson-9-activation-functions-in-cnns-57def9c6e759</li>
<li>ReLU Activation Function in Deep Learning - GeeksforGeeks, accessed July 4, 2025, https://www.geeksforgeeks.org/deep-learning/relu-activation-function-in-deep-learning/</li>
<li>ReLU Activation Function Explained - Built In, accessed July 4, 2025, https://builtin.com/machine-learning/relu-activation-function</li>
<li>How does ReLU function make it possible to let the CNN learn more complex features in input data?, accessed July 4, 2025, https://datascience.stackexchange.com/questions/126342/how-does-relu-function-make-it-possible-to-let-the-cnn-learn-more-complex-featur</li>
<li>Fully Connected Layer vs. Convolutional Layer: Explained | Built In, accessed July 4, 2025, https://builtin.com/machine-learning/fully-connected-layer</li>
<li>[딥러닝] AlexNet 모델의 개요 및 특징 - LCY - 티스토리, accessed July 4, 2025, <a href="https://lcyking.tistory.com/entry/AlexNet-%EB%AA%A8%EB%8D%B8%EC%9D%98-%EA%B0%9C%EC%9A%94-%EB%B0%8F-%ED%8A%B9%EC%A7%95">https://lcyking.tistory.com/entry/AlexNet-%EB%AA%A8%EB%8D%B8%EC%9D%98-%EA%B0%9C%EC%9A%94-%EB%B0%8F-%ED%8A%B9%EC%A7%95</a></li>
<li>CNN의 주요 모델 - AlexNet / 블로그 - 데이터메이커, accessed July 4, 2025, https://www.datamaker.io/blog/posts/34</li>
<li>AlexNet Explained | Papers With Code, accessed July 4, 2025, https://paperswithcode.com/method/alexnet</li>
<li>[Dive into Deep Learning / CNN] AlexNet - KUBIG 2022-2 활동 블로그 - 티스토리, accessed July 4, 2025, https://kubig-2022-2.tistory.com/113</li>
<li>[CNN] VGG16 model - Adore__ - 티스토리, accessed July 4, 2025, https://ai-woods.tistory.com/4</li>
<li>VGGNet 논문 리뷰, accessed July 4, 2025, <a href="https://velog.io/@pre_f_86/VGGNet-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0">https://velog.io/@pre_f_86/VGGNet-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0</a></li>
<li>VGGNet-16 Architecture: A Complete Guide - Kaggle, accessed July 4, 2025, https://www.kaggle.com/code/blurredmachine/vggnet-16-architecture-a-complete-guide</li>
<li>VGG-Net Architecture Explained - GeeksforGeeks, accessed July 4, 2025, https://www.geeksforgeeks.org/computer-vision/vgg-net-architecture-explained/</li>
<li>Deep Residual Learning for Image Recognition (ResNet Explained) - Analytics Vidhya, accessed July 4, 2025, https://www.analyticsvidhya.com/blog/2023/02/deep-residual-learning-for-image-recognition-resnet-explained/</li>
<li>[논문 리뷰] 5. ResNet의 구조 &amp; 구현, accessed July 4, 2025, https://advantageous-glass.tistory.com/14</li>
<li>[Classic] Deep Residual Learning for Image Recognition (Paper Explained) - YouTube, accessed July 4, 2025, https://www.youtube.com/watch?v=GWt6Fu05voI</li>
<li>ResNet(Residual Network) - velog, accessed July 4, 2025, https://velog.io/@qtly_u/ResNetResidual-Network</li>
<li>Residual neural network - Wikipedia, accessed July 4, 2025, https://en.wikipedia.org/wiki/Residual_neural_network</li>
<li>ResNet 구조 이해 및 구현 - 준세 단칸방 - 티스토리, accessed July 4, 2025, https://wjunsea.tistory.com/99</li>
<li>[논문정리] ResNet 개념 정리, accessed July 4, 2025, <a href="https://stevenkim1217.tistory.com/entry/ResNet-%EA%B0%9C%EB%85%90-%EC%A0%95%EB%A6%AC">https://stevenkim1217.tistory.com/entry/ResNet-%EA%B0%9C%EB%85%90-%EC%A0%95%EB%A6%AC</a></li>
<li>
<ol start="2">
<li>faster R-CNN - 공부하려고 만든 블로그 - 티스토리, accessed July 4, 2025, https://welcome-to-dewy-world.tistory.com/110</li>
</ol>
</li>
<li>Faster R-CNN Explained for Object Detection Tasks - DigitalOcean, accessed July 4, 2025, https://www.digitalocean.com/community/tutorials/faster-r-cnn-explained-object-detection</li>
<li>[딥러닝] Object detection (2)(R-CNN / Fast R-CNN / Faster R-CNN 총정리) - 비전공자 데이터분석 노트 - 티스토리, accessed July 4, 2025, https://bigdaheta.tistory.com/60</li>
<li>Faster RCNN in 2025: How it works and why it’s still the benchmark for Object Detection, accessed July 4, 2025, https://www.thinkautonomous.ai/blog/faster-rcnn/</li>
<li>Faster R-CNN Explained | Papers With Code, accessed July 4, 2025, https://paperswithcode.com/method/faster-r-cnn</li>
<li>[논문리뷰/CV] Faster R-CNN 모델 (R-CNN, Fast R-CNN과 비교) - 샤샤’s 데이터 분석 블로그, accessed July 4, 2025, https://shashacode.tistory.com/95</li>
<li>갈아먹는 Object Detection [4] Faster R-CNN - 퍼스트펭귄 코딩스쿨, accessed July 4, 2025, https://blog.firstpenguine.school/17</li>
<li>[논문 리뷰] YOLO: You Only Look Once: Unified, Real-Time Object Detection - 기억보단 기록을 - 티스토리, accessed July 4, 2025, https://haseong8012.tistory.com/46</li>
<li>YOLO Object Detection Explained: A Beginner’s Guide - DataCamp, accessed July 4, 2025, https://www.datacamp.com/blog/yolo-object-detection-explained</li>
<li>[YOLO] Object Detection - YOLO 기본 개념 - velog, accessed July 4, 2025, <a href="https://velog.io/@jiiina/YOLO-Object-Detection-YOLO-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90">https://velog.io/@jiiina/YOLO-Object-Detection-YOLO-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90</a></li>
<li>[AI/딥러닝] 객체 검출(Object Detection) 모델의 종류 R-CNN, YOLO, SSD - 고양이 미로, accessed July 4, 2025, https://rubber-tree.tistory.com/119</li>
<li>YOLO V3 Explained - Medium, accessed July 4, 2025, https://medium.com/data-science/yolo-v3-explained-ff5b850390f</li>
<li>YOLOv3 Explained | Papers With Code, accessed July 4, 2025, https://paperswithcode.com/method/yolov3</li>
<li>YOLOv3: Real-Time Object Detection Algorithm (Guide) - viso.ai, accessed July 4, 2025, https://viso.ai/deep-learning/yolov3-overview/</li>
<li>U-Net - Wikipedia, accessed July 4, 2025, https://en.wikipedia.org/wiki/U-Net</li>
<li>U-Net Architecture Explained - GeeksforGeeks, accessed July 4, 2025, https://www.geeksforgeeks.org/machine-learning/u-net-architecture-explained/</li>
<li>The U-Net : A Complete Guide | Medium, accessed July 4, 2025, https://medium.com/@alejandro.itoaramendia/decoding-the-u-net-a-complete-guide-810b1c6d56d8</li>
<li>UNet Architecture Explained In One Shot [TUTORIAL] - Kaggle, accessed July 4, 2025, https://www.kaggle.com/code/akshitsharma1/unet-architecture-explained-in-one-shot-tutorial</li>
<li>What are Skip Connections in Deep Learning? - Analytics Vidhya, accessed July 4, 2025, https://www.analyticsvidhya.com/blog/2021/08/all-you-need-to-know-about-skip-connections/</li>
<li>Do we really need that skip-connection? Understanding its interplay with task complexity | MICCAI 2023 - Accepted Papers, Reviews, Author Feedback, accessed July 4, 2025, https://conferences.miccai.org/2023/papers/216-Paper1258.html</li>
<li>UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation - PMC, accessed July 4, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC7357299/</li>
<li>DeepLabv3 &amp; DeepLabv3+ The Ultimate PyTorch Guide - LearnOpenCV, accessed July 4, 2025, https://learnopencv.com/deeplabv3-ultimate-guide/</li>
<li>A Comprehensive Guide on Atrous Convolution in CNNs - Analytics Vidhya, accessed July 4, 2025, https://www.analyticsvidhya.com/blog/2023/12/a-comprehensive-guide-on-atrous-convolution-in-cnns/</li>
<li>DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs - arXiv, accessed July 4, 2025, http://arxiv.org/pdf/1606.00915</li>
<li>Summary of DeepLabv3 paper - Swetha’s Blog, accessed July 4, 2025, https://swethatanamala.github.io/2018/08/15/summary-of-DeepLabv3-paper/</li>
<li>DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs - Liang-Chieh Chen, accessed July 4, 2025, http://liangchiehchen.com/projects/DeepLab.html</li>
<li>[1802.02611] Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation - arXiv, accessed July 4, 2025, https://arxiv.org/abs/1802.02611</li>
<li>ViT 살펴보기 | Vision Transformer - Standing-O, accessed July 4, 2025, https://standing-o.github.io/posts/vision-transformer/</li>
<li>[논문 리뷰] ViT(Vision Transformer), An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale - Yoonstory - 티스토리, accessed July 4, 2025, https://kk-yy.tistory.com/132</li>
<li>Vision Transformer(ViT) 논문 리뷰 - velog, accessed July 4, 2025, <a href="https://velog.io/@pre_f_86/Vision-TransformerViT-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0">https://velog.io/@pre_f_86/Vision-TransformerViT-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0</a></li>
<li>ViT: Vision Transformer. Transformer 구조는 자연어 처리 분야에서 표준으로 자리… - Taewan Cho, accessed July 4, 2025, https://taewan2002.medium.com/vit-vision-transformer-1d0555068f48</li>
<li>적대적 생성 신경망 GAN(Generative Adversarial Networks) - yolo_study - 티스토리, accessed July 4, 2025, <a href="https://yololifestudy.tistory.com/entry/%EC%A0%81%EB%8C%80%EC%A0%81-%EC%83%9D%EC%84%B1-%EC%8B%A0%EA%B2%BD%EB%A7%9D-GANGenerative-Adversarial-Networks">https://yololifestudy.tistory.com/entry/%EC%A0%81%EB%8C%80%EC%A0%81-%EC%83%9D%EC%84%B1-%EC%8B%A0%EA%B2%BD%EB%A7%9D-GANGenerative-Adversarial-Networks</a></li>
<li>생성적 적대 신경망 - 나무위키, accessed July 4, 2025, <a href="https://namu.wiki/w/%EC%83%9D%EC%84%B1%EC%A0%81%20%EC%A0%81%EB%8C%80%20%EC%8B%A0%EA%B2%BD%EB%A7%9D">https://namu.wiki/w/%EC%83%9D%EC%84%B1%EC%A0%81%20%EC%A0%81%EB%8C%80%20%EC%8B%A0%EA%B2%BD%EB%A7%9D</a></li>
<li>Generative Adversarial Network (GAN) - GeeksforGeeks, accessed July 4, 2025, https://www.geeksforgeeks.org/generative-adversarial-network-gan/</li>
<li>Generative Adversarial Networks: Build Your First Models - Real Python, accessed July 4, 2025, https://realpython.com/generative-adversarial-networks/</li>
<li>GAN의 기초(GAN,DCGAN,WGAN,CGAN) - velog, accessed July 4, 2025, https://velog.io/@tobigs-gm1/basicofgan</li>
<li>[ Computer Vision ] Collapse란? - KalelPark’s LAB - 티스토리, accessed July 4, 2025, https://kalelpark.tistory.com/179</li>
<li>Chapter 4-7. GAN(Generative Adversarial Networks) - NOW엑셈, accessed July 4, 2025, https://blog.ex-em.com/1896</li>
<li>GAN(Generative Adversarial Network)과 Stable diffusion의 이해와 정리 - MW Meta Writers, accessed July 4, 2025, https://metawriters.tistory.com/77</li>
<li>GAN 응용기술과 실제 활용사례 - 닐리리의 디지털 항해일지, accessed July 4, 2025, <a href="https://nilili.co.kr/ai%EC%8B%A4%ED%97%98%EC%8B%A4/gan-%EC%9D%91%EC%9A%A9%EA%B8%B0%EC%88%A0%EA%B3%BC-%EC%8B%A4%EC%A0%9C-%ED%99%9C%EC%9A%A9%EC%82%AC%EB%A1%80/">https://nilili.co.kr/ai%EC%8B%A4%ED%97%98%EC%8B%A4/gan-%EC%9D%91%EC%9A%A9%EA%B8%B0%EC%88%A0%EA%B3%BC-%EC%8B%A4%EC%A0%9C-%ED%99%9C%EC%9A%A9%EC%82%AC%EB%A1%80/</a></li>
<li>Applications of GANs in Image Generation and Style Transfer - ResearchGate, accessed July 4, 2025, https://www.researchgate.net/publication/390856906_Applications_of_GANs_in_Image_Generation_and_Style_Transfer</li>
<li>Style transfer with CycleGAN - DataScientest, accessed July 4, 2025, https://datascientest.com/en/style-transfer-with-cyclegan</li>
<li>GANs, 어떻게 활용되고 있을까? - 파이미디어랩, accessed July 4, 2025, <a href="https://www.paimedialab.com/post/gans-%EC%96%B4%EB%96%BB%EA%B2%8C-%ED%99%9C%EC%9A%A9%EB%90%98%EA%B3%A0-%EC%9E%88%EC%9D%84%EA%B9%8C">https://www.paimedialab.com/post/gans-%EC%96%B4%EB%96%BB%EA%B2%8C-%ED%99%9C%EC%9A%A9%EB%90%98%EA%B3%A0-%EC%9E%88%EC%9D%84%EA%B9%8C</a></li>
<li>GANs 4 - 실질적인 활용 사례, accessed July 4, 2025, <a href="https://velog.io/@smile_b/GANs-4-%EC%8B%A4%EC%A7%88%EC%A0%81%EC%9D%B8-%ED%99%9C%EC%9A%A9-%EC%82%AC%EB%A1%80">https://velog.io/@smile_b/GANs-4-%EC%8B%A4%EC%A7%88%EC%A0%81%EC%9D%B8-%ED%99%9C%EC%9A%A9-%EC%82%AC%EB%A1%80</a></li>
<li>Diffusion Models Beat GANs on Image Synthesis - velog, accessed July 4, 2025, https://velog.io/@philiplee_235/Diffusion-Models-Beat-GANs-on-Image-Synthesis</li>
<li>[스노피 AI] 디퓨전 모델(Diffusion model)에 대한 쉬운 설명 - Wafour (와포) 블로그, accessed July 4, 2025, <a href="https://wafour.tistory.com/entry/%EC%8A%A4%EB%85%B8%ED%94%BC-AI-%EB%94%94%ED%93%A8%EC%A0%84-%EB%AA%A8%EB%8D%B8Diffusion-model%EC%97%90-%EB%8C%80%ED%95%9C-%EC%89%AC%EC%9A%B4-%EC%84%A4%EB%AA%85">https://wafour.tistory.com/entry/%EC%8A%A4%EB%85%B8%ED%94%BC-AI-%EB%94%94%ED%93%A8%EC%A0%84-%EB%AA%A8%EB%8D%B8Diffusion-model%EC%97%90-%EB%8C%80%ED%95%9C-%EC%89%AC%EC%9A%B4-%EC%84%A4%EB%AA%85</a></li>
<li>Introduction to Diffusion Models for Machine Learning | SuperAnnotate, accessed July 4, 2025, https://www.superannotate.com/blog/diffusion-models</li>
<li>Diffusion model 설명 (Diffusion model이란? Diffusion model 증명) - 유니의 공부 - 티스토리, accessed July 4, 2025, https://process-mining.tistory.com/182</li>
<li>[Concept] Diffusion Models ( with. DDPM ) - Hosik’s 성장의 기록 - 티스토리, accessed July 4, 2025, https://hyoseok-personality.tistory.com/entry/Concept-Diffusion-Models-with-DDPM-DDIM</li>
<li>Text-to-Image 생성 모델 DALL-E 2 - velog, accessed July 4, 2025, <a href="https://velog.io/@yuhyeon0809/Text-to-Image-%EC%83%9D%EC%84%B1-%EB%AA%A8%EB%8D%B8-DALL-E-2">https://velog.io/@yuhyeon0809/Text-to-Image-%EC%83%9D%EC%84%B1-%EB%AA%A8%EB%8D%B8-DALL-E-2</a></li>
<li>그림 그려주는 AI : Stable Diffusion AI의 원리 (Latent Diffusion Model), accessed July 4, 2025, https://tech-savvy30.tistory.com/9</li>
<li>DALLE2 - Diffusion Model 논문 리뷰 - FFighting, accessed July 4, 2025, https://ffighting.net/deep-learning-paper-review/diffusion-model/dalle2/</li>
<li>Stable Diffusion vs. DALL/E 2: Which Wins for AI Art? - Latenode, accessed July 4, 2025, https://latenode.com/blog/stable-diffusion-vs-dall-e-2</li>
<li>DALL-E 2 vs Stability AI’s Stable Diffusion XL - Comparison of Image Generation Capabilities - promptmate.io, accessed July 4, 2025, https://promptmate.io/dall-e-vs-stable-diffusion/</li>
<li>(PDF) Generated Faces in the Wild: Quantitative Comparison of Stable Diffusion, Midjourney and DALL-E 2 - ResearchGate, accessed July 4, 2025, https://www.researchgate.net/publication/364126542_Generated_Faces_in_the_Wild_Quantitative_Comparison_of_Stable_Diffusion_Midjourney_and_DALL-E_2</li>
<li>A Survey of the Self Supervised Learning Mechanisms for Vision Transformers - arXiv, accessed July 4, 2025, https://arxiv.org/pdf/2408.17059</li>
<li>What Is Self-Supervised Learning? - IBM, accessed July 4, 2025, https://www.ibm.com/think/topics/self-supervised-learning</li>
<li>Self-Supervised Learning: Definition, Tutorial &amp; Examples - V7 Labs, accessed July 4, 2025, https://www.v7labs.com/blog/self-supervised-learning-guide</li>
<li>The Engineer’s Guide to Self-Supervised Learning - Lightly AI, accessed July 4, 2025, https://www.lightly.ai/blog/self-supervised-learning</li>
<li>Self-supervised learning - Wikipedia, accessed July 4, 2025, https://en.wikipedia.org/wiki/Self-supervised_learning</li>
<li>arxiv.org, accessed July 4, 2025, <a href="https://arxiv.org/html/2306.03000v3#:~:text=Neural%20Radiance%20Field%20or%20NeRF,an%20interpolation%20approach%20between%20scenes.">https://arxiv.org/html/2306.03000v3#:~:text=Neural%20Radiance%20Field%20or%20NeRF,an%20interpolation%20approach%20between%20scenes.</a></li>
<li>Neural radiance field - Wikipedia, accessed July 4, 2025, https://en.wikipedia.org/wiki/Neural_radiance_field</li>
<li>What is NeRF? - Neural Radiance Fields Explained - AWS, accessed July 4, 2025, https://aws.amazon.com/what-is/neural-radiance-fields/</li>
<li>BeyondPixels: A Comprehensive Review of the Evolution of Neural Radiance Fields - arXiv, accessed July 4, 2025, https://arxiv.org/html/2306.03000v3</li>
<li>A Step-by-Step Guide to Federated Learning in Computer Vision | by Amit Yadav | Biased-Algorithms | Medium, accessed July 4, 2025, https://medium.com/biased-algorithms/a-step-by-step-guide-to-federated-learning-in-computer-vision-0984e4a7f8d5</li>
<li>Federated Learning in Computer Vision | A Collaborative Revolution | by Saiwa - Medium, accessed July 4, 2025, https://medium.com/@saiwadotai/federated-learning-in-computer-vision-a-collaborative-revolution-a1d005087603</li>
<li>A Step-by-Step Guide to Federated Learning in Computer Vision - V7 Labs, accessed July 4, 2025, https://www.v7labs.com/blog/federated-learning-guide</li>
<li>Federated learning - Wikipedia, accessed July 4, 2025, https://en.wikipedia.org/wiki/Federated_learning</li>
<li>Differential privacy - Wikipedia, accessed July 4, 2025, https://en.wikipedia.org/wiki/Differential_privacy</li>
<li>Differential privacy for deep learning at GPT scale - Amazon Science, accessed July 4, 2025, https://www.amazon.science/blog/differential-privacy-for-deep-learning-at-gpt-scale</li>
<li>Differential Privacy and Deep Learning - GeeksforGeeks, accessed July 4, 2025, https://www.geeksforgeeks.org/deep-learning/differential-privacy-and-deep-learning/</li>
<li>Advancing Differential Privacy: Where We Are Now and Future Directions for Real-World Deployment, accessed July 4, 2025, https://hdsr.mitpress.mit.edu/pub/sl9we8gh</li>
<li>Homomorphic encryption - Wikipedia, accessed July 4, 2025, https://en.wikipedia.org/wiki/Homomorphic_encryption</li>
<li>Homomorphic Encryption - CyberArk, accessed July 4, 2025, https://www.cyberark.com/what-is/homomorphic-encryption/</li>
<li>Homomorphic Encryption For Computer Vision - CS231n, accessed July 4, 2025, https://cs231n.stanford.edu/reports/2022/pdfs/97p.pdf</li>
<li>컴퓨터 비전(Computer Vision) 학회 정리(Conferences) - 구르는돼지 - 티스토리, accessed July 4, 2025, https://rollingpig.tistory.com/26</li>
<li>ICCV 2025 - The Computer Vision Foundation, accessed July 4, 2025, https://iccv.thecvf.com/</li>
<li>Most Influential ECCV Papers (2024-09 Version) - Paper Digest, accessed July 4, 2025, https://www.paperdigest.org/2024/09/most-influential-eccv-papers-2024-09/</li>
<li>Most Influential ICCV Papers (2025-03 Version), accessed July 4, 2025, https://www.paperdigest.org/2025/03/most-influential-iccv-papers-2025-03-version/</li>
<li>CVPR 2025 unveils latest trends in AI, image and video synthesis, neural networks, and 3D computer vision - Robotics &amp; Automation News, accessed July 4, 2025, https://roboticsandautomationnews.com/2025/06/23/cvpr-2025-unveils-latest-trends-in-ai-image-and-video-synthesis-neural-networks-and-3d-computer-vision/92454/</li>
<li>Challenge 2025 | OpenDriveLab, accessed July 4, 2025, https://opendrivelab.com/challenge2025/</li>
<li>ECCV 2024 – Research Impact &amp; Leadership, accessed July 4, 2025, https://sites.gatech.edu/research/eccv-2024/</li>
<li>Recapping ECCV 2024 Redux: Day 1 - Voxel51, accessed July 4, 2025, https://voxel51.com/blog/recapping-eccv-2024-redux-day-1</li>
<li>[2305.05726] Vision-Language Models in Remote Sensing: Current Progress and Future Trends - arXiv, accessed July 4, 2025, https://arxiv.org/abs/2305.05726</li>
<li>딥러닝 vs 머신러닝: 2025년 최신 트렌드로 알아보는 성능과 적용 영역 완전정복 - 재능넷, accessed July 4, 2025, https://www.jaenung.net/tree/18915</li>
<li>ECCV 2024: Reflections and Key Takeaways - EVS - Embedded Vision Systems, accessed July 4, 2025, https://embeddedvisionsystems.com/eccv-2024-reflections-and-key-takeaways/</li>
<li>Ethics of Artificial Intelligence | UNESCO, accessed July 4, 2025, https://www.unesco.org/en/artificial-intelligence/recommendation-ethics</li>
<li>onlinedegrees.sandiego.edu, accessed July 4, 2025, <a href="https://onlinedegrees.sandiego.edu/ethics-in-ai/#:~:text=The%20main%20pillars%20of%20AI,are%20responsible%2C%20unbiased%20and%20trustworthy.">https://onlinedegrees.sandiego.edu/ethics-in-ai/#:~:text=The%20main%20pillars%20of%20AI,are%20responsible%2C%20unbiased%20and%20trustworthy.</a></li>
<li>Ethical Considerations and Bias in Computer Vision (CV) | by Xenonstack - Medium, accessed July 4, 2025, https://medium.com/xenonstack-ai/ethical-considerations-and-bias-in-computer-vision-cv-50db5bb57999</li>
<li>Algorithmic bias - Wikipedia, accessed July 4, 2025, https://en.wikipedia.org/wiki/Algorithmic_bias</li>
<li>4 Computer Vision and Society, accessed July 4, 2025, https://visionbook.mit.edu/fairness.html</li>
<li>Biased Technology: The Automated Discrimination of Facial …, accessed July 4, 2025, https://www.aclu-mn.org/en/news/biased-technology-automated-discrimination-facial-recognition</li>
<li>The Problem of Bias in Facial Recognition | Strategic Technologies …, accessed July 4, 2025, https://www.csis.org/blogs/strategic-technologies-blog/problem-bias-facial-recognition</li>
<li>What are examples of computer vision bugs related to race? - Milvus, accessed July 4, 2025, https://milvus.io/ai-quick-reference/what-are-examples-of-computer-vision-bugs-related-to-race</li>
<li>UB computer science professor weighs in on bias in facial recognition software, accessed July 4, 2025, https://www.buffalo.edu/news/tipsheets/2024/ub-ai-expert-facial-recognition-expert-ifeoma-nwogu.html</li>
<li>The Bias in the Machine: Facial Recognition Technology and Racial Disparities, accessed July 4, 2025, https://mit-serc.pubpub.org/pub/bias-in-machine</li>
<li>Seeing Clearly: Addressing Bias in Computer Vision Models &amp; AI - Leverege, accessed July 4, 2025, https://www.leverege.com/blogpost/seeing-clearly-addressing-bias-in-computer-vision-models-ai</li>
<li>How To Avoid Bias In Computer Vision Models - Roboflow Blog, accessed July 4, 2025, https://blog.roboflow.com/how-to-avoid-bias-in-computer-vision-models/</li>
<li>Advanced Bias Mitigation Techniques - Number Analytics, accessed July 4, 2025, https://www.numberanalytics.com/blog/advanced-bias-mitigation-techniques-machine-learning</li>
<li>Bias Mitigation Strategies and Techniques for Classification Tasks - Holistic AI, accessed July 4, 2025, https://www.holisticai.com/blog/bias-mitigation-strategies-techniques-for-classification-tasks</li>
<li>Wake up call for AI: Computer-vision research increasingly used for surveillance - Reddit, accessed July 4, 2025, https://www.reddit.com/r/science/comments/1lkdas8/wake_up_call_for_ai_computervision_research/</li>
<li>Privacy Risks in Computer Vision - Medium, accessed July 4, 2025, https://medium.com/@divya.it.saxena/privacy-risks-in-computer-vision-092a764d7281</li>
<li>Computer vision, surveillance, and social control | Montreal AI Ethics Institute, accessed July 4, 2025, https://montrealethics.ai/computer-vision-surveillance-and-social-control/</li>
<li>Ethical Considerations and Bias in Computer Vision (CV) - XenonStack, accessed July 4, 2025, https://www.xenonstack.com/blog/ethical-considerations-in-computer-vision</li>
<li>Privacy-Preserving AI: Techniques &amp; Frameworks | AI Receptionist Tips &amp; Insights | Dialzara, accessed July 4, 2025, https://dialzara.com/blog/privacy-preserving-ai-techniques-and-frameworks/</li>
<li>Privacy-Preserving Optics for Enhancing Protection in Face De-Identification - CVF Open Access, accessed July 4, 2025, https://openaccess.thecvf.com/content/CVPR2024/papers/Lopez_Privacy-Preserving_Optics_for_Enhancing_Protection_in_Face_De-Identification_CVPR_2024_paper.pdf</li>
<li>Examining the Societal Impact and Legislative Requirements of Deepfake Technology: A Comprehensive Study - ResearchGate, accessed July 4, 2025, https://www.researchgate.net/publication/379615642_Examining_the_Societal_Impact_and_Legislative_Requirements_of_Deepfake_Technology_A_Comprehensive_Study</li>
<li>Debating the ethics of deepfakes, accessed July 4, 2025, https://www.orfonline.org/expert-speak/debating-the-ethics-of-deepfakes</li>
<li>Navigating the Mirage: Ethical, Transparency, and Regulatory …, accessed July 4, 2025, https://walton.uark.edu/insights/posts/navigating-the-mirage-ethical-transparency-and-regulatory-challenges-in-the-age-of-deepfakes.php</li>
<li>The Ethics And Implications Of Deepfake Technology In Media And Politics. - Consensus, accessed July 4, 2025, https://consensus.app/questions/ethics-implications-deepfake-technology-media-politics/</li>
<li>Ethical Considerations in Artificial Intelligence: A Comprehensive Disccusion from the Perspective of Computer Vision - ResearchGate, accessed July 4, 2025, https://www.researchgate.net/publication/376518424_Ethical_Considerations_in_Artificial_Intelligence_A_Comprehensive_Disccusion_from_the_Perspective_of_Computer_Vision</li>
<li>A Practical Guide to Building With Ethical AI Frameworks - DhiWise, accessed July 4, 2025, https://www.dhiwise.com/post/a-practical-guide-to-building-with-ethical-ai-frameworks</li>
<li>AI Act | Shaping Europe’s digital future - European Union, accessed July 4, 2025, https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai</li>
<li>stlpartners.com, accessed July 4, 2025, <a href="https://stlpartners.com/articles/edge-computing/inside-the-eu-ai-act/#:~:text=The%20EU%20AI%20Act%20is%20the%20world&#x27;s%20first%20comprehensive%20attempt,rights%20without%20outright%20stifling%20innovation.">https://stlpartners.com/articles/edge-computing/inside-the-eu-ai-act/#:~:text=The%20EU%20AI%20Act%20is%20the%20world’s%20first%20comprehensive%20attempt,rights%20without%20outright%20stifling%20innovation.</a></li>
<li>3 Key Highlights from the EU AI Act - Keypoint Intelligence, accessed July 4, 2025, https://keypointintelligence.com/keypoint-blogs/3-key-highlights-from-the-eu-ai-act</li>
<li>2025년, 꼭 알아야 할 Vision AI 트렌드 5가지 - 슈퍼브 블로그, accessed July 4, 2025, https://blog-ko.superb-ai.com/top-5-vision-ai-trends-2025/</li>
<li>2025년, 꼭 알아야 할 AI비전 트렌드 7가지, accessed July 4, 2025, https://brunch.co.kr/@acc9b16b9f0f430/135</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>