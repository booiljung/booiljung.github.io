<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:가우스-뉴턴 최적화</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>가우스-뉴턴 최적화</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="../index.html">최적화 (Optimizations)</a> / <a href="index.html">비선형최소제곱문제 최적화</a> / <span>가우스-뉴턴 최적화</span></nav>
                </div>
            </header>
            <article>
                <h1>가우스-뉴턴 최적화</h1>
<h2>1. 서론</h2>
<p>비선형 최소제곱(Nonlinear Least Squares, NLS) 문제는 데이터 과학, 공학, 통계학, 기계학습 등 수많은 과학 및 기술 분야에서 나타나는 근본적인 최적화 문제이다.1 이 문제는 관측된 데이터에 비선형 모델을 가장 잘 부합시키는 파라미터를 찾는 과정에서 발생하며, 그 핵심은 모델 예측과 실제 관측값 사이의 오차 제곱합을 최소화하는 데 있다. 가우스-뉴턴(Gauss-Newton, GN) 방법은 이러한 NLS 문제 해결을 위해 특별히 설계된 가장 대표적이고 효율적인 반복 최적화 알고리즘 중 하나로 평가받는다. 이 방법은 2차 미분 정보(헤시안 행렬)를 사용하여 빠른 수렴 속도를 보이는 뉴턴 방법의 강력함과, 1차 미분 정보(그래디언트)만을 사용하여 계산이 간단한 경사 하강법의 실용성 사이에서 절묘한 균형점을 찾으려는 시도에서 탄생했다.</p>
<p>본 안내서는 가우스-뉴턴 방법의 핵심 원리를 다각도에서 심층적으로 분석하는 것을 목표로 한다. 먼저 비선형 최소제곱 문제의 수학적 정립을 시작으로, 가우스-뉴턴 알고리즘의 유도 과정을 상세히 추적한다. 이 과정에서 뉴턴 방법과의 근본적인 관계를 헤시안 행렬의 근사라는 관점에서 명확히 규명하고, 이를 통해 알고리즘의 강점과 약점을 분석한다. 나아가 수렴 속도와 조건, 그리고 알고리즘이 실패할 수 있는 내재적 한계를 고찰하며, 이러한 한계를 극복하기 위해 제안된 레벤버그-마르카르트(Levenberg-Marquardt)와 같은 핵심적인 개선 알고리즘을 탐구한다. 마지막으로, 이론적 논의에 그치지 않고 컴퓨터 비전 분야의 번들 조정(Bundle Adjustment)과 로보틱스 분야의 그래프 기반 SLAM(Graph-based SLAM)이라는 대규모 실세계 문제에 가우스-뉴턴의 원리가 어떻게 적용되고 확장되는지 구체적인 사례를 통해 분석함으로써, 이론과 실제를 연결하는 포괄적인 통찰을 제공하고자 한다.</p>
<h2>2.  비선형 최소제곱 문제의 정립</h2>
<p>비선형 최소제곱 문제는 <span class="math math-inline">m</span>개의 독립 변수-종속 변수 쌍으로 이루어진 관측 데이터 <span class="math math-inline">(x_i, y_i)</span>가 주어졌을 때, 이를 가장 잘 설명하는 <span class="math math-inline">n</span>개의 파라미터 벡터 <span class="math math-inline">\boldsymbol{\beta} = [\beta_1, \dots, \beta_n]^T</span>를 가진 비선형 모델 함수 <span class="math math-inline">f(x, \boldsymbol{\beta})</span>를 찾는 문제로 정의된다.1 문제의 목표는 각 관측값 <span class="math math-inline">y_i</span>와 모델의 예측값 <span class="math math-inline">f(x_i, \boldsymbol{\beta})</span> 사이의 차이, 즉 잔차(residual) <span class="math math-inline">r_i(\boldsymbol{\beta}) = y_i - f(x_i, \boldsymbol{\beta})</span>의 제곱합(Sum of Squared Errors, SSE)을 최소화하는 최적의 파라미터 <span class="math math-inline">\boldsymbol{\beta}</span>를 찾는 것이다.1</p>
<p>이 목적 함수 <span class="math math-inline">S(\boldsymbol{\beta})</span>는 <span class="math math-inline">m</span>개의 잔차로 구성된 벡터 <span class="math math-inline">\mathbf{r}(\boldsymbol{\beta}) = [r_1(\boldsymbol{\beta}), \dots, r_m(\boldsymbol{\beta})]^T</span>를 이용하여 다음과 같이 간결하게 표현할 수 있다.</p>
<p><span class="math math-display">
S(\boldsymbol{\beta}) = \sum_{i=1}^{m} r_i(\boldsymbol{\beta})^2 = \mathbf{r}(\boldsymbol{\beta})^T \mathbf{r}(\boldsymbol{\beta}) = ||\mathbf{r}(\boldsymbol{\beta})||_2^2
</span></p>
<p>일반적으로 데이터의 개수 <span class="math math-inline">m</span>이 파라미터의 개수 <span class="math math-inline">n</span>보다 크거나 같은 과결정 시스템(overdetermined system, <span class="math math-inline">m \ge n</span>)을 다루게 되는데, 이 경우 모든 잔차 <span class="math math-inline">r_i</span>를 동시에 0으로 만드는 완벽한 해는 거의 존재하지 않는다.7 따라서 오차의 총합을 최소화하는 최적의 근사해를 찾는 것이 현실적인 목표가 된다.</p>
<p>이러한 최적화 문제를 푸는 접근법은 크게 두 가지로 나뉜다. 첫째는 1차 최적화 기법으로, 대표적으로 경사 하강법(Gradient Descent)이 있다. 이 방법은 목적 함수의 1차 미분 정보인 그래디언트(gradient)만을 사용하여 함수의 값이 가장 가파르게 감소하는 방향으로 파라미터를 점진적으로 업데이트한다.8 구현이 매우 간단하지만, 수렴 속도가 느리고 최적의 스텝 크기(학습률, learning rate)를 결정하기 어려운 단점이 있다.10</p>
<p>둘째는 2차 최적화 기법으로, 뉴턴 방법(Newton’s method)이 대표적이다. 이 방법은 1차 미분 정보뿐만 아니라 2차 미분 정보인 헤시안(Hessian) 행렬까지 활용하여 목적 함수를 국소적으로 2차 함수로 근사한다.8 이 2차 모델의 최소점을 다음 스텝으로 삼기 때문에, 해 근처에서 매우 빠른 2차 수렴(quadratic convergence)이 가능하다.12 하지만 매 반복마다 헤시안 행렬을 계산하고 그 역행렬을 구해야 하므로 계산 비용이 매우 크다는 치명적인 단점을 가진다.</p>
<p>여기서 비선형 최소제곱 문제의 구조적 특성이 중요해진다. 일반적인 최적화 문제와 달리, NLS의 목적 함수 <span class="math math-inline">S(\boldsymbol{\beta})</span>는 ’잔차 제곱의 합’이라는 매우 구체적인 형태를 띤다. 가우스-뉴턴 방법의 독창성은 바로 이 구조적 특성을 깊이 파고들어, 2차 최적화의 핵심인 헤시안 행렬을 직접 계산하지 않고도 유사한 효과를 얻는 길을 찾아냈다는 점에 있다. 이 접근법의 논리적 흐름은 다음과 같다. 먼저, 일반적인 함수 <span class="math math-inline">g(\mathbf{x})</span>를 최적화하는 뉴턴 방법은 헤시안 <span class="math math-inline">H_g</span>를 필요로 한다. NLS 문제의 목적 함수 <span class="math math-inline">S(\boldsymbol{\beta}) = \mathbf{r}^T\mathbf{r}</span>의 헤시안을 직접 계산해보면, 그 구조가 <span class="math math-inline">2\mathbf{J}^T\mathbf{J} + (\text{2차 미분 항})</span> 형태로 나타남을 알 수 있다.2 여기서</p>
<p><span class="math math-inline">\mathbf{J}</span>는 잔차 벡터 <span class="math math-inline">\mathbf{r}</span>의 1차 미분인 자코비안(Jacobian) 행렬이다. 이는 목적 함수 <span class="math math-inline">S</span>의 2차 정보(헤시안) 중 상당 부분이 잔차 <span class="math math-inline">\mathbf{r}</span>의 1차 정보(<span class="math math-inline">\mathbf{J}</span>)만으로 구성된다는 중요한 사실을 시사한다. 이로부터 “만약 2차 미분 항을 무시할 수 있다면, 1차 미분 정보만으로 2차 최적화와 유사한 성능을 낼 수 있지 않을까?“라는 혁신적인 아이디어가 도출되며, 이것이 바로 가우스-뉴턴 방법의 출발점이 된다.</p>
<h2>3.  가우스-뉴턴 방법의 수학적 원리와 유도</h2>
<p>가우스-뉴턴 방법의 핵심 아이디어는 복잡한 비선형 목적 함수 <span class="math math-inline">S(\boldsymbol{\beta})</span>를 직접 다루는 대신, 그 구성 요소인 비선형 잔차 함수 <span class="math math-inline">\mathbf{r}(\boldsymbol{\beta})</span>를 현재 파라미터 추정치 <span class="math math-inline">\boldsymbol{\beta}_k</span> 근방에서 1차 테일러 급수를 이용해 선형 함수로 근사하는 것이다.1 파라미터의 작은 변화량 <span class="math math-inline">\Delta\boldsymbol{\beta}</span>에 대한 잔차의 변화는 다음과 같이 근사할 수 있다.</p>
<p><span class="math math-display">
\mathbf{r}(\boldsymbol{\beta}_k + \Delta\boldsymbol{\beta}) \approx \mathbf{r}(\boldsymbol{\beta}_k) + \mathbf{J}(\boldsymbol{\beta}_k) \Delta\boldsymbol{\beta}
</span></p>
<p>여기서 <span class="math math-inline">\mathbf{J}(\boldsymbol{\beta}_k)</span>는 현재 추정치 <span class="math math-inline">\boldsymbol{\beta}_k</span>에서 계산된 잔차 함수 <span class="math math-inline">\mathbf{r}</span>의 자코비안 행렬이며, 그 원소는 <span class="math math-inline">(J)_{ij} = \frac{\partial r_i}{\partial \beta_j}</span>로 정의된다.6</p>
<p>이 선형 근사식을 원래의 목적 함수 <span class="math math-inline">S(\boldsymbol{\beta}) = ||\mathbf{r}(\boldsymbol{\beta})||_2^2</span>에 대입하면, 다음 파라미터 업데이트 스텝 <span class="math math-inline">\Delta\boldsymbol{\beta}</span>를 찾기 위한 선형 최소제곱 문제를 얻게 된다.1</p>
<p><span class="math math-display">
\min_{\Delta\boldsymbol{\beta}} ||\mathbf{r}(\boldsymbol{\beta}_k) + \mathbf{J}(\boldsymbol{\beta}_k) \Delta\boldsymbol{\beta}||_2^2
</span></p>
<p>이 문제는 <span class="math math-inline">\Delta\boldsymbol{\beta}</span>에 대한 2차 함수이므로, 이를 최소화하는 해는 목적 함수를 <span class="math math-inline">\Delta\boldsymbol{\beta}</span>에 대해 미분하여 그 결과가 0이 되는 지점에서 찾을 수 있다.1 미분 과정은 다음과 같다.</p>
<p><span class="math math-display">
\frac{\partial}{\partial \Delta\boldsymbol{\beta}} ||\mathbf{r} + \mathbf{J} \Delta\boldsymbol{\beta}||_2^2 = \frac{\partial}{\partial \Delta\boldsymbol{\beta}} (\mathbf{r} + \mathbf{J} \Delta\boldsymbol{\beta})^T (\mathbf{r} + \mathbf{J} \Delta\boldsymbol{\beta}) = 2\mathbf{J}^T(\mathbf{r} + \mathbf{J} \Delta\boldsymbol{\beta}) = \mathbf{0}
</span></p>
<p>이 식을 <span class="math math-inline">\Delta\boldsymbol{\beta}</span>에 대해 정리하면, 가우스-뉴턴 방법의 핵심 업데이트 규칙인 정규방정식(Normal Equations)이 도출된다.1</p>
<p><span class="math math-display">
(\mathbf{J}^T \mathbf{J}) \Delta\boldsymbol{\beta} = -\mathbf{J}^T \mathbf{r}
</span></p>
<p>이 정규방정식은 <span class="math math-inline">\Delta\boldsymbol{\beta}</span>에 대한 선형 연립방정식이다. 만약 <span class="math math-inline">\mathbf{J}^T \mathbf{J}</span> 행렬이 가역(invertible)이라면, 업데이트 스텝 <span class="math math-inline">\Delta\boldsymbol{\beta}</span>는 다음과 같이 직접 계산할 수 있다.</p>
<p><span class="math math-display">
\Delta\boldsymbol{\beta} = -(\mathbf{J}^T \mathbf{J})^{-1} \mathbf{J}^T \mathbf{r}
</span></p>
<p>이러한 수학적 원리에 기반한 가우스-뉴턴 알고리즘의 단계별 절차는 다음과 같다.3</p>
<ol>
<li>
<p><strong>초기화:</strong> 파라미터의 초기 추정치 <span class="math math-inline">\boldsymbol{\beta}_0</span>와 수렴 판정을 위한 허용오차 <span class="math math-inline">\epsilon</span>을 설정한다.</p>
</li>
<li>
<p>반복: 다음 과정을 수렴할 때까지 반복한다.</p>
</li>
</ol>
<p>a.  현재 파라미터 <span class="math math-inline">\boldsymbol{\beta}_k</span>에서 잔차 벡터 <span class="math math-inline">\mathbf{r}(\boldsymbol{\beta}_k)</span>와 자코비안 행렬 <span class="math math-inline">\mathbf{J}(\boldsymbol{\beta}_k)</span>를 계산한다.</p>
<p>b.  정규방정식 <span class="math math-inline">(\mathbf{J}(\boldsymbol{\beta}_k)^T \mathbf{J}(\boldsymbol{\beta}_k)) \Delta\boldsymbol{\beta}_k = -\mathbf{J}(\boldsymbol{\beta}_k)^T \mathbf{r}(\boldsymbol{\beta}_k)</span>를 풀어 업데이트 스텝 <span class="math math-inline">\Delta\boldsymbol{\beta}_k</span>를 구한다.</p>
<p>c.  파라미터를 업데이트한다: <span class="math math-inline">\boldsymbol{\beta}_{k+1} = \boldsymbol{\beta}_k + \Delta\boldsymbol{\beta}_k</span>.</p>
<ol start="3">
<li><strong>종료 조건:</strong> 업데이트 스텝의 크기 <span class="math math-inline">||\Delta\boldsymbol{\beta}_k||</span>가 미리 설정한 허용오차 <span class="math math-inline">\epsilon</span>보다 작아지거나, 목적 함수 <span class="math math-inline">S(\boldsymbol{\beta})</span>의 감소량이 매우 작아지면 반복을 중단하고 <span class="math math-inline">\boldsymbol{\beta}_{k+1}</span>을 최적해로 간주한다.</li>
</ol>
<p>이 과정은 본질적으로 다루기 힘든 비선형 최적화 문제를 매 단계마다 풀기 쉬운 선형 대수 문제로 변환하는 ‘분할 정복(Divide and Conquer)’ 전략의 한 형태로 볼 수 있다. 즉, 전체 문제의 비선형성을 직접 다루는 대신, 현재 지점에서 가장 적합한 선형 모델을 만들고 그 모델의 최적해를 향해 한 걸음 나아가는 과정을 반복함으로써 점진적으로 해에 접근하는 것이다. 이 변환 과정이야말로 가우스-뉴턴 알고리즘의 정수라 할 수 있다.</p>
<h2>4.  뉴턴 방법과의 비교 분석: 헤시안 행렬 근사의 본질</h2>
<p>가우스-뉴턴 방법의 본질을 가장 명확하게 이해하는 방법은 이를 일반적인 2차 최적화 기법인 뉴턴 방법과 비교하는 것이다. 일반적인 함수 <span class="math math-inline">S(\boldsymbol{\beta})</span>를 최소화하기 위한 뉴턴 방법의 업데이트 스텝 <span class="math math-inline">\Delta\boldsymbol{\beta}_{\text{Newton}}</span>은 목적 함수의 그래디언트 <span class="math math-inline">\mathbf{g} = \nabla S</span>와 헤시안 행렬 <span class="math math-inline">\mathbf{H} = \nabla^2 S</span>를 이용하여 다음과 같이 정의된다.8</p>
<p><span class="math math-display">
\Delta\boldsymbol{\beta}_{\text{Newton}} = -\mathbf{H}^{-1} \mathbf{g}
</span></p>
<p>비선형 최소제곱 문제의 목적 함수 <span class="math math-inline">S(\boldsymbol{\beta}) = \mathbf{r}(\boldsymbol{\beta})^T \mathbf{r}(\boldsymbol{\beta})</span>에 대해 그래디언트와 헤시안을 직접 계산하면 다음과 같은 형태를 얻는다.2</p>
<ul>
<li><strong>그래디언트:</strong> <span class="math math-inline">\mathbf{g} = \nabla S = 2\mathbf{J}^T \mathbf{r}</span></li>
<li><strong>헤시안:</strong> <span class="math math-inline">\mathbf{H} = \nabla^2 S = 2\mathbf{J}^T \mathbf{J} + 2\sum_{i=1}^{m} r_i(\boldsymbol{\beta}) \nabla^2 r_i(\boldsymbol{\beta})</span></li>
</ul>
<p>여기서 <span class="math math-inline">\mathbf{J}</span>는 잔차 벡터 <span class="math math-inline">\mathbf{r}</span>의 자코비안이고, <span class="math math-inline">\nabla^2 r_i</span>는 개별 잔차 함수 <span class="math math-inline">r_i</span>의 헤시안 행렬이다. 뉴턴 방법은 이 완전한 헤시안 행렬 <span class="math math-inline">\mathbf{H}</span>를 사용해야 한다.</p>
<p>반면, 가우스-뉴턴 방법은 위 헤시안 표현식에서 두 번째 항, 즉 잔차 <span class="math math-inline">r_i</span>와 잔차의 2차 미분(<span class="math math-inline">\nabla^2 r_i</span>)의 곱으로 이루어진 항을 의도적으로 무시한다. 이 항을 생략함으로써 얻어지는 근사 헤시안 <span class="math math-inline">\mathbf{H}_{\text{GN}} = 2\mathbf{J}^T \mathbf{J}</span>를 뉴턴 방법의 업데이트 규칙에 대입하면 정확히 가우스-뉴턴의 정규방정식이 유도된다.2</p>
<p><span class="math math-display">
\Delta\boldsymbol{\beta}_{\text{GN}} = -(2\mathbf{J}^T \mathbf{J})^{-1} (2\mathbf{J}^T \mathbf{r}) = -(\mathbf{J}^T \mathbf{J})^{-1} \mathbf{J}^T \mathbf{r}
</span></p>
<p>이 헤시안 근사는 가우스-뉴턴 방법의 모든 특성을 결정하는 핵심적인 단계이다. 이 근사가 가져오는 장점과 단점은 명확하다.</p>
<ul>
<li><strong>장점:</strong> 가장 큰 장점은 계산 효율성이다. 복잡하고 비용이 많이 드는 2차 미분(<span class="math math-inline">\nabla^2 r_i</span>)을 계산할 필요가 전혀 없어진다.18 오직 1차 미분 정보인 자코비안만으로 2차 최적화에 근접하는 성능을 낼 수 있는 잠재력을 갖게 된다. 또한, 자코비안</li>
</ul>
<p><span class="math math-inline">\mathbf{J}</span>가 full rank를 가지면 <span class="math math-inline">\mathbf{J}^T \mathbf{J}</span>는 항상 양의 정부호(positive definite) 행렬이 된다. 이는 뉴턴 방법에서 헤시안이 양의 정부호가 아닐 경우 업데이트 방향이 상승 방향이 될 수 있는 문제와 달리, 가우스-뉴턴의 업데이트 방향은 항상 목적 함수를 감소시키는 하강 방향(descent direction)임을 보장해준다.19</p>
<ul>
<li><strong>단점:</strong> 이 근사는 특정 조건 하에서만 유효하다. 무시된 항 <span class="math math-inline">2\sum r_i \nabla^2 r_i</span>의 크기가 작아야 근사 헤시안이 실제 헤시안과 유사해진다. 이 항의 크기는 두 가지 요인에 의해 결정된다. 첫째, 최적해 근방에서 잔차 <span class="math math-inline">r_i</span>가 0에 매우 가까운 경우(small-residual problems). 둘째, 잔차 함수 <span class="math math-inline">r_i</span> 자체의 비선형성(즉, 곡률을 나타내는 <span class="math math-inline">\nabla^2 r_i</span>)이 작은 경우. 만약 잔차가 크거나(large-residual problems) 비선형성이 강하면 실제 헤시안과의 오차가 커져 수렴 속도가 현저히 저하되거나 최악의 경우 발산할 수 있다.17</li>
</ul>
<p>결국 가우스-뉴턴은 ‘최적화 문제의 구조적 특성을 이용한 의도적인 정보 손실’ 전략으로 해석할 수 있다. 뉴턴 방법이 요구하는 완전한 2차 정보 대신, 계산하기 쉬운 1차 정보(<span class="math math-inline">\mathbf{J}</span>)로 구성된 ‘근사 2차 정보’(<span class="math math-inline">\mathbf{J}^T\mathbf{J}</span>)를 사용하는 것이다. 이 트레이드오프의 성공 여부는 ’버려진 정보’의 중요성에 달려 있으며, 이는 가우스-뉴턴 알고리즘의 성능이 해결하고자 하는 문제 자체의 특성에 깊이 의존함을 의미한다.</p>
<h2>5.  수렴성 분석과 알고리즘의 한계</h2>
<p>가우스-뉴턴 방법의 성능을 평가하기 위해서는 수렴 속도와 조건을 이해하고, 알고리즘이 실패할 수 있는 한계점을 명확히 인지하는 것이 중요하다.</p>
<h3>5.1 수렴 속도와 조건</h3>
<p>가우스-뉴턴의 수렴 속도는 헤시안 근사의 정확도에 직접적으로 의존하며, 이는 최적해에서의 잔차 크기에 의해 결정된다.</p>
<ul>
<li><strong>2차 수렴 (Quadratic Convergence):</strong> 만약 최적해 <span class="math math-inline">\boldsymbol{\beta}^*</span>에서 모든 잔차가 0인 경우(<span class="math math-inline">\mathbf{r}(\boldsymbol{\beta}^*) = \mathbf{0}</span>), 즉 모델이 데이터를 완벽하게 설명하는 경우, 헤시안의 근사 오차 항(<span class="math math-inline">2\sum r_i \nabla^2 r_i</span>)이 0이 된다. 이 경우 가우스-뉴턴의 근사 헤시안은 실제 헤시안과 정확히 일치하게 되므로, 알고리즘은 뉴턴 방법과 동일하게 동작하며 2차 수렴이라는 매우 빠른 수렴 속도를 보인다.16</li>
<li><strong>선형 수렴 (Linear Convergence):</strong> 반면, 최적해에서 잔차가 0이 아닌 경우(large-residual problem), 근사 오차 항이 무시할 수 없는 영향을 미치게 된다. 이 오차 항으로 인해 수렴 속도는 2차에서 선형으로 저하된다. 수렴 속도가 느려지는 정도는 잔차의 크기와 문제의 비선형성에 비례한다.16</li>
</ul>
<p>이러한 수렴이 보장되기 위해서는 몇 가지 전제 조건이 충족되어야 한다.16 첫째, 초기 추정치  <span class="math math-inline">\boldsymbol{\beta}_0</span>가 최적해에 충분히 가까워야 국소적 수렴이 보장된다.1 둘째, 매 반복에서 자코비안 행렬 <span class="math math-inline">\mathbf{J}</span>가 full column rank를 가져야 한다. 이는 <span class="math math-inline">\mathbf{J}^T\mathbf{J}</span>가 가역 행렬임을 보장하며, 모델의 파라미터들이 데이터를 통해 잘 식별됨을 의미한다. 마지막으로, 앞서 언급했듯이 최적해에서의 잔차 크기가 작아야 빠른 수렴을 기대할 수 있다.</p>
<h3>5.2 알고리즘의 한계점</h3>
<p>가우스-뉴턴 방법은 효율적인 만큼 여러 내재적 한계점을 가지고 있다.</p>
<ul>
<li><strong>특이성 및 불량 조건 (Singularity and Ill-conditioning):</strong> 가장 치명적인 문제로, <span class="math math-inline">\mathbf{J}^T\mathbf{J}</span> 행렬이 특이(singular, rank-deficient)하거나 불량 조건(ill-conditioned)일 때 발생한다. 이 경우 역행렬 계산이 수학적으로 불가능하거나 수치적으로 매우 불안정해져, 계산된 업데이트 스텝 <span class="math math-inline">\Delta\boldsymbol{\beta}</span>가 비정상적으로 커지면서 알고리즘이 발산하게 된다.16 이는 모델의 파라미터 간에 선형 종속성이 존재하거나(과매개변수화), 주어진 데이터가 모든 파라미터를 결정하기에 불충분할 때 발생할 수 있다.</li>
<li><strong>발산 문제:</strong> 초기 추정치가 해에서 너무 멀리 떨어져 있거나, 문제의 비선형성이 매우 강할 경우, 1차 테일러 근사가 더 이상 유효하지 않게 된다. 이로 인해 계산된 스텝이 목적 함수를 오히려 증가시키는 방향으로 나아가 알고리즘이 발산할 수 있다.20</li>
<li><strong>극점 구분 불가:</strong> 뉴턴 계열 방법의 공통적인 문제로, 이 알고리즘은 그래디언트가 0인 지점(정류점, stationary point)을 찾을 뿐이다. 따라서 수렴한 지점이 우리가 원하는 극소점인지, 아니면 극대점이나 안장점(saddle point)인지 보장하지 않는다.23</li>
</ul>
<p>이러한 특징들을 종합하여 주요 최적화 알고리즘들을 비교하면 다음 표와 같다.</p>
<p><strong>Table 1: 주요 최적화 알고리즘 비교</strong></p>
<table><thead><tr><th>특징</th><th>경사 하강법 (Gradient Descent)</th><th>뉴턴 방법 (Newton’s Method)</th><th>가우스-뉴턴 (Gauss-Newton)</th><th>레벤버그-마르카르트 (Levenberg-Marquardt)</th></tr></thead><tbody>
<tr><td><strong>기본 아이디어</strong></td><td>그래디언트 반대 방향으로 이동</td><td>2차 근사 모델의 최소점으로 이동</td><td>잔차의 선형 근사 모델의 최소점으로 이동</td><td>GN과 GD의 적응적 결합 (신뢰 영역)</td></tr>
<tr><td><strong>업데이트 규칙</strong></td><td><span class="math math-inline">\Delta\mathbf{x} = -\alpha \mathbf{g}</span></td><td><span class="math math-inline">\Delta\mathbf{x} = -\mathbf{H}^{-1} \mathbf{g}</span></td><td><span class="math math-inline">\Delta\mathbf{x} = -(\mathbf{J}^T\mathbf{J})^{-1}\mathbf{J}^T\mathbf{r}</span></td><td><span class="math math-inline">\Delta\mathbf{x} = -(\mathbf{J}^T\mathbf{J}+\lambda\mathbf{I})^{-1}\mathbf{J}^T\mathbf{r}</span></td></tr>
<tr><td><strong>필요 정보</strong></td><td>1차 미분 (Gradient)</td><td>1차, 2차 미분 (Hessian)</td><td>1차 미분 (Jacobian)</td><td>1차 미분 (Jacobian)</td></tr>
<tr><td><strong>수렴 속도</strong></td><td>선형 (느림)</td><td>2차 (매우 빠름)</td><td>2차 (잔차 작을 시) / 선형 (잔차 클 시)</td><td>GN과 유사하나 더 안정적</td></tr>
<tr><td><strong>계산 비용/반복</strong></td><td>낮음</td><td>매우 높음 (Hessian 계산 및 역행렬)</td><td>중간 (Jacobian 계산 및 역행렬)</td><td>중간 (GN과 유사)</td></tr>
<tr><td><strong>강인성/안정성</strong></td><td>학습률에 따라 비교적 안정적</td><td>불안정 (H가 양의 정부호 아닐 시)</td><td>불안정 (<span class="math math-inline">\mathbf{J}^T\mathbf{J}</span>가 특이 행렬일 시)</td><td>매우 높음</td></tr>
<tr><td><strong>주요 장점</strong></td><td>구현 용이, 낮은 메모리</td><td>빠른 수렴 속도</td><td>NLS에 특화된 효율성</td><td>강인성, 넓은 수렴 반경</td></tr>
<tr><td><strong>주요 단점</strong></td><td>느린 수렴, 학습률 민감</td><td>높은 계산 비용, 수렴 보장 안됨</td><td>Large-residual 문제 및 특이 행렬에 취약</td><td>파라미터(<span class="math math-inline">\lambda</span>) 튜닝 필요</td></tr>
</tbody></table>
<p>이 표는 각 알고리즘이 ’속도-비용-안정성’이라는 다차원적 공간에서 어떤 위치를 차지하는지 명확히 보여준다. 가우스-뉴턴은 뉴턴 방법의 ’속도’와 경사 하강법의 ‘비용’ 사이에서 NLS 문제에 최적화된 지점을 찾으려 시도하며, 레벤버그-마르카르트는 여기에 ’안정성’을 더한 진화된 형태임을 직관적으로 이해할 수 있게 한다.</p>
<h2>6.  개선 알고리즘: 강인성과 효율성을 향하여</h2>
<p>가우스-뉴턴 방법의 내재적 한계를 극복하고 더 넓은 범위의 문제에 적용하기 위해 다양한 개선 알고리즘이 개발되었다. 이들은 주로 강인성(robustness)과 대규모 문제에 대한 확장성(scalability)을 높이는 데 초점을 맞춘다.</p>
<h3>6.1 라인 서치 (Line Search)</h3>
<p>가우스-뉴턴이 계산한 업데이트 방향 <span class="math math-inline">\Delta\boldsymbol{\beta}</span>은 올바르지만, 그 스텝의 크기가 너무 커서 오히려 목적 함수 값을 증가시키며 발산하는 경우가 있다. 라인 서치는 이를 방지하기 위한 간단하면서도 효과적인 전략이다. 업데이트 규칙을 <span class="math math-inline">\boldsymbol{\beta}_{k+1} = \boldsymbol{\beta}_k + \alpha \Delta\boldsymbol{\beta}_k</span>로 수정하고, 목적 함수 <span class="math math-inline">S</span>를 실제로 감소시키는 최적의 스텝 크기(step size) <span class="math math-inline">\alpha</span> (단, <span class="math math-inline">\alpha &gt; 0</span>)를 찾는 과정을 추가한다.16 이를 통해 알고리즘이 너무 과감한 스텝을 밟지 않도록 제어하여 수렴 안정성을 높일 수 있다.</p>
<h3>6.2 레벤버그-마르카르트 (Levenberg-Marquardt, LM) 알고리즘</h3>
<p>레벤버그-마르카르트 알고리즘은 가우스-뉴턴의 가장 성공적이고 널리 사용되는 개선안으로, ’감쇠 최소제곱법(Damped Least Squares)’으로도 불린다.13 이 방법은 가우스-뉴턴의 <span class="math math-inline">\mathbf{J}^T\mathbf{J}</span> 행렬이 특이하거나 불량 조건일 때 발생하는 문제를 해결하기 위해 정규방정식에 감쇠 인자(damping factor) <span class="math math-inline">\lambda</span>를 추가한다.25</p>
<p><span class="math math-display">
(\mathbf{J}^T \mathbf{J} + \lambda \mathbf{I}) \Delta\boldsymbol{\beta} = -\mathbf{J}^T \mathbf{r}
</span></p>
<p>여기서 <span class="math math-inline">\mathbf{I}</span>는 항등 행렬이며, <span class="math math-inline">\lambda</span>는 음이 아닌 스칼라 값이다. 이 감쇠 인자 <span class="math math-inline">\lambda</span>의 역할은 알고리즘의 동작을 동적으로 조절하는 것이다.</p>
<ul>
<li><span class="math math-inline">\lambda \to 0</span>: 감쇠 항의 영향이 사라져 원래의 가우스-뉴턴 방법에 가까워진다. 이는 현재의 2차 근사 모델을 신뢰하고 해를 향해 빠르게 수렴하려는 시도이다.25</li>
<li><span class="math math-inline">\lambda \to \infty</span>: <span class="math math-inline">\mathbf{J}^T\mathbf{J}</span>에 비해 <span class="math math-inline">\lambda\mathbf{I}</span>가 지배적이 되어, 업데이트 스텝은 <span class="math math-inline">\Delta\boldsymbol{\beta} \approx -\frac{1}{\lambda}\mathbf{J}^T\mathbf{r}</span>가 된다. 이는 그래디언트 <span class="math math-inline">\mathbf{g} = 2\mathbf{J}^T\mathbf{r}</span>에 비례하므로, 경사 하강법과 유사하게 동작한다. 이는 현재 모델을 불신하고, 가장 안전한 하강 방향으로 보수적인 스텝을 취하는 것을 의미한다.27</li>
</ul>
<p>LM 알고리즘은 신뢰 영역(Trust Region) 방법의 한 형태로도 해석될 수 있다. 즉, 현재의 선형 근사가 유효하다고 믿는 ‘신뢰 영역’ 반경 내에서만 해를 찾는 제약된 최적화 문제를 푸는 것과 같다.13</p>
<p><span class="math math-inline">\lambda</span>는 이 신뢰 영역의 크기를 간접적으로 조절하는 역할을 한다. 매 반복마다 실제 목적 함수 감소량과 모델이 예측한 감소량을 비교하여 그 비율(<span class="math math-inline">\rho</span>)을 계산하고, 이 비율에 따라 <span class="math math-inline">\lambda</span> 값을 동적으로 조절한다. 이를 통해 알고리즘은 해에서 멀리 있을 때는 경사 하강법처럼 안정적으로 움직이고, 해에 가까워지면 가우스-뉴턴처럼 빠르게 수렴하는 지능적인 전환을 수행한다.</p>
<h3>6.3 대규모 문제를 위한 기법</h3>
<p>수만 개 이상의 파라미터를 가진 대규모 문제(예: 번들 조정)에서는 <span class="math math-inline">\mathbf{J}^T\mathbf{J}</span> 행렬을 메모리에 명시적으로 구성하고 그 역행렬을 구하는 것이 계산적으로 불가능하다. 이러한 문제를 해결하기 위해 정규방정식을 근사적으로 푸는 기법들이 개발되었다.</p>
<ul>
<li><strong>켤레 기울기법 (Conjugate Gradient, CG):</strong> 정규방정식과 같은 거대한 선형 시스템 <span class="math math-inline">(\mathbf{A}\mathbf{x}=\mathbf{b})</span>을 직접적인 행렬 분해로 풀지 않고, 반복적인 방법으로 근사해를 구하는 알고리즘이다. 특히 시스템 행렬 <span class="math math-inline">\mathbf{A}</span>(<span class="math math-inline">\mathbf{J}^T\mathbf{J}</span>에 해당)가 희소(sparse)할 때 매우 효율적이다.2</li>
<li><strong>절단 가우스-뉴턴 (Truncated Gauss-Newton):</strong> CG 반복을 수렴할 때까지 수행하지 않고, 일정 수준의 정확도에 도달하면 중간에 중단하여 각 가우스-뉴턴 반복의 계산 비용을 크게 절감하는 기법이다. 이는 해에서 멀리 있을 때 굳이 정확한 스텝을 계산할 필요가 없다는 직관에 기반한다.2</li>
</ul>
<p>이러한 개선 알고리즘들의 발전 과정은 최적화 알고리즘 개발의 보편적인 패턴을 보여준다. 즉, 기본 알고리즘의 수렴 실패 사례를 분석하고, 이를 막기 위한 안전장치(라인 서치, 감쇠)를 추가하여 강인성을 높인 후, 대규모 문제에 적용하기 위해 계산 병목 지점(선형 시스템 풀이)을 근사적인 방법으로 대체하여 확장성을 확보하는 방향으로 진화한다.</p>
<h2>7.  응용 사례 연구: 컴퓨터 비전과 로보틱스</h2>
<p>가우스-뉴턴 방법과 그 변형들은 이론적인 중요성을 넘어, 컴퓨터 비전과 로보틱스 분야의 핵심적인 대규모 최적화 문제를 해결하는 데 필수적인 도구로 사용된다. 특히 번들 조정(Bundle Adjustment)과 그래프 기반 SLAM은 가우스-뉴턴 원리가 문제의 구조와 결합하여 어떻게 강력한 성능을 발휘하는지를 보여주는 대표적인 사례이다.</p>
<h3>7.1  번들 조정 (Bundle Adjustment, BA)</h3>
<p>번들 조정은 여러 장의 이미지에서 관측된 2D 특징점들의 위치 정보를 이용하여, 3D 공간상에 존재하는 점들의 3차원 좌표와 각 이미지를 촬영한 카메라의 위치/자세(외부 파라미터) 및 초점 거리 등(내부 파라미터)을 동시에 정밀하게 최적화하는 문제이다.30 이는 다중 시점 영상으로부터 3차원 구조를 복원하는 SfM(Structure from Motion)이나 로봇이 자신의 위치를 추정하며 지도를 작성하는 SLAM의 마지막 단계에서 전역적인 일관성을 확보하기 위한 핵심 후처리 과정으로 사용된다.32</p>
<p>BA의 비용 함수는 재투영 오차(reprojection error)의 제곱합으로 정의된다. 즉, 추정된 3D 점을 추정된 카메라 모델을 통해 2D 이미지 평면에 투영시킨 좌표(예측값)와, 이미지에서 실제 관측된 2D 특징점 좌표(관측값) 사이의 유클리드 거리 제곱합을 최소화하는 것이다.31</p>
<p><span class="math math-display">
\min_{\mathbf{C}_j, \mathbf{X}_i} \sum_{i} \sum_{j} v_{ij} ||\mathbf{u}_{ij} - \pi(\mathbf{C}_j, \mathbf{X}_i)||^2
</span></p>
<p>여기서 <span class="math math-inline">\mathbf{C}_j</span>는 <span class="math math-inline">j</span>번째 카메라의 파라미터, <span class="math math-inline">\mathbf{X}_i</span>는 <span class="math math-inline">i</span>번째 3D 점의 좌표, <span class="math math-inline">\pi(\cdot)</span>는 투영 함수, <span class="math math-inline">\mathbf{u}_{ij}</span>는 관측된 2D 좌표, <span class="math math-inline">v_{ij}</span>는 점 <span class="math math-inline">i</span>가 카메라 <span class="math math-inline">j</span>에서 보이는지 여부를 나타내는 이진 변수이다.</p>
<p>이 문제의 가장 중요한 특징은 자코비안과 헤시안(<span class="math math-inline">\mathbf{J}^T\mathbf{J}</span>) 행렬이 매우 거대하지만, 동시에 극도로 희소(sparse)한 블록 구조를 가진다는 점이다.32 그 이유는 하나의 재투영 오차 항 <span class="math math-inline">r_{ij} = \mathbf{u}_{ij} - \pi(\mathbf{C}_j, \mathbf{X}_i)</span>이 수많은 파라미터 중 오직 하나의 카메라 파라미터 <span class="math math-inline">\mathbf{C}_j</span>와 하나의 3D 점 파라미터 <span class="math math-inline">\mathbf{X}_i</span>에만 의존하기 때문이다.</p>
<p>이러한 희소 구조를 효과적으로 활용하기 위해 <strong>슈어 보수(Schur Complement) 트릭</strong>이 사용된다. 정규방정식을 카메라 파라미터와 3D 점 파라미터에 대한 블록 행렬로 재배열한 후, 대수적 조작을 통해 3D 점 파라미터에 대한 항을 소거한다. 이를 통해 거대한 전체 시스템을 직접 푸는 대신, 상대적으로 크기가 훨씬 작은 ’카메라 시스템’을 먼저 풀고, 그 결과를 이용해 각 3D 점의 위치를 계산하는 방식으로 문제를 분리할 수 있다.32 이 기법은 문제의 물리적 구조(카메라와 점의 관계)가 수학적 구조(행렬의 희소성)로 이어지고, 이를 다시 효율적인 대수적 기법(슈어 보수)으로 풀어내는 완벽한 예시이다. 덕분에 수십만 개 이상의 파라미터를 가진 대규모 BA 문제도 현실적인 시간 내에 해결이 가능해졌다.</p>
<h3>7.2  그래프 기반 SLAM (Graph-based SLAM)</h3>
<p>그래프 기반 SLAM은 로봇의 이동 경로(일련의 자세들)와 환경 지도(랜드마크)를 하나의 거대한 그래프로 표현하여 SLAM 문제를 해결하는 접근법이다. 이 그래프에서 노드(vertex)는 특정 시점의 로봇 자세 또는 랜드마크의 위치를 나타내고, 엣지(edge)는 두 노드 간의 공간적 제약 조건을 모델링한다. 이러한 제약은 로봇의 움직임 측정(오도메트리)이나 랜드마크 관측으로부터 얻어진다.34 센서 측정에는 항상 노이즈가 포함되므로, 시간이 지남에 따라 오차가 누적되어 그래프 전체의 일관성이 깨지게 된다.</p>
<p>그래프 기반 SLAM의 목표는 이처럼 일관성이 깨진 그래프를, 모든 엣지 제약 조건을 통계적으로 가장 잘 만족시키는 노드 구성으로 바로잡는 것이다. 이는 측정된 제약과 현재 추정된 노드 위치 간의 오차 제곱합을 최소화하는 비선형 최소제곱 문제로 공식화된다.37</p>
<p><span class="math math-display">
\mathbf{x}^* = \arg\min_{\mathbf{x}} \sum_{i,j} \mathbf{e}_{ij}(\mathbf{x}_i, \mathbf{x}_j)^T \mathbf{\Omega}_{ij} \mathbf{e}_{ij}(\mathbf{x}_i, \mathbf{x}_j)
</span></p>
<p>여기서 <span class="math math-inline">\mathbf{x}</span>는 모든 노드(자세, 랜드마크)의 상태 벡터, <span class="math math-inline">\mathbf{e}_{ij}</span>는 엣지에 해당하는 오차 함수, <span class="math math-inline">\mathbf{\Omega}_{ij}</span>는 해당 측정의 신뢰도를 나타내는 정보 행렬(불확실성의 역행렬)이다.</p>
<p>가우스-뉴턴 방법은 이 최적화 문제를 푸는 데 핵심적인 역할을 한다.</p>
<ol>
<li>각 오차 함수 <span class="math math-inline">\mathbf{e}_{ij}</span>를 현재 상태 <span class="math math-inline">\mathbf{x}</span>에 대해 선형화하여 자코비안 <span class="math math-inline">\mathbf{J}_{ij}</span>를 구한다.34</li>
<li>모든 엣지에 대한 자코비안과 정보 행렬을 취합하여 거대한 희소 선형 시스템 <span class="math math-inline">\mathbf{H}\Delta\mathbf{x} = -\mathbf{b}</span>를 구축한다. 여기서 헤시안 근사 행렬 <span class="math math-inline">\mathbf{H} = \mathbf{J}^T\mathbf{\Omega J}</span>이고, 그래디언트 관련 벡터 <span class="math math-inline">\mathbf{b} = \mathbf{J}^T\mathbf{\Omega e}</span>이다.35</li>
<li><span class="math math-inline">\mathbf{H}</span> 행렬의 0이 아닌 원소들의 패턴, 즉 희소 패턴은 그래프의 연결 구조(인접 행렬)와 정확히 일치한다.34</li>
<li>이러한 희소성을 활용하는 희소 촐레스키 분해(Sparse Cholesky Decomposition)나 켤레 기울기법과 같은 효율적인 선형 대수 솔버를 사용하여 업데이트 스텝 <span class="math math-inline">\Delta\mathbf{x}</span>를 계산하고, 이를 통해 전체 그래프의 상태를 업데이트한다.</li>
</ol>
<p>BA와 SLAM 사례에서 가우스-뉴턴은 단순한 최적화 도구를 넘어, 문제의 물리적/기하학적 구조를 계산 가능한 수학적 형태로 변환하고 이를 효율적으로 해결하는 ‘언어’ 또는 ’프레임워크’의 역할을 수행한다. 알고리즘의 성공은 순수한 수치적 성능뿐만 아니라, 문제의 본질적인 구조를 얼마나 잘 활용하는지에 달려 있음을 명확히 보여준다.</p>
<h2>8. 결론</h2>
<p>가우스-뉴턴 방법은 비선형 최소제곱 문제의 목적 함수가 ’잔차 제곱의 합’이라는 독특한 구조를 가진다는 점을 예리하게 활용한 최적화 알고리즘이다. 이 구조적 특성을 통해, 계산 비용이 높은 헤시안 행렬의 2차 미분 항을 생략하고 1차 미분 정보(자코비안)만으로 2차 최적화에 근접하는 성능을 달성함으로써 효율성을 극대화했다.</p>
<p>이론적으로 가우스-뉴턴 방법은 잔차가 작은 문제에서 뉴턴 방법과 유사한 2차 수렴 속도를 보이며 매우 강력한 성능을 발휘한다. 하지만 현실에서 마주하는 많은 문제, 특히 잔차가 크거나 파라미터 식별이 어려운 문제에서는 수렴이 선형으로 저하되거나, 자코비안의 불량 조건으로 인해 수치적으로 불안정해져 발산할 수 있다는 명백한 한계를 지닌다. 이러한 이론과 현실의 간극은 가우스-뉴턴의 빠른 수렴성과 경사 하강법의 안정성을 지능적으로 결합한 레벤버그-마르카르트 알고리즘의 개발을 촉진하는 중요한 계기가 되었다.</p>
<p>현대적 관점에서 가우스-뉴턴의 원리는 그 자체로 사용되기보다는, 더 큰 문제 해결 프레임워크의 핵심적인 이론적 기반으로 작용한다. 특히 컴퓨터 비전의 번들 조정과 로보틱스의 SLAM과 같은 대규모 최적화 문제에서, 가우스-뉴턴의 기본 원리는 문제의 희소 구조를 활용하는 슈어 보수나 켤레 기울기법과 같은 고급 선형 대수 기법과 결합된다. 이를 통해 수십만, 수백만 개의 파라미터를 가진, 과거에는 해결이 불가능했던 문제들을 효율적으로 풀 수 있는 강력한 프레임워크로 확장된다. 결국 가우스-뉴턴 방법은 단순한 수치 해석 알고리즘을 넘어, 복잡한 실세계의 기하학적 문제를 풀 수 있는 수학적 모델로 변환하고 해결하는 현대 계산 과학의 중요한 패러다임 중 하나로 확고히 자리매김하고 있다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>[개념 정리] 가우스-뉴턴 (Gauss-Newton) 최적화 기법 - xoft, accessed July 23, 2025, https://xoft.tistory.com/90</li>
<li>Approximate Gauss-Newton methods for nonlinear least squares problems - University of Reading, accessed July 23, 2025, https://www.reading.ac.uk/maths-and-stats/-/media/project/uor-main/schools-departments/maths/documents/0904pdf.pdf?la=en&amp;hash=45BCE1B5038E631F3DCAF7450E46B114</li>
<li>Solving Nonlinear Least Squares Problem Using Gauss-Newton Method - IJISET, accessed July 23, 2025, https://ijiset.com/vol4/v4s1/IJISET_V4_I01_35.pdf</li>
<li>[개념 정리] Levenberg-Marquardt 알고리즘 : 최적화 기법 - xoft - 티스토리, accessed July 23, 2025, https://xoft.tistory.com/82</li>
<li>[기초수학] 정사영과 최소제곱문제 - MINJU’s code story - 티스토리, accessed July 23, 2025, https://minjyuus.tistory.com/11</li>
<li>Gauss-Newton Method Explained - Number Analytics, accessed July 23, 2025, https://www.numberanalytics.com/blog/gauss-newton-method-explained</li>
<li>최소제곱 문제(Least Squares Problem) - kwan’s note - 티스토리, accessed July 23, 2025, https://reminder-by-kwan.tistory.com/30</li>
<li>뉴턴방법 (Newton’s Method) - DeepCampus - 티스토리, accessed July 23, 2025, https://pasus.tistory.com/197</li>
<li>[딥러닝] Gradient Descent &amp; Newton Method - velog, accessed July 23, 2025, <a href="https://velog.io/@tjswodud/%EB%94%A5%EB%9F%AC%EB%8B%9D-Gradient-Descent-Newton-Method">https://velog.io/@tjswodud/%EB%94%A5%EB%9F%AC%EB%8B%9D-Gradient-Descent-Newton-Method</a></li>
<li>경사하강법 (gradient desecent) vs 뉴턴-랩슨 방법 (Newton–Raphson method), accessed July 23, 2025, https://ploradoaa.tistory.com/36</li>
<li>28~29단계) 경사하강법 , 뉴턴 방법 , 함수 최적화 - 채채씨의 학습 기록 - 티스토리, accessed July 23, 2025, https://amber-chaeeunk.tistory.com/62</li>
<li>Advanced Gradient Descent Method - New Sight - 티스토리, accessed July 23, 2025, https://newsight.tistory.com/224</li>
<li>Nonlinear Optimization (2) - Gaussian-Newton Method, Levernberg-Marquardt Method - JJukE’s Brain - 티스토리, accessed July 23, 2025, https://jjuke-brain.tistory.com/entry/Nonlinear-Optimization-2-Gaussian-Newton-Method-Levernberg-Marquardt-Method</li>
<li>표적기동분석을 위한 Levenberg-Marquardt 적용에 관한 연구, accessed July 23, 2025, https://koreascience.kr/article/JAKO201525550548181.pdf</li>
<li>뉴턴-랩슨법의 이해와 활용(Newton-Raphson method) - 다크 프로그래머, accessed July 23, 2025, https://darkpgmr.tistory.com/58</li>
<li>Gauss–Newton algorithm - Wikipedia, accessed July 23, 2025, <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm">https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm</a></li>
<li>1 Gauss-Newton - Computer Science Cornell, accessed July 23, 2025, https://www.cs.cornell.edu/courses/cs4220/2017sp/lec/2017-04-19.pdf</li>
<li>함수최적화 기법 정리 (Levenberg–Marquardt 방법 등) - 다크 프로그래머 - 티스토리, accessed July 23, 2025, https://darkpgmr.tistory.com/142</li>
<li>Gauss-Newton Method - NEOS Guide, accessed July 23, 2025, https://neos-guide.org/guide/algorithms/gauss-newton/</li>
<li>뉴턴의 방법: 미적분학으로 방정식 풀기 - FasterCapital, accessed July 23, 2025, <a href="https://fastercapital.com/ko/content/%EB%89%B4%ED%84%B4%EC%9D%98-%EB%B0%A9%EB%B2%95--%EB%AF%B8%EC%A0%81%EB%B6%84%ED%95%99%EC%9C%BC%EB%A1%9C-%EB%B0%A9%EC%A0%95%EC%8B%9D-%ED%92%80%EA%B8%B0.html">https://fastercapital.com/ko/content/%EB%89%B4%ED%84%B4%EC%9D%98-%EB%B0%A9%EB%B2%95–%EB%AF%B8%EC%A0%81%EB%B6%84%ED%95%99%EC%9C%BC%EB%A1%9C-%EB%B0%A9%EC%A0%95%EC%8B%9D-%ED%92%80%EA%B8%B0.html</a></li>
<li>최적화 이론 기초 정리 (Gradient Descent, Newton Method, Gauss …, accessed July 23, 2025, https://gaussian37.github.io/math-calculus-basic_optimization/</li>
<li>비선형 회귀 분석의 방법 - Minitab, accessed July 23, 2025, https://support.minitab.com/ko-kr/minitab/help-and-how-to/statistical-modeling/regression/how-to/nonlinear-regression/methods-and-formulas/methods/</li>
<li>Optimzation - Haehwan Lee, accessed July 23, 2025, https://haehwan.github.io/posts/DS-optimization/</li>
<li>Gradient Descent 탐색 방법 - 다크 프로그래머, accessed July 23, 2025, https://darkpgmr.tistory.com/133</li>
<li>Levenberg–Marquardt algorithm - Wikipedia, accessed July 23, 2025, <a href="https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm">https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm</a></li>
<li>Nonlinear least squares - Gauss-Newton - JAXopt, accessed July 23, 2025, https://jaxopt.github.io/stable/nonlinear_least_squares.html</li>
<li>The Levenberg-Marquardt algorithm for nonlinear least squares curve-fitting problems - Duke People, accessed July 23, 2025, https://people.duke.edu/~hpgavin/lm.pdf</li>
<li>The Levenberg-Marquardt Algorithm, accessed July 23, 2025, https://sites.cs.ucsb.edu/~yfwang/courses/cs290i_mvg/pdf/LMA.pdf</li>
<li>Bundle adjustment for large problems - DiVA portal, accessed July 23, 2025, https://www.diva-portal.org/smash/get/diva2:1278268/FULLTEXT01.pdf</li>
<li>Bundle Adjustment in the Large - University of Washington, accessed July 23, 2025, https://homes.cs.washington.edu/~sagarwal/bal.pdf</li>
<li>Chapter 7 - Computer Vision Group, accessed July 23, 2025, https://cvg.cit.tum.de/_media/teaching/ss2019/mvg2019/material/multiviewgeometry7.pdf</li>
<li>[개념 정리] Bundle Adjustment - xoft - 티스토리, accessed July 23, 2025, https://xoft.tistory.com/91</li>
<li>Bundle Adjustment - everyday robotics - 티스토리, accessed July 23, 2025, https://zzziito.tistory.com/38</li>
<li>Robot Mapping Least Squares Approach to SLAM, accessed July 23, 2025, http://ais.informatik.uni-freiburg.de/teaching/ws12/mapping/pdf/slam15-ls-slam.pdf</li>
<li>Graph-based SLAM - Fan’s site, accessed July 23, 2025, https://yangfan.github.io/projects/SLAM/Graph-based-slam/</li>
<li>(PDF) A tutorial on graph-based SLAM - ResearchGate, accessed July 23, 2025, https://www.researchgate.net/publication/231575337_A_tutorial_on_graph-based_SLAM</li>
<li>Linear Algebra for Graph-Based SLAM in Robotics - Number Analytics, accessed July 23, 2025, https://www.numberanalytics.com/blog/linear-algebra-graph-based-slam-robotics</li>
<li>Analysis for Graph-Based SLAM Algorithms under g2o Framework - Carnegie Mellon University, accessed July 23, 2025, https://www.cs.cmu.edu/~tianxian/files/Analysis_for_Graph_Based_SLAM_Algorithms_under_g2o_Framework.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>