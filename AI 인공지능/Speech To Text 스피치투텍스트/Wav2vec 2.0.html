<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Wav2vec 2.0 (자기 지도 학습 기반 음성 표현)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Wav2vec 2.0 (자기 지도 학습 기반 음성 표현)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">Speech to Text</a> / <span>Wav2vec 2.0 (자기 지도 학습 기반 음성 표현)</span></nav>
                </div>
            </header>
            <article>
                <h1>Wav2vec 2.0 (자기 지도 학습 기반 음성 표현)</h1>
<h2>1. 서론: 자기 지도 학습을 통한 음성 표현의 혁신</h2>
<h3>1.1 전통적 자동 음성 인식(ASR)의 한계: 데이터 의존성</h3>
<p>전통적으로, 높은 성능의 자동 음성 인식(ASR) 시스템을 구축하는 과정은 방대한 양의 레이블링된 데이터, 즉 사람의 음성과 그에 해당하는 텍스트 전사(transcription) 데이터에 깊이 의존해왔다. 최첨단 시스템들은 수천 시간에 달하는 전사된 음성 데이터를 요구하는데 1, 이는 전 세계적으로 사용되는 약 7,000개의 언어 대다수에서는 현실적으로 확보하기 어려운 자원이다.1 이러한 데이터 의존성은 ASR 기술이 특정 언어 및 도메인에 편중되는 결과를 낳았으며, 기술 보급의 근본적인 병목 현상으로 작용했다.</p>
<p>더욱이, 레이블링된 데이터에만 의존하는 지도 학습(supervised learning) 방식은 인간이 언어를 습득하는 자연스러운 과정과 상당한 거리가 있다. 유아는 명시적인 지시나 교정 없이 주변 환경의 소리를 듣는 것만으로 언어의 구조와 표현을 학습한다.1 이 과정은 음성에 대한 양질의 표현(representation)을 먼저 내재화하고, 이를 바탕으로 구체적인 의미를 연결하는 방식으로 이루어진다. 이러한 인간의 학습 방식은 기계 학습 모델 역시 레이블이 없는 데이터로부터 내재적 구조를 스스로 학습할 필요성이 있음을 강력하게 시사한다.</p>
<h3>1.2 자기 지도 학습(SSL) 패러다임의 부상</h3>
<p>이러한 배경 속에서 자기 지도 학습(Self-Supervised Learning, SSL)이 강력한 대안으로 부상했다. SSL은 대규모의 레이블 없는 데이터를 활용하여 데이터 자체에 내재된 감독 신호(supervisory signal)를 통해 일반적인 데이터 표현을 학습하는 패러다임이다. 이후, 이렇게 사전 학습(pre-training)된 모델을 소량의 레이블 데이터를 사용하여 특정 과제에 맞게 미세 조정(fine-tuning)한다.1 이 방식은 데이터 레이블링에 드는 막대한 비용과 시간을 절감하면서도, 데이터의 풍부한 구조적 정보를 모델이 학습하도록 유도한다.</p>
<p>SSL 패러다임은 특히 자연어 처리(NLP) 분야에서 BERT(Bidirectional Encoder Representations from Transformers)와 같은 모델들을 통해 그 효용성이 입증되며 혁명적인 성공을 거두었다.1 BERT는 대규모 텍스트 코퍼스에서 문장의 일부 단어를 마스킹하고, 주변 문맥을 이용해 마스킹된 단어를 예측하는 과제를 통해 언어에 대한 깊은 양방향 문맥적 이해를 학습했다. 이러한 성공은 컴퓨터 비전 및 음성 처리와 같은 다른 분야의 연구자들에게 직접적인 영감을 주었으며, 각 데이터 양상(modality)의 고유한 특성에 맞는 SSL 방법론을 개발하려는 노력을 촉발시켰다.1</p>
<h3>1.3 Wav2vec 2.0의 등장과 그 의의</h3>
<p>이러한 흐름 속에서 2020년 Facebook AI Research(현 Meta AI)의 Alexei Baevski 연구팀은 Wav2vec 2.0을 발표하며 음성 처리 분야에 새로운 이정표를 세웠다. Wav2vec 2.0은 순수한 음성 오디오 데이터만으로 강력한 표현을 학습한 후, 소량의 전사된 데이터로 미세 조정하는 방식이 기존의 최첨단 준지도(semi-supervised) 방법론을 능가할 수 있음을 최초로 입증한 프레임워크이다.1</p>
<p>Wav2vec 2.0의 핵심적인 기여는 개념적으로 더 단순한 접근 방식을 통해 레이블링된 데이터에 대한 의존도를 획기적으로 낮추었다는 점에 있다. 실제로, 단 10분의 레이블 데이터와 53,000시간의 비레이블 데이터를 사용하여 사전 학습한 모델이 표준 벤치마크에서 기존의 최첨단 모델에 근접하는 우수한 성능을 달성했다.4 이는 데이터가 부족한 저자원(low-resource) 언어나 특정 도메인에서도 고성능 ASR 시스템 구축이 실현 가능함을 증명한 것으로, ASR 기술의 접근성을 크게 확장하는 계기가 되었다.1</p>
<p>Wav2vec 2.0의 성공은 ASR 연구의 패러다임을 근본적으로 전환시켰다. 과거의 연구가 ’어떻게 하면 더 많은 레이블 데이터를 수집할 것인가?’라는 질문에 집중했다면, Wav2vec 2.0 이후의 연구는 ’어떻게 하면 방대한 비레이블 데이터로부터 더 우수하고 일반화된 표현을 학습할 것인가?’라는 질문으로 그 초점을 옮겼다.1 이전까지 ASR 모델의 성능은 가용한 레이블 데이터의 양과 거의 정비례하는 경향을 보였으며, 이는 데이터 수집 및 레이블링 비용이 연구 개발의 가장 큰 장벽임을 의미했다. 그러나 Wav2vec 2.0이 극소량의 레이블 데이터만으로도 높은 성능을 달성할 수 있음을 보이면서, 성능의 핵심 변수가 레이블의 양이 아니라 사전 학습을 통해 획득한 ’표현의 질’에 있음이 명확해졌다. 결과적으로, 연구 및 개발에 투입되는 자원은 값비싼 레이블링 작업 대신, 더 많은 비레이블 데이터를 효과적으로 활용하여 더 우수한 SSL 알고리즘을 개발하는 방향으로 재분배될 수 있게 되었다. 이는 곧 ASR 기술의 민주화를 의미하며, 막대한 자본 없이도 다양한 저자원 언어나 특정 도메인에 특화된 고성능 ASR 모델을 개발할 수 있는 길을 열어 학계와 산업계 전반에 지대한 파급 효과를 가져왔다.10</p>
<h2>2. Wav2vec 2.0의 핵심 철학 및 학습 패러다임</h2>
<p>Wav2vec 2.0의 학습 프레임워크는 두 가지 명확한 단계, 즉 비지도 사전 학습과 지도 미세 조정으로 구성된다. 이 2단계 접근법은 모델이 먼저 데이터의 일반적인 구조를 학습한 후, 특정 과제에 대한 지식을 습득하도록 하여 학습의 효율성과 데이터 효율성을 극대화한다.</p>
<h3>2.1 단계: 비지도 사전 학습 (Unsupervised Pre-training)</h3>
<p>사전 학습 단계의 핵심 목표는 대규모의 레이블 없는 음성 데이터를 사용하여 음성의 보편적이고 풍부한 표현을 학습하는 것이다.2 이는 NLP 분야에서 단어 임베딩이 대규모 텍스트 코퍼스로부터 단어의 의미적, 통사적 관계를 학습하여 언어의 일반적인 표현을 구축하는 것과 철학적으로 유사하다.11</p>
<p>Wav2vec 2.0은 이 목표를 달성하기 위해 원시 음성 파형(raw waveform)을 입력받아 이를 잠재 공간(latent space)의 표현으로 변환한 뒤, 이 잠재 표현 시퀀스의 일부를 의도적으로 마스킹(masking)한다. 그 후 모델은 주변의 문맥 정보를 활용하여 마스킹된 부분의 올바른 표현이 무엇인지를 예측하는 대조 학습(contrastive task) 과제를 수행하도록 훈련된다.1</p>
<p>이 과정의 가장 독창적인 부분은 예측의 ‘정답’ 역할을 하는 이산적인 음성 단위(discrete speech units)조차도 외부적인 레이블이나 사전 지식 없이 데이터로부터 모델과 함께 학습된다는 점이다(jointly learned).1 즉, 모델은 스스로 문제를 정의하고 그 문제를 푸는 방법을 동시에 학습한다. 이는 외부의 감독 없이 시스템이 스스로 지식을 구축해나가는 완전한 형태의 자기 지도 학습을 구현한 것으로, Wav2vec 2.0의 핵심적인 혁신이라 할 수 있다.</p>
<h3>2.2 단계: 지도 미세 조정 (Supervised Fine-tuning)</h3>
<p>사전 학습을 통해 음성의 일반적인 구조와 특성에 대한 깊은 이해를 갖춘 모델은 이후 지도 미세 조정 단계를 거쳐 특정 다운스트림 과제, 주로 자동 음성 인식을 위해 최적화된다.10</p>
<p>미세 조정 방법론은 사전 학습된 모델의 대부분의 가중치는 그대로 유지한 채, 과제에 특화된 작은 분류 헤드(일반적으로 단일 선형 레이어)를 모델의 최상단에 추가하는 방식으로 이루어진다. 그 다음, 소량의 레이블링된 데이터(음성-텍스트 쌍)를 사용하여 전체 모델의 가중치를 소폭 업데이트한다.12</p>
<p>이 방식이 효과적인 이유는 사전 학습 단계에서 모델이 이미 음성의 복잡한 구조, 즉 음향적 특성과 음운적 규칙 등을 충분히 학습했기 때문이다. 따라서 미세 조정 단계에서는 방대한 음성 지식을 처음부터 학습할 필요 없이, 이미 학습된 풍부한 표현을 특정 음소나 단어와 같은 기호적 레이블에 매핑하는 방법만을 빠르게 학습하면 된다.1</p>
<p>이러한 사전 학습과 미세 조정의 2단계 접근법은 서로에게 필수적인 공생적 관계를 형성한다. 사전 학습은 ’일반성(generality)’을, 미세 조정은 ’특수성(specificity)’을 담당하며 시너지를 창출한다. 즉, 사전 학습은 모델에게 ’무엇이 음성인가’에 대한 근본적인 이해를 제공하고, 미세 조정은 ’이 특정 음성 표현이 어떤 단어에 해당하는가’를 가르친다. 사전 학습만으로는 특정 ASR 과제를 수행할 수 없다. 모델이 음성의 구조적 표현은 학습했지만, 그 표현이 어떤 문자 기호에 대응되는지는 모르기 때문이다. 반대로, 소량의 데이터로 미세 조정만 수행할 경우(즉, 처음부터 지도 학습만 진행할 경우) 모델은 음성의 기본적인 음향적 특성부터 특정 단어의 발음까지 모든 것을 한 번에 학습해야 하므로 쉽게 과적합(overfitting)에 빠지게 된다.</p>
<p>Wav2vec 2.0은 이 두 단계를 분리함으로써 학습의 복잡도를 효과적으로 분산시킨다. 사전 학습이 방대한 비레이블 데이터를 통해 문제의 탐색 공간을 의미 있는 저차원 매니폴드(manifold)로 효과적으로 축소해주면, 미세 조정은 그 축소된 공간 내에서 소량의 레이블 데이터만으로도 최적의 해를 효율적으로 찾을 수 있게 된다. 이러한 접근법은 모델의 재사용성과 확장성을 극대화한다. 잘 사전 학습된 단일 모델, 예를 들어 <code>facebook/wav2vec2-base-960h</code>와 같은 공개 모델은 7, ASR뿐만 아니라 화자 인식, 감정 인식, 음성 번역 등 다양한 음성 관련 다운스트림 과제를 위한 강력한 백본(backbone)으로 활용될 수 있다.3 이는 음성 처리 분야에서 “ImageNet moment“와 유사한 효과를 가져오며, 사전 학습된 거대 모델을 기반으로 다양한 응용 모델을 개발하는 생태계를 구축하는 기반이 되었다.</p>
<h2>3. 모델 아키텍처 상세 분석</h2>
<p>Wav2vec 2.0의 아키텍처는 원시 오디오 신호를 입력받아 문맥화된 표현을 출력하기까지 세 가지 핵심적인 구성 요소가 유기적으로 작동하도록 설계되었다. 이들은 각각 특징 인코더(Feature Encoder), 문맥 네트워크(Context Network), 그리고 양자화 모듈(Quantization Module)이다.3</p>
<h3>3.1 특징 인코더: 원시 음성 파형의 잠재 공간 변환</h3>
<p>특징 인코더는 모델의 가장 첫 부분에 위치하며, 연속적인 원시 음성 파형을 저차원의 잠재 특징 시퀀스로 변환하는 역할을 담당한다.</p>
<ul>
<li><strong>구조 및 입력:</strong> 특징 인코더는 16kHz로 샘플링된 원시 음성 파형(float array)을 직접 입력으로 받는 다층 1차원 컨볼루션 신경망(1D CNN)으로 구성된다.2</li>
<li><strong>상세 사양:</strong> 이 CNN은 총 7개의 블록으로 이루어져 있으며, 각 블록은 512개의 채널을 가진 시간적 컨볼루션(temporal convolution) 레이어를 포함한다. 입력에 가까운 레이어부터 순서대로 스트라이드(strides)는 <code>(5, 2, 2, 2, 2, 2, 2)</code>로, 커널 폭(kernel widths)은 <code>(10, 3, 3, 3, 3, 2, 2)</code>로 설정되어 있다.2 이러한 구조는 초기 레이어에서 큰 보폭으로 신호를 다운샘플링하여 시퀀스 길이를 빠르게 줄이고, 후속 레이어에서는 더 미세한 특징을 포착하도록 설계되었다.</li>
<li><strong>기능 및 출력:</strong> 이 CNN 스택은 높은 시간 해상도를 가진 원시 오디오의 차원을 효과적으로 축소하여, 약 20ms 간격으로 하나의 특징 벡터를 생성하는 잠재 음성 표현 시퀀스 <span class="math math-inline">Z = \{z_1, z_2,..., z_T\}</span>를 출력한다.3 전체 수용장(receptive field)은 400개의 오디오 샘플, 즉 25ms에 해당하며, 이는 음소(phoneme) 수준의 짧은 음향 특징을 포착하기에 적합한 시간 단위이다.3 각 컨볼루션 레이어 후에는 그룹 정규화(group normalization)와 GELU 활성화 함수가 적용되어 학습을 안정시킨다.8</li>
</ul>
<h3>3.2 문맥 네트워크: 트랜스포머를 이용한 문맥적 표현 구축</h3>
<p>문맥 네트워크는 특징 인코더로부터 생성된 지역적인 잠재 표현 시퀀스를 입력받아, 시퀀스 전체의 전역적인 문맥 정보를 통합하는 역할을 수행한다.</p>
<ul>
<li><strong>구조:</strong> 문맥 네트워크의 핵심은 표준 트랜스포머 인코더(Transformer encoder) 아키텍처이다.1 이는 멀티 헤드 셀프 어텐션(multi-head self-attention) 메커니즘을 통해 시퀀스 내 모든 시간 스텝 간의 관계를 모델링한다.</li>
<li><strong>입력 처리 및 모델 변형:</strong> 특징 인코더의 출력 <span class="math math-inline">Z</span>는 512차원의 벡터 시퀀스이다. 이 시퀀스는 트랜스포머의 내부 차원(model dimension)에 맞추기 위해 선형 특징 투영 레이어(feature projection layer)를 먼저 통과한다.2 Wav2vec 2.0은 두 가지 주요 모델 크기를 제공한다:</li>
<li><strong>BASE 모델:</strong> 12개의 트랜스포머 블록, 모델 차원 768, 피드포워드 네트워크(FFN) 내부 차원 3072, 그리고 8개의 어텐션 헤드로 구성된다.2</li>
<li><strong>LARGE 모델:</strong> 24개의 트랜스포머 블록, 모델 차원 1024, FFN 내부 차원 4096, 그리고 16개의 어텐션 헤드로 구성되어 더 큰 용량을 가진다.2</li>
<li><strong>위치 임베딩:</strong> NLP 분야의 표준 트랜스포머, 예를 들어 BERT가 고정된 사인/코사인 함수나 학습 가능한 임베딩을 통해 토큰의 ‘절대적 위치’ 정보를 주입하는 것과 달리, Wav2vec 2.0은 독특한 방식을 사용한다. 트랜스포머의 셀프 어텐션 레이어에 들어가기 전에, 입력 시퀀스에 컨볼루션 레이어를 적용하여 ’상대적 위치 임베딩(relative positional embedding)’을 동적으로 학습한다.2 이는 음성 신호의 시간적 연속성과 지역적 상관관계를 모델이 더 자연스럽게 학습하도록 유도하는 설계이다.</li>
<li><strong>기능 및 출력:</strong> 트랜스포머의 강력한 셀프 어텐션 메커니즘을 통해, 각 시간 스텝의 잠재 표현 <span class="math math-inline">z_t</span>는 시퀀스 전체의 정보를 종합적으로 고려하여 문맥화된 표현(contextualized representation) <span class="math math-inline">c_t</span>로 변환된다. 최종 출력은 이러한 문맥화된 표현들의 시퀀스 <span class="math math-inline">C = \{c_1, c_2,..., c_T\}</span>이다.12</li>
</ul>
<h3>3.3 양자화 모듈: 연속 신호의 이산화</h3>
<p>양자화 모듈은 사전 학습 단계에서만 사용되며, 연속적인 음성 특징을 이산적인 단위로 변환하여 대조 학습의 ’타겟’을 생성하는 핵심적인 역할을 한다.</p>
<ul>
<li>
<p><strong>필요성:</strong> 음성은 본질적으로 연속적인 신호이다. 그러나 BERT 스타일의 마스킹 기반 예측 과제를 음성에 적용하기 위해서는 예측해야 할 명확하고 이산적인(discrete) 타겟이 필요하다.3 양자화 모듈은 이러한 연속적인 잠재 표현을 유한한 개수의 이산적인 “음성 단위(speech units)“로 변환하는 역할을 수행한다.2</p>
</li>
<li>
<p><strong>코드북(Codebook) 개념:</strong> 양자화는 코드북이라는 개념을 통해 이루어진다. 모델은 <span class="math math-inline">G</span>개의 코드북(그룹)을 가지며, 각 코드북은 <span class="math math-inline">V</span>개의 코드워드(code word), 즉 학습 가능한 벡터로 구성된다.3 예를 들어, 기본 모델에서는</p>
</li>
</ul>
<p><span class="math math-inline">G=2</span>, <span class="math math-inline">V=320</span>으로 설정되어, 총 <span class="math math-inline">320 \times 320 = 102,400</span>개의 잠재적인 복합 음성 단위를 표현할 수 있다.3</p>
<ul>
<li><strong>검블-소프트맥스(Gumbel-Softmax)의 활용:</strong> 특징 인코더의 출력 <span class="math math-inline">z_t</span>는 선형 변환을 거쳐 각 코드북의 각 코드워드에 대한 로짓(logit)으로 변환된다. 이 로짓들을 사용하여 각 코드북에서 하나의 대표 코드워드를 선택해야 하는데, 단순한 <code>argmax</code> 연산은 미분이 불가능하여 역전파를 통한 학습이 어렵다. 이 문제를 해결하기 위해 Wav2vec 2.0은 검블-소프트맥스 트릭을 사용한다.1 검블-소프트맥스는 로짓에 검블 노이즈를 더한 후 소프트맥스를 취하는 방식으로,</li>
</ul>
<p><code>argmax</code>와 유사한 효과를 내면서도 전체 과정이 미분 가능(differentiable)하도록 만들어 종단간(end-to-end) 학습을 가능하게 한다.3</p>
<ul>
<li><strong>출력:</strong> 각 코드북에서 검블-소프트맥스를 통해 선택된 <span class="math math-inline">G</span>개의 코드워드 벡터를 서로 결합(concatenate)하고, 다시 한번 선형 변환을 거쳐 최종적인 양자화된 표현 <span class="math math-inline">q_t</span>를 생성한다.2 이</li>
</ul>
<p><span class="math math-inline">q_t</span>가 마스킹된 시간 스텝에 대한 예측 타겟으로 사용된다.</p>
<p>Wav2vec 2.0 아키텍처의 각 구성 요소는 텍스트와는 근본적으로 다른 음성 데이터의 물리적, 통계적 특성을 처리하기 위한 필연적인 설계의 결과물이다. 첫째, 원시 파형을 직접 처리하기 위해 CNN을 전면에 배치한 것은 음성 신호가 가진 강한 지역적 상관성(local correlation), 예를 들어 음성의 기본 주파수나 포먼트(formant)와 같은 음향적 패턴을 효율적으로 포착하기 위함이다. 동시에 CNN의 스트라이드를 통한 다운샘플링은 후속 트랜스포머의 계산 복잡도(시퀀스 길이에 제곱 비례)를 현실적인 수준으로 완화하는 실용적인 역할을 한다.3 둘째, 고정된 절대 위치 임베딩 대신 컨볼루션 기반의 상대 위치 임베딩을 채택한 것은 음성 인식에서 한 음소와 다른 음소 간의 ’상대적 시간 간격’과 패턴이 ’절대적 위치’보다 더 중요한 정보임을 반영한 것이다. 이 방식은 모델이 시간적 이동에 불변하는(time-shift invariant) 특징을 자연스럽게 학습하도록 돕는다.2 마지막으로, 양자화 모듈의 도입은 가장 핵심적인 설계 결정이다. 텍스트에는 ’단어’나 ’문자’와 같이 명확하게 정의된 이산 단위가 존재하지만, 연속적인 음성 신호에는 그러한 단위가 없다.3 양자화 모듈은 데이터로부터 이러한 이산 단위를 스스로 ’발견’하고 ’창조’하는 역할을 한다. 이는 사전에 정의된 음소(phoneme)와 같은 언어학적 단위에 의존하지 않고, 모델이 데이터 자체의 통계적 구조에 기반한 최적의 표현 단위를 학습하게 한다. 이는 특정 언어에 구애받지 않는 범용적인 음성 표현 학습을 가능하게 하는 핵심적인 메커니즘이다.4 결론적으로 Wav2vec 2.0은 단순히 NLP의 트랜스포머를 음성에 적용한 것이 아니라, 음성이라는 특정 양상의 본질을 깊이 이해하고 그에 맞는 아키텍처를 체계적으로 설계한 정교한 공학적 산물이다.</p>
<h2>4. 사전 학습 목표: 대조 학습과 손실 함수</h2>
<p>Wav2vec 2.0의 사전 학습은 모델이 레이블 없는 데이터로부터 의미 있는 표현을 학습하도록 유도하는 정교한 목표 함수(objective function)를 기반으로 한다. 이 목표는 잠재 공간 마스킹, 대조 손실, 그리고 다양성 손실이라는 세 가지 요소로 구성된다.</p>
<h3>4.1 잠재 공간 마스킹 전략</h3>
<p>Wav2vec 2.0의 학습 방식은 NLP의 BERT가 사용하는 마스크된 언어 모델링(Masked Language Modeling, MLM)에서 직접적인 영감을 받았다. 그러나 텍스트 토큰 대신, 특징 인코더가 생성한 연속적인 잠재 표현 시퀀스 <span class="math math-inline">Z</span>의 일부를 마스킹한다는 점에서 차이가 있다.1</p>
<p>마스킹 프로세스는 다음과 같이 진행된다. 먼저, 전체 시간 스텝 중 일정 비율 <span class="math math-inline">p</span>(논문에서는 6.5%로 설정)에 해당하는 시작 인덱스를 무작위로 비복원 추출한다. 그 다음, 선택된 각 시작 인덱스로부터 연속된 <span class="math math-inline">M</span>개(논문에서는 10개)의 시간 스텝에 해당하는 잠재 벡터들을 학습 가능한 단일 마스킹 벡터로 대체한다.12 이 마스킹된 구간들은 서로 겹칠 수 있으며, 모델은 이 마스킹된 부분의 정보를 주변 문맥을 통해 복원하는 과제를 수행하게 된다.</p>
<h3>4.2 대조 손실 (Contrastive Loss, <span class="math math-inline">L_m</span>)</h3>
<p>단순히 마스킹된 벡터를 복원하는 회귀(regression) 문제 대신, Wav2vec 2.0은 대조 학습(contrastive learning)이라는 더 강력한 프레임워크를 채택한다. 대조 학습의 목표는 마스킹된 특정 시간 스텝 <span class="math math-inline">t</span>에 대해, 문맥 네트워크가 생성한 예측 벡터 <span class="math math-inline">c_t</span>가 해당 시간 스텝의 ‘올바른’ 타겟 표현 <span class="math math-inline">q_t</span>를 여러 개의 그럴듯한 ‘방해물’(distractors) 중에서 정확하게 식별하도록 학습하는 것이다.1</p>
<p>이때 사용되는 디스트랙터 $ \tilde{q} $는 동일한 발화 내의 다른 마스킹된 시간 스텝들에서 균등하게 샘플링된 <span class="math math-inline">K</span>개의 양자화 표현으로 구성된다.1 이는 모델이 단순히 일반적인 음성 신호를 생성하는 것을 넘어, 주어진 문맥에 맞는 매우 구체적이고 정확한 음성 단위를 구별해내는 능력을 학습하도록 강제한다.</p>
<p>이러한 대조적 과제를 위한 손실 함수 <span class="math math-inline">L_m</span>은 다음과 같이 정의된다 1:<br />
<span class="math math-display">
L_m = - \log \frac{\exp(\text{sim}(c_t, q_t)/\kappa)}{\sum_{\tilde{q} \sim Q_t} \exp(\text{sim}(c_t, \tilde{q})/\kappa)}
</span></p>
<ul>
<li><span class="math math-inline">c_t</span>: 마스킹된 시간 스텝 <span class="math math-inline">t</span>에 대한 문맥 네트워크(트랜스포머)의 최종 출력 벡터이다.</li>
<li><span class="math math-inline">q_t</span>: 시간 스텝 <span class="math math-inline">t</span>에 대한 실제(ground truth) 양자화 표현으로, 예측의 정답 타겟이다.</li>
<li><span class="math math-inline">Q_t</span>: 정답 타겟 <span class="math math-inline">q_t</span>와 <span class="math math-inline">K</span>개의 디스트랙터를 모두 포함하는 후보 집합이다.</li>
<li><span class="math math-inline">\text{sim}(a, b)</span>: 두 벡터 <span class="math math-inline">a</span>와 <span class="math math-inline">b</span> 간의 코사인 유사도(cosine similarity)를 계산하는 함수이다.</li>
<li><span class="math math-inline">\kappa</span>: 온도(temperature)를 조절하는 하이퍼파라미터로, 유사도 점수 분포의 첨예도(sharpness)를 조절한다.</li>
</ul>
<p>이 손실 함수는 본질적으로 소프트맥스 교차 엔트로피(softmax cross-entropy) 손실과 동일한 형태를 가진다. 분자는 모델의 예측(<span class="math math-inline">c_t</span>)과 정답(<span class="math math-inline">q_t</span>) 간의 유사도를 높이는 방향으로, 분모는 모든 후보(정답+디스트랙터)에 대한 유사도의 총합을 나타낸다. 따라서 <span class="math math-inline">L_m</span>을 최소화하는 것은 정답 타겟에 대한 확률을 최대화하고, 디스트랙터에 대한 확률을 최소화하도록 모델을 훈련시키는 것과 같다.</p>
<h3>4.3 다양성 손실 (Diversity Loss, <span class="math math-inline">L_d</span>)</h3>
<p>사전 학습 과정에서 모델이 양자화 모듈의 코드북을 효과적으로 활용하도록 돕기 위해 다양성 손실이라는 추가적인 정규화(regularization) 기법이 도입된다.3 학습 초기 단계나 특정 조건 하에서, 모델은 소수의 특정 코드워드만을 반복적으로 사용하는 ‘코드북 붕괴(codebook collapse)’ 현상에 빠질 수 있다. 이는 모델의 표현력을 심각하게 저해하는 문제이다.</p>
<p>다양성 손실 <span class="math math-inline">L_d</span>는 이러한 문제를 방지하고 모델이 코드북에 있는 모든 코드워드를 가급적 균등하게 사용하도록 장려한다. 이는 각 코드북에 대해, 배치 내 모든 시간 스텝의 로짓을 평균내어 얻은 확률 분포의 엔트로피(entropy)를 최대화하는 방식으로 구현된다.3 엔트로피를 최대화하는 것은 확률 분포를 균등 분포(uniform distribution)에 가깝게 만드는 것과 같으므로, 모델이 특정 코드워드에 치우치지 않고 다양한 코드워드를 탐색하고 활용하도록 유도한다.</p>
<p>최종적으로 사전 학습에 사용되는 전체 손실 함수 <span class="math math-inline">L</span>은 대조 손실과 다양성 손실의 가중합으로 구성된다: <span class="math math-inline">L = L_m + \alpha L_d</span>, 여기서 <span class="math math-inline">\alpha</span>는 다양성 손실의 중요도를 조절하는 하이퍼파라미터이다.19</p>
<p>Wav2vec 2.0의 손실 함수 설계는 모델이 단순히 정해진 정답을 맞추는 것을 넘어, 그 ‘정답’ 자체의 품질과 다양성까지 학습 목표에 포함시킨다는 점에서 매우 정교하다. 대조 손실 <span class="math math-inline">L_m</span>은 문맥 네트워크가 양자화된 타겟 <span class="math math-inline">q_t</span>를 정확히 예측하도록 훈련시키며, 이는 ’어떻게 예측할 것인가’에 대한 학습에 해당한다. 동시에, <span class="math math-inline">L_m</span>으로부터 계산된 그래디언트는 문맥 네트워크뿐만 아니라 양자화 모듈과 특징 인코더까지 역전파된다. 이는 예측의 타겟이 되는 <span class="math math-inline">q_t</span> 자체를 예측하기 좋은 방향으로 업데이트하는 효과를 가져오며, 이는 ’무엇을 예측할 것인가’에 대한 학습을 의미한다. 여기에 다양성 손실 <span class="math math-inline">L_d</span>가 더해져 예측 타겟이 존재하는 공간, 즉 코드북이 풍부하고 다양하게 유지되도록(붕괴되지 않도록) 제어하는 역할을 한다. 이 세 가지 요소의 상호작용은 모델이 스스로 데이터의 구조를 발견하고, 그 구조를 표현할 어휘(코드북)를 창조하며, 그 어휘를 예측하는 문법(트랜스포머)을 동시에 학습하는 고도의 자기 조직화(self-organization) 과정을 형성한다. 이 ‘공동 학습(joint learning)’ 메커니즘이야말로 Wav2vec 2.0이 달성한 핵심적인 기술적 혁신이다.1</p>
<h2>5. 미세 조정: 지도 학습을 통한 음성 인식</h2>
<p>비지도 사전 학습을 통해 음성에 대한 풍부하고 일반화된 표현을 학습한 모델은, 특정 다운스트림 과제인 자동 음성 인식을 수행하기 위해 지도 미세 조정 단계를 거친다. 이 단계에서는 소량의 레이블링된 데이터를 사용하여 모델을 과제에 맞게 특화시킨다.</p>
<p>미세 조정 단계에서는 모델 아키텍처에 약간의 변경이 가해진다. 사전 학습 시 대조 학습의 타겟을 생성하기 위해 사용되었던 양자화 모듈은 더 이상 필요하지 않으므로 제거된다.11 대신, 사전 학습된 트랜스포머 문맥 네트워크의 최종 출력</p>
<p><span class="math math-inline">C</span> 위에 새로운 선형 레이어가 추가된다. 이 선형 레이어는 무작위로 가중치가 초기화되며, 출력 차원은 해당 언어의 어휘(vocabulary) 크기, 예를 들어 알파벳 문자, 숫자, 특수 기호 등의 개수와 일치하도록 설정된다.12</p>
<p>손실 함수로는 음성 인식 분야에서 표준적으로 사용되는 연결주의 시간적 분류(Connectionist Temporal Classification, CTC) 손실 함수가 사용된다.8 CTC 손실의 가장 큰 장점은 입력 음성 프레임 시퀀스와 출력 문자 시퀀스 간의 명시적인 정렬(alignment) 정보를 요구하지 않는다는 것이다. 음성은 발화 속도나 스타일에 따라 같은 단어라도 길이가 달라지기 때문에 프레임 단위의 정확한 레이블링이 어려운데, CTC는 가능한 모든 정렬 경로의 확률을 합산하여 이를 해결하므로 음성 인식에 매우 효과적이다.</p>
<p>과적합을 방지하고 모델의 일반화 성능을 높이기 위한 정규화 기법도 미세 조정 단계에서 중요한 역할을 한다. 흥미롭게도, 사전 학습에 사용되었던 마스킹 전략을 미세 조정 중에도 계속 적용할 수 있으며, 이는 데이터 증강(data augmentation)과 유사한 효과를 내어 과적합을 억제하는 정규화 기법으로 작용한다.11 또한, 시간 축과 주파수 축에서 특징의 일부를 가리는 SpecAugment와 유사한 마스킹 기법을 적용하여 모델의 강건성(robustness)을 더욱 향상시키고 최종적인 오류율을 크게 개선한다.15</p>
<h2>6. 성능 평가: LibriSpeech 벤치마크 결과</h2>
<p>Wav2vec 2.0의 성능은 주로 표준 영어 음성 인식 벤치마크인 LibriSpeech 데이터셋을 통해 평가되었다. 성능 평가의 핵심 지표는 단어 오류율(Word Error Rate, WER)로, 이는 자동 음성 인식 시스템의 정확도를 측정하는 가장 보편적인 기준이다. WER은 모델이 예측한 문장과 실제 정답 문장을 비교하여 발생한 세 가지 유형의 오류, 즉 치환(Substitutions, 잘못 인식된 단어), 삭제(Deletions, 누락된 단어), 삽입(Insertions, 불필요하게 추가된 단어)의 총 개수를 정답 문장의 전체 단어 수로 나눈 값이다. 따라서 WER은 낮을수록 더 높은 성능을 의미한다.20</p>
<p>LibriSpeech 벤치마크는 오디오북을 낭독한 데이터로 구성되며, 상대적으로 잡음이 적고 깨끗한 환경에서 녹음된 ‘test-clean’ 세트와, 약간의 배경 잡음이나 발음의 변이가 포함된 ‘test-other’ 세트로 나뉘어 평가된다. 이를 통해 모델이 이상적인 환경뿐만 아니라 좀 더 현실적인 환경에서도 얼마나 강건한 성능을 보이는지 종합적으로 측정할 수 있다.6</p>
<p>Wav2vec 2.0의 실험 결과는 음성 인식 분야에 큰 충격을 주었다. 특히, 극소량의 레이블 데이터만으로도 이전의 최첨단 모델들을 압도하는 성능을 보여주었기 때문이다. 가장 뛰어난 성능은 LARGE 모델을 대규모 비레이블 데이터셋인 LibriVox(LV-60k, 약 6만 시간)로 사전 학습한 후 미세 조정한 경우에 기록되었다.1</p>
<p>아래 표는 Wav2vec 2.0의 핵심적인 기여, 즉 데이터 효율성을 정량적으로 명확하게 보여준다. 이 표는 사용 가능한 레이블 데이터의 양이 10분에서 960시간으로 증가함에 따라 WER이 어떻게 체계적으로 감소하는지, 그리고 더 많은 비레이블 데이터(LS-960 vs LV-60k)를 사용했을 때의 이점을 한눈에 파악할 수 있게 한다. 이는 모델의 우수한 성능과 잠재력을 입증하는 핵심적인 증거 자료이다.</p>
<p><strong>LibriSpeech Low-Resource 및 Full-Resource 벤치마크 WER(%) 결과</strong></p>
<table><thead><tr><th>Labeled Data</th><th>Pre-training Data</th><th>Model</th><th>LM</th><th>test-clean (WER)</th><th>test-other (WER)</th></tr></thead><tbody>
<tr><td>10 min</td><td>LV-60k</td><td>LARGE</td><td>Transf.</td><td>4.8</td><td>8.2</td></tr>
<tr><td>1 hour</td><td>LV-60k</td><td>LARGE</td><td>Transf.</td><td>2.9</td><td>5.8</td></tr>
<tr><td>10 hours</td><td>LV-60k</td><td>LARGE</td><td>Transf.</td><td>2.6</td><td>4.9</td></tr>
<tr><td>100 hours</td><td>LV-60k</td><td>LARGE</td><td>Transf.</td><td>2.0</td><td>4.0</td></tr>
<tr><td>960 hours</td><td>LS-960</td><td>BASE</td><td>Transf.</td><td>2.1</td><td>4.8</td></tr>
<tr><td>960 hours</td><td>LS-960</td><td>LARGE</td><td>Transf.</td><td>2.0</td><td>4.1</td></tr>
<tr><td><strong>960 hours</strong></td><td><strong>LV-60k</strong></td><td><strong>LARGE</strong></td><td><strong>Transf.</strong></td><td><strong>1.8</strong></td><td><strong>3.3</strong></td></tr>
</tbody></table>
<p>표 설명: Labeled Data는 미세 조정에 사용된 LibriSpeech 데이터의 양을 의미한다. Pre-training Data는 사전 학습에 사용된 비레이블 데이터셋(LS-960: 960시간, LV-60k: 약 53,000시간)을 나타낸다. LM은 후처리 과정에서 Transformer 언어 모델을 사용했음을 의미한다. 1</p>
<p>결과에서 볼 수 있듯이, 단 10분의 레이블 데이터만으로도 <code>test-clean</code>에서 4.8%, <code>test-other</code>에서 8.2%의 WER을 달성했으며, 이는 당시 소량의 데이터로 달성할 수 있는 전례 없는 수준의 성능이었다. 레이블 데이터의 양이 1시간, 10시간으로 늘어남에 따라 성능은 꾸준히 향상되었다. 특히 960시간의 레이블 데이터를 모두 사용하고, 대규모 LV-60k 데이터로 사전 학습한 LARGE 모델은 <code>test-clean</code>에서 1.8%, <code>test-other</code>에서 3.3%라는 경이적인 WER을 기록하며 당시 최고의 준지도 학습 모델들을 능가하는 새로운 SOTA(State-of-the-Art)를 달성했다.6</p>
<h2>7. Wav2vec 2.0의 계보와 영향력</h2>
<p>Wav2vec 2.0은 독자적으로 등장한 모델이 아니라, 이전 연구들의 아이디어를 계승하고 발전시킨 결과물이며, 동시에 후속 연구들에게 지대한 영향을 미친 음성 처리 연구의 중요한 분기점이다.</p>
<h3>7.1 Wav2vec 1.0에서 2.0으로의 진화</h3>
<p>Wav2vec 2.0은 이름에서 알 수 있듯이 2019년에 발표된 Wav2vec 1.0의 직접적인 후속 모델이다. 두 모델은 레이블 없는 오디오 데이터로부터 표현을 학습한다는 공통된 목표를 공유하지만, 아키텍처와 학습 목표에서 중요한 진화를 이루었다.</p>
<ul>
<li><strong>아키텍처의 변화:</strong> Wav2vec 1.0은 원시 오디오를 인코딩하는 인코더 네트워크와 시간적 문맥을 통합하는 문맥 네트워크가 모두 순수한 CNN으로 구성된 구조였다.21 반면, Wav2vec 2.0은 지역적 특징 추출에 강한 CNN을 특징 인코더로 사용하고, 시퀀스 전체의 장거리 의존성(long-range dependency) 모델링에 탁월한 트랜스포머를 문맥 네트워크로 채택한 하이브리드 구조로 발전했다.21 이 변화는 모델이 음성의 미세한 음향적 특징과 거시적인 문맥 구조를 모두 효과적으로 학습할 수 있게 했다.</li>
<li><strong>학습 목표의 변화:</strong> Wav2vec 1.0의 자기 지도 학습 과제는 자기회귀적(auto-regressive) 방식이었다. 즉, 특정 시점까지의 오디오 정보를 바탕으로 미래의 잠재 오디오 샘플을 예측하는 대조 예측 과제를 수행했다.22 이에 반해, Wav2vec 2.0은 BERT에서 영감을 받아 비자기회귀적(non-autoregressive) 방식을 채택했다. 시퀀스의 중간 부분을 마스킹하고, 양방향 문맥을 모두 활용하여 마스킹된 위치의 이산적 표현을 예측하는 과제를 해결한다.3 이 방식은 더 풍부한 문맥 정보를 활용할 수 있어 더 강력한 표현 학습으로 이어진다.</li>
</ul>
<h3>7.2 BERT와의 비교: 음성과 텍스트의 만남</h3>
<p>Wav2vec 2.0은 종종 ’음성 분야의 BERT’로 비유된다. 이는 두 모델이 공유하는 핵심적인 철학과 기술적 유사성 때문이다.</p>
<ul>
<li><strong>공통점:</strong> 두 모델 모두 트랜스포머 인코더 아키텍처를 기반으로 하며, 입력의 일부를 마스킹하고 주변 문맥을 이용해 이를 예측하는 자기 지도 학습 목표를 통해 강력한 문맥적 표현을 학습한다는 핵심 아이디어를 공유한다.1</li>
<li><strong>차이점:</strong> 두 모델의 근본적인 차이는 처리하는 데이터의 양상(modality)에서 비롯된다.</li>
<li><strong>입력 양상:</strong> BERT는 사전에 정의된 어휘 사전을 통해 이산적인 텍스트 토큰 시퀀스를 입력으로 받는다. 반면, Wav2vec 2.0은 연속적인 신호인 원시 음성 파형을 직접 처리해야 하는 더 어려운 문제에 직면한다.3</li>
<li><strong>예측 타겟:</strong> BERT는 마스킹된 위치의 원래 토큰, 즉 사전에 명확히 정의된 어휘를 예측한다. 그러나 음성에는 이러한 명확한 단위가 없기 때문에, Wav2vec 2.0은 데이터로부터 이산적인 양자화 표현을 ’학습’하고, 이를 예측 타겟으로 삼는다.4 연속적인 신호를 처리하기 위해 예측의 타겟 자체를 학습 과정에 포함시킨 이 점이 Wav2vec 2.0의 가장 핵심적인 차별점이자 혁신이다.</li>
</ul>
<h3>7.3 후속 연구에 미친 영향: HuBERT와 data2vec</h3>
<p>Wav2vec 2.0의 성공은 음성 SSL 연구의 폭발적인 성장을 이끌었으며, 그 아이디어를 계승하고 발전시킨 여러 중요한 후속 모델들의 등장을 촉발했다.</p>
<ul>
<li><strong>HuBERT (Hidden-Unit BERT):</strong> HuBERT는 Wav2vec 2.0의 핵심 아이디어, 즉 ’마스킹된 음성 단위 예측’을 계승하되, 예측 타겟을 생성하는 방식을 정제했다.26 Wav2vec 2.0이 학습 과정 중에 ’온라인’으로 양자화하여 타겟을 생성하는 반면, HuBERT는 사전 학습 데이터에 대해 오프라인으로 k-means와 같은 클러스터링 알고리즘을 반복적으로 적용하여 이산적인 ’은닉 단위(hidden units)’를 생성하고, 이를 예측 타겟으로 사용한다. 이 방식은 더 안정적이고 일관된 타겟을 제공하여 일부 벤치마크에서 Wav2vec 2.0을 능가하는 성능을 보였다.</li>
<li><strong>data2vec:</strong> data2vec은 여기서 한 걸음 더 나아가, 이산적인 타겟 자체를 없애는 과감한 아이디어를 제안한다.26 data2vec은 마스킹된 위치에 대해, 모델 자신의 상위 레이어에서 나온 문맥화된 ‘잠재 표현’ 자체를 예측하도록 학습한다. 이는 일종의 자기 증류(self-distillation) 방식으로, 교사 모델(teacher model)이 생성한 연속적인 벡터 표현을 학생 모델(student model)이 모방하도록 훈련된다. 이로써 양자화나 클러스터링과 같은 복잡한 과정 없이도 효과적인 표현 학습이 가능해졌고, 동일한 학습 프레임워크를 음성, 컴퓨터 비전, 자연어 처리에 모두 적용할 수 있는 범용 SSL 프레임워크로 확장되었다.27</li>
</ul>
<p>이러한 후속 모델들의 등장은 Wav2vec 2.0이 음성 SSL 분야의 ’공통 과제(Common Task)’를 성공적으로 정의했음을 보여준다. Wav2vec 2.0 이후 등장한 대부분의 주요 음성 SSL 모델들은 “연속 신호 → 잠재 공간 인코딩 → 마스킹 → 문맥화 → 마스킹된 부분의 표현 예측“이라는 Wav2vec 2.0이 제시한 성공적인 청사진을 공유한다. HuBERT는 이 청사진에서 예측 타겟을 ’온라인 양자화된 단위’에서 ’오프라인 클러스터링된 단위’로 변경했고, data2vec은 타겟을 ’이산 단위’에서 ’연속적인 잠재 표현’으로 변경하는 등, ’무엇을 타겟으로 예측할 것인가’에 대한 다양한 변주를 통해 발전을 거듭하고 있다. 이는 Wav2vec 2.0이 단순히 하나의 성공적인 모델을 제시한 것을 넘어, 후속 연구자들이 탐구하고 개선할 수 있는 풍부하고 생산적인 ’연구의 장’을 열었음을 의미하며, 해당 분야의 발전을 가속화하는 데 결정적인 역할을 했다.</p>
<h2>8. 결론: 음성 처리의 새로운 지평</h2>
<p>Wav2vec 2.0은 음성 처리, 특히 자동 음성 인식 분야에서 패러다임의 전환을 이끈 기념비적인 모델로 평가된다. 이 모델의 기여는 여러 측면에서 요약될 수 있다.</p>
<p>첫째, Wav2vec 2.0은 자기 지도 학습을 통해 음성 인식에 필요한 레이블 데이터의 양을 극적으로 줄였다. 단 10분의 레이블 데이터만으로도 실용적인 성능을 달성할 수 있음을 입증함으로써, 데이터가 부족한 수많은 저자원 언어 및 특정 도메인에 고성능 ASR 기술을 적용할 수 있는 현실적인 길을 열었다.1 이는 음성 기술의 보편화와 민주화에 있어 중요한 진전이다.</p>
<p>둘째, 기술적으로 Wav2vec 2.0은 NLP 분야에서 검증된 BERT의 성공적인 패러다임을 본질적으로 다른 특성을 가진 연속적인 음성 신호에 효과적으로 적용하기 위한 독창적인 솔루션을 제시했다. 잠재 공간에서의 마스킹과, 예측 타겟 자체를 데이터로부터 함께 학습하는 공동 학습 기반의 양자화 메커니즘은 연속 신호를 이산적인 예측 과제로 변환하는 우아하고 강력한 방법론을 제공했다.1</p>
<p>셋째, Wav2vec 2.0은 음성 처리 분야에서 ‘사전 학습-미세 조정’ 접근법의 표준을 정립했다. 이 모델의 성공 이후, 대규모 비레이블 데이터로 사전 학습된 거대 모델을 기반으로 다양한 다운스트림 음성 과제를 해결하는 방식이 주류로 자리 잡았다. 이는 ASR을 넘어 화자 인식, 감정 인식, 음성 번역 등 여러 분야를 위한 강력한 기반 모델(foundation model)의 가능성을 입증한 것이다.16</p>
<p>Wav2vec 2.0이 연 새로운 지평은 현재도 계속해서 확장되고 있다. 실시간 음성 인식을 위한 스트리밍 ASR 모델 19, 다양한 언어를 동시에 처리하는 다국어 및 교차언어 모델(XLSR) 4, 그리고 음성을 넘어 이미지와 텍스트까지 아우르는 범용 멀티모달 모델(data2vec) 27에 이르기까지, 그 핵심 철학은 끊임없이 진화하고 있다. 이는 값비싼 레이블 데이터의 제약에서 벗어나, 세상에 존재하는 방대한 비정형 데이터로부터 스스로 지식을 추출하고 학습하는 인공지능의 미래를 선명하게 보여준다. Wav2vec 2.0은 그 미래를 향한 중요한 발걸음이었다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>wav2vec 2.0: A Framework for Self-Supervised Learning of Speech …, https://arxiv.org/pdf/2006.11477</li>
<li>Wav2Vec2: Self-A Supervised Learning Technique for Speech Representations, https://www.geeksforgeeks.org/nlp/wav2vec2-self-a-supervised-learning-technique-for-speech-representations/</li>
<li>An Illustrated Tour of Wav2vec 2.0 | Jonathan Bgn, https://jonathanbgn.com/2021/09/30/illustrated-wav2vec-2.html</li>
<li>Wav2vec 2.0: Learning the structure of speech from raw audio, https://ai.meta.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/</li>
<li>Wav2vec: State-of-the-art speech recognition through self-supervision - Meta AI, https://ai.meta.com/blog/wav2vec-state-of-the-art-speech-recognition-through-self-supervision/</li>
<li>wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations, https://www.researchgate.net/publication/342377803_wav2vec_20_A_Framework_for_Self-Supervised_Learning_of_Speech_Representations</li>
<li>wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations, https://huggingface.co/papers/2006.11477</li>
<li>Wav2Vec2 - Hugging Face, https://huggingface.co/docs/transformers/model_doc/wav2vec2</li>
<li>wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations - NeurIPS 2020, https://neurips.cc/virtual/2020/public/poster_92d1e1eb1cd6f9fba3227870bb6d7f07.html</li>
<li>[2202.05993] Wav2Vec2.0 on the Edge: Performance Evaluation - arXiv, https://arxiv.org/abs/2202.05993</li>
<li>Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech Representations, https://towardsdatascience.com/wav2vec-2-0-a-framework-for-self-supervised-learning-of-speech-representations-7d3728688cae/</li>
<li>Wav2Vec 2.0: Self-Supervised Learning of Speech Representations, https://neurosys.com/blog/wav2vec-2-0-framework</li>
<li>Brief Review — wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations - Sik-Ho Tsang, https://sh-tsang.medium.com/brief-review-wav2vec-2-0-a-framework-for-self-supervised-learning-of-speech-representations-9b9a8fdab85e</li>
<li>facebook (AI at Meta) - Hugging Face, https://huggingface.co/facebook/collections</li>
<li>[2006.11477] wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations : r/speechtech - Reddit, https://www.reddit.com/r/speechtech/comments/hez1kz/200611477_wav2vec_20_a_framework_for/</li>
<li>Daily Papers - Hugging Face, <a href="https://huggingface.co/papers?q=wav2vec+2.0">https://huggingface.co/papers?q=wav2vec%202.0</a></li>
<li>OPTIMIZE WAV2VEC2S ARCHITECTURE FOR SMALL TRAINING SET THROUGH ANALYZING ITS PRE-TRAINED MODELS ATTENTION PATTERN - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC10185057/</li>
<li>Wav2Vec-Aug: Improved self-supervised training with limited data - ISCA Archive, https://www.isca-archive.org/interspeech_2022/sriram22_interspeech.pdf</li>
<li>wav2vec-S: Adapting Pre-trained Speech Models for Streaming - ACL Anthology, https://aclanthology.org/2024.findings-acl.681.pdf</li>
<li>Working With Wav2vec2 Part 1: Finetuning XLS-R for Automatic Speech Recognition, https://hackernoon.com/working-with-wav2vec2-part-1-finetuning-xls-r-for-automatic-speech-recognition</li>
<li>Ranking Pretrained Speech Embeddings in Parkinson’s Disease …, https://www.medrxiv.org/content/10.1101/2025.01.29.25321319v1.full-text</li>
<li>(PDF) wav2vec: Unsupervised Pre-training for Speech Recognition - ResearchGate, https://www.researchgate.net/publication/332368694_wav2vec_Unsupervised_Pre-training_for_Speech_Recognition</li>
<li>wav2vec: Unsupervised Pre-Training for Speech Recognition - arXiv, https://arxiv.org/pdf/1904.05862</li>
<li>Wav2Vec: Learning Speech Without Labels | by Dong-Keon Kim | Medium, https://medium.com/@kdk199604/wav2vec-learning-speech-without-labels-0999efaacc08</li>
<li>Self-training and pre-training, understanding the wav2vec series - - Maël Fabien, https://maelfabien.github.io/machinelearning/wav2vec/</li>
<li>Daily Papers - Hugging Face, https://huggingface.co/papers?q=Wav2Vec-BERT-2.0</li>
<li>[PDF] wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations, https://www.semanticscholar.org/paper/wav2vec-2.0%3A-A-Framework-for-Self-Supervised-of-Baevski-Zhou/49a049dc85e2380dde80501a984878341dd8efdf</li>
<li>Figure 1 from wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations | Semantic Scholar, https://www.semanticscholar.org/paper/wav2vec-2.0%3A-A-Framework-for-Self-Supervised-of-Baevski-Zhou/49a049dc85e2380dde80501a984878341dd8efdf/figure/0</li>
<li>Layer-wise benchmarking of wav2vec 2.0 Large, HuBERT Large and Data2vec… | Download Scientific Diagram - ResearchGate, https://www.researchgate.net/figure/Layer-wise-benchmarking-of-wav2vec-20-Large-HuBERT-Large-and-Data2vec-Large-on-5-tasks_fig3_379882278</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>