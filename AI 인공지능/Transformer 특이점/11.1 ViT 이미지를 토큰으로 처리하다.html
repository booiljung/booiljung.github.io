<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:11.1 ViT, 이미지를 토큰으로 처리하다</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>11.1 ViT, 이미지를 토큰으로 처리하다</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">트랜스포머 싱귤래리티 (The Transformer Singularity)</a> / <span>11.1 ViT, 이미지를 토큰으로 처리하다</span></nav>
                </div>
            </header>
            <article>
                <h1>11.1 ViT, 이미지를 토큰으로 처리하다</h1>
<p>2025-12-22, G30DR</p>
<p>인공지능의 역사, 특히 컴퓨터 비전(Computer Vision)의 발전사에서 2020년 말은 하나의 거대한 분기점으로 기록된다. 지난 10년 가까이 이미지 인식 분야의 사실상 표준(De facto standard)으로 군림해 온 합성곱 신경망(Convolutional Neural Networks, CNN)의 아성에 도전하는 새로운 아키텍처가 등장했기 때문이다. 구글 리서치(Google Research) 팀이 발표한 “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale“이라는 도발적인 제목의 논문은 자연어 처리(NLP) 분야를 완전히 평정한 트랜스포머(Transformer)가 이미지 처리에서도 CNN을 대체하거나 그 이상의 성능을 발휘할 수 있음을 증명했다.1 이 사건은 단순한 성능 개선을 넘어, 서로 다른 데이터 모달리티(Modality)가 하나의 통일된 구조로 처리될 수 있다는 ’트랜스포머 싱귤래리티(Transformer Singularity)’의 서막을 알리는 신호탄이었다.</p>
<p>이 장에서는 비전 트랜스포머(Vision Transformer, ViT)의 가장 핵심적인 혁신, 즉 ’이미지를 어떻게 언어처럼 처리하는가’에 대한 메커니즘을 아주 상세하게, 그리고 근본적인 원리부터 파헤쳐 볼 것이다. 연속적인 2차원 신호인 이미지를 불연속적인 ’토큰(Token)’의 나열로 변환하고, 이를 통해 픽셀 간의 좁은 지역적 연결성(Locality)이 아닌 전역적 관계(Global Dependency)를 학습하는 과정은 기존 컴퓨터 비전의 패러다임을 근본적으로 뒤흔드는 접근이었다. 우리는 이 과정을 픽셀 수준의 미시적 관점부터 데이터셋 규모에 따른 거시적 스케일링 법칙까지 폭넓게 다루며, ViT가 어떻게 시각적 정보를 ‘읽어내는지’ 분석할 것이다.</p>
<h2>1.  시각적 언어의 탄생: 픽셀에서 패치로</h2>
<p>트랜스포머 아키텍처를 이미지 처리에 적용하려 할 때 연구자들이 직면한 가장 첫 번째 장벽은 데이터의 형태였다. 트랜스포머는 본래 기계 번역을 위해 고안된 모델로, 단어(Word)나 서브워드(Sub-word)라는 이산적(Discrete)이고 순차적인(Sequential) 데이터를 입력으로 받도록 설계되었다. 반면, 이미지는 연속적인(Continuous) 값을 가지는 픽셀들의 2차원 격자(Grid)이며, 그 정보량은 텍스트에 비해 압도적으로 많다.</p>
<h3>1.1  계산 복잡도의 장벽과 토큰화의 필요성</h3>
<p>가장 직관적인 접근은 이미지의 각 픽셀을 하나의 토큰으로 취급하는 것이다. 예를 들어, 표준적인 ImageNet 데이터셋의 해상도인 <span class="math math-inline">224 \times 224</span> 컬러 이미지를 생각해 보자. 이를 픽셀 단위로 펼친다면 <span class="math math-inline">224 \times 224 = 50,176</span>개의 시퀀스 길이가 생성된다. 트랜스포머의 핵심 연산인 셀프 어텐션(Self-Attention)은 입력 시퀀스 길이 <span class="math math-inline">N</span>에 대해 메모리와 계산 복잡도가 <span class="math math-inline">O(N^2)</span>로 증가하는 특성을 가진다. 5만 개가 넘는 토큰에 대해 <span class="math math-inline">N^2</span> 연산을 수행하는 것은 약 25억 회 이상의 내적(Dot product) 연산을 필요로 하며, 이는 현재의 하드웨어로도 감당하기 힘든 엄청난 계산 비용을 초래한다.1</p>
<p>따라서 이미지를 트랜스포머가 소화할 수 있는 수준의 길이로 압축하면서도, 원본 이미지가 가진 시각적 정보를 잃지 않는 효율적인 ‘토큰화(Tokenization)’ 전략이 필수적이었다. 이는 마치 긴 문단을 낱글자 단위(Character-level)로 처리하는 대신 의미 있는 단어 단위(Word-level)로 처리하여 문맥 파악의 효율을 높이는 것과 유사하다. ViT는 이 문제를 해결하기 위해 ’패치(Patch)’라는 개념을 도입했다. 이는 이미지를 픽셀 단위가 아닌, 고정된 크기의 작은 조각으로 나누어 처리하는 방식이다. 논문의 제목이 시사하듯, 이미지는 <span class="math math-inline">16 \times 16</span> 픽셀 크기의 패치들로 분할되며, 각 패치는 NLP에서의 ’단어’와 같은 역할을 수행하게 된다.1</p>
<h3>1.2  패치 분할(Patch Partitioning)의 수학적 모델링</h3>
<p>입력 이미지 <span class="math math-inline">x</span>의 차원이 <span class="math math-inline">H \times W \times C</span>라고 가정하자. 여기서 <span class="math math-inline">H</span>는 높이, <span class="math math-inline">W</span>는 너비, <span class="math math-inline">C</span>는 채널 수이다. 일반적인 RGB 이미지의 경우 <span class="math math-inline">C=3</span>이 된다. ViT는 이를 크기가 <span class="math math-inline">(P, P)</span>인 <span class="math math-inline">N</span>개의 패치로 재구성한다. 이때 총 패치의 개수, 즉 트랜스포머에 입력될 시퀀스의 길이 <span class="math math-inline">N</span>은 다음과 같이 계산된다.<br />
<span class="math math-display">
N = \frac{H \times W}{P \times P}
</span><br />
예를 들어, <span class="math math-inline">224 \times 224</span> 크기의 이미지를 <span class="math math-inline">16 \times 16</span> 크기의 패치(<span class="math math-inline">P=16</span>)로 나눈다면, 총 패치의 개수는 다음과 같다.<br />
<span class="math math-display">
N = \frac{224 \times 224}{16 \times 16} = 14 \times 14 = 196
</span><br />
이 과정은 5만 개가 넘던 픽셀 시퀀스를 196개의 패치 시퀀스로 획기적으로 줄여주며, 트랜스포머가 현실적인 시간 내에 연산 가능한 범위 내로 문제를 축소시킨다.4 <span class="math math-inline">N=196</span>은 NLP에서 다루는 일반적인 문장의 길이(보통 512 이하)와 매우 유사한 수준이므로, 기존의 트랜스포머 인코더 구조를 거의 수정 없이 그대로 사용할 수 있게 된다.</p>
<p>이러한 패치 분할 과정은 이미지의 2차원 공간적 구조(Spatial Structure)를 보존하는 유일한 전처리 단계이다. 이후의 모든 과정에서 이미지는 2차원 격자가 아닌 1차원 시퀀스로 취급되므로, 이 단계에서 인접한 픽셀들을 하나의 패치로 묶는 것은 지역적 정보(Local Information)를 보존하는 중요한 역할을 한다.</p>
<h3>1.3  비중첩 패치와 CNN의 차이점</h3>
<p>여기서 주목해야 할 점은 ViT의 패치 분할이 겹치지 않는(Non-overlapping) 방식으로 수행된다는 것이다. 전통적인 CNN은 스트라이드(Stride)를 커널 크기보다 작게 설정하여 윈도우가 겹치도록 이동하며(Sliding Window), 이를 통해 경계 부분의 정보를 부드럽게 연결하고 중복된 특징을 추출한다. 반면, ViT는 이미지를 격자 무늬로 칼로 자르듯 명확하게 분할한다. 각 패치는 서로 독립적인 정보 단위로 분리되며, 패치 내부의 픽셀 정보는 보존되지만 패치 경계선(Boundary)에 걸친 픽셀 간의 직접적인 연결성은 물리적으로 끊어지게 된다.3</p>
<p>이러한 “Hard Splitting” 방식은 초기에는 정보 손실이나 경계 아티팩트(Artifact)를 유발할 수 있다는 우려를 낳았으나, 결과적으로 트랜스포머의 강력한 전역적 어텐션 메커니즘이 패치 간의 관계를 학습함으로써 이러한 단절을 극복할 수 있음이 밝혀졌다. 이는 이미지를 ’연속된 신호’로 보던 관점에서 ’이산적인 의미 단위의 집합’으로 보는 관점으로의 전환을 의미한다.7</p>
<h3>1.4  2D 구조의 1D 평탄화 (Flattening)</h3>
<p>생성된 각 패치 <span class="math math-inline">x_p</span>는 원래 3차원 텐서(<span class="math math-inline">P \times P \times C</span>) 형태를 띤다. 예를 들어 <span class="math math-inline">P=16, C=3</span>이라면 각 패치는 <span class="math math-inline">16 \times 16 \times 3 = 768</span>개의 값을 가진 작은 육면체 데이터이다. 그러나 표준 트랜스포머는 벡터들의 시퀀스를 입력으로 받으므로, 이 3차원 패치를 1차원 벡터로 평탄화(Flattening)해야 한다.<br />
<span class="math math-display">
x_p \in \mathbb{R}^{P \times P \times C} \rightarrow x_{\text{flatten}} \in \mathbb{R}^{P^2 \cdot C}
</span><br />
즉, 각 패치는 크기가 <span class="math math-inline">P^2 \cdot C</span>인 벡터로 변환된다. <span class="math math-inline">16 \times 16</span> RGB 패치의 경우 <span class="math math-inline">16 \times 16 \times 3 = 768</span> 차원의 벡터가 된다. 이렇게 평탄화된 패치들의 나열은 다음과 같이 표현할 수 있다.<br />
<span class="math math-display">
\mathbf{x}_{\text{sequence}} = [x_1, x_2, \dots, x_N] \in \mathbb{R}^{N \times (P^2 \cdot C)}
</span><br />
이 지점에서 이미지는 더 이상 <span class="math math-inline">H \times W</span>의 공간을 가진 그림이 아니라, 순서가 있는(혹은 순서를 부여받아야 할) <span class="math math-inline">N</span>개의 벡터 집합이 된다. 이는 “이미지를 문장처럼, 패치를 단어처럼” 취급하겠다는 ViT의 철학이 구현되는 물리적 변환의 첫 단계이다.5</p>
<h2>2.  선형 투영과 임베딩 공간 (Linear Projection)</h2>
<p>평탄화된 패치 벡터(<span class="math math-inline">P^2 \cdot C</span> 차원)는 아직 트랜스포머 모델 내부에서 처리될 ’잠재 표현(Latent Representation)’이 아니다. NLP에서 ’사과’라는 단어를 원-핫 인코딩(One-hot encoding)이나 단순 정수 인덱스가 아닌, 고정된 차원의 밀집 벡터(Dense Vector)인 ’워드 임베딩(Word Embedding)’으로 변환하듯, ViT 역시 날것의 픽셀 값 벡터를 모델이 학습할 수 있는 차원 <span class="math math-inline">D</span>로 매핑해야 한다. 이 과정을 ’선형 투영(Linear Projection)’이라고 부른다.1</p>
<h3>2.1  학습 가능한 선형 레이어의 역할</h3>
<p>각 패치 벡터 <span class="math math-inline">x_i</span>는 학습 가능한 가중치 행렬 <span class="math math-inline">\mathbf{E} \in \mathbb{R}^{(P^2 \cdot C) \times D}</span>와 곱해져 <span class="math math-inline">D</span> 차원의 패치 임베딩으로 변환된다.<br />
<span class="math math-display">
\mathbf{z}_i = x_i \cdot \mathbf{E}
</span><br />
여기서 <span class="math math-inline">D</span>는 트랜스포머의 모든 레이어에서 유지되는 잠재 벡터의 크기(Hidden Size)이다. 예를 들어, ViT-Base 모델의 경우 <span class="math math-inline">D=768</span>, ViT-Large 모델은 <span class="math math-inline">D=1024</span>, ViT-Huge 모델은 <span class="math math-inline">D=1280</span>으로 설정된다.8 이 <span class="math math-inline">D</span> 값은 모델의 용량(Capacity)을 결정하는 중요한 하이퍼파라미터이다.</p>
<p>이 선형 투영 연산은 단순히 데이터의 차원을 맞추는 기술적인 절차 이상의 의미를 지닌다.</p>
<ol>
<li><strong>특징 추출의 시작:</strong> 날것의 픽셀 밝기(Intensity) 값을 모델이 이해할 수 있는 추상적인 특징 공간(Feature Space)으로 이동시킨다.</li>
<li><strong>정보의 압축과 확장:</strong> 패치의 원래 차원(<span class="math math-inline">P^2 \cdot C</span>)과 목표 임베딩 차원(<span class="math math-inline">D</span>) 사이의 관계에 따라 정보를 압축하거나 확장하여 표현력을 조절한다. <span class="math math-inline">16 \times 16</span> 패치의 경우 입력 차원(768)과 ViT-Base의 임베딩 차원(768)이 우연히 같지만, 다른 패치 크기나 모델 크기에서는 이 비율이 달라진다.</li>
</ol>
<h3>2.2  합성곱(Convolution)과의 등가성</h3>
<p>흥미로운 점은 이 선형 투영 연산이 수학적으로 <strong>커널 크기가 <span class="math math-inline">P \times P</span>이고 스트라이드(Stride)가 <span class="math math-inline">P</span>인 합성곱(Convolution) 연산</strong>과 완전히 동일하다는 것이다. 실제로 <code>Pytorch</code>나 <code>TensorFlow</code>로 구현된 대부분의 ViT 코드는 효율성을 위해 <code>Linear</code> 레이어 대신 <code>Conv2d</code> 레이어를 사용하여 이 단계를 수행한다.</p>
<pre><code class="language-Python"># PyTorch 스타일의 구현 예시
self.proj = nn.Conv2d(in_channels=3, out_channels=768, kernel_size=16, stride=16)
</code></pre>
<p>이 코드는 입력 이미지에 대해 <span class="math math-inline">16 \times 16</span> 크기의 필터를 16픽셀 간격으로 이동하며 적용하는 것과 같다. 이는 겹치지 않는 패치에 대해 선형 변환을 수행하는 것과 수학적으로 동치이다. 그러나 기존의 CNN이 <span class="math math-inline">3 \times 3</span> 같은 작은 커널을 사용하여 깊게 쌓아 올리며 점진적으로 수용 영역(Receptive Field)을 넓혀가는 것과 달리, ViT의 이 ’단일 합성곱 층’은 한 번에 <span class="math math-inline">16 \times 16</span>이라는 비교적 넓은 영역의 정보를 하나의 벡터로 압축해 버린다.2 이후에는 더 이상의 합성곱 연산 없이 오직 어텐션 메커니즘에 의존하게 된다. 이는 CNN의 귀납적 편향(Inductive Bias)을 최소화하고 트랜스포머의 범용적 처리 능력에 의존하겠다는 의도를 명확히 보여준다.</p>
<h3>2.3  물리적 차원과 의미론적 차원의 결합</h3>
<p>선형 투영층은 이미지 패치라는 ’물리적 배열’을 패치 임베딩이라는 ’의미론적 벡터’로 변환하면서도, 그 안에 내재된 시각적 유사성을 보존하려 시도한다.1 학습이 진행됨에 따라, 시각적으로 유사한 패턴(예: 수평선, 모서리, 특정 색상 패턴)을 가진 패치들은 임베딩 공간상에서도 서로 가까운 거리에 위치하도록 가중치 행렬 <span class="math math-inline">\mathbf{E}</span>가 업데이트된다. 이는 모델이 픽셀의 단순한 나열이 아닌, 이미지의 구성 요소(Visual Words)를 학습하는 기초가 된다.</p>
<p>결과적으로, 이 단계에서 생성된 패치 임베딩들의 시퀀스는 트랜스포머 인코더의 입력이 되며, 이후의 모든 연산(Self-Attention, MLP)은 이 임베딩 벡터들을 대상으로 수행된다. 즉, 픽셀 정보는 선형 투영을 거치면서 모델 내부의 ’언어’로 번역되는 것이다.</p>
<h2>3.  위치 임베딩: 무질서 속의 질서 (Positional Embeddings)</h2>
<p>트랜스포머 아키텍처를 이미지 처리에 적용할 때 가장 치명적인 약점이 될 수 있는 부분은 바로 ’순열 불변성(Permutation Invariance)’이다. 트랜스포머의 핵심인 셀프 어텐션 메커니즘은 입력된 토큰들의 순서를 전혀 고려하지 않는다. 집합(Set) 연산과 같아서, 입력 패치들의 순서를 무작위로 뒤섞어도 각 패치가 출력하는 값은 (자신이 어텐션하는 대상의 순서만 바뀔 뿐) 동일한 내용의 집합이 된다.1</p>
<h3>3.1  위치 정보의 절대적 중요성</h3>
<p>이미지에서 위치 정보는 단순한 부가 정보가 아니라 내용 그 자체를 결정하는 절대적인 요소이다. 사람의 얼굴 이미지를 9개의 조각으로 나누어 무작위로 섞어놓은 퍼즐(Jigsaw puzzle)을 상상해 보자. 눈이 입 아래에 있고, 코가 귀 옆에 있다면 우리는 그것을 ’얼굴’이라고 인식하기 어렵다. 이미지의 공간적 구조(Spatial Structure)가 곧 이미지의 의미(Semantics)를 형성한다.1</p>
<p>언어의 경우 “아버지가 방에 들어가신다“와 “방에 아버지가 들어가신다“처럼 순서가 약간 바뀌어도 의미가 통하는 경우가 있지만, 이미지에서 픽셀이나 패치의 위치가 바뀌면 그 의미는 완전히 파괴된다. 따라서 ViT는 패치 임베딩에 반드시 ’위치 정보’를 주입해야 한다. 이를 위해 각 패치 임베딩 벡터에 ‘위치 임베딩(Positional Embeddings)’ 벡터를 요소별 합(Element-wise sum)으로 더해준다.3<br />
<span class="math math-display">
\mathbf{z}_{\text{input}} = \mathbf{z}_{\text{patches}} + \mathbf{E}_{\text{pos}}
</span><br />
이렇게 하면, 동일한 시각적 정보를 가진 패치라도 위치에 따라 서로 다른 값을 가지게 되며, 트랜스포머는 이를 통해 “이 패치는 왼쪽 위에 있고, 저 패치는 오른쪽 아래에 있다“는 사실을 구별할 수 있게 된다.</p>
<h3>3.2  1차원 대 2차원 위치 임베딩의 역설</h3>
<p>직관적으로 생각하면, 이미지는 2차원 평면 구조이므로 <span class="math math-inline">(x, y)</span> 좌표 정보를 모두 반영한 2차원 위치 임베딩을 사용하는 것이 타당해 보인다. 예를 들어, 행(Row) 임베딩과 열(Column) 임베딩을 따로 만들어 더하거나 연결(Concatenate)하는 방식을 고려할 수 있다.</p>
<p>그러나 ViT 논문의 저자들이 수행한 삭감 연구(Ablation Study) 결과는 다소 충격적이었다. 그들은 다음 네 가지 방식을 비교했다.</p>
<ol>
<li><strong>No Positional Embedding:</strong> 위치 정보를 아예 주지 않음.</li>
<li><strong>1D Positional Embedding:</strong> 패치를 좌상단부터 우하단까지 1열로 쭉 나열하여 순서 번호(1, 2,…, N)만 부여.</li>
<li><strong>2D Positional Embedding:</strong> X좌표와 Y좌표 정보를 별도로 학습하여 결합.</li>
<li><strong>Relative Positional Embedding:</strong> 패치 간의 상대적 거리만 학습.</li>
</ol>
<p>실험 결과는 다음과 같았다.</p>
<ul>
<li><strong>위치 임베딩 없음:</strong> 성능이 현저히 떨어진다. 이는 공간 정보가 필수적임을 다시 한번 증명한다.</li>
<li><strong>1D vs 2D vs Relative:</strong> 놀랍게도, <strong>이 세 가지 방식 간의 성능 차이는 거의 없었다.</strong>.4</li>
</ul>
<p>이 결과는 트랜스포머 모델이 충분한 데이터와 학습을 통해, 단순히 1차원으로 나열된 패치 시퀀스 안에서도 스스로 2차원 공간적 관계를 복원하고 학습할 수 있는 강력한 능력을 갖추고 있음을 시사한다.5 모델은 훈련 과정에서 “7번 패치와 8번 패치는 가로로 인접해 있고, 7번과 21번 패치(너비가 14패치일 때)는 세로로 인접해 있다“는 사실을 위치 임베딩 벡터 간의 상관관계를 통해 스스로 깨닫게 된다.</p>
<p>결과적으로 ViT는 구현의 단순성을 위해 **학습 가능한 1차원 위치 임베딩(Standard learnable 1D position embeddings)**을 표준으로 채택했다.10 이는 인간이 굳이 2차원 구조를 명시적으로 주입하지 않아도, 데이터 기반 학습이 이를 극복할 수 있다는 ’데이터 중심 AI’의 철학을 뒷받침하는 사례이기도 하다.</p>
<h3>3.3  해상도 변경과 위치 임베딩의 보간(Interpolation)</h3>
<p>ViT를 운용할 때 한 가지 기술적 난관은 사전 학습(Pre-training)과 미세 조정(Fine-tuning) 시의 이미지 해상도가 다른 경우다. 일반적으로 대규모 데이터셋(JFT-300M, ImageNet-21k)으로 사전 학습할 때는 효율성을 위해 낮은 해상도(예: <span class="math math-inline">224 \times 224</span>)를 사용하고, 다운스트림 작업(Downstream Task)에 미세 조정할 때는 더 높은 정확도를 위해 높은 해상도(예: <span class="math math-inline">384 \times 384</span> 또는 <span class="math math-inline">512 \times 512</span>)를 사용한다.11</p>
<p>이미지 해상도가 커지면, 고정된 패치 크기(<span class="math math-inline">P=16</span>)로 인해 패치의 총 개수 <span class="math math-inline">N</span>이 늘어나게 된다.</p>
<ul>
<li><span class="math math-inline">224 \times 224 \rightarrow 14 \times 14 = 196</span>개 패치</li>
<li><span class="math math-inline">384 \times 384 \rightarrow 24 \times 24 = 576</span>개 패치</li>
</ul>
<p>기존에 학습된 위치 임베딩 <span class="math math-inline">\mathbf{E}_{\text{pos}}</span>는 길이가 196(정확히는 +1 토큰)으로 고정되어 있으므로, 576개로 늘어난 시퀀스에는 그대로 적용할 수 없다. 이때 ViT는 1차원 위치 임베딩 벡터들을 원래 이미지 상의 2차원 위치로 재배치한 뒤, **이중 선형 보간(Bicubic Interpolation)**을 수행하여 늘어난 해상도에 맞는 새로운 위치 임베딩 값을 계산해 낸다.5</p>
<p>예를 들어, 기존의 <span class="math math-inline">(1, 1)</span> 위치 임베딩과 <span class="math math-inline">(1, 2)</span> 위치 임베딩 사이의 값을 보간하여 더 촘촘해진 격자의 위치 값을 채워 넣는 것이다. 이는 ViT 아키텍처에서 2차원적 구조 정보(Inductive Bias)가 명시적으로 활용되는 거의 유일한 순간이다. 이 보간 과정 덕분에 ViT는 가변적인 입력 해상도에도 유연하게 대처할 수 있다.</p>
<h2>4.  토큰: 전체를 아우르는 대표자</h2>
<p>입력 시퀀스를 구성할 때, ViT는 이미지 패치 임베딩 외에 또 하나의 특별한 토큰을 시퀀스의 맨 앞에 추가한다. 바로 <code>[CLS]</code>(Classification) 토큰이다. 이는 자연어 처리 모델인 BERT(Bidirectional Encoder Representations from Transformers)에서 차용한 개념으로, ViT가 NLP 모델의 구조를 얼마나 충실히 계승했는지를 보여주는 상징적인 부분이다.2</p>
<h3>4.1  정보 집약을 위한 가상의 토큰</h3>
<p>일반적인 CNN 기반의 분류 모델(예: ResNet)은 마지막 합성곱 층의 출력인 특성 맵(Feature Map)에 전역 평균 풀링(Global Average Pooling, GAP)을 적용하여 공간 정보를 압축하고 하나의 벡터로 만든다. 초기 ViT 연구진도 이와 유사하게 모든 패치 출력의 평균을 사용하는 방식(Mean Pooling)을 고려했다. 하지만 그들은 BERT의 성공 방식을 그대로 따르기로 결정하고, 학습 가능한 파라미터로 초기화된 별도의 벡터 <span class="math math-inline">x_{\text{class}}</span>를 준비했다.<br />
<span class="math math-display">
z_0 = [x_{\text{class}}; x_1^p \mathbf{E}; x_2^p \mathbf{E}; \cdots ; x_N^p \mathbf{E}] + \mathbf{E}_{pos}
</span><br />
이 <code>[CLS]</code> 토큰은 이미지의 어떤 특정 패치 정보도 담고 있지 않은 상태(Random Initialization)로 시작한다. 그러나 트랜스포머의 인코더 층을 통과하면서 셀프 어텐션 메커니즘을 통해 다른 모든 패치 토큰들의 정보를 흡수(Aggregate)하게 된다.3</p>
<h3>4.2  토큰의 작동 원리</h3>
<p>트랜스포머의 셀프 어텐션은 모든 토큰이 다른 모든 토큰을 참조하는 과정이다.</p>
<ol>
<li>첫 번째 층에서 <code>[CLS]</code> 토큰은 모든 이미지 패치들과 상호작용하며 전체적인 이미지의 초기 정보를 수집한다.</li>
<li>층이 거듭될수록 <code>[CLS]</code> 토큰은 더 고차원적이고 추상적인 이미지의 문맥 정보를 업데이트한다.</li>
<li>마지막 층(<span class="math math-inline">L</span>)을 통과한 후, <code>[CLS]</code> 토큰의 위치에 해당하는 벡터 <span class="math math-inline">z_L^0</span>는 **이미지 전체를 대표하는 표현(Global Image Representation)**으로 간주된다.</li>
</ol>
<p>최종 분류를 위한 MLP 헤드(Classification Head)는 오직 이 <code>[CLS]</code> 토큰의 출력 벡터 하나만을 입력으로 받아 클래스를 예측한다. 나머지 패치 토큰들의 출력은 분류 작업에서 직접적으로 사용되지 않고 버려진다(물론 이들은 <code>[CLS]</code> 토큰이 정보를 잘 학습하도록 돕는 과정에서 간접적으로 기여한다).3</p>
<h3>4.3  GAP와의 비교 및 최신 트렌드</h3>
<p>ViT 논문 이후의 연구들에서는 <code>[CLS]</code>큰 대신 전역 평균 풀링(GAP)을 사용하는 것이 성능상 큰 차이가 없거나 오히려 더 낫다는 결과들도 보고되었다(예: Swin Transformer 등은 GAP를 주로 사용한다). GAP는 모든 패치의 정보를 균등하게 평균 내므로 전체적인 정보를 안정적으로 요약하는 반면, <code>[CLS]</code> 토큰은 어텐션 가중치에 따라 특정 정보에 더 집중할 수 있는 유연성을 가진다.</p>
<p>초기 ViT가 <code>[CLS]</code> 토큰을 고집한 것은 “이미지 처리를 텍스트 처리와 최대한 동일하게 만들겠다“는 의도가 강했기 때문이다. 이는 ViT를 텍스트-이미지 멀티모달 모델(예: CLIP, ALIGN)로 확장할 때 구조적 통일성을 제공하는 이점이 있다.</p>
<h2>5.  귀납적 편향의 제거와 스케일링의 미학</h2>
<p>ViT가 이미지를 토큰으로 처리하는 방식은 단순히 기술적인 변화가 아니다. 이는 컴퓨터 비전 모델이 세상을 바라보는 방식, 즉 ’귀납적 편향(Inductive Bias)’에 대한 철학적 전환을 의미한다.7 귀납적 편향이란 모델이 학습하지 않은 데이터에 대해 예측할 때 사용하는 추가적인 가정(Assumption)을 말한다.</p>
<h3>5.1  CNN의 강력한 귀납적 편향: 축복이자 족쇄</h3>
<p>CNN은 태생적으로 강력한 귀납적 편향을 가지고 설계되었다.</p>
<ol>
<li><strong>지역성(Locality):</strong> 이미지는 인접한 픽셀끼리 서로 밀접한 관련이 있다는 가정. CNN의 커널은 작은 영역만을 본다.</li>
<li><strong>이동 불변성(Translation Equivariance):</strong> 고양이가 이미지의 왼쪽 위에 있든 오른쪽 아래에 있든 동일한 ‘고양이’ 특징으로 인식되어야 한다는 가정. CNN의 가중치 공유(Weight Sharing)가 이를 보장한다.7</li>
</ol>
<p>이러한 편향은 데이터가 적을 때 매우 효율적이다. 모델이 처음부터 “가까운 것부터 봐라”, “위치는 중요하지 않다“라는 올바른 사전 지식을 가지고 학습을 시작하는 것과 같기 때문이다. 따라서 CNN은 적은 데이터로도 빠르게 수렴하고 좋은 성능을 낸다.12</p>
<h3>5.2  ViT의 자유도와 데이터 굶주림(Data Hungry)</h3>
<p>반면, ViT는 이러한 가정을 과감히 제거했다.</p>
<ul>
<li>이미지를 패치로 자르고 1차원 시퀀스로 만든다.</li>
<li>모든 패치가 첫 번째 레이어부터 서로 어텐션을 수행할 수 있게 한다(Global Context).</li>
<li>위치 정보를 강제하지 않고 학습 가능한 임베딩으로 제공한다.</li>
</ul>
<p>이로 인해 ViT에는 ’지역성’이나 ’이동 불변성’에 대한 하드코딩된 제약이 거의 없다. 모델은 이미지의 공간적 특성(가까운 픽셀이 중요하다는 사실 등)을 데이터로부터 처음부터(from scratch) 스스로 배워야 한다. 이 과정은 매우 어렵고 많은 데이터를 필요로 한다. 데이터가 충분하지 않은 경우(예: ImageNet-1k, 130만 장), ViT는 사전 지식이 부족하여 CNN보다 성능이 떨어지거나 과적합(Overfitting)되기 쉽다.8 이를 ViT의 “데이터 굶주림(Data Hungry)” 현상이라 부른다.7</p>
<p>초기 실험에서 ViT-Large 모델을 ImageNet-1k만으로 학습시켰을 때, 훨씬 작은 ResNet 모델보다도 성능이 낮게 나오는 굴욕을 겪기도 했다. 이는 귀납적 편향의 부재가 작은 데이터 영역에서는 약점으로 작용함을 보여준다.</p>
<h3>5.3  스케일링 법칙(Scaling Laws): 거인의 어깨 위에서</h3>
<p>그러나 데이터의 양이 압도적으로 많아지면 상황은 역전된다. 구글이 보유한 3억 장 규모의 비공개 데이터셋인 <strong>JFT-300M</strong>으로 사전 학습을 수행했을 때, ViT는 CNN 기반의 최신 모델(BiT, EfficientNet 등)을 모든 벤치마크에서 앞지르기 시작했다.8</p>
<table><thead><tr><th><strong>모델 (사전 학습 데이터)</strong></th><th><strong>ImageNet Top-1 정확도</strong></th><th><strong>특징</strong></th></tr></thead><tbody>
<tr><td><strong>ResNet (CNN)</strong></td><td>~78% (ImageNet-1k)</td><td>데이터가 적을 때 강함 (높은 귀납적 편향)</td></tr>
<tr><td><strong>ViT (ImageNet-1k)</strong></td><td>~78% 이하</td><td>CNN에 열세, 일반화 능력 부족</td></tr>
<tr><td><strong>ViT (ImageNet-21k)</strong></td><td>~84%</td><td>CNN과 대등하거나 우세해지기 시작</td></tr>
<tr><td><strong>ViT (JFT-300M)</strong></td><td><strong>88.55%</strong></td><td><strong>CNN 압도 (Scaling Law 실현)</strong></td></tr>
</tbody></table>
<p>[표 11.1] 데이터셋 크기에 따른 모델 성능 비교 8</p>
<p>위 표와 연구 결과들은 **“데이터가 충분히 크다면, 인간이 설계한 편향(Inductive Bias)보다 데이터 자체에서 패턴을 학습하는 것이 더 우월하다”**는 것을 시사한다.14 CNN의 귀납적 편향은 데이터가 적을 때는 가이드라인이 되지만, 데이터가 무한히 많아지면 오히려 모델의 표현력을 제한하는 ’유리천장(Glass Ceiling)’이 될 수 있다. 반면 ViT는 이러한 제약이 없기 때문에 데이터가 늘어나는 만큼 성능이 계속해서 향상되는 스케일링 법칙(Scaling Laws)을 따르게 된다.11</p>
<h3>5.4  연산 효율성의 역설</h3>
<p>더욱 놀라운 점은 대규모 학습 시의 연산 효율성이다. ViT는 CNN보다 파라미터 수가 많아 보일 수 있지만, 행렬 연산에 최적화된 트랜스포머의 구조 덕분에 하드웨어(TPU/GPU) 가속 효율이 매우 높다. JFT-300M 데이터셋으로 최고 성능을 달성할 때, ViT는 동급 성능의 CNN 모델(ResNet 기반 BiT-L) 대비 **2~4배 더 적은 컴퓨팅 자원(TPUv3-core-days)**을 소모했다.11 이는 훈련 비용을 획기적으로 절감할 수 있음을 의미하며, 초거대 AI 모델 시대에 ViT가 선택받는 결정적인 이유 중 하나가 되었다.</p>
<h2>6.  결론: 모달리티의 장벽을 허물다</h2>
<p>ViT가 이미지를 <span class="math math-inline">16 \times 16</span> 크기의 토큰으로 쪼개어 처리하는 방식은 단순한 기술적 시도를 넘어, 인공지능 역사에 남을 중요한 전환점이다. 픽셀을 패치로, 패치를 임베딩 벡터로 변환함으로써 이미지는 언어와 동일한 수학적 공간에 놓이게 되었다.</p>
<p>이 장에서 우리는 다음과 같은 핵심 통찰들을 확인했다.</p>
<ol>
<li><strong>패치 토큰화:</strong> <span class="math math-inline">O(N^2)</span>의 계산 복잡도를 극복하고 이미지를 시퀀스로 변환하는 효율적인 방법론.</li>
<li><strong>위치 임베딩:</strong> 1차원 시퀀스 속에서 2차원 공간 정보를 스스로 학습해 내는 모델의 능력.</li>
<li><strong>귀납적 편향의 제거:</strong> 인간의 사전 지식(Locality)을 줄이고 데이터의 힘(Scale)을 믿는 철학적 전환.</li>
<li><strong>통합 아키텍처:</strong> NLP와 비전 모델의 설계가 트랜스포머라는 하나의 축으로 수렴.</li>
</ol>
<p>물론 2차원 구조 정보를 무시하고 1차원 시퀀스로 변환하는 과정에서 발생하는 비효율성이나, 작은 데이터셋에서의 학습 불안정성 등은 여전히 해결해야 할 과제로 남아 있다.12 최근 연구들은 CNN의 장점(지역성, 효율성)을 ViT에 다시 도입하는 하이브리드(Hybrid) 구조나, 계층적(Hierarchical) 구조를 가진 Swin Transformer 등으로 진화하고 있다.7 또한 채널별로 독립적인 토큰을 생성하는 ChannelViT와 같은 연구는 다중 채널 이미지(위성, 의료) 처리에 대한 새로운 가능성을 보여준다.17</p>
<p>하지만 “이미지를 단어처럼 처리한다“는 ViT의 대담한 발상은 인공지능이 시각 정보를 처리하는 방식에 있어 되돌릴 수 없는 ’특이점(Singularity)’을 만들어냈다. 이제 우리는 텍스트, 이미지, 오디오, 비디오를 구분하지 않고 하나의 거대한 트랜스포머 모델에 쏟아부어 학습시키는 ’멀티모달 파운데이션 모델(Multimodal Foundation Models)’의 시대를 목격하고 있다. 그리고 그 중심에는 2020년, 이미지를 16x16 토큰으로 잘라내기 시작한 비전 트랜스포머의 혁신이 자리 잡고 있다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Vision Transformers (ViT) Explained - Pinecone, https://www.pinecone.io/learn/series/image-search/vision-transformers/</li>
<li>Vision Transformers Explained: The Future of Computer Vision? - Roboflow Blog, https://blog.roboflow.com/vision-transformers/</li>
<li>Vision Transformer explanation and implementation with PyTorch - GitHub, https://github.com/nerminnuraydogan/vision-transformer</li>
<li>Position Embeddings for Vision Transformers, Explained | by Skylar Jean Callis - Medium, https://medium.com/data-science/position-embeddings-for-vision-transformers-explained-a6f9add341d5</li>
<li>Vision Transformers (ViTs): Computer Vision with Transformer Models - DigitalOcean, https://www.digitalocean.com/community/tutorials/vision-transformer-for-computer-vision</li>
<li>Vision Transformer (ViT) Architecture - GeeksforGeeks, https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/</li>
<li>CNN vs Vision Transformer (ViT): Which Wins in 2025? | by HIYA CHATTERJEE - Medium, https://hiya31.medium.com/cnn-vs-vision-transformer-vit-which-wins-in-2025-e1cb2dfcb903</li>
<li>Introduction to ViT (Vision Transformers): Everything You Need to Know - Lightly, https://www.lightly.ai/blog/vision-transformers-vit</li>
<li>An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale (Vision Transformers) - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2021/03/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale-vision-transformers/</li>
<li>Decoding Vision Transformers: A Deep Dive into ViT and Its Multimodal AI Applications, https://medium.com/@az.tayyebi/decoding-vision-transformers-a-deep-dive-into-vit-and-its-multimodal-ai-applications-96ffd0634c87</li>
<li>ViT: Transformers began to conquer the computer vision field | by …, https://medium.com/@kdk199604/vit-transformers-began-to-conquer-the-computer-vision-field-777f3602136e</li>
<li>Vision Transformer vs. CNN for Object Detection - Roboflow Blog, https://blog.roboflow.com/vision-transformer-vs-cnn-for-detection/</li>
<li>Comparing Vision Transformers and Convolutional Neural Networks for Image Classification: A Literature Review - MDPI, https://www.mdpi.com/2076-3417/13/9/5521</li>
<li>Vision Transformers (ViT) Explained: Are They Better Than CNNs …, https://towardsdatascience.com/vision-transformers-vit-explained-are-they-better-than-cnns/</li>
<li>A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation - arXiv, https://arxiv.org/html/2510.04794v1</li>
<li>Scaling Pre-training to One Hundred Billion Data for Vision Language Models - arXiv, https://arxiv.org/html/2502.07617v1</li>
<li>Channel Vision Transformers: An Image Is Worth 1×16×16 Words - arXiv, https://arxiv.org/html/2309.16108v4</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>